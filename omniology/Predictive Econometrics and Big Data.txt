Studies in Computational Intelligence 753
Vladik Kreinovich
Songsak Sriboonchitta
Nopasit Chakpitak    Editors 
Predictive 
Econometrics 
and Big Data

Studies in Computational Intelligence
Volume 753
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Studies in Computational Intelligence” (SCI) publishes new develop-
ments and advances in the various areas of computational intelligence—quickly and
with a high quality. The intent is to cover the theory, applications, and design
methods of computational intelligence, as embedded in the ﬁelds of engineering,
computer science, physics and life sciences, as well as the methodologies behind
them. The series contains monographs, lecture notes and edited volumes in
computational intelligence spanning the areas of neural networks, connectionist
systems, genetic algorithms, evolutionary computation, artiﬁcial intelligence,
cellular automata, self-organizing systems, soft computing, fuzzy systems, and
hybrid intelligent systems. Of particular value to both the contributors and the
readership are the short publication timeframe and the world-wide distribution,
which enable both wide and rapid dissemination of research output.
More information about this series at http://www.springer.com/series/7092

Vladik Kreinovich
• Songsak Sriboonchitta
Nopasit Chakpitak
Editors
Predictive Econometrics
and Big Data
123

Editors
Vladik Kreinovich
Computer Science Department
University of Texas at El Paso
El Paso, TX
USA
Songsak Sriboonchitta
International College
Chiang Mai University
Chiang Mai
Thailand
Nopasit Chakpitak
International College
Chiang Mai University
Chiang Mai
Thailand
ISSN 1860-949X
ISSN 1860-9503
(electronic)
Studies in Computational Intelligence
ISBN 978-3-319-70941-3
ISBN 978-3-319-70942-0
(eBook)
https://doi.org/10.1007/978-3-319-70942-0
Library of Congress Control Number: 2017959632
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Econometrics is a branch of economics that uses mathematical (especially statis-
tical) methods to analyze economic systems, to forecast economic and ﬁnancial
dynamics, and to develop strategies for achieving desirable economic performance.
Traditional econometric techniques have been focused on the quantitative
description of economic phenomena. However, the ultimate goal of econometrics—
as well as the ultimate goal of science in general—is to predict future development
of economics and to develop strategies that optimize the future state of economics.
It is therefore desirable to develop techniques that are speciﬁcally aimed at pre-
dicting economic phenomena. Such predictive econometric techniques—and their
applications to real-life economic and ﬁnancial situations—are one of the main foci
of this volume.
Another focus of this book is related to the fact that in the modern world, in
which computers are ubiquitous, the amount of economic-related data generated
and processed by these computers has grown exponentially. The amount of
available economic data is so huge that many traditional statistical data processing
algorithms are no longer capable of processing all these data in real time. To
process this data, we need to utilize “big data” techniques speciﬁcally developed for
processing such huge amounts of data and we need to develop big data versions
of the state-of-the-art econometric techniques and algorithms. This is a new and
promising direction in econometrics. Big data is the main subject of this volume’s
keynote paper by Dr. Chaitanya Baru from the US National Science Foundation.
In addition to papers on predictive econometric techniques and on big data
applications, this book also contains applications of more traditional statistical
techniques to econometric problems.
We hope that this volume will help practitioners to learn how to apply new
predictive and big data econometric techniques and help researchers to further
improve the existing predictive and big data techniques and to come up with new
ideas on how econometric techniques can utilize large amounts of data to make
more accurate predictions.
We want to thank all the authors for their contributions and all anonymous
referees for their thorough analysis and helpful comments.
v

The publication of this volume is partly supported by the Chiang Mai School of
Economics (CMSE), Thailand. Our thanks go to Dean Pirut Kanjanakaroon and
CMSE
for
providing
crucial
support.
Our
special
thanks
go
to
Prof.
Hung T. Nguyen for his valuable advice and constant support.
We would also like to thank Prof. Janusz Kacprzyk (Series Editor) and
Dr. Thomas Ditzinger (Senior Editor, Engineering/Applied Sciences) for their
support and cooperation in this publication.
September 2017
Vladik Kreinovich
Songsak Sriboonchitta
Nopasit Chakpitak
vi
Preface

Contents
Keynote Address
Data in the 21st Century . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Chaitanya Baru
Fundamental Theory
Model-Assisted Survey Estimation with Imperfectly Matched
Auxiliary Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
F. Jay Breidt, Jean D. Opsomer, and Chien-Min Huang
COBra: Copula-Based Portfolio Optimization . . . . . . . . . . . . . . . . . . . .
36
Marc S. Paolella and Paweł Polak
Multiple Testing of One-Sided Hypotheses: Combining Bonferroni
and the Bootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
Joseph P. Romano and Michael Wolf
Exploring Message Correlation in Crowd-Based Data Using Hyper
Coordinates Visualization Technique . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
Tien-Dung Cao, Dinh-Quyen Nguyen, and Hien Duy Tran
Bayesian Forecasting for Tail Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
Cathy W. S. Chen and Yu-Wen Sun
Smoothing Spline as a Guide to Elaborate Explanatory Modeling . . . . .
146
Chon Van Le
Quantifying Predictive Uncertainty Using Belief Functions:
Different Approaches and Practical Construction. . . . . . . . . . . . . . . . . .
157
Thierry Denœux
Kuznets Curve: A Simple Dynamical System-Based Explanation . . . . . .
177
Thongchai Dumrongpokaphan and Vladik Kreinovich
vii

A Calibration-Based Method in Computing Bayesian Posterior
Distributions with Applications in Stock Market . . . . . . . . . . . . . . . . . .
182
Dung Tien Nguyen, Son P. Nguyen, Uyen H. Pham,
and Thien Dinh Nguyen
How to Estimate Statistical Characteristics Based on a Sample:
Nonparametric Maximum Likelihood Approach Leads to Sample
Mean, Sample Variance, etc. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
Vladik Kreinovich and Thongchai Dumrongpokaphan
How to Gauge Accuracy of Processing Big Data: Teaching
Machine Learning Techniques to Gauge Their Own Accuracy . . . . . . .
198
Vladik Kreinovich, Thongchai Dumrongpokaphan, Hung T. Nguyen,
and Olga Kosheleva
How Better Are Predictive Models: Analysis on the Practically
Important Example of Robust Interval Uncertainty . . . . . . . . . . . . . . . .
205
Vladik Kreinovich, Hung T. Nguyen, Songsak Sriboonchitta,
and Olga Kosheleva
Quantitative Justiﬁcation for the Gravity Model in Economics . . . . . . .
214
Vladik Kreinovich and Songsak Sriboonchitta
The Decomposition of Quadratic Forms Under Skew
Normal Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
Ziwei Ma, Weizhong Tian, Baokun Li, and Tonghui Wang
Joint Plausibility Regions for Parameters of Skew Normal Family . . . .
233
Ziwei Ma, Xiaonan Zhu, Tonghui Wang,
and Kittawit Autchariyapanitkul
On Parameter Change Test for ARMA Models with Martingale
Difference Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246
Haejune Oh and Sangyeol Lee
Agent-Based Modeling of Economic Instability . . . . . . . . . . . . . . . . . . .
255
Akira Namatame
A Bad Plan Is Better Than No Plan: A Theoretical Justiﬁcation
of an Empirical Observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
266
Songsak Sriboonchitta and Vladik Kreinovich
Shape Mixture Models Based on Multivariate Extended Skew
Normal Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
Weizhong Tian, Tonghui Wang, Fengrong Wei, and Fang Dai
viii
Contents

Plausibility Regions on Parameters of the Skew Normal
Distribution Based on Inferential Models . . . . . . . . . . . . . . . . . . . . . . . .
287
Xiaonan Zhu, Baokun Li, Mixia Wu, and Tonghui Wang
Measures of Mutually Complete Dependence for Discrete
Random Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303
Xiaonan Zhu, Tonghui Wang, S. T. Boris Choy,
and Kittawit Autchariyapanitkul
Applications
To Compare the Key Successful Factors When Choosing
a Medical Institutions Among Taiwan, China, and Thailand . . . . . . . . .
321
Tzong-Ru (Jiun-Shen) Lee, Yu-Ting Huang, Man-Yu Huang,
and Huan-Yu Chen
Forecasting Thailand’s Exports to ASEAN
with Non-linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339
Petchaluck Boonyakunakorn, Pathairat Pastpipatkul,
and Songsak Sriboonchitta
Thailand in the Era of Digital Economy: How Does Digital
Technology Promote Economic Growth? . . . . . . . . . . . . . . . . . . . . . . . .
350
Noppasit Chakpitak, Paravee Maneejuk, Somsak Chanaim,
and Songsak Sriboonchitta
Comparing Linear and Nonlinear Models in Forecasting
Telephone Subscriptions Using Likelihood Based Belief Functions. . . . .
363
Noppasit Chakpitak, Woraphon Yamaka,
and Songsak Sriboonchitta
Volatility in Thailand Stock Market Using High-Frequency Data . . . . .
375
Saowaluk Duangin, Jirakom Sirisrisakulchai, and Songsak Sriboonchitta
Technology Perception, Personality Traits and Online Purchase
Intention of Taiwanese Consumers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
392
Massoud Moslehpour, Ha Le Thi Thanh, and Pham Van Kien
Analysis of Thailand’s Foreign Direct Investment in CLMV
Countries Using SUR Model with Missing Data . . . . . . . . . . . . . . . . . . .
408
Chalerm Jaitang, Paravee Maneejuk, Aree Wiboonpongse,
and Songsak Sriboonchitta
The Role of Oil Price in the Forecasts of Agricultural
Commodity Prices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
Rossarin Osathanunkul, Chatchai Khiewngamdee, Woraphon Yamaka,
and Songsak Sriboonchitta
Contents
ix

Does Forecasting Beneﬁt from Mixed-Frequency Data
Sampling Model: The Evidence from Forecasting GDP Growth
Using Financial Factor in Thailand . . . . . . . . . . . . . . . . . . . . . . . . . . . .
430
Natthaphat Kingnetr, Tanaporn Tungtrakul, and Songsak Sriboonchitta
A Portfolio Optimization Between US Dollar Index
and Some Asian Currencies with a Copula-EGARCH Approach. . . . . .
443
Ji Ma, Jianxu Liu, and Songsak Sriboonchitta
Technical Efﬁciency Analysis of China’s Agricultural Industry:
A Stochastic Frontier Model with Panel Data . . . . . . . . . . . . . . . . . . . .
454
Ji Ma, Jianxu Liu, and Songsak Sriboonchitta
Empirical Models of Herding Behaviour for Asian Countries
with Confucian Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
464
Munkh-Ulzii, Massoud Moslehpour, and Pham Van Kien
Forecasting the Growth of Total Debt Service Ratio with ARIMA
and State Space Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
492
Kobpongkit Navapan, Petchaluck Boonyakunakorn,
and Songsak Sriboonchitta
Effect of Macroeconomic Factors on Capital Structure
of the Firms in Vietnam: Panel Vector Auto-regression
Approach (PVAR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
502
Nguyen Ngoc Thach and Tran Thi Kim Oanh
Emissions, Trade Openness, Urbanisation, and Income in Thailand:
An Empirical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
517
Rossarin Osathanunkul, Natthaphat Kingnetr, and Songsak Sriboonchitta
Analysis of Risk, Rate of Return and Dependency of REITs
in ASIA with Capital Asset Pricing Model . . . . . . . . . . . . . . . . . . . . . . .
536
Rungrapee Phadkantha, Woraphon Yamaka, and Roengchai Tansuchat
Risk Valuation of Precious Metal Returns by Histogram Valued
Time Series. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
549
Pichayakone Rakpho, Woraphon Yamaka, and Roengchai Tansuchat
Factors Affecting Consumer Visiting Spa Shop: A Case in Taiwan . . . .
563
Meng-Chun Susan Shen, I-Tien Chu, and Wan-Tran Huang
The Understanding of Dependent Structure and Co-movement
of World Stock Exchanges Under the Economic Cycle . . . . . . . . . . . . .
573
Songsak Sriboonchitta, Chukiat Chaiboonsri, and Jittima Singvejsakul
The Impacts of Macroeconomic Variables on Financials
Sector and Property and Construction Sector Index Returns
in Stock Exchange of Thailand Under Interdependence Scheme . . . . . .
590
Wilawan Srichaikul, Woraphon Yamaka, and Roengchai Tansuchat
x
Contents

Generalize Weighted in Interval Data for Fitting a Vector
Autoregressive Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
600
Teerawut Teetranont, Woraphon Yamaka, and Songsak Sriboonchitta
Asymmetric Effect with Quantile Regression
for Interval-Valued Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
613
Teerawut Teetranont, Woraphon Yamaka, and Songsak Sriboonchitta
The Future of Global Rice Consumption: Evidence from Dynamic
Panel Data Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
629
Duangthip Sirikanchanarak, Tanaporn Tungtrakul,
and Songsak Sriboonchitta
The Analysis of the Effect of Monetary Policy on Consumption
and Investment in Thailand . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
643
Jirawan Suwannajak, Woraphon Yamaka, Songsak Sriboonchitta,
and Roengchai Tansuchat
Investigating Relationship Between Gold Price and Crude Oil
Price Using Interval Data with Copula Based GARCH . . . . . . . . . . . . .
656
Teerawut Teetranont, Somsak Chanaim, Woraphon Yamaka,
and Songsak Sriboonchitta
Simultaneous Conﬁdence Intervals for All Differences of Means
of Normal Distributions with Unknown Coefﬁcients of Variation . . . . . .
670
Warisa Thangjai, Sa-Aat Niwitpong, and Suparat Niwitpong
Estimating the Value of Cultural Heritage Creativity from the
Viewpoint of Tourists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
683
Phanee Thipwong, Chung-Te Ting, Yu-Sheng Huang, Yun-Zu Chen,
and Wan-Tran Huang
A Bibliometric Review of Global Econometrics Research:
Characteristics and Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
692
Van-Chien Pham and Man-Ling Chang
Macro-Econometric Forecasting for During Periods of Economic
Cycle Using Bayesian Extreme Value Optimization Algorithm. . . . . . . .
706
Satawat Wannapan, Chukiat Chaiboonsri, and Songsak Sriboonchitta
Forecasting of VaR in Extreme Event Under Economic Cycle
Phenomena for the ASEAN-4 Stock Exchange . . . . . . . . . . . . . . . . . . . .
724
Satawat Wannapan, Pattaravadee Rakpuang, and Chukiat Chaiboonsri
Interval Forecasting on Big Data Context . . . . . . . . . . . . . . . . . . . . . . .
737
Berlin Wu
Contents
xi

Bayesian Empirical Likelihood Estimation for Kink Regression
with Unknown Threshold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
752
Woraphon Yamaka, Pathairat Pastpipatkul, and Songsak Sriboonchitta
Spatial Choice Modeling Using the Support Vector
Machine (SVM): Characterization and Prediction . . . . . . . . . . . . . . . . .
767
Yong Yoon
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
779
xii
Contents

Keynote Address

Data in the 21st Century
Chaitanya Baru1,2(B)
1 University of California San Diego, San Diego, USA
cbaru@ucsd.edu
2 US National Science Foundation, Alexandria, USA
Abstract. The past couple of decades have witnessed exponential
growth in data, due to the penetration of information technology across
all aspects of science and society; the increasing ease with which we are
able to collect more data; and the growth of Internet-scale, planet-wide
Web-based and mobile services—leading to the notion of “big data”.
While the emphasis so far has been on developing technologies to man-
age the volume, velocity, and variety of the data, and to exploit available
data assets via machine learning techniques, going forward the emphasis
must also be on translational data science and the responsible use of all
of these data in real-world applications. Data science in the 21st century
must provide trust in the data and provide responsible and trustwor-
thy techniques and systems by supporting the notions of transparency,
interpretability, and reproducibility. The future oﬀers exciting opportuni-
ties for transdisciplinary research and convergence among disciplines—
computer science, statistics, mathematics, and the full range of disci-
plines that impact all aspects of society. Econometrics and economics
can ﬁnd an important role in this convergence of ideas.
Keywords: Data science · Translational data science · Convergence
Responsible data management · Trustworthiness
1
Background and Context
The past few decades we have witnessed continuous growth in the amount of
data in all spheres across science and society, leading to the idea of the “4th
paradigm” of science [9], and giving rise to the phrase, “big data”. In the early
days of computing, data was managed simply, as “ﬂat ﬁles”. That was soon
followed by “structured ﬁles”—ﬁles with an internal or external index structure
to allow for faster access to the contents of the ﬁle [1]. Early applications of
such data were in the areas of accounting and ﬁnance, in addition to basic sci-
entiﬁc and cryptographic calculations. A decade or two later, in the 1980s, with
growing use of computers in business, ﬁnance, and commerce, and the need to
manage increasing amounts of data in multi-user environments, came the new
technology of relational databases. Corporate data was converted to structured
databases, following principles established by relational database theory [6].
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_1

4
C. Baru
During that same period, the amount of scientiﬁc data was also beginning to
grow due to the availability of higher resolution sensors and detectors; launch of
science initiatives like the Human Genome Project and supercolliders; and, the
ability to build larger supercomputers generating larger amounts of data—all
made possible by increasingly cheaper and more eﬃcient processing and storage
hardware. In science, the use of computing began to spread across many diﬀerent
research communities. Business applications went beyond transaction processing
and simple query processing to decision support and use in strategic planning.
The next knee in the data growth curve occurred with the growth of the
Internet and the advent of the Web—beginning in the 1990s and into the 2000s—
leading to the creation of “internet-scale” services, businesses, and operations.
While data has always been with us, this last inﬂexion in growth of data was
diﬀerent. Until then, data in the scenarios described above were collected “by
design”—through deliberate design of corporate databases and designed data
collection eﬀorts in science. The explosion in Web data—and afterwards data
from mobile computing and smartphones—gave rise to the notion of organic
data [8]: observational data that is not collected deliberately, but is available
simply as a “by-product” of other actions such as commercial transactions, social
media interactions, interactions with online services, location-based information
from mobile platforms, and the like. A major component of this huge explo-
sion in data was “unstructured” and “semistructured” data (really, data with
varying structure)—text, images, video, weblogs. In addition to the “organic”
nature of the data (versus “designed” data), another key characteristic was that
these data were being used for multiple purposes—not just for a single, designed
application. For example, location-based information from mobile platforms can
provide information about instantaneous population density in a given spatial
extent. That information could be used for multiple purposes: for traﬃc control;
for drawing up and modifying dynamic disaster response/evacuation plans; as a
spot census; for load balancing cell phone traﬃc across multiple cell towers; or,
for planning police and security operations.
The visible successes in data exploitation by large, Internet-scale (indeed,
planet-scale) companies, like Google, Facebook, Amazon, Uber, as well as by
other companies in other sectors such as, say, ﬁnance, intelligence, and others,
have led to the current enduring interest in “big data”. The Data Deluge has
been mentioned as a phenomenon [7]; there is the notion of the “unreasonable
eﬀectiveness of data”; and, data is said to be the “new oil”. There are also many
cautionary messages, for example, in “Weapons of Math Destruction” [16], and
concerns regarding bias, fairness, and transparency related to data1. As data
collection increases, and the power of data analytics begins to dominate every
aspect of our lives, one must pay serious attention to consequences from data-
driven decision making. This will be a key challenge of data in the 21st century.
1 Data and Society, https://datasociety.net/.

Data in the 21st Century
5
While the 20th century witnessed the invention of digital computing—ending
with the introduction of the Web, and the early 21st century witnessed explosion
in data—leading to the notion of “big data”, the deﬁning character of the data in
the 21st century will be in how we manage the data that we are collecting (or, one
might even say, “receiving”)—and how we use that data for the beneﬁt of science
and society. The transition must be made from just thinking about algorithms
and technology to developing uniﬁed, holistic approaches that bring together a
number of seemingly disparate concerns such as, say, data uncertainty, quality,
bias, fairness, in conjunction with systems that are able to provide transparency,
interpretability, and reproducibility of results. A convergence of ideas is needed2.
From a research standpoint, teams of researchers will need to come together to
tackle problems in a holistic fashion. While single investigator-driven research
will continue to thrive, it also misses a larger context and the holistic view, and
the important tradeoﬀs involved in that larger context. The issue of ethics and
the impacts of data-driven decision-making will be an essential concern.
In short, data in the 21st century will require a “reboot” of a sort. From the
deep silos of expertise that we have created over the past 4–5 decades, we now
need to move to a convergent approach, where experts from widely diﬀerent areas
and ﬁelds come together to solve complex, societal-scale problems. Indeed, the
fact that so much data is available at a time when societies and economies appear
to be undergoing a huge change is a boon: data-driven approaches will be essen-
tial, but they must be holistic in their approach. As noted in a recent Dagstuhl
workshop titled, “Data—Responsibly” [2], societies are becoming “data-driven,”
and large scale data analysis (“big data”) now reaches all of us in our private
lives, and is a dominant force in commercial domains as varied as manufacturing,
e-commerce and personalized medicine. Big data assists in and, in some cases,
fully automates decision making in both the public and private sectors. Data-
driven algorithms are used in criminal sentencing—determining who goes free
and who remains behind bars; in college admissions—granting or denying access
to education; and, in employment and credit decisions—oﬀering or withholding
economic opportunities. The promise of using these approaches is that they can
improve people’s lives, accelerate scientiﬁc discovery and innovation, and enable
broader participation. Yet, if not used responsibly, the big data phenomenon
can increase economic inequality and aﬃrm systemic bias, polarize rather than
democratize, and deny opportunities rather than improve access. Worse yet, all
this can be done in a way that is non-transparent and deﬁes public scrutiny.
While the massive explosion in data, and the “unreasonable eﬀectiveness” of
analytics on that data, have brought attention to the notion of “big data”, it
is important to exercise care and pay equal attention to all data—big or small.
All data have a lifecycle—from collection to curation, to use in analytics, to
preservation. Indeed, there is increasingly a discussion of “small data”—i.e., the
data that pertains to individuals and, therefore in some sense, “belongs to” an
individual. Thus, the key notions going forward are not just about “big data”,
but about data science, more broadly.
2 Convergence Research at NSF: https://www.nsf.gov/od/oia/convergence/index.jsp.

6
C. Baru
In the next sections, we will discuss 21st century issues related to foundations
of data science; issues in developing software systems for data science to manage
and analyze data; and translational data science—i.e., the translation of data
science concepts, theories, and systems into real-world applications. We will end
with some observations about big data and econometrics.
2
Data Science Foundations
In the United States, the BIGDATA research program was launched in 2012, as
a cross-agency eﬀort among NSF, NIH, and DARPA [5]. Shortly thereafter, a
BIGDATA program was established at NSF. The program funded research in
foundations of big data as well as innovative applications of big data [22]. A
federal R&D strategic plan for big data was released in the United States in
May 2016. Not surprisingly, much of the research in big data generally focused
on scaling issues in data management and machine learning, statistical modeling
issues with organic data, the so-called “volume, velocity, and variety” problems,
modeling of new types of data, especially, graphs, time series, and geospatial and
spatiotemporal data, and “needle in the haystack” problems. The explosion in
data motivated research on whether and how techniques scaled up with data size
(volume), and their performance at scale. For example, a project on “Analyti-
cal Approaches to Massive Data Computation with Applications to Genomics,”
funded by the NSF in 2013 [23], has the goal of designing and testing mathe-
matically well-founded algorithmic and statistical techniques for analyzing large
scale, heterogeneous and noisy data. The project was motivated by the chal-
lenges in analyzing molecular biology data that are typically large and noisy,
given that DNA/RNA sequence data repositories have been growing at a “super-
exponential” rate. However, the methods and techniques developed would be
broadly applicable to other scientiﬁc communities that process massive multi-
variant data sets.
Another project on “Big Tensor Data Mining” [17], addressed the issue of
insuﬃcient theory and methods for big sparse tensor representations of data. It
investigated the theory, scalability of algorithms and applications of tensor-based
representations of big data, addressing terabyte and petabyte scaling issues;
distributed fault-tolerant computations; and datasets with large proportions of
missing data. Early projects also addressed the data management aspects of
big data. For example, the project on “Formal Foundation for Big Data Man-
agement” developed a multi-platform software middleware for expressing and
optimizing ad hoc data analytics techniques. The goal of is to augment and inte-
grate existing analytics solutions to facilitate and improve methods typically use
by a community of users, to make it easier for end users to conduct complex data
analyses on big data and on large computer clusters. The project has developed
middleware that is accessible as a Web-based service3.
Another project on the “Theory and Algorithms for Parallel Probabilistic
Inference with Big Data, via Big Model, in Realistic Distributed Computing
3 http://myriadb.cs.washington.edu.

Data in the 21st Century
7
Environments,”4 focuses on scale-up and parallelization of Bayesian machine
learning—providing a powerful and theoretically justiﬁed framework for model-
ing a wide variety of datasets. A suite of complementary distributed inference
algorithms were developed for hierarchical Bayesian models, covering the most
commonly used Bayesian machine learning methods. The project focused on
combining speed and scalability with theoretical guarantees that allow one to
assess the accuracy of the resulting methods, thereby allowing practitioners to
make trade-oﬀs between speed and accuracy. The project developed techniques
applicable to a broad spectrum of hierarchical Bayesian models that can be com-
bined as needed for arbitrary probabilistic models, whether they are parametric
or nonparametric, discriminative or generative [27].
Foundational data science oﬀers many opportunities for interdisciplinary col-
laborations among computer scientists, statisticians, and mathematicians, to
fully consider the computation-statistics tradeoﬀ. Traditionally, the statistics
community has focused mainly on the inferential aspect, while the theoretical
computer science and mathematics communities have focused more on the com-
putational aspects. The NSF sponsored a workshop on Theoretical Foundations
of Data Science, in April 2016, to explore future directions in this area5. The
meeting brought together computer scientists (experts in algorithmics and the-
oretical aspects), statisticians, and mathematician, with the notion that future
progress in this area requires a convergence (at least) among these disciplines,
but perhaps more. A range of topics were discussed at the meeting, and incorpo-
rated into a workshop report6. There was a recognition that foundational issues
must be considered across the full lifecycle of data, rather than just the analy-
sis phase. It is well-known among data scientists that a large fraction of the
total data analysis time is spent in data preparation and preprocessing, which is
often ignored in theoretical studies. Indeed, data preparation and preprocessing
pose many intellectually challenging problems that are related to deep mathe-
matical issues that cannot be easily be formalized. It is not merely “engineer-
ing”, but rather a critical part of deploying models in production. It constitutes
the before and after of many machine learning problems that have a greater
cachet. Also, the report states, “data science involves iterative procedures with
a dynamic feedback loop. Typically, the formulations of so-called online algo-
rithms in machine learning are not particularly well-suited to understanding
the iterative aspect of data science more generally. Targets can change as more
data are acquired; instead of restricting our attention to idealized systems under
restrictive assumptions, dynamic data collection is general, heterogeneous, and
messy. Latency is an important issue, and there are also humans in the loop.
Data science foundations should investigate how data analysis techniques, user
behaviors, and the data collection process ﬁt together.”
4 https://nsf.gov/awardsearch/showAward?AWD ID=1447676&HistoricalAwards=
false.
5 TFoDS Workshop, http://www.cs.rpi.edu/TFoDS/.
6 TFoDS Workshop Report, http://www.cs.rpi.edu/TFoDS/TFoDS v5.pdf.

8
C. Baru
A workshop identiﬁed that a potentially high impact area of foundational
research is the integration of statistical and computational approaches into a
united theoretical framework. Topics for interdisciplinary collaboration could
include studies of randomized numerical linear algebra [11]; signal process-
ing/harmonic analysis on graphs; nonconvex statistical optimization; combining
physical and statistical models; mixed type and multi-modality data; applied
representation theory and non-commutative harmonic analysis; topological data
analysis and homological algebra; security, privacy, and algorithmic fairness;
provenance and reproducibility; and, the development of a common terminol-
ogy/language to enable interdisciplinary collaborations.
Indeed, NSF’s recent program in Transdisciplinary Research in Principles
of Data Science (TRIPODS) picks up on these recommendations and promotes
interdisciplinary research across computer science, statistics, and mathematics.
Phase I of this program has recently been funded, and supports twelve col-
laborative projects that explore research in this direction. For example, one of
the funded eﬀorts, “Towards a Uniﬁed Theory of Structure, Incompleteness &
Uncertainty in Heterogeneous Graphs,” brings together researchers from math-
ematics, statistics, and computer science to develop a uniﬁed theory of data
science applied to uncertain and heterogeneous graph and network data, with
the goal of developing a new foundation for data science to deal with incom-
plete, noisy, heterogeneous data with multiple modalities and multiple scales in
the context of graph and network data. A uniﬁed theory will be developed to
understand how to quantify the uncertainty in the system that arises from the
uncertainty in the relationships among its actors via transdisciplinary collabora-
tion among statisticians, mathematicians, and computer scientists. This project
plans to investigate models of algorithms on uncertain network data, combining
techniques from sub-linear algorithms with Bayesian methods. Another theme
is on how algorithms can beneﬁt from data uncertainty in the context of pri-
vacy, disclosure, and robustness to noise. Both of these themes require marry-
ing computational approaches to uncertainty with statistical and mathematical
approaches for uncertainty.
Another TRIPODS project, “Algorithms for Data Science: Complexity, Scal-
ability, and Robustness,” examines the complexity, scalability and robustness of
data science algorithms [10]. Since the challenges that a range of ﬁelds now face
are no longer easily handled by ideas from a single discipline, a central goal of the
project is to provide a common language and unifying methods for addressing
contemporary data science challenges. Each of the three disciplines of computer
science, mathematics, and statistics has rich theories of complexity and robust-
ness. These theories have inﬂuenced the design of the available tools that are used
to address real world computational problems. Going forward, there is a need
for new algorithms and design principles that unify ideas and provide a common
language for addressing contemporary data science challenges. The complexity
and algorithmic questions this work seeks to address include: (i) how to unify
various notions of complexity (which range from information theoretic to com-
putational to black box oracle models), (ii) how to unify notions of robustness

Data in the 21st Century
9
and adaptivity (e.g., how solutions and methods change as oracle models are
corrupted by random or adversarial noise), (iii) how to address optimization
challenges due to nonconvexity, and (iv) how to use these uniﬁed approaches to
design more eﬀective scalable tools, in theory and practice.
Foundations of data in the 21st century must embrace not only the techni-
cal issues, but also the larger issues of fairness and ethics. A third TRIPODS
project, “Data Science for Improved Decision-Making: Learning in the Context
of Uncertainty, Causality, Privacy, and Network Structures,” will examine issues
related to privacy and fairness, learning on social graphs, learning to intervene,
uncertainty quantiﬁcation, and deep learning. As data science becomes pervasive
across many areas of society, and as it is increasingly used to aid decision-making
in sensitive domains, it becomes crucial to protect individuals by guaranteeing
privacy and fairness. The project proposes to research the theoretical founda-
tions to providing such guarantees and to surface inherent limitations. Further-
more, data-driven approaches to learning good interventions (including policies,
recommendations, and treatments) inspire challenging questions about the foun-
dations of sequential experimental design, counterfactual reasoning, and causal
inference.
In sum, foundational approaches are needed to establish data science on
a ﬁrm multidisciplinary footing with principles drawn from computer science,
statistics, and mathematics, as well as from areas related to privacy, fairness,
and ethics.
3
Systems
Data science hardware and software systems must support the needs of data
in the 21st century. As mentioned earlier in Sect. 1, the primary focus of sys-
tems thus far has been on managing the scale (volume) of data and ensuring
that systems continue to perform well as data volumes and velocities increase.
Signiﬁcant work has also been done in integration of data from heterogeneous
sources, and in supporting the heterogeneity of data types, from structured data,
to images, video, and more recently graph data—a rapidly increasing modality
of data. A number of current eﬀorts focus on support for streaming data and
real time analytics on data streams.
Hardware systems. In terms of computing hardware, we are in a period of
“hardware renaissance”, with a number of new technologies becoming available
for data storage and for computing. Storage/memory hierarchies have become
deeper with the availability of solid state disk (SSD), and a variety of non-
volatile memories (NVM). Computing has entered the so-called “post CPU”
era—graphical processing units (GPUs) and other specialized hardware, FPGAs,
ASICs, and Google’s Tensorﬂow Processing Units [18], are being utilized to speed
up machine learning applications. Major software suites are available for machine
learning such as MXNet from Amazon Web Services (AWS)7, Google’s Tensor-
7 Apache MXnet, https://aws.amazon.com/mxnet/.

10
C. Baru
ﬂow software8, Microsoft’s Cognitive Toolkit9, and IBM’s Cognitive Comput-
ing suite10. Innovation in hardware and software can be expected to continue,
including in end-to-end software environments for data science to help increase
end-user productivity.
In terms of new directions and vistas for data in the 21st century, there is
a pressing need as well as community interest in developing systems to support
trustworthiness as an intrinsic feature of data science [14]11. Systems that sup-
port transparency, interpretability and reproducibility of results can promote
trustworthiness. The problem becomes quite challenging when data quality can
vary—over time and across diﬀerent sources, and when the data may be dynamic
in nature, as in the case of streaming, real time data ﬂows. Appropriate methods
and quantiﬁcation approaches are needed to capture uncertainty in data as well
as to ensure reproducibility and replicability of results. These issues become espe-
cially important when data are repurposed for a use diﬀerent than the one for
which they were was originally collected, and also when data are integrated from
multiple, heterogeneous sources of diﬀerent quality. Data collected by an original
researcher/community for a purpose may be well understood by that community
and properly characterized for the original purpose. However, as those data are
reused—as is often the case in big data applications—it is important to retain
provenance of the information, as well as develop metadata methods by which
data can be characterized for uses other than originally planned. Novel tech-
niques are needed for developing and maintain such provenance and metadata
information, especially since reuse and repurposing of data may not only be
useful and convenient but, in many cases, it may be the only option, since the
desired data may be diﬃcult, or too expensive, to collect.
Software systems. As mentioned earlier, relational databases emerged in the
1980s and have since become a dominant technology—underlying a vast number
of data-based applications. Relational databases incentivized thoughtful design
of enterprise-level data and databases and incorporated a number of key concepts
and useful features, including data normalization, ad hoc query processing, and
transaction support. In the process, they also developed sophisticated underlying
technologies for database query optimization and complex transaction process-
ing in complex, distributed environments. The systems were designed to scale
to terabyte and 100s of terabyte scale databases, at the high end, before the
cost became prohibitive. However, the Internet-scale, big data explosion of the
1990s and later required a similar set of features to relational database systems,
in some ways, but also quite diﬀerent in other ways. Big data needed to scale
to petabyte-size databases at reasonable cost; accommodate unstructured and
8 Google Tensorﬂow, https://www.tensorﬂow.org/.
9 Microsoft Cognitive Toolkit, https://www.microsoft.com/en-us/cognitive-toolkit/.
10 IBM Cognitive Computing, https://www.ibm.com/it-infrastructure/us-en/cogni
tive-computing/.
11 Administration Issues Strategic Plan for Big Data Research and Development,
https://obamawhitehouse.archives.gov/blog/2016/05/23/administration-issues-
strategic-plan-big-data-research-and-development.

Data in the 21st Century
11
semi-structured data, such as free text, dynamically formatted web log records,
images, and video; support basic ad hoc query processing capability, but not
necessarily the “full blown” SQL12 supported by relational databases systems.
Thus, query interfaces to these petabyte-scale databases could be much simpler;
and, since the system conﬁgurations were very large, i.e., 1000s of computing
and storage nodes in a typical compute cluster, the software had to function in
a distributed environment and operate in the presence of systems faults, with-
out using expensive hardware/software solution. Also, given the scale of the
datasets, and the fact that much of the computing involved statistical analysis
and machine learning, small variations in the data content from run to run were
permissible. In other words, the database was not required to follow strict con-
sistency rules as enforced by standard relational databases. Loose consistency
was acceptable [25].
Technologies that have been invented to address these big data issues include
noSQL databases13, Hadoop/MapReduce and all of its variants14, Spark15, and
systems like Storm for managing and processing data streams16.
While performance will always remain an important consideration, technolo-
gies, systems, and tools for data in the 21st century must also support “responsi-
ble” data management and analysis. This means building systems that intrinsi-
cally support notions of transparency, interpretability and reproducibility. Many
data analytics and predictive analytics algorithms and systems are not trans-
parent to end-users. How the underlying models work, and when and why such
models may fail, is not always clear. Dealing at multiple levels with bias in the
data—from algorithmic to decision support—is a key issue. Machine learning
systems may learn and reinforce pre-existing biases, which can be especially
problematic with human-related data, where such systems may lead to unfair
treatment of minority sections of a population. Enabling widespread adoption
of data science approaches requires assurances that not only will the system
operate securely and in a controlled fashion, but also that it will be “fair”, by
producing transparent, interpretable, dependable, reproducible—and, therefore,
trustworthy—solutions. Interdisciplinary, convergent research is needed in the
use of machine learning in data-driven decision-making and discovery systems
to examine how data can be used to best support and enhance human judgment.
From a computing systems point of view, this requires support for the full data
lifecycle: from data acquisition, to curation, analysis and use, decision support,
and preservation.
Intrinsic to the notion of trustworthiness is reproducibility—it should be
possible to reproduce previously produced results. While simple in concept, this
can be a complex issue and has received signiﬁcant attention. There have been a
number of discussions in the community at large, as well as in many individual
12 Structured Query Language, https://www.w3schools.com/sql/sql intro.asp.
13 noSQL Databases, http://nosql-database.org/.
14 Apache Hadoop, http://hadoop.apache.org/.
15 Apache Spark, https://spark.apache.org/.
16 Apache Storm, http://storm.apache.org/.

12
C. Baru
science domains/communities, about reproducibility [3,12]. While there is signif-
icant discussion about distinctions among terms like reproducibility, repeatabil-
ity, and replicability—the key notion here is that there should be some support
for the ability to reproduce results previously produced, speciﬁcally in the con-
text of computational and data-driven experiments, by the team that produced
the original result as well as by others. Even this seemingly simple notion can
be quite complex to deﬁne and implement—how should one deal with situations
where the results are statistical in nature, and there may not be a single deﬁn-
itive numerical answer?; what about big data, where the datasets are indeed
very large—does reproducibility require maintaining a copy of the entire origi-
nal dataset?; what about streaming data, where the data itself may be change
quickly over time?; how does one account for variations in hardware and software
when computations are run in one environment versus another?; and, how can
we determine standards for acceptability of results, accounting for run to run
variations?
In sum, with increasing use of data in every sphere of our personal lives and
in society at large, systems in the 21st century will be required to support the
broad notion of trustworthiness, which has many facets as introduced above.
Data as Infrastructure. To develop and test foundational approaches and sys-
tems for data science, there is a need for cyberinfrastructure. In addition to
hardware platforms and software tools and environments, infrastructure for data
science also includes the data itself, as well as the knowledge structures that may
be derived from that data, and enable use of the data. Science involves research
with data. Data science involves research on data. The data, and knowledge,
structures—and the algorithms and processes that act on them—are themselves
the target of study in data science. Arguably, data is to data science as com-
puters are to computer science. Thus, easy access to data is essential for data
science.
Interestingly, given all the interest and fascination in big data, easy access
to truly big data datasets is not available to most researchers and students
in academia. While data scientists in industry and government have access to
large amounts of data (though, even in those cases, oftentimes the access is
not easily facilitated. Organizations that provide easy access to such data have a
competitive advantage over those that don’t), academia does not have such ready
access to open, big data—other than whatever data that industry or government
might make available in the open. It was remarked recently that it is not possible
today in academia to do the “hello, world” problem for big data17. The “hello,
world” problem refers to a very simple, ﬁrst programming assignment usually
given to students in introductory programming courses, to ensure that they
know how to write, compile, and run a very simple program of one, or few, lines.
The equivalent for big data would be an assignment that might ask students to,
say, “Access a 1 petabyte (or, even, 100 TB) dataset; perform a sub-selection or
sampling operation to obtain 1 TB out of the 1 PB dataset; compute a simple
17 Personal communication with R.V. Guha, July 2016.

Data in the 21st Century
13
descriptive statistic on the 1 TB dataset; print the result. And, complete the
assignment by next class.” A typical student would not know where to go to
ﬁnd a 1PB or 100 TB (or, in many cases, a 10 TB) data sets, and where to get
resources to run even a simple sub-selection operation on a petabyte or 100 TB
of data. Thus, a key immediate challenge for the community is to make available
a larger number of large datasets for education and training.
Knowledge Networks/Knowledge Graphs. Another area where industry has been
making rapid progress and academia needs to catch up and keep us, is in creat-
ing large knowledge networks or knowledge graphs. Such graphs typically encode
information about real world entities and the relationship among these entities,
e.g., sports teams and the cities they are located in, their roster of players, and
the arenas they play in. Very large knowledge graphs of this type are being con-
structed by companies like Amazon, Google, Apple, Microsoft, etc., to support
“intelligent” applications like Alexa, Google Assistant, Siri, Cortana, respec-
tively. Extensive machine learning techniques are utilized to create such knowl-
edge graphs from unstructured and structured data on the Web and in corporate
databases, and they form the essential semantic information infrastructure for
next generation “smart” and “intelligent” applications. Yet, there is no corre-
sponding eﬀort in the open community that is open and accessible to academia.
Two meetings have been conducted so far to discuss the notion of an Open
Knowledge Network, organized by the federal Big Data Interagency Working
Group18, with a third workshop planned for October 4–5, 2017 at the National
Library of Medicine, Bethesda, MD [15]. Access to large-scale open datasets and
open knowledge networks, as infrastructure, will be essential for innovation in
data science, and for research in general.
4
Translational Data Science
The development of foundations of data science and trustworthy systems are, in
the end, in service of data-driven applications, across science and society. Trans-
lational data science is a new term that is being used for an emerging ﬁeld that
applies data science principles, techniques, and technologies to challenging scien-
tiﬁc and societal problems that hold the promise of having an important impact
on human or societal welfare. The term is also used when data science princi-
ples, techniques and technologies are applied to problems in diﬀerent domains in
general, including—but not restricted to—science and engineering research. A
workshop on Translational Data Science was held in June 2017 at the University
of Chicago19.
The term “translational research” emerged in a medical research context,
referring to the application of ﬁndings from basic science to enhance human
18 NITRD Big Data Interagency Working Group (BDIWG), https://www.nitrd.gov/
nitrdgroups/index.php?title=Big Data.
19 Translational Data Science workshop, https://cdis.uchicago.edu/tds-17/.

14
C. Baru
health and well-being20. The objective is to “translate” ﬁndings in fundamental
research into medical practice and meaningful health outcomes. As employed
in biomedical and medical research context, the term “translational” describes
“the process of turning observations in the laboratory, clinic and community into
interventions that improve the health of individuals and populations from diag-
nostics and therapeutics to medical procedures and behavioral interventions”21.
Correspondingly, “translational science” is “the ﬁeld of investigation focused on
understanding the scientiﬁc and operational principles underlying each step of
the translational process”. The notion of translating ﬁndings in a laboratory
setting into potential treatments for disease has received signiﬁcant attention
in biomedicine/medicine because (a) it was felt that research ﬁndings were not
making their way into clinical practice quickly enough, (b) there is always a
pressing need to apply the latest ﬁndings from fundamental research in basic
science towards enhancing human health and well-being, by producing promis-
ing new treatments that can be applied in practice, and (c) to ensure not only
that research knowledge reaches practice, but that it is implemented correctly
in practice.
In data science, the experience thus far has been that tools, techniques, and
technologies are being developed rapidly and deployed immediately into practice.
Thus, in a sense, research knowledge is already reaching practice! However, that
is primarily a technology-driven view of data science. Translational data science
requires a partnership between the technology side and the applications side,
so that methods are developed and implemented in a fashion that serves the
end users and the end applications. Formalizing and systematizing the data
science translation activity allows us to address a number of issues that lie at
the interface of theories and technologies, and applications. By identifying the
translation step as a distinct activity, one is able to focus and make that activity
itself the object of study and research. Data science methods and processes are
iterative in nature. They should also be closed-loop—lessons learned in practice
should inform the theory and modify techniques for future use. Important issues,
such as those related to ethics and policy, arise only at the translation phase.
Careful thought must be given to the structures and processes of translation and
the implications thereof. Thus, there is a need to study and better understand
these processes.
Translational medicine identiﬁes several distinct steps or phases or trans-
lation encompassing: basic research—research on fundamental aspects in sci-
ence; pre-clinical research—research to connect basic science results to human
medicine, for example, via use of model organisms and animal models; clini-
cal research—moving the research to human subjects, involving clinical trials,
for example; clinical implementation—implementation in a clinical setting; and,
public health—studying the impact of the new techniques in terms of popula-
tion health. The multiple steps indicate the complexity of the process; the need
20 Translational
Research,
wikipedia,
https://en.wikipedia.org/wiki/Translational
research.
21 National Center for Advancing Translational Science, https://ncats.nih.gov/.

Data in the 21st Century
15
for a careful, considered approach; and the need to consider the diversity of
response among the population to the treatment. It has also been recognized
that the move towards precision medicine and personalized medicine will make
this process even more complex due to the need for “clinical trial that focuses
on individual, not average responses to therapy” [19]. Data science has a similar
need for care in applying methods and processes, which can be a complex issue.
If data driven decisions are being made about every aspect of our lives, then it is
important to consider the impact of these on individuals. Furthermore, one of the
key promises of data science and big data is the ability to personalize responses.
That immediately raises a number of issues, related to privacy of information;
the micro-level impact of macro-level methods like machine learning on big data;
and, similar to the clinical trials issues, how personalization methods could be
tested prior to deployment—or, would the deployment itself be the test?
In sum, calling out translational data science as a distinct activity puts focus
on the collaborative nature of applying data science foundations and systems to
real applications, in practice. Many key issues in privacy and ethics, surface only
at the translational step. Translation requires “co-design”, where application
domain experts work in collaboration with data scientists to implement sys-
tems that meet the needs of the application. In the process, lesson learned in on
situation, or by one application domain, could be useful and helpful to another
application domain, thereby making the process more eﬃcient by reducing dupli-
cation and “not making the same mistakes”.
5
Data Science and Econometrics
Use of data science methods in any scientiﬁc application will require the abil-
ity to “explain” the results—black box approaches will typically not suﬃce—
though, they may suﬃce in some well-known or well-deﬁned parts of a larger data
analysis workﬂow. Workshops on causal inference have pointed out the impor-
tance of such inferencing methods [13,20]. At the same time, black box methods
could be useful, for example, machine learning methods are used extensively in,
say, astronomy for image processing, in order to identify candidate images that
require further analysis [4].
In his article on “New Tricks for Econometrics,” Hal Varian provides an
overview of a number of big data technologies which he deemed are useful for
the community [24]. To his point, some of the technologies mentioned in the
paper written in 2014 have already been overtaken by other newer technologies.
In the paper, he states “my standard advice to graduate students these days is
go to the computer science department and take a class in machine learning.”
That, indeed, is a reaction to the lack of a Translational Data Science area, and
corresponding curriculum. There is a diﬀerence between how a topic like machine
learning, or any other technical topic, would be taught to computer scientists
versus to, say, econometricians. While there may be a common core, there will
be signiﬁcant diﬀerences in the nature and style of coverage—including even the
examples selected—for computer science versus econometrics. Translational data

16
C. Baru
science can provide the home for such curriculum. While some econometricians
(or, researchers/scientists from any ﬁeld) may be interested in actually taking a
computer science course on the topic, for many, what is needed is the transla-
tional aspect of the subject matter. Or, even if a foundations course is taken in
computer science, the translational course will still be essential.
As described in Sect. 2, there are on-going collaborations among the com-
puter science, statistics, and mathematics community to deﬁne foundations of
data science. There is now an opportunity for econometrists to join in such col-
laborations to help develop foundations, but also to develop the translational
aspects of data science as it applies to economics and econometrics.
References
1. Abel, P.: Cobol Programming: A Structured Approach. Prentice Hall, Upper Sad-
dle River (1988)
2. Abiteboul, S., Miklau, G., Stoyanovich, J., Weikum, G.: Data, Responsibly. Semi-
nar 16291, Dagstuhl, 17–22 July 2016. http://www.dagstuhl.de/16291
3. ACM:
Artifact
Review
and
Badging,
June
2016.
https://www.acm.org/
publications/policies/artifact-review-badging
4. Ball, N.M., Brunner, R.J.: Data Mining and Machine Learning in Astronomy,
arxiv.org, August 2010. https://arxiv.org/abs/0906.2173
5. CCC
Blog,
Obama
Administration
Unveils
$200M
Big
Data
R&D
Initiative,
29
March
2012.
http://www.cccblog.org/2012/03/29/
obama-administration-unveils-200m-big-data-rd-initiative/
6. Codd, E.F.: The Relational Model for Database Management (Version 2 ed.). Addi-
son Wesley Publishing Company (1990). ISBN 0-201-14192-2
7. Economist: The Data Deluge, February 2010. http://www.economist.com/node/
15579717
8. Groves, R.: “Designed Data” and “Organic Data”, May 2011. https://www.census.
gov/newsroom/blogs/director/2011/05/designed-data-and-organic-data.html
9. Hey, T., Tansley, S., Tolle, K.: The Fourth Paradigm: Data-Intensive Scientiﬁc
Discovery. Microsoft Research (2009). ISBN 978-0-9825442-0-4
10. Kakade, S., Harchaoui, Z., Drusvyatskiy, D., Lee, Y.T., Fazel, M.: Algorithms
for data science: complexity, scalability, and robustness (2017). https://nsf.gov/
awardsearch/showAwardAWD ID=1740551&HistoricalAwards=false
11. Mahoney, M.W.: Lecture Notes on Randomized Linear Algebra, arXiv:1608.04481,
August 2016
12. National Academy of Sciences, Arthur M. Sackler Colloquia: Reproducibility
of research: issues and proposed remedies. http://www.nasonline.org/programs/
sackler-colloquia/completed colloquia/Reproducibility of Research.html
13. National Academy of Sciences: Reﬁning the Concept of Scientiﬁc Inference When
Working With Big Data: A Workshop, June 2016. http://sites.nationalacademies.
org/DEPS/BMSA/DEPS 171738
14. NITRD Big Data Interagency Working Group: The Federal Big Data R&D
Strategic Plan, May 2016. https://obamawhitehouse.archives.gov/sites/default/
ﬁles/microsites/ostp/NSTC/bigdatardstrategicplan-nitrd ﬁnal-051916.pdf
15. NITRD Big Data Interagency Working Group: 3rd Workshop on an Open Knowl-
edge Network (2017). https://www.nitrd.gov/nitrdgroups/index.php?title=Open
Knowledge Network

Data in the 21st Century
17
16. O’Neil, C.: Weapons of Math Destruction. Crown Publishing, New York (2016)
17. Papalexakis, E.E., Kang, U., Faloutsos, C., Sidiropoulos, N.D., Harpale, A.: Large
scale tensor decompositions: algorithmic developments and applications. IEEE
Data Eng. Bull. - Special Issue on Social Media 36, 59 (2013)
18. Sato, K., Young, C., Patterson, D.: An in-depth look at Google’s ﬁrst Tensor
Processing Unit (TPU), May 2017. https://cloud.google.com/blog/big-data/2017/
05/an-in-depth-look-at-googles-ﬁrst-tensor-processing-unit-tpu
19. Schork, N.: Personalized medicine: time for one-person trials. Nature 520(7549),
609–611
(2015).
https://doi.org/10.1038/520609a.
https://www.nature.com/
news/personalized-medicine-time-for-one-person-trials-1.17411
20. Shiﬀrin, R.M.: Drawing causal inference from Big Data, vol. 113, no. 27, pp. 7308–
7309 (2016). https://doi.org/10.1073/pnas.1608845113
21. Suciu, D., Balazinska, M., Howe, B.: A formal foundation for big data management.
https://nsf.gov/awardsearch/showAward?AWD ID=1247469&HistoricalAwards=
false
22. NSF: Core Techniques and Technologies for Advancing Big Data Science & Engi-
neering (BIGDATA) (2012). https://www.nsf.gov/pubs/2012/nsf12499/nsf12499.
htm
23. Upfal, E.: Analytical approaches to massive data computation with applications to
genomics (2012). https://nsf.gov/awardsearch/showAward?AWD ID=1247581&
HistoricalAwards=false
24. Varian, H.R.: Big data: new tricks for econometrics. J. Econ. Perspect. 28(2), 3–
28(2014).https://doi.org/10.1257/jep.28.2.3.http://www.aeaweb.org/articles?id=
10.1257/jep.28.2.3
25. Viotti, P., Vukolic, M.: Consistency in non-transactional distributed storage sys-
tems. ACM Comput. Surv. 49(1), 19:1–19:34 (2016). https://doi.org/10.1145/
2926965
26. Weinberger, K., Strogatz, S., Hooker, G., Kleinberg, J., Shmoys, D.: Data science
for improved decision-making: learning in the context of uncertainty, causality,
privacy, and network structures (2017). https://nsf.gov/awardsearch/showAward?
AWD ID=1740822&HistoricalAwards=false
27. Xing, E.P., Ho, Q., Dai, W., Kim, J.K., Wei, J., Lee, S., Zheng, X., Xie, P.,
Kumar, A., Yu, Y.: Petuum: a new platform for distributed machine learning on
big data. IEEE Trans. Big Data 1, 49 (2015). https://doi.org/10.1109/TBDATA.
2015.2472014

Fundamental Theory

Model-Assisted Survey Estimation
with Imperfectly Matched Auxiliary Data
F. Jay Breidt(B), Jean D. Opsomer, and Chien-Min Huang
Colorado State University, Fort Collins, USA
{FJay.Breidt,Jean.Opsomer,Chien-Min.Huang}@colostate.edu
Abstract. Model-assisted survey regression estimators combine aux-
iliary information available at a population level with complex sur-
vey data to estimate ﬁnite population parameters. Many prediction
methods, including linear and mixed models, nonparametric regression,
and machine learning techniques, can be incorporated into such model-
assisted estimators. These methods assume that observations obtained
for the sample can be matched without error to the auxiliary data. We
investigate properties of estimators that rely on matching algorithms that
do not in general yield perfect matches. We focus on diﬀerence estima-
tors, which are exactly unbiased under perfect matching but not under
imperfect matching. The methods are investigated analytically and via
simulation, using a study of recreational angling in South Carolina to
build a simulation population. In this study, the survey data come from
a stratiﬁed, two-stage sample and the auxiliary data from logbooks ﬁled
by boat captains. Extensions to regression estimators under imperfect
matching are discussed.
Keywords: Complex survey · Diﬀerence estimator
Probability sampling · Survey regression estimation
1
Introduction
1.1
Probability Sampling and Weighted Estimation
Let U = {1, 2, . . . , N} denote a ﬁnite population and let yk denote the (non-
random) value of some variable of interest for element k ∈U. We are interested
in the ﬁnite population total Ty = 
k∈U yk. As a motivating example, which
we return to in Sect. 4, suppose U is the set of all recreational angling boat
trips on the coast of the state of South Carolina in 2016. Further, suppose yk is
the number of anglers on the kth boat trip, so that Ty is the total number of
recreational angler trips on boats in South Carolina waters in 2016; or suppose
that yk is the number of black sea bass caught on the kth boat trip, so that Ty
is the total number of black sea bass caught in 2016.
Because it is often impractical to measure yk for all k ∈U, we instead
estimate Ty based on information obtained for a sample s ⊂U, which is selected
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_2

22
F. J. Breidt et al.
via a random mechanism. Following [8], let s denote one of the 2N possible
subsets of U, selected with probability given by p(s), the sampling design. The
sample membership indicators are random variables deﬁned by Ik = 1 if k ∈s,
Ik = 0 otherwise, so that s = {k ∈U : Ik = 1}. Let πk = E [Ik] = P [Ik = 1] and
suppose that these ﬁrst-order inclusion probabilities satisfy πk > 0 for all k ∈U.
The sampling design is then said to be a probability sampling design, for which
it is well known that the Horvitz-Thompson estimator [4]
Ty =

k∈U
yk
Ik
πk
=

k∈s
yk
πk
(1)
is unbiased for Ty, where the expectation is with respect to the probability
distribution p(·). Its variance is given by
Var

Ty

=

j,k∈U
Cov (Ij, Ik) yj
πj
yk
πk
=

j,k∈U
Δjk
yj
πj
yk
πk
,
(2)
where Δjk = πjk −πjπk and πjk = E [IjIk] = P [Ij = 1, Ik = 1] denotes a
second-order inclusion probability.
If the second-order inclusion probabilities satisfy πjk > 0 for all j, k ∈U, then
the design is a measurable sampling design and an unbiased variance estimator
is given by
V ( Ty) =

j,k∈U
Δjk
yj
πj
yk
πk
IjIk
πjk
;
(3)
this estimator or closely-related approximations are computed in standard survey
software such as proc surveymeans in SAS [9] or the survey package of R [6,7].
1.2
Auxiliary Information
In addition to observations obtained on the sample, auxiliary information may
be available from external records. Let A = {1, 2, . . . , A} denote the indices
for this external database and let aℓdenote the vector of auxiliary information
available for record ℓ∈A . We write Ax = 
ℓ∈A xℓfor sums over the database,
in particular noting that the size of the database is A1 = 
ℓ∈A 1.
The auxiliary vector aℓcould be used to construct a predictor, μ(aℓ) of yk
provided record ℓ∈A in the database matches element k ∈U in the population.
We assume for the present that the construction of the prediction method μ(·)
does not involve the sample, s. We write Aμ = 
ℓ∈A μ(aℓ).
2
Estimation Under Perfect Matching
2.1
Notation for Perfect Matching
We ﬁrst consider the case of perfect matching: suppose that every record in the
database can be matched to one and only one element in the population, and

Survey Estimation with Imperfect Matching
23
vice versa. We write
Mkℓ=

1,
if database record ℓ∈A matches element k ∈U,
0,
otherwise.
(4)
The appropriate predictor of yk would then be denoted 
ℓ∈A Mkℓμ(aℓ), to
reﬂect the matching step.
2.2
Diﬀerence Estimator Under Perfect Matching
Under this perfect matching scenario, 
k∈U Mkℓ= 1. It follows that an unbiased
estimator of Ty is given by the diﬀerence estimator,
Ty,diﬀ=

k∈U

ℓ∈A
Mkℓμ(aℓ) +

k∈s
yk −
ℓ∈A Mkℓμ(aℓ)
πk
=

ℓ∈A
μ(aℓ) +

k∈s
yk −
ℓ∈A Mkℓμ(aℓ)
πk
;
(5)
this is simply a more elaborate notation for a standard estimator (e.g., Eq. (4)
of [2]), to account for the matching step. The variance of the perfect-matching
diﬀerence estimator is
Var

Ty,diﬀ

=

j,k∈U
Δjk
yj −
ℓ∈A Mjℓμ(aℓ)
πj
yk −
ℓ∈A Mkℓμ(aℓ)
πk
.
(6)
The unbiased diﬀerence estimator will have smaller variance and mean square
error than the unbiased Horvitz-Thompson estimator (1) provided the residuals
{yk−
ℓ∈A Mkℓμ(aℓ)}k∈U in (6) have less variation than the raw values {yk}k∈U
in (2).
3
Estimation Under Imperfect Matching
3.1
Notation for Imperfect Matching
In practice, perfect matching may not be possible. The sampled element k might
have no corresponding record in the database. It might have a corresponding
record ℓ, but fail to match it perfectly due to missing values or inaccuracies
in the survey observation, the database record, or both. Similarly, the sampled
element might appear to match multiple database records due to agreement on
a number of data values.
Hence, we replace the Mkℓ= 0 or 1 by a possibly-fractional value mkℓ∈[0, 1],
computed via a deterministic algorithm that does not depend on the sample.
We refer to these values as match metrics. Assume that for any sampled element

24
F. J. Breidt et al.
k ∈U, the match metrics {mkℓ}ℓ∈A for every database record can be computed.
For example, sampled element k might match record ℓ1 perfectly, in which case
mkℓ=

1,
if ℓ= ℓ1,
0,
otherwise.
It might not match any records, in which case
mkℓ= 0,
for all ℓ∈A ;
or it might match three records ℓ1, ℓ2, ℓ3 equally well, in which case
mkℓ=

1/3,
if ℓ= ℓ1, ℓ= ℓ2 or ℓ= ℓ3,
0,
otherwise.
If 
ℓ∈A mkℓ< 1, then the matching algorithm has determined that there is
a non-trivial possibility that the sampled element does not match any database
record. This can occur when there is potential non-overlap between the target
population U and the database A . This is of interest in situations such as the
application we will describe in Sect. 4, where A is a possibly-incomplete set of
recreational angling trips self-reported by boat captains, while U is the actual
population of trips.
3.2
Diﬀerence Estimation Under Imperfect Matching
3.2.1
First Diﬀerence Estimator
Under imperfect matching, an estimator analogous to (5) is
Ty,diﬀ1 =

ℓ∈A
μ(aℓ) +

k∈s
yk −
ℓ∈A mkℓμ(aℓ)
πk
.
(7)
This estimator is no longer unbiased. Instead, its expectation is
E
	
Ty,diﬀ1

=

ℓ∈A
μ(aℓ) +

k∈U

yk −

ℓ∈A
mkℓμ(aℓ)

= Ty +

ℓ∈A

1 −

k∈U
mkℓ

μ(aℓ).
(8)
Its variance is
Var

Ty,diﬀ1

=

j,k∈U
Δjk
yj −
ℓ∈A mjℓμ(aℓ)
πj
yk −
ℓ∈A mkℓμ(aℓ)
πk
.
(9)
The variance is small if the match-weighted quantities {
ℓ∈A mkℓμ(aℓ)}k∈U are
good predictors of the response values {yk}k∈U. Under a measurable sampling
design, an unbiased variance estimator is given by
V ( Ty,diﬀ1) =

j,k∈U
Δjk
yj −
ℓ∈A mjℓμ(aℓ)
πj
yk −
ℓ∈A mkℓμ(aℓ)
πk
IjIk
πjk
(10)

Survey Estimation with Imperfect Matching
25
which, like (3), can be computed or closely approximated using standard survey
software.
The behavior of the estimator under three extreme cases is of interest. First,
if there is no matching at all, so that mkℓ≡0 for all k ∈U, ℓ∈A , then Ty,diﬀ1
becomes

ℓ∈A
μ(aℓ) +

k∈s
yk
πk
= Aμ + Ty,
(11)
with expectation Aμ + Ty and variance equal to that of the Horvitz-Thompson
estimator, (2). Eﬀectively, the estimator regards the sampling design as having
failed to cover the complete population, which is actually the disjoint union
A ∪U and not U. It thus separately estimates the totals for the database and
the universe and adds them together.
The second extreme case is that of full matching in the sense that

k∈U mkℓ= 1 for all ℓ∈A (this is not the same as perfect matching). In
this case, Ty,diﬀ1 is exactly unbiased for Ty by (8).
The third and ﬁnal extreme case can occur if a rare characteristic appears
in the population but is never encountered in the sample, so that yk ≡0 for all
k ∈s. In this case, the estimator (7) becomes

ℓ∈A
μ(aℓ) −

k∈s

ℓ∈A
mkℓμ(aℓ)
πk
.
(12)
This behavior may be undesirable, as for a non-negative characteristic with non-
negative predictions, the estimator predicts less than what is known to be present
in the database. This behavior is better than that of the Horvitz-Thompson
estimator, however, which would estimate zero for the population with such
a degenerate sample. Nonetheless, other diﬀerence-type estimators are worth
considering, including the one proposed below.
3.2.2
Second Diﬀerence Estimator
An alternative to Ty,diﬀ1 in (7) is obtained by an additional diﬀerencing adjust-
ment,
Ty,diﬀ2 = Ty,diﬀ1 +

k∈s

ℓ∈A
mkℓ{μ(aℓ) −yk}
πk
=

ℓ∈A
μ(aℓ) +

k∈s
yk(1 −
ℓ∈A mkℓ)
πk
.
(13)
The expectation of the estimator is
E

Ty,diﬀ2

= E

Ty,diﬀ1

+

k∈U

ℓ∈A
mkℓ{μ(aℓ) −yk}
= Ty +

ℓ∈A

1 −

k∈U
mkℓ

μ(aℓ) +

k∈U

ℓ∈A
mkℓ{μ(aℓ) −yk} . (14)

26
F. J. Breidt et al.
Its variance is
Var

Ty,diﬀ2

=

j,k∈U
Δjk
yj(1 −
ℓ∈A mjℓ)
πj
yk(1 −
ℓ∈A mkℓ)
πk
.
(15)
The variance is small if the matching is good in the sense that 
ℓ∈A mkℓ≃1 for
all k ∈U. Under a measurable sampling design, an unbiased variance estimator
is given by
V ( Ty,diﬀ2) =

j,k∈U
Δjk
yj(1 −
ℓ∈A mjℓ)
πj
yk(1 −
ℓ∈A mkℓ)
πk
IjIk
πjk
.
(16)
Again, like (3) and (10), this estimator can be computed or closely approximated
using standard survey software.
We next consider the behavior of Ty,diﬀ2 under the three extreme scenarios
described above. First, if there is no matching at all, so that mkℓ≡0 for all
k ∈U, ℓ∈A , then Ty,diﬀ2 reduces to Ty,diﬀ1 by (13) and has exactly the same
behavior.
Second, under full database matching in the sense that 
k∈U mkℓ= 1 for
all ℓ∈A , the expectation of Ty,diﬀ2 in (14) becomes
Ty +

ℓ∈A
μ(aℓ) −

k∈U

ℓ∈A
mkℓyk
(17)
so that, unlike Ty,diﬀ1 under this scenario, Ty,diﬀ2 is biased. The bias is small if

k∈U mkℓyk is close to μ(aℓ) for all ℓ.
Third, with yk ≡0 for all k ∈s, the estimate computed from (13) becomes
the full database total

ℓ∈A
μ(aℓ),
(18)
which may be preferable to either the zero estimate from Horvitz-Thompson or
the reduced database total of Ty,diﬀ1 from (12).
4
Simulation Experiments
4.1
Constructing the Population and Database
In the US state of South Carolina, there are about 500 operators of charter
boats who take recreational angling trips with paying customers. Each boat can
take multiple anglers, and over the course of 2016 there were about 50,000 angler
trips on approximately 15,000 boat trips. These boat trips, along with the boat’s
logbook data on number of anglers and number of ﬁsh of each species caught
by those anglers, are required to be reported to the South Carolina Department
of Natural Resources, though reporting is incomplete. After removing logbook
reports with missing values, we took the remaining N = 10, 647 as the universe

Survey Estimation with Imperfect Matching
27
U of actual boat trips to be studied. We then used a stochastic algorithm to
simulate a corresponding database A of logbook records and a set of match
metrics, [mkℓ]k∈U,ℓ∈A . In keeping with the real match metrics used in South
Carolina, at most ﬁve of the {mkℓ}ℓ∈A are non-zero for a given population
element k.
We simulated the database by ﬁrst sorting the universe in space and time, so
that nearby elements in the population tend to be from the same coastal loca-
tion and from nearby dates. We then used a Markov chain to determine the true
(but unobservable) matching state of the population elements: no match, per-
fect match, high-quality match, or low-quality match. The transition probability
matrix of the chain is as follows:
State
0
1
2
3 4 5 6
7
8 9 10 11 12 13 14 15 16
0
ρ0 ρ1 ρ2 0 0 0 0 ρ3 0 0
0
0
0
0
0
0
0
1
ρ0 ρ1 ρ2 0 0 0 0 ρ3 0 0
0
0
0
0
0
0
0
2
0
0
0
1 0 0 0
0
0 0
0
0
0
0
0
0
0
3
0
0
0
0 1 0 0
0
0 0
0
0
0
0
0
0
0
4
0
0
0
0 0 1 0
0
0 0
0
0
0
0
0
0
0
5
0
0
0
0 0 0 1
0
0 0
0
0
0
0
0
0
0
6
ρ0 ρ1 ρ2 0 0 0 0 ρ3 0 0
0
0
0
0
0
0
0
7
0
0
0
0 0 0 0
0
1 0
0
0
0
0
0
0
0
8
0
0
0
0 0 0 0
0
0 1
0
0
0
0
0
0
0
9
0
0
0
0 0 0 0
0
0 0
1
0
0
0
0
0
0
10
0
0
0
0 0 0 0
0
0 0
0
1
0
0
0
0
0
11
0
0
0
0 0 0 0
0
0 0
0
0
1
0
0
0
0
12
0
0
0
0 0 0 0
0
0 0
0
0
0
1
0
0
0
13
0
0
0
0 0 0 0
0
0 0
0
0
0
0
1
0
0
14
0
0
0
0 0 0 0
0
0 0
0
0
0
0
0
1
0
15
0
0
0
0 0 0 0
0
0 0
0
0
0
0
0
0
1
16
ρ0 ρ1 ρ2 0 0 0 0 ρ3 0 0
0
0
0
0
0
0
0
where 3
i=0 ρi = 1. This chain determines that an element k has no match (state
0); or determines that element k has a perfect match (state 1); or determines
that ﬁve successive elements k, k + 1, . . . , k + 4 are high-quality (HQ) matches
(states 2–6); or determines that ten successive elements k, k + 1, . . . , k + 9 are
low-quality (LQ) matches (states 7–16).
In the event of no match, no database record is created, and mkℓ= 0 for all
ℓ∈A .
In the event of a perfect match, a database record that matches element k is
created, and mkℓ= 1 for k = ℓand zero otherwise.
In the event of ﬁve HQ matches, ﬁve database records are created: the ﬁrst
record matches element k, the next record matches element k + 1, and so on
until the ﬁfth record matches element k + 4. Further, we generate ﬁve match
metric values that sum to one by independently generating Uk, Uk+1, . . . , Uk+4

28
F. J. Breidt et al.
as Uniform (0,1) and setting
(mk+i,k, mk+i,k+1, . . . , mk+i,k+4) =
1
4
i=0 Uk+i
(Uk, Uk+1, . . . , Uk+4)
and mk+i,ℓ= 0 otherwise for all ﬁve elements i = 0, 1, . . . , 4. That is, all ﬁve
elements have the same match metric values with the same ﬁve database records.
If we sample one of these ﬁve elements, we know that it (in truth) matches one
of the ﬁve database records with non-zero match metric values, but we do not
know which one.
In the event of ten LQ matches, ﬁve database records are created: the ﬁrst
record matches element k, the next record matches element k + 1, and so on
until the ﬁfth record matches element k + 4. The remaining ﬁve elements have
no matching database records. All ten population elements share the same
match metric values, constructed similarly to those for the HQ matches, but
with match metric values summing to 1/2 instead of 1: independently generate
Uk, Uk+1, . . . , Uk+4 as Uniform (0,1) and set
(mk+i,k, mk+i,k+1, . . . , mk+i,k+4) =
1
2 4
i=0 Uk+i
(Uk, Uk+1, . . . , Uk+4)
and mk+i,ℓ= 0 otherwise for all ten elements i = 0, 1, . . . , 9. Thus, if we sample
one of these ten elements, we think there might be no match at all (true for half
of the ten elements) or there might be a match among the ﬁve database records
(true for half of the elements), but we do not know which one.
We consider two population/database combinations, determined by the choice
of ρ0, ρ1, ρ2, ρ3. The “Poor Match” combination results in simulated proportions
of match metric values that closely mirror those in the actual South Carolina data,
while the “Better Match” combination has greatly improved matching:
Records ρ0
ρ1
ρ2
ρ3
No match LQ
HQ
Perfect
South Carolina
11.0%
52.5% 36.5% 0.0%
Poor Match
6836
0.35 0.20 0.25 0.20
8.6%
54.4% 31.7% 5.3%
Better Match
9031
0.10 0.20 0.60 0.10
2.3%
23.3% 69.8% 4.7%
Under the Poor Match combination, there are 6, 836 logbook records, so that
many of the N = 10, 647 population elements have no matching logbook records.
Under the Better Match combination, there are 9, 031 logbook records. For each
combination, we simulated the database once, and each population/database
combination was then ﬁxed for the remainder of the sampling experiment.
4.2
Estimation Properties Under Repeated Sampling
The sampling design used in our simulation study follows closely the design
actually used by the Marine Recreational Information Program (MRIP) in

Survey Estimation with Imperfect Matching
29
South Carolina. We stratiﬁed the population into ﬁfteen strata by crossing
three regions (each consisting of contiguous South Carolina counties) and ﬁve
waves (March–April, May–June, July–August, September–October, November–
December). Similar to MRIP, our sampling design selects particular sites on
particular days (“site-days”) and intercepts all boat trips on those selected site-
days. In MRIP, the site-days are selected with probability proportional to a
measure of size that is an estimate of ﬁshing activity (“pressure”) for the site-
day. In our design, we approximate this unequal probability design by allocating
an overall sample size of n = 500 site-days to the 15 strata using a database
Fig. 1. Boxplots for estimated total angler trips, based on 1000 simulated stratiﬁed
simple random samples for each population/database combination. Horizontal reference
line is at the true value. From left to right: Horvitz-Thompson estimator Ty (white
boxplot) under either combination; Ty,diﬀ1 and Ty,diﬀ2 (light gray boxplots) under the
Poor Match combination; Ty,diﬀ1 and Ty,diﬀ2 (dark gray boxplots) under the Better
Match combination.

30
F. J. Breidt et al.
Fig. 2. Boxplots for estimated total catch of red drum, based on 1000 simulated strat-
iﬁed simple random samples for each population/database combination. Horizontal
reference line is at the true value. From left to right: Horvitz-Thompson estimator
Ty (white boxplot) under either combination; Ty,diﬀ1 and Ty,diﬀ2 (light gray boxplots)
under the Poor Match combination; Ty,diﬀ1 and Ty,diﬀ2 (dark gray boxplots) under the
Better Match combination.
estimate of ﬁshing pressure for the stratum. We then selected site-days via sim-
ple random sampling without replacement within strata, and observed all boat-
trips on selected site-days (there may, in fact, be no trips for a selected site-day).
We chose n = 500 so that the number of selected site-days with non-zero ﬁsh-
ing activity closely matches the 109 non-zero site-days for South Carolina in
2016. Site-days are thus the primary sampling units (PSUs), selected via strati-
ﬁed simple random sampling, and boat-trips are the secondary sampling units,
selected with certainty within PSUs. Variance estimation needs to account for
this stratiﬁed two-stage structure.

Survey Estimation with Imperfect Matching
31
Fig. 3. Boxplots for estimated total catch of black sea bass, based on 1000 simulated
stratiﬁed simple random samples for each population/database combination. Horizon-
tal reference line is at the true value. From left to right: Horvitz-Thompson estimator
Ty (white boxplot) under either combination; Ty,diﬀ1 and Ty,diﬀ2 (light gray boxplots)
under the Poor Match combination; Ty,diﬀ1 and Ty,diﬀ2 (dark gray boxplots) under the
Better Match combination.
For each sampled boat-trip k in stratum h, the inclusion probability is πk =
nh/Nh where nh is the number of site-days allocated to stratum h and Nh is the
total number of site-days in stratum h, for h = 1, 2, . . . , 15.
For this setting, our vector aℓof auxiliary information available for each
element in the database includes time, location, number of anglers, and catch by
species for multiple species of ﬁsh. Number of anglers and catch by species are of
particular interest for estimation, and are observed for the sample of intercepted
trips. The predictor μ(aℓ) for a characteristic of interest then simply returns the
logbook value of the survey response: μ(aℓ) = logbook number of anglers when

32
F. J. Breidt et al.
Fig. 4. Boxplots for estimated total catch of gag grouper, based on 1000 simulated
stratiﬁed simple random samples for each population/database combination. Horizon-
tal reference line is at the true value. From left to right: Horvitz-Thompson estimator
Ty (white boxplot) under either combination; Ty,diﬀ1 and Ty,diﬀ2 (light gray boxplots)
under the Poor Match combination; Ty,diﬀ1 and Ty,diﬀ2 (dark gray boxplots) under the
Better Match combination.
yk = intercepted number of anglers, μ(aℓ) = logbook number of black sea bass
when yk = intercepted number of black sea bass, etc.
For each population/database combination, we drew 1000 independent strat-
iﬁed simple random samples from the ﬁxed population and constructed the esti-
mators Ty, Ty,diﬀ1, and Ty,diﬀ2 for several characteristics, including number of
angler trips and total catch for red drum, black sea bass, gag grouper, Atlantic
croaker, toadﬁsh, and wahoo. These species were chosen to reﬂect a variety of
reporting behaviors: in particular, they include species that are reported fre-
quently in the database and are common enough to appear frequently in the
on-site interviews, and species that are reported regularly but are rare enough
to appear infrequently in the interviews. We also computed variance estimates

Survey Estimation with Imperfect Matching
33
as in (3), (10), and (16), but using the standard approximation of ignoring ﬁnite
population corrections within strata. We present selected results here, noting
that the Horvitz-Thompson estimator Ty does not use the auxiliary information
and has the same behavior under either combination.
Side-by-side boxplots for estimated total angler trips are shown in Fig. 1, for
estimated red drum catch in Fig. 2, for estimated black sea bass catch in Fig. 3,
and for estimated gag grouper catch in Fig. 4. We further summarized the results
of the 1000 simulated samples for each estimator with the percent relative bias,
root mean square error (RMSE), RMSE ratio (with Horvitz-Thompson estima-
tor in the numerator), average estimated standard error (SE), and coverage of
nominal 95% conﬁdence intervals computed assuming approximate normality.
Results are presented in Table 1.
Table 1. Summary results for estimated angler trips, red drum catch, black sea bass
catch, and gag grouper catch, based on 1000 simulated stratiﬁed simple random sam-
ples for each population/database combination. Relative RMSE (Root Mean Square
Error) is RMSE of the estimator in the denominator and RMSE of Ty in the numerator.
Estimated SE (standard error) is for stratiﬁed simple random sampling, but ignoring
within-stratum ﬁnite population corrections. Conﬁdence interval coverage is for nomi-
nal 95% coverage under normality, using (estimator) ± 1.96 × (estimated SE).

Ty Poor Match
Better Match

Ty,diff1

Ty,diff2

Ty,diff1

Ty,diff2
Angler trips
Mean
37567.7 37600.0 37424.6 37534.3 37471.0
Percent Relative Bias
0.3
0.4
−0.1
0.2
0.0
Relative RMSE
1.0
2.1
2.4
3.1
3.9
RMSE
4427.2
2071.0
1828.9
1443.3
1138.3
Average Estimated SE
4555.0
2155.3
1915.4
1476.5
1138.3
Conﬁdence Interval Coverage
95.2
94.9
94.3
94.9
92.8
Red drum catch
Mean
37508.6 37266.3 37197.5 36887.2 37236.7
Percent Relative Bias
0.6
0.0
−0.2
−1.1
−0.1
Relative RMSE
1.0
1.5
2.3
1.7
3.5
RMSE
7417.2
4857.0
3164.3
4301.2
2086.9
Average Estimated SE
7270.7
4693.0
3042.5
4235.8
1960.8
Conﬁdence Interval Coverage
93.1
92.6
90.5
92.8
87.6
Black sea bass catch Mean
63094.5 63915.1 62853.5 63509.8 62806.4
Percent Relative Bias
−0.5
0.8
−0.9
0.2
−0.9
Relative RMSE
1.0
1.3
2.2
1.4
3.0
RMSE
23526.6 18008.0 10533.4 16287.8
7793.4
Average Estimated SE
22725.9 17063.2
9653.7 15397.5
6473.7
Conﬁdence Interval Coverage
87.5
91.2
83.1
92.4
76.9
Gag grouper catch
Mean
256.0
260.9
272.1
263.7
258.5
Percent Relative Bias
−3.4
−1.5
2.7
−0.5
−2.5
Relative RMSE
1.0
1.1
2.0
1.1
3.8
RMSE
209.2
196.2
105.0
188.8
54.4
Average Estimated SE
170.7
170.5
75.5
163.3
29.2
Conﬁdence Interval Coverage
72.4
79.9
62.2
86.0
45.5

34
F. J. Breidt et al.
The Horvitz-Thompson estimator is theoretically unbiased, and both diﬀer-
ence estimators are also nearly unbiased under each population/database com-
bination and for each quantity of interest. Due to the low bias in all cases,
the average estimated standard errors tend to be close to the RMSE’s over the
1000 simulations, with the exceptions occurring for rarely-caught species like
gag grouper. The sampling distributions of Ty and Ty,diﬀ2, which are nonneg-
ative by construction for nonnegative responses, are then highly skewed, with
corresponding poor conﬁdence interval coverage. The sampling distribution of
Ty,diﬀ1, which is not constrained to be nonnegative, tends to be more symmet-
ric and hence have better coverage with skewed distributions. This improved
coverage comes at the expense of worse RMSE.
Under each population/database combination and for each quantity of inter-
est, both diﬀerence estimators are better than the Horvitz-Thompson estimator,
in terms of lower RMSE. The ﬁrst diﬀerence estimator Ty,diﬀ1 is sometimes not
much better than Ty, but the second diﬀerence estimator Ty,diﬀ2 is often much
better than Ty, and is always better than Ty,diﬀ1.
5
Discussion
The diﬀerence estimators described here are feasible in practice, given an auxil-
iary database and a suitable matching algorithm. The methodology oﬀers sub-
stantial eﬃciency gains in a simulation study motivated by a real application
in ﬁsheries management. The simulation described here does not reﬂect any dif-
ferential reporting, allowing probabilities of the match states to depend on the
population characteristics. For example, boat captains catching only Atlantic
croaker might be less likely to ﬁle a report than captains catching other species.
The simulation also does not reﬂect diﬀerential measurement errors between the
survey interviews and the logbook reports. In current practice, the boat captain
is not required to ﬁle a logbook report immediately, and the catch recalled by
the captain at the time of reporting may diﬀer from the catch observed by an
interviewer at a dockside intercept. These are directions for further study, both
analytically and via simulation.
In results not reported here, we have also considered multiplicative adjust-
ments of the Horvitz-Thompson estimator, as opposed to the additive adjust-
ments of the diﬀerence-type estimators. These multiplicative adjustments lead to
ratio-type estimators that can be considered generalizations of capture-recapture
sampling, extending the work of [5]. These multiplicative adjustments, however,
seem particularly sensitive to poor matching and can have large biases and vari-
ances. Further study on such estimators is necessary.
Two other directions for generalization of the results reported here are (1)
allowing the predictor μ(aℓ) to be estimated from the sample and (2) allowing
the match metric values mkℓto depend on the sample. The ﬁrst of these gener-
alizations is standard in the survey literature (see [2] for an extensive review),
but will be novel in this context due to the uncertain matching. The second

Survey Estimation with Imperfect Matching
35
generalization is also novel; some of the techniques of [1,3] may be relevant in
determining suitable variance estimation strategies.
Acknowledgements. We thank Eric Hiltz of the South Carolina Department of Nat-
ural Resources (SC DNR) for development of the matching algorithm that motivated
this work, and for assistance with the South Carolina logbook records and other data
sources; John Foster of the National Oceanic and Atmospheric Administration Fish-
eries Service for assistance with the MRIP data for South Carolina; and Amy Dukes
(SC DNR), Brad Floyd (SC DNR) and Geoﬀrey White (Atlantic Coastal Cooperative
Statistics Program) for useful discussions.
References
1. Breidt, F.J., Opsomer, J.D.: Endogenous post-stratiﬁcation in surveys: classifying
with a sample-ﬁtted model. Ann. Stat. 36, 403–427 (2008)
2. Breidt, F.J., Opsomer, J.D.: Model-assisted survey estimation with modern predic-
tion techniques. Stat. Sci. 32(2), 190–205 (2017)
3. Dahlke, M., Breidt, F.J., Opsomer, J.D., Van Keilegom, I.: Nonparametric endoge-
nous post-stratiﬁcation estimation. Stat. Sin. 23, 189–211 (2013)
4. Horvitz, D.G., Thompson, D.J.: A generalization of sampling without replacement
from a ﬁnite universe. J. Am. Stat. Assoc. 47, 663–685 (1952)
5. Liu, B., Stokes, L., Topping, T., Stunz, G.: Estimation of a total from a population
of unknown size and application to estimating recreational red snapper catch in
Texas. J. Surv. Stat. Methodol. 5, 350–371 (2017)
6. Lumley, T.: Analysis of complex survey samples. J. Stat. Softw. 9(1), 1–19 (2004)
7. Lumley, T.: R package: survey: analysis of complex survey samples, version 3.30
(2014). https://www.r-project.org/(05.07.16)
8. S¨arndal, C.E., Swensson, B., Wretman, J.: Model Assisted Survey Sampling.
Springer, New York (1992)
9. SAS Institute: SAS/STAT 14.1 User’s Guide. Cary, NC (2015)

COBra: Copula-Based Portfolio Optimization
Marc S. Paolella1,2(B) and Pawel Polak3
1 Department of Banking and Finance, University of Zurich, Zurich, Switzerland
2 Swiss Finance Institute, Geneva, Lausanne, Lugano, Zurich, Switzerland
marc.paolella@bf.uzh.ch
3 Department of Statistics, Columbia University, New York, USA
Abstract. The meta-elliptical t copula with noncentral t GARCH uni-
variate margins is studied as a model for asset allocation. A method
of parameter estimation is deployed that is nearly instantaneous for
large dimensions. The expected shortfall of the portfolio distribution
is obtained by combining simulation with a parametric approximation
for speed enhancement. A simulation-based method for mean-expected
shortfall portfolio optimization is developed. An extensive out-of-sample
backtest exercise is conducted and comparisons made with common asset
allocation techniques.
Keywords: CCC · Expected shortfall · GARCH · Non-ellipticity
Student’s t-copula
JEL classiﬁcation: C13 · C32 · G11
1
Introduction
The use of copulae for modeling the joint distribution of ﬁnancial asset returns
is now a well-established ﬁeld, with the seminal and still highly relevant article
being Embrechts et al. (2002); see also Jondeau and Rockinger (2006). However,
it appears that the literature is moving towards more sophisticated and complex
copula constructions that are more able to adequately capture the co-movements
and dependency structures in asset returns, particularly for large dimensions; see,
e.g., Aas et al. (2009), Ausin and Lopes (2010), Scherer (2011), Christoﬀersen et al.
(2012), Patton (2012), Aas (2016), Fink et al. (2017), and the references therein.
As emphasized in McNeil et al. (2015), in the context of modeling asset returns
within a copula framework, the choice of the copula is critical for determining the
characteristics of the multivariate distribution. On the other hand, as pointed out
in Embrechts (2009), there is usually no clear guidelines regarding which copula
to use.
Before proceeding, we provide a list of the numerous abbreviations used
throughout the paper (and thank an anonymous referee for this useful
suggestion).
M.S. Paolella—Financial support by the Swiss National Science Foundation (SNSF)
through project #150277 is gratefully acknowledged.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_3

COBra: Copula-Based Portfolio Optimization
37
• APARCH: Refers to an Asymmetric Power ARCH process.
• ALRIGHT: The t-copula model proposed in Paolella and Polak (2015a), with
the acronym coming from the title, Asymmetric LaRge-scale (I)GARCH with
Hetero-Tails.
• COBra: The acronym for the title of this paper, Copula-Based Portfolio Opti-
mization.
• DCC: Dynamic Conditional Correlation.
• ES: Expected Shortfall.
• GARCH: Generalized Autoregressive Conditional Heteroskedasticity.
• KP: Refers to the method of estimation proposed in the paper of Krause and
Paolella (2014).
• MLE: Maximum Likelihood Estimator.
• MSE: Mean Squared Error.
• NCT: The noncentral (Student’s) t distribution.
• NCT∗: A location-shifted NCT random variable such that the mean is zero,
as given in (3) below.
• pdf: Probability density function.
• SIMBA: The Simulation-Based Approximate mean-ES portfolio optimization
method proposed in Sect. 3.4.
• VaR: Value at Risk.
In this paper, we choose to work with the relatively simple Student’s t cop-
ula, with noncentral-t (NCT) GARCH margins, as studied in Paolella and Polak
(2015a), hereafter denoted ALRIGHT. A detailed study of the t copula can be
found in Demarta and McNeil (2005) and McNeil et al. (2015). With respect
to the margin distribution, note that it is rather ﬂexible: Each is endowed
with its own tail thickness parameter (the degrees of freedom of the NCT)
and also an asymmetry parameter (via the noncentrality). While allowance for
non-Gaussianity via leptokurtic or heavy-tailed distributions for ﬁnancial asset
returns data is virtually understood now, the asymmetry of asset returns is
also important, and has been shown to be highly relevant for risk and density
forecasting by numerous authors; see, e.g., Jondeau and Rockinger (2003, 2012),
Haas et al. (2004), Kuester et al. (2006), Adcock (2010, 2014), Haas et al. (2013),
Adcock et al. (2015), and the references therein. The margin distribution is also
endowed with a GARCH-type structure for the evolution of the scale term that
allows for asymmetries for the eﬀect of the innovation term on the volatility,
sometimes referred to as the leverage eﬀect.
With respect to our choice of the t copula, besides the dispersion matrix, it
has only one shape parameter, ν0. For ﬁnite ν0, the copula exhibits tail depen-
dence, though not in an asymmetric way (see, e.g., Jondeau (2016)). For a given
window of data, ν0 is estimated, as discussed below, and does not vary over
time, except in the primitive way of allowing it to vary as the data window
moves through time. While this choice appears to be rather limited, particularly
in light of the aforementioned trend towards ever more sophisticated copula
constructions, the literature is rather silent on, ﬁrst, how to use copula-based
models for ﬁnancial asset allocation and, secondly, its performance in this regard

38
M. S. Paolella and P. Polak
as compared to more traditional allocation methods. Addressing these issues is
the point of this paper.
Compared to other recent methods of modeling multivariate asset returns
that are strongly superior to classic Markowitz allocation or use of the DCC-
GARCH model (e.g., Paolella and Polak (2015c); Gambacciani and Paolella
(2017)), use of a copula-based model has several beneﬁts. First, it has the ability
to use a two-step estimation procedure, ﬁrst estimating the univariate margin
distributions, and then the copula parameters, this being much faster than joint
maximum likelihood (ML). Second, the copula framework allows the distribu-
tion of asset returns to be non-elliptic, this being another accepted stylized fact
of asset returns; see, e.g., McNeil et al. (2015), Chicheportiche and Bouchaud
(2012), and the references therein. In particular, and diﬀering from other sophis-
ticated non-elliptic models such as Paolella and Polak (2015b), the margins
can be speciﬁed separately from the copula structure, and thus can accommo-
date heterogeneous tail behaviors of the assets, which is a deﬁnitive stylized
fact of asset returns; see, e.g., Paolella and Polak (2015a) for evidence. Third,
the model accommodates (possibly time-varying) tail dependence—a potentially
crucial aspect of risk management, especially during times of severe market
downturns, when correlations among assets tend to substantially increase, and
contagion eﬀects arise. Note that Gaussian based models, such as the popular
DCC-GARCH model (Engle 2009), cannot exhibit tail dependence.
In ALRIGHT, the marginal NCT-GARCH processes are estimated using ML,
but based on a closed-form and “vectorizable” saddlepoint approximation to the
NCT density, enabling vastly faster estimation than if the traditional expression
for the probability density function (pdf) of the NCT distribution were used. In
this paper, we use instead of GARCH the APARCH model of Ding et al. (1993),
as it is able to capture asymmetry in the impact on volatility. Also, estimation
of the NCT-APARCH univariate margin distributions is conducted using the
method presented in Krause and Paolella (2014), which we hereafter denote as
KP, and now discuss.
The KP method is unconventional in three respects. First, the APARCH
parameters are ﬁxed and not estimated, reminiscent of the suggestion of the 1994
RiskMetrics technical document. It is argued and demonstrated in KP that this
is superior to their estimation in terms of value-at-risk (VaR) predictive ability,
not to mention speed. Second, the location parameters (ai,0; see below) are each
estimated via an iterative convergent trimmed-mean procedure where the opti-
mal trimming amount is based on a pre-determined mapping to the estimated
degrees of freedom parameter. This has been shown in simulations in KP to be
nearly as good as use of the MLE, and signiﬁcantly outperforming the mean
(this being a very poor estimator when the tail thickness is relatively large) and
also the median in terms of mean squared error (MSE) accuracy, but obviating
the need for (the otherwise slow) joint maximum likelihood estimation of the
model parameters. Third, the estimation of the remaining two shape parame-
ters of the NCT distribution (degrees of freedom and noncentrality parameter)
is conducted based on a table-lookup procedure using sample quantiles, instead

COBra: Copula-Based Portfolio Optimization
39
of ML estimation. This is not only extraordinarily fast, but, depending on the
granularity of the table employed, (slightly) outperforms the MLE in terms of
MSE when using 250 observations (though as the sample size grows, the MLE
does, as the theory suggests, become slightly preferable).
The KP method also delivers the VaR and expected shortfall (ES) of the
conditional NCT distribution instantaneously, again having used a table-lookup
method. This obviates the otherwise slow integration of a heavy-tailed density
with a complicated pdf to obtain the ES, and the root-solving required to obtain
the quantile function—which involves the slow, repeated evaluation of the NCT
cumulative distribution function (cdf).
In ALRIGHT, the emphasis was on assessing and improving the out-of-
sample density prediction. This is arguably far more relevant than, say, reporting
the statistical signiﬁcance of certain coeﬃcients based on asymptotically valid p-
values (see, e.g., Nguyen, (2016), on the futility of using p-values for model selec-
tion), or inspection of in-sample ﬁt measures, when applications are concerned
with using the predictive density for volatility and correlation forecasting, and/or
portfolio construction. In this paper, we emphasize portfolio performance, this
being a strong test of a model that purportedly outperforms other constructs for
modeling and predicting the multivariate distribution of asset returns. To this
end, we develop a method to signiﬁcantly expedite the calculation of the mean,
variance, and ES of the portfolio distribution. In particular, as the weighted
sum of the margins of a t-copula is analytically intractable, extensive simulation
would be required for accurate assessment of the variance and, in particular, the
ES, of the portfolio distribution, rendering such a naive approach to be too slow
for extensive application. We propose use of various parametric approximations,
all of which are both highly accurate and fast to estimate, and, crucially, whose
mean and ES can be very quickly evaluated.
While our framework supports the use of the variance as a risk measure in
portfolio optimization, we concentrate on use of the ES because, in a non-elliptic
framework, optimal asset allocation requires the use of left-tail risk measures, as
opposed to the variance. The ES is considered in this regard to be ideal; see, e.g.,
Embrechts et al. (2002), and the consultative document of the Basel Committee
on Banking Supervision (2013). Empirical evidence favoring the use of mean-ES
instead of mean-variance portfolio optimization in the presence of non-elliptic
data is demonstrated, for example, in Paolella and Polak (2015c). See also Righi
and Ceretta (2013, 2015) and the references therein.
The remainder of this paper is as follows. Section 2 presents the copula model,
discusses the employed estimation methodology, and how it is forecast. Section 3
reviews how simulation is used for portfolio optimization and the concept of ES
span. Section 4 walks through some empirical demonstrations using both simu-
lated and real data, in order to assess the viability of the method and why it
could fail. Section 5 details a new heuristic algorithm for improved asset allo-
cation, and demonstrates its superior performance. Section 6 provides conclud-
ing remarks on the proposed methodology, discusses some of its beneﬁts com-
pared to more common asset allocation techniques, and provides some ideas for

40
M. S. Paolella and P. Polak
future research. Appendix A details the new method for calculating the ES of
the portfolio distribution. Appendix B discusses estimation of the Gaussian DCC
model, as used in our empirical work.
2
Model and Estimation
2.1
Notation
Let Ri,t denote the (percentage log) return on asset i at time t, which we assume
to be equally spaced in time (e.g., daily, ignoring weekend eﬀects). To desig-
nate an individual time series, we use the notation Ri,• = (Ri,1, . . . , Ri,T )′,
i = 1, . . . , d. For a particular point in time t, the set of all returns is R•,t =
(R1,t, . . . , Rd,t)′, t = 1, . . . , T, while D = R•,• = [R1,• | R2,• | · · · | Rd,•]
denotes the whole data set.
We will require the (singly) noncentral-t, hereafter NCT, distribution, and a
mean-zero version of it. Let Z ∼N(γ, σ2), and Y ∼χ2(ν), independently of Z.
Then
X = μ +
Z

Y/ν
∼NCT(μ, σ; ν, γ),
(1)
where μ and γ are location and noncentrality coeﬃcients, respectively, ν denotes
the degrees of freedom parameter, which we assume is bounded below by 1, and
σ is the scale parameter. With ζ = γ(ν/2)1/2Γ((ν −1)/2)/Γ(ν/2),
E[X] = μ + ζ
and
V(X) =
ν
ν −2

σ2 + γ2
−ζ2,
(2)
the variance existing if ν > 2 (see, e.g., Paolella 2007). A location-shifted NCT
such that the mean is zero is required; this is denoted as
NCT∗(ν, γ) = NCT(−ζ, 1; ν, γ),
Z ∼NCT∗(ν, γ),
E[Z] = 0.
(3)
2.2
Model: Marginal Speciﬁcations
The NCT-APARCH model embodies the two most important stylized facts of
conditional heteroskedasticity and leptokurtic tails of the innovations process
(in common with the ubiquitous t-GARCH model of Bollerslev 1987), but also
asymmetry in the innovations process and the eﬀect of shocks on volatility. In
particular, we assume Ri,• follows the location-scale process
Ri,t = ai,0 + σi,t Xi,t,
Xi,t
iid
∼NCT∗(νi, γi),
i = 1, . . . , d,
(4)
where the evolution of σi,t is governed by an APARCH(1, 1) process from Ding
et al. (1993), and studied extensively in He and Ter¨asvirta (1999a, b), Ling and
McAleer (2002), Karanasos and Kim (2006), and Francq and Zako¨ıan (2010,
Chap. 10.) It is given by
|σi,t|p = c0 + c1

|ϵi,t−1| −g1ϵi,t−1
p + d1|σi,t−1|p,
ϵi,t = σi,t Xi,t,
(5)

COBra: Copula-Based Portfolio Optimization
41
with c0, c1 > 0, d1 ≥0, |g1| < 1, and p > 0. From (2) and (3), the ﬁrst two
moments of Ri,t are given by
E[Ri,t] = ai,0,
V(Ri,t) = σ2
i,t

νi
νi −2

1 + γ2
i

−ζ2
i

,
ζi = γi
 νi
2 Γ
 νi −1
2

Γ
 νi
2
−1
, (6)
the variance existing if νi > 2, i = 1, . . . , d. As mentioned above, we assume
νi > 1, to ensure existence of the mean, but do not impose that the variance
exists. However, in our applications, it was always the case that all νi > 2, so
we ﬁx the power parameter p in (5) to be two. From an empirical point of view,
estimates of p are often between one and two, with large standard errors, and
ﬁxing it to any value in [1, 2] has very little eﬀect on the forecasts. For data sets
that are suspected of not possessing a conditional variance (see, e.g., Paolella,
2016, and the references therein), p can be taken to be unity in (5). As discussed
in Krause and Paolella (2014), the APARCH parameters are ﬁxed to judiciously
chosen values given by
c0 = 0.04, c1 = 0.05, d1 = 0.90, and g1 = 0.4.
(7)
2.3
Model: Student’s t Copula
With y = (y1, . . . , yd)′ ∈Rd, a0 = (a1,0, . . . , ad,0)′ ∈Rd, and similarly deﬁned
for parameter vectors ν, γ and σt, the joint density of the asset returns at time
t is
fR•,t(y; a0, ν, γ, σt, Υ, ν0) = fX(x; ν, γ, Υ, ν0)
σ1,tσ2,t · · · σd,t
,
x =
 y1 −a1,0
σ1,t
, . . . , yd −ad,0
σd,t
′
,
(8)
where fX(x; ν, γ, Υ, ν0) is given by
fX(x; ·) = C

Φ−1
ν0

Ψ(x1; ν1, γ1)

, . . . , Φ−1
ν0

Ψ(xd; νd, γd)

; Υ, ν0

d

i=1
ψ(xi; νi, γi);
(9)
Ψ and ψ denote, respectively, the NCT∗cdf and pdf; and Φ−1
ν0 (x) denotes the
Student’s t inverse cdf with ν0 ∈R>0 degrees of freedom evaluated at x ∈R.
Function C(·; ·) is the d-dimensional t-copula, referred to by Fang et al. (2002)
as the density weighting function, and given by
C(·; ·) = C(z1, z2, . . . , zd; Υ, ν0)
= Γ{(ν0 + d)/2}{Γ(ν0/2)}d−1

Γ{(ν0 + 1)/2}
d		Υ
		1/2

1 + z′Υ−1z
ν0
−(ν0+d)/2
d

i=1

1 + z2
i
ν0
(ν0+1)/2
,
(10)
where z = (z1, z2, . . . , zd)′ ∈Rd and Υ is a d × d correlation matrix, i.e.,
Υ =

ρij : ρii = 1, −1 < ρij < 1, i ̸= j, ρji = ρij; i, j = 1, . . . , d

.
(11)

42
M. S. Paolella and P. Polak
Thus, the model exhibits constant conditional correlation (CCC), and generalizes
the Gaussian CCC model ﬁrst presented in Bollerslev (1990). When the margins
are restricted to being symmetric (central Student’s t), we will also refer to the
model as FaK, as an abbreviation of the authors in Fang et al. (2002), who
considered the copula structure (albeit without GARCH eﬀects in the margins)
in detail.
2.4
Predictive Distribution: Simulation and Moments
Based on the obtained parameter estimates (denoted with hats), as discussed
below in Sect. 2.5, and the predicted scale terms σi,t+1, i = 1, . . . , d, computed
as the usual deterministic update of the APARCH recursion (5), the predictive
distribution at time t + 1 based on information at time t is given by
fR•,t+1|t(y; a0, ν, γ, σt+1, Υ, ν0) =
fX(x; ν, γ, Υ, ν0)
σ1,t+1|tσ2,t+1|t · · · σd,t+1|t
,
(12)
where
x =
y1 −a1,0
σ1,t+1|t
, . . . , yd −ad,0
σd,t+1|t
	′
.
As analytic expressions for the convolution of the margins (as required for
the portfolio distribution) are not available, simulation will be used, as discussed
further below. Generating a realization of R•,t+1|t, denoted with a tilde, is con-
ducted as follows Fang et al., (2002, p. 15): Draw Y = (Y1, . . . , Yd)′ from a
d-dimensional multivariate Student’s t distribution with location vector zero,
correlation matrix Υ and degrees of freedom parameter ν0, and set

Ri,t+1|t = ai,0 + σi,t+1Ψ−1
Φν0(Yi); νi, γi

,
i = 1, . . . , d,
(13)
where, as in (9), Ψ and Φ correspond to the NCT∗and Student’s t cdf, respec-
tively, with the NCT∗distribution given in (3).
Below, for portfolio optimization, it is desirable to have the mean and vari-
ance of R•,t+1|t. As each margin Yi is Student’s t with location zero, scale one,
and degrees of freedom ν0, it follows that Φν0(Yi) ∼Unif(0, 1). By construc-
tion, the NCT∗has mean zero. Thus, from the probability integral transform,
E[Ψ−1(Φν0(Yi); νi, γi)] = 0, and
E

R•,t+1|t

= a0.
(14)
Similarly, from (6), if νi > 2 for all i = 1, . . . , d,
dii ≡V

Ri,t+1|t

= σ2
i,t+1

νi
νi −2

1 + γ2
i

−ζ2
i
	
,
i = 1, . . . , d.
(15)
Determining the covariance matrix V(R•,t+1|t) associated with density (8) is not
straightforward. Indeed, Fang et al. (2002) and Abdous et al. (2005) are silent
on its oﬀ-diagonal elements. The simple expression
V

R•,t+1|t

= D ΥD,
if νi > 2 ∀i,
(16)

COBra: Copula-Based Portfolio Optimization
43
suggests itself as a ﬁrst-order approximation, where D is the d × d diagonal
matrix with ith diagonal element d1/2
ii
given in (15). Simulation conﬁrms its
viability when used with parameters typical for ﬁnancial asset returns, with the
obtained discrepancies of the variances and covariances having the same order
of magnitude.
2.5
Parameter Estimation
Model estimation is conducted via a three-step procedure. First, the parameters
of the marginal NCT-APARCH distributions are obtained via the KP method, as
discussed in the introduction. Second, using the NCT-APARCH ﬁltered residuals
from the ﬁrst step, (Ri,t −ai,0)/σi,t, the correlation matrix Υ is estimated using
the traditional plug-in estimator for correlation, possibly augmented with shrink-
age (see below), which diﬀers from conventional wisdom of using the Kendall’s
tau transform estimator (see, e.g., McNeil et al. 2005, p. 98, 230). The latter
is far slower than the simple plug-in correlation estimator, and does not result
necessarily in a positive deﬁnite matrix. Moreover, as demonstrated in Paolella
and Polak (2015a), despite evidence to the contrary, the use of the sample cor-
relation estimator is nearly as eﬃcient as use of the MLE in this context. Third,
the single remaining copula parameter is estimated, or actually just assigned,
based on the analysis in Paolella and Polak (2015a) showing that its estimation
is superﬂuous with respect to density forecasts. In particular, they show that it
is adequate to restrict ν0 = maxi νi, i = 1, . . . , d.
Given the near-instantaneous nature of all three steps, the entire model esti-
mation procedure is trivial: For d = 30 assets and T = 250 observations, we
require 0.17 seconds, while with d = 376 and T = 1, 000, we require 2.1 seconds
(based on a desktop PC with an Intel Core i7-4790k processor, and using Matlab
R2014B).
Because of the proliferation of the parameters as d increases, the estimated
correlation matrix Υ will be subject to large estimation error, and shrinkage can
be highly beneﬁcial; see, e.g., Wolf (2004) and the references therein. Denote by
Υsamp the sample correlation estimator. Shrinkage towards zero can be applied
to its oﬀ-diagonal elements by taking the estimator to be Υ = (1 −sΥ) Υsamp +
sΥI, for some 0 ≤sΥ ≤1. Alternatively, shrinkage towards the average of the
correlation coeﬃcients can be used, which can be algebraically expressed as, with
a = 1′( Υsamp −I)1/(d(d −1)) and 1 a d-vector of ones,
Υ = (1 −sΥ) Υsamp + sΥ

(1 −a)I + a11′
.
(17)
Use of (17), with sΥ = 0.2 for d = 30 was demonstrated in Paolella and Polak
(2015a) to be most eﬀective, in terms of out-of-sample density forecasting.
2.6
Calculating the Portfolio Expected Shortfall
Observe that the distribution of (weighted) sums of the margins of (12)
is not analytically tractable, so that simulation is required to conduct the

44
M. S. Paolella and P. Polak
portfolio optimization. In particular, for a valid portfolio weight vector w =
(w1, . . . , wd)′, denote by random variable Pt+1|t,w the return at time t + 1 given
information up to time t, and portfolio w. The predictive portfolio returns distri-
bution can be empirically generated by drawing s1 replications from (12), stored
in d × s1 matrix, say M, and then computing

PEmp
t+1|t,w = w′M.
(18)
The ES can be empirically approximated based on (18), though, being a tail
measure, a large number of replications s1 will be necessary. This approach
is too time consuming to get the desired accuracy, so we propose to use much
smaller samples and approximate its distribution with a ﬂexible parametric form,
from which the ES can be analytically calculated. The details are contained
in Appendix A, where four such distributions are considered, and a heuristic
algorithm for the choice of s1 is developed that leads to accurate approximation
of the ES.
3
Simulation–Based Portfolio Optimization
Even if one uses the same value of M from (18) during the optimization to
select w (which is what we suggest and do), observe that the ES (irrespective
of its calculation) will not be perfectly continuous (let alone diﬀerentiable) with
respect to the portfolio weights. As such, standard gradient and/or Hessian-based
optimization routines will not be eﬀective for obtaining the optimal portfolio
vector. This is not a serious drawback however: One can repeat the simulation
exercise with, say, s2 randomly chosen portfolio vectors, and then choose the
portfolio vector with the desired characteristics, such as the largest expected
return for a given ES, or the smallest ES for a given desired expected return.
This simulation-based method is illustrated and discussed in detail in Paolella
(2014), and brieﬂy reviewed in Sect. 3.1. Section 3.2 discusses and illustrates the
useful concept of ES span, while Sect. 3.3 introduces mean-variance portfolio
optimization and the associated ES with respect to the ES span.
3.1
Drawing Random Portfolio Weight Vectors
The natural starting point for sampling a random non-negative portfolio vector
w is to draw values that are uniform on the simplex w ∈[0, 1]d, w′1 = 1. It is
important to note that taking wi = Ui/S, for S = d
i=1 Ui, with the Ui being
i.i.d. standard uniform, does not result in a uniform distribution in the simplex.
As detailed in, e.g., Devroye (1986, Chap. 5), to simulate uniformly, it is required
to take logs, a point also correctly noted by Shaw (2010). That is, the portfolio
vector w is uniform in the simplex by taking
w = U(log)/1′U(log),
U(log) = (log U1, . . . , log Ud)′,
Ui
iid
∼Unif(0, 1).
(19)

COBra: Copula-Based Portfolio Optimization
45
Now consider taking instead
w = U(q)/1′U(q),
U(q) = (U q
1 , . . . , U q
d)′,
Ui
iid
∼Unif(0, 1).
(20)
The non-uniformity corresponding to q = 1 is such that there are a disproportion-
ate number of values close to the “1/N” (in our case, 1/d), i.e., equally weighted,
portfolio, and too few near the corner solutions. Observe that, as q →0, (20)
collapses to the 1/N portfolio. The ability of the 1/N portfolio to outperform
even sophisticated allocation methods goes back at least to Bloomﬁeld et al.
(1977), and is further detailed in DeMiguel et al. (2009), Brown et al. (2013),
and the references therein. As such, sampling with q = 1 in (20) is useful because
it covers the region around the equally weighted portfolio. As q →∞in (20), w
will approach a vector of all zeroes, except for a one at the position correspond-
ing to the largest Ui. Thus, a large value of q is very useful for exploring corner
solutions.
We sample by mixing over several types. In particular, out of the s2 values,
we take 40% from the uniform, 10% using q = 1, 40% using q = 64, and 10%
using q = 1024. We denote this as “mixed w sampling”. A data-driven heuristic
method for improved sampling in conjunction with a related method for asset
allocation is developed in Paolella (2017).
3.2
The ES Span
The diﬀerences in sampling techniques can be eﬀectively illustrated using the
so called ES-span, as introduced in Paolella (2014). With D = [R1,• | R2,• |
· · · | Rd,•] denoting a particular data set consisting of d assets, and a speciﬁed
probability ξ for the ES level, we deﬁne the distribution of possible values that
the ES can take on, over the set of all w, under uniform w sampling, and
conditional on a chosen model M, to be spanES(D, M, ξ). When not otherwise
speciﬁed, we use ξ = 0.01, i.e., 1% ES values. For sampling via (20) with a
particular value of q, we write spanES(D, M, ξ, q), and for the mixed w sampling
method, we write spanES(D, M, ξ, Mix).
The values obtained from simulation can be plotted as a histogram or kernel
density estimate, and convey knowledge of the distribution of the ES correspond-
ing to D (and M and ξ). Observe that use of optimization algorithms gives no
such information—they just return a single value (also dependent on D, M and
ξ) that one hopes is the global optimum. The spread of the ES values, mea-
sured as, say, the (sample) variance or interquartile range of spanES(D, M, ξ),
or other measures, such as the distance from the minimal ES value to, say, the
ES corresponding to the equally weighted portfolio, contain information about
the time-varying nature of the data and also indicates under what conditions
the 1/N portfolio is expected to do well. In general, the 1/N portfolio would be
expected to be close to the minimum ES (or minimum variance) portfolio during
relatively calm market periods, such that the returns are not very heavy-tailed,
in which case the central limit theorem is applicable and results in the 1/N

46
M. S. Paolella and P. Polak
portfolio being close to Gaussian, i.e., a distribution with exponential (and not
heavy) tails, and thus whose ES is relatively small, compared to other portfolios.
This aforementioned eﬀect can be seen by comparing the ES-spans associated
with the predictive density corresponding to the next trading day, based on the
year of daily data for years 2005 and 2008, and taking M to be the the copula
model discussed above in Sect. 2. The year 2005 was rather calm, while 2008
corresponds to the global ﬁnancial crisis. This is shown in Fig. 1. Indeed, the
ES corresponding to 1/N is smaller (larger) than the mode of spanES(D, M) for
2005 (2008).
3.3
ES Span for Mean–Variance Optimization
In the classic portfolio optimization framework going back to the seminal work
of Markowitz (1952), the returns are assumed to be an i.i.d. multivariate normal
process (or, more generally, from an elliptic distribution with existing second
moments). One wishes to determine the portfolio weight vector, say w∗, that
yields the lowest variance of the predictive portfolio distribution. We choose to
impose a no-short-selling constraint because it is common in practice, required by
some investors (such as pension funds), and also because it can be interpreted
as a type of shrinkage estimator that reduces the risk in estimated optimal
portfolios; see Jagannathan and Ma (2003). Let wM(D) denote the minimum
variance portfolio without short selling, and based on the data D treated as
an i.i.d. set of realizations, computed based on the sample plug-in mean and
variance-covariance estimators. Further, let wM(D, τ) be the same, but subject
to an expected percentage annual mean return of τ = τannual, i.e.,
wM

D, τ

= arg min
w∈W w′V

D

w
such that
w′E[D] ≥τdaily,
(21)
where E and V denote plug-in estimators of their arguments,
W = {w ∈[0, 1]d : 1′
d w = 1},
(22)
and, with discrete compounding,
τdaily = 100

1 +
τ
100
1/250
−1
	
,
τ = 100

1 + τdaily
100
250
−1
	
,
(23)
here calculated assuming 250 business days per year.
Observe that calculation of wM(D) and wM(D, τ) requires numeric opti-
mization, but these are convex programming problems whose solutions are fast
and numerically reliable. The ES values for wM(D) and wM(D, 0.10) are also
displayed in Fig. 1. As expected, the ES of wM(D, 0.10) exceeds that of wM(D).
It appears that wM(D, τ) can be used as a good reference point from which
random w should be generated. In particular, we suggest generating s5 values
of w according to
w = s6 wM + (1 −s6)
w,
(24)

COBra: Copula-Based Portfolio Optimization
47
Fig. 1.
Histogram
of
10,000
1%
ES
values
of
spanES(D, M, 0.01)
(top)
and
spanES(D, M, 0.01, Mix) (bottom), for the predictive density of January 3rd, 2006,
based on D being the d = 30 constituents (as of April 2013) of the Dow Jones Indus-
trial Average index (obtained from Wharton/CRSP) for the 252 trading days in year
2005 (left panels) and for January 2nd, 2009, based on the 253 trading days in 2008
(right panels); and such that model M is the copula model discussed herein, and using
the NCT method for the parametric approximation of the simulated portfolio returns
Pt+1|t,w to compute the ES, as discussed in Appendix A.
The ES of the 30 single stocks are shown as black vertical lines, though the graphs
were truncated to improve readability, so that not all values appear. 1/N refers to the
equally weighted portfolio, while wM(D) and wM(D, 0.10) refer to “Markowitz”-type
portfolio vectors, obtained by treating the data as i.i.d. and using the sample mean and
variance-covariance matrix to determine the minimum variance, and 10%-mean return
constrained minimum variance portfolios, respectively, and such that short selling is
not allowed.

48
M. S. Paolella and P. Polak
where 
w is generated via uniform w sampling and s6 is a tuning parameter
indicating how close the new random portfolios are to wM (as measured, for
example, in terms of Euclidean distance). Use of this method to obtain the ES-
span, which we denote by spanES(D, M, ξ, Mix+M), and values s5 = 1, 000 and
s6 = 0.90, resulted in graphics similar to those in Fig. 1, but with a cluster of
ES values in the neighborhood of the wM(D, τ) solution.
3.4
Mean–ES Optimization Based on Simulation
We propose a simulation-based approximate mean-ES portfolio method, here-
after SIMBA. There are numerous beneﬁts of using pure simulation instead of
an optimization algorithm; see the detailed discussion in Paolella (2014). The
objective is to deliver a portfolio vector that yields the lowest expected shortfall
of the predictive portfolio distribution at time t+1, conditional on a lower bound
of its expected daily percentage return τdaily,
w⋆= arg min
w∈W ES

Pt+1|t,w, ξ

such that
E

Pt+1|t,w

≥τdaily,
(25)
where W is given in (22) and ξ is a pre-speciﬁed probability associated with the
ES (for which we take 0.01).
Observe that, by the nature of simulated-based estimation, (25) will not be
exactly obtained, but only approximated. We argue that this is not a drawback:
All models, including ours, are wrong w.p.1; are anyway subject to estimation
error; and the portfolio delivered will depend on the chosen data set D, in par-
ticular, how much past data to use, and which assets to include; and, in the case
of non-ellipticity, also depends on the choice of ξ. As such, the method should be
judged not on how well (25) can be evaluated, but rather on the out-of-sample
portfolio performance, conditional on all tuning parameters and the heuristics
used to calculate (25).
4
Empirical Demonstration
Before applying our methods to real data, we investigate its performance using
simulated data and based on the true model parameters. This obviously unrealis-
tic setting serves as a check on the methodology and also (assuming the method
is programmed correctly), will illustrate the large variation in the performance
of the methods due strictly to the nature of statistical sampling. We begin with
the simpliﬁed case of the usual multivariate Student’s t distribution which, being
elliptic, implies that standard Markowitz allocation is optimal, provided that
second moments exist.
4.1
Use of the Multivariate t Distribution
We simulate ﬁrst from the multivariate t distribution (hereafter MVT), this
being a special case of our general model with all margins having a (central)
Student’s t distribution with the same degrees of freedom parameter, and no
GARCH eﬀects. We begin with a summary of important facts on the MVT
distribution.

COBra: Copula-Based Portfolio Optimization
49
4.1.1
Distribution Theory
Recall that the d-dimensional, zero-location vector, identity-scale matrix, mul-
tivariate Student’s t distribution with v > 0 degrees of freedom has density
fX(x; v) =
Γ
 v+d
2

Γ
 v
2

(vπ)d/2

1 + x′x
v
	−(v+d)/2
,
(26)
for x = (x1, . . . , xd)′. The characteristic function (c.f.) corresponding to (26) was
ﬁrst (correctly) given by Sutradhar (1986) (without use of the Bessel function,
but with diﬀerent expressions for when v is odd, even, and fractional), while
Song et al. (2014) derive it (and that of a type of generalized multivariate t)
by extending the method for the univariate case from Hurst (1995), resulting
in a much more compact expression in terms of the Bessel function. With t =
(t1, . . . , td)′ ∈Rd, it is given by
ϕX(t; v) = Kv/2 (∥√vt∥) (∥√vt∥)v/2
Γ (v/2) 2v/2−1
,
∥t∥=
√
t′t.
(27)
For vector μ = (μ1, . . . , μd)′ ∈Rd and d × d dispersion matrix Σ > 0 with
typical entry denoted σij and diagonal elements denoted σ2
j , j = 1, . . . , d, the
location-scale version of (26) is given by
fX(x; μ, Σ, v) =
Γ
 v+d
2

Γ
 v
2

(vπ)d/2 |Σ|1/2

1 + (x −μ)′ Σ−1 (x −μ)
v
	−(v+d)/2
,
(28)
denoted X ∼tv(μ, Σ). This distribution arises as follows: Let X ∼IGam(α, β)
with density
fX (x; α, β) = [βα/Γ (α)] x−(α+1) exp {−β/x} I(0,∞) (x) ,
α > 0, β > 0,
(29)
and trivial calculations conﬁrming that
E[Xr] = Γ(α −r)
Γ(α)
βr,
α > r,
(30)
so that
E[X] =
β
α −1,
V(X) =
β2
(α −1)2(α −2),
(31)
if α > 1 and α > 2, respectively. Let G ∼IGam (v/2, v/2), v ∈R>0 and let
Z = (Z1, Z2, . . . , Zd)′ ∼Nd (0, Σ). Then
X = (X1, X2, . . . , Xd)′ = μ +
√
GZ
(32)
follows a d-variate tv(μ, Σ) distribution. From (30),
E

G1/2
=

v/2Γ
 v−1
2

Γ
 v
2
 ,
v > 1,
(33)

50
M. S. Paolella and P. Polak
so that, for v > 1, E[X] exists, and, from (32), E[X] = μ + E

G1/2
E[Z] = μ.
From (31),
E[G] = v/(v −2), if v > 2,
(34)
implying the well-known results that
E[X] = μ, if v > 1,
V(X) =
v
v −2Σ, if v > 2.
(35)
Expression (32) is equivalent to saying that (X | G = g) ∼N (μ, gΣ), so that
fX (x; μ, Σ, v) =
 ∞
0
fX|G (x; g) fG (g; v/2, v/2) dg.
(36)
The c.f. corresponding to (28) is
ϕX(t; μ, Σ, v) = E

eit′X
= eit′μ Kv/2
√vΣ1/2t
√vΣ1/2t
v/2
Γ (v/2) 2v/2−1
.
(37)
Let j ∈{1, 2, . . . , n} and deﬁne t = (0, . . . , 0, t, 0, . . . , 0)′, where t appears in
the jth position. The marginal c.f. corresponding to Xj is Student’s t, seen as
follows. Observe that, with Σ1/2 symmetric (as can be obtained via the spectral
decomposition method of calculating it),
√vΣ1/2t
 = √v
√
t′Σt = √v|t|σj.
Thus,
ϕXj(t) = eitjμj Kv/2

v1/2|t|σj
 
v1/2|t|σj
v/2
Γ(v/2)2v/2−1
,
(38)
so that, from the uniqueness theorem of characteristic functions, Xj ∼tv(μj, σj).
See Ding (2016) for a simple derivation of (and corrections to mistakes in previous
literature) of the conditional distribution of subsets of X given a diﬀerent subset,
paralleling the well-known result for the multivariate normal.
The following result is necessary for calculating the portfolio distribution
when the assets follow a MVT distribution. Let X = (X1, . . . , Xd)′ ∼tv(μ, Σ)
with p.d.f. (28), and deﬁne S = d
j=1 aiXi = a′X for a = (a1, . . . , ad)′ ̸= 0, i.e.,
S is a non-zero weighted sum of the univariate margins. Then
ϕS(t) = ES

eitS
= EX

eita′X
= EX

ei(ta)′X
= ϕX(ta; μ, Σ, v)
= eita′μ Kv/2
√vtΣ1/2a
√vtΣ1/2a
v/2
Γ (v/2) 2v/2−1
= eitμS Kv/2

v1/2|t|κ

v1/2|t|κ
v/2
Γ (v/2) 2v/2−1
= eitμSϕT (κt; v),
(39)
where T ∼tv, μS = a′μ and κ =
Σ1/2a
 =
√
a′Σa > 0. Thus, S
d= μS + κT, a
location-scale Student’s t with v degrees of freedom, where
d= means equality in
distribution (and is not to be confused with the dimension d of X).

COBra: Copula-Based Portfolio Optimization
51
This method of proof can be extended to show a more general result encom-
passing (38) and (39). Let X ∼tv(μ, Σ). For 1 ≤k ≤d, c ∈Rk, and B a k × d
real matrix,
c + BX ∼tv (c + Bμ, BΣB′) .
(40)
This also follows from the more general statement for elliptic random variables;
see Kelker (1970), Cambanis et al. (1981), Fang et al. (1989), and McNeil et al.
(2015) for further details on elliptic distribution theory.
4.1.2
Empirical Performance Based on the MVT
We use d = 10, v = 4 degrees of freedom, and take the mean vector to be
i.i.d. N(0, 0.12), scale terms to be i.i.d. Exp(1, 1) (i.e., scale one, and location
one), and the oﬀ-diagonal elements of the correlation matrix of the MVT to be
i.i.d. Beta(4, 9), with mean 4/13 and such that the resulting matrix is positive
deﬁnite.
The next step is to compute the optimal portfolio vectors and the asso-
ciated realized returns, over moving windows of (arbitrary) length 1, 000 and
τ = 10%, using the long-only Markowitz method, and the allocation method
using the simulation method (19) and knowledge of the true MVT parameters.
This simulation exercise runs quickly, and it was conducted several times, with
six representative results shown in Fig. 2. Observe that the portfolio ES is triv-
ially calculated in this situation, as the MVT is in the elliptic class, as discussed
above, so that (weighted) sums of its margins remains in the class, i.e., is uni-
variate Student’s t. We plot the cumulative realized returns from the Markowitz
and MVT method via simulation from (19) based on s2 = 10, 000 replications,
as well as the performance of the equally weighted (“1/N”) portfolio. Overlaid
are 100 cumulative returns based on randomly selecting the portfolio weights at
each point in time to give a sense of if the computational methods are genuinely
outperforming pure luck.
In each of the six cases, the true MVT parameters are diﬀerent, but come
from the same underlying distribution, as discussed above. The fact that the
MVT case is using the true parameter values gives it an edge in terms of total
returns, as seen in the middle and lower left panels, though in other cases, it does
not perform better in ﬁnite-time experiments, such as in the middle right panel.
The take-away message is that, even over a period using 2,000 days of trading,
allocation based on the true model and true parameters may not outperform
the somewhat naive Markowitz approach (at least in terms of total return), and
that the latter can even be beaten by the very naive 1/N strategy.
4.2
Use of the Student’s t Copula Model with Known Parameters
We next take one step (of several) towards reality and leave the elliptic world,
using instead the t copula model with heterogeneous degrees of freedom, such
that, instead of the ﬁxed degrees of freedom v = 4 used above, we take the νi in
(4) and (8) to be realizations from a simulated Unif(2, 7) distribution. There are

52
M. S. Paolella and P. Polak
Fig. 2. Cumulative returns of the 1/N, Markowitz and MVT models, the latter using
the true parameter values. The thinner, dashed (red) line uses s2 = 1, 000 instead of
s2 = 10, 000. In all but the top left case, use of s2 = 10, 000 is at least as good as
s2 = 1, 000 and in some cases, such as the last four panels, leads to substantially better
results.
still no GARCH eﬀects, and we still assume the model, and the true parameters,
are known. We also keep the restriction that the margins are symmetric, so
that this coincides with the FaK model from Sect. 2.3. As the distribution of the
(weighted) sum of margins in this case is not analytically tractable, computation

COBra: Copula-Based Portfolio Optimization
53
of the ES is done via the method in Sect. 2.6, namely using the empirical VaR
and ES, obtained from s1 = 10, 000 draws.
Results for four runs are shown in Fig. 3, with other runs (not shown) being
similar. We obtain our hoped-for result that the t-copula model outperforms
Markowitz (which is designed for elliptic data), and does so particularly when
the set of νi tended to have smaller (heavier-tail) values. The 1/N portfolio
is also seen to be inferior in this setting, particularly in the last of the four
shown runs. The graphs corresponding to the t copula model are also such that
they systematically lie near or above the top of the cloud of cumulative returns
obtained from random portfolio allocations, indicating that accounting for the
heavy-tailed and heterogeneous-tailed nature of the data indeed leads to superior
asset allocation. This exercise also adds conﬁrmation to the fact that allocations
diﬀer in the non-elliptic case, particularly amid heavy tails, and also that the
algorithm for obtaining the optimal portfolio, and the method of calculating the
ES for a given portfolio vector, are working.
Fig. 3. Similar to Fig. 2, but based on the t copula model with symmetric Student’s t
margins without GARCH and with diﬀerent degrees of freedom, using the true para-
meter values (labeled FaK). All plots were truncated in order to have the same y-axis.

54
M. S. Paolella and P. Polak
4.3
Use of the Student’s t Copula Model with Unknown Parameters
The crucial next step is to still use knowledge that the data generating process
is the t copula (still without GARCH), but use parameter estimates instead of
the true values, based on the estimation method discussed in Sect. 2, along with
applying shrinkage to the estimated correlation matrix from (17), with the two
cases sΥ = 0 and sΥ = 0.30.
Figure 4 is similar to Fig. 3, and uses the same generated data, so that the
two ﬁgures can be directly compared. The degradation in performance of the
copula model is apparent: The realistic necessity of parameter estimation when
using parametric models takes a strong toll for all of the four runs shown, and
also shrinkage of the estimated correlation matrix does not help, but rather, at
least for the cases shown and the choice of sΥ = 0.30, predominantly hurts.
Fig. 4. Performance comparison using the same four data sets as in Fig. 3, and having
estimated the model parameters.
Note that, if the true multivariate predictive density were somehow available,
then the optimal portfolio can be elicited from it. However, this is probabilistic,
and thus only with repeated investment over very many time periods would
it be the case that, on average, the desired return is achieved with respect to
the speciﬁed risk. As (i) the true predictive density is clearly not attainable

COBra: Copula-Based Portfolio Optimization
55
(because the speciﬁed model is wrong w.p.1, along with the associated parameter
estimation error); and (ii) backtest exercises necessarily involve a ﬁnite amount
of data (so that the real long-term performance cannot be assessed with great
accuracy), assessment of genuine asset allocation performance is diﬃcult.
4.4
Alternative Investment Strategy
In light of the previous humbling results based on simulated data and use of the
FaK model, we now consider an alterative investment strategy that capitalizes
on the nature of how the optimal portfolio is determined. In particular, as we
use random sampling instead of a black-box optimization algorithm to determine
(25), we have access to s2 = 10, 000 portfolios. We apply the following algorithm
for a given expected return τ, for which we use 10%:
1. For a given data set of dimension d, window length T, and expected return τ,
estimate the copula model. In the below exercise, we still use simulated data,
and so, for simplicity, omit GARCH eﬀects.
2. Attempt s2 random portfolios (we use s2 = 10, 000 for d = 10), and if after
s2/10 generations, no portfolio reaches the desired expected annual return
(the τ-constraint), give up (and trading does not occur).
3. Assuming the exit in step 1 is not engaged, from the s2 portfolios, store those
that meet the τ-constraint, amassing a total of v valid portfolios.
4. If v < s2/100, then do not trade. The idea is that, if so few portfolios meet the
τ-constraint, then, taking the portfolio parameter uncertainty into account,
it is perhaps unlikely that the expected return will actually be met.
5. Assuming v ≥s2/100, keep the subset consisting of the s2/10 with the lowest
ES. (This requires that the stored ES, and the associated stored expected
returns and portfolio vectors, are sorted.)
6. From this remaining subset, choose that portfolio with the highest expected
return.
The core idea is to collect the 10% of portfolios yielding the lowest ES, and
then choose among them the one with the highest expected return. Observe how
this algorithm could also be applied to the Markowitz setting, using variance
as a risk measure, but then the sampling algorithm would need to be used,
as opposed to a direct optimization algorithm, as is applicable with (25). This
alternative method contains several tuning parameters, such as the choice of τ,
the window size, s2, the shrinkage amount for the correlation matrix, and the
(arbitrary) values of s2/100 in step 4, and s2/10 in step 5. In light of the risks
and pitfalls of backtest overﬁtting (see, e.g., Bailey et al. 2014; 2016), one is
behooved to investigate its performance for a range of such values (and data
sets), and conﬁrm that the results are reasonably robust with respect to their
choices around an optimal range.
Figure 5 shows the resulting graphs based again on the same four simulated
data sets as used above. There now appears to be some space for optimism,
and tweaking the—somewhat arbitrarily chosen—tuning parameters surely will

56
M. S. Paolella and P. Polak
lead to enhanced performance. Realistic application would entail (i) applying
the method also to real data; (ii) computing additional performance measures
such as the Sharpe and related ratios; and (iii) taking into account transaction
costs (these can be approximated by the simple but reasonably accurate method
in DeMiguel et al. 2013). Finally, for real daily returns data, it would be wise to
use the APARCH ﬁlter as discussed in Sect. 2.2.
Fig. 5. Similar to Fig. 4, with estimated parameters and not using shrinkage on the cor-
relation matrix, but having used the alternative investment strategy based on choosing
among the 10% of generated portfolios with the lowest ES the one with the highest
expected return.
4.5
Application to Real Data
We ﬁnally turn to the use of real data, using the daily (percentage log) returns
based on the closing prices of the 30 stocks associated with the DJIA index,
from Jan 2, 2001 to July 21, 2016 (conveniently including the market turmoil
associated with the Brexit event). We use the alternative investment strategy
from Sect. 4.4 and continue to use the FaK model instead of the more general
case with noncentral t margins because the calculation of the portfolio ES is
much faster in this case. Figure 6 shows the results in terms of the cumulative
returns. The results are not encouraging, to put it mildly. Based on the previous

COBra: Copula-Based Portfolio Optimization
57
progression of results, we can surmise two reasons for this: First, the fact that
the parameters need to be estimated contributes signiﬁcantly to the bad perfor-
mance. Second, having moved from simulated to real data, we can conclude that
the true data generating process is not being adequately captured by the FaK
model.
Fig. 6. Cumulative returns for the DJIA data using the FaK model, the 1/N allocation,
and 400 random portfolios (showing only the most extreme ones).
To help address these issues, we turn to a more general alternative investment
strategy that is applicable to all copula-based models under the assumption
that they can be simulated from, and also use the t copula model with NCT-
APARCH innovations, this being computationally more expensive because of
the noncentrality parameters and simulation from the t copula.
5
SIMBA: Simulation-Based Approximate Mean-ES
Optimized Portfolio Selection
Based on the disappointing performance when applied to real data of the FaK
model in conjunction with simple mean-ES optimization as well as the alternative
allocation method, we attempt to generalize the latter, capitalizing on the nature
of the simulation-based method of portfolio optimization. The proposed heuristic
algorithm, which we term SIMBA, addresses two situations: The ﬁrst, as applies
to our model, is such that the expected return of a candidate portfolio is known;
in our case, it is given as w′a0 using (14). The second applies to situations in
which more sophisticated copula structures are used, and the expectation is not
analytically known. Both cases assume that the ES is not analytically available.

58
M. S. Paolella and P. Polak
5.1
The SIMBA Algorithm
The SIMBA algorithm is given as follows, applicable to an h-step ahead opti-
mal asset allocation, for which we always use h = 1 in this paper. Let
Ei ≡E[Pt+1|t,wi].
1. Compute M from (18) based on s1 replications.
2. For the case in which the expected return of the portfolio is not analytically
known: (Alternate to step 3)
(a) Draw s2 random portfolio vectors wi, i = 1, . . . , s2, according to either
the uniform or mixed method as discussed in Sect. 3.1 above, and add to
this set the s5 values drawn via (24), and the s4 candidate vectors from
the remarks below. Denote by ˇs2 the total number of vectors.
(b) Approximate the ˇs2 expected returns Ei, and the associated ES values,
using one of the parametric forms discussed in Appendix A.
(c) Discard all portfolio vectors such that Ei < τdaily. Denote the remaining
number as ˇs∗
2. For given tuning parameters ˇsmin
2
and ˇsmax
2
, set
ˇsτ
2 = max

ˇsmin
2
, min

ˇs∗
2, ˇsmax
2

.
If ˇsmin
2
= 0 and ˇsτ
2 = 0, trading is not conducted. If ˇsτ
2 < ˇs∗
2, then sort
the ˇs∗
2 according to expected return, highest ﬁrst, and keep only the ﬁrst
ˇsτ
2 associated portfolio vectors.
3. For the case in which the expected return of the portfolio is analytically
known: (Alternative to step 2) Sample s2 portfolio vectors such that Ei ≥
τdaily, noting that, for each, the ES does not need to be calculated. To this
set, add the s4 and s5 candidate vectors, as discussed in step 2a, that satisfy
the mean constraint. Let ˇsτ
2 denote the total number of resulting vectors.
4. Calculate w⋆
0, the (approximate) min-ES or mean-ES portfolio:
(a) If step 2 is conducted and τdaily = −∞, then from the ˇsτ
2 ES values,
deliver the portfolio vector that leads to the portfolio with the lowest ES.
(b) If step 2 is conducted and τdaily > −∞, or if step 3 is conducted, then from
the ˇsτ
2 ES values, deliver the portfolio vector that leads to the portfolio
with the lowest ES if Ei ≥τdaily holds for some i = 1, . . . , ˇs2; otherwise,
return arg maxi Ei, the portfolio corresponding to the highest return.
5. Calculate w⋆
1: Based on the set of ˇsτ
s portfolio vectors resulting from step 2
or 3, deliver
w⋆
1 = arg max
wi
E


Pt+1|t,wi
 
ES


Pt+1|t,wi

,
(41)
this fraction being an obvious analog to many ratios, such as Sharpe, with
expected return divided by a risk measure, but computed not for a sample of
realized returns, but rather across diﬀerent portfolio vectors, for a particular
single point in time. We deem this object the performance ratio of individual
time forecasts, or PROFITS measure.

COBra: Copula-Based Portfolio Optimization
59
6. Calculate w⋆
2: Sort the ˇsτ
2 portfolio vectors obtained in step 2 or 3 according
to expected return, highest ﬁrst, and select the ﬁrst s3% of them. From these
vectors, return w⋆
2(s3) as the vector corresponding to the smallest portfolio
ES value. For example, w⋆
2(1) corresponds to use of 1% of the ˇsτ
2 values. For
s3 = 100, w⋆
2 and w⋆
0 coincide.
Remarks:
1. Observe that, if the expectation of the predictive portfolio distribution is
known (as a function of the estimated location terms a0 and any other relevant
parameters), then use of step 3 instead of step 2 will be faster.
2. By construction of w⋆
2, larger (smaller) values of ES are obtained for smaller
(larger) values of s3. This relates directly to an approximation of the eﬃ-
cient portfolio frontier. By taking advantage of sorting values in w⋆
2 (step
6), diﬀerent values of s3 can be processed at once without noticeable addi-
tional computational eﬀort, so that the eﬃcient frontier based on w⋆
2 could
be plotted.
3. For use with rolling windows exercises, we advocate augmenting the simulated
set of candidate portfolio vectors for estimators w⋆
i , i = 0, 1, 2, by carrying
over, say, s4 of the best performing vectors, e.g., in terms of expected return,
from the previous window. If not otherwise speciﬁed, we use s4 = ˇsτ
2/100.
4. Similar to (24), we suggest augmenting the set of vectors to be carried over
to the subsequent rolling window as follows. Let w∗denote some particular
portfolio vector from the current window. For the subsequent window, select,
say, s7 vectors among the set of current portfolio vectors, wj, j = 1, . . . , s7,
which are at most in a ﬁxed L2 distance from w∗, i.e., such that ∥wj −
w∗∥2
2 < s8 for all j, where s8 is another tuning parameter related to a portfolio
turnover constraint. This can serve to lower transaction costs and provide a
speed increase to SIMBA by reducing the number of sampled portfolios s2.
5.2
Second Empirical Portfolio Performance Comparison
We use for this empirical exercise the daily data for the period Jan 4, 1993,
to Dec 31, 2012, of the d = 30 constituents (as of April 2013) of the Dow
Jones Industrial Average index, as obtained from Wharton/CRSP, and moving
windows of length T = 250. In particular, the ﬁrst window spans Jan 4, 1993, to
Dec 28, 1993, and delivers estimated portfolio vectors for the subsequent trading
day, Dec 29, 1993. The window is then moved one period ahead (and thus changes
by two data points). As such, forecast dates comprise all 4, 787 business days
between Dec 29, 1993, and Dec 31, 2012. The SIMBA tuning parameters used
are s1 = 1e3, s2 = 1e4, s3 = 1, s4 = 0.01ˇsτ
2, and the default values for the other
si as stated above, in particular, s5 = 1, 000, s6 = 0.90, s7 = 1, 000, s8 = 0.1,
τannual = 0.1 and ξ = 0.01. For drawing the s2 portfolio vectors, “mixed w
sampling” is used, as described in Sect. 3.1.
The left panel of Fig. 7 shows the cumulative returns obtained from w⋆
i ,
i = 0, 1, 2, when the NCT-APARCH marginals are estimated using the fast KP

60
M. S. Paolella and P. Polak
method, and based on the stable Paretian methods for the ES; see Appendix A.4.
These are compared to the following benchmark models: (i) the equally weighted
(1/N) portfolio; (ii) the portfolio according to (21); and (iii) the portfolio similar
to (21), but such that the sample mean and variance are replaced by the values
obtained by using the standard Gaussian DCC-GARCH model of Engle (2002,
2009), which we denote by DCC, and whose calculations are described in detail
in Appendix B.
We see that the use of w⋆
2(1) outperforms the usual competition by a wide
margin in terms of total wealth. The exercise was repeated, but having used
maximum likelihood estimation for the NCT-APARCH marginals. In addition
to being far slower to estimate, the performance was blatantly lower, with a
terminal wealth of 250 instead of 300, as obtained using the fast KP-method
of estimation. This adds further evidence to the beneﬁt of ﬁxing the APARCH
parameters instead of estimating them.
Fig. 7. Left: Cumulative realized portfolio returns for the SIMBA mean-ES portfolios
w⋆
0, w⋆
1, and w⋆
2(1), estimated by the fast KP method, and using the stable Paretian
distribution parametric approximation for determining the ES, overlaid with the per-
formance of the three benchmark models.
Right: Quantile values over time based on 100, 000 paths of cumulative returns of
random portfolios. Each path consists of 505 portfolio returns, obtained from a random
portfolio characterized by a portfolio vector drawn uniformly according to (19). Note
the change of the y-axis, for improved reading.
It is of interest to assess which models, if any, beat randomly chosen portfo-
lios. It is well known that many active strategies do not perform better than ran-
dom allocations; see, for example Edwards and Lazzara (2014), Hough (2014),
and Clare et al. (2015). In particular, we wish to take the null hypothesis to
be that a proposed method does not outperform purely random asset alloca-
tions. To obtain the distribution of the null, the portfolio vector obtained by
drawing uniformly according to (19) is used at each point in time, and this is
conducted B = 100, 000 times. Instead of plotting the resulting B cumulative

COBra: Copula-Based Portfolio Optimization
61
returns graphs, we plot a set of quantiles, computed at each point in time. This
“monkey plot” is shown in the right panel of Fig. 7. The terminal wealth of the
copula-based strategy is well above the 99% quantile (with a terminal wealth of
about 220), lending evidence to the claim that the copula strategy is not per-
forming well by chance. Interestingly, the next best performer, as shown in the
left panel of Fig. 7, is the DCC model, which results in a terminal wealth just
below that of the 95% quantile of the random portfolios, suggesting that it is
not signiﬁcantly outperforming “lucky random allocations”. All other models lie
well below the 95% quantile.
6
Conclusions
The goal of this paper is to operationalize the use of a basic t-copula struc-
ture with ﬂexible and quickly-estimated NCT-APARCH margins, for portfolio
optimization. It embodies the proposed method of simulation for eliciting opti-
mal mean-ES portfolios based on a parametric approximation to the simulated
portfolio density for speed enhancement. Future work includes further reﬁne-
ment of the algorithm, application to a variety of data sets, and accounting for
transaction costs.
The better performance of the w⋆
2(1) estimator via the ALRIGHT-SIMBA
model (hereafter, just A-S) compared to the Gaussian DCC-GARCH model
(hereafter, just DCC) can be attributed to several factors.
1. A-S uses a copula framework, so that a basic form of tail dependence is
captured. Tail dependence is not possible in the DCC model. Via use of short
windows of estimation (we use 250) and re-estimation at every window, the
copula parameter ν0 is allowed to change through time, albeit in a primitive
manner. More advanced copula structures could be used that exhibit richer
tail dependency structures, possibly in conjunction with methods that are
explicitly designed to detect changes in tail dependence, such as B¨ucher et al.
(2015).
2. The A-S model is non-elliptic and heavy tailed; the DCC is not only elliptic,
but also Gaussian.
3. The marginals of A-S use an NCT-APARCH model, allowing for fat-tailed,
asymmetric innovations and volatility asymmetry. Moreover, estimation of
the predictive means of the constituent returns are more accurate, owing to
the use of the iterative trimmed mean procedure in KP, as opposed to the
sample mean, which is non-optimal for heavy-tailed data, or just setting the
mean to zero, in which case, only the minimum ES portfolio can be obtained,
this being of far less interest to investors.
4. The APARCH model in A-S uses ﬁxed parameters, instead of (ML) estima-
tion, and as argued and demonstrated in KP, is superior in terms of VaR
prediction and density forecasting.
5. While not related to performance, observe that model estimation via the KP-
method is much faster, despite using a NCT-APARCH model for the mar-
gins, compared to DCC, which estimates the GARCH model for each asset.

62
M. S. Paolella and P. Polak
Portfolio optimization, however, is not faster, because of the intractability
of the portfolio distribution, though the SIMBA method, with the paramet-
ric approximation to the portfolio distribution and its fast estimation and
calculation of mean and ES, does allow for reasonable computation time.
6. A-S uses only a CCC structure within the copula framework, but makes use
of shrinkage applied to the dispersion matrix. Weighted likelihood could also
be used, allowing the window size T to be larger while still mitigating the
eﬀect of a changing data generating process through time. We note that both
techniques of shrinkage and weighted likelihood could also be used in the
DCC framework. A possible avenue for future research is to incorporate a
Markov-switching structure into the ALRIGHT model, similar to Pelletier
(2006), thus allowing for a time-varying dispersion matrix Υ. In such a case,
larger sample sizes can and should be used, instead of T = 250.
7. Note that asymmetric extensions of DCC exist; namely that of Cappiello
et al. (2006) and Asai (2013); and presented in a general multivariate matrix
structure, allowing each asset to have its own asymmetric DCC parameter-
ization; though estimation of the matrix version of that model is subject to
the curse of dimensionality, and so not applicable for direct use, even with a
modest number of assets. Furthermore, it has yet to be demonstrated that
such extensions lead to improved portfolio performance and, if still conducted
under a Gaussian assumption, will most likely not be able to outperform the
A-S or other models that use non-Gaussian structures, such as Paolella (2014)
and Paolella and Polak (2015c).
A
Parametric Forms for Approximating the Distribution
of 
RP
We detail here the four candidate parametric structures mentioned in Sect. 2.6.
A.1
The Noncentral Student’s t
The ﬁrst is the location-scale NCT∗distribution (3). As location μ and scale σ
parameters need to be estimated along with the NCT∗shape parameters, we
compute
arg max
μ,σ fNCT

Pt+1|t,w; ν, γ, μ, σ

,
ν, γ = KP

Zt+1|t,w

,
Zt+1|t,w = Pt+1|t,w −μ
σ
.
(42)
Starting values are taken to be the 50% trimmed mean for μ (i.e., the lower
and upper 25% of the sorted sample are ignored) and, using (6) with ν = 4
and γ = 0, gives (s2/2)1/2 for σ, where s2 denotes the sample variance. Two
box constraints q0.25 < μ < q0.75 and (s2/10)1/2 < σ < s are imposed during
estimation, where qξ denotes the ξth sample quantile. The mean and variance are
then determined from (6), while the ES is, via a table-lookup procedure, given
essentially instantaneously from the KP method, noting that, for any probability
0 < ξ < 1, ES(Pt+1|t,w; ξ) = μ + σES(Zt+1|t,w; ξ).

COBra: Copula-Based Portfolio Optimization
63
A.2
The Generalized Asymmetric t
The second candidate is the ﬁve-parameter generalized asymmetric t, or GAt
distribution. The pdf is
fGAt(z; d, ν, θ) = K ×
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩

1 + (−zθ)d
ν
−(ν+1/d)
, if z < 0,

1 + (z/θ)d
ν
−(ν+1/d)
, if z ≥0,
(43)
where d, ν, θ ∈R>0, and K−1 = (θ−1 + θ)d−1ν1/dB(1/d, ν). It is noteworthy
because limiting cases include the generalized exponential (GED), and hence the
Laplace and normal, while the Student’s t (and, thus, the Cauchy) distributions
are special cases. For θ > 1 (θ < 1) the distribution is skewed to the right (left),
while for θ = 1, it is symmetric. See Paolella (2007, p. 273) for further details.
The rth moment for integer r such that 0 ≤r < νd is
E

Zr
= I1 + I2
K−1
= (−1)rθ−(r+1) + θr+1
θ−1 + θ
B

(r + 1)/d, ν −r/d

B

1/d, ν

νr/d,
i.e., the mean is
E

Z

= θ2 −θ−2
θ−1 + θ
B

2/d, ν −1/d

B

1/d, ν

ν1/d
(44)
when νd > 1, and the variance is computed in the obvious way. The cumulative
distribution function (cdf) of Z ∼GAt(d, ν, θ) is
FZ(z; d, ν, θ) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
¯BL

ν, 1/d

1 + θ2
,
if z ≤0,
¯BU

1/d, ν

1 + θ−2
+

1 + θ2−1, if z > 0,
(45)
where ¯B is the incomplete beta ratio,
L =
ν
ν +

−zθ
d ,
and
U =

z/θ
d
ν +

z/θ
d .
For computing the ES, we require E[Zr | Z < c] for r = 1. For c < 0, this is
given by
Sr(c) = (−1)rνr/d

1 + θ2

θr + θr+2 BL

ν −r/d, (r + 1)/d

BL

ν, 1/d

,
,
L =
ν
ν(−cθ)d .
(46)
The existence of the mean and the ES requires νd > 1.

64
M. S. Paolella and P. Polak
A.3
The Two-Component Mixture GAt
With ﬁve parameters (including location and scale), the GAt is a rather ﬂexible
distribution. However, as our third choice, greater accuracy can be obtained by
using a two-component mixture of GAt, with mixing parameters 0 < λ1 < 1
and λ2 = 1 −λ1. This 11 parameter construction is extraordinarily ﬂexible,
and should be quite adequate for modeling the portfolio distribution. We also
assume that the true distribution is not (single component) GAt, and that the
distributional class of two-component mixtures of GAt is identiﬁed. Its pdf and
cdf are just weighted sums of GAt pdfs and cdfs respectively, so that evaluation
of the cdf is no more involved than that of the GAt. Let P denote a K-component
mixGAt distribution, where each component has the three aforementioned shape
parameters, as well as location ui and scale ci, i = 1, . . . , K. First observe that
the cdf of the mixture is given by
FP (z) =
K

j=1
λjFZj
z −uj
cj
; dj, νj, θj
	
,
0 < λj < 1,
K

j=1
λj = 1,
(47)
where the ith cdf mixture component is given as the closed-form expression in
(45), so that a quantile can be found by simple one-dimensional root searching.
Similar to calculations for the ES of mixture distributions in Broda and Paolella
(2011), the ES of the mixture is given by
ESξ(P) = 1
ξ
 qP,ξ
−∞
xfP (x)dx = 1
ξ
K

j=1
λj
 qP,ξ
−∞
xc−1
j fZj
x −uj
cj
	
dx
= 1
ξ
K

j=1
λj

qP,ξ−uj
cj
−∞
(cjz + uj)c−1
j fZj(z)cjdz
= 1
ξ
K

j=1
λj

cj

qP,ξ−uj
cj
−∞
zfZj(z)dz + uj

qP,ξ−uj
cj
−∞
fZj(z)dz

= 1
ξ
K

j=1
λj

cjS1,Zj
qP,ξ −uj
cj
	
+ ujFZj
qP,ξ −uj
cj
	
,
(48)
where qP,ξ is the ξ-quantile of P, S1,Zj is given in (46), and FZj is the cdf
of the GAt random variable given in (45), both functions evaluated with the
parameters dj, νj, and θj from the mixture components, Zj, for j = 1, . . . , K.
While estimation of the two-component mixture GAt is straightforward using
standard ML estimation, it was found that this occasionally resulted in an infe-
rior, possibly bi-modal ﬁt that optically did not agree well with a kernel-density
estimate. This artefact arises from the nature of mixture distributions and the
problems associated with the likelihood. We present a method that leads, with
far higher probability, to a successful model ﬁt, based on a so-called augmented
likelihood procedure. The technique was ﬁrst presented in Broda et al. (2013)
and is adapted for the mixture GAt as follows.

COBra: Copula-Based Portfolio Optimization
65
Let f(x; θ) = K
i=1 λifi(x; θi) be the univariate pdf of a K-component
(ﬁnite) mixture distribution with component weights λ1, . . . , λK positive and
summing to one. The likelihood function is
ℓ⋆(θ; x) =
T

t=1
log
K

i=1
λifi(xt; θi),
(49)
where x = (x1, . . . , xT )′ is the sequence of evaluation points, and θ
=
(λ, θ1, . . . , θK)′ is the vector of all model parameters. Assuming that the θi
include location and scale parameters, ℓ⋆is plagued with “spikes”—it is an
unbounded function with multiple maxima, see, e.g., Kiefer and Wolfowitz
(1956). Hence, numerical maximization of (49) is prone (depending on fac-
tors like starting values and the employed numerical optimization method) to
result in inaccurate, if not arbitrary, estimates. To avoid this problem, an aug-
mented likelihood function is proposed in Broda et al. (2013). The idea is to
remove unbounded states from the likelihood function by introducing a smooth-
ing (shrinkage) term that, at maximum, drives all components to act as one
(irrespective of their assigned mixing weight) such that the mixture loses its
otherwise inherently large ﬂexibility. The suggested augmented likelihood func-
tion is given by

ℓ(θ; x) = ℓ⋆(θ; x) + κ
K

i=1
1
T
T

t=1
log fi(xt; θi),
(50)
where κ, κ ≥0, controls the shrinkage strength. If all component densities fi are
of the same type, larger values of κ lead to more similar parameter estimates
across components, with identical estimates in the limit, as κ →∞. At κ = 0,
(50) reduces to (49). The devised estimator,
θALE = arg max
θ

ℓ(θ; x),
is termed the augmented likelihood estimator (ALE) and is asymptotically con-
sistent, as T →∞. By changing κ, smooth density estimates can be enforced,
even for small sample sizes. For mixGAt with K = 2 and 250 observations, we
obtain κ = 10 as an adequate choice, which, in our empirical testing, guaranteed
unimodal estimates in all cases, while still oﬀering enough ﬂexibility for accurate
density ﬁts, signiﬁcantly better than those obtained with the single component
GAt.
A.4
The Asymmetric Stable Paretian
The fourth candidate we consider is the use of the asymmetric non-Gaussian
stable Paretian distribution, hereafter stable, with location μ, scale c, tail index
α, and asymmetry parameter β. We use the parametrization such that the mean,
assuming α > 1, is given by μ. (In Nolan 2015, and the use of his software, this

66
M. S. Paolella and P. Polak
corresponds to his ﬁrst parametrization; see also Zolotarev 1986; and Samorod-
nitsky and Taqqu 1994.)
This might at ﬁrst seem like an odd candidate, given the historical diﬃcul-
ties in its estimation and the potentially problematic calculation of the ES, given
the extraordinary heavy-tailed nature of the distribution and the problems asso-
ciated with the calculation of the density far into the tails; see, e.g., Paolella
(2016) and the references therein. We circumvent both of these issues as follows.
We make use of the estimator based on the sample characteristic function of
Kogon and Williams (1998), which is fast to calculate, and results in estimates
that are very close in performance to the MLE. We use the function provided in
John Nolan’s STABLE toolbox, saving us the implementation, and easily con-
ﬁrming via simulation that his procedure is correct (and very fast). For the ES
calculation, we ﬁrst need the appropriate quantile, which is also implemented
in Nolan’s toolbox. The ES integral can then be computed using the integral
expression given in Stoyanov et al. (2006), which cleverly avoids integration into
the tail.
This procedure, while feasible, is still too time consuming for our purposes.
Instead, we use the same procedure employed in Krause and Paolella (2014)
to generate a (massive) table in two dimensions (α and β) to deliver the VaR
(the required quantile) and the ES, essentially instantaneously and with very
high accuracy. There is one caveat with its use that requires remedying. It is
well-known, and as simulations quickly verify, that estimation of the asymmetry
parameter β is subject to the most variation, for any particular sample size.
The nature of the stable distribution, with its extremely heavy tails, relative to
asymmetric Student’s t distributions, will induce observations in small samples
that have a relatively large impact on the estimation of β. This is particularly
acute when using a relatively small sample size of T = 250. As such, we rec-
ommend use of a simple shrinkage estimator, with target zero and weight sβ,
namely delivering β = sβ βMLE. Some trial and error suggests sβ = 0.3 to be a
reasonable choice for T = 250.
The motivation for using the stable is the conservative nature of the delivered
ES. In particular, the ﬁrst three methods we discussed are all based on asymmet-
ric variations of the Student’s t distribution which, while clearly heavy-tailed (it
does not possess a moment generating function on an open neighborhood around
zero), still potentially possesses a variance; as opposed to the stable, except in
the measure-zero case of α = 2. As such, and because estimation is based on a
ﬁnite amount of data, the ES delivered from the stable will be expected to be
larger than those from the t-based models. This might be desirable when more
conservative estimates of risk should be used, and will also be expected to aﬀect
the optimized portfolio vectors and the performance of the method.
A.5
Discussion of Portfolio Tail Behavior and ES
It is worth mentioning that the actual tail behavior of ﬁnancial assets is not
necessarily heavy-tailed; the discussion in Heyde and Kou (2004) should settle

COBra: Copula-Based Portfolio Optimization
67
this point. This explains why, on the one hand, exponential-tailed distributions,
such as the mixed normal, can deliver excellent VaR predictions; see, e.g., Haas
et al. (2004), Haas et al. (2013), and Paolella (2013); while, on the other hand,
stable-Paretian GARCH models also work admirably well; see e.g., Mittnik et al.
(2002) and Mittnik and Paolella (2003).
Further, observe that the tail behavior associated with the Pt+1|t,w, given
the model and the parameters, is not subject to debate: by the nature of the
model we employ, it involves convolutions of (dependent) random variables with
power tails, and, as such, will also have power tails, and will (presumably) be in
the domain of attraction of a stable law. It is, however, analytically intractable.
Observe that it is fallacious to argue that, as our model involves use of the
(noncentral) Student’s t, with estimated degrees of freedom parameters (after
application of the APARCH ﬁlter) above two, the convolution will have a ﬁnite
variance, and so the stable distribution cannot be considered. It is crucial to
realize ﬁrst that the model we employ is wrong w.p.1 (and also subject to esti-
mation error) and, second, recalling that, if an i.i.d. set of stable data with,
say, α = 1.7 is estimated as a location-scale Student’s t model, the resulting
estimated degrees of freedom will not be below two, but rather closer to four.
As such, we believe it makes sense to consider several methods of determining
the ES, and compare them in terms of portfolio performance.
A.6
Comparison of Methods
The computation times for estimating the model and evaluating mean and ES
for each of the four methods discussed above were compared. Based on a sam-
ple size of s1 = 1e3, the NCT method requires, on average, 0.20 seconds. The
GAt and mixGAt require 0.23 and 1.96 seconds, respectively, while the stable
requires 0.00064 seconds. Generation of s1 = 1e6 (1e3) samples requires approx-
imately 2769.34 (2.91) seconds, and the empirical calculation of the mean and
ES based on s1 = 1e6 requires approximately 0.35 seconds. The bottleneck in
the generation of samples is the evaluation of the NCT quantile function in (13).
In summary, it is fastest to use s1 = 1e3 samples and one of the parametric
methods to obtain the mean and ES.
We now wish to compare the ES values delivered by each of the methods.
For this, we ﬁx the portfolio vector w to be equally weighted, and use 100 mov-
ing windows of data, each of length 250, and compute, for each method, the ES
corresponding to the one-day-ahead predictive distribution and the ﬁxed equally
weighted portfolio. All the ES values (the empirically determined ones as well as
the parametric ones) are based on (the same) 1e5 replications. The 100 windows
have starting dates 8 August 2012 to 31 December 2012 and use the d = 30 con-
stituents (as of April 2013) of the Dow Jones Industrial Average index from Whar-
ton/CRSP. The values are shown in Fig. 8, and have been smoothed to enhance
visibility. As expected, the stable ES values are larger than those delivered from
the t-based models and also the empirically determined ES values. The mixGAt is
the most ﬂexible distribution and approximates the empirical ES nearly exactly,
though takes the longest time to compute of the four parametric methods.

68
M. S. Paolella and P. Polak
Fig. 8. Comparison of ﬁve methods of estimating ES, as discussed above, for a sequence
of 100 rolling windows and based on the equally weighted portfolio. Each point was
calculated based on (across the methods, the same) 1e5 replications. Left: The 100
values, for each method, plotted as a function of time. Right: The deviations of the
four parametric methods from the empirical one.
A.7
Calibrating the Number of Samples s1
As stated in Sect. 2.6, we wish to determine a heuristic for selecting the number
of samples, s1, from the predictive copula distribution, in order to obtain the
ES. This is conducted as follows. The copula model is estimated for all non-
overlapping windows of length T = 250 based on the 30 components of the DJIA
returns available from 4 Jan. 1993 to 31 Dec. 2012 and the ES of the predictive
returns distribution for the equally weighted portfolio is computed. The goal is
to determine an approximation to the smallest value of s1, say s∗
1, such that
the sampling variance of the ES determined from the parametric methods is less
than some threshold. This value s∗
1 is then linked to the tail thickness of the
various predictive returns distributions over the non-overlapping windows.
To compute s∗
1 for a particular data set, the ES is calculated n = 50 times
for a ﬁxed s1, based on simulation of the predictive returns distribution, and
having used the NCT and stable parametric forms for its approximation. This is
conducted for a range of s1 values, and s∗
1 is taken to be the smallest number such
that the sample variance is less than a threshold value, For the NCT and stable
estimators, Fig. 9 shows the results for selected values of s1 for the NCT case.
As expected, ES variances across rolling windows decrease with s1 increasing.
As can be seen from the middle right panels, a roughly linear relationship is
obtained for the logarithm of ES variance. The analysis was also conducted for
the stable Paretian distribution, resulting in a similar plot (not shown).
A simple regression approach then yields the following. For a threshold of
exp(−2),
s1(νP ) =

100 + (49.5 −3.8 νP + 100.5 ν−1
P )2
I{νP ≤15} + 100I{νP >15}
s1(αP ) =

100 + (−54800 −13612 αP + 55151 α−1
P )2
I{αP ≤1.97} + 100I{αP >1.97} .
(51)

COBra: Copula-Based Portfolio Optimization
69
Fig. 9. Upper left: Percentage log returns of the equally weighted portfolio as used in
the other panels. Mid and lower left: Boxplots of 1% ES values obtained from 50 sim-
ulations based on s1 draws from the ﬁtted copula for diﬀerent non-overlapping rolling
windows of size 250, spanning Jan 4, 1993, to Dec 31, 2012. Timestamps denote the
most recent date included in a data window. All values are obtained via the NCT
estimator. Upper right: Boxplots of 1% ES values sorted in descending order by the
average ES value, overlayed by the average of the estimated degrees of freedom para-
meters. Mid right: ES variances in log scale across rolling windows for diﬀerent samples
sizes s1, sorted by the average ES value per window. Lower right: Linear approxima-
tion of the above panel using ordinary least squares regression, overlayed by another
linear approximation for the estimated degrees of freedom for the largest sample size
s1 = 3, 200 under study.

70
M. S. Paolella and P. Polak
The resulting procedure is then: From an initial set of 300 copula samples, the ES
is evaluated, s1 is computed from (51), and if s1 > 300, an additional s1 −−300
samples are drawn.
B
The Gaussian DCC-GARCH Model
Consider a d-dimensional vector of asset returns, Yt = (Yt,1, Yt,2, . . . , Yt,d)′. The
ith univariate series, i = 1, . . . , d, is assumed to follow a GARCH(1,1) model,
which is a special case of (5). We assume an unknown mean μi, so that Yt,i −
μi = ϵt,i = Zt,iσ2
t,i, σ2
t,i = c0,i + c1,i (Yt−1,i −μi)2 + d1,iσ2
t−1,i, and Zt,i are i.i.d.
standard normal.
B.1
Estimation Using Proﬁle Likelihood for Each GARCH Margin
The DCC multivariate structure can be expressed as
Yt|t−1 ∼Nd(μ, Ht),
Ht = DtRtDt,
(52)
with μ = (μ1, . . . , μd)′, D2
t = diag([σ2
t,1, . . . , σ2
t,d]), and {Rt} the set of d × d
matrices of time varying conditional correlations with dynamics speciﬁed by
Rt = Et−1 [ϵtϵ′
t] = diag

Qt
−1/2Qtdiag

Qt
−1/2,
(53)
t = 1, . . . , T, where ϵt = D−1
t
(Yt −μ). The {Qt} form a sequence of conditional
matrices parameterized by
Qt = S (1 −a −b) + a

ϵt−1ϵ′
t−1

+ bQt−1,
(54)
with S the d × d unconditional correlation matrix (Engle 2002, p. 341) of the
ϵt, and parameters a and b are estimated via maximum likelihood conditional
on estimates of all other parameters, as discussed next. Matrices S and Q0 can
be estimated with the usual plug-in sample correlation based on the ﬁltered ϵt;
see also Bali and Engle (2010) and Engle and Kelly (2012) on estimation of the
DCC model. Observe that the resulting Qt from the update in (54) will not
necessarily be precisely a correlation matrix; this is the reason for the standard-
ization in (53). See Caporin and McAleer (2013) for several critiques of this DCC
construction; and Aielli (2013) for a modiﬁed DCC model, termed cDCC, with
potentially better small-sample properties. The CCC model is a special case of
(52), with a = b = 0 in (54).
The mean vector, μ, can be set to zero, or estimated using the sample mean
of the returns, as in Engle and Sheppard (2001) and McAleer et al. (2008),
though in a more general non-Gaussian context, is best estimated jointly with
the other parameters associated with each univariate return series; see Paolella
and Polak (2017). Let Y = [Y1, . . . , YT ]′, and denote the set of parameters as θ.
The log-likelihood of the remaining parameters, conditional on μ, is given by

COBra: Copula-Based Portfolio Optimization
71
ℓ(θ; Y, μ) = −1
2

t

d ln(2π) + ln(|Ht|) + (Yt −μ)′ H−1
t
(Yt −μ)

= −1
2

t

d ln(2π) + 2 ln(|Dt|) + ln(|Rt|) + ϵ′
tR−1
t ϵt

.
Then, as in Engle (2002), adding and subtracting ϵ′
tϵt, ℓcan be decomposed as
the sum of volatility and correlation terms, ℓ= ℓV + ℓC, where
ℓV = −1
2

t

d ln(2π)+2 ln(|Dt|)+ϵ′
tϵt

,
ℓC = −1
2

t

ln(|Rt|)+ ϵ′
tR−1
t ϵt −ϵ′
tϵt

,
so that a two-step maximum likelihood estimation procedure can be applied:
First, estimate the GARCH model parameters for each univariate returns series
and construct the standardized residuals; second, maximize the conditional like-
lihood with respect to parameters a and b in (54) based on the ﬁltered residuals
from the previous step. We now discuss this ﬁrst step in more detail.
While Francq and Zako¨ıan (2004) prove the consistency and asymptotic nor-
mality of the GARCH model parameters, interest centers on their numeric esti-
mation. Dropping the subscript i, the choice of starting values for ˆc0, ˆc1, and
ˆd1 are important, as the log-likelihood can exhibit more than one local max-
ima. This issue of multiple maxima has been noted by Ma et al. (2006), Winker
and Maringer (2009), and Paolella and Polak (2015b), though seems to be often
ignored, and can lead to inferior forecasts and jeopardize results in applied work.
This unfortunate observation might help explain the results of Brooks et al.
(2001, p. 54) in their extensive comparison of econometric software. In particu-
lar, they ﬁnd that, with respect to estimating just the simple normal GARCH
model, “the results produced using a default application of several of the most
popular econometrics packages diﬀer considerably from one another”. Another
reason for discrepant results is the choice of ϵ0 and σ0 to start the GARCH(1,1)
recursion, for which several suggestions exist in the literature. We take ˆσ2
0 to be
the sample unconditional variance of the Rt, and ˆϵ2
0 = κˆσ2
0, where
κ := E

(|Z| −gZ)δ 
(55)
depends on the density speciﬁcation fZ (·) and is stated for the more general
APARCH model (5). For Z ∼N(0, 1), a trivial calculation yields
E

(|Z| −gZ)δ 
=
1
√
2π

(1 + g)δ + (1 −g)δ
2(δ−1)/2Γ
δ + 1
2
	
.
In our case, with δ = 2 and g = 0, this reduces to κ = E

|Z|2 
= 1.
Paolella and Polak (2015b) demonstrate the phenomenon of multiple maxima
with a real (and typical) data set, and propose a solution that is simple to
implement, making use of the proﬁle log-likelihood (p.l.) obtained by ﬁxing the
value of c0, and using a grid of points of c0 between zero and 1.1 times the sample
variance of the series. That is, for a ﬁxed value of c0, we compute
θp.l.(c0) = arg max
θp.l. ℓ(θp.l.; R),
θp.l. = (c1, d1)′.
(56)

72
M. S. Paolella and P. Polak
To obtain (with high probability) the global maximum, the following procedure
suggests itself: (i) Based on a set of c0 values, compute (56); (ii) take the value of
c0 from the set, say c∗
0, and its corresponding θp.l.(c∗
0) that results in the largest
log-likelihood as starting values, to (iii) estimate the full model. The ﬁner the
grid, the higher the probability of reaching the global maximum; some trials
suggest that a grid of length 10 is adequate. The use of more parameters, as arise
with more elaborate GARCH structures such as the APARCH formulation, or
additional shape parameter(s) of a non-Gaussian distribution such as the NCT
or stable Paretian, can further exacerbate the problem of multiple local maxima
of the likelihood.
B.2
Remarks on DCC
One might argue that only two parameters for modeling the evolution of an
entire correlation matrix will not be adequate. While this is certainly true, the
models of Engle (2002) and Tse and Tsui (2002) have two strong points: First,
their use is perhaps better than no parameters (as in the CCC model), and
second, it allows for easy implementation and estimation. Generalizations of the
simple DCC structure that allow the number of parameters to be a function of
d, and also introducing asymmetric extensions of the DCC idea, are considered
in Engle (2002) and Cappiello et al. (2006), though with a potentially very large
number of parameters, the usual estimation and inferential problems arise.
Bauwens and Rombouts (2007) consider an approach in which similar series
are pooled into one of a small number of clusters, such that their GARCH para-
meters are the same within a cluster. A related idea is to group series with respect
to their correlations, generalizing the DCC model; see, e.g., Vargas (2006), Billio
et al. (2006), Zhou and Chan (2008), Billio and Caporin (2009), Engle and Kelly
(2012), So and Yip (2012), Aielli and Caporin (2013), and the references therein.
An alternative approach is to assume a Markov switching structure between
two (or more) regimes, each of which has a CCC structure, as ﬁrst proposed
in Pelletier (2006), and augmented to the non-Gaussian case in Paolella et al.
(2017). Such a construction implies many additional parameters, but their esti-
mation makes use of the usual sample correlation estimator, thus avoiding
the curse of dimensionality, and shrinkage estimation can be straightforwardly
invoked to improve performance. The idea that, for a given time segment, the
correlations are constant, and take on one set (of usually two, or at most three
sets) of values. This appears to be better than attempting to construct a model
that allows for their variation at every point in time. The latter might be “asking
too much of the data” and inundated with too many parameters. Paolella et al.
(2017) demonstrate strong out-of-sample performance of their non-Gaussian
Markov switching CCC model with two regimes, compared to the Gaussian
CCC case, the Gaussian CCC switching case, the Gaussian DCC model, and the
non-Gaussian single component CCC of Paolella and Polak (2015b).

COBra: Copula-Based Portfolio Optimization
73
References
Aas, K.: Pair-copula constructions for ﬁnancial applications: a review. Econometrics
4(4), 1–15 (2016). Article 43
Aas, K., Czado, C., Frigessi, A., Bakken, H.: Pair-Copula Constructions of Multiple
Dependence. Insur. Math. Econ. 44, 182–198 (2009)
Abdous, B., Genest, C., R´emillard, B.: Dependence Properties of Meta-Elliptical Dis-
tributions. In: Duchesne, P., R´emillard, B. (eds.) Statistical Modeling and Analysis
for Complex Data Problems. Springer Verlag, New York (2005). Chapter 1
Adcock, C.J.: Asset pricing and portfolio selection based on the multivariate extended
skew-student-t distribution. Ann. Oper. Res. 176(1), 221–234 (2010)
Adcock, C.J.: Mean-variance-skewness eﬃcient surfaces, Stein’s lemma and the mul-
tivariate extended skew-student distribution. Eur. J. Oper. Res. 234(2), 392–401
(2014)
Adcock, C.J., Eling, M., Loperﬁdo, N.: Skewed distributions in ﬁnance and actuarial
science: a preview. Eur. J. Financ. 21(13–14), 1253–1281 (2015)
Aielli, G.P.: Dynamic conditional correlation: on properties and estimation. J. Bus.
Econ. Stat. 31(3), 282–299 (2013)
Aielli, G.P., Caporin, M.: Fast clustering of GARCH processes via gaussian mixture
models. Math. Comput. Simul. 94, 205–222 (2013)
Asai, M.: Heterogeneous asymmetric dynamic conditional correlation model with stock
return and range. J. Forecast. 32(5), 469–480 (2013)
Ausin, M.C., Lopes, H.F.: Time-varying joint distribution through copulas. Comput.
Stat. Data Anal. 54, 2383–2399 (2010)
Bailey, D.H., Borwein, J.M., L´opez de Prado, M., Zhu, Q.J.: Pseudo-mathematics and
ﬁnancial charlatanism: the eﬀects of backtest overﬁtting on out-of-sample perfor-
mance. Not. Am. Math. Soc. 61(5), 458–471 (2014)
Bailey, D.H., Borwein, J.M., L´opez de Prado, M., Zhu, Q.J.: The probability of back-
test overﬁtting. J. Comput. Finan. (2016). https://papers.ssrn.com/sol3/papers.
cfm?abstract id=2840838
Bali, T.G., Engle, R.F.: The intertemporal capital asset pricing model with dynamic
conditional correlations. J. Monetary Econ. 57(4), 377–390 (2010)
Fundamental Review of the Trading Book: A Revised Market Risk Framework. Con-
sultative document, Bank for International Settlements, Basel (2013)
Bauwens, L., Rombouts, J.V.K.: Bayesian clustering of many GARCH models. Econo-
metric Rev. 26(2), 365–386 (2007)
Billio, M., Caporin, M.: A generalized dynamic conditional correlation model for port-
folio risk evaluation. Math. Comput. Simul. 79(8), 2566–2578 (2009)
Billio, M., Caporin, M., Gobbo, M.: Flexible dynamic conditional correlation multi-
variate GARCH models for asset allocation. Appl. Financ. Econ. Lett. 2(2), 123–130
(2006)
Bloomﬁeld, T., Leftwich, R., Long, J.: Portfolio strategies and performance. J. Financ.
Econ. 5, 201–218 (1977)
Bollerslev, T.: A conditional heteroskedastic time series model for speculative prices
and rates of return. Rev. Econ. Stat. 69, 542–547 (1987)
Bollerslev, T.: Modeling the coherence in short-run nominal exchange rates: a multi-
variate Generalized ARCH approach. Rev. Econ. Stat. 72, 498–505 (1990)
Broda, S.A., Haas, M., Krause, J., Paolella, M.S., Steude, S.C.: Stable mixture GARCH
models. J. Econometrics 172(2), 292–306 (2013)

74
M. S. Paolella and P. Polak
Broda, S. A., Paolella, M. S:. Expected Shortfall for Distributions in Finance. In: ˇC´ıˇzek,
P., H¨ardle, W., and Rafal W. (eds.) Statistical Tools for Finance and Insurance
(2011)
Brooks, C., Burke, S.P., Persand, G.: Benchmarks and the accuracy of GARCH model
estimation. Int. J. Forecast. 17(1), 45–56 (2001)
Brown, S. J., Hwang, I., In, F.: Why Optimal Diversiﬁcation Cannot Outperform Naive
Diversiﬁcation: Evidence from Tail Risk Exposure (2013)
B¨ucher, A., J¨aschke, S., Wied, D.: Nonparametric tests for constant tail dependence
with an application to energy and ﬁnance. J. Econometrics 1(187), 154–168 (2015)
Cambanis, S., Huang, S., Simons, G.: On the theory of elliptically contoured distribu-
tions. J. Multivar. Anal. 11(3), 368–385 (1981)
Caporin, M., McAleer, M.: Ten things you should know about the dynamic conditional
correlation representation. Econometrics 1(1), 115–126 (2013)
Cappiello, L., Engle, R.F., Sheppard, K.: Asymmetric dynamics in the correlations of
global equity and bond returns. J. Financ. Econometrics 4(4), 537–572 (2006)
Chicheportiche, R., Bouchaud, J.-P.: The joint distribution of stock returns is not
elliptical. Int. J. Theor. Appl. Financ. 15(3), 1250019 (2012)
Christoﬀersen, P., Errunza, V., Jacobs, K., Langlois, H.: Is the potential for interna-
tional diversiﬁcation disappearing? a dynamic copula approach. Rev. Financ. Stud.
25, 3711–3751 (2012)
Clare, A., O’Sullivan, N., and Sherman, M.: Benchmarking UK mutual fund perfor-
mance: the random portfolio experiment. Int. J. Financ. (2015). https://www.ucc.
ie/en/media/research/centreforinvestmentresearch/RandomPortfolios.pdf
Demarta, S., McNeil, A.J.: The t copula and related copulas. Int. Stat. Rev. 73(1),
111–129 (2005)
DeMiguel, V., Garlappi, L., Uppal, R.: Optimal versus naive diversiﬁcation: how inef-
ﬁcient is the 1/N portfolio strategy? Rev. Financ. Stud. 22(5), 1915–1953 (2009)
DeMiguel, V., Martin-Utrera, A., Nogales, F.J.: Size matters: optimal calibration of
shrinkage estimators for portfolio selection. J. Bank. Financ. 37(8), 3018–3034 (2013)
Devroye, L.: Non-Uniform Random Variate Generation. Springer Verlag, New York
(1986)
Ding, P.: On the conditional distribution of the multivariate t distribution. Am. Stat.
70(3), 293–295 (2016)
Ding, Z., Granger, C.W.J., Engle, R.F.: A long memory property of stock market
returns and a new model. J. Empir. Financ. 1(1), 83–106 (1993)
Edwards, T., Lazzara, C.J.: Equal-Weight Benchmarking: Raising the Monkey Bars.
Technical report, McGraw Hill Financial (2014)
Embrechts, P.: Copulas: a personal view. J. Risk Insur. 76, 639–650 (2009)
Embrechts, P., McNeil, A., Straumann, D.: Correlation and dependency in risk manage-
ment: properties and pitfalls. In: Dempster, M.A.H. (ed.) Risk Management: Value
at Risk and Beyond, pp. 176–223. Cambridge University Press, Cambridge (2002)
Engle, R.: Anticipating Correlations: A New Paradigm for Risk Management. Princeton
University Press, Princeton (2009)
Engle, R., Kelly, B.: Dynamic equicorrelation. J. Bus. Econ. Stat. 30(2), 212–228 (2012)
Engle, R.F.: Dynamic conditional correlation: a simple class of multivariate generalized
autoregressive conditional heteroskedasticity models. J. Bus. Econ. Stat. 20, 339–350
(2002)
Engle, R.F., Sheppard, K.: Theoretical and Empirical Properties of Dynamic Con-
ditional Correlation Multivariate GARCH. NBER Working Papers 8554, National
Bureau of Economic Research Inc (2001)

COBra: Copula-Based Portfolio Optimization
75
Fang, H.B., Fang, K.T., Kotz, S.: The meta-elliptical distribution with given marginals.
J. Multivar. Anal. 82, 1–16 (2002)
Fang, K.-T., Kotz, S., Ng, K.-W.: Symmetric Multivariate and Related Distributions.
Chapman & Hall, London (1989)
Fink, H., Klimova, Y., Czado, C., St¨ober, J.: Regime switching vine copula models for
global equity and volatility indices. Econometrics 5(1), 1–38 (2017). Article 3
Francq, C., Zako¨ıan, J.-M.: Maximum likelihood estimation of pure GARCH and
ARMA-GARCH processes. Bernoulli 10(4), 605–637 (2004)
Francq, C., Zako¨ıan, J.-M.: GARCH Models: Structure Statistical Inference and Finan-
cial Applications. John Wiley & Sons Ltd., Chichester (2010)
Gambacciani, M., Paolella, M.S.: Robust normal mixtures for ﬁnancial portfolio allo-
cation. Forthcoming. In: Econometrics and Statistics (2017)
Haas, M., Krause, J., Paolella, M.S., Steude, S.C.: Time-varying mixture GARCH
models and asymmetric volatility. North Am. J. Econ. Financ. 26, 602–623 (2013)
Haas, M., Mittnik, S., Paolella, M.S.: Mixed normal conditional heteroskedasticity. J.
Financ. Econometrics 2(2), 211–250 (2004)
He, C., Ter¨asvirta, T.: Properties of moments of a family of GARCH processes. J.
Econometrics 92(1), 173–192 (1999a)
He, C., Ter¨asvirta, T.: Statistical properties of the asymmetric power ARCH model. In:
Engle, R.F., White, H. (eds) Cointegration, Causality, and Forecasting. Festschrift
in Honour of Clive W. J. Granger, pp. 462–474. Oxford University Press (1999b)
Heyde, C.C., Kou, S.G.: On the controversy over tailweight of distributions. Oper. Res.
Lett. 32, 399–408 (2004)
Hough, J.: Monkeys are better stockpickers than you’d think. Barron’s magazine (2014)
Hurst, S.: The characteristic function of the student t distribution. Financial Math-
ematics Research Report FMRR006-95, Australian National University, Canberra
(1995). http://wwwmaths.anu.edu.au/research.reports/srr/95/044/
Jagannathan, R., Ma, T.: Risk reduction in large portfolios: why imposing the wrong
constraints helps. J. Financ. 58(4), 1651–1683 (2003)
Jondeau, E.: Asymmetry in tail dependence of equity portfolios. Computat. Stat. Data
Anal. 100, 351–368 (2016)
Jondeau, E., Rockinger, M.: Conditional volatility, skewness, and kurtosis: existence,
persistence, and comovements. J. Econ. Dyn. Control 27, 1699–1737 (2003)
Jondeau, E., Rockinger, M.: The Copula-GARCH model of conditional dependencies:
an international stock market application. J. Int. Money Financ. 25, 827–853 (2006)
Jondeau, E., Rockinger, M.: On the importance of time variability in higher moments
for asset allocation. J. Financ. Econometrics 10(1), 84–123 (2012)
Karanasos, M., Kim, J.: A re-examination of the asymmetric power ARCH model. J.
Empir. Financ. 13, 113–128 (2006)
Kelker, D.: Distribution theory of spherical distributions and a location-scale parameter
generalization. Sankhy¯a, Series A 32(4), 419–430 (1970)
Kiefer, J., Wolfowitz, J.: Consistency of the maximum likelihood estimator in the
presence of inﬁnitely many incidental parameters. Ann. Math. Stat. 27(4), 887–906
(1956)
Kogon, S.M., Williams, D.B.: Characteristic function based estimation of stable para-
meters. In: Adler, R.J., Feldman, R.E., Taqqu, M.S. (eds) A Practical Guide to
Heavy Tails, pp. 311–335. Birkhauser Boston Inc. (1998)
Krause, J., Paolella, M.S.: A fast, accurate method for value at risk and expected
shortfall. Econometrics 2, 98–122 (2014)
Kuester, K., Mittnik, S., Paolella, M.S.: Value-at-risk prediction: a comparison of alter-
native strategies. J. Financ. Econometrics 4, 53–89 (2006)

76
M. S. Paolella and P. Polak
Ling, S., McAleer, M.: Necessary and suﬃcient moment conditions for the garch(r, s)
and asymmetric power garch(r, s) models. Econometric Theor. 18(3), 722–729 (2002)
Ma, J., Nelson, C.R., Startz, R.: Spurious inference in the GARCH(1,1) model when it
is weakly identiﬁed. Stud. Nonlinear Dyn. Econometrics 11(1), 1–27 (2006). Article
1
Markowitz, H.: Portfolio Selection. J. Financ. 7(1), 77–91 (1952)
McAleer, M., Chan, F., Hoti, S., Lieberman, O.: Generalized autoregressive conditional
correlation. Econometric Theor. 24(6), 1554–1583 (2008)
McNeil, A.J., Frey, R., Embrechts, P.: Quantitative Risk Management: Concepts, Tech-
niques, and Tools. Princeton University Press, Princeton (2005)
McNeil, A.J., Frey, R., Embrechts, P.: Quantitative Risk Management: Concepts, Tech-
niques, and Tools. Princeton University Press, Princeton (2015). Revised edition
Mittnik, S., Paolella, M.S.: Prediction of ﬁnancial downside risk with heavy tailed con-
ditional distributions. In: Rachev, S.T. (ed.) Handbook of Heavy Tailed Distributions
in Finance. Elsevier Science, Amsterdam (2003)
Mittnik, S., Paolella, M.S., Rachev, S.T.: Stationarity of stable power-GARCH
processes. J. Econometrics 106, 97–107 (2002)
Nguyen, H.T.: On evidential measures of support for reasoning with integrate uncer-
tainty: a lesson from the ban of P-values in statistical inference. In: Huynh, V.-N.,
Inuiguchi, M., Le, B., Le, B.N., Denoeux, T. (eds.) 5th International Symposium on
Integrated Uncertainty in Knowledge Modeling and Decision Making IUKM 2016,
pp. 3–15. Springer, Cham (2016)
Nolan, J. P.: Stable Distributions - Models for Heavy Tailed Data. Birkh¨auser, Boston
(2015, forthcoming). Chapter 1 online
Paolella, M.S.: Intermediate Probability: A Computational Approach. John Wiley &
Sons, Chichester, West Sussex, England (2007)
Paolella, M.S.: Multivariate asset return prediction with mixture models. Eur. J.
Financ. 21, 1–39 (2013)
Paolella, M.S.: Fast methods for large-scale non-elliptical portfolio optimization. Ann.
Financ. Econ. 09(02), 1440001 (2014)
Paolella, M.S.: Stable-GARCH models for ﬁnancial returns: fast estimation and tests
for stability. Econometrics 4(2), 25 (2016). Article 25
Paolella, M.S.: The univariate collapsing method for portfolio optimization. Economet-
rics 5(2), 1–33 (2017). Article 18
Paolella, M.S., Polak, P.: ALRIGHT: Asymmetric LaRge-Scale (I)GARCH with hetero-
tails. Int. Rev. Econ. Financ. 40, 282–297 (2015a)
Paolella, M.S., Polak, P.: COMFORT: A common market factor non-gaussian returns
model. J. Econometrics 187(2), 593–605 (2015b)
Paolella, M.S., Polak, P.: Portfolio Selection with Active Risk Monitoring. Research
paper, Swiss Finance Institute (2015c)
Paolella, M.S., Polak, P.: Density and Risk Prediction with Non-Gaussian COMFORT
Models (2017). Submitted
Paolella, M.S., Polak, P., Walker, P.: A Flexible Regime-Switching Model for Asset
Returns (2017). Submitted
Patton, A.J.: A review of copula models for economic time series. J. Multivar. Anal.
110, 4–18 (2012)
Pelletier, D.: Regime switching for dynamic correlations. J. Econometrics 131, 445–473
(2006)
Righi, M.B., Ceretta, P.S.: Individual and ﬂexible expected shortfall backtesting. J.
Risk Model Valid. 7(3), 3–20 (2013)

COBra: Copula-Based Portfolio Optimization
77
Righi, M.B., Ceretta, P.S.: A comparison of expected shortfall estimation models. J.
Econ. Bus. 78, 14–47 (2015)
Samorodnitsky, G., Taqqu, M.S.: Stable Non-Gaussian Random Processes: Stochastic
Models with Inﬁnite Variance. Chapman & Hall, London (1994)
Scherer, M.: CDO pricing with nested archimedean copulas. Quant. Financ. 11, 775–
787 (2011)
Shaw, W.T.: Monte Carlo Portfolio Optimization for General Investor Risk-Return
Objectives and Arbitrary Return Distributions: a Solution for Long-only Portfolios
(2010)
So, M.K.P., Yip, I.W.H.: Multivariate GARCH models with correlation clustering. J.
Forecast. 31(5), 443–468 (2012)
Song, D.-K., Park, H.-J., Kim, H.-M.: A note on the characteristic function of multi-
variate t distribution. Commun. Stat. Appl. Methods 21(1), 81–91 (2014)
Stoyanov, S., Samorodnitsky, G., Rachev, S., Ortobelli, S.: Computing the portfolio
conditional value-at-risk in the alpha-stable case. Probab. Math. Statistics 26, 1–22
(2006)
Sutradhar, B.C.: On the characteristic function of multivariate student t-distribution.
Can. J. Stat. 14(4), 329–337 (1986)
Tse, Y.K., Tsui, A.K.C.: A multivariate generalized autoregressive conditional het-
eroscedasticity model with time-varying correlations. J. Bus. Econ. Stat. 20(3), 351–
362 (2002)
Vargas, G.A.: An asymmetric block dynamic conditional correlation multivariate
GARCH model. Philippine Stat. 55(1–2), 83–102 (2006)
Winker, P., Maringer, D.: The convergence of estimators based on heuristics: theory
and application to a GARCH model. Comput. Stat. 24(3), 533–550 (2009)
Wolf, O.L.M.: Honey, I shrunk the sample covariance matrix: problems in mean-
variance optimization. J. Portfolio Management 30(4), 110–119 (2004)
Zhou, T., Chan, L.: Clustered dynamic conditional correlation multivariate garch
model. In: Song, I.-Y., Eder, J., Nguyen, T. M. (eds) Proceedings of the 10th Inter-
national Conference Data Warehousing and Knowledge Discovery, DaWaK 2008,
Turin, Italy, 2–5 September 2008, pp. 206–216 (2008)
Zolotarev, V.M.: One Dimensional Stable Distributions (Translations of Mathemati-
cal Monograph, Vol. 65). American Mathematical Society, Providence, RI (1986).
Translated from the original Russian verion (1983)

Multiple Testing of One-Sided Hypotheses:
Combining Bonferroni and the Bootstrap
Joseph P. Romano1 and Michael Wolf2(B)
1 Departments of Economics and Statistics, Stanford University, Stanford, USA
romano@stanford.edu
2 Department of Economics, University of Zurich, Zurich, Switzerland
michael.wolf@econ.uzh.ch
Abstract. In many multiple testing problems, the individual null
hypotheses (i) concern univariate parameters and (ii) are one-sided. In
such problems, power gains can be obtained for bootstrap multiple test-
ing procedures in scenarios where some of the parameters are ‘deep in
the null’ by making certain adjustments to the null distribution under
which to resample. In this paper, we compare a Bonferroni adjustment
that is based on ﬁnite-sample considerations with certain ‘asymptotic’
adjustments previously suggested in the literature.
1
Introduction
Multiple testing refers to any situation that involves the simultaneous testing of
several hypotheses. This scenario is quite common in empirical research in just
about any ﬁeld, including economics and ﬁnance. Some examples are: one ﬁts a
multiple regression model and wishes to decide which coeﬃcients are diﬀerent
from zero; one compares several forecasting strategies to a benchmark and wishes
to decide which strategies are outperforming the benchmark; and one evaluates a
policy with respect to multiple outcomes and wishes to decide for which outcomes
the policy yields signiﬁcant eﬀects.
If one does not take the multiplicity of tests into account, then the probability
that some of the true null hypotheses will be rejected by chance alone is generally
unduly large. Take the case of S = 100 hypotheses being tested at the same time,
all of them being true, with the size and level of each test exactly equal to α.
For α = 0.05, one then expects ﬁve true hypotheses to be rejected. Furthermore,
if all test statistics are mutually independent, then the probability that at least
one true null hypothesis will be rejected is given by 1 −0.95100 ≈0.994.
The most common solution to multiple testing problems is to control the
familywise error rate (FWE), which is deﬁned as the probability of rejecting at
least one of the true null hypotheses. In other words, one uses a global error rate
that combines all tests under consideration instead of an individual error rate
that only considers one test at a time.
Controlling the FWE at a pre-speciﬁed level α corresponds to controlling the
probability of a Type I error when carrying out a single test. But this is only
one side of the testing problem — and it can be achieved trivially by rejecting
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_4

Multiple Testing of One-Sided Hypotheses
79
a particular hypothesis under test with probability α without even looking at
data. The other side of the testing problem is ‘power’, that is, the ability to
reject a false null hypothesis.
In this paper, we shall study certain adjustments to ‘null sampling distrib-
utions’ with the hope of power gains in the setting where the individual null
hypotheses (i) concern univariate parameters and (ii) are one-sided.
2
Testing Problem
Suppose data X are generated from some unknown probability mechanism P. A
model assumes that P belongs to a certain family of probability distributions,
though we make no rigid requirements for this family; it may be a parametric,
semiparametric, or nonparametric model.
We consider the following generic multiple testing problem:
Hs : θs ≤0
vs.
H′
s : θs > 0
for
s = 1, . . . , S,
(1)
where the θs
..= θs(P) are real-valued, univariate parameters and the values
under the null hypotheses are always zero without loss of generality. We also
denote θ ..= (θ1, . . . , θS)′.
Remark 1 (Arbitrary Null Parameters). Of course, in practice the values of the
parameters under the null hypotheses (“null parameters”) may not always be
zero. But this situation can easily be handled by our framework as well. To see
how, denote the ‘original’ parameters of interest by γs and consider the multiple
testing problem
Hs : γs ≤γ0,s
vs.
H′
s : γs > γ0,s
for
s = 1, . . . , S,
(2)
where the null parameters γ0,s can take on any value. In such a case, simply
deﬁne θs ..= γs −γ0,s, for s = 1, . . . , S.
⊓⊔
The familywise error rate (FWE) is deﬁned as
FWEP ..= P{Reject at least one hypothesis Hs : θs ≤0}.
The goal is to control the FWE rate at a pre-speciﬁed level α while at the same
time to achieve large ‘power’, which is loosely deﬁned as the ability to reject
false null hypotheses, that is, the ability to reject null hypotheses Hs for which
θs > 0. For example, particular notions of ‘power’ can be the following:
• The probability of rejecting at least one of the false null hypotheses
• The probability of rejecting a particular false null hypothesis
• The expected number of the false null hypotheses that will be rejected
• The probability of rejecting all false null hypotheses

80
J. P. Romano and M. Wolf
Control of the FWE means that, for a given signiﬁcance level α,
FWEP ≤α
for any P.
(3)
Control of the FWE allows one to be 1 −α conﬁdent that there are no false
discoveries among the rejected hypotheses.
Control of the FWE is generally equated with ‘ﬁnite-sample’ control: (3) is
required to hold for any given sample size n. However, such a requirement can
often only be achieved under strict parametric assumptions or for special per-
mutation set-ups. Instead, we settle for asymptotic control of the FWE:
lim sup
n→∞
FWEP ≤α
for any P.
(4)
Note here that the statement “for any P” is meant to mean any P in the
underlying assumed model for the family of distributions generating the data;
for example, often one would assume the existence of some moments.
3
Multiple Testing Procedures
We assume that individual test statistics are available of the form
Tn,s ..=
ˆθn,s
ˆσn,s
,
where ˆθn,s is an estimator of θs based on a sample of size n and ˆσn,s is a
corresponding standard error.1 We also denote ˆθn ..= (ˆθn,1, . . . , ˆθn,s)′. We further
assume that these test statistics are ‘proper’ t-statistics in the sense that Tn,s
converges in distribution to the standard normal distribution under θs = 0, for
s = 1, . . . , S.
There exists by now a sizeable number of multiple testing procedures (MTPs)
designed to control the FWE, at least asymptotically. The oldest and best-
known such procedure is the Bonferroni procedure that rejects hypothesis Hs if
ˆpn,s ≤α/S, where ˆpn,s is a p-value for Hs. Such a p-value can be obtained via
asymptotic approximations or alternatively via resampling methods; for exam-
ple, an ‘asymptotic’ p-value is obtained as ˆpn,s ..= 1−Φ(Tn,s), where Φ(·) denotes
the c.d.f. of the standard normal distribution. Although the Bonferroni proce-
dure controls the FWE asymptotically under weak regularity conditions, it is
generally suboptimal in terms of ‘power’.
There are two main avenues of increasing ‘power’ while maintaining (asymp-
totic) control of the FWE. The ﬁrst avenue, dating back to [Hol79], is to use
stepwise procedures where the threshold for rejecting hypotheses becomes less
lenient in subsequent steps in case some hypotheses have been rejected in a ﬁrst
step. The second avenue, dating back to [Whi00], at least in nonparametric set-
tings, is to take the dependence structure of the individual test statistics Tn,s
1 This means that ˆσn,s is an estimator of the standard deviation of ˆθn,s.

Multiple Testing of One-Sided Hypotheses
81
into account rather than assuming a ‘worst-case’ dependence structure as the
Bonferroni procedure does; taking this true dependence structure into account —
in the absence of strict assumptions — requires the use of resampling methods,
such as the bootstrap, subsampling, and permutation methods. [RW05] suggest
to combine both avenues, resulting in resampling-based stepwise MTPs.
We start by discussing a bootstrap-based single-step method. An ideal-
ized method would reject all Hs for which Tn,s ≥d1 where d1 is the 1 −α
quantile under the true probability mechanism P of the random variable
maxs(ˆθn,s −θs)/ˆσn,s. Naturally, the quantile d1 not only depends on the mar-
ginal distributions of the centered statistics (ˆθn,s −θs)/ˆσn,s but, crucially, also
on their dependence structure.
Since the true probability mechanism P is unknown, the idealized critical
value d1 is not available. But it can be estimated consistently under weak
regularity conditions as follows. Take ˆd1 as the 1 −α quantile under ˆPn of
maxs(ˆθ∗
n,s −ˆθn,s)/ˆσ∗
n,s. Here, ˆPn is an unrestricted estimate of P. For example, if
X = (X1, . . . , Xn) with Xi
iid
∼P, then ˆPn is typically the empirical distribution
of the Xi. Furthermore, ˆθ∗
n,s is the estimator of ˆθs and ˆσ∗
n,s is the corresponding
standard error, both computed from X∗where X∗∼ˆPn. In other words, we
use the bootstrap to estimate d1. The particular choice of ˆPn depends on the
situation. In particular, if the data are collected over time a suitable time series
bootstrap needs to be employed; for example, see [DH97,Lah03].
We have thus described a single-step MTP. However, a stepwise improve-
ment is possible.2 In any given step j, one simply discards the hypotheses that
have been rejected so far and applies the single-step MTP to the remaining
universe of non-rejected hypotheses. The resulting critical value ˆdj necessarily
satisﬁes ˆdj ≤ˆdj−1, and typically satisﬁes ˆdj < ˆdj−1, so that new rejections may
result; otherwise the method stops with no further rejections.
This bootstrap stepwise MTP provides asymptotic control of the FWE under
remarkably weak regularity conditions. Mainly, it is suﬃcient that (i) √n(ˆθn−θ)
converges in distribution to a (multivariate) continuous limit distribution and
that the bootstrap consistently estimates this limit distribution; and that (ii)
the ‘scaled’ standard errors √nˆσn,s and √nˆσ∗
n,s converge to the same, non-zero
limiting values in probability, both in the ‘real world’ and in the ‘bootstrap
world’. Under even weaker regularity conditions, a subsampling approach could
be used instead; see [RW05]. Furthermore, when a randomization setup applies,
randomization methods can be used as an alternative; see [RW05] again.
4
Adjustments for Power Gains
As stated before, the bootstrap stepwise MTP of the previous section provides
asymptotic control of the FWE under weak regularity conditions. But in the
one-sided setting (1) considered in this paper, it might be possible to obtain
2 More precisely, the improvement is a stepdown method.

82
J. P. Romano and M. Wolf
further power gains by making adjustments for null hypotheses that are ‘deep
in the null’, an idea going back to [Han05].
To motivate such an idea, it is helpful to ﬁrst point out that for many parame-
ters of interest, θ, there is a one-to-one relation between the bootstrap stepwise
MTP of the previous section, which is based on an unrestricted estimate ˆPn of P,
and a bootstrap stepwise MTP that is based on a restricted estimate ˆP0,n of P,
satisfying the constraints of the S null hypotheses. In the latter approach the
critical value ˆd1 in the ﬁrst step is obtained as the 1 −α quantile under ˆP0,n of
maxs ˆθ∗
n,s/ˆσ∗
n,s. Here, ˆθ∗
n,s is the estimator of θs and ˆσ∗
n,s is the corresponding
standard error, both computed from X∗where X∗∼ˆP0,n. Note that in this lat-
ter approach, there is no (explicit) centering in the numerator of the bootstrap
test statistics, since the centering already takes place implicitly in the restricted
estimator ˆP0,n by incorporating the constraints of the null hypotheses.
For many parameters of interest, the unrestricted bootstrap stepwise MTP
of the previous section is equivalent to the restricted bootstrap stepwise MTP
of the previous paragraph based on an estimator ˆP0,n that satisﬁes θs(ˆP0,n) = 0
for s = 1, . . . , S. In statistical lingo, such a null parameter θ(ˆP0,n) corresponds
to a least favorable conﬁguration (LFC), since all the components θs(ˆP0,n) lie on
the boundary of the respective null hypotheses Hs.
Remark 2 (Example: Testing Means). To provide a speciﬁc example of a null-
restricted estimator ˆP0,n, consider the setting where X = (X1, . . . , Xn) with
Xi
iid
∼P, Xi ∈RS, and (θ1, . . . , θS)′ = θ ..= E(Xi). Then an unrestricted estima-
tor ˆPn is given by the empirical distribution of the Xi whereas a null-restricted
estimator ˆP0,n is given by the empirical distribution of the Xi −ˆθn, where ˆθn
is the sample average of the Xi. In other words, ˆP0,n is obtained by suitably
shifting ˆPn to achieve mean zero for all components.
⊓⊔
[Han05] argues that such an approach is overly conservative when some of
the θs lie ‘deep in the null’, that is, for θs ≪0. Indeed, it can easily be shown
that asymptotic control of the FWE based on the restricted bootstrap stepwise
MTP could be achieved based on an infeasible ‘estimator’ ˆP0,n that satisﬁes
θs(ˆP0,n) = min{θs, 0}.
(We use the term ‘estimator’ here, since such an ˆP0,n is infeasible in practice
because one does not know the true values θs.) Clearly, when some of the θs
are smaller than zero, one would obtain smaller critical values ˆdj in this way
compared to using the LFC.
The idea then is to adjust ˆP0,n in a feasible, data-dependent fashion such
that θs(ˆP0,n) < 0 for all θs ‘deep in the null’.

Multiple Testing of One-Sided Hypotheses
83
4.1
Asymptotic Adjustments
Based on the law of iterated logarithm, [Han05] proposes an adjustment ˆPA
0,n
that satisﬁes
θs(ˆPA
0,n) ..= ˆθn,s1{Tn,s<−√2 log log n},
(5)
where 1{·} denotes the indicator function of a set. Therefore, if the t-statistic
Tn,s is suﬃciently small, the parameter of the restricted bootstrap distribution
is adjusted to the sample-based estimator ˆθs, and otherwise it is left unchanged
at zero. How one can construct such an estimator ˆPA
0,n depends on the particular
application. In the example of Remark 2, say, ˆPA
0,n can be constructed by suitably
shifting the empirical distribution ˆPn.
[Han05] only considers a bootstrap single-step MTP. [HHK10] propose the
same adjustment (5) in the context of a bootstrap stepwise MTP in the spirit
of [RW05].
The adjustment (5) is of asymptotic nature, since one does not have to pay
any ‘penalty’ in the proposals of [Han05,HHK10]. In other words, the MTP
procedure proceeds as if θs(ˆPA
0,n) = θs in case θs(ˆPA
0,n) has been adjusted to
ˆθn,s < 0. The point here is that in ﬁnite samples, it may happen that Tn,s <
−√2 log log n even though θs ≥0 in reality; in such cases, the null distribution
ˆP0,n is generally too ‘optimistic’ and results in critical values ˆdj that are too
small. As a consequence, control of the FWE in ﬁnite samples will be negatively
aﬀected.
Also note that the cutoﬀ−√2 log log n is actually quite arbitrary and could
be replaced by any multiple of it, however big or small, without aﬀecting the
asymptotic validity of the method.
Remark 3 (Related Problem: Testing Moment Inequalities). The literature on
moment inequalities is concerned with the related testing problem
H : θs ≤0
for all s
vs.
H′ : θs > 0
for at least one s.
(6)
This is not a multiple testing problem but the multivariate hypothesis H, which
is a single hypothesis, also involves an S-dimensional parameter θ and is one-
sided in nature. For this testing problem, [AS10] suggest an adjustment to ˆP0,n
that is of asymptotic nature and corresponds to the adjustment of [Han05] for
testing problem (1). But then, in a follow-up paper, [AB12] propose an alterna-
tive method based on ﬁnite-sample considerations that incorporates an explicit
‘penalty’ for making adjustments to the LFC. The proposal of [AB12] is com-
putationally quite complex and also lacks a rigorous proof of validity. [RSW14]
suggest a Bonferroni adjustment as an alternative, which is simpler to implement
and also comes with a rigorous proof of validity.
⊓⊔
4.2
Bonferroni Adjustments
We now ‘translate’ the Bonferroni adjustment of [RSW14] for testing problem (6)
to the multiple testing problem (1).

84
J. P. Romano and M. Wolf
In the ﬁrst step, we adjust ˆP0,n based on a nominal 1 −β upper rectangular
joint conﬁdence region for θ of the form
(−∞, ˆθn,1 + ˆc ˆσn,1] × · · · × (−∞, θn,S + ˆc ˆσn,S].
(7)
Here, 0 < β < α and ˆc is a bootstrap-based estimator of the 1 −β quantile of
the sampling distribution of the statistic
max
s
θs −ˆθn,s
ˆσn,s
.
For notational compactness, denote the upper end of a generic joint conﬁdence
interval in (7) by
ˆun,s ..= ˆθn,s + ˆc ˆσn,s.
(8)
Then we propose an adjustment ˆPB
0,n that satisﬁes
θs(ˆPB
0,n) ..= min{ˆun,s, 0}.
(9)
How one can construct such an estimator ˆPB
0,n depends on the particular appli-
cation. In the example of Remark 2, say, ˆPB
0,n can be constructed by suitably
shifting the empirical distribution ˆPn.
In the second step, the restricted bootstrap stepwise MTP (i) uses θ(ˆPB
0,n)
deﬁned by (9) and (ii) is carried out at nominal level α −β as opposed to at
nominal level α. Feature (ii) is a ﬁnite-sample ‘penalty’ that accounts for the fact
that with probability β, the true θ will not be contained in the joint conﬁdence
region (7) in the ﬁrst step and, consequently, the adjustment in (i) will be overly
optimistic.
As reasonable ‘generic’ choice for β is β ..= α/10, as per the suggestion of
[RSW14].
It is clear that the Bonferroni adjustment is necessarily less powerful com-
pared to the asymptotic adjustment for two reasons. First, typically θs(ˆPA
0,n) ≤
θs(ˆPB
0,n) for all s = 1, . . . , S. Second, the asymptotic adjustment uses the full
nominal level α in the stepwise MTP whereas the Bonferroni adjustment only
uses the reduced level α −β. On the other hand, it can be expected that the
asymptotic adjustments will be liberal in terms of the ﬁnite-sample control of
the FWE in some scenarios.
4.3
Adjustments for Unrestricted Bootstrap MTPs
We have detailed the asymptotic and Bonferroni adjustments in the context of
the restricted bootstrap stepwise MTPs, since they are conceptually somewhat
easier to understand.
But needless to say, these adjustments carry over one-to-one to the unre-
stricted bootstrap stepwise MTPs of [RW05].

Multiple Testing of One-Sided Hypotheses
85
Focusing on the ﬁrst step to be speciﬁc, the asymptotic adjustment takes ˆd1 as
the 1−α quantile under ˆPn of maxs(ˆθ∗
n,s −ˆθA
n,s)/ˆσ∗
n,s. Here, ˆPn is an unrestricted
estimator of P and
ˆθA
n,s
..=
ˆθn,s
if Tn,s < −√2 log log n
0
otherwise
Furthermore, ˆθ∗
n,s and ˆσ∗
n,s are the estimator of θs and the corresponding stan-
dard error, respectively, computed from X∗, where X∗∼ˆPn.
On the other hand, the Bonferroni adjustment takes ˆd1 as the 1 −α + β
quantile under ˆPn of maxs(ˆθ∗
n,s−ˆθB
n,s)/ˆσ∗
n,s. Here, ˆPn is an unrestricted estimator
of P and
ˆθB
n,s
..= ˆθn,s −min{ˆun,s, 0},
(10)
with ˆun,s deﬁned as in (8). Furthermore, ˆθ∗
n,s and ˆσ∗
n,s are the estimator of θs
and the corresponding standard error, respectively, computed from X∗where
X∗∼ˆPn.
The computation of the critical constants ˆdj in subsequent steps j > 1 is
analogous for both adjustments.
Remark 4 (Single Adjustment versus Multiple Adjustments).
In principle, the
Bonferroni adjustments (10) could be updated in each step of the bootstrap step-
wise MTP by updating the joint conﬁdence region for the remaining part of θ
in each step, that is, for the elements θs of θ for which the corresponding null
hypotheses Hs have not been rejected in previous steps. This approach can be
expected to lead to small further power gains, though at additional computa-
tional (and software coding) costs.
⊓⊔
5
The Gaussian Problem
5.1
Single-Step Method
In this section, we derive an exact ﬁnite-sample result for the multivariate
Gaussian model, which motivates the method proposed in the paper. Assume
that W ..= (W1, . . . , WS)′ ∼P ∈P ..= {N(θ, Σ) : μ ∈RS} for a known covari-
ance matrix Σ. The multiple testing problem consists of S one-sided hypotheses
Hs : θs ≤0
vs.
H′
s : θs > 0
for
s = 1, . . . , S.
(11)
The goal is to control the FWE exactly at nominal level α in this model,
for any possible choice of the θs, for some pre-speciﬁed value of α ∈(0, 1).
Note further that, because Σ is assumed known, we may assume without loss
of generality that its diagonal consists of ones; otherwise, we can simply replace
Ws by Ws divided by its standard deviation. This limiting model applies to
the nonparametric problem in the large-sample case, since standardized sample

86
J. P. Romano and M. Wolf
means are asymptotically multivariate Gaussian with a covariance matrix that
can be estimated consistently.
First, if instead of the multiple testing problem, we were interested in the
single multivariate joint hypothesis that all θs satisfy θs ≤0, then we are in the
moment inequalities problem; see Remark 3. For such a problem, there are, of
course, many ways in which to construct a test that controls size at level α. For
instance, given any test statistic T ..= T(W1, . . . , WS) that is nondecreasing in
each of its arguments, we may consider a test that rejects H for large values
of T. Note that, for any given ﬁxed critical value c, Pθ{T(W1, . . . , WS) > c} is
a nondecreasing function of each component θs in θ. Therefore, if c ..= c1−α is
chosen to satisfy
P0

T(W1, . . . , WS) > c1−α

≤α,
then the test that rejects H0 when T > c1−α is a level α test. A reasonable
choice of test statistic T is the likelihood ratio statistic or the maximum statistic
max(W1, . . . , WS). For this latter choice of test statistic, c1−α may be determined
as the 1−α quantile of the distribution of max(W1, . . . , WS) when (W1, . . . , WS)′
is multivariate normal with mean 0 and covariance matrix Σ. Unfortunately, as
S increases, so does the critical value, which can make it diﬃcult to have any
reasonable power against alternatives. The same issue occurs in multiple testing,
as described below. The main idea of our procedure is to essentially remove
from consideration those θs that are ‘negative’.3 If we can eliminate such θs
from consideration, then we may use a smaller critical value with the hope of
increased power against alternatives.
In the multiple testing problem using the max statistic, one could simply
reject any θs for which Xs > c1−α. But as in the single testing problem above,
c1−α increases with S and therefore it may be helpful to make certain adjust-
ments if one is fairly conﬁdent that a hypothesis Hs satisﬁes θs < 0. Using this
reasoning as a motivation, we may use a conﬁdence region to help determine
which θs are ‘negative’. To this end, let M(1 −β) denote an upper rectangular
joint conﬁdence region for θ at level 1 −β. Speciﬁcally, let
M(1 −β) ..=

θ ∈RS :
max
1≤s≤S(θs −Ws) ≤K−1(1 −β)

(12)
=

θ ∈RS : θs ≤Ws + K−1(1 −β) for all 1 ≤s ≤S

,
where K−1(1 −β) is the 1 −β quantile of the distribution (function)
K(x) ..= Pθ

max
1≤s≤S(θs −Ws) ≤x

.
Note that K(·) depends only on the dimension S and the underlying covariance
matrix Σ. In particular, it does not depend on the θs, so it can be computed
under the assumption that all θs = 0. By construction, we have for any θ ∈RS,
that
Pθ{θ ∈M(1 −β)} = 1 −β.
3 Such a program is carried out in the moment inequality problem by [RSW14].

Multiple Testing of One-Sided Hypotheses
87
The idea now is that with probability at least 1 −β, we may assume that
θ will lie in Ω0 ∩M(1 −β) rather than just in Ω0, where Ω0 is the ‘negative
quadrant’ given by {θ : θs ≤0, s = 1, . . . , S}. Instead of computing the critical
value under θ = 0, the ‘largest’ value of θ in Ω0 (or the value under the LFC),
we may therefore compute the critical value under ˜θ, the ‘largest’ value of θ in
the (data-dependent) set Ω0 ∩M(1 −β). It is straightforward to determine ˜θ
explicitly because of the simple shape of the joint conﬁdence region for θ. In
particular, ˜θ has sth component equal to
˜θs ..= min{Ws + K−1(1 −β), 0}.
(13)
But, to account for the fact that θ may not lie in M(1−β) with probability β, we
reject any Hs for which Ws exceeds the 1 −α + β quantile of the distribution of
T ..= max(W1, . . . , WS) under ˜θ rather than the 1−α quantile of the distribution
of T under ˜θ. The following result establishes that this procedure controls the
FWE at level α.
Theorem 1. Let T ..= max(W1, . . . , WS). For θ ∈RS and γ ∈(0, 1), deﬁne
b(γ, θ) ..= inf{x ∈R : Pθ{T(W1, . . . , Wk) ≤x} ≥γ},
that is, as the γ quantile of the distribution of T under θ. Fix 0 < β < α. The
multiple testing procedure that rejects any Hs for which Ws > b(1 −α + β, ˜θ)
controls the FWE at level α.
Remark 5. As emphasized above, an attractive feature of the procedure is that
the ‘largest’ value of θ in Ω0 ∩M(1 −β) may be determined explicitly. This
follows from our particular choice of the initial joint conﬁdence region for θ. If,
for example, we had instead chosen M(1 −β) to be the usual Scheﬀ´e conﬁdence
ellipsoid, then there may not even be a ‘largest’ value of θ in Ω0 ∩M(1 −β). ⊓⊔
Proof of Theorem 1. First note that b(γ, θ) is nondecreasing in θ, since T is
nondecreasing in its arguments. Fix any θ. Let I0 ..= I0(θ) denote the indices of
true null hypotheses, that is,
I0 ..= {s : θs ≤0}.
Let θ∗
s
..= min(θs, 0) and let E be the event that θ ∈M(1 −β). Then, the
familywise error rate (FWE) satisﬁes
Pθ{reject any true Hs} ≤Pθ

Ec
+ Pθ{E ∩{reject any Hs with s ∈I0}}
= β + Pθ{E ∩{reject any Hs with s ∈I0}}.
But when the event E occurs and some true Hs is rejected — so that
maxs∈I0 Ws > b(1 −α + β, ˜θ) — then the event maxs∈I0 Ws > b(1 −α + β, θ∗)

88
J. P. Romano and M. Wolf
must occur, since b(1 −α + β, θ) is nondecreasing in θ and θ ≤˜θ when E occurs.
Hence, the FWE is bounded above by
β + Pθ

max
s∈I0 Ws > b(1 −α + β, θ∗)

≤β + Pθ∗
max
s∈I0 Ws > b(1 −α + β, θ∗)

because the distribution of maxs∈I0 Ws only depends on those θs in I0. Therefore,
the last expression is bounded above by
β + Pθ∗
max
all s Ws > b(1 −α + β, θ∗)

= β + 1 −(1 −α + β) = β + (α −β) = α.
⊓⊔
5.2
Stepwise Method
One can improve upon the single-step method in Theorem 1 by a stepwise
method.4 More speciﬁcally, consider the following method. Begin with the
method described above, which rejects any Hs for which Ws > b(1 −α + β, ˜θ).
Basically, one applies the closure method to the above and show that it may
be computed in a stepwise fashion. To do this, we ﬁrst need to describe the
situation when testing only a subset of the hypotheses. So, let I denote any
subset of {1, . . . , S} and let bI(γ, θ) denote the γ quantile of the distribution of
max(Ts : s ∈I) under θ. Also, let ˜θ(I) ..= {˜θs(I) : s ∈I} with ˜θs(I) be deﬁned
as in (13) except that K−1(1 −β) is replaced by K−1
I (1 −β), deﬁned to be the
1 −β quantile of the distribution (function)
KI(x) ..= Pθ{max
s∈I (θs −Ws) ≤x}.
The stepwise method can now be described. Begin by testing all Hs with
s ∈{1, . . . , S} as described in the single-step method. If there are any rejections,
remove the rejected hypotheses from consideration and apply the single-step
method to the remaining hypotheses. That is, if I is the set of indices of the
remaining hypotheses not previously rejected, then reject any such Hs if Ws >
bI(1 −α + γ, ˜θ(I)). And so on. (Note that at each step of the procedure, a new
joint conﬁdence region is computed to determine ˜θ(I), but β remains the same
in each step.)
Theorem 2. Under the Gaussian setup of Theorem 1, the above stepwise
method controls the FWE at level α.
Proof of Theorem 2. We just need to show that the closure method applied
to the above tests results in the stepwise method as described. To do this, it
suﬃces to show that if Hs is rejected by the stepwise method, s ∈I, and I ⊂J,
then when J is tested (meaning the Hs with s ∈J are jointly tested) and
the method rejects the joint (intersection) hypothesis, then it also rejects the
particular joint (intersection) hypothesis when just I is tested.
4 More precisely, the improvement is a stepdown method.

Multiple Testing of One-Sided Hypotheses
89
First, the distribution of maxθs∈I Ws is stochastically dominated by that of
maxθs∈J Ws (since we are just taking the max over a larger set), under any θ
and in particular under ˜θ(I). But the distribution of the maximum statistic
maxθs∈J Ws is monotone increasing with respect to θs because of the important
fact that, component wise,
˜θ(I) ≤˜θ(J).
Hence, the distribution of maxθs∈J under ˜θ(I) is further dominated by the dis-
tribution of maxθs∈J Ws under ˜θ(J). Therefore, the critical values satisfy
bI(1 −α + β, ˜θ(I)) ≤bJ(1 −α + β, ˜θ(J)),
which is all we need to show, since then any Hs for which Ws exceeds bJ(1 −
α + β, ˜θ(J)) will satisfy that Ws also exceeds bI(1 −α + β, ˜θ(I)).
⊓⊔
6
Monte Carlo Simulations
The data are of the form X ..= (X1, X2, . . . , Xn) with Xi
iid
∼N(θ, Σ), θ ∈RS,
and Σ ∈RS×S. We consider n = 50, 100.
For n = 50, we consider S = 25, 50, 100 and the following mean vectors
θ = (θ1, . . . , θS)′:
• All θs = 0
• Five of the θs = 0.4
• Five of the θs = 0.4 and S/2 of the θs = −0.4
• Five of the θs = 0.4 and S/2 of the θs = −0.8
For n = 100, we consider S = 50, 100, 200 and the following mean vectors
θ = (θ1, . . . , θS)′:
• All θs = 0
• Ten of the θs = 0.3
• Ten of the θs = 0.3 and S/2 of the θs = −0.3
• Ten of the θs = 0.3 and S/2 of the θs = −0.6
For S = 50, 100, the covariance matrix Σ is always a constant-correlation
matrix with constant variance one on the diagonal and constant covariance ρ = 0,
0.5 on the oﬀ-diagonal.
The test statistics Tn,s are the usual t-statistics based on the individual
sample means and sample standard deviations.
The multiple testing procedure is always the bootstrap stepwise MTP of
[RW05] and we consider three variants:
• LFC: No adjustment at all
• Asy: Asymptotic Adjustment
• Bon: Bonferroni Adjustment

90
J. P. Romano and M. Wolf
Note that for computational simplicity, Bon is based on a single adjustment
throughout the stepwise MTP; see Remark 4.
The nominal level for FWE control is α = 10% and the value of β for the
Bonferroni adjustment is chosen as β = 1% following the ‘generic’ suggestion
β ..= α/10 of [RSW14].
We consider two performance measures:
• FWE: Empirical FWE
• Power: Average number of rejected false hypotheses
The number of Monte Carlo repetitions is B = 50, 000 in each scenario and the
bootstrap `a la [Efr79] is based on 1,000 resamples always.
The results for n = 50 are presented in Sect. A.1 and the results for n = 100
are presented in Sect. A.2. They can be summarized as follows.
• As pointed out before, Asy is always more powerful than Bon necessarily.
• There are some scenarios where Asy fails to control the FWE, though the
failures are never grave: In the worst case, the empirical FWE is 10.6%.
• Bon can actually be less powerful than LFC (though never by much). This
is not surprising: When null parameters are on the boundary or close to the
boundary, then the ‘minor’ adjustment in the ﬁrst stage of Bon does not oﬀset
the reduction in the nominal level (from α to α −β) in the second stage.
• When null parameters are ‘deep in the null’, also the power gains of Bon over
LFC are noticeable (though never quite as large as the power gains of Asy
over LFC). Of course, such power gains would even be greater by increasing
the proportion of null parameters ‘deep in the null’ and/or the distance away
from zero of such null parameters.
7
Conclusion
In many multiple testing problems, the individual null hypotheses (i) concern
univariate parameters and (ii) are one-sided. In such problems, power gains can
be obtained for bootstrap multiple testing procedures in scenarios where some
of the parameters are ‘deep in the null’ by making certain adjustment to the
null distribution under which to resample. In this paper we have compared a
Bonferroni adjustment that is based on ﬁnite-sample considerations to certain
‘asymptotic’ adjustments previously suggested in the literature. The advantage
of the Bonferroni adjustment is that it guarantees better ﬁnite-sample control
of the familywise error rate. The disadvantage is that it is always somewhat less
powerful than the asymptotic adjustments.
A
Detailed Monte Carlo Results
A.1
Results for n = 50
See Tables 1, 2, 3 and 4.

Multiple Testing of One-Sided Hypotheses
91
Table 1. All θs = 0: FWE.
S
LFC Asy
Bon
ρ = 0
25
9.8
10.5 8.8
50
9.7
10.2 8.7
100
9.5
10.1 8.5
ρ = 0.5
25
10.0
10.0 9.0
50
9.8
9.8 8.8
100
9.9
9.9 8.9
Table 2. Five of the θs = 0.4: FWE | Power.
S
LFC Asy
Bon LFC Asy Bon
ρ = 0
25
8.9
9.4 7.9
2.7
2.8
2.6
50
9.1
9.6 8.2
2.2
2.2
2.1
100
9.4
10.0 8.4
1.7
1.8
1.6
ρ = 0.5
25
9.9
9.9 8.9
3.2
3.2
3.1
50
9.8
9.8 8.8
2.8
2.8
2.7
100
10.0
10.1 9.2
2.4
2.4
2.3
Table 3. Five of the θs = 0.4 and S/2 of the θs = −0.4: FWE | Power.
S
LFC Asy Bon LFC Asy Bon
ρ = 0
25
3.7
7.7
3.6
2.7
3.3
2.7
50
4.2
8.2
4.0
2.2
2.7
2.2
100
4.7
8.8
4.4
1.7
2.1
1.7
ρ = 0.5
25
5.3
7.6
4.7
3.1
3.6
3.2
50
5.9
8.0
5.3
2.8
3.2
2.7
100
6.6
8.4
5.9
2.4
2.8
2.3

92
J. P. Romano and M. Wolf
Table 4. Five of the θs = 0.4 and S/2 of the θs = −0.8: FWE | Power.
S
LFC Asy
Bon LFC Asy Bon
ρ = 0
25
3.7
8.8 6.9
2.7
3.4
3.2
50
4.2
9.3 7.1
2.2
2.8
2.6
100
4.7
9.8 7.4
1.7
2.2
2.0
ρ = 0.5
25
5.3
9.8 7.8
3.1
3.7
3.5
50
5.9
9.8 7.7
2.8
3.2
3.1
100
6.6
10.0 7.8
2.4
2.8
2.6
A.2
Results for n = 100
See Tables 5, 6, 7 and 8.
Table 5. All θs = 0: FWE.
S
LFC Asy
Bon
ρ = 0
50
9.8
10.2 8.8
100
10.0
10.4 8.9
200
10.0
10.6 8.9
ρ = 0.5
50
10.0
10.0 9.0
100
10.1
10.1 9.0
200
10.1
10.1 9.1
Table 6. Ten of the θs = 0.3: FWE | Power.
S
LFC Asy
Bon LFC Asy Bon
ρ = 0
50
8.8
9.1 7.9
5.4
5.5
5.3
100
9.5
9.8 8.5
4.5
4.5
4.3
200
9.7
10.2 8.7
3.6
3.7
3.5
ρ = 0.5
50
9.9
9.9 8.9
6.5
6.5
6.3
100
10.0
10.0 9.0
5.8
5.8
5.6
200
10.1
10.1 9.1
5.1
5.1
4.9

Multiple Testing of One-Sided Hypotheses
93
Table 7. Ten of the θs = 0.3 and S/2 of the θs = −0.3: FWE | Power.
S
LFC Asy Bon LFC Asy Bon
ρ = 0
50
3.3
7.3
3.4
5.4
6.4
5.4
100
4.3
8.4
4.1
4.5
5.3
4.4
200
4.7
8.9
4.4
3.6
4.4
3.6
ρ = 0.5
50
5.3
7.7
4.7
6.5
7.3
6.5
100
6.2
8.4
5.6
5.8
6.5
5.7
200
6.9
8.8
6.1
5.1
5.7
5.0
Table 8. Ten of the θs = 0.3 and S/2 of the θs = −0.6: FWE | Power.
S
LFC Asy
Bon LFC Asy Bon
ρ = 0
50
3.4
8.4 6.9
5.4
6.6
6.4
100
4.3
9.4 7.7
4.5
5.5
5.2
200
4.7
9.9 7.9
3.6
4.5
4.3
ρ = 0.5
50
5.3
9.9 8.3
6.5
7.4
7.1
100
6.2
10.0 8.4
5.8
6.5
6.3
200
6.9
10.2 8.6
5.1
5.9
5.6
References
[AB12] Andrews, D.W.K., Barwick, P.J.: Inference for parameters deﬁned by
moment inequalities: a recommended moment selection procedure. Econo-
metrica 80(6), 2805–2826 (2012)
[AS10] Andrews, D.W.K., Soares, G.: Inference for parameters deﬁned by moment
inequalities using generalized moment selection. Econometrica 78(1), 119–
157 (2010)
[DH97] Davison, A.C., Hinkley, D.V.: Bootstrap Methods and their Application.
Cambridge University Press, Cambridge (1997)
[Efr79] Efron, B.: Bootstrap methods: another look at the jackknife. Ann. Stat. 7,
1–26 (1979)
[Han05] Hansen, P.R.: A test for superior predictive ability. J. Bus. Econ. Stat. 23,
365–380 (2005)
[HHK10] Hsu, P.-H., Hsu, Y.-C., Kuan, C.-M.: Testing the predictive ability of tech-
nical analysis using a new stepwise test with data snooping bias. J. Empir.
Finance 17, 471–484 (2010)
[Hol79] Holm, S.: A simple sequentially rejective multiple test procedure. Scand. J.
Stat. 6, 65–70 (1979)

94
J. P. Romano and M. Wolf
[Lah03] Lahiri, S.N.: Resampling Methods for Dependent Data. Springer, New York
(2003)
[RSW14] Romano, J.P., Shaikh, A.M., Wolf, M.: A practical two-step method for
testing moment inequalities. Econometrica 82(5), 1979–2002 (2014)
[RW05] Romano, J.P., Wolf, M.: Exact and approximate stepdown methods for mul-
tiple hypothesis testing. J. Am. Stat. Assoc. 100(469), 94–108 (2005)
[Whi00] White, H.L.: A reality check for data snooping. Econometrica 68(5), 1097–
1126 (2000)

Exploring Message Correlation in Crowd-Based
Data Using Hyper Coordinates Visualization
Technique
Tien-Dung Cao(B), Dinh-Quyen Nguyen, and Hien Duy Tran
School of Engineering, Tan Tao University, Duc Hoa, Long An, Vietnam
dung.cao@ttu.edu.vn
Abstract. Analytical exploration for necessary information and insights
from heterogeneous and multivariate dataset is challenging in visual ana-
lytics research due to the complexity of data and tasks. One of the data
analytics target is to examine the relationship in the dataset, such as
considering how the data elements and subsets are connected together.
This work takes into account the direct and indirect connection rela-
tions: elements and subsets of elements might not only be directly linked
together, but also possibly be indirectly associated via the relationships
from other elements/subsets as well. Stream of messages instantly put on
the cyberspace from the crowd is an example for such kind of dataset. In
this paper, we present an approach to estimate the correlation between
streaming messages collection in terms of large scale data processing,
whilst the Hyper Coordinates visualization technique is designed to sup-
port those correlations exploration. The prototype tool is built to demon-
strate the concepts for crowd-based data in the ﬁnancial market domain.
Keywords: Hyper Coordinates · Multivariate data visualization
Message correlation · Direct/indirect relationship
1
Introduction
In this age of big data and internet of things, more than ever before, data are
everywhere thanks to the advances in ubiquitous devices, networking and data
management. This ﬂushes a wealth of valuable information that can be exploited
in many application scenarios from everyday personal lives, to businesses, and
to governments. But because big data are normally heterogeneous, from vari-
ous kinds of sources, and typically contains diﬀerent types of information, the
task of identifying and extracting valuable information necessary for a speciﬁc
usage scenario is complicated and challenging. With complex and multivariate
datasets, source-origin big data are usually chaotic and unwell-modeled for a
speciﬁc usage. Therefore, they must be normally cleansed (removed the dupli-
cate and similarity or unnecessary information), aggregated or augmented with
expertise knowledge, and transformed into well-modeled structures for further
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_5

96
T.-D. Cao et al.
analyses. The advances of today machine learning and artiﬁcial intelligence make
those steps more realistic but not always fulﬁlled. In many cases, analysts can
be overloaded in the exploration and examination process for data selection and
transformation. To deal with it, visualization supports in terms of visual analyt-
ics have been being taken into account [27].
One of the subjects for visual analytics research on multivariate data visu-
alization is to investigate the relationships of data in the datasets. There are
various types of relationships that can be taken into consideration, including set
relations – data elements of categories, connection relations – linking pairs of
data elements or sets, ordered relations – such as the orders of connections that
form the relations, quantitative relations – quantitative information of elements
in a set or connection, and spatially explicit relations – such as positions on
geographic maps [7]. Except the spatially explicit relations which are geometri-
cally constrained, all the remaining just-listed relationships are representable in
the Hyper Word Clouds visualization technique [20]. It is proved as those types
of relationships are relatively constituted to each others. Based on that tech-
nique, this paper additionally takes the direct and indirect aspects of connection
relations into account.
We investigate on that visual analytics problem with regard to the scenario
of ﬁnancial market domain. In the domain of ﬁnancial markets, understand-
ing the market inﬂuences from big data so that to predict the market trends
is an expectation from the business investors. The software company Sentiﬁ,1
in our showcase, investigates big data from the crowd to develop products for
global ﬁnancial markets, i.e., identifying and ranking the market inﬂuences and
predicting market trends. Sentiﬁhas developed a tool for cleansing and aug-
menting the crowd data around messages (pieces of text posted or discussed on
the cyberspace, mainly from Twitter). As mentioned above, the processed data
may still contain duplicated, similarity, or associated data. And thus, analysts
at Sentiﬁhave to analyze the relationships in the data to cleanse and structur-
ize the dataset. To that end, we develop to support Sentiﬁin spotting out how
messages streamed from the crowd are correlated together, considered as the
problem in aggregating and selecting important messages in clusters (see more
in Sect. 2).
Although this also concerns a classic problem in data mining, clustering mul-
tivariate and real-time updated big data is still the challenge due to the com-
plexity of data as well as the limitation of automatic analytical solutions. Several
approaches have been proposed to examine the correlation of messages to either
reduce the duplicate/similarity information [21,31] or connect messages in a log
ﬁle into a business process that allows users to eﬀectively navigate through the
process space and identify the ones of interest [19,22,25]. Ye et al. [34] also intro-
duced a data correlation for a stream based similarity clustering. However, it is
challenging because we here care about the correlation of message that does not
come only from direct relationship among messages – i.e., two messages share a
common content –, but also from indirect relationship as well. It means that
1 https://sentiﬁ.com.

Exploring Message Correlation
97
hidden/related information and/or domain expert’s knowledge may also be
useful information to make two messages cohere together. Unfortunately the
information like this usually is stored in diﬀerent datasets and the mentioned
approaches thus do not take into account this information. To this end, this
paper reaches the following contributions:
• An approach in which we do not only consider the direct connection rela-
tions of messages in a set for correlation estimation, but also their indirect
connection relations, i.e., the additional information that comes from other
associated elements/sets. At this point, we leverage the capability of today
parallel and cluster computing in computing correlation weights among mes-
sages because it is a large scale data processing problem.
• The Hyper Coordinates visualization technique, which is the extension of
Hyper Word Clouds, that supports the users in understanding, interacting,
and verifying relationships in multivariate dataset. This reduces the limitation
of solely automatic data computation.
• A prototype tool as a proof of concepts discussed in our novel approach.
The remaining sections of the paper is organized as follows. In Sect. 2, the work-
ing scenario at Sentiﬁis outlined for general problem identiﬁcation. Following
that, we discuss our main development in terms of message correlation computa-
tion and visualization support in Sects. 3 and 4, respectively. Section 5 will then
showcase the developed technique for the exploration of messages regarding Sen-
tiﬁscenario. Section 6 compares our development with existing research. Finally,
we conclude our work and shortlist future work in Sect. 7.
2
Working Scenario
2.1
Crowd-Based Data Analytics at Sentiﬁ
The software company Sentiﬁaims at developing products for global ﬁnan-
cial markets. By collecting and analyzing crowd-based data available on the
cyberspace, Sentiﬁidentiﬁes and ranks market inﬂuences, and then provides
predictions for market trends. This section describes a scenario for that devel-
opment, as outlined in Fig. 1:
Firstly, data are selectively collected and preprocessed with three major par-
allel activities:
1. Everyday, workers at Sentiﬁ– with the help of a software engine – collect and
verify social network proﬁles and store in their database a list of publishers
who talk about something on the Internet;
2. They also process to collect other information which are categories about
topic, name of entity, lexicon, source, impact and event. Here, Sentiﬁ’s domain
expertise is needed in manually deﬁning and reﬁning relevant information
extracted from the software engine;

98
T.-D. Cao et al.
Fig. 1. A general working scenario at Sentiﬁ
3. And, from the crowd data such as news articles, blogs, and Twitter tweets,
etc. – many of which are chaotic and contain tremendous stuﬀs which are
meaningless for ﬁnancial domain – Sentiﬁselects messages in terms of ﬁnan-
cial contexts by restructurizing and augmenting them with information from
the above-mentioned categories with the aim of a semantic engine. For
instance, a message is analyzed to be about someone/organizations (i.e., name
of entity) on one or some topics (e.g., “stock market”, “downing of oil price”,
etc.), from or related to one or some events (e.g., “Obama recommends Apple
to produce their product in US instead of China or other countries”), and
indicates how its information impacts to a list of topics, and so on.
To this point, messages are central objects in analyzing data streaming on
the Internet. However, the problem of redundancy and similarity exists amongst
the messages. In that regard, analytical support is expected as a next step in
structurizing veriﬁcation, message similarity detection, important messages iden-
tiﬁcation, data augmenting correction before the messages are sent to subscribers
or for reporting. Currently, Senﬁti employs a lot of staﬀs/experts in this phase,
therefore they need a tool to simplify the work. In the next section, the challenges
for this analytical support is discussed.
2.2
Problem Analysis
Message similarity detection is the most important requirement at the Analytics
phase of Sentiﬁ’s scenario. This is because they want to avoid the case of sending
a group of messages to their subscribers in which those messages have similar
content. This is not a new problem since it relates to message correlation problem

Exploring Message Correlation
99
which estimates a distance or weight between two messages. Several approaches
have been proposed for similar issues such as [10,19,21,31,34]. Unfortunately all
those approaches were proposed for a concrete context and purpose with limita-
tion of data available or unwell-structured of data. Therefore, the hidden/related
information – which is usually stored in diﬀerent datasets and linked to process-
ing data via direct and/or indirect relationships – does not take into account.
In ﬁnancial domain, considering all direct as well as indirect information for a
decision making is very important. For instance, since two messages talk about
two diﬀerent events, they usually have no correlation if considering only direct
information about events. However, if some additional information of events such
as: (i) two events are organized by the same community, (ii) they talk about the
same topics, or (iii) in the same context, and so on – then the messages still
somehow have a correlation. We call this type of correlation as indirect connec-
tion relationship. In other words, as examining correlation between messages,
all indirect relationships through many augmented data must take into account
beside the direct ones.
As mentioned earlier, the analytics phase do not only cover the message
similarity problem, it must also take other problems into consideration as well.
We highlight all those related problems in the following question list:
1. How two messages correlate together?
2. Which ones are important in a group of correlated messages?
3. Could an important message represent to all others in a group?
4. If yes, which missing information of the group that a message must cover?
5. Does the processing engine map correctly ﬁnancial terms?
All those questions raise us two following problems: (i) message clustering as
well as recognizing the representatives, (ii) visualizing message correlation that
allows domain experts to explore the message clusters, conﬁrming the represen-
tatives as well as interacting with data by correcting message value or merging
some of them together. Two those major problems are really necessary in cop-
ing with streaming messages from the crowd. In the two next sections, we will
introduce in details our solutions for those two problems.
3
Message Correlations
We present in this section an approach to measure the correlations between
messages that works as a back-end system in supporting the visualization tech-
nique described in Sect. 4. We ﬁrst deﬁne our supported data model in Sect. 3.1
in which the concepts of direct and indirect connection relations are described.
Section 3.2 presents a method to calculate weight – the parameter in measuring
correlation between two messages. Section 3.3 introduces the algorithms used for:
(i) calculating correlation weights of all pairs of messages; (ii) clustering mes-
sages into clusters and extracting their cluster representatives as well as their
properties; and (iii) calculating correlation weight for a new incoming message
based on the already calculated messages.

100
T.-D. Cao et al.
3.1
Data Model and Relationships
Before going further to discuss on message correlation, we describe in this section
the supported data model as well as the analysis on data relationships. We orga-
nize data into tables centered around a focus table (denoted, ft), in which the
correlation among rows of ft is measured based on its relationship with other
tables (denoted, rt = relation table). For instance, take a look into the Sentiﬁ’s
scenario, ft is the message table because we want to measure the correlation
among the rows of messages in this table. Whereas the other tables such as
event, topic, lexicon, publisher, etc. are the relation tables. Based on these con-
cepts (i.e., focus table and relation table), we deﬁne that a direct connection
relation is a relationship established by ft table and a relation table rt. An
indirect connection relation is a relationship established by two relation tables.
Deﬁnition 1 deﬁnes a formal deﬁnition of direct and indirect connection relations.
Deﬁnition 1. Let ft be a focus table in which Aft = (a1, a2, ..., an) is a set of
attributes of ft and RT = {rt1, rt2, ..., rtm} be a set of relation tables where
Arti = (a1
i , a2
i , ..., aki
i ) is a set of attributes of rti. A set of relationships2 (based
on ft and RT) R = Rd ∪Ru is deﬁned as follows:
• Let RTd ⊆RT, Rd = {rd
1, ..., rd
|RTd|} is a relationship set between ft and RTd
(called, direct connection relation), in which rd
i = [ak, al
i] | ak ∈Aft, al
i ∈
Arti.
• Ru = {ru
1 , ..., ru
h} is a relationship set in RT (called, indirect connection
relation), in which ru
i = [ak
p, al
q] | ak
p ∈Artp, al
q ∈Artq (Artp and Artq are the
set of attributes of relation table rtp and rtq ∈RT).
Constraints on Relationship: Basically, there are 4 types of constraints on a
binary connection relationship between elements on two tables, which are one-
to-one, one-to-many, many-to-one and many-to-many. However, we target them
here through ordered relations which indicates the sequence of connections in the
order from a source to a destination. Besides, set relation is used to explain a
data element belongs to a set (typically a category), and quantitative relation is
used to represent the quantitative information of data elements in a set (see more
in Sect. 4.1). For instance, take a look back to Sentiﬁ’s scenario, there can be
ordered relations which are directly –by meaning of direct connection relation–
from message to event, message to topic, message to name entity, message to
lexicon, message to impact, message to publisher and message to source. While
there are also ordered relations indirectly –by meaning of indirect connection
relation– via event such as event to topic, event to name entity, event to lexicon
and event to impact or via impact to topic.
2 We deﬁne here a binary relationship between two tables, i.e., we pick one attribute
up from each table to make a relationship. In real database, a relationship may
be created from several attributes. However, we can transform them into a single
attribute by specifying representative identiﬁer.

Exploring Message Correlation
101
3.2
Correlation Weight
Correlation weight is a parameter to measure the relationship between two mes-
sages. Two messages have a strong relationship if their correlation weight is high
and a weak relationship if the weight is low. If the weight is zero, there is no
relation between two messages. In our approach, the weight is calculated based
on two types of connection relation deﬁned in Deﬁnition 1, which are Rd and
Ru. Suppose that all ordered relations from the focus table to relation tables or
between relation tables are set relations, a constraint one-to-one is considered
as a set with only one element. Starting from the focus table, each direct con-
nection relation (denoted rd) will contribute a direct weight (denoted wd) to the
total weight. This direct weight is calculated by getting the size of intersection
set (denoted IS) of value sets of two rows mi and mj projected on attribute ak
of direct connection relation rd
k, multiplied with a factor wd
k deﬁned by user. In
addition, if both value sets of two rows mi and mj projected on attribute ak of
direct connection relation rd
k (denoted V Sk
mi and V Sk
mj) subtract to intersection
set (i.e., IS) are not empty, and the attribute ak is an object deﬁned by an
indirect connection relations, an indirect weight (denoted wu) of two value set
V Sk
mi and V Sk
mj is calculated by sum of correlation weights of all element pairs
(ei, ej) ∈(V Sk
mi \ IS × V Sk
mj \ IS). While calculating the correlation weight of
element pair (ei, ej), if the indirect connection relation is deﬁned from current
relation table to others, we recursively calculate the correlation weight of those
indirect connection relation until the end of ordered relation path. For instance,
in the Sentiﬁ’s scenario, we ﬁrst calculate the direct weight of two messages using
direct connection relations such as message-to-event, message-to-topic, message-
to-lexicon, etc. While considering connection relation message-to-event, suppose
that two messages mi, mj do not share the same event (i.e., wd
e = 0), we cal-
culate an indirect weight of two events emi and emj (i.e., wu
e ). To calculate
the indirect weight wu
e , we use the indirect connection relations via event such
as event-to-topic, event-to-impact, event-to-lexicon, etc. If emi and emj do not
share any common value set of topic or lexicon, it means topic and lexicon do
not contribute a factor to wu
e . However, suppose that emi and emj do not share
a common value set of impact, we continue to calculate an indirect factor of two
impacts, which emi and emj hold, because there is an indirect connection rela-
tion deﬁned from impact to topic. For each recursive step, a function is deﬁned
to scale an indirect factor to be smaller than a direct one. For instance, we deﬁne
the following function is deﬁned to scale the indirect factor of events.
f(x) = x/(max + min)
where max = MAX(weight(emi, emj)) and min = MIN(weight(emi, emj)).
Example 1: For the data model provided in Fig. 1, without considering any
unrelated ﬁelds, we suppose to have the two following messages:
• m1 = (id1, {event1, event2}, {topic1}, {lexicon1, lexicon2}, {name entity1},
{impact1}, publisher1, source1)

102
T.-D. Cao et al.
• m2 = (id2, {event2, event3}, {topic2}, {lexicon1, lexicon3}, {name entity1},
{impact2}, publisher1, source2)
To compute the correlation weight w between m1 and m2, we also suppose
to have factors already speciﬁed for each property as follows (event = 2, topic
= 1, name entity = 0.5, lexicon = 0.7, impact = 0.5, publisher = 0.3, source =
0.05), and thus w will be computed as w = 2 ∗we + 1 ∗wt + 0.7 ∗wl + 0.5 ∗wn +
0.5 ∗wi + 0.3 ∗wp + 0.05 ∗ws, where:
• we = wd
e + f(wi
e) is the weight of event in which wd
e =| {event1, event2} ∩
{event2, event3} |= 1 is the direct weight, and wi
e is the correlation weight
between the non-intersected set {event1, event3} which is calculated based on
their properties such as name entity, topic, lexicon and impact (i.e., indirect
connection relations of message via event). wi
e is calculated with the same
method applied for messages, so we do not present them in details but just
assume that wi
e is calculated = 1.5 and f(wi
e) = 0.65. As a result, we have
we = 1 + 0.65 = 1.65.
• wt =| {topic1} ∩{topic2} |= 0 is the weight of topic.
• wl =| {lexicon1, lexicon2}∩{lexicon1, lexicon3} |= 1 is the weight of lexicon.
• wn =| {name entity1} ∩{name entity1} |= 1 is the weight of name entity.
• wi = wd
i =| {impact1} ∩{impact2} |= 0 is the direct weight of impact, sup-
pose that the indirect weight of impact1 and impact2 is zero.
• wp =| {publisher1} ∩{publisher1} |= 1 is the weight of publisher.
• ws =| {source1} ∩{source2} |= 0 is the weight of source.
Finally, we have w = 2 ∗1.65 + 0 + 0.7 + 0.5 + 0 + 0.3 + 0 = 4.8.
3.3
Computing Correlation Weight
As discussed in Sect. 2.2, to help domain experts to explore the crowd-data
such as identifying message similarity, recognizing important messages, correct-
ing relations, visualization supports are needed. However, to visualize a data set,
we need a back-end system to estimate the correlation among messages as well as
other related properties. This section presents three algorithms for the following
purposes: (i) calculating correlation weights of all pairs of the available messages;
(ii) based on these weights, classifying messages into clusters that allow us to
overview the relationship of the whole dataset; (iii) calculating the correlation
weight of a new incoming message against the existing ones. Because of big data,
all the algorithms are designed to run on a parallel processing platform such as
Spark [1].
Algorithm 1 presents the algorithm to calculate correlation weight of all pairs
of messages. This algorithm requires 4 additional parameters beside the set of
messages. The ﬁrst and the second ones are two set of direct/indirect connection
relations. The third and the fourth ones are the impact factors corresponding to
the direct/indirect connection relations. In this algorithm, we ﬁrst calculate a
Cartesian product of all rows of the message table (line 2). This operation gives

Exploring Message Correlation
103
Algorithm 1. Correlation weights of all pairs of messages
Data: M is a message table;
Rd is a set of direct connection relations on M;
Ru is a set of indirect connection relations on attributes of M;
W d is a set of impact weights of Rd;
F is a set of scale functions.
Result: Correlation weight of all pairs in M.
1 begin
2
CP ←−M × M;
3
FCP ←−Remove all pairs (mi, mj) ∈CP such that mi.id after mj.id;
4
Load all weights of indirect connection relation into buﬀer;
5
for (mi, mj) ∈FCP do
6
w ←−0;
// each relation corresponds to an attribute of M
7
for r ∈Rd do
8
Ai ←−set of values of mi projected on attribute r;
9
Aj ←−set of values of mj projected on attribute r;
10
I ←−Ai ∩Aj;
11
wd
r ←−| I |;
12
wu
r ←−0;
13
if Ai \ I ̸= ∅and Aj \ I ̸= ∅and ∃ru ∈Ru deﬁned on attribute r then
14
A∗
i ←−Ai \ I;
15
A∗
j ←−Aj \ I;
16
for (ap, aq) ∈(A∗
i × A∗
j ) do
17
wu
r ←−wu
r + fr(weight(ap, aq));
// fr ∈F and weight(ap, aq) is queried in buffer
18
w ←−w + W d[r] * (wd
r + wu
r );
19
Store tuple (mi, mj, w) into result;
us two relation pairs between row ri and row rj, i.e., <ri, rj> and <rj, ri>. How-
ever, these relations are equivalent, so we remove one of them by deﬁning a rule
“rj after ri” then removing <rj, ri> (line 3). From line 5 to line 18, we calculate
the weights for the remainder pairs. For each pair, we examine all attributes (line
7), which are deﬁned in direct connection relations, of message table to compute
the factors they contribute to the total weight of the pair (i.e., variable w in line
6). For each attribute (denoted at), the factor of direct connection relation is
computed by counting the size of intersection set between two value sets of at
(line 8–11). If both value sets have remainder values (after removing all common
elements) and the indirect connection relation are deﬁned on table referring to
at (line 13), we recursively compute the correlation weights for all pairs in these
two remainder value sets. However, to speed up the performance, based on the
indirect connection relation set and the ordered relation property, we apply the
same method to compute the weight of all pair values of at before this processing,
and we just query its value from buﬀer or database (line 17). We next sum up all

104
T.-D. Cao et al.
weights of each pair to have a weight of indirect connection relation. Finally, the
correlation weight of two messages is the sum of all attribute factors (line 18).
Using the output of Algorithm 1, we propose Algorithm 2 to group messages
into clusters. The output of Algorithm 1 can be considered as a weighted graph, in
which each message is a vertex and the edges are the correlation message pairs.
We deﬁne clusters to be the connected components of this graph. Therefore,
by setting up a threshold and removing all edges with weak correlation, the
remainder graph is then a set of clusters (line 2–4) – where every cluster contains
messages that correlate strongly together. However, on the one side, from the
view of visualization, user usually want to see an overview of data before going
further to detail. On the other side, the domain experts also want to recognize
which ones are the important messages in every cluster. This requires us to select
the representatives for each cluster. We deﬁne that an important message is the
one which is correlated to many others. In the case that two messages have the
same number of correlations with the others, the one with stronger correlation
weight is more important. Using this rule, we rank the messages as follows:
rank(m) =

mi∈Cm
weight(m, mi)
(1)
where Cm is a set of messages that m has a correlation.
Although two messages belong to a cluster, they might still not share a com-
mon set of values because they do not have the direct correlation. For example,
A has correlation with B (denoted A ←→B), then B ←→C and C ←→D.
However, A may not have a correlation with D because they may have diﬀerent
value sets. Since both A and D have the highest rank in a cluster, we select both
as the representatives. After sorting messages by their rank, the user will decide
how many messages he/she wants to get as the representatives for each cluster
by indicating a percentage (line 7). Finally, the values of representative set are
extracted and stored for further calculation (line 8–9).
At this point, all calculations necessary for the visualization are processed.
However, there is another problem in the case that data are still being collected
from the crowd. It means that we need to continue to measure the correlation of
any new incoming message (against the existing ones). This problem is usually
carried out on-the-ﬂy where the calculated results are provided instantly to the
visualization component. In this case, using Algorithm 1 is not appropriate since
the available messages have been grouped into clusters and their representatives
and properties have been extracted, where the incoming message usually has no
correlation to all clusters but just a few. Therefore, Algorithm 3 is proposed for
this purpose.
We ﬁrst ﬁnd the cluster(s) where the new message m is close to (line 6), then
we only compute the correlation weight of this message against the messages in
the clusters that the new message close to (line 10). This allows us to reduce the
computation. If m belongs to some clusters, we remove all weak correlations to
identify which cluster that m really belongs to (line 13). If m still belongs to more
than one cluster, we merge those clusters into one and identify its representatives

Exploring Message Correlation
105
Algorithm 2. Clustering message
Data: CW: a set of tuple (mi, mj, w);
t is a threshold of correlation weight;
p top percent of message in cluster (i.e., representatives of cluster).
Result: A set of cluster and their representatives as well as values.
1 begin
// remove all weak correlations then we have clusters
2
C ←−Remove all tuple (mi, mj, w) in CW where w < t;
3
G ←−Build a graph from C;
4
CS ←−Get a set of connected components of G ;
// Note: each component is a cluster, then we identify
their representatives
5
for c ∈CS do
6
Rank messages in c using formula (1);
7
Tc ←−Get top p percent of messages in c;
8
Atc ←−Get all attribute values of messages in Tc;
9
Store tuple (c, Tc, Atc) into result;
Algorithm 3. Computing correlation weight for new incoming message
Data: CS: a set of clusters and their representative values;
m: incoming message;
Rd is a set of direct connection relations.
Result: Set of messages that m has correlation as well as m’s cluster.
1
begin
2
flag1 ←−false;
3
for c ∈CS do
// c consists of id, value sets of attribute i ∈1,...,n
where each attribute i represents a direct
relationship.
4
flag2 ←−false;
5
for r ∈Rd do
6
if value of m projected on attribute r belong to value set of attribute r of c then
7
flag2 ←−true;
8
break;
9
if flag2 = true then
// m belongs to c
10
Compute the correlation weight of m and messages in c (Similarly with lines 5 - 19 in
Algorithm 1);
11
flag1 ←−true;
12
if flag1 = true then
13
Remove all weak correlations then identify which cluster m really belongs to;
14
if m belong to more than one clusters then
15
Merge those clusters into one;
16
Re-identify the representatives of cluster;
17
else
// m does not belong to any cluster
18
A new cluster is added into CS where m is the representative;

106
T.-D. Cao et al.
(line 15–16). Finally, if m does not belong to any cluster, a new cluster is created
and this message is the representative of that new cluster (line 18).
4
Visualization Design
In this section, we continue with the Hyper Coordinates visualization technique
which is designed to support the analysts in visually exploring to understand
how the messages calculated in Sect. 3 are correlated and clustered together,
verifying their representatives, or updating the irrelevant data. Section 4.1 begins
with the general analysis on data and data relationships as well as the expected
interaction tasks. Following that, the details on visualization design in terms of
data relationships and interaction tasks are then provided in Sects. 4.2 and 4.3,
respectively.
4.1
Data Relationships and Tasks Analysis
Section 3.1 already presented the data model where the concepts of direct and
indirect connection relations and other types of data relationships in a dataset
are mentioned. In this section, in terms of visualization design, we further discuss
on those relationships – which are connection relations, set relations, quantitative
relations, and order relations. We consider all of those relationship types here
because they are relatively constituted to each others in communicating data
characteristics and tasks at hand.
When examining a data element in a relation table associated with a message
in the focus table, deﬁnitely we need to show there is a connection between that
associated element and the message, and thus the representation of such con-
nection in the visualization is requested. Additionally, it is expected to visually
diﬀerentiate the direct and indirect connection relations: visual direct connec-
tion is provided if the element is directly associated with the message, while
indirect connection is provided when the element is indirectly associated via the
relationships from other elements or other sets on other relation tables.
With a connection relation between a message and an associated element,
it obviously is also the connection between the focus table and a relation table
(see Deﬁnition 1). Therefore, one might easily understand that we also need
ways to represent the connection between the tables. This is a kind of set rela-
tions visualization. Further on, by visually representing set relations, we can
also communicate the situation that messages belonging to clusters (referring
Algorithm 2).
Next, we seek to communicate the weights that constitute the correlation
between messages or between any data elements or sets on tables. By that
request, it is meant that quantitative relations visualization are desired. Finally,
as introduced in Algorithm 1, correlation weights between messages are aggre-
gated from diﬀerent direct and indirect connections, such as from topic to impact,
then to event, and then to message tables. In that regard, the order of such con-
nection relations are expected as well.

Exploring Message Correlation
107
Regarding tasks analysis, we concentrate on low-level visualization tasks –
which are visual summarization, visual identiﬁcation, and visual comparison (in
terms of the typology deﬁned [4]). Firstly, we need interactions to support the
users in detecting important messages from correlated messages clusters. But
because the dataset is large, it is impracticable to visually display all the data
elements and relationships in details. Therefore, a visual summarization about
how messages are distributed over the clusters should be provided ﬁrst so that
the users will have a general overview. Secondly, from the overview of message
clusters, users normally want to dig into the view to ﬁnd out further information,
such as the messages and correlations that they are interested in. In other words,
we need supports for visual identiﬁcation of data of interest. And ﬁnally, since
messages and their correlations need to be veriﬁed by the analysts, we also need
supports for visual comparison on diﬀerent combinations of relationships for
correlation computation.
4.2
Design in Terms of Data Relationships
With the above discussion on data relationships analysis, visualization develop-
ment works around how direct and indirect connection relations, set relations,
quantitative relations, and order relations are visually represented. In our design,
it is accomplishable through the ways visual objects showing in parallel coordi-
nates for set, anchor path for connection and order, node size for quantitative,
and link for connection relations, amongst some other encodings using common
visual primitives.
4.2.1
Parallel Coordinates for Set Relations
The crucial idea behind the Hyper Coordinates is to represent the dimensions of
a multivariate dataset as parallel coordinates so that their variables and relation-
ships can be mapped to. Every coordinate, which is a parallel vertical column on
the screen (Fig. 2a) is designed to communicate a data table. Accordingly, when
data elements are shown on this column (referring Sect. 4.2.3), they communi-
cate the set relations about the table in which they belong to. The name of the
data set, such as table name, is placed on top of the column for table identiﬁ-
cation. To support connection relations between sets or their member elements,
we also use colors to diﬀerentiate the tables.
4.2.2
Anchor Paths for Connection and Order Relations
Anchor path is the concept developed to indicate the connection between the
tables, or set of elements in the column that the table represents for. It is named
as anchor because through it, the elements or links from the elements on the
columns are identiﬁable. Instead of looking at the detailed connection between
elements on the columns, looking at the anchor path is more vivid in examining
those connection and order relations (Fig. 2b). We simply draw curve path to
link pairs of tables for connection relations. The color of the path indicates the
direction from a table to another, and thus it is colored by the hue value of the

108
T.-D. Cao et al.
f
c
a
b
d
g
e
Fig. 2. Hyper Coordinates visualization design: (a) each column communicates set rela-
tion of elements in a table, where (b) table names on top are connected through the
anchor paths for connection and order relations; (c) size of node indicates its quantita-
tive relation in a set, while (d) types of connection relation between nodes are encoded
by color and solidity. During interaction, nodes and links are shown based on the level
of overview-detail: (e) overview of clusters, (f) overview of messages in a cluster of
interest, and (g) detail of nodes and connections between elements associated with a
message of interest.
source table. To diﬀerentiate the direct and indirect connection relations, the
solidity of line is used: solid line is for the direct connection, and dashed line is
for the indirect one. To avoid the matter of cluttering if the direct and indirect
paths are too close together, especially in the case they are coincident, they have
to be positioned further from each others, such as in two opposite sides above
and below the baseline of the table name.

Exploring Message Correlation
109
4.2.3
Nodes and Links for Quantitative and Connection Relations
Node-and-link is a basic way to represent connected elements in a set, especially
in graph. In Fig. 2c, sets of nodes are placed along the columns and they com-
municate well the sets that they belong to. In that way, the simplest encoding
is to use a dot for a node. Since node’s position is associated with set relations
(nodes have to be on the columns), size of the node is designed for quantitative
relations. To encode connection relations, path line linked every pair of nodes
is provided. The color and solidity of line indicate the source of the associated
table, as well as whether the connection is direct or indirect between nodes –
similarly to the design of anchor paths (Sect. 4.2.2). An indirect connection is
communicated through the two dashed lines in between two relation tables with
a solid line between the nodes on the same via table. Since the amount of nodes
and links are typically tremendous, linking lines have to be smoothly blended
as curves to minimize negative aﬀects as well as visual clutters (Fig. 2d). In
Sect. 4.3, we will present diﬀerent visual encodes designed for nodes and links
for diﬀerent levels of overview + detail exploration.
Finally, it is also possible to create node as a glyph with further visual cues.
Text can be added to nodes to form the parallel word clouds, where visual cues
are browsed and explored through interaction. However, this has to be carefully
examined since the dataset is typically so big and complex.
4.3
Design in Terms of Interaction Tasks
The three low-level tasks mentioned in Sect. 4.1 in fact follow the sequence of
tasks carried out in the typical visual information seeking mantra, which are
“overview ﬁrst, zoom and ﬁlter, then details on demand – together with relate,
history, and extract” [26]. This section describes how those interaction tasks
aﬀect our visualization development. Since the display space is restricted but
data are numerous, diﬀerent data have to be selected to be represented on the
Hyper Coordinates interface following user interactions. We design here three
levels of overview + detail interactions: overview of clusters (ﬁrst level), overview
of messages on a cluster (second level), and details of a message of interest
(third level). These three levels of overview+detail exploration are discussed with
regard to the management of visual primitives of nodes, links, anchor paths, and
parallel columns.
4.3.1
Overview of Message Clusters
With the large stream of messages from the crowd, they are clustered into groups
as presented in Sect. 3.3. Therefore, to examine the messages, the straightforward
strategy is to show them in clusters. Typically, users want to get general informa-
tion such as: for each cluster, how many messages are there, what is the highest
correlation weight of a message in that cluster, what relation tables contribute
to the correlation weights of the messages in that cluster, etc. This is because the
data are big and complex and thus it is impractical to show all their messages
as well as how the messages are correlated at the same time.

110
T.-D. Cao et al.
To that end, at the ﬁrst level of overview, we design to show only simple
information indicating the tables constitute to each cluster which give hints for
further interaction. Because messages are clustered together, the message column
in the parallel coordinates is used to communicate the clusters, while the other
columns are still for the other tables (users can interact to drag each column to
any horizontal position on the screen). For every cluster, straight lines are used to
represent connection relations between the relation tables and the focus table,
while small dots are shown on each relevant table columns to emphasize the
connections (in fact, these forms a group of overlapping lines from relation table
columns to the cluster column). To provide a coherent view on the interaction,
the colors of the nodes are unique with the colors of the tables. However, we
want to discern the connection lines with the similar ones in the second level for
messages overview – as presented next, thus we use monotonic color (grey) for
line connections (Fig. 2e).
Now, with numerous clusters shown as parallel lines on this overview level, we
employ ﬁsheye lens [24] to support the users in focusing on a cluster of interest
for further interactions.
4.3.2
Overview of Messages in a Cluster of Interest
Following the above overview level for clusters, when a user focuses on a cluster,
s/he is supported to explore its messages of interest. Therefore, we allow the
user to select a cluster of interest that leads to this second level of overview +
detail interaction.
Most of the display space is now reserved for the messages, while the general
context of the ﬁsheye clusters reduces. At this point, we only show the overview
of messages so that the user can skim and decide to continue to explore or move
to another cluster (we give hints about the clusters above and below the selected
cluster with information such as the number of messages in those clusters). The
message nodes and their connection tables are encoded completely like in the ﬁrst
level of overview, except two main things: (i) the nodes on the message column
is for messages, and thus we only encode information of message nodes, which
is its weight in the cluster, and (ii) connection lines are colorized by connection
columns’ colors to distinguish with the line of clusters (Fig. 2f).
Furthermore, at this level, to support the user to pay more attention to a
message, as the user hovers on it, we highlight the message of interest (without
changing visual encoding) while dimming the other messages (including their
connection links and colors). The anchor path is also updated with detailed
relation tables (direct vs indirect).
With such overview, users are given with hints about messages and correla-
tions, but they do not know how detailed the weights are aggregated. Therefore,
the next step is to support the user to zoom more and ﬁlter into details for a
selected message.

Exploring Message Correlation
111
4.3.3
Examine Messages of Interest in Details
With a selected message is now all related messages (they are all from the same
cluster) that contribute to the aggregated correlation weight of the selected mes-
sage to be explored. To support the users to perform this examination, all mes-
sages and associated elements on other tables are to be detailed. The display
space is the space used for the messages overview in the second level (but all
old messages and their associated nodes on other table columns as well as the
existing connection links are removed).
Then, we show detailed nodes for elements on their table columns and
detailed links for connection relations following the design provided in Sect. 4.2.3.
The colors of nodes and links indicate the associated tables. The size of a
node indicates how quantitative the correlation weight is. For connection rela-
tions, there are three types of lines between the nodes: the solid paths for
direct connection relations between elements on two diﬀerent tables, and the
solid paths between nodes on the same table –which represents a correlation
of messages– and dashed paths between diﬀerent tables for indirect connection
relations (Fig. 2g).
With that encoding, the user can interact by hovering on and selecting the
node or link that s/he wants to examine, where more visual cues are added
to the associated nodes and connection links. This supports the user in under-
standing the detailed correlations between messages. The node which is hov-
ered is highlighted bigger and with a tooltip. To ﬁlter the nodes and connec-
tions of interest, e.g. in checking their relationships, the user can click on a
node to include/exclude them from the examination – the excluded node is
dimmed with low opacity, while the included one will show with full opacity. To
include/exclude a node and all of its connected nodes, the user can press a prede-
ﬁned key while clicking on the node, where the connection links are highlighted
as well. S/he can also drag any node to any place on the associated column
for a clear understanding of relations between nodes (in addition to dragging
columns). By default, we show only the nodes and connections those contribute
to the correlation weights which form the cluster. However, the user might want
to check the unrelated data elements (properties) of messages as well (e.g., so
that s/he can decide whether or not a message should be a representative for
another one). As a result, we also design to include the unrelated data elements
when two messages are in comparison (but to avoid using so much ink on the
screen, they are only included as the user clicks on the connection link of inter-
est between two messages). To this point, tasks such as identifying, relating
information, and comparing nodes and links are easily carried out. In the next
section, we will present examples that analysts can do in exploring and verifying
messages through their correlations.
5
Showcase
In this section, we ﬁrst outline the overview of the system prototype developed
to actualize our proposed approach, in Sect. 5.1. Then, Sect. 5.2 describes the

112
T.-D. Cao et al.
Storage
Message Correlation 
engine
Message Clustering 
engine
Database
REST Service
User Interface
Streaming Processing 
engine
incoming 
messages
interaction
query data
streaming data
Fig. 3. System prototype overview
visualization interface, where sample cases are demonstrated in Sect. 5.3. The
results of user interactions are discussed in Sect. 5.4.
5.1
System Overview
We implement a system prototype based on the message correlation approach
and the visualization technique described above. Its aim is to provide an interface
for message correlations examination and veriﬁcation, as a part of the Analytics
component mentioned in Fig. 1 of Sect. 2. Figure 3 shows our prototype overview.
Ignoring data collection and pre-processing steps, starting from storage, the pro-
totype system measures the correlation of all message pairs by implementing
Algorithm 1. We use Spark framework [1] to implement this component. The
result is then passed to the next component, named Message Clustering, which
implements Algorithm 2 using Spark GraphX3 library for clustering. Because
the output of Message Clustering component is unwell-formatted and lacks of
related information for visualization, we store it on a database. We next imple-
ment a REST Service to transform the previous output into a well-formatted
structure which augments them with additional information before sending to
the client-side User Interface. In parallel with those components, we also imple-
ment Algorithm 3 for Streaming Processing component, which receives incoming
messages from a REST service of pre-processing engine and calculates the cor-
relation of the incoming message with the existing ones in the database. It then
notiﬁes to REST service for visualization updating. The User Interface is imple-
mented in Javascript, using D3js4, jQuery5, Bootstrap6, and Font Awesome7 for
visualization and interaction handling.
3 http://spark.apache.org/graphx/.
4 https://d3js.org/.
5 http://jquery.com/.
6 http://getbootstrap.com/.
7 http://fontawesome.io/.

Exploring Message Correlation
113
a
b
c
d
Fig. 4. Hyper Coordinates user interface: (a) toolbar, (b) anchor bar, (c) visualization
area, and (d) notiﬁcation bar.
5.2
The User Interface
The User Interface, as screenshotted in Fig. 4, includes a toolbar on top; then the
anchor bar; then the visualization area for clusters, messages, and their related
elements and tables visualization; and at the bottom of the visualization is a
notiﬁcation bar.
The toolbar is designed to contain a collection of controls to support the
users in manipulating the visualization elements in focus. There are four option
buttons to support the modes of browsing, editing, searching, and arranging data
of interest. The default mode is browsing which supports the users in exploring
how messages are correlated in clusters. If switching to editing mode, the user
can adjust the connections between nodes or update their factors to examine
the updated correlations and clusters. For searching or arranging mode, more
controls will be added: with searching, there will include a search box and a
multiple-select box that lists the tables for data searching or ﬁltering; while
with arranging, there includes a dropdown for selecting a table and criteria for
arranging on that table (such as arranging clusters by the number of messages,
or by highest correlation weight of the centroid message, or by the number of
connected tables, etc.).
The anchor bar is immediately above the visualization area for table names
and anchor paths manipulation. It is separately mentioned (along with the visu-
alization area) because we want to diﬀerentiate interaction tasks on it and tasks
on visualization area. The anchor bar is implemented following the design in
Sect. 4.2.2.
The visualization area is the main space where users interact with the data
elements (such as clusters, messages, ...) and connection links following the design
with three levels of overview + detail interactions discussed in Sect. 4.3. In the
default mode of browsing, the user can keep Ctrl key for overview browsing with

114
T.-D. Cao et al.
clusters, releasing Ctrl key for overview browsing with messages on a cluster,
and pressing the Spacebar to enter the level of detailed exploration. For editing
mode, the visualization does not change the level of zooming, while the user can
click on a link, a node, a parallel coordinate, or an anchor path to edit its value.
The notiﬁcation bar is provided so that the system can brieﬂy text some
messages to the users during interaction while not interfering his/her interaction
on the visualization area.
5.3
Sample Cases
Now, ignoring the back-end processing (i.e., computing message correlations),
we provide sample cases regarding the exploration and veriﬁcation of messages
of interest on the User Interface. These samples demonstrate how visualization
supports the analysts in answering the question mentioned in Sect. 2.2. Let’s
start with the situation that Jane, an analyst at Sentiﬁ, wants to explore just the
messages about “oil price”. To accomplish that action, she uses the search/ﬁlter
function to limit the number of clusters as well as messages to be shown on
the screen. With the list of ﬁltered clusters, she presses Ctrl key and hovers on
each cluster to see the overview of the messages on that cluster of interest, as
already provided in Fig. 2f. At this point, she can easily notice the important
messages in the cluster under exploration by looking at the size of the node as
well as the number of connections with other nodes on that cluster, for instance,
as arranging the ranked messages from the center of the cluster. To this point,
question Q2 is answered.
After that, Jane hovers on an overview message and press Spacebar key to
explore in details how a message of interest correlates with the other ones in
that cluster, as shown in Fig. 2g. This answers question Q1. To check whether a
message of interest can be the representative for other messages in the cluster or
not (question Q3), Jane continues to examine the correlations of that message
with every other message, accomplishable when she clicks to select every pair
of messages to look at their detail direct/indirect connections, as presented on
Fig. 5a. In addition to ﬁnding the answer for question Q3, Jane also gets the
answer for question Q4 by checking the diﬀerences between any two messages.
This is done when Jane hovers on a correlation link of the two messages, where
separated nodes connected on the other relation tables are highlighted (Fig. 5b).
With her domain expertise, Jane will decide whether to merge those messages
together or not, or whether a missing information should be added into the
representative message.
Finally, when with a message of interest, Jane may hover on any node to
examine the detailed data – such as through tooltip as in Fig. 5c). There, she can
check whether the semantic processing engine correctly mapped ﬁnancial terms
or not. To that end, question Q5 is also answered. From the interface in Fig. 5b
and c, choosing the edit mode, this analyst can manually correct the mapping
work of semantic processing engine by editing the ﬁnancial terms, making a new
connection relation or removing them from the list.

Exploring Message Correlation
115
a
b
c
Fig. 5. Exploring detail correlation of a message of interest: (a) direct/indirect con-
nection relation, (b) two messages comparison, (c) semantic engine veriﬁcation.
Besides, when having a new message streamed to the system, the Streaming
Processing component calculates its correlation against the existing messages in
the database. As having a major change, it is prompted on a cluster (if it is not
on the current cluster of interest) or as a (updated) node on the current cluster
of interest. It is then provided to Jane as blinking on the interface. Got noticed,
Jane might browse to that message to examine in details when she wants.
5.4
Discussion
We have shown on the above showcase the situation that a domain expert can
easy explore streaming messages in terms of their correlations and relationships,
answering the questions raised in Sect. 2.2. However, our current prototype sys-
tem does not allow updating data in editing mode back to the data storage sys-
tem yet. This is a challenge because we need mechanism to identify the eﬀected
messages as well as the solution for re-computing the correlations if the ana-
lysts make change on the data. This is also a limitation when a message needs
re-computation by the semantic processing engine to improve the correctness of
the mapping terms.

116
T.-D. Cao et al.
Regarding the visualization, we also have not provided solutions in the case
that (1) we need to show a lot of information on a message or a node under
examination (currently we just support showing tooltip), (2) we need to provide
a clearer view when having so much nodes or links on screen (such as how to
arrange the tooltips well instead of the overlapping ones presented on Fig. 5b).
6
Related Work
Visual analytics for real-time streaming text from the crowd is an active area of
research. Marcus et al. develop a system for sentiment analysis and visualization
from Twitter tweets [18]. The goal of that system is to identify signiﬁcant events
and sentiment in the streaming text. Frequency of searched keywords and senti-
ment classiﬁers over time windows are employed for peaks of high tweet activity
aggregation. And events are displayed on a timeline-based visualization, where
the users can interactively focus + context drill down to subevents and explore
further relevant data, such as from geographical maps or with popular URLs.
Ren et al. meanwhile leverage supports from the masses in analyzing events from
Weibo microblog streams [23]. The crowd sourcing feedbacks and comments from
users are collected using predeﬁned criteria, which are combined with the statis-
tics on how tweets and retweets are propagated to form the clusters of events. The
clusters are then tree-based displayed, such as through connected graphs, and
combined with other views for the experts to examine. They are some amongst
common work focusing on statistical analysis and visually support information
exploration using multiple views.
Gansner et al. [10], however, follow a diﬀerent approach. They compute
the similarity of Twitter messages based on semantic analysis – using Latent
Dirichlet Allocation model, then cluster and display the message relations as
clusters of nodes on a dynamic graph layout and graph-based stable packing
algorithm. Another work is the TextWheel system developed by Cui et al. [8].
Sentiment and semantic relations between selected keywords and news articles
are analyzed; then, patterns about those relations in terms of keywords are visu-
alized as a wheel, connected to a so-called document transportation belt for
articles, together with a trend chart for sentiment changes. The approach of
these two techniques are quite similar to ours: focusing on semantics of messages
or keywords relations to ﬁnd out the important information, and visualizations
are developed to communicate those relations. The diﬀerence is that they focus
only on few direct relations (i.e., between semantic relations of messages to
messages, or between keywords and articles), while ours examine many types
and impacts of relationships between messages and their associated user-deﬁned
tables, especially the aggregated direct + indirect connection relations.
In the next two subsections, we present related work regarding our algorithms
for message correlation computation and visualization development.

Exploring Message Correlation
117
6.1
Message Correlation
Message correlation is the process of ﬁnding relationships between messages that
belong to the same group with several purposes such as: ﬁnding messages belong
to the same process execution instance, classifying faults in a systems, reducing
storage data volume, or cohering many log ﬁles of a distributed system, etc. In the
last years, several approaches of message/event correlation have been proposed.
Some works such as [19,22,25] focus on ﬁnding correlation of events/messages for
a process of business or protocol interaction that allows users to eﬀectively navi-
gate through the process space and identify the ones of interest. While the other
works [21,31] analyze correlation of messages of log ﬁles of distributed systems
or clouds to minimize the log number without missing necessary information for
later analysis. Jakobson and Weissman [16] also focus on the same problem by
proposing a framework to ﬁnd the correlation among alarms of telecommunica-
tion network for class hierarchy and reducing the number of alarms displayed
on operators’ terminals. Focusing on data replication among datacenters in geo-
cloud environment and diﬃculty of determination of the replicas location, Ye
et al. [34] also propose a data correlation for a stream based similarity cluster-
ing, which uses a small number of micro clusters to represent huge number of
users and thus signiﬁcantly reducing the cost of replica placement algorithm. In
these works, although their approaches are either text-based search or semantic
analysis of semi-structure format, the correlation condition is computed using
direct relationship among messages. It means if two messages have a correlation,
they must share at least a common content (e.g., a word). However, in many case,
if the structure of message is well deﬁned with additional information, they still
have a correlation (i.e., indirect relation) even they do not share any common
content. This problem is not considered in these works.
6.2
Visualization
In our visualization development, we design to communicate (i) the overview
of correlations of messages in a collection, and (ii) the details of relationships
between messages and elements on relation tables that form the correlations.
There are various techniques dealing with the ﬁrst issue about how inter-
related elements in a collection are visually communicated. Since the elements
are connected together as a network, node-link graph is the classical mean to
represent the elements and the connections. The visualizations developed in the
above mentioned systems [18,23] use this common representation. Basically, size
or color of the nodes are used to represent node’s importance in the network,
while the positions are calculated based on the relative distances between the
nodes. Besides, directions of connections are also representable as link arrows
[30]. The elements can be grouped into or expanded from subsets for diﬀerent
zooming levels as well, which typically used when the groups of clusters of ele-
ments are separatable [17], or augmented with associated data through node
extensions as in [29]. However, this is not applicable for elements belonging to
diﬀerent clusters due to diﬀerent types of relationships as in our case.

118
T.-D. Cao et al.
The second classical technique is to encode the connection between every
pair of network elements as a cell in a matrix (elements are indicated twice as
rows and cells of the matrix). Visual matrices are introduced in the “Semiology of
graphics” of Bertin [3]. Ghoniem et al. [11] show that matrices outperform node-
link diagrams for large graphs or dense graphs in various low-level reading tasks,
except path ﬁnding. This technique therefore communicate well the overview of
message correlations, but multiple views are needed when additional data are
explored – using brushing and linking technique. A modiﬁcation is to show half
matrix to reduce visual ink and display space. Elements can also be placed along
an axis [32] or a circle [14], and connected by arc paths. With the axis-based arc
diagram, it completely satisﬁes as a coordinate in the parallel coordinates that
we develop.
The second issue in our development deals with multivariate data representa-
tion. One of the most popular techniques is the scatterplots matrix, which is an
array of scatterplots displaying all possible pairwise combinations of variables or
coordinates. This technique has been in use long before its publication [2]. It is
popular for multidimensional visualization due to the relative simplicity in com-
parison to other multidimensional visualization techniques, familiarity among
users, and for high visual clarity, as discussed in [9]. Further on multivariate
visualization techniques can be found in [12,33], where parallel coordinates [15]
is another popular technique that suits well our development.
Parallel coordinates maps points in Cartesian coordinates into lines con-
nected the vertical parallel line coordinates. This technique is used vastly for
correlation representation among variables in multivariate data analysis, since it
leverages the geometrical aspects of the presentation: the convergence or ﬂow of
lines communicate well relationships between many pairs of variables. From that,
various extension techniques has been developed for speciﬁc contexts. Tominski
et al. outline the parallel axes into the radical axes to support additional char-
acteristics of data and tasks [28]. Collins and Carpendale suggest to links visual-
ized 2D planes instead of just 1D coordinates [5]. In addition, local relationships
between connected lines can also be grouped for clusters examination [13]. For
text visualization, parallel word clouds [6] is the extension of parallel coordinates
for facets of text corpora analysis, and hyper word clouds [20] is developed based
on the parallel word clouds for text-based collections with four diﬀerent types
of data relationships.
7
Conclusion
This paper targeted a problem of visual analytics in ﬁnancial markets domain,
in which data are collected from the crowd with chaotic and unwell-modeled for
its speciﬁc usage. Though machine learning and artiﬁcial intelligence so far have
made the problem of analyzing such crowd-based data to be more realistic, it is
not convenient for the analysts to explore them without visualization. In that
context, we have developed the visualization technique Hyper Coordinates for
correlation messages examination and veriﬁcation. Supporting for this technique,

Exploring Message Correlation
119
we deﬁned a message correlation computation approach based on the deﬁnition
of direct and indirect connection relations between multivariate items in complex
data set, as well as mapping those relationship concepts together with the four
types of data relationships (i.e., connection relation, set relation, ordered relation
and quantitative relation) for the visualization. Our showcase has shown that
domain experts can explore their data from overview of clusters to details of
messages as well as interact with them to perform other tasks. The showcase
also shows us how the correlation is established among message as well as how
direct and indirect relations are distinguishable.
In the showcase, we demonstrated most of visualization development pro-
vided in Sect. 4. However, our visualization framework still hardcodes for our
speciﬁc case study and thus some functions are still missing. In the future we
thus plan to complete it and release a general framework for users to use with
any dataset that follows our deﬁnition.
Acknowledgment. The authors would like to thank Quang M. Le and Tuan A. Ta
at SentiﬁAG (Ho Chi Minh City oﬃce, Vietnam) for their supports and discussions
on working scenario.
References
1. Spark: Lightning-fast cluster computing. http://spark.apache.org
2. Andrews, D.F.: Plots of high-dimensional data. Biometrics 28(1), 125–136 (1972)
3. Bertin, J.: Semiology of Graphics: Diagrams, Networks, Maps. University of Wis-
consin Press, Madison (1983)
4. Brehmer, M., Munzner, T.: A multi-level typology of abstract visualization tasks.
IEEE Trans. Vis. Comput. Graph. 19(12), 2376–2385 (2013)
5. Collin, C., Carpendale, S.: VisLink: revealing relationships amongst visualizations.
IEEE Trans. Vis. Comput. Graph. 13(6), 1192–1199 (2007)
6. Collins, C., Viegas, F.B., Wattenberg, M.: Parallel tag clouds to explore and ana-
lyze faceted text corpora. In: IEEE Symposium on VAST 2009, pp. 91–98, October
2009
7. Collins, C., Penn, G., Carpendale, S.: Bubble sets: revealing set relations with
isocontours over existing visualizations. IEEE Trans. Vis. Comput. Graph. 15(6),
1009–1016 (2009)
8. Cui, W., Qu, H., Zhou, H., Zhang, W., Skiena, S.: Watch the story unfold with
textWheel: visualization of large-scale news streams. ACM Trans. Intell. Syst.
Technol. 3(2), 20:1–20:17 (2012)
9. Elmqvist, N., Dragicevic, P., Fekete, J.D.: Rolling the dice: multidimensional visual
exploration using scatterplot matrix navigation. IEEE Trans. Vis. Comput. Graph.
14(6), 1148–1539 (2008)
10. Gansner, E.R., Yifan, H., North, S.C.: Interactive visualization of streaming text
data with dynamic maps. J. Graph Algorithms Appl. 17(4), 515–540 (2013)
11. Ghoniem, M., Fekete, J.-D., Castagliola, P.: On the readability of graphs using
node-link and matrix-based representations: a controlled experiment and statisti-
cal analysis. Inf. Vis. 4(2), 114–135 (2005)
12. Grinstein, G., Trutschl, M., Cvek, U.: High-dimensional visualizations. In: Pro-
ceedings of the Visual Data Mining Workshop, KDD (2001)

120
T.-D. Cao et al.
13. Guo, P., Xiao, H., Wang, Z., Yuan, X.: Interactive local clustering operations for
high dimensional data in parallel coordinates. In: 2010 IEEE Paciﬁc Visualization
Symposium (PaciﬁcVis), pp. 97–104, March 2010
14. Holten, D.: Hierarchical edge bundles: visualization of adjacency relations in hier-
archical data. IEEE Trans. Vis. Comput. Graph. 12(5), 741–748 (2006)
15. Inselberg, A.: The plane with parallel coordinates. Vis. Comput. 1(2), 69–91 (1985)
16. Jakobson, G., Weissman, M.: Alarm correlation. IEEE Netw. 7(6), 52–59 (1993)
17. Kim, K., Ko, S., Elmqvist, N., Ebert, D.S.: WordBridge: using composite tag
clouds in node-link diagrams for visualizing content and relations in text corpora.
In: 2011 44th Hawaii International Conference on System Sciences (HICSS), pp.
1–8, January 2011
18. Marcus, A., Bernstein, M.S., Badar, O., Karger, D.R., Madden, S., Miller, R.C.:
TwitInfo: aggregating and visualizing microblogs for event exploration. In: Pro-
ceedings of the SIGCHI Conference on Human Factors in Computing Systems
(CHI 2011), pp. 227–236, New York. ACM (2011)
19. Motahari-Nezhad, H.R., Saint-Paul, R., Casati, F., Benatallah, B.: Event corre-
lation for process discovery from web service interaction logs. Int. J. Very Large
Data Bases 20(3), 417–444 (2011)
20. Nguyen, D.Q., Le, D.D.: Hyper word clouds: a visualization technique for last.fm
data and relationships examination. In: Proceedings of the 10th International Con-
ference on Ubiquitous Information Management and Communication (IMCOM
2016), pp. 66:1–66:7, New York. ACM (2016)
21. Pape, C., Reissmann, S., Rieger, S.: Restful correlation and consolidation of dis-
tributed logging data in cloud environments. In: The 8th International Conference
on Internet and Web Applications and Services, pp. 194–199 (2013)
22. Reguieg, H., Toumani, F., Motahari-Nezhad, H.R., Benatallah, B.: Using Mapre-
duce to scale events correlation discovery for business processes mining. In: 10th
International Conference Business Process Management, pp. 279–284 (2012)
23. Ren, D., Zhang, X., Wang, Z., Li, J., Yuan, X.: WeiboEvents: a crowd sourcing
Weibo visual analytic system. In: Proceedings of the 2014 IEEE Paciﬁc Visual-
ization Symposium (PACIFICVIS 2014), Washington, DC, pp. 330–334. IEEE
Computer Society (2014)
24. Sarkar, M., Brown, M.H.: Graphical ﬁsheye views of graphs. In: Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems (CHI 1992),
New York, pp. 83–91. ACM (1992)
25. Serrour, B., Gasparotto, D.P., Kheddouci, H., Benatallah, B.: Message correlation
and business protocol discovery in service interaction logs. In: 20th International
Conference Advanced Information Systems Engineering, pp. 405–419 (2008)
26. Shneiderman, B.: The eyes have it: a task by data type taxonomy for information
visualizations. In: Proceedings of the 1996 IEEE Symposium on Visual Languages,
Washington, DC, USA, pp. 336–343. IEEE Computer Society, September 1996
27. Thomas, J.J., Cook, K.A. (eds.): Illuminating the Path: The Research and Devel-
opment Agenda for Visual Analytics. IEEE CS Press, Los Alamitos (2005)
28. Tominski, C., Abello, J., Schumann, H.: Axes-based visualizations with radial
layouts. In: Proceedings of the 2004 ACM Symposium on Applied Computing
(SAC 2004), New York, pp. 1242–1247. ACM (2004)
29. van Ham, F., Perer, A.: Search, show context, expand on demand: supporting
large graph exploration with degree-of-interest. IEEE Trans. Vis. Comput. Graph.
15(6), 953–960 (2009)
30. van Ham, F., Wattenberg, M., Viegas, F.B.: Mapping text with phrase nets. IEEE
Trans. Vis. Comput. Graph. 15(6), 1169–1176 (2009)

Exploring Message Correlation
121
31. Wang, M., Holub, V., Parsons, T., Murphy, J., OSullivan, P.: Scalable run-time
correlation engine for monitoring in a cloud computing environment. In: 17th
IEEE International Conference and Workshops on Engineering of Computer Based
Systems, pp. 29–38 (2010)
32. Wattenberg, M.: Arc diagrams: visualizing structure in strings. In: Proceedings of
the IEEE Symposium on Information Visualization (InfoVis 2002), Washington,
DC, pp. 110–116. IEEE Computer Society (2002)
33. Wong, P.C., Bergeron, R.D.: 30 years of multidimensional multivariate visual-
ization. In: Scientiﬁc Visualization, Overviews, Methodologies, and Techniques,
Washington, DC, pp. 3–33. IEEE Computer Society (1997)
34. Ye, Z., Li, S., Zhou, X.: GCplace: geo-cloud based correlation aware data replica
placement. In: The 28th Annual ACM Symposium on Applied Computing, pp.
371–376 (2013)

Bayesian Forecasting for Tail Risk
Cathy W. S. Chen(B) and Yu-Wen Sun
Department of Statistics, Feng Chia University, Taichung, Taiwan
chenws@mail.fcu.edu.tw, sunyuwun@gmail.com
Abstract. This paper evaluates the performances of Value-at-Risk
(VaR) and expected shortfall, as well as volatility forecasts in a class of
risk models, speciﬁcally focusing on GARCH, integrated GARCH, and
asymmetric GARCH models (GJR-GARCH, exponential GARCH, and
smooth transition GARCH models). Most of the models incorporate four
error probability distributions: Gaussian, Student’s t, skew Student’s t,
and generalized error distribution (GED). We employ Bayesian Markov
chain Monte Carlo sampling methods for estimation and forecasting.
We further present backtesting measures for both VaR and expected
shortfall forecasts and implement two loss functions to evaluate volatil-
ity forecasts. The empirical results are based on the S&P500 in the U.S.
and Japan’s Nikkei 225. A VaR forecasting study reveals that at the 1%
level the smooth transition model with a second-order logistic function
and skew Student’s t error compares most favorably in terms of viola-
tion rates for both markets. For the volatility predictive abilities, the
EGARCH model with GED error is the best model in both markets.
Keywords: Backtesting · Expected shortfall
Skew Student’s t distribution · Smooth transition GARCH model
Second-order logistic function · Markov chain Monte Carlo methods
Value-at-Risk
1
Introduction
Investors regard risk management performance as one of the main criteria for
investment. Value-at-Risk (VaR) is a common risk measurement used for sub-
sequent capital allocation for ﬁnancial institutions worldwide, as chosen by the
Basel Committee on Banking Supervision. The benchmark of VaR, ﬁrst proposed
by J. P. Morgan [30], helps investors compare risk under diﬀerent portfolios (see
Duﬃe and Pan [14]) and also increases their investment conﬁdence.
According to regulations, central banks require that ﬁnancial institutions
deposit a certain amount of reserves in order to ensure that they have enough
money to cover any risk in the future and to prevent the bank from going bank-
rupt. For insurance companies, VaR denotes the threshold of the amounts under
claim, whereby the claims less than VaR represent the amount of compensation
a company can aﬀord; otherwise, we take it as the amount of reinsurance. For
a given portfolio, time horizon, and probability p, VaR denotes as a threshold
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_6

Bayesian Forecasting for Tail Risk
123
loss value, such that the probability that the loss on the portfolio over the given
time horizon exceeds this value is p. Although VaR is a popular measurement for
determining regulatory capital requirements, it suﬀers a number of weaknesses,
including its inability to capture “tail risk”.
The literature classiﬁes the method for calculating the VaR value into three
types. First, some ﬁnancial institutions employ sample return quantiles, called
historical simulation (HS), for nonparametric estimation of VaR (see Hendricks
[26]). Second, Engle and Manganelli [17] propose conditional autoregressive value
at risk (CAViaR), which is based on dynamic quantile regression. Gerlach, Chen
and Chan [22] further investigate the nonlinear CAViaR model to estimate quan-
tiles (VaR) directly. Third, parametric GARCH-type models, such as the autore-
gressive conditional heteroscedastic (ARCH) model of Engle [16] and the gener-
alized ARCH (GARCH) model of Bollerslev [5] have speciﬁed error distributions
that are well suited to quantile forecasting. VaR studies also use an IGARCH
(integrated generalized autoregressive conditional heteroscedastic) model and
RiskMetrics as benchmarks. However, it is widely known that volatility reacts
diﬀerently to a big price increase and a big price drop with the latter hav-
ing a greater impact. This phenomenon describes the leverage eﬀect. Volatility
asymmetry, as presented by Black [4], and other types of nonlinearity in ﬁnancial
data have received much attention recently. Nelson [31] proposes a model to cap-
ture asymmetric volatility, called the exponential GARCH model (EGARCH).
Glosten et al. [20] oﬀer another asymmetric GARCH model, popularly known as
the GJR-GARCH model, to capture asymmetric volatility via an indicator term
in the GARCH equation. Chen et al. [11] focus on nonlinear GARCH model
with variant smooth transition functions: ﬁrst- and second-order logistic func-
tions, and the exponential function.
The ﬁrst objective of this study is to employ the above-mentioned paramet-
ric models and HS approaches for forecasting VaR. We consider the family of
GARCH-type model with four error probability normal, Student’s t, the skew
Student’s t, and generalized error distribution (GED). Additionally, we forecast
tail risk based on a smooth transition (ST) GARCH model with a second-order
logistic function and both Student’s t errors. Chen et al. [11] show at the 1% level
that the ST model with a second-order logistic function and skew Student’s t
error is a worthy choice for VaR, when compared to a range of existing alterna-
tives. This model is worth our attention, and thus should include it in our risk
model.
There are several limitations from using VaR for determining regulatory cap-
ital requirements, including its inability to catch tail risk. For this reason, the
Basel Committee on Banking Supervision has considered alternative risk met-
rics, in particular expected shortfall (ES). ES denotes the expected value of the
return less than VaR with a signiﬁcant α at time t; it is also called conditional
tail expectation (see Acerbi and Tasche [1]. Artzner et al. [2] show that VaR
lacks a subadditive property, because the risk scatters over a portfolio, and thus
we can see that VaR is not coherent. Fllmer and Schied [19] note that ES is
coherent, and so ES is more useful than VaR. Many ﬁnancial institutions and

124
C. W. S. Chen and Y.-W. Sun
practitioners reply on VaR to evaluate the quantile forecast of ﬁnancial returns,
but it only measures a quantile of the distribution, while ignoring the important
facts contained in the tail beyond that quantile.
In order to make a better assessment of potential losses, the second objective
of this study is to calculate ES. Yamai and Yoshiba [35] compare VaR with
ES and prove that ES is a better risk measure than VaR in terms of tail risk.
Harmantzis et al. [25] also emphasize the tail factor with VaR and ES. Gerlach
and Chen [23] consider dynamic expectile and ES modeling and forecasting,
incorporating information from the daily range.
The problem of computing the integrals of ES when the error term follows a
non-Gausian distribution is quite complicated. To solve this problem eﬃciently
we adopt Bayesian Markov chain Monte Carlo (MCMC) methods. We employ
Bayesian methods to estimate model parameters via the GARCH-type models,
because they describe uncertainty of a statement about an unknown parame-
ter in terms of probability. Bayesian qunatile forecasting provides adequate VaR
forecasts (see Chen et al. [8,11,12]; Gerlach et al. [22]). Hoogerheide and van Dijk
[27] compare VaR and ES in a Bayesian framework and consider the Bayesian
predictive density. Within the Bayesian framework, Chen et al. [9] forecat VaR
depending on a range of parametric models. Chen, Weng, and Watanabe [11]
evaluate VaR based on various ST GARCH models, but they do not deal with
ES in their studies. Therefore, the target of this study is to evaluate both per-
formances of VaR and ES forecasting.
The MCMC methods enable us to solve complicated high dimensional models
and more eﬀectively estimate parameters. The advantages of the Bayesian app-
roach using MCMC methods are as follows. (i) The likelihood function is condi-
tional on the unobserved variables to compute the posterior distribution, making
the calculation of the parameters faster than the traditional maximum likelihood
estimation (see Papp [32]). (ii) This approach solves complex high dimensional
models and more eﬀectively estimate parameters. (iii) Bayesian MCMC methods
allow simultaneous inferences for all unknown parameters.
Common criteria to compare VaR models are the rate of violation and back-
testing. The rate of violation is the proportion of observations for which the
actual return is more extreme than the forecasted VaR level over a forecast
period. A forecast model’s violation rate (VRate) should be close to the nominal
level α. We study three hypothesis-testing methods for evaluating and testing
the accuracy of VaR models: (1) the unconditional coverage (UC) test of Kupiec
[29]: a likelihood ratio test whereby the true violation rate equals α; (2) the
conditional coverage (CC) test of Christoﬀersen [13]: a joint test combining a
likelihood ratio test for independence of violations and the UC test; and (3) the
Dynamic Quantile (DQ) test of Engle and Manganelli [17]. Both CC and DQ
are joint tests of the independence of a model’s violations and a proof of the UC
test, whereby the true violation equals the signiﬁcant α. The DQ test is generally
more powerful than the CC test (see Berkowitz et al. [3]).
We evaluate ES following Embrechts et al. [15], in order to deal with the
weakness of the violations that depend strongly on the VaR estimates without

Bayesian Forecasting for Tail Risk
125
suﬃciently reﬂecting the goodness or badness of these values. We apply the model
to daily returns of the S&P500 in the U.S. and Japan’s Nikkei 225. We evaluate
the volatility forecasts of all parametric models by mean squared error (MSE)
and quasi-likelihood (QLIKE). The prediction results show that the asymmetric
model with skew and fat tailed distribution improves quantile forecasts.
We organize this paper as follows. Section 2 illustrates a smooth transi-
tion heteroskedastic model. Section 3 demonstrates the Bayesian Approach via
the MCMC method. Section 4 describes the process of VaR and ES forecasts.
Section 5 mentions several methods to evaluate the volatility forecasts and quan-
tile forecasts. Section 6 lists analytic results, using the U.S. and Japanese stock
indices. Section 7 provides concluding remarks.
2
The Smooth Transition Heteroskedastic Model
This section describes a double ST GARCH model proposed by Chen et al. [11]
since it is the most complicated model in terms of estimation. This model is
capable of capturing mean and volatility asymmetry in ﬁnancial markets. We
use a second-order ST function that ensures the mean and volatility parameters
are smooth functions of past news or volatility. Chen et al. [11] show that the
ST model with a second-order logistic function and skew Student’s t error is a
worthwhile choice for VaR forecasting.
Suppose that {rt} represents the observation data. To incorporate diﬀerent
speeds of smooth transition functions for the mean and variance, we present the
time-varying ST GARCH model as:
rt = μ(1)
t
+ F(zt−d; γ1, c)μ(2)
t
+ at
(1)
at =

htεt,
εt
i.i.d.
∼D(0, 1),
ht = h(1)
t
+ F(zt−d; γ2, c)h(2)
t
μ(l)
t
= φ(l)
0 +
p

i=1
φ(l)
i rt−i
h(l)
t
= α(l)
0 +
g

i=1
α(l)
i a2
t−i +
q

i=1
β(l)
i ht−i,
l = 1, 2,
where μ(l)
t
and h(l)
t
are the respective conditional mean and volatility at regime l;
zt is the threshold variable; d is the delay lag; and D(0, 1) is an error distribution
with mean 0 and variance 1. We can choose lagged returns, or an exogenous vari-
able, e.g. other asset return, as the threshold zt. The time-varying ST GARCH
model in (1) highlights the model’s characteristic, which allows for an ST func-
tion with varying speed in the mean and variance. We consider a speciﬁcation
of the second-order logistic function in van Dijk et al. [34].
F(zt−d; γi, c) =
1
1 + exp

−γi(zt−d−c1)(zt−d−c2)
sz
, i = 1, 2
c1 < c2,

126
C. W. S. Chen and Y.-W. Sun
where c = (c1, c2)′, as proposed by Jansen and Ter¨asvirta [28]. Given a point
in time t, rt corresponds to a weighted average of two AR-GARCH models,
where the weights assigned to the two models depend on the values taken by
the transition functions F(zt−d; γi, c), i = 1, 2. The parameter γi determines
the smoothness of the change in the value of the F(zt−d; γi, c) function and the
smoothness of the transition from one regime to the other.
Chen et al. [11] discuss some properties of the ST GARCH model. For exam-
ple, when γ1 →0, the logistic functions is equal to a constant (equal to 0.5),
and the ST GARCH model reduces to a linear AR-GARCH model. When γ2
in the transition function is moderately large (not necessarily going to inﬁnity)
and c1 ̸= c2, then the ST GARCH model becomes the three-regime threshold
GARCH model of Chen et al. [10].
Note that the ST GARCH model contains the lagged AR(1) (i.e. p = 1) eﬀect
in each regime, which allows one to recognize whether the return series exhibits
asymmetry mean reversion or market eﬃciency.For the ST GARCH(1,1) model
in (1), we allow an explosive lower regime for the model. The restrictions for
positiveness and covariance stationarity are as follows:
α(i)
0 , α(i)
1 , β(i)
1
> 0, i = 1, 2
(2)

α(1)
1
+ α(2)
1

> 0,

β(1)
1
+ β(2)
1

> 0,

α(1)
1
+ 0.5α(2)
1

+

β(1)
1
+ 0.5β(2)
1

< 1,
(3)
α(1)
0
< b1, β(1)
1
< b2, α(1)
1
+ β(1)
1
< b3,
(4)
where b1, b2, and b3 are user-speciﬁed. In this study, we let b2, b3 ≥1 to allow
for explosive behavior.
For the purpose of VaR and ES studies, we also consider standard GARCH
model, IGARCH model, and RiskMetrics, as well as asymmetric GARCH
models - GJR-GARCH and EGARCH models. Appendix A gives the descrip-
tions of these models.
3
Bayesian Approach
We present a general likelihood functional form for any GARCH model, which
is:
L (r | θ) =
n

t
 1
√ht
pε
(rt −μt)
√ht

,
(5)
where r = (r1, . . . , rn)′, θ is the full parameter vector for any of the combinations
of model and error distribution considered, (μt, ht) are the mean and volatility
at time t, and pε(·) is the error density function for εt.
There are many versions of skew Student’s t distribution, such as in Hansen
[24] and Fern´andez and Steel [18]. Both versions introduce skewness into any
continuous unimodal and symmetric (with respect to 0) univariate distribution.

Bayesian Forecasting for Tail Risk
127
We adopt the approach of Hansen [24], denoted by St(η, ν), which has zero mean
and unit variance. The probability density function of skew Student’s t deﬁned
by Hansen [24] is as follows:
pε(εt|ν, η) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
bc

1 +
1
ν−2

bεt+a
1−η
2−(ν+1)/2
if
εt < −a
b
bc

1 +
1
ν−2

bεt+a
1+η
2−(ν+1)/2
if
εt ≥−a
b ,
(6)
where degrees of freedom ν and skewness parameter η satisfy 2 < ν < ∞, and
−1 < η < 1, respectively. We set the constants a, b, and c as:
a = 4ηc

ν−2
ν−1

, b2 = 1+3η2−a2, and c = Γ
 ν+1
2
 
Γ(ν/2)

π(ν −2)
−1
, where
Γ denotes the gamma function. The skew Student’s t distribution of Hansen [24]
includes Gaussian and Student’s t distributions as special cases.
GED is known as generalized normal distribution, which is a symmetric fam-
ily of distributions used in risk models, e.g. Chen et al. [9]. The shape of GED is
diﬀerent from a normal distribution. The GED can include the normal distrib-
ution and the Laplace distribution as special cases. The density function for εt,
a standardized GED with scale parameter σ, is:
pε(εt) =
λ
2σΓ(1/λ) exp

−
εt
σ

λ
,
(7)
where σ = [Γ( 1
λ)/Γ( 3
λ)]0.5, and λ ∈(0, ∞) is the tail-behaviour determining
parameter. When λ > 2, the distribution has thinner tails than a normal dis-
tribution. When λ = 2, it is exactly a normal distribution with mean 0 and
standard error σ; while for λ < 2, the distribution has positive excess kurtosis
relative to the normal distribution; see Figure 1 for this information. For real
asset return data, we expect λ < 2.
3.1
The MCMC Methods
We assume all parameters θ = (φ1, φ2, α1, α2, c, γ, ν, d) of the ST GARCH
model as in Eq. (1), which are a priori independent. Following the idea of Chen
et al. [11], we set up the priors as follows. We deﬁne the latent variable δ(i)
j ,
which determines the prior distribution of φ(i)
j , via a mixture of two normals:
φ(i)
j |δ(i)
j
∼(1 −δ(i)
j )N(0, k2τ (i)
j
2) + δ(i)
j N(0, τ (i)
j
2),
j = 1, . . . , p
δ(i)
j |γ1 =
1, if i = 1 or γ1 > ξ
0, if i = 2 and γ1 ≤ξ,
(8)
where i = 1, 2 denotes the regime, and j = 1, . . . , p denotes the lag order of the
AR mean terms in φj. Here, ξ is a speciﬁed threshold, and γ1 ≤ξ indicates that

128
C. W. S. Chen and Y.-W. Sun
Fig. 1. Shape of GED for λ = (0.5, 1, 2).
F(zt−d; γ1, c) →0.5; that is, an AR-GARCH model. As Gerlach and Chen [21]
suggest, we choose k to be a small positive value, so that if γ1 ≤ξ and δ(2)
j
= 0,
then the posterior value for the parameters φ(2)
j
will be weighted by the prior
value towards 0.
We take a constrained uniform prior for p(α), with the constraint deﬁned
by the indicator I(S), where S deﬁnes the constraints in Eqs. (2), (3), and (4).
We suggest the log-normal prior for γi, where γi ∼LN(μγi, σ2
γi). We choose a
discrete uniform prior for d, Pr(d) = 1/d0, where d = 1, . . . , d0. For ν degrees of
freedom, we deﬁne ρ = ν−1 and set it to I(ρ ∈[0, 0.25]) (see Chen et al. [7], for
more details). For the skew parameter, we set a ﬂat prior over η ∈(−1, 1). The
general priors for c1 and c2 are:
c1 ∼Unif(lb1, ub1);
c2|c1 ∼Unif(lb2, ub2),
where lb1 and ub1 are the ℘h1 and ℘1−h1−h2 percentiles of zt, respectively. For
example, if h1 = h2 = 0.1, then c1 ∈(℘0.1, ℘0.8). Furthermore, we set ub2 =
℘(1−h2) and lb2 = c1 +c∗, where c∗is a selected number that ensures c1 +c∗≤c2
and at least 100h2% of observations are in the range (c1, c2).
The conditional posterior distribution for each group is proportional to the
product of the likelihood function and the priors:
p(θl | ys+1,n, θ̸=l) ∝p(ys+1,n | θ) · p(θl | θ̸=l),
where θl is a parameter group, p(θl) is its prior density, and θ̸=l is the vector of
all model parameters, except for θl.

Bayesian Forecasting for Tail Risk
129
Except for the delay parameter d, the rest of the conditional posterior dis-
tributions are not standard. We use the adaptive Metropolis-Hastings (MH)
algorithm, which combines the random walk MH (RW-MH) algorithm (before
burn-in period) and the independent kernel MH (IK-MH) algorithm (after burn-
in period) (see Chen and So [6]). Appendix B gives the description of adaptive
MH algorithm. Convergence is monitored heavily using trace and ACF plots. We
set iteration frequency N samples and burn-in frequency M samples, but take
only every second iterate in the sample period for inference.
4
Bayesian Forecasting of Value-at-Risk and Expected
Shortfall
This section generates the one-day-ahead volatility and predicted return based
on the MCMC algorithm. We use the ST GARCH model with skew Student’s
t error as an example. The procedure to compute the one-day-ahead volatility
hn+1 and return rn+1 is given as follows.
1. Obtain μ[j]
n+1|θ[j] and h[j]
n+1|θ[j] using the in-sample data r.
2. Generate εn+1 ∼St(η, ν).
3. Calculate r[j]
n+1 = μ[j]
n+1 +

h[j]
n+1εn+1 and go to Step 1.
VaR is a very popular risk measure, which generally denotes the amount of
capital that the banks need to prepare. We assume a long position, such that
VaR forecast satisﬁes:
P(rn+1 < −VaRn+1|Fn) = α,
(9)
where Fn is the information available up to n; we typically let α be one percentile
or ﬁve percentile. A one-step-ahead VaR is the α-level quantile of the conditional
distribution rn+1|Fn ∼D(μn+1, hn+1), where D is the relevant error distribu-
tion. We compute the predictive distribution via the MCMC simulation and give
the quantile VaR by:
VaR[j]
n+1 = −

μ[j]
n+1 + D−1
α (θ[j])

h[j]
n+1

,
(10)
where D−1
α
is the inverse CDF of the distribution D. For standardized Student’s
t errors:
D−1
α
=
tα(ν[j])

ν[j]/(ν[j] −2)
,
where tα(ν[j]) is a value that follows Student’s t distribution with degrees of
freedom ν[j], and

ν[j]/(ν[j] −2) is an adjustment term for a standardized
Student’s t. Alternatively, one can compute VaRn+1 as the α-percentile of the

130
C. W. S. Chen and Y.-W. Sun
MCMC sample of rn+1 (see Takahashi et al. [33]). The forecasted one-step-ahead
VaR is the Monte Carlo posterior mean estimate
VaRn+1 =
1
N −M
N

j=M+1
VaR[j]
n+1.
(11)
Because we do not know the severity of the tail of rn+1, we compute ES for
evaluating the average of the worst outcomes of a probability distribution. The
one-step-ahead forecast of the ES with probability α satisﬁes
ESn+1 = E [rn+1|rn+1 < −VaRn+1] .
(12)
When rn+1 follows a normal distribution, we can easily compute ES with a closed
form, but the reality is that rn+1 is typically skewed and fat tailed. The integral
part in (12) would undoubtedly be much more challenging in nonlinear dynamic
models with skewed errors. As such, a Bayesian approach can overcome these
diﬃculties. This procedure executes the ES that is approximately obtainable by
using a MCMC technique.
To compute the tail of rn+1, we ﬁnd rn+1 that satisﬁes the condition and
then compute its expected value. To compute ES in (12), we provide a numerical
approximation of ES by using an adaptive MCMC scheme.
Step 1. Compute either VaRn+1 based on (11) or VaRn+1 as the α percentile
of rn+1.
Step 2. Generate one-step-ahead return r[j]
n+1|Fn, θ[j] from D(μ[j]
n+1, h[j]
n+1),
where D is the relevant error distribution.
Step 3.
If

r[j]
n+1 < −VaRn+1

is true,
then we save r[j]
n+1, for j = M +
1, . . . , N.
Step 4. Compute the average of r[j]
n+1, which satisﬁes the condition in Step 3.
Let n and m be the numbers of samples for the estimation and prediction,
respectively. We then estimate the one-day-ahead forecasts of VaR and ES based
on the rolling window approach.
5
Evaluation of Volatility and Quantile Forecasts
We evaluate the volatility forecasts of all models using two loss functions: mean
squared error (MSE) and quasi-likelihood (QLIKE). Let ˆσ2
t and ht be a volatility
proxy and a volatility forecast, respectively, and consider the two loss functions
LMSE
t
= (ˆσ2
t −ht)
2
2
,
MSE =
m

i=1
LMSE
t
,
(13)
LQLIKE
t
= ˆσ2
t
ht
−log ˆσ2
t
ht
−1,
QLIKE =
m

i=1
LQLIKE
t
,
(14)

Bayesian Forecasting for Tail Risk
131
where ˆσ2
t is a realized kernel that we download from the Oxford-Man Institute
“Realised Library”, and m is the out-of sample size.
Evaluation of Value-at-Risk
As a simple way to evaluate VaR, we compute the violation rate in the form:
VRate = 1
m
n+m

t=n+1
I (rt < −VaRt),
(15)
where n is the in-sample size, and m is the out-of-sample size. The result of
VRate should be close to the nominal level α. Furthermore, we prefer a model
with overestimated risk than one that underestimates risk.
Backtesting methods
We further consider three backtesting methods for evaluating and testing the
accuracy of the VaR models. We describe these three hypothesis-testing methods
as follows.
The UC test of Kupiec [29]: With null hypothesis H0 : α = α0, the likelihood
ratio test proposed by Kupiec [29] has the form:
LRuc = 2 · log
 ˆαX(1 −ˆα)m−X
αX(1 −α)m−X

∼χ2
1,
where X = number of violations, m = forecast period size, and ˆα = X/m.
The CC test of Christoﬀersen [13]: The CC test is a joint test that combines
a likelihood ratio test for independence of violations and the UC test, where the
independence hypothesis stands for VaR violations observed at two diﬀerent
dates being independently distributed.
LRind = 2 · log
L1
L0

; LRind ∼χ2
1.
We deﬁne Tij as the number of days when condition j occurred under present
status, and assuming that condition i occurred on the previous day, we get:
i, j =

1, if violation occurs
0, if no violation occurs,
L1 =
1

i=0
(1 −πi1)Ti0πTi1
i1 ,
L0 = (1 −π)
1
i=0 Ti0π
1
i=0 Ti1,
πi1 =
Ti1
(Ti0 + Ti1), and π = (T01 + T11)
m
,
with m being the forecast period size. Thus, the joint CC test is a chi-square
test, in which LRcc = LRuc + LRind, when LRcc ∼χ2
2.

132
C. W. S. Chen and Y.-W. Sun
The DQ test of Engle and Manganelli [17]: The DQ test is based on a linear
regression model of the hits variable on a set of explanatory variables, including
a constant, the lagged values of the hit variable, and any function of the past
information set suspected of being informative. H0: Ht = I(yt < -VaRt) −α is
independent of W . The test statistic is:
DQ(q) = H′W

W ′W
−1 W ′H
α(1 −α)
,
where W is lagged hits, lagged VaR forecasts, or other relevant regressors over
time that are discussed in detail by Engle and Manganelli [17]. Under H0,
DQ(q) ∼χ2
q. The DQ test is recognized to be more powerful than the CC
test.
Evaluation of expected shortfall
To evaluate the ES forecast models, Chen et al. [12] consider the ES rate, which
is the analogue of VRate. We implement the measure proposed by Embrechts
et al.[15], who evaluates ES based on two measures. V1(α) gives the standard
backtesting measure using the VaR estimates. Takahashi et al. [33] point out
that this measure depends strongly on the VaR estimates, without adequately
reﬂecting the correctness of these values. To correct this weakness, we combine
a penalty term V2(α), which evaluates the values that should happen once every
1/α days with V1(α).
Let δt(α) = rt −ESt(α) and q(α) be the α-quantile of δt(α). Here, κ(α) is
a set of time points when a violation happens, and τ(α) is a set of time points
when δt(α) < q(α) occurs, where q(α) is the empirical α-quantile of δt(α). We
deﬁne the measure as:
V (α) = |V1(α)| + |V2(α)|
2
,
(16)
where
V1(α) = 1
T1

t∈κ(α)
δt(α), V2(α) = 1
T2

t∈τ(α)
δt(α),
and T1 and T2 are the numbers of time points in κ(α) and τ(α), respectively.
Note that better ES estimates provide lower values of both |V1(α)| and |V2(α)|,
and therefore of V (α).
6
Analytic Results
In this study we consider a very long period for the VaR and ES forecast per-
formance over 19 risk models and 2 HS methods. From the R program “quant-
mod” package, we download the daily closing prices of (i) the S&P500 (U.S.)
and (ii) Nikkei 225 (Japan). We analyze all data with the daily return rt,
rt = [ln(Pt) −ln(Pt−1)] × 100, where Pt is the closing index price on day t.
The full data period covers January 3, 2006 to March 31, 2017. We consider
a learning (in-sample) period from January 4, 2006 to December 31, 2013 and

Bayesian Forecasting for Tail Risk
133
a validation (out-of-sample) period of January 2, 2014 to March 31, 2017. We
employ a rolling window approach to produce a one-step-ahead forecasting of
hn+1, VaR, and ES over all risk models. There are 817 and 793 prediction samples
for S&P500 and Nikkei 225, respectively, in the out-of-sample period.
Table 1 describes summary statistics for the in-sample period of the log
returns of the market indices, including sample mean, standard deviation, skew-
ness, excess kurtosis, extreme values, the Jarque-Bera normality (JB) test, and
the Ljung-Box values for both returns and squared returns in order to test the
null hypothesis of no autocorrelation up to the 5th lag.
For Nikkei 225, the mean of return is not statistically signiﬁcantly diﬀerent
from zero, and its Ljung-Box statistic does not reject the null hypothesis of
no autocorrelation up to the 5th lag, while autocorrelation exists in the S&P500
return. We decide to include the lagged AR(1) eﬀect for all risk models and both
markets during the rolling window approach in order to adjust autocorrelation
in the mean equation.
The kurtosis reveals that its distribution is leptokurtic, as is observed com-
monly in ﬁnancial returns. The normality test indicates a clear rejection for
both markets by the JB normality test under a 1% signiﬁcant level. The skew-
ness is obviously negative for both markets. In summary, the daily returns of
both markets have heavy tails and are negatively skewed.
Table 1. Summary statistics of market returns for the in-sample period (January 3,
2006 to December 31, 2013)
Returns
Mean
SD
Skewness
Excess
Min
Max
JB
Q(5)b
Q2(5)b
Kurtosis
testa
p-value
p-value
S&P500
0.0195
1.400
−0.315
9.509
−9.470
10.957
0.000
0.000
0.000
Nikkei 225
0.0006
1.678
−0.554
7.750
−12.111
13.235
0.000
0.235
0.000
a Jarque-Bera normality test.
b Q(5) and Q2(5) are the p-values of the Ljung-Box test for autocorrelation in the level of returns
and the squared returns up to the 5th lag.
The initial values for each parameter in the ST GARCH model are φi =
(0, 0), αi = (0.1, 0.1, 0.1), γi = 30, i = 1, 2, ν = 100, and (c1, c2) = (0, 0.1). We
choose the maximum delay, d0, to be 3. Regarding the GJR-GARCH model, the
set-up of initial values is φ = (0, 0.0), α = (0.05, 0.05, 0.1, 0.1), ν = 200, and
η = 0 for skew Student’s t errors, that is, there are no skewness and Gaussian
for the initial guess. We perform 20000 MCMC iterations and discard the ﬁrst
10000 iterates as a burn-in sample for each analyzed data series. To save space,
we do not provide the Bayesian estimation for all risk models.
In the out-of-sample period, we use a rolling window estimation scheme with
the window size ﬁxed to produce a one-step-ahead forecasting of hn+1, VaR,
and ES at the 1% and 5% levels cross 19 risk models. We also apply two HS
methods for VaR forecasting. The HS methods encompass the short-term HS
with an observation window of 25 days (HS-ST) and the long-term HS with an

134
C. W. S. Chen and Y.-W. Sun
Table 2. Volatility forecasts using two loss functions: MSE and QLIKE
S&P500
Nikkei 225
MSE
QLIKE
MSE
QLIKE
HS-ST
NA
NA
NA
NA
HS-LT
NA
NA
NA
NA
RiskMetrics
0.3725
0.3660
3.2264
0.7040
GARCH-n
0.3940
0.4069
4.4367
0.7103
GARCH-t
0.4052
0.4085
3.9871
0.7168
GARCH-st
0.3993
0.3969
3.8921
0.7089
GARCH-ged
0.3943
0.4079
4.2898
0.7170
IGARCH-n
0.4300
0.4093
5.7821
0.7250
IGARCH-t
0.4305
0.4161
5.2880
0.7392
IGARCH-st
0.4263
0.4055
5.2091
0.7317
IGARCH-ged
0.4263
0.4126
5.6719
0.7340
GJR-n
0.3960
0.3862
4.0685
0.7032
GJR-t
0.4392
0.3894
3.8291
0.7077
GJR-st
0.4359
0.3812
3.7680
0.7034
GJR-ged
0.3670
0.3445
3.9301
0.6945
EGARCH-n
0.4132
0.3954
2.7702
0.6552
EGARCH-t
0.4599
0.4056
2.6340
0.6600
EGARCH-st
0.4542
0.3982
2.5635
0.6569
EGARCH-ged
0.3673
0.3232
2.6075
0.6425
ST GARCH-t
0.4485
0.4287
3.3069
0.6926
ST GARCH-st
0.4413
0.4276
3.2165
0.6869
The datasets consist of 817 and 793 prediction samples from January 2,
2014 to March 31, 2017, for S&P500 and Nikkei 225, respectively.
The values in boxes indicate the best two favored models.
observation window of 100 days (HS-LT), which have been used for nonpara-
metric estimation of VaR (see Gerlch et al. [22]). Eventually, we obtain 817 and
793 prediction samples from January 2, 2014 to March 31, 2017 for S&P500 and
Nikkei 225, respectively.
Table 2 presents the results of evaluating the one-day-ahead volatility fore-
casts by MSE and QLIKE. We obtain the realized kernel, ˆσ2
t as in (13) and (14),
from the Oxford-Man Institute. The GJR-GARCH model with GED error per-
forms better than the others for the U.S. market based on MSE criteria, while
the EGARCH model with skew Student’s t error outperforms the others for the
Japan market. Based on QLIKE, the measure reveals that for the predictive abil-
ities, the EGARCH model with GED error is the best model in both markets.
We can see that the GED assumption plays a great role in volatility forecasts.

Bayesian Forecasting for Tail Risk
135
Table 3. Evaluating VaR prediction performance based on the U.S. and Japan stock
markets.
Violation
VRate
Violation
VRate
Violation
VRate
Violation
VRate
No
1%
No
5%
No
1%
No
5%
S&P500
Nikkei 225
HS-ST
38
4.65%
67
8.20%
42
5.30%
79
9.96%
HS-LT
14
1.71%
43
5.26%
21
2.65%
48
6.05%
RiskMetrics
19
2.33%
44
5.39%
23
2.90%
47
5.93%
GARCH-n
16
1.96%
40
4.90%
22
2.77%
47
5.93%
GARCH-t
9
1.10%
47
5.75%
14
1.77%
50
6.31%
GARCH-st
9
1.10%
41
5.02%
13
1.64%
46
5.80%
GARCH-ged
10
1.22%
41
5.02%
16
2.02%
48
6.05%
IGARCH-n
15
1.84%
41
5.02%
20
2.52%
49
6.18%
IGARCH-t
9
1.10%
48
5.88%
16
2.02%
51
6.43%
IGARCH-st
8
0.98%
41
5.02%
12
1.51%
44
5.55%
IGARCH-ged
10
1.22%
42
5.14%
16
2.02%
48
6.05%
GJR-n
12
1.47%
38
4.65%
22
2.77%
43
5.42%
GJR-t
8
0.98%
49
6.00%
16
2.02%
45
5.67%
GJR-st
6
0.73%
36
4.41%
13
1.64%
38
4.79%
GJR-ged
12
1.47%
48
5.88%
18
2.27%
46
5.80%
EGARCH-n
10
1.22%
38
4.65%
22
2.77%
47
5.93%
EGARCH-t
8
0.98%
50
6.12%
20
2.52%
47
5.93%
EGARCH-st
5
0.61%
36
4.41%
13
1.64%
45
5.67%
EGARCH-ged
13
1.59%
57
6.98%
19
2.40%
47
5.93%
ST GARCH-t
9
1.10%
37
4.53%
14
1.77%
47
5.93%
ST GARCH-st
8
0.98%
40
4.90%
11
1.39%
44
5.55%
The out-of-sample period: January 2, 2014 to March 31, 2017. A rolling window approach to produce a
one-step-ahead VaR and ES over all risk models. There are 817 and 793 prediction samples for S&P500
and Nikkei 225, respectively, in the out-of-sample period.
The values in boxes indicate the best favored models.
To evaluate VaR, Table 3 displays the results of the violation rate at the
α signiﬁcant levels. When VRate is less than α, risk and loss estimates are
conservative; while when VRate is greater than α, risk estimates are lower than
actuality, and ﬁnancial institutions may not allocate suﬃcient capital to cover
likely future losses. We prefer that solvency outweighs proﬁtability. In other
words, for models where VRate/α are equidistant from 1, lower or conservative
rates are preferable.
For the 1% level in the U.S. market, 4 models rank ﬁrst with a tie: IGARCH-
st, GJR-GARCH-t, EGARCH-t, and ST GARCH-st models. For the Japan mar-
ket, all models and HS methods under-estimate 1% risk levels. The ST GARCH-
st model performs best among the 19 risk models. A diﬀerent scenario applies at
the 5% level, where GARCH-n and ST GARCH-st dominate for the U.S. market,
while GJR-GARCH-n is favorable for the Japan market. The performance HS
methods are very poor and consistently underestimate risk.

136
C. W. S. Chen and Y.-W. Sun
Table 4. Evaluating VaR prediction performance for the U.S. stock market
S&P500
UC
CC
DQ
1%
5%
1%
5%
1%
5%
HS-ST
0.0000
0.0001
0.0000
0.0002
0.0000
0.0000
HS-LT
0.0628
0.7321
0.0117
0.2289
0.0000
0.0001
RiskMetrics
0.0012
0.6173
0.0010
0.8097
0.0000
0.0300
GARCH-n
0.0149
0.8911
0.0057
0.9902
0.0000
0.1912
GARCH-t
0.7740
0.3345
0.0102
0.6169
0.0000
0.2213
GARCH-st
0.7740
0.9808
0.0102
0.9987
0.0000
0.1819
GARCH-ged
0.5342
0.9808
0.0138
0.9987
0.0000
0.1630
IGARCH-n
0.0315
0.9808
0.0085
0.9987
0.0000
0.1874
IGARCH-t
0.7740
0.2635
0.0102
0.5319
0.0000
0.1850
IGARCH-st
0.9522
0.9808
0.1759
0.9987
0.0436
0.1819
IGARCH-ged
0.5342
0.8542
0.0138
0.9765
0.0000
0.1548
GJR-n
0.2081
0.6436
0.1716
0.8843
0.0878
0.5710
GJR-t
0.9522
0.2040
0.1759
0.4451
0.0623
0.1300
GJR-st
0.4232
0.4272
0.6941
0.6911
0.9528
0.4280
GJR-ged
0.2081
0.2635
0.1716
0.5293
0.0795
0.1359
EGARCH-n
0.5342
0.6436
0.0138
0.8777
0.0000
0.1178
EGARCH-t
0.9522
0.1552
0.1759
0.2946
0.0763
0.0254
EGARCH-st
0.2298
0.4272
0.4715
0.6818
0.8973
0.2042
EGARCH-ged
0.1179
0.0142
0.1281
0.0493
0.0470
0.0134
ST GARCH-t
0.7740
0.5303
0.0102
0.7952
0.0000
0.1135
ST GARCH-st
0.9522
0.8911
0.0063
0.9902
0.0000
0.1030
Tables 4 and 5 provide p-values of three backtests: UC, CC, and DQ tests. We
use four lags, as in Engle and Manganelli [17]. At α = 1%, most of the models are
rejected in the U.S. market, mainly by the DQ test. The GJR-GARCH models
fare best as a group, and EGARCH with skew Student’s t error is also adequate
in the results of the three tests. Under a similar situation for the Japan market
at α = 1%, the DQ test rejects most of the models. The ST GARCH models
represent “survival” as a group, whereas GARCH, GJR-GARCH, and EGARCH
models with skew Student’s t perform well in terms of surviving from the three
backtests. In summary, the DQ test is more powerful than the CC test, and the
DQ test prefers asymmetric models with skew tails.
Figures 2 and 3 illustrate the VaR forecasts under the 1% signiﬁcant level
compared to log-returns in three models: GJR-GARCH-t, EGARCH-t, and ST
GARCH-st. These three models perform best in terms of VRate, but the DQ
test rejects the ST GARCH model in the U.S. market. We observe that three
consecutive violations (August 20, 21, and 24, 2015) occur in the ST model for
the U.S. market. On Sunday night, August 23 2015, a large drop in equities in

Bayesian Forecasting for Tail Risk
137
Table 5. Evaluating VaR prediction performance for the Japan stock market.
Nikkei 225
UC
CC
DQ
1%
5%
1%
5%
1%
5%
HS-ST
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
HS-LT
0.0001
0.1872
0.0002
0.3305
0.0000
0.0000
RiskMetrics
0.0000
0.2440
0.0000
0.4499
0.0000
0.0050
GARCH-n
0.0000
0.2440
0.0002
0.4499
0.0000
0.0501
GARCH-t
0.0506
0.1045
0.0745
0.2672
0.0684
0.0162
GARCH-st
0.0976
0.3124
0.1130
0.5501
0.0761
0.0258
GARCH-ged
0.0114
0.1872
0.0251
0.3579
0.0097
0.0090
IGARCH-n
0.0003
0.1411
0.0012
0.2771
0.0000
0.0712
IGARCH-t
0.0114
0.0760
0.0251
0.1535
0.0030
0.0123
IGARCH-st
0.1769
0.4858
0.1562
0.7556
0.0362
0.1344
IGARCH-ged
0.0114
0.1872
0.0251
0.3579
0.0025
0.0631
GJR-n
0.0000
0.5900
0.0002
0.5259
0.0000
0.7426
GJR-t
0.0114
0.3930
0.0251
0.3668
0.0435
0.7207
GJR-st
0.0976
0.7867
0.2041
0.7748
0.6223
0.9567
GJR-ged
0.0021
0.3124
0.0063
0.5501
0.0065
0.3709
EGARCH-n
0.0000
0.2440
0.0002
0.4499
0.0000
0.5752
EGARCH-t
0.0003
0.2440
0.0012
0.4499
0.0003
0.5849
EGARCH-st
0.0976
0.3930
0.2041
0.6540
0.5897
0.5317
EGARCH-ged
0.0008
0.2440
0.0028
0.4499
0.0005
0.5458
ST GARCH-t
0.0506
0.2440
0.0745
0.4499
0.1299
0.0565
ST GARCH-st
0.3006
0.4858
0.1948
0.7556
0.1901
0.3360
Asia (Monday time) triggered a drop in index futures in the U.S. and Europe.
U.S. stock futures went down (7%) prior to the U.S. open. If we incorporate this
external news in our model, then we might avoid this violation. August 24, 2015
was the S&P 500’s worst day since 2011 and followed an 8.5% slump in China
markets.
We evaluate ES based on Embrechts et al. [15] and choose the smallest value
as the best one. The values in boxes of Table 6 indicate the best two favored
models. Regarding the backtesting measure for ES at the 1% level, IGARCH-st
and EGARCH-t perform as the best two favored models for the U.S. market,
while IGARCH-st and ST GARCH-st are the best two favored models for the
Japan market. The IGARCH-st performs the best for both markets at the 1%
and 5% signiﬁcant levels, and ST GARCH-st performs the best in the Japan
market at both 1% and 5% signiﬁcant levels. At the 5% level, the parsimonious
IGARCH-st model behaves as good as the nonlinear ST GARCH-st model at
the 5% level.

138
C. W. S. Chen and Y.-W. Sun
Fig. 2. VaR forecasts (blue line) and daily returns for S&P500 (817 prediction samples
from January 2, 2014 to March 31, 2017) at the 1% level. Upper panel: GJR-GARCH-t
model, middle panel: EGARCH-t, lower panel: ST GARCH-st.

Bayesian Forecasting for Tail Risk
139
Fig. 3. VaR forecasts (blue line) and daily returns for Nikkei 225 (793 prediction sam-
ples from January 2, 2014 to March 31, 2017) at the 1% level. Upper panel: GJR-
GARCH-st model, middle panel: EGARCH-st, lower panel: ST GARCH-st.

140
C. W. S. Chen and Y.-W. Sun
Table 6. Backtesting measure of Embrechts et al. [15] for the expected shortfall fore-
casts of S&P500 (817 out-of-sample period) and Nikkei 225 (793 out-of-sample period).
S&P500
Nikkei 225
1%
5%
1%
5%
HS-ST
NA
NA
NA
NA
HS-LT
NA
NA
NA
NA
RiskMetrics
0.6565
0.2990
0.9530
0.5912
GARCH-n
0.4516
0.2400
0.7530
0.4911
GARCH-t
0.0841
0.0963
0.5187
0.3421
GARCH-st
0.0529
0.0211
0.3140
0.2285
GARCH-ged
0.2952
0.1520
0.5702
0.3542
IGARCH-n
0.4667
0.2351
0.7744
0.4305
IGARCH-t
0.0582
0.0603
0.2880
0.2596
IGARCH-st
0.0315
0.0039
0.2800
0.1684
IGARCH-ged
0.3075
0.1345
0.4926
0.3031
GJR-n
0.3788
0.1480
0.7423
0.5362
GJR-t
0.1021
0.0450
0.5813
0.4344
GJR-st
0.1192
0.1117
0.3525
0.3665
GJR-ged
0.2489
0.0986
0.6287
0.4187
EGARCH-n
0.3712
0.1230
0.9340
0.5303
EGARCH-t
0.0140
0.0523
0.6384
0.4418
EGARCH-st
0.2085
0.1196
0.5736
0.3004
EGARCH-ged
0.2897
0.1066
0.8555
0.4876
ST GARCH-t
0.0819
0.0196
0.3331
0.2986
ST GARCH-st
0.1626
0.0502
0.1769
0.1760
The values in boxes indicate the best two favored models.
7
Conclusion
This study evaluates the performances of VaR and ES forecasts using parametric
statistical and HS approaches. We adopt RiskMetrics, GARCH, IGARCH, and
asymmetric GARCH models with four distributions in error: normal, Student’s t,
skew Student’s t, and GED. Furthermore, this study considers the smooth transi-
tion GARCH model with a second-order logistic function of Chen et al. [11]. This
extension makes it possible to estimate quantile forecasts, VaR and ES together.
We use Bayesian MCMC methods on these GARCH-type models for estimation,
inference, and quantile forecasts and evaluate the one-day-ahead volatility and
quantile forecasts.
In volatility forecasts, the GJR-GARCH and EGARCH models with GED
perform best for the U.S. market, whereas the EGARCH models with GED
or skew Student’s t errors are good choices to predict volatility for the Japan
market. A VaR forecasting reveals that the ST GARCH model with the second

Bayesian Forecasting for Tail Risk
141
logistic function and skew Student’s t compares most favorably in terms of vio-
lation rates for both markets. Both asymmetric models, GJR-t and EGARCH-t
models, are favorable by independence of violations at the 1% level for the U.S.
market. For the Japan market, a range of well-known GARCH models, including
RiskMetrics, and historical simulation are not competitive to the ST GARCH-st
model at the 1% level. Regarding the backtesting measure of ES, the IGARCH-
st performs the best for both markets at the 1% and 5% signiﬁcant levels, and
ST GARCH-st performs the best in Japan market at both 1% and 5% signiﬁ-
cant levels. The skew and heavy-tailed error distributions are important when
evaluating the quantile forecast of ﬁnancial returns.
The ﬁndings of this research contribute to a better understanding in the per-
formance of Bayesian forecasting of VaR and ES based on GARCH-type models.
Our Bayesian methodology also makes it possible to forecast the volatility and
quantiles by sampling from their predictive distributions.
Acknowledgments. Cathy W.S. Chen’s research is funded by the Ministry of Science
and Technology, Taiwan (MOST 105-2118-M-035-003-MY2).
A
Appendix
All GARCH-type models contain the lagged AR(1) eﬀect, which allows one to
recognize whether the return series exhibits asymmetry mean reversion or market
eﬃciency. The AR(1) coeﬃcient may not be signiﬁcant from time to time. The
risk models considered in the empirical study are given in detail below.
AR(1)-GARCH(1,1) model
rt = φ0 + φ1rt−1 + at,
(17)
at =

htεt,
εt
i.i.d.
∼D(0, 1),
ht = α0 + α1a2
t−1 + β1ht−1.
To ensure the AR(1)-GARCH(1,1) model is ﬁnite and positive, we restrict the
parameters by:
S1 :
|φ1| < 1,
(18)
S2 :
α0 > 0, α1, β1 ≥0, α1 + β1 < 1.
(19)
AR(1)-IGARCH(1,1)
If the AR polynomial of the GARCH representation in Eq. (17) has a unit root,
then we have an IGARCH model. Thus, α1 + β1 = 1, and IGARCH models are
unit-root GARCH models.
RiskMetrics
Developed by J. P. Morgan [30], still a popular method, speciﬁcally used for
VaR calculation. It is a special case of the IGARCH model with normal errors,

142
C. W. S. Chen and Y.-W. Sun
where α0 = 0, and is thus an exponentially weighted moving average of squared
shocks. The model form is:
rt = at,
(20)
at =

htεt,
εt
i.i.d.
∼N(0, 1),
ht = δht−1 + (1 −δ)a2
t−1,
where δ is a decay factor belonging to (0.9,1), since J.P. Morgan recommended
δ = 0.94 for computing daily volatility.
AR(1)-GJR-GARCH(1,1) model
rt = φ0 + φ1rt−1 + at,
(21)
at =

htεt, εt
i.i.d.
∼D(0, 1),
ht = α0 + (α1 + γ1It−1)a2
t−1 + β1ht−1,
where It−1 =

1, if at−1 ≤0
0, if at−1 > 0.
We give some constraints for the GJR-GARCH(1,1) model to ensure positiveness
and covariance stationarity.
S3 :
α0 > 0, α1, β1 ≥0, α1 + γ1 ≥0, α1 + β1 + 0.5γ1 < 1.
(22)
Equations (18) and (22) guarantee stationarity and positivity of the AR(1)-GJR-
GARCH(1,1) model.
AR(1)-EGARCH(1,1) model
rt = φ0 + φ1rt−1 + at,
(23)
at =

htεt,
εt
i.i.d.
∼D(0, 1),
ln ht = α0 + α1

|at−1| + γ1at−1

ht−1

+ β1 ln ht−1.
The stationary condition for the mean part is |φ1| < 1 as in (18). The model with
a positive at−1 contributes α1(1 + γ1) |εt−1| to log volatility, whereas a negative
at−1 gives α1(1 −γ1) |εt−1|. The γ1 parameter thus signiﬁes the leverage eﬀect
of at−1. Therefore, we expect γ1 to be negative.
B
Appendix
Adaptive MH Algorithm
In our past experience with GARCH-type models, the RW Metropolis algorithm
produces very high correlations and slow mixing among the iterates of the para-
meters α. The eﬀect is enhanced when true volatility persistence is high, i.e.

Bayesian Forecasting for Tail Risk
143
α1 + β1 is close to 1, which is the usual case with ﬁnancial time series. To speed
up mixing and to reduce the dependence, we revise the sampling method for α
after the burn-in period and switch to the independent kernel IK MH algorithm
for the sampling period (see Chen and So [6]).
RW-MH algorithm
1. Generate the new parameter group α∗from the random walk kernel with the
form:
α∗= α[j−1] + ε,
ε ∼N(0, ϵΣ),
where Σ is the diagonal variance-covariance matrix.
2. RW chains: Randomly generate p from U(0,1) and compare it with:
p∗= min

1,
π(α∗)
π(α[j−1])

.
If p∗> p, then accept α∗as the new estimate of the parameter group and
then go back to Step 1.
A suitable value of ϵ with good convergence properties can usually be selected
by having an acceptance probability of 25% to 50%.
IK-MH algorithm
1. Generate the new parameter group α∗from the independent walk kernel with
the form:
α∗= μα + ε,
ε ∼N(0, Σα),
where mean μα and covariance matrix Σα are obtained from the burn-in
period.
2. IK chains: Randomly generate p from U(0,1) and compare it with:
p∗= min

1,
π(α∗)q(α)
π(α[j−1])q(α∗)

,
where α[j] is the ith iterate of α and q(.) is a Gaussian proposal density. If
p∗> p, then accept α∗as the new estimate of the parameter group and then
go back to Step 1. Repeat until the MCMC iterates converge.
References
1. Acerbi, C., Tasche, D.: On the coherence of expected shortfall. J. Bank. Finance
26, 1487–1503 (2002)
2. Artzner, P., Delbaen, F., Eber, J.-M., Heath, D.: Coherent measures of risk. Math.
Finance 9, 203–228 (1999)
3. Berkowitz, J., Christoﬀersen, P.F., Pelletier, D.: Evaluating Value-at-Risk models
with desk-level data. Manage. Sci. 57, 2213–2227 (2011)

144
C. W. S. Chen and Y.-W. Sun
4. Black, F.: Studies of stock market volatility changes. In: Proceedings of the Ameri-
can Statistical Association, Business and Economic Statistics Section, pp. 177–181
(1976)
5. Bollerslev, T.: Generalized autoregressive conditional heteroskedasticity. J. Econ.
31, 307–327 (1986)
6. Chen, C.W.S., So, M.K.P.: On a threshold heteroscedastic model. J. Forecast. 22,
73–89 (2006)
7. Chen, C.W.S., Chiang, T.C., So, M.K.P.: Asymmetrical reaction to US stock-return
news: evidence from major stock markets based on a double-threshold model. J.
Econ. Bus. 55, 487–502 (2003)
8. Chen, C.W.S., Gerlach, R., Hwang, B.B.K., McAleer, M.: Forecasting Value-at-
Risk using nonlinear regression quantiles and the intra-day range. Int. J. Forecast.
28, 557–574 (2012a)
9. Chen, C.W.S., Gerlach, R., Lin, E.M.H., Lee, W.C.W.: Bayesian forecasting for
ﬁnancial risk management, pre and post the global ﬁnancial crisis. J. Forecast. 31,
661–687 (2012b)
10. Chen, C.W.S., Gerlach, R., Lin, M.H.: Falling and explosive, dormant, and rising
markets via multiple-regime ﬁnancial time series models. Appl. Stochast. Models
Bus. Ind. 26, 28–49 (2010)
11. Chen, C.W.S., Weng, M.C., Watanabe, T.: Bayesian forecasting of Value-at-Risk
based on variant smooth transition heteroskedastic models. Stat. Interface 10,
451–470 (2017)
12. Chen, Q., Gerlach, R., Lu, Z.: Bayesian Value-at-Risk and expected shortfall fore-
casting via the asymmetric Laplace distribution. Comput. Stat. Data Anal. 56,
3498–3516 (2012)
13. Christoﬀersen, P.: Evaluating interval forecasts. Int. Econ. Rev. 39, 841–862 (1998)
14. Duﬃe, D., Pan, J.: An overview of value at risk. J. Deriv. 4, 7–49 (1997)
15. Embrechts, P., Kaufmann, R., Patie, P.: Strategic Long-term ﬁnancial risks: single
risk factors. Comput. Optim. Appl. 32, 61–90 (2005)
16. Engle, R.F.: Autoregressive conditional heterosedasticity with estimates of variance
of United Kingdom inﬂation. Econometrica 50, 987–1008 (1982)
17. Engle, R.F., Manganelli, S.: CAViaR: conditional autoregressive value at risk by
regression quantiles. J. Bus. Econ. Stat. 22, 367–381 (2004)
18. Fern´andez, C., Steel, M.F.J.: On Bayeasian modeling of fat tail and skewness. J.
Am. Stat. Assoc. 93, 359–371 (1998)
19. Fllmer, H., Schied, A.: Convex measures of risk and trading constraints. Finance
Stochast. 6, 429–447 (2002)
20. Glosten, L.R., Jagannathan, R., Runkle, D.E.: On the relation between the
expected value and the volatility of the nominal excess return on stock. J. Finance
48, 1779–1801 (1993)
21. Gerlach, R., Chen, C.W.S.: Bayesian inference and model comparison for asymmet-
ric smooth transition heteroskedastic models. Stat. Comput. 18, 391–408 (2008)
22. Gerlach, R., Chen, C.W.S., Chan, N.C.Y.: Bayesian time-varying quantile forecast-
ing for value-at-risk in ﬁnancial markets. J. Bus. Econ. Stat. 29, 481–492 (2011)
23. Gerlach, R., Chen, C.W.S.: Bayesian expected shortfall forecasting incorporating
the intraday range. J. Finan. Econ. 14, 128–158 (2015)
24. Hansen, B.E.: Autoregressive conditional density estimation. Int. Econ. Rev. 35,
705–730 (1994)
25. Harmantzis, F.C., Miao, L., Chien, Y.: Empirical study of valueatrisk and expected
shortfall models with heavy tails. J. Risk Finance 7, 117–135 (2006)

Bayesian Forecasting for Tail Risk
145
26. Hendricks, D.: Evaluation of Value-at-Risk models using historical data. Econ.
Policy Rev. 2, 39–67 (1996)
27. Hoogerheide, L., van Dijk, H.K.: Bayesian forecasting of Value at Risk and expected
shortfall using adaptive importance sampling. Int. J. Forecast. 26, 231–247 (2010)
28. Jansen, E.S., Ter¨asvirta, T.: Testing parameter constancy and super exogeneity in
econometric equations. Oxford Bull. Econ. Stat. 58, 735–763 (1996)
29. Kupiec, P.H.: Techniques for verifying the accuracy of risk measurement models.
J. Deriv. 3, 73–84 (1995)
30. Morgan, J.P.: RiskMetrics: J. P. Morgan Technical Document, 4th edn. J. P.
Morgan, New York (1996)
31. Nelson, D.B.: Conditional heteroscedasticity in asset returns: a new approach.
Econometrica 59, 347–370 (1991)
32. Papp, R.: What are the advantages of MCMC based inference in latent variable
models? Stat. Neerl. 56, 2–22 (2002)
33. Takahashi, M., Watanabe, T., Omori, Y.: Volatility and quantile forecasts by real-
ized stochastic volatility models with generalized hyperbolic distribution. J. Fore-
cast. 2, 437–457 (2016)
34. van Dijk, D., Ter¨asvirta, T., Franses, P.H.: Smooth transition autoregressive models
- a survey of recent developments. Econ. Rev. 21, 1–47 (2002)
35. Yamai, Y., Yoshiba, T.: Value-at-risk versus expected shortfall: a practical per-
spective. J. Bank. Finance 29, 997–1015 (2005)

Smoothing Spline as a Guide to Elaborate
Explanatory Modeling
Chon Van Le(B)
School of Business, International University - VNU HCMC, Quarter 6, Linh Trung,
Thu Duc District, Ho Chi Minh City, Vietnam
lvchon@hcmiu.edu.vn
Abstract. Although there are substantial theoretical and empirical dif-
ferences between explanatory modeling and predictive modeling, they
should be considered as two dimensions. And predictive modeling can
work as a “fact check” to propose improvements to existing explanatory
modeling. In this paper, I use smoothing spline, a nonparametric cali-
bration technique which is originally designed to intensify the predictive
power, as a guide to revise explanatory modeling. It works for the hous-
ing value model of Harrison and Rubinfeld (1978) because the modiﬁed
model is more meaningful and ﬁts better to actual data.
Keywords: Predictive econometrics · Calibration · Smoothing spline
1
Introduction
Econometric modeling has two main purposes: causal explanation and empirical
prediction. Causal analysis determines whether an independent variable really
aﬀects the dependent variable and estimates the magnitude of the eﬀect. And
the second goal is to make predictions about the dependent variable, given the
observed values of the independent variables.
However, the two objectives have received unequal treatment among acad-
emic researchers, especially those who work with cross section and panel data.
The bulk of most econometric textbooks is given to issues relevant to causal
explanation. Shmueli (2010) attributes this to the assumption that models
with high explanatory power are expected to have high predictive power. He
argues that there is clear distinction, both theoretically and empirically, between
explanatory modeling and predictive modeling.
Although many academic statisticians may consider prediction as unacad-
emic (for example, Kendall and Stuart (1977) and Parzen (2001)), it has played
an increasingly important role in banking and ﬁnancial services and other indus-
tries. Recent emergence of Big Data has fueled explosive growth of predictive
modeling in the last decade. Large volumes, high velocity inﬂow, and a wide
variety of data, including semi-structured and unstructured data such as text
and images, have provided organizations with various opportunities for keeping
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_7

Smoothing Spline as a Guide
147
track of and making prompt adjustments to their performance (Laney 2001).
Competent predictive modeling to analyze big data has become a key to gain-
ing insights, forecasting the future, and automating non-routine decision making
which enable them to make big improvements such as better risk management,
smoothened operations, personalized services, etc. (ICAEW 2014).
On the other hand, heavy emphasis placed on explanatory modeling has
led to a widespread acceptance of regression results in many academic papers
with highly signiﬁcant/insigniﬁcant coeﬃcients but low R2. A large proportion
of variation in the dependent variable left unexplained may imply that several
important independent variables are missing or the functional form that repre-
sents the relationship between the regressand and regressor(s) is not appropriate.
In addition, since the linear regression model which is used most popularly is an
approximation to some unknown, underlying function, such an approximation
is likely to be useful only over a small range of variation of the independent
variables around their means. It can be asserted that whether or not a factor
has an impact on the dependent variable, but the magnitude of its impact, if
any, cannot be asserted because it may vary across diﬀerent observations.
Therefore, calibration techniques under predictive modeling should be used
to improve model speciﬁcation. Although they cannot explain what is wrong in
the orginal model, they can work as a guide to elaborate explanatory modeling.
In this paper, I focus particularly on smoothing spline and apply it to Harrison
and Rubinfeld’s (1978) study on housing prices for the Boston area. It is found
that their model, though already good, can be ﬁne-tuned to be more meaningful
and to ﬁt better to actual data.
The paper is structured as follows. Section 2 reviews predictive modeling
in comparison with explanatory modeling. Section 3 presents calibration tech-
niques, including smoothing splines. Section 4 outlines the data, the revised
model and its regression results. Conclusions follow in Sect. 5.
2
Predictive Modeling1
In economics, statistical methods are used mainly to test economic theory. The
theory speciﬁes causal and eﬀect relationships between variables. Explanatory
modeling applies statistical models to data for testing these relationships, that is,
whether a particular independent variable does inﬂuence the dependent variable.
In this type of modeling, the theory takes a governing role and the application
of statistical methods is done strictly through the lens of the theory (Shmueli
2010).
In contrast, predictive modeling refers to the application of statistical models
or data mining algorithms to data for predicting the dependent variable. Accord-
ing to Shmueli (2010), prediction involves point or interval prediction, predictive
distribution, or ranking of new or future observations. There is a disagreement
among statisticians on the value of predictive modeling. Many see it as unaca-
demic. Berk (2008) indicates that researchers in social sciences only do “causal
1 This part is to a large extent based on Shmueli (2010).

148
C. Van Le
econometric style”. As a consequence, many statistics textbooks write very little
on predictive modeling. Others such as Geisser (1975), Aitchison and Dunsmore
(1975), Friedman (1997) consider prediction as the foremost statistical applica-
tion. This is true for most organizations and individuals outside of academia
that are overwhelmed with exponential growth of data as a result of digital
technology, mobile technology, social media, public sector open data, computer
chips and sensors implanted in physical assets, etc. Increasingly large and rich
datasets often consist of complex patterns and relationships that are beyond the
reach of existing theories. Predictive modeling can help reveal new hypotheses
and causality or propose improvements to existing explanatory models.
Shmueli (2010) clariﬁes that explanatory power can testify the strength of a
causal hypothesis but cannot assess the distance between theory and empirics.
Predictive modeling can work as a “fact check” to evaluate the relevance of
theories in the light of actual data. Hence competing theories can be compared by
examining their respective predictive power. Ehrenberg and Bound (1993) state
that predictive modeling may create benchmarks of predictive accuracy under
which scientiﬁc development can lead to substantial theoretical and practical
gains.
The key element leading to the diﬀerence between explanatory and predict-
ing modeling is errors of measurement. Observed data normally do not measure
accurately their underlying constructs or variables. There has been considerable
discussion in empirical studies on how to obtain reasonable measures of interest
rates, proﬁts, services from capital stocks, etc. For this reason, “the operational-
ization of theories and constructs into econometric models and measurable data
creates a disparity between the ability to explain phenomena at the conceptual
level and the ability to generate predictions at the measurable level” (Shmueli
2010).
The disparity justiﬁes important diﬀerences in several aspects. Firstly, in
explanatory modeling, a statistical model “usually begins with a statement of a
theoretical proposition” (Greene 2012). It is built based on an economic model
that consists of mathematical equations that describe deterministic relationships
between independent variables and the dependent variable. The statistical model
represents a causal relationship, and independent variables are assumed to cause
the dependent variable. In predictive modeling, the statistical model is often built
from the data. It shows the association between independent variables and the
dependent variable. Interpreting the relationship between independent variables
and the dependent variable is not required.
Secondly, Shmueli (2010) claims that explanatory modeling is backward-
looking. A statistical model is used to test already existent hypotheses. On the
contrary, predictive modeling is forward-looking. The statistical model is built
to predict new observations.
Thirdly, explanatory modeling focuses on minimizing bias to secure reliable
estimates of the “true” model coeﬃcients. In contrast, since the goal of predictive
modeling is to obtain optimal predictions of the dependent variable based on a
regression model of whatever variables are available, it seeks to minimize the

Smoothing Spline as a Guide
149
sum of bias and estimation variance (Shmueli 2010). Therefore, in predictive
modeling, bias is not a big problem and can be tolerated as long as empirical
precision is improved. However, bias is a crucial issue in explanatory modeling.
Several methods have been proposed to deal with the omission of variables that
aﬀect the dependent variable and are correlated with independent variables that
are present in the statistical model.
Fourthly, explanatory modeling aims to estimate the theory-based statisti-
cal model with adequate statistical power for hypothesis testing. Consequently,
multicollinearity is often a major concern in causal explanation. When two or
more independent variables are highly correlated, the estimator is still unbiased
but less precise. In predictive modeling, all these independent variables should
be included if each variable contributes signiﬁcantly to the predictive power of
the model.
In addition, suﬃcient data are required for statistical inference. A variety of
data imputation methods such as zero-order method, data augmentation and
multiple imputation techniques, inverse probability weighting, etc. have been
used to ﬁll gaps in data sets. These methods seem to be constructed for para-
meter estimation and hypothesis testing. However, according to Shmueli (2010),
“beyond a certain amount of data, extra precision is negligible for purposes of
inference”. Predictive modeling needs a larger sample to reduce bias and variance
and to create holdout datasets for prediction testing.
Although explanatory modeling and predictive modeling substantially diﬀer
theoretically and empirically, they should be considered as two dimensions. And
a statistical model should be evaluated based on its explanatory power and pre-
dictive power, whose weights depend on the question of interest. The predictive
power can be intensiﬁed by calibration techniques which are discussed in the
next section.
3
Calibration and Splines
Calibration dates back to the early 1980s in dynamic computable general equilib-
rium models (Kydland and Prescott, 1982). It is a procedure to select numerical
values for the parameters of a model because sometimes no data are available to
estimate its parameters. Canova (1994) points out that econometric estimation
and calibration are two approaches to exposing general equilibrium models to
data. They both begin with formulating a general equilibrium model and select-
ing funtional forms for production, utility, and exogenous factors. But they are
diﬀerent in choosing the parameters. The estimation approach believes that the
model provides an accurate description of the data, or that the model is true, is a
data-generating process, and tests what attributes of the model are false, diverge
signiﬁcantly from the data. The calibration approach assumes the opposite view
as Box (1976) states that “all models are wrong”. As a result, the theoretical
model should be modiﬁed or calibrated to gain a better approximation of the
observed data. Otherwise, the model can produce systematically biased predic-
tions, either too high or too low on average, so it cannot be used for economic
decisions.

150
C. Van Le
According to Stine (2011), “a model is calibrated if its predictions are correct
on average,” or
E(y|ˆy) = ˆy.
The adjustment procedure starts with the non-calibrated predicted value from
a regression:
ˆy = ˆβ0 + ˆβ1x1 + · · · + ˆβkxk.
The predictive ability of the model can be improved if ˆy can be transformed to
a better predictor, for example, ˆˆy = h(ˆy) where h is a smooth function.
A popular smooth function is a spline function that was ﬁrst applied by
Poirier and Garber (1974). In order to be continuous and continuously diﬀeren-
tiable at the ‘joins’ or ‘knots’, the spline must take a form of quadratic, cubic or
a higher-degree polynomial. Suppose that there are two known knots ˆya and ˆyb
and that we use a cubic spline:
ˆˆy = αi0 + αi1ˆy + αi2ˆy2 + αi3ˆy3 + ε,
(1)
where the subsets are deﬁned by
i =
⎧
⎨
⎩
1 if ˆy ≤ˆya,
2 if ˆya < ˆy ≤ˆyb,
3 if ˆyb < ˆy.
Continuity of ˆˆy and the ﬁrst derivatives at the knots requires:
α10 + α11ˆya + α12ˆy2
a + α13ˆy3
a = α20 + α21ˆya + α22ˆy2
a + α23ˆy3
a,
α20 + α21ˆyb + α22ˆy2
b + α23ˆy3
b = α30 + α31ˆyb + α32ˆy2
b + α33ˆy3
b,
(2)
α11 + 2α12ˆya + 3α13ˆy2
a = α21 + 2α22ˆya + 3α23ˆy2
a,
α21 + 2α22ˆyb + 3α23ˆy2
b = α31 + 2α32ˆyb + 3α33ˆy2
b.
Additional restrictions should be imposed on the spline (1) before estimation if
we want the second derivatives to be continuous:
2α12 + 6α13ˆya = 2α22 + 6α23ˆya,
2α22 + 6α23ˆyb = 2α32 + 6α33ˆyb.
The cubic spline though allows discontinuities in the third derivatives at the
knots.
So far we have assumed that we have speciﬁed the locations of the knots
in advance. But in most cases, without further information on abrupt changes
over time or size thresholds, it is impossible to determine the knots beforehand.
Then we can resort to nonparametric smoothers. The oldest and simplest one
is the smoothing spline that connects the medians of equal-width intervals. The
number of intervals can be chosen by
Number of intervals = max{min(b1, b2), b3},

Smoothing Spline as a Guide
151
where b1 = round{10 × ln10(N)}, b2 = round(
√
N), b3 = min(2, N), and N is
the number of observations (StataCorp 2011). In each interval, the median of
y and the median of ˆy are calculated. A spline is ﬁt to these medians. If the
spline appears to deviate much from the 45◦line, then the original model can
be modiﬁed to better capture the “true”, complex relationships between the
dependent variable and independent variables. Consequently, the model would
be more eﬀective as an approximating function, that is, providing more rigor-
ous hypothesis testing of a regressor’s impact and an expectedly more accurate
estimate of the magnitude of that impact. I suggest using smoothing splines as
a guide to elaborate explanatory modeling. An example is presented in the next
section.
4
Example of Harrison and Rubinfeld (1978)
In a study of the willingness to pay for air quality improvements in Boston,
Harrison and Rubinfeld (1978) use data for 506 census tracts in the Boston
Standard Metropolitan Statistical Area (SMSA) in 1970 to estimate a housing
value equation
Ln(Medianvalue) = α0 + α1Room2 + α2Age + α3Ln(Distance) + α4Ln(Highway)
+ α5Tax + α6Pupil/Teacher + α7(Black −0.63)2
(3)
+ α8Ln(Lowstatus) + α9Crime + α10Zoning + α11Industry
+ α12Charles + α13Nox2 + ϵ,
where variables are deﬁned in Table 1. The results which are reported in the
second and third columns of Table 2 seemingly provide strong evidence on the
impacts of the independent variables, except Age, Zoning, and Industry. And R2
is relatively high. However, Fig. 1 indicates that the spline diverges considerably
from the 45◦line, especially at the small predicted values of the dependent
variable.
The spline can be approximated by a 3rd degree polynomial of the form:
y = Xα + γ2(ˆy −¯y)2 + γ3(ˆy −¯y)3 + ξ,
(4)
where X and ˆy are the set of independent variables and the predict values of the
original model.
The second and third columns of Table 3 show that the second term in Eq. (4)
that approximates the spline of the Harrison and Rubinfeld’s model is signiﬁcant
at 5% level. Moreover, the last two terms in (4) are jointly signiﬁcant at 10%
level (their F-statistics is 2.40), implying that the polynomial (4) ﬁts the data
better than the Harrison and Rubinfeld’s model. It suggests that the model can
be revised.
A closer look at the data set proposes several modiﬁcations as follows. Firstly,
since the index of accessibility to radial highways takes nine discrete values,
namely, 1, 2, 3, 4, 5, 6, 7, 8, and 24, the Highway index should be used instead

152
C. Van Le
Table 1. Variable deﬁnitions
Variable
Description
Medianvalue
Median value of owner-occupied homes
Room
Average number of rooms in owner-occupied homes
Age
Proportion of owner-occupied homes built before 1940
Black
Black proportion of population in the community
Lowstatus
Proportion of population that is lower status = 1
2(proportion of
adults without some high school education and proportion of male
workers classiﬁed as laborers)
Crime
Crime rate by town
Zoning
Proportion of a town’s residential land zoned for lots greater than
25,000 square feet
Industry
Proportion of nonretail business acres per town
Tax
Full value property tax rate ($/$10,000)
Pupil/Teacher Pupil-teacher ratio by town school district
Charles
Charles River dummy equals 1 if tract bounds the Charles River
and 0 otherwise
Distance
Weighted distance to 5 employment centers in the Boston area
Highway
Highway access index
Nox
Annual average nitrogen oxide concentration in pphm
Source: Harrison and Rubinfeld (1978).
1.5
2
2.5
3
3.5
4
Ln(Medianvalue)
1.5
2
2.5
3
3.5
4
Predicted Ln(Medianvalue)
Spline
45−degree line
Fig. 1. Smoothing spline based on Harrison and Rubinfeld’s (1978) model

Smoothing Spline as a Guide
153
Table 2. Housing value models
Harrison and Rubinfeld’s
(1978) modela (3)
Revised model (5)
Constant
4.558***
(0.1544)
4.595***
(0.2921)
Room2
0.0063***
(0.0013)
0.0231***
(0.0025)
Age
0.00009
(0.0005)
0.0052*
(0.0030)
Ln(Distance)
−0.1913***
(0.0334)
−0.2382***
(0.0301)
Ln(Highway)
0.0957***
(0.0191)
Highway
0.0139*
(0.0076)
Tax
−0.0004***
(0.0001)
Ln(Tax)
−0.1963***
(0.0427)
Tax600
−0.3011***
(0.0791)
Pupil/Teacher
−0.0311***
(0.0050)
−0.0316***
(0.0045)
(Black – 0.63)2
0.3637***
(0.1031)
0.1240
(0.0934)
Ln(Lowstatus)
−0.3712***
(0.0250)
Lowstatus
0.0547***
(0.0095)
Crime
−0.0119***
(0.0012)
−0.0225***
(0.0037)
Crime2
0.00016***
(0.00005)
Zoning
0.00008
(0.0005)
0.0004
(0.0005)
Industry
0.0002
(0.0024)
−0.0074***
(0.0027)
Charles
0.0914***
(0.0332)
0.0474b
(0.0292)
Nox2
−0.6380***
(0.1131)
−0.7763***
(0.1016)
Room×Age
−0.0009**
(0.00047)
Room×Lowstatus
−0.0118***
(0.0015)
Highway×Lowstatus
−0.0012***
(0.00016)
Highway×Industry
0.0019***
(0.0005)
Number of observations
506
506
R2
0.806
0.855
Notes: ***, **, * signiﬁcant at the 1%, 5%, 10% levels, respectively.
Standard errors in parentheses.
aResults are slightly diﬀerent from those in Harrison and Rubinfeld (1978).
b Charles River dummy has a p-value of 10.5.
Source: Author’s calculation.
of its log which may be less meaningful. Secondly, the Tax variable is replaced
by its log. While most of the observations in the sample have tax rates ranging
from $187 to $469 per $10,000, 137 tracts have unusually high tax rates of $666
and $711. A dummy variable, Tax600 which equals 1 if the tax rate is over
$600 and 0 otherwise, is included. Thirdly, as the lower status is measured as a
percentage of the population in the community, the Lowstatus variable should be
in original form, not in logarithmic form. Fourthly, squared crime rate, Crime2,
is added to allow for a changing impact of the crime rate on the housing value.
Fifthly, house prices depend on various attributes which are considered not only
separately but also together. Therefore, four interaction terms are included. The
modiﬁed housing value equation is

154
C. Van Le
Ln(Medianvalue) = β0 + β1Room2 + β2Age + β3Ln(Distance) + β4Highway
+ β5Ln(Tax) + β6Tax600 + β7Pupil/Teacher + β8(Black – 0.63)2
+ β9Lowstatus + β10Crime + β11Crime2 + β12Zoning
(5)
+ β13Industry + β14Charles + β15Nox2 + β16Room × Age
+ β17Room × Lowstatus + β18Highway × Lowstatus
+ β19Highway × Industry + ε.
Table 3. Polynomials approximating splines of the two models
Harrison and Rubinfeld’s (1978) Revised model
Constant
4.635***
(0.1581)
4.531***
(0.2951)
Room2
0.0072***
(0.0014)
0.0271***
(0.0033)
Age
0.0002
(0.0005)
0.0059**
(0.0030)
Ln(Distance)
−0.2048***
(0.0342)
−0.2519***
(0.0310)
Ln(Highway)
0.0997***
(0.0196)
Highway
0.0151**
(0.0076)
Tax
−0.00046*** (0.0001)
Ln(Tax)
−0.2048***
(0.0429)
Tax600
−0.2944***
(0.0820)
Pupil/Teacher
−0.0329***
(0.0051)
−0.0322***
(0.0045)
(Black – 0.63)2
0.3314***
(0.1055)
0.1108
(0.0942)
Ln(Lowstatus)
−0.3880***
(0.0274)
Lowstatus
0.0651***
(0.0111)
Crime
−0.0103***
(0.0023)
−0.0230***
(0.0040)
Crime2
0.00016*** (0.00005)
Zoning
0.0003
(0.0005)
0.00055
(0.00046)
Industry
0.0006
(0.0024)
−0.0077***
(0.0028)
Charles
0.0985***
(0.0334)
0.0539*
(0.0295)
Nox2
−0.6550***
(0.1139)
−0.7926
(0.1025)
Room×Age
−0.0010**
(0.00047)
Room×Lowstatus
−0.0134***
(0.0018)
Highway×Lowstatus
−0.0013***
(0.0002)
Highway×Industry
0.0019***
(0.0005)
( 
Ln(MV) – Ln(MV))2
−0.1371**
(0.0672)
−0.0913
(0.0675)
( 
Ln(MV) – Ln(MV))3
−0.0396
(0.0858)
−0.1096
(0.0668)
Number of observations
506
506
R2
0.808
0.856
Notes: ***, **, * signiﬁcant at the 1%, 5%, 10% levels, respectively.
Standard errors in parentheses.
Source: Author’s calculation.

Smoothing Spline as a Guide
155
Figure 2 shows that the spline based on the revised model does not deviate
much from the 45◦line, even at the extreme predicted values. This is conﬁrmed
by the fact that the last two terms in the spline-approximating polynomial in
the fourth and ﬁfth columns of Table 3 are individually and jointly insigniﬁcant
(their F-statistics is 1.68).
The regression results which are presented in the fourth and ﬁfth columns of
Table 2 diﬀer in several aspects from those of Harrison and Rubinfeld. Age and
Industry are now signiﬁcant. Externalities associated with industrial activities
such as noise, pollution, heavy traﬃc, and awful view negatively aﬀect housing
values as expected. Black proportion of population no longer has a positive
impact on housing values, which makes more sense as black neighbors are often
regarded as undesirable. In addition, the explanatory power of the revised model,
though already good, still improves since R2 increases by 5%. The example
of Harrison and Rubinfeld demonstrates that smoothing spline helps elaborate
modeling.
1.5
2
2.5
3
3.5
4
Ln(Medianvalue)
1.5
2
2.5
3
3.5
4
Predicted Ln(Medianvalue)
Spline
45−degree line
Fig. 2. Smoothing spline based on revised model
5
Conclusions
Greater weight has so far been put on causal explanation than on empirical
prediction due to the wrong assumption that models having high explanatory
power are supposed to have high predictive power. Explanatory modeling and
predictive modeling substantially diﬀer theoretically and empirically, but they

156
C. Van Le
should be considered as two dimensions. And predictive modeling can work as a
“fact check” to propose improvements to existing explanatory modeling.
In this paper, I use smoothing spline, a nonparametric calibration technique
which is originally designed to intensify the predictive power, as a guide to
revise explanatory modeling. It works for the housing value model of Harrison
and Rubinfeld (1978) as the modiﬁed model is more meaningful and ﬁts better
to actual data.
The world is producing enormous and complex amounts of data that often
contain sophisticated patterns and relationships beyond the reach of current
theories. Calibration techniques such as smoothing splines can help incorporate
new data, new variables into explanatory models so that new hypotheses and
causality can be revealed and tested. Furthermore, organizations and ﬁrms can
capture and exploit new implications of big data for their own sake.
References
Aitchison, J., Dunsmore, I.R.: Statistical Prediction Analysis. Cambridge University
Press, Cambridge (1975)
Berk, R.A.: Statistical Learning from a Regression Perspective. Springer, New York
(2008)
Box, G.E.P.: Science and statistics. J. Am. Stat. Assoc. 71, 791–799 (1976)
Canova, F.: Statistical inference in calibrated models. J. Appl. Econ. 9(S), S123–S144
(1994)
Ehrenberg, A., Bound, J.: Predictability and prediction. J. Roy. Stat. Soc. Ser. A
156(2), 167–206 (1993)
Friedman, J.H.: On bias, variance, 0/1-loss, and the curse-of-dimensionality. Data Min.
Knowl. Discov. 1, 55–77 (1997)
Geisser, S.: The predictive sample reuse method with applications. J. Am. Stat. Assoc.
70, 320–328 (1975)
Greene, W.H.: Econometric Analysis, 7th edn. Pearson, London (2012)
Harrison, D., Rubinfeld, D.L.: Hedonic housing prices and the demand for clean air. J.
Environ. Econ. Manage. 5(1), 81–102 (1978)
ICAEW. Big Data and Analytics—What’s New? (2014). https://www.icaew.com/
-/media/corporate/archive/ﬁles/technical/information-technology/technology/
what-is-new-about-big-data-v2.ashx
Kendall, M., Stuart, A.: The Advanced Theory of Statistics, 4th edn. Macmillan,
New York (1977)
Kydland, F.E., Prescott, E.C.: Time to build and aggregate ﬂuctuations. Econometrica
50(6), 1345–1370 (1982)
Laney, D.: 3D data management: controlling data volume, velocity and variety, Gartner
(2001). http://blogs.gartner.com/doug-laney/ﬁles/2012/01/ad949-3D-Data-Manage
ment-Controlling-Data-Volume-Velocity-and-Variety.pdf
Parzen, E.: Comment on statistical modeling: the two cultures. Stat. Sci. 16(3), 224–
226 (2001)
Poirier, D.J., Garber, S.G.: The determinants of aerospace proﬁt rates 1951–1971.
South. Econ. J. 41(2), 228–238 (1974)
Shmueli, G.: To explain or to predict? Stat. Sci. 25(3), 289–310 (2010)
StataCorp. Stata Release 12: Statistical Software, StataCorp LP (2011)
Stine, R.A.: Lecture Notes on Advanced Quantitative Modeling, The Wharton School
at the University of Pennsylvania (2011)

Quantifying Predictive Uncertainty Using
Belief Functions: Diﬀerent Approaches
and Practical Construction
Thierry Denœux(B)
Sorbonne Universit´es, Universit´e de Technologie de Compi`egne, CNRS,
UMR 7253 Heudiasyc, Compi`egne, France
thierry.denoeux@utc.fr
Abstract. We consider the problem of quantifying prediction uncer-
tainty using the formalism of belief functions. Three requirements for
predictive belief functions are reviewed, each one of them inducing a
distinct interpretation: compatibility with Bayesian inference, approxi-
mation of the true distribution, and frequency calibration. Construction
procedures allowing us to build belief functions meeting each of these
three requirements are described and illustrated using simple examples.
1
Introduction
Statistical prediction is the task of making statements about a not-yet-observed
realization y of a random variable Y , based on past observations x. An impor-
tant issue in statistical prediction is the quantiﬁcation of uncertainty. Typically,
prediction uncertainty has two components:
1. Estimation uncertainty, arising from the partial ignorance of the probability
distribution of Y , and
2. Random uncertainty, due to the variability of Y .
If the distribution of Y is completely known, there is no estimation uncertainty.
If Y is a constant, there is no random uncertainty: this is the case in parame-
ter estimation problems. In all practical problems of interest, both sources of
uncertainty coexist, and should be accounted for in the prediction method.
In this paper, we assume the past data X and the future data Y to be
independent, and we consider sampling models X ∼PX(·; θ) and Y ∼PY (·; θ),
where θ is a parameter known only to belong to some set Θ. The sample spaces
of X and Y will be denoted by X and Y , respectively. To keep the exposition
simple, we will assume Y to be a real random variable, with Y ⊆R.
The statistical prediction problem is treated diﬀerently in the Bayesian and
frequentist frameworks. Here, we brieﬂy outline the main approaches within each
of these two frameworks.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_8

158
T. Denœux
Bayesian Approach
In the Bayesian framework, X, Y and θ are considered as random variables. A
Bayesian posterior predictive distribution FB(y|x) can then be computed from
the conditional distribution F(y|x; θ) = F(y|θ) by integrating out θ,
FB(y|x) =

F(y|θ)p(θ|x)dθ,
(1)
where p(θ|x) is the posterior density of θ. The main limitation of this approach
is the necessity to specify a prior distribution p(θ) on θ. In many cases, prior
knowledge on θ is either nonexistent, or too vague to be reliably described by a
single probability distribution.
Frequentist Approach
In the frequentist framework, the prediction problem can be addressed in several
ways. The so-called plug-in approach is to replace θ in the model by a point
estimate θ and to estimate the distribution of Y by PY (·; θ). This approach
amounts to neglecting estimation uncertainty. Consequently, it will typically
underestimate the prediction uncertainty, unless the sample size is very large.
Another approach is to consider prediction intervals [L1(X), L2(X)] such that
the coverage probability
CP(θ) = PX,Y (L1(X) ≤Y ≤L2(X); θ)
(2)
has some speciﬁed value, perhaps approximately. The coverage probability can
take any value only if Y is continuous; consequently, we often make this assump-
tion when using this approach. Conﬁdence intervals do account for estimation
and prediction uncertainty, but they do not provide any information about the
relative plausibility of values inside or outside that set. To address the issue, we
may consider one-sided conﬁdence intervals (−∞, Lα(X)] indexed by α ∈(0, 1),
such that
CP(θ) = PX,Y (Y ≤Lα(X); θ)
(3)
is equal to α, at least approximately. Then, we may treat α-prediction limits
Lα(x) as the α-quantiles of some predictive distribution function Fp(y|x) [2,16].
Such a predictive distribution is not a frequentist probability distribution; rather,
it can be seen as a compact way of describing one or two-sided (perhaps, approx-
imate) prediction intervals on Y at any level.
In all the approaches summarized above, uncertainty about Y is represented
either as a set (in the case of prediction intervals), or as a probability distribution
(such as a frequentist or Bayesian predictive distribution). In this paper, we
consider approaches to the prediction problem where uncertainty about Y is
represented by a belief function. In Dempster-Shafer theory, belief functions are
expressions of degrees of support for statements about the unknown quantity
under consideration, based on evidence. Any subset A ⊆Y can be canonically
represented by a belief function, and any probability measure is also a particular
belief function: consequently, the Dempster-Shafer formalism is more general and

Quantifying Predictive Uncertainty Using Belief Functions
159
ﬂexible than the set-membership or probabilistic representations. The problem
addressed in this paper is to exploit this ﬂexibility to represent the prediction
uncertainty on Y based on the evidence of observed data x.
The interpretation of a predictive belief function will typically depend on
the requirements imposed on the construction procedure. There is, however, no
general agreement as to which properties should be imposed. The purpose of
this paper is to review some desired properties, and to describe practical con-
struction procedures allowing us to build predictive belief functions that verify
these properties. As we shall see, three main properties have been proposed in
previous work, resulting in three main types of predictive belief functions.
The rest of this paper is organized as follows. Some general deﬁnitions and
results related to belief functions are ﬁrst recalled in Sect. 2. The requirements
are then presented in Sect. 3, and construction procedures for the three types
of predictive belief functions considered in this paper are described in Sect. 4.
Section 5 contains conclusions.
2
Background on Belief Functions
In this section, we provide a brief reminder of the main concepts and results from
the theory of belief functions that will be used in this paper. The deﬁnitions of
belief and plausibility functions will ﬁrst be recalled in Sect. 2.1. The connection
with random sets will be explained in Sect. 2.2, and Dempster’s rule will be
introduced in Sect. 2.4.
2.1
Belief and Plausibility Functions
Let Ω be a set, and B an algebra of subsets of Ω. A belief function on (Ω, B)
is a mapping Bel : B →[0, 1] such that Bel(∅) = 0, Bel(Ω) = 1, and for any
k ≥2 and any collection B1, . . . , Bk of elements of B,
Bel
⎛
⎝
k
i=1
Bi
⎞
⎠≥

∅̸=I⊆{1,...,k}
(−1)|I|+1Bel
⎛
⎝
i∈I
Bi
⎞
⎠.
(4)
Given a belief function Bel, the dual plausibility function Pl : B →[0, 1]
is deﬁned by Pl(B) = 1 −Bel(B), for any B ∈B. In the Dempster-Shafer
theory of belief functions [22], Bel(B) is interpreted as the degree of support
in the proposition Y ∈B based on some evidence, while Pl(B) is a degree of
consistency between that proposition and the evidence.
If the inequalities in (4) are replaced by equalities, then Bel is a ﬁnitely
additive probability measure, and Pl = Bel. If the evidence tells us that Y ∈A
for some A ∈B, and nothing more, then it can be represented by a function
BelA that gives full degree of support to any B ∈B such that B ⊆A, and zero
degree of support to any other subset. It can easily be veriﬁed that BelA is a
belief function. If A = Ω, the belief function is said to be vacuous: it represent
complete ignorance on Y .

160
T. Denœux
Given two belief functions Bel1 and Bel2, we say that Bel1 is less committed
than Bel2 if Bel1 ≤Bel2; equivalently, Pl1 ≥Pl2. The meaning of this notion is
that Bel1 represents a weaker state of knowledge than that represented by Bel2.
2.2
Connection with Random Sets
A belief function is typically induced by a source, deﬁned as a four-tuple
(S , A , P, Γ), where S is a set, A an algebra of subsets of S , P a ﬁnitely
additive probability measure on (S , A ), and Γ a mapping from S to 2Ω. The
mapping Γ is strongly measurable with respect to A and B if, for any B ∈B,
we have
{s ∈S |Γ(s) ̸= ∅, Γ(s) ⊆A} ∈A .
We can then show [19], that the function Bel deﬁned by
Bel(B) = P({s ∈S |Γ(s) ̸= ∅, Γ(s) ⊆B})
P({s ∈S |Γ(s) ̸= ∅})
,
(5)
for all A ⊆B is a belief function. The dual plausibility function is
Pl(B) = P({s ∈S |Γ(s) ∩B ̸= ∅})
P({s ∈S |Γ(s) ̸= ∅})
.
(6)
The mapping Γ is called a random set. We should not, however, get abused by
the term “random”: most of the time, the probability measure P deﬁned on
(S , A ) is subjective, and there is no notion of randomness involved.
2.3
Consonant Random Closed Sets
Let us assume that Ω = Rd and B = 2Ω. Let π be an upper semi-continuous
map from Rd to [0, 1], i.e., for any s ∈[0, 1], the set sπ
def
= {x ∈Rd|π(x) ≥s}
is closed. Furthermore, assume that π(x) = 1 for some x. Let S = [0, 1], A be
the Borel σ-ﬁeld on [0, 1], μ the uniform measure, and Γ the mapping deﬁned
by Γ(s) = sπ. Then Γ is a random closed set [20]. We can observe that its
focal sets are nested: it is said to be consonant. The plausibility function is then
a possibility measure [25], and π is the corresponding possibility distribution.
Function Pl can be computed as Pl(B) = supx∈B π(x), for any B ⊆Rd. In
particular , Pl{x} = π(x) for all x ∈Ω.
2.4
Dempster’s Rule
Assume that we have two sources (Si, Ai, Pi, Γi) for i = 1, 2, where each Γi
is a multi-valued mapping from Si to 2Ω, and each source induces a belief
function Beli on Y . Then, the orthogonal sum of Bel1 and Bel2, denoted as
Bel1 ⊕Bel2 is induced by the source (S1 × S2, A1 ⊗A2, P1 ⊗P2, Γ∩), where
A1 ⊗A2 is the tensor product algebra on the product space S1 × S2, P1 ⊗P2 is
the product measure, and Γ∩(s1, s2) = Γ1(s1) ∩Γ2(s2). This operation is called
Dempster’s rule of combination [7]. It is the fundamental operation to combine
belief functions induced by independent pieces of evidence in Dempster-Shafer
theory.

Quantifying Predictive Uncertainty Using Belief Functions
161
3
Predictive Belief Functions
In this paper, we are concerned with the construction of predictive belief func-
tions (PBF), i.e., belief functions that quantify the uncertain on future data
Y , given the evidence of past data x. This problem can be illustrated by the
following examples, which will be used throughout this paper.
Example 1. We have observed the times between successive failures of an air-
conditioning (AC) system, as shown in Table 1 [21]. We assume the time ξ
between failures to have an exponential distribution E (θ), with cdf
F(ξ; θ) =

1 −exp(−θx)

I(ξ ≥0),
where θ is the rate parameter. Here, the past data x = (ξ1, . . . , ξn) is a realization
of an iid sample X = (Ξ1, . . . , Ξn), with Ξi ∼E (θ), and Y is a random variable
independent from X, also distributed as E (θ). Based on these data and this
model, what can we say about the time to the next failure of the system?
⊓⊔
Example 2. The data shown in Fig. 1(a) are annual maximum sea-levels
recorded at Port Pirie, a location just north of Adelaide, South Australia, over
the period 1923–1987 [5]. The probability plot in Fig. 1(b) shows a good ﬁt with
the Gumbel distribution, with cdf
FX(ξ; θ) = exp

−exp

−ξ −μ
σ

,
(7)
where μ is the mode of the distribution, σ a scale parameter, and θ = (μ, σ).
Suppose that, based on these data, we want to predict the maximum sea level Y
in the next m = 10 years. Assuming that the distribution of sea level will remain
unchanged in the near future (i.e., neglecting, for instance, the eﬀect of sea level
rise due to climate change), the cdf of Y is
FY (y; θ) = FX(y; θ)m = exp

−m exp

−y −μ
σ

.
(8)
The parameter θ is unknown, but the observed data provides information about it.
How to represent this information, so as to quantify the uncertainty on Y ? What
can be, for instance, a sound deﬁnition for the degree of belief in the proposition
Y ≥5?
⊓⊔
Table 1. Times between successive failures of an air-conditioning system, from [21].
23
261
87
7
120
14
62
47 225
71
246
21
42 20
5
12 120
11
3
14
71
11
14 11
16
90
1
16
52
95

162
T. Denœux
1930
1940
1950
1960
1970
1980
3.6
3.8
4.0
4.2
4.4
4.6
year
maximum sea level
(a)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Probability plot
Empirical distribution function
Theoretical distribution function
(b)
Fig. 1. Annual maximum sea-levels recorded at Port Pirie over the period 1923–1987
(a), and probability plot for the Gumbel ﬁt to the data (c).
In general, the evidence on Y may consist in (1) the observed data x and (2)
prior knowledge on θ, which can be assumed to be represented by a belief function
Bel0
θ. A predictive belief function on Y can thus be denoted as BelY (·; x, Bel0
θ).
If Bel0
θ is vacuous, we simple write BelY (·; x). The following three requirements
have been proposed for BelY .
R0: Likelihood principle
As we assume X and Y to be independent, the observation of X provides infor-
mation on Y only through the parameter θ. The likelihood principle [4,13] states
that all relevant information about θ, after observing X = x, is contained in
the likelihood function L(θ; x) = p(x; θ). Formally, this principle means that
two observations X and X′ generated by two diﬀerent random experiments,
with probability distributions p(x; θ) and p(x′; θ), provide the same information
about θ as long as p(x; θ) and p(x′; θ) are proportional, i.e., there is some con-
stant c = c(x, x′) not depending on θ, such that p(x; θ) = c·p(x′; θ) for all θ ∈Θ.
Consequently, we should also have

∀θ ∈Θ, p(x; θ) = c · p(x′; θ)

⇒BelY (·; x) = BelY (·; x′).
(9)
The likelihood principle was shown by Birnbaum in [4] to follow from two prin-
ciples generally accepted by most (but not all) statisticians: the conditionality
principle (see also [3, p. 25]) and the suﬃciency principle.
R1: Compatibility with Bayes
For some statisticians, Bayesian reasoning is a perfectly valid approach to statis-
tical inference provided a prior probability distribution is available, but is ques-
tionable in the absence of such prior information. Many authors have attempted

Quantifying Predictive Uncertainty Using Belief Functions
163
to generalize Bayesian inference to some “prior-free” method of inference. This
was, in particular, Dempster’s motivation in his early papers on belief functions
[6,8]. If we adopt this point of view, then a predictive belief function should coin-
cide with the Bayesian posterior predictive distribution if a probabilistic prior
is available. Formally, if Bel0
θ = P 0
θ is a probability measure, then the following
equality should hold,
BelY (A; x, P 0
θ ) = PB(A|x)
(10)
for all measurable event A ⊆Y , where PB(·|x) is the Bayesian posterior predic-
tive probability measure corresponding to (1). This requirement ensures that the
Bayesian and belief function approaches yield the same predictions when they
are provided with exactly the same information.
A PBF verifying requirements (9) and (10) will be called a Type-I PBF. It
can be seen as a representation of the evidence about Y from the observation of
X, and possibly additional information on θ; it becomes the Bayesian predictive
posterior distribution when combined with a probabilistic prior.
R2: Approximation of the true future data distribution
We may also consider that, if we knew the true value of parameter θ, then we
would equate the predictive belief function with the true distribution PY (·; θ) of
Y . If we do not know θ, but we have only observed a sample x of X, then the
predictive belief function should most of the time (i.e., for most of the observed
samples) be less committed than PY (·; θ) [1,10]. Formally, we may thus ﬁx some
α ∈(0, 1), and require that, for any θ ∈Θ,
PX

BelY (·; X) ≤PY (·; θ); θ

≥1 −α.
(11)
If X = (Ξ1, . . . , Ξn) is a sequence of observations, a weaker requirement is to
demand that (11) holds in the limit, as n →∞. A PBF verifying (11), at
least asymptotyically, will be called a type-II PBF. For most of the samples,
a type-II PBF is a lower approximation of the true probability distribution of
Y . It can thus be compared to the plug-in distribution PY (·; θ), which is also
an approximation of PY (·; θ). However, the PBF will generally be non-additive,
as a consequence of accounting not only for random uncertainty, but also for
estimation uncertainty.
R3: Calibration
Another line of reasoning, advocated by Martin and Liu [18], is to consider that
plausibility values be calibrated, in the sense that the plausibility of the true value
Y should be small with only a small probability [18, Chap. 9]. More precisely,
for any θ ∈Θ and any α ∈(0, 1), we my impose the following condition,
PX,Y (plY (Y ; X) ≤α; θ) ≤α,
(12)
or, equivalently,
PX,Y (plY (Y ; X) > α; θ) ≥1 −α,
(13)

164
T. Denœux
where plY (Y ; X) = PlY ({Y }; X) is the contour function evaluated at Y . Equa-
tions (12) and (13) may hold only asymptotically, as the sample size tends to
inﬁnity. It follows from (13) that the sets {y ∈Y |plY (y; X) > α} are predic-
tion sets at level 1 −α (maybe, approximately). A PBF verifying (13) will be
called a type-III PBF. It can be seen as encoding prediction sets at all levels;
as such, it is somewhat similar to a frequentist predictive distribution; however,
it is not required to be additive. Requirement (13) is very diﬀerent from the
previous two. In particular, a type-III PBF has no connection with the Bayesian
predictive distribution, and it does not approximate the true distribution of Y .
Rather, (12) establishes a correspondence between plausibilities and frequencies.
A type III-PBF can be seen as a generalized prediction interval.
In the following section, we introduce a simple scheme that will allow us to
construct PBF of each of the three kinds above, for any parametric model. We
will also mention some alternative methods.
4
Construction of Predictive Belief Functions
In [14,15], the authors introduced a general method to construct PBFs, by writ-
ing the future data Y in the form
Y = ϕ(θ, V ),
(14)
where V is a pivotal variable with known distribution [6,15,18]. Equation (14) is
called a ϕ-equation. It can be obtained by inverting the cdf of Y . More precisely,
let us ﬁrst assume that Y is continuous; we can then observe that V = FY (Y ; θ)
has a standard uniform distribution. Denoting by F −1
Y (·; θ) the inverse of the cdf
FY (·; θ), we get
Y = F −1
Y (V ; θ),
(15)
with V ∼U ([0, 1]), which has the same form as (14). When Y is discrete, (15)
is still valid if F −1
Y
now denotes the generalized inverse of FY ,
F −1
Y (V ; θ) = inf{y|FY (y; θ) ≥V }.
(16)
Example 3. In the Air Conditioning example, it is assumed that Y ∼E (θ),
i.e., FY (y; θ) = 1 −exp(−θy). From the equality FY (Y ; θ) = V , we get
Y = −log(1 −V )
θ
,
(17)
with V ∼U ([0, 1]).
⊓⊔
Example 4. Let Y be the maximum sea level in the next m years, with cdf given
by (8). From the equality FY (Y ; θ) = V , we get Y = μ −σ log log(V −1/m), with
V ∼U ([0, 1]).
⊓⊔

Quantifying Predictive Uncertainty Using Belief Functions
165
The plug-in prediction is obtained by plugging the MLE θ in (14),
Y = ϕ(θ, V ).
(18)
Now, the Bayesian posterior predictive distribution can be obtained by replacing
the constant θ in (14) by a random variable θB with the posterior cdf Fθ(·; x).
We then get a random variable YB with cdf FB(y|x) given by (1). We can write
YB = ϕ(F −1
θ
(U|x), V ).
(19)
The three methods described in the sequel somehow generalize the above
methods. They are based on (14), and on belief functions Belθ and BelV on θ
and V induced, respectively, by random sets Γ(U; x) and Λ(W), where U and
W are random variables. The predictive belief function on Y is then induced by
the random set
Π(U, W; x) = ϕ(Γ(U; x), Λ(W)).
(20)
Assuming that Π(u, w; x) ̸= ∅for any u, v and x, we thus have
BelY (A; x) = PU,W

Π(U, W; x) ⊆A

and
PlY (A; x) = PU,W

Π(U, W; x) ∩A ̸= ∅

for all subset A ⊆Y for which these expressions are well-deﬁned.
The three methods described below diﬀer in the choice of the random sets
Γ(U; x) and Λ(W). As will we see, each of the three types of PBF described in
Sect. 3 can be obtained by suitably choosing these two random sets.
4.1
Type-I Predictive Belief Functions
As shown in [11], Requirements R0 and R1 jointly imply that the contour func-
tion pl(θ, x) associated to Belθ(·; x) should be proportional to the likelihood
function L(·; x). The least committed belief function (in some sense, see [11])
that meets this constraint is the consonant belief function deﬁned by the follow-
ing contour function,
pl(θ; x) = L(θ; x)
L(θ; x)
,
(21)
where θ is a maximizer of L(θ; x), i.e., a maximum likelihood estimate (MLE) of
θ, and it is assumed that L(θ; x) < +∞. As it is consonant, the plausibility of
any hypothesis H ⊆Θ is the supremum of the plausibilities of each individual
values of θ inside H,
Plθ(H; x) = sup
θ∈H
pl(θ; x).
(22)
The corresponding random set is deﬁned by
Γℓ(U) = {θ ∈Θ|pl(θ; x) ≥U}
(23)

166
T. Denœux
with U ∼U ([0, 1]), i.e., it is the set of values of θ whose relative likelihood
is larger than a uniformly distributed random variable U. This likelihood-based
belief function was ﬁrst introduced by Shafer [22], and it has been studied by
Wasserman [23], among others.
The prediction method proposed in [14,15] consists in choosing Belθ deﬁned
by (21)–(22) as the belief function on θ, and PV , the uniform probability dis-
tribution of V , as the belief function on V . The resulting PBF BelY,ℓ(·; x) is
induced by the random set
Πℓ(U, V ; x) = ϕ(Γℓ(U; x), V ),
(24)
where (U, V ) has a uniform distribution in [0, 1]2.
By construction, combining Belθ(·; x) with a Bayesian prior P 0
θ by Dempster’s
rule yields the Bayesian posterior PB(·|x). The random set (24) then becomes
ΠB(U, V ; x) = ϕ(F −1
B (U|x), V ),
(25)
with (U, V ) uniformly distribution in [0, 1]2. This random set is actually a ran-
dom point, i.e., a random variable, and this rv is identical to (19): its distribution
is the Bayesian posterior predictive distribution. Consequently, the PBF BelY,ℓ
constructed by this method meets requirements R0 and R1.
Example 5. The contour function for the AC data of Example 1, assuming an
exponential distribution, is shown in Fig. 2(a). As it is unimodal and continu-
ous, the sets Γℓ(u; x) are closed intervals [θ−(u), θ+(u)], whose bounds can be
approximated numerically as the roots of the equation pl(θ; x) = u. From (17),
the random set Πℓ(U, V ; x) is then the random closed interval
Πℓ(U, V ; x) = [Y −(U, V ; x), Y +(U, V ; x)],
with
Y −(U, V ; x) = −log(1 −V )
θ+(U)
and
Y +(U, V ; x) = −log(1 −V )
θ−(U)
.
As shown by Dempster [9], the following equalities hold, for any y ≥0,
BelY ((−∞, y]) = PU,V (Y +(U, V ; x) ≤y)
PlY ((−∞, y]) = PU,V (Y −(U, V ; x) ≤y),
i.e., they are the cdfs of, respectively, the upper and lower bounds of Πℓ. Func-
tions BelY ((−∞, y]) and PlY ((−∞, y]) are called the lower and upper cdfs of the
random set Πℓ(U, V ; x). As explained in [15], they can be approximated by Monte
Carlo simulation: let (ui, vi), i = 1, . . . , N be a pseudo-random sequence gener-
ated independently from the uniform distribution in [0, 1]2. Let y−
i = y−(ui, vi; x)
and y+
i
= y+(ui, vi; x) be the corresponding realizations of the bounds of Πℓ.

Quantifying Predictive Uncertainty Using Belief Functions
167
0.005
0.010
0.015
0.020
0.025
0.030
0.035
0.0
0.2
0.4
0.6
0.8
1.0
pl( )
(a)
0
100
200
300
400
0.0
0.2
0.4
0.6
0.8
1.0
y
lower/upper cdf
(b)
Fig. 2. AC data. (a): Contour function; (b): Lower and upper cdf (solid lines) and
plug-in cdf (dotted line)
Then, the lower and upper cdfs can be approximated by the empirical cdfs of the
y+
i
and the y−
i , respectively. These functions are plotted in Fig. 2(b), together
with the plug-in cdf FY (y; θ), with θ = 1/x. We can observe that the plug-in cdf
is always included in the band deﬁned by the lower and upper cdf, which is a
consequence of the inequalities θ−(u) ≤θ ≤θ−(u) for any u ∈(0, 1]. We note
that θ = θ−(1) = θ+(1).
⊓⊔
Example 6. Let us now consider the Sea Level data of Example 2. The contour
function (21) for these data is plotted in Fig. 3(a). As the level sets Γℓ(u; x)
of this function are closed and connected, the sets Πℓ(U, V ; x) still are closed
intervals in this case [15]. To ﬁnd the bounds Y −(u, v; x) and Y +(u, v; x) for any
pair (u, v), we now need to search for the minimum an the maximum of ϕ(θ, v),
under the constraint pl(θ; x) ≥u. This task can be performed by a nonlinear
constrained optimization algorithm. The lower and upper cdfs computed using
this method are shown in Fig. 3(b).
⊓⊔
4.2
Type-II Predictive Belief Functions
The ϕ-equation (14) also allows us to construct a type-II PBF, such as deﬁned
in [10]. Let C(X) be a conﬁdence set for θ at level 1 −α, i.e.,
PX(C(X) ∋θ; θ) = 1 −α.
(26)
Consider the following random set,
ΠY,c(V ; x) = ϕ(C(x), V ),
(27)
which is a special case of the general expression (20), with Γ(U; x) = C(x) for
all U ∈[0, 1], W = V and Λ(V ) = V . The following theorem states that the
belief function induced by the random set (27) is a type-II PBF.

168
T. Denœux
μ
σ
3.80
3.85
3.90
3.95
0.16
0.18
0.20
0.22
0.24
(a)
3.5
4.0
4.5
5.0
5.5
6.0
0.0
0.2
0.4
0.6
0.8
1.0
y
lower/upper cdf
(b)
Fig. 3. Port Pirie sea-level data: (a): Contour plot of the relative likelihood function;
(b): Lower and upper cdfs of the type-I PBF; the central broken line corresponds to
the plug-in prediction.
Theorem 1. Let Y = ϕ(θ, V ) be a random variable, and C(X) a conﬁdence
region for θ at level 1 −α. Then, the belief function BelU,c(·; x) induced by the
random set ΠY,c(V ; x) = ϕ(C(x), V ) veriﬁes
PX(BelY,c(·; X) ≤PY (·; θ); θ) ≥1 −α,
(28)
i.e., it is a type-II PBF.
Proof. If θ ∈C(x), then ϕ(θ, V ) ∈ϕ(C(x), V ) for any V . Consequently, the
following implication holds for any measurable subset A ⊆Y , and any x ∈X ,
ϕ(C(x), V ) ⊆A ⇒ϕ(θ, V ) ∈A.
Hence,
PV (ϕ(C(x), V ) ⊆A) ≤PV (ϕ(θ, V ) ⊆A),
or, equivalently,
BelY,c(A; x) ≤PY (A; θ).
(29)
As (29) holds whenever θ ∈C(x), and PX(C(X) ∋θ; θ) = 1 −α, it follows that
(29) holds for any measurable event A with probability at least 1 −α, i.e.,
PX

BelY,c(·; X) ≤PY (·; θ); θ

≥1 −α.
⊓⊔
If C(X) is an approximate conﬁdence region, then obviously (28) will hold
only approximately. In the case where X = (X1, . . . , Xn) is iid, the likeli-
hood function will often provide us with a means to obtain a conﬁdence region
on θ. From Wilks’ theorem [24], we know that, under regularity conditions,
−2 log pl(θ; X) has approximately, for large n, a chi square distribution with p
degrees of freedom, where p is the dimension of θ. Consequently, the sets
Γℓ(c; X) = {θ ∈Θ|pl(θ; X) ≥c,

Quantifying Predictive Uncertainty Using Belief Functions
169
Table 2. Likelihood levels c deﬁning approximate 95% conﬁdence regions.
p 1
2
5
10
15
c 0.15 0.5 3.9e-03 1.1e-04 3.7e-06
with c = exp(−0.5χ2
p;1−α), are approximate conﬁdence regions at level 1 −α.
The corresponding predictive random set is
ΠY,c(V ; x) = ϕ(Γℓ(c; x), V ).
(30)
We can see that this expression is similar to (24), except that, in (30), the
relative likelihood function is cut at a ﬁxed level c. A similar idea was explored
in Ref. [26]. Table 2 gives values of c for diﬀerent values of p and α = 0.05.
We can see that c decreases quickly with p, which means that the likelihood-
based conﬁdence regions and, consequently, the corresponding PBFs will become
increasing imprecise as p increases. In particular, the likelihood-based type-II
PBFs will typically be less committed than the type-I PBFs.
Example 7. For the AC data, the likelihood-based conﬁdence level at level 1 −
α = 0.95 is
[θ−(c), θ+(c)] = [0.01147, 0.02352],
with c = 0.15. It is very close to the exact conﬁdence level at the same level,
⎡
⎣
θχ2
α/2,2n
2n
,
θχ2
1−α/2,2n
2n
⎤
⎦= [0.01132, 0.02329].
The corresponding Type-II PBF is induced by the random interval
ΠY,c(V ; x) =

−log(1 −V )
θ+(c)
, −log(1 −V )
θ−(c)

.
The lower and upper bounds of this interval have exponential distributions with
rates θ+(c) and θ−(c), respectively. Figure 4 shows the corresponding lower and
upper cdfs, together with those of the Type-I PBF computed in Example 5. We
can see that the Type-II PBF at the 95% conﬁdence level is less committed than
the Type-I PBF.
Example 8. Figure 5 shows the lower and upper cdfs of the type-II PBF con-
structed from the likelihood-based conﬁdence region with α = 0.05. The estimate
of the true coverage probability, obtained using the parametric bootstrap method
with B = 5000 bootstrap samples, was 0.94998, which is remarkably close to
the nominal level. The simulation method to compute these functions is simi-
lar to that explained in Example 6, except that we now have ui = c = 0.05 for
i = 1, . . . , n. The lower and upper cdfs form a conﬁdence band on the true cdf of
Y . Again, we observe that this band is larger than the one corresponding to the
type-I PBF.

170
T. Denœux
0
100
200
300
400
0.0
0.2
0.4
0.6
0.8
1.0
y
lower/upper cdf
Fig. 4. Lower and upper cdfs of the type-II PBF for the AC data (solid lines). The
type-I lower and upper cdf are shown as broken lines.
3.5
4.0
4.5
5.0
5.5
6.0
0.0
0.2
0.4
0.6
0.8
1.0
y
lower/upper cdf
Fig. 5. Lower and upper cdfs of the type-II PBF for the sea-level example (solid lines).
The type-I lower and upper cdf are shown as broken lines.
4.3
Type-III Predictive Belief Functions
The calibration condition (12) was introduced by Martin and Liu [17,18], in the
context of their theory of Inferential Models (IMs). An equivalent formulation is
to require that the random variable plY (Y ; X) be stochastically not less than a
random variable having a standard uniform distribution. In [18, Chap. 9], Martin
and Liu propose a quite complex method for constructing PBFs verifying this
requirements, based on IMs. It turns out that such Type-III PBFs (as we call
them in this paper) can be generated by a simple construction procedure based
on the ϕ-equation (14) and suitable belief functions on θ and V . Because the
notion of Type-III PBFs is intimately related to prediction sets, and prediction
sets at a given level can only be deﬁned for continuous random variables, we will
assume Y to be continuous in this section.

Quantifying Predictive Uncertainty Using Belief Functions
171
Let us ﬁrst assume that θ is known. In that case, predicting Y = ϕ(θ, V )
boils down to predicting V = F(Y ; θ). Consider the random interval
Λ(W) =
W
2 , 1 −W
2

,
with W ∼U ([0, 1]). It is easy to check that the induced contour function is
pl(v) = 1 −|2v −1| (it is a triangular possibility distribution with support [0, 1]
and mode 0.5), and pl(V ) ∼U ([0, 1]). Consider the predictive random set
ΠY (W) = ϕ(θ, Λ(W))
(31)
and the associated contour function
pl(y) = 1 −|1 −2F(y; θ)|.
(32)
It is clear that pl(Y ) = pl(V ) ∼U ([0, 1]), and the consonant belief function with
contour function (32) veriﬁes the calibration property (12). We can observe that
the transformation (32) from the probability distribution of Y to this possibility
distribution is an instance of the family of probability-possibility transformations
studied in [12]. The mode of the possibility distribution is the median y0.5 =
ϕ(θ, 0.5), and each α-cut ΠY (α) = [yα/2, y1−α/2] with α ∈(0, 1) is a prediction
interval for Y , at level 1 −α.
Until now, we have assume θ to be known. When θ is unknown, we could
think of replacing it by its MLE θ, and proceed as above by applying the
same probability-possibility distribution to the plug-in predictive distribution
FY (u; θ). As already mentioned, this approach would amount to neglecting the
estimation uncertainty, and the α-cuts of the resulting possibility distribution
could have a coverage probability signiﬁcantly smaller than 1−α. A better app-
roach, following [16], is to consider the exact or approximate pivotal quantity
V = F(Y ; θ(X)). We assume that θ is a consistent estimator of θ as the infor-
mation about θ increases, and V is asymptotically distributed as U ([0, 1]) [16].
However, for ﬁnite sample size, the distribution of V will generally not be uni-
form. Let G be the cdf of V , assuming that it is pivotal, and let Λ(W) be the
random interval
Λ(W) =

G−1(W/2), G−1(1 −W/2)

with W ∼U ([0, 1]) and corresponding contour function
pl(v) = 1 −
1 −2G(v)
 .
The random set
ΠY (W; x) = ϕ(θ(x), Λ(W))

172
T. Denœux
induces the contour function
pl(y; x) = 1 −
1 −2G{F[y; θ(x)]}
 .
(33)
As G(F(Y ; θ(X))) ∼U ([0, 1]), we have pl(Y ; X) ∼U ([0, 1]), and the focal
sets ΠY (α; X) are exact prediction intervals at level 1 −α. Consequently, the
consonant belief function with contour function (33) is a type-III PBF. We can
remark that it is obtained by applying the probability-possibility transformation
(32) to the predictive conﬁdence distribution F(y; x) = G{F[y; θ(x)]}.
When an analytical expression of the cdf G is not available, or V is only
asymptotically pivotal, an approximate distribution G can be determined by
a parametric bootstrap approach [16]. Speciﬁcally, let x∗
1, . . . , x∗
B be B and
y∗
1, . . . , y∗
B be B bootstrap replicates of x and y, respectively. We can compute
the corresponding values v∗
b = F(y∗
i ; θ(x∗
b)), b = 1, . . . , B, and the distribution
of V can be approximated by the empirical cdf
G(v) = 1
B
B

b=1
I(v∗
b ≤v).
Example 9. Consider again the AC example. For the exponential distribution,
it has been shown [16] that the quantity
V = F(Y, θ(X)) = 1 −exp(−Y θ(X))
is pivotal, and has the following cdf,
G(v) = 1 −

1 −1
n log(1 −v)
 −n
.
The predictive cdf is then
F(y; x) = G{F(y, θ(x))} = 1 −

1 + yθ(x)
n
−n
and the contour function of the type-III PBF is
pl(y; x) = 1 −

2

1 + yθ(x)
n
−n
−1

.
(34)
Figure 6(a) shows the contour function (34) for the AC data (solid line), together
with the contour function induced by the plug-in distribution (interrupted line). The
two curves are quite close in this case: for n = 30, the distribution of V is already
very close to the standard uniform distribution. Figure 6(b) shows the lower and
upper cdfs of the PBF, together with the Type-I and Type-II (1 −α = 0.95) lower

Quantifying Predictive Uncertainty Using Belief Functions
173
0
100
200
300
400
0.0
0.2
0.4
0.6
0.8
1.0
y
pl(y;x)
(a)
0
100
200
300
400
0.0
0.2
0.4
0.6
0.8
1.0
y
lower/upper cdf
type III
type I
type II
(b)
Fig. 6. AC example. (a): Contour function of the type-III PBF (solid line), and contour
function induced by the plug-in distribution (interrupted line); (b): Lower and upper
cdfs of the type-III PBF (solid lines). The type-I and type-II lower and upper cdf are
shown, respectively, as interrupted and dotted lines.
and upper cdfs for the same data. As the Type-III PBF is consonant, the lower and
upper cdfs can computed from the contour function as
PlY ((−∞, y]) = sup
y′≤y
pl(y′) =
!
pl(y; x)
if y ≤F −1(0.5; x)
1
otherwise,
and
BelY ((−∞, y]) = 1 −sup
y′>y
pl(y′) =

1 −pl(y; x)

I
"
y > F −1(0.5; x)
#
.
⊓⊔
Example 10. Let us now consider again the sea-level data. Here, the exact dis-
tribution of the quantity V = FY (Y ; θ(X)) is intractable, but it can be estimated
by the parametric bootstrap technique. Figure 7(a) shows the bootstrap estimate
of the distribution of V , with B = 10000. There is clearly a small, but discernible
departure from the uniform distribution. Figure 7(b) shows the contour function
of the type-III PBF, together with that induced by the plug-in predictive distri-
bution (corresponding to the approximation G(v) = v). Again, the two curves
are close, but clearly discernible. With n = 65, the prediction intervals computed
from the plug-in distribution have true coverage probabilities quite close to the
stated ones. Finally, the lower and upper cdf of the type-III PBF for the Port-
Pirie data are shown in Fig. 7(c), together with the corresponding functions for
the type-I and type-II PBFs. Comparing Figs. 6(b) and 7(c), we can see that,
in both cases, the type-I PBF is less committed than the type-III PBF. It is not
clear, however, whether this result holds in general.
⊓⊔

174
T. Denœux
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
v
G(v)
(a)
3.5
4.0
4.5
5.0
5.5
6.0
0.0
0.2
0.4
0.6
0.8
1.0
y
pl(y;x)
(b)
3.5
4.0
4.5
5.0
5.5
6.0
0.0
0.2
0.4
0.6
0.8
1.0
y
lower/upper cdf
(c)
Fig. 7. Sea-level example. (a): Bootstrap estimate of the cdf G of V = F(Y, θ(X));
(b): Contour function of the type-III PBF (solid line), and contour function induced
by the plug-in distribution (interrupted line); (c): Lower and upper cdfs of the type-III
PBF (solid lines). The type-I and type-II lower and upper cdf are shown, respectively,
as interrupted and dotted lines.
5
Conclusions
Being related to random sets, belief functions have greater expressivity than
probability measures. In particular, the additional degrees of freedom of the
belief function framework make it possible to distinguish between lack of infor-
mation and randomness. In this paper, we have considered diﬀerent ways of
exploiting this high expressivity to quantify prediction uncertainty. Based on
three distinct requirements, three diﬀerent kinds of predictive belief functions
have been distinguished, and construction procedures for each of them have
been proposed. Type-I belief functions have a Bayesian ﬂavor, and boil down
to Bayesian posterior predictive belief functions when a prior probability distri-
bution on the parameter is provided. In contrast, belief functions of the other
types are frequentist in spirit. Type-II belief functions correspond to a family of
probability measures, which contain the true distribution of the random variable
of interest with some probability, in a repeated sampling setting. Type-III belief

Quantifying Predictive Uncertainty Using Belief Functions
175
functions are “frequency-calibrated”, in so far as the true value of the variable of
interest rarely receives a small plausibility. It should be noticed by “frequentist”
predictive belief functions (of types II and III) are not compatible with Bayesian
inference, i.e., they do not allow us to recover the Bayesian posterior predic-
tive distribution when combined with a Bayesian prior. It thus seems that the
Bayesian and frequentist views cannot be easily reconciled, and diﬀerent infer-
ence procedures have to coexist, just as frequentist and Bayesian procedures in
mainstream statistics. Beyond philosophical arguments, the practicality of these
construction procedures, as well as their interpretability and acceptability by
decision-makers remain to be investigated.
References
1. Aregui, A., Denœux, T.: Constructing predictive belief functions from continuous
sample data using conﬁdence bands. In: G. De Cooman, J. Vejnarov´a, M. Zaﬀalon
(eds.) Proceedings of the Fifth International Symposium on Imprecise Probabil-
ity: Theories and Applications (ISIPTA 2007), pp. 11–20. Prague, Czech Republic
(2007)
2. Barndorﬀ-Nielsen, O.E., Cox, D.R.: Prediction and asymptotics. Bernoulli 2(4),
319–340 (1996)
3. Berger, J.O., Wolpert, R.L.: The likelihood principle: a review, generalizations, and
statistical implications. Lecture Notes-Monograph Series, vol. 6, 2nd edn. Institute
of Mathematical Statistics, Hayward (1988)
4. Birnbaum, A.: On the foundations of statistical inference. J. Am. Stat. Assoc.
57(298), 269–306 (1962)
5. Coles, S.G.: An Introduction to Statistical Modelling of Extreme Values. Springer,
London (2001)
6. Dempster, A.P.: New methods for reasoning towards posterior distributions based
on sample data. Ann. Math. Stat. 37, 355–374 (1966)
7. Dempster, A.P.: Upper and lower probabilities induced by a multivalued mapping.
Ann. Math. Stat. 38, 325–339 (1967)
8. Dempster, A.P.: A generalization of Bayesian inference (with discussion). J. Roy.
Stat. Soc. B 30, 205–247 (1968)
9. Dempster, A.P.: Upper and lower probabilities generated by a random closed inter-
val. Ann. Math. Stat. 39(3), 957–966 (1968)
10. Denœux, T.: Constructing belief functions from sample data using multinomial
conﬁdence regions. Int. J. Approx. Reason. 42(3), 228–252 (2006)
11. Denœux, T.: Likelihood-based belief function: justiﬁcation and some extensions to
low-quality data. Int. J. Approx. Reason. 55(7), 1535–1547 (2014)
12. Dubois, D., Foulloy, L., Mauris, G., Prade, H.: Probability-possibility transforma-
tions, triangular fuzzy sets, and probabilistic inequalities. Reliab. Comput. 10(4),
273–297 (2004)
13. Edwards, A.W.F.: Likelihood, expanded edn. The John Hopkins University Press,
Baltimore (1992)
14. Kanjanatarakul, O., Sriboonchitta, S., Denœux, T.: Forecasting using belief func-
tions: an application to marketing econometrics. Int. J. Approx. Reason. 55(5),
1113–1128 (2014)

176
T. Denœux
15. Kanjanatarakul, O., Sriboonchitta, S., Denœux, T.: Statistical estimation and pre-
diction using belief functions: principles and application to some econometric mod-
els. Int. J. Approx. Reason. 72, 71–94 (2016)
16. Lawless, J.F., Fredette, M.: Frequentist prediction intervals and predictive distri-
bution. Biometrika 92(3), 529–542 (2005)
17. Martin, R., Lingham, R.T.: Prior-free probabilistic prediction of future observa-
tions. Technometrics 58(2), 225–235 (2016)
18. Martin, R., Liu, C.: Inferential Models: Reasoning with Uncertainty. CRC Press,
Boca Raton (2016)
19. Nguyen, H.T.: On random sets and belief functions. J. Math. Anal. Appl. 65,
531–542 (1978)
20. Nguyen, H.T.: An Introduction to Random Sets. Chapman and Hall/CRC Press,
Boca Raton (2006)
21. Olkin, I., Gleser, L., Derman, C.: Probability Models and Applications. Macmillan,
New York (1994)
22. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press,
Princeton (1976)
23. Wasserman, L.A.: Belief functions and statistical evidence. Can. J. Stat. 18(3),
183–196 (1990)
24. Wilks, S.S.: The large-sample distribution of the likelihood ratio for testing com-
posite hypotheses. Ann. Math. Stat. 9(1), 60–62 (1938)
25. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. 1,
3–28 (1978)
26. Zhu, K., Thianpaen, N., Kreinovich, V.: How to make plausibility-based forecasting
more accurate. In: Kreinovich, V., Sriboonchitta, S., Huynh, V.N. (eds.) Robust-
ness in Econometrics, pp. 99–110. Springer, Berlin, Cham, Switzerland (2017)

Kuznets Curve: A Simple Dynamical
System-Based Explanation
Thongchai Dumrongpokaphan1 and Vladik Kreinovich2(B)
1 Department of Mathematics, Faculty of Science, Chiang Mai University,
Chiang Mai, Thailand
tcd43@hotmail.com
2 Department of Computer Science, University of Texas at El Paso,
500 W. University, El Paso, TX 79968, USA
vladik@utep.edu
Abstract. In the 1950s, a future Nobelist Simon Kuznets discovered the
following phenomenon: as a country’s economy improves, inequality ﬁrst
grows but then decreases. In this paper, we provide a simple dynamical
system-based explanation for this empirical phenomenon.
1
Kuznets Curve: A Brief Reminder and Need for an
Explanation
What is the Kuznets curve. In the 1950s, Simon Kuznets, an American
economist of Russian origin, showed that as the country’s Gross Domestic Prod-
uct (GDP) increases, inequality ﬁrst increases and then decreases again [1,2,4].
The resulting dependence on inequality on GDP looks like an inverted letter U
and is thus called an inverted U-shaped dependence or the Kunzets curve. For
this work, Professor Kuznets was awarded a Nobel Prize in Economics in 1971.
Kuznets curve: a controversy. The Kuznets curve is a purely empirical obser-
vation. Economists from diﬀerent sides of the political spectrum have come up
with diﬀerent (and mutually exclusive) explanations for this empirical fact.
On the one hand, free-market champions use the Kuznets curve as an argu-
ment that the governments should not interfere with the free market: inequality
will decrease by itself, as soon as the economy improves further. As Ronald
Reagan used to say, the rising tide lifts all the boats. Based on this argument,
these economists recommend that the best way to decrease inequality is to min-
imize the number of government regulations, and the free market will take care
of it.
On the other hand, economists who support the need for government regula-
tions note that while the decrease in inequality may indeed be an empirical fact,
in all the developed countries, there was a lot of government intervention, and
this intervention is what caused the inequality to decrease. Based on this argu-
ment, they recommend that the best way to decrease inequality is to continue
with the government regulations.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_9

178
T. Dumrongpokaphan and V. Kreinovich
An additional controversy. It should be mentioned that there is an additional
controversy related to the Kuznets curve: namely, some researcher doubt that
the Kunzets curve is indeed a universal phenomenon; see, e.g., [3].
What we do in this paper. In this paper, we show that the Kuznets curve
phenomenon naturally follows from the general system-based analysis.
2
Analysis of the Problem
Let us describe the phenomenon in precise terms. We start in a situation
when the overall economic output is small and therefore, most everyone is poor.
In such situations, while there may be a small minority of relatively rich people,
most people are poor. In this sense, there is not much inequality.
As the economy grows, people’s incomes grow. For each person, his or her
income grows until it reaches the level mi expressing the capability of this person
to earn money in the corresponding economy. People are diﬀerent, so they have
somewhat diﬀerent rates vi at which they move towards this larger income: some
go faster, some go slower.
For simplicity, we can assume that for each person, the rate does not change
with time, i.e., that the income of the i-th person income increases at this rate
until it reaches the value mi. At the rate vi, this takes time mi
vi
. So, under this
assumption, at each moment of time t, the income xi(t) of the i-th person is
equal to:
• xi(t) = vi · t when t ≤mi
vi
, and
• xi(t) = mi for t ≥mi
vi
.
The values mi are centered around the mean m, with random deviations
Δmi
def
= mi −m.
Similarly, the rates vi center around the mean v, with random deviations Δvi
def
=
vi −v.
Since there is no reason to believe that there is a correlation between mi and
vi, we will assume these variables to be independent.
How can we describe inequality. Perfect equality means that everyone’s
income is the same. This is equivalent to saying that the standard deviation of
income is 0. In general, if the standard deviation is equal to 10% of the average
income, then it is reasonable to conclude that we have less inequality that when
the standard deviation is equal to 20% of the average income. Thus, a natural
measure of inequality is the ratio between the income’s standard deviation and
its mean value.

Kuznets Curve: A Simple Dynamical System-Based Explanation
179
Now, we are ready to analyze how inequality changes when the economy
improves. Kuznets curve considers three stages:
• the starting stage, when the inequality level is relatively low,
• the intermediate stage, when the level of inequality increases, and
• the ﬁnal stage, when the level of inequality decreases.
We have already discussed that in the beginning, there is practically no inequal-
ity. So, to complete our analysis, we need to consider two other stages: the
intermediate stage and the ﬁnal stage.
What happens on the ﬁnal stage. Let us start with the ﬁnal stage, because,
as we will see, this stage is easier to analyze. In this ﬁnal stage, everyone reaches
their potential mi. Thus:
• the average income is equal to the average m of the values mi, and
• the standard deviation is equal to the standard deviation σm of the diﬀer-
ences Δmi.
So, on the ﬁnal stage, the inequality level is equal to the ratio
σm
m .
(1)
What happens on the intermediate stage. In the beginning of the inter-
mediate stage, when few people have reaches their potential mi, the income of
each person is equal to
xi(t) = mi
vi
· t = m + Δmi
v + Δvi
· t.
Here, m + Δmi can be represented as m ·

1 + Δmi
m

and similarly, v + Δvi
can be represented as v ·

1 + Δvi
v

. Thus,
xi(t) = m
v · t ·
1 + Δmi
m
1 + Δvi
v
.
Diﬀerences between diﬀerent people are, in most cases, not so large, so |Δmi| ≪
m and Δmi
m
≪1. Similarly, we can conclude that |Δvi| ≪v and thus, Δvi
v
≪1.
Thus, we can expand the above expression for xi(t) in terms of the small values
Δmi
m
and Δvi
v
and keep only linear terms in this expansion. As a result, we get
the following formula:
xi(t) = m
v · t ·

1 + Δmi
m
−Δvi
v

.

180
T. Dumrongpokaphan and V. Kreinovich
The mean value of Δmi and Δvi is 0, so the mean income is equal to
x(t) = m
m · t.
The standard deviation of Δmi is equal to σm, so the standard deviation of the
ratio Δmi
m
is equal to σm
m . Similarly, the standard deviation of the ratio Δvi
v
is
equal to σv
v .
Since the quantities Δmi and Δvi are assumed to be independent, the vari-
ance of the expression
1 + Δmi
m
−Δvi
v
(2)
is equal to the sum of the variances of Δmi
m
and Δvi
v . Thus, the corresponding
standard deviation is equal to

σ2m
(m)2 + σ2v
(v)2 .
The formula for xi(t) is obtained by multiplying this expression (2) by a constant
m
v · t. Thus, the standard deviation σx(t) can be obtained by multiplying the
standard deviation of the above expression (2) by the same constant:
σx(t) = m
v · t ·

σ2m
(m)2 + σ2v
(v)2 .
Dividing this standard deviation y the mean x(t), we get the following formula
for the inequality level at the intermediate stage:
σx(t)
x(t) =

σ2m
(m)2 + σ2v
(v)2 .
(3)
Conclusion. By comparing the inequality level (3) at the intermediate stage
and the inequality level (1) at the ﬁnal stage, one can easily see that at the
intermediate stage, the inequality is higher:

σ2m
(m)2 + σ2v
(v)2 > σm
m .
This is exactly the Kuznets curve phenomenon.
Thus, we have indeed arrived at a simple justiﬁcation of the Kuznets curve
phenomenon.
Acknowledgments. This work was supported by Chiang Mai University, Thailand.
This work was also supported in part by the National Science Foundation grants HRD-
0734825 and HRD-1242122 (Cyber-ShARE Center of Excellence) and DUE-0926721,
and by an award “UTEP and Prudential Actuarial Science Academy and Pipeline
Initiative” from Prudential Foundation.

Kuznets Curve: A Simple Dynamical System-Based Explanation
181
References
1. Kuznets, S.: Economic growth and income inequality. Am. Econ. Rev. 45, 1–28
(1955)
2. Maneejuk, P., Pastpipatkul, P., Sriboonchitta, S.: Economic growth and income
inequality: evidence from Thailand. In: Huynh, V.-N., Inuiguchi, M., Le, B., Le,
B.N., Denoeux, T. (eds.) Proceedings of the 5th International Symposium on Inte-
grated Uncertainty in Knowledge Modeling and Decision Making IUKM 2016, pp.
649–663. Springer, Cham (2016)
3. Roberts, J.T., Thanos, N.D.: Trouble in Paradise: Globalization and Environmental
Crises in Latin America. Routledge, London & New York (2003)
4. Yandle, B., Vijayaraghavan, M., Bhattarai, M.: The Environmental Kuznets Curve:
A Primer. The Property and Environment Research Center (2000)

A Calibration-Based Method in Computing
Bayesian Posterior Distributions
with Applications in Stock Market
Dung Tien Nguyen, Son P. Nguyen, Uyen H. Pham(B),
and Thien Dinh Nguyen
University of Economics and Law, VNU-HCM, Ho Chi Minh City, Vietnam
uyenph@uel.edu.vn
Abstract. Finding eﬀective methods to compute or estimate posterior
distributions of model parameters is of paramount importance in Bayesian
statistics. In fact, Bayesian inference has only been extraordinarily popular
in applications after the births of eﬃcient algorithms like the Monte Carlo
Markov Chain. Practicality of posterior distributions depends heavily on
the combination of likelihood functions and prior distributions. In certain
cases, closed-form formulas for posterior distributions can be attained; in
this paper, based on the theory of distortion functions, a calibration-like
method to calculate explicitly the posterior distributions for three crucial
models, namely the normal, Poisson and Bernoulli is introduced. The paper
ends with some applications in stock market.
Keywords: Calibration · Distortion function · Bayesian statistics
Posterior estimation · Stock market
1
Introduction
On the brink of the Fourth Industrial Revolution or Industry 4.0, data analytics
has been playing a more and more vital role in every sector of any country’s
economy, especially in business and ﬁnance. Among the most eﬀective tools in
studying data is Bayesian statistics. In brief, Bayesian inference is the process
of ﬁtting a probability model to a set of data and summarizing the result by a
probability distribution on the parameters of the model and on unobserved quan-
tities such as predictions for new observations. According to [1], the Bayesian
framework can be divided into the following three steps:
1. Setting up a full probability model - a joint probability distribution for all
observable and unobservable quantities in a problem. The model should be
consistent with knowledge about the underlying scientiﬁc problem and the
data collection process.
2. Conditioning on observed data: calculating and interpreting the appropriate
posterior distribution - the conditional probability distribution of the unob-
served quantities of ultimate interest, given the observed data.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_10

A Calibration-Based Method
183
3. Evaluating the ﬁt of the model and the implications of the resulting posterior
distribution: does the model ﬁt the data, are the substantive conclusions
reasonable, and how sensitive are the results to the modeling assumptions in
step 1? If necessary, one can alter or expand the model and repeat the three
steps.
In short, we start with a likelihood function which describes how our phe-
nomenon of interest generates data. Moreover, we are equipped with some prior
information in the forms of expertise in the data domain or some observations
on the data characteristics. This prior knowledge will also be described by a
probability distribution called the prior distribution. In other words, the prior
distribution summarizes things we know about the phenomenon before the data
are analyzed. The main work of the whole process is step 2, where we combine
the prior distribution with data (via the likelihood function) to create the pos-
terior distribution. Hence, the posterior distribution summarizes what is known
about the phenomenon after the data are taken into account.
The major contribution of this paper is another perspective on step 2. In our
opinion, the posterior distribution is created by calibrating the prior distribution
with data. The classical approach is via the likelihood function. We propose an
alternative method for the calibration process using the theory of distortion
functions.
2
A Review of Baysesian Statistics
In this section, four most fruitful likelihood functions in application are reviewed
with some basic well-known facts that serves as a preliminary to the new app-
roach in the next section.
In the following, let n and ¯x be the size and the mean of the collected data x,
respectively.
2.1
Normal distribution
Suppose, there is a normal-distributed population with known variance σ2.
We would like to infer the unknown mean μ. Let the likelihood function be
f(θ | x, σ2). In order to have a closed-form formula for the posterior distribu-
tion, we impose a normal prior distribution π(θ | μ0, α2) on the parameter space
of μ. Here, both μ0 and α2 are known. Then, the posterior distribution for μ is
also a normal distribution with mean
Eposterior (θ) =
(σ2/n)
(σ2/n) + α2 μ0 +
α2
(σ2/n) + α2 ¯x
and variance
Varposterior (θ) =
1
1/(σ2/n) + 1/α2
Thus, the posterior mean is a weighted sum of the prior mean and the sample
mean. Similarly, the posterior variance is half the harmonic mean of the prior
variance and the variance of the sample mean.

184
D. T. Nguyen et al.
2.2
Poisson Distribution
In this case, f(θ | x) is the Poisson likelihood of with mean θ as the unknown
parameter. The prior π(θ | α, β) is a gamma distribution with constant parame-
ters α and β. Then, the posterior is also a gamma distribution with parameters
α + x1 + · · · + xn and β + n, respectively. The mean is as follows
Eposterior (θ) = (α + x1 + · · · + xn)
(β + n)
= (1/n · α/β + 1/β · ¯x)
(1/n + 1/β)
This is the Bayes estimator of the mean of the Poisson. It is a weighted average
of the prior mean α/β and the sample mean ¯x from the data.
2.3
Gamma Distribution
Consider the case when f(θ | ν, x) is the Gamma likelihood with unknown para-
meter θ. (This includes the case ν = 1 of an exponential distribution with para-
meter θ, and the case ν = 1/2 of the squared normal distribution with mean zero
and variance 1/2θ). The prior π(θ | α, β) is chosen to be a gamma distribution
with parameters α and β. Then, the posterior is also a gamma distribution with
parameters α + nν and β + x1 + · · · + xn, respectively. The posterior mean is
Eposterior(θ) =
(α + nν)
(β + x1 + · · · + xn) =
(1/(nν) + 1/α
(1/(nν) · β/α + 1/α · ¯x/ν)
This is the Bayes estimator of θ. It is a weighted harmonic average of the α/β
and the estimator ν/¯x from the sample.
2.4
Bernoulli Distribution
Consider the Bernoulli likelihood f(θ | x) = θx(1−θ)(1−x) with unknown mean θ.
The prior π(θ | α, β) is selected to be a beta distribution with parameters α and
β. Then, the posterior is also a beta distribution with parameters α+x1+· · ·+xn
and β + n −(x1 + · · · + xn) with mean
Eposterior(θ) =
(α + x1 + · · · + xn)
(α + x1 + · · · + xn + β + n −(x1 + · · · + xn))
= (α + x1 + · · · + xn)
(α + β + n)
= (1/n · α/(α + β) + 1/(α + β) · ¯x)
(1/n + 1/(α + β))
This is the Bayes estimator of the parameter θ. It is a weighted average of
the prior probability α/(α + β)and the sample proportion ¯x.

A Calibration-Based Method
185
3
Distortion Function
Recall that distortion functions arose from the needs of a universal framework
for pricing both ﬁnancial and insurance risk in the 1990s.
In general, a function g : [0, 1] −→[0, 1] is a distortion function if g is non-
decreasing and g satisﬁes g(0) = 0, g(1) = 1.
A distortion function allows us to adjust the probability measure deﬁned by
some distribution function F(x). To be precise, let G be the function such that
1 −G(x) = g(1 −F(x))
Then, we have the following theorem:
Theorem 1. G is also a cumulative distribution function if and only if g is left
continuous at any u = 1 −F(x), where x ∈R+ and g is right-continuous at 0.
Theorem 1 shows that any distortion function g satisfying the theorem’s
hypothesis transforms a distribution into a new distribution.
Wang, in his pioneer works, devised the following distortion function (see
[4]).
Deﬁnition 1. Let λ ∈R be a parameter. Deﬁne
gλ(p) = Φ(Φ−1(p) + λ), 0 ≤p ≤1
where Φ is the cumulative distribution function of the standard normal distribution.
Wang’s function has been highly successful in capital asset pricing model
(CAPM) and in evaluation of insurance risk. One highlight is the fact that it
oﬀers another way to construct the Black-Scholes equation. On the other hand,
it has seen deep connections with expected utility theory and decision theory.
Following the succeses of Wang’s function, other authors such as Wirch (see
[5]) have invented various other distortion functions and have also found a wide
variety of applications.
Among diverse applications, the abilities of distortion functions to quantify
certain subjective evaluations particularly interest us. That has led us to deﬁne
two new families of distortion functions (see the next section). We have also
found a novel method to update some important prior distributions to the corre-
sponding posterior distributions in the bayesian framework.
Remark 1. All distortion functions so far satisfy Theorem 1. Our new families
are of no exception.
4
Main Results
4.1
Two New Distortion Functions
In this section, we introduce two new families of distortion functions.

186
D. T. Nguyen et al.
First is a distortion family to estimate the posterior distribution for the
Bernoulli proportion θ in the next section. This family is derived from the beta
distribution on two parameters.
Let I be the cumulative distribution function of the beta distribution on two
parameters α > 0 and β > 0, and I−1 be the corresponding quantile function.
Deﬁnition 2. The dual beta distortion function g : [0, 1] −→[0, 1] has four
parameters α, β, α′, β′, and is deﬁned as follows
g(p) = 1 −I

I−1(1 −p, α, β), α′, β′
Second is a distortion family built from the Gamma distribution which will
be used to construct the posterior for the Poisson rate parameter.
Let Γ be the cumulative distribution function of the gamma distribution
on two parameters α > 0 and β > 0, and Γ −1 be the corresponding quantile
function.
Deﬁnition 3. The dual gamma distortion function g : [0, 1] −→[0, 1] has four
parameters α, β, α′, β′, and is deﬁned as follows
g(p) = 1 −Γ

Γ −1(1 −p, α, β), α′, β′
4.2
Posterior Estimations
With a prior distribution and a functional form of the likelihood function, it
is usually not easy to estimate the posterior distribution of the desired para-
meter. The reason is that numerical computations of integration are, in many
cases, intractable. One popular way to avoid heavy calculations is to choose a
prior distribution compatible with the likelihood function so that a closed-form
formula for the posterior distribution exists, and can be computed theoretically.
In this work, when a close-formed formula exists, we propose a novel method
to obtain the posterior distribution with no integration of the likelihood func-
tion. The gem of the method lies in the simplicity of the calculations. We can
instantly write down the cumulative distribution function (cdf) for the posterior
by calibrating the prior cdf with an appropriate distortion function.
The ﬁrst theorem concerns the Wang’s distortion function (see [2]). Here, the
parameter of interest is the mean θ of a normally distributed population with
known variance σ2. We show that if the prior distribution for θ is chosen to be also
a normal distribution, then the Wang’s distortion function can always calibrate
the prior distribution to achieve the correct formula for posterior distribution.
Theorem 2. Let g(p) = Φ

λ1Φ−1(p) + λ

, 0 ≤p ≤1 be the Wang’s distortion
function with two parameters λ1 and λ. Suppose the prior distribution is the
normal distribution N(μ0, α2). Denote F(θ) to be the standardized cumulative
distribution of the prior distribution
F(θ) = Φ
θ −μ0
α


A Calibration-Based Method
187
Let
λ =
|α|
σ
√n

σ2
n + α2
λ1 = α



	
1
σ2
n + 1
α2
Then, the standardized cumulative distribution function G(θ) of the posterior
distribution is as follows
1 −G(θ) = g(1 −F(θ))
(1)
Thus, the posterior cdf G(θ) is obtained via a calibration of the prior cdf F(θ).
Proof. Consider the right-hand side of Eq. (1)
g(1 −F(θ)) = g

Φ

−θ −μ0
α

= Φ

−λ1 · θ −μ0
α
+ λ

= 1 −Φ

θ −(μ0 + α λ
λ1 )
α
λ1

On the other hand, let
μ′
0 =
σ2
n
σ2
n + α2 · μ0 +
α2
σ2
n + α2 · ¯X
α′2 =
1
1
σ2/n + 1
α2
Then, by a direct calculation with the Bayes’ formula, the cdf of the posterior
is also a normal cdf with mean μ′
0 and standard deviation α′. Moreover, G(θ) is
exactly
G(θ) = Φ

θ −(μ0 + α λ
λ1 )
α
λ1

Therefore, 1 −G(θ) is equal to the right-hand side of Eq. (1) which establishes
the theorem.
⊓⊔
The next theorem shows that our new dual beta distortion function with appro-
priate parameters recovers the posterior distribution for the Bernoulli proportion
θ. As usual, x denotes the dataset collected and n denotes the size of x.

188
D. T. Nguyen et al.
Theorem 3. Let g(p), 0 ≤p ≤1 be the dual beta distortion function with four
parameters α, β, α′, β′
g(p) = 1 −I

I−1(1 −p, α, β), α′, β′
where I is the beta distribution function with 2 parameters α, β. Suppose the
prior distribution is F(θ) = I(θ; α, β)
Let
α′ = α + x1 + · · · + xn
β′ = β + n −(x1 + · · · + xn)
Then, the posterior distribution G(θ) for the proportion θ is exactly
I(θ; α′, β′). Moreover,
1 −G(θ) = g(1 −F(θ))
(2)
Therefore, the posterior cdf G(θ) is obtained via a calibration of the prior cdf
F(θ) by the dual beta distortion function g.
Proof. Consider the right-hand side
g(1 −F(θ)) = g [1 −I(θ; α, β)]
= 1 −I[I−1(I(θ; α, β); α, β); α′, β′]
= 1 −I[θ; α′, β′]
On the other hand, by a direct calculation with the Bayes’ formula, the poste-
rior distribution for the Bernoulli proportion θ is exactly I[θ; α′, β′]. Therefore,
1 −G(θ) is equal to the right-hand side of the Eq. (2)
⊓⊔
Our third result is for the Poisson rate parameter θ with gamma prior dis-
tribution. Similar to all previous results, we prove that the posterior cdf for θ
is obtained by calibrating the prior cdf using our new dual gamma distortion
function.
Theorem 4. Let g(p) be the dual gamma distortion function with four parame-
ters as follows
g(p) = 1 −Γ

Γ −1(1 −p; α, β); α1, β1

where Γ() is the gamma cumulative distribution function and Γ −1 is the corre-
sponding quantile function.
Suppose the prior cumulative distribution is F(θ) = Γ(θ; α, β)
Let
α1 = α + x1 + · · · + xn
β1 = β + n
Then, the posterior cumulative distribution G(θ) is exactly Γ(θ; α1, β1). More-
over,
1 −G(θ) = g(1 −F(θ))
(3)
Therefore, the posterior cdf G(θ) is obtained via a calibration of the prior cdf
F(θ) by the dual gamma distortion function g.

A Calibration-Based Method
189
Proof. Consider the right-hand side
g(1 −F(θ)) = g[1 −Γ(θ; α, β)]
= 1 −Γ[Γ −1(Γ(θ; α, β); α, β); α1, β1]
= 1 −Γ[θ; α1, β1]
On the other hand, by a direct application of the Bayes’ formula, the posterior
cumulative distribution G(θ) is exactly Γ(θ; α1, β1).
Therefore, 1 −G(θ) is equal to the right-hand side of the Eq. (3).
⊓⊔
5
Experimental Illustrations
We collected historical data of Vietnam stock, US stock, 5 popular foreign
exchange rate and Gold price from Thomson Reuters as much as possible. We
have 658 stocks in Vietnam, 53 stocks in US to experiment. See Table 1 for the
duration of the time series data.
Table 1. Duration
Quantity From
To
Vietnamese stock 685
First trading day Nov 16th 2015
Forex and Gold
6
Feb 6th, 1996
Nov 16th 2015
US stock
53
First trading day Nov 16th 2015
Table 2 shows the exact rates of the data we had: (1) Exact rates of direction
with Normal distribution and (2) Exact rates of conﬁdence intervals with Gamma
and Beta distributions.
Table 2. Rates
Exact rates (%) Normal
Gamma Beta
Bayesian Distortion
0 – 50
0.1%
0.3%
6.7%
4.69%
50 – 60
0.3%
0.3%
12.2%
7.91%
60 – 70
2.9%
2.0%
24.3%
21.23%
70 – 80
55.8%
50.3%
39.7%
44.22%
80 – 90
39.5%
46.6%
16.0%
21.23%
90 – 100
1.3%
0.6%
1.2%
0.73%
Both estimations using the traditional Bayesian updating formula and the
new distortion function formula show promising results in forecasting trends and

190
D. T. Nguyen et al.
predicting using credible intervals. In the case of normal distribution, about 95%
of data gives the prediction accuracy from 70% to 90%. Meanwhile, in the case
of beta and gamma distributions, more than 50% of data have the accuracy from
70% to 90% in credible interval prediction.
The results are good for forecasting risks in stock prices, hence, will be helpful
for investors in portfolio management. The new method of Bayesian statistics
in general, and distortion function in particular provides an alternative in time
series prediction.
6
Conclusion
In this paper, we provide a new point of view towards computing the posterior
distributions in bayesian statistics, namely via calibrations of the correspond-
ing prior distributions. In our opinion, this perspective reﬂects the essence of
bayesian inference where we start which some initial beliefs expressed through
the prior distribution on the parameter space; then, as data arrive, we adjust our
beliefs on our model parameters by mixing the information from the collected
data with the initial beliefs to form the posterior distribution. The adjustments
can be done by transforming the probability measure on the parameter space
using a suitable distortion function. In the paper, we provide a few distortion
functions to calibrate some popular prior distributions corresponding to highly-
used likelihoods like the normal, Bernoulli and Poisson functions.
In applications, we also implement and compare this method with the tra-
ditional method using likelihood integration. The data in the study are real
stock market data from the US and Vietnam; the predicting results are quite
promising.
A lot of further research can be done: from estimating diﬀerent parameters
(apart from the mean and proportion) to extending the ideas of calibration using
distortion to many other common distributions.
Regarding application, a number of questions in risk management and ﬁnance
can be addressed by the new approach.
References
1. Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.:
Bayesian Data Analysis, 3rd edn. CRC Press (2013)
2. Wang, S.S.: A class of distortion operators for pricing ﬁnancial and insurance risks.
J. Risk Insur. 67(1), 15–36 (2000)
3. Bolstad, W.M.: Introduction to Bayesian statistics. Wiley (2013)
4. Wang, S.S.: Premium calculation by transforming the layer premium density. In:
Casualty Actuaries Reinsurance Semmar in New York, 26–27 June 1995
5. Wirch J.L.: Coherent beta risk measures for capital requirements. Ph.D. thesis,
Ontario, Canada (1999)
6. Nguyen, H., Pham, U., Tran, H.: On some claims related to Choquet integral risk
measures. Ann. Oper. Res. 195, 5–31 (2012)

A Calibration-Based Method
191
7. Acerbi, C.: Spectral measures of risk: a coherent representation of subjective risk
aversion. J. Bank. Finance 26(7), 1505–1518 (2002)
8. Artzner, P., Delbaen, F., Elber, J., Heath, D.: Coherent measures of risk. Mathe.
Finance 9, 203–228 (1999)
9. Artzner, P., Delbaen, F., Eber, J.M., Heath, D.: Coherent multiperiod risk adjusted
values and Bellmans principle. Ann. Oper. Res. 152, 5–22 (2007)
10. Bertoin, J.: L´evy processes. Cambridge University Press, Cambridge (1996)
11. Black, F., Scholes, M.: The pricing of options and corporate liabilities. J. Polit.
Econ. 81, 637–654 (1973)

How to Estimate Statistical Characteristics
Based on a Sample: Nonparametric Maximum
Likelihood Approach Leads to Sample Mean,
Sample Variance, etc.
Vladik Kreinovich 1(B) and Thongchai Dumrongpokaphan2
1 Department of Computer Science, University of Texas at El Paso,
500 W. University, El Paso, TX 79968, USA
vladik@utep.edu
2 Department of Mathematics, Faculty of Science, Chiang Mai University,
Chiang Mai, Thailand
tcd43@hotmail.com
Abstract. In many practical situations, we need to estimate diﬀerent
statistical characteristics based on a sample. In some cases, we know that
the corresponding probability distribution belongs to a known ﬁnite-
parametric family of distributions. In such cases, a reasonable idea is
to use the Maximum Likelihood method to estimate the corresponding
parameters, and then to compute the value of the desired statistical
characteristic for the distribution with these parameters.
In some practical situations, we do not know any family containing the
unknown distribution. We show that in such nonparametric cases, the
Maximum Likelihood approach leads to the use of sample mean, sample
variance, etc.
1
Need to Estimate Statistical Characteristics Based
on a Sample: Formulation of the Problem
Need to estimate statistical characteristics. In many practical situations,
we need to estimate statistical characteristic of a certain random phenomenon
based on a given sample.
For example, to check that for all the mass-produced gadgets from a given
batch, the valued of the corresponding physical quantity are within the desired
bounds, the ideal solution would be to measure the quantity for all the gadgets.
This may be reasonable to do if these gadgets are intended for a spaceship, where
a minor fault can lead to catastrophic results. However, in most applications,
it is possible to save time and money by testing only a small sample, and by
making statistical conclusions based on the results of this testing.
How do we estimate the statistical characteristics – ﬁnite-parametric
case: main idea. In many situations, we know that the actual distribution
belongs to a known ﬁnite-parametric family of distributions. For example, it is
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_11

How to Estimate Statistical Characteristics Based on a Sample
193
often known that the distribution is Gaussian (normal), for some (unknown)
values of the mean μ and standard deviation σ. In general, we know that the
corresponding probability density function (pdf) has the form f(x | θ) for some
parameters θ = (θ1, . . . , θn).
In such situations, we ﬁrst estimate the values of the parameters θi based
on the sample, and then compute the values of the corresponding statistical
characteristic (mean, standard deviation, kurtosis, etc.) corresponding to the
estimates values θi.
How do we estimate the statistical characteristics – ﬁnite-parametric
case: details. How do we estimate the values of the parameters θi based on the
sample? A natural idea is to select the most probable values θ. How do we go
from this idea to an algorithm?
To answer this question, let us ﬁrst note that while theoretically, each of the
parameters θi can take inﬁnitely many values, in reality, for a given sample size,
it is impossible to detect the diﬀerence between the nearby values θi and θ′
i.
Thus, from the practical viewpoint, we have ﬁnitely many distinguishable cases.
In this description, we have ﬁnitely many possible combinations of parameters
θ(1), . . . , θ(N). We consider the case when all we know is that the actual pdf
belongs to the family f(x | θ). There is no a priori reason to consider some of the
possible values θ(k) as more probable. Thus, before we start our observations,
it is reasonable to consider these N hypotheses as equally probable: P0(θ(k)) =
1
N . This reasonable idea is known as the Laplace Indeterminacy Principle; see,
e.g., [1].
We can now use the Bayes theorem to compute the probabilities P(θ(k) | x)
of diﬀerent hypotheses θ(k) after we have performed the observations, and these
observations resulted in a sample x = (x1, . . . , xn):
P(θ(k) | x) =
P(x | θ(k)) · P0(θ(k))
N

i=1
P(x | θ(i)) · P0(θ(i))
.
Here, the probability P(x | θ(k)) is proportional to f(x | θ(k)). Dividing both
numerator and denominator by P0 = 1
N , we thus conclude that
P(θ(k) | x) = c · f(x | θ(k))
for some constant c.
Thus, selecting the most probable hypotheses P(θ(k) | x) →max
k
is equivalent
to ﬁnding the values θ for which, for the given sample x, the expression f(x | θ)
attains its largest possible value. The expression f(x | θ) is known as likelihood,
and the whole idea is known as the Maximum Likelihood Method; see, e.g., [2].
In particular, for Gaussian distribution, the Maximum Likelihood method
leads to the sample mean
μ
def
= 1
n ·
n

i=1
xi

194
V. Kreinovich and T. Dumrongpokaphan
as the estimate for the mean, and to the sample variance
(σ)2 def
= 1
n ·
n

i=1
(xi −μ)2
as the estimate for the variance.
What if we do not know the family? In some practical situations, we do
not know a ﬁnite-parametric family of distributions that contains the actual one.
In such situations, all we know is a sample. Based on this sample, how can we
estimate the statistical characteristics of the corresponding distribution?
What we do in this paper. In this paper, we apply the Maximum Likelihood
method to the above problem. It turns out that the resulting estimates are
sample mean, sample variance, etc.
Thus, we get a justiﬁcation for using these estimates beyond the case of the
Gaussian distribution.
2
Nonparametric Maximum Likelihood Approach
to Estimating Statistical Characteristics Based
on a Sample: Continuous Case
Description of the case. Let us ﬁrst consider the case when the random
variable is continuous, i.e., when, in principle, it can take either all possible real
values – or at least all possible real values from a certain interval.
Possibility to discretize. Similar to the above case, while theoretically, we can
thus have inﬁnitely many possible values of the random variable x, in reality, due
to measurement uncertainty, very close values x ≈x′ are indistinguishable. Thus,
in practice, we can safely assume that there are only ﬁnitely many distinguishable
values x(1) < x(2) < . . . < x(M).
Possible probability distributions. In the discretized representations, to
describe the corresponding random variable, we need to describe the proba-
bilities pi = p(x(i)) of each of M values x(i), 1 ≤i ≤M. The only restriction
on these probabilities is that they should be non-negative and add up to 1:
M

i=1
pi = 1.
Let us apply the Maximum Likelihood Method: resulting formulation.
According to the Maximum Likelihood Method, out of all possible probability
distributions p = (p1, . . . , pn), we should select a one for which the probability
of observing a given sequence x1, . . . , xn is the largest.
The probability of observing each value xi is equal to p(xi). It is usually
assumed that diﬀerent elements in the sample are independent, so the probability
p(x | p) of observing the whole sample x = (x1, . . . , xn) is equal to the product
of these probabilities:
p(x | p) =
n

i=1
p(xi).

How to Estimate Statistical Characteristics Based on a Sample
195
In the continuous case, the probability of observing the exact same number
twice is zero, so we can safely assume that all the values xi are diﬀerent. In this
case, the above product takes the form
p(x | p) =

{xi : xi has been observed}.
We need to ﬁnd the values p1, . . . , pM that maximize this probability under the
constraints that pi ≥0 and
M

i=1
pi = 1.
Analysis of the problem. Let us explicitly describe the probability distribu-
tion that maximizes the corresponding likelihood.
First, let us notice that when the maximum is attained, the values pi cor-
responding to un-observed values should be 0. Indeed, if pi > 0 for one of the
indices i corresponding to an un-observed value xi, then we can, without chang-
ing the constraint
M

i=1
pi = 1, decrease this value to 0 and instead increase one
of the probabilities pi corresponding to an observed value xi.
Let I denote the set of all indices corresponding to observed values pi. Then,
in the optimal arrangement, we have pi = 0 for i ̸∈I. So, the constraint
M

i=1
pi = 1
takes the form 
i∈I
pi = 1, and the likelihood optimization problem takes the
following form: 
i∈I
pi →max under the constraint that 
i∈I
pi = 1.
This is a known and easy-to-solve optimization problem. The corresponding
maximum is attained when all the probabilities pi are equal to each other, i.e.,
when pi = 1
n. Thus, we arrive at the following conclusion.
Conclusion: we should use sample mean, sample variance, etc. In the
non-parametric case, the maximum likelihood method implies that out of all
possible probability distributions, we should select a distribution in which all
sample values x1, . . . , xn appear with equal probability pi = 1
n, and no other
values can appear.
So, as estimates of the desired statistical characteristics, we should select
characteristics corresponding to this sample-based distribution. The mean of this
distribution is equal to μ = 1
n ·
n

i=1
xi, i.e., to the sample mean. The variance of
this distribution is equal to 1
n ·
n

i=1
(xi −μ)2, i.e., to the sample variance.
Thus, for the nonparametric case, the maximum likelihood method implies
that we should use sample mean, sample variance, etc.
Discussion. Thus, we get a justiﬁcation for using sample mean, sample variance,
etc., in situations beyond their usual Gaussian-based justiﬁcation.

196
V. Kreinovich and T. Dumrongpokaphan
3
Nonparametric Maximum Likelihood Approach
to Estimating Statistical Characteristics Based
on a Sample: Discrete Case
Description of the case. In the discrete case, we have a ﬁnite list of possible
values x(1), . . . , x(M). To describe a probability distribution, we need to describe
the probabilities pi = p(x(i)) of these values.
Maximum Likelihood Approach: formulation of the optimization
problem. For each sample x1, . . . , xn, the corresponding likelihood
n
i=1
p(xi)
takes the form
p(x | p) =
M

i=1
pni
i ,
where ni is the number of times the value x(i) appears in the sample.
We must ﬁnd the probabilities pi for which the likelihood attains its largest
possible value under the constraint
n
i=1
pi = 1.
Optimizing the likelihood. To solve the above constraint optimization prob-
lem, we can use the Lagrange multiplier method that reduces it to the uncon-
strained optimization problem
M

i=1
pni
i
+ λ ·
 M

i=1
pi −1

→max
p
.
Diﬀerentiating this objective function with respect to pi, taking into account
that for A
def
=
M

i=1
pni
i , we get
∂A
∂pi
=

j̸=i
pnj
j
· ni · pni−1
i
= A · ni
pi
,
and equating the derivative to 0, we conclude that
A · ni
pi
+ λ = 0.
Thus, pi = const · ni. The constraint that
M

i=1
pi = 1 implies that the constant is
equal to 1 over the sum
n
i=1
ni = n. Thus, we get pi = ni
n . So, we arrive at the
following conclusion.
Conclusion. In the discrete case, for each of the possible values x(i), we assign,
as the probability pi, the frequency ni
n with which this value appears in the
observed sample.

How to Estimate Statistical Characteristics Based on a Sample
197
This is the probability distribution that we should use to estimate diﬀerent
statistical characteristics. For this distribution, the mean is still equal to the
sample mean, and the variance is still equal to the sample variance – same as
for the continuous case.
However, e.g., for entropy, we get a value which is diﬀerent from the contin-
uous case: there, the entropy is always equal to
−

i∈I
pi · ln(pi) = −n · 1
n · ln
	 1
n

= ln(n),
while in the discrete case, we have a diﬀerent value
−

i∈I
pi · ln(pi) = −
M

i=1
ni
n · ln
ni
n

.
Acknowledgments. This work was supported by Chiang Mai University, Thailand.
This work was also supported in part by the National Science Foundation grants HRD-
0734825 and HRD-1242122 (Cyber-ShARE Center of Excellence) and DUE-0926721,
and by an award “UTEP and Prudential Actuarial Science Academy and Pipeline
Initiative” from Prudential Foundation.
References
1. Jaynes, E.T., Bretthorst, G.L.: Probability Theory: The Logic of Science. Cambridge
University Press, Cambridge, UK (2003)
2. Sheskin, D.J.: Handbook of Parametric and Nonparametric Statistical Procedures.
Chapman and Hall/CRC, Boca Raton, Florida (2011)

How to Gauge Accuracy of Processing Big Data:
Teaching Machine Learning Techniques
to Gauge Their Own Accuracy
Vladik Kreinovich1(B), Thongchai Dumrongpokaphan2, Hung T. Nguyen3,4,
and Olga Kosheleva1
1 Department of Computer Science, University of Texas at El Paso,
500 W. University, El Paso, TX 79968, USA
{vladik,olgak}@utep.edu
2 Department of Mathematics, Faculty of Science, Chiang Mai University,
Chiang Mai, Thailand
tcd43@hotmail.com
3 Department of Mathematical Sciences, New Mexico State University,
Las Cruces, NM 88003, USA
hunguyen@nmsu.edu
4 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
Abstract. When the amount of data is reasonably small, we can usu-
ally ﬁt this data to a simple model and use the traditional statistical
methods both to estimate the parameters of this model and to gauge
this model’s accuracy. For big data, it is often no longer possible to ﬁt
them by a simple model. Thus, we need to use generic machine learning
techniques to ﬁnd the corresponding model. The current machine learn-
ing techniques estimate the values of the corresponding parameters, but
they usually do not gauge the accuracy of the corresponding general non-
linear model. In this paper, we show how to modify the existing machine
learning methodology so that it will not only estimate the parameters,
but also estimate the accuracy of the resulting model.
1
Need to Gauge Accuracy of Big Data Processing
Need for data processing. In many practical situations, we are interested in
the value of a quantity y which is diﬃcult – or even impossible – to measure
directly. For example, we may be interested:
• in tomorrow’s temperature y, or
• in the distance y to a faraway star.
Since we cannot measure this quantity y directly, we have to measure it
indirectly; namely:
• we measure easier-to-measure quantities x1, . . . , xn whose values determine
y, i.e., for which y = f(x1, . . . , xn) for some function f(x1, . . . , xn), and then
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_12

How to Gauge Accuracy of Processing Big Data
199
• we use the results xi of measuring xi and the known dependence f(x1, . . . , xn)
to estimate y as y = f(x1, . . . , xn).
The corresponding computations are known as data processing.
Need to ﬁnd the corresponding dependence. To be able to perform data
processing, we need to know the dependence y = f(x1, . . . , xn) between the
corresponding quantities xi and y.
• In some cases, this dependence can be determined based on the fundamental
physical principles.
• However, in many other cases, this dependence needs to be determined exper-
imentally, based on the known observation results.
Traditional approach to ﬁnding a dependence: a brief reminder. The
traditional statistical approach to ﬁnding the desired dependence has been
designed for situations in which the number of available observations is rea-
sonably small; see, e.g., [4]. In such situations, we can usually ﬁt the data with
some simple model – e.g., with the linear regression model in which the depen-
dence is linear: f(x1, . . . , xn) = a0 +
n
i=1
ai · xi. For such models, the traditional
statistical approach provides both:
• the estimates for the values of the corresponding parameters, and
• a good description of the accuracy of the corresponding estimated model
f(x1, . . . , xn), i.e., of the probability distribution of the approximation error
Δy
def
= y −f(x1, . . . , xn).
The emergence of big data. In the last decades, the progress in computer
and computer-based measurement technologies enabled us to get huge amounts
of data, amounts far exceeding the sample sizes for which we can apply the
traditional statistical techniques.
This phenomenon is known as big data; see, e.g., [3].
For big data, simple models are rarely possible. Real-life phenomena are
usually very complex. When we have a reasonable small amount of data, we can
still have simple approximate models that describe this data well. However, as
we increase the amount of data, we can no longer use simple models.
This phenomenon is easy to explain. In general in statistics, based on a
sample of size n, we can determine each parameter of the model with accuracy
∼1/√n.
• When the sample size n is reasonably small, the resulting inaccuracy usually
exceeds the size of the quadratic and higher order terms in the actual depen-
dence. So, within this accuracy, we can safely assume that the dependence is
linear.
• However, as the sample size increases, the accuracy 1/√n becomes smaller
than the quadratic and higher order terms – and thus, these terms can no
longer be ignored if we want to have a model that ﬁts all the data.

200
V. Kreinovich et al.
This phenomenon is well known. For example:
• When a comet appears and we have only few of its observations, we can safely
use simpliﬁed Newton’s equations – that assumes that only the Sun and the
Jupiter have to be taken into account – and get a good description of all the
observed data.
• However, as the number of measurements increases, we have to take into
account the gravitational inﬂuence of other planets to get a good ﬁt with
observations.
Machine learning techniques: the big-data analogs of the traditional
statistical data processing. For large data sizes, we cannot use simple few-
parametric models, we need complex models with large number of parameters.
In most situations, it is not possible to guess the exact form of the corresponding
non-linear dependence. So, we need to use techniques that do not assume any
speciﬁc form, but try to extract the form of the dependence from the data itself.
Such techniques are known as machine learning; see, e.g., [1,2]. For most of
machine learning approaches such as neural networks, there are universal approx-
imation results that show that every possible non-linear function can be, with
any given accuracy, approximated by the corresponding computational model.
Machine learning techniques: successes and limitations. Machine learn-
ing techniques have been very successful in many applications [1,2]. In many
practical applications, they use the observations to come up with a dependence
f(x1, . . . , xn) that provides a good ﬁt for the observed data.
However, what is lacking in most machine learning techniques is a good
understanding of how accurate is this model, i.e., what are the probabilities of
diﬀerent values of the approximation error Δy = y −f(x1, . . . , xn).
To be more precise, we can estimate overall characteristics of such an accu-
racy: e.g., we can take all the values of the approximation error corresponding
to all the input observations (x1, . . . , xn, y) and ﬁnd the standard deviation and
the whole distribution. But this will be the overall distribution.
We know that in many cases, the approximation accuracy depends on the
inputs x1, . . . , xn:
• for some inputs, the model f(x1, . . . , xn) provides a more accurate approxi-
mation, while
• for other inputs, the model provides less accurate approximation.
Traditional statistical methods enable us to ﬁnd the distribution of measurement
errors corresponding to each individual tuple x = (x1, . . . , xn).
It is desirable to have something similar for machine learning techniques as
well.
What we do in this paper. In this paper, we show how we can teach the
existing machine learning techniques
• to not only the approximate dependence model, but
• also to automatically gauge the accuracy of the resulting model.

How to Gauge Accuracy of Processing Big Data
201
2
Our Main Idea
Analysis of the problem. We would like to be able, for each possible tuple
x = (x1, . . . , xn), to generate not only a single numerical estimate for the corre-
sponding value y, but also the whole conditional probability distribution describ-
ing possible values y corresponding to this tuple.
From the computational viewpoint, a natural way to simulate a probabil-
ity distribution with a given cumulative distribution function (cdf) F(Y ) =
Prob(y ≤Y ) is:
• to start with the standard random number r which is uniformly distributed on
the interval [0,1] – and whose generation is supported by most programming
languages and programming environments – and
• apply the inverse function F −1(p) to this random number.
The resulting values y = F −1(r) are indeed distributed according to the given
probability distribution F(Y ).
The inverse function F −1(p) has a direct probabilistic meaning; namely,
• for each p ∈[0, 1],
• the value x = F −1(p) is the p-th quantile, i.e., the value x for which F(x) = p.
For p = 0.5, we get the median, for p = 0.25 and p = 0.75, we get the lower and
upper quartiles, etc.
From this viewpoint, what we want is to be able,
• for every possible tuple x = (x1, . . . , xn) and
• for every possible value p,
to come up with the p-th quantile of the conditional y-distribution corresponding
to this tuple. In precise terms, for the function Gx(y)
def
= F(y | x1, . . . , xn), we
want to be able to generate the quantile q = G−1
x (p) for which
Prob(y ≤q | x1, . . . , xn) = p.
Let us denote this quantile q by G(x1, . . . , xn, p).
Thus, we want to generate a function G(x1, . . . , xn, p) of n+1 variables. Once
we have this function, we will be able to ﬁnd, for each tuple x = (x1, . . . , xn), the
cdf corresponding y-distribution Fx(y) – as the inverse function Fx(p) = q−1
x (p)
to the function qx(p)
def
= Q(x1, . . . , xn, p).
Historical comment. The idea of combining stochastic processes with neural net-
works was originally proposed and actively promoted by Paul Werbos; see, e.g.,
[6,7] (see also [5]).
How can we use machine learning to ﬁnd the desired function
Q(x1, . . . , xn, p)? In the ideal world, for each tuple x = (x1, . . . , xn), we would
have several diﬀerent observations in which we have

202
V. Kreinovich et al.
• these same values of xi and
• diﬀerent values of y.
In this case, from this sample of diﬀerent values of y corresponding to the given
tuple x, we will be able to determine the conditional probability distribution
corresponding to this tuple x.
Speciﬁcally, once we have N such y-values, we can sort them into an increas-
ing sequence y(1) ≤. . . ≤y(N). Crudely speaking, these values correspond to the
quantiles p = 1/N, p = 2/N, etc. We therefore expect each predicted quantile
Q(x1, . . . , xn, j/N) to be close to the corresponding value y(j).
In practice, we do not have such “same-x” observations: diﬀerent observa-
tions correspond, in general, to diﬀerent tuples x. Since we cannot have diﬀerent
observations corresponding to the exact same tuple x, a natural idea is to use
observations corresponding to nearby tuples x.
Good news is that when we have big data, i.e., a very large amount of data,
it is highly probable that a few of the observations will be close to x – even when
the probability of being close to x is small.
Once we pick N such nearby tuples, we can sort the corresponding N y-values
into an increasing sequence y(1) ≤. . . ≤y(N). Crudely speaking, these values
correspond to the quantiles p = 1/N, p = 2/N, etc. In other words, each of these
values y(j) correspond to p = j/N. We therefore expect each predicted quantile
Q(x1, . . . , xn, j/N) to be close to the corresponding value y(j).
Thus, we arrive at the following algorithm for reconstructing the desired
function Q(x1, . . . , xn, p).
Resultingalgorithm.Westartwiththelistofobservations

x(k)
1 , . . . , x(k)
n , y(k)
.
Based on this list, we will form the extended tuples

x(k)
1 , . . . , x(k)
n , pj, y(k)
j

as
follows:
1◦. We ﬁx some number N – e.g., N = 10.
2◦. Then, for each original observation

x(k)
1 , . . . , x(k)
n , y(k)
, we do the following:
2.1◦We ﬁnd N −1 observations

x(ℓ)
1 , . . . , x(ℓ)
n , y(ℓ)
in which the x-tuple
x(ℓ) =

x(ℓ)
1 , . . . , x(ℓ)
n

is the closest to the original x-tuple
x(k) =

x(k)
1 , . . . , x(k)
n

.
2.2◦In the resulting 1+(N −1) = N observations, we have N diﬀerent y-values:
• the original y-value y(k) and
• N −1 values y(ℓ) corresponding to N −1 “nearest” observations.
Let us sort them into an increasing sequence
y(k)
(1) ≤y(k)
(2) ≤. . . ≤y(k)
(N).

How to Gauge Accuracy of Processing Big Data
203
2.3◦Then, we form N extended tuples

x(k)
1 , . . . , x(k)
n , j
N , y(k)
(j)

.
3◦. After that, we combine all the extended tuples corresponding to diﬀerent
original tuples into a single list.
4◦. To the resulting single list, we apply a machine learning algorithm and thus,
construct the desired function Q(x1, . . . , xn, p) that provides,
• for each tuple x = (x1, . . . , xn) and
• for each value p ∈[0, 1],
the p-th quantile of the y-distribution corresponding to this tuple x.
Which value N should we use: a comment
• The larger N, the more detailed is the resulting information about the prob-
ability distributions.
• On the other hand, if we take N to be too large, then some of the “closest”
tuples x(ℓ) may be too far away from the original tuple x(k) and thus, this
description will not be very accurate.
So, here, we have a usual trade-oﬀbetween accuracy and details – similar to
what we have, e.g., when we divide the real line into bins to build a histogram
approximating the actual probability density function ρ(x):
• if we use smaller bins, we get more details, but
• these details are at the expense of accuracy of approximating ρ(x): the smaller
bins, the fewer observations are in each bin, and thus, the less accurately the
corresponding frequencies represent the desired probabilities.
The actual value N should be determine empirically – or, if we have some
prior information, by using this prior information. As a rule of thumb, we suggest
using N = 10, since in our experience, this is where we get the largest number
of reliable details.
Acknowledgments. This work was supported by Chiang Mai University, Thailand.
This work was also supported in part by the National Science Foundation grants HRD-
0734825 and HRD-1242122 (Cyber-ShARE Center of Excellence) and DUE-0926721, and
by an award “UTEP and Prudential Actuarial Science Academy and Pipeline Initiative”
from Prudential Foundation.
One of the authors (VK) is thankful to Paul Werbos for inspiring talks and discussions.

204
V. Kreinovich et al.
References
1. Bishop, C.M.: Pattern Recognition and Machine Learning. Springer, New York
(2006)
2. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press, Cambridge
(2016)
3. Mayer-Sch¨onberger, V., Cukier, K.: Big Data: The Essential Guide to Work, Life,
and Learning in the Age of Insight. John Murray Publishers, London (2017)
4. Sheskin, D.J.: Handbook of Parametric and Nonparametric Statistical Procedures.
Chapman & Hall/CRC, Boca Raton, Florida (2011)
5. Turchetti, C.: Stochastic Models of Neural Networks. IOS Press, Amsterdam (2004)
6. Werbos, P.: A brain-like design to learn optimal decision strategies in complex envi-
ronments. In: Karny, M., Warwick, K., Kurkova, V. (eds.) Dealing with Complexity:
A Neural Networks Approach. Springer, London (1998)
7. Werbos, P.: Intelligence in the brain: a theory of how it works and how to build it.
Neural Netw. 22(3), 200–212 (2009)

How Better Are Predictive Models:
Analysis on the Practically Important Example
of Robust Interval Uncertainty
Vladik Kreinovich1(B), Hung T. Nguyen2,3, Songsak Sriboonchitta3,
and Olga Kosheleva1
1 Department of Computer Science, University of Texas at El Paso,
500 W. University, El Paso, TX 79968, USA
{vladik,olgak}@utep.edu
2 Department of Mathematical Sciences, New Mexico State University,
Las Cruces, NM 88003, USA
hunguyen@nmsu.edu
3 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
songsakecon@gmail.com
Abstract. One of the main applications of science and engineering is
to predict future value of diﬀerent quantities of interest. In the tradi-
tional statistical approach, we ﬁrst use observations to estimate the para-
meters of an appropriate model, and then use the resulting estimates
to make predictions. Recently, a relatively new predictive approach has
been actively promoted, the approach where we make predictions directly
from observations. It is known that in general, while the predictive app-
roach requires more computations, it leads to more accurate predictions.
In this paper, on the practically important example of robust interval
uncertainty, we analyze how more accurate is the predictive approach.
Our analysis shows that predictive models are indeed much more accu-
rate: asymptotically, they lead to estimates which are √n more accurate,
where n is the number of estimated parameters.
1
Formulation of the Problem
Predictions Are Important. One of the main applications of science and
engineering is to predict what will happen in the future:
• In science, we are most interesting in predicting what will happen “by itself”
– e.g., where the Moon will be a year from now.
• In engineering, we are more interested in what will happen if we apply a
certain control strategy – e.g., where a spaceship will be if we apply a certain
trajectory correction.
In both science and engineering, prediction is one of the main objectives.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_13

206
V. Kreinovich et al.
Traditional Statistics Approach to Prediction: Estimate then Predict.
The traditional statistical approach to prediction problems (see, e.g., [6]) is as
follows:
• First, we ﬁx a statistical model with unknown parameters. For example,
we can assume that the dependence of some quantity y on the quantities
x1, . . . , xn is described by a linear dependence y = a0 +
n
i=1
ai · xi + ε, where ε
is normally distributed with 0 mean and some standard deviation σ. In this
case, the parameters are a0, a1, . . . , an, and σ.
• Then, we use the observations to conﬁrm this model and estimate the values
of these parameters.
• After that, we use the model with the estimated values of the parameters to
make the corresponding predictions.
Traditional Statistical Approach to Prediction: Advantages and Lim-
itations. In the traditional approach, when we perform estimations, we do not
take into account what exactly characteristic we plan to predict. In the above
example, the same estimates for the parameters ai and σ are used, whether we
are trying to predict the future value of the quantity y or whether we are try-
ing to predict a diﬀerent quantity z that depends on y and on several other
quantities.
A natural advantage of this approach is that a computationally intensive
parameter estimation part is performed only once, and the resulting estimates
can then be used to solve many diﬀerent prediction problems. In the past, when
computations were much slower than now, this was a big advantage: by using pre-
computer estimates for the values of the corresponding parameters, we can per-
form many diﬀerent predictions fast, without the need to re-do time-consuming
parameter estimation part.
With this advantages, come a potential limitation: hopefully, by tailoring
parameter estimation to a speciﬁc prediction problem, we may able to make
more accurate predictions.
Predictive Approach. In the past, because of the computer limitations, we
had to save on computations, and thus, the traditional approach was, in most
cases, all we could aﬀord. However, now computers have become much faster. As
a result, in many practical situations, it has become possible to perform intensive
computations in a short period of time.
As a result, taking into account the above disadvantage of the traditional
approach, many researchers now advocate to use predictive approach to statistics,
in which we directly solve the prediction problem – i.e., in other words, on
the intermediate step of estimating the parameters, we take into account what
exactly quantities we need to predict; see, e.g., [1–3].
What We Do in This Paper. There are many examples of successful use of
the predictive approach. However, most of these examples remain anecdotal.

How Better Are Predictive Models: Case of Robust Interval Uncertainty
207
In this paper, on a practically important simple example of robust interval
uncertainty, we prove a general result showing that predictive models indeed
lead to more accurate predictions. Moreover, we provide a numerical measure of
accuracy improvement.
2
Robust Interval Uncertainty: A Brief Reminder
Measurement Uncertainty. Data processing starts with values that come
from measurement or from an expert estimate. Expert estimates are often impor-
tant, but of course, a measuring instrument provides much more data than an
expert. As a result, the overwhelming majority of data values come from mea-
surements.
With the exception of simplest cases like counting number of people in a
small group, measurement are not 100% accurate: the measurement result x
is, in general, diﬀerent from the actual (unknown) value of the corresponding
quantity. In other words, in general, we have a non-zero measurement error
Δx
def
= x −x.
What do We Know About Measurement Uncertainty: Case When
We Know the Probability Distribution and Case of Robust Interval
Uncertainty. In some situations, we know the probability distribution of the
measurement error; for example, in many practical cases, we know that the
measurement error is normally distributed, with 0 mean and known standard
deviation σ.
However, in many practical situations, the only information that we have about
the measurement error Δx is the upper bound Δ on its absolute value – the bound
provided by the manufacturer of the measuring instrument; see, e.g., [5].
In other words, we only know that the probability distribution of the mea-
surement error Δx is located on the interval [−Δ, Δ], but we do not have any
other information about the probability distribution. Such interval uncertainty
is a particular case of the general robust statistics; see, e.g., [4].
Why cannot we always get this additional information? To get information
about Δx = x−x, we need to have information about the actual value x. In many
practical situations, this is possible. Namely, in addition to the current measuring
instrument (MI), we often also have a much more accurate (“standard”) MI, so
much more accurate that the corresponding measurement error can be safely
ignored in comparison with the measurement error of our MI, and thus, the
results of using the standard MI can be taken as the actual values.
In such a situation, we can ﬁnd the probability distribution for the measure-
ment error Δx if, for each of several quantities, we measure this quantity both
by using the current MI and by using the standard MI. The diﬀerence between
the two measurement results is a good approximation to the corresponding mea-
surement error. Thus, the collection of such diﬀerences is a sample from the
desired probability distribution for Δx. Based on this sample, we can ﬁnd the
corresponding probability distribution.

208
V. Kreinovich et al.
In many situations, however, our MI is already state-of-the-art, no more-
accurate standard MI is possible. For example, in fundamental science, when we
perform state-of-the-art measurements, we use state-of-the-art measuring instru-
ments. For a billion-dollar project like space telescope or particle super-collider,
the best MI are used. In this case, it is not possible to apply the above technique,
so the best we can do is to use the bound Δ on the measurement error.
Another frequent case when we have to use Δ is the case of routine manufac-
turing. In this case, theoretically, we can calibrate every sensor, but sensors are
cheap and calibrating them costs a lot – since it means using expensive state-
of-the-art standard MIs. In routine manufacturing, such a calibration is just not
ﬁnancially possible – and not needed. For example, a simple thermometer for
measuring a body temperature is reasonable cheap. If we had to calibrate each
thermometer, it would become an order of magnitude more expensive – and
what is the purpose? Honestly, all we need to know is whether a patient has a
fever and, if yes, how severe, but the diﬀerence between, say 38.1 and 38.2 will
not result in any changes in medical diagnosis or treatment.
Robust Interval Uncertainty Is What We Consider in This Paper. In
view of the practical importance, in this paper, we consider the case of robust
interval uncertainty.
3
Comparing Predictive and Traditional Statistics
on the Example of Robust Interval Uncertainty:
Analysis of the Problem
Let Us Describe the Traditional Approach in Precise Terms. Let y
denote the quantity that we would like to predict.
To predict a quantity, we need to know the relation between this future
quantity y and certain “estimate-able” quantities x1, . . . , xn. Then, to predict y,
we:
• estimate the quantities x1, . . . , xn based on the measurement results, and then
• use these estimates and the known relation between y and xi to predict the
desired future value y.
Let us denote the corresponding relation between y and xi by y
=
f(x1, . . . , xn). Let us denote the measurement-based estimates for the quantities
xi by xi. In these terms, after generating these estimates, we get the following
prediction for y:
y
def
= f(x1, . . . , xn).
The quantities xi are estimated based on measurement results. Let v1, . . . , vN
denote all the quantities whose measurement results are used to estimate the
quantities xi. This estimation is based on the known relation between xi and vj.

How Better Are Predictive Models: Case of Robust Interval Uncertainty
209
Let us denote this relation by xi = gi(v1, . . . , vN), and let us denote the result
of measuring each quantity vj by vj. In these terms, the process of computing
estimates xi for the quantities xi consists of the following two steps:
• ﬁrst, we measure the quantities v1, . . . , vN;
• then, the results v1, . . . , vN of measuring these quantities are used to produce
the estimates xi = gi(v1, . . . , vN).
Overall, the traditional approach takes the following form:
• ﬁrst, we measure the quantities v1, . . . , vN;
• then, the results v1, . . . , vN of measuring these quantities are used to produce
the estimates xi = gi(v1, . . . , vN);
• ﬁnally, we use the estimates xi to compute the corresponding prediction
y = f(x1, . . . , xn).
How Will Predictive Approach Look in These Terms. The predictive
approach means that, instead of ﬁrst estimating the parameters xi and then using
these parameters to predict y, we predict y based directly on the measurement
results vj.
To make such a prediction, we need to know the relation between the
predicted quantity y and the measurement results. Since we know that y =
f(x1, . . . , xn) and that xi
=
gi(v1, . . . , vN), we thus conclude that y
=
F(v1, . . . , vN), where we denoted
F(v1, . . . , vN)
def
= f(g1(v1, . . . , vN), . . . , gn(v1, . . . , vN)).
In these terms, the predictive approach to statistics takes the following form:
• ﬁrst, we measure the quantities v1, . . . , vN;
• then, the results v1, . . . , xN of measuring these quantities are used to produce
the prediction y = F(v1, . . . , vN).
How Accurate are These Estimates and Predictions? We are interested
in the accuracy of the corresponding estimates and predictions.
For each estimated quantity xi, the estimation error Δxi is naturally deﬁned
as the diﬀerence xi −xi between the estimate xi = gi(v1, . . . , vN) and the actual
value xi = gi(v1, . . . , vN), i.e., the value that we would have got if we knew
the exact values vj of the measured quantities vj. Similarly, for the prediction,
the prediction error Δxi is naturally deﬁned as the diﬀerence y −y between
the estimate y = f(x1, . . . , xn) and the actual value y = gi(x1, . . . , xn), i.e., the
value that we would have got if we knew the exact values xi of the estimated
quantities xi.
Measurements are usually reasonably accurate, which means that the mea-
surement errors Δvj are reasonably small. So, we can substitute the formula
vj = vj −Δvj, expand the resulting expression for
Δxi = gi(v1, . . . , vN) −gi(v1, . . . , vN)
= gi(v1, . . . , vN) −gi(v1 −Δv1, . . . , vN −ΔvN)

210
V. Kreinovich et al.
in Taylor series, and keep only linear terms in this expansion. As a result, we
get the following formula:
Δxi =
N

i=1
gij · Δvj,
where we denoted gij
def
= ∂gi
∂vj
.
What can we conclude about the value Δxi? The only thing we know about
each of the measurement errors Δvj is that this measurement error can take any
value from the interval [−Δj, Δj]. The above sum attains its largest possible
value when each of the terms attains its largest value.
• when gij ≥0, the term gij · Δvj is an increasing function of Δvj, so its
maximum is attained when Δvj attains its largest possible value Δvj = Δj;
the resulting largest value of this term is gij · Δj;
• when gij < 0, the term gij · Δvj is a decreasing function of Δvj, so its
maximum is attained when Δvj attains its smallest possible value Δvj =
−Δj; the resulting largest value of this term is −gij · Δj.
In both cases, the largest possible value of the term is equal to |gij| · Δj. Thus,
the largest possible value Δx
i of Δxi is equal to
Δx
i =
N

j=1
|gij| · Δj.
(1)
One can easily check that the smallest possible value of Δxi is equal to −Δx
i .
Thus, possible values of Δxi form an interval [−Δx
i , Δx
i ].
Similarly, based on the estimates xi and bounds Δx
i on the estimation errors,
we can conclude that the possible values of the prediction error lie in the interval
[−Δ, Δ], where
Δ =
n

i=1
|fi| · Δx
i ,
(2)
and we denoted fi
def
= ∂f
∂xi
.
Alternatively, if we use the function F(v1, . . . , vN) to directly predict the
value y from the measurement results, we conclude that the possible value of the
prediction error lie in the interval [−δ, δ], where
δ =
N

j=1
|Fj| · Δj,
(3)
and we denoted Fj
def
= ∂F
∂vj
.

How Better Are Predictive Models: Case of Robust Interval Uncertainty
211
Preliminary Conclusion. Depending on whether we consider the traditional
statistical approach or the predictive approach, we get the same estimate y for
the predicted quantity y. However, for the accuracy Δy, we have, in general,
diﬀerent bounds:
• if we use the traditional approach, then we get the bound Δ as described by
the formulas (1) and (2);
• alternatively, if we use the predictive approach, we get the bound δ as
described by the formula (3).
Comparing the Two Bounds. One can see that δ is the actual bound: in
principle, the value δ can be attained if we take appropriate values of Δvj =
Δj · sign(Fj).
Since all possible values of Δy also lie in the interval [−Δ, Δ], the value δ
also lies in this interval, thus the estimate δ coming from the predictive approach
is smaller than or equal to the traditional estimate Δ. But is it better? Let us
compare the two expressions.
If we substitute the expression (1) into the formula (2), we conclude that
Δ =
n

i=1
⎛
⎝|fi| ·
⎛
⎝
N

j=1
|gij| · Δj
⎞
⎠
⎞
⎠,
i.e., equivalently,
Δ =
n

j=1
Cj,
(4)
where we denoted
Cj
def
=
n

i=1
|fi| · |gij| · Δj.
Since |a · b| = |a| · |b| and Δj > 0, we can thus conclude that
Cj =
N

i=1
|cij|,
(5)
where we denoted cij
def
= fi · gij · Δj.
On the other hand, the formula (3) takes the form
δ =
n

j=1
cj,
(6)
where cj
def
= |Fj|·Δj. By using the chain rule, we conclude that the derivative Fj
of the composition function F(v1, . . . , vN) takes the form Fj =
n
i=1
fi · gij. Thus,
the coeﬃcient cj in the formula (6) has the form
cj =
					
n

i=1
fi · gij
					 · Δj,

212
V. Kreinovich et al.
i.e., equivalently,
cj =
					
N

i=1
cij
					 .
(7)
By comparing formulas (4)–(5) with formulas (6)–(7), we can see that indeed
δ ≤Δ: indeed, since |a + b| ≤|a| + |b|, we have
cj =
					
n

i=1
cij
					 ≤
n

i=1
|cij| = Cj
and thus, indeed
δ =
N

j=1
cj ≤
N

j=1
Cj = Δ.
Is δ smaller? If yes, how smaller? To answer these equations, let us take into
account that, in principle, each term cij = fi · gij · Δj can take any real value,
positive and negative. A priori, we do not have any reason to believe that positive
values will be more frequent than negative ones, so it is reasonable to assume
that the mean value of each such term is 0. Again, there is no reason to assume
that the values cij are diﬀerent, so it makes sense to assume that all these
values are identically distributed. Finally, there is no reason to believe that
there is correlation between diﬀerent values, so its makes to consider them to be
independent.
Under these assumptions, for large n, the sum
n
i=1
cij is normally distributed,
with 0 mean and variance which is n times larger than the variance σ2 of the
original distribution of cij. Thus, the means value of the absolute value cj of this
sum is proportional to its standard deviation σ · √n.
On the other hand, the expected value μ of each term |cij| is positive, thus,
the expected value of the sum Cj =
n
i=1
|cij| of n such independent terms is equal
to μ · n.
For large n, μ · n ≫σ · √n. Thus, we arrive at the following conclusion.
4
Conclusion
In this paper, we compare:
• the traditional statistical approach, in which we ﬁrst use the observations
to estimate the values of the parameters and then use these estimates for
prediction, and
• the predictive approach to statistics, in which we make predictions directly
from observations.

How Better Are Predictive Models: Case of Robust Interval Uncertainty
213
We make this comparison on the example of the practically important case of
robust interval uncertainty, when the only information that we have about the
corresponding measurement error is the upper bound provided by the manufac-
turer of the corresponding measurement instrument.
It turns out that while predictive techniques require more computations, they
result in much more accurate estimates: asymptotically, √n times more accurate,
where n is the total number of parameters estimated in the traditional approach.
Acknowledgments. We acknowledge the partial support of the Center of Excellence
in Econometrics, Faculty of Economics, Chiang Mai University, Thailand. This work
was also supported in part by the National Science Foundation grants HRD-0734825
and HRD-1242122 (Cyber-ShARE Center of Excellence) and DUE-0926721, and by
an award “UTEP and Prudential Actuarial Science Academy and Pipeline Initiative”
from Prudential Foundation.
References
1. Briggs, W.: Uncertainty: The Soul of Modeling, Probability & Statistics. Springer,
Cham (2016)
2. Dutta, J.: On predictive evaluation of econometric models. Int. Econ. Rev. 21(2),
379–390 (1980)
3. Gneiting, T., Balabdaoui, F., Raftery, A.E.: Probabilsitic forecasts, calibration, and
sharpness. J. R. Stat. Soc. Part B 69(2), 243–268 (2007)
4. Huber, P.J., Ronchetti, E.M.: Robust Statistics. Wiley, Hoboken (2009)
5. Rabinovich, S.G.: Measurement Errors and Uncertainty: Theory and Practice.
Springer, Berlin (2005)
6. Sheskin, D.J.: Handbook of Parametric and Nonparametric Statistical Procedures.
Chapman & Hall/CRC, Boca Raton (2011)

Quantitative Justiﬁcation for the Gravity Model
in Economics
Vladik Kreinovich1(B) and Songsak Sriboonchitta2
1 Department of Computer Science, University of Texas at El Paso,
500 W. University, El Paso, TX 79968, USA
vladik@utep.edu
2 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
songsakecon@gmail.com
Abstract. The gravity model in economics describes the trade ﬂow
between two countries as a function of their Gross Domestic Products
(GDPs) and the distance between them. This model is motivated by the
qualitative similarity between the desired dependence and the depen-
dence of the gravity force (or potential energy) between the two bodies
on their masses and on the distance between them. In this paper, we
provide a quantitative justiﬁcation for this economic formula.
1
Gravity Model in Economics: A Brief Introduction
What is gravity model. It is known that, in general:
• neighboring countries trade more than distant ones, and
• countries with larger Gross Domestic Product (GDP) g have a higher volume
of trade than countries with smaller GDP.
Thus, in general, the trade ﬂow tij between the two countries i and j:
• increases when the GDPs gi and gj increase and
• decreases with the distance rij increases.
A qualitatively similar phenomenon occurs in physics: the gravity force fij
between the two bodies:
• increases when their masses mi and mj increase and
• decreases with the distance between then increases.
Similarly, the potential energy eij of the two bodies at distance rij:
• increases when the masses increase and
• decreases when the distance rij increases.
For the gravity force and for the potential energy, there are simple formulas:
fij = G · mi · mj
r2
ij
;
eij = G · mi · mj
rij
,
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_14

Quantitative Justiﬁcation for the Gravity Model in Economics
215
for some constant G. Both these formulas are a particular case of a general
formula
G · mi · mj
rα
ij
:
for the force, we take α = 2, and for the energy, we take α = 1.
By using the analogy with the gravity formulas, researchers have proposed
to use a similar formula to describe the dependence of the trade ﬂow tij on the
GDPs gi and on the distance rij:
tij = G · gi · gj
rα
ij
.
This formula – known as the gravity model in economics – has indeed been
successfully used to describe the trade ﬂows between diﬀerent countries; see, e.g.,
[2–6].
Remaining problem and what we do in this paper. While an analogy with
gravity provides a qualitative explanation for the gravity model, it is desirable
to have a quantitative explanation as well. Such an explanation is provided in
this paper.
2
Analysis of the Problem
What we want. We would like to have a formula that estimates the trade ﬂow
between the two countries tij as a function of their GDPs gi and gj and of the
distance rij between the two countries. In other words, we would like to come
up with a function F(a, b, c) for which
tij = F(gi, gj, tij).
(1)
To describe the corresponding function F(a, b, c), let us describe the natural
properties of such a function.
First natural property: additivity. At ﬁrst glance, the notion of a country
seems to be very clear and well deﬁned. However, there are many examples where
this notion is not that clear. Sometimes, a country becomes a loose confederation
of practically independent states. In other cases, several countries form such
a close trade union – from Benelux to European Union – that most trade is
regulated by the super-national organs and not by individual countries.
In all such cases, we have several diﬀerent entities i1, . . . , ik, . . . , iℓlocated
nearby forming a single super-entity. If we apply the formula (1) to each indi-
vidual entity ik, we get the expression
tikj = F(gik, gj, rikj).
Since all the entities ik are located close to each other, we can assume that
the distances rikj are all the same: rikj = rij. Thus, the above expression takes
the form tikj = F(gik, gj, rij).

216
V. Kreinovich and S. Sriboonchitta
By adding all these expressions, we can come up with the trade ﬂow between
the whole super-entity i and the country j:
tij =
ℓ

k=1
tikj =
ℓ

k=1
F(gik, gj, rij).
(2)
Alternatively, we can treat the super-entity as a single country with the overall
GDP gi =
ℓ
k=1
gik. In this case, by applying the formula (1) to this super-entity,
we get
tij = F(gi, gj, rij) = F
 ℓ

k=1
gik, gj, rij

.
(3)
It is reasonable to require that our estimate for the trade ﬂow should not
depend on whether we treat this loose confederation a single country or as several
independent countries. By equating the estimates (2) and (3), we conclude that
F(gi1, gj, rij) + . . . + F(giℓ, gj, rij) = F(gi1 + . . . + giℓ, gj, rij).
In other words, we must have the following additivity property for all possible
values a, . . . , a′, and b:
F(a, b, c) + . . . + F(a′, b, c) = F(a + . . . + a′, b, c).
(4)
A similar argument can be make if we consider the case when j is a loose
confederation of states. In this case, the requirement that our estimate for the
trade ﬂow should not depend on whether we treat this loose confederation as a
single country or as several independent countries leads to
F(gi, gj1, rij) + . . . + F(gi, gjℓ, rij) = F(gi, gj1 + . . . + gjℓ, rij),
i.e., to
F(a, b, c) + . . . + F(a, b′, c) = F(a, b + . . . + b′, c).
(5)
Second natural property: scale-invariance. The numerical value of the dis-
tance depends on what unit we use for measuring distance. For example, the
distance in miles in diﬀerent from the same distance in kilometers. If we replace
the original unit with a one which is λ times smaller, all numerical values of the
distance multiply by λ, i.e., each original numerical value rij is replaced by a
new numerical value
r′
ij = λ · rij.
It is reasonable to require that the estimates for the trade ﬂow should
not depend on what unit we use. Of course, we cannot simply require that
F(gi, gj, rij) = F(gi, gj, λ · rij) – this would mean that the trade ﬂow does not
depend on the distance at all. This is OK, since the numerical value of the trade
ﬂow also depends on what units we use: we get diﬀerent numbers if we use US

Quantitative Justiﬁcation for the Gravity Model in Economics
217
dollars or Thai Bahts. It is therefore reasonable to require that when we change
the unit for measuring rij, then after an appropriate change tij →t′
ij = μ · tij in
the measuring unit for trade ﬂow we get the same formula. In other words, we
require that for every λ > 0, there exists a μ > 0 for which
F(gi, gj, λ · rij) = μ · F(gi, gj, rij).
In other words, we require that
F(a, b, λ · c) = μ · F(a, b, c).
(6)
Third natural property: monotonicity. The ﬁnal natural property is that
as the distance increases, the trade ﬂow should decrease. In other words, the
function F(a, b, c) should be a decreasing function of c.
Now, we are ready to formulate our main result.
3
Deﬁnitions and the Main Result
Deﬁnition 1
• A
non-negative
function
F(a, b, c)
of
three
non-negative
variables
is
called additive if the following two equalities hold for all possible values
a, . . . , a′, b, . . . , b′, and c:
F(a, b, c) + . . . + F(a′, b, c) = F(a + . . . + a′, b, c);
F(a, b, c) + . . . + F(a, b′, c) = F(a, b + . . . + b′, c).
• A function F(a, b, c) is called scale-invariant if for every λ, there exists a μ
for which, for all a, b, and c, we have
F(a, b, λ · c) = μ · F(a, b, c).
• A function F(a, b, c) is called a trade function if it is additive, scale-invariant,
and increasing as a function of c.
Proposition 1. Every trade function has the form F(a, b, c) = G· a · b
cα for some
constants G and α.
Discussion. Thus, we have indeed justiﬁed the gravity model.
Proof of Proposition 1
1◦. Let us ﬁrst use the additivity property.

218
V. Kreinovich and S. Sriboonchitta
For every b and c, we can consider an auxiliary function fbc(a)
def
= F(a, b, c).
In terms of this function, the ﬁrst additivity property takes the form
fbc(a + . . . + a′) = fbc(a) + . . . + fbc(a′).
Functions of one variable that satisfy this property are known as additive. It is
known – see, e.g., [1] – that every non-negative additive function has the form
f(a) = k · a. Thus, F(a, b, c) = fbc(a) is equal to
F(a, b, c) = a · k(b, c)
for some function k(b, c).
Substituting this expression into the second additivity requirement, we con-
clude that
a · k(b + . . . + b′, c) = a · k(b, c) + . . . + a · k(b′, c).
Dividing both sides of this equality by a, we conclude that
k(b + . . . + b′, c) = k(b, c) + . . . + k(b′, c).
Thus, the function kc(b)
def
= k(b, c) is also additive. Hence, k(b, c) = kc(b) =
b · q(c) for some constant q(c) depending on c. Substituting this expression for
k(b, c) into the formula describing F(a, b, c) in terms of k(b, c), we conclude that
F(a, b, c) = a · b · c(q).
Hence, to complete the proof, it is suﬃcient to ﬁnd the function q(c).
2◦. For a = b = 1, we have F(a, b, c) = q(c). Thus, for these a and b, the fact
that F(a, b, c) is a decreasing function of c implies that q(c) is also an decreasing
function of c.
3◦. To ﬁnd the function q(c), let us now use scale invariance
F(a, b, λ · c) = μ(λ) · F(a, b, c).
Substituting F(a, b, c) = a · b · q(c) into this equality and dividing both sides
by a · b, we conclude that q(λ · c) = μ(λ) · q(c).
For every λ1 and λ2, we have
q((λ1 · λ2) · c) = μ(λ1 · λ2) · q(c).
On the other hand, we also have q(λ2 · c) = μ(λ2) · q(c) and thus,
q(λ1 · (λ2 · c)) = μ(λ1) · q(λ2 · c) = μ(λ1) · μ(λ2) · q(c).
By equating these two expressions for the same quantity q(λ1·λ2·c), we conclude
that
μ(λ1 · λ2) · q(c) = μ(λ1) · μ(λ2) · q(c).
Dividing both sides by q(c), we get
μ(λ1 · λ2) = μ(λ1) · μ(λ2).

Quantitative Justiﬁcation for the Gravity Model in Economics
219
Functions μ(λ) with this property are known as multiplicative.
Here, for every c, we have μ(λ) = q(λ · c)
q(c) . In particular, for c = 1, we get
μ(λ) = q(λ)
q(1) . Since q(c) is an increasing function, we conclude that μ(λ) is also
an increasing function.
It is known [1] that every monotonic multiplicative function has the form
μ(λ) = λ−α for some α > 0. From q(λ) = μ(λ) · q(1), we can conclude that
q(c) = G · c−α, where we denoted G
def
= q(1).
The proposition is proven.
4
Where Do We Go from Here
Trade ﬂow may depend on other characteristics. In the previous text, we
assumed that the trade ﬂow depends only on the GDPs and on the distance. In
reality, the trade ﬂow may also other depend on other characteristics, such as
the country’s population pi. Indeed, intuitively, the larger the population, the
more it consumes, so the larger its trade ﬂow with other countries.
Similar to GDP, population is an additive property, in the sense that if two
countries merge together, their population adds up. So, a natural question is:
how can we describe the dependence of the trade ﬂow on two or more additive
characteristics?
Let us describe this problem in precise terms. Let us consider the case
when each country is described by several additive characteristics, i.e., that gi
is now a vector consisting of several components gi = (g1i, . . . , gmi). We are
interested in ﬁnding the dependence tij = F(gi, gj, rij).
Let us describe the reasonable properties of this dependence.
Additivity and monotonicity. Similarly to the GDP-only case, we can con-
clude that
F(gi1 + . . . + giℓ, gj, rij) = F(gi1, gj, rij) + . . . + F(giℓ, gj, rij)
and
F(gi, gj1 + . . . + gjℓ, rij) = F(gi, gj1, rij) + . . . + F(gi, gjℓ, rij).
Also, similarly to the GDP-only case, it makes sense to require that the
function F(a, b, c) is a decreasing function of c.
Deﬁnition 2. Let m > 1.
• A non-negative function F(a, b, c) of three non-negative variables a, b ∈IRm
and c ∈IR is called additive if the following two equalities hold for all possible
values a, . . . , a′, b, . . . , b′, and c:
F(a, b, c) + . . . + F(a′, b, c) = F(a + . . . + a′, b, c);
F(a, b, c) + . . . + F(a, b′, c) = F(a, b + . . . + b′, c).

220
V. Kreinovich and S. Sriboonchitta
• A function F(a, b, c) is called scale-invariant if for every λ, there exists a μ
for which, for all a, b, and c, we have
F(a, b, λ · c) = μ · F(a, b, c).
• A function F(a, b, c) is called a trade function if it is additive, scale-invariant,
and increasing as a function of c.
Proposition 2. Every trade function has the form
F(gi, gj, rij) =

β

γ
Gβγ · gβi · gγj
rα
ij
for some constants Gβγ and α.
Example. For the case of GDP gi and population pi, we have
tij = Ggg · gi · gj + Ggp · gi · pj + Gpg · pi · gj + Gpp · pi · pj
rα
ij
.
An interesting property of this example is that, in contrast to the GDP-only
case, when we always had tij = tji, we can have “asymmetric” trade ﬂows for
which tij ̸= tji.
Proof of Proposition 2 is similar to the proof of Proposition 1: ﬁrst additivity
requirement implies that F(a, b, c) is linear in a, second – that it is linear in b,
so it is bilinear in a and b. Now, scale-invariance implies that all the coeﬃcients
of this bilinear dependence be proportional to r−α
ij
for some α > 0.
Discussion. It would be nice to test these formulas on real data.
Acknowledgments. We acknowledge the partial support of the Center of Excellence
in Econometrics, Faculty of Economics, Chiang Mai University, Thailand. This work
was also supported in part by the National Science Foundation grants HRD-0734825
and HRD-1242122 (Cyber-ShARE Center of Excellence) and DUE-0926721, and by
an award “UTEP and Prudential Actuarial Science Academy and Pipeline Initiative”
from Prudential Foundation.
References
1. Aczel, J., Dhombres, J.: Functional Equations in Several Variables. Cambridge Uni-
versity Press, Cambridge (1989)
2. Anderson, J.E.: A theoretical foundation for the gravity equation. Am. Econ. Rev.
69(1), 106–116 (1979)
3. Anderson, J.E., Van Wincoop, E.: Gravity with gravitas: a solution to the border
puzzle. Am. Econ. Rev. 93(1), 170–192 (2003)
4. Bergstrand, J.H.: The gravity equation in international trade: some microeconomic
foundations and empirical evidence. Rev. Econ. Stat. 67(3), 474–481 (1985)

Quantitative Justiﬁcation for the Gravity Model in Economics
221
5. Pastpipatkul, P., Boonyakunakorn, P., Sriboonchitta, S.: Thailand’s export and
ASEAN economic integration: a gravity model with state space approach. In:
Huynh, V.-N., Inuigichi, M., Le, B., Le, B.N., Denoeux, T. (eds.) Proceedings of
the 5th International Symposium on Integrated Uncertainty in Knowledge Modeling
and Decision Making IUKM 2016, pp. 664–674. Springer, Cham (2016)
6. Tinbergen, J.: Shaping the World Economy: Suggestions for an International Eco-
nomic Policy. The Twentieth Century Fund, New York (1962)

The Decomposition of Quadratic Forms Under
Skew Normal Settings
Ziwei Ma1, Weizhong Tian2, Baokun Li3, and Tonghui Wang4(B)
1 Department of Mathematical Sciences, New Mexico State University(USA)
and College of Science, Northwest A and F University, Xianyang, China
ziweima@nmsu.edu
2 Department of Mathematical Sciences, Eastern New Mexico University (USA)
and School of Sciences, Xi’an University of Technology, Xi’an, China
weizhong.tian@enmu.edu
3 School of Statistics, Southwestern University of Finance and Economics,
Chengdu, China
bali@swufe.edu.cn
4 Department of Mathematical Sciences,
New Mexico State University, Las Cruces, USA
twang@nmsu.edu
Abstract. In this paper, the decomposition properties of noncentral
skew chi-square distribution is studied. A given random variable U hav-
ing a noncentral skew chi-square distribution with k > 1 degrees of
freedom, can be partitioned into the sum of two independent random
variables U1 and U2 such that U1 has a noncentral skew chi-square dis-
tribution with 1 degree of freedom and U2 has the noncentral chi-square
distribution with k−1 degrees of freedom. Also if k > 2, this partition can
be modiﬁed into U = U1+U2, where U1 has a noncentral skew chi-square
distribution with 2 degrees of freedom and U2 has a central chi-square
distribution with k −2 degrees of freedom. The densities of noncen-
tral skew chi-square distributions with 1 degree of freedom, 2 degrees
of freedom, and k > 2 degrees of freedom are derived, and their graphs
are presented. For illustration of our main results, the linear regression
model with skew normal errors is considered as an application.
Keywords: Skew normal distributions · Skew chi-square distributions
Quadratic form · Decomposition
1
Introduction
There are many real world problems in which we need to analyze the data which
are asymmetrically distributed. We often assume that data are symmetrically
or approximately normally distributed so that the classical statistical methods
are applied. However, these data analysis results may have limitations such as a
lack of robustness against departures from the normal distribution and invalid
statistical inferences for skewed data. One of the solutions is to introduce an
extra skewness (or shape) parameter to normal distributions, thus getting skew
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_15

The Decomposition of Quadratic Forms Under Skew Normal Settings
223
normal distributions which have location, scale and shape parameters, to ﬁt
various types of skewed data, see Azzalini and his collaborators’ work [2–5] for
univariate and multivariate cases, respectively (see more details in Azzalini and
Capitanio [6]). In practice, the family of skew normal distributions is suitable
for the analysis of skewed data which is unimodally distributed (see Hill, Arnold
[1,11]).
For the situation when the location parameter is zero, the distribution of
quadratic forms of skew normal vectors was discussed by Genton et al. [8] , Gupta
and Huang [7] , and general skew-elliptical distribution was discussed by Fang
[9] and Genton and Loperﬁdo [10] . For the situation when location parameter
is not zero, Wang et al. [12] deﬁned the noncentral skew chi-square distribution,
studied the quadratic forms of generalized skew normal vectors, and analyzed basic
features such as moment generating function (mgf), independence and a version
of Cochran’s theorem. Later on, Ye and Wang [13] revisited the noncentral skew
chi-square distribution, derived the probability density function (pdf) and applied
this distribution to statistical inferences in linear mixed model with skew normal
random eﬀects and variance components models under skew normal setting (see
Ye et al. [14]). In this paper, we study the decomposition of noncentral skew chi-
square distributed random variable, and give two other representations of its pdf.
This paper is organized as follows. Features of multivariate skew normal
distribution and skew chi-square distribution such as their probability density
function, moment generating function are discussed in Sect. 2. Decompositions of
a noncentral skew chi-square distribution are studied in Sect. 3. For illustration,
applications of main results are given in Sect. 4.
2
Skew-Normal and Skew Chi-Square Distributions
A random vector Y ∈Rk is said to have a multivariate skew-normal distribution,
denoted by Y ∼SNk(μ, Σ, α), if its pdf is given by
f(y) = 2φk(y; μ, Σ)Φ

α′Σ−1/2 (y −μ)

,
where μ, α ∈Rk, Σ is a positive deﬁnite k × k matrix, φk(y; μ, Σ) is the pdf
of k-dimension normal distribution with mean μ and covariance matrix Σ, and
Φ(·) is the cumulative distribution function (cdf) of standard normal distribution
N(0, 1). The following lemmas will be used to prove our main results.
Lemma 2.1 (Azzalini [6]). If V ∼SNk (μ, Σ, α), and A is an orthogonal k × k
matrix, then
A′V ∼SNk (A′μ, A′ΣA, A′α) .
Lemma 2.2 (Wang et al. [12]). Let Y
∼SNn (μ, In, α). Then Y has the
following properties.

224
Z. Ma et al.
(a) The moment generating function of Y is given by
MY (t) = 2 exp

t′μ + t′t
2

Φ

α′t
(1 + α′α)1/2

,
for t ∈Rn and
(b) Two linear functions of Y , A′Y and B′Y are independent if and only if
(i) A′B = 0 and (ii) A′α = 0 or B′α = 0.
Lemma 2.3 (Wang et al. [15]). Let Y ∼SNn(ν, In, α0), and let A be a n × k
matrix with full column rank, then the linear function of Y , A′Y ∼SNk(μ, Σ, α),
where
μ = A′ν,
Σ = A′A,
and
α =
(A′A)−1/2A′α0

1 + α′
0 (In −A(A′A)−1A′) α0
.
Recall that the noncentral chi-square distribution with k degrees of freedom
and noncentrality parameter λ is deﬁned as the distribution of V ′V where V
follows k-dimensional normal distribution with location vector ν and scale matrix
Ik, denoted as V ∼Nk (ν, Ik). The noncentral skew chi-square distribution was
ﬁrst deﬁned in Wang et al. [12] and its modiﬁed version was given in Ye and
Wang [13].
2.1
Noncentral Skew Chi-Square Distribution with Degrees
of Freedom k > 1
Deﬁnition 1. Let Y ∼SNk (μ, Ik, α). The distribution of U = Y ′Y is called
as the noncentral skew chi-square distribution with degree of freedom k, the
noncentrality parameter λ = μ′μ, and the skewness parameters δ1 = μ′α, δ2 =
α′α, denoted by U ∼Sχ2
k (λ, δ1, δ2).
The following Lemma lists the basic properties of noncentral skew chi-square
distributions.
Lemma 2.4. Let U ∼Sχ2
k (λ, δ1, δ2).
(i) (Ye and Wang [13]) The pdf of U is given by
fU (u; λ, δ1, δ2) =
exp
	
−1
2 (λ + u)

Γ
 1
2

Γ
 k−1
2

2k/2−1 h (u; λ, δ1, δ2) ,
u > 0,
(1)
where
h (u; λ, δ1, δ2) =
 √u
√u
exp

λ1/2s
 
u −s2 k−3
2
Φ

α0

s −λ1/2
ds
and α0 = λ−1/2δ1/(1 + δ2 −δ2
1/λ)1/2.

The Decomposition of Quadratic Forms Under Skew Normal Settings
225
(ii) (Wang et al. [12]) The mgf of U is given by
MU (t) =
2 exp

t (1 −2t)−1 λ

(1 −2t)k/2
Φ
⎧
⎪
⎨
⎪
⎩
2t (1 −2t)−1 δ1

1 + δ2 (1 −2t)−11/2
⎫
⎪
⎬
⎪
⎭
(2)
for 0 < t < 1/2.
Remark 2.1. Note that for k = 1,the integral of h (x; λ, δ1, δ2) given in (1) is
inﬁnite so we need to discuss the noncentral skew chi-square distribution with 1
degree of freedom separately.
2.2
The Noncentral Skew Chi-Square Distribution with 1 Degree
of Freedom
For k = 1, Y 2 ∼Sχ2
1 (λ, δ1, δ2) with Y ∼SN(ν, 1, α) where λ = ν2, δ1 =
να, δ2 = α2 which implies these three parameters λ, δ1, and δ2 are determined
by ν and α so that they are not independent parameters.
Theorem 1. Suppose that Y ∼SN (ν, 1, α) and U = Y 2, then the pdf of U is
fU (u) = u−1/2 	
φ
√u; ν, 1

Φ

α
√u −ν

+ φ

−√u; ν, 1

Φ

α

−√u −ν

(3)
for u > 0.
Proof. For any u > 0, consider the cdf of U
FU (u) ≡P (U ≤u) = P

Y 2 ≤u

= P

−√u ≤Y ≤√u

=
 √u
−√u
2φ (s; ν, 1) Φ (α (s −ν)) ds.
The desired results follows by taking the derivative of FU(u) with respect
to u.
□
Remark 2.2. (i) When ν = 0, the pdf of U given in (3) can be written as
fU (u) = u−1/2φ
√u

= u−1/2e−u/2
√
2π
for
u > 0
which is the pdf of χ2
1(0) and is free of skewness parameter α.
(ii) When α = 0, the pdf of U given in (3) can be written as
fU(u, 1, ν) =
1
2√u

φ(√u −ν) + φ(√u + ν)

=
1
√
2πue−(u+ν2)/2 cosh(ν√u)
for u > 0 which is the pdf of χ2
1(λ) with λ = ν2.

226
Z. Ma et al.
Fig. 1. Density curves of Sχ2
1 with ﬁxed δ2 (left) and ﬁxed λ (right)
Fig. 2. Density curves of Sχ2
df(5, −4, 5) with degrees of freedom df = 2, 5, 10, 20
(iii) In general case, there is a relation among parameters λ, δ1 and δ2, which
is δ2
1 = λδ2. Therefore, these parameters are fully determined by ν and α. The
density curves Sχ2
1 (λ, δ1, δ2) are given Fig. 1. From Fig. 1, we can see that the
changes in values of ν (taking values −3, −1, 1, 3) inﬂuence the shapes of the
density curves for ﬁxed α = 1. Also, we can see that the changes in values of
α (taking values −3, −1, 1, 3) inﬂuence the shape of density curves as well ﬁxed
location parameter ν = 1.
Example 2.1. Consider Y = (Y1, · · · , Yk)′ ∼SNk(μ1k, Ik, α1k) and let Y =
1
k
k
i=1 Yi. Then it is easy to see that
√
k Y ∼SN(
√
kμ, 1,
√
kα), so by Deﬁnition
1, we have kY
2 ∼Sχ2
1(λ, δ1, δ2) with λ = kμ2, δ1 = kμα and δ2 = kα2.
3
Decomposition of Noncentral Skew Chi-Square
Distributions
First, we will prove a fundamental result of the decomposition of skew chi-square
distributions.

The Decomposition of Quadratic Forms Under Skew Normal Settings
227
Theorem 2. Let U ∼Sχ2
k (λ, δ1, δ2) with δ2 ̸= 0 and k ≥2, then U can be
partitioned into two independent random variables U1 and U2: U = U1 + U2,
where U1 ∼Sχ2
1 (λ1, δ∗
1, δ∗
2) and U2 ∼χ2
k−1 (λ −λ1) if and only if
(i) δ∗
i = δi for i = 1, 2 and
(ii) λ1 = δ2
1/δ2.
Proof. “If part” is trivial since the mgf of U can be rewritten as the product of
the mgf’s of U1 and U2. For the proof of “only if” part, let Y1 ∼SN (μ1, 1, α1)
with α1 = √δ2 and μ1 = δ1/√δ2, and let Y2 ∼Nk−1 (μ2, Ik−1) with μ2 =

λ −δ2
1/δ2, 0, · · · , 0
′
∈ℜk−1. Assume that Y1 and Y2 are independent. Then
the joint distribution of Y = (Y1, Y ′
2)′ is SNk (ν, Ik, α), where ν = (μ1, μ′
2)′ and
α = (α1, 0, · · · , 0)′. Note that
ν′ν = μ2
1 + μ′
2μ2 = δ2
1
δ2
+ λ −δ2
1
δ2
= λ,
ν′α = δ1,
and
α′α = δ2,
which implies Y ′Y ∼Sχ2
k (λ, δ1, δ2). On the other hand, Y ′Y = Y 2
1 +Y ′
2Y2 where
Y 2
1 ∼Sχ2
1 (λ1, δ1, δ2) with λ1 = δ2
1/δ2 and Y ′
2Y2 ∼χ2
k−1 (λ −λ1). Therefore, the
desired decomposition is obtained with U1 = Y 2
1 and U2 = Y ′
2Y2.
□
Based on Theorem 2, the density of U ∼Sχ2
k (λ, δ1, δ2) can be derived.
Theorem 3. The density of U ∼Sχ2
k (λ, δ1, δ2) with k > 1 is
fU (u) =
 u
0
g1 (u −v) h1 (v) dv
(4)
where g1 (·) is pdf of Sχ2
1 (λ1, δ1, δ2) and h1 (·) is the pdf of χ2
k−1 (λ2) with λ1 =
δ2
1
δ2 , and λ2 = λ −λ1.
Proof. By Deﬁnition 1, there exists Y ∼SNk (μ, Ik, α) such that Y ′Y
d= U
where λ = μ′μ, δ1 = μ′λ and δ2 = α′α. From Theorem 2, we select α =
(α1, 0, · · · , 0)′, in fact by Lemma 2.1, it is reasonable to assume that. It is easy
to obtain α1 = δ1/√δ2, and λ1 = δ2
1/δ2.
□
The following example shows the partition is not unique.
Example 3.1. Suppose that U ∼Sχ2
3(4, 2, 4), then let Yi = (Yi1, Yi2, Yi3)′ ∼
SN3(μi, I3, αi) for i = 1, 2, 3 with μ1 = (1,
√
3, 0)′, α1 = (2, 0, 0)′, μ2 = (2, 0, 0)′,
α2 = (1,
√
3, 0)′, μ3 = (
√
2, 0,
√
2)′, and α3 = (
√
2,
√
2, 0)′, then by deﬁnition
of noncentral skew chi-square distributions, it is clear that Y ′
i Yi ∼Sχ2
3(4, 2, 4)
for i = 1, 2, 3. However, the decompositions of these three quadratic forms are
diﬀerent.
For i = 1,

Y11
Y12

∼SN2

2
0

, I2,
 1
√
3

and Y13 ∼N (0, 1) are inde-
pendent which implies the decomposition of Y ′
1Y1 = U11 + U12 where U11 ∼
Sχ2
2(4, 2, 4) and U12 ∼χ2
1(0) are independently distributed.

228
Z. Ma et al.
For i = 2, Y21 ∼SN (1, 2) and

Y21
Y22

∼N2
√
3
0

, I2

are independently
distributed which implies the decomposition of Y ′
2Y2 = U21 + U22 where U21 ∼
Sχ2
1(1, 2, 4) and U22 ∼χ2
2(3) are independently distributed.
For i = 3,

Y31
Y32

∼SN2
√
2
0

, I2,
√
2
√
2

and Y33 ∼N
√
2, 1

are
independently distributed as well, which implies the decomposition of Y ′
3Y3 =
U31 + U32 where U31 ∼Sχ2
2(2, 2, 4) and U32 ∼χ2
1(2) are independently distrib-
uted.
To generalize above example, we obtain the following statement.
Corollary 3.1. Let U ∼Sχ2
k (λ, δ1, δ2) , U1 ∼Sχ2
k1(λ1, δ∗
1, δ∗
2) with δ2 ̸= 0 and
U2 ∼χ2
k−k1(λ −λ1) for k1 ≥2 are independent. Then U
d= U1 + U2 if and only
if
(i) δ2
1/δ2 ≤λ1 ≤λ;
(ii) δ∗
i = δi for i = 1, 2.
Proof. “If part” is trivial based on the mgfs of U, U1 and U3. For “only if”
Let us assume that k1 = 2 the method used here for k1 = 2 can be easily
extend to k1 > 2. Similarly to proof of Theorem 2, we construct a skew normal
vector Y =
Y1
Y2

which can satisfy all conditions for the desired decomposition.
Starting from Y1, let Y1 ∼SN2 (μ, I2, α) and Y2 ∼Nk−2 (ν, Ik−2) with ν =
√λ −λ1, 0, · · · , 0
′ ∈Rk−2 independently such that for μ, α ∈R2, μ′μ = λ1,
μ′α = δ1 and α′α = δ2 where δ2
1/δ2 ≤λ1 ≤λ. Those equations system is
always solvable, say α =
√δ2, 0
′, μ = (μ1, μ2)′ where μ1 =
δ1
√δ2 and μ2 =

λ1 −δ2
1/δ2. On one hand, we have
Y =

Y1
Y2

∼SNk (ν∗, Ik, α∗)
where ν∗=
μ
ν

and α∗=
α
0

, and following simple algebra, Y ′Y
∼
Sχ2
k (λ, δ1, δ2). On the other hand, Y ′Y
=
Y ′
1Y1 + Y ′
2Y2 where Y ′
1Y1
∼
Sχ2
2 (λ1, δ1, δ2) and Y ′
2Y2 ∼χ2
k−2 (λ −λ1) independently. Combining both sides,
the desired results follows.
For k1 > 2 case, we just set μ, α ∈Rk1 satisfying the same conditions, and
adjust ν correspondingly.
□
Consequently, we obtain another representation of pdf of U as follows.
Corollary 3.2. The pdf of U ∼Sχ2
k (λ, δ1, δ2) is
fU (u) =
 u
0
g2 (u −v) h2 (v) dv
(5)
where g2 (·) is pdf of Sχ2
k1 (λ1, δ1, δ2) and h2 (·) is the pdf of χ2
k−k1 (λ −λ1) for
k1 ≥2 where δ2
1/δ2 ≤λ1 ≤λ; .

The Decomposition of Quadratic Forms Under Skew Normal Settings
229
Fig. 3. Density curves of Sχ2
2(λ, 2, 5) with λ = 1, 2, 3, 4
Theorem 4. For U ∼Sχ2
k (λ, δ1, δ2) where k ≥2, the pdfs of U given by (1),
(4) and (5) are identical.
Proof. The pdfs given in (1), (4) and (5) have the same mgfs, therefore they
are identical.
□
The following graphs represent that the impact of parameters in noncentral
skew chi-square distribution on the shape of curves of pdf of Sχ2
k(λ, δ1, δ2) for
diﬀerent values of degrees of freedom k and other parameters λ, δ1, δ2 (Figs. 2,
3, 4 and 5).
Fig. 4. Density curves of Sχ2
2(5, δ1, 5) with δ1 = −4, −1, 1, 4
Remark 3.1. Note that from Corollary 3.1 there could be more ﬂexible decom-
position on noncentrality if the degrees of freedom for skew chi-square is great
than 1. So for the convenience, we choose k1 = 2 and λ1 = λ, then we have the
following result.
Corollary 3.3. Let U ∼Sχ2
k (λ, δ1, δ2) with δ2 ̸= 0. Then U can be partitioned
into the sum of two independent random variable U1 and U2 such that U1 ∼
Sχ2
2 (λ, δ1, δ2) and U2 ∼χ2
k−2 (0).

230
Z. Ma et al.
Fig. 5. Density curves of Sχ2
2(5, 2, δ2) with δ2 = 1, 2, 3, 4
Remark 3.2. (i) Notice above results, for all decompositions for U
∼
Sχ2
k(λ, δ1, δ2), the noncentral skew chi-square parts have exactly the same
skew parameters δ1, δ2 with original random variable U, which means “decom-
positions” is just decompositions of noncentral parameters λ and degrees of
freedom k;
(ii) Further generalizing the decompositions is possible to represent a non-
central skew chi-square distribution U ∼Sχ2
k(λ, δ1, δ2) as the independent sum
of more than two components, say U
d= U1+Σ k
i=1Ui where U1 ∼Sχ2
k1(λ1, δ1, δ2),
and Ui ∼χ2
ki(λi) for i = 2, · · · , k such that Σ k
i=1λi = λ and Σ k
i=1ki = k where
|δ1| = √λ1δ2 when k1 = 1 or |δ1| ≤√λ1δ2 when k1 > 1.
4
Partitions in Linear Regression Model with Skew
Normal Errors
In this section, we will consider the regression model with skew normal errors ,
using decomposition results of skew chi-square distributions. Consider the regres-
sion model with skew normal errors:
yi = β0 + β1xi1 + · · · + βp−1xi,p−1 + Ei
for
i = 1, · · · , n
which is equivalent to the matrix form
Y = Xβ + E ,
(6)
where
Y =
⎛
⎜
⎝
y1
...
yn
⎞
⎟
⎠, X =
⎛
⎜
⎝
1 x11 · · · x1,p−1
...
...
...
...
1 xn1 · · · xn,p−1
⎞
⎟
⎠, β =
⎛
⎜
⎝
β0
...
βp−1
⎞
⎟
⎠, E =
⎛
⎜
⎝
E1
...
En.
⎞
⎟
⎠.
Assume that the rank of design matrix X is p and E ∼SNn

0, σ2In, α

so that
Y ∼SNn(Xβ, σ2In, α). By Deﬁnition 1, we obtain
Y ′Y
σ2
∼Sχ2
n (λ, δ1, δ2) ,
(7)

The Decomposition of Quadratic Forms Under Skew Normal Settings
231
where λ = μ′μ/σ2, δ1 = α′μ/σ and δ2 = α′α with μ = Xβ. Consider the least
squares estimator, ˆβ, of β, and estimator, s2, of σ2, respectively, given by
ˆβ = (X′X)−1X′Y
and
s2 =
1
n −pY ′(In −PX)Y,
where PX = X(X′X)−1X′. From Lemma 2.3, we obtain ˆβ ∼SNp (β, Σ1, α1),
where
Σ1 = σ2(X′X)−1
and
α1 =
(X′X)1/2X′α

1 + α′(In −PX)α
.
In many cases, random errors are assumed to be identically distributed so that
α = α01n. Now we want to test the hypothesis H0 : β = β0 versus H1 : β ̸= β0.
We can use the partition
Y ′Y = Y ′PXY + Y ′ (In −PX) Y ≡U1 + U2.
(8)
Now it suﬃces to show that U1 and U2 have skew chi-square distributions and
U1 and U2 are independent. We obtain by Lemma 2.2 that PXY and (In −PX)Y
are independent so that ˆβ and U2 are independent. From Lemma 2.3, we know
that
(X′X)
1
2 ˆβ = (X′X)−1
2 X′Y ∼SNp

(X′X)
1
2 β, σ2Ip, αβ

,
where
αβ =
(X′X)−1/2X′α

1 + α′(In −PX)α
= (X′X)−1
2 X′1nα0.
Therefore by Deﬁnition 1, we obtain
(ˆβ −β0)′(X′X)(ˆβ −β0)
σ2
∼Sχ2
p(λ∗, δ∗
1, δ∗
2),
(9)
where
λ∗= (β −β0)′(X′X)(β −β0)
σ2
,
δ∗
1 = (β −β0)′X′α
σ
,
δ∗
2 = nα2
0.
(10)
Note that U1 = Y ′PXY = ˆβ′(X′X)ˆβ so that,
U1
σ2 ∼Sχ2
p

β′(X′X)β/σ2, α0β′X′1n/σ, nα2
0

.
(11)
Also, it is easy to show that
U2
σ2 = (n −p)S2
σ2
∼χ2
n−p(0).
(12)
Thus, we can deﬁne the test statistic for
H0 :
β = β0
v.s.
Ha :
β ̸= β0

232
Z. Ma et al.
as
F = (ˆβ −β0)′X′X(ˆβ −β0)/p
s2
∼SFp,n−p(λ∗, δ∗
1, δ∗
2),
(13)
the noncentral skew-F distribution with degrees of freedom p and n −p, non-
centrality λ∗and skewness parameters δ∗
1 and δ∗
2 deﬁned in (10). If H0 is
true, then λ∗= 0 and δ∗
1 = 0, F
∼Fp,n−p(0). If H0 is rejected, then
F ∼SFp,n−p(λ∗, δ∗
1, δ∗
2).
Acknowledgments. Authors would like to thank referee’s valuable comments which
led to improvement of this paper.
References
1. Aronld, B.C., Beaver, R.J., Groenevld, R.A., Meeker, W.Q.: The nontruncated
marginal of a truncated bivariate normal distribution. Psychometrica 58(3), 471–
488 (1993)
2. Azzalini, A.: A class of distributions which includes the normal ones. Scand. J.
Statist. 12(2), 171–178 (1985)
3. Azzalini, A.: Further results on a class of distributions which includes the normal
ones. Statistica 46(2), 199–208 (1986)
4. Azzalini, A., Dalla Valle, A.: The multivariate skew-normal distribution. Bio-
metrika 83(4), 715–726 (1996)
5. Azzalini, A., Capitanio, A.: Statistical applications of the multivariate skew normal
distribution. J. R. Stat. Soc. 61(3), 579–602 (1999)
6. Azzalini, A., Capitanio, A.: The Skew-Normal and Related Families, vol. 3.
Cambridge University Press, Cambridge (2013)
7. Gupta, A.K., Huang, W.J.: Quadratic forms in skew normal variates. J. Math.
Anal. Appl. 273, 558–564 (2002)
8. Genton, M.G., He, L., Liu, X.: Moments of skew-normal random vectors and their
quadratic forms. Stat. Probab. Lett. 51, 319–325 (2001)
9. Fang, B.Q.: The skew elliptical distributions and their quadratic forms. J. Multivar.
Anal. 87, 298–314 (2003)
10. Genton, M.G., Loperﬁdo, N.: Generalized skew-elliptical distributions and their
quadratic forms. Ann. Inst. Stat. Math. 57, 389–401 (2005)
11. Hill, M., Dixon, W.J.: Robustness in real life: a study of clinical laboratory data.
Biometrics 38, 377–396 (1982)
12. Wang, T., Li, B., Gupta, A.K.: Distribution of quadratic forms under skew normal
settings. J. Multivar. Anal. 100(3), 533–545 (2009)
13. Ye, R., Wang, T.: Inferences in linear mixed model with skew-normal random
eﬀects. Acta Math. Sin. Engl. Ser. 31(4), 576–594 (2015)
14. Ye, R., Wang, T., Sukparungsee, S., Gupta, A.K.: Tests in variance components
models under skew-normal settings. Metrika 78(7), 885–904 (2015)
15. Wang, Z., Wang, C., Wang, T.: Estimation of location parameter in the skew
normal setting with known coeﬃcient of variation and skewness. Int. J. Intell.
Technol. Appl. Stat. 9(3), 191–208 (2016)

Joint Plausibility Regions for Parameters
of Skew Normal Family
Ziwei Ma1,2, Xiaonan Zhu1, Tonghui Wang1(B),
and Kittawit Autchariyapanitkul3
1 Department of Mathematical Sciences, New Mexico State University,
Las Cruces, USA
{ziweima,xzhu,twang}@nmsu.edu
2 College of Science, Northwest A&F University, Yangling, China
3 Faculty of Economics, Maejo University, Chiang Mai, Thailand
kittawit a@mju.ac.th
Abstract. The estimation of parameters is a challenge issue for skew
normal family. Based on inferential models, the plausibility regions for
two parameters of skew normal family are investigated in two cases,
when either the scale parameter σ or the shape parameter δ is known.
For illustration of our results, simulation studies are proceeded.
Keywords: Skew normal distribution · Inferential models
Plausibility regions
1
Introduction
Skew data sets occur in many diverse ﬁelds, such as economics, ﬁnance, biomedi-
cine, environment, demography, and pharmacokinetics, just to name a few. In
conventional procedure, practitioners assume that the data are normally distrib-
uted to proceed statistical analysis. This restrictive assumption, however, may
result in not only a lack of robustness against departures from the normal dis-
tribution and but also in invalid statistical inferences, especially when data are
skewed. One solution to analyze skewed data is to extend the normal family by
introducing an extra parameter, thus getting skew normal distributions which
have location, scale and shape parameters (see Azzalini and his collaborators’
work [2,6] and reference therein). In many practical cases, skew normal distrib-
ution is suitable for the analysis of data which is unimodal empirical distributed
but with some skewness (see Arnold [1], Hill [8]). In past three decades, the
family of skew-normal distributions, including multivariate skew-normal distri-
butions, has been studied by many authors, e.g. Azzalini [3–5], Wang et al. [17],
Ye et al. [18].
However, estimating parameters of skew normal family is a challenge,
especially in relation to estimating shape parameter (see Azzalini [4] and
Pewsey [15]). Liseo and Loperﬁdo [9] pointed out the likelihood function is
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_16

234
Z. Ma et al.
increasing with positive probability, which leads to an inﬁnite maximum like-
lihood estimate for shape parameter, and the method of moments can give even
worse results. New methods are needed to solve this problem. Some methods
for estimation of parameters were studied by many authors, see Azzalini and
Capitanio [4], Sartori [16], Liseo and Loperﬁdo [9], Debarshi [7] and Mameli et
al. [14]. In particular, Zhu et al. [19] applied inferential models (IMs) to construct
plausibility interval for shape parameter.
In this study, we construct plausibility regions for two parameters of skew
normal population in two cases, with either shape parameter or scale parameter
is known, by inferential models (IMs). IMs are new methods of statistical infer-
ence introduced by Martin and Liu [10,12]. Comparing with Fisher’s ﬁducial
inference, Dempster-Shafer theory of belief functions and Bayesian inference,
IMs have several advantages: (i) IMs are free of prior distributions; (ii) IMs
depend only on the observed data. For more details of IMs, see Martin and his
collaborators’ work [10–13].
This paper is organized as following. The basic concepts on skew-normal
distributions and IMs are introduced brieﬂy in Sect. 2. Plausibility regions for
the parameters of skew normal population are obtained in two cases in Sect. 3.
Simulation studies are proceeded for illustration of our main result in Sect. 4.
2
Preliminaries
Throughout of this paper, we use φ(·) and Φ(·) to denote the probability density
function (pdf) and cumulative distribution function (cdf) of the standard normal
distribution, respectively. Let F(·) be the cdf of χ2(0) distribution and G(·) be
the cdf of skew normal distribution, and N(0, 1) and Uniform(0, 1) represent the
standard normal and uniform distributions, respectively.
2.1
Brief Review of Skew-Normal Distributions
A random variable Z is said to be skew-normal distributed with the shape para-
meter λ, denoted by Z ∼SN(λ), if its pdf is given by
f(z; λ) = 2φ(z)Φ(λz),
z, λ ∈R.
For any μ ∈R and σ > 0, the distribution of X = μ + σZ is said to be skew-
normal distributed with the location parameter μ, the scale parameter σ and
the shape parameter λ, denoted by X ∼SN(μ, σ2, λ) and the pdf of X is
f(x; μ, σ2, λ) = 2
σ φ
x −μ
σ

Φ
λ(x −μ)
σ

.
There is an alternative representation of Z ∼SN(0, 1, λ) given by
Z = δ|Z0| +

1 −δ2Z1,
δ ∈(−1, 1)
(1)
where Z0 and Z1 are independent N(0, 1) random variables and δ = λ/
√
1 + λ2.
This stochastic representation plays a vital role in establishing our IMs. See
Azzalini and Capitanio’s book [5] and reference therein for more details.

Joint Plausibility Regions for Parameters of Skew Normal Family
235
2.2
Inference Models
Let X be an observable random sample with a probability distribution PX|θ on
a sample space X, where θ is an unknown parameter, θ ∈Θ, a parameter space.
Let U be an unobservable auxiliary variable on an auxiliary space U, where
although U is unobservable, we assume that U and U are known. An association
is a map a : U × Θ →X such that
X = a(U, θ).
For any given statistical assertion on parameters, an IM consists of following
three steps.
Association Step (A-step). Suppose we have an association X = a(U, θ) and
an observation X = x, where x could be a scalar or vector, then the unknown θ
must satisfy
x = a(u∗, θ)
for some unobserved u∗of U. So from the observation X = x, we have the set
of solutions
Θx(u) = {θ ∈Θ : x = a(u, θ)},
x ∈X,
u ∈U.
Prediction Step (P-step). Since the true u∗is unobservable, to make a valid
inference, the key point is to predicate u∗. Let u →S(u) be a set-value map
from U to S, a collection of PU-measurable subsets of U. Then the random set
S : U →S is called a predictive random set of U with distribution PS = PU ◦S−1.
We will use S to predict u∗.
Combination Step (C-step). Deﬁne
Θx(S) = ∪
u∈SΘx(u).
For any assertion A of θ, i.e., A ⊆Θ, the belief function and plausibility function
of A with respect to a predictive random set S are deﬁned by,
belx(A; S) = PS{Θx(S) ⊆A : Θx(S) ̸= ∅};
plx(A; S) = PS{Θx(S) ̸⊆Ac : Θx(S) ̸= ∅}.
Note that
plx(A; S) = 1−belx(Ac; S),
belx(A; S)+belx(Ac; S) ≤1,
for all
A ⊆Θ.
Based on the plausibility function derived form an IM, the 100(1 −α)% level
plausibility region for θ follows
ΠX(α) = {θ : plX(θ; S) > α},
which is counter part of conﬁdence regions in classical statistics.

236
Z. Ma et al.
3
Plausibility Regions for Parameters of the Skew
Normal Family
Suppose that X1, . . . , Xn are identical distributed random variables from the
population SN(μ, σ2, λ) with stochastic representations
Xi = μ + σ(δ|Z0| +

1 −δ2Zi),
i = 1, . . . , n,
(2)
where (Z0, Z1, · · · , Zn)′ ∼Nn+1 (0, In+1) and δ = λ/
√
1 + λ2. See Azzalini and
Capitanio [5] for details.
It is clear that Xi ∼SN(μ, σ2, λ) for i = 1, · · · , n, but they are dependent
since they share the same component |Z0| when δ ̸= 0. The following result is
needed for establishing our IMs.
Theorem 1. Let X1, . . . , Xn be identically distributed with stochastic represen-
tations given in (2). Let the sample mean and sample variance be
¯X = 1
n
n

i=1
Xi
and
S2 =
1
n −1
n

i=1

Xi −¯X
2 ,
respectively. Then ¯X and S2 are independent,
¯X ∼SN

μ, nλ2 + 1
n (1 + λ2)σ2, √nλ

and
(n −1) S2
σ2 (1 −δ2) ∼χ2
n−1(0).
Proof. From the stochastic representations (2), it is easy to obtain
¯X = μ + σ

δ|Z0| +

1 −δ2 ¯Z
	
and
S2 = σ2 
1 −δ2
n −1
n

i=1

Zi −¯Z
2 ,
where ¯Z =
1
n

n
i=1 Zi. Note that ¯Z and (Z1 −¯Z)’s are independent so that
¯X and S2 are independent. For the distribution of ¯X, let Z∗= √n ¯Z, which is
distributed as N(0, 1). Then
¯X = μ + σ

δ|Z0| +

1 −δ2
n
Z∗

= μ + σ∗

δ∗|Z0| +

1 −δ2∗Z∗
	
,
where δ∗=
√nδ
√
1+(n−1)δ2 and σ∗= σ

1+(n−1)δ2
n
. Thus by deﬁnition, we have
¯X ∼SN(μ, σ2
∗, λ∗), where
λ∗=
δ∗

1 −δ2∗
=
√nδ
√
1 −δ2 = √nλ,
the distribution of ¯X is obtained. The distribution of S2 can be obtained directly
from
(n −1)S2
σ2(1 −δ2) =
n

i=1
(Zi −¯Z)2 ∼χ2
n−1(0).
□

Joint Plausibility Regions for Parameters of Skew Normal Family
237
3.1
Plausibility Function and Plausibility Region of (μ, σ) When δ
Is Known
Assume that the skewness parameter λ (or δ) is known. We want to construct the
plausibility region for unknown parameters (μ, σ) based on a sample X1, . . . , Xn
from a skew normal population.
A-step. From Theorem 1, we can have associations
(n −1)S2
σ2(1 −δ2) = F −1
n−1(U1)
and
¯X = μ + G−1(U2),
(3)
where Fn−1(·) and G(·) are the cdf’s of χ2
n−1(0) and SN

0,
nλ2+1
n(1+λ2)σ2, √nλ
	
,
respectively, and U1, U2 are independent uniformly distributed in interval (0, 1).
Thus for any observations ¯x and s2, and u1, u2 ∈(0, 1), we have the solution set
Θ(¯x,s2)(μ, σ) =

(μ, σ) : ¯x = μ + G−1 (u2) , (n −1) s2
σ2(1 −δ2) = F −1
n−1 (u1)

=

(μ, σ) : G (¯x −μ) = u2, Fn−1
 (n −1) s2
σ2(1 −δ2)

= u1

.
P-step. To predict auxiliary variables U1 and U2, we use the default predictive
random set
S(U1, U2) = {(u1, u2) : max{|u1 −0.5|, |u2 −0.5|} ≤max{|U1 −0.5|, |U2 −0.5|}}.
C-step. By the P-step, we have the combined set
Θ(¯x,S2) (S) =

(μ, σ) : max
G (¯x −μ) −0.5
,
Fn−1
 (n −1) s2
σ2 (1 −δ2)

−0.5


≤max
U1 −0.5
,
U2 −0.5


.
Theorem 2. For any singleton assertion A = {(μ, σ)},
bel(¯x,s2)(A; S) = 0,
pl(¯x,s2)(A; S) = 1 −max
2G (¯x −μ) −1
,
2Fn−1
 (n −1)s2
σ2(1 −δ2)

−1

2
,
and the 100(1 −α)% plausibility region
Π ¯
X,S2(μ, σ) = {(μ, σ) : pl(¯x,s2)(μ, σ) ≥α}.

238
Z. Ma et al.
Proof. It is clear that {Θ(¯x,s2)(S) ⊆A} = ∅, so bel(¯x,s2)(A; S) = 0.
pl(¯x,s2) (A; S) = 1 −bel(¯x,s2) (Ac; S) = 1 −PS

Θ(¯x,S2) (S) ⊆Ac
= 1 −max
2G (¯x −μ) −1
,
2Fn−1

(n−1)s2
σ2(1−δ2)
	
−1

2
.
Thus we can obtain the 100(1 −α)% plausibility region by its deﬁnition.
□
The following example is used for the illustration of Theorem 2.
Example 3.1. For a sample X1, · · · , Xn from skew normal population as
described above, graphs of plausibility function and the 95% plausibility region of
(μ, σ) are listed in Figs. 1, 2 and 3.
Fig. 1. Graphs of plausibility function and the 95% plausibility region of (μ, σ) based
on simulated data with μ = 1, σ = 1, δ = 1/
√
2 and n = 10.
3.2
Plausibility Function and Plausibility Region for (μ, δ) When σ
Is Known
Assume that the scale parameter σ is known. We want to construct the plausi-
bility region for unknown parameters (μ, δ) based a sample X1, . . . , Xn from a
skew normal population.
A-step. From Theorem 1, we obtain the associations
(n −1)S2
σ2(1 −δ2) = F −1
n−1(U1)
and
¯X = μ + G−1(U2),
(4)

Joint Plausibility Regions for Parameters of Skew Normal Family
239
Fig. 2. Graphs of plausibility function and the 95% plausibility region of (μ, σ) based
on simulated data with μ = 1, σ = 1, δ = 1/
√
2 and n = 20.
Fig. 3. Graphs of plausibility function and the 95% plausibility region of (μ, σ) based
on simulated data with μ = 1, σ = 1, δ = 1/
√
2 and n = 50.

240
Z. Ma et al.
where Fn−1(·) and G(·) are cdf’s of χ2
n−1 and SN

0,
nλ2+1
n(1+λ2)σ2, √nλ
	
respec-
tively, and U1, U2 are independent uniformly distributed in interval (0, 1). So for
any observations ¯x and s2, and u1, u2 ∈(0, 1), we have the solution set
Θ(¯x,s2)(μ, δ) =

(μ, δ) : ¯x = μ + G−1 (u2) , (n −1) s2
σ2(1 −δ2) = F −1
n−1 (u1)

=

(μ, δ) : G (¯x −μ) = u2, Fn−1
 (n −1) s2
σ2(1 −δ2)

= u1

.
Note that to guarantee the solution set Θ(¯x,s2)(μ, δ) ̸= ∅, we need u1 ≥
F

(n−1)s2
σ2
	
.
P-step. To predict auxiliary variables U1 and U2, we should use an elastic predic-
tive random set (see Martin and Liu [12] Chap. 5) as follow. If Fn−1

(n−1)s2
σ2
	
≤
1
2, we take
S(U1, U2) = {(u1, u2) : max {|u1 −0.5|, |u2 −0.5|} ≤max {|U1 −0.5|, |U2 −0.5|}},
otherwise, we take
S(U1, U2) = {(u1, u2) : max
Fn−1
(n −1) s2
σ2

−0.5
,
u1 −0.5
,
u2 −0.5


≤max
U1 −0.5
, |U2 −0.5|

C-step. By P-step, we have the combined set in two cases. If Fn−1

(n−1)s2
σ2
	
≤
1
2, then
Θ(¯x,S2) (S) =

(μ, δ) : max
G (¯x −μ) −0.5
,
Fn−1
 (n −1) s2
σ2 (1 −δ2)

−0.5


≤max
U1 −0.5
,
U2 −0.5


.
Otherwise,
Θ(¯x,S2) (S) =

(μ, δ) : max
⎧
⎨
⎩
G (¯x −μ) −0.5
,
Fn−1

(n−1)s2
σ2(1−δ2)
	
−0.5
,
Fn−1

(n−1)s2
σ2
	
−0.5

⎫
⎬
⎭
≤max
U1 −0.5
,
U2 −0.5


.
Theorem 3. For any singleton assertion A = {(μ, δ)},
bel(¯x,s2)(A; S) = 0,

Joint Plausibility Regions for Parameters of Skew Normal Family
241
If Fn−1

(n−1)s2
σ2
	
≤1
2, then
pl(¯x,s2) (A; S) = 1 −max
2G (¯x −μ) −1
,
2Fn−1
 (n −1) s2
σ2(1 −δ2)

−1

2
,
Otherwise,
pl(¯x,s2) (A; S) = 1 −max
⎧
⎨
⎩
2G (¯x −μ) −1
,
2Fn−1

(n−1)s2
σ2(1−δ2)
	
−1
,
2Fn−1

(n−1)s2
σ2
	
−1

⎫
⎬
⎭
2
,
and the 100(1 −α)% plausibility region
Π ¯
X,S2(μ, δ) = {(μ, δ) : pl(¯x,s2)(μ, δ) ≥α}.
Proof. It is clear that {Θ(¯x,s2)(S) ⊆A} = ∅, so bel(¯x,s2)(A; S) = 0. By deﬁnition
of plausibility function, we can compute plausibility in two cases.
If Fn−1

(n−1)s2
σ2
	
≤1
2, we have
pl(¯x,s2) (A; S) = 1 −bel(¯x,s2) (Ac; S) = 1 −PS

Θ(¯x,S2) (S) ⊆Ac
= 1 −max
2G (¯x −μ) −1
,
2Fn−1

(n−1)s2
σ2(1−δ2)
	
−1

2
.
Otherwise,
pl(¯x,s2) (A; S) = 1 −bel(¯x,s2) (Ac; S) = 1 −PS

Θ(¯x,S2) (S) ⊆Ac
= 1 −max
⎧
⎨
⎩
2G (¯x −μ) −1
,
2Fn−1

(n−1)s2
σ2(1−δ2)
	
−1
,
2Fn−1

(n−1)s2
σ2
	
−1

⎫
⎬
⎭
2
.□
Remark. Note that plausibility functions in Theorems 2 and 3 are the same
because we use the same association, but the plausibility regions are diﬀer-
ent, because in Theorem 2, we use this plausibility function to solve plausibility
regions for (μ, σ), while in Theorem 3, we use this plausibility function to obtain
plausibility regions for (μ, δ).
Similarly, the following example is used for the illustration of Theorem 3.
Example 3.2. For a sample X1, · · · , Xn from skew normal population as
described above, graphs of plausibility function and the 95% plausibility region of
(μ, δ) are given in Figs. 4, 5 and 6 for sample sizes n = 10, 20 and 50, respec-
tively.

242
Z. Ma et al.
Fig. 4. Graphs of plausibility function and the 95% plausibility region of (μ, δ) based
on simulated data with μ = 1, σ = 1, δ = 1/
√
2 and n = 10.
Fig. 5. Graphs of plausibility function and the 95% plausibility region of (μ, δ) based
on simulated data with μ = 1, σ = 1, δ = 1/
√
2 and n = 20.

Joint Plausibility Regions for Parameters of Skew Normal Family
243
Fig. 6. Graphs of plausibility function and the 95% plausibility region of (μ, δ) based
on simulated data with μ = 1, σ = 1, δ = 1/
√
2 and n = 50.
4
Simulation Studies
In this section, we perform two simulation studies on coverage probabilities of
95% plausibility regions for skew normal population of two cases discussed above.
4.1
Coverage Probabilities of the 95% Plausibility Regions
for (μ, σ) When δ Is Known
When δ = 1/
√
2, we choose sample sizes of 10, 20 and 50 and simulate 10,000
runs for diﬀerent parameters, which is given in Table 1.
Table 1. Simulation results of coverage probabilities of the 95% plausibility regions
for (μ, σ) when δ = 1/
√
2.
n
μ = 0
μ = 1
σ = 0.1 σ = 0.5 σ = 1
σ = 2
σ = 0.1 σ = 0.5 σ = 1
σ = 2
10 0.9500
0.9449
0.9493 0.9503 0.9489
0.9540
0.9479 0.9513
20 0.9472
0.9524
0.9514 0.9493 0.9481
0.9502
0.9510 0.9463
50 0.9521
0.9537
0.9482 0.9499 0.9506
0.9485
0.9487 0.9507
4.2
Coverage Probabilities of 95% Plausibility Regions for (μ, δ)
When σ Is Known
When σ = 1, we choose sample sizes of 10, 20 and 50 and run the simulation
10,000 times. Simulation results of coverage probabilities of the 95% plausibility
regions for (μ, δ) is listed in Table 2.

244
Z. Ma et al.
Table 2. Simulation results of coverage probabilities of the 95% plausibility regions
for (μ, δ) when σ = 1.
n
μ = 0
μ = 1
δ = 0.2 δ = 0.4 δ = 0.7 δ = 0.9 δ = 0.2 δ = 0.4 δ = 0.7 δ = 0.9
10 0.9495
0.9490
0.9518
0.9484
0.9502
0.9524
0.9513
0.9520
20 0.9487
0.9488
0.9493
0.9524
0.9515
0.9446
0.9481
0.9479
50 0.9513
0.9513
0.9508
0.9472
0.9489
0.9487
0.9540
0.9527
Acknowledgments. Authors would like to thank Professor Hung T. Nguyen for intro-
ducing this interesting and hot research topic to us. Also we would like to thank referee’s
valuable comments which led to improvement of this paper.
References
1. Aronld, B.C., Beaver, R.J., Groenevld, R.A., Meeker, W.Q.: The nontruncated
marginal of a truncated bivariate normal distribution. Psychometrica 58(3), 471–
488 (1993)
2. Azzalini, A.: A class of distributions which includes the normal ones. Scand. J.
Stat. 12(2), 171–178 (1985)
3. Azzalini, A.: Further results on a class of distributions which includes the normal
ones. Statistica 46(2), 199–208 (1986)
4. Azzalini, A., Capitanio, A.: Statistical applications of the multivariate skew normal
distribution. J. R. Stat. Soc. 61(3), 579–602 (1999)
5. Azzalini, A., Capitanio, A.: The Skew-Normal and Related Families, vol. 3.
Cambridge University Press, Cambridge (2013)
6. Azzalini, A., Dalla Valle, A.: The multivariate skew-normal distribution. Bio-
metrika 83(4), 715–726 (1996)
7. Dey, D. Estimation of the parameters of skew normal distribution by approxi-
mating the ratio of the normal density and distribution functions. Ph.D. thesis,
University of California Riverside (2010)
8. Hill, M., Dixon, W.J.: Robustness in real life: a study of clinical laboratory data.
Biometrics 38, 377–396 (1982)
9. Liseo, B., Loperﬁdo, N.: A note on reference priors for the scalar skew-normal
distribution. J. Stat. Plan. Inference 136(2), 373–389 (2006)
10. Martin, R., Liu, C.: Inferential models: a framework for prior-free posterior prob-
abilistic inference. J. Am. Stat. Assoc. 108(501), 301–313 (2013)
11. Martin, R.: Random sets and exact conﬁdence regions. Sankhya A 76(2), 288–304
(2014)
12. Martin, R., Liu, C.: Inferential models: reasoning with uncertainty. In: Monographs
on statistics and Applied Probability, vol. 145. CRC Press (2015)
13. Martin, R., Lingham, R.T.: Prior-free probabilistic prediction of future observa-
tions. Technometrics 58(2), 225–235 (2016)
14. Mameli, V., Musio, M., Sauleau, E., Biggeri, A.: Large sample conﬁdence intervals
for the skewness parameter of the skew-normal distribution based on Fisher’s
transformation. J. Appl. Stat. 39(8), 1693–1702 (2012)

Joint Plausibility Regions for Parameters of Skew Normal Family
245
15. Pewsey, A.: Problems of inference for Azzalini’s skewnormal distribution. J. Appl.
Stat. 27(7), 859–870 (2000)
16. Sartori, N.: Bias prevention of maximum likelihood estimates for scalar skew-
normal and skew-t distributions. J. Stat. Plan. Inference 136(12), 4259–4275
(2006)
17. Wang, T., Li, B., Gupta, A.K.: Distribution of quadratic forms under skew normal
settings. J. Multivar. Anal. 100(3), 533–545 (2009)
18. Ye, R., Wang, T., Gupta, A.K.: Distribution of matrix quadratic forms under
skew-normal settings. J. Multivar. Anal. 131, 229–239 (2014)
19. Zhu, X., Ma, Z., Wang, T., Teetranont, T.: Plausibility regions on the skewness
parameter of skew normal distributions based on inferential models. In: Robust-
ness in Econometrics, pp. 267–286. Springer (2017)

On Parameter Change Test for ARMA Models
with Martingale Diﬀerence Errors
Haejune Oh and Sangyeol Lee(B)
Department of Statistics, Seoul National University, Seoul 08826, Korea
sylee@stats.snu.ac.kr
Abstract. This study considers the CUSUM test for ARMA models
with stationary martingale diﬀerence errors. CUSUM tests are widely
used for detecting abrupt changes in time series models. Although they
perform adequately in general, their performance is occasionally unsatis-
factory in ARMA models. This motivates us to design a new test that can
simultaneously detect the ARMA parameter and variance changes. Its
null limiting distribution is derived under regularity conditions. Monte
Carlo simulations conﬁrm the validity of the proposed test.
Keywords: ARMA models · Parameter change test · CUSUM test
Martingale diﬀerence errors
1
Introduction
This study considers the parameter change test for ARMA models with martin-
gale diﬀerence errors. Since [11], the problem of testing for a parameter change
has been an important issue in many applied ﬁelds such as economics, engineering
and medicine. The change point problem has been a core issue among researchers
since time series often suﬀer from structural changes owing to changes in policy
and critical social events. It is widely known that detecting a change point is
crucial for further inferences and ignoring it can lead to a false conclusion. The
literature on the change point test for time series models is quite exhaustive. [5]
consider to use the CUSUM of squares test to detect multiple changes of vari-
ance of independent samples. Since then, numerous studies have been devoted
to the CUSUM tests for time series models: see [7] and the papers cited therein
for earlier works.
Conventionally, the CUSUM test is constructed based on parameter esti-
mates, score vectors, and residuals. Although the estimates-based test appeals
much to practitioners, it often has severe size distortions as seen in [8,9]. The
score vector-based test circumvents this phenomenon to a great extent in ARMA-
GARCH models: see [1,3,10], who consider the score vector-based CUSUM test
This research is supported by Basic Science Research Program through the National
Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT and
future Planning (No. 2015R1A2A2A010003894).
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_17

On Parameter Change Test for ARMA Models
247
in GARCH and AR models. However, this test cannot completely resolve the
problem, and as an alternative, the residual-based test has been used as in [6].
Nevertheless, the test has a serious defect not to be able to detect a change of
ARMA parameters (and location parameters within a more general framework).
Hence, in this study, we intend to design a test that can simultaneously detect
the ARMA parameter and variance changes in ARMA models with stationary
martingale diﬀerence errors that accommodate a broad class of ARMA-GARCH
type models. In particular, the variance change test turns out to be useful to
detect a change in GARCH parameters when the errors follow a GARCH model:
see Sect. 3.
The idea in our CUSUM set-up is to use the multiplications of the ARMA
part and error terms in the construction of the CUSUM test. We do this because
those terms form martingale diﬀerences under the null hypothesis, which, hence-
forth, leads to a desired Brownian bridge result, used for calculating critical
values, whereas their values before and after the change point are signiﬁcantly
diﬀerent and contribute to producing good powers. Compared with the score
vector-based CUSUM test, this test has merit to be a lot simpler in implemen-
tation, has less size distortions, and produces good powers.
The organization of this paper is as follows. Section 2 introduces a new
CUSUM test and investigates its limiting null distributions. Furthermore, the
asymptotic behavior of the CUSUM test under alternatives is investigated.
Section 3 conducts a simulation study and demonstrates the validity of the pro-
posed test. Sections 4 and 5 provide the proofs and concluding remarks.
2
CUSUM Test for ARMA Models
Let us consider the ARMA(p,q) model:
yt =
p

i=1
αiyt−1 + ϵt +
q

j=1
βjϵt−j,
(1)
where {ϵt} is either a sequence of i.i.d. random variables with zero mean and
ﬁnite variance or strictly stationary martingale diﬀerence sequence (m.d.s.). We
set θ = (α1, . . . , αp, β1, . . . , βq)T ⊂Θ ∈Rp+q.
In what follows, we assume the following conditions for the stationarity and
invertibility of {yt}:
(A1) For all θ ∈Θ, A(z)B(z) = 0 implies |z| > 1,
where A(z) = 1 −p
i=1 αizi and B(z) = 1 + q
j=1 βjzj.
(A2) A(z) and B(z) have no common roots.
Assume that y−p+1, y−p+2, . . . , y0, y1, . . . , yn are observed and one wishes to
test the following hypotheses:
H0 : The parameter θ does not change. vs.
(2)
H1 : not H0.

248
H. Oh and S. Lee
For a test, we propose the CUSUM test based on {(yt −ϵt)ϵt} as follows:
T θ
n :=
1
√nκ max
1≤k≤n

k

t=1
(yt −ϵt)ϵt −k
n
n

t=1
(yt −ϵt)ϵt
 ,
where κ2 = E

(yt−ϵt)2ϵ2
t

. Notice that {(yt−ϵt)ϵt} forms a martingale diﬀerence
sequence under the null hypothesis, which leads us to the weak convergence result
as shown in Theorem 1. Under the alternative, T θ
n is anticipated to have a large
value owing to the variation change of terms yt −ϵt, rather than ϵt themselves.
Because ϵt are unobservable, we approximate them with observable {ˆϵt}
which are the processes deﬁned recursively by
ˆϵt = yt −
p

i=1
ˆαiyt−i −
q

j=1
ˆβjˆϵt−j,
where ˆα1, . . . , ˆαp and ˆβ1, . . . , ˆβq are the estimators of α1, . . . , αp and β1, . . . , βq,
respectively. The initial values of ˆϵ−q+1, . . . , ˆϵ0 are set to zero when q > 0.
Below we impose some regularity conditions:
(A3) Under H0, √n(ˆαi −αi) = OP(1), i = 1, . . . , p, and √n(ˆβj −βj) = OP(1),
j = 1, . . . , q.
Then, we have the following result.
Theorem 1. Suppose that (A1)–(A3) and the following conditions hold:
(E1) {ϵt} is an i.i.d. sequence with zero mean, variance σ2 and Eϵ4
t < ∞, or
(E2) {ϵt} is a strongly mixing m.d.s. with (4 + δ)-th moment for some δ > 0
and mixing-order α(·) satisfying
E(ϵt|Ft−1) = 0,
Eϵ2
t = σ2,
1
n
n

t=1
E(ϵ2
t |Ft−1) →σ2,
1
n E

n

t=1
(ϵ2
t −σ2)
2
→ν2 > 0,
where Ft is the σ-ﬁeld generated by ϵi, i ≤t and 
k α(k)δ/(4+δ) < ∞.
Then, under H0, as n →∞,
ˆT θ
n :=
1
√nˆκ max
1≤k≤n

k

t=1
(yt −ˆϵt)ˆϵt −k
n
n

t=1
(yt −ˆϵt)ˆϵt

w
−→sup
0≤s≤1
|W◦
1(s)|,
where ˆκ2 =
1
n
n
t=1(yt −ˆϵt)2ˆϵ2
t −

1
n
n
t=1(yt −ˆϵt)ˆϵt
2
and W◦
1(·) denotes a
Brownian bridge: see [2] for its deﬁnition.
We reject H0 if ˆT θ
n ≥Cα at the nominal level α, where Cα is the 100 (1 −α)
quantile value of sup0≤s≤1 ∥W◦
1(s)∥. The critical values for α = 0.01, 0.05, 0.10
are provided in Table 1 in [7].

On Parameter Change Test for ARMA Models
249
Next, suppose that one wishes to test the following hypotheses:
H0 : σ2 does not change. vs.
(3)
H1 : not H0.
In this case, the ˆT θ
n cannot detect a variance change eﬃciently, and we consider
the CUSUM test based on {ϵ2
t}:
T σ
n :=
1
√nν max
1≤k≤n

k

t=1
ϵ2
t −k
n
n

t=1
ϵ2
t
 ,
where ν2 = E(ϵ2
0 −σ2)2 + 2 ∞
t=1 E(ϵ2
0 −σ2)(ϵ2
t −σ2).
Then, we have the following result.
Theorem 2. Suppose that (A1)–(A3) hold and either (E1) or (E2) hold.
Then, under H0, as n →∞,
ˆT σ
n :=
1
√nˆν max
1≤k≤n

k

t=1
ˆϵ2
t −k
n
n

t=1
ˆϵ2
t

w
−→sup
0≤s≤1
|W◦
1(s)|,
where ˆν2
=
1
n
n
t=1

ˆϵ2
t −1
n
n
t=1 ˆϵ2
t
2 + 2 1
n
l
k=1
n
t=k+1

ˆϵt −1
n
n
t=1 ˆϵ2
t


ˆϵt−k −1
n
n
t=1 ˆϵ2
t

with l := ln →∞and l = o(n1/4) as n →∞(cf. [12]).
Finally, suppose that one is interested in testing for a change in θ and σ
simultaneously. Then, we set up the hypotheses:
H0 : (θ, σ2) does not change. vs.
(4)
H1 : not H0,
and consider the test:
ˆTn = max
1≤k≤n
1
n
	 k

t=1
ˆUt −k
n
n

t=1
ˆUt

T
ˆΣ−1
	 k

t=1
ˆUt −k
n
n

t=1
ˆUt

,
where ˆUt =

(yt −ˆϵt)ˆϵt, ˆϵ2
t
T
and ˆΣ =
 ˆκ 0
0 ˆν

.
The following theorem can be proven similarly to Theorems 1 and 2 and is
omitted for brevity.
Theorem 3. Suppose that (A1)–(A3) hold and either (E1) or (E2) hold.
Then, under H0, as n →∞,
ˆTn
w
−→sup
0≤s≤1
∥W◦
2(s)∥2,
where W◦
2(·) denotes a 2-dimensional Brownian bridge.
Compared to the score vector-based CUSUM test, ˆTn is easier to use in
practice, because the dimension of the CUSUM test increases proportional to the
number of unknown parameters, whereas it is ﬁxed in our test. Our simulation
study in the next section conﬁrms the validity of the test, outperforming both
ˆT θ
n and ˆT σ
n .

250
H. Oh and S. Lee
3
Simulation Study
In this section, we evaluate the performance of the proposed CUSUM tests for
the ARMA(1,1) model with GARCH(1,1) errors:
yt = αyt−1 + ϵt −βϵt−1,
(5)
ϵt =

htηt,
ht = w + aϵ2
t−1 + bht−1,
where ηt
are assumed to follow N(0, 1),

4/5t(10), or 0.2N(1.6, 1) +
0.8N(−0.4, 0.2) (NM hereafter) distributions.
We consider the problem of testing the hypotheses:
H0 : The true parameter θ = (α, β) and σ2 does not change over y1, . . . , yn. vs.
H1 : θ change to θ′ = (α′, β′) or σ2 change to σ2′ at [n/2].
Note that Eϵ2
t = σ2 =
w
1−a−b for a stationary GARCH(1,1) model. For
observations with n = 300 and 500 generated from models (5), we calculate the
empirical sizes and powers for speciﬁc set-ups in (θ, σ2) with 500 repetitions,
summarized in Tables 1, 2 and 3.
The result shows that the estimate-based CUSUM test ˆT E
n appears to have
severe size distortions even when the sample size is moderate, while the residual-
based CUSUM tests have no severe size distortions. As anticipated, ˆT σ
n and ˆT θ
n
perform poorly for the change of ARMA parameters α and β and the variance σ2,
respectively, whereas ˆTn can eﬀectively detect all changes. It can be also observed
that the diﬀerent types of innovation distributions have little inﬂuence on the
result. Overall, our ﬁndings show that ˆTn makes a stable test and outperforms
ˆT θ
n and ˆT σ
n in terms of power.
4
Proofs
The lemma below can be easily proven in a manner similar to Theorem 2 of [13],
and the proof is omitted for brevity.
Lemma 1. Under the same conditions in Theorem 1, we have that for j = 1, 2,
max
1≤k≤n

1
√n
k

t=1
ˆϵj
t −
1
√n
k

t=1
ϵj
t
 = oP(1).
Moreover, we have
max
1≤k≤n

1
√n
k

t=1
yt(ˆϵt −ϵt) −
1
√n
k

t=1
yt(ˆϵt −ϵt)
 = oP(1).

On Parameter Change Test for ARMA Models
251
Table 1. Empirical sizes and powers for the ARMA(1,1) model with GARCH(1,1)
errors and ηt ∼N(0, 1)
(α, β, w, a, b) nominal level
n = 300
n = 500
0.05
0.10
0.05
0.10
size (0.3, 0.2, 0.1, 0.1, 0.2)
ˆTn
0.034 0.074 0.048 0.096
ˆT θ
n
0.046 0.070 0.046 0.088
ˆT σ
n
0.032 0.062 0.058 0.118
ˆT E
n
0.204 0.292 0.146 0.210
α
ˆTn
0.826 0.892 0.968 0.988
0.3 −→0.7
ˆT θ
n
0.882 0.936 0.982 0.990
ˆT σ
n
0.028 0.058 0.038 0.070
ˆT E
n
0.962 0.976 0.992 0.996
β
ˆTn
0.494 0.614 0.770 0.852
0.2 −→0.6
ˆT θ
n
0.586 0.690 0.846 0.914
ˆT σ
n
0.042 0.074 0.038 0.082
ˆT E
n
0.896 0.934 0.976 0.982
w
ˆTn
1.000 1.000 1.000 1.000
0.1 −→0.5
ˆT θ
n
0.044 0.060 0.040 0.084
ˆT σ
n
1.000 1.000 1.000 1.000
a
ˆTn
0.380 0.524 0.674 0.798
0.1 −→0.5
ˆT θ
n
0.022 0.050 0.040 0.072
ˆT σ
n
0.544 0.672 0.820 0.886
b
ˆTn
0.932 0.976 1.000 1.000
0.2 −→0.6
ˆT θ
n
0.038 0.078 0.054 0.106
ˆT σ
n
0.976 0.990 1.000 1.000
Proof of Theorem 1. We express
(yt −ˆϵt)ˆϵt = (yt −ϵt)ϵt +

yt(ˆϵt −ϵt) −(ˆϵ2
t −ϵ2
t)

.
Since {(yt −ϵt)ϵt, Ft} is a square integrable martingale diﬀerence sequence,
Theorem 23.1 of [2] implies that Tθ
n converges weakly to sup0≤s≤1 |W◦
1(s)|.
Hence, owing to Lemma 1,
1
√nκ max
1≤k≤n

k

t=1
(yt −ˆϵt)ˆϵt −k
n
n

t=1
(yt −ˆϵt)ˆϵt

w
−→sup
0≤s≤1
|W◦
1(s)|.
Since ˆκ →κ in probability, the theorem is asserted.
⊓⊔

252
H. Oh and S. Lee
Table 2. Empirical sizes and powers for the ARMA(1,1) model with GARCH(1,1)
errors and ηt ∼

4/5t(10)
(α, β, w, a, b) nominal level
n = 300
n = 500
0.05
0.10
0.05
0.10
size (0.3, 0.2, 0.1, 0.1, 0.2)
ˆTn
0.042 0.076 0.044 0.078
ˆT θ
n
0.032 0.062 0.024 0.062
ˆT σ
n
0.048 0.094 0.044 0.084
ˆT E
n
0.224 0.288 0.216 0.278
α
ˆTn
0.744 0.842 0.920 0.956
0.3 −→0.7
ˆT θ
n
0.828 0.878 0.956 0.964
ˆT σ
n
0.018 0.044 0.026 0.068
ˆT E
n
0.952 0.970 0.994 0.998
β
ˆTn
0.398 0.520 0.666 0.776
0.2 −→0.6
ˆT θ
n
0.504 0.648 0.754 0.832
ˆT σ
n
0.016 0.044 0.028 0.084
ˆT E
n
0.852 0.896 0.968 0.974
w
ˆTn
0.902 0.932 0.956 0.972
0.1 −→0.5
ˆT θ
n
0.026 0.054 0.040 0.064
ˆT σ
n
0.944 0.962 0.974 0.978
a
ˆTn
0.134 0.224 0.272 0.422
0.1 −→0.5
ˆT θ
n
0.020 0.038 0.034 0.058
ˆT σ
n
0.236 0.366 0.388 0.536
b
ˆTn
0.548 0.682 0.788 0.854
0.2 −→0.6
ˆT θ
n
0.034 0.074 0.038 0.082
ˆT σ
n
0.668 0.752 0.852 0.902
Proof of Theorem 2. Corollary 1 of [4] implies that Tσ
n converges weakly to
sup0≤s≤1 |W◦
1(s)|. Hence, owing to Lemma 1, we have
1
√nν max
1≤k≤n

k

t=1
ˆϵ2
t −k
n
n

t=1
ˆϵ2
t

w
−→sup
0≤s≤1
|W◦
1(s)|.
Moreover, using Lemma 1 and Theorem 4.2 of [12], we can easily see that ˆν is
consistent. This asserts the theorem.
⊓⊔

On Parameter Change Test for ARMA Models
253
Table 3. Empirical sizes and powers for the ARMA(1,1) model with GARCH(1,1)
errors and ηt ∼NM
(α, β, w, a, b) nominal level
n = 300
n = 500
0.05
0.10
0.05
0.10
size (0.3, 0.2, 0.1, 0.1, 0.2)
ˆTn
0.024 0.072 0.046 0.078
ˆT θ
n
0.024 0.064 0.030 0.052
ˆT σ
n
0.030 0.088 0.054 0.088
ˆT E
n
0.212 0.266 0.188 0.260
α
0.3 −→0.7
ˆTn
0.944 0.964 0.944 0.964
ˆT θ
n
0.954 0.974 0.954 0.974
ˆT σ
n
0.042 0.082 0.042 0.082
ˆT E
n
0.946 0.962 0.998 0.998
β
0.2 −→0.6
ˆTn
0.376 0.502 0.652 0.730
ˆT θ
n
0.470 0.590 0.720 0.784
ˆT σ
n
0.038 0.064 0.032 0.072
ˆT E
n
0.814 0.864 0.940 0.970
w
0.1 −→0.5
ˆTn
0.978 1.000 1.000 1.000
ˆT θ
n
0.024 0.056 0.018 0.048
ˆT σ
n
1.000 1.000 1.000 1.000
a
0.1 −→0.5
ˆTn
0.122 0.200 0.232 0.380
ˆT θ
n
0.038 0.046 0.014 0.036
ˆT σ
n
0.192 0.314 0.368 0.514
b
0.2 −→0.6
ˆTn
0.560 0.694 0.820 0.900
ˆT θ
n
0.024 0.060 0.038 0.056
ˆT σ
n
0.704 0.798 0.908 0.954
5
Concluding Remarks
In this study, we proposed a new CUSUM test which is handier to use in practice
and performs adequately for ARMA models with martingale diﬀerence innova-
tions. We derived its limiting null distribution and demonstrated its validity
through Monte carlo simulations. In particular, the models under considera-
tion accommodate ARMA-GARCH type models, and our test is applicable to
detecting the change of GARCH parameters as well as ARMA parameters when
dealing with ARMA-GARCH models. In fact, this problem can be handled in

254
H. Oh and S. Lee
more general settings such as location-scale time series models, which is our
on-going project.
Acknowledgements. We thank one anonymous referee for his/her careful reading
and valuable comments.
References
1. Berkes, I., Horv´ath, L., Kokoszka, P.: Testing for parameter constancy in GARCH
(p, q) models. Stat. Probab. Lett. 70(4), 263–273 (2004)
2. Billingsley, P.: Convergence of Probability Measures. Wiley, New York (1968)
3. Gombay, E.: Change detection in autoregressive time series. J. Multivar. Anal.
99(3), 451–464 (2008)
4. Herrndorf, N.: A functional central limit theorem for weakly dependent sequences
of random variables. Ann. Probab. 12, 141–153 (1984)
5. Incl´an, C., Tiao, G.C.: Use of cumulative sums of squares for retrospective detection
of changes of variance. J. Am. Stat. Assoc. 89(427), 913–923 (1994)
6. Lee, J., Lee, S.: Parameter change test for nonlinear time series models with
GARCH type errors. J. Korean Math. Soc. 52, 503–522 (2015)
7. Lee, S., Ha, J., Na, O., Na, S.: The cusum test for parameter change in time series
models. Scand. J. Stat. 30(4), 781–796 (2003)
8. Lee, S., Oh, H.: Parameter change test for autoregressive conditional duration
models. Ann. Inst. Stat. Math. 68(3), 621–637 (2016)
9. Lee, S., Song, J.: Test for parameter change in ARMA models with GARCH inno-
vations. Stat. Probab. Lett. 78(13), 1990–1998 (2008)
10. Oh, H., Lee, S.: On score vector- and residual-based CUSUM tests in ARMA-
GARCH models. Stat. Methods Appl. 1–22 (2017). https://doi.org/10.1007/
s10260-017-0408-9
11. Page, E.S.: A test for a change in a parameter occurring at an unknown point.
Biometrika 42(3/4), 523–527 (1955)
12. Phillips, P.C.B.: Time series regression with a unit root. Econometrica 55, 277–301
(1987)
13. Yu, H.: High moment partial sum processes of residuals in ARMA models and their
applications. J. Time Ser. Anal. 28(1), 72–91 (2007)

Agent-Based Modeling of Economic Instability
Akira Namatame(B)
National Defense Academy, Yokosuka, Japan
akiranamatame@gmail.com
Abstract. Networks increase interdependence, which creates challenges
for managing economic risks. This is especially apparent in areas such as
ﬁnancial institutions and enterprise risk management, where the actions
of a single agent (ﬁrm or bank) can impact all the other agents in inter-
connected networks. In this paper, we use agent-based modeling (ABM)
in order to analyze how local defaults of supply chain participants propa-
gate through the dynamic supply chain network and interbank networks
and form avalanches of bankruptcy. We focus on the linkage dependence
among agents at the micro-level and estimate the impact on the macro
activities. Combining agent-based modeling with the network analysis
can shed light on understanding the primary role of banks in lending to
the wider real economy. Understanding the linkage dependency among
ﬁrms and banks can help in the design of regulatory paradigms that rein
in systemic risk while enhancing economic growth.
Keywords: Agent-based economics · Systemic risk
Evolving credit networks · Financial networks
1
Introduction
Macro economy has created well deﬁned approaches and several tools that
seemed to serve us for the last decades. However, recent economic ﬂuctuations
and ﬁnancial crises emphasize the need of alternative frameworks and method-
ologies to be able to replicate such phenomena in order to a deeper understanding
the mechanism of economic crisis and ﬂuctuation. Financial markets are driven
by the real economy and in turn they also have a profound eﬀect on it. Under-
standing the feedback between these two sectors leads to a deeper understanding
of the stability, robustness and eﬃciency of the economic system [8]. Agent-based
approaches in Macroeconomics from bottom-up are getting more and more atten-
tion recently [13]. Research on this line has been initiated by the series of papers
by Delli Gatti et al. [4,5]. Their model simulating the behavior of interacting
heterogeneous agents (ﬁrms and banks) is able to generate a large number of
stylized facts. To jointly account for an ensemble of the facts regarding both
micro-macro properties together with macro aggregates including GDP growth
rates, output volatility, business cycle phases, ﬁnancial fragility, and bankruptcy
cascades, agent-based approaches are getting more and more attention recently.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_18

256
A. Namatame
Historically ﬁnancial markets were driven by the real economy and in turn
they also had a profound eﬀect on it. In recent decades, a massive transfer of
resources from the productive sector to the ﬁnancial sector has been one of the
characteristics of global economic systems. This process is mainly responsible for
the growing ﬁnancial instability [9,14]. In production sectors, there has been dra-
matic increase in the output volatility and uncertainty. Financial inter-linkages
play an important role in the emergence of ﬁnancial instabilities. Recently many
research have focused on the role of linkages along the two dimensions of conta-
gion and liquidity, and they suggest that regulators have to look at the interplay
of network topology, capital requirements, and market liquidity. In particular for
contagion, the position of institutions in the network matters and their impact
can be computed through stress tests even when there are no defaults in the
system [2].
For the data-driven study using empirical data, many scholars use a collection
of daily snapshots of the Italian interbank money market originally provided
by the Italian electronic Market for Inter-bank, referred to e-MID in the text
[6,11,18]. However, even central banks and regulators have only a dim view of
the interconnections between banks at a moment in time, and thus the systemic
risk in the ﬁnancial networks, and each bank’s contribution to this risk, are
poorly known. A natural starting point is to utilize complementary approach
to data-driven approach, basing their systemic risk measures on accessing and
interpreting data on balance sheets and trading. As understanding of the most
critical systemic attributes improves, this network description can be extended
to wider jurisdictions and can record more detail: complex transactions such as
credit risk derivatives, more complex institutional behavior such as internal risk
limit systems and responses to counterparty risk changes.
In this paper, we investigate the eﬀect of credit linkages on the macroeco-
nomic activity by developing the network-based agent model. In particular, we
study the linkage dependence among agents (ﬁrms and banks) at the micro-
level and to estimate their impact on the macro activities such the GDP growth
rate, the size and growth rate distributions of agents. We propose the model
reﬁnement strategy which validate through the some universal laws and proper-
ties based on empirical studies revealing statistical properties of macro-economic
time series [19]. The purpose of the network-based model of systemic risks is to
build up the dependence among agents (ﬁrms and banks) at the micro-level and
to estimate their impact on the macro stability.
Phase structure of hypothetical ﬁnancial systems: the relation between basic
network parameters such as connectivity, homogeneity and uncertainty, and
macroscopic systemic risk measures is non-linear, non-intuitive and diﬃcult to
predict. Such emergent features will reﬂect profound properties of real world
ﬁnancial networks that can be understood by ﬁrst looking at deliberately sim-
pliﬁed agent-based simulation models [16]. Simulation studies of complex hypo-
thetical ﬁnancial networks that map out these types of features will lead to
improved understanding of the resilience of networks, and perhaps ultimately to
pragmatic rules of thumb for network participants. This line of inquiry also links

Agent-Based Modeling of Economic Instability
257
systemic network theory strongly to other areas of network science, from which
we may draw additional ideas and intuition. We show that three stylized facts:
a fat-tailed size distribution of the ﬁrm sizes, a tent- shaped growth rate distri-
bution, the scaling relation of the growth rate variance with ﬁrm size. We then
address the questions of validating and verifying simulations. We validate with
the widely acknowledged “stylized facts” which describe the ﬁrm (and bank)
growth rates of fat tails, tent distribution, volatility, etc., and recall that some
of these properties are directly linked to the way time is taken into account. The
growth of ﬁrm size, the distribution of ﬁrm sizes, the distribution of sizes of the
new ﬁrms in each year and ﬁnd it to be well approximated by a log-normal [19].
We validate the simulation results in terms of such as (i) the distribution of the
logarithm of the growth rates, for a ﬁxed growth period of one year, and for
companies with approximately the same size S, displays an exponential form.
In the second part of our work, we investigate the eﬀect of credit linkages
on the ﬁrms activities to explain some key elements occurred during the recent
economic and ﬁnancial crisis. From this perspective, the network theory is a
natural candidate for the analysis of interacting agent systems. The ﬁnancial
sector can be regarded as a set of agents (banks and ﬁrms) who interact with
each other through ﬁnancial transactions. These interactions are governed by
a set of rules and regulations, and take place on an interaction graph of all
connections between agents. The network of mutual credit relations between
ﬁnancial institutions and ﬁrms plays a key role in the risk for contagious defaults.
In particular, we study the repercussions of inter-bank connectivity on agents’
performances, bankruptcy waves and business cycle ﬂuctuations. Our ﬁndings
suggest that there are issues with the role that the bank system plays in the
real economy and in pursuing economic growth. Indeed, our model shows that
a heavily- interconnected inter-bank system increases ﬁnancial fragility, leading
to economic crises and distress contagion.
2
Economic Risks in a Connected World
There is empirical evidence that as the connectivity of a network increases,
there is an increase in the network performance, but at the same time, there is
an increase in the chance of risk contagion which is extremely large. If external
shocks at some agents are propagated to the other connected agents due to
failure, the domino eﬀects often come with disastrous consequences. The network
is only as strong as its weakest link, and trade-oﬀs are most often connected to
a function that models system performance management. The qualiﬁcation of
risks lies in their connections. An interdependent risk to one system may present
an opportunity to other systems. Therefore a systemic risk impacts the integrity
of the whole system as well as its components.
In a networked world, the risks faced by any one agent depend not only on
that agents actions but also on those of others. The fact that the risk one actor
faces is often determined in part by the activities of others gives a unique and
complex structure to the incentives that agents face as they attempt to reduce

258
A. Namatame
their exposure to these interdependent risks. The concept of interdependent risks
refers to situations in which multiple agents act separately generate common
risk. Protective management can reduce the risk of a direct loss to each agent,
but there is still some chance of suﬀering damage from others actions. The fact
that the risk is often determined in part by the behavior of others imposes
independent risk structures on the incentives that agents face for reducing risk or
investing in risk mitigation measures. Kunreuther and Heal were initially led to
analyze such situations by focusing on the interdependence of security problems
[12]. An interdependent security setting is one in which each individual or ﬁrm
that is part of an interconnected system must decide independently whether
to adopt protective strategies that mitigate future losses. The analysis focused
on protection against discrete, low-probability events in a variety of protective
settings with somewhat diﬀerent cost and beneﬁt structures: airline security,
computer security, ﬁre protection, and vaccination. Under some circumstances,
the interdependent security problem resembles the familiar prisoners dilemma in
which the only equilibrium is the decision by all agents not to invest in protection
even though everyone would be better oﬀif they had decided to incur this cost.
In other words, a protective strategy that would beneﬁt all agents if widely
adopted may not be worth its cost to any single agent and it is better oﬀsimply
taking a free ride on the others’ investments.
The ﬁnancial crises triggered numerous studies on the systemic risks caused
by contagion eﬀects via interconnections in the modern banking networks. Sys-
temic risks result in continuous large-scale defaults or systemic failure among the
networked banks and ﬁnancial institutions [3,7]. The formulation of systemic risk
can greatly beneﬁt from a complex network approach. Allen and Gale introduced
the use of network theories to enrich our understanding of ﬁnancial systems and
studied how the ﬁnancial system responds to contagion when ﬁnancial institu-
tions are connected with diﬀerent network topologies [1]. They how the banking
network topology aﬀect the stability of both ﬁnance market and product market
by changing the density of connections among banks. While the risk of conta-
gion may be expected to be larger in a highly interconnected banking system,
we show that shocks may have complex eﬀects on ﬁnancial institutions as well
as the ﬁrms.
Many studies analyze the ﬁnancial systems such as ﬁnancial stability and
contagion using the network theory and other network analysis methods. In an
interbank market, banks facing liquidity shortages may borrow liquidity from
other banks that have liquidity surpluses. This system of liquidity swapping
provides the interbank market with enhanced liquidity sharing. Furthermore, it
also brings down the risk of contagion among the interconnected banks when
unexpected problems arise. Solvency or liquidity problems of a single bank can
travel through the interbank linkages to other banks and become a causality of
systemic failure; this highlights the importance of interbank markets for ﬁnancial
stability [15,17].

Agent-Based Modeling of Economic Instability
259
3
Agent Based Modeling
Our work is based on an existing agent-based model [10,20]. Here, we consider
multiple banks which can operate not only in the credit market but also in the
inter-bank market. In our model, ﬁrms may ask for loans from banks to increase
their production rate and proﬁt. If contacted banks face liquidity shortage when
trying to cover the ﬁrms’ requirements, they may borrow from a surplus bank
in the inter-bank network. In this market, therefore, lender banks share with
borrowers bank the risk for the loan to the ﬁrm. We model the inter-bank network
as preference attachment.
3.1
Firms Behaviour
The goods market is implemented following the model of Delli Gatti et al. [5]
where output is supply-determined, that is ﬁrms sell all the output they opti-
mally decide to produce. In the model of Delli Gatti, there are two types of
ﬁrms: Downstream (D) ﬁrms that produce consumption goods using labor and
intermediate goods; Upstream (U) ﬁrms that produce intermediate goods on
demand from D ﬁrms.
The D ﬁrms demand both for labor Ni(t), i = 1, 2, .., D, and for intermediate
goods Qi(t) depending on their ﬁnancial conditions, that is captured by net
worth Ai(t). Respectively, the demands of the ith ﬁrm are given by
Ni(t) = c1Ai(t)β, Qi(t) = c2Ai(t)β
(1)
The consumption goods are sold at a stochastic price ui(t) that is a random
variable extracted from a uniform distribution between (0, 1). The U ﬁrms pro-
duce intermediate goods employing only labour, so that for the jth, j = 1, 2, .., U,
the demand is
Qj(t) = c3Nj(t)
(2)
Many D ﬁrms can be linked to a single U ﬁrm but each D ﬁrm has only one
supplier for intermediate goods among the U ﬁrms. The price of intermediate
goods is
pj(t) = 1 + rj(t) = αAj(t)−α
(3)
where rj(t) is the interest on trade credit, which is assumed to be dependent
only on the ﬁnancial condition of the U ﬁrm. In particular, if the jth ﬁrm is not
performing well, it will give credit with a less favourable interest rate. While the
production of D ﬁrms is determined by their worth Ai(t), the production of U
ﬁrms is determined by the demand on the part of D ﬁrms.
Qj(t) = c2
D

i=1
Ai(t)β
(4)
Analogously, the demand for labor will be
Nj(t) = c1
D

i=1
Ai(t)β
(5)

260
A. Namatame
Each period a subset of ﬁrms enter in the credit market asking for credit. The
amount of credit requested by companies is related to their investment expendi-
ture, which is, therefore, dependent on interest rate and ﬁrm’s economic situa-
tion. If the net worth of the ﬁrms is not suﬃcient to pay the wage bill, they will
demand credit to a bank. For each ﬁrm the credit demand is
B(t) = c4N(t) −A(t)
(6)
where the functional form of N(t) changes if we are considering U ﬁrm or D
ﬁrm. We assume that many ﬁrms can be linked to a single bank but each ﬁrm
has only one supplier of loans. Without entering in the details, we point out that
the interest rate on loans is a decreasing function of the banks net worth and
penalizes ﬁnancially fragile ﬁrms.
The proﬁts of the D and U ﬁrm are evaluated from the diﬀerence between
their gains and the costs, and the proﬁt of the jth U ﬁrm at time t is given by
πj(t) = (1 + rj(t))Qj(t) −(1 + rj(t))Bj(t)
(7)
At each time step the net worth of each ﬁrm is updated according to
Aj(t + 1) = Aj(t) + πj(t) −Dj(t)
(8)
Bankruptcy occurs if the net worth becomes negative. The bankrupt ﬁrm leaves
the market. Therefore Dj(t) in Eq. (8) is the “bad debt”, that takes into account
the possibility that a borrower cannot pay back the loan because it goes bankrupt
(that is, Aj(t) ≤0). In this framework the lenders are the U ﬁrms and the banks
and both U and D ﬁrms can be borrowers. The total number of agents (U and
D ﬁrms) is kept constant over time. Therefore when ﬁrms fail, they are replaced
by new entrants which are on average smaller than incumbents. So, entrants’
size is drawn from a uniform distribution centered around the mode of the size
distribution of incumbent ﬁrms.
3.2
Bank Behavior
The primary purpose of banks is to channel their funds towards loans to compa-
nies. Consulted banks, having analyzed their own credit risk and the ﬁrm’s risk,
may grant the requested loan, when they have enough supply of liquidity. The
supply of credit is a percentage of banks’ equity because ﬁnancial institutions
adopt a system of risk management based upon an equity ratio. When consulted
banks do not have liquidity to lend, they can enter in the interbank system, in
order not to lose the opportunity of earning on investing ﬁrms.
Similar to ﬁrms, we have a constant population of competitive banks indexed
by j = 1, ..., B. Each bank has a balance sheet structure deﬁned as Sj(t) =
Ej(t) + Dj(t) with Sj(t) being the credit supply, Ej(t) the equity and Dj(t) the
debt. The primary function of banks activity is to lend their funds through loans
to ﬁrms, as this is their way to make money via interest rates. Bank j oﬀers its
interest rate to the borrower ﬁrm i:
rj,i = σAj(t)−β + θli(t)−θ
(9)

Agent-Based Modeling of Economic Instability
261
Where li(t) is the amount of lending. So the interest rate is decreasing with
the borrower’s ﬁnancial robustness. In a sense, we adopt the principle according
to which the interest rate charged by banks incorporates an external ﬁnance
premium increasing with the leverage and, therefore, inversely related to the
borrower’s net worth.
When ﬁrm i needs loan, it contacts a number of randomly chosen banks.
Credit linkages between ﬁrms and banks are deﬁned by some bipartite graph.
Contacted banks, checked the investment risk and their amount of liquidity,
oﬀer an interest rate in Eq. (9). After exploring the lending conditions of the
contacted banks, each ﬁrm asks the consulted banks for credit starting with
the one oﬀering the lowest interest rate. If in the credit market, the contacted
ﬁnancial institutions do not have enough supply of liquidity to fully satisfy the
ﬁrm’s loan, then banks can use the inter-bank market. As in the credit market,
the requiring bank (borrower) asks the lacking fraction of the loan requested
by the ﬁrm from a number of randomly chosen banks (lenders). Among the
contacted banks, the banks satisfying the risk threshold in Eq. (7) and having
enough supply of liquidity oﬀer the loan to the asking bank for an inter-bank
interest rate, which equals the credit market interest rate in Eq. (8). Among this
subset of oﬀering banks, the borrower bank chooses the lender bank, starting
with the one oﬀering the lowest interest rate.
At the end of each period t, after trading has taken place, ﬁnancial insti-
tutions update their proﬁts. The bank’s proﬁt depends on interests on credit
market (ﬁrst term), on interests on inter-bank market (second term), which can
be either positive or negative depending on bank j net position (lender or bor-
rower), on interests paid on deposits and equity (third term). Bank net worth
evolves according to:
Ej(t + 1) = Ej(t) −1 + πj(t) −Bij −Bkj
(10)
with the last two terms on the right side being ﬁrms and banks’ bad debts
respectively. Similar to ﬁrms, banks go bankrupt when their equity at time t
becomes negative, and the failed bank leaves the market.
The total number of banks is kept constant over time. Therefore when banks
fail, they are replaced by new entrants which are on average smaller than incum-
bents.
3.3
Credit Network Formation
We deﬁne the network formation dynamics, how D ﬁrms look for the linkage
with U ﬁrm and how D and U ﬁrms look for a bank to ask their loans. In order
to establish the product supply linkages, the ﬁrms take the partner choice rule,
that is, they search for the minimum of the prices charged by a randomly selected
set of possible suppliers. It can change supplier only if a better partner is found.
Similarly in order to establish the credit linkages the ﬁrms take the partner
choice rule: they search for the minimum of the interest rate charged among
the loan oﬀered banks. If contacted banks face liquidity shortage when trying

262
A. Namatame
to cover the ﬁrms’ requirements, they may borrow from a surplus bank in the
inter-bank system. In this market, therefore, lender banks share with borrower
banks the risk for the loan to the ﬁrm.
3.4
Interbank Network Formation
We model the inter-bank network based on some connection rules: (case 01)
random connection (random graph as a benchmark), (case 02) net-worth based
connection, an agent with higher net worth is selected as a partner, (case 03)
interest based connection, an agent oﬀering a lower interest is selected as a
partner.
Bankruptcies are determined as ﬁnancially fragile ﬁrms fail, i.e. their net
worth becomes negative. If one or more ﬁrms are not able to pay back their
debts to the bank, the bank’s balance sheet decreases and, consequently, the
ﬁrms’ bad debt, aﬀecting the equity of banks, can also lead to bank failures. As
banks, in case of shortage of liquidity, may enter the interbank market, the fail-
ure of borrower banks could lead to failures of lender banks. Agents’ bad debt,
thus, can bring about a cascade of bankruptcies among banks. The source of the
domino eﬀect may be due to indirect interaction between bankrupt ﬁrms and
their lending banks through the credit market, on one side, and to direct interac-
tion between lender and borrower banks through the inter-bank system, on the
other side. Their ﬁndings suggest that there are issues with the role that the bank
system plays in the real economy and in pursuing economic growth. Indeed, their
model shows that a heavily-interconnected inter-bank system increases ﬁnancial
fragility, leading to economic crises and distress contagion. The process of con-
tagion gains momentum and spreads quickly to a large number of banks once
some critical banks fail.
4
Simulations and Results
The model is studied numerically for diﬀerent values of the parameter p,
which drives the inter-bank connectivity. We consider an economy consisting of
N = 1000 ﬁrms and B = 50 banks and do simulation over the time period span of
T = 1000. Each ﬁrm is initially given the same amount of capital Ki(0) = 1, net-
worth Ai(0) = 65 and loan Li(0) = 35. We also set other parameters as follows:
τ = 4, ϕ = 0.8, ci(i = 1 −4) = 1, λ = 0.3, α = 0.1, χ = 0.8, ψ = 0.1, θ = 0.05.
The probability of attachment between ﬁrms and banks in the credit market
is x = 0.05. In this way the number of ﬁrm’s out-going links is less than three.
The reason for this is that in a highly connected random network, synchroniza-
tion could be achieved via indirect links. The eﬀects of direct contagion among
ﬁnancial institutions are easier to be tested in a network where indirect synchro-
nization is less likely to arise. We repeat simulations 100 times with diﬀerent
random seeds. We start by analyzing the eﬀect of inter-bank linkages on the
systemic risk. Then we analyze the correlation between the ﬁnancial and the
real sector of the economy.

Agent-Based Modeling of Economic Instability
263
In our model, bankruptcies are determined as ﬁnancially fragile ﬁrms fail,
that is their net worth becomes negative. If one or more ﬁrms are not able
to pay back their debts to the bank, the bank’s balance sheet decreases and,
consequently, the ﬁrms’ bad debt, aﬀecting the equity of banks, can also lead to
bank failures. As banks, in case of shortage of liquidity, may enter the interbank
market, the failure of borrower banks could lead to failures of lender banks.
Agents’ bad debt, thus, can bring about a cascade of bankruptcies among banks.
The source of the domino eﬀect may be due to indirect interaction between
bankrupt ﬁrms and their lending banks through the credit market, on one side,
and to direct interaction between lender and borrower banks through the inter-
bank system, on the other side.
One of the goals of our work is the study of bankruptcy avalanches and their
connection with the dynamics of the credit networks. Suppose that a random
price ﬂuctuation causes the bankruptcy of some U ﬁrms. Consequently, the loans
they took will not be fulﬁlled and the worth of the lenders (banks and D ﬁrms)
will decrease. Eventually, this will result in a bankruptcy of some of them and,
more importantly, in an increase of the interest rates charged on their old and
new borrowers. This, in turn, will increase the probability of a bankruptcy of a
D ﬁrm, and so on. The credit network has a scale free structure and then the
default of a highly connected agent may provoke an avalanche of bankruptcy.
The question we address here is whether phenomena of collective bankrupt-
cies are related to the initial setting of parameters. Usually simulation starts
with homogeneous ﬁrms and banks. Early stage of the simulation (before 200
period) many ﬁrms and banks default. However after that, especially banks and
D-ﬁrms grow as extremely heterogeneous agents and the size distribution obeys
power low, then a few banks and ﬁrms default occasionally. In order to answer
this mysterious observation, we investigate the eﬀects of inter-bank linkages on
contagion phase in the ﬁnancial market. In particular, we focus on one of the
most extreme examples of systemic failure, namely bank bankruptcies. Figure 1
(a)without bank-network
(b)with bank-network
Fig. 1. The bankrupt rate of the banks. (a) The banks are not linked each other and
operate independently. (b) The banks form ﬁnancial networks.

264
A. Namatame
shows the average number of failed banks. Collective bankruptcies arise from
the complex nature of agent interaction. To better analyze this observation, we
further need data-driven analysis of simulated data.
We are explicitly concerned with the potential of the inter-bank market to act
as a contagion mechanism for liquidity crises and to determine the eﬀect of the
banks connectivity on macroeconomic outcomes such business cycle ﬂuctuations
and bankruptcies.
Our ﬁndings suggest that there are issues with the role of the central bank
plays in the real economy and in pursuing economic growth. Indeed, our model
shows that the central bank plays a great role in heavily shocked economy by
clearing big debt. Without the central bank and it is the only the interbank
network to absorb a big shock, then ﬁnancial fragility increases, and leading to
economic crises and distress contagion.
5
Conclusion
The main purpose of this paper was to study the contagious bank defaults, that
is, the bank defaults that inﬂuence other banks through interconnectivity of
the networked banking system, and not the defaults caused by the fundamen-
tal weakness of a given bank. Since failed banks are not able to honour their
commitments in the interbank market, other banks may probably be inﬂuenced
to default, which may aﬀect more banks and cause further contagious defaults.
By modeling a three sector economy with goods, credit and interbank market,
we have been able to analyze the role of ﬁnancial network on the agents perfor-
mance and macro dynamics in terms of the growth and stability. Our results also
support that the interaction among market participants (ﬁrms and banks) is a
key element to reproduce important stylized facts about bankruptcy waves and
business cycle ﬂuctuations. In particular, we have shown that the existence the
strong linkages among banks generates larger bankruptcy cascades due to the
larger systemic risk. When our inter-bank network reaches the phase transition,
the presence of many interconnected banks suggests that the credit network is
more susceptible to the domino eﬀect. In this case, in fact, when failures occur,
many agents are potentially compromised. However, our model has shown that
the relationship between risk propagation and risk sharing cannot be clearly
deﬁned. Our ﬁndings suggest that there are issues with the role that the bank
system plays in the real economy and in pursuing economic growth. Our model
also shows that a heavily- interconnected inter-bank system increases ﬁnancial
fragility, leading to economic crises and distress contagion if the role of the cen-
tral bank is weak.
References
1. Allen, F., Gale, D.: Financial contagion. J. Polit. Econ. 108(1), 1–33 (2000)
2. Battiston, S., et al.: DebtRank: too central to fail? Financial networks, the FED
and systemic risk. Sci. Rep. 2, 1–6 (2012). Nature Publishing

Agent-Based Modeling of Economic Instability
265
3. Bramoulle, Y., Kranton, R.: Risk-sharing networks. J. Econ. Behav. Organ. 64
(3–4), 275–294 (2007)
4. Delli Gatti, D., Di Guilmi, C., Gallegati, M., Palestrini, A.: A new approach to
business ﬂuctuations: heterogeneous interacting agents, scaling laws and ﬁnancial
fragility. J. Econ. Behav. Organ. 56(4), 489–512 (2005)
5. Delli Gatti, D., Gallegati, M., Greenwal, B., Stiglitz, J.: The ﬁnancial accelerator
in an evolving credit network. J. Econ. Dyn. Control 34, 1627–1650 (2010)
6. Fricke, D., Lux, T.: Core-Periphery Structure in the Overnight Money Market:
Evidence from the E-mid trading platform. Kiel Working Paper, No. 1759 (2012)
7. Gai, P., Kapadia, S.: Contagion in ﬁnancial networks. In: Proceedings of Royal
Society Interface, pp. 466–480 (2010)
8. Gallegati, M., Greenwald, B., Richiardi, M.G., Stiglitz, J.E.: The asymmetric eﬀect
of diﬀusion processes: risk sharing and contagion. Glob. Econ. J. 8(3), 1–20 (2008)
9. Haldane, A., May, R.: Systemic risk in banking ecosystems. Nature 469, 351–355
(2011)
10. Ide, K., Namatame, A.: A mesoscopic approach to modeling and simulation of
systemic risks. In: IEEE CIFEr (2014)
11. Iori, G., Jafarey, S., Padilla, F.G.: Systemic risk on the interbank market. J. Econ.
Behav. Organ. 61, 525–542 (2006)
12. Kunreuther, H., Heal, G.: Interdependent security. J. Risk Uncertain. 26, 231–249
(2003)
13. Legendi, R., Gulyas, L.: Replication of the Macro ABM Model. CRISIS, working
paper (2012)
14. May, R.M., Levin, S.A., Sugihara, G.: Ecology for bankers. Nature 451, 893–895
(2008)
15. May, R.M., Arinaminpathy, N.: Systemic risk: the dynamics of model banking
systems. J. R. Soc. 7, 823–838 (2010)
16. Namatame, A., Chen, S.: Agent-Based Models and the Network Dynamics. Oxford
University Press, Oxford (2015)
17. Nier, E., Yang, J., Yorulmazer, T., Alentorn, A.: Network models and ﬁnancial
stability. J. Econ. Dyn. Control 31(6), 2033–2060 (2007)
18. Soramaki, K., Bech, M.L., Arnold, J., Glass, R.J., Beyeler, W.E.: The topology of
interbank payment ﬂows. Phys. A 379, 317–333 (2007)
19. Stanley, M., Amaral, L., Sergey, V., Havlin, L., Leschhorn, H., Maass, P., Stanley,
E.: Scaling behavior in the growth of companies. Nature 379, 804–806 (1996)
20. Suzuki, Y., Namatame, A., Aruka, Y.: Agent-based modeling of economic volatil-
ity and risk propagation on evolving networks agent-based modeling of economic
growth and crises. In: Handa, H., Ishibuchi, H., Ong, Y.S., Tan, K. (eds.) Proceed-
ings in Adaptation, Learning and Optimization, vol. 1. Springer, Cham (2015)

A Bad Plan Is Better Than No Plan:
A Theoretical Justiﬁcation of an Empirical
Observation
Songsak Sriboonchitta1 and Vladik Kreinovich2(B)
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
songsakecon@gmail.com
2 Department of Computer Science, University of Texas at El Paso,
500 W. University, El Paso, TX 79968, USA
vladik@utep.edu
Abstract. In his 2014 book “Zero to One”, a software mogul Peter
Thiel lists the lessons he learned from his business practice. Most of these
lessons make intuitive sense, with one exception – his observation that
“a bad plan is better than no plan” seems to be counterintuitive. In this
paper, we provide a possible theoretical explanation for this somewhat
counterintuitive empirical observation.
1
Formulation of the Problem
A bad plan is better than no plan: a counterintuitive empirical obser-
vation. In his 2014 book “Zero to One” [1], a software mogul Peter Thiel lists
the lessons he learned from his business practice.
Most of these lessons make intuitive sense, with one exception – his obser-
vation that a bad plan is better than no plan. At ﬁrst glance, this empirical
observation seems to be counterintuitive.
What we do in this paper. In this paper, we provide a possible theoretical
explanation for this empirical observation.
2
How to Describe This Problem in Precise Terms?
We need to describe actions. We decide between diﬀerent plans of action.
There may be many parameters that describe possible actions. For example, for
the economy of a country, the central bank can set diﬀerent borrowing rates,
the government can set diﬀerent values of the minimal wage and of unemploy-
ment beneﬁts, etc. For a company, such parameters include percentage of income
that goes into research and development, percentage of income that goes into
advertisement, etc.
In general, let us denote the number of such parameters by n, and the para-
meters themselves by x1, . . . , xn. From this viewpoint, selecting an action means
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_19

A Bad Plan Is Better Than No Plan: A Theoretical Justiﬁcation
267
selecting the appropriate values of all these parameters – i.e., in mathematical
terms, a point x = (x1, . . . , xn) in the corresponding n-dimensional space.
Initial state. Let x(0)
1 , . . . , x(0)
n
denote the values of the parameters correspond-
ing to the current moment of time t0. Our goal is to select parameters at future
moments of time t1 = t0 + h, t2 = t0 + 2h, . . . , tT = t0 · T · h, for some time
quantum h.
Changes cannot be too radical. Whether we talk about the economy of a
country or of a big company, it is very diﬃcult to make fast drastic changes,
there is a large amount of inertia in these economic systems.
Therefore, we will only consider possible actions x1, . . . , xn which are close
to the initial state, i.e., which have the form xi = x(0)
i
+ Δxi for some small
changes Δxi.
Changes x(tj+1)−x(tj) from one moment of time tj to the next one tj+1 are
even more limited. Let b be the upper bound on such changes:
∥x(tj+1) −x(tj)∥=




n

i=1
(xi(tj+1) −xi(tj))2 ≤b.
On the other hand, the very fact that we talk about changes means that we
are not completely satisﬁed with the current situations. The more changes we
undertake at each moment of time, the faster we will reach the desired state.
The size of each change is limited by the bound b. Within this limitation,
the largest possible changes are changes of the largest possible size b. Thus, we
assume that all the changes from one moment of time to the next one are of the
same size b:
∥x(tj+1) −x(tj)∥=




n

i=1
(xi(tj+1) −xi(tj))2 = b.
What is our objective. Since we talk about which plans are better, this
assumes that we have agreed on how we gauge the eﬀect of diﬀerent plans,
i.e., we have agreed on a numerical criterion y that describes, for each possible
action, how good is the result of this action.
The value of this criterion depends on the action: y = f(x1, . . . , xn) for some
function f(x1, . . . , xn). In some cases, we may know this function, but in general,
we do not know the exact form of this function. In other words, we know what
we want to optimize, but we do not necessarily know the exact consequences of
each action.
Since changes are small, we can simplify the expression for the objec-
tive function. We are interested in the values y = f(x1, . . . , xn) of the agreed-
upon objective function f(x1, . . . , xn) in the small vicinity of the original state

268
S. Sriboonchitta and V. Kreinovich
x(0) = (x(0)
1 , . . . , x(0)
n ). In other words, we are interested in the values
f(x1, . . . , xn) = f(x(0)
1
+ Δx1, . . . , x(0)
n
+ Δxn)
corresponding to small deviations Δxi.
Since the deviations Δxi are small, we can expand the objective function into
Taylor series in Δxi and keep only the main – linear – terms in this expansion.
In other words, it makes sense to consider a linear approximation of the original
objective function:
f(x1, . . . , xn) = f(x(0)
1
+ Δx1, . . . , x(0)
n
+ Δxn) = a0 +
n

i=1
ai · Δxi,
where a0
def
= f(x(0)
1 , . . . , x(0)
n ) and
ai
def
= ∂f
∂xi |x=x(0).
Maximizing this expression is equivalent to maximizing the linear part
n

i=1
ai · Δxi.
Thus, if we denote the deviations Δxi by ui, we arrive at the following problem:
• we start with the values u(0) = (u1, . . . , un) = (0, . . . , 0);
• at each moment of time, we change the action by a change of a given size b:
∥u(tj+1) −u(tj)∥=




n

i=1
(ui(tj+1) −ui(tj))2 = b;
• we want to gradually change the values ui so that the (unknown) objective
function
n
i=1
ai · ui gets as large as possible.
What does “no plan” mean. An intuitive understanding of what “no plan”
means is that at each moment of time, we undertake a random change, uncor-
related with all the previous changes.
In other words, at each moment of time, as the change vector u(tj+1)−u(tj),
we select a vector of length b with a random direction. Since we have no reason
to select one of the possible directions, we thus consider all the directions to be
equally probable.
In other words, we assume that the change vector uniformly distributed on
the sphere of radius b, and that the changes corresponding to diﬀerent moments
of time are independent. The resulting trajectory u(t) is thus an n-dimensional
random walk [2] (or, equivalently, n-dimensional Brownian motion).

A Bad Plan Is Better Than No Plan: A Theoretical Justiﬁcation
269
What we mean by a plan. Intuitively, a plan means that instead of going
in diﬀerent directions at diﬀerent moments of time, we have a systematic
change u(t).
We consider local planning, for a few cycles t1, t2, . . ., for which the diﬀerence
Δt
def
= t −t0 is small. Thus, we can expand u(t) = u(t0 + Δt) into Taylor series
and keep only linear terms in this expansion: u(t) = u(t0 + Δt) = u(t0) + v · Δt,
where
v
def
= du
dt |t=t0
.
By deﬁnition of the deviation u(t), we have u(t0) = 0, and thus, u(t) = v·Δt.
So, the change between each moment of time and the next one takes the form
u(tj+1) −u(tj) = v · (tj+1 −tj) = v · h.
In other words, in contrast to the no-plan case, when changes at diﬀerent
moments of time are completely uncorrelated, here the changes at diﬀerent
moments of time are exactly the same.
Of course, from the practical viewpoint, they cannot always be the same: if
a plan is bad, and we see that the desired objective functions decreases moment
by moment, we will abandon this plan and select a new one.
Of course, it does not make sense to abandon the plan after a single decrease
in the value of the objective function: it is known that even the best plans take
some time to turn the economy around. Let us denote by m the reasonable
number of decreases after which the plan will be abandoned.
From this viewpoint, selecting a plan means selecting a single change vector
of length b, and following it for m steps, after which:
• if we had m decreases in the value of the objective function, we select a
diﬀerent plan,
• otherwise, we continue with the original plan for all T moments of time.
What we mean by a possible bad plan. We consider the situation in which
we do not know the shape of the objective function. In this case, we do not know
which change vector to select, so we select a random vector w = (w1, . . . , wn) of
size a.
If a · w
def
=
n
i=1
ai · wi > 0, the resulting plan will lead to a consistent improve-
ment of an objective function, so we will have a good plan. Vice versa, if
a · w =
n

i=1
ai · wi < 0,
the resulting plan will lead to a consistent decrease of an objective function, so
we will have a bad plan.

270
S. Sriboonchitta and V. Kreinovich
Resulting description of two strategies. In this paper, we compare two
strategies:
• the no-plan strategy, and
• the possibly-bad-plan strategy.
In the no-plan strategy, we consider random walk with step size b.
In the possibly-bad-plan strategy, we select a random vector w of size b. If
a · w < 0, then after m moments of time, we select a new random vector, etc.
Which of the two strategies leads to better results? Both strategies rely
on a random choice. So, for the same situation, the same strategy may lead to
diﬀerent results.
Our goal is to improve the value of the objective function. Each strategy
sometimes improves this values, sometimes decreases it:
• For example, if every time we select a change vector which is improving, both
strategies will improve the value of the objective function.
• On the other hand, if every time, we select a decreasing change vector, both
strategies will decrease the value of the objective function.
Thus, for each of the two strategies, a reasonable performance measure is the
probability that by the ﬁnal time tT , this strategy will increase the value of the
objective function in comparison to its original value.
Let us compare these probabilities.
3
Comparing the Results of the No-Plan and the
Possibly-Bad-Plan Strategies
Case of the no-plan strategy. Under this strategy, the vector U
def
= u(tT )
describing the diﬀerence between the ﬁnal action and the original one is the sum
of T independent change vectors, each of which has a random direction.
All original distributions are invariant with respect to rotations. Thus, for
the sum of these change vectors, the distribution is still invariant with respect
to rotations – and hence, the direction e
def
=
U
∥U∥of the sum vector U is also
random: uniform on the unit sphere.
This implies, in particular, that the distribution of e is the same as the
distribution of the opposite vector −e.
The vector U leads to an improvement if U·a > 0, i.e., equivalently, if e·a > 0.
Since e and −e have the same distribution, the probability that e · a > 0 is the
same as the probability that (−e) · a > 0, i.e., that e · a < 0. The probability of
a degenerate case e · a = 0 is 0. Thus, with probability 1, we have two equally
probable cases: improving and decreasing. Therefore, the probability of each of
these cases is exactly 1/2.
So, for the no-plan strategy, the probability of improvement is 0.5.

A Bad Plan Is Better Than No Plan: A Theoretical Justiﬁcation
271
Case of the possibly-bad-plan strategy. In this case, similarly, with prob-
ability 1/2, we select an improving change vector w, in which case the value of
the objective function improves.
With the remaining probability 1/2, we select a decreasing change vector, in
which case the value of the objective function starts decreasing. However, it does
not necessarily always decrease: after m steps, once we see that the value of the
objective function decreases, we select a new change vector. In this case, with
probability 1/2, we will select an improving change vector – and with a positive
probability, the resulting improvement in the remaining T −m moments of time
will compensate for the decrease in the ﬁrst m steps.
Thus, in this case, the probability that this strategy will lead to an improve-
ment is larger than 1/2, since:
• in addition to the probability-1/2 situations in which we select an improving
change vector from the very beginning,
• we also have situations in which we ﬁrst decrease and then increase,
• and the probability of such additional situations is positive.
Conclusion: the possibly-bad-plan strategy is indeed better than the
no-plan strategy
• For the no-plan-strategy, the probability of improvement is 1/2, while
• for the possibly-bad-plan strategy, this probability is larger than 1/2.
So, indeed, the possibly-bad-plan strategy is theoretically better than the no-
plan strategy.
Thus, we indeed have a theoretical explanation for Thiel’s empirical obser-
vation.
How better? How larger is the probability of success of the possibly-bad-plan
strategy than 1/2?
To answer this question, let us consider what happens when T is suﬃciently
large, i.e., in mathematical terms, when T tends to inﬁnity. In this case, the
probability that the no-plan method will succeed remains the same: 1/2.
On the other hand, the probability that the possibly-bad strategy will succeed
tends to 1. Indeed, if in this strategy, at some moment of time t, we select an
improving change vector w, then with T →∞, the resulting increase (T −t)·(a·w)
will tend to inﬁnity and thus, eventually overcome decreases that happened
before moment t.
So, the only possibility not to improve is when we consistently select a
decreasing vector w.
• The probability of selecting such a vector in the beginning is 1/2.
• The probability of selecting it again after m iterations is also 1/2, so the
overall probability that we have a decrease for the ﬁrst 2m moments of time
is (1/2)2.
• Similarly, the probability that for all T/m selections, we selected a decreasing
vector, is equal to (1/2)T/m.

272
S. Sriboonchitta and V. Kreinovich
When T →∞, this probability tends to 0 and thus, indeed, the probability that
the possibly-bad-plan strategy will led to improvement tends to 1.
Acknowledgments. We acknowledge the partial support of the Center of Excellence
in Econometrics, Faculty of Economics, Chiang Mai University, Thailand. This work
was also supported in part by the National Science Foundation grants HRD-0734825
and HRD-1242122 (Cyber-ShARE Center of Excellence) and DUE-0926721, and by
an award “UTEP and Prudential Actuarial Science Academy and Pipeline Initiative”
from Prudential Foundation.
References
1. Thiel, P., Masters, B.: Zero to One: Notes on Startups, or How to Build the Future.
Crown Business, New York (2014)
2. Voit, J.: The Statistical Mechanics of Financial Markets. Springer, Heidelberg (2010)

Shape Mixture Models Based on Multivariate
Extended Skew Normal Distributions
Weizhong Tian1,2, Tonghui Wang3(B), Fengrong Wei4, and Fang Dai5
1 School of Science, Xian University of Technology, Xi’an 710054, China
weizhong.tian@enmu.edu
2 Department of Mathematical Sciences, Eastern New Mexico University,
Portales, NM 88130, USA
3 Department of Mathematical Sciences, New Mexico State University,
Las Cruces 88003, USA
twang@nmsu.edu
4 Department of Mathematics, University of West Georgia,
Carrollton, GA 30018, USA
fwei@westga.edu
5 School of Science, Xian University of Technology, Xi’an 710054, China
daifang@xaut.edu.cn
Abstract. In this paper, the class of the shape mixtures of extended
skew normal distributions is introduced. The posterior distributions for
the shaped parameters are obtained. The moment generating functions
for the posterior distributions of the shaped parameters are discussed.
Also Bayesian analysis for this shape mixture model is studied.
1
Introduction
In many real world problems, the assumptions of normality are violated as the
data possess some level of skewness. The class of skew normal distributions is
an extension of the normal distribution, allowing for the presence of skewness
was introduced in the papers by Azzalini [3,4], according to which a random
variable X is called a skew normal random variable if it has the probability
density function
f(x) = 2φ(x)Φ(αx),
where φ(·) and Φ(·) are the density function and the cumulative distribution
function of the standard normal distribution, and α is called the shaped para-
meter.
Azzalini and Dalla Valle [4] deﬁned the multivariate skew normal family. The
random vector V is said to have a multivariate skew normal distribution with
location parameter μ ∈ℜn, scale parameter Σ, and shape parameter α ∈ℜn,
denoted by V ∼SNn(μ, Σ, α), if its probability density function is given by
fV(v; μ, Σ, α) = 2φn(v; μ, Σ)Φ

α′Σ
−1/2(v −μ)

,
v ∈ℜ
n,
(1)
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_20

274
W. Tian et al.
where φn(v; μ, Σ) is the n-dimensional normal density function with mean vector
μ and covariance matrix Σ. Since then, the class of multivariate skew normal
distributions have been studied by many authors, see Azzalini [5], Gupta [8],
and Szkely [11].
For the case where the location parameter is zero, the quadratic forms under
skew normal settings were studied by Gupta and Huang [8], Genton et al. [7],
Huang and Chen [9], and Loperﬁdo [10]. For the general case where the location
parameter is not zero, the quadratical forms of skew normal models based on
stochastic representation were studied by Tian and Wang [13], and new versions
of Cochran’s theorem were obtained in Wang et al. [14] and Ye et al. [15].
The class of multivariate extended skew-normal distribution is an extension
of the multivariate skew normal distribution. It was introduced by Tian et al.
[12] and deﬁned as follows. The random vector Y ∈ℜn is said to have a mul-
tivariate extended skew normal distribution with location parameter μ ∈ℜn,
scale parameter Σ ∈Mn×n, shape parameter α ∈ℜn, and extended parameter
γ ≥−1, denoted by Y ∼ESNn(μ, Σ, α, γ) if its density function is given by
fY(y; α, γ) =
2
2 + γ φn(y; μ, Σ)

1 + γΦ

α′Σ−1
2 (y −μ)

,
(2)
where φn(z) is the n-dimensional normal density function with mean vector 0
and covariance matrix In. For the case where n = 1, the distribution with the
density function given in (2) is called the extended skew normal distribution,
denoted by Y ∼ESN(μ, σ2, α, γ).
Recently, Arellano-Valle et al. [1,2] introduced the class of shape mixtures of
skewed distributions by assuming that the shape vectors parameter follows a mul-
tivariate normal distribution or skew normal distribution. In this paper, a class of
shape mixtures models was introduced by taking the shape parameter follow the
skew normal mixing distribution, i.e., the conditional distribution of Y given S = s
is extended skew normal and the prior distribution of S is skew normal:
[Y |S = s] ∼ESN(μ, σ2, s, γ)
and
S ∼SN(η, ω2, α).
(3)
In (3), we assume the location-scale parameters (μ, σ2), (η, ω2), and extended
parameter γ are known. Based on properties of the extended skew normal distri-
bution given in Tian et al. [12], the class of shape mixture extended skew normal
distribution is an extension of the one given in Arellano-Valle et al. [1,2].
In this paper, the class of shape mixtures of dependent and independent
multivariate extended skew normal distributions with the Bayesian interpreta-
tions is analyzed. Moment generating functions for the posterior distribution
of the shape parameters are discussed for diﬀerent cases. In additional, some
useful results based on the Bayesian analysis are obtained. The organization of
this paper is as follows. The class of shape mixtures of dependent and indepen-
dent multivariate extended skew normal distributions is studied in Sect. 2. The
moment generating functions about the posterior distribution of shape para-
meters are obtained in Sect. 3. Bayesian analysis based on the shape mixtures
models are discussed in Sect. 4, and some remarks on this model are given in
Sect. 5.

Shape Mixture Models Based on Multivariate Extended Skew
275
2
Shape Mixture of Multivariate Extended Skew Normal
Distribution
Let μ = (μ1, · · · , μn)′ ∈ℜn and η = (η1, · · · , ηn)′ ∈ℜn be the location para-
meters, Σ = (σij) ∈Mn×n, Ω = (ωij) ∈Mn×n be positive deﬁnite scale matri-
ces in which σii = σ2
i , ωii = ω2
i . Let D(σ), D(ω) be n × n diagonal matrices
whose diagonal entries are the components of the vector σ = (σ2
1, · · · , σ2
n)′ and
ω = (ω2
1, · · · , ω2
n)′, and α = (α1, · · · , αn)′ ∈ℜn be the vector of skewness para-
meters.
Consider an observable random vector Y = (Y1, · · · , Yn)′ whose distribu-
tion is from the multivariate extended skew normal family. The components
Y1, · · · , Yn can be independent or dependent, conditionally on the shape para-
meter, which are diﬀerent or common for the Yi’s. In this section, we will derive
the shape mixture distributions for both independent and dependent compo-
nents based on the extended skew normal distributions. The following Lemmas
will be used to prove the properties of our main results.
Lemma 1 (Azzalini and Capitanio [5]). If V ∼SNn(0, Σ, α), and A is a non-
singular n × n matrix, then
A′V ∼SNn(A′ΣA, A−1α).
Lemma 2 (Zacks [16] and Chen [6]). Let U ∼Nk(c, C) be a non-siglar, mul-
tivariate normal random vector, then for any ﬁxed m-dimensional vector a and
m × k matrix A, we have that,
E[Φm(a + AU|b, B)] = Φm (Ac + a|b, B + ACA′) ,
(4)
where Φm(·|b, B) denotes the cumulative density function of a multivariate nor-
mal distribution with mean vector b and covariance matrix B.
2.1
Mixtures on Diﬀerent Shape Parameters
Assume that the vector of shape parameters S = (S1, · · · , Sn)′ ∈ℜn, with
independent Si’s, and n observations Y1, · · · , Yn given S = s = (s1, · · · , sn)′ are
also independent. Then we have the following result.
Theorem 1. Let [Y1|S = s], · · · , [Yn|S = s] be independent variables with
[Yi|S = s] ∼ESN(μi, σ2
i , si, γ). Let S1, · · · , Sn be independent random vari-
ables with Si ∼SN(ηi, ω2
i , αi), for i = 1, · · · , n. Then the probability density
function of Y is
f(y) =

4
2 + γ
n
|D(σ)|−1
2 φn (z)
×
n

i=1
	1
2 + γΦ2


ηizi
0

; 0,

1 + ω2
i z2
i ω2
i ziαi
ω2
i ziαi
1 + α2
i

,

276
W. Tian et al.
and the posterior density function of S|Y = y is
fS|Y=y(s) = |D(ω)|−1
2 φn (u)
n

i=1
[Φ(αiui) (1 + γΦ (sizi))]
×
n

i=1
	1
2 + γΦ2

ηizi
0

; 0,
1 + ω2
i z2
i ω2
i ziαi
ω2
i ziαi
1 + α2
i
−1
,
where z = D(σ)−1
2 (y −μ) and u = D(ω)−1
2 (s −η).
Proof. Let z = D(σ)−1
2 (y −μ) and u = D(ω)−1
2 (s −η). By (2), we know
that the joint probability density function of Y|S = s and the joint probability
density function of S, are given, respectively, by
fY|S=s(y) =

2
2 + γ
n
|D(σ)|−1
2
n

i=1
{φ(zi)[1 + γΦ(sizi)]} ,
and
f(s) = (2)n|D(ω)|−1
2
n

i=1
{φ(ui)Φ(αiui)} .
Thus the joint probability density function of Y and S is
f(y, s) = fY|S=s(y)f(s),
so that the marginal probability density function of Y can be obtained,
f(y) =

4
2 + γ
n
|D(σ)|−1
2 |D(ω)|−1
2
n

i=1

si
{φ(zi)φ(ti)Φ(αiti)[1 + γΦ(sizi)]} dsi
=

4
2 + γ
n
|D(σ)|−1
2 |φn (z)
n

i=1
1
2 + γE

Φ2
 zi
αi

ti +
 ηizi
0

; 0, I2
	
.
Consequently, using Lemma 2 with ti ∼N(0, ω2
i ) for i = 1, · · · , n, the conditional
distribution of S given Y = y is obtained by a straightforward application of
Bayes’ Theorem.
□
Note that if we assume that α = 0, then from Theorem 1, we can obtain the
following result.
Corollary 2.1. In Theorem 1, if we assume that S ∼Nn(η, D(ω)), then the
probability density function of Y is
f(y) =

2
2 + γ
n
|D(σ)|−1φn (z)
n

i=1

1 + γΦ

ηizi

1 + ω2
i z2
i

,

Shape Mixture Models Based on Multivariate Extended Skew
277
and the posterior density function of [S|Y = y] is
fS|Y=y(s) = 2n|D(ω)|−1φn (u)
n

i=1
[(1 + γΦ(sizi))]
(5)
×
n

i=1

1 + γΦ

ηizi

1 + ω2
i z2
i
−1
,
where z = D(σ)−1
2 (y −μ), and u = D(ω)−1
2 (s −η).
Now we consider the case where the components of S are dependent so that
S has a multivariate skew normal distribution, and Y given S = s is sampled
from a multivariate extended skew normal distribution.
Theorem 2. Let [Y|S = s] ∼ESNn(μ, Σ, s, γ) and S ∼SNn(η, Ω, α). Then
the probability density function of Y is
f(y) =

4
2 + γ
2
φn (y; μ, Σ)
×

1
2 + γΦ2

η′z
0

; 0,

1 + z′Ωz z′Ω
1
2 α
α′Ω
1
2 z 1 + α′α

,
and the posterior density function of [S|Y = y] is
fS|Y=y(s) = φn(s; η, Ω) [(1 + γΦ(s′z)) Φ(α′u)]
×

1
2 + γΦ2
η′z
0

; 0,

1 + z′Ωz z′Ω
1
2 α
α′Ω
1
2 z 1 + α′α
−1
,
where z = Σ
−1
2 (y −μ) and u = Ω
−1
2 (s −η).
Proof. From (2), the conditional probability density function of Y|S = s is
fY|S=s(y) =

2
2 + γ

φn(y; μ, Σ)[1 + γΦ(s′z)],
and the probability density function of S is
f(s) = 2φn(s; η, Ω)Φ(α′t),
where z = Σ
−1
2 (y −μ) and u = Ω
−1
2 (s −η). Similar to the proof of Theorem 1,
we can obtain the probability density function of Y
f(y) =

4
2 + γ

φn (y; μ, Σ)

s
φn(s; η, Ω)[1 + γΦ(s′z)]Φ(α′u)ds
=

2
2 + γ
2
φn (y; μ, Σ)
×

s
φn(s; η, Ω)

Φ

α′u

+ γΦ2

z′
α′Ω
−1
2

s −

0
α′Ω
−1
2 η

; 0, I2

ds.

278
W. Tian et al.
Consequently, the desired result follows from Lemma 2 with s ∼Nn(η, Ω)
and Bayes’ Theorem.
□
Similarly with α = 0, we obtain the following result.
Corollary 2.2. Let [Y|S = s] ∼ESNn(μ, Σ, s, γ), and S ∼Nn(η, Ω). Then
the probability density function of Y is
f(y) =

2
2 + γ

φn (y; μ, Σ)
	
1 + γΦ

η′z
√
1 + z′Ωz

,
(6)
and the conditional probability density function of [S|Y = y] is
fS|Y=y(s) = φn(s; η, Ω) (1 + γΦ(s′z))
1 + γΦ

η′z
√
1 + z′Ωz
 ,
where z = Σ
−1
2 (y −μ).
2.2
Mixtures on a common shape parameter
Assuming shape variable parameter S ∈ℜ, and we consider the situation where
Y given S is sampled from a dependent multivariate extended skew normal
distribution. Then we have the following result.
Theorem 3. Let [Y|S = s] ∼ESNn(μ, Σ, s1n, γ), and s ∼SN(η, ω2, α). Then
the probability density function of Y is
f(y) =

4
2 + γ

φn (y; μ, Σ)
(7)
×
	1
2 + γΦ2


η1′
nz
0

; 0,

1 + nω2z′z ω2αz′1n
αω21′
nz
1 + nα2ω2

,
and the posterior density function of [S|Y = y] is
fS|Y=y(s) = φ(s; η, ω2) (1 + γΦ(s1′
nz)) Φ(αu)
(8)
×
	1
2 + γΦ2

η1′
nz
0

; 0,
1 + nω2z′z ω2αz′1n
αω21′
nz
1 + nα2ω2
−1
,
where z = Σ
−1
2 (y −μ), and u = ω
−1(s −η).
Proof. The proof is similar to the proof given in Theorem 2 and thus it is
omitted.
□
Corollary 2.3. Let [Y|S = s] ∼ESNn(μ, Σ, s1n, γ), and s ∼SN(η, ω2). Then
the probability density function of Y is
f(y) =

2
2 + γ

φn (y; μ, Σ)
	
1 + γΦ

η1′
nz
√
1 + nω2z′z

(9)

Shape Mixture Models Based on Multivariate Extended Skew
279
and the posterior density function of [S|Y = y] is
fS|Y=y(s) = φ(s; η, ω2) (1 + γΦ(s1′
nz))
1 + γΦ

η1′
nz
√
1 + nω2z′z
 ,
(10)
where z = Σ
−1
2 (y −μ).
3
The Moment Generating Function for the Distribution
of the Shape Parameters
In this section, the moment generating functions of diﬀerent shape parameters
for giving the known observed values under diﬀerent cases are studied.
Theorem 4. Let [Y1|S = s], · · · , [Yn|S = s] be independent random variables
with [Yi|S = s] ∼ESN(μi, σ2
i , si, γ). Also let S1, · · · , Sn be independent random
variables with Si ∼SN(ηi, ω2
i , αi). Then the moment generating function of
S|Y = y is
MS|Y=y(t) = C0
n

i=1
exp
	
tiηi + 1
2tiω2
i
 
Φ(αitiωi, 1 + α2
i )
(11)
+ γΦ2


αitiωi
ziω2
i ti + ziηi

; 0,

1 + α2
i
αiziωi
αiziωi 1 + z2
i ω2
i

and the moment generating function of S′S|Y = y is
MS′S|Y=y(t)
= C0
n

i=1
exp
	
tη2
i + 2t2ω2
i η2
i
1 −2ω2
i t
 
Φ
2αiωiηit
1 −2ω2
i
, 1 −2ω2
i + αi
1 −2ω2
i

+ γΦ2
⎡
⎢⎢⎣
⎛
⎜
⎝
αi
1 −2ω2
i
ziωi
1 −2ω2
i
+ ziηi
⎞
⎟
⎠; 0,
⎛
⎜
⎜
⎝
1 +
α2
i
1 −2ω2
i
αiziωi
1 −2ω2
i
αiziωi
1 −2ω2
i
1 +
z2
i ω2
i
1 −2ω2
i
⎞
⎟
⎟
⎠
⎤
⎥⎥⎦
⎫
⎪
⎪
⎬
⎪
⎪
⎭
,
(12)
where
C0 =
n

i=1
	1
2 + γΦ2

ηizi
0

; 0,
1 + ω2
i z2
i ω2
i ziαi
ω2
i ziαi
1 + α2
i
−1
and z = D(σ)−1
2 (y −μ).
Proof. Let z = D(σ)−1
2 (y −μ) and u = D(ω)
−1
2 (s −η). By Theorem 1, for
t = (t1, t2, · · · , tn)′ ∈ℜn, the moment generating function of S|Y = y can be
obtained as

280
W. Tian et al.
MS|Y=y(t) = C0|D(ω)|−1φn(u)
n

i=1

ℜ
[Φ(αiui) (1 + γΦ (sizi)) exp{siti}] dsi
= C0
n

i=1

ℜ
φ(ui)Φ(αiui) (1 + γΦ [(ωiui + ηi)zi]) exp{ti(ωiui + ηi)}dui
= C0
n

i=1
exp

tiηi + 1
2tiω2
i
 
ℜ
φ(ui; tiωi, 1)Φ(αiui) (1 + γΦ [(ωiui + ηi)zi]) dui.
Note that Φ(αiui)Φ [(ωiui + ηi)zi] = Φ2


αi
ziωi

ui +

0
ziηi

; 0, I2

so that
terms given in above expression can be simpliﬁed as follows.
$
ℜ
φ(ui; tiωi, 1)Φ(αiui)dui = E [Φ(αiui)]
and
$
ℜ
φ(ui; tiωi, 1)Φ2


αi
ziωi

ui +

0
ziηi

; 0, I2

dui
= E
	
Φ2


αi
ziωi

ui +

0
ziηi

; 0, I2

,
where ui ∼N(tiωi, 1). Thus the desired result for MS|Y=y(t) is obtained by
Lemma 2.
Similarly, for t ∈ℜ, the moment generating function of S′S|Y = y is,
MS′S|Y=y(t) = C0
n

i=1
exp
	
tη2
i + 2t2ω2
i η2
i
1 −2ω2
i t

×
	
E [Φ(αiui)] + γE

Φ2


αi
ziωi

ui +

0
ziηi

; 0, I2

,
where ui ∼N
 2ωiηit
1 −2ω2
i t, (1 −2ω2
i t)−1

. Therefore by Lemma 2, the desired
result is obtained.
□
Theorem 5. Let [Y|S = s] ∼ESNn(μ, Σ, s, γ), and S ∼SNn(η, Ω, α). Then
MS|Y=y(t) = C2 exp
	
η′t + 1
2t′Ωt
 	
Φ(α′Ω
1
2 t; 0, 1 + α′α)
(13)
+ γΦ2


α′Ω
1
2 t
zΩt + z′η

; 0,
1 + α′α α′Ω
1
2 t
α′Ω
1
2 t 1 + z′Ωz

,
MS′S|Y=y(t) = C2 exp
%
tη′η + 2t2η′Ω
1
2 (In −2Ωt)−1Ω
1
2 η
&
|In −2ωt|−1
2 (14)
×
	
Φ

2tα′(In −2Ωt)−1Ω
1
2 η; 0, 1 + α′(In −2Ωt)−1α

+ γΦ2


2tα′(In −2Ωt)−1Ω
1
2 η
2tz′Ω
1
2 (In −2Ωt)−1Ω
1
2 η + z′η

; 0, Ψ

,

Shape Mixture Models Based on Multivariate Extended Skew
281
where
C2 =

1
2 + γΦ2

η′z
0

; 0,

1 + z′Ωz z′Ω
1
2 α
α′Ω
1
2 z 1 + α′α
−1
,
Ψ = I2 +
 α′
z′Ω
1
2
′
(In −2Ωt)−1
 α′
z′Ω
1
2

, and z = Σ
−1
2 (y −μ).
Proof. Let z = Ω−1
2 (y −μ) and u = Ω
−1
2 (s −η). By Theorem 2, for t ∈ℜn,
the moment generating function of S|Y = y is
MS|Y=y(t) = C2
$
ℜn
φn(s; η, Ω) [(1 + γΦ(s′z)) Φ(α′u)] ds
= C2
$
ℜn
φn(u); 0, In) exp{

Ω
1
2 u + η
′
t}
×


1 + γΦ

Ω
1
2 u + η
′
z

Φ(α′u)

du.
By using following two facts,
exp
	
−1
2u′u + u′Ω
1
2 t

= exp
	
−1
2

u −Ω
1
2 t
′ 
u −Ω
1
2 t

+ 1
2tΩt
and
Φ(α′u)Φ(z′Ω
1
2 u + z′η) = Φ2

 α′
z′Ω
1
2

u +
 0
z′η

; 0, I2

.
the above expression can be simpliﬁed as
MS|Y=y(t) = C2 exp
	
η′t + 1
2t′Ωt

×
	
E [Φ(α′U)] + γE

Φ2

 α′
z′Ω
1
2

u +

0
z′η

; 0, I2

,
where u ∼Nn

Ω
1
2 t, In

. Thus by Lemma 2, the desired result can be obtained.
Now for the moment generating function of s′s|Y = y, let t ∈ℜ. By using
the argument,
exp

−1
2u′u + (Ω
1
2 u + η)′(Ω
1
2 u + η)t
	
= exp{η′ηt} exp

−1
2

u′(In −2Ωt)u −4u′Ω
1
2 ηt
	
= exp

η′ηt + 2t2η′Ω
1
2 (In −2Ωt)−1Ω
1
2 η

|In −2Ωt|−1
2
× exp

−1
2

u −2t(In −2Ωt)−1Ω
1
2 η
′
(In −2Ωt)

u −2t(In −2Ωt)−1Ω
1
2 η
	
,

282
W. Tian et al.
we obtain
MS′S|Y=y(t) = C2 exp
%
tη′η + 2t2η′Ω
1
2 (In −2Ωt)−1Ω
1
2 η
&
|In −2ωt|−1
2
×
	
E [Φ(α′u)] + γE

Φ2

 α′
z′Ω
1
2

u +

0
z′η

; 0, I2

,
where u ∼Nn

2t(In −2Ωt)−1Ω
1
2 η, (In −2Ωt)−1
. By Lemma 2, the desired
result follows.
□
Similarly we can prove the following result.
Theorem 6. Let [Y|S = s] ∼ESNn(μ, Σ, s1n, γ), and S ∼SN(η, ω2, α).
Then the moment generating function of S|Y = y is
MS|Y=y(t) = C4 exp
	
ηt + 1
2t2ω
 	
Φ(αω; 0, 1 + α2)
(15)
+ γΦ2


αω
1n′zω2 + 1n′zη

; 0,

1 + α2
1n′zωα
1n′zωα 1 + (1n′zω)2

and the moment generating function of S2|Y = y is
Ms2|Y=y(t) = C4 exp

η2t + 2ω2η2t2
1 −2ω2t
	 
Φ
 2αωηt
1 −2ω2t; 0, 1 + α2(1 −2ω2t)−1

(16)
+ γΦ2
⎡
⎢⎣
⎛
⎜
⎝
2αωηt
1 −2ω2t
2tω2η1n
′z
1 −2ω2t + 1n
′zη
⎞
⎟
⎠; 0, Ψ2
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
,
where
C4 =
	1
2 + γΦ2


η1′
nz
0

; 0,

1 + nω2z′z ω2αz′1n
αω21′
nz
1 + nα2ω2
−1
,
Ψ2 = I2 +

α
1n′z′ω
′
(1 −2ωt)−1

α
1n′zω

, and z = Σ
−1
2 (y −μ).
4
Bayesian Analysis for the Shaped Mixtures Model
In this section, we will apply our main results to both linear regression model
and Hierarchical model.
4.1
Robust Inference for Unknown Location and Scale Parameters
Consider the linear regression model Y = Xβ + E, where Y = (Y1, . . . , Yn)′,
Xβ = μ = (μ1, · · · , μn)′ with μi = x′
iβ. Assume that location and scale
parameters are unknown, denoted by θ = (μ′, σ2)′. We will use three examples

Shape Mixture Models Based on Multivariate Extended Skew
283
to illustrate the robustness of θ. Note that the marginal likelihood functions of
Y , f(y|θ), can be obtain directly by setting up α = 0 in Theorems 1, 2 and 3.
We will discuss the moment generating function of f(Y|θ) under the assumption
that the prior shape parameter follows the normal distribution. The following
lemma will be used in our examples.
Lemma 3. Let W ∼N(a, b2) and Z ∼N(c, d2) are independent. Then
E

Φ

cW
√
1 + d2W 2

= E[Φ(WZ)].
Example 4.1. Let [Y1|S = s], . . . , [Yn|S = s] be independent random variables
with [Yi|S = s] ∼ESN(μi, σ2, si, γ) and let S ∼Nn(η, D(ω)). Then the moment
generating function of Y|θ is
MY|θ(t) =

2
2 + γ
n
exp
	
t′μ + 1
2t′D(σ)t

n

i=1
{1 + rE[Φ(ziui)]} ,
(17)
where t = (t1, · · · , tn)′ ∈ℜn, zi ∼N(tiσ2, σ2), Ui ∼N(ηi, ω2
i ), and ui’s are
independent.
Proof. The desired result can be obtained by Corollary 2.1 and Lemma 2.
□
Example 4.2. Let [Y|S = s] ∼ESNn(μ, σ2In, s, γ), S ∼Nn(η, Ω). Then the
moment generating function of Y is
MY(t) =

2
2 + γ

exp
	
t′μ + σ2
2 tt
 	
1 + γE

Φ

η′z
√
1 + z′Ωz

,
(18)
where t = (t1, · · · , tn)′ ∈ℜn, and Z ∼Nn (σt, In).
Proof. The proof can be done directly by Corollary 2.2.
□
Similarly, by Corollary 2.3, we can obtain the following result.
Example 4.3. Let [Y|S = s] ∼ESNn(μ, σ2In, s1n, γ), S ∼SN(η, ω2). Then
the moment generating function of Y′ is
MY|θ(t) =

2
2 + γ

exp

t′μ + σ2
2 t′t
	 
1 + γE

Φ

η1′
nz
√1 + ω2z′1n1′nz
	
, (19)
where t ∈ℜn, and Z ∼Nn (σt, In).
4.2
Hierarchical Bayesian Approach
In this section, we use the hierarchical Bayesian approach to construct a prior
distribution on the shape parameter in the extended multivariate skew normal
distribution. Suppose that we observed data from a random vector follows the
multivariate extended skew normal distribution, where the location parameters
and γ are assumed to be known, the prior distribution is the skew normal distri-
bution conditional on θ0, and θ0 follows a normal distribution, we need to use
our results to ﬁnd the useful information for the parameter θ.

284
W. Tian et al.
Example 4.4. Let Y|(θ, θ0) ∼ESNn(μ, Σ, θ, γ), θ|θ0 ∼SNn(0, Ω, θ0), and
θ0 ∼Nn(ν, Ψ). Then the posterior density of θ given Y = y is
fθ|y(θ) =
4
2 + γ φn(θ; 0, Ω)
	
Φ

θ′Ω−1
2 ν; 0, θ′Ω−1
2 ΨΩ−1
2 θ

+ γΦ2


θ′Ω−1
2 ν
θ′Σ−1
2 (y −μ)

; 0,

1 + θ′Ω−1
2 ΨΩ−1
2 θ 0
0
1

.
Proof. By Theorem 2, we obtain the posterior density of θ|(y, θ0), given by
fθ|y,θ0(θ) =
4
2 + γ φn(θ; 0, Ω)Φ(θ′
0u)[1 + γΦ(θ′z)].
Note that fθ|y(θ, θ0) = fθ|(y,θ0)(θ)f(θ0). Thus, by Bayesian calculation, we
have
fθ|X(θ) =
$
ℜn fθ|X,θ0(θ)f(θ0)dθ0
and, by Lemma 2, the desired result follows after simpliﬁcations.
□
Remark 4.1. In Example 4.4, if we assume ν = 0, that is θ0 ∼N(0, Ψ), then
the posterior density of θ given Y = y is,
fθ|X(θ) = 2φn(θ; 0, Ω)Φ

(x −μ)′Σ−1
2 θ

,
which is free of parameters γ and Ψ. Also the moment generating function of
[θ|y] and [θ′θ|y] are given by
Mθ|y(t) = 2 exp
	1
2t′Ωt

Φ
⎧
⎪
⎨
⎪
⎩
(y −μ)′Σ−1
2 Ω
3
2 t

1 + (y −μ)′Σ−1
2 Ω2Σ−1
2 (y −μ)
 1
2
⎫
⎪
⎬
⎪
⎭
,
t ∈ℜn
and
Mθ′θ|x(t) = |In −2Ωt|−1
2 ,
t ∈ℜ,
respectively. Therefore the posterior distribution of [θ′θ|y] is the same as the
one in the multivariate normal case.
5
Few Further Remarks
Continuing with the robust inference for the unknown location parameters and
assume σ2 = 1, we can obtain the moment generating function of U = Y′Y.
(i) Let [Y|S = s] ∼ESNn(μ, In, s, γ), and S ∼Nn(η, Ω), then the moment
generating function of U is
MU (t) =
2 exp
	 tμ′μ
1 −2t

(2 + γ)(1 −2t)n/2
	
1 + γE

Φ

η′z
√
1 + z′Ωz

,
(20)
where t ∈ℜ, and z ∼Nn
 2tμ
1 −2t, (1 −2t)In

.

Shape Mixture Models Based on Multivariate Extended Skew
285
(ii) Let [Y|S = s] ∼ESNn(μ, In, s1n, γ), and s ∼SN(η, ω2), then the moment
generating function of U is
MU (t) =
2 exp
	 tμ′μ
1 −2t

(2 + γ)(1 −2t)n/2

1 + γE

Φ

η1′
nz

1 + ω2(z′1n)2

,
(21)
where t ∈ℜ, and z ∼Nn
 2tμ
1 −2t, (1 −2t)In

.
Remark 5.1. If we assume that η = 0 in (20) or η = 0 in (21), then U in
(1) and (ii) are both reduced into the noncentral chi-square distributed random
variables. Also, if η′z = 0 in (20) or 1n′z = 0 in (21), then U in (1) and (ii) U
are both reduced into the noncentral chi-square distribution.
Acknowledgement. The authors thank Professor Vladik Kreinovich for valuable sug-
gestions that helped improve this article. The research of Weizhong Tian was partially
supported by Internal Grants from Eastern New Mexico University and Funds from
One Hundred Person Project of Shaanxi Province of China.
References
1. Arellano-Valle, R.B., Castro, L.M., Genton, M.G., Gmez, H.W.: Bayesian inference
for shape mixtures of skewed distributions, with application to regression analysis.
Bayesian Anal. 3(3), 513–539 (2008)
2. Arellano-Valle, R.B., Genton, M.G., Loschi, R.H.: Shape mixtures of multivariate
skew-normal distributions. J. Multivar. Anal. 100(1), 91–101 (2009)
3. Azzalini, A.: A class of distributions which includes the normal ones. Scand. J.
Stat. 12(2), 171–178 (1985)
4. Azzalini, A., Dalla, A.: The multivariate skew-normal distribution. Biometrika
83(4), 715–726 (1996)
5. Azzalini, A., Capitanio, A.: Statistical applications of the multivariate skew normal
distribution. J. Royal Stat. Soc. Ser. B (Stat. Methodol.) 61(3), 579–602 (1999)
6. Chen, J., Gupta, A.: Matrix variate skew normal distributions. Statistics 39(3),
247–253 (2005)
7. Genton, M., He, L., Liu, X.: Moments of skew-normal random vectors and their
quadratic forms. Stat. Probab. Lett. 51(4), 319–325 (2001)
8. Gupta, A., Huang, W.: Quadratic forms in skew normal variates. J. Math. Anal.
Appl. 273(2), 558–564 (2002)
9. Huang, W., Chen, Y.: Quadratic forms of multivariate skew normal-symmetric
distributions. Stat. Probab. Lett. 76(9), 871–879 (2006)
10. Loperﬁdo, N.: Quadratic forms of skew-normal random vectors. Stat. Probab. Lett.
54(4), 381–387 (2001)
11. Szkely, G., Rizzo, M.: A new test for multivariate normality. J. Multivar. Anal.
93(1), 58–80 (2005)
12. Tian, W., Wang, C., Wu, M., Wang, T.: The multivariate extended skew normal
distribution and its quadratic forms. In: Causal Inference in Econometrics, pp.
153–169. Springer (2016)

286
W. Tian et al.
13. Tian, W., Wang, T.: Quadratic forms of reﬁned skew normal models based on
stochastic representation. Random Operators Stoch. Equ. 24(4), 225–234 (2016)
14. Wang, T., Li, B., Gupta, A.: Distribution of quadratic forms under skew normal
settings. J. Multivar. Anal. 100(3), 533–545 (2009)
15. Ye, R., Wang, T., Gupta, A.: Distribution of matrix quadratic forms under skew-
normal settings. J. Multivar. Anal. 131, 229–239 (2014)
16. Zacks, S.: Parametric Statistical Inference: Basic Theory and Modern Approaches.
Elsevier, Philadelphia (2014)

Plausibility Regions on Parameters
of the Skew Normal Distribution Based
on Inferential Models
Xiaonan Zhu1, Baokun Li2, Mixia Wu3, and Tonghui Wang1(B)
1 Department of Mathematical Sciences, New Mexico State University,
Las Cruces, USA
{xzhu,twang}@nmsu.edu
2 School of Statistics, Southwestern University of Finance and Economy,
Chengdu, China
bali@swufe.edu.cn
3 College of Applied Sciences, Beijing University of Technology, Beijing, China
wumixia@bjut.edu.cn
Abstract. In this paper, plausibility functions and 100(1 −α)% plausi-
bility regions on location parameter and scale parameter of skew normal
distributions are obtained in several cases by using inferential models
(IMs), which are new methods of statistical inference. Simulation stud-
ies and one real data example are given for illustration of our results.
Keywords: Skew normal distribution · Closed skew normal
distribution · Noncentral closed skew chi-square distribution
Plausibility function · Inferential model
1
Introduction
Skew normal distributions introduced by Azzalini in 1985 [1] are suitable for the
analysis of data that is unimodal empirical distributed but with some skewness.
Since then, skew normal distributions and their generalizations have been stud-
ied and used by many researchers, such as Azzalini and Dalla Valle [4], Azzalini
and Capitanio [3], Wang et al. [11], Ye et al. [14], Ye and Wang [13] and Tian
and Wang [10]. Although skew normal distributions have some properties similar
to normal distributions, two important properties are absent: the closure for the
joint distribution of independent skew normal variables and the closure under
linear combinations. Therefore, to study statistical properties of a skew normal
population by samples, we usually have to assume that the sample is identically
distributed but not independent, e.g. Wang et al. [12] and Zhu et al. [15]. To
overcome this issue, in this paper, we assume that components of the sample are
independent and identically distributed (i.i.d.) from a skew normal population.
Then based on results we obtained, plausibility functions and 100(1−α)% plau-
sibility regions on location and scale parameters of skew normal distributions are
studied by using inferential models (IMs). When location and scale parameters
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_21

288
X. Zhu et al.
are known, inferences for skewness parameter based on a dependent sample by
using IMs were partially studied by Zhu et al. [15].
IMs are new methods of statistical inference introduced by Martin and Liu [8].
As Martin and Liu [9] note: “Comparing with Fishers ﬁducial inference, Dempster-
Shafer theory of belief functions and Bayesian inference, IMs not only provide
data-dependent probabilistic measures of uncertainty about the unknown parame-
ter, but does so with an automatic long-run frequency calibration property. Thus
IMs produce exact prior-free and prior-less probabilistic inference.” See [6–9] for
more details of IMs.
The paper is organized as follows. Skew normal distributions, their several
extensions and IMs are brieﬂy reviewed, and sampling distributions of skew
normal populations are brieﬂy studied in Sect. 2. Plausibility functions and
100(1−α)% plausibility regions about location parameter and scale parameter of
skew normal distributions are obtained in several cases, and corresponding sim-
ulation studies are provided in Sect. 3. Plausibility functions and 100(1 −α)%
plausibility regions of diﬀerence between two location parameters are given and
simulation studies are also provided in Sect. 4. One real data example is given
to illustrate our results.
2
Preliminaries
2.1
Sampling Distributions from a Skew Normal Population
Throughout this paper, we use Mn×m to denote the set of all n×m matrices over
the real ﬁeld R and Rn = Mn×1. For any B ∈Mn×m, B′ is the transpose of B.
In ∈Mn×n is the identity matrix. 1n is the column vector (1, · · · , 1)′ ∈Rn. Jn =
1n1′
n and ¯Jn = 1
nJn. Also φn(·; μ, Σ) and Φn(·; μ, Σ) are the probability density
function (p.d.f.) and cumulative distribution function (c.d.f.), respectively, of an
n-dimensional normal distribution with the mean vector μ ∈Rn and covariance
matrix Σ. Specially, φ(·) and Φ(·) are the p.d.f. and c.d.f. of the univariate
standard normal distribution.
The skew normal (SN) distribution is a generalization of the normal distrib-
ution introduced by Azzalini [1]. A random variable Z is said to be skew normal
with skewness parameter λ, denoted by Z ∼SN(λ), if its density function is
f(z; λ) = 2φ(z)Φ(λz),
where λ ∈R. For any μ ∈R and σ > 0, let X = μ + σZ, then X is called a
skew normal random variable with the location parameter μ, scale parameter σ
and skewness parameter λ, denoted by X ∼SN(μ, σ2, λ). The density function
of X is
f(x) = 2
σ φ
x −μ
σ

Φ

λx −μ
σ

.
Note that for any Z ∼SN(λ), Z2 ∼χ2
1 [2]. So if X ∼SN(μ, σ2, λ), then
(X−μ)2
σ2
∼χ2
1.

Plausibility Regions on Parameters of the Skew Normal Distribution
289
Multivariate skew normal distributions were deﬁned by Azzalini and Dalla
Valle [4] and Azzalini and Capitanio [3]. An n-dimensional random vector
X is said to have a multivariate skew normal distribution, denoted by X ∼
SNn(Σ, λ), if its density function is given by
f(x; Σ, λ) = 2φn(x; 0n, Σ)Φ (λ′x) ,
where parameters λ ∈Rn and Σ ∈Mn×n is positive deﬁnite. For more details
of univariate and multivariate skew normal distributions, see [2].
An extension of multivariate skew normal distributions was deﬁned by Wang
et al. [11] as follows (which generalized Azzalini and Dalla Valle’s deﬁnition).
Let Z ∼SNk(Ik, λ). Y = μ + B′Z is called a multivariate skew normal random
vector with location parameter μ ∈Rn, scale parameter B ∈Mk×n and shape
parameter λ ∈Rk, and is denoted by Y ∼SNn(μ, B, λ). The density of Y, if
exists, is given by
f(y; μ, B, λ) =
2
|Σ|
1
2 φn

Σ−1
2 (y −μ)

Φ

λ′BΣ−1(y −μ)
[1 + λ′(Ik −BΣ−1B′)λ]
1
2

,
where Σ = B′B. Note that if X ∼SNn(Σ, λ) or SNn(μ, B, λ) with λ ̸= 0n,
then components of X, X1, · · · , Xn, are not independent even if Σ = In.
In order to include joint distributions and linear combinations of independent
skew normal random variables, the closed skew normal distribution was deﬁned
by Gonz´alez-Far´ıas et al. [5]. An n-dimensional random vector X is distributed
according to a closed skew normal distribution with parameters m, μ, Σ, D, ν
and Δ, denoted as X ∼CSNn,m(μ, Σ, D, ν, Δ), if its density is
fn,m(x; μ, Σ, D, Δ) = cφn(x; μ, Σ)Φm[D(x −μ); ν, Δ],
where c−1 = Φm(0m; ν, Δ + DΣD′), n and m are positive integers, μ ∈Rn,
ν ∈Rm, D ∈Mm×n, and Σ ∈Mn×n and Δ ∈Mm×m are positive deﬁnite.
For simplicity, we use CSNn,m(μ, Σ, D, Δ) to denote CSNn,m(μ, Σ, D, 0m, Δ)
in this paper.
To study sampling distributions of SN(μ, σ2, λ), we need the following
deﬁnition.
Deﬁnition 1. Let X ∼CSNn,m(μ, In, D, Δ). The distribution of X′X is called
a noncentral closed skew chi-square distribution with degrees of freedom
n, the noncentrality parameter λ = μ′μ, skewness parameters δ1 = Dμ and
δ2 = DD′, and parameter Δ, denoted by X′X ∼CSχ2
n(λ, δ1, δ2, Δ).
Sampling distributions of SN(μ, σ2, λ) are given as follows.
Proposition 1. Suppose that X = (X1, · · · , Xn)′ is a random sample from
SN(μ, σ2, λ), i.e., Xi
i.i.d.
∼
SN(μ, σ2, λ), i = 1, · · · , n. Let ¯X =
1
n
n
i=1
Xi and
S2 =
1
n−1
n
i=1
(Xi −¯X)2 be the sample mean and sample variance respectively.
Then
¯X ∼CSN1,n

μ, σ2
n , λ
σ 1n, (1 + λ2)In −λ2¯Jn

,

290
X. Zhu et al.
and
(n −1)S2
σ2
∼CSχ2
n−1
	
0, 0n, λ2(In −¯Jn), In + λ2¯Jn

.
Proof. First, it can be shown that X ∼CSNn,n
	
μ1n, σ2In, λ
σIn, In

. Thus, by
Theorem 1 of [5],
¯X = 1
n
n

i=1
Xi ∼CSN1,n

μ, σ2
n , λ
σ 1n, (1 + λ2)In −λ2¯Jn

.
Second, note that (n−1)S2
σ2
= X′ 	 1
σ2 (In −¯Jn)

X. Since In−¯Jn is idempotent
of rank n −1, there is an orthogonal matrix P = [P1, P2] ∈Mn×n such that
In −¯Jn = P
In−1 0
0
0

P ′ = P1P ′
1.
Now let Y = 1
σP ′
1X. Then Y ∼CSNn−1,n
	 μ
σP ′
11n, In−1, λP1, In + λ2¯Jn

. Thus,
by Deﬁnition 1, we have
(n −1)S2
σ2
= Y′Y ∼CSχ2
n−1
	
0, 0n, λ2(In −¯Jn), In + λ2¯Jn

.
□
Proposition 2. Let X = (X1, · · · , Xn1)′ and Y = (Y1, · · · , Yn2)′ be indepen-
dent random samples from SN(μ1, σ2
1, λ1) and SN(μ2, σ2
2, λ2), respectively. Let
¯X = 1
n1
n1

i=1
Xi and ¯Y = 1
n2
n2

j=1
Yj. Then
¯X −¯Y ∼CSN1,n1+n2

μ1 −μ2, σ2
1
n1
+ σ2
2
n2
, D⋆, Δ⋆

,
where
D⋆=
1
n2σ2
1 + n1σ2
2

n2λ1σ11n1
−n1λ2σ21n2

,
and
Δ⋆=
⎛
⎝(1 + λ2
1)In1 −
n2λ2
1σ2
1
n2σ2
1+n1σ2
2
¯Jn1
λ1λ2σ1σ2
n2σ2
1+n1σ2
2 1n11′
n2
λ1λ2σ1σ2
n2σ2
1+n1σ2
2 1n21′
n1
(1 + λ2
2)In2 −
n1λ2
2σ2
2
n2σ2
1+n1σ2
2
¯Jn2
⎞
⎠.
Proof. Similar to the proof of Proposition 1, we can obtain
¯X ∼CSN1,n1

μ1, σ2
1
n1
, λ1
σ1
1n1, (1 + λ2
1)In1 −λ2
1¯Jn1

,
and
¯Y ∼CSN1,n2

μ2, σ2
2
n2
, λ2
σ2
1n2, (1 + λ2
2)In2 −λ2
2¯Jn2

.

Plausibility Regions on Parameters of the Skew Normal Distribution
291
Since ¯X and ¯Y are independent, by Theorem 3 of [5],
( ¯X, ¯Y )′ ∼CSN2,n1+n2(μ∗, Σ∗, D∗, Δ∗),
where μ∗= (μ1, μ2)′,
Σ∗=
 σ2
1
n1 0
0
σ2
2
n2

,
D∗=

λ1
σ1 1n1
0
0
λ2
σ2 1n2

,
and
Δ∗=
(1 + λ2
1)In1 −λ2
1¯Jn1
0
0
(1 + λ2
2)In2 −λ2
2¯Jn2

.
Note that ¯X −¯Y = (1, −1)( ¯X, ¯Y )′, so the desired result follows by Theorem 1
of [5].
□
2.2
Inferential Models
In this subsection, let’s brieﬂy review IMs, which were introduced by Martin and
Liu [8,9].
Let X be an observable random sample with a probability distribution PX|θ
on a sample space X, where θ is an unknown parameter, θ ∈Θ, a parameter
space. Let U be a well-known but unobservable auxiliary variable on an auxiliary
space U. An association is a map a : U × Θ →X such that X = a(U, θ).
An IM consists of three steps based on a ﬁxed association.
Association Step (A-step). Suppose we have an association X = a(U, θ) and
an observation X = x, where x could be a scalar or vector, then the unknown θ
must satisfy x = a(u∗, θ), for some unobserved u∗of U. So from the observation
X = x, we can construct sets of solutions
Θx(u) = {θ ∈Θ : x = a(u, θ)},
x ∈X,
u ∈U.
Prediction Step (P-step). To predict the unknown u∗, let u →S(u) be a set-
value map from U to S, where S is a collection of PU-measurable subsets of U.
Then the random set S : U →S is called a predictive random set of U with
distribution PS = PU ◦S−1. We will use S to predict u∗.
Combination Step (C-step). Deﬁne
Θx(S) = ∪
u∈SΘx(u).
For any assertion A of θ, i.e., A ⊆Θ, the belief function and plausibility function
of A with respect to a predictive random set S are deﬁned by,
belx(A; S) = PS{Θx(S) ⊆A : Θx(S) ̸= ∅};
plx(A; S) = PS{Θx(S) ̸⊆Ac : Θx(S) ̸= ∅}.

292
X. Zhu et al.
Note that
plx(A; S) = 1 −belx(Ac; S),
belx(A; S) + belx(Ac; S) ≤1,
for all A ⊆Θ.
To make a good inference for assertions, we need some concepts of validity.
Let X and Y be two random variables. We say that X is stochastically no
smaller than Y , denoted by X ≥st Y , if P(X > a) ≥P(Y > a), for all a ∈
R. A predictive random set S is valid for predicting the unobserved auxiliary
variable U if γS(U), as a function of U ∼PU, is stochastically no smaller than
Uniform(0, 1), where γS is called the contour function of S deﬁned by
γS(u) = PS(S ∋u),
u ∈U.
If γS(U) ∼Uniform(0, 1) then S is eﬃcient.
Remark 1. By Theorem 4.1 of [9], there is a simple way to construct a valid
predictive random set as follows. Let S be a collection of subsets of U. If S and
PU satisfy following conditions,
(i) S is nested, i.e., all elements of S can be ordered by inclusions,
(ii) There is some F ∈S such that PU(F) > 0,
(iii) All closed subsets of U are PU-measurable,
(iv) S contains ∅, U, and all of the other elements are closed subsets,
and deﬁne a predictive random set S, with distribution PS, supported on S,
such that
PS{F ⊆K} =
sup
F ∈S;F ⊆K
PU(F),
K ⊆U,
then S is valid.
Suppose X ∼PX|θ and let A be an assertion of interest. Then the IM with
a belief function belx(· ; S) is valid for A if
sup
θ̸∈A
PX|θ{belX(A; S) ≥1 −α} ≤α,
for all α ∈(0, 1).
The IM is said to be valid if it is valid for all A.
Remark 2. By Theorem 4.2 of [9], if the predictive random set S is valid and
Θx(S) ̸= ∅with PS-probability 1 for all x, then the IM is valid.
Given an IM, a 100(1 −α)% plausibility region is deﬁned by
Πx(α) = {θ ∈Θ : plx(θ; S) > α},
which is an IM-based counterpart of classical conﬁdence intervals.
3
One-Sample Cases
Let X = (X1, · · · , Xn)′ be a random sample from SN(μ, σ2, λ). In this section,
we are going to use X and IMs to make inferences about location parameter μ
and scale parameter σ of SN(μ, σ2, λ) in three diﬀerent cases, (i) μ is unknown
but σ and λ are known, (ii) σ is unknown but μ and λ are known, and (iii) σ
and μ are both unknown but λ is known and we are interested in σ.

Plausibility Regions on Parameters of the Skew Normal Distribution
293
3.1
The Plausibility Function and Plausibility Region for μ
Let X ∼(X1, · · · , Xn)′ be a random sample from SN(μ, σ2, λ) with unknown μ
and known σ and λ.
A-step: From Proposition 1, we know that
¯X = 1
n
n

i=1
Xi ∼CSN1,n

μ, σ2
n , λ
σ 1n, (1 + λ2)In −λ2¯Jn

.
So by the property of linear transformations of CSN distributions [5], we can
obtain an association
¯X = μ + F −1
1
(U),
where U ∼Uniform(0, 1) and F1 is the c.d.f. of CSN1,n

0, σ2
n , λ
σ1n, (1 + λ2)
In −λ2¯Jn

. For any observation ¯x ∈R and u ∈[0, 1], we have Θ¯x(μ) = {¯x −
F −1
1
(u)}.
P-step: To predict auxiliary variables U, we use the default predictive random
set
S(U) = [0.5 −|U −0.5|, 0.5 + |U −0.5|] .
C-step: By the P-step, we have
Θ¯x(S) =

¯x −F −1
1
(0.5 + |U −0.5|), ¯x −F −1
1
(0.5 −|U −0.5|)

.
So we can use the above IM to get the following result.
Theorem 1. For any assertion A = {μ},
bel¯x(A; S) = 0,
pl¯x(A; S) = 1 −|2F1(¯x −μ) −1|,
and the 100(1 −α)% plausibility region Π¯x(μ) is
¯x −F −1
1

1 −α
2

< μ < ¯x −F −1
1
α
2

.
Proof. It is clear that {Θ¯x(S) ⊆A} = ∅, so bel¯x(A; S) = 0.
pl¯x(A; S) = 1 −bel¯x(Ac; S)
= 1 −PS(Θ¯x(S) ⊆Ac)
= 1 −PU(¯x −F −1
1
(0.5 −|U −0.5|) < μ)
−PU(¯x −F −1
1
(0.5 + |U −0.5|) > μ)
= 1 −PU(|U −0.5| < |F1(¯x −μ) −0.5|)
= 1 −|2F1(¯x −μ) −1|.
By the deﬁnition, Π¯x(μ) = {μ : pl¯x(μ; S) > α}. Let pl¯x(A; S) = 1−|2F1(¯x−μ)−
1| > α and then solve it for μ. We have ¯x −F −1
1
	
1 −α
2

< μ < ¯x −F −1
1
	 α
2

. □

294
X. Zhu et al.
In Table 1 and Fig. 1, we provide a simulation study for this case with diﬀerent
μ, σ, λ and sample size n.
Table 1. Simulation results of AL, AU, ALength and CP of 95% plausibility regions
of μ when σ and λ are known.
μ = 0, σ = 1, λ = 1
μ = 2, σ = 2, λ = 5
n
AL
AU
ALength CP
n
AL
AU
ALength CP
10 −0.5164 0.5071 1.0235
0.9480 10 1.1699 2.7094 1.5394
0.9420
20 −0.3614 0.3623 0.7237
0.9492 20 1.4181 2.5089 1.0908
0.9550
40 −0.2571 0.2545 0.5117
0.9520 40 1.6013 2.3729 0.7715
0.9540
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
μ
pl(μ)
μ=0, σ=1, λ=1
n=10
n=20
n=40
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
μ
pl(μ)
μ=2, σ=2, λ=5
n=10
n=20
n=40
Fig. 1. Graphs of plausibility functions of μ based on simulated data when σ and λ
are known.
Remark 3. From the above proof, we can see that bel(A) = 0 for any single-
point assertion A. It also holds in the following theorems.
3.2
The Plausibility Function and Plausibility Region for σ
3.2.1
Location Parameter μ Is Known
Let X ∼(X1, · · · , Xn)′ be a random sample from SN(μ, σ2, λ) with unknown σ
and known μ and λ.
A-step: From Sect. 2.1, we know that
Xi−μ
σ
i.i.d.
∼
SN(λ), i = 1, · · · , n. So if
we deﬁne a statistic T =
n
i=1
(Xi −μ)2, then we have
T
σ2 ∼χ2
n. It gives us an
association
T
σ2 = F −1
2
(U),

Plausibility Regions on Parameters of the Skew Normal Distribution
295
where F2 is the c.d.f. of χ2
n and U ∼Uniform(0, 1). For any observation t > 0
and u ∈[0, 1], we have
Θt(σ) =

t
F −1
2
(u)

.
P-step: To predict auxiliary variables U, we use the default predictive random
set again
S(U) = [0.5 −|U −0.5|, 0.5 + |U −0.5|] .
C-step: By the P-step, we have
Θt(S) =

t
F −1
2
(0.5 + |U −0.5|),

t
F −1
2
(0.5 −|U −0.5|)

.
Based on this IM, we have the below result.
Theorem 2. For any assertion A = {σ},
belt(A; S) = 0,
plt(A; S) = 1 −
2F2
 t
σ2

−1
,
and the 100(1 −α)% plausibility region Πt(σ) is

t
F −1
2
	
1 −α
2

 < σ <

t
F −1
2
	 α
2

.
Proof.
plt(A; S) = 1 −belt(Ac; S)
= 1 −PS(Θt(S) ⊆Ac)
= 1 −PU

t
F −1
2
(0.5 −|U −0.5|)
< σ

−PU

t
F −1
2
(0.5 + |U −0.5|)
> σ

= 1 −PU

|U −0.5| <
F2
 t
σ2

−0.5


= 1 −
2F2
 t
σ2

−1
.
By the deﬁnition, Πt(σ) = {σ : plt(σ; S) > α}. Let plt(A; S) = 1−
2F2
	 t
σ2

−
1
 > α and then solve it for σ. We have

t
F −1
2 (1−α
2 ) < σ <

t
F −1
2 ( α
2 ).
□
In Table 2 and Fig. 2, we provide a simulation study of this case with diﬀerent
σ, μ, λ and sample size n.
3.2.2
Location Parameter μ Is Unknown
Let X ∼(X1, · · · , Xn)′ be a random sample from SN(μ, σ2, λ) with unknown σ
and μ and known λ.

296
X. Zhu et al.
Table 2. Simulation results of AL, AU, ALength and CP of 95% plausibility regions
of σ when μ and λ are known.
σ = 1, μ = 0, λ = 1
σ = 2, μ = 1, λ = 5
n
AL
AU
ALength CP
n
AL
AU
ALength CP
10 0.6803 1.7088 1.0285
0.9477 10 1.3597 3.4152 2.0555
0.9501
20 0.7562 1.4273 0.6711
0.9528 20 1.5086 2.8477 1.3390
0.9519
40 0.8168 1.2729 0.4561
0.9517 40 1.6343 2.5471 0.9127
0.9532
0.5
1.0
1.5
2.0
2.5
0.0
0.2
0.4
0.6
0.8
1.0
σ
pl(σ)
σ=1, μ=0, λ=1
n=10
n=20
n=40
1.0
1.5
2.0
2.5
3.0
3.5
4.0
0.0
0.2
0.4
0.6
0.8
1.0
σ
pl(σ)
σ=2, μ=1, λ=5
n=10
n=20
n=40
Fig. 2. Graphs of plausibility functions of σ based on simulated data when μ and λ
are known.
A-step: In this case, let S2 be the sample variance, i.e., S2 =
1
n−1
n
i=1
(Xi −¯X)2.
By Proposition 1, we know that
(n −1)S2
σ2
∼CSχ2
n−1(0, 0n, λ2(In −¯Jn), In + λ2¯Jn).
So one association is
(n −1)S2
σ2
= F −1
3
(U),
where F3 is the c.d.f. of CSχ2
n−1(0, 0n, λ2(In −¯Jn), In + λ2¯Jn) and U ∼
Uniform(0, 1). For any observation s2 > 0 and u ∈[0, 1], we have
Θs2(σ) =

(n −1)s2
F −1
3
(u)

.
P-step: To predict auxiliary variables U, let’s still use the default predictive
random set
S(U) = [0.5 −|U −0.5|, 0.5 + |U −0.5|] .
C-step: By the P-step, we have
Θs2(S) =

(n −1)s2
F −1
3
(0.5 + |U −0.5|),

(n −1)s2
F −1
3
(0.5 −|U −0.5|)

.

Plausibility Regions on Parameters of the Skew Normal Distribution
297
Thus, the following result can be obtained.
Theorem 3. For any assertion A = {σ},
bels2(A; S) = 0,
pls2(A; S) = 1 −
2F3
(n −1)s2
σ2

−1
,
and the 100(1 −α)% plausibility region Πs2(σ) is

(n −1)s2
F −1
3
	
1 −α
2

 < σ <

(n −1)s2
F −1
3
	 α
2

 .
Proof.
pls2(A; S) = 1 −bels2(Ac; S)
= 1 −PS(Θs2(S) ⊆Ac)
= 1 −PU

(n −1)s2
F −1
3
(0.5 −|U −0.5|)
< σ

−PU

(n −1)s2
F −1
3
(0.5 + |U −0.5|)
> σ

= 1 −PU

|U −0.5| <
F3
(n −1)s2
σ2

−0.5


= 1 −
2F3
 (n −1)s2
σ2

−1
.
By the deﬁnition, Πs2(σ) = {σ : pls2(σ; S) > α}. Let pls2(A; S) =
1 −
2F3

s2
σ2

−1
 > α and then solve it for σ. We have

(n−1)s2
F −1
3 (1−α
2 ) < σ <

(n−1)s2
F −1
3 ( α
2 ).
□
In Table 3 and Fig. 3, we provide a simulation study of this case with diﬀerent
σ, μ, λ and sample size n.
3.3
Simulation Study
In this subsection, we perform several simulation studies to compare average
lower bounds (AL), average upper bounds (AU), average lengths (ALength) and
coverage probabilities (CP) of 95% plausibility regions in three cases discussed
above. We choose sample sizes of 10, 20 and 40 for ﬁrst two cases and sample
sizes of 3, 5 and 7 for the third case due to the diﬃculty of computations (See
Remark 4 for more details). For each sample size, we simulated 10000 times for
diﬀerent parameters. We also provide graphs of plausibility functions based on
simulated data with diﬀerent parameters and diﬀerent sample sizes.
Remark 4. In this simulation study, we used n-dimensional spherical coordi-
nates to derive the density function of closed skew chi-square distributions. How-
ever, our result is not eﬃcient for computations. We are trying to derive a more
concise expression of the density function.

298
X. Zhu et al.
Table 3. Simulation results of AL, AU, ALength and CP of 95% plausibility regions
of σ when μ is unknown but λ is known.
σ = 1, μ = 0, λ = 1
σ = 2, μ = 1, λ = 5
n AL
AU
ALength CP
n AL
AU
ALength CP
3
0.4607 5.6140 5.1533
0.9488 3
0.8574 11.8638 11.0063
0.9475
5
0.5614 2.7225 2.1611
0.9494 5
1.0595 5.8163
4.7568
0.9484
7
0.6246 2.1140 1.4893
0.9505 7
1.1744 4.5305
3.356
0.9509
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
σ
pl(σ)
σ=0, μ=0, λ=1
n=3
n=5
n=7
1
2
3
4
5
6
7
8
0.0
0.2
0.4
0.6
0.8
1.0
σ
pl(σ)
σ=2, μ=1, λ=5
n=3
n=5
n=7
Fig. 3. Graphs of plausibility functions of σ based on simulated data when μ is unknown
but λ is known.
4
Two-Sample Case
Let X = (X1, · · · , Xn1)′ and Y = (Y1, · · · , Yn2)′ be independent random samples
from SN(μ1, σ2
1, λ1) and SN(μ2, σ2
2, λ2), respectively. In this section, we are
going to use X and Y to estimate the diﬀerence Δμ = μ1 −μ2 when all other
parameters are known.
A-step: By Proposition 2, we have
¯X −¯Y ∼CSN1,n1+n2

Δμ, σ2
1
n1
+ σ2
2
n2
, D⋆, Δ⋆

,
where D⋆and Δ⋆are given by Proposition 2. So we can obtain an association
¯X −¯Y = Δμ + F −1
4
(U),
where U ∼Uniform(0, 1) and F4 is the c.d.f. of CSN1,n1+n2

0, σ2
1
n1 + σ2
2
n2 , D⋆, Δ⋆

.
For any observation ¯x −¯y ∈R and u ∈[0, 1], we have Θ¯x−¯y(Δμ) = {¯x −¯y −
F −1
4
(u)}.
P-step: To predict auxiliary variables U, we use the default predictive random
set
S(U) = [0.5 −|U −0.5|, 0.5 + |U −0.5|] .

Plausibility Regions on Parameters of the Skew Normal Distribution
299
Table 4. Simulation results of AL, AU, ALength and CP of 95% plausibility regions
for Δμ with diﬀerent μi, σi, λi and sample sizes ni, i = 1, 2.
μ1 = 0, σ1 = 1, λ1 = 1
μ1 = 1, σ1 = 2, λ1 = 2
μ2 = 0, σ2 = 1, λ2 = 1
μ2 = 0, σ2 = 1, λ2 = 1
n1 = n2
AL
AU
ALength
CP
n1 = n2
AL
AU
ALength
CP
10
−0.7265
0.7208
1.4473
0.9470
10
−0.0296
1.9862
2.0159
0.9532
15
−0.5892
0.5927
1.1819
0.9513
15
0.1725
1.8188
1.6463
0.9501
20
−0.5094
0.5139
1.0233
0.9550
20
0.2754
1.7008
1.4253
0.9509
μ1 = 2, σ1 = 2, λ1 = 1
μ1 = 2, σ1 = 2, λ1 = 1
μ2 = 1, σ2 = 2, λ2 = 2
μ2 = 0, σ2 = 1, λ2 = 1
n1 = n2
AL
AU
ALength
CP
n1 = n2
AL
AU
ALength
CP
10
−0.3435
2.3422
2.6857
0.9554
10
0.8559
3.1443
2.2887
0.9500
15
−0.0827
2.1100
2.1927
0.9482
15
1.0634
2.9320
1.8686
0.9512
20
0.0782
1.9583
1.8801
0.9478
20
1.1826
2.8011
1.6184
0.9510
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
0.0
0.2
0.4
0.6
0.8
1.0
μ1 −μ2
pl
μ1=0, σ1=1, λ1=1
μ2=0, σ2=1, λ2=1
n1=n2=10
n1=n2=15
n1=n2=20
μ1=0, σ1=1, λ1=1
μ2=0, σ2=1, λ2=1
n1=n2=10
n1=n2=15
n1=n2=20
−0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
μ1 −μ2
pl
μ1=1, σ1=2, λ1=2
μ2=0, σ2=1, λ2=1
n1=n2=10
n1=n2=15
n1=n2=20
μ1=1, σ1=2, λ1=2
μ2=0, σ2=1, λ2=1
n1=n2=10
n1=n2=15
n1=n2=20
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
μ1 −μ2
pl
μ1=2, σ1=2, λ1=1
μ2=1, σ2=2, λ2=2
n1=n2=10
n1=n2=15
n1=n2=20
0
1
2
3
4
0.0
0.2
0.4
0.6
0.8
1.0
μ1 −μ2
pl
μ1=2, σ1=2, λ1=1
μ2=0, σ2=1, λ2=1
n1=n2=10
n1=n2=15
n1=n2=20
Fig. 4. Graphs of plausibility functions of Δμ based on simulated data with diﬀerent
μi, σi, λi and sample sizes ni, i = 1, 2.

300
X. Zhu et al.
C-step: By the P-step, we have
Θ¯x−¯y(S) =

¯x −¯y −F −1
4
(0.5 + |U −0.5|), ¯x −¯y −F −1
4
(0.5 −|U −0.5|)

.
So we can use the above IM to get the following result.
Theorem 4. For any assertion A = {Δμ},
bel¯x−¯y(A; S) = 0,
pl¯x−¯y(A; S) = 1 −|2F4(¯x −¯y −Δμ) −1|,
and the 100(1 −α)% plausibility region Π¯x−¯y(Δμ) is
¯x −¯y −F −1
4

1 −α
2

< Δμ < ¯x −¯y −F −1
4
α
2

.
The proof of Theorem 4 is similar to the proof of Theorem 1. In Table 4 and
Fig. 4, we provide a simulation study for this case with diﬀerent μi, σi, λi and
sample sizes ni, i = 1, 2.
5
One Example
Lastly, we use one real data example to illustrate our results.
Example 5.1. The data set was obtained from a study of leaf area index
(LAI) of robinnia pseudoscacia in the Huaiping forest farm of Shannxi Province
from June to October in 2010 (with permission of data owners). The LAI
is given in Table 5 below. By [13], the LAI is approximately distributed as
SN(1.2585, 1.83322, 2.7966) via the method of moment estimation. We use
Theorems 1 and 2 to explore the data again. For the location parameter μ, we
assume MME ˆσ = 1.8332 and ˆλ = 2.7966 to be true parameters and then apply
Theorem 1 to the data. The estimation of μ by IMs is 1.2602. Similarly, for the
scale parameter σ, we assume MME ˆμ = 1.2585 and ˆλ = 2.7966 to be true para-
meters and then apply Theorem 2 to the data. The estimation of σ by IMs is
1.8355. They are roughly equal to MME’s. The corresponding graphs of plausi-
bility functions of μ and σ are given in Fig. 5.
0.8
1.0
1.2
1.4
1.6
1.8
2.0
0.0
0.2
0.4
0.6
0.8
1.0
μ
Plausibity
Maximum at 1.260156
1.4
1.6
1.8
2.0
2.2
2.4
2.6
0.0
0.2
0.4
0.6
0.8
1.0
σ
Plausibity
Maximum at 1.83548
Fig. 5. Graphs of plausibility functions of μ and σ for Example 5.1.

Plausibility Regions on Parameters of the Skew Normal Distribution
301
Table 5. The observed values of LAI.
LAI
June July September October
4.87
3.32
2.05
1.50
5.00
3.02
2.12
1.46
4.72
3.28
2.24
1.55
5.16
3.63
2.56
1.27
5.11
3.68
2.67
1.26
5.03
3.79
2.61
1.37
5.36
3.68
2.42
1.87
5.17
4.06
2.58
1.75
5.56
4.13
2.56
1.81
4.48
2.92
1.84
1.98
4.55
3.05
1.94
1.89
4.69
3.02
1.95
1.71
2.54
2.78
2.29
1.29
3.09
2.35
1.94
1.34
2.79
2.40
2.20
1.29
3.80
3.28
1.56
1.10
3.61
3.45
1.40
1.04
3.53
2.85
1.36
1.08
2.51
3.05
1.60
0.86
2.41
2.78
1.50
0.70
2.80
2.72
1.88
0.82
3.23
2.64
1.63
1.19
3.46
2.88
1.66
1.24
3.12
3.00
1.62
1.14
Acknowledgments. We would like to thank Professor Hung T. Nguyen for introduc-
ing this interesting topic to us and referees for their valuable comments and suggestions
which improve this paper.
References
1. Azzalini, A.: A class of distributions which includes the normal ones. Scand. J.
Stat. 12(2), 171–178 (1985)
2. Azzalini, A.: The Skew-Normal and Related Families, vol. 3. Cambridge University
Press, Cambridge (2013)
3. Azzalini, A., Capitanio, A.: Statistical applications of the multivariate skew normal
distribution. J. R. Stat. Soc. 61(3), 579–602 (1999)

302
X. Zhu et al.
4. Azzalini, A., Dalla, V.A.: The multivariate skew-normal distribution. Biometrika
83(4), 715–726 (1996)
5. Gonz´alez-Far´ıas, G., Dom´ınguez-Molina, A., Gupta, A.K.: Additive properties of
skew normal random vectors. J. Stat. Plan. Infer. 126(2), 521–534 (2004)
6. Martin, R.: Random sets and exact conﬁdence regions. Sankhya A 76(2), 288–304
(2014)
7. Martin, R., Lingham, R.T.: Prior-free probabilistic prediction of future observa-
tions. Technometrics 58(2), 225–235 (2016)
8. Martin, R., Liu, C.: Inferential models: a framework for prior-free posterior prob-
abilistic inference. J. Am. Stat. Assoc. 108(501), 301–313 (2013)
9. Martin, R., Liu, C.: Inferential Models: Reasoning with Uncertainty, vol. 145. CRC
Press, New York (2015)
10. Tian, W., Wang, T.: Quadratic forms of reﬁned skew normal models based on
stochastic representation. Random Oper. Stochast. Equ. 24(4), 225–234 (2016)
11. Wang, T., Li, B., Gupta, A.K.: Distribution of quadratic forms under skew normal
settings. J. Multivar. Anal. 100(3), 533–545 (2009)
12. Wang, Z., Wang, C., Wang, T.: Estimation of location parameter in the skew
normal setting with known coeﬃcient of variation and skewness. Int. J. Intell.
Technol. Appl. Stat. 9(3), 191–208 (2016)
13. Ye, R., Wang, T.: Inferences in linear mixed models with skew-normal random
eﬀects. Acta Math. Sin. English Ser. 31(4), 576–594 (2015)
14. Ye, R., Wang, T., Gupta, A.K.: Distribution of matrix quadratic forms under
skew-normal settings. J. Multivar. Anal. 131, 229–239 (2014)
15. Zhu, X., Ma, Z., Wang, T., Teetranont, T.: Plausibility regions on the skewness
parameter of skew normal distributions based on inferential models. In: Robust-
ness in Econometrics, pp. 267–286. Springer (2017)

Measures of Mutually Complete Dependence
for Discrete Random Vectors
Xiaonan Zhu1, Tonghui Wang1(B), S. T. Boris Choy2,
and Kittawit Autchariyapanitkul3
1 Department of Mathematical Sciences, New Mexico State University,
Las Cruces, USA
{xzhu,twang}@nmsu.edu
2 Discipline of Business Analytics, The University of Sydney, Sydney, Australia
boris.choy@sydney.edu.au
3 Faculty of Economics, Maejo University, Chiang Mai, Thailand
Kittawit a@mju.ac.th
Abstract. In this paper, a marginal-free measure of mutually complete depen-
dence for discrete random vectors through subcopulas is deﬁned, which gener-
alizes the corresponding results for discrete random variables. Properties of the
measure are studied and an estimator of the measure is introduced. Several exam-
ples are given for illustration of our results.
Keywords: Discrete random vector · Mutually complete dependence
Dependence measure · Subcopula
1
Introduction
Complete dependence (or functional dependence) is an important concept in many
aspects of our life, such as econometrics, insurance, ﬁnance, etc. Recently, measures
of (mutually) complete dependence have been deﬁned and studied by many authors,
e.g. R´enyi [8], Schweizer and Wolff [9], Lancaster [5], Siburg and Stoimenov [12],
Trutschnig [16], Dette et al. [3], Tasena and Dhompongsa [14], Shan et al. [11], Tasena
and Dhompongsa [15] and Boonmee and Tasena [2]. However, measures in above
papers have some drawbacks. Some measures only work for continuous random vari-
ables or vectors and some measures rely on marginal distributions (See Sect. 2 for a
summary of several important measures). To the best of our knowledge, none of pre-
viously proposed measures are marginal-free and can describe (mutually) complete
dependence for discrete random vectors. To overcome this issue, in this paper, we deﬁne
a marginal-free measure of (mutually) complete dependence for discrete random vec-
tors by using subcopluas, which extends the corresponding results of discrete random
variables given in [11] to multivariate cases.
This paper is organized as follows. Some necessary concepts and deﬁnitions, and
several measures of (mutually) complete dependence are reviewed brieﬂy in Sect. 2. A
marginal-free measure of (mutually) complete dependence for discrete random vectors
is deﬁned and properties of this measure are studied in Sect. 3. An estimator of the
measure is introduced in Sect. 4.
c⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_22

304
X. Zhu et al.
2
Preliminaries
Let (Ω,A ,P) be a probability space, where Ω is a sample space, A is a σ-algebra of Ω
and P is a probability measure on A . A random variable is a measurable function from
Ω to the real line R, and for any integer n ≥2, an n-dimensional random vector is a
measurable function from Ω to Rn. For any a = (a1,··· ,an) and b = (b1,··· ,bn) ∈Rn,
we say a ≤b if and only if ai ≤bi for all i = 1,··· ,n. Let X and Y be random vectors
deﬁned on the same probability space. X and Y are said to be independent if and only if
P(X ≤x,Y ≤y) = P(X ≤x)P(Y ≤y) for all x and y. Y is completely dependent (CD) on
X if Y is a measurable function of X almost surely, i.e., there is a measurable function
φ such that P(Y = φ(X)) = 1. X and Y are said to be mutually completely dependent
(MCD) if X and Y are completely dependent on each other.
Let E1,··· ,En be nonempty subsets of R and Q a real-valued function with the
domain Dom(Q) = E1×···×En. Let [a,b] = [a1,b1]×···×[an,bn] such that all vertices
of [a,b] belong to Dom(Q). The Q-volume of [a,b] is deﬁned by
VQ([a,b]) = ∑sgn(c)Q(c),
where the sum is taken over all vertices c = (c1,··· ,cn) of [a,b], and
sgn(c) =

1,
if ci = ai for an even number of i′s,
−1,
if ci = ai for an odd number of i′s.
An n-dimensional subcopula (or n-subcopula for short) is a function C with the follow-
ing properties [7].
(i) The domain of C is Dom(C) = D1 × ··· × Dn, where D1,··· ,Dn are nonempty
subsets of the unit interval I = [0,1] containing 0 and 1;
(ii) C is grounded, i.e., for any u = (u1,··· ,un) ∈Dom(C), C(u) = 0 if at least one
ui = 0;
(iii) For any ui ∈Di, C(1,··· ,1,ui,1,··· ,1) = ui, i = 1,··· ,n;
(iv) C is n-increasing, i.e., for any u, v ∈Dom(C) such that u ≤v, VC([u,v]) ≥0.
For any n random variables X1,··· ,Xn, by Sklar’s Theorem [13], there is a unique
n-subcopula such that
H(x1,··· ,xn) = C(F1(x1),··· ,Fn(xn)),
for all (x1,··· ,xn) ∈R
n,
where R = R ∪{−∞,∞}, H is the joint cumulative distribution function (c.d.f.) of
X1,··· ,Xn, and Fi is the marginal c.d.f. of Xi, i = 1,··· ,n. In addition, if X1,··· ,Xn are
continuous, then Dom(C) = In and the unique C is called the n-copula of X1,··· ,Xn.
For more details about the copula theory, see [7].
Next, we are going to recall some measures of MCD and CD, which are equal to
0 if and only if two random variables (or vectors) are independent, and equal to 1 if

Measures of Mutually Complete Dependence for Discrete Random Vectors
305
and only if they are MCD or CD. In 2010, Siburg and Stoimenov [12] deﬁned an MCD
measure for continuous random variables as
ω(X,Y) =

3∥C∥2 −2
 1
2 ,
(1)
where X and Y are continuous random variables with the copula C and ∥· ∥is the
Sobolev norm of bivariate copulas given by
∥C∥=
 
|∇C(u,v)|2 dudv
 1
2
,
where ∇C(u,v) is the gradient of C(u,v).
In 2013, Tasena and Dhompongsa [14] generalized Siburg and Stoimenov’s mea-
sure to multivariate cases as follows. Let X1,··· ,Xn be continuous variables with the
n-copula C. Deﬁne
δi(X1,··· ,Xn) = δi(C) =
 ···
 [∂iC(u1,··· ,un)−πiC(u1,··· ,un)]2 du1 ···dun
 ···
 πiC(u1,··· ,un)(1−πiC(u1,··· ,un))du1 ···dun
,
where ∂iC is the partial derivative on the ith coordinate ofC and πiC : In−1 →I is deﬁned
by πiC(u1,··· ,un−1) = C(u1,··· ,ui−1,1,ui,··· ,un−1), i = 1,2,··· ,n. Let
δ(X1,··· ,Xn) = δ(C) = 1
n
n
∑
i=1
δi(C).
(2)
Then δ is an MCD measure of X1,··· ,Xn.
In 2015, Shan et al. [11] considered discrete random variables. Let X and Y be two
discrete random variables with the subcopula C. An MCD measure of X and Y is given
by
μt(X,Y) =
∥C∥2
t −Lt
Ut −Lt
 1
2
,
(3)
where t ∈[0,1] and ∥C∥2
t is the discrete norm of C deﬁned by
∥C∥2
t = ∑
i ∑
j
	
tC2
Δi,j +(1−t)C2
Δi,j+1
 Δvj
Δui
+

tC2
i,Δ j +(1−t)C2
i+1,Δ j
 Δui
Δvj

,
CΔi, j = C(ui+1,vj)−C(ui,vj),
Ci,Δ j = C(ui,vj+1)−C(ui,vj),
Δui = ui+1 −ui,
Δvj = vj+1 −vj,
Lt = ∑
i
(tu2
i +(1−t)u2
i+1)Δui +∑
j
(tv2
j +(1−t)v2
j+1)Δvj,
and
Ut = ∑
i
(tui +(1−t)ui+1)Δui +∑
j
(tvj +(1−t)vj+1)Δvj.

306
X. Zhu et al.
In 2016 Tasena and Dhompongsa [15] deﬁned a measure of CD for random vectors.
Let X and Y be two random vectors. Deﬁne
ωk(Y|X) =
  FY|X(y|x)−1
2

k
dFX(x)dFY(y)
 1
k
,
where k ≥1. The measure of Y CD on X is given by
ωk(Y|X) =

ωk
k(Y|X)−ωk
k(Y ⊥|X⊥)
ωk
k(Y|Y)−ωk
k(Y ⊥|X⊥)
 1
k
,
(4)
where X⊥and Y ⊥are independent random vectors with the same distributions as X and
Y, respectively.
In the same period, Boonmee and Tasena [2] deﬁned a measure of CD for contin-
uous random vectors by using linkages which were introduced by Li et al. [6]. Let X
and Y be two continuous random vectors with the linkage C. The measure of Y being
completely dependent on X is deﬁned by
ζp(Y|X) =
   ∂
∂uC(u,v)−Π(v)

p
dudv
 1
p
,
(5)
where Π(v) =
n
Π
i=1vi for all v = (v1,··· ,vn) ∈In.
From above summaries we can see that measures given by (1), (2) and (5) only work
for continuous random variables or vectors. The measure deﬁned by (3) only works
for bivariate discrete random variables. The measure given by (4) relies on marginal
distributions of random vectors. Thus it is worth considering marginal-free measures of
CD and MCD for discrete random vectors.
3
An MCD Measure for Discrete Random Vectors
In this section, we identify Rn1+···+nk with Rn1 ×···×Rnk, where n1,··· ,nk are positive
integers. So any n-dimensional random vector X can be viewed as a tuple of n random
variables, i.e., X = (X1,··· ,Xn), where X1,··· ,Xn are random variables. Also, if Y =
(Y1,··· ,Ym) is an m-dimensional random vector, we use (X,Y) to denote the (n + m)-
dimensional random vector (X1,··· ,Xn,Y1,··· ,Ym). Let ψ = (ψ1,··· ,ψn) : Rn →Rn be
a function. ψ is said to be strictly increasing if and only if each component ψi : R →R is
strictly increasing, i.e., for any ai and bi ∈R such that ai < bi, we have ψi(ai) < ψi(bi),
i = 1,··· ,n.
We will focus on discrete random vectors in this section, i.e., they can take on
at most a countable number of possible values. Let X = (X1,··· ,Xn) ∈L1 ⊆Rn and
Y = (Y1,··· ,Ym) ∈L2 ⊆Rm be two discrete random vectors deﬁned on the same prob-
ability space (Ω,A ,P). Their joint c.d.f. H, marginal c.d.f.’s F and G, and marginal
probability mass functions (p.m.f.) f and g are deﬁned, respectively, as follows.

Measures of Mutually Complete Dependence for Discrete Random Vectors
307
H(x,y) = P(X ≤x,Y ≤y),
F(x) = P(X ≤x),
G(y) = P(Y ≤y),
f(x) = P(X = x),
and
g(y) = P(Y = y),
for allx ∈Rnandy ∈Rm.
Also, we use Fi and fi to denote the marginal c.d.f. and p.m.f. of the ith component Xi
of X, i = 1,··· ,n, and use Gj and gj to denote the marginal c.d.f. and p.m.f. of the jth
component Yj of Y, j = 1,··· ,m, respectively. For simplicity, we assume that f(x) ̸= 0
and g(y) ̸= 0 for all x ∈L1 and y ∈L2. If C is the subcopula of the (n+m)-dimensional
random vector (X,Y), i.e.,
H(x,y) = C(u(x),v(y)),
for allx ∈R
n andy ∈R
m,
where
u(x) = (u1(x1),··· ,un(xn)) = (F1(x1),··· ,Fn(xn)) ∈In,
and
v(y) = (v1(y1),··· ,vm(ym)) = (G1(y1),··· ,Gm(ym)) ∈Im,
for all x ∈L1 and y ∈L2, then C is said to be the subcopula of X and Y. In addition, for
each vector e ∈E1 ×···×En, where Ei is a countable subset of R, let eL be the greatest
lower bound of e with respect to the coordinate-wise order, i.e., eL = (e′
1,··· ,e′
n) such
that if there exists some element in Ei that is less than ei, then e′
i is the greatest element
in Ei so that e′
i < ei, otherwise e′
i = ei, i = 1,··· ,n. We use 1n and ∞n to denote the
n-dimensional constant vector (1,··· ,1) and (∞,··· ,∞) ∈R
n.
To construct desired measures, a distance between two discrete random vectors is
deﬁned as follows.
Deﬁnition 1. Let X and Y be discrete random vectors. The distance between the con-
ditional distribution of Y given X and marginal distribution of Y is deﬁned by
ω2(Y|X) = ∑
y∈L2, ∑
x∈L1
[P(Y ≤y|X = x)−G(y)]2 f(x)g(y).
(6)
From the above deﬁnition, we can obtain the following two results.
Lemma 1. For any discrete random vectors X and Y, we have ω2(Y|X) ≤ω2
max(Y|X),
where
ω2
max(Y|X) = ∑
y∈L2

G(y)−(G(y))2
g(y).
Proof. By the deﬁnition,
ω2(Y|X) = ∑
y∈L2, ∑
x∈L1
[P(Y ≤y|X = x)−G(y)]2 f(x)g(y)
= ∑
y∈L2, ∑
x∈L1
[P(Y ≤y|X = x)2 −2P(Y ≤y|X = x)G(y)+(G(y))2]f(x)g(y)
≤∑
y∈L2, ∑
x∈L1
[P(Y ≤y|X = x)−2P(Y ≤y|X = x)G(y)+(G(y))2]f(x)g(y)

308
X. Zhu et al.
= ∑
y∈L2, ∑
x∈L1
[P(X = x,Y ≤y)−2P(X = x,Y ≤y)G(y)+(G(y))2 f(x)]g(y)
= ∑
y∈L2

G(y)−(G(y))2
g(y).
□
Lemma 2. Let X and Y be discrete random vectors. There is a function φ : L1 →L2
such that φ(X) = Y if and only if ω2(Y|X) = ω2
max(Y|X), i.e., P(Y ≤y|X = x) = 0 or 1
for all (x,y) ∈L1 ×L2.
Proof. For “if” part, suppose that P(Y ≤y|X = x) = 0 or 1 for all (x,y) ∈L1 × L2.
Then ∑
t≤y
P(Y = t|X = x) = 0 or 1. So ∑
t≤y
P(X = x,Y = t) = 0 or P(X = x). Thus there
exists a unique y(x) ∈L2, which depends on x, such that P(X = x,Y = y(x)) = P(X =
x), and P(X = x,Y = y) = 0 for all y ∈L2 with y ̸= y(x). Now if we deﬁne φ(x) = y(x)
for all x ∈L1, then φ(X) = Y.
For “only if” part, suppose that φ(X) = Y. Fix x ∈L1. It is sufﬁcient to show that
P(X = x,Y = y) = 0 for all y ̸= φ(x). Suppose that, on the contrary, there is y′ ∈L2
such that y′ ̸= φ(x) and P(X = x,Y = y′) ̸= 0, then there exists ω ∈Ω so that X(ω) = x
and Y(ω) = y′. So we have φ(X)(ω) ̸= Y(ω). It’s a contradiction.
□
Now we can deﬁne a measure of CD for two discrete random vectors as follows.
Deﬁnition 2. For any discrete random vectors X and Y, the measure of Y being com-
pletely dependent on X is given by
μ(Y|X) =
 ω2(Y|X)
ω2max(Y|X)
 1
2
=
⎡
⎢⎣
∑
y∈L2,
∑
x∈L1
[P(Y ≤y|X = x)−G(y)]2 f(x)g(y)
∑
y∈L2

G(y)−(G(y))2
g(y)
⎤
⎥⎦
1
2
.
(7)
Properties of the measure μ(Y|X) are given as follows.
Theorem 1. For any discrete random vectors X and Y, the measure μ(Y|X) has the
following properties:
(i) 0 ≤μ(Y|X) ≤1;
(ii) μ(Y|X) = 0 if and only if X and Y are independent;
(iii) μ(Y|X) = 1 if and only if Y is a function of X;
(iv) μ(Y|X) is invariant under strictly increasing transformations of X and Y, i.e., if
ψ1 and ψ2 are strictly increasing functions deﬁned on L1 and L2, respectively,
then μ (ψ2(Y)|ψ1(X)) = μ(Y|X).
Proof. Property (i) is obvious by Lemma 1. For Property (ii), note that μ(Y|X) = 0
if and only if ω2(Y|X) = 0. It is equivalent to P(Y ≤y|X = x) = G(y) for all (x,y) ∈
L1×L2, i.e., X andY are independent. Property (iii) follows Lemma 2. Lastly, since ψ1
and ψ2 are strictly increasing, we have P(ψ1(X) ≤ψ1(x)) = P(X ≤x) and P(ψ2(Y) ≤
ψ2(y)) = P(Y ≤y). Thus Property (iv) holds.
□

Measures of Mutually Complete Dependence for Discrete Random Vectors
309
Remark 1. (i) It can be shown that the measure given by (7) is a discrete version of
the measure ωk(Y|X) for random vectors given by (4) with k = 2. The difference
here is that based on (7), we are going to deﬁne a marginal-free measure by using
subcopulas for discrete random vectors.
(ii) The above measure may be simpliﬁed into
μ′(Y|X) =
⎡
⎢⎣
∑
y∈L2, ∑
x∈L1
[P(Y ≤y|X = x)−G(y)]2 f(x)
∑
y∈L2
[G(y)−(G(y))2]
⎤
⎥⎦
1
2
.
(8)
As indicated by Shan [10], μ′(Y|X) is well deﬁned only if Y is a ﬁnite discrete
random vector, i.e., if L2 is a ﬁnite set. Otherwise,
∑
y∈L2

G(y)−(G(y))2
may
diverge. However, the measure μ(Y|X) given by (7) is well deﬁned for all discrete
random vectors.
(iii) It is easy to see that ω2
max(Y|X) = 0 if and only if Y is a constant random vector,
i.e., if and only if there is y ∈Rm such that P(Y = y) = 1. In this case, Y is clearly
a function of X. Thus, without loss of generality, we assume that X and Y are not
constant random vectors.
Since most multivariate dependence properties of random variables can be deter-
mined by their subcopula C, we are going to redeﬁne the measure μ(Y|X) by using
subcopulas such that μ(Y|X) is free of marginal distributions of X and Y. First, note
that for any x ∈L1 and y ∈L2, we have
G(y) = H(∞n,y) = C(1n,v(y)),
(9)
f(x) = VH([(xL,∞m),(x,∞m)]) = VC([(u(x)L,1m),(u(x),1m)]),
(10)
g(y) = VH([(∞n,yL),(∞n,y)]) = VC([(1n,v(y)L),(1n,v(y))]),
(11)
and
P(Y ≤y|X = x) = P(X = x,Y ≤y)
P(X = x)
=
VH([(xL,y),(x,y)])
VH([(xL,∞m),(x,∞m)])
= VC([(u(x)L,v(y)),(u(x),v(y))])
VC([(u(x)L,1m),(u(x),1m)])
(12)
Thus, from Eqs. (9)–(12), we can redeﬁne μ(Y|X) as follows.

310
X. Zhu et al.
Deﬁnition 3. Let X and Y be two discrete random vectors with the subcopula C. Sup-
pose that the domain of C is Dom(C) = L ′
1 × L ′
2, where L ′
1 ⊆In and L ′
2 ⊆Im. The
measure of Y being completely dependent on X based on C is given by
μC(Y|X) =
 ω2(Y|X)
ω2max(Y|X)
 1
2
=
⎡
⎢⎢⎢⎣
∑
v∈L ′
2
∑
u∈L ′
1

VC([(uL,v),(u,v)])
VC([(uL,1m),(u,1m)]) −C(1n,v)
2
VC([(uL,1m),(u,1m)])VC([(1n,vL),(1n,v)])
∑
v∈L ′
2
[C(1n,v)−(C(1n,v))2]VC([(1n,v),(1n,vL])
⎤
⎥⎥⎥⎦
1
2
.
(13)
Remark 2. Based on the same idea, if X and Y are continuous random vectors with the
unique copula C, the measure μC(Y|X) given by (13) can be rewritten as
μC(Y|X)) =
⎡
⎢⎣
  
∂C
∂CX −CY
2 ∂CX
∂u
∂CY
∂v dudv
 CY(1−CY) ∂CY
∂v dv
⎤
⎥⎦
1
2
,
where CX and CY are copulas of X and Y. This is a marginal-free measure of CD for
continuous random vectors.
By using μC(Y|X) deﬁned in Deﬁnition 3, we can deﬁne a marginal-free measure
of mutual complete dependence for two discrete random vectors as follows.
Deﬁnition 4. For any discrete random vectors X and Y with the subcopula C, the MCD
measure of X and Y is deﬁned by
μC(X,Y) =

ω2(Y|X)+ω2(X|Y)
ω2max(Y|X)+ω2max(X|Y)
 1
2
,
(14)
where ω2(·|·) and ω2
max(·|·) are the same as those given in Deﬁnition 3.
The properties of the measure μC(X,Y) are given in the following theorem. The
proof is straightforward.
Theorem 2. Let X and Y be two discrete random vectors with the subcopula C. The
measure μC(X,Y) has following properties,
(i) μC(X,Y) = μC(Y,X);
(ii) 0 ≤μC(X,Y) ≤1;
(iii) μC(X,Y) = 0 if and only if X and Y are independent;
(iv) μC(X,Y) = 1 if and only if X and Y are MCD;
(v) μC(X,Y) is invariant under strictly increasing transformations of X and Y.
Remark 3. (i) The insufﬁciency of 2-copulas to describe joint distributions with given
multivariate marginal distributions was discussed by Genest et al. [4]. Let C be a
2-copula. They showed that
H(x1,··· ,xn1,xn1+1,··· ,xn1+n2) = C(H1(x1,··· ,xn1),H2(xn1+1,··· ,xn1+n2))

Measures of Mutually Complete Dependence for Discrete Random Vectors
311
deﬁnes a (n1 + n2) dimensional c.d.f., where n1 + n2 ≥3, for all marginal c.d.f.’s
H1 and H2 with dimensions n1 and n2, respectively, only if C is the bivariate inde-
pendence copula, i.e.,
C(u,v) = uv,
for all (u,v) ∈I2.
Thus, in this work, we have to use the (n+m)-subcopula of (X,Y) to construct a
marginal-free measure.
(ii) Both Shan et al. [11] and Tasena and Dhompongsa [15] tried to use copulas to
construct measures of functional dependence for discrete random variables or vec-
tors. However, we do not think that copulas should be used to construct measures
for discrete random variables or vectors because, for ﬁxed discrete random vari-
ables or vectors, the corresponding copulas may not be unique. Thus, as shown in
their papers, if we have different copulas for two ﬁxed discrete random variables,
copula-based measures may give us different results.
(iii) Boonmee and Tasena [2] used linkages to construct a marginal-free measure of
CD for continuous random vectors, but linkages have some defects. First, linkages
are deﬁned for continuous random vectors. Second, to ﬁnd the linkage of two
random vectors, they need to be transformed to uniform random vectors. It is not
convenient in applications (See Li et al. [6] for more details of linkages). Thus,
in this work, we prefer to use the subcopula of (X,Y) to construct marginal-free
measures, since subcopulas are not only good for discrete random vectors but also
more popular than linkages.
(iv) If both X and Y are discrete random variables with the 2-subcopula C, then we
have
ω2(Y|X) = ∑
v∈L ′
2
∑
u∈L ′
1
C(u,v)−C(uL,v)2
u−uL
−v
2
(u−uL)(v−vL),
ω2(X|Y) = ∑
u∈L ′
1
∑
v∈L ′
2
C(u,v)−C(u,vL)2
v−vL
−u
2
(u−uL)(v−vL),
ω2
max(Y|X) = ∑
v∈L ′
2
(v−v2)(v−vL)
and
ω2
max(X|Y) = ∑
u∈L ′
1
(u−u2)(u−uL).
In this case, the measure μC(X,Y) =

ω2(Y|X)+ω2(X|Y)
ω2max(Y|X)+ω2max(X|Y)
 1
2 is identical to the mea-
sure given by (3) with t = 0.
(v) If both X and Y are continuous random variables, i.e., max{u −uL,v −vL} →0,
then it can be show that
μC(X,Y) =

ω2(Y|X)+ω2(X|Y)
ω2max(Y|X)+ω2max(X|Y)
 1
2
=

3
  ∂C
∂u
2
+
∂C
∂v
2
dudv−2
 1
2
,
which is identical to the measure given by (1).

312
X. Zhu et al.
Next, we use two examples to illustrate our above results.
Example 1. Let X = (X1,X2) be a random vector with the distribution given in Table 1.
Let Y = (Y1,Y2) = (X2
1 ,X2
2 ). Then the distribution of Y, the joint distribution of X and Y,
and the corresponding subcopula are given in Tables 2, 3 and 4, respectively. It is easy to
show that ω2(Y|X) = ω2
max(Y|X) = 161/1458, ω2(X|Y) = 2699/38880, ω2
max(X|Y) =
469/2916. So μC(Y|X) = 1. μC(X|Y) = 0.6569 and μC(X,Y) = 0.8142.
Table 1. Distribution of X.
X2
X1
X2
−1
0
1
−1 1/18 2/18 3/18 6/18
0
1/18 2/18 2/18 5/18
1
1/18 3/18 3/18 7/18
X1
3/18 7/18 8/18 1
Table 2. Distribution of Y.
Y2 Y1
Y2
0
1
0
2/18 3/18
5/18
1
5/18 8/18
13/18
Y1 7/18 11/18 1
Table 3. Joint distribution of X and Y.
Y
X
Y
(−1,1) (−1,0) (−1,1) (0,−1) (0,0) (0,1) (1,−1) (1,0) (1,1)
(0,0) 0
0
0
0
2
18
0
0
0
0
2
18
(0,1) 0
0
0
2
18
0
3
18
0
0
0
5
18
(1,0) 0
1
18
0
0
0
0
0
2
18
0
3
18
(1,1)
1
18
0
1
18
0
0
0
3
18
0
3
18
8
18
X
1
18
1
18
1
18
2
18
2
18
3
18
3
18
2
18
3
18
1

Measures of Mutually Complete Dependence for Discrete Random Vectors
313
Table 4. Subcopula of X and Y.
V
U
(1,1)
( 3
18, 6
18) ( 3
18, 11
18) ( 3
18,1) ( 10
18, 6
18) ( 10
18, 11
18) ( 10
18,1) (1, 6
18) (1, 11
18)
( 7
18, 5
18) 0
0
0
0
2
18
2
18
0
2
18
2
18
( 7
18,1)
0
0
0
2
18
4
18
7
18
2
18
4
18
7
18
(1, 5
18)
0
1
18
1
18
0
3
18
3
18
0
5
18
5
18
(1,1)
1
18
2
18
3
18
3
18
6
18
10
18
6
18
11
18
1
Example 2. Let X = (X1,X2) be a discrete random vector, where X1 is a geometric
random variable with the success rate p = 1
2, X2 is a binomial random variable with
the number of trails n = 2 and the success rate p = 1
2, and X1 and X2 are indepen-
dent. Let Y = X1 −X2. Then the joint distribution and subcopula of X and Y are given
in Tables 5 and 6. By calculation, ω2(Y|X) = ω2
max(Y|X) = 1223/7168, ω2(X|Y) =
3407543/30965760 and ω2
max(X|Y) = 1/3. So μC(Y|X) = 1, μC(X|Y) = 0.3301 and
μC(X,Y) = 0.5569.
Table 5. Joint distribution of X and Y.
X
Y
···
X
−1 0
1
2
3
4
5
(1,0) 0
0
1
23
0
0
0
0
1
23
(1,1) 0
1
22
0
0
0
0
0
1
22
(1,2)
1
23
0
0
0
0
0
0
1
23
(2,0) 0
0
0
1
24
0
0
0
1
24
(2,1) 0
0
1
23
0
0
0
0
1
23
(2,2) 0
1
24
0
0
0
0
0
1
24
(3,0) 0
0
0
0
1
25
0
0
1
25
(3,1) 0
0
0
1
24
0
0
0
1
24
(3,2) 0
0
1
25
0
0
0
0
1
25
(4,0) 0
0
0
0
0
1
26
0
1
26
(4,1) 0
0
0
0
1
25
0
0
1
25
(4,2) 0
0
0
1
26
0
0
0
1
26
(5,0) 0
0
0
0
0
0
1
27
1
27
(5,1) 0
0
0
0
0
1
26
0
1
26
(5,2) 0
0
0
0
1
27
0
0
1
27
...
...
Y
1
23
22+1
24
23+1
25
23+1
26
23+1
27
23+1
28
23+1
29
···
1

314
X. Zhu et al.
Table 6. Subcopula of X and Y.
U
V
···
1
23
22+2+1
24
24+22+2+1
25
25+24+22+2+1
26
26+25+24+22+2+1
27
( 1
2, 1
22 )
0
0
1
23
1
23
1
23
( 1
2, 3
22 )
0
1
22
2+1
23
2+1
23
2+1
23
( 1
2,1)
1
23
2+1
23
1
2
1
2
1
2
( 2+1
22 , 1
22 )
0
0
1
23
2+1
24
2+1
24
( 2+1
22 , 3
22 )
0
1
22
1
2
22+1
24
22+1
24
( 2+1
22 ,1)
1
23
22+2+1
24
6+22+1
24
2+1
22
2+1
22
( 22+2+1
22
, 1
22 ) 0
0
1
23
2+1
24
22+2+1
25
( 22+2+1
22
, 3
22 ) 0
1
22
1
2
22+1
23
24+221
25
( 22+2+1
22
,1)
1
23
22+2+1
24
24+22+2+1
25
24+23+2+1
25
22+2+1
23
...
4
Estimators of μ(Y|X) and μ(X,Y)
In this section, we are going to construct estimators of measures μ(Y|X) and μ(X,Y).
Let X ∈L1 and Y ∈L2 be two discrete random vectors and [nxy] be their observed
multi-way contingency table. Suppose that the total number of observation is n. For
every x ∈L1 and y ∈L2, let nxy, nx· and n·y be numbers of observations of (x,y),
x and y, respectively, i.e., nx· =
∑
y∈L2
nxy and n·y =
∑
x∈L1
nxy. If we deﬁne ˆpxy = nxy/n,
ˆpx· = nx·/n, ˆp·y = n·y/n, ˆpy|x = ˆpxy/ ˆpx· = nxy/nx· and ˆpx|y = ˆpxy/ ˆp·y = nxy/n·y, then
estimators of measures μ(Y|X) and μ(X,Y) can be deﬁned as follows.
Deﬁnition 5. Let X ∈L1 and Y ∈L2 be two discrete random vectors with a multi-way
contingency table [nxy]. Estimators of μ(Y|X) and μ(X,Y) are given by
ˆμ(Y|X)
 ˆω2(Y|X)
ˆω2max(Y|X)
 1
2
,
and
ˆμ(X,Y) =

ˆω2(Y|X)+ ˆω2(X|Y)
ˆω2max(Y|X)+ ˆω2max(X|Y)
 1
2
,
where
ˆω2(Y|X) = ∑
y∈L2, ∑
x∈L1

∑
y′≤y,

ˆpy′|x −ˆp·y′
2
ˆpx· ˆp·y,
ˆω2
max(Y|X) = ∑
y∈L2
⎡
⎣∑
y′≤y,
ˆp·y′ −

∑
y′≤y,
ˆp·y′
2⎤
⎦ˆp·y,

Measures of Mutually Complete Dependence for Discrete Random Vectors
315
and ˆω2(X|Y) and ˆω2
max(X|Y) are similarly deﬁned as ˆω2(Y|X) and ˆω2
max(Y|X) by inter-
changing X and Y.
From the above deﬁnition, we have the following result. The proof is trivial.
Theorem 3. Let X and Y be discrete random vectors. Estimators ˆμ(Y|X) and ˆμ(X,Y)
have following properties,
(i) 0 ≤ˆμ(Y|X), ˆμ(X,Y) ≤1;
(ii) X and Y are empirically independent, i.e., ˆpxy = ˆpx· ˆp·y for all (x,y) ∈L1 ×L2 if
and only if ˆμ(Y|X) = ˆμ(X,Y) = 0;
(iii) ˆμ(Y|X) = 1 if and only if Y is a function of X. And ˆμ(X,Y) = 1 if and only if X
and Y are functions of each other.
Next, we use two example to illustrate our results.
Example 3. Suppose that we have the following multi-way contingency tables. Then
from Table 7, we have ˆμ(Y|X) = 0.0516, ˆμ(X|Y) = 0.0762 and ˆμ(X,Y) = 0.0642, so
X andY have very weak functional relations. However, from Table 8, we have ˆμ(Y|X) =
0.5746, ˆμ(X|Y) = 0.0465 and ˆμ(X,Y) = 0.3485, so the functional dependence of Y on
X is much stronger than the functional dependence of X on Y.
Example 4. The data given in Table 9 [1] is from a survey conducted by the Wright
State University School of Medicine and the United Health Services in Dayton, Ohio.
The survey asked students in their ﬁnal year of a high school near Dayton, Ohio,
Table 7. Contingency table of X and Y.
Y
X
n·y
(1,1) (1,2) (2,1) (2,2)
(1,1) 10
20
5
10
45
(1,2) 15
25
10
5
55
(2,1)
5
35
10
5
55
(2,2) 25
5
10
5
45
nx·
55
85
35
25
200
Table 8. Contingency table of X and Y.
Y
X
n·y
(1,1) (1,2) (2,1) (2,2)
(1,1) 43
2
3
40
88
(1,2)
4
42
40
6
92
(2,1)
2
3
3
2
10
(2,2)
1
4
2
3
10
nx·
50
51
48
51
200

316
X. Zhu et al.
Table 9. Alcohol (A), Cigarette (C), and Marijuana (M) use for high school seniors.
Alcohol use Cigarette use Marijuana use
Yes No
Yes
Yes
911 538
No
44 456
No
Yes
3
43
No
2 279
whether they had ever used alcohol, cigarettes, or marijuana. Denote the variables by
A for alcohol use, C for cigarette use, and M for marijuana use. By Pearson’s Chi-
squared test (A,C) and M are not independent. The estimations of functional depen-
dence between M and (A,C) are ˆμ(M|(A,C)) = 0.3097, ˆμ((A,C)|M) = 0.2776 and
ˆμ((A,C),M) = 0.2893.
Acknowledgments. We would like to thank Professor Hung T. Nguyen for introducing this inter-
esting topic to us and referees for their valuable comments and suggestions which greatly improve
this paper.
References
1. Agresti, A., Kateri, M.: Categorical Data Analysis, 2nd edn. Springer, Hoboken (2011)
2. Boonmee, T., Tasena, S.: Measure of complete dependence of random vectors. J. Math. Anal.
Appl. 443(1), 585–595 (2016)
3. Dette, H., Siburg, K.F., Stoimenov, P.A.: A copula-based non-parametric measure of regres-
sion dependence. Scand. J. Stat. 40(1), 21–41 (2013)
4. Genest, C., Quesada Molina, J., Rodr´ıguez Lallena, J.: De l’impossibilit´e de construire
des lois `a marges multidimensionnelles donn´ees `a partir de copules. Comptes rendus de
l’Acad´emie des sciences. S´erie 1, Math´ematique 320(6), 723–726 (1995)
5. Lancaster, H.: Measures and indices of dependence. In: Kotz, S., Johnson, N.L. (eds.) Ency-
clopedia of Statistical Sciences, vol. 2, pp. 334–339. Wiley, New York (1982)
6. Li, H., Scarsini, M., Shaked, M.: Linkages: a tool for the construction of multivariate dis-
tributions with given nonoverlapping multivariate marginals. J. Multivar. Anal. 56(1), 20–41
(1996)
7. Nelsen, R.B.: An Introduction to Copulas, 2nd edn. Springer, New York (2006)
8. R´enyi, A.: On measures of dependence. Acta Math. Hung. 10(3–4), 441–451 (1959)
9. Schweizer, B., Wolff, E.F.: On nonparametric measures of dependence for random variables.
Ann. Stat. 9(4), 879–885 (1981)
10. Shan, Q.: The measures of association and dependence through copulas. Ph.D. dissertation,
New Mexico State University (2015)
11. Shan, Q., Wongyang, T., Wang, T., Tasena, S.: A measure of mutual complete dependence in
discrete variables through subcopula. Int. J. Apporx. Reason. 65, 11–23 (2015)
12. Siburg, K.F., Stoimenov, P.A.: A measure of mutual complete dependence. Metrika 71(2),
239–251 (2010)

Measures of Mutually Complete Dependence for Discrete Random Vectors
317
13. Sklar, A.: Fonctions de r´epartition ´a n dimensions et leurs marges. Publ. Inst. Stat. Univ. Paris
8, 229–231 (1959)
14. Tasena, S., Dhompongsa, S.: A measure of multivariate mutual complete dependence. Int. J.
Apporx. Reason. 54(6), 748–761 (2013)
15. Tasena, S., Dhompongsa, S.: Measures of the functional dependence of random vectors. Int.
J. Apporx. Reason. 68, 15–26 (2016)
16. Trutschnig, W.: On a strong metric on the space of copulas and its induced dependence
measure. J. Math. Anal. Appl. 384(2), 690–705 (2011)

Applications

To Compare the Key Successful Factors When
Choosing a Medical Institutions Among Taiwan,
China, and Thailand
Tzong-Ru (Jiun-Shen) Lee1, Yu-Ting Huang2(B), Man-Yu Huang3,
and Huan-Yu Chen4
1 Department of Marketing, National Chung Hsing University,
No. 145 Xingda Rd., South Dist., Taichung City 402, Taiwan
trlee@dragon.nchu.edu.tw
2 Department of Business Administration, National Taiwan University,
Taipei, Taiwan
huangyt81@ntu.edu.tw
3 National Cheng-Chi University, Taipei, Taiwan
lemonde6201@gmail.com
4 National Chung Hsing University, Taichung, Taiwan
huanychen@gmail.com
Abstract. What are the most important factors that increases cus-
tomers’ (patients’) satisfaction with a general clinic, dental clinic and
cosmetic surgery clinic respectively in Asia? Our paper tries to answer
the question by conducting survey in Taiwan, Thailand, and China (in
the order of the time the survey was conducted), with Grey Relational
Analysis (GRA) methodology applied to identify key successful factors
in three regions. By the research, our paper found speciﬁc and interest-
ing phenomenon of medical institutions in each regions. ‘Doctor’s Skill’
is the general factor considered to be important across regions.
1
Introduction
Our paper aims to compare medical market among China, Thailand and Taiwan
and to sort out the diﬀerences among the three markets. The ultimate goal of
our paper is to identify the most vital factors, i.e. key successful factors, that
inﬂuence customers’ choice on medical institutions. The discoveries of this paper
can be further applied in the medical industry.
Three types of medical institutions are chosen in this paper, which are general
clinic, dental clinic and cosmetic surgery clinic. The former two are chosen as they
account for over 50% of all types of medical institutions, while cosmetic surgery
clinic is chosen as it is becoming a trend in these years. These medical institutions
are among the scope of our study and are involved in the questionnaire.
The methodology applied in this paper is introduced in Literature Review
in Sect. 2, while the details on the questionnaire design of this paper will be
explained in Questionnaire Design in Sect. 3. Explanations on the process of
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_23

322
T.-R. Lee et al.
analysis will be in Quantitative Research in Sect. 4. Results and Discoveries are
listed in Sects. 5 and 6 separately. Section 7 concludes the whole paper while,
last but not least, a list of all the references that is mentioned in the paper. In
this paper, we use the term “customer” instead of “patient” because people go
to dental and cosmetic surgery clinic not only due to their illness, but also in
the pursuit of a better appearance in most of the cases.
2
Literature Review
2.1
Critical Successful Factor (CSF)
Daniel D. Ronald of McKinsey Company proposed ‘critical successful factor’
(CSF) in 1961. Critical successful factor refers to the fundamental elements
necessary for a company’s or an institution’s success. To ensure the represen-
tativeness of the result, two criteria are set for the critical successful factor:
1. The number of critical successful factors should be at least three and no more
than six, 2. The number of critical successful factors should not exceed half the
number of all the factors examined. Adopting Daniel’s point of view, this paper
will take the two criteria of the critical successful factor to be that of the ‘key
successful factors’ of medical institutions sorted.
2.2
Grey Relational Analysis
Grey system theory is proposed by Deng (1989) to understand the degree of
inﬂuence among a series of known information. If the information in the system
can be completely analyzed and is distinctive, or the relations between input
parameters and output parameters are clear, then we say the system is ‘white
system’. On the contrary, information that cannot be analyzed or relations that
are completely unclear is ‘black system’. Gray system is the one between black
and white system, with vague construction, characteristics and parameters of
information but not totally unclear.
Grey relational Analysis (GRA) is an impact evaluation model that measures
the degree of similarity or diﬀerence between two sequences based on the grade of
relation (Zhang and Liu 2011). There are three advantages that the methodology
brings in the statistics perspective. First, the system can be established even with
incomplete or vague information. For example, while many possible elements are
involved in the reason why a president is elected (because the president is smart,
honest, powerful, or shares the same hometown with the electors, etc.), GRA
builds the system and sorts out the elements with highest relation to the event
only. Second, while traditional regression statistics requires a large amounts of
data to proceed to the analysis, GRA requires only four numbers of data to
acquire a representative result. Last but not least, the calculation procedure is
easy, and no complicated formulas are needed. Therefore, the Grey relational
analysis (GRA) has been widely applied in various ﬁelds (Wei 2010).

To Compare the Key Successful Factors
323
Lin et al. (2002) tried to optimize EDM (electrical discharge machining)
process with fuzzy logic and grey relational analysis (GRA). Chaang-Yung and
Kun-Li Wen (2007) evaluated the relationship between company attributes and
its ﬁnancial performance by applying grey relational analysis (GRA). Kuo et al.
(2008) applied grey relational analysis (GRA) to facilitate single alternative
works that are best for all performance attributes. In optics and laser domain,
Ulas and Ahmet (2008) presented an eﬀective approach for the optimization
of laser cutting process of St-37 steel with multiple performance characteristics
based on the grey relational analysis (GRA). Noorul Haq et al. (2008) improved
eﬀectively the responses in drilling Al/ SiC process with GRA.
The calculation procedure of grey relational analysis (GRA) can be simpliﬁed
into four major steps. To make the procedure more understandable, we will
explain the steps with the data of Taiwanese clinics.
Step 1. Calculate the diﬀerences between referential sequences
and comparative sequences
A Likert ﬁve-point scale is used to evaluate the criteria for the calculation of grey
relational coeﬃcients of all factors with the highest score being 5. Each factor is
graded with a score of 5 (Very Important), 4 (Important), 3 (Average), 2 (Unim-
portant), or 1 (Very Unimportant). The evaluation point in the questionnaire
is the so-called referential sequences, while the diﬀerence between the highest
score (i.e. 5 in our paper) and the evaluation point is “comparative sequences”.
The less diﬀerences between referential sequences and comparatives sequences
are, the more similar they are. The changes could be found from Tables 1 to 2.
Table 1. List of original scores of Taiwanese clinic (Comparative sequences)
Question items
Interviewees
1
2
· · ·
115 116
1
Clinic Division
*5
5
· · ·
4
4
2
Experts’ recommendations
4
3
· · ·
2
3
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
22
Payment Method: by Installments
4
1
· · ·
2
5
23
Clear and Speciﬁc Medical Instruction 5
5
· · ·
4
5
*Example: Interviewee of questionnaire No. 1 evaluated the score “5”
to question ‘’.
Step 2. Calculate the grey relational coeﬃcient with Eq. 1
Figure out the maximum (Δ max) and minimum (Δ min) of comparative
sequences for each question items, and calculate the grey coeﬃcient with Eq. (1).
γX0k, Xik = γ0ik = Δ min +ξΔ max +ΔXik + ξΔ max
(1)

324
T.-R. Lee et al.
Table 2. List of calculated diﬀerences between the highest score and comparative
sequences of Taiwanese clinic (referential diﬀerence)
Question items
Interviewees
1
2
· · ·
115 116
1
Clinic Division
*0
0
· · ·
1
1
2
Experts’ recommendations
1
2
· · ·
3
2
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
22
Payment Method: by Installments
1
4
· · ·
3
0
23
Clear and Speciﬁc Medical Instruction 0
0
· · ·
1
0
*Example: Diﬀerence between original scores and referential alterna-
tive, i.e. referential diﬀerence sequences.
A. i = 1, · · · , m; k = 1, · · · , m
B. ξ is the identiﬁcation coeﬃcient and belongs to [0, 1]. ξ makes the best
use when ξ ≤0.5463, and our paper take it as 0.5
C. X0(k) : Referential sequences
D. Xi(k) : Comparative sequences
E. γ0i(k) : Grey relational coeﬃcient
F. |Dela min and Δ max : A. The minimum and maximum of comparative
sequences for each question items.
Xi(k) is the score that the k −th respondent answers factor i, where i =
1, 2, · · · , 23; k = 1, 2, · · · , n, and n is the amount of valid questionnaires.
Step 3. Calculate the grey relational grade with Eq. 2
Γ(x0, xi) = Γ0i =
n

k=1
βkγ[x0(k), xi(k)]
(2)
A. βk is weight of each factor.
After step 1, with the sum of all the grey coeﬃcients and the average of each
question items, a system of grey relational grade will be established. The grey
relational grade represents the relationship between sequence and its comparison
sequence. If two factors in a system is changing toward the same tendency, then
they have a high extent of synchronous change, as well as a high extent of
the correlation. Then, grey relational grade, which is equal to grey relational
coeﬃcient under equal weighted index, is calculated (Table 3).
Step 4. Sort out key successful factors by raking grey relational grade
After calculating grey relational grade, we will rank all the factors through with
the grade. The higher the grade is, the more important the factors is. All the
question items by score are listed in Fig. 1. Finally, this paper obtains key suc-
cessful factors with Ronald (1961)’s criteria.

To Compare the Key Successful Factors
325
Table 3. Grey coeﬃcient of Taiwanese clinic
Question items
Interviewees
1
2
· · ·
115
116
1
Clinic Division
1
0.6
· · ·
0.67 1
2
Experts’ recommendations
1
0.43 · · ·
.33
1
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
22
Payment Method: by Installments
0.71 0.33 · · ·
0.4
0.6
23
Clear and Speciﬁc Medical Instruction 0.71 0.43 · · ·
1
1
Fig. 1. Taiwanese clinic grey relational grade in line chart
Here is an example by Taiwanese clinic data: (Due to limited space, this
paper lists only the degree of key successful factors). According to Daniel’s two
critical successful factor criteria: 1. The amount of critical successful factors
should be among three to six, and 2. The amount of critical successful factors
should not exceed half the amount of all the factors examined, we would not
involve factors after group 6 the amount of factors will exceed half of the total
amount (23/2 = 11.5 ≈11) and violate criterion 1. Moreover, we would not
involve factors after group 3 as we should not include more than six factors to
ensure the representativeness of our result (Table 4).
Therefore, our paper sorted out ﬁve key successful factors of Taiwanese clinic
as showed in table below:
Table 4. Key successful factors of Taiwanese clinic
Group
No.
Key successful factors
Grey relational grade Rank
1
15
Doctors Skill
0.885
1
20
Treatment Eﬀect
0.869
2
2
14
Doctor’s Explanation before
Doing Treatment
0.846
3
3
12
Nurse’s Attitude
0.821
4
11
Doctor’s Attitude
0.807
5
Data of dental clinic and cosmetic surgery clinic in both Taiwan and Thailand
are calculated with the same procedure.

326
T.-R. Lee et al.
3
Questionnaire Design
Three types of medical institutions are chosen in this paper, which are general
clinic, dental clinic and cosmetic surgery clinic. The former two are chosen as they
account for over 50% of all types of medical institutions, while cosmetic surgery
clinic is chosen as it is becoming a trend in these years. These medical institutions
are among the scope of our study and are involved in the questionnaire.
A ﬁve-point likert scale is the criteria of the questionnaire. Customers are
asked to evaluate the importance of each factors, from very unimportant to very
important, according to their own opinions.
The factors that inﬂuence customers’ decisions are distributed into three
stages in the questionnaire, which are before treatment, during treatment, and
after treatment. All the factors in three stages are explained as follows.
3.1
Question Items (Factors) in the Questionnaire
In this section, 23 question items are classiﬁed three diﬀerent periods, which are
before treatment, during treatment, and after treatment. With this principle,
ten question items (factors) were attributed to ‘factors before treatment’, eight
question items (factors) were concluded in ‘factors during treatment’, and ﬁve
question items (factors) were belonged to ‘factors after treatment’.
Factors before Treatment
(1) Clinic Division
Patients select doctors according to their specialty before going to a medical
institution (Boscarino and Steiber 1982; Lane and Linquist 1988). Therefore,
clinic division is a question item in our paper.
(2) Experts’ recommendations
Bayus (1985) claimed that positive or negative reputation is one of the key factors
that inﬂuences customers’ decision under the situation of insuﬃcient informa-
tion. The recommendations from other doctors is one of the most eﬃcient ways to
know the credibility of a doctor or a clinic/hospital (Wolinsky and Kurz (1984),
Malhotra (1983)). Therefore, our paper includes ‘experts’ recommendations’.
(3) Social Media Recommendation
Gu et al. (2012) noticed that customers do more research before conducting
‘high involvement treatment’, such as dental correction and plastic surgery. So
the questionnaire includes ‘social recommendation on the website’.
(4) Transportation Convenience
The distant from the patients’ location to the clinic is one of their concerns
while choosing a medical institution. The closer one has a greater advantage as
it reduces not only transportation costs but also time. Due to this reason, our
paper will take this factor into consideration.

To Compare the Key Successful Factors
327
(5) Parking Space
More and more people go around with cars; therefore, a parking space is an essen-
tial requirement, especially for treatments in the dental clinic or the cosmetic
surgery clinic that take a longer time.
(6) Waiting Time
‘Waiting time’ has been frequently mentioned since 1976. (Holtmann and Olsen
1976). Also, Otani et al. (2010a) included ‘registration waiting time’ into the
questionnaire items. Thus, it will be included in this paper as well.
(7) Reservation Explanation
Huang et al. (2008) mentioned reservation explanation in their paper. Since
reservation service is usual in dental clinic and cosmetic surgery clinic, a clear
reservation explanation can be an inﬂuential element to enhance customers sat-
isfaction. Therefore, ‘reservation explanation’ is one of the questionnaire items
in our paper.
(8) Reputation of the Medical Institution
Trust and reputation are often taken into consideration while people are making
decisions on whether or not to continue their interaction with a party in the
future (Josang et al. 2007). They also serve as signiﬁcant references for people
to decide whether to start business (we mean whether to go to the clinic here)
with the party. So ‘reputation of the medical institution’ will be included in our
paper.
(9) Doctor’s Specialty and Experience
Our paper included ‘doctor’s specialty and experience’ since doctor’s specialty
and experience is the ﬁrst thing that customers know about a doctor. Also, a
doctor who is professional and well-experienced wins credit from customers.
(10) (Customer’s) Service Experience
Beskind (1962) noted that customers make decision by their experience accu-
mulated in the past. I.e. customers’ high satisfaction keeps them tied with a
company while unpleasant experiences force them to go for another.
Factors during Treatment
(11) Doctor’s Attitude
Doctor is the person who interacts with customers the most during the treat-
ment. Boudreaux (2004) also mentioned ‘doctor’s caring’ in his research. Due to
the reasons above, ‘attitude: doctor’s attitude’ can be one of the questionnaire
items in our paper.
(12) Nurse’s Attitude
Wolinsky and Kurz (1984), Lane and Lindquist (1988), Boudreaux (2004), and
Otani et al. (2010b) mentioned nurse’s attitude to be an important issue in
medical caring in their paper. Therefore, our paper included the factor as well.

328
T.-R. Lee et al.
(13) Other Staﬀ’s Attitude
‘Other staﬀ’s’ in this paper are the staﬀs except for doctors and nurses, includ-
ing pharmacists, counter staﬀ, etc. Doctors and nurses cannot ﬁnish the whole
treatment process (from registration to distribute medicine) without the assis-
tance from other staﬀs. Wolinsky and Kurz (1984), Lane and Lindquist (1988)
even take ‘pharmacists’ attitude’ as a questionnaire items.
(14) Doctor’s Explanation before Doing Treatment
Boudreaux (2004) found that doctors’ clear explanation about the treatment
beforehand inﬂuences customer experience positively. Moreover, this clear expla-
nation leads to a positive inﬂuence on customer satisfaction (Otani et al. 2005).
Based on the studies, our paper put doctors’ explanation before doing treatment
into questionnaire.
(15) Doctors Skill
Cheng et al. (2003) found that ‘doctor’s skill’ aﬀected not only customers’ sat-
isfaction, but also decided if they will recommend to others. Otani et al. (2005)
deﬁnes ‘doctors skill’ as careful treatment. Our paper adopted that ‘doctors skill’
is an important factor.
(16) Medical Environment (quiet, cleanness and comfort, etc.)
A quiet, clean and comfortable environment helps customers to rest and recover,
so our paper put “medical environment (quiet, cleanness and comfort, etc.)” in,
too.
(17) Privacy Reserve
Inhorn (2004) discussed the privacy issue during treatment. Although Inhorn’s
paper (2004) discussed cases of the Middle East, we consider the element to be
worthwhile mentioning and include it in our paper.
(18) Medical Equipment
Whether the medical equipment is advanced relates to the comforts of customers
and the eﬀectiveness of the treatments. So this paper takes into consideration
“medical equipment” as one of the factors.
Factors after Treatment
(19) Treatment Costs
Many papers studying the factors of patients’ medical division choice men-
tioned that ‘Medical Caring Cost’ is one of the factors that patients concerned.
(Boscarino and Steiber (1982), Malhotra (1983), Wolinsky and Kurz (1984),
Lane and Lindquist (1988).
(20) Treatment Eﬀect
Treatment eﬀect is the direct proof or reference for customers to evaluate whether
the money spent is worthy or not. “Treatment eﬀect” is apparently an issue that
customers care about, so our paper takes it in the questionnaire.

To Compare the Key Successful Factors
329
(21) Payment Method: Credit Card
Surgery and treatments like orthodontics and denture are expensive, which has
made it inconvenient for patients to pay by cash at once. An alternative pay-
ment method should therefore be provided to enhance customer satisfaction.
Therefore, “payment method: credit card” is included in our paper.
(22) Payment Method: by Installments
Similar reason with “Payment Method: Credit Card”, paying in installments
is an aﬀordable alternative for customers. Thus, “Payment Method: by Install-
ments” is included in this paper.
(23) Clear and Speciﬁc Medical Instruction
Clear and speciﬁc medical instruction can be taken as “after-sales services” in
medical industry. It can be either beneﬁcial or harmful to the impression of
a medical institution. So “clear and speciﬁc medical instruction” is one of the
factors in our paper.
4
Quantitative Research and Qualitative Research
Our paper conducts quantitative research in Taiwan and Thailand, and a qual-
itative research in China. The results of the researches are compared in the end
of this paper. Although the two methodology applied seems to be so diﬀerent at
the ﬁrst glance, the comparison is actually valid and meaningful. The key point
that makes this possible is the ‘conﬁned inquiry range’, i.e. the same question
items.
During quantitative research, interviewees are asked to answer the impor-
tance of the 23 question items introduced in Sect. 3, and to sort out four to six
key successful factors among them. While proceeding to the qualitative research
in China, the sorted key successful factors in Taiwan and Thailand were main
items in qualitative research interview process.
4.1
Quantitative Research
The most prominent advantage of quantitative research is that facts are demon-
strated by numerical ﬁgures, which is more objective and can be assessed easily.
We apply the quantitative method to analyze customers’ needs in the cases of
Taiwan and Thailand.
4.1.1
Subjects
The target subjects of this research in Thailand were experienced doctors and
nurses, whereas in Taiwan they were patients in the general clinic, dental clinic
and cosmetic surgery clinic. In Thailand, experienced doctors and nurses have

330
T.-R. Lee et al.
enough knowledge regarding to their patients that they understand the needs of
patients. Thus, the answers should be representative enough to reﬂect medical
market in Thailand, and is adequate to be compared with the data of Taiwan.
4.1.2
Surveying Period
Thailand
The questionnaires were released in Thailand during 2014 November and
December by paper. 90 questionnaires were collected from doctors and nurses in
Chiang Mai (30 per type of medical institution). The overall response rate was
100%.
Taiwan
The questionnaire was released by both paper and online means from June 4th
2013 to July 2nd 2013. 116 questionnaires are collected in total, and 106 of them
are valid. The overall response rate as 87.6%.
4.2
Qualitative Research
Our paper inquired patients in medical institution at the east coast of China
instead of distributing questionnaires and conduct quantitative research for two
reasons: ﬁrst, it is time-consuming to collect all the questionnaires from 23
provinces in China; second, south east Chinese shares similar culture, language,
belief, custom, etc. with that of the Taiwanese. Therefore, it is a reasonable
assumption that the key successful factors in the two places will be similar. The
ﬁnal result of this research also supports this original assumption.
4.2.1
Subjects
This paper targets at 30 subjects that had experiences of going to diﬀerent types
of medical institutions. Most of the target subjects are from south east China
since people here present the highest cultural similarity to those of Taiwan.
4.2.2
Surveying Period
The subjects were inquired from July 1st to August 31st in 2015. 30 subjects
were inquired in total.
5
Results
5.1
Results
All the key successful factors of Taiwan and Thailand chosen follow Daniel’s
criteria mentioned in Literature Review in Sect. 2. Due to the space limit, this
paper only displays key factors of each types of medical institutions. The analysis
results in three places were listed.

To Compare the Key Successful Factors
331
The result of the inquiries in China is showed in Table 6 below:
Table 5. The key successful factors of Chinese customers in general clinic, dental clinic
and cosmetic surgery clinic:

332
T.-R. Lee et al.
Table 6. *P.S. Letter denotation of 5.1.1.:
5.2
Results of the Survey
The horizontal title of Table 7 refers to period of treatment (i.e. before treat-
ment, during treatment, and after treatment), while the vertical title of Table 7
refers to types of medical institutions. To understand the details of the key suc-
cessful factors in three regions clearly, we add a second vertical subordinate title
‘country’ (Tables 5 and 6).
Factors listed in Table 7 are the key successful factors of each institution but
not in the order of its priority. ‘N/A’ means no key successful factors are sorted
out from the category. Some factors are mentioned repeatedly, while some are
not. Factors mentioned repeatedly are the most signiﬁcant ones. However, if a
factor is mentioned only once in Table 7, the factor is especially important to
the country in certain period.

To Compare the Key Successful Factors
333
Table 7. Overall result: key successful factors of three medical institution in three
6
Findings
This chapter will conclude and explain the results from the last chapter and
reorganize the factors according to the frequency they show in three regions
(Tables 8, 9 and 10). Since the results are from patients’ intuition, and due to
the page limit, we will focus on the factors regarded to be important only in a
certain country and explore the reason behind the phenomenon. The explanation
below is based on the interview with a doctor in Jen-Ai Hospital.

334
T.-R. Lee et al.
6.1
General Clinic
According to Table 7, ‘doctor’s skill’ and ‘doctor’s attitude’ are considered to be
inﬂuential on customers’ choice of clinics for all of the three regions. ‘Doctor’s
explanation before doing treatment’ aﬀects Taiwanese and Thai, while ‘treat-
ment eﬀect’ is important to Taiwanese and Chinese. What we would like to dis-
cuss further in this paper is ‘nurse’s attitude’ for Taiwanese, and ‘other staﬀ’s
attitude’ and ‘clear and speciﬁc medical instruction’ for Thai. The frequency of
the key successful factors is listed in Table 8.
Table 8. Frequency of clinic key successful factors in the three regions
Frequency Key successful factor
Region
3
Doctor’s skill
All
Doctor’s attitude
All
2
Doctor’s explanation before doing treatment TW, THI
Treatment eﬀect
TW, CN
1
Nurse’s attitude
TW
Other staﬀ’s attitude
THI
Clear and speciﬁc medical instruction
THI
When choosing a general clinic,
(1) Nurse’s attitude is an inﬂuential factor for Taiwanese customers
In Taiwanese clinics, ‘nurse’ is an inﬂuential role because nurse is in charge
of most of the tasks in whole treatment procedure (e.g. registration, injection,
medicine giving). In other words, customers spend more time with nurse than
with doctor. Moreover, Taiwanese customers go to a clinic not only for physi-
cal treatment, but also for their mental comfort. Therefore nurse’s attitude is
especially important for the Taiwanese.
(2) ‘Other staﬀ’s attitude’ is an inﬂuential factor for Thai customers
Thai clinics pursue a high standardization process, which rely very much on
the eﬀorts of staﬀs. Most samples collected in this paper are from Maharaj
Nakorn Chiang Mai Hospital, which is a famous and internationalized one. In
high ranking hospitals, even registration time should not be wasted. In order to
run a complex treatment procedure while at the same time to keep it at a good
quality, the executers, i.e. other staﬀs, are doubtlessly important.
(3) ‘Clear and speciﬁc medical instruction’ is inﬂuential for Thai customers
Thai Government has been promoting ‘medical tourism’ since 2004. Such policy
has pushed Thailand to be one of the medical leaders in East Asia, as well as
the most famous ‘medical tourism’ destination for western people. To acquire
customers’ trusts, clear and speciﬁc medical instruction is a vital factor under
the exotic medical environment.

To Compare the Key Successful Factors
335
6.2
Dental Clinic
After concluding key successful factors of dental clinics in the three regions, we
found that ‘customer’s service experience’ and ‘doctor’s skill’ are vital factors
to all. ‘Doctors’ specialty and experience’ and ‘treatment eﬀect’ are essential
to Taiwanese and Chinese. ‘Doctor’s attitude’ is shown as signiﬁcant again to
Taiwanese and Thai. ‘Doctor’s explanation before doing treatment’ is important
to Chinese and Thai. As for the factors that only inﬂuence one region, ‘nurse’s
attitude’ again has eﬀect on the Taiwanese, while ‘medical environment (quiet
cleanness and comfort, etc.)’ and ‘privacy reservation’ are fundamental to Thai in
dental clinic. Table 9 shows the results mentioned. When choosing a dental clinic,
Table 9. Frequency of dental clinic key successful factor in the three regions
Frequency Key successful factor
Region
3
Customer’s service experience
All
Doctors skill
All
2
Doctor’s specialty and experience
TW, CN
Treatment eﬀect
TW, CN
Doctor’s attitude
TW, THI
Doctor’s explanation before doing treatment
CN, THI
1
Nurse’s attitude
TW
Medical environment (quiet cleanness and comfort, etc.) THI
Privacy reservation
THI
(1) ‘Nurse’s attitude’ is an inﬂuential factor for Taiwanese customers
Dental treatment is not always a one-time treatment. Customers have to go back
to the dental clinic for further treatment from time to time from a few weeks to sev-
eral years (e.g. braces). As in the general clinics, nurses assist not only daily opera-
tions but also treatments, and they are the main receptors to customers. Therefore,
‘nurse’s attitude’ is important to Taiwanese people in deciding a dental clinic.
(2) ‘Medical environment (quiet, cleanness and comfort, etc.)’ is an inﬂuential
factor for Thai customers
Infection is a concern for customers who care about medical environment. The
Thai people emphasize on ‘medical environment’ very much as infection is an
issue in dental treatments. But this is an inference that has not yet been con-
ﬁrmed.
(3) ‘Privacy reservation’ to Thai
If you do not want to encounter anyone in a dental clinic, then privacy reserva-
tion must signiﬁcant to you. Recent years, dental treatment includes not only
therapies, but also teeth shaping, which is similar to orthopedics. Customers do
not want their enhancement to be thought as ‘artiﬁcial’. In contrast, they would
rather to claim that they eat something special, massage cheek every day, etc.

336
T.-R. Lee et al.
to their delight. The reason why ‘privacy reservation’ is inﬂuential to Thai when
choosing a dental clinic is similar to that of the Taiwanese. More explanation
about the phenomena will be illustrated in Sect. 6.3 (1).
6.3
Cosmetic Surgery Clinic
After concluding cosmetic surgery clinic key successful factors for three regions,
‘doctors specialty’, ‘doctors skill’ and ‘doctor’s explanation before doing treat-
ment’ are fundamental to all of the three regions. ‘Reputation of the medical
institution’ and ‘treatment eﬀect’ is important to Taiwanese and Chinese; and
‘doctor’s attitude’ is inﬂuential to Chinese and Thai customers. Last but not
least, ‘privacy reservation’ is a major consideration for the Taiwanese, and ‘cus-
tomer’s service experience is especially vital to the Thai while choosing an
cosmetic surgery clinic. The result is presented in Table 10. When choosing an
cosmetic surgery clinic:
Table 10. Frequency of cosmetic surgery clinic key successful factors in the three
regions
Frequency Key successful factor
Region
3
Doctor’s specialty
All
Doctor’s skill
All
Doctor’s explanation
All
Before doing treatment
2
Reputation of the medical institution TW, CN
Treatment eﬀect
TW, CN
Doctor’s attitude
CN, THI
1
Privacy reservation
TW
Customer’s service experience
THI
(1) Privacy reservation’ to Taiwanese
Compared to ‘privacy reservation’ to Thai in dental clinic, Taiwanese not to be
seen or known to have visited an cosmetic surgery clinic. Most Taiwanese go to
cosmetic surgery clinics for a better appearance rather than accident recovery.
They pursue not only ‘beauty’, but also ‘nature beauty’. Although ‘artiﬁcial
beauty’ is a kind of beauty, but somehow ‘artiﬁcial’ is related to ‘fake’. People
who go to cosmetic surgery clinic value their appearance, and hope to be consid-
ered as a nature beauty. If they are seen in an cosmetic surgery clinic, then the
truth that they are not born to be a beauty will be known. That’s why ‘privacy
reservation’ is important to Taiwanese people.
(2) ‘Customer’s service experience’ is an inﬂuential factor for Thai customers
The eﬀect of an aesthetic surgery is shown directly on customers, from which the
service experience is also direct and easy to tell. Also, aesthetic surgery and its

To Compare the Key Successful Factors
337
related treatment are usually not covered by Health Insurance and is at the cus-
tomers’ own expense. In a country with high quality medical environment where
customers have more alternatives to choose from, it is to a clinic’s advantage to
possess the core competence that customers perceive to be important.
7
Conclusion
This paper sorts out the key successful factors for general clinic, dental clinic, and
cosmetic surgery clinic in Taiwan, Thailand, and China. The methodology Grey
Relational Analysis (GRA) is applied as it is eﬀective in the case of incomplete
data, which cannot be achieved by most of traditional statistical methods.
Generally speaking, Taiwanese and Chinese customers emphasize on “treat-
ment eﬀect” in the three kinds of medical institution. Also, ‘doctors skill’ is
considered to be important across all the three medical institutions in the three
regions. Throughout the ﬁndings, we conclude that doctor’s skill is the core
value and basic qualiﬁcation for medical institutions in Taiwan and China. It is
not only important but also is inﬂuential to other factors like ‘reputation of the
clinic’, ‘customer’s experience’, etc.
Although China and Thailand owns diﬀerent medical conditions and
resources, customers in both regions emphasize much on ‘doctor’s specialty and
experience’, ‘doctor’s explanation before doing treatment’, and ‘doctor’s skill’ in
dental clinic and cosmetic surgery clinic. Chinese customers might require more
on basic medical right, and Thai customers might pursue more on better quality.
Nowadays, it is noteworthy to understand the key successful factors of clinics
since they are the indicators for clinics to meet customer’s needs. The improve-
ment of the whole industry will enhance its quality, and brings win-win business.
We hope this paper can serve as the foundation for further studies on this matter.
References
Noorul Haq, A., Marimuthu, P., Jeyapaul, R.: Multi response optimization of machining
parameters of drilling Al/SiC metal matrix composite using grey relational analysis
in the Taguchi method. Int. J. Adv. Manufact. Technol. 37(3–4), 250–255 (2008)
Boudreaux, E.D., O’Hea, E.L.: Patient satisfaction in the emergency department: a
review of the literature and implications for practice. J. Emerg. Med. 26(1), 13–26
(2004)
Boscarino, J., Steiber, S.R.: Hospital shopping and consumer choice. J. Health Care
Mark. 2(2), 15–23 (1982). Spring
Kung, C.-Y., Wen, K.-L.: Applying grey relational analysis and grey decision-making to
evaluate the relationship between company attributes and its ﬁnancial performance
- a case study of venture capital enterprises in Taiwan. Decis. Support Syst. 43(3),
842–852 (2007)
Cheng, S.-H., Yang, M.-C., Chiang, T.-L.: Patient satisfaction with and recommenda-
tion of a hospital: eﬀects of interpersonal and technical aspects of hospital care. Int.
J. Qual. Health Care 15(4), 345–355 (2003)

338
T.-R. Lee et al.
Lin, C.L., Lin, J.L., Ko, T.C.: Optimisation of the EDM process based on the orthogo-
nal array with fuzzy logic and grey relational analysis method. Int. J. Adv. Manufact.
Technol. 19(4), 271–277 (2002)
Daniel, R.D.: Management information crisis. Harvard Bus. Rev. 35(5), 111–121 (1961)
Deng, J.L.: Introduction grey system. J. Grey Syst. 1(1), 1–24 (1989)
Huang, F.-F., Chen, M.-S., Huang, L.-R.: The decision of patients and analysis of
health check satisfaction. J. Orient. Inst. Technol. 28, 37–46 (2008)
Marcia, I.C.: Privacy, privatization, and the politics of patronage: ethnographic chal-
lenges to penetrating the secret world of middle eastern, hospital-based in vitro
fertilization. Soc. Sci. Med. 59(2004), 2095–2108 (2004)
Lin, J.L., Lin, C.L.: The use of the orthogonal array with grey relational analysis
to optimize the electrical discharge machining process with multiple performance
characteristics. Int. J. Mach. Tools Manufact. 42(2), 237–244 (2002)
Audun, J., Roslan, I., Coin, B.: A survey of trust and reputation systems for online
service provision. Decis. Support Syst. 43, 618–644 (2007)
Otani, K., Waterman, B., Faulkner, K.M., Boslaughand, S., Dunagan, W.C.: How
patient reactions to hospital care attributes aﬀect the evaluation of overall quality
of care, willingness to recommend and willingness to return. J. Health Care Manag.
55(1), 25–57 (2010a)
Lane, P.M., Lindquist, J.D.: Hospital choice: a summary of the key empirical and
hypothetical ﬁndings of the 1980s. J. Health Care Mark. 8(4), 5–20 (1988)
C¸ayda¸s, U., Hasslik, A.: Use of the grey relational analysis to determine optimum laser
cutting parameters with multi-performance characteristics. Optics Laser Technol.
40(7), 987–994 (2008)
Wei, G.W.: GRA method for multiple decision making the incomplete weight informa-
tion in intuitionistic fuzzy setting. Knowl.-Based Syst. 23(19), 243–247 (2010)
Wolinsky, F.D., Kurz, R.S.: How the public chooses and views hospital. Hosp. Health
Serv. Adm. 29, 58–67 (1984)
Kuo, Y., Yang, T., Huang, G.-W.: The use of grey relational analysis in solving multiple
attribute decision-making problems. Comput. Indus. Eng. 55(1), 80–93 (2008)
Zhang, S.F., Liu, S.Y.: A GRA-based intuitionistic fuzzy muilti-criteria group decision
making method for personal selection. Expert Syst. Appl. 38, 11401–11405 (2011)
Gu, B., Park, J., Konana, P.: Research note—the impact of external word-of-mouth
sources on retailer sales of high-involvement products. Inf. Syst. Res. 23(1), 182–196
(2012)
Beskind, H.: Psychiatric inpatient treatment of adolescents a review of clinical experi-
ence. Compr. Psychiatry 3(6), 354–369 (1962)
Otani, K., Herrmann, P.A., Kurz, R.S.: Patient satisfaction integration process: Are
there any racial diﬀerences? Health Care Manage. Rev. 35(2), 116–123 (2010b)
Bayus, B.L.: Word-of-mouth: the indirect eﬀects of marketing eﬀorts. J. Advertising
Res. 25, 31–9 (1985)
Holtmann, A.G., Olsen Jr., E.O.: The demand for dental care: a study of consumption
and household production. J. Hum. Resour. 546–560 (1976)
Kuehn, A.: Consumer brand choice as a learning process. J. Advertising Res. 2(March–
April), 10–17 (1962)
Malhotra, N.K.: Stochastic modeling of consumer preferences for health care institu-
tions. J. Health Care Market. 3(4) (1983)
Otani, K., Kurz, R.S., Harris, L.E., Byrne, F.D.: Managing primary care using
patient satisfaction measures/practitioner application. J. healthc. Manage. 50(5),
311 (2005)

Forecasting Thailand’s Exports to ASEAN
with Non-linear Models
Petchaluck Boonyakunakorn1(B), Pathairat Pastpipatkul2,
and Songsak Sriboonchitta2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
petchaluckecon@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai Faculty of Economics, Chiang Mai University,
Chiang Mai 52000, Thailand
ppthairat@hotmail.com, songsakecon@gmail.com
Abstract. This work focuses on forecasting Thailand’s exports to
ASEAN. Thailand’s exports to ASEAN reveal an overall increasing trend
with a ﬂuctuation since Thailand’s exports are integrated in the global
economy. However, the linear model might not be able to capture the
behavior of Thailand’s exports to ASEAN. Linear model cannot be
applied in some phenomena such as ﬂuctuation and structural breaks
in time series data. In this study, we ﬁnd that the Thailand’s exports-
to-ASEAN time series is non-linear via test of linearity, and ﬁnd that
there are two thresholds. Therefore, we forecast Thailand’s exports to
ASEAN with non-linear models. We employ four non-linear models,
SETAR, LSTAR, MSAR, and Kink AR model. The simple linear AR
model is also applied to compare with the non-linear models. To evalu-
ate the forecasting performance of ﬁve diﬀerent models, we use RMSE
and MAE as criteria. The forecasting results indicate that the SETAR
model is better than the other models. However, it is still not clear cut to
conclude that the non-linear models outperform linear model. However,
we can conclude that the SETAR is the most suitable for forecasting
Thailand’s exports to ASEAN compared with other non-linear models.
Keywords: Non-linear models · ASEAN · Forecasting
Thailand’s exports
1
Introduction
Thailand’s exports constitute a huge portion of Thai GDP, accounting for up to
70% of the country’s economic output. Meanwhile, the world average value of
exports of goods and services is only 30% of the world GDP. Clearly, economic
growth relies heavily on trade. The biggest destination for Thai exports is the
Association of Southeast Asian Nations (ASEAN) which accounts for approxi-
mately 25% of the total exports. Ten ASEAN members are Brunei, Cambodia,
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_24

340
P. Boonyakunakorn et al.
Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Vietnam and
Thailand. The main objective of ASEAN is to promote economic and political
cooperation among members. Becoming an ASEAN member provides a larger
economy of scale, one of the advantages, which provides a signiﬁcant support to
Thai exports. However, with diﬀerent sizes of markets and diﬀerent geography it
generates diﬀerences in the toughness of competition across export market desti-
nations (Mayer et al. [11]).
ASEAN is recognized in recent years as the biggest export market for
Thailand, and the above diﬀerences may inﬂuence stability in competing with
other countries. Export policy becomes one of the most important contributing
factors to successful exports. Thailand has been signiﬁcantly focusing on main-
taining its export capability to respond to changing environment. Therefore,
economic development strategies should be in line with export policy. One of
the key components supporting decision-making process is forecasting.
Forecasting is the heart of the strategic planning on exports. In addition,
there is a notion that export policy decision often uses the historical record of
exports. The data used in this study is also available for analysis with time
lag. Therefore, a theoretical model that increases accuracy in export forecasting
has gained more economic consideration. Figure 1 depicts Thailand’s exports to
ASEAN on monthly basis, revealing an overall increasing trend with a ﬂuctua-
tion. Due to global crisis, the value of Thailand’s exports to ASEAN recorded the
unusual lowest point in 2008, then it ﬂuctuated signiﬁcantly with an increasing
trend during January 2009 and June 2011. After that, the export dropped sub-
stantially again in 2011 as the ﬁnancial crisis began. These ﬁnancial crises clearly
pointed out that the global economy has an important role aﬀecting Thailand’s
exports. Since 2012, it has remained the same with slightly downward trend.
Linear models have been widely used for forecasting with time series data.
However, limitations caused by linear models have made practitioners encounter
diﬃculties in dealing with certain situations, which cannot be explored under
Fig. 1. Thailand’s exports to ASEAN during the period 2002–2016

Forecasting Thailand’s Exports to ASEAN with Non-linear Models
341
linear model assumptions. Tong and Lim [16] has cautioned that there are
phenomena that cannot be suitable to deal with using linear models, namely
asymmetry, time-irreversibility, sudden bursts of very large amplitude at change-
able time epoch, etc. Even though general linear models have been common in
econometrics for most of the twentieth century, non-linear models have been
introduced during the latter part, especially in econometrics for replicating key
features of the business cycle (Ferrara et al. [7]). Also, some researchers (e.g.
Brock and Potter [3]; Granger [8]) have supported that nonlinear behavior exists
in ﬁnancial time series and macroeconomic variables.
As non-linear models, Ter¨asvirta and Anderson [12] and Tong and Lim [16]
proposed threshold models known as one of the regime-switching models. Among
these regime-switching models, a model, known as self-exciting threshold AR
(SETAR) model, is initially introduced. Regimes in non-linear models are typi-
cally deﬁned by some lagged value of the time series. Then Hamilton [9] proposed
Markov-switching model with the notion that the regimes are identiﬁed by some
discrete Markov state variables.
Tiao and Tsay [14] stated that the threshold models provide better perfor-
mance compared with other linear models with respect to forecasting US GDP.
Djeddour and Boularouk [6], found an evidence that Threshold autoregressive
model (TAR) outperforms ARMA model when forecasting US export crude oil
in the mean square error. Bec et al. [1] proposed that an extension of threshold
models can predict GDP growth adequately after a recession. Since the model
allows for bounce-back eﬀects. Camacho et al. [4] compared linear with Markov-
switching models to investigate the forecasting performance of euro area GDP
and found that Markov-switching models provide more information about the
state of the economy. Ferrara et al. [7] found that the forecast accuracy of non-
linear models is better in almost 40%–45% of studied cases.
Nevertheless, some researchers found that the non-linear models do not per-
form better than the linear models with respect to forecasting performance, when
they applied linear and smooth-transition models to forecast industrial produc-
tion and exchange rate series, see Boero and Marrocu [2], and Sarantis [17].
Their results show that Markov-switching forecasting models cannot operate as
well as other linear models can. However, there is still no clear-cut evidence that
non-linear models operate better than the other ones in terms of forecasting
accuracy or reliability.
Figure 1 depicts that Thailand’s exports to ASEAN gently ﬂuctuate with
nonlinearity during the period 2002–2016. Subsequently, we employ the non-
linear model to forecast Thailand’s exports. This paper will apply ﬁve mod-
els and one falls into the category of linear model and the rest fall into
the category of non-linear model to forecast Thailand’s exports to ASEAN.
Autoregressive (AR) model, a linear model, followed by four non-linear models,
Self-exciting autoregressive (SETAR), Logistic STAR (LSTAR), Markov-
switching Autoregressive (MSAR) and Kink Autoregressive (AR) models. The
ﬁve diﬀerent models will be compared by means of Root Mean Squared Error
(RMSE) and the Mean Absolute Error (MAE). This paper is divided into four

342
P. Boonyakunakorn et al.
sections. Section 2 describes methodology of the ﬁve diﬀerent models employed
in this paper. Section 3 presents the empirical results. Conclusion and some sug-
gestion for further research are provided in the last section.
2
Methodology
2.1
The Linear Model
It is a simple Autoregressive (AR) model, a type of random process that current
value depends on its lag values. It is commonly used to model and predict the
future. For the sake of simplicity, we apply an autoregressive model of order 1.
The AR (1) model can be expressed as
yt = φ1yt−1 + σεt
(1)
where φ1 are the AR coeﬃcients at lag 1, εt ∼WN(0, 1), σ > 0 is the standard
deviation of distribution term.
2.2
Non-linear Models
2.2.1
Threshold Autoregressive (TAR) Model
In general, Threshold Autoregressive model refers to piecewise linear models
or regime switching models. Consequently, it is considered as one of non-linear
models, which are ﬁrstly introduced by Tong [15]. A regime switch occurs when
the threshold variable is held at the speciﬁc value of a variable. This is referred
to as a threshold. The model parameters can change in TAR model to capture
nonlinear dynamics, and they will change according to the value of a weakly
exogenous threshold variable ct.
In this paper, the threshold variable is the lagged dependent variable, which
is yt called Self-exciting autoregressive (SETAR) model. One of the advantages
of SETAR is to allow greater ﬂexibility in model parameters. During the SETAR
process, the regime switching of a lag variable is based on the threshold values
of the lag variable. Another advantage of SETAR model is the ability to cap-
ture some observed phenomena, which is in contrast to linear model, such as
irreversibility or jumps.
The basic SETAR at lag 1 is written as;
yt = φ1yt−1I(yt−1 ≤ci) + φ2yt−1I(yt−1 > ci) + εt,
(2)
where yt is the variable of interest, c is a threshold value. The assumption of εt
is i.i d. mean zero sequence with a bound density function, and E|ut|2y < ∞for
some γ > 2. Another assumption is |φ1| ≤1 and |φ2| ≤1.
The main characteristic of TAR model is the discontinuous feature of the
AR relationship. Smooth transition autoregressive (STAR) was proposed by
Ter¨asvirta and Anderson [12] as its continuous feature is more natural. In STAR
model, the indicator function is replaced by smooth function with sigmoid char-
acteristics. The basic STAR takes the following form;
yt = φ1yt−1Φi(yt−1; ψi≤ci) + φ2yt−1Φi(yt−1; ψi > ci) + εt
(3)

Forecasting Thailand’s Exports to ASEAN with Non-linear Models
343
Now, the transition function Φi(yt−1; ψi) is continuous function between
0 and 1, with parameter ψi. The option of transition function depends on
its regime-switching behavior. The common use is logistic function, where f
denotes the ﬁrst-order logistic function with parameters ψ = (γi, ci) for regime
i : f(yt−1; ψi) = (1 + exp(−γi(yt−1 −ci)))−1. This resultant model is known as
the Logistic STAR (LSTAR) model with its parameter c being threshold between
two regimes.
2.2.2
Markov Switching Model
With this model, it is assumed that there tends to be a diﬀerent regression model
correlated with its regime. Regime-switching models build on the concept of a
mixture of parametric distributions, whose mixture probabilities are subject to
unobserved state variable. For Markov switching model, regime-switching models
lie in the stochastic structure of the state variables which its state of unobserved
process is modeled by discrete space Markov chain.
In this paper, we focus on a Markov switching Autoregressive (MSAR) model.
A simple autoregressive model of order one in which the mean of the process
switches between two regimes. In this paper, we consider two states; of high
growth and of low growth according to a Markov chain process. It can be
written as;
yt = ust + φst(yt−1) + εSt, εSt ∼(0, σ2
St)
(4)
The regime transitions depend on the transition probabilities, which can be
expressed as;
P(St = i|St−1 = j) =
P00 P10
P01 P11

,
(5)
where Pij is the probability of transitioning from regime i, to regime j.
2.2.3
Kink Autoregression (AR) Model
Kink AR model comes from the combination of the simple AR model and the
Kink model introduced by Hansen [10]. The advantage of Kink model is to have
the continuous function in all variables. Meanwhile, the slope has a discontinuity
at a threshold point. The lag data is separated into two (or more) groups based
on its indicator function. For example, the Kink-AR (1) process of autoregressive
order m can be expressed as;
yt = u + φ1iyt−1I(yt−d ≤ri) + φ2iyt−iI(yt−d > ri) + εt,
(6)
where yt is interest variable, u is the mean of interest variable, φi1 and is a lower
regime autoregressive coeﬃcients, meanwhile φ2i is an upper regime one. I is an
indicator variable by having yt−d as a determinant of the switching point. r is
the threshold parameter and it also represents a kink point value. It presents
the regime of mean equations through its indicator function I(yt−d ≤ri) for a
lower regime, meanwhile I(yt−d > ri) for an upper one. ϵt is the innovation of
the time series process.

344
P. Boonyakunakorn et al.
2.3
Evaluation of Forecasting Model Performance
The accuracy of future forecast is the most concern. Therefore, it is essential
for evaluating the forecasting performance with its forecasting reliability that
it is based on out-of-sample performance. In this paper, only root mean square
error (RMSE) and mean absolute error (MAE) are the selected methods used
to measure forecasting accuracy and reliability.
2.3.1
Root Mean Square Error (RMSE)
RMSE benchmark evaluates the forecasting model performance by measuring
the average magnitude of the error. It takes the square root of squared the
diﬀerence between actual observations and forecasting values.
RMSE =



 1
n
n

i=1
(yi −ˆyi)2
(7)
2.3.2
Mean Absolute Error (MAE)
MAE is implied to measure the average magnitude of the errors, in the other word
it considers only the magnitude, and ignore the error direction. Consequently,
when all observation diﬀerences have the same weight, the average is over the
test of the actual observations and forecasting values.
MAE = 1
n
n

i−1
|yt −ˆyt|
(8)
The smallest values in each model generated by both RMSE and MAE are
preferred and compared to each other since the smallest values denote the best
forecasting ability of that model. Both values are in the range from zero to
inﬁnity to the error direction. The advantage of RMSE is to provide a relatively
high weight to large errors. Therefore, it is more ﬁtting to use when large errors
are a concern. In contrast, Willmott and Matsuura [18] stated that the RMSE
tends not to be an appropriate indicator of average model performance because
it can lead to misleading measure of average error. Therefore, in this case MAE
is more appropriate. Meanwhile Chai and Draxler [5] found that the RMSE is
superior to MAE when model errors are expected to be a Gaussian distribution,
and there are enough observations. Therefore, we apply both RMSE and MAE
to access model performance.
3
Data
Thailand’s exports to ASEAN data are monthly time series for the period from
January 2002 to December 2016 consisting of 180 data. Thailand’s exports to
ASEAN values are calculated from the values of Thai exports to all ten ASEAN’S
members; Brunei, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines,

Forecasting Thailand’s Exports to ASEAN with Non-linear Models
345
Singapore and Vietnam. The export data is directly obtainable from Bank of
Thailand (BOT) source. The application of the estimation method requires sta-
tionary data. We ﬁrstly use the log to transform Thailand’s exports to ASEAN
value, to reduce the asymmetry of time series data. Then we will apply unit
roots test to check the stationarity.
In order to compare forecasting performance with respect to RMSE and
MAE methods, the data is organized in the following terms. The data is ﬁrstly
separated into two parts consisting of in sample and out of sample. The ﬁrst 168
data contained in sample are for the period January 2001 to December 2015. The
remaining data in out of sample are for the period January 2016 to December
2016. Secondly, the remaining data will be used to compare with the actual data
via RMSE and MAE. The smallest RMSE and MAE is referred to better forecast
performance.
4
Empirical Results
Unit roots test is initially applied to check the stationarity of the log of Thailand’s
exports to ASEAN value by using Augmented Dickey-Fuller (ADF) unit root test
and Phillips-Perron (PP) unit root test. Table 1 shows that it is nonstationary
for a unit root with drift in both ADF and PP unit root test, but it is stationary
for a unit root with drift and trend. For the ﬁrst diﬀerence, both unit root tests
provide stationary results (Fig. 2).
Table 1. Unit root test
Test
Level
First diﬀerence
Constant Constant and trend Constant
Constant and trend
ADF –2.158
–2.764
–19.393*** –19.461***
PP
–2.012
–3.414*
–20.572*** –21.651***
Note: *** signiﬁcant at the 1% level, ** signiﬁcant at the 5% level, *
signiﬁcant at the 10% level.
Fig. 2. Log of ASEAN series and ASEAN diﬀerentiated series

346
P. Boonyakunakorn et al.
Table 2. Test of linearity
Hypothesis Test
Test 1 vs 2 10.673*
Test 1 vs 3 13.897
Test 2 vs 3
3.041
Note: *** signiﬁcant at
the 1% level, ** signiﬁ-
cant at the 5% level, *
signiﬁcant at the 10%
level.
In this paper, we employ test of linearity against bootstrap distribution from
Hansen [10]. Both the linear AR versus one threshold TAR, and Linear AR
versus two thresholds TAR are referred as the linearity test. If we reject the
linearity, therefore there is presence of a threshold. The last test is one-threshold
TAR versus two-thresholds TAR to choose whether there will be one or two
thresholds. The result demonstrated in Table 2 indicates that two-threshold TAR
is appropriate.
The order of the AR process is assumed as one lag for all the selected models.
The ﬁtted model criterion in this study is AIC. Table 3 shows that the SETAR
Table 3. Estimated parameters of the models
Parameter
Linear AR SETAR
LSTAR
MSAR
Kink AR
Constant
0.011*
–
–
–
0.005
(–0.007)
–
–
–
(–0.042)
Beta
–0.359***
–
–
–
–
(–0.07)
–
–
–
–
Constant (L)
–
0.114***
0.274**
0.018
–
–
(–0.04)
(–0.137)
(–0.008)
–
Beta (L)
–
0.316
1.17
–0.483*** –0.383*
–
(–0.315)
(–0.726)
(–0.082)
(–0.196)
Constant (H) –
0.00003
–0.272*
–0.018
–
–
(–0.009)
(–0.139)
(–0.035)
–
Beta (H)
–
–0.249*** –1.437*** 0.24
–0.336**
–
(–0.104)
(–0.726)
(–0.301)
(–0.161)
Threshold
–
–0.066
–0.105*** –
0.013
–
–
(–0.026)
–
(–0.138)
AIC
–797.484
–800.25
–798.815
–320.469
–311.828
Note: *** signiﬁcant at the 1% level, ** signiﬁcant at the 5% level, * signiﬁcant
at the 10% level.

Forecasting Thailand’s Exports to ASEAN with Non-linear Models
347
displays the lowest AIC of –800.250 with its threshold value of –0.066; meanwhile
the second lowest AIC of –798.815 belongs to LSTAR with its threshold value of
–0.105 at the level of 1% signiﬁcant, followed by Kink AR with the ﬁfth lowest
AIC of –311.828 and its threshold of 0.013.
For MSAR model, the time-varying probabilities in Table 4 show that the
dependence relies on the transition probabilities with probability of remaining
in the origin regime P(St = 1|St−1 = 1) being 0.9844 for the high output state
and P(St = 2|St−1 = 2) being 0.8665 for the low output state.
Table 5 shows that the SETAR has the lowest errors in both RMSE and MAE
criteria, which are 0.0928 and 0.0669 respectively, compared with the second
lowest errors Linear AR model, 0.0989 and 0.0675 respectively. Whereas, the
highest RMSE and MAE values are 0.1278 and 1.1065, which belong to Kink
AR model (Figs. 3 and 4).
Table 4. Transition probability
Regime 1
2
1
0.9844 0.1334
2
0.0156 0.8665
Table 5. The forecasting performance of the model
Selected model Linear AR SETAR
LSTAR MSAR Kink AR
RMSE
0.0989**
0.0928*** 0.1013
0.1102
0.1278
MAE
0.0675**
0.0669*** 0.0747
0.0747
0.1065
Note: *** the ﬁrst lowest values, ** the second lowest values.
Fig. 3. Plotting actual values with predicted values of all the models

348
P. Boonyakunakorn et al.
Fig. 4. Forecasting the growth of Thailand’s exports to ASEAN for next 6 months
5
Conclusion
In this paper, we employ ﬁve diﬀerent models namely Autoregressive (AR) model,
a linear model, followed by four non-linear models, Self-exciting autoregressive
(SETAR), Logistic STAR (LSTAR), Markov-switching Autoregressive (MSAR)
and Kink Autoregressive (AR) models to forecast Thailand’s exports to ASEAN.
The results show that SETAR has the best forecasting performance in terms of low-
est RMSE and MAE. The second best forecasting performance belongs to Linear
AR model. Meanwhile, the worst forecasting performance is from Kink AR model.
This indicates that among diﬀerent four non-linear models, the SETAR is
the only method that outperforms Linear AR model. Therefore, it is not clear
cut to conclude that all the non-linear models have outperformed linear fore-
casting model. However, we can conclude that the SETAR is the most suitable
for forecasting Thailand’s exports to ASEAN compared with other non-linear
models.
References
1. Bec, F., Bouabdallah, O., Ferrara, L.: The way out of recessions: a forecasting
analysis for some Euro area countries. Int. J. Forecast. 30(3), 539–549 (2014)
2. Boero, G., Marrocu, E.: The performance of non-linear exchange rate models: a
forecasting comparison. J. Forecast. 21(7), 513–542 (2002)
3. Brock, W.A., Potter, S.M.: 8 Nonlinear time series and macroeconometrics. In:
Maddala, G.S., Rao, C.R., Vinod, H.D. (Ed.) Handbook of statistics, North
Holland, Amsterdam, vol. 11, pp. 195–229 (1993)
4. Camacho, M., Quiros, G.P., Poncela, P.: Green shoots and double dips in the euro
area: a real time measure. Int. J. Forecast. 30(3), 520–535 (2014)
5. Chai, T., Draxler, R.R.: Root mean square error (RMSE) or mean absolute error
(MAE)? arguments against avoiding RMSE in the literature. Geoscientiﬁc Model
Dev. 7(3), 1247–1250 (2014)
6. Djeddour, K., Boularouk, Y.: Application of threshold autoregressive model: mod-
eling and forecasting using US export crude oil data. Am. J. Oil Chem. Technol.
9(1), 1–11 (2013)

Forecasting Thailand’s Exports to ASEAN with Non-linear Models
349
7. Ferrara, L., Marcellino, M., Mogliani, M.: Macroeconomic forecasting during the
great recession: the return of non-linearity? Int. J. Forecast. 31(3), 664–679 (2015)
8. Granger, C.W.: Non-linear models: where do we go next-time varying parameter
models. Stud. Nonlinear Dyn. Econometrics 12(3), 1–9 (2008)
9. Hamilton, J.D.: A new approach to the economic analysis of nonstationary time
series and the business cycle. Econometrica J. Econometric Soc. 57, 357–384 (1989)
10. Hansen, B.: Testing for linearity. J. Econ. Surv. 13(5), 551–576 (1999)
11. Mayer, T., Melitz, M.J., Ottaviano, G.I.: Market size, competition, and the product
mix of exporters. Am. Econ. Rev. 104(2), 495–536 (2014)
12. Ter¨asvirta, T., Anderson, H.M.: Characterizing nonlinearities in business cycles
using smooth transition autoregressive models. J. Appl. Econometrics 7(S1), 119–
136 (1992)
13. Ter¨asvirta, T., Van Dijk, D., Medeiros, M.C.: Linear models, smooth transition
autoregressions, and neural networks for forecasting macroeconomic time series: a
re-examination. Int. J. Forecast. 21(4), 755–774 (2005)
14. Tiao, G.C., Tsay, R.S.: Some advances in non-linear and adaptive modelling in
time-series. J. Forecast. 13(2), 109–131 (1994)
15. Tong, H.: On a threshold model. In: Chen, C. (ed.) Pattern Recognition and Sig-
nal Processing. NATO ASI Series E: Applied Sc. (29), pp. 575–586. Sijhoﬀand
Noordhoﬀ, Amsterdam (1978)
16. Tong, H., Lim, K.S.: Threshold autoregression, limit cycles and cyclical data. J.
R. Stat. Soc. Ser. B (Methodological) 42, 245–292 (1980)
17. Sarantis, N.: Modeling non-linearities in real eﬀective exchange rates. J. Int. Money
Finance 18(1), 27–45 (1999)
18. Willmott, C.J., Matsuura, K.: Advantages of The mean absolute error (MAE) over
The root mean square error (RMSE) in assessing average model performance. Clim.
Res. 30(1), 79–82 (2005)

Thailand in the Era of Digital Economy:
How Does Digital Technology Promote
Economic Growth?
Noppasit Chakpitak1, Paravee Maneejuk2,3(B), Somsak Chanaim2,3,
and Songsak Sriboonchitta2,3
1 International College, Chiang Mai University, Chiang Mai, Thailand
Nopasit@camt.info
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
mparavee@gmail.com, somsak ch@cmu.ac.th, songsakecon@gmail.com
3 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
Abstract. As Thailand has undergone the reformation in both social
and economic dimensions due to the digital economy, technologies are
now becoming the new driving forces of economic growth. Therefore,
an attempt of this study is to provide an empirical evidence on this
issue, examining how increases in digital technologies impact the Thai
economy. This study employs the stochastic frontier model estimated by
entropy approach to model the production function. Because of a speciﬁc
capability of this model, we are also able to ﬁnd out how eﬃciently those
technologies are utilized. The estimated results show that technologies
can contribute positively to the Thai economy although the magnitudes
are small. Moreover, our ﬁnding emphasizes that the digital technologies
are not being used at the maximum capability, therefore, there is still a
room for improvement in Thailand.
1
Introduction
Digital technology has profoundly become a player in our life, society as well as
economy. As we can see, most people around the world are now online and spend
a lot of time on the internet searching for contents, visiting social network sites,
or making online purchases. Impacts of digital technology have also pervaded
the ways entrepreneurs run their businesses, the ways markets are organized,
and a lot more. These boundless connectivity and business transformations are
examples of things that happen in the Digital Economy. But what does Digital
Economy really mean? This term has become well known to people since Don
Tapscott, a business executive, author, and business inspirator, introduced in
his 1995 best-seller book, titled “The Digital Economy: Promise and Peril in the
Age of Networked Intelligence.” This book shows how internet and digital tech-
nologies impact businesses as well as our economy. Digital economy is simply an
economy based on digital technologies by which economic activities are highly
related to digital things. For example, customers can buy products and obtain
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_25

Thailand in the Era of Digital Economy
351
services through the digital transactions; businesses can use innovation and tech-
nology to improve their processes and outputs, increase their eﬃciency as well
as proﬁtability. Moreover, there are also many products have been invented by
using advanced digital technologies, such as wireless networks and smartphone,
which in turn enable network of economic activities to be more worldwide.
Thailand is one of the countries that considers the digital technologies as a
new engine for driving economic growth. Let’s get back to early stage of economic
growth. We grew the economy by taking advantages of country’s resources, in
other words, the traditional drivers. But as the resources became limitedly avail-
able, the economy under traditional driving force went into a stage of maturity
and eventually stagnation. As such, Thailand has shifted its attention to a new
approach called “Creative Economy”. This approach is employed prior to the
digital economy; however, all are based upon the same thing that is knowledge
(Powell and Snellman [10]). The Thai government at that time paid attention
to the importance of knowledge and intellectual capabilities coupled with strong
culture, in inventing new and unique products as happened in Japan’s economy
as well as in many developed economies to add greater products and services
values. Despite the fact that this is the right way to propel the Thai economy for-
ward, the development process under creativity could not reach the goal. This is
due to many reasons: ﬁrst, prices of creative products could change dramatically;
and second, there were the disturbances and the unstable political situations
during that time. Hence, this approach is then transferred to the present era of
digitalization. The Thai government still considers the importance of knowledge
and intellectual capabilities, but together with digital technologies. The govern-
ment aims to grow the economy through the creation of digital technologies.
Many economists have spent their eﬀorts examining the impacts of digital-
ization particularly in terms of information and communications technologies
(ICTs) on the factor productivity and economic growth in empirical ways. The
literature shows a signiﬁcant role of digital technologies in stimulating developed
economies, such as the US, Europe, Japan, and Australia (see, e.g., Qu et al.
[11]; Gordon [12]; and Audretsch and Welfens [13]). However, less attention has
been paid to the developing counties. In particular, based on our knowledge,
the empirical study on this issue in the context of Thailand has not been con-
ducted, so this gap becomes a great opportunity for us to ﬁll up the gap by
providing the empirical evidence on how rising in digital technologies impacts
the economic growth in Thailand. In particular, we will estimate this impact
using broad indicators, for example telephone and mobile cellular subscriptions,
number of internet users, urban population growth, and gross capital formation.
Moreover, we also aim at quantifying how eﬃciently those technologies are uti-
lized. In particular, we would like to know whether Thailand can make full use
of digital technology to stimulate its economy.
We will introduce a quantitative method used to accomplish these goals as well
as the estimating technique and procedures through Sect. 3. But prior to that,
we will describe all considered variables through a section of data description or
Sect. 2. In Sect. 4, we will discuss about the estimated impacts of digital technolo-

352
N. Chakpitak et al.
gies on the Thai economy and also their estimated performances over time in terms
of the eﬃciency scores. Finally, Sect. 5 concludes.
2
Data Description
As mentioned earlier, the expansion of digital economy can be seen through
the continued growth of access to diﬀerent technologies. However, this study is
focusing on some speciﬁc indicators which can be described in the table below
(Table 1).
All the variables of Thailand are collected annually from 1975 to 2015, from
Thomson Reuters Datastream. In addition, we use Thailand’s gross domestic
product (GDP) measured in current US dollar as a dependent variable. The
descriptive statistics of all considered variables are presented in Table 2, and the
plots for each variable are shown in Fig. 1. This study relies on the framework of
the neoclassical growth model in which the Cobb-Douglas production function is
employed. But without a concern about the traditional economic driver, output
is a function of only digital technologies: ﬁxed telephone subscriptions, internet
users, mobile cellular subscriptions, urban population growth, and gross capital
formation.
3
Methodology
This study makes use of the stochastic frontier model (SFM) in examining how
digital technologies impact the economic growth. The SFM was simultaneously
proposed by Aigner et al. [1], Meeusen and van Den Broeck [3], and Battese and
Coelli [2]. This model comprises two error components; one is statistical noise
and the other is the technical ineﬃciency. The advantage of this model lies in
measuring changes in output relative to changes in input variables, which are
technologies. However, due to the limitation of our data, the conventional maxi-
mum likelihood for SFM is no longer appropriate. In addition, the assumption of
normal distribution and half-normal distribution for the noise and technical inef-
ﬁciency, respectively, becomes our concern. To improve the performance of this
model, two error components should not be restricted to speciﬁc distributions,
especially for a case that we only have a small sample size. To deal with this
problem, this study considers the entropy estimation, introduced in Golan [9],
which has been proved to be robust under the ill-posed and ill-conditioned prob-
lems. According to maximum entropy estimation, we establish a discrete set of
support points for each parameter and then estimate the probability associated
with each support point to arrive at the parameter estimate. For a stochas-
tic production frontier estimated by the generalized maximum entropy (GME)
approach, see, for examples, Macedo and Silva [15] and Campbell et al. [14].

Thailand in the Era of Digital Economy
353
Table 1. Data description
Variable
Label
Description
Fixed telephone subscriptions
(per 100 people)
TELt Fixed telephone subscriptions refers to the
sum of active number of analogueﬁxed
telephone lines, voice-over-IP (VoIP)
subscriptions, ﬁxed wireless local loop
(WLL) subscriptions, ISDN voice-channel
equivalents and ﬁxed public payphones
Internet users (per 100
people)
INTt
Internet users refers to the number of
individuals who have used the internet in
the last 12 months via computer, mobile
phone, personal digital assistant, games
machine, digital TV and other devices
Mobile cellular subscriptions
(per 100 people)
MBt
Mobile cellular subscriptions are
subscriptions to a public mobile telephone
service that provides access to the public
switched telephone network (PSTN) using
cellular technology. This variable includes
the number of postpaid subscriptions and
the number of active prepaid accounts (in
the past 3 months). It excludes subscriptions
via data cards or USB modems,
subscriptions to public mobile data services,
private trunked mobile radio, telepoint,
radio paging and telemetry services
Urban population growth
(annual %)
UPt
Urban population refers to people living in
urban areas as deﬁned by national
statistical oﬃces. It is calculated using
World Bank population estimates and
urban ratios from the United Nations World
Urbanization Prospects
Gross capital formation
(Unit: current local currency)
GCFt Gross capital formation or gross domestic
investment consists of outlays on additions
to the ﬁxed assets of the economy plus net
changes in the level of inventories. Fixed
assets include land improvements, plant,
machinery and equipment, construction of
roads, railways, and also buildings.
Inventories are stocks of goods held by ﬁrms
to meet temporary or unexpected
ﬂuctuations in production or sales, and
work in progress. Moreover, net acquisitions
of valuables are also considered capital
formation

354
N. Chakpitak et al.
Table 2. Descriptive statistics
log(GDPt) log(TELt) log(INTt) log(MBt) log(UPt)
log(GCFt)
Mean
10.9906
0.539868
0.306654
0.483752
0.444884
11.90412
Median
11.09099
0.721735
0
0.221569
0.462357
12.05308
Maximum 11.62313
1.048138
1.542701
2.159684
0.697229
12.55023
Minimum
10.17268
−0.28623
−1.85387
−2.69897
0
10.90902
Std. Dev
0.414931
0.484124
0.815474
1.20145
0.18377
0.492912
Skewness
−0.27582
−0.42193
−0.40881
−0.40563
−0.50845 −0.55686
Kurtosis
2.01738
1.557321
2.9977
2.840715
2.370628
2.04439
Source: Calculation.
Fig. 1. The plots of historical data of variables

Thailand in the Era of Digital Economy
355
3.1
A Stochastic Frontier Model
The most general form of the stochastic frontier model (SFM) can be written as
Yt = f(X′
tkβ) · TE
(1)
Yt = X′
tkβ + εt, t = 1, · · · , T,
(2)
εt = Ut −Vt,
(3)
where Yt is the output variable and X′
tk is T × K a matrix of K diﬀerent input
quantities. The term β represents (T × K) matrix of estimated parameters of
the input variables. The function f(·) is the functional form of SFM which is
imposed to be the Cobb–Douglas production function. The term TE denotes
technical eﬃciency and εt is the composite error term which consists of the
noise, Ut, and the ineﬃciency, Vt. In general, the distribution assumptions in
Eq. (3) are normal distribution for Ut and half-normal for the random term, Vt.
But in this study, we do not assume any distribution for Ut and Vt since the
entropy estimation is proposed to estimate the model. However, a restriction on
Vt is given as Vt ∈[0, ∞). These two error components are distributed identically
and independently from each other and the regressor.
In the context of the SFM, the technical eﬃciency or TE can be deﬁned as
the ratio of the observed output to the corresponding frontier output, conditional
on the levels of inputs. Therefore, the technical eﬃciency or TE is given by
TEt = exp[Xtk + Vt −Ut]
exp[Xtk + Vt]
(4)
3.2
Estimating Technique: Generalized Maximum Entropy (GME)
Approach
In this study, we propose to use maximum entropy estimator to estimate our
unknown parameters in Eq. (2). Before, we have discussed brieﬂy about the GME
estimator. However, the advantage and the properties of this estimator are pro-
vided at length in Golan [9] and Mittelhammer et al. [7], and more recently in
Tonini and Pede [5].
In brief, the core advantage of the GME estimator is that, ﬁrst, it eﬃciently
takes into account all the information contained in each data point. Second,
it is less aﬀected by outlier since we can give the probability weight between
signal and noise in the objective function. Third, it is a robust estimator as it
does not require any assumption regarding the error terms. And fourth, the GME
estimator does not require strong behavioral assumptions on the underlying data
generating process.
The maximum entropy concept consists of inferring the probability distri-
bution that maximizes information entropy given a set of various constraints.
Let pk be a proper probability mass function on a ﬁnite set of β. Shannon [6]
developed his information criteria and proposed a classical entropy, that is:
H(p) = −
K

k=1
pk log pk
(5)

356
N. Chakpitak et al.
where K
k=1 pk = 1. The entropy measures the uncertainty of a distribution
and reaches a maximum when pk is uniformly distributed (Wu [8]). With the
underlying model, the SFM, the objective entropy can be written as
H(p, w, v) = −
K

k=1
M

m=1
pkm log pkm −
T

t=1
U

u=1
wtu log wwtu −
T

t=1
N

n=1
vtn log vtn
(6)
where pkm, wtu, and vtn are the probability of estimated parameters, noise Ut,
and ineﬃciency Vt. The constraint of this objective function is given by
Yt =
M

m=1
pkmzkmXk
t +
M

m=1
wtmhtm −
M

m=1
vtmgtm,
(7)
and
M

m=1
pkm = 1,
M

m=1
wkm = 1,
M

m=1
vkm = 1.
(8)
The support values zkm, htm, and gtm, are needed to estimate the unknown
parameters, noise Ut, and ineﬃciency Vt in the SFM. Thus,
βk =
M

m=1
pkmzkm
(9)
Ut =
M

m=1
wtmhtm
(10)
Vt =
M

m=1
vtmgtm
(11)
Suppose that SFM has one regressor k = 1; therefore, the optimization prob-
lems from Eqs. (7) and (9) can be solved by the Lagrangian method, which takes
the form
L = H(p, w, v) + λ′
1

Yt −
M

m=1
pkmzkmXk
t −
M

m=1
wtmhtm +
M

m=1
vtmgtm

+ λ′
2(1 −
M

m=1
pm) + λ′
3(1 −
M

m=1
wm) + λ′
4(1 −
M

m=1
vm)
(12)

Thailand in the Era of Digital Economy
357
where λ′
i, i = 1, 2, 3, 4 are the vectors of Lagrangian multiplier. The ﬁrst-order
conditions are:
∂L
∂pm
= −log(pm) −
M

m=1
λ1mzmXt −λ2t = 0
(13)
∂L
∂wtm
= −log(wtm) −
M

m=1
λ1mhtm −λ3t = 0
(14)
∂L
∂vtm
= −log(vtm) −
M

m=1
λ1mgtm −λ4t = 0
(15)
∂L
∂λ1
= Yt −
M

m=1
pkmzkmXk
t −
M

m=1
wtmhtm +
M

m=1
vtmgtm = 0
(16)
∂L
∂λ2
= 1 −
M

m=1
pm
(17)
∂L
∂λ3
= 1 −
M

m=1
wm
(18)
∂L
∂λ4
= 1 −
M

m=1
hm
(19)
Solving the optimization of this problem, we yield the optimal and unique solu-
tion as in the following:
pm =
exp[−zm

t λ1Xt]
M
m=1 exp[−zm

t λ1Xt]
(20)
wtm =
exp[−λ1htm]
M
m=1 exp[−λ1htm]
(21)
vtm =
exp[−λ1gtm]
M
m=1 exp[−λ1gtm]
(22)
Finally, this study employs a bootstrap approach as proposed by Efron [4] to esti-
mate the standard error of each parameter. Bootstrapping is a general approach
in statistical inference based on replacement of the true sampling distribution
for a statistic by resampling from the actual data. In the bootstrap SFM pro-
cedure, the entropy method is used to estimate the parameters of the SFM. In
this study, we estimate the bootstrap standard errors from 1,000 repetitions.
In this study, the supports for zkm are given as
[
βk −3σβk, 
βk −1.5σβk, 
βk, 
βk + 1.5σβk, 
βk + 3σβk],
the supports for htm as
[−σv, −.75σv, 0, .75σv, σv],

358
N. Chakpitak et al.
and the supports for gtm as
σv
6 [0, .75, 1.5, 2.25, 3].
We estimate βk and σv by using package frontier in program R.
4
Empirical Results
This section is about the empirical results. We apply the previous algorithm to
examine the impacts of digital technologies on the GDP. All considered variables
within the framework of neoclassical economics, are modeled under the SFM,
which is given by
log(GDP)t = β0 + β1 log(TELt) + β2 log(INTt) + β3 log(MBt) + β4 log(UPt)
+ β5 log(GCFt) + Ut + Vt.
Table 3 reports the bootstrap means, which are our estimated coeﬃcients of
the variables used in the growth equations above, and their 95% conﬁdent inter-
vals from 1,000 repetitions of SFM. The result suggests that the eﬀects of any
changes in the given digital-technology variables including gross capital forma-
tion (GCF) will be translated into change in output of the country. However,
the magnitude of the eﬀects is surprisingly small. For example, the estimate for
ﬁxed telephone subscriptions (TEL) indicates that a 1% change in ﬁxed tele-
phone subscribers is associated with 0.0278% change in GDP. This eﬀect size
is quite similar to those of internet users (INT) and urban population growth
(UP). The estimate for mobile cellular subscriptions (MB) is relatively lower in
value. It can contribute just 0.0033% to the GDP. On the other hand, GCF just
shows the highest eﬀect size. We ﬁnd that a 1% change in gross capital formation
is associated with 0.7943% change in GDP.
The overall result indicates that technologies can contribute positively to
Thailand’s gross domestic product although the magnitudes are small. Figure 1
shows historical data for internet users and we can see that it increases grad-
ually since 1996, when Thailand obtained internet access as the third country
in Southeast Asia. During the second half of the 20th century, the number of
internet users grew rapidly as broadband internet improved. Internet becomes
readily available in most cities and towns, and expands to the countryside. More-
over, telephone service providers are also being nested within the internet, for
example releasing 3G and 4G, which in turn enable people to access internet
more easily. These rapid improvements make the number of internet hosts in
Thailand the highest in Southeast Asia (World Factbook, [16]), and hence the
growing digital economy. We believe that technologies especially internet can
contribute to the economy much more than the value occurs in the empirical
results. This is probably because productivity and eﬃciency of economic activi-
ties due to digital technologies are diﬃcult to measure. So, we may think about
a new measurement for this.

Thailand in the Era of Digital Economy
359
Table 3. Estimated results
Variable
Estimated coeﬃcient Bootstrap conﬁdence interval 95%
Intercept
1.7303
[1.6094, 1.8328]
log(TELt) 0.0278
[−0.1612, 0.1860]
log(INTt)
0.0101
[−0.0202, 0.0400]
log(MBt)
0.0033
[−0.0114, 0.015]
log(UPt)
0.0285
[−0.1365, 0.2052]
log(GCFt) 0.7943
[0.7794, 0.8009]
Fig. 2. Histogram plots for the bootstrap replications of parameters (β0, β1, β2, β3, β4
and β5) in the SFM

360
N. Chakpitak et al.
Figure 2 shows a graphical display of bootstrap parameters through his-
tograms. All the bootstrap parameters in SFM are estimated by maximum
entropy approach. We can see that β0, β2, β3, β4, and β5 seem to have a rea-
sonable shape. The bootstrap distribution of the intercept, and the coeﬃcients
of log(INTt), log(MBt), log(UPt), and log(GCFt) are reasonably symmetric.
4.1
Technical Eﬃciency
This section is conducted to answer the question of how eﬃciently those tech-
nologies are utilized. In fact, this is a contribution of the SFM allowing us to
access the information about technical eﬃciency (TE). The technical eﬃciency is
deﬁned by the ratio of the observed output to the corresponding frontier output
conditional on the levels of inputs used by a country. As such, the technologies
are said to be used eﬃciently to produce output if they are used in minimum
amount to produce maximum output, in other words, the highest TE score.
Figure 3 displays the TE values of production under the digital economy varying
between 0.765 and 0.776. The horizontal axis represents year, spanning from
1975 to 2015. This ﬁgure indicates that beyond the year 2005, Thailand can uti-
lize the digital technologies to propel the economy in terms of GDP with higher
eﬃciency levels than in the past. However, there is still a room for individuals,
businesses, as well as the government to improve further in using the new tech-
nologies, since the TE score does not yet reach the maximum value, i.e. 1. This
means the technologies and innovation are not being used at full eﬃciency at
present.
1975
1980
1985
1990
1995
2000
2005
2010
2015
u
0.764
0.766
0.768
0.77
0.772
0.774
0.776
TE
TE
Fig. 3. Estimates of technical eﬃciency

Thailand in the Era of Digital Economy
361
5
Conclusion
Thailand nowadays undergoes the reformation in both social and economic
dimensions due to the digital economy. The government, policy makers as well
as academics realize the advantages of digital technologies in driving economic
growth. This study attempts to provide an empirical evidence on this issue,
examining how increases in digital technologies impact the Thai economy in
terms of GDP. We estimate the impact using a set of variables, namely ﬁxed tele-
phone subscriptions, internet users, mobile cellular subscriptions, urban popula-
tion growth, and gross capital formation. The stochastic frontier model estimated
by entropy approach is used to model the production function. The speciﬁc capa-
bility of this model allows us to ﬁnd out how eﬃciently those technologies are
utilized. The estimated results show that technologies can contribute positively
to the Thai economy although the magnitudes are small. The result on technical
eﬃciency indicates that presently Thailand can make more use of the digital
technologies than in the past. However, the result shows there is still a room for
improvement in Thailand since the digital technologies are not being used at the
maximum capability.
This is crucial because if we can make full use of digital technologies to
produce output, to create new businesses, and to compete in a global market,
and this will bring about a rise in the country’s competitiveness. Apart from
business, making full use of technologies will also bring about higher quality
of life through an access to information and essential public services as well
as entertainments. Moreover, Thai people will become digitally competent by
international standards and be able to make full and creative use of technology
for their jobs.
References
1. Aigner, D.J., Lovell, C.A.K., Schmidt, P.J.: Formulation and estimation of sto-
chastic frontier production function models. J. Econ. 6, 21–37 (1977)
2. Battese, G.E., Coelli, T.J.: Frontier production functions, technical eﬃciency and
panel data: with application to paddy farmers in India. In: Gulledge, T.R., Lovell,
C.A.K. (eds.) International Applications of Productivity and Eﬃciency Analysis,
pp. 149–165. Springer, Dordrecht (1992)
3. Meeusen, W., van Den Broeck, J.: Eﬃciency estimation from Cobb-Douglas pro-
duction functions with composed error. Int. Econ. Rev. 18, 435–444 (1977)
4. Efron, B.: Bootstrap methods: another look at the jackknife. Ann. Stat. 7, 1–26
(1979)
5. Tonini, A., Pede, V.: A generalized maximum entropy stochastic frontier measuring
productivity accounting for spatial dependency. Entropy 13(11), 1916–1927 (2011)
6. Shannon, C.E.: A mathematical theory of communication. ACM SIGMOBILE
Mob. Comput. Commun. Rev. 5(1), 3–55 (2001)
7. Mittelhammer, R.C., Judge, G.G., Miller, D.J.: Econometric Foundations Pack
with CD-ROM, vol. 1. Cambridge University Press, Cambridge (2000)
8. Wu, X.: A weighted generalized maximum entropy estimator with a data-driven
weight. Entropy 11(4), 917–930 (2009)

362
N. Chakpitak et al.
9. Golan, A.: Information and entropy econometrics-editor’s view. J. Econ. 107(1),
1–15 (2002)
10. Powell, W.W., Snellman, K.: The knowledge economy. Annu. Rev. Sociol. 30, 199–
220 (2004)
11. Qu, J., Simes, R., O’Mahony, J.: How do digital technologies drive economic
growth? Econ. Record 93(S1), 57–69 (2016)
12. Gordon, R.J.: Secular stagnation on the supply side: US producivity growth in the
long run. Commun. Strateg. 1(100), 19–45 (2015)
13. Audretsch, D.B., Welfens, P.J.: The New Economy and Economic Growth in
Europe and the US. Springer Science & Business Media, Heidelberg (2013)
14. Campbell, R., Rogers, K., Rezek, J.: Eﬃcient frontier estimation: a maximum
entropy approach. J. Prod. Anal. 30(3), 213–221 (2008)
15. Macedo, P., Silva, E.: A stochastic production frontier model with a translog spec-
iﬁcation using the generalized maximum entropy estimator. Econ. Bull. 1, 587–596
(2010)
16. United States. Central Intelligence Agency, Government Publications Oﬃce (eds.):
The World Factbook 2014–15. Government Printing Oﬃce (2015)

Comparing Linear and Nonlinear Models
in Forecasting Telephone Subscriptions Using
Likelihood Based Belief Functions
Noppasit Chakpitak1, Woraphon Yamaka2(B), and Songsak Sriboonchitta2
1 International College, Chiang Mai University, Chiang Mai, Thailand
2 Faculty of Economics, Centre of Excellence in Econometrics,
Chiang Mai University, Chiang Mai, Thailand
woraphon.econ@gmail.com
Abstract. In this paper, we experiment with several diﬀerent models
with belief function to forecast Thai telephone subscribers. This approach
will provide an uncertainty about predicted values and yield a predictive
belief function that quantities the uncertainty about the future data.
The proposed forecasting models include linear AR, Kink AR, Threshold
AR, and Markov Switching AR models. Next, we compare the out-of-
sample performance using RMSE and MAE. The results suggest that the
out-of-sample belief function based KAR forecast is more accurate than
other models. Finally, we ﬁnd that the growth rate of Thai telephone
subscription in 2016 will fall around 6.08%.
Keywords: Telephone subscriptions
Likelihood-based belief functions
Linear and nonlinear autoregressive model · Forecasting
1
Introduction
Over the last decade, the rapid development of mobile phone and the expansion
of the Internet, information and communications technologies are decreasing
the ﬁxed telephone subscriptions. Telephone subscription here refers to the sum
of active number of analogue ﬁxed telephone lines, voice-over-IP (VoIP) sub-
scriptions, ﬁxed wireless local loop (WLL) subscriptions, ISDN voice-channel
equivalents and ﬁxed public payphones. Thus, forecasting the number of tele-
phone subscribers is very important for resource allocation and network plan-
ning. Moreover, it can help network providers optimize their resources, obviate
the waste of resources, and prepare the appropriate policy during this downturn
of ﬁxed telephone industry. So, a potent forecasting model is needed to obtain
accurate predictions.
In the context of forecasting, we desire to have a prediction of the future
growth of telephone subscription and assess uncertainty about them. Our study
considers the work of Kanjanatarakul et al. [8] which proposed the use of the-
ory of belief functions as an approach to achieve predicted intervals which is
based on a mathematical theory of Shafer [13]. This approach involves two steps.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_26

364
N. Chakpitak et al.
First step, a belief function on the parameter space in the forecasting model is
estimated from the normalized likelihood given the observed data. Second, the
prediction is made by writing the prediction value as a function of the parame-
ter and an auxiliary random variable with known distribution. This approach
is applied to the estimation by using the likelihood-based approach introduced
in Shafer [13] and further justiﬁed in Denouex [5]. The application to the pre-
diction problem can be found in the study of Kanjanatarakul et al. [8,9]. In
this study, we have interest to employ this approach to make a forecast, how-
ever, we are worried about the selection of the appropriate forecasting models
in the ﬁrst step. As in the literature, the application of belief function based
forecasting models have been found in many studies, such as Kanjanatarakul
et al. [8,9], Autchariyapanitkul et al. [1], Thianpaen et al. [11], but these studies
were conducted under a linear assumption. In economic data, there might exist
both linear and nonlinear behaviours, and hence using a wrong forecasting model
might bring about a large prediction bias [10].
The purpose of this study is to ﬁnd the appropriate model to predict the
future growth of telephone subscription in Thailand. Therefore we need to com-
pare the forecasting performance of linear and nonlinear models based on belief
function. In this study, we consider four competing Autoregressive (AR) models,
that are linear model, Threshold model of Tong [12], Kink model of Chan and
Tsay [2] and Hansen [7], and Markov Switching of Hamilton [6]. Our aim is to
examine whether forecasts from a nonlinear model are preferable to those from
a linear model in terms of forecast accuracy as well as forecast encompassing.
The remainder of this article is organized as follows. In the second section, we
brieﬂy review linear and nonlinear AR models. In the third section, the notion of
likelihood based belief function will be recalled. In the fourth section, we explain
the construction of a forecasting problem. In the ﬁfth section, we forecast the
growth of ﬁxed telephone subscription. The ﬁnal section provides a summary
and concludes.
2
A Brief Review of Linear and Nonlinear Models
2.1
Linear AR Model
Let {yt} , t = 1, ..., T be the observed data which are expressed as a linear com-
bination of past observations yt−p. In the autoregressive model of order (p), we
have
yt = α +
P

p=1
βpyt−p + εt
(1)
where βp is the autoregressive coeﬃcient at order (p). εt is the white noise which
is assumed to be normally distributed with mean zero and variance σ2.

Comparing Linear and Nonlinear Models in Forecasting Telephone
365
2.2
Kink AR Model
The Kink model is proposed by Chan and Tsay [2] and Hansen [7] in the context
of nonlinear regression, but in this study we use nonlinear autoregression context.
The basic idea of kink autoregression is that a piecewise linear autoregression
segment is split by threshold or kink parameter which focuses on lag variables
(yt−p). In this study we consider two regimes, hence the KAR model can be
described as
yt = α0 +
P

p=1
β−
p (yt−p ≤rp) +
P

p=1
β+
p (yt−p > rp) + εt,
(2)
where α0 is the intercept term, β−
p and β+
p are lower and upper regime autore-
gressive coeﬃcients, respectively, and rp is the threshold parameter or kink point
value deﬁning the regime through indicator function (yt−p ≤rp) and (yt−p > rp)
for upper regime. The εt term is the innovations of the time series process,
assumed to be normally distributed, εt ∼N(0, σ2).
2.3
Threshold AR Model
The Threshold model of Tong [12] is a nonlinear model which is decomposed
into two regimes, inside which is autoregression, and the sudden transition
across regimes is controlled by threshold variable r. This model is called “self-
exciting threshold autoregression (SETAR)”. It can be described as a piecewise
linear approximation to the general univariate autoregressive model of order (p)
such that
yt = α(1)
0
+ P
p=1 β(1)
p yt−p + ε1,t
if yt−d < r
yt = α(2)
0
+ P
p=1 β(2)
p yt−p + ε2,t
if yt−d ≥r
(3)
where α(j)
0
and β(j)
p
are the estimated parameters of regime j in the model, r is
threshold parameter and εt ∼N(0, σ2) is an T × 1 vector of independent and
identically distributed (iid) errors with normal distribution. Each regime is the
function of the past realizations of yt sequence itself. If yt−d < r, the model is
in lower regime at time t. In this model, we assume that the lag order (p) is the
same in both regimes.
2.4
A Markov Switching AR Model
In the autoregression-type structure, we can establish the Markov switching
autoregression model as
yt = β0,St +
p

p=1
βp,Styt−p + εt,St
(4)
where εt,St ∼i.i.d. N(0, σ2
St), yi,t is dependent variable and yt−p is lag dependent
variable at time t −p. β0,St denotes regime de-pendent intercept term βp,St and

366
N. Chakpitak et al.
βp,St denote the regime dependent autoregressive coeﬃcients of AR(p). The state
(or regime) is represented by St. Let {St}T
t=1 be the ﬁnite state Markov chain
with state or regime {1, ..., h}, thus the stationary transition probabilities
pij = P(St+1 = i |St = j ),
(5)
where pij is the probability of transition from regime iat time t + 1 conditional
on regime j at time t and
h
j=1
pij = 1. Thus, we can write the transition matrix as
P =
⎡
⎢⎢⎢⎢⎣
p11 p12 · · · p1j
p21
...
p2j
...
...
...
pi1 pi2 . . . pij
⎤
⎥⎥⎥⎥⎦
(6)
3
Likelihood-Based Belief Function
Shafer proposed, on intuitive grounds, a more direct approach in which a belief
function BelΘ
y on Θ is built from the likelihood function. In the recent writing
with paper, Denoeux [5] justiﬁed three basic principles of likelihood based belief
function, consisting the likelihood principle, compatibility with Bayesian infer-
ence and the least commitment principle. Let y ∈R denote the observed data
and θ ∈Θ is unknown parameter in the model. The likelihood function is a
mapping Ly from Θ to [0, ∞) which is deﬁned by
Ly(θ) = cfθ(y).
(7)
where c > 0 is an arbitrary multiplicative constant. When we rescale the likeli-
hood into the interval [0,1], by transformation we obtain the relative likelihood
Ry(θ) = L(θ)
L(θ),
(8)
where L(θ) is a likelihood function and L(θ) = sup θ ∈ΘL(θ), i.e., maximum
likelihood estimate (MLE) of θ. Shafer [13] mentioned that this relative likelihood
refers to a “relative plausibility”. Thus, it can be considered as the contour
function ply(θ) of a belief function BelΘ
y on Θ
ply(θ) = Ry(θ) = L(θ)
L(θ)
(9)
If BelΘ
y is assumed to be consonant belief function deﬁned from the normalized
likelihood function given observed data, then the plausibility is given by
PlΘ
y (H) = sup
θ∈H
ply(θ),
(10)

Comparing Linear and Nonlinear Models in Forecasting Telephone
367
for any hypothesis H ⊆Θ. The focal sets of BelΘ
y , which are the plausibility
regions, can be deﬁned as the set of parameter values θ whose relative plausibility
greater than some threshold
Γy(ω) = {θ ∈Θ |ply(θ) ≥ω } ,
(11)
where ω is uniformly in [0, 1]. This belief function BelΘ
y is equivalent to the
random set induced by the Lebesgue measure λ on [0, 1]and the multi-valued
mapping Γy from [0, 1] →2Θ [8].
4
Forecasting
In this study, we will propose a general solution to the forecasting problem in the
context of linear and nonlinear models. The forecasting problem is the inverse
of the previous one: given some knowledge about θ obtained by observing y, we
now can make statements about some random quantity y ∈R whose conditional
distribution given y depends on θ.
4.1
Problem Solution
To forecast the future data yf = (yT +1, ..., yT +h), the sampling model used by
Dempster [3,4] is introduced here. In this model, the forecast data Yf is expressed
as a function of the proper probability mass function θ which is obtained by past
observed data y = (y1, ..., yT ), and an unobserved auxiliary s ∈S variable with
known probability measure μ not depending on θ
yf = ϕ(θ, s),
(12)
where ϕ is deﬁned in such a way that the distribution of Yf for ﬁxed θ is gz,θ(yf).
When yf is a continuous random variable, Eq.(12) can be computed by
yf = G−1
y,θ(s),
(13)
where G−1
y,θ is the inverse conditional cumulative distribution function (cdf) of
Yf |y and s is uniformly in [0, 1]. Let Γy be the multi-valued mapping from
[0, 1]×U →2Y with ϕ, we get a new multi-valued mapping Γy from [0, 1]×S →2Y
deﬁned as
Γy : [0, 1] × U →2Y
(ω, s) →ϕ(Γy(ω, s).
(14)
The predictive belief function Bely and plausibility Ply are then induced by the
multi-valued mapping Γy and λ ⊗μ on [0, 1] × U as follows:
Bely(H) = (λ ⊗μ) {(ωi, si) |ϕy(Γy(ωi, si) ⊆H } ,
(15)
Ply(H) = Bely(H) = (λ ⊗μ) {(ωi, si) |ϕy(Γy(ωi, si) ∩H ̸= φ} ,
(16)
for all H ⊆Y .

368
N. Chakpitak et al.
4.2
Example linear and nonlinear model: prediction
Easy to explain, here we rewrite our linear and three nonlinear models in Eqs.(1)–
(4) as
yt = f(θ |yt−p ),
(17)
where f(·) is the distribution function of yt for ﬁxed θ. The data yt is expressed
as a function of the parameter θ and observed in the past yt−p. Then, we can
write, equivalently,
fy(θ, s) = f(θ |yt−p ) + σφ−1(s), U[0, 1]
(18)
where φ(·) is the normal quantile function and s has a uniform distribution in
the interval [0,1].
To forecast yt |yt−1 , the predictive belief function and plausibility function
can then be approximated using Monte Carlo simulation as in the following:
(1) We draw ωi and si independently from uniform [0,1] N draws. Then we
calculate the focal sets, the interval deﬁned by the following lower and upper
bounds:
fy(Γy(ω, s)) = {f(θ, s) |ply(θ) ≥ω }
=

yL(si, ωi), yU(si, ωi)

(19)
where yL(si, ωi) and yU(si, ωi) is a lower bound and upper bound, respec-
tively, which have normal likelihood N(min fy(ωi, si))and N(maxfy(ωi, si)),
respectively. This optimization problem can be solved using the nonlinear
optimization which takes the form as
yL(si, ωi) = min fy(ωi, si)
(20)
subject to
ply(θ) ≥ωi,
(21)
And
yU(si, ωi) = max fy(ωi, si)
(22)
subject to
ply(θ) ≥ωi,
(23)
(2) Thus, the predictive belief (Bely) and plausibility (Ply) functions that Y will
be less than or equal to yt are
Bely([0, y]) ≈1
N
N

i=1
Fyt−p,min fy(ωi,si)(y),
(24)
Ply([0, y]) ≈1
N
N

i=1
Fyt−p,max fy(ωi,si)(y),
(25)

Comparing Linear and Nonlinear Models in Forecasting Telephone
369
Similarly, the lower and upper expectations of yL and yUwith respect to
Bely and Ply, respectively, can be approximated by
¯yL = 1
N
N

i=1
min fy(ωi, si)
(26)
¯yU = 1
N
N

i=1
max fy(ωi, si)
(27)
Finally, to obtain the predictive yf, we take the mean of the sum of ¯yL and
¯yU, that is
yf = ¯yL + ¯yU
2
(28)
4.3
Constructing a Likelihood Function of Linear and Nonlinear
Models
According to the deﬁnition of the likelihood function based belief function as de-
scribed in Sect. 2, the likelihood L(θ |y ) is very important and the approximation
of Bely and Ply should only depend on the likelihood function. In this study,
we consider 4 competing likelihood models: AR, Kink AR, Threshold AR, and
Markov Switching AR where their functions can be written as follows:
(1) AR likelihood
L(θ |y ) =
1

2πσ2
t

1
2σ2
j
(yt −α −
P

p=1
βpyt−p)2

(29)
(2) Kink likelihood
L(θ |y ) =
1

2πσ2
t

1
2σ2
j
(yt −α0 −
P

p=1
β−
p (yt−p ≤rp) −
P

p=1
β+
p (yt−p > rp))2

(30)
(3) Threshold likelihood
L(θ |y ) =
2

j=1
⎧
⎨
⎩
1

2πσ2
j

1
2σ2
j
(yt −α(1)
0
−
P

p=1
β(1)
p yt−p)2
⎫
⎬
⎭I(yt−d < r)·
⎧
⎨
⎩
1

2πσ2
j

1
2σ2
j
(yt −α(2)
0
−
P

p=1
β(2)
p yt−p)2
⎫
⎬
⎭I(yt−d ≥r)
(31)

370
N. Chakpitak et al.
(4) Markov Switching likelihood
L(θ |y ) =
2

j=1
1

2πσ2
St=j
⎛
⎝
1
2σ2
St=j
(yt −β0,St=j −
p

p=1
βp,St=jyt−p)2
⎞
⎠(St = j |θt−1) ,
(32)
where Pr(St = j |θt−1) is ﬁlter probabilities obtained from Hamilton
ﬁlter [6].
5
Forecasting the growth of ﬁxed telephone subscription
Given the special characteristics of the growth of ﬁxed telephone subscription,
a number of alternative forecasting models have been proposed, including linear
AR, KAR, TAR and MS-AR. The data set considered, derived from the Thomson
Reuters Data stream, consists of yearly data, from the ending of 1975 to 2015,
covering 40 observations. The data is plotted in Fig. 1. Then, we perform Aug-
mented Dickey Fuller unit root test on ﬁxed telephone subscription growth, and
use model of constant term and trend term for testing, and determine lagging
order number of AR model according to Akaiki Information Criterion (AIC).
The ADF-statistic result is −1.4954, which is greater than the critical value
at 1% signiﬁcance level (−3.6104). This indicates that the growth of telephone
subscription is stationary. Then, the lag selection for four competing models is
shown in Table 1. The results show that lag 1 provides the lowest AIC for all
models.
Telecphon subscription
1980
1990
2000
2010
-0.05
0.00
0.05
0.10
0.15
0.20
0.25
growth
Fig. 1. Growth rate of telephone subscription in Thailand.
5.1
Assessing Forecasting Performance: Out of Sample Forecast
To compare the forecasting performance, our study focuses on the out-of-sample
forecasting ability of the nonlinear and the linear models. It is important to
note that we examine exclusively the out-of-sample forecasting ability of the
four competing models. The out-of –sample forecasts refer to the period from

Comparing Linear and Nonlinear Models in Forecasting Telephone
371
Table 1. Lag selection for linear and nonlinear models
AR
Lag(1)
Lag(2)
Lag(3)
Lag(4)
AIC
−237.0589 −235.3267 −234.066
−232.8945
KAR
Lag(1)
Lag(2)
Lag(3)
Lag(4)
AIC
−96.54266 −95.1245
−94.9974
−93.4575
TAR
Lag(1)
Lag(2)
Lag(3)
Lag(4)
AIC
−235.7989 −233.1144 −235.2477 NaN
MS-AR Lag(1)
Lag(2)
Lag(3)
Lag(4)
AIC
−107.3489 −104.4907 NaN
NaN
Source: Calculation
Note : NaN refers to the estimation faced with the small
degrees of freedom thus causing a maximum likelihood to
fail to converge.
2011 to 2015, which is the testing period for our models. We perform a recursive
forecasting to update telephone subscription growth year by year, totally 5 years,
which give us enough data points to evaluate out-of sample forecast performance.
We compare the out-of-sample forecasts using two approaches, namely the root
mean square error (RMSE) and mean square error (MAE).We can compute the
RMSE and MAE as in below:
MAE = 1
N
N

j=1
|ej|,
RMSE =



 1
N
N

j=1
e2
j,
where N=5 and e = ytrue −yf.
The RMSE and MSE of the forecasts are reported in Table 2. The out of sam-
ple fore-casts made in the 2011-2015 are based on four diﬀerent models. Here,
the smaller the MAE and RMSE, the superior the corresponding model’s perfor-
mance. As indicated in this Table, the RMSE and MSE of the forecasts from the
belief function based KAR model is lower than the other belief function based
models. In addition, we also compare the performance of belief based model and
non-belief based model. We can observe the better forecast when belief function
is take into account. This implies that belief function based KAR outperforms,
and in general more accurate than, the other forecasting models. We expect that
if the model has a nonlinear characteristic and the assumption of the likelihood
structure based belief function is not correctly speciﬁed, a less accurate predic-
tion is obtained. Thus, we conclude that for telephone subscription growth, the
belief function based KAR model is preferred to the other models on the basis
of the forecast accuracy approach.

372
N. Chakpitak et al.
Table 2. Out of sample forecasting performance
Belief based model
Actual
AR(1)
KAR(1) TAR(1)
MS-AR(1)
Forecast 2011
−0.028
−0.0234 −0.0307 −0.0247 0.045
Forecast 2012
−0.0456 −0.0388 −0.0392 −0.0128 −0.0566
Forecast 2013
−0.0535 0.0116
−0.0325 0.0019
−1.8343
Forecast 2014
−0.0634 0.0084
−0.0201 0.0212
0.7838
Forecast 2015
−0.0694 0.0092
−0.0552 0.0145
−1.2531
MAE
0.0454
0.0174
0.0606
0.7791
RMSE
0.0559
0.0226
0.052
1.0291
Non-Belief based model Actual
AR(1)
KAR(1) TAR(1)
MS-AR(1)
Forecast 2011
−0.028
−0.0259 −0.0106 −0.0297 −0.0226
Forecast 2012
−0.0456 −0.0043 −0.025
−0.0121 0.0086
Forecast 2013
−0.0535 0.0127
0.0052
0.0024
−0.0014
Forecast 2014
−0.0634 0.0263
−0.0052 0.0147
0.0204
Forecast 2015
−0.0694 0.0371
0.0187
0.0249
0.0141
MAE
0.0714
0.0553
0.0621
0.0627
RMSE
0.0611
0.0486
0.0527
0.0558
Source: Calculation
-0.2
-0.1
0.0
0.1
0.0
0.2
0.4
0.6
0.8
1.0
cumulative belief/plausibility
PL
BEL
fitted
Fig. 2. Lower and upper cdf for one step ahead prediction: The forecasts made for
2016, with blue lines corresponding to the estimated expected values of forecast value..
Fig. 3. Marginal contour functions ply(θ) in two-dimensional parameter sub-spaces:
Kink AR model

Comparing Linear and Nonlinear Models in Forecasting Telephone
373
5.2
One Step-Ahead Prediction for Likelihood Based Belief
Function
The prediction is very important because of its relevance to the current policy
debate on whether Thai telephone subscription growth will be up or down. The
forecasts were made for year 2016. We then use belief function based KAR to
predict telephone subscription growth in the future. Figure 2 shows the predictive
belief function BelY concerning telephone subscription growth in 2016. These
plots pre-sent the lower and upper cumulative density functions (cdf) for the
prediction problem obtained from Ply(H) and Bely(H). The expected prediction
yf in 2016 is plotted by the vertical red dotted line. We can observe that the
value of growth rate is around −0.0608 in 2016.
Furthermore, Fig. 3 shows the accuracy of our estimation using the contour
plots of plausibility ply(θ) in two-dimensional parameter subspaces of belief func-
tion based Kink AR model. These contour plots show two-dimensional slices of
the contour function, where one of the ﬁve parameters is ﬁxed to its maximum
likelihood. The results of these contour plots show that the estimated parame-
ters of the model can reach the maximum plausibility, as presented by the red
cross. According to Fig. 3, we provide four sub-plots of plausibility function of
two parameters. For example, consider the top left sub plot, we have a plausi-
bility function pl(β0, β−
1 ) of two parameters intercept β0, and coeﬃcient β−
1 . In
the proﬁle plausibility of the intercept parameter, we ﬁx and just vary β−
1 and
vice versa. We can observe that the estimated parameters of Kink likelihood can
reach the maxi-mum plausibility. For other parameters, we also observe good
optimization.
6
Concluding Remarks
The paper compared out-of-sample forecasts of yearly growth of Thai telephone
subscription, generated by belief function based forecasting models. In this study,
we proposed various linear and nonlinear AR models, namely linear AR, TAR,
KAR, and MS-AR models, with belief function framework for the Thai telephone
subscription. Prior to making a prediction value, Akaiki information criterion
(AIC) is conducted to select the appropriate lag length for the four competing
models. We found that lag 1 is appropriate since it showed the minimum value
of AIC. The comparison of out-of-sample forecasts is carried out on the basis of
two methods: RMSE and MAE. Over-all, the results indicate that the inclusion
of nonlinear structure in the forecasting model is important in out-of-sample
forecast. We ﬁnd that belief function based KAR performs well in out-of-sample
forecast relative to other models. Finally, we apply the best ﬁt model to predict
the growth rate of Thai telephone sub-scription in 2016 and ﬁnd that the growth
rate will fall around 6.08%.
Acknowledgements. The authors are grateful to Puay Ungphakorn Centre of Excel-
lence in Econometrics, Faculty of Economics, Chiang Mai University for the ﬁnancial
support.

374
N. Chakpitak et al.
References
1. Autchariyapanitkul, K., Chanaim, S., Sriboonchitta, S., Denoeux, T.: Predicting
stock returns in the capital asset pricing model using quantile regression and belief
functions. In: Third International Conference Belief Functions: Theory and Appli-
cations, September 2014, Oxford, United-Kingdom. LNAI, vol. 8764, pp. 219–226.
Springer (2014)
2. Chan, K., Tsay, R.S.: Limiting properties of the least squares estimator of a con-
tinuous threshold autoregressive model. Biometrika 45, 413–426 (1998)
3. Dempster, A.P.: New methods for reasoning towards posterior distributions based
on sample data. Ann. Math. Stat. 37, 355–374 (1966)
4. Dempster, A.P.: The Dempster-Shafer calculus for statisticians. Int. J. Approx.
Reason. 48(2), 365–377 (2008)
5. Denouex, T.: Likelihood-based belief function: justiﬁcation and some extensions to
low-quality data. Int. J. Approx. Reason. 55, 1535–1547 (2014)
6. Hamilton, J.D.: A new approach to the economic analysis of nonstationary time
series and the business cycle. Econom. J. Econom. Soci. 57, 357–384 (1989)
7. Hansen, B.E.: Regression kink with an unknown threshold. J. Bus. Econ. Stat. 35,
228–240 (2017)
8. Kanjanatarakul, O., Sriboonchitta, S., Denoeux, T.: Forecasting using belief func-
tions: an application to marketing econometrics. Int. J. Approx. Reason. 55(5),
1113–1128 (2014)
9. Kanjanatarakul, O., Denoeux, T., Sriboonchitta, S.: Prediction of future observa-
tions using belief functions: a likelihood-based approach. Int. J. Approx. Reason.
72, 71–94 (2016)
10. Khiewngamdee, C., Yamaka, W., Sriboonchitta, S. Forecasting asian credit default
swap spreads: a comparison of multi-regime models. In: Robustness in Economet-
rics, pp. 471–489. Springer International Publishing (2017)
11. Thianpaen, N., Liu, J., Sriboonchitta, S.: Time series forecast using AR-belief
approach. Thai J. Math. 14, 527–541 (2016)
12. Tong, H.: Threshold Models in Non-linear Time Series Analysis. Lecture Notes in
Statistics, vol. 21. Springer, Berlin (1983)
13. Shafer, G.A.: Mathematical Theory of Evidence. Princeton University Press,
Princeton (1976)

Volatility in Thailand Stock Market Using
High-Frequency Data
Saowaluk Duangin1(B), Jirakom Sirisrisakulchai2, and Songsak Sriboonchitta2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
Saowaluk.econ@gmail.com
2 Center of Excellence in Econometrics, Faculty of Economics,
Chiang Mai University, Chiang Mai 52000, Thailand
Sirisrisakulchai@hotmail.com, songsakecon@gmail.com
Abstract. The objective of this research is twofold: First, we aim
to investigate the performance of conventional GARCH and GARCH-
jump models when the data has high frequency. Second, the obtained
conditional volatility from the best ﬁt model is used to forecast and
matched with the macroeconomic news announcement. We use GARCH
and GARCH-jump models with high-frequency dataset of log return of
Thailand stock market index (SET) from January, 2008 to December,
2015. We ﬁnd that the volatility estimations by these two models have the
same pattern but volatility estimation by GARCH-jump is higher than
conventional GARCH model. However, the GARCH (1,1) and GARCH
(1,1)-jump performances are non-stationary to estimate the volatility for
5 min interval return of SET but are stationary to estimate for 15 min,
30 min, 1 h, and 2 h returns of SET. Our results also show the match-
ing jump point with macroeconomic news announcement. The empirical
results support our assumption that macroeconomic news announcement
may lead to volatility change in SET.
1
Introduction
There are many indicators can reﬂect the economy’s health such as Gross Domes-
tic Product (GDP), Consumer Price Index (CPI), unemployment rate, etc.
However, the stock market can be used as an indicator of economic health.
When an economy move to expansion period then the stock market expected to
rising. There are many endogenous and exogenous factors aﬀecting a stock mar-
ket such as economic factors, world events, politics, natural disasters, etc. It is
well known that new information can aﬀect stock prices. The eﬃcient market
hypothesis (EMH) posits that “if markets are eﬃcient and current, prices always
reﬂect all available information”. News is a major driving force of asset price
volatility with implications on many segments of the economy both domestically
and event globally. Investors need to know how news release aﬀects stock return
and volatility. When new information occurs, the stock market will respond to
both bad and good news. Some traders and investors try to reason out or extract
the information contained in news to re-allocate their portfolio and decide on
risk management.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_27

376
S. Duangin et al.
Macroeconomic reports such as gross domestic product, consumer price
index, inﬂation rate, and unemployment rate are important sources of informa-
tion transcribing the changes in economic fundamental indicators or policies into
diﬀerent metaphors of return and volatility of stock market. On August 1, 2008
consumer price index (CPI)1 was announced to be 2.70%, which was well below
the expected value of 3.85% reported by Bloomberg. The CPI was released at
10.30 am and the Stock Exchange of Thailand (SET) index went up sharply after
the news release. The impact of macroeconomic news announcements on stock
market has previously been focused on the link between macroeconomic news
and developed markets such as U.S., U.K. and Germany. Andersen and Bollerslev
[2] and Rangel [3] found signiﬁcant impact of macroeconomic announcements on
stock market.
Only few studies have investigated the impacts of macroeconomic news
announcementa on Thailand stock market. Sidney [4] used low frequency data
(daily and monthly data) and showed the impact of domestic and foreign macro-
economic announcements on SET by using GARCH type model estimation and
found that SET responded to unexpected news but the impact of Thai news
announcement on SET seemed higher than foreign news announcement.
However, stock will show quick reaction to news in the order of minutes or
less. Currently, with the advanced computers and communications technology,
the data are readily available in high frequency. The high-frequency data have
a large sample size and complex structure. The high-frequency data is the data
at frequencies higher than daily data. A recent research using high-frequency
data at 5 min interval to investigate the impact of macroeconomic announce-
ments on stock market and found that high-frequency data are signiﬁcant for
the identiﬁcation of impact of news on market.
Hussain [5] investigated the response of European and U.S. equity markets
to monetary policy. He found that the monetary policy decision had immediate
inﬂuence on both European and U.S. stock markets. He also suggested that high-
frequency data can separate the eﬀect of monetary policy action from that of
macroeconomic news announcement. Mastronardi et al. [6] estimated the impact
of news announcements on Italian equity market. The results indicated that the
response of the returns to the news is very quick, at least within ten minutes.
Gurgul et al. [7] analyzed the impact of U.S. macroeconomic news announce-
ments on Warsaw Stock Exchange and found that investors reacted to U.S.
macroeconomic news immediately and quickly. Huang [8] studied the response
of ﬁnancial market to macroeconomic news announcements and found the sig-
niﬁcant jumps on news day more than no-news day and market response to news
varied with economic situation.
For Thailand stock market, only Wongswan [9] used minute-by-minute data
and found that the announcements from developed economies exerted inﬂuence
on Thailand’s stock market. Moreover, Wongswan [9] also suggested that the
1 World Bank [1] “Consumer price index reﬂects changes in the cost to the average
consumer of acquiring a basket of goods and services that may be ﬁxed or changed
at speciﬁed intervals, such as yearly.”

Volatility in Thailand Stock Market Using High-Frequency Data
377
important domestic macroeconomic announcements such as GDP and inﬂation
had signiﬁcant impact on SET as well.
The standard GARCH model has been applied and widely used to study the
eﬀect of macroeconomic news on stock market with low-frequency data. This
standard GARCH model is appropriate when good and bad news have asym-
metric shocks on return and volatility. Nikkinen et al. [10] used GARCH model
for analyzing the behavior of volatilities for U.S. macroeconomic news on each
of the 35 stock markets. Vrugt [11] used GARCH models that allow asymme-
tries, multiplicative and pre- and post- announcement eﬀect for investigating
the impact of U.S. and Japanese macroeconomic news announcements on each
of the 4 stock markets.
Mastronardi et al. [6] estimated the impact of macroeconomic news on Italian
equity market and focused on the overall impact of macroeconomic news using
GARCH models to extract the conditional returns and variances. Ibrahim and
Topuz [12] used GARCH model to examine the eﬀect of news announcements on
volatility behavior. Their results showed U.S. and Turkish GDP announcements
had signiﬁcant impact on volatility of Turkish stock market.
Previous studies used GARCH-type models that focused on conditional
return and conditional variance of stock market returns. Hanousek et al. [13]
estimated the impact of macroeconomic news announcements on European stock
market using GARCH-M model which included conditional variance and condi-
tional return. Brenner et al. [14] used the parsimonious multivariate GARCH-
DCC model for study on volatility and co-movement of U.S. ﬁnancial market
upon macroeconomic news release. Caken et al. [15] assumed that a negative
news will generate shock more than positive news. They also found that GJR-
GARCH model was an appropriate estimation for analyzing impact of U.S.
macroeconomic news announcements on stock markets. Wallenius et al. [16]
applied EGARCH model with a Guassian normal distribution of error to study
the eﬀect of EU macroeconomic news on volatility of CIVETS stock markets.
However, GARCH-iump model has been applied in few studies. GARCH-
Jump model is used to capture the volatility when jump occurs. Lu et al. [17]
applied GARCH-jump model and bivariate GARCH model with correlated Pois-
son jump and compared these models with standard two dimensional GARCH
models. Their result suggested that the performance of GARCH-jump model
is better than GARCH model without jump. Rangel [3] presented Poisson-
Gaussian-GARCH process with time-varying jump intensity for large market
movement to examine the eﬀect of macroeconomic announcements on stock mar-
ket. Sidorov et al. [18] suggested GARCH-jump model augmented with news
intensity to improve the power of GARCH-jump model. They compared these
models and found that GARCH-jump model augmented with news intensity
performs slightly better than GARCH model without jumps.
The objective of this research is twofold: First, we aim to investigate the
performance of conventional GARCH and GARCH-jump models when the data
has high frequency. Second, the obtained conditional volatility from the best
ﬁt model is forecasted and used to capture the eﬀect of macroeconomic news

378
S. Duangin et al.
announcements. This paper contributes to the existing literature in the way that
econometric model contributes to the literature on GARCH-jump model with
four distributions. We also contribute to the literature on estimating volatility
using Thailand’s stock market high-frequency data (5 min, 10 min, 1 h, and 2 h
frequencies) from January 04, 2008 to December 29, 2015. Using this method-
ological framework, we analyze the following research question: “How Thailand’s
stock market reacts when macroeconomic news is released?”
The remainder of this study is organized as follows. Section 2 outlines the
theoretical and assumption. Section 3 describes the model. Section 4 describes the
data with summary statistics. Section 5 presents the empirical results. Finally,
Sect. 6 contains conclusion.
2
Theoretical and Hypothesis
2.1
Theoretical
Macroeconomic news announcement inﬂuence stock market returns when news
changes in the information set. The stock price will response to the surprise
news. The stock price link with information set as Eq. (1) which stock price (Pt)
equal the present discounted value of expected earning (d) given information set
at time t(It) with the discount rate (rt).
Pt = E
 ∞

τ=1
dt+τ
1 + rt+τ
 It

(1)
Macroeconomic news announcements inﬂuence stock market returns when there
is a change in the information set. The stock price will respond to the surprising
news. The stock price links with information set as in Eq. (1) where stock price
(Pt) equals the present discounted value of expected earning (d) given informa-
tion set at time t(It) with the discount rate (rt).
The stock price is determined by many factors such as the risk-free rate
of interest, growth expectation, and equity risk premium. Hence macroeconomic
news has inﬂuence on stock price when the news leads to change in one or more of
these factors. For example, an unexpected increase of GDP in the announcement
may lead to the upward movement of stock price by decreasing the discount
factor.
From the literature, Funke and Matsuda [19] suggested many hypotheses to
explain the eﬀect of macroeconomic news on return and volatility of stock mar-
ket. In this study, we observe the following assumptions:
(i) Macroeconomic news has immediate eﬀect on stock market
Many studies showed that stock market reacts within minutes after news release.
Rangel [3] showed the results that the eﬀect of shocks seems to have a short dura-
tion. Mastronardi et al. [6] suggested that the stock return response to the news

Volatility in Thailand Stock Market Using High-Frequency Data
379
is very quick at least within 10 min. Gurgul et al. [7] also suggested that investors
immediately and quickly react to U.S. macroeconomic news.
(ii) Stock market will have diﬀerent responses to diﬀerent types of
macroeconomic news
For example, interest rate that plays an important role in the economy may have
a stronger eﬀect on stock market than other news releases. Rangel [3] showed
that only PPI and inﬂation shock have persistent eﬀect on the market. Sidney
[4] suggested that after ﬁnancial crisis, oil price announcement has eﬀect on Thai
stock market (41%) more than the eﬀect of GDP announcement (21%).
(iii) Macroeconomic news eﬀect on stock market depends on the eco-
nomic condition
Sometime, the stock market is a general measure of the state of economy. During
economic expansion (recession), an economy grows (contracts) and generally
leads to good (bad) stock market performance better than the time with lower
(higher) rate of economic growth. For example, stock market performed badly
during ﬁnancial crisis in 2007–2008. De Goeij [20] suggested that the stocks
respond diﬀerently to the same macroeconomic news in the diﬀerent state of
economy. They also presented the opposite eﬀect of news as the good news
during recession state is a bad news for stock market.
3
Model Description
The GARCH model proposed by Bollerslev [21] is an eﬃcient way to deal with
stock price data which usually observe the volatility clustering. From previous
studies, GARCH model is most widely adopted to investigate the impact of
news on stock price. However, we focus on the eﬀect of macroeconomic news
announcements on conditional means and variance of SET, and the conven-
tional GARCH models might not be appropriate for evaluation in this study.
To overcome this problem, we consider the GARCH-jump model proposed by
Jorion [22] and developed by Maheuand McCurdy [23]. However, it is not clear
whether news adds any value to a GARCH-jump model.
3.1
Univariate GARCH Model
The GARCH model is an important tool for analyzing the ﬁnancial data espe-
cially to forecast volatility. In the application study, GRACH (1,1) model is
suﬃcient to capture the volatility clustering in the data (Bollerslev, Chou and
Kroner, [24]). Thus, the GARCH (1,1) speciﬁcation for the volatility model fol-
lowing Bollerslev [21] can be expressed as

380
S. Duangin et al.
Rt = α0 + εt,
(2)
εt = σtzt
(3)
σ2
t = β0 + β1ε2
t−1 + β2σ2
t−1,
(4)
where Rt is log return of SET index from time t −1 to t, εt is the error term,
zt is standardize residual which is assumed to be normal (N), student-t (sT),
skewed student-t (ssT), and skewed normal (sN) distributions. σ2
t is conditional
variance as presented in Eq. (3). In this model, we need some restriction as the
follows: β0, β1, β3 ≥0 and β2 + β3 ≤1 which make the GARCH process stable.
3.2
GARCH-Jump Intensive Model
To estimate the impact of macroeconomic news on return and volatility of SET,
GARCH-jump intensity model is used following Rangel [3]. We assume that
investors know all information in time t−1, when they make investment decision
at time t. In GARCH-jump model, it is assumed that the news process has two
separate components which cause two types of error. The GARCH (1,1) with
jump can be expressed as
Rt = α0 + (ε1,t + ε2,t),
(5)
σ2
t = β0 + β1 (ε1,t + ε2,t)2 + β2σ2
t−1.
(6)
Suppose the news process have two components are normal and unusual news.
Thus, the error consists of two error components: ε1,t and ε2,t where ε1,t denotes
normal news while ε2,t denotes unusual news announcements. In the ﬁrst com-
ponent errors reﬂects the impact of normal news on volatility such that:
ε1,t = ztσt,
(7)
where zt assumed to be normal distribution, zt ∼N(0, 1), student-t distribution,
zt ∼sT(0, 1, d.f.), skewed normal distribution, ssN(0, 1, ν), skewed student-t
distribution, zt ∼ssT(0, 1, d.f., ν), d.f. is degrees of freedom, and ν is skewness
parameter.
Then, let Jt is cumulative jump size from t−1 to t, Jt =
nt

k=1
Yt,k The second
component ε2,t is deﬁned by
ε2,t = Jt −E(Jt|It−1),
(8)
where Yt,k is size of k-jump that occur from time t −1 to t, 1 ≤k ≤ηt, and
it is assumed to random form Yt,k ∼N(θ, δ2). It−1 denote the past information
set containing the realized values of all relevant variables up to time t −1. ηt
is number of jumps between time t −1 and t, which is random from Poisson
distribution and its conditional density is
P(nt = j|It−1) = exp(−λt)λj
t
j!
, j = 0, 1, · · ·
(9)

Volatility in Thailand Stock Market Using High-Frequency Data
381
where λt be intensity parameter of Poisson distribution and the conditional jump
intensity has a form as an Autoregressive-moving-average (ARMA) process:
λt = a + b(λt−1) + cξt−1,
(10)
where ξt−1 is an intensity residual which is deﬁned as
ξt−1 = E (nt−1|It−1) −λt−1 =
∞

j=0
j · P(nt−1 = j|It−1) −λt−1
(11)
Therefore, following the derivation of Sidorov et al. [17] the error of unexpected
news becomes
ε2.t =
nt

k=1
Yt,k −θλt.
(12)
To estimate the unknown parameters in model, we conduct a maximum likeli-
hood estimator (MLE) to estimate the unknown parameters and obtained the
estimated parameters of our models. In addition, the Akaike information crite-
rion (AIC) is conducted to compare the performance of standard GARCH and
GARCH-jump models.
4
Data and Descriptive Statistics
4.1
The Stock Exchange of Thailand
We are interested in the short-lived eﬀects of macroeconomic news announce-
ments on the Stock Exchange of Thailand (SET). So, we choose high-frequency
data set, which is obtained from Bloomberg database. The sample period is from
the ﬁrst trading day of 2008 to the last trading day of 2015 corresponding to the
trading hours which give 116,806 return observations.
SET trading hours contain two sessions which are (i) Morning session: open at
random between 9.55–10.00 am and close at 12.30 pm and (ii) Afternoon session:
open at random between 2.25–2.30 am and close at random between 4.35–4.40
pm in Bangkok time (GMT+07:00). The data has been removed for the overnight
and lunch break time. Moreover, from assumption (2) that assumes investors to
react diﬀerently in diﬀerent states of economy including ﬁnancial crisis, we then
separate the data set into two groups: crisis period (2008–2011) and non-crisis
period (2012–2015). Table 1 present the list of the group of samples include
5 min, 10 min, 1 h, and 2 h data set. From the results of JB test that presented
in Table 1 we can conclude to reject null hypothesis of normality test for all data
sets. We use the return on SET (Ri,t) on the intervals i on trading day t deﬁne
by price changes of the log of SET index;
Ri,t = ln
 SETi,t
SETi,t−1

(13)

382
S. Duangin et al.
Table 1. Data description
Thailand stock market index
5 min
15 min
30 min
1 h
2 h
Min
−0.08492
−0.085985 −0.08113
−0.08418
−0.08418
Max
0.073738
0.500774
0.078079
0.055464
0.055464
Mean
0.000003
0.000011
0.000017
0.000037
0.000062
SD
0.001415
0.003478
0.003139
0.004341
0.005506
variance
0.000002
0.000012
0.00001
0.000019
0.00003
Skewness −1.112303 80.23905
−0.288609 −0.573509 19.24633
Kurtosis
309.421
11635.34
61.6277
29.7679
13.3329
Obs.
116806
37012
23497
11160
7007
JB test
−2.20E-16 −2.20E-16 −2.20E-16 −2.20E-16 −2.20E-16
4.2
Macroeconomic News
We examine the eﬀect of macroeconomic news announcements on SET by focus-
ing on diﬀerent types of macroeconomic news for Thailand and U.S. The news
related to three diﬀerent areas are (i) news about the real sectors such as
GDP and trade balance, (ii) news about the indicators such as consumer price
index, and (iii) news about interest rate. Macroeconomic news is derived from
Bloomberg database and in the case of the interest rate, it is derived from data
provided by Bank of Thailand (BOT).
5
Empirical Result
As mentioned in the introduction the objective of this paper is to investigate the
eﬀect of macroeconomic news on Thai stock market. The GARCH models are
used to estimate with diﬀerent error distributions, and then the results are com-
pared to choose the appropriate model for forecasting the conditional variance.
There will be three main steps being studied. First the model selection will be
investigated with Akaike Information Criterion (AIC) score. The second step is
to show the performance of GARCH (1,1) and GARCH-jump models. The ﬁnal
step is matching jump with macroeconomic news announcement.
5.1
Model Selection
For GARCH and GARCH-jump models, four diﬀerent types of error term dis-
tribution are considered in this paper. The error distribution is assumed to
be a normal, Student’s t-distribution, skewed normal and skewed Student’s t-
distribution. Therefore, AIC scores as presented in Table 2 are used to compare
the ﬁt of four distributions of GARCH (1,1) and GARCH (1,1) -jump models
for each sample set for model selection. The model with the lowest AIC score
indicates the best ﬁt model with the given data.

Volatility in Thailand Stock Market Using High-Frequency Data
383
According to AIC values in Table 2, sixteen models of GARCH (1,1) and
GARCH-jump models are ﬁt with skewed Student’s t-distribution while seven
models ﬁt with Student’s t-distribution.
5.2
GARCH and GARCH-Jump Model Performance
Based on the results of comparing the ﬁt of error distributions, then we compare
the diﬀerent volatilities of GARCH (1,1) and GARCH-jump models. Table 3
shows the maximum likelihood estimates for conventional GARCH (1,1) and
GARCH-jump model for log return of 5 min, 15 min, 30 min, 1 h, and 2 h interval
of SET index. Moreover, the estimated variances of GARCH (1,1) and GARCH-
jump models based on the ﬁt distributions are shown in Figs. 1, 2, 3, 4 and 5.
The estimates of β1 in GARCH (1,1) model are positive and slightly bigger
than GARCH-jump model. All estimates of β1 are signiﬁcant at 1% level imply-
ing that volatility is governed by the error term of the mean equation in both
GARCH (1,1) and GARCH-jump models. The estimates of β2 in GARCH (1,1)
and GARCH-jump models also positive, indicating that the part of variance in
the previous period is carried over into the period.
For volatility persistence, (β1 +β2) in GARCH model ranging from 0.9966 to
1.378 in average is more than one, suggesting that the results are non-stationary.
This implies that the return in current period has no eﬀect on the conditional
variance in the future in GARCH model. While volatility persistence in GARCH-
jump model ranging from 0.04490 to 1.41 with the average of 0.759821 is closer
to one, thus suggesting that the results are stationary in GARCH-jump model.
This implies that current period’s return has a signiﬁcant eﬀect on the forecasted
variance for many periods in the future. However, the volatility persistence of
both GARCH and GARCH-jump models for 5 min interval calculated at 1.378
and 1.41 respectively is more than one, indicating that the 5 min return in current
period has no eﬀect on the conditional variance in the future.
The results presented in Table 3, also include the maximum likelihood esti-
mate of jump and size of jump for GARCH-jump model. The estimates of para-
meter, θ, are positive implying that jumps are associated with positive movement
in the price. Only the estimate of parameter, θ, of full sample 5 min interval for
GARCH-jump model is negative. That means the jump is related to the negative
moment in price.
The average size of jump, δ, for each sample group is diﬀerent for diﬀerent
sample sets. For full sample of each return interval, jump size ranges from 0.18
to 0.90 with the average of 0.1177 with the full sample of 5 min interval having
the largest jump size. For crisis period sample sets, jump size ranges from 0.091
to 0.12132 or averagely 0.1023. In addition, jump size of non-crisis period sample
sets ranges from 0.101 to 0.143 or on average 0.12248.
To compare the ﬁt of the two models for each sample group, AIC values can
be used. For all sample sets except 5 min interval sample set, the AIC values of
GARCH (1,1) model are lower than GARCH-jump model. Therefore, most of
high-frequency data sample sets of SET are ﬁt with GARCH (1,1) model.

384
S. Duangin et al.
Table 2. Akaike information criterion (AIC) of GARCH (1,1) and GARCH-jump for log return of SET index
Distribution Normal distribution
Student’s t distribution
Skewed normal distribution Skewed student’s t distribution
AIC
Model 1
Model 2
Model 1
Model 2
Model 1
Model 2
Model 1
Model 2
5 min interval
Full sample
−10.42926 −9.510514
−11.40829a −11.05668a −10.91726 −10.16307
−11.40828
−10.98551
Crisis
−8.721228 −9.508688
−9.669448
−10.57239
−10.49894 −9.598413
−10.96492a −10.57329a
Non-crisis
−10.10628 −10.35973
−11.84327
−10.57239
−11.1569
−9.107664
−11.84351a −11.57781a
15 min interval
Full sample
−9.048074 −9.513328
−10.16938
−9.886377
−9.613663 −8.235634
−10.17038a −9.900273a
Crisis
−8.721228 −8.569735a −8.993172
−8.440301
−9.158108 −8.100525
−9.670103a −8.440301
Non-crisis
−10.10628 −9.631970a −10.51820a −9.440204
−10.11536 −9.559096
NaN
−9.439454
30 min interval
Full sample
−9.048385 −8.758237
−9.437911
−9.156204a −9.048562 −8.731171
−9.438803a −9.09839
Crisis
−8.531825 −8.331201a −8.993172
−8.128483
−8.532273 −8.286697
−8.993449a −6.378013
Non-crisis
−9.543667 −9.344096
−9.853482
−9.716459
−9.546483 −8.096991
−9.854886a −9.728172a
1 h interval
Full sample
−8.461647 −8.078114a −8.685571
−7.600734
−8.42547
−6.252816
−8.685884a −7.247264
Crisis
−7.851969 −7.582842a −8.084771a −6.76081
−7.851615 −7.57741
−8.084513
−6.765364
Non-crisis
−8.912634 −8.693096
−9.120711
−6.601504
−8.916469 −8.702813a
−9.121122a −6.714225
2 h interval
Full sample
−7.968553 −7.593852
−8.126073
−7.455873
−7.970744 −7.602265a
−8.126544a −7.007179
Crisis
−7.437588 −7.145501
−7.599856a −7.023626
−7.437127 −7.149645
−7.59956
−7.380108a
Non-crisis
−8.40484
−8.152931
−8.547664
−8.307640a −8.412274 −8.165664
−8.548172a −7.143676
Source: Calculation
Note: a is the smallest AIC value for each sample set.

Volatility in Thailand Stock Market Using High-Frequency Data
385
Table 3. Maximum likelihood estimates of GARCH (1,1) and GARCH-jump for log
return of SET index
α0
β0
β1
β2
ν
d.f.
θ
δ
λ
5 min interval
Full sample
Model 1 1.84E-05
3.05E-07
1.00E+00
3.78E-01
2.53E+00
−1.70E-06 −1.13E-08 −3.66E-02 −1.07E-02
−2.08E-02
Model 2 0.00001
0.0012
1
0.4145
2.46841
−0.00284
0.18611
0.18586
−3.40E-06 −1.78E-06 −2.38E-01 −1.63E-03
−3.61E-03 7.30E-05
−2.03E-02 −1.43E-02
Crisis period
Model 1 1.64E-05
5.13E-07
9.23E-01
2.91E-01
9.71E-01
2.72E+00
−4.92E-06 −2.06E-08 −3.78E-02 −1.24E-02 −6.35E-03 −3.35E-02
Model 2 0.00001
0.00132
0.92238
0.25057
0.98791
2.7156
0.08952
0.09148
0.08834
−8.31E-06 −1.84E-06 −2.67E-01 −0.00206
−0.01629
−0.01394
−0.00105
−0.01529
−0.00118
Non-crisis period
Model 1 2.05E-05
2.88E-07
1.00E+00
3.20E-01
1.02E+00
2.46E+00
−2.95E-06 −1.48E-08 −5.28E-02 −1.74E-02 −5.13E-03 −2.50E-02
Model 2 0.00001
0.00084
1
0.34864
0.97194
2.4345
0.0466
0.14316
0.09504
−4.14E-06 −1.71E-06 −4.48E-01 −7.74E-04 −8.13E-03 −0.0067
−2.30E-04 −0.01543
−1.56E-03
15 min interval
Full sample
Model 1 3.64E-06
4.79E-08
3.96E-02
9.57E-01
9.59E-01
2.62E+00
−7.72E-06 −3.22E-09 −2.37E-03 −1.77E-03 −6.42E-03 −3.72E-02
Model 2 0.00001
0.00008
0.03971
0.95734
0.95936
2.62612
0.09992
0.10007
0.10006
−1.22E-05 −1.81E-06 −0.0162
−4.60E-04 −7.92E-03 −1.53E-02 −0.00018
−0.02396
−1.19E-03
Crisis period
Model 1 −1.50E-05 2.43E-07
5.90E-02
9.25E-01
9.62E-01
2.51E+00
−1.65E-05 −2.70E-08 −6.33E-03 −4.85E-03 −1.05E-02 −5.38E-02
Model 2 0.00001
0.00085
0.10066
0.80009
0.10065
0.09993
0.10066
−4.20E-05 −1.94E-06 −0.04726
−1.55E-03
−0.00076
−0.01751
−1.82E-03
Non-crisis period
Model 1 3.79E-05
2.14E-08
0.02638
0.97242
2.65424
−6.60E-06 −2.57E-09 −0.00235
−0.00175
−0.05433
Model 2 1.06E-05
1.30E-03
1.20E-02
4.60E-01
8.99E-02
1.01E-01
9.26E-02
−2.06E-05 −1.90E-06 −4.62E-01 −3.31E-03
−0.0006
−0.01622
−1.47E-03
30 min interval
Full sample
Model 1 2.61E-05
4.54E-08
3.22E-02
9.69E-01
9.62E-01
2.75E+00
−1.37E-05 −5.11E-09 −2.52E-03 −1.92E-03 −8.17E-03 −5.36E-02
Model 2 4.49E-05
1.04E-04
3.23E-02
9.69E-01
2.76E+00
9.99E-02
1.00E-01
1.00E-01
−2.19E-05 −1.87E-06 −3.36E-02 −8.76E-05
−2.53E-02 −7.53E-04 −2.47E-02 −1.10E-03
Crisis period
Model 1 1.63E-05
1.18E-07
3.44E-02
9.65E-01
9.72E-01
2.64E+00
−2.61E-05 −2.02E-08 −4.29E-03 −3.38E-03 −1.20E-02 −7.15E-02
Model 2 1.31E-05
2.43E-03
1.83E-02
3.62E-01
9.53E-02
1.00E-01
9.72E-02
−4.09E-05 −1.97E-06 −1.98E-01 −5.14E-03
−0.00177
−0.01924
−0.00351
Non-crisis period
Model 1 2.60E-05
4.23E-08
3.12E-02
9.66E-01
9.48E-01
2.88E+00
−1.63E-05 −6.70E-09 −3.38E-03 −3.10E-03 −1.15E-02 −8.27E-02
Model 2 0.00001
0.00001
0.08241
0.99393
0.89861
4.30075
0.0441
0.14712
0.14832
−1.63E-05 −1.94E-06 −1.83E-02 −1.34E-04 −1.56E-02 −2.67E-01 −1.92E-03 −0.04281
−2.42E-02
1 h interval
Full sample
Model 1 8.66E-05
7.91E-08
4.48E-02
9.57E-01
9.71E-01
3.14E+00
−2.78E-05 −1.42E-08 −4.36E-03 −3.45E-03 −1.20E-02 −1.03E-01
Model 2 0.00001
0.00291
0.03221
0.31299
0.0906
0.10065
0.09468
−4.43E-05 −1.97E-06 −1.46E-01 −5.74E-03
−0.00225
−0.02428
−0.00526
(continued)

386
S. Duangin et al.
Table 3. (continued)
α0
β0
β1
β2
ν
d.f.
θ
δ
λ
Crisis period
Model 1 1.42E-04
1.69E-07
3.93E-02
9.59E-01
3.23E+00
−4.91E-05 −4.60E-08 −6.19E-03 −5.43E-03
−1.67E-01
Model 2 3.24E-05
3.26E-03
2.42E-02
3.50E-01
9.60E-02
1.00E-01
9.77E-02
−8.02E-05 −1.99E-06 −2.23E-01 −8.71E-03
−0.0083
−0.04236 −0.01979
Non-crisis period
Model 1 7.74E-05
1.03E-07
4.12E-02
9.55E-01
9.65E-01
3.09E+00
−3.18E-05 −2.24E-08 −5.62E-03 −5.36E-03 −1.59E-02 −1.34E-01
Model 2 0.00001
0.00271
0.03483
0.09899
0.98105
0.06597
0.10178
0.07876
−4.06E-05 −1.98E-06 −1.66E-01 −1.21E-02 −1.72E-02
−0.00528 −0.03613 −8.30E-03
2 h interval
Full sample
Model 1 1.57E-04
1.78E-07
5.47E-02
9.43E-01
9.64E-01
3.77E+00
−4.58E-05 −3.66E-08 −5.91E-03 −5.38E-03 −1.51E-02 −1.80E-01
Model 2 0.00001
0.005
0.04489
0.00001
0.98699
0.0935
0.1016
0.09227
−6.75E-05 −9.87E-05 −1.82E-01 −2.42E-02 −1.99E-02
−0.00555 −0.03468 −3.03E-03
Crisis period
Model 1 2.66E-04
3.26E-07
4.98E-02
9.46E-01
3.87E+00
−7.95E-05 −1.02E-07 −8.22E-03 −7.75E-03
−2.81E-01
Model 2 0.00001
0.0008
0.07221
0.87993
0.95257
3.82591
0.12097
0.121323
0.12108
−2.01E-04 −1.99E-06 −1.30E-01 −1.58E-03 −6.27E-02 −0.18319
−0.00057 −0.08332 −4.60E-03
Non-crisis period
Model 1 1.37E-04
2.30E-07
4.95E-02
9.39E-01
9.58E-01
3.73E+00
−5.30E-05 −5.63E-08 −7.66E-03 −8.55E-03 −2.08E-02 −2.41E-01
Model 2 0.00001
0.00029
0.06957
0.93256
3.66788
0.11757
0.11761
0.11759
−7.53E-05 −1.98E-06 −1.63E-01 −7.08E-04
−0.16301
−0.00173 −0.06376 −0.00056
Source: Calculation.
Notes: Model 1 and Model 2 are GARCH (1,1) and GARCH-jump model respectively. Standard error (SE) are
in parentheses.
Figure 1 top panel is volatility plot of 5 min interval of SET using the GARCH
(1,1) model in Eqs. (2–4). Figure 1 bottom panel plots volatility of 5 min interval
of SET using the GARCH-jump model in Eqs. (5–12). We can see that they have
the same movement but volatility of GARCH-jump is much higher and changes
more dramatically over time than GARCH (1,1) model. Figures 2, 3, 4 and 5
also show the higher volatility in GRACH-jump model, more than GARCH
(1,1) model of 15 min, 30 min, 1 h, and 2 h interval sample sets which can be
considered the same as the 5 min sample set.
5.3
Matching Jumps with Macroeconomic News Announcement
In this section, we study the role of macroeconomic news announcements on SET
volatility during the 5 min time point of all 116,806 trading days. From Fig. 1
bottom panel, we focus some point of jumps to match with Thai and U.S. news
announcements. The highest jump is on October 8, 2008 (11243th observation),
corresponding to Bank of Thailand announcement on interest rate. Moreover,
there appears volatility of SET jump in many points on the day of Band of Thai-
land announcement on interest rate such as on 11 March 2008 (expected = 2.30%,

Volatility in Thailand Stock Market Using High-Frequency Data
387
Volatility
0.00
0
20000
40000
60000
80000
100000
120000
0.02
0.04
0.06
0.08
SET 5 minutes: GARCH
Obs.
Obs.
Volatility
0.05
0
20000
40000
60000
80000
100000
120000
0.06
0.07
0.08
0.09
SET 5 minutes: GARCH-Jump
Fig. 1. Volatility of 5 min interval SET index: 2008:01–2015:12. Note: Vertical scales for
each panel are diﬀerent. Top panel is volatility plots of GARCH (1,1) model. Bottom
panel is volatility plot of GARCH-jump model.

388
S. Duangin et al.
Volatility
0.002
0
10000
20000
30000
SET 15 minutes: GARCH
Obs.
0.004
0.008
0.006
0.010
0.012
0.014
0.0670
0.0665
0.0660
Volatility
0
10000
20000
30000
SET 15 minutes: GARCH-Jump
Obs.
0.0665
0.0650
Fig. 2. Volatility of 15 min interval SET index: 2008:01–2015:12. Note: Vertical scales
for each panel are diﬀerent. Left panel is volatility plots of GARCH (1,1) model. Right
panel is volatility plot of GARCH-jump model.
Volatility
0.005
0.015
0
5000
10000
15000
SET 30 minutes: GARCH
Obs.
0.010
20000
0.0580
Volatility
0
5000
10000
15000
SET 30 minutes: GARCH-Jump
Obs.
20000
0.0575
0.0585
0.0590
0.0595
0.0660
Fig. 3. Volatility of 30 min interval SET index: 2008:01–2015:12. Note: Vertical scales
for each panel are diﬀerent. Left panel is volatility plots of GARCH (1,1) model. Right
panel is volatility plot of GARCH-jump model.
Volatility
0
2000
SET 1 hour: GARCH
Obs.
0.020
0.015
0.010
0.005
4000
6000
8000
10000
0.0650
10000
0.0652
Volatility
0
2000
4000
6000
SET 1 hour: GARCH-Jump
Obs.
0.0654
0.0656
0.0658
8000
Fig. 4. Volatility of 1 h interval SET index: 2008:01–2015:12. Note: Vertical scales for
each panel are diﬀerent. Left panel is volatility plots of GARCH (1,1) model. Right
panel is volatility plot of GARCH-jump model.

Volatility in Thailand Stock Market Using High-Frequency Data
389
0.020
0.015
0.010
0.005
5000
Volatility
0
1000
2000
3000
SET 2 hours: GARCH
Obs.
4000
0.025
6000
7000
5000
Volatility
0
1000
2000
3000
SET 2 hours: GARCH-Jump
Obs.
4000
6000
7000
0.0708
0.0710
0.0712
0.0714
0.0716
Fig. 5. Volatility of 2 h interval SET index: 2008:01–2015:12. Note: Vertical scales for
each panel are diﬀerent. Left panel is volatility plots of GARCH (1,1) model. Right
panel is volatility plot of GARCH-jump model.
actual = 2.40%), 1 May 2009 (expected = 1.70%, actual = 1.80%) and 12 January
2011 (expected = 2.95%, actual = GARCH-jump2.90%).
The volatility of SET jump also took place on the day that Thailand for-
eign reserve announcement was made such as on 10 October 2008 (expected =
$103.5b, actual = $102.1b), 4 April 2010 (expected = $144.8b, actual = $144.2b),
and 12 December 2014 (expected = $157.1b, actual = $158.5b).
For U.S. macroeconomic news, we examine two types of news which are
consumer price index (CPI) and gross domestic product (GDP) announcement.
The CPI announcements match with volatility jump point on 30 September
2008 (expected = 5.60%, actual = 5.40%) and 15 May 2009 (expected = −0.60%,
actual = −0.70%) etc. In addition, GDP announcement matches with jump
point on 30 October 2008 (expected = −0.50%, actual = −0.30%). However, some
points of jump have many news announcements such as on 16 January 2008,
Thailand’s interest rate was announced at 3.25% and U.S.’s CPI was announced
at 4.10%.
6
Conclusions
This study examines the eﬀect of macroeconomic news announcements on Thai
stock market. The key assumption is that the news has a short-lived eﬀect on
return and volatility of SET. We study by using high-frequency data of SET
return including 5 min, 10 min, 1 h, and 2 h data sets from January 04, 2008 to
December 29, 2015.
The results show that the GARCH (1,1) and GARCH-jump model for
most sample sets are ﬁt with skewed Student’s t-distribution and Student’s t-
distribution. Our results also show the estimated coeﬃcients of GARCH (1,1)
are positive and slightly bigger than those of GARCH-jump model. Moreover,
the results indicate that the volatility estimation of two models have the same
pattern but volatility of GARCH-jumps is higher than standard GARCH model.
The ﬁnding also suggested that 5 min interval data set is not suﬃcient to estimate
the volatility in GARCH (1,1) and GARCH-jump process, but these models are
stationary for the lower data sets such as 15 min, 30 min, 1 h, and 2 h interval.

390
S. Duangin et al.
Jump size in non-crisis period is bigger than crisis-period. This result con-
tradicts the general evidence that investors are more uncertain about economy
during recession. Then the stock market should have a higher volatility during
crisis period compared to non-crisis period.
By matching jump point with macroeconomic news announcements, our
empirical results support our assumption that macroeconomic news announce-
ment may lead to volatility change in SET. However, at the time of jump in SET
return, many events also occurred. There emerge some suggestions for future
work. Firstly, we need to test jump occurring to conﬁrm that the surprising
macroeconomic news announcements can cause jumps in the SET by following
the jump detection of Huang [8] and Yao and Tian [25].
Secondly, macroeconomic news announcements can make jump of SET by
increasing or decreasing volatility. Future works may incorporate Markov regime-
switching in GARCH-jump model to estimate size and duration of the eﬀect of
macroeconomic news announcements on SET.
Acknowledgement. We gratefully acknowledge the support and generosity of Chiang
Mai University and Centre of Excellence in Econometrics, Faculty of Economics, Chiang
Mai University, without which the present study could not have been completed.
References
1. World Bank, Consumer Price Index for Thailand [DDOE02THA086NWDB],
retrieved from FRED, Federal Reserve Bank of St. Louis, 6 August 2017. https://
fred.stlouisfed.org/series/DDOE02THA086NWDB
2. Andersen, T.G., Bollerslev, T., Diebold, F.X., Vega, C.: Real-time price discovery
in global stock, bond and foreign exchange markets. J. Int. Econ. 73(2), 251–277
(2007)
3. Rangel, J.G.: Macroeconomic news, announcements, and stock market jump inten-
sity dynamics. J. Bank. Finance 35(5), 1263–1276 (2011)
4. Sidney, A.E.: The impact of information announcements on stock volatility. AU J.
Manage. 10(1) (2012)
5. Hussain, S.M.: Simultaneous monetary policy announcements and international
stock markets response: an intraday analysis. J. Bank. Finance 35(3), 752–764
(2011)
6. Mastronardi, R., Patan´e, M., Tucci, M.P.: Macroeconomic news and Italian equity
market (2013)
7. Gurgul, H., W´ojtowicz, T., Suliga, M.: The reaction of intraday WIG returns to
the US macroeconomic news announcements. Metody Ilo`sciowe w Badaniach Eko-
nomicznych 14(1), 150–159 (2013)
8. Huang, X.: Macroeconomic news announcements, systemic risk, ﬁnancial market
volatility and jumps (2015)
9. Wongswan, J.: Transmission of information across international equity markets.
Rev. Financ. Stud. 19(4), 1157–1189 (2006)
10. Nikkinen, J., Omran, M., Sahlstr¨om, P., ¨Aij¨o, J.: Global stock market reactions
to scheduled US macroeconomic news announcements. Glob. Finance J. 17(1),
92–104 (2006)

Volatility in Thailand Stock Market Using High-Frequency Data
391
11. Vrugt, E.B.: US and Japanese macroeconomic news and stock market volatility in
Asia-Paciﬁc. Pac.-Basin Finance J. 17(5), 611–627 (2009)
12. Gok, I.Y., Topuz, S.: The impact of the domestic and foreign macroeconomic news
announcements on the Turkish stock market. Financ. Stud. 20(3) (2016)
13. Hanousek, J., Koˇcenda, E., Kutan, A.M.: The reaction of asset prices to macroeco-
nomic announcements in new EU markets: evidence from intraday data. J. Financ.
Stab. 5(2), 199–219 (2009)
14. Brenner, M., Pasquariello, P., Subrahmanyam, M.: On the volatility and comove-
ment of US ﬁnancial markets around macroeconomic news announcements. J.
Financ. Quant. Anal. 44(06), 1265–1289 (2009)
15. Cakan, E., Doytch, N., Upadhyaya, K.P.: Does US macroeconomic news make
emerging ﬁnancial markets riskier? Borsa Istanbul Rev. 15(1), 37–43 (2015)
16. Wallenius, L., Fedorova, E., Ahmed, S., Collan, M.: Surprise eﬀect of euro area
macroeconomic announcements on CIVETS stock markets. Prague Econ. Pap.
26(1) (2017)
17. Lu, X., Kawai, K.I., Maekawa, K.: Estimating bivariate GARCH-jump model based
on high frequency data: the case of revaluation of the Chinese Yuan in July 2005.
Asia-Pac. J. Oper. Res. 27(02), 287–300 (2010)
18. Sidorov, S.P., Revutskiy, A., Faizliev, A., Korobov, E., Balash, V.: GARCH model
with jumps: testing the impact of news intensity on stock volatility. In: Proceedings
of the World Congress on Engineering, vol. 1 (2014)
19. Funke, N., Matsuda, A.: Macroeconomic news and stock returns in the United
States and Germany. Ger. Econ. Rev. 7(2), 189–210 (2006)
20. De Goeij, P., Hu, J., Werker, B.: Is macroeconomic announcement news priced?
Bankers Mark. Investors 143, 4–17 (2016)
21. Bollerslev, T.: Generalized autoregressive conditional heteroskedasticity. J. Econ.
31(3), 307–327 (1986)
22. Jorion, P.: On jump processes in the foreign exchange and stock markets. Rev.
Financ. Stud. 1(4), 427–445 (1988)
23. Maheu, J.M., McCurdy, T.H.: News arrival, jump dynamics, and volatility com-
ponents for individual stock returns. J. Finance 59(2), 755–793 (2004)
24. Bollerslev, T., Chou, R.Y., Kroner, K.F.: ARCH modeling in ﬁnance: a review of
the theory and empirical evidence. J. Econ. 52(1–2), 5–59 (1992)
25. Yao, W., Tian, J.: The role of intra-day volatility pattern in jump detection: empir-
ical evidence on how ﬁnancial markets respond to macroeconomic news announce-
ments (2015)

Technology Perception, Personality Traits
and Online Purchase Intention of Taiwanese
Consumers
Massoud Moslehpour1,3(&)
, Ha Le Thi Thanh1,3,
and Pham Van Kien2
1 Department of Business Administration, Asia University, Taichung, Taiwan
mm@asia.edu.tw, writetodrm@gmail.com,
lethithanhha.cs2@ftu.edu.vn
2 Business Administration Department, University of Economics and Finance,
Ho Chi Minh City, Vietnam
kienpv@buh.edu.vn
3 College of Management, Business Administration, Asia University,
Taichung, Taiwan
Abstract. The aim of the study is to examine the inﬂuences of personality
characteristics and perception of technology on e-purchase intention. This study
uses a questionnaire survey in collecting relevant data. The target sample is
Taiwanese consumers. Multi Regression Analysis is used to test the model and
hypotheses. For the measurement model, descriptive analyses and factor anal-
ysis are assessed to verify the validity and reliability of the data. As results, the
impact of perceived ease of use is the strongest inﬂuence on online buying
intention. In addition, perceived usefulness, perceived ease of use, and openness
to experience have signiﬁcant impacts on online purchase intention, thereby
mediating the relationship between consciousness and online purchase intention.
Providing guidelines for strategic plan, technological project, marketing pro-
gram decision, and website design for online suppliers. This study also has
signiﬁcant implications for personalization, e-commerce, and marketing in
online stores. Due to the limited knowledge of the impact of personality traits
and perception of technology on customers online purchase intention, the cur-
rent study appears to be a newly emerging topic in the ﬁeld of marketing
research.
Keywords: Conscientiousness  Openness to experience
Perceived ease of use  Perceived usefulness  Online purchase intention
1
Introduction
Worldwide electronic commerce has signiﬁcantly advanced due to the prominent
development of the internet technology. It allows customers to seek for products’
features, price, and functionality online. Consumers no longer need go to the actual
store to browse, compare prices or shop for goods (Wind and Mahajan 2001).
Accordingly, internet technology has reshaped the way customers buy merchandises.
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_28

Statista (2013) study of 4,887 participants indicates that a large number of online
consumers earn $ 50,000 or more yearly and nearly 80% of them are with college
degree academic level. The top categories of online sales relate to technological and
electronic products, crafts, handmade products, accessories and clothes (Li 2013).
E-commerce in Asia-Paciﬁc region is becoming more and more popular. In 2013
Asia-Paciﬁc reached to an increasing level of 30% which puts them to account for over
one-third of total e-commerce sales all over the world. Regarding the online market in
Taiwan, there were 17,656,414 Internet users in 2012 (International Telecommunica-
tion Union Statistics, 2012). An average Taiwanese spent annually NT$16,586 (US
$568) in 2012 rising from NT$13,864 in 2010 on purchasing through the internet
which accounts over 60% of online people for whole country. Since Taiwan’s online
shopping market is growing rapidly, it important to determine factors which drive
purchase intention (Li 2013).
According to Tsai (2003), personality is one of the roots of understanding con-
sumer purchase intention. Personality is the internal force that motivates customers to
affect a particular behaviour, to unconsciously motivate customers; therefore, marketers
have to understand the effect of personality as a direct link to the consumer’s mind.
Previous studies indicate that shopper’s character is a substantial factor for the success
of e-vendors (Barkhi and Wallace 2007). Online purchase intention has been described
by different views, such as consuming in relative to demographical characteristics
(Brown et al. 2003; Korgaonkar et al. 2004; Park et al. 2004; Stafford et al. 2004),
emotional and psychological characteristics (Huang 2003; Xia 2002), realizing nega-
tive and positive aspects of virtual transactions (Bhatnagar and Ghose 2004; Feather-
man and Pavlou 2003; Garbarino and Strabilevitz 2004; Huang et al. 2004), shopping
motivation (Novak et al. 2000; Wolﬁnbarger and Gilly 2001), and orientation of
purchasing (Schiffman and Kanuk 2000). Nevertheless, a few of the previous research
has investigated the inﬂuence of personality characteristics on customers purchase
intention in online stores (Chen 2011; Zhou and Lu 2011). Furthermore, consuming
behaviour is substantially inﬂuenced by individual factors of users (Mount et al. 2005).
The authors of these previous studies conducted their research under the models of
attitude which seem to be enough sustainable foundation. It is, therefore, undoubted
that purchasing toward e-commerce is necessary to examine personality traits in the
relationship with technology perception, and that makes the current study urgent and
topical.
This paper attempts to provide unique insights into the online consumer’s mindset
by discussing the determinative factors affecting buying intention of participants. This
study takes partially from the technology acceptance model and partially from the big
ﬁve personality traits model. Individuals perceive technology differently. Furthermore,
personality trait also makes a difference in the way consumers shop online. This study
aims at discovering whether: (1) a client’s perception about virtual vendors is adjusted
by e-vendors’ arrangements of their virtual environment based on consumer’s per-
sonality trait, and (2) the adjustments would result in the ways which will cause clients
to buy products easier (Barkhi and Wallace 2007). Taking into consideration the
above-mentioned facts, this study touches on key characteristics to help sellers to
comprehend online purchase intention and how to attract more customers in future.
Technology Perception, Personality Traits and Online Purchase
393

Deﬁnition of Terms and Abbreviations
This section presents the deﬁnition of main concepts used in this study as well as the
abbreviations used throughout this study. The key terms used throughout the research
are deﬁned as follows:
Online purchase intention (hereinafter referred to as INT). Online purchase
intention is a plan to buy a particular food or service at a virtual store in the future in
which buyers need to evaluate criterions toward quality of online shopping website,
search information, and review product and so on (Hausman and Siekpe 2009; Poddar
et al. 2009).
Perceived
usefulness
(hereinafter
referred
to
as PU). Perceived
usefulness
demonstrates the level which a person believes that approaching a speciﬁc technology
would enrich this individual’s job performance (Ahmad and Barkhi 2011).
Perceived ease of use (hereinafter referred to as PEOU). Perceived ease of use is
the ability to recognize and exploit the users’ perception of usefulness of system related
to its current and future usage as rated by users (Davis 1989).
Openness to experience (hereinafter referred to as OPE). “Openness to experience
is described as being intellectually curious, open to new ideas, involves imaginative
and creative cognition styles” (Migliore 2011, p. 39).
Conscientiousness (hereinafter referred to as CON). “Conscientiousness is descri-
bed as the way individuals control, regulate, and direct their impulses, as related to
decision-making and action-oriented behaviours” (Migliore 2011, p. 40).
2
Literature Review
2.1
Review on Inﬂuential Factors
Online purchase intention (INT)
E-purchase intention reﬂects the desire of clients to buy through the internet. It is
believed that a shopper is more likely to buy from virtual stores when e-commerce sites
provide satisfactory tools, including products or services catalogues, searching func-
tions, pricing comparison sheets, buying carts, online payment systems, and outlining
devices (Chen et al. 2010). Thus, considering the importance corresponding to each
factor plays the important role for online vendors to draw and maintain consumers. The
theory of reasoned action model was developed based on the foundation of Information
Integration model. The direct factor of behaviour refers to result of a consumer’s
intention to decide a particular action. According to determinants of behavioural
intention, there are two main features toward the aspects of personality which are
shopper’s attitude and the subjective criteria (Ajzen 1991). Thus, online shopping
intention is most likely to consider as a long-term tendency of buyer to represent
behaviour of buying in context of virtual stores (Chen 2011).
394
M. Moslehpour et al.

Perceived usefulness (PU)
“Usefulness of purchasing from a virtual store can be deﬁned in terms of perceived
beneﬁts and the overall perceived advantages of shopping online. In addition to lower
prices, the lower costs of information searching may contribute to the perceptions of the
usefulness of making purchases from a virtual store when compared to making a
purchase from a traditional store or even when compared to another virtual store”
(Barkhi and Wallace 2007). As argued by Kim et al. (2003), the e-commerce websites
supply functions and aid shoppers to perform exactly decisions of buying a
product/service. Shambara (2013) observed that virtual stores can create helpful ser-
vices for consumer’s purchase. It is evident that the services are not as convenient as in
traditional market (e.g. immediate comparison among a variety of products or services).
Certainly, it will be counted as the convenience of shopping, and as a result in leading
to the growth of advantageous attitudes toward web-based purchase.
Perceived ease of use (PEOU)
PEOU is easily to learn and become professional at accessing websites and internet
functions (e.g. technology and web-interface) on e-commerce sites. It is about per-
ceived necessary technological elements (Buton-Jones and Hubona 2005). More
speciﬁcally, a technology is more favorable for using than another if it is most likely to
be approved by online shoppers. Another words, the more complicated a technological
application is perceived to be, the ratio of the website’s users (Selamat et al. 2009).
Personality Traits
The researches of individual characteristic have used an essential analyzing tool for
considering human behaviour. As the most common model of personality traits, the Big
Five model evaluate the most noticeable sides of personality (Huang and Yang 2010).
Human’s traits are basic constructing slabs of personality, which is more likely to lead
constant shapes of person’s thinking, feeling, and manner (Pervin 1996). In addition,
the prototype of Big Five can conﬁrm impacts of time’s ﬂow (Hampson and Goldberg
2006) as well as cultural exchanges (Costa and McCrae 1995). This model includes
elements of extraversion, conscientiousness, agreeableness, neuroticism (emotional
stability) and openness to experience. In this paper, authors will limit the variables of
personalities by using only conscientiousness as independent variable and openness to
experiences as mediating variables. Both variables are related to the area of personal
behaviour.
Conscientiousness (CON)
People with highly conscientious characteristic are often concentrated, careful, trust-
worthy, and well organized, whereas unconscientious persons are the most likely to
express their distraction, disorganization and having ﬂexibility (Migliore 2011). CON
is identiﬁed by words of “precise,” “efﬁcient,” “orderly,” and “persistent”. It is believed
that CON persons normally concern about factor of effectiveness.
Openness to experience (OPE)
According to the study of Migliore (2011), open-minded persons to experience
something new are individuals of intelligence, curiosity, free thought and ﬂexible
action which the measurement is based signiﬁcantly on personality test (i.e. tendency to
fantasize, emotional awareness, intellectual curiosity…) Conversely, others are
Technology Perception, Personality Traits and Online Purchase
395

referring to narrow and conservative thought. Therefore, open-mind is a noteworthy
forecaster of the overall virtual use. Also, open-mind persons are more likely to
experience an online shopping via the internet (McElroy et al. 2007). The result means
that open-minded individual is more likely to approach e-purchase to conﬁrm their
inquisitiveness and ﬁnd out freshly adventured practices (Tuten and Bosnjak 2001).
2.2
Interrelations Between Variables
The study of Devaraj et al. (2008) examined the usefulness of conscientiousness
relating to two technological factors including perceived ease of use and intention to
experience. It indicated that their relationship is more signiﬁcant for persons referring
to much more conscientiousness (b = 0.16). Punnoose (2012) (b = 0.098) and Sullivan
(2012) (b = 0.24) also found that conscientiousness has an important inﬂuence on
perceived usefulness.
As researched by Lim and Ting (2012), (b = 0.410) perceived usefulness is related
to purchase attitude among online shoppers. If consumers ﬁnd out the usefulness of an
e-commerce website for shopping, they will have the better attitude which advanta-
geously leads to intended purchase of a product or service. Other studies by Yoon and
Barker Steege (2013) (b = 0.346), Punnoose (2012) (b = 0.394), (Aldás-Manzano
et al. 2009) (b = 0.164), and Devaraj (2008) (b = 0.19) also conﬁrm the relationship
between PU and INT.
Several researchers have found a positive relation between the perceived ease of
use and purchase intention. It is discussed that clear and understandable online shop-
ping sites, which require for their users less mental efforts to make a purchase, are more
attractive for potential customers, than more complicated ones (Childers et al. 2001).
Lim and Ting (2012) considered that clients with “perceived ease of use” tend to have a
higher intention of buying things from a virtual stores (b = 0.395). Thus, the ease of
use relating to e-commerce website’s functions and interfaces are urgent need of
forecasting the user’s intention towards e-purchase. Perceived ease of use is highly
relevant to clients’ manner in experience online shopping accordingly to applications of
internet, which is strongly associated with intention to purchase (b = 0.33) (Yulihasri
and Daud 2011).
Bosnjak et al. (2007) based on Mowen’s hierarchical model of personality explain
and predict people’s disposition to make e-purchases (Mowen 2000). Almost all ele-
ments have positive relationship, including the relationship between conscientiousness
and openness to experiences. This paper uses a hierarchy model with the following
hierarchy: surface, situational, compound and elemental. CON and OPE are among the
hierarchical elemental characteristic included in the Big Five dimensions of character
(Mowen 2000). Figure 1 shows the meaningful interrelations among these three
factors.
396
M. Moslehpour et al.

2.3
Theoretical Framework and Hypotheses
The Framework guides the growth of the study hypotheses, which examines the
interrelations among examination variables. The framework shows that personality
traits can affect online purchase intention. The model, depicted on Fig. 2, suggests that
e-purchase intention is a dependent variable (DV) which are inﬂuenced by the con-
scientiousness as independent variable (IV). Moreover, it is assumed that the effects of
conscientiousness is fully mediated by perceived usefulness, perceived ease of use, and
openness which in turn, inﬂuence on the dependent variable. The study hypotheses are:
H1: CON has positive association with PU
H2: CON has positive association with PEOU
H3: CON has positive association with OPE
H4: PU has positive association with INT
H5: PEOU has positive association with INT
H6: OPE has positive association with INT
H7: CON has positive association with INT
H8: PU, PEOU and OPE mediates the relationship between CON and INT.
Conscientiousness
Online Purchase
Openness (OPE)
(CON)
Intention (INT)
Fig. 1. The positive relationship between variables
Conscientiousness 
(CON)
Perceived 
Usefulness (PU)
Perceived Ease of 
Use (PEOU)
Openness to 
Experience (OPE)
Online Purchase 
Intention (INT)
H1
H2
H3
H4
H5
H6
H7
Fig. 2. Research model
Technology Perception, Personality Traits and Online Purchase
397

3
Research Methodology
In the current study, we collected the data based on information gathered from previous
literature review and then speciﬁc literature research area in Taiwan to evaluate the
present situation about online shopping purchase of Taiwanese. Survey research
instrument is used in order to study inﬂuential personality and technology on online
purchase intention. IN this section we describe data analysis process by using quan-
titative methods. The results of statistical analysis will help to identify the results from
the questionnaire and respondents to make the conclusion to obtain a general overview
of the entire research.
3.1
Sample Selection
The population of this research is Taiwanese consumers. The use of technological
applications to purchase products or services on virtual stores is a common way for
Taiwanese. Consumers who are more comfortable to access technological applications
can possess certain characteristics that will lead to their online shopping decision.
Therefore, this situation can create a large number of online consumers in general and a
potential e-commerce market in separate (Hayhoe et al. 2000). A total of 316 usable
(out of 380) questionnaires were collected from Taiwanese consumers aged from 16 to
45 year-old.
The survey questionnaire of this paper is built fundamentally by the features which
are chosen based on considerations for the research framework, deﬁnition of the
variables and literature reviews. We use Likert scale for all of the research questions
including ﬁve scores ranking from 1 to 5. In which, score 1 and score 5 are corre-
sponding to “strongly disagree” and “strongly agree” perspectival.
The data collection committed through online questionnaire to let the respondents
answer questions by using google drive and the paper-and-pencil survey. The time of
collecting data is over a month period (1st of March, 2016 * 1st April, 2016).
3.2
Data Analysis Procedure
At the end of the survey participation period, the results were exported to an excel ﬁle.
The quantitative data was coded, tabulated and imported into the SPSS software.
The SPSS software is used for this research to study factor analysis and descriptive data
analysis to test the collected ﬁgures validly and reliably. Descriptive statistics shows
the value of the personality measure for every personality dimensions. This study
employs the multiple regression analysis to examine the hypotheses. The results are
summarized by using descriptive statistics. The used tools are Cronbach’s alpha,
Kaiser-Meyer-Olkin, Bartlett’s Test of Sphericity. To measure the factor loading the
EFA is employed. Multi regression analysis is used to test the model and hypotheses.
398
M. Moslehpour et al.

4
Result and Findings
4.1
Demographic Characteristic
In this study 316 survey questionnaires are gathered from Taiwanese consumers. The
demographic information including respondent’s gender, age, education, income level
and occupation are shown in Table 1 below.
4.2
Reliability Test
Reliable test, which should be carried out before an examination of validity, is regarded
to reduce risks of random error and bring suitable outcomes not only over time but also
across circumstances (Chen et al. 2015). The reliability coefﬁcient of Cronbach’s alpha
conducted is obtained to evaluate internal consistency among the survey instruments
(see Table 2). If the value of coefﬁcient alpha is between 0.6 and 0.8 the instrument is
considered reliable, and a value over 0.8 is considered highly reliable. The results of
the reliability test indicate that CON and INT are reliable, and OPE, PU, PEOU are
highly reliable as presented in Table 3. The whole instrument is highly reliable.
Table 1. Demographic analysis
Characteristics
Frequency Proportion
Gender
Male
128
40.5%
Female
188
59.5%
Age
16–25
171
54.1%
26–35
129
40.8%
35–45
16
5.0%
Education
High school or Below 29
9.8%
Bachelor Degree
14
4.4%
Master Degree
223
70.6%
Doctor of Philosophy
50
15.8%
Income
5,000 NT or below
89
28.2%
5,001–10,000 NT
30
9.5%
10,001–20,000 NT
32
10.2%
20,001–40,000 NT
132
41.8%
40,001–60,000 NT
28
8.9%
60,001 NT or above
5
1.6%
Occupation Student
106
33.5%
Employee
139
43.9%
Self-employed
25
7.9%
Unemployed
2
0.6%
Other
44
13.9%
Total
316
100%
Technology Perception, Personality Traits and Online Purchase
399

4.3
Exploratory Factor Analysis (EFA)
“Exploratory factor analysis is an important tool for researchers. It can be useful for
reﬁning measures, evaluating construct validity, and in some cases testing hypotheses”
(Chen et al. 2015). This study conducts exploratory factor analysis, KMO and Bartlett’s
Test of Sphericity to test for the reliability of the instrument. The assumptions are that:
(1) if the result of KMO is over 0.6 then the instrument passes the Bartlett’s Test of
Sphericity; (2) values of the anti-image correlation matrix and commonality are larger
than 0.5; (3) Eigenvalues is over 1; (4) factor loadings should be 0.5 or higher and
ideally 0.7 or higher (Coakes et al. 2009) then the instrument is valid. All factor
loadings are higher than 0.5 which means this research is highly reliable. For the KMO
measure of sampling adequacy, in this analysis the KMO is 0.835 which is higher than
0.6. The chi-square was v2 = 3133.25, it was signiﬁcant q < 001. All factor loading
are above 0.5 (Table 4).
4.4
Multi Regression Analysis
Multiple regression is a useful method that ﬁnds the independent factors’ inﬂuence into
dependent variable. Multiple regressions are largely conducted to discover the inﬂu-
ences among variables relating to marketing investigation. The study of hypothesis is
on the basic of the standardized path coefﬁcient. Following to test of the hypothesis, the
p-value of the r-path coefﬁcient must be substantial at .05. This study intends to
identify factors inﬂuencing conscientiousness towards purchase intention in online
market. This research examines the variables effecting on online purchase intention.
These factors, based on the research model, are Conscientiousness (CON), Perceived
Usefulness (PU), Perceived Ease of Use (PEOU), and Openness to Experience (OPE).
Table 5 indicates the results of multi regression analyses.
A multiple linear regression was calculated to predict online purchase intention
based on conscientiousness, perceived usefulness, perceived ease of use, and openness
to experience. A signiﬁcant regression was found (F (4, 311) = 20.563, p < .000), with
Table 2. Reliability statistics (N = 316)
Cronbach’s alpha Number of items
.855
21
Table 3. Cronbach’s alpha
Cronbach’s Alpha Number of items
CON
.792
5
OPE
.871
7
PU
.823
3
PEOU .882
4
INT
.609
2
400
M. Moslehpour et al.

Table 4. EFA table
Variables and items
Component
loading
I see myself as someone who…
CON1 (does a thorough job)
.806
CON2 (is a reliable worker)
.599
CON3 (perseveres until the task is ﬁnished)
.627
CON4 (does things efﬁciently)
.796
CON5 (makes plans and follows through with them)
.757
OPE1 (is original & comes up with new ideas)
.842
OPE2 (is curious about many different things)
.622
OPE3 (has an active imagination)
.839
OPE4 (is inventive)
.879
OPE5 (values artistic & aesthetic experiences)
.782
OPE6 (likes to reﬂect & plays with ideas)
.623
OPE7 (is sophisticated in art, music, or literature)
.595
Online shopping…
PU1 (helps me to make purchases faster)
.844
PU2 (helps me to make cheaper purchases)
.801
PU3 (makes it easier for me to make purchases)
.869
PEOU1 (instructions are easy to follow)
.877
PEOU2 (is easy to learn how to use)
.893
PEOU3 (websites are easy to operate)
.866
PEOU4 (makes it easy to ﬁnd what I want)
.645
INT1 (I am eager to learn about purchases online)
.790
INT2 (I intend to compare prices in an online store and in a traditional
one)
.764
KMO
.835
Cumulative %
65.187
P-value
.000
Table 5. Multi regression analysis CON, PU, PEOU, OPE and INT
R Square
Unstandardized
Standardized T
Sig.
.209
Coefﬁcients
Coefﬁcients
B
Std. Error Beta
(Constant) 1.305 .310
4.215 .000
CON
.055
.071
.043
.770
.442
OPE
.148
.059
.134*
2.518 .012
PU
.106
.050
.112*
2.103 .036
PEOU
.360
.055
.355***
6.527 .000
Dependent Variable: INT, *p < .05, **p < .01, ***p < .001
Technology Perception, Personality Traits and Online Purchase
401

an R2 of 0.209. Participants’ predicted online purchase intention’s equation is followed
as: INT = 1.305 + 0.106 (PU) + 0.148 (OPE) + 0.360 (PEOU) (Fig. 3).
5
Discussion and Conclusion
5.1
Discussion
For the overall study and from previous research, there are some similar topics that
other researcher studied about personality traits and technology perception as they
relate to online purchase intention. By combining personality traits and technology
perception, this study offers eight hypotheses. These hypotheses are created and tested
by using simple regression and multi regression analysis. The purpose of this research
is to test the relationship between the independent variables (CON, PU, PEOU and
OPE) and dependent variable (INT). Furthermore, the mediation by PU, PEOU, and
OPE between CON and INT is tested.
CON has a signiﬁcant inﬂuence on PU is supported (b = 0.25***). Devaraj et al.
(2008), Punnoose (2012) and Sullivan (2012) observed support for this signiﬁcant
connection between CON and PU. Intentions to experience the technology is stronger
for individuals with higher CON. This research supports previous research ﬁndings
regarding traditional shopping for online shopping. Costa and McCare (1995) char-
acterized CON personality trait as people who are naturally motived and those who
strive to aim at achievements at a high level and perform positive actions. The signs of
conscientious person are self-control reﬂected in a need for performance, order, and
persistence. Therefore, because CON represents an intrinsically driving force for
improvement, conscientious people are cautious when reviewing whether technological
applications will allow them to be more effective. On the other hand, if a CON person
concludes that a technology is not beneﬁcial, then he/she will not use it (Devaraj et al.
2008).
CON has a signiﬁcant inﬂuence on PEOU (b = 0.32***). CON is one of the human
characters of thinking inside out, persistence and cautioning. Is the characteristic of a
person expecting that actions will end with highest possible results? Person of CON is
.25***
.32***
.11*
.31***
.36***
.13*
.04 with mediation
.25*** w/o mediation
Perceived 
Usefulness (PU)
Conscientiousness 
(CON)
Perceived Ease of 
Use (PEOU)
Online Purchase 
Intention (INT)
Openness to 
Experience (OPE)
Fig. 3. Result of multiple regression analysis on proposed framework
402
M. Moslehpour et al.

often efﬁcient and ordered as opposed to free and easy. It mean that this person who is
generally organized and dependable will perform actions based on a clear and par-
ticular schedule instead of immediately thought. PEOU refers to technologies and
interfaces on e-commerce websites which is more favorable to access than another is
more likely to be accepted by the participants. Because they are very precise, perfec-
tionist, and demand effectiveness, they prefer something that is very efﬁcient to use.
When they look for a website, they will eagerly compare for some features that make
the website easier, and thus more efﬁcient, to use. They are very aware of the use-
fulness of the feature in the website.
CON signiﬁcantly inﬂuences OPE (b = 0.31***). Conscientious personalities
usually are very concerned about effectiveness. OPE is implied as being intellectually
curious, open to new ideas, involves imaginative and creative cognition styles
(Migliore 2011). CON personality demands something that is efﬁcient to do. Because
of that reason, CON personality has to be more open with new idea which is developed
in the environment which can make their life more efﬁcient.
PU signiﬁcantly inﬂuences INT is supported (b = 0.11*). Previous studies indi-
cated that shoppers’ PU of an online store positively inﬂuence their buying intentions
and repurchase intention in the future. Online purchase intention is determined by
perceived usefulness of technological innovation (Davis 1989; Tong 2010). The results
of this study imply that Taiwanese customers that perceive technology as a useful tool
for shopping online tend to buy more online. The result is consistent compared with
studies of Aldás-Manzano et al. (2009) and Yoon and Steege (2013). They found that
online shoppers in different countries have the alike expectation of looking for
advantages of purchase via the internet by themselves. As a result of this, PU is the key
driver of usage behaviour and intention to purchase.
PEOU has a signiﬁcant inﬂuence on INT (b = 0.36***). This result agrees with
similar previous research by Yulihasri and Daud (2011). Therefore, technologies which
are perceived as easy to use will stimulate customers to purchase online. It was shown
that clear and understandable online shopping sites, which require of their users less
mental efforts to make a purchase, are more attractive for potential customers, than
more complicated ones (Childers et al. 2001). This results are very important, since
they revealed that convenient website, which is free from effort in order to make an
order is one of the main factors of successful virtual store.
OPE is a sustainably determining factor in relating to INT, with (b = 0.13*). People
with high OPE person, who is the most likely to explore something new, like to
experience e-activities. As a given by Tuten and Bosnjak (2001), it is reasonable to
satisfy this person’s interest and ﬁnd out new ways adventurously. This is also con-
sidered that OPE is the important forecasting element effecting on growth of using the
internet’s applications generally (McElroy et al. 2007). That means that people who
exhibit this personality trait (openness) would better do purchase online than go to
traditional store. The results are in accordance with Arnold and Reynolds (2003) who
stated that a person with higher level of OPE is more likely to buy things in virtual
stores.
CON signiﬁcantly inﬂuence INT is supported (b = 0.25***). The awareness of
high conscientious people is very high. They prefer to make themselves do something
that is more efﬁcient and well organized. They prefer to do something orderly and not
Technology Perception, Personality Traits and Online Purchase
403

something spontaneous. There is a possibility that conscientious people prefer to shop
online rather than traditional shopping because shopping online is more organized,
more efﬁcient, and more speciﬁc to buy something, Moreover, going to the traditional
stores, staying trafﬁc, wandering around, wasting time with the possibility of buying
nothing is not very attractive to conscientious people. Online store provides more detail
information about prices, products, and even how to buy something until it arrives to
the destination place. Thus, it will help users to compare the price or even the product
from one website to another and help them to buy something that match with their
need. Hence, it also will be more comfortable and more effective for those who seek for
efﬁciency and precisely.
PU, PEOU, OPE mediate the relationship between CON and INT, is supported (b
w/mediation = 0.04) and (b w/o mediation = 0.25***). Experiential shoppers and
goal-oriented shoppers may be considered as two kinds of clients (Wolﬁnbarger and
Gilly 2001; Barkhi and Wallace 2007). Conscientious people tend to be target-oriented
clients, which have oriented tasks, efﬁciency, lucidity, and deliberateness and might
enjoy the efﬁciency of virtual stores (Barkhi and Wallace 2007). CON inﬂuences INT.
However, when mediated by three other variables (PU, PEOU and OPE) this inﬂuence
is insigniﬁcant. CON’s inﬂuence is diminished by the presence of mediating variables.
In summary, the study aims to explore whether CON have correlation with INT
among Taiwanese people, examine the role of PU, PEOU, OPE as mediators in rela-
tionship between CON and INT. In particular, eight hypotheses were postulated. There
are eight hypothesis and all hypothesis was accepted.
5.2
Implementations
This research shows evident that personal traits are an important determinant affecting
e-shoppers’ buying intention. As a result, it is a noteworthy background toward
studying the ﬁeld of online consuming behaviour. Actually, online vendors can use the
result as fundamental reference material to plan an effective consumer-oriented strategy
and attract many more consumers to buy things online.
The results also suggest that although consciousness people are attracted to shop-
ping only, however, they are mainly inﬂuenced by the presence of other factors such as
OPE, PEOU and PU. Their intention to purchase online will increase through PU,
PEOU and their OPE of new technology. Online store owners and web designers can
attract consciousness people by increasing PEOU and PU while considering the per-
sonality traits of OPE. Therefore, this research is more necessary to web designers,
vendors supplying technological application and software and vendors selling
products/services online particularly. For instant, the web-designer is needed to create
eye-catching sales interface of websites, besides the technological applications of smart
tools and functionality by a software company to help users easily manipulate even
unprecedented access to technology previously. And duties of online providers is to
ﬁnd ways to encourage customer’s interest in the products and services in their virtual
stores by the ways of promotion, quality, new features and pricing competition.
In summary, this study indicated the signiﬁcant roles of CON, PU, PEOU and OPE
in predicting INT among Taiwanese consumers. Furthermore, the results showed that
the role of PU, PEOU and OPE as mediators in the relationship between CON and INT
404
M. Moslehpour et al.

is signiﬁcant. In particular, eight hypotheses were postulated. There were eight
hypotheses and all hypothesis were accepted.
References
Ajzen, I.: The theory of planned behavior. Organ. Behav. Hum. Decis. Process 50(3), 179–211
(1991)
Ahmad, N., Barkhi, R.: The contextual and collaborative dimensions of avatar in real-time
decision making. In: Supporting Real Time Decision-Making, pp. 133–156. Springer (2011)
Aldás‐Manzano, J., Ruiz‐Mafé, C., Sanz-Blas, S.: Exploring individual personality factors as
drivers of M-shopping acceptance. Ind. Manage. Data Syst. 109(6), 739–757 (2009). https://
doi.org/10.1108/02635570910968018
Arnold, M.J., Reynolds, K.E.: Hedonic shopping motivation. J. Retail. 79(2), 77–95 (2003)
Barkhi, R., Wallace, L.: The impact of personality type on purchasing decisions in virtual stores.
Inf. Technol. Manage. 8(4), 313–330 (2007)
Bakos, Y.: A strategic analysis of electronic marketplaces. MIS Q. 15(3), 295–310 (1991)
Bhatnagar, A., Ghose, S.: A latent class segmentation analysis of E-shoppers. J. Bus. Res. 57(5),
758–767 (2004)
Bosnjak, M., Galesic, M., Tuten, T.: Personality determinants of online shopping: Explaining
online purchase intentions using a hierarchical approach. J. Bus. Res. 60(6), 597–605 (2007)
Brown, M., Pope, N., Voges, K.: Buying or browsing? An exploration of shopping orientations
and online purchase intention. Eur. J. Mark. 37(11), 1666–1684 (2003)
Buton-Jones, A., Hubona, G.S.: Individual differences and usage behaviour: revisiting a
technology acceptance model assumption. Date Base Adv. Inf. Syst. 36(2), 58–77 (2005)
Chen, J.K., Batchuluun, A., Batnasan, J.: Services innovation impact to customer satisfaction and
customer value enhancement in airport. Technol. Soc. 43, 219–230 (2015)
Chen, Y.H., Hsu, I.C., Lin, C.C.: Website attributes that increase consumer purchase intention: A
conjoint analysis. J. Bus. Res. 63(9), 1007–1014 (2010)
Chen, T.: Personality traits hierarchy of online shoppers. Int. J. Mark. Stud. 3(4), 23–39 (2011)
Childers, T.L., Carr, C.L., Peck, J., Carson, S.: Hedonic and utilitarian motivations for online
retail shopping behavior. J. Retail. 77(4), 511–535 (2001)
Coakes, S.J., Steed, L., Ong, C.: SPSS 16.0 for Windows: Analysis Without Anguish. Wiley,
Australia (2009)
Davis, F.D.: Perceived usefulness, perceived ease of use, and user acceptance of information
technology. MIS Q. 13(3), 318–340 (1989). https://doi.org/10.2307/249008
Devaraj, S., Easley, R.F., Crant, J.M.: How does personality matter? Relating the ﬁve-factor
model to technology acceptance and use. Inf. Syst. Res. 19(1), 93–105 (2008)
Featherman, M., Pavlou, P.A.: Predicting E-services adoption: a perceived risk facets
perspective. Int. J. Hum Comput. Stud. 59(2), 451–474 (2003)
Garbarino, E., Strabilevitz, M.: Gender differences in the perceived risk of buying online and the
effects of receiving a site recommendation. J. Bus. Res. 57(5), 768–775 (2004)
Hampson, S.E., Goldberg, L.R.: A ﬁrst large-cohort study of personality-trait stability over the 40
years between elementary school and midlife. J. Pers. Soc. Psychol. 91(3), 763–779 (2006)
Hausman, A.V., Siekpe, J.S.: The effect of web interface features on consumer online purchase
intentions. J. Bus. Res. 62(1), 5–13 (2009)
Hayhoe, C., Leach, L., Turner, P., Bruin, M., Lawrence, F.: Differences in spending habits and
credit use of college students. J. Consum. Aff. 34(2), 113–133 (2000)
Technology Perception, Personality Traits and Online Purchase
405

Huang, J.H., Yang, Y.C.: The relationship between personality traits and online shopping
motivations. Social Behav. Pers. Int. J. 38(5), 673–679 (2010)
Huang, M.H.: Modeling virtual exploratory and shopping dynamics: an environmental
psychology approach. Inf. Manag. 41(1), 39–47 (2003)
Huang, W.Y., Schrank, H., Dubinsky, A.J.: Effect of brand name on consumers’ risk perceptions
of online shopping. J. Consum. Behav. 4(1), 40–50 (2004)
Kim, D., Ferrin, D., Rao, H.: Study of consumer trust on consumer expectations and satisfaction:
The Korean experience. In: ACM International Conference Proceeding, vol. 50(1), pp. 310–
315 (2003)
Kim, J.B.: An empirical study on consumer ﬁrst purchase intention in online shopping:
integrating initial trust and TAM. Bus. J. 12(5), 125–150 (2012). https://doi.org/10.1007/
s10660-012-9089-5
Korgaonkar, P., Silverblatt, R., Becerra, E.: Hispanics and patronage preferences for shopping
from the internet. J. Comput. Mediated Commun. 9(3), 31–44 (2004)
Li, J.: Study: online shopping behavior in the digital era. Mark. Res. 3(5), 13–21 (2013)
Lim, W.M., Ting, D.H.: E-shopping: an analysis of the technology acceptance model. Modern
Appl. Sci. 6(4), 49–62 (2012). https://doi.org/10.5539/mas.v6n4p49
McElroy, J.C., Hendrickson, A.R., Townsend, A.M., DeMarie, S.M.: Dispositional factors in
internet use: personality versus cognitive style. MIS Q. 31(2), 809–820 (2007)
Migliore, L.A.: Relation between big ﬁve personality traits and Hofstede’s cultural dimensions:
Samples from the USA and India. Cross Cult. Manage. Int. J. 18(1), 38–54 (2011)
Mount, M.K., Barrick, M.R., Scullen, S.M., Rounds, J.: Higher-order dimensions of the big ﬁve
personality traits and the big six vocational interest types. Pers. Psychol. 58(2), 447–478
(2005). https://doi.org/10.1111/j.1744-6570.2005.00468.x
Mowen, J.: The 3M Model of Motivation and Personality. Kluwer Academic Press, Norwell
(2000)
Novak, T.P., Hoffman, D.L., Yung, Y.F.: Measuring the customer experience in online
environments: a structural modeling approach. Mark. Sci. 19(1), 22–42 (2000)
Park, J., Lee, D., Ahn, J.: Risk-focused e-commerce adoption model: a cross-country study.
J. Global Inf. Manage. 5(7), 6–30 (2004)
Pervin, L.A.: The Science of Personality. Wiley, New York (1996)
Poddar, A., Donthu, N., Wei, Y.: Web site customer orientation, web site quality, and purchase
intentions: the role of web site personality. J. Bus. Res. 62(3), 441–450 (2009)
Punnoose, A.C.: Determinants of intention to use e-learning based on the technology acceptance
model. J. Inf. Technol. Educ. Res. 11(3), 301–337 (2012)
Schiffman, L.G., Kanuk, L.L.: Consumer Behavior. Prentice-Hall, Upper Saddle River (2000)
Selamat, Z., Jaffar, N., Ong, B.H.: Technology acceptance in Malaysian banking industry. Eur.
J. Econ. Finance Adm. Sci. 1(17), 143–155 (2009)
Sosor: Factors inﬂuencing the Usage of Mobile Banking in a Mongolian Context, pp. 1–90. Asia
University (2012)
Stafford, T.F., Turan, A., Raisinghani, M.S.: International and cross-cultural inﬂuences on online
shopping behavior. J. Glob. Inf. Manage. 7(2), 70–87 (2004)
Statista: Statistics and facts about global e-commerce (2013). http://www.statista.com/topics/871/
online-shopping/
Tong, X.: A cross-national investigation of an extended technology acceptance model in the
online shopping context. Int. J. Retail Distrib. Manage. 38(10), 742–759 (2010)
Tsai, L.H.: Relationships between Personality Attributes and Internet Marketing (Doctoral
dissertation). Retrieved from ProQuest Dissertation and Theses database. (UMI No. 3078519)
(2003)
406
M. Moslehpour et al.

Tuten, T.L., Bosnjak, M.: Understanding differences in web usage: The role of need for cognition
and the ﬁve factor model of personality. Soc. Behav. Pers. 29(3), 391–398 (2001)
Venkatesh, V., Agarwal, R.: Turning visitors into customers: a usability-centric perspective on
purchase behavior in electronic channels. Manage. Sci. 52(3), 367–382 (2006)
Wind, J., Mahajan, V.: Digital Marketing: The Challenge of Digital Marketing. Wiley, New York
(2001)
Wolﬁnbarger, M., Gilly, M.: Shopping online for freedom, control and fun. Calif. Manage. Rev.
43(2), 34–56 (2001)
Xia, L.: Affect as information: the role of affect in consumer online behaviors. Adv. Consum.
Res. 29(1), 93–100 (2002)
Yoon, H.S., Steege, L.M.B.: Development of a quantitative model of the impact of customers’
personality and perceptions on Internet banking use. J. Comput. Hum. Behav. 29(3), 1113–
1141 (2013)
Yuslihasri, I.A., Daud, A.K.: Factors that inﬂuence customers buying intention on shopping
online. Int. J. Mark. Stud. 3(1), 128–143 (2011)
Zhou, T., Lu, Y.: The effects of personality traits on user acceptance of mobile commerce. Int.
J. Hum. Comput. Interact. 27(6), 545–561 (2011)
Zikmund, W.G.: Business Research Methods. Dryden Press, Fort Worth (2000)
Technology Perception, Personality Traits and Online Purchase
407

Analysis of Thailand’s Foreign Direct
Investment in CLMV Countries Using SUR
Model with Missing Data
Chalerm Jaitang1, Paravee Maneejuk2,3(B), Aree Wiboonpongse1,
and Songsak Sriboonchitta2,3
1 Faculty of Economics, Prince of Songkla University, Song Khla 90110, Thailand
chalermjaitang@gmail.com, aree.w@psu.ac.th
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
mparavee@gmail.com, songsakecon@gmail.com
3 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
Abstract. Thai enterprises and companies have turned their attentions
to CLMV countries, since the establishment of ASEAN Economic Com-
munity (AEC) in 2015, due to market and production opportunities. This
study is conducted with an attempt to provide useful information help-
ing the Thai investors make investment decision. In particular, this study
examines the determinants of outward Thailand’s direct investment to
the CLMV countries, and later estimates marginal eﬀects. During the
analysis, data unavailability in the CLMV and missing values in many
available variables become destructive. This study handles this prob-
lem by using the bootstrap-based expectation maximization with boot-
strapping (EMB). Once a complete data set is obtained, this study then
employs the Seemingly Unrelated Regression (SUR) model to analyse
the eﬀects of the considered variables on Thailand’s direct investment in
the CLMV group. The estimated results show distinct determinants for
the countries, which can be useful to investors.
1
Introduction
The eclectic paradigm provides that a ﬁrm engages in Foreign Direct Invest-
ment (FDI) if three main conditions are satisﬁed: an ownership advantage, a
location advantage, and an internalization advantage [2]. Eclectic paradigm has
remained the dominant analytical framework for accommodating a variety of
operationally testable economic theories of FDI and foreign activities of multi-
national enterprises (MNEs) [4]. Dunning [3] proposed a four-stage investment
development path to describe how countries inward and outward FDI positions
evolve as local ﬁrms develop transitional corporations capacities. In stage one,
there is little movement toward undertaking FDI. In stage two, there is still little
movement toward outward investment. The outward investment that does occur
is also most likely to support trade, but it is increasingly designed to support
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_29

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
409
products that require larger scale production and more capital. In stage three,
as economy matures, companies seek to beneﬁt from their distinctive capabil-
ities and competencies. Outward investment may be driven by either resource
or market-seeking motives. Finally, in stage four, the post-industrial or services
stage, outward FDI depends more on capabilities through knowledge creation
and the blurring of the distinction between products and services.
Four distinctive phases of Thai outward FDI can be discerned [11]. The ﬁrst
phase (early stage) before the ﬁrst half of the 1980s saw a limited amount of
Thailand’s investment abroad. Much of the overseas investment during this phase
went to a few key destinations such as the United States, Hong Kong (China),
Singapore and Japan. These four economies accounted for over 85 % of the net
Thai equity capital investment abroad. The second phase (take-oﬀstage) took
place between 1986 and 1996 when Thai outward FDI increased rapidly. Thai
companies ventured further aﬁeld to such locations as Australia, Canada, as
well as European countries. While the United States and Hong Kong (China)
continued to be the principal host economies, other Asian countries, particularly
ASEAN countries and China, have emerged as signiﬁcant destinations because
of the cost advantage, market size and business opportunities. The third phase
(ﬁnancial crisis impact stage), the period 1997 to 2002, saw a dramatic decline in
Thai outward FDI due to the impact of the ﬁnancial crisis, which signiﬁcantly
aﬀected the ability of Thai enterprises to invest or maintain their investment
abroad. Thai outward FDI to China, Europe, Hong Kong (China) and the United
States fell considerably and ﬂows to ASEAN during this period also fell by 36 %
in absolute terms, compared with the period before the ﬁnancial crisis (1989–
1996). Despite the decline, Thai outward FDI at this stage was greater than in
the earlier period. The fourth phase (recovering stage), which started in 2003,
saw a recovery in outward FDI. ASEAN and China were the main recipients
while FDI to Europe and the United States began to pick up but remained at
the low level. Manufacturing was the most active sector for Thai outward FDI.
Thailand’s integration into the ASEAN Economic Community (AEC) in 2015
is beneﬁcial for Thai enterprises and companies investing in neighboring coun-
tries, especially Cambodia, Laos, Myanmar and Vietnam (henceforth CLMV)
since they will bring about market and production opportunities for Thailand.
The motive for ﬁrms to engage in foreign production and MNEs activity can be
classiﬁed into four groups: natural resource seeking, market seeking, eﬃciency
seeking, and strategic asset or capability seeking [5]. Thailand can use CLMV as
a production base to solve its labor and energy problems. Moreover, Thailand,
with its strategic geographical location, can position itself as a major service hub
of the sub-region [10]. The development of transportation infrastructure in Thai-
land between 2013 and 2020, including the construction of new railway linking
Bangkok with neighboring countries, is expected to reduce transportation costs
within supply chain centered on Thailand [8]. The integrated mainland ASEAN
will bring about larger market size which will allow Thai companies to invest
more in CLMV countries. Thai companies can also relocate or expand their
low value-added production activities to the neighbouring countries, especially

410
C. Jaitang et al.
CLMV, while upgrading their domestic production to higher value-added activ-
ities. This study aims to analyze the opportunities exist in CLMV countries,
and then provide this information to help Thai investors make investment deci-
sion. This study examines the relation between Thailand’s direct investment in
CLMV countries and four groups of the motive factors, and also estimates their
marginal eﬀects on Thailand’s outward FDI.
The rest of the paper is organized as follows. Section 2 provides a brief deﬁni-
tion of variables used in the study, which are classiﬁed into four diﬀerent groups.
The methodology is presented in Sect. 3. Section 4 provides the empirical ﬁnding.
Finally, Sect. 5 contains conclusions.
2
Data Descriptions
This section presents the factors that may inﬂuence outward Thailand’s direct
investment (unit: million U.S. dollars). These factors relate not only to the coun-
try’s economic performance, but also to policy decisions taken by its government.
The factors are classiﬁed into four diﬀerent groups namely natural resource seek-
ing, market seeking, eﬃciency seeking, and strategic asset or capability seeking
according to Dunning and Lundan [5] The ﬁrst group is resource seeking con-
taining minimum wage, labor force, corporate income tax rate, agriculture value
added, industry value added, and service value added. The second group is mar-
ket seeking including gross domestic product, gross domestic product growth
rate, and gross domestic product per capita (in terms of constant 2010 US$).
The third group is eﬃciency seeking containing openness rate, inﬂation rate, and
exchange rate. The last group is capability seeking. The variables in this group
consist of general government ﬁnal consumption expenditure, gross capital for-
mation, and patent. The deﬁnitions of variables in each group as well as their
labels are illustrated in Table 1.
3
Methodology
Since the CLMV region, which is designated as least developed countries, is our
area of interest, we are now facing a problem of data unavailability. During a
process of data collection, we ﬁnd that the data of CLMV countries are consid-
erably unavailable. Some variables for some countries are unavailable and many
of them are missing. This becomes a limitation of the analysis. So, we decided
to eliminate the unavailable variables and considered only the variables that are
oﬃcially provided. However, missing data occurred in the remaining variables is
still an obstacle. In fact, missing data is ubiquitous problem for least developed
and underdeveloped countries, and multiple imputation is a general approach to
this problem. It creates a multiple ﬁlled in of the incomplete data set. In this
study, we consider particularly the bootstrap-based EMB (Expectation Maxi-
mization with Bootstrapping) of Honaker et al. [6] for dealing with missing data
occurred in CLMV countries. This algorithm uses the classical EM algorithm
on the multiple bootstrapped data from incomplete data to draw values of the

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
411
Table 1. Data descriptions
Type of Motive
Variable
Label
Description
Resource
seeking
Wage
Wit
Gross average nominal monthly wages (Unit: U.S. dollars per month)
Labor force
Lit
Labor force comprises people ages 15 and older who supply labor for the production of goods and services
during a speciﬁed period. It includes people who are currently employed and people who are unemployed
but seeking work as well as ﬁrst-time job-seekers. (Unit: absolute value in thousands)
Corporate income tax
rate
CITit
A direct tax levied on a juristic company or partnership carrying on business (Unit: %)
Agriculture value added
Ait
Agriculture corresponds to ISIC divisions 1–5 and includes forestry, hunting, and ﬁshing, as well as
cultivation of crops and livestock, production. (constant 2010 U.S. dollars) (Unit: million U.S. dollars)
Industry value added
Iit
Industry corresponds to ISIC divisions 10–45 and includes manufacturing (ISIC divisions 15–37). It
comprises value added in mining, manufacturing, construction, electricity, water, and gas. (constant 2010
U.S. dollars) (Unit: million U.S. dollars)
Service value added
Sit
Services correspond to ISIC divisions 50–99. They include value added in wholesale and retail trade
(including hotels and restaurants), transport, and government, ﬁnancial, professional, and personal services
such as education, health care, and real estate services. (constant 2010 U.S. dollars) (Unit: million U.S.
dollars)
Market seeking
Gross domestic product
growth rate
Git
Annual percentage growth rate of GDP (Unit: %)
Gross domestic product
GDPit
GDP expressed in constant 2010 U.S. dollars (Unit: million U.S. dollars)
Gross domestic product
per capita
GDP capit
GDP per capita is gross domestic product divided by midyear population (constant 2010 US$) (Unit: U.S.
dollars)
Eﬃciency
seeking
Inﬂation Rate
INFit
CLMV countries’ core inﬂation (Unit: %)
Openness rate
OP ENit
The ratio of trade (exports and imports) to GDP (Unit: %)
Exchange rate
EXit
Average exchange rate (Unit: local currency per U.S. dollar)
Capability
seeking
Gross capital formation
Gcfit
Consists of outlays on additions to the ﬁxed assets of the economy plus net changes in the level of
inventories (Unit: million U.S. dollars)
General government ﬁnal
consumption expenditure
EXPit
Includes all government current expenditures for purchases of goods and services (including compensation
of employees), also includes most expenditures on national defense and security, but excludes government
military expenditures that are part of government capital formation (Unit: million U.S. dollars)
Patent
P atit
Patent applications are worldwide patent applications ﬁled through the Patent Cooperation Treaty
procedure or with a national patent oﬃce for exclusive rights for an invention–a product or process that
provides a new way of doing something or oﬀers a new technical solution to a problem.(Unit: pieces)
Note: i represents countries in CLMV and t corresponds to time.

412
C. Jaitang et al.
complete data parameters, and then draw imputed values to ﬁll in the missing
values. After obtaining completed data set, we can use this data in the empirical
analysis.
Empirically, this study employs a well-known Seemingly Unrelated Regres-
sion (SUR) model of Zellner [12] to analyze eﬀects of the considered variables
(as shown in Table 1) on Thailand’s direct investment in CLMV countries, with
regard to appropriateness of this research tool. A brief idea of this model is
that it consists of several regression equations in which each equation can have
its own dependent and independent variables [9]. However, all equations are
related somehow by the error terms. When we consider economic condition of
each of the four CLMV countries in detail, we will ﬁnd that Cambodia, Laos,
Myanmar, and Vietnam have both political and economic disparities. These in
turn inﬂuence the direct investment from foreign countries, including Thailand.
So, we may write regression equation for each of the CLMV countries separately,
having its own FDI from Thailand and potentially diﬀerent sets of explanatory
variables.
3.1
Seemingly Unrelated Regression (SUR) Model
The seemingly unrelated regression (SUR) model is deﬁned by the set of regres-
sions. Suppose there are M regression equations,
yit = xitβi + εit
i = 1, ..., M; t = 1, ..., T,
(1)
which can be written in a vector form as
Y = Xβ + ε.
(2)
The subscript t corresponds to time and i represents equation number corre-
sponding to the CLMV countries, where C, L, M, and V stand for Cambodia,
Laos, Myanmar, and Vietnam respectively. However, we are now showing a struc-
ture of the SUR model in general form with M regression equations, i = 1, ...., M.
Y is a vector of dependent variables, X is a matrix of independent variables or
so-called regressors, and β is a vector of unknown regression parameters. ε is a
vector of the error terms which has a zero mean and variance-covariance matrix
Γ ⊗I, where I is the unit matrix of order M and Γ = σijI, i ̸= j. Equation (2)
can be written in the compact form as
X =
⎡
⎢⎢⎢⎢⎢⎢⎣
x1t 0 · · · · · ·
0
0 x2t · · · · · ·
0
...
...
...
...
...
...
0
0 · · · · · · xMt
⎤
⎥⎥⎥⎥⎥⎥⎦
, Y =
⎡
⎢⎢⎢⎢⎢⎢⎣
y1t
y2t
...
...
yMt
⎤
⎥⎥⎥⎥⎥⎥⎦
, β =
⎡
⎢⎢⎢⎢⎢⎢⎣
β1
β2
...
...
βM
⎤
⎥⎥⎥⎥⎥⎥⎦
, ε =
⎡
⎢⎢⎢⎢⎢⎢⎣
ε1t
ε2t
...
...
εMt
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(3)

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
413
In addition, this model assumes that the errors of each equation are related.
Thus, under this assumption, the variance-covariance matrix is given by
Γ =
⎡
⎢⎢⎢⎣
σ11I σ12I
σ1MI
σ21I σ22I
σ2MI
...
...
...
σM1I · · · · · · σMMI
⎤
⎥⎥⎥⎦.
(4)
The generalized least squares (GLS) estimator is conducted to estimate the
unknown parameters β in the SUR model, which can be shown as
β = (X′Γ −1X)
−1X′Γ −1Y
(5)
3.2
Expectation Maximization with Bootstrapping (EMB)
for Handling Missing Data
Under a missing at random (MAR) assumption of Honaker and King [7], the
pattern of missing data Dmiss only depends on the observed data Dobs. We can
deﬁne the MAR assumption by
p(M
D) = p(M
Dobs),
(6)
where D denotes both data sets of Dobs and Dmiss; and M is a missingness
matrix with cell mij = 1 if dij ∈Dmis otherwise mij = 0. Here D is assumed
to have a multivariate normal distribution with mean vector μ and covariance
matrix Σ. Thus the likelihood of observed data is given by
p(Dobs, M |Θ) = p(M
Dobs )p(Dobs |Θ)
(7)
where Θ = {μ, Σ} is the complete-data parameters. Since we are interested in
making inference about the complete data parameters, we can write the likeli-
hood as
L(Θ
Dobs) ∝p(Dobs |Θ)
(8)
We can rewrite using the law of iterated expectations, and thereby
p(Dobs |Θ) =
	
p(D |Θ)dDmis.
(9)
In this study, we add Bayesian priors into individual cell values in order to
improve the imputation models and obtain potentially valuable and extensive
information. Thus, the posterior distribution can be done by combining ﬂat
priors with the likelihood function, Eq. (9), using Bayes theorem. Thus, the
posterior distribution can be formed as follows
p(Θ
Dobs) ∝p(Dobs |Θ) =
	
p(D |Θ)dDmis
(10)

414
C. Jaitang et al.
Then, the EMB algorithm, which combines the classical EM algorithm with
a bootstrap approach [1], takes draws from this posterior. For each draw, we
bootstrap the data to simulate estimation uncertainty, and then run the EM
algorithm to estimate the model of the posterior for the bootstrapped data.
Once we have draws of the posterior of the completed data parameters, we
create imputations by drawing values of Dmis from its distribution conditional
on Dobs and the draws of Θ. In this study, the number of imputed data sets to
create is m = 10, meaning that we have 10 imputed data sets for the SUR model.
To combine the results, we estimate SUR model using these 10 data sets and
obtain (β1, ..., β10). Then, the parameter coeﬃcient of the multiple imputation
point estimate is
¯β = 1
m
m

i=1
βi.
(11)
The standard error of the parameter ¯β is given by
SE(β)2 = 1
10
10

i=1
SE(βi)2 +
 10

i=1
(βi −¯β)
2/(10 −1)

(1 + 1/10)
(12)
4
Empirical Analysis
This section consists of three parts. In the ﬁrst subsection, we describe features
of the data through descriptive statistics. Moreover, this subsection also presents
a missingness map to illustrate where the missing value occurs in the data set.
The missing values in data set are still of our concern, so that we go further and
predict the missingnesses in the second subsection. Finally, the third subsection
presents the main results of this study and some discussions on our ﬁndings.
4.1
Descriptive Statistics
This study considers the data of Cambodia, Laos, Myanmar, and Vietnam
(CLMV countries) relevant to four groups of variables, as stated in Sect. 2,
namely natural resource seeking, market seeking, eﬃciency seeking, and capa-
bility seeking. The variables contained in each group are described in Table 1.
Moreover, we also use the data of Thailand’s outward FDI to the CLMV coun-
tries as the dependent variable. This variable is denoted by FDIi, where subscript
i represents countries in CLMV. All the data are collected annually from 1988
to 2015, covering 28 observations. We ﬁnd that the data of corporate income
tax rate and patent applications are not available for the case of CLMV coun-
tries, thus we have to eliminate these two terms. The descriptive statistics of the
remaining variables are presented in Table 2.
To deﬁne more clearly the missing values, we then plot missingness map
of the data for each country. The use of this map is to show which particular

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
415
Table 2. Descriptive statistics
Cambodia F DIC
GDPcap GDP
G
INF
Gcf
EX
L
EXP
OPEN W
A
I
S
Min
−3.44
314.9
1407
−0.091
−0.826
132.3
426.2
4072
1260
0.1171 20
788.5
202.3
658.5
Median
1.34
548.1
4477
0.09423
6.246
747.9
3944.8
6126
3521
1.116
46
1544.9
965
1609.5
Mean
2.009
598.8
7225
0.1058
26.097
1110.8
3374.3
6244
5979
0.9411 54.75 2233.3
1464.3 2501.9
Max
9.14
1020.9
19958
0.3242
191
3690.4
4184.9
8800
14977
1.4475 128
4822.3
4500
6706.8
SD
2.708
224.1
5561.21
0.0912
48.001
986.86
1101.01 1560.84 4652.6
0.4008 24.14 1407.3
1297.9 1870.62
Missing
9
1
1
2
8
1
4
Laos
F DIL
GDPcap GDP
G
INF
Gcf
EX
L
EXP
OPEN W
A
I
S
Min
−4.41
402.8
1.26E + 04 −2.01
−26.317 89.03
800.7
1838
2822
0.3391 8
302.6
202.3
320.6
Median
0.53
713.7
1.82E + 09 7.011
7.742
326.66
8147.9
2547
7480
0.5466 12
746.1
965
741
Mean
2.583
810.1
3.79E + 09 6.813
16.965
976.52
7498.6
2610
10932
0.5589 29.67 1127.8
1464.3 1483
Max
21.22
1531.2
1.24E + 10 14.191
128.409
4131.73 10655.2 3573
26201
0.8005 110
2900
4500
4833.1
SD
6.013
326.1
3.66E + 09 2.4853
29.497
1094.09 3151.2
528.6
8358.1
0.1192 31.06 777.366 1230.3 1395.02
Missing
9
1
12
1
4
Myanmar
F DIM GDPcap GDP
G
INF
Gcf
EX
L
EXP
OPEN W
A
I
S
Min
−16.87 184.6
6.48E + 09 −11.352 −1.723
449
5.44
19003
303
0.2901 20
2592
437.8
1488
Median
0.39
389.4
2.60E + 10 8.564
10.82
1094
6.12
24735
3184
0.4014 30.97 5040
1138.7 3052
Mean
1.069
542.1
3.23E + 10 8
16.612
4452
6.07
24454
5233
0.4133 33.28 7899
5384.4 7400
Max
21.47
1308.7
6.56E + 10 13.844
58.104
23490
6.75
29918
18908
0.6896 61.06 18812
23000
25038
SD
6.506
360.4
2.36E + 10 5.2727
17.105
6864.33 0.39
3443.6
5852.7
0.1171 13.21 5836.5
7751.5 8147.02
Missing
9
12
10
1
9
11
5
Vietnam
F DIV
GDPcap GDP
G
INF
Gcf
EX
L
EXP
OPEN W
A
I
S
Min
−231.1 412.6
6.29E + 09 4.688
−1.77
830.8
11032.6 29707
73419
0.1623 8
2507
1381
1842
Median
7.78
846.9
3.66E + 10 6.734
8.23
10188.9 15858.9 43502
313627
1.1252 19
7862
12979
13060
Mean
99.7
925.5
6.35E + 10 6.726
28.519
17737.6 16259.5 43407
474560
1.1174 25.39 12748
23078
25261
Max
937.3
1684.7
1.94E + 11 9.54
374.354
49963.7 21697.6 55654
1242937
1.6982 54
35000
71694
90000
SD
251.72
383.5
5.84E + 10 1.378
71.485
16435.6 3309.4
7817.45 398020.9 0.4075 16.32 10428.1 22805
25882.8
Missing
3
1
10
1
5
Source: Calculation

416
C. Jaitang et al.
observation in a series is not provided or missing. All data is illustrated with
a grid in which its color refers to missingness status. The meaning of colors is
that a blue grid represents missing value and a pink grid represents observed
value. In each map, the horizontal axis shows variable labels and the vertical
axis shows the number of observations, which is totally 28 observations. Figure 1
shows that Myanmar has maximum number of missing values at the border
of these variables. This country does not provide long-term historical data of
macroeconomic variables especially, for example, GDP, openness rate (OPEN),
inﬂation (INF), and general government ﬁnal consumption expenditure (EXP).
Moreover, all the countries in CLMV do not provide the latest data on openness
rate and gross capital formation (GCF). However, overall results suggest that
FDIC, EXPL, GDPM, and EXPV are most likely to have missing data. (Note
that the subscripts refer to countries in CLMV).
 
Cambodia Data
FDIC
EXPC
WC
KHR
OPENC
GcfC
GC
SC
IC
AC
LC
INFC
GDPC
GDPcC
13
28
Missing
Observed
Laos Data
EXPL
FDIL
WL
OPENL
GcfL
SL
IL
AL
LL
LAK
INFL
GL
GDPL
GDPcL
13
28
Missing
Observed
Myanmar Data
GDPM
OPENM
INFM
EXPM
FDIM
WM
GcfM
SM
IM
AM
LM
MMK
GM
GDPcM
13
28
Missing
Observed
Vietnam Data
EXPV
WV
FDIV
OPENV
GcfV
SV
IV
AV
LV
VND
INFV
GV
GDPV
GDPcV
13
28
Missing
Observed
Fig. 1. Missingness map of the data for CLMV countries
4.2
Predicting Missing Values
Multiple imputation involves imputing values for each missing data in the
matrix and creating a completed data set. Across these completed data sets,

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
417
the observed values are the same, but the missing values are ﬁlled or replaced
with a sample of values from the predictive distribution of missing data.
Figure 2 shows the prediction of missing values in the time series from 1988
to 2015. We illustrate the observed value and predicted value for gross capital
formation of Cambodia (top left panel), foreign direct investment of Thailand
to Cambodia (top right panel), inﬂation rate of Myanmar (bottom left panel),
and government expenditure of Vietnam (bottom right panel). In these panels,
the black points represent observed data while the red points are the mean
imputation for each of the missing values, along with their 95% conﬁdence bands.
Here, we draw these bands by imputing each of missing values 100 times to
obtain the imputation distribution for those observations. The imputation of
each missing value is considerably reasonable as it is located within the line.
After the imputation with EMB algorithm, we apply the SUR model to the
completed data sets and use a generalized least squares (GLS) technique to
estimate unknown parameters. Finally, we combine the estimated parameters
obtained from the SUR model into one set of parameters, and then use the
mean values of the new parameter set to make inferences and draw conclusions.
Consequently, the estimated results are shown in Table 3, in the next subsection.
1990
1995
2000
2005
2010
2015
0
1000
2000
3000
4000
Gross Capital Formation of Cambodia
time
C
fc
G
1990
1995
2000
2005
2010
2015
-20
0
20
40
60
FDI THAI-CAM
time
L
I
D
F
1990
1995
2000
2005
2010
2015
0
20
40
60
80
100
120
Myanmar inflation(%)
M
F
N
I
1990
1995
2000
2005
2010
2015
6
0
+
e
3
-
6
0
+
e
2
-
6
0
+
e
1
-
0
0
+
e
0
6
0
+
e
1
Vietnam Goverment Expenditure
V
P
X
E
Fig. 2. Panels showing mean imputations with 95% bands (in red) and observed data
point (in black)
4.3
Estimated Results
This section will analyze the eﬀects of the considered variables (as shown in
Table 3) on Thailand’s direct investment in CLMV countries using Seemingly

418
C. Jaitang et al.
Table 3. Estimated results for CLMV countries
Variable
FDIC
FDIL
FDIM
FDIV
Constant −1.6738a
9.4041
−11.4831 −2.1963
(0.9105)
(8.5267)
(0.90637) (2.8705)
GDPcap
0.0638
0.0546
0.0206a
0.0065
(0.0932)
(0.0452)
(0.0126)
(0.0563)
GDP
0.0350c
0.0004a
0.00001
−0.00001
(0.0083)
(0.0003)
(0.00001) (0.00001)
G
3.0190c
0.3823
0.0022
−2.4471
(1.0424)
(0.6091)
(0.0031)
(4.7427)
INF
−0.1406b −0.0416
0.0086
0.7371
(0.0585)
(0.0522)
(0.0220)
(3.6177)
Gcf
0.0033
0.0097c
0.0068a
−0.0097c
(0.0057)
(−0.0010) (0.0039)
(0.0027)
EX
−0.0030a
−0.0009
−0.0085
0.0326
(0.0019)
(0.0014)
(0.0547)
(0.0820)
L
0.0053a
−0.0437
0.0041
0.0365
(0.0029)
(0.0422)
(0.0052)
(0.0981)
EXP
0.0009
−0.0004
0.0006b
0.0003
(0.0004)
(0.0005)
(0.0003)
(0.0021)
OPEN
−4.2095
5.1161
1.9879
5.0173*
(11.064)
(8.3304)
(1.5165)
(3.0058)
W
−1.6738a
0.3644
0.3033
0.7957
(0.9105)
(0.3325)
(0.8297)
(3.1472)
A
0.0638
−0.0291)
−0.0183c
0.1141c
(0.0932)
(0.0675)
(0.0019)
(0.0079)
I
0.0350c
9.4041
−0.0026
−0.0642b
(0.0083)
(8.5267)
(0.0295)
(0.0328)
S
3.0190c
0.0546
−0.0252
−2.4471
(1.0424)
(0.0452)
(0.0296)
(4.7427)
Source: Calculation
Note: a, b, and c indicate signiﬁcance at the 90%, 95% and
99% levels, respectively.
Standard errors are reported in parentheses.
Unrelated Regression model. The empirical results indicate that four motiva-
tions persuade Thailand’s direct investment to the neighboring countries: Cam-
bodia, Laos, Myanmar and Vietnam. Myanmar’s GDP per capita (GDPcap),
and market seeking motivation, attract Thai ﬁrms to invest in Myanmar. The
implicit reason provides that Thai products are categorized to be higher brand
premium in neighboring countries, especially Myanmar. That is the reason why

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
419
Myanmar’s GDP per capita positively aﬀects to Thai direct investment. Besides,
gross domestic product (GDP) positively aﬀects Thai companies to invest in
Cambodia and Laos. For instance, in the agro-based industry, ﬁrms mainly
expanded their business to Cambodia and Laos to seek new market opportu-
nities. And GDP growth rate (G) also has favourable eﬀect on Thailand’s direct
investment in Cambodia. The enlarged market will also allow Thai companies
to invest more.
Meanwhile, inﬂation rate (INF) and exchange rate (EX), representing the eﬃ-
ciency seeking motivation, adversely aﬀect Thai direct investment in Cambodia
since the value of Cambodia currency ﬂuctuated over time. Cambodia’ average
core inﬂation also was high, reﬂecting ineﬃciency of monetary measures. But
Vietnam’s openness rate (OPEN) has beneﬁcial eﬀect on Thai direct investment
signiﬁcantly.
On the other hand, the resource seeking motivation is interesting. First and
foremost, average monthly wages have detrimental eﬀect on Thailand’s direct
investment in Cambodia. A major incentive for a multinational enterprise to
invest abroad is to outsource labor intensive production to countries with lower
wages and Cambodia’ average monthly wage is higher than others’. Further-
more, industry and service value-added in Cambodia have satisfactory eﬀect on
Thai direct investment. Vietnam’s agriculture value-added impacts positively on
Thailand’s direct investment but the Vietnam’s industry value-added adversely
aﬀects Thailand’s direct investment. Meanwhile Myanmar’s agriculture value-
added negatively aﬀects Thailand’s direct investment. With the increase of min-
imum wage in Thailand to 300 baht (roughly USD 10) per day, some Thai
investors determine to invest more in CLMV especially labor intensive manu-
facturing sector in the high value-added. The number of labor force from where
labor is supplied for the production of goods and services, has beneﬁcial eﬀect
on Thailand’s direct investment in Cambodia.
Consider the capability seeking motivation: general government ﬁnal con-
sumption expenditure (EXP) and gross capital formation (Gcf), they have
advantageous eﬀect on Thailand’s direct investment in Myanmar. Besides, the
outlays on gross capital formation (Gcf) also positively aﬀect Thailand’s direct
investment in Laos but adversely aﬀect that in Vietnam. Both factors work
like pro-active measures of the recipient countries to promote the foreign direct
investment.
5
Conclusions
The establishment of ASEAN Economic Community (AEC) in 2015, has oﬀered
Thai enterprises and companies an opportunity to invest in neighboring coun-
tries, especially Cambodia, Laos, Myanmar and Vietnam (henceforth CLMV).
Investment in CLMV countries can bring about both market and produc-
tion opportunities for Thailand’s as the companies can relocate or expand
their low value-added activities to those neighboring countries, while upgrading
their domestic production to higher value-added activities. Accordingly, the key

420
C. Jaitang et al.
investment considerations in CLMV countries become our concern. The motives
for ﬁrms to engage in foreign production and MNEs activities can be classi-
ﬁed into four groups, namely natural resource seeking, market seeking, eﬃciency
seeking, and capability seeking. And hence, this study examines the determi-
nants of outward Thailand’s direct investment to CLMV countries based on
these motivations, and later estimate marginal eﬀects.
But during the process of data collection, we faced a problem of data unavail-
ability in the CLMV countries, and missing values in many available variables.
So, we employ the bootstrap-based EMB (Expectation Maximization with Boot-
strapping) for dealing with this missing data problem and eliminate variables
that are unavailable. Once obtaining a completed data set, we proceed to the
empirical analysis. The Seemingly Unrelated Regression (SUR) model is used to
analyze the eﬀects of the considered variables on Thailand’s direct investment
in CLMV. The estimated results suggest that among the speciﬁed variables,
based on the four motivations, GDP per capita, gross capital formation, and
government ﬁnal consumption expenditure can attract Thai ﬁrms to invest in
Myanmar. Gross domestic product as well as gross capital formation are con-
sidered advantageous compared to other variables for encouraging Thailand’s
direct investment in Laos, while gross capital formation and openness rate are
encouraging the outward FDI of Thailand to Vietnam.
Cambodia seems to be the most interesting country for Thai investors to
invest in. The investors are attracted by GDP and its growth rate, as well as labor
force in Cambodia. However, our empirical result also shows that Cambodia does
not succeed in persuading Thai ﬁrms with its lower wage due to the fact that
its average monthly wage is higher than others’.
Acknowledgements. The authors are grateful to Centre of Excellence in Economet-
rics, Chiang Mai University for the ﬁnancial support, and also Mr. Woraphon Yamaka
who kindly helps ensure that our code is correct. Without their supports, this study
could not be completed.
References
1. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete
data via the EM algorithm. J. Roy. Stat. Soc. Ser. B (Methodol.) 39, 1–38 (1977)
2. Dunning, J.H.: Explaining changing patterns of international production: in
defence of the eclectic theory. Oxford Bull. Econ. Stat. 41(4), 269–295 (1979)
3. Dunning, J. H. Multinational Enterprises and the Global Economy, pp. 272–274.
Addison-Wesley, Wokingham (1993)
4. Dunning, J.H.: The eclectic paradigm as an envelope for economic and business
theories of MNE activity. Int. Bus. Rev. 9, 163–190 (2000)
5. Dunning, J.H., Lundan, S.M.: Multinational Enterprises and the Global Economy,
2nd edn. Edward Elgar, Cheltenham (2008)
6. Honaker, J., King, G.: What to do about missing values in time-series cross-section
data. Am. J. Polit. Sci. 54(2), 561–581 (2010)
7. Honaker, J., King, G., Blackwell, M.: Amelia II: a program for missing data. J.
Stat. Softw. 45(7), 1–47 (2011)

Analysis of Thailand’s FDI in CLMV Using SUR Model with Missing Data
421
8. Oizumi, K.: The potential of the Thailand-Plus-One business model-a new frag-
mentation in east asia. RIM Pac. Bus. Ind. 13, 2–20 (2013)
9. Pastpipatkul, P., Maneejuk, P., Wiboonpongse, A., Sriboonchitta, S.: Seemingly
unrelated regression based copula: an application on Thai rice market. In: Causal
Inference in Econometrics, pp. 437–450. Springer International Publishing (2016)
10. Rattanakhamfu, S., Tangkitvanich, S. Strategies and Challenges of Thai Compa-
nies to Invest CLMV. Thailand Development Research Institute (2015)
11. Wee, K.H.: Outward foreign direct investment by enterprise from Thailand.
Transnational Corporations 16(1), 89–116 (2007)
12. Zellner, A.: An eﬃcient method of estimating seemingly unrelated regressions and
tests for aggregation bias. J. Am. Stat. Assoc. 57(298), 348–368 (1962)

The Role of Oil Price in the Forecasts
of Agricultural Commodity Prices
Rossarin Osathanunkul1(B), Chatchai Khiewngamdee2, Woraphon Yamaka3,
and Songsak Sriboonchitta3
1 Faculty of Economics, Chiang Mai University, Chiang Mai, Thailand
orossarin@gmail.com
2 Department of Agricultural Economy and Development, Faculty of Agriculture,
Chiang Mai University, Chiang Mai, Thailand
3 Centre of Excellence in Econometrics, Faculty of Economics,
Chiang Mai University, Chiang Mai, Thailand
Abstract. The objective of this paper is to examine whether includ-
ing oil price to the agricultural prices forecasting model can improve
the forecasting performance. We employ linear Bayesian vector autore-
gressive (BVAR) and Markov switching Bayesian vector autoregressive
(MS-BVAR) as innovation tools to generate the out-of-sample forecast
for the agricultural prices as well as compare the performance of these
two forecasting models. The results show that the model which includes
the information of oil price and its shock outperforms other models.
More importantly, linear model performs well in one- to three-step-ahead
forecasting, while Markov switching model presents greater forecasting
accuracy in the longer time horizon.
1
Introduction
The relationship between agricultural commodity prices and crude oil price is
of concern to both policy makers and academics. Increase in oil prices is consid-
ered to be a main driver for rising prices of agricultural crops since it is highly
relevant in agricultural production. For instance, in practice, oil is required for
running agricultural equipment, and used for transporting and distributing crops
to retailers. In addition, crude oil prices also relate to the agricultural commodity
prices in the context of substitution eﬀect between fossil fuel and biofuel. Agri-
cultural commodities such as corn, soybean, and oilseeds are alternatively used
to produce fuel, called biofuel or alternative energy, in the light of unexpected
increase in prices of fossil fuel like crude oil. This development of alternative
energy, in turn, drives up the prices of these crops, and then brings about per-
sistent increases in other agricultural commodities prices due to the limitation
of planting area [1].
This situation has raised concerns among policy makers and academics about
the transmission of oil price shocks to agricultural commodity prices. According
to literature review, there are a vast number of studies on the price relations
between crude oil and agricultural commodities, and the spillover eﬀect of oil
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_30

The Role of Oil Price in the Forecasts of Agricultural Commodity Prices
423
price shocks on the commodity prices. For example, the studies of Saghaian [2]
and Esmaeili and Shokoohi [3] have shown the empirical evidences supporting
the relationship between oil prices and commodity prices, and also the inﬂuence
of oil price shocks on prices of agricultural product. On the contrary, few studies
could not ﬁnd the signiﬁcant eﬀect of oil shocks on the variation in crop price,
such as Zhang et al. [4] and Campiche et al. [5]. In other words, the unexpected
increases in oil price could not explain all of commodity prices increase. The
main reason is that other factors, in particular, government policies and world
economic situation, can also cause the price variation, and often in emerging
markets where the crop prices are more susceptible to domestic policies. More-
over, emerging markets do not have enough power to inﬂuence movements, so
that they have to take the eﬀects from external price shocks.
Recently, Gupta and Kotz [6] pointed out that the oil price shocks can pro-
duce the eﬀect not only on agricultural commodity prices, but also on both
macroeconomic and ﬁnancial variables. These authors considered the use of unex-
pected oil price shocks in forecasting the nominal interest rate in South Africa,
and disaggregated the shocks into positive and negative parts. They found that
the forecasting with a special concern about oil price shocks can outperform and
produce more accurate forecasts than the one excluding the shocks. This per-
suades us to revisit the issue of price volatility in agricultural commodity owing
to unexpected change in oil price. To be more speciﬁc, this study focuses on the
eﬀects of oil price shocks on agricultural commodity prices, particularly on corn,
wheat, and sugar, and aims to evaluate the role of oil price as well as its shocks
in forecasting commodity price movements, as introduced by Gupta and Kotz
[6].
In this study, we deal with the forecasting issue within the context of the
vector autoregressive (VAR) and the Markov Switching vector autoregressive
(MS-VAR) models, focusing on the commodity forecasts and structural break
in the mean growth rate of the commodity prices. To estimate the unknown
parameters in the models, we employ Bayesian estimation. Pastpipatkul, et al.
[7] and Gupta and Kotz [6] suggested that Bayesian estimation can avoid possible
misspeciﬁcation errors due to an identiﬁcation of a stochastic or deterministic
trend for the estimation of parameters.
The rest of the paper is organized as follows: Sect. 2 describes the method-
ology used in this study that is Markov-switching VAR model associated with
Bayesian approach. Then data description and empirical results are provided in
Sect. 3. Section 4 comprises the conclusion.
2
Methodology
2.1
Markov-Switching VAR
In this section, we brieﬂy outline the structure and the estimation of the model
that is used in the forecasting. The general speciﬁcation of a Markov Switching

424
R. Osathanunkul et al.
VAR model can be written as
Y ′
t = C(st) +
P

p=1
Ap(st)Y ′
t−p + ε′
tΣ−1(st), t = 1, · · · , T
(1)
where Y ′
t is n × 1 vector of endogenous variables, Ap is n × n matrix of regime
dependent autoregressive coeﬃcients order p, C is vector of regime dependent
intercept terms, ε is the vector of n unobserved noise, and Σ is n × n positive
matrix of variance. st is the time t realization of a discrete latent process that we
call a “state” or “regime”. To determine the number of lag p, we employ Bayesian
information criterion (BIC) and the lowest BIC is preferred. Note that, in the
case of linear VAR model, the state or regime latent variable, st, is not included in
the model, and thus all unknown parameters are regime independent. Following
Sim, Waggoner, and Zha [8], they provided a distributional assumption with
densities of the MS-VAR disturbances as follows
P(εt|Yt−1, (st), Θ) = N(εt|0n×1, In),
(2)
and on the information set
P(Yt|Yt−1, st, Θ) = N(Yt|C(st), Az(st), Σz(st)),
(3)
where Θ is all unknown parameters in the model. We also assume that the
evolution of the latent variable driving the regime changes, (st), is governed by
a ﬁrst-order Markov chain with transition probabilities matrix. Suppose, we have
h regimes, the transition matrix can take the form as:
Q =
⎡
⎢⎢⎢⎣
ρ11 ρ12 · · · ρ1h
ρ21 ρ22 · · · ρ2h
...
...
...
ρh1 ρh2 · · · ρhh
⎤
⎥⎥⎥⎦
(4)
In this study, we adopt a Bayesian approach to estimate all unknown para-
meters. This approach allows us to incorporate the prior distribution to the
parametric likelihood function in order to construct the joint posterior distri-
bution for the parameters and the latent variables (st). The likelihood of the
MS-VAR model is built by Yt.
ln P(Yt|Q, Θ, st) =
T

t=1
ln
	
st∈h
P(Yt|Q, Θ)P(st|Q, Θ)

(5)
where P(Yt|Q, Θ) is the density used to sample the probability that (st) is in
regime i given (st−1) = j. To sample the parameter set in the model, we employ
a Gibbs sampling algorithm as suggested by Sim, Waggoner, and Zha [8]. The
estimation of the MS-VAR model in Eq. (1) depends on the joint posterior dis-
tribution of Θ and Q which is derived by Bayesrule, thus we have
P(Q, Θ(st)|YT ) ∝P(Yt|Q, Θ(st))p(Q, Θ(st)),
(6)

The Role of Oil Price in the Forecasts of Agricultural Commodity Prices
425
where p(Q, Θ(st)) denotes the prior of Q and Θ. However, the estimation of
this model is diﬃcult to get the converge solution; the block EM algorithm of
Nason and Tallman [9] is considered here and the blocks are Bayesian Vector
Autoregressive (BVAR) regression coeﬃcients for each regime (separating for
intercepts, AR coeﬃcient, and error covariance) and transition matrix.
For the prior of this model, we consider the prior distributions for the MS-
VAR coeﬃcients that belong to the following Normal-Inverse-Wishart prior. We
assume A(st), N(n, Σ) and Σ ∼IW(Ψ, d), where b, Σ, Ψ and d is a vector
of hyperparameters. Then we employ the Markov chain Monte Carlo (MCMC)
Gibbs sampler in order to estimate the marginal likelihoods and Bayes factor or
marginal posterior distribution of interest for inference by running 2,000 steps
of MCMC simulator as follows:
(1) Draw A1 from P(Σ1|Σ0, C0, A0, Q0, Yt).
(2) Draw Σ1 from P(Σ1|Σ0, C0, A1, Q0, Yt).
(3) Draw C1 from P(C1|Σ1, C0, A1, Q0, Yt).
(4) Draw Q1 from P(C1|Σ1, C0, A1, Q0, Yt).
This completes a Gibbs iteration and we obtain A1, Γ1, C1, and Q1. Then,
using these new parameters as starting values in order to repeat the prior iter-
ation of A(i), Γ (i), C(i), and Q(i) draws. Repeating the previous iterations for
1,000 times to obtain a sequence of random draws:
(A1, Σ1, C1, Q1), · · · , (A10000, Σ10000, C10000, Q10000)
(7)
Finally, we can obtain the estimated parameters from the mean of each para-
meter draw.
3
Empirical Results
3.1
Data Analysis
We collected the data from Thomson Reuters database. The dataset used is
weekly data which goes from April 1st, 2005 to March 31st, 2017 for a total of
627 observations and the data variables consist of CBoT Corn Futures, CBoT
Wheat Futures, ICE-US Sugar Futures, and Crude Oil Futures. To compute oil
price shock, we then deﬁne the oil price (OP) process as a random walk with
the appropriate moving average representation:
OPt = α + βOPt−1 + εt
where εt is deﬁned as a shock of oil price. Thus, this provides us with one
additional measure of oil prices that represent oil price shock (Table 1).

426
R. Osathanunkul et al.
Table 1. Data description
Corn
Wheat
Sugar
Oil
shock
Mean
0.0831
0.0442
0.1086
−0.0184
0.0001
Median
0.2309
−0.0403
−0.2362
0.1722
0.1300
Maximum
20.2837
15.9499
20.0751
25.1791
23.5200
Minimum
−25.4272 −16.9865 −19.2067 −37.0059 −0.3673
Std. Dev
4.4984
4.6053
4.7304
5.3485
0.0534
Skewness
−42.3358 13.3674
4.2099
−46.63
−0.4909
Kurtosis
611.4265
387.9439
417.633
832.9197
8.0320
Jarque-Bera 27167.29
2203.749
3627.781
76346.15
684.5177
Probability
0
0.000016
0
0
0
ADF-test
−24.2844 −24.6926 −25.5893 −24.2499 −24.9737
3.2
Model Selection
In this study, we aim to compare the performance of oil price in forecasting
agricultural prices including CBoT Corn Futures, CBoT Wheat Futures, ICE-
US Sugar Futures, and Crude Oil Futures. Our benchmark case is the model
that does not include any measure of oil prices, which we denote BVAR1 (Yt =
{Cornt, Wheatt, Sugart}). We then compare three commodity price models, the
benchmark BVAR1 and the other two that include various features of oil price.
The additional models are organized as follows:
(i) BVAR2 (Yt = {Cornt, Wheatt, Sugart, Oilt}) denotes the BVAR1 model
that includes oil price data;
(ii) BVAR3 (Yt = {Cornt, Wheatt, Sugart, Oilt, shockt}) denotes the BVAR1
model that includes oil price and oil price shocks. Hence, the BVAR1 model
is nested within the respective BVAR2, and BVAR3 models. However, a
non linear behavior might exist in the agriculture data. To investigate the
nature of a potential nonlinear behavior in our data, we also consider various
null and alternative hypotheses which result in the following two competing
forecasting models: Model I: A BVAR model and Model II: A MS-BVAR
model. Thus, we have six models for further investigation.
Prior to forecasting, we compare Bayesian inferences from the six alterna-
tive models considered in Subsect. 3.1. Throughout this section, all inferences
are based on 10,000 Gibbs simulations, after discarding the initial 2,000 Gibbs
simulations in order to mitigate the eﬀects of initial conditions. Table 2 summa-
rizes the marginal likelihoods of each of the models. Notice that the marginal
likelihoods suggest that BVAR1, BVAR2 and BVAR3 are not dominated by MS-
BVAR1, MS-BVAR2, and MS-BVAR3. Thus, we conclude that there exists no
structural break in the agricultural prices.

The Role of Oil Price in the Forecasts of Agricultural Commodity Prices
427
Table 2. Value of log marginal likelihood
Log marginal likelihood
BVAR1
3275.319
MS-BVAR1 2742.67
BVAR2
4313.178
MS-BVAR2 2910.925
BVAR3
6847.236
MS-BVAR3 5180.732
Source: Calculations.
Note: By using BIC selection, lag 1 is
an appropriate lag for all models.
3.3
Forecasting Performance over Time
In this section, we conduct an out-of-sample forecast in order to compare the
forecasting performance among models. We ﬁrstly calculate the one- to eight-
weekly-ahead forecasts and compare the forecast value with the real data. Here,
this dataset is based on an out-of-sample period of week 8, 2017 to week 13, 2017,
with an initial in-sample period that spans week 13, 2005 to week 7, 2017. Then,
we analyze the forecasting performance among our proposed models with the
BVAR1 model using root mean square error (RMSE). In doing so, we calculate
the actual RMSE for BVAR1 model, while, for other models, we show the relative
RMSE value. By providing the relative RMSE, we can easily compare the bench-
mark model with other models. In other words, a value of relative RMSE larger
than one indicates that the benchmark BVAR1 model is superior to other mod-
els. Furthermore, we perform MSE-F test, proposed by McCracken [10] which
is used to test the null hypothesis that the forecasting ability are equal between
the restricted BVAR1 model and unrestricted MS-BVAR1, BVAR2, MS-BVAR2,
BVAR3 and MS-BVAR3 models. The statistic is written as
MSE-F = (N −R −h + 1) × d
MSE1
where N denotes the total samples, R represents the number of observations
in an in-sample period, h is a forecasting horizon and d = MSE0 −MSE1.
In this case, MSEi = (N −R −h + 1)−1
T −h

t=R
(ui,t+1)2, where i = 1,0 with ui
is the forecasting error. Since, the alternative hypothesis is that the MSE for
the unrestricted model forecast, MSE1, is less than the MSE for the restricted
model forecast, MSE0. Hence, a positive value of MSE-F statistic indicates that
the restricted BVAR1 model forecasts are inferior to competing models.
Table 3 presents the RMSE for MS-BAVR1, BVAR2, MS-BVAR2, BVAR3
and MS-BVAR3 models relative to BVAR1 model which does not include oil
price and oil price shock. The results show that, on average, the relative RMSE
values of all models are less than one. This indicates that the forecasts from

428
R. Osathanunkul et al.
Table 3. Out-of-sample root-mean square error (week 8, 2017 to week 13, 2017)
Model
1 steps
2 steps
3 steps
4 steps
5 steps
6 steps
7 steps
8 steps
Average
BVAR1
3.5386
3.2168
3.0654
2.9827
3.0344
2.9617
2.8894
2.7697
3.0573
MS-BVAR1 0.9996*** 1.0166***
1.0039***
0.9869*** 0.9913*** 0.9971*** 0.9857*** 1.0004***
0.9977
BVAR2
0.9997*** 0.9993*** 0.9984*** 1.0005***
0.9995*** 1.0001***
1.0006***
1.0000***
0.9998
MS-BVAR2 1.0027***
1.0084***
1.0122***
0.9855*** 0.9901*** 0.9993*** 0.9888*** 0.9999*** 0.9984
BVAR3
0.9996*** 0.9995*** 0.9983*** 1.0004***
0.9994*** 1.0001***
1.0004***
1.0001***
0.9997
MS-BVAR3 0.9995*** 1.0040***
1.0043***
0.9899*** 0.9885*** 0.9869*** 0.9852*** 1.0040***
0.9953
BVAR1 model, which excludes oil price and oil price shock, are less precise than
those from other models including Markov switching models. In addition, the
model with the lowest value of RMSE is MS-BVAR3 model that includes oil
price and oil price shock. It implies that including oil price and oil price shock
to the model can improve the performance of agricultural prices forecasting.
Moreover, based on MSE-F test, the improvements are statistically signiﬁcant.
Interestingly, all nonlinear models, namely MS-BVAR1, MS-BVAR2, and
MS-BVAR3, outperform their linear counterparts according to the lower value
of relative RMSE despite the conclusion, in previous section, from the marginal
likelihoods that there is no structural break in the data. One possible explana-
tion is that nonlinear model performs better than linear model in longer time
horizon forecasting and hence making the average value of relative RMSE of
nonlinear model lower than linear model. As shown in Table 3, from four- to
eight-step-ahead forecasting, the relative RMSE value of MS-BVAR1 is less than
one, while those values are greater than one in the shorter steps. Likewise, the
relative RMSE value of MS-BVAR2 and MS-BVAR3 from four- to eight-step-
ahead forecasting are lower than BVAR2 and BVAR3, respectively. This ﬁnding
is consistent with several researches, for instance, Altavilla and De Grauwe [11],
Chen and Hong [12], and Marcellino [13]. Therefore, we can conclude that the
Markov switching BVAR models are superior to their BVAR counterparts in
longer time horizon forecasting.
4
Conclusions
This paper aims to investigate whether including oil price to the agricultural
prices can improve the forecasting accuracy. We consider Bayesian vector autore-
gressive (BVAR) and Markov Switching Bayesian vector autoregressive (MS-
BVAR) as a forecasting model in this study. We use weekly data to avoid the
mismatch trading days across the markets. The data consist of CBoT Corn
Futures, CBoT Wheat Futures, ICE-US Sugar Futures, and Crude Oil Futures.
To compare the performance of the models, we compared out-of-sample forecasts
of weekly agriculture returns. In this study, the dataset is based on an out-of-
sample period of week 8, 2017 to week 13, 2017, with an initial in-sample period
that spans week 13, 2005 to week 7, 2017.
We use BVAR models to generate the respective forecasts. We consider three
BVAR models excluding oil price, including oil price, and including oil price and
oil price shock denoted as BVAR1, BVAR2, and BVAR3, respectively. We also

The Role of Oil Price in the Forecasts of Agricultural Commodity Prices
429
consider the Markov switching version of these three models. In this study, we
further investigate the structural break in our forecasting model. We compare
the performance of the BVAR1, BVAR2, and BVAR3 with a structural break
in VAR. The results show that the logged marginal likelihoods for linear BVAR
models are lower than MS-BVAR.
We conduct an out-of-sample forecast from one- to eight-step-ahead and com-
pare the forecasting performance of our forecasting models using RMSE criterion.
The results show that all of the competing models outperform BVAR1 model
according to the average value of relative RMSE. Additionally, MS-BVAR3
model which includes oil price and oil price shock has the best forecasting perfor-
mance in our study. It indicates that including oil price and oil price shock to the
model can improve the forecasting performance of the agricultural prices. Fur-
thermore, these improvements are statistically signiﬁcant based on MSE-F test.
More importantly, BVAR models perform well in one- to three-step-ahead fore-
casting. Markov switching BVAR models, however, present greater forecasting
accuracy in the longer time horizon.
References
1. Wang, Y., Wu, C., Yang, L.: Oil price shocks and agricultural commodity prices.
Energ. Econ. 44, 22–35 (2014)
2. Saghaian, S.H.: The impact of the oil sector on commodity prices: correlation or
causation? J. Agric. Appl. Econ. 42(03), 477–485 (2010)
3. Esmaeili, A., Shokoohi, Z.: Assessing the eﬀect of oil price on world food prices:
application of principal component analysis. Energ. Policy 39(2), 1022–1025 (2011)
4. Zhang, Z., Lohr, L., Escalante, C., Wetzstein, M.: Food versus fuel: what do prices
tell us? Energ. Policy 38(1), 445–451 (2010)
5. Campiche, J.L., Bryant, H.L., Richardson, J.W., Outlaw, J.L.: Examining the
evolving correspondence between petroleum prices and agricultural commodity
prices. In: The American Agricultural Economics Association Annual Meeting,
Portland, OR (2007)
6. Gupta, R., Kotz, K.: The role of oil prices in the forecasts of South African interest
rates: a Bayesian approach. Energ. Econ. 61, 270–278 (2017)
7. Pastpipatkul, P., Yamaka, W., Wiboonpongse, A., Sriboonchitta, S.: Spillovers
of quantitative easing on ﬁnancial markets of Thailand, Indonesia, and the
Philippines. In: International Symposium on Integrated Uncertainty in Knowledge
Modelling and Decision Making, pp. 374–388. Springer (2015)
8. Sims, C.A., Waggoner, D.F., Zha, T.: Methods for inference in large multiple-
equation Markov-switching models. J. Econ. 146(2), 255–274 (2008)
9. Nason, J.M., Tallman, E.W.: Business cycles and ﬁnancial crises: the roles of credit
supply and demand shocks. Macroecon. Dyn. 19(4), 836–882 (2015)
10. McCracken, M.W.: Asymptotics for out of sample tests of Granger causality. J.
Econ. 140(2), 719–752 (2007)
11. Altavilla, C., De Grauwe, P.: Forecasting and combining competing models of
exchange rate determination. Appl. Econ. 42(27), 3455–3480 (2010)
12. Chen, Q., Hong, Y.: Predictability of equity returns over diﬀerent time horizons:
a nonparametric approach. Manuscript, Cornell University (2010)
13. Marcellino, M.: A Comparison of Time Series Models for Forecasting GDP Growth
and Inﬂation. Bocconi University, Italia (2007)

Does Forecasting Beneﬁt from Mixed-Frequency
Data Sampling Model: The Evidence
from Forecasting GDP Growth Using Financial
Factor in Thailand
Natthaphat Kingnetr1(B), Tanaporn Tungtrakul1,
and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
natthaphat.kingnetr@outlook.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. It is common for macroeconomic data to be observed at dif-
ferent frequencies. This gives a challenge to analysts when forecasting
with multivariate model is concerned. The mixed-frequency data sam-
pling (MIDAS) model has been developed to deal with such problem.
However, there are several MIDAS model speciﬁcations and they can
aﬀect forecasting outcomes. Thus, we investigate the forecasting perfor-
mance of MIDAS model under diﬀerent speciﬁcations. Using ﬁnancial
variable to forecast quarterly GDP growth in Thailand, our results sug-
gest that U-MIDAS model signiﬁcantly outperforms the traditional time-
aggregate model and MIDAS models with weighting schemes. Addition-
ally, MIDAS model with Beta weighting scheme exhibits greater forecast-
ing precision than the time-aggregate model. This implies that MIDAS
model may not be able to surpass the traditional time-aggregate model
if inappropriate weighting scheme is used.
1
Introduction
Policy makers require reliable forecasting of economic growth. An accurately
gross domestic product (GDP) measuring helps policy makers, economists, and
investors determine appropriate policies and ﬁnancial strategies. Forecast of real
GDP growth depends on many economic variables, while the publication by
statistical agencies of GDP data is generally delayed by one or two quarters. For
forecasting GDP growth, Thailand’s GDP is available only as quarterly data
while other economic variables to be used as leading indicators may be available
in monthly data. There is a huge literature including [6] for the United States
of America and [3] for Euro area who employed ﬁnancial variables as leading
indicators of GDP growth. Ferrara and Marsilli [7] concluded that the stock
index could improve forecasting accuracy on GDP growth.
Thus, involving data sampled at diﬀerent frequencies in forecasting model
seems be to beneﬁcial. From the literature, a way of using high frequency indi-
cators to forecast low frequency variable is the Mixed Data Sampling (MIDAS)
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_31

Does Forecasting Beneﬁt from Mixed-Frequency Data Sampling Model
431
model proposed by Ghysels et al. [10]. It has been applied in various ﬁelds such as
ﬁnancial economics [13] and macroeconomics [4,5,15] to forecast GDP. Clements
and Galvo [5] concluded that the predictive ability of the indicators in compari-
son with an autoregression is stronger. It also allows the regressed and the regres-
sors to be sampled at diﬀerent frequencies and is a parsimonious way of allowing
lags of explanatory variables. MIDAS regression model combined with forecast
combination schemes if large data sets are involved are computationally easy to
implement and are less prone to speciﬁcation errors. Based on the parsimony of
representation argument1, the higher frequency part of conditional expectation
of MIDAS regression is often formulated in terms of aggregates which depend on
a weighting function. However, there are many weighting schemes such as Step,
Exponential Almon, and Beta (analogue of probability density function) [9].
The objective of this paper is to use such important ﬁnancial leading indicator
as Stock Exchange of Thailand (SET) index to forecast Thailands quarterly
GDP growth by using the diﬀerent weighted MIDAS models. In addition to
MIDAS model with weighting schemes, we also consider the traditional time-
aggregate model and the unrestricted MIDAS model. This will allow us to see
whether high frequency data render any beneﬁt in predicting lower frequency
data, and if it does, which model speciﬁcation performs the best in this setting of
forecasting Thailands quarterly GDP growth. The result of study will be useful
for government in imposing appropriate policies and strategies for stabilising
countrys economy.
The organisation of this paper is as follows. Section 2 describes the scope of
the data used in this study. Section 3 provides the methodology of this study and
provides the estimation of this study. Section 4 discusses the empirical results.
Conclusion of this study is drawn in Sect. 5.
2
Data
The data in this study consist of Thailand’s quarterly gross domestic product
(GDP) and monthly Stock Exchange of Thailand (SET) index. GDP is obtained
from the Bank of Thailand while SET index is obtained from the Stock Exchange
of Thailand. The series cover period of 2001Q1 to 2016Q4, while data during
2001Q1 to 2015Q4 are used for model estimation, the rest are left for out-of-
sample forecast evaluation. All variables are transformed into year-to-year (Y-o-
Y) growth rate to reduce the risk of having seasonality. Figures 1 and 2 provide
the plot of GDP growth and SET index growth.
It can be seen from the ﬁgures that there is a huge drop in GDP growth around
the end of 2008 and the beginning of 2009. The SET growth also changes in similar
manner during the same period. This is believed to be the results of US ﬁnancial cri-
sis. Also, around the end of 2011, it can be seen that there is a drop in GDP growth
1 Also called “The principle of parsimony”, it states that the parsimonious model
speciﬁcation is the model that is optimally formed with the smallest numbers of
parameters to be estimated [2].

432
N. Kingnetr et al.
Fig. 1. Quarterly GDP growth 2001Q1 to 2016Q4
Fig. 2. Monthly SET index growth 2001Q1 to 2016Q4
which was caused by the great ﬂood in Thailand. Again, the SET index growth fol-
lows in the same direction. These ﬁgures may suggest that ﬁnancial variable such
as the SET index is a potential predictor for GDP.
3
Methodology
Prior to model estimation and forecasting, it is recommended to check whether
series in the study is stationary or not. Therefore, in this section, we begin with
brief information regarding the unit root tests, followed by forecasting models
employed in this study.

Does Forecasting Beneﬁt from Mixed-Frequency Data Sampling Model
433
3.1
Unit Root Tests
We start with the Augmented Dickey-Fuller (ADF) test [18] which is very well-
known and widely-used in empirical works. The test model can be speciﬁed as
Δyt = α0 + α1yt−1 +
p

i=1
α2iΔyt−i + εt
(1)
where yt is the time series being tested and εt is residual. The hypothesis testing
can be speciﬁed as H0 : α1 = 0 for non-stationary against H1 : α1 < 1 for
stationary.
Next, the Phillips-Perron (PP) test [17] has been frequently used as an alter-
native test to the ADF test. The test employs the same null hypothesis of non-
stationary as in the ADF test. However, the advantage of this test is that the
additional lagged dependent variable is not required in the presence of serial
correlation. Additionally, it is robust to the functional form of the error term
in the model since the test is non-parametric. However, the test requires large
sample properties in order to perform well.
Unlike the ADF test and the PP test, the Kwiatkowski-Phillips-Schmidt-
Shin (KPSS) test introduced by Kwiatkowski et al. [16] has the null hypothesis
of stationary. With alternative way of interpreting the null hypothesis, the KPSS
test complements other unit root tests.
By looking at the results from each test, we can have a better view before
making a conclusion on whether the series is stationary, non-stationary, or incon-
clusive. This is important since the stationarity of the series is required for the
forecasting models considered in the study. Now, we are going to describe the ﬁve
approaches that incorporate higher frequency data in forecasting lower-frequency
variables.
3.2
Time-Aggregate Model
Traditionally, when one working on forecasting that involves mixed frequency
data, all series must be converted into the same frequency. That means all the
series will be transformed into the frequency matching that of series which was
observed at the lowest frequency. As pointed out by Armesto et al. [1], this can
be easily done by taking an average of values from high frequency data within
the time frame of low frequency data. For instance, we work on variable X
which is measured monthly and Y being observed quarterly data. Then, X will
be transformed to match the same frequency of Y by taking the average of X
at each respective quarter. After transformation, we can now use the new Y to
help predict X. This is so-called the time-aggregate model. Suppose that we are
interested in one step forecast, the model can be mathematically speciﬁed as
Yt = α +
p

i=1
βiLiYt +
r

j=1
γjLjXt + εt
(2)

434
N. Kingnetr et al.
with
Xt = 1
m
m−1

k=0
X(m)
t−(k/m)
(3)
where Yt is a lower-frequency variable; X(m)
t−(k/m) denotes the data from high
frequency variable k periods prior to the low frequency period t; m is the fre-
quency ratio between high and low frequency series (In the case of quarterly and
monthly, m = 3 since the higher frequency monthly variable can be observed
three times within each quarter); Xt is the average of X(m)
t−(k/m) at the low fre-
quency period t. L is a lag operator such that LYt = Yt−1, L2Yt = Yt−2 and
so on. i and j denote the selected lag lengths which are determined by Akaike
Information Criterion (AIC). This approach is limited to the fact that it assumes
coeﬃcients of XH
(t−k,m) within each period t to be the same. In addition, there
may be information loss due to the averaging [14].
3.3
MIDAS Regression Models
Ghysels et al. [10] proposed a Mixed Data Sampling (MIDAS) approach to deal
with various frequencies in multivariate model. Particularly, a MIDAS regression
tries to deal with a low-frequency variable by using higher frequency explanatory
variables as a parsimonious distributed lag. It also does not use any aggregation
procedure and can be modelled for the coeﬃcients on the lagged explanatory
variables as allowing long lags in distributed lag function with only small number
of parameters that have to be estimated [5]. The general form of MIDAS model
is given by
Yt = α + γW (θ) X(m)
t−h + εt
(4)
where X(m)
t−h is an exogenous variable measured at higher frequency than Yt. h
is forecasting step. If h = 1, it means we are going to forecast the dependent
variable by one period ahead using current and historical information of X.
W (θ) smooths historical values of X(m)
t−h. Unlike the time-aggregate model which
simply takes the average, there are some weighting schemes here, controlled by
estimated parameter θ that allows us to convert the variable more eﬃciently. It
can be written as
W (θ) =
K

k=1
ω (k; θ) L(k−1)/m
(5)
where K is the optimal number of lagged high frequency variable to be employed
in the model. L is the lag operator such that
L(k−1)/mX(m)
t−h = X(m)
t−h−( k−1
m ),
and ω (k; θ) is the weighting function that can be in various forms. It can be
noticed that it is possible to include the lagged dependent variable into the

Does Forecasting Beneﬁt from Mixed-Frequency Data Sampling Model
435
MIDAS model. Tungtrakul et al. [19] found that it provides a better forecast
accuracy. Hence, the general form of MIDAS model becomes
Yt = α +
p

i=h
βiLiYt−h + γ
 K

k=1
ω (k; θ) L(k−1)/mX(m)
t−h

+ εt
(6)
Now, we will discuss the diﬀerent MIDAS weighting schemes employed in this
study.
3.3.1
Step Weighting Scheme
Rather than transforming the high frequency variable to match the lower one,
this approach directly includes all lags of high frequency variable into the model.
It takes each lagged high frequency variable as an explanatory variable in the
model. Thus, no information has been lost. The MIDAS model under step weight-
ing scheme with step length of s can be speciﬁed as follows:
Yt = α +
p

i=h
βiYt−i +
K

k=1
γk,sX(m)
t−h−(k−1)/m + εt.
(7)
However, this approach puts a restriction on the coeﬃcient of lagged high
frequency variable (γk,s), which is determined by the step parameter (s). For
instance, if the step parameter is equal to three (s = 3), it means the ﬁrst three
lagged have the same coeﬃcient, the next three lags will then employ another
same coeﬃcient. This pattern will continue to the last lag that is incorporated
in the model.
For demonstration purpose, consider the case that p = 1, h = 1, m = 3,
K = 4, and s = 2, then the MIDAS model with step weighting scheme can be
speciﬁed as
Yt = α + β1Yt−1 + γ1,2X(3)
t−1 + γ2,2X(3)
t−1−1/3 + γ3,2X(3)
t−1−2/3 + γ4,2X(3)
t−2 + εt. (8)
If Yt is the GDP growth for the third quarter of 2017, then X(3)
t−1 is a value
of an indicator from June 2017, X(3)
t−1−1/3 is from May 2017, X(3)
t−1−2/3 is from
April 2017, and X(3)
t−2 is from March 2017. Also, the restriction on parameters
are γ1,2 = γ2,2, and γ3,2 = γ4,2.
Another drawback of the step weighting scheme is that the model may suﬀer
from large numbers of parameters due to high diﬀerence in frequency between
high and low frequency series [1]. Suppose that we work on annual series and
monthly, we can see that we have got at least 12 coeﬃcients to be estimated.
Thus, the estimation outcome may not be satisfactory.
3.3.2
Exponential Almon Weight
This weighting scheme has been employed in various empirical studies due to
its ﬂexibility despite involving a few parameters in estimation [8]. The weighing

436
N. Kingnetr et al.
scheme can be speciﬁed as
ω (k; θ) =
exp

kθ1 + k2θ2

K
k=1 exp (jθ1 + j2θ2)
.
(9)
To have a better view how the MIDAS model with Exponential Almon
weighting scheme is mathematically speciﬁed, let us consider the case that opti-
mal lagged high frequency variable is 3 (or K = 3), the ratio between high and
low variables is 3 (or m = 3), no lagged dependent variable, and forecasting for
one step ahead (h = 1). The model then can be written as follows
Yt = α + γ
⎛
⎜
⎜
⎜
⎜
⎜
⎝
exp(θ1+θ2)
3
k=1 exp(kθ1+k2θ2)

X(3)
t−1

+
exp(2θ1+4θ2)
3
k=1 exp(kθ1+k2θ2)

X(3)
t−1−( 1
3)

+
exp(3θ1+9θ2)
3
k=1 exp(kθ1+k2θ2)

X(3)
t−1−( 2
3)

⎞
⎟
⎟
⎟
⎟
⎟
⎠
+ εt.
(10)
Suppose that Ytis measured at the 4th quarter of 2017, then X(3)
t−1 is from
September 2017, X(3)
t−1−( 1
3) is from August 2017 and X(3)
t−1−( 2
3) is from July 2017.
α, γ, θ1, and θ2 can be estimated by using either maximum likelihood approach
or non-linear least squares (NLS) approach. Ghysels et al. [10] pointed out that
the number of parameters in the MIDAS model with exponential Almon weight
is not inﬂuenced by the number of lagged high frequency variables. This impor-
tant feature of MIDAS regression model allows us to employ large lagged high
frequency variables and, at the same time, maintain parsimonious parameter
estimation [1,12].
3.3.3
Beta Weight
It is another weighting scheme, which is an analogue of probability density func-
tion. It has been considered in empirical works as alternative to the exponential
Almon weight [11]. According to Armesto et al. [1], this weighting scheme can
be speciﬁed as follows
ω (k; θ) =
f
 k
K , θ1, θ2

K
k=1 f
 k
K , θ1, θ2
,
(11)
where
f (x, a, b) = xa−1(1 −x)b−1Γ (a + b)
Γ (a) Γ (b)
,
(12)
and
Γ (a) =
 ∞
0
e−xxa−1 dx
(13)
is the gamma function. θ1 and θ2 are parameters that control the weighing value
for each lagged high preference variable.

Does Forecasting Beneﬁt from Mixed-Frequency Data Sampling Model
437
3.4
Unrestricted MIDAS (U-MIDAS) Model
It can be noticed MIDAS models with weighting schemes may not completely
extract all information from high frequency variable [14] since it still involves a
frequency transformation. Kingnetr et al. [14] further asserted that the forecast-
ing outcome may be satisfactory in the MIDAS model with exponential Almon
weight framework when the diﬀerence in sampling frequencies between variables
in the study is relatively small. Additionally, the model requires assumption on
weighting scheme which may or may not be appropriate for every series. The
MIDAS with exponential Almon may work well with one series, but not another.
Foroni and Marcellino [8] suggested an alternative approach, the unrestricted
MIDAS (U-MIDAS) regression model, to deal with the issue.
The basic idea of U-MIDAS model is similar to the MIDAS with step weight-
ing scheme, except that the coeﬃcient of each lagged high frequency variable is
allowed to diﬀer. Suppose that low frequency data is measured quarterly, while
the high frequency is measured monthly, the U-MIDAS model for h-step fore-
casting can be written as
Yt = α +
p

i=1
βiYt−h−i +
K

k=1
γkX(m)
t−h−(k−1)/m + εt.
(14)
Yt is a quarterly variable at period t, X(m)
t−h−(k−1)/m is a monthly indicator mea-
sured at k −1 months prior to the last month of the quarter at period t −h, h
is the forecasting step, m is a frequency ratio, K is a number of monthly data
used to predict Yt.
By taking each lagged high frequency variable as additional explanatory vari-
able in the model, the parameters in U-MIDAS model can simply be estimated
using OLS estimation [8]. However, the U-MIDAS will lose its parsimonious fea-
ture if the number of frequency ratio between high and low frequency variables is
large. For instance, forecasting monthly series using daily series will involve more
than 20 parameters to be estimated, which would lead to undesirable estimation
and forecasting results.
4
Empirical Results
In this section, we begin with the results of unit root tests, followed by the
results from each forecasting model considered in the study and discussion on
their forecasting performances.
The results of unit root test are reported in Table 1. In the case of GDP
growth, the null hypothesis of non-stationary cannot be rejected in the case
of ADF test without intercept (speciﬁcation C). However, the rest of the tests
show that GDP growth is stationary. Similarly, all tests, except for ADF with
trend and intercept, conclude that SET growth is stationary. Therefore, it is
reasonable to conclude that both series are stationary and can be undergone
model estimation and forecasting.

438
N. Kingnetr et al.
Table 1. Unit root tests
Test
Speciﬁcation
Conclusion
A: (Intercept) B: (Trend and Intercept) C: (None)
Panel I: GDP growth
ADF
0.000*
0.000*
0.286
Stationary
PP
0.003*
0.007*
0.028**
KPSS 0.302
0.040
–
Panel II: SET growth
ADF
0.062***
0.166
0.044**
Stationary
PP
0.005*
0.023**
0.002*
KPSS 0.087
0.058
–
Note:
1. The null hypothesis of ADF and PP unit root tests is non-stationary,
while the KPSS is stationary.
2. For ADF and PP tests, the number represents p-value.
3. For KPSS test, the number represents the test statistics.
4. The critical values for KPSS test for speciﬁcation A (B) at 1%, 5%, and
10% are 0.739 (0.216), 0.463 (0.146), and 0.347 (0.119), respectively.
5. *, **, *** denote the rejection of null hypothesis at 1%, 5%, and 10%
levels of statistical signiﬁcance, respectively.
As far as model estimation is concerned, the data sample during the period
of 2001Q1 to 2015Q4 is employed. The linear least squares estimation technique
is used to estimate parameters for the time-aggregate model, while parameters
in the MIDAS models are handled by the non-linear least squares. The optimal
lag lengths for all models are chosen by Akaike information criterion (AIC) with
the maximum of 24 lags. Then, we forecast the quarterly GDP growth rates
for 2016. Since we are interested in comparing forecasting performance between
models, it is advised to investigate how these models perform through ﬁgure.
Figure 3 provides a plot of actual value of quarterly GDP growth rate and its
forecasted values from diﬀerent models.
It can be seen from Fig. 3 that the time-aggregate model, MIDAS model
with Beta weighting, and U-MIDAS model seem to predict the GDP growth
rate closer to the actual values than the MIDAS models with exponential Almon
and step weighting schemes. However, as the forecasting horizon expands, the
former three models seem to perform worse than the latter two. Table 2 provides
forecasting results in details together with lag selection for each model.
It is possible to notice that it is still uncertain to see which model can gen-
erally perform better. Therefore, we now turn to the root mean square error
(RMSE) for evaluation. Table 3 shows the RMSEs for each model at each fore-
casting horizon.
The results from Table 3 suggest that, overall, the unrestricted MIDAS (U-
MIDAS) model exhibits higher forecasting accuracy than the rest of the models

Does Forecasting Beneﬁt from Mixed-Frequency Data Sampling Model
439
Fig. 3. Forecast and actual quarterly GDP growth in 2016
Table 2. Quarterly GDP growth forecast in 2016
Period
Model
Actual value
A
B
C
D
E
2016Q1
2.720 2.199 2.295 2.742 2.789 3.125
2016Q2
3.152 2.711 2.792 3.324 3.329 3.637
2016Q3
4.009 3.026 3.104 3.515 3.476 3.224
2016Q4
4.457 3.521 3.565 4.449 4.408 2.960
Lag selection 2
4
3
4
4
Note: The description for each model is as follows, (A) Time-
aggregate model, (B) MIDAS model with step weighting, (C)
MIDAS model with exponential Almon weighting, and (D)
MIDAS model with Beta weighting, and (E) U-MIDAS model.
The number is rounded to nearest thousandth.
in this study. The conclusion here is also consistent with the recent empirical
work by [14]. In addition, the superior in forecasting precision may due to the
fact that, in U-MIDAS framework, information of high frequency variable is fully
utilised. The results also suggest that the forecasting improvement is rather mod-
erate, when it comes to the comparison between MIDAS models with weighting
schemes and the traditional time-aggregate model. According to RMSEs, we can
see that only the MIDAS model with beta weighting scheme can outperform the
time-aggregate model in this study. This implies that using higher frequency will
not improve the outcome after all if the inappropriate weighting scheme is cho-
sen. Nevertheless, we can conclude that using high-frequency variable to predict
the lower frequency one improves forecasting precision under U-MIDAS model,
provided that the diﬀerence in frequency between series in a study is small.

440
N. Kingnetr et al.
Table 3. Forecast evaluation based on RMSEs
Model Horizon
1
2
3
4
A
0.405
0.446
0.581
0.902
B
0.926
0.926
0.764
0.719
C
0.830
0.837
0.687
0.668
D
0.382
0.349
0.331
0.798
E
0.335 0.322 0.300 0.769
Note: The description for each model
is
as
follows,
(A)
Time-aggregate
model, (B) MIDAS model with step
weighting, (C) MIDAS model with
exponential Almon weighting, and (D)
MIDAS model with Beta weighting,
and (E) U-MIDAS model. The num-
ber is rounded to nearest thousandth.
At each horizon, the lowest RMSEs are
in bold.
5
Conclusion
In this paper, we investigate the forecasting performance of 5 diﬀerent forecast-
ing models, including the time-aggregate model, the MIDAS model with step
weighting, exponential Almon weighting, and beta weighting, and the U-MIDAS
model. Thailand’s quarterly GDP growth was forecasted using a ﬁnancial vari-
able, SET index, as a predictor. Unlike the time-aggregate model, the MIDAS
model with weighting scheme allows us to eﬃciently utilise the information of
high frequency variable to forecast lower frequency variable. However, it still
involves the concept of frequency conversion, as in the time-aggregate model,
via weighting schemes.
On the other hand, the U-MIDAS model fully exhausts information of high
frequency variable. The model directly incorporates high frequency variable into
forecasting model without frequency conversion. The data in this study spans
from 2001Q1 to 2016Q4 with 2016Q1 to 2016Q4 being left out for forecasting
performance evaluation. Our results, based on RMSEs, show that the U-MIDAS
model has greater forecasting precision than other models in this study. This
implies that, under the U-MIDAS framework, using high frequency variable to
predict lower frequency variable improves the forecasting accuracy.
In addition, we found that the improvement of using higher frequency variable
to predict lower frequency variable is rather small when it comes to the MIDAS
model with weighting scheme. The forecasting results may even be worst if the
weighting scheme is not appropriately chosen. If one wishes to employ such MIDAS
model, the results suggest that the MIDAS model with Beta weighting scheme

Does Forecasting Beneﬁt from Mixed-Frequency Data Sampling Model
441
performs best among other weighting schemes. Otherwise, the traditional time-
aggregate seems to provide acceptable predicting accuracy for short-horizon.
Nevertheless, this study focused on four-period forecasting using single pre-
dictor and ignored the possibility of having structural break in time series due to
the limitation of approaches. Therefore, the recommendation for future research
would be the inclusion of additional predictors in the model, longer forecasting
horizon and controlling for potential structural breaks.
Acknowledgements. The authors would like to thank the anonymous reviewer for
useful suggestions which have greatly improved the quality of this paper. This research
is supported by the Puay Ungphakorn Center of Excellence in Econometrics, Chiang
Mai University.
References
1. Armesto, M.T., Engemann, K., Owyang, M.: Forecasting with mixed frequencies.
Review 92, 521–536 (2010)
2. Asteriou, D., Hall, S.G.: Applied Econometrics, 2nd edn. Palgrave Macmillan,
Leicester (2011)
3. Bell´ego, C., Ferrara, L.: Forecasting Euro-area recessions using time-varying binary
response models for ﬁnancial variables. Working papers 259, Banque de France
(2009)
4. Clements, M.P., Galv˜ao, A.B.: Macroeconomic forecasting with mixed-frequency
data. J. Bus. Econ. Stat. 26(4), 546–554 (2008)
5. Clements, M.P., Galv˜ao, A.B.: Forecasting US output growth using leading indi-
cators: an appraisal using MIDAS models. J. Appl. Econ. 24(7), 1187–1206 (2009)
6. Estrella, A., Rodrigues, A.R., Schich, S.: How stable is the predictive power of the
yield curve? Evidence from germany and the united states. Rev. Econ. Stat. 85(3),
629–644 (2003)
7. Ferrara, L., Marsilli, C.: Financial variables as leading indicators of GDP growth:
Evidence from a MIDAS approach during the Great Recession. Appl. Econ. Lett.
20(3), 233–237 (2013)
8. Foroni, C., Marcellino, M.: A survey of econometric methods for mixed-frequency
data. Working Paper 2013/06, Norges Bank (2013)
9. Ghysels, E., Kvedaras, V., Zemlys, V.: Mixed frequency data sampling regression
models: the R package midasr. J. Stat. Softw. Art. 72(4), 1–35 (2016)
10. Ghysels, E., Santa-Clara, P., Valkanov, R.: The MIDAS Touch: Mixed Data Sam-
pling Regression Models. CIRANO Working Papers 2004s–20, CIRANO (2004)
11. Ghysels, E., Santa-Clara, P., Valkanov, R.: There is a risk-return trade-oﬀafter
all. J. Financ. Econ. 76(3), 509–548 (2005)
12. Ghysels, E., Santa-Clara, P., Valkanov, R.: Predicting volatility: getting the most
out of return data sampled at diﬀerent frequencies. J. Econ. 131(1–2), 59–95 (2006)
13. Ghysels, E., Valkanov, R.I., Serrano, A.R.: Multi-period forecasts of volatility:
direct, iterated, and mixed-data approaches. EFA 2009 Bergen Meetings Paper
(2009)
14. Kingnetr, N., Tungtrakul, T., Sriboonchitta, S.: Forecasting GDP Growth in
Thailand with Diﬀerent Leading Indicators Using MIDAS Regression Models, pp.
511–521. Springer International Publishing, Cham (2017)

442
N. Kingnetr et al.
15. Kuzin, V., Marcellino, M., Schumacher, C.: MIDAS vs. mixed-frequency VAR:
nowcasting GDP in the euro area. Int. J. Forecast. 27(2), 529–542 (2011)
16. Kwiatkowski, D., Phillips, P.C., Schmidt, P., Shin, Y.: Testing the null hypothesis
of stationarity against the alternative of a unit root. J. Econ. 54(1), 159–178 (1992)
17. Phillips, P.C.B., Perron, P.: Testing for a unit root in time series regression. Bio-
metrika 75(2), 335 (1988)
18. Said, S.E., Dickey, D.A.: Testing for unit roots in autoregressive-moving average
models of unknown order. Biometrika 71(3), 599–607 (1984)
19. Tungtrakul, T., Kingnetr, N., Sriboonchitta, S.: An Empirical Conﬁrmation of the
Superior Performance of MIDAS over ARIMAX, pp. 601–611. Springer Interna-
tional Publishing, Cham (2016)

A Portfolio Optimization Between US Dollar
Index and Some Asian Currencies
with a Copula-EGARCH Approach
Ji Ma1,2, Jianxu Liu1,3(B), and Songsak Sriboonchitta1,3
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
majiyn@hotmail.com, liujianxu1984@163.com, songsakecon@gmail.com
2 Yunnan Academy of Social Sciences, Kunming 650031, China
3 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. There is a strong correlation between the value of the US dol-
lar and the Asian currencies. EGARCH-copula model, with the skewed
student-t distribution and the skewed general error distribution, can be
used to capture the dependence correlation between US dollar and an
Asian currency from those seven currencies in this paper. Building a
bivariate portfolio based on the ﬁtted EGARCH-copula models can be
used to make portfolio optimization with the methods of max return, min
risk and max sharpe ratio, to obtain a positive and reasonable return.
Keywords: Exchange rate · Copula-EGARCH · Portfolio optimization
1
Introduction
Markowitz (1952) propose the mean-variance model to construct the optimal
portfolio which build up the foundation of the modern investment theory. At a
given conﬁdence level, Baumol (1963) create the concept of Value-at-Risk (VaR)
to analyze the worst loss. Copula-GARCH approach can be used to analyze the
conditional dependence structure for two correlated variables (Aloui et al. 2013;
Sun et al. 2008; Huang et al. 2009; Patton 2012; Sriboonchitta et al. 2013; Wu and
Lin 2014; Chen 2015). This paper uses a EGARCH-copulas to study the portfolio
optimization between the US dollar (USD) index and seven major Asian curren-
cies exchange rate. Aloui and Assa (2016) exchange rate corresponds to the trade
weighted US dollar (TWEXB) index, measuring the movement of dollar against
the currencies of a broad group of major U.S. trading partners. Therefore, in this
paper we assume TWEXB as a tradable currencies, a ﬁnancial asset, with its value
as a price in US dollar to measure the value of Dollar. Daily logarithm returns
for the eight assets have been used for EGARCH(1,1) with residuals of skewed
student-t distribution or skewed general error distribution. Forty copula models
has been estimated for model selection to conduct the parameter estimation and
standard error with robustness of the most adequate model.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_32

444
J. Ma et al.
We used the daily data from January, 4, 2006 to December, 31, 2015 and the
empirical results for daily data provided evidence of the dependence structure
between US dollar and Asian currencies. Based on GARCH-copula model, VaR
and optimization of portfolios can be estimated (Lee and Lin 2011; Wang et al.
2010). With the estimation and forecasting for eight assets and it’s seven pair
portfolio with EGARCH-copula models, the portfolio optimization of the seven
portfolios made the positive and reasonable return.
We introduce the methodology in the following section. Then we describe
the data and descriptive statistics. After showing the empirical results, the last
section make the conclusion.
2
Methodology
We ﬁrst ﬁt skewed EGARCH models for univariate time series for obtaining
the marginal distribution of the residuals. Then, we estimation the dependence
structure for seven pairs assets with six copula functions (Gaussian, Student-t,
Clayton, Frank, Gumbel and Joe copulas, see Necula 2010; McNeil 2015; Yan
2007; Kole et al. 2007; Wiboonpongse et al. 2015). Finally, portfolio optimization
methods, such like the maximum mean, the minimum risk, the maximum Sharpe
ratio, have been used to deal with the optimal portfolio allocation.
2.1
EGARCH with Skewed Distributions
An EGARCH(1,1) model was taken in this research We take the log return of
assets as {Xt}, t = 1, ..., T where μ is the expected return and ϵt is a zero-mean
white noise,
Xt = μ + ϵt,
(1)
when the series ϵt is not serially independent, we can use a the exponential
generalized autoregressive conditional heteroskedastic model (ϵt-EGARCH) by
Nelson (1991) as:
ϵt = σtηt,
(2)
where ηt is standard skew general error distribution (sged) or standard skew
student-t distribution (sstd) and
log(σ2
t ) = ω + α (|ηt−1| −E |ηt−1|) + γηt−1 + βlog(σ2
t−1),
(3)
where ηt ∼fsged or ηt ∼fsstd.
2.2
Copula and Skalar Theorem
Sklar’s theorem (1959) which is the general idea of copula states that for a given
joint multivariate distribution function there exist a function–known as copula
function–such that
FXY (x, y) = C(FX(x), FY (y)),
(4)

A Portfolio Optimization
445
where FXY (x, y) is the joint distribution of X and Y , u = FX(x) and v = FY (y).
If C is a copula function, then the function FXY is a joint distribution function
with margins FX and FY . The conditional copula function (Patton 2006) can be
expressed as
FXY |W (x, y|w) = C(FX|W (x|w), FY |W (y|w)|w),
(5)
where W is the conditioning variable, FX|W (x|w) is the conditional distribu-
tion of X|W = w, FY |W (y|w) is the conditional distribution of Y |W = w and
FXY |W (x, y|w) is the conditional distribution of (X, Y )|W = w.
2.3
Bivariate Copula Models
2.3.1
The Bivariate Gauss Copula
This bivariate Gaussian copula is given by
CGa (u1, u2, ρ) =
 φ−1(u1)
−∞
 φ−1(u2)
−∞
1
2π

(1 −ρ2)
exp

−v2
1 −2ρv1v2 + v2
2
2 (1 −ρ2)

dv2dv1,
(6)
ρ denotes the correlation of u1 and u2.
2.3.2
The Bivariate Student t-Copula
The bivariate student-t copula is the following function:
Ct (u1, u2, ρ, v) =
 t−1
v
(u1)
−∞
 t−1
v
(u2)
−∞
1
2π

(1 −ρ2)
exp

1 + r2 −2ρrs + s2
v(1 −ρ2)
−v+2
2
dr ds,
(7)
where ρ is the linear correlation coeﬃcient between the two random variables
and t−1
v
denotes the inverse of the univariate Student-t distribution function
with degrees of freedom.
2.3.3
The Bivariate Clayton Copula
The bivariate Clayton copula is given as following:
CClayton(u1, u2; ω) =

u1
−ω + u2
−ω −1
	−1
ω ,
(8)
where ω ∈[−1, ∞).

446
J. Ma et al.
2.3.4
The Bivariate Frank Copula
The function of the bivariate frank copula is:
CF rank (u1, u2; λ) = −1
λ log

λ

1 −e−λ	
−

1 −e−λu1	 
1 −e−λu2	
1 −e−λ

,
(9)
where λ ∈(−∞, 0) ∪(0, +∞).
2.3.5
The Bivariate Gumbel Copula
The bivariate gumbel copula is given as below:
CGumbel(u1, u2; δ) = exp

−

−logu1
	δ +

−logu2
	δ 1
δ 
,
(10)
where δ ∈[1, ∞).
2.3.6
The Bivariate Joe Copula
The function of the bivariate Joe copula is given as:
CJoe(u, v; θ) = 1 −

(1 −u)θ + (1 −v)θ −(1 −u)θ(1 −v)θ 1
θ ,
(11)
where θ ∈[1, ∞).
2.4
Portfolio Optimization
Suppose there are N risky assets whose returns are given by the random vari-
ables R1, ...Rn, RN. There are three methods based on portfolio optimization are
imposed in terms of the maximum mean, the minimum risk, and the maximum
Sharpe ratio.
2.4.1
Maximum Return
Then the maximize return mean-variance portfolio can be described as
max
RT w,
(12)
wi ≥0, i = 1, 2, ..., n, subject to n
t=1 wi = 1, and wT Σw ≤σ2. where wi is
deﬁned as the vector of portfolio weight of asset i and restricted to be positive.
RT w is the expected return of a portfolio. Σ is deﬁned as the variance-covariance
matrix. Therefore, the term wT Σw represents the variance of the portfolio return
which a variance of an eﬃcient portfolio must not exceed the variance of an
individual asset.

A Portfolio Optimization
447
2.4.2
Minimum Risk
The method of the minimum risk, or variance, can be expressed as below:
min
σ2
p = wT Σw
s.t.
RT w ≥R,
(13)
where n
t=1 wi = 1, (wi ≥0, i = 1, 2, ..., n) and Σ is the covariance matrix and
the R is the given level of return.
2.4.3
Maximum Sharpe Ratio
The maximum Sharpe ratio of a given asset is deﬁned by the expected excess
return of an asset over the risk-free rate to its standard deviation as shown below:
max
Sa = E (Ra) −Rf
σa
,
(14)
where Sa denotes the Sharpe ratio of portfolio (a) and E (Ra) = RT w, and w
is the weight of a portfolio. Ra and Rf are returns of portfolio (a) and risk-free
asset respectively. In this study, Rf is assumed to be zero. Standard deviation
of portfolio (a) deﬁnes by σa.
3
Data and Descriptive Statistics
This study uses eight currencies of US, China, Japan, Korea, India, Thailand,
Malaysia, and Singapore. Those currencies are represented by TWEXB (Trade
Weighted US Dollar Index), CNY, JPY, KRW, INR, THB, MYR, and SGD,
respectively. Remarkably, TWEXB was used as an indicator to measure the
value of US dollar in terms of a basket of major foreign currencies excluding
US dollar. Therefore, in this paper we assume TWEXB as a tradable currency
representing US dollar. We built seven pairs of portfolio for each Asian currencies
with TWEXB. The data derives from the Federal Reserve Economic Data at the
Federal Reserve Bank of S.T. Louis, the United States. The range of data are
from January, 2, 2006 to December, 31, 2015 with a daily series. Log returns
series has been calculated for ﬁtting the model with totally 2512 daily returns
for eight assets, which 2011 returns are for estimation and another 501 returns
are for forecasting. The returns of eight assets are shown in Fig. 1.
The descriptive statistics for TWEXB and seven Asian currencies are
reported in Table 1, which shows that the standard deviation of sex of seven
Asian currencies, except CNY, are higher than that of TWEXB returns which
shows that Asian currencies generally has higher volatilities. The skewness sta-
tistic for currencies of US, China, Japan, Korea, and Malaysia are negative,
indicating the returns are signiﬁcantly skewed to left, and for those of India,
Thailand, Singapore are positive, indicating the returns skewed to right. Signif-
icant, thereby indicating that the oil returns are signiﬁcantly skewed to the left.

448
J. Ma et al.
Fig. 1. Daily returns on eight currencies
Table 1. Descriptive statistics and stochastic properties of return series.
Minimum Maximum Mean
Stdev. Skewness Kurtosis
TWEXB −0.023
0.017
0.000
0.003
−0.067
4.61
CNY
−0.01
0.01
−0.0001 0.001
−0.063
15.813
JPY
−0.052
0.033
−0.0001 0.007
−0.304
4.928
KRW
−0.132
0.101
0.000
0.009
−0.663
44.649
INR
−0.038
0.039
0.0002 0.006
0.145
6.308
THB
−0.035
0.045
−0.0001 0.005
0.041
11.776
MYR
−0.027
0.017
−0.0001 0.004
−0.234
2.819
SGD
−0.022
0.027
−0.0001 0.004
0.265
5.095
With respect to the excess kurtosis statistics, the values all currencies, except
MYR with slightly negative, are signiﬁcantly positive (great than 3), thereby
implying that the distribution of returns has larger, thicker tails than the normal
distribution. Based on the statistics, we could assume the we should use skewed
distribution on the residual for the single series to ﬁt an EGARCH model.

A Portfolio Optimization
449
4
Empirical Results
We use eight return series separately to ﬁt the EGARCH(1,1) model with either
SSTD and SGED distribution for the residuals and use AIC (Akaike information
criterion) and BIC (Bayesian information criterion) to select the best model.
The empirical result for both AIC and BIC tend to give the same suggestion
on SSTD distribution for TWEXB, JPY, INR and SGD, also give the same
advices on SGED distribution for CNY, KRW, THB, MYR. The detail showed
in Table 2.
Using the ﬁtted EGARCH(1,1) for each of the eight assets, the estimation
for the parameters of the eight EGARCH models with robustness. We use the
eight best ﬁtted EGARCH models to ﬁgure out the residuals with dependence
between TWEXB with each of the other seven Asian currencies results are listed
in Table 3.
This research use 40 copulas to ﬁt seven EGARCH-copula models, like Nor-
mal, Student-t, Clayton, Gumbel, Frank and Joe copulas, for seven pairs. Here
we only show six copulas of the total 40. The estimated parameters of the corre-
lation coeﬃcient and its standard deviation for each adequate copula shows the
statistically signiﬁcant level for all parameters for the seven copula models. The
details are showed in Table 4.
We use the goodness-of-ﬁt test for the 40 copulas and the AIC shows that
the pair of TWEXB/CNY can be best ﬁtted by a Gaussian copula, the rest of
the other six can be best ﬁtted with a Student-t copula that shown in Table 5.
Seven pairs of the portfolio has been calculated for the optimization with
three portfolio choice methods. The all three methods, including the max return,
the min risk, the max Sharpe ratio, have been used for the portfolio optimization.
Table 2. Selecting eGARCH(1,1) model with SSTD and SGED by using AIC and BIC
Assets
AIC
BIC
SSTD
SGED
SSTD
SGED
TWEXB
−8.8177a
−8.8158
−8.8010a
−8.7991
CNY
−11.499
−11.499a
−11.482
−11.483a
JPY
−7.3071a
−7.3014
−7.2903a
−7.2847
KRW
−7.6434
−7.6485a
−7.6266
−7.6318a
INR
−7.7436a
−7.7294
−7.7269a
−7.7127
THB
−8.5665
−8.5675a
−8.5498
−8.5508a
MYR
−8.3783
−8.3856a
−8.3616
−8.3688a
SGD
−8.6568a
−8.6483
−8.6401a
−8.6316
Notes: EGARCH(1,1) with skewed student t distribution and
SGED distribution. aRepresents the chosen distribution.

450
J. Ma et al.
Table 3. Parameter estimation with robustness for EGARCH(1,1) with skewed distri-
butions
Pars
TWEXB
CNY
JPY
KRW
Dist
SSTD
SGED
SSTD
SGED
omega
−0.086*** −0.251*** −0.147***
−0.102***
0.011
0.017
0.023
0.007
alpha
0.030**
0.013
−0.024
0.068***
0.011
0.022
0.015
0.016
beta1
0.993***
0.982***
0.985***
0.990***
0.001
0.001
0.002
0.0005
gamma1
0.107***
0.344***
0.11
0.156***
0.022
0.011
0.087
0.006
skew
1.042***
1.000***
0.986***
1.053***
0.03
0.001
0.026
0.024
shape
8.172
0.848***
5.781*
1.209***
4.363
0.058
2.574
0.052
Pars
INR
THB
MYR
SGB
Dist
SSTD
SGED
SGED
SSTD
omega
−0.240*** −0.284*** −0.317***
−0.112***
0.02
0.023
0.035
0.011
alpha
0.035***
0.013*
0.024
0.035**
0.016
0.016
0.017
0.012
beta1
0.977***
0.975***
0.971***
0.990***
0.002
0.002
0.003
0.001
gamma1
0.221***
0.299***
0.264***
0.127***
0.01
0.023
0.027
0.03
skew
1.047***
1.000***
0.999995***
1.096***
0.026
0.002
0.003
0.034
shape
4.233***
1.095***
1.151***
6.021***
0.457
0.058
0.067
1.008
Notes: Signiﬁcant level ***0.1%, **1%, *5%.
As we seen in Table 6, the results show that all three methods produce positive
and reasonable return for two years. Among the three methods, the max Sharpe
ratio make the best performance for US/CNY and US/JPY with 1.160 and 1.193,
on another hand, max return works the best for other ﬁve pair of portfolios such
like US/KRW, US/INR, US/THB, US/MYR and US/SGD with 1.210, 1.212,
1.158, 1.259 and 1.198.

A Portfolio Optimization
451
Table 4. Estimates of the dependence parameters of diﬀerent copula models
Normal
Student-t
Clayton
Gumbel
Frank
Joe
TWEXB/CNY
0.243***
0.243***
0
0.219***
1.164***
1.441***
1.210***
−0.021
−0.021
0
−0.029
−0.019
−0.135
−0.029
TWEXB/JPY
0.211***
0.221***
4.546***
0.258***
1.158***
1.350***
1.194***
−0.021
−0.024
−0.563
−0.032
−0.019
−0.141
−0.027
TWEXB/KRW
0.501***
0.503***
13.046***
0.709***
1.443***
3.363***
1.551***
−0.015
−0.016
−4.049
−0.039
−0.025
−0.147
−0.036
TWEXB/INR
0.462***
0.466***
14.597***
0.584***
1.399***
3.108***
1.512***
−0.016
−0.017
−4.874
−0.037
−0.024
−0.145
−0.035
TWEXB/THB
0.496***
0.498***
16.072***
0.625***
1.457***
3.311***
1.605***
−0.015
−0.016
−5.829
−0.037
−0.025
−0.146
−0.037
TWEXB/MYR
0.468***
0.479***
7.072***
0.640***
1.432***
3.238***
1.545***
−0.016
−0.018
−1.294
−0.038
−0.025
−0.148
−0.037
TWEXB/SGD
0.777***
0.782***
7.776***
1.672***
2.215***
7.269***
2.576***
−0.007
−0.008
−1.465
−0.056
−0.041
−0.191
−0.058
Notes: Signiﬁcant level ***0.1%, **1%, *5%.
Table 5. Results for the goodness-of-ﬁt test of diﬀerent copula functions
Normal Std-t
Clayton Gumbel Frank
Joe
TWEXB/CNY
−119
−
−74
−93
−113
−62
TWEXB/JPY
−89
−175
−86
−110
−90
−84
TWEXB/KRW −577
−590
−453
−519
−535
−384
TWEXB/INR
−478
−490
−336
−449
−465
−347
TWEXB/THB
−562
−571
−395
−533
−523
−419
TWEXB/MYR −493
−534
−412
−468
−490
−337
TWEXB/SGD
−1856
−1903 −1450
−1775
−1744 −1401
Notes: t copula is a symmetrical and heavy tail distribution. All pairs,
except TWEXB/CNY, has a tail dependence and symmetrical relation.
Table 6. Portfolio optimization via vary approaches
Portfolio
Max mean Min risk Sharpe ratio
US/CNY
1.077
1.136
1.160
US/JPY
1.180
1.171
1.193
US/KRW 1.210
1.152
1.150
US/INR
1.212
1.134
1.096
US/THB
1.158
1.153
1.125
US/MYR 1.259
1.253
1.166
US/SGD
1.198
1.161
1.128

452
J. Ma et al.
5
Conclusion
In this paper, we introduced EGARCH model for skewed residuals for US dollar
and seven Asian currencies. Then the bivariate EGARCH-copulas was used to
make portfolios. To maximize the portfolio with four methods, the maximum
portfolio found out a reasonable return from 1.16 to 1.259 for 512 trading days;
The Gaussian copula and the student-t copula were selected by seven pairs of
portfolio, respectively. Moreover, all four portfolio optimization methods ﬁgure
out the positive returns. Among the three portfolio optimization methods, the
max return and the max Sharpe ratio made better performance.
Acknowledgements. This work has been supported by the Faculty of Economics and
the Puey Ungphakorn Centre of Excellence in Econometrics at Chiang Mai University.
References
Aloui, R., Ben Aissa, M.S., Nguyen, D.K.: Conditional dependence structure between
oil prices and exchange rates: a copula-GARCH approach. J. Int. Money Fin. 32,
719–738 (2013). https://doi.org/10.1016/j.jimonﬁn.2012.06.006
Aloui, R., Assa, M.S.B.: Relationship between oil, stock prices and exchange rates: a
vine copula based GARCH method. North Am. J. Econ. Fin. 37, 458–471 (2016)
Baumol, W.J.: An expected gain-conﬁdence limit criterion for portfolio selection.
Manag. Sci. 10(1), 174–182 (1963)
Chen, Q.A., Wang, D., Pan, M.Y.: Multivariate time-varying G-H copula GARCH
model and its application in the ﬁnancial market risk measurement. Math. Prob.
Eng. (2015). https://doi.org/10.1155/2015/286014
Huang, J.J., Lee, K.J., Liang, H.M., Lin, W.F.: Estimating value at risk of portfolio
by conditional copula-GARCH method. Insur. Math. Econ. 45(3), 315–324 (2009).
https://doi.org/10.1016/j.insmatheco.2009.09.009
Jondeau, E., Rockinger, M.: The copula-GARCH model of conditional dependencies:
an international stock market application. J. Int. Money Fin. 25(5), 827–853 (2006).
https://doi.org/10.1016/j.jimonﬁn.2006.04.007
Kole, E., Koedijk, K., Verbeek, M.: Selecting copulas for risk management. J. Bank.
Fin. 31(8), 2405–2423 (2007). https://doi.org/10.1016/j.jbankﬁn.2006.09.010
Lee, W.C., Lin, H.N.: Portfolio value at risk with Copula-ARMAX-GJR-GARCH
model: evidence from the gold and silver futures. Afr. J. Bus. Manag. 5(5), 1650–1662
(2011)
Markowitz, H.: Portfolio selection. J. Fin. 7(1), 77–91 (1952)
McNeil, A.J.: Dependence modeling with copulas. J. Time Ser. Anal. 36(4), 599–600
(2015). https://doi.org/10.1111/jtsa.12126
Necula, C.: A Copula-GARCH model. Ekonomska Istrazivanja-Economic Research
23(2), 1–10 (2010)
Nelson, D.B.: Conditional heteroskedasticity in asset returns: a new approach. Econo-
metrica 59(2), 347–270 (1991)
Patton, A.J.: A review of copula models for economic time series. J. Multivar. Anal.
110, 4–18 (2012). https://doi.org/10.1016/j.jmva.2012.02.021
Sriboonchitta, S., Nguyen, H.T., Wiboonpongse, A., Liu, J.X.: Modeling volatility and
dependency of agricultural price and production indices of Thailand: static versus
time-varying copulas. Int. J. Approximate Reasoning 54(6), 793–808 (2013). https://
doi.org/10.1016/j.ijar.2013.01.004

A Portfolio Optimization
453
Sun, J.F., Frees, E.W., Rosenberg, M.A.: Heavy-tailed longitudinal data modeling
using copulas. Insur. Math. Econ. 42(2), 817–830 (2008). https://doi.org/10.1016/
j.insmatheco.2007.09.009
Wang, Z.R., Chen, X.H., Jin, Y.B., Zhou, Y.J.: Estimating risk of foreign exchange
portfolio: using VaR and CVaR based on GARCH-EVT-Copula model. Phys. A
Stat. Mech. Appl. 389(21), 4918–4928 (2010). https://doi.org/10.1016/j.physa.2010.
07.012
Wiboonpongse, A., Liu, J.X., Sriboonchitta, S., Denoeux, T.: Modeling dependence
between error components of the stochastic frontier model using copula: application
to intercrop coﬀee production in Northern Thailand. Int. J. Approximate Reasoning
65, 34–44 (2015). https://doi.org/10.1016/j.ijar.2015.04.001
Wu, C.C., Lin, Z.Y.: An economic evaluation of stock-bond return comovements with
copula-based GARCH models. Quant. Fin. 14(7), 1283–1296 (2014). https://doi.
org/10.1080/14697688.2012.727213
Yan, J.: Enjoy the joy of copulas: with a package copula. J. Stat. Softw. 21(4), 1–21
(2007)

Technical Eﬃciency Analysis of China’s
Agricultural Industry: A Stochastic Frontier
Model with Panel Data
Ji Ma1,2, Jianxu Liu1,3(B), and Songsak Sriboonchitta1,3
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
majiyn@hotmail.com, liujianxu1984@163.com, songsakecon@gmail.com
2 Yunnan Academy of Social Sciences, Kunming 650031, China
3 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. This paper imposed the translog stochastic frontier produc-
tion model to analyze the China’s province-level agriculture productiv-
ity by using panel data during 2002–2012 on 31 provinces in China.
The results show that China’s province-level agriculture productivity has
been improved for over 11 years. Hunan, Bejing and Shanghai approached
the agriculture technical eﬃciency frontier. The agriculture technical eﬃ-
ciencies in underdeveloped area such like Guizhou, Yunnan and Anhui
increased sharply and approached to the national province-level mean,
60%, in terms of the technical eﬃciencies over 11 years which, however,
still have 40% space to be improved. We recommend that the provinces
with lower technical eﬃciency, such as Anhui, Yunnan and Guizhou,
should learn experiences from those provinces that have high technical
eﬃciency so that improving the agricultural productivities of themselves.
Keywords: Stochastic frontier analysis · Translog production function
Technical eﬃciency · Agriculture productivity · China
1
Introduction
Under a national urbanization campaign, it is the ﬁrst time that the proportion
of rural population is less than 50% at the end of 2011 in China according to
a government announcement in 2012. By going with a decrease in rural pop-
ulation, however, China’s agriculture output has been increased for decade to
feed a giant economic body with the most population in the world. It is the
agriculture productivity that plays signiﬁcant role. A large number of research
papers focused on the development of China’s agriculture productivity. Tong
et al. (2012) point towards a rapid expansion of agricultural output and produc-
tivity during the 1980s and a slowdown during the 1990s, raising questions about
the sustainability of these growth rates. Few studies cover the 2000s and most
estimate productivity at the national rather than the provincial level. There-
fore, this study aims to a province-level agriculture productivity during year
2002–2012.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_33

Technical Eﬃciency Analysis of China’s Agricultural Industry
455
Liu et al. (2017) There are two primary methods of eﬃciency measures,
namely stochastic frontiers and data envelopment analysis (DEA), which involve
econometric methods and mathematical programming, respectively. Stochastic
frontier models make assumptions about the functional form of production or
cost functions, and can deal eﬀectively with the presence of noise in the data,
whereas DEA models make no assumptions about the functional forms, but can-
not deal eﬀectively with measurement error. Therefore, the stochastic frontier
model has been maturely applied into analyzing the technical ineﬃciency or
the technical eﬃciency. The ﬁrst proposal of the stochastic frontier production
function was token independently by Aigner et al. (1977) and Meeusen and van
den Broeck (1977); Since after it had been extended by Forsund et al. (1980),
Schmidt (1986), Battese (1992), and Greene (1993), Battese (1995). The stochas-
tic frontier production function assumes there is existed a technical ineﬃciency
for production activities. Based on an input factors analysis on production out-
put, it took a two error component to analysis the production ineﬃciency. It
will ﬁnally ﬁgure out the eﬀect of the technical eﬃciency for both of technical
change along with time and technical diﬀerence along with cross section. In this
paper, we expand to a translog production function because the better adequate
than Cobb-Douglas frontier model (Ngwenya et al. 2010). Furthermore, in order
to explore the substitute elasticity eﬀects, instead of Cobb-Douglas production
function, the translog production frontier, Ngwenya et al. (1977), Pitt and Lee
(1981), Coelli et al. (2005), is widely used to analyze technical eﬃciencies for
developing countries.
Using the stochastic frontier analysis to study productivity has been imposed
in China’s relevant issues. Fan (1991) used a frontier production function to sepa-
rate agricultural growth into input growth, technical change, institutional reform
and eﬃciency change. Lin (1992) employed a ﬁxed eﬀects model on provincial
data to evaluate the eﬀects of decollectivization (HRS).
In this paper, we will ﬁrstly introduce the methodology, then describe the
dataset applied in this paper, thirdly show and analyze the empirical results,
make a conclusion at the last section.
2
Econometric Methodology
In this section, we summarize the method of the translog stochastic frontier
production function analysis for the panel data. We ﬁrst reviewed the translog
stochastic frontier production function, then we use the model to estimate the
technical ineﬃciencies and ﬁnally we estimate the technical eﬃciencies.
2.1
Translog Stochastic Frontier Production Function
In the paper, we imposed the translog stochastic frontier production model with
panel data for China which include 31 provinces (i = 1 to 31). The time period
covers eleven years (t = 2002 to 2012). Based on the production model we

456
J. Ma et al.
attempt to analyze the technical eﬃciency for the agriculture industry in China.
The translog stochastic frontier production models listed as below
ln Yit = β0 +
5

j=1
βj ln xj,it + 1
2
5

j=1
5

k=1
βjk ln xj,it ln xk,it + Vit −Uit,
(1)
and the symbols and unit of variables can be checked in Table 1.
Y , the output, denotes the nominal agricultural added value (including farming,
forestry, husbandry and ﬁshing);
x represents the factors of inputs on agriculture, including;
x1, labor, represents the number of people employed in agricultural industry;
x2, land, or the sown area of crops, refers to area of land sown or transplanted
with crops regardless of being in cultivated area or non-cultivated area;
x3, denotes mechanical power;
x4, amount of fertilizer used;
x5 denotes geomembrane, the plastic membrane for agriculture used;
βj denotes the parameters of the input elasticity on output;
βjk represents the alternative elasticity between input j and input k, where
βjk = βkj;
Vits are assumed to be independent and identically distributed as normal random
variables with mean zero and variance σ2
v independent of the Uits;
Uits are non-negative random variables, associated with technical ineﬃciency of
production, which are assumed to be independently distributed.
2.2
Technical Ineﬃciency
The technical ineﬃciency, Uit, is obtained by a truncation (at zero) of the normal
distribution with variance σ2
u and mean zitδ (Refer to Eq. 2).
Uit = δ0 + Σ4
m=1δmzm,it + Wit,
(2)
zm,it denotes the explanatory variables which caused the technical ineﬃciencies,
m = 1, 2, 3, 4 represents 4 explanatory variables, i = 1, 2, ..., 31 represents 31
provinces of the mainland China:
z1 is population density;
z2 is the non-agricultural GDP per capita;
z3 is the available credit per capita;
z4 is the number of teachers per hundred;
δ is the eﬀects of the explanatory variables;
δ0 is the constant term;
δm represents the eﬀects of 4 explanatory variables, respectively;
Wit is deﬁned by the truncation of the normal distribution with zero mean and
variance σ2
W .

Technical Eﬃciency Analysis of China’s Agricultural Industry
457
2.3
Technical Eﬃciency
The technical eﬃciency of production for the i-th province at the t-th observation
is deﬁned by Eq. (3),
TEit = exp(−Uit) = exp(−zitδ −Wit),
(3)
The parameters of the translog stochastic frontier production model, the
technical ineﬃciency, and the technical eﬃciencies of china’s agriculture are esti-
mated and the variance parameters are expressed in terms of γ and σ2 which
are deﬁned by γ = σ2
u/σ2 and σ2 = σ2
u + σ2
v. The estimation for the eﬀects or
parameters of the stochastic frontier model deﬁned by Eqs. (1)–(3) are estimated
by using the method of maximum likelihood.
3
Data
In this study, the dataset has 10 indicators which derive from the website of
the National Bureau of Statistics of China (NBSC). The study period of data is
from 2002 to 2012 because of the availability. It covers 31 provinces, autonomous
regions and municipalities (using province instead in following content). Some
variables directly obtained the data from NBSC website, such like the agriculture
output, the employed labor in farming, forestry, husbandry, and ﬁshery, the sown
area, the mechanical power, the fertilizer and the geomembrane. However, some
other variable has to be calculated by the author based on data from NBSC as
following. The population density used the population divided the area of the
province; the non-agricultural GDP per capita is the summation of the gross
output value of the second and third industries divided population; the available
credit per capita used the saving deposit in urban and rural households divided
to the population; the number of teachers per hundred used the number of full-
time Teachers divided population and times 100.
4
Empirical Results
A summary statistics of the variables in the translog stochastic frontier model is
presented in Table 1. The descriptive province-level average values of 10 variables
in the model along with 31 provinces in China includes: the agriculture output
is about 99.6 billion Yuan; the amount of labor is somewhat 9.4 million persons;
the land area is slightly more than 5000 thousand hectares; the mechanical power
is a little more than 25 million Kilowatts; the fertilizer is about 1.64 million tons;
the usage of geomembrane is 62.4 thousand tons; the population density is 405
persons per km2 which the minimum 2.2 persons in the Tibet and 3754 persons
in Shanghai; the non-agriculture GDP per capita is 21.6 thousand yuan; the
available credit per capita is 16.9 thousand Yuan; and the number of teachers
per hundred population is 0.7. Table 2 presents the parameter estimators for the
translog stochastic frontier functions of the China’s provincial level agriculture

458
J. Ma et al.
Table 1. Summary of the China province-level agricultural production 2002–2012
Variable
Symbol Unit
Mean
Min
Max
Output
y
108 Yuan
996.3
41.4
4281.7
Labor
x1
104 Person
939.6
33.4
3393
Land
x2
103 Hectare 5052.7
231.2
14262.2
Mechanical power
x3
104 KW
2529.7
95.3
12419.9
Fertilizer
x4
104 Ton
164
3
684.4
Geomembrane
x5
Ton
62418.9 440.6
343524
Population density
z1
person/km2 405.2
2.2
3753.6
Non-agricultural GDP per capita z2
104 Yuan
2.1558
0.2508 9.0037
Available credit per capita
z3
104 Yuan
1.6869
0.1977 10.4615
Number of teachers per hundred
z4
Person
0.7033
0.3669 1.2669
Observations
341
Sources: Author’s tabulation based on data extracted from the Chinese National
Statistical Bureau website.
productivity by using the maximum-likelihood method. The estimated parame-
ters can be referred to the Eqs. (1) and (2) in the translog model. The estimation
of the translog production function, Eq. (1), shows the positive eﬀects of labor
(β1), fertilizer (β4) and geomembrane (β5) and the negative eﬀects of mechanical
power (β3) on the agriculture output. However, the eﬀect of land (β2) is not sta-
tistical signiﬁcant. The main two factors aﬀect the output are fertilizer and labor
with closely 0.66 and 0.62. It means 1% input increase on fertilizer and labor will
cause 0.66% and 0.62% increase in agriculture output, respectively. Mechanical
power has negative relation with agriculture output. It may because that it’s
applied on harvesting rather than sowing seeds and maintenance. If the price
stays invariant, input on mechanic cant make eﬀect on agriculture output, but
it can increase the cost to cause a negative eﬀect on output. Currently, China’s
agriculture value improvement doesn’t beneﬁt from increase on agricultural land
supply, therefore the eﬀect of land on agricultural output is not signiﬁcant. The
substitute elasticity of input βjk is not the focus of the paper we only present it
in the Table 2. In the ineﬃciency equation, there are a positive relation of the
population density (δ1) with 6.41 and a negative relation of the non-agriculture
GDP per capita (δ2) with −0.47 signiﬁcantly on the ineﬃciency. Furthermore,
it is not statistically signiﬁcant for the available credit per capita (δ3). However,
it is somewhat statistically signiﬁcant on 15% signiﬁcant level for the number
of teacher per hundred persons (δ4) with −0.0074. We found that 1% popula-
tion density increase will cause 6.41% increase in technical ineﬃciency. But 1%
increase in the non-agriculture GDP per capita and the number of teacher per
hundred persons will cause 0.47% and 0.0074 decrease in technical ineﬃciency.
The detail is in Table 2.
The predicted mean of the technical eﬃciencies of the agriculture productiv-
ity for each province are presented in Table 3. The mean value of each province

Technical Eﬃciency Analysis of China’s Agricultural Industry
459
Table 2. MLE estimates of the panel stochastic frontier model
Parameter
Estimate Std. error z-value
Pr(>|z|)
(Intercept)
β0
7.0871
0.0654
108.297
<2.2e–16***
ln(labor)
β1
0.6189
0.0482
12.8282 <2.2e–16***
ln(land)
β2
–0.0253
0.0886
–0.2862 0.7747
ln(mechanical)
β3
–0.5048
0.0824
–6.1262 9.000e–10***
ln(fertilizer)
β4
0.6582
0.0643
10.2356 <2.2e–16***
ln(geomembrane)
β5
0.2382
0.0408
5.8428 5.133e–09***
1
2 [ln(labor)]2
β11
–0.0918
0.1091
–0.8409 0.4004
1
2 [ln(landr)]2
β22
–0.1046
0.1316
–0.7946 0.4269
1
2 [ln(machanical)]2
β33
0.1719
0.0731
2.3509 0.0187*
1
2 [ln(fertilizer)]2
β44
0.3487
0.105
3.3216 0.0009***
1
2 [ln(geomembrane)]2
β55
–0.0611
0.0232
–2.6283 0.0086**
ln(labor) ln(land)
β12
0.0000
0.1874
–0.0002 0.9998
ln(labor) ln(mechanical)
β13
0.3247
0.1272
2.5526 0.0106909*
ln(labor) ln(fertilizer)
β14
–0.1914
0.122
–1.5693
0.1166
ln(labor) ln(geomembrane)
β15
–0.1135
0.0566
–2.0035 0.0451*
ln(land) ln(mechanical)
β23
0.539
0.1634
3.2995 0.0010***
ln(land) ln(fertilizer)
β24
0.1096
0.1631
0.6721 0.5015
ln(land) ln(geomembrane)
β25
–0.1189
0.0684
–1.7371 0.0823
ln(mechanical) ln(fertilizer)
β34
–1.1188
0.1875
–5.968
2.402e–09***
ln(mechanical) ln(geomembrane)
β35
0.1123
0.0651
1.724
0.0847
ln(fertilizer) ln(geomembrane)
β45
0.2475
0.0601
4.1151 3.869e–05***
Z (Intercept)
δ0
1.2805
0.0686
18.6782 <2.2e–16***
Z (Population Density)
δ1
6.4128
0.6469
9.9133 <2.2e–16***
Z (Nonagricultural GDP per
capita)
δ2
–0.4767
0.0735
–6.4878 8.709e–11***
Z (available credit per capita)
δ3
0.0326
0.0765
0.4258 0.6703
Z (teacher per hundred persons)
δ4
–0.0074
0.0047
–1.5764 0.1149
Sigma square
σ2 = σ2
u + σ2
v
0.0494
0.0049
10.0546 <2.2e–16***
Gamma
γ = σ2
u/σ2
0.4513
0.1466
3.0791 0.0021**
Notes: Signiﬁcant codes 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘’ 1.
over 11 years, the predicted technical eﬃciencies, obtained from the translog
model, range from 0.356 to 0.993. Hunan province has the highest agriculture
technical eﬃciency 0.993 and the subsequent two provinces are Beijing and
Tianjin with 0.906 and 0.845, respectively. On the contrary, Guizhou province
has the lowest agriculture technical eﬃciency 0.356. The second and third last
provinces are Yunnan and Anhui with the TEs 0.392 and 0.395. The results in
Table 3 reveals there are huge technical eﬃciency gap in terms of the mean of
the TEs between the top three and the bottom three provinces which shown in
Fig. 1.
The variance parameter γ = 0.45 shows that the technical ineﬃciency take
account for 45% of the total variance σ2. It indicates that there exit signiﬁcant
technical ineﬃciencies in the agriculture of China during the period 2002–2012.
The gap between the actual agriculture output and the frontier output were

460
J. Ma et al.
Table 3. Summary statistics of TEs by province
Rank Region
Mean S.D
Rank Region
Mean S.D.
1
Hunan
0.993
0.002 17
Shanghai
0.530
0.260
2
Beijing
0.906
0.104 18
Hubei
0.522
0.243
3
Tianjin
0.845
0.171 19
Chongqing 0.511
0.206
4
Zhejiang
0.793
0.201 20
Sichuan
0.509
0.165
5
Inner Mongolia 0.728
0.237 21
Jiangxi
0.507
0.167
6
Liaoning
0.718
0.224 22
Hainan
0.489
0.171
7
Guangdong
0.711
0.222 23
Shaanxi
0.477
0.228
8
Jiangsu
0.683
0.268 24
Guangxi
0.472
0.168
9
Fujian
0.663
0.231 25
Tibet
0.460
0.084
10
Jilin
0.611
0.223 26
Shanxi
0.449
0.184
11
Heilongjiang
0.599
0.189 27
Gansu
0.409
0.117
12
Shandong
0.590
0.251 28
Henan
0.403
0.150
13
Qinghai
0.589
0.220 29
Anhui
0.395
0.149
14
Xinjiang
0.558
0.147 30
Yunnan
0.392
0.099
15
Ningxia
0.551
0.229 31
Guizhou
0.356
0.088
16
Hebei
0.548
0.205
Fig. 1. Average eﬃciency of each year with 95% conﬁdence interval
caused by the technical ineﬃciency. We choose Hunan, Beijing, Tianjing and
Guizhou, Yunnan, Anhui as the top 3 and bottom 3 provinces in terms of the
province-level TEs and the result shown in Fig. 2. Hunan has approached the

Technical Eﬃciency Analysis of China’s Agricultural Industry
461
Fig. 2. Annual technical eﬃciency of Hunan, Beijing, Tianjin, Anhui, Yunnan, and
Guizhou
frontier since 2002 until 2012 with ﬂuctuation around 0.993. Beijing, Shang-
hai have been monotonic increased from 0.65, 0.5 in 2002 up to close to the
production frontiers in 2012. For eleven years’ development, the three lowest
TEs provinces increased sharply from 0.24 in 2002 to, respectively, 0.65, 0.6 and
0.55 in 2012 which means all three provinces reached or close to the nation’s
average province-level TEs 0.6.
5
Conclusion
The study of the China’s province-level agriculture output research was token by
using the translog stochastic frontier production function analysis. The results
point out (1) the technical eﬃciency of China’s agriculture production has been
increasing year after year. The agricultural advanced region has been approached
to the technical eﬃciency frontier with a strong convergence trend suck like
Beijing and Shanghai but a ﬂuctuation in Hunan. The future research may ﬁnd
the reason of the variation. The technical eﬃciency in agriculture for the under-
developed area increased sharply close to the average province level of China,
however, still need to be improved for a big gap to the eﬃciency frontier; (2)
High population density is the major reasons caused the technical ineﬃciencies.
It may because the proportion of the agricultural population on total popula-
tion has been continuously decreased to cause the insuﬃciency of the eﬀective
labor supply. The non-agriculture GDP per capita as a reason of reducing the

462
J. Ma et al.
technical ineﬃciency which are beneﬁts from the development of the proﬁtable
non-agriculture industries. Education, in terms of number of teacher per hundred
persons, has the positive eﬀect on reducing the technical ineﬃciency. Therefore,
the paper intends to give suggestions: In terms of improve the TEs, China still
need to increase its input on labor, fertilizer and geomembrane especially for
those provinces in the underdeveloped area to stimulate the agriculture output
directly. Meanwhile, the country need to enlarge its investment in the second and
third industries and to increase the number of teacher in underdeveloped area
especially rural area to improve the agriculture system toward to the technical
frontier.
Acknowledgements. This work has been supported by the Faculty of Economics and
the Puey Ungphakorn Centre of Excellence in Econometrics at Chiang Mai University.
References
Aigner, D., Lovell, C.A.K., Schmidt, P.: Formulation and estimation of stochastic fron-
tier production function models. J. Econom. 6(1), 21–37 (1977)
Baten, M.A., Kamil, A.A., Haque, M.A.: Modeling technical ineﬃciencies eﬀects in
a stochastic frontier production function for panel data. Afr. J. Agric. Res. 4(12),
1374–1382 (2009)
Battese, G.E.: Frontier production functions and technical eﬃciency: a survey of empir-
ical applications in agricultural economics. Agric. Econ. 7(3), 185–208 (1992)
Battese, G.E., Coelli, T.J.: Frontier Production Functions, Technical Eﬃciency and
Panel Data: With Application to Paddy Farmers in India. In: Gulledge, T.R., Lovell,
C.A.K. (eds.) International Applications of Productivity and Eﬃciency Analysis, pp.
149–165. Springer, Dordrecht (1992). A Special Issue of the Journal of Productivity
Analysis
Battese, G.E., Coelli, T.J.: A model for technical ineﬃciency eﬀects in a stochastic
frontier production function for panel data. Empir. Econ. 20(2), 325–332 (1995)
Battese, G.E., Rao, D.S.P., O’Donnell, C.J.: A metafrontier production function for
estimation of technical eﬃciencies and technology gaps for ﬁrms operating under
diﬀerent technologies. J. Product. Anal. 21(1), 91–103 (2004)
Bin, P., Vassallo, M.: The growth path of agricultural labor productivity in China: a
latent growth curve model at the prefectural level. Economies 4(3), 1–20 (2016)
Cao, K.H., Birchenall, J.A.: Agricultural productivity, structural change, and economic
growth in post-reform China. J. Dev. Econ. 104, 165 (2013)
Carter, C.A., Chen, J., Chu, B.J.: Agricultural productivity growth in China: farm
level versus aggregate measurement. China Econ. Rev. 14(1), 53–71 (2003)
Chen, P.C., Yu, M.M., Chang, C.C., Hsu, S.H.: Total factor productivity growth in
China’s agricultural sector. China Econ. Rev. 19(4), 580–593 (2008)
Chen, Z., Song, S.: Eﬃciency and technology gap in China’s agriculture: a regional
meta-frontier analysis. China Econ. Rev. 19(2), 287–296 (2008)
Coelli, T.J., Rao, D.S.P., O’Donnell, C.J., Battese, G.E.: An introduction to eﬃciency
and productivity analysis. Springer (2005)
Fan, S.G., Zhang, X.B.: Production and productivity growth in Chinese agriculture:
new national and regional measures. Econ. Dev. Cult. Change 50(4), 819–838 (2002)

Technical Eﬃciency Analysis of China’s Agricultural Industry
463
Foster, A.D., Rosenzweig, M.R.: Agricultural productivity growth, rural economic
diversity, and economic reforms: India, 1970–2000. Econ. Dev. Cult. Change 52(3),
509–542 (2004)
Gautam, M., Yu, B.X.: Agricultural productivity growth and drivers: a comparative
study of China and India. China Agric. Econ. Rev. 7(4), 573–600 (2015)
Hermann-Pillath, C., Kirchert, D., Pan, J.C.: Prefecture-level statistics as a source of
data for research into China’s regional development. China Q. 172, 956–985 (2002)
Huang, C.J., Huang, T.H., Liu, N.H.: A new approach to estimating the metafrontier
production function based on a stochastic frontier framework. J. Prod. Anal. 42(3),
241–254 (2014)
Liu, J., Rahman, S., Sriboonchitta, S., Wiboonpongse, A.: Enhancing productivity
and resource conservation by eliminating ineﬃciency of Thai rice farmers: a zero
ineﬃciency stochastic frontier approach. Sustainability 9(5), 770 (2017)
Meeusen, W., van Den Broeck, J.: Eﬃciency estimation from Cobb-Douglas production
functions with composed error. Int. Econ. Rev. 18(2), 435–444 (1977)
Ngwenya, S.A., Battese, G.E., Fleming, E.M.: The relationship between farm size and
the technical ineﬃciency of production of wheat farmers in the eastern free state,
province of South Africa. Agrekon 36(3), 283–302 (1997). samevatting: die verhoud-
ing tussen plaasgrootte en die tegniese doeltreﬀendheid van koringboere in die oos
vrystaat
Pitt, M.M., Lee, L.-F.: The measurement and sources of technical ineﬃciency in the
Indonesian weaving industry. J. Dev. Econ. 9(1), 43–64 (1981)
Schmidt, P.: Frontier production functions. Econom. Rev. 4(2), 289–328 (1985)
Si, W., Wang, X.Q.: Productivity growth, technical eﬃciency, and technical change in
China’s soybean production. Afr. J. Agric. Res. 6(25), 5606–5613 (2011)
Sriboonchitta, S., Liu, J., Wiboonpongse, A., Denoeux, T.: A double-copula stochastic
frontier model with dependent error components and correction for sample selection.
Int. J. Approx. Reason. 80, 174–184 (2017)
Tong, H., Fulginiti, L.E., Sesmero, J.P.: Chinese Regional Agricultural Productivity:
1994–2005, p. 10. Lilyan E. Fulginiti Publications (2009)
Wiboonpongse, A., Liu, J., Sriboonchitta, S., Denoeux, T.: Modeling dependence
between error components of the stochastic frontier model using copula: applica-
tion to intercrop coﬀee production in Northern Thailand. Int. J. Approx. Reason.
65, 34–44 (2015)
Zhang, Y.J., Brummer, B.: Productivity change and the eﬀects of policy reform in
China’s agriculture since 1979. Asian Pac. Econ. Lit. 25(2), 131–150 (2011)
Zhou, X.B., Li, K.W., Li, Q.: An analysis on technical eﬃciency in post-reform China.
China Econ. Rev. 22(3), 357–372 (2011)
Zhou, Y.H., Zhang, X.H., Tian, X., Geng, X.H., Zhang, P., Yan, B.J.: Technical and
environmental eﬃciency of hog production in China - a stochastic frontier production
function analysis. J. Integr. Agric. 14(6), 1069–1080 (2015)

Empirical Models of Herding Behaviour
for Asian Countries with Confucian Culture
Munkh-Ulzii1
, Massoud Moslehpour2(&)
,
and Pham Van Kien3
1 School of International, Relations and Public Administration,
National University of Mongolia, Ulan Bator, Mongolia
ulzii03@gmail.com
2 Department of Business Administration, College of Management,
Asia University, Taichung, Taiwan
writetodrm@gmail.com, mm@asia.edu.tw
3 International Business Faculty, Banking University,
Ho Chi Minh City, Vietnam
kienpv@buh.edu.vn
Abstract. The purpose of this study is to investigate the insights of herding
behavior in the Confucian markets by conducting a set of empirical tests. More
speciﬁcally this study investigates a sample of 7 countries and 13 markets to
gain a deeper understanding of the causes of herding behavior and the potential
factors that cause investors to behave in a group manner. Following a com-
prehensive review of the existing methodologies on herding behavior this study
employs return dispersion approaches and herding tests developed by Chang
et al. (2000) and Tan et al. (2008). This study investigates a sample of 13 stock
markets of seven Asian economies with three different hypotheses. Those
economies, which are considered to have Confucian culture, are mainland
China, Hong Kong, Japan, South Korea, Taiwan, Singapore, and Vietnam. The
hypotheses of this study aim to investigate formation of herding behavior in
different market and economic circumstances. In testing the empirical models,
this study uses OLS regression for the main test as well as regression with
Newey and West (1987) for the robustness test of each result from OLS
regression analysis. Data of this study consists of 13 index returns (Shanghai A
and B share, Shenzhen A and B share, Hang Seng index, NIKKEI225, TOPIX,
KOSDAQ, KOSPI, Straits Times Index, TAIEX, and indices of Hanoi and
Hochiminh city Stock Exchanges) and returns of their constituent stocks. The
time period of the sample data is from January 01, 1999 to December 31, 2014.
All data were collected from the Thomson Reuters Datastream database.
According to the empirical ﬁndings, all hypotheses are accepted. The sample
markets demonstrate signiﬁcant herding behavior in general and signiﬁcant
herding behavior in different markets conditions, such as in rising-falling mar-
kets and high-low market volatility states. This study has some major contri-
butions to the literature of herding behavior and the link between herding
behavior and cultural aspects. First, this study uses dataset of 13 Confucian
stock markets of seven Asian economies, with time range from 1999 to 2014.
Second, this research developed and tested three different hypotheses, and all of
them are accepted. Third, this study adds the new dimension of the cultural
aspects in order try to explain the root causes to herding behavior among
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_34

investors in the equity markets. Recognizing that the Confucian culture appears
to be one of the most inﬂuential cultural aspects in management, this study
examines herding behavior of Confucian culture in stock markets under the
umbrella of one empirical study. According to the ﬁndings of this study,
Confucian culture has a positive and signiﬁcant effect on herding behavior
among investors in equity markets.
Keywords: Herding behavior  Herding tendency  Emerging markets
Advanced markets  Confucian markets  Confucian culture
1
Introduction
It is difﬁcult to make a precise deﬁnition about what is herding. Generally, it is a
correlated behavioral pattern across individuals. Theoretically, herding behavior is a
human behavior to mimic the behavior and actions of other people. Studies point out that
herding behavior can be both rational and irrational (Chang et al. 2000). And, according
to its motive, there are several types of herding behavior, such as information based
herding, reputation based herding, and compensation based herding, as well as spurious
herding. According to the sequential decision theory each trader observes the decisions
of made by other investors in order to make his/her own decision. From the standpoint of
the trader this is a rational act because decisions of other investors can include some
important information for the trader. In this sense, herding behavior is rational.
1.1
Research Problem
Several studies have been held to catch up other factors that affect herding behavior on
equity markets. Economou et al. (2011) tested Portuguese, Spanish, Italian and Greek
markets to ﬁnd herding effects by using daily stock return data during time period of
1998–2008. They found herding behavior from markets Greece and Italy, however they
did not ﬁnd evidence from Spanish and Portuguese markets. Mobarek et al. (2014)
conducted another study on stock markets of eleven European countries using daily
stock return data from 2001 to 2012. They divide sample countries into three categories
– continental, PIIGS, and Nordic. They found that there is no herding during normal
times, but the markets herd during crisis times and extreme market situation. Namely,
Nordic markets were extensively impacted by the Eurozone crisis, while Continental
and the PIIGS were more affected by the global ﬁnancial crisis. However, these two
studies found herding in cross-country level, the patterns and causes of herding were
not the same from country to country, even though they all belong to the same general
category – Europe. Otherwise, their outcomes suggest that there may exist some other
reasons that make investors to herd, and that may be culture.
Therefore, above two studies not only found herding behavior from international
markets, but also they found that stock markets of different countries react differently to
the same events. It is thought that western markets are more mature, also the western
personality is more individualistic; thus investors there would hardly display herding
behavior. Unfortunately, these assumptions collapsed by their results. Therefore, now
we know that even developed markets herd, and more interestingly cultural aspects
Empirical Models of Herding Behaviour
465

may play an important role in herding besides information asymmetry, markets sen-
timents and fundamentals. However, ﬁndings of Economou et al. (2011) and Mobarek
et al. (2014) suggest that cultural features matter in herding, the question of “why is it
so” left inconclusive. Thus, now we have two studies support the idea of that culture
may have an effect on formation of herding behavior. Are there other studies on how
culture effects managerial decision making process?
Some more studies also discuss the potent impact of cultural factors on managerial
decision making. Siegel et al. (2011) found the negative correlation between egali-
tarianism distance and international portfolio distribution. Aggarwal et al. (2012) found
that the facets of culture have the potential to promote foreign portfolio management.
Ferris et al. (2013) reported that managerial overconﬁdence tendency rises and falls
with individualism and long-term orientation traits. Holderness (2009) showed that
there exists a reciprocity among culture and ownership concentration of public cor-
porations. Thus, these studies also found supportive results that national culture appears
to be one of the main dominant factors in investment decision making.
Furthermore, culture might also emerge to be correlated with the aggregate
economic activities of a country. Thus, studies also devoted attempts to check whether
national culture has effects on the economic growth. Chow et al. (1991) found
signiﬁcant inﬂuence of individualism of employees on manufacturing performance.
Li et al. (2013) found signiﬁcant and positive relationship between GDP and indi-
vidualism and uncertainty avoidance. Xue and Cheng (2013) discuss that culture and
loan markets are correlative.
Thus, now we have more evidence on and conﬁdence in that culture has a direct
impact on behavioral decisions of people and has an indirect effect on aggregate
economic activities. Therefore, we make the ﬁrst assumption in our study that for-
mation of herding tendency among investors might also be affected by cultural settings.
And, formation herding tendency means herding behavior almost to happen. It is just a
progression of a process. However, previous studies found out a clear evidence on how
culture directly impacts on managerial decision making, they did not ﬁnd out the actual
reason of why culture does it. Yet next two studies did ﬁnd a smoking gun.
Chang and Lin (2015) tested herding behavior among stock market investors at
cross-country level. Their study sample, which is consisted of 50 stock markets, is
made in a way to reﬂect as much geographical regions as possible. The speciﬁc feature
of their study is that, besides using equity return data they employed Hofstede cultural
index to check whether cultural aspects appear to be another reason why investor herd
on the market. Interestingly, most of the herding behavior they found were mostly
among Confucian markets. More speciﬁcally, they found that (1) markets under the
inﬂuence of Confucian culture demonstrate a high power distance, low individualism,
and high masculinity; (2) also, these dimensions found to have signiﬁcantly positive
effect on formation of herding tendency. The authors argue that people under the
inﬂuence of Confucian culture, which focus on ethics, obedience, humanism, and
collectivism, display the acceptance of unequal power distribution among the disad-
vantaged members of national institutions and organizations, close connection with the
public, and the clear distinction between gender roles. In other words, investors in stock
markets dominated by Confucian culture emphasize public morality and follow the
behavior of the majority of people. It not only impacts investors’ decision-making, but
466
Munkh-Ulzii et al.

also frames their reaction to a certain informational shock. Otherwise, it effects on
investors’ behaviour in the stock markets. Therefore, Confucian philosophy may
trigger investors to exhibit herding behavior.
Findings of Chang and Lin (2015) were in line with the ﬁndings of study conducted
by Beckmann et al. (2008). Arguing that cultural differences translate into different
behavior and that these differences are relevant for investment behavior, Beckmann
et al. (2008) examined different cultured countries (Japan, Thailand, US, and Germany)
by using cultural dimension approach of Hofstede. According to their ﬁndings, cultural
dimensions of individualism and power distance were also found to be signiﬁcantly
correlated with herding behaviour. They found a strong signiﬁcant evidence that asset
managers in less individualistic countries exhibit more herding behaviour, and tend to
follow market trends more closely. Moreover, societies with high power distance pay
much attention on seniority. In societies, where power distance is high, age appears to
be the strong factor in investment behavior, such as risk taking and portfolio allocation
in a conservative way. Supporting this theory, Beckmann et al. (2008) also found that
higher age positively inﬂuences on formation of herding tendency. Moreover, they
conclude that societies with high power distance not only prefer older managers for
promotion, but they also consider those who have less experience. Fifty percent of their
sample was from Japan, which is also a Confucian country.
The common things of these two studies are, ﬁrst, they both examined cultural
effects on herding behavior; second, samples of either studies includes Confucian
countries; third, they used same research instruments in their empirical analysis; and
ﬁnally they found similar results. In summary, Chang and Lin (2015) found that
Confucian markets tend to have high power distance, low individualism, and high
masculinity, which appear to be the stepping stones to formation of herding behavior.
While, Beckmann et al. (2008) found that markets under dominance of high power
distance and low individualism more likely to exhibit herding behavior. And, the half
of the sample of their study is data from Japan, which is a Confucian country.
Thus, these studies provide us with empirical evidences that encourage us to make
the second assumption in our study that Confucian culture positively impacts on for-
mation of herding tendency among investors in equity markets. What is the matter with
Confucian culture, why it impacts on formation of herding tendency? This is a very
interesting and big question. Thus, we will discuss about what Confucian culture is and
the links between this culture and managerial decision making process in the next
section.
1.2
Confucian Philosophy and Its Impacts on Managerial
Decision-Making
The legacy of Confucian philosophy is a system that highlights the signiﬁcance of hard
work, loyalty, devotion, learning, and social hierarchy. What are the core teachings of
Confucian? In Confucianism individuals are not regarded as important as a
group. Thus, this philosophy teaches that a person should honor ﬁrst his/her duty to
family and society. In order words, personal needs to be sacriﬁced for the sake of group
welfare. In this manner, it helped to develop a managerial mindset with greater
emphasis on collectivism, teamwork, and harmony in relationships. Therefore, when
Empirical Models of Herding Behaviour
467

Chinese managers face difﬁcult situations, they usually would not exhibit personal
behavior to existing situation, rather they would follow the tendency of the group that
he/she belong to. Chinese management culture differs from western culture in large
extent. Confucian culture is based on collectivism, whereas the westerns tend to be very
individualistic.
1.2.1
Confucian Management Theories “Guanxi” and “Mianzi”
There are two fundamental theories in Confucianism - “guanxi” and “mianzi”, which
have been powerful and essential among Chinese communities. Due to the strong
foundation of these theories, every man in Chinese society is a businessman to expand
his wealth and fame. And, the Confucian philosophy is mainly about how to make the
nation ﬂourish by enhancing individual wealth and fame via synergy of powerful
network of collaboration and friendship.
Guanxi, which is the essence of Confucianism, is one of the most important factors
to deal with when somebody enters into business and investment in Chinese society.
Guanxi can be interpreted as “relationship” or “networking”. Sometimes guanxi ﬁnds
very complicated both for the members and non-members of the network. The problem
is that it usually requires obligations or indebtedness, a mechanism of favors and debts
among members in the network, which itself is based on relationships governed by
Confucian teachings. Therefore, in order to establish and maintain a position among a
guanxi network it requires some wisdom of manipulation of human feeling, reciprocity
of favors and giving face (mianzi). Guanxi is very powerful among Chinese societies,
so that, personal networks and loyalties are thought to be more important than orga-
nizational afﬁliations or legal standards (Luo 1997; Buttery and Leung 1998).
Even though the concept of “mianzi” has broad meaning, the degree of concern is
much higher and its manifestations are somehow different in the Confucianism. In
Confucian communities the notion of mianzi carries a meaning of showing respect for
one’s social status. To maintain face means to stay trustworthy and to honor obligations
in one’s guanxi. Sometimes, it is more important to give face to others than to protect
own face (Buttery and Leung 1998). Giving favors to others shows that one has
capacity beyond one’s peers, and thus gains face or respect in return. In reciprocity, the
person who have received a favor should return it whenever got to return it, even if this
reciprocity may be harmful one’s social reputation and lead to lose his face. Losing
face is not only failure in reputation but also indication of lack of trust. Thus, subor-
dinates are always advised to do their best to give superiors face by asking favors from
them, but always to do it within the boundaries because of the code of reciprocity. In
Confucian societies, guanxi and mianzi are two sides of the same thing, they are
equally very important aspects (Luo 1997; Su and Littleﬁeld 2001).
The core regions of the east Asian cultural sphere are China, Taiwan, Hong Kong,
South Korea, Japan, and Vietnam, as well as territories settled by Chinese people, such
as Singapore. We assume that the Confucian culture has a positive impact on formation
of herding behaviour among investors in stock markets of Confucian countries. China,
Korea, Japan, Taiwan, Singapore, and Vietnam are Confucian countries, thus investors
of these countries are more likely to exhibit herding behavior.
468
Munkh-Ulzii et al.

2
Literature Review
Chan et al. (2000) examined the herding behavior among international markets, US,
Hong Kong, Japan, South Korea, and Taiwan by employing daily stock price data with
time range from January 1963 to December 1997. They used cross-sectional absolute
deviation method, which is the revised version of the model that was developed by
Christie and Huang (1995). They found no evidence of herding from markets of US
and Hong Kong and partial evidence from Japan. However, they found signiﬁcant
herding from South Korea and Taiwan. They also found that macroeconomic infor-
mation impacts heavily on formation of herding behavior than ﬁrm-speciﬁc informa-
tion. Finally, their results conclude that stock return dispersion, as a function of the
market return, is higher during up market than down market. They also tested Christie
and Huang (1995) model, however, they did not ﬁnd herding except market of Taiwan.
Following the Chan et al. (2000) model Tan et al. (2008) found herding behavior in
Chinese A1 and shares. Due to their hesitated in accuracy of estimation of beta, sug-
gested by Chang et al. (2000), they used standard deviation in estimation of the return
dispersion, which was adopted by Christie and Huang (1995). Moreover, they also
tested the asymmetric impact of herding behavior by varying market return, trading
volume, and volatility. They found herding behavior among investors of a share of
Shanghai market under rising market circumstances with high volume of trading of
stocks and volatility. However, there found no signiﬁcant evidence of herding in B (see
Footnote 1) shares. The main participants of A share market are local individual
investors who are considered to be insufﬁcient in skills and experience of investment.
While investors of B shares market are mostly foreigners who possess with more skills
and knowledge in investment rather investors of A share. In this fashion, they conclude
that the discrepancies of investors of A and B shares may impact on variance of
intensity of herding behavior in each market. This study also tested herding behaviour
and cross-market information effect, however they did not ﬁnd evidence for return
dispersions of markets inﬂuence each other. According to Tan et al. (2008), the dis-
similarity can be explained due to the difference of sample population.
Demirer et al. (2010) measures herding behavior by using daily data of stock
returns for 689 stocks traded on the Taiwan Stock Exchange with time period from
January 1995 to December 2006. In the analysis they used Christie and Huang (1995)
model, Chan et al. (2000) model, and state space models. They found no evidence for
herding when they used Christie and Huang (1995) model, however they did ﬁnd
signiﬁcant herding when they used non-linear model of Chan et al. (2000) and the state
space based models proposed by Hwang and Salmon (2004). They also found that the
herding is stronger during periods of market losses. Thus, this paper suggest that
investors need more diversiﬁed opportunities, speciﬁcally during periods of market
losses. The authors pointed out the following three contributions of their study: First,
this study contributes herding literature with empirical results of an emerging yet
1 A shares are shares of the Renminbi currency that are purchased and traded on the Shanghai and
Shenzhen stock exchanges. This is contrast to Renminbi B shares, which are owned by foreigners
who cannot purchase A-shares due to Chinese government restrictions.
Empirical Models of Herding Behaviour
469

relatively sophisticated Taiwanese stock market at the sector level by using ﬁrm level
data. Second, this paper used a set of different models. Third, this study discusses the
practical usage of different herding measures for investors who face to systematic and
unsystematic risks.
My and Truong (2011) examined existence of herding behavior in the Vietnamese
stock market by using daily stock price data at the Ho Chi Minh Stock Exchange with
time period from March 2002 to July 2007. In testing herding they used Christie and
Huang (1995) model robustness test and Chan et al. (2000) model for the main tests.
They also tested asymmetric effects of herding. This study found an evidence of
herding behavior in the Vietnamese stock market. The authors discuss that this herding
behavior can be explained due to the lack of transparency in information and ﬁnancial
management, the high magnitude of market volatility and thin trading. Moreover, the
study found that return dispersion is less during upward markets than downward
markets. It maybe be due to that Vietnamese investors behave more uniformly in rising
markets than in declining markets. Based on the empirical ﬁndings, this study discusses
that herding appears to be one of the reasons of market volatility, thus ﬁnancial
institutions may concern on policies adjustments to avoid from possible destabilization
effects on the ﬁnancial markets. With the Christie and Huang (1995) approach no
herding was found.
Yao et al. (2014) measured existence of herding behavior among the Chinese A and
B stock markets. To do so, they used daily and weekly ﬁrm level and market level data
of equity prices for all ﬁrms and indices listed on the Shanghai Stock Exchange and the
Shenzhen Stock Exchange with time period from January 1999 to December 2008.
Additionally, monthly data on industry classiﬁcation, market capitalization, earnings per
share, and market-to-book values are also collected for all ﬁrms included in the sample.
In data analysis, this study used modiﬁed version of Christie and Huang (1995) and
Chan et al. (2000) models as herding test. They ﬁndings showed that the magnitude of
herding behavior among investors was heterogeneous, speciﬁcally herding was stronger
in the B-share markets. Also, they found that across markets herding was stronger at
industry-level, was stronger for the largest and smallest stocks, and was stronger for
growth stocks relative to value stocks. Results of their study show that herding was more
prevalent during declining markets. Finally, they found that the magnitude of herding
behavior was at tapering state. The authors explained it was maybe due to the effec-
tiveness of regulatory reforms in China aimed at improving information efﬁciency and
market integration.
Aiming to explore the determinants of investor decision-making in international
stock markets Chang and Lin (2015) conducted a study on herding behaviour by using
daily market return data and industrial index data for 50 stock markets to calculate the
cross-sectional absolute deviation of returns. This study used extended version of Chan
et al. (2000) model. Moreover, to investigate the inﬂuence of national culture on
investment herding tendency, this study employs the national culture indexes proposed
by Hofstede (2001) for empirical analysis. Finally, in order to test behavioral pitfalls on
herding tendency, this study used daily data of price-to-book ratios as proxies for
excessive optimism, while daily trading volumes data were used as proxies for over-
conﬁdence and the disposition effect. The time period of entire dataset of this study
ranges from January 1965 to July 2011. The authors argue that unlike prior studies,
470
Munkh-Ulzii et al.

their study examines the effects of national culture and behavioral pitfalls on invest-
ment decision-making process. Their empirical results show that herding behaviour
exhibits in Confucian and less sophisticated stock markets. Moreover, this study found
that some national culture indexes were closely correlated with herding behaviour.
Finally, they found that behavioral pitfalls were dominant on formation of herding.
2.1
Hypothesis Development
Most of the studies on herding behavior used the return dispersion approach as the
main research model. Thus, it is reasonable for us to follow the trend. Thus, we will
generally refer to those studies in hypothesis development. Before we made a general
proposition that Confucian culture has a positive impact on formation of herding
behaviour among investors in stock markets of Confucian countries. To verify this
assumption, we will go through the following three different hypotheses.
2.1.1
Development of Hypothesis 1
“Guanxi” and “Mianzi” are the core values and main management theories in Con-
fucian philosophy. These theories have been the key factors in being China to be one of
the strong markets in all times. In other words, a network of cooperation and friendship,
which is powered by Confucian management theories, is simply a well-established
market place. Thus so, China appears to be one of the places where market economy
was born. With a such powerful foundation, it is apparent that Confucian philosophy is
the most powerful management force in the world in terms of its inﬂuence on decision
making process of managers. Therefore, the likelihood of inﬂuence of this culture on
formation of herding behavior is much convincing.
Besides Confucian culture, there are also other factors may induce herding
behavior. Moreover, there is a relatively high degree of government involvement in the
operations of the equity markets, with the imposition of many restrictions, such as price
limits, no short-selling and an intraday trading ban. The heavy intervention of the
central bank in adjusting interest rate policy is thought to have some impact on
investors in this market, especially in the case of upward adjustment. As, banks
mobilize capital at higher costs, they are obliged to raise their lending interest rate on
securities loans which might reduce investors’ proﬁtability. This potentially leads to a
stronger tendency towards investor herding (Christie and Huang 1995; Chang et al.
2000; Tan et al. 2008; Economou et al. 2011; Lao and Singh 2011; Mobarek et al.
2014; Yao et al. 2014; Chang and Lin 2015). Thus, our study develops the following
ﬁrst hypothesis for empirical testing:
Hypothesis 1: Herding behaviour exists among the Confucian markets.
2.1.2
Development of Hypothesis 2 and 3
Presumably, investor behavior might be inﬂuenced by the direction of the market. In
fact, some studies have found that the level of herding behavior changes under different
market conditions, depending on whether the market is rising or declining. It is
therefore hypothesized that investor have a greater tendency to herd in downward
markets than in upward markets. It can be argued that the fear of the potential loss
Empirical Models of Herding Behaviour
471

when the market is decreasing looms larger than the pleasure of potential gain when the
market is increasing. As a result, this behavior is likely to induce investors to mimic the
aggregate market when it is falling. A possible explanation for this behavior is that
investors may experience less disappointment when others also make the same
investment decision and this decision eventually turns out to be poor. Thus, herding
behavior is expected to be more pronounced when a market is falling than when it is
rising (Chang et al. 2000; Gleason et al. 2004; Tan et al. 2008; Chiang and Zheng 2010;
Chiang et al. 2010; Lao and Singh 2011; My and Truong 2011; Bhaduri and Mahapatra
2013; Yao et al. 2014; Mobarek et al. 2014).
In addition, McQueen et al. (1996) found that while all stocks tend to respond
quickly to negative macroeconomic news, small stocks tend to exhibit a delay in
reacting to positive macroeconomic news. Since good macroeconomic news often
entails an increase in stock prices, the slow reaction implies a postponement in the
incorporation of good news into the prices of small stocks. This may therefore lead to
an extra increase in market return dispersion in an increasing market but not in a
declining market. This implies that herding is likely to be more pronounced in
downward markets that are characterized by small stocks.
Christie and Huang (1995) and Chang et al. (2000) note that herding behaviour may
be more pronounced during periods of market stress. Consequently, we investigate
whether the herding behaviour documented above varies with market conditions.
Speciﬁcally, we examine potential asymmetries in herding behaviour as the trading
environment is characterized by different states of market returns and volatility. Return
volatility and trading volume may help characterize such periods, so we use them to
gain additional insight regarding the level of herding behaviour under different market
conditions. Since the direction of the market return may affect investor behavior we
develop the hypothesis two to examine possible asymmetries in herd behavior con-
ditional on whether the market is rising or falling:
Hypothesis 2: Asymmetric herding behaviour exists among the Confucian markets
when the market rises and falls.
Also, we develop hypothesis four to examine potential asymmetric effects of
herding behavior with respect to market volatility:
Hypothesis 3: Asymmetric herding behaviour exists among the Confucian markets
during high and low volatility states of markets.
3
Methodology and Data
3.1
Data Collection
This study uses daily returns of market indexes of Confucian stock markets and their
constituent stocks. Data of this study consist from 13 indexes (Shanghai A and B share,
Shenzhen A and B share, Hang Seng index, NIKKEI225, TOPIX, KOSDAQ, KOSPI,
Straits Times Index, TAIEX, and indices of Hanoi and Hochimin city Stock Exchan-
ges) of nine stock markets of six Confucian countries. Market return and individual
472
Munkh-Ulzii et al.

stock returns calculated by Datastream by using discrete or simple return calculation
method. All data were collected from the Thomson Reuters Datastream database. The
time period of the sample data is from January 01, 1999 to December 31, 2014.
3.2
Methodology
This study conducts several tests and analysis, which can be called as data oriented and
empirical model oriented in terms of their purpose. Normally, all tests and analysis use
data, however one checks the reliability and validity the data, and makes sure it for an
empirical analysis, while the another tests relationship between several variables by
using the data. The data oriented tests in our study are test of descriptive statistics and
unit root test as the Augmented Dickey-Fuller (ADF), while empirical model or
empirical tests are correlation test and multiple regression analysis. Also, while we did
empirical models estimation by using multiple regression analysis, we also conducted a
robustness regression analysis by using regression with Newey-West standard errors
(1987). In data analysis, we used softwares, such as Stata 13.1, Microsoft Excel, and
MathType.
3.3
Empirical Models
3.3.1
Measuring Herding Behavior (H1)
A couple of approaches in detecting herding behaviour by using individual stock
returns and market return, were developed by Christie and Huang (1995) and Chang
et al. (2000). Christie and Huang (1995) discussed that the decision making process of
investors depends upon overall market conditions. According to their view during
normal time periods, rational asset pricing models anticipate that the return dispersion
will increase with the absolute value of the market return, since individual investors are
making decision based on their own information, which is diverse. However, during
time periods of extreme market situations, investors tend to oppress their own beliefs,
and mimic the collective actions in the market. Individual equity returns under these
conditions tend to cluster around the overall market return. Therefore, they argue that
herding will be more common during periods of occurrence of extreme returns on the
market portfolio. They proposed the following equation in their empirical testing:
CSSDt ¼ a þ bLDL
t þ bUDU
t þ et
ð1Þ
where CSSDt is cross-sectional standard deviation at time (t), a is an intercept, DL
t is a
dummy variable at time (t) taking value of 1 when market return at time (t) lies in the
extreme lower tail of the return distribution, otherwise 0. DU
t is a dummy variable at
time (t) taking value of 1 when market return at time (t) lies in the extreme upper tail of
the return distribution, otherwise 0. bL is a coefﬁcient at DL
t . bU is a coefﬁcient at DU
t .
And et is error term at time (t). To measure the return dispersion, Christie and Huang
(1995) proposed the CSSD, which is expressed as:
Empirical Models of Herding Behaviour
473

CSSDt ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
Ri;t  Rm;t

2
N  1
v
u
u
u
t
ð2Þ
where N is the number of ﬁrms in the portfolio. Ri;t is individual stock returns of stock
(i) at time (t). Rm;t is market return at time (t). This model suggests that, if herding
occurs, investors will make similar decisions, leading to lower return dispersions. Thus,
statistically signiﬁcant and negative coefﬁcients bL and bU in Eq. (1) would indicate
the presence of herding behaviour.
Demirer and Kutan (2006) used Christie and Huang (1995) method to test herding
behaviour in Chinese stock markets. They employed daily stock returns of 375 ﬁrms
but did not ﬁnd evidence of herding. One of the drawbacks related with Christie and
Huang (1995) approach is that it requires the deﬁnition of extreme returns. Christie and
Huang (1995) discussed that this deﬁnition is arbitrary, and they employed values of
one and ﬁve percent as the cut-off points to identify the upper and lower tails of the
return distribution. In reality, traders not always ﬁnd the same in their opinion about
extreme return, and the characteristics of the return distribution may change over time.
Moreover, herding behavior can happen to some extent over the entire return distri-
bution, but become more noticeable during periods of market stress, and the Christie
and Huang (1995) approach is designed to detect herding only during periods of
extreme returns.
An optional method to the Christie and Huang (1995) approach for herding was
proposed by Chang et al. (2000). They tested multiple international stock markets by
using Christie and Huang (1995) method, and did not ﬁnd evidence of herding
behaviour in developed markets, such as the US and Hong Kong. However, they did
ﬁnd evidence of herding behaviour in the emerging stock markets of South Korea and
Taiwan. Chang et al. (2000) discussed that the Christie and Huang (1995) method is a
rigorous method, which requires a far greater degree of non-linearity to ﬁnd herding
behavior. While the method that they developed showed that when stock return dis-
persion is measured by the cross-sectional absolute deviation of returns, rational asset
pricing models predict not only that dispersion of individual stocks is an increasing
function of the market return, yet that the relation is linear as well. However, a
heightened tendency of investors to herd around the market consensus during periods
of large price movements is enough to convert the linear relation into a non-linear
relation. Thus, to detect herding behavior, Chang et al. (2000) used a non-linear
regression speciﬁcation, which is alike in spirit to the market timing approach of
Treynor and Mazuy (1966). The herding method of Chang et al. (2000) ﬁnds very
helpful to herding over the entire distribution of market return with the following
speciﬁcation:
CSADt ¼ a þ c1 Rm;t

 þ c2 Rm;t

2 þ et
ð3Þ
The CSADt is cross-sectional absolute deviation at time (t), a measure of return
dispersion, which is calculated as follows:
474
Munkh-Ulzii et al.

CSADt ¼ 1
N
X
N
i¼1
Ri;t  Rm;t


ð4Þ
where Rm;t

 is absolute value of market return at time (t), CSADt is a cross-sectional
absolute deviation at time (t). Chang et al. (2000) discussed that rational asset pricing
models suggest a linear relationship between the return dispersion in individual stock
returns and market return. In order words, when the absolute value of the market return
increases, return dispersion of individual stock also increases. During periods of
comparatively large market price movements, traders may behave in an uniform
manner, exhibiting herding behavior. Herding is likely to increase the correlation
among stock returns, and the related dispersion among returns will decrease, or at least
increase at a less-than-proportional rate with the portfolio average. Thus, Chang et al.
(2000) included a nonlinear market return - Rm;t

2 in their empirical model in Eq. (3),
and a signiﬁcantly negative coefﬁcient c2, would indicate the occurrence of herding
behaviour. As the market exhibits large price movements, investors more likely to
degrade their own beliefs and follow the trend of the market. Return dispersion of
individual stock return tend to either decrease or increase at a decreasing rate during
this conditions.
The calculation approach of return dispersion measure CSADt of this study follows
Christie and Huang (1995), Gleason et al. (2004), and Tan et al. (2008), but not Chang
et al. (2000). The reason of not following the method of Chang et al. (2000) is that their
measure relies on the accuracy of the speciﬁcation of a single market factor of the
CAPM, which may be questionable (Tan et al. 2008). Studies, usually avoided their
approach in measuring the CSADt for the same reason.
Thus, in the empirical testing of the hypothesis 1–3 we will base on the Eq. (3).
And, in testing hypothesis 7–9 we will use Eq. (4) for the calculation of the dependent
variable, but independent variables will have different speciﬁcations. Finally, in
examination of the hypothesis 10 we will use the Eqs. (1) and (2).
In the empirical examination of the hypothesis 1, the Eq. (5) will be utilized. The
Eqs. (3) and (5) are same, except in the Eq. (5) the idea of multiple markets is infused.
CSADi;t ¼ a þ c1 Ri;m;t

 þ c2 Ri;m;t

2 þ ei;t
ð5Þ
where CSADi;t is return dispersion of market (i) at time (t), a is an intercept, Ri;m;t

 is
absolute value of market return of market (i) at time (t),
Ri;m;t

2 is squared value of
market return of market (i) at time (t), ei;t is error term of market (i) at time (t). The
CSADi;t for each market will be calculated according Eq. (4). A statistically signiﬁcant
and negative coefﬁcient c2 would indicate the presence of herding behavior. In this
analysis data of stock markets indices and their constituents of China, Hong Kong,
Japan, South Korea, Singapore, and Taiwan, as well as Vietnam were used to analysis
the empirical model.
Empirical Models of Herding Behaviour
475

3.3.2
Measuring Asymmetric Herding Behavior (H2 and 3)
Measuring Asymmetric Herding Behaviour During Up and Down Market (H2)
Several studies also examined the asymmetric herding behavior, however document
different results. In all ﬁve markets (US, Hong Kong, Japan, South Korea and Taiwan)
that Chang et al. (2000) examined, the return dispersion of individual stock increased
as a function of the aggregate market return was higher during up market, compara-
tively to down market. Otherwise, investors exhibited herding behaviour in down
market time. These results were consistent with the directional asymmetry documented
by McQueen et al. (1996).
Using a dataset of NYSE (US) Gleason et al. (2004) found a weak presence of
asymmetric herding behaviour during up and down markets. However, they discussed
that investors may be more inclined to herd in down markets. Their ﬁndings indicated
that return dispersion need not necessarily to be identical, during periods of market
stress, in up and in down markets. According to McQueen et al. (1996) there is a
delayed reaction to good news, whereas market participants react more quickly to bad
news. In other word, it suggests that when traders react to bad news during down
markets have a great tendency to follow the aggregate market. Otherwise, investors
may afraid the potential loss during a down market time more than they enjoy the
potential gain during an up market period of stress. This phenomenon is also described
as ‘‘myopic loss aversion’’ (Benartzi and Thaler 1993). This type of trading behavior
leads to lower dispersion and the possibility of herding in down markets. In overall,
they concluded that their ﬁndings provided a weak support for hypothesis of myopic
loss aversion.
Examining China, A and B share markets Tan et al. (2008) found that herding
occurs in both rising and falling market conditions. Moreover, herding found to be
stronger, especially for the Shanghai A-share market, during up markets, high trading
volume states and high volatility states. However, there was a very weak asymmetry in
the B-share markets. They documented that the obvious difference in investor behavior
may be due to the different speciﬁcations of A and B share markets. A-share market is
dominated by local investors, whereas the B share market is dominated by foreign
investors.
Fu and Lin (2010) found an evidence of asymmetric reaction of herding that
investors’ tendency of the Shanghai and Shenzhen stock markets toward herding is
signiﬁcantly higher during down markets. My and Truong (2011) documented that in
stock market of Vietnam, upward markets have less return dispersion than downward
markets. In other word, investment behaviour in the stock market of Vietnam was more
uniformly in up markets than in down markets.
By analyzing 18 markets, which is consist of markets from regions of Latin
America, North America, Asia, Chiang and Zheng (2010) found that herding asym-
metry is more profound in Asian markets (Hong Kong, Japan, China, South Korea,
Indonesia, Malaysia, Singapore, Thailand, and Taiwan) during rising markets.
Lao and Singh (2011) found that herding behaviour is greater in Chinese stock
markets when the market is falling and the trading volume is high. Moreover, Bhaduri
and Mahapatra (2013) documented that herding behaviour is prevalent during up
476
Munkh-Ulzii et al.

markets, which is in line with ﬁndings of Tan et al. (2008), but their study used data
from Indian stock market. Finally, Yao et al. (2014) discussed that herding behaviour in
Chinese stock markets were more pronounced under conditions of declining markets.
Mobarek et al. (2014) measured asymmetric herding behavior among 11 European
markets by using dummy variable in detecting herding during up and down markets,
high and low trading volume states, and high and low market volatility states. Their
ﬁndings were somehow consistent with conclusion of McQueen et al. (1996). They
found evidence of herding in down markets. Moreover, they found that some evidence
of a signiﬁcant herding effect during low volume trading periods. Finally, they detected
a signiﬁcant herding during high and low volatility periods.
In overall, most of the works, almost all of which are studies on stock markets of
Confucian countries, found that herding behavior is more prevalent during down
market than up market. Few tests were conducted by using dummy variable, however
most of the studies employed the approaches speciﬁed in the Eqs. (6)–(9). Thus, we
will also follow these models. Since the direction of the market return can inﬂuence on
decisions of investors, we test possible asymmetric effects in herd behavior conditional
on whether the market is rising or falling. Thus, the following empirical model will be
estimated separately for positive and negative market returns:
CSADUP
i;t ¼ a þ cUP
1
RUP
i;m;t

 þ cUP
2
RUP
i;m;t

2
þ ei;t
If Ri;m;t [ 0
ð6Þ
CSADDOWN
i;t
¼ a þ cDOWN
1
RDOWN
i;m;t

 þ cDOWN
2
RDOWN
i;m;t

2
þ ei;t
if Ri;m;t\0
ð7Þ
where CSADUP
i;t is return dispersion of market (i) at time (t) when the market rises, a is
an intercept, RUP
i;m;t

 is absolute value of market return of market (i) at time (t) when the
market rises,
RUP
i;m;t

2
is squared value of market return of market (i) at time (t) when
the market rises, ei;t is error term of market (i) at time (t). Similarly, the variables with
superscript “DOWN” in the Eq. (7) refer to the scenario in which the market declines.
The CSADi;t for each market in the Eqs. (6) and (7) will be calculated according to the
Eq. (4). A statistically signiﬁcant and negative coefﬁcients cUP
2
or cDOWN
2
would indi-
cate the presence of herding behavior. In this analysis data of stock markets indices and
their constituents of China, Hong Kong, Japan, South Korea, Singapore, and Taiwan,
as well as Vietnam were used to analysis the empirical model.
Measuring Asymmetric Herding Behaviour During High and Low Volatility
States (H3)
Furthermore, we examine asymmetric effects of herding behavior in regard to volatility
of stock markets. As we did with trading volume, we characterize market volatility to
be high when the observed volatility gets higher than the moving average of volatility
over the previous 30 days. We deﬁne volatility as low when it does not exceed the
moving average of volatility over the previous 30 days. We have also checked moving
averages of 7 and 90 days, which is different from the settings of Tan et al. (2008), who
used much longer time periods of 60, 90 and 120 day moving averages. We designed
Empirical Models of Herding Behaviour
477

so, because studies discuss that sentiment of investors usually happens in short period
of time (Chang et al. 2000). The asymmetric effects are examined using the following
empirical models:
CSADd2HIGH
i;t
¼ a þ cd2HIGH
1
Rd2HIGH
i;m;t

 þ cd2HIGH
2
Rd2HIGH
i;m;t

2
þ ei;t
ð8Þ
CSADd2LOW
i;t
¼ a þ cd2LOW
1
Rd2LOW
i;m;t

 þ cd2LOW
2
Rd2LOW
i;m;t

2
þ ei;t
ð9Þ
where CSADd2HIGH
i;t
is return dispersion of market (i) at time (t) when market return
volatility is high, a is an intercept, Rd2HIGH
i;m;t

 is absolute value of market return of
market (i) at time (t) when market return volatility is high,
Rd2HIGH
i;m;t

2
is squared
value of market return of market (i) at time (t) when market return volatility is high, ei;t
is error term of market (i) at time (t). Similarly, the variables with the superscript
“d2  LOW” refer to the scenario in when market return volatility is low. The super-
script “d2” refers to market return volatility, and it is calculated as the standard devi-
ation of market return time with the square root of 252 trading days. The CSADi;t for
each market in the Eqs. (8) and (9) will be calculated according to the Eq. (4). A sta-
tistically signiﬁcant and negative coefﬁcients cd2HIGH
2
or cd2LOW
2
would indicate the
presence of herding behavior. In this analysis data of stock markets indices and their
constituents of China, Hong Kong, Japan, South Korea, Singapore, and Taiwan, as well
as Vietnam were used to analysis the empirical model.
4
Data Analysis
4.1
Results of Descriptive Statistics
Table 1 analysis descriptive statistics of return dispersion measure of CSAD of sample
markets. According to Table 1, stock markets of KOSDAQ (2.414891) and KOSPI
(2.162403) of Korea Exchange and HNX (2.222067) of Hanoi city of Vietnam have the
highest mean values, while they also have highest volatility as standard deviation of
0.8694664, 0.9576033, and 0.7862742 respectively. The lowest mean value got SHB
(1.107081) of Shanghai B market of China, while lowest volatility resulted to NI225
(0.5594453) of Tokyo Exchange. According to the Eq. (4) CSAD cannot have negative
values, thus we report only positive values. According to Table 4, the maximum value
of CSAD resulted to VNINDEX (10.95739) market of Hochimin city exchange of
Vietnam. Also, markets of KOSDAQ (8.036667) and KOSPI (10.22663) of Korea
Exchange have higher maximum values.
Also, Chang et al. (2000) discussed that Asian equity market returns and returns of
individual stocks are characterized by higher magnitudes of volatility with standard
deviations, which is also supports our results, since our study concentrates on Asian
stock markets, and they variability in terms of market return and return dispersion are a
478
Munkh-Ulzii et al.

quit high. Moreover, McQueen et al. (1996) and Gleason et al. (2004) discussed that
investors may fear potential losses in the downward market price movements more than
they enjoy the potential gains in the upward markets, which leads them to be likely to
follow herds, the consequence being a reduction in return dispersion. Since the return
dispersion (CSAD) tend to be low in most of the cases across our sample, except few
markets discussed before, the conclusions of McQueen et al. (1996) and Gleason et al.
(2004) also support our ﬁndings.
4.2
Results of ADF Unit Root Test
According to the studies on herding behavior, they checked the stationarity of their
dependent variable, which is return dispersion (CSAD) (Chang et al. 2000; Lao and
Singh 2011; Yao et al. 2014). Thus, this study has also checked the stationarity of the
dependent variable. The dependent variables of the empirical models of our study are
return dispersion measures of CSAD. Stationarity is checked by Dickey-Fuller test;
however, it may create a problem of autocorrelation. Thus, Dickey-Fuller developed a
test called Augmented Dickey-Fuller test. Thus, we used Augmented Dickey-Fuller
(ADF) test. Results of ADF test rejected the null hypothesis, otherwise it means that the
dependent variable of CSAD has no unit root and is stationary.
4.3
Results of the Empirical Models
Having studied about the background of herding behavior in stock markets we learnt
that investors make decision differently according to many factors, such as experience,
economic circumstances, or even weather situation and so on, which are all external
factors. Moreover, several studies highlighted the impacts of some common force on
herding behavior, which is identiﬁed as cultural aspects. If that is so, the one of the
strongest cultures that impacts on not only in individual and social life, but also on
business and economic world is the Confucian culture. Thus, we made an assumption
that Confucian culture has a positive impact on formation of herding behaviour among
investors in stock markets of Confucian countries. We did a comprehensive literature
Table 1. Descriptive statistics of return dispersion measure of CSAD
 VNINDEXCSAD        1879     1.71228    .6571653          0   10.95739
HNXINDEXCSAD        1899    2.222067    .7862742          0   5.575065
   TAIEXCSAD        3779    1.549348    .6564749          0   6.014132
     STICSAD        3679    1.175904    .5900925          0   5.554045
   KOSPICSAD        4168    2.162403    .9576033          0   10.22663
  KOSDAQCSAD        3778    2.414891    .8694664          0   8.036667
   TOPIXCSAD        3913    1.455059    .6241186          0   6.133608
   NI225CSAD        3587    1.252892    .5594453          0   4.767389
     HSICSAD        3610    1.204146    .5875868          0   7.300818
     SZBCSAD        4174    1.272493     .693877          0   5.287333
     SZACSAD        3877    1.453948    .6842355          0   6.678263
     SHBCSAD        4174    1.107081    .6864486          0    5.12972
     SHACSAD        3982    1.504224    .7679289          0   7.220784
                                                                      
    Variable         Obs        Mean    Std. Dev.       Min        Max
Empirical Models of Herding Behaviour
479

review on topics of herding behavior and Confucian aspects of business and eco-
nomics, which led us to develop a consequent ﬁve hypotheses. These empirical models
are designed to test whether the chosen sample of Confucian markets exhibit herding
behavior in general and different market states, as well as whether investors of those
markets herd during special events. Also, this study tested relationship between
behavioral states and exhibition of herding behavior. In addition, as we mentioned
before, a robustness regression analysis with Newey-West standard errors (1987) was
conducted. Following studies on herding behavior, we present the key values and
coefﬁcients of our empirical analysis, such as the total number of observations in each
model, adjusted R2, coefﬁcients of intercept, variables, and test statistics, as well as
p-value. In overall, we have gotten quit good results. Thus, this section discusses about
the results of our empirical tests through Tables 2, 3, 4, 5 and 6.
4.3.1
Results of Measurement Herding Behavior (H1)
Hypothesis 1 assumes that herding behaviour exists among the Confucian markets.
Table 2 reports results of the empirical testing the hypothesis 1, which was examined
by the model in the Eq. (5). The independent variables of the model consist of market
return, while the dependent variable is return dispersion measure of CSAD. According
to the deﬁnition of the model in Eq. (5), a statistically signiﬁcant and negative coef-
ﬁcient c2 would indicate herding behavior. All sample markets were examined by the
model. The empirical model has been tested with OLS regression and regression with
Newey-West standard errors (1987). The intercept a shows the location of return
dispersion measure at 0 point. The coefﬁcient c1 shows an increasing function of
market return in absolute terms to the return dispersion measure.
OLS Regression: According to Panel A in Table 2, the coefﬁcients of c2 are statis-
tically signiﬁcant at 1% and negative for all markets, except HSI, TOPIX, and STI. The
Table 2. Results of general herding test (H1)
480
Munkh-Ulzii et al.

Table 3. Results of analysis of herding behaviour during UP markets (H2UP)
Table 4. Results of analysis of herding behaviour during DOWN markets (H2DOWN)
Empirical Models of Herding Behaviour
481

Table 5. Results of analysis of herding behaviour during HIGH volatility states (H4HIGH VOL)
Table 6. Results of analysis of herding behaviour during LOW volatility states (H4LOW VOL)
482
Munkh-Ulzii et al.

coefﬁcient for the markets of HSI, TOPIX, and STI are negative and statistically
signiﬁcant at 5%.
The coefﬁcients of t-statistics of c2 are in line with the results of p-values; the
highest value in t-statistics has SHB market (−22.86), while lowest t-statistics value
resulted to STI market (−2.17). The highest adjusted R2 value resulted to the model of
HSI market (.3815), while the model of VNINDEX market (.1548) appears to have the
lowest adjusted R2.
Robustness Regression: Results of the robustness regression with Newey-West
standard errors (1987) in Panel B in Table 2 show that the coefﬁcient c2 for all markets
are negative, however, the p-values for the markets of HSI and STI are not statistically
signiﬁcant. P-values of rest of the markets are statistically signiﬁcant at 1% and 5%
(TOPIX). The coefﬁcients c2 of t-statistics of the robustness test are in line with the
results of p-values; the highest value in t-statistics has SHB market (−22.42), while
lowest t-statistics value resulted to HSI market (−0.81).
Summary and Decision: In overall, according to the results of the model in Panel A
and Panel B in Table 2, we accept the hypothesis 1.
Our ﬁndings from the testing of hypothesis 1 are consistent with the ﬁndings of
similar studies, such as Tan et al. (2008), Chiang et al. (2010), Chiang and Zheng
(2010), Demirer et al. (2010), Lao and Singh (2011), Bhaduri and Mahapatra (2013),
Mobarek et al. (2014), Yao et al. (2014), and Chang and Lin (2015). These studies
found some evidence of herding behavior among stock markets investors by using the
methodology of Chang et al. (2000), which is shown in the Eq. (3). The half of these
studies concentrated in Confucian markets, such as China, Taiwan, and South Korea.
While there is another bunch of studies used the model of Chang et al. (2000) in the
Eq. (3), however did not ﬁnd evidence of herding. The half of those studies also
targeted on Confucian markets, such as Vietnam and China.
By testing the hypothesis 1 with the empirical model in the Eq. (5), this study found
an overwhelming evidence of herding behavior among the Confucian markets. How-
ever, previous studies tested herding among investors of Confucian markets, they used
to concentrate only on a single country, such as China or Taiwan. However, our study
brings all Confucian markets forward, which appears to be one of the contributions of
this study to the existing literature of herding.
Discussion: The coefﬁcient c2 is negative and statistically signiﬁcant for all markets.
Otherwise, the linear relation between CSADt and Ri;m;t

 obviously does not hold in
our models. In order illustrate the relationship, we take the following general quadratic
relationship between CSADt and Ri;m;t

:
CSADi;t ¼ a þ c1 Ri;m;t

 þ c2 Ri;m;t

2
ð10Þ
According to the quadratic relationship, CSADt reaches its maximum value when
Ri;m;t

¼ 
c1
2c2


. In other words, when Ri;m;t

 increases over the range where realized
average daily returns in absolute terms are less than Ri;m;t

, then CSADt is still in an
Empirical Models of Herding Behaviour
483

increasing trend. However, as the value of Ri;m;t

 exceeds the value of Ri;m;t

, then
return dispersion measure CSAD starts to increase at a decreasing rate. For instance,
substituting the estimated coefﬁcients for Taiwan Stock Exchange (c1 = 0.5884697 and
c2 = −0.0717432) into the quadratic relation speciﬁed in the Eq. (10) indicates that
CSADt reaches a maximum when Ri;m;t

 = Ri;m;t

 = 4.10%. This suggests that during
large price movements in the market return that surpass the threshold level Ri;m;t

, the
CSADt increases at decreasing rate as illustrated in the Fig. 1 (Chang et al. 2000).
Furthermore, besides statistical signiﬁcance and negativity of the coefﬁcient c2, the
size of the coefﬁcient also shows magnitude of herding behavior in each market (Lao
and Singh 2011). In this case, size of the coefﬁcient c2 is larger in markets of TAIEX
(−0.072), HNX (−0.076), and VNINDEX (−0.174). In contrast, size of the coefﬁcient
is smaller in markets of STI (−0.01), NI225/TOPIX (−0.006), and HSI (−0.004). The
differences in size of the coefﬁcient show that herding behavior is more prevalent in the
markets of TAIEX, HNX, and VNINDEX, which are emerging markets. While, the
advanced markets of Singapore, Japan, and Hong Kong display much less herding
behavior.
4.3.2
Results of Measurement of Herding Behavior During Up and Down
Markets (H2)
Hypothesis 2 assumes that asymmetric herding behaviour exists among the Confucian
markets when the market rises and falls. Tables 3 and 4 reports results of the empirical
testing the hypothesis 2, which was examined by the models in the Eq. (6), which is for
up markets and Eq. (7), which is for down markets. According to the deﬁnition of the
models in Eqs. (6) and (7), statistically signiﬁcant and negative coefﬁcients cUP
2
and
cDOWN
2
would indicate herding behavior in up and down markets. All sample markets,
which are totally 13 market, were examined by the models. The empirical models have
been tested with OLS regression and regression with Newey-West standard errors
0
2
4
6
TAIEX CSAD
-10
-5
0
5
10
TAIEXRm
Fig. 1. Relationship between the daily return dispersion (CSADi;t) and the corresponding
equally-weighted market return (Ri;m;t) for TAIEX of Taiwan Stock Exchange.
484
Munkh-Ulzii et al.

(1987). The intercept a shows the location of return dispersion measure at 0 point. The
coefﬁcient c1 shows an increasing function of market return in absolute terms to the
return dispersion measure.
OLS Regression (UP MARKET): According to Panel A in Table 3, the coefﬁcients
of c2 for the whole sample during up markets are statistically signiﬁcant at 1% and
negative. The highest value of the adjusted R2 during up markets is for HSI market
(.4388), while the lowest value for it during up markets resulted to SHA market
(.2356). The coefﬁcients of t-statistics of c2 are in line with the results of p-values; the
highest value in t-statistics during up markets resulted to SZB market (−22.42), while
lowest value is for STI market (−2.88).
OLS Regression (DOWN MARKET): According to Panel A in Table 4, the coef-
ﬁcient c2 during down markets is statistically signiﬁcant at 1% and negative for eight
markets out of 13 markets. The coefﬁcient c2 of markets of STI, TOPIX, NI225, and
HSI and positive, as well in most of the cases statistically not signiﬁcant. However the
coefﬁcient c2 for SZA market is negative, it is not statistically signiﬁcant.
The highest value of the adjusted R2 during down markets is for KOSDAQ market
(.4146), while the lowest value for adjusted R2 is for VNINDEX market (.0642). The
coefﬁcients of t-statistics of c2 are in line with the results of p-values; the highest value
in t-statistics during down markets resulted to HNX market (−8.65).
Robustness Regression (UP MARKET): Results of robustness regression in Panel B
in Table 6 show that coefﬁcients c2 during up markets of 11 indices out of 13 markets
are statistically signiﬁcant at 1% and negative. The coefﬁcients of markets of HSI and
STI are negative but statistically not signiﬁcant.
The highest value of t-statistics for the coefﬁcient c2 during up markets has SZB
(−24.63), while lowest value resulted to HSI market (−1.60).
Robustness Regression (DOWN MARKET): According to Panel B in Table 4, the
coefﬁcients c2 during down markets are statistically signiﬁcant at 1% and negative for
seven markets, while the coefﬁcient for SHA is statistically signiﬁcant at 5%. The
coefﬁcients of rest of the markets (SZA, HSI, NI225, TOPIX, STI) are either statisti-
cally not signiﬁcant or positive.
The highest value of t-statistics for the coefﬁcient c2 during down markets resulted
to SHB market (−8.32), while lowest value resulted to SZA (−0.01).
Summary and Decision: In overall, according the results of the main regression and
robustness regression analysis, investors more exhibit herding behavior during up
markets than down markets. However, over 50% of down markets also displays
herding behavior in both regression tests. Thus, we accept the hypothesis 2.
Our results contradict ﬁndings of others studies, which discussed that herding
behavior is more prevalent during down markets (McQueen et al. 1996; Chang et al.
2000; Gleason et al. 2004; Fu and Lin 2010; Lao and Singh 2011; Yao et al. 2014;
Mobarek et al. 2014). However, the results of hypothesis 2 examination are consistent
with the ﬁndings of studies of Tan et al. (2008), Chiang and Zheng (2010), My and
Truong (2011), and Bhaduri and Mahapatra (2013), which found that herding behavior
Empirical Models of Herding Behaviour
485

more occurs during up market states. The studies, whose results are in line with our
ﬁndings, used data from Asian markets.
4.3.3
Results of Measurement of Herding Behavior During High
and Low Volatility States (H3)
Hypothesis 3 postulates that asymmetric herding behaviour exists among the Confucian
markets during high and low volatility states of markets. Tables 5 and 6 report results
of the empirical testing the hypothesis 3, which were examined by the empirical models
in the Eq. (8), which is for high volatility states and Eq. (9), which is for low volatility
states of sample markets. According to the deﬁnition of the empirical models in
Eqs. (8) and (9), statistically signiﬁcant and negative coefﬁcients cd2HIGH
2
and cd2LOW
2
would indicate herding behavior during high and low volatility states. All sample
markets were examined by these models. The empirical models have been tested with
OLS regression and regression with Newey-West standard errors (1987). The intercept
a shows the location of return dispersion measure at 0 point. The coefﬁcient c1 shows
an increasing function of market return in absolute terms to the return dispersion
measure.
OLS Regression (HIGH VOLATILITY): According to Panel A in Table 5, the
coefﬁcients of cd2HIGH
2
for all markets, except the markets of HSI, NI225, TOPIX, and
STI, are statistically signiﬁcant at 1% and negative. Which means herding behavior
exist during high volatility states in nine markets out of 13 markets. The coefﬁcient for
non-linearity for the markets of HSI, NI225, TOPIX, and STI are either statistically not
signiﬁcant or positive, which means they do not exhibit herding behavior during high
volatility states.
The highest value of adjusted R2 during the high volatility states resulted to the
model of HSI market (0.4294), while lowest value got to the model of VNINDEX
(0.1863) market. Moreover, the highest value in t-statistics during the high volatility
states resulted to the model of SZB market (−16.21), while the lowest value got to the
model of HSI market (0.36).
This study examined the Eq. (8) with moving averages of 7, 30, and 90 days,
however Table 5 reports only the moving averages of 30 days. The results of 7 and 90
day moving averages are consistent with results of the coefﬁcients of 30-day moving
average.
OLS Regression (LOW VOLATILITY): According to Panel A in Table 6, the
coefﬁcient cd2LOW
2
of the empirical model in the Eq. (9) is statistically signiﬁcant at
1% and negative for all sample markets, except market of STI. The coefﬁcient for STI
is negative but not statistically signiﬁcant. This results means that herding behavior
exist among the sample markets of this study during low volatility states of market.
The highest value in t-statistics during the low volatility states resulted to SHB
market (−12.89), while the lowest value resulted to STI market (−0.40). The highest
value in adjusted R2 during the low volatility states resulted to the model of KOSPI
market (0.3250), while the lowest value resulted to the model of VNINDEX market
(0.1117).
486
Munkh-Ulzii et al.

This study examined the Eq. (9) with moving averages of 7, 30, and 90 days,
however Table 6 reports only the moving averages of 30 days. The results of 7 and 90
day moving averages are consistent with results of the coefﬁcients of 30-day moving
average.
Robustness Regression (HIGH-VOLATILITY): According to Panel B in Table 5,
the coefﬁcient cd2HIGH
2
is statistically signiﬁcant and negative for all markets, except
markets of HSI, NI225, TOPIX, and STI. The coefﬁcient cd2HIGH
2
for the markets of
HSI, NI225, TOPIX, and STI are statistically not signiﬁcant and positive.
The highest value in t-statistics resulted to SZB market (−16.14), while the lowest
value is for HSI market (0.14). In overall, nine markets out of 13 markets exhibit
herding behavior during high volatility states.
This study examined the Eq. (8) with moving averages of 7, 30, and 90 days,
however Table 10 reports only the moving averages of 30 days. The results of 7 and 90
day moving averages are consistent with results of the coefﬁcients of 30-day moving
average.
Robustness Regression (LOW-VOLATILITY): According to Panel B in Table 6,
the coefﬁcient cd2LOW
2
of the empirical model in the Eq. (9) is statistically signiﬁcant
and negative for all sample markets, except market of STI. The only coefﬁcient for
non-linearity of HSI market is statistically signiﬁcant at 5%, however the coefﬁcients
for rest of the markets are statistically signiﬁcant at 1%. The coefﬁcient for STI is
negative but statistically not signiﬁcant. The results means that herding behavior exist
among the sample markets of this study during low volatility states of market.
The highest value in t-statistics during the low volatility states resulted to SHB
market (−12.09), while the lowest value resulted to STI market (−0.26).
This study examined the Eq. (9) with moving averages of 7, 30, and 90 days,
however Table 6 reports only the moving averages of 30 days. The results of 7 and 90
day moving averages are consistent with results of the coefﬁcients of 30-day moving
average.
Summary and Decision: According to the results in Tables 10 and 11, nine markets
exhibit herding behavior out of 13 markets when the market volatility is high, while 12
markets display herding behavior out of 13 markets when market volatility is low. And,
these outcomes are exactly in line with the results of robustness regression analysis. In
orders words, more than 69.2% of the sample of this study exhibit herding behavior
during periods of high and low volatility states of market, however herding is more
prevalent when the market volatility is low. Thus, we accept the hypothesis 4.
Discussion on the Results of Asymmetric Herding Tests: This section has discussed
about results of hypotheses 2 and 3, which were correspondingly tested by the
empirical models in the Eqs. (6) and (7) and Eqs. (8) and (9). Generally, we found
herding behavior overwhelmingly from all sample markets, in all speciﬁcations of our
empirical models. However, if we look the results more in detail we can see that
herding behavior is more profound: in up markets than down markets; herding behavior
is more prevalent during low trading volume states than high trading volume states; and
herding behavior is stronger during low volatility states than high volatility states.
Empirical Models of Herding Behaviour
487

These results in contradiction with ﬁndings of some studies, while they are in agree-
ment of others studies as discussed below.
Some studies that examined asymmetric herding behavior found herding in both of
rising and falling markets. However, they concluded that herding is more prevalent
during declining markets, high trading volume states and high volatility states
(McQueen et al. 1996; Chang et al. 2000; Gleason et al. 2004; Tan et al. 2008; Fu and
Lin 2010; Lao and Singh 2011; Mobarek et al. 2014; Yao et al. 2014).
While, studies also found contradicting results. For example, while they found
herding in falling markets Tan et al. (2008) also found herding from rising markets.
Chiang and Zheng (2010) found that herding asymmetry is more profound in Asian
markets (Hong Kong, Japan, China, South Korea, Indonesia, Malaysia, Singapore,
Thailand, and Taiwan) during rising market states. My and Truong (2011) found that
investment behaviour in the stock market of Vietnam was more uniformly in up
markets. Bhaduri and Mahapatra (2013) found herding is more prevalent during up
markets in Indian stock market. Mobarek et al. (2014) measured asymmetric herding
behavior among 11 European markets by using dummy variable in detecting herding
during up and down markets, high and low trading volume states, and high and low
market volatility states. Moreover, they found that some evidence of a signiﬁcant
herding effect during low volume trading periods.
We also make a brief conclusion as follows. First, investors of sample markets
found to exhibit more uniform behavior during up markets because up market is a bull
market. During bull-market investors are more optimistic in trading. Bull market and
investor optimism do not mean that investor make decision solely on they own. When
investors too optimistic they also tend to exhibit risky behavior by doing risky
investment decisions, such as buying too many stock either based on they own decision
or based on decisions made by other traders (Kurov 2010). Thus, bull market states can
also be a stepping-stone of possible herding behavior, which has been shown by
empirical results of this study.
Second, investors of sample markets of this study found to display more herding
behavior during low trading volume states than high trading volume states. According
to studies in market sentiments, one of the common features of falling markets is low
trading volume state. When the market is falling and the trading volume is low, there is
a great uncertainty in the market. Because trading volume is falling the investors in the
market would think that the market is also falling, thus they would probably decide to
convert shares into cash by selling them out as soon as possible (Jansen and Tsai 2010).
Therefore, this kind of investment sentiments make the fear on the market more
contagious, ﬁnally end up with formation of herding behavior when the trading volume
gets low.
Finally, investors of sample markets of this study exhibit more herding behavior
during low volatility states of market. Low volatility state of a market is similar
condition to up or rising markets. In order words, these features of a market are kind of
positive and pleasant market condition for investors. As discussed before, when
investors feel positive about the market they are more willing to invest more, even they
are ready to make consensus decisions with others, in results of what herding behavior
will happen.
488
Munkh-Ulzii et al.

5
Conclusion
The empirically tested hypothesis 1 assumed that a herding behaviour exists among the
Confucian markets. The empirical test was conducted on all markets in our sample.
According to our empirical ﬁndings, the hypothesis 1 was accepted. We found a
signiﬁcant herding behaviour from the entire sample population, no matter whether
they are emerging or advance markets. Thus, our ﬁndings contradict the existing theory
on herding, which discusses that herding behavior hardly exist among advanced
markets, while support the rationale of this study that Confucian culture has a positive
inﬂuence on formation of herding behavior.
The empirically tested hypotheses 2 and 3 correspondingly assumed that, ﬁrst, an
asymmetric herding behaviour exists among the Confucian markets during (H2) the
rising and falling markets and (H3) during high and low volatility states of markets. The
empirical tests have been conducted on whole sample markets. According to our
empirical ﬁndings the hypotheses 2 and 3 are accepted. The details of our ﬁndings are
broken down in the next paragraphs.
Generally, this study detected a signiﬁcant herding behaviour from the entire
sample in each state of market, where they are speciﬁed with high and low returns and
volatility. However, detail results show that herding behaviour is more profound in
(1) up markets than down markets; and (2) during low volatility states than high
volatility states.
We try to deliberate the ﬁndings a bit more here. First, investors of sample markets
display more uniform behaviour in up markets, because up market is a bull market.
During bull markets, investors are more forward looking and optimistic in trading.
Investors’ optimism in bull markets does not mean that there is no herding behaviour.
When investors get to be too optimistic, they also tend to make risky investment
decisions, such as buying high, either based on their own decision or based on deci-
sions made by other traders. Thus, bull market states have high probability to have a
positive effect on formation of herding behaviour, which has been eventually
demonstrated by the empirical ﬁndings of this study.
Second, we found that investors exhibit more herding behavior in low volatility
states of market. When volatility is low, a market rather heads upward, which means it
has similar features with a bull market. Otherwise, an up market exhibits positive and
pleasant conditions for investors. Therefore, as investors feel positive about the market,
they become more willing to make more investment, and even sometimes they join into
a group decision, in results of what herding behavior happens.
References
Aggarwal, R., Kearney, C., Lucey, B.: Gravity and culture in foreign portfolio investment.
J. Bank. Fin. 36(2), 525–538 (2012)
Beckmann, D., Menkhoff, L., Suto, M.: Does culture inﬂuence asset managers’ views and
behavior? J. Econ. Behav. Organ. 67(3), 624–643 (2008)
Bhaduri, S.N., Mahapatra, S.D.: Applying an alternative test of herding behavior: a case study of
the Indian stock market. J. Asian Econ. 25, 43–52 (2013)
Empirical Models of Herding Behaviour
489

Chang, E.C., Cheng, J.W., Khorana, A.: An examination of herd behavior in equity markets: an
international perspective. J. Bank. Fin. 24(10), 1651–1679 (2000)
Chang, C.H., Lin, S.J.: The effects of national culture and behavioral pitfalls on investors’
decision-making: herding behavior in international stock markets. Int. Rev. Econ. Fin. 37,
380–392 (2015)
Chiang, T.C., Zheng, D.: An empirical analysis of herd behavior in global stock markets. J. Bank.
Fin. 34(8), 1911–1921 (2010)
Chiang, T.C., Li, J., Tan, L.: Empirical investigation of herding behavior in Chinese stock
markets: evidence from quantile regression analysis. Glob. Fin. J. 21(1), 111–124 (2010)
Chow, C.W., Shields, M.D., Chan, Y.K.: The effects of management controls and national
culture on manufacturing performance: an experimental investigation. Account. Organ. Soc.
16(3), 209–226 (1991)
Christie, W.G., Huang, R.D.: Following the pied piper: do individual returns herd around the
market? Finan. Anal. J. 51(4), 31–37 (1995)
Demirer, R., Kutan, A.M.: Does herding behavior exist in Chinese stock markets? J. Int. Finan.
Mark. Inst. Money 16(2), 123–142 (2006)
Demirer, R., Kutan, A.M., Chen, C.D.: Do investors herd in emerging stock markets? Evidence
from the Taiwanese market. J. Econ. Behav. Organ. 76(2), 283–295 (2010)
Economou, F., Kostakis, A., Philippas, N.: Cross-country effects in herding behaviour: evidence
from four south European markets. J. Int. Finan. Mark. Inst. Money 21(3), 443–460 (2011)
Ferris, S.P., Jayaraman, N., Sabherwal, S.: CEO overconﬁdence and international merger and
acquisition activity. J. Finan. Quant. Anal. 48(01), 137–164 (2013)
Fu, T., Lin, M.: Herding in China equity market. Int. J. Econ. Finan. 2(2), 148 (2010)
Gleason, K.C., Mathur, I., Peterson, M.A.: Analysis of intraday herding behavior among the
sector ETFs. J. Empir. Finan. 11(5), 681–694 (2004)
Holderness, C.G.: The myth of diffuse ownership in the United States. Rev. Finan. Stud. 22(4),
1377–1408 (2009)
Hofstede, G.H.: Culture’s Consequences: Comparing Values, Behaviors. Institutions and
Organizations Across Nations. Sage, Thousand Oaks (2001)
Hwang, S., Salmon, M.: Market stress and herding. J. Empir. Finan. 11(4), 585–616 (2004)
Jansen, D.W., Tsai, C.L.: Monetary policy and stock returns: ﬁnancing constraints and
asymmetries in bull and bear markets. J. Empir. Finan. 17(5), 981–990 (2010)
Kurov, A.: Investor sentiment and the stock market’s reaction to monetary policy. J. Bank. Finan.
34(1), 139–149 (2010)
Lao, P., Singh, H.: Herding behaviour in the Chinese and Indian stock markets. J. Asian Econ. 22
(6), 495–506 (2011)
Li, K., Grifﬁn, D., Yue, H., Zhao, L.: How does culture inﬂuence corporate risk-taking?
J. Corp. Finan. 23, 1–22 (2013)
McQueen,
G.,
Pinegar,
M.,
Thorley,
S.:
Delayed
reaction
to
good
news
and
the
cross-autocorrelation of portfolio returns. J. Finan. 51(3), 889–919 (1996)
Mobarek, A., Mollah, S., Keasey, K.: A cross-country analysis of herd behavior in Europe. J. Int.
Finan. Mark. Inst. Money 32, 107–127 (2014)
My, T.N., Truong, H.H.: Herding behavior in an emerging stock market: empirical evidence from
Vietnam. Res. J. Bus. Manag. 5(2), 51–76 (2011)
Newey, W.K., West, K.: A simple positive semi-deﬁnite, heteroskedasticity and autocorrelation
consistent covariance matrix. Econometrica 55, 703–708 (1987)
Siegel, J.I., Licht, A.N., Schwartz, S.H.: Egalitarianism and international investment. J. Finan.
Econ. 102(3), 621–642 (2011)
Tan, L., Chiang, T.C., Mason, J.R., Nelling, E.: Herding behavior in Chinese stock markets: an
examination of A and B shares. Paciﬁc-Basin Finan. J. 16(1), 61–77 (2008)
490
Munkh-Ulzii et al.

Treynor, J., Mazuy, K.: Can mutual funds outguess the market? Harvard Bus. Rev. 44(4), 131–
136 (1966)
Xue, M., Cheng, W.: National culture, market condition and market share of foreign bank. Econ.
Model. 33, 991–997 (2013)
Yao, J., Ma, C., He, W.P.: Investor herding behaviour of Chinese stock market. Int. Rev. Econ.
Finan. 29, 12–29 (2014)
Empirical Models of Herding Behaviour
491

Forecasting the Growth of Total Debt Service
Ratio with ARIMA and State Space Model
Kobpongkit Navapan1, Petchaluck Boonyakunakorn1(B),
and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
kobpongkit.nav@gmail.com, petchaluckecon@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
songsakecon@gmail.com
Abstract. Since the global ﬁnancial crisis erupted in September 2008,
many recent economists have been worried about the health of ﬁnan-
cial institutions. Consequently, many recent researches have put great
emphasis on study of total debt service ratio (TDS) as one of the early
warning indicators for ﬁnancial crises. Accurate TDS forecasting can
have a huge impact on eﬀective ﬁnancial management as a country can
monitor the signal of ﬁnancial crisis from a TDS’s future trend. There-
fore, the purpose of this paper is to ﬁnd the modeling to forecast the
growth of TDS. Autoregressive integrated moving average (ARIMA)
models tends to be the most popular forecasting method with indispens-
able requirement of data stationarity. Meanwhile, State Space model
(SSM) allows us to examine directly from original data without any
data transformation for stationarity. Furthermore, it can model both
structural changes or sudden jumps. The empirical result shows that the
SSM expresses lower prediction errors with respect to RMSE and MAE
in comparison with ARIMA.
Keywords: Total debt service ratio
Autoregressive integrated moving average model · State Space Model
1
Introduction
The global ﬁnancial crisis in September 2008 is recognized as one of the worst
ﬁnancial crises since the Great Depression of the 1930s. It has pushed economies
around the world into recession. Lots of concerns have been placed on studies of
household debt level, as household debts can be related to loans that ﬁnancial
institutions lent to the individual in a country. The individual may use the loans
with a variety of reasons, such as personal consumption or business purposes.
Subsequently, an increased income would not only reﬂect to the individual’s
abilities of loan repayments, but also contribute to economic growth through
loans. However, if excessive private sector debt remains for a long time, it may
adversely aﬀect economic stability.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_35

Forecasting the Growth of TDS with ARIMA and State Space Model
493
One of the useful indicators for growing accretion of ﬁnance’s vulnerabilities
in recent economy is the total debt service ratio (TDS), which is fundamentally
known as a key player of measurement of the ﬁnancial limitations caused by
private sector liability. Analyzing TDS time series intensively generates many
advantages to both ﬁnancial and economic forecasting, especially as an early
warning indicator (EWI) for banking crisis. Recently there have been a lot of
literature on EWIs of economic crisis, but there are a few of them focusing on
accurate prediction ability. This paper is to fulﬁll the gap in this literature ﬁeld.
Drehmann and Juselius [1] stated that the debt service ratio (DSR) can be
used as a very useful EWI to forecast forthcoming systemic banking crises within
one or two years in advance. Accordingly, the government agency or the central
bank ideally uses total debt service ratio (TDS) to supervise ﬁnancial institutions
to enforce or seek cooperation in case not to let ﬁnancial institutions to make
loans to borrowers. An advantages of DSR in comparison with other leverage
measures like debt-to-GDP ratio is to provide more accurate prediction, because
it explicitly captures signiﬁcant movements, specially interest rate movement of
maturities that undermine repayment ability. Moreover, it was found that time
series analysis of TDS prior to economic crashes can generate some information
which is related to the size of the subsequent output losses and this characteristic
can be used as an important indicator to warn for impending systemic banking
crises. Drehmann and Juselius [2] showed that both the credit-to-GDP and DSR
consistently outperform other measure, whereas the ﬁrst one tends to be the
best indicator for longer period whereas, the second dominates shorter period.
Furthermore Kida [9]’s study also showed that a decrease in TDS can be
contributed by an increase in income. Relatively, if the TDS ratio is high, it can
be referred to the state that a country’s international ﬁnances are not healthy.
The ratio should be at the range of 0 to 20 %. One of the case study examples
carried out by Faruqui [6] referred what happened in Canada. It was reported in
the Bank of Canada’s Financial System Review that the TDS continued rising
but still below its historical average in the fourth quarter in 2007 suggesting that
households’ debt burden is still considered at manageable levels.
Moreover, a high level of TDS can cause a signiﬁcantly negative impact on
consumption and investment, proposed by Bank for International Settlements
(BIS). There are many countries in Asia, for example Malaysia, and Hong Kong,
where TDS is used as a ﬁnancial tool to solve the debt problem. In Malaysia, it is
used to deal with household or bubble in the economy with respect to vulnerable
borrowers. Whereas it is used to deal with Hong Kong’s bubble in real estate
prices.
The measure of DSR is basically deﬁned as interest payments and debt repay-
ments divided by income. With this formula it provides advantageous ability to
deﬁne factors such as changes in interest rate or maturities that aﬀect borrowers’
repayment capacity. Consequently, this advantage makes the DSR better than
other established leverage measures, such as the debt-to-GDP ratio (Drehmann
and Juselius [3]).

494
K. Navapan et al.
The DSR’s properties have been explored to constructed TDS at the aggre-
gate level with a basic idea of the measure that debt service cost−interest and
amortizations−on the aggregate debt stock are repaid at a given lending rate in
equal portions over the maturity of the loan (installment loans). The justiﬁcation
for the change deﬁnes that the diﬀerences between the repayment structures of
individual loans will be canceled in the aggregate.
After exploring the basic formula for calculating the ﬁxed debt service costs
(DSC) of an installment loan and dividing it by income Yjt, TDS formula is
mathematically written as in the following equation. Furthermore, it can be
interpreted into terms at aggregate level with the TDS for sector j at time t and
it is expressed as
TDSjt = DSCt
Yt
=
ijt
(1 −(1 + ijt)−sjt)
∗Djt
Yjt
,
(1)
where Djt is the total stock of debt, ijt indicates the average interest rate on
the existing stock of debt per quarter, Yjt indicates quarterly income, and sjt
indicates the average remaining maturity in quarters.
Accurate TDS ratio prediction tends to have a signiﬁcant impact on eﬀec-
tive ﬁnancial management, which relatively contribute to economic stability. A
signal of ﬁnancial crisis from a TDS’s future trend would be one of the supple-
mentary information for impending ﬁnancial crises. Therefore, the forecasting of
the growth of DSR ratio should be paid more attention.
Among of forecasting models, we begins with an autoregressive moving aver-
age (ARMA) models for investigating the time series data, as ﬁrstly introduced
by Box and Jenkins. ARMA only requires historical time series data to calculate
along with an basic assumption of stationarility in data, which is that all mean,
variance, and covariance should be constant over time.
Another point is that dynamic linear models (DLMs) as known as linear State
Space Models (SSM) can be suitable for modeling with a wide range of sequence
univariate or multivariate time series data. It provides more ﬂexibilities than
ARIMA in term of treating non-stationary data with instability in the mean
and variance, model structural changes or sudden jumps. Additionally, SSM is
easy to interpret and understand the results (Petris et al. [13]). SSM allows us
to investigate directly from time series data. This is more natural to model time
series data without its preliminary transformation.
As stated by Jacques and Siem [7], it was demonstrated that the ARIMA
approach consists of a problem as the stationary of real time series data is hardly
exist, especially in ﬁelds of economics, ﬁnance and social. Therefore, they suggest
using SSM instead of ARIMA since SSM is never required stationarity time
series data. SSM provides a powerful probabilistic structure, oﬀering a ﬂexible
framework for many applications. Subsequently, the application of SSM in time
series analysis has been widely increased, for instance, the studies of Durbin and
Koopman [4] and Migon et al. [10].

Forecasting the Growth of TDS with ARIMA and State Space Model
495
ARMA can be represented as SSM. That may be usefully regarded the bene-
ﬁts of dynamic linear models. A recent study carried out by Omekara et al. [11]
provides a support that SSM provides more adequate than ARIMA approaches.
Therefore, this paper we will compare the predicted ability of ARIMA with SSM
with respect to the growth of DSR. The following is divided into 5 sections. The
second section focuses on methodology, meanwhile the third section describes
the studied data. Empirical results are in the forth section and conclusion is in
the last section.
2
Methodology
2.1
Autoregressive Integrated Moving Average (ARIMA) Model
It is common to ﬁnd dependence in time series data. The primary objective of
ARIMA model is to model this dependence and investigate univariate stochastic
time series data. It is widely applied for forecasting since it oﬀers great ﬂexibility
in forecasting of time series with many supportive evidences from academic stud-
ies. Moreover, it is a convenience methodology in terms of using only historical
data.
ARMA model be composed of the Autoregressive (AR) and Moving average
(MA) models. The AR refers to the level of its current observations rely on
the level of its lagged observations. In time series data, the MA refers to the
observations of a random variable at current time t can be inﬂuenced by both
the shock at time t, and the shocks occurred before the time. The stationary
time series Yt is ARMA(p, q), which can be written as;
Yt = u +
p

j=1
φjYt−j +
q

j=1
ψjεt−j + εt
(2)
where εt is white noise process with variance σ2 and the parameter φ1, ..., φp
satisfy a stationary condition. ARMA model requires a stationary time series,
which indicates that the mean, variance, and covariance should be constant over
time. Otherwise it will result in spurious regression leading to biased results. The
integrated autoregressive moving average (ARIMA) model will be applicable if
the time series is non stationary. Therefore, the time series Yt will be ARIMA
(p, d, q) model. The time series is diﬀerent stationary, where d the number of
times is applied to diﬀerence a process before it is stationary. We use Autocor-
relation function (ACF) and Partial Autocorrelation function (PACF) to guide
us the lag of p and q. The best ﬁtted model is conﬁrmed AIC criterion.
2.2
The State Space Model (SSM)
Usually SSM is referred to probabilistic structure model describing the depen-
dence relationship between the latent variable and the studied observations in
terms of probabilities whether the data can be in either continuous or discrete

496
K. Navapan et al.
forms (Kalman [8]). Kalman ﬁlter refers to an optimal algorithm for inferring
linear Gaussian systems. Its primary objective is to draw inference from the
observations to the relevant properties of unobserved series of vectors . Other
objectives are to estimate of parameters and to forecast.
For a univariate time series yt, we begin with a very simple model called ran-
dom walk with noise model (or local level model) which is at this state expressed
as
yt = θt + vt,
vt ∼N(0, V ),
(3)
θt+1 = θt + wt,
wt ∼N(0, W),
(4)
where θt denotes the unobserved level at time t, whereas vt denotes the obser-
vation disturbance at time t = (1, ..., n). The only parameters of the model
are the observation and evolution variances V and W. Distributions vt and wt
are assumed to be mutually independent. The Eq. (3) is called the observation
equation or measurement equation. The Eq. (4) is called the state equation.
2.2.1
State Space Model Representation of ARIMA Models
The basic ARIMA (p, q) model as shown in (2) can be written the ARIMA
models in state space form. Now we assume that u is typically denoted as zero,
so the deﬁning relation is expressed as
Yt =
r

j=1
φjYt−j +
r−1

j=1
ψjεt−j + εt,
(5)
where r = max(p, q + 1), φj = 0 for j > p and ψj = 0 for j > q. The ARIMA
(p, q) models in state space form can be described by the matrices which can be
showed as
F =
1 0 . . . 0
, G =
⎡
⎢⎢⎢⎢⎢⎣
φ1
1 0 . . . 0
φ2
0 1 . . . 0
...
...
...
φr−1 0 . . . 0 1
φr
0 . . . 0 0
⎤
⎥⎥⎥⎥⎥⎦
, R =
1 ψ1 . . . ψr−2 ψr−1

(6)
After a r-dimensional state vector θt = (θ1,t, ..., θr,t) is deﬁned, then the
ARMA model can be referred to the following SSM representation:
Yt = Fθt
(7)
θt+1 = Gθt + Rεt
(8)
The two equations together in (7) and (8) are called dynamic linear models
(DLMs) as known as linear SSM, where V = 0 and W = RR
′σ2 are contained.
Whereas σ2 represents the variance of the error sequence (εt). In this study, we
use maximum likelihood to estimate dynamic system (Petris et al. [13]).

Forecasting the Growth of TDS with ARIMA and State Space Model
497
2.3
Evaluation of Forecasting Model Performance
To evaluate the forecasting model performance, we employ two benchmarks
which are Root mean square error (RMSE) and Mean absolute error (MAE)
for checking forecasting ability between the two models.
RMSE benchmark evaluates the forecasting model performance by measuring
the average magnitude of the error. It takes the square root of squared the
diﬀerence between actual observations and forecasting values. In other words, it
is to measure the diﬀerences between actual values and predicted values. RMSE
can be represented as;
RMSE =



 1
n
n

i=1
(yj −ˆyj)2,
(9)
where, y is the predicted value of time t, and ˆy is the actual value.
MAE is also to measure the average magnitude of the errors. With this
method, it principally reﬂects the magnitude and avoids the problem of posi-
tive and negative forecast errors oﬀsetting one another. Consequently, MAE is
computed as follows;
MAE = 1
n
n

i=1
|yi −ˆyi|
(10)
The smallest values in each model generated by both RMSE and MAE are
preferred since the smallest values denote the best forecasting ability of that
model.
3
Data
The time series data sets of TDS from quarter 1 in 2001 to quarter 3 in 2016.
There are 63 observations. The data is received from Bank for international
settlements (BIS) database. The data is also divided into 2 periods due to a
measure of predicted ability with RMSE and MAE. The ﬁrst period known as
in sample, covers a period from quarter 1 in 2001 to quarter 4 in 2014 with 56
observations in total and the second period known as out of sample covers a
period from the latter two years with 7 observation in total.
4
Empirical Results
Using natural logarithms to summarize changes in terms of continuous com-
pounding can be meaningful over looking a simple percent changes, however the
graph on the left hand side in Fig. 1 indicates that the graph of log of TDS
clearly is a non stationary during the studied period. Therefore, we transform
the data to be the growth of TDS. The left hand graph in Fig. 1 shows the data
with logarithm transformation and it clearly that the growth of TDS in the right
hand side becomes more stationary compared to the graph of the log of TDS.

498
K. Navapan et al.
Fig. 1. Log of TDS and the growth of TDS
Fig. 2. Plot the ACF/PACF of the growth of TDS
Then we check the stationary of the growth of TDS by using the Augmented
Dickey-Fuller (ADF), the result accepts hypothesis the time series are non sta-
tionary. Therefore, it becomes essential to ﬁrst diﬀerence the growth of DSR.
Then it becomes stationary time series data. Then we identify the numbers of p
and q through plotting of ACF and PACF as shown in Fig. 2.
The Partial ACF plot shown in Fig. 2 has a signiﬁcant cut oﬀafter lag 1
as pointed out by an AR (1) model. Therefore, the ARIMA (1,1,0) tends to
be the ﬁrst candidate models. Then ARIMA (1, 1, 0) is tested for ﬁtting with
variations including ARIMA (1, 1, 1), ARIMA (2, 1, 0), ARIMA (2, 1, 1) with
respect to lowest numbers of lag p and lag q based on AIC criteria. The result
show ARIMA (1, 1, 0) provides the lowest of AIC.

Forecasting the Growth of TDS with ARIMA and State Space Model
499
The SSM with representation of ARMA models, we also selected the best
model ARMA (p, q) based on AIC criteria. ARMA (1, 1) in SS form provides
the lowest value AIC.
In order to compare ARIMA (1, 1, 0) with ARMA(1, 1) in SS form, it is
clearly that the lag of growth DSR are signiﬁcant negatively related to present
growth DSR as shown in Table 1.
Table 2 shows that the RMSE and MAE values of the two model are signif-
icantly diﬀerent. The RMSE and MAE values of the ARIMA model are 00191
and 0.0181 respectively. Meanwhile, the smallest RMSE and MAE values, 0.0095
and 0.0099, belong to the SS model. It concludes that the SS model has the bet-
ter forecasting performance with respect to the minimum values of RMSE and
MAE.
Table 1. Estimated of ARIMA model and SS model
Model
ARIMA (1,1,0)
ARMA (1,1) in SS form
Coeﬃcients Std.Error Coeﬃcients Std.Error
Intercept –
–
−0.809
0.592
ar(1)
−0.597***
0.115
−7.536***
0.191
ma(1)
–
–
1.000
0.592
Note *** signiﬁcant at 99 %
Table 2. Forecasting Criteria for the models.
Models
RMSE
MAE
ARIMA
0.0191
0.0181
SS model 0.0095* 0.0099*
Note *** lowest values com-
pared to the other model
Fig. 3. Comparison of forecasting abilities between ARIMA and SS model

500
K. Navapan et al.
The previous ﬁndings show that the state space model has a better forecasting
ability than ARIMA. Figure 3 shows three lines generated by actual data, and
forecasting values generated by the ARIMA model and the state space model.
Considering the actual values line as the center line, it is obvious that overall the
ARIMA line is further than the state space line. This characteristic also supports
the previous empirical results. Furthermore, the predicted values generated by
the ARIMA model line are far higher than the actual values over the period.
5
Conclusion
The paper focuses on forecasting the growth of TDS in Thailand. The time
series data sets of DSR from quarter 1 in 2001 to quarter 3 in 2016, totally are
63 observations. The data is divided into 2 periods. The ﬁrst period known as
in sample, covers a period from quarter 1 in 2001 to quarter 4 in 2014 with 56
observations in total and the second period known as out of sample covers a
period from the latter two years with 7 observation in total. The studying of the
ARIMA models, ARIMA (1, 1, 0) provides the lowest AIC. Meanwhile, the SS
model with representation of ARMA models, ARMA (1, 1) in SS form provides
the lowest AIC. To compare the forecasting performance of ARIMA model and
SSM based on RMSE and MAE criteria, the result shows that SSM provides the
better performance, as the predicted values generated by the ARIMA model are
far higher than the actual values compared to the predicted values generated by
the state space model over the period.
For further discussion as an informative EWI for forthcoming systemic bank-
ing crises, TDS’s forecasting ability is accurate within ﬁrst two years (Drehmann
and Juselius [1]). Even though, the state space model tends to be the best model
in this study, its predicted values generates almost a straight line. This charac-
teristic could be another important issue in which needs a further exploration.
References
1. Drehmann, M., Juselius, M.: Do debt service costs aﬀect macroeconomic and ﬁnan-
cial stability? BIS Q. Rev. pp. 21–34 (2012)
2. Drehmann, M., Juselius, M.: Evaluating early warning indicators of banking crises:
satisfying policy requirements. Int. J. Forecast. 30(3), 759–780 (2014)
3. Drehmann, M., Illes, A., Juselius, M., Santos, M.: How much income is used for
debt payments? BIS Q. Rev. (2015)
4. Durbin, J., Koopman, S.J.: A simple and eﬃcient simulation smoother for state
space time series analysis. Biometrika 89(3), 603–615 (2002)
5. Dynan, K., Johnson, K., Pence, K.: Recent changes to a measure of US household
debt service. Fed. Res. Bull. 89, 417 (2003)
6. Faruqui, U.: Indebtedness and the Household Financial Health: An Examination of
the Canadian Debt Service Ratio Distribution. Bank of Canada, Ottawa, Canada
(2008)
7. Jacques, J.F.C., Jan, K.S.: An Introduction to State Space Time Series Analysis.
Oxford University Press inc, New York (2007). 174 pages

Forecasting the Growth of TDS with ARIMA and State Space Model
501
8. Kalman, R.E.: A new approach to linear ﬁltering and prediction problems. J. Basic
Eng. 82(1), 35–45 (1960)
9. Kida, M.: Financial vulnerability of mortgage-indebted households in New Zealand-
evidence from the Household Economic Survey. Reserve Bank N. Z. Bull. 72(1),
5–12 (2009)
10. Migon, H.S., Gamerman, D., Lopes, H.F., Ferreira, M.A.: Dynamic models. Handb.
Stat. 25, 553–588 (2005)
11. Omekara, C.O., Okereke, O.E., Ehighibe, S.E.: Time series analysis of interest rate
in Nigeria: a comparison of Arima and state space models. Int. J. Probab. Stat.
5(2), 33–47 (2016)
12. Ramos, P., Santos, N., Rebelo, R.: Performance of state space and ARIMA models
for consumer retail sales forecasting. Robot. Comput. Integr. Manuf. 34, 151–163
(2015)
13. Petris, G., Petrone, S., Campagnoli, P.: Dynamic linear models. In: Dynamic Linear
Models with R, pp. 31–84 (2009)

Eﬀect of Macroeconomic Factors on Capital
Structure of the Firms in Vietnam: Panel Vector
Auto-regression Approach (PVAR)
Nguyen Ngoc Thach1(B) and Tran Thi Kim Oanh2
1 Head of Research Institute, Banking University of Ho Chi Minh City,
36 Ton That Dam Street, District 1, Ho Chi Minh City, Vietnam
thachnn@buh.edu.vn
2 Banking University of Ho Chi Minh City, Ho Chi Minh City, Vietnam
Abstract. The article examines the impact of macroeconomicfactors on
capital structure during the period of economic recession and economic
recovery. The authors collected data from the ﬁnancial statements of 82
ﬁrms listed in Vietnam stock market during the Quarter 1/2007-Quarter
2/2016 and using PVAR. The results demonstrate that during economic
recession, the economic growth, the bond market, credit market posi-
tively impacted the capital structure whereas the stock market showed
negative impacts on the capital structure. During economic recovery,
economic growth positively impacted on the capital structure and the
remaining macroeconomic variables negatively impacted on the capital
structure. In addition, capital structure was aﬀected bymicroeconomic
variables such as proﬁtability, asset structure, size, growth and liquidity.
Keywords: Capital structure · Economic recession · PVAR
1
Introduction
Maximizing proﬁts and business value are important targets of enterprises. In
order to achieve those targets, managers must employ right decisions in choosing
investment opportunities as well as optimally organize and manger their busi-
nesses. Capital structure, one of ﬁnancial tasks in corporate governance, plays a
key role.
After witnessing a period of high economic growth in the ﬁrst half of 2000s,
the global economy experienced an economic recession from 2007 to 2010,
which negatively aﬀected the business activities of Vietnam’s enterprises. The
impact was demonstrated by a sharp increase in number of enterprises ceasing
their operation during the period. According to the General Statistics Oﬃce of
Vietnam (2015), in 2014, there were 58,322 enterprises the faced with diﬃculties
and dissolved, 14.5% rising compared to that of the previous year. One of the
main causes for the situation was the volatility in macroeconomic environment,
which propelled businesses into ﬁnancial diﬃculties. However, in the context
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_36

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
503
of such traumatized economy, most Vietnam’s enterprises lacked a speciﬁc and
long-term plan for capital restructure but still relied on subjective decisions
regarding capital structure, ignoring the circumstances of the economy in each
speciﬁc period.
Over the last several years, many studies on capital structure have been
published. Most of them are about the impacts of micro-economic variables on
the capital structure of businesses in diﬀerent countries. These studies applied
diﬀerent approaches and methodologies, but mainly the Pooled OLS, the Fixed
eﬀect model (FEM), the Random eﬀect model (REM) and the General method of
moments (GMM). In this report, the authors use PVAR to analyze and compare
the impact of those variables on the capital structure of Vietnam’s businesses in
the two periods of recession and recovery of the world economy.
2
Rationale and Empirical Studies
2.1
Rationale
Most studies regarding the capital structure focus on the following theories:
The MM theory
The theory was proposed by Modigliani and Miller (1958) based on the theory
of perfect markets with the absence of taxes, concluding that the business value
and the weighted average cost of capital (WACC) are independent of the capital
structure. The theory was continued to be further studied in tax environment
(1963), drawing a conclusion that the value of the business would increase if it
utilizes debt from the beneﬁts of tax shield. The weighted average cost of capital
(WACC) of businesses utilizing debts is lower than that of debt-free businesses.
However, the theory was based on unrealistic premises (perfect competitive mar-
ket, absolute rationality, perfect information). Still, the theory serves as the basis
for the emergence of more realistic theories later.
The pecking-order theory (POT)
The theory proposes a hierarchy of priorities in selecting funding options but
does not address the existence or non-existence of an optimal capital structure
for businesses. The POT theory states that the capital structure accords with
the following funding order: internal capital from retained earnings, debts, last,
new equity (Donaldson 1961).
Trade-oﬀtheory (TOT)
The static Trade-oﬀtheory was initiated by Kraus and Litzenberger (1973) and
further developed into the dynamic TOT (Myers and Majluf 1984). According to
the static TOT, enterprises can easily and quickly achieve their optimal capital
structures, reﬂecting the tradeoﬀbetween debt’s beneﬁts from tax shield and the
cost of capital exhaustion. Each enterprise has only one optimal capital structure.
Conforming to the dynamic TOT, under the impacts of microeconomic and

504
N. N. Thach and T. T. K. Oanh
macroeconomic environment, enterprises cannot immediately reach the optimal
capital structure without experiencing a gradual adjustment. Also, the optimal
capital structure will vary in each speciﬁc period. Despite the diﬀerence in view,
there is a common approach of the two theories based on the tradeoﬀbetween
cost and beneﬁt for business to obtain its optimal capital structure and maximize
its value.
The theory of agency cost
The theory was proposed by Jensen and Meckling (1976). Agency cost incurs
due to the asymmetry of information between enterprise’s managers and owners.
Therefore, enterprises tend to increase the use of debt in order to reduce agency
cost because once doing so, managers must be more cautious in their ﬁnancial
decisions, business risks would be lessened and the eﬃciency of business activities
would improve.
The signaling theory
The signaling theory was also developed based on the information asymme-
try between enterprises and investors. Investors usually analyze enterprise’s
activities, speculate on the current situation and forecast the prospect of the
enterprises. They believe issuing of debt is a positive signal about the business
prospect, thus the stock price would go up. In contrast, that enterprises issue new
equity indicates a negative signal, which would make stock price to fall. From
the perspective of investors, only a business has not good prospects want to be
funded by equity in order to share the risk of the business with new investors
(Asquith and Mullins 1983).
The Market-timing theory
This theory was originated from the study of Baker and Wugler (2002), stating
that the diﬀerence between market value and book value is the determining fac-
tor for enterprise’s capital structure. In case of a high price-to-book ratio (P/B),
enterprises will issue new equity to mobilize capital. Meanwhile, enterprises usu-
ally use debt when P/B is low.
The above mentioned theories have various views, however they do not con-
ﬂict but rather complement each other in comprehensively explaining the man-
ager’s decisions of funding sources.
2.2
Other Related Studies
Although theoretical and empirical research on capital structure varies in per-
spectives and methodologies, they generally focus on the following aspects: capi-
tal structure is inﬂuenced by micro variables (Truong and Nguyen 2015; Vatavu
2015); the combined eﬀect of micro and macro variables to capital structure
(Jong et al. 2008; Nor et al. 2011; Khanna et al. 2015); capital structure impacts
business value or determines the optimal capital structure threshold (Ahmad
et al. 2012; Wang and Zhu 2014). However, the common limitation of these

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
505
studies is that they merely focus on the capital structure of businesses in long
term without analyzing speciﬁc economic contexts in each period (stage) of the
economic cycle.
Since the 1970s, economic crises have occurred at high frequency, intensity
and complexity, causing severe socio-economic consequences for many nations all
over the world. This trend has resulted in studies on capital structure in combi-
nation with global and regional economic crises such as 1997–1998, 2007–2010.
However, the number of these works remains modest. Other noticeable studies
are Ariﬀet al. (2008); Fosberg (2013); Alves and Francisco (2015); Iqbal and
Kume (2014). In Vietnam, currently only Truong and Nguyen (2015) refer to
this issue. However, studies in Vietnam and abroad mainly focus on addressing
and giving solutions to ﬁx the impacts of economic downturns on capital struc-
ture without thoroughly analyzing the capital structure adjustment of businesses
in response to the context of the economy during recession and recovery. This is
a scientiﬁc gap in studying capital structure in Vietnam and abroad.
In terms of methodology, most of the published studies on this subject only
use Pooled OLS, FEM, REM or GMM. The purpose of using these models is
simply to verify the positive or negative impacts of macroeconomic variables on
capital structure. However, they are unable to analyze the mechanism driving the
eﬀects of these variables to the capital structure decisions of enterprises as well as
unable to explain how a macroeconomic shock impacts on the behavior of adjust-
ing the capital structure of enterprises and how long this impact will last. Only
Khanna and Associates (2015) used PVAR in panel data to study the impacts of
macroeconomic variables such as economic growth, inﬂation and stock indexes
on capital structure. In Vietnam, according to the authors, there has not been
any studies applying PVAR to study this issue. This is a gap in study methodol-
ogy because socio-economic characteristics in the context of unstable economic
recession and recovery require the use of an appropriate method to study of the
impact of microeconomic and macroeconomic factors to the ﬁnancial situation,
especially the capital structure of the business. Therefore, in the present study,
the authors analyze the impact of micro variables and the mechanism of the
impact of macro variables on the capital structure of Vietnam’s enterprises in
the period of recession (from Q1/2007 to Q4/2010) and economic recovery (from
Q1/2011 to Q2/2016) by using PVAR.
3
Data, Model and Study Methodology
3.1
Data
Based on grounded theories and empirical researchers, variables aﬀecting the
capital structure are selected to build the model (as shown in Table 1).

506
N. N. Thach and T. T. K. Oanh
Table 1. Variables and measurement.
Symbol
Variables
Expectation
Theory
Measurement
Studies
Endogenous variables
TDR
Capital
structure
+
Total liabili-
ties/Total
assets
Vo et al. (2014);
V˘atavu (2015)
GDP
Economic
growth
+ −
TOT, agency
cost POT
Quarterly
growth rates
of real GDP
Ariﬀet al. (2008);
Jong et al. (2008);
Khanna et al. (2015)
RATE
Loan interest
rate
+ −
MM POT,
market-timing,
TOT
Average loan
interest
Nor et al. (2011);
Allayannis et al.
(2003); Zerriaa and
Noubbigh (2015)
LNVNINDEX
Stock market
−
Market timing
Natural
logarithm of
VNINDEX
Jong et al. (2008);
Alves and Francisco
(2015); Khanna et al.
(2015)
BOND
Bond market
+
TOT, agency
cost
Market
capitalization
value/GDP
Jong et al. (2008);
Nor et al. (2011)
Exogenous variables
SIZE
Firm size
+ −
TOT, Agency
cost, POT
Natural
logarithm of
total assets
Jong et al. (2008); Vo
et al. (2014)
TANG
Asset
structure
+ −
TOT, POT
Agency cost
Fixed
assets/Total
assets
Vo et al. (2014);
V˘atavu (2015)
GRO
Firm growth
+ −
POT Agency
cost
Quarterly
growth rate of
total assets
Jong et al. (2008); Vo
et al. (2014)
LIQ
Short-term
liquidity
+ −
TOT POT,
Agency cost
Short-term
assets/Short-
term
liabilities
Vo et al. (2014);
V˘atavu (2015)
VOL
Business risk
+ −
TOT POT
Standard
deviation
(EBIT/Total
assets)
Vo et al. (2014);
V˘atavu (2015)
MTR
Coiporate
income tax
+
TOT MM
Corporate
income
tax/Proﬁt
before tax
Jong et al. (2008);
V˘atavu (2015)
Note: (+) is positive relationship between capital structure and explanatory vari-
ables. (−) is negative relationship between capital structure and explanatory vari-
ables. Source: Author’s compilation.
The authors used balance sheets extracted from ﬁnancial statements of 82
randomly selected enterprises from those listed on the Vietnam Stock exchanges,
which were continuously in operation from Q1/2007 to Q2/2016 (82×38 = 3, 116
observations). The samples were highly representative, provided by Ban Viet
Capital Securities (VCSC). In addition, macroeconomic variables werecollected
from IMF, ADB and AsianBondsOnline.

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
507
3.2
Models and Research Methodology
PVAR for economic recession period (Model 1) and economic recovery (Model
2) with latency k are described as follows:
Yit = μ0 + A1Yit−1 + . . . + AkYit−k + βxXit + eit,
∀i = 1, 2, . . . , N, t = 1, 2, . . . , T.
Where Yit = (TDRit, GDPit, RATEit, LNV NINDEXit, BONDit): is a
random vector level of dependent variables; Yit−p: vector level of dependent
variables lentency; A1, A2, . . . , Ak: matrices k × k; Xit: exogenous vectors level
(1 × k), including variables listed in Table 1; βx: matrices (l × k) coeﬃcient esti-
mation; eit: Fixed eﬀects due to unobservable characteristics of enterprises and
constant eﬀects over time, eit|yit−1 ∼N(0; σ2
e).
The classical VAR model is applied to the stationery and non-coherent time
series, originated from Sims’s study (1980) on the transmission mechanism of
macro variables. Eakin et al. (1988) continued to propose VAR model to process
panel data (PVAR). Since PVAR was proposed based on the classical VAR
model, there were still some shortcomings such as the deviated estimated para-
meters or loss of observations when taking lags. To ﬁx this disadvantage, Love
and Zicchino (2006) introduced and used PVAR based on the application of
GMM to ensure the uniformity of balance variances, preventing self-correlation
and maintaining data conservation.
3.3
Basic Tests
3.3.1
Stationery Test
When estimating PVAR, the variables in use must be stationery. The authors
used the Augmented Dickey-Fuller (ADF) to test the variables. Table 2 shows
that all variables are stationery at 0.
Table 2. PVAR unit root test results.
VARIABLES STATISTIC T VARIABLES
STATISTIC T
TDR
526,1311∗∗∗
TANG
291,4663∗∗∗
ROE
2405,4509∗∗∗
LIQ
702,1896∗∗∗
VOL
1858,4298∗∗∗
LNVNINDEX 381,9070∗∗∗
SIZE
493,4786∗∗∗
BOND
207,7378∗∗∗
MTR
1638,3719∗∗∗
RATE
207,7378∗∗∗
GRO
1343,1092∗∗∗
GDP
561,0923∗∗∗
Note:
∗∗∗corresponds
to
1%
of
signiﬁcance
level.
Source:
Author’s calculation.

508
N. N. Thach and T. T. K. Oanh
Table 3. Lags criteria results of Model 1 and Model 1.
Latency CD
Statistic J
Model 1
1
0.9997
874.1953
2
0.9998
567.1583∗
3
0.9999
867.7019
Model 2
1
0.9979 1,286.025
2
0.9980 1,173.926
3
0.9986
510.0104∗
Note: ∗represents the selected
latency
corresponding
with
criteria. Source: Author’s cal-
culation
Fig. 1. AR root test results of Model 1 and Model 2. Source: Author’s calculation.
3.3.2
Optimal Latency Test
Andrews and Lu (2001) proposed to use the Moment Model Selection Criteria
(MMSC) with determination coeﬃcient CD and J-Pvalue statistics to determine
the optimal latency. The results shown in Table 3 indicate that PVAR optimal
latency in model 1 is 2 and model 2 is 3.
3.3.3
Model Stability Test
Research conducted AR test. Figure 1 shows that all the solutions of Model 1 and
Model 2 are in the unit circle. PVAR model ensures stability and sustainability.

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
509
4
Study Outcomes
Table 4 represent the regression results of Model 1 and Model 2.
Table 4. PVAR results of Model 1 and Model 2
Criteria
Model 1
Model 2
L.TDR
0.726∗∗∗
0.722∗∗∗
[8.30]
[7.99]
L.LNVNINDEX
−0.178∗∗∗
−0.0726∗∗∗
[−3.74]
[2.85]
L.BOND
3.597∗∗∗
−0.495∗∗∗
[5.35]
[−5.03]
L.RATE
1.231c
−0.0316
[4.90]
[−0.56]
L.GDP
2.669∗∗∗
0.105
[3.53]
[0.84]
ROE
−0.251∗∗
−0.0549∗
[−2.05]
[−1.50]
VOL
−0.26
0.219
[−1.12]
[1.06]
SIZE
−0.328∗∗∗
0.0369∗
[−3.09]
[0.62]
MTR
−0.131
−0.0356∗
[−1.63]
[−1.91]
GRO
−0.00804∗∗
0.00186∗
[−2.07]
[0.46]
TANG
0.299∗∗∗
0.0810∗
[2.85]
[1.67]
LIQ
−0.00202∗
0.000261∗
[1.53]
[0.93]
N
1230
1722
Note: ∗, ∗∗, ∗∗∗correspond to the signiﬁ-
cance level of 10%, 5% and 1%; [] value of
standard deviation. Source: Author’s cal-
culation.
4.1
Economic Recession
In order to analyze the mechanism and direction of the impact of macroeconomic
variables on the capital structure of Vietnam’s enterprises when shocks happen,
the authors analyzed the push function (Fig. 2).
4.1.1
Impacts of Bond Market on the Capital Structure
As the bond market went up by one standard deviation, the bond market
improved, enterprises increased 3.597% of debts in the ﬁrst quarter and damp-
ened in the fourth quarter. This result was consistent with the theory TOT and
Jong et al. (2008); Nor et al. (2011).

510
N. N. Thach and T. T. K. Oanh
Fig. 2. Impulse response function Model 1. Source: Author’s calculation.
4.1.2
Impacts of Economic Growth on the Capital Structure
Figure 2 shows that GDP has a positive impact on TDR. When GDP increased
by one standard deviation, corporate debt use increased by 2.669% and the
increase in debt use declined gradually as of Q3, in line with Ariﬀand Associates
(2008).
4.1.3
Impacts of Credit Market on Capital Structure
With the shrinkage of credit market, credit balance declined, loan conditions
became more diﬃcult and RATE increased by one standard deviation but pos-
itively impacted TDR. TDR increased by 1.231% at 5% signiﬁcance level. This
increase lasted for nine quarters. The result is consistent with MM theory and
Allayannis et al. (2003); Zerriaa and Noubbigh (2015).
4.1.4
Impacts of Stock Market on the Capital Structure
When the stock market increased by one standard deviation, TDR fell by 0.178%
at 1% signiﬁcance level. The decline of debt using lasted in 1 quarter, in agree-
ment with Khanna et al. (2015) and Alves and Francisco (2015).
As analyzed above, there is a relation between TDR and macroeconomic
variables in the context of economic downturn. However, TDR does not only
depend on macroeconomic shocks but also under the eﬀect of microeconomic
variables.
The capital structure of the previous period positively inﬂuences the capital
structure of the later one. This indicates that a rise in debt using in the previ-
ous period would make the debt using in the later period increase by 0.726%,
harmonizing with Nor et al. (2011) and Khanna et al. (2015).

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
511
Proﬁtability negatively correlated with the capital structure at 1% statistical
signiﬁcance level. The results was explained by POT, Nor et al. (2011) and
Truong and Nguyen (2015). However, the results also demonstrated an ineﬃcacy
in using debts of Vietnam’s enterprises, lowering businesse’s proﬁtability.
Firm size and scale negatively inﬂuences the capital structure. This indicates
that during economic downturn, large-scale businesses usually have high proﬁt,
large equity, good reputation and ﬁnancial capacity can easily issue new equity
to the market. The result is analogous to Fosberg (2013) and Proenca et al.
(2014).
Firm growth negatively aﬀects the capital structure, which is similar to the
Agency cost theory and Proenca et al. (2014) when studying the correlation in
the economic recession period.
Asset structure has positive correlation with capital structure. Indeed, enter-
prises whose ﬁxed assets are large when issuing secured debts or mortgaged debts
are more likely to have access to loans and better policies. This results match
with POT, TOT, Alves and Francisco (2015) and Iqbal and Kume (2014).
Solvency has negative impact on capital structure, consistent with POT,
Nor et al. (2011) and Proenca et al. (2014) when studying this correlation in the
economic recession period.
4.2
Economic Recovery
Similarly, Fig. 3 demonstrates the push function of Model 2.
Fig. 3. Impulse response function Model 2. Source: Author’s calculation.

512
N. N. Thach and T. T. K. Oanh
4.2.1
Impacts of Bond Market on Capital Structure
Table 3 shows that when the bond market expanded by one standard deviation,
TDR fell by 0.495% and this trend prolonged in 10 quarters. This result contra-
dicts with the study on economic recession and TOT but consents to Vo et al.
(2014). The reason for the result is the characteristics of bond markets of devel-
oping countries in general and Vietnam in particular, which is small in scale,
undiversiﬁed products mainly comprising of Government bonds (which account
for 95.4% of the market value). Besides, the market capitalization value of the
government bond market increased from 13.68% GDP (economic recession) to
19.03% GDP during the observation period. In contrast, the scale of corporate
bond market slightly decreased with market capitalization value fell from 1.01%
GDP to 0.92% GDP. The cause of such development was the fact that credit
market remained as a traditional mobilization source or Vietnam’s enterprises,
therefore when loan conditions loosened; there was a shift from issuing debts
towards borrowing directly from ﬁnancial institutions.
4.2.2
Impacts of Economic Growth on Capital Structure
Similar to the economic downturn period, Table 3 shows that when GDP
increased by one standard deviation, TDR rose by 0.105% in Q1 and slowly
diminished as of Q6. This implies a rise in consumption demand in that period,
encouraging businesses to expand their production, which led to high capital
demand. Furthermore, the low cost of capital exhaustion and reduction in bank-
ruptcy risk made enterprises increase the use of debts to take advantages of the
tax shields. This result is consistent to TOT, Jong et al. (2008); Nor et al. (2011)
and Khanna et al. (2015).
4.2.3
Impacts of Credit Market on Capital Structure
In contrast with the study in economic recession, Table 3 shows that when credit
market shrank, RATE increased by one standard deviation, TDR dropped by
0.312% at 5% of signiﬁcance level. This trend lasted for 6 quarters. It indicates
that the credit market positively correlates with TDR, consistent to POT and
Nor et al. (2011)
4.2.4
Impacts of Stock Market on Capital Structure
Similar to the economic recession, Fig. 4.3 exhibits a decrease of TDR by 0.073%
when the stock market went up by one standard deviation. The trend prolonged
for 2 quarters since the economic shock. The result conforms with the market-
timing theory and Vo et al. (2014).
The results of Model 2 also indicate that in economic recovery period, other
variables such as solvency, scale and speed of growth positively inﬂuence the
capital structure, contravening the results of the study in economic recession
period. This result could be explained by Keynes (1936). That is, when the
economy recovers, global and domestic demand for goods and services increases.
Hence, in order to meet the demand of the market and the new business cycle,

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
513
Vietnam’s ﬁrms expand their operations, increase their asset investment and
actively seek new business opportunities. However, the result in this period also
points out that Vietnam’s businesses should prioritize their investment in short-
term projects and/or assets with high proﬁtability to increase their short-term
solvency as well as minimize risks arising under the context of unstable and
unsustainable economic recovery. Besides, businesses should also control and
recalculate their taxable income in this period. This is also a signiﬁcant source
of capital enhancing capital resources for businesses.
5
Conclusion and Policy Recommendations
5.1
Conclusion
The PVAR regression results show that TDR is aﬀected by macroeconomic vari-
ables such as bond market, economic growth, credit market and stock market.
However, the direction and magnitude of the impact of macroeconomic variables
on TDR varies. Vietnam economy under the global economic recession, macro-
economic variables had a strong impact on TDR, which suggests that managers
were cautious during this period. However, as the global economy recovered and
the process inﬂuenced the macroeconomic movements in the country, managers
did not properly address the macroeconomic variables. Macroeconomic variables
showed weak impacts on TDR, speciﬁcally:
The bond market has a positive impact on TDR during the economic down-
turn. A shock would increase the impact of the bond market, making businesses
increase their debt use by 3.597%. Conversely, as the economy recovers, TDR
fell by 0.495% under the impact of the bond market.
Economic growth has a positive impact on TDR in both economic recession
and recovery. In particular, the strongest impact was during the economic down-
turn. Despite the diﬃculties of the period, economic growth still made TDR rise
by 2.699%, in accordance with the characteristics of Vietnam economy during
this period, whose growth rate was high due to recent WTO accession.
The credit markethas a negative impact on TDR during economic recession.
Conversely, as the economy recovered, credit markets positively inﬂuenced TDR.
Thereby we ﬁnd that, despite the economic downturn, diﬃcult operating condi-
tions, ﬁnancial exhaustion and bankruptcy risk, businesses still increased their
use of debt. This demonstrates the fact that Vietnam’s enterprises have low
ﬁnancial autonomy, depends heavily on loans as well as undiversiﬁed channels
for capital mobilization.
The stock market has a negative impact on capital structure in both eco-
nomic recession and recovery, in line with the Market-timing theory. However,
the strongest impact was during the economic downturn, when a stock market
shock made the TDR decrease by 0.178%.
In addition, the estimated PVAR model also indicates that during the eco-
nomic downturn, the pace of TDR adjustment of Vietnam’s enterprises (0.726%)
is faster than that when the economy recovers (0.722%). This result reaﬃrms
the content of the TOT theory that the capital structure of a business varies

514
N. N. Thach and T. T. K. Oanh
from time to time, however, this theory does not yet indicate the rate of ﬁrm’s
adjustment of target capital structure in each speciﬁc economic context. This
study, therefore, provides additional evidences for the argument of the TOT
theory. That is, during economic downturn, managers adjusts capital structure
faster than in economic recovery to achieve the target capital structure, ensuring
ﬁnancial security and increasing the value of the company.
In addition, asset structure has a positive impact, while proﬁtability has a
negative impact on TDR. Other variables including solvency, ﬁrm growth and
size have a negative impact on TDR in the context of economic recession. Con-
versely, as the economy recovers, these variables have a positive impact on cor-
porate TDR. Furthermore, during the economic recovery period, businesses need
to control expenses and especially recalculate their taxable income.
5.2
Policy Recommendations
For enterprises
First, enterprises should diversify their forms of capital mobilization to reduce
debts, make the most of capital sources and increase the use of ﬁnancial instru-
ments.
Second, the capital restructure of businesses must be associated with each
stages and target of development, speciﬁc ﬁnancial situation, business size,
domestic and foreign macroeconomic environment in line with stages of eco-
nomic cycle- economic recession and recovery.
Third, restructure capital in the direction of increasing owner’s equity and
self-ﬁnancing capacity of businesses. The study outcomes show that the prof-
itability of enterprises negatively inﬂuences the capital structure. This proves
that proﬁtability is an important capital sources that can help ﬁrms actively meet
their capital needs while still keeping control. Additionally, businesses should
actively mobilize equity from outside such as issuing shares, doing joint ventures
and associates.
Fourth, capital restructuring should be associated with the restructuring of
investment portfolios, especially investment in ﬁxed assets. However, enterprises
should also note that the increase of ﬁxed assets must be associated with capital
structure adjustment in the direction of strengthening long-term source, avoiding
ﬁnancial imbalances. Besides, the study results also show that Vietnam’s enter-
prises should prioritize investments in short-term projects or assets with high
yields that can enhance their short-term liquidity and help avoid risks arising
from the economic conditions of unstable and unsustainable recovery.
Fifth, capital restructure must be in line with the recalculation of taxable
income and tax planning in order to optimize the amount of tax payable within
the legal framework. It means that ﬁrms should recalculate their taxable income
to achieve tax deduction that are higher than their reduction of business income.

Eﬀect of Macroeconomic Factors on Capital Structure of the Firms
515
For the Government and related agencies
First, the Government should simultaneously implement polices to stabilize the
macro economy, curb inﬂation, ensure rational economic growth, create favorable
business environment and assist enterprise to better access to the funds serving
their purpose of restructuring.
Second, the Government should improve regulations and policies to promote
and facilitate the development of ﬁnancial markets, especially the bond market
and the stock market. They are not only the channels that help mobilize capital
for enterprises but also the channels for capital withdrawal under the market
mechanism. Therefore, sound and developed stock and bond markets are sig-
niﬁcant conditions to ensure the success of the capital restructuring process of
enterprises. Thus, the capital restructuring of enterprises must be associated
with the development of these markets. The Government should employ policies
to strengthen, stabilize and soundly develop credit markets.
Third, the corporate income tax is 22%, which is relatively higher than that
of other countries in the region such as Singapore, Hong Kong (17%) and Taiwan
(16.5%). The Government should adjust the corporate income tax to improve
competitiveness and support enterprises in capital restructuring.
References
Ariﬀ, M., Taufq, H., Shamsher, M.: How capital structure adjusts dynamically during
fnancial crises. J. Faculty Bus. Bond Univ. 12, 15–25 (2008)
Asquith, P., Mullins, D.W.: The impact of initiating dividend payments on sharehold-
ers. J. Bus. 56(1), 77–96 (1983)
Ahmad, Z., Abdullah, N.M.H., Roslan, S.: Capital structure eﬀect on ﬁrms perfor-
mance: focusing on consumers and industrials sectors on Malaysian ﬁrms. Int. Rev.
Bus. Res. Papers 8(5), 137–155 (2012)
Alves, P., Francisco, P.: The impact of institutional environment in ﬁrms’ capital struc-
ture during the recent ﬁnancial crises. Q. Rev. Econ. Financ. 57, 129–146 (2015)
Andrews, D.W.K., Lu, B.: Consistent model and moment selection procedures for
GMM estimation with application to dynamic panel data models. J. Econometr.
101, 123–164 (2001)
Allayannis, G., Brown, G.W., Klapper, L.F.: Capital structure and ﬁnancial risk: evi-
dence from foreign debt use in East Asia. J. Financ. 58, 2667–2709 (2003)
Baker, M.P., Wurgler, J.: Market timing and capital structure’. J. Financ. 57(1), 1–32
(2002)
Donaldson, G.: Corporate Debt Capacity: A Study of Corporate Debt Policy and
the Determination of Corporate Debt Capacity. Harvard University, Massachusetts
(1961)
Eakin, H.D., Newey, W., Rosen, H.S.: Estimating vector autoregressions with panel
data. Econometrica 6, 1371–1395 (1988)
Fosberg, R.H.: Short-term debt ﬁnancing during the ﬁnancial crisis. Int. J. Bus. Soc.
Sci. 4, 1–5 (2013)
Iqbal, A., Kume, O.: Impact of ﬁnancial crisis on ﬁrms capital structure in UK, France,
and Germany. Multinat. Financ. J. 18, 249–280 (2014)
Jensen, M., Meckling, W.: Theory of the ﬁrm: managerial behaviour, agency costs
ownership structure’. J. Financ. Econ. 3(4), 305–360 (1976)

516
N. N. Thach and T. T. K. Oanh
Jong, A.D., Kabir, R., Nguyen, T.T.: Capital structure around the world: the roles
of of ﬁrm- and country-speciﬁc determinants. J. Banking Financ. 32(9), 1954–1969
(2008)
Khanna, S., Srivastava, A., Medury, Y.: The eﬀect of macroeconomic variables on the
capital structure decisions of indian ﬁrms: a vector error correction model/vector
autoregressive approach. Int. J. Econ. Financ. 5(4), 968–978 (2015)
Keynes, J.M.: General Theory on Employment, Interest and Money. The University of
Adelaide, Adelaide (1936)
Kraus, A., Litzenberger, R.H.: A state-preference model of optimal ﬁnancial leverage.
J. Financ. 33, 911–922 (1973)
Love, I., Zicchino, L.: Financial development and dynamic investment behavior: evi-
dence from panel VAR. Q. Rev. Econ. Financ. 46, 190–210 (2006)
Modigliani, F., Miller, M.H.: The cost of capital, corporation ﬁnance and the theory
of investment. Am. Econ. Rev. 48, 261–297 (1958)
Myers, S.C., Majluf, N.S.: Corporate ﬁnancing and investment decisions when ﬁrms
have information that investors do not have. J. Financ. Econ. 13, 187–221 (1984)
Nor, F.M., Haron, R., Ibrahim, K., Ibrahim, I., Alias, N.: Determinants of target capital
structure evidence on south east Asia countries. J. Bus. Policy Res. 6, 39–61 (2011)
Proenca, P., Laureano, R.M., Laureano, L.M.: Determinants of capital structure and
the 2008 ﬁnancial crisis: evidence from Portuguese SMEs. Procedia-Soc. Behav. Sci.
150, 182–191 (2014)
Sims, C.: Macroeconomics and reality. Econometrica 48, 1–47 (1980)
Genral Statistics Oﬃce (2015). Published information. http://www.vtc.vn/chuyen-
gia-noi-gi-ve-trien-vong-kinhte-viet-nam-nam-2015-d188147.html. Accessed 10 Dec
2016
Truong, H.T., Nguyen, P.T.: Determinants of capital structure of a-reits and the global
ﬁnancial crisis. Paciﬁc Rim Prop. Res. J. 18, 3–19 (2015)
Vatavu, S.: The impact of capital structure on ﬁnancial performance in Romanian
listed companies. Procedia Econ. Financ. 32, 1314–1322 (2015)
Vo, T.T.A., Tran, K.L., Le, T.N.A., Tran, T.D.: Study on the impact of macro factors
on the capital structure of enterprises listed on Vietnam’s stock exchanges. J. Econ.
Dev. 207, 19–27 (2014)
Wang, J., Zhu, W.: The impact of capital structure on corporate performance based
on panel threshold model. Comput. Modell. New Technol. 18(5), 162–167 (2014)
Zerriaa, M., Noubbigh, H.: Determinants of capital structure: evidence from Tunisian
listed ﬁrms. Int. J. Bus. Manage. 10, 121–135 (2015)

Emissions, Trade Openness, Urbanisation,
and Income in Thailand: An Empirical Analysis
Rossarin Osathanunkul1(B), Natthaphat Kingnetr1,
and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
orossarin@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
Abstract. This study investigates the relationship between emissions,
income, energy consumption, trade openness, and urbanisation in
Thailand over the period of 1971 to 2014. The ARDL cointegration tech-
nique is employed and CUSUM and CUSUMSQ tests are used to ensure
the stability of the estimated results. Our ﬁndings indicate there is a long
run relationship among variables for the case of CO2 emissions while
there is none for the SO2. The results indicate an increase in income can
cause signiﬁcantly more CO2 emissions. Energy consumption also con-
tributes to environmental degradation with slight impact, while there
is no eﬀect from trade openness. On the contrary, urbanisation greatly
helps reduce CO2 emissions in the long run.
Keywords: Emissions · Income · Thailand · EKC · ARDL
1
Introduction
Thailand is one of the emerging economies in Asia, and is expected to grow
at a faster rate than high income economies. In 2015, the ASEAN Economic
Community (AEC) was established. Thailand is one of the AEC members. The
goal of the AEC is “to transform ASEAN into a region with free movement of
goods, services, investment, skilled labour, and a freer ﬂow of capital” [31]. One
of the major priority actions for AEC is to develop and promote environmen-
tally friendly industries. The rapid economic growth and the establishment of the
AEC would impact Thailand environmentally. Increasing economic growth and
trade openness may lead to an increase in demand for environmental inputs and
consumptive goods, and a rise in production of goods and the subsequent pollu-
tion. Alternatively, although economic growth may lead to an initial increase in
pollution, later as income growth, a nation can invest in environmentally friendly
technologies that lead to a reduction in pollution.
In addition, the mitigation of carbon emissions through international and
international policies will need to be agreed upon by the policy makers of the
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_37

518
R. Osathanunkul et al.
major emitting countries that include China, the United States, India, Russia,
the European Union, and Japan, that account for about seventy ﬁve percent of
the global emissions. However, the cooperation of rapidly growing countries such
as the Republic of Korea, Brazil, South Africa, Indonesia, Mexico, and Thailand
is also needed to put constraints on carbon emissions [20]. Some of the policies
include a local or global cap and trade system of carbon emission allowances,
fuel eﬃciency and energy standards, emission credits, renewable energy portfolio
standards, taxes, and subsidies. Therefore, it is important to empirically esti-
mate the relationship between economic growth, carbon emissions, and energy
consumption in Thailand.
The relationship between income and pollution has been well described as
the environmental Kuznets curve (EKC) hypothesis introduced by Grossman
and Krueger in 1991 [37]. The hypothesis indicates an inverted U-shaped rela-
tionship between emissions and income. It shows that countries initially face
an increase in environmental degradation as their economy expands, but this
will subsequently decrease when economic development reaches a certain level.
Dinda [12] suggests that early in its development, a country would rely on agricul-
ture and industry which draw heavily on natural resources in order to expand its
economy, leading to a negative impact on environmental quality. This is referred
to as the “scale eﬀect” [5]. After a certain period, the structure of industry
then changes into information-intensive industries and services, the population
becomes more concerned about environmental quality leading to enforcement
of environmental regulations, investments, protection, and advancement of new
environmentally friendly technology results in a decrease in environmental dam-
age. This is referred to as “the technique eﬀect” [5].
An increase in trade openness due to a change in trade policy would cause a
country to specialise in the production of goods that could have either a positive
or negative impact on the environment depending on which is used, the pollution
haven hypothesis (PHH) or the factor endowment hypothesis (FEH) [5,11,14].
Cole [10] argues that the pollution haven hypothesis (PHH) occurs when there
is trade between a developed country with more stringent environmental regu-
lations and a developing country with lax environmental regulations. The dif-
ference in regulations encourages pollution-intensive ﬁrms to move away from
the developed country into the developing country. Hence, with the opening of
trade, the level of pollution is expected to increase in countries with lax regula-
tions while decreasing for countries with stringent regulations.
On the other hand, the factor endowment hypothesis (FEH) points to the
diﬀerence in environmental regulations. It is argued that dirty goods are capital-
intensive goods and more likely to be produced by a capital-abundant country
than a capital-scarce country. By opening up to trade, dirty-intensive ﬁrms would
move to a developed country from a developing country. Therefore, emissions in
the capital-abundant country would rise, while the developing country beneﬁts
from the trade [38]. Brack [7] suggests that trade is good for the environment
as it improves environmental standards in industry. When foreign environmen-
tal standards are high, exporting ﬁrms have to apply various standards when

Emissions, Trade Openness, Urbanisation, and Income in Thailand
519
producing goods. This may reduce environmental damage and improve the ﬁrm’s
productivity.
In spite of many papers examining the EKC hypothesis, to our knowl-
edge, only four studies investigating the applicability of EKC hypothesis in
Thailand have been done so far. These studies are Arouri et al. [6], Saboori
and Sulaiman [29], Bureecam [8], and Naito and Traesupap [21]. These studies
employed empirical models which may result in multicollinearity from squared
variables. It is the purpose of this research to avoid this potential problem.
Therefore, this study will extend those by using the new empirical model for
the EKC study suggested by Narayan and Narayan [23] using an autoregres-
sive distributed lag (ARDL) approach, and contribute to the existing literature
by also considering SO2 emissions. The remainder of this paper is organized as
follows. Review of literature is in Sect. 2. Section 3 gives the analytical methodol-
ogy. Then, the results are presented and discussed in Sect. 4, and Sect. 5 provides
conclusions and policy implications.
2
Literature Review
There have been numerous studies investigating the EKC hypothesis; however,
only four studies consider Thailand. Though diﬀerent emissions, economic indica-
tors, and time scale were utilised, these studies concluded that the EKC hypoth-
esis is applicable to Thailand.
Arouri et al. [6] studied the EKC hypothesis in the case of Thailand over
the period of 1970 to 2010 using the ARDL approach. The study employed CO2
emissions as the environmental degradation variable and real GDP per capita,
energy consumption, trade openness and urbanisation as its determinants. Their
study found that CO2 emissions and real GDP per capita follow the pattern given
by the EKC hypothesis. Energy consumption and trade openness increase the
emissions while urbanisation causes a reduction.
Saboori and Sulaiman [29] investigated the relationship between CO2 emis-
sions, energy consumption, and economic growth in Association of Southeast
Asian Nations (ASEAN) countries for the period of 1971 to 2009 using the
ARDL approach. Their study found the applicability of the EKC hypothesis in
Singapore and Thailand with the energy consumption promoting an environ-
mental degradation.
Bureecam [8] investigated the relationship between municipal solid wastes
generation, income, population and tax for the period of 1992 to 2006 using the
ordinary least squares regression (OLS) method. The study found the applica-
bility of the EKC hypothesis with the turning point of real GDP at 3.2 to 3.7
trillion Thai Baht approximately. Moreover, the result indicates that taxation
and population contribute to an increase in solid wastes.
Naito and Traesupap [21] studied the eﬀects of gross provincial product
(GPP) and shrimp farming on mangrove deforestation in 23 provinces in
Thailand during 1975 to 2004 using panel data technique. They found the
applicability of the EKC hypothesis along an estimated value of GPP per capita

520
R. Osathanunkul et al.
at the turning point which has not been reached, and the shrimp farming has
led to mangrove deforestation signiﬁcantly. Although the development of semi-
intensive and intensive aquaculture systems caused mangrove deforestation, the
intensive shrimp farming, developed during the 1990s, improved the situation.
Surprisingly, other studies in the EKC literature found a controversy in the
result. Studies which found support of the EKC hypothesis include those that;
examined the relationship between energy consumption, CO2 emissions, and
economic growth in Europe [1], and heavy industries in Canada [15]; looked at
CO2 emissions and growth in developing economies [23]; looked at the relation-
ship between CO2 emissions, energy consumption, FDI, and GDP for Brazil,
Russian Federation, India, and China [24]; and looked at relationship between
CO2 emissions, energy consumption, economic growth, trade openness, and
urbanisation of newly industrialised countries [17]. Acaravci and Ozturk [1] and
Hamit-Haggar [15] found the presence of a long-run relationship between car-
bon emissions, energy consumption, and real GDP. Within Europe, of which
Acaravci and Ozturk [1] found the evidence of the EKC hypothesis only holds
for Denmark and Italy. Additionally, Hamit-Haggar [15] showed that for the short
run, economic growth has an impact on greenhouse gas emissions within indus-
tries. Their study found that energy consumption has an insigniﬁcant impact on
economic growth. Hossain [17] found that over time higher energy consumption
leads to an increase in CO2 emissions. On the other hand, economic growth,
trade openness, and urbanisation are good for the environment in the long run.
Narayan and Narayan [23] suggested that if long run elasticity is smaller than
short run elasticity then it implies that country is facing a decline in CO2 emis-
sions as the economy grows. Sharma [36] investigated the factors that aﬀect CO2
emissions. His results show that GDP per capita in middle and low income coun-
tries, energy consumption, and urbanisation have statistically signiﬁcant positive
inﬂuence on CO2 emissions.
Studies that did not ﬁnd evidence of the EKC included one investigating
the relationship between CO2 emissions, GDP per capita, capital, labour force,
export, import, and energy consumption in Vietnam [4]. Jaunky [18] investigated
the relationship between CO2 emissions and income for high-income countries.
His results show that there is a positive impact running from GDP per capita to
CO2 emissions. In terms of income elasticity, CO2 emissions are inelastic in both
periods with the long run being more inelastic. This reﬂects that over time the
amount of CO2 emissions will be stable and the EKC hypothesis will not hold. In
addition, Al-Mulali et al. [4] found that fossil-fuel energy consumption increases
the pollutants, while renewable energy consumption has no signiﬁcant eﬀect in
reducing pollution. Furthermore, labour force helps reduce pollution. He and
Richard [16] studied the relationship between per capital GDP and per capita
CO2 emissions in Canada. They used a semi-parametric and a non-ﬂexible mod-
elling methods and found little evidence in favour of the environmental Kuznets
curve hypothesis. Wong and Lewis [40], instead of estimating the EKC for air
pollutants, utilised water quality variables on the Lower Mekong Basin region in
Asia. They did not ﬁnd evidence of an EKC for water pollutants. They suggested

Emissions, Trade Openness, Urbanisation, and Income in Thailand
521
that the results are entirely dependent on the model, error speciﬁcation, and the
type of pollutants as well.
Most of the previous studies above are based on the original empirical model
speciﬁcation for the EKC hypothesis which involves both GDP and squared
of GDP variables in the model. Narayan and Narayan [23] pointed out that the
results from the old model may have suﬀered from the problem of multicollinear-
ity between GDP and its square.
3
Methodology and Data
3.1
Empirical Model
Following Saboori et al. [30], the general form of the EKC hypothesis can be
formulated as
E = f(Y, Y 2, Z)
(1)
where E is a level of pollution, Y is the income, and Z are other variables which
aﬀect the emissions. The quadratic form of income (Y 2) will allow the model
to capture the inverted U-shape relationship between emissions and income.
However, Narayan and Narayan [23] pointed out that including the squared and
cubed income may result in a biased result due to high correlation between them
and suggested the alternative empirical model which excludes the squared term.
To empirically investigate the relationship between emissions, income, energy
consumption, and trade openness in Thailand, the study will follow a similar
methodology as in Al-Mulali et al. [4] which used the new model speciﬁcation
for the EKC hypothesis study proposed by Narayan and Narayan [23]. Two types
of emissions are investigated in this study: CO2 emissions (denoted as C) and
SO2 emissions (denoted as S). Therefore, the empirical model to be estimated
can be speciﬁed as
ln Et = α0 + α1 ln Yt + α2 ln ENt + α3 ln TRt + α4 ln URt + εt
(2)
where ln is natural logarithm1; Et demotes emissions per capita; Yt is real GDP
per capita; ENt is energy use per capita; TRt is trade openness; URt is an
urbanisation; εt is assumed to be the normal distributed error term with constant
variance and zero mean.
However, a drawback of this model speciﬁcation is that the turning point for
income where emission begins to decline cannot be found, but the applicability
of EKC hypothesis can still be investigated. The parameters in the model will
be estimated under long-run and short-run frameworks (more on this later). By
1 Using the logarithm transformation could reduce the risk of having heteroskedas-
ticity and allows the estimated coeﬃcients to be interpreted as elasticity, hence it
reﬂects the impact from a percentage change in explanatory variable to the percent-
age change in dependent variable [41]. Also, Acaravci and Ozturk [1] pointed out
that “the growth rate of the relevant variable will be obtained by their diﬀerenced
logarithms.”.

522
R. Osathanunkul et al.
comparing the income elasticity of emissions (α1) between the long run and the
short run, one can conclude that EKC hypothesis is applicable if the long run is
lower given that both values are positive [23]. The α2 > 0 indicates that energy
consumption increases the emissions. Otherwise, it will decrease the emissions
if α2 < 0. If an opening up to trade leads to the environmental improvement,
then α3 < 0. Otherwise, α3 > 0 implies that trade contributes to environmental
degradation. For the impact on emissions from urbanisation, it is expected to be
negative (α4 < 0) as found by Arouri et al. [6].
3.2
Data Description
The period of study is limited by the availability of the data. The annual time
series data within the period of 1971 to 2014 obtained from diﬀerent sources will
be used. Table 1 shows the descriptive statistics of the data.
Table 1. Descriptive statistics of variables, 1971 to 2014
Variable
Mean
Median
Maximum Minimum SD
CO2 emissions per capita
2.059
2.061
4.099
0.473
1.246
SO2 emissions per capita
0.010
0.010
0.018
0.004
0.004
Real GDP per capita
2911.747 2982.571 5635.643
946.849
1495.951
Energy use per capita
960.190
859.703 2012.058
360.578
526.189
Trade openness
84.540
78.213
140.437
34.802
36.363
Urbanisation
31.787
29.848
49.174
21.442
7.308
3.2.1
Data from the World Bank
Thailand’s real GDP per capita is measured as 2010 US dollar, trade openness
which is the sum of exports and imports of goods and services measured as a
share of gross domestic product, and the urbanisation measured as the percent-
age of urban population to the total population.
3.2.2
Data from the International Energy Agency (IEA)
Thailand’s energy use based on Total Primary Energy Supply (TPES) is mea-
sured as KTOE (thousand metric tons of oil equivalent). The data will later
be transformed into per capita term and measured as KGOE (kilograms of oil
equivalent).
3.2.3
Data from the Emissions Database for Global Atmospheric
Research (EDGAR)
The data for CO2 emissions and SO2 emissions, measured as gigagram (Gg), are
transformed into metric ton per capita. However, the data for SO2 emissions is

Emissions, Trade Openness, Urbanisation, and Income in Thailand
523
only available up to 2010. The CO2 emissions are evaluated based on fossil fuel
use and industrial processes (cement production, carbonate use of limestone and
dolomite, and non-energy use of fuels and other combustion). The data exclude
short-cycle biomass burning (such as agricultural waste burning) and large-scale
biomass burning (such as forest ﬁres).
3.3
The ARDL Bounds Testing Approach to Cointegration
The autoregressive distributed lag (ARDL) bound testing approach to cointegra-
tion proposed by Pesaran and Shin [26] and developed to be used with a small
sample size by Narayan [22] will be employed to investigate the relationship
between the emissions, income, and energy consumption and trade openness.
Faridi and Murtaza [13] suggest that the ARDL approach is far superior to the
Engle-Granger and Johansen technique when working with a small sample size.
Pesaran et al. [28] and Ahmed et al. [3] point out that the ARDL approach has a
high ﬂexibility. For instance, it can be used with a mix of I(0) and I(1) variables
whereas the Engle and Granger method has to use I(1) variables. Thus, it is
not necessary to conduct a unit roots test. Shahe Emran et al. [35] argue that
the ARDL method could correct for an endogeneity of explanatory variables.
Shahbaz et al. [33] assert that a dynamic unrestricted error correction model
(UECM) used in the ARDL approach “integrate[s] the short run dynamics
with the long-run equilibrium without losing any long-run information”. Lastly,
the estimates are unbiased and eﬃcient [2]. The UECM speciﬁcation in Arouri
et al. [6] was diﬀerent in terms of lag length speciﬁcation compared to other
papers in this literature. Thus, this study follows Shahbaz et al. [34] who have
authored many papers in the EKC literature using the ARDL methodology. The
UECMs for each case of emissions are expressed as follows:
Δ ln Et = α0+
p

i=1
α1iΔ ln Et−i +
p

i=0
α2iΔ ln Yt−i +
p

i=0
α3iΔ ln ENt−i
+
p

i=0
α4iΔ ln TRt−i +
p

i=0
α5iΔ ln URt−i + γ1 ln Et−1
+γ2 ln Yt−1 + γ3 ln ENt−1 + γ4 ln TRt−1 + γ5 ln URt−1 + ωt
(3)
where Δ is the ﬁrst diﬀerence operator; ωt is independently and identically dis-
tributed error term; p represents the lag length. The optimal lag lengths are
decided using 2 diﬀerent criteria that are the Akaike information criterion (AIC)
and Schwarz criterion (SC). In the case that each criterion provides a diﬀerent
lag length, the SC will be used due to its success and popularity in time series
modelling [9]. Pesaran et al. [27] argue that SC performs better than AIC in
a small sample size and provides a consistent model selection. Nevertheless,
Halicioglu [14] suggests that in a situation or case where SC gives the optimal
lag length of zero, AIC should be used instead. Equation 3 will be estimated

524
R. Osathanunkul et al.
using the ordinary least squares (OLS) method. The conclusion for the existence
of long-run relationships among the variables depends upon the F-test for the
joint signiﬁcance of the lagged levels of the variables. The null hypothesis of no
cointegration can be speciﬁed as
H0 : γ1 = γ2 = γ3 = γ4 = γ5 = 0
Narayan [22] provides two sets of critical values for the test at diﬀerent levels
of statistical signiﬁcance. The lower bound critical value is used if all variables
are I(0) and the upper bound is used when all variables are I(1). If the F-test
statistic lies above the upper bound critical value, the null hypothesis is rejected,
concluding that variables are cointegrated. This can be applied even in the case
of a mix of I(0) and I(1) variables. If the test statistic is lower than the lower
bound critical value, the null hypothesis cannot be rejected. However, if the test
statistic lies between the lower and upper bound critical values then the result
would be inconclusive.
3.4
Long-Run Coeﬃcient Estimation
If cointegration among the variables exists, the next step is to apply the ARDL
model speciﬁcation to estimate the long-run equilibrium model using the ordi-
nary least squares (OLS) method. The ARDL model for this study can be written
as
ln Et = θ0+
k

i=1
θ1i ln Et−i +
m

i=0
θ2i ln Yt−i +
n

i=0
θ3i ln ENt−i
+
p

i=0
θ4i ln TRt−i +
q

i=0
θ5i ln URt−i + μt
(4)
where k, m, n, p, and q are the lag lengths. The lag lengths will be selected
using AIC, SC and HQ. As in the cointegration test, the lag lengths from SC
will be used in cases where other criteria provide a diﬀerent lag length. θ are
the estimated long-run multipliers. μ is the error term. The estimates obtained
from the ARDL model will provide long-run coeﬃcients for the cointegrating
equations which can be speciﬁed as follows:
Et = α0 + α1 ln Yt + α2 ln ENt + α3 ln TRt + α4 ln URt + εt
(5)

Emissions, Trade Openness, Urbanisation, and Income in Thailand
525
where
α0 =
θ0
1 −
k
i=1
θ1i
; α1 =
m

i=0
θ2i
1 −
k
i=1
θ1i
; α2 =
n
i=0
θ3i
1 −
k
i=1
θ1i
α3 =
p
i=0
θ4i
1 −
k
i=1
θ1i
; α4 =
q
i=0
θ5i
1 −
k
i=1
θ1i
We can see that Eq. (5) is the same as Eq. (2), but the values of coeﬃcients
are now obtained through the ARDL modelling. Estimation of the long-run
coeﬃcients by imposing the OLS method directly on Eq. (2) may give a biased
result since it does not take into account non-stationary variables, which is not
the case for the ARDL approach [25]. In addition, the estimated coeﬃcients are
considered to be super consistent [39].
3.5
Short-Run Coeﬃcient Estimation
After the ARDL model is estimated for the long-run coeﬃcients, the next step is
to estimate short-run dynamic coeﬃcients by employing the error correction rep-
resentation of the ARDL model. The general error correction model is speciﬁed
as follows:
Δ ln Et = λ0+
k

i=1
λ1iΔ ln Et−i +
m

i=0
λ2iΔ ln Yt−i +
n

i=0
λ3iΔ ln ENt−i
+
p

i=0
λ4iΔ ln TRt−i +
q

i=0
λ5iΔ ln URt−i + ψECTt−1
(6)
Here, ψ is the coeﬃcient of the error correction term. It represents the speed
of adjustment and is expected to have value between −1 and 0. This is the
requirement to guarantee that the disequilibrium in dependent variables will
eventually converge to an equilibrium level. ECTt−1 denotes the error correction
term obtained from the cointegrating equation.
As already mentioned, the applicability of the EKC hypothesis can be seen
from a comparison between long run elasticity and short run elasticity of emis-
sions with respect to income. If they are statistically signiﬁcant and the long
run is lower than the short run, it can be concluded that there may be an
inverted U-shape relationship between emissions and income, hence the EKC
hypothesis [23].

526
R. Osathanunkul et al.
3.6
Model Stability Tests
This study also applies the cumulative sum (CUSUM) and cumulative sum of
squares (CUSUMSQ) tests on the estimated ARDL model to ensure the stability
of the estimated coeﬃcients. Halicioglu [14] argues that the results of cointegra-
tion do not imply that the estimated coeﬃcients are stable. The tests focus on the
recursive residuals which are standardised one-step-ahead prediction error [19].
In addition, the estimated model is likely to under-predict the period out of
the sample range and result in recursive error. The CUSUM and CUSUMSQ
tests are based on the plot of the sum of the recursive and test against the null
hypothesis that it has an expected value of zero.
4
Empirical Results and Discussion
4.1
Unit Roots Tests
Although the unit roots test may be unnecessary as the ARDL approach can be
used with variables being either I(0) or I(1), it is better to be cautious and apply
the unit roots test to make sure that all variables are not I(2). We ﬁrst employed
two unit roots tests—the Augmented Dickey-Fuller (ADF) unit roots test and
KPSS unit roots test—under two model speciﬁcations; with the intercept, and
with the intercept and trend terms.
Table 2. Augmented Dickey-Fuller unit roots test
Variable Level
1st Diﬀerence
Intercept
Intercept and trend Intercept
Intercept and trend
Lag Test statistic Lag Test statistic
Lag Test statistic Lag Test statistic
ln C
1
−1.044
1
−1.269
0
−4.017∗
0
−4.065∗∗
ln S
1
−1.830
1
−1.873
0
−3.706∗
0
−3.747∗∗
ln Y
1
−1.443
1
−1.629
0
−3.883∗
0
−4.041∗∗
ln EN
1
−0.261
3
−2.684
0
−4.8923∗
0
−4.815∗
ln TR
0
−1.295
0
−2.083
0
−6.964∗
0
−6.983∗
ln UR
2
0.738
1
−2.831
1
−1.691
1
−2.026
Note: The null hypothesis of ADF test is non-stationary. ∗and ∗∗denotes statistical
signiﬁcant at 1%, 5% level respectively
The results of the ADF tests as shown in Table 2 indicate that most of the
variables are stationary at the 1st diﬀerence with a rejection of the null hypothe-
sis of non-stationary at 5% level of statistical signiﬁcance, while the urbanisation
is not stationary. The ﬁndings from KPSS test as presented in Table 3 showed
trade openness to be stationary at the level, while the rest are stationary at the
1st diﬀerence. Furthermore, we employ the ZA unit roots test proposed by Zivot
and Andrews [42] since the ADF and KPSS unit roots test may give a biased
result if the structural break is present in time series data. Following Arouri

Emissions, Trade Openness, Urbanisation, and Income in Thailand
527
Table 3. KPSS unit roots test
Variable Level
1st Diﬀerence
Intercept
Intercept and trend Intercept
Intercept and trend
NW Test statistic NW Test statistic
NW Test statistic NW Test statistic
ln C
5
0.809∗
1
0.145∗∗∗
3
0.221
0
0.105
ln S
5
0.524∗∗
4
0.125∗∗∗
3
0.201
3
0.066
ln Y
5
0.828∗
5
0.153∗∗
3
0.169
3
0.073
ln EN
5
0.826∗∗∗
5
0.093∗∗∗
4
0.098
4
0.091
ln TR
5
0.822∗
4
0.112
1
0.128
2
0.069
ln UR
5
0.805∗
5
0.156∗∗
5
0.259
5
0.164∗∗
Note: NW denotes Newey-West automatic bandwidth. The null hypothesis of KPSS test
is stationary. ∗, ∗∗, and ∗∗∗denotes the rejection of null hypothesis at 1%, 5%, and 10%
level of statistical signiﬁcance, respectively.
Table 4. ZA unit roots test
Variable Level
1st Diﬀerence
Lag Test statistic Break year Lag Test statistic Break year
ln C
1
−3.489
1989
0
−5.707∗
1997
ln S
1
−3.933
1990
0
−4.361
1988
ln Y
1
−3.957
1988
0
−4.958∗∗∗
1996
ln EN
3
−3.688
1988
0
−6.848∗
1984
ln TR
0
−3.479
1988
0
−7.817∗
1987
ln UR
1
−5.025∗∗∗
2001
1
−5.075∗∗∗
2001
Note: The critical values for 1%, 5%, and 10% level of signiﬁcance are −5.57,
−5.08, and −4.82, respectively. The null hypothesis of the test is non-stationary.
∗, ∗∗, and ∗∗∗denotes the rejection of null hypothesis at 1%, 5%, and 10% level
of statistical signiﬁcance, respectively
et al. [6] and Shahbaz et al. [32], the intercept and trend model of the ZA unit
roots test is employed in this study.
The ZA unit root test results from Table 4 show that urbanisation is sta-
tionary at the level and the rest are stationary at the 1st diﬀerence. Combining
all the tests, it can be concluded that the data is a mixture of I(0) and I(1)
variables. Therefore, the ARDL bounds test for cointegration is appropriate in
this study.
4.2
Cointegration Test
After the unit roots tests indicating that all variables are not I(2), the next step
is to ﬁnd the existence of a long-run relationship between variables using the
ARDL bounds test for cointegration.
The results from Table 5 indicate that there is a cointegrating equation in case
of CO2 emissions only. This means there is a long run relationship between CO2

528
R. Osathanunkul et al.
Table 5. ARDL cointegration test results
Case
Lag length
F-statistic
Conclusion
CO2
1
10.525∗
Cointegrated
SO2
1
2.256
Not cointegrated
Critical value Lower bound Upper bound
At 1% level
4.428 (4.590) 6.250 (6.368)
At 5% level
3.202 (3.276) 4.544 (4.630)
At 10% level
2.660 (2.696) 3.838 (3.898)
Note:
1. ∗, ∗∗and ∗∗∗denotes statistically signiﬁcant at 1%, 5% and 10%
level respectively.
2. The critical values are from the Case III table provided by Narayan
(2005) with k = 4, n = 40 for CO2 emissions and n = 35 for SO2
emissions (provided in parenthesis).
emission, income, energy consumption, and trade openness in Thailand and that
the EKC hypothesis can be investigated. On the other hand, such relationship is
not found for SO2 emissions. Since the cointegration test indicates that variables
are cointegrated, the next step is to estimate the relevant ARDL models.
Table 6 shows that the ARDL(1, 1, 1, 1, 1) is selected. The value of R2
is 0.99 which is very high, indicating the strong correlation among variables.
The diagnostic tests, under 1% level of statistical signiﬁcance, indicate that the
estimated ARDL model does not suﬀer from serial correlation, heteroskedasticity
nor have model misspeciﬁcation.
4.3
Long-Run and Short-Run Elasticities
Using the results from the selected ARDL model, the long-run elasticities can
be obtained as shown in Table 7.
It can be seen that all variables, except for trade openness, are statistically
signiﬁcant at 1% level. The real GDP per capita has positive impact on CO2
emissions. It seems CO2 emissions is income elastic as an increase in real GDP
per capita by 1% will increase the emissions by 1.43% in the long run. Energy
consumption also has positive relationship with CO2 emissions. It shows that
0.32% increase in CO2 emissions is resulted from 1% increase in energy consump-
tion in the long run; hence, CO2 emissions is energy inelastic. Trade openness
has negative impact on CO2 emissions. The expansion of international trade by
1% is expected to decrease CO2 emissions by approximately 0.15% but it is not
statistically signiﬁcant. Lastly, urbanisation greatly reduces CO2 emissions at
the rate of 1.08% for each 1% increase in the share of urban population to total
population.
To ﬁnd the applicability of EKC hypothesis, the estimation of short-run
dynamic coeﬃcients using error correction representation of the ARDL model
is required [23]. The results for short-run elasticities are shown in Table 8. The

Emissions, Trade Openness, Urbanisation, and Income in Thailand
529
Table 6. ARDL Model estimation results
Panel I: ARDL(1, 1, 1, 1, 1) Dependent variable: ln Ct
Regressor
Coeﬃcient Standard error T - Ratio Prob.
ln Ct−1
0.292∗∗
0.141
2.072
0.046
ln Yt
0.875∗
0.182
4.817
0.000
ln Yt−1
0.138
0.263
0.524
0.604
ln ENt
0.328∗∗
0.133
2.458
0.019
ln ENt−1
−0.102
0.131
−0.777
0.443
ln TRt
0.067
0.065
1.032
0.309
ln TRt−1
−0.175∗
0.062
−2.829
0.008
ln URt
0.784
0.719
1.091
0.283
ln URt−1
−1.547∗∗
−2.136
8.692
0.040
Constant
−6.033∗
1.126
−5.356
0.000
R2
0.999
Adjusted R2
0.999
Panel II: Diagnostic tests
A: Serial correlation
F(1, 32) = 1.966 (0.171)
B: Functional form
F(1, 32) = 5.345 (0.027)
C: Heteroskedasticity F(1, 40) = 0.788 (0.380)
Note:
1. The null hypothesis for the diagnostic tests is as follows. (A): no ser-
ial correlation, (B): no functional form misspeciﬁcation, and (C): no Het-
eroskedasticity. For information about the tests, see [25].
2. ∗, ∗∗, and ∗∗∗indicate statistically signiﬁcant at 1%, 5%, and 10% level,
respectively.
coeﬃcient of the error correction term has negative sign and is statistically sig-
niﬁcant at 1% level, conﬁrming the existence of long-run relationship among
variables and indicating that approximately 71% of disequilibrium in CO2 emis-
sions is adjusted within 1 year. The elasticity of CO2 emissions with respect to
real GDP per capita in the short run is 0.88 and statistically signiﬁcant at 1%
level, showing that a 1% increase in real GDP per capita will only increase CO2
emissions by 0.88%, compared with the long run which is 1.43%. This means in
the long run the impact from GDP toward CO2 emissions increases as economy
grows, rejecting an applicability of EKC hypothesis. The greater long run GDP
elasticity of CO2 emissions may imply that Thailand, as developing country, is in
the beginning phase of EKC hypothesis where an increase in income will result
in environmental degradation. Hence, the current data suggest that it has not
reached the turning point. The result is contrast with the previous studies [6,29].
For energy consumption, it is statistically signiﬁcant at 1% level. The elastic-
ity of CO2 emissions with respect to energy consumption per capita in the short
run is about 0.33. This indicates that if energy consumption increases by 1%,
CO2 emissions is expected to increase by 0.33%, which is inelastic. Similarly, the

530
R. Osathanunkul et al.
Table 7. Estimated long-run coeﬃcients
Dependent variable: ln Ct
Regressor Coeﬃcient Standard error T - Ratio Prob.
ln Yt
1.430∗
0.149
9.622
0.000
ln ENt
0.320∗
0.095
3.357
0.002
ln TRt
−0.153
0.121
−1.261
0.216
ln URt
−1.076∗
0.097
−11.107
0.000
Note: ∗indicates statistically signiﬁcant at 1% level.
Table 8. Estimated short-run coeﬃcients
Dependent variable: Δ ln Ct
Regressor Coeﬃcient Standard error T - Ratio Prob.
Δ ln Yt
0.875∗
0.138
6.348
0.000
Δ ln ENt
0.328∗
0.104
3.147
0.004
Δ ln TRt
0.067
0.048
1.400
0.171
Δ ln URt
0.784∗∗
0.376
2.086
0.045
ECTt−1
−0.708∗
0.092
−7.668
0.000
Constant
−6.033
0.787
−7.681
0.000
Note: ∗, ∗∗, and ∗∗∗indicates statistically signiﬁcant at 1%,
5% and 10% level, respectively.
energy elasticity of CO2 emissions in the long run is about 0.32%. If nothing
has been done, CO2 emissions will increase further due to the use of energy in
the long run though the impact seems to be slightly lower. Since the values are
very close, it may be appropriate to conclude that the eﬀect of energy use on
CO2 emissions is rather stable over time. The ﬁnding is contrast to Saboori and
Sulaiman [29] where the elasticity is increasing over time and their study found
that CO2 emissions is energy elastic in the long run.
The elasticity of CO2 emissions with respect to trade openness in the short
run shows a positive sign. An increase in trade openness by 1%, CO2 emissions
is expected to increase 0.07% approximately, which is very small, suggesting
that trade openness is inelastic. However, it is not statistically signiﬁcant, in
contrast to Arouri et al. [6]. The insigniﬁcant positive elasticity may reﬂect that
the factor endowment hypothesis is not completely dominated by the pollution
haven hypothesis.
For the case of urbanisation, a 1% increase is expected to increase CO2 emis-
sions by 0.78% which is contrast to that of Arouri et al. [6]. In addition, compar-
ing with the result from the long run, it seems that the eﬀect completely shifts to
opposite direction where urbanisation reduces CO2 emissions. This suggests that
the CO2 emissions will increase when the urban area begins to expand. With
more migrations into cities and ineﬃcient transportation management, more
pollution is likely. As things start to settle down, a better public transportation

Emissions, Trade Openness, Urbanisation, and Income in Thailand
531
and establishment is now well planned, people in urban area may tend to use
less private transportation and experience a shorter commuting time reducing a
chance of having higher CO2 emissions later on.
4.4
Cumulative Sum (CUSUM) and Cumulative Sum of Squares
(CUSUMSQ) Tests
The estimated ARDL model does not indicate that the estimated coeﬃcients
are stable over time. The conclusion for the stability of estimated coeﬃcients
depends upon the line plotted by the test. If the plotted lines are within 5%
level of signiﬁcance range, it is possible to say that the coeﬃcient is stable over
time [32].
-20
-15
-10
-5
0
5
10
15
20
82
84
86
88
90
92
94
96
98
00
02
04
06
08
10
12
14
CUSUM
5% Significance
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
82
84
86
88
90
92
94
96
98
00
02
04
06
08
10
12
14
CUSUM of Squares
5% Significance
Fig. 1. CUSUM and CUSUMSQ plots
According to Fig. 1, the straight lines represent critical bounds at 5% level of
signiﬁcance. The coeﬃcients estimated from the ARDL model are stable because
the CUSUM and CUSUMSQ test lines are within the critical bounds.
5
Conclusions and Policy Implications
This study investigates the relationship between emissions, trade openness,
urbanization, and income in Thailand over the period of 1971 to 2014 and exam-
ines the applicability of EKC hypothesis using a new model speciﬁcation. CO2
emissions and SO2 emissions are investigated in this study. The ARDL cointe-
gration technique is employed and CUSUM and CUSUMSQ tests are used to
ensure the stability of the estimated results.
The results indicate that there is a long run relationship among variables
for the case of CO2 emissions while there is none for the SO2. Therefore, only
determinants of CO2 emissions are further investigated in the study. Our results
suggest that EKC hypothesis is not found in Thailand. An increase in income
drives more CO2 emissions at increasing rate. Energy consumption also con-
tributes to environmental degradation with slight impact over time. In the case

532
R. Osathanunkul et al.
of trade openness, it does not exhibit signiﬁcant impact on the emissions, sug-
gesting the pollution haven hypothesis being oﬀset by the factor endowment
hypothesis. Lastly, we found that urbanisation contributes to CO2 emissions in
the short run, while it helps reduce CO2 emissions in the long run.
In terms of policy implications, the study suggests that Thai policy makers
should seriously focus on environmental regulations by setting a new emission
standard or encouraging the use of new environmentally friendly equipment in
industrial sector. In addition, the policy makers would also need to provide
incentives to both the private producers and consumers of polluting goods, so
there can be greater investment, adoption and widespread use of environmentally
friendly goods. Incentives can be in the form of tax breaks, loan guarantees, and
making technologies available to companies at low prices. In addition, taxes can
be imposed on high polluting sources while subsidies are earmarked towards
the low carbon technologies. This will signiﬁcantly reduce CO2 emissions in the
future.
Even though energy consumption, trade openness and urbanisation still have
moderate impacts on CO2 emissions compared to the one from income. The
study still would like to suggest that Thailand should put some eﬀorts in slowing
the growth of fossil based energy consumption. This can be achieved by invest-
ments in environmentally friendly and eﬃcient technology, substituting for fossil
based sources of energy with alternative sources such as wind and solar energy,
and providing incentives to consumers and industry to encourage the use of low
carbon alternatives. Turning to these renewable energy sources would help main-
tain a diversiﬁed future sustainable source of energy. In addition, it is believed
to reduced pollution related mortality, reduced natural climate related disasters,
improved health and productivity. As far as trade openness is concerned, the
more stringent environmental regulation on establishing foreign ﬁrms. In case of
urbanisation, the government should encourage the use of public transport with
eﬃcient provision of such services along with well-planned urbanisation.
To the extent how these variables are positively associated with carbon emis-
sions, an understanding of the mechanism and channels through which these
factors aﬀect carbon emissions will be required. Eﬀorts to identify these chan-
nels will be needed if meaningful carbon public policies and targets that yield
reduction in carbon emissions are to be achieved. Given the current lack of under-
standing on these mechanisms, eﬀorts to increase information in both the private
and public sectors should be encouraged. This may require the policy makers to
allocate resources to academic institutions and the private sector on researches
seeking to determine the contribution of diﬀerent production processes and mech-
anisms within the economy to aggregate emissions. A deeper understanding may
help policy makers with tools to design and adopt trade and economic policies
that help reduce CO2 emissions.
Areas for further research include expanding and conducting the analysis
on other types of emissions like particulate matters and considering the role
of technological transfer and adoption through variables such as foreign direct
investment in inﬂuencing income and emissions and the contribution of the value

Emissions, Trade Openness, Urbanisation, and Income in Thailand
533
chain to emissions. Furthermore, the future work may thoroughly investigate
the behaviour of emissions through the decomposition approach which concerns
about scale, composition, and technical eﬀects. This would provide a better view
of how economic expansion contributes to the emissions.
Acknowledgements. The authors would like to thank the anonymous reviewer for
useful suggestions which have greatly improved the quality of this paper. This research
is supported by the Chiang Mai University Research Funding and the Puay Ungphakorn
Centre of Excellence in Econometrics, Faculty of Economics, Chiang Mai University.
References
1. Acaravci, A., Ozturk, I.: On the relationship between energy consumption, CO2
emissions and economic growth in Europe. Energy 35(12), 5412–5420 (2010)
2. Afzal, M., Farooq, M.S., Ahmad, H.K., Begum, I., Quddus, M.A.: Relationship
between school education and economic growth in Pakistan ardl bounds testing
approach to cointegration. Pak. Econ. Soc. Rev. 48(1), 39–60 (2010)
3. Ahmed, M.U., Muzib, M., Roy, A.: Price-wage spiral in Bangladesh: evidence from
ardl bound testing approach. Int. J. Appl. Econ. 10(2), 77–103 (2013)
4. Al-Mulali, U., Saboori, B., Ozturk, I.: Investigating the environmental Kuznets
curve hypothesis in Vietnam. Energy Policy 76, 123–131 (2015)
5. Antweiler, W., Copeland, B.R., Taylor, M.S.: Is free trade good for the environ-
ment? Am. Econ. Rev. 91(4), 877–908 (2001)
6. Arouri, M., Shahbaz, M., Onchang, R., Islam, F., Teulon, F.: Environmental
Kuznets curve in Thailand: cointegration and causality analysis (2014-204) (2014)
7. Brack, D.: Trade and Environment: Conﬂict or Compatibility? Royal Inst. of Inter-
national Aﬀairs (1998)
8. Bureecam, C.: An empirical analysis based on the environmental Kuznets curve
hypothesis (ekc) in the relationship between Thailand’s economic growth and envi-
ronmental quality. In: 47th Kasetsart University Annual Conference, pp. 148–156.
Bangkok (2009)
9. Cavanaugh, J.E., Neath, A.A.: Generalizing the derivation of the schwarz informa-
tion criterion. Commun. Stat. Theory Methods 28(1), 49–66 (1999)
10. Cole, M.A.: Trade, the pollution haven hypothesis and the environmental Kuznets
curve: examining the linkages. Ecol. Econ. 48(1), 71–81 (2004)
11. Cole, M.A., Elliott, R.J.R.: Determining the trade-environment composition eﬀect:
the role of capital, labor and environmental regulations. J. Environ. Econ. Manag.
46(3), 363–383 (2003)
12. Dinda, S.: Environmental Kuznets curve hypothesis: a survey. Ecol. Econ. 49(4),
431–455 (2004)
13. Faridi, M.Z., Murtaza, G.: Disaggregate energy consumption, agricultural output
and economic growth in Pakistan. Pak. Dev. Rev. 52(4), 493–516 (2013)
14. Halicioglu, F.: An econometric study of CO2 emissions, energy consumption,
income and foreign trade in Turkey. Energy Policy 37(3), 1156–1164 (2009)
15. Hamit-Haggar, M.: Greenhouse gas emissions, energy consumption and economic
growth: a panel cointegration analysis from Canadian industrial sector perspective.
Energy Econ. 34(1), 358–364 (2012)
16. He, J., Richard, P.: Environmental Kuznets curve for CO2 in Canada. Ecol. Econ.
69(5), 1083–1093 (2010)

534
R. Osathanunkul et al.
17. Hossain, M.S.: Panel estimation for CO2 emissions, energy consumption, economic
growth, trade openness and urbanization of newly industrialized countries. Energy
Policy 39(11), 6991–6999 (2011)
18. Jaunky, V.C.: The CO2 emissions-income nexus: evidence from rich countries.
Energy Policy 39(3), 1228–1240 (2011)
19. Kennedy, P.: A Guide to Econometrics, 5th edn. Blackwell, Malden (2003)
20. Libecap, G.D.: Addressing global environmental externalities: transaction costs
considerations. J. Econ. Lit. 52(2), 424–479 (2014)
21. Naito, T., Traesupap, S.: Is shrimp farming in Thailand ecologically sustainable?
J. Fac. Econ. 16, 55–75 (2006)
22. Narayan, P.K.: The saving and investment nexus for China: evidence from cointe-
gration tests. Appl. Econ. 37(17), 1979–1990 (2005)
23. Narayan, P.K., Narayan, S.: Carbon dioxide emissions and economic growth: panel
data evidence from developing countries. Energy Policy 38(1), 661–666 (2010)
24. Pao, H.T., Tsai, C.M.: Multivariate Granger causality between CO2 emissions,
energy consumption, FDI (foreign direct investment) and GDP (gross domestic
product): evidence from a panel of BRIC (Brazil, Russian Federation, India, and
China) countries. Energy 36(1), 685–693 (2011)
25. Pesaran, B., Pesaran, M.H.: Time Series Econometrics Using Microﬁt 5.0. Oxford
University Press, Oxford (2009)
26. Pesaran, M.H., Shin, Y.: An autoregressive distributed lag modelling approach
to cointegration analysis, pp. 371–413. Cambridge University Press, Cambridge
(1999)
27. Pesaran, M.H., Shin, Y., Smith, R.J.: Bounds testing approaches to the analysis
of long-run relationships. Report, Faculty of Economics, University of Cambridge
(1999)
28. Pesaran, M.H., Shin, Y., Smith, R.J.: Bounds testing approaches to the analysis
of level relationships. J. Appl. Econ. 16(3), 289–326 (2001)
29. Saboori, B., Sulaiman, J.: CO2 emissions, energy consumption and economic
growth in Association of Southeast Asian Nations (ASEAN) countries: a coin-
tegration approach. Energy 55, 813–822 (2013)
30. Saboori, B., Sulaiman, J., Mohd, S.: Economic growth and CO2 emissions in
Malaysia: a cointegration analysis of the environmental Kuznets curve. Energy
Policy 51, 184–191 (2012)
31. ASEAN Secretariat: ASEAN Economic Community Blueprint. ASEAN Secre-
tariat, Jakarta, Indonesia (2008)
32. Shahbaz, M., Sbia, R., Hamdi, H., Ozturk, I.: Economic growth, electricity con-
sumption, urbanization and environmental degradation relationship in United
Arab Emirates. Ecol. Ind. 45, 622–631 (2014)
33. Shahbaz, M., Shabbir, M.S., Butt, M.S.: Eﬀect of ﬁnancial development on agri-
cultural growth in Pakistan. Int. J. Soc. Econ. 40(8), 707–728 (2013)
34. Shahbaz, M., Uddin, G.S., Rehman, I.U., Imran, K.: Industrialization, electricity
consumption and CO2 emissions in Bangladesh. Renew. Sustain. Energy Rev. 31,
575–586 (2014)
35. Shahe Emran, M., Shilpi, F., Alam, M.I.: Economic liberalization and price
response of aggregate private investment: time series evidence from India. Can.
J. Econ./Revue canadienne d’conomique 40(3), 914–934 (2007)
36. Sharma, S.S.: Determinants of carbon dioxide emissions: empirical evidence from
69 countries. Appl. Energy 88(1), 376–382 (2011)

Emissions, Trade Openness, Urbanisation, and Income in Thailand
535
37. Stern, D.I.: The environmental Kuznets curve: a primer. Report, Centre for Climate
Economics & Policy, Crawford School of Public Policy, The Australian National
University (2014)
38. Temurshoev, U.: Pollution haven hypothesis or factor endowment hypothesis: the-
ory and empirical examination for the US and China. Report, The Center for
Economic Research and Graduate Education - Economic Institute, Prague (2006)
39. Verbeek, M.: A Guide to Modern Econometrics, 4th edn. Wiley, Chichester (2012)
40. Wong, Y.L.A., Lewis, L.: The disappearing environmental Kuznets curve: a study
of water quality in the lower Mekong basin (LMB). J. Environ. Manage. 131,
415–425 (2013)
41. Wooldridge, J.M.: Introductory Econometrics: A Modern Approach, 6th edn. Cen-
gage Learning, Boston (2015)
42. Zivot, E., Andrews, D.W.K.: Further evidence on the great crash, the oil-price
shock, and the unit-root hypothesis. J. Bus. Econ. Stat. 10(3), 251–270 (1992)

Analysis of Risk, Rate of Return
and Dependency of REITs in ASIA with Capital
Asset Pricing Model
Rungrapee Phadkantha1, Woraphon Yamaka1,2,
and Roengchai Tansuchat1,2(B)
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
rungrapee.ph@gmail.com, woraphon.econ@gmail.com
2 Center of Excellence in Econometrics,
Chiang Mai University, Chiang Mai 50200, Thailand
roengchaitan@gmail.com
Abstract. This study introduces an approach to ﬁtting a copula based
seemingly unrelated regression to an interval-valued data set. This app-
roach consists of ﬁtting a model on the appropriate point of the inter-
val values assumed by the variables in the learning set. To ﬁnd the
appropriate point of the interval values, we assign weights in calculat-
ing the appropriate value between intervals by using convex combination
method. We apply this methodology to quantify the risk and dependence
of Real Estate Investment Trust (REITs) in Asia. Our results suggest
that Hong Kong and Japan markets have a positive sign of the beta
and both markets have less volatility than the global REITs market. On
the other hand, we ﬁnd that the estimated beta for Singapore market
shows a negative relationship with global REITs market. We conclude
that Singapore market can be viewed as a hedge against higher risk in
Asian REITs.
1
Introduction
This paper considers the relationship between returns and risk of Real Estate
Investment Trust (REITs) in Asia. The steady growth in the Asian property
market has been the major driver for the development of REITs in Asia over the
last ten years. In November 2000, Japan was the ﬁrst country in Asia to establish
a REIT market. Since then, there have been seven countries, namely Japan,
Singapore, South Korea, Thailand, Taiwan, Malaysia and Hong Kong, joined the
REIT market. In addition, other Asian countries such as China, India, Pakistan
and the Philippines are also in the process of implementing their own REIT
regimes. In just over a decade, the number of Asian REITs has grown to 138
REITs across the seven Asian REIT markets contributing a market capitalization
more than US $118 billion. Asian REITs have become a major component of
the global property portfolio, accounting for 12% of the global REIT market in
2012 [6].
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_38

Analysis of Risk, Rate of Return and Dependency of REITs in ASIA
537
Asian REITs have also delivered strong performance over the last ﬁve
years, signiﬁcantly outperforming the major REIT markets of the US, UK and
Australia. The development of Asian REITs is further supported by favorable
changes in the regulatory structures in recent years. This has been received pos-
itively by both local and international investors [9]. Due to the ongoing demand
for investment in the REITs, the price index tends to increase continuously over
the last decade.
However, the decline of the price index due to the subprime mortgage crisis
caused the investors to face huge risks, especially in Japan, Hong Kong and
Singapore. These three countries have a high market capitalization in Asian,
and the movements of their price indexes contribute a large eﬀect to the global
REITs markets [3]. Thus, this study would measure the risk of these three
markets. To measure the risk, one of the most well-known approaches, Capital
Asset Pricing Model (CAPM) of Sharpe [12] and Lintner [5] is considered in our
study. This approach has an ability to explain the relationship between expected
return and risk of each market. However, all these three markets belong to the
Asian REITs market, any change in each market conditions or policy regulations
may aﬀect other markets. So we can expect that there might exist a correla-
tion between each CAPM equation. Thus the model seems appropriate for this
structural conﬁguration is Seemingly Unrelated regression (SUR) model. After
Zellner [17] introduced multivariate regressions, Seemingly Unrelated regression
(SUR) model has become popular in both statistics and econometrics. In the
recent years, due to the strong assumption in multivariate normal, a Copula
approach of Sklar in 1959 has been applied to join the error term εi,t of SUR
model (see,Wichitaksorn [16], Pastpipatkul et al. [7]). Thus, this study employs
a copula based SUR model as proposed in Pastpipatkul et al. [7] to quantify the
risk of Japan, Hong Kong and Singapore markets. However, there has been other
recent extension of the CAPM to the interval data, as seen in Piamsuwannakit
et al. [11] and Phochanachan et al. [10]. Their work is about using the interval
data is range of highest and lowest data to predict the return of the stock and
applying CAPM to model this return. They found that using the interval data in
CAPM can provide a better result than using the closing price for prediction. In
this study, therefore, addresses a copula based SUR model for predicting interval
data by using a convex combination method which was introduced in Chanaim
et al. [2].
As a consequence, in this paper two contributions are made. First, copula
based SUR model is adapted to interval data characterized by convex combina-
tion method and applied to the CAPM approach. Second, we will quantify the
risk of Japan, Hong Kong and Singapore REIT markets and their dependency
The remainder of the paper is organized as follows. Section 2 brieﬂy reviews
a CAPM and a convex combination method, Sect. 3 presents our methodology,
and Sect. 4 presents the simulation study. The application of our purposed model
is reported in Sect. 5. Finally, Sect. 6 summarizes and presents the conclusions
of this paper.

538
R. Phadkantha et al.
2
Review
2.1
Capital Asset Pricing Model
The basic idea behind Capital asset pricing model (CAPM) consists of two parts.
The ﬁrst part is the time value of money is the idea that money available at
the present time is worth more than the same amount in the future due to its
potential earning capacity. This core principle of ﬁnance holds that, provided
money can earn interest, any amount of money is worth more the sooner it is
received. TVM is also referred to as present discounted value which corresponds
to the risk-free. The risk-free rate is the minimum return an investor expects
for any investment with zero risk. In many application studies, the interest rate
on a government bond and Treasury bill is often used as the risk-free rate for
investors. The second part of the CAPM formula represents risk β and calculates
the amount of compensation the investor needs for taking on additional risk. β
reﬂects how risky an asset is compared to overall market risk and is a function
of the volatility of the asset and the market as well as the correlation between
the two.
βi = cov(rit, rMt)
σ2
M
(1)
where rit is the return of the asset i,rMt is the return of the market, and σ2
M is
the variance of the return of the market. According to the studies of Sharpe [12]
and Lintner [5], CAPM is used to describe the relationship between beta of an
asset and its corresponding return and returns can be explained as:
rit −rft = β0 + βi,(rMt −rft) + εit,
(2)
where rft is risk free, εit is the random disturbance term in the CAPM at time
t.The coeﬃcient βi refers to a relationship between the return of asset i(rit) and
the rate of return on market (rMt). Moreover, it is also viewed as a systematic
risk. It represents a sensitivity of the asset i to the overall market. If βi < 1, the
asset is less volatile than the market. On the other hand, βi > 1,the asset is more
volatile than the market. In this study, we can view Hong Kong, Singapore, and
Japan REITs as rit while the global REITs can be viewed as rMt.
2.2
Center Method
The center method was ﬁrst applied to time series model by Billard and Diday
[1]. They proposed this method to the linear regression model. The main idea
of this method is the estimated slope parameter is based on the center or mid-
point of the interval data. Let Yt =

Y U
t , Y L
t

, i = 1, ..., T where Y U
t denotes the
upper bound value of observed interval Yi while Y L
t
denotes the lower bound of
observed interval Yi. To obtain the mid-point value of Yi, we can derive by
Y C
t
= Y U
t
+ Y L
t
2
(3)

Analysis of Risk, Rate of Return and Dependency of REITs in ASIA
539
2.3
Convex Combination Method
However, Chanaim et al. [2] suggested that the center method may lead to the
misspeciﬁcation problem since the midpoint of the intervals might not be the
good representative of the intervals. To overcome this problem, Chanaim et al.
[2] suggested a convex combination approach for interval data, which can be
derived by
Y CC = wY U
t
+ (1 −w)Y L
t ,
w ∈[0, 1]
(4)
where w is the weight parameter of the interval data with values [0,1]. The
advantage of this method lies in the ﬂexibility to assign weights in calculating
the appropriate value between intervals.
3
Methodology
3.1
Copula Based Seemingly Unrelated Regression with Interval
Valued Data
After Zellner [17] introduced multivariate regressions, Seemingly Unrelated
regression (SUR) model has become popular in both statistics and economet-
rics. In the recent years, due to the strong assumption in multivariate normal,
a Copula approach of Sklar [13] in 1959 has been applied to join the error term
of SUR model by Pastpipatkul et al. [7]. The present study addresses a copula
based SUR model for predicting interval data. Our approach consists of ﬁtting
a model to the appropriate point of the interval values assumed by the interval
variables in the learning set and applies this model to the lower and upper bounds
of the interval values of the explanatory interval variables, Xi,t = [XU
i,t, XL
i,t] to
predict the lower and upper bounds of the interval value of the dependent vari-
able, Yit =

Y U
it , Y L
it

respectively. Consider the structure of SUR model with n
equations, the typical setup of SUR
Yi,t = X′
i,tβi + εi,t
, i = 1, ..., n; t = 1, ...T
(5)
where Yi = (T × 1) vector of dependent variable in equation ith. Xi is (T × K)
matrix of Ki independent variables or explanatory variables in equation ith and
βi is (K ×1) vector of an unknown parameters in each equation ith. εi,t is a vec-
tor of the error terms which are independent and identically distributed and also
assumed to be correlated. Generally, the dependence between εi,t are modeled
through a multivariate distribution, especially the multivariate normal distribu-
tion. However, a limitation of multivariate distribution to the multivariate SUR
model is the linear relationship. The correlation analysis can only capture linear
relationship, but not many other types of dependence. Moreover, it cannot be
used for heavy-tailed distributions. To relax this assumption, we can use the cop-
ulas to model the nonlinear dependence of error terms in the multivariate SUR
model [7,8]. In this study, we address a copula based SUR model for predicting

540
R. Phadkantha et al.
interval data, therefore a convex combination method is employed and we can
rewrite Eq. 5 as follows:
wyYi,t + (1 −wy)Yi,t = (wi1X1,t + (1 −wi1)X1,t )β1 + , ...,
+(wikXk,t + (1 −wik)Xk,t )βi + εi,t
(6)
3.2
Copulas
According to a general Sklar’s theorem in 1959 [13], let H be an n-dimensional
distribution with marginals Fi i = 1, 2, ...n. Then there exists an n-copula C
such that for all x1, ..., xn in ¯R
H(x1, ..., xn) = C(F1(x1), ..., Fn(xn))
(7)
where C is copula distribution function of a n-dimensional random variable.
Furthermore, if the marginals are continuous, then the copula C is unique. Oth-
erwise, C is uniquely determined on n
i=1 R(Fi) where R(Fi) is denoted as the
range of the marginal function Fi. Conversely, if C is an n-copula C and Fi
i = 1, 2, ...n are univariate marginal distributions of n variables, then the func-
tion H : ¯Rn →[0, 1] is also deﬁned in Eq. (7). We can model the marginal
distribution and joint dependence separately. If we have a continuous marginal
distribution, the copula can be determined by
C(u1, ..., un) = C(F −1
1
(u1), ..., F −1
n (un))
(8)
where u is uniform [0,1]. There are two important classes of copula, namely
Elliptical copulas and Archime-dean copulas. The symmetric Gaussian and t-
copulas are the families of the copula in Elliptical class, which are simply the cop-
ulas of elliptical contoured distribution. For the asymmetric dependence cases,
Clayton, Gumbel, Joe, and Frank are four important families in Archimedean
class. In this study, we consider these six copulas to join the error terms of SUR
model. For a brief review of the Copula approach, refer to Joe [4], Smith [15]
and Smith, Gan and Kohn [14].
4
Experiment Study
4.1
Simulation Study
In the simulation study, we applied elliptical copulas (Gaussian and Student-t)
and Archimedean copulas (Clayton, Gumbel, Joe and Frank) to model the depen-
dence structure of the SUR. In this study, the simulation is the realization of
SUR with two equations, thus we generated random data from the following
speciﬁcations
wy1Y U
1,t + (1 −wy1)Y L
1,t
wy2Y U
2,t + (1 −wy2)Y L
2,t

=
 2 + 1(w11XU
11,t + (1 −w11)XL
11,t) + ε1,t
0.5 −2(w21XU
21,t + (1 −w21)XL
21,t) + ε2,t (9)

Analysis of Risk, Rate of Return and Dependency of REITs in ASIA
541
Table 1. True parameter value of simulation study when n=100
Parameter Gaussian Student-t Clayton
Gumbel
Joe
Frank
β01
2.0002
2.0003
2.0653
1.9974
1.9961
2.0241
(0.0051)
(0.0038)
(0.0420) (0.0489) (0.0063) (0.0285)
β11
0.9250
1.0286
1.0775
0.9221
0.8283
1.0470
(0.0335)
(0.0336)
(0.0976) (0.0489) (0.0415) (0.0941)
σ1
0.3283
0.3361
0.4461
0.4087
0.3797
0.4532
(0.0164)
(0.0166)
(0.0250) (0.0489) (0.0184) (0.0267)
β02
0.4298
0.5257
0.5388
0.4101
0.3528
0.5067
(0.0326)
(0.0331)
(0.0955) (0.0468) (0.0412) (0.0935)
β21
−2.0008
−2.0013
−1.9988 −1.9969 −2.0001 −1.9848
(0.0045)
(0.0037)
(0.0266) (0.0087) (0.0071) (0.0297)
σ1
0.8299
0.8328
0.9479
0.9950
0.9584
0.9593
(0.0031)
(0.0173)
(0.0285) (0.0209) (0.0186) (0.0285)
ν2
3.2548
0.4111
0.3598
3.5455
3.7454
3.8424
(0.1588)
(0.0255)
(0.0157) (0.5846) (0.8657) (0.8386)
wy1
0.2984
0.3001
0.3011
0.3017
0.2977
0.3075
(0.0026)
(0.0019)
(0.0147) (0.0046) (0.0043) (0.0164)
w11
0.3992
0.3993
0.3970
0.3950
0.4012
0.3958
(0.0031)
(0.0022)
(0.0226) (0.0209) (0.0053) (0.0217)
wy2
0.5004
0.4989
0.4869
0.4933
0.4990
0.5010
(0.0042)
(0.0030)
(0.0295) (0.0084) (0.0069) (0.0935)
w21
0.5996
0.5997
0.5823
0.5996
0.5964
0.5950
(0.0046)
(0.0032)
(0.0273) (0.0083) (0.0076) (0.0276)
θ
0.5001
0.5201
5.9945
5.0001
5.0021
4.1111
(0.0255)
(0.0015)
(0.8569) (0.4936) (0.5302) (0.9706)
Source: Calculation
Note: () is standard error.
where wy1, w11, wy2, w21 are set to be 0.3, 0.4, 0.5, and 0.6, respectively. The
error terms are assumed to follow a normal distribution with ε1,t ∼N(0, 0.5) and
ε2,t ∼t(0, 1, 4) for Eq. (9). For the dependence parameter for the copula function,
we set the true value for the Gaussian and Student-t dependence coeﬃcient at
0.5 with degree of freedom 4, the true value for the Clayton, Gumbel, Joe and
Frank dependence coeﬃcient is set at 5. The general simulation scheme goes
as follows. We perform copula based SUR model using sample size n = 100 and
n = 200 for all cases (Table 1).
The estimation results from Table 2 show the estimated parameters and we
observed that our proposed model and method produced the unbiased parameter
estimates when compared with true value. We ﬁnd that the estimated parameters
are close to the true value and the standard error is very low. In addition,

542
R. Phadkantha et al.
Table 2. True parameter value of simulation study when n=200
Parameter Gaussian Student-t Clayton
Gumbel
Joe
Frank
β01
1.9993
2.0009
2.0058
2.0005
2.0014
1.9908
(0.0038)
(0.0025)
(0.0175) (0.0045) (0.0061) (0.0207)
β11
0.9758
1.0188
1.0766
0.9417
0.7936
0.0035
(0.0255)
(0.0245)
(0.0535) (0.0288) (0.0305) (0.0602)
σ1
0.3612
0.3663
0.3597
0.3603
0.3957
0.4379
(0.0128)
(0.0126)
(0.0144) (0.0126) (0.0134) (0.0186)
β02
0.4693
0.5154
0.6197
0.4491
0.3244
0.5352
(0.0031)
(0.0242)
(0.0494) (0.0287) (0.0300) (0.0595)
β21
−2.0003
−2.0009
−1.9987 −2.0011 −1.9994 −1.9952
(0.0036)
(0.0029)
(0.0169) (0.0054) (0.0057) (0.0206)
σ1
0.9553
0.9608
0.9509
0.9496
0.9775
0.9420
(.01331)
(0.0131)
(0.0155) (0.0130) (0.0137) (0.0213)
ν2
.5789
4.4447
3.9248
3.9548
3.9248
4.5158
(0.3090)
(0.4256)
(0.0102) (0.0125) (0.0671) (0.7555)
wy1
0.3002
0.3002
0.2942
0.3001
0.2979
0.2948
(0.0018)
(0.0014)
(0.0084) (0.0026) (0.0027) (0.0115
w11
0.4001
0.3994
0.4115
0.4019
0.4002
0.3943
(0.0023)
(0.0017)
(0.0118) (0.0034) (0.0040) (0.0147)
wy2
0.4983
0.4994
0.4941
0.4978
0.5023
0.5168
(0.0031)
(0.0023)
(0.0154) (0.0043) (0.0051) (0.0196)
w21
0.5992
0.5991
0.6058
0.6008
0.5995
0.6028
(0.0032)
(0.0023)
(0.0143) (0.0025) (0.0048) (0.0188)
θ
0.4994
0.5158
5.0045
5.0001
4.9588
5.01255
(0.1225)
(0.0010)
(0.5649) (0.3442) (0.3718) (0.7124)
Source: Calculation
Note: () is standard error.
when the sample size increases, a more accurate estimation is obtained which is
consistent with the √n-consistency of the coeﬃcient estimators. Therefore, we
can conclude that the proposed model has a good ﬁnite sample performance and
is reasonable for CAPM application.
5
Application Study to CAPM Model with Interval
REIT Returns
This section presents benchmark result. We analyze the returns and risks of
ASIAN REITs markets.

Analysis of Risk, Rate of Return and Dependency of REITs in ASIA
543
5.1
Data Description
This paper considers three REITs in Asia. We use daily interval returns of three
REITs in ASIA including Hong Kong, Japan and Singapore for the period of
April 1, 2009 to February 24, 2017. The data is collected from Yahoo Finance
and Thomson Reuters.
Global REITs (GL) is a free-ﬂoat adjusted, market capitalization-weighted
index designed to track the performance of listed real estate companies in both
developed and emerging countries worldwide. Constituents of the Index are
screened on liquidity, size and revenue.
Hong Kong REITs (HK) is real estate investment trusts listed in Hong Kong.
The Hang Seng REIT Index will serve as the basis for index products, including
funds and derivatives as well as a benchmark for investors in the REIT asset
class.
Japan REITs (JP) is a capitalization-weighted index of all Real Estate
Investment Trusts listed in the Tokyo Stock Exchange, and is calculated using
the same methodology as the TOPIX. The index was developed with a base
index value of 1000 as of March 31, 2003. This is a price return index.
Singapore REITs (SIN) is an index measures the performance of the 20
largest and most tradable trusts of the REIT sector listed on Singapore Exchange
(SGX). The index provider adjusts for each Reits free ﬂoat that is, the number
of shares publicly available to investors and then attaches weights for each REIT
constituent according to its free-ﬂoat-adjusted market capitalization.
In this application study, we can write our model speciﬁcations as follows:
⎡
⎣
wy1HKU
1,t + (1 −wy1)HKL
1,t
wy2JP U
2,t + (1 −wy2)JP L
2,t
wy3SIN U
3,t + (1 −wy3)SIN L
3,t
⎤
⎦=
⎡
⎣
αHK + βHK(w11GLU
t + (1 −w11)GLL
t ) + ε1,t
αJP + βHK(w21GLU
t + (1 −w21)GLL
t ) + ε2,t
αSIN + βHK(w31GLU
t + (1 −w31)GLL
t ) + ε3,t
⎤
⎦
Table 3. Summary statistics for interval valued data of three REITs in Asia.
HKU
HKL
JP U
JP L
SINU
SINL
GLU
GLL
Mean
0.007448
−0.013814 0.01038
−0.00973
−0.004678 0.004135
0.006977
−0.005848
Median
0.005889
−0.010833 0.00869
−0.008451 −0.003701 0.003261
0.004826
−0.004203
Maximum
0.072484
0.016273
0.16169
0.068755
0.035914
0.063108
0.955659
0.885077
Minimum
−0.054514 −0.120214 −0.072972 −0.118814 −0.10479
−0.043515 −0.033312 −0.999721
Std. Dev
0.008914
0.011502
0.012763
0.012143
0.007764
0.007694
0.019843
0.026766
Skewness
1.268982
−2.819685 1.7256
−1.272618 −2.415195 1.525084
38.07373
−5.068424
Kurtosis
12.16828
16.07497
16.57828
11.55111
24.835
11.76051
1812.039
1085.047
Jarque-Bera 10882.47
24381.55
23602.73
9571.87
60137.04
10347.51
3.94E+08
1.41E+08
Probability
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
Obs
2886
2886
2886
2886
2886
2886
2886
2886
ADF-test
4.4805***
4.9929***
5.2992***
5.4118***
3.4803***
6.9890***
5.6122***
5.1792***
Source: Calculation
Note: *,**,*** denote signiﬁcant at 90%, 95%, and 99%, respectively

544
R. Phadkantha et al.
The descriptive statistics are given in Table 3, which show that Japan REITs
has the highest standard deviation, while Singapore REITs has the lowest. The
Jarque-Bera calculated is compared to chi-square critical value at an indepen-
dent degree 2. If the Jarque-Bera calculated is greater than that the data is
not normally distributed. In this paper, the Jarque-Bera statistic indicates that
all series are not normally distributed as it rejects the null hypothesis. The
Augmented Dickey-Fuller (ADF) is applied to check unit roots in the series.
Unit root test for each variable has a statistical signiﬁcance level of 0.01. This
means that all of REITs returns are stationary in characteristics. Therefore,
these variables can be used to estimate the model in the next section.
5.2
Estimates Results
Table 3 shows AIC and BIC criteria for comparison of the copula families as well
as the marginal distributions. To choose the best family and marginal distribu-
tion for our data, we will look at the minimum AIC and BIC criteria. As we can
see, the minimum AIC and BIC are −62394.33 and −62286.91, respectively as
bolded. So we should choose Gaussian as the appropriate copula for data and
the proper distribution is student-t. We, thus, choose Gaussian as the copula
function to link marginal distributions of REITs in Asia as joint distribution
(Table 4).
Table 4. AIC and BIC criteria for model choice
Marginal
Copula
distribution
Gaussian
Student-t Frank
Joe
Clayton
Gumbel
Normal/Normal/
−53553.18 −55274.07 −55331.28 −51700.88 −55455.99 −52062.84
Normal
−53445.77 −55166.65 −55235.8 −51605.4 −55360.51 −51967.36
Normal/Normal/
−43425.57 −57402.96 −47056.75 −45980.15 −56691.39 −46446.54
Student-t
−43318.15 −57295.54 −46961.26 −45884.67 −56595.91 −46351.06
Normal/Student-t/ −49892.21 −55247.96 −48668.8 −47194.18 −56631.57 −47776.71
Normal
−49784.8
−55140.54 −48573.32 −47098.7 −56536.09 −47681.23
Normal/Student-t/ −59988.07 −60494.55 −60620.71 −58069.73 −59211.22 −60442.27
Student-t
−59880.65 −60387.14 −60525.23 −57974.25 −59115.74 −60346.79
Student-t/Normal/ −56464.46 −56519.5 −44745.65 −44883.61 −56527.03 −35962.46
Normal
−56357.05 −56412.08 −44650.17 −44788.13 −56431.55 −35866.98
Student-t/Normal/ −57285.1
−58783.24 −54931.43 −51626.6 −57211.87 −58100.58
Student-t
−57177.68 −58675.82 −54835.94 −51531.12 −57116.39 −58005.1
Student-t/Normal/ −55608.88 −58575.42 −59204.73 −58022.76 −56156.11 −57883.47
Normal
−55501.46 −58468
−59109.24 −57927.27 −56060.62 −57787.99
Student-t/
-62394.33 −61251.1 −61812.89 −62031.51 −62126.63 −61902.03
Student-t/
Student-t
−62286.91 −61143.68 −61717.41 −61936.03 −62031.15 −61806.55
Source: Calculation

Analysis of Risk, Rate of Return and Dependency of REITs in ASIA
545
Tables 5 and 6 show the results of parameter estimation through the appli-
cation of the CAPM model. Our main contribution is that we can estimate the
weight parameters in order to ﬁnd the appropriate value between intervals. These
parameters seem to have econometric interpretation. If any weight is greater or
lower than 0.5, it indicates that the market is likely to be in uptrend or down-
turn. Then, lets consider the estimated beta of each equation which indicates the
sensitivity of securities to systematic risk. If the beta is equal to 1, it means that
the individual market has changed in return equal to the global REITs return.
If the beta is greater than 1, it means that the individual market is subject to a
change in the Asian REITs return, which is higher than the global market risk.
On the other hand, if the beta is less than zero, it indicates that the individual
market has a change in the rate of return of the market where the risk is inversely
proportional to the global REITs return. The results show that Hong Kong and
Japan markets have a positive sign of the estimated beta, βHK = 0.30948 and
βJP = 0.17323, hence this indicates that Hong Kong and Japan markets are less
volatile than the global REITs. On the other hand, we ﬁnd that the estimated
beta for Singapore market shows a negative relationship with global REITs. Sur-
prisingly, our beta has a negative sign. Can beta be negative? The answer is yes.
The beta can be negative. A more intuitive way of thinking about this is that
Table 5. Estimated results
αHK
Coeﬃcient
S.E.
βHK
−0.00076*** 1.99E-04
σ1
0.30948***
1.54E-02
wy1
0.00565***
0.00010398
wy1
0.61995***
0.00847734
αJP
0.52438***
0.03678616
βJP
−0.00066**
3.42E-04
σ2
0.17323***
2.58E-02
wy2
−0.00915*** 1.66E-04
w21
0.45527***
1.41E-02
αSP
0.50092***
1.05E-01
βSP
−0.00040**
1.75E-04
βSP
−0.49251*** 1.50E-02
σ3
−0.00510*** 8.62E-05
wy3
0.55359***
2.03E-02
w31
0.52375***
2.34E-02
Source: Calculation
Note: *,**,*** denote rejection of
the null hypothesis at the 10%, 5%
and 1% signiﬁcance levels, respec-
tively.

546
R. Phadkantha et al.
Table 6. Copula Parameter
Dependence parameter from SUR Copula
Hong Kong Japan
Singapore
Hong Kong 1.0000
0.1737***
−0.1079***
Japan
0.1737***
1.0000
−0.2445***
Singapore
−0.1079*** -0.2445*** 1.0000
Source: Calculation
Note: *,**,*** denote rejection of the null hypothesis at the
10%, 5% and 1%, signiﬁcance levels, respectively.
a negative beta investment represents insurance against some other risks that
aﬀect portfolio return. We can say that Singapore market can be viewed as a
hedge against higher risk in Asian REITs.
Moreover, lets consider the copula dependence parameters in Table 6. The
result depicts the dependency of each country on the basis of the dependence
value. We ﬁnd the signiﬁcant positive dependence between Hong Kong and Japan
REITs which is 0.1737 While Singapore and Hong Kong show a signiﬁcant
negative dependence which is −0.1079. Finally, the dependence coeﬃcient of
the Japan and Singapore REITs is −0.2445. These results conﬁrm that Japan,
Hong Kong, and Singapore RETIs are correlated to each other and our model
is reasonable for use in this line of application
6
Conclusion
The high volatility of the REIT price indexes due to the hamburger crisis caused
the investors to face huge risks, especially in Japan, Hong Kong and Singapore.
These three countries have a high market capitalization in Asian, and the move-
ments of their price indexes contribute a large eﬀect to the global REITs market.
Thus, in this study, it would be of great beneﬁt to measure the risk of these three
markets. To quantify the risk, we consider the CAPM approach. Moreover, we
also expect a correlation between CAPM of each country. Thus, in this study,
we employ a copula based SUR models as proposed in Pastpipatkul et al. [7] to
quantify the risk of Japan, Hong Kong and Singapore markets. However, there
has been other recent extension of the CAPM to the interval data, as seen in
Piamsuwannakit et al. [11]. They found that using the interval data in CAPM
can provide a better result than using the closing price for prediction. Thus,
we propose copula based SUR model for analysis with interval data. To ﬁnd
the appropriate point data in the interval data we employ convex combination
method which was introduced in Chanaim et al. (2016) [2].
Before we apply our model to the real data, we conduct a simulation study
to conﬁrm the accuracy of our model. The results conﬁrm that our model is
accurate and has a good ﬁnite sample performance and is reasonable for CAPM
application. In the application study, we choose the best model speciﬁcation

Analysis of Risk, Rate of Return and Dependency of REITs in ASIA
547
using AIC and BIC. Thus Gaussian copula is chosen to be a linkage between
Student-t of the Hong Kong, Japan, and Singapore CAPM equations.
Finally, the results of the best ﬁt model speciﬁcation show that Hong Kong
and Japan markets have a positive sign of the beta and both markets are less
volatile than the global REITs market. On the other hand, we ﬁnd that the
estimated beta for Singapore market shows a negative relationship with global
REITs market. We conclude that Singapore market can be viewed as a hedge
against higher risk in Asian REITs.
References
1. Billard, L., Diday, E.: Regression analysis for interval-valued data. In: Data Analy-
sis, Classiﬁcation, and Related Methods, pp. 369–374. Springer, Heidelberg (2000)
2. Chanaim, S., Sriboonchitta, S., Rungruang, C.: A convex combination method
for linear regression with interval data. In: Proceedings of the 5th International
Symposium on Integrated Uncertainty in Knowledge Modelling and Decision Mak-
ing, IUKM 2016, Da Nang, Vietnam, 30 November– 2 December 2016, vol. 5, pp.
469–480. Springer (2016)
3. Fang, H., Chang, T.Y., Lee, Y.H., Chen, W.J.: The impact of macroeconomic
factors on the real estate investment trust index return on Japan, Singapore and
China. Investment Manage. Financ. Innov. 13(4-1) (2016)
4. Joe, H.: Asymptotic eﬃciency of the two-stage estimation method for copula-based
models. J. Multivar. Anal. 94(2), 401–419 (2005)
5. Lintner, J.: The valuation of risky assets and the selection of risky investments in
stock portfolios and capital budgets. Rev. Econ. Stat. 47(1), 13–37 (1965)
6. Newell, G.: The investment characteristics and beneﬁts of Asian REITs for retail
investors. Asia Paciﬁc Real Estate Association (APREA) (2012)
7. Pastpipatkul, P., Maneejuk, P., Wiboonpongse, A., Sriboonchitta, S.: Seemingly
unrelated regression based copula: an application on Thai rice market. In: Causal
Inference in Econometrics, pp. 437–450. Springer (2016a)
8. Pastpipatkul, P., Panthamit, N., Yamaka, W., Sriboochitta, S.: A copula-based
markov switching seemingly unrelated regression approach for analysis the demand
and supply on sugar market. In: Proceedings of the 5th International Symposium
on Integrated Uncertainty in Knowledge Modelling and Decision Making, IUKM
2016, Da Nang, Vietnam, 30 November–2 December 2016, vol. 5, pp. 481–492.
Springer (2016b)
9. Pham, A.K.: An empirical analysis of real estate investment trusts in Asia: Struc-
ture, performance and strategic investment implications (2013)
10. Phochanachan, P., Pastpipatkul, P., Yamaka, W., Sriboonchitta, S.: Threshold
regression for modeling symbolic interval data. Int. J. Appl. Bus. Econ. Res. 15(7),
195–207 (2017)
11. Piamsuwannakit, S., Autchariyapanitkul, K., Sriboonchitta, S., Ouncharoen, R.:
Capital asset pricing model with interval data. In: Integrated Uncertainty in
Knowledge Modelling and Decision Making, pp. 163–170. Springer (2015)
12. Sharpe, W.F.: The Capital Asset Pricing Model: A “Multi-Beta” Interpretation.
Stanford University, Graduate School of Business (1973)
13. Sklar, A.: Fonctions de de r´epartition n dimensions et leurs marges. Publications
de l’Institut de Statistique de l’Universit de Paris (1959)

548
R. Phadkantha et al.
14. Smith, M.S., Gan, Q., Kohn, R.J.: Modelling dependence using skew t copulas:
Bayesian inference and applications. J. Appl. Econometrics 27(3), 500–522 (2012)
15. Smith, M.S., Khaled, M.A.: Estimation of copula models with discrete margins
via Bayesian data augmentation. J. Am. Stat. Assoc. 107(497), 290–303 (2012)
16. Wichitaksorn, N.: Estimation of bivariate copula-based seemingly unrelated Tobit
models (2012)
17. Zellner, A.: An eﬃcient method of estimating seemingly unrelated regressions and
tests for aggregation bias. J. Am. Stat. Assoc. 57(298), 348–368 (1962)

Risk Valuation of Precious Metal Returns
by Histogram Valued Time Series
Pichayakone Rakpho1, Woraphon Yamaka1,2, and Roengchai Tansuchat1,2(B)
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
pichayakone@gmail.com, woraphon.econ@gmail.com, roengchaitan@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. The price of precious metals is highly volatile and it can bring
both risk and fortune to traders and investors, and therefore should be
examined. In this paper, we introduce an approach to ﬁtting a Copula-
GARCH to valued time series and apply this methodology to the daily
histogram returns of precious metals consisting of gold, silver, and plat-
inum. The study also conducts a simulation study to conﬁrm the accu-
racy of the model and the result shows that our model performs well.
In the empirical study, our results suggest investing on gold and plat-
inum in high proportion while silver is not recommended for inclusion in
the precious metal portfolio. Moreover, precious metal portfolio of the
intraday 30-min returns gives lower risk when compared with portfolio of
the intraday 60-min returns. Therefore, investors should not hold assets
for long period of time because the long-term holding is likely to face a
higher risk.
1
Introduction
Even though precious metals are recommended asset to hedge or to diversify
in the portfolio in many poor situations such as a stock market crash, inﬂation
or a declining dollar, precious metal price still has volatility under uncertainty
situation particularly as occurred during the ﬁnancial crisis of 2007–2009. The
three precious metal prices from 2007 to 2017 are shown in Fig. 1. During this
period with stock market collapse, credit crunch, and dollar’s devaluation, the
holding of stocks became a high-risk investment and thus investors preferred
to hold precious metals as a safe haven in this troubled situation. However,
after the economic crisis, investors are more interested in investing in precious
metals. With more purchases, metal prices have continuously risen. In addition,
precious metal price still had uncertainty situation in 2014. The main reason
for this weakness in the second half of 2014 was US dollar appreciation [10]. As
evident in Fig. 1, prices of all precious metals are volatile and move together.
As mentioned above, the prices of precious metals are highly volatile and this
bring both risk and fortune to traders and investors, and therefore should be
examined. The Value at Risk (VaR) estimation is a tool to measure risk. It is
a ﬁnancial market instrument for measurement and evaluation of the portfolio
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_39

550
P. Rakpho et al.
Fig. 1. Precious metal prices during 2007–2017 Source: Thomson Reuters, 2017
market risk associated with ﬁnancial asset and commodity price movements. It
represents in the form of the expected worst loss of a portfolio over a given time
horizon at a given conﬁdence level.
In the literature, VaR has been applied in metal markets to measure risk in
several studies. Hammoudeh et al. [5] examined the volatility and correlation
dynamics in price returns of gold, silver, platinum and palladium using copula
model and VaR. The results are useful for participants in the global ﬁnancial
markets that needed to invest in precious metals in the light of high volatility.
Demiralay and Ulusoy [2] predicted the Value at Risk of four major precious
metals (gold, silver, platinum, and palladium) using FIGARCH, FIAPARCH
and HYGARCH or long memory volatility models, under normal and student-t
innovations distributions. The results showed that these models perform well
in forecasting a one-day-ahead VaR and have potential implications for port-
folio managers, producers, and policy makers. Khemawanit and Tansuchat [12]
measured VaR by applying GARCH-EVT-Copula model and found that gold
and silver should invest in a high investment proportion, whereas palladium and
platinum have little investment proportion. However, most empirical researches
about value at risk in precious metal returns which lower frequencies such as
daily closing price weekly or monthly and used generally the GARCH models.
In the analysis of the risks by the closing price still does not reﬂect the true
volatility of precious metals. Due to the close prices is ignoring information that
happened during the day. Therefore, In this paper purposes analysis of value
at risk for precious metal returns using high frequency data. By using the full
information contained in the histogram, we ﬁnd that there are advantages in the
estimation and prediction of a speciﬁc interval [4].
In ﬁnancial markets, the price of an asset (stocks, bonds, exchange rates,
etc.) during intraday trading is observed as high frequency data, i.e., tick by tick.
However in a huge number of studies, many researchers prefer to analyze their
works based on daily closing price or even at lower frequencies such as weekly or
monthly. It may be claimed that tick-by-tick data or price will generate a huge

Risk Valuation of Precious Metal Returns by Histogram Valued Time Series
551
amount of data from which it will be diﬃcult to discriminate information from
noise. But if we consider only the closing prices, we might miss valuable intraday
information. Thus, in this study, we propose an alternative way to collect these
information by constructing daily histogram-valued time series (HTS) from the
intraday data [4]. The histograms, when indexed by time, will create a HTS.
From a time series perspective, it is possible to deﬁne the HTS as a collection of
histograms ordered over time.
In this context, an important issue is how volatility and dependence of pre-
cious metals can be measured, when the data are HTS. Here we consider Copula-
GARCH model which proposed by Jondeau and Rockinger [10] as a tool to
measure the risk and evaluate the dependence of precious metals. Hence, the
study aim to ﬁt the Copula-GARCH model to HTS data set. To deal with the
HTS data set, we employ the method for histogram-valued variables which pro-
posed by the study of Irpino and Verde [7]. They also presented an alternative
deﬁnition of mean for histogram-valued variables, which produces a mean distri-
bution, that they termed it barycentric histogram of the histograms. To the best
of our knowledge, Copula-GARCH models for HTS has not been considered yet.
Therefore, this fact becomes one of our motivations to work on this paper.
The remaining of the paper is organized as follows. Section 2 is the review
of the methodology. Section 3 is estimation procedure. Section 4 shows the sim-
ulation study. Section 5 is on data description, and unit root tests. Section 6
describes the empirical results of estimates about the risk, VaR and ES, optimal
portfolio, and optimal portfolio weights. Finally, Sect. 7 provides some conclud-
ing remarks.
2
Methodology
The Copula-GARCH model employed in this study was previously proposed by
Patton [15], Rockinger and Jondeau [11] and it generally consists of two parts:
the ﬁrst part is to model the marginal distribution of returns by GARCH process
and the second is a proper copula function to link the margins together. In this
section, we brieﬂy introduce Copula-GARCH with histogram valued data.
2.1
Histogram-Valued Variables
As we mention above, we consider a high frequency data, i.e., tick by tick.
Although, tick-by-tick data can generate a huge amount of information, it will be
diﬃcult to deal with this huge data. Thus, we can transform this data into his-
togram data and ﬁnd the appropreiate value between in the range of histogram.
According to Dias and Brito [3], we can deﬁne histogram-valued variables as
follows:
Deﬁnition 2.1. Yt = {y(1), ..., y(T)} is a histogram-valued variable at time
t when to each time t corresponds an empirical distribution y(t) that can be

552
P. Rakpho et al.
represented by a histogram
Hy(1) =

[Iy(t)1, Iy(t)1], pt1; [Iy(t)2, Iy(t)2], pt2; ...[Iy(t)nt, Iy(t)nt], ptnt

(1)
where Iy(t)i and Iy(t)i represent the lower and upper bound of the y(t) at each
unit i ∈{1, 2, ..., nj} which are uniformly distributed. pti is the frequency asso-
ciated with the subinterval [Iy(t)i, Iy(t)i] and nj
i=1 pij = 1, nt is the number of
subintervals for the empirical distribution y(t), t = 1, ..., T.
To ﬁnd the single value data in each y(t), the quantile function, Ψ −1
Yt , which
proposed in Irpino and Verde [7], is employed
Ψ −1
Yt =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Iy(j)1 +
t
pt1 ay(t)1
if 0 ≤P < pj1,
Iy(j)2 + t −pp1
pt2
ay(t)2
if pt1 ≤P < (pj1 + pj1),
...
Iy(j)nt + t −(pj2 + pt1 + · · · + ptnt
ptm
ay(t)nt
if (pt2 + pt1 + · · · + ptnt) ≤P < 1
(2)
where ay(t)i = [Iy(t)i −Iy(t)i] with {i ∈1, . . . nt}.
Note that when we work with histogram-valued variables, the frequency asso-
ciated with the subinterval pit and the number of subintervals in the histogram
nt may be diﬀerent; the subinterval of histogram Hy(t) are considered ordered
and disjoint, and if this is not the case, it must be possible to rewrite them in
the required form Dias and Brito [3].
2.2
GARCH Model
Following Bollerslev [1], a GARCH model can be described by the following
equations
yit = ci + hitZit = ci + εit,
(3)
ηit = Zit/hit,
(4)
h2
it = α0 +
M

m=1
αmε2
it−m +
N

n=1
βnh2
it−n
(5)
where Eqs. 3 and 5 are, respectively, the conditional mean and variance equation,
given past information. ci is the intercept term of the mean equation, εit is the
residual term of asset i at time t, hit is the conditional variance and ηit is a

Risk Valuation of Precious Metal Returns by Histogram Valued Time Series
553
sequence of i.i.d which is assumed to have a normal distribution, a student-
t distribution, and a skewed-t distribution. αm(m = 1, . . . , M) and βn(n =
1, . . . , N) are non negative parameters and αm + βn ⩽1 to assure that hit > 0
and covariance stationarity. As already mentioned, from Eq. 2 in this work we
choose to represent the distributions by quantile functions. However, when we
multiply a quantile function by a negative number we do not obtain a non-
decreasing function. Therefore, it is necessary to impose positivity restrictions
on the parameters of the model. Denoting Ψ −1
yit the quantile function of the
predicted distribution yit; we rewrite the mean equation of GARCH regression
model as follows:
Ψ −1
yit = ci + εit,
(6)
2.3
Copula-GARCH Model
First of all, we brieﬂy explain the deﬁnition of the copula which was introduced
in Sklars theorem
Deﬁnition 2.3. A n-dimensional copula is a multivariate cumulative distribu-
tion function c : [0, 1]d →[0, 1], whose margins have the uniform distribution
on the interval [0, 1]. Let F denote a d-dimensional distribution functions with
marginal distribution function Fy1, . . . , Fyd then, there exists a copula C, such
that
F(y1, . . . , yd) = C(Fy1(y1), . . . , Fyd(yd)), ∀(y1, . . . , yd) ∈Rd
(7)
where, the function F is the joint distribution function with marginal distri-
bution functions Fy1, . . . , Fyd which are the marginal distributions speciﬁed
as standard univariate GARCH processes. A copula is a cumulative distribu-
tion function with uniform marginal distributions. ui, C(ui, . . . , ud) = Pr(U1 ⩽
u1), . . . , Pr(Ud ⩽ud), where ui = Fyi(yi) is uniform [0, 1] interval.
In this study, we consider multivariate dimensional copulas with GARCH(1,
1) which is mostly employ in many ﬁnancial data. We consider two families of
copulas used in our paper, namely: the normal copula and the Student-t-copula,
(see, Joe [9] and Hofert, Mchler, Mcneil [6]) to join the GARCH equations.
3
Estimation Procedure
To estimate the Copula-GARCH model with histogram value data, we begin to
derive the log-likelihood for the model as follows:
log l(Θ, θ
Ψ −1
yit ) =
T

t=1
log

	
cθ(Fy1(Ψ −1
y1t
Θ1), ..., Fyd(Ψ −1
ydt |Θd))

d

i=1
fi(Ψ −1
yit |Θi )

(8)

554
P. Rakpho et al.
where, Θ is the estimated parameter in GARCH equations and θ is estimated
copula parameters. Note that the copula connects margins to a multivariate dis-
tribution function without any constrains on marginal distributions. We model
the marginal distribution of the standardized innovations ηit, i = 1, . . . , d, by
cumulative distribution function (cdf). Then, we ﬁt the six chosen families of the
copulas to the vector of various marginal distributions namely: normal, student-
t, and skewed student-t distributions. For each Copula-GARCH speciﬁcation, we
estimate the parameters by maximizing the log- likelihood function in Eq. (8).
To select the best ﬁt model, that with the lowest Akaike Information criteria
and Bayesian Information criteria is preferred (see, pastpipatkul, [13,14]).
In addition, our study aims to measure the Value at Risk (V aR) of the
portfolios of our histogram returns. In the next step, we generate 10,000 Monte
Carlo histogram return of asset i, Sit from the best ﬁt Copula-GARCH(1, 1).
Then, the value of a portfolio at t is given by
Vt =
d

i=1
ωiSit,
where ωi is set to be equally weighted. We deﬁne the proﬁt and loss function
from period t = T to t = T + 1 as LT +1 = VT +1 −VT . Thus, V aRT +1(α) is the
V aR at time T + 1, of a speciﬁc portfolio of assets over a time horizon from T
to T + 1 with the conﬁdent level 1 −α, satisﬁes:
P(LT +1 > V aRT +1(α)) = 1 −α
Thus,
P(LT +1 ⩽V aRT +1(α)) = α
and it is easily seen that V aRT +1(α) is the α-th quantile from the distribution
of LT +1, i.e.
V aRT +1(α) = F −1
LT +1(α).
where FLT +1stands for the distribution function of LT +1.
In this computation, we compute all the risk measures at the 1%, 5%, and
10% levels. Additionally, We ﬁnally obtain expected return and risk to ﬁnd the
optimal weights of the portfolios.
4
Applied Examples
4.1
Simulation Example
In the simulation example, we applied Gaussian copulas to model the dependence
structure of the GARCH(1, 1). The simulation is the realization of GARCH(1,
1) with three equations, thus we generated random histogram data from the
following speciﬁcations:
Yit = ΨYit
(9)

Risk Valuation of Precious Metal Returns by Histogram Valued Time Series
555
Ψ −1
Yit = ci + hitεit,
(10)
h2
it = αi0 + αi1ε2
it−1 + βi1h2
it−1, i = 1, . . . , 3
(11)
The error terms, εit, are assumed to follow εit ∼N(0, 0.5) and εit ∼
t(0, 1, v = 4) and ε3t ∼std(0, 1, v = 4, skew = 1.5). The parameters of the
model C, α0, α1 and β1 are set to be C = 0.2, 0.1, 0.3, α0 = 0.001, 0.001, 0.001,
α1 = 0.2, 0.1, 0.3 and β1 = 0.7, 0.8, 0.6. For the dependence parameter for the
copula function, we set the true value for the Gaussian dependence coeﬃcient
at 0.5. We run Copula-GARCH model with histogram valued data using sample
size n = 100 and we then convert the single value data to histogram data by
Fig. 2. Box plot of simulated histogram valued data Source: Calculations
Table 1. Simulation example results
Parameter i = 1
i = 2
i = 3
c1
0.1813 (0.0003) 0.1007 (0.0023) 0.2923 (0.0021)
αi0
0.0004 (0.0001) 0.0004 (0.0020) 0.0004 (0.0015)
αi1
0.2079 (0.0021) 0.0991 (0.0012) 0.2991 (0.0010)
βi1
0.7929 (0.0001) 0.7929 (0.0561) 0.6929 (0.0497)
ν1
3.5784 (0.0158) 3.7445 (0.1245)
skewi
1.1154 (0.0024)
θ1
1
0.4868 (0.1471) 0.4868 (0.1122)
θ2
0.4868 (0.1471) 1
0.5043 (0.1312)
θ3
0.4868 (0.1122) 0.5043 (0.1312) 1
Source: Calculation
Note: () is standard errors.

556
P. Rakpho et al.
using a piecewise linear approximation of the empirical cumulative distribution
function using the Ramer-Douglas-Peucker algorithm (see, Irpino [7,8]). We plot
each histogram data for i equation in Fig. 2.
Table 1 shows the estimated parameters and we observed the model produced
the unbiased parameter estimates when compared with the true values. We found
that the estimated parameters are close to the true values and the standard error
is reasonable.
5
Data
5.1
Descriptive Statistics
In the ﬁrst step, we construct a daily histogram with the 30-min and 60-min form
gold, silver, platinum returns during the period January 1, 2015 to December 8,
Table 2. Summary statistics for intraday precious metal
Intraday 30-min returns
Gold
Sliver
Platinum
Mean
–0.00000432
–0.00000695
–0.0000116
Std. dev
0.000364
0.000592
0.001047
Skewed
–1.062584
–3.323920
–135.3521
Kurtosis
289.0925
229.4621
22506.04
Min
–0.013834
–0.026695
–0.174192
Max
0.013763
0.014936
0.013285
Jarque-Bera 1.16E+08
72680126
7.17E+11
Probability
0.000000
0.000000
0.000000
ADF-test
(–183.13)*** (–126.23)*** (–183.52)***
Intraday 60-min returns
Gold
Sliver
Platinum
Mean
–0.00000662
–0.0000097
0.0000187
Std. dev
0.000365
0.000658
0.001421
Skewed
–0.700144
–4.807569
–108.3149
Kurtosis
153.2437
239.3651
13277.04
Min
–0.010918
–0.026695
–0.174192
Max
0.011210
0.014066
0.009315
Jarque-Bera 15982254
39618004
1.25E+11
Probability
0.000000
0.000000
0.000000
ADF-test
(–127.13)*** (–126.68)*** (–130.93)***
Source: Calculation
Note: ***, **, * are signiﬁcant at 1%, 5%, and 10% level,
respectively.

Risk Valuation of Precious Metal Returns by Histogram Valued Time Series
557
2016. All precious metals price information is derived from Bloomberg and pre-
cious metals are traded at London Metal Exchange. we use the precious metal
prices (secondary data) to calculate the natural log returns which are deﬁned as
ri,t = ln (Pi,t/Pi,t−1) where Pi,t is the ith metal price at time t, ri,t is the ith
log return of metal price at time t, i and indicated the ith precious metal price.
The descriptive statistics shows in Table 2. In log return of metal price every 30
and 60 min found that platinum has standard deviation higher than gold and
silver. In additional, Table 2 shows that standard deviation of 60 min is higher
than 30 min. Since the movement of stock prices need a time to be change. Thus,
if we consider high frequency data i.e. 30 min, its prices might not change much
when compare to low frequency data 60 min. Investors need time to make a buy
or sell decided so the low frequency data should be less volatile than high fre-
quency data. The Jarque-Bera statistic indicates that all series are not normally
distributed because rejects the null hypothesis, thus the return series of precious
metal price is non-normal distribution. The Augmented Dickey-Fuller (ADF) is
applied to check unit roots in the series. Unit root test for each variable have a
statistical signiﬁcance level of 0.01. This means that all of precious metal returns
is stationary characteristics. Therefore, these variables can be used to estimate
Copula-GARCH model in the next step.
6
Result
Table 3 shows AIC and BIC for comparison of the copula families as well as the
marginal distributions, when a daily histogram data is constructed from 30 and
60-min returns, respectively. To choose the best family and marginal distribution
for our data, we will look at the minimum AIC and BIC. The results of these
two criteria are the same. We can observe that student-t copula is chosen to
be a linkage between skewed Student-t margins of gold, silver, and platinum
equations. Thus, we will interpret our results from this model speciﬁcation as
presented in Table 4.
Figure 3 shows the results for estimated VaR and CVaR (ES). The calcu-
lated VaR and CVaR (ES) of the portfolio with an equally weighted portfolio
of three precious metals (Gold, Silver and Platinum,) at level of 1%–10% under
the equally weighted assumption of intraday 30-min returns and intraday 60-min
returns. In period t + 1, the estimated CVaR (ES) is higher than VaR and the
risk of intraday 60-min returns is more than intraday 30-min returns.
Figure 4 shows the results of the eﬃcient frontiers of the portfolio under
diﬀerent expected returns, which come from the optimized portfolio based on
mean-CVaR (ES) model. To get these results, we applied the Monte Carlo simu-
lation to simulate a set of 10,000 samples and to estimate the expected shortfall
of an optimal weighted portfolio.
For the discussion above, we focused on estimating the VaR and CVaR (ES) of
an equally weighted portfolio. In Fig. 4, each plot here illustrates each portfolio.

558
P. Rakpho et al.
Table 3. AIC and BIC criteria for model choice
Marginal
30 min
60 min
distribution Copula
Copula
Student-t
Normal
Student-t
Normal
BIC
AIC
BIC
AIC
BIC
AIC
BIC
AIC
N-N-N
–14369
–14440
–14296 –14366 –11671
–11742
–11647 –11717
N-N-T
–14445
–14515
–14387 –14458 –11699
–11769
–11678 –11748
N-N-sT
–14453
–14523
–14396 –14467 –11701
–11771
–11679 –11749
N-T-N
–14488
–14559
–14487 –14557 –11725
–11795
–11718 –11788
N-T-T
–14622
–14693
–14621 –14692 –11761
–11832
–11755 –11825
N-T-sT
–14633
–14703
–14632 –14702 –11764
–11834
–11757 –11827
N-sT-N
–14585
–14655
–14564 –14634 –11869
–11939
–11861 –11931
N-sT-T
–14680
–14750
–14664 –14734 –11902
–11972
–11895 –11965
N-sT-sT
–14690
–14760
–14674 –14744 –11906
–11977
–11898 –11968
T-N-N
–14430
–14500
–14357 –14428 –11773
–11844
–11747 –11818
T-N-T
–14515
–14586
–14456 –14527 –11805
–11876
–11779 –11850
T-N-sT
–14523
–14594
–14465 –14535 –11807
–11878
–11781 –11851
T-T-N
–14562
–14632
–14556 –14627 –11843
–11914
–11826 –11897
T-T-T
–14702
–14773
–14698 –14769 –11885
–11955
–11865 –11935
T-T-sT
–14713
–14783
–14708 –14779 –11887
–11958
–11866 –11937
T-sT-N
–14663
–14733
–14631 –14701 –11974
–12044
–11962 –12033
T-sT-T
–14768
–14839
–14738 –14808 –12013
–12083
–11997 –12067
T-sT-sT
–14778
–14848
–14748 –14818 –12018
–12088
–12000 –12071
sT-N-N
–14431
–14501
–14358 –14429 –11773
–11844
–11747 –11818
sT-N-T
–14516
–14587
–14457 –14527 –11805
–11876
–11779 –11850
sT-N-sT
–14524
–14595
–14466 –14536 –11807
–11878
–11781 –11851
sT-T-N
–14563
–14633
–14557 –14627 –11843
–11914
–11826 –11897
sT-T-T
–14703
–14773
–14699 –14769 –11885
–11956
–11865 –11935
sT-T-sT
–14713
–14784
–14709 –14780 –11887
–11958
–11866 –11937
sT-sT-N
–14664
–14734
–14632 –14702 –11974
–12044
–11962 –12033
sT-sT-T
–14769
–14839
–14739 –14809 –12013
–12083
–11997 –12067
sT-sT-sT
−14779 −14849 –14749 –14819 −12018 −12088 –12000 –12071
The ﬁrst plot shows that low risk provides low return. It means that the greater
return, the greater risky it will be. For the suggestion, for risk-averse investors,
they would better choose the low return with low risk while risk-lover investors
suit for high risk, high return.

Risk Valuation of Precious Metal Returns by Histogram Valued Time Series
559
Table 4. Estimation result of Copula-GARCH(1, 1)models
Intraday 30-min returns
Variable
Gold
Silver
Platinum
C
–0.00099***
(–6.119)
–0.00122***
(–4.268)
–0.00220***
(–1333.421)
α0
0.00000
(0.872)
0.00006
(0.884)
0.00000
(0.006)
α1
0.02460***
(1.999)
0.35870
(0.820)
1.00000***
(6.978)
β
0.96830***
(70.398)
0.45040*
(2.292)
0.52080***
(17.973)
skew
0.97850***
(20.002)
0.97050***
(21.643)
0.91460***
(27.293)
shape
3.69900***
(5.916)
2.22200***
(8.309)
3.00700***
(13.493)
log-likelihood
2501.76
2237.488
2397.461
θ1
1
0.572
0.5457
θ2
0.572
1
0.6144
θ3
0.5457
0.6144
1
Intraday 60-min returns
Variable
Gold
Silver
Platinum
C
–0.00115***
(–3.49)
–0.00430***
(–979.301)
–0.00167***
(–3.467)
α0
0.00000
(0.971)
0.00000
(0.005)
0.00001
(1.671)
α1
0.01029
(0.905)
1.00000***
(6.668)
0.05274*
(2.349)
β
0.96080***
(19.629)
0.55560***
(16.453)
0.92980***
(16.746)
skew
0.99860***
(19.629)
0.86380***
(30.157)
0.92980***
(16.746)
shape
3.63800***
(6.376)
2.66900***
(20.191)
4.52000***
(4.785)
log-likelihood
2087.514
1853.21
1876.643
θ1
1
0.5042
0.5246
θ2
0.5042
1
0.5112
θ3
0.5246
0.5112
1
Source: Calculation
Note: ***, **, * are signiﬁcant at 1%, 5%, and 10% level,
respectively.
Table 5 shows the optimal investment proportion of precious metal portfolio
at the eﬃcient line. This result illustrates that most of the investment propor-
tion is gold, followed by silver and platinum. We also observe that the risk of
precious metal portfolio of the intraday 60-min returns is larger than portfolio

560
P. Rakpho et al.
Fig. 3. Value at risk & expected shortfall at level of 1%–10% Source: Calculations
Fig. 4. The eﬃcient frontiers of CVaR under mean Source: Calculations
of the intraday 30-min returns. This indicates that diﬀerent frequency will give
a diﬀerent risk value. Therefore, we should consider the frequency of the data
before measuring the risk of the portfolio. Then, lets consider the Sharpe ratio
which is the average return earned in excess of the risk-free rate per unit of
volatility or total risk [16]. Generally, the greater the value of the Sharpe ratio,
the more attractive the risk-adjusted return. We ﬁnd that the Sharpe ratio of
precious metal portfolio, when portfolio of the intraday 30-min returns, is larger
than the portfolio, when portfolio of the intraday 60-min returns.

Risk Valuation of Precious Metal Returns by Histogram Valued Time Series
561
Table 5. Optimal investment proportion of precious metal portfolio
Portfolio of the intraday 30-min returns
Portfolio Gold
Silver
Platinum Return
Risk
Sharpe
1
0.528679 0.000000 0.471321
0.001461% 0.379441% 0.003849
2
0.527224 0.000000 0.472776
0.001463% 0.379442% 0.003859
3
0.525769 0.000000 0.474232
0.001468% 0.379443% 0.003868
4
0.524313 0.000000 0.475687
0.001473% 0.379444% 0.003878
5
0.522858 0.000000 0.477142
0.001475% 0.379445% 0.003888
6
0.521403 0.000000 0.478597
0.001479% 0.379455% 0.003898
7
0.519948 0.000000 0.480052
0.001485% 0.379459% 0.003908
8
0.518492 0.000000 0.481508
0.001488% 0.379465% 0.003917
9
0.517037 0.000000 0.482963
0.00149%
0.379475% 0.003927
10
0.515582 0.000000 0.484418
0.0015%
0.3795%
0.003937
Portfolio of the intraday 60-min returns
Portfolio Gold
Silver
Platinum Return
Risk
Sharpe
1
0.811356 0.000000 0.188644
0.00057%
0.805855% 0.000699
2
0.810340 0.000000 0.189660
0.000574% 0.805857% 0.000712
3
0.809323 0.000000 0.190677
0.000581% 0.805859% 0.000725
4
0.808307 0.000000 0.191693
0.000595% 0.80586%
0.000739
5
0.807290 0.000000 0.192710
0.000611% 0.805872% 0.000752
6
0.806274 0.000000 0.193726
0.000624% 0.805876% 0.000766
7
0.805257 0.000000 0.194743
0.000627% 0.805884% 0.000779
8
0.804241 0.000000 0.195759
0.000645% 0.805886% 0.000793
9
0.803224 0.000000 0.196776
0.00065%
0.805888% 0.000806
10
0.802208 0.000000 0.197793
0.0007%
0.8059%
0.000819
Source: Calculation
Note: ***, **, * are signiﬁcant at 1%, 5%, and 10% level, respectively.
7
Conclusion
In this paper, we focus on the risk that occurs from investing in precious metals
traded at London Metal Exchange. We apply Copula-GARCH to ﬁt daily his-
togram valued time series constructed from intraday 30-min returns and intraday
60-min returns of the precious metals (gold, silver, platinum). Then, we calcu-
late the risk of our portfolios by applying Value at Risk (VaR) and Expected
Shortfall (ES) concepts. The empirical results showed that student-t copula is
the most appropriate copula function to link the GARCH(1, 1) with skewed
student-t distribution of gold, silver, and platinum. The estimated VaR and ES
are calculated based on 1%–10% levels. Finally, we obtain expected return and
risk to ﬁnd the optimal weights of the portfolios and the results suggest that
gold and platinum should have a high investment proportion while silver should

562
P. Rakpho et al.
have a low proportion in the optimal portfolio. When, we compare the portfolios
which were constructed from diﬀerent frequencies. We ﬁnd that a precious metal
portfolio of the intraday 30-min returns gives lower risk when compared portfolio
of the intraday 60-min returns. Therefore, investors should not hold assets for
long period of time because it will lead to the higher risk.
References
1. Bollerslev,
T.:
Generalized
autoregressive
conditional
heteroskedasticity.
J.
Econom. 31(3), 307–327 (1986)
2. Demiralay, S., Ulusoy, V.: Non-linear volatility dynamics and risk management of
precious metals. N. Am. J. Econ. Financ. 30, 183–202 (2014)
3. Dias, S., Brito, P.: Distribution and symmetric distribution regression model for
histogram-valued variables. arXiv preprint arXiv:1303.6199 (2013)
4. Gonzlez-Rivera, G., Arroyo, J.: Autocorrelation function of the daily histogram
time series of SP500 intradaily returns (2009)
5. Hammoudeh, S., Malik, F., McAleer, M.: Risk management of precious metals. Q.
Rev. Econ. Financ. 51(4), 435–441 (2011)
6. Hofert, M., Mchler, M., Mcneil, A.J.: Likelihood inference for Archimedean copulas
in high dimensions under known margins. J. Multivar. Anal. 110, 133–150 (2012)
7. Irpino, A., Verde, R.: A new Wasserstein based distance for the hierarchical cluster-
ing of histogram symbolic data. In: Data Science and Classiﬁcation, pp. 185–192.
Springer, Heidelberg (2006)
8. Irpino, A.: R-Package HistDAWass (2016)
9. Joe, H.: Asymptotic eﬃciency of the two-stage estimation method for copula-based
models. J. Multivar. Anal. 94(2), 401–419 (2005)
10. Jondeau, E., Rockinger, M.: The Copula-GARCH model of conditional depen-
dencies: an international stock market application. J. Int. Money Financ. 25(5),
827–853 (2006)
11. Rockinger, M., Jondeau, E.: Conditional dependency of ﬁnancial series: the copula-
GARCH model. FAME research paper No. 69 (2002)
12. Khemawanit, K., Tansuchat, R.: The analysis of value at risk for precious metal
returns by applying extreme value theory, Copula model and GARCH model.
IJABER 14(2), 1011–1025 (2016)
13. Pastpipatkul, P., Maneejuk, P., Wiboonpongse, A., Sriboonchitta, S.: Seemingly
unrelated regression based copula: an application on Thai rice market. In: Causal
Inference in Econometrics, pp. 437–450. Springer, Cham (2016)
14. Pastpipatkul, P., Panthamit, N., Yamaka, W., Sriboochitta, S.: A copula-based
Markov switching seemingly unrelated regression approach for analysis the demand
and supply on sugar market. In: Integrated Uncertainty in Knowledge Modelling
and Decision Making: 5th International Symposium, IUKM 2016, Da Nang, Viet-
nam, 30 November–2 December 2016, pp. 481–492. Springer, Cham (2016)
15. Patton, A.J.: Applications of copula theory in ﬁnancial econometrics. Unpublished
Ph.D. dissertation, University of California, San Diego (2002)
16. Sharpe, W.F.: Mutual fund performance. J. Bus. 39(S1), 119–138 (1966). https://
doi.org/10.1086/294846
17. Verde, R., Irpino, A.: Ordinary least squares for histogram data based on Wasser-
stein distance. In: Proceedings of COMPSTAT 2010, pp. 581–588. Physica-Verlag
HD (2010)

Factors Aﬀecting Consumer Visiting Spa Shop:
A Case in Taiwan
Meng-Chun Susan Shen
(✉), I-Tien Chu, and Wan-Tran Huang
Department of Business Administration, Asia University, Taichung, Taiwan
kesalan@gmail.com, chuitien@gmail.com,
wthuangwantran7589@gmail.com
Abstract. The research aims to search for factors that inﬂuence the customer’s
decision-making process regarding spa services of a case study spa in Taiwan
based on a Count Data Model. The estimation is via Poisson regression analysis
and negative binomial regression analysis. 167 questionnaires were collected
from Taiwanese customers. The results of both Poisson regression and negative
binomial regression are statistically signiﬁcant at the conventional levels, which
provides the predictions of the consumer’s decision-making. The study shows
that the customer’s demography and customer satisfaction towards the case study
spa have an impact on a consumer’s decision-making process when selecting spa
services. Therefore, the spa can consider its marketing strategies based on the
result.
Keywords: Spa · Consumer decision making · Count data analysis
Poisson regression · Negative binomial regression · STATA
1
Introduction
The healing power of spa, which has thousands of years of history, is believed to be a
natural way for a person to recover from physical exhaustion and mental distress. With
the aromatherapy and massage provided by spas, the customers are able to relax and
ﬁnd peace in mind. Hence, attracting people around the world to pay a visit, searching
for balance of life, spa is currently a growing business and has become a competitive
industry [1].
With the launch of new labor policy, “One Fixed Day Oﬀ and One Flexible Rest
Day Policy”, in Taiwan in 2017, the importance of resting and relaxing has become the
center of Taiwanese’s attention. For a long time, Taiwanese people have valued the
importance of hard work and long working hours [2]. The labor policy is to assure that
all employees are entitled to have suﬃcient rest periods to avoid any possible health
issues in the future. The basic regulation is that a worker is entitled to have two regular
days oﬀ every seven days. One day is a regular leave and the other one is a rest day [3].
Therefore, health services have expanded and grown in Taiwan to promote the well-
being of people. Meanwhile, people are also searching for alternatives from health care
facilities. An emphasis on prevention and health promotion makes health service busi‐
ness widely popular in Taiwan [4]. Among all the choices for people to relax when free,
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_40

one of the popular options for Taiwanese is visit a spa. Spa entrepreneurs are trying to
compete to be known and recognized from many aspects, such as service programs
provided, interior decoration, and customer services. In other words, customers are the
key factor that bring business success in the spa industry [5].
The purpose of the study is to ﬁnd out the factors that aﬀect the consumer’s decision
on visiting a spa shop. The factors may collectively inﬂuence the customer’s decision-
making process. Some of the factors are observed, but some are not. The observed are
labeled as x while unobserved as y. The factors that aﬀect the customer’s decision
through a function:
D = h(x, y).
(1)
We can look at the function as the decision-making process. In other words, given x and
y, the customer’s choice can be determined.
Since y is not observed, the customer’s decision cannot yet be determined exactly.
Hence, the unobserved terms are considered random with density f(y). The probability
that the customer chooses a particular spa and services from all possible outcomes is
simply the probability that the unobserved factors are. Therefore, the behavioral process
results in that outcome: P(D|x) = Prob(y s.t. h(x,y) = D).
We can deﬁne an indicator function J[h(x,y) = D] that takes the value of 1 when the
statement in brackets is true and 0 when the statement is false. In other words, J[·] = 1
if the value of y, combined with x, induces the customer to choose outcome D, which is
the going to a certain spa, and 0 if the value of y, combined with x, induces the customer
to choose some diﬀerent outcome. Then the probability that the customer chooses
outcome D is the expected value of this indicator function. The expectation is over all
possible values of the unobserved factors as follows:
P(D|x) =Prob (J[h(x, y) = D] = 1)
= ∫J[h(x, y) = D]f(y)dy.
(2)
Stated in this form, the probability is an integral for the outcome of the behavioral process
over all possible values of the unobserved factors. To calculate this probability, the
integral must be evaluated [6].
The purpose of the study is to identify the unobserved factors that may inﬂuence the
customer’s decision making when choosing a spa. Then, the result can provide the case
company the insight to improve their strategic plan and create marketing strategy by
establishing the appropriate prices as well as promoting and maintaining customer
loyalty. Moreover, the project can be used as a guideline for future research related to
spa companies and help them understand the fundamental factors that make impact on
customers choosing spa services.
2
Literature Review and Hypotheses
Nowadays, consumers demand a higher standard of service from the spa industry, which
means the spa industry has become highly competitive. Hence, it is important for
564
M.-C. S. Shen et al.

entrepreneur and staﬀ of a spa to continuously improve and develop to increase their
competitiveness to survive. Furthermore, the customer’s satisfaction is the foundation
of word of mouth marketing strategy. However, we must also search for other channels
for positive reviews to be spread around.
Tabashi, searching for the most successful spas worldwide, suggests that a positive
image created by having professional management has positive impact on customer’s
decision when choosing a spa [7]. In other words, a professional image can be created
by having good organization structure, providing quality services to customers, and
carrying good products. When a spa has healthy management in organization with strong
marketing strategies, it can attract new customers to use the services and products, which
can help develop the existing customer’s loyalty to the spa.
Lu et al. claim that income is one of the important factors when customers choosing
a spa after surveying Taiwanese lifestyle. Taiwanese long for good health, family time,
and wealth [8]. In addition, according to a survey conducted by Taiwan Ministry of
Economic Aﬀairs in 2003, the percentage of Taiwanese people visiting hot springs
increases every year. While the number of people traveling for health purposes continues
to rise, hot spring tours are the result of consumers’ needs in search for the balance of
health, family time, and budget. While Taiwanese are willing to pay for health services,
including spas, marketing strategies are needed for expanding the current number of
customers.
Lu et al. also suggest that the perceived quality of service, price, and risk are highly
related to the customer’s decision making [9]. The ﬁndings help researchers and spas
realize the factors that have impact on the consumer’s decision making. Overall, good
services and quality products with acceptable prices provided are more likely to satisfy
customers. Therefore, the factors of administration and product will be used as a variable
in this project of study.
Niratisayaputi claims that people usually visit the spas recommended by family,
friends, and colleagues. While word of mouth plays an important role in decision
making, customers also take location, promotion, and staﬀ of a spa into consideration.
However, the most important factor, based on the research, is the quality of the staﬀ
members whom the customers rely on for professional suggestions. They should demon‐
strate speaking politely, developing good relationship with customers, and oﬀering
quality treatments. In other words, putting the customers ﬁrst is the quality that the spa
goers are looking for. Moreover, the results between male and females are diﬀerent.
Male customers focus on services and the cleanness of the location. Female customers
tend to check the prices each time they visit. Since the research implies that adminis‐
tration and staﬀ are the main variables, the project will analyze the factors in this category
in details [10].
Word of mouth, the information passing from one consumer to another, has impact
on the customer’s decision-making process when choosing a product or service [11];
however, it may not be the only channel that consumers obtain information. Therefore,
the project also examines other channels of information, such as television shows and
magazines, to ﬁnd alternative accesses to promote services or products. Therefore, a spa
Factors Aﬀecting Consumer Visiting Spa Shop: A Case in Taiwan
565

must create marketing strategies to reinforce quality consciousness, brand conscious‐
ness, fashion consciousness, recreation consciousness, impulse consciousness, price
consciousness, and brand loyalty.
Therefore, the hypothesis, the possible factors, that would inﬂuence the customer’s
decision making are as follows.
Hypothesis 1: The customer chooses the case study spa based on the customer’s
demography.
Hypotheses 2: The customer chooses the case study spa based on the customer’s
satisfactory level towards the case spa’s services, staﬀ, and products.
Hypotheses 3: The customer chooses the case study spa based on the customer’s
other concerns.
3
Methodology
To ﬁnd out the factors that aﬀect consumers visiting a spa shop, the project consists of
three stages. First, the initial interview to collect basic information regarding the care
study spa. Secondly, questionnaires are provided to the case study spa’s customers to
collect data. The ﬁnal stage is the count data analysis.
3.1
Case Study Spa Information
The case study spa chain was founded in 1997. Although it once expanded its branch to
Mainland China in 2006, but the services in China were terminated due to the diﬃculties
in human resources and ﬁnancial support. Currently, the care study spa has 51 branch
stores with over 500 employees in Taiwan and has become one of the largest and famous
beauty spa in Taiwan. The case study company is still considering to branch out more
in the near future.
The case study spa focuses on serving female customers only. The philosophy of the
case study spa, “Beauty is a reﬂection of both inner and outer beauty,” is to promote
outer beauty through both inner and outer treatments. Therefore, customers are provided
with various treatment programs, hoping that they will be able to ﬁnd the ones that best
suit their needs.
While the case study spa is able to provide quality beauty products from domestic
and international brands, the greatest challenge for them is to ﬁnd skillful professional
staﬀ because the skills require long-term training and practice. Another issue for the
case study spa is the innovation of the business plan. The case study spa needs to
constantly renew and update their facilities and equipment; otherwise, they can lose their
current advantages and be out of the business.
3.2
Questionnaire and Data Collection
The questionnaire survey consists of three parts. Part 1 is the basic personal data.
Customers are asked to provide personal demographic data, such as gender, marital
status, age, occupation, income, education level, and sources of information (Table 1).
566
M.-C. S. Shen et al.

Table 1. Demographic variable names and description
Variable names
Variable description
Gender
Gender, but only female customers in this study
Status
Married or single
Age
Age
Occupation
Occupation
Income
Income per month
Education
Education level
Resources
How do you know about the spa case company?
Part 2 is the customer’s satisfaction towards the case study spa in terms of quality
of treatments, staﬀ, products, and environment. This part provides more information
regarding the factors that the customer may choose the case study spa. Part 3 is the survey
of spa services used by the respondent within the past two months. This part shows the
results of descriptive statistics as well as the reliability and validity analysis. The samples
are analyzed with count data generated by STATA.
3.3
Count Data Analysis
Winkelman suggests that regression is a tool to apply for researchers who would like to
explore data or weight the evidence in data for or against a speciﬁc hypothesis. Regres‐
sion model is used in two cases. First, the function is to reduce the dimension of complex
information. The second function is the test economic theories, quantify theoretical
relations as well as predict and forecast [12].
Poisson Regression Model has two components: a distributional assumption and a
speciﬁcation of the mean parameter distribution. A model needs to be correctly speciﬁed
to apply the results for maximum likelihood estimators. This requirement is more
binding for count data models than for the normal linear models. The count model
depends on the type of available data. Count data analysis in this project is estimated
with STATA to provide support. Poisson process is a special case of count process
which, in turn, is a special case of a stochastic process.
Cameron indicates that count data is concerned with models of event counts. An
event count refers to the number of times an event occurs [13]. For example, the number
of airline accidents or earthquakes. The main focus of count data is regression analysis
of event counts. The benchmark model for count data is the Poisson distribution. It is
useful at the outset to review fundamental properties and characterization results of the
Poisson distribution [14].
4
Empirical Results
The respondents of the survey are Taiwanese customers who have experienced any
forms of spa services in the case study spa within a duration of two months. The number
of returned complete questionnaires is 167 units.
Factors Aﬀecting Consumer Visiting Spa Shop: A Case in Taiwan
567

4.1
Personal Information Analysis
The result reveals that women aged between 25 to 54 are the primary users of the case
study spa. The majority of the customers are oﬃce workers and self-employed with the
average monthly income ranges between 20,000 NTD and 99,999 NTD. Approximately
80% of the participants have a college degree or higher. Therefore, the project suggests
that women with higher level of education, stable jobs, and above-average income are
most likely the target customers. They are most likely the ones who suﬀer from stress
at work and are able to aﬀord spa services. The primary information resource is still the
word of mouth, while those customers received information from friends and colleges
with similar background or working environment. However, approximately 40% of the
participants obtained information from the Internet.
4.2
Customer Satisfaction and Other Factors
The second part of the questionnaire is to examine the customer’s satisfaction. Conse‐
quently, the case study spa is able to improve current services and forming eﬀective
marketing strategies. On one hand, the spa should cultivate customer loyalty from
current customers in order to attract new customers by sharing the reviews. On the other
hand, the case study spa should carefully examine the not-so-popular programs and ﬁnd
ways to persuade potential customers to try the services.
Over half of the customers have been with the case study spa for 2 to 10 years.
Approximately 80% of the customers spend from 1,000 NTD to 10,000 NTD at the spa
every month. The spa provides various kinds of treatment, from facial treatments to body
treatments and specialized treatments.
The satisfaction survey suggests that the top three popular treatments are “Aroma
and hydraulic spa capsule,” “Relaxing aesthetic muscle with ADS light,” and “Lower
body healthcare.” Taiwanese female customers care much about the overall well-being
and the ﬁtness of lower body. However, with a total of 25 treatments provided by the
case study spa, it is challenging to predict the possibility of purchase with such wide
range of choices. Therefore, it relies on the customer’s satisfactory level regarding the
case study spa’s staﬀ and other possible factors.
The factors adopted in the questionnaire that may or may not inﬂuence the customer’s
decision-making process are location, spa administration, staﬀ, and spa products. For
location, 110 out of 167 customers consider location a very important factor. While a
spa is located near their homes or oﬃces, a greater possibility they would pay an initial
visit. It would be even more attractive if the spa can provide free parking nearby. When
it comes to administration and staﬀ, customers expect to see enthusiastic and well-
trained staﬀ with proﬁcient knowledge and skills. That is the initial trust between the
spa and the customer. Finally, customers expect to have reliable spa products and vari‐
eties to choose from based on their needs and budget. Therefore, these are additional
factors that the care study spa should consider about.
568
M.-C. S. Shen et al.

4.3
Reliability Test
To analyze the reliability of the measurement tools or the questionnaire survey, we put
all the data and test Cronbach’s alpha reliability coeﬃcient before running other
methods. To test construct reliability, values of Cronbach’s alpha for all constructs are
reported by using STATA. The value of Cronbach’s alpha is estimated for each item
Table 2. Reliability assessment of variables
I tem
Obs
Sign
Item-test
correlation
Item-test
correlation
Average interitem
covariance
Alpha
Status
167
+
0.3502
0.2823
.0284601
0.6316
Age
167
+
0.6698
0.5794
.0231476
0.5901
Occupation
167
+
0.4497
0.3304
.0262828
0.6209
Income
167
+
0.5397
0.3887
.024064
0.6099
Education
167
+
0.2510
0.1454
.0289568
0.6400
Resources
167
–
0.2300
0.1188
.0291762
0.6426
Knowcompany
167
+
0.3743
0.2548
.0273647
0.6296
Payment
167
+
0.5986
0.5114
.0247929
0.6036
Goodloc
167
–
0.2677
0.2005
.0291086
0.6367
Carpark
167
–
0.2445
0.2027
.0296087
0.6389
Surround
167
–
0.0747
0.0169
.0303724
0.6460
Space
167
–
0.1246
0.0756
.0301047
0.6433
Branches
167
–
0.0664
0.0018
.0304794
0.6477
Satisﬁed
167
+
0.1915
0.1191
.0296197
0.6413
Other
167
+
0.1156
0.0961
.0302824
0.6434
Goodknow
167
–
0.1014
0.0356
.0302392
0.6456
Stfavailable
167
+
0.2074
0.1585
.0296952
0.6400
Solveprob
167
–
0.0897
0.0486
.0302721
0.6441
Cantrust
167
–
0.1510
0.0831
.0299155
0.6432
Enthusiastic
167
+
0.2426
0.1713
.0292493
0.6382
Professional
167
–
0.1135
0.0477
.0301614
0.6450
Respondreq
167
+
0.1452
0.0784
.0299553
0.6434
Servicemind
167
+
0.2043
0.1385
.0295667
0.6403
Goodmang
167
+
0.1854
0.1237
.0297142
0.6411
Highquality
167
–
0.1691
0.1031
.0297994
0.6421
Wellbrand
167
+
0.3862
0.3274
.0283914
0.6303
Canchoose
167
+
0.1782
0.1082
.0297234
0.6419
Reliable
167
+
0.2148
0.1423
.0294467
0.6399
Chooseappate
167
–
0.2977
0.2393
.0290335
0.6354
Appateprice
167
–
0.1797
0.1116
.02972
0.6417
Satisﬁedr~t
167
–
0.0264
−0.0453
.0307844
0.6504
Spavis
167
+
0.7555
0.4918
.0165355
0.6179
Test scale
.0285633
0.6439
Factors Aﬀecting Consumer Visiting Spa Shop: A Case in Taiwan
569

and shown in Table 2. For all measures, if Cronbach’s alpha value is greater than 0.60,
it means the questionnaire survey has reasonable reliability.
4.4
Count Data Model Estimation
Count data model estimation is used to identify factors aﬀecting customer’s decision-
making process. To analyze the process, all the questions from questionnaire survey are
chosen to be independent variable that predict which factors have impact to the customer.
The project survey chooses the count data analysis to analyze the data by using STATA.
The model suggests that there are 4 variables, marital status, age, occupation and
information resources, having impact on spa visiting in case study spa because the p-
value is less than 0.0, which means the result is signiﬁcant. The negative binomial
regression summary shows the relationship between spa visiting and customer demog‐
raphy. We ﬁnd that the result of negative binomial regression and Poisson regression
have some diﬀerences. Only two variables, age and occupation, are accepted because
their p-value are less than 0.05.
The same procedure is repeated for spa location, spa administration and staﬀ, and
spa products. For spa location, good location is strongly signiﬁcant while having branch
spas can also be a selling point, according to the Poisson regression. With the negative
binomial regression analysis, good location also shows strongly signiﬁcant. For spa
administration and staﬀ, Poisson regression shows that 4 variables have signiﬁcant
results: staﬀ are always available, service can be trusted, staﬀ are enthusiastic to provide
services, and staﬀ have a service mind. However, only enthusiasm and service mind are
accepted in the binomial regression model. Finally, for spa product, Poisson regression
shows that good brands and choosing appropriate products are the key factors while
binomial regression agrees with the result.
5
Conclusion
Based on the outcome of the study, there are quite a few unobserved factors that may
inﬂuence the customer’s decision-making process when choosing a spa. The case study
spa attracts Taiwanese female customers between ages of 25 to 54, with suﬃcient
monthly income ranging from NTD 20,000 to NTD 99,999. Those female customers
are most likely well-educated and with stressful jobs. The customer’s information
resources are most likely word of mouth or the Internet. They care about the overall
well-being and the ﬁtness of the lower part of the body. When it comes to the customer’s
satisfaction, the customer may put emphasis on the case study spa’s location, the staﬀ’s
availability, the trustworthiness of the services as well as the enthusiasm of the staﬀ.
Finally, the customer may be inﬂuenced by the quality and the variety of the spa products.
In terms of the results of the study, we have a few recommendations for spas in
Taiwan. First, the spa should develop a user-friendly website to promote services and
products since the number of the Internet users is increasing. Inviting current customers
and bloggers to write positive reviews may help. While word of mouth is still powerful,
the spa company may encourage the current customers and develop their loyalty by
570
M.-C. S. Shen et al.

providing them exclusive and extensive services if they are able to refer new customers.
Future research can focus on the level of how the online reputation or online word of
mouth inﬂuences the customer’s decision-making process. Spa service is rather personal
and intimate, which is diﬀerent from buying clothes or accessories. How much can the
customer trust a stranger on the Internet would be a question.
Moreover, the spa must provide staﬀ training on a regular basis because they can
directly inﬂuence the customer’s satisfactory level. The staﬀ must have workshops
regarding how to take care of customer’s individual needs, to improve professional skills
and knowledge as well as to develop positive relation with customers. The case study
spa provides a wide variety of treatments; therefore, the customer should depend on the
staﬀ’s expertise to choose the appropriate treatment based on the customer’s needs,
budgets, and other concerns. A mentor-apprentice system can be formed to train the new
staﬀ while the workshop or training is not yet provided. The key is the develop the team
spirit rather than having staﬀ competing with each other.
In addition, as part of customer satisfaction, the spa should consider its pricing strat‐
egies and location. Since each customer has diﬀerent buying power, depending on the
customer’s income, the price of treatments and products would have to be ﬂexible
enough to meet the customer’s needs. When it comes to choosing a location, it depends
on if the spa provides an easy access to the customer. A spa may choose to be located
in the area where customers can get to by public transportation. However, again the spa
is rather intimate and personal, some customers may prefer to drive for the sake of
privacy. Therefore, having a few branches with parking space for easy access may play
a role in customer satisfaction.
In the future, while the number of male customers using spa services increases, it is
possible that the case study spa will expand their business to serve the male customers.
Therefore, the case study spa should conduct a research on the male customer’s needs
before launching the services for male because male customers are physically and
mentally diﬀerent from the female ones. That is, treatments for both genders should be
diﬀerent if another set of research is created to survey on male customers’ needs and
satisfaction.
References
1. Dahlan, N., Yusoﬀ, Y. M.: Service Innovation in the Business Models on the Spa and Med
Beauty at the Saujana. Case Studies in Innovation, Centre for Service Research, Manchester
Business School, The University of Manchester in Collaboration with SRII Service
Innovation SIG, pp. 11–13 (2010)
2. Sui, C.: Deaths Spotlight Taiwan’s ‘Overwork’ Culture. BBC News, Taipei (2012)
3. Ministry of Labor, Republic of China. https://laws.mol.gov.tw
4. Spas and the Global Wellness Market: Spas and the Global Wellness Market: Synergies and
Opportunities, prepared by SRI International, May 2010
5. Lawrimore, E.W.: “Buck”: The 5 Key Success Factors: A Powerful System for Total Business
Success. Lawrimore Communications Inc. (2011)
6. Train, K.: Discrete Choice Methods with Simulation, pp. 3–6. Cambridge University Press
(2002)
Factors Aﬀecting Consumer Visiting Spa Shop: A Case in Taiwan
571

7. Tabashi, M.H.: Current research and events in the spa industry. Cornell Hosp. Q. 51(1), 102–
117 (2010)
8. Lu, I.Y., Shiu, J.Y.: Decision-making framework of customer perception of value in
Taiwanese spa hotels. Soc. Behav. Pers. Int. J. 39(9), 1183–1192 (2008)
9. Lu, I.Y., Shiu, J.Y.: Customers’ behavioral intentions in the service industry: an empirical
study of Taiwan spa hotels. Asian J. Qual. 10(3), 73–85 (2012)
10. Niratisayaputi, N.: The Factor that Aﬀect for Spa Business Entrepreneur, Faculty of
Economics, Chiang Mai University (2010)
11. Ozcan, K.: Consumer-to-consumer interactions in a networked society: word-of-mouth
theory, consumer experiences and network dynamics. Dissertation of University of Michigan
(2004)
12. Winkelmann, R.: Econometric Analysis of Count Data. Springer, Heidelberg (2008)
13. Cameron, C., Trivedi, P.: Regression Analysis of Count Data. Cambridge University Press,
Cambridge (1998)
14. Taylor, H.M., Karlin, S.: An Introduction to Stochastic Modelling. Academic Press, San
Diego and New York (1994). Revised edition
572
M.-C. S. Shen et al.

The Understanding of Dependent Structure
and Co-movement of World Stock Exchanges
Under the Economic Cycle
Songsak Sriboonchitta, Chukiat Chaiboonsri(B), and Jittima Singvejsakul
Faculty of Economics, Chiang Mai University, Chiang Mai, Thailand
songsakecon@gmail.com, chukiat1973@gmail.com, Jittimasvsk@gmail.com
Abstract. This study was to focus on the patterns of economic
booms (bull markets) and recessions (bear markets) among world stock
exchanges such as Europe (Euro Stoxx), USA (S&P 500), Asia (SSE
composite index and Nikkei 225 index) and ASEAN (FTSE ASEAN).
Monthly data was collected during 2000 to 2016. Econometrically,
we employed Markov Switching Bayesian Vector Autoregressive model
(MSBVAR) to determine regional switches within these ﬁnancial data
sets as well as CD-Vine copula approaches was used to explore the conta-
gions and patterns of structural dependences. To clarify the connectional
details in each type of switching regimes, the results presented the Ellipti-
cal copula was chosen and it indicated these monthly collected data con-
tained symmetrical dynamics co-movements. In addition, it implied the
stock markets were assumed to have small ﬂuctuations since the govern-
ments had stable policies to control the risk and asymmetric information
in ﬁnancial markets eﬃciently. Base on CD-Vine copula trees, the results
indicated Asia and European stock markets had a strongly dependence
in economic booms and recessions during the pre-crisis period (2000 to
2008). Conversely, in the post-crisis period, the US stock market and
ASEAN stock market became the strong dependence with Europe. This
meant that capital ﬂows was mostly transferred between Europe and
Asia ﬁnancial markets during the pre-crisis periods (2009 to 2016). After
that, the direction of capital ﬂows were changed dramatically to the US
stock market in the post-crisis periods. Predictively, this seems that the
capital ﬂows will return to European and US ﬁnancial market, which
these two continents have a strongly long-term ﬁnancial dependence and
deeply positive diplomacy.
Keywords: MSBVAR · CD-vine copula · Bull markets
Bear markets · Stock markets
1
Introduction
Because the ﬁnancial crisis negatively aﬀected the economic system in the
United States during 2008, triggered by collapse in house prices, and caused
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_41

574
S. Sriboonchitta et al.
the Great Recession. This leaded the world economy to be suﬀered dramatically
(Bloomberg 2009). This was the underline of the global ﬁnancial crisis and caused
European banks to enormously lost their liquidity in the ABS market. Moreover,
the reliance on US currency for European banks had been sharply decreased
(Lane 2013). Additionally, this can be seen from the low expansion rates of
GDP in ASEAN, US, Europe, Japan and China during the period between
2000 and 2015, which were respectively represented in Figs. 1 and 2. For the
pre-crisis (2000–2008), GDP in these ﬁve countries slightly grew up. In partic-
ularly, the economic expansion rate of japan did not change. In the post-crisis
(2009–2015), this can be seen that the economic growth in many countries around
the world continuously grew up. This is because the eﬀect from the transferences
of capital ﬂows in the term of ﬁnancial markets. Accordingly, this paper inten-
sively explored a structural cycling pattern between them in the Worlds Stock
Fig. 1. Gross Domestic Product or GDP of ASEAN in current US dollar for during
period of 2000–2015 Source: World development indicators.
Fig. 2. Gross Domestic Product or GDP of China, Japan, Euro area and the United
States in current US dollar for during period of 2000–2015 Source: World development
indicators.

The Understanding of Dependent Structures Under the Economic Cycle
575
Exchanges as well as rare ﬁnancial structural dependences, and these ﬁndings
can be the solution to understand the deeply ﬁnancial structures between major
stock markets around the world that is useful information for supporting domes-
tic and foreign investors to predict and plan their investments.
2
The Objective and Scope of Research
The objective of this research is to explore the pattern of structurally ﬁnancial
dependences in bull and bear markets among stock markets in US, Europe, Asia,
and ASEAN during 2000 to 2016. The monthly time-series data such as in S&P
500 index (US), Europe (the Euro Stoxx), China (SSE composite index), Japan
(Nikkei 225 index) and ASEAN (FTSE ASEAN) were collected to be considered,
and they were divided into 2 periods: pre-crisis (2000 to 2008) and post-crisis
(2009 to 2016).
3
Methodology
3.1
The Markovian Switching Bayesian VAR Model
This paper has two steps to determine the pattern of structural dependences
among the capital markets. First, the Markov Switching Bayesian VAR model
was employed to determine regime changes within data, and examine correlations
among the European, US, Asia and ASEAN stock market. This found regimes
for bull and bear markets.
The Markovian switching is constructed by combining two or more dynamic
models via the Markovian switching mechanism (Hamilton 1994) and this can
be shown in Eq. 1.
zt = α0 + βzt−1 + εt, St = o,
(1)
ε = i.i.d. random variables with zero means and variances σ2
t
|β| < 1.
This is stationary AR (1) processed with mean α0/(1 −β)when St = 0, and
it switches to another stationary AR (1) process with mean (α0 + α1)/(1 −β)
when St is changed from zero to one. Then it provided that α1 ̸= 0, this model
admits two dynamic structures at diﬀerent levels, depending on the value of
state variables St.
The evaluation of the latent variable drives regime changes, St, is governed
by the ﬁrst-order Markov chain condition with constant transition probabilities
expressed as the (SxS) transition probability matrix (P):
pr = (St = j|St−1 = i) = pij,
(2)

576
S. Sriboonchitta et al.
p =
⎛
⎜
⎜
⎝
p11 p12 . . . p1s
p21 p22 . . . p2s
.
.
. . .
.
p1s p2s . . . pss
⎞
⎟
⎟
⎠.
(3)
Bayesian statistics was applied to do econometrical estimations, and this
inference allows us to obtain a joint posterior distribution of parameters and
latent variables. Bayesian simulated methods are well suited to estimate Markov
Switching models (Kim and Nelson 1999). Conditionally, the value at risk (VaR)
analysis allows parameters of the model to be considered as random variables.
Generally, the typical VAR analysis is often constrained by the limited size of
data sets, which are not compatible models with large numbers of parameters.
The Bayesian method tackles this over-parameterisational problem by assigning
initial probabilities into many parameters. Furthermore, the construction of a
BVAR model will reduce the complexities involved future extensions (Canova
2007).
3.2
ARMA-GJR Model for Marginal Distributions
Technically, the CD-Vine copula was adopted to estimate the pattern of struc-
tural dependences among stock markets. We will ﬁnd the major stock markets
of bull and bear markets in pre-post crisis. ARMA-GJR model was used to con-
duct marginal distributions for the copula model. The form of the ARMA (P,
Q)-GJR (K, L) model can be expressed as Eq. 4.
rt = c + Σp
i=1φirt−i + Σp
i=1Ψiεt−i + εi
(4)
εt = htηt
(5)
h2
t = ω + Σk
i=1αiε2
t−i + Σk
i=1γiI[εt−i < o]ε2
t−i + Σl
i=1βih2
t−i,
(6)
where Σp
i=1φi < 1, ω > 0, αi > 0, βi > 0, αi + γi > 0 and Σk
i=1αi + Σl
i=1βi +
1
2Σk
i=1γi < 1. The formulas (4) and (6) are call mean equation and variance
equation, respectively; the formula (5) describes the residual εt is consist of
standard variance ht and standardized residuals ηt; the leverage coeﬃcient γi is
applied to negative standardized residuals. In addition, the standardized residual
are assumed to be the skewed student-t or skewed generalized error distribution
and the cumulative distributions of standardized residuals are formed to plug
into copula model.
3.3
Copula
The fundamental theorem is based on the concept of (Sklar 1959) and this can
be shown in Eq. 7,
F(x1, x2, . . . , xn) = C(F1(x1), F2(x2), . . . , Fn(xn)).
(7)

The Understanding of Dependent Structures Under the Economic Cycle
577
F: n-dimensional distribution with marginal Fi , i = 1, 2, 3
x1, x2, . . . , xn :random vectors
C: n-copula for all x1, x2, . . . , xn.
The function C is a distribution function that has uniform margins between
zero and one, and it is labelled as the copula function. It binds the univariate
margins F1 and F2 to produce bivariate distribution F.
3.4
The C-D Vine Copulas Construction
Vine copula models are graphical representation to specify pair copula construc-
tions (PCCs) introduced by (Joe 1996). These models are consequently devel-
oped by Bedford and Cook (2001, 2002). Basically, a principle for constructing
multivariate copula generated from the product of bivariate pair copula was sta-
tistically described as canonical (C-) vines and (D-) vines by Aas et al. (2009).
This contribution was a ﬂexible model since bivariate copulas can easily accom-
modate complex structural dependences such as asymmetric dependences or
strong joint tail behaviors (Joe et al. 2010). Based on previous reviews, this
has been already pointed out the estimated patterns of relation among ﬁnancial
markets in world exchanges are deﬁned as X = x1, x2, x3, x4, x5, with marginal
distribution function F1, F2, F3, F4, F5, and corresponding densities. As a result,
it can be written as Eq. 8.
f(x1, x2, x3, x4, x5) =f(x1)f(x2|x1)f(x3|x1, x2)
f(x4|x1, x2, x3)f(x5|x1, x2, x3, x4),
(8)
where C is the copula associated with F via Sklar theorem. From Eq. 5, it can
be determined the conditional density of x2, and given x1 as
f2|1(x2|x1) = f(x1, x2)
f1(x1)
= c1,2(F1(x1), F2(x2))f2(x2),
(9)
and
f2,3|1(x3|x1, x2) = f(x2, x3|x1)
f(x2|x1)
= c2,3|1(F(x2|x1), F(x3, x1))f(x3|x1)
= c2,3|1(F(x2|x1), F(x3, x1))c1,3(F1(x1), F3(x3))f3(x3),
(10)
and
f3,4|1,2(x4|x1, x2, x3) = f(x3, x4|x1, x2)
f(x2, x3|x1)
= c3,4|1,2(F(x3|x1, x2), F(x4|x1, x2))f3(x3)f(x4|x1, x2)
= c3,4|1,2(F(x3|x1, x2), F(x4|x1, x2))c1,4
(F1(x1F4(x4))f4(x4)c2,4(F2(x2F4(x4))f4(x4),
(11)

578
S. Sriboonchitta et al.
and
f4,5|1,2,3(x5|x1, x2, x3, x4) = f(x4, x5|x1, x2, x3)
f(x3, x4|x1, x2)
= c4,5|1,2,3(F(x4|x1, x2, x3), F(x5|x1, x2, x3))f(x5|x1, x2, x3)
c1,5(F1(x1)F5(x5))f5(x5)
= c4,5|1,2,3(F(x4|x1, x2, x3), F(x5|x1, x2, x3))c2,5
(F1(x1)F5(x5))f5(x5)c2,5f5(x5)c3,5(F3(x3)F5(x5))f5(x5).
(12)
Therefore, the ﬁve-dimensional joint can be shown in terms of bivariate cop-
ula c1,2, c2,3|1, c1,3, c3,4|1,2, c1,4, c2,4, c4,5|1,2,3, c1,5, c2,5, c3,5 Based on graphical
of canonical (C-) and D-vines copula was presented by Fig. 3.
Fig. 3. Examples of ﬁve-dimensional C-vine tree (left panel) and D-vine tree (right
panel) Source: Brechmann and Schepsmeier (2013)
Considering Fig. 3, on the left-panel trees represented the decomposition of a
ﬁve-dimensional joint density function. The circled nodes are on the ﬁrst-tree and
it showed the four marginal density functions, f1, f2, f3, f4, f5. The remaining
nodes on the other trees are not used in the ﬁgure. Each edge corresponds to a
pair-copula function.
On the other hand, on the right-panel trees represented the decomposition
of ﬁve-dimensional joint density functions. The circles nodes showed the ﬁve
marginal density functions written as f1, f2, f3, f4, f5. Each edge is labeled with
the pair-copula of the variables. The edges in level i become nodes for level i+1.
The edges for the ﬁrst tree are labeled as 1,2, 2,3, 3,4 and 4,5. The second tree

The Understanding of Dependent Structures Under the Economic Cycle
579
has edges labeled as 1, 3|2, 2, 4|3 and 3, 5|4. The third tree’s edges were labeled
as 1, 4|23 and 2, 5|34. Finally, the tree number fourth has only one edge labeled
as 1, 5|234 (Durante and Sempi 2009).
3.5
Bivariate Copula Families
The package CD-Vine provides a wide range of bivariate copula families, which
are divided into two major classes such as elliptical and Archimedean copulas
(Joe 1997 and Nelsen 2006). Elliptical copulas are directly obtained by invert-
ing Sklar Theorem (Eq. 7). Given a multivariate distribution function F with
invertible margins F1 and F2, then
C(u1, u2) = F(F −1
1
(u1), F(F −1
2
(u2)),
(13)
C: F is elliptical
u1, u2 ∈[0, 1]
F: distribution functions of invertible marginals F1, F2,
which are also implemented in CD-Vine, and they are the multivariate Student-t
copula. Consequently, this type of copula models can be expressed in Eq. 20,
C(u1, u2, u3, u4, u5) = tρ,ν(t−1
ν (u1), t−1
ν (u2), t−1
ν (u3), t−1
ν (u4), t−1
ν (u5))
(14)
ρ ∈(−1,1) and is dependence parameter
ν: degree of freedom for student t copula ν > 2.
Which tρ,ν is the multivariate Student-t distribution function contained cor-
relation parameters, ρ and ν, t−1
ν
denotes the inverse univariate Student-t distri-
bution function with ν degrees of freedom. Both copulas are obviously symmetric
and have lower and upper tail dependence coeﬃcients.
Multivariate Archimedean copulas, on the other hand, are deﬁned as
C(u1, u2, u3, u4, u5) = Ψ [−1](Ψ(u1), Ψ(u2), Ψ(u3), Ψ(u4), Ψ(u5)),
(15)
where: [0, 1] · · · [0, ∞] is a continuous strictly decreasing convex function such
that Ψ(1) = 0 and Ψ −1 is the pseudo-inverse,
Ψ −1(t) = Ψ −1(t), 0 ≤t ≤Ψ(0) or Ψ −1(t) = 0, Ψ(0) ≤t ≤∞,
(16)
Ψ is called the generator function of the copula C.
In addition, this paper implemented the common single parameter, which is
in the Archimedean family (Clayton copula). This is a more ﬂexible structure
allows non-zero lower and upper tail to be the diﬀerent dependent coeﬃcient
(Nelson 2006), then the Clayton are deﬁned as
Ψ = 1
θ(t−θ −1),
(17)

580
S. Sriboonchitta et al.
parameter range: θ > 0
Kendall′sτ:
θ
θ+2,
Tail dependence (lower, upper): (2
−1
θ , 0).
4
Data Description
The world stock exchanges data considered in this study consisted ﬁve largest
economics, for instances, the United States stock market (S&P 500 index),
European stock markets (the Euro Stoxx), China stock market (SSE composite
index), Japan stock market (Nikkei 225 index) and ASEAN stock markets (FTSE
ASEAN). Basically, all of data was transformed to be standardized residuals of
monthly log return observations (203 observations).
Considering Fig. 4, it provided the descriptive index returns of monthly data
in world exchanges during 2000 to 2016. Furthermore, Table 1 presents the gen-
erally statistical data.
Fig. 4. The index return of monthly data in world exchange during period of 2000 to
2016. Source: Thomson Routers Corp database.
5
Empirical Results of Research
5.1
The Results of Marginal Testing for Copula Model Estimation
Based on the LM-test, this already conﬁrmed that all of residual terms was
satisﬁed for marginal models, which were employed to estimate the CD-Vine
copula models. Additionally, the result of the KS testing already indicated that
the marginal model is eﬃciently speciﬁed to estimate the CD-Vine copula model
(Table 2).

The Understanding of Dependent Structures Under the Economic Cycle
581
Table 1. The descriptive statistics of the index return of monthly data in world
exchanges during period of 2000 to 2016
Items
S&P 500
Euro Stoxx
SSE
NIKKEI 225
FTSE ASEAN
Mean
0.002332
−0.001768
0.003468
0.000108
0.010439
Median
0.007791
0.007097
0.007078
0.003953
0.017787
Maximum
0.102307
0.137046
0.242528
0.120888
0.18341
Minimum
−0.185636
−0.206236
−0.282783
−0.27216
−0.377193
Std. Dev
0.043343
0.054506
0.08062
0.05823
0.066147
Skewness
−0.728593
−0.638291
−0.543509
−0.735032
−1.124449
Kurtosis
4.492987
4.102723
4.554236
4.379683
8.028629
Jarque-Bera
36.81406
24.06951
30.42681
34.37986
256.6652
Probability
0
0.000006
0
0
0
Phillips-Perron test statistic
−12.76037
−13.01843
−12.65397
−12.37694
−11.36490
(0.0000)
(0.0000)
(0.0000)
(0.0000)
(0.0000)
Sum
0.473446
−0.358927
0.704046
−0.022008
2.119088
Sum Sq. Dev
0.379481
−0.358927
1.312931
0.684933
0.883836
Observations
203
203
203
203
203
Table 2. Testing of the marginal distribution models based on LM-test (lag 2) and
K-S test.
S&P 500 Euro stoxx SSE
Nikkei 225 FTSE ASEAN
L-M test 0.7985
0.4919
0.9574 0.2783
0.6446
K-S test
0.000
0.000
0.000
0.000
0.000
5.2
The Estimated Results of the Bull and Bear Markets
in Pre-crisis and Post-crisis Periods Based on the Markovian
Switching Bayesian VAR Model
Expressly, the results were represented in Table 3 showed that the Markovian
Switching Bayesian VAR model computationally estimated the ﬂuctuated
regimes of ﬁve ﬁnancial stock indexes. Econometrically, the regimes are deﬁned
as Bull and Bear market. First, the index of the S&P ﬁnancial market con-
tained boom periods rather than recessions, which were 113 months and 90
months, respectively. Second, Euro stock indexes had recession situations more
than expansions, which were 94 months and 109 months, respectively. Third, the
ﬁnancial market in China (SSE) included expanding times more than recessions,
which were 110 months and 93 months, respectively. Forth, Japanese ﬁnancial
equity (Nikkei 225) contained booming situations more than recessing times,
which were 109 months and 94, respectively. Lastly, the ﬁnancial market in South
East Asia (FTSE ASEAN) had the ﬂuctuated situations between bull and bear
markets, which had 103 months for the booming periods and 100 months for
recessions.

582
S. Sriboonchitta et al.
Table 3. Testing number of bull market and bear market based on MSBVAR
S&P 500 Euro stoxx SSE Nikkei 225 FTSE ASEAN
Bull market (Months)
113
94
110
109
103
Bear market (Months)
90
109
93
94
100
5.3
The Estimation Results of the Contagion and Pattern
of Structural Dependences Toward World Exchanges in Bull
and Bear Markets Based on CD-Vine Copula Approach
There are two kinds of copula estimations. Elliptical and Archimedean copulas
were used to estimate the pattern of dependences among world exchanges. The
estimated result was investigated by CD-vine copula approach and it was rep-
resented in Appendix A. The best model based on AIC and BIC is Elliptical
copula, which is the T-copula model. Accordingly, this result based on CD-vine
indicated that there is a contagion among the two periods, which are the pre-
crises periods (2000–2008) and post-crises periods (2009–2016).
5.4
The Results of Estimation in the Pattern of Structural
Dependences Among Five Stock Markets of Economic Boom
(Bull Market) and Economic Recession (Bear Market) Based
on CD-Vine Trees from T-copula
5.4.1
Pre-crises Periods (2000–2008)
(a) The Elliptical t-copula of C-vine in Bull and Bear markets
As we see in Fig. 5, the ﬁnancial market in Europe was assumed to be the
central place that capital inﬂows and outﬂows were transferred during the post-
crises periods. Obviously, in the Bull situation, the markets between Europe and
Asia (ASEAN, Japan, and China) were the strongly structural dependence in
terms of capital ﬂows. Similarly, in the Bear market, the Asian ﬁnancial market
still strongly depended on the recessing time in the European market, but the
US ﬁnancial market had a weakly structural dependence with European in the
post-crisis periods. As a result, this implied that the capital ﬂows had been
mostly transferred between Europe and Asia during 2000 to 2008.
(b) The Elliptical t-copula of D-vine in Bull and Bear markets
Considering Fig. 6, the D-vine copula model provided the diﬀerent struc-
tural dependence from the C-vine model. In other words, the estimated result
stated that the Asean stock market strongly depended on the Japanese ﬁnancial
market. This structural dependence was stronger than the pair of European and
Asean. Accordingly, this can be indicated that most of capital inﬂows were trans-
ferred around Asia continent for bull situations during the pre-crises periods. On
the other hand, for recessing times during pre-crises periods, the D-vine result

The Understanding of Dependent Structures Under the Economic Cycle
583
Fig. 5. The estimation results of the pattern of structural dependences among Bull
and Bear markets in Elliptical (t-copula) from C-Vine during the pre-crises periods
(2000–2008)
Fig. 6. The estimation results of the pattern of structural dependences among Bull
markets in Elliptical (t-copula) from D-Vine during the pre-crises periods (2000–2008)
(as seen details in Fig. 7) showed that capital inﬂows were inversely moved from
Asean to Europe, but the structural dependence between the US ﬁnancial market
(Euro stoxx) and European market are quite weak.
5.4.2
Post-crises Periods (2009–2016)
(a) The Elliptical t-copula of C-vine in Bull and Bear markets
Considering into C-vine’s trees in Fig. 8, the European ﬁnancial market and
Asia stock indexes were a strong dependence during 2009 to 2016. In other
words, capital inﬂows were still exchanged intensively between European and
Asian stock markets after the economic crisis, especially the subprime crisis, had
been passed. Conversely, US and Japanese stock markets became the strongly
structural dependence with the European ﬁnancial market in recessing periods.

584
S. Sriboonchitta et al.
Fig. 7. The estimation results of the pattern of structural dependences among Bear
markets in Elliptical (t-copula) from D-Vine during the pre-crises periods (2000–2008)
Fig. 8. The estimation results of the pattern of structural dependences among Bull
and Bear markets in Elliptical (t-copula) from C-Vine during the post-crises periods
(2009–2016)
(b) The Elliptical t-copula of D-vine in Bull and Bear markets
According to details of the D-vine copula in bull periods during the post-crises
periods (as seen in Fig. 9), it is obvious that US and Asean stock markets strongly
depended on the Euro ﬁnancial market. This can be implied that capital inﬂows
from Europe had been started to change the direction from Asian continent
to North America. However, the structural dependences of ﬁnancial markets
between Asia, North America, and Europe were still strong in the post-crises
periods. On the other hand, speaking to details of the D-vine copula in bear
periods during the post-crises periods (as seen in Fig. 10), the result showed that
capital inﬂows were transferred inside Asia continent rather than internationally
moving to other continents in the recessing time during 2009 to 2016.

The Understanding of Dependent Structures Under the Economic Cycle
585
Fig. 9. The estimation results of the pattern of structural dependences among Bull
markets in Elliptical (t-copula) from D-Vine during the post-crises periods (2009–2016)
Fig. 10. The estimation results of the pattern of structural dependences among Bear
markets in Elliptical (t-copula) from D-Vine during the post-crises periods (2009–2016)

586
S. Sriboonchitta et al.
6
Conclusion
For this paper, the patterns of structural dependences among world stock
exchanges were successfully estimated. Empirically, the section of MSBVAR
results were conﬁrmedly divided the ﬁve ﬁnancial indexes into two periods,
including economic boom (bull markets) and economic recession (bear markets).
This explained that all of ﬁve ﬁnancial markets contained cyclical movements
and ﬂuctuated time-series trends, which cannot be directly estimated by assump-
tions of linearity. This study also found that there is a contagion among these
ﬁnancial indexes as well as two types of copula models, including Elliptical and
Archimedean, were investigated. However, the Elliptical copula is chosen to esti-
mate collected variables in this paper. The study on the structural cycling pat-
terns clariﬁed the Elliptical t-copula indicated the information is symmetric.
This implied that investors could easily receive same information inside these
ﬁve ﬁnancial markets (Nermuth 1982). Therefore, this stated that governments
have freedom choices to interfere the ﬁnancial markets or let them adjust them-
selves to have an independently stable system for controlling risks and asym-
metric information in their ﬁnancial structures. Interestingly, the prior research
of Lemmon and Ni (2008) found that speculative demands for equity options
were positively related to most investor sentiments. Especially, if they have high
leverage, they are also perfect vehicles for speculation. This empirical research
conﬁrmed that the Elliptical copula was suitable to estimate stock markets in
this paper.
Speciﬁcally considering Elliptical CD-vine copula’s results (t-copulas), in the
pre-crisis (2000–2008), this seemed that capital ﬂows were mostly transferred
between Europe and Asia stocks in both bull and bear markets, but there was a
small capital ﬂow between US and European ﬁnancial markets. In other words,
there was the strongly structural dependence of European and Asia stocks since
the ﬁnancial crisis in US was starting, and this cause negatively impacted the
conﬁdence rate of ﬁnancial sectors in US during that period. In the post-crisis
(2009–2016), similar to the result of the pre-crisis periods, the capital ﬂows
between Europe and Asia were still a strong dependence in bull situations, and
the ﬁnancial markets between US and Europe were deﬁned as a structural inde-
pendence, meaning that capital ﬂows from these two continents mostly moved
out to other places rather than domestically transferring. Interestingly, in recess-
ing time, the CD-vine copulas’ results indicated that the direction of capital ﬂows
from Europe to US stock markets (North America) had been returned since
US’s economy was systematically recovered. Hence, this can be implied that the
transference of funds, especially from Europe to US ﬁnancial markets, would be
predictively increased in the upcoming future, and ﬁnancial investments in US
can be positively mentioned.

The Understanding of Dependent Structures Under the Economic Cycle
587
Appendix A
See (Tables 4, 5, 6 and 7).
Table 4. C-vine copula testing in bull markets during pre-crisis and post-crisis periods
Canonicals (C-vine)
2000–2008
2009–2016
Bull markets
Parameters
SE.
Parameters
SE.
t-copula, clayton
t-copula, clayton
t-copula, clayton
t-copula, clayton
β1,2
199.9810, 0.0604
0.448, 0.007
199.9895, 0.0000
0.0.35, 0.000
β1,3
199.9763, 0.0000
2.398, 0.000
199.9895, 0.2128
0.012, 0.048
β1,4
199.9761, 0.0000
0.020, 0.000
199.9852, 0.0000
0.015, 0.000
β1,5
169.5743, 0.0000
948.211, 0.000
18.8452, 0.0001
0.002, 0.006
β2,3|1
199.9466, 0.0000
0.292, 0.000
199.7074, 0.0000
0.217, 0.000
β2,4|1
9.0388, 0.0994
16.042, 0.005
11.4471, 0.0925
28.950, 0.129
β2,5|1
5.9405, 0.0603
7.870, 0.007
199.9219, 0.0001
0.125, 0.001
β3,4|12
199.2320, 0.0000
1.474, 0.000
199.9749, 0.0302
0.023, 0.057
β3,5|12
198.5553, 0.0000
3.183, 0.000
5.9234, 0.0566
4.964, 0.075
β4,5|123
199.9657, 0.0000
0.022, 0.000
193.9490, 0.0003
373.888, 0.038
AIC
12.2180, 16.9958
5.3087, 11.6756
BIC
30.7194, 35.4972
23.8102, 30.1771
Log-likelihood
3.891, 1.502
7.346, 4.162
Table 5. C-vine copula testing in bear markets during pre-crisis and post-crisis periods
Canonicals (C-vine)
2000–2008
2009–2016
Bear markets
Parameters
SE.
Parameters
SE.
t-copula, clayton
t-copula, clayton
t-copula, clayton
t-copula, clayton
β1,2
149.9176, 0.0588
59.615, 0.013
4.2073, 0.1184
1.772, 0.112
β1,3
9.4854, 0.0000
0.014, 0.000
199.8413, 0.0000
0.064, 0.000
β1,4
5.5792, 0.0000
8.531, 0.000
3.3533, 0.1246
2.293, 0.070
β1,5
7.0223, 0.0000
0.000, 0.000
199.7341, 0.0000
0.339, 0.000
β2,3|1
47.7609, 0.1112
208.494, 0.065
199.4929, 0.0000
0.350, 0.000
β2,4|1
41.2768, 0.0000
468.237, 0.000
199.9691, 0.0825
0.018, 0.070
β2,5|1
61.3775, 0.0799
140.201, 0.096
4.6029, 0.0000
0.387, 0.000
β3,4|12
4.9238, 0.1120
4.347, 0.102
199.4031, 0.0784
0.326, 0.067
β3,5|12
2.0072, 0.2922
2.121, 0.119
6.5769, 0.0605
3.402, 0.091
β4,5|123
5.2875, 0.0000
7.527, 0.000
4.7700, 0.1838
4.747, 0.102
AIC
2.0584, 10.8949
11.7961, 11.8260
BIC
20.1250, 28.9615
29.8628, 29.8926
Log-likelihood
8.971, 4.553
4.102, 4.087

588
S. Sriboonchitta et al.
Table 6. D-vine copula testing in bull markets during pre-crisis and post-crisis periods
D-vine
2000–2008
2009–2016
Bull market
Parameters
SE.
Parameters
SE.
t-copula, clayton
t-copula, clayton
t-copula, clayton
t-copula, clayton
β1,2
199.8507, 0.0322
0.257, 0.071
199.9999, 0.0000
0.000, 0.000
β1,3
199.8912, 0.0000
0.164, 0.000
199.9399, 0.0000
0.331, 0.001
β1,4
199.0949, 0.0000
1.233, 0.000
199.9985, 0.0000
0.000, 0.000
β1,5
199.8215, 0.0002
0.286, 0.001
200.0000, 0.0008
0.001, 0.100
β2,3|1
199.7572, 0.0000
0.411, 0.000
10.7776, 0.2105
0.000, 0.049
β2,4|1
8.1526, 0.0915
0.002, 0.091
7.9665, 0.0900
15.599, 0.138
β2,5|1
15.2141, 0.0000
44.325, 0.000
200.0000, 0.0355
17.621, 0.073
β3,4|12
199.8254, 0.0000
0.202, 0.000
199.9790, 0.0000
0.000, 0.000
β3,5|12
8.4032, 0.0445
14.787, 0.069
199.9998, 0.0000
0.003, 0.002
β4,5|123
196.3542, 0.0000
5.720, 0.000
8.4306, 0.0002
18.963, 0.050
AIC
12.4287, 17.5261
5.2284, 12.2178
BIC
30.9302, 36.0275
23.7298, 30.7193
Log-likelihood
3.786, 1.237
7.386, 3.891
Table 7. D-vine copula testing in bear markets during pre-crisis and post-crisis periods
D-vine
2000–2008
2009–2016
Bear markets
Parameters
SE.
Parameters
SE.
t-copula, clayton
t-copula, clayton
t-copula, clayton
t-copula, clayton
β1,2
94.4415, 0.0480
293.303, 0.056
3.7243, 0.1396
0.132, 0.103
β1,3
71.3509, 0.0955
172.282, 0.076
199.5202, 0.0000
0.860, 0.000
β1,4
2.8300, 0.0001
0.000, 0.000
199.7652, 0.0688
0.941, 0.063
β1,5
4.9670, 0.0000
0.001, 0.001
2.5303, 0.1818
2.523, 0.088
β2,3|1
7.8843, 0.0000
17.143, 0.000
199.4808, 0.0000
0.499, 0.000
β2,4|1
64.4572, 0.0000
482.304, 0.000
199.7846, 0.1164
7.407, 0.080
β2,51
2.0700, 0.2827
0.215, 0.126
5.4687, 0.0444
5.356, 0.090
β3,4|12
6.7095, 0.0000
12.632, 0.000
45.5691, 0.1238
4.600, 0.089
β3,5|12
5.7604, 0.0334
5.838, 0.106
4.3722, 0.0000
502.558, 0.000
β4,5|123
71.6256, 0.0000
225.931, 0.000
199.9049, 0.0001
0.508, 0.030
AIC
2.5875, 12.7461
10.7336, 11.9499
BIC
20.6541, 30.8127
28.8002, 30.0165
Log-likelihood
8.706, 3.627
4.633, 4.025
References
1. Avdulai, K.: The Extreme Value Theory as a Tool to Measure Market Risk. Work-
ing paper 26/2011.IES FSV. Charles University (2011). http://ies.fsv.cuni.cz
2. Behrens, C.N., Lopes, H.F., Gamerman, D.: Bayesian analysis of extreme events
with threshold estimation. Stat. Model. 4, 227–244 (2004)
3. Chaithep, K., Sriboonchitta, S., Chaiboonsri, C., Pastpipatkul, P.: Value at risk
analysis of gold price return using extreme value theory. EEQEL 1(4), 151–168
(2012)
4. Christoﬀersen, P.: Evaluating interval forecasts. Int. Econ. Rev. 39, 841–862 (1998)
5. Einmahl, J.H.J., Magnus, J.R.: Records in athletics through extreme-value theory.
J. Am. Stat. Assoc. 103, 1382–1391 (2008)

The Understanding of Dependent Structures Under the Economic Cycle
589
6. Embrechts, T., Resnick, S.T., Samorodnitsky, G.: Extreme value theory as a risk
management tool. North Am. Actuar. J. 3(2), 30–41 (1999)
7. Ernst, E., Stockhammer, E.: Macroeconomic Regimes: Business Cycle Theories
Reconsidered. Working paper No. 99. Center for Empirical Macroeconomics,
Department of Economics, University of Bielefeld (2003). http://www.wiwi.
uni-bielefeld.de
8. Garrido, M.C., Lezaud, P.: Extreme value analysis : an introduction. Journal de la
Socit Franaise de Statistique, 66–97 (2013). https://hal-enac.archives-ouvertes.fr/
hal-00917995
9. Hamilton, J.D.: Regime Switching Models. Palgrave Dictionary of Economics
(2005)
10. Jang, J.B.: An extreme value theory approach for analyzing the extreme risk of
the gold prices, 97–109 (2007)
11. King, R.G., Rebelo, S.T.: Resuscitating Real Business Cycles. Working paper 7534,
National Bureau of Economic Research (2000). http://www.nber.org/papers/
w7534
12. Kisacik, A.: High volatility, heavy tails and extreme values in value at risk esti-
mation. Institute of Applied Mathematics Financial Mathematics/Life Insurance
Option Program Middle East Technical University, Term Project (2006)
13. Kupiec, P.: Techniques for verifying the accuracy of risk management models. J.
Deriv. 3, 73–84 (1995)
14. Manganelli, S., Engle, F.R.: Value at Risk Model in Finance. Working paper No.75.
European Central Bank (2001)
15. Marimoutou, V., Raggad, B., Trabelsi, A.: Extreme value theory and value
at risk: application to oil market (2006). https://halshs.archives-ouvertes.fr/
halshs-00410746
16. Mierlus-Mazilu, I.: On generalized Pareto distribution. Rom. J. Econ. Forecast. 1,
107–117 (2010)
17. Mwamba, J.W.M., Hammoudeh, S., Gupta, R.: Financial Tail Risks and the Shapes
of the Extreme Value Distribution: A Comparison between Conventional and
Sharia-Compliant Stock Indexes. Working paper No. 80. Department of Economics
Working Paper Series, University of Pretoria (2014)
18. Neves, C., Alves, M.I.F.: Testing extreme value conditions an overview and recent
approaches. Stat. J. REVSTAT 6, 83–100 (2008)
19. Perez, P.G., Murphy, D.: Filtered Historical Simulation Value-at-Risk Models and
Their Competitors. Working paper No. 525. Bank of England (2015). http://www.
bankofengland.co.uk/research/Pages/workingpapers/default.aspx
20. Perlin, M.: MS Regress - The MATLAB Package for Markov Regime Switching
Models (2010). Available at SSRN: http://ssrn.com/abstract=1714016
21. Pickands, J.: Statistical inference using extreme order statistics. Ann. Stat. 3, 110–
131 (1975)
22. Rockafellar, R.T., Uryasev, S.: Conditional value-at-risk for general loss distrib-
utions. J. Bank. Financ. 26, 1443–1471 (2002). http://www.elsevier.com/locate/
econbase
23. Sampara, J.B., Guillen, M., Santolino, M.: Beyond Value-at-Risk: Glue VaR Dis-
tortion Risk Measures. Working paper No. 2. Research Institute of Applied Eco-
nomics, Department of Econometrics, Riskcenter - IREA University of Barcelona
(2013)
24. Taghipour, A.: Banks, stock market and economic growth: the case of Iran. J. Iran.
Econ. Rev. 14(23), 19–40 (2009)

The Impacts of Macroeconomic Variables
on Financials Sector and Property
and Construction Sector Index Returns
in Stock Exchange of Thailand Under
Interdependence Scheme
Wilawan Srichaikul1, Woraphon Yamaka1,2, and Roengchai Tansuchat1,2(B)
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
srichaikul.w@gmail.com, woraphon.econ@gmail.com, roengchaitan@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. This paper investigates the impacts of macroeconomic vari-
ables, namely consumer price index, exchange rate, minimum loan rate
and oil price movement, on the ﬁnancials and the property & construc-
tion stock index return in the Stock Exchange of Thailand (SET). The
monthly data is collected from January 2004 to November 2016, covering
155 observations. We employ a copula based SUR regression as a tool for
this study. Ten copula functions are considered in this regression and the
best copula function is selected based on Akaike’s Information Criterion
(AIC) and Bayesian Information Criterion (BIC). The estimated results
show that Gumbel 270 copula is the most appropriate function for being
the linkage between the marginal distributions of residuals of ﬁnancials
sector and property & construction sector equations. In addition, the
marginal distribution is also tested, and the result shows that normal
distribution is the best ﬁt for the marginal distribution for both ﬁnan-
cials and property & construction equations. Our results suggest that the
exchange rate can exert signiﬁcant impact on both sectors. The depen-
dency parameter also suggests that dependency between ﬁnancials sector
and property & construction sector is negative, and very low dependency,
meaning when the impact of macroeconomic variables in one of these two
sectors, it just has a little eﬀect to another one sector.
1
Introduction
The Stock Exchange of Thailand (SET) is composed of eight main sectors,
namely agro & food industry, consumer product, ﬁnancials, industrials, property
& construction, resources, services, and technology. Among these eight sectors,
the ﬁnancials sector and the property & constrution sector account for the largest
portion of the Stock Exchange of Thailand, with capital market share of 23.97 %
and 12.08 % respectively [16]. The study of Abdelgalil [1] also suggested that
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_42

The Impacts of Macroeconomic Variables
591
these two sectors have a strong relationship since the property & construction
price can be used as an indicator of ﬁnancial stability. Thus, these two sectors
are highly relevant for monetary policy decision making.
Figure 1 shows the movements of ﬁnancials sector and property & construc-
tion sector from 2004 to 2016. We observe that ﬁnancials sector and property
& construction sector have a high volatility over time. There are several factors
aﬀecting these movements of the two sectors; particularly, such macroeconomic
variables as interest rate, exchange rate, oil price, and consumer price index.
From the literature, we found there are many macroeconomic factors that
have impacts on stock price index returns. The study of Suriani et al. [14]
explained that the market value of ﬁrms and the stock prices can be signiﬁ-
cantly aﬀected by multiple factors where exchange rate ﬂuctuation is an impor-
tant one. According to the ﬁnancial theory, the value of ﬁrm should be inﬂuenced
by exchange rates and interest rates. The upward and downward exchange rate
movements may determine the stock prices of the ﬁrms. Moreover, the study of
Jamil and Ullah [8] suggested the foreign exchange rate as another factor that
impacts the stock returns.
The interest rate is also called the cost of capital. Interest rate can be clas-
siﬁed into two types: the savings interest rate and the borrowing interest rate.
By that an increase in the interest rate aﬀects the discount rate, which ulti-
mately decreases the value of a stock. A related explanation is that an increase
in the interest rate makes alternative investment opportunities more attractive.
Speciﬁcally, when the interest rate rises, investors tend to invest less in stock
and more in other investment assets, causing stock prices to fall [3]. Moreover,
in the study of Talla [15] also suggested that inﬂation can aﬀect stock market
either positively or negatively. In addition, the unexpected and expected inﬂation
are also determines the direction of the relationship between stock market and
inﬂation. When demand exceeds supply, ﬁrms tend to increase their prices. This
would increase their earnings, which would lead to an increase in dividends paid
resulting in an increase in demand for the ﬁrms stock and eventually increasing
its stocks value.
Fig. 1. Stock price index of ﬁnancials sector and the property & construction sector in
the Stock Exchange of Thailand Source: Thomson Reuters, 2017

592
W. Srichaikul et al.
Finally, another factor that aﬀects the stock return is the movement of oil
price. Many researchers have studied the relationship between the movement
of oil price and stock market return such as Aydogan [2]; Ghosh and Kanjilal
[5]; Jouini [9] and Pastpipatkul, et al. [12]. They also conﬁrmed a relationship
between stock and oil price. From these literature reviews, we found that stock
market and macroeconomic factors are linked and there are various factors that
aﬀect stock market. Thus, in this study, we will introduce these factors and
investigate the impact of these factors on SET market.
The main purpose of this paper is to examine the impact of macroeconomic
variables on the ﬁnancials sector and property & construction sector in SET
markets. In addition, this paper studies to dependence between ﬁnancials sector
and property & construction sector. Therefore, this study applies copula based
Seemingly Unrelated Regression (SUR) model which was introduced in Past-
pipatkul, et al. [11] as a tool. By using this model, our study can gain more
ﬂexibility from its ability to link the diﬀerent marginal distributions of residuals
of each equation in the model. In particular it makes the model more realistic
and far from the unrealistic assumption like a normal distribution. In the orig-
inal SUR model of Zellner [18], Gaussian distributions were used, but the we
do not want to make this strong assumption. Therefore, copula based model is
consider in this study.
The remainder of this study is organized as follows. Section 2 brieﬂy a
methodologies used in this study, Sect. 3 presents estimation procedures, and
Sect. 4 presents the data used in this study. In Sect. 5, we present the empirical
results. Section 6 present our concludes.
2
Methodology
2.1
Seemingly Unrelated Regression (SUR) Model
The SUR model was introduced by Zellner [18]. It can be viewed as a system of
linear regression equations or multivariate regression equations. The dependent
variable, Yi is diﬀerent in each equation, and the error of equation is assumed
to be correlated. Thus, SUR model can gain eﬃciency or improve estimation by
combining information from diﬀerent equations through the error terms. Con-
sider the structure of SUR model, as it consists of several regression equations,
let us say M equations, we can write the general from as in the following.
Yi = Xiβi + εi
i = 1, . . . , M
(1)
where Yi = (T × 1) vector with element yit, Xi is (T × Ki) matrix of Ki inde-
pendent variables or explanatory variables in equation i = 1, . . . , M and βi is
(K × 1) vector of an estimated parameter in each equation ith. εi is a vector of
the error terms errors in diﬀerent moments of time are independence, but diﬀer-
ent error components at the same moment of time t are, in general, correlated
and also assumed to be correlated across equations. Thus,
E[εitεit | X] = 0
(2)

The Impacts of Macroeconomic Variables
593
whereas E[εitεjt | X] = σij and ε ∼N(0, Σ). Σ is a matrix of non-negative
variance-covariance for M equations, such that
Σ(εtε
′
t) =
⎡
⎢⎢⎢⎣
σ11 σ12
σ1M
σ21 σ22
σ2M
...
...
...
σM1 · · · · · · σMM
⎤
⎥⎥⎥⎦
(3)
For example, consider SUR model with 2 equations, we can write matrix of the
model as
y1t
y2t
	
=
β10
β20
	
+
β11 β12
β21 β22
	
·
x11t x12t
x21t x22t
	
+
ε1t
ε2t
	
,
(4)
Σ(εtε
′
t) =

ε′
1tε1t ε′
1tε2t
ε′
2tε1t ε′
2tε2t
	
=

σ11 σ12
σ21 σ22
	
(5)
In the application study, xt in each equation can be either diﬀerent or the
same. In this study, we consider using a maximum likelihood (ML) estimator
to estimate all unknown parameters in the SUR model, thus the log-likelihood
function of this model can be written as
L =
1

(2π)MT |Σ|
exp

−1
2tr

(εit)′(Σ)−1(εit)

.
(6)
Taking logarithm, we obtain
log L = −(MT
2 ) log(2π) + (T
2 ) log
Σ−1 −
1
2tr

(εit)′(Σ−1)(εit)

,
(7)
2.2
Basic concepts of copula
In this section, we review the theorem of the copula approach which is employed
to improve the SUR model as mention before. The most fundamental theorem,
which describes the dependence in copulas, is the Sklars theorem [13]. Sklar
[13] has proposed the link between the marginal distributions which is possible
to have diﬀerent distributions with the same correlation, but diﬀerent depen-
dence structure. The linkage between the marginal distributions is called copula.
Formally, let H bean M-dimensional joint distribution function of the random
variables xM with marginal distribution function FM. They, there exists the
n-copula C such that for all xM.
H(x1, . . . , xM) = C(F1(x1), . . . , FM(xM))
(8)
where C is copula distribution function of a M-dimensional random variable.
If the marginals are continuous, C is unique. Equation 8 deﬁnes a multivariate

594
W. Srichaikul et al.
distribution function F. Thus, we can model the marginal distribution and joint
dependence separately. If we have a continuous marginal distribution, the copula
can be determined by
C(u1, . . . , uM) = C(F −1
1
(u1), . . . , F −1
M (uM))
(9)
where u is uniform [0, 1]. In the copula approach, it proposes various families
to join the marginal. In this study, we consider 10 classes of copulas namely,
Gaussian, Student-t, Frank, Clayton, Gumbel, Joe, Clayton 90, Clayton 270,
Gumbel 90 and Gumbel 270 copula in Hofert, Machler, and McNeil [6].
3
Estimation of Copula Based SUR Model
In this study, we consider bivariate copula based SUR model to derive the return
in both ﬁnancials and property & construction stock markets. First of all, the
Augmented Dickey-Fuller (ADF) test is employed to check the stationary of our
data series. Note that ML estimation is conducted in this model, therefore it
is important to derive the complete likelihood of this model (see, Pastpipatkul,
et al. [11]). The complete copula based SUR likelihood can derived by
∂2
∂u1∂u2 F(u1, u2) =
∂2
∂u1∂u2 C(F1(u1), F1(u1))
= f1(u1)f2(u2)c(F1(u1), F2(u2))
(10)
where u1 and u2 are the marginals of each equation which assumed to be either
normal or student-t distribution, f1(u1) and f2(u2) are either normal or student-
t density function, and c(F1(u1), F2(u2)) is a probability density function of
bivariate copula. Then, we take a logarithm to transform Eq. 10, we obtain
log L(Θ) =
T

t=1
log {(L(Θ1 |y1t, X1) + L(Θ2 |y2t, X2)+ c(F1(u1), F2(u2))} (11)
where L(Θ1 |y1t, X1) and L(Θ2 |y2t, X2) are the likelihood functions in Eq. 6
and log c(F1(u1), F2(u2) is a bivariate copula density assumed for Gaussian,
Student-t, Frank, Clayton, Gumbel, Joe, Clayton 90, Clayton 270, Gumbel 90
and Gumbel 270 copulas. Finally, we use the ML estimator to maximize logL(Θ)
and obtain the estimated parameters of the model. However, in this study, we
propose diﬀerent marginal and copula families to SUR model, so we select the
appropriate model speciﬁcation using Akaiki Information Criteria (AIC) and
Bayesian Information Criteria(BIC). The lowest AIC and BIC are preferred in
this selection.

The Impacts of Macroeconomic Variables
595
3.1
Macroeconomic Speciﬁcation
PF INCIAL, t = β0 + β1CPIt + β2EXt + β3OIL PRICEt + β4MLRt + εt
(12)
PP ROP CON, t = β0 + β1CPIt + β2EXt + β3OIL PRICEt + β4MLRt + εt
(13)
where PF INCIAL, t is the ﬁnancials stock price index, PP ROP CON, t is the prop-
erty & construction stock price index, CPIt is consumer price index, EXt is the
exchange rate per US. dollar, OILPRICEt is the crude oil price (WTI), MLRt
is the minimum loan rate.
4
Data
4.1
Descriptive Statistics
In this study is conducted using Financials stock price index, Property & Con-
struction price index obtained from Thomson Reuters, Consumer price index
(CPI), Oil price, Exchange rate (EX) and Minimum Loan Rate (MLR) obtained
from Data Stream. The data is monthly from January 2004 to November 2016
covering 155 observations. The descriptive statistics are given in Table 1. It can
be seen that oil price has the highest standard deviation, while CPI has the low-
est. The Jarque-Bera statistic indicates that all series, except exchange rate, are
not normally distributed because it rejects the null hypothesis. The Augmented
Dickey-Fuller (ADF) test at level with none, intercept and trend and intercept
suggest that the value of ADF test at this level of all variables are less than
Mackinnon critical value at 1% level. Therefore, these variables are stationary.
Table 1. Summary statistics
Financials
Property &
construction
CPI
EX
Oil price
MLR
Mean
−0.0047
−0.0027
−0.002
0.0005
−0.0025
−0.0008
Median
−0.0153
−0.0095
−0.0018
0.0021
−0.015
0.0000
Max
0.3363
0.3742
0.0298
0.0524
0.3193
0.0714
Min
−0.1552
−0.1734
−0.0223
−0.0412
−0.238
−0.0715
Std. dev
0.0640
0.0685
0.0054
0.0166
0.1047
0.0170
Skewness
1.1168
1.0995
1.0841
−0.0745
0.4445
0.0027
Kurtosis
7.2433
8.0022
11.584
3.2139
3.1022
9.3472
Jarque-Bera
148.51***
192.83***
506.30***
0.4393
5.1736**
260.1884***
Augmented Dickey-Fuller
None
(−9.73)*** (−9.02)***
(7.54)***
(−10.56)*** (−10.65)*** (−3.80)***
Intercept
(−9.75)*** (−9.01)***
(−8.18)*** (−10.54)*** (−10.63)*** (−3.80)***
Trend and
intercept
(−9.72)*** (−9.06)***
(−8.34)*** (−10.66)*** (−10.69)*** (−3.88)***
Source: Calculation
Note: ***, **, * are signiﬁcant at 1%, 5%, and 10% level, respectively.

596
W. Srichaikul et al.
5
Empirical Results
5.1
Estimation Results for the Financials Sector and Property
and Construction Sector
First of all, we ﬁnd the best ﬁt model to explain the eﬀects of macroeconomic
variables on the ﬁnancials and property & construction stock sectors. By com-
parison the minimum of AIC and BIC of various types of model speciﬁcation. In
this study, we consider ten copula functions, namely Gaussian, Student-t, Frank,
Clayton, Gumbel, Joe, Clayton 90, Clayton 270, Gumbel 90 and Gumbel 270
copulas, to join the marginal distributions of ﬁnancials and property & construc-
tion equations in SUR system. The assumption for each marginal distribution
that we made here is either normal or student-t distribution. Therefore, we have
40 model speciﬁcations for the model selection. The results of AIC and BIC of
model speciﬁcations are shown in Table 2.
From Table 2, we observe that among the trial runs of several alternative
marginal distribution and copula functions for SUR model, it is evident that
given normal distributions for both the ﬁnancials and property & construction
equations and joint distribution by rotated Gumbel 270-degree copula present
the lowest AIC and BIC when compared with the other speciﬁcations. Rotated
Gumbel 270-degree copula indicates that the dependence distribution between
ﬁnancials sector and property & construction sector is likely to have tail negative
correlation rather than positive correlation, meaning that it may has a negative
correlation between both sectors in the market downturn. In addition, we believe
that the copulas could improve the eﬃciency of SUR, we also try to compare
the results between the copula based SUR and the conventional SUR model.
According to the results shown in Table 2, our result shows the superiority of
copula based SUR model in this study. The estimated results from the best ﬁt
speciﬁcation is shown in Table 3.
Table 3 shows the results of the best ﬁt model. We ﬁnd that the eﬀect of
exchange rate is statistically signiﬁcant at 1% level and it has a negative impact
on the ﬁnancials and property & construction stocks. Meanwhile, other vari-
ables are not statistically signiﬁcant to explain the ﬁnancials and property &
construction stock markets.
The exchange rate is signiﬁcant for negative relationship with sector indices
return. The rational reason to support the negative relationship between foreign
exchange rate risk and sector indices return is demonstrated by the eﬀect of the
size of foreign currency in denominating asset and liabilities in balance sheet of
companies listed in the same industry. The ﬂuctuation in unanticipated move-
ment of foreign currency directly aﬀects the ﬁrm’s balance sheet by creating
transaction gain or losses based on the net foreign exposure.
We ﬁnd that the minimum loan rate is insigniﬁcant. This is in consonance
with the ﬁnding in a study by Wisudtitham [17] they also suggested that interest
risk not the determinant on the sector indices return in Thailand market, because
the big ﬁrms will have more borrowing alternatives than smaller ﬁrms and could
seek out cheaper rates. Smaller ﬁrms will not have access to these alternatives,

The Impacts of Macroeconomic Variables
597
Table 2. AIC and BIC criteria for model choice
Copula
Marginal distributions
Normal/
Normal/
Student-t/
Student-t/
normal
student-t
normal
student-t
Gaussian
1125.437
1165.085
1130.338
1173.036
1122.424
1165.122
1147.294
1193.042
Student-t
1132.871
1172.519
1127.494
1170.192
1120.480
1163.178
1157.121
1202.869
Clayton
1110.250
1149.898
1101.094
1143.792
1095.549
1138.247
1148.322
1194.070
Gumbel
1111.316
1150.964
1121.928
1164.626
1112.132
1154.830
1136.874
1182.622
Frank
1103.981
1143.629
1117.950
1160.648
1111.536
1154.234
1131.711
1177.459
Joe
1063.783
1103.432
1084.691
1127.389
1074.528
1117.226
1102.546
1148.294
clayton90
957.0425
996.6906
983.5725
1026.270
978.1121
1020.810
1004.682
1050.430
Clayton 270 957.0969
996.7450
983.5741
1026.272
978.230
1020.928
1004.682
1050.43
Gumbel90
933.4206
973.0687
958.4369
1001.135
953.861
996.559
978.4022
1024.150
Gumbel 270 932.8928
972.5409
958.0118
1000.71
952.8069
995.5049
977.4244
1023.172
SUR
AIC
1082.238
BIC
1033.662
Source: Calculation
making them more vulnerable to interest rate changes. This means that a big
ﬁrm will have less sensitive to change in the loan rate. Thus, we can conclude
that the ﬁnancials sector and property & construction sector are not be aﬀected
by minimum loan rate since these two sectors contain many big ﬁrms which are
not sensitive to this loan rate.
The oil price is also not signiﬁcant because Thai government has subsidized
the energy consumption including liqueﬁed petroleum gas (LPG), natural gas
for vehicles (NGV), diesel, electricity and biofuel blends [7]. Hence, we expect
that the eﬀect of oil price is limit to both sectors. Then, consider the consumer
price index, it is also insigniﬁcant. The same ﬁnding was found in the study of
Limpanithiwat and Rungsombudpornkul [10] who also suggested that there is no
apparent relationship between inﬂation and stock prices in Thailand. They men-
tioned that investors have diﬀerent perspective toward the inﬂuence of consumer
price index on stock prices in Thailand.

598
W. Srichaikul et al.
Table 3. Estimation of copula based SUR for the ﬁnancials sector and property &
construction sector
Financials
Copula based SUR
Coeﬃcient
Std. error
Intercept
0.00516
0.00413
CPI
0.41847
0.93606
EX
−2.07767*** 0.26916
OIL PRICE
0.003573
0.04928
MLR
−0.16595
0.26644
Property & construction
Intercept
0.00083
0.00475
CPI
−0.37714
0.99191
EX
−2.38527*** 0.27746
OIL PRICE
0.07363
0.05054
MLR
−0.40856
0.27262
Copula dependence −1.10000*** 0.09681
Source: Calculation
Note: ***, **, * are signiﬁcant at 1%, 5%, and
10% level, respectively.
Finally, our model also provides a result of dependence between the ﬁnan-
cials and property & construction equations. The dependency between these two
sectors is found to join by rotated Gumbel 270-degree copula. We can see that
the estimated dependence is −1.1000, which corresponds to Kendalls tau coef-
ﬁcient (−0.0909), thus indicating a negative correlation between the ﬁnancials
sector and property & construction sector returns meaning when the impact of
macroeconomic variables in one of these two sectors, it just has a little eﬀect to
another one sector.
6
Conclusion
In this paper, we employed copula based SUR to investigate the eﬀect of macro-
economic variables, consisting of consumer price index, exchange rate, minimum
loan rate and oil price movement, on the ﬁnancials and the property & construc-
tion indices in the Stock Exchange of Thailand (SET). Among the trial runs,
the results show that the model given normal distributions for both the ﬁnan-
cials and property & construction equations and joint distribution by Gumbel
270 copula is the most appropriate for investigating and explaining our problem.
According to our estimated results, we ﬁnd that consumer price index, minimum
loan rate and the movement of oil price do not signiﬁcantly aﬀect both sectors.
In the case of oil price, it is very important for investors to monitor since the
Thai Government so far has subsidized the oil price in order to prevent any

The Impacts of Macroeconomic Variables
599
impacts from the shock of world oil price [7]. Moreover, Thai government should
control exchange rate to have less ﬂuctuate that will increase the conﬁdence of
investor. In addition, we also ﬁnd very low dependency and likely to have tail
negative correlation between ﬁnancials and property & construction returns.
References
1. Abdelgalil, E.: Relationship between real estate and ﬁnancial sectors in Dubai
economy. J. Prop. Res. 18, 1–18 (2005)
2. Aydogan, B.: Crude oil price shocks and stock returns: evidence from Turkish
stock market under global liquidity conditions. Int. J. Energy Econ. Policy 5(1),
54 (2015)
3. Forson, J.A., Janrattanagul, J.: Selected macroeconomic variables and stock mar-
ket movements: empirical evidence from Thailand (2014)
4. Genest, C., MacKay, J.: The joy of copulas: bivariate distributions with uniform
marginals. Am. Stat. 40(4), 280–283 (1986)
5. Ghosh, S., Kanjilal, K.: Co-movement of international crude oil price and Indian
stock market: evidences from nonlinear cointegration tests. Energy Econ. 53, 111–
117 (2014)
6. Hofert, M., Machler, M., McNeil, A.J.: Likelihood inference for Archimedean cop-
ulas in high dimensions under known margins. J. Multivar. Anal. 110, 133–150
(2012)
7. IISD: A citizens′ guide to energy subsidies in Thailand. The International Institute
for Sustainable Development (2013)
8. Jamil, M., Ullah, N.: Impact of foreign exchange rate on stock prices. IOSR J. Bus.
Manag. (IOSR-JBM) 7(3), 45–51 (2013)
9. Jouini, J.: Return and volatility interaction between oil prices and stock markets
in Saudi Arabia. J. Policy Model. 35, 1124–1144 (2013)
10. Limpanithiwat, K., Rungsombudpornkul, L.: Relationship between inﬂation and
stock prices in Thailand (2010)
11. Pastpipatkul, P., Maneejuk, P., Wiboonpongse, A., Sriboonchitta, S.: Seemingly
unrelated regression based copula: an application on thai rice market. In: Causal
Inference in Econometrics, pp. 437–450. Springer International Publishing (2016)
12. Pastpipatkul, P., Yamaka, W., Sriboonchitta, S.: Co-movement and dependency
between New York stock exchange, London stock exchange, Tokyo stock exchange,
oil price, and gold price. In: International Symposium on Integrated Uncertainty
in Knowledge Modelling and Decision Making, pp. 362–373. Springer International
Publishing, October 2015
13. Sklar, M.: Fonctions de r´epartition `a n dimensions et leurs marges. Univ. Paris 8,
229–231 (1959)
14. Suriani, S., Kumar, M.D., Jamil, F., Muneer, S.: Impact of exchange rate on stock
market. Int. J. Econ. Financ. 5(IS), 385–388 (2015)
15. Talla, J.T.: Impact of macroeconomic variables on the stock market prices of the
Stockholm stock exchange (OMXS30). International Business School (2013)
16. The Stock Exchange of Thailand: SET Index Series (2017). https://marketdata.
set.or.th/mkt/sectorialindices.do
17. Wisudtitham, K.: The Eﬀect of Market, Interest Rate, and Exchange Rate Risks on
Sector Indices Return: Evidence from the Stock Exchange of Thailand. Thammasat
University, Bangkok (2013)
18. Zellner, A.: An eﬃcient method of estimating seemingly unrelated regressions and
tests for aggregation bias. J. Am. Stat. Assoc. 57(298), 348–368 (1962)

Generalize Weighted in Interval Data
for Fitting a Vector Autoregressive Model
Teerawut Teetranont1(B), Woraphon Yamaka1,2, and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
teetranont@gmail.com, woraphon.econ@gmail.com, songsakecon@gmail.com
2 Puey Ungphakorn Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
Abstract. This paper employ VAR model to analyse and investigate
the relationship among oil, gold, and rubber prices. A convex combi-
nation approach is proposed to obtain appropriate value of the interval
data in VAR model. The construction of interval VAR model based on
the convex combination method for the analysis of their forecast per-
formance are also introduced and discussed via the simulation study, as
well as comparing the performance with conventional center method. To
illustrate the usefulness of the proposed model, an empirical application
on a weekly sample of commodity price is provided. The results show the
performance of our proposed model and also provide some relationship
between commodity prices.
1
Introduction
Since Billard and Diday [1,3] proposed a linear regression to interval-valued
data,nmely, they propose to regress the centers of the intervals of the depen-
dent variable on the centers of the intervals of the regressors. The model has
been considered and applied in various studies during the last decade. The main
advantage of interval data is that it can capture uncertain characteristics that
cannot be fully described with a real number. In several situations, the available
information is formalized in terms of intervals. Therefore, considering the mini-
mum and maximum values of the interval-valued data may arise more complete
insight about the phenomenon than considering the single-valued data [10,12].
Blanco-Fernndez, et al. [2] suggested that interval data are useful to model vari-
ables with uncertainty in their formalization, due to an imprecise observation or
an inexact measurement, ﬂuctuations, grouped data or censoring.
So far the literatures have been proposed to deal with interval data such
as Center and MinMax methods of Billard and Diday [1], Center and Range
method of Neto and Carvalho [6], and model M by Blanco-Fernndez, Corral,
Gonzlez-Rodrguez [4]. These methods aim to construct the model without tak-
ing the interval as a whole. However, we expect that that these methods may
not robust to explain the real behavior of the interval data. Especially, a center
method, the mid-point value of data (weight = 0.5) is proposed to be a represen-
tative of the interval data and this may lead to the misspeciﬁcation of the model.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_43

Generalize Weighted in Interval Data for Fitting a VAR Model
601
Therefore, it will be of great beneﬁt to relax this assumption of mid-point and
assign appropriate weights between intervals. Thus in this study, a convex com-
bination approach which proposed by Chanaim et al. [5] is employed to obtain
the appropriate weights.
The main interest of this paper lies in the system equation models for the
interval data. To our best knowledge, there have been no studies done on system
equation with interval data before. Hence, we extend the linear regression of
Billard and Diday [1], Neto and Carvalho [6], Fernndez, Corral, Gonzlez-
Rodrguez [4] and Chanaim et al. [5] by proposing Vector Autoregressive(VAR)
model for the Interval data context. Therefore, this become one of the main con-
tribution of our study by proposing an Interval VAR with convex combination
method. Furthermore, to show the performance of our model in the real applica-
tion study, this is an additional study to the empirical literature, which focused
on the relationships among gold, crude oil prices and rubber prices in commod-
ity markets. Investigating these commodity prices has been attracting increasing
interests among researchers, See Sang et al. [11], Lakshmi and Visalakshmi [8],
Li and Yang [9] and Gupta et al. [7].
The commodities especially oil, gold and rubber have an important role in
economics because of they are the main components of many common goods
in the industry. The globalization causes these commodity prices become more
integrated. It was believed that the performance of commodity prices would be
aﬀected by each others. Nevertheless, alongside this increased interest in futures
trading in the commodity market, the data was normally collected and analyze
d from single point type which is viewed as classical one. Due to an explosive
growth in methods for dealing with data, the data are now not necessarily col-
lected in the form of single point anymore. In contrast to earlier research, we thus
replace a single value of commodity prices with the range of high and low histor-
ical weekly data into the VAR model. Speciﬁcally, using interval data instead of
open, closing or average prices can improve the estimation. Incorporating inter-
val regression into traditional ﬁnancial econometrics improves eﬃciency in model
estimations and forecasting.
As a consequent, in this paper two contributions are made. First, VAR model
are adapted to Interval data characterized by convex combination method and
Maximum likelihood techniques are used to estimate them. Second, we will try
to identify if there is a causation relationship between the price of crude oil,
gold, and rubber by using an Interval VAR model.
The paper is organized as follows. This paper is organized into ﬁve main
sections including this introduction. Section 2 provides a literature review and
research methodology. Section 3 is to put forward a simulation study. Section 4 is
provide a data description and empirical results of the relationship among crude
oil, gold, and rubber prices, while Sect. 5 summarizes and gives some concluding
remarks.

602
T. Teetranont et al.
2
Review and Research Methodology
In this section, we will brieﬂy review a conventional center methods which were
proposed Billard and Diday [1]. We then present the convex combination method
to ﬁtting a vector autoregressive model to interval-valued data.
2.1
Review of Center Methods and Convex Combination Method
Billard and Diday [1] proposed this method and applied in the linear regression
model. The main idea of this method is the estimate slope parameter is based
on the center of the interval data. Let Yi = [Y i, Y i], i = 1, · · · , T where Y i is
the maximum observed values of Yi and Y i the minimum observed values Yi of
Y i. Thus, we can compute the center of the intervals as
Yc = Y i + Y i
2
(1)
However, the center method may lead to the misspeciﬁcation problem since the
midpoint of the intervals might not be an appropriate representative value of the
intervals. To overcome the misspeciﬁcation of the center method, Chanaim et al.
[5] generalize the center method using convex combination approach, which can
be rewritten in the simplest form as
Y w = wY + (1 −w)Y , w ∈[0, 1]
(2)
where w is the weight parameter of the interval data with values [0, 1]. The
advantage of this method lies in the ﬂexibility to assign weights in calculating
the appropriate value between intervals.
2.2
Vector Autoregressive (VAR) Model for Interval-Valued Data
Consider the Y th order autoregression for the k dimensional time series vector
(y1t, · · · , ykt)′, t = 1, · · · , T; this, for example, can be written in the general
form as follows:
Yt = A0 +
p

i=1
AiYt−i + ut,
(3)
where yt is the vector of endogenous variables A0 is k × 1 the vector of intercept
term; Ai are k × k matrix of autoregressive coeﬃcient and ut is the error term
with normal distribution ut ∼N(0, Σ)). Thus, we can extend (3) as follows:
⎡
⎢⎣
y1,t
...
yk,t
⎤
⎥⎦=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
A01 + A11y1,t−1 + · · · + A1py1,t−p + u1,t
...
A0k + Ak1yk,t−1 + · · · + Akpyk,t−p + uk,t
(4)

Generalize Weighted in Interval Data for Fitting a VAR Model
603
In this study, the convex combination method is applied to V AR(p) model for
interval-valued data, thus we can rewrite (3) as
Y w
t
= A0 +
p

i=1
AiY w
t−i + ui,
(5)
where
Y w
t
=
⎡
⎢⎣
Y w
1,t
...
Y w
k,t
⎤
⎥⎦=
⎡
⎢⎢⎣
w1y1,t + (1 −w1)y1,t
...
wkyk,t + (1 −wk)yk,t
⎤
⎥⎥⎦
(6)
Thus, for example, a bivariate V AR(1) model equation by equation has the form
Y w
1,t
Y w
2,t

=

A01 + A11Y w
1,t−1 + A11Y w
2,t−1 + u1,t
A02 + A21Y w
1,t−1 + A22Y w
2,t−1 + u2,t
(7)
In the estimation technique, we employ the Maximum likelihood estimator to
estimate all unknown parameters in the Interval VAR(p) model. Due distur-
bances are normal distributed, the conditional density is multivariate normal
distributed:
Y w
t
| Y w
t−1, · · · , Y w
t−p ∼N(A0 +
p

i=1
AiY w
t−i, Σ),
(8)
and the condition density of Interval VAR(p) model becomes:
f(A, Σ, c, w |Y ) = (2π)(−k/2)Σ−11/2

−

1
2Σ(Y w
t −c +
p

i=1
AiY w
t−i)
2
(9)
where W is the vector of weight wk, see (2). Thus the likelihood function which
is the product of each term of densities for t = 1, · · · , T can be derived as
L(θ, Y ) =
T

i=1
f(A, Σ, A0, w | Y w)
(10)
where θ = {A, Σ, A0, W} is the parameter set of Interval VAR(p) model. Then,
maximizing (11),
θ = arg min L(θ, Y w)
(11)
We obtained the estimated parameter of this model, θ.
3
Simulation Result
In this section, a simulation study has been carried out in order to evaluate the
performance of convex combination method in Interval VAR model. Speciﬁcally,
the simulation study aims at oﬀering a better insight into the eﬃciency of the
method and its accuracy to capture the optimum weight in the intervals. For
this purpose, we consider the following three cases of weight in the intervals:

604
T. Teetranont et al.
Case 1: Center of interval data: w1 = 0.5, w2 = 0.5
Case 2: Deviate from center of interval data: w1 = 0.3, w2 = 0.3
Case 3: Extremely deviate from center of interval data: w1 = 0.1, w2 = 0.9
The general simulation scheme goes as follow. We perform 100 replications for
bivariate Interval VAR(1) model using sample size n = 50 and n = 100 for all the
three cases. In each simulation, we draw the initial values of the series from a
uniform distribution with minimum value 0 and maximum value. Likewise, the
error are drawn randomly from a multivariate normal distribution with mean 0
and covariance matrix,
Σ =

1 0.5
0.5 1

.
Based on these initial values and innovations we simulate the data forward in
time until we have 100 sample size. In this simulation study, we generate random
data sets from these models speciﬁcation:
w1y1,t + (1 −w1)y1,t
w2y2,t + (1 −w2)y2,t

=

2 + 1(w1y1,t−1 + (1 −w1)y1,t−1) −1(w2y2,t−1 + (1 −w2)y2,t−1) + u1,t
0.4 + 0.2(w2y2,t−1 + (1 −w2)y2,t−1) + 0.7(w1y1,t−1 + (1 −w1)y1,t−1) + u2,t
The resulting 100 independent sets of parameter estimates for each case is eval-
uated by their mean square error (MSE) and bias.
Next, we include a comparison in the simulation study in order to evaluate
the performances of the convex combination method to center method in the
Interval VAR context. The comparison between methods is accomplished by
bias and MSE. The simulation results are summarized in Table 1.
According to Tables 1 and 2, we consider both small (n = 50) and large
(n = 100) sample sizes. We can state that the Interval VAR parameters esti-
mated using the convex combination method performs well in this simulation
study and both Bias and MSE tend to go to zero when the sample size increases
in all cases. We then compare the convex combination method to the center
method and we observe that both case 1 and 2 have competitive performances.
The Bias and MSE of the convex combination method exhibit a better perfor-
mance in some estimated parameters thus we cannot conclude that the convex
combination method outperforms center method with regard to the Bias and
MSE. However, Tables 1 and 2 also displays some interesting diﬀerences. The
Bias and MSE of the convex combination method are now less than that of cen-
ter method. This prompts the questions: What has caused this relative change
in Bias and MSE? To answer this question, Tables 1 and 2 provide a diﬀerent
cases of the weight of intervals and we can observe that when the true weight
extremely deviate from center the Interval VAR parameters estimated using the
convex combination method performs signiﬁcantly better than center method.
Thus, we conclude that the convex combination method outperforms the center
method with regard to the Bias and MSE for the Monte Carlo experiments with
the weight of intervals extremely deviate from the mid-point of intervals.

Generalize Weighted in Interval Data for Fitting a VAR Model
605
Table 1. Mean results of Bias and MSE on the validation set based on 100 independent
repetitions.
Parameter
Convex combination method
Center method
N = 100
TRUE Bias
MSE
Bias
MSE
Case 1: center
A01
2
−0.1716
0.0579
0.6941
0.5329
A11
1
−0.0038
0.0004
−0.0037
0.0004
A12
−1
0.0043
0.0028
−0.0091
0.0027
A02
0.4
−0.1605
0.0424
−0.0021
0.0501
A21
0.2
−0.0008
0.0003
−0.0011
0.0003
A22
0.7
−0.0102
0.002
−0.0215
0.0038
Σ11
1
−0.0077
0.0235
1.3132
1.8594
Σ12
0.5
−0.0047
0.0124
0.6189
0.4697
Σ22
1
−0.0395
0.0233
1.2591
1.7124
w1
0.5
−0.0922
0.0296
w2
0.5
0.0207
0.0073
Case 2: deviate from center
A01
2
0.78275
0.6559
−0.0517
0.0409
A11
1
0.0015
0.0005
−0.0043
0.0005
A12
−1
−0.0154
0.0038
−0.0802
0.0715
A02
0.4
−0.1194
0.0477
0.4556
0.2503
A21
0.2
0.0039
0.0052
0.0018
0.0004
A22
0.7
−0.0276
0.0037
−0.019
0.0026
Σ11
1
0.0229
0.0224
1.4639
2.2718
Σ12
0.5
−0.0265
0.01429
0.5731
0.3917
Σ22
1
−0.0284
0.0236
1.3221
1.8626
w1
0.3
0.2165
0.0524
w2
0.7
−0.1041
0.0203
Case 3: extremely deviate from center
A01
2
−1.1631
1.4515
−3.8839
15.3848
A11
1
−0.0001
0.0001
0.0035
0.0002
A12
−1
−0.0263
0.001
−0.0319
0.0019
A02
0.4
0.3685
1.3798
4.882
24.1735
A21
0.2
−0.0007
0.0003
−0.0022
0.0002
A22
0.7
0.006
0.0015
0.0201
0.0015
Σ11
1
0.1209
0.0513
7.4393
56.4107
Σ12
0.5
−0.1078
0.0708
−3.5829
13.2447
Σ22
1
3.5238
1.3012
5.2646
28.5008
w1
0.1
−0.4104
0.2185
w2
0.9
0.1429
0.0239
Source: Calculation

606
T. Teetranont et al.
Table 2. Mean results of Bias and MSE on the validation set based on 100 independent
repetitions.
Parameter
Convex combination method
Center method
N = 100
TRUE Bias
MSE
Bias
MSE
Case 1: center
A01
2
−0.0072
0.0181
0.7798
0.6961
A11
1
−0.0314
0.0022
−0.0009
0.0011
A12
−1
0.0122
0.0038
−0.0104
0.0061
A02
0.4
0.1359
0.0422
0.2341
0.1721
A21
0.2
−0.0112
0.0013
0.0042
0.0016
A22
0.7
−0.0287
0.0145
−0.0165
0.0059
Σ11
1
−0.1043
0.0474
1.2754
1.8255
Σ12
0.5
0.0723
0.1054
0.6103
0.4828
Σ22
1
−0.1138
0.1012
1.3367
2.0085
w1
0.5
0.0074
0.0668
w2
0.5
0.0085
0.0265
Case 2: deviate from center
A01
2
0.8987
0.8492
2.1646
4.808
A11
1
0.0298
0.0043
−0.0061
0.001
A12
−1
−0.1083
0.0294
−0.0281
0.0068
A02
0.4
−0.2053
0.1555
0.112
0.1291
A21
0.2
0.0236
0.0025
−0.0002
0.001
A22
0.7
−0.0574
0.0096
−0.0353
0.006
Σ11
1
0.1737
0.1851
1.5744
2.7681
Σ12
0.5
0.2809
0.3947
0.751
0.7344
Σ22
1
−0.0621
0.0223
1.3744
2.1757
w1
0.3
0.1713
0.0549
w2
0.7
−0.2013
0.0643
Case 3: extremely deviate from center
A01
2
−1.1496
1.4172
−4.3242
19.6237
A11
1
0.0118
0.0201
−0.0127
0.001
A12
−1
0.0162
0.0569
−0.0241
0.0022
A02
0.4
3.8648
15.8903
5.1518
27.414
A21
0.2
−0.0026
0.0118
0.0071
0.0008
A22
0.7
−0.0026
0.0076
0.0096
0.0022
Σ11
1
0.0611
0.0832
7.1968
53.9514
Σ12
0.5
−0.0824
0.1496
−3.3917
12.4925
Σ22
1
3.1016
12.2641
5.2377
28.7605
w1
0.1
−0.2347
0.2681
w2
0.9
0.1512
0.0368
Source: Calculation

Generalize Weighted in Interval Data for Fitting a VAR Model
607
4
Empirical Results
4.1
Data Description
In this study, the data set consists of the Brent oil price and COMEX gold
price, and Rubber sheet spread 3 (RSS3) rubber price for the period from 9
April 2013 to 6 October 2016, covering 785 observations. Interval data is the
most important issue in examining the interaction among these three commodity
prices. Therefore, we have considered the daily minimum and maximum of these
prices and the data were collected form Thomson Reuters DataStream, Faculty
of Economics, Chiang Mai University.
Due to, the prices are non-stationary, they exhibit an upward and downward
trend. Therefore we need to transform our data to make them stationary. To pre-
serve the interval format, we calculate daily returns with respect to the previous
Fig. 1. Lower-upper limits of expanded and centered returns

608
T. Teetranont et al.
Table 3. Summary statistics.
rOil
(Max)
rOil
(Min)
rGold
(Max)
rGold
(Min)
rRubber
(Max)
rRubber
(Min)
Mean
0.0136
−0.0151
0.0058
−0.0062
0.0092
−0.0104
Median
0.0088
−0.0115
0.0045
−0.0053
0.0073
−0.0082
Maximum
0.1308
0.0529
0.0734
0.0366
0.0970
0.0511
Minimum
−0.0747
−0.1197
−0.0236
−0.1200
−0.0506
−0.1000
Std. dev.
0.0216
0.0194
0.0097
0.0109
0.0163
0.0161
Skewness
1.4952
−1.2394
1.1532
−2.1282
0.6956
−0.8222
Kurtosis
8.0826
6.5808
7.3102
20.7835
6.0319
6.0495
Jarque-Bera
1136.02
619.57
780.63
10922.73
363.50
392.13
ADF-test(prob.)
0
0
0
0
0
0
Source: Calculation
day weighted average price, that is,
rmax,t = ymax,t −yavg,t−1
yavg,t−1
rmin,t = ymin,t −yavg,t−1
yavg,t−1
where ymin,t, ymax,t and yavg,t−1 are minimum, maximum, and average price of
our intervals. Figure 1 illustrates our interval returns.
The Table 3 shows the descriptive statistics of our intervals. The skewness
of minimum values of rOil, rGold, and rRubber are negative, while a positive
skewness are shown in their maximum values. For the excess kurtosis statistics,
all of variables in this study are positive, thereby indicating that the distribu-
tions of returns have larger, thicker tails than the normal distribution. Similarly,
the Jarque-Bera tests also conﬁrm that all data series do not follow a normal
distribution. In addition, the result of the Augmented-Dickey-Fuller test shows
that t all series data are stationary.
4.2
Empirical Results
When conducting VAR model we need the optimal number of lags. To determine
how many lags to use in the Interval VAR model, two selection criteria can be
used. In this study, we employ Akaike information criterion (AIC) and Bayesian
information criterion (BIC) to ﬁnd an appropriate number of lag lengths and
the minimum value of AIC and BIC is preferred. The result in Table 4 reveal
that the AIC and BIC for lag = 1 are provided lowest values when compare to
the others. Therefore, in this study, we choose the lag length p = 1 for Interval
VAR model.
Next, the interval VAR parameters estimated using the convex combination
and center methods are compared to show the performance of our model in
real application study. In this study, we investigate the relationship among gold,
oil and rubber prices and the results are shown in Table 5. The comparison of

Generalize Weighted in Interval Data for Fitting a VAR Model
609
Table 4. Interval VAR Lag length criteria
Lag AIC
BIC
1
−36457.28∗−36359.32∗
2
−38274.56
−37787.34
3
−38219.77
−37524.22
4
−38233.62
−37329.16
5
−38107.25
−36988.25
Source: Calculation
Table 5. Estimation results for Interval VAR(1)
Convex combination method Center method
Parameter
SE
Parameter SE
rOil
A01
−0.0283∗∗∗0.0041
−0.0056
0.0006
rOil(t-1)
0.5914
0.3546
0.2316∗∗∗
0.0348
rGold(t-1)
0.517
0.5979
0.1325∗
0.08
rRubber(t-1) 0.3825∗
0.1509
−0.1334
0.0899
rGold
A02
−0.3318∗∗∗0.0014
−0.0001
0.0003
rOil(t-1)
2.5527∗∗∗
1.0241
−0.0058
0.0176
rGold(t-1)
1.2752
2.9691
0.2843∗∗∗
0.0404
rRubber(t-1) 0.9009
1.0028
−0.0748
0.0453
rRubber A03
3.3712∗∗∗
0.2665
−0.0001
0.0002
rOil(t-1)
8.7365∗∗
3.4424
0.0425∗∗
0.0136
rGold(t-1)
−1.4894
8.3092
0.0485∗∗∗
0.0313
rRubber(t-1) −1.0574
1.6341
−0.0216
0.0351
w1
0.8001∗∗∗
0.0024
w2
0.1041∗∗∗
0.0003
w3
0.4515∗∗∗
0.0036
LL
18249.4
7504.75
AIC
−36457.28
−14967.5
BIC
−36359.32
−14869.55
methods is based on AIC and BIC which are the measure of model ﬁt. We
learned that Interval VAR with convex combination method has a lowest AIC
and BIC which indicate that the results from the convex combination method is
more prefer than the center method.
Consider, the estimated parameter result obtaining from Interval VAR with
convex combination we can see that rOil is signiﬁcantly driven by rRubber, an 1%
change in the rRubber will change the rOil by 0.3825% in the same direction.
Due to the fact that synthetic rubber is manufactured from the crude oil we
expect that the decrease in natural rubber price could make synthetic rubber

610
T. Teetranont et al.
less attractive and thereby lowering the oil price. For the case of Gold price, the
reaction of rGold is positive in ﬁrst lag period of rOil. The main reason behind
this relation is that prices of crude oil partly account for inﬂation. If gasoline,
which is produced by oil, is more expensive, transport goods price will go up
and thereby increasing an inﬂation. Then, gold price tend to appreciate with
inﬂation rising. So, an increase in the price of crude oil can, eventually, translate
into higher gold price. For rubber price, the reaction of the rRubber are positive
in the ﬁrst lag period of rOil, and the shock coeﬃcients is statistically signiﬁcant
at 5% level. It is inconsistent with the expectation whereby the rubber price have
been eﬀected by the oil price due the fact that synthetic rubber is manufactured
from the crude oil, thus crude prices drop, the rates of synthetic rubber will also
drop and put high pressure to rubber price. As a result, natural rubber prices
will drop in tandem with crude oil prices.
4.3
Impulse Response
The impulse response of Interval VAR model shows the response to one standard
deviation shock in the error terms of other variables and the results are plot
in Fig. 2. The X axis shows the time period and the Y shows the shock in
the movement trend. In the left vertical panel, it displays the impulse response
function for the changes in three commodity prices to a shock of oil price. The
feedback of oil shock diﬀers considerably among commodity prices. We observe
that the shock in oil has a great and persistent positive eﬀects on rubber and
its price. It, then, falls sharply and reaches the steady state within 3 days. For
Fig. 2. Impulse response function

Generalize Weighted in Interval Data for Fitting a VAR Model
611
response in gold, it creates a small positive response to gold price. In the middle
vertical panel of Fig. 2, it displays the impulse response function for the changes
in three commodity prices to a shock of gold price. In this regime, a positive
response in oil, gold and gold are obtained whereas the response of its price
has larger response than the others. Lastly, the right vertical panel displays the
impulse response function for the changes in three commodity prices to a shock
of rubber price. The interesting results are obtained since the response of rubber
prices to the shock of its price is converge to their equilibrium within 2.5 days.
These results indicate that there is a long run relationship among rubber, gold
and oil prices; and their own prices.
5
Concluding Remarks
The main interest of this paper lies in the system equation models for the inter-
val data. So far the literature had been focusing on the linear regression model
with interval data. To our best knowledge, there have been no studies done on
system equation with interval data before. Hence, we extend the linear regres-
sion to system equation VAR model. Thus, we propose a new estimation method
of Interval-valued data in the VAR model. We introduce a convex combination
method, which proposed in Chanaim et al. [5], to Interval VAR model. The
advantage of the convex combination method lies in the ﬂexibility to assign
weights in calculating the appropriated point valued data in the intervals. To
show the performance of our purposed model, we have carried both simulation
study and application study. In the simulation study, our results show the supe-
rior of the convex combination method in Interval VAR model when the weight
of intervals is given to extremely deviate from the mid-point of intervals. In the
application study, we focus on the relationships among gold, crude oil prices and
rubber prices in commodity markets. The results show that there exhibit some
signiﬁcant relationship among oil, gold and rubber prices in the interval context.
When we compare Interval VAR with convex combination method and center
method in the real application, we ﬁnd that the convex combination method is
outperform the convention one.
References
1. Billard, L., Diday, E.: Regression analysis for interval-valued data. In: Data Analy-
sis, Classiﬁcation, and Related Methods, pp. 369–374. Springer International Pub-
lishing (2000)
2. Blanco-Fernndez, A., Garca-Brzana, M., Colubi, A., Kontoghiorghes, E.J.: Exten-
sions of linear regression models based on set arithmetic for interval data. arXiv
preprint arXiv:1210.5881 (2012)
3. Billard, L., Diday, E.: Symbolic regression analysis. In: Classiﬁcation, Clustering,
and Data Analysis, pp. 281–288(2002)
4. Blanco-Fernndez, A., Corral, N., Gonzlez-Rodrguez, G.: Estimation of a ﬂexible
simple linear model for interval data based on set arithmetic. Comput. Stat. Data
Anal. 55(9), 2568–2578 (2011)

612
T. Teetranont et al.
5. Chanaim, S., Sriboonchitta, S., Rungruang, C.: A convex combination method for
linear regression with interval data. In: Integrated Uncertainty in Knowledge Mod-
elling and Decision Making: 5th International Symposium, IUKM 2016, Da Nang,
Vietnam, 30 November–2 December 2016, Proceedings 5, pp. 469–480. Springer
International Publishing (2016)
6. Neto, E.D.A.L., de Carvalho, F.D.A.: Constrained linear regression models for sym-
bolic interval-valued variables. Comput. Stat. Data Anal. 54(2), 333–347 (2010)
7. Gupta, R., Kean, G.J.S.E., Tsebe, M.A., Tsoanamatsie, N., Sato, J.R., et al.: Time-
varyingcausality between oil and commodity prices in the presnce of structural
breaks and nonlinearity. Economia Internazionale/Int. Econ. 68(4), 469–491 (2015)
8. Lakshmi, P., Visalakshmi, S.: Reconnoitering the causal relationship in crude oil
market during crisis. J. Bus. Manag. Sci. 1(6), 128–132 (2013)
9. Li, M., Yang, L.: Modeling the volatility of futures return in rubber and oil–A
Copula-based GARCH model approach. Econ. Model. 35, 576–581 (2013)
10. Phochanachan, P., Pastpipatkul, P., Yamaka, W., Sriboonchitta, S.: Threshold
regression for modeling symbolic interval data. Int. J. Appl. Bus. Econ. Res. 15(7),
195–207 (2017)
11. Sang, W.C., Sriboonchitta, S., Rahman, S., Huang, W.T., Wiboonpongse, A.: Mod-
eling volatility and interdependencies of Thai rubber spot price return with climatic
factors, exchange rate and crude oil markets. J. Finan. Rev. 16, 1–20 (2012)
12. Tibprasorn, P., Khiewngamdee, C., Yamaka, W., Sriboonchitta, S. Estimating eﬃ-
ciency of stock return with interval data. In: Robustness in Econometrics, pp.
667–678. Springer International Publishing (2017)

Asymmetric Eﬀect with Quantile Regression
for Interval-Valued Variables
Teerawut Teetranont1(B), Woraphon Yamaka1,2, and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
teetranont@gmail.com, woraphon.econ@gmail.com, songsakecon@gmail.com
2 Puey Ungphakorn Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
Abstract. In this paper, we propose a quantile regression with interval
valued data using a convex combination method. The model we pro-
pose generalizes series of existing models, say typically with the center
method. Three estimation techniques consisting EM algorithm, Least
squares, Lasso penalty are presented to estimate the unknown parame-
ters of our model. A series of Monte Carlo experiments are conducted to
assess the performance of our proposed model. The results support our
theoretical properties. Finally, we apply our model to empirical data in
order to show the usefulness of the proposed model. The results imply
that the EM algorithm provides a best ﬁt estimation for our data set
and captures the eﬀect of oil diﬀerently across various quantile levels.
1
Introduction
In ﬁnance time series, such as stock, exchange rate, and asset prices, the data
is mostly presented in single valued-data (closing price). Although it is useful in
many situations, it fails in cases where a set of values is observed may times in
each time period. For example, in the daily asset price, a sequence of prices is also
available for each daily period. Therefore, if we consider either open price or close
price and ignore the intraday variability of asset prices, this may bring about an
inaccurate result and can not capture the ﬂuctuation in the dynamics of the real
economic and ﬁnancial phenomena even in a single day [12]. In addition, if we use
minimum and maximum values during the day and regress both dependent and
independent variables with respect to the corresponding minimum and maximum
values, we might not obtain a meaningful answers since a daily ﬂuctuations are
often very random. Thus, using values like daily minimum and daily maximum
which are eﬀected by these ﬂuctuations does not lead to meaningful regression
models.
To solve this problem, Arroyo et al. [1] suggested using interval valued data
where the higher and lower prices of each sequence are considered. Giordani [6]
also suggested that this kind of data oﬀers a more complete insight about the
phenomenon at hand than the average values Therefore, an interval-valued data
yt can be characterized by the pair of values yU
t and yL
t with yU
t > yL
t , where
yU
t
and yL
t denote as the upper and the lower bound of interval valued data,
respectively.
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_44

614
T. Teetranont et al.
Recently, interval valued data has been employed to deal with new data
forms related to multivariate situation, pattern recognition, and artiﬁcial intel-
ligence. It becomes important in the ﬁnancial ﬁeld and has also been applied
in econometric studies especially in linear regression analysis. There are several
proposals to ﬁt a regression model to interval data. The topic has been exten-
sively analyzed especially in the last decade, see Arroyo et al. [2], for review of
the literatures. The regression model, which is a statistical tool for the investi-
gation of relationships between dependent variables and explanatory variables,
has received considerable attention and been proposed in the analysis of inter-
val data in many studies. The linear regression with interval valued data was
ﬁrstly introduced in Billard and Diday [3,4]. They introduced the simplest cen-
ter method with linear regression where the representation of an interval can be
done in terms of the midpoint, say yM
t
= (yU
t + yL
t )/2. Further approaches con-
sider two separate regressions, namely lower bound and upper bound regression,
and both regressions share the same regression coeﬃcients. In a similar manner,
Lima Neto and de Carvalho [10] extended this approach by running two diﬀerent
linear regression models with the ﬁrst regression model on the midpoints of the
intervals and the second one on the ranges which are obtained from yR
t = yU
t −yL
t .
Subsequently, Lima Neto and de Carvalho [10] imposed non-negative constraints
on the regression coeﬃcients of the model for the range and use an inequality
constraint to ensure mathematical coherence between the predicted values of yU
t
and yL
t .
Regression model for interval data has also increasingly become an area in
economic and ﬁnancial researches and can be found in many studies such as
the works of Gonzalez and Lin [7], Rodrigues and Salish [14], Piamsuwannakit,
et al. [11], and Tibprasorn, et al. [16]. However, these studies ﬁt a regression
line through the means of the covariates and do not focus on the information
at the outlier. In general, stock prices and market data exhibit heavy tail, dis-
playing the extreme price, thus the assumption of normality might lead to the
wrong estimation result. In fact, economic factors are often notorious for con-
taining extreme values due to erratic market reaction to news, thus presenting
a non-Gaussian error distribution. To overcome these problems, Koenker and
Bassett [8] extended the conventional Ordinary Least Squares (OLS) regression
to quantile regression. The advantage of quantile regression is that it can provide
estimates more robust against outliers in the response measurements and solve
the estimation problems related to the impact of outliers and the fat-tailed error
distribution. In addition, we can obtain a more comprehensive analysis of the
relationship between response and covariate variables [17].
However, in this paper, we are going to extend the interval valued data to
quantile regression. Moreover, we aim to construct the model without taking
the midpoint of intervals. We expect that the center method may not be robust
enough to explain the real behavior of the interval data nor able to solve the
misspeciﬁcation problem of the model. Therefore, it will be of great beneﬁt to
relax this assumption of mid-point and assign appropriate weights between inter-
vals. Thus in this study, a convex combination approach which was proposed by

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
615
Chanaim et al. [5] is employed to obtain the appropriate weights. In addition,
in the context of estimation technique, we introduce various estimation methods
for estimating quantile regression with interval valued data. We ﬁrst present an
expected-maximum likelihood algorithm (EM-algorithm), least squares estima-
tor (LSE) and lasso penalty of Tibshirani [15].
To the best of our knowledge, the estimation of quantile regression model with
interval data using convex combination method has not been considered yet. This
fact becomes one of our motivations to work on this paper. Furthermore, to show
the performance of our model, we employ simulation study and real application
study. This is an additional study to contribute to the empirical literature, with
a focus on the impact of oil prices on the stock returns of Thailand. We focus
on this market because it has an important role in the Thai economy and is
attractive to foreign investors. The stock market may also reﬂect many scenarios
in the domestic economy. When share price is high, the value of the ﬁrm relative
to the replacement cost of its stock of capital is also high.
This paper is structured as follows: The next section introduces the quantile
regression model with interval valued data and its estimation. Section 3 consists
of simulations and demonstrates the estimation of parameters. Discussion is
made in Sect. 4 regarding an empirical application to examine the impact of
changes in real oil prices on the real stock returns of Thailand, Indonesia, and
Philippines (TIP countries) while conclusions are given in Sect. 5.
2
Methodology
Koenker and Roger [9] introduced quantile regression as a variant of regression
technique used in statistics and econometrics. It is in contrast to the linear regres-
sion model which results in estimates that are approximated at the conditional
mean of the response variable given certain values of the predictor variables. In
the classical quantile regression model, it was proposed to work with the time
series where each period is represented by a single value data. Here, our study
aims to go beyond that. The interval valued data is applied in quantile regression
model using convex combination method.
2.1
Quantile Regression Model
2.1.1
Model Structure
The model is represented by
w1(τ)yL
t + (1 −w(τ))yU
t = α(τ) +
k

i=1
βi(τ)[wi2(τ)xL
it + (1 −wi2(τ))xU
it] + εt(τ) (1)
where yL
t is the lower bound of dependent variable yt and yU
t the upper bound of
dependent variable, t = 1, · · · , T. xL
it is matrix T × k lower bound of k indepen-
dent variables and xU
it is matrix T ×k upper bound of k independent variables. w
is the weight parameter of the interval data with values [0,1]. The advantage of

616
T. Teetranont et al.
this method lies in the ﬂexibility to assign weights in calculating the appropriate
value between intervals βi(τ). is 1 × k vector of coeﬃcients and the error term
εt(τ) has a distribution which depends on the quantile. Thus, τ th(0 < τ < 1)
conditional quantile of yt given x′
it is simply as
Q(w1(τ)yL
t +(1−w1(τ))yU
t (τ | xL, xU) = [wi2(τ)xL
it + (1 −wi2(τ))xU
it]βi(τ)
(2)
In the simplest expression, we can rewrite (Eq. 1) in the reduced form equation as
[yt|w1] = α(τ) +
k

i=1
[xit | w2i]βi(τ) + εt(τ)
(3)
where yt|w1 is the vector of expected dependent variable conditional on w1 and
xit is T × k matrix of k expected independent variables conditional on w2.
2.1.2
Least Squares Estimation
In quantile regression with interval valued data, the estimation proceeds similarly
to the conventional linear least squares estimator. To obtain an estimate of all
conditional quantile function we simply minimize a weighted sum of absolute
residuals (see, Koenker and Bassett [8]. The τ speciﬁc coeﬃcient vector β(τ)
can be estimated by minimizing the loss function:
β(τ) = arg min
β(τ)
n

j=1
ρτ(yt −α(τ) −xitβi(τ)),
(4)
where the function ρτ(·) is the check function, and this gives τ th the sample
quantile with its solution.
ρτ(L) = (1 −τ)(L < 0) + (τ)(L ≥0)
T
(5)
where L is the loss function. The resulting minimization problem, when xitβi(τ)
is formulated as a linear function of parameters, can be solved by linear pro-
gramming method.
2.1.3
An EM Algorithm
When the error is assumed to be asymmetrically Laplace distributed (ALD),
quantile estimation is equivalent to the parametric case. The minimization of the
objective function (4) is equivalent to the maximum likelihood theory. According
to Yu and Moyeed [19], the ALD is a continuous probability distribution which
is generalized from the Laplace distribution in which its probability density
function is given by:
f(y | μ, σ, τ) = τ(1 −τ)
σ
exp

−ρτ
y −μ
σ

(6)

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
617
where μ is location parameter, σ > 0 is scale parameter and 0 < τ < 1 is skew
parameter. The εt ∼ALD(0, σ, τ) and i.i.d. in (2) is assumed. The function ρτ
is check function deﬁned by (5).
In this study, the EM algorithm is proposed to estimate our proposed model.
In terms of (3), ﬁrstly, we utilize the same mixture representation of the ALD
in Reed and Yu [13] as follows:
εt(τ) = θzt + ψ√ztet
(7)
where θ =
1 −2τ
τ(1 −τ), ψ2 =
2
τ(1−]tau)σ , zt ∼exp(1), et ∼N(0, 1) and zt is
independent of et. Therefore, we can rewrite (3) as
[yt | w1] = α(τ) + xit | w2iβi(τ) + ψ√ztet.
(8)
And the joint probability density function (6) becomes
f(y | β(τ), α(τ), w(τ), z) =
1
ψ√2πzt
exp

−ρτ
(yt | w1 −α(τ) −x2iβi(τ))2
2ψ2zt

(9)
According to Zhou et al. [20], in this EM algorithm, the estimation consists of
two steps as in the following:
(1) Expectation likelihood (E-step): Given ith iteration with a current vector of
Θ(τ) = {β(τ), α(τ), w(τ)}, we need to evaluate the expected complete-data
log-likelihood function with respect to the conditional on zt given yt and
Θi(τ), that is given by
Q(Θ(τ)|y, Θ(i−1)(τ)) = E(L(Θ(τ)|y, z)|y, Θ(i−1)(τ)),
(10)
where the log-likelihood function of complete data is
L(Θ(τ)|y, z) = −T log(2√πψ) −1
2
T

t=1
log zt
−
1
2ψ2
T

t=1
(yt|w1 −α(τ) −xitw2iβu(τ) −θzt)2
zt
−1
σ
T

t=1
zt
(11)
(2) Maximize Expectation likelihood (M-step): Update Θi(τ) to Θ(i+1)(τ) by
maximizing an expected likelihood function Q(Θ(τ)|y, Θ(i−1)(τ)). Then, let
φi
t = E(z−1
t
|yi, Θ) =
1
τ(1 −τ) ·
yi|w1 −α(τ) −xit|w2iβi(τ)
.
(12)
By solving
∂Q(Θ(τ)|y, Θ(i−1)
∂β
= 0,
(13)
∂Q(Θ(τ)|y, Θ(i−1)
∂wi
= 0,
(14)
∂Q(Θ(τ)|y, Θ(i−1)
∂wi
= 0.
(15)

618
T. Teetranont et al.
Thus, we obtain

βi(τ), αi(τ), wi(τ)
	
and then we can repeat the above E step
and M step until the largest change in the value of any parameter is convergence
to minimum error. And the ﬁnal convergence value of Θ(τ) = {β(τ), ατ), w(τ)}
is the MLE of unknown coeﬃcient vector of our proposed model.
2.1.4
Lasso Penalty Method
Lasso Penalty is an extension of the lease squares method. The basic idea of
this method is to penalize the coeﬃcients of diﬀerent covariates at a diﬀerent
level by using adaptive weights. Following the detail in Wu and Liu [18], the
adaptive-lasso quantile regression estimated as weights was introduced and the
loss function in (4) can be rewritten as follows
β(τ) = arg min
β(τ)
n

j=1
ρτ(yt −α(τ) −xitβi(τ)) + λ
d

j=1
ωj|βi(τ)j|
(16)
with respect to βi(τ), where ωj is the weights which are obtained from ωj =
1
|βi(τ)j|γ for γ > 0. λ is the value of the penalty parameter that determines how
much shrinkage is done under the presumption that the ﬁrst coeﬃcient is an
intercept parameter that should not be subject to the penalty. is a vector and it
should have length equal to the number of the covariates. The adaptive-LASSO
penalized quantile regression can also be solved using the linear programming
method.
3
Experiment Study
3.1
Simulation Study
We perform Monte Carlo simulations to assess the ﬁnite sample performance of
our proposed quantile regression with interval valued data using convex com-
bination method. Three estimation methods are used consisting EM algorithm,
Barrodale and Roberts least squares algorithm (LS), see detail in Koenker and
Bassett [8] and Lasso penalty, see Wu and Liu [18]; and these three estimators are
compared to determine the appropriate estimation technique for our proposed
model.
The sampling experiments are based on the 3 model speciﬁcations
Model 1: Center of interval data: w1 = w2 = 0.5
0.5yL
t + (1 −0.5)yU
t = 1 + 3(0.5xL
t + (1 −0.5)xU
t ) + εt
Model 2: Deviate from center of interval data:w1 = 0.3, w2 = 0.7
0.3yL
t + (1 −0.3)yU
t = 1 + 3(0.7xL
t + (1 −0.7)xU
t ) + εt
Model 3: Extremely deviate from center of interval data: w1 = 0.2, w2 = 0.9
0.2yL
t + (1 −0.2)yU
t = 1 + 3(0.9xL
t + (1 −0.9)xU
t ) + εt

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
619
In this section, we also consider diﬀerent quantile levels of our proposed model;
hence εt is assumed to be asymmetric Laplace distribution with skew parameter
τ = (0.20, 0.5, 0.80). We simulate 50 and 100 samples for the Monte Carlo exper-
iment using the speciﬁed parameters of each case. For each simulation case, we
perform 100 replications for all the three cases. In each simulation, we proceed
as follows.
(1) We draw εt(τ) from the ALD with mean 0, variance 1, and skew parameter
τ = (0.20, 0.5, 0.80).
(2) Random variable xU
t
from normal distribution with mean 0, variance 1.
Then, we generate xU
t according to xL
t = xU
t −U(0, 2). This guarantees that
the bounds are not crossing each other.
(3) xt of the intervals have been computed by xt = w1xU
t +(1−w1)xL
t where w1
is random U(0, 1). And generate dependent variables yt = xtβ(τ) + εt(τ).
(4) Compute the upper and lower bounds of yt by yL
t = yt −U(0, 2) and yU
t =
yt −(1 −w2)yL
t
w2
, where w2 is random U(0, 1).
To assess the performance of our proposed model and compare our results,
the resulted B = 100 independent sets of parameter estimates for each case is
evaluated by their Bias values:
Bias =
B

b=1
( 
Θb −Θtrue
b
)/B,
where 
Θb is the estimated parameter in each draw and Θtrue is the true para-
meter.
Then, we also include a comparison in the simulation study in order to evaluate
the performances of the convex combination method and the center method in the
quantile regression context. The comparison between methods is accomplished by
bias and MSE. The simulation results are summarized in Tables 1, 2 and 3.
To investigate the performance of our model and compare diﬀerent estimation
algorithms, we repeat the process of data generation and parameter estimation
100 times independently using sample size 50 and 100. Tables 1, 2 and 3 summa-
rize the results of 100 repetitions for the cases: model 1, model 2, and model 3.
Bias values of the parameters, at each quantile level, are reported. We compare
the estimated Bias values delivered by three cases of interval data: center, devi-
ate from center and extremely deviate from center with those provided by Center
and Convex combination methods. For case 1: when the appropriate weight is
located at the center of interval data (Table 1), we ﬁnd that center method com-
pletely outperforms the convex combination method at every quantile level and
every sample size.
For cases 2 and 3, we present the simulation in Table 2 for the case of deviate
from center and Table 3 for extremely deviate from center. Similar resuls are
obtained. We ﬁnd that with weight deviate from the center of interval, convex
combination method has a lower Bias when compared with the center method at

620
T. Teetranont et al.
Table 1. Simulation results for Model 1
Bias
Interval method
Center method
Convex combination method
Estimation
LS
EM
LASSO
LS
EM
LASSO
N=50
α(.2)
0.0422
0.0627
0.0174
0.4916
0.2522
0.1644
β(.2)
−0.0277
0.0063
−0.0203
−0.046
−0.0766
−0.0779
σ(.2)
3.9801
−0.023
4.1726
3.9593
0.0006
3.8731
w1(0.2)
n.a
n.a
n.a
0.0638
0.1656
0.1917
w2(0.2)
n.a
n.a
n.a
−0.2136
−0.1916
−0.2566
N=100
α(.2)
0.0077
0.0321
−0.0222
0.1685
0.2913
0.3048
β(.2)
0.0042
−0.0294
−0.0346
−0.0235
0.1301
−0.0595
σ(.2)
4.1274
−0.022
4.1004
4.2021
0.0317
4.1279
w1(0.2)
n.a
n.a
n.a
0.1011
0.1878
0.1229
w2(0.2)
n.a
n.a
n.a
−0.2716
−0.1508
−0.1969
N=50
α(.5)
−0.0757
−0.0076
−0.0398
0.1424
0.2116
−0.0403
β(.5)
0.0001
−0.0115
−0.0541
−0.0548
−0.1305
−0.0044
σ(.5)
1.7882
−0.0185
1.7799
1.687
−0.0185
1.723
w1(0.5)
n.a
n.a
n.a
0.1742
0.1385
0.1529
w2(0.5)
n.a
n.a
n.a
−0.1642
−0.1481
−0.2231
N=100
α(.5)
0.0045
−0.0359
−0.0106
−0.0953
0.1904
0.0661
β(.5)
0.0032
−0.0325
−0.0318
−0.0003
−0.0003
−0.0192
σ(.5)
1.849
−0.0109
1.8638
1.8165
−0.029
1.7444
w1(0.5)
n.a
n.a
n.a
0.2061
0.1074
0.1505
w2(0.5)
n.a
n.a
n.a
−0.2229
−0.1932
−0.2001
N=50
α(.8)
−0.0369
−0.0146
−0.0785
−0.2996
−0.266
−0.4361
β(.8)
−0.1281
0.014
−0.137
−0.01
0.0931
−0.1318
σ(.8)
4.0642
−0.0003
3.8955
4.0966
−0.0419
4.0298
w1(0.8)
n.a
n.a
n.a
0.0914
0.2231
0.1271
w2(0.8)
n.a
n.a
n.a
−0.2323
−0.2655
−0.2607
N=100
α(.8)
−0.0111
−0.0378
0.0042
−0.2111
−0.2462
−0.2155
β(.8)
0.0345
0.0149
−0.0005
−0.0246
−0.1141
−0.0956
σ(.8)
4.0923
−0.0054
4.0351
4.1604
−0.0247
4.1254
w1(0.8)
n.a
n.a
n.a
0.0961
0.1699
0.0805
w2(0.8)
n.a
n.a
n.a
−0.2457
−0.2177
−0.2626
Source: Calculation
Note: () is quantile level
every quantile level. Thus, we can indicate that the convex combination method
outperforms the center method when the weight is deviated from the center of
intervals for every quantile level.
This simulation study also compares the estimation algorithms to ﬁnd the
appropriate estimation technique that ﬁts our proposed model. We ﬁnd all esti-
mation techniques to perform well in this simulation study since the estimated
parameters values are close to the true values. However, it is not clear which
estimator provides the best ﬁt results. So, we can say that these estimation
techniques can be used to estimate unknown parameters of our model. When
we consider the number of observations, we ﬁnd that given the large sample size
(n = 100) the mean estimates have a smaller bias when compared with small
sample size (n = 50).

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
621
Table 2. Simulation results for Model 2
Bias
Interval method
Center method
Convex combination method
Estimation
LS
EM
LASSO
LS
EM
LASSO
N=50
α(.2)
−0.8605
−0.7763
−0.8007
−0.141
−0.6409
−0.5047
β(.2)
0.0496
0.1197
0.0663
-0.0082
0.1374
−0.0588
σ(.2)
4.0324
−0.0314
4.244
4.1122
0.035
3.9554
w1(0.2)
n.a
n.a
n.a
−0.0659
−0.0442
0.0919
w2(0.2)
n.a
n.a
n.a
−0.1382
−0.1264
−0.1871
N=100
α(.2)
−0.8233
−0.8458
−0.8344
−0.1527
−0.1511
−0.3788
β(.2)
0.0658
0.0865
0.0444
−0.0162
0.2004
−0.0892
σ(.2)
4.1174
−0.0042
4.1936
4.2281
0.0076
4.156
w1(0.2)
n.a
n.a
n.a
−0.0105
0.0668
0.0388
w2(0.2)
n.a
n.a
n.a
−0.0982
−0.0864
−0.1299
N=50
α(.5)
−0.8025
−0.7098
−0.8615
−0.1166
−0.0892
−0.0927
β(.5)
0.0515
0.1365
0.0951
−0.0058
−0.0671
−0.0045
σ(.5)
1.8398
0.0392
1.7864
1.8219
−0.0368
1.7802
w1(0.5)
n.a
n.a
n.a
0.0448
0.0792
0.0871
w2(0.5)
n.a
n.a
n.a
−0.0106
0.0059
0.0105
N=100
α(.5)
−0.8551
−0.8809
−0.8477
0.0458
0.0158
−0.0513
β(.5)
0.0813
0.1433
0.0723
−0.0043
0.004
−0.0227
σ(.5)
1.7491
0.0207
1.8266
1.7524
0.0018
1.7801
w1(0.5)
n.a
n.a
n.a
0.0153
−0.0014
0.0457
w2(0.5)
n.a
n.a
n.a
0.0126
0.006
0.0134
N=50
α(.8)
−0.7896
−0.671
−0.9064
−0.8724
−0.3592
−0.9064
β(.8)
0.2235
−0.0318
0.0328
0.0376
−0.0266
−0.0749
σ(.8)
4.1234
−0.0427
4.0622
3.8769
−0.0515
4.1015
w1(0.8)
n.a
n.a
n.a
0.1265
0.0919
0.0882
w2(0.8)
n.a
n.a
n.a
−0.1022
−0.012
−0.1304
N=100
α(.8)
−0.8243
−0.8197
−0.84
−0.5347
0.0477
−0.6141
β(.8)
0.1257
0.0457
0.0516
0.0084
0.0035
−0.021
σ(.8)
4.0812
0.0023
4.1731
4.1839
0.0011
4.189
w1(0.8)
n.a
n.a
n.a
0.0174
0.0087
0.0388
w2(0.8)
n.a
n.a
n.a
−0.1085
0.0165
−0.1276
Source: Calculation
Note: () is quantile level
In summary, from evaluating our model performance and its three estimation
techniques’ performances, we reach a similar conclusion to those obtained from
evaluating their bias values. Our proposed model performs well in the simulation
study and the convex combination method shows the superiority when the weight
is deviated from the center. In addition, with a larger sample size, a lower bias
estimate is obtained.

622
T. Teetranont et al.
Table 3. Simulation results for Model 3
Bias
Interval method
Center method
Convex combination method
Estimation
LS
EM
LASSO
LS
EM
LASSO
N=50
α(.2)
−1.3116
−1.2914
−1.2888
−0.4381
−0.2894
−0.6031
β(.2)
0.0618
0.2148
0.1645
0.039
−0.1175
−0.0479
σ(.2)
4.1467
0.0113
4.2156
3.9995
−0.0295
4.0379
w1(0.2)
n.a
n.a
n.a
0.1475
−0.1308
0.0131
w2(0.2)
n.a
n.a
n.a
−0.153
0.0017
−0.1038
N=100
α(.2)
−1.3043
−1.3464
−1.3704
−0.4265
−0.1926
−0.5975
β(.2)
0.1425
0.1118
0.05994
0.0565
−0.02
−0.0706
σ(.2)
4.2235
0.0085
4.0601
4.0121
−0.2037
4.2214
w1(0.2)
n.a
n.a
n.a
0.0315
−0.0771
0.1114
w2(0.2)
n.a
n.a
n.a
−0.0007
0.0826
0.0021
N=50
α(.5)
−1.1829
−1.3411
−1.3248
−0.3805
−0.1884
−0.5189
β(.5)
0.2023
0.0466
0.0956
0.0115
0.0677
0.0043
σ(.5)
1.93
0.0103
1.87
1.7785
−0.0673
1.754
w1(0.5)
n.a
n.a
n.a
0.0272
−0.0087
0.0912
w2(0.5)
n.a
n.a
n.a
0.0797
0.0962
0.0547
N=100
α(.5)
−1.2798
−1.2935
−1.25
−0.0016
0.2913
−0.1409
β(.5)
0.1334
0.1381
0.1118
−0.0036
0.13
−0.0416
σ(.5)
1.8145
0.0301
1.8727
−0.0022
0.0317
1.7957
w1(0.5)
n.a
n.a
n.a
−0.1404
0.1878
−0.0988
w2(0.5)
n.a
n.a
n.a
0.1147
−0.1508
0.1029
N=50
α(.8)
−1.2183
−1.2945
−1.3475
−1.2133
−0.6353
−1.1072
β(.8)
0.1989
0.1292
0.1008
0.0584
−0.0901
0.0261
σ(.8)
3.9875
0.0097
3.9895
4.0908
−0.1121
4.1459
w1(0.8)
n.a
n.a
n.a
0.0997
0.0198
0.0331
w2(0.8)
n.a
n.a
n.a
−0.0956
0.012
−0.0902
N=100
α(.8)
−1.2986
−1.2752
−1.2095
−0.8271
−0.4554
−0.9515
β(.8)
0.0986
0.1531
0.1485
−0.0377
−0.1329
−0.0235
σ(.8)
4.1316
0.0186
4.1737
4.2708
−0.0284
4.2254
w1(0.8)
n.a
n.a
n.a
0.0072
−0.1407
0.0881
w2(0.8)
n.a
n.a
n.a
−0.0424
0.0852
−0.0449
Source: Calculation
Note: () is quantile level
4
Empirical Illustration: SET and Oil Low/High Return
Interval
4.1
Data description
In this study, the daily data set consists of the Brent oil price and SET index of
Thailand for the period from 4 January 2012 to 18 October 2016, covering 1209
observations. We have considered the daily minimum and maximum of these
prices and the data were collected form Thomson Reuters DataStream, Faculty
of Economics, Chiang Mai University. In addition, the prices are non-stationary;
they exhibit an upward and downward trend. Therefore we need to transform
our data to make them stationary. To preserve the interval format, we calculate
daily returns with respect to the previous day weighted average price, that is,

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
623
rymax,t = ymax,t −yavg,t
yavg,t−1
rymin,t = ymin,t −yavg,t
yavg,t−1
,
where ymin,t, ymax,t and yavg,t−1 are minimum, maximum, and average price of
our intervals. Figure 1 shows our interval returns, and the description of interval
returns are shown in Table 4. We can observe that both lower and upper returns
exhibit low volatilities where standard deviation of lower and upper oil returns,
OILL and OILU), and standard deviation of lower and upper SET returns, SETL
and SETU, are lower than 0.04, and they vary within range [−0.1661 −0.1255];
whereas the SET interval return varies within the wider range [−0.0960 – 0.0691].
Prior to estimating our proposed model, these interval returns have to be checked
by the Augmented Dickey Fuller unit roots test and we ﬁnd that all interval
returns are stationary at the level with 1% signiﬁcant.
Fig. 1. High/Low returns of daily oil price and SET index
Table 4. Data description
OILU
OILL
SETU
SETL
Mean
0.0109
−0.0134
0.0057
−0.005
Median
0.0086
−0.0097
0.0053
−0.0034
Maximum 0.1265
0.0365
0.0691
0.0458
Minimum
−0.1255
−0.1661
−0.0277 −0.096
Std. Dev
0.0339
0.0325
0.0082
0.0102
Skewness
−21.8241 −23.3027 0.9526
−1.5934
Kurtosis
658.2207
701.5841
9.2632
12.5008
JB-test
21704706 24672987 2157
5054
ADF-test
0
0
0
0
Source: Calculation

624
T. Teetranont et al.
4.2
Empirical Results
In this study, we summarize and report the estimation results from our empir-
ical data. We employ three estimation techniques and both convex combi-
nation and center methods as presented above which obtained the estimates
α(τ), β(τ), σ(τ), w1(τ), w2(τ) for τ = 0.1, 0.2, 0.5, 0.8, 0.9. We run our pro-
posed model using both convex combination and center methods delivered from
LS, EM, and LASSO. We observe that the Akaki Information Criterion (AIC) of
quantile regression using convex combination method shows a lower AIC when
compared with quantile regression using center method. In addition, we compare
the performance of the diﬀerent estimation techniques by considering the AIC.
We ﬁnd that EM algorithm provides the smallest AIC at all quantile levels.
Moreover, our estimated results from EM-algorithm seem to have an eco-
nomic interpretation. We found a signiﬁcant eﬀect of oil price on SET index.
The empirical evidence shows the eﬀect of oil to be diﬀerent across various
quantile levels (see, Fig. 2). The coeﬃcient β(τ) of oil evolves from positive to
Table 5. Estimation results
Estimation Center method
Convex combination method
LS
EM
LASSO
LS
EM
LASSO
α(0.1)
−0.0046
−0.0051
−0.0093
−0.0447
−0.0311
−0.00311
(0.0006)
(0.0001)
(0.0004)
(0.0005)
(0.0001)
(0.0004)
β(0.1)
0.0371
0.0389
0.00001
0.0341
0.0349
0.00001
(0.0247)
(0.0098)
(0.0246)
(0.0298)
(0.0094)
(0.0219)
σ(0.1)
0.0014
0.0013
(0.0001)
(0.0001)
w1(0.1)
0.1142
0
0
(0.0966)
(0.1463)
(0.1051)
w2(0.1)
0.2976
0.7283
1
(0.3725)
(0.4029)
(0.9495)
AIC
−7572.98 −7570.79 −7228.58 −7609.66 −7708.72 −7706.47
α(0.2)
−0.0012
−0.0013
−0.0054
−0.0214
−0.0205
−0.0215
(0.0003)
(0.0002)
(0.0003)
(0.0003)
(0.0001)
(0.0002)
β(0.2)
0.0236
0.0128
0.00001
0.012
0.0283
0.00001
(0.0211)
(0.0102)
(0.0197)
(0.0179)
(0.0092)
(0.0193)
σ(0.2)
0.0021
0.002
(0.0001)
(0.0001)
w1(0.2)
0.1627
0
0.1627
(0.0634)
(0.0395)
(0.0634)
w2(0.2)
0.5166
1
0.5166
(2.8889)
(0.5333)
(2.8889)
(continued)

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
625
Table 5. (continued)
Estimation Center method
Convex combination method
LS
EM
LASSO
LS
EM
LASSO
AIC
−8004.4
−8001.42 −7726.51
−7998.8
−8100.85 −7998.8
α(0.5)
0.0039
0.0039
0.001
0.0052
0.0053
0.0052
(0.0002)
(0.0002)
(0.0003)
(0.0002)
(0.0002)
(0.0002)
β(0.5)
−0.0041
−0.0041
−0.00001
−0.001
−0.0035
−0.00001
(0.0178)
(0.0114)
(0.0165)
(0.01558) (0.0009)
(0.0159)
σ(0.5)
0.0029
0.0029
(0.0001)
(0.0001)
w1(0.5)
0.0085
0
0.0085
(0.0001)
(0.0215)
(0.0001)
w2(0.5)
0.5063
0
0.5063
(8.7932)
(1.4043)
(8.7932)
AIC
−8312.58 −8310.59 −8175.71
−8347.02 −8350.29 −8349.01
α(0.8)
0.0092
0.0095
0.0062
0.0113
0.0162
0.011
(0.0003)
(0.0002)
(0.0003)
(0.0003)
(0.0002)
(0.0003)
β(0.8)
−0.0378
−0.027
−0.00001
−0.0171
−0.101
−0.00001
(0.0216)
(0.0107)
(0.0198)
(0.0174)
(0.01)
(0.0162)
σ(0.8)
0.0022
0.0021
(0.0001)
(0.0001)
w1(0.8)
0
0.4843
0
(3.2506)
(0.0014)
(0.3256)
w2(0.8)
0
1
0
(5.8502)
(0.1578)
(5.8502)
AIC
−7933.62 −7931.74 −7978.8
−7885.53 −7976.23 −7885.04
α(0.9)
0.0126
0.013
0.0096
0.0525
0.0558
0.0531
(0.0005)
(0.0001)
(0.0004)
(0.0005)
(0.0001)
(0.0004)
β(0.9)
−0.0272
−0.0213
−0.0001
−0.0279
−0.0164
−0.00001
(0.0245)
(0.0099)
(0.024)
(0.0245)
(0.0083)
(0.02311)
σ(0.9)
0.0015
0.0014
(0.0001)
(0.0001)
w1(0.9)
0.1551
0.8791
0.1551
(0.0034)
(0.0372)
(0.0034)
w2(0.9)
0.8869
1
0.8869
(2.3008)
(0.1478)
(2.3008)
AIC
−7466.86 −7464.8
−7601.266 −7466.85 −7984.54 −7466.19
Source: Calculation

626
T. Teetranont et al.
Fig. 2. Linear relationship between the interval Oil price and SET Index
negative as the quantile increases. Consider the w1(τ) and w2(τ), the diﬀerent
results are obtained. We observe that w1(τ) of dependent variable SET interval
return increases as the quantile increases while w2(τ) of independent variable
oil interval return is high at the extreme quantile level. This indicates that SET
index tend to move to the upper bound when the market goes up. For oil interval
returns, we can indicate that the returns tend to move to the upper bound when
the market enters the extreme event such as market boom and market crash.
5
Conclusions
The analysis of interval-valued data has mainly focused on ﬁtting classical regres-
sion models to the lower and upper bounds of the intervals; however, the optimal
weight of intervals is computed at the center of the intervals. Moreover, the inter-
val valued data mostly works on the linear regression model where the model
is approximated at the conditional mean of the response variable. Thus, in this
study, we aim to apply an interval valued data in quantile regression in order to
estimate either the conditional median or other quantiles of the response variable.
Moreover, to relax a strong assumption of the center method, we also proposed a
convex combination method to ﬁnd an appropriate weight between the intervals.
Therefore, in this study, we proposed a quantile regression with interval valued
data using convex combination method. Moreover, we also employ three esti-
mation techniques, namely EM algorithm, Barrodale and Roberts least squares
algorithm (LS), and Lasso penalty and then compare these estimators in order
to determine the appropriate estimation technique for our proposed model. We
have conducted a simulation study and shown the performance of our proposed

Asymmetric Eﬀect with Quantile Regression for Interval-Valued Variables
627
model as well as compared the performance of center and convex combination
methods. We ﬁnd at any given weight deviated from center, convex combination
appears superior to center method at every quantile level and estimation tech-
niques. Furthermore, we have highlighted an empirical aspect of our model with
interval time series of daily SET index and oil price to assess our model through
real data set; and aimed at examining the eﬀect of oil price on SET index. The
results show that EM algorithm provides a best ﬁt estimation for our data set
and that the eﬀect of oil varies across various quantile levels. The coeﬃcient of
β(τ) oil evolves from positive to negative as the quantile increases.
References
1. Arroyo, J., Esp´ınola, R., Mat´e, C.: Diﬀerent approaches to forecast interval time
series: a comparison in ﬁnance. Comput. Econ. 37(2), 169–191 (2011)
2. Arroyo, J., Gonzalez-Rivera, G., Mate, C.: Forecasting with interval and histogram
data some ﬁnancial applications. In: Ullahand, A., Giles, D. (eds.) Handbook of
Empirical Economics and Finance, pp. 247–280. Chapman and Hall (2011)
3. Billard, L., Diday, E.: Regression analysis for interval-valued data. In: Data Analy-
sis, Classiﬁcation and Related Methods, Proceedings of the Seventh Conference of
the International Federation of Classiﬁcation Societies (IFCS 2000), pp. 369–374.
Springer, Belgium (2000)
4. Billard, L., Diday, E.: Symbolic regression analysis. In: Proceedings of the Eigh-
teenth Conference of the International Federation of Classiﬁcation Societies (IFCS
2002), Classiﬁcation, Clustering and Data Analysis, pp. 281–288. Springer, Poland
(2002)
5. Chanaim, S., Sriboonchitta, S., Rungruang, C.: A convex combination method for
linear regression with interval data. In: Proceedings of the 5th International Sym-
posium on Integrated Uncertainty in Knowledge Modelling and Decision Making,
IUKM 2016, Da Nang, Vietnam, 30 November–2 December 2016, pp. 469–480.
Springer (2016)
6. Giordani, P.: Linear regression analysis for interval-valued data based on the Lasso
technique. Technical report, 6 (2011)
7. Gonzalez-Rivera, G., Lin, W.: Interval-valued Time Series: Model Estimation based
on Order Statistics (No. 201429) (2014)
8. Koenker, R.W., Bassett Jr., G.: Tests of linear hypotheses and I1 estimation.
Econometrica Econometric Soc. 50(1), 43–61 (1982)
9. Koenker, R.: Quantile Regression (No. 38). Cambridge University Press (2005)
10. Lima-Neto, E.A., De Carvalho, F.A.T.: Constrained linear regression models for
symbolic interval-valued variables. Comput. Stat. Data Anal. 54, 333–347 (2010)
11. Piamsuwannakit, S., Autchariyapanitkul, K., Sriboonchitta, S., Ouncharoen, R.:
Capital asset pricing model with interval data. In: Integrated Uncertainty in
Knowledge Modelling and Decision Making, pp. 163–170. Springer (2015)
12. Phochanachan, P., Pastpipatkul, P., Yamaka, W., Sriboonchitta, S.: Threshold
regression for modeling symbolic interval data. Int. J. Appl. Bus. Econ. Res. 15(7),
195–207 (2017)
13. Reed, C., Yu, K.: A Partially collapsed Gibbs sampler for Bayesian quantile regres-
sion (2009)
14. Rodrigues, P.M., Salish, N.: Modeling and forecasting interval time series with
threshold models. Adv. Data Anal. Classif. 9(1), 41–57 (2015)

628
T. Teetranont et al.
15. Tibshirani, R.J.: Regression shrinkage and selection via the lasso. J. Roy. Statist.
Soc. Ser. B 58, 267–288 (1996)
16. Tibprasorn, P., Khiewngamdee, C., Yamaka, W., Sriboonchitta, S.: Estimating
eﬃciency of stock return with interval data. In: Robustness in Econometrics, pp.
667–678. Springer (2017)
17. Waldmann, E., Kneib, T.: Bayesian bivariate quantile regression. Stat. Model.
(2014). 1471082X14551247
18. Wu, Y., Liu, Y.: Variable selection in quantile regression. Stat. Sin. 19, 801–817
(2009)
19. Yu, K., Moyeed, R.A.: Bayesian quantile regression. Stat. Probab. Lett. 54(4),
437–447 (2001)
20. Zhou, Y.H., Ni, Z.X., Li, Y.: Quantile regression via the EM algorithm. Commun.
Stat. Simul. Comput. 43(10), 2162–2172 (2014)

The Future of Global Rice Consumption:
Evidence from Dynamic Panel Data Approach
Duangthip Sirikanchanarak1(B), Tanaporn Tungtrakul1,
and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
doungtis@gmail.com, tanaporn.tung@gmail.com, songsakecon@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. This study investigates the future outlook of global rice con-
sumption using dynamic panel data regression (DPD) with penalised
ﬁxed eﬀect model. The three main factors aﬀecting rice consumption
include previous rice demand, GDP per capita, and world rice price. The
data set covers 73 countries that is almost 80% of world rice consumption
from 1960 to 2015. We separate these countries into 4 groups based on
income levels classiﬁed by the World Bank including low income, lower
middle-income, upper middle-income, and high income. The results show
that, at the global scale, rice consumption is expected to be slightly
higher. Such demand is driven by rising demand from the upper middle-
and high income countries, while it is oﬀset by the lower demand from
lower middle- and low income countries.
Keywords: Dynamic panel data · Fixed eﬀect · Forecasting
Rice consumption
1
Introduction
Rice is the main food of half of the world’s population, and it supplies 20% of
the calories consumed worldwide [10]. The decreasing growth of rice production
is partly aﬀected by climate change while there is increase in world population
growth rate [19]. This may give rise to the need for new production approach to
meet the competing demands for rice. There have been arguments that whether
the genetically modiﬁed foods need or not. Global rice consumption per capita
does not change much from year to year (0.27%). Therefore, if the global rice
production increases, the excess of rice supply will be stored. On the other hand,
if the global rice production decrease, the stocks will be used. However, many
studies indicated that the food security issue is important for the policy makers
to impose appropriate policy to achieve more food in the future. Moreover, the
rice demand projections will help governments, donors, and businesses to allocate
investment resources for increasing rice production and ensure that global rice
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_45

630
D. Sirikanchanarak et al.
supplies are enough and the producers also produce it as eﬃciently as possible
[20]. The United Nations [21] predicted that the world’s population will increase
more than double by 2020, following the increase of population in China and
India, the largest and second largest rice consumption countries in the world.
The International Rice Research Institute (IRRI) [19] and the Food and Agri-
culture Organization (FAO) studied the future rice consumption by using time
series data with linear regression model. They found that one of the key determi-
nants of future rice consumption was the rate of population growth. Kubo and
Purevdorj [10] conducted a research on the relationship between population and
rice consumption. They concluded that whether rice would be enough for the
increasing population depends on the rice production in Asia. Timmer et al. [20]
discovered that the long-run dynamic of rice consumption may have four basic
forces to consider or explain namely (1) population growth, (2) income growth,
(3) declining real price for rice, and (4) the gradual shift of workers from rural to
urban employment. They applied a step-wise regression model with time series
to estimate the global rice consumption. Moreover, they suggested that panel
estimation was the standard for estimating parameters when pooled cross-section
and time-series data are available.
Apart from the changes in population that are usually studied in other lit-
eratures, the rice consumption per capita is also another important factor that
aﬀects the total consumption. In this case, if both factors rise, the overall rice
demands will be accelerated. In contrast, even if the higher in population but
partitioned with the lower in the consumption per capita, the total demand on
rice will grow only in small amount. Abdullah et al. [1] predicted the rice con-
sumption per capita would be 55.2-62.7 kg in 2015, 49.1–61.2 kg in 2025, and
42.4–58.1 kg in 2050. Alexandratos and Bruinsma [2] analyzed the world agricul-
ture toward 2030/2050. They found that the world per capita rice consumption
has levelled after the late 1980s, following mild declines in several countries of
East and South Asia and small increases in other regions. These trends have pro-
jected to continue and the average of the developing countries may fall to 57 kg
in 2050. FAO [6] reported the food outlook of global food markets that the rice
consumption per capita in 2016 and 2017 were 54.5 kg and 54.6 kg, respectively.
This study employs a dynamic panel data (DPD) model with penalised ﬁxed
eﬀects. The advantage of using the panel data approach is the increase in data
points and therefore the power of statistical estimation [7]. The ﬁxed eﬀects
model we have chosen is a common choice for economists, and is generally more
appropriate than the random eﬀects model [9]. The dynamic panel data approach
has been widely used for analyzing various economic issues such as the relation-
ship between energy consumption and GDP growth [7], the determinants of
Foreign Direct Investment (FDI) into Central and Eastern European Countries
[5], the inﬂuence of tourism and the composition of human capital on economic
growth [16,23] and the eﬀects of internet adoption on reducing corruption [13].
Their ﬁndings suggest that the dynamic panel data model is an accurate model
for investigating the relationship between macroeconomic factors. Therefore, it
is of practical interest to investigate the predictive performance of the dynamic

The Future of Global Rice Consumption
631
panel data model. We do that by considering the forecasting of the global rice
consumption based on key factors.
Our aim in this study is to forecast the global rice consumption per capita.
We separate countries into 4 groups based on income levels including low income,
lower middle-income, upper middle-income, and high income that are classiﬁed
by the World Bank. We take three variables–previous rice demand, income, and
world price of rice as the main drivers of change in rice consumption per capita
to be estimated by the dynamic panel data regression model with ﬁxed eﬀect for
forecasting. The main results of this research will be useful for the policy makers
particularly in the middle- and low-income countries where rice consumption is
the largest in the world. In addition, it is also appropriate for supporting the
interests of producers, investors, and traders.
The remainder of this analytical task is organized into six parts. In Sect. 2, we
brief the theoretical background. In Sect. 3, we describe the data set and provide
methodology corresponding to the dynamic panel data regression model. The
results on estimation and forecasting of global rice consumption are presented
in Sect. 4. Policy implication is discussed in Sect. 5. And in the last section, we
provide some concluding remarks.
2
Theoretical Background
The traditional theory of demand recognizes that the demand for a commodity is
its quantity which consumers are able and willing to buy at various prices during
a given period of time. So, for a commodity to have demand, the consumer must
possess willingness to buy it, the ability or means to buy it, and it must be
related to per unit of time i.e. per day, per week, per month or per year. The most
important objective of demand analysis is forecasting demand. Forecasting refers
to predicting the future level of demand on the basis of current and past trends, as
the resource allocation relies on predicted results. There are two types of demand
functions that are individual demand function and market demand function. The
market demand function refers to the total demand for a commodity or service
of all the buyers. In this research, we focus on individual demand function which
is the basis of demand theory and it can be expressed mathematically as follows:
Dx = f(Px, Py, I, T, U)
(1)
where Dx is demand for a commodity x, Px and Py are the prices of commodities
x and y, respectively. Commodities x and y are substitutes. I is income of a
consumer and Engle curves [12], T is taste and preference of a consumer, and
U is other variables such as quality of commodity, advertisement and publicity,
fashion or demonstration eﬀect, etc.
The individual demand function expressed above is a listing of variables
that aﬀect the individual demand. All the above factors play very important
role in the determining demand for a commodity or service if all the above
stated factors are taken as variable. In microeconomics [15], it is important to
understand that Law of Demand assumes partial equilibrium which means that

632
D. Sirikanchanarak et al.
if other things remain constant then whenever the price of a commodity changes
then the demand for that commodity changes in the opposite direction. It can be
stated that, “conditional on all else being equal, as the price of a good increases
( ↑), quantity demanded decreases (↓); conversely, as the price of a good decreases
(↓), quantity demanded increases (↑)”. If, on the other hand, general equilibrium
analysis is used in explaining the demand then impact of some of these other
factors can be explained as follows:
Price of a commodity. As the price of commodity falls a commodity becomes
cheaper in a market and rational consumer will try to demand more units of
the same to maximize his satisfaction and vice-versa when price rises. Therefore
rise in price fall in demand and fall in price rise in demand. But if the good
is inferior, the increase in purchasing power caused by the price decrease may
cause less of the good to be bought.
In 1895, Robert Giﬀen observed Giﬀen’s paradox. This paradox has been
explained that the change in price and the resulting change in the quantity
demanded could move in the same direction. A Giﬀen good is a staple food,
such as bread or rice, which forms large percentage of the diet of the poorest
sections of a society, and for which there are no close substitutes. From time to
time the poor may supplement their diet with higher quality foods, and they
may even consume the odd luxury, although their income will be such that they
will not be able to save. A rise in the price of such a staple food will not result
in a typical substitution eﬀect, given there are no close substitutes. If the real
incomes of the poor increase they would tend to reallocate some of this income
to luxuries, and if real incomes decrease they would buy more of the staple good,
meaning it is an inferior good. Assuming that the money incomes of the poor
are constant in the short run, a rise in price of the staple food will reduce real
income and lead to an inverse income eﬀect.
Prices of substitute good. Demand for a commodity also depends upon the prices
of its close substitutes. If price of close substitute falls then demand for that
commodity also falls and vice-versa. Therefore demand also depends upon the
number and degree of close substitutes available in market and the range of price
change.
Income of a consumer. Consumer income is the basic determinant of the quantity
demanded of the product. Generally the people with higher disposable income
spend a larger amount of income than those with the lower income. Income
demand relationship is more varied in nature than that between demand and
its other determinants. To explain the varied relationship between income and
demand we classify goods and services into four broad categories.
First, essential consumer good is necessary goods for the health, safety or
welfare of consumers. The goods which people need no matter how high the price
is are basic or necessary goods. The second is inferior good. It is a good which
quantity demanded decreases when consumer income rises or quantity demanded
rises when consumer income decrease. Inferior goods is often associated with
lower socio-economic group. The third is normal good which consumer demand

The Future of Global Rice Consumption
633
increase when their income increases. Last, prestige good or luxury good is often
associated with wealth and the wealthy.
3
Data and Methodology
3.1
Data Set
The panel data contains 73 countries from 1960 to 2015. These countries include
the major rice consuming countries covering 78% of the total world rice consump-
tion. We divide them into four sub-panels based on the diﬀerence in income levels
deﬁned by the World Bank: low income group, lower middle-income group, upper
middle-income group and high income group (Table 1). The variables used for the
analysis include demand for rice per capita, rice price, and GDP per capita that
is indicator of income per capita. This dataset is constructed from World Bank,
United States Department of Agriculture (USDA), and United Nations Statis-
tics Division (UN). We separated the data set into two sections. The actual data
used for model selection and estimation are from 1960 to 2013, while 2014 and
2015 are left for forecast evaluation. All variables are transformed into growth
rate.
Figure 1 presents information on rice consumption per capita and GDP per
capita of aggregated 73 countries as a whole. This plot shows that since the early
Table 1. List of 73 sample countries, grouped by income level
Income level
Country
Low income
countries
Afghanistan, Benin, Burkina Faso, Chad, Liberia, Madagascar,
Nepal, Senegal, Sierra Leone, Togo
Lower middle
income
countries
Bangladesh, Bolivia, Cambodia, Cameroon, Cole d’Ivoire, Ghana,
Guatemala, Honduras, India, Kenya, Mauritania, Morocco,
Nicaragua, Nigeria, Pakistan, Papua, Philippines, Sri Lanka,
Swaziland, Syrian, Zambia
Upper middle
income
countries
Algeria, China, Colombia, Costa Rica, Dominican, Ecuador,
Guyana, Iran, Irag, Jamaica, Malaysia, Mexico, Panama, Peru,
South Africa, Suriname, Thailand, Turkey, Venezuela
High income
countries
Australia, Canada, Chile, Hong Kong, Israel, Japan, Korea,
Singapore, Switzerland, Trinidad, United State, EU (12 countries)
Note that: Gross National Income (GNI) is the total incomes of citizens in the country
living both domestically and internationally. In brief, we could say that GNI is the
gross domestic product (GDP) plus the incomes earned from residents living overseas
minus that earned from nonresident. World Bank classiﬁes all countries into four
income groups by using GNI per capita. Low income economies are deﬁned as those
with a GNI per capita of 1,025 USD or less; lower middle income economies are those
with a GNI per capita between 1,026 USD and 4,035 USD; upper middle income
economies are those with a GNI per capita between 4,036 USD and 12,475 USD;
high-income economies are those with a GNI per capita of 12,476 USD or more.
Source: World Bank

634
D. Sirikanchanarak et al.
Fig. 1. Trend and Engel curve of global rice consumption per capita
Fig. 2. Per capita rice consumption classiﬁed (kg) by income groups. The interquartile
range box represents the middle 50% of the data set. The line in the rectangle shows
the median and the horizontal lines on either side show the greatest value and least
value, excluding outliers. Dots represent those who consumed a lot less than normal
(outliers).
1990s, with strong economic growth in many countries particularly in major rice
consuming countries such as China and India, the global rice consumption per
capita had the upward trend from 58 kg per year in 1960’s to 71 kg per year in
1990’s (Fig. 1(a)). For the past two decades, global rice consumption per capita
has been ﬂat between 2001 and 2007 due to severe drought in China, India
and Vietnam [14]. The resulted situation led India and Vietnam to announce
the ban on non-Basmati rice export since 2007 and world rice prices so rapidly
increased to record levels in 2008 that we called this situation as “the global
rice crisis” [17]. After 2010, the trend of global rice consumption per capita kept
growing. Figure 1(b) plots the demand for rice consumption per capita. It shows
that change in price and that in quantity move in the same direction because rice
is a staple food according to Giﬀen’s paradox that we mentioned in the previous
section. Furthermore, we found that a positive income elasticity of demand for
rice is associated with normal and necessary goods; an increase in income will
lead to an increase in rice consumption (Fig. 1(c)).

The Future of Global Rice Consumption
635
Table 2. Summary statistics of main variables
Variable
# of countries Min
Max
Median Mean Std. dev.
(a) Rice consumption per capita growth rate
Low income
10
–61.033 182.367 1.063
4.781
28.501
Lower middle-income
21
–80.544 348.303 0.378
5.217
30.614
Upper middle-income 19
–65.002 329.195 0.783
3.561
24.788
High income
23
–66.574 337.716 1.030
2.099
19.841
Total
4 (73)
–18.054
16.417 0.188
0.226
4.638
(b) GDP per capita growth rate
Low income
10
–50.319
72.489 3.635
4.649
13.906
Lower middle-income
21
–62.313 115.362 5.806
6.128
14.023
Upper middle-income 19
–65.918 100.541 7.063
7.403
14.348
High income
23
–54.225
53.761 7.564
8.180
11.456
Total
4 (73)
–19.157
24.893 5.989
6.151
7.186
(c) Rice export price growth rate
Thai WR5%
–40.634 123.702 2.993
6.121
28.816
Source: Calculation
Shown in Fig. 2 are rice consumption per capita classiﬁed by income level.
We found that the upper middle-income countries have the highest amount of
rice consumption per capita where rice is the main diet particularly China and
India. Lower middle-income countries are the second largest rice consumer. The
third largest demand for consumption is in low income countries because the
population face with limited income and the high income countries is the minimal
demand for rice consumption due to the diﬀerent pattern of diet.
Table 2 presents descriptive statistics of variables by income levels to be used
for estimation. We found that the rice consumption per capita growth in the
low and lower middle-income groups have a larger average growth and higher
volatility than the upper middle- and high income groups. It can be implied
that the trend of rice consumption in the latter two groups may increase. The
low and lower middle-income countries apparently have GDP per capita growth
lower than the others which leads to the issue whether or not they will have
suﬃcient food for consumption in the future. Therefore, this research aiming
at forecasting the rice consumption per capita is important issue for dealing
with this.
3.2
Methodology
In this section, we provide brief information regarding unit root tests and
dynamic panel regression with penalised ﬁxed eﬀects model. The details of this
study procedure are described as below.

636
D. Sirikanchanarak et al.
3.2.1
Unit Root Tests
The panel unit root test should be conducted before using these data for estima-
tion to avoid spurious regression. In this study, we use two unit root tests which
are based on the cross-sectional independence hypothesis, and they include the
LLC test by Levin et al. [11] and the IPS test by Im et al. [8].
3.2.2
Dynamic Panel
Static panel data model, random and ﬁxed eﬀects panel data models do not
allow us to use observable information of previous periods in the model. However,
many economic issues are dynamic by nature and use panel data structure to
understand adjustment like in our study. The present rice demand depends on the
past demand. Therefore, we conduct dynamic panel data models that use current
and previous information and sensible modeling assumption. In the Arellano-
Bond framework [3], the value of dependent variable in the previous period is
used to predict the current value of the dependent variable. The relationship of
interest can be written as:
yit = x′
itβ1 + yi(t−1)β2 + αi + ϵit
(2)
where yit is the dependent variable for country i at time t, xit are the set of
explanatory variables, yi(t−1) is the previous period of dependent variable, αi
is unobserved country-speciﬁc and time invariant eﬀect with E(αi) = α and
V ar(αi) = σ2
α, and ϵit are assumed to be independently distributed across coun-
tries with zero mean.
From the ﬁxed eﬀects framework, one assumption is the time invariant unob-
served variable is related to the regressors. When unobservables and observables
are correlated, the endogeneity problem occurs that yields inconsistent parame-
ter estimates if we use a conventional linear panel data estimator. One solution
is taking ﬁrst diﬀerence of the relationship of interest in Eq. 2.
Δyit = Δx′
itβ1 + Δyi(t−1) + Δϵit
E(Δyi(t−1)Δϵit) ̸= 0
(3)
where Δyit = yit −yi(t−1), Δyi(t−1) = yi(t−1) −yi(t−2), and Δϵit = ϵit −ϵi(t−1).
We take the ﬁrst diﬀerence of Eq. 2 to get rid of αi, which is correlated with our
regressors. However, we created a new endogeneity problem. In Eq. 3, there is
the correlation between the lagged dependent (Δyi(t−1)) and error term (Δϵit).
Thus, the estimation of Eq. 3 by Ordinary Least Squares (OLS) will be biased
and provide inconsistent result. Arellano and Bond [3] suggest the second lags
(Δyi(t−s) for s ≥2) in level as instrumental variable (z) in Generalized Method
of Moment (GMM) to solve this problem. It can generate the set of moment
conditions which can be written as
E(Δyi(t−2)Δϵit) = 0
E(Δyi(t−3)Δϵit) = 0
...
E(Δyi(t−j)Δϵit) = 0
(4)

The Future of Global Rice Consumption
637
where E[ziΔϵi] = 0 for i = 1, 2, ....N. As to the use of one-step or two-step
estimator, Bond [4] suggested that these GMM estimators have focused on the
results for the one-step estimator rather than the two step estimator. Huang
et al. [7] also mentioned that the dependence of the two-step matrix on estimated
parameters makes the usual asymptotic distribution approximations less reliable
for two-step estimator. Thus, the robust one-step estimator is conducted.
Therefore, we forecast demand for rice per capita growth by using previous
demand for rice, GDP per capita, and rice price as indicators. According to
Eq. 4, we can specify the model as follows
PRCit = β1PRCi(t−1) + β2GDPit + β3RPit + αi + ϵit
(5)
where PRCit is per capita rice consumption of country i at time t, GDPit is
gross domestic product per capita of country i at time t, and RPit is world rice
price at time t.
The actual data used for model selection and estimation are from 1960 to
2013, while 2014 and 2015 are left for forecast evaluation. We calculate the
out-of-sample root mean square error (RMSE) for each case. The calculation of
RMSE can be written as:
RMSE =
h
t=1(
PRCt −PRCt)2
h
(6)
where h is the maximum number of forecasting period; 
PRCt and PRCt are the
forecast and actual value at period t respectively.
4
Empirical Results
In this section, we report the results of the unit root test and the discussion on
the estimation and forecasting of the rice consumption per capita using DPD
model. The LLC and IPS panel unit root tests are employed. The presence of the
unit root cannot be found for all the variables of interest (Table 3). Therefore, we
can conclude that all series are stationary and appropriate for further analysis.
The model speciﬁcation are selected based on Akaike Information Crite-
rion. The estimated results from the DPD regression are shown in Table 4. The
demand for rice consumption depends on previous demand for rice consumption,
GDP, and rice price. Further analysis reveals a negative feedback relationship
between the current and the two last periods rice consumption per capita growth.
It means that an increase in rice consumption per capita growth in the two last
periods may bring about further decrease in rice consumption per capita growth
in the current period. There is a negative relationship between rice consumption
per capita growth and GDP growth in lower and upper middle-income groups,
while the rest are positive relationship. In the case of rice consumption per
capita growth and rice price growth, there is positive relationship in low and
lower middle-income groups, while it is negative for the rest of groups.

638
D. Sirikanchanarak et al.
Table 3. Unit root tests
Series
Levin, Lin & Chu t
Im, Pesaran and Shin W-stat
Statistic Prob. Inference
Statistic Prob. Inference
Low income
%ΔPRCi,t –16.582
0.000
Stationary –17.918
0.000
Stationary
%ΔGDPi,t –10.797
0.000
Stationary –12.687
0.000
Stationary
%ΔPRi,t
–18.864
0.000
Stationary –16.597
0.000
Stationary
Lower middle-income
%ΔPRCi,t –18.942
0.000
Stationary –26.126
0.000
Stationary
%ΔGDPi,t –16.655
0.000
Stationary –16.070
0.000
Stationary
%ΔPRi,t
–27.337
0.000
Stationary –24.052
0.000
Stationary
Upper middle-income
%ΔPRCi,t –17.205
0.000
Stationary –26.741
0.000
Stationary
%ΔGDPi,t –13.906
0.000
Stationary –15.619
0.000
Stationary
%ΔPRi,t
–26.003
0.000
Stationary –22.878
0.000
Stationary
High income
%ΔPRCi,t –18.267
0.000
Stationary –19.333
0.000
Stationary
%ΔGDPi,t
–9.472
0.000
Stationary –10.890
0.000
Stationary
%ΔPRi,t
–20.665
0.000
Stationary –18.181
0.000
Stationary
World
%ΔPRCi,t –10.139
0.000
Stationary –12.859
0.000
Stationary
%ΔGDPi,t
–6.658
0.000
Stationary
–5.609
0.000
Stationary
%ΔPRi,t
–11.931
0.000
Stationary –10.497
0.000
Stationary
Source: Calculation
Table 4. The estimated result from the dynamic panel
Independent
variable
Low income
(1)
Lower middle
(2)
Upper middle
(3)
High
income (4)
World (5)
Constant
5.9297
6.8901
5.3313
3.0664
0.32574
%ΔPRCi,t−2
–0.3556
–0.2608
–0.3283
–0.2814
–0.3143
%ΔGDPi,t−2
0.1131
–0.1186
–0.0409
0.0081
0.0092
%ΔPRi,t−2
0.0674
0.0712
–0.0028
–0.0599
–0.0019
Number of
Obs
500
1,050
950
600
200
Number of
group
10
21
19
12
4
Source: Calculation

The Future of Global Rice Consumption
639
Table 5. Out-of-sample forecast of the dynamic panel data
2014
2015
Forecast
Actual Forecast RMSE Actual Forecast RMSE 2016
2017
Low income
–0.156
–1.253
1.098
0.676
0.404
0.799
–0.953 –2.089
Lower middle
–0.684
–0.773
0.089
0.993
–2.539
2.499
–1.397 –0.473
Upper middle
0.598
–0.904
1.502
–1.374
0.336
1.609
–0.134
0.705
High income
–0.267
–0.664
0.397
–1.337
0.605
1.402
1.056
0.848
World
–0.913
0.204
1.117
–3.198
–0.269
2.217
0.346
0.982
Source: Calculation
Fig. 3. Forecast and actual per capita rice consumption growth rate (— Actual,
· · · Forecast)
Table 5 presents the performance of DPD model by considering root mean
square error (RMSE) and we make prediction through 2017. We can see that
the estimates from DPD model get wider forecasting error in the upper middle-
income group than the rest of the groups. The changes in the rice consumption
per capita growth in the upper middle-income group may be inﬂuenced by other
factors which the model could not capture in this study. Hence, the RMSE in this
case seems to become largest when considering 2 forecasting periods. Figure 3
plots the estimates and forecasts of rice consumption per capita growth based

640
D. Sirikanchanarak et al.
on DPD model compared with the actual series which are classiﬁed into four
income groups. Moreover, the model performs relatively well for short period
forecasting. Therefore, in general it can be concluded that the DVD model can
be used to forecast rice consumption per capita growth quite well. However, for
the rest of income groups, the predictive value is quite well based on RMSE.
Thus, we can conclude that the DPD model is proper for estimating the per
capita rice consumption growth.
Lastly, we use the model to forecast the rice consumption per capita growth
in the two next years. Our projections indicate that the global rice consumption
per capita growth of 73 countries has tendency to slowdown to 0.346% and
0.982% in 2016 and 2017, respectively due to decreasing rice consumption per
capita growth in low income countries, lower middle-income countries, and upper
middle-income countries. Conversely, the rice consumption per capita growth
seem to increase in high income countries in the next two periods. While, FAO
[6] predicted the rice consumption per capita growth rate in 2016 and 2017 are
0.0% and 0.18%, respectively. Abdullah et al. [1] predicted that 55.2–62.7 kg in
2015 and declined to 49.1–61.2 kg in 2025 and 42.4-58.1 kg in 2050. Timmer et al.
[20] forecasted the total rice consumption trend to decrease since 2030 to 2050
while USDA [22] predicted that will slowdown in two next years. The results of
all studies above revealed that the trend of world rice consumption is slowdown.
However, we believe that our estimate model has good validation because we
gain information from panel data and the performance of our model is more
accurate.
5
Policy Implications
According to our ﬁndings, the factors of past rice consumption per capita growth,
GDP growth, and world rice price growth can be used as leading indicators to
forecast rice consumption per capita growth. Hence, policy makers including
stakeholders in the rice related industries should attend to these factors. The
negative relationship between the per capita rice consumption growth in this
year (%ΔPCRt) and that in the year before last year (%ΔPCRt−2) indicates
that the future of rice dietary trends is the decline in rice consumption per capita.
These results are supported by the expert opinions from FAO and IRRI. The
trend of health concern is rising leading to decreasing carbohydrate intakes. In
addition, a report from World Bank [18] shows a sign of aging society because
of the declining in population growth among rice-eating countries. It was argued
that rapidly aging population will require less food per capita due to the natural
decline in metabolic needs. Hence with these reasons, the development in the
rice exporting countries should concentrate primarily on maintenance of existing
yields including improving nutritional quality of rice in line with the trend of
health concern and aging society such as organic rice. The agricultural policy
should also encourage rice farmers to adjust their farming system to meet the
needs of the global market. Lastly, the government should support the cross-
disciplinary processing of rice for medical, industrial, energy uses, and etc.

The Future of Global Rice Consumption
641
6
Conclusions
This paper investigates the future of global rice consumption per capita growth
using the dynamic panel data model. We separate countries into 4 groups based
on income level classiﬁed by the World Bank. The panel data as a whole covers 73
countries from 1961 to 2015. We employ three key factors having eﬀect on that–
past rice consumption per capita growth, per capita GDP growth, and world rice
price growth. The empirical results reveal that there is a negative relationship
between current and previous rice consumption per capita growth. Trend of the
demand for rice consumption may slowdown because of declining per capita rice
consumption growth rate. The forecasting performance of the model performs
quite well based on RMSE specially, in short period forecasting.
Lastly, we would recommend some policies for policy makers and stakehold-
ers in rice related industries. The policy makers should develop agricultural
strategies such as promoting the policies which support the changing of dietary
patterns and the long run sustainable economic growth. For instance, the policy
should concentrate on improving rice quality, encouraging alternative crops to
rice farmers, and supporting the cross-disciplinary processing of rice. All of these
suggestions would help increase the value addition and price of rice products.
Furthermore, the beneﬁts from these policies would help keep purchasing power
of rice farmers when the demand for rice consumption tends to decrease.
Acknowledgements. The ﬁrst author is grateful to the full scholarship from the
Bank of Thailand. In addition, she would like to express much of her appreciations to
Mr. Tanarat Rattanadamrongaksorn for his encouragement in bringing this interesting
issue to our attention. Also, the second and third authors wish to thank the Puey
Ungphakorn Centre of Excellence in Econometrics, Faculty of Economics, Chiang Mai
University for giving them ﬁnancial supports.
References
1. Abdullah, A.B., Ito, S., Adhana, K.: Estimate of rice consumption in Asian coun-
tries and the world towards 2050. In: Proceedings for Workshop and Conference
on Rice in the World at Stake, vol. 2, pp. 28–43 (2006)
2. Alexandratos, N., Bruinsma, J., et al.: World agriculture towards 2030/2050: the
2012 revision. Technical report, ESA Working paper Rome, FAO (2012)
3. Arellano, M., Bond, S.: Some tests of speciﬁcation for panel data: Monte Carlo
evidence and an application to employment equations. Rev. Econ. Stud. 58(2),
277–297 (1991)
4. Bond, S.R.: Dynamic panel data models: a guide to micro data methods and prac-
tice. Port. Econ. J. 1(2), 141–162 (2002)
5. Carstensen, K., Toubal, F.: Foreign direct investment in Central and Eastern Euro-
pean countries: a dynamic panel analysis. J. Comp. Econ. 32(1), 3–22 (2004)
6. Food and Agricultural Organization of The United Nation (FAO): Food outlook
(2017). http://www.fao.org/3/a-i7343e.pdf
7. Huang, B.N., Hwang, M.J., Yang, C.W.: Causal relationship between energy con-
sumption and GDP growth revisited: a dynamic panel data approach. Ecol. Econ.
67(1), 41–54 (2008)

642
D. Sirikanchanarak et al.
8. Im, K.S., Pesaran, M.H., Shin, Y.: Testing for unit roots in heterogeneous panels.
J. Econom. 115(1), 53–74 (2003)
9. Judson, R.A., Owen, A.L.: Estimating dynamic panel data models: a guide for
macroeconomists. Econ. Lett. 65(1), 9–15 (1999)
10. Kubo, M., Purevdorj, M., et al.: The future of rice production and consumption.
J. Food Distrib. Res. 35(1), 128–142 (2004)
11. Levin, A., Lin, C.F., Chu, C.S.J.: Unit root tests in panel data: asymptotic and
ﬁnite-sample properties. J. Econom. 108(1), 1–24 (2002)
12. Lewbel, A.: Engel curves. In: The New Palgrave Dictionary of Economics, 2 edn.
(2008)
13. Lio, M.C., Liu, M.C., Ou, Y.P.: Can the internet reduce corruption? A cross-
country study based on dynamic panel data models. Gov. Inf. Q. 28(1), 47–53
(2011)
14. Mohanty, S.: Trends in global rice consumption. Rice Today 12(1), 44–45 (2013)
15. Nicholson, W., Snyder, C.M.: Intermediate Microeconomics and Its Application.
Cengage Learning (2014)
16. Sequeira, T.N., Ma¸c˜as Nunes, P.: Does tourism inﬂuence economic growth? A
dynamic panel data approach. Appl. Econ. 40(18), 2431–2441 (2008)
17. Sirikanchanarak, D., Liu, J., Sriboonchitta, S., Xie, J.: Analysis of transmission
and co-movement of rice export prices between Thailand and Vietnam, pp. 333–
346 (2016)
18. Smil, V., et al.: Feeding the world: how much more rice do we need. In: Rice is
Life: Scientiﬁc Perspectives for the 21st Century, pp. 21–23 (2005)
19. The International Rice Research Institute (IRRI): Bigger harvest a cleaner planet
(2000). http://www.irri.org/publications/annual/pdfs/ar2000/biggerharvests.pdf
20. Timmer, C.P., Block, S., Dawe, D.: Long-run dynamics of rice consumption, 1960–
2050. In: Rice in the Global Economy: Strategic Research and Policy Issues for
Food Security, pp. 139–174 (2010)
21. United Nations: World population to 2300. United Nations, New York (2004)
22. United States of Agricultural Department (USDA): Rice: world markets and trade
(2017). https://apps.fas.usda.gov/psdonline/circulars/grain.pdf
23. Zhang, C., Zhuang, L.: The composition of human capital and economic growth:
evidence from china using dynamic panel data analysis. China Econ. Rev. 22(1),
165–171 (2011)

The Analysis of the Eﬀect of Monetary Policy
on Consumption and Investment in Thailand
Jirawan Suwannajak1, Woraphon Yamaka1,2, Songsak Sriboonchitta1,2,
and Roengchai Tansuchat1,2(B)
1 Faculty of Economics, Chiang Mai University, Chiang Mai 50200, Thailand
Suwannajak.j@gmail.com, woraphon.econ@gmail.com, roengchaitan@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 50200, Thailand
Abstract. This study highlights on the analysis of Thai monetary policy
transmission channels, i.e. interest rate, credit, exchange rate, and asset
price channels, to private consumption and private investment. The ana-
lytical methods are Time Varying Parameter Vector Autoregressive (TVP-
VAR) with stochastic volatility, and its impulse response function. The
results showed that the credit channel contribute the greatest impact on
private consumption and investment. We also found that the eﬀect of mon-
etary policy to private consumption and investment are vary over time.
1
Introduction
Stabilizing and driving economic growth are targets of economic development
in many countries. There are various factors aﬀecting economic growth. Two of
the important factors that drive economic growth are private consumption and
investment [10] which are viewed as demand side factors [21]. Figure 1 shows
the percentage of Thai private consumption which is about 11% of gross domes-
tic product (GDP), while the percentage of Thai private investment is 45% of
gross domestic product (GDP). Thus, it is important to stabilize these two fac-
tors. And, the most powerful tool of policy makers and government is monetary
policy [2].
Monetary policy is implemented by central banks to stimulate or raise domes-
tic private consumption and investment. To maintain the appropriate level of
economic growth, production and inﬂation, the central bank needs to adjust
its ﬁnancial liquidity and interest rate through monetary instruments, such as
reserve requirements and open market operations. The most important chan-
nels for adjusting the money supply are interest rate, bank deposit rate, long-
term interest rates, credit expansion and exchange rate. Pootrakool and Sub-
haswadikul [22] suggested that these tools can stabilize price level and contribute
to economic growth.
Rational Expectation is introduced in a standard New Keynesian model.
Lucas [15] explained that the rational expectations hypothesis implies that every
economic agent makes optimal use of information in forming expectations [8].
This hypothesis follows two assumptions: (i) the average estimate of the outcome
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_46

644
J. Suwannajak et al.
private 
consumption, 
15.71%
government 
expenditure, 
17.14%
private 
investment, 
64.29%
net export, 
2.86%
Fig. 1. Structure of Thai GDP Source: Oﬃce of the National Economic and Social
Development Board, 2016
by participants in the market is correct and based on correct probability distri-
butions, (ii) forecasting errors are uncorrelated in successive time periods [16].
While the assumption of rational expectations has little empirical implication,
it has startling implications for eﬀectiveness of monetary policy [17]. Firstly,
Sargent and Wallace [23] showed that under the rational expectation the eﬀect
of a change in money on economic activity would only be temporary. In addition,
if the rational expectations and not adaptive expectations are included in the
analysis of Fisher nominal interest rate relation, the eﬀect of the money has no
lasting eﬀect to the economy [4].
Secondly, Sargent and Wallaces [24] showed that changing the paths of out-
put and employment are not aﬀected by systematic monetary policy. However,
the systematic monetary policy aﬀects to expected inﬂation but has no aﬀect
to unemployment [17]. This is because rational economic agents expect the
eﬀect of the policy action and adopt it part of the information set on decision
making [12]. Thirdly, Lucas [15] showed that in a business cycle framework of
imperfect information, market clearing and rational expectations, monetary pol-
icy will only be eﬀective as long as it distorts relative prices [16]. Therefore, by
conducting of monetary policy must consider agent expectation. This is because
result of rational expectation eﬀect on eﬀective of monetary policy and the
impact to real economic sector [7].
There were numerous studies dealing with the transmission of monetary pol-
icy to economic growth, for examples, the studies by Kim [13], Angeloni and
Kashyap [1], Benmank and Blinder [3], Petra [21], Fujiwara [9], and Pastpipatkul
et al. [20]. Although these researches provided a better theoretical understanding,
there is a scarcity in the number of empirical researches attempting to analyze
how economic growth reacts to transmission of monetary policy over time.
To investigate the eﬀective monetary policy transmission channels to private
consumption and investment in Thailand, we need to understand the mechanisms
of monetary policy transmission which can be distinguished into four channels
i.e. credit channel, asset price channel, exchange rate channel, and interest rate
channel (Fig. 2).

The Analysis of the Eﬀect of Monetary Policy
645
Fig. 2. The channel to transmission of monetary policy
Source: Bank of Thailand,
2012
The objectives of this study are investigate the eﬀects of the four monetary
policy transmission channels-credit, asset price, exchange rate, and interest rate-
on private consumption and investment in Thailand over time, and conduct
impulse response analysis for ﬁnding an explanation for the movement of the
private consumption and investment when it comes as a shock to the system
through diﬀerent monetary policy transmission channels.
To achieve our objectives, we employ the Time Varying Parameter-Vector
Autoregressive (TVP-VAR) with stochastic volatility model in order to capture
the time-varying eﬀects of monetary policy transmission on private consumption
and private investment, and apply Bayesian estimation to estimate the unknown
parameters in the model. Our concern is that the limited data sets in our study
may lead a VAR model to face a degree of freedom problem. Van de Schoot
et al. [26] suggested that Bayesian estimation can provide reliable results when
sample size is small. It is more informative, ﬂexible, and eﬃcient than MLE
and the implementation of Bayesian prior on the parameter of the model can
reduce the estimation uncertainty and to obtain accurately the inference and the
forecast [19].
The next section brieﬂy summarizes some of the methodologies used in this
study. In Sect. 3, we describe our data. The results are reported in Sect. 4. Finally,
Sect. 5 provides the conclusion and policy suggestions.
2
Methodology
2.1
Bayesian VAR Models
To study the macroeconomic data, VAR model is often employed. However,
the over-parameterization often exists in the estimation of VAR due to a large
number of parameter estimates with too few observations. Doan et al. [6]; Sims
and Zha [25] introduced estimating VAR model using Bayesian approach called
BVAR model. Thus, BVAR has become popular in the macroeconomic studies.

646
J. Suwannajak et al.
Similar to VAR formula, the BVAR(p) model can be written as
Yi,t =
p

p=1
βpYi,t−p + εt
(1)
where Yi,t is a k × 1 of yi,t which has a observation at time t = (1, ..., T) for
ith variable. Yt−p is a lag of Yt this describes each variable with its own lag p
variable and other variables based on lag p. βj is K ×K matrix of autoregressive
coeﬃcients, a0 is the intercept term, while εt stands for the white noise processes
of a zero-mean vector with positive deﬁnite contemporaneous covariance matrix
Σ and zero covariance matrices at all the other lags.
2.2
Time Varying Parameter-Vector Autoregressive with Stochastic
Volatility
The TVP-VAR with stochastic volatility model can be used to capture the poten-
tial time-varying nature of the underlying structure in any macroeconomic seg-
ment of interest in a ﬂexible and robust manner. This model allows the shift in
the parameters over time. Following [5,11,18] the Time-Varying VAR model can
be written as
Yi,t =
P

p=1
βi,tYi,t−p + εt, εt ∼N(0, Σt,1)
(2)
Time varying equation is
βi,t = Fβi,t−1 + ut, ut ∼N(0, Σ2)
(3)
Stochastic volatility equation is
Σt,1 = γ(ht), ht+1 = φht+1 + ηt, ηt ∼N(0, Σ3)
(4)
where Yi,t is a vector of endogenous variables, βi,t is a matrix of time varying
parameters; Σt,1 is matrix of time varying standard deviation; εt is the error
of the mean Eq. (2); F is a time varying coeﬃcient of βi,t−1; ut is error of the
time varying Eq. (3). ht is stochastic volatility and ηt is error of the stochastic
volatility Eq. (4). Here, we assume γ > 0 to guarantee that Σt is non-negative.
To estimate these parameters, a Gibbs sampler is employed in this estimation.
(See [5,18]).
2.3
Prior, Likelihood, Posterior
In the Bayesian approach, to estimate the unknown parameters of the model,
θ = {Σ, F, γ, φ, c} we need to construct a posterior density of the model which
is derived from
g(θ |y ) ∝f(y |θ)p(θ),
(5)

The Analysis of the Eﬀect of Monetary Policy
647
where, f(y |θ) is a density function or likelihood function and p(θ) is a prior
information on θ which is assumed to be available in the form of density function
after having updated by looking at the data. To estimate both BVAR and TVP-
BVAR models, we conduct a Gibbs sampler where the prior is assumed to be
normal-Wishart prior for BVAR model (see [14]). For TVP-BVAR model, the
prior distributions proposed in this paper are chosen as follows: First of all, the
priors for Σ is assumed to be inverse-Wishart. The priors for time varying F, γ,
and φ are assumed to have normal distribution. Then, we generate the samples
from the joint posterior distribution of the coeﬃcients θn = {Σn, F n, γn, φn} by
Gibbs sampling. Blocking Gibbs sampler is conducted here since it is easy to
draw g(θ |y ). Gibbs sampling is carried out in ﬁve steps, yielding the following
algorithm (see [18]).
(1) Set the initial start for

Σ0, F 0, γ0, φ0
(2) Draw Σn from g(Σn−1, F n−1, γn−1, φn−1 |y )
(3) Draw F n from g(Σn, F n−1, γn−1, φn−1 |y )
(4) Draw γn and φn from g(Σn, F n, γn−1, φn−1 |y )
(5) Repeat step (2)
These draws can be used as the basis for making inferences by appealing to
suitable ergodic theorems for Markov chains. To derive the conditional posterior
of each block, our study refers to Nakajima [18]. By using a standard Gibbs
sampler algorithm for a given prior p(θ), we use the average of the Markov chain
on θ as an estimate of θ. For the number of draws, we will repeat the algorithm
until the posterior looks stationary and close to normal.
2.4
Macroeconomic Speciﬁcation
From Fig. 2 and VAR Model, the relationship among 6 variables are speciﬁed as
follow:
PCIt = a1 +
n

i=1
A1iPCIt−i+
n

i=1
B1iPIIt−i+
n

i=1
C1iIRt−i
+
n

i=1
D1iCRt−i+
n

i=1
E1iSETt−i+
n

i=1
F1iREERt−i + ε1t
(6)
PIIt = a2 +
n

i=1
A2iPCIt−i+
n

i=1
B2iPIIt−i+
n

i=1
C2iIRt−i
+
n

i=1
D2iCRt−i+
n

i=1
E2iSETt−i+
n

i=1
F2iREERt−i + ε2t
(7)

648
J. Suwannajak et al.
IRt = a3 +
n

i=1
A3iPCIt−i+
n

i=1
B3iPIIt−i+
n

i=1
C3iIRt−i
+
n

i=1
D3iCRt−i+
n

i=1
E3iSETt−i+
n

i=1
F3iREERt−i + ε3t
(8)
CRt = a4 +
n

i=1
A4iPCIt−i+
n

i=1
B4iPIIt−i+
n

i=1
C4iIRt−i
+
n

i=1
D4iCRt−i +
n

i=1
E4iSETt−i+
n

i=1
F4iREERt−i + ε4t
(9)
SETt = a5 +
n

i=1
A5iPCIt−i+
n

i=1
B5iPIIt−i+
n

i=1
C5iIRt−i
+
n

i=1
D5iCRt−i+
n

i=1
E5iSETt−i+
n

i=1
F5iREERt−i + ε5t
(10)
REERt = a6 +
n

i=1
A6iPCIt−i+
n

i=1
B6iPIIt−i+
n

i=1
C6iIRt−i
+
n

i=1
D6iCRt−i+
n

i=1
E6iSETt−i+
n

i=1
F6iREERt−i + ε6t
(11)
where PCIt is private consumption index, PIIt is private investment index,
IRt is interbank rate, CRt is bank deposit rate, SETt is Stock Price Index of
Thailand, REERt is Real Eﬀective Exchange Rate, which has a observation at
time t = (1, ..., T). a is constant. A B C D E and F is coeﬃcients of lag. εt is
stands for the white noise processes of a zero-mean vector with positive deﬁnite
contemporaneous covariance matrix Σ and zero covariance matrices at all the
other lags.
3
Data
3.1
Descriptive Statistic
In this study, we deal with growth rate of six variables which are private invest-
ment index (PII), private consumption index (PCI), commercial bank credit
(CR), interbank rate (IR), exchange rate (REER), Stock Exchange of Thai-
land (SET). All variables are monthly data and collected from January, 2000 to
December, 2016. The data are transform to the growth rate in order to avoid
the nonstationary problem. The descriptive statistics are given in Table 1. The
table shows that PCI has the highest standard deviation (3.8956), while CR has
the lowest (0.0148). The Jarque-Bera statistic indicates that all series are not
normally distributed became they reject the null hypothesis. The Augmented
Dickey-Fuller (ADF) test at level with none, intercept, trend and intercept is
also applied to check unit roots in the series. Unit root test for each variable has
a statistical signiﬁcance level of 0.01. Therefore, these variables are stationary.

The Analysis of the Eﬀect of Monetary Policy
649
Table 1. Lag length criteria BVAR
Variable
PII
PCI
IR
CR
REER
SET
Mean
0.0023
0.3716
0.0088
0.0049
−9.55E−05
0.0045
Max
0.0927
52.950
1.052
0.0685
0.0797
0.4059
Min
−0.468
−4.6969
−0.3397
−0.0594
−0.2335
−0.8175
Std. Dev.
0.0378
3.8956
0.1417
0.0148
0.0215
0.0963
Skewness
−9.5475
12.3456
3.8285
−0.0089
−5.9532
−2.5922
Kurtosis
119.746
165.523
27.1253
6.98066
70.7364
30.4060
Jarque-Bera
118368
228574.9
5418.94
134.031
40007.79
6580.348
Probability
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
ADF-test None
(−12.21)*** (−14.26)*** (−13.75)*** (−3.31)*** (−16.76)*** (−13.88)***
Intercept
(−12.22)*** (−14.36)*** (−13.76)*** (−5.26)*** (−16.72)*** (−13.89)***
Trend and
Intercept
(−12.19)*** (−14.63)*** (−13.72)*** (−5.40)*** (−16.69)*** (−13.94)***
Source: Calculation.
Note: *, **, and *** denote signiﬁcant at 10%, 5%, and 1%, respectively
4
Results
Prior to analyzing the eﬀect of monetary policy transmission through diﬀerent
channels on private consumption and investment in Thailand, we check the order
of lag of our data from BVAR model. We conduct a Akaiki information criterion
(AIC), Schwarz criterion (SQ), and Hannan–Quinn information criterion (HQ).
The results indicate that lag 3 is better only for AIC, for other lag 1 is better
(Table 2).
Table 2. Lag length criteria BVAR
Model
Lag AIC
SC
HQ
BVAR.lag 1 1
−21.0853
−20.38034* −20.79987*
BVAR.lag 2 2
−21.29097
−19.98177
−20.76089
BVAR.lag 3 3
−21.33768* −19.42424
−20.56295
BVAR.lag 4 4
−21.16719
−18.64949
−20.1478
Source: Calculation.
4.1
Time Varying Eﬀect of Monetary Policy
The results of the eﬀect of monetary policy transmission through diﬀerent chan-
nels on private investment and private consumption are illustrated in Figs. 3
and 4, respectively. The graph plot a matrix of Time Varying Parameters,
βi,t, from our TVP-VAR model. We found that the eﬀect of (CR), which is
denoted as credit channel, is positive to private investment and the range of the
eﬀect is around 0.0362 to 0.0368 (Fig. 3(a)). We observe that the eﬀect increases
since year 2011 to late 2012. After that the size of positive eﬀect has gradually
decreased. We expect that the European ﬁnancial crisis in 2012 led the eﬀect

650
J. Suwannajak et al.
Fig. 3. Time varying eﬀect of each channel on private investment index
Source:
Calculations
of CR on private investment to become smaller. The reason is that the ﬁnan-
cial institutions and investors had less conﬁdence in the economy and thereby
retarding their investment decision.
For the interest rate (IR), we refer to it as expectation channel. The eﬀect of
IR on private investment is diﬀerent from CR, see Fig. 3(b). We found that IR has
a negative eﬀect on private investment. We observed an interesting result in this
eﬀect. The negative eﬀect of IR on investment is high during 2013–2014, which
corresponds to the economic downturn in Europe. In times of economic downturn,
the lower interest rates could encourage additional investment spending.

The Analysis of the Eﬀect of Monetary Policy
651
Fig. 4. Time varying eﬀect of each channel on private consumption index
Source:
Calculations
Consider the eﬀect of the exchange rate (REER) and SET, which represent
exchange rate and asset price channel, respectively. Both channels show the
positive eﬀect on private investment where the time varying eﬀect of exchange
rate channel has the range between 0.0010 and 0.0014 and the range between
0.107 and 0.103 for asset price channel (see Fig. 3(c) and (d)). In the case of
exchange rate channel, we can observe a huge eﬀect of exchange rate on private
investment in late 2013.
This means that an increase in THB/USD (depreciation) would make exports
more competitive and appear cheaper to foreigners. This would increase demand
for exports and investment. We found that this period corresponds to the eco-
nomic recovery in USA which is an important and large trading partner of
Thailand. Therefore, the demand for Thai export products increased during
this period.
For the eﬀect of monetary policy transmission on private consumption, the
results of Fig. 4 show that most monetary policy transmission channels generated
positive eﬀect except for asset price channel (SET). We also observe that the size

652
J. Suwannajak et al.
of eﬀect of these channels gradually increases overtime. Whereas, it is noteworthy
that credit (CR) channel has higher ﬂuctuation than the others.
4.2
Impulse Response
Finally, the responses of the private investment and private consumption to
shocks in monetary policy transmission channels for over 5 months are illustrated
in Figs. 5 and 6.
4.2.1
The Eﬀect of Monetary Policy Transmission on Private
Investment
Consider the interest rate channel, private investment index (PII) shows a small
ﬂuctuating response to interbank rate (IR) in the ﬁrst four months and converged
to equilibrium within 5 months. For credit channel, we found that PII responded
to commercial bank rate (CR) shock with a small positive direction. We expect
that ﬁrms can raise capital from sources other than commercial banks so that
the shock of this channel did not aﬀect investment (Fig. 5(b)).
In exchange rate channel, Fig. 5(c) shows that PII responded to the shock
of real eﬀective exchange rate (REER) in the positive rather than the negative
direction. We observe that the shock of REER is not persistent. It has a positive
response to the shock of REER and then falls sharply and reaches its minimum
after about 3 months. However, the shock moved to equilibrium within 5 months.
For asset price channel in Fig. 5(d), it shows that SET index (SET) creates a
positive sharp-shaped response in PII, following which it begins to decrease and
eventually overshoots, thereby leading to a decrease in PII about 4–5 months
later. Moreover, when we compare the eﬀects of all channels on private invest-
ment, credit channel apparently had the most impact on private investment.
Fig. 5. Impulse response of private investment index to shock of monetary policy trans-
mission channel Source: Calculations

The Analysis of the Eﬀect of Monetary Policy
653
4.2.2
The
Eﬀect
of
Monetary
Policy
Transmission
on
Private
Consumption
Figure 6 also presents the impulse response function for the changes in private
consumption to a shock of monetary policy transmission. The results illustrated
that the shock of interbank rate (IR) causes the private consumption index (PCI)
to fall initially before returning to the equilibrium within 4–5 months. For the
shock of commercial bank rate (CR), the response of the private consumption
is limited. This indicated that any shocks of CR will not lead to the private
consumption change.
Fig. 6. Impulse response of private consumption index to shock of monetary policy
transmission channel Source: Calculations
Finally, consider the shock of real eﬀective exchange rate (REER) and SET
index (SET). The result depicted that the response of PCI is quite ﬂuctuat-
ing since there are both positive and negative directions, see Fig. 6(c) and (d),
respectively. In addition, when we compare the eﬀects of all channels on private
consumption, it is concluded that the most impact to private consumption is
from credit channel.
5
Conclusion
Monetary policy is implemented to achieve macroeconomic target. It is neces-
sary for policy makers to understand the eﬀect of monetary policy transmitted
through diﬀerent channels on speciﬁc macroeconomic variables. The monetary
policy transmission channels i.e. interest rate, exchange rate, asset price, and
credit, can play an important role in monetary policy transmission. In this study,
we employ a TVP-VAR model and estimate the unknown parameters using
Bayesian approach. Our model has an ability to predict how possible changes in

654
J. Suwannajak et al.
policy can aﬀect the economy, and our result can help the government decide on
the best monetary policy.
Our study points out that although credit channels it will have an impact
on private consumption and investment faster than other channels, but it also
aﬀect volatile private consumption and investment. Therefore, the central bank
should consider the transmission of monetary policy in other channels, such as
the asset price channel, policy maker should develop a stock market to be more
eﬀective such as the dissemination of information on asset prices. To quality
and transparent including money supply control in the system. To be according
with the needs of the people. To reduce volatility of stock prices, which aﬀect
private consumption index. This will provide a great beneﬁt for investment. In
addition, the credit channel should be monitored since the private investment
and consumption are very sensitive to the change in commercial banks’ interest
rates.
References
1. Angeloni, I., Kashyap, A.K., Mojon, B. (eds.): Monetary Policy Transmission in
the Euro Area: A Study by the Eurosystem Monetary Transmission Network. Cam-
bridge University Press, Cambridge (2003)
2. Banuso, F.B., Odior, E.S.: Macroeconomic volatility and government consumption
expenditure: implication for public welfare: a dynamic macroeconometric stochas-
tic model. Int. J. Econ. Fin. 4(2), 140 (2012)
3. Bernanke, B.S., Blinder, A.S.: The federal funds and the channels of monetary
transmission. Am. Econ. Rev. 82, 901–921 (1992)
4. Blanchard, O.: What do we know about macroeconomics that Fisher and Wicksell
did not? Q. J. Econ. 115, 1375–1408 (2000)
5. Del Negro, M., Primiceri, G.E.: Time-varying structural vector autoregression and
monetary policy: a corrigendum. Federal Reserve Bank of New York StaﬀReport
619 (2013)
6. Doan, T., Litterman, R., Sims, C.: Forecasting and conditional projection using
realistic prior distributions. Econometric Rev. 3, 1–144 (1984)
7. Eusepi, S., Giannoni, M.P., Preston, B.J.: Long-term debt pricing and monetary
policy transmission under imperfect knowledge (2012)
8. Friedman, M.: Monetary policy: theory and practice. J. Money Credit Bank. 14(1),
98–118 (1982)
9. Fujiwara, I.: Output composition of the monetary policy transmission mechanism
in Japan. Topics in Macroecon. 4(1) (2004)
10. Heim, J.J.: The impact of consumer conﬁdence on consumption and investment
spending. J. Appl. Bus. Econ. 11(2), 37 (2010)
11. Kaewsompong, N., Sriboonchitta, S., Maneejuk, P., Yamaka, W.: Relationships
among prices of rubber in ASEAN: Bayesian structural VAR model. Thai J. Math.
101–116 (2016)
12. Kantor, B.: Rational expectations and economic thought. J. Econ. Lit. 17(4), 1422–
1441 (1979)
13. Kim, H.E.: Was the Credit a Key Monetary Transmission Mechanism Following
The Financial Crisis in The Republic of Korea? The World Bank Policy Working
Paper No. 2013 (1999)

The Analysis of the Eﬀect of Monetary Policy
655
14. Koop, G., Korobilis, D.: Bayesian multivariate time series methods for empirical
macroeconomics. Found. Trends Econometrics 3(4), 267–358 (2010)
15. Lucas, R.E., Sargent, T.J. (eds.): Rational Expectations and Econometric Practice,
vol. 2. University of Minnesota Press, Minneapolis (1981)
16. Macesich, G.: Monetary Policy and Rational Expectations. Praeger Publishers,
New York (1987). 154 pages
17. Mankiw, N.G.: A quick refresher course in macroeconomics. J. Econ. Lit. XXVIII,
1645–1660 (1990)
18. Nakajima, J.: Time-Varying Parameter VAR model with stochastic volatility: An
overview of methodology and empirical applications (No. 11-E-09). Institute for
Monetary and Economic Studies, Bank of Japan (2011)
19. Pastpipatkul, P., Yamaka, W., Wiboonpongse, A., Sriboonchitta, S.: Spillovers of
quantitative easing on ﬁnancial markets of Thailand, Indonesia, and the Philip-
pines. In: International Symposium on Integrated Uncertainty in Knowledge Mod-
elling and Decision Making, pp. 374–388. Springer, Cham, October 2015
20. Pastpipatkul, P., Yamaka, W., Sriboonchitta, S.: Eﬀect of quantitative easing on
ASEAN-5 ﬁnancial markets. In: Causal Inference in Econometrics, pp. 525–543.
Springer International Publishing (2016)
21. Petra, G.K.: European Central Bank Working Paper Series: “Interest Rate Reac-
tion Function and The Taylor Rule in The Euro Area” (2003). www.Sciendirect.
com
22. Pooptakool, K., Subhaswadikul, M.: The monetary transmission mechanism. In:
Bank of Thailand Research Symposium (2000)
23. Sargent, T.J., Wallace, N.: Rational expectations and the dynamics of hyperinﬂa-
tion. Int. Econ. Rev. 14, 328–350 (1973)
24. Sargent, T.J., Wallace, N.: “Rational” expectations, the optimal monetary instru-
ment, and the optimal money supply rule. J. Polit. Econ. 83(2), 241–254 (1975)
25. Sims, C., Zha, T.: Bayesian methods for dynamic multivariate models. Int. Econ.
Rev. 39, 949–968 (1998)
26. Van De Schoot, R., Broere, J.J., Perryck, K.H., Zondervan-Zwijnenburg, M., Van
Loey, N.E.: Analyzing small data sets using Bayesian estimation: the case of post-
traumatic stress symptoms following mechanical ventilation in burn survivors. Eur.
J. Psychotraumatol. 6(1), 25216 (2015)

Investigating Relationship Between Gold Price
and Crude Oil Price Using Interval Data
with Copula Based GARCH
Teerawut Teetranont1(B), Somsak Chanaim1,2, Woraphon Yamaka1,2,
and Songsak Sriboonchitta1,2
1 Faculty of Economics, Chiang Mai University, Chiang Mai 52000, Thailand
teetranont@gmail.com, somsak ch@cmu.ac.th, woraphon.econ@gmail.com,
songsakecon@gmail.com
2 Center of Excellence in Econometrics, Chiang Mai University,
Chiang Mai 52000, Thailand
Abstract. This study investigates and compares the performance
of center method, equal weighted convex combination and unequal-
weighted convex combination methods through various GARCH and
copula-based approaches for the analysis of relationship between gold
and crude oil prices using interval data in Comex and Nymex tradings.
The results of this study conﬁrm that unequal-weighted convex combina-
tion method improves the estimation and it tends to perform better than
both the center method and its equal-weighted variant. In addition, the
marginal from the best ﬁt GARCH model is used to measure dependence
via copula function in the form of Student-t copula as selected according
to the lowest AIC among all candidates. Finally, we can conclude that
there exists the dependence between Comex and Nymex not only in the
normal event, but also in the extreme event.
1
Introduction
Commodities, especially gold and crude oil, and bond are important instruments
to diversify the risk. In the calculation of optimum risky weights for portfolio,
gold and crude oil are the most attractive commodities to be included for hedg-
ing risks in portfolio of investors. Gold and crude oil have played the vital role
in economics. In the last decade, many economists paid much attention to inves-
tigating the volatility and the relationship between gold and crude oil prices.
The most eﬀective tool that is employed to measure this volatility and rela-
tion is Copula based GARCH model. The multivariate GARCH models have
demonstrated to be useful and eﬀective for analyzing the pattern of multivariate
random series and estimating the conditional linear dependence of volatility or
co-volatility in diﬀerent markets.
The study of the volatility and the relationship between gold and crude oil
prices, using Copula based GARCH, has been intensively conducted in the last
decade. However, those studies investigated their works using a closing price
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_47

Investigating Relationship Between Gold Price and Crude Oil Price
657
data series. Thus, if we consider only the closing prices, we might lack a valuable
intraday information and the obtained results might not be reasonable [11].
Recently some studies have proposed to use interval data, e.g. the lowest and
the highest price during each day or period of time, as an alternative to single
value data. In the ideal world, we should be able to predict both the lowest
daily price and the highest daily price. However, in practice, this is diﬃcult, so
we would like to predict at least some daily price between these bounds. In the
past, researchers tried to predict the representative of the lowest and highest
prices, for example, a Center and MinMax methods of Billard and Diday [3],
Center and Range method of Neto and Carvalho [10], and model M by Blanco-
Fernndez, Corral, Gonzlez-Rodrguez [4], to deal with the interval data. These
methods aim to construct the model without taking the interval as a whole.
However, we expect that these methods, especially center method, may not be
robust enough to explain the real behavior of the interval data and this may
lead to the misspeciﬁcation of the model. Therefore, it will be of great beneﬁt
to relax this assumption of mid-point of center method and assign appropriate
weights between intervals. Thus in this study, a convex combination method of
Chanaim et al. [6] is employed to obtain the appropriate weights.
This study investigates and compares the dependence structure of crude oil
and gold prices using diﬀerent interval values for copula-based GARCH model
estimation and prediction. The examined interval value methods include the
center method, equal weighted, and unequal-weighted convex combination. The
main ﬁndings will conﬁrm the usefulness of the convex combination in copula-
based GARCH approach for evaluating the relationship, joint distribution and
co-movement between crude oil prices and gold prices for investors whose invest-
ment interest is in gold and crude oil.
The remainder of the paper is organized as follows: Sect. 2 provides method-
ology of study. Section 3 proposes the empirical results. Section 4 summarizes
this paper.
2
Methodology
In this section, we brief the convex combination in GARCH, EGARCH, and
GJR-GARCH models; and copula family for estimating joint density of the
obtained marginal from the GARCH families.
2.1
Operation with Interval Arithmetic
Let pi = [pi, pi] be lower and upper interval data at time i. This data can be
deﬁned for arithmetic operations as in the following:
1. Addition
pi + pj = [pi + pj, pi + pj]
(1)
2. Subtraction
pi −pj = [pi −pj, pi −pj]
(2)

658
T. Teetranont et al.
3. Multiplication
pi.pj = [min A, max A].A = {pipj, pipj, pipj, pipj}
(3)
4. Division , p > 0
1
pj
= [ 1
pj
, 1
pj
]
(4)
5. Addition and Multiplication by scalar
pi + a = {pi + a, pi + a}
(5)
a.pi =
⎧
⎨
⎩
[a.p, a.p], a < 0
0,
a = 0
[a.p, a.p,
a > 0
(6)
6. logarithm function, pi > 0
log pi =
log pi, log pi

(7)
2.2
Center Method
This method has been proposed by Billard and Diday [3]. The main idea is that
it uses the center of the interval data pc
t which is obtained from upper and lower
values of interval, say pt and pt, and can be derived by
pc
t =
pt + pt
2
(8)
2.3
Autoregressive Moving Average-GARCH Model
Many previous studies suggested that volatility of ﬁnancial return data is not
constant over time, but is rather clustered. This issue can be tackled using volatil-
ity modeling. Within a class of autoregressive processes with white noises having
conditional heteroscedastic variances, this paper considers a GARCH(1,1) model
to estimate the dynamic volatility. It is the workhorse model and mostly applied
in many ﬁnancial data. The model is able to reproduce the volatility dynamics of
ﬁnancial data. Thus, in this study, we consider ARMA(p,q)-GARCH(1,1) which
can be written as
rt = φ0 +
p

i=1
φirt−i +
q

i=1
ϕiεt−i + εt
(9)
εt = σtηt
(10)

Investigating Relationship Between Gold Price and Crude Oil Price
659
σ2
t = ω0 + ω1σ2
t−1 + ω2ε2
t−1
(11)
where ηt is a strong white noise which has normal distribution with mean zero
and variance one. σ2
t is the conditional variance in GARCH process by Tim
Bollerslev [5]. Some standard restrictions on the variance parameters are given.
ω1, ω2 > 0, ω1 + ω2 < 1
(12)
Furthermore, Simon [12] presented a family of variance models in asymmetry
EGARCH model and GJR-GARCH model.
2.4
EGARCH (Exponential GARCH)
From GARCH model, by introducing the parameters λ and ν, for λ = ν =1.
Then we can rewrite GARCH(1,1), Eq. (11) as
log σ2
t = ω0 + ω1 log σ2
t−1 + ω2
|εt−j|
σt−j
−E
|εt−j|
σt−j
	
+ ω3
|εt−j|
σt−j

(13)
The form of the expected value terms associated with ARCH coeﬃcients in the
EGARCH equation depends on the distribution of innovation. If the innovation
distribution is Gaussian, then
E
|εt−j|
σt−j
	
= E {|Zt−j|} =

2
π
(14)
If the innovation distribution is Student’s t with ν > 2 degrees of freedom, then
E
|εt−j|
σt−j
	
= E {|Zt−j|} =

v −2
π
Γ
 v−1
2

Γ
 v
2

(15)
2.5
GJR-GARCH
The GJR-GARCH model is a GARCH variant that includes leverage terms for
modeling an asymmetric volatility clustering. In the GJR formulation, large
negative changes are more likely to be clustered than positive changes. The GJR
model is named for Glosten, Jagannathan, and Runkle [8]. The GJR-GARCH
model is a recursive equation for the variance process, and the simple GJR-
GARCH(1,1) can be written as
σ2
t = ω0 + ω1σ2
t−1 + ω2ε2
t−1 + ω3I[εt−1 < 0]ε2
t−1
(16)
The indicator function I[εt−1 < 0] equals 1 if εt−1 < 0, and 0 otherwise. Thus,
the leverage coeﬃcients are applied to negative innovations, giving negative
changes additional weight. For stationarity and positivity, the GJR model has
the following constraints
ω0 > 0, ω1 ⩾0, ω2 ⩾0, ω2 + ω3 ⩾0, ω1 + ω2 + ω3 < 1

660
T. Teetranont et al.
2.6
Convex Combination Method
The convex combination method is applied to deal with the interval return data,
where the appropriate value over the range of interval can be computed by
rcc
t = α0rt + (1 −α1)rt
(17)
where α0 and α1 are the weighted parameters with value between 0 and 1. In this
study, we consider both ﬁxed weighted and unequal-weighted convex combina-
tion methods. Thus, we set α = 0.5 for ﬁxed weighted convex combination while
α ε [0, 1] is set as the parameter to be estimated for unequal-weighted convex
combination method. For example, in the case of ARMA(1,1)-GARCH(1,1), we
can rewrite Eqs. (9)–(11) as
αrt + (1 −α)rt = φ0 + φ1(αrt−1 + (1 −α)rt−1) + ϕ1εt−1 + εt
(18)
rcc
t = φ0 + φircc
t−1 + ϕ1εt−1 + εt
εt = σtηt, ηt ∼N(0, 1)
(19)
σ2
t = ω0 + ω1σ2
t−1 + ω2ε2
t−1, ω0, ω1, ω2 > 0
(20)
2.7
Model Selection by Akaike Information Criterion (AIC)
In this study, we compare our models using Akaike information criterion applied
from Kullback Leibler Information. It is deﬁned as:
AIC = −2 ln(ˆL) + 2K
(21)
where ˆL is maximized value of likelihood function, K is the number of parameters
in the model.
2.8
Bivariate Copula Approach
Let X, Y
be random variables, the continuous marginal distributions are
F(x), G(y) then H(x, y) is a joint distribution, then 2-dimensional copulas
C : [0, 1]2 →[0, 1] can be deﬁned by
Copula if property
Fi(xi) = u, G(yi) = v and H(x, y) = C(u, v)
so
C(0, v) = C(u, 0) = 0
C(u, 1) = u, C(1, v) = v, u < u′, v < v′
C(u′, v′) −C(u, v′) −C(u′, v) + C(u.v) ≥0,

Investigating Relationship Between Gold Price and Crude Oil Price
661
where C is copula function of marginal distribution random 2 variables. If mar-
ginal has continuous distribution, the copula function is
• Gaussian Copula
C(u, v) = Φ(Φ−1(u), Φ−1(v))
(22)
Lower and upper tail dependence or order parameters of Gaussian Copula is
kL = ku=
2
(1+ρ).Φ−1
n
is quantile function for normal distribution function and
x = Φ−1(u), y = Φ−1(v) and u, v ∈[0, 1]
• Student−t Copula
C(u, v) =
 t−1
v
(u)
−∞
 t−1
v
(v)
−∞
ft1(v)(x, y)dxdy
(23)
where t−1
v (u) and t−1
v (v) are quantile functions with student−t distribution,
where v is degree of freedom and ft1(v)(x, y) is joint density function.
• Frank Copula
C(u, v) = −1
θ log[1 + (e−θu −1)(e−θv −1)
(e−θ −1) ], θ ∈R −{0}
(24)
• Clayton Copula
C(u, v) = (u−θ + v−θ −1)−1
θ , θ > 0
(25)
• Gumbel Copula
C(u, v) = exp(−[(−log u)θ + (−log v)θ]
1
θ ), θ ⩾1
(26)
• Joe Copula
C(u, v) = 1 −(uα + vα −uαvα)
1
α , θ ≥1
(27)
Furthermore,this study also uses the bivariate copula family, presented by
Joe [9] for asymmetric lower and upper tail dependence including,
• BB1 coupla
C(u, v; θ, δ) = 1 + [(u−θ −1)δ]−1
θ , θ > 0, δ ⩾0
(28)
• BB2 coupla
C(u, v; θ, δ) = 1 + δ−1 log(eδ(u−θ−1) + eδ(v−θ−1) −1)]−1
θ , θ, δ > 0
(29)
• BB3 copula
C(u, v; θ, δ) = exp(−[δ−1 log(eδ˜u−θ + eδ˜v−θ −1)]−1
θ ), θ ⩾0, δ ⩾0
(30)

662
T. Teetranont et al.
• BB4 copula
C(u, v; θ, δ) = (u−θ + v−θ −1 −[(u−θ −1)−δ + v−θ −1)−δ]−1
δ )−1
θ , θ ≥1, δ ≥0 (31)
• BB5 copula
C(u, v; θ, δ) = exp(−[xθ + yθ −(x−θδ + y−θδ)−1
δ ]
1
θ ), θ ⩾1, δ ⩾0
(32)
• BB6 copula
C(u, v; θ, δ) = 1 −(1 −exp−[log(1 −u−θ))δ + (−log(1 −v−θ))δ]
1
δ )
1
θ , θ ⩾1, δ ⩾0
(33)
• BB7 copula
C(u, v; θ, δ) = 1 −(1 −[(1 −u−θ)−δ + (1 −v−θ)−δ −1]−1
δ )−1
θ , θ ⩾1, δ ⩾0
(34)
• BB8 copula
C(u, v; θ, δ) = δ−1(1 −{1 −η−1[1 −(1 −δu)ϑ][1 −(1 −δv)ϑ]}
1
ϑ
(35)
where ϑ ⩽1, 0 < δ ⩽1, η = 1 −(1 −δ)ϑ
3
Empirical Result
3.1
Data Description
The data set consists of the Comex and Nymex for the period from 8 May 2009
to 15 July 2016, covering 376 observations. Interval data is the most important
issue in examining the interaction among these commodity prices. Therefore, we
have considered the weekly minimum and maximum of these prices and they were
collected from Thomson Reuters DataStream. The data description is shown in
Table 1 and the interval return plot is show in Figs. 1 and 2.
3.2
Results of Optimal Weights for ARMA-GARCH,
ARMA-EGARCH and ARMA-GJR GARCH Model Using
Convex Combination Method
In this section, we use Comex and Nymex interval returns to estimate ARMA-
GARCH(1,1), ARMA-EGARCH and ARMA-GJR-GARCH models with the
convex combination method to ﬁnd the appropriate weights in the model in
the range of [0,1]. We conduct a grid search to ﬁnd the best ﬁt weight. Here,
the AIC is used to determine the appropriate weight in the interval [0,1] and
the results are shown in Table 2. Then, we compare three GARCH models

Investigating Relationship Between Gold Price and Crude Oil Price
663
Table 1. Data description and statistics of Comex and Nymex using interval return
data
Statistics Comex
Nymex
Low
High
Low
High
Min
−0.1834 −0.0073 −0.276
0.0051
Max
0.013
0.1226
0.0012 0.2848
Mean
−0.0315
0.0337 −0.0673 0.0662
Variance
0.0006
0.0005
0.0024 0.002
Skewness −1.9016
0.9806 −1.4245 1.5509
Kurtosis
10.1393
4.5066
5.4954 6.2506
source: calculation
2010
2011
2012
2013
2014
2015
2016
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
Interval return of Comex
Fig. 1. Comex interval return
2010
2011
2012
2013
2014
2015
2016
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
Interval return of Nymex
Fig. 2. Nymex interval return
using Akaike Information Criterion (AIC) and the lowest AIC is preferred. The
results are also provided in Table 2 and we ﬁnd that the GJR-GARCH model
with Student−t distribution is appropriate for present volatility of Comex and
EGARCH model with Student−t distribution is appropriate for present volatility
of Nymex. Therefore, we use this GARCH speciﬁcation to obtain our marginals.
Table 3 presents the results of Comex from the estimation by GJR-GARCH
models. The results show that ω1 + ω2 = 0.89. This indicates that Comex
exhibits a signiﬁcantly high persistent volatility. Table 4 presents the results
of Nymex from the estimation by EGARCH models. The results show that
ω1 + ω2 = 0.96. This indicates that Nymex exhibits a signiﬁcantly high per-
sistent volatility. Moreover, we try to compare the results of the model with

664
T. Teetranont et al.
Table 2. Results of AIC value in family of GARCH
Family of GARCH
ARMA(p,q)
Distribution
Comex
ARMA(p,q)
Nymex
AIC
α
AIC
α
GARCH(1,1)
p = 2, q = 0
Normal
−1975.88
0.41
p = 4, q = 3
−1534.17
0.4
p = 4, q = 3
Student-t
−1981.73
0.51
p = 4, q = 3
−1532.77
0.43
EGARCH(1,1)
p = 4, q = 3
Normal
−1981.03
0.41
p = 2, q = 1
−1560.48
0.45
p = 4, q = 3
Student-t
−1982.6
0.41
p = 4, q = 3
−1567.34*
0.40*
GJR GARCH(1,1)
p = 2, q = 0
Normal
−1973.88
0.41
p = 4, q = 3
−1555.03
0.42
p = 4, q = 3
Student-t
−1983.73*
0.40*
p = 4, q = 3
−1555.57
0.4
source: calculation
Table 3. Estimated results of Comex by ARMA GJR-GARCH
ARIMA(4,0,3) with Student-t distribution
Parameter
Value
Standard error
Constant
0.00761
0.0019
AR(1)
−0.06159
0.09549
AR(2)
−0.59763
0.03891
AR(3)
0.50141
0.06967
AR(4)
−0.21909
0.0583
MA(1)
0.44164
0.09855
MA(2)
0.68876
0.07449
MA(3)
−0.37918
0.09753
Dof
13.418
7.498
GJR GARCH(1,1), Student-t distribution
ω0
0.000024 0.000022
ω1
0.85545
0.10459
ω2
0.05522
0.05242
ω3
0.00972
0.05459
Degree of freedom
13.418
7.498
AIC
−1983.73
AIC (center method) −1978.51
source: calculation
convex combination and the center method, we ﬁnd that the AIC of convex com-
bination is lower than center method for both ARMA(3.4)-GJR-GARCH(1,1)
and ARMA(3,4)-EGARCH(1,1). This result indicates the superiority of convex
combination method over the center method.
3.3
In-Sample Forecast and Volatility
Then, the best ﬁt GARCH model is used to predict the return of intervals and
volatility of Comex and Nymex as shown in Figs. 3 and 4. These ﬁgures illustrate
the accuracy of the predicted return against actual interval return (upper panel)

Investigating Relationship Between Gold Price and Crude Oil Price
665
Table 4. Estimated results form EGARCH of Nymex
ARIMA(4,0,3) with Student-t distribution
Parameter
Value
Standard error
Constant
0.0009
0.00001
AR(1)
−0.31467
0.04812
AR(2)
0.77496
0.03358
AR(3)
0.75047
0.02937
AR(4)
−0.324847 0.04747
MA(1)
0.64736
0.01685
MA(2)
−0.60512
0.01967
MA(3)
−0.95722
0.01944
Dof.
13.0926
0.00009
EGARCH(1,1), Student-t distribution
ω0
0.00246
0.00598
ω1
0.99951
0.00002
ω2
−0.03632
0.03329
ω3
−0.11246
0.0202
Degree of freedom
13.0926
0.00009
AIC
−1567.34
AIC (center method) −1547.61
source: calculation
and closing price returns (bottom panel) to see the performance of GJR-GARCH
and EGARCH with convex combination. In addition, the predicted volatility
σ2
t is also plotted in the middle panel. From the graph, it is obvious that the
performance is satisfactory. Diﬀerent results of predicted volatilities are shown
in the middle of Figs. 3 and 4. We observed that the volatility of Comex is high
during 2013–2014 corresponding to the Greek crisis. For Nymex, we observed
that the volatility is high during 2015–2016. We expected that the increasing
doubts about the success of the oil producers meeting and rising production as
well as the record US and global crude oil inventories have put a high pressure
on crude oil prices.
3.4
Results of Copulas
In this section, copula model is employed to measure the dependency of Comex
and Nymex. The obtained standardized residuals from the best ﬁt GARCH
process are used to compute the dependence in the copula model. First of all,
we present the scatter plot of copula between Comex and Nymex in Fig. 5. We
observed an unclear relationship between Comex and Nymex, thus the various
families of copulas are proposed to capture the relationship between these two
variables.

666
T. Teetranont et al.
2010
2011
2012
2013
2014
2015
2016
-0.1
-0.05
0
0.05
0.1
return
Comex return
combination of true return
forecasting return
2010
2011
2012
2013
2014
2015
2016
2
3
4
5
6
7
8
10-4
Volatilities forecast
Fig. 3. Volatilities forecast and interval return of Comex
2010
2011
2012
2013
2014
2015
2016
-0.1
-0.05
0
0.05
0.1
0.15
return
Nymex return
combination of true return
forecasting return
2010
2011
2012
2013
2014
2015
2016
0
1
2
3
4
5
10-3
Volatilities forecast
Fig. 4. Volatilities forecast and interval return of Nymex

Investigating Relationship Between Gold Price and Crude Oil Price
667
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Comex
Nymex
Fig. 5. Scatter plot between Comex and Nymex prices
Table 5. Estimation results of copulas
Copula
AIC
Copula
AIC
Gaussian
−17.2708
BB6
−15.0331
Student-t
−19.5115* BB7
−16.7158
Frank
−13.4916
BB8
−15.3787
Clayton
−17.0466
Rotated Joe
−14.6511
Gumbel
−16.1062
Rotated BB1
−16.2804
Joe
−12.5512
Rotated BB7
−11.1828
BB1
−17.0757
Rotated BB8
−17.2221
ρ of student-t
0.2479
Lower-Upper tail dependence “[0.0392 0.0392]”
Degree of freedom
8.4774
Kendall tau
0.1594
source: calculation
Finally, the results of copula model are presented in Table 5. We found that
among 14 copula families, Student-t copula function shows the lowest value of
AIC (−19.5115). Thus, we selected Student-t copula function to explain the
dependence between Comex and Nymex. The result indicates that there exists
a weak positive dependence between Comex and Nymex (ρ = 0.2479, degree of
freedom = 8.4774). Moreover, we found the tail dependence between these two,
where the upper and lower tail dependence was found to be 0.0392. We can
conclude that there exists a dependence between Comex and Nymex not only in
the normal event, but also in the extreme event.

668
T. Teetranont et al.
4
Conclusion
This study investigates the performance of convex combination via various
GARCH families and copula-based approach. In this study, we consider crude oil
and gold with interval data as the application study. The results conﬁrm that the
EGARCH and GJR-GARCH with convex combination method improve the esti-
mation. We also used the obtained standardized residuals from GARCH process
with the copula model to measure the dependency of crude oil and gold. We
found that among the various copula families, Student-t copula shows the low-
est value of AIC, thus we used a Student-t copula function to join the marginal
of crude oil and gold. The result of copula model showed that there exists a
positive dependence between these two variables. Moreover, we also found the
positive tail dependence which indicates that there exists a dependence in the
extreme event.
References
1. Akaike, H.: Information theory and an extension of the maximum likelihood prin-
ciple. In: Proceedings of 2nd International Symposium on Information Theory,
Budapest, pp. 267–281 (1973)
2. Abbate, A., Marcellino, M.: Point, interval and density forecasts of exchange rates
with time-varying parameter models. Discussion paper, Deutsche Bundesbank, No.
19/2016 (2016)
3. Billard, L., Diday, E.: Regression analysis for interval-valued data. In: Data Analy-
sis, Classiﬁcation and Related Methods, Proceedings of the 7th Conference for the
IFCS, IFCS 2002, pp. 369–374. Springer, Berlin (2000)
4. Blanco Fernndez, A., Corral, N., Gonzlez Rodrguez, G.: Estimation of a ﬂexible
simple linear model for interval data based on set arithmetic. Comput. Stat. Data
Anal. 55(9), 2568–2578 (2011). North-Holland
5. Bollerslev, T.: Generalized autoregressive conditional heteroskedasticity. J. Econ.
31, 307–327 (1986)
6. Hansen, B.E.: Reversion. Econometrics. www.ssc.wisc.edu/bhansen (2013)
7. Chanaim, S., Sriboonchitta, S., Rungruang, C.: A convex combination method
for linear regression with interval data. In: Integrated Uncertainty in Knowl-
edge Modelling and Decision Making, 5th International Symposium, IUKM 2016,
Da Nang, Vietnam, 30 November–2 December 2016, Proceedings, pp. 469-480.
Springer (2016)
8. Engle, R.F.: Dynamic conditional correlation - a simple class of multivariate
GARCH models. J. Bus. Econ. Stat. 20(3), 339–350 (2002)
9. Glosten, L., Jagannathan, R., Runkle, D.: On the relation between the expected
value and the volatility of the nominal excess return on stocks. J. Financ. 1993,
1779–1801 (1993)
10. Joe, H.: Multivariate Models and Multivariate Dependence Concepts. CRC Press,
Boca Raton (1997)
11. Neto, L.E.A., de Carvalho, F.A.T.: Centre and range method for ﬁtting a linear
regression model to symbolic interval data. Comput. Stat. Data Anal. 52, 1500–
1515 (2008)

Investigating Relationship Between Gold Price and Crude Oil Price
669
12. Phochanachan, P., Pastpipatkul, P., Yamaka, W., Sriboonchitta, S.: Threshold
regression for modeling symbolic interval data. Int. J. Appl. Bus. Econ. Res. 15(7),
195–207 (2017)
13. Hentschel, L.: All in the family nesting symmetric and asymmetric GARCH models.
J. Financ. Econ. 39(1995), 71–104 (1994). Simon. Elsevier

Simultaneous Conﬁdence Intervals for All
Diﬀerences of Means of Normal Distributions
with Unknown Coeﬃcients of Variation
Warisa Thangjai(B), Sa-Aat Niwitpong, and Suparat Niwitpong
Department of Applied Statistics, Faculty of Applied Science, King Mongkut’s
University of Technology North Bangkok, Bangkok 10800, Thailand
wthangjai@yahoo.com, {sa-aat.n,suparat.n}@sci.kmutnb.ac.th
Abstract. This paper presents a procedure for simultaneous conﬁdence
interval estimation for the diﬀerences of means of several normal popu-
lations with unknown coeﬃcients of variation. The proposed approaches
are a generalized conﬁdence interval approach (GCI approach) and
method of variance estimates recovery approach (MOVER approach). A
Monte Carlo simulation was used to evaluate the performance in terms of
coverage probability, average width and standard error. The simulation
results indicated that the GCI and MOVER approaches are satisfac-
tory in terms of the coverage probability, but the average widths of the
MOVER approach are slightly shorter than the average widths of the
GCI approach. The proposed approaches are illustrated by an example.
Keywords: Simultaneous conﬁdence intervals · Mean · GCI approach
MOVER approach · Coeﬃcient of variation
1
Introduction
The problem of estimating the normal population mean when the coeﬃcient of
variation is known has been considered continuously in many ﬁelds such as medi-
cine, biology, and chemistry. For example, Searls [20] provided the minimum mean
squared error (MMSE) estimator to estimate mean of normal distribution when
the coeﬃcient of variation is known. Khan [4] discussed the mean estimation with
known coeﬃcient of variation in one sample case. Gleser and Healy [3] considered
the normal mean estimation when the coeﬃcient of variation is known using the
minimum quadratic risk scale invariant estimator. Bhat and Rao [1] considered
the problem of testing the normal mean with known coeﬃcient of variation.
Niwitpong et al. [12] and Niwitpong and Niwitpong [14] proposed conﬁdence inter-
val for the diﬀerence between normal means with known coeﬃcients of variation.
The conﬁdence intervals for the normal mean with known coeﬃcient of variation
were considered by Niwitpong [10], Niwitpong and Niwitpong [13], and Niwit-
pong [11]. The conﬁdence intervals for common mean of normal distributions with
known coeﬃcient of variation were proposed by Sodanin et al. [24].
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_48

SCIs for Diﬀerences of Normal Means with Unknown CVs
671
In practice, the coeﬃcient of variation is often unknown. This is because
the population mean and population variance are unknown. Several researchers
have considered the problem of estimating the normal population mean with
unknown coeﬃcient of variation. For example, Srivastava [22] provided uniformly
minimum variance unbiased (UMVU) estimator for the normal mean estimation
with unknown coeﬃcient of variation using the minimum mean squared error
estimator of Searls [20]. Srivastava and Singh [23] derived the UMVU estimator
of the exact relative eﬃciency ratio of the estimator of Srivastava [22]. For more
details about the mean of normal distribution with unknown coeﬃcient of varia-
tion, see the research papers of Sahai [16], Sahai and Acharya [17], and Sodanin
et al. [24].
The problem of multiple comparisons of parameters has received considerable
attention in previous literature. Malley [9] constructed simultaneous conﬁdence
intervals (SCIs) for ratios of means of normal distributions. Kharrati-Kopaei
et al. [5] proposed simultaneous ﬁducial generalized conﬁdence intervals (SFG-
CIs) for the successive diﬀerences of exponential location parameters under het-
eroscedasticity. Zhang [31] presented SFGCIs for several inverse Gaussian popu-
lations. Sadooghi-Alvandi and Malekzadeh [15] proposed a parametric bootstrap
approach to construct SCIs for ratios of means of several lognormal distributions.
Li et al. [8] provided a parametric bootstrap SCIs for diﬀerences of means from
several two-parameter exponential distributions. Kharrati-Kopaei [6] described
SCIs for the diﬀerences of location parameters of successive exponential distrib-
utions in the unbalanced case under heteroscedasticity.
To our knowledge, there is no previous work on constructing SCIs for diﬀer-
ences of normal means with unknown coeﬃcients of variation using generalized
conﬁdence interval approach (GCI approach) and method of variance estimates
recovery approach (MOVER approach). In this paper, the GCI and MOVER
approaches to construct the SCIs are presented. Conﬁdence interval estimation
using the GCI approach was introduced by Weerahandi [29] and has been con-
sidered by Krishnamoorthy and Lu [7], Tian [26], Tian and Wu [27], and Ye
et al. [30]. The MOVER approach to construct conﬁdence interval for sum of
two populations was introduced by Zou and Donner [32] and Zou et al. [33].
Also, the MOVER approach for conﬁdence interval estimation for diﬀerence of
two populations was proposed by Donner and Zou [2]. Several researchers have
used the concept of MOVER approach to obtain conﬁdence interval; see e.g.,
Sangnawakij et al. [18] and Sangnawakij and Niwitpong [19]. In this paper, the
concepts of the GCI and MOVER approaches are extended to k populations and
use to construct SCIs of diﬀerences of normal means with unknown coeﬃcient
of variation.
The remainder of this paper is organized as follows. The GCI and MOVER
approaches to construct SCIs for diﬀerences of means of several normal distrib-
utions with unknown coeﬃcients of variation are presented in Sect. 2. Section 3,
simulation results are presented to evaluate the coverage probabilities, average
widths, and standard errors of the GCI approach in comparison to the MOVER
approach. Section 4, the GCI and MOVER approaches are illustrated with an
example. And ﬁnally, the conclusions are presented in Sect. 5.

672
W. Thangjai et al.
2
Simultaneous Conﬁdence Intervals
Assume that X = (X1, X2, . . . , Xn) are independent random samples each hav-
ing the same normal distribution with mean μ and variance σ2. Let τ = σ/μ
be the coeﬃcient of variation (CV). Let ¯X and S be sample mean and sample
standard deviation for X, respectively.
Searls [20] proposed the following minimum mean squared error (MMSE)
estimator for the normal population mean
θ =
n
j=1
Xj
n + τ 2 =
¯X
1 + (σ2/nμ2) =
n ¯X
n + (σ2/μ2).
(1)
However, the CV is usually unknown in practice. Therefore, Srivastava [21]
considered the following estimate for the mean with unknown CV
ˆθ =
n
j=1
Xj
n + ˆτ 2 =
¯X
1 + (S2/n ¯X2) =
n ¯X
n + (S2/ ¯X2).
(2)
Let Xi = (Xi1, Xi2, . . . , Xini) be a random sample from the i-th normal
population N

μi, σ2
i

, i = 1, 2, . . . , k. For the i-th sample, let ¯Xi and Si be
sample mean and sample standard deviation of Xi, respectively. And let ¯xi and
si be the observed values of ¯Xi and Si, respectively. The estimator of Srivastava
[21] has the following form
ˆθi =
ni ¯Xi
ni + (S2
i / ¯X2
i ).
(3)
In this paper we are interested in constructing SCIs for θil = θi −θl, i, l =
1, 2, . . . , k, i ̸= l. The estimate for the diﬀerences of means of normal distributions
with unknown coeﬃcients of variation, θil, i, l = 1, 2, . . . , k, i ̸= l, is
θil = θi −θl =
ni ¯Xi
ni + (σ2
i /μ2
i ) −
nl ¯Xl
nl + (σ2
l /μ2
l ).
(4)
The estimator of θil = θi −θl is obtained by
ˆθil = ˆθi −ˆθl =
ni ¯Xi
ni + (S2
i / ¯X2
i ) −
nl ¯Xl
nl + (S2
l / ¯X2
l ).
(5)
Deﬁnition 1: Let X = (X1, X2, . . . , Xn) be a random sample of size n from
normal distribution with mean μ and variance σ2. Let ¯X and S be sample mean
and sample standard deviation for X, respectively. Let θ = n ¯X
 
n + (σ2/μ2)

be the mean with unknown coeﬃcient of variation. Let ˆθ = n ¯X
 
n + (S2/ ¯X2)

be the estimated mean with unknown coeﬃcient of variation for θ. And let τ1

SCIs for Diﬀerences of Normal Means with Unknown CVs
673
and τ2 be the mean and variance of ˆθ, respectively. According to Thangjai et al.
[25], the distribution of
√n

ˆθ −τ1
 D
→N (0, nτ2) ,
(6)
where τ1 = A · B and τ2 = C · D, where
A =
⎛
⎝
μ
1 +

σ2
nμ2+σ2
 
1 + 2σ4+4nμ2σ2
(nμ2+σ2)2

⎞
⎠
B =
⎛
⎜
⎝1 +

nσ2
nμ2+σ2
2 
2
n + 2σ4+4nμ2σ2
(nμ2+σ2)2


n +

nσ2
nμ2+σ2
 
1 + 2σ4+4nμ2σ2
(nμ2+σ2)2
2
⎞
⎟
⎠
C =
⎛
⎝
μ
1 +

σ2
nμ2+σ2
 
1 + 2σ4+4nμ2σ2
(nμ2+σ2)2

⎞
⎠
2
D =
⎛
⎜
⎝σ2
nμ2 +

nσ2
nμ2+σ2
2 
2
n + 2σ4+4nμ2σ2
(nμ2+σ2)2


n +

nσ2
nμ2+σ2
 
1 + 2σ4+4nμ2σ2
(nμ2+σ2)2
2
⎞
⎟
⎠.
2.1
Generalized Conﬁdence Interval Approach
Deﬁnition 2: Let X = (X1, X2, . . . , Xn) be a random sample from a distri-
bution F (x|δ) and let x be the observation of X. Let δ = (θ, ν) be a vector of
unknown parameters where θ is a parameter of interest, ν is a vector of nuisance
parameters. Let R (X; x, δ) be a function of X, x, and δ. The random quantity
R (X; x, δ) is called be generalized pivotal quantity (GPQ) if the following two
conditions are satisﬁed; see Weerahandi [29]:
(i) The distribution of R (X; x, δ), X = x, is free of all unknown parameters.
(ii) The observed value of R (X; x, δ), X = x, is the parameter of interest θ.
Let R (α/2) and R (1 −α/2) be the 100 (α/2)-th and the 100 (1 −α/2)-th
percentiles of R (X; x, δ), respectively. Then (R (α/2) , R (1 −α/2)) becomes a
100 (1 −α) % two-sided generalized conﬁdence interval for θ.
It is well known that ¯Xi and S2
i are independently distributed with
¯Xi ∼N

μi, σ2
i
ni

, (ni −1) S2
i
σ2
i
∼χ2
ni−1; i = 1, 2, . . . , k,
(7)
where χ2
ni−1 denotes a chi-square distribution with ni −1 degrees of freedom.
According to Tian and Wu [27], the generalized pivotal quantities for esti-
mating μi and σ2
i based on the i-th sample are given by
Rμi = ¯xi −Zi
√Ui

(ni −1) s2
i
ni
(8)

674
W. Thangjai et al.
and
Rσ2
i = (ni −1) s2
i
Vi
,
(9)
where Zi denotes standard normal distribution, and Ui and Vi denote chi-square
distribution with ni −1 degrees of freedom.
Therefore, the generalized pivotal quantity for θi based on the i-th sample is
obtained by
Rθi =
niRμi
ni +

Rσ2
i /R2μi
,
(10)
where Rμi is deﬁned in Eq. (8) and Rσ2
i is deﬁned in Eq. (9).
Thus the generalized pivotal quantity for θil = θi −θl, i, l = 1, 2, . . . , k, i ̸= l
is deﬁned by
Rθil = Rθi −Rθl =
niRμi
ni +

Rσ2
i /R2μi
 −
nlRμl
nl +

Rσ2
l /R2μl
.
(11)
Therefore, the 100 (1 −α) % two-sided simultaneous conﬁdence interval for
all diﬀerences of normal means with unknown coeﬃcients of variation θil based
on GCI approach is deﬁned by
SCIil(GCI) =

Lil(GCI), Uil(GCI)

= (Rθil (α/2) , Rθil (1 −α/2)) ,
(12)
where Rθil (α/2) and Rθil (1 −α/2) denote the 100 (α/2)-th and 100 (1 −α/2)-
th percentiles of Rθil, respectively.
The following algorithm is used to construct the SCI based on GCI approach:
Algorithm 1.
Step 1 Calculate the values of ¯xi and s2
i , the observed values of ¯Xi and S2
i ,
i = 1, 2, . . . , k.
Step 2 Generate the values of Zi from standard normal distribution, i =
1, 2, . . . , k.
Step 3 Generate the values of Ui from chi-square distribution with ni−1 degrees
of freedom, i = 1, 2, . . . , k.
Step 4 Compute the values of Rμi from equation (8), i = 1, 2, . . . , k.
Step 5 Generate the values of Vi from chi-square distribution with ni−1 degrees
of freedom, i = 1, 2, . . . , k.
Step 6 Compute the values of Rσ2
i from equation (9), i = 1, 2, . . . , k.
Step 7 Compute the values of Rθi from equation (10), i = 1, 2, . . . , k.
Step 8 Compute the values of Rθil from equation (11), i, l = 1, 2, . . . , k, i ̸= l.
Step 9 Repeat Step 2 - Step 8 a large number of times, m = 2500, and from
these m values, obtained the Rθil.
Step 10 Compute the 100 (α/2)-th percentile of Rθil deﬁned by Rθil (α/2) and
compute the 100 (1 −α/2)-th percentile of Rθil deﬁned by Rθil (1 −α/2).

SCIs for Diﬀerences of Normal Means with Unknown CVs
675
2.2
Method of Variance Estimates Recovery Approach
First, the conﬁdence interval for mean of normal distribution with unknown
coeﬃcient of variation is considered. Recall that the standard normal distribution
with the mean 0 and variance 1 is deﬁned by
Z =
¯X −μ

V ar(ˆθ)
∼N (0, 1) .
(13)
The student’s t-distribution with n −1 degrees of freedom can be deﬁned as
the distribution of the random variable T with
T =
Z

χ2
n−1
n−1
=
¯X −μ

V ar(ˆθ)

(n −1) S2
(n −1) σ2
=
¯X −μ

V ar(ˆθ)
σ
s

≈
¯X −μ

V ar(ˆθ)
 ˆσ
s

,
(14)
where (n−1)S2
σ2
∼χ2
n−1 and ˆσ = S.
Therefore, the 100 (1 −α) % two-sided conﬁdence interval for the mean of
normal distribution with unknown coeﬃcient of variation θi, i = 1, 2, . . . , k, is
obtained by
li = ¯xi −t1−α/2
si
ˆσi

V ar(ˆθi)
(15)
and
ui = ¯xi + t1−α/2
si
ˆσi

V ar(ˆθi),
(16)
where V ar(ˆθi) = E · F, where (from Eq. (6))
E =
⎛
⎜
⎜
⎝
μi
1 +

σ2
i
niμ2
i +σ2
i
 
1 + 2σ4
i +4niμ2
i σ2
i
(niμ2
i +σ2
i)
2

⎞
⎟
⎟
⎠
2
F =
⎛
⎜
⎜
⎜
⎝
σ2
i
niμ2
i
+

niσ2
i
niμ2
i +σ2
i
2 
2
ni + 2σ4
i +4niμ2
i σ2
i
(niμ2
i +σ2
i)
2


ni +

niσ2
i
niμ2
i +σ2
i
 
1 + 2σ4
i +4niμ2
i σ2
i
(niμ2
i +σ2
i)
2
2
⎞
⎟
⎟
⎟
⎠.
In the case of a diﬀerence of two parameters, Donner and Zou [2] proposed
the method of variance estimates recovery approach (MOVER approach) to
construct a 100 (1 −α) % two-sided conﬁdence interval (L, U) for θ1 −θ2 where

676
W. Thangjai et al.
θ1 and θ2 denote two parameters of interest, L and U denote the lower limit
and upper limit of the conﬁdence interval, respectively. The (li, ui) contains the
parameter values for θi where i = 1, 2. The lower limit L and upper limit U are
deﬁned by
L = ˆθ1 −ˆθ2 −

ˆθ1 −l1
2
+

u2 −ˆθ2
2
(17)
and
U = ˆθ1 −ˆθ2 +

u1 −ˆθ1
2
+

ˆθ2 −l2
2
.
(18)
It is reasonable to extend the concept of Donner and Zou [2] to construct
a 100 (1 −α) % two-sided conﬁdence interval (L, U) for θil = θi −θl, i, l =
1, 2, . . . , k, i ̸= l. The lower limit L and upper limit U are obtained by
L = ˆθi −ˆθl −

ˆθi −li
2
+

ul −ˆθl
2
(19)
and
U = ˆθi −ˆθl +

ui −ˆθi
2
+

ˆθl −ll
2
,
(20)
where ˆθi and ˆθl are deﬁned in Eq. (3), li and ll are deﬁned in Eq. (15), and ui
and ul are deﬁned in Eq. (16).
Therefore, the 100 (1 −α) % two-sided simultaneous conﬁdence interval for
all diﬀerences of normal means with unknown coeﬃcients of variation θil based
on MOVER approach is deﬁned by
SCIil(MOV ER) =

Lil(MOV ER), Uil(MOV ER)

,
(21)
where
Lil(MOV ER) = ˆθi −ˆθl −

ˆθi −li
2
+

ul −ˆθl
2
Uil(MOV ER) = ˆθi −ˆθl +

ui −ˆθi
2
+

ˆθl −ll
2
.
3
Simulation Studies
In this section, results of the simulation study in which the SCIs based on GCI
approach (SCIGCI) are compared with the SCIs based on MOVER approach
(SCIMOV ER) are presented. The performance of these two approaches was eval-
uated through the coverage probabilities, average widths, and standard errors of
the conﬁdence intervals.

SCIs for Diﬀerences of Normal Means with Unknown CVs
677
The following algorithm was used to obtain the coverage probabilities of two
simultaneous conﬁdence intervals:
Algorithm 2.
Step 1 Generate Xi, a random sample of sample size ni from normal population
with parameters μi and σ2
i , for i = 1, 2, . . . , k. Calculate ¯xi and si (the observed
values of ¯Xi and Si).
Step 2 Construct the two-sided SCIs based on the GCI approach (SCIil(GCI))
from Algorithm 1 and record whether or not all the values of θil = θi −θl,
i, l = 1, 2, . . . , k, i ̸= l, are in their corresponding SCIGCI.
Step
3 Construct the two-sided SCIs based on the MOVER approach
(SCIil(MOV ER)) from equation (21) and record whether or not all the values
of θil = θi −θl, i, l = 1, 2, . . . , k, i ̸= l, are in their corresponding SCIMOV ER.
Step 4 Repeat Step 1 - Step 3 a large number of times, M = 5000. Then, the
fraction of times that all θil = θi −θl, i, l = 1, 2, . . . , k, i ̸= l, are in their corre-
sponding SCIs provides an estimate of the coverage probability.
Table 1. The coverage probabilities of 95% of two-sided simultaneous conﬁdence inter-
vals for all diﬀerences of means of normal distributions with unknown coeﬃcients of
variation: 3 sample cases.
(n1, n2, n3)
(σ1, σ2, σ3)
SCIGCI
SCIMOV ER
(10, 10, 10)
(0.1, 0.1, 0.1)
0.9606
0.9625
(0.1, 0.2, 0.3)
0.9594
0.9599
(20, 20, 20)
(0.1, 0.1, 0.1)
0.9563
0.9569
(0.1, 0.2, 0.3)
0.9546
0.9555
(10, 20, 30)
(0.1, 0.1, 0.1)
0.9547
0.9557
(0.1, 0.2, 0.3)
0.9548
0.9551
(30, 30, 30)
(0.1, 0.1, 0.1)
0.9530
0.9547
(0.1, 0.2, 0.3)
0.9523
0.9522
(50, 50, 50)
(0.1, 0.1, 0.1)
0.9505
0.9521
(0.1, 0.2, 0.3)
0.9488
0.9496
(30, 50, 100)
(0.1, 0.1, 0.1)
0.9486
0.9483
(0.1, 0.2, 0.3)
0.9530
0.9543
(100, 100, 100)
(0.1, 0.1, 0.1)
0.9506
0.9512
(0.1, 0.2, 0.3)
0.9491
0.9493
(200, 200, 200)
(0.1, 0.1, 0.1)
0.9483
0.9485
(0.1, 0.2, 0.3)
0.9499
0.9508
(500, 500, 500)
(0.1, 0.1, 0.1)
0.9500
0.9503
(0.1, 0.2, 0.3)
0.9496
0.9494
(1000, 1000, 1000)
(0.1, 0.1, 0.1)
0.9473
0.9490
(0.1, 0.2, 0.3)
0.9475
0.9485

678
W. Thangjai et al.
Table 2. The average widths (standard errors) of 95% of two-sided simultaneous con-
ﬁdence intervals for all diﬀerences of means of normal distributions with unknown
coeﬃcients of variation: 3 sample cases.
(n1, n2, n3)
(σ1, σ2, σ3)
SCIGCI
SCIMOV ER
(10, 10, 10)
(0.1, 0.1, 0.1)
0.1996 (0.0122)
0.2002 (0.0120)
(0.1, 0.2, 0.3)
0.4254 (0.0626)
0.4207 (0.0608)
(20, 20, 20)
(0.1, 0.1, 0.1)
0.1309 (0.0056)
0.1313 (0.0055)
(0.1, 0.2, 0.3)
0.2780 (0.0387)
0.2769 (0.0382)
(10, 20, 30)
(0.1, 0.1, 0.1)
0.1487 (0.0161)
0.1490 (0.0161)
(0.1, 0.2, 0.3)
0.2626 (0.0196)
0.2623 (0.0192)
(30, 30, 30)
(0.1, 0.1, 0.1)
0.1049 (0.0037)
0.1051 (0.0035)
(0.1, 0.2, 0.3)
0.2235 (0.0310)
0.2229 (0.0307)
(50, 50, 50)
(0.1, 0.1, 0.1)
0.0799 (0.0022)
0.0801 (0.0021)
(0.1, 0.2, 0.3)
0.1701 (0.0233)
0.1698 (0.0231)
(30, 50, 100)
(0.1, 0.1, 0.1)
0.0820 (0.0073)
0.0820 (0.0073)
(0.1, 0.2, 0.3)
0.1468 (0.0097)
0.1467 (0.0095)
(100, 100, 100)
(0.1, 0.1, 0.1)
0.0560 (0.0011)
0.0560 (0.0010)
(0.1, 0.2, 0.3)
0.1190 (0.0161)
0.1188 (0.0161)
(200, 200, 200)
(0.1, 0.1, 0.1)
0.0394 (0.0006)
0.0394 (0.0005)
(0.1, 0.2, 0.3)
0.0836 (0.0113)
0.0836 (0.0112)
(500, 500, 500)
(0.1, 0.1, 0.1)
0.0249 (0.0003)
0.0248 (0.0002)
(0.1, 0.2, 0.3)
0.0527 (0.0071)
0.0527 (0.0071)
(1000, 1000, 1000)
(0.1, 0.1, 0.1)
0.0176 (0.0002)
0.0175 (0.0001)
(0.1, 0.2, 0.3)
0.0373 (0.0050)
0.0372 (0.0050)
In the simulation, four conﬁguration factors are considered to evaluate the
performance of the two approaches: (1) sample cases: k = 3 and k = 5; (2)
population means: μ1 = μ2 = . . . = μk = 1; (3) population standard deviations:
σ1, σ2, . . . , σk; (4) sample sizes: n1, n2, . . . , nk. See the following tables for speciﬁc
combinations of conﬁgurations. The nominal conﬁdence level was chosen to be
0.95. The simulation results from k = 3 and k = 5 are presented in the following
four tables.
For k = 3, it is seen from Tables 1 and 2 that both the SCIGCI and
SCIMOV ER perform satisfactorily in terms of the coverage probability, aver-
age width, and standard error of the conﬁdence interval. In almost all cases, the
coverage probabilities of the SCIGCI are similar to those of the SCIMOV ER.
However, the average widths of the SCIMOV ER are slightly shorter than those
of the SCIGCI. As seen from Tables 3 and 4 for k = 5, the performances of
SCIGCI and SCIMOV ER are similar to those of k = 3. Therefore, the MOVER
approach performs better than the GCI approach in terms of average width.
However, the GCI approach can be considered as an alternative to construct
SCIs for all pairwise diﬀerences of means from several normal distributions with
unknown coeﬃcients of variation.

SCIs for Diﬀerences of Normal Means with Unknown CVs
679
Table 3. The coverage probabilities of 95% of two-sided simultaneous conﬁdence inter-
vals for all diﬀerences of means of normal distributions with unknown coeﬃcients of
variation: 5 sample cases.
(n1, n2, n3, n4, n5)
(σ1, σ2, σ3, σ4, σ5)
SCIGCI
SCIMOV ER
(10, 10, 10, 10, 10)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9631
0.9645
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9583
0.9584
(20, 20, 20, 20, 20)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9551
0.9569
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9551
0.9553
(10, 10, 20, 30, 30)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9569
0.9580
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9578
0.9587
(30, 30, 30, 30, 30)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9532
0.9543
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9536
0.9533
(50, 50, 50, 50, 50)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9511
0.9521
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9526
0.9526
(30, 30, 50, 100, 100)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9523
0.9529
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9526
0.9531
(100, 100, 100, 100, 100)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9539
0.9543
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9496
0.9501
(200, 200, 200, 200, 200)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9509
0.9514
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9509
0.9514
(500, 500, 500, 500, 500)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9490
0.9490
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9514
0.9516
(1000, 1000, 1000, 1000, 1000)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.9502
0.9503
(0.1, 0.1, 0.2, 0.3, 0.3)
0.9515
0.9518
Table 4. The average widths (standard errors) of 95% of two-sided simultaneous con-
ﬁdence intervals for all diﬀerences of means of normal distributions with unknown
coeﬃcients of variation: 5 sample cases.
(n1, n2, n3, n4, n5)
(σ1, σ2, σ3, σ4, σ5)
SCIGCI
SCIMOV ER
(10, 10, 10, 10, 10)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.1993 (0.0082)
0.1998 (0.0081)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.4248 (0.0422)
0.4197 (0.0409)
(20, 20, 20, 20, 20)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.1310 (0.0038)
0.1313 (0.0037)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.2791 (0.0258)
0.2778 (0.0254)
(10, 10, 20, 30, 30)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.1515 (0.0107)
0.1518 (0.0107)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.2624 (0.0129)
0.2621 (0.0127)
(30, 30, 30, 30, 30)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.1049 (0.0025)
0.1051 (0.0024)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.2229 (0.0202)
0.2222 (0.0200)
(50, 50, 50, 50, 50)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.0801 (0.0015)
0.0802 (0.0014)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.1699 (0.0151)
0.1696 (0.0150)
(30, 30, 50, 100, 100)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.0822 (0.0048)
0.0823 (0.0048)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.1433 (0.0064)
0.1433 (0.0063)
(100, 100, 100, 100, 100)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.0560 (0.0008)
0.0560 (0.0007)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.1189 (0.0104)
0.1188 (0.0104)
(200, 200, 200, 200, 200)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.0394 (0.0004)
0.0394 (0.0003)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.0836 (0.0073)
0.0836 (0.0073)
(500, 500, 500, 500, 500)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.0248 (0.0002)
0.0248 (0.0001)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.0527 (0.0046)
0.0527 (0.0046)
(1000, 1000, 1000, 1000, 1000)
(0.1, 0.1, 0.1, 0.1, 0.1)
0.0176 (0.0001)
0.0175 (0.0001)
(0.1, 0.1, 0.2, 0.3, 0.3)
0.0372 (0.0032)
0.0372 (0.0032)

680
W. Thangjai et al.
4
An Empirical Application
In this section, an example is given to illustrate the usage of the GCI approach
and MOVER approach in practice. This data set was provided by Walpole
et al. [28]. Testing patient blood samples for HIV antibodies, a spectropho-
tometer determines the optical density of each sample. Ten diﬀerent runs at
four randomly selected laboratories were measured. The summary statistics are
as follows: ¯x1 = 1.0037, ¯x2 = 1.0579, ¯x3 = 1.0997, ¯x4 = 1.0224, s2
1 = 0.0056,
s2
2 = 0.0227, s2
3 = 0.0301, s2
4 = 0.0238, n1 = 10, n2 = 10, n3 = 10, and n4 =
10. For illustration, the 95% simultaneous conﬁdence intervals for the six dif-
ferences of normal means with unknown coeﬃcients of variation, θil = θi −θl,
i, l = 1, 2, 3, 4, i ̸= l, obtained by using the two approaches, are given in Table 5.
The results show that the MOVER approach performs much better than the
GCI approach in the sense that the widths of the MOVER approach are shorter
than the widths of the GCI approach in almost all cases. Therefore, this result
conﬁrms our simulation study in the previous section for the same values of ni =
10 and the diﬀerent values of σi, i = 1, 2, . . . , k.
Table 5. The 95% simultaneous conﬁdence intervals for all pairwise diﬀerences of
means of normal distributions with unknown coeﬃcients of variation.
Parameters CIGCI
CIMOV ER
Lower
Upper Lower
Upper
θ2 −θ1
−0.0705 0.1740 −0.0659 0.1745
θ3 −θ1
−0.0378 0.2314 −0.0390 0.2312
θ4 −θ1
−0.1124 0.1355 −0.1038 0.1413
θ3 −θ2
−0.1198 0.2143 −0.1224 0.2061
θ4 −θ2
−0.2011 0.1171 −0.1896 0.1186
θ4 −θ3
−0.2488 0.0851 −0.2434 0.0886
5
Discussion and Conclusions
In this paper, GCI and MOVER approaches were proposed to construct SCIs
for the diﬀerences of normal means with unknown coeﬃcients of variation. The
concepts of the GCI and MOVER approaches are extended to k populations
and used to construct the SCIs. The performances of these approaches were
investigated using Monte Carlo simulations. Coverage probabilities of the GCI
approach are similar to those of the MOVER approach in almost all cases. How-
ever, the average widths of the MOVER approach are slightly shorter than those
of the GCI approach. Moreover, the MOVER approach is recommended for use
since it is conceptually simpler than the GCI approach in practice. The coverage
probability of the MOVER approach is similar to the result in research papers
by Donner and Zou [2].

SCIs for Diﬀerences of Normal Means with Unknown CVs
681
Acknowledgements. The ﬁrst author gratefully acknowledges the ﬁnancial support
from Science Achievement Scholarship of Thailand.
References
1. Bhat, K.K., Rao, K.A.: On tests for a normal mean with known coeﬃcient of
variation. Inter. Stat. Rev. 75, 170–182 (2007)
2. Donner, A., Zou, G.Y.: Closed-form conﬁdence intervals for function of the normal
standard deviation. Stat. Meth. Med. Res. 21, 347–359 (2010)
3. Gleser, L.J., Healy, J.D.: Estimating the mean of a normal distribution with known
coeﬃcient of variation. J. Am. Stat. Assoc. 71, 977–981 (1976)
4. Khan, R.A.: A note on estimating the mean of a normal distribution with known
coeﬃcient of variation. J. Am. Stat. Assoc. 63, 1039–1041 (1968)
5. Kharrati-Kopaei, M., Malekzadeh, A., Sadooghi-Alvandi, M.: Simultaneous ﬁducial
generalized conﬁdence intervals for the successive diﬀerences of exponential loca-
tion parameters under heteroscedasticity. Stat. Prob. Lett. 83, 1547–1552 (2013)
6. Kharrati-Kopaei, M.: A note on the simultaneous conﬁdence intervals for the
successive diﬀerences of exponential location parameters under heteroscedasticity.
Stat. Meth. 22, 1–7 (2015)
7. Krishnamoorthy, K., Lu, Y.: Inference on the common means of several normal
populations based on the generalized variable method. Biometrics 59, 237–247
(2003)
8. Li, J., Song, W., Shi, J.: Parametric bootstrap simultaneous conﬁdence intervals
for diﬀerences of means from several two-parameter exponential distributions. Stat.
Prob. Lett. 106, 39–45 (2015)
9. Malley, J.D.: Simultaneous conﬁdence intervals for ratios of normal means. J. Am.
Stat. Assoc. 77, 170–176 (1982)
10. Niwitpong, S.: Conﬁdence intervals for the normal mean with known coeﬃcient of
variation. World Acad. Sci. Eng. Technol. 6, 1365–1368 (2012)
11. Niwitpong, S.: Conﬁdence intervals for the normal mean with a known coeﬃcient
of variation. Far East J. Math. Sci. 97, 711–727 (2015)
12. Niwitpong, S., Koonprasert, S., Niwitpong, S.: Conﬁdence interval for the diﬀer-
ence between normal population means with known coeﬃcients of variation. Appl.
Math. Sci. 6, 47–54 (2012)
13. Niwitpong, S., Niwitpong, S.: On simple conﬁdence intervals for the normal mean
with a known coeﬃcient of variation. World Acad. Sci. Eng. Technol. 7, 1444–1447
(2013)
14. Niwitpong, S., Niwitpong, S.: Conﬁdence intervals for the diﬀerence between nor-
mal means with known coeﬃcients of variation. Ann. Oper. Res. 247, 1–15 (2016)
15. Sadooghi-Alvandi, S.M., Malekzadeh, A.: Simultaneous conﬁdence intervals for
ratios of means of several lognormal distributions: a parametric bootstrap app-
roach. Comput. Stat. Data Anal. 69, 133–140 (2014)
16. Sahai, A.: On an estimator of normal population mean and UMVU estimation of
its relative eﬃciency. Appl. Math. Comput. 152, 701–708 (2004)
17. Sahai, A., Acharya, R.M.: Iterative estimation of normal population mean using
computational-statistical intelligence. Comput. Sci. Technol. 4, 500–508 (2016)
18. Sangnawakij, P., Niwitpong, S., Niwitpong, S.: Conﬁdence intervals for the ratio
of coeﬃcients of variation of the gamma distributions. Lecture Notes in Computer
Science, vol. 9376, pp. 193–203 (2015)

682
W. Thangjai et al.
19. Sangnawakij, P., Niwitpong, S.: Conﬁdence intervals for coeﬃcients of variation
in two-parameter exponential distribution. Commun. Stat. Simul. Comput. 46,
6618–6630 (2017)
20. Searls, D.T.: The utilization of a known coeﬃcient of variation in the estimation
procedure. J. Am. Stat. Assoc. 59, 1225–1226 (1964)
21. Srivastava, V.K.: On the use of coeﬃcient of variation in estimating mean. J. Indian
Soc. Agric. Stat. 26, 33–36 (1974)
22. Srivastava, V.K.: A note on the estimation of mean in normal population. Metrika
27, 99–102 (1980)
23. Srivastava, V.K., Singh, R.S.: Uniformly minimum variance unbiased estimator of
eﬃciency ratio in estimation of normal population mean. Stat. Prob. Lett. 10,
241–245 (1990)
24. Sodanin, S., Niwitpong, S., Niwitpong, S.: Conﬁdence intervals for common mean
of normal distributions with known coeﬃcient of variation. Lecture Notes in Com-
puter Science, vol. 9978, pp. 574–585 (2016)
25. Thangjai, W., Niwitpong, S., Niwitpong, S.: Conﬁdence intervals for the common
mean of several normal populations with unknown coeﬃcients of variation. Com-
mun. Stat. Simul. Comput. (2017, Submitted)
26. Tian, L.: Inferences on the common coeﬃcient of variation. Stat. Med. 24, 2213–
2220 (2005)
27. Tian, L., Wu, J.: Inferences on the common mean of several log-normal populations:
the generalized variable approach. Biometrical J. 49, 944–951 (2007)
28. Walpole, R.E., Myers, R.H., Myers, S.L., Ye, K.: Probability and Statistics for
Engineers and Scientists. Prentice Hall, Upper Saddle River (2012)
29. Weerahandi, S.: Generalized conﬁdence intervals. J. Am. Stat. Assoc. 88, 899–905
(1993)
30. Ye, R.D., Ma, T.F., Wang, S.G.: Inferences on the common mean of several inverse
Gaussian populations. Comput. Stat. Data Anal. 54, 906–915 (2010)
31. Zhang, G.: Simultaneous conﬁdence intervals for several inverse Gaussian popula-
tions. Stat. Prob. Lett. 92, 125–131 (2014)
32. Zou, G.Y., Donner, A.: Construction of conﬁdence limits about eﬀect measures: a
general approach. Stat. Med. 27, 1693–1702 (2008)
33. Zou, G.Y., Taleban, J., Hao, C.Y.: Conﬁdence interval estimation for lognormal
data with application to health economics. Comput. Stat. Data Anal. 53, 3755–
3764 (2009)

Estimating the Value of Cultural Heritage
Creativity from the Viewpoint of Tourists
Phanee Thipwong1, Chung-Te Ting2, Yu-Sheng Huang2,
Yun-Zu Chen3, and Wan-Tran Huang1(&)
1 Department of Business Administration, Asia University, Taichung, Taiwan
Panee_tipwong@yahoo.com, wthuangwantran7589@gmail.com
2 Department of Tourism, Food and Beverage Management, Chang Jung
Christian University, Tainan, Taiwan
{ctting,yshuang}@mail.cjcu.edu.tw
3 Department of Business Administration, Chang Jung Christian University,
Tainan, Taiwan
winniechen0224@gmail.com
Abstract. Creativity in cultural heritage raises expectations of added value, and
helps promote local economic development through new elements introduced to
the original cultural industry. The purpose of this study is to discuss the will-
ingness to pay (WTP) for commercialization and creativity of cultural heritage.
However, no general market price for cultural heritage exists. Thus, the
Contingent Valuation Method (CVM) in the non-market valuation method was
applied to determine whether commercialization and creativity of cultural her-
itage through a personally inherent attitude and through different preferences
could analyze the tourists’ willingness to pay, and to consider the factors that
inﬂuence their WTP. In this study, 410 subjects were used for the CVM con-
struction of the WTP based on three situations when cultural heritage and cre-
ativity are combined, as well as in the application of the double-bounded
dichotomous choice model of survival analysis to estimate the WTP-inﬂuencing
factors. The results showed that the higher the income of the subject, the higher
of WTP for value-added services for preserving cultural heritage, participating
in activities, and helping local development.
Keywords: Cultural heritage  Willingness to pay
Contingent valuation method
1
Introduction
International Council on Monuments and Sites (ICOMOS) deﬁnes heritage as a broad
concept that includes both natural and the cultural environments. It involves land-
scapes, historic sites, environment, biodiversity, collections, previous and continuing
cultural practices, knowledge, and living experiences. It records and expresses the long
processes of historic development, becoming the essence of diverse national, regional,
indigenous, and local identities. It is an integral part of modern life. Hence, cultural
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_49

heritage of historical signiﬁcance, traditions, and artistic value reﬂects not only our
ancestors’ lifestyles and attitudes toward life, but also those of the modern people.
Culture and cultural heritage are crucial to a people’s identity, self-respect, and
dignity [1]. Culture and cultural heritage are prominent resources in any society; thus,
cultural heritage serves as a tourism attraction. Tourism leads to ﬁnancial and political
support for the management of this heritage. Cultural heritage is an important resource.
Tourist trips typically include cultural heritage elements, ranging widely from a journey
to a historical town center, to a visit to a museum or a stroll around a historic garden
[2]. Therefore, a unique cultural heritage is an important attraction that helps develop
the tourism industry.
For this reason, the traditional historic sites are undergoing a redeﬁnition and
reinterpretation, creating and making cultural heritage competitive and attractive [3].
Landry claims that a city’s cultural heritage reﬂects its history, industries, and art assets,
including the physical buildings and landmarks, as well as the intangible local and
aboriginal life traditions, festivals, customs, legends, hobbies, and passion. Amateur
cultural activities, such as the ubiquitous traditional food and cooking, leisure activities,
clothing, and subculture, are also included [4]. In recent years, intangible heritage in the
form of indispensable performances and visual arts have become a wave of popular
emerging global culture industry. The concept of cultural industries comprises all
enterprises, including self-employed individuals whose economic activities focus on
production, dissemination, and intermediation of artistic and cultural products or ser-
vices [5]. In the other words, cultural industry covers a broad range (e.g., music
industry, publishing industry, arts, ﬁlm industry, and so on).
Belova et al. deﬁnes “creative industries” as entrepreneurial activities in which
economic value is linked to cultural content. Creative industries bring together the
traditional strengths of classical culture with the “added value” of entrepreneurial
strategies and the new knowledge-based electronic and communications skills. Thus,
creative industry enhances the economic impact of the familiar cultural giants, placing
particular emphasis on the contemporary. Experimented and engaged with modernity,
culture is able to renew and revitalize itself. The creative industries are thus the source
of the innovation and competitiveness essential to urban cultures in the fast changing
and globalizing world [6]. A couple of examples are the Creative Industries Devel-
opment Strategy proposed by Singapore and Creative Industries Mapping Documents
1998 by UK.
Ministry of Economic Affairs of Taiwan explains that “cultural and creative
industries” are simply those industries that have their origins in innovation or cultural
accretion. They have the potential to create wealth or jobs through the production and
utilization of intellectual property, which can help to enhance the living environment
for society as a whole [7]. While cultural and creative industry nowadays is an
important sign of global economic development, the industry itself and its extended
products have obtained huge proﬁts, not only for enterprises and countries, but also for
natural markets and civilizations all over the world. The marginal beneﬁt for sight-
seeing promotion is apparent. Thus, many local governments around the world try their
best promoting positive self-images by cultivating culture and creative industries. Due
to globalization and technology trends, modern economy has transformed into an
innovative knowledge-based economic structure.
684
P. Thipwong et al.

In this study, Tainan City’s cultural heritage is the research object. Tainan City is
Taiwan’s most ancient city that, during the Ming and Qing dynasties, served as the
location of the government and was the political, economic, and cultural center of
Taiwan. The city, as a result, possesses abundant potential cultural heritage.
The contingent valuation method (CVM) technique has great ﬂexibility, allowing
valuation of a wider variety of non-market goods and services than any of the indirect
techniques possible. It is, in fact, the only method currently available for estimating
nonuse values. In natural resources, the contingent valuation method (CVM) studies
generally derive values through the elicitation of the willingness to pay (WTP) of
respondents to prevent damage to natural resources or to restore damaged natural
resources [8]. Because no general market price of cultural heritage is available, this
study applies the non-market valuation of contingent valuation method (CVM) to build
the situation of cultural heritage creativity and to understand the willingness to pay
(WTP) of tourists to Tainan for the creation of cultural heritage goods, as well as for
commercialization.
Based on this description, this study uses the double-bounded dichotomous choice
as one of CVMs to ﬁnd out the willingness to pay (WTP) through the factors that
inﬂuence the economic value of creativity in cultural heritage and to provide the
reference of the relevant units for future cultural heritage planning. Hanemann et al.
indicates that the major advantage of the double-bounded dichotomous choice is that
one could identify the location of the maximum willingness to pay (WTP) value from
the data derived from this approach and that is an incentive compatible method. The
statistical efﬁciency of conventional dichotomous choice contingent valuation surveys
can be improved by asking each respondent a second dichotomous choice question
which depends on the response to the ﬁrst question—if the ﬁrst response is “yes,” the
second bid is some amount greater than the ﬁrst bid; while, if the ﬁrst response is “no,”
the second bid is some amount smaller [9].
2
Contingent Valuation Method (CVM)
CVM refers to a hypothetical survey method of valuing the beneﬁts of an intervention in
monetary terms by estimating the maximum WTP of an individual. Currently, an issue
in the application of CVM is the technique used to elicit this monetary valuation. The
most obvious way to measure non-market values is through directly questioning indi-
viduals on their WTP for a product or service. CVM is a survey or questionnaire-based
approach to the valuation of non-market products and services [10].
This study uses the questionnaire as a tool. The method has four non-market
ﬁnancial evaluation: open-ended, bidding game, payment card, and closed-ended. In
this study, the closed-ended of double-bounded dichotomous was used. In other words,
questionnaire design in the CVM is adopted by a closed-ended CV. The unique aspect
of the dichotomous-choice questions is that subjects are asked if they would pay a ﬁxed
sum of money for the item being evaluated. Moreover, the double-bounded dichoto-
mous choice approach is statistically more efﬁcient than the single-bounded dichoto-
mous choice approach [9, 11]. A typical close-ended question is a yes-no question.
Estimating the Value of Cultural Heritage Creativity
685

A respondent simply read the statement and answer yes or no. It is not necessary for the
respondent to elaborate thoughts or ideas.
According to Alberini, the questionnaire optimum amount is selected in the pre-test
questionnaire. The amounts are set from low to high, removing approximately 10% of
the extreme number, and selecting the amounts of 20%, 40%, 60%, and 80% as the ﬁrst
bids. If the response of the subjects in the ﬁrst bid is “no,” the ﬁrst bid will cut in half to
determine the amount for the second bid. Conversely, if the response of the subjects in
the ﬁrst bid is “yes,” the amount of the ﬁrst bid is increased by half the amount to set
the second bid. Alberini recommends that the double-bound dichotomous choice of bid
be divided into 4 to 6 groups. The most appropriate bid is divided into four groups [12].
This study focuses on three aspects whether a respondent has the willingness to pay
for creativity of cultural heritage. The three questions are: (1) Are you willing to pay
$___ to preserve cultural heritage? (2) Are you willing to pay $___ to participate in
activities if Tainan City accedes to artistic and cultural activities that can improve the
quality of leisure? (3) Are you willing to pay $___ to help if Tainan City undergoes
local development. If the respondent’s initial answer to each question with a “bid” of
money is “yes,” then a lightly increased amount is provided to see whether the
respondent is willing to pay more. On the contrary, if the initial answer to each question
is “no,” the “bid” is slightly decreased to evaluate the willing to pay.
In addition, the application of CVM to cultural heritage, due to the complexity of
the issue itself, is highly difﬁcult. Thus, it must be designed for simple and easy
situations. Subjects need to understand that the context because CVM is crucial to the
study of cultural heritage creativity.
3
Empirical Result
In this study, the socioeconomic background (gender), cultural heritage creativity
cognitive
factors
(socioemotional
cognition,
value-added
services
cognition,
overall-image cognition), cultural heritage creativity cognitive cluster (leisure emo-
tional expression group, pursuit of quality in leisure group, cold and conservative
group), travel experiences (participate in archaeological activities) of the subjects were
included in the model. Based on the foregoing theoretical model, an empirical model of
the cultural heritage creativity WTP was set using following formula:
WTP1 ¼ f ln income; sex; fac1; fac2; fac3; d1; d2; d3
ð
Þ
ð1Þ
WTP2 ¼ f ln income; sex; fac1; fac2; fac3; d1; d2; d3
ð
Þ
ð2Þ
WTP3 ¼ f ln income; sex; fac1; fac2; fac3; d1; d2; d3
ð
Þ
ð3Þ
Here, WTP1, WTP2, and WTP3, respectively, are three aspects of willing to pay:
preserving cultural heritage, participating in activities, and helping local development.
The socioeconomic variables of ln income is logarithmic of the income of the subject;
sex is a gender variable, male is 1, female is 0; fac1 is socioemotional cognition; fac2 is
value-added services cognition; fac3 is overall-image cognition; d1 and d2 are the
686
P. Thipwong et al.

dummy variables of the cluster; the leisure emotional expression group is set to 1 and 0;
the pursuit of quality in leisure group is set to 0 and 1; the cold and conservative group
is set to 0 and 0; and d3 is the dummy variable of participating in archaeological
activities, with participation as 1, did not participate as 0, preceding variable deﬁnition,
as shown in Table 1. In this study, after establishing the evaluation model, and in
accordance to the aforementioned variable data, the empirical analysis of the cultural
heritage creativity WTP pay for the maximum likelihood estimation (MLE) was
performed.
Preserving cultural heritage is shown in Table 2. Assessment model of the
log-normal was at 10% signiﬁcance, whereas the overall-image cognition (fac3) was
estimated to be negative and signiﬁcant. At 5% signiﬁcance, the leisure emotional
expression group (d1) was estimated to be positive and signiﬁcant. The assessment
model of the Weibull was at 10% signiﬁcance in the ln income was estimated to be
positive and signiﬁcant. Also, at 10% signiﬁcance, the value-added services cognition
(fac2) value of positive and signiﬁcant. At 1% signiﬁcance, the overall-image cognition
(fac3) value was negative and signiﬁcant. The assessment model of the gamma was at
5% signiﬁcance; thus, leisure emotional expression group (d1) was estimated to be
positive and signiﬁcant.
Participating in activities is shown in Table 3. Assessment model of the log-normal
distribution at 5% signiﬁcance of ln income was estimated to be positive and signiﬁ-
cant. At 5% signiﬁcance, the socioemotional cognition (fac1) was estimated to be
Table 1. Cultural heritage creativity WTP variables
Variable name
Variable descriptions
Socioeconomic variables
ln
income
Logarithmic of subject’s income (TWD)
Sex
Male as 1, Female as 0
Cultural heritage creativity
cognitive factors
fac1
Socioemotional cognition
fac2
Value-added services cognition
fac3
Overall-image cognition
Cultural heritage creativity
cognitive cluster
d1
Dummy variable of leisure emotional expression
group is set to 1
Dummy variable of pursuit of quality in leisure
group is set to 0
Dummy variable of cold and conservative group is
set to 0
d2
Dummy variable of leisure emotional expression
group is set to 0
Dummy variable of multi-cognition group is set to 1
Dummy variable of cold and conservative group is
set to 0
Travel experiences
d3
Dummy variable of participate in archaeological
activities, Participation = 1, non-participation = 0
Estimating the Value of Cultural Heritage Creativity
687

Table 2. Preserving cultural heritage results of the evaluation function
Variable name
Evaluation function of the probability of allocation
patterns
Log-normal
Weibull
Gamma
Intercept term
6.11 (0.67)
7.27 (0.71)
5.46 (0.61)
Socioemotional variables ln
income
0.97 (0.62)
1.36 (0.71)*
0.73 (0.54)
Sex
0.12 (0.13)
0.17 (0.13)
0.08 (0.12)
Cultural heritage
creativity cognitive
factors
fac1
−0.48 (0.33)
−0.64 (0.39)
−0.39 (0.29)
fac2
0.17 (0.18)
0.31 (0.19)*
0.04 (0.17)
fac3
−0.34 (0.17)*
−0.51 (0.17)***
−0.19 (0.16)
Cultural heritage
creativity cognitive
cluster
d1
0.38 (0.17)**
0.24 (0.18)
0.40 (0.16)**
d2
−0.02 (0.24)
−0.13 (0.25)
0.11 (0.22)
Travel experiences
d3
0.20 (0.24)
0.36 (0.24)
0.12 (0.22)
Scale
1.00 (0.05)
0.90 (0.05)
0.92 (0.07)
Log likelihood
−417.74
−435.39
−412.11
Restricted Log likelihood
−427.33
−447.10
−421.30
Log likelihood ratio
19.18
23.42
18.38
*,**,*** respectively signiﬁcant at 10%, 5%, 1%. Log-likelihood ratio = (−2)  (Restricted Log
likelihood −Log Likelihood), v2(7,0.95) = 14.07.
Table 3. Participating activities of the evaluation function
Variable name
Evaluation function of the probability of allocation
patterns
Log-normal
Weibull
Gamma
Intercept term
6.87 (0.76)
7.96(0.83)
6.13 (0.67)
Socioemotional variables ln
income
1.69 (0.69)**
1.85 (0.83)**
1.46 (0.64)**
Sex
−0.11 (0.15)
0.00 (0.15)
−0.14 (0.13)
Cultural heritage
creativity cognitive
factors
fac1
−0.76 (0.37)**
−0.79 (0.45)*
−0.72 (0.34)**
fac2
0.24 (0.21)
0.24 (0.22)
0.18 (0.19)
fac3
−0.49 (0.19)**
−0.54 (0.21)***
−0.37 (0.18)**
Cultural heritage
creativity cognitive
cluster
d1
0.30 (0.20)
0.16 (0.22)
0.32 (0.18)*
d2
−0.03 (0.27)
0.04 (0.29)
0.05 (0.24)
Travel experiences
d3
0.22 (0.27)
0.37 (0.29)
0.14 (0.24)
Scale
1.12 (0.06)
1.00 (0.05)
1.04 (0.07)
Log likelihood
−425.46
−444.68
−419.51
Restricted Log likelihood
−435.10
−454.39
−428.70
Log likelihood ratio
19.28
19.42
18.38
*,**,*** respectively signiﬁcant at 10%, 5%, 1%. Log-likelihood ratio = (−2)  (Restricted Log
likelihood −Log Likelihood), v2(7,0.95) = 14.07.
688
P. Thipwong et al.

negative and signiﬁcant. At 5% signiﬁcance, the overall-image cognition (fac3) was
estimated to be negative and signiﬁcant. The assessment model of the Weibull distri-
bution at 5% signiﬁcance ln income was estimated to be positive and signiﬁcant. At
10% signiﬁcance, the socioemotional cognition (fac1) was estimated to be negative and
signiﬁcant. At 1% signiﬁcance, the overall-image cognition (fac3) was estimated to be
positive and signiﬁcant. The assessment model of the gamma at 5% signiﬁcance of ln
income were estimated to be positive and signiﬁcant. At 5% signiﬁcance, the socioe-
motional cognition (fac1) was estimated to be negative and signiﬁcant. At 5% sig-
niﬁcant, the overall-image cognition (fac3) was estimated to be positive and signiﬁcant.
At 10% signiﬁcance, the leisure emotional expression group (d1) was estimated to be
positive and signiﬁcant.
Helping local development is shown in Table 4. Assessment model of the
log-normal distribution at 10% signiﬁcance of value-added services cognition (fac2)
were estimated to is positive and signiﬁcant. At 5% signiﬁcant, the overall-image
cognition (fac3) estimate is negative and signiﬁcant. At 10% signiﬁcance, the leisure
emotional expression group (d1) was estimated to be positive and signiﬁcant. The
assessment model of the Weibull at 10% signiﬁcance of ln income was estimated to be
positive and signiﬁcant. At 10% signiﬁcance, value-added services cognition (fac2)
was estimated to be positive and signiﬁcant. At 5% signiﬁcance, the overall-image
cognition (fac3) was estimated to be positive and signiﬁcant. The assessment model of
the gamma at 10% signiﬁcance of overall-image cognition (fac3) was estimated to be
Table 4. Local development results of the evaluation function
Variable name
Evaluation function of the probability of allocation patterns
Log-normal
Weibull
Gamma
Intercept term
5.36 (0.68)
6.42 (0.71)
4.83 (0.72)
Socioemotional
variables
ln
income
0.97 (0.63)
1.29 (0.70)*
0.80 (0.60)
Sex
0.13 (0.14)
0.18 (0.14)
0.12 (0.14)
Cultural heritage
creativity
cognitive factors
fac1
−0.39 (0.33)
−0.51 (0.38)
−0.33 (0.31)
fac2
0.36 (0.21)*
0.42 (0.22)*
0.32 (0.21)
fac3
−0.39 (0.19)**
−0.40 (0.19)**
−0.35 (0.18)*
Cultural heritage
creativity
cognitive cluster
d1
0.34 (0.19)*
0.13 (0.19)
0.42 (0.19)**
d2
−0.17 (0.27)
−0.18 (0.27)
−0.13 (0.26)
Travel
experiences
d3
−0.03 (0.25)
0.06 (0.25)
−0.08 (0.24)
Scale
1.09 (0.06)
0.93 (0.05)
1.10 (0.06)
Log likelihood
−423.24
−437.59
−421.48
Restricted Log likelihood
−432.12
−445.97
−430.89
Log likelihood ratio
17.76
16.76
18.82
*,**,*** respectively signiﬁcant at 10%, 5%, 1%. Log-likelihood ratio = (−2)  (Restricted Log
likelihood −Log Likelihood), v2(7,0.95) = 14.07.
Estimating the Value of Cultural Heritage Creativity
689

positive and signiﬁcant. At 5% signiﬁcance, leisure emotional expression group (d1)
was estimated to be positive and signiﬁcant.
The results of whether a consumer is willing to pay (TWD/year/person) for cre-
ativity in cultural heritage from three aspects are show in Table 5: preserving cultural
heritage, 922.9 TWD; participating in activities, 914.2 TWD; helping local develop-
ment, 620.9 TWD.
4
Conclusions and Discussion
The results demonstrate that the higher the income of the subjects, the higher of WTP
for Weibull, value-added services cognition (fac2) factor in preserving cultural heritage
and helping local development. When the subjects strongly agree, WTP is higher for
Weibull in the leisure emotional expression group (d1). When subjects can more easily
express their own leisure emotional needs, WTP was higher for the log-normal and
gamma distributions for the cultural heritage creativity.
The highest amount of WTP in the three situations is for preserving cultural her-
itage, for which subjects were willing to pay 922.9 TWD. This result shows that people
with cultural heritage cognition want to maintain cultural heritage to give it more
added-value. The lowest amount of WTP in the three situations is helping the local
development, where subjects are willing to pay 620.9 TWD. The possible reason for
this result is that the subjects are not local residents and have no local identity; thus,
they do not think of the local development combined with the cultural heritage can
affect WTP.
In fact, participating activities has similar amount as preserving cultural heritage.
Subjects were willing to pay 914.2 TWD, possibly because subjects have positive
experience of participating activities. Watching ballet dance or appreciating modern art
display at a historical building can provide the subject with a unique sensation and then
encourage them to participate more often. At the same time, the cultural heritage can be
better known by people.
As a result, in order to develop better cultural heritage creativity and tourism,
cultural heritage management is crucial. While the consumers are willing to pay for
preserving cultural heritage, local governments should work on the recognition of
heritage in both local residents and outsiders. Take Tainan for example. Being a city full
Table 5. Cultural heritage creativity WTP
WTP item
WTP
(TWD/year/person)
Lower (TWD/year
person)
Upper
(TWD/year/person)
Preserving cultural
heritage
922.9
630.5
1369.7
Participating in
activities
914.2
587.1
1453.0
Help local
development
620.9
416.0
941.3
690
P. Thipwong et al.

of cultural heritage in Taiwan, its government should pay attention to making Tainan
and its culture recognized by potential consumers in order to help its tourism sustain.
While consumers have various activities to participate, the tour becomes more inter-
esting and attractive to greater number of potential consumers. Hence, the creativity
added in cultural heritage is the key to the sustainability of tourism. In other words,
cultural heritage creativity helps the local tourism, and the proﬁt made from the tourist
industry can possibly help preserving the local cultural heritage. Such dynamic eco-
nomic value will make contribution to the local development, helping the local indus-
tries to thrive.
References
1. Andersen, K.: Sustainable Tourism and Cultural Heritage: A Review of Development
Assistance and its Potential to Promote Sustainability. World Bank, Washington (1999)
2. De la Torre, M.: Assessing the Values of Cultural Heritage: Research Report. The Getty
Conservation Institute, Los Angeles (2002)
3. Nasser, N.: Planning for urban heritage places: reconciling conservation, tourism, and
sustainable development. J. Plann. Lit. 17(4), 467–479 (2003)
4. Landry, C.: The Creative City: A Toolkit for Urban Innovators. Earthscan Publications Ltd.,
London (2008)
5. Fesel, B., Söndermann, M.: Culture and creative industries in Germany. German commission
for UNESCO, Graﬁsche Werkstatt Druckerei und Verlag (2007)
6. Belova, E., Cantell, T., Causey, S., Korf, E., O’Connor, J.: Creative industries in the modern
city: Encouraging enterprise and creativity in St. Petersburg. Tacis Cross Border
Cooperation Small Project Facility (2002)
7. Ministry of Economic Affairs: White Paper on SMEs in Taiwan, pp. 162–165 (2004)
8. Lipton, D.W., Wellman, K., Sheifer, I.C., Weiher, R.F.: Economic valuation of natural
resources: a handbook for coastal policymakers. National Oceanic and Atmospheric
Administration (NOAA), USA (1995)
9. Hanemann, M., Loomis, J., Kanninen, B.: Statistical efﬁciency of double-bounded
dichotomous choice contingent valuation. Am. J. Agric. Econ. 73, 1255–1263 (1991)
10. Smith, R.D.: Contingent valuation: indiscretion in the adoption of discrete choice question
formats? Centre for Health Program Evaluation Working paper 74 (1997)
11. Boyle, G.: Dichotomous-choice, contingent-valuation questions: functional form is impor-
tant. Northeast. J. Agric. Resour. Econ. 19(2), 25–31 (1990)
12. Alberini, A.: Efﬁciency vs bias of willingness-to-pay estimates: bivariate and inter-data
models. J. Environ. Econ. Manag. 29, 169–180 (1995)
Estimating the Value of Cultural Heritage Creativity
691

A Bibliometric Review of Global Econometrics
Research: Characteristics and Trends
Van-Chien Pham1 and Man-Ling Chang2(&)
1 Department of Business Administration, Asia University,
Taichung 41354, Taiwan
2 Department of Business Administration, National Chung Hsing University,
Taichung 402, Taiwan
manllian@dragon.nchu.edu.tw
Abstract. Using a bibliometric analysis, this research analyses on the Social
Science Citation Index (SSCI) publications from the Institute for Scientiﬁc
Information (ISI), Web of Science database, based on 12965 publications from
1497 journals during 1992 to 2016. The research was assessed the research’s
characteristics and trends of most productive countries/regions and institutions,
was pointed out the sharp increasing in China on econometrics research,
Applied Economics (England) published the most econometrics articles. The
research was also pointed out the temporal evolution of recent hot econometrics
research issues. Global trends and characteristics was found throughout this
research can give a general overview for further researches on econometrics.
Keywords: Trend research  Bibliometric analysis  Econometrics
SSCI  Web of Science
1
Introduction
Bibliometric analysis is a analyzing method based on statistics, is applied widely in
sciences and also in research analysis. The research characteristics and/or trends of
research in economics were also conducted by bibliometricians, such as in ﬁnancial [1],
accounting [2], sport-economics [3], as well as management [4]. Despite importance of
researching about, there is not many approaches by bibliometric method about
econometrics research so far. By bibliometric method, Baltagi [5] studied the charac-
tersistics of econometrics research by ranking the most productive authors, institutions
and countries based on the data of 16 leading international journals in the area and
cover the period 1989–2005.
In this research, we will use statistical analysis approach with titles, abstracts,
author keywords of articles, information about countries and institutions, in order to
dissect and assess global trends and characteristics of econometrics research from 1992
to 2016, so as to bring a panorama about econometrics research to researchers and to
help them to identify suitable researching orientation as well.
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_50

2
Data Sources and Research Methodology
Data for this research was obtained from the online version of the SSCI, the Web of
Science. The online version of the SSCI database was searched by the keyword
“Econometrics” as a part of the title, abstract, and keywords (author keywords) to
compile all manuscripts related to econometrics research. Besides, the term “Econo-
metric” was also used as supporting searching-word because of differences in the
terminology of authors.
The impact factor (IF), subject category and rank in category of the journals were
gathered from the 2015 Journal Citation Reports (JCR). The contributions of different
countries/territories and institutes were determined by the participation of at least one
author of the publication, through the author addresses. The term “independent
country” was understood as all authors who are from the same country, the term
“international collaborative” was used when the coauthors are from more than one
country.
Due to limitations of the SSCI database, before 1992 articles were incomplete
abstract information, while the analysis in this study needs to use the related infor-
mation in the title, abstract and keywords. Therefore, the publications before 1992 will
not be considered. All of the articles in the 25-years period 1992–2016 were evaluated
by the following aspects: document type and language of publication, publication
characteristics by region, country, institution, author, relevant journals and hot topic
analysis.
3
Result and Discussion
3.1
Document Type and Language of Publication
Seventeen document types were found in the total 12965 publications during the
25-years study period. The journal article, as the most frequency document type,
comprises 92.0% (11913) of the total production, and followed distantly by proceed-
ings papers (802; 6.2%), book reviews (416; 3.2%), editorial materials (289; 2.2%),
and reviews (244; 1.9%).
One hundred ninety articles were published in 1992, and the number of articles
went up slightly to 335 in 2004, then increased rapidly in the period from 2005 to 2013
with 853 articles published in 2013, before gently to reach a pick of 909 articles in
2015 and decreased in 2016 (Fig. 1). The number of proceedings papers and book
reviews decreased after 2008 and maintained a low level afterwards. Journal articles
represented the majority of document types, therefore only the 11913 articles were used
in further analysis.
There was 16 languages in use, ninety-seven percent of total articles in this period
were published in English, other languages that were generally less appeared were
Spanish (0.95%), French (0.75%), Czech (0.48%), German (0.35%).
A Bibliometric Review of Global Econometrics Research
693

3.2
Countries/Continents Research Characteristics
In recent researches about the global research trends, the total of articles index (in-
cluding independent and international collaborative articles) were commonly used for
analyzing research trends [6, 7]. The top 20 productive countries ranked by the total
number of econometrics articles with more than 150 articles published includes two
North American countries (USA and Canada), thirteen Europe countries (UK,
Germany, France, Italy, Spain, Netherlands, Switzerland, Greece, Belgium, Sweden,
Austria, Norway, and Turkey), four Asia countries (China, Japan, South Korea, Tai-
wan), and Australia. USA is the most frequency country in econometrics research, with
4414 articles, accounting to 38% of total articles in the world, followed by UK (1647,
14%), Germany (848, 7.4%). Research by European scholars increased rapidly and has
become the most productive continent since 2004 (see Fig. 2).
Fig. 1. Trends of articles, proceedings papers, and book reviews on Econometrics from 1992 to
2016
694
V.-C. Pham and M.-L. Chang

Besides, the Fig. 2 also shows that while there are the dwelling of research by
Europe and American authors in recent years, research trend of econometrics in Asia
continued to increase strongly. To clarify this trend line, an analysis for the Asia is
conducted with ﬁve most productive countries in the continent. The result in Fig. 3
indicates the rise of Asia after 2005 until now is due to the sharply increasing of China.
Research trend of econometrics in China increased sharply from 2006 onwards, with
520 articles in the priod 1992–2016 (300 articles were published in the last 4 years),
China has the highest rate of publication growth in the world.
3.3
Most Relevant Journals and Subject Categories
In total, 11913 articles were published in 1445 journals. There were 10 journals with
more than 100 publication articles in econometrics from 1992–2016, one-sixth of all
articles was published in this ten journals (Table 1). Applied Economics (England)
published the most econometrics articles (346, 2.9%), following by Journal of
Econometrics (Netherlands, 331), and Econometric Theory (USA, 236).
Six of the top 10 most relevant journals were classiﬁed under Economics subject
category by JCR in 2015, three of them were classiﬁed under the subject categories
related to Mathematics.
Fig. 2. Publication trends of regions
A Bibliometric Review of Global Econometrics Research
695

3.4
Institutions’ Research Characteristics
Table 2 ranks the top 11 most inﬂuential institutions by total number of articles pub-
lished with more than 100 publication articles in econometrics research from 1992–
2016, which is as many as ten institutions were in the USA, the another institution is
University of Cambridge in UK. The leading institutions were University of Illinois
with 131 articles (accounting for 1.14% of total publications), University of California
Berkeley (128, 1.11%), and University of Wisconsin (121, 1.05%). Notably, there was
the appearance of a ﬁnancial institution, the World Bank, in the top 11 most inﬂuential
institutions, the only institutions in the list is not a university. Most of publications of
the World Bank on econometrics were published by researchers in Development
Research Group.
The University of Illinois was also ranked at ﬁrst place about number of ﬁrst author
articles, while the highest number of corresponding author articles and independent
articles belong to the University of Wisconsin. In econometrics research, University of
California at Berkeley had the strongest researching collaboration with outside insti-
tutions, also had the highest inter-instutionally collaborative ratio among the 11 most
inﬂuential institutions, along with Cornell University and Stanford University (84%).
All the top 11 most inﬂuential institutions in econometrics research (actually all 39
leading institutions with the number of articles of more than 60, which the table does
not shows) have the high inter-instutionally collaborative ratio of more than 60%.
Fig. 3. Publication trends of ﬁve most active countries of Asia
696
V.-C. Pham and M.-L. Chang

3.5
Authorship Characteristics
Table 3 indicates the top 15 most productive authors with more than 19 publications in
the period 1992–2016. Bhat, the most productive researcher from University of Texas
at Austin (USA), published 37 articles, followed by Phillips (35), and Mcaleer (34).
Bhat also published the highest number of articles as corresponding author (33).
Table 1. Top 10 most relevant journals ranked by total number of articles with impact factor,
JCR category and rank in category.
Journal
Number
of
Articles
(%)
Impact
factor
(IF)
JCR Category
Rank in
Category
Country of
Publisher
Applied
Economics
346 (2.9)
0.586
Economics
236/345
England
Journal of
Econometrics
331 (2.8)
1.611
Mathematics,
Interdisciplinary
Applications
30/101
Netherlands
Econometric
Theory
236 (2.0)
1.162
Mathematics,
Interdisciplinary
Applications
Statistics and
Probability
48/101
50/123
USA
Economic
Modelling
205 (1.7)
0.997
Economics
149/345
Netherlands
Energy Policy
163 (1.4)
3.045
Energy & Fuels
Environmental
Sciences
29/88
59/225
England
Energy
Economics
153 (1.3)
2.862
Economics
22/345
Netherlands
American Journal
of Agricultural
Economics
148 (1.2)
1.436
Agricultural
Economics &
Policy
5/17
USA
Economics
Letters
143 (1.2)
0.603
Economics
230/345
Switzerland
Applied
Economics
Letters
142 (1.2)
0.378
Economics
280/345
England
Journal of
Applied
Econometrics
108
(0.91)
1.872
Economics
Social sciences,
Mathematical
Methods
57/345
9/49
England
A Bibliometric Review of Global Econometrics Research
697

Table 2. Top 11 most inﬂuential institutions of articles during 1992–2016
Institute
Total
number of
articles (%)
Rank of number
of ﬁrst author
articles (%)
Rank of number of
corresponding author
articles (%)
Rank of number
of independent
articles (%)
Rank of number of
inter-instutionally
collaborative articles (%)
Inter-instutionally
collaborative ratio *
University of
Illinois, USA
131 (1.14)
1 (0.67)
2 (0.67)
3 (0.62)
2 (1.5)
76
University of
California at
Berkeley, USA
128 (1.11)
3 (0.57)
3 (0.55)
13 (0.42)
1 (1.6)
84
University of
Wisconsin, USA
121 (1.05)
2 (0.66)
1 (0.73)
1 (0.96)
12 (1.1)
60
World Bank, USA
110 (0.96)
7 (0.50)
8 (0.48)
6 (0.50)
5 (1.3)
77
Cornell University,
USA
110 (0.96)
5 (0.50)
4 (0.53)
22 (0.36)
3 (1.4)
84
Harvard
University, USA
104 (0.90)
4 (0.51)
5 (0.51)
2 (0.70)
15 (1.1)
66
Massachusetts
Institution of
Technology, USA
103 (0.90)
5 (0.50)
6 (0.50)
26 (0.34)
4 (1.3)
83
University of
Maryland, USA
102 (0.89)
10 (0.44)
9 (0.47)
11 (0.44)
9 (1.2)
78
University of Penn,
USA
101 (0.88)
18 (0.37)
16 (0.39)
26 (0.34)
7 (1.3)
83
University of
Cambridge, UK
101 (0.88)
9 (0.48)
11 (0.44)
5 (0.56)
12 (1.1)
72
Stanford
University, USA
101 (0.88)
14 (0.42)
13 (0.41)
29 (0.32)
5 (1.3)
84
* the percentage of inter-instutionally collaborative articles in total institution articles
698
V.-C. Pham and M.-L. Chang

Phillips was ranked at ﬁrst on total publications as ﬁrst author (23), as well as inde-
pendent author (16). As noted by Chang and Ho [1], a bias can exit in authorship
analysis due to the same name of two or more authors, or the different names was used
by an author in his or her manuscripts.
3.6
Most Frequently Cited Articles
Table 4 lists the top 13 most frequently cited articles ranked by total citations from the
publication year to 2016. Eight of these 13 articles were published in the ﬁrst decade of
the 21st century, 5 before 2000, and no articles published in the last 7 years (among
2010–2016). Journal of Health Economics and Journal of Econometrics are two journal
in which two of their articles are listed in the top 13. The most frequently cited was
“How much should we trust differences-in-differences estimates?” by Bertrand, Duﬂo,
and Mullainathan. This articles was published in 2004 and was cited 1776 times among
2004–2016, also had the highest average time cited per year (136.6 times).
Analysing the time cited each year from the publication year until 2016 for the top
13 most frequently cited articles showns two articles with the rapidly rising in the
number of time the articles was cited. That is “How much should we trust
differences-in-differences estimates?” that noted above and “How to do xtabond2: An
introduction to difference and system GMM in Stata” by David Roodman.
Table 3. Top 15 most productive authors on econometrics between 1992 and 2016
Author
Rank (TP) Rank (TPF) Rank (TPR) Rank (TPI)
Bhat, CR
1 (37)
4 (17)
1 (33)
11 (6)
Phillips, PCB
2 (35)
1 (23)
2 (22)
1 (16)
Mcaleer, M
3 (34)
29 (8)
19 (10)
126 (2)
Franses, PH
4 (23)
22 (9)
9 (12)
126 (2)
Baltagi, BH
5 (22)
2 (22)
3 (20)
6 (7)
Hendry, DF
6 (21)
6 (15)
7 (14)
4 (9)
Fingleton, B
6 (21)
4 (17)
6 (15)
2 (12)
Anselin, L
6 (21)
3 (18)
5 (16)
6 (7)
Tsionas, EG
9 (20)
17 (10)
9 (12)
11 (6)
Plantinga, AJ
9 (20)
55 (6)
40 (7)
400 (1)
Filippini, M
9 (20)
9 (12)
157 (4)
400 (1)
Serletis, A
12 (19)
8 (14)
4 (17)
400 (1)
Egger, P
12 (19)
9 (12)
12 (11)
25 (4)
Colombo, MG 12 (19)
6 (15)
30 (8)
#N/A
Chavas, JP
12 (19)
29 (8)
30 (8)
126 (2)
TP: total publications
TPF: total publications as the ﬁrst author
TPR: total publications as the corresponding author
TPI: total publications as the independent author
N/A: not available
A Bibliometric Review of Global Econometrics Research
699

The manuscript was published in 2009 about application of system generalized
method-of-moments estimators with xtabond2 in Stata is also the most recently articles
was published in the list.
Table 4. Top 13 most frequently cited econometrics research articles ranked by total citations to
2016
Authors
Journal
PY
Title
TC
C/Y
Bertrand, M; Duﬂo,
E; Mullainathan, S
Quarterly
Journal of
Economics
2004
How much should we trust
differences-in-differences
estimates?
1776
136.6
Brazier, J; Roberts,
J; Deverill, M
Journal of
Health
Economics
2002
The estimation of a
preference-based measure of
health from the SF-36
1315
87.7
Fornell, C;
Johnson, MD;
Anderson, EW;
Cha, JS; Bryant,
BE
Journal of
Marketing
1996
The American customer
satisfaction index: Nature,
purpose, and ﬁndings
933
44.4
Manning, WG;
Mullahy, J
Journal of
Health
Economics
2001
Estimating log models: to
transform or not to transform?
886
55.4
Goffe, WL; Ferrier,
GD; Rogers, J
Journal of
Econometrics
1994
Global optimization of statistical
functions with simulated
annealing
820
35.7
Dufﬁe, D; Pan, J;
Singleton, K
Econometrica
2000
Transform analysis and asset
pricing for afﬁne jump-diffusions
818
48.1
Papke, LE;
Wooldridge, JM
Journal of
Applied
Econometrics
1996
Econometric methods for
fractional response variables with
an application to 401(k) plan
participation rates
762
36.3
Heckman, JJ;
Ichimura, H;
Todd, P
Review of
Economic
Studies
1998
Matching as an econometric
evaluation estimator
760
40.0
Gali, J; Gertler, M
Journal of
Monetary
Economics
1999
Inﬂation dynamics: A structural
econometric analysis
744
41.3
Bai, JS; Ng, S
Econometrica
2002
Determining the number of
factors in approximate factor
models
728
48.5
Imbens, GW
Review of
Economics
and Statistics
2004
Nonparametric estimation of
average treatment effects under
exogeneity: A review
685
52.7
Smith, JA; Todd,
PE
Journal of
Econometrics
2005
Does matching overcome
LaLonde’s critique of
nonexperimental estimators?
673
56.1
Roodman, David
Stata Journal
2009
How to do xtabond2: An
introduction to difference and
system GMM in Stata
632
79.0
PY: Publication year; TC: Total citations from the publication year to 2016; C/Y: Average citations per year
700
V.-C. Pham and M.-L. Chang

Table 5. Top 24 most frequency of author keywords used among 1992–2016 and in ﬁve 5-years
periods
Author
keywords
1992–2016 total
number of articles
(%)
1992–
2016
Rank
(%)
1992-1996
Rank (%)
1997-2001
Rank (%)
2002–
2006
Rank
(%)
2007-2011
Rank (%)
2012–
2016
Rank
(%)
Spatial
econometrics
391
1 (4.8)
24 (0.54)
5 (2.1)
1 (4.1)
1 (5.3)
1 (5.6)
Econometrics
319
2 (3.9)
2 (7.3)
2 (4.2)
2 (3.5)
2 (3.6)
2 (3.9)
Econometric
models
282
3 (3.5)
1 (7.8)
1 (5.4)
3 (3.3)
3 (3.4)
3 (2.7)
Panel data
221
4 (2.7)
24 (0.54)
4 (3.3)
4 (3.1)
4 (3.0)
5 (2.6)
Economic
growth
173
5 (2.1)
43 (0.27)
12 (1.0)
6 (1.6)
5 (2.1)
4 (2.7)
Cointegration
164
6 (2.0)
4 (3.0)
3 (4.0)
5 (2.4)
6 (2.0)
8 (1.5)
China
154
7 (1.9)
24 (0.54)
30 (0.60)
7 (1.5)
7 (1.7)
6 (2.5)
Forecasting
106
8 (1.3)
3 (4.1)
6 (1.9)
9 (1.1)
8 (1.1)
11 (1.1)
Productivity
95
9 (1.2)
5 (1.9)
6 (1.9)
8 (1.3)
13 (0.97)
12 (1.0)
Econometric
analysis
90
10 (1.1)
43 (0.27)
55 (0.30)
9 (1.1)
22 (0.76)
7 (1.6)
Econometric
modeling
88
11 (1.1)
8 (1.6)
6 (1.9)
24 (0.61)
17 (0.84)
10 (1.2)
Foreign direct
investment
(FDI)
85
12 (1.0)
43 (0.27)
20 (0.75)
37 (0.52)
8 (1.1)
9 (1.3)
Innovation
79
13 (0.97)
43 (0.27)
20 (0.75)
16 (0.79)
8 (1.1)
12 (1.0)
Time series
78
14 (0.96)
5 (1.9)
20 (0.75)
13 (0.87)
11 (1.1)
16 (0.86)
Efﬁciency
67
15 (0.82)
16 (0.81)
9 (1.3)
37 (0.52)
12 (1.1)
21 (0.69)
Monetary policy
62
16 (0.76)
16 (0.81)
10 (1.2)
11 (1.0)
20 (0.80)
34 (0.56)
Endogeneity
61
17 (0.75)
#N/A
20 (0.75)
21 (0.70)
28 (0.67)
14 (0.89)
Instrumental
variables
57
18 (0.70)
12 (1.4)
15 (0.90)
24 (0.61)
26 (0.72)
25 (0.64)
Causality
57
18 (0.70)
24 (0.54)
30 (0.60)
16 (0.79)
17 (0.84)
23 (0.67)
Unemployment
56
20 (0.69)
16 (0.81)
30 (0.60)
13 (0.87)
53 (0.46)
17 (0.81)
Convergence
53
21 (0.65)
24 (0.54)
30 (0.60)
16 (0.79)
22 (0.76)
34 (0.56)
Bayesian
econometrics
53
21 (0.65)
43 (0.27)
#N/A
46 (0.44)
22 (0.76)
17 (0.81)
Inﬂation
52
23 (0.64)
43 (0.27)
30 (0.60)
16 (0.79)
22 (0.76)
34 (0.56)
Growth
52
23 (0.64)
16 (0.81)
30 (0.60)
24 (0.61)
17 (0.84)
40 (0.53)
Economic
development
48
25 (0.59)
#N/A
55 (0.30)
24 (0.61)
31 (0.63)
23 (0.67)
Poverty
47
26 (0.58)
#N/A
69 (0.15)
37 (0.52)
15 (0.88)
40 (0.53)
Climate change
47
26 (0.58)
#N/A
69 (0.15)
73 (0.26)
53 (0.46)
14 (0.89)
International
trade
46
28 (0.56)
#N/A
20 (0.75)
46 (0.44)
77 (0.34)
19 (0.78)
Bootstrap
46
28 (0.56)
43 (0.27)
12 (1.0)
24 (0.61)
38 (0.55)
46 (0.50)
* N/A: not avaiable
A Bibliometric Review of Global Econometrics Research
701

3.7
Hot Issues
Researching the temporal evolution of hot research issues identiﬁes for us the most
attractive matters of researchers (and also of entreprises, institutions). It is very useful
for apprehending the research trends in recent years, then establishing better direction
for further researches. The bibliometric approach is one of the best ways to analyze
this. In analyzing hot issues, the bibliometric method uses statistical techniques to
analyzing frequency of words in titles, author keywords, and occasionally in abstracts.
To identify hot issues in econometrics research, author keywords, titles, and abstracts
statistical analyses were used.
Analysis of single word in the titles shows some of the most used word by authors
is “analysis” (1244 articles, 10.4% of total articles), “econometric” (1191, 10.0%),
“evidence” (994, 8.3%), “model” (782, 6.6%), “economic” (647, 5.5%), “market” (605,
5.1%), “spatial” (600,5.0%), “growth” (558, 4.7%). “Economic”, “spatial”, “impact”,
“china”, “development” are the words which the strong uptrends, while there is the
downtrends in the use of “estimation”, “forecasting”, “application”, and “modelling”.
The most frequency of author keywords were used in the period 1992–2016 with
more than 100 articles including “spatial econometrics” (391 articles), “econometrics”
(319), “econometric models” (282), “panel data” (221), “economic growth” (173),
“cointegration” (164), “china” (154), and “forecasting” (106) (see Table 5).
For more detailed analysis of the global trends in econometrics research, ﬁve
macroeconomics and four microeconomics issues were examined. Figure 4 shows the
comparison of the growth trends of ﬁve macroeconomics hot issues in econometrics
research. “Monetary policy” (including “exchange rate”) is the most interesting subject
Fig. 4. Comparison the growth trends of ﬁve macroeconomic groups
702
V.-C. Pham and M.-L. Chang

in many years before 2012. However, from 2012, “economic growth” takes its place.
Research on economic growth in econometrics increased rapidly from 2004 to 2014
before dwelling in the last two years. Recent researches on economic growth, through
Fig. 5. Comparison the growth trends of four microeconomic groups
Fig. 6. Comparison the trends of three structures of data
A Bibliometric Review of Global Econometrics Research
703

econometrics methods, provided evidences to clarify the relationship between eco-
nomic growth and other social and economic issues. Detailed analysis for all 213
articles with the word “economic growth” in the title shows that energy consumption is
the most interesting issue in the relationship with economic growth (with 22 in total
213 articles), which is to be found in researches for China [8], as well as Asean
countries [9], Aﬁca [10], and Central America [11]. There are also much work on
inﬂation, international trade, foreign direct investment with the uptrends, and are all
increased in 2016.
On the other hand, in applied econometrics in microeconomics research, pricing is
the hostest issue, which is strongly cared from 2006 (Fig. 5). Involving to “pricing”
matters includes “asset pricing”, “price elasticity”, “hedonic pricing”, “option pricing”.
Research on demand and productivity although had the rising trend-lines, but decreased
sharply in 2016.
3.8
Structures of Data Used in Applied Econometrics Research
Data structures of panel data (longitudinal data), time series data, and cross-sectional
data are widely used in applied econometrics to describe various econometric situations
[12]. Figures 6 shows the comparison of the trends of this three data structures in
econometrics research. Before 2004, econometrics scholars prefer to use time series
data best. The interest in time series econometrics continued to grow rapidly, however,
showing signs of slowing down and giving up this most attractiveness place to panel
data econometrics from 2009. The increase in publications ralated to panel data
indicates the expectation of researchers for more realistic speciﬁcations and better
understanding of microeconomic behaviors [13] as noted by two founder fathers of
panel data econometrics in their article published in 1966: “One of the main reasons for
being interested in panel data is the unique possibility of uncovering disaggregate
dynamic relationships using such data sets” [14].
4
Conclusions
Researching on SSCI database of econometrics has already gained some signiﬁcant
results about global research trends and characteristics. Most of the articles originated
from USA researchers, according to 38% of all articles, while China has the highest
rate of publication growth in the world, shows the biggest attention to econometrics
research. Universities in USA overwhelmingly occupy nine out of eleven places in the
list of most productive institutions of econometrics articles during 1992–2016. Applied
Economics and Journal of Econometrics have the most number of published articles.
Recent researches concentrate on hot issues: spatial econometrics, economic growth,
forecasting, and panel data econometrics.
This research indicated global trends and characteristics on econometrics research
in the period 1992–2016. These trends shall orientate for further researches about
econometrics.
704
V.-C. Pham and M.-L. Chang

References
1. Chang, C.-C., Ho, Y.-S.: Bibliometric analysis of ﬁnancial crisis research. Afr. J. Bus.
Manage. 4(18), 3898–3910 (2010)
2. Merigó, J.M., Yang, J.-B.: Accounting research: a bibliometric analysis. Aust. Account. Rev.
27(1), 71–100 (2017). https://doi.org/10.1111/auar.12109
3. Santos, J., Garcia, P.: A bibliometric analysis of sport economics research. Int. J. Sport
Finance 6(3), 222–244 (2011)
4. Podsakoff, P., MacKenzie, S., Podsakoff, N., Bachrach, D.: Scholarly inﬂuence in the ﬁeld of
management: a bibliometric analysis of the determinants of university and author impact in
the management literature in the past quarter century. J. Manag. 34(4), 641–720 (2008)
5. Baltagi, B.H.: Worldwide econometrics rankings: 1989-2005. Econometric Theory 23, 952–
1012 (2007). https://doi.org/10.1017/S026646660707051X
6. Han, J.-S., Ho, Y.-S.: Global trends and performances of acupuncture research. Neurosci.
Biobehav. Rev. 35, 680–687 (2011)
7. Li, J., Wang, M.-H., Ho, Y.-S.: Trends in research on global climate change: a Science
Citation Index Expanded-based analysis. Global Planet. Change 77, 13–20 (2011)
8. Wang, S., Zhou, C., Li, G., Feng, K.: CO2, economic growth, and energy consumption in
China’s provinces: investigating the spatiotemporal and econometric characteristics of
China’s CO2 emissions. Ecol. Ind. 69, 164–195 (2016)
9. Heidari, H., Katircioglu, S., Saeidpour, L.: Economic growth, CO2 emissions, and energy
consumption in the ﬁve Asean countries. Int. J. Electr. Power Energy Syst. 64, 785–791
(2015)
10. Wolde-Rufael, Y.: Energy consumption and economic growth: the experience of African
countries revisited. Energy Econ. 31(2), 227–234 (2009)
11. Apergis, N., Payne, J.: Energy consumption and economic growth in central America:
evidence from a panel cointegration and error correction model. Energy Econ. 31(2), 211–
216 (2009)
12. Burdisso, T., Sangiácomo, M.: Panel time series: Review of the methodological evolution.
Stata J. 16(2), 424–442 (2016)
13. Dupont-Kieffer, A., Pirotte, A.: The early years of panel data econometrics. Hist. Polit. Econ.
43, 258–282 (2011). https://doi.org/10.1215/00182702-1158754
14. Balestra, P., Nerlove, M.: Pooling cross-section and time-series data in the estimation of a
dynamic model: the demand for natural gas. Econometrica 34, 585–612 (1966)
A Bibliometric Review of Global Econometrics Research
705

Macro-Econometric Forecasting
for During Periods of Economic Cycle
Using Bayesian Extreme Value
Optimization Algorithm
Satawat Wannapan(B), Chukiat Chaiboonsri, and Songsak Sriboonchitta
Faculty of Economics, Chiang Mai University,
Chiang Mai, Thailand
lionz1988@gmail.com, chukiat1973@gmail.com
Abstract. This paper aims to computationally analyze the extreme
events which can be described as crises or unusual times-series trends
among the macroeconomic variables. These data are statistically esti-
mated by employing the optimally extreme point for supporting policy
makers to specify the economic expansion target and economic warn-
ing level. The Nonstationary Extreme Value Analysis (NEVA) applying
Bayesian inference and Newton-optimal method are employed to com-
plete the researchs solutions and estimate the time-series variables such
as GDP, CPI, FDI, and unemployment rate collected during 1980 to
2015. The results show there are extreme values in the trend of macroe-
conomic factors in Thailand economic system. This extreme estimation
is presented as an interval. In addition, the empirical results from the
optimization approach state that the exactly extreme points can be com-
putationally found. Ultimately, it is clear that the computationally sta-
tistical approach, especially Bayesian statistics, is inevitably important
for econometric researches in the recent era.
Keywords: Extreme event
Nonstationary Extreme Value Analysis (NEVA) · Newtons method
Diﬀerential Evolution Monte Carlo (DE-MC) · Macroeconomics
1
Introduction
Macroeconomics has been mentioned as an element that forms the part of a
complex whole. In other words, the study of possible sources of economic ﬂuctu-
ations has been the major preoccupation in macroeconomics in recent years. As
seen in Fig. 1, the examples of macroeconomic factors such as Thailand GDP,
consumer price index (CPI), foreign direct investment (FDI), and rates of unem-
ployment, evidently indicate that there are many ﬂuctuated time-series trends
found in Thai economic system during 1980 to 2015. Accordingly, this seems
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_51

Macro-Econometric Forecasting of Economic Cycle
707
that to investigate these oscillated points for explaining and forecasting economic
situations is extremely crucial.
Interestingly, the question is fundamental if one shall gain insight into the
workings of the economy, and aid in the formulation and conduct of economic
policy (Bjrnland 2000). To address this issue, the combination between a mul-
tiplex econometric method and mathematically statistical analysis, including
the Bayesian extreme value model and Newton-optimal processing, was com-
putationally employed to focus on a precise estimation, which is crucial for
economic predictions. Consequently, for the former model, extreme events in
the macroeconomic factors will be clariﬁed, and these empirical results can evi-
dence the rare situations in Thai economic system. For the latter method, the
precisely extreme point will be computationally announced for providing policy
recommendations.
Fig. 1. Presenting the macroeconomic variables of Thailand economic system (GDP,
CPI, FDI, and unemployment rate)
2
The Objective and Scope of Research
The paper aims to study the extreme events of Thailand macroeconomic vari-
ables as well as clarify the extremely optimal point to notify the alarming sign for
policy makers. In addition, the scope of this research intensively focuses on math-
ematically computational approaches applying to the econometric analysis. The
overview of the conceptual framework and Bayesian extreme value optimization
algorithm under a covering of this study is presented in Fig. 2.

708
S. Wannapan et al.
Fig. 2. The conceptual frame work and Bayesian extreme value optimization algorithm
of the research
3
Literature
Undeniably, a business cycle in an economic system and macroeconomic analysis
have been considerably interested and econometrically investigated by many
scholars for few decades. In business cycle theory, this study has been a renewed
interest since the instability of the world economy in the aftermath of the oil
price shocks in the 1970’s. More recently, a new branch of the classical models
referred to as the Real Business Cycle (RBC) models have been developed, that
emphasize real productivity shocks, as opposed to aggregated demand shocks, as
a source of economic ﬂuctuations (Kydland and Prescott 1982; Bjrnland 2000).
According to macroeconomic research papers, for instance, Chow (2001),
Hrdahl et al. (2004), Visco (2014), Hall et al. (2014), and Wu and Xia (2016),
this group of researchers worked on the topic that speciﬁed the way to under-
stand and explain economic crises as well as econometrical forecasting. Also, their
empirical statements are only based on normal situational data and classically
statistical analyses. On the other hand, focusing on the subjective statistical

Macro-Econometric Forecasting of Economic Cycle
709
analysis called Bayesian extreme value approach for macroeconomic factorial
estimating, this seems that there are few researchers studying on this method.
For example, Calabrese and Giudici (2015) who explored the generalized extreme
value (GEV) for the basis of both macroeconomic and bank-speciﬁc microeco-
nomic factors, and Chaiboonsri and Chaitip (2017) who investigated the extreme
linkage between the ﬁnancial indexes such as SGX, KLSE, SET, IDX, and PSE
and the macroeconomic variable (GDP) based on Bayesian inferences. Interest-
ingly, this paper is the extension of the Bayesian extreme value computation by
employing the optimization processing called Newton Method. Moreover, this
mathematically computational analysis relied on macroeconomic factors can be
one of useful solutions regarding the issue of economic alarming signs, which
have been diﬃcultly deﬁned for long time.
4
Research Methodology
4.1
The ADF Unit Root Test Based on Bayesian Inference
The ADF test analyzes the null hypothesis that a time-series index is stationary
against the alternative (non-stationary data). Conditionally, dynamics in data
have the autoregressive moving average model (ARMA) (Said and Dickey 1984).
The ADF test is based on estimating the regression test, which is expressed by
the Eq. (1),
Δyt = c + α
′Dt + ϕyt−1 +
p

j=1
γjΔyt−j + εt.
(1)
The prior density of ϕ is formulated and expressed in the Eq. (2),
p(θ) = p(φ)p(a∗|φ)
(2)
The marginal likelihood for φ is
l(φ|D)α

l(ϕ|D)φ(a∗|φ)da∗
(3)
After setting the prior, we give the error measure and compute a single point
estimate of WLS for the weight. The likelihood and the prior are speciﬁed. Thus,
we calculate the posterior distribution over w, which explains our belief level,
via Bayes rule (Eq. (4)),
p(w|t, α, σ2) = likelihood × prior
normalised factor = p(t|w, σ2)p(w|α)
p(t|α, σ2)
.
(4)
In academic researches, Bayesian statistics considers hypotheses regarding
multiple parameters by adapting Bayes factor comparisons. Let Mi be the model
devised in the term of the null hypothesis and Mj be the model of the alternative
hypothesis. The posterior odds ratio of Mi and Mj is shown in the Eq. (5),
pr(Mi|y)
pr(Mj|y) = pr(y|Mi)
pr(y|Mj) × π(Mi)
Mj
.
(5)

710
S. Wannapan et al.
The Bayes factor is summarized by the statistical model proposed by Jeﬀrey
(1961). The interpretation in half-units on Jeﬀreys scales was simply explained
as (Table 1):
Table 1. Presentation the summary of Jeﬀreys Guideline for model comparison
Items
The recommendations of Jeﬀrey’s was modiﬁed by Authors
BF < 1/10
Strong evidence for Mj
1/10 < BF < 1/3 Moderate evidence for Mj
1/3 < BF < 1
Weak evidence for Mj
1 < BF < 3
Weak evidence for Mi
3 < BF < 10
Moderate evidence for Mi
10 < BF
Strong evidence for Mi
Source: modiﬁed from Jeﬀreys (1991) by authors
4.2
Generalized Pareto Distributions (GDP) and Non-stationary
Extreme Value Theory Analysis
At the beginning, focusing on the class of issues where the behavior of the
distributions over (below) a high (small) threshold is of interest, characterized
as extreme events. A random quantity with distribution function F(x) under
certain conditions shown by Pickands (1975), F(x|u) = P(X ≤u+x|X > u), can
be approximated by a Generalized Pareto distribution (GPD), which is deﬁned
in Eq. (6).
G(x|ζ, σ, u) =

1 −(1 + ζ(x−u)
σ
)−1/ζ, if ζ ̸= 0
1 −exp[−(x−u)
σ
], if ζ = 1
(6)
where σ > 0 and ζ are the scale and shape parameter, respectively. The data
exhibit heavy tail behavior when ζ > 0. To conduct the threshold come from a
GPD introduced in Eq. (6), the proposed model assumes that observations under
the threshold, u, is generated from a certain distribution with parameters, η.
The model is here denoted as H(.|η) . Hence, the distribution function F of any
observation X can be described as
F(x|η, ζ, σ, u) =

H(x|η), if x < u
H(u|η) + [1 −H(u|η)]G(x|ζ, σ, u), if x ≥u.
(7)
For an Eq. (7) which is a sample size n, x = (x1, ......, xn) from F, parameter
vector θ = (η, σ, ζ, u) , A = (i : xi) < u, and B = (i : xi) , the likelihood function
is deﬁned as (Behrens et al. 2004)
L(θ; x) =

A
h(x|η)

B
(1 −H(u|η)){ 1
σ [1 + ζ(xi −u)
σ
]−(1+ζ)/ζ
+
},
(8)

Macro-Econometric Forecasting of Economic Cycle
711
for ζ ̸= 0, and L(θ; x) = 
A h(x|η) 
B(1 −H(u|η))[(1/σ)exp{(xi −u)/σ}], for
ζ = 0.
The threshold, u, as shown in Eq. (8), is the point where the density has a
discontinuity, which depends on the larger or smaller jumped density, and each
case the choice of which observations will be considered as exceedances that can
be more obvious or less evident. The smaller jumped density is more diﬃcult
to estimate the threshold. Thus, strong discontinuities, or large jumps, indicate
separation of the data such that it is expected the parameter estimation would
be easier. As shown in Fig. 3, the density model of threshold is schematically
presented.
Fig. 3. Presentation the schematics of the threshold model (Modiﬁed from Behrens
et al. 2004)
In terms of Nonstationary Extreme Value Analyses (NEVA) using Bayesian
Inference, applying from Cheng et al. (2014), NEVA employs a Bayesian techni-
cal computation to infer the GPD distribution of parameters under stationary
and non-stationary conditions. Typically, the Bayesian inference consisting of
Markov chain Monte Carlo (MCMC) approach for obtaining the posterior dis-
tribution of parameters from an arbitrary distribution has been popular and
used in several investigations of extremes (Coles and Powell 1996; Cheng et al.
2014). This approach integrates the knowledge brought by a prior distribution
and the observational vector; −→y = (yt)t=1:Nt; and the posterior distribution of
parameters; θ = (μ, σ, ζ). In this case, Nt refers the number of observations in
the observable vector −→y . The omission prior for the location and scale parame-
ters are non-informative normal distributions. It can be adjusted to informative
priors, and other choices of distribution functions can be used in NEVA.
Assuming the linkage between observations, the Bayes theorem for estimation
of GPD parameters under the non-stationary assumption can be expressed as
(Renard et al. 2006; Coles 2001; Cheng et al. 2014) (see an Eq. (9))
p(β|−→y , x)αp(−→y |β, x)p(β|x)
(9)

712
S. Wannapan et al.
p(β|−→y , x) =

Nt,t=1
p(−→y |β, x(t)) =

Nt, t=1
p(−→y |μ(t), σ, ζ),
(10)
where β = (μ1, μ0, σ, ζ) in Eq. (10) are estimated parameters. Conversely, the
stationary model can be described as a special case of the above two equation
without x(t),
p(θ|−→y )αp(−→y |θ)p(θ) =

Nt,t=1
p(yt|θ)p(θ),
(11)
where x(t) refers to the set of all covariate values under the non-stationary
assumption. The outcomes of posterior distributions p(θ|−→y ) as well as
p(θ|−→y , x) contribute information about parameters under the stationary con-
dition θ = (μ, σ, ζ) or non-stationary model β
= (μ1, μ0, σ, ζ) (see an
Eq. (11)). Technically, in this paper, NEVA generates a large number of
realizations from the parameter joint posterior distribution. Modernly, the
Differential Evolution Markov Chain (DE −MC) (Ter Braak 2006; Cheng
et al. 2014) is applied. The DE-MCMC conducts and utilizes the genetic algo-
rithm, which is the Diﬀerential Evolution (DE) for global optimization over the
parameter space with the (DE) approach.
Obviously, the DE-MC algorithm employs the ﬁtness assignment scheme used
in the corresponding Multi-objective Shuﬄed Complex Evolution Metropolis
(MOSCEM) algorithm, which has demonstrated certain eﬀectiveness and eﬃ-
ciency in the multi-objective optimization of macro-econometric models, and it
is relied on the number of external non-dominated points (Zhu and Li 2010).
Inside the framework of the ﬁtness assignment, the calculation of a population
P with m individual conﬁgurations, C1, Cm, is based on the strength Si of each
non-dominated conﬁguration Ci. The strength Si is described as the proportion
of conﬁgurations in P dominated by Ci. Then, the ﬁtness of an individual Ci is
represented as follows, (see an Eq. (12)),
fit(Ci) =
si
1 + i=j
i
sj,
(12)
where fit(Ct) = si refers to Ct is non-dominated. Conversely, fit(Ct) =
si denotes Ct is dominated. The conﬁgurations with ﬁtness less than 1.0
are technically non-dominated. The ﬁtness function inﬂuences to the non-
dominated solutions with less dominated conﬁgurations while those with a
lot of neighbors in their niche are penalized (Vrugt 2002; Cheng et al. 2014).
Modernly, the new algorithm of Monte Carlo simulated processing employs
differential Evolution (DE) to produce new conﬁgurations based on the cur-
rent population. Inside this scheme, conﬁgurations Ci(x1, ...., xn) and mutant
vectors Vi is normally set as follows, (see an Eq. (13)),
Vi = Cr1 + F(Cr2 −Cr3)
(13)
where i, rl, r2, and r3 are randomly integer numbers in the interval [1, m], and
F > 0 is a tunable ampliﬁcation control constant. In addition, other formations
of mutant vector generating are also provided in the study explored by Storn

Macro-Econometric Forecasting of Economic Cycle
713
and Price (1997). Consequently, a new conﬁrmation C
′
i(x1′, ...., x
′
n) is simulated
by the crossover operation on Vi and Ci, and these can be deﬁned in Eq. (14).
x
′
j =
vj,
j = (k)n, [k + 1]n, ..., [k + L + 1]n,
xj,
otherwise
(14)
where [.]n indicates the modulo operation with modulus n, k is a randomly
generated integer from the interval [0, n −1], L is an integer drawn from [0,
n −1] with probability Pr(L = l) = (CR)l, and CR ∈[0, 1) is the crossover
probability.
Necessarily, Monte Carlo movements apply the Metropolis Transitions for
generating new population. Deﬁning this transition as a probability, the metropo-
lis transition relies on the variation of the ﬁtness function value. The Metropolis-
Hastings ratio is expressed as (see an Eq. (15)):
w(Ci →Ci
t) = e
−(fit(c
′
t)fit(Ct)
T
,
(15)
where T is the simulated macroeconomic population, which is used to control the
acceptance rate of the Metropolis transitions. Also, the new form C
′
i generated
by DE, which is conﬁrmed with the probability as follows (see an Eq. (16)):
min(1, w(ci →c
′
i))
(16)
Ultimately, the majority of the computations of DE-MC convolutions such as
the generating of new conﬁgurations, evaluation of objective functions, ﬁtness
assignments, and Metropolis transitions can be independently complemented,
and this modernly simulated calculation only requires scrutinizing operations to
access the shared population information, which is particularly suitable for a
massively parallel computing system with shared memory (Cheng et al. 2014).
4.3
The Statistical Probability for Generating Random Variable
Sets
4.3.1
Random Sets
Since the data (information) from the Bayesian extreme value estimation is
resulted as an interval of the ﬁnite numerical set, random elements based on
the mathematical theory of probability are used to performance experimental or
observing data sets. A random element is a map X : Ω →U (two arbitrary sets)
where Ω equipped with a σ - ﬁeld and U equipped with σ - ﬁeld (U) such that
X−1(U) ⊆A. When P is a probability measure on, the law of X is PX = PX−1
(probability measure on U). The law Px contains all probabilistic information
for studying X, and the range U of X is the space of our data where U = Rd
is an Euclidean space and continuous function on [0, 1]; U = C([0; 1]). Hence, a
random element whose range U is a space of set called a random set (roughly
speaking: it is a set obtained at random) (Nguyen 2014).
Considering into random sets as sampling designs, they are the outcome of
a random experiment where the results of the experiment are sets. For instance,

714
S. Wannapan et al.
when we wish to obtain a portion of a (ﬁnite) population, we randomly design a
procedure to achieve it. Formally, as shown in Eq. (17), let U be a ﬁnite set and
its power set 2U is interested. In order to choose elements of 2U, a probability
density function f: 2U is [0, 1], 
A⊆U f(A)=1 is designed at random. A sampling
design is a ﬁnite random set S on the population of interest, and it is a random
element; S : (Ω, A, P) →(2U, 22U , pS) with probability density f(.)(P(S =
A)) = f(A). Its distribution F is deﬁned as 2u →[0,1] where P(S = A) = F(A) =

B⊆A f(B). Thus, the degree of the separation of the design is characterized
by the coverage function (Nguyen 2014). This can be described as follows
πS : U →[0, 1],
πS(u) = P(u ∈S) =

u∈A
f(A)
(17)
where the simple random sample, which is a random sample of size n, is explained
as (see an Eq. (18))
P(A) =
 1
N
n ,
if |A| = n.
0,
if |A| ̸= n
(18)
For the experimental data sampling, this type of data sample is created by
random sampling designs. Let p(.) be given and supposed that A ⊆V such that
(see an Eq. (19))

K∈A
Xk →

K∈V
Xk
(19)
4.4
Investigating the Extreme Point by Using Newton Optimization
Approach
Following the process of random variable generating, the interesting point that
can be noticed in the randomly extreme variable analysis is which point is
the exact extreme position for macroeconomic factors using in this study. To
mathematically solve the issue, a computationally operational method, namely
Newtons method is employed to investigate the optimization for clarifying an
extreme spot in Thai macroeconomic variables. Typically, the basic idea of New-
tons method is used as a basis on linearization. Given R1 →R1 is a diﬀerential
function. Thus, we are trying to solve the equation as (see Eq. (20))
F(x) = 0
(20)
Technically, the initial point x0 is made to be the starting points, and assem-
bled the linear approximation of F(x) in the neighbor of x0 : F(x0+h) ≈F(x0)h
and resolve the consequent linear formation F(x0) + F
′(x0)h = 0 As a result,
the recurrent method is expressed as follows (Polyak 2007):
xk+1 = xk −F
′(xk)−1F(xk),
k = 0, 1, .....
(21)
The above equation (Eq. (21)) is the method invented by Newton in 1669.
More precisely, this arising linear equation is dealt with polynomial which is

Macro-Econometric Forecasting of Economic Cycle
715
expressed of F(x0 +h). Moreover, the progress in improvement of the method is
related to such famous mathematicians such as Fourier who proved the quadratic
method convergences in the neighborhood of a root in an Eq. 18 as well as
Cauchy provided the multidimensional extension and used the method to prove
the existence of a root of an equation in 1829 (Polyak 2007). In this paper, the
authors employ the values of random variables, which are the results calculating
from the random sampling variables, to be the data set for lag optimizing.
Theoretically, the quadratic convergence in Newtons method is based on two
fundamentals.
Theorem 1. The main convergence result is deﬁned as continuously diﬀeren-
tiable on the data set: B = |x : |x −x0| ≤r| which is the visible linear oper-
ation model (F
′(x0)), where (x ∈B), expressed as |F
′(x0)−1F(x0)| ≤η and
|F
′(x0)−1F
′(x)| ≤K such that h = Kη < 1
2 and r ≥
η
h2k (2h)2k. Consequently,
the process in an Eq. (21) is well clariﬁed and it will converge to x∗with a
quadratic rate as follows (Polyak 2007),
|xk −x∗| ≤
η
h2k (2h)2k.
(22)
Theorem 2. Suppose F is deﬁned and twice continuously diﬀerentiable on the
balls B = [x : |x −x0| ≤r] Diﬀerentially, the linear function F
′(x) is invertible
on B and |F
′(x)−1| ≤β, |F
′(x)| ≤K, |F
′(x)| ≤η, where x ∈B. Now, we can
achieve linear parameters as follows: h = Kβ2η < 2 and r ≥βη ∞
n=0( h
2 )2k−1.
Thus, the solution x∗∈B and the progress converge to x∗with quadratic rate
can be deﬁned as (see an Eq. (22))
|xk −x∗| ≤βη(h/2)2k−1
1 −(h/2)2k .
(23)
Graphically, as presented in Fig. 4, the convergence processing of Newtons
method approximates F
′(x) by its tangent line at x(0) whose root, x(1), serves
as the next approximation of the true, x∗. The next similar step yields x(2),
which is already gone closer to the root at x∗
Fig. 4. Presentation the convergence processing in Newton-optimal approach

716
S. Wannapan et al.
5
Empirical Results
5.1
Descriptive Information
Generally, the basic information, including average values, max-min points, and
standard deviation are displayed in Table 2. Technically, collected variables such
as GDP, CPI, FDI, and unemployment rate are transformed into growth rates.
Table 2. Descriptive data of Thailand macroeconomic variables
GDP
CPI
FDI
UNEM
Mean
5.196206
3.993222 2.360140 1.948571
Median
5.533828
3.323594 2.278402 1.500000
Maximum
13.28811
19.70425
6.434801 5.800000
Minimum
−7.633734 −0.898069 0.419531 0.700000
Std.Dev
4.032781
3.755795 1.483663 1.221468
Observations 35
35
35
35
Source: From computed
5.2
The Test of Data Stationary Conditions
Obviously, macroeconomic factors especially used to do the investigation of
extreme dependences are the time-series information. Before computationally
statistical analyses, these variables should be checked the data stationary condi-
tion. Technically, the Bayesian inference is employed to verify the stable condi-
tion. The unit-root test is shown in Table 3, and the details have both stationary
(I(0)) and non-stationary (I(1)) variables.
Table 3. The ADF unit root test based on Bayesian inference in yearly data of Thailand
macroeconomic factors during 1980 to 2015
Variables
Bayesian
factor
model
Hypotheses
Numbers of
MCMC
regress
iterations
Bayesian
factor
ratios
(M1/M2)
Interpretation
of the
Bayesian
factor
Result
Thai GDP
Model 1
Model 2
H0 (Mi):
Non-stationary
H1 (Mj): Stationary
101,000
74,000
0.204
Moderate
evidence for
Mj
I(0)
CPI
Model 1
Model 2
H0 (Mi):
Non-stationary
H1 (Mj): Stationary
101,000
92,000
0.000
Strong
evidence for
Mj
I(0)
FDI
Model 1
Model 2
H0 (Mi):
Non-stationary
H1 (Mj): Stationary
101,000
92,000
0.205
Strong
evidence for
Mj
I(0)
Unemployment
rate
Model 1
Model 2
H0 (Mi):
Non-stationary
H1 (Mj): Stationary
101,000
101,000
68
Strong
evidence for
Mi
I(0)
Source : From computed

Macro-Econometric Forecasting of Economic Cycle
717
5.3
The Investigation of the Extreme Value Estimation Using
Bayesian Inference
As expressed in Table 4, with the Bayesian extreme value analysis called Non-
stationary Extreme Value Analysis (NEVA), its advantage is this approach can
computationally explore the extreme events of the observable variables, even
though the set of data contains both stationary and non-stationary conditions
(Cheng et al. 2014). Empirically, the results deduced from the Bayesian approach
found that the values of each factor are displayed as an interval. The details are
shown in the Figs. 5, 6, 7 and 8 in the Appendix (part 1), respectively. Descrip-
tively, the interval of the extreme value regarding Thailand GDP is between 8%
and 13%. For the consumer price index (CPI), the length of the extreme inter-
val is during 5.8% to 12.5%. Considering the extreme result of yearly foreign
direct investments (FDI), the interval is the length between 2 and 4%. For the
factor of Thailand unemployment rates, the non-stationary estimated outcome
shows that the extreme interval is between 2.3 and 4.4%. Lastly, for the general
Thailand stock value, the extremely calculated interval is between 200 and 500%,
respectively.
5.4
The Newtons Method for Optimizing the Extreme Values
To extend the details of the results estimated from Bayesian extreme value
processing, the Newton-optimization method can be the eﬃciently computa-
tional statistical tool for providing the precisely optimal position to clarify that
which an extreme value should be announced to be the alarming point. By
comparing with the general mean, the process of the Newton-optimal approach
is more complex and reliable since the simulated iterations were employed. As
seen the details in Table 3, the empirical results presented that there are the
diﬀerences between the Newton-optimal value and general mean. Descriptively,
the optimal point for GDP is 10.55 (10.50%), CPI is 8.96 (9.15%), FDI is 3.18
(3.00%), and unemployment rate is 3.496 (3.35%), respectively. In addition, the
optimal extreme point of each macroeconomic factor is graphically displayed in
the Figs. 9, 10, 11 and 12 in the Appendix (part 2).
Table 4. Presentation the optimally extreme value calculation of Thailand macroeco-
nomic factors
Variables
(Growth rate)
The extreme interval
(Bayesian extreme
estimation) (Percent)
General mean
(Percent)
Optimal value
(Newtons method)
(Percent)
GDP
8 13
10.5
10.55
CPI
5.8 12.5
9.15
8.96
FDI
2–4
3
3.18
Unemployment
rate
2.3–4.4
3.35
3.496
Source: From computed

718
S. Wannapan et al.
6
Conclusion
This paper successfully clariﬁes the extreme events among ﬁve macroeconomic
factors such as the expansion rates of Thailand GDP, CPI, FDI, unemployment,
and stock values. The main propose of this study is to apply the computation-
ally statistical approaches, including the nonstationary extreme value analysis
(NEVA) using Bayesian inference, random variable processing, and Newton-
optimal method, to solve the alarming point based on extreme events. With
time-series data collected during 1980 to 2015 (35 observations), the empirically
estimated results state that the NEVA method can clearly provide the interval of
extreme value measurement of each variable. Moreover, the Newton optimization
method can computationally investigate the exact position for the speciﬁcation
of economic alarming levels.
Following the previous statement, to precisely forecasting the alarming sign for
the economic system, especially in the macroeconomic level, is very necessary. The
empirical outcomes in this paper such as the interval of extreme value calculations
and the optimal point can be usefully supported policy makers to simultaneously
decide the economic expansion target and economic alarm level. Accordingly, it is
obvious that mathematical theorems and computationally statistical methods are
inevitably crucial for econometric researches in the recent era.
Appendix A: The Bayesian extreme value estimation
(Part 1)
Fig. 5. Presentation the Bayesian extreme result regarding Thailand GDP

Macro-Econometric Forecasting of Economic Cycle
719
Fig. 6. Presentation the Bayesian extreme result regarding CPI
Fig. 7. Presentation the Bayesian extreme result regarding FDI

720
S. Wannapan et al.
Fig. 8. Presentation the Bayesian extreme result regarding unemployment rate
Appendix B: The Newton-optimal processing (Part 2)
Fig. 9. Presentation the Newton-optimal point for the growth rate of GDP

Macro-Econometric Forecasting of Economic Cycle
721
Fig. 10. Presentation the Newton-optimal point for the growth rate of CPI
Fig. 11. Presentation the Newton-optimal point for the growth rate of FDI
Fig. 12. Presentation the Newton-optimal point for the growth rate of unemployment

722
S. Wannapan et al.
References
Behrens, C.N., Lopes, H.F., Gamerman, D.: Bayesian analysis of extreme events with
threshold estimation. Stat. Modell. 4, 227–244 (2004)
Bjrnland, H.C.: VAR Models in Macroeconomic Research. Statistics Norway Research
Department, Norway (2000)
Calabrese, R., Giudici, P.: Estimating bank default with generalised extreme value
regression models. J. Oper. Res. Soc. 66(11), 1783–1792 (2015)
Cheng, L., AghaKouchak, A., Gilleland, E., Katz, R.W.: Non-stationary extreme value
analysis in a changing climate. Clim. Change 127, 353–369 (2014)
Chaiboonsri, C., Chaitip, P.: Forecasting methods for safeguarding ASEAN-5 stock
exchanges during extreme volatility. Int. J. Trade Global Markets 10(1), 123–130
(2017)
Chow, G.C.: Econometric and economic policy. Stat. Sin. 11, 631–660 (2001)
Coles, S.: Introduction to Statistical Modeling of Extreme Values. Springer, London
(2001)
Coles, S.G., Powell, E.A.: Bayesian methods in extreme value modelling. Int. Stat. 64,
114–193 (1996)
Hall, S.G., Roudoi, A., Albu, L.L., Lupu, R., C˘alin, A.C.: Lawrence R. Klein and the
economic forecasting a survey. Roman. J. Econ. Forecast. 17(1), 5–14 (2014)
Hrdahl, P., Tristani, O., Vestin, D.: A joint econometric model o f macroeconomic
and term structure dynamics. Working paper number 405. European Central Bank
(2004). http://www.ecb.int
Jeﬀreys, H.: Theory of Probability, 3rd edn. Oxford University Press, New York (1961)
Kydland, F.E., Prescott, E.C.: Time to build and aggregate ﬂuctuations. Econometrica
50, 1345–1370 (1982)
Nguyen, H.T. Probability for statistics in econometrics. Center of Excellence in Econo-
metrics, Faculty of Economics, Chiang Mai University, Thailand (2014). http://old.
viasm.edu.vn/wp-content/uploads/2014/11/VIASMWorkshop.pdf
Pickands, J.: Statistical inference using extreme order statistics. Ann. Stat. 3, 119–131
(1975)
Polyak, B.T.: Newtons method and its use in optimization. Eur. J. Oper. Res. 181,
1086–1096 (2007)
Renard, B., et al.: An application of Bayesian analysis and Markov chain Monte Carlo
methods to the estimation of a regional trend in annual maxima. Water Resour. Res.
42 (2006)
Said, S.E., Dickey, D.: Testing for unit roots in autoregressive moving-average models
with unknown order. Biometrika 71, 599–607 (1984)
Storn, R., Price, K.: Diﬀerential evolution – a simple and eﬃcient heuristic for global
optimization over continuous spaces. J. Glob. optim. 11, 341–359 (1997)
Ter Braak, C.J.F.: A Markov chain Monte Carlo version of the genetic algorithm diﬀer-
ential evolution: easy Bayesian computing for real parameter spaces. Stat. Comput.
16, 239–249 (2006)
Visco, I.: Lawrence R. Klein: macroeconomics, econometrics and economic policy. J.
Pol. Model. 36, 605–628 (2014)
Vrugt, J.A., Gupta, H.V., Bastidas, L.A., Boutem, W., Sorooshian, S.: Eﬀective and
eﬃcient algorithm for multiobjective optimization of hydrologic models. Water
Resour. Res. 39(8), 1214–1232 (2002)
Wu, J.C., Xia, F.D.: Measuring the macroeconomic impact of monetary policy at the
zero lower bound. J. Money Credit Bank. 48(2–3), 254–291 (2016)

Macro-Econometric Forecasting of Economic Cycle
723
Zhu, W., Li, Y.: GPU-accelerated diﬀerential evolutionary Markov chain Monte Carlo
Method for multi-objective optimization over continuous space. In: Proceedings of
the 2nd Workshop on Bio-Inspired Algorithms for Distributed Systems, BADS 2010,
pp. 1–8 (2010)

Forecasting of VaR in Extreme Event Under
Economic Cycle Phenomena for the ASEAN-4
Stock Exchange
Satawat Wannapan(B), Pattaravadee Rakpuang, and Chukiat Chaiboonsri
Faculty of Economics, Chiang Mai University, Chiang Mai, Thailand
lionz1988@gmail.com, kaiimook1994@gmail.com, chukiat1973@gmail.com
Abstract. This paper was proposed to computationally investigate
the cycling details and risk management of the ASEAN-4 ﬁnancial
stock indexes, including Bangkok Bank (BBL), Development Bank of
Singapore Limited (DBS), Commerce International Merchant Bankers
(CIMB), and Bank Mandiri (Mandiri). These daily time-series data
were observed during 2012 to 2017. Technically, this paper employed
the econometric tool called Markov Switching Model (MS-model), the
extreme value application called Generalized Pareto Distribution (GPD-
model), and the risk management method called Value at Risk (VaR)
to provide the estimated solutions and recommendations for investing
in these ﬁnancial stocks. Empirically, the switching regime estimation
resulted that these four ﬁnancial indexes obviously contain real business
cycling movements, which were described as bull and bear regimes. Addi-
tionally, the results estimated by the GPD model conﬁrmed that there
were extreme events inside the trends of the four stock indexes. Ulti-
mately, the outcomes calculated by the risk measurement for extreme
cases, which were economic crises, stated that there was an enormously
high risk to considerably invest only in short earnings within these four
ﬁnancial stock indexes. Consequently, long-run investment should be
mentioned.
Keywords: Financial stock index · ASEAN · MS-model
GPD estimation · Value at Risk (VaR)
1
Introduction
In the new era of modern economic developments, the ﬁnancial sectors, especially
in the ASEAN Economic Community (AEC), are on which one of the driving eco-
nomic factors that many researchers are now focusing. Generally, most countries
in ASEAN have been deﬁned as the emerging market, which can interestingly
be the target for investing. However, not every country gives priority to ﬁnancial
sectors intensively, even though this ﬁeld is necessary. As a result, to clarify the
details of permanent ﬁnancial issues in ASEAN countries is inevitable. Thus,
the examination on the ﬁnancial sectors in ASEAN are really signiﬁcant due to
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_52

Forecasting of VaR in Extreme Event Under Economic Cycle Phenomena
725
Fig. 1. Presentation the descriptive data of ASEAN ﬁnancial stock indexes
speculations and long-term investments. In particular, the risk forecasting in the
ﬁnancial market is one part that is important to decide a suitable choice for the
portfolios of investors.
Consideration into the ﬁnancial index of major countries in ASEAN, the
details were graphically shown in Fig. 1. BBL - Bangkok Bank is one of the
biggest ﬁnancial companies in Thailand. DBS - The Development Bank of
Singapore Limited is the major Singaporean banking service. For Malaysia,
CIMB - Commerce International Merchant Bankers Bhd is deﬁned to be the
ﬁnancial leader, and Mandiri Bank Mandiri is the main banking company in
Indonesia. Moreover, AEC countries have been recently improving their power
on bargaining international transactions in the world ﬁnancial market. Undeni-
ably, many investors are very interested to invest in the various types of business
tasks. Consequently, this paper is contributed to statistically investigate the rare
situations, which can be also expressed as extreme dependences, in the ﬁnancial
sectors of ASEAN countries and conclude suitable recommendations for address-
ing the issue.
2
The Objective of Research
The main objective of this paper is to explore extreme events and provide choices
for risk reduction in the ﬁnancial indexes which are the banking stock indicators
in four countries of AEC for recommending suitable choices of ﬁnancial invest-
ments. The data was collected as daily time-series information, which was during
2012 to 2016.

726
S. Wannapan et al.
3
Literature
Because the Real Business Cycle theory (RBC) based on microeconomics can be
an adequate basis for understanding business cycles, the theoretical research con-
ducted by King and Rebelo (2000) stated that the real business cycle approach
as the new orthodoxy of macroeconomics (New Keynesian school) is now com-
monly applied, employing to the tasks in monetary economics, public ﬁnance,
asset pricing and so on. The empirical outcome of this remeasurement suggested
the nature of macroeconomic ﬂuctuations originated from real causes (shocks)
as regimes, which is a prematurely dismissed idea ignored by economists that
needs to be considered. In addition, Ernst and Stockhammer (2003) analyzed the
Austrian business cycle model to compare the Keynesian and classical economic
thoughts, using the result to develop the dynamics of European economic ﬂuctu-
ations. Interestingly, for the new era of econometric research, real business cycle
theory is becoming more and more crucial for describing economic ﬂuctuations
in real circumstances.
To extend the issue regarding economic cycle in this econometric investiga-
tion, extreme value theory has been considered to predict risks in ﬁnancial factors
(for example, banking and insurance) and to discover probability of risks in the
future. The exploration on the testing of GEV conditions as well as the theo-
retical review in the statistical tools by Embrechts et al. (1999) and Neves and
Alves (2008) revealed the results needed to forecast the probability on each situ-
ation in a teletraﬃc data set. Furthermore, the example studied by Einmahl and
Magnus (2008) examined the statistics of athletic performance using the theory
of estimation to specify suitable instruments for athletes and beyond. Moreover,
Andulai (2011) used the theory of risk estimation in the ﬁnancial institutions in
Europe which relied on the underlying theory analysis to predict the risk level in
European stock markets. Lastly, Garrido and Lezaud (2013) showed the overview
of probability and statistical tools on the theory to estimate and provide a real
parameter on the GEV index. Interestingly, this paper would apply the GPD
extreme analysis to improve the restriction of GEV calculation.
In order to reduce the loss deviation, the value at risk (VaR) adapted in this
paper is important to specify risk management. Manganell and Engle (2001)
employed the Historical Simulation VaR method to evaluate the risk level and
showed the overview. Furthermore, the exploration on the conditional Value-at-
risk (VaR) explained ﬁnancial risks based on the distribution of random sampling
data by Rockafellar and Uryasev (2002). In addition, Sampera et al. (2013) used
the GlueVaR, which is a new measurement of risk in ﬁnancial instruments and
insurance instruments, explaining the relationship in GlueVaR, VaR, and TVaR.
The paper showed the terms of GlueVaR as an overall risk measure of non-
ﬁnancial problems. Lastly, Perez and Murphy (2015) used the conditional VaR
to evaluate risks of ﬁnancial instruments as well comparing unﬁltered variables
and ﬁltered variables of the VaR that are considered alternatives in the upcoming
future (Fig. 2).

Forecasting of VaR in Extreme Event Under Economic Cycle Phenomena
727
4
The Framework of Research
Fig. 2. The conceptual framework of the research
5
Methodology
5.1
The Markov Switching Model (MS-Model)
Theoretically, the Markov Switching Model (MS-model) explains the institu-
tional changes on business and economic cycles (boom periods and recession
periods), which can be eﬀectively employed to investigate and clarify the move-
ment of price indexes in stock markets. In particular, this paper focused on the
yearly bank stock indicators during 2012–2017. As stated by Hamilton (2005),
Markov regime switching models are a type of speciﬁcation of which the selling

728
S. Wannapan et al.
point is the ﬂexibility in handling processes driven by heterogeneous states of
the world. Technically, this model is deﬁned as follows,
yt = cst + φyt−1 + εt.
(1)
As the detail in Eq. (1), the value Cst = 1 for t = 1, 2, 3, ..., n and Cst = 2
for t = t0 + 1, t0 + 2, ..., t0 + n follow the assumption of a normal distribution
with zero mean and variance u(0, σn
st) (Perlin 2010). In addition, the theory of
probability is applied for MS model, which is supposed that the boom period is
Cst = 1 and the recession period is Cst = 2. Thus, this can be written as
Pr(st = j|st−1 = i, st−2 = k, ...., yt−1, yt−2, ...) = Pr(st = j|st−1 = i) = pij.
(2)
Assuming that the econometrician observes yt directly but can only make an
inference about the value of st based on what we see happening with yt. This
inference will take the form of two probabilities
ξit = Pr(st = j|Ωt, θ),
(3)
for j = 1, 2, these two probabilities sum to 1 by construction. Here, Ωt =
[yt, yt−1, ..., y1, y0] denotes the set of observations obtained as of date t and θ,
which are a vector of population parameters. Thus, the above example would
be θ = (σ, φ, c1, c2, p11, p22) and it presumes to be known with certainty. The
inference is performed iteratively for t = 1, 2, ..., T with step t accepting as input
the values in Eq. (4),
ξit = Pr(st−1 = j|Ωt, θ).
(4)
The key of magnitudes is in order to perform iterations of the densities under
the two regimes. The details are expressed as Eq. (5),
ηjt = f(yt|st = j, Ωt−1; θ) =
1
√
2πσ exp[−(yt −cj −φyt−1)2
2σ2
],
(5)
where j = 1, 2. Speciﬁcally, we can calculate the conditional density of the
observation from Eq. (6),
f(yt|Ωt−1; θ) =
2

i=1
2

i=1
pijξi,t−1ηjt,
(6)
and the desired output is then
ξij =
2
i=1 pijξi,t−1ηjt
f(yt|Ωt−1; θ)
.
(7)
As a result of executing of iterations, we will have succeeded in evaluating
the sample conditional log likelihood of the observed data, which is described as
follows
logf(y1, y2, ..., yT |y0; θ) =
T

t=1
logf(yt|Ωt−1; θ).
(8)
For the speciﬁed value of θ, the estimation of the value θ can be then obtained
by maximizing by numerical optimization.

Forecasting of VaR in Extreme Event Under Economic Cycle Phenomena
729
5.2
The Generalized Pareto Distribution Model for Extreme Value
Estimating
The Generalize Pareto Distribution model (GPD) includes the extreme value
theory that explained about the case of extreme value information on the behav-
ior of the distribution high threshold exceedances. Studying on extreme cases,
the distribution function that a random quantity on F(x) beneath condition is
shown by Pickands (1975),
F(x|u) = P(X ≤u + X ≤u).
(9)
From Mierlus-Mazilu (2010), this paper examines the Univariate Generalized
Pareto Distribution, where X is a random variable, u is a designed threshold,
y = x −u are the overabundances and xF is the ﬁnal process of F. Then, Fu is
veriﬁed and written as,
Fu(Y ) = F(u + y) −F(u)
1 −F(u)
= F(X) −F(u)
1 −F(u)
.
(10)
The Eq. (10) is can approximated by a Generalized Pareto Distribution
(GPD), which is deﬁned as
G(x|ζ, σ, u) =

1 −(1 + ζ(x−u)
σ
)−1/ζ,
if ζ ̸= 0
1 −exp[−(x−u)
σ
],
if ζ = 0.
(11)
For the Eq. (11), where σ > 0 and ξ is the scale and shape parameter, respec-
tively. The information of parameter is heavy when ξ > 0. The Eq. (12) showed
and conducted the threshold of GPD model. Assuming in the object GDP that
under the threshold, u is generalized from a certain distribution with parameters,
η. The model is represented as H|(.|η). Consequently, the distribution function
F of any observation X able to described as
F(x|η, ζ, σ, u) =

H(x|n),
if x < u
H(u|η) + [1 −H(u|η)]G(x|ζ, σ, u),
if x ≥0.
(12)
Considering the Eqs. (13) and (14), there is a sample size n, x = (x1, ..., xn)
from F, parameter vector θ = (η, σ, ξ, u), A = (i : xi < u) and B = (i : xi ≥u)
the likelihood function is deﬁned as (Behrens et al. 2004),
L(θ; x) =

A
h(x|η)

B
h(1 −H(u|η))[ 1
σ (1 + ξ(xi −u)
σ
)−1+ξ
ξ ],
(13)
for ξ ̸= 0 and
L(θ; x) =

A
h(x|η)

B
h(1 −H(u|η))[( 1
σ )exp(xi −u
σ
)],
ξ = 0.
(14)
Obviously, the Eq. (14) represented the threshold, where u is the matter den-
sity having discontinuity which depends on the larger or smaller jump density.

730
S. Wannapan et al.
Fig. 3. Presentation the schematics of the threshold model (Applying from Behrens
et al. 2004)
In each event observation, it studies the analysis that is able to be more obvious
and less evident. However, it is more diﬃcult to predict the threshold. Subse-
quently, strong discontinuities or large jumps show the separation of information
that is easier for evaluating the parameter estimation. Hence, the density model
of threshold is schematically presented in Fig. 3.
5.3
The Value at Risk Model (VaR)
This theory is generated to estimate risk reduction, especially in this paper.
This risk management method is employed to investigate the risks of bank stocks
indexes of top banking companies in each of the four countries. At the beginning,
the reliability result of VaR has the procedure that consists of the comparison of
the VaR estimation with actual realized loss in the next period and the statistical
accuracy in the result of VaR. In the test, the ﬁrst process is called unconditional
coverage test proposed by Kupiec (1995). This conditional estimation speciﬁes
the frequency exceedances, which are in the estimated line level of VaR, and
the violation of independence property is employed to check the accuracy of the
VaR measurement. Conditionally, the violations are also randomly distributed,
and the likelihood ratio test is statistically used to evaluate as follows,
LRu = 2[log((N
T )N)(1 −N
T )T −N −log(pN(1 −p)T −N)].
(15)
Considering the Eq. (15), H0 : N
T = p and H1 : N
T ̸= p are the correct null and
alternatives, respectively. If LRu →x2 under H0 that implies good speciﬁcation.
Let p as the evaluated failure rate (p = 1−q where the conﬁdence level for VaR is
q). When T is the total number of such trials, N is the number of failures that is
able to be modeled with a binominal distribution with probability of occurrence
equals to α.
Furthermore, the second test is called conditional coverage test proposed
by Chirtofersen (1998). This conditional calculation explains the checking accu-
racy of the VaR results whether the conditional coverage test and independence

Forecasting of VaR in Extreme Event Under Economic Cycle Phenomena
731
property were used. The null hypothesis that indicated the failure process is
independent and the expected proportion of violations equals p. This can be
expressed as follows
LRcc = −2log[(1−p)T −NpN]+2log[(1−π01)n00]π01
01(1−π11)n10πn11
11 ] →x2. (16)
The number of observations with value i followed by j is nij for i, j = 0, 1
respectively, and the corresponding probabilities are πij =
nij

j nij . The value
i, j = 1 denote that a violation is made, while i, j = 0 indicates the opposite. To
estimate the value at risk for the peak over threshold in GPD, it is able to be
written as Generalized Pareto Distribution estimating the value at risk (Kisacik
2006), which is expressed as Eq. (17).
V aRGP D = u + (σ
ξ )[( n
Nu
α)]−ξ, ξ ̸= 0,
(17)
where
u : threshold
σ : scale parameter
ξ : shape parameter
α : 1-p
Nu : number of exceedances
n : sample size
ESq = E(X|X > V aRq) = V aRq + E(X −V aRq|X > V aRq),
(18)
from the Eq. (18), this is the expected shortfall (ES), which is described as the
expected loss size and it is able to be written as GPD distribution,

ESq =

V aR
1 −ξ + (ˆτ(h) −ξh)
1 −ξ
.
(19)
6
Empirical Results
6.1
Descriptive Data
Generally, the descriptively statistical information is represented in Table 1.
Each variable collected from ASEANs stocks such as BBL (Thailand), DBS
(Singapore), CIMB (Malaysia), and Mandiri (Indonesia) has been shown as a
basic average, median, and max-min calculation and checked for stationarity
condition to ensure the conversion to long-run equilibriums. Consequently, the
results indicate that the Singapore banking stock has the biggest volume of
banking stock index.

732
S. Wannapan et al.
Table 1. Descriptive data of the collected stock indexes
BBL
(Thailand)
DBS
(Singapore)
CIMB
(Malaysia)
Mandiri
(Indonesia)
Maximum 6.72
79.02
1.99
0.93
Minimum
4.13
74.66
0.91
0.49
Average
5.30
76.67
1.44
0.72
Medien
5.28
76.64
1.46
0.73
Noted: From computation
6.2
Expected Durations of Regimes
As seen in the details of Table 2, the Markov Switching Model found the number
of expected duration regimes, which are expansions (Bull markets) and depres-
sions (Bear markets). For the BBL index, the result contains 1,022 days for the
bull period and 281 times for recessions. Speaking to the DBS index, it seems the
bull time equals 1,079, and 113 days for regimes of recession periods. In the case
of CIMB index, it is approximately 1,011 times for bull periods and nearly 291
days for bear periods. Lastly, the boom period and recession period of Mandiri
index are 762 and 533 times, respectively.
Table 2. Presentation the expected duration of regimes of ﬁnancial stock indexes
BBL
(Thailand)
DBS
(Singapore)
CIMB
(Malaysia)
Mandiri
(Indonesia)
Bull (Daily time periods)
1,022
1,079
1,011
762
Bear (Daily time periods) 281
113
291
533
Noted: From computation
6.3
The Extremely Estimated Parameters of the GPD Model
In Table 3, to demonstrate the value of estimated parameters using the value
at risk of Generalized Pareto Distribution (V aRGP D) on each situation in the
economy (expansion and depression periods), three parameters are deﬁned as a
scale parameter (σ), shape parameter (ξ), and threshold (μ). At the beginning,
BBL index includes 1.0233 for the scale estimator (σ), 0.1882 for the shape
(ξ), and 0.02 (μ) is the threshold in expansion periods. On the other hand, the
scale, shape, and threshold parameters are estimated and respectively expressed
as 1.0767 (σ), 0.2275 (ξ), 0.03 (μ) in depression periods. Considering the DBS
index, in boom periods, there are 1.0498 (σ), 0.1496 (ξ), and 0.025 (μ) for the
GPD estimators, respectively. On the other side, for decreasing periods, there
are 1.0332 (σ), 0.3226 (ξ), 0.01 (μ), respectively. In the matter of the CIMB
index, the results of estimated parameters in bull periods are 0.9613 (σ), 0.2016

Forecasting of VaR in Extreme Event Under Economic Cycle Phenomena
733
Table 3. Presentation of GPD estimators for ﬁnancial stock indexes
BBL (Thailand)
DBS (Singapore)
CIMB (Malaysia)
Mandiri (Indonesia)
Bull
Bear
Bull
Bear
Bull
Bear
Bull
Bear
Scale parameter
1.0233
1.0767
1.0498
1.0332
0.9613
0.9699
1.0131
1.0169
Shape parameter
0.1882
0.2275
0.1496
0.3226
0.2016
0.1662
0.2302
0.2802
Threshold
0.02
0.03
0.0025
0.01
0.01
0.025
0.02
0.05
Noted: From computation
(ξ), 0.01 (μ). Conversely, the bear periods are 0.9699 (σ), 0.1662 (ξ), 0.025 (μ),
respectively. Lastly, the Mandiri index contains 1.0131 for the scale parameter
(σ), 0.2302 for the shape estimator (ξ), and 0.05 for the threshold (μ). On the
other hand, 1.0169 (σ), 0.2802 (ξ), and 0.05 (μ) are the GPD parameters in
recession times.
6.4
Results of the Value at Risk (VaR) and Expected Shortfall (ES)
Estimations
Empirically, Table 4 reveals two values: risk value of least loss and the percentage
return in the short-run (Expected shortfall: ES). The data were represented as
two periods of the economic system (expansion period and recession period). As
estimated results of risk values, the BBL index in expansion periods contains
1.57% for risk measurement and 2.68% for the risk in the bear period. The risk
values of DBS are 1.92% in the boom periods and 2.88% in recession times. In
addition, the results of the CIMB index approximately provide 2.30% and 3.09%
in boom periods and recession periods, respectively. Lastly, the Mandiri index
contains 1.40% in boom periods and 1.61% in bear periods.
Additionally, Table 4 showed the percentage of expected shortfalls, which are
estimated by using normally situational data. Firstly, the BBL index includes
2% and 1% in the bull and bear markets, respectively. For the DBS index, the
result indicates 0.26% and 1%, which are in the boom and depression periods,
respectively. For CIMB index, the most minimum return of these indexes is 1.00%
and 2.00%, respectively. Lastly, the Mandiri index has the values of expected
shortfalls, which are approximately 2.00 perncet in boom periods and 5.00% in
recession periods.
The Table 4 also displayed the comparison between risk measurements (VaR)
and expected shortfalls. The empirical outcomes can be implied that there are
two groups of the ﬁnancial stock indexes, including a positively ﬁnancial yield
(BBL and Mandiri) and negatively ﬁnancial (DBS and IMB) group. The results
show BBL and Mandiri contain the optimistic trend for investing in both bull
and bear periods, which indicates the expected shortfalls are more than risk
measurements. Conversely, the results of DBS and CIMB provide negative signs,
which are not suitable for speculating in both bull and bear periods.

734
S. Wannapan et al.
Table 4. Presentation the results of VaR and ES
BBL
(Thailand)
DBS
(Singapore)
CIMB
(Malaysia)
Mandiri
(Indonesia)
Bull period
VaR value
1.57%
1.92%
2.30%
1.40%
Expected shortfall 2.00%
0.26%
1.00%
2.00%
Bull period
VaR value
2.68%
2.88%
3.09%
1.61%
Expected shortfall 3.00%
1.00%
2.00%
5.00%
Noted: From computation
7
Conclusion
The purpose of this paper was to computationally investigate the volume of risks
in four major ﬁnancial stock indexes of AEC countries such as BBL (Bangkok
Bank, Thailand), DBS (The Development Bank of Singapore, Singapore), CIMB
(Commerce International Merchant Bankers Bhd, Malaysia) and Mandiri (Bank
Mandiri, Indonesia). The daily time-series data was observed during the period
of 2012 to 2017. Methodologically, this paper focused on forecasting economet-
ric methods. For example, the Markov Switching Model (MS) for economical
regime speciﬁcation, Generalized Pareto Distribution (GPD) for the extreme
value theory, and the calculation of values at risks (VaR) were all employed.
Empirically, from the investigation of economic cycles, the results estimated
from the switching model representing four ﬁnancial stock indexes were compu-
tationally separated into two states, including the boom and recession periods
(see details in Table 2). These separated regimes can clarify the details of eco-
nomic cycles eﬃciently. Moreover, this application of the switching model can
successfully conﬁrm that the ﬂuctuations of ASEAN banking stock indexes obvi-
ously occur and consist of the business cycle theory.
Based on the eventual analysis relating to the separated regimes estimated by
the MS-model, the results of the GPD models displayed that there are evidently
unusual events among the trends of ASEAN banking stocks (see the details
in Table 3). The computational results calculated from the GPD model were
then employed to clarify risk measurement by using Value at Risk forecasting
(VaR). Demonstrably, the results conﬁrm that two indexes of ASEAN banking
stock indexes are not suitable for short-run investing. For example, in the case of
DBS and CIMB indexes, the computational results indicated that these ﬁnancial
stock indexes contain the higher chance to loose rather than gaining proﬁts. This
can be implied that the indexes in Singapore and Malaysia are not suitable for
speculation, which the risk measurement estimated by the Value at Risk (VaR)
(Seen the details in Table 4) showed that the chance to get lost is higher than the
chance to get earning proﬁts. To address this issue, it obviously seems that a long-
run strategy to achieve positive outcomes from these banking stocks should be

Forecasting of VaR in Extreme Event Under Economic Cycle Phenomena
735
intensively focused. On the other hand, in the case of BBL and Mandiri indexes,
the results showed these indexes can be ﬁnancially speculated. To deal with this
issue, an appropriate time to invest and accurately computational prediction are
intensively required.
Ultimately, it is undeniable that investments in banking systems are not
always suitable for short-term speculations. The empirical results from this inves-
tigation also conﬁrm that the stability of banking systems is inevitably related
to the banking stock indexes. Long-run investment in banking stocks is crucial,
and this should be carefully considered by authorities. As a result, this study can
strongly conclude the uniqueness of the banking system in Asean countries really
needs somethings to clarify a suitable prediction, for instance, econometrical and
computational tools.
References
Avdulai, K.: The Extreme Value Theory as a Tool to Measure Market Risk. Working
paper 26/2011.IES FSV. Charles University (2011). http://ies.fsv.cuni.cz
Behrens, C.N., Lopes, H.F., Gamerman, D.: Bayesian analysis of extreme events with
threshold estimation. Stat. Modell. 4, 227–244 (2004)
Chaithep, K., Sriboonchitta, S., Chaiboonsri, C., Pastpipatkul, P.: Value at risk analy-
sis of gold price return using extreme value theory. EEQEL 1(4), 151–168 (2012)
Christoﬀersen, P.: Evaluating interval forecasts. Int. Econ. Rev. 39, 841–862 (1998)
Einmahl, J.H.J., Magnus, J.R.: Records in athletics through extreme-value theory. J.
Am. Stat. Assoc. 103, 1382–1391 (2008)
Embrechts, T., Resnick, S.T., Samorodnitsky, G.: Extreme value theory as a risk man-
agement tool. North Am. Actuarial J. 3(2), 30–41 (1999)
Ernst, E., Stockhammer, E.: Macroeconomic Regimes: Business Cycle Theories Recon-
sidered. Working paper No. 99. Center for Empirical Macroeconomics, Department
of Economics, University of Bielefeld (2003). http://www.wiwi.uni-bielefeld.de
Garrido, M.C., Lezaud, P.: Extreme value analysis: an introduction. J. de la
Socit Franaise de Statistique, 66–97 (2013). https://hal-enac.archivesouvertes.fr/
hal-00917995
Hamilton, J.D.: Regime Switching Models. Palgrave Dictionary of Economics (2005)
Jang, J.B.: An extreme value theory approach for analyzing the extreme risk of the
gold prices. J. Financ. Rev. 6, 97–109 (2007)
King, R.G., Rebelo, S.T.: Resuscitating Real Business Cycles. Working paper 7534,
National Bureau of Economic Research (2000). http://www.nber.org/papers/w7534
Kisacik, A.: High volatility, heavy tails and extreme values in value at risk estimation.
Institute of Applied Mathematics Financial Mathematics/Life Insurance Option Pro-
gram Middle East Technical University, Term Project (2006)
Kupiec, P.: Techniques for verifying the accuracy of risk management models. J.
Derivat. 3, 73–84 (1995)
Manganelli, S., Engle, F.R.: Value at Risk Model in Finance. Working paper No. 75.
European Central Bank (2001)
Marimoutou, V., Raggad, B., Trabelsi, A.: Extreme Value Theory and Value
at Risk: Application to oil Market (2006). https://halshs.archives-ouvertes.fr/
halshs-00410746
Mierlus-Mazilu, I.: On generalized Pareto distribution. Roman. J. Econ. Forecast. 1,
107–117 (2010)

736
S. Wannapan et al.
Mwamba, J.W.M., Hammoudeh, S., Gupta, R.: Financial Tail Risks and the Shapes of
the Extreme Value Distribution: A Comparison between Conventional and Sharia-
Compliant Stock Indexes. Working paper No. 80. Department of Economics Working
Paper Series, University of Pretoria (2014)
Neves, C., Alves, M.I.F.: Testing extreme value conditions an overview and recent
approaches. Stati. J., REVSTAT 6(1), 83–100 (2008)
Perez, P.G., Murphy, D.: Filtered Historical Simulation Value-at-Risk Models and
Their Competitors. Working paper No. 525. Bank of England (2015). http://www.
bankofengland.co.uk/research/Pages/workingpapers/default.aspx
Perlin, M.: MS Regress - The MATLAB Package for Markov Regime Switching Models
(2010). Available at SSRN: http://ssrn.com/abstract=1714016
Pickands, J.: Statistical inference using extreme order statistics. Ann. Stat. 3, 110–131
(1975)
Rockafellar, R.T., Uryasev, S.: Conditional value-at-risk for general loss distributions.
J. Bank. Financ. 26, 1443–1471 (2002). http://www.elsevier.com/locate/econbase
Sampara, J.B., Guillen, M., Santolino, M.: Beyond value-at-risk: glue VaR distor-
tion risk measures. Working paper No. 2. Research Institute of Applied Economics,
Department of Econometrics, Riskcenter - IREA University of Barcelona (2013)
Taghipour, A.: Banks, stock market and economic growth: the case of Iran. J. Iran.
Econ. Rev. 14(23) (2009)

Interval Forecasting on Big Data Context
Berlin Wu(&)
Department of Mathematical Sciences, National ChengChi University,
Taipei, Taiwan
berlin@nccu.edu.tw
Abstract. Purpose: The object of this research is to construct an optimal
internal forecasting method in big data context.
Design/methodology/approach: An intelligent model construction, includ-
ing consumer behavior and market information, structural changes detection,
nonlinear pattern recognition, spatial causality, semantic processing mode is
presented.
Findings: The major drawback in forecasting ﬁeld is that the statistical
forecasting result is derived from historical data but it often encounters
non-realistic problem when people predict future trends or market changes in
real world.
Practical Applications: Construction of Big Data platform will be a new
technique provides to solve the structured change and uncertain problems.
According to the artiﬁcial intelligence evolution and on line improvement to the
market conditions, it will do a better performance to prevailing future event.
Originality: We efﬁciently integrate the idea of structure change, entropy and
market behavior in the forecasting process.
Conclusion: Since historical time series analysis has difﬁcult to prove the
relationship/causality with future events. Especially in the case of a structural
change, the future is full of high uncertainty, ambiguity and unexpected.
Keywords: Structure change  Interval forecasting methods  Big data
Fuzzy entropy
1
Introduction
Despite current time series analysis have reached an increasingly sophisticated level,
researchers on this ﬁeld still feel dilemma in the forecasting performance. Economic
forecasting mostly comes after economic model construction, while model construction
is based on historic data without information from the future events. The major
drawback in econometric forecasting is that econometric model based on the result of
historical trends, it encounters difﬁcult phenomenon in practice [13, 16]. With these
well-constructed models people still cannot predict future changes or market trends.
Since historical time series analysis has difﬁcult to prove the relationship/causality with
It is not the strongest nor the most intelligent who will survive, but those who can best manage
change.
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_53

future events. Especially with the present of a structural change, the future is full of
high uncertainty and ambiguity [1–3].
In this paper we introduce an integrated process to construct and forecasting a time
series model with structure changed characteristic on the big data context. The tech-
nique of changing period detection is used for an effective procedures instead of unit
root test in the model construction.
According to its artiﬁcial intelligence evolution and on line improvement to the
market conditions, it will do a quick response to prevailing future event. A Big Data
platform construction will be a new technique provides to solve the structured change,
market trend and uncertain problem.
Time series analysis and classical analysis biggest difference is that in order to
detect the structural transformation of substituted stable type with a single test. In the
model construction process, it is often assumed that the data type is stationary or by a
single test [8, 9]. But structural changes in the market economy, steady type of
hypothesis is difﬁcult to set up. Especially in the number of multiple columns situa-
tions, consistent smooth lines and structural change is almost impossible. Therefore, we
need to more carefully consider the leadership behind the relationship between them.
We consider the data base in a large background of macroeconomic factors, A Fore-
casting model selection will be proceeded by the following analysis process:
1. System variables and related domain are examined
2. Apply the transition interval test, structural changes in the latest issue of the model
section,
3. Large model within the entire kinetic energy and the kinetic energy of the external
integration of consumer behavior and market trends and forecast
4. The forecasting result will take the big data context for forecasting with the new
system parameters of consumer behavior and market trends
Finally, an empirical study will be demonstrated in an efﬁcient way on the big data
context with the applied concept of fuzzy entropy property, the market behavior and
forecasting trend.
2
Dynamic Trend with Fuzzy Entropy
2.1
Trend with Fuzzy Entropy
The concept of structural change in a time series should be based on the dynamic trend
of variable. It should be a gradually-emerging change interval and not a change point
whereby the change happens abruptly at a certain point of time.
When we use set theory to examine whether there is any change point in a time
series, ﬁrst we cluster the time series, ﬁnd out the cluster center, and then use the fuzzy
membership degree, fuzzy entropy and other relevant concepts to perform classiﬁca-
tion. The deﬁnition of fuzzy membership degree is as follows:
Deﬁnition 2.1 Fuzzy Membership Degree. Let a time series be
xt; t ¼ 1; 2; . . .; N
f
g,
with C1 and C2 being two cluster centers of the time series, lit; i ¼ 1; 2: to represent
738
B. Wu

the membership degree of an element xt in the time series Xt to C1 and C2, the
membership degree is thus deﬁned as: lit ¼ 1 
xtCi
j
j
P2
i¼1 jxtCij :
Entropy is a concept in the Thermodynamics study [10, 11, 15], whose original
meaning is the degree at which work can be transformed. The Statistical Physics
provides another deﬁnition to it: the measure to describe the random motion. In
addition, the Probability Theory and Information Theory give it a more common
deﬁnition: measure of the unboundedness of a random variable, or the measure of the
amount of missing information. So fuzzy entropy is used to measure the uncertainty of
fuzzy sets, and is an important tool for the processing of fuzzy data, while the mem-
bership degree is used to characterize elements that do not clearly belong to some
particular sets.
So the change interval that studies variables, compared with the classical method of
investigation for time series, has better descriptive power. [12] suggested that the use of
fuzzy entropy is effective in identifying whether a structural change happens in a time
series. Besides, it can also be used together with the mean cumulated fuzzy entropy of t
times, to observe the change in message of fuzzy entropy, based upon which a standard
for the classiﬁcation of change model can be established [4–7].
Deﬁnition 2.2 Fuzzy Entropy. Let a time series be
xt; t ¼ 1; 2; . . .; N
f
g,with lit being
the membership degree of xt to the cluster centers Ci ( i = 1,2,…,k), the fuzzy entropy is
thus deﬁned as:
d xt
ð Þ ¼  1
k
  X
k
i¼1
litln lit
ð
Þ þ 1  lit
ð
Þln 1  lit
ð
Þ
½

Deﬁnition 2.1. Mean Cumulated Fuzzy Entropy for one-variable time series. Let
xt; t ¼ 1; 2;    ; N
f
g be a time series with fuzzy entropy dðxtÞ. The mean cumulated
fuzzy entropy is thus deﬁned as:
MSdðxtÞ ¼ 1
t
X
t
i¼1
dðxiÞ
ð2:1Þ
There is usually a threshold level k set up for fuzzy classiﬁcation, because no matter it
is for nature or humanities, the determination for classiﬁcation is very subjective and
often non-unanimous. Hence, an objective measure is needed. According to empirical
experience, k cannot take a value too huge or too small, otherwise the classiﬁcation
cannot be done or too many classes will be created. So a value for k between 0.001 and
0.1 will be ideal.
Here we wish to ﬁnd two cluster centers. This is determined based on common
experience of empirical analysis and trend of time series. The procedures are as
follows:
Step 1:
Use the k-means method (Sharma, 1996) to ﬁnd out two cluster centers C1 and
C2 in time series
xt
f g, and determine the membership degree lit; i ¼ 1; 2 of
xt
f g to the two cluster centers.
Interval Forecasting on Big Data Context
739

Step 2:
Compute the fuzzy entropy dðxtÞ, mean cumulated fuzzy entropy MSdðxtÞ ¼
1
t
P
t
i¼1
dðxiÞ and the median of this series Median MSdðxtÞ
ð
Þ that correspond to
xt.
Step 3:
Take a suitable threshold value k, classify the mean cumulated fuzzy entropy
MSdðxtÞ series that correspond to xt. If the mean cumulated fuzzy entropy
MSdðxtÞ falls into the interval 0; Median MSdðxtÞ
ð
Þ  k
½
Þ, we will use 1 to
represent
Group
1;
if
MSdðxtÞ
falls
into
the
interval
Median MSdðxtÞ
ð
Þ  k; Median MSdðxtÞ
ð
Þ þ k
½
Þ, we use 2 to represent Group
2; and if MSdðxtÞ falls into the interval Median MSdðxtÞ
ð
Þ þ k; 1
½
, 3 will be
used to represent Group 3.
Step 4:
If the result of classiﬁcation is inconsistent, we then make an adjustment to the
result. If it is consistent, go to Step 5.
Step 5:
Select an appropriate determination level. If the number of consecutive
samples is greater than [], then these consecutive samples belong to the same
group. During classiﬁcation, if more than one group is found, we know that
structural change happens in this time series. Thereafter, ﬁnd the change
interval.
2.2
Data Mining for Variable Interaction
Under the big data context, the impact of macroeconomic variables is very much
complex. Each factor leads a pattern with a nonlinear trend and have certain causality
property with other variables. Therefore, we must ﬁrst clarify the main economic
variables in considerations. Then by real-time intelligence, market consumption,
investment, and trade data for economic growth to make better predictions. Since the of
macroeconomic data is highly correlated with leading or lag causation, the activities of
a single economic forecast market performance conditions is not enough and t is the
inevitable choice of the statistical agency.
“Big” big data does not lie in how much amount of data, but rather a kind of
“information as to the large and pluralism” methodology. The mobile Internet, the next
generation Internet, cloud computing, networking, social networks, there are many
large and small departments and units built up ﬁnancial systems, human resources
systems and client systems, their mutual combination brings new wave of expansion
and development of its business intelligence will greatly enrich statistics pipeline
sources, affecting the quality of statistics sources. People pick through the data for the
past, reveal the law to face the future, predict trends, will greatly improve the ability of
the government’s macro decision-making judgments and standards.
Based on the new economic statistics of data mining and processing, we can see
that the current macroeconomic variables analysis facing a major information inno-
vation. In order to promote the quality of forecasting, the macroeconomic factors need
to be reviewed before data processing to improve the macroeconomic accounts.
In order to reform the data process of macroeconomic accounting method, “big data”
concept requires us to extract information from the mass of “gold mine” from which to
740
B. Wu

create a new accounting value. Their reformation breaks the traditional thinking using
new statistical techniques, and actively improve the various types of cargo bulk products,
commodity production, sale, transport statistical monitoring system platform. Using
various types of commodity trading platform, the Internet of Things, mobile Internet,
cloud computing, enterprise ﬁnancial systems and other data information. By extraction,
transformation, integration of information, establishment of GDP accounting data
warehouse. Variable economic census database, the basic unit of directory database,
database agricultural census, census basic information database, the tertiary industry
census database, industrial census database, input-output survey data and other dynamic
information. Accounting information reduce uncertainty and difﬁcult to capture judge.
Table 1 demonstrates the main macroeconomic mining projects and interaction.
In GDP accounting, for example, economists do GDP accounting process often feel
confused or troubled. It is because that the data items and weights are not so precise and
consistent, information collected is incomplete, delayed, inadequate representation and
other issues. Message bias will result in variable effectiveness analysis, signiﬁcant
deviations modeling and conclusions. Such as armaments security value, the value of
real estate was often played down or taken into account, the value of tourism, enter-
tainment, education and cultural, medicine and underground economic accounting, still
cannot be fully calculated in GDP accounting process. Therefore, how to optimize the
GDP accounting items of information sources and the weight in order to improve data
quality is very important. Under the new “National Economic Industry Classiﬁcation
Benchmark, China, 2011 Edition” improving the GDP accounting industry classiﬁ-
cation, improve the relevance and usefulness of the industry reﬂects the problem;
management, assessment and GDP-related performance and coordination of relevant
variables. Therefore, in the background of big data, whether accounting, modeling,
forecasting, and should have a new basis for supporting measures.
2.3
Residuals Evaluation with Interval Data
An interval-valued fuzzy set can be viewed as a continuous fuzzy set, which further
represents uncertain matters. Take “test results” as an example. In foreign countries, A,
B, C and D are used to evaluate a student’s result, whereby A represents 100–80 marks,
B represents 79–70 marks, C represents 69–60 marks, D represents 59–50 marks, in
place of numerical scores. In the past we think that obtaining a high score means
learning well. However, does a student who gets 85 marks have better learning ability
than another who scores 80 marks? Not necessarily so. That’s why the interval-valued
fuzzy set resolves the phenomena of uncertainty.
When a sample of interval-valued data is available, we have to consider its opera-
tions However, there is still no complete deﬁnition for the measure of interval distance
(see Wu and Nguyen [13]). How to deﬁne a well-deﬁned interval distance? First we
represent the interval with ðci; riÞ with c being the center, r being radius. This way, the
interval distance can be considered as the difference of the center plus the difference of
the radius. The difference of the center can be seen as the difference in location, and the
difference of the radius can be seen as the difference in scale. However, in order to lower
the impact of the scale difference on the location difference, we take the ln value of the
scale difference, and then plus 1 to avoid the ln natural log value becoming negative.
Interval Forecasting on Big Data Context
741

Table 1. The main macroeconomic variable and its interaction
GDP
Exchange
rate
Interest
Energy
Stock
market
Inﬂation
Unemployment
rate
Real
estate
Population
Tempreture
CPI
1
1
−1
−1
1
2
1
2
0
1
Raw material
1
−1
1
1
0
1
−2
0
1
0
Labor
2
−1
1
1
0
1
1
0
1
0
R&D
2
0
1
1
0
0
−1
0
0
0
Investment
2
1
0
1
1
1
1
1
1
1
Import/export
2
2
−2
−1
2
1
1
−1
0
1
Medicine/health
1
1
1
1
0
0
1
0
1
1
Tourism,
entertainment
1
1
1
1
1
0
1
−1
1
1
Cultural &
education
1
1
1
0
0
0
1
0
1
1
National
defence
−1
−2
1
1
−1
0
1
1
1
0
742
B. Wu

Once
such
a
transformation
has
been
selected,
instead
of
the
original
trapezoid-data, we have a new value y = f(x). In the ideal situation, this new quantity y
is normally distributed. (In practice, a normal distribution for y may be a good ﬁrst
approximation.) When selecting the transformation, we must take into account that, due
to the possibility of a rescaling, the numerical values of the quantity x is not uniquely
determined.
Deﬁnition 2.3 Scaling for an interval fuzzy number on R. Let A ¼ ½a; b be an
interval fuzzy number on U with its center cA = (b −a)/2. Then the defuzziﬁcation
number RA of A ¼ ½a; b is deﬁned as
RA ¼ cA þ
A
k k
2 lnðe þ A
k kÞ where;
A
k k is the length of the interval:
However, there are few literatures and deﬁnitions appear on the measurement system.
In this section, a well-deﬁned distance for interval data will be presented.
Example 2.1. Let x1 = [2,3], x2 = [1.5,3.5], x3 = [1,4], x4 = [2,3.5], Then,
Rx1 ¼ 2:5 þ
1
2  lnðe þ 1Þ ¼ 2:89; Rx2 ¼ 2:5 þ
2
2  lnðe þ 2Þ ¼ 3:14
Rx3 ¼ 2:5 þ
3
2  lnðe þ 3Þ ¼ 3:36; Rx4 ¼ 2:75 þ
1:5
2  lnðe þ 1:5Þ ¼ 3:27
Deﬁnition 2.4. Let Ai ¼ ½ai; bi (i = 1, 2, n) is a sequence of interval fuzzy number on
U with its center cA = (b-a)/2. Then the distance between Ai and Aj is deﬁned as
dðAi; AjÞ ¼ cAi  cAj

 þ
Ai
k
k
2 lnðe þ Ai
k
kÞ 
Aj


2 lnðe þ Ai
k
kÞ


The traditional ﬁve-point scale used in scoring is often ﬁve options. The sequence score
from small to large is often 1, 2, 3, 4, 5 points then to calculate the sum. According to
this principle, the scoring method can also apply in discrete type of fuzzy numbers. It
can let the distance of option value become larger, and it might help achieve a sig-
niﬁcant level after testing.
Deﬁnition 2.5. Interval means square error (IMSE) of prediction for a sample of
interval-valued fuzziness.Let si ¼ ai; bi
½
; i ¼ 1; . . .; N
f
g be an interval time series, with
prediction interval being ^si ¼ ^ai; ^bi


and ei ¼ d si;^si
ð
Þ beging the error between the
prediction interval and the actual interval, thus:
IMSE ¼ 1
l
X
N þ l
i¼N þ 1
e2
i
where l is the forecasted expectancy value.
Interval Forecasting on Big Data Context
743

Example 2.2. Let ^A =
3; 6
½
; 2; 6
½
; 5; 8
½
; 3; 8
½

f
g be an interval time series of fore-
casting data with respect to the real time series. A ¼
3; 4
½
; 3; 5
½
; 5; 7
½
; 4; 5
½

f
g. Table 2
illustrates the distance from forecasting values and real values.
Example 2.3. The distribution of 27 sets of interval data of unemployment rate (unit:
percent) is shown as Fig. 1, as follows:
If we wish to divide the data into two groups, using Deﬁnition 2.5, we can obtain
two interval clusters I1 ¼ 1:83; 2:5
ð
Þ and I2 ¼ 3:71; 5:2
ð
Þ The result of clustering is as
follows (Fig. 2):
Table 2. Distance for the interval data
Sample
Forecasting
value
Actual
value
Distance
dðAi; AjÞ ¼ cxi  cxj

 þ
Ai
k
k
2 lnðe þ Ai
k
kÞ 
Aj
k k
2 lnðe þ Ai
k
kÞ


[3,6]
[3,4]
d s1;^s1
ð
Þ ¼ 3:5  4:5
j
j þ 0:86  0:38
j
j: ¼ 1:48
3
[2,6]
[3,5]
d s3;^s3
ð
Þ ¼ 4  4
j
j þ 1:05  0:64
j
j ¼ 0:41
4
[5,8]
[5,7]
d s4;^s4
ð
Þ ¼ 6  6:5
j
j þ 0:86  0:64
j
j ¼ 0:72
5
[3,8]
[4,5]
d s5;^s5
ð
Þ ¼ 4:5  5:5
j
j þ 0:86  0:38j ¼ 1:48
IMSE ¼ 1
4  1:482 þ 0:412 þ 0:722 þ 1:482
ð
Þ ¼ 1:15
0
1
2
3
4
5
6
1
3
5
7
9 11 13 15 17 19 21 23 25 27
Fig. 1. Chart of interval time series
0
2
4
6
1
3
5
7
9 11 13 15 17 19 21 23 25 27
Fig. 2. Result of interval clustering
744
B. Wu

3
An Integrated Decision System with Forecasting
3.1
Model Construction and Forecasting
In the traditional research, people observe the microeconomic index from the ﬁnancial
point of view, try to ﬁnd the internal market and marcoeconomic relevance. However,
we ﬁnd that it is not sufﬁcient to make projections based on internal economic statistics
analysis. The performance of individual economic statistics is in global reﬂecting
internal (local) individual economic trends and is closely related to the individual
economic and economic situation of the global environment. Therefore, we think that
in the overall economic prosperity (overall economic indicators) and the internal
operation of the company (stocks earnings data), in order to more accurate forecasting
economic trends.
The traditional research for the econometric trend is mostly focus on the GDP
investigation. Hence the association variables computation are considered highly on
these ﬁnancial indicators. But we think that only use these variables is not sufﬁcient,
since the demonstration on the microeconomics data is not only inﬂuenced by its local
economic condition. It is inﬂuebced by the big environment, the global economic
condition. Therefore we take these two function into considerations. It will make our
foresting result more realistic and robustic.
Let yt be a time series in a dynamic system modeled by with
yt þ 1 ¼a1 þ /1yt þ signðX1tÞf big data X1t
ð
Þ þ signðX2tÞg big data X2t
ð
Þ
þ sign Dyi
ð
Þ
Xt
i¼0
yi
yi þ 1
e Dyi
j
j þ et þ 1
Be an integrated transferred time series, f is the function with a set of corresponding
exogenous variables X1t ¼ o1; o2; . . .; o50
f
g, D yt is the variation of entropy yt is the
national miroeconomic variables, X2t ¼ w1; w2; . . .; w50
f
g a set of corresponding
indogenous variables, Pt
i¼0 Dyie Dyi
j
j is the cumulative dynamic entropy.
For the interval data type, we extend last equation into the following
xt þ 1 ¼ a1 þ /1xt þ signðXtÞf1 X1t
ð
Þ þ sign Dxi
ð
Þg1 X2t
ð
Þ þ sign Dxi
ð
Þ P
t
i¼0
xi
xi þ 1 e Dxi
j
j þ et þ 1
lt þ 1 ¼ a2 þ /2lt þ signðXtÞf2 Xt
ð
Þ þ sign Dli
ð
Þg2 X2t
ð
Þ þ sign Dli
ð
Þ P
t
i¼0
li
li þ 1 e Dli
j
j þ dt þ 1
8
>
>
>
<
>
>
>
:
Where xt ¼ center of yt, lt ¼ length of yt, et þ 1  WNð0; r2
eÞ, dt þ 1  WN 0; r2
d

	
.
3.2
Forecasting with Financial Entropy
The concept of entropy originates from machine engineering, where in a process
involving heat it is a measure of the portion of heat becoming unavailable for doing
work. In information theory, Entropy is a measure of unpredictability or information
content in a random variable. Since entropy can be measured in price values or range of
Interval Forecasting on Big Data Context
745

prices, in this context, the term will be referred to the ratio entropy, which ratio
quantiﬁes of the time series. The role of entropy rate for a dynamic process is one bit
per value. For exmple, if the process demonstrates upward trend, and hence the entropy
rate, is lower. This is because, if asked to predict the next outcome, we could choose
the most frequent result and be right more often than wrong. The entropy of a message
multiplied by the length of that message is a measure of how much information the
message contains. If some messages come out smaller, at least one must come out
larger. In practical use, this is generally not a problem, because we are usually only
interested in compressing certain types of messages, for example English documents as
opposed to gibberish text, or digital photographs rather than noise, and it is unimportant
if our compression algorithm makes certain kinds of sequences larger. However the
problem can still arise even in every day use when applying a compression algorithm to
an already compressed data.
Wu [12] suggested that, the use of fuzzy entropy is effective in identifying whether
a structural change happens in a time series. Besides, it can also be used together with
the mean cumulated fuzzy entropy of t times, to observe the change in message of fuzzy
entropy, based upon which a standard for the classiﬁcation of change model can be
established.
xt þ 1 ¼ a1 þ /xt þ et þ 1
et þ 1  WN 0; r2
e

	
lt þ 1 ¼ a1 þ lt þ dt þ 1
dt þ 1  WN 0; r2
d

	

ð3:1Þ
Let ^xt ið Þ and ^lt ið Þ be the one step forecasting value of the interval time series as in
Eq. (2.1), xf ið Þ and lf ið Þ We denote the revised forecasting value as
xf 1
ð Þ ¼ ^xt 1
ð Þ þ sign lt  lt1
ð
Þ lt
lt1
lf 1
ð Þ ¼ ^lt 1
ð Þ þ sign xt  xt1
ð
Þ xt
xt1
(
ð3:2Þ
xf ið Þ ¼ ^xt ið Þ þ sign ^lt ið Þ ^lt i  1
ð
Þ

	 ^lt ið Þ
^lt i1
ð
Þ
lf ið Þ ¼ ^lt ið Þ þ sign ^xt ið Þ  ^xt i  1
ð
Þ
ð
Þ
^xt ið Þ
^xt i1
ð
Þ
8
<
:
ð3:3Þ
3.3
An Integrated Decision System for Fuzzy Time Series Analysis
and Forecasting
Here we wish to ﬁnd two cluster centers. This is determined based on common
experience of empirical analysis and trend of time series. The procedures are as
follows:
Step 1: Use the k-means method to ﬁnd out two cluster centers C1 and C2 in time
series xt
f g, and determine the membership degree lit; i = 1,2 of
xt
f g to the
two cluster centers.
746
B. Wu

Step 2: Compute
the
fuzzy
entropy
d xt
ð Þ,
mean
cumulated
fuzzy
entropy
MSd xt
ð Þ ¼ 1
t
Pt
i¼1 d xi
ð Þ, and Median
MSd xt
ð Þ
ð
Þ of this series, that corre-
spond to xt.
Step 3: Take a suitable threshold value k, classify the mean cumulated fuzzy entropy
MSd xt
ð Þ series that correspond with xt.
If the mean cumulated fuzzy entropy MSd xt
ð Þ falls into the interval [0,
Median ðMSd xt
ð Þ) −k), we assign 1 to represent Group 1;
if MSd xt
ð Þ falls into the interval [Median (MSd xt
ð Þ) −k, Median
(MSd xt
ð Þ) + k), we assign 2 to represent Group 2;
if MSd xt
ð Þ falls into the interval [Median (MSd xt
ð Þ) + k,1], we assigne 3
to represent Group 3.
Step 4: If it is consistent, go to Step 5. Otherwise adjust the level k and go to step 3.
Step 5: Step 5: Select an appropriate determination level a. If the number of con-
secutive samples is greater than [ aN], then these consecutive samples
belong to the same group. During classiﬁcation, if more than one group is
found, we know that structural change happens in this time series. There-
after, ﬁnd the change interval.
Step 7: Construct a system of AR(1) model for the center and length of the dynamic
process.
Step 8: Forecasting the time series as in the Eq. (3.2).
4
Empirical Studies
We consider the daily gold price time series during January 2006 to October 2015. Data
source is from New York Mercantile Exchange. The weekly interval time series (with
minimum and maximum) are derived from these 2418 daily data into 500 interval data
with the form [center, radius] by the maximum price and the minimum price weekly.
The trend of time series are illustrated at Fig. 3(a). Figure 3(b) shows the trend with
groups. We use the threshold fuzzy classiﬁcation method (TFC method) to perform
classiﬁcation and construct a more comprehensive model. The statistical softwares, R
and Minitab, are perform to do the computational work.
Firstly, we establish some models including general time series model, fuzzy time
series model, and our threshold fuzzy classiﬁcation model, by means of 487 weekly
data from January 2006 to July 2015. We retain 13 weekly data from August 2015 to
October 2015 to make forecasting performance comparison.
From the computation, classiﬁcation is successful and structural change happens.
So we need to consider the radius of gold prices from weeks 145, the ﬁrst week of Jan.
2009. Also, we can see weeks 289 to weeks 467 (the second week of Oct. 2011 to the
second week of March 2015) as well as weeks 468 to weeks 487 (the third week of
March 2015 to the fourth week of July 2015) are two groups with consecutive samples
greater than 19. This means the classiﬁcation is successful and structural change
happens. So we need to consider the center of gold prices from weeks 468 (the third
week of March 2015). While for the radius of gold price is from 144 on.
Interval Forecasting on Big Data Context
747

We construct a new threshold fuzzy model only using the last 20 weeks data. The
best-ﬁtted model for the center is as follows:
1  1:31B þ 0:41B2

	
ct ¼ 115:97 þ et; t  468
ð4:1Þ
The best-ﬁtted model for the radius is as follows:
1  0:92B
ð
Þrt ¼ 1:19 þ 1  0:75B
ð
Þet; t  145
ð4:2Þ
Forecasting (ﬁttings) Comparison with other models.
After constructing the model, we look at the core interest of this study – the
forecasting ability. Table 3 is the comparison results for the forecast of gold price.
Compared with Moving-Average model, Exponential smoothing model, ARIMA
model and threshold fuzzy classiﬁcation model (TFC model), we can see TFC model is
much better than other four models based on three statistical criteria.
Average model, Exponential-Smoothing model, Exponential smoothing model,
ARIMA model and TFC model, we can see TFC model is better than other four models
based on the MSE (Table 4).
(a)
(b)
Fig. 3. (a) Chart of mean cumulated fuzzy entropy of gold price center, (b) The classiﬁcation
diagram for Fig. 3, obtained by setting k = 0.01
Table 3. Forecasting comparison with other models gold price (Center)
Model
Performance indices
MSE
MAE MAPE
Moving average model
2399.92 43.4
3.78%
Exponential smoothing
2120.61 40.54 3.53%
ARIMA
1506.61 33.93 2.96%
TFC model with B-data
265.04 13.56 1.19%
748
B. Wu

Table 5 is the comparison results for the forecast of gold price intervals. It reveals
that IMSE and IMAE of TFC model are much smaller than those of the traditional
models. Furthermore, if we take the Coverage Rate into consideration, the TFC model
outerperforms the ARIMA model. In addition, the forecasted interval value of ARIMA
and TFC models for test data (from 468 to 487) is illustrated at Table 5.
Overall, the forecasts of gold price we made, especially for weekly center and fuzzy
interval are excellent. In addition, the forecast interval of gold prices using the fuzzy
classiﬁcation method will provide a better forecasting ability for the uncertainty in
forecasting gold prices effectively. At the end, we also ﬁnd that if the change period of
its structural change can be determined for a time series, better results on the model
construction and forecasting ability can be produced.
Forecasting result
The result of 1. overall external economic fortunes (overall economic indicators) 2.
internal company operating conditions (stocks earnings data) and 3. the entropy state
are shown at table are demonstrated at Table 6.
Table 4. Forecasting comparison with other models for Gold Price (Radius)
Model
Performance indices
MSE
MAE MAPE
Moving average model 48.65 5.46
40.00%
Exponential smoothing 31.59 4.07
32.57%
ARIMA
24.95 3.83
35.01%
TFC model
23.78 4.1
40.67%
Table 5. Forecasting Performance of Gold Price (Interval, from 468–487)
Performance Indices
IMSE
IMAE Fitting performance
Moving average model 2469.90 44.17
64.4%
Exponential smoothing 2169.42 41.12
64.3%
ARIMA
1544.95 34.48
68.5%
TFC model
283.32 14.14
82.6%
Table 6. The result for the contribution value of center forecasting
Step 1
Step 2
Step 3
Center Radius Center Radius Center Radius
1.macroeconomics
6
1
8
−1
7
0
2.internal effect
7
−1
9
2
−2
2
3.entropy state
10
1
9
2
7
2
Total
23
1
26
3
12
4
Interval Forecasting on Big Data Context
749

The forecasting for out of data are showing at Table 7.
5
Conclusion
In this research we use big data context and new measurement system to do the
forecasting work. Big data can be summarized into real number data and non-real data.
Especially for non-real data, such as semantic data, interval data, trapezoid data,
graphics, digital information encoded it, and so we will present a new statistical
analysis of new technologies, new metrics and decision evaluation.
These proposed methods illustrate a innovative applications. Clarify the properties
of measurement system for fuzzy data and their statistical analysis; extension of linear
model building to more general random elements. Propose strategy for selecting an
optimal models based on the ﬁtted density and procedure to check the stability of the
models’ parameters.
Since the model speciﬁcation testing and forecasting is particularly important, the
experience from the inherent economic signiﬁcance and statistical signiﬁcance of
uncertainty needs to be considerate. The main idea of Interval data operation is to
transform the data into centroid and length/area reduce the vague set of supported by
randomly selecting a subset of samples. In addition the soft ranking techniques are
proposed to measure the order of separation margin and to be effective in the gray zone.
In the empirical study there are a number of distinct artifuture intelligent theories to
explore the phenomenon in the economic model construction. Empirical studies will
demonstrate that our frontier and efﬁcient forecasting techniques will be more efﬁcient
information comparing to the point value forecasting either in academic or the realistic
ﬁeld.
Some topics that are worth researching for future study are:
1. Is there a better way to determine the cluster centers for intervals, so that the
relationship between intervals can be described more clearly?
2. This study mainly discusses the analysis of interval data with regards to Z-type,
K-type, pi-type and S-type fuzzy data. This seems to be worth further research.
3. How does a change interval differ when the threshold value k and signiﬁcance level
a differ? Meaning that, how to deﬁne a threshold value k and signiﬁcance level a
that are more effective, and ﬁnd out a more meaningful structural change time.
Table 7. Weakly gold price forecasting for out of data
Period
Real value
Forecasting
Center Radius Interval
Center
Radius Interval
488 (2015/8/3-7)
1089.9
4.2
(1086,1094) 1073 + 23 5.7
(1062,1084)
489 (2015/8/10-14) 1113.7
9.5
(1104,1123) 1097 + 26 7.0
(1070,1092)
490 (2015/8/17-21) 1139.8 11.4
(1117,1163) 1122 + 12 8.2
(1062,1084)
491 (2015/8/24-28) 1137.9 15.5
(1122,1153)
750
B. Wu

References
1. Kim, D.-W., Lee, K.H., Lee, D.: On cluster validity index for estimation of the optimal
number of fuzzy clusters. Pattern Recogn. 37(10), 2009–2025 (2004)
2. Kumar, K., Wu, B.: Detection of change points in time series analysis with fuzzy statistics.
Int. J. Syst. Sci. 32(9), 1185–1192 (2001)
3. Lai, W., Wu, B.: Overcome the sort problem of low discrimination by interval fuzzy number.
In: Innovative Management, Information and Production Nonlinear Mathematics for
Uncertainty and its Applications, pp. 249–258. Springer, Berlin (2013)
4. Pakhira, M.K., Bandyopadhyay, S., Maulik, U.: A study of some fuzzy cluster validity
indices, genetic clustering and application to pixel classiﬁcation. Fuzzy Sets Syst. 15(2),
191–214 (2005)
5. Windham, M.P.: Cluster validity for fuzzy clustering algorithms. Fuzzy Sets Syst. 5(2),
177–185 (1981)
6. Namwong, J., Wattanakul, K.: Measuring of happiness of people in Tippanate’s community,
Chiang Mai province. Empirical Econometrics Quant. Econ. Lett. 1(2), 59–70 (2012)
7. San, O.M., Huynh, V., Nakamori, Y.: An alternative extension of the K-means algorithm for
clustering categorical data. Int. J. Appl. Math. Comput. Sci. 14(2), 241–247 (2004)
8. Sudtasan, T.: Detection of regime switching in stock prices before “window dressing” at the
yearend using genetic algorithm. Int. J. Intell. Technol. Appl. Stat. 5(2), 143–155 (2012)
9. Sudtasan, T., Suriya, K.: Making proﬁt in stock investment before XD dates by using genetic
algorithm. In: Watada, J., et al (eds.) Innovative Management in Information and Production.
Springer Science and Business Media, New York (2013)
10. Tseng, F., Tzeng, G.: A fuzzy seasonal ARIMA model for forecasting. Fuzzy Sets Syst. 126,
367–376 (2002)
11. Tseng, F., Tzeng, G., Yu, H., Yuan, B.: Fuzzy ARIMA model for forecasting the foreign
exchange market. Fuzzy Sets Syst. 158, 609–624 (2005)
12. Wu, B., Chen, M.: Use fuzzy statistical methods in change periods detection. Appl. Math.
Comput. 99, 241–254 (1999)
13. Wu, B., Nguyen, H.: New statistical analysis on the marketing research and efﬁciency
evaluation with fuzzy data. Manag. Decis. 52, 1330–1342 (2014)
14. Yang, C.C., Cheng, Y.-T., Wu, B., Sriboonchitta, S.: Hold a mirror up to nature: A new
approach on correlation evaluation with fuzzy data. In: Quantitative Modeling in Marketing
and Management, pp. 285–299. World Scientiﬁc (2012)
15. Zeng, W., Li, H.: Relationship between similarity measure and entropy of interval valued
fuzzy sets. Fuzzy Sets Syst. 157(11), 1477–1484 (2006)
16. Zhou, H.D.: Nonlinearity or structural break? - data mining in evolving ﬁnancial data sets
from a Bayesian model combination perspective. In: Proceedings of the 38th Hawaii
International Conference on System Sciences (2005)
Interval Forecasting on Big Data Context
751

Bayesian Empirical Likelihood Estimation
for Kink Regression with Unknown Threshold
Woraphon Yamaka(B), Pathairat Pastpipatkul, and Songsak Sriboonchitta
Centre of Excellence in Econometrics, Faculty of Economics,
Chiang Mai University, Chiang Mai, Thailand
woraphon.econ@gmail.com
Abstract. Bayesian inference provides a ﬂexible way of combining data
with prior information from our knowledge. However, Bayesian estima-
tion is very sensitive to the likelihood. We need to evaluate the likelihood
density, which is diﬃcult to evaluate, in order to use MCMC. Thus, this
study considers using the Bayesian empirical likelihood(BEL) approach
to kink regression. By taking the empirical likelihood into a Bayesian
framework, the simulation results show an acceptable bias and MSE val-
ues when compared with LS, MLE, and Bayesian when the errors are
generated from both normal and non-normal distributions. In addition,
BEL can outperform the competing methods with quite small sample
sizes under various error distributions. Then, we apply our approach to
address a question: Has the accumulation of foreign reserves eﬀectively
protected the Thai economy from the ﬁnancial crisis? The results demon-
strate that foreign reserves provide both positive and negative eﬀects on
economic growth for high and low growth regimes of foreign reserve,
respectively. We also ﬁnd that foreign reserves seem to have played a
role in oﬀsetting the eﬀect of the crisis when the growth rate of foreign
reserves is less than 2.48%.
Keywords: Bayesian empirical likelihood · Kink regression
GDP · Foreign reserves
1
Introduction
There is a lot of evidence that suggest that the structure of the economic data
might exhibit nonlinear behavior during the last few decades, especially after
the economic globalization. Thus, a number of nonlinear models have been pro-
posed in the past 30–40 years. The purpose of this study is to propose a modern
method of time series analysis in the evaluation of econometric models, in par-
ticular non-linear model. Recently, the classical methods, for example Ordinary
Least Squares, Bayesian, and Maximum likelihood estimators, are almost always
used and perhaps the most commonly and widely accepted estimators in para-
meter estimation in nonlinear model. However, these methods often face the
problem of poor estimation such as bias and inconsistency of the results since
the data are assumed to have normal distribution. In the estimation, the time
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_54

Bayesian Empirical Likelihood Estimation for Kink Regression
753
series approach to modeling typically involves a set of strong assumptions. It
might not be appropriate to assume that error is a sequence of independent and
identically normal distributed random variables [7]. Moreover, [11,15,17] also
conﬁrm that the explanatory variable is also conditionally non-normally dis-
tributed in many applications, thus the assumption of normality may then yield
biased estimation for model parameters of interest. In addition, many researchers
are also concerned about the limited data that bring about an underdetermined,
or ill-posed problem for the observed data, in which case the traditional esti-
mation techniques have diﬃculty obtaining the optimal solution. As is widely
understood, the larger sample size of data can bring the higher probability of
ﬁnding a statistically signiﬁcant result [14]. [1] suggested that when the samples
are limited, it is often hard to get meaningful results.
Thus, in this study, we propose an alternative estimation called Bayesian
empirical likelihood (BEL) since it can relax a strong assumption of normality
and the limited data. Typically, Bayesian estimation analyses do not assume
large samples, as is the case with maximum likelihood and least squares esti-
mation. A smaller data size can be estimated without losing any power while
retaining precision [16]. [10] proposed that, Bayesian estimation requires only
1:3 ratio of parameters to observations. For least squares estimation, [18] showed
that it is inadmissible when the number of coeﬃcients in the model is larger.
Thus, it is reasonable here to conduct a Bayesian estimation for constructing
distributions based on limited information.
As we mentioned above, although the Bayesian estimation is common and
reasonable with small data size, it still produces a biased estimation or even
misleading results when the true distribution of error may not be normally dis-
tributed especially if it is heavy-tailed or skewed [21]. We know that for the
Bayesian estimation, the important step is the simulation from the posterior
distribution by using Markov Chain Monte Carlo (MCMC). However, Bayesian
estimation is very sensitive to the likelihood; we need to evaluate the likelihood
density, which is diﬃcult to do, in order to use MCMC. Due to the complexity
of the estimation and model, the likelihood is normally assumed to have normal
or other parametric likelihood density. In these cases, it is required to develop a
more robust likelihood to make statistical inference on Bayesian estimation. To
this end, throughout this paper, we incorporate an empirical likelihood into a
Bayesian framework.
The Empirical Likelihood as a robust an alternative to classical likelihood
approaches was ﬁrst introduced by [13]. It has many interesting properties and is
eﬃcient as parametric likelihood [12,19]. The main idea of Empirical Likelihood
is to use a maximum-entropy discrete distribution supported by the observed
data and constrained by nonlinear equations related with the parameters of the
model. In brief, its a non-parametric likelihood, which is fundamental for the
likelihood-based statistical methodology. This study uses empirical likelihood
to approximate the likelihood in the Bayesian computation to be followed by
Bayesian inference.

754
W. Yamaka et al.
In the recent decade, the BEL method has been discussed by [2,4]. The BEL
inference has been applied to various time series models. For example, [20,21]
considered BEL estimation in Quantile model; [3] developed a BEL approach for
linear regression model. However, to our knowledge, there is no work done on
the BEL inference on kink regression model. Here we develop a BEL inference
on nonlinear kink regression of [5] based on the constructed EL and the prior
distribution of parameters including a kink or threshold parameter. The Markov
Chain Monte Carlo (MCMC) method is presented to make Bayesian inference
on parameters by using the Metropolis-Hastings algorithm.
The rest of the paper is organized as follows. In Sect. 2, we introduce the kink
regression, and discuss BEL approach. The simulation study is conducted to
demonstrate the ﬁnite sample performance of the BEL approach through Monte
Carlo simulations in Sect. 3. In Sect. 4, we use microeconomic real data example
to show that the BEL approach can be used as alternative method in nonlinear
model. In this section, we try to addresses a question: Has the accumulation of
foreign reserves eﬀectively protected the Thai economy from the ﬁnancial crisis?
Concluding remarks are given in Sect. 5.
2
Kink Regression Model
2.1
Model Structure
In this study, the following two-regime kink regression model is considered
Yt = β1
−(x′
1,t ≤γ1)−+ β1
+(x′
1,t > γ1)++, ...., +βK
−(x′
K,t ≤γK)−
+βK
+(x′
K,t > γK)+ + αZ + εt
(1)
where Yt is [T × 1] sequence of response variable at time t, x′k,tis a matrix of
(T ×K) predictor variables at time t. Here the relationship of x′k,t and Yt changes
at the unknown location or kink point γK, thus β is a matrix of (T × K × 2)
unknown parameters where (β1
−, ..., βK
−) and (β1
+, ..., βK
+) are the coeﬃcients
with respect to variable x′k,t for value of x′k,t ≤γk and with respect to variable
x′k,t for value of x′k,t > γk , respectively. Z is a vector of covariates whose
relationship with Yt is linear. Following [5], the predictor variables are subject
to regime-change at unknown kink point or threshold variable (γ1, ..., γK) and
thereby separating this regressor into two regimes. These threshold variables are
compact and strictly in the interior of the support of (x1,t, ..., xK,t). In addition,
the error term of the model εt is a vector of error for which we do not assume
any distribution, but we only assume that E(εt) = 0.
2.2
Constructing Empirical Likelihood
Since the distribution of errors εt in Eq. (1) is unspeciﬁed, the likelihood function
is unavailable. Therefore, it is necessary to ﬁnd an appropriate likelihood. In this
study, we adopt empirical likelihood (EL) of [13] as an alternative parametric

Bayesian Empirical Likelihood Estimation for Kink Regression
755
likelihood in our Bayesian estimation. In this section, we will brieﬂy discuss the
concept of empirical likelihood, and its relationship with estimating functions.
Let p1, ..., pk be the set of implied probability weights allocated to the data
and θ ∈

β1
−, ..., βK
−, β1
+, ..., βK
+, α, γ1, ..., γK

. It carries a lot of information
about the stochastic properties of the data. Then, let X−= (x′t ≤γ)−and
X+ = (x′t > γ)+, the empirical likelihood for estimated parameters in Eq. (1),
in the spirit of [13], is
EL(θ) = max
T

t=1
pi.
(2)
By taking logarithm Eq. (2), we have
EL(β) = max
T

t=1
log pt,
(3)
where the maximization is subject to the constraints
T

t=1
pt
∂m(X−
it , X+
it ; θ)
∂θ
(yt −m(X−
it , X+
it ; θ)) = 0,
(4)
T

t=1
pt = 1,
(5)
where, m(X−
it , X+
it ; θ) = β1
−(x′1,t ≤γ1)−+ β1
+(x′1,t > γ1)+, ...., βK
−(x′K,t ≤
γK)−+βK
+(x′K,t > γK)+ +αZ. Sometime the high dimensionality of the para-
meter space (θ, p1, ..., pT ) makes the above maximization problem diﬃcult to
solve, and leads to expressions which are hard to maximize. Instead of maximiz-
ing EL(θ) with respect to the parameters (θ, p1, ..., pT ) jointly, we use a proﬁle
likelihood.
The empirical likelihood, Eq. (3), is essentially a constrained proﬁle likeli-
hood, with constraints Eqs. (4) and (5). In getting the empirical likelihood at
each candidate parameter value θ, this optimization problem can be solved for
the optimal pt. Suppose, we know θ, then we can write the empirical likelihood
as
EL(θ, p1, ..., pT ) = EL(p1, ..., pT )
(6)
We maximize this proﬁle empirical likelihood to obtain (p1, ..., pT ). By conduct-
ing the Lagrange multipliers, we can maximize the empirical likelihood in Eq. (3)
subject to the constraints in Eqs. (4) and (5) as
L(p, λ0, λ1) =
T
t=1 log(pt) + λ0(
T
t=1 pt −1)
+ λ′
1
T
t=1 pt
∂m(X−
it , X+
it ; θ)
∂Ak
(Yt −m(X−
it , X+
it ; θ)),
(7)
where λ ∈R is the Lagrange multipliers. It is a straightforward exercise to show
that with the ﬁrst order condition for L with respect to pt, and setting the

756
W. Yamaka et al.
derivative to zero, we can ﬁnd that λ0 = −T, and by deﬁning λ = −Tλ1, we
obtain the optimal pt as
pt = 1
T

1 + λ′ ∂m(X−
it , X+
it ; θ)
∂θ
(Yt −m(X−
it , X+
it ; θ))
−1
(8)
Then, substituting the optimal pt into the empirical likelihood in Eq. (2) we
obtain
EL(θ) = max
T

t=1
1
T

1 + λ′ ∂m(X−
it , X+
it ; θ)
∂θ
(Yt −m(X−
it , X+
it ; θ))
−1
.
(9)
By taking logarithm, we get
log EL(θ) =
T

t=1
log(1 + λ′ ∂m(X−
it , X+
it ; θ)
∂θ
(Yt −m(X−
it , X+
it ; θ)) −Tlog(T).
(10)
To compute the proﬁle empirical likelihood at a θ, it consists of two estimation
steps. Firstly, it is important to solve a nonlinear optimization to obtain pt, λ,
and EL(θi) which depends on θi. Second step, the proﬁle empirical likelihood is
then maximized with respect to candidate θi. Then, we propose another candi-
date θi to repeat the ﬁrst step again. After EL(θi) is computed for all candidates
θi, the maximum value of EL(˜θi) is selected. The maximization problem can now
be represented as the problem of minimizing Q(λ)
Q(λ) = −
T

t=1
log(1 + λ′ ∂m(X−
it , X+
it ; θ)
∂θ
(Yt −m(X−
it , X+
it ; θ)).
(11)
subject to 0 ≤pt ≤1, that is
1 + λ′ ∂m(X−
it , X+
it ; θ)
∂θ
(Yt −m(X−
it , X+
it ; θ)) ≥1/T
(12)
To compute θ, one use a nested optimization algorithm where the outer maxi-
mization loop with respect to θ encloses the inner minimization loop with respect
to λ. Some comments on the inner loop and the outer loop are in order. In the
application study, the number of all possible θi can be so large that it becomes
infeasible and insensible to evaluate them all. Thus, we can employ a standard
least squares estimator to get the estimated θLS and specify the sensible range
of candidate θi = [−2θLS, 2θLS].
2.3
Bayesian Empirical Likelihood for Kink Regression
The posterior distribution consists of the estimation of empirical likelihood func-
tion and the prior distribution. It is derived using Bayes rule. Here, we brieﬂy
remind how it is derived. Let (Ω, A, P) be a probability space. Let An, n ≥1, be

Bayesian Empirical Likelihood Estimation for Kink Regression
757
a countable, measurable partition of Ω, and B ∈A be an event with P(B) > 0.
Then, for any n ≥1,
P(An |B ) =
P(B |An)P(An)
∞

j=1
P(B |Aj)P(Aj)
(13)
Indeed, we have
P(An |B ) = P(An ∩B)
P(B)
= P(B |An P(An))
P(B)
(14)
And writing
B = B ∩Ω = B ∩(∪∞
j=1Aj) = U ∞
j=1(B ∩Aj)
(15)
We have
P(B) =
∞

j=1
P(B ∩Aj) =
∞

j=1
P(B |Aj)P(Aj).
(16)
Consider the discrete case, let X, Y be discrete random variables. Then
P(X = x |Y = y) =
P(Y = y |X = x)P(X = x)

x′ P(Y = y |X = x′ )P(X = x′)
(17)
is the conditional density of X given Y . Here, P(X = x) denotes prior density of
X, while P(Y = y) denotes empirical likelihood density. Thus, P(X = x |Y = y)
is the posterior density and we can rewrite Eq. 17 as
P(θ |Y, X ) ∝EL(θ) · π(θ),
(18)
where π(θ) denotes a prior density of each estimated parameters. To estimate
the posterior distribution in kink regression models, [20] suggested that the value
of the empirical likelihood is relatively easy to compute given θ which makes the
Metropolis-Hastings algorithm of [6] feasible for sampling from the posterior.
However, it remains for this study to derive the conditional posterior distribution
for unknown parameter θ. If we select a proper prior, the posterior in Eq. 18 is
also proper. In this study, we choose the priors as follows.
We take θ = {α, β−, β+} to be normally distributed with mean θ0 and vari-
ance 0.01, γk is assumed to have uniform distribution. Hence, the conditional
posteriors of θ and γk can be computed as in the following:
(1) The conditional posterior distribution for θ is
θ∗=
	
X′X
(ε′ε/n)2 + 0.01

−1
×
	
X′X
(ε′ε/n)2 ˜θ + 0.01(θ0)

,
(19)
where ˜θ = (X′X)−1X′Y and X = {x′k,t ≤γk, x′k,t > γk} =

x−
k,t, x+
k,t

.

758
W. Yamaka et al.
(2) The conditional posterior distribution for γk can be written as
P(γk |θ, Y, X ) =

1
EL(θ |Y, X ) · π(θ),
(20)
To sample all of these parameters based on conditional posterior distribution, we
employ the Markov Chain Monte Carlo, Metropolis-Hastings algorithm, which
is especially useful in extracting marginal distributions from fully conditional
distributions. We run the Gibbs sampler for 20,000 iterations where the ﬁrst
5,000 iterations serve as a burn-in period. For Metropolis-Hastings algorithm,
we apply it to ﬁnd kink value Θ, γk where the acceptance ratio is
r =
EL(θ∗|Y, X)π(θi−1 |θ∗)
EL(θi−1 |Y, X) π(θ∗|θi−1)
(21)
Then, we set
θj =
θj−1
U < r
θ∗
j
U > r
(22)
where θj−1 is the estimated vector of parameter at (j −1)th draw and θ∗
j is
candidate vector of parameters which random from N(θj−1, 0.01). U is Uni-
form(0,1). This means that if the candidate θ∗
j looks good, keep it; otherwise,
keep the current value θj−1. By using a MetropolisHastings algorithm, we esti-
mate the parameters using the average of the Markov chain on θ as an estimate
of θ, when the posterior density is likely to be close to normal and the trace of
θj looks stationary.
2.4
Testing for a Kink Eﬀect
Since the non-linear structure of the model has been considered in this study, we
develop Bayes factor which is a reliable testing procedures in model comparison
and kink eﬀect test in the Bayesian approach. The purpose of this Bayes factor is
to check whether or not kink regression model is signiﬁcant relative to the linear
regression model. It can be used to assess the models of interest namely linear
and kink regression, so that the best ﬁt model will be identiﬁed given a data
set and a possible model set. In short, Bayes factor is a useful tool for selecting
a possible model [8]. Bayes factor is used for the ratio of the posterior under
one model to another model. In this study, we consider the linear model to be a
null model denoted by M1 and the switching model to be an alternative model
denoted by M2. More speciﬁcally, Bayes factor BF is given by
BF = P(Y, X |M1)
P(Y, X |M2) =

EL(Y, X |θ1)π(θ1 |M1)dθ1

EL(Y, X |θ2)π(θ2 |M2)dθ2
,
(23)
where P(Y, X |M1) and P(Y, X |M2) are the posterior density of the null model
and alternative model, respectively. θ1 and θ2 are the vector of parameters of M1
and M2, respectively. For choosing the appropriate model, we follow the idea of
[9] which can be summarized in Table 1.

Bayesian Empirical Likelihood Estimation for Kink Regression
759
Table 1. Interpretation
log (BF) <2
2 −6
6 −10
>10
Supports M2
Positive to M1
Strong support to M1
Very strong support M1
Source: Kass and Raftery [9]
3
Simulation Study
In this section, we use Monte Carlo simulations to investigate the performance
of the BEL method in terms of coverage probability and estimation eﬃciency.
We focus on the asymptotic properties of the posterior distribution in Eq. 18, to
check whether the empirical likelihood is valid for posterior inference based on
the criteria provided in [20]. Moreover, we also compare the performance of our
BEL estimation on kink regression model with the Bayesian with normal likeli-
hood and prior, least squares (LS) and maximum likelihood (ML) estimations.
To compare these methods, the study used bias and Mean Squared Error (MSE)
approaches.
In the simulation, we use the following equation to generate the dataset Yt
Yt = α + β1
−(x′
1,t ≤γ1)−+ β1
+(x′
1,t > γ1)+ + εt
(24)
where the true value for parameters α, β1
−, and β1
+ are α = 1, β1
−= −2,
and β1
+ = 0.5, respectively. The threshold value is γ1 = 6. The covariate x′1,t
is independently generated from the standard normal distribution N(γ1, 1) to
guarantee that γ1 is located in x′1,t. To make a fair comparison, we consider the
following random errors εt: (1) χ2(2), (2)N(0, 1), (3)t(0, 1, 4), (4)Unif(−2, 2),
and (5) non-standard normal scale mixture, N(0, 1) with respect to a uniform
distribution supported on (−1, 1), denoted by NUnif. In this Monte Carlo sim-
ulation, we consider sample size n = 20 and n = 40. Then, we assess the perfor-
mance of our proposed method through the bias and Mean Squared Error (MSE)
of each parameter in which the bias and MSE of each parameter are given by
bias =
N −1
N

r=1
(˜θr−θr)
 ,
and
MSE = N −1
N

r=1
(˜θr−θr)2.
where N = 100 is the number of bootstrapping; and ˜θr and θr are the estimated
value and true value, respectively.
Tables 2, 3, 4, 5 and 6 report the results of the sampling experiments for
the chi-squared, normal, student-t, uniform and mixture normal-uniform error
distributions, respectively. We found that BEL is not the best method when
compared with LS, ML and Bayesian methods. Although some of the bias and
MSE values of estimated parameters from BEL are lower than other methods,

760
W. Yamaka et al.
Table 2. Kink regressions with χ2(2) error
BEL
LS
ML
Bayesian
n = 20
bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.7703 0.7878 0.9571 8.3447 1.8247 3.6852 1.4142
2.5118
β1
−
0.7745 2.0034 0.1165 0.2790 0.0292 0.0364 −0.1464 0.1174
β1
+
2.7387 8.7729 0.2414 0.6313 0.0139 0.0424 0.0012
0.0388
γ1
0.3120 1.8453 0.3776 4.0403 0.1610 0.2899 0.1821
0.1979
BEL
LS
ML
Bayesian
n = 100 bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.7255 0.9069 2.029
4.1938 2.0254 4.1782 1.9125
3.6989
β1
−
0.7078 0.8438 0.0111 0.0059 0.0103 0.0057 0.0217
0.0033
β1
+
1.4887 3.4708 0.0057 0.0047 0.0066 0.0047 0.0209
0.0026
γ1
0.1023 0.8919 0.0155 0.0672 0.0103 0.0641 0.104
0.0145
Source: Calculations
Table 3. Kink regressions with N(0, 1) error
BEL
LS
ML
Bayesian
n = 20
bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.0947 0.3337 0.1217 0.2334 0.1025 0.2392 0.0883 0.214
β1
−
0.2124 0.5303 0.0088 0.0151 0.0004 0.0148 0.0021 0.0181
β1
+
0.2036 0.5025 0.0374 0.0376 0.0521 0.0338 0.0423 0.0173
γ1
0.1874 0.2389 0.0122 0.1479 0.0623 0.1092 0.141
0.2192
BEL
LS
ML
Bayesian
n = 100 bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.0303 0.0217 0.0728 0.0709 0.0727 0.0709 0.0195 0.0321
β1
−
0.0093 0.0099 0.0191 0.0052 0.0191 0.0051 0.006
0.0018
β1
+
0.0181 0.0062 0.0050 0.0022 0.0050 0.0022 0.0225 0.0021
γ1
0.0146 0.0693 0.0476 0.0388 0.0476 0.0388 0.0657 0.0155
Source: Calculations
the BEL does not show evidence of completely outperforming other methods.
However, it still provides a good estimation and could be an alternative estima-
tion for kink regression model, especially when the sample size is small (n = 20).
We found that the performance of BEL estimator is diﬀerent when n = 20 and
n = 40 since the values of bias and MSE are lower when the sample size increases.
In the case of small sample size, we compared the results of BEL estimator with
other competing methods and found that the bias and MSE of its estimated
parameters are mostly lower than other methods. In contrast, the larger sample
size cannot bring about lower bias and MSE of the parameters when compared

Bayesian Empirical Likelihood Estimation for Kink Regression
761
Table 4. Kink regressions with t(0, 1, 4) error
BEL
LS
ML
Bayesian
n = 20
bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.5109 0.6225 0.3432 0.3984 0.2062 0.3262 0.2558 0.2631
β1
−
0.0784 0.0258 0.1035 0.0280 0.0444 0.0143 0.066
0.0082
β1
+
0.0176 0.0078 0.0347 0.0116 0.0069 0.0041 0.0327 0.0108
γ1
0.1887 0.1661 0.2073 0.1813 0.0576 0.0966 0.1728 0.0484
BEL
LS
ML
Bayesian
n = 100 bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.3661 0.39
0.1013 0.0651 0.0747 0.0425 0.0719 0.0483
β1
−
0.051
0.0536 0.0004 0.0038 0.0102 0.0034 0.0101 0.0027
β1
+
0.4207 0.9698 0.0093 0.0019 0.0019 0.0011 0.0289 0.0056
γ1
0.1607 0.0723 0.0474 0.0197 0.0278 0.0116 0.0219 0.0375
Source: Calculations
Table 5. Kink regressions with Unif(−2, 2) error
BEL
LS
ML
Bayesian
n = 20
bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.0911 0.4862 0.0655 0.2768 0.0656 0.2769 0.1392 0.2179
β1
−
0.0233 0.0236 0.0239 0.0255 0.0239 0.0255 0.0824 0.0247
β1
+
0.0457 0.2012 0.0007 0.024
0.0001 0.024
0.0161 0.0295
γ1
0.1793 0.1976 0.1352 0.1481 0.1352 0.1481 0.1328 0.4431
BEL
LS
ML
Bayesian
n = 100 bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.0332 0.0524 0.0279 0.0735 0.0279 0.0735 0.0453 0.0696
β1
−
0.0163 0.033
0.0131 0.003
0.0131 0.003
0.0061 0.0039
β1
+
0.0228 0.0686 0.0288 0.0047 0.0288 0.0047 0.0148 0.0032
γ1
0.0038 0.0136 0.0634 0.0331 0.0635 0.0331 0.0253 0.0218
Source: Calculations
with LS, ML, and Bayesian. This indicates that BEL is useful and more accurate
when the sample size is small and even if non-normal distribution is assumed.
In summary, we can conclude that the overall performance of the BEL esti-
mator applied to kink regression is good over a wide range of error distributions.
The BEL estimators produce acceptable bias and MSE values compared with
LS, ML, and Bayesian when the errors are generated from both normal and non-
normal distributions. In addition, BEL can outperform the competing methods
with small sample sizes under various error distributions.

762
W. Yamaka et al.
Table 6. Kink regressions with N(0, 1) −Unif(−1, 1) error
BEL
LS
ML
Bayesian
n = 20
bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.3699 0.2559 0.4297 0.2618 0.4297 0.2618 0.4204 0.2327
β1
−
0.069
0.7991 0.0078 0.0049 0.0078 0.0049 0.0129 0.005
β1
+
0.0129 0.6453 0.0126 0.0016 0.0126 0.0016 0.0051 0.0034
γ1
0.0079 0.0138 0.0551 0.0185 0.0552 0.0185 0.0525 0.0179
BEL
LS
ML
Bayesian
n = 100 bias
MSE
bias
MSE
bias
MSE
bias
MSE
α
0.361
0.3221 0.3136 0.1233 0.3136 0.1232 0.3418 0.1531
β1
−
0.0059 0.0018 0.0003 0.0013 0.0003 0.0013 0.062
0.002
β1
+
0.0035 0.0018 0.0056 0.0009 0.0056 0.0009 0.0023 0.0012
γ1
0.0015 0.0099 0.0159 0.015
0.0159 0.015
0.0256 0.027
Source: Calculations
4
Empirical Analysis
This study investigates the basic question whether the accumulation of foreign
reserves protected the Thai economy from the ﬁnancial crisis. Motivated by this,
we ﬁt a Kink model for answering this basic question, which is given as follows:
GDPt = α + β1
−(FRt ≤γ)−+ β1
+(FRt > γ)+ + εt,
where GDPt is the growth of Gross Domestic product and FRt is the growth of
foreign exchange reserves of Thailand. The data is quarterly time series data and
the sample period is Q1/1993-Q4/2016. Table 7 gives summary statistics, Jarque-
Bera (JB) test, and Augmented Dickey-Fuller (ADF) test, to better understand
the characteristics of the series. It can be seen that our data is stationary with
non-normality. This is conﬁrmed by the p value < 0.01 of the ADF and JB tests,
respectively, in Table 7. Prior to estimating the kink regression model, we also
had a concern about the nonlinear behavior in our model. In this study, therefore,
we conduct a nonlinear structure test for our data. Bayes factor (BF) test is used
to determine whether our model appears to have a nonlinear behavior. Using the
Bayes factor formula, the results shown in Table 8 provide the values of Bayes
factor of these two models in which we ﬁnd that the value of log(BF) is equal to
−0.03997. This result means the model M2 is more anecdotally supported by the
data under consideration than the model M1, and hence, the data is more likely
to have the nonlinear structure. This result led us to reject the null hypothesis
of linear regression (M1) for the relationship between foreign exchange reserves
and Gross Domestic product for the Thai economy, which means that the kink
regression model is better to describe this non-linear relationship.

Bayesian Empirical Likelihood Estimation for Kink Regression
763
Table 7. Data description
FR growth GDP growth
Mean
0.023169
0.016913
Median
0.023414
0.016678
Maximum
0.257449
0.107358
Minimum
−0.150070
−0.083100
Std. Dev.
0.054383
0.025234
Skewness
0.589845
−0.514270
Kurtosis
6.405468
7.101790
JB Probability
0
0
ADF-Probability 0
0
Source: Calculations
Table 8. Bayes factor of kink eﬀect
P(Y, X |M1)
P(Y, X |M2)
BF
log(BF )
Interpret
Regime 1 vs. Regime 2
−91.2986
−95.02217
0.9608
−0.03997
Support M2
Source: Calculations
Table 9. Coeﬃcients (standard errors) from kink regression
Parameter
BEL
α
0.0278 (0.0003)
β1
−(regime 1)
0.3670 (0.0155)
β1
+(regime 2)
0.5222 (0.0218)
γ
0.0248 (0.0028)
Acceptance rate 0.4624
Source: Calculations
Note: () is standard deviation
4.1
Application Results
The kink regression model is then estimated by BEL estimator, and the esti-
mated results are shown in Table 9. We can observe a diﬀerent sign of eﬀect of
foreign reserves on the GDPt in these two regimes. In this study, we interpret
regime 1 and 2 as low and high foreign reserves growth regimes, respectively. The
results demonstrate that the growth of foreign exchange reserves shows positive
coeﬃcient (0.3670) in regime 1 and negative coeﬃcient (−0.5222) in regime 2.
Consider the kink or threshold point (γ), we can observe that there exists a
statistically signiﬁcant kink eﬀect in our model.
We now can plot a ﬁtted kink regression line in Fig. 2. The result illustrates
a steep positive slope for foreign reserves with a kink or threshold point (γ1)
around 0.0248, switching to a low negative slope above that point. For regime
1, (FRt ≤0.0248), the growth of FR by one percentage is found to increase

764
W. Yamaka et al.
Fig. 1. Histograms based on the MCMC draws.
Fig. 2. Kink plot of GDP growth and Debt/GDP
the growth rate of GDP by 0.1755%. This result indicates the usefulness of
foreign reserves during the crisis if the growth rate of foreign reserves is less
than 2.34%. In contrast, the growth of FR by one percent is found to decrease
the growth rate of GDP by 0.0037%, when FRt > 0.0248. We expect that
the increase in foreign reserves in this regime can reduce domestic savings and
thereby decreasing domestic investment and economic growth.
Figure 2 depicts the histograms based on the MCMC draws, which gives
some basic insight into the geometry of the posteriors obtained in the empirical
analysis. The results demonstrate a good convergence behaviour and it seems to
converge to the normal distribution; thus we can get accurate posterior inference
for parameters that appear to have good mixing (Fig. 1).
5
Concluding Remarks
In this study we propose using empirical likelihood as a working likelihood for
kink regression with unknown threshold in Bayesian estimation. The Empirical
Likelihood can be employed as a robust alternative to classical likelihood. The

Bayesian Empirical Likelihood Estimation for Kink Regression
765
BEL approach can relax a strong assumption of normality and the limited data.
Although the idea of BEL approach is not new, our work provides an important
addition to the literature by employing this approach for non-linear kink regres-
sion model. Moreover, we also propose using a Bayes factor to check whether or
not kink regression model is signiﬁcant relative to the linear regression model. It
can be used to assess the models of interest namely linear and kink regression,
so that the best ﬁt model will be identiﬁed given a data set and a possible model
set. In short, Bayes factor is a useful tool for selecting a possible model.
We then conduct a simulation study to show the performance and accuracy
of BEL estimation for kink regression model Simulation results validate that
our BEL approach can provide accurate estimates for all unknown parameters
including a kink parameter. We ﬁnd that our BEL estimator applied to kink
regression performs well over a wide range of error distributions. The BEL esti-
mators produce acceptable bias and MSE values compared with LS, ML, and
Bayesian when the errors are generated from both normal and non-normal dis-
tributions. In addition, BEL can outperform the competing methods with quite
small sample sizes under various error distributions.
Finally, the empirical results demonstrate that foreign reserves provide both
positive and negative eﬀects on economic growth for high and low growth regimes
of foreign reserves, respectively. We also ﬁnd that foreign reserves seem to have
played a role in oﬀsetting the eﬀect of the crisis when the growth rate of foreign
reserves is less than 2.48%.
Acknowledgements. The authors are grateful to Puay Ungphakorn Centre of Excel-
lence in Econometrics, Faculty of Economics, Chiang Mai University for the ﬁnancial
support.
References
1. Button, K.S., Ioannidis, J.P., Mokrysz, C., Nosek, B.A., Flint, J., Robinson, E.S.,
Munaf, M.R.: Power failure: why small sample size undermines the reliability of
neuroscience. Nat. Rev. Neurosci. 14(5), 365–376 (2013)
2. Chang, I.H., Mukerjee, R.: Bayesian and frequentist conﬁdence intervals arising
from empirical type likelihoods. Biometrika 95, 139–147 (2008)
3. Chib, S., Shin, M., Simoni, A.: Bayesian empirical likelihood estimation and com-
parison of moment condition models. arXiv preprint arXiv:1606.02931 (2016)
4. Fang, K.T., Mukerjee, R.: Empirical-type likelihoods allowing posterior credible
sets with frequentist validity: higher-order asymptotics. Biometrika 93, 723–733
(2006)
5. Hansen, B.E.: Regression kink with an unknown threshold. J. Bus. Econ. Stat.
35(2), 228–240 (2017)
6. Hastings, W.K.: Monte Carlo sampling methods using Markov chains and their
applications. Biometrika 57, 97–109 (1970)
7. Howrey, E.P.: The role of time series analysis in econometric model evaluation. In:
Evaluation of Econometric Models, pp. 275–307. Academic Press (1980)
8. Kwon, Y.: Bayesian analysis of threshold autoregressive models (2003)

766
W. Yamaka et al.
9. Kass, R.E., Raftery, A.E.: Bayes factors. J. Am. Stat. Assoc. 90(430), 773–795
(1995)
10. Lee, S.Y., Song, X.Y.: Evaluation of the Bayesian and maximum likelihood
approaches in analyzing structural equation models with small sample sizes. Mul-
tivar. Behav. Res. 39(4), 653–686 (2004)
11. Lourens, S., Zhang, Y., Long, J.D., Paulsen, J.S.: Bias in estimation of a mixture
of normal distributions. J. Biometrics Biostatistics 4 (2013)
12. Mykland, P.A.: Bartlett identities and large deviations in likelihood theory. Ann.
Stat. 1105–1117 (1999)
13. Owen, A.B.: Empirical Likelihood. Wiley, New York (1998)
14. Peto, R., Pike, M.C., Armitage, P., Breslow, N.E., Cox, D.R., Howard, S.V., Smith,
P.G.: Design and analysis of randomized clinical trials requiring prolonged obser-
vation of each patient. II. analysis and examples. Br. J. Cancer 35(1), 1 (1977)
15. Sriboochitta, S., Yamaka, W., Maneejuk, P., Pastpipatkul, P.: A generalized infor-
mation theoretical approach to non-linear time series model. In: Robustness in
Econometrics, pp. 333–348. Springer International Publishing (2017)
16. Van de Schoot, R., Broere, J.J., Perryck, K.H., Zondervan-Zwijnenburg, M., Van
Loey, N.E.: Analyzing small data sets using Bayesian estimation: the case of post-
traumatic stress symptoms following mechanical ventilation in burn survivors. Eur.
J. Psychotraumatology 6(1), 25216 (2015)
17. Yamaka, W., Pastpipatkul, P., Sriboonchitta, S.: Has the accumulation of foreign
reserves protect the thai economy from ﬁnancial crisis?: An approach of empirical
likelihood. Int. J. Econ. Res. 14(6), 275–285 (2017)
18. Stein, C.: Inadmissibility of the usual estimator for the mean of multivariate normal
distribution. In: Proceedings of the Third Berkeley Symposium on Mathematical
Statistics and Probability, vol. 1, pp. 197–206 (1955)
19. Variyath, A.M., Chen, J., Abraham, B.: Empirical likelihood based variable selec-
tion. J. Stat. Plann. Infer. 140(4), 971–981 (2010)
20. Yang, Y., He, X.: Bayesian empirical likelihood for quantile regression. Ann. Stat.
40(2), 1102–1131 (2012)
21. Zhang, Y.Q., Tang, N.S.: Bayesian empirical likelihood estimation of quantile struc-
tural equation models. J. Syst. Sci. Complex. 30(1), 122–138 (2017)

Spatial Choice Modeling Using the Support
Vector Machine (SVM): Characterization
and Prediction
Yong Yoon(B)
Faculty of Economics, Chulalongkorn University, Bangkok, Thailand
yong.y@chula.ac.th
Abstract. We take a cursory look at the support vector machine (SVM)
as a useful and eﬀective algorithm for characterizing and predicting spa-
tial choice problems in economics. Beginning with a discussion of the
SVM for the linearly separable case as well as the nonlinear non-separable
case using the soft margin SVM and kernels, we then describe how the
SVM can be used to characterize and predict spatial choice models, which
can be seen as a special case of discrete choice models, using examples
from a simple 1-D to the more complex multi-dimensional features space.
Keywords: Support vector machine · Spatial choice modeling
Characterization and prediction
1
Introduction
Big data and statistical or machine learning has arrived in economics in a big
way [12,13]. Amongst various tools, the support vector machine (SVM) originally
developed by Vapnik [17] and Alexey Ya. Chervonenkis in the early 1960s has
become widely used not only for machine learning problems like pattern recog-
nition, but also in other areas such as genetics, politics [11], and marketing [3].
Here, we take a look at how the SVM can be a useful and eﬀective algorithm for
characterizing and predicting spatial choice models in economics.
As a way of introducing the spatial choice problem, a good place to begin
perhaps is with the “two ice cream vendors on a beach” example inspired by
Hotelling [10]. The basic setting is to imagine a stretch of beach, say, a hundred
meters long, on which two ice cream vendors are selling ice cream, each located
at either end of the beach. Assuming that both oﬀer the same ﬂavors and the
prices they charge are the same, it is reasonable to assume that a sunbather
located somewhere in between the two vendors would choose to buy from the
nearest one. From this basic setting, we abstract from the physical distance
of Hotelling’s paper by considering instead attributes of a good or service and
individual preferences.
For example, suppose instead of ice cream vendors we have two Thai restau-
rants, i ∈{1, 2}, selling Thai food with spiciness xi ∈[0, 10], denoting the degree
c
⃝Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0_55

768
Y. Yoon
of spiciness ranging from “not at all spicy” to “very very spicy”. Say we have
y ∈{1, 2, · · · , j} potential customers that have individual preferences for spici-
ness S{j} ∈[0, 10]. Then we can deﬁne the utility of the j−th consumer in
choosing to visit restaurant i as:
u{j}
i
= C{j}
i
−|xi −S{j}|
where C{j}
i
is the opportunity cost of the j−th consumer reaching restaurant i.
For simplicity, we may assume that C{j}
i
is constant and the same for all con-
sumers, such that utility matters only on how “close” a restaurant’s spiciness is
to an individual’s ideal taste.
R(x1 = 2)
R(x2 = 9)
S{1} = 7
0
1
2
3
4
5
6
7
8
9
10
x
h
h
Fig. 1. Simple 1-D linear spatial choice model
Figure 1 illustrates the simple 1-D linear spatial choice model with two choices
of restaurants R oﬀering spiciness of x1 = 2 and x1 = 9. Suppose a certain indi-
vidual has taste S{1} = 7. Then because restaurant oﬀering spiciness of x1 = 2
is further away to the individual’s preferred taste compared to restaurant oﬀering
spiciness of x1 = 9, i.e., since |x1 −S{1}| > |x2 −S{1}| or |9 −7| > |2 −7|, the
individual will choose the restaurant oﬀering spiciness of x1 = 9 resulting in larger
utility for the consumer than if restaurant oﬀering spiciness of x1 = 2 was chosen.
Rather than considering the closeness of attributes to a consumer’s ideal point
as the decision rule for such spatial choice problems, we propose the modeling
of such a spatial choice problem by the SVM algorithm as an alternative and
eﬀective way to characterizing and predicting choice, and one that is also easily
applicable to the multi-dimensional attribute space. Essentially, the SVM can
easily construct a hyperplane in a high- or inﬁnite-dimensional space, which can
then be used for classiﬁcation, regression, or other tasks. We exploit the idea
that the SVM separates the feature space by a hyperplane that has the largest
distance to the nearest training-data point(s) of any class (i.e. so-called functional
margin), thus the large margin classiﬁer eﬀectively divides the space such that
it meaningfully matches characterization and prediction of an economic agents’
spatial choice.
For our simple 1-D linear spatial model, referring to Fig. 2, the SVM algo-
rithm ﬁnds H = 5.5, the separating hyperplane (or rather a point in this case
as we lack suﬃcient dimensions for this simple example) such that any individ-
ual with S{i} < (>) 5.5 would choose restaurant oﬀering spiciness of x1 = 2
(x2 = 9). What is interesting to note is that the SVM splits the feature space

Spatial Choice Modeling Using the Support Vector Machine
769
R(x1 = 2)
R(x2 = 9)
H = 5.5
0
1
2
3
4
5
6
7
8
9
10
h
h
Fig. 2. Splitting the linear space with a ‘hyperplane’ by SVM
right in the middle of the two choices by the hyperplane, such that the choice
alternatives on either side of the hyperplane will be at an equal distance from
the hyperplane. A consumer positioned on the hyperplane will be indiﬀerent to
the two restaurants.
2
Support Vector Machine (SVM)
Assume a two-dimensional space with a number of points represented by x and
labeled yi ∈{1, −1}. The SVM constructs a hyperplane H deﬁned as w · x = 0
(i.e. with parameters (w, b)), where the vector w is perpendicular to the hyper-
plane.
w · x + b = 1
w · x + b = 0
w · x + b = −1
w
2
||w||
x2
x1
b
∥w∥
Fig. 3. Maximum-margin hyperplane and margins for a SVM

770
Y. Yoon
For any arbitrary point represented by a vector u which could be on either
side of the hyperplane H, projecting u onto w such that w · u ≥c and deﬁning
c = −b gives the following decision rule:
w · u + b ≥0,
then + ve
(1)
which deﬁnes one side of the hyperplane, say, where characterization or choices
are +ve. We have now to deﬁne some constraints to establish b and w exactly.
Say, given a sample point x, then
w · x+ + b ≥1 and w · x−+ b ≤−1
To simplify the two equations above, multiplying both equations by yi gives
yi(w · xi + b) ≥1 (the same for both equations), which can be re-written as
yi(w · xi + b) −1 ≥0.
As shown in Fig. 3 we could also draw two separate and parallel boundaries,
which we may call margins, running parallel on either side of the hyperplane by
yi(w · xi + b) −1 = 0
(2)
Taking a point on each of the margins, say x+ and x−, say for positive and
negative examples (depicted by white and black circles in Fig. 3), we can measure
their distance apart as
(x+ −x−) · w
∥w∥=
2
∥w∥
(3)
since x+ = 1 −b and x−= 1 + b, by Eq. 2. And what the SVM does, because
it looks for the hyperplane with the largest margin, is to maximize the margin
2/||w||, or what is the same thing, ∀i ∈{1, . . . , n}, as
min 1
2||w||2
(4)
subject to
yi(w · xi + b) ≥1
which is a rather simple quadratic programming (QP) problem for which there
are readily available algorithms of complexity O(n3).1 But, when n and d are
large (tens of thousands) even the best QP methods fail. However, one very
desirable characteristics of the SVM is that most of the data points end up
being irrelevant.2
Setting up the Lagrangian,
L = 1
2||w||2 −
n

i=1
αi [yi(w · xi + b) −1]
(5)
1 For example, so-called interior point algorithms that are variations of the Karmarkar
algorithm for linear programming.
2 The relevant data are only the points that end up exactly on the margin of the
optimal classiﬁer and these are often a very small fraction of n.

Spatial Choice Modeling Using the Support Vector Machine
771
the F.O.C gives,
∇wL = 0, i.e., w =
n

i=1
αiyixi
(6)
∂L
∂b = 0, i.e.,
n

i=1
αiyi = 0
(7)
αi [yi(wxi + b) −1] = 0, for all i ≤n.
(8)
The above Karush-Kuhn-Tucker (KKT) conditions for optimality provide a
complete characterization of the optimal hyperplane. The normal w must be a
linear combination of the observed vectors x, given by Eq. 6; the coeﬃcients of
this linear combination must add up to 0, which is Eq. 7, and; ﬁnally the com-
plementarity slackness conditions Eq. 8 tell us that the only non-zero Lagrange
multipliers αi are those associated to the vectors x exactly on the margin, i.e.,
corresponding to Eq. 2 which are the so-called support vectors and they are the
only ones needed in Eq. 6. The support vectors are the observations x(s)
i
at the
exact distance 1/||w|| from the separating hyperplane, i.e., satisfying Eq. 2. And
as mentioned above, the number of such vectors is usually much smaller than n
which makes it possible to consider very large numbers of examples points and
higher dimensions with xi having many coordinates.
3
Duality: Soft-Margin SVM and Kernels
Of course, more realistically, the feature space may not be as easily linearly
separable as assumed so far. The SVM literature proposes two further extensions
to tackle not-so-easily linear separable spaces, (1) by using soft margins, and,
(2) by transformation using kernels.
The soft margin SVM largely due to Cortes and Vapnik [4] introduces extra
“slackening” to which instance ξi ≥0 can be thought of as instances that are
misclassiﬁed, and 0 otherwise. The optimization problem is then reformulated
as
min 1
2||w||2 + C

ξi
(9)
subject to
yi(w · xi + b) ≥1 −ξi
with ∀i ∈{1, . . . , n} and ξ ≥0, and where C  ξi serves as some penalty for
slackness. Then given a new instance xi, the classiﬁer f(x) = sign(w · xi + b).3
The corresponding Lagrangian is then,
L = 1
2||w||2 + C

ξi −
n

i=1
αi [yi(w · xi + b) −1 + ξi] −

βiξi
(10)
3 Note also that for the hard-margin form, we can set C to a very large value close to
inﬁnity resulting in a huge penalty.

772
Y. Yoon
where the constraint  βiξi is included because of the non-negative constraint
on ξ ∈[0, ∞]n.
The F.O.C give ∇wL = 0, i.e., w = n
i=1 αiyixi, (as in Eq. 6), ∂L/∂b = 0,
i.e., n
i=1 αiyi = 0, (as in Eq. 7), and L/∂ξ = 0, i.e., α = C −β or β = C −α.
Fig. 4. Soft-margin SVM
Figure 4 shows the case where data points are not easily linearly separable
(left panel). By applying the soft margin SVM (right panel), separation is found
with all points in the margin and those classiﬁed incorrectly serving as support
vectors.
For the primal problem (Eqs. 4 and 9), it is easy to show that its dual
problem is
max
n

i=1
αi −1
2
n

i,j=1
αiαjyiyj(xi · xj)
(11)
subject to

αiyi = 0, and αi ∈[0, C]
and, by the strong duality property for such quadratic programming prob-
lems, the solution to this dual problem yields the same solution as the primal
(see [2,5,6]).
What is important to note here, however, is that the dual tells us that the
optimization problem essentially depends on the dot product (inner product of
data vectors), xi · xj. Thus, we have the decision rule for any ui that

αiyixi · ui + b ≥0 then + ve

Spatial Choice Modeling Using the Support Vector Machine
773
This leads us to the “kernel trick”, which is essentially a way of transform-
ing the original data points with a kernel function in such as a way that allows
the SVM algorithm to ﬁt the maximum-margin hyperplane by linear separa-
tion in the higher-dimensional transformed feature space. The idea is to apply
a kernel or an appropriate mapping function, or more speciﬁcally a Grim func-
tion which follows the so-called Mercer’s condition, on the dot product, i.e.,
K(xi, xj), which would allow the linear SVM to ﬁnd a separating hyperplane
in the higher-dimension feature space which can then be translated back into
decision boundaries in the original data space.4 The objective function Eq. 11
can be re-written as
max
n

i=1
αi −1
2
n

i,j=1
αiαjyiyjK(xi · xj)
(12)
and with the dual representation of the optimal
weight
vector w
=
n
i=1 αiyiφ(xi) for some appropriate non-linear mapping function, φ : I = R2 →
F = Rd from, say, a 2-dimensional input space I into a higher d-dimensional fea-
ture space F, the equation for the optimal separating hyperplane can be written
as
w · x + b =

αiyiK(xi · ui) + b = 0
where αi are the optimal Lagrange multipliers obtained by maximizing Eq. 12
and b is the optimal perpendicular distance from the origin now with w and
support vectors x(s) in the higher dimension feature space F.
Figure 5 shows the decision boundary for a non-linear case (left panel)
using the Gaussian kernel, which is represented with the kernel transformation
on a higher-dimensional feature space (right panel) that is linearly separable.
Although many useful kernel functions correspond to an inﬁnite-dimensional φ
vector, which is equivalent to SVM with inﬁnite number of features, yet the
algorithm does not bear much additional computational cost as long as K itself
is easy to compute (i.e. the Gram matrix will be in O(m2) dimension).5
4 If K(xi, xj) the satisﬁes the Mercer’s condition, i.e.

a

b K(a, b)g(a)g(b)dadb ≥0,
then K(a, b) = φ(a) · φ(b) for some transformation φ and the similarity function K
is referred to as a kernel or mercer kernel. The Mercer condition simply asserting
that the Gram matrix K has a positive-deﬁnite structure for any possible data set,
i.e., g′ · K · g ≥0, for all xi.
5 Some of the most commonly used kernels are (1) the dth-degree polynomial kernel,
(x·xi+θ)d, (2) the Gaussian or radial basis kernel, exp(−γ||x−xi||2), (3) the sigmoid
kernel, tanh(κ1(x·xi)+κ2), and (4) the inverse multi-quadratic, (

||x −xi||22σ2 +
c2)−1.

774
Y. Yoon
φ
Fig. 5. Separation by the Gaussian kernel: exp(−γ||x −xi||2)
4
Characterizing and Predicting Spatial Choice Models
Using the SVM
In economics, spatial choice models can be viewed as a special case of the broader
class of discrete choice models, which were popularized by the Economics Nobel
Prize laureate McFadden [7–9], amongst others. More speciﬁcally, spatial choice
models considered here satisfy the conditions for discrete choice, namely, that
the choices or set of alternatives are collectively exhaustive, mutually exclusive,
and ﬁnite.6 As another application, let’s go back to Hotelling’s model mentioned
in the introduction by assuming this time two attributes, x1 and x2, spanning
a 2-dimensional feature space and having black and white labels or examples
positioned as shown in Fig. 6 (top panel).
Clearly the linear SVM will struggle to ﬁt a separating hyperplane on this
nonlinear non-separable feature space. Hence, we can employ the “kernel trick”
by adding another dimension z (shown in the bottom panel) that allows for
the linearly separation by the SVM in this higher-dimensional feature space.
This translated back into the original space results in four separate quadrants
that neatly predicts black (or white) labels depending on whether the attributes
x1 and x2 taken together lie in the ‘northwest’ or ‘southeast’ (‘northeast’ or
‘southwest’) quadrants. More formally, for a certain individual with preferences
S{x1} and S{x2} for attributes x{1} and x{2} respectively that fully characterize
two mutually independent and distinct choices, yi ∈{black, white}, then we
may predict that the individual will choose either black or white depending on
6 See Train (2003) [16]. Empirically discrete choice models employ many interesting
forms such as the Binary/Multinomial Logit and Probit, Conditional Logit, Nested
Logit, Generalized Extreme Value Models, Mixed Logit, and Exploded Logit. Dis-
crete choice models have been used extensively in transportation, psychology, energy,
housing, marketing, voting, and many more.

Spatial Choice Modeling Using the Support Vector Machine
775
Fig. 6. The kernel trick
the ‘closeness’ of features describing black and white to his or her preferences.
Furthermore, a useful measure of ‘closeness’ is the euclidean distance, D. That
is, for the 2-D feature space, we have for label l ∈{black, white}
Dl =

(x{1}
l
−S{x1})2 + (x{2}
l
−S{x2})2
and the individual will choose black which result in higher utility if Dl=black <
Dl=white, and vice versa (or indiﬀerent if Dl=black = Dl=white).
Characterization and prediction by the euclidean distance can easily be gen-
eralized for cases with multiple m features, i.e. Xi ∈Rm. The important point
here is to note that characterization by the SVM and prediction (in contrast
to other machine learning techniques such as decision trees, neural networks
and the k-nearest neighbors algorithms, amongst others) is exactly the same
as what we would have had with the euclidean distance method. This should
not be surprising because the SVM as a large margin classiﬁer divides the fea-
ture space by a linear hyperplane that maximizes the separation of the diﬀerent
classes or examples with respect to the support vectors and does this precisely
by the euclidean distance. Moreover, increasing features does not necessarily

776
Y. Yoon
increase the computational burden of SVM very much even for the nonlinear
non-separable case, because by employing the kernel method when moving into
a higher dimensional space often requires computation of only a few ﬁnite sup-
porting vectors, i.e. most αi in Eq. 12 would be zero and once an optimal set of
alpha is learned, all other xi where αi = 0 can be discarded and the resulting
sparse solution is desirable as it reduces the required memory and computation
requirements for classiﬁcation.
Compared to the logistic model, which is used often in discrete choice model-
ing, a number of papers [3,15] have shown that the SVM outperforms characteri-
zation and prediction compared to the logistic regression. In general, parametric
models, including discriminant analysis, which is another popular classiﬁcation
method, tend to face structural inadequacy especially when dealing with rela-
tional or nested boundaries between choices [18]. As a last point, over-ﬁtting
in high dimensional settings with a small number of examples may not be an
issue with spatial choice modeling. On the contrary, it may be desirable that
SVM allows us more complex and interesting boundaries for classiﬁcation and
prediction of spatial choice models.
5
Concluding Remarks
We have looked at the support vector machine (SVM) as a useful algorithm to
modeling spatial choice problems. With linearly separable data, the SVM can be
viewed as a standard quadratic programming (QP) problem, for which optimiza-
tion using the Lagrangian and the dual QP involved only m variables or m2 dot
products. we also treated the soft margin variant for nonlinear non-separable
data by including a margin or extra slackness that allowed for misclassiﬁca-
tion, which essentially is similar to using a simple surrogate loss term called a
hinge loss and applying a form on L-2 regularization. The optimization was seen
to have a dual form that requires only pair-wise similarity between the data.
More generally, we showed how to exploit these dual forms to work with the
SVM that can operate implicitly in a very high-dimensional feature space by
deﬁning non-linearity similarity kernels that measure the dot product in those
spaces, making the classiﬁer independent of the implicit feature dimension while
remaining computationally in the quadratic to cubic in the data space. This
makes SVM somewhat minimalist in terms of memory and computational time.
We also discussed ﬁrst using a simple linear 1-D case, then a higher 2-D
feature space, how the SVM can be exploited for spatial choice modeling. Gen-
eralization for m-dimension space is straightforward. What is important to note
is that even for the more complex and higher dimensional feature space, the
major advantage of the SVM is that it did not necessarily suﬀer from the curse
of dimensionality because only computation involving a few support vectors
mattered. Furthermore, the SVM has been found to be superior in terms of clas-
siﬁcation and prediction compared to other standard techniques like the logistic
regression commonly used in discrete choice modeling. The treatment of SVM
and demonstration of the easiness of translating the SVM to spatial choice prob-
lems in the discussion above is admittedly superﬁcial, but we hope that we have

Spatial Choice Modeling Using the Support Vector Machine
777
demonstrated suﬃciently that the easiness of the abstraction of attributes and
the choices (or labels) of the SVM make it a natural choice for characterization
and prediction of spatial choice models.
Admittedly, the utility function adopted here is a simple, deterministic one,
and understanding how the SVM would apply in relation to more complex,
stochastic utility models could be a further extension. Furthermore, there are
further technical issues to consider. For example, although the SVM is essentially
a two-class classiﬁer, it would be interesting to see how solving multi-class spatial
choice problems can be done with multi-class extensions of the basic SVM [14].
Also we have only looked at supervised learning, and unsupervised learning based
on ideas of the SVM, so-called support vector clustering (SCV) due to Ben-Hur
et al. [1] and others can also be explored further in the context of spatial choice
modeling.
References
1. Ben-Hur, A., Horn, D., Siegelmann, H.Y., Vapnik, V.: Support vector clustering.
J. Mach. Learn. Res. 2, 125–137 (2001)
2. Burges, C.J.C.: A tutorial on support vector machines for pattern recognition.
Data Min. Knowl. Disc. 2(2), 121–167 (1998)
3. Cui, D., Curry, D.: Prediction in marketing using support vector machine. Mark.
Sci. 24(4), 595–615 (2005)
4. Cortes, C., Vapnik, V.: Support vector networks. Mach. Learn. 20(3), 273–297
(1995)
5. Cristianini, N., Shawe-Taylor, J.: An Introduction to Support Vector Machines and
Other Kernel-Based Learning Methods. Cambridge University Press, Cambridge
(2000)
6. Hastie, T., Tibshirani, R., Jerome, F.: The Elements of Statistical Learning.
Springer New York Inc., New York (2001)
7. McFadden, D.L.: Conditional logit analysis of qualitative choice behaviour. In:
Zarembka, P. (ed.) Frontiers in Econometrics, pp. 105–142. Academic Press, New
York (1973)
8. McFadden, D.L.: Econometric models of probabilistic choice. In: Manski, C.F.,
McFadden, D.L. (eds.) Structural Analysis of Discrete Data with Econometric
Applications, pp. 198–272. MIT Press, Cambridge (1981)
9. McFadden, D.: Econometric analysis of qualitative response models. In: Griliches,
Z., Intriligator, M.D. (eds.) Handbook of Econometrics, vol. 2, 1st edn, chap. 24,
pp. 1395–1457. Elsevier (1984)
10. Hotelling, H.: Stability in competition. Econ. J. 39(153), 41–57 (1929)
11. Jakulin, A., Buntine, W., La Pira, T.M., Brasher, H.: Analyzing the US senate in
2003: similarities, networks, clusters and blocs. Polit. Anal. 17(3), 291–310 (2009)
12. Mullainathan, S., Spiess, J.: Machine learning: an applied econometric approach.
J. Econ. Perspect. 31(2), 87–106 (2017)
13. Varian, H.: Big data: new tricks for econometrics. J. Econ. Perspect. 28(2), 3–28
(2014)
14. Shigeo, A. Multiclass support vector machines. In: Support Vector Machines for
Pattern Classiﬁcation, pp. 113–161. Springer, London (2010)

778
Y. Yoon
15. Salazar, D.A., Velez, J.I., Salazar, J.C.: Comparison between SVM and logistic
regression: which one is better to discriminate? Rev. Colomb. Estadistica 35(2),
223–237 (2012)
16. Train, K.: Discrete Choice Methods with Simulation. Department of Economics,
SUNY-Oswego, Oswego (2003)
17. Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer Science, Berlin
(2000)
18. West, P.M., Brockett, P.L., Golden, L.L.: A comparative analysis of neural net-
works and statistical methods for predicting consumer choice. Mark. Sci. 16(4),
370–391 (1997)

Author Index
A
Autchariyapanitkul, Kittawit, 233, 303
B
Baru, Chaitanya, 3
Boonyakunakorn, Petchaluck, 339, 492
Breidt, F. Jay, 21
C
Cao, Tien-Dung, 95
Chaiboonsri, Chukiat, 573, 706, 724
Chakpitak, Noppasit, 350, 363
Chanaim, Somsak, 350, 656
Chang, Man-Ling, 692
Chen, Cathy W. S., 122
Chen, Huan-Yu, 321
Chen, Yun-Zu, 683
Choy, S. T. Boris, 303
Chu, I-Tien, 563
D
Dai, Fang, 273
Denœux, Thierry, 157
Duangin, Saowaluk, 375
Dumrongpokaphan, Thongchai, 177, 192, 198
H
Huang, Chien-Min, 21
Huang, Man-Yu, 321
Huang, Wan-Tran, 563, 683
Huang, Yu-Sheng, 683
Huang, Yu-Ting, 321
J
Jaitang, Chalerm, 408
K
Khiewngamdee, Chatchai, 422
Kingnetr, Natthaphat, 430, 517
Kosheleva, Olga, 198, 205
Kreinovich, Vladik, 177, 192, 198, 205, 214,
266
L
Lee, Sangyeol, 246
Lee, Tzong-Ru (Jiun-Shen), 321
Li, Baokun, 222, 287
Liu, Jianxu, 443, 454
M
Ma, Ji, 443, 454
Ma, Ziwei, 222, 233
Maneejuk, Paravee, 350, 408
Moslehpour, Massoud, 392, 464
Munkh-Ulzii, 464
N
Namatame, Akira, 255
Navapan, Kobpongkit, 492
Nguyen, Dinh-Quyen, 95
Nguyen, Dung Tien, 182
Nguyen, Hung T., 198, 205
Nguyen, Son P., 182
Nguyen, Thien Dinh, 182
Niwitpong, Sa-Aat, 670
Niwitpong, Suparat, 670
O
Oanh, Tran Thi Kim, 502
Oh, Haejune, 246
Opsomer, Jean D., 21
Osathanunkul, Rossarin, 422, 517
P
Paolella, Marc S., 36
Pastpipatkul, Pathairat, 339, 752
Phadkantha, Rungrapee, 536
Pham, Uyen H., 182
© Springer International Publishing AG 2018
V. Kreinovich et al. (eds.), Predictive Econometrics and Big Data, Studies in Computational
Intelligence 753, https://doi.org/10.1007/978-3-319-70942-0
779

Pham, Van-Chien, 692
Polak, Paweł, 36
R
Rakpho, Pichayakone, 549
Rakpuang, Pattaravadee, 724
Romano, Joseph P., 78
S
Shen, Meng-Chun Susan, 563
Singvejsakul, Jittima, 573
Sirikanchanarak, Duangthip, 629
Sirisrisakulchai, Jirakom, 375
Sriboonchitta, Songsak, 205, 214, 266, 339,
350, 363, 375, 408, 422, 430, 443, 454,
492, 517, 573, 600, 613, 629, 643, 656,
706, 752
Srichaikul, Wilawan, 590
Sun, Yu-Wen, 122
Suwannajak, Jirawan, 643
T
Tansuchat, Roengchai, 536, 549, 590, 643
Teetranont, Teerawut, 600, 613, 656
Thach, Nguyen Ngoc, 502
Thangjai, Warisa, 670
Thanh, Ha Le Thi, 392
Thipwong, Phanee, 683
Tian, Weizhong, 222, 273
Ting, Chung-Te, 683
Tran, Hien Duy, 95
Tungtrakul, Tanaporn, 430, 629
V
Van Kien, Pham, 392, 464
Van Le, Chon, 146
W
Wang, Tonghui, 222, 233, 273, 287, 303
Wannapan, Satawat, 706, 724
Wei, Fengrong, 273
Wiboonpongse, Aree, 408
Wolf, Michael, 78
Wu, Berlin, 737
Wu, Mixia, 287
Y
Yamaka, Woraphon, 363, 422, 536, 549, 590,
600, 613, 643, 656, 752
Yoon, Yong, 767
Z
Zhu, Xiaonan, 233, 287, 303
780
Author Index


International Journal of Computer Vision 43(1), 29–44, 2001
c⃝2001 Kluwer Academic Publishers. Manufactured in The Netherlands.
Representing and Recognizing the Visual Appearance of Materials
using Three-dimensional Textons
THOMAS LEUNG ∗AND JITENDRA MALIK
Computer Science Division, University of California at Berkeley, Berkeley, CA 94720-1776, USA
Received January 4, 2000; Revised February 23, 2001; Accepted February 23, 2001
Abstract.
We study the recognition of surfaces made from different materials such as concrete, rug, marble, or
leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface
attributes: (1) reﬂectance and (2) surface normal. In this paper, we provide a uniﬁed model to address both these
aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated
local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or
stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local
irradiance distribution, represented as a set of linear Gaussian derivative ﬁlter outputs, under different lighting and
viewing conditions.
Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the
order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these
textons. We demonstrate the application of this representation for recognition of the material viewed under novel
lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance
of materials under novel conditions.
Keywords:
3D texture, texture recognition, texture synthesis, natural material recognition
1.
Introduction
We study the recognition of surfaces made from differ-
ent materials such as concrete, rug, marble, or leather
on the basis of their textural appearance. Such natural
textures arise from spatial variation of two surface at-
tributes: (1) reﬂectance and (2) surface normal. In this
paper, we provide a uniﬁed model to address both of
these aspects of natural texture.
In the past, texture recognition/discrimination has
been posed primarily as a 2D problem. Viewpoint
and illumination are assumed constant. Two of the
representative techniques are Markov random ﬁelds
(Chellappa and Chatterjee, 1985; Cross and Jain, 1983;
Mao and Jain, 1992; Yuan and Rao, 1993; Zhu et al.,
1998) and ﬁlter responses (Fogel and Sagi, 1989; Jain
∗Present address: Compaq Cambridge Research Laboratory.
and Farrokhsia, 1991; Malik and Perona, 1990; Puzicha
et al., 1997; Rubner and Tomasi, 1999). In all these
work, surface normal variations are ignored. However,
nature shows an abundance of such relief textures. Ex-
amples are shown in Fig. 1. Notice in particular the ef-
fects of surface normal variations: specularities, shad-
ows, and occlusions. Figure 2 shows samples of the
same material under different viewpoint/lighting set-
tings. The appearance looks drastically different in the
3 images. Recognizing that they belong to the same
material is a challenging task.
Variations due to surface relief cannot be dealt with
by simple brightness normalization or intensity trans-
forms (such as histogram matching). For example, if
the surface structure is a ridge, a dark-light transition in
one image under one illumination will become a light-
dark transition when the light source is moved to the
other side of the ridge. Shadows also cause signiﬁcant

30
Leung and Malik
Figure 1.
Some natural 3D textures from the Columbia-Utrecht database (Dana et al., 1999). Left to right: “Pebbles”, “Aluminum Foil”,
“Sponge”, “Rabbit Fur”, “Concrete” and “Painted Spheres”. These textures illustrate the effects caused by the 3D nature of the material:
specularities, shadows, and occlusions.
Figure 2.
The same patch of the material “Crumpled Paper” imaged under three different lighting and viewing conditions. The aspect ratio
of the ﬁgure is determined by the slant of the surface. Even though the three images are corresponding patches from the same material, the
appearances are drastically different.
problems: two regions will have the same brightness
under one illumination; while the shadowed region will
be darker in another. These two problem cases are il-
lustrated in Fig. 3.
The complexity in the relationship between the im-
age intensity values to the viewing/lighting settings
and the properties of 3D textures led to recent interest
in building explicit models for 3D textures (Chantler,
1994; Chantler and McGunnigle, 1995; Dana and
Nayar, 1998; Dana and Nayar, 1999b; Dana et al., 1999;
Koenderink and van Doorn, 1996; Koenderink et al.,
1999; Leung and Malik, 1997; van Ginneken et al.,
1998). From these analytical models, such as Gaussian
distributed height variation, or cylindrical models, low-
order statistical quantities, e.g. brightness distribution
or correlation length, are derived. However, these mod-
els are rather simple and they lack the expressiveness
to solve the general problems of natural material rep-
resentation, recognition, and synthesis under varying
lighting and viewing conditions.
The main idea of this paper is the following—at the
local scale, there are only a small number of percep-
tually distinguishable micro-structures on the surface.
For example, the local surface relief ˆn(x, y) might cor-
respond to ridges, grooves, bumps, hollows, etc. These
could occur at a continuum of orientations and heights,
but perceptually we can only distinguish them up to an
equivalence class. Similarly, reﬂectance variations fall
into prototypes like stripes, spots, etc. Of course one
can have the product of these two sources of variation.
Our goal is to build a small, ﬁnite vocabulary of
micro-structures, which we call 3D textons. This term
is by analogy to 2D textons, the putative units of preat-
tentive human texture perception proposed by Julesz
nearly 20 years ago. Julesz’s textons (Julesz, 1981)—
orientation elements, crossings and terminators—fell

Representing and Recognizing the Visual Appearance
31
Figure 3.
(a) A ridge: under one illumination, the ridge appears as a light-dark transition, while it appears as a dark-light transition under
another. (b) Shadows: under one illumination, two regions have the same brightness, while under another, the brightness is different.
into disuse as they did not have a precise deﬁnition
for gray level images. In this paper, we re-invent
the concept and operationalize it in terms of learned
co-occurences of outputs of linear oriented Gaussian
derivative ﬁlters. In the case of 3D textons, we look
at the concatenation of ﬁlter response vectors corre-
sponding to different lighting and viewing directions.
Once we have built such a universal vocabulary of
3D textons, the surface of any material such as marble,
concrete, leather, or rug can be represented as a spatial
arrangement (perhaps stochastic) of symbols from this
vocabulary. Only a small number of views are needed
for this. Suppose we have learned these representations
for some materials, and then we are presented with a
single image of a patch from one of these materials
under a novel illumination or viewpoint, the objective
is to recognize which one. We have developed a recog-
nition algorithm using a Markov Chain Monte Carlo
(MCMC) sampling method.
The structure of this paper is as follows. In Section 2,
we show an operationalization of ﬁnding 2D textons
from images. We analyze images of different view-
ing and lighting conditions together and extend the
notion of textons to 3D textons in Section 3. The algo-
rithm for computing a 3D texton vocabulary is given in
Section 4. How a material is represented in terms of the
learned textons is discussed in Section 5. We contrast
our 3D texton model with principle component analy-
sis in Section 6. The problem of 3D texture recognition
is presented in Section 7. Results are shown for classi-
fying materials under novel viewing and lighting con-
ditions. In Section 8, we present an application of the
3D texton vocabulary to predict the appearance of tex-
tures under novel viewing and lighting conditions. We
conclude in Section 9. Some of the results presented
here were published in Leung and Malik (1999).
2.
2D Textons
We will characterize a texture by its responses to a
set of orientation and spatial-frequency selective linear
ﬁlters (a ﬁlter bank). This approach has proved to be
useful for segmentation (Fogel and Sagi, 1989; Malik
and Perona, 1990), recognition (Puzicha et al., 1997;

32
Leung and Malik
Figure 4.
The ﬁlter bank used in our analysis. Total of 48 ﬁlters: 36 oriented ﬁlters, with 6 orientations, 3 scales, and 2 phases, 8 center-surround
derivative ﬁlters and 4 low-pass Gaussian ﬁlters.
Rubner and Tomasi, 1999), as well as synthesis (de
Bonet and Viola, 1998; Heeger and Bergen, 1995; Zhu
et al., 1998).
The representation of textures using ﬁlter responses
is extremely versatile, however, one might say that it
is overly redundant (each pixel is represented by Nf il
ﬁlter responses, where Nf il is usually around 50). More-
over, it should be noted that we are characterizing tex-
tures, entities with some spatially repeating properties
by deﬁnition. Therefore, we do not expect the ﬁlter re-
sponses to be totally different at each pixel over the
texture. Thus, there should be several distinct ﬁlter re-
sponse vectors and all others are noisy variations of
them.
This intuition leads to our proposal of clustering
the ﬁlter responses into a small set of prototype re-
sponse vectors. We call these prototypes textons. Al-
gorithmically, each texture is analyzed using the ﬁlter
bank shown in Fig. 4. There are a total of 48 ﬁlters
(36 elongated ﬁlters at 6 orientations, 3 scales, and
2 phases, 8 center-surround difference of Gaussian ﬁl-
ters, and 4 low-pass Gaussian ﬁlters.). Each pixel is
now transformed to a Nf il = 48 dimensional vector.
These vectors are clustered using a vector quantiza-
tion algorithm, in particular K-means (Ball and Hall,
1967; Duda and Hart, 1973; Gersho and Gray, 1992;
MacQueen, 1967; Ripley, 1996; Sebestyen, 1962). The
criterion for this algorithm is to ﬁnd K centers such
that after assigning each data vector to the nearest cen-
ter, the sum of the squared distance from the centers
are minimized. K-means is a greedy algorithm which
iteratively performs the following two operations:
(1) assign data vectors to the nearest of the K cen-
ters; (2) update each of the K centers to the mean of
the data vectors assigned to it. These two steps are con-
tinued until the algorithm converges and a local mini-
mum of the criterion is achieved. These centers are the
textons.Theassociatedﬁlterresponsevectorsarecalled
the appearance vectors, ck ∈RNf il, k = 1, . . . , K.
What the textons encode can be visualized by re-
constructing local image patches from the appearance
vectors ck. These local image patches, Pk, can be con-
sidered as ﬁlters detecting particular image structures.
The reconstruction task is trivial for orthogonal or
self-inverting ﬁlter banks (Burt and Adelson, 1983;
Heeger and Bergen, 1995; Vaidyanathan, 1993). For
non-orothogonal, and non-self-inverting ﬁlter banks,
reconstructing the image from ﬁlter responses can be
set up as a least-square problem. First construct the ﬁl-
ter matrix F as follows: arrange each ﬁlter as a row vec-
tor and concatenate them to form a matrix. In this repre-
sentation, convolution with the ﬁlter bank is equivalent
to multiplying each local image patch with F. Each
of the Pk can be obtained by multiplying the appear-
ance vector ck with the pseudo-inverse of F (Jones and
Malik, 1992). This is illustrated in Fig. 5. The origi-
nal image is shown in (a). The textons (K-means clus-
ter centers) are reconstructed and shown in (b). Notice
that they correspond to the dominant local structures in
the image. We can quantize the ﬁlter responses at each
pixel x of the image to the K textons. To reconstruct
the whole image from this array of ﬁlter responses,
we ﬁrst independently reconstruct the intensity at each
pixel from the quantized ﬁlter responses. Then, we it-
erate to make sure that the ﬁlter responses of the recon-
structed image agrees with the raw ﬁlter responses. The
result of this process for Fig. 5(a) is shown in (c). The
close resemblance between (a) and (c) suggests that the
quantization does not introduce much error perceptu-
ally and that the reconstruction algorithm is doing a
good job.
In the next section, we will extend the texton theory
to 3D textures—texture with signiﬁcant local surface
relief. For more discussions on 2D textons, the readers

Representing and Recognizing the Visual Appearance
33
Figure 5.
Illustration of K-means clustering and reconstruction from ﬁlter responses with K = 20. (a) Original image. (b) the K-means centers
reconstructed as local ﬁlters. These centers correspond to the dominant features in the image: bars and edges at various orientations and phases;
(c) Reconstruction of the quantized image. Close resemblance between (a) and (c) suggests that quantization does not introduce much error
perceptually.
are referred to Malik et al. (1999), where we applied the
idea of textons to the problem of image segmentation.
3.
3D Textons
For painted textures with Lambertian material, charac-
terizing one image is equivalent to characterizing all
the images under all lighting and viewing directions.
However, for 3D textures, this is not the case. The ef-
fects of masking, shadowing, specularity, and mutual
illumination will make the appearance of the texture
look drastically different according to the lighting and
viewing directions (Fig. 2). The presence of albedo
variations on a lot of natural textures only makes the
problem more difﬁcult.
Let us ﬁrst consider what the problems are if we try
to characterize a 3D texture with only 1 image using
the K-means clustering algorithm on ﬁlter outputs de-
scribed in Section 2. Suppose the image of the texture
consists of thin dark-light bars arising from 3 causes:
(1) albedo change; (2) shadows; and (3) a deep groove.
Despite the different underlying causes, all these events
produce the same appearance in this particular lighting
and viewing setting. Quite naturally, the K-means al-
gorithm will cluster them together. What this means is
that pixels with the same label will look different under
different lighting and viewing conditions: (1) the
albedo change varies according to the cosine of the
lighting angle (assuming a Lambertian surface); (2) the
location of the shadow boundary changes according to
the direction of the light; and (3) the deep groove re-
mains the same for a wide range of lighting and view-
ing conditions (Haddon and Forsyth, 1998; Koenderink
and van Doorn, 1980). Thus, we will pay a signiﬁcant
price for quantizing these events to the same texton.
To characterize 3D textures, many images at differ-
ent lighting and viewing directions will be needed. Let
the number of images be Nvl, with Nvl ≫1 (Nvl = 20
in our experiments). The argument is that if any two
local texture structures are equivalent under Nvl
different lighting and viewing conditions, we can safely
assume that the two structures will look the same under
all lighting and viewing conditions. Notice that work
in the literature have attempted to show that 3–6 im-
ages will be able to completely characterize a struc-
ture in all lighting and viewing conditions (Belhumeur
and Kriegman, 1998; Shashua, 1997). These results are
not applicable because of the very restrictive assump-
tions they made: Lambertian surface model and the ab-
sence of occlusion, shadows, mutual illumination, and
specularity. Indeed, deviations from these assumptions
are the deﬁning properties of most, if not all, natural
3D textures.
What this means is that the co-occurrence of ﬁlter
responses across different lighting and viewing con-
ditions speciﬁes the local geometric and photometric
properties of the surface. If we concatenate the ﬁlter
responses of the Nvl images together and cluster these
long Nf ilNvl data vectors, the resulting textons will en-
code the appearances of dominant features in the image
under all lighting and viewing conditions. Let us ﬁrst
understand what these textons correspond to. Consider
the following two geometric features: a groove and a
ridge. In one image, they may look the same, however,
at many lighting and viewing angles, their appearances
are going to differ considerably. With the ﬁlter response
vectors from all the images, we can tell the difference
between these two features. In other words, each of the
K-means centers encodes geometric features such as
ridges at particular orientations, bumps of certain sizes,

34
Leung and Malik
Figure 6.
Each image at different lighting and viewing directions is ﬁltered using the ﬁlter bank. The response vectors are concatenated together
to form data vectors of length Nf ilNvl. These data vectors are clustered using the K-means algorithm. The resulting centers are the 3D textons
and the associated ﬁlter response vectors are called the appearance vectors.
grooves of some width, etc.. Similarly, the K-means
centers will also encode albedo change vs. geometric
3D features, as well as reﬂectance properties (e.g. shiny
vs. dull). The appearances of different features and dif-
ferent materials at various lighting and viewing angles
are captured by the ﬁlter responses. Thus, we call these
K-means centers 3D textons, and the corresponding
Nf ilNvl ﬁlter response vectors, the appearance vectors.
A schematic diagram illustrating the steps of ﬁltering,
concatenating ﬁlter responses, and K-means clustering
is shown in Fig. 6.
4.
Constructing the Vocabulary of 3D Textons
Our goal in this paper is to use images from a set of
training materials to learn a vocabulary which can char-
acterize all natural materials. This is a realistic goal
because, as we have noted, the textons in the vocabu-
lary are going to encode the appearances of local geo-
metric and photometric features, e.g. grooves, ridges,
bumps, reﬂectance boundaries etc. All natural materi-
als are made up of these features. In this section, we
will describe the exact steps taken to construct this uni-
versal 3D texton vocabulary.
All the images used in this paper are taken from
the Columbia-Utrecht dataset (Dana et al., 1999)
(http://www.cs.columbia.edu/CAVE/curet/).
There are 60 different materials, each with 205 images
at different viewing and lighting angles.1 20 materi-
als are taken randomly as the training set. For each
material, 20 images of different lighting and viewing
directions are used to build the texton vocabulary. The
20 images for each material are registered using the
standard area-based sum-of-square-differences (SSD)
algorithm.
To compute the universal vocabulary, the following
steps are taken:
1. For each of the 20 training materials, the ﬁlter bank
is applied to each of the Nvl = 20 images under
different viewing and lighting conditions. The re-
sponse vectors at every pixel are concatenated to-
gether to form a Nf ilNvl vector.
2. For each of the 20 materials individually, the K-
means clustering algorithm is applied to the data
vectors. The number of centers, denoted by K, is
400. The K-means algorithm ﬁnds a local minimum
of the following sum-of-square distance error:
Err =
N

i=1
K

k=1
qik∥xi −ck∥2
where
qik = 1
if ∥xi −ck∥2 < ∥xi −c j∥2
∀j = 1, . . . , K and j ̸= k
qik = 0
otherwise

Representing and Recognizing the Visual Appearance
35
N denotes the number of pixels; xi is the concate-
nated ﬁlter response vector of the ith pixel and ck
is the appearance vector for the kth center. The K-
means algorithm is initialized by random samples
from all the data vectors.
3. The centers for all the materials are merged to-
gether to produce a universal alphabet of size
K = 8000.
4. The codebook is pruned down to K = 100 by merg-
ing centers too close together or getting rid of those
centers with too few data assigned to them.
5. The K-means algorithm is applied again on samples
from all the images to achieve a local minimum.
Steps 2 to 4 can be viewed as ﬁnding an initialization
for the ﬁnal K-means step in 5.
The learned vocabulary should possess two very im-
portant properties:
1. Expressiveness: the vocabulary learned should be
able to characterize each of the materials well to
allow for the discrimination between them.
2. Generalization: it should generalize well beyond the
training materials. In other words, it should be as ex-
pressive for novel materials as for training materials.
An evaluation of these two properties is shown in
Fig. 7. For each material, the ﬁlter responses from a
frontal-parallel image of each material is quantized
into the 3D texton vocabulary—ﬁlter responses at each
pixel are replaced by the appearance vector of the 3D
texton labeled at the pixel. An image is reconstructed
from the quantized ﬁlter response vectors using the
algorithm described in Section 2. The SSD error be-
tween the reconstructed image and the original image
is plotted in the ﬁgure.2 The errors from vocabular-
ies of different sizes are also plotted for comparison.
The upper diagram is the SSD error for the training
materials. The lower diagram is the one for novel ma-
terials. Notice two points: (1) there is no signiﬁcant dif-
ference in average reconstruction error between train-
ing materials and novel materials. In other words, our
texton vocabulary is encoding generic features, rather
than material-speciﬁc properties. This is an indication
of good generalization. (2) The SSD errors are small
for almost all materials.3 The 3D texton vocabulary
is doing a very good job encoding the properties of
the materials. This reconﬁrms our intuition that tex-
tures are made of a small set of features. Moreover, the
differences between reconstruction errors from vocab-
ularies of different sizes are not signiﬁcant. In all the
texture recognition results in this paper, the same texton
vocabulary of size 100 is used. Of course, comparing
reconstruction error is not the best way to evaluate the
vocabulary. The real test is to use the vocabulary for the
recognition and synthesis of natural materials, which
we will show in Sections 7 and 8.
In our studies here, only 20 (Nvl = 20) different
viewing and lighting directions are used. 20 images
form a very sparse sampling of the viewing and il-
lumination spheres. When more images are available,
we should take advantage of them. However, this does
not mean that we need to run the clustering algorithm
on a formidably large dimensional space. We argue
that 20 images are enough to make sure that each 3D
textonrepresentsdifferentlocalgeometric/photometric
structures. Therefore, to enlarge the appearance vector
of each texton, we can simply append to the vectors
the average of ﬁlter responses at pixels with the corre-
sponding label.
5.
Representing Visual Appearance of Materials
using 3D Textons
Once we have built such a vocabulary of 3D textons,
we can acquire a model for each material to be classi-
ﬁed. Using all the images (under different viewing and
lighting conditions) available for each material, each
point on the surface is assigned one of the 100 tex-
ton labels by ﬁnding the minimum distance between
the texton appearance vectors to the ﬁlter responses
at the point. The surface of any material such as mar-
ble, concrete, leather, or rug can now be represented
as a spatial arrangement of symbols from this vocabu-
lary. For the problem of material recognition, we ignore
the precise spatial relationship of the symbols and use
a histogram representation for each material. Sample
histograms for 4 materials are shown in Fig. 8. Notice
that these histograms are very different from each other,
thus allowing good discrimination. The chi-square sig-
niﬁcance test is used to provide a measure between the
similarity of two histograms (h1 and h2):
χ2(h1, h2) = 1
2
#bins

n=1
(h1(n) −h2(n))2
h1(n) + h2(n)
(1)
The signiﬁcance for a certain chi-square distance is
given by the chi-square probability function: P(χ2 | ν).
P(χ2 | ν) is the probability that two histograms from
the same model will have a distance larger than χ2 by

36
Leung and Malik
Figure 7.
SSD reconstruction error for different materials. Top: the 20 training materials used to create the texton vocabulary. Bottom: 20 novel
materials. Several vocabularies of different sizes are created: “◦” for K = 800; “∗” for K = 400; “×” for K = 200; and “+” for K = 100.
Notice two points about the texton vocabulary: (1) there is no signiﬁcant difference in average reconstruction error between training and novel
materials—good generalization; (2) SSD errors are small for almost all materials—high descriptive power.
chance; and ν = #bins −1. P(χ2 | ν) is given by the
incomplete gamma function (Press et al., 1988):
P(χ2 | ν) = Q(ν/2, χ2/2)
and
(2)
Q(a, x) =
1
(a)
 x
0
e−tta−1 dt
where (a) is the gamma function.
6.
Textons versus Principal Component Analysis
The texton representation can be considered as a
form of data compression. It is not the only way
for compressing data. Principal component analysis
(PCA) is one of the most common ones. PCA is in
fact a very popular approach for object and texture

Representing and Recognizing the Visual Appearance
37
Figure 8.
Top to bottom: the histograms of labels for the materials: “Rough Plastic”, “Plaster-a”, “Pebbles,” and “Terrycloth” respectively.
These histograms are used as the material representation for the task of texture recognition. The histograms are very different from each other,
thus allowing good discrimination.
recognition (Belhumeur and Kriegman, 1998; Dana
and Nayar,1999a; Georghiades et al., 1998; Murase
and Nayar, 1995; Sirovitch and Kirby, 1987; Turk and
Pentland, 1991). In PCA approaches, each image is
represented as an element in RK, a linear subspace
of the pixel space: RN (N is the number of pixels and
K ≪N). Object models are represented by a collection
of example training images. In other words, each object
model is a subset of RK. Classiﬁcation is done using
nearest neighbor, or other more sophisticated classi-
ﬁers, like support vector machines.
The main problem of applying PCA to represent 3D
texture is that PCA is intrinsically linear. The major ef-
fects caused by the surface relief of natural materials—
shadows, occlusion, specularities, mutual illumination,
. . . etc, are non-linear properties. Moreover, PCA is ap-
plicable only if the surface reﬂectance is Lambertian.
However, most, if not all, materials are highly non-
Lambertian. Because of all these, we argue that the
texton representation, which is not based on any lin-
earity assumption, is more appropriate.
7.
Texture Recognition
In this section, we will demonstrate algorithms and
results on texture recognition.
7.1.
3D Texture Recognition from Multiple
Viewpoint/Lighting Images
We ﬁrst investigate 3D texture recognition when multi-
ple images of each sample are given. Every time we get
a sample of the material, 20 images of different light-
ing and viewing directions are provided. From these
images, a texton labeling is computed. Then the sam-
ple is classiﬁed to be the material with the smallest

38
Leung and Malik
chi-square distance between the sample histogram and
the model histogram. In this experiment, 20 training
materials are used to construct the texton vocabulary of
size 100. 40 different materials are to be classiﬁed. The
models are obtained from random 100 × 100 patches
from the images. For each material, 3 novel samples of
size 100 × 100 are to be classiﬁed. The overall recog-
nition rate is 95.6%.4
Another way to demonstrate the result is to use the
similarity matrix in Fig. 9. Each element in the ma-
trix ei j is given by the chi-square probability function
Figure 9.
Similarity matrix for 14 materials. Each entry eij is given by the chi-square probability function (Eq. (2)) that samples of material j
will be classiﬁed as material i. As shown in this ﬁgure, for example, “Leather” and “Rough Plastic” are likely to be classiﬁed correctly; while
“Plaster-a” and “Plaster-b” are likely to be mistaken between them. Sample images from these four materials are shown as well.
(Eq. (2)) that samples of material j will be classiﬁed
as material i. Here, we only show the probability for
14 materials because of space limitations. As shown in
the ﬁgure, for example, “Leather” and “Rough Plastic”
are likely to be classiﬁed correctly; while “Plaster-a”
and “Plaster-b” are likely to be mistaken between each
other. This is reasonable because the two different types
of plaster indeed look very similar, as shown from the
images in the bottom of the ﬁgure.
“Receiver Operation Characteristics” (ROC) curves
are also good indications of the preformance. The ROC

Representing and Recognizing the Visual Appearance
39
Figure 10.
Receiver operation characteristics (ROC) curve for a
very simple texture recognition problem. The top-left corner repre-
sents perfect recognition performance. The diagonal line refers to
chance. The performance for our algorithm is very good. The recog-
nition achieves a 97% detection rate with only a 3% false alarm rate.
curve is a plot of the probability of detection versus the
probability of false alarms. It is parametrized by a de-
tection threshold. In our case, it is a threshold τ on the
chi-square distance. For any incoming sample, we de-
clare that it is the same as material n if the chi-square
distance between their histograms is smaller than τ.
If the sample is indeed material n, we have a detec-
tion, otherwise, it is a false alarm.5 Figure 10 shows
the ROC curve for our recognition problem. The top-
left corner represents perfect recognition. Our algo-
rithm performs very well. The recognition performance
achieve a 97% detection rate with only a 3% false alarm
rate.
The requirement that any material is presented with
multipleimagesatdifferentlightingandviewingcondi-
tions may seem unreasonable. However, if the material
is on a curved surface, it is essentially equivalent to hav-
ing multiple images of the same material illuminated
and viewed differently.
7.2.
3D Texture Recognition from a Single Image
Let us now consider the much more difﬁcult problem of
3Dtexturerecognition:foreachmaterial,thehistogram
model is built from 4 different light/view conditions;
and for each sample to be classiﬁed, we only have a
single image under any known illumination condition
and viewing geometry. This problem is very similar to
theproblemformulationofobjectrecognition—givena
small number of instances of the object, try to recognize
it under all poses and illumination.6 However, in the
context of texture recognition, this problem is rarely
studied.
Given the illumation and viewing conditions for the
novel image, we know to which portion of the appear-
ance vector the ﬁlter outputs of the incoming image is
to be compared. However, a problem arises from the
fact that given only 1 image, ﬁnding the texton label
for each pixel is very difﬁcult. As noted before, in just
one single viewing and lighting condition, physically
different features may have the same appearance. Thus,
texton assignment to the pixels is ambiguous. Simply
commiting to the label with the smallest distance can
result in a texton histogram that has no resemblance to
that of the target material.
The intuition of our approach is the following:
if the texton labeling of the incoming image is known,
the material identity can be assigned to the model with
the minimum chi-square distance between the incom-
ing texton histogram and the histogram of the model
material. On the other hand, if the material identity is
known, a texton labeling of the image can be estimated
by matching the histograms of the labeling to that of
the material. We solve this chicken-and-egg problem
using a Markov chain Monte Carlo (MCMC) algo-
rithm. First, each pixel i is allowed Ni possible texton
labelings. The MCMC algorithm will try to ﬁnd the
best labelling given the possibilities and the material
type.
An MCMC algorithm with metropolis sampling for
ﬁnding texton labelling is shown below. For each ma-
terial n and the corresponding model histogram hn, do:
1. Randomly assign a label to each pixel i among the
Ni possibilities. Call this assignment the initial state
x(t) with t = 0;
2. Compute the probability of the current state P(x(t))
using Eq. (2) with hn as the model histogram;
3. Obtain a tentative new state x′ by randomly chang-
ing M labels of the current state;
4. Compute P(x′) using Eq. (2);
5. Compute α =
P(x′)
P(x(t));
6. If α ≥1, the new state is accepted, otherwise, accept
the new state with probability α;
7. Goto step 2 until the states converge to a stable dis-
tribution.

40
Leung and Malik
Figure 11.
The decay of the χ2 distance between the histogram of the state x(t) and the histogram of a model material. Solid line: correct
material. Dashed line: wrong material. The decay of the distance is much faster and the minimum much smaller for the correct material.
What the MCMC algorithm does is to draw samples
from the following distribution: P(labelling|material
n) or P(x | hn) where x is in the space of possible la-
bellings. P(x | hn) is given by the chi-square probabil-
ity function in Eq. (2). Once the states settle in a stable
distribution, we can compute the probability that the
incoming image sample is drawn from material n by
computing maxt P(x(t) | hn).
MCMC algorithms have been applied to computer
vision for a long time, most well-known in the paper by
Geman and Geman (Geman and Geman, 1984), where
the problem of image restoration is studied. For de-
tails about variations in MCMC algorithms, conver-
gence properties, and methods to speed up conver-
gence, please consult (Gilks et al., 1996).
In our experiments, each pixel is allowed to have 5
possible labels, chosen from the closest 5 textons. In
other words, Ni = 5 ∀i. For each iteration, we are
allowed to change the labels of 5% of the pixels (M
in step 3).7 Figure 11 shows typical behavior of the
MCMC algorithm. The solid line is the decay of the
χ2 distance between the histogram of the state x(t) and
hn where material n is the correct material while the
dashed line is that for a wrong material.
The recognition performance is shown in the ROC
curves in Fig. 12. (These ROC curves are obtained the
same way as the one in Section 7.1.) The 5 different
Figure 12.
Texture recognition under novel lighting and viewing
conditions. The 5 different curves represent 5 randomly chosen novel
viewing and lighting directions for the samples to be classiﬁed. Each
curve is the average performance for 40 materials. The model his-
togram for each material is obtained using images from 4 different
view/light settings. The performance of our algorithm is excellent—a
87% detection rate with 13% false alarm.

Representing and Recognizing the Visual Appearance
41
Figure 13.
Texture synthesis of training materials used to create the 3D texton vocabulary. The materials are “Plaster-a” for the ﬁrst two
rows and “Concrete-c” for the last two. First column: texture mapping; middle column: ground truth; last column: synthesized results. Texture
mapping produces images that look “ﬂat”, while our algorithm correctly captures the highlights, shadows, and occlusions.
curves correspond to 5 randomly chosen novel viewing
and lighting directions for the samples to be classiﬁed.
The curves are showing the average performance for
40 materials. The model histogram for each material
is obtained using images from 4 different view/light
settings. The top-left corner of the plot stands for per-
fect performance. Given the difﬁculty of the task, the
performance of our algorithm is very good. The algo-
rithm achieves a 87% detection rate with a 13% false
alarm rate. One interesting comparison to make will be
to contrast the performance of our algorithm with that
of a human.
8.
Novel View/Light Prediction
The universal 3D texton vocabulary can also be used
to predict the appearance of materials at novel viewing
and lighting conditions. This application is of primary
interest in computer graphics.

42
Leung and Malik
Figure 14.
Predicting appearance of novel materials at various lighting and viewing conditions. The materials are “Plaster-a” for the ﬁrst two
rows and “Crumpled Paper” for the last two. First column: traditional texture mapping; middle column: ground truth; last column: results using
texton vocabulary. Our algorithm correctly ´captures the highlights, shadows and occlusions while traditional texture mapping produces images
that look “ﬂat”.
Suppose we are given n images of a novel texture
taken at n different known illumination and viewing
directions. We compute the ﬁlter responses of the im-
ages and concatenate them to form a nNf il data vector.
These data vector can be labeled to one of the K el-
ements in the texton vocabulary by matching to the
corresponding sections of the Nvl Nf il dimensional ap-
pearance vectors. In other words, each pixel in the input
texture is labelled to one of the K 3D textons. Recall
that the appearance vectors of the 3D textons encode
precisely how each texton changes its appearance when
illumination or viewing directions are changed. There-
fore, we can predict exactly how the image is trans-
formed under a novel illumination direction or viewing
geometry.
Results for novel view/light prediction are shown in
Figs. 13 and 14. In these examples, 4 images of the
material under different light/view arrangements are
given. We then predict the appearance of the material at
other lighting and viewing conditions using the texton

Representing and Recognizing the Visual Appearance
43
vocabulary.8 The novel lighting and viewing conﬁgu-
rations are up to 30◦away from the 4 example condi-
tions. The results shown in Fig. 13 are for training ma-
terials (those used to compute the texton vocabulary).
Figure 14 shows the results for novel materials. The
ﬁrst columns show images obtained using traditional
texture mapping from a frontal parallel image; middle
columns show the ground truth and the third columns
display our results. Because traditional texture map-
ping assumes the surface is painted and Lambertian,
it produces images that look “ﬂat”. Our method, on
the other hand, correctly captures the 3D nature of the
surface—highlights, shadows, and occlusions.
9.
Discussion
In this paper, we have presented a framework for repre-
sentingtexturesmadeupofbothreﬂectanceandsurface
normal variations. The basic idea is to build a universal
texton vocabulary that decribes generic local features
of texture surfaces. Using the texton vocabulary and
an MCMC algorithm, we have demonstrated excellent
results for recognizing 3D textures from a single im-
age under any lighting and viewing directions. We also
demonstrated how our model can be used to predict the
appearance of natural materials under novel illumina-
tion condition and viewing geometry.
The current work can be combined with a texture
synthesis algorithm to generate new samples of mate-
rials under all viewing and illumination conditions. The
algorithm proposed in Efros and Leung (1999) is par-
ticularly promising. The basic idea of Efros and Leung
(1999) is to synthesize texture by growing pixels. The
pixel value to be grown is obtained by sampling from
an example texture image. In our 3D texton model, we
can grow a array of textons instead of pixel values.
Given the array of textons, we can synthesize an image
at any illumination and viewing condition. This is done
by picking a section of the appearance vectors of the
3D textons and reconstructing an image from them.
Acknowledgment
The authors would like to thank the Berkeley vision
group, especially, Serge Belongie, Chad Carson,
Alyosha Efros, David Forsyth, Jianbo Shi, and Yair
Weiss for useful discussions. This research was sup-
ported by (ARO) DAAH04-96-1-0341, the Digital
Library Grant IRI-9411334, and a Berkeley Fellowship
to TL.
Notes
1. More images if the material is anisotropic.
2. We recognize that the SSD error is by no means perceptually
correct, but it is a convenient way of comparing two images.
3. Error is large for aluminum, which is very specular.
4. Recognition rate is 95.0% for training materials (those used to
create the texton vocabulary) and 96.3% for novel materials.
There is no signiﬁcant difference between the performance for
the training materials and that of the novel materials in all our
experiments. Therefore, we will report only the overall recog-
nition performance. The main reason for this indifference in
performance is that the texton vocabulary attains good generaliza-
tion, thus is encoding generic local features, rather than retaining
material-speciﬁc information.
5. This is equivalent to making 40 2-class decisions. For example,
if the threshold τ is too small, we will have 1 detection and 39
false positives. On the other hand, if τ is too large, we will have
0 detection and 0 false positive.
6. However, most object recognition algorithms require a large num-
ber of training examples.
7. A cooling schedule can deﬁnitely be employed here. At ﬁrst,
more sites are allowed to change to speed up the exploration of
the space. When the distribution is close to convergence, fewer
sites are allowed to alter to “ﬁne-tune” the distribution.
8. In these results, to achieve best quality and to reduce quantization
error, a texton vocabulary of size 2000 is computed.
References
Ball, G. and Hall, D. 1967. A clustering technique for summarizing
multi-variate data. Behavioral Science, 12:153–155.
Belhumeur, P. and Kriegman, D. 1998. What is the set of images of an
object under all possible illumination conditions?. International
Journal of Computer Vision, 28(3):245–260.
Burt, P. and Adelson, E. 1983. The laplacian pyramid as a compact
image code. IEEE Transactions on Communications, 31(4):532–
540.
Chantler, M. 1994. Towards illuminant invariant texture classiﬁca-
tion. In Proc. IEE Coll. on Texture Classiﬁcation: Theory and
Applications.
Chantler, M. and McGunnigle, G. 1995. Compensation of illuminant
tilt variation for texture classiﬁcation. In Proceedings Fifth Inter-
national Conference on Image Processing and its Applications,
pp. 767–771.
Chellappa,R.andChatterjee,S.1985.Classiﬁcationoftexturesusing
Gaussian Markov random ﬁelds. IEEE Transactions on Acoustics,
Speech, Signal Processing, 33(4):959–963.
Cross, G. and Jain, A. 1983. Markov random ﬁeld texture models.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
5(1):25–39.
Dana, K. and Nayar, S. 1998. Histogram model for 3D textures.
In Proceedings IEEE Conference on Computer Vision and Pattern
Recognition, Santa Barbara, CA, pp. 618–624.
Dana, K. and Nayar, S. 1999a. 3D textured surface modelling.
In Proceedings Workshop on the Integration of Appearance and
Geometric Methods in Object Recognition, pp. 46–56.
Dana, K. and Nayar, S. 1999b. Correlation model for 3D texture.
In Proceedings IEEE 7th International Conference on Computer
Vision, Vol. 2. Corfu, Greece, pp. 1061–1066.

44
Leung and Malik
Dana, K., van Ginneken, B., Nayar, S., and Koenderink, J. 1999.
Reﬂectance and texture of real-world surfaces. ACM Transactions
on Graphics, 18(1):1–34.
de Bonet, J. and Viola, P. 1998. Texture recognition using a non-
parametric multi-scale statistical model. In Proceedings IEEE
Conference on Computer Vision and Pattern Recognition, Santa
Barbara, CA, pp. 641–647.
Duda, R. and Hart, P. 1973. Pattern Classiﬁcation and Scene Analy-
sis, John Wiley & Sons. New York, N.Y.
Efros, A. and Leung, T. 1999. Texture synthesis by non-parametric
sampling. In Proceedings IEEE 7th International Conference on
Computer Vision, Vol. 2. Corfu, Greece, pp. 1033–1038.
Fogel, I. and Sagi, D. 1989. Gabor ﬁlters as texture discriminator.
Biological Cybernetics, 61:103–113.
Geman, S. and Geman, D. 1984. Stochastic relaxation, Gibbs distri-
butions, and the Bayesian restoration of images. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 6:721–741.
Georghiades, A., Kriegman, D., and Belhumeur, P. 1998. Illu-
mination cones for recognition under variable lightin: Faces.
In Proceedings IEEE Conference on Computer Vision and Pat-
tern Recognition, Santa Barbara, CA, pp. 52–58.
Gersho, A. and Gray, R. 1992. Vector Quantization and Signal Com-
pression, Kluwer Academic Publishers: Boston, MA.
Gilks, W., Richardson, S., and Spiegelhalter, D. 1996. Markov Chain
Monte Carlo in Practice, Chapman and Hall.
Haddon, J. and Forsyth, D. 1998. Shading primitives: Finding folds
and shallow grooves. In Proceedings IEEE 6th International Con-
ference on Computer Vision, Bombay, India, pp. 236–241.
Heeger, D. and Bergen, J. 1995. Pyramid-based texture analy-
sis/synthesis. In Computer Graphics (SIGGRAPH ’95 Proceed-
ings), Los Angeles, CA, pp. 229–238.
Jain, A. and Farrokhsia, F. 1991. Unsupervised texture segmentation
using Gabor ﬁlters. Pattern Recognition, 24:1167–1186.
Jones, D. and Malik, J. 1992. Computational framework to deter-
mining stereo correspondence from a set of linear spatial ﬁlters.
Image and Vision Computing, 10(10):699–708.
Julesz, B. 1981. Textons, the elements of texture perception, and their
interactions. Nature, 290(5802):91–97.
Koenderink, J. and van Doorn, A. 1980. Photometric invariants re-
lated to solid shape. Optica Acta, 27(7):981–996.
Koenderink, J. and van Doorn, A. 1996. Illuminance texture due to
surface mesostructure. Journal of the Optical Society America A,
13(3):452–463.
Koenderink, J., van Doorn, A. Dana, K. and Nayar, S. 1999. Bidirec-
tionalreﬂectiondistributionfunctionofthoroughlypittedsurfaces.
International Journal of Computer Vision, 31(2/3):129–144.
Leung,T.andMalik,J.1997.Onperpendiculartextureor:Whydowe
seemoreﬂowersinthedistance?.InProceedingsIEEEConference
on Computer Vision and Pattern Recognition, San Juan, Puerto
Rico, pp. 807–813.
Leung, T. and Malik, J. 1999. Recognizing surfaces using three di-
mensional textons. In Proc. IEEE International Conference on
Computer Vision, Corfu, Greece.
MacQueen, J. 1967. Some methods for classiﬁcation and analysis of
multivariate observations. In Proc. Fifth Berkeley Symposium on
Math. Stat. and Prob., Vol. I. pp. 281–297.
Malik, J., Belongie, S., Shi, J., and Leung, T. 1999. Textons, con-
tours and regions: Cue integration in image segmentation. In Pro-
ceedings IEEE 7th International Conference on Computer Vision,
Corfu, Greece, pp. 918–925.
Malik, J. and Perona, P. 1990. Preattentive texture discrimination
with early vision mechanisms. Journal of the Optical Society of
America A, 7(5):923–932.
Mao, J. and Jain, A. 1992. Texture classiﬁcation and segmentation us-
ing multiresolution simultaneous autoregressive models. Pattern
Recognition, 25(2):173–188.
Murase, H. and Nayar, S. 1995. Visual learning and recognition of
3-D objects from appearance. International Journal on Computer
Vision, 14(1):5–24.
Press, W., Flannery, B., Teukolsky, S., and Vetterling, W. 1988.
Numerical Recipes in C, Cambridge University Press.
Puzicha, J., Hofmann, T., and Buhmann, J. 1997. Non-parametric
similarity measures for unsupervised texture segmentation and
image retrieval. In Proceedings IEEE Conference on Computer
Vision and Pattern Recognition, San Juan, Puerto Rico, pp. 267–
272.
Ripley, B. 1996. Pattern Recognition and Neural Networks,
Cambridge University Press.
Rubner, Y. and Tomasi, C. 1999. Texture-based image retrieval with-
out segmentation. In Proceedings IEEE 7th International Confer-
ence on Computer Vision, Vol. 2. Corfu, Greece, pp. 1018–1024.
Sebestyen, G. 1962. Pattern recognition by an adaptive process of
sample set construction. IRE Trans. Info. Theory, 8:S82–S91.
Shashua, A. 1997. On photometric issues in 3D visual recogni-
tion from a single 2D image. International Journal on Computer
Vision, 21(1/2).
Sirovitch, L. and Kirby, M. 1987. Low-dimensional procedure for the
characterization of human faces. Journal of the Optical Society of
America A, 2:519–524.
Turk, M. and Pentland, A. 1991. Eigenfaces for recognition. Journal
of Cognitive Neuroscience, 3(1):71–86.
Vaidyanathan, P. 1993. Multirate Systems and Filter Banks, Prentice-
Hall: Englewood Cliffs, N.J.
van Ginneken, B., Stavridi, M., and Koenderink, J. 1998. Diffuse
and specular reﬂectance from rough surfaces. Applied Optics,
37(1):130–139.
Yuan, J. and Rao, S. 1993. Spectral estimation for random ﬁelds
with applications to Markov modeling and texture classiﬁcation.
In Markov Random Fields: Theory and Application, R. Chellappa
and A. Jain (Eds.). Academic Press.
Zhu, S., Wu, Y., and Mumford, D. 1998. Filters, random ﬁelds and
maximum entropy (FRAME): Towards a uniﬁed theory for texture
modeling. International Journal of Computer Vision, 27(2):107–
126.


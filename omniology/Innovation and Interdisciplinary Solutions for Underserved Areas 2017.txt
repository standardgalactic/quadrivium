123
Cheikh M. F. Kebe
Assane Gueye
Ababacar Ndiaye (Eds.)
Innovation and 
Interdisciplinary 
Solutions for 
Underserved Areas
First International Conference, InterSol 2017
and Sixth Collogue National sur la Recherche en
Informatique et ses Applications, CNRIA 2017
Dakar, Senegal, April 11–12, 2017
Proceedings
204

Lecture Notes of the Institute
for Computer Sciences, Social Informatics
and Telecommunications Engineering
204
Editorial Board
Ozgur Akan
Middle East Technical University, Ankara, Turkey
Paolo Bellavista
University of Bologna, Bologna, Italy
Jiannong Cao
Hong Kong Polytechnic University, Hong Kong, Hong Kong
Geoffrey Coulson
Lancaster University, Lancaster, UK
Falko Dressler
University of Erlangen, Erlangen, Germany
Domenico Ferrari
Università Cattolica Piacenza, Piacenza, Italy
Mario Gerla
UCLA, Los Angeles, USA
Hisashi Kobayashi
Princeton University, Princeton, USA
Sergio Palazzo
University of Catania, Catania, Italy
Sartaj Sahni
University of Florida, Florida, USA
Xuemin Sherman Shen
University of Waterloo, Waterloo, Canada
Mircea Stan
University of Virginia, Charlottesville, USA
Jia Xiaohua
City University of Hong Kong, Kowloon, Hong Kong
Albert Y. Zomaya
University of Sydney, Sydney, Australia

More information about this series at http://www.springer.com/series/8197

Cheikh M. F. Kebe
• Assane Gueye
Ababacar Ndiaye (Eds.)
Innovation and
Interdisciplinary
Solutions for
Underserved Areas
First International Conference, InterSol 2017
and Sixth Collogue National sur la Recherche en
Informatique et ses Applications, CNRIA 2017
Dakar, Senegal, April 11–12, 2017
Proceedings
123

Editors
Cheikh M. F. Kebe
Université Cheikh Anta Diop
Dakar
Sénégal
Assane Gueye
University of Maryland, College Park
College Park
USA
and
Université Alioune Diop de Bambey
Bambey
Senegal
Ababacar Ndiaye
Université Assane Seck de Ziguinchor
Ziguinchor
Sénégal
ISSN 1867-8211
ISSN 1867-822X
(electronic)
Lecture Notes of the Institute for Computer Sciences, Social Informatics
and Telecommunications Engineering
ISBN 978-3-319-72964-0
ISBN 978-3-319-72965-7
(eBook)
https://doi.org/10.1007/978-3-319-72965-7
Library of Congress Control Number: 2017962880
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Societies living in underserved areas face multifactorial problems — energy, water,
climate, food, health, education, transportation, social development, economic growth,
etc. — that are not amenable to separate single-discipline investigation, but require
collaboration between many types of expertise. For instance, let us consider the issue of
health in rural areas. Because of the shortage of doctors and medical workers in these
areas, a solution to this problem that is widely agreed upon is telemedicine. However,
telemedicine in rural areas requires suitable connectivity solutions that can be powered
using renewable energy. The solution also has to be economically and socially viable.
In fact, any sustainable solution to the problems facing people in underserved areas
must involve expertise from different disciplines, hence the relevance and the important
of “interdisciplinarity.”
This edited volume contains the 35 contributions presented at the inaugural Con-
ference on Innovation and Interdisciplinary Solutions for Underserved Areas (InterSol
2017) held in Dakar-Senegal, April 11–12, 2017. These include 17 papers for the main
track (nine long papers and eight short papers) and ﬁve papers on the “Special Session
on Development Innovation Learning and Innovation in Learning.” InterSol 2017 was
also co-located with the 6th CNRIA Workshop (Collogue National sur la Recherche en
Informatique et ses Applications), during which 13 papers presented.
The 17 papers in the main track are thematically categorized into the following six
sessions:
1. “Fundamental Sciences for Energy and Environment” contains two articles that use
mathematical models to study solar radiation and water treatment process.
2. “Innovations in Economic Development” has two articles that study how the uti-
lization of technologies has economically impacted people in underserved areas.
3. “Education” includes four papers on innovation and the usage of distance learning
tools to enable education in remote and underserved areas.
4. “Environment and Energy” contains two articles studying the potentials of wind and
solar energy in sub-Saharan Africa.
5. “Information and Communication Technologies as an Enabler” has four papers
presenting how ICT has enabled several sectors in Africa from banking to trans-
portation and electricity payment.
6. “Health Care” includes three articles that study applications of new technologies to
facilitate doctor–patient relationships.
The ﬁve papers in the special session investigate the best learning technologies to
support development innovation diffusion in various sectors: from policy circles to end
users such as farmers through to higher education and research.
The CNRIA workshop is a highly interactive event that enables an effective inter-
change of results and ideas among researchers, industry, and product developers. The
13 papers presented during the 2017 gathering address issues in several areas in

networking and computer sciences, including: network architecture and security,
software engineering, data management, and signal processing.
InterSol 2017 witnessed a massive participation from the research community,
papers that address issues related to underserved areas, and extensive exchanges
between researchers, beneﬁciaries, and industrial partners. A major innovation of
InterSol 2017 was the organization of a round table bringing together researchers and
end users to discuss interdisciplinary solutions for underserved areas.
Convinced with the essential role that interdisciplinary research and collaboration
should play in addressing issues in underserved areas, we believe that this edited
volume as well as the InterSol conference will be of great interest to researchers and
practitioners working on solutions for underserved areas.
November 2017
Cheikh M. F. Kebe
Assane Gueye
Ababacar Ndiaye
VI
Preface

Organization
Steering Committee
Steering Committee Chair
Imrich Chlamtac
CREATE-NET and University of Trento, Italy
Steering Committee
Cheikh M. F. Kebe
Université Cheikh Anta Diop de Dakar; Ecole
Supérieure Polytechnique, Senegal
Assane Gueye
University of Maryland College Park, USA;
Université Alioune Diop, Bambey, Senegal
Organizing Committee
General Chair
Cheikh M. F. Kebe
Université Cheikh Anta Diop de Dakar; Ecole
Supérieure Polytechnique, Senegal
General Co-chair
Assane Gueye
University of Maryland College Park, USA;
Université Alioune Diop de Bambey, Senegal
Technical Program Committee Co-chairs
Austin Ezecurra
Université de Victoria, Canada
Mohamed Mejri
Université Laval, Quebec, Canada
Noble Banadda
Makarere University, Uganda
Workshops Chair
Nour Abdoul
Université Quebec Trois Rivieres, Canada
Publicity and Social Media Chair
Ilincan Adrian
Université du Quebec à Rimouski, Canada
Sponsorship and Exhibits Chairs
Moussa Lo
Université Gaston Berger de Saint Louis, Senegal
Amadou Seidou Maiga
Université Gaston Berger de Saint Louis, Senegal

Publications Chairs
Cheikh M. F. Kebe
Université Cheikh Anta Diop de Dakar; Ecole
Supérieure Polytechnique, Senegal
Assane Gueye
University of Maryland College Park, USA;
Université Alioune Diop de Bambey, Senegal
Ababacar Ndiaye
Université Assane Seck de Ziguinchor, Senegal
Panels Chair
Bamba Gueye
Université Cheikh Anta Diop de Dakar, Senegal
Posters and PhD Track Chairs
Senghane Mbodji
Université Alioune Diop de Bambey, Senegal
Alassane Diop
Université Alioune Diop de Bambey, Senegal
Local Chair
Ababacar Ndiaye
Université Assane Seck de Ziguinchor, Senegal
Web Chairs
Marie-Héléne Mballo
Université Alioune Diop de Bambey, Senegal
Amy Séne
Université Alioune Diop de Bambey, Senegal
Conference Manager
Lenka Laukova
European Alliance for Innovation
Technical Program Committee
Cheikh M. F. Kebe
Université de Dakar; Ecole Supérieure Polytechnique,
Senegal
Ababacar Ndiaye
Université de Ziguinchor, Senegal
Senghane Mbodji
Université Alioune Diop de Bambey, Senegal
Adrian Ilincan
Université du Quebec à Rimouski, Canada
Abdoul Nour
Université Quebec Trois Rivieres, Canada
Natewinde Sawadogo
University of Ouagadougou, Burkina Faso
Austin Ezecurra
Université de Victoria, Canada
Leary Jon Sumanik
Wind Empowerment, UK
Assane Gueye
Université Alioune Diop de Bambey;
University of Maryland College Park, USA
Alassane Diop
Université Alioune Diop de Bambey;
Universite Laval, Canada
Mohamed Mejri
Université Laval, Canada
Noble Banadda
Makarere University, Uganda
Papa A. Ndiaye
Université Cheikh Anta Diop, Senegal
Moussa Lo
Université Gaston Berger, Senegal
Amadou S. Maiga
Université Gaston Berger, Senegal
VIII
Organization

Bamba Gueye
Université Cheikh Anta Diop, Senegal
Taib Fall
Université Gaston Berger de Saint Louis, Senegal
Nicholas Kiggundu
Makarere University, Uganda
Sherien Elagroudy
Ain Shams University, Egypt
Ramatoulaye Mbacke
Duke University, USA
Maimouna Diouf
Aix-Marseille Université, France
Nelson Kara
University of California Berkeley, USA
Ghada Basioni
Ain Shams University, Egypt
Luiz Fernando Lavado Villa
Paul Sabatier University, France
Organization
IX

Contents
Science, Energy and Environment
A Multivariate Regression Model for the Assessment of Solar Radiation
in the Senegalese Territories. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Ousmane Wane, A. A. Navarro, L. Ramírez, R. X. Valenzuela,
José M. Vindel, F. Ferrera Cobos, Cheikh M. F. Kébé,
and L. F. Zarzalejo
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation
Process for Wastewater Treatment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
Maryam Khadim Mbacké, Cheikhou Kane, Ndèye Oury Diallo,
Mamadou Baldé, and Codou Mar Diop
Study of the Correlation Between the Dust Density Accumulated
on Photovoltaic Module’s Surface and Their Performance
Characteristics Degradation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
Ababacar Ndiaye, Cheikh M. F. Kébé, Boudy Ould Bilal,
Abdérafi Charki, Vincent Sambou, and Papa A. Ndiaye
Education
Strengthening Partnerships Between Universities and SMEs Within
the Open Innovation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
Jean-Charles Cadiou and Emmanuel Chene
Forming Groups of Mobile Learners that Promote Collaborative Learning
Supported by Mobile Devices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
Marie Hélène Wassa Mballo, Alassane Diop, Richard Hotte,
and Ibrahima Niang
Virtual Classroom Solution with WebRTC in a Collaborative Context
in Mathematics Learning Situation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
Pape Mamadou Djidiack Faye, Amadou Dahirou Gueye,
and Claude Lishou
Economic Development
Impact of the Utilization of the Biodigester in the Populations of Bambey
and Perspectives for Mass Adoption and Valorization . . . . . . . . . . . . . . . . .
81
I. Diallo, Assane Gueye, Omar Sene, M. Kare, P. I. Ndiaye,
P. Diouf, Madiop Diouf, A. Dieng, A. Sene, and I. P. Thiao

How to Increase Intuition for Entrepreneurship Spirit in Innovation
Process?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Henri Samier, Emmanuel Chene, Hervé Chistofol, and Simon Richir
Tic Enabler
Development of Prepaid Electricity Payment System for a University
Community Using the LUHN Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . .
107
Oluranti Jonathan, Ambrose Azeta, and Sanjay Misra
Leveraging GPS and SMS-Based Bus Tracking Architecture
for an Efficient Transportation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Bassirou Kassé, Moussa Diallo, and Bamba Gueye
Quantify the Maturity of Internet Banking Security Measures in WAEMU
(West African Economic and Monetary Union) Banks . . . . . . . . . . . . . . . . .
125
Marie Ndaw, Gervais Mendy, Samuel Ouya, and Diaraf Seck
An Opportunistic Connectivity Network for Rural Areas in Senegal . . . . . . .
131
A. Gueye, C. Mahmoudi, O. I. Elmimouni, M. L. Gueye,
and S. O. Ndiaye
Innovation
Entrepreneurship, Education and Youth Employment in Africa: Reframing
Higher Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
Pazisnewende François Kabore
Capacity Development for Agricultural Innovation Systems in Burkina
Faso: What’s New with CDAIS Project? . . . . . . . . . . . . . . . . . . . . . . . . . .
150
Eveline M. F. W. Compaoré Sawadogo and Natewinde Sawadogo
Mechanisms for Strengthening Evidence-Based Policy and Practice:
A Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
Rose Omari
Healthcare
Evaluation of a Cloud Based Health Information System . . . . . . . . . . . . . . .
165
Priscilla Ajayi, Nicholas Omoregbe, Sanjay Misra, and Davies Adeloye
A Decision Support System for Pediatric Diagnosis. . . . . . . . . . . . . . . . . . .
177
Precious Iheme, Nicholas Omoregbe, Sanjay Misra, Foluso Ayeni,
and Davies Adeloye
XII
Contents

CNRIA Workshop
Polar Coding Steganographic Embedding Using Successive Cancellation . . . .
189
Birahime Diouf, Idy Diop, Khadidiatou Wane Keita, Madiop Diouf,
Sidi Mohamed Farsi, Khaly Tall, and Ousmane Khouma
Session-HB: Improving the Security of HB þ with a Session
Key Exchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Ahmad Khoureich Ka
Increase MIMO Systems Performances by Concatenating Short Polar
Codes to Spatial Time Block Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
Madiop Diouf, Idy Diop, Ibra Dioum, Birahime Diouf, Khaly Tall,
Sidi Mohamed Farsi, and Lamine Sane
Towards a Model of Integration of Underserved Cultural Factors
in Software by Reverse Localisation: Case Study in Yemba Culture . . . . . . .
222
Mathurin Soh
Geometric Approach of Blind Channel Estimation. . . . . . . . . . . . . . . . . . . .
234
Agbeti Bricos Ahossi, Ahmed Dooguy Kora, and Roger Marcelin Faye
Towards a Conceptual Framework to Scaffold Self-regulation
in a MOOC. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
Gorgoumack Sambe, François Bouchet, and Jean-Marc Labat
Megamodel Consistency Management at Runtime . . . . . . . . . . . . . . . . . . . .
257
El Hadji Bassirou Toure, Ibrahima Fall, Alassane Bah,
and Mamadou Samba Camara
Neural Networks for Biomedical Signals Classification Based on Empirical
Mode Decomposition and Principal Component Analysis . . . . . . . . . . . . . . .
267
Abdoul Dalibou Abdou, Ndeye Fatou Ngom, Samba Sidibé,
Oumar Niang, Abdoulaye Thioune, and Cheikh H. T. C. Ndiaye
A Software Architecture for Centralized Management of Structured
Documents in a Cooperative Editing Workflow. . . . . . . . . . . . . . . . . . . . . .
279
Milliam Maxime Zekeng Ndadji and Maurice Tchoupé Tchendji
Classification Model of Spikes Morphology Using Principal Components
Analysis in Drug-Resistant Epilepsy . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
Ousmane Khouma, Mamadou Lamine Ndiaye, Idy Diop, Samba Diaw,
Abdou K. Diop, Sidi Mohamed Farsi, Birahime Diouf, Khaly Tall,
and Jean J. Montois
Free Space Passive Optical Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304
Ahmed D. Kora, Cheikh B. Rabany, Zacharia Damoue, and Ibra Dioum
Contents
XIII

SBSD: Towards a Proactive Sensor-Based Schistosomiasis Detection . . . . . .
312
Bassirou Kassé, Moussa Diallo, Bamba Gueye, and Halima Elbiaze
Towards Meningitis Ontology for the Annotation of Text Corpora . . . . . . . .
322
W. R. Cédric Béré, Gaoussou Camara, Sadouanouan Malo, Moussa Lo,
and Stanislas Ouaro
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
XIV
Contents

Science, Energy and Environment

A Multivariate Regression Model
for the Assessment of Solar Radiation
in the Senegalese Territories
Ousmane Wane1(&), A. A. Navarro2, L. Ramírez2, R. X. Valenzuela2,
José M. Vindel2, F. Ferrera Cobos2, Cheikh M. F. Kébé1,
and L. F. Zarzalejo2
1 CIFRES, Ecole Supérieure Polytechnique - UCAD,
BP 5085, Dakar-Fann, Senegal
pa.ousmane1@gmail.com
2 CIEMAT, Energy Department – Renewable Energy Division,
Av. Complutense, 40, 28040 Madrid, Spain
Abstract. Senegal has a great solar potential, so it could be used to shift from a
diesel-based power generation to cheaper renewable energy resources. To
exploit this inexhaustible natural resource, the global horizontal irradiation
remains one of the key parameters for any solar energy project at a given
location. This work establishes a multiple linear regression approach to estimate
the solar radiation in the Senegalese territories using the information of the
global network of weather geostationary satellites (Meteosat and GOES),
satellites database and the ground measurement data available in the website of
the World Radiation Data Center (WRDC) as inputs to the model. Jointly a set
of multivariate regression models, a statistical analysis between Meteonorm data
and outputs of different linear combinations are presented in this work, which
also gives the opportunity to appreciate the precision and consistency of each
solar radiation model on different locations in the study area.
Keywords: Solar radiation  Global horizontal irradiation
Multivariate regression model  Satellites database
1
Introduction
Senegal, like sub-Saharan countries, presents a signiﬁcant energy supply gap charac-
terized by scarcity of petroleum in this part of the sub-region to which is added the
constant ﬂuctuations in the price per barrel. Moreover the production equipment aging
electric power whose fuel is the dominant one represents a constraint to remedy the
inadequate supply to energy demand. For example, in 2000 only 5% of rural house-
holds are connected on the national electrical network (Youm et al. 2000). Although
nowadays there is a progress in this sector and most of villages in Senegal are
unconnected to the national network.
Taking into account what has been stated, the policy orientation toward the
strengthening of our production systems becomes a challenge that must be tackled in
order to satisfy the distribution of electricity in quality and quantity to the population.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 3–15, 2018.
https://doi.org/10.1007/978-3-319-72965-7_1

Nevertheless for the realization/execution of solar energy conversion projects in a
country or region, it is necessary to collect full information on solar resources. This
information on the solar resource facilitates decision-making on different technologies
that can be used locally or regionally, as well as to the investments needed for its
realization. Therefore, the geographical assessment of the solar resource analysis is
without doubt the ﬁrst step in the deployment of development strategies of solar energy
in a particular region.
As much as the study area is deprived of radiometric stations with good spatial
coverage, it remains interesting to use an appropriate methodology for estimating solar
radiation. Consequently, the solar radiation derived from satellite images is an
advanced methodology widely used that offers high reliability and accuracy estimates
(Amillo et al. 2014; Rigollier et al. 2004). Nowadays the estimation methods using
information from meteorological satellites and/or spatial interpolation are typical when
determining the value of solar radiation in a pixel of a rasterized geographic region
(Amillo et al. 2014; Perez et al. 1997; Posselt et al. 2014; Rigollier et al. 2004). They
found out that is the basis of such web applications PVGIS (Photovoltaic Geographical
Information System) and Meteonorm offering solar radiation data and other meteoro-
logical parameters (Perpiña Castillo et al. 2016). Šúri et al. (2007) present an analysis
and mapping of the potential for solar electricity in Europe producible using a model of
solar radiation and climate data available on PVGIS (Šúri et al. 2007). In addition if the
estimation model has input data from different sources including in situ data available,
these will help consolidate the mathematical approach to estimate solar radiation
(Lefèvre et al. 2007; Zarzalejo et al. 2009).
2
Study Area
With a Sahelian climate, Senegal is between 12º and 17º north latitude and 11º and 18º
west longitude. From a general point of view, the country has two seasons: a rainy
season and a dry season. Both transitions to go from one season to another are hardly
noticeable. This reﬂects the consideration of these two seasons in this part of the earth.
The winter or rainy season that begins in southern of the country in May, gradually
spread over the territory between May and October with a peak in August. However, a
great disparity was noted with less precipitation in the northern part, a climate like
BWh, compared to the southern which is the tropical climate (Aw) with a dry winter
(Kottek et al. 2006). Between November and April the country is watered by conti-
nental trade winds, it is the dry season.
The highest temperatures were recorded during the rainy season and in the north
east area. They decrease as and as we approach the coastal areas. The lowest values
were observed in January and February.
Except for the Southern region with some rugged terrain whose altitude does not
exceed 581 m at the highest point of the foothills of the Fouta Djallon (Guinea), the
topography of the Senegalese territory is more or less ﬂat and does not rise above
130 m.
4
O. Wane et al.

3
Multivariate Regression Model
In general, a regression model consists to study, analyse and interpret the relationship
between a dataset (Y) called dependent or response variable and an independent or
explicative variable (X) through a linear function deﬁned by Y ¼ aX þ b, where a and
b are real constants (Núñez et al. 2011). Instead of correlating an independent variable
with another dependent variable, the multiple regression model allows to express the
dependent variable from a linear combination of two or more independent variables.
Thus one of the goals of multivariate regression analysis is to ﬁnd the correlation
coefﬁcients or real constants appropriate to explain relevant aspects between the
dependent variable and the set of independent variables (Montgomery and Runger
2003; Núñez et al. 2011). These coefﬁcients weighted independent variables are then
used to ﬁt the model. The dependent variable is the parameter to be modelled.
The starting hypothesis model of multivariate linear regression associates the
dependent variable to a linear combination of p independent variables weighted by
coefﬁcients (aðiÞ
k ) plus a random perturbation (eðiÞ).
yðiÞ ¼ aðiÞ
0 þ
X
p
k¼1
aðiÞ
k xðiÞ
k þ eðiÞ
ð1Þ
Where y ið Þ is the i-th dependent variable (i = 1,2,…,n) and x ið Þ
k
is the i-th obser-
vation of k-th independent variable (k = 1,2,…,p) with p  n.
Each coefﬁcient regression aðiÞ
k
reﬂects the contribution of the variable xðiÞ
k
in the
response yðiÞ. eðiÞ measures the effect of all the variables not included in the model that
affect the response variable. For a given observation, this term is achieved by calcu-
lating the difference between the observation on the dependent variable and the cor-
responding estimated value. In addition to the initial hypothesis and an inference in our
model the following hypothesis is established (Zarzalejo 2005):
– The number of observations (n) must be greater than the number of explicative
variables (p). In other words, if too many independent variables are included in a
model it is very likely that the regression coefﬁcients are biased in the direction
opposite to the null hypothesis (H0 below) (Núñez et al. 2011).
– The same as the response variables, the forecast errors are independent between
them. The errors are null hope E e ið Þ


¼ 0: The random perturbation has on average
0. Otherwise, the expected value of the respond variable is only a function of
regression coefﬁcients and explicative variables.
– The distribution of errors follow a normal law of mean equal to zero. It is also
assumed which is the case for the distribution of the response variables.
– The error variance is constant and doesn’t depend on the independent variables.
A Multivariate Regression Model
5

From the (Eq. 1), we have the traditional model of simple linear regression:
Y ¼ Xa þ e
ð2Þ
Where a is the vector of the regression coefﬁcients, e is the vector of random
perturbations and Y is dependent variable. X is the matrix of independent variables.
The regression coefﬁcient matrix is an indicator of the contribution of each
explicative variable in the model. However, the regression coefﬁcients are inﬂuenced
by factors such as variance and linearity of the explicative variables. The variance is a
measure of dispersion of the variables in question. Therefore the determination of the
optimal regression coefﬁcients leads to study and analyse the perturbations (errors) of
the model. Hence, the idea is to minimize the errors. Otherwise one seeks to minimize
the distance between the values of the dependent variable and predicted values. This
will be done by ﬁnding the solution of the equation that minimizes the sum of per-
turbations (e) using the method of least squares.
4
Data
Some studies like Obrecht in 1990, Ba and Nicholson in 2001 and Diabaté et al. in
2004 estimated/mapped the solar radiation in Africa using satellites data and available
pyranometers data (Ba and Nicholson 2001; Diabaté et al. 2004; Obrecht et al. 1990).
The problem they faced and which is continuing to recurrent, is the scarcity of radiation
stations in parts of the continent to validate their mapping of solar radiation studies
using satellite information. The lack of ground measurement solar radiation data with
good spatial coverage and over a sufﬁciently long period on the sub-region, leads us to
work with satellite data and reduced ground data available in situ. Even if the data
exist, they aren’t often over a sufﬁciently long period or in a nearby present. Besides, it
will add the possibility of non-coincidence between the ﬁeld data and satellite infor-
mation like in (Diabaté et al. 2004).
4.1
Ground Data
In our study a total of 10 World Radiation Data Center (WRDC) ground stations
identiﬁed in Senegal are used for modelling the monthly average of global horizontal
irradiation (GHI). The WRDC, sponsored by the World Meteorological Organization
(WMO), collects and archives radiometric data around the world to ensure the avail-
ability of these data for research by the international scientiﬁc community. The WRDC
is one of the world or national data centers with as much radiometric ground stations in
the Senegalese territories. The selected recording period for this work ranges between
1984 and 1991 and is far from completed for these stations identiﬁed in Senegal
(Table 1). The same locations are used for a set of training points to download the GHI
from other and following databases used in this work.
The time series of global horizontal irradiation are measured using thermoelectric
pyranometer Kipp & Zonen CM5 model (WRDC 2016). This solar radiation value is
6
O. Wane et al.

the monthly average of the sum of the energy of solar radiation that reaches one square
meter in a horizontal surface in a day (GHI) and is given in J/cm2. For the remainder of
this work the data are converted in Wh/m2/day and represent the dependent variable for
the multivariate regressive model.
4.2
Satellites Data
In each of these ten stations identiﬁed in the Fig. 1, we also got the global solar
radiation from the NASA-SSE database with monthly average incident data on the
surface of the earth time series between 1984 and 2004 (NASA-SSE 2016). The same
has been done with the web applications such as PVGIS that provides monthly values.
Indeed, PVGIS offers two solar radiation values: a solar radiation calculated from the
Helioclim database and the other using the CMSAF data (PVGIS 2016). The monthly
average of the sum of GHI estimations from satellite images (GOES and Meteosat)
provided by CMSAF, Climate Monitoring Satellite Application Facility between 1998
Table 1. Details of the 10 WRDC stations using as well as possible to get the maximum solar
radiation information on the extent of the Senegal. These geo-locations have been used as the
training points for downloading solar radiation data from various databases.
Station name
Code
Lat. (º) Long. (º) Elev. (m) Years of the measurements
Bambey
616411 14.42
−16.28
20
1984–1988
Dakar/Yoff
616410 14.44
−17.3
27
1984; 1986–1991
Kédougou
616990 12.34
−12.34
178
1988–1990
Linguere
616270 15.23
−15.07
20
1984; 1986–1991
Louga
616121 15.37
−16.13
38
1985; 1989–1991
Matam
616300 15.39
−13.15
15
1984–1991
Nioro du Rip
616871 13.44
−15.47
18
1988–1991
Podor
616120 16.39
−14.58
6
1985–1991
Tambacounda 616870 13.46
−13.41
49
1984–1987
Ziguinchor
616950 12.33
−16.16
26
1986–1991
Fig. 1. WRDC Station locations identiﬁed in Senegal
A Multivariate Regression Model
7

and 2005 and June 2006–Dec. 2011, Helioclim between 1985 and 2005 (Huld et al.
2012; PVGIS 2016). These datasets represent the sets of independent variables for
modelling the solar radiation.
PVGIS: The PVGIS (Photovoltaic Geographical Information System) is a web appli-
cation that provides climate data and tools needed to assess performance of photo-
voltaic systems (PV) in Europe, Africa and southwest Asia (PVGIS 2016).
NASA-SSE: National Aeronautics and Spatial Administration - Surface meteorology
and Solar Energy is a renewable energy resource website developed under project
POWER (Prediction of Worldwide Energy Resource) piloted by NASA (NASA-SSE
2016).
4.3
Focus on the Inputs Data
All the data used in this study have been downloaded into a format ﬁle that were
unusable directly. So, it was only possible to display the data on the computer screen
when accessing website and introducing the geographical coordinates of the training
point. From there, these data were collected manually and it is likely to induce some
error. In addition, one of the obstacles is the non-coincidence of this time series.
All inputs data are measured in Wh/m2/day and referred to as monthly average of
the sum of global solar radiation energy that hits one square meter on a horizontal
surface in one day.
The rest of databases provides a unique monthly value for each month. So, we are
left with an established database from ﬁve sources listed above. This allows us to have
ﬁve different monthly average solar radiation values for each of these ten geo-locations
(Fig. 1). As follows, the data of each database in a given location represent a variable
with 120 observations: it’s the annual period (10 stations, 12 months) for modelling the
monthly average of the sum of the global solar radiation. Nevertheless a seasonal
separation for the datasets had been made also according to the dry and rainy seasons.
In framework of our study, the rainy and dry season correspond respectively to the
months from May to October and November to April.
This separation of inputs data are used in the Eq. (1) according to the different
linear combinations possible:
Model 1 : YWRDC ¼ a0 þ a1  XCMSAF þ a2  XHelioclim þ a3  XNASA þ e1
ð3Þ
Model 2 : YWRDC ¼ b0 þ b1  XCMSAF þ b2  XHelioclim þ e2
ð4Þ
Model 3 : YWRDC ¼ c0 þ c1  XCMSAF þ c2  XNASA þ e3
ð5Þ
Model 4 : YWRDC ¼ d0 þ d1  XHelioclim þ d2  XNASA þ e4
ð6Þ
Where YWRDC represent the dataset of solar radiation obtained from WRDC and Xi
is the dataset corresponding to the i database, with i ¼ CMSAF; Helioclim; NASA
f
g.
To validate the estimated dataset, the Meteonorm data are used. Meteonorm offers
access to accurate meteorological for any place on Earth (METEOTEST 2016).
8
O. Wane et al.

The Meteonorm database stands out among the various sources used as part of this
work. In Meteonorm, several different international databases (ground stations and
satellite data) are included, checked for ensuring reliability and forming a single
comprehensive database permitting worldwide simulation of solar energy systems,
buildings and environmental simulations (Remund et al. 2015). This choice was also
motivated by the fact that Meteonorm uses the same weather stations WRDC identiﬁed
above. The irradiance downloaded from Meteonorm for the same training points in the
Fig. 1 is a monthly mean hourly global horizontal solar radiation and is given in W/m2.
This data converted in Wh/m2/day.
5
Validation Model
One of the steps after the determination of the regression coefﬁcients is to evaluate the
contribution of each regression coefﬁcient, following the hypotheses listed below:
– H0 : a1 ¼ a2 ¼ . . . ¼ ap ¼ 0. In other words, there is no contribution of any
independent variable in the response. If this hypothesis is true, the model spells:
yðiÞ ¼ aðiÞ
0 þ eðiÞ.
– H1 : ap 6¼ 0. That is, at least one of the independent variables (explicative) makes a
contribution.
The idea is to see whether the addition of a variable as the result of other variables
in the regression model makes a signiﬁcant contribution to the proportion of variance
due to regression. For this, we use the theoretical distribution statistics: the test of
Fisher for testing the model in its entirety or the student test to see the contribution of
each estimator. The Fisher statistic or the Student is an analysis of the variance of the
variables by calculating a probability distribution obeying a normal distribution. Under
the null hypothesis and the hypothesis of independence, the ratio of means squares
regression of the variance and the residual variance, deﬁned as FM, follows a Fisher
distribution with p and n  ðp þ 1Þ
½
 degrees of freedom:
FM ¼
atXtY  nY2


=p
YtY  atXtY
ð
Þ= n  ðp þ 1Þ
½
  Fp;nðp þ 1Þ
ð7Þ
Where atXtY  nY2
ð
Þ and YtY  atXtY
ð
Þ represent matrix form of the expression of
regression Sum of squared (SSR) and sum of squared errors (SSE), respectively.
Fp;nðp þ 1Þ is the tabulated Fisher distribution function. Thus the critical value (p-value)
of FM is deﬁned as the probability that Fp;nðp þ 1Þ is superior to FM:
p  value ¼ P(Fp;nðp þ 1Þ  FMÞ
ð8Þ
When FM > p-value, the null hypothesis (H0) is rejected. At least one of the inde-
pendent variables makes a signiﬁcant contribution to the response. This is leading to
study the contribution of each independent variable.
A Multivariate Regression Model
9

With the student distribution, the test focuses on the contribution of each inde-
pendent variable. Under the normality hypothesis of the distribution of response
variables, the sampling distribution of a regression coefﬁcient, a1 for example, is that of
a normal distribution whose variance is s2ða1Þ and the mean is equal to the expected
value of so-called coefﬁcient. The comparison of this expected value to the estimated
value deﬁned by t, follows the student Statistics with n  ðp þ 1Þ
½
 degrees of freedom
and ðaÞ signiﬁcance level:
tM ¼ ^ak  ak
sðakÞ  ta=2;ðnp1Þ
ð9Þ
Where, ^ak is the k-th estimator of the matrix of regression coefﬁcients, ak is the
expected value of the k-th estimator, ta=2; ðnp1Þ is given by the t-student table and
sðakÞ is the square root of the k-th diagonal term of the matrix of covariance,
SSE=ðn  p  1Þ  XtX
ð
Þ1.
The signiﬁcance level represents the region where the values are not compatible
with the null hypothesis. For example, to evaluate the contribution of the k-th esti-
mator, we simply need to apply the null hypothesis (H0) for this estimator, i.e. ak ¼ 0
or the mean value of the distribution is zero. Unlike the ﬁsher statistics, it is possible to
obtain the p-value (critical value) and the conﬁdence interval at a desired signiﬁcance
level ðaÞ for each estimator. In the study we choose a threshold of 0.05 for the
signiﬁcance level for the bounds. In other word, the conﬁdence level for the bounds is
95%. The same analysis previously done with the Fisher statistic is used here to
calculate the critical value of each estimator. For a given estimator, this critical value
represents the probability that ta=2; ðnp1Þ is superior to t of the corresponding esti-
mator. The conﬁdence bound deﬁnes the area of acceptance or rejection of the null
hypothesis. This range is deﬁned by:
ak  ta=2; ðnp1Þ
ð10Þ
The null hypothesis is rejected when:
tM\  ta=2; ðnp1Þ or tM [ ta=2; ðnp1Þ: In other words Pðta=2; ðnp1Þ  tMÞ\0; 05
In order to evaluate the proposed models in its entirety, we use the classical
estimators as Mean Bias Error (MBE) and Root Mean Square Error (RMSE). The
combination of the bias and precision is statistical indicator that deﬁnes the perfor-
mance of an estimator. The MBE evaluates the systematic error in the estimation
(Zarzalejo 2005; Walther and Moore 2005). The MBE and RMSE are given by the
following expressions (Walther and Moore 2005):
MBE ¼
X
n
i¼1
^y ið Þ  y ið Þ
n
ð11Þ
10
O. Wane et al.

RMSE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
n
i¼1
^y ið Þ  y ið Þ

2
n
v
u
u
u
t
ð12Þ
Where y ið Þ is the Meteonorm data, ^y ið Þ is the estimated solar radiation data and n
represents the dimension of the dataset.
6
Results and Discussion
The regression coefﬁcients and the evaluation parameters of these estimators constituted
by the standard deviation, conﬁdence interval, F (Fisher) and t (Student) statistic, and
p-value of these estimators are illustrated in this table. Due to the annual and seasonal
separation of data, each linear combination or multivariate regression model are tested
three times thus allowing to compare the results for a better estimation of solar radiation.
The statistical analysis between the solar radiation outputs models and Meteonorm
data, separated the dataset into annual and seasonal period included in spatially dis-
tributed. As the objective is to have a high coefﬁcient of determination with a smallest
bias and a good accuracy, it becomes difﬁcult to conform from one model to another or
if the model would sealed to choose the right period of analysis. The conclusion on the
choice of a model might be the right method to estimate solar radiation in our study can
be supported by scientiﬁc reasoning on the probability estimators according to the
critical value (p-value) of the estimators (correlation coefﬁcients) and the statistical
parameter listed above. A p-value less than 0.05 is generally accepted as the threshold
where the null hypothesis is rejected and at least one independent variable or the
independent variable in question brings signiﬁcant contribution to the model. From the
above, Table 2 summarizing the statistical analysis of the estimators of models we can
note that none of the cases studied has a p-value less than 0.05 for all these estimators.
This reﬂects that some variables whose p-value less than 0.05 don’t make a signiﬁcant
contribution to the response. The correlation of outputs model with Meteonorm data
illustrated in the following table and ﬁgures conﬁrm what has been found about the
choice of a models with regard to others (Fig. 2).
With the corresponding period from November to April or dry season, the corre-
lation results of the outputs M1 model with Meteonorm data gives a RMSE of the order
of 11.49% with 10.74% distortion error and a coefﬁcient of determination of 0.99.
Comparing these results with those obtained from other linear combinations for the
same period of separation of data (dry season), the model M1 has more inaccuracy and
distortion of a higher error than others. Because each of the other models (M2, M3 and
M4) have a much lower MBE with a value of approximately −1.94% identical to M2,
M3 and M4. The RMSE of each these three is ranging from 7.35% to 7.76% (see
Table 3). Thus, for the dry season the M3 model which uses as NASA-SSE data inputs
and PVGIS-CMSAF is more suitable than others for the assessment of horizontal
global solar radiation on a monthly average.
To evaluate the most ideal model for the rainy season, the same difference noted
earlier with the statistical parameters with the dry period is more or less observed for
the period corresponding to the month storm. Except the ﬁrst linear combination,
A Multivariate Regression Model
11

Table 2. Statistical analysis of the models M1, M3 and M4 according to the annual period, dry
seasonal and rainy seasonal, respectively.
Parm. Coef.
SD
Inf. bound Sup. bound tM
p-value
Model Nº 1 (M1): Annual period
a0
1264,86 387,68 497.32
2032.93
3,26
0,0015
a1
0,57
0,13
0.32
0.82
4,56
0,0000
a2
−0,18
0,12
−0.41
0.056
–1,50
0,1400
FM = 40,43
p-value = 0,0000
Model Nº 3 (M3): Dry season
c0
804,08
485,55 −168.09
1776.26
1,66
0,1030
c1
0,63
0,18
0.28
0.98
3,56
0,0008
c2
0,11
0,19
−0.27
0.48
0,56
0,5767
FM = 41.87
p-value = 0,0000
Model Nº 4 (M4): Rainy season
d0
2468,45 583,21 1301.52
3637.38
4,23
0,0000
d1
0,64
0,15
0.34
0.95
4,26
0,0000
d2
−0,15
0,11
−0.37
0.08
−1,28 0,2061
FM = 14.65
p-value = 0,0000
Annual period
Dry seasonal
Rainy seasonal
Model Nº 1.
Model Nº 3.
Model Nº 4.
Fig. 2. Results of the linear correlation of the models M1, M3 and M4
12
O. Wane et al.

whose bias is −6.65% with quadratic error of 7.65%, the MBE is approximately equal
to −0.85% M2, M3 and M4 with a RMSE of 5.76% M2 and 5.20% for M3 and M4. In
addition, when comparing the coefﬁcients of determination of these different models,
we can observed that the M4 remains the most adequate to evaluate the solar radiation
during the rainy season.
In an analysis based on annual period, a slight difference was noted compared to the
values of the statistical parameters calculated when switching from one model to
another. For this period of separation of data, MBE is the same for all models and their
value is equal to −1.37% for all considered models. The determination coefﬁcients are
between 0.68 and 0.73 and the root mean square errors vary from 6.50% up to 6.99%.
In conclusion, the model M1 is the most suitable choice for the estimation of solar
radiation without distinction on the dry and rainy seasons.
7
Conclusions
The vulnerability of energy supply systems in sub-Saharan Africa is an obstacle to
development that must be addressed to ensure energy security in the region. The energy
mix is an alternative to offer a qualitative and quantitative supply of electricity to the
population. Renewable energy technologies are a way to reach this energy mix. The
main advantages of the use of renewable energies are the diversiﬁcation of energy
supply, the use of new production and distribution of energy, which ensures the
competitiveness of the Senegalese electrical system, and minimize the impact on
environment.
From satellite data and ground data, the monthly mean of the sum of solar radiation
energy that reaches a square meter on a horizontal plane in a day has been studied for
estimating the solar resource in a geographical point in Senegal necessary for all solar
Table 3. Linear correlations coefﬁcient between the estimated values for the different models
and the Meteonorm data.
Model Linear combinaison
Period
MBE (%) RMSE (%)
M1
GHI ¼ fðCMSAF; Helioclim; NASAÞ Annual
−1.37
6.50
Rainy
−6.26
7.65
Dry
−10.74
11.49
M2
GHI ¼ fðNASA; HelioclimÞ
Annual
−1.37
6.93
Rainy
−0.85
5.76
Dry
−1.94
7.76
M3
GHI ¼ fðNASA; CMSAFÞ
Annual
−1.37
6.52
Rainy
−0.85
5.20
Dry
−1.94
7.35
M4
GHI ¼ fðCMSAF; HelioclimÞ
Annual
−1.37
6.99
Rainy
−0.85
5.20
Dry
−1.94
7.48
A Multivariate Regression Model
13

energy project at national level. For example, it is necessary to have a good number of
measuring stations or training points with a good spatial distribution in the study area to
develop a map of solar energy potential. The constraints related to the radiometric data
from available weather stations in the country made the interest of our research project.
This linear combination of satellite and ground data is important to elaborate a Typical
Meteorological Years (TMYs) in the future solar energy systems studies. The need for
a meteorological data base represents an advance in the ﬁeld of assessing solar resource
on the extent of Senegalese territories. Such a study can serve as a scientiﬁc contri-
bution or reference for future projects to install photovoltaic or solar thermal power
plants.
Acknowledgement. An acknowledgement to the Radiation Solar Group of CIEMAT for
supervising this work, not forgetting the Spanish Cooperation Agency (AECID) for ﬁnancing of
my stay at this research center.
References
Amillo, A.G., Huld, T., Müller, R.: A new database of global and direct solar radiation using the
eastern meteosat satellite, models and validation. Remote Sens. 6, 8165–8189 (2014). https://
doi.org/10.3390/rs6098165
Ba, M.B., Nicholson, S.E.: Satellite-derived surface radiation budget over the african continent.
Part II: climatologies of the various components. J. Clim. 14, 60–76 (2001). https://doi.org/10.
1175/1520-0442(2001)014<0060:SDSRBO>2.0.CO;2
Diabaté, L., Blanc, P., Wald, L.: Solar radiation climate in Africa. Sol. Energy 76, 733–744
(2004). https://doi.org/10.1016/j.solener.2004.01.002
Huld, T., Müller, R., Gambardella, A.: A new solar radiation database for estimating PV
performance in Europe and Africa. Sol. Energy 86, 1803–1815 (2012). https://doi.org/10.
1016/j.solener.2012.03.006
Kottek, M., Grieser, J., Beck, C., Rudolf, B., Rubel, F.: World map of the Köppen-Geiger climate
classiﬁcation updated. Meteorol. Zeitschrift 15, 259–263 (2006). https://doi.org/10.1127/
0941-2948/2006/0130
Lefèvre, M., Wald, L., Diabaté, L.: Using reduced data sets ISCCP-B2 from the Meteosat
satellites to assess surface solar irradiance. Sol. Energy 81, 240–253 (2007). https://doi.org/
10.1016/j.solener.2006.03.008
Zarzalejo, L.F.: Estimación de la radiación global horaria a partir de imagenes de satelite.
Desarrollo de modelo empíricos. Universidad Complutense de Madrid (2005)
METEOTEST: Meteonorm [WWW Document] (2016). http://meteonorm.com. Accessed 9 Feb
2016
Montgomery, D.C., Runger, G.C.: Applied Statistics and Probability for Engineers. Wiley,
Hoboken (2003)
NASA-SSE: Langley Research Center Atmospheric Science Data Center Surface meteorological
and Solar Energy (SSE) web portal supported by the NASA LaRC POWER Project [WWW
Document] (2016). https://eosweb.larc.nasa.gov/sse/. Accessed 8 Feb 2016
Núñez, E., Steyerberg, E.W., Núñez, J.: Estrategias para la elaboración de modelos estadísticos
de regresión. Rev. Esp. Cardiol. 64, 501–507 (2011). https://doi.org/10.1016/j.recesp.2011.
01.019
14
O. Wane et al.

Obrecht, D.: Météorologie solaire et images satellitaires : cartographie du rayonnement solaire,
détermination de l’albédo des sols et évaluation de l’ennuagement (1990)
Perez, R., Perez, R., Seals, R., Zelenka, A.: Comparing satellite remote sensing and ground
network measurements for the production of site/time speciﬁc irradiance data. Sol. Energy 60,
89–96 (1997). https://doi.org/10.1016/S0038-092X(96)00162-4
Perpiña Castillo, C., Batista e Silva, F., Lavalle, C.: An assessment of the regional potential for
solar power generation in EU-28. Energy Policy 88, 86–99 (2016). https://doi.org/10.1016/j.
enpol.2015.10.004
Posselt, R., Mueller, R., Trentmann, J., Stockli, R., Liniger, M.A.: A surface radiation
climatology across two Meteosat satellite generations. Remote Sens. Environ. 142, 103–110
(2014). https://doi.org/10.1016/j.rse.2013.11.007
PVGIS: Photovoltaic Geographical Information System - Interactive Maps [WWW Document]
(2016). http://re.jrc.ec.europa.eu/pvgis/apps4/pvest.php?map=africa. Accessed 2 Jan 2016
Remund, J., Müller, S., Kunz, S., Huguenin-Landl, B., Studer, C., Klauser, D., Schilter, C.,
Lehnherr, R.: Meteonorm: Global Meteorological Databases. Handbook Part I : Software v7
(2015)
Rigollier, C., Lefèvre, M., Wald, L.: The method Heliosat-2 for deriving shortwave solar
radiation from satellite images. Sol. Energy 77, 159–169 (2004). https://doi.org/10.1016/j.
solener.2004.04.017
Šúri, M., Huld, T.A., Dunlop, E.D., Ossenbrink, H.A.: Potential of solar electricity generation in
the European Union member states and candidate countries. Sol. Energy 81, 1295–1305
(2007). https://doi.org/10.1016/j.solener.2006.12.007
Walther, B.A., Moore, J.L.: The concepts of bias, precision and accuracy, and their use in testing
the performance of species richness estimators, with a literature review of estimator
performance. Ecography 28(6), 815–829 (2005)
WRDC: World Radiation Data Centre Online Archive (2016) [WWW Document]. http://wrdc-
mgo.nrel.gov/. Accessed 30 Jan 2016
Youm, I., Sarr, J., Sall, M., Kane, M.M.: Renewable energy activities in Senegal: a review.
Renew. Sustain. Energy Rev. 4, 75–89 (2000). https://doi.org/10.1016/S1364-0321(99)
00009-X
Zarzalejo, L.F., Polo, J., Martín, L., Ramírez, L., Espinar, B.: A new statistical approach for
deriving global solar radiation from satellite images. Sol. Energy 83, 480–484 (2009). https://
doi.org/10.1016/j.solener.2008.09.006
A Multivariate Regression Model
15

Modeling the Crystal Violet Kinetics Removal
by Electrocoagulation Process for Wastewater
Treatment
Maryam Khadim Mbacké, Cheikhou Kane(&), Ndèye Oury Diallo,
Mamadou Baldé, and Codou Mar Diop
Laboratoire d’Electrochimie et des Procédés Membranaires,
Ecole Supérieure Polytechnique, Université Cheikh Anta Diop,
B.P. 5085, Dakar Fann, Senegal
cheikhoukane.esp@gmail.com
Abstract. A batch reactor is used with an aluminum anode and a stainless steel
cathode to decolorize crystal violet solutions by electrocoagulation. Optimiza-
tion of operating parameters showed that for initial concentrations ranging from
5 to 100 mg/L, initial pH ranging from 6.72 to 5.43, current density of
250 A/m2, conductivity of 4.27 mS/cm, and inter-electrode distance of 0.5 cm,
a yield of 99.75% is achieved on reduction of color and 98.5% for reduction of
chemical oxygen demand (COD).
Modeling of crystal violet elimination kinetics were tested. Results showed
that homogenous second order kinetics and adsorption kinetics of pseudo sec-
ond order can be applied. Various isotherms models were investigated. Fre-
undlich isotherm gave better correlation with a coefﬁcient of 0.989.
Variable Order Kinetic (VOK) model has been associated with Freundlich
isotherm to take into account production efﬁciency of adsorbent over the time.
Results showed through a statistical analysis that this approach provides a good
correlation.
Keywords: Electrocoagulation crystal violet  Optimization  Modeling
Adsorption
1
Introduction
Electrocoagulation (EC) is an electro-chemical technique that offers enhanced coagu-
lation in which metal ions are generated on application of potential difference, which
forms complex metal hydroxides for removal of impurities. The metal ions generated
by anodic oxidation form metal hydroxides due to simultaneous electrolysis of water;
these metal hydroxides have higher sorption capacity and thus favor enhanced removal;
while generation of cathodic gases assist in electro ﬂoatation of impurities [1]. Crystal
violet is a cationic dye and widely used as a purple dye for textiles such as cotton and
silk. It provides a deep violet color for paints and printing ink. It is also used to dye
paper [2].
Different technological approaches like adsorption, coagulation, biological treat-
ment, advanced oxidation process, solar photocatalytic treatment [3], UV and ozone
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 16–30, 2018.
https://doi.org/10.1007/978-3-319-72965-7_2

treatment, membrane separation technologies etc. have been tested for the removal of
such dyes. All the above processes have been found to have certain limitations in their
applications.
Electrocoagulation offers some distinct advantages over existing processes such as
ambient operability conditions, no threat of secondary pollution due to the absence of
any extra chemicals. The EC process is attractive due to its simplicity of operation,
control and effective removal efﬁciency [2].
In this study, decolorization of the crystal violet dye solution by electrocoagulation
method has been investigated. The performance of electrocoagulation method for
removing color from this dye solution has been evaluated. The operating parameters
such as current density, pH, conductivity, inter electrode distance has been optimized.
An important objective was to obtain a variable order kinetic VOK model that could
make reliable prediction of the efﬁciency of the electrocoagulation process. The VOK
model has the advantage of a good description of homogeneous kinetic aspects,
adsorption isotherm and variation of the absorbent mass in known time.
So the various phenomena involved in the electrocoagulation process will be
modeled by the VOK model associated with the appropriate adsorption isotherm.
Interactions between adsorption strength and the initial concentration of crystal violet
will be evaluated to predict the optimal conditions in the case of a set industrial use.
2
Materials and Methods
2.1
Characteristics of Crystal Violet
Crystal violet or gentian violet is a triarylmethane dye. When dissolved in water, the
dye has a blue-violet color with an absorbance maximum at 592 nm and an extinction
coefﬁcient of 87,000 M−1cm−1. The color of the dye depends on the acidity of the
solution. At a pH of 1.0, the dye is green with absorption maxima at 420 nm and
620 nm, while in a strongly acidic solution (pH = −1), the dye is yellow with an
absorption spectrum maximum at 420 nm. Different colors are a result of the different
charged states of the dye molecules. In the yellow form, all three nitrogen atoms carry a
positive charge, of which two are protonated, while the green color corresponds to a
form of the dye with two of the nitrogen atoms positively charged. At neutral pH, both
extra protons are lost to the solution, leaving only one of the nitrogen atoms positive
charged [4].
2.2
Experimental Device
The experiments were conducted in a cell equipped with an anode of aluminum and a
stainless steel cathode. The system operates in batch mode. A VoltaLab PGZ 100
potentiostat type which can provide up to 1 A is used to power the cell.
The operating parameters are optimized by varying respectively: sample volume of
100 mL; the initial concentration from 5 to 100 mg/L; the current density from 100 to
300 A/m2 (for an electro-active surface of 14 cm2, the current is varied from 0.1 to
0.4 A); the initial pH of 3 to 9 (by addition of NaOH and HCl solutions at 0.1 N); the
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
17

initial conductivity of 0.9 to 4.27 (by addition of a mass of NaCl ranging from 0.1 to
1 g/L); the inter-electrode distance (ied) of 0.25 to 1.5 cm.
Dye concentration was spectroscopically determined using the Beer-Lambert law
(Absorbance = e  ‘  C) applied at 592 nm, where e is the molar absorptivity, ‘ is
the solution thickness and C the CV concentration. The absorbance are measured by a
spectrophotometer SPECORD 250 PLUS.
2.3
Electrode Reactions
Aluminum electrodes are usually used as sacriﬁcial anodes, and their electrochemical
dissolution allows generation of the corresponding cations according to the reaction (1).
Al $ Al3 þ
ðaqÞ þ 3e
ð1Þ
Stainless steel plates involved as cathode producing gaseous hydrogen (2).
2H2O þ 2e $ 2OH
aq þ H2ðgÞ
ð2Þ
The electrogenerated metallic ions (here Al3+) are hydrolyzed in the electrochemical
cell to produce aluminum hydroxide according reactions (3)
Al3 þ
ðaqÞ þ 3OH $ Al OH
ð
Þ3ðsÞ
ð3Þ
Al(OH)3(s), so called coagulants, can interact with charged (or not charged) species
of dye particles in the water and adsorbs them; in fact Al(OH)3(s) neutralizes the
electrostatic charges on disperses particles to reduce the electrostatic interparticle
repulsion so that the van der Waals attraction predominates, thus enhancing agglom-
eration; consequence is that ﬂocks appears, which, thanks to the gaseous hydrogen
produced at the cathode, ﬂoat. It is believed that the main pollutant removal mechanism
observed during electrocoagulation is adsorption and entrapment onto the amorphous
aluminum
hydroxide
precipitate
formed.
Except
aluminum
consumption,
the
electro-coagulation processes do not require the addition of any chemicals.
2.4
Modeling Experiments
Based on the optimized parameters, we performed treatments on solutions with variable
initial concentrations of crystal violet. The absorbance is measured before and after
treatment to evaluate the percentage of fading. The equilibrium data such as the con-
centration and the adsorption capacity is achieved when the estimated discoloration
percentage is 98, which corresponds to a quasi-stability of the ﬁnal concentration.
These equilibrium data are used to test the different adsorption isotherms. In deter-
mining the adsorption capacity, the samples are ﬁltered when any of the balance is
judged reached. After ﬁltering the slurry obtained is dried in an oven and weighed. It is
believed that the adsorption capacity is equal to the mass of dye adsorbed on the mass
of adsorbent.
18
M. K. Mbacké et al.

The electrodes were weighed before and after each treatment to assess the total
mass of consumed aluminum. This allows calculating the Faraday efﬁciency ðucÞ.
To evaluate the efﬁciency of the formation of aluminum hydroxide ðuAlÞ, the
amount of dissolved aluminum in the treated and ﬁltered solution was determined with
a multiparameter bench photometer from Hanna Instruments HI83099.
2.5
Model Development
2.5.1
Kinetics and Adsorption Models
First and second order reaction kinetics were used to study the decolorization kinetics
of crystal violet.
The contaminants are usually adsorbed at the surface of the metal hydroxides
generated during the electrocoagulation process. In order to identify the mechanism of
the adsorption process, it is important to establish the most appropriate correlation for
the equilibrium curves. Isotherm models with two and three parameters have been
considered to establish the relationship between the amounts of crystal violet adsorbed
onto the aluminum hydroxides and its equilibrium concentration in the aqueous
solution containing crystal violet.
Equilibrium adsorption isotherm data were analyzed according to the Langmuir,
Freundlich, Fritz–Schlunder, Radke–Prausnitz and Toth models.
2.5.2
Variable Order Kinetic: Model Development
VOK model is based on a combination of three laws:
– the faraday law;
– the adsorption model static.
A constant courant density fade is proportional to the production of aluminum.
Faraday efﬁciency describes the efﬁciency with which charge (electrons) are
transferred in a system facilitating an electrochemical reaction.
Faraday’s law is deﬁned:
mtheo ¼ n Itheot
F
 M
z
ð4Þ
mtheo: Mass of aluminum obtained for a ﬁxed current Itheo
t: duration of treatment (min)
F: Faraday constant 96500 C
M: Molar mass of aluminum
z: electrical charge z = 3
Faradic and aluminum yields are given by uc and uAl respectively:
uc ¼ ½AlT
½Altheo
ð5Þ
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
19

uAl ¼ Al
½
T  Al
½
dis
½AlT
¼ 1  Al
½
dis
½AlT
ð6Þ
½AlT: Total concentration of aluminum obtained by weighing the electrodes before
and after treatment
½Altheo: Theoretical concentration of aluminum obtained from the faraday law
Mameri et al. [5] show that in the electrocoagulation process producing Al3+ is a
limiting factor for ﬂock formation. When the current density is 200 A/m2 the
adsorption reaction is instantaneous.
The change in the dye removal rate is proportional to the adsorbent the production
rate according to the following relationship:
 d C
½ 
dt ¼ 1
2 uAl  uc  n*Itheo
F
 M
zV  C
ð7Þ
C represent the adsorption capacity which is depending on the model adopted.
Table 3 gives the expressions of different VOK equations depending on the
adsorption isotherm model chosen.
3
Results and Discussion
3.1
Optimization of Operating Parameters
pH of the solution plays an important role in electrochemical and chemical coagulation
process [5].
Under certain conditions, various complex and polymer compounds can be formed
via hydrolysis and polymerization reaction of electrochemically dissolved aluminum
ions. Hence, experiments were conducted to study the effect of pH on the crystal violet
removal.
The results obtained are shown graphically in Figs. 1 and 2 for 100 mg/L crystal
violet initial concentration.
Figure 1 shows the variation of initial pH observed during experiments. We note
that whatever the initial pH of the solution, the pH stabilizes around 8 after 5 min of
treatment.
Indeed, as shown in Eq. 5, the reduction of water at the cathode results in a constant
production of hydroxides ions which maintains the pH to a basic level. In addition, the
electrocoagulation process is characterized by a buffer effect and is observed when the
aluminum hydroxide begins to form.
Figure 2 shows that for very acid initial pH (pH = 3) and for the neutral and basic
pH (pH = 7 and pH = 9), the removal rate of the crystal violet is quite slow. For initial
pH = 5, the speed improves. To an initial solution of 100 mg/L, pH (no change) is
5.43. This value pH gave the best removal rate.
The effects of pH of the initial solution on the electrocoagulation process are related
to the solubility of aluminum hydroxide as a function of pH.
20
M. K. Mbacké et al.

The hydrolysis of aluminum either released from the electrodes in electrocoagu-
lation and the adsorption of the hydrolysis products on the particles are greatly affected
by the varying pH of the suspension. Figure 3 shows the solubility diagram for alu-
minum hydroxide Al(OH)3(s), assuming only monomeric species. Non hydrolyzed Al3+
and cationic hydrolysis products, Al(OH)2+ and Al OH
ð
Þ þ
2 are the dominant species in
solution at pH values less than 5. The cationic hydrolysis products, rather than the non
hydrolyzed Al3+, are known to adsorb strongly on negatively charged particles leading
to charge neutralization, and also to charge reversal with excess amount of aluminum
ions [6]. This explains the low crystal violet removal efﬁciencies due to electrical
repulsion between the particles at pH values less than 5 in the case of electrocoagu-
lation. At pH values 5–8.5, especially with excessive aluminum concentration, amor-
phous aluminum hydroxide precipitation becomes important. This precipitation may
occur either on the surface of quartz particles or the hydroxide precipitates formed in
the bulk may attach to the particles. At pH values higher than 8.5, the aluminate ion, Al
(OH)4
−, becomes the predominant species which cannot be adsorbed on the negatively
charged quartz particles. Hence, the turbidity removal efﬁciencies deteriorate.
In an electrocoagulation process, when the distance of the electrodes increases, the
electrical current is decreased since the potential drop is proportional to the
inter-electrode distance. Reducing this distance is of great importance for reducing the
electrolysis energy consumption especially when the conductivity is low. For this
reason, investigation of this parameter is helpful. When the distance of the electrodes
increased from 0.25 to 1.5 cm, the removal rate of the crystal violet progressively
decreases when the inter-electrode distance increases. When the inter electrode distance
is less than 0.5 cm (ied = 0.25) that speed decreases due to overvoltage phenomena.
These results are shown in Fig. 4.
Current density is the one of the important factors in electrochemical processes as it
determines the coagulant dosage rate, bubble production rate, size and growth of the
ﬂocks, which can affect the efﬁciency of the electrocoagulation.
0
2
4
6
8
10
0
5
10
15
iniƟal pH
Ɵme (min)
pH=3
pH=5
pH=7
pH=9
pH=10
Fig. 1. Evolution of initial pH over
time i = 250 A/m2; ied = 0.5 cm; con-
ductivity = 4.27 mS/cm; initial concen-
tration = 100 mg/L
0
20
40
60
80
100
0
5
10
15
c(mg/l)
Ɵme (min)
pH=3
pH=5
pH=7
pH=9
natural soluƟon pH=5.43
Fig. 2. Evolution of crystal violet concentration
over time with different initial pH; i = 250 A/m2;
ied = 0.5 cm; conductivity = 4.27 mS/cm; initial
concentration = 100 mg/L
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
21

Figure 5 shows that the current density between 100 and 200 A/m2, the removal
rate of the crystal violet is quite low. When a current density of 250 A/m2 is imposed,
this speed is signiﬁcantly improved.
NaCl is usually employed to increase the conductivity of the solutions to be treated
by electrocoagulation. Increase in salt concentration, increases the ion concentration in
the solution and hence reduces the resistance between the electrodes. Increase in salt
concentration decreases the cell voltage at constant current density and reduces the
power consumption in electrolytic cells.
To see the effect of solutions conductivity on dye removal various experiments
were performed using NaCl as the electrolyte in the range of 0.2–1.0 g/L. Figure 6
shows the results.
Fig. 3. Solubility diagram of aluminum
hydroxide
Al(OH)3(s)
considering
only
monomeric aluminum species
0
20
40
60
80
100
0
5
10
15
c(mg/L)
time (min)
ied=0.25
ied=0.5
ied=1
ied=1.5
Fig. 4. Evolution of crystal violet concen-
tration over time with different inter elec-
trode distance; i = 250 A/m2; pH = 5.43;
conductivity = 4.27 mS/cm; initial concen-
tration = 100 mg/L
0
20
40
60
80
100
0
5
10
15
c(mg/l)
time (min)
100A/m2
150A/m2
200A/m2
250A/m2
Fig. 5. Evolution of crystal violet concentra-
tion over time with different current density;
ied = 0.5 cm; conductivity = 4.27 mS/cm; ini-
tial concentration = 100 mg/L; pH = 5.43
0
20
40
60
80
100
0
5
10
15
C (mg/L)
time (min)
cond=0.9mS NaCl 0.2 g/L
cond=1.76mS NaCl 0.4 g/L
cond=2.48mS NaCl 0.6 g/L
cond=3.38mS NaCl 0.8 g/L
cond=4.27mS NaCl 1 g/L
Fig. 6. Evolution of crystal violet concen-
tration over time with different initial con-
ductivity; i = 250 A/m2; ied = 0.5 cm; initial
concentration = 100 mg/L; pH = 5.43
22
M. K. Mbacké et al.

When the initial conductivity of the solution increases, the removal rate of the crystal
violet increases also. For NaCl mass of 1 g/L (initial conductivity = 4.27 mS/cm) a very
signiﬁcant degradation is observed which does not go beyond.
3.2
Electrocoagulation Kinetics
Kinetics studies of treatment process have important role in determining the hydraulic
retention time in any reactor system to achieve desired removal [7]. So, rate constant is
very signiﬁcant in the design of wastewater treatment units. It is very essential to know
the type of reaction rates for design a wastewater treatment unit. Rate of reaction
describes the rates of change in concentration of reactant per unit time.
The kinetics of ﬁrst and second order have been studied in comparison with
experimental results. Figures 7 and 8 show the results obtained.
The results show that for concentrations up to 40 mg/L, the ﬁrst and second order
kinetics are valid. When moving at high concentrations (60 and 100 mg/L), the kinetics
of second order is most signiﬁcant coefﬁcients with higher correlation to 0.97.
The homogeneous kinetic ignores adsorption phenomena involved in the electro-
coagulation process. To analyze these effects, the pseudo second-order kinetics was
studied.
Considering the results obtained with the second order kinetics and considering that
thepollutantisgenerallyadsorbedatthe surface oftheﬂocks generated electrochemically,
critical analysis of the electrocoagulation of crystal violet dye reveals that there are two
separate processes taking place [7]:
0
1
2
3
4
5
0
5
10
15
ln(c)
time (min)
c=5mg/L
c=20mg/L
c=40mg/L
c=60mg/L
Fig. 7. Linear representation of the ﬁrst
order kinetics with variable initial concen-
trations: i = 250 A/m2; ied = 0.5 cm;
cond = 4.3 mS/cm y5 = −0,0855x + 1,5505
R2 = 0,9823; y20 = −0,0502x +
2,9482
R2 = 0,9796; y40 = −0,077x + 3,6238
R2 = 0,9812; y60 = −0,1136x + 3,7959
R2 = 0,8614; y100 = −0,1231x + 4,3433
R2 = 0,8986
0
0.2
0.4
0.6
0.8
0
5
10
15
1/c
Ɵme (min)
c=5mg/L
c=20mg/L
c=40mg/L
c=60mg/L
c=100mg/L
Fig. 8. Linear representation of the second
order kinetics with variable initial concen-
trations: i = 250 A/m2; ied = 0.5 cm;
cond = 4.3 mS/cm; y5 = 0,0345x + 0,1827
R2 = 0,9935; y20 = 0,0038x + 0,0505
R2 = 0,997; y40 = 0,0036x + 0,0239
R2 = 0,9963; y60 = 0,0054x + 0,0216
R2 = 0,9758; y100 = 0,0037x + 0,0112
R2 = 0,9844
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
23

– electrochemical process through which the metal ﬂocks are generated;
– physico-chemical process through which the efﬂuents are adsorbed on the surface of
the ﬂocks.
The present experimental data have been veriﬁed with the pseudo-second order
model.
This model supposes that two reactions occurs either in series or in parallel occur;
the ﬁrst one is fast and reaches equilibrium quickly and the second is a slower reaction
that can continue for a long period of time [8]. Figure 9 shows the result obtained.
Different regression coefﬁcient obtained with the pseudo second order kinetics are
shown in Table 1.
Figure 9 shows the experimental results which conﬁrm the homogeneous kinetics
and indicate that crystal violet adsorption follows pseudo second order. Table 1 shows
that adsorption coefﬁcients obtained experimentally are of the same order of grader
than those obtained with the model.
The effect of the initial concentration on the rate constant shows that the reaction
rate constant (k3) decreased signiﬁcantly for increasing initial crystal violet concen-
trations (Table 1). This may be due to the fact that for a given charge, the number of
hydroxides ﬂocks generated was insufﬁcient and an increase in the ratio of the initial
concentration to the amount of ﬂocks generated resulted in a reduction of the yield of
crystal violet removal [9].
0
1
2
3
4
8
28
t/
Ɵme (min)
c=20mg/L
c=60mg/L
c=100mg/L
Fig. 9. Linear
representation
of
the
pseudo second order adsorption kinetics
with
variable
initial
concentration
i = 250 A/m2; ied = 0.5 cm; conductiv-
ity = 4.3 mS/cm;
y20 = 0,1108x −
0,6431
R2 = 0,9979;
y60 = 0,0367x
0,3033 R2 = 0,9912; y100 = 0,0325x −
0,3281 R2 = 0,9932
Table 1. Adsorption
kinetic
model
parameters
obtained using pseudo second order model
Initial crystal
violet
concentration
(mg/L)
20
60
100
k3
0.0194
0.0044
0.0032
Cethe
9.025
27.25
30.77
Ceexp
10.98
26.55
30.33
R2
0.9979
0.9912
0.9932
24
M. K. Mbacké et al.

3.3
Adsorption Isotherm
Figures 10 and 11 give the results obtained with the representation of the linear model
of Langmuir and Freundlich.
Ce
CmaxL
¼
KLCe
1 þ KLCe
ð8Þ
Ce ¼ KFC1=mF
e
ð9Þ
where Ce(mg g−1) is the adsorbed amount at equilibrium, Ce is the equilibrium con-
centration of the adsorbate (mg.l−1), KL is Langmuir equilibrium constant and CmaxL the
maximum adsorption capacity (mg g−1).
KF is Freundlich constant and mF is the heterogeneity factor. The KF value is related
to the adsorption capacity; while the 1/mF value is related to the adsorption intensity.
The Langmuir model assumes a monolayer deposition on a surface with a ﬁnite
number of identical sites (Mckay et al. 1982) [10]. It is well known that the Langmuir
equation is valid for a homogeneous surface.
The Freundlich adsorption isotherm typically ﬁts the experimental data over a wide
range of concentrations. This empirical model includes considerations of surface
heterogeneity and exponential distribution of the active sites and their energies.
The Freundlich constant KF is a measure of the adsorption capacity. It represent the
content of pollutants in the adsorbent at equilibrium. A high KF reﬂects a signiﬁcant
adsorption capacity. Constant 1/mF is an index of afﬁnity crystal violet to aluminum
hydroxide. It is noted in the literature that this parameter is very often between 0.70 and
0.95 [10].
The regression lines obtained with the two models will allow to evaluate the
constants CmaxL ; KL;mF; KF:
y = 0,0151x + 0,0381
R² = 0,9516
0
0.1
0.2
0.3
0
5
10
15
1/Γe
1/Ce
Langmuir model
Fig. 10. 1/Ce = f(1/Ce) Langmiuir iso-
therm
ied = 0.5 cm;
Conductivity =
4.3 mS;
i = 250 A/m2
CmaxL ¼ 26:24
and KL = 2.524
y = 0,7447x - 3,6363
R² = 0,9894
-6
-5
-4
-3
-2
-1
0
-3
-2
-1
0
1
lnΓe (g/g)
lnCe (mg/L)
Freundlich model
Fig. 11. ln(Ce) = f(lnCe) Freundlich iso-
therm
ied = 0.5 cm;
Conductivity =
4.3 mS; i = 250 A/m2 1/mF = 0. KF =
0.02635 L/mg
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
25

Figures 10 and 11 showed that the correlation coefﬁcient R2 substantially equal to
0.99, the Freundlich constant KF = 26.35 L/mg and the value of the intensity of the
adsorption 1/mF = 0.74 obtained with the Freundlich model indicates that this model is
most suitable to describe the phenomena involved in the bleaching solutions crystal
violet by the process of electrocoagulation.
The models of Langmuir and Freundlich can be easily linearized to determine the
constants (CmaxL; KL;mF; KF). This linearization is however not as evident in other
models tested. And to assess the parameters of these models, the adsorption capacity of
the values obtained experimentally are subject to the tool solver excel. The solver tool
allows giving these approximate values to different parameters (A, B, a, b, KT, KRP,
mRP, mT, CmaxRP, CmaxT) to have a resolution models with theoretical adsorption
capacities close to those obtained by experiments.
The data obtained in Table 2 are used in the numerical calculation of Matlab
software to analyze the different models in combination with a variable order kinetics.
3.4
Variable Order Kinetic Model (VOK)
The variable order kinetic (VOK) gives a description of the electrocoagulation process
taking into account the variation of the mass of adsorbent over time. This model has
been tested successfully in the case of the removal of ﬂuoride electrocoagulation. Hu
et al. [11]. showed that the model successfully describes the ﬂuoride removal reaction,
except in a system in which the initial concentration of the acid is less than the initial
ﬂuoride concentration.
Essadki et al. [12] indicated that when the evolution of ﬂuoride content was
independent of stirring speed, experimental results shows that the kinetics of ﬂuoride
removal could be modelled using a variable-order-kinetic (VOK) approach coupled
with a Langmuir–Freundlich adsorption model.
These different forms that can take the crystal violet in solution are a model based
only on the homogeneous kinetics does not identiﬁes all phenomena involved. On the
other hand, the adsorption isotherms do not take into account the variation of the mass
of adsorbent which is proportional to the amount of dye removed. VOK. The model
appears to be best suited to study complex dyes such as crystal violet.
Table 2. Adsorption isotherm constants for crystal violet
Parameters
Langmuir Freundlich Fritz-Schlûnder Radke-Prausnitz Toth
Cmax (mg/g) 26.24
–
–
61.1998
0.1256
K (l/mg)
2.524
0.02635
–
0.8221
0.3483
A (l/g)
–
–
0.02927
–
–
B (l/g)
–
–
0.04285
–
–
a
–
–
0.8576
–
–
b
–
–
2.1359
–
–
m
–
1.3428
–
0.5276
0.9054
26
M. K. Mbacké et al.

Figure 12 shows that for low concentrations (c = 5 mg/L) the different models used
describe the experimental results but the Freundlich model provides a more accurate
superposition of theoretical and experimental data. Indeed is low when the concen-
tration monolayer adsorption is easily veriﬁable because the amount of adsorbent is
produced largely sufﬁcient.
When the crystal violet concentration is relatively high (100 mg/L), Fig. 13 shows
that the model Langmuir and Fritz Schlünder are not very suitable. Indeed the solute
content of the solution begins to constitute a brake for a monolayer adsorption with
energetically equivalent sites.
3.5
Statistical Analysis
Statistical analysis was performed according to the method called “student test on two
samples” [13].
tobs is called the test statistic and is expressed as follows:
tobs ¼ x  y
sxy
ð10Þ
sxy ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
p
nx

s2
p
ny
s
ð11Þ
s2
p ¼
Pnx
1 x  x
ð
Þ2 þ Pny
1 y  y
ð
Þ2
nx  1
ð
Þ þ ny  1


ð12Þ
0
1
2
3
4
5
0
10
20
30
concentraƟon (mg/L)
Ɵme (min)
iniƟal concentraƟon C=5mg/L
experience
VOK Langmuir
VOK Freundlich
VOK Radke–Prausnitz 
VOK Toth
VOK Fritz–Schlunder 
Fig. 12. Matlab resolution for different models
with low normalized concentrations ied = 0.5
cm;
Conductivity = 4.3 mS;
i = 250 A/m2,
C0 = 5 mg/L
0
20
40
60
80
100
0
20
40
60
80
Normalized concentraƟon (%)
Ɵme (min)
iniƟal concentraƟon C=100 mg/L
experience
VOK Langmuir
VOK Freundlich
VOK Radke-Prausnitz
VOK Toth
VOK Fritz-Schlûnder
Fig. 13. Matlab
resolution
for
different
models with high normalized concentrations
ied = 0.5 cm;
Cond = 4.3 mS;
i = 250
A/m2, C0 = 100 mg/L
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
27

x average experimental values for a given initial concentration; y averaging the
values obtained by modeling for a given initial concentration; sxy estimator of the
standard deviation of the difference x  y; s2
p total variance of the experimental values
and model values; nx number of experimental data; ny number data obtained by
modeling.
Following a risk percentage a, the Student test deﬁne a critical value ta which
depends on the number of group data and the total degree of freedom (df). The value ta
is obtained from the student table [14].
So for 2 data groups (experience and model) and a total variance based on the
number of samples in each model, Table 3 gives the results obtained. The risk we have
chosen is the same for all models its value is a = 0.05.
Decision is made as follow: if tobs
j
j [ ta it is considered that experimental values
and the values obtained with the model are considered too different to the model to be
upheld.
Table 3 conﬁrms the results seen in Figs. 12 and 13. For low, concentrations the
values observed in the different models are less than the values. All the isotherms can
be validated in this range of concentrations with a better result for Freundlich isotherm.
For higher concentrations, Freundlich isotherms, RadkePrausnitz and Toth verify
experimental results with ever better correlation for the Freundlich isotherm.
At the two extremes of concentrations studied, the model Freundlich works best.
This shows that the VOK model in association with Freundlich isotherm best describes
the electrocoagulation process in the case of crystal violet removal.
The low student has been expanded to different initial concentrations in relation to
the Freundlich isotherm and variable order kinetic (VOK). Table 4 gives the results
obtained.
The results follow a relationship between the intensity of the adsorption and the
initial concentration. The parameter 1/mF in which the model Freundlich represents the
order of the reaction is variable depending on the initial concentration (Fig. 14).
The obtained correlation coefﬁcient shows a linear relationship between the
intensity of adsorption and the initial concentration and assumes that different sites with
several adsorption energies are involved. These results show that the VOK model can
be used to the kinetic removal adsorption of crystal violet in wastewater by
electrocoagulation.
Table 3. Statistical analysis with different models
Model
Initial concentration = 5 mg/L
Initial concentration = 100 mg/L
ddl
a
ta
tobs
j
j
ddl
a
ta
tobs
j
j
Langmuir
14
0.05
1.761
1.157
18
0.05
1.734
6.29
Freundlich
10
0.05
1.812
0.0127
18
0.05
1.734
0.142
RadkePrausnitz
14
0.05
1.761
0.0361
18
0.05
1.734
0.530
Toth
14
0.05
1.761
0.3297
18
0.05
1.734
1.09
Fritz-Schlûnder
14
0.05
1.761
0.1103
18
0.05
1.734
6.941
28
M. K. Mbacké et al.

4
Conclusion
This work was a ﬁrst step to improve the knowledge regarding the modeling of the
kinetic of crystal violet dye removal by electrocoagulation (EC) with aluminum as
sacriﬁcial electrodes and stainless steel as cathode. The EC method applied in this
study provided a sensitive, rapid and reliable technique for the removal of crystal violet
in a synthetic wastewater. By applying linear methods, the mechanism of EC was
modeled using second order, and pseudo-second order models and it was shown that
the EC kinetic followed pseudo-second order kinetic and that the reaction rate constant
was inﬂuenced by the initial concentration. The metal hydroxides generated by EC can
efﬁciently remove crystal violet by adsorption, and the EC process was modeled using
adsorption isotherm models, showing that the Freundlich model accurately described
experimental data. The use of the Variable Order Kinetic (VOK) model which Fre-
undlich equation and the statistical analysis has shown that this model ﬁtted well the
experimental data with a correlation coefﬁcient of 0.9894. The elimination kinetics of
crystal violet by electrocoagulation is therefore variable order kinetics.
The linear relationship between the initial concentration and the intensity of the
adsorption conﬁrms the variability of the order of the reaction and shows efﬁcient
adsorption regardless of the initial concentration.
Table 4. Statistical analysis with VOK Freundlich model for different initial concentrations
Initial concentration (mg/L) 5
10
15
20
60
100
Parameters
VOK Freundlich
ddl
10
8
8
12
14
18
a
0.05
0.05
0.05
0.05
0.05
0.05
ta
1.812
1.860
1.860
1.782
1.761
1.734
tobs
j
j
0.0127 0.0325 0.0214 0.3306 0.1587 0.142
mF
1.7818 1.8428 1.9028 1.9928 2.5728 3.3228
0
0.2
0.4
0.6
0
20
40
60
80
100
1/mF
iniƟal concentraƟons (mg/L)
order of reacƟon
Fig. 14. Linear representation of the Freundlich coefﬁcient as a function of the initial
concentrationsied = 0.5 cm;
Conductivity = 4.3 mS;
i = 250 A/m2
y = −0,0027x + 0,566
R2 = 0,9918
Modeling the Crystal Violet Kinetics Removal by Electrocoagulation Process
29

References
1. Canizares, P., Martınez, F., Rodrigo, M.A., Jimenez, C., Saez, C., Lobato, J.: Modeling of
wastewater by electrocoagulation processes part II: application to dye-polluted wastewaters
and oil-in-water emulsions. Sep. Purif. Technol. 60, 147–154 (2008)
2. Ghosh, D., Medhi, C.R., Solanki, H., Purkait, M.K.: Decolorization of crystal violet solution
by electrocoagulation. J. Environ. Prot. Sci. 2, 25–35 (2008)
3. Dubey, S.K., Srivastava, P., Verma, A., Rajor, A.: Solar photo-catalytic treatment of textile
wastewater for biodegradability enhancement. Int. J. Environ. Eng. 1(2), 152–164 (2009)
4. https://en.wikipedia.org/wiki/Crystal_violet. Accessed 15 July 2015
5. Mameri, N., Yeddou, A.R., Lounici, H., Belhocine, D., Grib, H., Bariou, B.: Deﬂuoridation
of septentrional Sahara water of North Africa by electrocoagulation process using bipolar
aluminium electrodes. Water Res. 32, 1604–1612 (1998)
6. Chen, G., Yue, P.L.: Electrocoagulation and electroﬂotation of restaurant wastewater.
J. Environ. Eng. 126(9), 858–886 (2000)
7. Behnamfard, A., Salarirad, M.M.: Equilibrium and kinetic studies on free cyanide adsorption
from aqueous solution by activated carbon. J. Hazard. Mater. 170, 127–133 (2009)
8. Chithra, K., Balasubramanian, N.: Modeling electrocoagulation through adsorption kinetics.
J. Model. Simul. Syst. 1, 124–130 (2010)
9. Ouaissa, Y., Chabani, M., Amrane, A., Bensmaili, A.: Removal of tetracycline by
electrocoagulation: kinetic and isotherm modeling through adsorption. J. Environ. Chem.
Eng. 2, 177–184 (2014)
10. Mckay, G., Blair, H.S., Gardener, J.R.: Adsorption of dyes on Chitin. I. Equilibrium studies.
J. Appl. Polym. Sci. 27, 3043–3057 (1982)
11. Hu, C.-Y., Lo, S.-L., Kuan, W.-H.: Simulation the kinetics of ﬂuoride removal by
electrocoagulation (EC) process using aluminum electrodes. J. Hazard. Mater. 145, 180–185
(2007)
12. Essadki, A.H., Gouricha, B., Azzib, M., Vial, C., Delmasd, H.: Kinetic study of
deﬂuoridation of drinking water by electrocoagulation/electroﬂotation in a stirred tank
reactor and in an external-loop airlift reactor. Chem. Eng. J. 164, 106–114 (2010)
13. Méthodologie expérimentale: quelques tests statistiques Audrey Dussutour, Christian Jost 2,
pp. 1–2, janvier 2005
14. Baldé, M.: Cours statistique chapitre 4 tests d’hypothèses, p. 34. Université Cheikh Anta
Diop
30
M. K. Mbacké et al.

Study of the Correlation Between the Dust Density
Accumulated on Photovoltaic Module’s Surface and Their
Performance Characteristics Degradation
Ababacar Ndiaye1,2(✉), Cheikh M. F. Kébé2, Boudy Ould Bilal2,4, Abdéraﬁ Charki3,
Vincent Sambou2, and Papa A. Ndiaye2
1 Département de Physique, UFR – Sciences et Technologies, Université Assane Seck
de Ziguinchor, BP 523, Ziguinchor, Senegal
ab.ndiaye@univ-zig.sn, ababacar.ndiay@gmail.com
2 Centre International de Formation et de Recherche en Energie Solaire (CIFRES),
Ecole Supérieure Polytechnique – UCAD, BP 5085, Dakar-Fann, Senegal
3 Université d’Angers–ISTIA–LASQUO, 62 Avenue Notre Dame du Lac, 49000 Angers, France
4 Ecole des Mines de Mauritanie (EMiM), BP 5259, Nouakchott, Mauritania
Abstract. One of the major constraints related to the operating conditions of
photovoltaic systems in the Sahelian environment is relative to the deposits of
dust on the surface of the PV modules. Indeed, the Sahel is characterized by
frequent and permanent sandstorm that impact on its strong solar potential.
However, opinions are still divided on the signiﬁcant impact of dust on photo‐
voltaic modules production. This paper deals with the correlation between the
density of deposited dust on the surface of crystalline silicon photovoltaic
modules and the impact on their performances. This study is carried out on two
diﬀerent modules a monocrystalline and a polycrystalline technology. It focuses
on the open-circuit voltage (VOC), the short-circuit current (Isc), the ﬁll factor (FF)
and more particularly on the maximum power (Pmax). This paper presents the
methodology and the experimental study used to measure the environmental
parameters (irradiation, temperature, and humidity), the density of deposited dust
and the performance characteristics (ISC, VOC, FF and Pmax). Finally, the results
of the correlation between the density of deposited dust and the performance
characteristics of PV modules are presented.
Keywords: Module photovoltaic · Degradation · Dust · Correlation
1
Introduction
Accumulation of dust on the surface of PV module can greatly affect their performance
especially in desert areas. However, the desert regions are the most suitable for the produc‐
tion of photovoltaic electricity because of the abundant availability of solar radiation
throughout the year. Currently, the idea of setting up large photovoltaic fields in the Sahe‐
lian countries with a view of exporting the energy produced to other countries is under
discussion [1]. The accumulation of dust on the surface of the PV modules depends on
various parameters such as the tilt angle, the type of installation, the humidity.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 31–42, 2018.
https://doi.org/10.1007/978-3-319-72965-7_3

Hottel and Woertz were among the ﬁrst to work on the impact of dust on solar systems
[2]. They recorded a maximum performance degradation of 4.7%, with an average inci‐
dent solar radiation loss of less than 1%. Another study on the accumulation of dust on
a photovoltaic system installed in a village near Riyadh showed a 32% reduction in
performance after eight months of exposure without cleaning the modules [3]. A reduc‐
tion of photovoltaic power of 17% on modules installed in Kuwait City was noted after
six days without cleaning the modules [4]. In addition, the study also indicated that the
inﬂuence of dust on PV performance would be higher during the dry seasons. Another
study on the eﬀects of dust on PV modules in Palo Alto, California, has shown 2%
reduction in the short circuit current Compared to the initial Isc of the module [5, 6].
Shaharin et al. have found that the power reduction due to the deposition of dust on the
PV module can be up to 18% [7].
Loss of power due to dust depends on the type of dust; the time since the last rain
and the cleaning schedule [8]. Detrick et al. Argue that the average annual power losses
of a photovoltaic module ranges from 1% to 4% [9]. In areas where rainfall is frequent,
it has been shown that rain can clean the PV modules at a performance restoration point
of 1% loss on full power [10]. In a more recent study done in Crete, the average annual
losses of power due to dust was 5.86%, with 4% to 5% during the winter and 6% to 7%
in the summer [8]. A study was carried out on photovoltaic systems installed in Egypt
comparing the energy produced by a clean module, a module that was exposed without
being cleaned for a period of one year and a module that was exposed to dust but cleaned
every two months. The results showed that the module which remained one year without
being cleaned produces 35% less energy than the clean module. The one cleaned every
two months produces 25% less compared to the clean module [11].
It would be interesting for photovoltaic system users, to know how often the module
needs to be cleaned. If frequent cleaning cannot be done, it will be important to quantify
the losses of performance due to dust to take this into account. The study of dust eﬀects
on PV modules would also be useful to determine the technology to use, the module
types and the implementation. Establishing the correlation between the performance
degradation and the density of deposited dust on PV modules allows answer the question
concerning the recommended frequency of cleaning the modules.
2
Materials and Methods
2.1
Methodology
This study of the correlation between the density of deposited dust and the variation of
the performance characteristics is performed using new solar modules, made by one
manufacturer but with diﬀerent technologies.
After being thoroughly cleaned, the two modules have been characterized before
being exposed under natural sunlight within the same conditions (sunshine, temperature,
humidity). The measurements are carried out weekly for a period of six weeks.
Throughout the week, average environmental parameters such as wind speed, sunshine,
ambient temperature and humidity are measured.
32
A. Ndiaye et al.

By the end of the week, the performance characteristics of the modules are measured
under the standard testing conditions with the PV module analyzer (IV-400).
The density of deposited dust on each module is also determined by a method which
we present in the following section.
Thus, the value of each parameter is determined, taking into account the reference
values of the parameters measured on the clean module at the beginning of the experi‐
ment. This normalized value, which reﬂects the variation of the parameter, is correlated
with the density of the deposited dust on each module. The summary of the diﬀerent
stages of the methodology adopted is presented in Fig. 1.
Fig. 1. The proposed methodology.
2.2
Dust Density Determination
Two diﬀerent options are available to measure the density of the deposited dust on the
PV modules. The ﬁrst is by determining the cleaned module weight at the beginning of
the experiment and that of the dusty module at the end of the experiment. The diﬀerence
between these two measurements gives the amount of deposited dust on the surface of
the module. The ratio between the weight of the dust on the surface and the module’s
Study of the Correlation Between the Dust Density Accumulated on PV Module’s
33

area gives the density of the dust deposits [12]. However, this technique requires long-
term exposure which would allow collecting a signiﬁcant amount of measurable dust
relative to the weight of the module.
We opted for the second method which consists in using two glass slats of 67 g
exposed side by side of each module and with the same inclination as this one. The slats
are identical with a surface of 10 cm2 i.e. 10 cm × 10 cm. At the end of each week, the
slats are weighed with a high precision balance. The weight of the deposited dust on the
module is obtained by comparing the weight of the slat with dust and the weight of the
clean slate. This gives the density of the deposited dust per square centimeter (cm2).
2.3
Presentation of the Experimental Platform
The technical speciﬁcations of the two modules used in this study are given in Table 1.
The use of monocrystalline and polycrystalline modules also makes it possible to study
the correlation according to the technology.
Table 1. Technical speciﬁcations of the modules.
Modules
Technology
Manufacturers
References
Parameters
Values
1
Monocrystalline
TENESOL
TE55-36P
Pmax (W)
55
Vmax (V)
17,79
Imax (A)
3,1
Vco (V)
22
Icc (A)
3,3
FF (%)
75
2
Polycrystalline
LORENTZ
LTZ50E
Pmax (W)
50
Vmax (V)
17,1
Imax (A)
2,9
Vco (V)
21,7
Icc (A)
3,2
FF (%)
72
The characteristics of the high precision Kern balance used to obtain the dust density
are also given in Table 2. This balance has a lid to prevent losses during the quantiﬁcation
of the dust collected with the lamellae.
Table 2. Characteristics of the balance used for the weighing of dust deposits.
Manufacturer
Reference
Range
Precision
Resolution
Speciﬁcity
Kern
Kern ABS/ABJ
1 mg–220 g
1 mg
0, 1 mg
Automatic taring
Figure 2 shows the experimental platform including the two modules and the
lamellae exposed for dust collection.
34
A. Ndiaye et al.

Fig. 2. Experimental platform.
Each module is associated with two glass slides for dust collection. A reference cell
is installed on the platform to measure the overall incident solar radiation at the surface
of the modules. The PV module analyzer “IV-400” is used to measure the performance
characteristics of the modules.
2.4
Environmental Parameters During the Experimental Period
The experiment conducted to study the correlation between the dust density and the
performance characteristics of the poly and monocrystalline modules lasted six weeks.
The main environmental parameters measured during this period are presented in
Table 3.
For such a study, it is necessary to determine the environmental parameters during
the experiment time to analyze and give the interpretation of the correlation results.
Study of the Correlation Between the Dust Density Accumulated on PV Module’s
35

Table 3. Characteristic parameters of the six measurement weeks
No.
week
Wind speed (m.s−1)
Ambient temperature (°C)
Relative humidity (%)
Irradiation
(W.m−2)
moy
max
min
moy
max
min
moy
max
min
moy
Max
1
1,2
4,7
0,2
25,19
37,7
20
73,8
98,9
24,3
632
710
2
1,58
4,9
0,02
26,89
36,1
22,3
70,3
91,4
27,2
710
721
3
1,98
5,3
0,02
27,3
34
23,5
70,8
90
39,8
702
735
4
2,13
5,2
0,1
28
35
24
73
91
35
812
820
5
1,8
4,8
0,3
27
34
23
68
87
40
708
790
6
1,6
4,9
0,02
23
34
27
72
90
39
835
937
3
Results and Discussions
3.1
Evaluation of Dust Density
The amount of dust deposited on each slide is determined at the end of each week. The
quantity of dust on each module is given by the mean of the deposits on the two lamellas
of ten square centimeters (10 cm2) associated with it.
We assume that the distribution of dust on the surface of the module is homogeneous.
The following ﬁgure (Fig. 3) shows the evolution of the dust density at the surface
during the period for each module.
Fig. 3. Variation of the dust density for six weeks.
The accumulation of dust at the surface of the modules induces the increase of its
density over time. The evolution of this density is highly dependent on environmental
parameters such as wind speed, ambient temperature and humidity.
36
A. Ndiaye et al.

Correlation between the dust density and the variation of the photovoltaic modules
characteristics is studied in the following paragraphs.
3.2
Variation of Performance Characteristics with Dust Density
To study the performance characteristics variation as a function of the dust density,
standardized values should be considered. For each parameter, the value measured for
the clean module before starting the experiment is taken as a reference in the normali‐
zation. The normalized value is given by the ratio between the value relative to the
module with a deposit of dust and the one relative to the clean module.
All measurements are recorded under the standard testing conditions using the
analyzer.
The standardized variations of the diﬀerent performance characteristics for the two
technologies are presented in the following ﬁgures (Figs. 4, 5, 6 and 7).
Fig. 4. Variation of the normalized short-circuit current as a function of the dust density.
Fig. 5. Variation of the open circuit voltage normalized to the density of dust.
Study of the Correlation Between the Dust Density Accumulated on PV Module’s
37

Fig. 6. Variation normalized power as a function of the dust density.
Fig. 7. Variation of standardized ﬁll factor as a function of dust density.
It is noticed that the standardized values of the short-circuit current and the power
decrease with the dust density increase for both technologies. This decrease reﬂects the
inﬂuence of the dust on the eﬃciency of photovoltaic modules. We note that this decline
progresses quite rapidly, in six weeks of exposure without cleaning the modules:
– The short-circuit current has decreased by 30% for the monocrystalline and 34% for
the polycrystalline compared to the clean modules values for a dust density of
30.5 mg/cm2 and 21.6 mg/cm2, respectively.
– The maximum power decrease is about 26% for the monocrystalline and 40% for the
polycrystalline compared to the clean module characteristics for the same densities
of dust.
The open circuit voltage and the ﬁll factor remain constant despite the increase of
dust density for both technologies.
This result relative to the open circuit voltage is in phase with those of [10, 13]
according to which this is not aﬀected by the accumulation of dust.
We shall hereafter study the link between these variations and the density of depos‐
ited dust.
38
A. Ndiaye et al.

3.3
Correlation Between the Deposited Dust Density and the Modules
Characteristics Degradation
We have studied the most appropriate correlations between the modules performance
characteristics variation and the density of dust. The Correlation was carried out for all
parameters by exponential and polynomial regressions.
Figures 8, 9, 10 and 11 show the diﬀerent correlations for each characteristic and
technology.
Fig. 8. Correlation between standardized short-circuit current and dust density by exponential
and polynomial regressions.
Fig. 9. Correlation between standardized open circuit voltage and dust density by exponential
and polynomial regressions.
Study of the Correlation Between the Dust Density Accumulated on PV Module’s
39

Fig. 10. Correlation between standardized power and dust density by exponential and polynomial
regressions.
Fig. 11. Correlation between standardized ﬁll factor and dust density by exponential and
polynomial regressions.
The objective is to determine the best regression scenario that elucidates more the
relationship between the parameter in question and the dust density.
The previous graphs (Figs. 8, 9, 10 and 11) highlight the regression curve, its equation
and the determination coeﬃcient for each characteristic related to the two technologies.
Table 4 summarizes the main ﬁndings of this study.
Table 4. Determination coeﬃcient of exponential and polynomial regressions.
Regressions
Determination coeﬃcient (R2)
Monocrystalline
Polycrystalline
Iccdust
Iccclean
Vcodust
Vcoclean
Pmaxdust
Pmaxclean
FFdust
FFclean
Iccdust
Iccclean
Vcodust
Vcoclean
Pmaxdust
Pmaxclean
FFdust
FFclean
Exponential
0,880
0,007
0,893
0,078
0,992
0,002
0,972
0,011
Polynomial
0,993
0,522
0,986
0,461
0,995
0,018
0,990
0,110
40
A. Ndiaye et al.

This experiment allows to propose a relation between the characteristics’ variation
and the density of dust.
The short-circuit current and the power which are the more sensitive characteristics
to the deposition of dust have a determination coeﬃcients very close to one with the
polynomial regression as shown in Table 4.
The open circuit voltage and the ﬁll factor have low correlation coeﬃcients for both
regressions.
However, the previous study [11] on the impact of dust, with a PV module exposed
during one year without being cleaned, showed that the ﬁll factor could be degraded to
a maximum of 18% Module. Thus, we may think that six weeks of exposure are too
short to detect a change in the ﬁll factor.
4
Conclusion
This paper deals with the experimental study of the correlation between the variation of
the performance characteristics of photovoltaic modules and the density of dust. It
proposes a method for dust density determination assuming that the distribution of dust
on the module’s surface is homogeneous.
The correlation establishment showed that the variation of the short-circuit current
and the maximum power as a function of the dust density follows a polynomial regres‐
sion law with a very good coeﬃcient of determination (0.99%) for the Two technologies.
The study carried out in this paper is contribution for PV modules cleaning frequency
determination in the study area.
References
1. EPIA: European Photovoltaic Industry Association: Global Market Outlook for Photovoltaics
until 2015 (2011)
2. Hottel, M.C., Woertz, B.B.: Performance of ﬂat plate solar heat collectors. ASME Trans.
64, 91–104 (1942)
3. Salim, A., Huraib, F., Eugenio, N.: PV power-study of system options and optimization. In:
Proceedings of the 8th European PV Solar Energy Conference, Florence, Italy (1988)
4. Wakim, F.: Introduction of PV power generation to Kuwait. Kuwait Institute for Scientiﬁc
Researchers, Kuwait City (1981)
5. Kaldellis, J.K., Kapsali, M.: Simulating the dust eﬀect on the energy performance of
photovoltaic generators based on experimental measurements. Energy 36(8), 5154–5161
(2011)
6. Katz, G.B.: Eﬀect of Dust on Solar Panels (2011). www.gregorybkatz.com/Home/eﬀect-of-
dust-on-solar-panels
7. Shaharin, S.A., Haizatul, H.H., Leh, N.S.H.N., Razali, M.S.: Eﬀects of dust on the
performance of PV panels. World Acad. Sci. Eng. Technol. 58, 588–593 (2011)
8. Kymakis, E., Kalykakis, S., Papazoglou, T.M.: Performance analysis of a grid connected
photovoltaic park on the Island of Crete. Energy Convers. Manag. 50(3), 433–438 (2009)
9. Detrick, A., Kimber, A., Mitchell, L.: Performance evaluation standards for photovoltaic
modules and systems. In: Proceedings of the 31st IEEE Photovoltaics Specialists Conference,
Lake Buena Vista, USA, January 2005, pp. 1581–1586 (2005). ISBN 0-7803-8707-4
Study of the Correlation Between the Dust Density Accumulated on PV Module’s
41

10. Ndiaye, A., Kébé, C.M., Ndiaye, P.A., Charki, A., Kobi, A., Sambou, V.: Impact of dust on
the PV modules characteristics after an exposition year in Sahelian environment: the case of
Senegal. Int. J. Phys. Sci. 8(21), 1166–1173 (2013). Academic Journals
11. Ibrahim, M., Zinsser, B., El-Sherif, H., Hamouda, E., Makrides, G., Georghiou, G.E.,
Schubert, M., Werner, J.H.: Advanced photovoltaic test park in Egypt for investigating the
performance of diﬀerent module and cell technologies. In: Proceedings of the 24th
Symposium Photovoltaic Solar Energy, Staﬀelstien, Germany, March 2009
12. El-Shobokshy, M.S., Hussein, F.M.: Eﬀect of dust with diﬀerent physical properties on the
performance of photovoltaic cells. Sol. Energy 51(6), 505–511 (1993)
13. Ndiaye, A., Kébé, C.M.F., Charki, A., Ndiaye, P.A., Sambou, V., Kobi, A.: Degradation
evaluation of crystalline-silicon photovoltaic modules after a few operation years in a tropical
environment. Sol. Energy 103, 70–77 (2014). Elsevier ScienceDirect
42
A. Ndiaye et al.

Education

Strengthening Partnerships Between Universities and
SMEs Within the Open Innovation Framework
Jean-Charles Cadiou1(✉) and Emmanuel Chene2
1 Faculté des Sciences et Technologies, LS2N, Université de Nantes, Nantes, France
jean-charles.cadiou@univ-nantes.fr
2 Institut d’Administration des Entreprises, LEMNA, Université de Nantes, Nantes, France
emmanuel.chene@univ-nantes.fr
Abstract. Since about twenty years, the theme of the innovation marks the
public’s policies of the European Union. In this context, on one hand, universities
are incited to value the result of their research; on other hand, SMEs are encour‐
aged to strengthen their capacity to innovate. The common sense might suggest
just that these two types of organizations just have to work together remove a
mutual beneﬁt from it. Unfortunately, relationships between academic and socio-
economic world are struggling to establish themselves and remain largely focused
on large companies. Studies led on collaborations universities-SMEs or on tech‐
nology transfer from University towards SMEs draw up balance sheets more than
very reserved. This state of fact continues in spite of the various structures of
intermediation set up by the public authorities since more than a dozen years. It
is therefore urgent to consider new forms of organization. In this context, this
article presents an original structuring of the valuation of the research in of a
university in order to answer the question.
Keywords: Open innovation · Organizational change · University and SMEs
1
Introduction
Since twenty years, public policies of European countries are strongly involved on the
theme of innovation as a vector of the economic development. In this ﬁeld, becoming
particularly aware of the economic importance of SMEs and their ability to create and
develop jobs, Governments decided to promote innovation in SMEs.
In order to act in this ﬁeld two axes of stimulation were set up. First, universities
have been encouraged to disseminate and enhance the results of their research [1].
Second, SMEs were encouraged to strengthen their capacity of innovation to better face
the technical and economic changes and to contribute to the creation of new jobs [2]. In
this beneﬁcial framework, common sense would have wanted that universities and SMEs
are moving closer to work together. Each entity would have its role in the economic
chain. Universities would spread their knowledge and would transfer their technologies
towards SMEs which would incorporate them into their new products commercially
meaningful. This virtuous partnership to spread to the civil society the innovations, to
participate in their own economic development and thereby feeds the needs of the citizen.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 45–55, 2018.
https://doi.org/10.1007/978-3-319-72965-7_4

Thus the joint eﬀorts of both sides would be beneﬁcial to the development of the
economic development and in the employment in the region where they are located [3].
This essential notion would so allow to reveal universities as strong actors of the regional
economic development.
In reality, in France, studies show that less than 10% of innovative SMEs used regu‐
larly to external information sources [4]. From both SMEs and universities, several
reasons explain this situation. SMEs have opposing characteristics. On positive side,
they are predisposed to innovation because their size allows them to have ﬂexibility in
their organization, a good reactivity as well as a great proximity to their customers. But
on negative side, they are penalized by the lack of human and ﬁnancial resources. Add
to these elements, heads of the company, yet at the origin of new ideas, are too often
involved in short term. That strongly impacts their power of innovation [5–7]. They have
diﬃculty to grasp the necessary changes to adapt them to the new key success factors.
SMEs lean essentially on their internal design skills. These are not enough diﬀerenti‐
ating, and includes very rarely skills outcome of research [8]. The low absorption
capacity of the new knowledge of SMEs further complicates this aspect. Focused on
their technological business [4], SMEs are struggling to evolve their basic skills. Aware
of these shortcomings, managers of SMEs complain that there is no system of support
simpler innovation for them [9] and does not turn of themselves towards the universities.
Universities are deeply rooted in their historical missions: teaching and research.
The United States aware of the decline in their economic competitiveness voted in 1980
the Bayh-Dole act that deeply changed American public research. Universities were
encouraged to commercialize the technologies developed from their research results and
to hold patents. Technology transfer oﬃces were then created for this purpose [10].
Gradually an economy of science settled down in the service of the economic develop‐
ment of the country [11]. In the 1990s, Europe has decided to follow the same direction
and to give the universities a third mission: the valorization and dissemination of
research results. Because of these changes, universities were encouraged to take part in
the development of the capacities of innovation of the SMEs. However fearing that the
marketing becomes the goal of the academic research [1], oppositions have emerged.
Policy choices had diﬃculty in being set up because of the culture shock between the
functioning of researchers and business leaders. Indeed the objectives of researchers are
in connection with their intellectual interest and recognition of their scientiﬁc
community. They are more attracted by high-tech projects than by the real needs of
SMEs, especially when the latter are of low-technology level. Recognition of academics
is based on the scientiﬁc notoriety while the recognition of the head of SMEs confronts
in the economic results. This cultural gap has not favored the emergence of fruitful
exchanges.
So, generally, SMEs prefer to innovate in house so going without the diﬀerentiating
impact that research could bring to him. Even if overall SMEs innovate, they are very
far from to integrate innovation at the heart of their strategy. Truly innovative SMEs
represent only a small part of all SMEs. So, the role of each of both entities remains
regrettably separated. The zone of co-creation, yet essential to the intersection of inter‐
ests, and crucial for the cross-fertilization, remains empty. It raises a real problem for
46
J.-C. Cadiou and E. Chene

the development of the territories even though the University could fully contribute to
their development.
In order to promote the innovation in SMEs and ensure a rapprochement with
universities, multiple initiatives to bring the SMEs and the universities closer were
launched in the years 1980 to 1990, and then in the 2000s. The national and regional
public policies have created various structures of intermediation between the academic
and the economic worlds. Without exhaustibility, we can quote the technological plat‐
forms, the incubators, and technology parks ﬁrst. In a second time, in France, came the
poles of competitiveness and the thematic research institutes and the regional poles of
innovation. The balance sheets of its experiences [5, 12–14], realized decade after
implementations are very reserved. The main criticism concerns the fact that the struc‐
tures settled on the territories have not really contributed to the expected economic
development. Similarly the more qualitative beneﬁts of these structures - learning
networking, mutual conﬁdence between stakeholders within a territory, intensity of
informal trade - have not suﬃciently valued. They are always very diﬃcult to quantify.
In this framework, this paper presents the initiative of a French University (the
Université de Nantes) that decided to directly answer the problem. It has implemented
its own structure of interfacing with the socioeconomic world which is both totally in
adequacy with the needs of SMEs and in line with the expectations of researchers. In
the spirit of entrepreneurial University [10], the University of Nantes created, inside a
private subsidiary company, a new profession, that of “technology maker” or “maker of
innovation” at the conﬂuence of science and the economic development of companies
and of its territory.
2
The University Group a New Concept
Universities and particularly large multidisciplinary universities with their numerous
laboratories and teaching departments represent a great potential of knowledge and
know-how very important for a territory. However this great wealth is also hard to read
by all of the socio-economic world. In order to actually serve the economic development
and allow companies and especially SMEs to better understand what they can oﬀer, the
universities must organize themselves clearly. It is necessary that the universities clearly
present their technology oﬀerings and services that implement them to be able to work
eﬀectively with the companies. The details and internal structuring of universities do
not interest companies and have to stay in background. Only, reputation and scientiﬁc
radiation of the University are enough to attract the interest of companies. Universities
must therefore only present the answers they oﬀer as do all economic providers to
companies, because only the technical eﬃciency interested the companies. The Univer‐
sity of Nantes do that around the concept of the university group.
2.1
Global Context
The academic researcher work for a long time with the companies. However in most
cases this is collaborations through collaborative research with major companies. These
Strengthening Partnerships Between Universities and SMEs
47

actions are often conducted in a totally independent manner by diﬀerent researchers
without clear organization within the University. This creates a fuzzy on what is actually
carried out. This led the company to think that partnerships with the University can be
done only within the strict framework of the research. That’s why, academics do not
appear as real economic allies to companies and notably SMEs who do not search
knowledge and promising innovative solutions. Unlike large companies seeking science
in collaboration, SMEs search only mature technologies and quick answers to their
market.
Of course the historical profession of the University is the creation of new knowledge
and collaborative research work is a voice to allow the transfer of research results. Now,
in this context, regularly, universities share IP with companies and thus ensure ﬁnancial
returns. But the universities should not settle only for those activities which are within
this framework mainly for large companies which in France represent only 5–6% of the
socio-economic world. In keeping with their mission of valorisation, it must be able to
support the needs of all of the companies. In this context, it must be able to implement
the structure necessary to make sure that all companies can ﬁnd the solutions they need.
There is a speciﬁc scale to set the level of technological maturity of a technology. It
is the TRL scale (acronym for Technology Readiness Level) with 9 levels. This scale is
now also used to estimate the levels where is an innovation vis-à-vis the market. A
project on level ranging from TRL 1 to TRL 3 corresponds to a research activity. Levels
from the TRL 4–6 match to the level of high R&D. Finally, levels of TRL 7 to 9 corre‐
sponds to technological levels. The TRL 9 indicates a technology is ready to be launched
on the market. The products are ready to be marketed.
With this scale we can also deﬁne the needs of diﬀerent types of companies. Thus,
a large company will come to work with a laboratory on levels that can be of level 2 or
3. That is to say in the upstream phase of creation of knowledge. These companies
typically own in-house service that have the capacities to understand the research results
and to conduct R&D activities. Of course they have all the downstream engineering
services to transform the results of R&D into products (TRL 6 to 7). The smallest but
very innovative companies usually have an R&D department and therefore have in-
house services to work with a laboratory at the R&D level (usually on TRL 5 or 6). And
to complete the innovative classic SMEs own only an internal engineering service
working from the level of TRL 7. The rest of the SMEs are generally equipped with a
technological service intervening on the last levels 8 and 9, to adapt their products.
According to this description, it is clear that if a university wants to be able to work
with all the companies, it must be able to carry out its missions on all the levels of the
scale. Obviously, through its laboratories, universities master TRL 1 to 3 perfectly and
for a multidisciplinary university in many scientiﬁc ﬁelds. By extrapolation, and even
if this is not their professional priority, during a short period, researchers can also work
on TRL levels 4 to 6, notably in the collaborative framework of open innovation. It is
therefore clear that the challenge for universities is to be able to acquire a complementary
structure that will ensure, alongside researchers, levels TRL 7 to 9 and to manage
permanently the activities on TRL 4 to 6. The University of Nantes set up such organ‐
ization within the framework of its concept of university group. Thanks to that, it can
now work with all companies whatever their needs. Its activities with companies can go
48
J.-C. Cadiou and E. Chene

from the research to the realization of technological solutions relying on all its potential
of knowledge. The university is now becoming a fully actor in regional economic devel‐
opment. The following sections describe the global organization and the speciﬁc struc‐
ture that was set up.
2.2
The Approach of the University of Nantes and Global Structuration
The University of Nantes is a large multidisciplinary university covering the major
disciplinary ﬁelds of science and technology, health, humanities and social sciences,
and law economics management. Thanks to this wealth of knowledge and following
public incentives previously mentioned, the University should have become one of the
major players in the regional economy. However, about ten years ago, the University of
Nantes became aware of the very low impact which it had on the regional economy and
in particular the SMEs that surrounded it and which represent more than 92% of the
companies in the territory. Although the relations between the university and the major
groups were still to be strengthened, its relations with SMEs were almost entirely to be
created.
On the strength of this observation, to be able to set up a real dynamic of relations
with all companies, the University of Nantes decided to deploy a global strategy. For
that, it decided to work on two complementary axes in order to set up an activity to
support all the companies in its territory, regardless of their size, large or small.
The ﬁrst point of the strategy was to deﬁne clearly from its areas of competence the
generic economic oﬀers that it could provide to companies and to centralize the general
communication of its oﬀers on a single highly visible point. On the innovation aspect
based on the research results, the second point of the strategy focused on the creation of
an interface structure with the socio-economic world. This one, which is responsible for
providing simultaneous support for researchers and companies in their relations, is the
real heart of the exchange and of the round-trip process between academics and compa‐
nies. This structure is presented in more detail in Sect. 3. It should be noted as we shall
see it, in the diagram of Fig. 2, the same approach was initiated on the training aspect
for company.
To introduce the presentation of the interface dedicated to the innovation, the
following part presents the economical generic oﬀer and the general organization of the
university with its three interfaces. Section 3 is devoted to the interfacing structure
dedicated to the support of innovation and the profession of technological maker.
To meet the needs of the companies, the university has simpliﬁed its language and
proposed three generic oﬀers. These are deﬁned in terms of action which the client
companies have to implement in order to develop their economic performance. These
oﬀers leaning on research and teaching jobs of the universities are described in an “action
vocabulary” and are focalized on the needs of the companies. These three generic oﬀers
are: INNOVATE to ensure your diﬀerentiation on your markets in face of your compet‐
itors; UPDATE SKILLS of your companies by training your employees in new tech‐
nologies; RECRUIT graduates from university to develop human resources. To carry
this global business message and be in permanent connection with companies, the
Strengthening Partnerships Between Universities and SMEs
49

University of Nantes set up a speciﬁc service called “space for companies”. Figure 1
below shows the overall oﬀer of the university in simpliﬁed form.
Fig. 1. Organisation of the oﬀers of Université de Nantes to the companies.
On this diagram we ﬁnd the “space for companies”, the three specialized structured
each responsible for the realization of a one of three oﬀers, as well as laboratories and
teaching departments. Meanwhile the diagram also shows a network of sector engineers
between the laboratories and the interfacing structures. These last connected to the
“space for companies”, have the role of constantly animating and monitoring everything
developed by the teacher-researchers within the establishment. It allows to keep a
constant watch on the activity of the university in order to feed the permanent creation
of new technological oﬀers developed by the interfacing structures.
Each interface is able to answer directly to the companies and act directly in many
case. But the “space for companies” is able to answer widely to the companies and could
help the SMES to formulate clearly the request. In summary, it is a key to enter in the
university for the lost companies. With this organization the university group answer to
the economic world in each level. Private or internal status of the interfaces is chosen
to be the most eﬀective in their work. However, regardless of the status of the interface,
the functioning of its activities requires a private economic model in order to make
sustainable activity.
The diagram in Fig. 2 below shows the functional and synergistic organization of all
the diﬀerent structures at the university group. Around the classical structures of the
laboratories and training departments we ﬁnd the three specialized interfacing structures.
Some of the structure are in private form, but the university still owns them. Others are
internal services of the university.
50
J.-C. Cadiou and E. Chene

Fig. 2. This scheme represents the concept of the university group with the three incentives.
The concept of the university group thus corresponds to the set of all internal and
external structures. They all share the same image with companies. As indicated by the
various arrows, all the structures work closely together in order to ensure a high reactivity
and to ensure simultaneous response on all aspects if necessary.
This work of permanent exchange also makes it possible to make transverse inno‐
vations. The university answer to the companies thanks to the all part of the system but
quite particularly thanks to his private interface CAPACITÉS, which is the most original
feature of the system. That why, in the last part we make a speciﬁc focus on this interface
dedicated at the innovation.
3
The Interface for “Technological Maker”: CAPACITÉS
Thus the interfacing structure dedicated to the innovation is CAPACITÉS set by the
university in 2005. This is the subsidiary company of the university. The private status
of the subsidiary is a major asset in the relationship that maintenance with its private
clients. Thus fully integrated into the university group, it is also fully integrated into the
economic world. To further increase its visibility, in 2015, the university opened the
capital of CAPACITÉS to the Chamber of Commerce and Industry of Nantes. The shares
are now held by the university at 93% (which thus keeps the whole control of its subsid‐
iary company) and by the Chamber of Commerce and Industry of Nantes at 7%.
CAPACITÉS presents a turnover of 5 M€ and account about 65 employees. It has for
object to realize the activities of transformation into generically technology the results
of the research of the university. And from these it has to create an oﬀer of services to
help companies integrate these technologies into their products or processes. The
employees of CAPACITÉS work directly in the laboratories in rented oﬃces. So, in one
hand, through this proximity they work closely with researchers to develop sources of
innovation through internal R&D. On the other hand, as partners of companies, they are
Strengthening Partnerships Between Universities and SMEs
51

constantly listening to them and especially SMEs. From this permanent economic watch,
they are able to understand the needs and therefore to work in anticipation to ﬁnd solu‐
tions from sources of innovation that they master. If the use of existing knowledge and
know-how does not provide suitable answers, then they can seize associate researchers
to remove the technological or scientiﬁc locks. In this case, the staﬀ of CAPACITÉS
help the company to deﬁne and to write the expected speciﬁcations. Similarly, when
researchers deliver scientiﬁc results, they accompany their clients in the implementation
of the results and in their integration. To carry out its missions CAPACITÉS is organized
in “team of skills”. Each one is composed of engineers and technicians who works within
the laboratories (half of them are PhD). Their role is to develop the sources of innovation
and to put in place the activities necessary to enable the client companies to use them.
Each “team of skills” has its speciﬁc domain in relation with the scientiﬁc domain of
the associated researchers. To carry out their missions engineers remains permanently
link with them. To ensure a strong and symbiotic link with the laboratory teams, each
team is supervised by a researcher. Latter may be involved alone in the laboratory or be
accompanied by other researchers associated with the project. But whatever the situa‐
tion, the teams are integrated into the laboratory valorization policy. Internally a team
is managed by an engineer who assumes the roles of business manager and project
engineer at the same time. Each team has it proper business model to assume under joint
responsibility of leading researcher and engineer the economic proﬁtability of the activ‐
ities of team.
The schema of the following Fig. 3 presents the forward-return cycle between the
business world and the world of research that the staﬀ of the cells has to insure.
Fig. 3. Organisation of the relation between the Université de Nantes and the companies.
This permanent cycle allows alternatively to feed a world while recharging at the
same time before returning to feed the other world there also to be recharge. In this cycle:
the ﬁrst part goes from the world of research to the business world (step 1 to 3); the
second from the world of business to the laboratories (step 4 to 6).
The various actions carried out at each stage of the ﬁrst part are: step 1 (know-how
sharing between the laboratory and the associated team); step 2 (Internal process of
R&D. Engineers use of the know-how to turn knowledge into potential solutions (gener‐
ically technology). In this step the stall of the team of skill create value and sometimes
transform it into patents); step 3 (the engineers provide innovative answers to companies
52
J.-C. Cadiou and E. Chene

using solutions from the previous step). The actions carried out at each stage of the
second part are: step 4 (watch services on the future needs through exchanges with the
company, listen and analysis of needs in order to anticipate); step 5 (Internal process of
D&R. In its step it is the opposite action of R&D. In this action the engineers dismount
the needs in order to analyze the needs and to look for new needs of science); step 6 (the
engineers provide the laboratory with new technological or scientiﬁc locks and question
the laboratory).
The Fig. 4 below describes how the relationships between the teams of skills, labo‐
ratories and companies are organized.
Fig. 4. General organisation.
The development of a team of skills is another way to cultivate and ensure the
increased use of research innovations, especially by SMEs. It is also a complementary
form oﬀered to researchers who wish to be involved in the development of research
while remaining fully active in research. In the past, the only solutions were the creation
of start-ups. With this solution all parties are winners: all the companies including SMEs,
university and researchers as well as economic territory. The university is very often
vertically organized in silos. Every scientiﬁc skill remains in its laboratory. Indeed, even
though transdisciplinary research projects are developing, science is still very discipli‐
nary. However, the needs of the companies are obviously global. CAPACITÉS with all
its teams works as much as possible in a completely transversal way to associate all the
sources of innovation that it has in the teams. Indeed, turned towards the permanent
development of new solution the engineers of diﬀerent teams organize themselves as
they wish to answer their customers.
4
Discussion and Conclusion
The universities must become real actors in the economic development of their territory
and irrigate companies with their sources of innovation. To do this, the universities must
develop a new profession as technological maker and accompanying companies. In [7],
Haussmann describes how limitation that have the companies forms barriers to turn their
knowledge of the market into products or services. The combination of the new skills
of the university with its traditional research skills and market skills of companies is a
response to its limitations and especially for SMEs.
Strengthening Partnerships Between Universities and SMEs
53

The choice of the university to create its subsidiary and the team of skills approach
respond well to future challenges of territorial economic development and the aspira‐
tions of researchers. The creation of start-up to valorize the results of the research,
introduced for nearly 20 years, as the way to stimulate the development of innovation,
has left aside many people and has not kept its economic promises. With hindsight, a
large number of start-ups disappeared rapidly or remained at very precarious stages of
development. The creation of a start-up must lead to a quick development of the business
and when the initial investment is very important. Of course, in this context, creations
are necessary, but many project of start-ups, at least at the beginning, often ﬁnd them‐
selves faced with the same problems as SMEs, even though they have a high absorptive
capacity. Like SMEs, they can beneﬁt from the support of the subsidiary. Thus everyone
ﬁnds himself there: the companies by constantly diﬀerentiating themselves from their
competitors; the academics being continuously involved in the valorization of their
works but remaining fully involved in their research. So, implementation of a team of
skills within the subsidiary of the university is an interesting solution. It allows a
researcher to associate his activities of research to a team of valuation. Settle all activities
beside those the other teams, within the same company, allows to strengthen the
economic weight of each unit by aggregating it to the others. It does not create many
fragile societies, but a stronger company given a better visibility to the university.
References
1. Philpott, K., Dooley, L., O’Reilly, C., Lupton, G.: The entrepreneurial university: examining
the underlying academic tensions. Technovation 31(4), 161–170 (2011)
2. Cooke, P., Morgan, K.: The network paradigm: new departures incorporate and regional
development. Environ. Plann. D Soc. Space 11(5), 543–564 (1993)
3. Cooke, P.: The role of research in regional innovation systems: new models meeting
knowledge economy demand. Int. J. Technol. Manag. 28(3/4/5/6), 507–533 (2004)
4. Genet, C.: La diﬀusion, des connaissances vers les PME: vers un modèle d’exploration
collective. Revue Internationale PME 20(1), 91–119 (2007)
5. North, D., Smallbone, D., Vickers, I.: Public sector support for innovating SMEs. Small Bus.
Econ. 16(4), 303–317 (2001)
6. Raymond, L., Blili, S., El Alami, D.: L’écart entre le consultant et la P.M.E.: analyse et
perspectives. Gestion 28(4), 52–60 (2004)
7. Hausman, A.: Innovativeness among small businesses: theory and propositions for future
research. Ind. Mark. Manage. 34(8), 773–782 (2005)
8. Perrin, J.: Concevoir l’innovation industrielle, méthodologie de conception de l’innovation.
CNRS, Paris (2001)
9. Mcadam, R., Keogh, W.: Transitioning towards creativity and innovation measurement in
SMEs. Creativity Innov. Manag. 13(2), 126–139 (2004)
10. Grimaldi, R., Kenney, M., Siegel, D.S., Wright, M.: 30 years after Bayh-Dole: re-assessing
academic entrepreneurship. Res. Policy 40(8), 1045–1057 (2011)
11. Dasgupta, P., David, P.A.: Toward a new economics of science. Res. Policy 23(5), 487–521
(1994)
12. Hassink, R.: Technology transfer agencies and regional economic development. Eur. Plan.
Stud. 4(2), 167–184 (1996)
54
J.-C. Cadiou and E. Chene

13. Hassink, R.: Technology transfer infrastructures: some lessons from experiences in Europe,
the US and Japan. Eur. Plan. Stud. 5(3), 167–183 (1997)
14. Kaufman, A., Tödling, F.: How eﬀective is innovation support for SMEs? An analysis of the
region of Upper Austria. Technovation 22, 147–159 (2002)
Strengthening Partnerships Between Universities and SMEs
55

Forming Groups of Mobile Learners that Promote
Collaborative Learning Supported by Mobile Devices
Marie Hélène Wassa Mballo1(✉), Alassane Diop2, Richard Hotte3,
and Ibrahima Niang1
1 University Cheikh Anta Diop, Dakar, Senegal
mariehelene.mballo@uadb.edu.sn, iniang@ucad.sn
2 University Alioune Diop, Bambey, Senegal
alassane.diop@uadb.edu.sn
3 Télé Université du Québec (TÉLUQ), Montreal, Canada
richard.hotte@teluq.ca
Abstract. The educational system of today is marked by advances in information
and communication technologies. Initially we attended computer-assisted
learning, then mobile technology has in turn been integrated into the education
system, hence the Mobile Learning. The technical capabilities of mobile devices
associated with wireless technologies make them remote learning tools in their
own right. Mobile Learning is a real potential for distance learning because it
allows the learner to learn anywhere and at any time to ensure better collaboration
between learners of mobile learning, gathered in small groups, hence the new
concept of Mobile Computer Supported Collaborative Learning (MCSCL). One
of MCSCL’s problem is the learner groups’ management. This problem is linked
to the high mobility of learners (change of position, disconnection of the network,
etc.). In our review of the literature we have made a classiﬁcation of learner group
training methods ensuring a better interaction while taking into account the mobi‐
lity of the learners. In the context of disadvantaged areas, mobile phones can be
used for learning.
Keywords: Mobile learning
Mobile Computer Supported Collaborative Learning
1
Introduction
The socio-constructivist approach encourages learning through an interaction of the
learner with his peers and the learner with his teacher. This approach is applied through
diﬀerent methods in the educational system, one of these methods is the collaborative
learning method.
Collaborative learning [1, 2] aims to improve the success of learners. It focuses on
working in small groups in which learners of diﬀerent abilities and talents strive to
achieve a common goal.
With the proliferation of mobile devices (smart phones, tablets…) and advances in
mobile technology, collaborative online education tends to use mobile devices as a
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 56–65, 2018.
https://doi.org/10.1007/978-3-319-72965-7_5

learning medium. This leads us to deﬁne a new concept that is collaborative learning
supported by mobile devices “Mobile Computer Supported Collaborative Learning
(MCSCL)” [3].
One of MCSCL’s key issues is to train motivated and diligent groups of learners in
their learning activities. In this paper we will focus on the formation of these groups of
learners in the MCSCL.
2
Learning Group Formation in Collaborative Learning
The various studies carried out on collaborative learning in the classroom [4–23] have
a very positive record. Indeed, the learner develops attitudes that ensure better academic
performance. Collaborative learning [24] suggests that learners are responsible and
endowed with social skills. Indeed learners are responsible for their learning as well as
that of the others. Collaborative learning is an approach that gives the learner a lot of
freedom. The activities are not very directed and the learners manage the bulk of their
group work. For example, the roles of learners are not assigned by the teacher in the
case of collaborative learning, but learners negotiate these roles among themselves.
The MCSCL has a deﬁnite advantage for learners who are very mobile because of
their professional occupation. Indeed, it ensures a strong collaboration and interaction
between the learners, a greater autonomy of learning for the learner who has the possi‐
bility to come into contact with the other learners as well as the teacher anywhere and
at any time.
The training of groups in distance learning platforms is done manually according to
the objectives of the trainer. However, what about the number of learners who assidu‐
ously use these collaborative tools? Or learners who are not willing to use them? This
proves to be a major problem in the implementation of collaborative distance learning
if we really want to respond to the principles of collaborative learning, where the inter‐
dependence of learners is mandatory.
To promote collaborative distance learning, it will be necessary to:
• Train groups of learners whose numbers are small: to improve and facilitate the
positive interdependence of learners. The collaborative approach recommends
training small groups, ranging from 2 to 4 or 5 learners per team [7, 25]. Absolutely,
a small number of learners promotes meaningful interactions while facilitating coor‐
dination and group management;
• Form groups of heterogeneous learners that facilitate positive interdependence, better
learning outcomes and real interaction;
• Place a collaborative pedagogical strategy such as collective problem solving:
– Establish a structuring or assistant technological system that aims to direct learners
into their activities and learning. (i) Structuring systems provide an interface that
guides learners in carrying out their collaborative activities. These systems struc‐
ture activities and interaction situations. (ii) Assisting systems collect data from
the interaction and analyze it to assist learners or trainers (supervisors).
Forming Groups of Mobile Learners
57

The primary objective of working in a group is to promote the acquisition of social
skills by learners, so the establishment of heterogeneous groups is the best approach
[26]. Learner-formed groups, which are homogeneous, do not guarantee in the long run
an environment conducive to collaborative learning [26–29].
In practice, during this 21st century coinciding with the advent of remote learning
platforms, many researchers are attempting to oﬀer learner group training solutions in
Computer Assisted Collaborative Learning (CSCL). Thus, researchers are using math‐
ematical models [30–35], algorithms grouping learners [36–44], implement intelligent
systems [45].
What we observe, however, in these works is that they focus more on static learners.
This aspect of learner mobility should be taken into account. In terms of mobility, we
refer to the apparatus used for learning, namely a mobile device.
Unlike the CSCL, mobile learning is best suited to collaborative learning because of
the mobility that enhances the interactivity between the learners. So instead of having
static groups, we will focus more on dynamic groups depending on the position of the
learner. This brings us to the concept MCSCL.
3
State of the Art on the Formation of Groups of Learners in
MCSCL
The MCSCL is a particularly dynamic environment. This environment must be able to
adapt to changes in the learner’s context and this, periodically (position, distance
between learners, and availability of learning objects…).
However, the MCSCL is generally exposed to a number of limitations such as:
• Technical problems related to mobile technology: limited storage capacity, limited
lifetime, frequent disconnection…
• Social problems: individualism, incomprehension…
• Geographical problems: climate change, displacement …
These problems in the MCSCL impact the formation of groups. This means that it
will be necessary to form dynamic groups taking into account these diﬀerent aspects in
order to ensure positive interdependence and to maintain the motivation and interaction
of the learners.
Teams tend to propose mechanisms for group formation of learners in the MCSCL.
Article [30] deals with a review of the literature on the problems of learners’ group
formation in the MCSCL. Indeed, the authors of this article provide research avenues
to the MCSCL community in order to propose learner group training solutions.
We ﬁnd that some studies propose the formation of heterogeneous groups to promote
interaction, others propose the creation of homogeneous groups. On the other hand,
Messeguer et al. [31], El-Bishouty et al. [32], Tan et al. [33], Mujkanovic et al. [34],
Muehlenbrock [46] focus on the learning environment to form groups. However,
researchers recommend heterogeneous groups that promote collaborative learning
through the interactions between learners and their motivation.
58
M. H. W. Mballo et al.

We have thus identiﬁed three essential criteria for the formation of learners’ group,
namely:
• The personal characteristics of the learners: This is about data that can help identify
learners;
• Learning behavior: It is a matter of collecting data on the learner’s behavior during
the learning activity. These data can be: social interaction, participation in learning
activities…
• Contextual information: relates to real-time data provided from the learner’s mobile
device.
We ﬁnd that the majority of the works use mainly the characteristics of the learner,
to create groups of learners, these characteristics are: age, level of knowledge, experi‐
ence… To enrich the group of learners with the aim of having homogeneous or heter‐
ogeneous groups, tools to analyze the learner’s behavior can be used, these tools can be
digital portfolios, intelligent systems. An interesting aspect of mobile technology is the
ability to have real-time information regardless of location and time, which is why the
contextual information criterion is used for training groups of learners. In this work the
most used information is the location of the learner who can be recovered through
Wi-Fi tools, GPS…
However on the twelve articles, only the authors Yin and others [35] combine the
three criteria for the formation of groups with the establishment of homogeneous group.
We believe that combining these three criteria allows for a generic learner group training
system that can be adapted to any learning context.
Another aspect that we have in these articles is the possibility of having groups that
can be customized according to the needs of the trainer or the learner, and dynamic
groups that change over time due to the mobility of the learners. Two articles Zurita
et al. [36], Tan et al. [33] propose a method for the dynamic management of learner
groups.
4
Proposal for a Training System for a Learner’s Group in a Mobile
Learning Context
The MCSL proves to be a practical learning approach for those who wish to improve
their knowledge or carry out continuous training. We want to propose a mobile learning
system for professionals who have spatio-temporal constraints to carry out their learning.
Distance learning is beneﬁcial only when there is real collaboration between learners.
Cooperatively alludes to the establishment of a group of learners. How to train these
groups is our main concern. Should it be done manually or automatically by the algo‐
rithm implementation? But the manual training of groups proves to be complex, because
many parameters have to be taken into account and the motivation of the learners must
be maintained in the learning activities. As seen previously many works, coinciding with
advances in mobile technology, propose learner group training approaches in the
MCSCL. Through this study of the state of the art, we propose generic learner group
formation architecture in the MCSCL (Fig. 1).
Forming Groups of Mobile Learners
59

Fig. 1. Generic learner group training architecture.
We intend to propose an algorithm for the formation of heterogeneous groups of
learners that receive as parameters:
• Personal traits: gender, title (employed, unemployed), work experience, level of
domain knowledge (highest diploma);
• Geographical position: we retrieve the geographical coordinates of learners from
their mobile device by activating their GPS;
• Social we analyze the rate of interaction of learners in terms of answers given to the
questions of other learners.
In output we have:
• The group size we set at ﬁve. A small number of learners promote meaningful inter‐
actions while facilitating coordination and group management [7, 25];
• We choose a hierarchical organization (super-group and subgroup) of groups to
facilitate assignment from the learners to the groups;
• The groups trained are updated periodically to take into account the high mobility of
learners who change positions frequently.
4.1
Implementation of the Algorithm
We intend to deploy the solution in Senegal for professionals who wish to improve their
skills by performing continuous training [37, 38]. We ﬁnd that these professionals face
many spatio-temporal constraints to attend a face-to-face training.
The learners are grouped in open digital spaces (ODS) according to their geograph‐
ical position [39], their proﬁle, as well as the browsing history of the web pages. These
data are stored in databases and analyzed and exploited to form groups of learners. The
principle of the algorithm is to associate the learner with the nearest ENO after locating
it. For this, our algorithm is based on the Dijkstra algorithm [40] which serves to solve
the problem in the shortest path.
60
M. H. W. Mballo et al.

4.2
Principle of Group Formation
Method 1
In this ﬁrst method, our algorithm is characterized by (Fig. 2):
• Groups that are identiﬁed by a ﬁxed value and are named ODS;
• ODSs are organized in ascending order;
• The search for a current ODs: the Dijkstra algorithm is applied to determine the ODS
closest to the learner (mobile node) who wants to connect;
• The ODS is uniquely identiﬁed by the couple (Latitude, Longitude).
Fig. 2. Search algorithm with method 1.
By applying the principle of Djisktra, to search the current ODS of node I, we will
browse the nodes step by step starting with the node that has the identiﬁer. Thus our
search begins with the node Kédougou and ends with the node Saint Louis which will
be the current ODS of the node I.
Doing an analysis of the algorithm, the parameter of complexity is related to the
number n corresponding to the number of ODS. To improve the algorithm it will be
necessary to reduce the number of ODSs to be traveled to ﬁnd a current ODS.
Method 2
In this second method we try to improve the algorithm of the ﬁrst method, so our second
proposed algorithm is characterized by:
• Associate a group number for each ODS;
• Associate with each mobile node (learner) a group number that corresponds to its
original group;
• Regroup the ODSs by group;
• Find the current ODS starting from the originating group of the mobile node;
• Each group covers an identiﬁer interval;
• The maximum number of ODSs in a group is limited to ﬁve.
• The groups are as follows:
– Group 0: identiﬁer is between 11 and 11.9
– Group 1: identiﬁer is between 12 and 12.9
– Group 2: identiﬁer is between 13 and 13.9
Forming Groups of Mobile Learners
61

– Group 3: identiﬁer is between 14 and 14.9
– Group 4: identiﬁer is between 15 and 15.9
– Group 5: identiﬁer is between 16 and 16.9
By always analyzing this algorithm in relation to the previous one, the complexity
is less. Decidedly, the number of ODSs to be covered is reduced since the ODSs are
organized in groups. Figure 3 shows the time taken during the localization of ODS using
the two methods presented above:
Fig. 3. Comparison of the two methods. (Color ﬁgure online)
The second method (red curve) with constant complexity is the best approach to
implement because we ﬁnd a real reduction in the search time of the closest ODS
according to the parameters deﬁned previously. Indeed, the maximum time observed in
the second method is 0.6 s contrary to method 1 where the maximum observed time
reaches 80 s.
5
Conclusion and Perspectives
This paper is part of our doctoral research work. The objective of our work is to propose
a mobile learning solution to Senegalese professionals who face spatio - temporal
constraints to continue their learning by attending classes. Thus we have established a
detailed state of the art on the practices of Mobile Learning. One of the major issues in
Mobile Learning is to be able to form groups of learners that are sustainable over time
while ensuring real collaboration between these learners. This has prompted us to focus
our research on the formation of learner groups in Mobile Computer Supported
62
M. H. W. Mballo et al.

Collaborative Learning (MCSCL). In summary, what we can retain is that to form groups
of learners assuring a positive interdependence, the following criteria must be taken into
account: Personal traits; Geographic position; Social interactions.
The algorithm that we have proposed can be adapted to any mobile learning situation.
As a perspective, we plan to deploy the solution and then evaluate its impact in the
learning process of Senegalese professionals.
References
1. Abrami, P.C.: L’apprentissage coopératif: Théories, méthodes et activités (1995)
2. Johnson, D.W., Johnson, R.T., Holubec, E.J.: Cooperation in the Classroom Revised Edition.
Interaction Book Company, Edina (2008)
3. Caballé, S., Xhafa, F., Barolli, L.: Using mobile devices to support online collaborative
learning. Mob. Inf. Syst. Mob. Wireless Netw. 6, 27–47 (2010)
4. Slavin, E.R.: Cooperative Learning: Theory, Research and Practice, 2nd edn. Allyn & Bacon,
Boston (1995)
5. Roseth, C.J., Johnson, D.W., Johnson, R.T.: Promoting early adolescents achievement and
peer relationships: the eﬀects of cooperative, competitive, and individualistic goal structures.
Psychol. Bull. 134(12), 223–246 (2008)
6. Johnson, D.W., Johnson, R.T.: Cooperation and Competition: Theory and Research.
Interaction Book Company, Edina (1989)
7. Johnson, D.W., Johnson, R.: An educational psychology success story: social
interdependance theory and cooperative learning. Educ. Res. 38(15), 365–379 (2009)
8. Windschitl, M.: Using small-group discussions in science lectures: a study of two professors.
College Teaching 47(11), 23–27 (1999)
9. Slavin, R.E., Hurley, E.A., Chamberlain, A.: Cooperative Learning and Achievement: Theory
and Research (2003)
10. Johnson, D.W., Johnson, R.T., Smith, K.: The state of cooperative learning in postsecondary
and professional settings. Educ. Psychol. Rev. 19(11), 15–29 (2007)
11. Shachar, H., Fischer, S.: Cooperative learning and the achievement of motivation and
perceptions of students in 11th grade chemistry classes. Learn. Instr. 14(11), 69–87 (2004)
12. Johnson, D.W., Johnson, R.T., Taylor, B.: Impact of cooperative and individualistic learning
on high-ability students’ achievement, self-esteem, and social acceptance. J. Soc. Psychol.
133(16), 839–844 (1993)
13. Nichols, J., Miller, R.: Cooperative learning and student motivation. Comtemporary Educ.
Psychol. 19(12), 167–178 (1994)
14. Topping, K.J., Thurston, A., Tolmie, A., Christie, D., Murray, P., Karagiannidou, E.:
Cooperative learning in science: intervention in the secondary school. Res. Sci. Technol.
Educ. 29(11), 91–106 (2011)
15. Bandura, A.: A social cognitive theory of personality. In: Pervin, L., John, O. (eds.) Handbook
of Personality, pp. 154–196. Guilford, New York (1999)
16. Law, Y.K.: The eﬀects of cooperative learning on enhancing Hong Kong ﬁfth graders’
achievement goals, autonomous motivation and reading proﬁciency. J. Res. Reading 34(14),
402–425 (2011)
17. Elliot, A.J.: A conceptual history of the achievement goal construct. In: Elliot, A.J., Dweck,
C.S. (eds.) Handbook of Competence and Motivation, pp. 52–72. Guilford, New York (2005)
18. Bertucci, A., Conte, S., Johnson, D.W.: The impact of size of cooperative group on
achievement, social support, and self-esteem. J. Gen. Psychol. 137(13), 256–272 (2010)
Forming Groups of Mobile Learners
63

19. Chapman, E.S., Cope, M.T.: Group reward contingencies and cooperative learning:
immediate and delayed eﬀects on academic performance, self esteem, and sociometric ratings.
Soc. Psychol. Educ. 7(11), 73–87 (2004)
20. Slavin, R.E., Lake, C., Davis, S., Madden, N.: Eﬀective programs for struggling readers: a
best-evidence synthesis. Educ. Res. Rev. 6(11), 1–26 (2011)
21. Berndt, T.J., Keefe, K.: Friends’ inﬂuence on adolescents’ adjustment to school. Child Dev.
66(15), 1312–1329 (1995)
22. Buhrmester, D.: Intimacy of friendship, interpersonal competence, and adjustment during
preadolescence and adolescence. Child Dev. 61(14), 101–1111 (1990)
23. Wentzel, K.R., Barry, C.M., Caldwell, K.: Friendships in middle school: inﬂuences on
motivation and school adjustment. J. Educ. 96(12), 195–203 (2004)
24. George, S.: Apprentissage collectif à distance. SPLACH: un environnement informatique
support d’une pédagogie de projet, Maine (2001)
25. Gillies, R.M.: The eﬀects of cooperative learning on junior high school students during small
group learning. Learn. Instr. 14(12), 197–213 (2004)
26. Ballantine, J., MvCourt Larres, P.: Cooperative learning: a pedagogy to improve students’
generic skills. Educ. + Train. 49(12), 126–137 (2007)
27. Lejk, M., Wyvill, M., Farrow, S.: Group assessment in systems analysis and design: a
comparison of the performance of streamed and mixed ability groups. Assess. Eval. Higher
Educ. 24(11), 5–14 (1999)
28. Rouiller, Y., Howden, J.: La pédagogie coopérative: Reﬂets de pratiques et
approfondissements (2009)
29. Stein, R.F., Hurd, S.: Using student teams in the classroom: a faculty guide
30. Amara, S., Macedo, J., Bendella, F., Santos, A.: Group formation in mobile computer
supported collaborative learning contexts: a systematic literature review. J. Educ. Technol.
Soc. 19, 258–273 (2016)
31. Messeguer, R., Medina, E., Royo, D., Navarro, L., Juarez, J.P.: Group prediction in
collaborative learning. In: The 6th IEEE International Conference on Intelligent
Environments (2010)
32. El-Bistoury, M., Ogata, H., Rahman, S., Yano, Y.: Social knowledge awareness map for
computer supported ubiquitous learning environment. Educ. Technol. Soc. 13(14), 27–37
(2010)
33. Tan, Q., Kinshuk, Jeng, Y.L., Huang, Y.M.: A collaborative mobile virtual campus system
based on location-based dynamic grouping. In: Proceedings of the 10th IEEE International
Conference on Advanced Learning Technologies, Los Alamitos (2010)
34. Mujkanovic, A., Lowe, D., Willey, K.: Adaptive group formation to promote desired
behaviours. In: Profession of Engineering Education: Advancing Teaching, Research and
Careers: 23rd Annual Conference of the Australian Association for Engineering Education,
Melbourne, Australia (2012)
35. Yin, C., Dong, Y., Tabata, Y., Ogata, H.: Recommendation of helpers based on personal
connections in mobile learning. In: Seventh IEEE International Conference on Wireless,
Mobile and Ubiquitous Technology in Education, Los Alamitos (2012)
36. Zurita, G., Nussbaum, M., Salinas, R.: Dynamic grouping in collaborative learning supported
by wireless handhelds. Educ. Technol. Soc. 8(13), 149–161 (2005)
37. Mballo, M.H., Hotte, R., Diop, A., Niang, I.: Mobile learning, a solution to vocational training
in Senegal. In: International Conference on Web & Open Access to Learning, Dubai, United
Arab Emirates (2014)
64
M. H. W. Mballo et al.

38. Mballo, M.H., Diop, A., Hotte, R., Niang, I.: A model of computer support to mobile learning
in Senegalese educational system. In: Global Summit on Computer Information Technology-
International Conference on Education & eLearning Innovation, ICEELI 2015 (2015)
39. Mballo, M.H., Diop, A., Hotte, R., Niang, I.: Mobile learning: hierarchical architecture. In:
Word Symposium on Computer Application & Research, WSCAR 2016 Egypt (2016)
40. Wikipedia: Algorithme de Dijkstra (2016). https://fr.wikipedia.org/wiki/Algorithme_de_
Dijkstra. Accès le 2016
41. Huang, Y.M., Wu, T.T.: A systematic approach for learner group composition utilizing U-
learning portfolio. Educ. Technol. Soc. 14(13), 102–117 (2011)
42. Huang, J.J., Yang, S.J., Huang, Y.M., Hsiao, I.Y.T.: Social learning networks: build mobile
learning networks based on collaborative services. Educ. Technol. Soc. 13(13), 78–92 (2010)
43. Hsieh, J.C., Chen, C., Lin, H.F.: Social interaction mining based on wireless sensor networks
for promoting cooperative learning performance in classroom learning environment. In: The
6th IEEE International Conference on Wireless, Mobile, and Ubiquitous Technologies in
Education, Los Alamitos (2010)
44. Giemza, A., Manske, S., Hoppe, H.U.: Supporting the formation of informal learning groups
in a heterogenous information environment. In: Proceeding of the 21st International
Conference on Computers in Education, Bali, Indonesia (2013)
45. Yang, F., Wang, M., Shen, R., Han, P.: Community-organizing agent: an artiﬁcial intelligent
system for building learning communities among large numbers of learners. Comput. Educ.
49(12), 131–147 (2007)
46. Muehlenbrock, M.: Formation of learning groups by using learner proﬁles and context
information. In: Proceedings of the 2005 Conference on Artiﬁcial Intelligence in Education:
Supporting Learning Through Intelligent and Socially Informed Technology, Amsterdam
(2005)
Forming Groups of Mobile Learners
65

Virtual Classroom Solution with WebRTC
in a Collaborative Context in Mathematics
Learning Situation
Pape Mamadou Djidiack Faye1(✉), Amadou Dahirou Gueye2, and Claude Lishou3
1 Virtual University, Dakar, Senegal
papedjidiack.faye@uvs.edu.sn
2 Alioune Diop University, Bambey, Senegal
dahirou.gueye@uadb.edu.sn
3 Cheikh Anta Diop University, Dakar, Senegal
clishou@ucad.sn
Abstract. The acquisition of practical skills is an essential point in the STEM
(Science Technology Engineering and Mathematics). In the case of distance
learning, it becomes even more diﬃcult since students are physically remote
practical work rooms and laboratories they must use to obtain these skills. In
research, solutions of virtual classrooms (BigBlueButton, Openﬁre) have been
proposed to solve this problem. These solutions integrate video, audio, chat,
screen sharing, audio recording of the diﬀerent actors. However, with the devel‐
opment of terminals, these solutions are not readily adaptable to a mobile phone
and are not as easily integrated on training platforms. In addition, a virtual class‐
room for STEM must also oﬀer shared features such as presentation slides online,
whiteboards, and writing mathematical formulas. While these features are not
available with WebRTC, we intend in this paper combine the WebRTC with
command line tools for extending document viewer opportunities for an online
presentation quality and allow learners and teachers to communicate in real time
from a browser. Our virtual classroom solution can be easily integrated into
distance learning platforms.
Keywords: Distance learning · Virtual classrooms · WebRTC
Training platforms
1
Introduction
Nowadays, with the Information and Communications Technologies (ICT), the open
and distance training are becoming more extents in all sectors of education activities in
general and education in particular. However, distance education poses speciﬁc prob‐
lems. Among these problems, there is the feeling of isolation experienced by learners,
the accompanying deﬁcit and lack of relationship with other actors; which leads to a
high dropout rate [1–3]. This as geographical distance and physical learning commun‐
ities and those educators begin to be oﬀset by devices that introduce spaces of commu‐
nication and collaborative learning process in the form of FAQ, forums or messaging.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 66–77, 2018.
https://doi.org/10.1007/978-3-319-72965-7_6

Moreover, the technologies can reduce communication time and physical distancing.
The solutions of “virtual classes” are numerous and easily integrated with the LMS
(Learning Management System) [4–6]. A virtual classroom allows bringing together a
group of learners and a teacher/facilitator. This system closer to the Webinar Workshop
or used in connection with video conferencing, remote meetings and company training,
can reproduce the conditions of a classroom [7].
To reduce the eﬀects of the geographical separation between actors of digital univer‐
sities, researchers proposed IP SMSC center solutions incorporating value added serv‐
ices. [8] However, the authors do not provide solutions to Open Training and Distance
to extend the functionality of their e-learning platform.
In addition, distance learning platforms exist and incorporate self assessment tools
and online assessment. [9] This is the case of Moodle, which is used by many univer‐
sities. [10] Moreover, it also realizes that Google and openﬁre use WebRTC API to
provide solutions for online presentation tools. A presentation utility must allow at least
the participants to the following three operations: (1) audio conference (2) video confer‐
ence (3) loading and synchronous PowerPoint document reading. Now the WebRTC
technology provides a solution to the problems of audio and video. The question is how
to make the presenter can load a document Power Point and make its real time presen‐
tation with several participants? However, this functionality requires a lot of interactivity
between participants. For example; every time the presenter makes a presentation page
of change, action must be notiﬁed in real time to all participants. And this requires
knowing the structure of a power point paper to know at every moment what page it is.
Another problem on the e-Learning platforms is the ability of the presenter to annotate
or write mathematical formulas that are visible in real time by participants.
The rest of the paper is organized as follows: in Sect. 2, we ﬁrst present the state of
the art on WebRTC and basic tools used in our approach. Then we present in Sects. 3
and 4 architecture integration tools and provide timing diagrams representing operating
scenarios. Finally, in Sect. 5, we implement our solution to be presented in the form of
snap in Moodle.
2
Related Work
We present in this part of the WebRTC technology and basic tools (ImageMagick, and
Ghostscript) used in our approach.
2.1
WebRTC (Web Browsers with Real Time Communications)
WebRTC is an open source project introduced by Google in 2011 [11] which ensures
communications in real time via a JavaScript API. [12] The project aims to develop a
technology allowing web browsers to support point to point interactive communications
and provide data exchange synchronous mode [13, 14]. The WebRTC is intended to
give browsers the ability to oﬀer audio, video or written, ﬁle transfer, screen sharing and
remote control of computers.
Virtual Classroom Solution with WebRTC in a Collaborative Context
67

The main components of the WebRTC API defined by the W3C working groups
(World Wide Web Consortium) and IETF (Internet Engineering Task Force) are [15, 16].
• MediaStream: Allows a browser to access the camera and microphone;
• RTCPeerConnection: Enable audio and video calls;
• RTCDataChannel: Allows the browser to send data in a peer-to-peer connection.
Figure 1 shows, first WebRTC API C++ implemented in some browsers and also the web
WebRTC API allowing web developers to integrate services offered by the WebRTC into
their applications.
Fig. 1. WebRTC architecture
In our solution, the WebRTC plays a very important role. In addition to allowing us
to have audio and video communication between participants; it is also used to manage
the interaction between them. For that sum we pressed the option of WebRTC’s chat to
send commands to the various participants. These commands can be:
• Control Change page number: the presenter sent to the participant to update their
page numbers.
• Annotation control: it is also sent by the presenter.
• Microphone oﬀ control or speaker: it may happen that at some point the presenter
does not want to hear what participants are spirited to say or want to disable is set
Mute.
• Mathematical given shipment control.
68
P. M. D. Faye et al.

2.2
GhostScript
Ghostscript software consists of a panel with the task of analyzing, translating into
various other formats, but also to run programs written in PostScript and PDF, view
them and print them. PostScript and PDF ﬁles can even be mutually translated from one
language to the other. [17] Ghostscript incorporates features that improve the graphics
usually kept to a minimum on the ﬁles to PostScript and PDF [18] format.
In our solution, it is used to convert a presentation (postscript …) PDF document
before being converted into image. A presentation can be in PDF or PostScript format
even. While access to the structure of the document type can be complex if you try to
handle this dimension browser. That is why we ﬁrst standardized presentation by
converting PDF to facilitate its translation into images.
2.3
ImageMagick
ImageMagick is a software suite to create, edit, compose, or convert bitmap images. It
can read and write images in a variety of formats (over 200) including PNG, JPEG,
JPEG-2000, GIF, TIFF, DPX, EXR, WebP, Postscript, PDF, and SVG. Use ImageMa‐
gick to resize, ﬂip, mirror, rotate, distort, shear and transform images, adjust image
colors, apply various special eﬀects, or draw text, lines, polygons, eclipses and Bézier
curves [19, 20].
ImageMagick is free software delivered as a ready to run binary distribution or as
source code that you may use, copy, modify, and distribute in both open and proprietary
applications. It is distributed under the Apache 2.0 license. The ImageMagick develop‐
ment process ensures a stable API and ABI. Before each ImageMagick release, we
perform a comprehensive security assessment that includes memory error and thread
data race detection to prevent security vulnerabilities [19].
Almost all programming languages have extensions or libraries to interact with the
ImageMagick API, although you could also use it via command line.
In our solution, ImageMagick and used for converting a PDF document into images
to facilitate the handling of the presentation at the client side.
Most welcome.
3
Architecture and Operation of the Solution
In this part we will present the architecture and functionality of our solution.
3.1
Architecture
This architecture consists of several components. Each of these components plays a
speciﬁc role (Fig. 2).
• User + Browser: Is the client. The browser can be any browser supporting WebRTC.
This allows our solution to be compatible with tablets.
• Apache: The container of our application.
Virtual Classroom Solution with WebRTC in a Collaborative Context
69

• Ghostscript: It is responsible for translating a PostScript document in PDF format
• ImageMagick: Its role is to cut a PDF document Images
• WebRTC server: It is he who plays the signaling server for technology WebRTC
Fig. 2. Solution architecture
Création Presentation
Conversion ok
Persistence  Ok
Creation confirmed
Persistence
image conversion Request
Conversion ok
PDF Conversion Request
Sends data presentation + Presentation Postscript or PDF
sending form
Request creation form
User + Browser
Apache server
GhostSript
Image Magick
SGBD
Conversion ok
Persistence  Ok
Creation confirmed
Persistence
image conversion Request
Conversion ok
PDF Conversion Request
Sends data presentation + Presentation Postscript or PDF
sending form
Request creation form
Fig. 3. Scenario of creating a presentation
70
P. M. D. Faye et al.

3.2
Some Operations Scenarios
To better explain this architecture we use scenarios workings of representatives in the
form of deﬁned sequence diagrams.
3.2.1
Creating a Presentation
In this section we show the diﬀerent interactions when creating a presentation.
(1) Presenter: Application Form
(2) Apache: Sending the form
(3) Presenter: Sending data from the presentation with an attachment presentation in
PDF or Postscript format
(4) Apache: Use Ghostscript for translating the PDF document if it is not already in
PDF
(5) Apache: Use ImageMagick to translate the document in PDF Images
(6) Apache: Persistence of data in the database
(7) Apache server: Notiﬁcation presenter.
3.2.2
Scenario of Creating a Presentation
In this section we show the diﬀerent interactions when a ﬁrst user connects to the
presentation (Fig. 4).
Connection scenario of a user presentation
connection confirmed
Initialize the connection with all users online
presentation Request + ID presentation
research Presentation
Presentation found
Presentation + sends initialization parameters and connection to the server WebRTC
connection request to the WebRTC server
creating identifier
Acceptance + connection identifier
request users list
sending users list
User + Browser
Apache Server
SGBD
WebRTC Server
connection confirmed
Initialize the connection with all users online
presentation Request + ID presentation
research Presentation
Presentation found
Presentation + sends initialization parameters and connection to the server WebRTC
connection request to the WebRTC server
creating identifier
Acceptance + connection identifier
request users list
sending users list
Fig. 4. Connection scenario of user in presentation
Virtual Classroom Solution with WebRTC in a Collaborative Context
71

(1) User: Application presentation interface to the Apache server by including the
identiﬁer of the presentation
(2) Apache: Plays presentation as an image, the more connection settings to the server
WebRTC
(3) User: Request Connection server WebRTC
(4) WebRTC server: User identiﬁer Creation
(5) WebRTC server: Accept the connection and sends the identiﬁer to the customer
(6) User: Request list of connected users
(7) WebRTC server: Sending list of connected users (the list and empty at this time as
this is the ﬁrst connection
(8) User: Creating connection to the other participants
(9) WebRTC server: Connection Acceptance with other participants.
3.2.3
Update Presentation Slide Page Number
In this section we show the various interactions during the update in real time of the
presentation page number (Fig. 5).
Mise a jour numero de page diapositive
Changing presentation pages
Persistence ok
Persistence ok
Persistence
Persistence of page number
Notification
Change Ok
Changing page
Request changing page Number + Current page number
Sends current page number in a text message
Moderator
Apache server
SGBD
Participants
webRtc server
Changing presentation pages
Persistence ok
Persistence ok
Persistence
Persistence of page number
Notification
Change Ok
Changing page
Request changing page Number + Current page number
Sends current page number in a text message
Fig. 5. Page number change scenario
(1) Moderator: Change slide in local
(2) Moderator: Sending slide change message to WebRTC server as text by putting the
current page number there
(3) WebRTC server: Transmission message to the participant connected to the presen‐
tation
(4) Participants: Update the current page on the slide
72
P. M. D. Faye et al.

(5) Moderator: Persistence of the page number on the database
(6) Apache: Persistence ok
3.2.4
Update Annotation
In this section we show the various interactions during the update in real time annotations
(Fig. 6).
update  annotation
Persistence ok
Persistence ok
Persistence
Persistence annotation on the Database
Construction text message containing the  annotation coordinate
Drawing annotation
Sending annotation coordinates
request annotation update
reproduction annotation
update ok
Notification
Moderator
Apache server
SGBD
Participants
WebRTC server
Persistence ok
Persistence ok
Persistence
Persistence annotation on the Database
Construction text message containing the  annotation coordinate
Drawing annotation
Sending annotation coordinates
request annotation update
reproduction annotation
update ok
Notification
Fig. 6. Annotation update scenario
(1) Moderator: Drawing Annotation
(2) Moderator: Building text message containing the coordinates and the color of the
annotation
(3) Moderator: Send SMS containing the annotation data annotation
(4) WebRTC server: Transmission message to the participant connected to the presentation
(5) Participants: Reproduction of the annotation on their interfaces
(6) Moderator: Persistence of the annotation data on the database
(7) Serveur Apache: Persistance ok
Virtual Classroom Solution with WebRTC in a Collaborative Context
73

4
Realization
4.1
Online Slide Presentation
The Fig. 7 below shows the interface of a slide presentation. On the latter, we see a
teacher with an online course followed by several students. From this interface the
teacher can do the following:
• give voice to a student,
• disable students microphones,
• change the page of the slide,
• add annotations,
• change the current panel to display the whiteboard or the mathematics table real time
on all screens of participants,
• discuss to real time with participants in chat.
Fig. 7. Presentation slide
4.2
Slides Presentation with Annotations
In Fig. 8, we see a teacher presenter making annotations during a presentation of slides
online. These annotations are visible in real time on all participants’ screens.
74
P. M. D. Faye et al.

Fig. 8. Presentation slide with annotations
4.3
Mathematical Board
Figure 3 shows the display of mathematics board, capturing visible mathematical
formulas in real time by all participants. These can also communicate with other
connected video players. This allows everyone to participate, assist or guide the
reasoning of the designated player to solve the problem.
With this table only the moderator can give out to a participant so that it can write
mathematical formulas (Fig. 9).
Fig. 9. Mathematical board
5
Conclusion
In this paper we proposed a virtual classroom solution easily integrated into training
platforms. This is a very useful tool for students open distance training by giving them
Virtual Classroom Solution with WebRTC in a Collaborative Context
75

the ability to communicate in real time with audio, video, chat among themselves and
with their teachers while remaining within the Moodle environment. The solution also
allows players to open distance training, exchange large ﬁles by the ﬂexibility of the
WebRTC technology.
This platform also allows students to mathematics not to have limits for mathematics
in their TP for entering mathematical formulas. Some sessions using our plugin enabled
these students to be comfortable in the formulation of their online math answers. The
audio recording feature of our solution enables language teachers ODL to test the level
of understanding and oral restitution students.
Future work should be oriented towards the integration of TP functionality for
programming languages.
References
1. Mbala Hikolo, A.: Analyse, conception, spéciﬁcation et développement d’un système multi
agents pour le soutien des activités en formation à distance. Thèse Université de Franche
Comté, soutenue le 16 octobre 2003
2. Henri, F., Lundgren-Cayrol, K.: Apprentissage collaboratif à distance. Presses de l’Université
du Québec (2001)
3. Faerber, R.: Accompagner les apprentissages à distance et collaborer en petits groupes.
Communication présentée au Quatrième congrès AECSE 2001, Lille, Septembre 2001
4. Komlo, C., KisToth, L.: Virtual and online classrooms of e-learning. In: 2013 IEEE 63rd
Annual Conference International Council on Educational Media (ICEM), pp. 1–8 (2013).
https://doi.org/10.1109/CICEM.2013.6820153
5. Kartyas, G., Gati, J.: Interrelated components of virtual classroom to achieve model for
teaching practice. In: International Symposium on Computational Intelligence and Intelligent
Informatics, ISCIII 2007, pp. 255–259 (2007). https://doi.org/10.1109/ISCIII.2007.367398
6. Nyarko, M., Ventura, N.: E-learning: virtual classrooms as an added learning platform. In:
2010 IEEE Region 8 International Conference on Computational Technologies in Electrical
and Electronics Engineering (SIBIRCON), pp. 426–431 (2010). https://doi.org/10.1109/
SIBIRCON.2010.5555116
7. Vaufrey, C.: Classes Virtuelles: Quand Le Formateur Devient Animateur D’apprentissages.
Thot Cursus. Date de publication, 21 janvier 2014. http://cursus.edu/article/21332/classes-
virtuelles-quand-formateur-devient-animateur/#.VNDeNS73QTb
8. Ouya, S., Gueye, A.D., Sy, K., Niane, M.T., Lishou, C.: Contribution to reducing the eﬀects
of geographical separation between actors of virtual universities: proposal of an IP-SMSC
integrating value-added services solutions. In: 2015 International Conference on Interactive
Collaborative Learning (ICL), pp. 1145–1150. IEEE (2015). https://doi.org/10.1109/ICL.
2015.7318195
9. Aydin, C.C., Tirkes, G.: Open source learning management systems in e-learning and Moodle.
In: 2010 IEEE Education Engineering (EDUCON), pp. 593–600, 14–16 April 2010
10. Martín-Blas, T., Serrano-Fernández, A.: The role of new technologies in the learning process:
Moodle as a teaching tool in Physics. Comput. Educ. 3, 35–44 (2009)
11. Elleuch, W.: Models for multimedia conference between browsers based on WebRTC. In:
2013 IEEE 9th International Conference on Wireless and Mobile Computing, Networking
and Communications (WiMob), pp. 279–284, 7–9 October 2013
76
P. M. D. Faye et al.

12. Zeidan, A., Lehmann, A., Trick, U.: WebRTC enabled multimedia conferencing and
collaboration solution. In: Proceedings of the World Telecommunications Congress, WTC
2014, pp. 1–6, 1–3 June 2014
13. Vogt, C., Werner, M.J., Schmidt, T.C.: Leveraging WebRTC for P2P content distribution in
web browsers. In: 2013 21st IEEE International Conference on Network Protocols (ICNP),
pp. 1–2, 7–10 October 2013
14. Hinow, F., Veloso, P.P., Puyelo, C., Barrett, S., Nuallain, E.O.: P2P live video streaming in
WebRTC. In: 2014 World Congresson Computer Applications and Information Systems
(WCCAIS), pp. 1–6, 17–19 January 2014
15. Sredojev, B., Samardzija, D., Posarac, D.: WebRTCtechnologyoverview and signaling
solution design and implementation. In: 2015 38th International Convention on Information
and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1006–
1009, 25–29 May 2015
16. W3CEditor’sDraft. http://w3c.github.io/mediacapturemain/getusermedia
17. Ghostscript Web Site. http://www.ghostscript.com/
18. Peter Deutsch, L.: Aladdin Ghostscript version 4.03, electronic distribution (1996).
ftp.cs.wisc.edu://pub/ghost/aladdin
19. http://www.imagemagick.org/script/index.php
20. Still, M.: The Deﬁnitive Guide to ImageMagick. Apress, Berkely (2005)
Virtual Classroom Solution with WebRTC in a Collaborative Context
77

Economic Development

Impact of the Utilization of the Biodigester
in the Populations of Bambey and Perspectives
for Mass Adoption and Valorization
I. Diallo1, Assane Gueye1, Omar Sene1(&), M. Kare1, P. I. Ndiaye1,
P. Diouf2, Madiop Diouf2, A. Dieng2, A. Sene2, and I. P. Thiao2
1 Université Alioune Diop de Bambey (UADB), Bambey, Senegal
papomarsene@gmail.com
2 Union Régionale des Associations Paysannes
de la Région de Diourbel (URAPD), Bambey, Senegal
Abstract. This paper is a preliminary report on the impact of the biodigester in
the department of Bambey, Senegal. The analysis is based on data gathered as
part of a join study between the University Alioune Diop of Bambey and the
Union of Farmers’ Associations of the Region of Diourbel. A survey was
undertaken during the month of January 2017 among 24 households, half of the
families owning a biodigester and half not having one. The analysis of the data
has revealed that the biodigester has very high potentials to get families out of
poverty. We analyze parameters such as self-sufﬁciency, ability to invest on and
develop additional activities, having savings, and even commercialization of
agricultural production and we have found a clear contrast between families that
own a biodigester and families that do not. However, despite the opportunities
offered by the biodigester, most households ﬁnd it difﬁcult to own one. The
study has also shown that the tasks needed to operate the biodigester are mainly
manual and that an automatization of the process presents high potentials.
Furthermore, the lack of market information constitutes an obstacle to the
commercialization of the agricultural products. With regards to this aspect, the
survey has shown that the implementation of an information system that dis-
seminate market and demand information will highly beneﬁt the populations.
Keywords: Biodigester  Clean energy  Agriculture  Information systems
Finance
1
Introduction
For decades now, Senegalese agriculture has been facing persistent difﬁculties that are
due to many factors such as a deﬁcit of rainfall, a degradation of the natural resources, a
decline in soil fertility and a low valorization. This is despite many ﬁnancial and
material efforts by the government and its development partners to support the agri-
culture sector.
Similar difﬁculties are observed in the energy sector which is characterized by a
deep contrast between urban and rural areas. Conventional energy supply is mostly
concentrated in cities while 60% of the population live in rural areas and have little or
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 81–93, 2018.
https://doi.org/10.1007/978-3-319-72965-7_7

no access to energy. As a consequence, 87% of the households use ﬁrewood and
animal dung as fuel for cooking and lightning. Because of the severe deforestation,
rural women (who are mostly in charge of cooking tasks) are ﬁnding it increasingly
difﬁcult to collect necessary ﬁrewood for daily cooking. In addition to accelerating
deforestation, the use of this fuel also poses health issues to women and children.
Faced with all these problems, one of the solutions adopted by the Senegalese
government (as well as many other African governments) was the setting up of a
national biogas program whose objective was not only to provide an adequate response
to climate deﬁcits but above all to meet the subsistence needs of the populations. The
program, started in 2009, aimed at providing biogas energy services to 8,000 house-
holds in rural and urban areas over ﬁve (5) years.
The expected beneﬁts of such a program are countless. They include (among
others): a signiﬁcant reduction of domestic energy expenses, a direct transfer of efforts
from the traditional system (collect of ﬁrewood) to more proﬁtable economic activities,
the substitution of petroleum products which allows the country to reduce its needs of
foreign aids, the exploitation of the bio-fertilizer (see Sect. 2) which increases agri-
cultural production while reducing expenditure related to the use of synthetic fertilizer,
the reduction of respiratory diseases caused by greenhouse gas (especially among
women and children), and indeed, the reduction of the drain of wood reserves as well as
a cleaner environment.
Unfortunately, despite all the potentials of the biodigester, a 2014 report [1] has
shown that on the 8,000 units planned in 2009, only 587 was built in 2013. The reasons
for this, according to the report, were ﬁnancial and technical. First, despite a subvention
of 35% by the state, the total price of the biodigester was way above the annual revenue
of an average household. Second, the technicians did not follow technical recom-
mendations and the families were not properly trained for the maintenance of the units.
This low performance of the program prompted the government to start a second phase
in which 80% of the building cost was supported by the state, with a new target of
10,000 units with 1000 of them to be built in 2015. Although this has the potential to
boost the number of biodigester in the country, the total target of 10,000 units would
only cover 2% of the population. Furthermore, a recent survey [2] operated among 138
households in the regions of Diourbel, Fatick, Louga and Ziguinchor has shown that
57% of the biodigesters are not functional.
Hence, to enable an effective mass adoption of the biodigester, which is necessary
for a qualitative transformation of the opportunities it offers, one needs to undertake a
serious study of the repetitive failures and proposes alternative solutions. The study in
[2] has found that ﬁnance remains to be a big issue with the farmers. In fact, many
families with non-functioning biodigester could not ﬁnish the construction of the
biodigester and had to abandon in the middle of the process. Others, who were able to
ﬁnish, did not have the bovines, which provide the raw materials (dung) needed to keep
the biodigester functional.
In this project, we take an innovative and holistic approach to addressing the issues
that hampers the development of the biogas in the department of Bambey. First, in the
82
I. Diallo et al.

quest of solutions to the problems, we choose to directly collaborate with the farmers
(through the Union of Farmers’ Associations of the Region of Diourbel (URAPD))
who are full partners in the project. Second, instead of simply running a survey to
identify the problems, we closely work with the farmers to pinpoint their root causes
and jointly engineer solutions. Third, we leverage a recently established partnership
between the university and the farmers’ union to together build a support system and
develop the training necessary to build, maintain and fully exploit the biodigester.
Finally, we have put together an interdisciplinary team that includes economists,
chemists, engineers, as well as practicians from the URAPD.
The present paper is a preliminary report on the work of the join team and is
organized as follows. After this introduction, we give a brief presentation of the
biodigester in Sect. 2. Our survey is presented in Sect. 3, which contains a theoretical
model of our study as well as a discussion of the empirical results. In Sect. 4, we then
explore possible ways to enhance the whole ecosystem around the biodigester (from
ﬁnding ﬁnances, to building, exploiting and even commercializing agricultural prod-
ucts). We then conclude this preliminary report in Sect. 5.
2
The Biodigester
The biodigester is often presented as a mechanical stomach [3]. It is fed with organic
material, which is decomposed by micro-organisms in an oxygen-free environment to
produce biogas (methane and carbon dioxide). The decomposition follows a four step
bio-transformation process that lasts for about 4 weeks. The biogas is then used as a
source of energy (mostly for cooking and lighting) and the leftover solid waste can be
directly used as bio-fertilizer or mixed with other vegetal materials to produce compost.
Any organic material can be used to feed the biodigester. In Senegal, biodigesters are
mainly fed with animal (mostly bovine) and human waste. The biogas produced by the
system has no odor and dissolves very easily with ambient air, which makes it less
hazardous than the gas commonly used in Senegal.
There exists different technical designs for the biodigester [4]. In Senegal, the most
widespread design is the GGC2047 [5] model shown in Fig. 1 and which is built
underground. It has several main components. The inlets or feeders, mostly composed
of human latrine (1) and animal dung mixer (2), are the entry point where human/
animal wastes are introduced into the biogas system. Theses wastes then follow an
entrance tube that enters the biodigesters tank (3) where the 4-step bio-transformation
process occurs. The produced biogas consists of about 60% methane, 40% carbon
dioxide. It is ﬁrst accumulated and stored in the biodigesters. Then it exits the system
via a gas pipe, which is connected to the ﬁnal utilization system (cooking or lighting).
The digested slurry ﬂows to the outlet tank through the manhole (4). It is a nutrient rich
fertilizer and is collected via an exit tube into the compost pit (5), where it is mixed
with vegetal elements to produce the compost.
Impact of the Utilization of the Biodigester in the Populations
83

3
Survey and Results
A survey was undertaken during the period of January 15–21, 2017 among 24
households. It was carried after a stratiﬁcation of these households into 2 groups: 12
households with a biogas and 12 without. Among the households without biogas, 5
have cattle and the rest do not have cattle.
The goal of the survey was to derive a ﬁrst comparative analysis of the households
based on the availability or not of a biogas considering different factors such as: need
and motivation, perception of the biogas as a mean to alleviate life condition, the
possibilities it offers with respect to market gardening, awareness about the drip
watering system, self-sufﬁciency and quality of life, the ability of families to com-
mercialize their products in the market, and the impact of gender in the activities of the
populations.
1. Theoretical Model Speciﬁcations
We adopt a logistic model [6] to test the impact of the different factors on the prob-
ability of an increased agricultural proﬁtability. In this model, we are interested in
factors (personal, institutional, economic cultural, and social) that might impact agri-
culture proﬁtability. As such, we consider, in our model, a variable yi that represents
the proﬁtability of a given farmer, and a vector X(1  K) of variables that is composed
with the different factors (personal, institutional, economic cultural, and social). Con-
sidering that yi is a binary variable (proﬁtability of not), the non-linear probabilistic
model to be estimated is given by:
y
i ¼ ai þ bXi þ ei;
where y
i is an estimation of yi b is deﬁned as the ratio of the probability pi that event
i occurs and the probability that it does not occur 1  pi
ð
Þ, ai is a reference value when
all the Xi are set to be equal to zero, and ei represent the noise and are considered to iid
with a logistic distribution (as discussed next).
Fig. 1. Schema of the biodigester
84
I. Diallo et al.

The choice of the logistic model is motivated by the binary nature of the dependent
variable. More precisely, the variable is speciﬁed as follow: yi ¼ 1 in case of an
afﬁrmation and yi ¼ 0 otherwise.
The model that links the two variables (yi and X(1  K)) can then be written as:
Pr yi ¼ 1
ð
Þ ¼ F ai þ bXi
ð
Þ;
Pr yi ¼ 0
ð
Þ ¼ 1  F ai þ bXi
ð
Þ
In the above equation, Pr yi ¼ 1
ð
Þ is the probability that the event “Augmentation of
agricultural proﬁtability” and is given by the logistic formula:
Prob yi ¼ 1
f
g ¼
eai þ bXi
1 þ eai þ bXi ¼
1
1 þ eaibXi
a. Empirical Approach
Based on the above theoretical model, we test the hypothesis of “Augmentation of
agricultural proﬁtability” using the following equation:
ACOVI ¼ f PCOMM; DIVENTE; SDEMA; RESEAU; PROCOMM; FORMCOM
ð
GENRE; MOTIVATION; EPARGNE; ECLAIRÞ
Where the variables in the right hand side of the equation are described in the
following table (Table 1):
A summary of the statistical analysis is shown in Table 2.
As suggested in the table above, some of the statistics obtained with the data are not
signiﬁcant. This might be due to the representativeness of the sample which consists of
a very small percentage of the population. Another reason might be the model. In fact,
in this preliminary report, we have used the logistic model for pre-analysis, without any
guarantee that it is the best ﬁt. These represent some limitations that we plan to address
in subsequent reports of this study. However, despite the cited weaknesses, the
Table 1. Description of the variables
Variable
Description
PCOMM
Communication infrastructure/Information
DIVENTE
Selling opportunities
SDEMA
Market Demand
RESEAU
Network and Information System
PROCOMM
Commercialization
FORCOMM
Formation in commercialization/marketing techniques
GENRE
Gender
MOTIVATION Motivation
EPARGNE
Savings
ECLAIR
Lighting for market gardening during evening time
Impact of the Utilization of the Biodigester in the Populations
85

empirical results offer a wide range of differences and paradoxes in the comparative
impacts between the families that own a biogas and those that do not own one. The
main goal of the present paper is to relate those observations without dwelling into their
interpretation. As a way to cross-validate the observations, work sessions have been
held with the URAPD actors, who, in many cases gave explanations why certain
phenomenon were observed.
b. Data Analysis
At ﬁrst sight, an overall analysis of households with biogas have shown that, despite
the fact that almost all those families have indicated that they had an extra source of
income before owning a biogas, they have acknowledged that it is thanks to the biogas
that they have seen a marked improvement in their living conditions. For 83% of the
families that own a biogas, it has given them the opportunity to do poultry farming,
selling fertilizer, market gardening, and ﬁsh farming (the last two still not yet imple-
mented). However, they survey have shown that the (extra) activities related to the
biogas do not constitute a handicap to the other activities and tasks the families used to
have. In fact, only 25% have afﬁrmed that they indeed needed to borrow time destined
for other activities to efﬁciently operate the biodigester. On the other hand, all
respondents consider that it would be pertinent to automatize the different tasks needed
to operate the biodigester. When it comes to the ability for families to sustain
Table 2. Statistical analysis
Dependent Variable: ACOVI
Method: logistic
Date: 01/22/17 Time: 22:42
Samples: 24
Included observations: 24
Variable
Coefﬁcient
Std. Error t-Statistic
Prob.
PCOMM
−0.320444 0.440582
−0.727319 0.4790
DIVENTE
0.653665 0.478291
1.366669 0.1933
SDEMA
0.573974 0.462306
1.241547 0.2348
RESEAU
0.039677 0.520818
0.076182 0.9404
PROCOMM
0.098184 0.305687
0.321192 0.7528
FORMCOM
0.528917 0.463648
1.140773 0.2731
GENRE
0.623403 0.428617
1.454453 0.1679
MOTIVATION
0.388366 0.432264
0.898446 0.3841
EPARGNE
0.420646 0.290773
1.446647 0.1700
ECLAIR
−0.695360 0.455505
−1.526570 0.1491
R-squared
0.169992 Mean dependent var
0.625000
Adjusted R-squared
−0.363585 S.D. dependent var
0.494535
S.E. of regression
0.577482 Akaike info criterion
2.034058
Sum squared resid
4.668796 Schwarz criterion
2.524914
Log likelihood
−14.40869
F-statistic
0.318589
Durbin-Watson stat
1.501898 Prob(F-statistic)
0.955117
86
I. Diallo et al.

themselves throughout the year, 42% of the families having a biodigester needed to
have out-of-season activities, while 58% of families without the biogas needed out-of-
seasons activities.
Motivation and Needs
The survey have shown that the population are strongly motivated to acquire a
biodigester. In fact, among the households without biogas, we have notice that the
quasi-totality (92%) have taken actions destined to earning one. The actions include:
inquiring about the biodigester (100% of the respondents), saving money to get one
(42%), and looking for alternate funding (8%).
Impact on the quality of life
Overall, the survey has shown that having a biogas is a determinant factor in improving
people’s quality of life. In fact, among all respondents, 63% have observed an
improvement in their life quality; and the large majority of people who have observed
such improvement own a biodigester. Also, 67% of people with biodigester have
observed an improvement in their life quality, while 58% without biogas have declared
having observed some improvement in their life style in the last years.
The analysis has shown a clear difference in the possibility of savings between the
population owning a biodigester (50%) and the population not having one (17%). In
economic theory, savings is an important indicator because it shows the capability of
the families to invest and their aptitude to create and develop income generating
activities. Regarding this aspect, the analysis has shown that 58% of families with
biogas have projects to develop new activities, while 42% of the households without
biogas have new activity projects. Investments by families with biogas have mainly
been on activities that are related to the biodigester, while investment among other
families are mostly on activities unrelated to the biogas such as selling of ﬁrewood.
There is a large gap between families who consider that they are in a difﬁcult situation
and they will not be able to get out of it: 58% of people without biogas against 25% of
respondent with biogas. When asked the question “what do you think will help you get
out of it”, 42% of families without biogas respond that they will need an external fund
while 8% of the people with biodigester feel that they need to develop the activities
related to the biodigester.
Market gardening, drip watering, and working during evening time
The analysis of the data has revealed the farmers are well aware of market gardening
and drip watering technics. In fact, among all the surveyed families, only 4% afﬁrm not
knowing about market gardening and only 13% did not know about drip watering. The
paradox observed at this level is that households that have more resources to develop
market gardening, have done less: 8% of the families with biogas are doing market
gardening, against 25% of the households without the biogas. A similar phenomenon is
observed when people are asked about the “pertinence of night time lightening of the
gardens”. In fact, only 17% of the families with biogas had a favorable opinion about
night time lightening, while 58% of the families without biogas feel that night time
lightning will be useful for market gardening. This indicates that we are in a situation
where either a family is into biogas or it is into market gardening. This is surprising
because it is the families with biogas that are more encouraged to getting into market
Impact of the Utilization of the Biodigester in the Populations
87

gardening because of their ability to produce organic fertilizer, which is known to be
very efﬁcient for market gardening. Overall, 46% of the respondents feel that night time
lightning will improve productivity, and the majority of that group (80%) think that this
improvement will be more than 50% of the current productivity. One important reasons
given by some of the interviewers is the following: because of the insufﬁciency of
water, families have to take turns to water their plants (if many families use the
watering system at the same time, the rate will drop a lot and each one will suffer). But,
currently, watering has to be done during the day time. If however, a there were light
during the evening, some families will be able to use the system during the day while
others in the evening.
Quality of life and Self-sufﬁciency
The quality of life in the studied localities depends on local factors including (and not
limited to) the ability of the households to ensure food self-sufﬁciency throughout the
year, their propensity to remain debt-free and what kind of roof they own.
The overall observation is that, with regard to the number of families who have
food self-sufﬁciency, there is a ratio of 3-to-1 between families with biodigester (88%)
and families without (33%). With regard to the level of indebtedness, the data suggests
with high conﬁdence that it is a very good indicator on whether the biodigester can help
families get out of poverty. In fact, our survey has revealed that 92% of families
without biogas are indebted against only 25% of families owning one.
The hypothesis that the type of roof is an indicator of poverty is not conﬁrmed by
the data. Here, the observed paradox is that none of the surveyed families owning a
biodigester have a brick house, while among the families without biodigester, 4 out of
12 (33%) are living in a brick house. In general, most of the houses are built in zinc.
They represent 67% of the surveyed population. In second position, come the straw-
roofed houses that represent 29% of the households. Only a tiny fraction of the
respondents live in a bricked roof.
Commercialization and Market Opportunities
We made the hypothesis that a family that is more self-sufﬁcient will be better able to
commercialize larger portions of its production. In other terms, if a family is able to
commercialize part of its production, it is because it has some level of self-sufﬁciency
in its production. After the analysis of the data, we observed that 42% of the house-
holds commercialize no less than 20% of their production. However, we have noticed
that a large fraction of the households (33% among the ones with biodigester and 58%
among the ones without) have difﬁculties in the commercialization. These difﬁculties
are more due to the lack of appropriate communication means that would enable
families to acquire good information about market and demand. In fact, among the
respondents, 38% use words-of-mouth to get information about the market, 25% use
the telephone, and most of the families have no prior information before going to the
market. Almost, all the households are aware of such difﬁculties and 63% of them are
convinced that having a communication network that would provide information will
help them to better sell their products. Also, 15 out of 24 respondents believe that a
formation in marketing and commercialization techniques will provide substantial
help. Despite the constraints mentioned above, more than half of the households (58%)
consider that their production is ﬂexible enough to adapt to most market demand.
88
I. Diallo et al.

Women Empowerment
In developing countries, especially in rural areas, people rely on biomass, such as
fuelwood, charcoal, agricultural waste and animal dung, to meet their energy needs for
cooking. Cooking is mostly a women’s task, who need to go into the bush to collect the
woods and/or animal dung and carry them back home, usually on their head, in a trip
that can take hours of walk. During the cooking process, they expose themselves and
sometime their children to the air pollution caused by the smoke. In fact, the World
Health Organization (WHO) estimates that 7 million premature deaths per year are
directly attributable to indoor air pollution from the use of solid fuels [7]. That is more
than 4 000 deaths per day, more than half of them children under ﬁve years of age. This
means that indoor air pollution associated with biomass use is directly responsible for
more deaths than malaria, almost as many as tuberculosis and almost half as many as
HIV/AIDS.
Our survey has shown that women in household with a biogas beneﬁts from it in
many ways. First, by using the gas produced from the biodigester, women no longer
need to spend hours and kilometers of walk to cut, get and carry the ﬁrewood. This
gives them time to engage into other activities such as education for themselves or for
their children and revenue generating activities (e.g. home gardening). Also with the
biogas, cooking time becomes much shorter. Moreover, with the biogas, women can
now cook in a smoke-free environment (see Fig. 2). This drastically participates in
improving their health. Overall, the survey has clearly shown that the biodigester is an
effective means to empower women.
4
Future Perspectives
a. Automatization
The study has shown that the most manual and labor-intensive task of the biodigester
system is the production of bio-fertilizer. When the residual waste gets into the compost
pit, it needs to be mixed with vegetal elements in order to produce the compost. This is
a very labor-intensive process that that can even be dangerous because for a good mix,
the farmers needs to get into the compost pit, while the compost is still wet. Once the
compost is mixed and dried, it needs to be dug out the hole. This process is as intensive
Fig. 2. (Left) Woman cooking with biogas (source URAPD). (Right) Kitchen with ﬁrewood
(source APS).
Impact of the Utilization of the Biodigester in the Populations
89

as the mixing (or even heavier labor because now the compost pit is full). In addition to
that, the quantities to be mixed (vegetal elements and residual waste) are determined
using bare approximation. This can lead to a compost that is sub-optimal or even
sometime unusable, as it was the case with one of our visited household (see Fig. 3).
The main problem with the current design is that the compost pit is a big hole dug
into the ground. We envision to automatize the mixing and unloading process by using:
a temporary storage system for the leftover waste, a transport belt, a “leaky bucket” for
the vegetal elements, and a mixer bowl. The storage system will be equipped with a
metered pump with a constant rate. Waste will be pumped out of the storage and carried
over the belt. The “leaky bucket” will constantly pore the needed amount of vegetal
substances to produce optimal fertilizer. Mixing will then be done in the mixing bowl
and the ﬁnal product will be unloaded to a ﬁnal storage unit for drying. The whole
system will be powered by solar energy.
b. Rural Network
Our survey has elucidated the importance of a networking infrastructure in rural areas.
Such network would enable many applications such as: (1) a rural online marketplace
where farmers could ﬁnd information about products demand and prices, (2) an
IoT-based information system that can enable remote monitoring of farms and smart
agriculture, (3) telemedicine services that will enable doctors from the urbans areas to
diagnose/monitor patients in rural areas, and (4) a system to collect and disseminate
environment data (just to name a few).
As network infrastructure, we propose a solution that is based on a simple
opportunistic principle: given any situation, use the best connectivity solution avail-
able; and when no solution is available at the moment, use delay/intermittent tolerant
(DTN) solutions to offer “ofﬂine” services. Our network will be backboned using point-
to-point Long Distance Wiﬁlinks [8]. On the access links, it will use cellular data
services wherever available. If none of the data services is available, we will use
Fig. 3. (Left) Farmer mixing compost. (Right) Suboptimal compost due to wrong mixed
quantities
90
I. Diallo et al.

(compressed) SMS services as a support for data communication. In areas with no
network coverage, we will use DTN solutions [9]. For the seamless integration of all
these technologies, we implement our lightweight platform on NDN (Named Data
Networking) [10], a new architecture proposed for the future Internet.
The focus of our solution will be on building a “local” Internet that interconnects
rural communities among themselves (in-out approach). This is complementary to the
(out-in approach) effort of bringing Internet to rural communities. However, this task is
harder to achieve and needs a huge preliminary investment (mostly from government to
build infrastructures and lay ﬁber), which unfortunately have been taking too long to
materialize. In the meantime, rural communities will be waiting; and hence missing
tremendous opportunities that would be possible by being “locally” connected. This is
what motivated our in-out approach to building a “local Internet” for rural commu-
nities. Bearing in mind that the ultimate end will be to connect communities to the
global Internet, we plan to build a system that is “easily pluggable to the Internet”.
c. Formation
Recently, the University of Bambey (UADB) and the farmers’ union (URAPD) have
signed a partnership agreement whose main objective are social development and
capacity building in the region of Diourbel. This convention is part of the university
community service, which is one of UADB’s missions. It is based on three pillars:
Training, Support and Research.
The training focuses on the know-how of the farmers and their experience. It
involves both theoretical and practical aspects and is sanctioned with a professional
degree from the university. For the case of the biodigesters, the training will focus on
three main areas. First, farmers will be formed on the engineering techniques to build
and maintain a biodigester. The second part of the formation consists of teaching to
farmers about optimization and resource allocation techniques (for example, how much
crop or biofertilizer should one sell/keep given current market information?). The ﬁnal
module will be on commercialization and marketing techniques.
The formation will involve both onsite and distance education. For the latter, the
implementation of the rural connectivity network will be of vital importance.
d. Crowdfunding for the Biodigesters
Raising funds is an important step towards the realization of any development project,
whether it is in economic, social or environment oriented. However, despite the exis-
tence of many sources to ﬁnance development projects, most of the funding require-
ments set by governments, international institutions and NGO are very selective and
hard to satisfy by the majority of farmers. In addition, a big fraction of the funds usually
do not reach the intended beneﬁciaries because of administrative expenses and some-
time corruption. This has led to the development of new, participative funding methods,
such as crowdfunding [11], which is based on public participation to fund projects.
Crowdfunds mostly target small scope projects such as small retailers, artisans,
farmers’ associations, and projects in social, economic and environmental develop-
ment. The funds are usually provided by people with the willingness to help without
going through the traditional processes, but rather via lending platforms, easily
accessible online (e.g., Microworld, Babyloan, Wiseed, Ecobole). Despite critics,
mostly due to observed dysfunctions in some platforms, crowdfunding is today
Impact of the Utilization of the Biodigester in the Populations
91

considered as a credible alternative (to the traditional funding methods) that offers a
direct and targeted process and is free from the many constraints observed with the
traditional methods. It is a reliable and quick way to raise funds for development
projects. Furthermore, in crowdfunding, the risk is very limited for the lenders because
they participate in small amounts for each funded project.
As stated earlier, one of the reasons why the biodigester is still not adopted in mass
is that many farmers lack the funds necessary to acquire and maintain one. As a
solution, we propose to leverage the existing crowdfunding services. Funding a
biodigester via crowdfunds involves three main actions. First, in crowfunding, the
success of a project crucially depends on how it is presented. We aim to work with the
farmers to build the case for the biodigester by putting forward the many potentially
beneﬁts (economic development, women empowerment, healthcare and environment)
through a quantitative study of the cited beneﬁts. Second we will evaluate the details of
the ﬁnancial needs by considering the whole chain: from building the biodigester to
commercialization of the products (crops or biofertilizer). The ﬁnal step will be the
production of a business plan that is based on these previous studies. The business plan
will also include a strategy for fund allocation once funds become available.
5
Conclusion
In this work, jointly done by the Universite Alioune Diop de Bambey (UADB) and the
Union of Farmers’ Associations of the Region of Diourbel (URAPD), we preliminarily
report on the reasons why the biodigesters are not delivering the expected potentials
beneﬁts and propose remediation measures. Our survey has shown that the lack of
ﬁnance is the main cause of the limited adoption of the biodigester. As a solution, we
are exploring crowdfundings sources to provide to farmers the funds necessary to build
and operate the digester. Another ﬁnding is the intensive manual labor needed to
operate the systems (especially the production of biofertilizer). We accordingly propose
a way to automatize this process. Finally, we discovered that farmers were lacking the
market information and the marketing skills needed to make the most of their produced
goods. With regards to that, we are building an information system that will enable
farmers to acquire quasi-real-time price and market information. We also leverage the
recently partnership framework between the university and the farmers associations to
provide formation program that will give to farmers the needed technical and marketing
skills to commercialize their product. Follow ups of this paper will focus on the
implementation and assessment of the proposed solutions.
References
1. Report EnquetePlus: Bilan Decevant, Perspectives Prometteuses. http://www.enqueteplus.
com/content/programme-national-de-biogaz-bilan-d%C3%A9cevant-perspectives-prometteuses.
Accessed 21 Feb 2017
2. Ba, A.: Contribution du Biodigesteur sur le Renforcement de la Resilience des Menages
Ruraux. Master Thesis, University of Thies, October 2016
92
I. Diallo et al.

3. Gómez, C.R.B., Plata, G.O., Hernández, M.A.D., Suárez, J.H.R.: Instrumentation of an
animal manure biodigester to analyze its dynamic behavior. In: 2010 IEEE ANDESCON,
Bogota, Colombia, 15–17 September 2010
4. Engineers Without Borders: Biodigester Design and Construction: Understanding the Basics
of
a
Biodigester.
https://www.ewbboston.org/wiki/lib/exe/fetch.php?media=projects:
tanzania:biodigester_design_construction_building_-_rural_costa_rica.docx.
Accessed
15
Jan 2017
5. Sahu, O., Abatneh, Y.: Study of biodigester design for fuel and fertilizer. Int. J. Sustain.
Green Energy 2(4), 147–152 (2013)
6. Hosmer, D.W., Lemeshow, S.: Applied Logistic Regression, 2nd edn. Wiley, New York
(2000)
7. WHO Report: 7 Million Premature Deaths Annually Linked to Air Pollution, March 2014.
http://www.who.int/mediacentre/news/releases/2014/air-pollution/en/. Accessed 25 Jan 2017
8. Subramanian, L., Surana, S., Patra, R., Nedevschi, S., Ho, M., Brewer, E., Sheth, A.:
Rethinking Wireless for the Developing World. University of California Berkeley, White
paper, November 2006
9. Fall, K.: A delay-tolerant network architecture for challenged internets. In: SIGCOMM,
August 2003
10. Xylomenos, G., Ververidis, C.N., Siris, V.A., Fotiou, N., Tsilopoulos, C., Vasilakos, X.,
Katsaros, K.V., Polyzos, G.C.: A survey of information-centric networking research. IEEE
Commun. Surv. Tutorials 16(2), 1024–1049 (2014)
11. Vachelard, J., Gambarra-Soares, T., Augustini, G., Riul, P., Maracaja-Coutinho, V.: A guide
to scientiﬁc crowdfunding. PLoS Biol. 14(2) (2016)
Impact of the Utilization of the Biodigester in the Populations
93

How to Increase Intuition for Entrepreneurship
Spirit in Innovation Process?
Henri Samier1(✉), Emmanuel Chene2, Hervé Chistofol1, and Simon Richir1
1 University of Angers, Arts et Metiers ParisTech, Angers, France
{henri.samier,herve.christofol,simon.richir}@ensam.eu
2 LEMNA Laboratory, University of Nantes, Nantes, France
emmanuel.chene@univ-nantes.fr
Abstract. In 2017, how to increase intuition for entrepreneurship in innovation
process? Especially for generations to come, who are the current students, now
faster and natively digital because they are born with a phone or a tablet in hand?
They are fully connected and always active on social networks, which diverts
their attention during the course. Thus the simultaneous use of several media
captures their attention, that is the reason why we have to change the way of
teaching with variations of rhythm if we do not want to be faced with an army of
computers that ramparts between teacher and students… Beside their attention
time is reduced and consequently the lesson has to be more fractionated. We
introduce in this paper how to increase the digital natives’ attention with a new
teaching method we named: “Intelligence, Creativity & Intuition”. Our method
is based on three learning channels: Visual, Auditory and Kinesthetic.
Keywords: Pedagogy · Innovation · Teaching · Intuition · Entrepreneurship
1
Introduction
Crises and transitions cycles are the cause of the mutation of the world we are currently
living. We’re living a fabulous time where digital technologies are shaking the world
and enabling engineering students to develop new abilities as well as deeply changing
their relationship to information and knowledge.
For over a decade virtual and augmented reality has revolutionized the design
methods, the innovation process and the relationship between the Engineer and the
Product. Those technologies have widely impacted the value chain of the company and
its organization. Those ones have now to be more agile and human (Laloux 2014).
In 1995, we created one of the ﬁrst postgraduate training at the University of Angers
in France to master the “Strategic Information”. Three years ago, we opened the ﬁrst
engineer cycle “Intangible Assets and Knowledge Management”. These new programs
anticipate the evolution of current issues. And today, the classical teaching methods no
longer meet students’ “digital native” or “Y Generation” needs. They have shorter
learning cycles, diﬀerent decision-making processes and a new form of more agile
intelligence.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 94–103, 2018.
https://doi.org/10.1007/978-3-319-72965-7_8

Each generation has had its own centers of interest depending inter alia on the
economical and social environment. Bobos in the 50’s were centered on anti conformism
and on the individual’s interiority, Yoyos or Generation X in the 90’s were more «I,
myself» orientated whereas Generation Y «digital natives» are centered on human link
with the others thanks to the new technological tools.
Generation Y’s engineers do not have the same values, needs and desires as Gener‐
ation X. Their temporality is not the same as its predecessor because they are born with
an iPod in hand. Today thanks to social networks, phones and other tablets there are
totally connected.
Those young people are subjected to so many temptations in their daily life with the
web and social networks that they hardly maintain attention in the classroom.
Their attention should be captured when using multiple media at once and we must
change the way of teaching with variations of rhythm if we do not want to be faced with
an army of computers that rampart between teacher and students… The proposed
experiment can punctuate the course to keep the attention of students, to interact with
them and involve them in the educational process. For example, we start the course with
a musical introduction to achieve a symbolic break with the previous course.
Then we continue with a question on a topical issue related to the course. At the end
of the replies and exchanges with students, we begin the main course itself, which alter‐
nates theoretical elements and practical applications embellished with images. Then at
some point, we stop the course with a shifted video on a subject not directly related to
the topic of the day.
The goal is to change rhythm and refocus attention on a new subject for a few
minutes. Then a short debrieﬁng session is important to mentally integrate the main
points of this video, so we take the rest of the time to do it. With this experiment, we
observed that students closed up their computers after a few minutes.
This approach refocuses attention on what happens in the classroom. It is a “zapping”
method we use and impose them and which has the advantage of synchronizing the
course at their current level of attention (Generation Y). We’ll show how the introduction
of “zapping” in our own teaching provides a real added value in active learning.
On one hand we’ll present the theoretical foundations of this method and on the other
hand we’ll develop the special alchemy of pedagogical sequence that produces amazing
and promising results for accelerated Innovation Education.
2
New Way of Learning
At the University of Angers, the Institute of the Engineering techniques of Angers
(ISTIA) was a pioneer in the integration of the new technologies and it opened Trainings
of Master and Engineer very original at that time.
There was for example the creation of the Masters such as “Strategic Information
and Technological Innovation (1996), Digital Modelization and Virtual Reality (2000)
and regarding the engineer curriculum, the option “Intangible Assets and Knowledge
Management (2012) which trains the Engineer of the knowledge of tomorrow.
How to Increase Intuition for Entrepreneurship Spirit?
95

Today our engineer students get new skills based on ever more accessible technol‐
ogies. These skills make them develop new agilities and they broadly inﬂuence their
vision of knowledge and learning and their relationship with their teachers too.
Why do the classical teaching methods no longer meet the digital generation
students’ needs? Each generation has had its own interest centers according to the
economical and social environment. “Bobos” in the 50’s focused on the ﬁght against
conformism and the interiority of the human whereas “Yoyos”, generation X in the 90’s
were more “me” orientated. Generation Y also called Millenials are centered on the
human link with the others thanks to the new technological tools. “Millenial generation”
people have diﬀerent values for their experience is diﬀerent from the one of generation
X. Students are faster with digital because there were born with a phone or tablet in the
hand. They are fully connected and always active on social networks which diverts their
attention during the course (Robinson et al. 1999).
Their attention should be captured when using multiple media at once and we must
change the way of teaching with variations of rhythm if we do not want to be faced with
an army of computers that rampart between teacher and students…
3
New Teaching Method
We have decided to experiment a new type of course we called “brain shaking”, about
“Innovation, Creativity and Intuition” (ICI) with the aim that our students improve their
intuitive, emotional, creative, innovative skills (Goleman 1995; Corsi et al. 2006).
We have investigated several ways in the developing of new pedagogic methods.
We are going to show you the method enhancing the development of intuition with
engineer students. The tested method consists in 7 Steps: Step 1: musical introduction,
Step 2: course sequence “brain shaking”, Step 3: Zapping of the course, Step 4: course
sequence “Five I”, Step 5: Exercise of music and kinesthesia, Step 6: feeling of intuition,
Step 7 course about intuition..
3.1
Step n°1: Musical Introduction
The course starts with a musical introduction which creates a kind of decompression
chamber after their previous course or activity and contributes to regulate their mood
too (North and Hargreaves 2009). It is important to note that the eﬀects of music are
diﬀerent depending on the introvert or extravert nature of the student (Doyle and
Furnham 2012).
Some researches (Campbell and Hawley 1982) show that in library introvert students
choose isolated places to study and are only 25% to listen to music while working
whereas extravert students prefer to sit in noisy places and are 50% to listen to music
and study at the same time (Daoussis and McKelvie 1986).
We are deﬁnitely aware of the fact that listening to music can either have a focusing
eﬀect of attention or have an attention scattering eﬀect. Our goal as teachers is to open
a new space-time at the beginning of the course.
96
H. Samier et al.

We do notice concerning some students that they relax, with a smile in a couple of
seconds and according to the researches of Olds and Milner (1954), the feeling of
pleasure may be increased upon activation of a brain area called «reward circuitry».
This “pleasure and joy eﬀect” which occurs by secretion of neuromediators such as
dopamine and oxytocin has been observed in many works (North and Hargreaves
2003; Blood and Zatorre 2001; Salimpoor et al. 2011). The music has indeed an eﬀect
on our feelings expression, on our memory and on learning upon activation of the tonsil
and all subcortical nuclei located in the temporal lobe (Blood et al. 1999; Koelsch et al.
2005, 2006). Lots of researches have shown the inﬂuence of music on people well being
(North and Hargreaves 2003; Lesuik 2005; Schellenberg 2005). We have made the
assumption that music could bring wellness to the students and could play on their
listening capacity, on their attention, and thus improve their presence “fully alive”.
According to H. Platel, researcher in neuropsychology at the University of Caen
“The pathways of music in the brain are much more complex than those of speech, for
example, and seek diﬀerent brain areas: music stimulates, relaxes, soothes pain, but also
has the ability to increase plasticity of the brain and causes changes in synaptic connex‐
ions”. For the professor, listening to music while working stimulates memory and makes
feel less tired (Fig. 1).
Fig. 1. Vibrations and music inﬂuence to the memory.
Besides listening to music induces a change in the skin conductivity, a change in
heart rate, breathing and body temperature (Salimpoor et al. 2011). It generates lower
negative emotions and change in time perception.
We start the course by some pieces of music chosen by the teacher whose duration
varies between 2 and 3 min. This listening step lets the students ﬁnish to sit in the
amphitheater and to release their mental during this musical break. We have noticed the
students group got calmer, more attentive at the beginning of the course. Today the
students of Generation Y have a musical culture for they listen to music every day
through the natural access to online platforms such as musique: Napster, Spotify, Deezer,
Apple Music, Rhapsody, Rdio et Google Music. To increase the eﬃciency of this
musical introduction, every student should ideally be able to choose his own music to
be 100% a source of pleasure and motivation as revealed by the work of Krause et al.
(2014).
How to Increase Intuition for Entrepreneurship Spirit?
97

3.2
Step n°2: Course Sequence “Brain Shaking”
This step consists in making the engineer student think about the way he questions the
world and he questions himself. The idea is to make the student aware of the fact his
certainties can limit his vision of the world. They also restrain in his creativity and in
his exploration search ﬁelds.
Our ancestors used to consider the earth was ﬂat. This collective belief was chal‐
lenged by the discovery of the Geocentric model (the sun turning around the earth), then
itself later challenged by the Heliocentric system notion and ﬁnally replaced by the idea
of the solar system we all today believe in. Important is to recall that those scholars were
in trouble when they supported those theories.
Nowadays with the Einstein Theory of “Quantic entranglement” and gravitational
waves (observed in 2015), we may by at the era of a paradigm change and at a beginning
of new scientiﬁc inquiries.
For the students those inquiries lead to think about the impact of the scientiﬁc
certainties on the world. It means to wonder which current scientiﬁc assertions may
make our great children smile in the future.
This is important in the questioning of the students facing their beliefs and thus their
limits. The fact that a student believes that something is impossible, makes it impossible,
whereas if he thinks it is possible, until proven otherwise, then he can explore his limits
to check all possibilities to act.
Seneca said: “It is not because things are diﬃcult that we do not dare, but because
we do not dare that they are diﬃcult. Extrapolating this quote, we can say “This is not
because things are true that we believe them, it is because - as we believe them they
become true”. If the student realizes that “the only limits are the one that we impose”
then he can allow himself to be more creative, inventive and innovative.
3.3
Step n°3: Zapping of the Course
The zapping step consists in diverting the student’s attention from the course matter to
another issue for a couple of minutes. For that we use some 1 to 3 min videos about
issues like: dare to do, fears, conﬁdence, assertivity, the power of an idea, mental silence,
presence, the power of words, and the non judgment. Students will be ﬁrst asked to
explain the subject dealt by the video. Then a discussion break will enable the students
to intervene and react. Finally the teacher will formally debrief the video. This short
ﬁlm said to be a zapping step in the learning process enables to capture the attention of
the group, making them concentrate themselves again before coming back to the course
issue.
A example: My Fears, video to see “Dare”: https://www.youtube.com/watch?
v=sb2YOg_dkQM, The video shows people in situations of fear. Those people when
they face their fears and overcome them, realize then they succeed in doing thing more
easily. Fear can therefore be considered as the distance between me and my reality. The
idea of the video is to show that often the thing holding us in our head.
If students ask how to get out of his own fears, we can make them watch video “Your
deepest fears”: https://www.youtube.com/watch?v=2_fDhqRk_Ro.
98
H. Samier et al.

In this video the character says a bright text of Marianne Williamson: “Our deepest
fear is not that we are inadequate. Our deepest fear is that we are powerful beyond
measure. It is our light, not our darkness that most frightens us. We ask ourselves, Who
am I to be brilliant, gorgeous, talented, fabulous? Actually, who are you not to be? You
are a child of God. Your playing small does not serve the world. There is nothing
enlightened about shrinking so that other people won’t feel insecure around you. We
are all meant to shine, as children do. We were born to make manifest the glory of God
that is within us. It’s not just in some of us; it’s in everyone. And as we let our own light
shine, we unconsciously give other people permission to do the same. As we are liberated
from our own fear, our presence automatically liberates others”.
Students will reﬂect on their deepest fears which limit their action and work to
dissolve them.
3.4
Step n°4: Course Sequence “Five I”
This step lists the “5 I” of the perfect creative, intuitive innovator. For our pedagogic
team, the right innovator must develop his inspiration, imagination, his diﬀerent intel‐
ligences, his recklessness (insouciance in French) and his intuition. Each notion will be
presented and explained and connected to the others.
Inspiration is based on ideas coming from the nature (like biomimetism), from the
arts and more generally from outside of yourself.
Imagination ﬁnds its origin in your readings, your education and your dreams. The
development of imagination is possible through the book reading, even science ﬁction
comics, that’s why we bought over 100 science ﬁction books in the library for students.
Intelligences according Gardner (1999) are the logic (ability with mathematics and
numbers), linguistic (ability with poems and literature, kinesthesia (ability to learn with
doing something), interpersonal (ability with myself), intrapersonal (ability with other
people, with my ﬁend and my family, emotional (ability with my feeling, my emotional
maturity, musical (ability with rhythms, spatial (ability with 3D space) and naturalistic
(ability with the nature). The student realizes the multiplicity of the intelligences shapes.
He can assess his level in each intelligence type.
In an oral exercise, in circle we can make them speak in pairs on their types of
intelligences (Fig. 2).
Fig. 2. Students during an exercise talking about their intelligences types
How to Increase Intuition for Entrepreneurship Spirit?
99

At the end of this sequence we note that inspiration and imagination can be increased,
everyone has his own intelligences, recklessness and intuition are rarely approached in
teaching. The challenge is to make the student discover how to develop his intuition
(Samier 2014).
3.5
Step n°5: Cymatic, Music and Kinestesia
A ﬁrst video about cymatic presenting the principle of the wave creating the shape
according to Jenny studies (1967, 1974) is played. (https://www.youtube.com/watch?
v=WaYvYysQvBU, https://www.youtube.com/watch?v=3-Mc41OO0ho) (Fig. 3).
Fig. 3. Vibrations according to the frequencies create geometric shapes that are diﬀerent.
As we can see in the video, the vibrations also create shapes in the water and as our
body is more than 70% water, we can question if the vibrations have an inﬂuence on the
water in our body and if they temporarily alter certain forms of liquid parts. And since
music is a form of vibration, we can conclude that music aﬀects our body.
Thus the students realize that music, as a vibes has an inﬂuence on the vegetal, animal
and human world. They will be able to feel music (drums) and unleash their body
expression blindfolded (by a mask Quies for example) in order they can’t see each others.
This exercise takes place in a large room so that the students can move 7 to 10 min long.
Then at the end of the piece of music the students remain standing without any movement
feeling their breath and sensations inside. They are now ready to feel their intuition.
The exercise, led the student to “let go of the mind” in order to focus on the vibration
of the music, the movement of their bodies and attention to their breathing. After
completing this sequence, we can then experience the feeling of intuition.
3.6
Step n°6: Sequence “Intuition Feeling”
Another music, calmer this time, starts this step. The students are asked to breath
according to a guided “squared” breathing method and this 5 times as follows: They
100
H. Samier et al.

have to breath in 5 s, block their breath 5 s, breath out 5 s, block their breath 5 more
seconds.
The idea is to keep students’ attention on their breathing by having letting go of their
mind through “African Drum”.
Then eyes closed with the thumb and index ﬁnger, they will measure their pulse
under the throat and observe it, feel it (amplitude and frequency).
With their free hand, they will slightly hit the pinna (ear part) from the back to the
front of the ear in order to feel the increase or decrease of the radial pulse ﬂuctuation.
Then the pulse becomes normal again. This perception of the radial pulse variation
(Vascular Autonomic Signal) was discovered by Doctor Nogier and Nogier (1985). He
studied the interdependency in the functioning of the psychological and biological
systems which impacts the radial pulse.
Feeling this variation is a possible access to your intuition. Your intuition tells you
what’s good for you. So if this ﬂuctuation is lower this will mean it is positive for you.
On the contrary a stronger ﬂuctuation has a negative meaning.
Once they are aware of their pulse variation the student should be able to test their
own sense of intuition, for instance by using the “U theory” (Sharmer 2008).
3.7
Step n°7: “Intuition” Course Sequence
This so-called inverted last step (for the theory arrives after the practice) develops the
diﬀerent theories concerning intuition. Some philosophers like Aristote, Kant, Bergson,
Jung and scientists like Pointcaré, Einstein, and even a musician like Mozart have all
expressed an opinion about intuition without being able to describe how it works and
how to use it (Shirley 1996). None said how to develop it and for us, it’s Schulz and
Lisa (1998) and Hogarth (2001), who has opened a gate for teaching intuition.
In a second time we approach the theory of the “Vacuum Auricular System” together
with the cymatic concept to make the students understand the phenomenon of intuition
felt in the previous exercise.
4
Discussion
With this workshop “Innovation, Creativity, Intuition” we can see the engineers are
aware of their intuition, they are more creative, are clearly more able to innovate.
The seven proposed steps are necessary to challenge their limiting beliefs, to connect
to their breath, to feel their body and let go of their mind, which blocks the access to
intuition.
Their new discovered intuition capacities enable them to take decision faster in
projects and hence are more eﬃcient and productive. As a matter of fact since 3 years
the engineer students in the option innovation have all been inventors of a license thanks
their collective project. This may be due to the innovate learning described here above
in this article.
The main motivations of the students are based on passion, envy to create or their
need of recognition. All the department’s teachings “Quality, Innovation Reliability”
How to Increase Intuition for Entrepreneurship Spirit?
101

contribute to such positive results. The question is now how to sustain the ﬁnancing of
the patent deposit…
5
Conclusion
The integration of music, the zapping method and the introducing of videos help to
capture the students’ attention. We do observe they rapidly shut their laptops, switched
oﬀ their mobile phone to listen to the course.
The proposed method allows to refocus the attention of students by listening to
music, to increase the quality of attention and presence by zapping through videos and
debrieﬁng and thus to get a good quality of student learning. These students are actually
learners rather than extras in a classroom.
The “Five I” make possible to explore inspiration, imagination, the diﬀerent intel‐
ligences, recklessness, and at last intuition. Music and the rhythm of drums make the
students’ mental leash.
Students understand that innovation, creativity and intuition are multidimensional,
linked together and dependent on their ability to let go the mental and to balance their
mental, emotional and instinctive center.
They allow themselves to listen to the ﬁnest signals of their body to reach the
“Vacuum auricular System” and to feel then their intuition. This workshop “Innovation,
Creativity, Intuition” was experimented with engineer students in 5th year and in Master
2 of the ISTIA in Angers, students of “Arts et Métier Paristech” in Paris and in Angers
too, students of the University of Cairo, students of the VUB University in Brussels, and
also with students of the Business School of Hanoi in Vietnam. We are trying to train
teachers in order to transmit those new teaching methods in France and in other countries.
References
Blood, A.J., Zatorre, R.J.: Intensely pleasure responses to music correlate with activity in brain
regions implicated in reward and emotion. Proc. Natl. Acad. Sci. 98(20), 11818–11823 (2001)
Blood, A.J., Zatorre, R.J., Bermudez, P., Evans, A.C.: Emotional responses to pleasant and
unplesant music correlate with activity in paralimbic regions. Nat. Neurosci. 2, 382–387 (1999)
Campbell, J.B., Hawley, C.W.: Study habits and Eysenck’s theory of extraversion - introversion.
J. Res. Pers. 16, 139–146 (1982)
Corsi, P., et al. (eds.): Innovation Engineering: The Power of Intangible Networks. Hermes
Sciences Publishing ISTE, London (2006)
Daoussis, L., Mckelvie, S.J.: Musical preferences and eﬀects of music on a reading comprehension
test for extraverts and introverts. Percept. Motor Skills 62, 283–289 (1986)
Doyle, M., Furnham, A.: The distracting eﬀects of music on the cognitive test performance of
creative and non-creative individuals. Think. Skills Creat. 7, 1–7 (2012)
Garner, H.: Intelligence Reframed: Multiple Intelligence for the 21st Century. Basic Books, New
York (1999). 292 pages
Goleman, D.: Emotional Intelligence. Bantam, New York (1995)
Hogarth, R.M.: Educating Intuition. The University of Chicago Press, Chicago (2001)
Jenny, H.: Cymatics: The Structure and Dynamics of Waves and Vibrations, vol. 1 (1967)
Jenny, H.: The Structure and Dynamics of Waves and Vibrations, vol. 2 (1974)
102
H. Samier et al.

Krause, A., North, A.C., Hewit, L.: Music sélection behaviors in everyday listening. J. Broadcast.
Electron. Media 58(2), 306–323 (2014)
Koelsch, S., Fritz, T., Schulze, K., Alsop, D., Schlaug, G.: Adults and children processing music:
an fMRI study. NeuroImage 25, 1068–1076 (2005)
Koelsch, S., Fritz, T., Von Cramon, D.Y., Müller, K., Friederici, A.D.: Investigating emotion with
music: an f MRI study. Hum. Brain Mapp. 27, 239–250 (2006)
Laloux, F.: Reinventing Organizations. Nelson Parker, Brussels (2014). 360 pages
Lesuik, T.: The eﬀect of music listening on work performance. Psychol. Music 33(2), 173–191
(2005)
Nogier, P., Nogier, R.: The Man in the Ear. Maisonneuve, Sainte Ruﬃne (1985)
Nogier, P.: Handbook to Auriculotherapy, 2nd edn. Satas, Brussells (1998)
North, A.C., Hargreaves, D.J.: Is music important? The Psychol. 16(8), 406–410 (2003)
North, A.C., Hargreaves, D.J.: The power of music. The Psychol. 22(2), 1012–1014 (2009)
Olds, J., Milner, P.: Positive reinforcement produced by electrical stimulation of septal area and
other regions of rat brain. J. Comput. Physiol. Psychol. 47(6), 419–427 (1954)
Robinson, K., et al.: Our Futures: Creativity, Culture & Education. National Advisory Committee
on Creative and Cultural Education, London (1999)
Salimpoor, V.N., Benovoy, M., Larcher, K., Dagher, A., Zatorre, R.J.: Anatomically distinct
dopamine release during anticipation and experience of peak emotion to music. Nat. Neurosci.
14(2), 257–262 (2011)
Samier, H.: The development of intuition & perceptive insight in the process of Competitive
Intelligence and Knowledge Management. In: ICI Conference, ECIS European Competitive
Intelligence Symposium, Germany (2014)
Scharmer, O.: Theory U: Leading from the Future as it Emerges. Berrett-Koehler Publisher, San
Francisco (2008)
Schellenberg, E.G.: Music and cognitive abilities. Am. Psychol. Soc. 14(6), 317–320 (2005)
Schulz, M.D., Lisa, M.: Awakening Intuition, Using your Mind-Body Network for Insight and
Healing. Three Rivers Press, New York (1998)
Shirley, D., Langan-Fox, J.: Intuition: a review of the literature. Psychol. Rep. 79(2), 563–584
(1996)
Williamson, M.: A Return to Love: Reﬂections on the Principles of a Course in Miracles, vol. 7,
no. 3, pp. 190–191. Harper Collins, New York (1992)
How to Increase Intuition for Entrepreneurship Spirit?
103

Tic Enabler

Development of Prepaid Electricity Payment System
for a University Community Using the LUHN Algorithm
Oluranti Jonathan, Ambrose Azeta, and Sanjay Misra
(✉)
Department of Computer and Information Sciences, Covenant University,
Ota, Ogun State, Nigeria
{jonathan.oluranti,ambrose.azeta,
sanjay.misra}@covenantuniversity.edu.ng
Abstract. This work presents a University Community based electricity prepaid
billing system. Generally in Nigeria, electricity customers face a lot of problems
with respect to their electricity bills from the distribution companies. The chal‐
lenges they face include wrongly calculated bills as a result inaccurate reading of
meters, general human errors in bill preparation among others. In some other
semi-automated systems in which prepaid meters are used, consumers waste
much time in purchasing utility units for electricity. This is the case presently at
the university community we are considered in this work. This paper presents the
design and implementation of a combination of a web-based and SMS alert
prepaid electricity system called for the community. The implementation of the
system was done using C# programming language and Microsoft SQL Server as
the database platform. The system incorporates the Luhn algorithm for generating
pins for use on the simulated prepaid meters. The system is able to run on the
university intranet and can also serve as internet based application.
Keywords: Prepaid electricity · Luhn algorithm · Payment system · Pin
1
Introduction
Prepayment utility meters systems medium through which customers purchase utility
units that enable them use services like electricity, water, waste, cable among others, in
advance [1]. The introduction of prepaid meters in the early part of last decade was
received with great expectation that billing issues would be over. It was a feeling of
freedom at last from the erstwhile electromechanical meters used for billing at that time.
The use of prepaid meters for billing electricity consumption was to eliminate most of
the challenges hitherto faced with use of the old meters. Such challenges include wrongly
calculated bills as a result inaccurate reading of meters; general human errors in bill
preparation; payment of bills through wrong agents among several others. The prepaid
meters on the other hand brought about a number of beneﬁts to both users and system
that include [1–3] elimination of monthly bills or collection hassles; reduced operational
costs; elimination of disconnects and reconnects; immediate collection of revenue;
among others. The prepaid meters work by measuring actual electrical use and removing
units in real-time [2].
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 107–114, 2018.
https://doi.org/10.1007/978-3-319-72965-7_9

Although these beneﬁts exist, there is the need to take advantage of the advances in
technology to further improve on the beneﬁts. For instance in the Covenant University
community which we considered for this work, the processes leading to the ﬁnal
purchase of electricity units appear to be long and inconvenient to most consumers.
Consumers in need of electricity units usually have to ﬁrst pay the amount required at
the bank, convert the duplicate teller to a receipt before going to the point of purchase.
The question to ask is whether it is possible to evolve a system that eliminates all these
unnecessary procedures that waste time. It is in response to this, that a simple prepaid
system was proposed as a solution to the issue.
There have been several eﬀorts in Nigeria and other parts of the world to build similar
systems for various reasons and needs of people. In 2006, [4] designed and implemented
a SMS-based control for monitoring systems. The SMS component was used for status
reporting such as power failure. However, the work did not consider issues that concern
billing for electricity usage.
The work of [5] proposed a method for using telecommunication systems to automate
transmission of data to facilitate bill generation at the server end and also to the customer
via SMS, Email. [6] also developed a card reader-based prepaid electricity metering
system but for a multiphase system. The work of [7] focused on the controlling of home
appliances remotely and providing security when the user is away from the place using
an SMS- based wireless Home Appliance Control.
[8] developed an energy eﬃcient and low cost solution for street lighting system
using Global System for Mobile communication [GSM] and General Packet Radio
Service [GPRS]. The whole set-up provides the remote operator to turn oﬀ the lights
when not required, regulate the voltage supplied to the streetlights and prepare daily
reports on glowing hours.
[9] proposed in their paper, a prepaid energy meter behaving like a prepaid mobile
phone. The meter contains a prepaid card analogous to mobile SIM card. The prepaid
card communicates with the power utility using mobile communication infrastructure.
Once the prepaid card is out of balance, the consumer load is disconnected from the
utility supply by the contactor. The power utility can recharge the prepaid card remotely
through mobile communication based on customer requests. The work of [12] focused
on the design and implementation of an intelligent sms-based remote metering system.
[10, 11] are both online and web-based platforms respectively for bill payments but not
for prepaid purposes.
In this paper therefore, we present the design and implementation of a web-based
electricity prepaid system for a university community. The remaining sections of this
paper are divided into review of related work, system requirements, system design and
modeling, system implementation and conclusion.
1.1
Luhn Algorithm
The LUHN formula was created by IBM scientist Hans Peter Luhn and described in
U.S. Patent No. 2,950,048, ﬁled on January 6, 1954, and granted on August 23, 1960
[10]. Because the algorithm is in the public domain, it can be used by anyone. We used
an algorithm based on the luhn algorithm for the generation and veriﬁcation of the pin
108
O. Jonathan et al.

created by the system. The LUHN formula is widely used to generate the check digits
of many diﬀerent primary account numbers. Almost all institutions that create and
require unique account or identiﬁcation numbers use the Mod 10 algorithm. The algo‐
rithm is in the public domain and is in wide use today. It is speciﬁed in ISO/IEC 7812-1.
It is not intended to be a cryptographically secure hash function; it was designed to
protect against accidental errors, not malicious attacks. Most credit cards and many
government identiﬁcation numbers use the algorithm as a simple method of distin‐
guishing valid numbers from mistyped or otherwise incorrect numbers.
Based on ANSI X4.13, the LUHN formula (also known as the modulus 10 – or mod 10
– algorithm) is used to generate and/or validate and verify the accuracy of credit card
numbers.
Most credit cards contain a check digit, which is the digit at the end of the credit card
number. The first part of the credit-card number identifies the type of credit card (Visa,
MasterCard, American Express, etc.), and the middle digits identify the bank and customer.
To generate the check digit, the LUHN formula is applied to the number. To validate
the credit-card number, the check digit is ﬁgured into the formula.
Here’s how the algorithm works for verifying credit cards; the math is quite simple:
i. Starting with the second to last digit and moving left, double the value of all the
alternating digits.
ii. Starting from the left, take all the unaﬀected digits and add them to the results of
all the individual digits from step 1. If the results from any of the numbers from
step 1 are double digits, make sure to add the two numbers ﬁrst (i.e. 18 would yield
1 + 8). Basically, your equation will look like a regular addition problem that adds
every single digit.
iii. The total from step 2 must end in zero for the credit-card number to be valid.
2
System Requirements
A requirement states what a product or service is intended to perform. It takes into
consideration the attributes and characteristics that a system is expected to possess so
as to meet the need of a user.
i. Users and Administrators must login to access the services.
ii. Users must be able to provide required information when required to.
iii. The system should provide a notiﬁcation to customers showing success or failure
of their transaction which could serve as a receipt.
iv. The system should be able send an email containing the pin to a user’s email
address.
v. The system should be able to display available amount of electricity energy credit
units for purchase and the price.
vi. The system should be able to provide a payment method.
vii. The system should keep transaction history.
viii. The system should be able to generate electricity energy credit units pins for STS
Meters.
Development of Prepaid Electricity Payment System
109

ix. The system should be able to verify legit pin and give notiﬁcation of wrong or
already used pin.
3
System Design and Modeling
System design is the process of deﬁning the components, interfaces, modules, data and
architecture for a system to satisfy the speciﬁed requirements. This could also be seen
as the applications of systems theory to product development. Modeling helps the system
analyst understand the functionalities of the system. It is also used to validate the soft‐
ware requirement by examining from a diﬀerent point of view. Modeling is used to
design a software application before coding begins. The Uniﬁed Modeling Language
(UML) is used in this paper to describe the system from various perspectives.
3.1
Activity Diagram
The activity diagram depicts the workﬂow of activities within the system. It graphically
represents the ﬂow of performance of various actions by the system entities. The ﬂow
of the system activities for the administrator is as shown in Fig. 1 below.
Admin Login
Select user 
management
Edit rate or Price of 
Units
View transaction 
history
Is login 
valid?
yes
no
Fig. 1. Activity diagram for administrator
110
O. Jonathan et al.

3.2
Sequence Diagram
This is an interaction diagram that shows how processes operate with one another and
what is their order. It shows object interactions arranged in time sequence and depicts
the objects and classes involved in the scenario and the sequence of messages exchanged
between the objects needed to carry out the functionality of the scenario. In Fig. 2 below,
the login details of administrator are validated before access is granted to the application.
Fig. 2. Sequence diagram for administrator’s activities
4
System Implementation and Comparison
SQL Server Management Studio (SSMS) was used as the relational database due to its
ﬂexibility easy to use GUI and the easy integration it allows for with Visual Studio as
Development of Prepaid Electricity Payment System
111

they are both products of Microsoft. Bootstrap was used to style the web interface of
the system to ensure easy and user friendly GUI. Visual studio 2012 was used as the
Integrated Development Environment (IDE) and ASP.Net and C# as backend for design
of this project. Visual studio was used because of its adaptability. The implementation
consists of three major modules which are user interface, energy meter interface and
administrator interface. Figures 3 and 4 below are sample snapshots of the system
implementation.
Figure 3 below represents the user-friendly login screen. To gain access into the
system, a user is required to supply a valid email and password for authentication. The
use of email for login purpose was adopted because it easy for users to remember emails.
Emails are also unique worldwide and this eliminates issues of collisions in usernames.
The use of email also makes it easy to contact a user in case of emergency. Figure 4
shows a pin (4012331580375712) that was generated using the Luhn algorithm and
loaded on the application. The new balance (00781.11) also shows on the virtual meter.
Fig. 3. General login screen
The system developed has improved features when compared with other existing
systems that perform similar functions. Some of the features include ease of use, better
performance, and friendly user interface, among others. The testing methods applied to
the application included unit and integration testing in line with software development
standards. Unit testing was used to test individual modules in order to locate local errors.
With unit testing, a system developer is able to detect errors in coding and logic in each
module. Integration testing on the other hand was carried out to discover errors associ‐
ated within the interface.
112
O. Jonathan et al.

Fig. 4. Adding of units purchased after successful validation of the pin
5
Conclusion
The covenant university community electricity prepayment system was conceptualized
following the consideration of the procedures involved in the former system. With this
new platform, members of the community can from anywhere initiate and purchase
electricity utility units. The developed system followed a simple software development
approach of system analysis, design and implementation. The application is web-based
and runs both on the intranet and internet. The system was properly tested for accuracy
and performance-related functionalities. To further improved on the system, in the future
we intend to extend the application to a mobile app in order to cater for convenience
and usability attributes of the present one.
References
1. Bruce, W.: Prepayment utility meter systems: a case study from Aleut region (2006). https://
www1.eere.energy.gov/tribalenergy/guide/pdfs/cs_aleutian_pribilof.pdf
2. Omijeh, B.O., Ighalo, G.I.: Design of a robust prepaid energy metering and billing system.
J. Res. Natl. Dev. 10(3), 146–153 (2012)
3. Directinsite: Advantages of electronic payment system. www.directinsite.com (2016). http://
www.directinsite.com/The-Advantages-of-an-Electronic-Payment-System#sthash.
3cNiWrYP.dpuf
4. Stanescu, D., Ciubotaru-Petrescu, B., Chiciudean, D., Cioarga, R.: Wireless solutions for
telemetry in civil equipment and infrastructure monitoring. In: 3rd Romanian-Hungarian Joint
Symposium on Applied Computational Intelligence (SACI) (2006). http://www.bmf.hu/
conferences/saci2006/Ciubotaru.pdf
5. Sharma, S., Shoeb, S.: Design and implementation of wireless automatic meter reading
system. Int. J. Eng. Sci. Technol. (IJEST) 3(3), 2329–2334 (2011)
Development of Prepaid Electricity Payment System
113

6. Ling, Z., Sihong, C., Biao, G.: The design of prepaid polypase electricity meter system. In:
IEEE Conference on Intelligent Computing and Integrated Systems (ICISS) (2010)
7. Malik, S.H., Aihab, K., Erum, S.: SMS-based wireless home appliance control system
(HACS) for automating appliances and security. Issue Informing Sci. IT 6, 887–894 (2009)
8. Maheswari, C., Jejanthi, R.: Implementation of energy management structure for street
lighting system. J. Mod. Appl. Sci. 5, 6–10 (2009)
9. Amit, J., Mohnish, B.: A prepaid meter using mobile communication. Int. J. Eng. Sci. Technol.
3(3), 160–166 (2011)
10. Luhn, H.P.: US Patent 2,950,048 - computer for verifying numbers, 23 August 1960. Luhn
algorithm. http://www.google.com/patents/US2950048. Accessed 1 May 2016
11. 1000 Projects: Web based bill payment utility CSE project. 1000projects.org (2016). http://
1000projects.org/web-based-bill-payment-utility-cse-project.html
12. Wasi-ur-Rahman, M., Tanvir, A., Lutful K.: Design-of-an-intelligent-SMS-based-remote-
metering-system. In: International Conference on Information and Automation, ICIA 2009,
pp. 1040–1043 (2009)
114
O. Jonathan et al.

Leveraging GPS and SMS-Based Bus Tracking
Architecture for an Eﬃcient Transportation
Bassirou Kass´e, Moussa Diallo(B), and Bamba Gueye
Universit´e Cheikh Anta Diop de Dakar, Dakar, Senegal
moussa.diallo@ucad.edu.sn
Abstract. Nowadays, we note a huge population growth in Dakar
Region due mainly to the drift from the land. Therefore, the suburbs keep
growing and daily workers need to be transported since economic activi-
ties and most urban infrastructures are located in Dakar-downtown. The
lack of punctuality and the amount of time wasted at a bus stop are big
concerns for the national society of transportation “Dakar Dem Dikk”
(DDD). We aim to enhance the communication system by allowing the
bus passengers to know either how many bus stops it remains for a given
bus to reach a ﬁxed bus stop; or the estimated distance between a target
bus and its position. The proposed architecture is based on GPS sensors
and mobile networks that relay bus position towards bus passengers that
request the information. Relying on two heuristics, Landmark-Based and
Polygon-Based approach, the obtained results show that 70% of targets
are localized with an error distance less than 100 m.
Keywords: Geolocation · Tracking · Sensors networks
Internet of thing
1
Introduction
During last decades we note a huge population growth in Dakar Region due
mainly to the fact that economic activities and urban infrastructures are located
in Dakar-downtown. Dakar Region, which is a peninsula, is the smallest and most
populated region in Senegal. The population is estimated roughly to 4 millions
of inhabitants for an area of 550 km2.
Since daily workers need to be transported in Dakar-downtown, Dakar roads
are in chaos during workday. This chaos is accelerated since everyday the number
of new private vehicles and the clandestine exploitation of private vehicles as
means of public transportation increase. Consequently, diurnal traﬃc jam occurs
and during rush hour, the wasted time can be up to 4 h from Dakar-downtown
to suburbs. The big concern with the increased number of cars is air pollution.
Indeed, the car pollution is high since most of vehicles are bought in second hand
from European countries.
The national society bus transportation “Dakar Dem Dikk” (DDD) has no
Information System and it operates 17 bus lines that cover 352.4 km within
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 115–124, 2018.
https://doi.org/10.1007/978-3-319-72965-7_10

116
B. Kass´e et al.
Dakar Region. Bus Passengers suﬀer with the lack of punctuality which is
increased according to traﬃc jam. Therefore, the time wasted while waiting for
the bus is a big concern for potential bus passengers. Potential bus passengers
prefer to use private bus transportation even if it is more expensive. The number
of customers is reduced as well as the company’s revenues.
To overcome these limitations, intelligent transport system should be used by
the DDD society [1–3]. We propose an information system, called DDDT (Dakar
Dem Dikk Tracking) that can predict buses arrival in order to help bus passengers
in their choice when they travel. The DDDT architecture system is based on a
GPS, Short Message Service (SMS), and Web Services technology. From a SMS
sent by a target bus passenger and having as inputs the bus number line and
its direction, the information system is able to provide either the remaining bus
stops from a ﬁxed bus stop, or the distance between the target and the bus.
The remainder of the paper is structured as follows. Section 2 depicts the
diﬀerent components of the DDDT architecture. In Sect. 3, we design the
Landmark-Based and Polygon-Based bus geolocation technique and the DDDT
methodology to localize target hosts. Following that, we present in Sect. 4 our
experimental results. Finally, Sect. 5 concludes this work.
2
Components of the Bus Tracking Architecture System
2.1
Data Acquisition Module
The core embedded data acquisition (DAQ) system is based on an Arduino Uno
board (AUB). Figure 1 illustrates the set of components which is connected to
the AUB. The used Global Positioning System (GPS) hosts a high performance
receiver and a low power consumption Arduino GPS shield V.1.1 of ITead Studio
with an internal GPS Antenna. It can track up to 20 satellites at a same time
and performs fast Time-To-First-Fix (TTFF) during weak signal environments1.
The DAQ system collects through the GPS shield the latitude, longitude, veloc-
ity, altitude and bus direction. In order to send the collected information to a
SMS gateway, a GSM shield2 with a compact right angle antenna type is also
connected to the AUB.
Furthermore, due to unpredictable road conditions, it is not suitable to send
SMS periodically [2,3]. For instance, in situation of traﬃc jam, it is not neces-
sary to send SMS continuously since the bus is not in motion. Therefore, it is
mandatory to deal with congestion on the transport network. To overcome the
limitations of previous works, we propose to send SMS even if the bus is moving
in respect of a given distance (Fig. 2). In order to mitigate the number of trans-
mitted SMS, every τ seconds the AUB collects the geographic location of the
target bus. Afterwards, the AUB computes and adds up the distances travelled
along the road. However, a SMS with the last collected information will be sent
1 http://store.iteadstudio.com/images/produce/Shield/Shields/gpsshield/
ArduinoGPSshield DS.pdf.
2 http://www.embeddedartists.com/products/acc/cell 2g shield.php.

GPS and SMS-Based Bus Tracking
117
Fig. 1. Data acquisition components.
only if the cumulative distance D exceeds a threshold distance Dl. The distance
Dl is estimated with respect to our micro-segmentation heuristic which splits
the paths into several short and linear micro-segments. The values of τ and Dl
depend on several factors such as distance between bus stops, roads condition,
and rush hour. According to the illustrated algorithm in Fig. 2, we are able to
mitigate the numbers of transmitted SMS.
2.2
Bus Tracking Architecture System
Figure 3 represents the bus tracking architecture system where the SMS sent
by the embedded system are analyzed by the Kannel server which is used as
SMS gateway Server. Afterwards, we consider web services in order to fetch the
latitude, longitude, velocity, time and date from retrieved messages and store
these data in a MySQL data base.
The communication between target bus passengers and server is also based
on SMS services. When a target bus passenger needs to use our service, he
will just send a SMS to a Kannel Server. In order to estimate the bus arrival,
the server needs to receive the target bus stop name or number, and the bus
direction. It is worth noticing that the geographic position of bus stops along
the used route is well-known. Furthermore, a mapping between each bus stop and
its corresponding geographic position is done. At each received SMS, a tracking
process is done in order to retrieve bus position and its direction. Next, the bus
tracking system will respond by sending back a SMS to the target host. It should

118
B. Kass´e et al.
Fig. 2. SMS transmission process.
Fig. 3. Bus tracking architecture system.
be noted that the SMS ﬂows back the passengers through the SMS gateway. The
SMS contains the number of bus stops that will be crossed with respect to your
actual bus stop as well as the geographic distance between your position and the
targeted bus.

GPS and SMS-Based Bus Tracking
119
3
Bus Geolocation Approaches
3.1
Overview on Distance Estimation
The shortest distance over the surface of the earth between two GPS trace points
A and B is given by the Haversine formula [2,4,5]:
DAB = 2Rbtan−1(
√
b,
√
1 −b)
(1)
where b = sin2( latA −latB
2
) + cos(latA) × cos(latB) × sin2( lonA −lonB
2
), R is the
radius of the earth and latA, lonA, latB, lonB are respectively the latitude and
longitude of the GPS trace points A and B.
However the distance travelled by a bus between these two points along the
road is often greater than that the distance given by Eq. (1). These two distances
are equal only if the path that relies these two points is linear or very short. By
proposing a micro segmenting approach Aradhya et al. [4] try to overcome this
limitation. The goal of micro-segmentation, which splits the path into several
short and linear micro-segments, is to increase the accuracy of the distance
estimation. By so doing, the add up micro-segments is then closer to the actual
distance travelled by the bus. For instance, Fig. 4 shows the path between to bus
stops crossed by the line bus number 10 of DDD transportation. On can easily
see that the micro segmenting approach is more suitable.
In this paper, we design and evaluate two geolocalization techniques based
on micro-segmenting approach. Therefore, the travelled path along the road bus
Fig. 4. Geographical distance estimation.

120
B. Kass´e et al.
Fig. 5. Locating a host with landmark-based geolocation approach.
is split into micro-segments (l) having equal distance. Each micro-segment has
an unique identiﬁer called ID. Our proposed heuristics aim to accurately match
each received GPS trace point into a micro-segment along the road.
3.2
Landmark-Based Geolocation Approach
Figure 5 illustrates the Landmark-Based geolocation (LB) process. The LB
heuristic localize the bus position by performing a correlation between each
received GPS trace point and the set of landmarks. It should be noted that a
landmark is a well known geographic position point (latitude, longitude) along
a given bus line. Afterwards, the closest landmark with respect to the received
bus position is considered as the bus estimation position.
Indeed, at each received GPS trace point, the LB heuristic seeks a conﬁdence
region. The area of the conﬁdence region is given by a square surface as follows;
lat −δlat ≤lat1 ≤lat + δlat
lon −δlon ≤lon1 ≤lon + δlon
(2)
where (lat, lon) are the latitude and longitude of the received GPS trace point
and (δlat, δlon) are used to ﬁnd the boundary of our conﬁdence region. The
center of the square is the received GPS trace point and the values of (δlat, δlon)
can be tuned in order to obtain Δ1 as shown in Fig. 5. Next, the number of
landmarks within the square surface is determined and will be used as testbed.
By considering the conﬁdence region, we reduce the computation time compared
to previous work that veriﬁes the distance between the whole set of landmarks [4].
Thereby, the micro-segment ID is known if the number of landmarks is equal
to 1. Otherwise, the same process is repeated by considering Δi < Δ1 until the

GPS and SMS-Based Bus Tracking
121
micro-segment ID is not found. As soon as the micro-segment ID is known, our
bus tracking tool can determine either the bus geographic bus position, or the
remaining bus stops with respect to a ﬁxed bus stop, or the distance between
the bus stop and a target bus passenger.
3.3
Polygon-Based Geolocation Approach
Figure 6 depicts the diﬀerent steps of locating a host with a Polygon-Based (PB)
heuristic. Firstly, the PB approach selects the nearest set of landmarks according
to the received GPS trace point. Afterwards, the selected set of landmarks will
form the vertices of a polygon which area represents our conﬁdence region. Next,
the centroid of the polygon is determined and the nearest landmark with respect
to the centroid of the polygon is chosen as the bus estimation position. The
same process Like LB approach is considered in order to determine the closest
landmark with respect to the centroid of the polygon.
Fig. 6. Locating a host with polygon-based geolocation approach.
During weak signal environments, when the received GPS trace points is far
away according to the bus road, as illustrated in Fig. 6, by considering the cen-
troid of the polygon, we are able to shift the potential wrong location towards
the landmarks. However, with respect to the number of turnings or curvatures
along the bus road, as shown in Fig. 7, an error may happen on distance estima-
tion. Nevertheless, the PB approach will be more suitable in case of weak signal
environments.
4
DDD Tracking Evaluation
4.1
Experimental Settings
To analyze and compare the performance of the two proposed detection methods,
simulations are carried out the line number 10 of DDD transportation over a
distance of 15.5 km. We collected the GPS trace points of all the bus stops
which are 28 in each direction. Also, we realized the micro-segmentation of the
15.5 km with two three sampling distance equals to 65 m, 130 m, and 195 m.

122
B. Kass´e et al.
Fig. 7. Potential mismatch with the polygon-based geolocation approach.
For instance, a sampling distance of 65 m means that the distance between two
landmarks is equal to 65 m and 239 landmarks are considered over the bus road.
Each micro-segment has a unique ID which depends on its position along the
bus road. All these information are stored in our MySQL data base. In order to
localize a target host, the following steps are performed: “i” the DAQ module
which is conﬁgured to send by SMS the geographic location of the targeted bus
is embedded in the bus. The geographical position of the bus is sent only if a
distance equals to ﬁxed threshold Dl = 100 m is travelled by the bus; “ii” at each
received SMS, the DDDT process is run in order to retrieve the bus position.
Finally, based on well known geographical position of 130 target bus passengers,
we evaluate the estimation provided by the DDDT tools with respect to the
actual position of the target.
4.2
Results
Figures 8, 9 and 10 compare the CDF (Cumulative Distribution Function) of
the location error estimation of Landmark-Based (LB) and Polygon-Based (PB)
approach with respect to several sampling distances and Δ conﬁdence region val-
ues. The purpose of these ﬁgures is to study how LB and PB approach estimate
Fig. 8. Location estimation error according to a conﬁdence region Δ = 100 m.

GPS and SMS-Based Bus Tracking
123
Fig. 9. Location estimation error according to a conﬁdence region Δ = 150 m.
Fig. 10. Location estimation error according to a conﬁdence region Δ = 200 m.
the position of target bus passengers with respect to a ﬁxed bus station stop.
The location error estimation for a given bus passenger is the diﬀerence between
its actual geographic location and its estimated location. On the x-axis, we have
the error distance location for diﬀerent target bus passengers with respect to
their actual position. On the y-axis, we show the probability that the location
estimation for the target bus passengers have an error less than x.
We note that the sampling distance has en eﬀect on the accuracy of the loca-
tion estimation. Concerning LB approach, the obtained results show that the
conﬁdence region Δ has less impact on the accuracy compared to the sampling
distance. We observe in Figs. 8(a), 9(a) and 10(a) that the sampling distance
equals to 65 m is a stricter upper bound on error location than other sampling
distances. The reason is because to the fact that when the sampling distance
increases, the potential landmark points that can be considered as location esti-
mation for a targeted bus passenger is reduced. Consequently, the computation
time is reduced but the error location can be increased. The LB approach esti-
mates, 70% of the target bus passengers with a error distance less than 100 m.
Furthermore, when the conﬁdence region Δ ranges from 100 m to 200 m, with

124
B. Kass´e et al.
the a sampling distance equals to 65 m, LB approach localizes 97% of the target
bus passengers with a error distance less than 200 m.
The performance gap between sampling distance is larger in the LB app-
roach. With the PB approach (Figs. 8(b), 9(b) and 10(b)), the sampling distance
which is equal to 130 m outperforms other sampling distances. Nevertheless, the
obtained curves with respect to a sampling distance equal to 65 m illustrate the
same trend for δ values equal to 100 m and 150 m (Figs. 8(b) and 9(b)). According
to a sampling distance equals to 130 m (resp. 65 m), the LB approach estimates
90% (resp. 88%) of the target bus passengers with a error distance less than
200 m; whereas with both sampling distances 62% are localized with an error
distance less than 100 m.
The obtained results exhibit that the LB approach outperforms the PB app-
roach. The fact is due how LB heuristic maps the target bus passenger into the
closest landmark; whereas for the PB heuristic it is the centroid of the polygon
that is mapped into the closest landmark. Notwithstanding, the PB approach
has the advantage of providing a conservative upper bound on the error distance.
In fact, in situation where the target hosts tend to be far away from the land-
mark points, the centroid of the polygon will be able to shift the target towards
the baseline which is formed by the set of landmarks.
5
Conclusion
Since economic activities and most urban infrastructures are located in Dakar-
downtown, the DDD society should provide an eﬃcient transportation system.
Relying on private transportation in order to overcome the lack of punctual-
ity of buses is not a good deal and then increase the air pollution in Dakar
Region. Nevertheless, the provided DDDT architecture is able to provide either
the remaining bus stops from a ﬁxed bus stop, or the distance between the tar-
get and the bus. Two geolocation heuristics were designed and the LB approach
localizes 70% of targets with an error distance less than 100 m. We are investi-
gating to predict the waiting time at bus stop as well to develop applications
which are based on the web-mapping for users with a 3G or Internet connection.
References
1. Zhang, K., Yu, F.: Intelligent supervision system of passenger transport based on
IOT. In: ICLEM 2014, pp. 1058–1064 (2014)
2. Singla, L., Bhatia, P.: GPS based bus tracking system. In: Computer, Communica-
tion and Control (IC4), Indore, pp. 1–6 (2015)
3. Maruthi, R., Jayakumari, C.: SMS based bus tracking system using open source
technologies. Int. J. Comput. Appl. 86(9), 44–46 (2014). ISSN 0975–8887
4. Biswas, A., Pilla, G., Tamma, B.R.: Microsegmenting: an approach for precise dis-
tance calculation for GPS based its applications. In: 2013 IEEE Recent Advances
in Intelligent Computational Systems (RAICS), Trivandrum, pp. 327–332 (2013)
5. Haversine Formula and Destination point given distance and bearing from start
point. http://www.movable-type.co.uk/scripts/latlong.html

Quantify the Maturity of Internet Banking Security
Measures in WAEMU (West African Economic
and Monetary Union) Banks
Marie Ndaw1(✉), Gervais Mendy2, Samuel Ouya2, and Diaraf Seck2
1 GIM-UEMOA, Dakar, Senegal
lisandaw@gmail.com
2 UCAD, ESP, LIRT, Dakar, Senegal
{gervais.mendy,samuel.ouya,diaraf.seck}@ucad.edu.sn
Abstract. WAEMU banks should provide more facilities and convenience to
theirs customers but they also should take all steps and expensive measures to
make online transactions more safe and secure. Several type of controls are
proposed to manage diﬀerent risks related to inﬁltrations, breaching, stealing data
and cyber-attacks. Controls maturity are measured using the actual method which
is manual, require time and personal investment and have some estimation error.
In this paper, we propose and apply two models on all internet banking controls
in order to automatize the quantiﬁcation of their maturity. The both models allow
optimal assessment of security measures and aim to improve security of Ebanking
transactions including increase of comfort and time saving for underserved areas.
The results of our study also enable economic/social well-being and ﬁnancial
inclusion to people in rural areas who use internet connection for mobile banking
transactions.
Keywords: Internet banking · Risk · Security · Control
1
Introduction
Online banking is the use of the internet to deliver banking services such as funds
transfer, paying bills, viewing current and savings account balance, purchasing ﬁnancial
instruments and certiﬁcates of deposits. The online services are easy to use, permit
banking anytime, anywhere and anyhow and might diﬀer from bank to bank. This facil‐
itates all the functions and provide many advantages as compared to WAEMU traditional
banking services but insecure channel such as the internet might not be the best base for
bank customer relations as trust might partially be lost.
2
Literature Review
Several work are focused on Ebanking risk management. Sullivan in [1] showed that
available risk measures may not capture the types of risk banks are exposed to with
internet banking, such as security or operational risks. Abdou et al. in [2] concluded that
the banks are working hard to mitigate the various risks. However, they cannot afford to
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 125–130, 2018.
https://doi.org/10.1007/978-3-319-72965-7_11

become complacent when considering the adverse impact of increasing customer
complaints, financial data, news headlines, corporate governance issues and credit crunch
fall out which are all potentially critical of their risk control measures. Drimer et al. [3]
showed that Bank website includes a prefix which customizes the user prompts and
reduces the risk of social engineering because the field descriptions are more specific.
Diﬀerent types of attack exist and should be addressed by banks in order to improve
the online transaction security. Weest in [4] classiﬁed E-banking attacks into three cate‐
gories: Local attacks, Remote Attacks and Fraud Prevention Technologies. Otsuka in
[5] related the Federal Financial Institutions Examination Council issued updated
authentication guidance which suggest to reinforces and stresses the importance of
performing periodic risk assessment and implement layered security controls at various
points in the transaction process.
Banks shoul implement appropriate controls by taking into account diﬀerent types
of Ebanking risks; Ting et al. research [6] disclosed new frameworks which state that
the ﬁnancial risk, time risk, security risk, performance risk and social risk have a signif‐
icant relationship towards consumers behavioral intention to use online banking. Zarei
in [7] showed that Banks must be conscious of diﬀerent types of risks related to
Ebanking: Cross border risks, Security risks, Legal and Ethical risks, Operational risks,
Reputational Risks, Strategic risks, Money laundering risks and Traditional risks. Li
et al. in [8] related that online auction sites often claim that they have no control over
the quality, safety or legality of the items advertised, the truth or accuracy of the listings,
thus exposing bidders to potential risks and fraudulent transactions. Malhotra and Singh
in [9] showed that Internet banking has a negative and signiﬁcant impact on risk which
shows that its adoption has increased the risk proﬁle of banks. So its necessary to apply
eﬃcacy security measures at diﬀerent level. Hole et al. in [10] concluded that the popu‐
larity of online Banking has attracted the internet criminals to attack online Banking
customers. Sarma and Singh in [11] related that biometric technology has played an
important role to control the risk factors through Authentication system. The imple‐
mentation of appropriate authentication methodologies should start with an assessment
of the risks faced by the Internet banking systems. Security measures should also concern
customers. In [12], severals guidelines for safe online banking are proposed. Osun‐
muyiwa in [13] proposed others guidelines like never click on links or applications that
you receive in emails or text messages and avoid using unsecured public wireless
connections.
3
Problem
We have noted that WAEMU banks use diﬀerent types of measures to secure online
transactions and periodically assess their risks by calculating residual criticality. Jenkins
in [14] related that the residual criticality gives an appreciation of the impact of imple‐
mented controls on identiﬁed risks. Lipol and Haq in [15] showed that it is obtained by
estimation of maturity of the implemented controls on risk criticality. Exposure of
WAEMU banks is clearly increasing, raising new possibly systemic risks [16]. Africa
country gets broadband connectivity, usually without adequate defenses, cybercrime
126
M. Ndaw et al.

spikes within a few days. The regional eﬀect is signiﬁcant because less-developed coun‐
tries are more vulnerable [17]. The actual used method for assessing controls maturity
is manual and has some limits including: time and personal investment, diﬀerent appre‐
ciation of maturity. Controls assessment is diﬃcult and not automatized; this may cause
unsecured online transitions, loss of customer conﬁdence for underserved areas
customers.
4
Our Contribution
We propose to automatize calculation of controls maturity by deﬁning two models which
allow automatic calculation of controls maturity. For this, we work with WAEMU banks
which oﬀer traditional banking services and electronic banking products like with‐
drawal, payment using credit, debit or prepaid card, E-banking and M-banking. We also
take into account all local banking regulations for WAEMU banks, safety standards of
information system and electronic banking, collaborations and partnerships. This sample
is representative because other banks have approximately the same risk considering their
similar activity, infrastructure and their dependance on local laws and regulations. The
two models are ﬁnally tested on controls related to type of information system compo‐
nent and type of Ebanking risks. The results are satisfactory for both models and suggest
to choose the second which have the best correlation rate with estimation and is very
easy to use.
4.1
Models Principles
To propose those models, we used the following 7 principles:
– Principle 1: Risk may have one or more controls
– Principle 2: Control is deﬁned to treat the identiﬁed and assessed risks
– Principle 3: Control have one maturity and three types (preventive, detective or
corrective)
– Principle 4: Only mature control can reduce likelihood and severity of risk
– Principle 5: Preventive control may reduce the likelihood of the risk (P)
– Principle 6: Detective control may reduce severity of the risk (G).
– Principle 7: Corrective control may reduce severity of the risk (G)
4.2
The Proposed Models
Model 1: The ﬁst proposed model is declines as follows:
Maturitycontrols =
[( ni∑
1
(
ai ∗i
)
+
nj∑
1
(
aj ∗j
)
+
nk∑
1
(
ak ∗k
))/
(ni + nj + nk)
]
(1)
The ﬁrst model has six independent parameters:
– [ai][aj][aj]: Maturity Index of preventive detective and corrective controls
– [i] [j] [k]: prevention detective corrective index
Maturity of Ebanking Security Measures
127

And depends on 3 independent variables:
ni [nj][nk]: number of preventive detective corrective controls.
Model 2: The second proposed model which calculate residual criticality is declines
as follows:
Maturitycontrols =
[
MatCoef ∗
( nctl
∑
1
(mati
)
/
nctl
)]
(2)
For each characteristic, we have deﬁned a impact percentage based on 10 main
characteristics of control maturity in order to obtain a maturity coeﬃcient (Table 1):
Table 1. Main characteristics of control
Level
Characteristics
Percentage
1
Exits
10%
2
Exits and Documented
15%
3
Exits, Documented and Executed
35%
4
Exits, Documented, Executed and Traceable
45%
5
Exits, Documented, Executed, Traceable and Eﬀective
55%
6
Exits, Documented, Executed, Traceable, Eﬀective and Eﬃcient
65%
7
Exits, Documented, Executed, Traceable, Eﬀective, Eﬃcient and Self-
assessed
75%
8
Exits, Documented, Executed, Traceable, Eﬀective, Eﬃcient, Self-
assessed and Managed
85%
9
Exits, Documented, Executed, Traceable, Eﬀective, Eﬃcient, Self-
assessed, Managed and Reported
95%
10
Exits, Documented, Executed, Traceable, Eﬀective, Eﬃcient, Self-
assessed, Managed, Reported and Archived
100%
Maturity Coeﬃcient (MatCoef = 0.1); Control maturity([mati] = [1, 2, 3, 4, 5])
Number of Control ([nctl] = [1, 2, 3]).
Models advantages: The two proposed models are innovation and have several advan‐
tages including: optimal assessment of security measures, automatic calculation of
controls maturity, decrease of estimation error rate, reduced time for obtaining controls
maturity. Also This result help to improve online transactions security, increase
customer conﬁdence, comfort and time saving. Otherwise, secure internet connection
allow mobile payment which enable ﬁnancial inclusion and payments system eﬃciency
for rural people.
4.3
Tests
Application of two models on all controls: We apply the two models on all E-banking
controls using the deﬁned formulas.
128
M. Ndaw et al.

Application of two models on controls by type of to information system component:
We test the models on controls by type of information system component as indicated in
the below graph (Fig. 1):
2
4
6
8 
10
type of information system component 
Estimation
Modele1
Modele2
Controls Maturity
7
8
9
10
Fig. 1. Controls maturity by type of information system component
Results 1
– Models values are equal, upper or lower than estimation values
– Residual values between models and estimation are random
– Correlation rate with estimation values is 96% for model 1 and 97.5% for model 2
– Model 1 have more parameters than Model 2 which is more easier
Application of the model on controls by type of Ebanking risks. We also test the
model on controls by type of Ebanking risks as indicated in the below graph (Fig. 2):
2
4
6
8
type of Ebanking risks
Estimation
Modele1
Modele2
Controls Maturity
8.0 9.0 10.0 11.0
Fig. 2. Controls maturity by type of Ebanking risks
Results 2
– Models values are equal, upper or lower than estimation values
– Residual values between models and estimation are random
– Correlation rate with estimation values is 95% for model 1 and 97% for model 2
– Model 1 have more parameters than Model 2 which is more easier
Maturity of Ebanking Security Measures
129

5
Conclusion
The online banking provides many beneﬁts to WAEMU customers but it also aggravates
traditional banking risks. Increased use of mobile services and internet as a new distri‐
bution channel for WAEMU banking transactions requires more attention against frau‐
dulent activities. Our study was taken with an objective to deﬁne and test two models
for calculation of online banking controls maturity. The models values were compared
to estimation values given by interlocutors during the traditional working sessions. The
both models allow an automatic calculation of Ebanking controls maturity, help to
increase WAEMU banks proﬁt and enable convenience and ﬂexibility for underserved
area. After testing and comparison, Model 2 has better correlation rate and is very easy
to implement. In our future works, we will apply the two models on mobile banking and
mobile payment risks.
References
1. Sullivan, R.J.: How has the adoption of internet banking aﬀected performance and risk in
banks. Financial Industry Perspectives (2000)
2. Abdou, H., English, J., Adewunmi, P.: An investigation of risk management practices in
electronic banking (2014)
3. Drimer, S., Murdoch, S.J., Anderson, R.: Optimised to fail: card readers for online banking.
In: Dingledine, R., Golle, P. (eds.) FC 2009. LNCS, vol. 5628, pp. 184–200. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-03549-4_11
4. Weest, C.: Threats to Online Banking. Symantec Security Response, Dublin (2005)
5. Otsuka, K.: Online Banking Risk eFraud: Hands oﬀ my Account. CPA Risk Management,
CUNA Mutual Group (2011)
6. Ting, A.H., Kuen, F.S., Xien, G.L., Ying, L.Y., Yew, W.S.: Online banking in Malaysia from
consumer perception on risk, April 2013
7. Zarei, S.: Risk Management of Internet Banking, Iran (2011). ISBN: 978-960-474-273-8
8. Li, H., Ward, R., Zhang, H.: Risk, Convenience, Cost and Online Payment Choice: A Study
of eBay Transactions. Georgia Institute of Technology, Atlanta
9. Malhotra, P., Singh, B.: The impact of internet banking on bank performance and risk.
Eurasian J. Bus. Econ. 2(4), 43–62 (2009)
10. Zhang, F.: An Analysis of the Online Banking Security Issues Reported by, Hole, Moen,
Tjostheim. University of Auckland
11. Sarma, G., Singh, P.K.: Internet Banking: Risk Analysis and Applicability of Biometric
Technology for Authentication. ISSN 2229 - 6107
12. Risk Mitigation Best Practices for Mobile and Online Banking, State Bank of Belle Plaine
Consumers
13. Osunmuyiwa, O.: Online Banking and the Risks Involved (2013)
14. Koﬃ, W.S.: The Fintech Revolution: An Opportunity for the West African Financial Sector.
School of Economics (2016)
15. Net Losses: Estimating the Global Cost of Cybercrime. Economic impact of cybercrime II,
June 2014
16. Jenkins, B.: Risk Analysis helps establish a good security posture (1998)
17. Lipol, L., Haq, J.: Risk Analysis Method: FMEA/FMECA in the Organizations. University
of Boras, Sweden (2011)
130
M. Ndaw et al.

An Opportunistic Connectivity Network for Rural Areas
in Senegal
A. Gueye1,2(✉), C. Mahmoudi3, O. I. Elmimouni4, M. L. Gueye1, and S. O. Ndiaye1
1 Universite Alioune Diop de Bambey, Bambey, Senegal
dahirou.gueye@uadb.edu.sn
2 University of Maryland, College Park, USA
3 LACL Laboratory, University Paris-Est Créteil, Créteil, France
4 Mohammadia School of Engineering, Rabat, Morocco
Abstract. In this paper, we present an opportunistic connectivity approach to
building a network for rural areas in Senegal. Our proposed solution is based on
a simple principle: given any situation, use the best connectivity solution avail‐
able; and when no solution is available at the moment, use delay/intermittence
tolerant solutions to oﬀer “oﬄine” services. Our network is built using point-to-
point Long Distance Wiﬁ links which are used wherever available. If a LD-Wiﬁ
link is not available, we use SMS service as a support for data communication.
In areas with no coverage, we use DTN (Delay Tolerant Networking) solutions.
For the seamless integration of all these technologies, we implement our light‐
weight platform on NDN (Named Data Networking), a new architecture proposed
for the future Internet. A low cost implementation of the solution has been
deployed in a test environment using Raspberry Pis and GSM dongles.
Keywords: Rural network deployment · DTN · NDN · SMS
Long Distance Wiﬁ
1
Introduction
In the last decade, Africa has witnessed an increasing rate of adoption of new technol‐
ogies. In Senegal the penetration rate for mobile phones is 116.6% while that of Internet
continues to increase at a 60.28% rate [1]. According to the WEF (World Economic
Forum)’s “Network Readiness Index” ranking [2], Senegal is on the top list in West
Africa and occupies the 14th place continent-wide. This leading position is partially due
to the favorable geographic position of the country, which is a hub for many underwater
cables [3] and serves as an interconnection point for its neighbors.
Despite the privileged geographic position and the many eﬀorts by the public and
private actors, there remains a lot to be done in terms of connectivity, as shown by a
recent report from the government [1]. The three main cellular providers (Orange, Tigo
and Expresso) oﬀer 3G services but mostly in urban areas. 4G services have only recently
been launched, only by Sonatel and only in major cities. ADSL services are still oﬀered
in a monopolized way and is almost inexistent in most of the country. The government’s
eﬀorts to lay ﬁber has been very slow and to date, only 9000 km of ﬁber have been
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 131–137, 2018.
https://doi.org/10.1007/978-3-319-72965-7_12

installed. In addition to that, the cost of connectivity remains relatively high [4]. Overall,
a lot of eﬀort needs to be made in terms of diversiﬁcation of the services and the service
providers, accessibility to connectivity and enhancement of current infrastructures as
well as building new ones.
This need of infrastructure building is even more pronounced in rural areas, which
have received a small portion or even none of recent investments. In fact, due to low
density of population in these areas, private companies (and unfortunately government)
focus their investments in urban areas, where returns in investment (RIO) are higher.
This has led to a widening digital gap between rural and urban areas within the country
and the many opportunities that can potentially be enabled by ICTs are yet to be fully
witnessed by the countryside.
To address the lack of connectivity in rural areas, we propose a novel connectivity
architecture that is based on a simple opportunistic principle: given any situation, use
the best connectivity solution available; and when no solution is available at the moment,
use delay tolerant networking (DTN) principles to oﬀer oﬄine services. Our network
will be backboned using point-to-point Long Distance Wiﬁ links [5]. On the access links,
it uses data services wherever available. If none of the data services is available, we use
SMS service as a support for data communication. In areas with no network coverage,
we use DTN solutions [6]. For the seamless integration of these diﬀerent technologies,
we use the newly proposed “Named Data Networking” (NDN) architecture, which is
found to be naturally suitable to environments where connectivity is intermittent.
The remaining parts of this paper is organized as follow. The next Sect. 2 presents
a non-exhaustive list of works that are related to the present paper. In Sect. 3 we discuss
the details of our proposed solution by focusing on NDN as well as our deployment test.
Closing remarks and future are presented in Sect. 4.
2
Related Work
Developing rural connectivity networks has been a topic of widespread interest in the
last decade. The World Bank has funded a couple of such projects [7]. Village Base
Station (VBTS) [8] is another economical and optimized architecture for areas of low
user density and limited infrastructure. It is a GSM base station designed to be deployed
“oﬀ the grid” to locations without power or network infrastructure. It uses DTN prin‐
ciples, but mostly in order to optimize energy usage. There have been a lot of activities
in the ﬁeld of delay tolerant networking DTN: IBR-DTN [9] and ION- DTN [10] are
open source implementations of DTN. ZebraNet [11] is an application of DTN for real
biological. Although the application area is the same as ours, the technologies used in
these projects are diﬀerent from our NDN.
The work in [12] is the most related to ours. It applies the NDN paradigm into
Vehicular Network (V-NDN) and implements an architecture that enables networking
among computing devices independent from whether they are connected through wired
infrastructure, ad-hoc, or DTN. An experiment was run in the campus of UCLA. Despite
the diﬀerence in the targeted application, the paper shares many commonalities with our
paper. One fundamental diﬀerence in our approaches is the opportunistic principle that
132
A. Gueye et al.

guides our implementation, and which they do not consider (probably because of their
focus on V-NET). Also, due to the characteristics of our application environment (rural),
our paper is more focused on the DTN aspect of the network. Finally, [12] has not
implemented our SMS features.
3
Opportunistic Connectivity
Rural areas are characterized by factors that are a-priori unfavorable to the development
of new technologies. First, the areas are diﬃcultly accessible because transportation
infrastructures are often impractical or inexistent. This translates to high costs for
building and maintenance of network infrastructures, which most of the private (or
public) actors are not willing to bear. In addition, population are sparsely scattered over
the areas with a very low density. As a consequence, covering the rural areas requires
large scale deployment, while the expected RIO is quite low. Thus, any viable solution
should have a relatively low building and maintenance cost.
Another characteristic of rural areas is that connectivity, if it exists, might be very
intermittent because of many reasons. First, downtime periods tend to be longer in rural
areas because of the lack of local expertise and appropriate transportation infrastructures.
More seriously, grid power is inexistent or at the best unreliable. Also, renewable energy
solutions are yet to be adopted in rural areas. As a consequence, network infrastructures,
which highly rely on grid power, are subjects to the many ups and downs of the grid.
Hence, a realistic approach to the problem of connectivity should assume that links will
be up and down at a rate that is much higher than usual.
3.1
Our Three Pillar Technologies
Bearing this in mind, we propose an opportunistic connectivity architecture that
combines diﬀerent technologies as follows: at any given time, uses the best communi‐
cation solution available. Our current implementation is built upon three main pillars:
Long-Distance WIFI (LD-WIFI), SMS for Data (SMS4D), and Delay Tolerant
Networking (DTN) principles by using moving entities as data mules.
LD-WIFI is a low cost solution that extends the IEEE 802.11 technology to kilo‐
meters while keeping the cheap hardware. It has been tested and deployed in many parts
of the world [13]. It uses unlicensed band, has been implemented in open source and is
today available online and oﬀ the shelves. In our project, we use LD- WIFI as backbone
by building a set of point-to-point links.
SMS4D is an approach that uses the short messaging system as a support for data
communication. For that, we ﬁrst note that, because of its potentially low data rate,
SMS4D will not be suitable certain applications such as live video. However, for appli‐
cations that tolerate delay and are not data-greedy, SMS4D might be very suitable. In
our current implementation we use GammuSMS Deamon [14] and a GSM dongle as a
gateway. We buy prepaid cards from Orange (Sonatel) with a subscription that gives up
to 200 sms a day and costs 1000FCFA (about $1.6).
An Opportunistic Connectivity Network for Rural Areas in Senegal
133

Finally, there is exist situations where none of the technologies above is available.
In these cases, we use DTN principles to continue the “communication” oﬄine. For that,
we rely on the mobility of (physical) agents such us human, vehicles and drones which
can serve as carriers for the communication. In this work, we implement the DTN prin‐
ciple as a new forwarding strategy for NDN.
By combining diﬀerent (and independent) communication infrastructures, our
system gains in robustness. Moreover, with the implementation of the DTN principle,
we provide connectivity to areas without network prior coverage. We discuss the details
of our implementation in the next section.
3.2
Implementation Platform: Named Data Networking
Our proposed platform integrates the technologies mentioned above in a seamless
manner by using the newly proposed “Named Data Networking” architecture which is
found to be naturally suitable to environments where connectivity is intermittent.
NDN is an architecture in which data is requested by name rather than location. It is
based on two main principles that are suitable for DTN: (1) Interest packets stay, up to
a certain duration, in the Pending Interest Table (PIT) unless it is satisﬁed (2) Data
packets are stored in the Content Store to enable caching feature. These behaviors match
the requirements to build DTN as the request or the response can live in a disconnected
node while waiting for active links to transmit the packets to the next hop. This makes
NDN very appealing for communication in underserved areas.
In NDN, it is the forwarding strategy that determines when and where an Interest
packet is transmitted. Existing forwarding strategies [15] focus of QoS and congestion
control by assuming that the link layer is stable enough and direct links exist between
the diﬀerent nodes physically or over the air. That assumption is not true in underserved
areas where connectivity might be intermittent for several reasons, as discussed above.
As a consequence, supporting DTN requires a speciﬁc forwarding strategy that enhance
the opportunistic aspect of NDN data transmission.
We implement a DTN forwarding strategy based on the follow principles: First, as
the data may stay in a transit node for a long duration, the strategy ensures that the
interest packet stays active in the PIT until it is satisﬁed. Second, the DTN strategy plays
a proactive role to identify the forwarding opportunities each time that a link is detect
for a new hop. Moreover, the strategy handles the priorities for diﬀerent layer 2 tech‐
nologies. It may use SMS to transmit a packet if no Wiﬁ link is available and no mule
shows up for a certain duration. Figure 1 gives a sketch of our DTN strategy.
DTN Forwarding Strategy (simplified):
1- 
Receive(interest/data)
2- 
If Wifi interface is connected to next hop: send to next hop 
2-   Else:
• If SMS-Enabled-Appl: send to SMS gateway
• Else: keep in content store and forward upon data mule detection event
Fig. 1. DTN strategy
134
A. Gueye et al.

NDN introduces a new paradigm for accessing content where content can be
retrieved from the network caches instead of a systematic retrieval from the source. For
rural areas connected using DTN, this feature is valuable as the content retrieved in a
given area will stay in the caches available within this area and can be retrieved instantly
by other users without a need for an active connection to the source. Moreover, if
multiple users are requesting the same content within a short time interval, the requests
will be aggregated which allows a natural optimization of the network usage. This built-
in optimization is very important in the rural connectivity setting where it is expected
that most links will be of low data rate (e.g., SMS4D links). In fact, in this case, having
less traﬃc to carry inside the network is vital.
We implement our DTN forwarding strategy [16] and integrate it with the NFD
(NDN Forwarding Deamon) [17], which we run on Raspberry Pi3’s. We use the
NDNCxx API [18] to develop our consumer and producer test applications. A proof-of-
concept experiment was carried at the end of the month of January in the campus of the
University Alioune Diop of Bambey.
3.3
Test Deployment
In our experiment we deﬁne three network elements: a producer generates random data
(as in [18]), a consumer requests the data, and a mobile node (data mule) acts as relay
or data mule (when suitable). All nodes are Wiﬁ-capable R-Pi3’s that run an imple‐
mentation of our platform. The consumer and producer are equipped with GSM dongles.
The consumer and one of the LD-Wiﬁ radios are co-located at one building (Site A in
Fig. 2). Three hundred meters away, sits the producer with the other LD- Wiﬁ radio.
The mobile node was handled by a pedestrian and was powered using a portable charger.
It played the role of a data carrier in all our experiments.
Fig. 2. Opportunistic connectivity: proof-of-concept deployment
In our ﬁrst experiment, we place the consumer and the mobile in the same network
in Site A and connect it to Site B via LD-Wiﬁ. In this setting, all tree nodes are in the
same network and the mobile node is just a “relay”. In the second experiment, we deac‐
tivate the Wiﬁ interfaces and activate the SMS gateway. In this case, when the consumer
sends a request, it realizes that the Wiﬁ interface is down and directly uses the SMS to
send the interest directly to the producer. The producer gets the request and responds
An Opportunistic Connectivity Network for Rural Areas in Senegal
135

also using SMS. In the last experiment, we deactivate the LD-Wiﬁ and use a smartphone
as (short range) Wiﬁ hot spot. We ﬁrst put all nodes in the same network and then move
the producer to Site B, so that the Wiﬁ interfaces are all up, but the producer is discon‐
nected to the rest of the network. When the consumer generates the interest, it realizes
that there is an interface to the next hop (the mobile node), to which it forwards the
interest. Upon reception, the mobile ﬁnds that the interface is up, but there is no connec‐
tion to the next hop (the producer). It then keeps the interest in its content store. From
this point, the pedestrian starts walking towards Site B, having with him the Wiﬁ hot
spot and the mobile node. Since Site B is 300 m away, the consumer gets disconnected
from the network at some point. Upon arrival, the producer detects the Wiﬁ interface,
reconnects to the mobile node to receive the interest and sends it the response. A similar
process gets the data back to the consumer.
4
Conclusion and Future Work
In this paper we present a new architecture for rural connectivity. It is based on a simple
opportunistic principle: given any situation, use the best connectivity solution available;
and when no solution is available at the moment, use delay/intermittent tolerant solutions
to offer “offline” services. We implement a version of our platform that builds on three
pillars: LD-Wifi, SMS, and DTN principles. For a seamless integrations of these different
technologies, we implement our lightweight solution on NDN. A proof-of-concept deploy‐
ment has been carried using Raspberry Pi3 at the campus of the University Alioune Diop
of Bambey. A larger scale deployment is being prepared with two use case applications: a
rural online market platform and a system for environmental data collection. Such large
scale deployment will also involve evaluating the performance of our system by studying
metrics such as the size of the content store, the average time to get a response, the
“optimal number” of data carriers, the data rate obtained with SMS4D etc. In this deploy‐
ment, we also plan to integrate other communication technologies such as ZigBee and
Bluetooth.
Acknowledgments. This work was partially accomplished under NIST Cooperative Agreement
No. 70NANB16H024 with the University of Maryland. It was also partially supported by the
University Alioune Diop of Bambey PGF-Sup program (Crédit IDA 4945-SN).
References
1. ARTP: Strategie Numerique du Senegal. http://www.gouv.sn/-Senegal-Numerique-
2016-2025-.html. Accessed 29 Jan 2017
2. WEF Network Readiness Index (2016). http://reports.weforum.org/global-information-
technology-report-2016/networked-readiness-index/. Accessed 28 Jan 2017
3. Submarine Cables. http://www.submarinecablemap.com/. Accessed 3 Feb 2017
4. ICT Development Index. https://www.itu.int/net4/ITU-D/idi. Accessed 3 Feb 2017
136
A. Gueye et al.

5. Patra, R., Nedevschi, S., Surana, S., Sheth, A., Subramanian, L., Brewer, E.: WiLdnet: design
and implementation of high performance Wiﬁ based long distance networks. In: Proceedings
of 4th Conference on Networked Systems Design & Implementation, Berkeley, CA, USA,
p. 7 (2007)
6. Fall, K.: A delay-tolerant network architecture for challenged internets. Intel Research
Berkeley, Technical report IRB-TR-03-003 (2003)
7. World Bank Rural connectivity in Zambia. http://projects.worldbank.org/P157054/?
lang=en&tab=details. Accessed 15 Jan 2017
8. Heimerl, K., Brewer, E.: The village base station. In: Proceedings of the 4th ACM Workshop
on Networked Systems for Developing Regions (NSDR 2010). ACM, New York (2010)
9. Doering, M., Lahde, S., Morgenroth, J., Wolf, L.: IBR-DTN: an eﬃcient implementation for
embedded systems. In: CHANTS 2008, 15 September 2008, San Francisco, California, USA
(2008)
10. ION-DTN. http://dtnsetup.com/. Accessed 15 Jan 2017
11. Liu, T., Sadler, C., Zhang, P., Martonosi, M.: Implementing software on resource- constrained
mobile sensors: experiences with Impala and ZebraNet. In: The Second International
Conference on Mobile Systems, Applications, and Services (MobiSys 2004), June 2004
12. Grassi, G., Pesavento, D., Wang, L., Pau, G., Vuyyuru, R., Wakikawa, R., Zhang, L.:
Vehicular inter-networking via named data ACM HotMobile 2013 poster. Mobile Comput.
Commun. Rev. 17, 23–24 (2013)
13. Zennaro, M., Bagula, A., Gascon, D., Noveleta, A.B.: Planning and deploying long distance
wireless sensor networks: the integration of simulation and experimentation. In: Nikolaidis, I.,
Wu, K. (eds.) ADHOC-NOW 2010. LNCS, vol. 6288, pp. 191–204. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-14785-2_15
14. Gammu SMS. https://wammu.eu/smsd/. Accessed 5 Jan 2017
15. Marica, A., Campolo, C., Molinaro, A.: Forwarding strategies in named data wireless ad hoc
networks: design and evaluation. J. Netw. Comput. Appl. 50, 148–158 (2015)
16. NDN-DTN Strategy. https://github.com/elmiomar/. Accessed 5 Jan 2017
17. NDN Forwarding Deamon. http://named-data.net/doc/NFD/current/. Accessed 15 Dec 2016
18. NDNCxx. https://github.com/named-data/ndn-cxx. Accessed 15 Dec 2016
An Opportunistic Connectivity Network for Rural Areas in Senegal
137

Innovation

Entrepreneurship, Education and Youth Employment
in Africa: Reframing Higher Education
Pazisnewende François Kabore
(✉)
Jesuit University Institute (CERAP/IDDH), Abidjan, Côte d’Ivoire
fpk2@georgetown.edu, francois.kabore@cerap-inades.org
Abstract. Given the high rate of unemployment in most countries in Sub-
Saharan Africa policy makers innovate by fostering job creation and self employ‐
ment through entrepreneurship education. We build on the Theory of Planned
Behavior developed by Ajzen (1991) and use Probit regression to investigate the
marginal impact of entrepreneurship education on entrepreneurial intent using
survey data gathered in Côte d’Ivoire in 2015. We ﬁnd that an ecosystem of
entrepreneurship increases entrepreneurial intent by 50% while an entrepreneur‐
ship awareness course does not have any statistically signiﬁcant impact. This
result suggests reframing higher education in order to favor the entry into the
private sector by the youth.
Keywords: Entrepreneurship · Education · Innovation · Youth employment
Africa
1
Introduction
The high rate of unemployment coupled with the apparent inability of formal education
to prepare youth for jobs or to train job creators (not only job seekers) raises the funda‐
mental question of the appropriate innovation in the education system. According to the
2015 report of the International Labor Organization (ILO)1, Africa has 226 million youth
aged 14 to 35 years and an average of more than 10 millions of youth will be entering
the job market every year in the coming years. In countries like Côte d’Ivoire 79% of
the population is aged less than 25 years. Unfortunately, this youth is plagued with high
unemployment as 75% of the unemployed are aged 14–35 years. Meanwhile, the size
of public job openings for 2016 was not above ten thousand jobs by the middle of the
year 20162. It is therefore obvious that governments can no longer rely only on public
jobs.
1 International Labor Organization (ILO) http://www.ilo.org/global/statistics-and-databases/
meetings-and-events/international-conference-of-labour-statisticians/19/lang--en/index.htm.
2 Exactly 8321 public jobs opening by may 2016.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 141–149, 2018.
https://doi.org/10.1007/978-3-319-72965-7_13

In a 2013 report on “Youth employment and insertion support program”3, the African
Development Bank (AfDB) found that the increase in unemployment is stronger among
youth who attended university than not. The aforementioned 2015 ILO report suggests
that youth with tertiary education are two to three times more likely to be unemployed
than those without education. Consequently, the data on unemployment and education
mismatch raises security issues (Kabore (2016)) and questions the relevance of higher
education in Côte d’Ivoire and in most countries in West Africa. Massaquoi et al. (2014),
Sabbi et al. (2014) and Thia (1998) show that there is skills mismatch respectively in
agricultural education in Siera Leone, in self-employment skills in Ghana and in general
education in Ivory Coast. The results of Sabbi et al. (2014), in particular, suggest that
the Millennium Development Goals (MDG) educational objectives promote enrolment
without commensurate incorporation of quality or self-employable skills teaching and
learning.
Since, formal education does not meet the employment needs of students, an inno‐
vation in higher education could consist in encouraging entrepreneurship. In that
regards, statistics from the European Union conﬁrms that jobs are created more by small
and medium enterprises than by large and global enterprises4. Building on the Theory
of Planned Behavior (TPB) by Ajzen (1991), several empirical studies tried to predict
the likelihood of people undertaking entrepreneurial activities. According to TPB,
intentions to perform behaviors of diﬀerent kinds can be predicted with high accuracy
from attitudes toward the behavior, subjective norms, and perceived behavioral control;
and these intentions, together with perceptions of behavioral control, account for consid‐
erable variance in actual behavior. Our study goes along the lines of Fayolle and Benoît
(2013), Boissin et al. (2009), Autio (2001) and Mintzberg (2004). Autio (2001) studies
the impact of entrepreneurial education among students across three continents: America
(USA), Europe (Finland, UK, and Sweden) and Asia. Autio (2001) contributes to the
literature by showing, in a multi-cultural environment, a weak inﬂuence of subjective
norms as reﬂected in the perceived general acceptability of entrepreneurship as a career
choice on entrepreneurial intent.
Fayolle and Benoît (2013), on the other hand, studied the impact of a short awareness
compulsory program on entrepreneurial attitude for a group of 275 French students.
Their results indicate that the initial level of entrepreneurial intention and prior entre‐
preneurial exposure are not good predictors of an entrepreneur’s behavior as modeled
by the TPB. There also appears to be no observable eﬀect on the students in the short
term, even though on average, a signiﬁcantly positive impact on the student’s attitudes
toward entrepreneurship and perceived behavioral control in the medium term (six
months after the end of the program) has been observed. One would expect at least MBA
trainings in business schools to prepare students to be good managers and job creators.
However, given that conventional MBA programs are mostly for young people with
little or no experience, Mintzberg (2004) argues that regular business school training
3 African Development Bank (AfDB). “Appraisal Report” http://www.afdb.org/ﬁleadmin/
uploads/afdb/Documents/Côte_d_Ivoire_-_Youth_Employability_and_Inser‐
tion_Support_Programme__PAAEIJ__-_Appraisal_Report.pdf.
4 European Union, http://ec.europa.eu/growth/smes/ See deﬁnition of SME by the EU: https://
ec.europa.eu/growth/smes/business-friendly-environment/sme-deﬁnition_en.
142
P. F. Kabore

emphasizes on analytics and techniques. As a result, it leaves graduates with the wrong
impression that they have been trained as managers. He claims that business school
training should take an innovative approach by encouraging practicing managers to learn
from their experience. To the best of our knowledge, no such studies like Autio (2001)
or Fayolle and Benoît (2013) has been carried out in Africa. We therefore ﬁll the gap
by investigating the marginal impact of entrepreneurial education on student’s entre‐
preneurial intent in higher education across many institutions of higher education in
Abidjan, Côte d’Ivoire. The contribution of such a study is to provide higher education
with a novel and innovative approach to higher education. After the introduction, the
second part of the paper discusses the methodology and the data. The third part presents
the results and interpretation of the results before the conclusion, in part four.
2
Methodology and Data
2.1
Design
Ajzen (1991) deﬁnes intention as an indicator of the willingness to try something, the
eﬀort that one is ready to exert in order to behave in a certain way. Along the same lines,
Davidson (1995) considers entrepreneurial intent as the decision to start a business in a
planed way. It matters at this point to restate that the study focuses on the determinants
of the willingness of students to engage in entrepreneurial activities. Ajzen’s model
suggests the following equation to capture entrepreneurial intent.
Intent = 𝛂∗Beliefs + 𝛃∗Subjective_Norm + 𝝑∗Perceived_Control
(1)
According to Eq. (1), above, three main variables determine intention. The attitudes
towards the behavior which we term “beliefs” to reﬂect the fact that this is about how
an individual evaluates the consequences of his behavior. The subjective norms (Subjec‐
tive_Norm) refer to the internalized value system that derives from social values. The
perceived control (Perceived_Control) is an evaluation of one’s capacity to implement
the behavior.
We use stratiﬁed random sampling to select a random 300 students based on the
distribution of public versus private institutions, and universities versus “grandes
écoles” in Abidjan as reﬂected in the 2013–2014 statistical yearbook of higher educa‐
tion5. Given that the study aims at investigating the marginal impact of entrepreneurship
education on entrepreneurial intent, we focused on having a representative sample of
institutions of higher education in Abidjan.
5 The book is titled «Annuaire Statistique de l’Enseignement Supérieur de Côte d’Ivoire 2013–
2014».
Entrepreneurship, Education and Youth Employment in Africa
143

The interviewees6 come from 22 institutions of higher education located mainly in
four city halls (communes) of Abidjan: Adjamé, Cocody, Plateau and Yopougnon. As
shown in Table 1 above, 55% of them are from universities while 45% are from grandes
écoles. The latter tend to be elite schools while universities tend to welcome pretty large
number of students. The student body is almost fairly distributed between private
(47,92%) and public school (52,08%). The interviews took place, over a month, from
August 12th to September 15th. A pilot test of the questionnaire was ﬁrst administered
to students in CERAP to insure the proper understanding of the questions by inter‐
viewees. Questions were then rephrased for use during the survey. A typical survey
would take between 10 to 15 min. Students interviewed have a very diverse background
in terms of the topics studied at university (sciences, literature, business administration,
etc.).
Table 1. Sampling of origin institutions of the interviewees
Type/Institution
Private
Public
Total
%
University
16
116
132
55%
“Grandes Ecoles”
99
9
108
45%
Total
115
125
240
100%
%
47,92%
52,08%
100%
In order to answer the research question on the marginal impact of entrepreneurship
education on entrepreneurial intent, two hypotheses were tested:
Hypothesis 1: An entrepreneurship course has an impact on entrepreneurial intent
Hypothesis 2: An entrepreneurship ecosystem has an impact on entrepreneurial intent,
where an entrepreneurship ecosystem refers to both theoretical and practical courses
aimed at developing entrepreneurship. It includes courses, mentoring, exposure to
entrepreneurial activities, ﬁnancial and management skills building as well as anything
included in business development services.
Given the data gathered and the binary nature of the dependent variable, a probit
regression was used to predict the marginal impacts of the independent variables on the
dependent variable of interest. The following sub-section provides a description of the
main variables of interest.
2.2
Descriptive Statistics
The dependent variable measures student entrepreneurial intent, i.e., the willingness of
a student to engage in entrepreneurial activities. Given the deﬁnition discussed above,
6 I am very grateful to two students Jaël Konan from the Jesuit University Institute (CERAP-
INADES) and to Gnougon Ouattara (from the National School for Applied Economics and
Statistics, ENSEA) who helped in the data collection. Gnougon Ouattara wrote an M.A.
dissertation at the ENSEA, using the same data. I am also greatful to Monde Muyangwa
(Woodrow Wilson Center, Washington DC) for very insightful ideas on reframing higher
education in Africa.
144
P. F. Kabore

it is worth recalling that entrepreneurial intent captures the disposition, the beliefs and
the conﬁdence of a student to take the necessary steps to be an entrepreneur. Close to
half of the students, i.e., 47.5% of them intend to be entrepreneurs (see Table 2 below).
Table 2. Variable characteristics
#
Variable name
Freq./Mean
Freq. (%)/St. Dev.
Comments
1
Entrepreneurial
intent
114
47.5
Measures entrepreneurial intent.
Intends/wants to be an entrepreneur
to be self-employed
2
Entrepreneurship
course
75
31.25
Have already taken a course in
entrepreneurship, as part of their
curriculum or as an elective
3
Prepared to
entrepreneurship
76
31.67
Think that overall, their curriculum
prepares them to be entrepreneurs
4
Entrepreneurship
course or
preparation
129
53.75
Either took a course in
entrepreneurship or think they
curriculum prepared them to it, or
both
5
Entrepreneurship
course and
preparation
22
9.17
Took an entrepreneurship course
and think their curriculum prepared
them to entrepreneurship
6
Science major
70
29.17
Math, Sciences, biology, etc.
(versus literature, etc.)
7
Sex (males)
144
60
Majority are males
8
Internship
58
24.17
Have done internship
9
Type of school
(Private vs.
public)
108
45
Majority in public school
10
Age in years
22.89
2.93
Min Max = [16–35]
11
Year at University 2.52
1.32
Min Max = [1–5]
Abs. = absolute, Freq. = Frequency, St. dev. = Standard Deviation
Three independent variables are worth being discussed: “entrepreneurship course”,
“entrepreneurship ecosystem”, and “internship”. An entrepreneurship course is any
course (from an entrepreneurship awareness course to a thorough in-depth one) that a
student declares to have taken. Given the new reform of higher education in place in
Côte d’Ivoire and other countries in the CAMES7 region, such course would be a two
7 CAMES stands for «Africa and Malagasy Council for Higher Education», Conseil Africain et
Malgache pour l’Enseignement Supérieur. The countries of CAMES engaged in a reform
called «LMD reform» that organises studies at university in three degrees: a BA, a MA and a
Doctorate. This reform builds on the «Bologna Process» in higher education which took place
in Western Europe.
Entrepreneurship, Education and Youth Employment in Africa
145

to three credits course, i.e., a 30 to 45 hours8. The second independent variable of interest
is entrepreneurship ecosystem. An ecosystem gathers together the livings in a particular
biotope or place and these livings interact. The value added of this variable in comparison
to the previous one (entrepreneurship course) is that it has a practical component. This
practical component could include some type of projects to be conducted, interactions
with enterprises, mentoring, etc. Overall, this variable is a proxy for “does the curriculum
prepares students to be entrepreneurs”? To that extent, this variable is less about the
beliefs than about the education content of the curriculum, as perceived or experienced
by students. That’s why this study could also be thought as an analysis of the demand
for entrepreneurship education in Abidjan. The third variable of interest is “internship”.
Students answer the question: “have you done an internship”? An internship refers to a
period of practical work whereby students have work experience outside of their class‐
rooms.
We also control for variables of interest such as gender and student science back‐
ground. Only 29,17% of the students have a science background while 60% of the inter‐
viewee are males. They are 23 years old on average and have already spent 2.5 years on
average at university. In the sample, students have a maximum of 5 years spent at
university. Table 2 below provides an overview of the main variables and the section
below discusses the results of the probit regressions and also provides policy recom‐
mendations.
3
Results and Interpretation
3.1
Results of the Regression Analysis
The results of the regression analysis suggest that taking an entrepreneurship awareness
course does not have a statistically signiﬁcant impact on entrepreneurial intent. On the
other hand, beneﬁting from an entrepreneurship ecosystem increases the likelihood of
a student choosing to become an entrepreneur by 50% (see Table 3 below). We test two
other models with variation of the ecosystem variables. Model (1) that tests the impact
of an entrepreneurial course could be considered as a baseline. In model (3), we put
together students who either took awareness courses or beneﬁted from an entrepreneur‐
ship course. In model (4) only students who declared to beneﬁt from both an awareness
course and an entrepreneurship ecosystem are taken into account. The magnitude of the
marginal impact of the ecosystem variable conﬁrms the importance of beneﬁting from
an ecosystem versus just taking an awareness course: 50% for model (2), 32% for model
(3) and 42% for model (4). These marginal eﬀects are signiﬁcant with respectively 95%,
99% and 99.9% conﬁdence interval.
8 A weakness of this variable though is that we cannot control for quality (content), quantity
(number of credit hours) and type (compulsory or elective) of the course across institutions
and programs.
146
P. F. Kabore

Table 3. Marginal impact of education on entrepreneurial intent
Y = Entrepreneurial
intent
Model (1)
Model (2)
Model (3)
Model (4)
Ent. course
0.01
(0.12)
Ent. Ecosystem
0.50***
(7.74)
Course or Ecosystem
0.32***
(4.59)
Course and Ecosystem
0.42**
(4.57)
Internship
−0.23*
(−2.42)
−0.22*
(−2.14)
−0.24*
(−2.46)
−0.23*
(−2.30)
Valuing Entrep.
0.38***
(4.16)
0.30*
(2.75)
0.33**
(3.23)
0.34***
(3.41)
Constant
Yes
Yes
Yes
Yes
Region ﬁxed eﬀect
Yes
Yes
Yes
Yes
Log likelihood ration
−146.74
−124.87
−137.12
−140.95
R-Square
0.12
0.25
0.17
0.15
Observations
240
240
240
240
t statistics in parentheses, * p < 0.05, ** p < 0.01, *** p < 0.001
A counter intuitive result concerns the marginal impact of the variable “internship”.
The variable “internship” is expected to capture the practical side of the curricula and
the ability of the curricula to prepare students for either the job market or entrepreneur‐
ship. Results show that doing an internship reduces the likelihood of a student willing‐
ness to become an entrepreneur. Does this conﬁrm that most students are prepared to be
job seekers and not job creators? Surprisingly enough, we expected also the variable
“type of institution of higher education” to matter. However, its marginal eﬀect is not
statistically signiﬁcant across the diﬀerent models tested. As a matter of fact it could be
assumed that students in private (vs public) institutions would beneﬁt from diﬀerent
exposure with respect to entrepreneurship. This surprising situation is also the case for
the variable “science major”. What type of policy recommendations could be drawn
from such results?
3.2
Policy Implications
The very ﬁrst policy implication of these results is that, beside the information it gives
to students, an entrepreneurship course does not lead students to engage in entrepre‐
neurship. As governments in West Africa, in general, and in Côte d’Ivoire, in particular,
strive to innovate in higher education, this result is a warning against ineﬀective spending
of resources on entrepreneurship curricula. On the other hand, schools which aim at
promoting entrepreneurship should choose to build entrepreneurship ecosystems.
Favoring the emergence of an entrepreneurship ecosystem has the advantage of insuring
Entrepreneurship, Education and Youth Employment in Africa
147

some type of return on investment as the marginal impact on student entrepreneurial
intent is pretty strong (50% increase).
Last but not least, a ﬁnal policy implication concerns the type of internships done
by students. First of all, less than a quarter of all student interviewed have done an
internship (see Table 1). The aforementioned LMD reform is supposed to provide both
a strong theoretical and practical training. Schools should be encouraged to raise the
share of students doing internships. However, more importantly, internships could not
only consist in exposure to wage jobs but also in professional experiences whereby
students learn to be job creators. This raises in turn the issue of the collaboration between
the private sector (mostly ﬁrms) and schools with the mediation of governments, the
civil society, communities, parents, and all education stakeholders. In a country that
experiences a solid and relatively sustained economic growth and where business is
booming as conﬁrmed by the high inﬂow of foreign direct investment9, schools could
favor internships10 with entrepreneurs to raise a new race of Ivorians who are eager to
be job creators rather than just job seekers.
4
Conclusion: Reframing Higher Education in West Africa
This study investigated the determinants of entrepreneurship intent among students in a
sample of 22 institutions of higher education in the city of Abidjan. We found that simple
entrepreneurship awareness courses do not have a statistically significant impact on
students’ decision to engage in entrepreneurship. However, when students benefit from an
entrepreneurship ecosystem, their likelihood of becoming entrepreneurs increases by 50%.
The contribution of this study is threefold. First, it fills the gap in the literature on innova‐
tive or alternative ways of educating that promotes entrepreneurship by providing empir‐
ical evidence from Côte d’Ivoire. Second, it confirms along the lines of Autio (2001) and
Fayolle (2013) that entrepreneurship education matters for entrepreneurial intent based on
the Theory of Planned Behavior. Third, it follows that governments and policy makers who
are eager to promote entrepreneurship in higher education should focus on creating entre‐
preneurship ecosystems instead of just requiring institutions of higher education to include
entrepreneurship awareness courses in their curricula.
The study suggests also that internships do indeed prepare students to be good job
seekers. If the aim of education is also to train job-creators, then rethinking the goals of
internships is crucial. Overall, the results of this study call for reframing higher education
in West Africa, in general, and in Côte d’Ivoire, in particular. Further studies linking
entrepreneurship intent and the actual creation of enterprises would complement this
study. Second, looking at the determinants of entrepreneurship in education curricula at
secondary or even primary school level would be appropriate as a decision to become
an entrepreneur might come earlier in life. Last but not least, this study focused on the
demand side of entrepreneurship education. Investigating the supply side would help
9 See the study by BNP Paribas Bank at https://www.tradesolutions.bnpparibas.com/fr/
implanter/cote-d-ivoire/investissement on FDI in Côte d’Ivoire, consulted January 26th, 2017.
10 Note that in the business world, a typical experience or exposure with entrepreneurs with the
aim of becoming an entrepreneur will be termed as coaching rather than internship.
148
P. F. Kabore

check whether there is any mismatch between the perceptions of students and school
administration about the aim of education curricula as far as entrepreneurship is
concerned.
References
African Development Bank (AfDB). “Appraisal Report”. http://www.afdb.org/ﬁleadmin/
uploads/afdb/Documents/Côte_d_Ivoire_-_Youth_Employability_and_Insertion_Support_
Programme__PAAEIJ__-_Appraisal_Report.pdf
Agence d’Etudes et de Promotion de l’Emploi-AGEPE: L’Emergence de l’Esprit Entrepreneurial
en Côte d’Ivoire: le Cas des Demandeurs d’Emploi Inscrits à l’AGEPE. Document de travail.
3. Chargé d’Etudes: Mlle Thia Aline, 47 p. (1998)
Ajzen, I.: The theory of planned behavior. Organ. Behav. Hum. Decis. Process. 50, 179–211
(1991)
Boissin, J., et al.: Les déterminants de l’intention de créer une entreprise chez les étudiants: un
test empirique. Management, vol. 12, pp. 28–51. AIMS (2009)
Côte 
d’Ivoire, 
ENSETE 
(2013). 
http://www.ins.ci/n/documents/travail_enfant/Enqute
%20nationale%202013.pdf
Conseil 
Africain 
et 
Malgache 
pour 
l’Enseignement 
Supérieur 
(CAMES). 
http://
www.lecames.org/index.php/acceditation/plan-strategique-2015-2019. In an audience with
the Secretary General of the CAME (http://www.lecames.org) in Ouagadougou (April 2016,
Burkina Faso), he insisted on the need to have universities work with both governments and
the private sector to develop a demand driven higher education that promotes job creation, in
the framework of Public Private Partnership (PPP)
Davidson, P.: Determinants of entrepreneurial intentions. In: Jönköping International Business
School. RENT IX workshop, Piacenza, Italy, 23–24 November 1995
Fayolle, A., Benoît, G.: The impact of entrepreneurship education on entrepreneurial attitudes and
intention: hysteresis and persistence. J. Small Bus. Manage. 53(1), 75–93 (2013). https://
doi.org/10.1111/jsbm.12065
International Labor Organization (ILO). http://laborsta.ilo.org/applv8/data/c3e.html
International Labor Organization (ILO). http://www.ilo.org/global/statistics-and-databases/
meetings-and-events/international-conference-of-labour-statisticians/19/lang–en/index.htm
Kabore, P.F.: Entrepreneurship, youth employment, and violent extremism in Côte d’Ivoire. In:
The Southern Voices Network: Research Paper, vol. 14, 8 p. (2016). https://
africaupclose.wilsoncenter.org/entrepreneurship-youth-employment-and-violent-extremism-
in-west-africa/
Massaquoi, S.B., et al.: The impact of tertiary education institutions on rural agricultural
communities in Sierra Leone. J. Educ. Res. Africa-Revue Africaine de Recherche en Educ.
6, 65–81 (2014)
Ministère de l’Enseignement Supérieur et de la Recherche Scientiﬁque: Annuaire Statistique de
l’Enseignement Supérieur de Côte d’Ivoire, pp. 2013–2014 (2014)
Mintzberg, H.: Managers Not MBAS, 464 p. Berrett-Koehler Publishers, Inc., San Francisco
(2004)
Autio, E.: Entrepreneurial intent among students: testing an intent model in Asia, Scandinavia
and USA (2001)
Sabbi, M., et al.: Pre-tertiary education and self-employable skills in Ghana: the perspective of
teachers, educational researchers and educational administrators. African Education
Development Issues Réseau Ouest et Centre African de Recherche en Educ. 6, 51–76 (2014)
Entrepreneurship, Education and Youth Employment in Africa
149

Capacity Development for Agricultural Innovation
Systems in Burkina Faso: What’s New with CDAIS
Project?
Eveline M. F. W. Compaoré Sawadogo1(✉) and Natewinde Sawadogo2
1 Institute for Environment and Agricultural Research (INERA), National Centre for Scientiﬁc
and Technological Research (CNRST), Ouagadougou, Burkina Faso
compeve@yahoo.fr
2 University Institute for Initial and Continuous Education (UIFIC),
University of Ouagadougou II, Ouagadougou, Burkina Faso
natewinde.sawadogo@yahoo.fr
Abstract. It is believed that potentially the National Innovation System (NIS)
model is a more comprehensive framework for the diﬀusion of innovation when
compared to the integrated approach around cash crops, Training and Visits, and
even to the more recently used approaches such as Farming Systems and Multi-
Actor Platform. Qualitative approach is used to evaluate the diﬀusion of Capacity
Development for Agriculture Innovation Systems (CDAIS) approach, imple‐
mented in Burkina Faso since 2015. The paper argues that, like the NIS approach,
CDAIS approach will fail because of the competing interest of those involved in
its implementation, independently from the technical quality of the model.
Keywords: CDAIS · NIS · Burkina Faso · Innovation diﬀusion models
1
Introduction
This paper argues that the implementation of the NIS policy model for innovation
diﬀusion for socio-economic development in Burkina Faso in the agricultural innovation
systems by CDAIS project was shaped by leading project managers’ limited compe‐
tences. The new tool also had to compete with older more familiar tools. In the end, it
is believed that such a project compare to other projects implemented in Burkina Faso,
will fail to bring about the expected improvements in policy design and practice in the
agricultural innovation systems.
Qualitative methods, including in-depth interviews combined with documentary
review were used to gain new insights into the diﬃculties encountered by the CDAIS
project when the project partners were trying to implement the NIS framework in the
agricultural sector in 8 developing countries. Participation in meetings and interviews
were analyzed against a backdrop of detailed historical studies, based on examining a
large amount of grey literature from the project.
Findings show that the Capacity Development for Agricultural Innovation Systems
(CDAIS) project is important in fulﬁlling the UN’s Sustainable Development Goals;
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 150–155, 2018.
https://doi.org/10.1007/978-3-319-72965-7_14

especially 1 – ending poverty, and 2 – ending hunger, achieving food security and
improved nutrition and the promotion of sustainable agriculture. Innovation in tropical
rural areas is therefore a key to driving sustainable growth and poverty reduction through
socially inclusive agricultural systems. CDAIS project speciﬁcity is that it uses contin‐
uous learning cycles, simple participatory tools to improve the functional capacities for
innovation of innovation actors in Africa, Asia and Central America. In total, eight pilot
countries are taking part to the study. CDAIS project brings together key partners and
actors to address commonly identiﬁed challenges and opportunities in speciﬁc regions
or value chains. Together, international, national and local partners come together to
develop and implement capacity development plans for agricultural innovation. CDAIS
supports the Tropical Agriculture Platform (TAP), which reviews and consolidates
knowledge on agricultural innovation into a global ‘TAP framework’.
This paper is going to give the background of the NIS framework, then its translation
in Burkina Faso, followed by its evaluation compared with the CDAIS strategies.
2
Overview of the Origins of the National Innovation System (NIS)
Framework
In many African countries, such as in Burkina Faso, Strengthening the Agricultural
Innovation Systems has become a must. The paper prefaces the discussion in relation
to such a topic by reﬂecting on an apparent paradox that the project claims for. While
agricultural innovation systems has never been better studied, and understood, many of
the projects in relation to capacity development for agricultural innovation systems have
failed to fundamentally change the institutional and policy setting which constitute the
environment within which innovation systems actors are organized.
Since the Organization for Economic Co-operation and Development (OECD)’s
landmark publication [1], the framework known as National Innovation System (NIS)
has become increasingly popular. It was ﬁrst developed as a theory, concept, and analyt‐
ical perspective used to study the ﬂow of technology and information among people,
enterprises and institutions.
The National Innovation System theory and concept was ﬁrst developed in the 1980s
by Freeman in 1982 and Lundvall in 1985 to explain technological innovation as the
result of the complex interaction between institutions [2]. This conceptual framework
began to be adopted in Science, Technology and Innovation Studies in the 1990s to study
innovation systems [2–5]. Godin [6] has shown that the ‘system approach’ developed
thanks partly to the contribution of the Organization for Economic Co-operation and
Development (OECD) in this area and its very early work from the 1960s. In overall,
the National Innovation System theory stresses that the ﬂows of technology and infor‐
mation among people, enterprises and institutions are key to the innovative process.
Thus, innovation and technology development are the result of a complex set of rela‐
tionships among actors in the system, which includes enterprises, universities and
government research institutes [6]. The inclusion of the country’s research system in
this complex set of relationships is another distinctive element of this approach. Godin
[6: 476–477] notes that
Capacity Development for Agricultural Innovation Systems
151

The National Innovation System framework [theory] suggests that the research system’s ultimate
goal is innovation, and that the system is part of a larger system composed of sectors such as
government, university, and industry and their environment. The framework also emphasized
the relationships between the components or sectors, as the ‘cause’ explaining the performance
of innovation systems.
3
The Emergence of the NIS Framework in Burkina Faso
Burkina Faso, like a number of post-colonialist African states, sought to develop its
economic base, especially in relation to agriculture. It recognized the importance of
science and innovation at an early stage, and sought to boost development using insti‐
tutional methods as well as market-based approaches.
It is only from the early 2000s that the concept of NIS appears clearly in policy circles
and with it a less linear approach to research and innovation. This new development
strategy in policy discourse became explicit and was detailed through the ﬁve years of
the FRSIT/CRDI [7: 1, 8: 1] collaborative research project on “the analysis of the inno‐
vation systems and strengthening of linkages between actors for socio-economic devel‐
opment in Burkina Faso”. The project was introduced into Burkina Faso as a speciﬁc
project of innovation diﬀusion in 2006 and by IDRC/CRDI Canada, who themselves
were inspired by the wider literature on ‘national innovation systems’. The aim1 of the
project was to understand2 innovation development and facilitate access to ﬁnance and
loans. The project [7: 3] included a training workshop on “the analysis of innovation
systems and the strengthening of linkages between actors for the socio-economic devel‐
opment of Burkina Faso”3. Since 2010, the FRSIT which led the NIS project4 has been
integrated into the policy department of the newly created Ministry of Scientiﬁc
Research and Innovation (MRSI), with the eﬀect of providing a policy narrative for the
country’s developing science, technology and innovation policy5. The process of stabi‐
lisation of the NIS diﬀusion tool at niche level, through the learning processes,
1 (1) Identify and understand the needs of the users of the research results (2) organise users and
producers of innovation with respect to their competence (3) access more easily loans from
banks (4) establish partnerships and negotiate contracts and (5) support policymaking deci‐
sions.
2 The project had two phases. First the researchers at the FRSIT were expected to familiarize
themselves with the NIS framework, as a theory, by studying cases of sectoral innovation. The
second phase was to support the translation of the NIS framework, as a policy frame-work, for
socio-economic knowledge-based development.
3 In French: analyse des systèmes d’innovations et renforcement des liens entre les acteurs au
service du développement socio-économique au Burkina Faso.
4 This represents the outcome of the second phase of the project.
5 Plan d’Action Prioritaires de la Politique Nationale de Recherche Scientiﬁque et Technolo‐
gique (PAP-PNRST); Politique Nationale de la Recherche Scientiﬁque et Technologique
(PNRST) Plan d’Action Opérationnel de la Stratégie National de Valorisation des Technolo‐
gies, des Inventions et des Innovations (PAO-SNVTII); Stratégie National de Valorisation des
Technologies, des Inventions et des Innovations (SNVTII). All these plans and strategieswer‐
eadoptedin 2012 and incorporate FRSIT/CRDIactivities and plans.
152
E. M. F. W. Compaoré Sawadogo and N. Sawadogo

experimentation and network building involved in this has failed. Such learning
processes consisted in undertaking training sessions, meetings, symposiums, workshops
and conferences to experiment with and build up the network of relevant actors who
were interested in this. The introduction of the NIS as an innovation diﬀusion tool did
not manage to create a new and suﬃciently favorable socio-technical system capable of
improving the living conditions of the population in Burkina Faso. These ﬁndings
emerged from a detailed study of the country-speciﬁc process of the appropriation of
the NIS at both the strategic policy design level and the operational level through the
case of Biotechnology cotton innovation system. The success of the NIS would be a
matter of completely uprooting the old successive approaches in order to settle on a
totally new one. Such a reality has contributed to making the diﬀusion of the NIS tool
for innovation diﬀusion unsuccessful, because of the lack of attention to the type of
implementation actions needed to embed and develop this approach.
4
CDAIS Project in Burkina Faso
In 2015, a project has emerged to strengthen the capacity development for agricultural
innovation systems. It aims at strengthening the capacity for agricultural innovation
systems by improving the capacities to navigate in complexity, to collaborate, toreﬂect
and learn, and to engage in strategic and political processes. Thus, in its implementation
in Burkina Faso, the content of these key words was not explicit so that one knows how
an ideal AIS operates. Indeed, so far, one cannot see the extent to which the AIS (Agri‐
cultural Innovation Systems) framework is being implemented, even though challenges
are identiﬁed and solutions are being built together with farmers, agri-business and
consumers, but without referring to the AIS framework. What is diﬃcult for the manage‐
ment of such a project is to know what to consider concretely as a functioning AIS. The
project leaders need to know clearly what the speciﬁc variables are. They need to be
able to clearly identify the standards indicators of the AIS theory. In my point of view
this is capital. Thus, by knowing the AIS theory and the concrete standard indicators,
one can see to what extent is a case study, a speciﬁc AIS works in practice and how it
should have worked in light with the AIS theory. This is still a big challenge in Burkina
Faso to make such shift because it is hard for innovation actors who have been in the
old approaches to see and really understand the new way of doing things that CDAIS
project is trying to promote. Up to now, the project has not shown a clear strategy that
is inspired from the AIS framework. Instead, due to limited knowledge of project
management team at national level on the AIS framework, CDAIS is struggling to
develop the capacity for a successful AIS. As can be seen CDAIS project up to now
resemble more to the multi-actor platform that has proven already its limits in Burkina
Faso. Based on Compaoré’s [9] study of the NIS in Burkina Faso, one of the possible
questions would be whether technically the NIS tool for innovation diﬀusion is capable
of being more eﬀective improving capacities than the previous ones such as farming
systems, the multi-actor platform etc. As with Compaoré’s [9: 326] ﬁndings, we are
inclined to support an aﬃrmative view for several reasons: “Indeed, I believe that
potentially, the NIS tool is more comprehensive for the diﬀusion of innovation when
Capacity Development for Agricultural Innovation Systems
153

compared to the integrated approach around cash crops, Training and Visits, and even
to the more recently used approaches such as Farming Systems and Multi-Actor Plat‐
form”.
5
Discussion and Conclusion
So far, CDAIS project in Burkina Faso has not proved to have deeper understanding
about AIS framework and a NIS framework. Instead, it relies on old approaches where
it is hard for one to understand the extent to which the project is promoting AIS as a best
tool for successful innovation diﬀusion.
More specifically, the NIS is an innovation diffusion tool in which the role of the
government is prominently stated and tested. Many of the innovation system
scholars made the role of government in innovation dynamics explicit [4]. For
instance, the acquisition of technology, its use and diffusion involve the govern‐
ment whose role in the system is essential. It is clearly set out there that through
policymakers and government play a key role by making choices which inform strat‐
egies at national level [10]. The government is considered as a lead actor which
influences through policy and institutional design. In addition, governance influ‐
ences Foreign Direct Investment (FDI) [11: 2] because, “despite their limited size,
FDI inflows have had a positive impact, in as much as they generated employment
in the formal sector and generated local value-addition”. On the other hand, such a
flow is supported within the framework of government Intellectual Property Right
(IPR) regulation; by designing the IPR system for the sake of business and govern‐
ment direct scientific research such as financing universities, supporting business
R&D [12: 23], “the Government has a responsibility to contribute to the formation
of the human and social capital needed to evaluate, choose, implement and modify
foreign technologies”.
The commitment of the government to provide actors with such a context is thus
required in order to produce a favourable social and political context for the networking
of ﬁrms and knowledge producers. None of the previous approaches has made this
explicit as is done in the NIS policy tool. The wording in the Farming Systems and Multi-
Actor Platform approaches should not mislead one as to their consideration of this macro
level aspect. In neither case is there a comparable documentation of the working of the
State within the system as there is in the NIS framework. An application of the NIS tool
for innovation diﬀusion would have at least resulted in the creation of institutions and
other resources for which the government is the best actor. While the Multi-Actor Plat‐
form approach is mentioned in Burkina Faso innovation policy, the reality is that this
approach unlike the NIS tool has not tested and approved the standards and mechanisms
of the best ways for government intervention in the innovation system. The CDAIS
project has not demonstrated otherwise.
An application of the NIS would have at least sensitized the government and other
relevant actors to the need for building research infrastructures; because the NIS policy
tool for innovation diﬀusion has demonstrated that domestic resources are key to
sustainable successful innovation diﬀusion.
154
E. M. F. W. Compaoré Sawadogo and N. Sawadogo

References
1. OECD: Managing National Innovation Systems. OECD, Paris (1999)
2. Lundvall, B.-A.: National innovation systems-analytical concept and development tool. Ind.
Innov. 14(1), 95–119 (2007)
3. Nelson, R.R.: National Innovation Systems: A Comparative Analysis. University Press,
Oxford (1993)
4. Freeman, C.: The ‘National System of Innovation’ in historical perspective. Camb. J. Econ.
19(1), 5–24 (1995)
5. Lundvall, B.-A.: National Systems of Innovation: Towards a Theory of Innovation and
Interactive Learning. Pinter, London (1992)
6. Godin, B.: National innovation system the system approach in historical perspective. Sci.
Technol. Hum. Values 34(4), 476–550 (2009)
7. FRSIT/CRDI: Compte rendu de l’Atelier de Lancement du Projet FRSIT/CRDI: Analyse des
Acteurs du System d’Innovation et Renforcement des liens entre les Acteurs au Service du
Développement Socio-Economique au Burkina Faso, Ouagadougou (2008)
8. FRSIT/CRDI: Analyse des Acteurs du System d’Innovation et Renforcement des liens entre
les Acteurs au Service du Développement Socio-Economique au Burkina Faso, Atelier
National sur l’Etat des Lieux de la situation des Acteurs du System d’Innovations, Kombissiri,
du 24 au 27 Juin 2009 (2009)
9. Compaoré, M.F.W.E.: The role of the National Innovation Systems Framework in facilitating
socio-economic development in Burkina Faso: model and policy practice. Ph.D. thesis,
University of Nottingham (2015)
10. Freeman, C.: Technology Policy and Economic Performance: Lessons from Japan. Pinter,
London (1987)
11. Undp, A.: The human development index—a new development indicator? Intereconomics
26(5), 236–243 (1991)
12. Feinson, S.: National innovation systems, overview and country cases. In: Knowledge Flows,
Innovation and Learning in Developing Countries, vol. 13, p. 38. OECD, Paris (2003)
Capacity Development for Agricultural Innovation Systems
155

Mechanisms for Strengthening Evidence-Based Policy
and Practice: A Review
Rose Omari
(✉)
Science and Technology Policy Research Institute, Council for Scientiﬁc and Industrial Research,
P.O. Box CT 519, Cantonments, Accra, Ghana
romari@csir-stepri.org, rose.omari@yahoo.com
Abstract. Evidence-based policymaking helps people make well informed deci‐
sions about policies, programmes and projects by putting the best available
evidence from research at the heart of policy development and implementation.
Research has the potential to inﬂuence policy at any stage of the policy cycle.
However, many factors limit evidence-based decision-making both at individual
and organisational levels. Nevertheless, it is imperative not only for policymakers,
but also for researchers, to improve the availability and dissemination of sound
research. Fundamentally, there needs to be increased communication and inter‐
action between the research and policy worlds to strengthen the integration of
policy and evidence. This will be achieved by setting up mechanisms which will
facilitate greater use of evidence by policymakers. This paper reviews the
strengths and weaknesses of some research-policy bridging models and draws
lessons for advancing the quest to bridge research-policy gap particularly in the
science, technology and innovation, and agricultural sectors.
Keywords: Evidence-based policy · Evidence-based practice
Research-policy gap · Research dissemination · Bridging models
1
Introduction
Evidence-based policymaking is an approach that helps people make well informed
decisions about policies, programmes and projects by putting the best available evidence
from research at the heart of policy development and implementation. It uses the best
available research and information on program results to guide decisions at all stages of
the policy process and in each branch of government. It identiﬁes what works, highlights
gaps where evidence of program eﬀectiveness is lacking, enables policymakers to use
evidence in budget and policy decisions, and relies on systems to monitor implementa‐
tion and measure key outcomes, and using the information to continually improve
program performance.
The word “policy” is not a tightly deﬁned concept but a highly ﬂexible one, used in
diverse ways on various occasions. Webster’s dictionary has several closely related
deﬁnitions. They are:
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 156–161, 2018.
https://doi.org/10.1007/978-3-319-72965-7_15

• A deﬁnite course or method of action selected (by government, institution, group or
individual) from among alternatives and in the light of given conditions to guide and,
usually, to determine present and future decisions.
• A speciﬁc decision or set of decisions designed to carry out such a course of action.
• Such a speciﬁc decision or set of decisions together with the related actions designed
to implement them.
• A projected programme consisting of desired objectives and the means to achieve
them.
In English usage, policies are “made” and “implemented” in the same way that deci‐
sions are made and implemented. Yet it is possible to have policies that are not or cannot
be implemented, so that, conceptually, actions that implement policies need not neces‐
sarily be part of policy itself [1]. A policy is a set of coherent decisions with a common
long-term purpose. Government policies are often supported by special legislation.
Policies are usually national policies (not district or provincial) and are not normally
limited in time [1].
In fact, policy and practice, which are based on research evidence, are seen to produce
better outcomes, e.g. saving lives and improving development performance. However,
policy development and implementation are still often weakly informed by research
evidence thus creating a wide gap between research and policy. On the one hand,
research aims to investigate, learn and produce knowledge by gathering information,
contemplation, trial, and/or synthesis. In development context, that may involve action-
research or academic study ranging, as examples, from a pilot project, to a laboratory
experiment, a consultation exercise, a quantitative survey, a literature review, participant
observation or a participatory evaluation. It might be led by beneﬁciaries, development
practitioners or academics from scientiﬁc and social science disciplines. On the other
hand, policy aims for continuity or change of a practice, including plans and their evolu‐
tion when put into practice (that is, the ‘how’ as well as the ‘what’ of decisions [2].
The objective of this review is to understand mechanisms for bridging research-
policy gap so as to identify eﬀective ways of enhancing research-policy and research-
practice linkages particularly in the science, technology and innovation (STI) and agri‐
cultural sectors. It has been established that public research institutions and their inter‐
actions with policy makers and users of research results play a signiﬁcant role in the
creation and diﬀusion of knowledge in any system of innovation [3, 4]. The research
and development (R&D) institutions are expected to provide the structured application
of STI to boost the competitiveness of STI, agricultural and other economic sectors.
Therefore, deploying R&D is critical for raising agricultural productivity and value
chain development for improved socio-economic development especially in Africa. STI
and agricultural research and development are the driving forces behind the industrial
and agricultural revolutions that have helped to transform the economies of developed
and some emerging countries such as Brazil, China, India and Thailand. However, this
has not been the case in Ghana because a wide gap still exists between research and
policy and research and practice. Bridging the gap between the national research system
and policy and practice has become crucial in this globally competitive era. Research-
policy and research-practice linkages allow for exchanges that enhance understanding
of the technological needs among local industries, and capitalise on innovative options
Mechanisms for Strengthening Evidence-Based Policy and Practice
157

to harness and exploit local research outputs for business solutions. In addition, such
interactions encourage research and innovation in areas of relevance for the STI and
agricultural sectors and private sector growth.
2
Why Bridge Research-Policy Gap?
The uptake of research evidence in the policy making process has become the front
burner of global discourses on approaches and strategies for development. It is therefore
not surprising that international development agencies and other research funders are
placing increasing emphasis on the need to communicate research evidence to policy
makers. This has resulted in a ﬂurry of activities aimed at supporting the communication
of research evidence to policy makers. For example, a study commissioned by UNESCO
in Tanzania in 2002 assessed the research–policy linkages of science-related ministries
and their research organizations with the objective to understand mechanisms for inter‐
acting with policymakers and users of research outputs. In the agricultural sector in
Ghana, the USAID Agriculture Policy Support Project is being implemented with the
purpose of increasing the capacity of the Government of Ghana (GOG), the private
sector, and Civil Society Organizations to implement evidence-based policy formation,
implementation, research, and advocacy, and perform rigorous monitoring and evalua‐
tion of agricultural programs implemented under the Medium-Term Agriculture Sector
Investment Plan (METASIP).
Furthermore, several international development organisations have research
programmes aimed at understanding the links between research and policy. An example
is the Global Development Network that recently started a three-year international
research programme to explore research-policy linkages.
While eﬀorts are being made to devise mechanisms for bridging research-policy gap,
it should be noted that policymaking is inherently a political process. Hence, many
factors jostle with evidence to take centre stage in policy formation both at an individual
level and at an organisational level. For example, time constraints will aﬀect the mech‐
anisms available to mobilize evidence – urgent issues require diﬀerent approaches than
processes to develop strategic policy directions. Thus, clearly, the onus lies not only
with policymakers, but also with researchers, to improve the availability and dissemi‐
nation of sound research to inﬂuence policy.
3
Models for Bridging Research-Policy Gap
To overcome the stumbling block for linking evidence into policy, innovative models
are needed. This paper reviews some models taking a cue from Weiss [3] who proposed
seven models of research-policy linkage, which have been adapted by writers such as
Nutley et al. [4], Nutley and Webb [5], and Young et al. [6]. Other authors have adopted
a simpler framework contrasting the two ‘ideal types’ of research utilisation: the engi‐
neering model and the enlightenment model [5, 7]. Landry et al. [8] propose a diﬀerent
approach involving four models. While these models have their strengths and weak‐
nesses, Jones and Seelig [9] distinguished three broad models namely engineering,
158
R. Omari

engagement and enlightenment models. These three models present alternative concep‐
tions both of how research actually links to policy and of how it should link that is, they
are both explanatory and normative models. A brief description of each the of the three
models is provided below.
The engineering model of research-policy relations encompasses the ‘knowledge-
driven’ and ‘problem-solving’ models in Weiss’s [3] typology, and the ‘technological’
model in the formulation by Landry et al. [8]. In this model, the link between research
and policy is essentially linear: ‘a problem exists; information or understanding is
lacking either to generate a solution to the problem or to select among alternative solu‐
tions; research provides the missing knowledge; and a solution is reached’ [7]. The
purpose of research is primarily to assist in solving policy problems by providing rele‐
vant empirical evidence and conclusions [3]. The deﬁnition of the policy problem is
mainly the responsibility of the policy or decision-maker. The assumption is that deci‐
sion makers have a clear idea of their goals and their information needs, and they engage
scientists to provide data, analysis and interpretation of research ﬁndings. In the engi‐
neering model, the focus is on applied research, that is, the research is driven primarily
by the needs of the intended users, and is centred on a speciﬁc problem or set of problems.
In this model, the role of the researchers is primarily technical, that is, providing the
evidence and conclusions to help solve a policy problem. The policy-maker commis‐
sions the research to ﬁll knowledge gaps and is the end-user of research ﬁndings. Thus,
the relations between researchers and policy-makers are often contractual. The model
demonstrates clearly how policy-makers seek ‘answers’ from research for the develop‐
ment of evidence-based policy however, it is widely criticised as simplistic and wildly
optimistic [9].
The engagement model of research-policy relations encompasses the ‘interactive’,
‘political’ and ‘tactical’ models in Weiss’s typology. In this model, the linkages between
researchers and policy-makers are portrayed as interactive, complex and multi-dimen‐
sional. In this model, the purpose of research is to bring the distinctive knowledge, skills
and values to bear on policy issues, through ongoing engagement and interaction of
researchers and policy-makers. The type of research can be basic or applied, but is
characterised above all by its commitment to policy-relevance. The engagement model
is inherently political hence researchers need to understand and take account of this
political environment. This model clearly demonstrates that research can play a key role
in policy development, but this is contingent on many factors and circumstances,
including the political skills of researchers who can themselves sometimes become
inﬂuential ﬁgures [4]. Thus, policy-makers interact with researchers out of a commit‐
ment to research-informed policy. While researchers and policy makers have distinctive
roles and positions in policy processes, their relations are often characterised by collab‐
oration and partnership, and moderate to high levels of consensus on policy goals.
Researchers also seek to develop links with interest groups and the media, as these
groups are important in bringing research ﬁndings to the attention of policy-makers [10].
Researchers need to be both committed to the values and methods of research and
capable of engaging eﬀectively in the world of policy and politics. Policy makers need
to be not only responsive to the political environment but also receptive and open to the
ﬁndings and implications of policy research [9]. The major criticism of this model is the
Mechanisms for Strengthening Evidence-Based Policy and Practice
159

dangers of the politicisation of research and the development of somewhat complacent
‘policy communities’ comprising researchers and policy-makers of similar views.
The enlightenment model encompasses the ‘enlightenment’ and ‘intellectual enter‐
prise’ models in Weiss’s typology, and reﬂects the longstanding liberal-democratic
tradition that emphasises the importance of the independence of academic research [11].
In this model of research-policy linkages, relations between researchers and policy-
makers are indirect, and research is undertaken for the beneﬁt not of policy-makers as
such but of the entire society. Research tends to be driven by the theoretical and concep‐
tual framework of academic disciplines rather than by speciﬁc policy questions.
Research provides the ‘intellectual background of concepts, orientations and empirical
generalisations that inform policy’ [7]. Proponents point to evidence suggesting that
policy-makers often welcome research that challenges prevailing frames of reference
and makes them rethink comfortable assumptions [7]. However, the model pays little
attention to the processes linking research and policy. It suggests no strategies for
ensuring that the ﬁndings of scientiﬁc research are utilised by decision-makers.
4
Other Means of Facilitating Research-Policy Linkages
The models described earlier all lead to the generation of knowledge or evidence.
However, the way the evidence is presented matters when policy needs to be influ‐
enced. An understanding of how several types of research evidence make their way
to policy makers would make communications strategies far more effective. Berkout
and Scoones [12], identifies two processes: ‘snowballs’ (the accumulation of
research impacts within policy elites) and ‘whispers’ (the reinterpretation of research
findings in broader constituencies). Saywell and Cotton [13] have described the
process in terms of the limestone model (information trickles like water through
porous rock), the gadfly model (information gets through because dissemination is
prioritised as much as research itself), and insider model (researchers exploit links
with policy-makers). The ‘limestone’ model is essentially passive requiring nothing
more of the researcher than to conduct the research and present findings in a read‐
able way. It is hoped that the findings will gradually seep into the consciousness of
the public and decision-makers. The ‘gadfly’ model involves sporadic, but enthusi‐
astic participation in policy processes, based on a strong commitment to policy and
social change. The ‘insider’ model involves close, continuous engagement with
policy processes, and identification with the goals and needs of decision-makers.
5
Conclusion
No single model may be adequate in eﬀectively bridging research-policy gap hence it
may be prudent for researchers to consider which model or combination of models
represents their stance with respect to policy processes. Whatever approach is adopted,
there is a need to acknowledge the complexity of the policy process: it often takes time
and patience and multiple messages conveyed through multiple channels before science
has an impact.
160
R. Omari

References
1. ILRI (International Livestock Research Institute): Livestock Policy Analysis. ILRI Training
Manual 2. ILRI, Nairobi, Kenya, p. 264 (1995). http://www.fao.org/wairdocs/ilri/x5547e/
x5547e00.htm#Contents
2. Shankland, A.: Analysing policy for sustainable livelihoods. Research Report 49. Institute of
Development Studies, Brighton, Sussex (2000)
3. Weiss, C.H.: The many meanings of research utilization. Public Adm. Rev. 39(5), 426–431
(1979)
4. Nutley, S., Walter, I., Davies, H.: From knowing to doing: a framework for understanding
the evidence-into-practice agenda. Discussion Paper 1, Research Unit for Research Utilisation
University of St Andrews, Fife (2000)
5. Nutley, S., Webb, J.: Evidence and the policy process. In: Davies, H., Nutley, S., Smith, P.
(eds.) What Works? Evidence-based Policy and Practice in Public Services. The Policy Press,
Bristol (2000)
6. Young, K., Ashby, D., Boaz, A., Grayson, L.: Social science and the evidence based policy
movement. Soc. Policy Soc. 1(3), 215–224 (2002)
7. Bulmer, M.: The Uses of Social Research - Social Investigation in Public Policy-Making.
Contemporary Social Research Series. George Allen & Unwin, London (1982)
8. Landry, R., Amara, N., Lamari, M.: Climbing the Ladder of Research Utilisation: Evidence
from Social Science Research. Society for Social Studies of Science, San Diego (1999)
9. Jones, A., Seelig, T.: Understanding and enhancing research-policy linkages in Australian
housing: a discussion paper. AHURI Positioning Paper No. 75, Australian Housing and Urban
Research Institute Limited, Melbourne (2004). https://www.ahuri.edu.au/research/position-
papers/75
10. Weiss, C.H.: Research and policy-making: a limited partnership. In: Heller, F. (ed.) The Use
and Abuse of Social Science. Sage Publications, London (1986)
11. Hammersley, M.: The Sky is Never Blue for Modernisers: The Threat Posed by David
Blunkett’s Oﬀer of ‘Partnership’ to Social Science. British Educational Research Association
(2000). http://www.bera.ac.uk
12. Berkout, F., Scoones, I.: Knowing how to change, environmental policy learning and transfer.
Dev. Res. Insights 30, 1–2 (1999)
13. Saywell, D., Cotton, A.: Spreading the Word: Practical Guidelines for Research
Dissemination Strategies. Water, Engineering and Development Centre, Loughborough, UK
(1999)
Mechanisms for Strengthening Evidence-Based Policy and Practice
161

Healthcare

Evaluation of a Cloud Based Health Information System
Priscilla Ajayi, Nicholas Omoregbe, Sanjay Misra
(✉), and Davies Adeloye
Covenant University, Ota, Nigeria
{priscilla.ajayi,nicholas.omoregbe,sanjay.misra,
davies.adeloye}@covenantuniversity.edu.ng
Abstract. In ensuring adequate antenatal and postnatal care which has been a
major challenge in Africa and indeed Nigeria, there has been a need to evaluate
and profound a comprehensive Health Information System (HIS) required for
reducing Maternal Mortality Ratio (MMR) in the continent of Africa. There is a
need for promptness and eﬃcient healthcare for mothers to stem infant mortality
ratios while giving the mothers a chance to life in diﬃcult circumstances during
child delivery. In some health care facilities in Nigeria the presence of HIS is
obvious but it has not fully achieved the aim by which it was implemented due
to lack of evaluation of the application. Hence this presentation tends to evaluate
a usability of Health Information System using Cloud Platform for Antenatal and
Postnatal Clinic in Nigeria to assess its suitability and sustainability in healthcare
services towards improving maternal health. In the usability evaluation of the
cloud based HIS, some factors were considered. These factors include: How
simple it is to navigating the HIS. How simple users understand and use the HIS.
How easy is it for users to reuse the HIS after a period of non-usage. If the HIS
features are well structured. How easy is it for a user to use the application to
satisfaction within a short period. Similarity of the layout when navigating
diﬀerent pages and functions on the HIS. Satisfaction of the users of HIS on its
basic features and prompt error messages as required during usage. The extents
to which the HIS links are been descriptive and self-informing.
Keywords: Antenatal · Postnatal · Health Information System
Cloud computing
1
Introduction
Evaluation needs to be conducted on a completed application to check if the application
meets the need for which it was deployed. Evaluation can be conducted on criteria based
approach or tutorial-based approach. The criteria-based approach gives a measure of
products quality in terms of its sustainability, maintainability, and usability. Tutorial-
based approach provides a pragmatic evaluation of the usability of existing software to
check for its adaptability to an existing requirement [1].
In ISO 9241-11 standard usability is deﬁned in terms of eﬀectiveness, eﬃciency and
satisfaction in a particular context of use. The intention was to emphasize that usability
is an outcome of interaction rather than a property of a product [2]. Usability is being
able to measure the extent to which a product can be used by the speciﬁed users to
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 165–176, 2018.
https://doi.org/10.1007/978-3-319-72965-7_16

achieve speciﬁc goals for which system was created to ascertain its eﬀectiveness, eﬃ‐
ciency and satisfaction [3].
For the purpose of this presentation, a criteria-based approach was adopted to ascer‐
tain the usability or otherwise of the Cloud Based HIS in the Nigeria healthcare facility.
It is on record that Maternal Mortality Ratio (MMR) has been above 800 per 100,000
live births this is the condition over the span of last two decades in Nigeria [4, 5]. One
of the diﬃculties in Nigeria and numerous Africa nations identiﬁes with the quality and
immediacy of antenatal and/or post-natal care given [5, 7]. For instance, results of a
survey of 452 pregnant women accessing antenatal services in southwest Nigeria uncov‐
ered that the waiting time before antenatal consultation was 131.1 min, and around 66%
of women were unhappy about their involvement in decision-making with respect to
birth planning and postpartum contraception [5]. Although, general survey indicated
irregularities in quality of services rendered [7]. However recent advancements in tech‐
nology, notably HIS, have helped in the response to a large portion of these diﬃculties
in health care, particularly in view of evidence from high income settings [8].
HIS application is in use in some healthcare facilities in Nigeria, but it has not been
eﬀective enough for record keeping and interoperability [9]. This has been so because
of lack of evaluation of the existing HIS application.
The paper is organized as follows. In the next section, we will present the framework
of the HIS. In Sect. 3 the usability factors of the Cloud Based HIS will be considered.
While Sect. 4 will deal with the questionnaire which is divided into two sections: The
ﬁrst section shows the group of Participators (Health Care Practitioners and Patients)
and their experience. The second section shows the Participators’ perception of the HIS
based on each of the usability factors earlier highlighted. Finally the conclusion drawn
is in Sect. 5.
2
Framework of the HIS
Recent advancements in technology, notably HIS, have helped in the response to many
of the challenges in health care, particularly in light of conﬁrmation from high income
settings [8]. Notwithstanding, numerous African nations do not have a sorted out HIS
at all levels of care. In most hospitals with the exception of some faith-based ones,
individual patients are saddled with the obligation of keeping health records in the form
of booklets or vaccination cards [10]. Patient’s information in most health facilities
across Nigeria are stored manually [11]. The few facilities using electronic storage have
the data exclusively residing with them and not available electronically to other health
facilities within Nigeria [7]. This has prompted to endless loss of material assets and
non-utilization of valuable manpower. Amid crises, medical histories are not promptly
available, with this often resulting in wrong treatment plans [10]. Numerous researchers
and health workers have thought widely on practical ways to enhance access to real time
patient data at the point of care in Nigerian hospitals, particularly in the quest to
encourage speedy analysis and treatment of cases [12]. Due to challenges highlighted,
the HIS application has been developed. The System Architecture of the developed
application is as represented in Fig. 1.
166
P. Ajayi et al.

Fig. 1. System architecture of the antenatal/postnatal clinic system
In Fig. 1, the Consultant/Record Oﬃcer queries or adds data to the Local Database
which inadvertently communicates with the Central Database as controlled by the Chief
Medical Oﬃcer. An algorithm is used to categorize the data. After which the result is
displayed. Also, the Administrator (Chief Medical Oﬃcer) is in charge of updating the
Central Database but can delegate the responsibility. When data is sent to the database,
it ﬁrst passes through the staging area where it is cleaned before being stored in the data
warehouse. Cleaning a data ensures that the data is in the right format before being sent
to the database. This is to ensure data consistency.
3
Usability Attributes of the Cloud Based HIS
The major attributes of usability involves measuring the eﬀectiveness of the interface
between the users and the application. It means that the application is easier to learn and
made use by the patients [13].
In order to evaluate the usability of the Cloud Based HIS, the following usability
factors were considered [1, 2, 14, 15]:
3.1
Simplicity
This looks at the ease of users to and use the HIS. It also looks at how applicable the
software is for its intended purpose. The application should be easily browsed without
complications.
Evaluation of a Cloud Based Health Information System
167

3.2
Completeness
It checks if the users are contented with the basic features of the HIS and errors messages
been prompted when required.
3.3
Navigation
The HIS should have its most important features on the dashboard of the application for
easy identiﬁcation by users. This would make the application easy to navigate.
3.4
Satisfaction
It shows how satisﬁes the users are in the overall functions of the application and how
easy it is for users to complete given task without having too much steps.
3.5
Hypertext Structure
The information about the HIS features must be structured appropriately to ensure that
there are active links to various HIS functions and features needed by the users.
3.6
Memorability
When users return to the design page(s), they should be able to easily re-establish a
proﬁcient level of understanding after a period of considerable absence.
3.7
Self-evidence
The links to important information and functions are easily understood by the users, also
these links are easily seen and very descriptive to the users.
3.8
Consistency
The design, layout and graphics of the application remains the same when navigating
diﬀerent pages on the application. This feature is to help the user understand the layout
of the HIS faster.
4
Questionnaire
The questionnaire is divided into two sections. The ﬁrst section shows the group of the
Participators (Health Practitioners and Patients) and their experience. The second section
shows the Participators’ perception of the HIS based on each of the usability factors
earlier highlighted. The Participators were asked to indicate their level of agreement
with each question. Web enabled devices was used to administer the HIS to each Partic‐
ipators. The Participators were allowed to use the HIS without intervention of the
168
P. Ajayi et al.

administrator excepts Participators needs to be reinstructed due to lack of concentration.
The questionnaires were administered immediately after each task to improve the accu‐
racy. All data were collected using a ﬁve point scale from “1”, as “Strongly Disagree”
to “5” as “Strongly Agree”.
4.1
Participators
HIS is limited in scope to health care facilities, practitioners and patients. In this case,
the healthcare facility within Covenant University, known as Covenant University
Medical Center. 49 persons participated in the usability study of this work. This number
is prescribed as required for such study [3, 16]. The Participators comprise of health
practitioners and patients in Nigeria.
4.2
Data Analysis
The statistics showing the percentage of Participators in the use of computer software
is given in Table 1. It reveals that the computer software is more engaged by health
practitioners than the patients due to the functionality of the applications which is more
tuned to the health care practitioners.
Table 1. Skill of Participators in the use of software
No of participators Health practitioner
Patients
Participators type
49
79.59%
20.41%
By interpretation, not all of the respondents are experts in the use of modern tech‐
nology (Table 2).
Table 2. Shows diﬀerent levels of computer literacy that Participators occupy.
No of participators
Novice
Average
Good
Expert
Computer literacy
49
18%
32.65%
30.61%
18.37%
In addition, most of the respondents use a laptop to enhance their work (Tables 3
and 4).
Table 3. Devices used to access the HIS
No of participators
Laptop
Desktop
Mobile device
Devices used
49
44.9%
36.73%
18.37%
Table 4. Shows the diﬀerent Tasks carried out on the HIS.
No of
participators
Patients
registration
Seminar
attendant
Appointment
booking
Reports
generation
Doctors
attending to
patients
Task carried out
49
40.82%
12.24%
30.61%
100%
20.41%
Evaluation of a Cloud Based Health Information System
169

4.3
Statistical Package for Social Sciences (SPSS)
For all the Participators, an overall score was computed for each of the usability dimen‐
sion by averaging all the ratings on the questionnaire that was used. The Statistical
Package for Social Sciences (SPSS) was used to generate the frequency distribution,
mean and standard deviations and all the relevant charts for all the ratings (Table 5).
Table 5. Descriptive statistical analysis of questionnaire data
Usability attributes
Mean rating
Standard deviation
Variance
Simplicity
4.20
0.54
0.30
Completeness
4.09
0.49
0.24
Navigation
4.20
0.53
0.28
Satisfaction
4.03
0.73
0.54
Hypertext structure
4.16
0.52
0.28
Memorability
4.20
0.53
0.28
Self-evidence
4.20
0.53
0.28
Completeness
4.09
0.49
0.24
4.4
Reliability Test
The reliability estimates have been calculated from the data bank. The reliability and
convergent validity by Cronbach’s alpha estimate produced a result of 0.738 which is
above 0.7 recommended [17]. This is an indication of the questionnaire’s reliability. The
maximum deﬁnition of Cronbach’s Alpha coeﬃcient theoretical is 1.0. The reliability
statistics and the Cronbach’s alpha value are shown in Tables 6 and 7 respectively.
4.5
Reliability Statistics
Table 6. SPSS test cases
N
%
Valid
49
100
Excluded(a)
0
0
Total
49
49
Table 7. Cronbach’s alpha
Cronbach’s alpha
N of items
.738
8
4.6
Discussion
The score of the usability attributes as gathered from the respondents is talked about as
follows:
170
P. Ajayi et al.

4.6.1
Simplicity
The mean rating for “simplicity” was 4.03. This demonstrates the users found the HIS
simple to utilize and get it. The frequency graph is as shown in Fig. 2.
Fig. 2. Simplicity analysis
4.6.2
Completeness
The mean rating for “Completeness” was 4.09. This shows most of the users were happy
with the fundamental features of the HIS. This is shown in Fig. 3.
Fig. 3. Completeness analysis
Evaluation of a Cloud Based Health Information System
171

4.6.3
Navigation
The rating for “Navigation” was 4.20 which shoes that, most of the users did not have
challenges with navigating the HIS. This is shown in Fig. 4.
Fig. 4. Navigation analysis
4.6.4
Satisfaction
Most of the respondents were of the opinion that the HIS required few steps to complete
any task thereby saving time. The mean rating was 4.03 and this is shown in Fig. 5.
Fig. 5. Satisfaction analysis
The mean rating was 4.03 and most of the users were of the supposition that the HIS
required few stages to ﬁnish any errand consequently saving time. This is appeared in
Fig. 5.
172
P. Ajayi et al.

4.6.5
Hypertext Structure
The mean rating of 4.16 for “Hypertext Structure” demonstrates that the vast majority
of the users observed the HIS to be well organized and that there were dynamic
connections to the diﬀerent HIS capacities and components. This is shown in Fig. 6.
Fig. 6. Hypertext structure analysis
4.6.6
Memorability
Memorability is a property that could be impacted by the recurrence of visits to the HIS.
It was measured by requesting that the users return to the HIS and attempt to review
how to play out the fundamental tasks in the HIS following a time of 7 days. Its mean
rating of 4.20 is appeared in Fig. 7.
Fig. 7. Memorability analysis
4.6.7
Self-evidence
This demonstrates the HIS was found to contain tabs and connections to essential func‐
tions. Such tabs, buttons and interfaces on the HIS were self educating. This is appeared
in Fig. 8 with a mean rating of 4.20.
Evaluation of a Cloud Based Health Information System
173

Fig. 8. Self evidence analysis
4.6.8
Consistency
Some of the users were of the conclusion that the HIS had a decent format that was
reliable as they explored from one page to the next. The mean rating is given as 4.16.
This is as appeared in Fig. 9.
Fig. 9. Consistency analysis
Numerous usability studies suggest that system with “Good Usability” should have
a mean rating of 4 on a 1–5 scale and 5.6 on a 1–7 scale [17]. We adopted the approach
of a 1–5 scale, and conclude that the HIS had a “Good Usability” on mobile devices
based on the following mean ratings of the given usability attributes, shown in Table 8.
174
P. Ajayi et al.

Table 8. Usability attribute ratings
Usability attributes
Mean rating
Simplicity
4.20
Completeness
4.09
Navigation
4.20
Satisfaction
4.03
Hypertext structure
4.16
Memorability
4.20
Self-evidence
4.20
Consistency
4.16
5
Conclusion
This research presentation tends to evaluate a Cloud Based Health Information System
for Antenatal and Postnatal Clinic in Nigeria, assessing its suitability and sustainability
in healthcare services towards improving maternal health in Nigeria and Africa as a
whole. It also highlights the importance of a detailed and secured health information
system as a foundation to addressing perennial maternal health issues in Nigeria and
many African countries. Hence, this research has implemented key concepts such as
web services for easy integration, programing Radio-frequency identiﬁcation (RFID)
cards for identiﬁcation and easy retrieval of patients’ records, authentication of patient’s
records through biometric devices in other to ensure a secured system free from external
factors. The unique advantage of this evaluated Cloud Based HIS is its adaptability as
a unique software able to integrate seamlessly into an existing e-solutions. Working as
software may also help it in supporting primary health care centers that might not be
able to aﬀord paying for a fully developed application. This research is not without
foreseeable limitation, notably the unavailability of adequate computer systems, occa‐
sional power outages, inadequate servers and lack of systems connectivity in many local
hospitals. However, with proper implementation plans, research inputs, technical
support, sensitization, availability of funds, and government support, the overall goal of
improving maternal health may still be within achievable limits.
References
1. Jackson, M., Crouch, S., Baxter, R.: Software evaluation: tutorial-based and critical-based
assessment software sustainability institute, November 2011. https://www.software.ac.uk/
sites/default/ﬁles/SSI-SoftwareEvaluationCriteria.pdf
2. Bevan, N., Carter, J., Harker, S.: What have we learnt about usability since 1998? In:
HumaComputer Interaction, Part 1, HCII, pp. 143–151 (2015)
3. Adewunmi, W.: Deployment and usability evaluation of mobile access to institutional
repository (M.Sc), Covenant University (2012)
4. Fagbamigbe, A.F., Idemudia, E.S.: Assessment of quality of antenatal care services in
Nigeria: evidence from a population-based survey. Reprod. Health 12, 88 (2015)
Evaluation of a Cloud Based Health Information System
175

5. Oladapo, O.T., Iyaniwura, C.A., Sule-Odu, A.O.: Quality of antenatal services at the primary
care level in southwest Nigeria. Afr. J. Reprod. Health 12, 71–92 (2008)
6. World Bank: Maternal mortality ratio, 19 January 2016. http://data.worldbank.org/indicator/
SH.STA.MMRT
7. Jo, Y., Labrique, A.B., Lefevre, A.E., Mehl, G., Pfaﬀ, T., Walker, N., Friberg, I.K.: Using
the lives saved tool (LiST) to model mHealth impact on neonatal survival in resource-limited
settings. PLoS One, 9(7), e102224 (2014). http://dx.doi.org/10.1371/journal.pone.0102224
8. Chiba, Y., Oguttu, M.A., Nakayama, T.: Quantitative and qualitative veriﬁcation of data
quality in the childbirth registers of two rural district hospitals in Western Kenya. Midwifery
28, 329–339 (2012)
9. Ajayi, P., Omoregbe, N., Adeloye, D., Misra, S.: Development of a secured cloud based health
information system for antenatal and postnatal clinic in an African Country. Front. Artif.
Intell. Appl. 282, 197–210 (2016)
10. Ngabo, F., Nguimfack, J., Nwaigwe, F., Mugeni, C., Muhoza, D., Wilson, D.R., et al.:
Designing and implementing an Innovative SMS-based alert system (RapidSMS-MCH) to
monitor pregnancy and reduce maternal and child deaths in Rwanda. Pan Afr. Med. J. 13, 31
(2012)
11. Ajiboye, B.A., Adekoya, A.J., Alawiye, M.K., Oyedipe, W.J.: Knowledge and utilization of
health information and communication technologies (HICTs) by health workers of the North-
Eastern health zone of Ogun State, Nigeria. Inf. Health Soc. Care, 39(2), 104–123 (2014).
http://dx.doi.org/10.3109/17538157.2013.858044
12. Brinkel, J., Kramer, A., Krumkamp, R., May, J., Fobil, J.: Mobile phone-based mHealth
approaches for public health surveillance in sub-Saharan Africa: a systematic review. Int. J.
Environ. Res. Publ. Health 11, 11559–11582 (2014)
13. Daher, W.: Evaluating Computer Programs: Tools and Assessment (2006)
14. Ikhu-Omoregbe, N.A.: Development of a formal framework for usable operations support in
e-health based systems. (Ph.D.), Covenant University (2007)
15. Odusote, B.O.: GUISET: A Conceptual Design of a Grid-enabled Portal for E-Commerce
On-Demand Services. (M.Sc), Covenant University (2011)
16. Faulkner, L.: Beyond the ﬁve-user assumption: beneﬁts of increased sample sizes in usability
testing. Behav. Res. Meth. Instrum. Comput. 35(3), 375–383 (2003)
17. Sauro, J., Kindlund, E.: A method to standardize usability metrics into a single score.
Retrieved from Site Info Alexa Internet (2014). Zocdoc.com, Accessed 2005
176
P. Ajayi et al.

A Decision Support System for Pediatric Diagnosis
Precious Iheme, Nicholas Omoregbe, Sanjay Misra
(✉), Foluso Ayeni,
and Davies Adeloye
Covenant University, Ota, Nigeria
{Precious.Iheme,Nicholas.Omoregbe,Foluso.Ayeni,Sanjay.Misra,
Davies.Adeloye}@covenantuniversity.edu.ng
Abstract. Newborns are fragile and have a high risk of dying within the ﬁrst 28
days of their life, therefore they require quality care from conception. This
research aims at implementing a mobile pediatric diagnostic system for the rural
settlers in Nigeria, reducing childhood mortality and providing an alternative
pediatric professional. 581 records classiﬁed with naïve Bayes and decision-
stump-tree classiﬁer gave a higher accuracy level for naïve Bayes. A decision-
support system is developed to aid health workers in rural areas in providing
quality health service for children below six, which will provide low-cost medical
service and contribute to reducing childhood mortality.
Keywords: Child healthcare · Mobile technology · Naïve bayes
Pediatric disease
1
Introduction
Newborns are highly prone to health risk due to their fragile nature; the risk of a child
dying is high during the ﬁrst 28 day of their life in several developing countries including
Africa [1]. Hence, they require quality care from the time of conception to the post-natal
stage and further. About 6.3 million children under the age of 5 died in 2013, preterm
birth, delivery complications and infections source a great number of neonatal deaths
[2–5]. These early child deaths are due to conditions that could be prevented or treated
with access to simple, aﬀordable interventions [6–8]. Children in sub-Saharan Africa
are more than fourteen times more likely to die before they get the age of 5 than children
in developed regions. In Nigeria about 2300 children under age ﬁve die each day, the
rural areas recording a larger number of infant mortality. One of the SDGs goals
(sustainable development goals) is to end preventable deaths of newborns and children
less than 5 years of age [9–11]. UNICEF also, in partnership with the Nigerian govern‐
ment to improve the quality of healthcare services as well as decrease the rate of infant
mortality [12].
Mobile technology has improved the quality of health care services, especially in
developing countries [13, 14]. In many places around the world, there has been an
explosion of mobile apps, remote monitoring devices, and online instructional materials
[15, 16]. This has brought updated and constant information to health assistants such as
midwives who are on the forefronts of care and assisting rural populations who need
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 177–185, 2018.
https://doi.org/10.1007/978-3-319-72965-7_17

medical treatment and advice [17, 18]. Hence incorporating the goals of UNICEF and
SDGs into a mobile application will make those services readily accessible to its target
especially in the rural areas.
Development of such application requires data gathering, preprocessing, processing
(data mining) and evaluation (as shown in Fig. 2) before it can be deployed as knowledge.
Because of uncertainty and incomplete information faced when designing automated
healthcare systems [19], naive Bayes classiﬁer which has been tested and trusted to
handle incomplete data successfully despite its simplicity [20–23]. In the present work
we have developed a decision support system using WEKA tools. We have also devel‐
oped a mobile application by using our own algorithms [24] where we also compared
the results obtained by applying WEKA tool with our own system.
The rest of this paper is organized as follows; Sect. 2 give the statement of the
problem and objective of the study, Sect. 3 shows a review of related ﬁndings and tech‐
nologies, Sect. 4 shows the Research methodology, Sect. 5 shows the validation of the
system, Sect. 5 shows the result and discussion, Sect. 6 shows the conclusion and future
work.
2
Statement of Problem and Objective of Study
Low pediatrician to patient ratio which has contributed to the increase of infant mortality,
hence alternative measures are required to improve healthcare service. Also the devel‐
opment of decision support systems has contributed to easy accessibility to quality,
healthcare services in another country especially the developed country. The imple‐
mentation of this system in Nigeria, especially for the rural settlers, will reduce child‐
hood mortality and morbidity and well as bridge the gap of the distribution of medical
professional within the six geopolitical zones, especially those with the low pediatrician
to patient ratio. The research aims to implement a decision support pediatric diagnostic
system using the naïve Bayes classiﬁer particularly for under-ﬁves, and a mobile expert
system using this model is proposed towards improving the quality of health services
available for this age group in Nigeria.
This work gathered requirements, standard symptoms, case deﬁnitions and diag‐
nostic criteria of some common childhood diseases. It modeled core functionalities of
the system. A naive Bayesian framework model for learning the standard symptoms,
case deﬁnitions, and diagnostic criteria as well as inference making was designed. A
prototype using the Bayesian framework was also designed. The main objective of this
work is to explore the usefulness of data mining techniques and Decision Support tech‐
nologies on Healthcare Systems.
3
Brief Review of Related Findings/Technologies
The advent of mobile technology has encouraged the design of complex systems in a
simple mobile application. The advantage of these systems cannot be overemphasized,
from the bridging distance between experts and clients to creating an avenue for interns
to learn amongst many others.
178
P. Iheme et al.

Support vector machine was used in [25] to identify the particular heart valve disease
that a patient is diagnosed with. It was also used by [26] to develop an automatic medical
diagnostic system using statistical medical information provided by the user.
Decision tree classiﬁcation algorithm was used to extract from laboratory ﬁnding
the distinguishing factor between a cognitive heart failure and those of the dyspnea class
[27]. In [28], it was used to prevent whether cancer is benign or malignant using the 10-
fold cross validation test.
The decision tree C4.5, bagging with decision tree C4.5, and bagging with a naive
Bayes classiﬁcation algorithms was compared in the identiﬁcation of heart diseases in
patients using the 10 fold validation to compute the confusion matrix [29].
Support Vector Machines (SVM) and naïve Bayes classiﬁer were compared for text
categorization wikitology. Naïve Bayes prove to be better with 28.78% improvement
using the 10-fold validation [30].
Nematzadeh compared decision tree with naïve Bayes method in the classiﬁcation
of researcher’s cognitive styles in an academic environment. Naïve Bayes produced a
better accuracy with 93.83637 [31].
Diﬀerent artiﬁcial intelligence methods have been used to design decision support
systems all with their individual advantages depending on the nature of data obtained.
In [32] the self-organizing fuzzy neutral network with truth-value fuzzy inference was
used in the classiﬁcation of acute lymphoblastic leukemia (ALL) subtypes using gene
expression data.
4
Research Methodology
In order the achieve the objectives of this research, existing patient record on some
common childhood diseases were observed, preprocessed, evaluated and compared with
another classiﬁer. Diagnostic criteria were obtained through interaction with doctors,
reviewing WHO data to identify standard symptoms, case deﬁnitions and diagnostic
criteria of some common childhood diseases, about 1107 data on symptoms and diag‐
nosis from patient notes of children between 0–5 were obtained.
The data mining architecture was used to model the system on a mobile platform.
The system works oﬄine and uses dialog technique via SMS to obtain answers from the
user.
4.1
System Architecture
Data mining architecture is made up of the business and data layers including the user
interface (as depicted in Fig. 1).
A Decision Support System for Pediatric Diagnosis
179

Fig. 1. System architecture
The user interface requires dialog between the user and the system. The choice of
the user determines the tool that will be used. The business component involves data
processing, which is achieved through four major tasks namely: preparation and prepro‐
cessing, processing (data mining), Evaluation and knowledge deployment. The data is
stored in a data warehouse where persistent data can be extracted [33].
Data PreparaƟon 
/Preprocessing
Data processing 
(data Mining)
EvaluaƟon
Knowledge 
Deployment
Fig. 2. Paediatric application developmental stages
The data-mining algorithm used is the Naïve Bayes classiﬁer. The model learns a
set of preprocessed data collected and the output is classiﬁed based on the diﬀerent
diseases of focus for inference purpose. The framework is then integrated into a mobile
platform for easy accessibility of the system to target audience.
4.2
Naïve Bayes Classiﬁer
A classiﬁer is a function that maps input features to outclass label. Its goal is to learn
from a labeled training set of input-output pairs for predictive purpose [34]. Naïve Bayes
classiﬁer had its name from its naïve assumption of independence between the diﬀerent
features of class label.
180
P. Iheme et al.

The Bayesian concept is explained thus;
A tuple X belong to a class C. X has a measurement made on a set of attribute n.
Given a hypothesis H, classiﬁcation problem determines that the hypothesis holds given
evidence (H/X). P(H/X) is a posterior probability e.g. the probability that patient X with
attributes fever and vomiting will belong to a class malaria. P(H) is a prior probability
e.g. the probability that any patient will have malaria. P(X) is a prior probability e.g. the
probability that a fever with fever has malaria.
Bayes theorem: (P(H)∕(X)) = (P(X∕H)P(H))∕(P(X))
(1)
The steps taken to achieve classiﬁcation in naïve Bayes are stated below:
Step 1: D is a training set and its class variables. Each tuple X is represented by an n-
dimensional attribute vector (X = x1, x2… xn) with n measurement on the n attribute
(a1, a2… an). Hence the data to be used for classiﬁcation must be presented in the format
understandable by classier (usually in a tabular, comma separated or Attribute-Rela‐
tion ﬁle format (ARFF) format).
Step 2: The classiﬁer calculates the prior probabilities for each class. That is, Given
m classes C1, C2,…, Cn and a tuple X, the classiﬁer will predict that X belong to the
class with the highest posterior probability, conditioned on X.
P(Ci∕X) > P(Ci
) P(X) for 1 ≤j ≤m, j ≠1
(2)
Step 3: since P(X) is constant for all the class, only P(Ci/X) P(Ci) needs to be maxi‐
mized. P(Ci) = CiD/D.
Step 4: To reduce computation in evaluating P(X/Ci), the naïve Bayes assumption of
class-conditional independence is made.
P(X∕Ci) =
∏n
(k=1) P(Xk∕Ci
) (P(X1∕Ci
), P(X2∕Ci
) … P (Xn∕Ci
))
(3)
Step 5: To predict the class label of X, P(X/Ci) P(Ci) is evaluated for each class Ci.
P(X∕Ci
) P(Ci
) > P(X∕Cj
) P(Cj
) for 1 ≤j ≤m, j ≠i
(4)
5
Validation and Discussion of the Model
After preprocessing have been carried out on the data collection 581 were selected after
removing irrelevant and missing attributes. The data was evaluated using the WEKA
tool for classiﬁcation.
The result from the classiﬁcation using the 10 fold cross validation test option, shows
that 333 of the dataset were correctly classiﬁed whereas 247 were incorrectly classiﬁed
gave 57.4% accuracy, with a means square error of 0.3929. Malaria had the higher
incorrectly classiﬁed instances (92 instances) while measles has the least incorrectly
classiﬁed instances (i.e. 14 instances) (Fig. 3).
A Decision Support System for Pediatric Diagnosis
181

Fig. 3. Result of data evaluation using the 10 fold validation
Using the 66% split option the accuracy obtained was 55.3% (as shown in Fig. 4).
109 instances were classiﬁed correctly, 88 were incorrectly classiﬁed with a means
square error of 0.3998. Malaria had the higher incorrectly classiﬁed instances (28
instances) while measles has the least incorrectly classiﬁed instances (6).
Fig. 4. Result of data evaluation using the 66% split
182
P. Iheme et al.

The result of the Naïve Bayes classiﬁer was compared with that of the decision stump
tree classiﬁer, the result obtained was lower than the accuracy level from the naïve Bayes
classiﬁer.
The result from the decision stump tree (as shown on Fig. 5) reveals that 285 instances
were classiﬁed correctly, 295 were incorrectly classiﬁed, with a means square error of
0.4026. Pneumonia had the higher incorrectly classiﬁed instances (130 instances) while
malaria has the least incorrectly classiﬁed instances (55 instances).
Fig. 5. Result for data evaluation using the decision stump tree
The result obtained will be suitable to design a decision support system. This system
will require veriﬁcation of diagnosis by an expert. However, it will provide an avenue
for interns and medical technician to have more knowledge of the diseases diagnosed
by the system.
6
Conclusions and Future Work
From this research, a decision system was developed to aid health workers in the rural
areas in Nigeria in providing quality health service for children below age six. One
advantage of this system is that it is SMS-based; hence the patients can receive medical
advice from the comfort of their homes. This also reduces the number of the patient a
doctor will have to see in a day and reduce long waits at the hospitals and delayed
diagnosis, and consequently death.
This research is signiﬁcant as it provides a low-cost medical service to children
especially in the rural regions, contribute to reduction of childhood mortality through
accessible medical service, educate medical personnel in the rural areas of diagnosis of
A Decision Support System for Pediatric Diagnosis
183

childhood disease, and make data available to doctors for further research as well as
bridging the communication gap between doctors and patients.
This research is a contribution to the achievement of SDG’s goal to reduce childhood
mortality between 2016 and 2030 and UNICEF’s Accelerating Child Survival and
Development in rural areas.
Acknowledgement. We acknowledge the support and sponsorship provided by Covenant
University through the Centre for Research, Innovation and Discovery (CUCRID).
References
1. UN-DESA.: Transforming our world: the 2030 Agenda for Sustainable Development (2015).
https://sustainabledevelopment.un.org/post2015/transformingourworld
2. Chan, M., Lake, A.: Towards ending preventable child deaths. Lancet 379 (2012)
3. Wardlaw, T., You, D., Newby, H., Anthony, D., Chopra, M.: Child survival: a message of
hope but a call for renewed commitment in UNICEF report. Reprod. Health 10, 1–4 (2013)
4. George, C.M., Vignola, E., Ricca, J., Davis, T., Perin, J., Tam, Y., Perry, H.: Evaluation of
the eﬀectiveness of care groups in expanding population coverage of Key child survival
interventions and reducing under-5 mortality: a comparative analysis using the lives saved
tool (LiST). BMC Publ. Health 1–9 (2015)
5. Shifa, G.T., Ahmed, A.A., Yalew, A.W.: Early days of life are crucial for child survival in
gamo gofa zone, Southern Ethiopia: A community based study. BMC Pediatr. 16, 1–10 (2016)
6. Hershey, C.L., Doocy, S., Anderson, J., Haskew, C., Spiegel, P., Moss, W.J.: Incidence and
risk factors for malaria, pneumonia and diarrhea in children under 5 in UNHCR refugee
camps: A retrospective study. Conﬂ. Health 5, 1–11 (2011)
7. Kahabuka, C., Kvåle, G., Hinderaker, S.G.: Factors associated with severe disease from
malaria, pneumonia and diarrhea among children in rural Tanzania – A hospital-based cross-
sectional study. BMC Infect. Dis. 12, 1–9 (2012)
8. Yakoob, M.Y., Theodoratou, E., Jabeen, A., Imdad, A., Eisele, T.P., Ferguson, J., Jhass, A.,
Rudan, I., Campbell, H., Black, R.E., Bhutta, Z.A.: Preventive zinc supplementation in
developing countries: impact on mortality and morbidity due to diarrhea, pneumonia and
malaria. BMC Publ. Health 11, 1–10 (2011)
9. WHO: Health in 2015: from MDGs to SDGs (2016). http://www.who.int/gho/publications/
mdgs-sdgs/en/
10. Sachs, J.D.: Happiness and Sustainable Development: Concepts and Evidence. World
Happiness
11. Ahmed, M.: The challenges ahead. Education (2015)
12. UNICEF: Accelerating Child Survival and development. The Nigerian Child: United Nations
Children’s Fund Nigeria Newsletter, pp. 1–12 (2008)
13. Sa, J.H.G., Rebelo, M.S., Brentani, A., Grisi, S.J., Iwaya, L.H., Simplicio Jr., M.A., Carvalho,
T.C.M.B., Gutierrez, M.A.: Georeferenced and secure mobile health system for large scale
data collection in primary care. Int. J. Med. Inform. 94, 91–99 (2016)
14. Sun, N., Rau, P.L.P., Li, Y., Owen, T., Thimbleby, H.: Design and evaluation of a mobile
phone-based health intervention for patients with hypertensive condition. Comput. Hum.
Behav. 63, 98–105 (2016)
15. West, D.M.: Using mobile technology to improve maternal health and ﬁght Ebola: A case
study of mobile innovation in Nigeria. Center for Technological Innovation at Brookings
(2015)
184
P. Iheme et al.

16. Hampshire, K., Porter, G., Owusu, S.A., Mariwah, S., Abane, A., Robson, E., Munthalie, A.,
DeLannoy, A., Bangog, A., Gunguluzaf, N., Milnere, J.: Informal m-health: How are young
people using mobile phones to bridge healthcare gaps in Sub-Saharan Africa? Soc. Sci. Med.
142, 90–99 (2015)
17. Uddin, M.J., Shamsuzzaman, M., Horng, L., Labrique, A., Vasudevan, L., Zeller, K.,
Chowdhury, M., Larson, C.P., Bishai, D., Alamj, N.: Use of mobile phones for improving
vaccination coverage among children living in rural hard-to-reach areas and urban streets of
Bangladesh. Vaccine 34(2), 276–283 (2016)
18. Domek, G.J., Contreras-Roldan, I.L., O’Leary, S.T., Bull, S., Furniss, A., Kempe, A.,
Asturias, E.J.: SMS text message reminders to improve infant vaccination coverage in
Guatemala: A pilot randomized controlled trial. Vaccine 34(21), 2437–2443 (2016)
19. Nada, N., Adepa, I., Fatin, G.: Design a fuzzy expert system for pediatrics diseases diagnosis.
Raf. J. Comp. Maths 5(2), 155–173 (2008)
20. Corani, G., Zaﬀalon, M.: JNCC2: An extension of naive Bayes classiﬁer suited for small and
incomplete data sets. Environ. Model. Softw. 23(7), 960–961 (2008)
21. Soria, D., Garibaldi, J.M., Ambrogi, F., Biganzoli, E.M., Ellis, I.O.: A ‘non-parametric’
version of the naive bayes classiﬁer. Knowl. Based Syst. 24(6), 775–784 (2011)
22. Chen, J., Huang, H., Tian, F., Tian, S.: A selective bayes classiﬁer for classifying incomplete
data based on gain ratio. Knowl. Based Syst. 21(7), 530–534 (2008)
23. Bounhas, M., Hamed, M.G., Prade, H., Serrurier, M., Mellouli, K.: Naive possibilistic
classiﬁers for imprecise or uncertain numerical data. Fuzzy Sets Syst. 239, 137–156 (2014)
24. Iheme, P., Omoregbe, N., Misra, S., Adeoye, D., Adewole, A.: Mobile-Bayesian diagnostic
system for childhood infectious diseases. In: Proceedings of ICADIWT 2017, Maxico (2017)
25. Maglogiannis, I., Loukis, E., Zaﬁropoulos, E., Stasis, A.: Support vectors machine-based
identiﬁcation of heart valve diseases using heart sounds. Comput. Methods Programs Biomed.
95(1), 47–61 (2009)
26. Kampouraki, A., Vassis, D., Belsis, P., Skourlas, C.: e-Doctor: A web based support vector
machine for automatic medical diagnosis. Procedia Soc. Behav. Sci. 73, 467–474 (2013)
27. Son, C.S., Kim, Y.N., Kim, H.S., Park, H.S., Kim, M.S.: Decision-making model for early
diagnosis of congestive heart failure using rough set and decision tree approaches. J. Biomed.
Inform. 45(5), 999–1008 (2012)
28. Sumbaly, R., Vishnusri, N., Jeyalatha, S.: Diagnosis of breast cancer using decision tree data
mining technique. Int. J. Comput. Appl. 98, 1–9 (2014)
29. Tu, M.C., Shin, D., Shin, D. (eds.): A comparative study of medical data classiﬁcation
methods based on decision tree and bagging algorithms. In: Eighth IEEE International
Conference on Dependable, Autonomic and Secure Computing (2009)
30. Hassan, S., Raﬁ, M., Shaikh, M.S.: Comparing SVM and naive bayes classiﬁers for text
categorization with Wikitology as knowledge enrichment. In: IEEE 14th International
Multitopic Conference (INMIC) (2011)
31. Nematzadeh, B. Z.: Comparison of Decision Tree and Naive Bayes Methods in Classiﬁcation
of Researcher’s Cognitive Styles in Academic Environment (2012)
32. Tung, W.L., Quek, C.: GenSo-FDSS: a neural-fuzzy decision support system for pediatric
ALL cancer subtype identiﬁcation using gene expression data. Artif. Intell. Med. 33(1), 61–
88 (2005)
33. Winsemann, T., Köppen, V.: Persistence in enterprise data warehouses. Otto-von-Guericke
University Magdeburg, Technical Reports (2012)
34. Murphy, K.P.: Naive bayes classiﬁers. University of British Columbia (2006)
A Decision Support System for Pediatric Diagnosis
185

CNRIA Workshop

Polar Coding Steganographic Embedding
Using Successive Cancellation
Birahime Diouf(&), Idy Diop, Khadidiatou Wane Keita,
Madiop Diouf, Sidi Mohamed Farsi, Khaly Tall,
and Ousmane Khouma
Department of Computer Science, Polytechnic Institute (ESP),
Cheikh Anta Diop University (UCAD), Dakar, Senegal
dioufbira11@yahoo.fr, idydiop@yahoo.fr,
{wane.keita,madiop.diouf}@esp.sn, farsism@yahoo.com,
khalytal@gmail.com, ousmane.khouma@ucad.edu.sn
Abstract. In this paper, we propose a practical adaptive embedding method-
ology based on Successive Cancellation (SC) polar coding. The new proposed
SC-based Polar Coding Steganography (SC-PCS) deﬁnes message bits as frozen
bits of the SC decoder and computes path metrics according to embedding costs
of pixels of the cover image. Simulation results demonstrate that SC-PCS
minimizes an arbitrary embedding distortion while embedding covert message.
Keywords: Adaptive embedding  Additive distortion  Polar code
Steganography  Successive cancellation  Syndrome coding
1
Introduction
With the development of internet and diversity of communications media, information
security becomes a necessity and even a priority. To satisfy this demand, techniques
such as cryptography protect information but do not guarantee discretion. However,
often the existence of the communication must be kept secret. Steganography is today
positioned, deservedly, as a means to address this widely shared concern. It is to
conceal secret information in others unsuspected media such as text, image (used in this
paper), audio or video so that only the recipient is aware of the existence of the
communication. The majority of modern steganographic techniques are based on
embedding a secret message while minimizing the embedding distortion [1]. This
approach consists of two complementary tasks that both participate to increase
steganographic security [2]. Most of the proposed methods focus on one of these two
tasks. The ﬁrst is to effectively deﬁne the embedding costs of all the pixels of the cover
image. To design costs several methods are proposed in current state of the art such as
HUGO (Highly Undetectable steGO) [3], WOW (Wavelet Obtained Weights) [4],
UNIWARD (Spatial UNIversal WAvelet Relative Distortion) [5], HILL (HIgh Low
Low) [6], MiPOD (Minimizing the Power of Optimal Detector) [7]. The second task,
that we are interested in, relies on minimizing the distortion deﬁned from costs using
practical coding. For practice, matrix embedding is proposed by Crandall [8]. The ﬁrst
implementation of matrix embedding was provided by Westfeld [9] who used
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 189–201, 2018.
https://doi.org/10.1007/978-3-319-72965-7_18

Hamming codes in F5 algorithm. The current state of the art methods includes BCH
(Bose-Chaudhuri-Hocquenghem) [10], RS (Reed Solomon) [11], LDPC (Low Density
Parity Check) [12] and STC (Syndrome Trellis Codes) [13].
Our work is a contribution to methods that use codes to minimize distortion func-
tion. The codes used for this shake are polar codes (PC) introduced by Arikan [14] as the
ﬁrst capacity-achieving codes. Several decoding types exist for PC such as Successive
Cancellation (SC) [14], SC List (SCL) [15], Linear Programming (LP) [16] and
Adaptive LP (ALP) [17]. PC are demonstrated to be applicable in steganography with
constant proﬁle and wet paper codes [18] and minimize embedding impact [19–21] for
these two important frameworks. The ﬁrst adaptive embedding method based on polar
codes is proposed in [22], in which ML-certiﬁcate ALP decoder is used. The natural
decoding technique of polar code is SC which achieves the rate-distortion bound. These
beneﬁces of polar codes and their optimality to Payload-Limited Sender (PLS) problem
emphasized by Filler et al. [13] motive the use of SC decoding to minimize distortion
function and increase embedding efﬁciency.
The rest of this paper is organized as follows. In Sect. 2, we present steganography,
distortion function and practical embedding coding techniques. In Sect. 3, we review
polar coding and SC decoding. The proposed steganographic embedding technique
based on SC is described in Sect. 4. Section 5 shows experimental results of the tests.
Conclusions are drawn in Sect. 6.
2
Steganography Based on Minimizing Additive Distortion
2.1
Distortion Deﬁnition for Adaptive Embedding
In modern steganography, most of the proposed techniques are based on embedding
secret message while minimizing the embedding distortion between cover x 2 X and
stego y 2 Y ¼ I1      In images which can be deﬁned in additive form [13]:
D x; y
ð
Þ ¼
Xn
i¼1 qi x; yi
ð
Þ;
ð1Þ
where qi x; yi
ð
Þ is a local distortion measure and denotes the cost of replacing pixel xi by
yi if digital images are used. In such deﬁnition, we assume that the embedding changes
are mutually independent. The additive distortion can be rewritten as follows:
D x; y
ð
Þ ¼
Xn
i¼1 qi  yi 6¼ xi
½
;
ð2Þ
where P
½  is the logical operator that is equal to 1 if relation P is true and 0 else. Several
methods exist to calculate the embedding costs. HUGO was the ﬁrst method proposed
to calculate costs from features vectors difference in SPAM space. WOW and UNI-
WARD use directional ﬁlter to design costs. HILL cost function is designed using one
high-pass ﬁlter and two low-pass ﬁlters. MiPOD is model based and computes costs by
minimizing the power of the most efﬁcient detector.
190
B. Diouf et al.

2.2
Optimal Embedding Problem and Practical Coding
For message embedding there are two methods: Distortion-Limited Sender (DLS) and
Payload-Limited Sender (PLS) [13]. However, PLS that consists in embedding a ﬁxed
payload while minimizing the embedding distortion, is the most used compared with
DLS that is to maximize payload while introducing an expected distortion. Let the
stego image y be a random variable over Y and its distribution px y
ð Þ , PrðyjxÞ, the
optimal embedding is realized when replacing each pixel xi with probability px;i:
px;i yi
ð Þ ¼
exp kqi x; yi
ð
Þ
ð
Þ
P
yi2Ii exp kqi x; yi
ð
Þ
ð
Þ
ð3Þ
where k 2 0; 1
½
½ is a parameter obtained by solving the following constraint
h px
ð
Þ ¼ 
X
y2Y px y
ð Þ log2 px y
ð Þ ¼ m;
ð4Þ
where m is the size of the message m 2 M ¼ 0; 1
f
gm and px y
ð Þ ¼ Qn
i¼1 px;i yi
ð Þ. The
constraint can be rewritten as follows:

Xn
i¼1
X
yi2Ii px;i yi
ð Þ log2 px;i yi
ð Þ ¼ m;
ð5Þ
In practice syndrome coding can be used to implement the embedding operation. In
this context, let C be the linear code of length n and dimension n  m and consider
binary embedding operation, then the embedding Emb : X  M ! Y and extraction
Ext : Y ! M functions, respectively used by the sender and the recipient are
Emb x; m
ð
Þ ¼ arg minLSB y
ð Þ2C m
ð Þ D x; y
ð
Þ
Ext y
ð Þ ¼ LSB y
ð Þ  HT ¼ m;
ð6Þ
where LSB y
ð Þ ¼ LSB y1
ð
Þ; . . .; LSB yn
ð
Þ
ð
Þ, H 2 0; 1
f
gmn is a parity-check matrix and
C m
ð Þ ¼ fz 2 0; 1
f
gnjzHT ¼ mg denotes the coset of m. These two functions verify
Ext Emb x; m
ð
Þ
ð
Þ ¼ m
8x 2 X; 8m 2 M:
ð7Þ
An embedding coding algorithm can be evaluated via its embedding efﬁciency
e a
ð Þ ¼ m=D ¼ an=D (in bits/distortion unit) in comparison with the optimal embed-
ding derived from (3), where a ¼ m=n is called the relative payload. It is known that
random linear code with syndrome coding is capacity-achieving for the PLS problem.
However, random code is not practical due to the exponential complexity needed for
the decoder. Non-random codes such as Hamming, BCH, RS, LDPC and STC are used
to approach the optimal embedding. Notes that STC is currently the most used. Polar
codes are known to be optimal for PLS problem as pointed out by Filler et al. [13]. Wet
paper codes are used to embed data in a cover image for which some pixels are
forbidden to be altered. Such pixels, called wet pixels, are characterized by inﬁnite cost
qi ¼ 1 and then Ii ¼ xi
f g. The others, called dry pixels, may be changed and have
Polar Coding Steganographic Embedding Using Successive Cancellation
191

ﬁnite cost qi\1. The wet paper framework is an interesting topic in steganography
and is largely addressed in our previous works [19–21].
3
Polar Codes and SC Decoding
3.1
Polar Codes, First Capacity Achieving Codes
Introduced by Arikan [14], Polar Codes (PC) are deﬁned as the ﬁrst codes that achieve
channel capacity I W
ð
Þ (Shannon’s threshold) in a large class of channel W with low
encoding and decoding time complexity O nlogn
ð
Þ, where n ¼ 2p is the block-length.
PC construction is designed using channel polarization which provides them their
recursive nature. Channel polarization consists in constructing polarized channels
W ið Þ
n : 1  i  n from n independent copies of W. It is made up two steps: channel
combining that associates n copies of W and recursively creates n-inputs channel Wn
and channel splitting that subdivides Wn into n channels W ið Þ
n
[19]:
ð8Þ
The main idea of polar coding is to access to each channel W ið Þ
n
and send the
information bits across the most reliable ones i.e. with lowest reliability parameter
Z W ið Þ
n


and the frozen (ﬁxed) bits through the remaining channels. The information
and frozen bits indices will be denoted by A and it complementary Ac, respectively. We
denote by ai
1 ¼ a1; . . .; ai
ð
Þ; 1  i  n, ^ui
1;e and ^ui
1;o the sub-vectors consisting of ele-
ments with odd and even indices. The generator matrix is deﬁned from the Kronecker
product Gp
2
of p copies of G2 ¼
1
0
1
1


by Gn ¼ BnGp
2 , where Bn the bit-reversal
permutation matrix. The polar encoding is based on the relation c ¼ uGn, where u is the
source word and c the codeword. When traveling through the channel, c can be
changed into a received word r. The transition probability is denoted by W rijci
ð
Þ.
3.2
Successive Cancellation Decoding and Polar Codes
SC decoding is the ﬁrst and inherent decoding proposed by Arikan for PC. It is
governed by the recursive nature of PC construction. First, the bit ^u1 is decoded given r.
Then, ^u2 is given from ^u1 and r, and so far i.e. ^ui is estimated given the previously
decoded bits ^ui1
1
and the received word r. For frozen bits the value is known ^ui ¼ ui.
The aim of SC decoder is to provide estimated source-word ^u given A, uAc and r.
The SC decoder gives its decision using the following function h:
^ui ,
ui;
if i 2 Ac
0
if L ið Þ
n
r; ^ui1
1


 1
1;
else
8
<
:
; 1  i  n
ð9Þ
192
B. Diouf et al.

where
L ið Þ
n
r; ^ui1
1


¼
W ið Þ
n r;^ui1
1
j0
ð
Þ
W ið Þ
n r;^ui1
1
j1
ð
Þ,
with
L 1
ð Þ
1
ri
ð Þ ¼ L ri
ð Þ ¼ W rij0
ð
Þ
W rij1
ð
Þ
is
LR
(Likelihood-Ratio). In LLR domain non-frozen bits are chosen depending on if
LL ið Þ
n
r; ^ui1
1


 0 or not. Figure 1 shows the graph of the SC decoder with n ¼ 8. This
process starts on the right side of the graph where the received word bits LRs L ri
ð Þ are
combined in pairs by moving towards the left side. The graph consists of log n ¼ 3
stages each of which contains n ¼ 8 nodes. For each node of stage j, the LR is
calculated from two incoming LRs La and Lb of stage j  1 using f or g function:
f La; Lb
ð
Þ ¼ 1 þ La  Lb
La þ Lb
;
ð10Þ
and
g La; Lb; ^usum
ð
Þ ¼ g^usum La; Lb
ð
Þ ¼ L 12^usum
ð
Þ
a
 Lb
ð11Þ
where ^usum is the binary partial sum of previously estimated bits. In LLR domain
f LLa; LLb
ð
Þ ¼ 2 tanh1 tanh LLa=2
ð
Þ  tanh LLb=2
ð
Þ
ð
Þ
ð12Þ
and
g^usum LLa; LLb
ð
Þ ¼ LLa  1
ð
Þ^usum þ LLb;
ð13Þ
where LLa , log La
ð
Þ and LLb , log Lb
ð
Þ are LLRs values. The min-sum approxima-
tion, used with LDPC codes, can also be exploited to reduce this complexity:
f LLa; LLb
ð
Þ 	 ~f LLa; LLb
ð
Þ , sign LLa
ð
Þ  sign LLb
ð
Þ  min LLa
j
j; LLb
j
j
ð
Þ:
ð14Þ
At the left side of the graph, estimated bits ^ui are provided using function h (9).
Fig. 1. Graph of SC decoding for n ¼ 8.
Polar Coding Steganographic Embedding Using Successive Cancellation
193

The SC decoding can be seen as a code tree search. In this context, it begins at the
root node that has two paths with labels 0 and 1 and metrics W ið Þ
n
rj0
ð
Þ and W ið Þ
n
rj1
ð
Þ,
respectively. Then, the path with highest metric is chosen. This path results, in turn, in
two paths with labels 0 and 1 and metrics W ið Þ
n
r; ^u1j0
ð
Þ and W ið Þ
n
r; ^u1j1
ð
Þ. Generally, at
each level ^ui is decoded by comparing the two path metrics W ið Þ
n ðr; ^ui1
1 j0Þ and
W ið Þ
n ðr; ^ui1
1 j1Þ if i is not a frozen position. If i is a frozen position, ^ui ¼ ui. This
procedure continues down to the leaf nodes where the last hard decision is made to
estimate ^un. Notice that SC decoder provides good performance if the bloc length n is
sufﬁciently large. We will use this decoding technique to deﬁne our steganographic
scheme.
4
SC-Based Adaptive Polar Coding Embedding
In channel coding, polar codes are shown to be capacity achieving. Another beneﬁt and
reason why we are interested in polar codes is their optimality for Payload Limited
Sender (PLS) as emphasized out by Filler et al. [13]. Polar code construction in
steganographic context is largely explained in [18]. For simpliﬁcation purposes, in the
rest of the paper, we will denote by x and y the LSB vectors of the cover and the stego
images, respectively. The practical steganographic embedding using codes is based on
syndrome coding with the extraction constraint yHT ¼ m. We will ﬁrst investigate the
impact of this relation in the frozen bits values choice when using SC.
4.1
Deﬁnition of Secret Message Bits as Frozen Bits
We will show that the Steganographic Polar Coding (SPC) under SC encoding is
equivalent to SC decoding of polar codes in channel coding when uAc ¼ m. Indeed, it is
known from [14] that polar codes are deﬁned in terms of an invertible matrix Gn via the
encoding relation c ¼ uGn i.e. u ¼ cG1
n
¼ cGn, because G1
n
¼ Gn. For the sake of
simpliﬁcation we denote G ¼ Gn. The source word can be split into two parts
u ¼ uA; uAc
ð
Þ, where the information word uA ¼ ui : i 2 A
ð
Þ and the frozen word uAc ¼
ui : i 2 Ac
ð
Þ [23]. Then, we can write u ¼ uA; uAc
ð
Þ ¼ cGA; cGAc


where GA and GAc
are the submatrices consisting of columns of G whore indices are in A and Ac,
respectively. From the deﬁnition of polar code parity check matrix [16] whose trans-
pose HT is obtained by selecting the columns of G with indices in Ac [18, Lemma 1],
we have GAc ¼ HT with uAc ¼ cHT. Let a source word u such that uAc ¼ m. Then
searching y such that yHT ¼ yGAc ¼ m is equivalent to SC decoding of a polar code
where secret message bits are deﬁned as frozen bits.
In the original SC decoding, given frozen bits uAc, one decodes the source word u
such that there exists a codeword c ¼ uGn ¼ uA; uAc
ð
ÞGn. In the steganographic case,
we have u ¼ uA; m
ð
Þ, y ¼ uGn and yHT ¼ m. Then, we obtain a m –coset polar code
where the stego word y is a code word. Then, naturally, to ﬁnd the stego word, we can
194
B. Diouf et al.

use SC polar decoding where frozen bits uAc ¼ m. However, this fact does not affect
the SC decoder performance because it is independent of frozen bits values choice [14].
This decoding technique outputs u rather than the code word. However, we can simply
obtain the m –coset polar codeword y by applying the encoding relation uGn. The next
step is to explain how the LR and LLR-metrics will be calculated.
4.2
Metric Calculation of SC for Steganography
In digital communication the classical SC decoder task is to ﬁnd the information word
u from the received word r by using metrics calculated from transition probabilities
W receivedjtransmited
ð
Þ ¼ W rijci
ð
Þ ¼ pe if ri 6¼ ci and W rijci
ð
Þ ¼ 1  pe else, for
Binary Symmetric Channel (BSC), where pe is the error probability. In steganography,
given the cover word x and the secret message m, the sender has to search a stego word
y such that yHT ¼ m. As seen in previous subsection, the decoder does not directly
output the stego word y but gives ﬁrst an information word u that, encoded with uGn,
provides the searched stego word y. Since that is the stego word we seek, given the
cover word, then transition probabilities in steganography will be denoted by
W coverjstego
ð
Þ ¼ W xijyi
ð
Þ. This can be interpreted as the probability that the corre-
sponding pixel changes (if yi ¼ xi) or not (if yi 6¼ xi i.e. yi ¼ 1  xi). The deﬁnition of
W xijyi
ð
Þ would verify the conditions of transition probabilities in SC decoding and be
provided depending on the embedding costs qi (cost of replacing pixel xi by yi.).
When all the pixels have the same sensitivity to change qi ¼ 1 (constant proﬁle), all
the transition probability W xijyi
ð
Þ ¼ 1=2. Let consider an arbitrary distortion, the idea
is to assign great values of change probabilities W xij1  xi
ð
Þ (then small values of
non-change probabilities W xijxi
ð
Þ) for pixels with small value of embedding costs and
small change probabilities (then great non-change probabilities) for pixels with great
embedding costs. Then, let
W xijyi
ð
Þ ¼
di
1  di
if yi ¼ xi
else

;
ð15Þ
where di ¼ qi=qmax and qmax is maximum of the costs set. It is easy to verify that this
deﬁnition of transition probabilities satisﬁes the above concerns and, additionally
W xij0
ð
Þ þ W xij1
ð
Þ ¼ 1 We can condense by:
W xijyi
ð
Þ ¼
xi ¼ yi
½

ð
Þ di
ð Þ þ
xi 6¼ yi
½

ð
Þ 1  di
ð
Þ:
ð16Þ
The logical operator applied on binary values xi and yi allows rewriting as follows:
W xijyi
ð
Þ ¼ 1  xi  yi
j
j
ð
Þ di
ð Þ þ
xi  yi
j
j
ð
Þ 1  di
ð
Þ:
ð17Þ
After deﬁning W xijyi
ð
Þ, we can obtain the transition probabilities corresponding to
polarized channels W ið Þ
n . In steganographic SC decoder, transition probability of
Polar Coding Steganographic Embedding Using Successive Cancellation
195

channel W ið Þ
n
denotes the likelihood of ui given the channel outputs (cover x ¼ xn
1 and
i  1 previously decoded bits ui1
1 ). They are expressed by [14]:
W ið Þ
n
x; ui1
1 jui


¼
X
un
i þ 1
1
2n1 Wnðxn
1jun
1Þ


¼
X
un
i þ 1
1
2n1
Yn
i¼1 WðxijyiÞ


: ð18Þ
Unfortunately, this expression is not practical. That is why Arikan deﬁned recursive
formulas, that can be labeled f and g, to calculate these transition probabilities:
W 2i1
ð
Þ
n
xn
1; ^u2i2
1
j^u2i1


¼ f Wa; Wb
ð
Þ ¼
X
^u2i2 0;1
f
g
1
2 Wa  Wb;
ð19Þ
and
W 2i
ð Þ
n
xn
1; ^u2i1
1
j^u2i


¼ g Wa; Wb
ð
Þ ¼ 1
2 Wa  Wb;
ð20Þ
where Wa ¼ W ið Þ
n=2 xn=2
1 ; ^u2i2
1;o

 ^u2i2
1;e j^u2i1 
 ^u2i
	

, Wb ¼ W ið Þ
n=2 xn
n=2 þ 1; ^u2i2
1;e j^u2i
	

. Let
L ið Þ
n
xn
1; ^ui1
1


¼
W ið Þ
n
xn
1;^ui1
1
j0
ð
Þ
W ið Þ
n
xn
1;^ui1
1
j1
ð
Þ, then LRs can be recursively calculated using functions
(10) f La; Lb
ð
Þ ¼ L 2i1
ð
Þ
n
xn
1; ^u2i2
1


and (11) g^usum La; Lb
ð
Þ ¼ L 2i
ð Þ
n
xn
1; ^u2i1
1


, where La ¼
L ið Þ
n=2 xn=2
1 ; ^u2i2
1;o

 ^u2i2
1;e
	

and Lb ¼ L ið Þ
n=2 xn
n=2 þ 1; ^u2i2
1;e
	

. This recursion continue until
the last level LRs of length 1, where the LR calculation is given directly by
L 1
ð Þ
1
xi
ð Þ ¼ L xi
ð Þ ¼ W xij0
ð
Þ
W xij1
ð
Þ. From (17), the LR is deﬁned by
L xi
ð Þ ¼ W xij0
ð
Þ
W xij1
ð
Þ ¼ 1  xi
ð
Þ di
ð Þ þ xi
ð Þ 1  di
ð
Þ
xi
ð Þ di
ð Þ þ 1  xi
ð
Þ 1  di
ð
Þ ¼
di
1di
if xi ¼ 0
1di
di
if xi ¼ 1
(
:
ð21Þ
In logarithmic domain, the Log-Likelihood Ratios (LLR) are deﬁned by
LL xi
ð Þ ¼ log L xi
ð Þ
ð
Þ 1  2xi
ð
Þ  log
di
1  di


¼ 1  2xi
ð
Þ  log
qi
qmax  qi


:
ð22Þ
These steganographic frozen bits and the LLRs will be used in the steganographic SC.
4.3
Steganographic Successive Cancellation Algorithm
Once we have given the frozen bits values and the LRs or LLRs (metrics) for the choice
of unfrozen bits, we can implement the SC decoder of polar codes for steganography
which is given by Algorithm 1:
196
B. Diouf et al.

The running SC-PCS provides the stego medium y  uGn that will be transmitted
to the recipient of the secret message. Once the stego medium is received, the recipient
has two ways to ﬁnd de covert message. Firstly, he can encode the stego medium to
ﬁnd the source word by u ¼ yGn. Then, he selects uAc ¼ yGn
ð
ÞAc¼ m. The second
alternative is to use the usual matrix relation via the parity check matrix m ¼ yHT. In
this manner, the embedding process provides the stego objet whose syndrome is the
secret message m and minimizing the additive distortion D.
5
Experimental Results
The implementation of the proposed SC-PCS is given by the following steps:
– Construct the used polar code and initialize these parameters;
– Update channel transition probabilities calculated from costs using (17);
– Apply the SC-PCS embedding (Algorithm 1) using LLR path-metrics from (22);
Example 1: Let consider a cover x ¼ 0; 1; 1; 1; 0; 0; 1; 0
ð
Þ, a secret message m ¼
1; 0; 1; 0
ð
Þ and the corresponding stego object y. The different steps of the SC-PCS
embedding with a polar code of bloc length n ¼ 8 and dimension k ¼ 4 are:
– When we use the construction method of polar code then, A ¼ 4; 6; 7; 8
f
g, Ac ¼
1; 2; 3; 5
f
g and the parity check matrix is:
Polar Coding Steganographic Embedding Using Successive Cancellation
197

H =
1
1
1
1
1
1
1
1
0
0
0
0
1
1
1
1
0
0
1
1
0
0
1
1
0
1
0
1
0
1
0
1
2
664
3
775 and HT ¼
1
0
0
0
1
0
0
1
1
0
1
0
1
0
1
1
1
1
0
0
1
1
0
1
1
1
1
0
1
1
1
1
2
66666666664
3
77777777775
ð23Þ
– Generate randomly an embedding cost q ¼ 39; 57; 8; 6; 54; 78; 94; 13
ð
Þ, for exam-
ple. In this case, the mean is qmean ¼ 43:6250, the maximum is qmax ¼ 94.
– The
channel
transition
probabilities
update
in
LLR
domain
provides
LL ¼ 0:3438; 0:4321; 2:3749; 2:6856; 0:3001; 1:5841; Inf ; 1:8295
ð
Þ.
– When applying the SC-PCS embedding, the stego objet obtained is y ¼
0; 1; 1; 1; 1; 0; 1; 0
ð
Þ with distortion function D x; y
ð
Þ ¼ 54. The change object is
e ¼ 0; 0; 0; 0; 1; 0; 0; 0
ð
Þ.
In this example, the optimal stego medium (corresponds to the one which mini-
mizes distortion function) is obtained by using the SC decoder even with very small
length n ¼ 8. The total embedding distortion is D x; y
ð
Þ ¼ 54.
We will apply our SC-PCS on 512  512 8-bit gray scale digital images coming
from BOSSbase database version 1:01 (Break Our Stego System) containing 10:000
images of pgm format obtained by rescaling and cropping natural images of various
sizes of eight different cameras. Since the SC decoder performance is good if the bloc
length n is enough large, then we can consider the image entirely i.e. the cover size
n ¼ 512  512 ¼ 262144 ¼ 218. This value is sufﬁciently large to provide good per-
formance. Note that, with the existing cost computation methods, often the high costs
crowd into smooth area and low costs in textured area of the image (Fig. 2). Then, after
calculating the costs, we will use the bit-reversal permutation matrix Bn as suggested
and largely explained in [21]. This will scatter the pixels of the cover image and then
increase the success probability of the optimal stego image search.
(a) Cover
(b) Stego
Smooth area
Textured area
Fig. 2. Cover vs stego images of ‘1013’ from Boss base.
198
B. Diouf et al.

Figure 2 shows the cover image ‘1013.pgm’ and the corresponding stego image
after embedding a 0:4 bpp (bit per pixel) payload. The secret message is generated in a
random format. The pixels of some smooth areas are located at the bottom of the
images. These pixels will be assigned to very high cost (for example: 1010 in practice).
For the embedding, the images are reshaped in a single vector of size 218 from which
we extract the LSB vector that we make use to apply SC-PCS.
To investigate the performances of our adaptive steganographic scheme we compute
the embedding efﬁciency embeff ¼ m=D x; y
ð
Þ in comparison with other codes w.r.t
reciprocal relative payload 1=a (Fig. 3).
As shown by Fig. 3, SC-PSC provides similar performance that ALP-PCS [22] and
better performance than Hamming and STC but for a ¼ 1=2 it is lower that STC’s.
Notice that this result is not surprising and could be better if we are referred to the
theoretical results given in the literature about polar codes (capacity-achieving) when
applied in PLS problem as emphasized in [13].
6
Conclusion
In modern adaptive steganographic scheme, there are two levers on which one could
emphasis to increase the security of stego-system. Either focus on designing distortion
measure (embedding costs), or concentrate on deﬁning a near optimal embedding
coding scheme. We have opted for the second option and have proposed, in this paper,
a practical and efﬁcient embedding method based SC polar coding. Indeed, as shown
by the test results and the practical examples, this SC-PCS scheme minimizes arbitrary
additive distortion function properly deﬁned and provides better performance than
STC. The main advantage of SC compared to ALP is that it always provides a valid
stego medium. The simulation results show the good performance of polar codes in
terms of embedding efﬁciency. Additionally, since SC is the natural and basic decoding
technique of polar code, it is important to design steganographic embedding scheme
Fig. 3. Embedding efﬁciency of the steganographic SC polar decoder SC_PCS.
Polar Coding Steganographic Embedding Using Successive Cancellation
199

based on it. This allows future improvements if others decoding methods based on SC
are improved.
The application of SC algorithm in steganography opens interesting perspective to
improve further the embedding efﬁciency and move closer the optimal bound. Then,
we plan to use the list version of SC called SCL [15] albeit more complex than SC.
Other perspectives are to adapt the SC-PCS in JPEG domain and in non-binary
embedding operation with multilayered construction.
References
1. Ker, A.D., Bas, P., Böhme, R., Cogranne, R., Craver, S., Filler, T., Fridrich, J., Pevný, T.:
Moving steganography and steganalysis from laboratory to real world. In: Proceedings of the
IH&MMSec 2013. ACM, Montpellier, France, June 2013
2. Holub, V.: Content Adaptive Steganography – Design and Detection. Ph.D. thesis,
Binghamton University, May 2014
3. Pevný, T., Filler, T., Bas, P.: Using high-dimensional image models to perform highly
undetectable steganography. In: Böhme, R., Fong, P.W.L., Safavi-Naini, R. (eds.) IH 2010.
LNCS, vol. 6387, pp. 161–177. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-
642-16435-4_13
4. Holub, V., Fridrich, J.: Designing steganographic distortion using directional ﬁlters. In: 4th
IEEE International Workshop on Information Forensics and Security, Tenerife, Spain,
December 2012
5. Holub, V., Fridrich, J., Denemark, T.: Universal distortion design for steganography in an
arbitrary domain. EURASIP J. Inf. Secur. Special Issue on Revised Selected Papers of the
1st ACM IH and MMS Workshop, vol. 1, pp. 1–13, January 2014
6. Li, B., Wang, M., Huang, J., Li, X.: A new cost function for spatial image steganography. In:
International Conference on Image Processing (ICIP), Paris, France, pp. 4206–4210, October
2014
7. Sedighi, V., Cogranne, R., Fridrich, J.: Content-adaptive steganography by minimizing
statistical detectability. IEEE Trans. Inf. Forensics Secur. 11(2), 221–234 (2016)
8. Crandall, R.: Some notes on steganography. Steganography Mailing List (1998)
9. Westfeld, A.: F5—A Steganographic Algorithm. In: Moskowitz, Ira S. (ed.) IH 2001. LNCS,
vol. 2137, pp. 289–302. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-45496-
9_21
10. Schönfeld, D., Winkler, A.: An embedding with syndrome coding based on BCH codes. In:
8th ACM Workshop on Multimedia and Security, pp. 214–223 (2006)
11. Fontaine, C., Galand, F.: How reed-solomon codes can improve steganographic schemes.
EURASIP J. Inf. Secur. 2009, 1–10 (2009)
12. Diop, I., Farssi, S.M., Chaumont, M., Khouma, O., Diouf, H.B.: Utilisation des codes LDPC
en stéganographie. COmpression et REprésentation des Signaux Audiovisuels (CORESA
2012), Lille, France, pp. 98–104, mai 2012
13. Filler, T., Judas, J., Fridrich, J.: Minimizing additive distortion in steganography using
syndrome-trellis codes. IEEE Trans. Inf. Forensics Secur. 6(3), 920–935 (2011)
14. Arikan, E.: Channel polarization: A method for constructing capacity-achieving codes for
symmetric binary-input memoryless channels. IEEE Trans. Inf. Theory IT 55(7), 3051–3073
(2009)
15. Tal, I., Vardy, A.: List decoding of polar codes. In: Proceedings of the IEEE International
Symposium on Information Theory Proceedings (ISIT), August 2011
200
B. Diouf et al.

16. Goela, N., Korada, S.B., Gastpar, M.: On LP decoding of polar codes. In: IEEE Transactions
Information Theory Workshop (ITW), Dublin (2010)
17. Taranalli, V., Siegel, P.H.: Adaptive linear programming decoding of polar codes. In: IEEE
Symposium on Information Theory (ISIT), pp. 2982–2986, June–July 2014
18. Diouf, B., Diop, I., Farssi, S.M., Tall, K., Fall, P.A., Diop, A.K., Sylla, K.: Using of polar
codes in steganography. In: International Conference on Advances in Computer Science and
Engineering (CSE), Atlantis Press, Los Angeles, vol. 42, pp. 262–266, July 2013
19. Diouf, B., Diop, I., Farssi, S.M., Khouma, O.: Minimizing embedding impact in
steganography using polar codes. In: IEEE International Conference on Multimedia
Computing and Systems (ICMCS 2014), Marrakesh, Morocco, pp. 105–111, April 2014
20. Diouf, B., Diop, I., Farssi, S.M., Khouma, O.: Practical polar coding method to minimize the
embedding impact in steganography. In: IEEE International Science and Information
(SAI) Conference, London, United Kingdom, July 2015
21. Diouf, B., Diop, I., Farssi, S.M.: Performances of polar codes in steganographic embedding
impact minimization. In: ICACT Transactions on Advanced Communications Technology
(ICACT-TACT), South Korea, vol. 5(5), pp. 927–935, September 2016
22. Diouf, B., Diop, I., Wone, K., Farssi, S.M., Khouma, O., Diouf, M., Tall, K.: Adaptive linear
programming of polar codes to minimize additive distortion in steganography. In:
Proceedings of the IEEE International Science and Information (SAI) Conference, United
Kingdom, London, pp. 1086–1092, July 2016
23. Arikan, E.: Systematic polar coding. IEEE Commun. Lett. 15(8), 860–862 (2011)
Polar Coding Steganographic Embedding Using Successive Cancellation
201

Session-HB: Improving the Security of HB+
with a Session Key Exchange
Ahmad Khoureich Ka(B)
Department of Computer Science,
Alioune Diop University of Bambey, Bambey, Senegal
ahmadkhoureich.ka@uadb.edu.sn
Abstract. The HB+ protocol, designed by Juels and Weis to miti-
gate forgery and counterfeiting risks on RFID tags, is well suited for
those resource-constrained devices. The protocol comes in response to
the search for a solution to improve the security of the HB protocol
published in 2001 by Hopper and Blum that was not resistant to active
attacks. However, Gilbert et al. showed that HB+ cannot resist against
a simple man-in-the-middle attack. In this paper, we propose to run a
lightweight session key exchange as a pre-protocol to establish the tag
and reader secrets for HB+. The resulting protocol denoted Session-HB
is provably resistant to man-in-the-middle attacks.
Keywords: HB+ · RFID tags · Authentication · LPN
Session key exchange
1
Introduction
The rapid progress we see today in the use of the RFID chips is due to its advan-
tages over barcodes (timeliness in data collection, no need of human involvement,
read/write for tags, etc.). RFID tags are used for animal tracking, anti-theft for
merchandise in stores, payment and access control. Some of these uses require
security, especially authentication. Since the tag can be forged, the design of
well-suited authentication protocols, which do not leak sensitive information,
is of great need. Well-suited protocols because RFID tags are resource con-
strained devises, they have no computational power and storage for standard
cryptographic tools (e.g. RSA, AES, hash functions. etc.). This has motivated
Hopper and Blum to invent the HB protocol [12], a lightweight authentication
protocol for low cost RFID tags that has inspired many researchers to pro-
pose HB-like protocols. The HB protocol is only resistant to passive adversary
but falls in front of active ones. Its resistance to passive attacks lies on the
Learning Parity with Noise (LPN) known to be a hard problem [2–4,12,21]. To
strengthen HB, Juels and Weis introduce the HB+ protocol [13], which is secure
against passive and active attacks [13,15] but not against man-in-the-middle ones
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 202–212, 2018.
https://doi.org/10.1007/978-3-319-72965-7_19

Session-HB
203
e.g. GRS attack [9]. Since that time many researchers have published proto-
cols [5–7,10,16,18] they claim resistant to man-in-the-middle attacks but many
of them have weaknesses [8,11,19].
In this paper, we propose Session-HB a new protocol that follows the same
framework as hHB introduced by Ka [14]. hHB is a two stages protocol; in the
ﬁrst stage the reader sends a session key to the tag and in the second stage the
reader do r HB+ rounds to authenticate the tag. Although hHB has explicit secu-
rity proofs against man-in-the-middle attacks, its transmission cost is unaccept-
ably high for resource-constrained devices [1]. This drawback of hHB is mainly
due to the transmission of the shared secrets by the reader. Using a pre-protocol
to renew the tag and reader secrets is not a new idea. We ﬁnd it in the work
of Bringer et al. [6] named HB++. This latter protocol is also a tentative to ﬁx
the shortcoming of HB+ that is its weakness against the GRS attack [9]. But
HB++ doesn’t keep its promise to resist to man-in-the-middle attacks [11]. The
proposal Session-HB improves the transmission cost of the secrets and at the
same time is provably resistant to man-in-the-middle attacks.
This paper is organized as follows: in Sect. 2, we brieﬂy present the LPN
problem and describe the HB+ protocol and its weakness. Section 3 exposes
our proposal Session-HB, its security arguments and parameter values. Finally,
Sect. 5 gives the conclusion.
2
The HB+ Protocol
At Crypto 2005 Juels and Weis presented HB+ an improvement of the HB pro-
tocol [12] which exploits the hardness of the Learning from Parity with Noise
(LPN) problem. The HB+ protocol is secure against passive and active attacks.
2.1
LPN Problem
The LPN is the problem of ﬁnding the k-bit string x from the following system
of noisy equations.
⎧
⎨
⎩
a0 · x = z0 ⊕ν0
· · ·
an · x = zn ⊕νn
where ai ←{0, 1}k, zi = ai · x and νi ∼Berε the Bernoulli distribution with
parameter ε ∈]0, 1/2[, (i.e. if ν ←Berε then Pr[ν = 1] = ε and Pr[ν = 0] = 1−ε).
More formally, let Ax,ε be the distribution deﬁned by:
{a ←{0, 1}k; ν ←Berε : (a, ⟨x, a⟩⊕ν)}
The LPN problem is to distinguish oracle access to Ax,ε from oracle access to
the uniform distribution on (k + 1)-bit strings. The LPN is known to be a hard
problem [2,3,21] and the best known algorithms for solving it have running time
of 2Θ(k/ log k) [3].

204
A. K. Ka
2.2
HB+ Design and Weakness
HB+ is a lightweight protocol with a very simple design, see Fig. 1. Its resistance
to active attacks comes from the introduction of a random blinding factor b. The
reader and the tag share two secrets x ∈{0, 1}k1 and y ∈{0, 1}k2. A round of
HB+ consists of the following steps:
1. The tag randomly selects a blinding factor b ←{0, 1}k2 and sends it to the
reader.
2. The reader responds with a randomly selected challenge vector a ←{0, 1}k1.
3. The tag selects ν according to Berε then computes and sends to the reader
the bit z = a · x ⊕b · y ⊕ν.
Fig. 1. A round of the HB+ protocol.
The entire authentication process consists of executing r times the HB+
round. The reader recognizes the outcome yes or no (Verify a · x ⊕b · y = z)
of each round. If the number of no does not exceed a threshold u, the tag is
authenticated. One consequence of the probabilistic nature of the authentication
is that a honest tag can be rejected by a honest reader (False Rejection) or a
counterfeit tag be accepted (False Acceptance). Fortunately, false rejection and
false acceptance happen with negligible probabilities in k1 (because r = r(k1)):
PF R =
r

i=u+1
r
i

εi(1 −ε)r−i,
PF A = 1
2r
u

i=0
r
i

The main weakness of HB+ is that it succumbs to man-in-the-middle attacks.
A simple man-in-the-middle attack named GRS attack [9] has been successfully
mounted against HB+. The GRS attack consists of adding a perturbation ei
(the vector with all 0s but 1 at position i) to the challenge vector a and observe
the result of the authentication process of a honest tag. This perturbation is
eﬀective if ei · x = 1. Thus if the authentication succeeds with a probability
greater than PF A, it means that the bit at the position i of x is 0 otherwise it
is 1. The GRS attack is simple and has motivated many researchers to propose
solutions for the HB+ protocol [5–7,16,18] but many of them show weaknesses
in their design [8,11,19].

Session-HB
205
3
The Proposal Session-HB
Session-HB follows the idea developed by hHB [14] and HB++ [6] that is to use
a pre-protocol to renew the tag and reader secrets in the HB+ protocol. Session-
HB introduces a lighter session key exchange than the one of hHB and unlike
HB++ is resistant to man-in-the-middle attacks.
3.1
First Stage of Session-HB: A Lightweight Session Key Exchange
The lightweight session key exchange protocol we introduce here is intended to
constitute the ﬁrst stage of Session-HB. The tag and the reader share two k-
bit secrets s1 and s2. The following steps describe the protocol (see Fig. 2 for a
graphical representation):
Fig. 2. The ﬁrst stage of Session-HB for the establishment of session keys x and y used
in the second stage.
1. The reader selects ξ and τ
randomly from {0, 1}k, computes s′
1
=
MixBits(s1, ξ) and τ ′ = τ ⊕s′
1. MixBits is a mixing function that is used
to randomize the positions where s′
1 and s2 have the same bits thus making
the extraction of the secrets x and y at the ﬁnal step random. The mixing
function also helps to consider s′
1 as a one-time pad even if it’s not a perfect
one. After that the reader concatenates ξ and τ ′ and transmits the result to
the tag.
2. Upon receiving ξ||τ ′, the tag computes s′
1 = MixBits(s1, ξ) and retrieve τ.
3. The reader compares s′
1 and s2 and extracts the session keys x and y from
τ. The extraction is done as follow: if at some position i, s′
1 and s2 have the
same bit, the bit of τ at that position belongs to x otherwise it belongs to y.
For example if s′
1 = 00101000, s2 = 01110101 and τ = 10010001 then x = 100
and y = 01001. The tag do the same as the reader to obtain the keys x and
y. Note that the size of x and y are around k/2 as stated in the following
theorem.
Theorem 1. Let s1 and s2 be two binary strings of length n as in our lightweight
session key exchange. If the binary string ξ||τ ′ sent by the reader to the tag in the
second step of the lightweight session key exchange is not modiﬁed by an active
attacker then the length l of the extracted session key x satisﬁes l = Θ(n/2).

206
A. K. Ka
Proof (Sketch of the proof). Let s′
1 = a1, . . . , an and s2 = b1, . . . , bn. Since s1 and
s2 are randomly and independently selected from {0, 1}k, s′
1 and s2 are sequences
of independent and identically distributed random variables of expected values
1/2 and variances 1/4. Let
Xi =

0
if ai ̸= bi
1
if ai = bi.
Xi is a random variable with expected value 1/2 and variance 1/4. The size of
the extracted session key x is equal to the sum X1 + X2 + . . . + Xn and by the
law of large numbers, for any ε > 0, Pr(| l
n −1
2| < ε) →1 as n →∞. This means
that for large n the size of x is around n/2.
3.2
Second Stage of Session-HB
The secrets x and y are obtained from the ﬁrst stage. Their lengths are not ﬁxed
(|x| = Θ(k/2) and |y| = Θ(k/2) where k = |s1| = |s2|). Therefore the second
stage of Session-HB is identical to the HB+ protocol with some little adjustments
(see Fig. 3 for a graphical representation).
Fig. 3. A single round of the second stage of the Session-HB authentication protocol.
It is executed r times for the authentication of the tag.
The steps of one round of the second stage of the Session-HB are the following:
1. The tag randomly selects a blinding factor b ←{0, 1}⌈k/2⌉and sends it to the
reader, ⌈k/2⌉is the ceil of k/2.
2. The reader responds with a randomly selected challenge vector a
←
{0, 1}⌈k/2⌉.
3. Instead of computing a · x ⊕b · y as in the HB+ protocol, the tag com-
putes Φ(a, x) ⊕Φ(b, y) where Φ is a very simple and lightweight function, see
Algorithm 1. So the tag sends to the reader the bit z = Φ(a, x) ⊕Φ(b, y) ⊕ν.
4. The reader accepts the round if z = Φ(a, x) ⊕Φ(b, y).
The second stage of Session-HB consists of r executions of the above steps.

Session-HB
207
Algorithm 1. Function Φ
function Φ(u, v)
n = |u|
▷u = u1 . . . un
m = |v|
▷v = v1 . . . vm
if n < m then
i = ⌈m−n
2
⌉+1,
j = i + n −1
▷⌈m−n
2
⌉: ceil of m−n
2
v′ = vi · · · vj.
return u · v′
end if
if n >= m then
i = ⌈n−m
2
⌉+1,
j = i + m −1
u′ = ui · · · uj.
return u′ · v
end if
end function
3.3
Parameter Values for Session-HB
For the mixing function in the ﬁrst stage of Session-HB, we opted for the one
proposed in [20] which is extremely lightweight. It uses only bitwise right shift
and additions as shown below.
The MixBits function
Z = MixBits(X,Y)
Z = X;
for(i=0; i<32; i++) {
Z = (Z>>1) + Z + Z + Y ;
}
We set the size of the pre-shared keys s1 and s2 to 128 bits (|s1| = |s2| =
k = 128 bits), hence the size of the secrets in the second stage will be around
64 (|x| = |y| = Θ(64)). These values do not follow the recommendations of [17]
but we think it does not weaken the security of the protocol since x and y are
session keys. The false acceptance and the false rejection rates are respectively
functions of (r, u) and (r, u, ε). We deﬁne r = 1164, u = 0.348 × r and ε = 0.25
and get PF A = 2−80 and PF R = 2−40.
Table 1. Parameters, storage and transmission costs of some protocols
Protocol
Parameters
Storage costs
Transmission costs
HB+ [13]
ε = 0.25; kx = 80; ky = 512; r = 1164
592
690252
HB++ [6]
ε = 0.25; k = 768; r = 731
768
118582
hHB [14]
ε = 0.25; ks = 256; ky = 512; r = 1164
768
752703
GHB# [22]
ε = 0.25; kX = 80; kY = 512; m = 1164
689088
1756
Session-HB
ε = 0.25; k1 = 128; k2 = 128; r = 1164
256
150412

208
A. K. Ka
With these settings the transmission cost of Session-HB is signiﬁcantly lower
than that of [14] (see Table 1) and one of the lowest in the family of HB-like
protocols [1].
4
Security Arguments
In this section, we prove that Session-HB is secure against active and man-in-
the-middle attacks.
4.1
Security Deﬁnitions
Active attacks. Such attacks are performed in two stages: a learning phase in
which the adversary interacts with a honest tag a polynomial number of times
hoping a leak of information about the secrets, and a veriﬁcation phase in which
the adversary tries to authenticate to the reader.
Man-in-the-middle attacks. These are the most powerful attacks against an
authentication protocol. The adversary can tamper with messages exchanged
between the tag and the reader in a polynomial number of instances of the
protocol. Then, he can analyse their eﬀect on the reader’s decision (accepting
or rejecting the tag) in order to gain information about the secrets. Finally,
with the information supposedly obtained on the secrets, the adversary tries to
authenticate to the reader.
4.2
Security of Session-HB Against Active Attacks
Theorem 2. If HB+ with parameters ε ∈]0, 1
2[, r = r(k) and u > ε · r is secure
against active attacks then Session-HB with the same settings of parameters is
secure against active attacks.
Proof. In this proof we reduce HB+ to Session-HB. That is we prove that if
there is A a probabilistic polynomial-time adversary that can break Session-HB
by an active attack, then we can construct A′ a probabilistic polynomial-time
algorithm that can break HB+ by the same type of attack.
Now let’s consider A be a probabilistic polynomial-time adversary interacting
with the tag in at most q executions of the Session-HB protocol. Suppose A can
break Session-HB by an active attack with success probability at least ε. We use
A to construct a probabilistic polynomial-time algorithm A′ that performs an
active attack on HB+. Let ε′ be the success probability of A′. For the learning
phase of the attack, A′ uses its interactions with a honest HB+ tag to emulate
for A a Session-HB tag for q times as follow:
First, A′ receives from A a bit string representing the concatenation ξ||τ ′ accord-
ing to the ﬁrst stage of the Session-HB Protocol. Second, A and A′ repeat the
following three steps r times (the number of rounds of the Session-HB protocol).

Session-HB
209
1. A′ responds to A with the blinding vector b received from the HB+ tag,
2. A sends a challenge vector a to A′ which in turn forwards it to the HB+ tag,
3. A′ sends the response z of the HB+ tag to A.
For the veriﬁcation phase of the attack, A′ simulates a Session-HB reader only
once for A. First, A′ sends a bit string of length |ξ||τ ′| according to the ﬁrst stage
of the Session-HB protocol. Second, A′ performs with A the following steps r
times:
1. A′ receives from A a blinding vector b and forwards it to the HB+ reader,
2. A′ receives a challenge vector a from the HB+ reader and forwards it to A,
3. A′ receives from A a response z and forwards it to the HB+ reader.
Taking into account the way that A′ uses A, one can see that the latter adversary
cannot distinguish if he performs an active attack on Session-HB or he is executed
as a subroutine by A′. Thus the success probability of A attacking Session-HB
equals the success probability of A′ attacking HB+. That is ε = ε′. Since ε′ is
negligible because HB+ is secure against active attacks, then ε is negligible. This
concludes the proof.
4.3
Security of Session-HB Against MITM Attacks on the First
Stage of the Protocol
Here we prove the security of Session-HB when an adversary A performs a
man-in-the-middle attack on its ﬁrst stage. The ﬁrst stage of Session-HB is a
lightweight session key exchange, see Fig. 2. We deﬁne the following game:
Setup: The tag and the reader have the secrets s1 and s2.
Attack: A has access to a legitimate tag and a legitimate reader and interacts
with them q times where each interaction is as follow:
A receives a commitment c = ξ||τ ′ from the reader and sends c ⊕c′ to the tag.
After that, A leaves the second stage of Session-HB to proceed normally. The
reader outputs accept if at least for r −u rounds of the second stage Φ(a, x) ⊕
Φ(b, y) = z (see Fig. 3).
Winning Condition: A wins the game if he makes the reader outputs accept
for an interaction where c′ ̸= 0.
We now show that an adversary has a negligible success probability of winning
the above game, which means that an adversary cannot gain information about
the secrets s1 and s2.
Theorem 3. If the adversary cannot win the above game with non negligible
probability then he cannot succeed in breaking Session-HB with a man-in-the-
middle attack on its ﬁrst stage.
Proof. Clearly, if an adversary modiﬁes the concatenation ξ||τ ′ sent by the reader
to the tag in the ﬁrst stage of Session-HB, the extracted secrets x and y will
be diﬀerent on both sides (tag and reader). Thus in the second stage (which is
similar to the HB+ protocol), from the point of view of the reader, the tag is

210
A. K. Ka
sending random responses to challenges. Therefore the probability of winning
the game corresponds to the probability of false accept which is negligible. This
means the adversary gains no information on the long-lived secrets s1 and s2.
The theorem follows.
4.4
Security of Session-HB Against MITM Attacks on the Second
Stage of the Protocol
Here we make an heuristic analysis of the security of Session-HB when a man-
in-the-middle attack is mounted on its second stage.
The second stage of Session-HB is similar to HB+. The view of the adversary
when he perturbs q instances of the protocol by adding α, β and γ respectively
to a, b and z is close the view of an adversary doing the same against HB#
(an HB-like protocol introduced by Gilbert et al. [10]). Because for Session-
HB, these interactions lead to equations involving q · r challenge pairs (a, b), q
secret pairs (x, y) and q reader’s decisions (r is the number of rounds) while
for HB# it is q challenge pairs (a, b), r secret pairs (x, y) (in the form of a
pair of matrices (X, Y ) of r columns each) and q reader’s decisions (accept or
reject). The single successful attack [19] we know against HB# perturbs the
protocol by superimposing α, β and γ obtained from eavesdropping a previous
instance of the protocol to other instances. This attacks works on HB# because
the eavesdropped instance uses the same secrets X and Y . Therefore we believe
that, it cannot works on Session-HB because each instance of the protocol uses
a diﬀerent secret pair (x, y). Also, whatever the means used to ﬁnd the session
secrets x or y or both, it is highly unlikely to compute the long-lived secrets s1
and s2 from x and y because they are not directly related.
5
Conclusion
We have presented here a new protocol named Session-HB. It follows an idea,
implemented in the protocols HB++ and hHB, which consists of renewing the
HB+ secrets at each authentication process. This was done by introducing a new
lightweight session key exchange between the tag and the reader. Contrary to
hHB, Session-HB has a signiﬁcant lower transmission cost which is one of the
lowest in the family of HB-like protocols. Unlike HB++, Session-HB is provably
secure against man-in-the-middle attacks.
References
1. Armknecht, F., Hamann, M., Mikhalev, V.: Lightweight authentication protocols
on ultra-constrained RFIDs - myths and facts. In: Saxena, N., Sadeghi, A.-R. (eds.)
RFIDSec 2014. LNCS, vol. 8651, pp. 1–18. Springer, Cham (2014). https://doi.org/
10.1007/978-3-319-13066-8 1
2. Blum, A., Furst, M., Kearns, M., Lipton, R.J.: Cryptographic primitives based on
hard learning problems. In: Stinson, D.R. (ed.) CRYPTO 1993. LNCS, vol. 773, pp.
278–291. Springer, Heidelberg (1994). https://doi.org/10.1007/3-540-48329-2 24

Session-HB
211
3. Blum, A., Kalai, A., Wasserman, H.: Noise-tolerant learning, the parity problem,
and the statistical query model. J. ACM (JACM) 50(4), 506–519 (2003)
4. Blum, M., Hopper, N.J.: A secure human-computer authentication scheme. Tech-
nical report, CMU-CS-00-139, School of Computer Science, CMU (2000)
5. Bringer, J., Chabanne, H.: Trusted-HB: a low-cost version of HB+ secure against
man-in-the-middle attacks. IEEE Trans. Inf. Theory 54(9), 4339–4342 (2008)
6. Bringer, J., Chabanne, H., Emmanuelle, D.: HB++: a lightweight authentication
protocol secure against some attacks. In: IEEE International Conference on Perva-
sive Services, Workshop on Security, Privacy and Trust in Pervasive and Ubiquitous
Computing - SecPerU 2006, pp. 28–33. IEEE (2006)
7. Duc, D.N., Kim, K.: Securing HB+ against GRS man-in-the-middle attack. In:
Institute of Electronics, Information and Communication Engineers, Symposium
on Cryptography and Information Security (2007)
8. Frumkin, D., Shamir, A.: Un-trusted-HB: security vulnerabilities of trusted-HB.
IACR Cryptology ePrint Archive, p. 44 (2009)
9. Gilbert, H., Robshaw, M., Sibert, H.: Active attack against HB+: a provably secure
lightweight authentication protocol. Electron. Lett. 41(21), 1169–1170 (2005)
10. Gilbert, H., Robshaw, M.J.B., Seurin, Y.: HB#: increasing the security and eﬃciency
of HB+. In: Smart, N. (ed.) EUROCRYPT 2008. LNCS, vol. 4965, pp. 361–378.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78967-3 21
11. Gilbert, H., Robshaw, M.J.B., Seurin, Y.: Good variants of HB+ are hard to ﬁnd.
In: Tsudik, G. (ed.) FC 2008. LNCS, vol. 5143, pp. 156–170. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-85230-8 12
12. Hopper, N.J., Blum, M.: Secure human identiﬁcation protocols. In: Boyd, C. (ed.)
ASIACRYPT 2001. LNCS, vol. 2248, pp. 52–66. Springer, Heidelberg (2001).
https://doi.org/10.1007/3-540-45682-1 4
13. Juels, A., Weis, S.A.: Authenticating pervasive devices with human protocols. In:
Shoup, V. (ed.) CRYPTO 2005. LNCS, vol. 3621, pp. 293–308. Springer, Heidelberg
(2005). https://doi.org/10.1007/11535218 18
14. Ka, A.K.: hHB: a harder HB+ protocol. In: SECRYPT 2015 - Proceedings of the
12th International Conference on Security and Cryptography, pp. 163–169 (2015)
15. Katz, J., Shin, J.S.: Parallel and concurrent security of the HB and HB+ protocols.
In: Vaudenay, S. (ed.) EUROCRYPT 2006. LNCS, vol. 4004, pp. 73–87. Springer,
Heidelberg (2006). https://doi.org/10.1007/11761679 6
16. Leng, X., Mayes, K., Markantonakis, K.: HB-MP+ protocol: an improvement on
the HB-MP protocol. In: IEEE International Conference on RFID 2008, pp. 118–
124. IEEE (2008)
17. Levieil, ´E., Fouque, P.-A.: An improved LPN algorithm. In: De Prisco, R., Yung,
M. (eds.) SCN 2006. LNCS, vol. 4116, pp. 348–359. Springer, Heidelberg (2006).
https://doi.org/10.1007/11832072 24
18. Munilla, J., Peinado, A.: HB-MP: a further step in the HB-family of lightweight
authentication protocols. Comput. Netw. 51(9), 2262–2267 (2007)
19. Ouaﬁ, K., Overbeck, R., Vaudenay, S.: On the security of HB# against a man-in-
the-middle attack. In: Pieprzyk, J. (ed.) ASIACRYPT 2008. LNCS, vol. 5350, pp.
108–124. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-89255-
7 8

212
A. K. Ka
20. Peris-Lopez, P., Hernandez-Castro, J.C., Tapiador, J.M.E., Ribagorda, A.: Advances
in ultralightweight cryptography for low-cost RFID tags: Gossamer protocol. In:
Chung, K.-I., Sohn, K., Yung, M. (eds.) WISA 2008. LNCS, vol. 5379, pp. 56–68.
Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-00306-6 5
21. Regev, O.: On lattices, learning with errors, random linear codes, and cryptogra-
phy. In: Proceedings of the Thirty-Seventh Annual ACM Symposium on Theory
of Computing, STOC 2005, pp. 84–93. ACM (2005)
22. Rizomiliotis, P., Gritzalis, S.: GHB
#: a provably secure HB-like lightweight
authentication protocol. In: Bao, F., Samarati, P., Zhou, J. (eds.) ACNS 2012.
LNCS, vol. 7341, pp. 489–506. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-31284-7 29

Increase MIMO Systems Performances by Concatenating
Short Polar Codes to Spatial Time Block Codes
Madiop Diouf
(✉), Idy Diop, Ibra Dioum, Birahime Diouf, Khaly Tall,
Sidi Mohamed Farsi, and Lamine Sane
Department of Computer Science,
Polytechnic Institute (ESP)/Cheikh Anta DIOP University (UCAD), Dakar, Senegal
{madiop.diouf,idy.diop,ibra.dioum,khaly.tall,
lamine.sane}@esp.sn, dioufbira11@yahoo.fr, farsism@yahoo.com
Abstract. Polar codes, proposed by Erdal Arikan, have attracted a lot of interest
in the ﬁeld of channel coding for their capacity-achieving trait as well as their low
encoding and decoding complexity in order O (NlogN) under successive cancel‐
lation (SC) decoder. However, there remains one signiﬁcant drawback, that is,
the error correction performance of short and moderate length polar codes is
unsatisfactory, especially when compared with low-density parity check (LDPC)
codes and turbo codes. In this paper, we propose a concatenation scheme perform‐
ance, which employs a short polar encoder following to Spatial Time Block Codes
(STBC), and we develop an eﬃcient detector for Multiple Input Multiple Output
(MIMO) antennas, which adaptively combines Minimum Mean Square Error
Successive Interference Canceller together (MMSE-SIC). We also compared to
Maximum Likelihood in the literature and ﬁnally present a simulation results in
binary input Additive White Gaussian Noise (BI-AWGN) with binary phase shift
keying (BPSK) modulation, and we observe that, our proposed concatenation
scheme signiﬁcantly outperforms the Maximum Likelihood performance in the
high Signal-to-Noise-Ratio (SNR).
Keywords: Polar codes · STBC · BPSK · MIMO · MMSE-SIC · BER · FER
1
Introduction
Polar codes, proposed by Arikan [1] are a big breakthrough in coding theory, which
proven to be capacity achieving for any given Binary Discrete Memoryless Channel
(BDMC) W based on the phenomenon called channel polarization. Moreover, they can
be implemented with a simple encoder and a simple SC decoder, both with low
complexity of the order of O(NlogN), where N is the code block length. Due to those
excellent proprieties, polar codes have arisen a lot of research interest among researchers
especially the combination with MIMO antennas for data transmission [2]. Unfortu‐
nately, the performance of long length polar codes is generally used.
STBC provides full spatial diversity in the collocated MIMO systems, but it doesn’t
have the coding gain over fading channels. In the documentation many approach of
concatenate STBC techniques to other codes have been proposed [3, 4]. In [5, 6], a
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 213–221, 2018.
https://doi.org/10.1007/978-3-319-72965-7_20

concatenation scheme of good encoding and decoding named Polar codes with STBC
called Polar-STBC of long length have been discussed and achieved suﬃcient gain due
to this concatenation. In [7] authors propose the antennas detection and reduce the
complexity of the receiver by oﬀering Maximum Likelihood detection algorithm. In [8]
a linear ﬁlter detection MMSE-SIC using a small polar code, which allowed reducing
the complexity while maintaining the BER, performance is presented. In [6] long polar
codes concatenate to STBC gives good BER performance when ML is applied to the
detector.
In this paper, we analysis the BER and FER performances of short polar codes
concatenated to STBC when MMSE-SIC detector is set to the output. We compare the
result with a Maximum Likelihood Detector (MLD) as described in [6]. We are working
on short codes because their hardware implementation is more easily.
The rest of the paper is organized as follows. Section 2 gives a brief review of Polar
codes. The system model Polar-STBC and the Soft Output detector are presented in
Sect. 3. Section 4 gives ﬁrstly numerical simulations of the Polar-STBC and STBC only
systems using MMSE-SIC at the receiver. While Sect. 5 conclude the document.
2
Polar Codes
The polar encoding can be represented in [5] as,
xN
1 = uN
1 GN
(1)
Where uN
1 is the source code, xN
1 is the encoded code, and we call GN the generator
matrix of polar codes of code length N. From the basic facts of channel polarization, we
know that, some of the polarized channels are used to transmit the information bits,
whereas the remaining is used to transmit the frozen bits. Alternatively, we can denote
the polar encoding with another expression, namely,
xN
1 = uAGN(A) ⊕uc
AGN(Ac)
(2)
Where uA and uc
A denote the part of the source code which contains information bits
respectively for an arbitrary set A ⊂{1, 2, … , N}, and Ac denotes the complementary
set of A. Finally GN(A) and GN(Ac) denotes the sub matrix of GN generated by the row
with indices in A and Ac respectively.
The construction of polar codes is based on channel polarization [5, 8].
3
System Model
In our system model, we consider a transmission scheme using MIMO communication
system with LT antennas at the transmitter and LR transmitter at the receiver. The channel
is assumed to be in ﬂat fading, Rayleigh channel, with Additive White Gaussian Noise
Channel (AWGN). We propose double encoding, a small polar coding following to
STBC, after their concatenation. The items are sent to the MIMO systems. To
214
M. Diouf et al.

cooperative diversity system, Rayleigh channel and AWGN are also use. We used the
same encoding oﬀer to the ﬁrst section (Fig. 1).
Fig. 1. The system detailed diagram block
After polar encoding, these polar code words are STBC encoded and fed to the Lt
transmitting antennas by using
(
x1 −x∗
2
x2 x∗
1
)
(3)
In STBC, the information bits to be transmitted are ﬁrst divided into two parts, one
part for selecting transmit antenna pair, and the other part for BSPKS modulation, to get
two modulation symbols x1, x2. The row of matrix corresponds to the transmit time slot
and the column of the matrix to the transmit antenna. In the ﬁrst time slot, x1, x2 are
respectively transmitted by two active transmit antennas, and −x∗
2, x∗
1 are transmitted by
the same transmit antenna pair in second time slot.
For NR = 4, there are two diﬀerent codebooks 1, 2, which can be denoted as
1 =
{[
x1
x2
−x∗
2 x∗
1
0 0
0 0
][
0 0
0 0
x1
x2
−x∗
2 x∗
1
]}
2 =
{[
0 x2
0 −x∗
2
x2 0
x∗
1 0
][
x2 0
x∗
1 0
0 x1
0 −x∗
2
]}
ej𝜃
Where each codebook has two diﬀerent codewords i,j j = 1, 2, and the codewords
in the same codebook do not have overlapping non-zero column, 𝜃 is a rotation angle,
which can be optimized for a given modulation format to ensure maximum diversity
and coding gain. It is assumed the 2 × Nr codeword  is transmitted over a NTxNR
Rayleigh ﬂat fading MIMO channel H, which remains constant in two consecutive
symbols intervals.
lthe received signal can be written as:
Increase MIMO Systems Performances
215

y = √ρH𝐱+ 𝐧
(4)
where ρ is the average SNR at the each antenna, and n is the 2 × Nr noise matrix. The
entries of both H and n are independent an identically distributed (i.i.d) and
y = [y1, … , yLR]T ∈ℂLRx1 is the received signal vector, x = [x1, … , xLR]T ∈ℂLRx1 is the
transmitted symbol vector.
In the noisy channel here the BER is mainly expressed as a function of the normalized
by Eb
/N0, (Energy per bit to the noise power spectral density ratio). Here in BPSK
modulation and AWGN channel, the BER as the function of the Eb
/N0 is given by:
BER = 1
2erf(
√
Eb
/
N0)
(5)
After these two encoding, our contribution is to set the detector MMSE-SIC to the
output, the proposed algorithm is presented to the previous section. Simulation results
of Bit Error Rate (BER) result versus Signal to Noise Ratio (SNR) shown that the
MMSE-SIC is a good detector for short polar code. To support this theory, we compare
our result to Maximum Likelihood Detector (MLD) as described by Zhao et al. in [6].
3.1
Review of MMSE-SIC
MMSE-SIC has been one of the most popular suboptimal detector for MIMO systems,
which outperforms considerably Zero Forcing (ZF) and MMSE at the expense of a mild
increase in the computational complexity [10]. See top of this page for detailed algo‐
rithm, in which ̄H[i] ∈ℂ(LR−i+1)X(LR−i+1) represents the channel of the residual data stream
at the ith step and  denote the set of transmit antennas with the largest Signal to Noise
Ratio (SNR). Specially at the ith step, there are (i −1) data stream to be cancelled in the
previous steps.
The MMSE-SIC principle is to detect signal in one iteration by nulling out other co-
Channel Interference. The idea is to use MMSE detector in order to exploit this
combining weight matrix [10]. If the signal is detected, it is immediately fed back to the
linear combining process and its contribution is cancelled from the received signal in
the next detection iteration and found the next minimum MSE as shown [8–10].
216
M. Diouf et al.

MMSE-SIC detects the data stream of the maximum SNR and subtracts a replica
signal of the detected symbol from the received signal. This process is repeated until the
last transmitted data is cancelled.
This algorithm allows us to determine the ﬁrst BER and probably FER performances.
3.2
Noise Improving in MMSE-SIC
In this subsection, an analysis about the noise enhancement at each step of MMSE-SIC
is proved [11]. We noted Cn[i] ∈ℂ(LR−i+1)X(LR−i+1) as the covariance matrix of the noise
compenent at the ith step. Then, we have
Rn[i] = σ2
nP[i]H↓[i] ̂H[i]P[i],
(6)
Rn[i] = σ2
nV↓[i]D−1[i]V[i],
(7)
Where
D−1[i] = Diag(⋋1[i], ⋋2[i], … , ⋋1LT −i + 1[i])
(8)
⋋l[i] =
(⋋l[i] + σ2
n)2
⋋l[i]
(9)
With ⋋l[i] satisfying
⋋l[i] ≥⋋l[i] ≥… ≥⋋1LT −i + 1[i]
(10)
And Vn[i] ∈ℂ(LR−i+1)X(LR−i+1) denotes the unitary matrix that diagonalizes H↓[i] ̂H[i]
and ⋋l[i] represents an eigenvalue of H↓[i] ̂H[i]. The number of negligeable ⋋l[i] repre‐
sents also the number of noise enhancement directions.
Increase MIMO Systems Performances
217

Simulation results of Bit Error Rate (BER) versus Signal to Noise Ratio (SNR)
shown that the MMSE-SIC is a good detector for short polar code. To support this theory,
we compute the FER performances and ﬁnally we compare our result to Maximum
Likelihood Detector (MLD) as described by Zhao et al. in [6].
3.3
Maximum Likelihood (ML) Detector
The maximum likelihood detector provides BER performances of all MIMO detectors,
but at exceedingly high complexity [6, 7]. For Q ary modulation and LR transmit antenna
system, the number of symbol set combination is QLR, leading to exponential complexity.
ML detection performs a search of the closet symbol combination sent to receiver there‐
fore, its solution is given by:
̂y = argminy∈SLT ‖x −Hy‖2
(11)
4
Simulation Results
In this ﬁrst section, we present the BER results for the proposed MMSE-SIC receiver
using Polar-STBC.
In Fig. 2, BER performances is analysis at 6 × 10−2 for Polar-STBC using Nt = 2
and Nr = 2 vs STBC only using Nt = 2 and Nr = 2 and Nt = 2 and Nr = 3.
Fig. 2. BER performance between small polar-STBC and STBC only
We noted a big improvement when the BER is 6 × 10−2, the SNR for Polar-STBC
2 * 2 MIMO antennas is about 1 dB improvement over the STBC only using Nt = 2 and
Nr = 2 and better than STBC only using Nt = 2 and Nr = 3 around 0.3 dB improvement.
At the other hand we introduces the frame error rate performance versus SNR per
receiver antenna (e.g. SNR = 2 ∗Es
N0
) in Fig. 3.
218
M. Diouf et al.

Fig. 3. FER performance between small polar-STBC and STBC only
The FER allows to judge eﬃciency of coding, more its weak, better it is. The FER
represents the number of erroneous messages after detection on the number of trans‐
mitted messages.
Its shown in Fig. 3 that the FER at 6 × 10−2, a slight improvement of Polar-STBC
using Nt = 2 and Nr = 2 versus OSTBC only using Nt = 2 and Nr = 2 about 1.7 dB, but
also outperform STBC only using Nt = 2 and Nr = 3 MIMO around 0.4 dB. These results
illustrate that the STBC used in MIMO provide transmit diversity communication over
fading channel, but also the coding gain is improved by using polar channel coding.
After simulation result, we compared to ML detection paper [6]. For instance, we
added number of transmitters Nt = 4 and number receiver antennas Nr = 4.We observe
that our proposed scheme concatenation signiﬁcantly outperforms the polar-STBC with
ML all over if the number of antenna is the same.
We noted that at BER = 10−2 the proposed outperform to 1 dB compared to polar-
STBC (N = 4096) with STBC (4 × 4 antennas) using ML detector which proves that
our proposal is also a good candidate for MIMO antennas (Fig. 4).
Increase MIMO Systems Performances
219

Fig. 4. BER performance concatenation Polar-STBC with detector ML and MMSE-SIC
5
Conclusion
In this paper, we presented a combination of STBC to short polar length codes when
MMSE-SIC is applied to the detector. This soft output is compared to ML by improve
the number of transmit and receiver’s antennas. The proposed scheme permits to achieve
near optimal BER performance over highly correlated performance MIMO channels at
much reduced complexity. This proposition opens many perspectives such as architec‐
ture implementation for small Polar-STBC codes.
References
1. Arikan, E.: Channel Polarization: a method for constructing capacity-achieving codes for
symmetry binary-input memoryless channel. IEEE. Inf. Theory 55(7), 3051–3073 (2009)
2. Gesbert, D., Shaﬁ, M., Shiu, D., Smith, P., Naguib, A.: From theory to practice: an overview
of MIMO space-time coded wireless systems. IEEE J. Sel. Areas Commun. 21(3), 281–302
(2003)
3. Mehranand, F., Maunder, R.G.: Wireless MIMO systems employing joint Turbo-like STBC
codes with bit-level algebraically-interleaved URSCs. In: Proceedings of IEEE International
Wireless Symposium, April 2013
4. Futaki, H., Ohtsuki, T.: LDPC-based space-time transmit diversity schemes with multiple
transmit antennas. In: Proceedings of the 57th IEEE Semiannual Vehicular Technology
Conference (VTC), 2003-Spring, vol. 4, pp. 2589–2593, April 2003
5. Hou, J., Siegel, P.H., Milstein, L.B.: Design of multi-input multi-output systems based on
low-density parity-check codes. IEEE Trans. Commun. 53(4), 601–611 (2005)
220
M. Diouf et al.

6. Zhao, S., Sun, Q., Feng, M.-K., Zheng, B.: A concatenation scheme of polar codes and space-
time block codes in multiple-input multiple-output channels. In: 6th International Congress
on Image and Signal Processing (CISP 2013) (2013)
7. Jegadeeswari, E., Arunachalaperumal, C., Bavithra, K.B.: Design of low complexity polar
coded spatial modulation. Middle-East J. Sci. Res. 23(5), 981–986 (2015). IDOSI
Publications
8. Diouf, M., Diop, I., Dioum, I., Farssi, S.M., Diouf, B., Tall, K.: Soft output detection for
MIMO systems using binary Polar codes. In: 5th International Conference on Multimedia
Computing and Systems – IEEE Conference, Marrakech, Morocco, 29 September–1 October
2016. To appear in December 2016
9. Foschini, G.J., Golden, G.D., Valenzuela, R.A., Wolniansky, P.W.: Simpliﬁed processing for
high spectral eﬃciency wireless communication employing multi-element arrays. IEEE J.
Sel. Area Commun. 17, 1841–1852 (1999)
10. Diouf, M., Diop, I., Dioum, I., Farssi, S.M., Diouf, B., Tall, K.: Study of Polar codes MIMO
channels polarization using MIMO system. In: Computing Conference Science and
Information, London, pp. 681–690 (2016)
11. Tran, X.N., Fujino, T.: Combined MMSE-SIC multiuser detection for STBC-OFDM. IEICE
Trans. Commun. E89-B(5), 280–283 (2006)
Increase MIMO Systems Performances
221

Towards a Model of Integration of Underserved
Cultural Factors in Software by Reverse
Localisation: Case Study in Yemba Culture
Mathurin Soh(B)
Department of Mathematics and Computer Science,
University of Dschang, P.O. Box 67, Dschang, Cameroon
mathurin.soh@univ-dschang.org
Abstract. This paper aims at contributing towards an empowerment of
underserved culture through computer applications. It is inspired by the
conviction that successful strategies can lead to balanced chances for all
cultures in the context of computer applications. It raises and investigates
the problem of cultural divide observable in computer applications and
pave the way to the preservation of underserved cultures with such tools.
We ﬁnd that reverse software localisation approach as deﬁned by [1]
can help under-resourced cultures not to be engulfed by the dominating
Western way of building computer applications. An approach to integrate
underserved cultural factors in computer applications is provided and
consist of investigation of cultural markers, followed by translation or
adaptation, implementation and evaluation by end-users. As a case study,
we investigate the cultural localisation of editor for the Yemba culture
in Cameroon. It gives access to the use of a text editor to users ﬂuent
in Yemba culture. The adaptation of most computer software to non-
native environments does not always capture the features of the target
culture. Despite some trade-oﬀs, this work has a strong symbolic value
in the empowerment and preservation of the Yemba culture, and for
underserved cultures.
Keywords: Cultural divide · Cultural factors · Localisation · Editor
Underserved culture · Yemba
1
Introduction
Computer applications and Information and Communication Technologies
(ICTs) tools have become daily useful and are the centerpiece of the global
information society. These software are viewed as artifacts which interact with
cultures of societies in which they function [2]. Kersten et al. [2] prove that
like any other product, these applications contains embedded designer’s cultural
values and objectives. The same authors speciﬁes that some of the embedding
occurs unconsciously, inherited via the cultural programming of its human cre-
ators; other parts of it are intentional via design requirements explicitly obtained
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 222–233, 2018.
https://doi.org/10.1007/978-3-319-72965-7_21

Integration of Underserved Culture in Software by Reverse Localisation
223
by researching its target markets [2]. End-user’s who understand the language
and culture of computer applications are able to use them, while other people
might be entirely locked out or unable to fully enjoy the beneﬁts these appli-
cations brings [3]. This way of designing and programming computer applica-
tions result in a cultural digital divide, observed in ICT tools and computer
applications between underserved cultures and western ones. The greatest chal-
lenge remains on how computer applications globally built, but locally used [4],
could be adapted to fully support end-users’ cultural features and contents, to
help underserved cultures not to be engulfed by the dominating western way of
building interfaces. Globally, this can be solved by means of software localisation,
which is the adaptation of software to linguistic, cultural and technical require-
ments of a target market. Localising user interfaces has been proven beneﬁcial
for both user satisfaction and work eﬃciency [5]. Besides, the local population
has the right to practice and refresh its own cultural traditions, customs and
knowledge while using technologies and computer applications in its own terms
and ways. Yacob [6] oﬀers a broad interpretation that deﬁnes the localisation as:
the transfer of cultural consciousness into a computer system, making the com-
puter a natural extension of the society it serves. But the adaptation of computer
software to non-native environments does not always capture the features of the
target culture. It is emphasized in [7], that successful software systems must be
written so that adapting them to a particular culture can be done easily.
This paper aims at raising and investigating the problem of preservation of
underserved cultures through computer applications and ICT tools which have
become daily useful tools. The matter of this work is to ﬁnd a model to integrate
underserved cultural factors in computer applications, information systems, in
order to preserve underserved cultures through such tools.
In the remainder of this paper, the Sect. 2 presents the cultural digital divide
problem. Because the digital divide is as a complex and dynamic phenomenon,
this section speciﬁes the software engineering based counterpart. The Sect. 3
discusses related works on capturing and integrating cultural features of under-
served cultures in computer applications for the purposes of alleviating cultural
digital divide. The Sect. 4 is about the reverse software localisation approach.
The Sect. 5 provides the methodology. It speciﬁes the paradigms, the processes
and the technologies. In the Sect. 6, we apply it to the case of preservation of
Yemba Culture. Its ﬁrst part speciﬁes major features of Yemba culture. In its
second part, the reverse approach combined to our methodology is applied to
the reverse localisation of a text editor in Yemba. The third part presents the
results and the related open issues. We end with a conclusion in the Sect. 7.
2
The Cultural Digital Divide in Software Development
The term culture has many deﬁnitions and does not have a uniﬁed deﬁnition,
but has been modiﬁed depending on diﬀerent research disciplines [8]. Hofstede
[9] deﬁnes it as “the collective programming of the mind which distinguishes the
members of one group or category of people from one another”. Considered as

224
M. Soh
the software of the mind [9], culture is thus the integration of human behaviour
that includes attitudes, norms, values, ideas, beliefs, rules, material dimensions,
actions, communications and groups (ethnic, religious, social, etc.). Composed
of overt and covert factors, culture is like an iceberg with invisible and visible
parts.
Computer applications’ views are designed as a matter of taste as preferences
vary from person to person [8]. These preferences can be found deeply-rooted in
culture. For example, software applications like text editors, spreadsheets have
managed to become an every-day tool for the vast majority of the population.
Existing ones have, if any, a Western culturally biased interface [10]. The use of
such tools is more diﬃcult for users who are from other languages and cultures,
resulting in a cultural digital divide. In order to compute a cultural product,
it is recommended in [11] to pay attention to, amongst others, the following
characteristics: text direction (left-to-right, right-to-left), date and time formats,
numeric and currency formats, icons, buttons and colours, etiquette, policy, tone,
formality and metaphors.
The currently ongoing cultural divide in computer applications may accel-
erate the inequalities among cultures in the digital world [11,12]. The idea of
the cultural digital divide refers to the growing gap between the underserved
cultures, which are not used in computers or the internet; and the privileged
cultures, especially western ones, whose features are used in most computer
applications, tools and Internet. The preservation of cultural factors has become
at present a subject of fundamental importance for most communities. To tackle
such a complex subject, the use of informatics is required, since the tradi-
tional methods, based especially on paper documentation, are absolutely inade-
quate [13].
These issues can have causes in the interplay of software development and
integration of underserved cultures’s factors or aspects. In fact, during develop-
ment, designers and developers inconsciously integrate their own cultural values,
norms, practices in the process and/or the ﬁnal product. And the end-users who
understand the designers’ culture are able to use the product while others might
entirely be locked out; some unable to fully enjoy its beneﬁts, some are quite
excluded, or for others their culture is progressively absorbed. This result in a
more and more cultural digital divide. For example, considering an overt cul-
tural factor like language, statistics shows that about 77.9% Internet users of
world total population speaks ten languages [14]. These languages are respec-
tively English, Chinese, Spanish, Arabic, Portuguese, Japanese, Malay, Russian,
French and German. Knowing that language is the vector of culture, we can
conclude that cultures not used in ICTs and Computer applications are likely to
be underserved. Software development could help these cultures in integrating
their own overt or covert cultural factors in computer applications in order to
preserve them in such tools. Thus, solving the problem of cultural digital divide
turns into ﬁnding an approach to build computer applications that integrate
end-users’ cultural factors.

Integration of Underserved Culture in Software by Reverse Localisation
225
3
Related Works
The cultural digital divide has received too little attention in scientiﬁc commu-
nity. Research increasingly shows that one of the essential ways to attack digital
inequalities is by addressing the fact that technologies are always created with
cultural biases built-in that limit their use [13]. This means that the divide will
be lessened only when, in addition to providing basic access, we address seriously
cultural diﬀerences and the diﬀerences in power that come with them [15]. In
correcting this bias, the focal point of research should be shifted towards the
people. We should make computing technology available, understandable, and
participable for everyone regardless of culture, gender, age, income, language,
degree of disability, or ethnicity [4].
The preservation of cultural heritage objects through the use of computer
modeling techniques has attracted considerable attention in computer graphics,
geometric modeling, and virtual reality communities [16]. This problem has less
attention in software development area. Despite the cultural inﬂuence of the
designers own cultural preferences on the design process, there have been limited
studies examining this.
From personal computing till cultural computing, there exist many attempts
to go closer end-user’s culture in computer applications. Cultural computing is
more than integrating cultural aspects into the interaction. It is about allow-
ing the end-user to experience an interaction that is closely related to the core
aspects of his/her culture. In a way that let him engage with an augmented real-
ity using the values and attributes of his own culture. As such, it is important
to understand one’s cultural determinants and how to render them during the
interaction [17]. Research in this domain of cultural computing is concentrated
between three critical domains: numerical arts, virtual worlds and technology
adaptation. The latter make use of localisation, necessary to the adoption of
technologies by others cultures.
In [13], Bartolotta et al. deal with the cultural heritage inventory selected
according to historical urban centres; archaeological sites; extra-urban architec-
tonic assets (castles, towers etc.). To handle the great deal of information related
to this heritage, a suitable Geographic Information System (GIS) is designed,
aimed at having diﬀerent information (texts, photos, drawings, talking or music
sounds, videotapes) and easy to be integrated and updated in future.
According to [18], ICTs has been applied successfully to numerous remote
Indigenous communities around the world. The greatest gains have been made
when requirements have been ﬁrst deﬁned by Indigenous members of the com-
munity, then pattern matched to an ICT solution. The same work shows the
potential uses of ICT in key fundamental areas to the continuing presence of a
culture. It uses GIS, multimedia clips, digital document archives for the preser-
vation of culture. But, because culture is something alive and ever-changing, its
preservation cannot be achieved by ICT alone; it requires the spiritual element
behind the history to be actively reinvigorated into a community to make its
presence felt in a long-lasting manner.

226
M. Soh
Other attempts to solve the problem of the digital divide have focused on
a shallow interpretation of technical literacy as simply learning computer pro-
grams, unaware that technological forms are culturally shaped and need to be
reshaped to ﬁt a wider variety of cultural styles and forms [15]. One of the best
ways to do this, is encourage and provide technical training for people who have
lived experience as members of underserved groups. These culturally competent,
technically savvy individuals can then work as facilitators for marginalized com-
munities to empower them to represent themselves in digital media on their own
cultural ground via their own cultural forms.
The realization of a GIS and ICT tools means a great step forward in the
ﬁeld of cultural heritage inventory and handling [13]. But preserving culture
takes more than ICT [18]. The advantages of this ICT tool is clear enough
also from this work, however limited it is. The inherent problem with ICT is
that while it is good at preserving tangible knowledge it has diﬃculty with
how to treat tacit knowledge. ICT output, no matter how well represented, is
usually one-dimensional [18]. Modelling cultural factors and its handling within
computer applications console based or Internet based need the development of
techniques of visualization, like the VRML (Virtual Reality Modelling Language)
graphic language. In fact, they are absolutely more fascinating than traditional
two-dimensional representations, and the possibility to query and explore the
system, through Internet too, make imaginable a marked increase of the GIS
applications in the ﬁeld of cultural heritage [13].
One drawback of such approaches is that they cannot eﬀectively capture and
tackle the complex nature of users’ cultural requirements in the target culture.
But, according to Kersten et al. [2], there is no solid theory that links software
and culture, or the way ideas and values are implemented in software. Such
a theory is required and needs to go beyond the consideration of the surface
manifestations of culture that have been widely accepted in software interna-
tionalization methodologies and address the core components of software that,
we believe, inﬂuences our ideas and values [2]. In [19], Anacleto models cul-
tural diﬀerences by comparing knowledge bases of common sense statements.
This work points out that this kind of knowledge can help computer systems to
consider cultural diﬀerences. [8] presents an approach to factoring culture into
user models, by introducing the cultural user model ontology (CUMO), which
describe how and to which extend it can accurately represent the users cul-
tural background. Reinecke et al. [5] propose a new approach to localisation by
modeling the users culture according to its understanding in cultural anthropol-
ogy. Contrasting this view with cultural inﬂuences on user interface perception
and preferences, they obtain an intersection of aspects that need to be included
in a cultural user model, and deduce which user interface aspects have to be
adaptable.

Integration of Underserved Culture in Software by Reverse Localisation
227
4
Reverse Localisation Approach for the Integration
of Cultural Factors
With the cultural imperialism of western computer applications, preserving cul-
tural features of underserved communities turns to modelling indigenous cultural
factors and integrating them into nowadays computer tools and applications [20].
The suited approach is named reverse localisation deﬁned by Schaler [1]. Despite
the so rare literature about reverse localisation, Schaler [1] deﬁnes it as keeping
or intentionally introducing linguistic or cultural strangeness into digital content
for a particular target locale. The aim is to intentionally diﬀerentiate a digital
product or service from the dominating culture in that locale [1]. The eﬀect of
reverse localisation is manyfold: the product or service in question is certainly set
apart from potential competitors, the values and connotations associated with
it play on a spirit of adventure, sometimes they just plainly take advantage of
existing stereotypes, and almost always cause a sense of curiosity and heightened
sense of attention [1].
Reverse localisation diﬀers from normal software localisation in the way of
adapting the software. Normal software localisation adapts computer applica-
tion’s linguistic, cultural, regional diﬀerences and technical requirements of a
source language, culture, region to its target counterparts. The reverse software
localisation go from a target language, culture to a source one. As an example,
gedit the famous text editor of the Linux environment localised in arabic, will
have texts typed from right to left, in arabic characters. Even the systems but-
tons will be placed at the left instead of the right. If we reverse localise gedit for
arabic language, we would introduce to it, some linguistic or cultural features
from an underserved culture (for example western date formats) with the aim
of intentionally diﬀerentiating gedit from the dominating culture in that locale
(arabic). The resulting product could have features from both source and target
languages.
The recommendation to localisers is to abandon the well-trodden path of
localisation as we know it, with all its emphasis on the avoidance of even the
smallest sign of cultural diversity and cultural diﬀerences [1]. According to the
author it will encourage clients to get to know local customs, learn about other
languages, taste the delights of foreign cuisine, wear strange clothes, learn how
to play the instruments of other cultures; most of all encourage them to bring
all their cultural baggage and enjoy the clash of cultures wherever they go and
whatever they do.
5
Methodology
Given the advantages of computer applications and ICT tools for digital preser-
vation above, our global approach is to create culturally richer representations
like interactive applications based on the cultural values. Cultural diﬀerences
play a very important role in matching computer interfaces to the expectations
of users from diﬀerent national and cultural backgrounds. But to date, there has

228
M. Soh
been little systematic research as to the extent of such diﬀerences, and how to
produce software that automatically takes into account these diﬀerences [19]. As
proposed in [21], the methodology is an adaptation of a 4-phase cross-cultural
interface design strategy respectively Investigation, Translation, Implementation
and Evaluation.
The Investigation is to learn to understand the processes of the target culture,
to Identify its cultural markers which may be included in the interface. A foraging
study is an excellent way in which to identify cultural markers, to assess the user
attitudes and extract culturally computable features. This phase of investigation
is based on common sense knowledge. In this phase, because they are based on
common sense of knowledge, some symbolisms or abstractions investigated might
be useful or correspond to some global ones. At the same time, some of world
known symbolisms may have a wider applicability. So this phase ends with a
classiﬁcation of investigated features.
The Translation phase is to develop a cultural model to enable the developers
to determine the similarities and diﬀerences present in the targeted culture.
For the Implementation phase, it is to develop a reverse localised application,
being sure to employ the culture model established in phase 2 as well as the
information obtained in phase 1, and to Implement a prototype and perform
usability testing with targeted cultures.
Finally the Evaluation phase help to assess. This user-centered evaluation
is an empirical, uses observational evaluation method that ensures usability of
interactive systems by including users early and continually throughout user
interaction development. The method relies heavily on usage context (for exam-
ple, user task, user motivation, and so on) as well as a solid understanding of
human-computer interaction.
6
Case Study: Preservation of Yemba Culture
in a Text Editor
6.1
Investigation of Yemba Cultural Factors
The Yemba culture is rich in traditions that have been preserved and passed
from one generation to another since immemorial time. This culture is multi-
faceted and is expressed in diﬀerent forms, ranging from its people and language,
food, music and dance, art, artifacts, theatre and literature to its ethnic values
and ethical norms. It encompasses such things as language, storytelling and oral
communication, the use of proverbs as a form of education, art, music, food,
spirituality, craftsmanship, history, ancestry, naming, rites of passage, housing,
livestock, food, clothing, work, marriage, beliefs, traditional medicine and geog-
raphy. Some main overt cultural speciﬁcities are:
– its language which has 33 character except the X and the Q characters;
– its traditional grassﬁeld clothing consist of skins of either domesticated or wild
animals, geometric forms;

Integration of Underserved Culture in Software by Reverse Localisation
229
– its date and time formats: In Yemba region of Cameroon, every week has eight
days and not seven as in the Gregorian calendar. This explains its instability
compared to the seven days of the Gregorian week. Each day has a meaning
related to activities that are reserved or inspired by the history of the village.
Each of these days is related and sometimes suited to particular cultural
ceremonies: day market area; day for celebration of traditional ceremonies
like traditional weddings and intense agricultural activities days. In general,
time is indicated by events that mark: this fact took place during the harvest
of particular plants for planting such other dry season or rainy; we say for
example “I was born the year it was the invasion of locusts”; the Year where
he had eclipse; “at sunrise or sunset”. Today, the thing with calendars has
evolved in the mother tongue where the days, weeks, months or even years
actually have a name.
6.2
Translation of Terms
A Vocabulary of computer terminology and word processing domain in Yemba
language was addressed. The localisation of word processing tools for Yemba
language and culture demands more than mere translation of computer terms
into Yemba. It demanded the creation of Yemba equivalent terms, which involves
application of scientiﬁc strategies and principles for technical-term creation done
with the help of Yemba study committee.
6.3
Computational Results
To integrate Yemba cultural factors into the localised text editor environment,
we need to tackle the following issues: colour schemes, pictures and images,
sounds, historical data, hand signals, symbols, product names and acronyms.
But to the far of our knowledge, these cultural features are not formalized at all,
except the date, time and days representation issues. The Yemba calendar has
12 months. Each week has eight days, instead of 7 days in the generic calendar.
The days are Mbouowa, Mbouolo, Meta, Mbouokeu, Mbouotchou, Efaa, Djielah
and Ngan. As they are not pair with the normal days of a week, we cant associate
them with. The work of reverse localisation which is of national scale, oﬀers a
culturally-based framework to process words.
The prototype of our reversed localised text editor shown in Fig. 1. is built
using the architectural pattern Model-view-controller (MVC). The model is rep-
resented by the domain-speciﬁc representation of the information on which the
application operates, i.e. text processing. The View renders the model into a
form suitable for interaction like in Fig. 1 The Controller is composed of pro-
cesses and responds to events, typically user actions, and invokes changes on the
model and perhaps the view. The modeling of the text editor application, is done
by using use UML language and the resulting protype is developed with java pro-
gramming language. It supports the following basic features such as Insert and
Delete text, Copy, Cut and Paste, Page size and margins, Search and replace,
Word wrap, Print. Our reverse localised text editor supports additional features

230
M. Soh
Fig. 1. Prototype of a reverse localised text editor in Yemba culture
that enable a user to manipulate and format documents in more sophisticated
ways. These full features are Management ﬁles capabilities, font speciﬁcations,
Spell checker, WYSIWYG (what you see is what you get) functions.
6.4
Issues and Trade-oﬀs
The reverse localisation results in solving many issues from linguistic to hard-
ware and software issues. For the integration of Yemba main cultural factors in
the localised text editor environment, we identiﬁed some of the problems and
addressed them.
– Terminological Issues: Language problems are often caused by terminol-
ogy. Whenever English language software is translated into a local language,
decisions are taken on mapping from English terms to local terms. Inevitably,
some measures of arbitrariness are attached to this procedure. In consequence,
some aspects of localised software may appear stranger to the local audience
than the English (foreign language) original. This goes some way towards
explaining why many users when faced with a choice between a localised (fully
translated) application and an English-language original, express a preference
for the latter.
– Linguistic Issues: The development of the Yemba language in the direc-
tion of technological development has suﬀered over the years due to the use
of English as the language of technology in Cameroon. Hence, many of the
English terms used in nowadays text editors do not have corresponding Yemba
terms. Therefore it was necessary to develop terms that can convey the mean-
ings of the original English terms to the Yemba user.

Integration of Underserved Culture in Software by Reverse Localisation
231
– Hardware Issues: The localised text editor must support the character set
of the Yemba language and must be conﬁgured to present numbers and other
values in the local format.
– Software Issues: Localising text editor might require adding other software
such as a new spell checker that recognizes words in the local language. More
over, the new localised tool will need a tool for statistical purposes, gram-
matical corrections, search tools, . . . The creation and translation of localised
content is more demanding, mostly in respect of language skills and awareness
of tools appropriate to the work, such as appropriate fonts and the means to
code in extended Latin and non-Latin scripts.
– Shortcut Issues: There are varying degrees of reverse localisation. Yet there
are no obvious criteria for guiding the appropriate level of localisation. Evi-
dence from our study of localised text editors shows that original menu short-
cuts (such as Ctrl-C for Copy) are consistently retained rather than changed
to accord with localised menu commands. The assumptions underlying this
decision are obscure but may presume that retaining shortcut consistency
across localised versions is beneﬁcial to local users [22]. When the well known
shortcut keys are mapped to a localised Yemba version of the File menu, some
of these mnemonics are inappropriate. For example, because of the inexistence
of the X character in Yemba language, the shortcut Crtl-X helping to cut texts
is not suited.
All these issues results in trade-oﬀs between performance, the ease of use, and
the ease of implementation.
7
Conclusion
Our attempt to the cultural reverse localisation of an text processing system in
the Yemba culture, can contribute to ﬁll the gap between computer programs
designers, developers, and ﬁnal users whose culture is Yemba. Our localised
text editor oﬀers to Yemba speakers a tool to easily produce documents in their
language, because a language is the mirror, the soul of the people and its culture,
its mark of existence. Our work, extensible to other languages, can contribute to
the preservation of underserved cultures in computer applications. This proposed
research aims at investigating the need for a cultural interface that extends
beyond the traditional translated interface. By making use of cultural models
and exhaustive usability testing, the most eﬀective culturally sensitive interface
can be discovered and recommended for use with a text editor application. When
fully implemented, the proposed system will contribute to lessen the cultural
digital divide observed in computer applications.
Acknowledgments. We thank the Yemba language Study Committee, the anony-
mous reviewers for their careful reading of our manuscript, for their detailed and con-
structive comments and suggestions.

232
M. Soh
References
1. Sch¨aler, R.: Reverse Localisation. Localis. Focus Int. J. Localis. 6(1), 39–48 (2007)
2. Kersten, G.E., Matwin, S., Noronha, S.J., Kersten, M.A.: The software for cul-
tures and the cultures in software. In: Hansen, H.R., Bichler, M., Harald, H. (eds.)
Proceedings of the 8th ECIS 2000, Vienna, Austria, vol. 1, pp. 509–514. (2000)
3. Wolﬀ, F.: Eﬀecting change through localization: localization guide for free and
open source. IDRC, Canada (2011)
4. Tedre, M., K¨ahk¨onen, E., Kommers, P.: Is universal usability universal only to us?
In: Proceedings of ACM Conference CUU 2003, Vancouver, Canada (2003)
5. Reinecke, K., Schenkel, S., Bernstein, A.: Modeling a users culture. In: Blanchard,
E.G., Allard, D. (eds.) The Handbook of Research in Culturally-Aware Information
Technology: Perspectives and Models. IGI Global, Hershey (2010)
6. Yacob, D.: Localize or be localized: an assessment of localisation frameworks. In:
The International Symposium on ICT: Education and Application in Developing
Countries. IEEE Press, Addis Abeba (2004)
7. Keller, B., Perez-Quinones, M., Vatrapu, R.: Cultural issues and opportunities in
computing education. In: 9th International Conference on Engineering Education,
pp. 14–18 (2006)
8. Reinecke, K., Reif, G., Bernstein, A.: Cultural user modeling with CUMO: an
approach to overcome the personalization bootstrapping problem. In: Proceedings
of the First International Workshop on Cultural Heritage on the Semantic Web at
the 6th ISWC, pp. 83–90 (2007)
9. Hofstede, G.: Cultures and Organizations: Software of the Mind. McGraw-Hill
Companies, Inc., New York (1997)
10. Hestres, L.E.: The inﬂuence of American culture on software design: Microsoft
Outlook as a case study. GNOVIS Georget. Univ. J. Commun. Cult. Technol. 3(1)
(2003). 22 pages
11. Beelders, T., McDonald, T., Blignaut, P.: A proposed study to determine the eﬀect
of culture on the usability of word processors. In: Proceedings of CHI-SA 2005, pp.
29–33 (2005)
12. Tedre, M., Sutinen, E., Kahkonen, E.: Ethnocomputing: ICT in cultural and social
context. Commun. ACM 49(1), 126–130 (2006)
13. Bartolotta, M., Di Naro, S., Brutto, M., Misuraca, P., Villa, B.: Information Sys-
tems for preservation of cultural heritage, In: International Archives of Photogram-
metry and Remote Sensing, XXXIII, Amsterdam, pp. 864–871 (2000)
14. Internet World Stats. http://www.internetworldstats.com/stats7.htm. Accessed 9
Mar 2017
15. Cultural
Politics.
http://culturalpolitics.net/digital-cultures/cultural-divide.
Accessed 19 Mar 2017
16. Vilbrandt, C., Pasko, G., Pasko, A., Fayolle, P.A., Vilbrandt, T., Goodwin, J.R.,
Goodwin, J.M., Kunii, T.L.: Cultural heritage preservation using constructive
shape modeling. Comput. Graph. Forum 23(1), 25–41 (2004)
17. Rauterberg, M.: From personal to cultural computing: how to assess a cultural
experience. In: Kemper, G., von Hellberg, P. (eds.) uDayIVInformation nutzbar
machen, pp. 13–21. Pabst Science Publ. (2006)
18. Michael, K., Dunn, L.: The use of information and communication technology for
the preservation of Aboriginal culture: the Badimaya people of Western Australia.
In: Dyson, L. (ed.) Information Technology and Indigenous People. Idea Group
Publishing, Hershey (2006)

Integration of Underserved Culture in Software by Reverse Localisation
233
19. Anacleto, J., Lieberman, H., Tsutsumi, M., Neris, V., Carvalho, A., Espinosa, J.,
Godoi, M., Zem-Mascarenhas, S.: Can common sense uncover cultural diﬀerences
in computer applications? In: Bramer, M. (ed.) IFIP AI 2006. IIFIP, vol. 217, pp.
1–10. Springer, Boston (2006). https://doi.org/10.1007/978-0-387-34747-9 1
20. Smith, A., Reitsma, L., Van den Hoven, E., Kotz´e, P., Coetzee, L.: Towards pre-
serving indigenous oral stories using tangible objects, In: Proceedings of the 2nd
ICCC, Amsterdam, pp. 86–91 (2011)
21. Jagne, J., Smith, S.G., Duncker, E., Curzon, P.: Cross-cultural interface design
strategy. Technical report, Interaction design Centre, Middlesex University (2004)
22. Grigas, G., Jevsikova, T., Strelkauskyt, A., Cox, S.: Localisation issues of software
shortcuts. Int. J. Localis. 11, 40–53 (2011)

Geometric Approach of Blind Channel Estimation
Agbeti Bricos Ahossi1, Ahmed Dooguy Kora2(✉), and Roger Marcelin Faye1
1 Ecole Supérieure Polytechnique, Université Cheikh Anta Diop, Dakar, Senegal
2 Ecole Supérieure Multinationale des Télécommunications, 10 000 Dakar, Senegal
ahmed.kora@esmt.sn
Abstract. This paper introduces a geometric approach of channel estimation
(GACE). It is a blind channel estimation method for multiple input multiple output
systems. GACE is based on a two-step geometric approach of source separation
(GASS) that outperforms the existing ones. It is an approximated maximum like‐
lihood estimation method which proceeds by the determination of the polyhedral
edges tilts representing the matrix parameters. It operates by identifying matrix
parameters using a geometric consideration depending on the probabilistic
hypothesis of the sources. The simplicity of this method is based on a cloud
observation, which is used to determine the edge of parallelogram describing the
matrix channel parameters. In this paper, the case of real channel parameters and
complex data sources for higher modulation order are performed. The simulation
results show the eﬃciency of the proposed approach.
Keywords: Channel estimation · Blind · Geometric approach
Sources separation · MIMO
1
Introduction
The huge demand of bandwidth despite the progress of signal processing is still chal‐
lenging the spectral channel eﬃciency. One potential solution is the blind channel esti‐
mation which becomes an opportunity for new investigations. It is the reason why in
recent years, many researchers in signal processing have focused on the problem of blind
sources separation approaches [1–10]. This is due to voracity in data rate of applications
and bandwidth cost of actual methods to ensure the restitution of the transmitted
sequence. Moreover, with increasing variety of applications requiring high data rate,
wireless communication systems become more and more complex at the physical layer.
Eﬃcient method could considerably increase the data rate. Blind sources separation
(BSS) consists in retrieving a vector of n independent sources signals noted, determined
from a vector of m observations. The term blind assumes that there is no prior informa‐
tion on the n sources and on the mixture system. According to the value of n and m, we
can deduce:
– under-determination problem if m < n, less observations points than sources number;
– over-determination problem if m > n, more observations points than sources number;
– determination problem if m = n, as observations points equals to sources number.
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 234–244, 2018.
https://doi.org/10.1007/978-3-319-72965-7_22

Diﬀerent methods including subspace [9, 11], receiver diversity [12, 13], precoding
and geometric approach [1, 14–17] have been investigated. Our interest in simple and
accurate approach of channel estimation without any prior information on the trans‐
mitted data has led to reinvestigate on the pure geometrical method of blind source
separation which was ﬁrst introduced by Puntonet et al. in [1] which pointed out the
disadvantage of previous source separation approach due to its complex algebraic calcu‐
lations. Puntonet has noted that an alternative approach could consist of estimating, by
geometric method, these channel parameters. This has the advantage to be simpler
compared to the others. The existing geometric approach of sources separations has been
abandoned because of its poor performance. In addition to this, it has just considered
the case of real parameters of the sources and the channels. MIMO systems channel
estimation for complex channel with better performance has been investigated in this
paper. The proposed method operates by identifying matrix parameters from geometric
consideration depending on the probabilistic hypothesis of the source. It is an approxi‐
mated maximum likelihood estimation method which proceeds by determining the
polyhedral edges tilts representing the matrix parameters. The simplicity of this method
is based on the cloud observation which is used to determine the edge of parallelogram
which describes the matrix channel parameter.
The rest of this paper is organized as follows. In Sect. 2, a summary of related works
considering the simple case study of real channel and real source has been presented.
Section 3 depicts the new geometric approach of source separation for 2 × 2 systems
(GASS2), which is more eﬃcient compared to the existing one. Section 4 tackles the
geometric approach of channel estimation (GACE) in the case of real channel parameters
coupled with higher modulation. A conclusion ends this work.
2
Related Works
Let’s consider an access system where a given user terminal equipped with a system of
n sources or transmitting antennas which send data to a common base station using also
m receiving antennas (Fig. 1). To simplify our work, the case of two sources and two
receivers, (2 × 2), is studied. It could be generalized to higher MIMO systems. The data
sent are supposed real. The number of possible combinations of symbols to be trans‐
mitted according to the number of antennas result in a cloud of p groups symbols. The
Fig. 1. Transmission system
Geometric Approach of Blind Channel Estimation
235

modulated data is sent without any training sequences. The received data depend on the
channel conditions. Once the data is received at the destination, it is processed in order
to estimate ﬁrstly the channel parameters.
The data are received after it had undergone channel mixture. In Fig. 1, the signal
processing at the receiver side is divided in successive groups of n data blocs which are
a mixture of signals from diﬀerent antennas. At this stage, the communication channel
state information between a given transmit antenna and a receiving antenna is unknown
and the transmitted sequences are also a priori completely unknown by the receiver. In
the case of coherent detection, the channel parameters need to be estimated and these
are used to recover the symbols sequences sent by each transmitting antenna. The main
related work is the one of Puntonet et al. [14–16]. The problem of blind source separation
and blind channel estimation lead to a restitution of a message transmitted without
knowledge on the sources and the channels properties. The system could be separated
into three components which are: the transmitting part including the sources, the channel
and the receiving part.
It has been noticed in [14–16] the complex calculations as disadvantage of algebraic
approach. An alternative approach has been to investigate on geometric technique to
estimate the channel parameters. The main advantage is that it is simpler. The data are
transmitted simultaneously by all the transmit antennas. The received signal at the desti‐
nation by each antenna branch i at a given time t is aﬀected by the channel conditions
at this moment and it could be expressed as follows:
∀i ∈{1, ⋯, m} yi =
n∑
j=1
hijxj + Wi
(1)
where yi is the signal at the ith receiving antenna from the n transmits antennas;
– hi,j is the channel parameter between the source j and the ith receiving antenna;
– xj is the data emitted by the jth transmit antenna;
– n is the number of transmit antennas;
– Wi is an additive hite Gaussian noise (AWGN) with zero mean and variance σ2 at the
receiving branch i.
Receiving antennas generate m mixtures of n sources.
Then, as in [14–16], the system studied could be rewritten at the cost of constant
algebraic factors as:
{
y1 = x1 + ax2
y2 = bx1 + x2
(2)
where
– y1 and y2 are the received samples by antennas 1 and 2 respectively,
– a and b are the unknown channel parameters to be determined based on geometric
approach,
– x1 and x2 are data sent by transmitting antennas 1 and 2 respectively,
– the condition of Puntonet is assumed to be satisﬁed for:
236
A. B. Ahossi et al.

H =
(
1 a
b 1
)
The approach presented in [14–16] to estimate a and b consists in translating to the
origin the highest point determined with the highest norm from the cloud of points
described by the parameters (y1, y2).
Puntonet and Ali Mansour have presented the previous geometric algorithm in two
steps. The ﬁrst step consists in translating the cloud using the maximum norm and a
second step consists in estimating the edge slope on the limiting parallelogram of cloud.
Actually, their approach presents an instability while processing the ﬁrst step. The limi‐
tation of this old method relays in the translated cloud while inappropriate norm is
founded at the ﬁrst iteration. It sometimes requires over two, three or four iterations to
get satisfactory results. More disconcerting, the max used instead of the min translates
the cloud in the ﬁrst or third dial. This prevaricates the edge calculation using the trans‐
lated cloud. These might be probably the main reason why this method is abandoned.
3
GASS2 Algorithm
In this section, a geometric approach of source separation for 2 × 2 systems denoted
GASS2 is presented. It is an improved algorithm compared to the existing ones.
A geometric approach of source separation for 2 × 2 were discussed in [17] but it is
reﬁned in this paper as follows:
– First step, computation of the cloud to get the minima of each axis by (min (y1), min
(y2)) that means translating the values of each antenna to origin by using the minimum
of the value received on the antennas,
– Second step consists to estimate the slope of the limiting parallelogram using the
(min(yi (n)/yj(n)) for i, j Є{1,2} and i ≠ j.
An evaluation test on 1000 samples has been performed in Fig. 2. A cloud weight
with 8 as size of source alphabet is considered. The results clearly show that GASS2
has outperformed the old geometric approach reducing drastically the RMSE which
were order of 10−1 to order of 10−16.
The new way the second step is implemented gives better results in comparison with
the one of Puntonet and Ali Mansour in [15, 16] where the edge were computed with
the new origin. Better result is obtained while the edge has been computed from the max
point of axis for all points having this coordinate superior to this max.
Geometric Approach of Blind Channel Estimation
237

Fig. 2. GASS2 performance compared to
previous geometric algorithm
Fig. 3. Two sub cases of GASS2
With Fig. 3, it is easy to show that both ways to implement the second step of GASS2
present the same order (10−16) of performance but the implementation with the edge
calculation departure estimation is slightly more eﬃcient to the implementation without
estimating edge departure calculation. It is then interesting to deduce that for all height
of cloud, the algorithm with edge departure calculation is always more eﬃcient than the
algorithm without edge departure calculation for any cloud height as shown on the
following picture where the brown color represents the part of parallelogram with
missing data in the observation space. In red marked, we have the departure for the edge
calculation (Fig. 4).
Fig. 4. Departure calculation problem (Color
ﬁgure online)
Fig. 5. GASS2 and alphabet height
The simulations have been done with a 5000 cloud weight in order to have more
representative cloud. The following three tests compare each time GASS2 algorithm
without edge departure calculation to edge departure calculation.
238
A. B. Ahossi et al.

In Fig. 5, we compare both way for diﬀerent size of source alphabet. It can be seen
that the case with edge departure present better performance when the source alphabet
size increase and on the other side, the performance without edge departure slightly
degrades around 10−16.
One can conclude from Fig. 5 that it shows edge departure calculation algorithm as
more accurate because it is not badly aﬀected by the size of the alphabet.
Figure 6 depicts the case of diﬀerent gaps between consecutives elements of source
alphabet for the same size with 8 as source alphabet size. The results show that the RMSE
have the same behavior when the gap increase but edge departure algorithm is still more
accurate.
Fig. 6. GASS2 for gap between consecutives
source alphabet
Fig. 7. GASS2 for alphabet ﬁrst element with
source alphabet size up to 8 and gap 2
The results in Fig. 7 is the investigation outputs of alphabets with the same set size
and the same consecutive symbol deviation but diﬀerent beginnings set. The mapping
constellation size decreases when higher values of consecutive symbols are chosen.
Figure 7 shows a decreasing performance while increasing the level of the ﬁrst source
alphabet. The same shape in both cases with better performance for departure calculation
is depicted.
Figure 8 conﬁrms the accuracy of GASS2 for any combination of channel parameters
(a, b). It can be seen that the channel estimation still give the same order of RMSE around
10−16.
Geometric Approach of Blind Channel Estimation
239

Fig. 8. GASS2 performance with mesh 0.005
for source alphabet size 8
Fig. 9. GASS2 performance with mesh 0.025
for source alphabet size 8
For practical reasons, we adopt a meshing step 0.025 to view channel parameters
restitution topology as shown in Fig. 9 because it is easier to appreciate and compare a
topology at this meshing level. This mesh allows us to compare easily diﬀerent topol‐
ogies in their presentation and view. The topologies are presented with an interpolation
option. Compared to the ﬁner meshing in Fig. 8, the interpolation between RMSE points
is not acceptable but this can give a diﬀerent overview on topology according to the
selected meshing and the source alphabet.
9.93%, 8.12% and 8.54% are respectively the percentage for the perfect estimated
combinations for the precision 0.025, 0.005 and 0.001 depending on the parameters
tested (source alphabet size, deviation between consecutives words, source alphabet
beginning) as showed in the previous pictures (Table 1).
Table 1. Combination for a perfect estimation and diﬀerent channel precision meshing for source
alphabet size 8.
Mesh (Channel parameters
precision)
All combination estimated
Combination estimated
perfectly
0.025
1681
167
0.005
40401
3281
0.001
1002001
85631
The two previous topologies obtained have shown that, for some combination of a
and b, the RMSE of the parameters estimation is equal to zero and that means the esti‐
mation is perfect and provides the right channel parameters and a general order of 10−16
compared the old algorithm where we have less performances in the order of 10−1,5 in
general.
This new algorithm presented is improved according to the two ways to implement
the second step in the noisy conditions on the channel propagation where it has been
considered increasing SNR (Signal Noise Ratio) per step of 10 dB in order to appreciate
the resistance to noise.
240
A. B. Ahossi et al.

Figure 10 depicts the bad impact of noise. In its ﬁrst plot, it has been noticed the
same performances as previously presented for both ways to estimate the slope (without
and with edge departure calculation) on 20 series of 1000 tests, the second plot and third
plot show that if only one of both receivers is aﬀected by the noise, the performance
increases with the SNR but in the fourth plot, it can be noticed the actual limitations of
geometric approaches have not improved anymore even for SNR greater than 40 dB.
Fig. 10. RMSEs in noisy condition
4
Geometric Approach of Channel Estimation (GACE2)
The estimation of a and b where performed before in [17] with the improved algorithm
of the geometric approach by translation to the origin of the lowest point determined
with the minima on each axe from the cloud of points described by the parameters (y1,
y2) as:
(min(y1), min(y2))
(3)
y = Hc + W
(4)
where x = c and W are complex variables. PSK or QAM symbols are considered but the
channel is supposed real. Expression (4) could be rewritten as:
y = H(ℜ(c) + jℑ(c)) + W
(5)
Geometric Approach of Blind Channel Estimation
241

In the case of noiseless model
y = H(ℜ(c)) + jH(ℑ(c)) = ℜ(y) + jℑ(y)
(6)
where the considered system presents imaginary and real parts as follows:
{
ℜ(y) = Hℜ(c)
ℑ(y) = Hℑ(c)
(7)
For a 2 × 2 MIMO system, we get:
⎧
⎪
⎨
⎪⎩
( ℜ(y1
)
ℜ
(
y2
)
)
=
(
1 a
b 1
)( ℜ(c1
)
ℜ
(
c2
)
)
( ℑ
(
y1
)
ℑ
(
y2
)
)
=
(
1 a
b 1
)( ℑ
(
c1
)
ℑ
(
c2
)
)
(8)
From the complex cloud of points, we can extract two sub-clouds, one for the real
part and the other for the imaginary part. Then the coeﬃcients a and b can be derived
from the ratio of real part of y2 over real part of y1 and the ratio of imaginary part of y2
over imaginary part of y1. In theory, it is possible to return the same coeﬃcients by the
two extracted clouds when the constellation presents for real and imaginary parts the
same conﬁguration and the same representativeness. We have experienced slightly
Fig. 11. RMSEs topologies for 8PSK constellation for real part, imaginary part, mean and gap
view
242
A. B. Ahossi et al.

diﬀerent values in this case study. In the determination of the polyhedral edges tilts
representing the matrix parameters, the tilts are not determined according to the origin
but respectively from the point with highest norm in each axis. This method assures that
in the absence of noise the tilts are calculated based on the origin.
Reading Fig. 11 above one can notice that the mean are adequate in certain case and
the gap is negligible.
5
Conclusion
Blind channel estimation remains a challenge. This work has focused on geometric
approach of channel estimation (GACE). An improved geometric approach of source
separation (GASS) has been introduced. A 2 × 2 system with real channel and complex
channel parameters have been discussed. The appropriate GACE algorithm has been
derived.
References
1. Zhu, H., Zhang, S., Zhao, H.: Single channel source separation and parameter estimation of
multi-component PRBCPM-SFM signal based on generalized period. Digit. Sig. Process.
40, 224–237 (2015)
2. Acar, Y., Doğan, H., Panayırcı, E.: On channel estimation for spatial modulated systems over
time-varying channels. Digit. Sig. Process. 37, 43–52 (2015)
3. Besseghier, M., Djebbar, A.B.: New design of pilot patterns for joint semi-blind estimation
of CFO and channel for OFDM systems. AEU Int. J. Electron. Commun. 69(4), 759–764
(2015)
4. Michelusi, N., Mitra, U.: Cross-layer design of distributed sensing-estimation with quality
feedback—part II: myopic schemes. IEEE Trans. Sig. Process. 63(5), 1243–1258 (2015).
https://doi.org/10.1109/TSP.2014.2388440
5. Wenbo, D., Fang, Y., Wei, D., Jian, S.: Time–frequency joint sparse channel estimation for
MIMO-OFDM systems. IEEE Commun. Lett. 19(1), 58–61 (2015). https://doi.org/10.1109/
LCOMM.2014.2372006
6. Du, J., Yuan, C., Zhang, J.: Semi-blind parallel factor based receiver for joint symbol and
channel estimation in amplify-and-forward multiple-input multiple-output relay systems. IET
Commun. 9(6), 735–744 (2014). https://doi.org/10.1049/iet-com.2014.0553
7. Frederico, B., et al.: Multiple-antenna techniques in LTE-advanced. IEEE Commun. Mag.
50(3), 114 (2012)
8. Comon, P., Jutten, C.: Handbook of Blind Source Separation: Independent Component
Analysis and Applications. Academic Press, Cambridge (2010)
9. Gao, F., Zeng, Y., Nallanathan, A., Ng, T.-S.: Robust subspace blind channel estimation for
cyclic preﬁxed MIMO ODFM systems: algorithm, identiﬁability and performance analysis.
IEEE J. Sel. Areas Commun. 26(2), 378–388 (2008)
10. Shin, C., Heath, R.W., Powers, E.J.: Blind channel estimation for MIMO-OFDM systems.
IEEE TVT 56(2), 670–685 (2007)
11. Abed-meraim, K., Cardoso, J., Gorokhov, A., Loubaton, P., Mouliness, E.: On subspace
methods for blind identiﬁcation of single input multiple output FIR systems. IEEE Trans.
Signal Process. 45(1), 42–55 (1997)
Geometric Approach of Blind Channel Estimation
243

12. Wang, H., Lin, Y., Chen, B.: Data – eﬃcient blind OFDM channel identiﬁcation using
receiver diversity. IEEE Trans. Signal Process. 51(10), 2613–2622 (2003)
13. Kora, A.D., Cances, J.P., Meghdadi, V., Vianou, A.: Blind MIMO OFDM channel estimation
based on receiver diversity. Mediterr. J. Electron. Commun. 3(1), 31–39 (2007)
14. Mansour, A.: Contribution à la séparation aveugle de sources: Ph.D. Dissertation. At Institut
National Polytechnique de Grenoble, pp. 41–46 (1992)
15. Puntonet, C.G., Prieto, A., Jutten, C., Rodriguez-Alvarez, M., Ortega, J.: Separation of
sources: a geometry based procedure for reconstruction of n-valued signal. Sig. Process.
46(3), 267–284 (1995)
16. Puntonet, C.G., Mansour, A., Jutten, C.: Geometrical algorithm for blind separation of
sources. In: Actes du XVeme colloque GRETSI, Juan-Les-Pins, France, pp. 273–276 (1995)
17. Ahossi, A.B., Kora, A.D., Faye, R.M.: MIMO blind channel estimation based on geometric
approach of source separation. In: ICEER 2013, pp. 63–68 (2013)
244
A. B. Ahossi et al.

Towards a Conceptual Framework to Scaﬀold
Self-regulation in a MOOC
Gorgoumack Sambe(B), Fran¸cois Bouchet, and Jean-Marc Labat
Sorbonne Universit´es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606,
4 place Jussieu, 75005 Paris, France
{gorgoumack.sambe,francois.bouchet,jean-marc.labat}@lip6.fr
Abstract. MOOCs are part of the ecosystem of self-learning for which
self-regulation is one of the pillars. Weakness of self-regulation skills
is one of the key factors that contribute to dropout in a MOOC. We
present a conceptual framework to promote self-regulated learning in a
MOOC. This framework relies on the use of a virtual companion to pro-
vide metacognitive prompts and a visualization of indicators. The aim
of this system will not only be to improve the quality of learning on the
MOOC but also to help reducing attrition.
Keywords: MOOCs · Dropout · Self-learning · Meta-cognition
Self-regulation · Virtual companion
1
Introduction
Massive Open Online Courses (MOOCs) are a recent innovation in the ﬁeld of e-
learning that allows individuals to take a course for free. The term coined in 2008
refer to the course “Connectivism and Connective knowledge”- CCK08 given by
Downes and Siemens and taken by 2500 learners online. CCK, like ITYPA1, is
a cMOOC based on the theory of connectivism where learners build and share
knowledge and contents within a community of people.
MOOCs have really taken oﬀwith Thurn’s course on artiﬁcial intelligence
from Stanford University in which 160,000 learners registered, with 15% of them
successfully completing the course. This course, like “ABC de la gestion de
projet” (GDP)2, is a xMOOC based on a transmissive approach, which is a
teacher-centered approach in which the teacher is the dispenser of knowledge
and ﬁnal evaluator even if some activities are peer-evaluated. xMOOCs usually
comprise videos presenting the pedagogical content, quizzes and homework for
self-evaluation and a forum to interact with the pedagogical team and other
learners. Platforms like Coursera, Edx, Udacity and FUN in France oﬀer courses
1 ITYPA: “Internet Tout y est pour Apprendre” is the ﬁrst French cMOOC.
2 GDP (Introduction to Project Management) is the ﬁrst French xMOOC and one of
the most prominent French MOOC with over 130,000 persons registered in 8 sessions
over 4 years.
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 245–256, 2018.
https://doi.org/10.1007/978-3-319-72965-7_23

246
G. Sambe et al.
in a wide range of topics: Mathematics, Computer Science, Social Sciences and
so on.
Despite their popularity, MOOCs are characterized by a high dropout rate
(80–90% on average [9,13]). Several factors that contribute to attrition in
MOOCs have been identiﬁed:
1. student’s intent and engagement [6,11,15,16],
2. social factors like low participation to interactions or inﬂuence of peers drop-
ping out thus reducing the social interactions within the MOOC [17,21],
3. weakness of metacognitive skills like self regulation skills and poor time man-
agement skills [10,12,14].
MOOCs, like all self-learning environments, require learners to be able to self-
regulate their learning. Self-regulated Learning (SRL) is one of three pillars of
self-learning and it is demonstrated that, when they master SRL skills, students
become more engaged in their learning and achieve stronger gains in learning [19].
These skills should therefore be promoted and developed during learning [22].
In Computer-Based Learning Environments (CBLE) like Intelligent Tutoring
Systems and Adaptative Hypermedia, many strategies have been used and have
demonstrated they can have a positive impact on students’ SRL skills. Among
these, we can mention the visualization of indicators and the use of metacogni-
tive prompts which can be supported by pedagogical agents to introduce more
emotional and aﬀective elements into learning. These tools have been evaluated
on “closed context” CBLEs [1], but little has been done to scaﬀold these com-
petences in an open system like a MOOC. “Closed context” refers to the use or
the evaluation of a CBLE in a classroom context and a short learning session.
Based on prior research on scaﬀolding SRL skills on CBLE and the fact that
weakness in these skills is a dropout factor in a MOOC, we propose a scaﬀolding
framework based on a combination of techniques of visualization and prompts.
This framework is developed to promote SRL skills and is generic enough to be
used in a self-learning context like a MOOC.
The paper is organized as follows: the next section provides the background
of our framework, SRL skills. Next, we present related work on strategies used
to scaﬀold SRL skills in a learning environment, both in “closed context” and
in MOOCs. Then we present the scaﬀolding framework and indicators before
concluding by making suggestions for future research.
2
Self-regulation
SRL skills refers to students’ skills to create speciﬁc goals for their work, to plan
strategies for achieving these goals, and to monitor and adapt these strategies
as they progress. For [19], “self regulated learning refers to the process by which
learners personally activate and sustain cognitions, aﬀects, and behaviors that
are systematically oriented toward the attainment of learning goals”.
In the literature, it is recognized that SRL is an ongoing cyclic process that
consists of three phases [23] (cf. Fig. 1):

Towards a Conceptual Framework to Scaﬀold Self-regulation in a MOOC
247
Fig. 1. The three phases of SRL [23]
1. planning (Forethought Phase): planning phase consists for students in setting
achievable short and long term goals and to select strategies that best address
a speciﬁc learning challenge.
2. monitoring (Performance Phase): during the monitoring phase, students
implement the selected strategies and make ongoing adjustments to their
plan as they monitor their progress.
3. evaluation (Self-Reﬂection Phase): the evaluation phase consists for students
in estimating results and evaluating the eﬀectiveness of each strategy. Feed-
back from the evaluation phase is then applied at the start of the next
SRL cycle.
There are diﬀerent processes subsumed by metacognitive skills such as (1)
goal-settings and time-management, that refer to specifying intended action or
outcomes and time estimation and allocation, (2) self-monitoring, that refers to
observing and tracking performance and outcomes, (3) self-evaluation, that is a
process comprising self-judgments of present performance and self-reactions to
these judgments. These skills should be developed during learning, which is why
many strategies are used to scaﬀold these competences [1,8].
3
Related Work
As mentioned earlier, in CBLE, many tools have been used to try to scaﬀold
SRL like visualization of indicators, metacognitive prompts and so on:
Visualization of indicators grouped into a dashboard is used to inform
learners regarding the state of their action and interaction. In the same register,
Open Learner Model (OLM) makes all or part of the learner model available
to them, and is not so much about state of action and interaction as it is about
the state of the learner’s knowledge. Learner model represents part or learner’s
knowledge/misconceptions inferred by the system based on the learner’s per-
formance. These tools are evaluated on many CBLE and provide feedback to
learners, leading them to self-evaluate and to use SRL strategies [1,5].

248
G. Sambe et al.
Metacognitive Prompts are alerts or questions designed to scaﬀold
metacognitive processes. They are designed to induce planning, monitoring or
self-evaluation of one’s learning processes. It is recognized that questions such as
“What is our plan?” and “Have our goals changed?”, and reﬂection prompts such
as “To do a good job on this project, we need to. . . ” can help guiding students
self-monitoring. As we mentioned, prompts can be supported by pedagogical
agents which are Human-Machine Interfaces simulating a human-like interface
between the learner and the learning environment. They can also be combined
with an OLM visualization [7]. Several research works mention that metacogni-
tive prompts have a positive impact on SRL skills and that they help students
to self-initiate SRL processes and then to improve learning in CBLE [1,3]. Sup-
ported by a pedagogical agents, their positive impact on SRL and on aﬀect and
emotion has been demonstrated [1,2,4].
Nevertheless, these strategies have been used and evaluated mostly in a
“closed context” (4 h in [2], 2 h in [4]).
In the context of MOOCs, many strategies have been proposed to scaﬀold
SRL like using a task-list or mind-maps, but not implemented yet [10,14,18].
[12] proposed and implemented a strategy to scaﬀold SRL that consisted in
recommending SRL strategies at the beginning of a MOOC. First, based on a
SRL framework, they coded SRL strategies used by highly successful learners
during a previous session of that MOOC and synthesized them into recommen-
dations. Then, in a experiment, they evaluated the eﬀects of providing those
recommendations to learners in the same course. Results suggest that merely
prompting recommendations of SRL strategies at the beginning of the course
was not enough to improve SRL process and that therefore it is a mandatory to
embed “technological aids that adaptively support SRL throughout the course
to better support learners in MOOCs”.
4
Proposition
Our long term objective is to support the learner in a MOOC in metacognitive
dimensions and also in aﬀective and emotional dimensions with an expressive
and adaptative virtual companion. According to the learners’ goals and behav-
ior obtained through previous direct interaction with them and by analyzing
the traces of interaction with the MOOC platform, the virtual companion inter-
acts with them in diﬀerent phases of self-regulation through notiﬁcations and
gives them access to indicators. This will allow learners to better organize their
progress, improve their learning and thus may help in reducing attrition and
increasing the performance of learners present throughout the MOOC.
From a technical point of view, we propose to develop a companion as a
standalone widget, giving the possibility to integrate it into any MOOC platform
such as Canvas, Open edX, Moodle. . . It will be based on a traces extraction
engine, an indicators generation engine, an inference engine and a display engine
(cf. Fig. 2).

Towards a Conceptual Framework to Scaﬀold Self-regulation in a MOOC
249
Fig. 2. Overall architecture of the companion
4.1
Scaﬀolding Planning of Learning
Throughout an analysis of several theories of SRL, [20] reports that goal setting
is the strategy that is common to most and that it also becomes a central, foun-
dational and critical strategy to be used when developing self-regulated learning.
He also reports that eﬀective management of time has been linked to student
achievement in diﬀerent studies and that it can be viewed as an anticipatory
strategy that can prompt students to use other SRL processes. Therefore we
choose to base our framework on these two strategies. Goal setting and time
management are planning strategies, which consist for learners in listing their
tasks and the expected results, and to estimate and allocate their time.
A MOOC learning scenario can be described as a succession of steps often
scheduled for a week. Each step made up of mandatory or optional atomic activ-
ities. We consider that activities of the learning scenario can be classiﬁed into
learning activities, assessment and access to information. To scaﬀold SRL pro-
cesses, we propose to give learners the possibility to set their goals with the
companion and to manage their time. First, we propose to oﬀer the learner
opportunity to set a global goal in the MOOC and a speciﬁc goal on each step
and activity, and then to manage their time by setting deadlines for each train-
ing sequence of the MOOC and/or for activities that compose it. We therefore
consider the following for each activity/sequence of each learner:
– learner’s goal: we associate to each activity/sequence a goal indicator which
takes a binary value indicating whether or not the learner has chosen to per-
form that activity. To reduce learner’s interactions with the companion, it
is deduced from the learner’s overall objective that is requested as a choice
within a limited list of options at the beginning of the MOOC.
– validation of activity/sequence: we associate to each activity a validation
threshold as a binary value. For multiple choice questions, it is equal to the
completion of the questionnaire. For the realization of a learning activity or
an access to information resource, it will be deduced from the MOOC traces
through the click of a validation button, or the approximate time of activity
of the learner on the web page.

250
G. Sambe et al.
– deadline for completion: for each activity/sequence of the learning scenario,
we deﬁne a deadline date to be chosen by learners. They can chose it for all
activities of the current week and unrealized activities from previous weeks,
the ﬁrst time they connect each week. If the student does not deﬁne it, it can
be inferred by the system based on its objectives.
Learners’ goals will thus be requested during their ﬁrst connection to the MOOC
through notiﬁcations and their planning can be set, if they want, by choosing a
delay associated to each sequence. So, at the date d, for each learner l using the
companion, we have a planned scenario pland(l) such as:
pland(l) = {(A, Goal(A), CT(A))}
Where A is an activity or a step, Goal(A) is the goal of the student for this
activity and CT(A) the completion time chosen by the learner or inferred by
the system.
This will educate learners in the use of SRL skills and provide us with a
planned scenario from which indicators on metacognitive skills can be deduced.
Social interactions are not considered in the planned scenario.
4.2
Scaﬀolding Monitoring and Evaluation
Even if both these strategies (time management, goal-setting) are the basis of
our framework, we have to help students on the monitoring process and the
evaluation process. Research supports the hypothesis that eﬀective self-regulated
learning depends on correct evaluations of one’s capabilities and progress in
learning and that self-observation is necessary. So, we propose to support learners
on self-observation and in evaluation process on self-judgment strategy.
To scaﬀold self-observation, we rely on a permanent display of indicators on
the metacognitive virtual companion. Displayed indicators are relative to the
proﬁle of the learner inferred by the system based on traces. We present some
indicators that can be deduced from this framework in the following section,
independently of the visualization eventually chosen to display them.
To scaﬀold self-judgment, we oﬀer the learner the opportunity to issue a per-
sonal judgment on each of the sequences/activities through the virtual compan-
ion. We associate to each completed activity a value of judgment on a Likert scale
for each of the following dimensions corresponding to an evaluation strategy:
judgments of learning, content evaluations, feelings of knowing, ease-of-learning.
So, we associate to each learner a set of judgments for each step/activity of the
eﬀective learning scenario. Practically, at the date d, for each learner l, we have
an eﬀective scenario scd(l) such as:
scd(l) = {(A, V T(A), ϑ(A))}
Where A is an activity or a step, V T(A) is the validation time for this activity
and ϑ(A) the set of judgments of activity A done by the learner l.
This will raise awareness about self-reﬂection on the evaluation process but
will also allow to build indicators related to metacognitive skills.

Towards a Conceptual Framework to Scaﬀold Self-regulation in a MOOC
251
4.3
Indicators
This framework allows us to use metacognitive prompts and to deduce indicators
for self-observation. Some conventional indicators on learner’s activity and inter-
action and on cognitive processes and social dynamic are presented in Table 1
Table 1. Cognitive and social indicators
Indicators
Nature of indicator
Duration of Activity on the MOOC
Activity
Number of actions in the MOOC
Number of connections
Score on sequence/activities validated for each type
(learning/evaluation)
Cognitive
Average marks in assessments
Timeline of sequence/activities done
Duration of activity on the forum
Social
Number of post on the forum
Graphic of relations on the forum
Despite the poverty of MOOCs’ traces compared to what can be available
in some CBLE in closed context, we checked availability of traces allowing to
calculate indicators mentioned on Table 1. A ﬁrst analysis of traces coming from
a MOOC which is hosted in a customized Canvas platform provided by French
company Unow and traces of a standard Moodle platform allowed us to conﬁrm
the availability of data on both platforms. This data will be retrieved from the
database used by the platform and log ﬁles.
It should be noted that although the data are available, their format can be
very heterogeneous between diﬀerent platforms. To give the possibility to embed
the virtual companion into any MOOC platform, we designed the extraction layer
with a connector and a data transformation module which are speciﬁcs to each
of them.
The advantage of this framework is to make available indicators relative to
SRL skills that can be used by educators. They can also be accessible to the
learner to promote SRL skills. We give in Table 2 some indicators related to this
dimension. Traces for calculating metacognitive indicators are obtained through
interactions between the learner and the companion. If learners are not using
the companion, we would display their cognitive, social and activity indicators.
4.4
Example of Application
In order to illustrate the way our framework will be used, we consider here an
example of a concrete application on the GDP MOOC, which is one of our envi-
sioned testbed. GDP is the ﬁrst French xMOOC and is taught by R. Bachelet

252
G. Sambe et al.
Table 2. Metacognitive indicators
Indicators
Nature of indicator
Score on sequence/activities validated on time for each type
(learning/evaluation)
Planning process
Timeline of sequence/activities done on time)
Score of judgment of the learning on activities/sequences
Evaluation process
from “Centrale Lille”. In the latest session (the eighth), three certiﬁcates corre-
sponding to diﬀerent workloads were oﬀered. In our work, we are interested in
the basic one and the advanced one. The course lasted seven weeks, provided
quizzes, weekly assignments and a ﬁnal examination. The course has a core com-
posed of four modules and two specialization modules had to be chosen from a
list of 13 available modules.
To obtain the basic certiﬁcate, it was required to complete the quizzes and
the exam with a minimum of 2800 points out of 4000 and to validate at least two
specializations modules. Modules were opened each Monday and the deadline for
quizzes was set to the last day of the course. In order to obtain the advanced
certiﬁcate, participants were required to submit three assignments out of four, to
participate to peer-evaluation and auto-evaluation of assignments, and to pass
the basic one. They also had to reach a minimal score of 4900 points out of 7000.
Assignments were based on a case study and assessed through peer evaluation.
Learners could lose or gain some points according to the quality of their peer-
evaluation. In interactions, some new discussion threads were initiated every
week and were accessible during the course. Threads were opened and moderated
by the MOOC staﬀ.
In this context, we envision the following scenario. At the ﬁrst connection
of learners in the MOOC, we oﬀer them the option to use the companion and
to choose their goals (Fig. 3). We describe the weekly process followed by the
companion in Fig. 4. Depending on learner’s goals, the companion has a speciﬁc
behavior. Let us take as an example the ﬁrst week of GDP. The basic certiﬁcate
track for the ﬁrst week S1 is composed of 18 activities: 2 informational resources
to read S1I1 and S1I2, 8 learning activities in video format [S1V 1, S1V 8], 7 MCQ
[QS1V 2, QS1V 8] and 1 assignment QS1. We present in Table 3 the activity of
a learner who wants to validate the basic certiﬁcate and plans to do learning
activities and MCQ of this ﬁrst week on day 4, and the assignment on day 5.
But that learner validates only one part of their planned scenario on day 4 and
validates the other part on day 7.
We show in Table 4 methods initiated by the companion (column 2), scores on
learning activities, MCQ and assignments validated and scores on those validated
on time or delayed (column 3) and the eﬀective scenario of the learner (column 4).
So his planned scenario is set by the companion to
planj1(l) = {(A, 1, j4)}

{(QS1, 1, j5)}
where A represents all activities of week 1 without QS1.

Towards a Conceptual Framework to Scaﬀold Self-regulation in a MOOC
253
Fig. 3. First connection of the learner
Fig. 4. Process of the companion in the week
Table 3. Example of learner activity
Day
Time spent on activities (mn)
Activities validated by the learner
1
S1I1:2, S1V1:8 . . .
2
3
4
S1V4:17, S1AI1:10 S1V5:22 QS1V5:7
SI1, [S1V 1, S1V 4], [QS1V 2, QS1V 4]
5
S1V6:12 S1A-QS:12 S1V5:17 QS1V5:9
6
S1V5:13
7
S1V7:12 S1AI1:6
S1I2, [S1V 5, S1V 8], [QS1V 5, QS1V 8], QS1
8
. . .
Even if we include only indicators generated in the case of delay, it is clear
that all interactions with the companion and all actions in the platform generate
traces which can be used for generating indicators. We can notice that the gener-
ation of indicators and inference rules is based on trace analysis and educational
data mining techniques. According to the learner’s behavior deduced from indi-
cators such as the scores calculated in the previous example and the inference
system, the companion should show relevant notiﬁcations and indicators. In this
example, we can imagine two possible interventions from the companion:
– in days 4 and 5, the learner spent a lot of time between the learning activity
5, S1V5 and the MCQ related to this video, QS1V5. The companion should
be able to detect this and propose them to go to the forum to interact with
other learners.

254
G. Sambe et al.
Table 4. Example of companion activity
Day Activity of the companion
Scores
Activities of
eﬀective scenario
1
generate plan notif, update plan
LA = 8,
MCQ = 7, QS = 1
∅
2
3
4
update scores,
generate evaluation notif
LAV = 4/8,
MCQV = 3/7
LAVT = 4,
MCQVT = 3
5
LAD = 4/8
MCQD = 4/7
S1I1, [S1V 1, S1V 4],
[QS1V 2, QS1V 4]
6
Same as
5 + QSD = 1/1
7
update scores,
generate evaluation notif
LAV = 8/8,
MCQV = 7/7
QSVD = 1/7,
LAVD = 4/8
MCQVD = 4/7,
QSVD = 4/7
8
generate plan notif
S1I1, S1I2
[S1V 1, S1V 8],
[QS1V 2, QS1V 8],
QS1
LA: Total of learning activities online, MCQ: total of MCQ online, QS: total of
assignments online, LAV: LA validated, MCQV: MCQ validated, QSV: QS vali-
dated on time, LAVT: LAV on time, MCQVT: MCQV on time, QSVT: QSV on
time, LAD: LA delayed, MCQD: MCQ delayed, LAVD: LA validated but delayed,
MCQVD: MCQ validated but delayed, QSVT: QSV validated but delayed
– between days 4 and 7, the learner visited several times the informational
resource about advanced certiﬁcate (S1AI1) and spent signiﬁcant amount of
time on that resource as well as on the advanced assignment (S1A-QS). In
the beginning of week 2, the companion could therefore consider suggesting
the learner to change their goals and to maybe try validating the advanced
certiﬁcate.
5
Conclusion and Perspective
In this article, we introduced a conceptual framework generic enough to be seen
as a tool for promoting SRL in a self-learning context, but particularly in a
MOOC. Based on literature on SRL, we introduced this framework to under-
pin an adaptative virtual companion. This companion will interact with learners

Towards a Conceptual Framework to Scaﬀold Self-regulation in a MOOC
255
through prompts in the diﬀerent steps of SRL processes and give them a feed-
back through some cognitive and social indicators to scaﬀold SRL process. The
framework allows the availability of metacognitive indicators for educators but
also for learners. However, we must consider the case where the learner does not
use the tool and we do not have indicators in the metacognitive dimension.
This work is a ﬁrst step for the implementation of a virtual companion on
a MOOC, based on literature in SRL and on scaﬀolding of SRL on CBLE. Our
aim in the future is to implement and evaluate the impact of the companion
on SRL skills, learning process and persistence on a MOOC. We plan an initial
experiment with a limited number of learners using a Moodle platform. Then
we will make an experiment in a real context using Canvas platform. We plan to
collect data and traces and use them to validate our strategy through an analysis
based on educational data mining methods.
Acknowledgments. This research is partially supported by University Assane Seck
of Ziguinchor (UASZ), S´en´egal.
References
1. Azevedo, R., Aleven, V.: International Handbook of Metacognition and Learn-
ing Technologies. Springer, New York (2013). https://doi.org/10.1007/978-1-4419-
5546-3
2. Azevedo, R., Martin, S.A., Taub, M., Mudrick, N.V., Millar, G.C., Grafsgaard,
J.F.: Are pedagogical agents’ external regulation eﬀective in fostering learning with
intelligent tutoring systems? In: Micarelli, A., Stamper, J., Panourgia, K. (eds.)
Intelligent Tutoring Systems. volume 9684, pp. 197–207. Springer International
Publishing, Cham (2016)
3. Bannert, M., Mengelkamp, C.: Scaﬀolding hypermedia learning through metacog-
nitive prompts. In: Azevedo, R., Aleven, V. (eds.) International Handbook of
Metacognition and Learning Technologies, pp. 171–186. Springer, New York (2013).
https://doi.org/10.1007/978-1-4419-5546-3 12
4. Bouchet, F., Harley, J.M., Azevedo, R.: Can adaptive pedagogical agents’ prompt-
ing strategies improve students’ learning and self-regulation? In: Micarelli, A.,
Stamper, J., Panourgia, K. (eds.) Intelligent Tutoring Systems. volume 9684, pp.
368–374. Springer International Publishing, Cham (2016)
5. Bull, S., Kay, J.: Open learner models as drivers for metacognitive processes.
In: Azevedo, R., Aleven, V. (eds.) International Handbook of Metacognition and
Learning Technologies, pp. 349–365. Springer, New York (2013). https://doi.org/
10.1007/978-1-4419-5546-3 23
6. Cisel, M.: Analyzing completion rates of ﬁrst French xMOOC. In: Proceedings of
the European MOOC Stakeholder Summit 2014, p. 26 (2014)
7. Feyzi-Behnagh, R., Azevedo, R., Bouchet, F., Tian, Y.: The role of an open learner
model and immediate feedback on metacognitive calibration in MetaTutor. In: The
2016 Annual Meeting of the American Educational Research Association (2016)
8. Ge, X.: Designing learning technologies to support self-regulation during ill-
structured problem-solving processes. In: Azevedo, R., Aleven, V. (eds.) Inter-
national Handbook of Metacognition and Learning Technologies, pp. 213–228.
Springer, New York (2013). https://doi.org/10.1007/978-1-4419-5546-3 15

256
G. Sambe et al.
9. G¨utl, C., Rizzardini, R.H., Chang, V., Morales, M.: Attrition in MOOC: lessons
learned from drop-out students. In: Uden, L., Sinclair, J., Tao, Y.-H., Liberona, D.
(eds.) LTEC 2014. CCIS, vol. 446, pp. 37–48. Springer, Cham (2014). https://doi.
org/10.1007/978-3-319-10671-7 4
10. Khalil, H., Ebner, M.: MOOCs completion rates and possible methods to improve
retention-a literature review. Inb: World Conference on Educational Multimedia,
Hypermedia and Telecommunications 2014, pp. 1305–1313 (2014)
11. Kizilcec, R.F., Piech, C., Schneider, E.: Deconstructing disengagement: analyzing
learner subpopulations in massive open online courses. In: Proceedings of the Third
International Conference on Learning Analytics and Knowledge, pp. 170–179. ACM
(2013)
12. Kizilcec, R.F., P´erez-Sanagust´ın, M., Maldonado, J.J.: Recommending self-
regulated learning strategies does not improve performance in a MOOC. In: Pro-
ceedings of the Third ACM Conference on Learning at Scale, Edinburgh, UK (2016)
13. Meyer, R.: What it’s like to teach a MOOC (and what the heck’s a MOOC?). The
Atlantic (2012)
14. Nawrot I., Doucet, A.: Building engagement for MOOC students: introducing sup-
port for time management on online learning platforms. In: Proceedings of the 23rd
International Conference on World Wide Web, WWW 2014 Companion, Repub-
lic and Canton of Geneva, Switzerland, pp. 1077–1082. International World Wide
Web Conferences Steering Committee (2014)
15. Ramesh, A., Goldwasser, D., Huang, B., Daume III, H., Getoor, L.: Modeling
learner engagement in MOOCs using probabilistic soft logic. In: NIPS Workshop
on Data Driven Education (2013)
16. Reich, J.: MOOC completion and retention in the context of student intent. EDU-
CAUSE Review Online (2014)
17. Ros´e, C.P., Carlson, R., Yang, D., Wen, M., Resnick, L., Goldman, P., Sherer, J.:
Social factors that contribute to attrition in MOOCs. In: Proceedings of the First
ACM Conference on Learning @ Scale Conference, pp. 197–198. ACM (2014)
18. Sambe, G.: Vers un apprentissage autor´egul´e dans les MOOC. In: Rencontres
Jeunes Chercheurs Environnements Informatiques pour l’Apprentissage Humain,
pp. 101–106 (2016)
19. Schunk, D.H., Zimmerman, B.J.: Motivation and Self-regulated Learning: Theory,
Research, and Applications. Routledge, Upper Saddle River (2012). Google-Books-
ID: MDQLfOg0jX0C
20. Terry, K.P.S.: The eﬀects of online time management practices on self-regulated
learning and academic self-eﬃcacy. Ph.D. thesis, Virginia Polytechnic Institute and
State University (2002)
21. Yang, D., Wen, M., Rose, C.: Peer inﬂuence on attrition in massive open online
courses. In: Proceedings of Educational Data Mining (2014)
22. Zimmerman, B.J., Bonner, S., Pagnoulle, C., Kovach, R., Smets, G.: Des
apprenants autonomes: Autor´egulation des apprentissages. De Boeck Sup´erieur,
April 2000
23. Zimmerman, B.J., Campillo, M.: Motivating self-regulated problem solvers. In:
Davidson, J.E., Sternberg, R.E. (eds.) The Psychology of Problem Solving, pp.
233–262. Cambridge University Press, New York (2003)

Megamodel Consistency Management
at Runtime
El Hadji Bassirou Toure(B), Ibrahima Fall, Alassane Bah,
and Mamadou Samba Camara
Institut de Recherche pour le D´eveloppement (IRD),
´Ecole Sup´erieure Polytechnique (ESP),
Universit´e Cheikh Anta Diop de Dakar (UCAD), Dakar, Senegal
{bassirou.toure,ibrahima.fall}@esp.sn,
{alassane.bah,mamadousamba.camara}@ucad.edu.sn
Abstract. This paper addresses the problem of ensuring consistency,
correctness and other properties in dynamically changing software sys-
tems. The approach uses a Megamodel that represents the current state
of the system at runtime including some rules. These rules are formu-
lated as Hoare-Triples and allow to check whether modiﬁcations to the
software system result in a consistent state, otherwise to ﬁx changes that
are likely to violate the megamodel integrity.
Keywords: Runtime software evolution · Runtime veriﬁcation
Megamodeling · Correctness · Axiomatic semantics
1
Introduction
Software architectures increasingly rely on abstractions to describe system com-
ponents, the way the components interact and the rules governing the compo-
sition of components into systems. Very often these descriptions address high-
level and complex aspects of the software [1]. And this increased complexity
of systems requires raising the level of abstraction. Model-Driven Engineering
(MDE) is an approach that advocates software abstraction through an exclusive
use of models. In fact, MDE speciﬁcally considers software models which are
abstractions of static or dynamic properties of a software system [2]. In fact,
it is a “recent” Software Engineering (SE) ﬁeld which promotes the exclusive
use of models in the software system development, maintenance and evolution.
A model is recursively deﬁned as an artifact which consists of model elements,
conforms to a speciﬁc metamodel and represents a given view of a system. A
metamodel can be deﬁned as a model of the language used to represent a given
model [3]. MDE also distinguishes prescriptive models and descriptive models. A
prescriptive model is a model which represents the manner a system has to be
created, i.e. the system is built on the basis of the model. While a descriptive
model provides information about an existing system. This distinction allows to
deﬁne model correctness and system validity which are two concepts related to
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 257–266, 2018.
https://doi.org/10.1007/978-3-319-72965-7_24

258
E. H. B. Toure et al.
the causal connection between a model and the system it represents. In fact,
a model is correct if all its statements are true for the represented system. A
system is valid according to its speciﬁcations if no statement in the model is
false for that system [7].
MDE uses the concept of a megamodel as a building block for modeling in the
large. For that a megamodel must ignore the internal details of global entities
such as models and metamodels by stressing on the “big picture”, i.e. system
architecture, assignment of models as parameters or results for model transfor-
mations. Indeed a megamodel is considered as being a model whose elements
are models and which considers the interconnections between multiple models
in the form of model operations [3]. That is why megamodels are often used
for representing the components of an architecture and the interactions between
them [5].
Software architectures are often made to evolve due to changing user needs
and/or to execution environment. Such changes are often done through adap-
tation mechanisms in which components are often added to or removed from
multiple operations in the system. In this context for a system with few changing
requirements, it probably suﬃces to apply these adaptations at the development-
time. However in a dynamically evolving system, changes to the execution envi-
ronment, to the components and/or interconnections, may take place at runtime.
Indeed in many application domains there is a need that the system accommo-
dates changes dynamically, without stopping or disturbing the operation of those
parts of the system unaﬀected by the changes [11].
Moreover the identity of the components that a system will utilize may not
be known until the execution phase. For that a software architecture is then
modeled as a collection of particular MDE models which are called runtime
models. A runtime model provides a view, on a running software system, that
is usually used for managing the system to which it is causally connected [4].
The causal connection between models and represented systems means that each
time the system changes, then the model is updated, and similarly, if the model
changes, it causes the proper system change [6].
In this paper we propose the use of a runtime megamodel to represent the
current state of a running system. It consists of related runtime models which
represent runtime artifacts. Such runtime artifacts may include component cre-
ation and destruction, exceptions/errors, operation inputs and output, component
operations invocation, dynamic artifact types, dynamic component names, and so
on. Modifying part of a system in one model can thus introduce inconsistencies
with related parts of the system speciﬁed in other models. Thus these models
can be inconsistent with each other since they describe the system from diﬀerent
aspects and views. Therefore there is an inherent need to preserve consistency
between those models registered in a runtime megamodel even when the software
evolves.
To ensure consistency and functional validity of the dynamically changing
systems, a runtime veriﬁcation system is required. Runtime veriﬁcation is a
method of checking whether the active execution trace of software adheres to
its speciﬁcations [13]. Against to other veriﬁcation techniques such as model

Megamodel Consistency Management at Runtime
259
checking which aims at checking all possible execution traces of the software, run-
time veriﬁcation reduces the veriﬁcation scope to one execution of the software;
this increases the accuracy of the veriﬁcation, especially for dynamic artifacts of
the software.
For that we will consider the runtime megamodel as being an execution envi-
ronment or a program in which instructions are operations that mainly compute
with component models by adding or removing them even though these changes
have not to violate the correctness of the megamodel vis −`a −vis to the repre-
sented systems. Accordingly we use techniques for proving programs correctness
known as Hoare’s axiomatic semantics and some inference rules in order to set
up our runtime veriﬁcation technique which enables us to keep the megamodel
consistent at anytime.
Organization of the paper. The remainder of this paper is articulated as
follows. In Sect. 2, we begin with the presentation of an illustrative example
which will be used throughout the paper. Section 3 Presents the approach by
using the running example to illustrate it. Section 4 is reserved for the conclusion.
2
Illustrative Example
In this illustrative example which will be used all around the paper, we suppose
that we have a megamodel M which represents a model of a system S (see
Fig. 1). Given that we have a new tool T to plug in the system S which is
already deployed and running.
Fig. 1. The running Example

260
E. H. B. Toure et al.
For that we suppose that T will be added to S to the data dimension.
– Let M be the megamodel representing the system S.
– Let msd be the component model representing the data model of S.
– Let mtd be the component model representing the data model of T.
– Let mapd be the component model representing a morphism for msd and mtd.
3
Our Approach
3.1
Overview of the Approach
The presented approach proposes a runtime megamodel to represent a running
software system. The approach will use a megamodel which represents an exe-
cution model of the running system to which it is causally connected. Indeed
the megamodel is deployed with the system and used as a basis for model man-
agement and changes representation. Our approach should also provide infer-
ence rules for reasoning about, specifying, and representing change operations.
Thereby checking the megamodel’s consistency by deﬁning a formal-safe execu-
tion as well as an execution semantic for each operation likely to modify the
megamodel contents.
3.2
The Runtime Megamodel
To create a composite system, the ﬁrst step should be to highlight the system
components which represent the artifact that will be put in the new system.
To facilitate this process, component models will be stored in a megamodel con-
sidered as a registry of components and their interactions. The access and the
management of such a megamodel should be done at runtime. Each component
model will have a name to identify it and a type to represent its metamodel. A
component model should also specify the information it represents. Such infor-
mations can be a functional or nun-functional element, a process element, a data
element, etc. A Component model can either be used to create a new artifact, in
this case it is said a prescriptive model or it can be used to describe an existing
artifact and in that case it is called a descriptive model Fig. 2. An extensive
study of the semantic of each operation is presented in [5] to which we refer the
reader for further information.
Application
In our application, the megamodel is represented through an ecore metamodel
(Fig. 2). It is considered as an environment which is managed using a textual
Domain-Speciﬁc Language (DSL): the Mega Operation Management and Execu-
tion (MOME) which is being constructed.
3.3
Changes in the Megamodel
The megamodel consists of component models and global operation models. A
global operation model can be considered as a type of a global operation instance.

Megamodel Consistency Management at Runtime
261
Fig. 2. The megamodel metamodel.
Therefore, it represents a model of future interactions which are global operation
instances. And those global operation instances link some component instances.
That is, a global operation model deﬁnes some interaction rules, is instantiated
on component instances and allows to dynamically set connections between com-
ponents. A global operation model will be applied to component models already
registered in the megamodel, and its results are new component models that will
have to be put in the megamodel.
The management of the megamodel (megamodelling) sounds like program-
ming where the megamodel plays the role of the execution environment (pro-
gram) [9]. The content of the megamodel is modiﬁed by the execution of global
operation execution. Therefore, the megamodel is proposed to dynamic and fre-
quent changes. Such changes may include the introduction of new components;
the recreation of failed components; the modiﬁcations of component interconnec-
tions; the change component operating parameters, etc. A global operation mod-
els corresponds to a set of operations which are executed in response to changes
related to the underlying system state changes. Otherwise these changes have to
leave the megamodel consistent.
Application
In the megamodel, the changes representation are performed through the global
Operation execution. For example at Figs. 3 and 4, we show respectively how
the Merge global Operation is represented in a MOME program and a part of its
execution semantics.

262
E. H. B. Toure et al.
Fig. 3. A MOME program
Fig. 4. A part of the execution semantics of the MOME program

Megamodel Consistency Management at Runtime
263
3.4
Ensuring Correctness of the Megamodel
These changes necessitate to focus on the way the megamodel react after a
change. Hoare Triples allow us to ﬁx this problem by proposing a formal-safe
execution and an execution semantics for each global operation.
The megamodel acts as an execution environment whose instructions are
the global operation executions. A global operation model involves a set of pre-
conditions P, a sequence of operations Seq, and a set of post-conditions Q.
{P}Seq{Q}
The pre-conditions represent a set of states of the system from which a given
sequence of operations can be used.
Possibly each operation may have a side eﬀect which refers to its impact on
other components. Each operation may impact either some component models
or other global operation models.
The post-conditions deﬁne a set of states that satisfy the required result after
the execution of a global operation models.
To achieve this, as in previous works [8], we have deﬁned a formal-safe exe-
cution which ensures that execution of a global operation model does not lead
to some inconsistencies in the megamodel. We have also deﬁned an execution
semantic for each global operation model, which means its observable behavior
(its side-eﬀect).
Execution semantic for global operations
For the Merge global operation which allows to integrate T with S through
their data dimension, we deﬁne its execution semantic as follows:
Operation Merge: merged ←−Merge(msd, mapd, mtd)
Pre-conditions
Post-conditions
P1 :: = msd ∈M
P2 :: = level(msd, “terminal”)
P3 :: = type(msd, “UML”)
P4 :: = dimension(msd, “data”)
P5 :: = mtd ∈M
P6 :: = level(mtd, “terminal”)
P7 :: = type(mtd, “UML”)
P8 :: = dimension(mtd, “data”)
P9 :: = mapd ∈M
P10 :: = merged /∈M
Q1 :: = msd ∈M
Q2 :: = level(msd, “terminal”)
Q3 :: = type(msd, “UML”)
Q4 :: = dimension(msd, “data”)
Q5 :: = mtd ∈M
Q6 :: = level(mtd, “terminal”)
Q7 :: = type(mtd, “UML”)
Q8 :: = dimension(mtd, “data”)
Q9 :: = mapd ∈M
Q10 :: = merged ∈M
We have the triplet:
{P} Merge (msd, mapd, mtd) {Q}; where P = ∩Pi and Q = ∩Qi
Similarly, for each global operation model, we deﬁne its execution semantic
(Fig. 4).

264
E. H. B. Toure et al.
Setting up a global operation model may lead to the execution of other global
operations, namely its ripple eﬀect. In such cases, often the output of a global
operation model may correspond to the input of another global operation model.
Safe executions of global operations
Before invoking the Merge operation, we have to call at ﬁrst the global operation
Map with the two model parameters. Indeed the third parameter (map) to Merge
is a morphism describing the elements of msd and mtd that are equivalent and
should be “merged” into a single element mapd in M. Therefore we can note a
logical precedence between some global operations deﬁned in the megamodel. To
consider this, we have to set up a deductive system which enables the deduction
of new theorem from some theorems already proved. A rule of inference takes
the form “If ⊢X and ⊢Y then ⊢Z”, i.e. if assertions of the form X and Y have
been proved as theorems, then Z also is thereby proved as a theorem.
For that we will use two inference rules presented in [10], that is the rule of
consequence and the rule of composition. After that, we present an example in
which these two rules are applied.
(i) Rules of consequence:
If ⊢{P}S{R} and ⊢R ⊃Q then ⊢{P}S{Q}
If ⊢{P}S{R} and ⊢P ⊂Q then ⊢{Q}S{R}
(ii) Rule of composition:
If ⊢{P}S1{Q1} and ⊢{Q1}S2{R} then ⊢{P}(S1; S2){R}
⊢{P}S1{Q1} ⊢{Q1}S2{R}
⊢{P}(S1; S2){R}
global operation
We can deﬁne the inference rule for the Merge global operation which enables us
to integrate models.
Application
Considering the previous example. We have to describe an execution of Merge
between the two models, namely msd and mtd. However before invoking the
Merge Operation, it must be necessary to call at ﬁrst the Map operations. For
that we deﬁne an execution semantic of Map.
Operation. Map : mapd ←−Map(msd, mtd)
Pre-conditions
Post-conditions
M1 :: = mtd ∈M
M2 :: = msd ∈M
M3 :: = mapd /∈M
M4 :: = type(msd) == type(mtd)
M5 :: = dimension(msd) == dimension(mtd)
N1 :: = mtd ∈M
N2 :: = msd ∈M
N3 :: = mapd ∈M
N4 :: = type(msd) == type(mtd)
N5 :: = dimension(msd) == dimension(mtd)

Megamodel Consistency Management at Runtime
265
We have the triplet:
(i) A1 ←−{M}Map(msd, mtd){N}; where M =∩Mi and N = ∩Ni
Operation. Merge : merged ←−Merge(msd, mapd, mtd)
Pre-conditions
Post-conditions
I1 :: = mtd ∈M
I2 :: = msd ∈M
I3 :: = mapd ∈M
I4 :: = merged /∈M
I5 :: = type(msd) == type(mtd)
I6 :: = dimension(msd) == dimension(mtd)
R1 :: = mtd ∈M
R2 :: = msd ∈M
R3 :: = mapd ∈M
R4 :: = merged ∈M
R5 :: = type(msd) == type(mtd)
R6 :: = dimension(msd) == dimension(mtd)
We have the triplet: (ii) A2 ←−{I}Merge (msd, mapd, mtd){R}; where I = ∩Ii
and R = ∩Ri
Using the rules of consequence on (i) and (ii) we obtain:
⊢{M}Map{N} and ⊢I ⊃N then, ⊢{M}Merge {R}
We have: A3 ←−{M}Merge{R} →(iii)
More formally, using the rules of composition we obtain:
⊢{M}Map{N} ⊢{I}Merge{R}
⊢{M}Merge{R}
Merge
4
Conclusion
In this paper we have proposed a runtime megamodel to represent a running
software system. The approach uses a megamodel which represents an execution
model of the running system to which it is causally connected. Indeed the meg-
amodel is deployed with the system and used as a basis for model management
and changes representation. Our approach also provides inference rules for rea-
soning about, specifying, and representing change operations. Thereby checking
the megamodel’s consistency by deﬁning a formal-safe execution as well as an
execution semantic for each operation likely to modify the megamodel contents.
References
1. Shaw, M., DeLine, R., Klein, D.V., Ross, T.L., Young, D.M., Zelesnik, G.: Abstrac-
tions for software architecture and tools to support them. IEEE Trans. Softw. Eng.
21(4), 314–335 (1995)
2. Bislimovska, B.: Textual and content based search in software model repositories
(Doctoral dissertation, Italy) (2014)
3. Bezivin, J., Jouault, F., Valduriez, P.: On the need for megamodels. In: Proceedings
of the 19th Annual ACM Conference on Object Oriented Programming, Systems,
Languages, and Applications, October 2004
4. Vogel, T., Giese, H.: A language for feedback loops in self-adaptive systems: exe-
cutable runtime megamodels. In: The 7th International Symposium on Software
Engineering for Adaptive and Self-managing Systems, pp. 129–138, June 2012

266
E. H. B. Toure et al.
5. Toure, E.B., Fall, I., Bah, A., Camara, M.B. Megamodel-based Management of
Dynamic Tool Integration in Complex Software Systems. In Proceedings of the
Federated Conference on Computer Science and Information Systems (FedCSIS)
(2016)
6. Song, H., Huang, G., Chauvel, F., Sun, Y.: Applying MDE tools at runtime: exper-
iments upon runtime models. In: Models@run.time 10, vol. 641, pp. 25–36 (2010).
CEUR-WS.org
7. Seidewitz, E.: What models mean. IEEE Software, September/October 2003
8. Bousso, M., Sall, O., Thiam, M., Lo, M., Toure, E.H.B.: Ontology change estima-
tion based on axiomatic semantic and entropy measure. In: Signal Image Technol-
ogy and Internet Based Systems (SITIS), pp. 458–465, November 2012
9. Vignaga, A., Jouault, F., Bastarrica, M.C., Bruneli`ere, H.: Typing in model man-
agement. In: Paige, R.F. (ed.) ICMT 2009. LNCS, vol. 5563, pp. 197–212. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-02408-5 14
10. Hoare, C.A.R.: An axiomatic basis for computer programming. Commun. ACM
12(10), 576–583 (1969)
11. Kramer, J., Magee, J.: The evolving philosophers problem: dynamic change man-
agement. IEEE Trans. Softw. Eng. 16(11), 1293–1306 (1990)
12. Aßmann, U., Bencomo, N., Cheng, B.H., France, R.B.: Models@runtime (Dagstuhl
seminar 11481). Dagstuhl Rep. 1(11), 91–123 (2012)
13. Malakuti, S., Bockisch, C., Aksit, M.: Applying the composition ﬁlter model for
runtime veriﬁcation of multiple-language software. In: 20th International Sympo-
sium on Software Reliability Engineering, pp. 31–40. IEEE, November 2009

Neural Networks for Biomedical Signals
Classiﬁcation Based on Empirical Mode
Decomposition and Principal
Component Analysis
Abdoul Dalibou Abdou2, Ndeye Fatou Ngom1(B), Samba Sidib´e1,
Oumar Niang1, Abdoulaye Thioune3, and Cheikh H. T. C. Ndiaye3
1 Laboratoire Traitement de l’Inforrmation et Syst´emes Intelligents (LTISI),
Ecole Polytechnique de Thies (EPT), Thies, Senegal
fngom@ept.sn
2 Univert´e de Thies, LTISI-EPT, Thies, Senegal
abdouldalibou@gmail.com
3 Univerit´e Cheikh Anta Diop de Dakar, LTISI-EPT, Dakar, Senegal
Abstract. The three main events presented in the electrocardiogram
(ECG) signal of each heartbeat are: the P wave, the QRS complex and
the T wave. Each event contains its own peak, making this important to
analyze their morphology, amplitude and duration for cardiac abnormal-
ities. In this study, we propose a system for biomedical signal analysis
based on empirical mode decomposition. Mustispectral analysis is ﬁrst
performed to remove noise, detect QRS complex and compute the QRS
wide. Then statistical features and QRS wide are after used as inputs of
classiﬁer based on neural network model. The proposed methodology is
tested on real biomedical data and discussed.
Keywords: Empirical mode decomposition · Neural network
ECG signal classiﬁcation
1
Introduction
The three main events presented in the electrocardiogram (ECG) signal of each
heartbeat are: the P wave, the QRS complex and the T wave. Each event contains
its own peak, making this important to analyze their morphology, amplitude
and duration for cardiac abnormalities. In order to provide tools to contribute
to more accurate diagnosis, several signal processing algorithms have been devel-
oped to facilitate the continuous follow up and customized care. Al-Ashkar [1]
used non linear ﬁltering scheme for edge detection according a transition slope
sign. Rodriguez et al. [11] proposed feature extraction based on Hilbert trans-
form, adaptative threshold and principal component analysis. Empirical mode
decomposition (EMD) has been also widely used for source separation and noise
elimination. Indeed, EMD is adaptive, depends on the position of the extrema
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 267–278, 2018.
https://doi.org/10.1007/978-3-319-72965-7_25

268
A. D. Abdou et al.
of the signal, is non-linear and non-stationary [4,6,7,12]. Many methods have
been developed for classiﬁcation based on non linear features of ECG signals [3],
machine vector support [9], Markov chain models [2] and artiﬁcial neural net-
work (ANN) models [8,10,13]. Compared to the other methods, Neural networks
oﬀer a number of advantages: less formal statistical training, ability to implic-
itly detect complex nonlinear relationships between dependent and independent
variables, ability to detect all possible interactions between predictor variables,
and the availability of multiple training algorithms [14]. However, NN is often
described as a black box learning approach. In this study, we propose a system
for biomedical signal analysis based on Empirical Mode Decomposition (EMD)
and multilayer perceptron neural network model. The Butterworth ﬁltering is
ﬁrst used for signal noise elimination and then QRS detection is carried out using
empirical Mode Decomposition (EMD). Statistical features and morphological
feature such as QRS wide are used as inputs of a Multi Layer Perceptron network
model. The schematic diagram of the processing steps is illustrated by Fig. 1.
The paper is organized as follows. Section 2 recalls related works and some
basics concepts. Section 3 deals with the proposed noise elimination method,
QRS wide detection and the proposed. Section 4 illustrates the developed method
on real biomedical data and discusses the obtained results. The proposed method
is applied to PhysioNet ECG database for classiﬁcation of normal and abnormal
ECG signals. Section 5 gives conclusion and perspectives of this study.
Fig. 1. Schematic diagram of the processing steps: noise elimination process is ﬁrst
performed before a classiﬁcation based on features extracted using empirical mode
decomposition (EMD) and principal component analysis (PCA)
2
Tools for Biomedical Signal Analysis
This section recalls some basics tools for ECG arrhythmia analysis such as
feature extraction, empirical mode decomposition and the classiﬁer construc-
tion basis.

Biomedical Signals Classiﬁcation
269
2.1
Feature Extraction
In general, the ECG signal of a single cardiac cycle lies on the P, T and QRS
complex waves as depicted in Fig. 2. The ECG analysis is related to the detection
of QRS because of the presence of low amplitudes, negative polarities and noise.
The QRS wide is one of the most feature selected for signal recognition and
classiﬁcation. In this study, the QRS detection is performed using the empirical
mode decomposition (EMD).
Fig. 2. Electrocardiogram QRS complex [11]: cardiac cycle lies on the P, T and QRS
complex waves.
2.2
Empirical Mode Decomposition
EMD algorithms decompose iteratively a complex signal s (n) into elementary
AM-FM type components called Intrinsic Mode Functions (IMF) [6]:
s (n) = rk (n) +
K

k=1
imfk (n)
(1)
where imfk is the k-th mode or IMF of the signal and rk stands for residual
trend. Sifting procedure generates a ﬁnite number of IMFs. Indeed, the under-
lying principle of the EMD decomposition is to locally identify in the signal, the
most rapid oscillations deﬁned as the waveform interpolating local maxima and
minima. To do so, these later points are interpolated with cubic spline to yield
the upper and lower envelopes. The mean envelope is then subtracted from the
initial signal and the same interpolation scheme is reiterated. In this study, we
use the algorithm presented in [6] for the empirical mode decomposition.
2.3
Principal Component Analysis
Principal component analysis is carried out to select the most important features
of the ECG data, among computed statistical properties (mean, variance, covari-
ance, correlation, energy, power...), to reduce their number and at the same time

270
A. D. Abdou et al.
retain as possible of their class discriminatory information. The PCA involves
three main phases
1. Computation of variance matrix from data X
V =

X −˜X
 
X −˜X
T
(2)
where X is the data matrix and tildeX the mean vector of X.
2. Calculate the array of eigenvectors E and diagonal matrix of eigenvalues
E−1V E = D
(3)
3. Sort the eigenvectors in E in descending order of eigenvalues in D and project
the data on these eigenvector directions by taking the inner product between
the data matrix and the sorted eigenvector matrix
U =

ET 
X −˜X
T T
(4)
2.4
Classiﬁer Construction
ECG beat recognition and classiﬁcation depend on various features (morpholog-
ical, temporal and statistical). These features can be used as input of a neural
network classiﬁer. The most used neural networks are Multi Layer Perceptron
neural networks (MLP-NN). A MLP consist of an input layer, several hidden
layer and an output layer. A node also called a neuron includes a summer and
nonlinear activation function g as illustrated by Fig. 3.
Fig. 3. Schematic illustration of a node (neuron): the inputs of a neuron are multiplied
by weights summed up with bias terms.
The inputs (x1, ..., xK) to the neuron are multiplied by weights wki and
summed up together with the constant bias term θi. The resulting ni =
K
j=1 wjixj + θi is the input to the activation function g.
3
The Proposed Biomedical Analysis Methods
This section describes the proposed methods for noise removal, QRS wide extrac-
tion and classiﬁcation. The proposed classiﬁcation system adopts diﬀerent meth-
ods following morphological feature extraction through empirical mode decom-
position, QRS complex detection and the most discriminant statistical features
extraction using principal component analysis. For each signal, the noise is ﬁrst
removed before performing a QRS complex detection and classiﬁcation.

Biomedical Signals Classiﬁcation
271
3.1
Noise Removal
We are interested in noise elimination method for QRS wide detection. Zhang
et al. [15] listed many advantages of low Butterworth low pass ﬁlter of order 6
with frequencies from 5 to 15 for ECG signal noise removal. Other methods are
based on empirical mode decomposition. Indeed, the ﬁrst IMF contains mostly
high frequency noise and some QRS information [7]. However, if the ﬁrst IMF
are removed and others retained, the resulting output may contains considerable
level of noise. In this study we propose noise removal method based on Butter-
worth ﬁltering and the ﬁrst IMF removal method. The principle consist at ﬁrst
perform Butterworth ﬁltering for a smooth preprocessing and then remove the
ﬁrst IMF which contains high frequency noise.
3.2
QRS Complex Detection
Detection of QRS complex is the entry point of almost all ECG analysis tech-
nique. For a signal, we propose the step by step process described in Algorithm 1
for the QRS wide computation.
Algorithm 1. Pseudocode to compute the QRS wide
1: function larger(x)
2: x : ecg records
3: m, im : the maxima of x and it’s index
4: iim : the extremum maxima index
5: l1, l2 : previous and next index from iim
6: w : the QRS with
7: ex ←extrema(x) extrema
8: m ←max(x)
9: im, iim ←find(x = max(x)), find(ex = im)
10: l1, l2 ←ex(iim −1), ex(iim + 1)
11: w ←(l2 −l1)
12: return w
Our method for QRS detection is based on empirical mode decomposition
(EMD). Indeed, It has been shown that the ﬁrst IMFs will have the QRS infor-
mation as QRS regions is high frequency component [4,7,12]. The two ﬁrst
IMFs (Eq. 1) contain only the most low frequency and waves like P and T are
ﬁltered out from consideration [7]. Thus, in this study we suppose that the ﬁrst
IMF contains the QRS complex. The proposed method involves the computation
of the ﬁrst IMF before the QRS computation as described in Algorithm 1.
3.3
The Neural Network Model
The input data for the Neural Network are: the QRS width detection and
the most discriminant statistical properties among mean, variance, root mean

272
A. D. Abdou et al.
squared (RMS), energy and power. Each input data is associated a weight and
is computed from each record. If for example, the RMS is not a discriminant
properties then it’s weight is equal to zero.
For each node, the input is multiplied by weights (pm, en, pen, pp, pl, r): sp =
m ∗pm + v ∗pv + en ∗pen + p ∗pp + l ∗pl + s ∗r where m, pv, e, en, p, l, s are
respectively the mean, the variance, the energy, the power, the QRS width and
the RMS. Then, the diﬀerence s = sp −w between the neural network threshold
w and sp is used as input of the activation (sigmoid) function deﬁned as follows
f (x) =
1
(1 + exp(−x))
(5)
Algorithm 2 gives detailed description of the step processing of a node.
Algorithm 2. Pseudocode to compute a node output
1: function neurone(x,w)
2: x; ecg records
3: w : the node’s threshold
4: n : length(x)
5: s ←0
6: for i ←1 to n do
7:
s ←s + x(1, i) ∗x(2, i)
8:
i ←i + 1
9: end for
10: s ←s −w
11: y ←1/(1 + exp(−s))
12: return y
Let us suppose that, the number of input of the classiﬁer is six (the mean, the
variance, the energy, the power, the QRS width and the RMS). If a statistical
parameter is not considered after ACP analysis, then it’s associated weight is
equal to zero. We propose to compute the 1st node with all the six parameters,
the six other nodes (2nd, 3rd, 4th, 5th, 6th, 7th) with ﬁve parameters (the variance,
the energy, the power, the QRS width and the RMS) and the 8th node with the
output of the node 1, the node 2, the node 3, the node 4, the node 5, the
node 6 and the node 7. The computed nodes are then assembled using the step
processing described in Algorithm 3.
The training provide the thresholds of the network nodes. It is implemented
with a function called train which take as input the feature vector from the
training set database and gives as output a threshold. The threshold deﬁnes the
neural network quality and is computed from the nodes of the neural network.
Algorithm 4 describes the training evaluation process.
The error rate and the performance rate are used as validation and test
parameters during the classiﬁcation process (see Algorithm 5). The network per-
formance is evaluated with the percentage of correct ecg classiﬁcation results.

Biomedical Signals Classiﬁcation
273
Algorithm 3. Pseudo code for the neural network
1: function network(x,w)
2: x : ecg records
3: w : the threshold
4: v1 ←vector(x): six parameters
5: v2 ←[v1(1), v1(2), v1(3), v1(4), v1(5)]
6: v3 ←[v1(1), v1(2), v1(3), v1(4), v1(6)]
7: v4 ←[v1(1), v1(2), v1(3), v1(5), v1(6)]
8: v5 ←[v1(1), v1(2), v1(4), v1(5), v1(6)]
9: v6 ←[v1(1), v1(3), v1(4), v1(5), v1(6)]
10: v7 ←[v1(2), v1(3), v1(4), v1(5), v1(6)]
11: s1 ←neurone(v1, w)
12: s2 ←neurone(v2, w)
13: s3 ←neurone(v3, w)
14: s4 ←neurone(v4, w)
15: s5 ←neurone(v5, w)
16: s6 ←neurone(v6, w)
17: s7 ←neurone(v7, w)
18: v8 ←[s1, s2, s3, s4, s5, s6, s7]
19: s ←neurone(v8, w): the output of the network
20: return s
Algorithm 4. Pseudo code for the network performance evaluation
1: function evaluate(t,s)
2: t : test db
3: s : desired output
4: w ←train(t) threshold
5: r ←network(t, w) computed output
6: e ←error(s, r)
7: p ←performance(s, r)
8: v ←[e, p]
9: return v
4
Application to Biomedical Signal Analysis
The proposed methods for biomedical signal analysis are applied to PhysioNet
ECG database using Matlab 2013 software. The ECG records used in this study
is the MIT-BIH arrhythmia database [5]. Example of computed features on the
10 ﬁrst ecg records are resumed in Table 1.
4.1
MIT BIH Databases Records
The MIT-BIH arrhythmia database contains two channels ambulatory recordings
obtaining from 48 subjects studied by BIH Arrhythmia Laboratory between 1975
and 1979. The recordings were digitized at 360 samples per second per channel
with 11 bit resolution over a 10 mV range. In this study, the second channel
recording is studied. The records (100, 101, 102, 103, 104, 105, 106, 107, 108,

274
A. D. Abdou et al.
Table 1. Computed characteristics on a sample of ECG records
N o MIT-MIH
patient
Mean
(mm)
Variance
(mm2)
Root mean
square (mm)
Energy
(mm2)
Power
(mm2)
QRS wide
(ms)
1
100
−0.078
0.028
0.168
0.034
4.116
0.0500
2
101
0.152
0.015
0.122
0.038
4.59
0.1170
3
102
0.087
0.100
0.316
0.107
4.116
0.3000
4
103
0.182
0.002
0.041
0.035
4.212
0.0550
5
104
0.138
0.001
0.033
0.020
2.408
0.2050
0
50
100
150
200
250
300
350
400
450
500
−1
0
1
ecg 100
0
50
100
150
200
250
300
350
400
450
500
−0.5
0
0.5
IMF1
0
50
100
150
200
250
300
350
400
450
500
−0.2
0
0.2
IMF2
0
50
100
150
200
250
300
350
400
450
500
−0.2
0
0.2
IMF3
0
50
100
150
200
250
300
350
400
450
500
−0.2
0
0.2
IMF4
0
50
100
150
200
250
300
350
400
450
500
−0.1
0
0.1
IMF5
0
50
100
150
200
250
300
350
400
450
500
−0.1
0
0.1
IMF6
0
50
100
150
200
250
300
350
400
450
500
−0.3
−0.2
−0.1
IMF7
Fig. 4. MIT BIH patient 100 ECG empirical mode decomposition
109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 200, 201, 202,
203, 205, 207, 208, 209, 210, 212, 213, 214, 215, 217, 219, 220, 221, 222, 223, 228,
230, 231, 232, 233, 234) are numbered respectively from 1 to 48.
4.2
Noise Elimination
For each ecg record of the MIT BIH sample, a noise elimination procedure is
ﬁrst performed before the QRS wide computation. The proposed method is based
on empirical mode decomposition (EMD). Figure 4 shows an electrocardiogram
record and the resulting IMFs after the EMD decomposition.
Figure 5 illustrates the noise removal method proposed on two sets of
ECG samples.
If the ﬁrst IMF is removed, the signal shape is more smooth but there are
appearance of non desired artifacts. With the removal of the second IMF, the
shape is conserved but the noise are not removed. The Butterworth ﬁlter gives
good results but it changes sometimes the shape of signal. Indeed, for the MIT
BIH patient 116 the shape is locally changed whenever the Butterworth ﬁlter is
performed. We obtained better results with the proposed method which consist
at ﬁrst to perform Butterworth ﬁltering and after remove the ﬁrst IMF. The
amplitude of the QRS decreases but the computation of the extrema is more
easy. And as we are concerned in this study with the computation of QRS wide
from the extrema, the proposed method gives better results.

Biomedical Signals Classiﬁcation
275
100
200
300
400
500
−1.4
−1.2
−1
−0.8
−0.6
MIT BIH 116 patient ecg
100
200
300
400
500
−1.5
−1
−0.5
Ecg after Butterworth filtering
100
200
300
400
500
−1.6
−1.4
−1.2
−1
−0.8
−0.6
Ecg minus the first IMF
100
200
300
400
500
−1.4
−1.2
−1
−0.8
−0.6
Ecg minus the second IMF
100
200
300
400
500
−1.5
−1
−0.5
Second IMF removal after Butterworh filtering
100
200
300
400
500
−1.5
−1
−0.5
0
Removal of first IMF after Butterworth filtering
(a) MIT BIH patient 116
100
200
300
400
500
−0.6
−0.4
−0.2
0
0.2
MIT BIH 203 patient ecg
100
200
300
400
500
−0.6
−0.4
−0.2
0
0.2
Ecg after Butterworth filtering
100
200
300
400
500
−0.6
−0.4
−0.2
0
0.2
0.4
Ecg minus the first IMF
100
200
300
400
500
−0.6
−0.4
−0.2
0
0.2
Ecg minus the second IMF
100
200
300
400
500
−0.6
−0.4
−0.2
0
0.2
Second IMF removal after Butterworh filtering
100
200
300
400
500
−0.4
−0.2
0
0.2
Removal of first IMF after Butterworth filtering
(b) MIT BIH patient 203
Fig. 5. EMD based noise elimination for QRS wide detection: the proposed method
consist at ﬁrst to perform Butterworth ﬁltering and then remove the ﬁrst imf
4.3
Features Extraction and Analysis
After noise removal, the ECG QRS width is computed according the step by step
process described in Algorithm 1. The higher QRS width (0.3 ms) is obtained
with ECG record of MIT BIH patient 102. The lower QRS width (0.038 ms) is
computed from the ECG record of MIT BIH patient 124. The mean and the
dispersion computed from the ecg records are respectively 0.13 ms and 0.12.
From the considered statistical features, the most discriminant are extracted.
The evolution of the selected characteristics according to the ecg record number
is illustrated in Fig. 6.
To study the inﬂuence of the selected feature, we perform ecg record shape
similarity analysis. From the selected statistical properties and the QRS wide,
a clustering based on the minimal linkage distance is performed. The result is
illustrated in Fig. 7.
Among the more similar ecg records are: ecg 12 (MITBIH patient 112) and
ecg 22 (MITBIH patient 123), ecg 5 (MITBIH patient 104) and ecg 9 (MITBIH
patient 108), ecg 2 (MITBIH patient 101) and ecg 4 (MITBIH patient 103).
Among the most dissimilar ecg records are: ecg 12 (MITBIH patient 112) and

276
A. D. Abdou et al.
5
10
15
20
25
30
35
40
45
−1
−0.5
0
0.5
1
Ecg record number
statistical features
mean
Energy
5
10
15
20
25
30
35
40
45
−1
−0.5
0
0.5
1
Ecg record number
Selected parameters
mean
Energy
QRS wide
Fig. 6. Evolution of the selected feature characteristics according to the ecg record
number
12 22 28 30 29  3 24  5
 9
 2
 4
 1
 8 27  6 10 19 16 18 14 26 25 13 23 21 15 17 20  7 11
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Fig. 7. ECG clustering from the feature vector
ecg 11 (MITBIH patient 111), ecg 22 (MITBIH patient 123) and ecg 11 (MITBIH
patient 111). Ecg records of the MIT BIH patients 116 and 203 are also very
dissimilar (Fig. 4).
4.4
Classiﬁcation
The output coding is deﬁning as follows: 0 for abnormal signal and 1 normal
signal. The output of the neural network (Algorithm 3) computed from all the
nodes are then used for the ecg records classiﬁcation. Algorithm 5 gives the main
processing of the classiﬁer.
The network is created with 70% training set, 15% validation set and 15%
test set. The results are compared with clinical experimental results available
on the physio-net web set. The performance of the classiﬁcation is 82%. The
samples that were not correctly classiﬁed are MIT BIH patient 213, 214, 217,
220, 222, 231, 232 and 233.
Future studies will aim to improve the neural network architecture by more
straightening the choice of the number of nodes, the training set, the test set
and the validation set. The proposed classiﬁer and the performance evaluation
model will also be improved.

Biomedical Signals Classiﬁcation
277
Algorithm 5. Pseudo code for the classiﬁer
1: function classify(x, w)
2: x : ecg records
3: w ←train(t): the threshold
4: fs ←filter(x) the ﬁltered signal
5: v ←vector(fs): the vector of the ﬁltered signal attributes
6: rn ←network(v, w) the neural network output
7: if rn ≤0.5 then
8:
c ←0 for abnormal class
9: else
10:
c ←1 for normal class
11: end if
12: return c
5
Conclusion
In this work, tools for biomedical signal analysis are proposed. We ﬁrst pro-
pose noise removal process based on empirical mode decomposition (EMD) and
the Butterworth ﬁltering. Then, QRS complex detection method from EMD is
proposed and an algorithm for QRS wide computation is given and tested on
ECG records. This latter set was used to perform data analysis and study ECG
records similarities. A neural network classiﬁer taking as input statistical char-
acteristics and the QRS wide is also proposed and applied on MIT BIH ECG
records. In future works, the following improvements will be considered: improve
the proposed classiﬁcation method, use more geometrical ECG features, extends
the classiﬁcation method to biomedical signals of higher dimensions, performs
validation tests and add security supplement for data privacy management.
References
1. Al-Ashkar, T.: Medical multi signal signature recognition applied to cardiac diag-
nostic (2012)
2. Chang, P.C., Lin, J.J., Hsieh, J.C., Weng, J.: Myocardial infarcation classiﬁcation
with multi-lead ECG using hidden Markov models and Gaussian mixture models.
Appl. Soft Comput. 12, 3165–3175 (2012)
3. Fatin, A.E., Salim, N., Harris, A.R., Swee, T.T., Ahmed, T.: Arrhythmia recogni-
tion and classiﬁcation using combined linear and nonlinear feature of ECG signals.
Comput. Methods Programs Biomed. 127, 52–63 (2016)
4. Jin, F., Sugavaneswaran, L., Krishnan, S., Chauhan, V.S.: Quantiﬁcation of frag-
mented QRS complex using intrinsic time-scale decomposition. Biomed. Signal
Process. Control 31, 513–523 (2017)
5. Moody, G.B., Mark, R.G.: The impact of the MIT-BIH arrhythmia database. IEE
Eng. Med. Biol. 20, 45–50 (2001)
6. Niang, O., Thioune, A., Delechelle, E., Lemoine, J.: Spectral intrinsic decomposi-
tion method for adaptative signal representation. ISRN Sig. Process. 9, 3 (2012)
7. Pal, S., Mitra, M.: Empirical mode decomposition based on ECG enhencement and
QRS detection. Comput. Biol. Med. 42, 83–92 (2012)

278
A. D. Abdou et al.
8. Pasa, L., Testolin, A., Sperdui, A.: Neural networks for sequential data: a pre-
training approach based on hidden Markov models. Neurocomputing 169, 323–333
(2015)
9. Polat, K., Gunes, S.: Detection of ECG arrythmia using a diﬀerential expert system
approach based on principal component analysis and least square support vector
machine. Appl. Math. Comput. 186, 898–906 (2007)
10. Ripoll, V.J.R., Wojdel, A., Romero, E., Ramos, P., Brugada, J.: ECG assessment
based on neural networks with pretraining. Appl. Soft Comput. 49, 399–406 (2016)
11. Rodriguez, R., Mexicano, A., Bila, J., Cervantes, S., Ponce, R.: Feature extrac-
tion of electrocardiogram signals by aplying adaptative threshold and principal
component analysis. J. Appl. Res. Technol. 13, 261–269 (2015)
12. Slimane, Z.H., Nait Ali, A.: QRS complex detection using empirical mode decom-
position. Digit. Signal Proc. 20, 1221–1228 (2010)
13. Tjolleng, A., Jung, K., Hong, W., Lee, W., Lee, B., You, H., Son, J., Park, S.:
Classiﬁcation of a driver’s cognitive workload levels using artiﬁcial neural network
on ECG signals. Appl. Ergon. 59, 326–332 (2017)
14. Tu, J.V.: Advantages and disadvantages of using artiﬁcial neural networks versus
logistic regression for predicting medical outcomes. J. Clin. Epidemiol. 45, 1225–
1231 (1996)
15. Zhang, H.: An improved QRS wave group detection algorithm and matlab imple-
mentation. Phys. Procedia 25, 1010–1016 (2012)

A Software Architecture for Centralized
Management of Structured Documents
in a Cooperative Editing Workﬂow
Milliam Maxime Zekeng Ndadji(B) and Maurice Tchoup´e Tchendji(B)
Department of Mathematics and Computer Science, University of Dschang,
PO Box 67, Dschang, Cameroon
{ndadji.maxime,maurice.tchoupe}@univ-dschang.org,
ndadjimaxime@yahoo.fr,ttchoupe@yahoo.fr
Abstract. Nowadays, electronic documents are widely used as media of
exchange between actors involved in a given business process. Generally,
their contents provide information on both what has already been done
on this procedure and what remains to be done and by whom it should
be done. Badouel and Tchoup´e proposed a modelling of the life cycle of
such documents through an administrative workﬂow in which, each of the
participants manipulates a partial replica of the document representing
at some point, the current state of the process execution; the overall state
of the process being obtained by merging diﬀerent partial replicas. This
paper presents a modular software architecture for the implementation
and management of such workﬂow systems.
Keywords: Software architecture · Structured documents
Cooperative editing workﬂow · Workﬂows management system
Cross fertilization
1
Introduction
Workﬂow technology, emerged in the 80s, knows a quickly growing in the indus-
try of computer-aided production. This interest is due to the ability of this one,
to allow companies to reduce costs of their productions, to quickly and easily
develop new products and services, and therefore, to be competitive [4]. Work-
ﬂow technology provides methods and tools (notations, management systems, ...)
for the speciﬁcation, automation, optimization and monitoring of business
processes [4].
A complex workﬂow system may be composed of several subsystems (sites)
distributed across a network; these coordinating themselves through a workﬂow
management system that can use for this purpose, documents (artifacts) issued
from the various subsystems. Badouel and Tchoup´e [5] have theorized a business
process running approach in which, the state of the process at some point is
represented by a document, and stakeholders from subsystems work by editing
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 279–291, 2018.
https://doi.org/10.1007/978-3-319-72965-7_26

280
M. M. Zekeng Ndadji and M. Tchoup´e Tchendji
and exchanging (partial) replicas of documents representing their perceptions
of the workﬂow at any given time. Therefore, each subsystem (actor) has a
partial view of the overall state of the workﬂow at any given time, and the
current (global) workﬂow state is given by the merging of diﬀerent artifacts
(documents) from the various subsystems. In their model, collaborations between
actors can be divided into three sequential phases (Fig. 1): the distribution phase
where global artifact (a structured document) is replicated (partially) to each
subsystem (site), the edition phase in which local processes of subsystems are
executed, inducing an update of the local replica of the global artifact, and ﬁnally,
the synchronization phase in which the various local artifacts updated are merged
into a global artifact. Thus presented, the execution of such a workﬂow is similar
to the cooperative editing of a structured document. That is why in the rest of
this manuscript, in order to alleviate the speech and use an easily accessible
jargon, we will only use expressions from the ﬁeld of structured editing.
We present in this paper, a modular software architecture for the imple-
mentation of a centralized management of cooperative editing workﬂows where,
stakeholders operate on partial replicas of the document edited collectively. Some
business components of this architecture have already been developed in previ-
ous works [5,12–14]. We describe here primarily, the other technical components
and their orchestration for the eﬀective implementation of a cooperative editing
workﬂow management system as described in [5].
The rest of this manuscript is organized as follows: we present a literature
review on cooperative editing workﬂows as well as software architectures used for
their implementation (Sect. 2). We then present (Sect. 3), a software architecture
for designing cooperative editing workﬂow management systems as described in
[5], and a sample implementation via an editor prototype we developed. Section 4
is devoted to the conclusion.
2
Basic Concepts and Approaches to Manage
Cooperative Editing Workﬂows
2.1
Workﬂow and Cooperative Editing
A workﬂow can be deﬁned as a collection of tasks organized and performed either
by software systems, by humans or both, in order to accomplish some business
process [4]. The aim of the latter is to streamline, coordinate and control busi-
ness processes in an organized, distributed and computerized environment. [4,17]
informally deﬁne a business process as an ordered sequence of tasks (made by
men or by programs, even both sometimes) that meets a speciﬁc scheme and
leads to a speciﬁc result (tracking a medical record [15] is a frequent example of
business process). It is obvious that a business process can be (considered as)
a Computer-Supported Cooperative Work (CSCW). Therefore, workﬂow man-
agement1 needs to facilitate trade, coordination, collaboration and co-decision
1 Informally, we design by workﬂow management, the modeling and computerized
management of all tasks and various actors involved in a business process.

Management of Structured Documents in a Cooperative Editing Workﬂow
281
between the participants to the underlying business process. To do this, we can
use electronic documents [4,5,10] as media (carriers): it is said in this case, that
the workﬂow is document-centric [3]. In such workﬂows, documents move from
one site to another, and are edited progressively2: They therefore contain, at
any time, informations that are indispensable to the cooperation. Since these
documents are edited by diﬀerent actors, it is a cooperative editing.
Cooperative editing is a work of hierarchically organized groups that oper-
ates according to a schedule involving delays and a division of labor (coordina-
tion). Like any CSCW, cooperative editing is subject to spatial and temporal
constraints. Thus, we distinguish distributed or not, and asynchronous or syn-
chronous cooperative editing. When distributed, the various editing sites are
geographically dispersed and each of them has a local copy of the document
to be edited; systems that support such an edition should oﬀer algorithms for
data replication [23] and for the fusion of updates. When asynchronous, various
co-authors get involved at diﬀerent times to bring their diﬀerent contributions.
A cooperative editing workﬂow goes generally, from the creation of the doc-
ument to edit, to the production of the ﬁnal document through the alternation
and repetition of distribution, editing and synchronization phases. The literature
is full of several cooperative editing workﬂows and of their management systems.
We present a few in the next section.
2.2
Cooperative Editing Workﬂows and Management Approaches
Real-Time Cooperative Editing Workﬂows. In these generally centralized
systems (Etherpad3 [8], Google Docs4, Framapad5, Fidus Writer6 [11], ...), the
original document is created by a co-author on the central server. The latter
then invites his colleagues to join him for the editing; they therefore connect
to the editing session usually identiﬁed by a URL (distribution phase, although
the document is generally not really duplicated). During an editing session (syn-
chronous editing phase), all connected co-authors work on a single copy of the
document but in diﬀerent contexts. When the integration is automatic, changes
performed by one of them are immediately (automatically) propagated to be
incorporated into the basic document (synchronization phase), and the latter is
then redistributed to others. The changes are saved progressively and the server
usually keeps multiple versions of the document.
The majority of real-time editors uses the model of operational transfor-
mations [6,16]. Their architectures are therefore based on the one deﬁned by
this model. Meaning that, they distinguish two main components: an integration
2 Actors do not necessarily publish documents through specialized software (texts
editors); they perform their tasks related to their respective areas of expertise and
documents are amended accordingly.
3 http://www.etherpad.org/.
4 https://www.docs.google.com/.
5 http://www.framasoft.org/.
6 https://www.ﬁduswriter.org/.

282
M. M. Zekeng Ndadji and M. Tchoup´e Tchendji
algorithm, responsible for the receipt, dissemination and execution of operations
and a set of processing functions that are responsible of “merging” updates by
serializing two concurrent operations. This type of editing workﬂow works great
for small groups.
Asynchronous Cooperative Editing Workﬂows: This edit mode is distin-
guished by real distribution phases in which, the document to edit is replicated
on diﬀerent sites, using appropriate algorithms [23]. A co-author may then con-
tribute at any time by editing his local copy of the document. Here we focus on
a few asynchronous cooperative editors operating in client-server mode.
Wikiwikiweb: Wikis [19,22] are a family of collaborative editors for editing
web pages from a browser. To edit a page on a Wiki, you must duplicate it and
contribute. After editing, you just have to save to publish a new version of that
page. In the case of a competing editing, it is the last published version which
will be visible. Even though it is still possible to access the previously published
versions, there is no guarantee that a new version of the page preserves intentions
(incorporates aspects) of previous versions. For this aspect, a Wiki can be seen
much more as a web page version manager.
CVS (Concurrent Versions System): Under CVS [2], versions of a docu-
ment are managed in a space called repository and each user has a personal
workspace. To edit a document, the user must create a replica in his workspace.
He will amend this replica, then will release a new version of the document in
the repository. In case the document is edited by several people concurrently and
at least one update has already been published, the author wishing to publish a
new update will be forced to consult and integrate all previous updates through
dedicated tools integrated in CVS.
SVN (Subversion): SVN7 [21] was created to replace CVS. Its main goal was to
propose a better implementation of CVS. So as CVS, SVN relies on an optimistic
protocol of concurrent access management: the copy-edit-merge paradigm. SVN
provides many technical changes like: a new commit algorithm, the management
of metadata versions, new user commands and many others features.
Git: The main purpose of Git8 [20] is the management of various ﬁles in a
content tree considered as a deposit (all ﬁles of a source code for example). To
edit a deposit, a given user connects to it and clones (forks). He obtains a copy
of that deposit, modiﬁes it locally through a set of commands provided by Git.
Then he oﬀers his contribution to primary maintainer who can validate it and
thus, merges it with the original deposit. During this operation, new versions of
modiﬁed ﬁles are created in the main repository. It is therefore possible under
Git, to access any revision of a given ﬁle.
7 http://www.subversion.apache.org/.
8 https://www.git-scm.com/.

Management of Structured Documents in a Cooperative Editing Workﬂow
283
Badouel and Tchoup´e Cooperative Editing Workﬂow. Badouel and
Tchoup´e [5] proposed a workﬂow for cooperative editing of structured docu-
ments (those with regular structures deﬁned by grammatical models such as
DTD, XML schema [18], ...), based on the concept of “view”. The authors use
context-free grammars as documents templates. A document is thus, a derivation
tree for a given grammar.
The life cycle of a document in their workﬂow can be sketched as follows:
initially, the document to edit (t) is in a speciﬁc state (initial state); various co-
authors who are potentially located in distant geographical sites, get a copy
of t that they edit locally. For several reasons (conﬁdentiality, security, eﬃ-
ciency, ... [12]), a given co-author “i” has not necessarily access to all the gram-
matical symbols that appear in the tree; only a subset of them can be considered
relevant for him: that is his view (Vi). The locally edited document, is therefore
a partial replica (denoted tVi) of the original document. This one is obtained by
projection (π) of the original document with regard to the view of the consid-
ered co-author (tVi = πVi(t)). The edition is asynchronous and local documents
obtained are called updated partial replicas denoted by tmaj
Vi
.
The authors of [5] focus only on the positive edition; edited documents are
only increasing, and the co-authors cannot remove portions of the document
when a synchronization was already performed. For both ensure that property
and to be able to tell a co-author where he shall contribute, the documents being
edited are represented by trees with buds that indicate the only places where
editions are possible. Buds are typed; a bud of sort X is a leaf node labeled Xω: it
can only be edited (extended in a sub-tree) by using a X-production (production
with X as left hand side).
When a synchronization point9 is reached, all contributions tmaj
Vi
of diﬀerent
co-authors are merged in a single global document tf 10. To ensure that the
merging is always possible (convergence), the authors of [5] assume that on each
site, the editions are controlled by a local grammar. These local grammars are
obtained from the global one, by projection along the corresponding views [12]. It
can still happen that updates being not compatible11 and therefore the merging
being impossible. To overcome this, we have proposed in [13], an algorithm to
reconcile partial replicas in conﬂict.
Figure 1 give an overview with a BPMN (Business Process Modeling Nota-
tion) orchestration diagram, of the workﬂow of cooperative editing of a struc-
tured document according to Badouel and Tchoup´e [5] proposal; at site 1,
9 A synchronization point can be deﬁned statically or triggered by a co-author as soon
as certain properties are satisﬁed.
10 Sometimes the edition should be continued after the merging (it is so, when there
still buds in the merged document): the document must be redistributed to each of
the n co-authors, a (partial) replica tVi of tf, ensuring that tVi = πVi(tf), for the
continuation of the editing process.
11 This is particularly the case if there is at least one node of the global document
accessible by more than a co-author and edited by at least two of them using diﬀerent
productions.

284
M. M. Zekeng Ndadji and M. Tchoup´e Tchendji
Fig. 1. A BPMN orchestration diagram sketching a cooperative editing workﬂow of a
structured document according to Badouel and Tchoup´e [5].
operations of (re)distribution and merging of the document in accordance with
a (global) model G are realized; at sites 2 and 3, edition of partial replicas
in accordance with (local) models G1 and G2 derived by projecting the global
documents template G are done.
In summary, the workﬂow of Badouel and Tchoup´e is diﬀerent from the
others because of the concept of view and by the fact that it manipulates exclu-
sively (partial) structured documents. The other workﬂows globally diﬀer in their
approaches and objectives.
We propose in the next section, a ﬂexible software architecture for automat-
ing business processes that could be modelled as workﬂows of Badouel and
Tchoup´e [5].
3
A Software Architecture for Centralized Management
of Cooperative Editing Workﬂows
3.1
Motivations
The contribution we make to the implementation of workﬂow management sys-
tems is based on the type of workﬂow speciﬁed in [5]. The choice focused on this
type of workﬂow is motivated by the fact that:
1. It applies to structured documents: this leads to the fact that we can locally
perform validations in accordance with a local model derived from the global
one;
2. It is particularly suited for administrative workﬂows: concepts of view and
partial replica introduced by Badouel and Tchoup´e, make that the type
of workﬂow they oﬀer is particularly adapted for the speciﬁcation of many

Management of Structured Documents in a Cooperative Editing Workﬂow
285
administrative processes. Consider, for example, the process “tracking a med-
ical record in a health center with the reception and consultation services”:
the aforesaid record can be modelled as a structured document in which the
members of the host service (reception) cannot view and/or modify certain
information contained therein; those information, requesting the expertise of
the consulting staﬀfor example. Therefore we can associate views to each of
these services. It remains only to specify the medical record’s circuit and a
workﬂow of the type described in [5] is obtained;
3. It does not exist a generic architectural model describing precisely an app-
roach for the implementation of this type of workﬂow: the only prototype
[14] which was designed around the concepts handled (view, partial replica,
merging, . . . ) for this type of workﬂow, was more a graphic tool (editor) for
the experimentation of concepts and algorithms presented in [5]; workﬂow
management is not addressed in it: this tool cannot be used to specify an
editing workﬂow, it does not support routing or storage of artifacts, nothing
is done concerning monitoring, . . . yet these concerns are among the most
important to be taken care of by a workﬂow management infrastructure [10].
3.2
Proposed Architecture
Overall Operations. The architecture that we propose is composed of three
tiers: clients, a central server and administration tools. We consider that, each
participant in a given workﬂow has a client. Initially, the workﬂow owner (com-
parable to a deposit owner in Git (see footnote 8) [20]) connects to the server
from his client. He creates his workﬂow by specifying all necessary informations
(the workﬂow name, the overall grammar, diﬀerent participants, their rights and
their views, the basic document and the workﬂow’s circuit), then triggers the pro-
cess. Next, participants concerned by the new created workﬂow receive an alert
message from the system, inviting them to participate. Each participant must
therefore connect himself to the server to obtain a partial replica of the workﬂow
model (encoded in a speciﬁcation ﬁle written in a dedicated DSL (Domain Spe-
ciﬁc Language [1])) and state (his local document model, a partial replica of the
initial document, ...) according to his rights and his view on the given workﬂow.
A given participant performs its duties and submits its local (partial) replica to
the central server which performs synchronizations as soon as possible and the
process continues (see Fig. 1) until the end. For speciﬁc needs (authentication,
access to corporate data, . . . ), clients and server may require the intervention
of an administration tool (database, paperwork and many others). These three
tiers are interconnected around a middleware as presented in Fig. 2.
Server Architecture. The server is responsible for the storage, restoration,
execution and monitoring of workﬂows. Its architecture is based on three basic
elements as shown in Fig. 2(a): its model, storage module and its runtime engine.
1. The model: it is the one orchestrating all the tasks supported by the server.
It consists of a workﬂow engine, a set of parsers and three communication

286
M. M. Zekeng Ndadji and M. Tchoup´e Tchendji
Fig. 2. A software architecture (3-tiers) for centralized management of workﬂows of
cooperative editing of structured documents.
interfaces (the interface with the middleware, that with the storage module
and the one with the runtime engine).
2. The storage module: it is responsible for the storage of workﬂows. Like CVS, it
maintains a main repository for each workﬂow. The repository space of a given
workﬂow includes its speciﬁcation ﬁle written in a DSL [1] which is the subject
of a work in progress12. There are also (global) document versions showing
the state of the workﬂow at given times. These versions of the underlying
documents, facilitate the control and monitoring of workﬂows.
3. The runtime engine: it consists of implementations of projection, expansion
[5] and consensual merging [13] algorithms. These implementations are used
by the workﬂow engine in the realization of these tasks. A runtime engine
written entirely in Haskell, was proposed in [14]. However, it is quite rigid
and almost impossible to adapt to the architecture presented here. To this
end, we present in Sect. 3.3, a more ﬂexible version of the latter.
12 Indeed, workﬂows such as we manipulate, can not be easily speciﬁed in their entirety
with the help of current notations (BPMN, statescharts [5], Petri Network with
Objects [15], . . . ).

Management of Structured Documents in a Cooperative Editing Workﬂow
287
Client Architecture. The client (Fig. 2(b)) is also based on three entities: a
model, an editing engine and a storage module. The model is responsible for
organizing and controlling the execution of tasks and user commands. For each
new local workﬂow, the model generates an editing environment which is used
by the editing engine to provide conventional facilities of structured document
editors (compliance check, syntax highlighting, graphical editing of documents
presentations, . . . ). Each workﬂow is locally represented by a speciﬁcation ﬁle
and by one structured document (an update of a partial replication of a global
one) representing the current perception of the overall workﬂow from the current
local site. When reaching synchronization phases, the local structured document
is forwarded to the server site, where it is merged with others in one structured
document representing the current state of the overall workﬂow: they are there-
fore, coordination supports between workﬂow engines of the client and of the
server.
The Middleware. The middleware is responsible for the interaction between
diﬀerent tiers of our architecture. It must be designed so that the coupling
between these tiers is as weak as possible. One can for this purpose, consider a
Service-Oriented Architecture (SOA) in which:
– Our clients are service clients;
– The server is a service provider for clients and a client of services oﬀered by
the administration tools;
– The administration tools are service providers.
With such an architecture, we can guarantee the independence of each tier and
thus, an easier maintenance.
3.3
TinyCE v2
Due to its technical nature and to the number of technologies it needs for its
instantiation, the architecture presented in this paper has not yet been fully
implemented. However, many of its components have already been implemented
and tested in a test project called TinyCE v213 (a Tiny Cooperative Editor
version 2).
TinyCE v2 is an editor prototype providing graphic and cooperative editing
of the abstract structure of structured documents. It is used following a net-
worked client-server model. Its user interface oﬀers to the user, facilities for the
creation of workﬂows (documents, grammars, actors and views (Fig. 3)), edit-
ing and validation of partial replicas (Fig. 4). Moreover, this interface also oﬀers
to him, functionality to experiment the concepts of projection, expansion and
consensual merging (Fig. 5). TinyCE v2 is designed using Java and Haskell lan-
guages. It oﬀers several implementations of our architecture concepts namely:
parsers, storage modules, server’s runtime engine, workﬂow engines and commu-
nication interfaces.
13 TinyCE v2 is a more advanced version of TinyCE [14].

288
M. M. Zekeng Ndadji and M. Tchoup´e Tchendji
1- Workﬂow name, synchronization server and then, click on "Next"
2- Deﬁnitions of productions, axiom of the grammar and various views.
3- Informations on workﬂow owner and on diﬀerent co-authors
4- Creation of the global initial document
Fig. 3. Some screenshots showing process of creation of a workﬂow of cooperative
editing in TinyCE v2.
Fig. 4. Some screenshots of TinyCE v2 showing the authentication window of a co-
author (Auteur1) as well as those displaying the various local and remote workﬂow in
which he is implicated.

Management of Structured Documents in a Cooperative Editing Workﬂow
289
Fig. 5. An illustration of consensual merging in TinyCE v2.
As in [14], the runtime engine of TinyCE v2 exploits the possibility oﬀered
by Java, to run an external program. Indeed, we designed an interface of TinyCE
v2 (runtime interface) capable of launching a Haskell interpreter (GHCi - Glas-
gow Haskell Compiler interactive - [7] in this case) and make it execute various
commands. When creating a workﬂow, TinyCE v2 generates a Haskell program
ﬁle (.hs) [9], containing data types and functions necessary to achieve the oper-
ations of projection, expansion and consensual merging on the structured docu-
ment representing the state of that workﬂow. In this way, we considerably reduce
the use frequency of parsers presented in [14]. The functions are more open to
changes as they are contained in a text ﬁle and not in a compiled program as
in [14]. In fact, the main diﬀerences between our Java-Haskell cross-fertilization
approach and the one of [14] are almost the same that drive the debates on inter-
preted and compiled languages; our approach is likened to interpreted languages
and that of [14], to compiled languages. So, even though our approach can present
security risks (that can be addressed using PKI (Public Key Infrastructure) and
standard encryption systems like AES (Advanced Encryption Standard), RSA
(Rivest Shamir Adleman) ...), it has the advantage of being portable and easier
to maintain.

290
M. M. Zekeng Ndadji and M. Tchoup´e Tchendji
4
Conclusion
We presented in this paper, a 3-tiers software architecture for the production of
a system for centralized management of administrative workﬂows that can be
modelled as cooperative editing of structured documents. This type of workﬂow
has been subject of previous works. The particularity here is that, according to
their views, actors only have partial perception of the overall document. Previous
works has been focused on the deﬁnition of basic concepts, mathematical models
and punctual functions of (models and documents) replication and fusion (some
of them are written in the lazy functional language named Haskell). Here, we
have proposed a modular architecture for such a system by presenting a coher-
ent arrangement of independent modules. Our architecture is ﬂexible because it
leaves to the designer the choice of certain design methods and to the developer,
the one of the main languages and tools to use. Inspired by the TinyCE system
[14], we proposed an implementation of our architecture. In short, the architec-
ture presented here, its diﬀerent components and their implementation are the
fruit of this work.
Some of our subsequent work focus on the design of a DSL for specifying
workﬂows theorized in [5]. An avenue to explore is rather in creating a DSEL
(Domain Speciﬁc Embedded Language) of BPMN and enrich it by the con-
cepts of grammatical model, view, editing, compliance and validation. We are
also investing to complete the implementation of the architecture presented in
this paper.
References
1. Van Deursen, A., Klint, P., Visser, J.: Domain-speciﬁc languages: an annotated
bibliography. ACM SIGPLAN Not. 35(6), 36 (2000)
2. Berliner, B.: CVS II: parallelizing software development. In: Proceedings of the
USENIX Winter 1990 Technical Conference, Berkeley, Californie, Etats-Unis, pp.
341–352. USENIX Association (1990)
3. Frye, C.: Move to workﬂow provokes business process scrutiny. Softw. Mag. 14(4),
77–85 (1994)
4. Georgakopoulos, D., Hornick, M., Sheth, A.: An overview of workﬂow management:
from process modeling to workﬂow automation infrastructure. Distrib. Parallel
Databases 3, 119–153 (1995)
5. Badouel, E., Tchoup´e, M.: Merging hierarchically structured documents in work-
ﬂow systems. Electron. Notes Theor. Comput. Sci. 203(5), 3–24 (2008). Proceed-
ings of the Ninth Workshop on Coalgebraic Methods in Computer Science (CMCS
2008), Budapest
6. Oster, G.: R´eplication optimiste et coh´erence des donn´ees dans les environnements
collaboratifs r´epartis. Autre [cs.OH]. Th`ese de Doctorat/Ph.D., Universit´e Henri
Poincar´e - Nancy I (2005)
7. GHC: The Glasgow Haskell Compiler: GHC. http://www.haskell.org/ghc/
8. Giannetti, J., Lord, M.-A.: Une plateforme Web pour soutenir la r´e´ecriture collab-
orative: EtherPad, Premi`ere partie. Formation et profession 23(1), 71–73 (2015).
https://doi.org/10.18162/fp.2015.a51

Management of Structured Documents in a Cooperative Editing Workﬂow
291
9. Haskell: A purely functional language. http://www.haskell.org
10. Institute of Management Accountants: Implementing automated workﬂow manage-
ment. Business Performance Management, IMA Publication Number 00354 (2002).
ISBN 0-86641-290-5
11. Wilm, J., Frebel, D.: Real-world challenges to collaborative text creation. ACM,
14 September 2014. ISBN 978-1-4503-2964-4
12. Tchoup´e, M.T., Atemkeng, M.T., Djeumen, R.: Un mod`ele de documents stable
par projections pour l’´edition coop´erative asynchrone. In: CARI 2014 Proceedings,
vol. 1, pp. 325–332 (2014)
13. Tchoup´e, M.T., Ndadji, M.M.Z.: R´econciliation par consensus des mises `a jour des
r´epliques partielles d’un document structur´e. In: CARI 2016 Proceedings, vol. 1,
pp. 84–96 (2016)
14. Tchoup´e, M.T.: Fertilisation crois´ee d’un langage fonctionnel et d’un langage objet:
application `a la mise en oeuvre d’un prototype d’´editeur coop´eratif asynchrone. In:
CARI 2010 - Yamoussoukro, pp. 541–549 (2010)
15. Chaˆabane, M.A., Bouzguenda, L., Bouaziz, R., Gargouri, F.: Sp´eciﬁcation des pro-
cessus workﬂows ´evolutifs versionn´es. Schedae, pr´epublication num´ero 11, fascicule
num´ero 2, pp. 21–29 (2007)
16. Tlili, M.: Infrastructure P2P pour la R´eplication et la R´econciliation des Donn´ees.
Base de donn´ees [cs.DB]. Th`ese de Doctorat/Ph.D., Universit´e de Nantes (2011)
17. Curcin, V., Ghanem, M.: Scientiﬁc workﬂow systems - can one size ﬁt all? In:
Proceedings of the 2008 IEEE, CIBEC 2008 (2008)
18. W3C. extensible markup language (xml), W3C Recommendation 1.0 (second edi-
tion), octobre 2000
19. Cunningham,
W.:
Wikiwikiweb
history
(2005).
http://c2.com/cgi/wiki?
WikiHistory
20. Wikip´edia: git - Wikip´edia. https://fr.wikipedia.org/wiki/git
21. Some of the authors of Subversion: Version Control with Subversion. http://
svnbook.red-bean.com/
22. Wikimedia: Wikipedia: the free encyclopedia that anyone can edit (2005)
23. Saito, Y., Shapiro, M.: Optimistic replication. ACM Comput. Surv. V(3), 1–44
(2005)

Classiﬁcation Model of Spikes Morphology Using Principal
Components Analysis in Drug-Resistant Epilepsy
Ousmane Khouma1(✉), Mamadou Lamine Ndiaye1, Idy Diop1, Samba Diaw1,
Abdou K. Diop1, Sidi Mohamed Farsi1, Birahime Diouf1, Khaly Tall1,
and Jean J. Montois2
1 Medical Imagery and Bioinformatics Laboratory (LIMBI),
Ecole Supérieure Polytechnique (ESP), Cheikh Anta Diop University, Dakar, Senegal
{ousmane.khouma,mamadoulamine.ndiaye}@ucad.edu.sn
2 Signal and Image Processing Laboratory (LTSI INSERM RENNES 1), Rennes, France
Abstract. Epilepsy is one of the diseases that are more subject to consultation
in neurological clinics. To help neurologists to accurately diagnose this disease,
several technological tools have been developed. Electroencephalography (EEG)
of scalp or deep is a signal acquisition tool from electrical discharges of the brain
areas. These signals are often accompanied by transient events commonly called
interictal paroxystic events (IPE) or spikes of short durations. Analysis of these
IPE could help with the diagnosis of drug-resistant epilepsy. With this intention,
we will ﬁrst of all seek to detect IPE, by separating them from the basic activity
of signal EEG. In this paper, we propose spike detection method based on
Smoothed Nonlinear Energy Operator (SNEO) using adaptive threshold. Then
we will implement a new approach using principal components analysis (PCA)
before classiﬁcation to separate the events detected according to their morphol‐
ogies. The objective in the long term is to characterize their space-time distribu‐
tion over all the duration of the EEG signal.
Keywords: Epilepsy · Spike detection · SNEO · PCA
Unsupervised classiﬁcation
1
Introduction
Epilepsy is a neurologic aﬀection which touches most of the world population. It is
characterized by an abnormal and excessive discharge of a more or less important
neuronal population. This disease poses, still today, of the problems of treatments
(drugs) [1], and modern medicine does not encounter yet today diﬃculties of treating
certain cases. Indeed, some forms of epilepsy called drug-resistant are resistant to all
medication [2]. In these cases, a surgical treatment can be considered but its application
requires a complex pre-surgical evaluation. In order to ﬁnd new solutions therapeutic,
the scientiﬁc communities and medical study, since years, the way in which the crises
occur.
We distinguish two periods in the life cycle from an epileptic: critical periods char‐
acterized by the crises and the interictal periods characterized by interictal paroxystic
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 292–303, 2018.
https://doi.org/10.1007/978-3-319-72965-7_27

events (IPE). IPE are observed at 1% of non-epileptic subjects and around 60 to 90% of
epileptic subjects [3]. They are a complementary source of information in the diagnosis
and localization of epilepsy. They are characterized by a brief initial phase, sharp and
strong amplitude. The interictal spikes occurrence is higher than the seizures frequency.
The clinical characterization of these events rests on their density, their topography or
their morphology. The morphological deﬁnition of the IPE varies much from one patient
to another. A simple spike (P: Positive or N: Negative) is characterized by a pointed
peak or trough, according to their duration, between 20 and 70 ms.
For analyzing well, the spikes morphology, IPE must be detected. In literature, spikes
detections methods were grouped according to their criterion of spike detection in nine
(9) categories [4]: Methods based on traditional recognition techniques, known as
Mimetic Techniques, Methods based on morphological analysis, Methods based on
parametric approaches, Methods based on independent component analysis, Methods
based on artiﬁcial neural networks, Methods based on clustering techniques, Methods
employed data Mining and other classiﬁcation techniques and Methods utilizing knowl‐
edge-based rules. Paper [5] gives some comparison methods that are more used in this
research area. In this paper, we propose a new method of spike detection based on
Smoothed Nonlinear Energy Operator (SNEO) using adaptive threshold. After the step
of detection, we implement an algorithm of extraction of the all spikes detected by using
a window. Spikes extracted will be assignment and clustering using principal compo‐
nents analysis (PCA) and k-means algorithm for unsupervised classiﬁcation. This treat‐
ment would characterize the space-time distribution of the IPE in EEG signal. Brief
replies on this distribution would enable us to characterize the bond still little known
between the space-time distribution of IPE and the arrival of the crises, thus bringing a
signiﬁcant complement in the diagnosis of the epilepsies.
After the introduction, the Sect. 2 presents the adaptive spike detection. In the
Sect. 3, we give of principal components analysis for the clustering. In the Sect. 4, we
explain the proposed process of the spikes morphology classiﬁcation. The Sect. 5
analyzes the results obtained starting from the real data recorded among patients
suﬀering from drug-resistant epilepsy of the temporal lobe. An experimental validation
of the results is made. The last section concludes the paper.
2
Adaptive Spike Detection Based on SNEO
In this part, we use the discrete operator of Teager - Kaiser [6] (Eq. 1) or Nonlinear
Energy Operator (NEO) to measure the authorities of energy of the signal.
ψℝ[x(n)] = x2(n) −x(n −1)x(n + 1)
(1)
Thus the operator can detect changes for instantaneous amplitudes (A) or instanta‐
neous frequencies (Ω) of a signal (Eq. 2).
En = x2
n −xn+1xn−1 ≃A2Ω2
(2)
Classiﬁcation Model of Spikes Morphology
293

It is an analyzer time-frequency which can simultaneously consider the amplitude
and the frequency of instantaneous information of the entry signal. By consequence
NEO is used to amplify the activities in a signal. However, NEO is sensitive to the noises
and has a problem of the growth of the terms of the operator [7]. To reduce this problem,
Mukhopadhyay and Ray [8] suggested SNEO to detect the events of points in signals
EEG by convoluting ψℝ[x(n)] with a smoothed window of time domain is expressed by:
ψS[x(n)] = w(n) ∗ψℝ[x(n)]
(3)
Where operator * represents the product of convolution and w(n) the smoothed
window. The choice of the type of window and its length is very important to carry out
the suﬃcient reduction of interferences without losing much its temporal resolution
which is very useful for the spike detection. In the order to achieve this goal, the window
of Bartlett with the implementation of a numerical ﬁlter was selected to overcome the
complexity of the algorithm as far as possible [8].
With a window of length L, Eq. 3 becomes then:
ψS[x(n)] =
∑L−1
k=0 w(k).ψℝ[x(n −k)]
(4)
The following ﬁgure gives the stages of the spikes detection with SNEO and an
adaptive block for the threshold of detection.
Fig. 1. Adaptive spike detection using SNEO
The main aim of the detector is to decide if there is presence of a spike in the noise
of the bottom or if there is noise only. Values of ψS[x(n)] are continuously compared
with a threshold T. Compared to the stage of the decision, a spike is present if
ψS[x(n)] > T, if not the signal is considered to contain only noise. The threshold is the
focal point with adaptive detection. Only the density of probability under the null
assumption and the probability of false alarm (Pfa
) are necessary. As the Teager-Kaiser
operator is nonlinear, the density of probability cannot be evaluated in a ﬁrm way. In
statistics, several approaches were proposed to circumvent this limitation. However, in
accordance with the need for simplicity, a parametric approach is adopted. Thus, as the
294
O. Khouma et al.

noise is a random process, the exit of the diagram (Fig. 1) is also a random process
having like median value μ and standard deviation σ. The threshold can thus be estimated
by the following equation:
T = μ + pσ
(5)
Where T is the adaptive threshold of detection to determine the presence of a spike
and the multiplier p depends Pfa [8].
3
Principal Components Analysis
Principal components analysis (PCA) is a fundamental method in multidimensional
descriptive statistics [9]. It is used for visualization of complex data. This technique is
based on the reduction of the attributes. It calculates initially the matrix of correlation
R of Na attributes (number of elements in complex data). In the second time, it seeks the
eigenvalues 𝜆i and the eigenvectors vi of R. To ﬁnish, it selects among the N attributes,
NPCA which have the greatest eigenvalues [10]. It is thus a question of obtaining the most
relevant summary of the initial data.
2D representation is made through the ﬁrst two eigenvalues. These determine the
factors (principal components) that return alone almost all of the dispersion of the point
cloud. In addition, the axes passing through the origin 0 are generated by the eigenvectors
associated with the selected values. These axes must pass the best in the middle clouds.
And they are not correlated. Each component is a linear combination of the two eigen‐
vectors. Therefore, we use PCA for mixtures data before applying the clustering.
4
Classiﬁcation of Spikes Morphology
In this paper, cityblock distance [11] is used because it is very simple for their imple‐
mentation. Cityblock distance or Manhattan is determinate by the following expression.
dCB
(x1(k), x2(k)
) =
∑N
k=1 ||x1(k) −x2(k)||
(6)
4.1
Extraction of Detected Spikes
We use the wavelets [12] transform after the phase of detection to be able well to extract
IPE. So a window of extraction is open around the moment of occurrence of each IPE:
by using information a priori over the intermediate duration of IPE. We chose a window
of length L = 128 samples, that is to say 0.5 s (256 is the sample frequency for our data).
A matrix E
pre−ext(t) then is obtained. The matrix contains events pre-extracts deﬁned by:
e
pre−ext i(t) with i ∈
{
0, 1, … , NIPE −1
} (t ∈{0, 1, … , L} and NIPE or Nspike is the number
of detected spikes). The event is to represent by:
Classiﬁcation Model of Spikes Morphology
295

e
pre−ext i(t) = v(t) x
∏
[̂𝜏i−a,̂𝜏i+b]
(t)
(7)
Where v(t) is an EEG signal and 
∏
[̂𝜏i−a,̂𝜏i+b]
(t) is rectangular function (̂𝜏i is the moment
of occurrence or the instant detection of the IPE extracted, a = 32 and b = 95). We will
apply the wavelet Daubechies 6 to the matrix obtained after extraction.
4.2
Application of K-Means Algorithm
There are several approaches of clustering, the unsupervised is selected. The algorithm
must be executed several times Nexecution before choosing the best execution. The oper‐
ation of K-means could be described step by step:
• Stage 1: choice NC randomness elements xi(k) among Nspike. They represent the cent‐
roids at the ﬁrst execution.
• Stage 2: calculation distance elements-centroids by using cityblock.
• Stage 3: attribution each element xi(k) nearest cluster Cj of the centroid among
Nspike.
• Stage 4: calculation the centroid of each cluster Centj(k): mean of the elements xi(k)
belonging to a cluster Cj. And we replace the old centroids by these new centroids.
If there is no movement compared to the preceding iteration, the execution is stopped.
• Stage 5: return at stage 2.
The following ﬁgure gives us the various phases and loops of K-means (Fig. 2).
Fig. 2. Functioning of our k-means algorithm
296
O. Khouma et al.

4.3
Criterion of Stop an Execution of the K-means
The Eq. (8) calculate the mean error; after each iteration of index q of an execution p of
K-means.
Ep
q =
∑Nc−1
j=0
∑
x(n)∈Ci d
(
x(n), Centp
j (n)
)
, q ∈
{
0, … , Nexecution −1
}
(8)
The criterion of stop an execution of the K-means is given by the following expres‐
sion:
|||||
Ep
q −Ep
q−1
Ep
q−1
|||||
< 𝜀
(9)
This criterion is attained with the iteration q = Np
iterations, p ∈
{
0, … , Nexecution −1
}.
We choose 𝜀= 10−5.
4.4
Quality Standard of an Execution of K-means
The results of clustering vary according to the bad or good initialization of the centroids.
After Nexecution executions, the best in the NC clouds can be chosen. Then, the execution
which minimizes the following equation must be selected:
pchosen = arg
min
p∈{0,…,Nexecution−1}
{
Ep
Np
iteration
}
(10)
5
Analysis and Validation Results
In this part, real neuronal data are used for diﬀerent patients. These data were extracted
in the database of the Signal and Image Processing Laboratory of Renne1 University,
France [13]. We will indicate the two databases by “Base BIG” and “Base GONT”. They
contain hundreds of IPE of the type P, PP, N, NN, NP, PN, PW, PPW, NW, NNW, NPW
and PNW. In more these data were labeled by an expert of the laboratory.
The simulations were made using a DELL-PC INSPIRON 1440 Pentium (R) dual-
core CPU T4400 @ 2.20 GHz and 3 GB RAM. And all the solutions proposed were
implemented under the environment of Matlab.
5.1
Evaluation of Adaptive Spike Detection Using SNEO
In this section, we present the results of proposed spike detection. To evaluate this
method, we use several indices. By deﬁnition, false alarms (FA) give us the detected
parts while they are not spikes. As for true positive (TP), they inform us that the spikes
were well detected. Finally, non-detections (false negative: FN) represent spikes not
found by the detector.
Classiﬁcation Model of Spikes Morphology
297

• The rate of false alarms is calculated as follow: RFA =
FA
TP + FA
• The sensitivity (rate of true positive) is a measure of the detector’s ability to detect
the spikes: sensitivity = RTP =
TP
TP + TN
• The selectivity is a measure of the ability of the detector to reject false alarms
selectivity =
TP
TP + FA = 1 −RFA
The detection methods are often based on the principle of thresholding deciding
event detection if any measurement exceeds a predetermined threshold. The higher this
threshold is, the lower the algorithm is permissive and if false alarms decrease of sensi‐
tivity for its fall.
Similarly, a detection algorithm requires a third measure, it is the detection time
(DT), that is to say, the time diﬀerence between the actual beginning of the spike to the
electrical point of view (with the EEG, not so clinically) and the time when it is detected.
DT = |||tspike −talgo|||
(11)
Where tspike is the actual start time of the spike and talgo given by the detection algo‐
rithm. This performance indicator is calculated for each spike and evaluates the ability
of the algorithm to detect an IPE more or less early.
Finally, taking into account these performance indicators, we trace the ROC curves
for the detector.
Unlike many detection methods, with SNEO we don’t use a lot of parameters to
entry. Just give the length of the smoothing window and the estimation parameter of the
adaptive threshold.
In the Fig. 3, we have the EEG signal (blue) with the moments of detection of spikes
(green). In this ﬁgure, all spikes were detected.
Fig. 3. IPE detected using adaptive SNEO (Color ﬁgure online)
298
O. Khouma et al.

We apply this method on all signals of the two databases. In more the execution time
of the signals of one hour duration is approximately equal to 4.6 s. It is a very good time
because we can detect hundreds of spike in a signal of duration one hour.
In addition, we represent in the ROC space [14] (Fig. 4) the performance indicators
of SNEO for diﬀerent morphologies. In Fig. 4, we have two curves of the two databases
(databases BIG and GONT). It is noted that the two curves have points which are near
to the ideal functioning point for a detector (point of coordinates (0%, 100%)). With this
ideal point, the sensitivity and the selectivity are all equal to 100%. The curve of database
GONT is above that database BIG. Consequently, it has the best point with sensitivity
and selectivity equal respectively 100% and 98%.
Fig. 4. Performances of spikes detection with diﬀerent databases
Moreover, GONT presents for certain morphologies more false alarms than BIG. In
these cases, BIG is more selective than GONT.
Consequently, the results of detection depend on the nature and the morphology of
the spikes. This last depend on the state of vigilance of the patient: concentrate, light
sleep, deep sleep etc.
5.2
Results Spikes Classiﬁcation
Before classifying the spikes, we extract them by using a window from length 256
samples i.e. 1 s. The extracted spikes are recorded in a textual ﬁle to facilitate their
loading under Matlab. Then, we apply wavelet ﬁltering to the obtained matrix.
In addition, according to the experts, we meet in practice seldom more than 3
morphologies of IPE (spikes) on very diﬀerent over one hour from recording. We will
evaluate our classiﬁer on mixtures of 3 types of IPE to the maximum.
Classiﬁcation Model of Spikes Morphology
299

Fig. 5. Clustering and identiﬁcation for mixture of two diﬀerent spikes (P and NPW)
Fig. 6. Clustering and identiﬁcation for mixture of three diﬀerent spikes (N, PN and PP)
As explains it very well [15], “the evaluation of the pertinent of the groups formed
in unsupervised classiﬁcation remains an open problem. The diﬃculty comes mainly
owing to the fact that the evaluation of the results of algorithms of clustering is subjective
by nature because there are often various possible relevant regroupings for the same data
ﬁle”. Therefore, we use the indices of the confusion matrix [14] to evaluate the perform‐
ances of our classiﬁcation.
300
O. Khouma et al.

Figures 5 and 6 show the clustering of diﬀerent spikes in a given mixture. In each
ﬁgure, we can clearly identify clusters corresponding to each spike. These clusters give
us principal components (PC) representation of various spikes. And we note that the
position of the clusters is exactly the morphology of classiﬁed spikes.
The good distribution of clusters depends on the performance of the k-means algo‐
rithm. Consequently, these ﬁgures (Figs. 5 and 6) show that the mixed spikes were well
classiﬁed with a small false alarms rate. These results are conﬁrmed on Fig. 7.
Fig. 7. Performances of unsupervised classiﬁcation using k-means
In Fig. 7, the curve of the mixtures with 2 spikes is above that of the mixtures with
3 spikes. Moreover, the points of the curve with 2 spikes are more in the North-West
(tending towards the ideal point of a classiﬁer) in ROC that those of the 3 spikes. Conse‐
quently, the results in the case of the mixtures with 2 spikes are better than those of 3
spikes. That is due to confusion between spikes i.e. those which have identical morpho‐
logical parts. Thus it is easier to make the separation of the spikes in a mixture of 2 IPE.
In addition, the results of the mixtures with 3 spikes are also very satisfactory.
Indeed, our proposals applied to signals of duration lower or equal to one hour. We
ﬁnd hundreds spikes in these signals which could be classiﬁed correctly according to
their morphology. Moreover, the average time of execution of the classiﬁcation of the
various mixtures is estimated at 30 s.
6
Conclusion
The biomedical signals are not easy to treat considering their complexity by nature. The
tools for signal treatment developed can take part in the assistance with the diagnosis of
the epilepsy. In this paper, we proposed adaptive spike detection based on SNEO. The
results showed that detector SNEO gives us very satisfactory performances in terms of
Classiﬁcation Model of Spikes Morphology
301

sensitivity, selectivity and time execution. Moreover, the median values of sensitivity
and selectivity are respectively equal to 99% and of 92% on the tests carried out. And
our detector appeared robust with morphological variability which can exist between
spikes of the EEG signals for diﬀerent patients. After the detection and the extraction
spikes, the k-means algorithm associated with the principal components analysis gives
us clusters which represent exactly the morphology of the concerned spikes. Moreover,
the representation in ROC space gives us points near to the ideal point for a good
classiﬁcation. For the various mixtures, we obtain good rates of true positive. We could
have respectively 94% and 88% in cases of mixtures with 2 spikes and 3 spikes. So our
algorithm was able to distinguish diﬀerent spikes extracted according to their
morphology. Thus, combinations of detection and classiﬁcation will enable us to
conceive an assistance system with the diagnosis of the drug-resistance epilepsy.
To continue this work, we intend to use others methods of unsupervised and super‐
vised classiﬁcation in order to compare them. We will conceive a new model to detect
the presence of slow wave in the interictal paroxystic event.
References
1. Löscher, W., Schmidt, D.: Modern antiepileptic drug development has failed to deliver: ways
out of the current dilemma. Epilepsia 52(4), 657–678 (2011)
2. Sagher, O.: Editorial: epilepsy surgery. J. Neurosurg. 118, 167–168 (2012)
3. Bourien, J., Bartolomei, F., Bellanger, J.J.: A method to identify reproducible subsets of co-
activated structures during interictal spikes. Application to intra-cerebral EEG in temporal
lobe epilepsy. Clin. Neurophysiol. 116, 443–455 (2005)
4. Tzallas, A.T., Tsipouras, M.G., Tsalikakis, D.G., Karvounis, E.C., Astrakas, L., Konitsiotis,
S., Tzaphlidou, M.: Automated epileptic seizure detection methods: a review study.
Department of Medical Physics, Medical School, University of Ioannina, Ioannina, Greece
(2012)
5. Khouma, O., Ndiaye, M.L., Farsi, S.M., Montois, J.-J., Diop, I., Diouf, B.: Comparative
methods of spike detection in epilepsy. In: Science and Information Conference (SAI), pp.
749–745. IEEE, London (2015)
6. Kaiser, J.F.: Some useful properties of teager’s energy operators. In: Proceedings of IEEE
ICASSP 1993, Minneapolis, NN, April 1993, vol. 3, pp. 149–152 (1993)
7. Hassanpour, H., Boashash, B.: A time-frequency approach for EEG spike detection. Iran J.
Energy Environ. 2(4), 390–395 (2011)
8. Mukhopadhyay, S., Ray, G.C.: A new interpretation of nonlinear energy operator and its
eﬃciency in spike detection. IEEE Trans. Biomed. Eng. 49(12), 1526–1533 (2002)
9. Pages, J., Escoﬁer, B.: Introduction à l’analyse en composantes principales à partir de l’étude
d’un tableau de notes. Méthode d’analyses statistiques multidimensionnelles en didactiques
des mathématiques, IRMAR et IRESTE NANTES, pp. 27–29 (1995)
10. Voisine, N.: Approche adaptative de coopération hiérarchique de méthodes de segmentation,
application aux images multi composantes. Ph.D. thesis, Université de Rennes 1, France
(2002)
11. McCune, B., Grace, J.B.: Analysis of Ecological Communities. MjM Software Design,
Gleneden Beach (2002)
12. Mallat, S.: A Wavelet Tour of Signal Processing. Academic Press, Cambridge (1998)
13. http://www.ltsi.univ-rennes1.fr/
302
O. Khouma et al.

14. Fawcett, T.: An introduction to ROC analysis. Pattern Recogn. Lett. 27, 861–874 (2006).
Science Direct
15. Candillier, L., Tellier, I., Torre, F., Bousquet, O.: Évaluation en cascade d’algorithmes de
clustering. CAP Lille (2006)
Classiﬁcation Model of Spikes Morphology
303

Free Space Passive Optical Network
Ahmed D. Kora1(✉), Cheikh B. Rabany2, Zacharia Damoue1, and Ibra Dioum2
1 Ecole Superieure Multinationale de Telecommunications, 10000 Dakar, Senegal
ahmed.kora@esmt.sn
2 Universite Cheick Anta DIOP, 5085 Dakar-Fann, Senegal
Abstract. This paper proposes free space passive optical network (FSPON) as
a potential candidate of high data rate access network based on optical wireless
transmission. FSPON could be considered for indoor as well as outdoor deploy‐
ments. Pending the progress in FSO range, it is appropriate for short range
communication network. The system architecture and network architecture are
depicted. A key component for the deployment success is the alignment of the
remote terminal equipment’s with the free space optical splitter (FSOS). The main
advantages and performances of a user terminal station are discussed. The
performance comparison has been made with a simple point to point free space
optical link. The FSPON capacity is limited to 6 users at the cost of low modu‐
lation scheme and high signal to noise ratio.
Keywords: Free space optics · FSO · Passive optical network · PON · FSPON
1
Introduction
Developments in optical transmission have revolutionized the lives of users of infor‐
mation and communications technology [1, 2]. The advent of the ﬁber and the latest
innovations enable transmission of ten order to hundreds of terabits data rates. This is
possible through wavelength division multiplexing technology (WDM) [3, 4]. WDM
technique by reducing the gap between two adjacent wavelengths provides the ability
to simultaneously send information in parallel by occupation of the available spectrum.
Thus, each forwarded data stream throughput is maximized in order to optimize the
transmission capacity of the ﬁber. The limitations of the ﬁber from a technical point of
view have been greatly improved from the moment where it is introduced into the access
network. This leads to a successful user experience of on provided services. The ﬁber
comes and replace the copper overwhelmed by the growing demand of bandwidth.
Actually, the desire to satisfy the needs of users has generated increasingly bandwidth-
intensive with network applications or software. Several implementation architecture
solutions of the ﬁber in the access network have been proposed. The ﬁrst simplest solu‐
tions were the Point -to- Point architecture. The high cost of deployment of these solu‐
tions has contributed to its gradual shift except for wholesale customers to loop solution
and more recently in passive optical networks. Passive optical networks are currently
the types of networks whose architecture seems the most economical and best suited for
certain types of customers and localities. The ﬁrst generations of PON solutions oﬀered
© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 304–311, 2018.
https://doi.org/10.1007/978-3-319-72965-7_28

capacity in the order of hundreds of megabits shared among thirty-two or sixty-four
users. The capabilities of these systems have gradually evolved gigabit and oﬀers on
market performance of 40 Gbps. All these solutions have conﬁrmed beyond doubt that
the ﬁber is a necessity to resolve transmission capacity needs. This makes it a comple‐
mentary solution to satellites that are designed to solve real problems of distance in a
network or between continents. Satellites proven with Broadcast solutions like televi‐
sion. What hinder the more the deployment of optical ﬁber-based solutions is the
deployment cost of which at least half goes in the civil works. To this is also added the
long delays in implementation and the multitude of permissions issues related to the
crucible and operation of certain arteries. A possibility to maintain the performance
provided by the ﬁber while minimizing the above mentioned limitations led to the intro‐
duction of optical transmission over space [5–11]. Optical transmission solutions over
space called free space optic (FSO) are now a hope for operators and ISPs. It gives the
advantage of providing long delays and especially the reduction of investment costs
which directly or indirectly augment the beneﬁt to be earned. Diﬀerent types of FSO
equipment with varying data rates could be ﬁnd in the market. These equipment’s used
in the case of terrestrial transmission are limited by distance. Explanations found thanks
to the diﬀerent research includes among others the turbulence phenomenon in the light
propagation path. Turbulence partially amends the path of the luminous ﬂux. It random‐
ness is actually the real problem that still causes additional work in research. The
proposed interim solutions are the use of more eﬃcient coding in combination of
multiple antenna systems known as MIMO [9]. Work was also carried out for the opti‐
mization of the size of the receiving antenna to recover the maximum possible trans‐
mitted photon. Some researchers have addressed the use of repeater systems to overcome
these diﬃculties but this also compromise an additional cost.
Pending progress in the near future to increase the range of FSO systems, we propose
in this paper a study of the optical free space transmission version of passive optical
network (PON) as an equivalent image of the wireless local loop (WLL). One could also
call a type of optical wireless local loop (OWLL). Clear advantages of OWLL are speed
of deployment, low-cost solution very skilled broadband today. Immediate applications
of this approach can be justiﬁed by the multiple interests both indoor in a large room
and external environment on reasonable ranges for outdoor applications. Indoor video
application for example. In premises or campus areas, the free space passive optical
network (FSPON) also provides a faster and cheaper solution for the deployment of a
broadband network.
The rest of this paper is organized as follows. Section 2 presents the FSPON archi‐
tecture. Section 3 is dedicated to the study of the performance of this system for more
appropriate modulation scenarios fashionable compared to an initial system point to
point. The last section is the conclusion.
Free Space Passive Optical Network
305

2
FSPON System Architecture Description
The Fig. 1 below shows three main parts of the proposed network architecture. The part
at the operator’s central oﬃce side depicts an OLT equipped with a free space optical
transceiver called free space optical line terminal (FSOLT).
Fig. 1. Free space passive optical network architecture. This shows a ﬁgure consisting of a free
space optical line terminal (FSOLT), a free space optical splitter (FSOS) and “N” free space optical
network terminals (FSONT).
The FSOLT is linked over free space to the second part denoted as splitter is adapted
to receive and split in downstream the optical signal to the end users terminals known
as optical network terminal (ONT).
The appropriate version of splitter and ONT in our context are called FSOS (free
space optical splitter) and FSONT (free space optical network terminal). The passive
optical splitter is provided with an antenna in charge of collecting the optical downstream
signal from the FSOLT and many other antennas at the customer side. The subscribers
of the same cell are allocated a set of antennas at the FSOS. This optical splitter induced
considering its imperfection local loss and misalignment loss that does not include losses
related to splitting the signal. Customers at their end also have stations in charge of
transmitting and receiving as with ONT in the case of the conventional PON. Coordi‐
nating the transmission between the FSOS and the multiple FSONT requires a delicate
adjustment to minimize interference and pointing oﬀsets.
Figure 2 describes the main components of the FSOLT. The extra part of the FSOLT
in comparison to classic OLT is the FSO transceiver which exempts the use of ﬁber.
FSOLT is equipped with application servers and FSO transceivers interconnected thanks
to a router. Application servers conﬁgured depend on the proﬁle of the equipment sold
by the network operators. One of the router interfaces is connected to the driver. The
driver has to adapt the incoming signal to low and high tensions that could change the
state of the external modulator depending on the nature of the binary data. The modulator
receives a constant light power from a laser. Variation of the light power at the output
of the Antenna is due to the behavior of the modulator inﬂuenced by the transmitted
data.
306
A. D. Kora et al.

Fig. 2. Free space optical line terminal (FSOLT) system architecture. The FSOLT is located at
central oﬃce.
Figure 3 details the components of the FSONT. An antenna in charge of collecting
the optical signal dispersed over the space feeds a photodiode. The photodiode trans‐
forms the optical signal into electrical and its output is connected to a pre-ampliﬁer
which delivers a more powered electrical signal. The resulted signal of the ﬁlter is post
processed in order to detect the user transmitted data to be dispatched by the switch
according to the service port.
Fig. 3. Free space optical network terminal (FSONT). This shows a ﬁgure composed of the classic
ONT at the back side liked to an FSO transceivers.
As discussed in the previous section, the main limitation of the FSPON is the range.
The range limitation is due to turbulence in addition to attenuation phenomena. In the
propagation path, the temperature of the weather may vary. This variation aﬀects the
refraction index of the local environment.
n1sini1 = n2sini2
(1)
Free Space Passive Optical Network
307

Since Snell Descartes discovery, it is established that a diﬀerent refraction index may
change the light path direction (1).
The received optical signal can be expressed as follows:
y = (P)1∕2h.x + n
(2)
P is the average transmitted power, h is the channel gain,
y and x are respectively the received and transmitted data,
n is the noise.
3
Simulation Results
All the FSPON performances have been simulated with MATLAB. Low turbulent
channel, short ranges up to 600 m, wavl = 850 × 10−9 and Cn = 0.75 × 10−16 have been
considered. The modulation were pulse position modulation (PPM) of 8 states and 2
PPM. The number of end users were limited to a maximum of 6 because of the perform‐
ance degradation based on the simulation results.
Fig. 4. FSPON performance with 2 end users, 8 PPM and splitter at half-way
In Fig. 4 a user communication link were simulated for a maximum of 2 end users
capacity of the FSPON. Comparison were done with a simple point to point FSO link.
An idle splitter without any other additional loss in each case is placed at half-way. An
8 PPM modulation were simulated. Even if the performance improves for shorter range,
it can be seen that the variation of distance is not the important limitation factor since
600 m, 300 m and 200 m have approximately the same link performance while the other
308
A. D. Kora et al.

parameters does not change. The simulation results present a gap of 3 dB at a BER of
10−3 obtained with a SNR = 9 dB for FSPON.
Figure 5 shows the simulation results for a double number of user in comparison to
the simulation in Fig. 4.
Fig. 5. FSPON performance with 4 end users, 8 PPM
Fig. 6. FSPON performance with 6 end users with 2 PPM, the splitter is at half-way.
Free Space Passive Optical Network
309

In order to support more user a lower modulation scheme might be set. It is the reason
why the simulations performed in Figs. 6 and 7 focused on 2 PPM.
Fig. 7. FSPON performance with 6 end users, 2 PPM, splitter at 250 m of the end users.
Figures 6 and 7 present simulation results performed for a 6 users FSPON with a
modulation scheme of 2 PPM but the position of the splitter were changed. In Fig. 6,
the splitter were half-way but the splitter were ¼ way from the end users in Fig. 7. The
performances show the low impact of distance for full range and also for the position
of the splitter. There is a gap of about 16 dB in comparison to point to point optical link
in both cases at a BER of 4 × 10−3.
Our simulations clearly show in the case of low turbulence the great impact of the
supported number of user which degrades the FSPON link performance in comparison
to the distance. Six users could be supported in the simulated conditions at the cost of
low modulation scheme (2 PPM).
4
Conclusion
Free space passive optical network (FSPON) has been presented. The increasing needs
of bandwidth constitutes an opportunity for a low cost and fast deployment of FSPON.
The diﬃculty is the channel behavior leading to attenuation and deviation of the optical
signal because of turbulence. Splitting the optical signal cost higher bit error rate
performance and shorter range. The maximum number of user in comparison to a simple
point to point wireless optical is limited to approximately 6 and this depends on the
signal to noise ratio and considered modulation scheme.
310
A. D. Kora et al.

References
1. Bachar, I.S., Kora, A.D., Faye, R.M., Aupetit-Berthelemot, C.: Universal access index
assessment and appropriate optimization strategy. Univ. Access Inf. Soc. (UAIS) 16(4)
(2016). https://doi.org/10.1007/s10209-015-0447-7
2. Soidridine, M.M., Claude, L., Kora, A.D.: Green cloud architecture for African local
collectivities. In: IEEE ICAST (2013). https://doi.org/10.1109/ICASTech.2013.6707513
3. Kora, A.D.: DWDM/OOC and large spectrum sources performance in broadband access
network. Int. J. Distrib. Parallel Syst. (IJDPS) 3(3), 185–195 (2012)
4. Diouf, M.D., Kora, A.D., Ringar, O., Aupetit-Berthelemot, C.: Evolution to 200G passive
optical network. Comput. Technol. Appl. (CTA) 3(11), 723–728 (2012)
5. Leitgeb, E., Plank, T., Loschnigg, M., Mandl, P.: Free space optics in diﬀerent (civil and
military) application scenarios in combination with other wireless technologies. In: 16th
International Telecommunications Network Strategy and Planning Symposium (Networks),
pp. 1–7 (2014). https://doi.org/10.1109/NETWKS.2014.6959207
6. Kaur, P., Jain, V.K., Kar, S.: Performance analysis of free space optical links using multi-
input multi-output and aperture averaging in presence of turbulence and various weather
conditions. IET Commun. 9(8), 1104–1109 (2015). https://doi.org/10.1049/iet-com.
2014.0926
7. Clarke, B., Hamilton, K., Hembree, D., Marsh, T., Young, C.: Low-cost, high-speed FSO
communication link. In: Senior Design Project. Georgia Institute of Technology (2007)
8. Vitasek, J., Latal, J., Hejduk, S., Bocheza, J., Koudelka, P., Skapa, J., Siska, P., Vasinek, V.:
Atmospheric turbulences in free space optics channel. In: International Conference on
Telecommunications and Signal Processing (TSP), pp. 104–107 (2011). https://doi.org/
10.1109/TSP.2011.6043763
9. Dordjevic, I., Denic, S., Anguita, J., Vasic, B., Neifeld, M.A.: LDPC coded MIMO optical
system for communication over the atmospheric turbulence channel. J. Lightwave Technol.
25(5), 478–487 (2008)
10. Zhu, X., Kahnn, J.M.: Performance bounds for coded free space optical communications
through atmospheric turbulence channels. IEEE Trans. Commun. 51(08), 1233–1239 (2003)
11. Trunga, H.D., Tuana, D.T., Phamb, A.T.: Pointing error eﬀects on performance of free-space
optical communication systems using SC-QAM signals over atmospheric turbulence
channels. Int. J. Electron. Commun. 68(9), 869–876 (2014). https://doi.org/10.1016/j.aeue.
2014.04.008
Free Space Passive Optical Network
311

SBSD: Towards a Proactive Sensor-Based
Schistosomiasis Detection
Bassirou Kass´e1, Moussa Diallo1, Bamba Gueye1(B), and Halima Elbiaze2
1 Universit´e Cheikh Anta Diop de Dakar, Dakar, Senegal
{bassirou.kasse,moussa.diallo,bamba.gueye}@ucad.edu.sn
2 Universit´e du Qu´ebec `a Montr´eal, Montr´eal, Canada
halima.elbiaze@uqam.ca
Abstract. After the malaria, Schistosomiasis or Bilharzia is the second
disease that calls for admission to hospital. In fact, the Schistosoma that
transmits the illness lives in water points. The proposed Sensor-Based
Schistosomiasis Detection (SBSD) architecture considers data collected
by several sensors such as water temperature, water point pH, solar irra-
diation that are deployed in a natural environment, in order to develop
more-sensitive disease-prediction and control-model. The main goal is to
stop the transmission cycle of Bilharzia by forbidding the access of water
point, for treatment, when the environmental factors are favourable.
Keywords: Schistosomiasis · Sensors networks · Internet of Things
Schistosomiasis or Bilharzia is a parasitic disease aﬀecting more than 200 mil-
lion people distributed over 76 world countries [1]. In sub-Saharan Africa, 165
million people are aﬀected which represent roughly 82.5% of the people reported
to be infected all over the world. The main reasons are due to the spread of
hydraulic developments and the fact that daily life activities in rural areas are
done around water points. Furthermore, the lack of safe drinking water and san-
itary infrastructure increase human water contact. Consequently, water point
can be contaminated by human faeces and urine.
The urinary and intestinal Schistosomiasis is a signiﬁcant public health prob-
lem in Senegal with a prevalence rate varying between 0.3% and 1%. In Senegal,
the treatment is based on Praziquantel that is not eﬀective and may aggravate
symptoms. The epidemic of intestinal schistosomiasis in Richard Toll region after
the newly built dams of Diama and Manantali on the Senegal River and the
related irrigation projects is now legendary in Africa [2]. Actually, the bilharzia
life cycle is based on physical and chemical factors [3–6]. The physical factors
include temperature, solar irradiance, water movement, water-level ﬂuctuation
and desiccation, depth of water; whereas the chemical factors, such as salinity,
the ion balance, the hydrogen-ion concentration, are measured within the water
point where the intermediate hosts that transmit the bilharzia are living.
Despite the eﬀorts made by the World Health Organization and the Bil-
harzia local control programs, the number of patients infected remains constant
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 312–321, 2018.
https://doi.org/10.1007/978-3-319-72965-7_29

Sensor-Based Schistosomiasis Detection
313
in Africa. The diagnosis of Bilharzia, if the patients go to health centres, is per-
formed by the microscopic examination of urine or faeces in order to demonstrate
the presence of eggs. Quite often, without symptoms and morbidity, rural people
that have Positive serology do not go to hospital for diagnostic, and then con-
tinue to infect water point. In the case of positive diagnostic, the treatment can
be done by chemotherapy or prophylaxis. According to chemotherapy approach,
the Praziquantel treatment is eﬃcient in the situation of low outbreak and rein-
fection. Nevertheless, in the case of high morbidity and reinfection, as observed
in Richard Toll in northern Senegal, the Praziquantel has very low cure rates
[1]. Furthermore, the Praziquantel has no impact on immature schistosome and
eggs [1]. Recent studies [7,8] showed that existing drugs such as Praziquantel
and Arthemeter are not eﬀective against acute Bilharzia.
Therefore, prophylaxis approaches should be more investigated. So far, exist-
ing solutions are based on: i the use of chemicals such as copper sulfate in order
to destroy the snails that act are intermediate hosts. This solution can break
the ecosystem of water points; ii The improvement of sanitation and the impli-
cation of women committee and social economical stakeholder in rural areas.
These solutions are not very eﬃcient due to the lack of information in real time
of infected water points and the identiﬁcation of people that require priority
treatment. Therefore, we promote the use of Wireless Sensors Networks (WSN)
that enable continuous and remote monitoring of water points [9]. During the
last decade, WSN are widely used for surveillance and monitoring [10,11].
Our Sensor-Based Schistosomiasis Detection (SBSD) architecture aims to
remotely monitor physical and chemical environment factors of water points in
order to determine environmental changes. To the best of our knowledge, SBSD
is the ﬁrst tool that is deployed, tested in a natural environment and is able to
predict in real time the potential risk of bilharzia transmission.
The remainder of the paper is structured as follows. Section 1 discuss the bil-
harzia life cycle transmission and the background on design metrics. In Sect. 2,
we introduce the SBSD architecture and its methodology to use sensors net-
works with Internet or 3G network to monitor water point. Following that, we
present in Sect. 3 our experimental results. Finally, Sect. 4 concludes this work.
1
Background on Design Metrics
The bilharzia life cycle is based on physical and chemical factors. Climate factors,
including water temperature, water pH, solar irradiance, have a signiﬁcant eﬀect
on freshwater snails population dynamics [4–6].
According to the bilharziasis life cycle transmission, the solar irradiance plays
an important role when the eggs are released, by an infected person, on contact
with water points. Indeed, the diurnal light intensity has a great impact on
the maturation of eggs [3]. Furthermore, a more recent work [5] shows the cor-
relation between the light intensity and the breeding of snails. Moreover, the
breeding includes the following milestones: oviposition, larvae, and youth. The
light intensity has a stimulating action on the reproduction between adult snails

314
B. Kass´e et al.
and the growth of larvae and young forms. The mean solar irradiation in Sene-
gal is roughly equals to 242 µmol/m2/s1 which represents a solar irradiance of
5, 8 kWh/m2/day. Therefore, this value illustrates a high degree of light intensity.
On the other hand, chemical factors including salinity, ion balance, hydrogen-
ion concentration have a deep eﬀect upon breeding conditions of snails. A water
pH range from 6.5 to 8.5 is mandatory for an optimal conditions of development
of most aquatic organisms [6]. For instance, the optimum water pH, according
to favourable breeding conditions, for Bulinus snails (respectively Biomphalaria)
lies between 6.0 to 6.5 (respectively 7.0 to 8.2) [3].
Finally, the water temperature has an important role in solubility of gases
such as oxygen necessary for the balance of aquatic life. The metabolic activity
of aquatic organisms is also accelerated according to the ﬂuctuations of temper-
ature. If the temperature of the water is warm enough during a long period, the
freshwater snails that transmit schistosomiasis are able to growth up in suitable
condition. The optimal temperature that enables favourable breedings is mea-
sured between 25 ◦C to 28 ◦C [3,4]. Afterwards, when the temperature ranges
from 28 ◦C to 30 ◦C, the conditions are unfavourable. In such away, the breed-
ing ceases [3]. Nevertheless, a temperature upper than 42 ◦C, during two hours,
causes snail’s death [4].
As summary, a correlation is noted between schistosome vectors, water pH,
water temperature and solar irradiance. Thereupon, by measuring these param-
eters with WSN, we can whenever determine: (i) if the condition are favourable
for the maturation of eggs; (ii) whether the intermediate hosts freshwater snails
can still alive, growth up optimally; (iii) even if a successful breeding of snails is
possible. We aim to predict whether physical and chemical factors are favourable
to the bilharzia transmission life cycle by deploying an eﬃcient wireless sen-
sors network.
2
Sensor-Based Schistosomiasis Detection (SBSD)
Description
Figure 1 depicts our general architecture which enables to measure diﬀerent phys-
ical and chemical environment factors. This architecture is formed by three main
layers called application layer, network layer, and perception layer.
The application layer contains two sub layers. The ﬁrst one is dedicated to
users that request diﬀerent kind of information and the second one represents
the service management layer which contains several servers that manage the
whole architecture. Afterwards, the goal of the network layer is to route the
collected information from the data acquisition modules which are located at the
perception layer towards the service management sub layer where information
are gathered.
It should be noted that in this work only the sensors module located in the
collecting layer is considered (Fig. 1). Therefore, water temperature, water pH,
1 Atlas IRENA, http://irena.masdar.ac.ae/.

Sensor-Based Schistosomiasis Detection
315
Fig. 1. SBSD architecture.
and solar irradiation sensors are used during the ﬁrst evaluation phase of the
SBSD. The data acquisition (“DAQ”) architecture is formed by a DAQ module
that can embed diﬀerent type of sensors. Wireless sensors networks are usually
energy-constrained networks. Therefore, we consider DAQ modules which use
solar panel as power supply.
In our context, the Waspmote [12] is chosen as DAQ module. A Waspmote is
just an enhanced Libelium [12] mote which is used for wireless sensors networks.
Thence, the deployed sensors are Waspmote type and connected directly to the
DAQ module (Fig. 2(a)).
Two Waspmote devices have been used during the test. The ﬁrst one embeds
the pH and temperature sensors, whereas the second one hosts the solar irradi-
ance sensor. Figure 2(a) shows the water pH and water temperature which are
connected to the Waspmote. The embedded battery in the Waspmote is charged
by an external solar panel. This kit measures the water pH and the water tem-
perature, and then sends the collected data to a gateway. Figure 2(b) illustrates
the solar irradiance sensors with its external solar panel.

316
B. Kass´e et al.
Fig. 2. Data acquisition in SBSD architecture.
Afterwards, the data collected by our two Waspmotes are sent to the Mesh-
lium device which is ﬁxed on the top of the pylon (Fig. 2(b)). It represents
a gateway router for Waspmotes sensor networks. It is worth noticing that the
Meshlium is a Power over Ethernet network device. It receives in its local MySQL
database the data sent by Waspmotes’s via the ZigBee-Pro protocol. Addition-
ally, the Meshlium re-transmits, in a ﬁxed interval time, these data towards an
external database or a Cloud platforms by using either its Ethernet or 3G inter-
face. Indeed, according to our SB2D architecture, we used the Internet network
and an external MySQL database.
3
SBSD Evaluation
3.1
Experimental Settings
The botanical garden of University Cheikh Anta Diop is used as real environ-
ment experimental test bed before the deployment in the Richard Toll area. The
water point (Fig. 2(a)), where the Waspmote 1 is deployed, is located within the
botanical garden. The Waspmote 2 as well the Meshlium gateway are placed
on top of the roof of the Computer Science Building. The Waspmote 1 (respec-
tively Waspmote 2) sends the pH and water temperature (respectively the solar
irradiance). The distance as the crow ﬂies between the targeted water point and
the Computer Science building is approximately 200 m.

Sensor-Based Schistosomiasis Detection
317
The Waspmote device embeds several hardware components such as an
ATmega1281 microcrontroller running at 14 MHz, a Xbee-ZB-Pro transmitter
using the ZigBee-Pro protocol [13] within the 2.4 GHz frequency. The ZigBee-
Pro is an enhanced version of ZigBee technology which is based on the IEEE
802.15.4 standards [10]. ZigBee-Pro has the ability to provide low-power wireless
connectivity as well to manage large networks having up to thousands nodes.
The dataset we consider is obtained during a period of 2 weeks in early
July 2016. The frame sent by Waspmote has a length of 128 octets and contains
several information including “ID”’s Waspmote, frame type, frame number, type
of sensor, measured value, battery voltage, timestamp, etc. The sampling interval
is ﬁxed to 1 min (respectively 2 min) for Waspmote 1 (respectively Waspmote 2).
Moreover, the sampling rate is a tuning parameter. The frames sent by Waspmote
are received by the Meshlium by the intermediate of its XBee ZigBee radio
interface.
Thereafter, a Sensor Parser software parses the frames and store the data
in the Meshlium’s database. Finally, in each interval of 1 min the received data
are synchronized with an external MySQL database by using a ﬁxed couple of
internet IP address and port number. The stored information can be accessed
from a Web user interface. In order to reduce experimental cost, Meshlium’s
Ethernet interface is used during the data transfer. Notwithstanding, its 3G
radio interface has been successfully tested.
 0
 500
 1000
 1500
 2000
 0
 2000
 4000
 6000
 8000  10000  12000
Solar irradiation ( µmol/m2/s)
Number of Samples
Fig. 3. Solar irradiance.
3.2
Results
According to Figs. 3 and 4 the x-axis depict the obtained number of samples as
function of timestamp during the measurements campaign; whereas the y−axis
represent the measured values either for the solar irradiance (Fig. 3), or the

318
B. Kass´e et al.
 5
 6
 7
 8
 9
 10
 11
 12
 13
 0
 5000
 10000
 15000
 20000
 10
 15
 20
 25
 30
Temperature
pH
Water pH
Water temperature ( °C)
Number of Samples
Fig. 4. Water pH and water point temperature.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0
 500
 1000
 1500
 2000
Cumulative Probability
Solar irradiance ( µmol/m2/s)
Fig. 5. CDF of solar irradiance.
water pH or water temperature (Fig. 4). Figure 3 illustrates the solar irradiance
as function of number of samples. For instance, a value of 0 means that there is
no measured light intensity. Figure 4 illustrates the water temperature and the
water pH as function of number of samples.
Figure 5 provides the CDF (Cumulative Distribution Function) of the solar
irradiance. According to More than 40% of measurements have a irradiation
upper than 500 µmol/m2/s which means very high light intensity. It should be
noted that July month belongs the dry season in Senegal where the temperature
is very high.

Sensor-Based Schistosomiasis Detection
319
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9  10  11  12
Cumulative Probability
Water pH
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 5
 10
 15
 20
 25
 30
 35
Cumulative Probability
Water temperature  (°C)
(a) Water ph
(b)
Fig. 6. Cumulative Distribution Function.
Figure 6(a) provides the CDF of the water pH. More than 60% of samples
are a pH roughly equal to 7 that means a neutral water during this period.
About 20% of samples have a pH value upper than 8. It should be noted that
a favourable breeding conditions of intermediate hosts snails lies between 6.5 to
8.5 (Sect. 1).
Figure 6(b) provides the CDF of the water point temperature. 60% of
obtained samples have a temperature less than 28 ◦C. In contrast, 40% of sam-
ples are upper than 28 ◦C which means an unfavourable breeding conditions.
Indeed, the temperature should be warm enough (i.e., between 25 ◦C to 28 ◦C)
during a long period in order to enable the snail’s maturation from oviposition
to youth.

320
B. Kass´e et al.
According to obtained results during the two early weeks of July, we can
stipulate that the combination of physical and chemical factors are favourable
to the breedings conditions of intermediate hosts snails with respect to a couple
of days.
4
Conclusion
SBSD tool provides a potentially vital capability for use by disease control
program managers, particularly in less-developed countries. This tool detects
contaminated water source, and thus enables to take proactive decisions such as
prohibit the infected area, or prevent the miracidium to infect the mollusc or
the parasitic larvae to enter through the skin of humans. Therefore, we are able
to stop the transmission cycle of schistosomes. We considered a high sampling
rate.
For instance, we plan to analyze the measurement of one year, in order to ﬁnd
what is the appropriate sensing rate in order to reduce the amount of gathered
data. In parallel, according to the Sensor-Based Bilharzia Detection project we
aim to deploy the SBSD architecture in diﬀerent water points located in the
Richard Tool area. This deployment can help to reduce the infection rate and
the morbidity with respect to the Schistosomiasis. An alert system should be
designed in order to inform rural population. In such situation, we expect to use
rural radio or and SMS as communication media.
Acknowledgement. This work has been partially supported by the AUCC, now
called Universities Canada, in the Sensor-Based Bilharzia Detection project. Also, the
authors would like to thank other project partners.
References
1. Gryseels, B., Polman, K., Clerinx, J., Kestens, L.: Human schistosomiasis. Lancet
368(9541), 1106–1118 (2006)
2. Stelma, F., Talla, I., Polman, K., Niang, M., Sturrock, R.F., Deelder, A.M.,
Gryseels, B.: Epidemiology of Schistosoma mansoni infection in a recently exposed
community in northern Senegal. Am. J. Trop. Med. Hyg. 49(6), 701–706 (1993)
3. Study Group on the Ecology of Intermediate Snail Hosts of Bilharziasis World
Health Organization, World Health Organization technical report series; no. 120.
http://apps.who.int/iris/bitstream/10665/40374/1/WHO TRS 120.pdf
4. Barbosa, F., Barbosa, C.: The bioecology of snail vectors for schitosomiasis in
Brazil. Cad. Saude Publ. 10(2), 200–209 (1994)
5. McCreesh, N., et al.: Eﬀect of water temperature and population density on the
population dynamics of Schistosoma mansoni intermediate host snails. Parasites
Vectors 7, 1–9 (2014)
6. Safaa, A.-A.: A study of pH values in the Shatt Al-Arab river. Int. J. Mar. Sci.
6(29), 1–8 (2016)
7. Pratico, L., Mariani, B., Brunetti, E., Maserati, R., Antonella, B., Novati, S.,
Chichino, G.: Failure of repeated treatment with praziquantel and arthemeter in
four patients with acute schistosomiasis. J. Travel Med. 21(2), 133–136 (2014)

Sensor-Based Schistosomiasis Detection
321
8. Logan, S., Armstrong, M., Moore, E., Nebbia, G., Jarvis, J., Suvari, M., Bligh,
J., Chiodini, P.L., Brown, M., Doherty, T.: Acute schistosomiasis in travelers: 14
years’ experience at the hospital for tropical diseases, London. Am. J. Trop. Med.
Hyg. 88(6), 1032–1034 (2013)
9. Lynch, J.P., Loh, K.J.: A summary review of wireless sensors and sensor networks
for structural health monitoring. Shock Vibration Digest 38(2), 91–130 (2006)
10. Yick, J., Mukherjee, B., Ghosal, D.: Wireless sensor network survey. Comput. Netw.
52(12), 2292–2330 (2008)
11. Othman, M.F., Shazali, K.: Wireless sensor network applications: a study in envi-
ronment monitoring system. Procedia Eng. 41, 1204–1210 (2012)
12. Libelium, Internet of Things Platform Provider, Libelium Comunicaciones Dis-
tribuidas S.L. http://www.libelium.com
13. ZigBee Alliance: New zigbee pro feature: green power connecting battery-
free devices. White Paper (2012). http://www.zigbee.org/zigbee-for-developers/
network-speciﬁcations/zigbeepro/

Towards Meningitis Ontology for the Annotation
of Text Corpora
W. R. C´edric B´er´e1,4(B), Gaoussou Camara2, Sadouanouan Malo3,
Moussa Lo4, and Stanislas Ouaro1
1 LAMI, Universit´e Ouaga 1 Pr Joseph Ki-Zerbo, 03 BP 7021, Ouaga, Burkina Faso
cedric.bere@univ-ouaga.bf, ouaro@yahoo.fr
2 EIR-IMTICE, Universit´e Alioune Diop de Bambey, BP 30, Bambey, Senegal
gaoussou.camara@uadb.edu.sn
3 Universit´e Polytechnique de Bobo-Dioulasso, 01 BP 1091, Bobo, Burkina Faso
sadouanouan@yahoo.fr
4 LANI, Universit´e Gaston Berger, BP 234, Saint-Louis, Senegal
cedric.bere@gmail.com, moussa.lo@ugb.edu.sn
Abstract. Ontologies are formal models that describe the semantics of
domain knowledge in order to facilitate automatic processing and provide
reasoning possibilities. In the context of this work, we aim at designing
an ontology for assisting data and event extraction from text corpora in
social medias. These items extracted will be used as input for meningi-
tis spreading simulation within a surveillance system. In this paper, we
describe the methodology we propose for building such an ontology. Our
approach is based on the reuse of some NeOn scenarios and the use of
an annotation tool for text corpus of the biomedical domain. This tool
takes as input a text corpus on meningitis and allows us to identify rel-
evant ontological resources. The paper is ending by presenting examples
of conceptual models resulting from the application of the ﬁrst step of
our methodology.
Keywords: Ontology · Meningitis · Annotation · Text corpus · Neon
1
Introduction
Meningitis is an infectious disease with disastrous consequences in many coun-
tries, particularly in African countries. It has viral or bacterial origin [14]. Detec-
tion of the ﬁrst cases is crucial for taking appropriate measures to prevent a
possible epidemic, hence the need of its surveillance [8]. This requires rapid col-
lection and integration of data and events related to the risk factors of its spread.
However, developing countries, particularly those in West Africa, are faced with
a lack of a real-time data collection system. This situation delays considerably
risk analysis and decision-making. It is in this context that we have initiated
a project to set up a simulation platform dedicated to the control and preven-
tion of the spread of meningitis epidemics in Burkina Faso. This platform will
c
⃝ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018
C. M. F. Kebe et al. (Eds.): InterSol 2017/CNRIA 2017, LNICST 204, pp. 322–331, 2018.
https://doi.org/10.1007/978-3-319-72965-7_30

Towards Meningitis Ontology
323
be part of the surveillance and awareness system on meningitis of the Nouna1
Health Research Center in Burkina Faso. In this platform, the early detection of
an epidemic risk is made by a qualitative simulation [12]. The models to imple-
ment take as input, qualitative data and events contained in text corpora from
social medias. The use of an ontology makes possible to identify the relevant
corpora and to extract the data and events related to the domain [2]. In their
work, Camara et al. [4] have shown that the use of ontologies are suitable for
improving epidemiological surveillance capabilities. For instance, domain ontolo-
gies provide a common vocabulary for the semantic annotation of resources in
the surveillance domain.
In this paper, our purpose is to deﬁne a methodology for building an ontol-
ogy of meningitis domain that will used in extracting data and events from
text messages published in social medias. In addition to this, the ontology will
allow data integration, domain knowledge sharing and eﬃcient communication
between actors within the epidemiological surveillance system.
In the remainder of this paper, we ﬁrst present the related works in Sect. 2.
Then, in Sect. 3, we present the Neon methodology which is the basis of the
methodology we have proposed. In Sect. 4 we describe the methodology we have
designed for the building a meningitis ontology. Our Sect. 5 presents examples of
conceptual models of meningitis knowledge resulting from the application of the
ﬁrst step of our methodology. Finally, we conclude and draw the perspectives of
our work.
2
What Are Ontologies and Ontology Engineering?
On one hand, the notion of ontology meets many deﬁnitions that evolve over time
[3,9,20]. According to the purpose of our work, the deﬁnition from Studer seems
more complete and more suitable: An ontology is a formal, explicit speciﬁcation
of a shared conceptualization.
On the other hand, ontology engineering is the discipline that studies prin-
ciples, methods and tools for the building and management of the evolution of
ontologies during their life cycles [7]. Several methods and good practices for the
development of ontologies have been proposed in the literature [13,16,19]. One
method could be considered as better than the other according to the ontology
purpose, the actors involved in the process, etc. These methodologies could also
be compared according to the type of the engineering process: from scratch, by
integration or fusion with other ontologies, collaborative building, evolution of
existing ontologies, etc.
In the context of our work, the choice of the ontology engineering method
takes into account the fact that the semantics of the vocabulary used is closely
related to a domain (contextualized vocabulary). We take also into account the
fact that we build a modular application ontology [1]. To meet these criteria,
Neon appears as the most exhaustive method which is ﬂexible enough to allow
us mixing its scenarios and enrich the methodology by integrating new steps.
1 http://www.crsn-nouna.bf/.

324
W. R. C. B´er´e et al.
3
The NeOn Methodology Overview
The methodological approach we propose in this paper is inspired from Neon
project2 [21]. This choice is motivated by the fact that we aim at building a
modular ontology that requires knowledge from the domain experts involved in
the project. In addition to this, we intend to reuse existing ontological resources.
Neon is well adapted to our context because it is a ﬂexible methodology compared
to others. The Neon methodology proposes 9 scenarios that one can choose and
combine to develop an ontology. The Neon methodology also provides a glossary
of activities3, two life-cycle models and a set of guidelines or methodological
recommendations. The 9 scenarios are:
• Scenario 1: construction of ontologies from scratch (from speciﬁcation to
implementation);
• Scenario 2: reuse and re-engineering of non-ontological resources;
• Scenario 3: reuse of ontological resources;
• Scenario 4: reuse and re-engineering of ontological resources;
• Scenario 5: reuse and fusion of ontological resources;
• Scenario 6: reuse, fusion and re-engineering of ontological resources;
• Scenario 7: reuse of ontology design patterns and patterns;
• Scenario 8: restructuring of ontological resources;
• Scenario 9: localization of ontological resources.
In the remainder of the paragraph we detail some scenarios of the Neon
methodology that we use in the methodology for building the meningitis
ontology.
Scenario 1 of Neon: The activities in this scenario consist in specifying the
purpose, scope, intent, and applications or uses that will be made of the ontology.
It is also necessary to determine in this scenario the technical constraints in
terms of reusability and modularity. All these speciﬁcations will be recorded in
a speciﬁcation document which will be used for the validation of the ontology by
the domain experts. A ﬁrst conceptual model of knowledge should be provided
at the end of the activities of this scenario.
Scenario 3 of Neon: The activities of this scenario lead to the total or partial
reuse of existing ontological resources. This is due to the abundance of ontolo-
gies and ontology repositories that lead us to explore these existing ontologies
for reuse. There are three main activities: research, selection and evaluation of
candidate ontologies.
Scenario 5 of Neon: The activities of this scenario consist in the selection of
ontological resources of the same domain (if they exist) to be reused in order to
create a new ontological resource. This scenario is therefore conditioned by the
existence of these ontological resources of the actual domain.
2 http://www.neon-project.org/.
3 http://mayor2.dia.ﬁ.upm.es/oeg-upm/ﬁles/pdf/NeOnGlossary.pdf.

Towards Meningitis Ontology
325
Scenario 7 of Neon: The activities of this scenario involve reusing design
patterns and best practices in ontology design. This approach ensures better
ontology design and implementation.
Scenarios can be combined and are ﬂexible. However, Scenario 1 is a manda-
tory scenario regardless of the combination to be made.
4
Meningitis Domain Ontology Building Methodology
4.1
Main Steps of the Methodology
We propose here a methodology to build the meningitis domain ontology. Our
methodology is based on the NeOn approach. The ontology we propose to build
is an extension of the Infectious Disease Ontology (IDO)4 [6].
The choice of the Neon methodology is ﬁrstly motivated by the ﬂexibility
of the use of its scenarios. And secondly, because NeOn is a methodology that
has been adopted successfully in many projects including a previous work of our
research team for the building of the schistosomiasis domain ontology [5].
As mentioned in the previous paragraph, we will not apply all the nine sce-
narios presented in the NeOn methodology. We consider the scenarios 1, 3, 5
and 7 as illustrated in Fig. 2. The approach is thus customized in ﬁve (5) main
steps as follows:
1. Step 1: this step consists of an implementation of Neon Scenario 1. This
Scenario is a mandatory.
2. Step 2: annotation of text corpora. This step allows us identifying rele-
vant ontologies from biomedical repositories. This annotation phase will be
detailed in the next section.
3. Step 3: this step is the implementation of Neon Scenario 3. It consists of the
reuse of the relevant ontologies suggested by the annotator at step 2.
4. Step 4: this step involves the implementation of Neon Scenario 5. At this
stage, we use ontology alignment and/fusion techniques for the enrichment of
the ontology from step 3.
5. Step 5: this step is the implementation of Scenario 7 of Neon. It consists of
the use of ontological design pattern.
The step 2 of our methodology involves two main activities: the constitution
of a text corpus and the semantic annotation. The constitution of the corpus
is essential and will consist in the construction of a relevant corpus. This cor-
pus must cover as large as possible the biological, clinical and epidemiological
perspectives of the meningitis disease.
For the construction of the corpus we used two essentially sources. The ﬁrst
source [10] is realized by a group of multidisciplinary experts under the initiative
of the INSERM. This group of experts analyzed the available global literature
4 http://infectiousdiseaseontology.org/.

326
W. R. C. B´er´e et al.
Fig. 1. Methodology of building of the meningitis ontology
Fig. 2. Modeling the biology of meningococcus.

Towards Meningitis Ontology
327
on meningitis (about 500 articles) and took into account almost all aspects of
the disease. This corpus has several advantage:
• the corpus presents in a single document a review of the scientiﬁc knowledge
on meningitis: the biological aspects of the bacterium, the treatment and
prevention means and the epidemiological strategies of the disease;
• recommendation in terms of research direction on meningitis are recorded in
the same document. Some of which are currently applied.
The second source is a set of documentary resources on meningitis listed in the
INSERM iPubli-Inserm documentary database.
The validation of the deﬁnitive corpus is made by experts in meningitis
domain. The text corpus validated by all the domain experts is ﬁnally used
for the semantic annotation step. Ontology extraction will be performed on the
multitude of biomedical ontologies available in Bioportal repository.
4.2
Using the Bioportal NCBO Annotator
The approach we have adopted for the building of a meningitis ontology includes
the reuse of existing ontological resources. We used the NCBO annotator to
exploit the large number of ontological resources existing in the biomedical
domain. The annotator operates on the deposit of bioportal. This annotator
allows us to retrieve relevant ontologies to a speciﬁc domain from a text corpus
or from a set of key words or terms derived from a text corpus [11].
An ontology is considered as relevant according to 3 criteria:
• coverage: the ontology must cover the dataset as broad as possible;
• connectivity: the ontology must contain suﬃcient terms/concepts linked to
other existing ontologies of the same domain;
• size: the number of concepts must be great enough.
According to the principle of OBO Foundry [18] (Open Biological and
Biomedical Ontology Foundry), the reuse of the ontologies extracted from the
bioportal repository will guarantee the reusability of our ontology in the biomed-
ical domain.
5
Applying Step 1 of Our Methodology
In this section, we will implement the following activities deﬁned in the scenario
to provide the conceptual models of the diﬀerent modules of the meningitis
ontology: speciﬁcation, acquisition and modeling.
• Speciﬁcation: the building of the meningitis ontology is guided by the pur-
pose of using it for identifying relevant messages from social medias and
extract data and events related to risk factors for the spread of the disease.

328
W. R. C. B´er´e et al.
• Acquisition: this task is done by interviews with meningitis domain experts
and by using non ontological resources such as text corpora as said in pre-
vious sections. Text corpora helps for knowledge acquisition and ontological
resources discovery from biomedical ontology repository.
• Modeling: the modeling phase provides as results conceptual models of
knowledge that will be formalized by the ontology. These conceptual models
support validation of knowledge of the ontology by domain experts.
The two conceptual models presented in Subsects. 5.1 and 5.2 represent mod-
ules extracted from the global meningitis ontology model.
5.1
Modeling the Biology of the Pathogen
We present here a scenario which allows us to model the biology of the pathogen
of meningitis and more particularly that of meningococcus. Meningococcal
meningitis is an inﬂammation of the meninges. The organism responsible for
meningococcal meningitis is Neisseria meningitidis or meningococcus which is
an encapsulated Gram-negative diplococcus bacterium belonging to the genus
Neisseria. This bacterium has for its sole host a human being. Meningococci are
classiﬁed according to serological typing. There are 13 distinct meningococcal
groups including 6 serogroups (A, B, C, W-135, X, Y) witch are the major fatal
causes of meningitis. They are represented Respectively in our model by Nm A,
Nm B, Nm C, Nm W-135, Nm X, Nm Y.
5.2
Modeling Meningococcal Transmission, Acquisition
and Infection
Transmission of the germ is achieved by close contact with infected nasopharyn-
geal secretions or by inhalation of droplets of cough (cough, sneeze) emitted by
a carrier. After the germ has been acquired by a healthy individual, the bac-
terium is transported (germ transport process) from the respiratory tract to the
nasopharynx through the outer membranes of the bacterium. The membrane
proteins PorA and PorB are used to facilitate the movement of the bacteria to
the nasopharyngeal mucosa. Then, the bacteria adhere to the mucosa. Coloniza-
tion starts on the mucous membranes using a multifactorial process involving
pili, twitching motility, lipooligosaccharide (LOS) and other more complex mech-
anisms (horizontal genetic exchange, molecular mimicry . . . ) [17]. Inﬂammation
of the meninges occurs as soon as the bacterium crosses the blood-brain barrier
(BBB) to invade the Cerebrospinal Fluid (CSF). Mechanisms for the penetration
of BHM are complex and are still poorly known [15].

Towards Meningitis Ontology
329
Fig. 3. Modeling meningococcal transmission, acquisition and infection
6
Conclusion and Perspectives
In this paper, we presented the methodology we have proposed for building a
meningitis domain ontology. Our proposal is based on the Neon methodology.
We also used a text corpus annotation tool for suggesting ontological resources
relevant to meningitis domain and eligible for possible reuse in the meningitis
ontology engineering.
In the following of this work, the existing ontological resources identiﬁed by
the annotation tool will be reused partially or completely according to the level
of coverage of knowledge related to meningitis. At the end of the construction
of the ontology will come the phase of validation of this ontology by the experts
of the meningitis domain. Then, the ontology is used for the annotation of text
corpora from social medias that will be processed for extraction of data and
events related to meningitis risk factors.

330
W. R. C. B´er´e et al.
References
1. Abb`es, S.B., Zargayouna, H., Nazarenko, A.: Une cartographie de domaine pour
guider la construction d’une ressource s´emantique. In: 23es Journ´ees francophones
d’Ing´enierie des Connaissances, Paris, France, pp. 100–103, June 2012
2. Bendaoud, R.: Construction et enrichissement d’une ontologie `a partir d’un corpus
de textes, pp. 353–358. ARIA, March 2006
3. Borst, W.N.: Construction of Engineering Ontologies for Knowledge Sharing
and Reuse. info:eu-repo/semantics/doctoralThesis, Universiteit Twente, Enschede,
September 1997
4. Camara, G.: Conception d’un syst`eme de veille ´epid´emiologique `a base d’ontologies:
application `a la schistosomiase au S´en´egal. Ph.D. thesis. Th`ese de doctorat dirig´ee
par Despres, Sylvie et Lˆo, Moussa Informatique Paris 6 2013 2013PA066619 (2013)
5. Camara, G., Despres, S., Lo, M.: IDOSCHISTO: une extension de l’ontologie noyau
des maladies infectieuses (IDO-Core) pour la schistosomiase. In: Faron-Zucker, C.
(ed.) IC - 25`emes Journ´ees francophones d’Ing´enierie des Connaissances, Clermont-
Ferrand, France. Session 1: Construction, peuplement et exploitation d’ontologies,
pp. 39–50, May 2014
6. Cowell, L.G., Smith, B.: Infectious Disease Ontology. In: Sintchenko, V. (ed.) Infec-
tious Disease Informatics, pp. 373–395. Springer, New York (2010). https://doi.
org/10.1007/978-1-4419-1327-2 19
7. G´omez-P´erez, A., Fern´andez-L´opez, M., Corcho, O.: Ontological Engineering: With
Examples From the Areas of Knowledge Management, e-Commerce and the Seman-
tic Web. Springer, London (2006). https://doi.org/10.1007/b97353
8. Greene, M.: Epidemiological monitoring for emerging infectious diseases. Proc.
SPIE 7666, 76661B-1–76661B-5 (2010)
9. Gruber, T.R.: Toward principles for the design of ontologies used for knowledge
sharing. Int. J. Hum. Comput. Stud. 43(5–6), 907–928 (1995)
10. Institut national de la sant´e et de la recherche m´edicale (Inserm): M´eningites
Bact´eriennes: Strat´egies de traitement et de pr´evention. Les ´editions Inserm (1996)
11. Jonquet, C., Musen, M.A., Shah, N.H.: Building a biomedical ontology recom-
mender web service. J. Biomed. Semant. 1(1), S1 (2010)
12. Kuipers, B.: Qualitative simulation. Artif. Intell. 29(3), 289–338 (1986)
13. Mizoguchi, R., Ikeda, M.: Towards ontology engineering. J. Jpn. Soc. Artif. Intell.
13, 9–10 (1998)
14. OMS: Contrˆole des ´epid´emies de m´eningite en Afrique: Guide de r´ef´erence rapide
`a l’intention des autorit´es sanitaires et des soignants (2015)
15. Paireau, J.: Spatial epidemiology of meningococcal meningitis in Niger - inﬂuence
of climatic, epidemiologic and socio-demographic factors on the spatio-temporal
dynamics of epidemics. Theses, Universit´e Pierre et Marie Curie - Paris VI, July
2014
16. Psych´e, V., Mendes, O., Bourdeau, J.: Apport de l’ing´enierie ontologique aux envi-
ronnements de formation ´a distance. Sciences et Technologies de l’Information et de
la Communication pour l’´Education et la Formation (STICEF) 10, 89–126 (2003)
17. Rouphael, N.G., Stephens, D.S.: Neisseria meningitidis: biology, microbiology, and
epidemiology. Methods Mol. Biol. (Clifton, NJ) 799, 1–20 (2012)
18. Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters, W., Goldberg,
L.J., Eilbeck, K., Ireland, A., Mungall, C.J., Leontis, N., Rocca-Serra, P., Rutten-
berg, A., Sansone, S.A., Scheuermann, R.H., Shah, N., Whetzel, P.L., Lewis, S.:
The OBO Foundry: coordinated evolution of ontologies to support biomedical data
integration. Nat. Biotechnol. 25, 1251–1255 (2007)

Towards Meningitis Ontology
331
19. Staab, S., Studer, R., Schnurr, H.-P., Sure, Y.: Knowledge processes and ontologies.
IEEE Intell. Syst. 16(1), 26–34 (2001)
20. Studer, R., Richard Benjamins, V., Fensel, D.: Knowledge engineering: principles
and methods. IEEE Trans. Data Knowl. Eng. 25, 161–197 (1998)
21. Su´arez-Figueroa, M.C., G´omez-P´erez, A., Fern´andez-L´opez, M.: The NeOn
methodology for ontology engineering. In: Su´arez-Figueroa, M.C., G´omez-P´erez,
A., Motta, E., Gangemi, A. (eds.) Ontology Engineering in a Networked World,
pp. 9–34. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-24794-
1 2

Author Index
Abdou, Abdoul Dalibou
267
Adeloye, Davies
165, 177
Ahossi, Agbeti Bricos
234
Ajayi, Priscilla
165
Ayeni, Foluso
177
Azeta, Ambrose
107
Bah, Alassane
257
Baldé, Mamadou
16
Béré, W. R. Cédric
322
Bilal, Boudy Ould
31
Bouchet, François
245
Cadiou, Jean-Charles
45
Camara, Gaoussou
322
Camara, Mamadou Samba
257
Charki, Abdéraﬁ
31
Chene, Emmanuel
45, 94
Chistofol, Hervé
94
Compaoré Sawadogo, Eveline M. F. W.
150
Damoue, Zacharia
304
Diallo, I.
81
Diallo, Moussa
115, 312
Diallo, Ndèye Oury
16
Diaw, Samba
292
Dieng, A.
81
Diop, Abdou K.
292
Diop, Alassane
56
Diop, Idy
189, 213, 292
Diouf, Birahime
189, 213, 292
Diouf, Madiop
81, 189, 213
Diouf, P.
81
Dioum, Ibra
213, 304
Elbiaze, Halima
312
Elmimouni, O. I.
131
Fall, Ibrahima
257
Farsi, Sidi Mohamed
189, 213, 292
Faye, Pape Mamadou Djidiack
66
Faye, Roger Marcelin
234
Ferrera Cobos, F.
3
Gueye, A.
131
Gueye, Amadou Dahirou
66
Gueye, Assane
81
Gueye, Bamba
115, 312
Gueye, M. L.
131
Hotte, Richard
56
Iheme, Precious
177
Jonathan, Oluranti
107
Ka, Ahmad Khoureich
202
Kabore, Pazisnewende François
141
Kane, Cheikhou
16
Kare, M.
81
Kassé, Bassirou
115, 312
Kébé, Cheikh M. F.
3, 31
Keita, Khadidiatou Wane
189
Khouma, Ousmane
189, 292
Kora, Ahmed Dooguy
234, 304
Labat, Jean-Marc
245
Lishou, Claude
66
Lo, Moussa
322
Mahmoudi, C.
131
Malo, Sadouanouan
322
Mar Diop, Codou
16
Mbacké, Maryam Khadim
16
Mballo, Marie Hélène Wassa
56
Mendy, Gervais
125
Misra, Sanjay
107, 165, 177
Montois, Jean J.
292
Navarro, A. A.
3
Ndaw, Marie
125
Ndiaye, Ababacar
31
Ndiaye, Cheikh H. T. C.
267
Ndiaye, Mamadou Lamine
292
Ndiaye, P. I.
81
Ndiaye, Papa A.
31
Ndiaye, S. O.
131

Ngom, Ndeye Fatou
267
Niang, Ibrahima
56
Niang, Oumar
267
Omari, Rose
156
Omoregbe, Nicholas
165, 177
Ouaro, Stanislas
322
Ouya, Samuel
125
Rabany, Cheikh B.
304
Ramírez, L.
3
Richir, Simon
94
Sambe, Gorgoumack
245
Sambou, Vincent
31
Samier, Henri
94
Sane, Lamine
213
Sawadogo, Natewinde
150
Seck, Diaraf
125
Sene, A.
81
Sene, Omar
81
Sidibé, Samba
267
Soh, Mathurin
222
Tall, Khaly
189, 213, 292
Tchoupé Tchendji, Maurice
279
Thiao, I. P.
81
Thioune, Abdoulaye
267
Toure, El Hadji Bassirou
257
Valenzuela, R. X.
3
Vindel, José M.
3
Wane, Ousmane
3
Zarzalejo, L. F.
3
Zekeng Ndadji, Milliam Maxime
279
334
Author Index


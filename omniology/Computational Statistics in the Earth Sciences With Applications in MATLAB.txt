Computational Statistics in the Earth Sciences
Based on a course taught by the author, this book combines the theoretical underpinnings
of statistics with the practical analysis of earth sciences data using MATLAB. The book is
organized to introduce the underlying concepts and then extends these to data, covering
methods that are most applicable to earth sciences. Topics include classical parametric
estimation and hypothesis testing and more advanced nonparametric and resampling
estimators. The method of least squares is explored in detail. Multivariate data analysis,
not often encountered in introductory texts, is presented later in the book, and compos-
itional data are treated at the end. Data sets and bespoke MATLAB scripts used in the book
are available online, as well as additional data sets and suggested questions for use by
instructors. Aimed at entering graduate students and practicing researchers in the earth and
ocean sciences, this book is ideal for those who want to learn how to analyze data using
MATLAB in a statistically rigorous manner.
Alan D. Chave is a senior scientist at Woods Hole Oceanographic Institution and holds the
Walter A. and Hope Noyes Smith Chair for Excellence in Oceanography. He has also been
a Chartered Statistician (United Kingdom) since 2003 and has taught a graduate-level
course in statistics in the MIT/WHOI Joint Program for 20 years. For over 30 years he
has conducted research using the magnetotelluric method, primarily in the oceans, and
electromagnetic measurements to deﬁne the barotropic water velocity. Dr. Chave has also
designed instrumentation for optical and chemical measurements in the ocean and has
played a leadership role in developing long-term ocean observatories worldwide. He has
been an editor of the Journal of Geophysical Research and editor-in-chief of Reviews of
Geophysics and is the co-author of The Magnetotelluric Method (Cambridge University
Press, 2012).
13:26:38, subject to the Cambridge Core terms of use,

13:26:38, subject to the Cambridge Core terms of use,

Computational Statistics in
the Earth Sciences
With Applications in MATLAB
ALAN D. CHAVE
Woods Hole Oceanographic Institution, Woods Hole, Massachusetts
13:26:38, subject to the Cambridge Core terms of use,

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
4843/24, 2nd Floor, Ansari Road, Daryaganj, Delhi – 110002, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107096004
DOI: 10.1017/9781316156100
© Alan D. Chave 2017
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2017
Printed in the United Kingdom by TJ International Ltd. Padstow Cornwall
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging-in-Publication Data
Names: Chave, Alan Dana.
Title: Computational statistics in the earth sciences : with applications in MATLAB / Alan D. Chave,
Woods Hole Oceanographic Institution, Woods Hole, Massachusetts.
Description: Cambridge : Cambridge University Press, 2017. | Includes bibliographical
references and index.
Identiﬁers: LCCN 2017009160 | ISBN 9781107096004 (hardback)
Subjects: LCSH: Earth sciences–Statistical methods. | Earth sciences–Statistical methods–Data processing. |
Mathematical statistics–Data processing. | MATLAB.
Classiﬁcation: LCC QE26.3 .C43 2017 | DDC 519.50285–dc23 LC record
available at https://lccn.loc.gov/2017009160
ISBN 978-1-107-09600-4 Hardback
Additional resources for this publication at www.cambridge.org/chave.
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.
13:26:38, subject to the Cambridge Core terms of use,

Contents
Preface
page xi
1 Probability Concepts
1
1.1 Fundamental Concepts and Motivating Example
1
1.2 Probability Philosophies
4
1.3 Set Theory
5
1.4 Deﬁnition of Probability
8
1.5 Finite Sample Spaces
10
1.5.1 Simple Sample Spaces
10
1.5.2 Permutations
11
1.5.3 Combinations
12
1.6 Probability of a Union of Events
13
1.7 Conditional Probability
14
1.8 Independent Events
15
1.9 Bayes’ Theorem
16
2 Statistical Concepts
18
2.1 Overview
18
2.2 The Probability Density Function
18
2.2.1 Discrete Distributions
18
2.2.2 Continuous Distributions
20
2.2.3 Mixed Distributions
22
2.3 The Cumulative Distribution and Quantile Functions
23
2.4 The Characteristic Function
25
2.5 Bivariate Distributions
27
2.6 Independent and Exchangeable Random Variables
28
2.7 Conditional Probability Distributions
30
2.8 Functions of a Random Variable
32
2.9 Functions of Two or More Random Variables
34
2.10 Measures of Location
36
2.11 Measures of Dispersion
39
2.12 Measures of Shape
41
2.13 Measures of Direction
42
2.14 Measures of Association
44
2.15 Conditional Expected Value and Variance
44
v
13:26:48, subject to the Cambridge Core terms of use,

2.16 Probability Inequalities
45
2.17 Convergence of Random Variables
46
3 Statistical Distributions
48
3.1 Overview
48
3.2 MATLAB Support for Distributions
48
3.3 Discrete Distributions
49
3.3.1 Bernoulli Distribution
49
3.3.2 Binomial Distribution
50
3.3.3 Negative Binomial Distribution
53
3.3.4 Multinomial Distribution
54
3.3.5 Hypergeometric Distribution
56
3.3.6 Poisson Distribution
59
3.4 Continuous Distributions
62
3.4.1 Normal or Gaussian Distribution
62
3.4.2 Stable Distributions
65
3.4.3 Rayleigh Distribution
67
3.4.4 Lognormal Distribution
69
3.4.5 Gamma Distribution
70
3.4.6 Exponential Distribution
72
3.4.7 Weibull Distribution
74
3.4.8 Beta Distribution
76
3.4.9 Generalized Extreme Value Distribution
78
3.4.10 Bivariate Gaussian Distribution
80
3.4.11 Directional Distributions
81
4 Characterization of Data
86
4.1 Overview
86
4.2 Estimators of Location
86
4.3 Estimators of Dispersion
91
4.4 Estimators of Shape
93
4.5 Estimators of Direction
95
4.6 Estimators of Association
100
4.7 Limit Theorems
101
4.7.1 The Laws of Large Numbers
101
4.7.2 Classic Central Limit Theorems
102
4.7.3 Other Central Limit Theorems
104
4.7.4 The Delta Method
105
4.8 Exploratory Data Analysis Tools
106
4.8.1 The Probability Integral Transform
106
4.8.2 The Histogram and Empirical CDF
107
4.8.3 Kernel Density Estimators
111
4.8.4 The Percent-Percent and Quantile-Quantile Plots
115
4.8.5 Simulation
120
vi
Contents
13:26:48, subject to the Cambridge Core terms of use,

4.9
Sampling Distributions
122
4.9.1 Chi Square Distributions
122
4.9.2 Student’s t Distributions
125
4.9.3 The F Distributions
128
4.9.4 The Correlation Coefﬁcient
130
4.10 Distributions for Order Statistics
135
4.10.1 Distribution of a Single Order Statistic
135
4.10.2 Distribution of the Sample Median
137
4.10.3 Joint Distribution of a Pair of Order Statistics
138
4.10.4 Distribution of the Interquartile Range
138
4.11 Joint Distribution of the Sample Mean and Sample Variance
140
5 Point, Interval, and Ratio Estimators
142
5.1 Overview
142
5.2 Optimal Estimators
142
5.2.1 Consistency
142
5.2.2 Unbiased Estimators
143
5.2.3 Efﬁciency and the Cramér-Rao Lower Bound
144
5.2.4 Robustness
147
5.2.5 Sufﬁcient Statistics
148
5.2.6 Statistical Decision Theory
152
5.3 Point Estimation: Method of Moments
154
5.4 Point Estimation: Maximum Likelihood Estimator
155
5.5 Interval Estimation: Conﬁdence and Tolerance Intervals
160
5.6 Ratio Estimators
166
6 Hypothesis Testing
169
6.1 Introduction
169
6.2 Theory of Hypothesis Tests I
171
6.3 Parametric Hypothesis Tests
177
6.3.1 The z Test
177
6.3.2 The t Tests
178
6.3.3 The χ2 Test
186
6.3.4 The F Test
188
6.3.5 Bartlett’s M Test for Homogeneity of Variance
189
6.3.6 The Correlation Coefﬁcient
190
6.3.7 Analysis of Variance
192
6.3.8 Sample Size and Power
194
6.4 Hypothesis Tests and Conﬁdence Intervals
195
6.5 Theory of Hypothesis Tests II
196
6.5.1 Likelihood Ratio Tests for Simple Hypotheses
197
6.5.2 Uniformly Most Powerful Tests
198
6.5.3 Likelihood Ratio Tests for Composite Hypotheses
200
vii
Contents
13:26:48, subject to the Cambridge Core terms of use,

6.5.4 The Wald Test
207
6.5.5 The Score Test
208
6.6 Multiple Hypothesis Tests
210
7 Nonparametric Methods
214
7.1 Overview
214
7.2 Goodness-of-Fit Tests
214
7.2.1 Likelihood Ratio Test for the Multinomial Distribution
214
7.2.2 Pearson’s χ2 Test for Goodness-of-Fit
219
7.2.3 Kolmogorov-Smirnov Test
222
7.2.4 Cramér–von Mises Tests
228
7.2.5 Jarque-Bera Test
230
7.3 Tests Based on Ranks
231
7.3.1 Properties of Ranks
231
7.3.2 Sign Test
232
7.3.3 Signed Rank Test
235
7.3.4 Rank Sum Test
237
7.3.5 Ansari-Bradley Test
240
7.3.6 Spearman Rank Correlation Test
241
7.3.7 Kendall’s Tau
242
7.3.8 Nonparametric ANOVA
243
7.4 Meta-analysis
245
8 Resampling Methods
247
8.1 Overview
247
8.2 The Bootstrap
247
8.2.1 The Bootstrap Distribution
247
8.2.2 Bootstrap Parameter Estimation
251
8.2.3 Bootstrap Conﬁdence Intervals
255
8.2.4 Bootstrap Hypothesis Tests
259
8.2.5 Bias Correction for Goodness-of-Fit Tests
265
8.3 Permutation Tests
267
8.3.1 Principles
267
8.3.2 One-Sample Test for a Location Parameter
268
8.3.3 Two-Sample Test for a Location Parameter
270
8.3.4 Two-Sample Test for Paired Data
274
8.3.5 Two-Sample Test for Dispersion
275
8.4 The Jackknife
277
9 Linear Regression
281
9.1 Motivating Example
281
9.2 Statistical Basis for Linear Regression
283
viii
Contents
13:26:48, subject to the Cambridge Core terms of use,

9.3 Numerical Considerations
286
9.4 Statistical Inference in Linear Regression
289
9.4.1 Analysis of Variance
289
9.4.2 Hypothesis Testing on the Regression Estimates
291
9.4.3 Conﬁdence Intervals
292
9.4.4 The Runs and Durbin-Watson Tests
294
9.5 Linear Regression in Practice
295
9.5.1 Assessing the Results
295
9.5.2 Examples
298
9.6 Robust and Bounded Inﬂuence Regression
316
9.6.1 Robust Estimators
317
9.6.2 Bounded Inﬂuence Estimators
327
9.7 Advanced Linear Regression
335
9.7.1 Errors in Variables
335
9.7.2 Shrinkage Estimators
336
9.7.3 Logistic Regression
340
10 Multivariate Statistics
344
10.1 Concepts and Notation
344
10.2 The Multivariate Gaussian Distribution
346
10.2.1 Derivation of the Multivariate Gaussian Distribution
346
10.2.2 Properties of the MV Gaussian Distribution
347
10.2.3 The Sample Mean Vector and Sample Covariance Matrix
348
10.2.4 The Complex Multivariate Gaussian Distribution
349
10.3 Hotelling’s T2 Tests
350
10.4 Multivariate Analysis of Variance
354
10.5 Hypothesis Tests on the Covariance Matrix
362
10.5.1 Sphericity Test
363
10.5.2 Comparing Covariance Matrices
364
10.5.3 Test of Independence
365
10.6 Multivariate Regression
366
10.7 Canonical Correlation
371
10.8 Empirical Orthogonal Functions
373
10.8.1 Theory
374
10.8.2 Choosing the Number of Eofs
377
10.8.3 Example
378
10.8.4 Empirical Orthogonal Function Regression
385
11 Compositional Data
391
11.1 Introduction
391
11.2 Statistical Concepts for Compositions
392
11.2.1 Deﬁnitions and Principles
392
ix
Contents
13:26:48, subject to the Cambridge Core terms of use,

11.2.2 Compositional Geometry
395
11.2.3 Compositional Transformations
400
11.3 Exploratory Compositional Data Analysis
405
Appendix 11A: MATLAB Functions to Produce Ternary Diagrams
429
References
435
Index
444
x
Contents
13:26:48, subject to the Cambridge Core terms of use,

Preface
Statistics, and especially its application to data, is an essential tool in the sciences, and the
earth and ocean sciences are no exception. However, the study of statistics beyond a very
rudimentary level is usually neglected in undergraduate curricula, and hence entering
graduate students in the earth and ocean sciences need to acquire a background in statistics
early in their tenure. For this reason, a course entitled, “Computational Data Analysis”
(MIT 12.714), was devised more than 20 years ago for the Massachusetts Institute of
Technology/Woods Hole Oceanographic Institution Joint Program in Oceanography. An
abbreviation of this book constitutes the ﬁrst half of the course, with the remainder being
devoted to spectral analysis. The computational tool used in the course and book is
MATLAB, which has become nearly ubiquitous in geophysics and oceanography. The
emphasis in the course is on analyzing data rather than abstract theory, and given that it is a
graduate course, homework constitutes the analysis of data sets using the tools presented in
lectures. Representative data sets and questions are available on the Cambridge University
Press website at http://www.cambridge.org/chave. In addition, all the exemplar data sets
and MATLAB functions used in this book can be found at the same location.
The book constitutes 11 chapters that present introductory statistics at an entering
graduate level in a manner that is logical (at least to the author). The material in the ﬁrst
two chapters appears in every book on statistics, constituting the theory of probability
and statistical concepts that underlie the remainder of the
book. Chapter 1 introduces
probability concepts ﬁrst through MATLAB examples and then describes set theory and
the Kolmogorov axioms that constitute the basis for probability theory. Permutation and
combination are described, leading to the probability of a union of events, conditional
probability, the concept of independence, and ﬁnally Bayes’ theorem.
Chapter 2 is entitled “Statistical Concepts” and covers the probability density, cumula-
tive distribution, quantile, and characteristic functions for discrete and continuous distribu-
tions. These concepts are extended to the bivariate case, and then independence and
exchangeability are formally deﬁned. Conditional, joint, and marginal distributions are
introduced, and methods to transform distributions as random variables change are
described. The chapter then sets out population measures of location, dispersion, shape,
direction, and association. It closes by deﬁning conditional expectation and variance that
underlie the theory of least squares, and probability inequalities and convergence.
Chapter 3 describes the major discrete and continuous distributions that are encountered
in the sciences and contains considerable material that is useful for reference.
Chapter 4 moves from the theoretical basis of statistics into the characterization of data.
It ﬁrst extends the population measures of location, dispersion, shape, direction, and
xi
13:26:21, subject to the Cambridge Core terms of use,

association to sample entities and introduces the MATLAB tools for this purpose. Limit
theorems are then described. A set of exploratory data analysis tools is set out, including
the histogram, the empirical cumulative distribution function, the kernel density estimator,
percent-percent and quantile-quantile plots, and simulation. These are extensively illus-
trated by examples using real data. The major sampling distributions used throughout the
remainder of the book are introduced, and then the distributions of the order statistics are
derived, including that for the sample median and interquartile distance. The chapter closes
by describing the joint distribution of the sample mean and variance.
Chapter 5 ﬁrst covers a set of estimator optimality criteria, including consistency,
unbiasedness, efﬁciency, robustness, and sufﬁciency, which are essential to characterize
estimator performance. It then introduces the methods of moments and maximum likeli-
hood, conﬁdence and tolerance intervals, and ratio estimators. All of these topics are
characterized through MATLAB examples.
Chapter 6 describes the theory of hypothesis testing, including the concept of a likeli-
hood ratio test and its asymptotic approximation via the Wald and score tests. A set of
standard parametric hypothesis tests is introduced and illustrated with data. The concept of
statistical power receives considerable emphasis. The chapter closes by setting out the
testing of multiple simultaneous hypotheses that is becoming essential in the modern world
of big data.
Chapter 7 covers goodness-of-ﬁt and rank-based testing. The multinomial likelihood
ratio test is emphasized over its widely used asymptotic approximation, the Pearson χ2
goodness-of-ﬁt test. The Kolmogorov-Smirnov test is deﬁned, and its use to place conﬁ-
dence bounds on percent-percent and quantile-quantile plots is described. The more
powerful Anderson-Darling goodness-of-ﬁt test is also described. A set of standard non-
parametric rank hypothesis tests is then introduced that typically lack power over their
parametric counterparts but are less sensitive to mixtures of distributions that are common
in earth and ocean sciences data. The chapter closes by discussing meta-analysis.
Chapter 8 describes resampling methods that are being used more extensively as
computational power increases. The most widely used resampling tool is the bootstrap
that is based on sampling with replacement, hence yielding approximate results that are
useful in characterizing complicated estimators and conﬁdence intervals for them. A Monte
Carlo approach to removing bias from goodness-of-ﬁt tests is then described. Permutation
hypothesis testing based on resampling without replacement then receives considerable
attention. Permutation methods yield exact tests and, in the author’s opinion, are the tool of
choice for earth and ocean sciences data.
Chapter 9 sets out the theory of linear regression beginning with examples and then the
statistical basis for the realistic case where the predictors are random variables. Numerical
considerations receive attention, followed by a set of statistical tools to characterize least
squares estimates and quantify their consistency with their statistical basis. Procedures for
assessing a linear regression are described and illustrated through examples. Robust and
bounded inﬂuence extensions that are the linear regression tools of choice in the earth
sciences are then deﬁned and illustrated. Errors in variables, shrinkage estimators, and the
general linear model close the chapter.
xii
Preface
13:26:21, subject to the Cambridge Core terms of use,

Chapter 10 introduces multivariate statistical methods beginning with the real and
complex multivariate Gaussian distributions. It then covers the multivariate extensions of
the Student t and F distributions, Hotelling’s T2 and Wilks’ Λ, and their application in
hypothesis testing, multivariate analysis of variance, and multivariate regression. Canon-
ical correlation is introduced, and the chapter closes with the theory and application of
empirical orthogonal functions (often called principal components).
Chapter 11 covers the analysis of compositional data that are highly multivariate but
constrained so that the sum of their parts equals a constant for each observation. Such data
are very common in the earth sciences; rock composition in weight percent or parts per
million is a prominent example. The theory of compositional data has only evolved in the
past 30 years and is beginning a migration from mathematical statistics into science
domains. A set of MATLAB functions for analyzing compositional data and their presen-
tation on ternary diagrams is provided to facilitate this process. Exploratory analysis of
rock composition data provides an exemplar.
The MATLAB version used to produce this book is R2015b. The only changes in
functionality that would affect the treatment is the addition of stable distribution objects
in R2016a, and a change from rose to polarhistogram to create a polar histogram, as used
in Chapters 4 and 6.
I acknowledge the two co-lecturers for 12.714, ﬁrst Marcia McNutt and then Tom
Herring, for their inﬂuence on the material in this book. I also recognize the graduate
students at both MIT and WHOI who have suffered through the course over the past two
decades.
xiii
Preface
13:26:21, subject to the Cambridge Core terms of use,

13:26:21, subject to the Cambridge Core terms of use,

1
Probability Concepts
1.1 Fundamental Concepts and Motivating Example
Probability is the mathematical description of uncertainty, and many of its abstractions
underlie the statistical concepts that are covered in Chapter 2. The material in the ﬁrst two
chapters of this book is very standard, and is covered at an elementary level by DeGroot &
Schervish (2011) and at a more advanced level by Wasserman (2004) and Rice (2006).
A core purpose of this section is to illustrate a set of concepts using MATLAB as a tool.
An experiment is an activity whose result is not known a priori with certainty. An event or
outcome is one element of a collection of results obtained by performing an experiment.
A simple event is an event that cannot be broken down into some combination of other
events. A composite event is an event that is not simple. The sample space is the collection
of all possible outcomes of an experiment. Since the possible outcomes are known in
advance, the sample space can be deﬁned before an experiment is performed.
A motivating example can be produced using the MATLAB unidrnd(N, M, k) function
that gives k realizations of M random draws from the integers ranging between 1 and N. For
M = 5 and N = 50, the outcome of a single experiment is
unidrnd(50, 5, 1)
ans =
41
46
7
46
32
Note that the MATLAB result depends on a random number seed and may differ
depending on the version. The random number generator may be initialized with the same
value by issuing the “rng default” command before the unidrnd one, and doing so before
the ﬁrst call will duplicate the results of this section.
If unidrnd is called a second time, a different outcome is obtained
unidrnd(50, 5, 1)
ans =
5
14
28
48
49
1
.002
13:29:21, subject to the Cambridge Core terms of use,

Each time the command is executed, an experiment is performed. The value for one
experiment is an event, such as {41, 46, 7, 46, 32}. The sample space is all possible
permutations of ﬁve integers lying between 1 and 50 and can be written down prior to
performing any experiments. As shown in Section 1.5.3, there are 2,118,760 unique
elements in the sample space.
A probability measure assigns a likelihood in the form of a real number lying between
0 and 1 to each event in a sample space. A probability measure for a given integer lying
between 1 and N can be inferred by carrying out a large number of trials and counting the
number of occurrences of that integer compared with the number of trials. It is reasonable
to expect that any integer lying between 1 and N will occur with an equal probability of
1=N for a single trial. This can easily be checked by simulation. A heuristic demonstration
obtains from plotting a histogram of 100 draws of ﬁve integers ranging from 1 to 50, as
shown in Figure 1.1.
histogram(reshape(unidrnd(50, 5, 100), 1, 500))
The ordinate is the number of times a given value has occurred over 100 draws
and 5 variables, whereas the abscissa assigns them to 50 equal interval bins covering
the integers between 1 and 50. One would expect intuitively that the ordinate would
be about 10 on average, which is approximately true, but with considerable variabil-
ity. Repeating the experiment with 1000 realizations, whose outcome ought to be
about 100
histogram(reshape(unidrnd(50, 5, 1000), 1, 5000))
The result appears much more uniform (Figure 1.2).
A random variable (rv) is a real-valued, measurable function that transforms an event
into a real number. Since a random variable depends on an outcome that is not known a
priori, the value of a random variable is uncertain until after an experiment has been
performed. An example of an rv is the minimum value from a given trial using unidrnd.
This can be summarized using a histogram (Figure 1.3).
0
0
5
10
10
15
20
20
30
40
50
Figure 1.1
Histogram of 100 random draws of ﬁve values from the uniformly distributed integers between 1 and 50.
2
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

histogram(min(unidrnd(50, 5, 100)))
Figure 1.4 shows the result from 1000 replications.
histogram(min(unidrnd(50, 5, 1000)))
Figures 1.3 and 1.4 hint at an important underlying idea: the value of an rv observed
many times will exhibit a characteristic pattern with variability. The number of occurrences
(i.e., the heights of the histogram bins) for a particular range of the integers (the abscissa)
for a sufﬁciently large number of replications will stabilize around some set of values that
deﬁnes the probability distribution. While the distribution for the integer draws is uni-
formly distributed, that for the minimum value is not.
The expectation or expected value of an rv is a single number that gives its average value
(in some well-deﬁned sense). If all of the possible values of the rv are not equally likely (as
for the minimum value of uniformly distributed integer draws but not for the uniformly
distributed integers themselves), it may not make sense to use the simple arithmetic
average. Rather, it would be more appropriate to use a weighted average, with the weights
given by the probabilities of occurrence. This will be quantiﬁed in Chapter 2.
15
20
25
30
35
40
0 0
2
4
6
8
10
12
5
10
Figure 1.3
Histogram of the minimum value obtained from 100 random draws of ﬁve values from the uniformly distributed
integers between 1 and 50.
00
20
40
60
80
100
120
10
20
30
40
50
Figure 1.2
Histogram of 1000 random draws of ﬁve values from the uniformly distributed integers between 1 and 50.
3
1.1 Fundamental Concepts and Motivating Example
.002
13:29:21, subject to the Cambridge Core terms of use,

1.2 Probability Philosophies
The term probability enters into daily conversation (e.g., “It probably will rain tomorrow”),
although such usage is typically imprecise. In fact, lack of a clear deﬁnition exists in formal
theory as well as in daily life. There are two main deﬁnitions for probability, with their
respective schools failing to agree on some basic concepts, and the ensuing conﬂict
sometimes exhibits a quasi-religious dimension.
Frequentist or classical inference is based on three postulates:
1. Probability is the limiting relative frequency of occurrence of an event if the process
creating it is repeated many times under identical conditions;
2. Parameters are invariant constants, so probability statements cannot be made about
them; and
3. Statistical inference procedures should be designed to yield well-deﬁned large-sample
frequency properties.
The frequentist interpretation of probability given by postulate 1 pervades Section 1.1 and
is itself not precise. For example, what is meant by “repeated many times” or “under
identical conditions”? The remaining two postulates will be examined later in this book.
The Bayesian or subjective interpretation is based on three alternate postulates:
1. Probability statements are equivalent to claims about degree of belief, so the interpret-
ation of probability is based on the judgment of the person making the assignment;
2. Probability statements can be assigned to objects that do not exhibit random variation,
such as constant parameters; and
3. Statistical inferences about parameters may be made after deﬁning their probability
distributions.
The Bayesian approach has its origins in a paper by Bayes (1763). It is sometimes
controversial because of its inherent subjectivity, although it is playing a growing role in
0
0
20
40
60
80
100
120
10
20
30
40
Figure 1.4
Histogram of the minimum value obtained from 1000 random draws of ﬁve values from the uniformly distributed
integers between 1 and 50.
4
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

applied statistics and machine learning due to the rapid recent increase in computational
capability. This book will focus on the frequentist approach. A comprehensive survey of
Bayesian data analysis is provided by Gelman et al. (2013).
Despite the controversy about the meaning of probabilities that are assigned to events
from experiments, there is agreement that once probabilities are assigned, the mathematical
theory of probability gives a methodology to study them. The following sections circum-
scribe the scope of the theory of probability through deﬁning set theory, the probability
axioms
and
their
corollaries,
ﬁnite
sample
spaces,
conditional
probability,
and
independence.
1.3 Set Theory
A formal mathematical model for probability follows from the theory of sets. All the
entities introduced in Section 1.1 can be deﬁned using set theory. Denote the sample space
by S. An event or outcome is a part A of the sample space, written as A 2 S. The symbol
2 means “is an element of.” Some outcomes in the sample space signify that the event
A occurred, and all other outcomes signify that it did not. Let B denote an outcome that is
not in S. Then B =2 S, where =2 means “is not an element of.”
Let A  S be an event in the sample space. The symbol  means “is a subset of” or
“belongs to.” Conversely, the symbol ⊄means “does not belong to.” Then A is contained
in another event B if every outcome in A also belongs to B or, in symbols, A  B.
Equivalently, if A  B, then B  A, where the symbol  means “is contained in.”
Some fairly obvious corollaries are
If
A  B
and
B  A,
then
A ¼ B
(1.1)
where the last statement indicates that the events A and B are identical;
If
A  B
and
B  C,
then
A  C
(1.2)
The null or empty set ∅is the subset of A that contains no outcomes, so
∅ A  S
(1.3)
Some sets contain a ﬁnite number of elements, whereas others may contain an inﬁnite
number. In that case, a countably inﬁnite set, in which there is a one-to-one correspondence
between the set elements and the natural numbers {1, 2, 3, ...}, must be distinguished from
an uncountably inﬁnite set that is neither ﬁnite nor countable. Examples of countable sets
include the integers, the even integers, the odd integers, and the prime numbers. Examples
of uncountable sets include the real numbers and the numbers contained on a speciﬁed
interval of the real line.
The operations of set theory include union, intersection, and complement. These are
equivalent to the logical operations || or “or,” && or “and,” and ~ or “not” in MATLAB. It
is standard practice to use Venn diagrams to visualize these concepts. The Venn diagram
for the sample space S is shown in Figure 1.5.
5
1.3 Set Theory
.002
13:29:21, subject to the Cambridge Core terms of use,

If A 2 S is an event in a sample space, then AC or “A complement” or “not A” is the set
of possible outcomes in S that are not in A. The Venn diagram illustrating the concept of
complement is shown in Figure 1.6.
Some obvious implications that can all be proved using either logic or Venn diagrams are
AC

C ¼ A
(1.4)
∅C ¼ S
(1.5)
SC ¼ ∅
(1.6)
If A and B are events, then the event A [ B, or “A union B” or “A or B,” is the event
containing all possible outcomes in A alone, B alone, and in both A and B. The Venn
diagram illustrating union is shown in Figure 1.7.
Figure 1.5
Venn diagram of the sample space S.
Figure 1.6
Venn diagram for the event A and its complement.
Figure 1.7
Venn diagram illustrating the union of events A and B.
6
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

Some obvious implications follow that can be derived by drawing Venn diagrams:
A [ B ¼ B [ A
(1.7)
A [ ∅¼ A
(1.8)
A [ A ¼ A
(1.9)
A [ S ¼ S
(1.10)
If A  B, then A [ B ¼ B
(1.11)
A [ B [ C ¼ A [ B
ð
Þ [ C ¼ A [ B [ C
ð
Þ
(1.12)
Equation (1.7) is the commutative property, whereas (1.12) is the associative property. The
union of three events may be written
A1 [ A2 [ A3 ¼
[
3
i¼1
Ai
(1.13)
and extends trivially to N events.
If A and B are two events, then the event A \ B, or “A intersection B,” “A and B,” or
AB (although the last form will be avoided in this book to minimize confusion with
multiplication) is the event containing all possible outcomes belonging to both A and
B. The Venn diagram illustrating intersection is shown in Figure 1.8.
Some consequences follow directly and can also be proved using Venn diagrams:
A \ B ¼ B \ A
(1.14)
A \ ∅¼ ∅
(1.15)
A \ A ¼ A
(1.16)
A \ S ¼ A
(1.17)
If A  B, then A \ B ¼ A
(1.18)
A \ B \ C ¼ A \ B
ð
Þ \ C ¼ A \ B \ C
ð
Þ
(1.19)
Figure 1.8
Venn diagram illustrating the intersection of two events A and B. The intersection is the white area between the
two events.
7
1.3 Set Theory
.002
13:29:21, subject to the Cambridge Core terms of use,

Equation (1.14) is the commutative property, whereas (1.19) is the associative property.
The intersection of three events may be expressed as
A1 \ A2 \ A3 ¼
\
3
i¼1
Ai
(1.20)
and generalizes trivially to N events.
If A \ B ¼ ∅, then A and B are mutually exclusive or disjoint. This appears as
nonintersecting sets A and B on a Venn diagram.
Set theory will conclude with two less intuitive pairs of relations. De Morgan’s theorem
holds that
A [ B
ð
ÞC ¼ AC \ BC
A \ B
ð
ÞC ¼ AC [ BC
(1.21)
The distributive properties combine intersection and union:
A [ B
ð
Þ \ C ¼ A \ C
ð
Þ [ B \ C
ð
Þ
(1.22)
A \ B
ð
Þ [ C ¼ A [ C
ð
Þ \ B [ C
ð
Þ
(1.23)
Both of these can be proved using Venn diagrams, and the reader is encouraged to do so.
1.4 Deﬁnition of Probability
For each event A 2 S, a number Pr A
ð Þ may be assigned that measures the probability that
A will occur. In order to satisfy the mathematical deﬁnition of probability, the number
Pr A
ð Þ must satisfy three conditions called the Kolmogorov axioms
Pr A
ð Þ  0
(1.24)
Pr S
ð Þ ¼ 1
(1.25)
and, for every inﬁnite sequence of disjoint events
Ai
f
g,
Pr
[
∞
i¼1
Ai ¼
X
∞
i¼1
Pr Ai
ð
Þ
(1.26)
The mathematical deﬁnition of probability holds that on a sample space S, the probability
distribution (or probability) for a set of events Ai
f
g 2 S is the set of numbers Pr Ai
ð
Þ that
satisfy the Kolmogorov axioms.
A number of important corollaries may be derived from the Kolmogorov axioms
(1.24)(1.26). Because A and AC are disjoint,
Pr AC


¼ 1  Pr A
ð Þ
(1.27)
From corollary 1 and because ∅¼ SC,
Pr ∅
ð
Þ ¼ 0
(1.28)
8
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

Equation (1.26) can be extended to a ﬁnite number of events
Pr
[
N
i¼1
Ai
 
!
¼
X
N
i¼1
Pr Ai
ð
Þ
(1.29)
Since A can range from ∅to S,
0  Pr A
ð Þ  1
(1.30)
Finally,
If A  B, then A ¼ A \ B and Pr A
ð Þ  Pr B
ð Þ
(1.31)
For any two events A and B, the general addition law is
Pr A [ B
ð
Þ ¼ Pr A
ð Þ þ Pr B
ð Þ  Pr A \ B
ð
Þ
(1.32)
and will be used extensively throughout this book. If the two events A and B are disjoint,
then Pr A \ B
ð
Þ ¼ 0, and the probability of the union of A and B is just the sum of their
individual probabilities. Boole’s inequality
Pr A [ B
ð
Þ  Pr A
ð Þ þ Pr B
ð Þ
(1.33)
is a simple corollary of the general addition law. Both the general addition law and Boole’s
inequality extend to an arbitrary number of events.
Let the set of events
Ai
f
g be disjoint and exhaust S so that
Ai
f
g fully partitions the
sample space. If B is any other event, the set of events
Ai \ B
f
g is also disjoint (see
Figure 1.9), and hence
B ¼
[
N
i¼1
Ai \ B
ð
Þ
(1.34)
It is not necessary that B intersect each Ai, as is depicted in Figure 1.9, because
Ai \ B ¼ ∅in its absence. The law of total probability follows directly from corollary 3
Pr B
ð Þ ¼
X
N
i¼1
Pr Ai \ B
ð
Þ
(1.35)
Both (1.34) and (1.35) hold for N ! ∞.
Figure 1.9
The intersection of event B with a partition
Ai
f
g of a sample space.
9
1.4 Deﬁnition of Probability
.002
13:29:21, subject to the Cambridge Core terms of use,

1.5 Finite Sample Spaces
1.5.1 Simple Sample Spaces
Experiments for which there can be only a ﬁnite number of possible outcomes are carried
out in ﬁnite sample spaces such that S ¼ si
f g, i ¼ 1, :::, N. A simple sample space is one
for which each si has a corresponding probability pi ¼ 1=N. In other words, no single
element in the sample space occurs with higher or lower probability than another. In this
case, the probability of A is simply Nr A
ð Þ=N, where Nr A
ð Þ is the number of outcomes in A.
Computing probability becomes a simple matter of counting.
Example 1.1 A sample of six brands of malt beverage consists of equal parts of distinct
brands of stout and ale. Let A be the event “all stout.” There are three possible outcomes in
A, each with probability 1/6; hence Pr A
ð Þ ¼ 3=6 ¼ 1=2. Let A be the event “all stout” and
B be the event “Guinness.” Then Pr A \ B
ð
Þ ¼ 1=6 by counting the intersection of A and B.
However, Pr A [ B
ð
Þ ¼ 3=6 ¼ 1=2. The general law of addition (1.32) also gives this result
because
Pr A [ B
ð
Þ ¼ Pr A
ð Þ þ Pr B
ð Þ  Pr A \ B
ð
Þ ¼ 3=6 þ 1=6  1=6 ¼ 1=2
Example 1.2 In the game of dice, why does it pay to consistently bet on getting a 6 at least
once in four throws of a single die but not on a double 6 at least once in 24 throws of
two dice?
Betting on dice was a constant pasttime in the French court of the early 1700s. An
observant gambler, the Chevalier de Mere, came up with the preceding rule but lacked an
explanation for it. He consulted the mathematician Blaise Pascal, who calculated the
probabilities. The probability of not seeing a 6 in a given throw is 5/6. Presuming that
successive throws are independent, the probability of not seeing a 6 in four throws is (5/6)4.
The probability of seeing a 6 is therefore 1  (5/6)4, or 0.518, which is favorable to the
gambler.
One double roll has 36 possible outcomes. The probability of seeing a double 6 once in
24 throws of two dice is 1  (35/36)24 = 0.491. A gambler would lose on this bet in the
long term. It is fair to conclude that the Chevalier was a remarkably observant man to come
up with this rule purely based on watching!
Suppose that an experiment is performed in two parts, where the ﬁrst has M possible
outcomes and the second has N possible outcomes independent of the number of outcomes
from the ﬁrst part. Then S contains MN possible outcomes, where each element is an
10
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

ordered pair consisting of an outcome from the ﬁrst and second experiments. For a simple
sample space, the counting approach still pertains. This extends trivially to experiments
with more than two parts.
Example 1.3 A 16-bit binary word is a sequence of 16 digits that may take a value of either
0 or 1. How many different 16-bit words are there?
Each bit has two possible outcomes (0 or 1). There are 16 bits, so the number of different
words is 22. . .2 = 216 = 65,536.
1.5.2 Permutations
A permutation is an ordered collection of objects. From a sample of N objects, select one
specimen at random and set it aside, then select a second specimen and set it aside, and
ﬁnally select a third specimen and set it aside. There are N N  1
ð
Þ N  2
ð
Þ possible
outcomes, or permutations, because the collection was sampled without replacement.
The sample space S is some arrangement of three objects, each of which is a permutation.
This is easily generalized to k selections without replacement, yielding for the number of
different ordered samples
qN,k ¼ N N  1
ð
Þ    N  k þ 1
ð
Þ
(1.36)
where qN,k is the permutation of N things taken k at a time. In the limit where k ! N,
qN,N ¼ N!
(1.37)
meaning that there are N! possible permutations of N objects. Consequently, the permuta-
tion of N objects taken k at a time is
qN,k ¼
N!
N  k
ð
Þ!
(1.38)
where 0! 	 1. In MATLAB, the factorial is given by factorial(n). However, even with
double-precision numbers, it is only accurate up to n = 170, and in practice, it is best to use
(1.36) for computation via the prod function.
Example 1.4 For a rock collection, suppose that the probability of drawing k samples with
different identiﬁers is wanted. Presuming that k  N, the number of outcomes is the
number of data vectors such that all k components are different. Since the ﬁrst component
s1 can have N possible values, the second component s2 can have N  1 possible values,
and so on; this is just the permutation of N objects taken k at a time qN,k. The probability
that k different rock specimens will be selected is the number of permutations divided by
the number of possible k vectors in the N-dimensional sample space, or qN,k=Nk. This
example mixes sampling with and without replacement, as is appropriate.
11
1.5 Finite Sample Spaces
.002
13:29:21, subject to the Cambridge Core terms of use,

Sampling with replacement yields a larger sample space than sampling without replace-
ment. As is intuitively obvious, the distinction between sampling with and without
replacement becomes moot as N ! ∞.
1.5.3 Combinations
Consider a set of N distinct objects from which k are to be drawn as a subset, where k  N.
The number of distinct subsets that may be drawn must be determined. In this instance,
ordering of the objects in a subset is irrelevant because no two subsets can consist of the same
elements. Such a subset is called a combination. Let cN,k denote the number of combinations
of N elements taken k at a time. The number of permutations of N elements taken k at a time is
qN,k, and the number of combinations cN,k must be smaller to correct for duplication. Each
combination of k elements has k! permutations, hence qN,k ¼ k!cN,k. Consequently,
cN,k ¼ qN,k
k! ¼
N!
N  k
ð
Þ!k! 	
N
k


(1.39)
where the last term is the binomial coefﬁcient and is read “N things taken k at a time.” In
MATLAB, the binomial coefﬁcient is implemented as nchoosek(n, k).
Example 1.5 Suppose that the geophysics chair wants to form a committee of 6 out of the
25 staff to constitute a cricket team to challenge the physical oceanography department to a
match. How many committee possibilities are there?
The number of groups of six different people who might serve is c25,6 ¼ 177,100. This
is much smaller than the number of possible committees q25,6 ¼ 127,512,000, ignoring the
fact that it is unconventional for a single person to serve in multiple committee slots.
Example 1.6 Suppose that there are 45 glasses of beer on a table, of which 15 are Sam
Adams and 30 are Bud Lite. Ten glasses will be consumed at random by a group of
graduate student volunteers. What is the probability that all 10 glasses will be
Sam Adams?
There are
45
10


possible combinations, each of which has equal probability because
the sampling is random. The number of different combinations in which 10 San Adams can
be selected from the pool of 15 is
15
10


. For Bud Lite, it is
30
0


. Consequently, the
number of paired combinations with 10 Sam Adams and 0 Bud Lite is
15
10


30
0


45
10



 9:4  107
12
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

The probability for a sample with 3 Sam Adams and 7 Bud Lite is
15
3


30
7


45
10



 0:29
This is an example of sampling without replacement because once a specimen is selected
and consumed, it is permanently removed from the sample.
1.6 Probability of a Union of Events
For
two
events
A1
and
A2,
the
general
addition
law
(1.32)
holds
that
Pr A1 [ A2
ð
Þ ¼ Pr A1
ð
Þ þ Pr A2
ð
Þ  Pr A1 \ A2
ð
Þ. Extending the general addition law to
three events A1, A2, and A3 gives
Pr A1 [ A2 [ A3
ð
Þ ¼ Pr A1
ð
Þ þ Pr A2
ð
Þ þ Pr A3
ð
Þ  Pr A1 \ A2
ð
Þ  Pr A2 \ A3
ð
Þ
 Pr A1 \ A3
ð
Þ þ Pr A1 \ A2 \ A3
ð
Þ
(1.40)
This can be proved with a Venn diagram and can be extended to N events:
Pr
[
N
i¼1
Ai
 
!
¼
X
N
i¼1
Pr Ai
ð
Þ 
X
i<j
Pr Ai \ Aj


þ
X
i<j<k
Pr Ai \ Aj \ Ak


   
þ 1
ð
ÞNþ1Pr A1 \    \ AN
ð
Þ
(1.41)
where P
i<j ¼ PN
j¼1
Pj1
i¼1. The terms involving single events are called marginal prob-
abilities, whereas those containing intersections of events are called joint probabilities. In
practice, the latter often are difﬁcult to estimate, and sometimes only an upper bound can
be obtained by assuming that the events are disjoint.
Example 1.7 A lazy TA in the rocks for jocks class is faced with a collection of N rock
samples that need to be sorted into N bins. Being a geophysicist, and hence convinced that
“you’ve seen one rock, you’ve seen ‘em all,” she simply places the specimens in bins at
random, ﬁguring that no one will notice until next year when she won’t be TA any more.
What is the probability that at least one of the rocks will be in the right bin?
Let Ai be the event that the ith rock is placed in the correct bin. The quantity of interest is
the probability that the ﬁrst rock is in the right bin or the second rock is in the right bin and
so on, or the union of all of the possible events
pN ¼ Pr
[
N
i¼1
Ai
 
!
13
1.6 Probability of a Union of Events
.002
13:29:21, subject to the Cambridge Core terms of use,

It follows that the probability that any particular rock will go into the right bin is
Pr Ai
ð
Þ ¼ 1=N. Therefore, the ﬁrst term in (1.41) is PN
i¼1Pr Ai
ð
Þ ¼ N 1=N
ð
Þ ¼ 1. Since
rock 1 could be placed in any bin and rock 2 in any of the remaining N  1 bins, the
probability that both rocks 1 and 2 will be in the correct bin is
Pr A1 \ A2
ð
Þ ¼
1
N N  1
ð
Þ
so the second term in (1.41) is the number of combinations of N things taken two at a time
times this probability
X
i<j
Pr Ai \ Aj


¼
N
2


N N  1
ð
Þ ¼ 1
2!
Applying similar reasoning to the triplet term,
X
i<j<k
Pr Ai \ Aj \ Ak


¼
N
3


N N  1
ð
Þ N  2
ð
Þ ¼ 1
3!
By induction, the result for N terms is
pN ¼ 1  1
2! þ 1
3!     þ 1
ð
ÞNþ1
N!
and is a rapidly converging series. In the limit N ! ∞, this becomes 1  1/e 
 0.632. The
probability that at least one rock will be in the correct bin is not very large! Note that the
probability that all of the rocks are in the correct bins is 1=N!, which is a very small number
for any substantial number of samples.
1.7 Conditional Probability
How does the probability for an event A change if it is known that B has already occurred?
This new probability is written Pr AjB
ð
Þ and is read “probability of A given B.” It is a
conditional probability.
If B is known to have occurred, then the sample space S includes B, and the outcome of
any experiment must contain B. Consequently, the entity of interest is outcomes in B that
also result in the occurrence of A. This is just A \ B. It is natural to deﬁne Pr AjB
ð
Þ as the
proportion of the total probability Pr B
ð Þ that is represented by Pr A \ B
ð
Þ, or
Pr AjB
ð
Þ ¼ Pr A \ B
ð
Þ
Pr B
ð Þ
(1.42)
14
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

Equation (1.42) is not deﬁned if Pr B
ð Þ ¼ 0. Therefore, the conditional probability is the
joint probability of A and B divided by the marginal probability for B. It is also true that
Pr BjA
ð
Þ ¼ Pr A \ B
ð
Þ
Pr A
ð Þ
(1.43)
because the intersection of two events is commutative. Equations (1.42) and (1.43) are
distinct unless Pr A
ð Þ ¼ Pr B
ð Þ.
Example 1.8 Over the past 200 Ma, the geomagnetic ﬁeld has been normal and reversed
with approximately equal frequency (hence probability). However, during the Cretaceous,
there was a 33 Ma interval in which the ﬁeld was normal. Given that a seamount is
normally magnetized, what is the probability that it is Cretaceous in age?
Let the event A be eruption during the Cretaceous quiet interval and the event B be
normal polarity. It is known that Pr B
ð Þ ¼ 0:5 from observations over 200 Ma. If seamount
production
and
preservation
have
been
uniform
over
the
past
200
Ma,
then
Pr A
ð Þ ¼ 33=200 ¼ 0:17. It follows that Pr A \ B
ð
Þ ¼ Pr A
ð Þ because the entire Cretaceous
quiet interval is normal. Then Pr AjB
ð
Þ ¼ 0:17=0:5 ¼ 0:34. The chances are about 33%
that a normally magnetized seamount is Cretaceous in age. In reality, this is an underesti-
mate due to variations in the rate of volcanism through time and the subduction of some
Cretaceous seaﬂoor.
1.8 Independent Events
Suppose that two events occur independently of one another. Their joint probability is just the
product of their marginals, or Pr A \ B
ð
Þ ¼ Pr A
ð ÞPr B
ð Þ. Equivalently, if the events are inde-
pendent, the conditional probability Pr AjB
ð
Þ ¼ Pr A
ð Þ because the prior occurrence of B is
irrelevant. This yields Pr A \ B
ð
Þ ¼ Pr A
ð ÞPr B
ð Þ from the deﬁnition of conditional probability.
Independence of two events is deﬁned by Pr A \ B
ð
Þ ¼ Pr A
ð ÞPr B
ð Þ. If independence is
known on physical grounds, such as for radioactive decay of an isotope in a sample where
it is at a low concentration so that one radioactive decay does not inﬂuence another, then
Pr A \ B
ð
Þ can easily be estimated.
Example 1.9 A sample of water from a hydrothermal vent ﬁeld contains 222Rn and 40K,
both of which undergo radioactive decay at known rates. Given this information, plus the
concentration of Rn and K in the samples, plus the relative concentration of the two
isotopes of Rn and K in earth, the probabilities for decay of 222Rn and 40K in a given unit
15
1.8 Independent Events
.002
13:29:21, subject to the Cambridge Core terms of use,

of time are 0.93 and 0.75. What is the probability of observing either a 222Rn or a
40K decay in the same time interval?
Let the events A and B be 222Rn and 40K decay, respectively. It has been speciﬁed that
Pr A
ð Þ ¼ 0:93 and Pr B
ð Þ ¼ 0:75. Since the two events are independent, Pr A \ B
ð
Þ ¼
Pr A
ð ÞPr B
ð Þ ¼ 0:70. From the general law of addition (1.32), Pr A [ B
ð
Þ ¼ 0:93þ
0:75  0:70 ¼ 0:98.
The concept of independence can be extended to more than two events. For example, for
three events to be independent, Pr Ai \ Aj


¼ Pr Ai
ð
ÞPr Aj


and Pr Ai \ Aj \ Ak


¼
Pr Ai
ð
Þ Pr Aj


Pr Ak
ð
Þ for i, j, k = 1, ..., 3.
Example 1.10 An inventor has built a system to automatically pick foraminifera from a
sediment sample and sort them according to species. Suppose that the system produces a
false pick with probability p and a correct pick with probability q ¼ 1  p. Further suppose
that 10 picked forams selected at random are tested and that these tests are independent.
What is the probability that two of the 10 forams were picked correctly?
The sample space contains all possible arrangements of 10 forams, whether correctly
or incorrectly picked. For j = 1, ..., 10, let Ij be the event that the jth item is incorrectly
picked and Cj that it is correctly picked. Because of independence, Pr I1 \ C1 \ I2
ð
\C2 \    \ I10 \ C10Þ ¼ Pr I 1
ð
Þ    Pr C10
ð
Þ ¼ p2q8. No matter in what order they occur,
the probability that two picks are incorrect is p2q8. Hence the probability that there will be
two false picks in 10 samples is p2q8 times the number of combinations producing them, or
10
2


p2q8. For a 1% error rate, p = 0.01 and q = 0.99, and the probability of getting two
false picks is 0.00415, or about 0.4%.
1.9 Bayes’ Theorem
Let Ai
f
g be a set of k disjoint events that span the sample space S. Let B be another event
in the same sample space. The conditional probability of the jth disjoint event given
B follows from its deﬁnition (1.42)
Pr AjjB


¼ Pr Aj \ B


Pr B
ð Þ
¼
Pr Aj


Pr BjAj


P
k
i¼1
Pr Aj


Pr BjAj


(1.44)
The numerator on the right is just the joint probability expressed in terms of the conditional
probability. The denominator follows from the law of total probability (1.35). The result is
16
Probability Concepts
.002
13:29:21, subject to the Cambridge Core terms of use,

called Bayes’ theorem. It gives a simple rule for computing the conditional probabilities of
each event Aj given B from the conditional probabilities of B given Aj and the uncondi-
tional probabilities for Aj. It also leads to the concepts of prior and posterior probabilities
that lie at the heart of Bayesian statistics.
Example 1.11 Suppose that a test exists for a serious disease. The known statistics for the
test are that if one has the disease, the test will give a positive result 99.99% of the time, but
if one does not have the disease, there is a 0.01% chance of a positive result. The chance of
a given member of the population having the disease is about 1 in 1000 based on national
incidence. An individual takes the test and learns that he got a positive test result. What is
the probability that he has the disease?
Let A be the event that the individual has the disease and B be the event that the test
result is positive. Using Bayes’ theorem,
Pr AjB
ð
Þ ¼
Pr BjA
ð
ÞPr A
ð Þ
Pr BjA
ð
ÞPr A
ð Þ þ Pr BjAC


Pr AC


¼
0:9999  0:001
0:9999  0:001 þ 0:0001  0:999 ¼ 0:909
This is substantially less than the reliability of the test. If the incidence of the disease is
lower, the probability of the test being right drops a lot. For instance, if the chance of
having the disease is 1/10,000 or 1/100,000, the probability drops to 0.5 and 0.091,
respectively. This example illustrates why it makes little sense to test a large segment of
the population for a rare disease unless the accuracy of the test is very high.
17
1.9 Bayes’ Theorem
.002
13:29:21, subject to the Cambridge Core terms of use,

2
Statistical Concepts
2.1 Overview
This chapter builds on the probability concepts of Chapter 1 to construct the theoretical
foundation for computational data analysis. The key ideas that are introduced include the
probability density function (pdf) for discrete, continuous, and mixed distributions; the
cumulative distribution function (cdf), which is the sum or integral of the pdf; the quantile
function, which is the inverse of the cdf; and the characteristic function, which serves as an
alternate pathway for computing the pdf and cdf. A discussion of the bivariate distribution
extends the prior univariate descriptions to two variables, with the full multivariate case
deferred until Chapters 10 and 11 (although there will be some slight cheating in Chap-
ter 3), and to independence of random variables (rvs). The formalism that enables trans-
formation from one or more variables to another set (e.g., Cartesian to circular coordinates)
is then described, and the distribution of the largest and smallest of a set of random
variables is derived as an introduction to the order statistics that are covered in Chapter 4.
A number of theoretical entities for location (e.g., the expected value), dispersion (e.g., the
variance), shape (e.g., the skewness), direction (e.g., the mean direction), and the covar-
iance between two rvs is described as a counterpart to the sample entities that are presented
in Chapter 4. The concept of conditional probability from Chapter 1 is extended to the
expected value and variance, leading to the laws of total expectation, variance, and
covariance. Finally, the chapter closes by extending the concept of inequality to stochastic
variables, leading to convergence relations for rvs that pervade the remainder of the book.
2.2 The Probability Density Function
2.2.1 Discrete Distributions
A distribution may be discrete (i.e., it exists only for speciﬁc values on the real line),
continuous (i.e., it exists for any value on the real line between a given set of bounds
that may include ∞), or a mixture of continuous and discrete. An rv X is discrete
or, equivalently, X has a discrete distribution if X can take only a countably ﬁnite
number k of different values or at most an inﬁnite sequence of distinct values (such as
all of the integers). It may not take on all possible values in an interval and remain
discrete.
18
.003
13:30:02, subject to the Cambridge Core terms of use,

The probability function or probability mass function or probability density function or
pdf of a discrete rv X is deﬁned as the function f x
ð Þ that assigns a real number x to each
value of X with the following two properties:
f x
ð Þ ¼ Pr X ¼ x
ð
Þ
x ¼ xi, i ¼ 1, :::, N
(2.1)
f x
ð Þ ¼ 0
x 6¼ xi, i ¼ 1, :::, N
(2.2)
By the Kolmogrov axioms, PN
i¼1 f xi
ð Þ ¼ 1, where it is understood that N can approach
inﬁnity in (2.1) and (2.2).
A discrete probability function is shown in Figure 2.1, where the sum of all the values is
1. A similar plot could be obtained using disttool in MATLAB, which allows one to plot
the pdf of a wide range of discrete and continuous distributions and is a useful tool for the
reader to become familiar with some standard distributions.
For X 2 A, where A is a subset of the possible values of x, the corresponding probability
is obtained by summing all values of f x
ð Þ lying in A. Mathematically, this is written
Pr X 2 A
ð
Þ ¼
X
xi2A
f xi
ð Þ
(2.3)
As an example, suppose the rv X is equally likely to take on any of the integer values
between 1 and N. The probability density function of X is
f x
ð Þ ¼ 1
N
x ¼ 1, :::, N
This represents the outcome of an experiment that results in one of the integers on the
closed interval 1; N
½
 being chosen at random. The uniform distribution on integers only
exists for a ﬁnite sequence of integers and cannot be extended to all the integers. In
MATLAB, this is unidpdf(x, n).
An alternate way to represent a discrete pdf uses a weighted sequence of Dirac delta
functions. The Dirac delta function δ x  x0
ð
Þ is a generalized function deﬁned on the
real line whose value is zero everywhere except when its argument is zero, where it is
inﬁnite. Rigorous derivation requires the use of measure theory, but a working deﬁn-
ition follows from
0
0.05
0.1
0.15
0.2
0.25
0.3
0
2
4
6
8
Figure 2.1
The binomial distribution for p = 0.5 and eight trials.
19
2.2 The Probability Density Function
.003
13:30:02, subject to the Cambridge Core terms of use,

ð∞
∞
g x
ð Þδ x  x0
ð
Þdx ¼ g x0
ð Þ
As a consequence, a discrete pdf can be represented using a comb of Dirac delta functions
f x
ð Þ ¼
X
N
i¼1
ai δ x  xi
ð
Þ
(2.4)
where PN
i¼1ai ¼ 1. Computation of probabilities obtains by integration of (2.4). For
example, let x1  a < b  xN. Then the probability on the interval a; b
½
 is
ðb
a
f x
ð Þ dx ¼
X
xi2 a;b
½

ai
(2.5)
The biggest advantage of the Dirac comb representation of a discrete pdf is that mathemat-
ical manipulation of it is identical to that for continuous distributions, so mixed distribu-
tions can be handled easily if the discrete part is represented by (2.4).
2.2.2 Continuous Distributions
A rv X has a continuous distribution if there exists a function f x
ð Þ  0 deﬁned on the real
line such that, for every subset A, the probability that X takes a value in A is the integral of
f over the set A. Mathematically, this is
Pr X 2 A
ð
Þ ¼
ð
x2A
f x
ð Þdx
(2.6)
where the Kolmogorov axioms require that
Ð
S f x
ð Þdx ¼ 1. The bounds on the last integral
cover all allowed values of x (or the sample space S ). The function f is the pdf of X.
It does not have dimensions of probability; rather, its integral over some part of the real line
gives that quantity (see Figure 2.2). For example, the probability that X lies between a and b is
Pr a < x  b
ð
Þ ¼
ðb
a
f x
ð Þ dx
(2.7)
This probability is also given by Pr a < x < b
ð
Þ or Pr a  x  b
ð
Þ or Pr a  x < b
ð
Þ, unlike
for a discrete variable where inclusion of the endpoints of an interval is required to
correctly estimate the probability.
It immediately follows that Pr X ¼ c
ð
Þ ¼ 0 for any c 2 S, or else
Ð
S f x
ð Þdx ! ∞. This
does not imply that X ¼ c is an impossible abscissa value because if it were, then X could
not take on any value. Rather, it is necessary to integrate f x
ð Þ over a ﬁnite, but not
inﬁnitesimal, range to get the probability. As a consequence, the pdf is not unique; it can
change at a ﬁnite or inﬁnite number of points without changing the integral over some
20
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

range and hence altering the probability. This generally requires introducing discontinuities
into an otherwise continuous pdf. The natural choice is the continuous version, although
that is not a mathematical requirement.
Example 2.1 Let a and b be real numbers with a < b, and consider an experiment in which
X is selected from the interval a; b
ð
in such a way that the probability is proportional to the
interval length b  a. The pdf is
f x
ð Þ ¼
1
b  a
a < x  b
and zero otherwise. Then Pr c < X  d
ð
Þ ¼ d  c
ð
Þ= b  a
ð
Þ if a  c  d  b, and
Pr c < X  d
ð
Þ ¼ d  a
ð
Þ= b  a
ð
Þ if c  a  d  b, and so on. Pr X  d
ð
Þ ¼ b  d
ð
Þ=
b  a
ð
Þ if a  d  b, 1 if d  a, and 0 if d  b. In MATLAB, the uniform continuous
distribution on the interval
a; b
ð
at the values in the vector x is given by unifpdf
(x, a, b).
Example 2.2 The quantization noise from an analog-to-digital converter is uniformly
distributed because while the voltage it measures is continuous, the converter has a
minimum resolution of the least signiﬁcant bit, and hence the uncertainty of the real
voltage is uniformly distributed over ½ to ½ bit.
It is possible for a pdf to exist over an unbounded interval of the real line as long as integrals
over parts of the real line give ﬁnite probability and the integral over the entire interval is 1. It
is also possible for f x
ð Þ to be unbounded at some point as long as its integral remains ﬁnite.
Example 2.3 Let f x
ð Þ ¼ 2x1=3=3 on 0; 1Þ
½
. This is unbounded at the origin, but the integral
over its support is unity, and the integral over any part of 0; 1Þ
½
is bounded.
A
Pr(A)
x
f(x)
Figure 2.2
The probability density function for a continuous distribution, showing that the probability over a subset
of the real axis is obtained from the area under the pdf.
21
2.2 The Probability Density Function
.003
13:30:02, subject to the Cambridge Core terms of use,

2.2.3 Mixed Distributions
Mixed distributions for variables that may be both continuous and discrete also exist. An
example of a physical process with a mixed distribution is pressure ﬂuctuations on the
seaﬂoor, which not only has a continuous power spectrum (that is analogous to the pdf with
frequency or period as the abscissa) caused by processes such as stochastic wind forcing,
but also has superimposed ocean tides that exist only at a countably ﬁnite set of frequencies
and are zero elsewhere.
Continuous and discrete variables may be treated simultaneously by replacing the
familiar Riemann integral
Ð b
a f x
ð Þdx with the Riemann-Stieltjes integral
Ð b
a g x
ð ÞdH x
ð Þ,
where g x
ð Þ and H x
ð Þ are real-valued functions (or alternately, by going to the
general deﬁnition of an integral under the Riemann-Lebesque theorem; see Doob 1993
or Carter & van Brunt 2000). Specifying the partition of the interval
a; b
½
 to be
a ¼ x0 < x1 <    < xn ¼ b, the Riemann-Stieltjes integral is deﬁned to be the limit as
the interval approaches zero of
ðb
a
g x
ð ÞdH x
ð Þ 
X
n1
i¼0
g yi
ð Þ H xiþ1
ð
Þ  H xi
ð Þ
½

(2.8)
where yi is a value (e.g., the midpoint) on the subinterval
xi; xiþ1
½
. If H x
ð Þ is
differentiable with derivative h x
ð Þ [formally, if H x
ð Þ has a pdf h x
ð Þ under Lebesgue
measure], then
ðb
a
g x
ð ÞdH x
ð Þ ¼
ðb
a
g x
ð Þh x
ð Þdx
(2.9)
If h x
ð Þ ¼ 1, meaning that H x
ð Þ ¼ x, then the Riemann integral is recovered. If H x
ð Þ
contains a set of step function discontinuities, then h x
ð Þ will include a sequence of delta
functions, and the integral becomes the sum of a continuous part and a discrete part,
yielding a mixture distribution. Riemann-Stieltjes notation is used in many textbooks on
advanced statistics, such as the classics by Stuart & Ord (1994) and Stuart, Ord, & Arnold
(1999).
Mixed distributions can occur in practical problems. Suppose that X and Y are the
times when earthquakes occur on two subparallel faults, that p is the probability
that earthquakes occur on both faults at the same time, and hence 1  p is the
probability that earthquakes do not occur simultaneously. If the earthquakes occur at
the same time, then the pdf for x is f x x
ð Þ, while if the faults fail at different times,
their bivariate pdf (see Section 2.5) is f x; y
ð
Þ. It immediately follows that f x; y
ð
Þ
cannot be continuous because the probability that the paired rvs (X, Y) lie on the
line x = y is p and not 0, as would be required for a continuous distribution. The
bivariate pdf becomes continuous in the limit p ! 0 and hence is a mixture
distribution for nonzero p.
22
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

2.3 The Cumulative Distribution and Quantile Functions
The distribution function or cumulative distribution function or cdf is deﬁned as
F x
ð Þ ¼ Pr X  x
ð
Þ
(2.10)
This deﬁnition applies to continuous, discrete, or mixed distributions. The cdf has three
key properties:
1. F x
ð Þ is nondecreasing as x increases: if x1 < x2, then F x2
ð
Þ  F x1
ð
Þ;
2. lim
x!∞F x
ð Þ ¼ 0 and lim
x!∞F x
ð Þ ¼ 1; and
3. F x
ð Þ is always continuous from the right: F x
ð Þ ¼ F xþ
ð
Þ at all points x, where
F xþ
ð
Þ ¼ lim
y!x8y>x F y
ð Þ.
A cdf need not be continuous, and property 3 guarantees that probability is always deﬁned
even in the presence of discontinuities in F x
ð Þ. It follows that continuity of a cdf requires
that F x
ð Þ ¼ F x
ð
Þ ¼ F xþ
ð
Þ, where F x
ð
Þ ¼
lim
y!x8y<x F y
ð Þ.
Probabilities may be determined directly from a known cdf. There are four types of
intervals to consider
Pr X > x
ð
Þ ¼ 1  Pr X  x
ð
Þ ¼ 1  F x
ð Þ
(2.11)
This is called the complementary cumulative distribution function or tail distribution or
survivor function. In addition,
Pr x1 < X  x2
ð
Þ ¼ F x2
ð
Þ  F x1
ð
Þ
(2.12)
Pr X < x
ð
Þ ¼ F x
ð
Þ
(2.13)
Pr X ¼ x
ð
Þ ¼ F xþ
ð
Þ  F x
ð
Þ
(2.14)
where (2.14) is zero for a continuous distribution.
For a discrete distribution, F x
ð Þ will have a jump of magnitude Pr X ¼ xi
ð
Þ ¼ f xi
ð Þ
at each point where f x
ð Þ exists. A discrete rv can be represented equally well by a cdf
or a pdf. Figure 2.3 shows the cdf for the same discrete distribution as in Figure 2.1.
00
0.2
0.4
0.6
0.8
1
2
4
6
8
Figure 2.3
The cdf for the binomial distribution with p = 0.5 and eight trials.
23
2.3 The Cumulative Distribution and Quantile Functions
.003
13:30:02, subject to the Cambridge Core terms of use,

Note the stairstep appearance, with discrete jumps in probability occurring at integer
intervals.
For a discrete distribution, an alternate representation of the cdf uses indicator functions.
An indicator function takes only the values 0 and 1 and is 1 when its argument x 2 A and
0 when x =2 A, where A is an event of interest. For example, A could consist of all the
values on the real line where a discrete distribution is nonzero. Denote the indicator
function by 1A xi
ð Þ. The cdf is given by
F x
ð Þ ¼
X
xi2 ∞;x
ð

ai1A xi
ð Þ
(2.15)
where Pai ¼ 1.
For a continuous distribution, the cdf may be obtained by integration of the pdf
F x
ð Þ ¼
ðx
∞
f tð Þdt
(2.16)
f x
ð Þ ¼ ∂xF x
ð Þ
(2.17)
As for a discrete rv, a continuous rv can be represented equally well by a cdf or
a pdf.
It is sometimes useful to turn a statistical problem around so that instead of asking
for Pr X  x
ð
Þ, a probability level is chosen and used to determine the corresponding
value for the distribution abscissa. This can be accomplished by searching through the
values of F x
ð Þ until the probability level of interest is found (and that is exactly what
was done by eye in the days of statistical tables that are now gathering dust in
university libraries). However, if F x
ð Þ is continuous and uniquely invertible, F1 p
ð Þ
can be deﬁned such that
x ¼ F1 p
ð Þ
(2.18)
F1 p
ð Þ is called the inverse cumulative distribution or quantile function. Some properties
of the quantile function include
1. F1 p
ð Þ is a nondecreasing function of p;
2. F1 F x
ð Þ
½
  x; and
3. F F1 p
ð Þ


 p.
Like the cdf, the quantile function depends only on the underlying probability distribu-
tion. Any two rvs with the same distribution will have the same quantile function. With
some adjustment, the concept of quantiles extends to discrete as well as continuous
distributions.
For a given probability p, F1 p
ð Þ given by (2.18) is called the p-quantile or 100p
percentile of x. If x is the 100p percentile, then 100p percent of the distribution is at or
below x. The 0.5 quantile or 50th percentile is called the median. The 0.25 quantile or 25th
percentile is called the lower quartile. The 0.75 quantile or 75th percentile is called the
upper quartile.
24
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

Example 2.4 Suppose F x
ð Þ ¼ ex with support ∞; 0
ð
. The quantile function is obtained by
solving p ¼ F x
ð Þ for x, yielding F1 p
ð Þ ¼ log p. The median is 0.6931. The lower and
upper quartiles are 1.3863 and 0.2877, respectively.
The concepts in this section extend to rvs that represent directions on a circle (and can
easily be extended to a sphere). Two standard references on directional distributions are
Fisher (1995) and Mardia & Jupp (2000).
The cdf for rvs comprising random angles
θi
f g is deﬁned by
F x
ð Þ ¼ Pr 0 < θ  x
ð
Þ
0  x  2π
(2.19)
where
F x þ 2π
ð
Þ  F x
ð Þ ¼ 0
 ∞< x < ∞
(2.20)
Equation (2.19) depends on a reference direction that simply adds a constant to F x
ð Þ.
Equation (2.20) speciﬁes that any set of angles with a length 2π has a probability of
1 because x is deﬁned modulo 2π. The probability for an interval modulo 2π is given by
(2.12), but the second property at the beginning of Section 2.3 must be replaced by F 0
ð Þ ¼
0 and F 2π
ð
Þ ¼ 1. Presuming that F x
ð Þ is continuous, the pdf is deﬁned as in Section 2.2.2,
although the support becomes 0; 2π
ð
Þ, and the pdf is modulo 2π.
2.4 The Characteristic Function
An alternate way to specify a probability distribution of an rv X is through the complex-
valued characteristic function or cf given by the inverse Fourier transform of the pdf
ϕ tð Þ ¼
ð∞
∞
eitxf x
ð Þdx
(2.21)
or, conversely, if the cf is speciﬁed, the pdf is the dual given by
f x
ð Þ ¼ 1
2π
ð∞
∞
ϕ tð Þeitxdt
(2.22)
where the integral is the Cauchy principal value. The cf converges both absolutely and
uniformly in t. Consequently, it may be integrated or differentiated behind the integral sign.
Further, the cf of a real-valued rv always exists because it is the integral of a bounded
continuous function of ﬁnite measure. The cdf is given by
F x
ð Þ  F 0
ð Þ ¼ 1
2π
ð∞
0
 1  eitx
it
ϕ tð Þdt
(2.23)
25
2.4 The Characteristic Function
.003
13:30:02, subject to the Cambridge Core terms of use,

While in the past the cf was used as a tool to compute the moments of a distribution, in
the modern world it is important because there are classes of probability distributions for
which closed-form expressions for the pdf do not exist, but simple expressions for the cf
are available. The most important of these are the stable distributions that will be intro-
duced in Section 3.4.2.
Some key properties of the characteristic function include
1. ϕ 0
ð Þ ¼ 1 follows from the requirement that the integral of the pdf over its support be 1;
2. ϕ tð Þ
j
j ¼
Ð ∞
∞eitxf x
ð Þdx

 
Ð ∞
∞eitx
j
jf x
ð Þdx 
Ð ∞
∞f x
ð Þdx ¼ 1;
3. ϕ tð Þ
j
j ! 0 as tj j ! ∞implies that the pdf is continuous;
4. ϕ tð Þ
j
j !
= 0 as tj j ! ∞implies that the pdf is discrete;
5. ϕ t
ð
Þ ¼ ϕ tð Þ∗(Hermitian property, where the superscript * denotes the complex
conjugate), so for a real rv that is symmetric about the origin, the cf is real-valued
and even;
6. ϕ x þ y
ð
Þ ¼ ϕ x
ð Þϕ y
ð Þ when x and y are independent; and
7. ϕ tð Þ ¼ Ð 1
0 eitF1 p
ð Þdp, where F1 p
ð Þ is the quantile function.
The real part of the cf is even, and the imaginary part is odd. As a consequence, (2.22) can
be rewritten as
f x
ð Þ ¼ 1
π
ð∞
0
Re ϕ tð Þ
½
 cos xt
ð
Þdt þ 1
π
ð∞
0
Im ϕ tð Þ
½
 sin xt
ð
Þdt
(2.24)
which is often useful for numerical integration purposes.
Example 2.5 The Gaussian pdf is e xμ
ð
Þ2= 2σ2
ð
Þ=
ﬃﬃﬃﬃﬃ
2π
p
σ


. Derive its cf.
ϕ tð Þ ¼
1ﬃﬃﬃﬃﬃ
2π
p
σ
ð∞
∞
eitxe xμ
ð
Þ2= 2σ2
ð
Þ ¼ eitμt2σ2=2
using Gradshteyn & Ryzhik (1980, 3.323-2). This becomes real when μ ¼ 0 so that the
distribution is symmetric about the origin, and properties 13 and 5 hold.
For directional rvs, the requirement that the angle θ is deﬁned modulo 2π means that the
variable t in the cf must take on only integer values. Consequently, the directional cf is
given by
ϕk ¼
ð2π
0
eikθf θ
ð Þdθ
k ¼ 0,  1,  2, :::
(2.25)
for a continuous directional rv. It is obvious that ϕ0 ¼ 0 and ϕk ¼ ϕ∗
k . The complex
sequence given by (2.25) comprises the coefﬁcients in the Fourier expansion for F θ
ð Þ, and
presuming that it is square integrable, the pdf is given by
26
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

f θ
ð Þ ¼ 1
2π
X
∞
k¼∞
ϕkeikθ
(2.26)
which is the directional analog to (2.22).
2.5 Bivariate Distributions
The bivariate distribution is the joint distribution of two variables and can be discrete,
continuous, or mixed. The joint pdf is deﬁned by
f x; y
ð
Þ ¼ Pr X ¼ x \ Y ¼ y
ð
Þ
(2.27)
Where f x; y
ð
Þ  0 and ∬f x; y
ð
Þdxdy ¼ 1 or P
i
P
jf
xi; yj

	
¼ 1 (for continuous and
discrete distributions, respectively), and the integration or summation takes place over
the support of x and y.
The remaining probability concepts for bivariate variables are straightforward. For any
subset A in the xy plane
Pr X; Y
ð
Þ 2 A
½
 ¼
∬
x, y2A
f x; y
ð
Þdxdy
(2.28)
Probabilities may be computed by integrating the pdf over the relevant part of the real
plane. For a discrete variable, probabilities can be arranged in table form, where each entry
is the probability for an ordered pair of values
xi; yj

	
.
Example 2.6 An oil company wishes to take a statistical approach to ﬁnding new producing
wells. For its existing wells, the company determines how many producing wells were
drilled into formations of a given age. Let the rvs X and Y be formation type and geologic
age, respectively. Table 2.1 gives the results.
Table 2.1 shows the joint probabilities; summing rows or columns gives the marginal
probabilities. For example, summing the ﬁrst row gives the probability for Permian
formations of all facies types, while summing the ﬁrst column gives the probability for
Table 2.1 Probability of Finding Oil as a Function of Formation Type and Age
Age
Sandstone
Limestone
Shale
Conglomerate
Y-Marginal
Permian
0.15
0.10
0.05
0
0.30
Triassic
0.10
0.08
0.02
0
0.20
Jurassic
0.20
0
0
0
0.20
Cretaceous
0.05
0.12
0.03
0
0.20
Tertiary
0
0
0.10
0
0.10
X-marginal
0.50
0.30
0.20
0
1
27
2.5 Bivariate Distributions
.003
13:30:02, subject to the Cambridge Core terms of use,

sandstone of any age. From the table it is clear that (1) conglomerates do not bear
hydrocarbons, (2) sandstone offers the best probability for striking oil, (3) Tertiary drilling
should be in shales, and (4) Permian formations are the best bet overall.
The bivariate cdf is deﬁned by
F x; y
ð
Þ ¼ Pr X  x \ Y  y
ð
Þ
(2.29)
and can be expanded for particular choices of intervals such as
Pr a  X  b \ c  Y  d
ð
Þ ¼ Pr a  X  b \ Y  d
ð
Þ  Pr a  X  b \ Y  c
ð
Þ
¼ Pr X  b \ Y  d
ð
Þ  Pr X  a \ Y  d
ð
Þ
½

 Pr X  b \ Y  c
ð
Þ  Pr X  a \ Y  c
ð
Þ
½

¼ F b; d
ð
Þ  F a; d
ð
Þ  F b; c
ð
Þ þ F a; c
ð
Þ
(2.30)
The cdf of X alone (or the marginal distribution of X) can be obtained from the bivariate
cdf by taking an appropriate limit
F1 x
ð Þ ¼ Pr X  x
ð
Þ ¼ lim
y!∞Pr X  x \ Y  y
ð
Þ ¼ lim
y!∞F x; y
ð
Þ
(2.31)
Note that the marginal cdf for either x or y can be derived from the joint distribution, but the
reverse is not true without additional information about the relationship between x and y.
The marginal pdf of X can be obtained from the bivariate pdf by integration
f 1 x
ð Þ ¼ Pr X ¼ x
ð
Þ ¼
ð
y2S
f x; y
ð
Þdy
(2.32)
where the integral is over all possible values for y. The marginal pdf for eitherx or y can be
derived by integration of the joint pdf, but the reverse is not true without additional
information about the covariance of x and y.
Example 2.7 Let f x; y
ð
Þ ¼ e xþy
ð
Þ with support 0; ∞Þ
½
. The marginal distribution for x is
f x
ð Þ ¼ ex Ð ∞
0 eydy ¼ ex.
The concept of a bivariate distribution extends to many variables as multivariate distribu-
tions and will be covered in Chapters 10 and 11.
2.6 Independent and Exchangeable Random Variables
Expanding on the concepts introduced in Section 1.8, if X and Y are independent rvs, then
their probabilities factor
28
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

Pr X  x \ Y  y
ð
Þ ¼ Pr X  x
ð
ÞPr Y  y
ð
Þ
(2.33)
The reverse is also true: if their probabilities factor, then a pair of variables is independent.
More precisely, X and Y are independent if and only if F x; y
ð
Þ ¼ F1 x
ð ÞF2 y
ð Þ or
f x; y
ð
Þ ¼ f 1 x
ð Þf 2 y
ð Þ. If the joint distribution is known and can be expressed as the product
of the marginal distributions for two variables, then the variables are independent.
For a discrete variable where X can take on the values xi
f g, i ¼ 1, :::, r, and Y can take
on the values
yj
n o
, j ¼ 1, :::, s, Pr X ¼ xi \ Y ¼ yj

	
¼ pij. Let Pr X ¼ xi
ð
Þ ¼ Ps
j¼1
pij ¼ piþ and PrðY ¼ yjÞ ¼ Pr
i¼1 pij ¼ pþj. Then X and Y are independent if and only
if pij ¼ piþpþj.
Example 2.8 Returning to Example 2.6 and Table 2.1, p11 ¼ 0:15, p1þ ¼ 0:30, and
pþ1 ¼ 0:50, and hence sandstone and Permian are independent. However, p12 ¼ 0:10,
p1þ ¼ 0:30, and pþ2 ¼ 0:30, and hence p12 6¼ p1þpþ2. Limestone and Permian are not
independent.
The concept of independence extends to N rvs. If an N-variate set of rvs
X1; :::; XN
f
g is
independent, and each has the same marginal distribution, then X1; :::; XN
f
g are independ-
ent and identically distributed or iid. If F and f are, respectively, the cdf and pdf for the rvs,
then X1, :::, XN 	 F and X1, :::, XN 	 f , where the symbol ~ means “is distributed as” and
not “is approximately.” An equivalent statement is that X1; :::; XN
f
g is a random sample of
size N from F or f .
A closely related principle that underlies the nonparametric and resampling methods that
will be covered in Chapters 7 and 8 is exchangeability. A set of rvs
X1; :::; XN
f
g (either
ﬁnite or inﬁnite in length) is exchangeable if any ﬁnite permutation of their indices (i.e., the
permutation operates on a ﬁnite set of indices with the remainder ﬁxed) results in no
change to their joint probability distribution. A set of iid rvs is always exchangeable, as is a
set of jointly Gaussian rvs with identical covariances. Exchangeability is a weaker
assumption than independence; for example, if rvs are drawn from a ﬁnite population
without replacement, they are clearly dependent but remain exchangeable.
A simple example of a set of data that are exchangeable but not independent
follows from considering the empirical deviates from the grand mean for two
data sets. Let
Xi
f
g and
Yi
f
g comprise N1 and N 2 rvs, respectively, and let
G ¼
PN1
i¼1Xi þ PN2
i¼1Yi

	
= N1 þ N 2
ð
Þ
be
their
grand
mean.
The
new
rvs
X0
i ¼ Xi  G


and
Y0
i ¼ Yi  G


are exchangeable but are dependent due to the grand
mean term.
A simple transformation can often be applied to make a set of rvs exchangeable.
Suppose that a set of rvs
X1; :::; XN
f
g has a population mean μ and a distribution
F x  μ
ð
Þ and an independent set of rvs
Y1; :::; YM
f
g has a population mean λ and
a distribution F y  λ
ð
Þ. The transformed independent variables X0
i ¼ Xi  μ and
Y0
i ¼ Yi  λ are exchangeable.
29
2.6 Independent and Exchangeable Random Variables
.003
13:30:02, subject to the Cambridge Core terms of use,

2.7 Conditional Probability Distributions
The conditional distribution is a generalization of the concept of conditional probability
introduced in Section 1.7. Conditional distributions describe the probabilities for events
determined by rvs conditional on the occurrence of other events described by other rvs. In
other words, the probabilities for some rvs may change once other rvs are observed.
Following on the deﬁnition of conditional probability in Section 1.7,
Pr XjY ¼ y
ð
Þ ¼ Pr X \ Y ¼ y
ð
Þ
Pr Y ¼ y
ð
Þ
(2.34)
presuming that Pr Y ¼ y
ð
Þ is nonzero. Equation (2.34) is called the conditional probability
density function of X given that Y ¼ y. The rule to remember is that conditional distribu-
tion equals joint distribution divided by marginal distribution, just as it is for events.
There are two nonequivalent conditional pdfs for a speciﬁed joint distribution, given by
g1 xjy
ð
Þ ¼ f x; y
ð
Þ
f 2 y
ð Þ
(2.35)
g2 yjx
ð
Þ ¼ f x; y
ð
Þ
f 1 x
ð Þ
(2.36)
Equation (2.35) depends on all possible values of X for a given value of Y, and equation
(2.36) depends on all possible values of Y for a given value of X. The sum or integral
(depending on whether the distribution is discrete or continuous) over all possible values of
x for g1 and y for g2 must be 1, which follows immediately from the deﬁnition.
If the joint distribution is known, the marginal and conditional distributions can be
computed by summation or integration. Further, if the marginal and conditional distributions
are known, the joint distribution can be recovered. It is often easier to obtain the conditional
distribution, so this is sometimes a useful pathway to compute a joint distribution.
Since f x; y
ð
Þ ¼ g1ðx j yÞ f 2 y
ð Þ ¼ g2ðy j xÞf 1 x
ð Þ from (2.35) and (2.36), a generalization
of Bayes’ theorem to distributions is given by
g1 x j y
ð
Þ ¼ g2ðy j xÞf 1 x
ð Þ
f 2 y
ð Þ
g2 y j x
ð
Þ ¼ g1ðx j yÞf 2 y
ð Þ
f 1 x
ð Þ
(2.37)
From the deﬁnition of independence, f x; y
ð
Þ ¼ f 1 x
ð Þf 2 y
ð Þ. It immediately follows that x
and y are independent if and only if g1ðx j yÞ ¼ f 1 x
ð Þ and g2ðy j xÞ ¼ f 2 y
ð Þ.
Example 2.9 A simple model of a crystalline rock is an aggregation of spherical crystals of
varying radius, with f rð Þ being the pdf for the radius r. Suppose that the rock is sliced to make a
thin section, yielding a two-dimensional cross section of the three-dimensional rock in which
the spherical crystals appear as circles of varying radius. Letthepdf of the circles beg ρ
ð Þ, where
ρ is the radius of the circles (see Figure 2.4). What is the relationship between f rð Þ and g ρ
ð Þ?
30
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

This problem can be solved by assuming that the orientation of the cross sections is
random in space and ﬁnding the conditional distribution h ρjr
ð
Þ. For a given value of the rv
R ¼ r, let X be the distance from the center of a sphere to the center of the corresponding
circle (see Figure 2.4). Since the sampling is assumed to be random, it follows that X is
uniformly distributed on 0; r
½
 and that X ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2  P2
p
. The conditional cdf is
Hðρ j rÞ ¼ Pr Ρ  ρjR ¼ r
ð
Þ
¼ Pr
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2  X2
p
 ρ

	
¼ Pr X 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2  ρ2
p

	
¼ 1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2  ρ2
p
r
The radius ρ cannot exceed r, ensuring that the cdf is bounded by 0 and 1. The conditional
pdf follows by differentiation:
h ρ j r
ð
Þ ¼
ρ
r
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2  ρ2
p
The law of total probability (Section 1.4) states that
g ρ
ð Þ ¼
ð
hðρ j rÞ f rð Þdr
where the integral is taken over all possible values of the radius of the spheres.
Consequently,
g ρ
ð Þ ¼
ð∞
0
h ρ j r
ð
Þ f rð Þdr ¼
ð∞
ρ
ρ
r
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2  ρ2
p
f rð Þdr
because r cannot be smaller than ρ. This is an Abel integral equation that occurs frequently in
image analysis. In practice, one can estimate g ρ
ð Þ from measurements on a series of randomly
orientedthinsectionsandthenestimatef rð Þ byanapproximateinversionoftheintegralequation.
Figure 2.4
A planar slice of a sphere of radius r at a distance x from its center, producing a circle of radius ρ.
31
2.7 Conditional Probability Distributions
.003
13:30:02, subject to the Cambridge Core terms of use,

Example 2.10 The concepts of conditional probability also extend to discrete distributions.
For the oil model of Example 2.6, conditional probability tables can be constructed.
Pr (X = x | Y = y) is the probability that a rock is of a particular type given a value for
its age. The result is given in Table 2.2.
The rows add to 1. For this example, if it is Tertiary, it is shale; if it is Jurassic, it is sandstone.
2.8 Functions of a Random Variable
Suppose that an rv X has a pdf f x
ð Þ and a cdf F x
ð Þ. Deﬁne a new rv Y ¼ r X
ð Þ, where r is
a function. The cdf of Y is
G y
ð Þ ¼ Pr Y  y
ð
Þ ¼ Pr r X
ð Þ  y
½
 ¼
ð
x8r x
ð Þy
f x
ð Þdx
(2.38)
The integration takes place over all values of x such that r x
ð Þ  y.
Example 2.11 Let X be drawn from a uniform distribution with support [1, 1) so that
f x
ð Þ ¼ 1=2. Compute the pdf of Y ¼ X2.
The rv Y must lie on [0, 1), so
G y
ð Þ ¼ Pr Y  y
ð
Þ ¼ Pr X2  y


¼ Pr 
ﬃﬃﬃy
p  X 
ﬃﬃﬃy
p


¼
ðﬃﬃy
p
 ﬃﬃy
p
f x
ð Þdx
It follows immediately that g y
ð Þ ¼ 1= 2
ﬃﬃﬃy
p


. The pdf of y is unbounded at y = 0.
It is not true in general that Y ¼ r X
ð Þ will have a continuous distribution when X does,
and in that instance, (2.38) is the way to obtain the distribution for Y. However, when the
Table 2.2 Conditional Probability of Finding Oil
Age
Sandstone
Limestone
Shale
Permian
0.50
0.33
0.17
Triassic
0.50
0.25
0.10
Jurassic
1.0
0
0
Cretaceous
0.25
0.60
0.15
Tertiary
0
0
1.0
32
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

distribution of Y is continuous, there is another approach that yields the pdf directly. Let X
be a rv with pdf f x
ð Þ for which Pr a < X  b
ð
Þ ¼ 1. Let Y ¼ r X
ð Þ, where r is continuous
and monotone (i.e., either strictly increasing or decreasing) on
a; b
ð
. Suppose that
a < X  b if and only if α < Y  β, and let X ¼ s Y
ð Þ be the unique inverse
mapping of r X
ð Þ. Then the pdf of Y is g y
ð Þ ¼ ∂ys y
ð Þ

f s y
ð Þ
½
 for s y
ð Þ on a; b
ð
(or y on
α; β
ð
) and zero otherwise.
Example 2.12 A standardized Gaussian variable has the pdf f x
ð Þ ¼ 1=
ﬃﬃﬃﬃﬃ
2π
p


ex2=2. Derive
the pdf of y ¼ x2.
Since ∞< x  ∞, it follows that 0 < y  ∞. The inverse mapping is s y
ð Þ ¼
ﬃﬃﬃy
p . Then
g y
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1=8πy
p
ey=2, which is the chi square distribution with one degree of freedom.
Note that no integral had to be computed to get this result.
These concepts easily extend to large numbers of rvs. For a set of rvs
Xi
f
g, i ¼ 1, :::, N
with joint pdf f x1; :::; xN
ð
Þ, deﬁne a new rv Y ¼ r X1; :::; XN
ð
Þ. The cdf of Y can be
computed from ﬁrst principles. Let A 
 S comprise the subset containing all values
x1; :::; xN
f
g such that r x1; :::; xN
ð
Þ  y. The cdf is
G y
ð Þ ¼ Pr Y  y
ð
Þ ¼ Pr r X1; :::; XN
ð
Þ  y
½

¼
ð
  
ð
A
f x1; :::; xN
ð
Þ dx1    dxN
(2.39)
If G y
ð Þ is continuous, then the pdf can be found by differentiation.
Example 2.13 Suppose that the independent rvs Xi
f
g, i ¼ 1, :::, N, are a random sample from
a distribution with pdf f x
ð Þ and cdf F x
ð Þ. The largest value in the sample is YN ¼ max Xi
f
g,
and the smallest value is Y1 ¼ min Xi
f
g. Compute the pdf and cdf of YN and Y1.
For any allowed value of y,
GN Y
ð Þ ¼ Pr YN  y
ð
Þ ¼ Pr X1  y \    \ XN  y
ð
Þ ¼ Pr X1  y
ð
Þ    Pr XN  y
ð
Þ
¼ F y
ð Þ    F y
ð Þ ¼ F y
ð Þ
½
N
The pdf follows by differentiation
gN y
ð Þ ¼ N F y
ð Þ
½
N1f y
ð Þ
The distribution of the smallest value follows from the same reasoning
G1 y
ð Þ ¼ Pr Y1  y
ð
Þ ¼ 1  Pr Y1 > y
ð
Þ ¼ 1  Pr X1 > y \    \ XN > y
ð
Þ
¼ 1  Pr X1 > y
ð
Þ    Pr XN > y
ð
Þ ¼ 1  1  F y
ð Þ
½
    1  F y
ð Þ
½
 ¼ 1  1  F y
ð Þ
½
N
The pdf is
g1 y
ð Þ ¼ N 1  F y
ð Þ
½
N1f y
ð Þ
33
2.8 Functions of a Random Variable
.003
13:30:02, subject to the Cambridge Core terms of use,

The bivariate distribution of Y1 and YN can also be derived from ﬁrst principles
Pr Y1  y1 \ YN  yN
ð
Þ ¼ Pr 1  Y1 > y1 \ YN  yN
ð
Þ
¼ Pr YN  yN
ð
Þ  Pr Y1 > y1 \ YN  yN
ð
Þ
¼ Pr YN  yN
ð
Þ  Pr y1 < X1  yN; :::; y1 < XN  yN
ð
Þ
¼ GN yN
ð
Þ 
Y
N
i¼1
Pr y1 < Xi  yN
ð
Þ
¼ F yN
ð
Þ
½
N  F yN
ð
Þ  F y1
ð
Þ
½
N
The bivariate joint pdf follows from differentiation as
g y1; yN
ð
Þ ¼ ∂2
y1yN G y1; yN
ð
Þ ¼ N N  1
ð
Þ F yN
ð
Þ  F y1
ð
Þ
½
N2f y1
ð
Þf yN
ð
Þ
Since neither the pdf nor the cdf factors, it follows that the variables are dependent.
The minimum and maximum values are examples of order statistics that are obtained
by ranking and ordering a set of rvs and are covered in more detail in Section 4.10.
In fact, these are extreme order statistics and are the subject of a whole subﬁeld of
statistics – extreme value theory – that arises in the earth sciences in the study of ﬂoods
or severe storms, among other areas. Note that once a set of data is ranked and ordered,
the data are no longer independent or identically distributed even when the original
data are iid.
2.9 Functions of Two or More Random Variables
Transformation of a bivariate (or multivariate, because the procedure is the same) pdf is a
straightforward but tedious task. Suppose that Yj ¼ rj Xi
ð
Þ, i, j ¼ 1, :::, p, and that the
mapping is uniquely invertible so that Xi ¼ si Yj


. If the partial derivatives of the
transformation functions si with respect to the variable yj exist, then
g y1; :::; yN
ð
Þ ¼ det J
j
j f s1; :::; sN
ð
Þ
(2.40)
where J is the Jacobian matrix whose i, j element is ∂yjsi, and det is the determinant.
Example 2.14 Consider the distribution of two independent rvs X1 and X2. Derive the
distribution of their product and quotient. What are these distributions when X1 and X2 are
standard Gaussian?
Let the joint pdf of X1 and X2 be f x1; x2
ð
Þ. Beginning with the quotient distribution, let
Y1 ¼ X1=X2 and Y2 ¼ X2. The inverse transformations are X1 ¼ Y1Y2 and X2 ¼ Y2, and
the Jacobian determinant is y2, so the joint pdf of Y1 and Y2 is
g y1; y2
ð
Þ ¼ y2
j
jf y1y2; y2
ð
Þ
34
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

and the marginal pdf of Y1 is
g1 y1
ð
Þ ¼
ð∞
∞
y2
j
j f y1y2; y2
ð
Þdy2
If the variables are independent standardized Gaussian, then
g y
ð Þ ¼ 1
2π
ð∞
∞
xj j ex2=2ex2y2=2dx
This is symmetric about x = 0, so
g y
ð Þ ¼ 1
π
ð∞
0
xex2
y2þ1
ð
Þ=2
½
dx ¼
1
π 1 þ y2
ð
Þ
which is the standardized Cauchy distribution (or equivalently, Student’s t distribution with
one degree of freedom). It falls to zero much more slowly with y than the Gaussian
distribution because its tails are algebraic rather than exponential. The Cauchy distribution
is often used as a simple analogue for a long-tailed normal distribution. It is also a member
of the stable distribution family covered in Section 3.4.2.
For the product distribution, let Y1 ¼ X1X2 and Y2 ¼ X2. The inverse transformations
are X1 ¼ Y1=Y2 and X2 ¼ Y2, and the Jacobian determinant is 1=y2, so the joint pdf of
Y1 and Y2 is
g y1; y2
ð
Þ ¼ 1
y2
j
j f y1=y2; y2
ð
Þ
and the marginal pdf of Y1 is
g1 y1
ð
Þ ¼
ð∞
∞
1
y2
j
j f y1=y2; y2
ð
Þdy2
If the variables are independent standardized Gaussian, then
g y
ð Þ ¼ 1
2π
ð∞
∞
1
xj j ex2=2e y=x
ð
Þ2=2dx ¼ 1
π K0 yj j
ð
Þ
using Gradshteyn & Ryzhik (1980, 3.471-9), where K0 x
ð Þ is a modiﬁed Bessel function of
the second kind of order 0.
As a useful special case, consider linear transformations of the form Y ¼ A  X, where A is
an N  N matrix, and the dot denotes the inner product. Then X ¼ A1  Y, presuming that
A is invertible, and det J ¼ det A1 ¼ 1= det A. The pdf of the transformed variables is
g y1; :::; yN
ð
Þ ¼ f x1; :::; xN
ð
Þ= det A
j
j ¼ f A1  y


= det A
j
j
(2.41)
35
2.9 Functions of Two or More Random Variables
.003
13:30:02, subject to the Cambridge Core terms of use,

Example 2.15 Let two rvs X1 and X2 have a joint pdf f x1; x2
ð
Þ. Compute the pdf of their
sum and difference.
Let Y1 ¼ X1 þ X2 and Y2 ¼ X2, which inverts to X1 ¼ Y1  X2 and X2 ¼ Y2. The
transformation matrix
A ¼
1
1
0
1


So
A1 ¼
1
1
0
1


and det A1 ¼ 1, yielding
g y1; y2
ð
Þ ¼ f y1  y2; y2
ð
Þ
The marginal pdf of y1 is
g1 y1
ð
Þ ¼
ð∞
∞
f y1  y2; y2
ð
Þdy2
If X1 and X2 are independent, then g1 y1
ð
Þ ¼
Ð ∞
∞f 1 y1  z
ð
Þf 2 zð Þdz, which is just the
convolution of the marginal pdfs.
Using the same procedure, it is easy to show that the pdf of the difference between two
rvs is
g y
ð Þ ¼
ð∞
∞
f y þ z; z
ð
Þ dz ¼
ð∞
∞
f 1 y þ z
ð
Þ f 2 zð Þdz
where the second integral holds for independent variables. The pdf of the difference
between two rvs is the correlation of the marginal pdfs.
2.10 Measures of Location
Location parameters are a class of measurable factors that characterize the center of a
probability distribution. For a given pdf f , θ is a location parameter if the form of the
distribution is f x  θ
ð
Þ. There are many location parameters, but they divide into three
classes: means, median, and mode.
Suppose that an rv has a discrete distribution with pdf f x
ð Þ. The arithmetic mean or
expectation or expected value or ﬁrst moment of the rv is
36
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

μ ¼ E X
ð Þ ¼
X
x
x f x
ð Þ
(2.42)
provided that the sum is absolutely convergent P
x xj j f x
ð Þ < ∞


. For a continuous pdf,
μ ¼ E X
ð Þ ¼
ð∞
∞
x f x
ð Þ dx
(2.43)
provided that the integral is absolutely convergent
Ð ∞
∞xj j f x
ð Þdx < ∞


. If X is bounded
so that Pr a < X  b
ð
Þ ¼ 1, then E X
ð Þ always exists.
The expected value is in a loose sense the most probable value, or the center of gravity,
or the mean of the distribution f x
ð Þ. Conceptually, it can be thought of as the arithmetic
average of a large number of iid draws from the distribution. For a symmetric pdf,
the expected value occurs at the point of symmetry xo , where f xo  δ
ð
Þ ¼ f xo þ δ
ð
Þ for
all δ.
Example 2.16 The pdf of an rv that is the square root of the sum of the squares of two
independent Gaussian rvs is Rayleigh. This is the distribution of the magnitude of complex
rvs such as the Fourier transform at a given frequency. The pdf is f x
ð Þ ¼ xex2=2 for x  0.
The expected value is
μ ¼
ð∞
0
x2ex2=2dx ¼
ﬃﬃﬃπ
2
r
Example 2.17 The Cauchy distribution has the pdf f x
ð Þ ¼ 1= π 1 þ x2
ð
Þ
½
 with support
∞; ∞
ð
Þ. Because f x
ð Þ is symmetric about x = 0, one might conclude that the expected
value is zero. However, the Cauchy distribution is not absolutely integrable, and hence the
expected value does not exist.
The expected value of a function follows directly from the relation Y ¼ r X
ð Þ, yielding
E r x
ð Þ
½
 ¼
ð∞
∞
r x
ð Þ f x
ð Þdx
(2.44)
and can be computed without ﬁnding the pdf of r X
ð Þ. The characteristic function of
Section 2.4 is just the expected value of eitx.
Example 2.18 Find E X2


for a Rayleigh rv.
E X2


¼ Ð ∞
0 x3ex2=2dx ¼ 2. This result could have been obtained by ﬁrst ﬁnding the
pdf for x2 and then computing its expected value. That would deﬁnitely be the hard way to
solve the problem.
37
2.10 Measures of Location
.003
13:30:02, subject to the Cambridge Core terms of use,

The key properties of the expected value that can easily be derived from the deﬁnition are
1. If Y ¼ aX þ b, where a and b are constants, then E Y
ð Þ ¼ aE X
ð Þ þ b (afﬁne property).
2. If there exists a number a such that Pr X  a
ð
Þ ¼ 1, then E X
ð Þ  a. If there exists a
number b such that Pr X  b
ð
Þ ¼ 1, then E X
ð Þ  b.
3. E X1 þ    þ XN
ð
Þ ¼ E X1
ð
Þ þ    þ E XN
ð
Þ if E Xi
ð
Þ exists. This linearity property
holds regardless of any dependence between the rvs.
4. If a set of rvs
Xi
f
g is independent, then E QN
i¼1Xi

	
¼ QN
i¼1E Xi
ð
Þ. This follows from
the properties of the joint pdf of independent variables.
Example 2.19 A sample of rocks contains M rock types with the proportion of type 1 being
p. Let N rocks be drawn at random without replacement. Find the expected value of the
number of type 1 rocks drawn.
Let Xi be 1 if type 1 is drawn and 0 otherwise. Note that the set of rvs
Xi
f
g is not
independent because the sampling is without replacement. Then Pr Xi ¼ 1
ð
Þ ¼ p, E Xi
ð
Þ ¼
p, and E X1 þ    þ XN
ð
Þ ¼ Np.
There are two additional types of mean location parameters that are less frequently used,
and both require that the variables be nonnegative. The harmonic mean is the reciprocal of
the expected value of 1=x:
1
μH
¼
ð∞
0
f x
ð Þdx
x
(2.45)
The geometric mean is given by
log μG ¼
ð∞
0
log x
ð Þf x
ð Þdx
(2.46)
Equation (2.46) has the important property that the geometric mean of a ratio is the ratio of
the geometric means and hence is the only mean that can be used when one is dealing with
normalized variables. Compositional data are a prominent example of normalized variables
and will be examined in Chapter 11. Finally, the harmonic mean is always smaller than the
geometric mean, which is, in turn, smaller than the expected value.
The median ~μ of the distribution of X is the point where Pr X  ~μ
ð
Þ  1=2 and
Pr X  ~μ
ð
Þ  1=2. In terms of the pdf, this is
ð~μ
∞
f x
ð Þdx ¼
ð∞
~μ
f x
ð Þdx ¼ 1
2
(2.47)
In other words, the median is the point for which half the probability lies above and half
lies below. The median is always unique for continuous distributions. However, for
38
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

discrete distributions, the median may not be unique and may not coincide with an allowed
value. In some respects, the median is a better representation of the average of a distribu-
tion because not all distributions have a mean (remember the Cauchy distribution), but all
do have a median.
Example 2.20 The median of the Rayleigh distribution is found from
Ð ~μ
0 xex2=2dx ¼ 1=2,
which yields ~μ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 log 2
p
.
A generalization of the median is obtained by deﬁning the N quantiles of a distribution
ðQj
∞
f x
ð Þ dx ¼
j  1
2
N
j ¼ 1, :::, N
(2.48)
The quantiles divide the area under the pdf into N þ 1 probability intervals, with the ﬁrst
and last having probability increments of size 1= 2N
ð
Þ, and the remainder having probabil-
ity increments of 1=N. If the quantiles are known, a lot of information about the distribu-
tion is available. The median is the middle quantile.
The mode μ* is the maximum value on the pdf if it exists and hence in a very loose
sense is the most probable value. The mode is easily obtained as the solution to
∂x f x
ð Þ ¼ 0 subject to ∂2
x f x
ð Þ < 0. There may be more than one mode for a distribu-
tion, in which case it is multimodal and the mode is not unique, and otherwise it is
unimodal. For a symmetric unimodal distribution, the expected value, median, and
mode coincide.
2.11 Measures of Dispersion
Dispersion is a measure of the spread or width or scale of a distribution. Dispersions
divide into three classes: measures of the deviation from some central value (e.g., the
variance), measures of the distance between speciﬁed representative values (e.g., the
range), and measures of the deviation of the population among themselves (e.g., the mean
difference).
The most widely used scale parameter is the variance, or mean of the square of the
deviation from the mean, and is deﬁned as
σ2 ¼ var X
ð Þ ¼ E
X  μ
ð
Þ2
h
i
¼
ð∞
∞
x  μ
ð
Þ2f x
ð Þdx
(2.49)
Because the variance is the expected value of a nonnegative random variable, it follows
that it must be nonnegative. The variance exists only when (2.49) is ﬁnite. If X is ﬁnitely
bounded, then the variance always exists. If the variance is small, then the distribution
39
2.11 Measures of Dispersion
.003
13:30:02, subject to the Cambridge Core terms of use,

is tight around μ, and if the variance is large, then the distribution is broad around μ.
The standard deviation σ is the positive square root of the variance. The precision is the
inverse of the variance and is sometimes taken as 1= 2σ2
ð
Þ.
Some key properties of the variance that are easily derived from the deﬁnition are
1. var X
ð Þ ¼ 0 if and only if Pr X ¼ c
ð
Þ ¼ 1 (i.e., when the probability is concentrated at
a single point);
2. var aX þ b
ð
Þ ¼ a2var X
ð Þ (i.e., shifting the mean of a distribution by b does not
change its spread and hence its variance);
3. var X
ð Þ ¼ E X2


 E X
ð Þ2, a formula that is often useful for computation; and
4. If
Xi
f
g are independent, var X1 þ    þ XN
ð
Þ ¼ var X1
ð
Þ þ var XN
ð
Þ. However,
when the rvs are dependent, how they covary must also be considered.
A less commonly used measure of scale is the average absolute deviation from the
median given by
~σ ¼
ð∞
∞
x  ~μ
j
j f x
ð Þdx
(2.50)
The average absolute deviation from the mean can also be applied. A more useful
measure of scale is the median absolute deviation from the median or MAD, which is the
solution σMAD to
F ~μ þ σMAD
ð
Þ  F ~μ  σMAD
ð
Þ ¼ 1
2
(2.51)
The sample counterpart to (2.51) is especially useful when data contain a fraction of
extreme values because it is relatively insensitive to such unusual data.
The range is the difference between the largest and smallest value in a population.
However, this is not an especially useful measure of dispersion because it says nothing
about the behavior of the population between its extremes, and obviously has no meaning
if the distribution limits are inﬁnite. A more useful measure of distance between represen-
tative points of the distribution is the interquartile range, which is the difference between
the upper and lower quartiles and hence contains half the probability around the distribu-
tion center. It is given by
σIQ ¼ F1 0:75
ð
Þ  F1 0:25
ð
Þ
(2.52)
The interquartile range exists when the variance is undeﬁned and is twice the MAD for
symmetric distributions.
The mean difference is deﬁned by
Δ ¼
ð∞
∞
ð∞
∞
x  y
j
jf x
ð Þf y
ð Þdx dy
(2.53)
and is the mean deviation of the variable x  y about zero when x and y are independently
distributed as f . The mean difference is the average of the differences between all possible
40
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

pairs of variables independent of sign, and hence depends on the dispersion of the variables
among themselves rather than about some central value.
Coefﬁcients of variation are combinations of scale and location parameters that are
dimensionless and hence useful for comparing different populations measured in distinct
units. The Pearson coefﬁcient of variation is σ=μ and is the most widely used in practice.
An alternative is the Gini coefﬁcient of variation Δ= 2μ
ð
Þ. Both of these have the disadvan-
tage that they depend on the expected value and hence are useful only when there is a
natural choice of origin.
2.12 Measures of Shape
The mean and variance are summary properties about a distribution. However, higher-
order moments may be required and serve as measures of distribution shape. Deﬁne the kth
noncentral moment
μ0
k ¼ E Xk


(2.54)
and the kth central moment
μk ¼ E
X  μ
ð
Þk
h
i
(2.55)
that both exist for order k provided that E
X
j jk

	
< ∞. If the kth moment is the ﬁrst to be
undeﬁned, then all the j moments for j < k exist, but the ones that are larger than k do not.
The noncentral moments may be found using the cf
∂k
t ϕ tð Þ ¼ ik
ð∞
∞
eitxxkf x
ð Þdx
(2.56)
so
μ0
k ¼ i
ð
Þk ∂k
t ϕ tð Þ


t¼0
(2.57)
A measure of the asymmetry of a distribution is the third central moment μ3 normalized
by σ3 and is called the skewness. It vanishes (along with all higher-order odd moments) for
a symmetric distribution. A positive skewness means that the right tail of the distribution is
longer, whereas the opposite holds for a negative skewness.
A measure of the ﬂatness of a distribution is given by the fourth central moment μ4
normalized by σ4 and is called the kurtosis. A high-kurtosis distribution has a sharp peak
and longer tails, whereas a low-kurtosis distribution has a rounded peak and shorter tails.
The excess kurtosis is μ4=σ4  3 but is frequently simply called the kurtosis, which is
often a source of confusion. A distribution with zero excess kurtosis is mesokurtic, and the
Gaussian is a classic example. A distribution with positive excess kurtosis is leptokurtic,
whereas one with negative excess kurtosis is platykurtic.
41
2.12 Measures of Shape
.003
13:30:02, subject to the Cambridge Core terms of use,

The skewness and kurtosis are intuitive rather than precise. A better way of comparing
the shapes of two probability distributions F and G is as follows:
1. G is more skewed to the right than F if G1 F
ð Þ is convex for all x; and
2. When F and G are both symmetric, G has greater kurtosis than F if G1 F
ð Þ is convex
for all x > ~μ, where a function on an interval is convex upward if the line segment
between any two points on its graph lies above the function. This extends trivially to
convex downward functions.
2.13 Measures of Direction
The trigonometric moments are given by
αk ¼ E cos kθ
ð
Þ ¼
ð2π
0
cos kθ f θ
ð Þdθ
βk ¼ E sin kθ
ð
Þ ¼
ð2π
0
sin kθ f θ
ð Þdθ
(2.58)
The sequence of trigonometric moments αk; βk
f
g, for k ¼ 0,  1,  2, :::, for the random
variable θ completely speciﬁes the cf, and because a probability distribution on a circle is
uniquely speciﬁed by the cf, so are the cdf and (usually) the pdf. Consequently, in contrast
to distributions on the real line, a distribution on a circle is completely deﬁned by its
moments.
Following on (2.25), the kth element in the cf may be written (for k  0)
ϕk ¼ ρkeiμk
(2.59)
where ρk is the kth resultant length, and μk is the kth direction. The most important case is
k ¼ 1, and its trigonometric moments, resultant, and direction will be, respectively, α, β, ρ,
and μ. These are called, in turn, the mean cosine moment, mean sine moment, mean
resultant length, and mean direction. The mean direction is equivariant under rotation
because the mean direction of θ0 ¼ θ  ϑ is given by μ0 ¼ μ  ϑ. The mean resultant length
is invariant under rotation and reﬂection. Further, the kth trigonometric moment about the
mean direction is given by αk þ iβk, where
αk ¼ E cos k θ  μ
ð
Þ
½

βk ¼ E sin k θ  μ
ð
Þ
½

(2.60)
with α0 ¼ 1 and β0 ¼ β1 ¼ 0. Consequently, the mean resultant length is given by α1.
Finally, the median direction ~μ must satisfy
ð~μ
~μπ
f θ
ð Þdθ ¼
ð
~μþπ
~μ
f θ
ð Þdθ ¼ 1
2
(2.61)
42
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

and is not unique unless the distribution is unimodal. For computational purposes, the
median direction is better obtained by solving
min~μ π 
ð2π
0
π  θ  ~μ
j
j
j
j f θ
ð Þdθ
2
4
3
5
(2.62)
The circular variance is given by
ν ¼ 1  ρ
(2.63)
and lies on 0; 1
½
. The circular variance is zero if and only if the distribution is tightly
concentrated around the mean direction μ. Conversely, the circular variance is one when
there is no angular concentration around any given direction. The circular standard
deviation is given by
σ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 log ρ
p
(2.64)
and is not the square root of the circular variance (2.63). It has its origin in the deﬁnition of
the Gaussian distribution wrapped onto the unit circle, as described in Mardia & Jupp
(2000, chap. 3). The circular dispersion is an alternate measure of dispersion to the circular
variance and standard deviation and is given by
δ ¼ 1  ρ2
2ρ2
(2.65)
and is important in statistical inference for the mean direction.
Shape parameters for directional distributions are analogous to the standard skewness
and kurtosis. The directional skewness is given by
ς ¼
β2
1  ρ
ð
Þ3=2
(2.66)
and the directional kurtosis is
κ ¼ α2  ρ4
1  ρ
ð
Þ2
(2.67)
Finally, the directional p-quantile is the solution ~μp to
ð~μp
~μπ
f θ
ð Þdθ ¼ p
(2.68)
where p is a probability. The interquartile range for directional data follows as the angular
difference between ~μ0:25 and ~μ 0:75.
43
2.13 Measures of Direction
.003
13:30:02, subject to the Cambridge Core terms of use,

2.14 Measures of Association
The mean and variance give information about the marginal distributions in the bivariate or
multivariate cases but not about the joint distribution because they provide no information
about how the different variables are related. Let μx ¼ E X
ð Þ, μy ¼ E Y
ð Þ, σ2
x ¼ var X
ð Þ,
and σ2
y ¼ var Y
ð Þ. The covariance of X and Y is deﬁned as
cov X; Y
ð
Þ ¼ E
X  μx
ð
Þ Y  μy

	
h
i
(2.69)
The covariance exists provided that the respective variances are ﬁnite, and may be positive
or negative.
The correlation of X and Y is the normalized covariance given by
ρ X; Y
ð
Þ ¼ cov X; Y
ð
Þ
σxσy
(2.70)
and lies between 1 and 1. When ρ is positive or negative, the two rvs are said to be
positively or negatively correlated.
Some key properties of the covariance that can be derived from the deﬁnition include
1. cov X; Y
ð
Þ ¼ E XY
ð
Þ  E X
ð ÞE Y
ð Þ.
2. If X and Y are independent with ﬁnite variance, cov X; Y
ð
Þ ¼ ρ X; Y
ð
Þ ¼ 0. However,
cov X; Y
ð
Þ ¼ ρ X; Y
ð
Þ ¼ 0 does not prove independence, as can be demonstrated by
example: let X ¼ 1; 0; 1
f
g with equal probability. Then E X
ð Þ ¼ 0. Let Y ¼ X2,
which is clearly dependent on X. Then E XY
ð
Þ ¼ E X3


¼ 0; hence cov X; Y
ð
Þ ¼ 0.
The variables are uncorrelated but not independent.
3. var X  Y
ð
Þ ¼ var X
ð Þ þ var Y
ð Þ  2cov X; Y
ð
Þ.
4. var aX þ bY þ c
ð
Þ ¼ a2 var X
ð Þ þ b2 var Y
ð Þ þ 2ab cov X; Y
ð
Þ.
5. var PN
i¼1Xi

	
¼ PN
i¼1var Xi
ð
Þ þ 2PN
j¼1
Pj1
i¼1cov Xi; Yj


.
These extend trivially to the correlation.
2.15 Conditional Expected Value and Variance
The conditional expected value and variance are just the expected value and variance
computed using the conditional rather than the marginal distribution. The conditional
expected value is
E XjY ¼ y
ð
Þ ¼
ð∞
∞
xf xjy
ð
Þdx
(2.71)
44
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

and the conditional variance is
var XjY ¼ y
ð
Þ ¼
ð∞
∞
x  μ y
ð Þ
½
2f xjy
ð
Þdx
(2.72)
where μ y
ð Þ ¼ E XjY ¼ y
ð
Þ. The conditional expected value and variance are themselves
rvs because they depend on a speciﬁc value for the rv Y. Therefore, they have a pdf, an
expected value, and a variance of their own, as deﬁned by
E X
ð Þ ¼ EY E XjY
ð
Þ
½

(2.73)
var X
ð Þ ¼ E var XjY
ð
Þ
½
 þ var E XjY
ð
Þ
½

(2.74)
Equation (2.73) is the law of total expectation, whereas (2.74) is the law of total variance.
A generalization of the latter is the law of total covariance
cov X; Z
ð
Þ ¼ E cov X; ZjY
ð
Þ
½
 þ cov E XjY
ð
Þ; E ZjY
ð
Þ
½

(2.75)
which reduces to (2.74) when X = Z .
If X must be predicted without any information about Y, the best estimate for X is E X
ð Þ
with variance var X
ð Þ. However, if a priori information about Y is available, then the
expected value of the conditional expectation is a better estimate for X and will have a
lower variance.
2.16 Probability Inequalities
Probability inequalities are useful for placing bounds on entities that are difﬁcult to
compute directly. There are many of these, and only a few useful ones will be presented.
Let an rv X be nonnegative (i.e., Pr X  0
ð
Þ ¼ 1) and have a ﬁnite expected value. For
any number t > 0, the Markov inequality states that
Pr X  t
ð
Þ  E X
ð Þ
t
¼ μ
t
(2.76)
The Markov inequality is applicable only for large values of t and in fact produces nothing
useful if t  μ.
The Chebyshev inequality applies to an rv X that can take any value but has a ﬁnite
variance. For any number t > 0, it holds that
Pr X  μ
j
j  t
ð
Þ  var X
ð Þ
t2
¼ σ2
t2
(2.77)
and is not meaningful unless t > 1. The Chebyshev inequality is easily derived using the
Markov inequality on the rv X2. Both of these inequalities hold for any distribution for the rvs.
Hoeffding (1963) introduced several inequalities that provide much sharper bounds
than the Chebyshev inequality when rvs with zero expected value are bounded.
45
2.16 Probability Inequalities
.003
13:30:02, subject to the Cambridge Core terms of use,

The most general of these applies when 0  Xi  1, such as obtains with Bernoulli rvs,
and is given by
Pr XN  E XN


 t


 e2Nt2
(2.78)
where XN is the sample mean. Hoeffding generalized (2.78) to variables with arbitrary
lower and upper bounds. Because Pr XN  E XN



  t


¼ Pr XN  E XN


 t


þ
Pr XN  E XN


 t


, it follows that
Pr XN  E XN



  t


 2e2Nt2
(2.79)
which is usually simply called Hoeffding’s inequality.
Example 2.21 Let t ¼ 4σ. A Chebyshev bound of 1/16 can be placed on the probability that
an rv X differs from its expected value by more than four standard deviations regardless of
the distribution for X. However, this bound is relatively crude in some cases; for the
Gaussian distribution, the actual value is about 104.
A better bound than the Chebyshev inequality can be obtained using Cantelli’s inequality
when only one tail of the distribution is of interest. Let X be an rv with ﬁnite variance;
compute the standardized rv Z ¼ X  μ
ð
Þ=σ. Cantelli’s inequality holds that
Pr Z  t
ð
Þ 
1
1 þ t2
(2.80)
Cantelli’s inequality can be used to prove that the mean and median are always within one
standard deviation of each other. Setting t = 1 in (2.80) yields
Pr Z  1
ð
Þ ¼ Pr X  μ  σ
ð
Þ  1
2
Changing the sign of X and μ gives
Pr X  μ  σ
ð
Þ  1
2
and proves the result.
2.17 Convergence of Random Variables
Convergence theory for rvs is concerned with the characteristics of sequences of rvs and, in
particular, their limiting behavior. In the absence of stochasticity, a sequence of real
numbers
xi
f g converges to a value x if xN  x
j
j < ε as N ! ∞for every number ε > 0.
This becomes subtle when the variables are random because two continuous rvs are equal
with probability zero [i.e., Pr Xi ¼ Xj


¼ 0 for distinct i, j]. This leads to three concepts:
convergence in distribution, convergence in probability, and convergence almost surely.
46
Statistical Concepts
.003
13:30:02, subject to the Cambridge Core terms of use,

Let X1, X2, :::, XN be a sequence of rvs, and let Y be another rv. Let FN x
ð Þ be the cdf of
XN and G y
ð Þ be the cdf for Y. Then the following deﬁnitions pertain:
1. XN
converges
in
distribution
to
Y
(written
symbolically
as
XN !
d
Y)
if
lim
N!∞FN x
ð Þ ¼ G y
ð Þ at all points where G y
ð Þ is continuous.
2. XN
converges
in
probability
to
Y
(written
symbolically
as
XN !
p
Y)
if
Pr XN  Y
j
j > ε
ð
Þ ! 0 as N ! ∞for every number ε > 0.
3. XN
converges
almost
surely
(written
symbolically
as
XN !
as
Y)
if
Pr lim
N!∞XN  Y
j
j ¼ 0

	
¼ 1.
Caution: Convergence almost surely implies convergence in probability, which in turn
implies convergence in distribution. However, the reverse relationships do not hold.
There is one exception that is almost pathological: if XN !
d
Y and Pr Y ¼ a
ð
Þ ¼ 1
for some number a, then XN !
p
Y.
Some of these convergence properties are preserved under transformations. Let
XN, Y, UN, and V be rvs and g be a continuous function. Then the following relationships
hold:
1. If XN !
as
Y and UN !
as
V, then XN þ UN !
as
Y þ V.
2. If XN !
as
Y and UN !
as
V, then XNUN !
as
YV.
3. If XN !
p
Y and UN !
p
V, then XN þ UN !
p
Y þ V.
4. If XN !
p
Y and UN !
p
V, then XNUN !
p
YV.
5. If XN !
p
Y, then g XN
ð
Þ !
p
g Y
ð Þ.
6. If XN !
d
Y, then g XN
ð
Þ !
d
g Y
ð Þ.
7. If XN !
d
Y and UN !
d
a, where a is a constant, then XN þ UN !
d
Y þ a.
8. If XN !
d
Y and UN !
d
a, where a is a constant, then XNUN !
d
aY.
The last two of these constitute Slutsky’s theorem which holds for convergence in
probability as well as convergence in distribution. Note that if XN !
d
Y and UN !
d
V,
then XN þ UN does not necessarily converge in distribution to Y þ V.
47
2.17 Convergence of Random Variables
.003
13:30:02, subject to the Cambridge Core terms of use,

3
Statistical Distributions
3.1 Overview
This chapter builds on the foundations in Chapters 1 and 2 and provides an overview of
the main statistical distributions that occur in many science ﬁelds, with emphasis on
those that are important in the earth sciences, such as the lognormal and generalized
extreme value distributions. The treatment of distributions is divided between discrete
and continuous types, both of which are important for the analysis of earth sciences
data. MATLAB support for each of the distributions is described and used to illustrate
many of the characteristics and applications of the distributions. Much of the material
in this chapter is provided for reference purposes in subsequent chapters or for
general use.
The classic reference on univariate discrete distributions is Johnson, Kotz, & Kemp
(1993), and the classic references on univariate continuous distributions are Johnson,
Kotz, & Balakrishnan (1994, 1995). Johnson, Kotz, & Balakrishnan (1997) and Kotz,
Balakrishnan, & Johnson (2000) extend the discrete and continuous treatments to the
multivariate case.
3.2 MATLAB Support for Distributions
MATLAB has provided support for a large set of distributions since early releases of the
statistics toolbox. These were implemented using a mnemonic for the distribution name
followed by “pdf,” “cdf,” “inv,” or “rnd” for, respectively, the probability density function,
the cumulative distribution function, the quantile function, and random draws. For
example, the Gaussian distribution is obtained by prepending “norm” to one of these
postﬁxes and supplying parameters such as the argument range and location/scale
parameters, with default values for the latter if omitted.
In later releases of MATLAB, support was added for object-oriented programming,
and an additional method to access probability distributions was implemented based
on object-oriented principles. An increasingly large fraction of the supported
distributions can only be accessed using this approach, so it behooves the user to
understand how it functions. This chapter will focus on the use of distribution objects
for this reason.
48
.004
13:31:22, subject to the Cambridge Core terms of use,

MATLAB enables the creation of a distribution object consisting of its parameters and
a model description through the makedist function. For example, to create a normal
distribution object, use
pd = makedist('Normal');
which defaults to a mean of 0 and a standard deviation of 1 because these parameters were
not supplied. To create a normal distribution object with a speciﬁed mean and standard
deviation, provide keyword-value pairs for them
pd = makedist('Normal', 'mu', 1, 'sigma', 2);
Once a probability distribution object has been created, numerous derived quantities can
be obtained using the distribution handle pd. To obtain the pdf, cdf, quantile function, or
random draws from the distribution speciﬁed by pd, the methods pdf, cdf, icdf, and
random are used. For example, to deﬁne the pdf of a Gaussian distribution with mean
1 and standard deviation 2,
y = pdf(pd, x);
returns the pdf values at the arguments contained in x. The cdf of the same distribution is
given by
y = cdf(pd, x);
and the complementary cdf (see Section 2.2.2) follows from
y = cdf(pd, x, 'upper');
The quantile function and random draws operate in an analogous manner. MATLAB also
implements a number of distribution parameters using additional methods. For example,
the mean is obtained from
mean(pd)
The standard deviation, variance, median, and interquartile range are obtained by replacing
mean with std, var, median, and iqr, respectively.
MATLAB also provides a GUI called disttool that presents the pdf or cdf of a large
number of probability distributions and facilitates exploration of their behavior as the
distribution parameters are changed.
3.3 Discrete Distributions
3.3.1 Bernoulli Distribution
Consider an experiment having only two possible outcomes: live or dead, success or
failure, hit or miss, and so on, for which the variable 0 or 1 can be assigned as a proxy.
A random variable (rv) with this characteristic is called an indicator or Bernoulli variable.
49
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

Let Pr X ¼ 1
ð
Þ ¼ p and Pr X ¼ 0
ð
Þ ¼ 1  p. The pdf of a Bernoulli variable is
f x; p
ð
Þ ¼ px 1  p
ð
Þ1x
(3.1)
for x = 0 or 1 (and 0 otherwise). The parameter p is both the probability that a sample with
outcome 1 will be drawn and the population proportion with outcome 1. The expected
value and variance are E X
ð Þ ¼ p and var X
ð Þ ¼ p 1  p
ð
Þ. If a set of rvs
Xi
f
g is iid and
each has a Bernoulli distribution with parameter p, then the sequence is called a set of
Bernoulli trials.
Example 3.1 If a coin is tossed, and 1 is assigned for heads and 0 for not heads (or tails),
then the outcome of N coin tosses is a set of Bernoulli trials with parameter p = 0.5 for a
fair coin.
3.3.2 Binomial Distribution
The binomial distribution arises whenever the underlying independent events in a process
have two possible outcomes whose probabilities remain constant. It applies when a sample
of ﬁxed size N is drawn from an inﬁnite population where each population element is
independent and has the same probability for some attribute. It also applies when a sample
of ﬁxed size is drawn from a ﬁnite population where each element has the same probability
for some attribute and the sampling is done randomly with replacement. Consequently, the
binomial distribution applies to random sampling of an rv that is in some loose sense not rare.
Suppose that the outcome of an experiment is a Bernoulli variable and that the experi-
ment is repeated independently N times. Let the rv X be the number of successes in the N
Bernoulli trials. The probability of a given sequence of N trials containing exactly x
successes and N  x failures is px 1  p
ð
ÞNx because of independence. Further, there are
cN,x possible combinations of the N trials. Consequently, X has the probability density
function given by
bin x; N; p
ð
Þ ¼
N
x


px 1  p
ð
ÞNx
x ¼ 0, :::, N
(3.2)
and zero elsewhere. Equation (3.2) is called the binomial distribution with parameters N
and p and has ﬁnite support on the integers between 0 and N. The Bernoulli distribution is a
special case of the binomial distribution with N ¼ 1.
The expected value and variance of the binomial distribution are E X
ð Þ ¼ Np and
var X
ð Þ ¼ Np 1  p
ð
Þ. The skewness and kurtosis are
1  2p
ð
Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Np 1  p
ð
Þ
p
and
3 þ 1  6p 1  p
ð
Þ
½
= Np 1  p
ð
Þ
½
, respectively. The skewness is positive (negative) if
p < >
ð Þ1=2, and the distribution is symmetric (skewness of zero) if and only if p ¼ 1=2.
The mode is
N þ 1
ð
Þp
b
c, where the ﬂoor function x
b c is the greatest integer less than or
equal to x, and hence the binomial distribution is unimodal. When p < 1= N þ 1
ð
Þ, the
mode occurs at the origin. The binomial median may not be unique, and there is no single
formula for it, although there are numerous special cases.
50
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

The binomial cdf is
Bin x; N; p
ð
Þ ¼
X
x
b c
i¼0
N
i


pi 1  p
ð
ÞNi
(3.3)
which is also called the regularized incomplete beta function, denoted by I1p N  x; x þ 1
ð
Þ.
The pdf and cdf are accessed as described in Section 3.2. However, the quantile function
given by
pd = makedist('Binomial', 'N', n, 'p', p);
y = icdf(pd, pp)
returns the least integer y such that the binomial cdf evaluated at pp equals or exceeds y
because of the discrete form of the distribution, as is also true of other discrete distributions
described in this section. Figure 3.1 shows the binomial distribution for several pairs of
parameters.
Suppose that the binomial variable X is standardized by subtracting the expected value
and dividing by the standard deviation
X0 ¼
X  Np
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Np 1  p
ð
Þ
p
(3.4)
This standardization is sometimes called studentization for reasons that will become
apparent in Chapter 4. Then the De Moivre-Laplace theorem states that X0 !
p
N 0; 1
ð
Þ,
where N 0; 1
ð
Þ is the standardized Gaussian distribution (see Section 3.4.1), so
lim
N!∞Pr α < X0 < β
ð
Þ ¼
1ﬃﬃﬃﬃﬃ
2π
p
ðβ
α
eu2=2du
(3.5)
Equation (3.5) is an early form of the classic central limit theorem that is described in
Section 4.7.2. This was important for computation of binomial probabilities in the past
because the factorials in the pdf become tedious to compute for large N. The Gaussian
approximation becomes quite accurate for N > 50.
0
0
0.05
0.1
0.15
0.2
0.25
0.3
2
4
6
8
10
Figure 3.1
The binomial pdf for N = 10 and p = 0.25 (circles), 0.5 (x), and 0.75 (squares).
51
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

A key property of the binomial distribution is that of additivity. If a set of N Bernoulli rvs
Xi
f
g has parameter p, then the rv Y ¼ X1 þ    þ XN is binomial with parameters N and
p. Further, if the Xi
f
g are k independent binomial rvs with parameters Ni and p, then their
sum is also binomial with parameters N ¼ N1 þ    þ Nk and p.
A few examples of binomial distributions in the earth sciences include
• Abundant mineral occurrence in samples of ﬁxed size (0 = not present, 1 = present);
• Abundant fossil occurrence in samples of ﬁxed size (0 = not present, 1 = present);
• Incidence of producing versus dry holes in the petroleum industry (0 = dry, 1 =
producing); and
• Occurrence of cross-beds in sandstone (0 = not present, 1 = present).
Note the keyword “abundant” in the preceding; if the occurrence is rare, then the distribu-
tion is Poisson.
Example 3.2 Suppose that there is a 10% chance that a drilled well will produce oil.
Twenty-ﬁve holes are randomly drilled. What is the probability that at most, at least and
exactly three of the wells will produce oil?
Let the rv X represent well production, with 1 indicating a producing hole and 0 indicat-
ing a dry one. X is a binomial random variable with parameters (25, 0.1). First, create a
distribution object with these parameters:
pd = makedist('Binomial', 'N', 25, 'p', 0.1);
The MATLAB function pdf can be used to compute Pr X  3
ð
Þ, which is the probability
that at most three wells will produce oil
sum(pdf(pd, 0:3))
ans =
0.7636
The same result can be obtained using cdf(pd, 3). The probability that at least three wells
will be productive is Pr X  3
ð
Þ
sum(pdf(pd, 3:25))
ans =
0.4629
The same result can be obtained using cdf(pd, 25)  cdf(pd, 2) or 1  cdf(pd, 2) because
cdf(pd, 25)  1. The probability that exactly three wells will produce is Pr X ¼ 3
ð
Þ
pdf(pd, 3)
ans =
0.2265
In contrast, the probability that exactly 25 wells will be productive is vanishingly small
pdf(pd, 25)
ans =
1.0000e25
52
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

while the probability of a complete strikeout is surprisingly large
pdf(pd, 0)
ans =
0.0718
3.3.3 Negative Binomial Distribution
The negative binomial distribution is a generalization of the binomial distribution to the
case where, instead of counting successes in a ﬁxed number of trials, the trials are observed
until a ﬁxed number of successes is obtained. Let p be the probability of success in a given
trial, and let the ﬁxed integer k denote the number of successes observed after x trials. Let
the rv X denote the number of trials. Because of independence, a given sequence of trials
has probability pk 1  p
ð
Þxk. The last trial succeeded, and the remaining k  1 successes
that already occurred are assigned to the remaining x  1 trials as cx1,k1, yielding
nbin x; p; k
ð
Þ ¼
x  1
k  1


pk 1  p
ð
Þxk
x ¼ 0, 1, :::
(3.6)
As a cautionary note, the negative binomial distribution is sometimes deﬁned so that the
ﬁrst parameter r is the exact number of failures (rather than the number of trials) that occur
before the kth success is achieved. This is the approach used in MATLAB. Equation (3.6)
can be easily transformed into this version, yielding
nbin r; p; k
ð
Þ ¼
k þ r  1
r


pk 1  p
ð
Þr
r ¼ 0, 1, :::
(3.7)
Note that the support of (3.6) and (3.7) is inﬁnite, in contrast to the binomial distribution.
Figure 3.2 illustrates the negative binomial distribution for several parameter values.
The expected value and variance for the negative binomial distribution are
1  p
ð
Þr=p
and
1  p
ð
Þr=p2,
respectively,
and
hence
the
negative
binomial
distribution
is
00
0.1
0.2
0.3
0.4
0.5
2
4
6
8
10
Figure 3.2
The negative binomial pdf for k = 3 successes with p = 0.25 (circles), 0.5 (x), and 0.75 (square). Note the reverse
behavior as compared to the binomial pdf in Figure 3.1.
53
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

overdispersed (i.e., the standard deviation is larger than the mean). The skewness and
kurtosis are 2  p
ð
Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r 1  p
ð
Þ
p
and 3 þ p2 þ 6 1  p
ð
Þ
½
= r 1  p
ð
Þ
½
, respectively.
The negative binomial cdf is
Nbin r; p; k
ð
Þ ¼ Ip k; r þ 1
ð
Þ
(3.8)
The keyword to create a negative binomial distribution object in MATLAB is
“NegativeBinomial.”
The geometric distribution is a special case of the negative binomial distribution with
r ¼ 1. It possesses a memory-less property Pr X ¼ x þ sjX  x
ð
Þ ¼ Pr X ¼ s
ð
Þ. As for the
binomial distribution, the negative binomial and geometric distribution quantile functions
return the least integer x such that the cdf evaluated at x equals or exceeds y because of the
discrete form of the distribution. There is no separate keyword to specify a geometric
distribution object, but it can be accessed directly [i.e., the geometric pdf is obtained from
geopdf(x, p)]. A distribution object can otherwise be created for it by setting k = 1.
Example 3.3 For Example 3.2, what is the probability that exactly 10 failures are observed
before three successes are achieved? What is the probability that no more than 20 failures
are observed before three successes are achieved?
pd = makedist('NegativeBinomial', 'R', 3, 'p', 0.1);
pdf(pd, 10)
ans =
0.0230
cdf(pd, 20)
ans =
0.4080
3.3.4 Multinomial Distribution
The multinomial distribution is the multivariate generalization of the binomial and is
introduced here rather than in Chapter 10 because it plays an important role in goodness-
of-ﬁt testing, as described in Chapter 7. Suppose that a population contains K different
classes of items (K  2), with the proportion of items of the ith class given by pi > 0 for
i ¼ 1, :::, K such that PK
i¼1pi ¼ 1. Further suppose that N items are selected at random with
replacement, and let the rv Xi denote the number of selected items that are of the ith class.
Then the random vector
X1; :::; XK
f
g has the multinomial distribution with parameters N
and
p1; :::; pK
f
g.
Deﬁne the multinomial coefﬁcient
N
x1, x2, :::, xK


¼
N!
x1!x2!:::xK!
(3.9)
54
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

If x1, :::, xK are nonnegative integers such that PK
i¼1xi ¼ N, then the multinomial pdf is
given by
mnom x; N; p
ð
Þ ¼
N
x1, :::, xK


px1
1    pxK
K
(3.10)
Equation (3.10) reduces to the binomial pdf (3.2) when K ¼ 2. The marginal distribution
for any one class may be found by summing over the remaining classes but would be a
formidable task. However, note that xi can be interpreted as the number of successes in N
trials, each of which has probability of success pi and probability of failure 1  pi, and
hence xi is a binomial rv with pdf
bin xi; N; pi
ð
Þ ¼
N
xi


pxi
i 1  pi
ð
ÞNxi
(3.11)
The multinomial distribution has the expected value E Xi
ð
Þ ¼ Npi, the variance
var Xi
ð
Þ ¼ Npi 1  pi
ð
Þ (both of which follow from the binomial marginal distribution)
and the covariance cov Xi; Xj


¼ Npipj. The negative value for the covariance of a
multinomial distribution makes sense because there are only N outcomes to be shared
among K classes, and hence, if one of the classes becomes large, most of the remaining
classes will have to decrease. The covariance matrix is singular with rank K  1 because of
the sum constraint on the class probabilities.
The multinomial distribution is important in goodness-of-ﬁt testing. It is the joint
distribution of the heights of the bins in a histogram. It is also the joint distribution for
counted entities in geology, such as point counts or fossil counts, as long as the counts are
absolute rather than expressed as a proportion.
Example 3.4 Based on the historical record, an oil company knows that drilled holes
produce oil and gas with probabilities 0.05 and 0.1, and dry holes are encountered with
probability 0.85. The oil company drills 50 new holes. What is the probability that it will
drill exactly two oil- and ﬁve gas-bearing holes? What is the probability of obtaining
exactly two oil-bearing and no more than ﬁve gas-bearing holes?
This problem will be solved using the function mnpdf rather than the distribution object
for the multinomial distribution. The probability vector is
p = [0.05 0.1 0.85];
The pdf for a multinomial distribution with 50 trials will be constructed as follows:
n = 50;
x1 = 0:n;
x2 = 0:n;
[y1, y2 ] = meshgrid(x1, x2);
y3 = n + 1 - y1 - y2;
y = mnpdf([y1(:) y2(:) y3(:)],repmat(p, (n + 1)^2, 1));
55
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

y = reshape(y, n + 1, n + 1);
sum(sum(y))
ans = 1.0000
Consequently, the result is a pdf. The probability of obtaining exactly two oil- and ﬁve gas-
bearing holes is
y(3, 6)
ans =
0.0060
The probability of obtaining exactly two oil- and no more than ﬁve gas-bearing holes is
sum(y(2, 1:6))
ans =
0.0248
3.3.5 Hypergeometric Distribution
Suppose that there is a collection of objects containing a of type 1 and b of type 2. Further
suppose that N objects are selected at random from the sample without replacement.
Let the rv X be the number of type 1 objects obtained in the draw. It can neither be larger
than N nor a, so x  min a; N
ð
Þ. The number of type 2 objects is N  x and cannot
exceed b; hence x must be at least N – b. Since x is also nonnegative, x  max 0; N  b
ð
Þ.
Taken
together,
the
value
of
x
must
be
an
integer
lying
on
the
interval
max 0; N  b
ð
Þ; min N; a
ð
Þ
½
. The hypergeometric pdf for exactly x type 1 objects is given
by the ratio of combinations
hyge x; a; b; N
ð
Þ ¼
a
x


b
N  x


a þ b
N


(3.12)
Equation (3.12) can be recast in terms of the probability p of a type 1 object being drawn
and the total number of objects M by replacing a and b with Mp and M 1  p
ð
Þ, respect-
ively. The resulting pdf is
hyge x; M; N; p
ð
Þ ¼
Mp
x


M 1  p
ð
Þ
N  x


M
N


(3.13)
and is the more commonly encountered form. The hypergeometric distribution has already
been encountered in Example 1.6.
56
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

The expected value and variance for the hypergemetric distribution are E X
ð Þ ¼ aN=
a þ b
ð
Þ ¼ Np and var X
ð Þ ¼ Nab aþbN
ð
Þ=½ aþb
ð
Þ2 aþb1
ð
Þ ¼ Np 1p
ð
Þ M N
ð
Þ=
M 1
ð
Þ. The variance of a hypergeometric distribution is smaller than that of a binomial
distribution by the factor
M N
ð
Þ= M 1
ð
Þ as a consequence of sampling without
replacement. As M ! ∞, the factor becomes 1, and their variances become identical.
Figure 3.3 illustrates the hypergeometric pdf. The hypergeometric cdf cannot be
expressed in terms of elementary functions and in fact is proportional to a generalized
hypergeometric function.
The hypergeometric distribution is appropriate for studying sampling from ﬁnite popu-
lations without replacement, where the binomial is the distribution for studying sampling
from ﬁnite populations with replacement, or else inﬁnite populations. The hypergeometric
distribution reduces to the binomial distribution for large M because the distinction
between sampling with and without replacement becomes moot. Consequently, it also
reduces to the Gaussian for large M by the DeMoivre-Laplace theorem (3.5).
Example 3.5 Suppose that the 100 staff members of the geophysics department are
sharply divided in their taste for alcoholic beverages, with 45 identifying themselves as
beer drinkers, 40 as wine drinkers, and 15 as teetotalers. The director is appointing the
members of a committee to choose the drinks for the St. Patrick’s Day party by selecting
eight members at random and in a batch (so without replacement). Find the joint
distribution of the number of beer drinkers, wine drinkers, and teetotalers on the
committee, the marginal distributions for each group, and the probability that a majority
will be beer drinkers.
There are i beer drinkers, j wine drinkers, and k teetotalers on the committee, where
i þ j þ k ¼ 8. They are composed of i beer drinkers from the group of 45, j wine drinkers
from the group of 40, and k teetotalers from the group of 15. By combinatorics, there are
100
8


possible and equally likely committees. The joint distribution is Pr i beer drinkers
ð
\ j wine drinkers \ k teetotalersÞ
0
0
0.1
0.2
0.3
0.4
2
4
6
8
10
Figure 3.3
The hypergeometric pdf for N = 20, M = 15, and p = 0.33 (circles), M = 20 and p = 0.5 (x), and M = 25 and
p = 0.75 (square).
57
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

f i; j; k
ð
Þ ¼
45
i


40
j


15
k


100
8


where i, j, k lie on 0; 8
½
 and i þ j þ k ¼ 8. The marginal distribution for beer drinkers is
f 1 ið Þ ¼
45
i


55
8  i


100
8


and for wine drinkers is
f 2 jð Þ ¼
40
j


60
8  j


100
8


and for teetotalers is
f 3 k
ð Þ ¼
15
k


85
8  k


100
8


The marginal distributions f 1  f 3 are all hypergeometric distributions, while the joint
distribution f is a multivariate generalization of the hypergeometric. Note that the joint
distribution is not the product of the marginals, so the numbers of each type of drinker are
not independent. This makes sense because (for example) knowing the number of beer
drinkers changes the distribution of the number of wine drinkers.
The probability that there will be a majority of beer drinkers is the probability that there
are at least ﬁve beer lovers on the committee. This is easily obtained as the sum from
5 through 8 of f 1 ið Þ or using MATLAB (since makedist does not support the hypergeo-
metric distribution)
sum(hygepdf(5:8, 100, 45, 8))
ans =
0.2518
or this could be obtained from 1  hygecdf(4, 100, 45, 8).
The probability of a beer drinker majority occurring by chance is only about 25% despite
the fact that beer drinkers comprise 45% of the staff. The probability of getting a majority
of teetotalers is only 0.0017. The imbibers are pretty safe.
58
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

3.3.6 Poisson Distribution
The Poisson distribution applies to rare independent events from inﬁnite samples, where
the binomial is the distribution for common independent events from large samples. It can
be derived as a limiting form of the binomial distribution. Take the binomial distribution
(3.2), and let λ ¼ Np so that
bin x; N; λ
ð
Þ ¼
N
x


λ
N
 x
1  λ
N

Nx
¼
λx
x!
 
1  λ
N

N N N  1
ð
Þ    N  x þ 1
ð
Þ
Nx 1  λ=N
ð
Þx
(3.14)
Let N ! ∞and p ! 0 (i.e., consider the limit of a large sample and a rare event) such that
Np ¼ λ is a constant. Note that no condition is being placed on the size of λ. Evaluating the
last two terms in (3.14) gives
lim
N!∞
N N  1
ð
Þ    N  x þ 1
ð
Þ
N x 1  λ=N
ð
Þx
¼ 1
(3.15)
lim
N!∞
1  λ
N

N
¼ eλ
(3.16)
∴
lim
N!∞
p!0
bin x; N; λ
ð
Þ ¼ pois x; λ
ð
Þ ¼ eλλx
x!
x ¼ 0, 1, :::
(3.17)
and 0 otherwise. This result was ﬁrst obtained by De Moivre (1711) and was subsequently
rederived by Poisson (1837). The ﬁrst comprehensive study of the Poisson distribution is
due to von Bortkiewicz (1898). The support of the Poisson distribution is inﬁnite, in
contrast to the ﬁnite support of the binomial distribution. As for the binomial distribution,
there is an addition law: if
Xi
f
g are independent and each is Poisson with mean λi, their
sum is also Poisson with a mean given by P
iλi.
The expected value and variance for the Poisson distribution are E X
ð Þ ¼ λ and
var X
ð Þ ¼ λ and are identical. These are, respectively, Np and Np 1  p
ð
Þ for the bino-
mial distribution. The ﬁrst immediately follows from the deﬁnition of λ, and the second
follows when p is very small so that p2 may be neglected compared to p. The skewness is
1=
ﬃﬃﬃ
λ
p
, and the kurtosis is 3 þ 1=λ. The mode is λ
b c in general and reduces to two equal
maxima at λ  1 and λ if λ is an integer. The median lies between λ  1
b
c and λ þ 1
b
c but
does not have an exact representation. Equation (3.17) increases monotonically with λ for
ﬁxed x when λ  x and decreases monotonically thereafter. Figure 3.4 illustrates the
Poisson pdf.
The Poisson cdf is
Pois x; λ
ð
Þ ¼ Γ x þ 1; λ
ð
Þ
Γ x
ð Þ
(3.18)
59
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

where
Γ x; α
ð
Þ ¼
ð∞
x
tα1etdt
x > 0
(3.19)
is the complementary incomplete gamma function [the incomplete gamma function γ x; λ
ð
Þ
is given by (3.19) with integration limits of 0 and x]. Equation (3.19) reduces to Γ α
ð Þ when
x ¼ 0. Equation (3.18) is the complementary incomplete gamma function ratio.
Poisson distributions are common in the earth sciences:
• Radioactive decay where X is the number of
α; β; γ
ð
Þ particles emitted per unit time is
Poisson; and
• The occurrence of rare mineral grains in point counting, where X is the number of
observed grains.
They are also common in everyday life:
• The number of telephone calls coming into an exchange during a unit of time is Poisson
if the exchange services customers who act independently;
• The number of accidents (such as falls in the shower) for a large population in a given
period of time is Poisson because such accidents are presumably rare and independent
(provided that there was only one person in the shower). Insurance companies fre-
quently use a Poisson model; and
• The number of vehicles that pass a marker on a road in a given unit of time in light trafﬁc
(so that they operate independently) is Poisson.
The Poisson distribution is the counting distribution for Poisson processes that occur
frequently in nature. The topic of Poisson processes is an area of active research. However,
any physical process X tð Þ that satisﬁes ﬁve conditions will be Poisson (Johnson, Kotz, &
Kemp 1993):
1. X 0
ð Þ ¼ 0;
2. For t > 0, 0 < Pr X tð Þ > 0
½
 < 1;
0
0
0.1
0.2
0.3
0.4
2
4
6
8
10
Figure 3.4
The Poisson pdf for λ = 1 (circles), 3 (x), and 5 (squares) out to x = 10.
60
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

3. The number of occurrences of a phenomenon in any pair of disjoint intervals must be
independent (e.g., the fact that a lot of radioactive decays are observed in one time
interval has no effect on the number observed in the next time interval, presuming that
one is not dealing with a nuclear reactor or weapon);
4. The probability of an event during a particular time interval is proportional to the length
of that interval; and
5. The probability of two or more events in a particular interval must be smaller than that
for just one or, formally,
lim
δ!0
Pr X t þ δ
ð
Þ  X tð Þ  2
½

Pr X t þ δ
ð
Þ  X tð Þ ¼ 1
½
 ¼ 0
(3.20)
If all ﬁve conditions are met, the result is a Poisson process, and the number of events in a
time interval Δt will follow the Poisson distribution with mean λΔt.
Example 3.6 Radioactive decay of 40K in seawater gives an average of ﬁve γ particles per
second. What is the probability of observing 10 decays in 2 seconds? 20 decays in 1
second?
The process is Poisson with λ = 5 in any 1-second interval. In a 2-second interval, it will
be Poisson with λ = 10. Using MATLAB,
pd = makedist('Poisson', 'lambda', 10);
pdf(pd, 10)
ans =
0.1251
pd = makedist('Poisson', 'lambda', 5);
pdf(pd, 20)
ans =
2.6412e-07
Example 3.7 In a classic study (von Bortkiewicz 1898; reproduced as example 4 in
Andrews & Herzberg 1985), the number of fatalities resulting from being kicked by a
horse was recorded for 10 corps of Prussian cavalry over a period of 20 years in the late
1800s, giving 200 corps-years of data. These data and the probabilities from a Poisson
model with λ = 0.61 are listed in Table 3.1. The ﬁrst column gives the number of deaths
per year. The second column lists the number of times that number of deaths was
observed in 200 corps-years. The third column is the relative frequency obtained by
dividing the second column by 200. The fourth column is the Poisson probability with
λ = 0.61. The ﬁt is quite pleasing. One could use this result to predict the probability of
a given number of deaths from horse kicking per year if one ran an insurance company,
for example.
61
3.3 Discrete Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

3.4 Continuous Distributions
3.4.1 Normal or Gaussian Distribution
The Gaussian distribution is the most important distribution in statistics for a number of
reasons:
1. The Gaussian distribution may be derived from a number of simple and widely
applicable models.
2. The Gaussian distribution is the central model behind the mathematical theory of errors.
3. The classic central limit theorem (Section 4.7.2) states that if a sufﬁciently large sample
is drawn from any distribution with a ﬁnite variance, the sample mean will be Gaussian.
While this is often true, it breaks down sometimes in earth science problems because the
actual data variance is inﬁnite, and in any case the amount of data required to reach the
asymptotic limit is ill deﬁned.
4. The mathematical form of the Gaussian distribution leads to simple expressions and
simple forms. This is more of an excuse than a reason; mathematical convenience is no
replacement for realistic descriptions, and with computers, simulations can replace
analytic calculations.
As an example of point 1, Herschel (1850) considered the two-dimensional probability
distribution for errors in measuring the position of a star. Let x and y be the zonal and
meridional errors, respectively, with marginal distributions f(x) and g(y). Herschel postu-
lated two conditions that ﬂow from the underlying assumption of directional homogeneity
of the errors
1. The errors in x and y are independent, so the joint distribution h x; y
ð
Þ ¼ f x
ð Þg y
ð Þ.
2. If this expression is written in polar coordinates
r; θ
ð
Þ, then the transformed joint
distribution h r; θ
ð
Þ must be independent of θ.
The second condition leads to the relation
f x
ð Þg y
ð Þ ¼ h
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 þ y2
p


(3.21)
Table 3.1 Fatalities from Horse Kicking in the Prussian Cavalry
No. of deaths/year
Observed
Relative frequency
Poisson probability
0
109
0.545
0.543
1
65
0.325
0.331
2
22
0.110
0.101
3
3
0.015
0.021
4
4
0.005
0.003
62
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

Selecting y ¼ 0, this simpliﬁes to f x
ð Þg 0
ð Þ ¼ h xj j
ð
Þ, and selecting x ¼ 0, it simpliﬁes
to f 0
ð Þg y
ð Þ ¼ h yj j
ð
Þ. Dividing both equations by f 0
ð Þg 0
ð Þ and noting that a change of
sign for either x or y does not change the relationships shows that f ¼ g ¼ h. Taking
logarithms gives
log f x
ð Þ=f 0
ð Þ
½
 þ log f y
ð Þ=f 0
ð Þ
½
 ¼ log
f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 þ y2
p

	
f 0
ð Þ
h
i
(3.22)
Taking y to be zero so that the second term in (3.22) vanishes, the only possible solution is
log f x
ð Þ=f 0
ð Þ
½
 ¼ ax2
(3.23)
where a is a constant, and therefore
f x
ð Þ ¼ f 0
ð Þeax2
(3.24)
Equation (3.24) can represent a ﬁnite probability only if a < 0, and f 0
ð Þ is determined
by making the total probability equal to 1. Letting a ¼ 1=2σ2 and f 0
ð Þ ¼ 1=
ﬃﬃﬃﬃﬃ
2π
p
σ,
Equation (3.24) becomes
f x
ð Þ ¼
1ﬃﬃﬃﬃﬃ
2π
p
σ
ex2=2σ2
(3.25)
which is the Gaussian pdf. Thus the only marginal pdf satisfying Herschel’s postulates is
the Gaussian.
Let X be a continuous rv on the real line with mean μ and variance σ2, where ∞
μ  ∞and σ2 > 0. Then the normal distribution in standard form is
N μ; σ2


¼
1ﬃﬃﬃﬃﬃ
2π
p
σ
e xμ
ð
Þ2=2σ2
(3.26)
The expected value is E X
ð Þ ¼ μ, and since E X2


¼ μ2 þ σ2, var X
ð Þ ¼ E X2



E X
ð Þ
½
2 ¼ σ2. Figure 3.5 shows the Gaussian distribution for several parameter pairs.
The odd central moments higher than second order (e.g., the skewness and higher-order
odd moments) are all zero, and hence the Gaussian distribution is symmetric around the
mean. The even central moments are nonzero, and in particular, the kurtosis is 3. The
–4
–2
0
2
4
0
0.5
1
1.5
Figure 3.5
The Gaussian distributions N(0,1) in solid line, N(0, 0.3) in dashed line, and N(1, 1) in dotted line.
63
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

symmetry point for the Gaussian is the mean μ. The mean μ, median ~μ, and mode μ* are
identical. Inﬂection points where the second derivative is zero occur at μ  σ. The
Gaussian is unimodal and inﬁnitely differentiable. Four other key characteristics of the
Gaussian distribution are
1. If X and Yare two independent Gaussian rvs with means μx and μy and variances σ2
x and
σ2
y, then the linear combination aX þ bY, where a and b are constants, will also be
Gaussian with mean aμx þ bμy and variance a2σ2
x þ b2σ2
y.
2. If X and Y are independent and their sum X þ Y is Gaussian distributed, then X and Y
are each Gaussian (Cramér’s theorem).
3. If X and Y are independent and X þ Y and X  Y are also independent, then X and Y
must be Gaussian (Bernstein’s theorem).
4. If X and Y are jointly Gaussian and uncorrelated, then they are independent. The
Gaussian is the only distribution for which this property holds. For non-Gaussian rvs,
lack of correlation does not imply independence.
If an rv X 	 N μ; σ2
ð
Þ, then Y ¼ aX þ b 	 N aμ þ b; a2 σ2
ð
Þ. If a ¼ 1=σ and b ¼ μ=σ,
Y is the standardized rv that is distributed as N 0; 1
ð
Þ. This is why only N 0; 1
ð
Þ is tabulated
(insofar as tabulation makes any sense in the modern world) because any other Gaussian
distribution can be easily derived from it. The standardized normal distribution is
N 0; 1
ð
Þ ¼ φ x
ð Þ ¼
1ﬃﬃﬃﬃﬃ
2π
p
ex2=2
(3.27)
This is the default using MATLAB when the parameters are not speciﬁed.
The standardized Gaussian cdf is
Φ x
ð Þ ¼
ðx
∞
φ tð Þdt ¼ 1 þ erfðx=
ﬃﬃﬃ
2
p
Þ
h
i	
2
(3.28)
where erf is the error function [erf(x) in MATLAB] that cannot be expressed in closed form.
This is called the probability integral and is the default in MATLAB when the parameters
are not speciﬁed. The Gaussian cf was derived in Section 2.4. The quantile function is
Φ1 p
ð Þ ¼
ﬃﬃﬃ
2
p
erf 1 2p  1
ð
Þ
(3.29)
where erf 1 x
ð Þ is the inverse error function. By deﬁnition, Φ x
ð Þ ¼ Pr X  x
ð
Þ, and since
Pr X  x
ð
Þ ¼ 1  Φ x
ð
Þ, Φ x
ð Þ þ Φ x
ð
Þ ¼ 1, so tables only list probabilities for x  0.
Example 3.8 Suppose that an estimate is wanted for Pr 0 < x  10
ð
Þ for a N 1; 3
ð
Þ rv.
Standardize the variable as Z ¼ X  1
ð
Þ=
ﬃﬃﬃ
3
p
, and consider the equivalent expression
Pr 0:5774 < Z  5:1962
ð
Þ ¼ Φ 5:1962
ð
Þ  Φ 0:5774
ð
Þ
¼ Φ 5:1962
ð
Þ þ Φ 0:5774
ð
Þ  1
Therefore, Pr 0 < X  10
ð
Þ ¼ 0:7181. Of course, with MATLAB, this transformation
becomes unnecessary, and the answer can be obtained with two lines of code.
64
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

pd = makedist('Normal', 'mu', 1, 'sigma', sqrt(3));
cdf(pd, 10) – cdf(pd, 0)
ans =
0.7181
Let pk ¼ Pr X  μ
j
j  k σ
ð
Þ ¼ Pr Z  k
ð
Þ. Table 3.2 shows the fraction of the probability
within k standard deviations of the mean compared to the result from the Chebyshev
inequality, which is of little value for Gaussian variates.
The mean absolute deviation (MAD) for the Gaussian is given by
~σ ¼
1ﬃﬃﬃﬃﬃ
2π
p
ð∞
∞
xj jex2=2dx ¼
ﬃﬃﬃ
2
π
r
ð∞
0
xex2=2dx ¼
ﬃﬃﬃ
2
π
r
(3.30)
The MAD is the solution of Φ σMAD
ð
Þ  Φ σMAD
ð
Þ ¼ 1=2 or Φ σMAD
ð
Þ ¼ 3=4, yielding
σMAD ﬃ0:6595. The interquartile distance is twice the MAD for the Gaussian because the
distribution is symmetric. Half of its probability lies within 0.6595 of the mean.
3.4.2 Stable Distributions
A random variable is stable if a linear combination of two independent realizations of it has
the same distribution. Let a and b be positive constants, let X be stable, and let X1 and X2
be independent copies of X. The condition for stability is
aX1 þ bX2 ¼
d cX þ d
(3.31)
Where c > 0 and ¼d denotes “equal in distribution,” meaning that the rvs on either side of it
have the same distribution. The term stable is used because the shape is unchanged under a
transformation such as (3.31). This characteristic clearly applies to the Gaussian distribu-
tion with c2 ¼ a2 þ b2 and d ¼ a þ b  c
ð
Þμ by the addition rule. However, there is an
important extended family of stable distributions for which (3.31) also holds.
Stable distributions were ﬁrst described by Lévy (1925) and may be deﬁned through
their cf
Table 3.2 Gaussian Probabilities as a Function of Standard Deviations from the Mean
k
pk
pcheb
1
0.6826
0
2
0.9544
0.5
3
0.9974
0.8889
4
0.99997
0.9375
5
0.9999994
0.9600
65
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

ϕst tð Þ ¼ eγα tj jα 1þiβ tan
πα
2ð Þsgn tð Þ γ1α tj j1α1
ð
Þ
½
þiδt
α 6¼ 1
¼ eγ tj j 1þiβ 2
π sgn tð Þ log γ tj j
ð
Þ
½
þiδt
α ¼ 1
(3.32)
Stable distributions are parameterized by the tail thickness α 2 0
ð , 2, skewness (not to be
confused with the third normalized moment) β 2 1; 1
½
, scale γ 2 0; ∞
ð
Þ, and location
δ 2 ∞; ∞
ð
Þ; the latter two parameters are analogous to the standard deviation and mean.
The pdf st x; α; β; γ; δ
ð
Þ obtained as the Fourier transform of (3.32) cannot be expressed in
closed form using elementary functions except for three special cases:
1. When α ¼ 2, the distribution is N δ; 2γ2
ð
Þ, and β has no meaning;
2. When α ¼ 1, β ¼ 0, the distribution is Cauchy (see Section 2.9); and
3. When α ¼ 1=2, β ¼ 1, the distribution is Lévy.
There are several alternate expressions to (3.32) for the stable cf that frequently cause
confusion, but the present version (the 0-parameterization of Nolan [1998]) has the
advantage of being continuous in the parameters. It is also a location-scale parameteriza-
tion, in the sense that if X 	 st x; α; β; γ; δ
ð
Þ, then X  δ
ð
Þ=γ 	 st x; α; β; 1; 0
ð
Þ.
Stable distributions are unimodal, have an inﬁnitely differentiable pdf, and their support
is doubly inﬁnite except when β = 1 and α < 1, where they are bounded on (totally
skewed to) the left (right) or right (left), respectively. The left (right) bound for β ¼ 1 1
ð Þ
is δ þ γ tan πα=2
ð
Þ δ  γ tan π α=2
ð
Þ
½
. Stable distributions possess a reﬂection property:
st x; α; β; γ; δ
ð
Þ ¼ st x; α; β; γ; δ
ð
Þ. Further information may be found in Feller (1971),
Samorodnitsky & Taqqu (1994), and Uchaikin & Zolotarev (1999). However, these books
are written at an advanced level and are not readily accessible to the applied statistician.
There is a serious need for a modern textbook on stable distributions.
Stable distributions possess ﬁnite variance only when α = 2. For 1 < α < 2, stable
distributions have ﬁnite mean but inﬁnite variance, whereas for 0 < α  1, both the mean
and the variance are undeﬁned. The tails of stable distributions are algebraic except for the
Gaussian end member, with the tail thickness decreasing with α. For example, the Cauchy
distribution has 1= xj j2 tails, whereas the Lévy distributionhas a 1=x1:5 right tail. More generally,
st x; α; β; γ; δ
ð
Þ ! xj j αþ1
ð
Þ as x ! ∞for 0 < α < 2 as long as β 6¼ 1. As a consequence,
stable data with inﬁnite variance exhibit more extreme values and much more variability than
Gaussian ones. Figures 3.6 and 3.7 show symmetric
β ¼ 0
ð
Þ and skewed standardized
γ ¼ 1; δ ¼ 0
ð
Þ examples of stable distribution pdfs. Very long tails are quite evident.
Aside from the empirical observation that stable distributions ﬁt many types of data in
ﬁelds ranging from physics to ﬁnance, as described by Uchaikin & Zolotarev (1999), they
also have signiﬁcance due to the generalized central limit theorem described in Section
4.7.3. Further, the existence of stably distributed real-world data is intimately intertwined
with governing physics that contains fractional derivatives; Oldham & Spanier (1974)
provide a description of fractional calculus. Spatial and temporal fractional derivatives
reﬂect the existence of long-range ordering in the respective domain. Meerschaert (2012)
summarizes the arguments. For example, in geophysics, some of the nonlinear and none-
quilibrium processes that occur in the ionosphere and magnetosphere where short-period
geomagnetic variations originate do display evidence of fractional derivative and multi-
fractal behavior along with self-organized criticality, and as a consequence, magnetotelluric
66
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

data are pervasively stably distributed. Chave (2014) described a method to analyze such
data that takes advantage of their stable nature. The importance of the stable distributions
will certainly grow as their presence in nature becomes increasingly recognized.
The paucity of closed-form expressions for the density/distribution functions and their
derivatives means that numerical methods must be used both to compute those entities and
to estimate the stable parameters from data. Nolan (1997) describes algorithms to compute
univariate stable density functions. Maximum likelihood estimation (covered in Chapter 5)
of the stable parameters was introduced by DuMouchel (1975) and reﬁned by Nolan (2001).
MATLAB did not provide support for stable distributions other than the Gaussian and
Cauchy (which is equivalent to the Student’s t distribution with one degree of freedom)
until R2016a, when stable distribution objects were implemented. This appears to be based
on third-party MATLAB software from www.robustanalysis.com.
3.4.3 Rayleigh Distribution
The Rayleigh distribution describes the magnitude of a vector quantity such as current
speed in the ocean computed from its two horizontal vector components, assuming that
–10
–5
0
0
0.1
0.2
0.3
0.4
0.5
5
10
Figure 3.6
Symmetric (β = 0), standardized (γ = 1, δ = 0) stable distributions with tail thickness parameters α of 1.8
(solid line), 1.2 (dashed line), and 0.6 (dotted line).
–10
–5
0
0.1
0.2
0.3
0.4
5
0
10
Figure 3.7
Skewed (β = 1), standardized (γ = 1, δ = 0) stable distributions with tail thickness parameters α of 1.8
(solid line), 1.2 (dashed line), and 0.6 (dotted line).
67
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

the latter are uncorrelated and Gaussian distributed with a common variance and zero
mean. Alternately, the Rayleigh distribution describes the magnitude of proper (meaning
that they are uncorrelated with the complex conjugate; see Section 10.2.4) complex
Gaussian data
The Rayleigh pdf is given by
rayl x; γ
ð
Þ ¼ x
γ2 ex2=2γ2
x  0
(3.33)
where the scale parameter γ > 0. The pdf is easily derived from the distribution of two
independent zero-mean Gaussian variates with a common variance by transforming to
polar coordinates and integrating over all possible values of the angle. Figure 3.8 shows the
Rayleigh pdf for a range of parameters.
The Rayleigh cdf is
Rayl x; γ
ð
Þ ¼ 1  ex2=2γ2
(3.34)
The expected value is γ
ﬃﬃﬃﬃﬃﬃﬃﬃ
π=2
p
, and the variance is γ2 4  π
ð
Þ=2. The skewness is
2
ﬃﬃﬃπ
p
π  3
ð
Þ= 4  π
ð
Þ3=2  0:63, so the right tail is longer than the left one, and the kurtosis
is 32  3π2
ð
Þ= 4  π
ð
Þ2  3:25; hence the distribution is leptokurtic. The geometric and
harmonic means are
ﬃﬃﬃ
2
p
γee=2 (where e  0.5772 in the exponent is the Euler-Mascheroni
constant) and
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=π
p
γ, respectively. The median γ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
log 4
p
is larger than the mean, and the
mode γ lies below both. The mean, variance, and median can be veriﬁed using MATLAB
for a given value of γ = 2 as follows:
pd = makedist('Rayleigh', 'b', 2);
[mean(pd) sqrt(pi/2)*2;
var(pd) 2*(4 - pi)
median(pd) 2*sqrt(log(4))]
ans =
2.5066
2.5066
1.7168
1.7168
2.3548
2.3548
10
5
0
15
20
0
0.2
0.4
0.6
0.8
Figure 3.8
The Rayleigh pdf with scale parameters 1 (solid line), 3 (dashed line), and 5 (dotted line).
68
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

3.4.4 Lognormal Distribution
The lognormal distribution is the distribution of a variable whose logarithm is Gaussian. It
is the distribution of an rv that is the product of a large number of iid variables, just as the
Gaussian distribution is that for an rv that is the sum of a large number of iid variables. It
plays an important role in ﬁelds ranging from the earth sciences to ﬁnance.
Let X e N μ; σ2
ð
Þ, and deﬁne a new rv Y ¼ eX. Then the distribution of Y is the
lognormal distribution logn y; μ; σ2
ð
Þ ¼ N log y; μ; σ2
ð
Þ. The lognormal pdf has the form
logn y; μ; σ
ð
Þ ¼
1
ﬃﬃﬃﬃﬃ
2π
p
σy
e log yμ
ð
Þ2=2σ2
y > 0
(3.35)
This follows through a simple change of variables, as in Section 2.8. The lognormal cdf is
Logn y; μ; σ
ð
Þ ¼ 1
2 erfc  log y  μ
ﬃﬃﬃ
2
p
σ


¼ Φ
log y  μ
σ


(3.36)
where erfc x
ð Þ is the complementary error function [erfc(x) in MATLAB].
The expected value is eμþσ2=2, and the variance is ðeσ2  1Þe2μþσ2. The skewness and
kurtosis are
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
eσ2  1
p
ðeσ2 þ 2Þ and e4σ2 þ 2e3σ2 þ 3e2σ2  3, respectively. The skewness
is always positive, and consequently, the right tail is longer than the left one; hence the
distribution is leptokurtic. The lognormal distribution falls toward zero quickly and
toward inﬁnity slowly (Figure 3.9). The geometric and harmonic means are eμ and
eμσ2=2, respectively. The median and mode are eμ (hence the same as the geometric
mean) and eμσ2, respectively, and the mean exceeds the median, which, in turn, exceeds
the mode.
The mean, variance, and median formulas can be veriﬁed using MATLAB.
pd = makedist('LogNormal','mu',1,'sigma',2);
[mean(pd) exp(1+2^2/2);
var(pd) (exp(2^2)-1)*exp(2+2^2);
00
1
1
2
2
3
3
4
4
5
Figure 3.9
The lognormal pdf logn(0, 0.1) in solid line, logn(0, 1) in dashed line, and logn(1, 1) in dotted line.
69
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

median(pd) exp(1)]
ans =
1.0e+04 *
0.0020
0.0020
2.1623
2.1623
0.0003
0.0003
The lognormal distribution is of major importance in the earth sciences. Examples include
1. The φ-fraction in sedimentology that is deﬁned from the lognormal distribution. Let X
be grain diameter in millimeters, and let φ ¼ log2x. This choice is based on the
empirical observation that X is lognormal;
2. Sedimentary bed thickness; and
3. Permeability and pore size.
Many geophysical processes are nonlinear (e.g., magnetospheric processes or their
magnetic effects) and hence are the result of many multiplicative steps. The statistical
distribution of such processes will tend to lognormal rather than Gaussian as a result.
Example 3.9 Suppose that a particle of initial size y0 is subject to repeated impacts and
that after each impact, a proportion Xi of the particle remains, where Xi is an rv. If the
Xi
f
g are modeled as iid, after the ﬁrst impact, the particle size is Y1 ¼ X1y0; after the
second impact, Y2 ¼ X1X2y0; and after the Nth impact,YN ¼ X1X2    XNy0. Then
log YN ¼ log y0 þ PN
i¼1 log Xi . The distribution of particle size will be approximately
lognormal.
3.4.5 Gamma Distribution
The gamma distribution is actually a family of distributions rather than a single one, of which
the exponential and chi square distributions are the most important members. It is frequently
used to model waiting times in reliability and life testing. The pdf has the general form
gam x; α; β
ð
Þ ¼ βα
Γ α
ð Þ xα1eβx
x  0
(3.37)
where α is the shape parameter and β is the rate parameter (sometimes stated as 1/β, when
it is called the scale parameter), both of which must be positive. In standard form, β = 1.
The gamma distribution is sometimes expressed as a three-parameter distribution with x
replaced by x  γ, where γ is a location parameter, and x > γ.
The gamma cdf is
Gam x; α; β
ð
Þ ¼ γ βx; α
ð
Þ
Γ α
ð Þ
(3.38)
70
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

where γ x; a
ð
Þ is the incomplete gamma function given by
γ x; a
ð
Þ ¼
ðx
0
ta1etdt
(3.39)
Note the distinct versions of the complementary incomplete and incomplete gamma
function in the cdf for the Poisson distribution (3.18) and (3.38).
The expected value and variance are α/β and α/β2, respectively. The skewness and
kurtosis are 2=
ﬃﬃﬃα
p
and 3 þ 6=α, so the right tail of the distribution is always longer
than the left one; hence the distribution is leptokurtic. The geometric and harmonic
means are eψ α
ð Þ=β [where ψ x
ð Þ is the digamma function] and β= α  1
ð
Þ. The mode is
α  1
ð
Þ=β if α  1, which is smaller than the mean, whereas the distribution tends to
inﬁnity as x ! 0 for α < 0, so the mode occurs at the origin. There is no simple
expression for the median. The standard gamma distribution tends to N 0; 1
ð
Þ as
α ! ∞.
The mean and variance formulas can be veriﬁed using MATLAB [but note that the
parameter β in (3.37) is inverted in MATLAB, or is a scale parameter]:
a = 1;
b = 2;
pd = makedist('Gamma', 'a', a, 'b', b);
[mean(pd) a*b;
var(pd) a*b^2]
ans =
2
2
4
4
A key property of the gamma distribution is a reproductive one. If a set of N rvs
Xi
f
g is independently gamma distributed with parameters αi; β
ð
Þ, then PN
i¼1Xi is also
gamma distributed with parameters
PN
i¼1αi; β


. Note the similarity to the binomial
distribution. A second property is the scaling property. If X 	 gam x; α; β
ð
Þ, then
aX 	 gam x; aα; β
ð
Þ.
There are a number of important special cases of the gamma distribution:
1. The exponential distribution obtained when α = 1 is used in lifetime, reliability, and
failure analysis and is important enough that it deserves its own subsection.
2. The chi square distribution family obtained when α ¼ ν=2, β ¼ 1=2 is the distribution
of the sum of squared central (i.e., zero-mean) normal variates used widely in statistical
inference. It will be covered in Section 4.9.1.
3. The Erlang distribution obtained when α is an integer is the distribution of the waiting
time of the αth occurrence of a Poisson process with parameter λ = β. This is a
generalization of the role of the exponential distribution in Poisson processes that will
not be pursued further.
71
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

3.4.6 Exponential Distribution
The exponential distribution is used in geophysics to model the time between earthquakes
or geomagnetic reversals and in engineering to model component or system lifetimes. It
has the pdf (Figure 3.10)
expe x; β
ð
Þ ¼ βeβx
x  0
(3.40)
The exponential cdf is
Expe x; β
ð
Þ ¼ 1  eβx
(3.41)
The mean, variance, skewness, and kurtosis all follow from the deﬁnitions for the gamma
distribution, except that the harmonic mean is not deﬁned. The median is log 2=β, whereas
the mode is at the origin.
Caution: The MATLAB deﬁnition of the exponential distribution uses 1/β in place of β
in (3.40) and (3.41).
The survival or reliability function is the probability that a component or system will
survive beyond a given time and is just the complementary cdf Pr X > x
ð
Þ. For the
exponential distribution, the survival function is
R x; β
ð
Þ ¼ 1  expe x; β
ð
Þ ¼ eβx
(3.42)
The hazard or failure rate function is the rate at which failures occur and has units of
failures per unit time. It is given by ∂xR x; β
ð
Þ=R x; β
ð
Þ, which for the exponential
distribution is
h x; β
ð
Þ ¼ expe x; β
ð
Þ
R x; β
ð
Þ
¼ β
(3.43)
and hence the hazard is independent of x. The exponential is the only distribution used to
model failure that has this property, and this is due to the following important behavior:
Pr X > x þ djX > d
ð
Þ ¼ Pr X > x þ d \ X > d
ð
Þ
Pr X > d
ð
Þ
¼ Pr X > x þ d
ð
Þ
Pr X > d
ð
Þ
¼ R d; β
ð
Þ
(3.44)
00
1
2
3
4
5
0.5
1
1.5
2
2.5
3
Figure 3.10
The exponential pdfs expe x; 0:3
ð
Þ in solid line, expe x; 1
ð
Þ in dashed line, and expe x; 3
ð
Þ in dotted line.
72
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

The conditional probability is independent of x, so the exponential distribution has a memory-
less property. If X were the time between a pair of events, this says that if the event has not
occurred in t units of time, then the probability that the event will occur over the next d units of
time is R d; β
ð
Þ, which is the same as the probability that it will occur in a time interval d from 0.
Hence it does not matter (for example) that a large earthquake occurred at time t because the
system has no memory of that event. It is not necessary to consider past occurrences of an event
to predict future ones. The exponential distribution is the only continuous distribution with this
memory-less property. However, the discrete geometric distribution is also memory-less.
Example 3.10 Suppose that geomagnetic reversals occur independently and that the lifetime
of normal polarity intervals is expe x; β
ð
Þ. Let the rv Xi be the length of the ith normal
interval, and assume that there are N normal intervals. What is the distribution of the length
of time until the ﬁrst normal to reverse transition occurs?
Let
Y1 ¼ min Xi
ð
Þ be an rv representing the time to the ﬁrst normal to reverse
transition. Then
Pr Y1 > t
ð
Þ ¼ Pr X1 > t \    \ XN > t
ð
Þ
¼ Pr X1 > t
ð
Þ    Pr XN > t
ð
Þ
¼ eNβt
The distribution of Y1 is expe x; Nβ
ð
Þ. It is easy to show that Y2 is distributed as
expe x; N  1
ð
Þβ
ð
Þ and so on. The pdf expe x; Nβ
ð
Þ falls off more rapidly than
expe x; N  j
ð
Þβ
ð
Þ for j ¼ 1, :::, N  1, and hence it is more probable that the ﬁrst normal
to reverse transition occurs earlier than the last one does.
Example 3.11 The rate of radioactive decay of a certain isotope is Poisson with parameter λ.
Suppose that the variable of interest is how long it will take until a radioactive decay is
observed. Let Y1 be the time until the ﬁrst radioactive decay, and let X be the number of
particles observed in time t. It follows that Y1  t if and only if X  1 (i.e., the ﬁrst
radioactive decay is observed by time t if and only if at least one radioactive decay has
occurred). The rv X is Poisson with mean λt, so, for t > 0, Pr Y1  t
ð
Þ ¼ Pr X  1
ð
Þ ¼
1  Pr X ¼ 0
ð
Þ ¼ 1  eλt, where the last term is just the Poisson pdf when x = 0. This is
the cdf for Y1, where it is understood that t is positive.. The pdf follows by differentiation
and is the exponential distribution.
The exponential distribution describes the elapsed time before the occurrence of an event
that is Poisson (i.e., a rare event). This is the reason that it serves as a reasonable starting
model for the intervals between earthquakes and geomagnetic reversals. In this sense, it is
the continuous counterpart to the geometric distribution that describes the number of
Bernoulli trials required until a discrete process changes state.
73
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

Example 3.12 The occurrence of ﬂoods in a given region is a Poisson process of μ ﬂoods
per year, whereas their amplitude X X > 0
ð
Þ is a random variable with cdf H(x). Find the
cdf for the magnitude of the largest ﬂood over an interval of duration t using an exponential
distribution for H(x).
The distribution of ﬂood occurrence is f n; μt
ð
Þ ¼ eμ t μt
ð
Þn=n! for nonnegative integer
n. By the law of total probability, the distribution for the magnitude of the largest ﬂood is
G x
ð Þ ¼
X
n
f n; μt
ð
ÞH x; n
ð
Þ
where the sum is over all possible values of n. Recall (Example 2.13) that the cdf of the
largest element in a set of n rvs is F n x
ð Þ. Substituting, the distribution for the magnitude of
the largest ﬂood can be written
G x
ð Þ ¼ eμ t X
∞
n¼1
μtH x
ð Þ
½
n
n!
¼ exp μt 1  H x
ð Þ
½

f
g
Let H x
ð Þ ¼ 1  eβ xλ
ð
Þ for the exponential cdf with a scale and location parameter. The
ﬂood occurrence cdf becomes
G x
ð Þ ¼ exp μt eβ xλ
ð
Þ


This is Gumbel’s extreme value distribution and will be further described in Section 3.4.9.
Note that this example mixes discrete and continuous distributions, as is appropriate for the
problem under consideration.
3.4.7 Weibull Distribution
The Weibull distribution is used to model failures that are more complicated than can be
described by the exponential distribution, such as when the conditions for strict random-
ness break down. For example, one might expect the time interval between earthquakes to
be small immediately after a major earthquake and increase with time thereafter. In this
case, the time interval between aftershocks might be better modeled by a Weibull distribu-
tion with shape parameter ς < 1 than by an exponential distribution.
The Weibull pdf is
weib x; ξ; γ; ς
ð
Þ ¼ ς
γ
x  ξ
γ

ς1
e xξ
ð
Þ=γ
½
ς
(3.45)
for x  ξ, with location parameter ξ, scale parameter γ > 0, and shape parameter ς > 0. It
reduces to the exponential distribution when ς ¼ 1, ξ ¼ 0, and to the Rayleigh distribution
74
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

when ς ¼ 2, ξ ¼ 0, and consequently can be viewed as an interpolator that spans these two
distributions. The Weibull pdf is frequently seen without the location parameter ξ.
The Weibull cdf is
Weib x; ξ; γ; ς
ð
Þ ¼ 1  e xξ
ð
Þ=γ
½
ς
(3.46)
so the survival function is
R x; ξ; γ; ς
ð
Þ ¼ e xξ
ð
Þ=γ
½
ς
(3.47)
and the hazard function is
h x; ξ; γ; ς
ð
Þ ¼ ς
γ
x  ξ
γ

ς1
(3.48)
The hazard function is independent of x when ς ¼ 1 (the exponential distribution) but is a
decreasing function of x when ς < 1 and an increasing function of x when ς > 1. This
leads to a simple interpretation for the shape parameter ς in a reliability context: a value less
than 1 means that the failure rate decreases over time due to the removal of early problems
from infant mortality, and a value greater than 1 means that an aging process is in effect so
that failures increase with time. This deﬁnes the bathtub curve in reliability theory, where
failures decrease rapidly at the start, are relatively constant through the design life, and then
rise when the end of design life is reached.
When ξ ¼ 0, the expected value and variance are γΓ 1=ς
ð
Þ=ς and γ2 2Γ 2=ς
ð
Þ
f
ς Γ 1 þ 1=ς
ð
Þ
½
2g=ς , respectively. The expressions for the skewness and kurtosis are compli-
cated. The geometric and harmonic means are γee=ς and (when ς > 1) γς= ς1
ð
Þ ς  1
ð
Þ2=
ς Γ 1= ς  1
ð
Þ
½

f
g. The median and mode are γ log 2
ð
Þ1=ς and γ ς  1
ð
Þ=ς
½
1=ς for ς > 1. The
mode is at the origin when ς  1. Figure 3.11 shows the Weibull distribution for a scale
parameter of 1 and a variety of shape parameters.
The mean, variance, and median formulas can be veriﬁed using MATLAB.
00
1
1
2
2
3
3
4
4
5
Figure 3.11
The Weibull pdf with γ = 1, ς = 0.5 (solid line), γ = 1, ς = 2 (dashed line), and γ = 2, ς = 5 (dotted line).
75
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

s = 1;
c = 0.5;
pd = makedist('Weibull', 'a', s, 'b', c);
[mean(pd) s*gamma (1+1/c);
var(pd) s^2*(gamma(1+2/c) - gamma(1+1/c)^2);
median(pd) s*log(2)^(1/c)]
ans =
2.0000
2.0000
20.0000
20.0000
0.4805
0.4805
c = 1.5;
pd = makedist('Weibull', 'a', s, 'b', c);
[mean(pd) s*gamma(1 + 1/c);
var(pd) s^2*(gamma(1 + 2/c) - gamma(1 + 1/c)^2);
median(pd) s*log(2)^(1/c)]
ans =
0.9027
0.9027
0.3757
0.3757
0.7832
0.7832
3.4.8 Beta Distribution
The beta distribution has support [0, 1] and plays a major role in the distribution of the
order statistics, as will be shown in Section 4.10. It is also used to describe probabilities for
events that are constrained to happen within a speciﬁed time window and in likelihood
ratio testing. The beta pdf in standard form is
beta x;β1;β2
ð
Þ ¼ Γ β1 þ β2
ð
Þ
Γ β1
ð
ÞΓ β2
ð
Þxβ11 1  x
ð
Þβ21 ¼ xβ11 1  x
ð
Þβ21
B β1;β2
ð
Þ
0  x  1
(3.49)
where β1 > 0 and β2 > 0 are shape parameters, and B a; b
ð
Þ is the beta function. MATLAB
implements the beta function as beta(b1, b2). Figure 3.12 shows the beta pdf for a few
values of the shape parameters. The beta cdf is the regularized incomplete beta function
ratio introduced in Section 3.3.2:
Beta x; β1; β2
ð
Þ ¼ Ix β1; β2
ð
Þ
(3.50)
The expected value and variance are β1= β1 þ β2
ð
Þ and β1β2=½ β1 þ β2
ð
Þ2 β1 þ β2 þ 1
ð
Þ.
The expressions for the skewness and kurtosis are complicated. The geometric and
harmonic means are eψ β1
ð
Þψ β1þβ2
ð
Þ and (when β1 > 1) β1  1
ð
Þ= β1 þ β2  1
ð
Þ. The mode
is β1  1
ð
Þ= β1 þ β2  2
ð
Þ when β1 and β2 are larger than 1, and occurs at the distribution
endpoints otherwise. When it is unique, the mode is always smaller than the median, which
is, in turn, smaller than the mean. There is no general closed form expression for the
76
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

median, but there are numerous special cases, and it can be obtained numerically by
solving Ix β1; β2
ð
Þ ¼ 1=2.
The mean, variance, and median can be veriﬁed in MATLAB after deﬁning a function
handle for the numerical solution for the median
b1 = 0.5;
b2 = 0.5;
pd = makedist('Beta', 'a', b1, 'b', b2);
fun = @(x) betainc (x, b1, b2) - 0.5;
[mean(pd) b1/(b1 + b2);
var(pd) b1*b2/((b1 + b2)^2*(b1 + b2 + 1));
median(pd) fzero(fun,0.5)]
ans =
0.5000
0.5000
0.1250
0.1250
0.5000
0.5000
Repeating using different values for the shape parameters:
b1 = 1.5;
b2 = 0.5;
pd = makedist('Beta', 'a', b1, 'b', b2);
fun = @(x) betainc(x, b1, b2) - 0.5;
[mean(pd) b1/(b1 + b2);
var(pd) b1*b2/((b1 + b2)^2*(b1 + b2 + 1));
median(pd) fzero(fun, 0.5)]
ans =
0.7500
0.7500
0.0625
0.0625
0.8368
0.8368
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
Figure 3.12
The beta pdf with parameters (0.5, 0.5) in solid line, (0.4, 0.6) in dashed line, (2, 2) in dotted line, (4, 2) in dash-dot
line, and (2, 5) in gray line. The arcsine distribution corresponds to (0.5, 0.5), and the parabolic distribution
corresponds to (2, 2).
77
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

Some symmetry properties of the beta distribution include
1. beta x; β1; β2
ð
Þ ¼ beta 1  x; β2; β1
ð
Þ and Beta x; β1; β2
ð
Þ ¼ Beta 1  x; β2; β1
ð
Þ (reﬂec-
tion symmetry); and
2. Let
X 	 beta x; β1; β2
ð
Þ
and
Y 	 beta x; β2; β1
ð
Þ.
Then
E X
ð Þ ¼ 1  E Y
ð Þ
and
var X
ð Þ ¼ var Y
ð Þ.
In addition, the beta distribution can take on a wide range of shapes depending on its
parameters. When the shape parameters are identical, then
1. The pdf is symmetric about ½;
2. The expected value and median both equal ½; and
3. The skewness is zero.
4. When the shape parameters are both smaller than 1, the pdf is U-shaped and bimodal,
with the modes at the endpoints (see Figure 3.12).
5. When the shape parameters equal 1, the result is the uniform distribution on (0, 1).
6. When the shape parameters are both larger than 1, the pdf is symmetric and unimodal,
with the mode at ½.
When the shape parameters are not identical, a nonexhaustive list of properties
include
1. When β1 and β2 are both smaller than 1, the pdf is U-shaped and bimodal, with the
modes at the endpoints, and has positive skew when β1 < β2 and negative skew
otherwise.
2. When β1 and β2 are both larger than 1, the pdf is unimodal and has positive skew when
β1 < β2 and negative skew otherwise.
3. When β1 < 1 and β2  1, the pdf is J-shaped with a right tail, has a positive skew, and is
strictly decreasing. The mode is zero.
4. When β1  1 and β2 < 1, the pdf is J-shaped with a left tail, has negative skew, and is
strictly increasing. The mode is 1.
3.4.9 Generalized Extreme Value Distribution
Extreme value distributions are limit distributions for the maximum or minimum of a
set of rvs, and are very important in the earth sciences, especially in hydrology, where
they are used to model ﬂood or river outﬂow data, and in meteorology, where they are
used to model rainfall or extremes of wind speed. It can be proved that there are only
three types of extreme value distributions: the Gumbel distribution, which was derived
in Example 3.12; the Fréchet distribution; and a transformed form of the Weibull
distribution that was covered in Section 3.4.7. These are sometimes referred to as the
Type I, II, and III extreme value distributions, respectively. These three distributions can
be combined into a single form, the generalized extreme value (gev) or Fisher-Tippett
distribution.
78
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

The gev pdf is given by
gev x; ξ; γ; ς
ð
Þ ¼ 1
γ 1 þ ς x  ξ
γ



 1þ1=ς
ð
Þ
e 1þς
xξ
γ
ð
Þ
½

1=ς
ς 6¼ 0
¼ 1
γ e xξ
ð
Þ=γee xξ
ð
Þ=γ
ς ¼ 0
(3.51)
where ξ is the location parameter, γ > 0 is the scale parameter, and ς is the shape parameter.
The support of (3.51) is x > ξ  γ=ς when ς > 0 and x < ξ  γ=ς when ς < 0, and the pdf
is zero outside those ranges. Consequently, the pdf support has a lower limit for positive
shape parameters and an upper bound for negative shape parameters. When ς = 0, the
support is ∞; ∞
ð
Þ. When ς < 0, the gev pdf is a transformed Weibull. When ς = 0, it is a
transformed Gumbel, and when ς > 0, it is Fréchet. The ﬁrst two are not the same as the
standard Weibull and Gumbel distributions. Rather than using this terminology, it is safer
to work only with the generalized extreme value distribution. It is more useful to note that
for ς > 0, the right tail is algebraic, the distribution has inﬁnite variance for ς  1=2 and
inﬁnite mean for ς  1. For ς = 0, the tails are exponential, whereas for ς < 0, the upper tail
is truncated. Figure 3.13 illustrates this behavior.
The cdf is
Gev x; ξ; γ; ς
ð
Þ ¼ e 1þς
xξ
γ
ð
Þ
½

1=ς
ς 6¼ 0
¼ ee xξ
ð
Þ=γ
ς ¼ 0
(3.52)
with the same support as the pdf.
The preceding forms are appropriate for maximal extreme values. To obtain their
versions for minimal ones, replace x with –x in (3.52), and subtract the result from 1.
The pdf follows by differentiation and will be distinct from (3.51).
The expected value is ξ þ γ Γ 1  ς
ð
Þ  1
½
=ς when ς < 1 and ς 6¼ 0, ξ þ eγ (where
e  0:5772 is the Euler-Mascheroni constant) when ς = 0, and ∞otherwise. The variance
is γ2 Γ 1  2ς
ð
Þ  Γ 1  ς
ð
Þ
½
2
n
o
=ς2 when ς < 1/2 and ς 6¼ 0, γ2π2=6 when ς = 0, and ∞
otherwise. The skewness and kurtosis are complicated expressions that exist only for ς <
1=3 and ς < 1=4, respectively, when ς 6¼ 0. For ς = 0, the skewness is 4
ﬃﬃﬃﬃﬃ
54
p
z 3
ð Þ=
–5
0
5
0
0.2
0.4
0.6
0.8
1
Figure 3.13
The generalized extreme value distribution with ξ = 0 and γ = 1 for shape parameters of 1.0 (solid line), 0.5
(dashed line), 0.0 (dotted line), 0.5 (dash-dot line), and 1.0 (gray line).
79
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

π3  1:14 [where z x
ð Þ is the Riemann zeta function; z 3
ð Þ  1:2021], and the kurtosis is
27/5. The geometric and harmonic means cannot be expressed in closed form. The median
is ξ þ γ
log 2
ð
Þς  1
½
=ς when ς 6¼ 0 and ξ  γ log log 2
ð
Þ when ς = 0. The mode is ξ þ
γ
log 2
ð
Þς  1
½
=ς when ς 6¼ 0 and ξ when ς = 0.
The mean, variance, and median can be veriﬁed using MATLAB
xi = 0;
gam = 2;
sig = 0.4;
pd = makedist('GeneralizedExtremeValue', 'k', sig, 'sigma',
gam, 'mu', xi);
[mean(pd) xi + gam*(gamma(1 - sig) - 1)/sig;
var(pd) gam^2*(gamma(1 - 2*sig) - gamma(1 - sig)^2)/sig^2;
median(pd) xi + gam*(log(2)^(-sig) - 1)/sig]
ans =
2.4460
2.4460
59.3288
59.3288
0.7895
0.7895
sig = 0;
euler = -psi(1);
%Euler’s constant using the digamma function
pd = makedist('GeneralizedExtremeValue', 'k', sig, 'sigma',
gam, 'mu', xi);
[mean(pd) xi + euler*gam;
var(pd) gam^2*pi^2/6;
median(pd) xi - gam*log(log(2))]
ans =
1.1544
1.1544
6.5797
6.5797
0.7330
0.7330
3.4.10 Bivariate Gaussian Distribution
Let Z1 and Z2 be independent N 0; 1
ð
Þ rvs. Their joint pdf is the product of two marginal
Gaussian distributions
f z1; z2
ð
Þ ¼ 1
2π e z2
1þz2
2
ð
Þ=2
(3.53)
Consider
two
new
rvs
X1
and
X2,
where
X1 ¼ σ1Z1 þ μ1
and
X2 ¼ σ2
ρZ1 þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
Z2


þ μ2. Their joint pdf may be found by linear transformation as in
Section 2.9. The relationship given by
X1
X2


¼
σ1
0
σ2ρ
σ2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p


Z1
Z2


þ
μ1
μ2


(3.54)
80
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

has the inverse
Z1
Z2


¼
1
σ1σ2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
σ2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
0
σ2ρ
σ1


X1  μ1
X2  μ1


(3.55)
The determinant of the matrix in (3.54) is σ1σ2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
, so the joint pdf of X1 and X2 is
N2 x1; x2; μ1; μ2; σ1; σ2; ρ
ð
Þ ¼
1
2πσ1σ2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
e
1
2 1ρ2
ð
Þ
x1μ1
σ1

2
2ρ
x1μ1
σ1


x2μ2
σ2


þ
x2μ2
σ2

2


(3.56)
which is the bivariate Gaussian distribution with E Xi
ð
Þ ¼ μi, var Xi
ð
Þ ¼ σ2
i , and popula-
tion correlation coefﬁcient ρ. It generalizes to N terms to give the multivariate Gaussian, as
described in Chapter 10. Since Z1 and Z2 are independently N 0;1
ð
Þ and X1 and X2 are
linear combinations of Z1 and Z2, the marginal distributions for X1 and X2 are N μi; σ2
i


.
If X1 and X2 are uncorrelated, ρ ¼ 0. The joint pdf of X1 and X2 then becomes the
product of the marginal ones, and hence X1 and X2 are independent. The Gaussian is the
only distribution for which this holds. Lack of correlation does not prove independence
unless the rvs are Gaussian.
MATLAB does not explicitly implement the bivariate normal distribution but does
provide the multivariate normal distribution as functions rather than distribution objects.
The pdf and cdf are given by mvnpdf(x, mu, sigma) and mvncdf(x, mu, sigma), where mu
is a p-vector and sigma is the p  p covariance matrix. The bivariate Gaussian distribution
obtains when p = 2. MATLAB does not provide the quantile function for the multivariate
normal distribution. Random draws are given by mvnrnd(mu,sigma).
3.4.11 Directional Distributions
There are a wide variety of directional distributions on the circle and sphere, as thoroughly
described in Mardia & Jupp (2000, chap. 3). The most fundamental distribution on a unit
circle (i.e., a circle with a radius of unity) is the uniform distribution given by
f θ
ð Þ ¼ 1
2π
(3.57)
such that probability is proportional to arc length, and there is no concentration of
probability about any given direction.
The simplest transformation of the uniform distribution on a circle (3.57) is its perturb-
ation by a cosine function to give the cardioid distribution
card θ; ρ; μ
ð
Þ ¼ 1
2π 1 þ 2ρ cos θ  μ
ð
Þ
½

ρj j < 1
2
(3.58)
whose name ensues because r ¼ card θ; ρ; μ
ð
Þ deﬁnes a cardioid in polar coordinates.
The mean resultant length of (3.58) is ρ, the mean direction is μ, and it reduces to (3.57)
when ρ ¼ 0.
81
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

The most useful distribution on the unit circle is the von Mises (1918) distribution,
which is easily derivable from the bivariate Gaussian distribution (3.56). Suppose that X1
and X2 are independent Gaussian variables with population means μ1 and μ2 and common
population variance σ2. Their joint pdf is given by (3.56) with correlation coefﬁcient ρ ¼ 0
and σ1 ¼ σ2 ¼ σ. The joint pdf can be transformed to polar coordinates with x1 ¼ r cos θ
and x2 ¼ r sin θ using the method of Section 2.9. Deﬁning β ¼ 1=σ, λ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
μ2
1 þ μ2
2
p
, and
μ ¼ tan 1 μ2=μ1
ð
Þ, the joint pdf in polar coordinates is
g r; θ; β; λ; μ
ð
Þ ¼ β2r
2π eβ2 r2þλ2
ð
Þ=2eβ2λr cos θμ
ð
Þ
(3.59)
The marginal distribution for r is obtained by integrating (3.59) over all possible azimuths,
which, in turn, requires the integration of an exponential with a cosine argument. That term
can be expanded as an inﬁnite series of modiﬁed Bessel functions of the ﬁrst kind using the
generating function
et cos ϑ ¼ I0 tð Þ þ 2
X
∞
k¼1
Ik tð Þ cos kϑ
ð
Þ
(3.60)
Performing the integration gives
g1 r; β; λ
ð
Þ ¼ β2reβ2 r2þλ2
ð
Þ=2I0 β2λr


(3.61)
Deﬁne the concentration parameter κ ¼ β2λ2, and let the dimensionless radius be r0 ¼ r=λ.
The distribution for θ conditional on a particular value of the dimensionless radius r0 ¼ r∗
is obtained by dividing (3.59) by (3.61):
g θ; κ; μjr0 ¼ r∗
ð
Þ ¼ eκr∗cos θμ
ð
Þ
2π I0 κr∗
ð
Þ
(3.62)
Setting r∗¼ 1 yields the distribution for direction on the unit circle, or the von Mises
distribution:
mises θ; κ; μ
ð
Þ ¼ eκ cos θμ
ð
Þ
2πI0 κ
ð Þ
(3.63)
As κ ! 0, I0 κ
ð Þ and the exponential term go to 1, so the distribution is just the uniform
distribution (3.57). For small κ, the exponential term in (3.63) may be replaced with the
ﬁrst two terms in its series expansion to give mises θ; κ; μ
ð
Þ ﬃcard θ; κ=2; μ
ð
Þ, and the
von Mises distribution becomes cardioidal. As κ ! ∞, the distribution approaches
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
κ=2π
p
eκ cos θμ
ð
Þ1
½
. For any value of the argument of the cosine term save 0, the
exponential is decreasing with κ faster than
ﬃﬃﬃκ
p
increases, and the result is zero. When
θ  μ ¼ 0, the distribution is inﬁnite and hence becomes the point distribution δ θ  μ
ð
Þ in
the large κ limit. The von Mises distribution is unimodal and symmetric about the mean
direction μ. The cdf cannot be expressed in closed form and must be computed numeric-
ally. Figure 3.14 illustrates the von Mises distribution and suggests the end member
behaviors.
82
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

The kth sine moment about the mean direction μ is zero by symmetry, while the kth
cosine moment about the mean is given by
αk ¼
1
2π I0 κ
ð Þ
ð2π
0
cos k θ  μ
ð
Þeκ cos θμ
ð
Þdθ ¼ Ik κ
ð Þ
I0 κ
ð Þ
(3.64)
because the modiﬁed Bessel function of the ﬁrst kind is deﬁned by
Ik κ
ð Þ ¼ 1
2π
ð2π
0
cos kϑ eκ cos ϑdϑ
(3.65)
Consequently, the mean resultant length ρ is given by (3.64) with k = 1.
MATLAB does not provide support for the von Mises distribution. The von Mises
distribution is quite distinct from the marginal distribution for angle obtained by integrating
(3.59) over all possible values of the radius. Since for positive a,
ð∞
0
rear2ebrdr ¼ 2
ﬃﬃﬃa
p þ
ﬃﬃﬃπ
p beb2= 4a
ð
Þ 1 þ erf b= 2
ﬃﬃﬃa
p
ð
Þ
½

f
g
4a3=2
(3.66)
by Gradshteyn & Ryzhik (1980, 3.462-5), the marginal distribution for the angle is given
by
g2 θ; κ; μ
ð
Þ ¼ 1
2π
eκ=2 þ
ﬃﬃﬃﬃﬃ
πκ
2
r
cos θ  μ
ð
Þeκ sin 2 θμ
ð
Þ=2
1 þ erf
ﬃﬃﬃκ
2
r
cos θ  μ
ð
Þ




0
B
B
@
1
C
C
A
(3.67)
and reduces to the uniform distribution on a circle (3.57) as κ ! 0. Equation (3.67) is the
distribution of the angle regardless of the value of the radius (which is the fundamental
deﬁnition of a marginal distribution), whereas (3.63) is the distribution of the angle
conditional on the radius being equal to 1. It is not possible to derive (3.63) from (3.67)
because neither explicitly includes the joint distribution (3.59). However, they do have a
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
Figure 3.14
The von Mises pdf for θ in the range [0, π] for κ = 0.1 (solid line), 1 (dashed line), and 10 (dotted line).
83
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

relationship in the large concentration parameter, small angular difference limit. For large
κ, the asymptotic form of I0 κ
ð Þ 	 eκ=
ﬃﬃﬃﬃﬃﬃﬃﬃ
2πκ
p
, so (3.63) reduces to
mises θ; κ; μ
ð
Þ ﬃ
ﬃﬃﬃﬃﬃκ
2π
r
eκ cos θμ
ð
Þ1
½

(3.68)
For large x, erf x
ð Þ 	 1  ex2=
ﬃﬃﬃπ
p x, and (3.67) reduces to
g2 θ; κ; μ
ð
Þ ﬃ
ﬃﬃﬃﬃﬃκ
2π
r
cos θ  μ
ð
Þeκ sin 2 θμ
ð
Þ=2
(3.69)
and bears little resemblance to (3.68). However, for small values of θ  μ, sin θ  μ
ð
Þ 
θ  μ and cos θ  μ
ð
Þ  1  θ  μ
ð
Þ2=2, and (3.68) becomes the angular Gaussian
distribution
mises θ; κ; μ
ð
Þ ﬃ
ﬃﬃﬃﬃﬃκ
2π
r
eκ θμ
ð
Þ2=2
(3.70)
whereas with the further condition that θ  μ
ð
Þ2=2  1, (3.69) becomes identical.
If the angle on a circle must be characterized without a priori information about the
magnitude, the expected value and variance using the angular marginal distribution
(3.67) will provide the best estimate. However, if it is known (or desired) that the
magnitude be such that the parameter space is conﬁned to the unit circle, then the
expected value using the von Mises distribution will yield a better estimate and will
have a lower variance,
The generalization of the von Mises distribution to a spherical geometry is important in
paleomagnetism and plate tectonics. As a ﬁrstprinciples derivation, consider a three
Cartesian component geomagnetic vector x, and model it using a Gaussian pdf with a
mean three-vector μ and a common scalar variance σ2
f x; μ; σ2


¼ e xμ
ð
ÞT xμ
ð
Þ= 2σ2
ð
Þ
2π
ð
Þ3=2σ3
(3.71)
where • denotes the inner product. This must be transformed from Cartesian coordinates
x; y; z
ð
Þ, where by convention x points north, y points east, and z points down, to F; θ; ϕ
ð
Þ,
where F ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 þ y2 þ z2
p
is the scalar magnitude, θ ¼ tan 1 z=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 þ y2
p


is the inclin-
ation,
and
ϕ ¼ tan 1 x=y
ð
Þ
is
the
declination.
The
inverse
transformation
is
x ¼ F cos θ cos ϕ,
y ¼ F cos θ sin ϕ,
and
z ¼ F sin θ.
The
Jacobian
determinant
is
F2 cos θ. Consequently, the transformed pdf is
f
F; θ; ϕ; μF; μθ; μϕ; σ


¼ F 2 cos θ
2π
ð
Þ3=2σ3 e F cos θ cos ϕμF cos μθ cos μϕ
ð
Þ
2= 2σ2
ð
Þ
 e F cos θ sin ϕμF cos μθ sin μϕ
ð
Þ
2= 2σ2
ð
Þe F sin θμF sin μθ
ð
Þ2= 2σ2
ð
Þ
¼ F 2 cos θ
2π
ð
Þ3=2σ3 e F2þμ2
F
ð
Þ= 2σ2
ð
ÞeFμF cos ξ=σ2
(3.72)
84
Statistical Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

where cos ξ ¼ cos θ cos μθ cos ϕ  μϕ


þ sin θ sin μθ is the cosine of the off-axis angle
between a particular unit magnetic vector and the mean unit vector, and
μF; μθ; μϕ


are
the mean intensity, inclination, and declination, respectively.
Love & Constable (2003) give the following for the marginal distribution for intensity
obtained by integrating (3.72) over both the angular terms:
f F F; μF; σ
ð
Þ ¼
ﬃﬃﬃ
2
π
r
F
μFσ e F2þμ2
F
ð
Þ= 2σ2
ð
Þ sinh
μFF
σ2


(3.73)
Consequently, the conditional distribution for inclination and declination given a particular
value for the intensity F* is obtained by dividing the joint distribution (3.72) by the
intensity marginal distribution (3.73). Deﬁning k ¼ μ2
F=σ2 to be the concentration param-
eter, the result is
f
θ; ϕ; κ; μθ; μϕjF ¼ F∗


¼
κF∗=μF cos θ
4π sinh κF∗=μF
ð
Þ eκF∗cos ξ=μF
(3.74)
Setting the dimensionless intensity F∗=μF ¼ 1 gives the distribution of inclination and
declination on the unit sphere
fisher θ; ϕ; κ; μθ; μϕ


¼ κ cos θ
4π sinh κ eκ cos ξ
(3.75)
Equation (3.75) is called the Fisher distribution or the von Mises-Fisher distribution and
was given by Fisher (1953), although it was actually derived much earlier in a statistical
mechanics context by Langevin (1905). Equation (3.75) is exact, commensurate with the
model assumptions implicit to (3.71). Chave (2015) carried out a comparison of (3.75)
with the inclination-declination marginal distribution, showing that they are similar only in
the limit of large concentration parameter and very small off-axis angle, as was the case for
the von Mises distribution on a circle.
85
3.4 Continuous Distributions
.004
13:31:22, subject to the Cambridge Core terms of use,

4
Characterization of Data
4.1 Overview
This chapter marks the transition from the theory to the practice of data analysis. It ﬁrst
introduces the concept of an estimator, or formula for computing a statistic, and then
elaborates on the sample counterparts to the population entities for location, dispersion,
shape, direction, and association that were covered in Chapter 2. Using this information,
the key limit theorems for random variables (rvs) are described, of which the most
important are the two laws of large numbers and the classic central limit theorem (CLT),
which pertains to rvs whose distributions have a ﬁnite variance. Extensions of the CLT to
directional data and to an rv whose distribution has inﬁnite variance are then described.
A wide variety of visualization tools to characterize rvs are introduced, for which the key
ones are the percent-percent and quantile-quantile plots and the kernel density estimator.
Their implementations using MATLAB are presented by example. The three arguably most
important distributions in statistics, the Student’s t, chi square, and F distributions, which
are the sampling distributions for the sample mean and variance, are then deﬁned. The
concept of an order statistic obtained by sorting a set of rvs into ascending order and their
distributions is described, leading to the sample median and interquartile range. Finally, the
joint distribution of the sample mean and sample variance that is one of the most important
relationships in statistics is derived and explained.
4.2 Estimators of Location
A parameter is an attribute of a statistical distribution, such as the population mean or
variance in the Gaussian distribution of Section 3.4.1. Under the frequentist philosophy
of statistics, a parameter is a ﬁxed entity without statistical properties. Suppose there are
N iid samples
Xi
f
g of an rv with population mean μ and variance σ2. Form the estimator
XN ¼ 1
N
X
N
i¼1
Xi
(4.1)
An estimator is a statistic (or function that depends on a given set of data but not on any other
unknown parameters) that speciﬁes how to compute a parameter for a speciﬁc set of
measurements of an rv Xi
f
g. The probability distribution of a statistic is called the sampling
86
.005
13:29:40, subject to the Cambridge Core terms of use,

distribution and is discussed in Section 4.9. An estimator changes when an element Xi is
added or removed and hence is itself an rv. An estimate is a particular realization of an
estimator, such as X10. Equation (4.1) is the sample mean. The sample mean is an example of
a point estimator that yields a speciﬁc value for the parameter (although point estimators can
also be vector valued). Alternately, an estimator can be an interval estimator that yields a
range of values for the parameter of interest or a ratio estimator that describes the ratio of two
sets of rvs. These concepts will be explored further in Chapter 5.
The expected value of the sample mean is
E XN


¼ μ
(4.2)
using (4.1). The expected value of the sample mean is the population mean and hence it is
an unbiased estimator.
Presuming that the data are independent, the variance of the sample mean is
var XN


¼ 1
N 2 var
X
N
i1
Xi
 
!
¼ σ2
N
(4.3)
The distribution of the sample mean becomes increasingly concentrated about the popula-
tion mean as N increases and hence in some sense becomes a better estimate for it as the
number of data rises. This statement can be made more precise using the Chebyshev
inequality (2.77). Because the mean and variance of the sample mean are μ and σ2/N,
it follows that
Pr XN  μ

  t


 σ2
Nt2
(4.4)
This can be used to ask questions such as how many samples are required to get the sample
mean close to the true mean at some speciﬁed probability level? For example, if t ¼ 0:1 σ,
then the right side of (4.4) is 100/N. Consequently, 100 data values are required to even
make (4.4) meaningful, and to achieve a value of 0.05 requires 2000 data values.
Equation (4.3) gives the variance of the sample mean if the rvs used to compute it are
independent. If the rvs are correlated, then the variance of the sample mean is given by the
sum of their covariances
var XN


¼ 1
N
X
N
i¼1
X
N
j¼1
cov Xi; Xj


¼ 1
N
X
N
i¼1
var Xi
ð
Þ þ 2
N
X
i1
i¼1
X
N
j¼1
cov Xi; Xj


(4.5)
If the rvs have a common or scalar variance σ2 and a common correlation ρ, then (4.5)
reduces to
var XN


¼ σ2
N þ N  1
N
ρσ2
(4.6)
87
4.2 Estimators of Location
.005
13:29:40, subject to the Cambridge Core terms of use,

and hence the variance of the sample mean increases (decreases) as the correlation
becomes more positive (negative). Moreover,
lim
N!∞XN ¼ ρσ2
(4.7)
so if the variables are standardized, the sample mean divided by the variance is approxi-
mately the correlation of the rvs in the large sample limit.
A variant on the sample mean is the weighted sample mean. Let wi
f
g be a set of weights.
Then the weighted sample mean is given by
X
0
N ¼
P
N
i¼1
wixi
P
N
i¼1
wi
(4.8)
If the weights are already normalized, then the denominator in (4.8) is unity, and the
weighted sample mean is given by its numerator. A typical choice for the weights is the
inverse of a measure of the reliability for a given rv.
In MATLAB, the sample mean of a vector of data is given by mean(x). If x is a matrix,
then the mean of each column is returned. If the row means are required, then mean(x, 1)
will compute them, and the mean along any dimension of an N-dimensional array follows
from mean(x, ndim). The weighted mean is easily computed by mean(w.*x), where w is a
matrix of weights that is already normalized, or else mean(w.*x)/sum(w) if they are not.
MATLAB also implements the trimmed mean y = trimmean(x, percent) that removes
the largest and smallest values according to the parameter percent. If there are N values
in x, then the k = N*(percent/100)/2 highest and lowest values in x are trimmed before the
arithmetic mean is computed. This is sometimes useful to reduce the effect of outlying data
lying at the top or bottom of the data distribution. A third parameter ﬂag controls how
trimming is accomplished when k is not an integer: “round” (default) rounds k to the
nearest integer, and “ﬂoor” rounds k down to the next smallest integer. A fourth parameter
ndim operates as for mean.
MATLAB also provides the function nanmean(x) that computes the arithmetic mean
while skipping over NaN (not a number) values. It operates exactly as mean does.
The harmonic mean estimator is given by
HN ¼
N
P
N
i¼1
1
xi
¼
N Q
N
i¼1
xi
P
N
i¼1
Q
N
j¼1
xj
xi
(4.9)
A weighted version of the harmonic mean is straightforward. The harmonic mean provides
the best average when the random variables are rates. For example, if a ship travels a ﬁxed
distance at a speed a and then returns at a speed b, then the harmonic mean of a and b is the
average speed and will return the actual transit time when divided into the total distance
traveled. Further, the harmonic mean is dominated by the smallest values in a set of rvs
88
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

and cannot be made arbitrarily large by the addition of large values to the set. As a
consequence, it is robust to large values but strongly inﬂuenced by small ones. MATLAB
implements the harmonic mean as harmmean(x). Its attributes are identical to those
for mean.
The geometric mean estimator is a measure of central tendency based on the product of a
set of rvs, where the sample mean uses their sum, and is given by
GN ¼
Y
N
i¼1
xi
 
!1=N
¼ e
1
N
P
N
i¼1
log xi
(4.10)
The geometric mean pertains only to positive numbers due to the logarithm in the second
form of (4.10). If Xi
f
g and Yi
f
g are two sets of rvs, then the geometric mean of the ratio
X=Y is the ratio of the geometric means of X and Y. This makes the geometric mean the
appropriate estimator when averaging normalized entities, such as compositions covered in
Chapter 11. Further, recall from Section 2.10 that the harmonic mean is always smaller
than the geometric mean, which is, in turn, smaller than the arithmetic mean, provided that
a set of rvs has entries with different values. MATLAB implements the geometric mean as
geomean(x). Its behaviors are identical to mean.
If a set of N random variables
Xi
f
g is arranged in ascending order so that
X 1
ð Þ  X 2
ð Þ      X N
ð Þ
(4.11)
Then X ið Þ is the ith order statistic. Even if the unordered
Xi
f
g are iid, the
X ið Þ


are
necessarily dependent because they have been placed in rank order. The order statistics for
a set of data are easily obtained using the MATLAB sort(x) function.
The sample median is the middle-order statistic given by
~XN ¼ X
N=2
b
cþ1
ð
Þ
(4.12)
The median (4.12) may not be uniquely deﬁned if N is even. A simple workaround in this
instance is to deﬁne the median as the average of the two adjoining middle-order statistics.
MATLAB implements the sample median as median(x), with additional options as
for mean.
A more general location estimator than the sample median is the pth sample quantile
given by interpolating the order statistics:
q ¼ ^F 1
N
p
ð Þ ¼ 1  λ
ð
Þx ið Þ þ λx iþ1
ð
Þ
(4.13)
where i ¼ Np
b
c and λ ¼ Np  Np
b
c, and ^F 1
N
x
ð Þ is the inverse of the empirical cdf that is
discussed further in Section 4.8.2. The sample median is obtained when p ¼ 0:5, and the
sample minimum (maximum) is obtained when p ¼ 0 1
ð Þ. The lower and upper sample
quartiles are given when p is 0.25 or 0.75.
The sample mode is deﬁned as the most frequent or common value in a sample and is
most useful for discrete variables and moderate to large data sets. MATLAB implements
mode(x) that operates on vectors or matrices as for mean. If there are multiple values
occurring with equal frequency, mode returns the smallest of them, and consequently, the
function is not suitable for use with multimodal data. It is also unlikely to be accurate with
continuous data, where the occurrence of rvs with the same value is rare.
89
4.2 Estimators of Location
.005
13:29:40, subject to the Cambridge Core terms of use,

Example 4.1 The ﬁle cavendish.dat contains 29 measurements of the density of the Earth as
obtained by Henry Cavendish in 1798 using a torsion balance. These data are given
in Stigler (1977, table 8), except that the value 4.88 has been replaced with 5.88. The data
are presented as a multiple of the density of water and so are dimensionless. Compare the
harmonic, geometric, and arithmetic means with the median. What do they suggest? Use
the trimmed mean to further investigate the issue.
data = importdata(‘cavendish.dat’);
histogram(data, 10)
A histogram of the data in Figure 4.1 suggests that there are outlying values at the bottom
and top of the distribution, and the remainder of the data appear to be skewed to the left,
although the number of data is too small to be deﬁnitive.
harmmean(data)
ans =
5.4805
geomean(data)
ans =
5.4845
mean(x)
ans =
5.4886
median(data)
ans =
5.4700
The three types of means are in the appropriate order and all cluster around 5.48, which
appears to be near the center of the histogram. By contrast, the median is smaller than any
of the means. Applying a trimmed mean with a 5% cutoff and the default mode of round
will remove one data value at the top and bottom of the distribution.
5
5.2
5.4
5.6
5.8
0
1
2
3
4
5
Figure 4.1
Histogram of the Cavendish density of Earth data.
90
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

trimmean(data, 5)
ans =
5.4896
The result does not shift the mean very much, but the histogram in Figure 4.1 shows that
there are multiple data at the extremes of the distribution. Increasing the percent parameter
in trimmean to 20 removes three data each at the top and bottom of the distribution and
gives a value of 5.4848.
4.3 Estimators of Dispersion
The estimator for the sample variance is given by the sum of squared differences between
an rv and its sample mean
^s2
N ¼ 1
N
X
N
i¼1
Xi  XN

2 ¼ 1
N
X
N
i¼1
X2
i  XN

2
(4.14)
Since E ^s2
N


¼ N  1
ð
Þσ2=N, the sample variance is biased, but asymptotically unbiased.
For this reason, (4.14) is sometimes called the biased sample variance. An unbiased
estimator for ﬁnite N can be obtained by dividing it by N  1 instead of N. This is the
unbiased sample variance
^s02
N ¼
1
N  1
X
N
i¼1
Xi  XN

2
(4.15)
Conceptually, the denominator in (4.15) is smaller by 1 because the sample mean has been
computed from the same set of rvs, and in the process imposing a linear relationship
among them.
The standard deviation is the square root of (4.14) or (4.15). However, because the
square root is not a linear function, the value computed from (4.15) is not unbiased. For
Gaussian data, a correction factor is given by
E ^sN
ð
Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
N  1
r
Γ N=2
ð
Þ
Γ N  1
ð
Þ=2
½
 σ
(4.16)
For N = 3, 10, 30, and 100, the correction factors are 0.886, 0.973, 0.991, and 0.998,
respectively, and hence the bias is not a serious issue except in the presence of a very small
number of data. Note also that the bias in the estimator ^sN is upward. Alternate correction
factors exist for some non-Gaussian distributions but will not be described further.
MATLAB implements the sample variance as var(x) and by default is the unbiased
estimator (4.15). Adding a second attribute var(x, 1) returns the sample estimator (4.14),
and var(x, w) returns the variance using weights in w that should be positive and sum to
unity. When x is a matrix, var returns the variance of the columns, but this can be changed
91
4.3 Estimators of Dispersion
.005
13:29:40, subject to the Cambridge Core terms of use,

by calling it with a third parameter ndim that speciﬁes the dimension over which the
variance is to be computed. The function nanvar ignores NaN values and otherwise
operates as for var. The function std returns the standard deviation given by the square
root of the variance and operates as for var but does not implement the bias correction
factor (4.16). The function nanstd ignores NaN values and otherwise operates as std does.
The sample median absolute deviation (MAD) is given by
^sMAD ¼ X  X
N=2
b
cþ1
ð
Þ


N=2
b
cþ1
ð
Þ
(4.17)
and can be obtained by two sorts of the rvs. Because it operates on the medians, it is
insensitive to extreme data in the tails of the sampling distribution and hence is said to be
robust.
MATLAB implements mad(x). However, the default is to return the mean deviation from
the mean given by mean(abs(x - mean(x))), which is not useful as a measure of dispersion
unless the data are free of extreme values, in which case the variance or standard deviation
operates well and is more familiar (and more efﬁcient). To compute the MAD using medians,
it must be called as mad(x, 1). A third parameter may be passed to specify the dimension
along which the statistic is to be computed as for var. The function mad treats NaN values as
missing data. There is a signiﬁcant lack of consistency in how MATLAB handles NaNs
across different functions, and hence it is important to carefully check the documentation.
An estimator for the range is given by
^sr ¼ X N
ð Þ  X 1
ð Þ
(4.18)
MATLAB implements the sample range as range(x) with the same behaviors as for mean.
A sample estimator for the interquartile range is
^sIQ ¼ X 3N=4
ð
Þ  X N=4
ð
Þ
(4.19)
using the order statistics (4.11) and is implemented in MATLAB as iqr(x).
MATLAB also provides the function zscore(x, ﬂag), which returns the standardized
form of the rvs in x by subtracting the mean and dividing by the standard deviation. The
parameter ﬂag is 0 or 1 as the standard deviation is based on the unbiased (default) or
biased version of the variance. Matrices are treated as for var.
Example 4.2 Returning to the Cavendish density of the Earth data, compare the standard
deviation, MAD, and interquartile range for them. Remove the two smallest and largest
data values and repeat. What does the result suggest?
std(data)
ans =
0.2154
n = length(data);
sqrt(2/(n - 1))*gamma(n/2)/gamma((n - 1)/2)*std(data, 1)
ans =
0.2098
92
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

mad(data, 1)
ans =
0.1500
iqr(data)
ans =
0.2925
The MAD is slightly larger than half the interquartile range, suggesting a distribution
that is approximately symmetric. The bias correction to the standard deviation is very
small.
data = sort(data);
data = data(3:n - 2);
std(data)
ans =
0.1685
mad(data,1)
ans =
0.1300
iqr(data)
ans =
0.2725
The standard deviation has shifted downward by about 20% due to the removal of four
data points. By contrast, the MAD and interquartile range are changed only slightly, but the
interquartile range is now larger than twice the MAD, suggesting that asymmetry has
increased.
4.4 Estimators of Shape
Recall from Section 2.12 that skewness is a measure of the asymmetry of a distribution. The
sample counterpart measures the distributional asymmetry of a set of rvs and is given by
^s3 ¼
1
N
X
N
i¼1
Xi  XN

3
1
N
X
N
i¼1
Xi  XN

2
"
#3=2
(4.20)
Equation (4.20) is a biased estimator. A bias correction is often applied, yielding the so-
called unbiased estimator of skewness
^s0
3 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N N  1
ð
Þ
p
N  2
^s3
(4.21)
93
4.4 Estimators of Shape
.005
13:29:40, subject to the Cambridge Core terms of use,

For N = 3, 10, 30, and 100, the bias corrections are 4.08, 1.18, 1.05, and 1.02, and hence
the bias correction pertains only to small numbers of data but requires at least three
elements in the data set. Further, the bias in (4.20) is downward.
Recalling that the kurtosis is a measure of the ﬂatness of a distribution, the sample
estimator for the kurtosis of a set of rvs is
^s4 ¼
1
N
X
N
i¼1
Xi  XN

4
1
N
P
N
i¼1
Xi  XN

2

	2
(4.22)
As for the skewness, there is a bias-corrected form of the kurtosis given by
^s0
4 ¼
N  1
N  2
ð
Þ N  3
ð
Þ
N þ 1
ð
Þ^s4  3 N  1
ð
Þ
½
 þ 3
(4.23)
The bias correction in (4.23) is afﬁne, in contrast to the skewness, where it is multiplica-
tive, and requires at least four elements in the data set. For a value in (4.22) of 3 (the
kurtosis of a Gaussian distribution) and N = 10, 30, and 100, the bias-corrected values
(4.23) are 4.13, 3.23, and 3.06, respectively, and so the bias in (4.22) is downward.
MATLAB provides functions skewness and kurtosis to compute these statistics. The
function skewness(x, ﬂag) returns (4.20) when ﬂag = 1 (default when ﬂag is absent) and
(4.21) when ﬂag = 0. The function kurtosis behaves the same way. Both functions handle
matrices as for mean, and both treat NaNs as missing data.
MATLAB also includes the function moment(x, order), which returns the orderth
central sample moment of the rvs in x. The normalization is by N, so the function gives
the sample variance for order = 2. Note that moment gives only the numerator of (4.20) or
(4.22) and so does not generalize the skewness or kurtosis to higher orders.
A serious issue using the skewness, kurtosis, and moments higher than 4 with real data
(and especially earth sciences data) is robustness. As the order of a moment increases, the
inﬂuence of extreme data rises dramatically. For this reason, it is statistical best practice to
examine a given data set using some of the tools in Section 4.8 to characterize their
distribution and look for outlying data before blindly using the variance, skewness, or
kurtosis as summary statistics.
Example 4.3 Again returning to the Cavendish density of Earth data from Example 4.1,
compute and compare the skewness and kurtosis with and without the two smallest and
largest data with and without applying the bias correction.
data = importdata('cavendish.dat');
skewness(data)
ans =
0.1067
skewness(data, 0)
94
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

ans =
0.1126
kurtosis(data)
ans =
2.3799
kurtosis(data, 0)
ans =
2.4974
Note the ~10% effect that the bias correction has on the skewness. The skewness is
positive, implying that the right tail of the data distribution is longer than the left one,
which is consistent with Figure 4.1. The excess kurtosis (see Section 2.12) for the bias-
corrected sample kurtosis is about 0.6, so the data distribution is platykurtic, meaning
that it has a broad peak and short tails, again consistent with Figure 4.1.
data = sort(data);
data = data(3:n -2);
skewness(data)
ans =
0.4140
skewness(data, 0)
ans =
0.4409
kurtosis(data)
ans =
2.2672
kurtosis(data, 0)
ans =
2.3809
The effect of removing four data is substantial. The skewness shifts upward by about a
factor of 4, remaining positive and implying a long right tail, consistent with Figure 4.1.
The kurtosis changes downward slightly.
4.5 Estimators of Direction
When a set of rvs represents direction on a circle or sphere, the classical estimators for
mean or variance break down because of discontinuities (i.e., 359 is not 358 distant
from 1) and because circular or spherical distributions can be bimodal or multimodal
(e.g., consider paleomagnetic data from a rock formation that record both normal and
reversed polarities). The natural way to deal with directional data is via trigonometric
moments as in Section 2.13. Suppose that N rvs Xi
f
g are unit vectors with corresponding
95
4.5 Estimators of Direction
.005
13:29:40, subject to the Cambridge Core terms of use,

angles
θi
f g. Then the mean direction is the resultant of adding all of the unit vectors
together. The Cartesian coordinates of a given Xi are
cos θi; sin θi
ð
Þ, and their vector
resultants are
^C ¼
X
N
i¼1
cos θi
(4.24)
and similarly for ^S. Note that the ordering of the rvs in the resultants is immaterial
(i.e., exchangeability holds). The sample mean direction for θ is
θ ¼ tan 1
^S
^C
 !
(4.25)
The sample median direction is obtained by ﬁnding the value of ~θ that minimizes the
function
Φ ~θ
 
¼ π  1
N
X
N
i¼1
π  jθi  ~θj


(4.26)
Since (4.26) is not a smooth function of its argument, this can present numerical challenges.
Deﬁne ^R
2 ¼ ^C
2 þ ^S
2. The estimator ^R is the resultant length and lies in the range 0; N
½
.
The mean resultant length is R ¼ ^R=N and lies on 0; 1
½
. When R ¼ 1, all the rvs are
coincident, but R ¼ 0 does not always imply that the rvs are uniformly scattered around the
unit circle, and consequently, it is not useful as a measure of dispersion.
The sample circular variance is deﬁned as
^V ¼ 1  R
(4.27)
and has similar limitations to the mean resultant length. However, the circular standard
deviation is not the square root of (4.27) but rather is given by
^v ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 log 1  ^V


q
(4.28)
The kth noncentral trigonometric moment of the rvs is given by
^μ0
k ¼ ^C0
k þ i ^S0
k ¼ ^R0
kei^θ0
k
(4.29)
where
^C0
k ¼ 1
N
X
N
i¼1
cos k θi
ð
Þ
(4.30)
and similarly for ^Sk. Consequently,
^R0
k ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^C02
k þ ^S02
k
q
(4.31)
and
^θ0
k ¼ tan 1
^S
0
k
^C0
k
 
!
(4.32)
96
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

The kth central trigonometric moment of the rvs is calculated relative to the sample
mean direction (4.25)
^μk ¼ ^Ck þ i ^Sk ¼ ^Rkei^θk
(4.33)
where
^Ck ¼ 1
N
X
N
i¼1
cos k θi  θ




(4.34)
and similarly for the remaining estimators in (4.33). Using trigonometric identities, it can be
shown that the ﬁrst and second central trigonometric moments are given by ^μ 1 ¼ ^R1 ¼ R
and ^μ 2 ¼ ^R2 ¼ PN
i¼1 cos 2 θi  θ




=N. The sample circular dispersion is deﬁned as
^δ ¼ 1  ^R2


= 2R
2


(4.35)
and plays a role in hypothesis testing about directional data.
The sample skewness for circular data is given by
^s3 ¼ R2 sin θ2  2θ


1  R

3=2
(4.36)
and the sample kurtosis is
^s4 ¼ R2 cos θ2  2θ


 R
4
1  R

2
(4.37)
For the von Mises distribution (3.63), the best estimator (actually, the maximum likelihood
estimator, or mle, deﬁned in Section 5.4) for the mean direction ^μ is (4.25), while the mle
for the concentration parameter is obtained by solving
I1 ^κ
ð Þ
I0 ^κ
ð Þ ¼ R
(4.38)
for ^κ.
All of these estimators generalize to a spherical geometry, although the algebra
gets more complicated. For a three-dimensional (3D) rv, there are three Cartesian unit
vectors represented as
Xi; Yi; Zi
ð
Þ that will be transformed to polar coordinates
cos θ cos ϕ; cos θ sin ϕ; sin θ
ð
Þ, where θ is the latitude and ϕ is the longitude. The mean
direction on a sphere is obtained by summing the Cartesian unit vectors, computing the
resultant length, and obtaining the direction cosines ^sx ¼ PN
i¼1xi=^R, and so on. The mean
polar coordinates then follow as θ ¼ cos 1^sz and ϕ ¼ tan 1 ^sy=^sx


. MATLAB does not
provide support for directional data.
Example 4.4 The ﬁle paleocurrent.dat (taken from Fisher [1995, app. B.6]) contains
30 values of the cross-bed azimuths of paleocurrents measured in the Bedford Anticline
97
4.5 Estimators of Direction
.005
13:29:40, subject to the Cambridge Core terms of use,

in New South Wales, Australia. Evaluate the data set and compute the vector resultants,
sample mean direction, mean resultant length, circular variance, sample skewness, and
sample kurtosis. What do these tell you about the data? Fit a von Mises distribution to the
data and qualitatively evaluate the ﬁt.
The MATLAB graphic function rose(x) produces a circular histogram of the data
shown in Figure 4.2. The data are clustered around 250, and there may be outliers
below 180.
data=importdata('paleocurrent.dat');
rose(pi*data/180)
chat = sum(cosd(data))
chat =
-8.9432
shat = sum(sind(data))
shat =
-21.7155
thetabar = atand(shat/chat)
thetabar =
67.166
The values for shat and chat indicate that the mean direction is in the third quadrant, so
180 needs to be added to this value.
thetabar = thetabar + 180
thetabar =
247.6166
2
4
6
30
210
60
240
90
270
120
300
150
330
180
0
Figure 4.2
Rose diagram of the paleocurrent data.
98
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

This value is consistent with Figure 4.2.
rhat=sqrt(chat^2 + shat^2);
rbar = rhat/length(data)
rbar =
0.7828
vhat = 1 - rbar
vhat =
0.2172
c2hat = sum(cosd(2*(data - thetabar)))
c2hat =
10.9547
s2hat = sum(sind(2*(data - thetabar)))
s2hat =
0.7323
theta2bar = atand(s2hat/c2hat)
theta2bar =
3.8244
r2bar=sqrt(c2hat^2+s2hat^2)/length(data)
r2bar =
0.3660
skew = r2bar*sind(theta2bar - 2*thetabar)/(1 - rbar)^1.5
skew =
-2.7122
kurt = (r2bar*cosd(theta2bar - 2*thetabar) - rbar^4)/(1 - rbar)^2
kurt =
-13.0959
The skewness and kurtosis suggest a distribution that is heavier toward small values of
angle and that is platykurtic. A von Mises distribution ﬁt to the data has a mean direction of
thetabar, and the concentration parameter is easily determined using the nonlinear root
ﬁnder function fzero.
fun=@(x) besseli(1, x)./besseli(0, x) - rbar;
kappabar = fzero(fun, 1)
kappabar =
2.6757
histogram(data, 10, 'Normalization', 'pdf')
hold on
theta = 160:.1:340;
plot(theta, exp(kappabar*cosd(theta - thetabar))/   
(360*besseli(0, kappabar)))
ylabel('Probability Density')
xlabel('Angle')
99
4.5 Estimators of Direction
.005
13:29:40, subject to the Cambridge Core terms of use,

Figure 4.3 compares the ﬁtted von Mises distribution to a histogram of the data
with pdf normaliztion. The ﬁt is not particularly good, probably because the data set is
fairly small.
4.6 Estimators of Association
The unbiased sample covariance matrix computed from a matrix x
$ whose N rows are
observations and p columns are variables is
cov x
$
 
¼
x
$ jN x
$ =N

H

x
$ jN x
$ =N


= N  1
ð
Þ
(4.39)
where jN is a 1  N vector of ones, and the superscript H denotes the Hermitian
(i.e., complex conjugate) transpose so that (4.39) encompasses complex data. The Hermitian
transpose becomes the ordinary transpose when the rvs are real. The p  p covariance matrix
is Hermitian for complex and symmetric for real rvs. The quantity jN x
$ =N is a 1  p
vector of the sample means of the columns of x
$. The biased sample covariance matrix
obtains by replacing the numerator in (4.39) with N.
The function cov(x) returns the variance if x is a vector and the covariance matrix if x is a
matrix oriented as in (4.39). For matrices, diag(cov(x)) returns a vector of p variances.
When called with two vectors or matrices of the same size, cov(x, y) returns their
covariance. The default is the unbiased version, but cov(x, 1) returns the biased covariance
or second-moment matrix of the rvs about their mean. The function nancov ignores NaN
values and otherwise operates like cov. The function corrcov(x) converts the covariance
matrix in x into the correlation matrix.
The function corrcoef(x, y) returns the correlation coefﬁcient with the same behaviors as for
cov. It has additional options for hypothesis testing that will be described in Chapters 6 and 7.
200
250
300
Angle
0
0.002
0.004
0.006
0.008
0.01
0.012
Probability Density
Figure 4.3
Empirical pdf of the paleocurrent data compared to a von Mises distribution with parameters κ ¼ 2:6757,
μ ¼ 247:6166.
100
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

4.7 Limit Theorems
4.7.1 The Laws of Large Numbers
The laws of large numbers are one of the major achievements of probability theory, stating
that the mean of a large sample of rvs is close to the expected value of their distribution.
This is easy to demonstrate using MATLAB.
x = unidrnd(10, 1000, 1);
y = [];
for i = 1:1000
y = [y mean(x(1:i))];
end
plot(y)
hold on
x = 5.5*ones(size(y));
plot(x)
The MATLAB script draws 1000 numbers from the integers between 1 and 10 and then
computes the mean for 1 through 1000 variables. The result is shown in Figure 4.4. The
population mean is 5.5, and the sums converge to that value fairly rapidly (at about
180 rvs) and then ﬂuctuate around the population value.
There are two versions of the law of large numbers. The weak law of large numbers
states that if
Xi
f
g are iid with a common population mean μ, then XN !
p μ. In other
words, the distribution of XN becomes increasingly concentrated around μ as N increases.
The weak law of large numbers is easily proved using Chebyshev’s inequality.
The strong law of large numbers states that if
Xi
f
g are iid with a common population
mean μ, then XN !
as μ. The proof of the strong law is more complicated than that for the
weak law. If the variables are independent but not identically distributed, then it can be
shown that XN  E XN


!
as 0 under the condition that each rv has ﬁnite variance. This is
called Kolmogorov’s strong law of large numbers.
0
200
400
600
800
1000
Number of Draws
5
6
7
8
9
10
Running Mean
Figure 4.4
Running mean of 1000 random draws from the integers between 1 and 10. The dashed line is the population value of 5.5.
101
4.7 Limit Theorems
.005
13:29:40, subject to the Cambridge Core terms of use,

4.7.2 Classic Central Limit Theorems
The classic central limit theorem states that a large iid sample drawn from any distribution
with a population mean μ and ﬁnite variance σ2 will yield a sample mean XN that is
approximately normal with mean μ and variance σ2/N. More precisely, the Lindeberg-Lévy
form of the CLT is given by
XN  μ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
var XN


q
¼
ﬃﬃﬃﬃ
N
p
XN  μ
σ
!
d
N 0; 1
ð
Þ
(4.40)
This means that probability statements about the sample mean for rvs from any distribution
can be made using the Gaussian distribution provided that the sample is large enough. The
simplest proof of the classic CLT uses characteristic functions. A remaining issue is the
deﬁnition of large, which is difﬁcult to quantify.
Example 4.5 An oceanographer makes 25 independent measurements of the salinity of
seawater. She knows that the salinity data are identically distributed but does not know
what distribution they are drawn from and wants to know (1) a lower bound for the
probability that the average salinity will differ from the true mean by less than σ/5 and (2) a
CLT estimate of the probability.
From the Chebyshev inequality (2.77), Pr XN  μ

  t


 σ2= Nt2


. Let t ¼ σ=5.
Then Pr XN  μ

  σ=5


 σ2=25
ð
Þ= σ=5
ð
Þ2 ¼ 1. Therefore, Pr XN  μ

 < σ=5


¼ 0,
which is not a particularly useful bound.
From the central limit theorem (4.40),
Pr XN  μ

  σ=5


¼ Pr
ﬃﬃﬃﬃﬃ
25
p
XN  μ

=σ  1


¼ Φ 1
ð Þ  Φ 1
ð
Þ ¼ 2Φ 1
ð Þ  1 ¼ 0:6826
Example 4.6 Obtain 1000 realizations of 1000 random draws from the uniform distribution
on the interval [1, 1]. Plot the empirical pdf for 1, 10, 100, and 1000 point averages. Does
the CLT apply? Repeat for 100 realizations of 100 random draws from the Cauchy
distribution in standard form. Does the CLT apply? Why or why not?
E X
ð Þ ¼ 0 from the support of the uniform distribution. A MATLAB script and output
that evaluates the ﬁrst part is
x = unifrnd(-1, 1, 1000, 1000);
subplot(2, 2, 1); histogram(x(1, :))
subplot(2, 2, 2); histogram(mean(x(1:10, :)))
subplot(2, 2, 3); histogram(mean(x(1:100, :)))
subplot(2, 2, 4); histogram(mean(x))
102
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

The result is shown in Figure 4.5 and looks like the uniform distribution in the absence
of averaging but does look more Gaussian as the number of variables being averaged rises.
In addition, the variance decreases, so the distribution is tighter around the mean.
The Cauchy distribution is equivalent to the t distribution with one degree of freedom.
The MATLAB script is
x = trnd(1, 100, 100);
subplot(1, 3, 1); histogram(x(1, :));
subplot(1, 3, 2); histogram(mean(x(1:10, :)));
subplot(1, 3, 3); histogram(mean(x));
The result is shown in Figure 4.6. The peak is not increasingly concentrated as the
number of data rises, and in fact, the reverse is true, because more extreme values appear
when the averaging rises. The problem is that μ and σ2 are undeﬁned for the Cauchy
distribution, so the classic CLT does not apply.
A more general form of the classic CLT is the Lyapunov CLT. Let
Xi
f
g be N independ-
ent but not necessarily identically distributed rvs, each of which has mean μi and
variance σ2
i . Under certain conditions on the central moments higher than 2, this version
of the CLT is
P
N
i¼1
X i  μ i
ð
Þ
P
N
i¼1
σ2
i
!
d
N 0; 1
ð
Þ
(4.41)
–1
–0.5
0
0.5
1
0
50
100
150
–1
–0.5
0
0.5
1
0
50
100
150
–0.3
–0.2
–0.1
0
0.1
0.2
0
100
200
–0.1
–0.05
0
0.05
0.1
0
50
100
150
Figure 4.5
Empirical pdfs for 1000 realizations of 1000 random draws from the uniform distribution on [1, 1] averaged
1 time (upper left), 10 times (upper right), 100 times (lower left), and 1000 times (lower right).
103
4.7 Limit Theorems
.005
13:29:40, subject to the Cambridge Core terms of use,

The Lyapunov CLT states that the sum of independent but not necessarily identically
distributed rvs is approximately Gaussian with mean PN
i¼1μ i and variance PN
i¼1σ2
i . When
the rvs are identically distributed, then this reduces to (4.40). In practice, if the third central
moment exists, then the Lyapunov CLT holds.
The CLTs provide an explanation for the empirical observation that the distribution of
many physical rvs is Gaussian. However, the CLT should be used with caution as a
justiﬁcation for treating a problem with Gaussian statistics. In particular, in the earth
sciences, it is very common to have data that are mostly Gaussian but that have a small
fraction of extreme data (in the sense that they would not be expected under a Gaussian
model) that often are drawn from distributions with inﬁnite variance, such as the general-
ized extreme value distribution or the stable distribution family. In that instance, the classic
CLT does not apply because the data variance is not ﬁnite.
Note that these CLTs give an asymptotic distribution for the sample mean. Convergence
is most rapid when the data values are near the Gaussian peak and is much slower in the
tails. However, if the third central moment of the rvs exists, then the Berry-Esseen theorem
shows that convergence occurs at least as fast as 1=
ﬃﬃﬃﬃ
N
p
. This is not particularly rapid; note
that the sample variance converges to the population value as 1/N.
4.7.3 Other Central Limit Theorems
There are specialized CLTs for the order statistics and directional data, and in addition,
there is a generalized CLT that applies to rvs with inﬁnite variance.
Consider the rth order statistic X rð Þ from a sample of size N obtained from a continuous
cdf F x
ð Þ. Let xp ¼ F1 p
ð Þ be the p-quantile, where lim N!∞r=N ¼ p. Under these
assumptions, the CLT for the order statistics is
–100
0
100
0
10
20
30
40
50
60
–200
0
200
0
10
20
30
40
50
60
–50
50
0
0
10
20
30
40
50
Figure 4.6
Empirical pdfs for 100 realizations of 100 random draws from the standardized Cauchy distribution averaged (from
left to right) 1, 10, and 100 times.
104
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N
p 1  p
ð
Þ
s
f xp


X rð Þ  xp


!
d
N 0; 1
ð
Þ
(4.42)
where f x
ð Þ is the pdf corresponding to F x
ð Þ. Equation (4.42) was proved by Mosteller
(1946).
The CLT for directional rvs is a specialization of the CLT for bivariate rvs. Deﬁne
C1 ¼ ^C=N where ^C is given by (4.24) and similarly for S1. Let C1 and S1 denote the
population values for C1 and S1. Then, the CLT for circular data is
C1; S1


!
d
N2
C1; S1
ð
Þ; Σ
$ =N
h
i
(4.43)
Where N2 is the bivariate Gaussian distribution that was described in Section 3.4.10, and Σ
$
is the covariance matrix whose elements can be expressed in terms of the population
noncentral trigonometric moments
Σ
$¼
1 þ C0
2  2 C0
1

2
h
i
=2
S0
2  2C0
1S0
1


=2
S0
2  2C0
1S0
1


=2
1  C0
2  2 S0
1

2
h
i
=2
8
<
:
9
=
;
(4.44)
Let Xi
f
g be a set of iid variables drawn from F x
ð Þ that possibly possesse inﬁnite mean
and/or inﬁnite variance. The cdf F x
ð Þ is in the domain of attraction of a stable distribution
with 0 < α < 2 if there exist constants aN > 0 and bN 2 R such that (Feller 1971, chap. 6)
P
N
i¼1
Xi  bN
aN
!
d
st α; β; 1; 0
ð
Þ
(4.45)
In other words, a distribution has a domain of attraction if and only if it is stable. Further, the
norming constant aN is proportional to N1=α. The generalized central limit theorem holds
that the sum of rvs drawn from symmetric distributions with power law tails proportional to
1  α, where 0 < α < 2, will converge to a symmetric stable distribution with tail
thickness parameter α. When α > 2, the sum of rvs converges to a Gaussian distribution.
4.7.4 The Delta Method
The delta method is a way to provide an approximation to the distribution of a function of a
variable that has an asymptotic Gaussian distribution and hence builds on the CLT.
Suppose that the classic CLT (4.40) holds for a set of rvs
Xi
f
g with sample mean XN.
Let η x
ð Þ be a function. The delta method holds that
ﬃﬃﬃﬃ
N
p
η XN


 η μ
ð Þ
σ η0 μ
ð Þ
!
d N 0; 1
ð
Þ
(4.46)
The proof follows by expanding η XN


in a ﬁrst-order Taylor series to get
η XN


	 η μ
ð Þ þ η0 μ
ð Þ XN  μ


(4.47)
105
4.7 Limit Theorems
.005
13:29:40, subject to the Cambridge Core terms of use,

The terms can be rearranged to yield
XN  μ 	 η XN


 η μ
ð Þ
η0 μ
ð Þ
(4.48)
Substituting into (4.40) gives (4.46).
Example 4.7 Suppose that Xi
f
g are a random sample from a Poisson distribution. Remem-
bering that the mean and variance of the Poisson distribution are identical, the classic CLT
states that
ﬃﬃﬃﬃ
N
p
XN  λ


=
ﬃﬃﬃ
λ
p
is approximately standard Gaussian for large N. Consider the
function η x
ð Þ ¼ x2. Applying the delta method, XN

2 is approximately normal with mean
λ2 and variance
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N=λ2
q
=2.
4.8 Exploratory Data Analysis Tools
4.8.1 The Probability Integral Transform
An rv X has a continuous distribution for which the cdf is F x
ð Þ. Deﬁne a new rv U through
the transformation
u ¼ F x
ð Þ
(4.49)
Then U has the uniform distribution on [0, 1]. Equation (4.49) is the probability integral
transform. Its inverse is
x ¼ F1 u
ð Þ
(4.50)
because both the cdf and the quantile function are monotone. The proof of the probability
integral transformation is straightforward
Pr F1 u
ð Þ  x


¼ Pr u  F x
ð Þ
½

¼ F x
ð Þ
(4.51)
where the last step holds because Pr u  y
ð
Þ ¼ y for a uniformly distributed rv. The
probability integral transform is important for computing random variables from arbitrary
distributions, in goodness-of-ﬁt testing, and in plotting data against a target distribution.
A simple way to generate random variables from any distribution is inverse transform
sampling, in which a random number is generated from the uniform distribution on [0, 1]
using one of several algorithms that will not be described and applying the inverse
106
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

probability integral transform (4.50) to get random numbers from F x
ð Þ. As a demonstration
that this works, it is easy to generate a set of uniform rvs in MATLAB and apply the
inverse transform to get Rayleigh rvs, with the result shown in Figure 4.7.
x = unifrnd(0, 1, 10000, 1);
y = raylinv(x, 1);
h = histogram(y, 50, 'Normalization', 'pdf');
hold on
xx = h.BinEdges + h.BinWidth/2;
plot(xx, raylpdf(xx, 1))
A more advanced method to generate random variables from any distribution is the
acceptance-rejection method, which is especially applicable when the target pdf f x
ð Þ is
difﬁcult to compute. Let g x
ð Þ be an auxiliary or instrumental pdf such that f x
ð Þ < αg x
ð Þ,
where α > 1 is an upper bound on the ratio f x
ð Þ=g x
ð Þ. Samples are taken from αg x
ð Þ and
accepted or rejected based on a probability test. For example, an implementation due to
Von Neumann (1951) uses samples X from g x
ð Þ and U from the uniform distribution,
computation of f x
ð Þ= αg x
ð Þ
½
, and then testing u < f x
ð Þ= αg x
ð Þ
½
. If the test holds, then x is
accepted as a random draw from f x
ð Þ and is otherwise rejected. The limitations of
acceptance-rejection sampling are that a large number of rejections ensue unless α is
chosen accurately or if f x
ð Þ is sharply peaked, and inefﬁciency as the dimensionality of
the distribution rises.
4.8.2 The Histogram and Empirical CDF
Let Xi
f
g be a data sample of size N. Deﬁne a set of r bins bi that partition the data range
min xi
ð Þ; max xi
ð Þ
½
 according to some rule
bL
1; bU
1


, :::, bL
r ; bU
r


. For example, the rule
could partition the data range into equal-sized intervals. The midpoints of the bin partitions
are called the bin centers bL
1 þ bU
1


=2, :::, bL
r þ bU
r


=2, and their common boundaries are
0
0
0.2
0.4
0.6
0.8
1
2
3
4
Figure 4.7
Probability histogram of 10,000 random draws from a standardized Rayleigh distribution using the inverse
probability integral transform compared with the Rayleigh pdf (solid line).
107
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

called bin edges bU
1 ¼ bL
2, :::, bU
r1 ¼ bL
r . A frequency histogram is a plot of the counts of
data in each deﬁned bin against a characteristic of the bins such as the bin centers or, more
commonly, a bar plot of the data counts against the bin ranges. A relative frequency
histogram comprises the data counts normalized by the number of data against the bin
characteristics. Other normalization algorithms can also be applied.
MATLAB implements the frequency histogram as the function histogram(x), and it has
been used repeatedly since Chapter 1. It produces a bar plot with the bins uniformly
distributed across the data range by default. The number of bins can be controlled using a
scalar second-argument nbins, and the bin edges can be managed by providing an alterna-
tive vector second-argument edges. In addition, a series of name-value pairs can be used to
more ﬁnely control the binning algorithm. A useful one is “Normalization” followed by
“count” to produce a frequency histogram (default), “probability” to produce a relative
frequency histogram, and “pdf,” where the height of each bar is the number of observations
divided by N times the width of each bin. There are additional options to produce other
types of plots. There are numerous other name-value pairs that provide considerable
ﬂexibility to the user.
Let Xi
f
g be an iid sample of size N from a cdf F x
ð Þ, and rank them to obtain the order
statistics (4.11). A nonparametric sample or empirical cdf ^F N x
ð Þ can be constructed by
deﬁning it to be the proportion of the observed values that are less than or equal to x. The
empirical cdf is easily constructed in terms of the order statistics as
^F N x
ð Þ ¼
0
x < x 1
ð Þ
k
N
x k
ð Þ  x < x kþ1
ð
Þ
1
x N
ð Þ > x
(4.52)
An alternative but equivalent deﬁnition of the empirical cdf obtains from
^F N x
ð Þ ¼
P
N
i¼1
1 xi  x
ð
Þ
N
(4.53)
In either case, the result is a monotonically increasing set of step functions with step size
1/N assuming that the data are unique and larger steps if they are not.
The empirical cdf is useful for exploratory data analysis because it approximates the true
cdf increasingly well as the number of data increases and hence provides critical input
about the data distribution needed for statistical inference. It can also be compared to
standard cdfs such as those described in Chapter 3 to assess whether the data are consistent
with a given distribution.
MATLAB provides the function [ f, x] = ecdf(y) that produces the empirical cdf in f at
the values in x from the data in y. If it is called without the left-hand side, ecdf produces a
plot of the empirical cdf. MATLAB also implements the function ecdfhist( f, x, m) that
produces a probability histogram (hence an empirical pdf) from the output of ecdf using m
bins (which defaults to 10 if not provided). Alternately, histogram will create an empirical
cdf if the name-value pair “Normalization,” “cdf”is used.
108
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

Example 4.8 Obtain 100 and 1000 random draws from the standard exponential distribu-
tion, and plot the empirical cdf and pdf for each.
x100 = exprnd(1, 100, 1);
x1000 = exprnd(1, 1000, 1);
[f1, x1 ] = ecdf(x100);
[f2, x2 ] = ecdf(x1000);
plot(x1, f1, x2, f2)
ecdfhist(f1, x1, 20)
ecdfhist(f2, x2, 20)
Figures 4.8 and 4.9 show the results. The empirical cdfs and pdfs are much smoother
with the larger number of samples, which ought not to be surprising. Figure 4.10 shows the
empirical pdf computed using histogram and is identical to Figure 4.9.
Because it depends on a random sample, the empirical cdf is itself an rv and hence has a
probability distribution. It is readily apparent that k ¼ N ^F N x
ð Þ e bin k; N; F x
ð Þ
ð
Þ, so
Pr ^F N x
ð Þ


¼ k
N ¼ 1
N
N
k


F x
ð Þ
½
k 1  F x
ð Þ
½
Nk
(4.54)
for k = 0,. . .,N, and hence the pdf of the empirical cdf is different for each value of k.
Equation (4.54) is bin k; N; F x
ð Þ
½
=N. The expected value and variance follow directly from
the characteristics of the binomial distribution described in Section 3.3.2, yielding
E ^FN x
ð Þ


¼ F x
ð Þ and var ^FN x
ð Þ


¼ F x
ð Þ 1  F x
ð Þ
½
=N. Consequently, the empirical
cdf is an unbiased estimator for the population cdf.
Using Chebyshev’s inequality (2.77), it follows that
Pr
^FN x
ð Þ  F x
ð Þ

  t


 F x
ð Þ 1  F x
ð Þ
½

Nt2
(4.55)
00
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
Figure 4.8
The empirical cdf computed from 100 (solid line) and 1000 (dashed line) random draws from the standardized
exponential distribution.
109
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

so ^FN x
ð Þ !
p F x
ð Þ. In fact, the stronger statement ^FN x
ð Þ !
as F x
ð Þ holds because of the
strong law of large numbers. An even stronger statement follows from the Glivenko-
Cantelli theorem, supx FN x
ð Þ  F x
ð Þ
j
j !
as 0, where supx is the supremum or the least
element that is greater than or equal to all the elements (or, equivalently, the least upper
bound). This shows that convergence of the empirical cdf to the true one occurs uniformly.
Consequently, the empirical cdf serves as a well-behaved estimator for the population cdf.
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
0
2
4
6
8
Figure 4.9
Empirical pdfs computed using ecdfhist for 100 (left) and 1000 (right) random draws from the standardized
exponential distribution.
0
10
20
30
40
50
0
2
4
6
8
0
50
100
150
200
250
300
0
2
4
6
8
Figure 4.10
Empirical pdfs computed using histogram for 100 (left) and 1000 (right) random draws from the standardized
exponential distribution.
110
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

A much tighter bound than (4.55) can be placed using Hoeffding’s inequality (2.79)
because the empirical cdf is a binomial variable:
Pr FN x
ð Þ  F x
ð Þ
j
j  t
½
  2e2Nt2
(4.56)
From the classic CLT, it follows that the asymptotic distribution of the empirical cdf is
Gaussian because
ﬃﬃﬃﬃ
N
p
^FN x
ð Þ  F x
ð Þ


!
d
N 0; F x
ð Þ 1  F x
ð Þ
½

f
g
(4.57)
However, given the existence of the exact distribution for the empirical cdf and the ease
with which binomial entities can be computed, it makes limited sense to use the asymptotic
form (4.57).
4.8.3 Kernel Density Estimators
Kernel density estimators differ from regular or probability histograms in that they produce a
smoothed rather than a binnedrepresentationof a distribution.Let Xi
f
g bea set of iid rvs drawn
from an unknown pdf f x
ð Þ. The kernel density estimator that represents the shape of f x
ð Þ is
^f δ x
ð Þ ¼ 1
Nδ
X
N
i¼1
K x  xi
δ


(4.58)
where K x
ð Þ is a kernel function that is symmetric and integrates to unity over its support,
and δ > 0 is the smoothing bandwidth. The smoothing bandwidth is a free parameter, and
it is intuitive to make it as small as possible, but there is a tradeoff between bias and
variance that must also be considered. A wide range of kernel functions is in common use,
but the standard Gaussian works well in most cases. Alternatives include the Epanechnikov
K x
ð Þ ¼ 3 1  x2
ð
Þ=4 on [1, 1], which is optimally minimum variance, and the half cosine
K x
ð Þ ¼ π cos πx=2
ð
Þ=4 on [1, 1], among others.
The choice of bandwidth has a signiﬁcant effect on the result, as will be shown later. For
a Gaussian kernel and Gaussian data, the optimal value of the bandwidth is
δ ¼
4^s5
N
3N

1=5
(4.59)
More complex analyses in terms of mean integrated squared error can be performed, but in
the end they depend on the unknown f x
ð Þ and hence are at best approximations.
MATLAB implements the kernel density estimator as ksdensity(x). It produces a plot if no
arguments are returned and otherwise returns the ordinate and abscissa as [f, xi] = ksdensity(x).
By default, the function evaluates the kernels at 100 equally spaced points that cover the range
of x and uses a Gaussian kernel function. However, there are a wide range of options:
1. Adding a third parameter bw to the left-hand side returns the bandwidth.
2. Adding the argument “support” followed with “unbounded” (default) or “positive” or
[a, b] speciﬁes the support of the pdf.
111
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

3. Adding the argument “kernel” followed by “normal” (default), “box,” “triangle,” and
“epanechnikov” chooses the kernel function. Alternately, a function handle to a user-
deﬁned kernel function can be provided.
4. Adding the argument “function” followed by “pdf” (default), “cdf,” “icdf,” “survivor,”
or “cumhazard” chooses the type of distribution function to ﬁt.
5. Adding the argument “npoints” followed by a number speciﬁes the number of equally
spaced points where the kernel function will be centered. The default is 100.
Example 4.9 Obtain 10,000 random draws, with half from N 0; 1
ð
Þ and half from N 3; 1
ð
Þ,
compute the kernel density estimator for the data, and then vary the bandwidth and
compare with the actual distribution.
x = normrnd(0, 1, 1, 5000);
x = [x normrnd(3, 1, 1, 5000)];
[f, xi, bw ] = ksdensity(x);
plot(xi,f)
bw
bw =
0.3716
hold on
[f, xi ] = ksdensity(x, 'bandwidth', 0.1);
plot(xi, f, 'k--')
[f, xi ] = ksdensity(x, 'bandwidth', 1);
plot(xi, f, 'k:')
plot(xi, 0.5*(normpdf(xi, 0, 1) + normpdf(xi, 3,1)), 'k-.')
In Figure 4.11, the default bandwidth produces a reasonable approximation to the
actual pdf and resolves the bimodality well. When the bandwidth is too low, the pdf
–10
–5
0
5
10
0
0.05
0.1
0.15
0.2
0.25
Figure 4.11
Kernel density pdf estimates computed for 5000 N(0, 1) + 5000 N(3,1) random draws using a Gaussian kernel
function with default bandwidth of 0.3716 (solid line), 0.1 (dashed line), and 1.0 (dotted line) compared with the
actual pdf evaluated at the kernel density estimator abscissa values (dash-dot line).
112
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

appears erratic, whereas if it is too high, it fails to show the bimodality. The actual pdf
appears as a set of line segments because there are only 100 points along the abscissa.
Example 4.10 Obtain 10,000 random draws from the arcsine distribution, and compute the
pdf with support that is unbounded and conﬁned to [1, 1]. The arcsine distribution is the
beta distribution with parameters ½, ½. Repeat for the cdf.
x = betarnd .5, .5, 10000, 1);
[f, xi ] = ksdensity(x);
plot(xi, f)
hold on
[f, xi ] = ksdensity(x, 'support', [0 1 ]);
plot(xi, f, 'k--')
plot(xi, betapdf(xi, .5, .5), 'k:')
In the absence of any information about the support of the distribution, tapered tails
appear at the bottom and top of the distribution in Figure 4.12, and it violates the properties
of the beta distribution. When the support is limited to [0, 1], the distribution looks almost
like the analytic form. However, the left and right endpoints of the arcsine distribution are
inﬁnite, and there is a mismatch between the kernel density and the exact pdf at these
points that is inevitable for a numerical result. Consequently, using the kernel density
estimator for any sort of quantitative inference could be dangerous, and the kernel density
estimator certainly will not integrate to unity. Further, forcing it to integrate to unity will
introduce a large mismatch into the center of the distribution.
The kernel density estimator for the cdf in Figure 4.13 exceeds the lower and upper
support bounds in the absence of constraints but agrees very well with the analytic values
once they are in place.
–0.5
0
0.5
1
1.5
0
1
2
3
4
5
Figure 4.12
Random draws from the arcsine distribution ﬁt by a kernel density estimator for the pdf using a Gaussian kernel
function with unbounded support (solid line) and support bounded to [0,1] (dashed line) compared with the actual
pdf (dotted line).
113
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

Example 4.11 Obtain 10,000 random draws from the Cauchy distribution, and compute a
kernel density estimator for them. The Cauchy distribution is equivalent to the t distribu-
tion with one degree of freedom.
Figure 4.14 shows the result. One would anticipate difﬁculty because the Cauchy
distribution has algebraic tails, and hence a small number of kernel density estimates
evenly distributed across the support is not adequate for its characterization. The top
–0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
Figure 4.13
Random draws from the arcsine distribution ﬁt by a kernel density estimator for the cdf using a Gaussian kernel
function with unbounded support (solid line) and support bounded to [0,1] (dashed line) compared with the actual
cdf (dotted line).
–4
–2
0
2
4
´ 104
0
2
4 ´ 10–4
–500
0
500
0
2
4 ´ 10–4
–4
–2
0
2
4
´ 104
0
0.1
0.2
0.3
50
0
–50
0
0.1
0.2
0.3
Figure 4.14
Kernel density estimators for 10,000 random draws from the Cauchy distribution using the default 100 points
estimated evenly across the support (top left), the center of that kernel density estimate (top right), and the kernel
density estimator using 10,000 points estimated evenly across the support (bottom left) and the center of that
estimate (bottom right).
114
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

two panels of the ﬁgure conﬁrm this assertion. Increasing the number of kernel density
estimates to 10,000 produces a much cleaner result with the penalty of more
computation.
4.8.4 The Percent-Percent and Quantile-Quantile Plots
The percent-percent or p-p plot is a very useful qualitative tool for assessing the distribu-
tion of a data sample. It can be made quantitative by adding error bounds and assessing the
goodness-of-ﬁt, as will be described in Chapter 7. The p-p plot is based on the probability
integral transform of Section 4.8.1, consisting of a plot of the uniform quantiles against the
order statistics of a data set transformed using a target cdf. If the target cdf is the correct one
for the data, then the plot should approximate a straight line. Deviations from a straight line
are suggestive of the nature of the distribution and hence can be used to reﬁne the target
distribution. The p-p plot is most sensitive at the mode of the distribution and hence is
appropriate for evaluating heavy-tailed distributions but is not very useful for detecting
outliers.
Let
X ið Þ


be the N-order statistics computed for a set of rvs, and let the quantiles of the
uniform distribution be
ui ¼ i  1=2
ð
Þ=N
i ¼ 1, :::, N
(4.60)
If F∗x
ð Þ is a target cdf of the location-scale type, then the p-p plot consists of (4.60) plotted
against F∗
x ið Þ  ^μ


=^σ


, where ^μ and ^σ are location and scale estimates.
A useful variant on the p-p plot was introduced by Michael (1983). The standard p-p plot
has its highest variance near the distribution mode and its lowest variance at the tails. An
arcsine transformation applied to (4.60) gives
ri ¼ 2 sin 1
ﬃﬃﬃﬃui
p
ð
Þ=π
(4.61)
whose probability distribution is
ri e π sin rð Þ=2
(4.62)
for 0  r  1. The order statistics of data distributed as (4.62) have the same asymptotic
variance. Consequently, use of a sine transformation realizes a stabilized p-p plot that has
constant variance at all points. The variance-stabilized p-p plot consists of (4.61) plotted
against
si ¼ 2
π sin 1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
F∗
x ið Þ  ^μ


=^σ


q
(4.63)
The variance-stabilized p-p plot is superior to the ordinary p-p plot and is recommended for
standard use. MATLAB does not provide support for either the standard or variance-
stabilized p-p plot.
115
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

Example 4.12 Compute 1000 random draws from the standardized Cauchy distribution, and
produce stabilized p-p plots of them against the standardized normal and Cauchy distribu-
tions. What differences do you see?
x = trnd(1, 1, 1000);
u = ((1:1000) - 0.5)/1000;
r = 2/pi*asin(sqrt(u));
s = 2/pi*asin(sqrt(normcdf(sort(x))));
subplot(1, 2, 1); plot(r, s, 'k+ ')
axis square
s = 2/pi*asin(sqrt(tcdf(sort(x), 1)));
subplot(1, 2, 2); plot(r, s, 'k+ ')
axis square
Figure 4.15 shows the result. The stabilized p-p plot against the normal cdf is long tailed, as
evidenced by a sharp downturn at the distribution tails, and there are more subtle differences
near the distributionmode.Theplot usingthe Cauchy as thetarget is a straight line, as expected.
Example 4.13 Compute 1000 random draws from the standard Rayleigh distribution.
Produce variance-stabilized p-p plots using the lognormal and generalized extreme value
distributions with mle parameters obtained from lognﬁt and gevﬁt and the standard
Rayleigh cdf. What are the differences, and why?
Figure 4.16 shows the variance-stabilized p-p plots. The lognormal distribution has a
deﬁcit of probability at its lower end and an excess of probability around the mode
compared with the Rayleigh. It then has a deﬁcit of probability above the mode but
0
0.2
0.4
0.6
0.8
1
Sine Quantiles
0
0.2
0.4
0.6
0.8
1
Transformed Data
0
0.2
0.4
0.6
0.8
1
Sine Quantiles
0
0.2
0.4
0.6
0.8
1
Transformed Data
Figure 4.15
Variance-stabilized p-p plots for 1000 random draws from the standard Cauchy distribution using the standard
Gaussian (left) and the standard Cauchy (right) as the target.
116
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

ultimately has a fatter tail. Consequently, the initial slope is less than 1 and then increases
toward the mode, resulting in increasing slope, but the slope decreases beyond that point
and then turns upward. The generalized extreme value distribution is very similar to the
Rayleigh but has a slight deﬁcit of probability at the bottom of the distribution followed by
an excess around the mode. This results in subtle upward curvature at the bottom of the
distribution and downward curvature at the middle. The Rayleigh plot is, of course, a
straight line. Figure 4.17 shows the corresponding pdfs.
The quantile-quantile or q-q plot is a very useful qualitative tool for assessing the distribu-
tion of a data sample. It can be made quantitative by adding error bounds and assessing the
goodness-of-ﬁt, as will be described in Chapter 7. It consists of a plot of the N quantiles of a
target distribution against the order statistics computed from the sample, where the latter may
be scaled for easier interpretation. The target distribution quantiles are given by the target
quantile function applied to the uniform quantiles (4.60), and the order statistics follow from a
0
0.5
1
Sine Quantiles
0
0.2
0.4
0.6
0.8
1
Transformed Data
0
0.5
1
Sine Quantiles
0
0.2
0.4
0.6
0.8
1
0
0.5
1
Sine Quantiles
0
0.2
0.4
0.6
0.8
1
Figure 4.16
Variance-stabilized p-p plots for 1000 random draws from the standard Rayleigh distribution plotted against a
target standard lognormal distribution (left), a target standard generalized extreme value distribution (middle), and
a standard Rayleigh target distribution (right).
00
2
4
6
8
0.2
0.4
0.6
0.8
Figure 4.17
Probability density functions for the three target distributions of Figure 4.16. The standard Rayleigh pdf is shown as
a solid line, the lognormal with mle parameters is shown as a dashed line, and the generalized extreme value
distribution with mle parameters is shown as a dotted line.
117
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

sort and optional scaling of the data. The q-q plot emphasizes the distribution tails and hence is
a complement to the p-p plot that is very useful for studying light-tailed data and detecting
outliers, but it is less useful for systematically heavy-tailed data. If a q-q plot is a reasonable
approximation to a straight line, then the data are drawn from the distribution corresponding to
the quantile function. Curvature or other departures from linearity are clues to deﬁning the
correct data distribution. Finally, a small fraction of unusual data will typically plot as
departures from linearity at the extremes of the q-q plot, and hence it is a useful tool to detect
data that are not consistent with the bulk of the sample.
MATLAB provides the function qqplot(x) that produces a plot of the standard normal
quantiles against the order statistics of the data in x. It also allows two samples to be compared
using qqplot(x, y). For other distributions, a q-q plot is easily obtained from the script
n = length(x)
u = ((1:n) - 0.5)/n;
q = expinv(p);
plot(q, sort(x), '+ ')
where any quantile function can be substituted for the exponential one. The probability as
deﬁned in the ﬁrst line ascribes 1= 2N
ð
Þ to the ﬁrst and last intervals and 1=N to the
remaining ones so that sum(u) = 1.
Example 4.14 Compute 1000 random draws from the standardized Cauchy distribution, and
produce q-q plots of them against the standardized normal and Cauchy distributions. What
differences do you see?
u = ((1:10000) - 0.5)/10000;
x = trnd(1, 1, 10000);
subplot(1, 2, 1); plot(norminv(u), sort(x), 'k+')
axis square
subplot(1, 2, 2); plot(tinv(u, 1), sort(x), 'k+')
axis square
Figure 4.18 shows the results. The Cauchy distribution has algebraic tails, whereas the
Gaussian has exponential ones; hence the q-q plot against normal quantiles shows sharp
upward curvature at the tail extremes. Note that the q-q plot is dominated by a small
number of extreme values in this case and that the sense of curvature caused by long tails
has the opposite sense for q-q than for p-p plots (i.e., compare Figures 4.15 and 4.18). The
Cauchy q-q plot is close to a straight line, but note that the bulk of the data appears over a
small part of the center of the plot. This is a limitation of the q-q plot for heavy-tailed data.
Example 4.15 Compute 1000 random draws from the standard Rayleigh distribution.
Produce q-q plots using the lognormal and generalized extreme value distributions with
mle parameters and the standard Rayleigh cdf. What are the differences, and why?
118
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

Figure 4.19 shows the result. The lognormal distribution initially has a surplus of
probability followed by a deﬁcit of probability that is manifest as an initial steep positive
slope that then decreases. The generalized extreme value distribution exists for negative
argument, although this is masked in the p-p plot of Figure 4.15. An initial probability
deﬁcit is seen as upward curvature at the bottom of the distribution. The Rayleigh plot is a
straight line.
In many instances, the outcome of a statistical procedure requires the identiﬁcation and
elimination of anomalous data. This process is called censoring, and accurate statistical
–4
–2
2
4
0
Normal Quantiles
–2
–1.5
–1
–0.5
0
0.5
1
Sorted Data
–1
–0.5
0
0.5
1
Cauchy Quantiles
´ 104
–2
–1.5
–1
–0.5
0
0.5
1 ´ 104
´ 104
Figure 4.18
Quantile-quantile plots for 10,000 random draws from the standard Cauchy distribution using the standard Gaussian
cdf as the target (left) and the standard Cauchy cdf as the target (right).
10
5
0
4
2
0
5
0
Log Normal Quantiles
0
1
2
3
4
Sorted Data
–5
GEV Quantiles
0
1
2
3
4
Rayleigh Quantiiles
0
1
2
3
4
Figure 4.19
Quantile-quantile plots for 1000 random draws from the standard Rayleigh distribution plotted against a target
lognormal distribution with mle parameters (left), a target generalized extreme value distribution with mle
parameters (middle), and a standard Rayleigh target distribution (right).
119
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

inference requires that the distribution of the data must be truncated to reﬂect it. Let fX x
ð Þ
be the pdf of a random variable X before censoring. After truncation, the pdf of the
censored random variable X0 becomes
fx0 x0
ð Þ ¼
fx x
ð Þ
Fx d
ð Þ  Fx c
ð Þ
(4.64)
where c  x0  d, and FX x
ð Þ is the cdf for X. Let the original number of rvs in X be N, the
number of data in X0 after censoring be m, and the number of data censored from the lower
and upper ends of the distribution be m1 and m2, respectively, so that m ¼ N  m1  m2.
Then suitable choices for c and d are the m1th and N – m2th quantiles of the original
distribution fX x
ð Þ. The m quantiles of the truncated distribution can be computed from that
of the original distribution using
F1
x
Qj


¼ Fx d
ð Þ  Fx c
ð Þ
½
 j  1=2
m
þ Fx c
ð Þ
(4.65)
Equation (4.65) should be used to produce p-p or q-q plots if data have been censored.
4.8.5 Simulation
In many cases, the distribution of combinations of variables like products or quotients are
of theoretical interest, but in most cases, obtaining an analytic expression for the distribu-
tion is intractable. In such a case, a qualitative or quantitative understanding can be
achieved by simulation.
Example 4.16 Suppose that the distribution of the ratio of a standardized Rayleigh to a
standardized Gaussian rv is of interest. By drawing an equally sized set of Rayleigh and
Gaussian rvs and then computing their point-by-point quotient, a set of rvs is generated
from the distribution of interest and can then be characterized using a kernel density
estimator or other assessment tool.
x = raylrnd(1, 10000, 1);
y = normrnd(0, 1, 10000, 1);
z = x./y;
ksdensity(z, 'npoints', 10000)
Figure 4.20 shows the result truncated at abscissa values of 
75 (the actual simulation
extends from 2500 to 17,500). The resulting pdf is very heavy tailed and sharply peaked
at the origin, and even with 10,000 kernel centers, the middle of the distribution is not
adequately characterized.
This example illustrates an important principle: in many cases the ratio of two rvs will
result in a distribution with algebraic tails and inﬁnite variance. This occurs because the
ratio of two distributions is the same as the product of a ﬁrst distribution with the inverted
form of a second distribution. Lehmann & Schaffer (1988) reviewed the characteristics of
120
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

inverted distributions, the most relevant of which is a proof that the inverted probability
density function g y
ð Þ of the random variable Y ¼ 1=X given the original distribution f x
ð Þ
has the property
lim
y!∞
g y
ð Þ
1= 1 þ y2
ð
Þ ¼ f 0þ
ð
Þ
(4.66)
Consequently, when the left side of (4.66) tends to a ﬁnite constant, the inverted distribu-
tion has a right Cauchy tail, and it is lighter or heavier than Cauchy when f 0þ
ð
Þ is zero or
inﬁnity. An analogous relationship holds for the left tail. This establishes Cauchy tails
(hence inﬁnite mean and variance) for the inverted distributions of many common random
variables such as Gaussian, Student’s t, and exponential ones. However, when f 0þ
ð
Þ ¼ 0,
the inﬁnite variance property does not always hold; suitable examples with ﬁnite variance
include many inverted gamma and beta distributions.
Repeating the simulation for the product rather than the ratio of the two distributions
gives a well-behaved distribution with exponential tails, as in Figure 4.21, and uses the
default value of 100 for the number of kernel centers.
–60
60
–40
40
–20
20
0
0
0.05
0.1
0.15
Simulated PDF
Figure 4.20
Simulation of the ratio of standardized Rayleigh and Gaussian variables presented as a kernel density estimate with
10,000 kernel centers. The abscissa has been truncated at 
75.
–10
–5
5
0
10
0
0.1
0.2
0.3
0.4
0.5
Simulated PDF
Figure 4.21
Simulation of the product of standardized Rayleigh and Gaussian variables presented as a kernel density estimate
with 100 kernel centers.
121
4.8 Exploratory Data Analysis Tools
.005
13:29:40, subject to the Cambridge Core terms of use,

4.9 Sampling Distributions
A sampling distribution is the distribution of a statistic based on a set of observations of an
rv and can, in principle, be derived from the joint distribution of the rvs. It is the
distribution of the statistic for all possible samples of a speciﬁed size from a given
population and hence depends on the distribution of the population, the sample size, and
the statistic of interest. Sampling distributions are important because they allow statistical
inferences to be drawn from them rather than from the joint distribution of the rvs, which
usually simpliﬁes calculations considerably.
Example 4.17 If N rvs Xi
f
g are drawn from N μ; σ2
ð
Þ, then XN is distributed as N μ; σ2=N
ð
Þ.
The sampling distribution for the sample mean is N μ; σ2=N
ð
Þ.
Example 4.18 If N rvs Xi
f
g have the cdf F x
ð Þ, then the sampling distribution for max Xi
ð
Þ
is NF x
ð ÞN1f x
ð Þ.
Example 4.19 If N1 rvs X1i
f
g are drawn from N μ 1; σ2
1


and N2 rvs X2i
f
g are drawn from
N μ 2; σ2
2


, then the sampling distribution for XN,1  XN,2 is N μ1  μ2; σ2
1=N1 þ σ2
2=N 2


.
Example 4.20 If N rvs
Xi
f
g are each the square root of the sum of the squares of two
uncorrelated Gaussian variables with zero mean and a common variance, then their
sampling distribution is Rayleigh.
There are four important sampling distributions that are widely used in applied statistics:
the chi square, Student’s t, F, and correlation coefﬁcient distributions, along with their
noncentral variants.
4.9.1 Chi Square Distributions
The chi square distribution (sometimes chi squared or χ2) is the distribution of the sum of
squares of random samples from the standard normal distribution and was ﬁrst derived by
Pearson (1900). When there is a need to distinguish it from the noncentral chi square
distribution, it is called the central chi square distribution. It plays a major role in
likelihood ratio hypothesis testing, goodness-of-ﬁt testing, and in the construction of
122
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

conﬁdence intervals. As noted in Section 3.4.5, the chi square distribution is the gamma
distribution with α ¼ ν=2, β ¼ 1=2, where ν is usually an integer in applications, although
this is not a theoretical requirement. Consequently, it shares all the properties of the gamma
distribution.
If the means of the normal rvs used to compute the chi square statistic are not zero, the
corresponding distribution becomes noncentral chi square, as derived by Fisher (1928).
The noncentral chi square distribution plays an important role in hypothesis testing,
notably in deﬁning the alternate hypothesis to estimate the power of a test when the null
distribution is chi square. These two distributions will be considered together in this
section.
Let Xi
f
g be N independent rvs from N 0; 1
ð
Þ. Then the rv YN ¼ PN
i¼1X2
i is distributed as
chi square with N degrees-of-freedom and is denoted as YN e χ2
N. The chi square pdf is
chi2 x; ν
ð
Þ ¼
1
2ν=2Γ ν=2
ð
Þ
x ν=2
ð
Þ1ex=2
x  0
(4.67)
where ν is called the degrees-of-freedom. The gamma function Γ x
ð Þ can be expressed in
closed form when ν is an integer because Γ k
ð Þ ¼ k  1
ð
Þ! and Γ 1=2
ð
Þ ¼
ﬃﬃﬃπ
p , and hence
(4.67) can be simpliﬁed in that instance. Figure 4.22 shows representative chi square pdfs.
Let Xi e N μi; σ2
i


, and deﬁne Y0
ν ¼ Pν
i¼1 Xi=σi
ð
Þ2. The distribution of Y0
ν is
nchi2 x; ν; λ
ð
Þ ¼ 1
2 e xþλ
ð
Þ=2 x
λ
  ν2
ð
Þ=4
Iν=21
ﬃﬃﬃﬃﬃ
λx
p


(4.68)
where the noncentrality parameter is
λ ¼
X
ν
i¼1
μi
σi
 2
(4.69)
and Iν x
ð Þ is a modiﬁed Bessel function of the ﬁrst kind. Figure 4.23 shows the noncentral
chi square distribution for three degrees-of-freedom and a variety of noncentrality
parameters.
0
0.2
0
2
4
6
8
10
0.4
0.6
0.8
1
Chi Square PDF
Figure 4.22
The chi square pdf with ν = 1 (solid line), 2 (dashed line), 3 (dotted line), and 5 (dash-dot line).
123
4.9 Sampling Distributions
.005
13:29:40, subject to the Cambridge Core terms of use,

The central chi square cdf is
Chi2 x; ν
ð
Þ ¼ γ x=2; ν=2
ð
Þ
Γ ν=2
ð
Þ
(4.70)
which is the incomplete gamma function ratio. The noncentral chi square cdf is a weighted
average of central chi square distributions, resulting in a fairly complicated expression that
is omitted.
The expected value of the central chi square distribution is ν, and the variance is 2ν.
The skewness and kurtosis are
ﬃﬃﬃﬃﬃﬃﬃ
8=ν
p
and 3 þ 12=ν. Hence the distribution is leptokurtic,
and the heaviness of the right tail decreases with rising ν. The geometric mean is 2eψ ν=2
ð
Þ
[where ψ x
ð Þ is the digamma function], and the harmonic mean is 2Γ ν=2
ð
Þ=Γ ν=2  1
ð
Þ
for ν > 2 and is otherwise undeﬁned. The mode is at the origin for ν  2, and the
distribution is J-shaped, whereas the distribution is unimodal with the mode at ν  2
thereafter.
The expected value of the noncentral chi square distribution is ν þ λ, and the variance is
2 ν þ 2λ
ð
Þ. The skewness and kurtosis are 2
ﬃﬃﬃ
2
p
ν þ 3λ
ð
Þ= ν þ 2λ
ð
Þ3=2 and 3 þ 12 ν þ 4λ
ð
Þ=
ν þ 2λ
ð
Þ2, respectively. Each of these entities reduces to the central value when λ ¼ 0. The
mode of the noncentral chi square distribution occurs at the value of x that satisﬁes
nchi2 x; ν; λ
ð
Þ ¼ nchi2 x; ν  2; λ
ð
Þ.
By the CLT, a standardized central chi square variate satisﬁes
χ2
N  νN
ﬃﬃﬃﬃﬃﬃﬃﬃ
2νN
p
!
d
N 0; 1
ð
Þ
(4.71)
with ν > 50 being sufﬁciently large for the Gaussian approximation to be accurate. The
limiting distribution of a standardized noncentral chi square variable is N 0; 1
ð
Þ for either
ﬁxed λ as ν ! ∞or ﬁxed ν as λ ! ∞.
By the additivity property of the gamma distribution family, if N rvs Xi
f
g are independ-
ent, and each is chi square distributed with νi degrees-of-freedom, then PN
i¼1Xi is also chi
square with total degrees-of-freedom νN ¼ PN
i¼1νi. This makes it straightforward to
calculate statistics that combine many chi square entities, such as when assessing the ﬁt
10
5
0
15
20
0
0.05
0.1
0.15
0.2
Noncentral Chi Square PDF
Figure 4.23
The noncentral chi square distribution with three degrees-of-freedom and noncentrality parameters of 1 (solid line),
3 (dashed line), 5 (dotted line), and 10 (dash-dot line).
124
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

of a model to data where the misﬁt of a given datum is measured as the squared difference
between it and the model value. The noncentral chi square distribution shares the additivity
property with the central version: if N rvs
Xi
f
g are independent and each is noncentral
chi square distributed with noncentrality parameter λi and νi degrees-of-freedom, then
PN
i¼1Xi is also noncentral chi square with total noncentrality parameter λN ¼ PN
i¼1λi and
degrees-of-freedom νN ¼ PN
i¼1νi.
If p linear constraints are imposed on the random sample Xi
f
g, then the degrees-of-freedom
are reduced to N  p. This applies to both the central and noncentral chi square distributions.
An example of a linear constraint is estimation of the mean from the data and could be
something more complicated.
The chi square distribution can be accessed in MATLAB using the gamma distribution
object. MATLAB also supports the chi square distribution pdf, cdf, quantile function, and
random draws through chi2pdf(x, v), chi2cdf(x, v), chi2inv(p, v), and chi2rnd(v), respect-
ively. It also implements the noncentral chi square distribution as ncx2pdf(x, v, lambda),
ncx2cdf(x, v, lambda), ncx2inv(p, v, lambda), and ncx2rnd(v, lambda).
4.9.2 Student’s t Distributions
The t distribution is the work of William S. Gossett, who was employed by the Guinness
Brewing Company in Dublin, Ireland. He wrote under the pseudonym Student because
Guinness would not allow him to publish using the company name since the company did
not want competitors to know that it was using statistical techniques. For a historical
perspective, see Hanley, Julian, & Moodie (2008). Gossett (1908) also published the ﬁrst
statistical paper to use simulation to verify a theoretical result and did so at least 40 years
before computers became available.
The t distribution applies in sampling and statistical inference when neither the mean nor
the variance of the rvs is known a priori and hence must be estimated from the data. Prior to
Gossett’s work, the normal distribution was used interchangeably but incorrectly when σ2
was known or unknown. The difference between the Gaussian and t distributions is
especially important for small samples, and they are essentially indistinguishable when
the degrees-of-freedom exceed 30.
Deﬁne two independent rvs Z e N 0; 1
ð
Þ and Y e χ2
ν. Deﬁne a new rv X ¼ Z=
ﬃﬃﬃﬃﬃﬃﬃﬃ
Y=ν
p
¼
ﬃﬃﬃν
p Z=
ﬃﬃﬃﬃ
Y
p
. The distribution of X may be derived through a change of variables, as
described in Section 2.9. The distribution of X is Student’s t distribution and has the pdf
tee x; ν
ð
Þ ¼ Γ ν þ 1
ð
Þ=2
½

ﬃﬃﬃﬃﬃ
νπ
p
Γ ν=2
ð
Þ 1 þ x2=ν

 νþ1
ð
Þ=2
(4.72)
where ν is the degrees-of-freedom, and the support is
∞; ∞
ð
Þ. As for the chi square
distribution, the t distribution exists for real as well as integer ν. When ν = 1, it is a Cauchy
distribution. Figure 4.24 shows the t pdf over a range of values for ν.
As for the chi square distribution, there is a noncentral form of the t distribution. Deﬁne
two independent rvs Z e N 0; 1
ð
Þ and Y e χ2
ν, and let X ¼
ﬃﬃﬃν
p
Z þ λ
ð
Þ=
ﬃﬃﬃﬃ
Y
p
, where λ is the
noncentrality parameter. The rv X is distributed as the noncentral t distribution. The
125
4.9 Sampling Distributions
.005
13:29:40, subject to the Cambridge Core terms of use,

noncentral version of the t distribution is quite important in hypothesis testing, especially
when the alternate hypothesis or power is of interest for a null hypothesis that is t
distributed. Its pdf is given by
ntee t; ν; λ
ð
Þ ¼
1
2 ν1
ð
Þ=2
ﬃﬃﬃﬃﬃﬃ
π ν
p
Γ ν=2
ð
Þ
eν λ2= νþt2
ð
Þ
ν
ν þ t2

 ν1
ð
Þ=2

ð∞
0
xνe1
2 xλt= ﬃﬃﬃﬃﬃﬃﬃ
νþt2
p
ð
Þ
2
dx
(4.73)
The integral term in (4.73) was originally derived by R. L. Fisher as the νth repeated
integral of the Gaussian pdf and can be expressed in closed form as
ð∞
0
xνe xμ
ð
Þ2=2dx ¼ 2ν=2μ Γ ν=2
ð
Þ þ 1
½
1F1
1  ν
2
; 3
2 ;  μ2
2


þ 2 ν1
ð
Þ=2Γ ν þ 1
ð
Þ=2
½
1F1  ν
2 ; 1
2 ;  μ2
2


(4.74)
where 1F1 a; b; x
ð
Þ is Kummer’s hypergeometric function. Equation (4.74) can also be
expressed in terms of parabolic cylinder functions, but that is no more enlightening.
Solutions to Equation (4.73) must be obtained numerically.
Johnson, Kotz, & Balikrishnan (1995) give a number of alternative expressions for the
noncentral t pdf, none of which are substantially simpler than (4.73). The noncentral
t distribution is unimodal but asymmetric when the noncentrality parameter is nonzero,
with the right tail heavier when μ > 0, and vice versa. Figure 4.25 shows the noncentral
t distribution for ﬁve degrees-of-freedom over a range of noncentrality parameters. When
λ < 0, the pdf is a mirror image around the origin of the pdf for the same value of λ > 0.
The central t cdf is
Tee x; ν
ð
Þ ¼ Ix ν=2; ν=2
ð
Þ
(4.75)
–4
–2
2
0
4
0
0.1
0.2
0.3
0.4
Probability Density
Figure 4.24
The standard normal distribution (solid line) compared with the t distribution with 1 (dashed line), 2 (dotted line), 5
(dash-dot line), and 10 (gray line) degrees-of-freedom.
126
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

where Ix a; b
ð
Þ is the regularized incomplete beta function ratio and is functionally the same
as the cdf for the beta distribution. Table 4.1 shows the difference between standard
Gaussian and t distribution probabilities for small values of ν and illustrates the large
differences that ensue for small degrees-of-freedom.
Expressions for the noncentral t cdf are complicated and are omitted.
The expected value of the central t distribution is zero for ν > 1 and otherwise does not
exist. The variance is ν= ν  2
ð
Þ when ν > 2 and does not exist for ν  2. The skewness is
zero for ν > 3 and otherwise does not exist. The kurtosis is 3 þ 6= ν  4
ð
Þ for ν > 4 and
otherwise does not exist. In general, the ﬁrst ν  1 moments of the central t distribution
exist but not moments of higher order. The mode and median are both zero.
In contrast to the central t distribution, the odd moments of the noncentral t distribution are
nonzero, as is apparent from Figure 4.25. The expected value is μ ¼ ν=2
ð
Þ1=2λΓ ν  1
ð
Þ=2
½
=
Γ ν=2
ð
Þ for ν > 1 and otherwise does not exist. The variance is ν 1 þ λ2


= ν  2
ð
Þ  μ2 for
ν > 2 and otherwise does not exist. Both of these expressions reduce to the central value
when λ ¼ 0.
MATLAB does not implement Student’s t distribution through distribution objects.
MATLAB supports Student’s t pdf, cdf, quantile function, and random draws through
tpdf(x, v), tcdf(x, v), tinv(p, v), and trnd(v), respectively. It also implements the noncentral
t as nctpdf(x, v, lambda), nctcdf(x, v, lambda), nctinv(p, v, lambda), and nctrnd(p, v,
lambda).
–10
–5
0
5
10
0
0.1
0.2
0.3
0.4
Probability Density
Figure 4.25
The noncentral t distribution with ﬁve degrees-of-freedom and noncentrality parameters of 1 (dashed line), 3
(dotted line), and 5 (dash-dot line) compared with the central t distribution with ﬁve degrees-of-freedom
(solid line).
Table 4.1 Comparison of Gaussian and Student t Distribution Quantiles
k
Pr (|Z| < k)
Pr (|t| < k, 2)
Pr (|t| < k, 5)
Pr (|t| < k, 10)
Pr (|t| < k, 20)
1
0.6827
0.5774
0.6368
0.6591
0.6707
2
0.9545
0.8165
0.8981
0.9266
0.9407
3
0.9974
0.9045
0.9699
0.9867
0.9929
4
0.99994
0.9428
0.9897
0.9975
0.9993
127
4.9 Sampling Distributions
.005
13:29:40, subject to the Cambridge Core terms of use,

4.9.3 The F Distributions
The central F distribution arises in many hypothesis testing problems where the variances
of two independent random samples from normal distributions are compared. It also
frequently occurs as the null distribution in likelihood ratio tests. Its noncentral counterpart
is used in assessing the power of a hypothesis test where the null distribution is F.
Consider two independent random variables Y and Z such that Y e χ2
μ and Z e χ2
ν. Form
the new rv X ¼ Y=μ
ð
Þ= Z=ν
ð
Þ, where μ and ν are the degrees-of-freedom for the numerator
and denominator, respectively. The distribution of X is the F distribution and can be
obtained as a speciﬁc case of the quotient distribution derived in Section 2.9 when the
numerator and denominator are chi square distributions. The resulting pdf is
eff x; μ; ν
ð
Þ ¼
μμ=2νν=2
B μ=2; ν=2
ð
Þ x μ=2
ð
Þ1 ν þ μx
ð
Þ μþν
ð
Þ=2
x  0
(4.76)
where B α; β
ð
Þ is the beta function.
Figure 4.26 illustrates the F pdf for a few choices of parameters. Note the tendency
toward Gaussianity as both the numerator and denominator degrees-of-freedom get large.
The pdf always tends to zero for large x and does the same as x approaches zero provided
that μ > 2. It is always positively skewed. The order of the paired parameters μ; ν
ð
Þ is
important. The F distribution for μ; ν
ð
Þ is not the same as for ν; μ
ð
Þ unless μ ¼ ν. Further, if
an rv X e Fμ,ν, then 1=X e Fν,μ, and if an rv X e tν, then X2 e F1,ν.
As for the chi square and t distributions, there is a noncentral form of the F distribution.
Actually, there are two of them for the cases where the numerator or both the numerator
and denominator are noncentral chi square, and these are called the singly and doubly
noncentral F distributions, respectively. The noncentral F distributions are important in
analysis of variance and hypothesis testing involving linear models. Considering the singly
noncentral case, let Y e nchi2 x; μ; λ
ð
Þ with λ > 0 and Z e χ2
ν be independent rvs, and take
X e Y=μ
ð
Þ= Z=ν
ð
Þ. The rv X is distributed as the singly noncentral F distribution whose pdf
is given by
00
1
2
3
4
1
2
3
4
F PDF
Figure 4.26
The F pdf for paired degrees-of-freedom μ; ν
ð
Þ of 1, 1 (solid line), 2, 10 (dashed line), 10, 100 (dotted line), and
100, 100 (dash-dot line).
128
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

nceff x; μ; ν; λ
ð
Þ ¼
μμ=2νν=2x μ=2
ð
Þ1
B μ=2; ν=2
ð
Þ ν þ μx
ð
Þ μþν
ð
Þ=2 eλ=2
Γ μ=2
ð
Þ
Γ μ þ ν
ð
Þ=2
½


X
∞
k¼0
λ=2
ð
Þkμk Γ μ þ ν
ð
Þ=2 þ k
½

k!Γ μ=2
ð
Þ þ k
½

x
ν þ μx

k
x  0
(4.77)
The leading term in (4.77) is the central F distribution (4.76). The remaining terms in (4.77)
reduce to unity when λ ¼ 0. Figure 4.27 shows some representative examples of the non-
central F pdf. The pdf for the doubly noncentral F distribution is even more complicated than
(4.77), involving a double rather than a single sum, and is less frequently used in practice.
The expected value for the central F distribution is ν= ν  2
ð
Þ for ν > 2 and otherwise does
not exist. The variance is 2ν2 μ þ ν  2
ð
Þ=½μ ν  2
ð
Þ2 ν  4
ð
Þ for ν > 4 and is otherwise
undeﬁned. The mode is given by ν μ  2
ð
Þ= μ ν þ 2
ð
Þ
½
 for μ > 2. The mode is at the origin for
μ ¼ 2, whereas if μ < 2, the pdf is inﬁnite at the origin, and hence the mode is undeﬁned.
The expected value of the noncentral F distribution is ν μ þ λ
ð
Þ= μ ν  2
ð
Þ
½
 for ν > 2
and
does
not
exist
otherwise.
The
variance
is
2ν2½ μ þ λ
ð
Þ2 þ μ þ 2λ
ð
Þ ν  2
ð
Þ=
½μ2 ν  2
ð
Þ2 ν  4
ð
Þ for ν > 4 and does not exist otherwise.
The F cdf is the regularized incomplete beta function ratio
Eff x; μ; ν
ð
Þ ¼ Iμ x= μ xþν
ð
Þ
μ
2 ; ν
2


(4.78)
and is functionally the same as the cdf for the beta and t distributions. Consequently,
if an rv X e Beta μ=2; ν=2
ð
Þ, then νX= μ 1  X
ð
Þ
½
 e Eff μ; ν
ð
Þ, and if X e Eff μ; ν
ð
Þ, then
μX=ν
ð
Þ= μX=ν þ 1
ð
Þ e Beta μ=2; ν=2
ð
Þ.
MATLAB supports the F pdf, cdf, quantile function, and random draws as fpdf
(x, v1, v2), fcdf(x, v1, v2), ﬁnv(p, v1, v2), and frnd(v1, v2). The singly noncentral F
distribution is implemented as ncfpdf(x, v1, v2, lambda), ncfcdf(x, v1, v2, lambda), ncﬁnv
(p, v1, v2, lambda), and ncfrnd(v1, v2, lambda). MATLAB does not implement the doubly
noncentral F distribution.
0
0
1
2
3
4
5
0.2
0.2
0.4
0.6
0.8
1
Probability Density
Figure 4.27
The central F distribution with μ; ν
ð
Þ set to 10,100 (solid line) compared with the noncentral F distribution with
the same degrees-of-freedom parameters and a noncentrality parameter of 1 (dashed line), 3 (dotted line), 10
(dash-dot line), and 30 (gray line).
129
4.9 Sampling Distributions
.005
13:29:40, subject to the Cambridge Core terms of use,

4.9.4 The Correlation Coefﬁcient
The distribution for the bivariate correlation coefﬁcient (often called the Pearson correl-
ation coefﬁcient after Karl Pearson) obtained by normalizing the off-diagonal element of
(4.39) when p ¼ 2 by the square root of the product of its diagonal elements was ﬁrst
derived by Fisher (1915), although it received considerable subsequent attention due to the
complexity of the statistics for even this simple estimator. See Johnson, Kotz, & Balik-
rishnan (1995, chap. 32) for details.
Let a given pair of rvs be Xi; Yi
f
g under the condition of mutual independence for the N
distinct values of the index. The correlation coefﬁcient is given by
^r ¼
P
N
i¼1
Xi  XN


Yi  YN


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
Xi  XN

2 P
N
i¼1
Yi  YN

2
s
(4.79)
Assume that the joint distribution of Xi; Yi
f
g is the bivariate normal distribution (3.56), so
the marginal distributions of the two variables are Gaussian. The pdf of the correlation
coefﬁcient can be expressed in at least six forms, as shown in Johnson, Kotz & Balikrish-
nan (1995, chap. 32). The most useful version is due to Hotelling (1953)
corr r; ρ; N
ð
Þ ¼ N  2
ð
ÞΓ N  1
ð
Þ 1  ρ2
ð
Þ
N1
ð
Þ=2 1  r2
ð
Þ
N4
ð
Þ=2
ﬃﬃﬃﬃﬃ
2π
p
Γ ν  1=2
ð
Þ 1  ρr
ð
ÞN3=2
 2F1 1=2; 1=2; N  1=2; 1 þ ρr
ð
Þ=2
½

 1  r  1
(4.80)
where ρ is the population correlation coefﬁcient, and N is the number of data. The Gauss
hypergeometric function admits the series representation
2F1 a; b; c; x
ð
Þ ¼
X
∞
k¼0
a
ð Þk b
ð Þk
c
ð Þk k!
xk
(4.81)
where the Pochhammer symbol is
x
ð Þk ¼ Γ x þ k
ð
Þ
Γ x
ð Þ
(4.82)
It is relatively easy to implement a function that computes the series expression (4.81),
although over part of parameter space the series is slow to converge. A numerical trick to
speed up the convergence of the series uses a continued fraction implementation of the
Padé approximants (Hänggi, Roesel, & Trautmann 1978).
function [Sum ] = Hypergeometric2f1(a, b, c, x)
%Computes Gauss hypergeometric function using Padé approximants to
%accelerate convergence
130
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

rerr = 1e-8; %relative error
aerr = 1e-35; %absolute error
last = 0;
s = [];
if a < 0 && a - ﬁx(a) == 0
kup = -a;
elseif b < 0 && b - ﬁx(b) == 0
kup = -b;
else
kup = 1000; %upper limit is arbitrary and should not be reached
end
if x ~= 1
for k = 0:kup
pocha = Pochhammer(a, k);
pochb = Pochhammer(b, k);
pochc = Pochhammer(c, k);
s = [s pocha*pochb*x^k/(pochc*gamma(k + 1))];
Sum = Padesum(s);
if abs(Sum - last) <= rerr*abs(Sum) + aerr
return
end
last = Sum;
end
else
Sum = gamma(c)*gamma(c - a - b)/(gamma(c - a)*gamma(c - b));
end
end
function [Result] = Pochhammer(x, k)
% Computes Pochhammer's symbol
if k == 0
Result = 1;
return
else
i = 0:k - 1;
s = x + i;
Result = prod(s);
return
end
end
function [Cf] = Padesum(s)
%Computes sum from 1 to n of s(i) using Padé approximant
implemented with
%continued fraction expansion; see Z. Naturforschung, 33a,
402–417, 1978.
%
131
4.9 Sampling Distributions
.005
13:29:40, subject to the Cambridge Core terms of use,

%Input variable
%s--series of values to be summed, may be complex
%Output variable
%Cf--sum of the series
%
n = length(s);
D = [];
x = zeros(1, n);
d = [];
t = zeros(1, n);
D(1) = s(1);
d(1) = D(1);
if n == 1
Cf = d(1);
return
end
D(2) = s(2);
d(2) = -D(2)/D(1);
if n == 2
Cf = d(1)/(1 + d(2));
return
end
for i = 3:n
L = 2*ﬁx((i - 1)/2);
%update x vector
x(L:-2:4) = x(L - 1:-2:3) + d(i - 1)*x(L - 2:-2:2);
x(2) = x(1) + d(i - 1);
% interchange odd and even parts
t(1:2:L -1) = x(1:2:L - 1);
x(1:2:L - 1) = x(2:2:L);
x(2:2:L) = t(1:2:L - 1);
% compute cf coefﬁcient
D(i) = s(i) + s(i - 1:-1:i - L/2)*(x(1:2:L - 1)).';
d(i) = -D(i)/D(i - 1);
end
%evaluate continued fraction
Cf
= 1;
for k = n: -1:2
Cf = 1 + d(k)/Cf;
end
Cf = d(1)/Cf;
end
A complete closed form expression for the cdf does not exist, although approximations
for small degrees-of-freedom are extant, and numerical integration of (4.80) is
132
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

straightforward. The expected value and variance can be expressed in terms of Gauss
hypergeometric functions, although in practice they are easier to obtain by numerical
integration. A reasonable approximation for the expected value is ρ  ρ 1  ρ2
ð
Þ=
2 ν þ 6
ð
Þ
½
 and for the variance is 1  ρ2
ð
Þ2= ν þ 6
ð
Þ.
When the population correlation coefﬁcient is zero so that the variables are uncorrelated,
the bivariate normal distribution can be transformed into polar coordinates and integrated
over the azimuth to yield
corr r; 0; N
ð
Þ ¼ Γ N  1
ð
Þ=2
½
 1  r2
ð
Þ
N4
ð
Þ=2
ﬃﬃﬃπ
p Γ N=2
ð
Þ  1
½

(4.83)
Equation (4.83) is symmetric about r ¼ 0 and gives a measure of the variability or spread
of the sample correlation coefﬁcient when there is no actual correlation. Figure 4.28
presents (4.83) for several values of the degrees-of-freedom, showing the large uncertainty
in the correlation coefﬁcient when that quantity is small.
Figure 4.29 shows (4.80) evaluated for ﬁxed degrees-of-freedom of 10 and the popula-
tion correlation coefﬁcient set to 0.2, 0.4, 0.6, and 0.8. The pdfs for negative values of the
–1
–0.5
0
0.5
1
Correlation Coefficient
0
1
2
3
4
Probability Density
Figure 4.28
The pdf of the correlation coefﬁcient for a zero population value given by (4.83) when the degrees-of-freedom are 6
(solid line), 10 (dashed line), 30 (dotted line), and 100 (dash-dot line).
–1
–0.5
0
0.5
1
Correlation Coefficient
0
1
2
3
4
Probability Density
Figure 4.29
The pdf for the correlation coefﬁcient with 10 degrees-of-freedom and a population correlation coefﬁcient of 0.2
(solid line), 0.4 (dashed line), 0.6 (dotted line), and 0.8 (dash-dot line).
133
4.9 Sampling Distributions
.005
13:29:40, subject to the Cambridge Core terms of use,

population correlation coefﬁcient are mirror images about the origin of these curves. The
pdfs for positive (negative) values of the population correlation coefﬁcient are increasingly
skewed to the left (right) as the magnitude increases. When the population correlation
coefﬁcient is 
1, the pdf is a Dirac delta function at 
1.
r = -1:.01:1;
N = 10;
rho = 0.2;
Pdf = [];
for i = 1:length(r)
Pdf = [Pdf (N - 2)*gamma(N - 1)*(1 - rho^2)^((N - 1)/2)*   
(1 - r(i)^2)^((N - 4)/2)*   
Hypergeometric2f1(1/2, 1/2, N - 1/2, (1 +rho*r
(i))/2)  
/(sqrt(2*pi)*gamma(N - 1/2)*(1 - rho*r(i))^   
(N - 3/2))];
end
plot(r,Pdf)
The central moments can all be obtained by numerically by integrating the pdf with
suitable weighting. First, deﬁne a function handle for the expected value integrand.
fun = @(r, rho, N) r*(N - 2)*gamma(N - 1)*(1 - rho^2)^   
((N - 1)/2)*(1 - r^2)^((N - 4)/2)*   
Hypergeometric2f1(1/2, 1/2, N - 1/2,   
(1 + rho*r)/2)/(sqrt(2*pi)*gamma(N - 1/2)*   
(1 - rho*r)^(N - 3/2));
Because the function Padecf does not accept vector-valued arguments, it is necessary to
call the MATLAB numerical integration routine with the “ArrayValued second”, “true”
name-value pair.
integral(@(r) fun(r,.2,10), -1, 1, 'ArrayValued', true)
ans =
0.1896
Repeating with the population correlation coefﬁcient set to 0.4, 0.6, and 0.8 gives the
expected value for the four curves in Figure 4.29 of 0.19, 0.38, 0.58, and 0.78, all of which
are smaller than the modes of the pdfs. The variance can also be obtained numerically in an
analogous manner.
fun1 = @(r, rho, N, xm) (r - xm)^2*(N - 2)*gamma(N - 1)*   
(1 - rho^2)^((N - 1)/2)*(1 - r^2)^   
((N - 4)/2)*   
Hypergeometric2f1(1/2,1/2,N-1/2,  
(1 + rho*r)/2)/(sqrt(2*pi)*   
gamma(N - 1/2)*(1 - rho*r)^(N - 3/2));
134
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

xm = integral(@(r) fun(r, .2, 10), -1, 1, 'ArrayValued', true);
Var = integral(@(r) fun1(r, .2, 10, xm), -1, 1, 'ArrayValued',
true)
ans =
0.1045
Repeating for the remaining values of rho, the variances of the four curves in Figure 4.29
are 0.105, 0.085, 0.055, and 0.021, respectively. This is consistent with the gradual
sharpening of the peaks as the population correlation coefﬁcient rises.
4.10 Distributions for Order Statistics
The order statistics were introduced in (4.11) and are obtained by sorting a set of rvs into
ascending order. As was discussed in Section 2.8, once a set of independent rvs is
converted to order statistics through sorting, dependence is introduced. The classic
reference on order statistics is due to H. A. David; the most recent version is David &
Nagaraja (2003).
4.10.1 Distribution of a Single Order Statistic
Let
Xi
f
g be iid with cdf F x
ð Þ. Let Fr x
ð Þ denote the cdf of the rth order statistic X rð Þ. In
Section 2.8 it was established that FN x
ð Þ ¼ F x
ð Þ
½
N and F1 x
ð Þ ¼ 1  1  F x
ð Þ
½
N and are
special instances of the distribution of a single order statistic. For the general case, it
follows that
Fr x
ð Þ ¼ Pr X rð Þ  x


¼ Pr at least r of the Xi  x
ð
Þ
¼
X
N
i¼r
bin i; N; F x
ð Þ
ð
Þ
¼
X
N
i¼r
N
i
 
!
Fi x
ð Þ 1  F x
ð Þ
½
Ni
¼ Beta F x
ð Þ; r; N  r þ 1
ð
Þ
(4.84)
where the last term is the beta cdf (3.50) or, equivalently, the regularized incomplete beta
function ratio. Equation (4.84) holds for any discrete or continuous parent distribution.
When the parent distribution is continuous, the pdf follows by differentiation
f r x
ð Þ ¼
1
B r; N  r þ 1
ð
Þ Fr1 x
ð Þ 1  F x
ð Þ
½
Nrf x
ð Þ
(4.85)
Since (4.84) and (4.85) depend on F x
ð Þ elevated to integer powers, analytic solutions may
not be available except in the simplest cases, and recourse to numerical integration will be
required.
135
4.10 Distributions for Order Statistics
.005
13:29:40, subject to the Cambridge Core terms of use,

Example 4.21 Find the distribution of the order statistics when the parent population is
uniform on [0, 1].
The cdf for the uniform distribution is F x
ð Þ ¼ x on [0, 1] and zero elsewhere. The pdf
for the rth order statistic can immediately be written using (4.85)
f r x
ð Þ ¼
1
B r; N  r þ 1
ð
Þ xr1 1  x
ð
ÞNr
0  x  1
which is the beta distribution with parameters (r, N – r + 1). The pdfs for N = 10 and r = 1,
3, 7, and 10 appear in Figure 4.30. Note that the spread depends on the order statistic of
interest.
The expected value of the rth order statistic is
E X rð Þ


¼
ð
x f r x
ð Þdx ¼
1
B r; N  r þ 1
ð
Þ
ð∞
∞
xFr1 x
ð Þ 1  F x
ð Þ
½
Nr f x
ð Þdx
(4.86)
The higher order moments can be computed in a like manner. Closed-form solutions are
usually difﬁcult to derive.
Example 4.22 Find the expected value of the rth order statistic for the uniform distribution
on [0, 1].
^μ rð Þ ¼
1
B r; N  r þ 1
ð
Þ
ð1
0
xr 1  x
ð
ÞNrdx ¼ B r þ 1; N  r þ 1
ð
Þ
B r; N  r þ 1
ð
Þ
¼
r
N þ 1
For N = 10 and r = 1, 3, 7, and 10, the expected values are 0.0909, 0.2727, 0.6364, and
0.9091. Compare these values to Figure 4.30.
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
Probability Density
Figure 4.30
The pdf for the ﬁrst (solid line), third (dashed line), seventh (dotted line), and tenth (dash-dot line) order statistics
for a population of 10 whose parent distribution is the uniform distribution on [0, 1].
136
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

4.10.2 Distribution of the Sample Median
From (4.85), the pdf of the median is given by
f
N=2
b
cþ1
ð
Þ x
ð Þ ¼
1
B
N=2
b
c þ 1; N  N=2
b
c
ð
Þ F N=2
b
c x
ð Þ 1  F x
ð Þ
½
N N=2
b
c1f x
ð Þ
(4.87)
Figure 4.31 shows the median pdf for N = 3, 11, and 101 for a standardized Gaussian
parent population. Note the increasing concentration as N rises, suggesting a decreasing
variance with increasing sample size, as for the sample mean.
The expected value of the median follows from the deﬁnition. This cannot be easily
obtained in closed form but can be evaluated numerically. The following MATLAB script
implements the expected value.
fun = @(x, N) x.*normcdf(x).^ﬂoor(N/2).*(1 - normcdf(x))   
.^(N - ﬂoor(N/2) - 1).*normpdf(x)/   
beta(ﬂoor(N/2) + 1, N - ﬂoor(N/2));
integral(@(x) fun(x, N), -Inf, Inf)
The result is ﬂoating point zero. This can be veriﬁed at larger or smaller values of N and
suggests that the sample median is unbiased.
The variance of the median is just the second moment of the distribution (4.87)
because the ﬁrst moment vanishes. Using the same numerical approach with a leading
x.^2 in the deﬁnition of fun, computation gives 0.4487 for N = 3, 0.1372 for N = 11,
0.0155 for N = 101, and 0.0016 for N = 1001. Presuming that the variance goes like
ασ2=N, where α is a constant and σ2=N is the variance of the sample mean, yields
estimates for α of 1.3461, 1.5092, 1.5655, and 1.5701, respectively. The exact asymp-
totic value is π / 2 	 1.5708. Note that the variance of the sample median is about 60%
larger than that of the sample mean for a normal parent, which means that one needs
substantially more data to get the same variance for the sample median as for the
sample mean.
–2
–1
0
1
2
0
1
2
3
4
Median PDF
Figure 4.31
The sampling distribution for the median for a standard normal parent population of sizes 3 (solid line), 11 (dashed
line), and 101 (dotted line).
137
4.10 Distributions for Order Statistics
.005
13:29:40, subject to the Cambridge Core terms of use,

4.10.3 Joint Distribution of a Pair of Order Statistics
The joint pdf of X rð Þ and X sð Þ, where 1  r < s  N, can be derived using similar
reasoning. Assuming that x < y, the joint cdf of the rth and sth order statistics is given by
Frs x; y
ð
Þ ¼ Pr at least r Xi  x \ at least s Xi  y
ð
Þ
¼
X
N
j¼s
X
j
i¼r
Pr exactly i X i  x \ exactly j X i  y
ð
Þ
¼
X
N
j¼s
X
j
i¼r
N!
i! j  i
ð
Þ! N  j
ð
Þ! F i x
ð Þ F y
ð Þ  F x
ð Þ
½
ji 1  F y
ð Þ
½
Nj
(4.88)
The pdf for a continuous distribution follows by differentiation
f rs x; y
ð
Þ ¼
N!
r  1
ð
Þ! s  r  1
ð
Þ! N  s
ð
Þ! Fr1 x
ð Þf x
ð Þ F y
ð Þ  F x
ð Þ
½
sr1
 1  F y
ð Þ
½
Nsf y
ð Þ
(4.89)
To ﬁnd the pdf of the difference of two order statistics W ¼ X sð Þ  X rð Þ, the variables
in (4.89) must be changed from x; y
ð
Þ to x; w
ð
Þ, where w ¼ y  x, and then integrated
over the support of x. The Jacobian for the transformation is unity. Deﬁne Φrs to
be the leading constant in (4.89). The pdf of the difference of two order statistics is
given by
f w
ð Þ ¼ Φrs
ð∞
∞
F r1 x
ð Þf x
ð Þ F x þ w
ð
Þ  F x
ð Þ
½
sr1f x þ w
ð
Þ 1  F x þ w
ð
Þ
½
Nsdx
(4.90)
4.10.4 Distribution of the Interquartile Range
The pdf of the interquartile range follows from (4.90) by setting s ¼ 3N=4 and
r ¼ N=4
f IQ x
ð Þ ¼ Φ3N=4,N=4
ð∞
∞
FN=41 x
ð Þf x
ð Þ F x þ w
ð
Þ  F x
ð Þ
½
N=21f x þ w
ð
Þ 1  F x þ w
ð
Þ
½
N=4dx
(4.91)
Analytic integration of (4.91) is intractable except for the simplest distributions, such as the
uniform distribution, and hence numerical approaches are required. Rather than attempting
to compute the leading constant term, it is easier to compute the integral and then
normalize the result such that its integral is unity. Let F x
ð Þ be the standardized Gaussian
distribution whose interquartile range is approximately 1.3161. The following script will
compute the pdf for the interquartile range for any choice of N. Figure 4.32 shows the
138
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

distribution for N = 4, 12, 48, and 100. Note the increasingly tight distribution about the
Gaussian value for large N.
fun
=
@(x,
w,
N)
normcdf(x).^(N/4
-
1).*normpdf(x).*  
(normcdf(x + w) -normcdf(x)).^(N/2 - 1).*   
normpdf(x + w).*(1 - normcdf(x + w)).^(N/4);
f = [];
s = integral2(@(x,w) fun(x, w, N), -Inf, Inf, 0, Inf);
for w = 0:.01:5
f = [f
integral(@(x) fun(x, w, N), -Inf, Inf)/s];
end
The expected value can be computed using the following script:
fun = @(x, w, N) normcdf(x).^(N/4 - 1).*normpdf(x).*   
(normcdf(x + w) - normcdf(x)).^   
(N/2 - 1).*normpdf(x + w).*   
(1 - normcdf(x + w)).^(N/4);
fun1 = @(x, w, N) w.* normcdf(x).^(N/4 - 1).*normpdf(x).*   
(normcdf(x + w) - normcdf(x)).^(N/2 - 1).*   
normpdf(x + w).*(1 - normcdf(x + w)).^(N/4);
e = integral2(@(x, w) fun1(x, w, N), -Inf, Inf, 0, Inf,   
'AbsTol', 1e-12, 'RelTol', 1e-8);
e = e/integral2(@(x, w) fun(x, w, N), -Inf, Inf, 0, Inf)
The result is 1.3264, 1.3297, 1.3406, and 1.3316 and is not affected by changing the
tolerances. These values are all slightly larger than the population value of 1.3161, but it is
unclear whether this represents bias or numerical round off. The cause will be revealed in
Section 8.2.2.
Repeating this for the variance using the population expected value gives 0.5363,
0.1934, 0.0510, and 0.0169, respectively.
0
0.5
0
1
2
3
4
1
1.5
2
2.5
IQR PDF
Figure 4.32
The sampling distribution for the interquartile range of a standard Gaussian rv for a sample size of 4 (solid line), 12
(dashed line), 48 (dotted line), and 100 (dash-dot line).
139
4.10 Distributions for Order Statistics
.005
13:29:40, subject to the Cambridge Core terms of use,

4.11 Joint Distribution of the Sample Mean and Sample Variance
Let N independent rvs
Xi
f
g be distributed as N μ; σ2
ð
Þ so that
X
N
i¼1
Xi  μ
ð
Þ2 ¼
X
N
i¼1
Xi  XN þ XN  μ

2
¼
X
N
i¼1
Xi  XN

2 þ
X
N
i¼1
XN  μ

2 þ 2
X
N
i¼1
Xi  XN


XN  μ


(4.92)
For ﬁxed N, the second term is the constant XN  μ

2 summed N times, and the third term
is the constant 2 XN  μ


times
X
N
i¼1
Xi  XN


¼ N XN  N XN ¼ 0
(4.93)
As a result, (4.92) divided by the population variance becomes
X
N
i¼1
Xi  μ
σ

2
¼
X
N
i¼1
Xi  XN
σ

2
þ N
XN  μ
σ

2
(4.94)
Denote the ﬁrst term on the right side of (4.94) as Q1 and the second as Q2.
According to Cochran’s theorem (Cochran 1934), the sum of the squares of standardized
Gaussian variables
Ui
f
g can be written
X
N
i¼1
U2
i ¼
X
k
i¼1
Qi
(4.95)
where each Qi is a sum of squares of linear combinations of the Ui
f
g. The concept of ranks
will be discussed in Chapter 7, but for the moment it sufﬁces to deﬁne ri ¼ rank x ið Þ


¼ i.
Denote the ranks of the Qi in (4.95) by ri, and require that the sum of all of them be N.
Cochran’s theorem states that the Qi
f
g are independent and that each one has a chi square
distribution with ri degrees-of-freedom.
Returning to (4.94), the rank of Q1 is N  1 because there are N rvs with one linear
constraint among them to determine the sample mean. The rank of Q2 is 1 because it is the
square of just one linear combination of standard Gaussian rvs. Consequently, the condi-
tions of Cochran’s theorem are met, and Q1 and Q2 are independent. Since σ2Q1=N ¼ ^s2
N,
where ^s2
N is the sample variance (4.14) and XN ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2Q2=N
p
þ μ, it follows that the
sample mean and the sample variance are independent. The Gaussian distribution is the
only distribution for which this is true. This result was ﬁrst proved using geometric
arguments by Fisher (1915) and constituted a core concern in the seminal paper by Gossett
(1908). David (2009) provides a historical perspective.
Cochran’s theorem also gives
N XN  μ

2
e σ2χ2
1
(4.96)
140
Characterization of Data
.005
13:29:40, subject to the Cambridge Core terms of use,

and
X
N
i¼1
Xi  XN

2
e σ2χ2
N1
(4.97)
Consequently, the ratio of (4.96) and (4.97), with each divided by its respective degrees-of-
freedom, is given by
N XN  μ

2
Xi  XN

2= N  1
ð
Þ e F1,N1 ¼ tN1
ð
Þ2
(4.98)
Taking the square root of both sides of (4.98) gives
ﬃﬃﬃﬃ
N
p
XN  μ


^s0
N
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
XN  μ


^sN
e tN1
(4.99)
Note that the difference between the sample mean and population mean is scaled by
ﬃﬃﬃﬃ
N
p
when the unbiased estimate of the standard deviation is used but is scaled by
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
when
the sample standard deviation is used. In either case, the ratio is distributed according to
Student’s t distribution with N  1 degrees-of-freedom. From (4.96), it follows that
P
N
i¼1
Xi  XN

2
σ2
¼ N^s2
N
σ2
e χ2
N1
(4.100)
There is an interesting analogue for uniformly distributed directional data. If a sample of
directional data is independent and uniformly distributed, then the joint distribution of
θi þ α is the same as the joint distribution of θi. Consequently, the sample mean direction
(4.25) is equivariant under rotation (the sample mean of θi þ α is the same as the sample
mean of θi), and the sample mean resultant length R is invariant. As a result, the joint
distribution of θ þ α and R is the same as the joint distribution of θ and R, and the
conditional distribution of θ given R is invariant under rotation. The uniform distribution is
the only distribution that is invariant under rotation; hence θ is uniformly distributed, and θ
and R are independent.
141
4.11 Joint Distribution of the Sample Mean and Sample Variance
.005
13:29:40, subject to the Cambridge Core terms of use,

5
Point, Interval, and Ratio Estimators
5.1 Overview
There are numerous types of estimators available for statistical inference, of which
the method of moments, the maximum likelihood estimator (mle), Bayesian estima-
tors, and the method of least squares are the most widely used. Each of these classes
of estimators has its own characteristics, and different ones may be the “best” choice
for distinct statistical inference scenarios. Choosing an estimator ﬁrst requires a set of
objective criteria to deﬁne the term best, and consistency, unbiasedness, efﬁciency,
robustness, and sufﬁciency are introduced for this purpose. All these criteria save
robustness have their origin in Fisher (1922). In practice, estimators that simultan-
eously meet the ﬁve described criteria do not exist, and hence the data analyst is left
to make subjective decisions about which ones deserve the most emphasis. In the
process of describing efﬁciency, the Crámer-Rao lower bound on the variance will be
derived and is widely used in practice to characterize the dispersion of estimators. The
method of moments and the maximum likelihood estimator (mle) are discussed in detail,
leaving the method of least squares to Chapter 9. Finally, conﬁdence and tolerance
intervals are described, and the issue of estimating the ratios of random variables (rvs) is
covered.
5.2 Optimal Estimators
In the sequel, estimators or statistics will be denoted using hatted Greek letters, such as ^λk.
An estimate will be shown with a particular value for the index and no hat, such as λN. The
only exceptions to these practices are the sample mean and sample median that were
introduced in Chapter 4 and are among the most commonly used statistics.
5.2.1 Consistency
Optimal estimators should become more precise as the number of data increases. Let ^λk
denote a sequence of estimators for the parameter λ indexed by k for an increasingly large
number of values from a set of rvs
Xi
f
g. A consistent estimator has the property that
^λk !
p
λ or, equivalently, for arbitrarily small but positive ε
142
.006
13:32:26, subject to the Cambridge Core terms of use,

lim
k!∞Pr
^λk  λ

 > ε


! 0
(5.1)
An estimator that is not consistent is said to be inconsistent.
Example 5.1 The sample mean estimator Xk has variance σ2=k and E Xk


¼ μ. As k increases,
the pdf for Xk is more tightly clustered around μ. This makes it a consistent estimator.
More formally, for Gaussian rvs,
Pr XN  μ

  ε


¼ Pr
ﬃﬃﬃﬃ
N
p
XN  μ


σ

ﬃﬃﬃﬃ
N
p
ε
σ
 
!
¼ 2 1  Φ
ﬃﬃﬃﬃ
N
p
ε
σ

	


Since any cdf approaches 1 for large values of the argument, the last term approaches zero
as N ! ∞.
Consistency can be demonstrated using any theorem that establishes convergence
in probability, such as the Markov inequality (2.76) for nonnegative rvs or the Chebyshev
inequality (2.77) when the measure of the difference between the sample and population
values is the absolute value. Alternatively, if the estimator contains sums of rvs (as is
often the case), the law of large numbers can be invoked.
The continuous mapping theorem holds that continuous functions preserve limits even
when their arguments are sequences of rvs. Let g x
ð Þ be a function that is continuous at λ.
Then g
^λk

is a consistent estimator for g λ
ð Þ.
The sample mean, sample median, and sample variance are all examples of consistent
estimators. However, consistency is an asymptotic property, and demonstrating that an
estimator is consistent says nothing about its behavior for ﬁnite k. Further, if there is one
consistent estimator, then an inﬁnite number of consistent estimators can be constructed
from it because k  α
ð
Þ^λk= k  β
ð
Þ is also consistent for any ﬁnite constants α and β.
The deﬁnition of consistency given in this section is sometimes called weak consistency.
If convergence in probability is replaced by convergence almost surely, then the estimator
possesses strong consistency.
5.2.2 Unbiased Estimators
A consistent estimator will have a value in the vicinity of the true value for large numbers
of data. Deﬁne the bias to be
B
^λk, λ

¼ E
^λk

 λ
(5.2)
An unbiased estimator has B
^λk, λ

¼ 0 for all possible values of λ. An asymptotically
unbiased estimator has B
^λk, λ

! 0 as k ! ∞. While consistent estimators are not
necessarily unbiased, a consistent estimator with a ﬁnite mean value for large k must be
asymptotically unbiased.
143
5.2 Optimal Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

Example 5.2 The sample mean Xk is an unbiased estimator for the population mean μ for
any distribution for which it exists.
It is possible for an estimator to be unbiased but not consistent. For an iid random sample
Xi
f
g, deﬁne an estimator for the mean to be any particular ﬁxed value of the random
variables ^λk ¼ xk. Since the sampling distribution of ^λk is the same as the distribution of
the rvs, and this holds for any size N  k for the sample because only the kth sample is
used in the estimator, it follows that E
^λk

¼ E X
ð Þ, and the estimator is unbiased.
However, it is not consistent because it does not converge to anything. It is also possible
for an estimator to be biased but consistent. An example is the sample variance ^s2
N.
There may be no unbiased estimators, or there may be more than one unbiased and
consistent estimator. For example, the sample mean is a consistent and unbiased estimator
for the population mean of a Gaussian sample. The asymptotic variance of the sample
median ~Xk was shown to be π σ2= 2k
ð
Þ in Section 4.10.2, so the sample median is
consistent. The sample median is also an unbiased estimator of the population mean, as
can be seen from the symmetry of its distribution. Thus there are at least two consistent and
unbiased estimators for the population mean of a Gaussian sample. Further optimality
criteria are required to deﬁne best estimator.
5.2.3 Efﬁciency and the Cramér-Rao Lower Bound
Qualitatively, it would seem that the estimator whose sampling distribution is most tightly
concentrated around the true parameter would be a “best estimator.” This can be quantiﬁed
using the mean squared error (mse) criterion:
mse ^λk
h
i
¼ E
^λk  λ

2


¼ var ^λk
h
i
þ E ^λk


 λ
h
i2
(5.3)
The second term on the right is the square of the bias (5.2), and it vanishes if the estimator
is unbiased, in which case the mse equals the variance. Consequently, different unbiased
estimators can be compared through their variances.
Example 5.3 The sample median and sample mean are both unbiased estimators for the
population mean. However, the variance of the sample median is π /2 times the variance of
the sample mean and hence has a higher mean square error.
It has been shown previously that there are at least two estimators for the variance, the
sample variance where the sum of squared differences between a set of rvs and their sample
mean ^s2 is divided by N and the unbiased variance divided by N  1. A third estimator can
be deﬁned that is normalized by N þ 1, and it can be shown for Gaussian data that
144
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

mse ^s2= N þ 1
ð
Þ


< mse ^s2
N


< mse ^s02
N


(5.4)
Consequently, even for something as simple as the variance, there is no estimator that is
simultaneously unbiased and minimum mse.
When the joint pdf of the observations f xi; λ
ð
Þ is regarded as a function of λ for a given
set of samples
xi
f g, it is called a likelihood function L λ; xi
ð
Þ. The score function is the
derivative of the log likelihood with respect to the parameter:
∂λ log L λ; x
ð
Þ ¼ ∂λL λ; x
ð
Þ
L λ; x
ð
Þ
(5.5)
and has zero expected value, as can be easily shown from the deﬁnition. The variance of
the score is
I λ
ð Þ ¼ var ∂λ log L λ; x
ð
Þ
½
 ¼ E
∂λ log L λ; x
ð
Þ
½
2
n
o
(5.6)
and is the Fisher information for a single rv. Presuming that the log likelihood function is
twice differentiable, the Fisher information is also given by
I λ
ð Þ ¼ E ∂2
λ log L λ; x
ð
Þ


(5.7)
The Fisher information measures the amount of information that an rv carries about an
unknown parameter. An rv with high Fisher information corresponds to one whose squared
score function is frequently large. From (5.7), the Fisher information is also a measure of
the curvature of the log likelihood near its maximum value. If the log likelihood is sharply
peaked around its maximum, then the Fisher information is high, whereas if it is very
broad, the Fisher information is low.
Let ^Ρ be an estimator for some function of the parameter ρ λ
ð Þ. It can be shown that
var ^Ρ
 
 ρ0 λ
ð Þ
½
2
IN λ
ð Þ
(5.8)
where IN λ
ð Þ ¼ NI λ
ð Þ is the sample Fisher information. Equation (5.8) is the Cramér-Rao
inequality that was introduced independently by Cramér (1945) and Rao (1945) and gives
a lower bound for the variance of any estimator.
When the estimator ^Ρ is biased, let ρ λ
ð Þ ¼ B ^λ; λ


þ λ, where the ﬁrst term on the right
is the bias (5.2), so
var ^Ρ
 

1 þ B0 ^λ; λ


h
i2
IN λ
ð Þ
(5.9)
When ρ λ
ð Þ ¼ λ, the numerator of (5.8) is 1, and the Cramér-Rao bound becomes the
inverse of the sample Fisher information. The variance of an unbiased estimator cannot be
smaller than this value.
When the parameter λ is a p-vector, the Fisher information becomes a p  p matrix
called the Fisher information matrix. Extension to the matrix form is straightforward. The
elements of the Fisher information matrix are
145
5.2 Optimal Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

Iij ¼ E ∂λi log L λ; x
ð
Þ∂λj log L λ; x
ð
Þ


(5.10)
and the Cramér-Rao bound is
cov ^Ρ
 
J
$  I
$1  J
$T
(5.11)
where J
$¼ ∂λρ is the Jacobian matrix whose i, j element is ∂λjρi, and the inequality X
$  Y
$
means that X
$ is more positive deﬁnite than Y
$. The matrix form of the Fisher information
occurs frequently. For example, if one were estimating the three parameters of the
generalized extreme value distribution, the Fisher information matrix is 3  3, and in the
absence of independence of the parameters, the off-diagonal elements deﬁne their
covariance.
An unbiased estimator that achieves the minimum-variance Cramér-Rao lower bound is
said to be efﬁcient. An efﬁcient estimator is also the minimum variance unbiased (mvu)
estimator and has the lowest possible mse. If the asymptotic variance of an unbiased
estimator is equal to the lower bound, it is asymptotically efﬁcient. This may not hold for a
ﬁnite sample, and other estimators may also be asymptotically efﬁcient. A measure of
efﬁciency for the univariate case is
e ^λ
 
¼
1
IN λ
ð Þvar ^λ
 
(5.12)
with obvious matrix extensions when the estimator is multivariate. Equation (5.12) must be
less than or equal to 1, with equality achieved when the estimator is mvu.
Example 5.4 Let Xi
f
g be N random samples from the exponential distribution with param-
eter β as given by (3.40). Determine the variance for the estimator ^λ ¼ N  1
ð
Þ= Pn
i¼1xi


using the Cramér-Rao bound.
The log likelihood is log L βjx
ð
Þ ¼ log β  βx. The Fisher information for a single
datum is given by (5.7) as 1=β2. The Fisher information for the entire sample is N=β2.
By the reproductive property of the gamma distribution (Section 3.4.5), PN
i¼1xi is distrib-
uted as gam x; N; β
ð
Þ. The mean of 1=PN
i¼1xi is β= N  1
ð
Þ, so the estimator ^λ is unbiased.
The variance of 1=PN
i¼1xi is β2=

N  1
ð
Þ2 N  2
ð
Þ

. The Cramér-Rao variance bound is
the inverse sample Fisher information, or β2=N, so the variance of ^λ is larger than the
theoretical lower limit. The estimator efﬁciency (5.12) is N  2
ð
Þ=N, which is smaller than
one for three or more samples.
A measure of the relative efﬁciency of a pair of unbiased estimators is the ratio of their
variances that must lie between 0 and 1.
Example 5.5 The sample mean is the minimum variance unbiased estimator for the popula-
tion mean of a Gaussian distribution and has variance σ2=N. The relative efﬁciency of the
sample median with variance πσ2=2N is 2/π  0.637.
146
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

For a mvu estimator to exist, the log likelihood function must be factorable as
log L λ; xi
ð
Þ ¼ S λ
ð Þη λ; xi
ð
Þ
(5.13)
where S λ
ð Þ is independent of the observations. If the log likelihood function cannot be
written in this form, then an estimator that reaches the Cramér-Rao lower bound does
not exist.
Example 5.6 The likelihood function for the Cauchy distribution is
L λ; xi
ð
Þ ¼
Y
N
i¼1
1
π 1 þ xi  λ
ð
Þ2
h
i
The log likelihood function is
log L λ; xi
ð
Þ ¼ N log π 
X
N
i¼1
log 1 þ xi  λ
ð
Þ2
h
i
This does not have the form of (5.13), and hence a mvu estimator for the location
parameter does not exist.
While efﬁcient estimators are desirable, they do not always exist. In some circumstances,
small sample variance may be a more important criterion for “best” than bias, and hence
estimators with bias may be acceptable. This tradeoff can be quantiﬁed by examining the
mean square error (5.3).
5.2.4 Robustness
The most commonly used statistics (e.g., the sample mean and sample variance) are not robust
to the presence of a small fraction of outlying data. To see this, use the normrnd function in
MATLAB to create 10 random numbers drawn from the N 1; 1
ð
Þ distribution. Their sample
mean is 1.0729, their median is 0.9818, and their sample variance is 0.9785. Now replace the
last value with a single random number from N 1; 100
ð
Þ. A new sample mean of 8.3497,
median of 1.0866, and variance of 507.7205 are obtained. The mean has increased by a factor
of ~8, the variance has increased by a factor of more than 500, but the median is almost
unchanged. This relative insensitivity of the median to outlying data is termed robustness.
Deﬁne the p-norm by
y
k kp ¼
X
N
i¼1
yi
j jp
 
!1=p
p  1
(5.14)
The sample median and mean are obtained, respectively, by minimizing (5.14) after
replacing yi with xi  ~XN for p ¼ 1 and xi  XN for p ¼ 2 or by minimizing the L1 and
L2 norms. The preceding example suggests that an L1 estimator like the median will prove
147
5.2 Optimal Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

to be of better utility than an L2 estimator like the sample mean or variance because of
robustness. However, L1 estimators are typically less efﬁcient than L2 estimators (e.g.,
recall that the variance of the sample median is π=2 times the variance of the sample mean).
In addition, computation of L1 estimators is much more resource intensive than for L2
estimators.
Robustness is typically more of an issue for estimates of dispersion such as the variance
than for estimates of location such as the sample mean and becomes even more important
for higher-order moments. It is possible to produce robust estimates of scale – in the
preceding example, the median absolute deviation (MAD) is 0.4414 and 0.4490, respect-
ively, without and with the outlying data point. The MAD is relatively immune to up to
50% outlying data and is an especially robust estimator for scale. However, like the
median, it has lower efﬁciency than its L2 counterparts.
A key concept in robustness is that of a ﬁnite sample breakdown point, deﬁned as the
smallest fraction of anomalous data that can render the estimator useless. The smallest
possible breakdown point is 1/N (i.e., a single anomalous point can destroy the estimate).
The sample mean and the sample variance both have breakdown points of 1/N.
5.2.5 Sufﬁcient Statistics
Sufﬁcient statistics play a key role in statistical inference because, if they exist, a minimum
variance estimator must be a function of the sufﬁcient statistics. Further, if it exists, the
efﬁcient estimator must be a function of the sufﬁcient statistics, and any function of the
sufﬁcient statistics will be asymptotically efﬁcient.
Suppose that two scientists A and B want to estimate the salinity of seawater from N
measurements of its electrical conductivity. Scientist A has access to all the raw data.
Scientist B has access only to a summary statistic ^λN. Suppose that, for all practical
purposes, scientist B can estimate salinity as well as scientist A despite not having all
the data. Then ^λN is a sufﬁcient statistic for salinity. Sufﬁcient statistics provide all the
information necessary to estimate a parameter and provide a summary description of the
measurements.
Example 5.7 The sample mean is sufﬁcient for the population mean of a Gaussian distribu-
tion because no further information about the population mean can be derived from the
sample itself. However, the sample median is not sufﬁcient for the population mean. For
example, if the values above the sample median are only slightly larger than it but those
below the sample median extend to very small values, then having this information would
provide additional information about the population mean.
A more formal deﬁnition holds that a statistic ^λk is sufﬁcient for λ if the conditional
distribution of
Xi
f
g given that ^λk takes on a particular value λ∗does not depend on λ, or
Pr X; λj^λk ¼ λ∗


¼ Pr Xj^λk ¼ λ∗


(5.15)
148
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

A necessary and sufﬁcient condition for ^λk to be a sufﬁcient statistic is that the joint pdf (or
likelihood function) be factorable as follows
f N x1; . . . ; xN; λ
ð
Þ ¼ U x1; . . . ; xN
ð
ÞV ^λk; λ


(5.16)
where U and V are nonnegative functions. Equation (5.16) is called the Fisher-Neyman
factorization theorem. Conversely, if the joint distribution cannot be factored as in (5.16),
then a sufﬁcient statistic does not exist.
Example 5.8 Let Xi
f
g be a random sample from a Poisson distribution with unknown mean λ.
Show that PN
i¼1xi is a sufﬁcient statistic for λ.
The Poisson pdf is pois x; λ
ð
Þ ¼ λxeλ=x! for x = 0, 1. . .. The joint pdf is
poisN x1; . . . ; xN; λ
ð
Þ ¼
Y
N
i¼1
eλλxi
xi!
¼
Y
N
i¼1
1
xi!
 
!
eNλλ
P
N
i¼1
xi
0
@
1
A
The ﬁrst term in parentheses on the right is U, and the second is V, so the joint pdf factors as
in (5.16), and hence PN
i¼1xi is a sufﬁcient statistic for λ. However, PN
i¼1xi does not achieve
full efﬁciency because the sample mean has a lower variance.
Example 5.9 Let
Xi
f
g be a random sample from the uniform distribution on (0, λ] with
unknown parameter λ. Find a sufﬁcient statistic for λ.
The uniform pdf is unif x; λ
ð
Þ ¼ 1=λ on (0, λ]. Consider the order statistics x ið Þ. None of
the order statistics, including the largest one x N
ð Þ, can exceed λ. Therefore, the joint pdf is
unif x1; . . . ; xN; λ
ð
Þ ¼ 1
λN
if x N
ð Þ ¼ λ and zero otherwise. This may be succinctly written as
unif x1; . . . ; xN; λ
ð
Þ ¼ 1
λN 1 λ  x N
ð Þ


where 1 x
ð Þ is the indicator function deﬁned in Section 2.3. Since the joint pdf can be
factored into a function of λ alone and a function of x N
ð Þ, the largest order statistic is a
sufﬁcient statistic for λ.
Example 5.10 Show that the data x1, :::, xk are a sufﬁcient statistic.
Let ^λk ¼ x1, :::, xk, and set U ¼ 1 in (5.16). Then f xi; λ
ð
Þ ¼ V ^λk; λ


, and hence the
data are a sufﬁcient statistic.
149
5.2 Optimal Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

At a minimum, the data are always a sufﬁcient statistic but are hardly a parsimonious
representation. The order statistics are a more useful sufﬁcient statistic that always exists,
and sometimes order statistics are the only sufﬁcient statistic other than the data that is possible,
such as for the Cauchy distribution. In fact, outside the exponential family of distributions, it is
rare for a sufﬁcient statistic to exist whose dimensionality is smaller than the sample size.
A minimal sufﬁcient statistic is a sufﬁcient statistic that is a function of all other possible
sufﬁcient statistics so that it is the simplest one possible. Minimal sufﬁcient statistics are
not unique because any monotone function of one is also minimal sufﬁcient. However,
they are important because a minimal sufﬁcient statistic achieves the most parsimonious
representation of a data sample of any possible sufﬁcient statistic.
Suppose that a set of rvs has distribution f x; λ
ð
Þ and that there is an estimator ^λ for the
distribution parameter. The ratio f x; λ
ð
Þ=f x0; λ
ð
Þ is constant for any pair of data values x and
x0 if and only if ^λ Xi
ð
Þ ¼ ^λ X0
i


. Then ^λ Xi
ð
Þ is a minimal sufﬁcient statistic for λ. This is the
Lehmann-Scheffé theorem for minimal sufﬁcient statistics.
A statistic ^λ for λ is a complete sufﬁcient statistic if E

g
^λ

¼ 0 for any λ means
Pr

g
^λ

¼ 0

¼ 1 where g x
ð Þ is a function that does not depend on any unknown param-
eters. Any complete sufﬁcient statistic must also be minimal sufﬁcient. Any one-to-one
function of a sufﬁcient, minimal sufﬁcient, or complete sufﬁcient statistic is sufﬁcient,
minimal sufﬁcient, or complete sufﬁcient, respectively.
Suppose that x1, :::, xN are iid data drawn from a k-parameter exponential family
distribution whose joint distribution is parameterized as
f N xijλ
ð
Þ ¼ e
P
k
i¼1
Aj λ
ð ÞBj xi
ð ÞþCj xi
ð ÞþDj λ
ð Þ
½

(5.17)
Univariate distributions in the exponential family, including the normal, binomial,
Poisson,
and
gamma
distributions,
meet
this
condition.
Then
the
vector
PN
i¼1B1 xi
ð Þ; . . . ; PN
i¼1Bk xi
ð Þ
h
i
is
1. A joint sufﬁcient statistic for λ; and
2. A minimal joint sufﬁcient statistic for λ provided that there is not a linear constraint on
its k elements.
This is a more explicit result than obtains from the Fisher-Neyman factorization theorem (5.16).
Example 5.11 Find a sufﬁcient statistic for the gamma distribution with α unknown and
β known.
In this case, k ¼ 1 in (5.17). The gamma pdf may be written as
gam x; α; β
ð
Þ ¼ βα
Γ α
ð Þ xα1eβx ¼ eα log βþ log x
ð
Þe log xþβx
ð
Þe log Γ α
ð Þ
Let A ¼ α, B ¼ log β þ log x, C ¼  log x þ βx
ð
Þ, and D ¼  log Γ α
ð Þ. The pdf factors
according
to
(5.17),
and
hence
a
sufﬁcient
statistic
exists.
It
is
given
by
N log β þ PN
i¼1 log xi. The statistic is minimal and complete sufﬁcient.
150
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

Example 5.13 Let X be an rv from N 0; σ2
ð
Þ. Based on Example 5.12, a minimal and
complete sufﬁcient statistic for the population variance is PN
i¼1x2
i . However, Example
5.10 shows that the data themselves are a sufﬁcient statistic. Since the data are not a
function of PN
i¼1x2
i , they are not a minimal sufﬁcient statistic.
Some additional examples, stated without derivation, include
1. If Xi e unif x; a; b
ð
Þ, then X 1
ð Þ; X N
ð Þ


are complete joint sufﬁcient statistics for a; b
ð
Þ;
2. If Xi e unif x; a; b
ð
Þ with a (b) known, then X N
ð Þ X 1
ð Þ


is a complete sufﬁcient statistic
for b (a);
3. If Xi e exp e x; θ; β
ð
Þ, where θ is a location parameter, then
X 1
ð Þ; XN


are complete
joint sufﬁcient statistics for θ; β
ð
Þ; and
4. If Xi are standard Cauchy with a location parameter θ, then the order statistics are
minimal sufﬁcient statistics.
A statistic that does not depend on the parameters in the joint distribution is an ancillary
statistic. By Basu’s theorem, any complete sufﬁcient statistic is independent of every ancillary
statistic. Consequently, if a statistic ^λ is minimal sufﬁcient and ^τ is an ancillary statistic but
^τ ¼ g
^λ

so that the two statistics are not independent, then ^λ is not complete. Conversely,
independence between a minimally sufﬁcient statistic and every ancillary statistic can be
proved without deriving their joint distributions, which is often a difﬁcult task.
Example 5.14 Two rvs X1 and X2 are drawn from N μ; σ2
ð
Þ. Deﬁne the difference statistic
d ¼ X1  X2 whose distribution is N 0; 2σ2
ð
Þ. Since d does not depend on μ, it is an
ancillary statistic for μ. However, if σ2 is unknown, d is not ancillary for that parameter.
Let ^λk be an estimator for λ, and let ^θk be sufﬁcient for λ. Deﬁne ^ϕk ¼ E ^λkj^θk
h
i
. Then
E
^ϕk  λ
2
h
i
 E
^λk  λ
2
h
i
, with equality occurring only when ^λk ¼ ^ϕk. This is the
Example 5.12 Find a joint sufﬁcient statistic for the Gaussian distribution N μ; σ2
ð
Þ.
The pdf factors as
N x; μ; σ2


¼
1ﬃﬃﬃﬃﬃ
2π
p
σ
eμ2= 2σ2
ð
Þex2= 2σ2
ð
Þeμx=σ2
Let
A1 ¼ μ=σ2, A2 ¼ 1= 2σ2
ð
Þ,
B1 ¼ x, B2 ¼ x2,
C ¼ 0,
and
D ¼ μ2= 2σ2
ð
Þ
log
ﬃﬃﬃﬃﬃ
2π
p
σ


. This has the form of (5.17); hence the joint sufﬁcient statistic is the vector
PN
i¼1x; PN
i¼1x2
i


and is both minimal and complete joint sufﬁcient. The one-to-one
functions of the joint sufﬁcient statistic
XN;^s
2
N


and
XN;^sN


are also minimal and
complete joint sufﬁcient.
151
5.2 Optimal Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

Rao-Blackwell theorem that was introduced independently by Rao (1945) and Blackwell
(1947). The Rao-Blackwell estimator ^ϕk is an improved estimator over the original one.
The Rao-Blackwell theorem is the rationale for basing estimators on sufﬁcient statistics
because lower mse estimators than the sufﬁcient statistic do not exist.
Example 5.15 A minimal sufﬁcient statistic for the parameter p in a Bernoulli distribution is
the sum of the data in a random sample. Take the mean of the ﬁrst two data points as a trial
estimator. Derive the Rao-Blackwell estimator, and compare its mse with the sufﬁcient
statistic.
Deﬁne the sufﬁcient statistic ^T ¼ PN
i¼1xi so that
E
x1 þ x2
ð
Þ=2
^T


¼ E x1j^T


=2 þ E x2j^T


=2 ¼ E x1j^T


where the last term follows from exchangeability. The sample contains exactly ^T ones and
1  ^T zeroes conditional on ^T , so E x1j^T


¼ ^T =N, and the Rao-Blackwell estimator is
the sample mean.
The mses are given by the variances of the trial and Rao-Blackwell estimator because
both are unbiased. The variance of a Bernoulli distribution is p 1  p
ð
Þ. Consequently, the
mses of the trial and Rao-Blackwell estimators are p 1  p
ð
Þ=2 and p 1  p
ð
Þ=N, respect-
ively. Presuming that N > 2, the mse of the Rao-Blackwell estimator is smaller than that of
the trial estimator by a factor of N=2.
5.2.6 Statistical Decision Theory
The previous ﬁve sections have presented a nonexhaustive series of criteria or “beauty
principles” that can be used to distinguish and categorize estimators. While it is not
possible to simultaneously optimize all of them, a formal theory exists for comparing them
that is called decision theory.
Consider a parameter λ and its estimator ^λ. Deﬁne a loss function ρ

λ, ^λ

that maps the
parameter space onto the real line and measures the disparity between λ and ^λ. Some
examples of loss functions include
• 0–1 loss ρ

λ, ^λ

¼ 1

λ ¼ ^λ

;
• L1 or absolute difference loss ρ

λ, ^λ

¼ λ  ^λ

; and
• L2 or squared difference loss ρ

λ, ^λ

¼

λ  ^λ
2.
A loss function is an rv because the estimator ^λ depends on the random sample. Conse-
quently, its expected value is called the risk of the estimator:
R

λ, ^λ

¼ E ρ

λ, ^λ

h
i
¼
ð
ρ λ; ^λ x
ð Þ
h
i
f x; λ
ð
Þdx
(5.18)
152
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

For the L2 loss function, the risk is identical to the mean squared error (5.3). Clearly, two
estimators can be compared through their risk functions, but this does not necessarily
provide an unambiguous result, as can be seen in the following example.
What is needed is a summary statistic for the comparison of risk functions. An estimator ^λ
is admissible if no other estimator ^λ
0 exists such that R

λ, ^λ
0
 R

λ, ^λ

8 λ, where strict
inequality exists for at least some values of the parameter. However, admissible estimators
may be a large class.
A commonly used summary statistic utilizes minimization of the maximum risk:
R
^λ

¼ supλ R

λ, ^λ

h
i
(5.19)
where the supremum is the least upper bound. For Example 5.16, the maximum risk is of
little value if the Rayleigh distribution parameter is unbounded. However, suppose that it is
known to lie on [1, 2]. In this case, the estimator ^λ2 is preferred because it has the smaller
maximum risk. An estimator that minimizes the maximum risk for all possible estimators is
called a minimax rule. The estimator ^λ is minimax if
supλ R

λ, ^λ

h
i
¼ inf~λ supλ R λ;~λ






(5.20)
where the inﬁmum is the greatest lower bound over all possible estimators ~λ. For standard
Gaussian data, the sample mean is minimax for the population mean for any loss function
Example 5.16 Let an rv X e rayl x; λ
ð
Þ using an L2 loss function, and deﬁne two estimators
^λ1 ¼ X and ^λ2 ¼ 1. The risk functions corresponding to the two estimators are
R

λ, ^λ1

¼ 3 
ﬃﬃﬃﬃﬃ
2π
p


λ2 and R

λ, ^λ2

¼ 1  λ
ð
Þ2. Figure 5.1 compares the two, showing
that neither dominates the other for all values of λ, so there is no clear decision between the
two estimators.
0
0.5
1
1.5
2
Lambda
0
0.5
1
1.5
2
Risk Function
Figure 5.1
Risk functions for ^λ1 (solid line) and ^λ2 (dashed line).
153
5.2 Optimal Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

that is convex and symmetric. No other estimator for the population mean is minimax, and
consequently, the sample mean is the “best” estimator.
Example 5.16 For an L2 loss function, consider the following ﬁve estimators for the
population mean of a standardized Gaussian distribution given a set of N rvs:
1. Sample mean
2. Sample median
3. First value x1
4. The constant 10
5. The weighted mean with weights wi that sum to 1
The risk functions for these ﬁve estimators are, respectively, 1=N, π= 2N
ð
Þ, 1, 10  μ
ð
Þ2,
and 1=N þ PN
i¼1 wi  w
ð
Þ2. The second, third, and last of these are inadmissible because
the ﬁrst offers a lower risk for N > 1. The fourth is admissible because it is arbitrarily small
when λ is near 10, but it is not minimax. Consequently, the minimax criterion favors the
sample mean of these ﬁve choices of estimators.
5.3 Point Estimation: Method of Moments
The method of moments is the simplest way to estimate the parameters in a distribution
from a set of iid observations and depends only on the law of large numbers. It requires that
the population and sample moments be equated at successive orders until enough sample
moments are available to compute the population moments. If there are p parameters, then
this becomes a system of p equations in p unknowns that can be readily solved.
Example 5.18 For a set of rvs drawn from a normal population, E X
ð Þ ¼ μ and
E X2


¼ μ2 þ σ2, so the method of moments estimator is ^μ ¼ XN and ^σ2 ¼ X
2
N  ^μ2,
where X
2
N ¼ 1
N
PN
i¼1x2
i
Example 5.19 For a set of rvs drawn from the gamma distribution, E X
ð Þ ¼ α=β and
E X2


¼ α α þ 1
ð
Þ=β2. Then ^α ¼ XN

2= X
2
N  XN

2
h
i
and ^β ¼ XN= X
2
N  XN

2
h
i
are
the method of moments estimators.
Method of moments estimators have the advantage of simplicity (typically, analytic
simplicity) and can serve to initialize numerical procedures to compute statistical param-
eters in complex situations. Method of moments estimators are consistent but often biased,
154
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

and are asymptotically Gaussian, but they lack the optimality property of efﬁciency and
may or may not be sufﬁcient statistics. In addition, they may yield estimates that lie outside
the sample space, in which case they are unreliable. Further, they may not be unique; for
example, the sample mean and sample variance are both method of moments estimators for
the parameter in a Poisson distribution.
5.4 Point Estimation: Maximum Likelihood Estimator
It is natural to seek the value of the parameter λ for which the likelihood function L λ; xi
ð
Þ is
maximized as the most probable value for the parameter because this maximizes the
probability of observing the data that have been collected. In practice, it may be easier to
maximize the log likelihood rather than the likelihood; both quantities have the same
maximum. Equivalently, one can minimize the negative log likelihood, which often is the
case for numerical optimization algorithms. Mathematically, this means seeking solutions of
∂λ log L λ; xi
ð
Þ ¼ 0
(5.21)
subject to ∂2
λL λ; xi
ð
Þ < 0. The solution ^λk to (5.21) is the maximum likelihood estimator
(mle). In practice, the likelihood function may have multiple maxima, in which case the
largest one is chosen, or it may have equal maxima, in which case the mle is not unique.
Example 5.20 Suppose that
Xi
f
g are N independent rvs from a Poisson distribution with
parameter λ. Derive the mle ^λ for λ.
The joint distribution is the product of the marginal pdfs because the data are iid
poisN xi; λ
ð
Þ ¼ eNλ Y
N
i¼1
λxi
xi!
The log likelihood is
log L λ; xi
ð
Þ ¼ Nλ þ
X
N
i¼1
xi log λ  log xi!
ð
Þ
Setting the derivative with respect to λ to zero, the solution is the sample mean, and the
second derivative with respect to λ is negative, so the sample mean is the mle.
Example 5.21 Suppose that Xi
f
g are independent rvs from N μ; σ2
ð
Þ where neither μ nor σ2
is known. Derive the mles for μ and σ2.
The likelihood function is
L μ; σ2; xi


¼
1
2π σ2
ð
ÞN=2 e
1=2σ2
ð
ÞP
N
i¼1
xiμ
ð
Þ2
155
5.4 Point Estimation: Maximum Likelihood Estimator
.006
13:32:26, subject to the Cambridge Core terms of use,

The log likelihood function is
log L μ; σ2; xi


¼  N
2 log 2π  N
2 log σ2  1
2σ2
X
N
i¼1
xi  μ
ð
Þ2
Taking the derivatives of log L with respect to μ and σ2 and setting them to zero yields
^μ ¼ XN and ^σ2 ¼ ^s2
N. Formally, it is necessary to also show that the second derivatives are
negative, which is left as an exercise for the reader. It can also be shown that the sample
mean is the mle for μ when σ2 is known.
Example 5.22 The generalized extreme value distribution pdf (Section 3.4.9) is
gev x; ς; ξ; γ
ð
Þ ¼ 1
γ
1 þ ς x  ξ
γ

	 1þ1=ς
ð
Þ
e 1þς xξ
ð
Þ=γ
½
1=ς
The likelihood and log likelihood functions are easily constructed from the distribution.
Taking derivatives with respect to ς, ξ, and γ and setting them equal to zero gives three
equations
X
N
i¼1
ς þ 1
1 þ ς xi  ξ
ð
Þ=γ  1 þ ς xi  ξ
ð
Þ=γ
½
 1þ1=ς
ð
Þ


¼ 0
X
N
i¼1
log 1 þ ς xi  ξ
ð
Þ=γ
½
  ς 1 þ ς
ð
Þ
γ
X
N
i¼1
xi  ξ
1 þ ς xi  ξ
ð
Þ=γ þ
X
N
i¼1
N 1 þ ς xi  ξ
ð
Þ=γ
½
1=ς

log 1 þ ς xi  ξ
ð
Þ=γ
½

ς

xi  ξ
ς 1 þ ς xi  ξ
ð
Þ=γ
½



¼ 0
N 
X
N
i¼1
1 þ 1=ς
ð
Þ
ς xi  ξ
ð
Þ=γ
1 þ ς xi  ξ
ð
Þ=γ þ xi  ξ
γ
1 þ ς xi  ξ
ð
Þ=γ
½
 1þ1=ς
ð
Þ


¼ 0
These are a coupled nonlinear set of equations that must be solved numerically.
Further, it is necessary to show that the second derivatives are all negative, which is
a real mess.
Example 5.23 In Example 4.4 it was stated that the parameters for the von Mises
distribution could be estimated using the mean direction and the solution to a transcenden-
tal equation. Show that these are the mles.
The von Mises log likelihood function is given by
log L ν; κjθi
ð
Þ ¼ κ cos θi  ν
ð
Þ  log 2π
ð
Þ  log I0 κ
ð Þ
Taking the derivative with respect to ν gives PN
i¼1 sin θi  ν
ð
Þ ¼ 0, whose solution ^ν after
expanding the sine function is the sample mean direction (4.25). Taking the second
156
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

derivative with respect to ν gives a result that is always negative, so the sample mean
direction is the mle. Taking the derivative with respect to κ and using the fact that
∂κI0 κ
ð Þ ¼ I1 κ
ð Þ gives the transcendental equation I1 κ
ð Þ=I0 κ
ð Þ ¼ R, which was solved
numerically in Example 4.4. Since the second derivative of the log likelihood with respect
to κ is negative, its solution is the mle.
Let ϒ ¼ υ λ
ð Þ be a one-to-one (i.e., monotonically increasing or decreasing) mapping of λ.
The likelihood function (or the log likelihood function) of υ (denoted ^υk) will be maxi-
mized at λ ¼ ^λk. This is the equivariance property of the mle: If ^λk is the mle for λ, then
^υk ¼ υ
^λk

is the mle for ϒ.
Example 5.24 It has been established that the mles for the mean and variance of a normal
distribution are the sample mean and sample variance, respectively. Then the mle for the
standard deviation is the square root of the sample variance.
Choosing the mle ^λk to maximize the likelihood function L is equivalent to choosing ^λk to
maximize V in the Fisher-Neyman factorization theorem (5.16), and hence ^λk must be a
function of only a sufﬁcient statistic or must itself be a sufﬁcient statistic. Further, ^λk will
be unique and inherit all the optimality properties of sufﬁcient statistics.
Example 5.25 The mle for the parameter in the Poisson distribution is the sample mean.
Since it is an unbiased estimator, sufﬁciency establishes that it is the unique efﬁcient
estimator for the parameter.
Example 5.26 The sufﬁcient statistic for the uniform distribution on (0, λ) is the largest
order statistic x N
ð Þ. From inspection of the joint pdf given in Example 5.9, it is obvious that
the mle is also the largest order statistic. However, this cannot be an unbiased estimator
because x N
ð Þ  λ.
The mle has a number of desirable optimality properties. Under appropriate smoothness
constraints on the likelihood function, the mle from an iid sample is an asymptotically
consistent estimator. This applies whether there is a unique maximum for the likelihood
function or not. The mle is also asymptotically normal with variance given by the inverse
of the sample Fisher information (or for vector mles, a covariance matrix given by the
inverse of the sample Fisher information matrix). Mathematically, this is
^λN  λ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1=IN ^λN


r
!
d
N 0; 1
ð
Þ
(5.22)
157
5.4 Point Estimation: Maximum Likelihood Estimator
.006
13:32:26, subject to the Cambridge Core terms of use,

Consequently, the standard error on the mle can be obtained from the Fisher information or
alternately using the delta method of Section 4.7.4. The mle is asymptotically efﬁcient,
achieving the Cramér-Rao lower bound as the sample size approaches inﬁnity. This means
in particular that no asymptotically unbiased estimator has a lower asymptotic mse than the
mle. The mle is also asymptotically minimax. However, note that all these properties are
large sample ones and do not specify the ﬁnite sample properties. Finally, if the mle is
unique, then it is a function of the minimal sufﬁcient statistic. This is especially useful for
the exponential family of distributions, where the minimal sufﬁcient statistic is usually
easy to ﬁnd.
MATLAB simpliﬁes the process of obtaining the mle for the distributions that it
supports. For example, it provides the function parmhat = gevﬁt(x) that computes the
three parameters in the generalized extreme value distribution given the data in x, where
parmhat is a 3-vector containing the shape, scale, and location parameters. It provides
similar estimators for the mle for most of the distributions described in Chapter 3.
Alternately, invoking the ﬁtdist function creates a distribution object with mle parameters.
For example, pd = ﬁtdist(x, ‘Normal’) creates a distribution object from the data in x with
mle parameters for the mean and variance.
The same functionality can be obtained using the general mle function that is invoked
using parmhat = mle(x, ‘distribution’, dist), where dist is a character string that speciﬁes
the distribution. The list of distributions mle supports is more extensive than those directly
implemented as pdf, cdf, and quantile functions.
The function mle also enables the user to deﬁne the distribution to be ﬁt using parmhat =
mle(x, ‘pdf ’, pdf, ‘cdf ’, cdf, ‘start’, start), where pdf and cdf are function handles to
custom pdf and cdf routines, and start is an initial guess for the parameters. It is also
possible to substitute the negative log likelihood for the pdf and cdf using the keyword
‘nloglf’.
The function mle is a wrapper to call one of the two optimization functions in
MATLAB, with the default being fminsearch and the alternative being fmincon. The
former is an unconstrained nonlinear multivariable function minimizer based on the
simplex method of Lagarias et al. (1998) that does not require numerical differentiation
but is rather a direct search approach. The fmincon algorithm allows constraints to be
applied to the minimization and, in addition, allows the gradient and Hessian matrix to be
user provided for greater accuracy.
For complicated situations, the “Optimization Toolbox” provides additional function
minimizers that may be more suitable. For example, the function fminunc is a multi-
variable nonlinear unconstrained function minimizer. Calling this routine with a user-
supplied routine that provides the negative log likelihood function along with its
gradient and Hessian typically gives better performance than mle with fminsearch.
There are even more sophisticated algorithms in the MATLAB “Global Optimization
Toolbox.”
MATLAB provides the function mlecov that computes the asymptotic covariance matrix
for the mle. In its simplest form, it is invoked using acov = mlecov(parmhat, x, ‘pdf ’, pdf ),
where pdf is a function handle. An alternative call based on the negative log likelihood
function is also available.
158
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

Example 5.27 The ﬁle geyser.dat contains 299 measurements of the waiting time in minutes
between eruptions of Old Faithful in Yellowstone National Park taken from Azzalini &
Bowman (1990). A kernel density plot of the data shows that their distribution is bimodal.
Fit a mixture Gaussian distribution to the data using the mle.
The mixture pdf for a single rv is given by
f x; μ1; μ2; σ2
1; σ2
2; α


¼ αN μ1; σ2
1


þ 1  α
ð
ÞN μ2; σ2
2


where 0  α  1. The log likelihood is given by
log L μ1; μ2; σ2
1; σ2
2; α; x


¼
X
N
i¼1
log αN xi; μ1; σ2
1


þ 1  α
ð
ÞN xi; μ2; σ2
2




A MATLAB script to ﬁt this distribution is
fun = @(lambda, data, cens, freq) - nansum(log(lambda(5)/
(sqrt(2*pi)*lambda(3))* . . .
exp(-((data - lambda(1))/lambda(3)).^2/2) + . . .
(1 - lambda(5))/(sqrt(2*pi)*lambda(4))* . . .
exp(-((data - lambda(2))/lambda(4)).^2/2)));
start = [55 80 5 10 .4];
parm = mle(geyser, 'nloglf', fun, 'start', start)
Warning: Maximum likelihood estimation did not converge. Iteration limit exceeded.
In stats/private/mlecustom at 241
In mle at 229
parm =
54.2026
80.3603
4.9520
7.5076
0.3076
Note the use of nansum in the negative log likelihood function to avoid overﬂow when
the objective function is far from its minimum. The four arguments to the function handle
must be provided even when the last two are not used. The start vector is an eyeball ﬁt to
the data plot in Figure 5.2, and the parameters are in the order μ1, μ2, σ1, σ2, α. The
20
40
60
80
100
120
140
Waiting Time (min)
0
0.01
0.02
0.03
0.04
Probability Density
Figure 5.2
Kernel density estimator (solid line) and mixture Gaussian distribution with mle parameters (dashed line) for the Old
Faithful waiting time data.
159
5.4 Point Estimation: Maximum Likelihood Estimator
.006
13:32:26, subject to the Cambridge Core terms of use,

5.5 Interval Estimation: Conﬁdence and Tolerance Intervals
Conﬁdence intervals are an alternative to point estimators for characterizing an unknown
parameter. Instead of simply estimating the parameter, an interval on which it lies is
constructed for which the probability may be stated a priori.
Assume that Xi
f
g are a random sample from a distribution with parameter λ. Let Y be an
rv that is a function of
Xi
f
g and λ with a continuous cdf F y
ð Þ that does not explicitly
depend on λ. There exists a number υ such that
F υ
ð Þ ¼ Pr y xi; λ
ð
Þ  υ
ð
Þ ¼ 1  α
(5.23)
for which the probability inequality indicates a constraint on λ. The constraint may be
written as ξ0  λ  ξ1, where ξ0 and ξ1 are speciﬁc values of a statistic that does not
depend on λ. Consequently,
F ξ1
ð
Þ  F ξ0
ð
Þ ¼ Pr ξ0  λ  ξ1
ð
Þ ¼ 1  α
(5.24)
or, equivalently,
solution did not converge before the default limit to the number of iterations was reached.
This can be set to a higher value using “options” and the statset function to change its
value.
parm = mle(geyser, 'nloglf', fun, 'start', start, 'options',
statset('MaxIter', 300))
parm =
54.2026
80.3603
4.9520
7.5076
0.3076
The result is identical to four signiﬁcant ﬁgures.
m1 = parm(1);
m2 = parm(2);
s1 = parm(3);
s2 = parm(4);
a = parm(5);
ksdensity(geyser)
hold on
x = 20:.1:140;
plot(x, a/(sqrt(2*pi)*s1)*exp(-((x - m1)/s1).^2/2) + . . .
(1 - a)/(sqrt(2*pi)*s2)*exp(-((x - m2)/s2).^2/2))
The result is shown in Figure 5.2. The data are bimodal with approximate modes of
55 and 80 minutes. The ﬁtted distribution matches the means in the kernel density
estimator but is more sharply peaked, probably because a Gaussian distribution is not
the appropriate model. A distribution with ﬂatter peaks would be more appropriate.
160
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

ðξ1
ξ0
f x
ð Þdx ¼ 1  α
(5.25)
The quantity 1  α is the probability that the random interval
ξ0; ξ1
ð
Þ contains the
parameter λ. The interval
ξ0; ξ1
ð
Þ is a conﬁdence interval for λ, and ξ0 and ξ1 are the
lower and upper conﬁdence limits. The conﬁdence level is 1  α.
Caution: A conﬁdence interval is not a probability statement about λ because λ is a ﬁxed
parameter rather than an rv.
In Example 5.28, the conﬁdence interval was obtained by assigning α/2 of the
probability to lie below ξ and above ξ in (5.24), respectively. This choice is
called the central conﬁdence interval but is in reality only one of an inﬁnite number
of possible choices. Conﬁdence intervals with different amounts of probability below
and above the lower and upper conﬁdence limits are called noncentral. A noncentral
conﬁdence interval should be used (for example) when the penalty for overestimat-
ing a statistic is higher than that for underestimating it. In such an instance, the
one-sided conﬁdence interval obtained from Pr λ  ξ1
ð
Þ ¼ 1  α would be appropri-
ate. Central conﬁdence limits typically have a shorter length than noncentral ones
and are usually the most appropriate ones to use. Central conﬁdence limits will be
symmetric about the sample statistic only for symmetric distributions, such as
Student’s t.
Example 5.28 Let
Xi
f
g be a random sample from N μ; σ2
ð
Þ with unknown mean
and variance. The statistic ^tN1 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
XN  μ


=^sN, where ^sN is the sample
standard deviation given by the square root of (4.14), is distributed as Student’s t
(4.72) with N  1 degrees-of-freedom. As a result, the conﬁdence interval on μ
deﬁned by
Pr XN þ
ξ0^sN
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
 μ  XN þ
ξ1^sN
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p

	
¼ 1  α
requires choosing a constant ξ such that ξ0
j
j ¼ ξ1 ¼ ξ, where
ðξ
-ξ
teeN1 x
ð Þdx ¼ 1  α
The value of ξ can be computed for given choices of α and N from the cdf TeeN1 x
ð Þ.
Because of symmetry, 2TeeN1 ξ
ð Þ  1 ¼ 1  α, so TeeN1 ξ
ð Þ ¼ 1  α=2.
161
5.5 Interval Estimation: Conﬁdence and Tolerance Intervals
.006
13:32:26, subject to the Cambridge Core terms of use,

The result in Example 5.29 is approximate because binoinv returns the least integer x such
that the binomial cdf evaluated at x equals or exceeds the probabilities 0.025 or 0.975. This
is called a conservative conﬁdence interval and follows from the probability statement
Pr ξ0  λ  ξ1
ð
Þ  1  α
(5.26)
This conﬁdence interval states that the parameter λ lies on ξ0; ξ1
ð
Þ at least 100 1  α
ð
Þ% of
the time. The conservative conﬁdence interval may be the only one that can be constructed,
especially for discrete distributions.
Statistical problems often present with several parameters rather than just one, and hence
the requirement for simultaneous conﬁdence intervals on several parameters arises. One
obvious approach would be simultaneous conﬁdence interval assertions such as
Pr ξ0  λ0  ξ1 \ ζ 0  λ1  ζ 1
ð
Þ ¼ 1  α
(5.27)
This is rarely possible unless the parameters are independent, and even then it is usually
impractical for more than two parameters.
With p parameters, the use of p univariate 1  α conﬁdence intervals is inappropriate
because the overall tail probability pα will exceed the desired value of α. The most widely
Example 5.29 The central conﬁdence interval for the binomial distribution may be obtained
by solving
X
j
k¼0
N
k

	
pk
u 1  pu
ð
ÞNk ¼ α=2
and
X
N
k¼j
N
k

	
pk
l 1  pl
ð
ÞNk ¼ α=2
for pl and pu for a given choice of j and N. Let N = 20 and j = 5, so the mle for the
distribution parameter ^p ¼ 0:25. An exact answer can be obtained by solving these two
equations numerically, but an approximate answer ensues from using the binomial quantile
function.
binoinv(.025, 20, .25)
ans =
2
binoinv(.975, 20, .25)
ans =
9
Note that the conﬁdence interval (2, 9) is not symmetric about 5 because the binomial
distribution is skewed when the probability p 6¼ 0:5.
162
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

used solution to this quandary is the Bonferroni conﬁdence interval based on a family of
probability inequalities, as formalized in Section 6.6. The general idea is to use the
Student’s t statistic as for the univariate conﬁdence interval after dividing the signiﬁcance
level among the variables for which a simultaneous conﬁdence interval is desired by using
1 – α/p in the usual conﬁdence interval expression. The Bonferroni approach does allow
some variables to be given more signiﬁcance than others, but generally they are treated
equally. The result is conservative because the overall conﬁdence interval on all the
variables is no greater than 1  α.
Example 5.30 Returning to the geyser data of Example 5.27, compute Bonferroni conﬁ-
dence intervals for the parameters.
The asymptotic covariance matrix is easily obtained by calling mlecov.
acov = mlecov(parm, geyser, 'nloglf', fun)
acov =
0.4666
0.1545
0.1606
-0.1417
0.0064
0.1545
0.4012
0.1176
-0.1235
0.0052
0.1606
0.1176
0.2686
-0.1048
0.0050
-0.1417
-0.1235
-0.1048
0.2571
-0.0049
0.0064
0.0052
0.0050
-0.0049
0.0009
The off-diagonal terms are not vanishingly small but will be ignored for present
purposes. The uncertainties on the parameters follow from (5.22) using the diagonal
elements of the inverse of the Fisher information matrix (5.10) output by mlecov. Assum-
ing that the total tail probability is 0.05, the normal p-quantile 2.3263 for a probability of
1  0:05=5 ¼ 0:99 is used to construct conﬁdence intervals on the parameters. The
MATLAB script is
xp = norminv(1 - .05/5);
sigma=sqrt(diag(acov));
[parm' - xp*sigma parm' + xp*sigma]
ans =
52.6136
55.7917
78.8868
81.8338
3.7464
6.1576
6.3280
8.6873
0.2368
0.3784
Note that the conﬁdence bounds on the means are much tighter than for the remaining
parameters.
The concept of conﬁdence limits extends to the order statistics in a straightforward manner
and yields the remarkable result that the conﬁdence intervals of the quantiles are distribu-
tion free that is due originally to Thompson (1936). Let the p-quantile be given by
163
5.5 Interval Estimation: Conﬁdence and Tolerance Intervals
.006
13:32:26, subject to the Cambridge Core terms of use,

xp ¼ F1 p
ð Þ, where p is a probability level. The probability that xp lies between two order
statistics X rð Þ and X sð Þ (assuming that r < s) is (David 1981, sec. 2.5)
Pr X rð Þ < xp < X sð Þ


¼ Pr X rð Þ  xp


 Pr X sð Þ  xp


¼ Beta p; r; N  r þ 1
ð
Þ  Beta p; s; N  s þ 1
ð
Þ
(5.28)
Equating (5.28) to the probability 1  α gives a result that does not depend on F x
ð Þ or f x
ð Þ
and hence on the parent distribution. This result holds for any discrete or continuous
distribution.
Example 5.31 Find the α ¼ 0:05 conﬁdence interval on the sample median for sample sizes
N = 11, 101, and 1001.
From symmetry considerations, the indices of the conﬁdence interval on the sample
median will be equal distances from that of the median; hence s ¼ N  r þ 1. The
solution of
Beta 0:5; i; N  i þ 1
ð
Þ  Beta 0:5; N  i þ 1; i
ð
Þ  0:95
for r given N = 11, 101, and 1001 can be obtained by direct evaluation starting with i ¼ 1
and extending to i ¼ N=2
b
c þ 1. For N = 11, this yields
n = 11;
for i = 1:ﬂoor(n/2) + 1
[i betacdf(.5, i, n – i + 1) - betacdf(.5, n – i + 1, 1)]
end
1.0000
0.9990
2.0000
0.9932
3.0000
0.9653
4.0000
0.8828
5.0000
0.7178
6.0000
0.4844
so
X 3
ð Þ  ~X11  X 9
ð Þ.
Repeating
this
for
the
remaining
sample
sizes
gives
X 42
ð
Þ  ~X101  X 60
ð
Þ and X 474
ð
Þ  ~X1001  X 528
ð
Þ, respectively. These conservative con-
ﬁdence intervals apply for any distribution for the sample.
A closely related entity to the conﬁdence interval is the tolerance interval, which is a
conﬁdence interval for a distribution rather than for a statistic. The distinction can be
illustrated by considering a collection of rock samples, each of which has had its p-wave
velocity measured in the laboratory. The upper conﬁdence limit for the mean of the
p-wave velocities is an upper limit to the mean of all the samples. The upper tolerance
limit is an upper bound on the p-wave velocity that a speciﬁed fraction of the samples
will not exceed.
A tolerance interval contains at least a proportion p of the population with probability α.
Mathematically, the random interval ξ0; ξ1
ð
Þ deﬁned by
164
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

Pr F ξ1
ð
Þ  F ξ0
ð
Þ  p
½
 ¼ α
(5.29)
where p is a free parameter, deﬁnes the tolerance interval at probability level α. The
tolerance bound deﬁned by (5.29) is independent of the usually unknown cdf F x
ð Þ if and
only if ξ0 and ξ1 are order statistics. Let
X ið Þ


be the order statistics obtained by ranking
a random sample
Xi
f
g, and let ξ0 ¼ X rð Þ and ξ1 ¼ X sð Þ, where 1  r < s  N. As can
be easily shown, the random interval F X sð Þ


 F X rð Þ


e Beta x; s  r; N  s þ r þ 1
ð
Þ.
Consequently, for a given choice of N, p, and α, the index difference s  r can be
computed using
Beta 1  p; N  s þ r þ 1; s  r
ð
Þ ¼ α
(5.30)
where the symmetry property Beta x; a; b
ð
Þ ¼ Beta 1  x; b; a
ð
Þ has been applied. The
outcome is a distribution-free method to estimate the tolerance interval for a random
sample.
Example 5.32 Estimate the tolerance interval at α ¼ 0:05 for a sample of size 100 and p =
0.5, 0.75, and 0.95.
Let i ¼ s  r so that Beta 1  p; 100  i þ 1; i
ð
Þ ¼ 0:05 is to be solved for i. By trial
and error using MATLAB,
for i = 40:50
[i betacdf (.5, 100 – i + 1, i) - 0.05]
end
ans =
40.0000
-0.0324
41.0000
-0.0216
42.0000
-0.0057
43.0000
0.0166
44.0000
0.0467
45.0000
0.0856
46.0000
0.1341
47.0000
0.1921
48.0000
0.2586
49.0000
0.3322
50.0000
0.4102
and hence s  r ¼ 42 for p = 0.5. Any interval X rð Þ; X rþ42
ð
Þ


for r = 1,. . ., 58 serves as
a tolerance interval for p = 0.5, α = 0.05. Solving for the remaining two cases gives s 
r = 18 and 2, respectively. In practice, it is usually the lower (r = 0) or upper
(s ¼ N þ 1) tolerance intervals that are of interest. These limiting values establish the
proportion of the population that exceed or fall below a given order statistic,
respectively.
165
5.5 Interval Estimation: Conﬁdence and Tolerance Intervals
.006
13:32:26, subject to the Cambridge Core terms of use,

5.6 Ratio Estimators
Ratios are frequently of interest in the earth sciences and are a long-standing problem in
statistics for which the estimation of a conﬁdence interval is often difﬁcult. There are
two statistically distinct approaches: computing either the ratio of averages or the
average of ratios. The latter often results in a distribution with inﬁnite variance, and
hence the median becomes the most useful measure of location; conﬁdence intervals on
it can be constructed from (5.28). It is more common to use a ratio of averages
approach. If the numerator and denominator of a ratio are themselves averages, then
each may be well behaved such that the classic central limit theorem pertains separately
to the numerator and denominator, and hence either Fieller’s theorem (described below)
or the delta method of Section 4.7.4 may be used for statistical inference. However, it is
difﬁcult to quantify when the asymptotic limit is reached. Further, if the ratio has
bounded support, there is no guarantee that a ratio of averages will maintain the
bounds.
It is easy to show that the ratio estimator is always biased. Let an estimator ^r be
obtained from the ratio of the sample means of the random variables Y and X. A ﬁrst-
order Taylor series expansion about the population parameters for a sample of size N
yields
E ^rð Þ  r þ
1
Nμ2
X
rσ2
X  cov X; Y
½



(5.31)
Consequently, the sample ratio estimate bias increases if the coefﬁcient of variation (ratio
of the standard deviation to the mean) of the denominator becomes large and is reduced
when the covariance of the two variables has the same sign as r. A naive bias-corrected
form follows by subtracting the second term of (5.31) from ^r. However, the bias is O 1=N
ð
Þ
and hence is small when N is of moderate size.
The most general approach to conﬁdence interval estimation for ratios is due to
Fieller (1940, 1944, 1954). The construction of Fieller conﬁdence limits for the ratio
r ¼ μy=μx follows by noting that since linear combinations of Gaussian variables remain
Gaussian, so is μy  rμx. Consequently, dividing the sample version of this term by an
estimate of its standard deviation yields a quantity that is distributed as Student’s t. That
statistic is
^T ¼
YN  ^r XN
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
var YN


 2^r cov YN; XN


þ ^r2var XN


q
(5.32)
^T is a pivot because (5.32) has a distribution that does not depend on any of its constituent
parameters. Let tα=2 be the 1  α=2 quantile of Student’s t distribution with ν degrees-of-
freedom. The Fieller conﬁdence interval follows from the deﬁnition
1  α ¼ Pr tα=2  ^T  tα=2


¼ Pr ^α r2 þ ^β r þ ^χ ¼ 0
h
i
(5.33)
166
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

where
^α ¼ XN

2  t2
α=2var XN


^β ¼ 2 t2
α=2cov YN; XN


 YNXN
h
i
^χ ¼ YN

2  t2
α=2var YN


(5.34)
and corresponds to the set of r values for which ^T lies within the 1  α range of Student’s t.
The conﬁdence interval is given by
^β 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^β2  4 ^α ^χ
q
2^α
 r 
^β þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^β2  4 ^α ^χ
q
2^α
(5.35)
provided that ^α  0 and ^β
2  4 ^α ^χ  0. Equation (5.35) is asymmetric about the sample
estimate for r.
Equation (5.35) is the solution to (5.33) when the square of the denominator divided by
its variance is signiﬁcant, or
XN

2=var XN


> t2
α=2. There are two other cases: when
^β
2  4 ^α ^χ  0 and ^α < 0, the conﬁdence interval is the complement of (5.35) given by
∞;
^β þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^β2  4 ^α ^χ
q
2^α
3
5
0
@
 r 
^β 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^β2  4 ^α ^χ
q
2^α
, ∞
2
4
1
A
(5.36)
whereas if ^β
2  4 ^α ^χ < 0 and ^α < 0, the conﬁdence interval is ∞; ∞
ð
Þ. In both of these
cases, the conﬁdence interval is inﬁnite, and little or nothing can be said about r.
The occurrence of inﬁnite conﬁdence intervals is a consequence of the denominator
becoming arbitrarily close to zero, and Von Luxburg & Franz (2009) give an intuitive
explanation for its effects. Equation (5.36) pertains when the denominator is not signiﬁ-
cantly different from zero, but the numerator is well deﬁned. In that instance, as the
numerator is divided by a number that is close to zero, the absolute value and sign of
the result are uncontrolled, leading to a disjoint pair of inﬁnite conﬁdence intervals. If the
numerator is also zero within the statistical uncertainty, then any result is possible because
zero divided by zero is undeﬁned. In either case, the statistic of interest is not well
speciﬁed, and little can be concluded from the data.
A widely used alternative to Fieller’s method is the delta method of Section 4.7.4. This
can be applied directly to a ratio y=x by expanding that quantity in a two-variable ﬁrst-order
Taylor series and then taking the variance of the result. For the ratio of the sample means,
this is
var
YN
XN

	
 YN

2
XN

2
var YN


YN

2
 2 cov YN; XN


YNXN
þ
"
var XN


XN

2
#
(5.37)
It is common practice to neglect the middle covariance term in (5.37), although this can
lead to large, undetectable errors.
Delta method conﬁdence intervals using Student’s t may be computed with the standard
error obtained from (5.37). Such a conﬁdence interval will always be symmetric about ^r
167
5.6 Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

and cannot be unbounded, in contrast to the Fieller conﬁdence interval. The delta method
will break down when the denominator of the ratio statistically approaches zero and will
severely underestimate the conﬁdence interval as that limit is approached. However, the
delta method produces a reasonable approximation to the Fieller conﬁdence interval when
the denominator is sufﬁciently precise.
Hirschberg & Lye (2010) provide a lucid geometric representation and comparison of
the Fieller and delta methods and suggest that the delta method produces a good approxi-
mation to the Fieller interval when the denominator of the ratio is precise and the signs of
the ratio and the covariance of the numerator and denominator in (5.37) are the same, but
the method is less accurate when the signs of the ratio and the covariance of the numerator
and denominator in (5.37) differ. They also indicate that when their results differ, the
Fieller interval provides better coverage and hence is preferred.
Gleser & Hwang (1987) proved that any statistical method that is not able to produce
inﬁnite conﬁdence intervals for a ratio will lead to arbitrarily large errors or conversely, will
have coverage probability that is arbitrarily small. For this reason, the Fieller approach is
preferred to the delta method, and given that it does not require a substantial increase in
computational burden, it is recommended for general use.
168
Point, Interval, and Ratio Estimators
.006
13:32:26, subject to the Cambridge Core terms of use,

6
Hypothesis Testing
6.1 Introduction
In hypothesis testing, the goal is to make a decision to accept or reject some statement
about the population based on a random sample. The starting point is a statistical
hypothesis that is a conjecture about one or more populations. This may be phrased in
working terms, or it may be stated mathematically. Some instances of the former include
• An engineer believes that there is a difference in accuracy between two sensors; and
• A climate scientist thinks that the mean surface temperature of Earth has increased over
the past decade.
Statistical hypotheses are formulated in two parts under the Neyman-Pearson lemma
(Neyman & Pearson 1933) that constitutes its theoretical basis. The Neyman-Pearson
lemma will be described quantitatively in Section 6.5, but the basics are included at the
outset. The ﬁrst component is the null hypothesis denoted by H0, which is the conjecture to
be tested. Departures from the null hypothesis are usually of interest. The second part is the
alternate hypothesis denoted by H1. If H0 is rejected, then H1 is accepted. However, it can
never be known with absolute certainty that a given hypothesis is true, and hence the
statement is made that the null hypothesis is accepted (or rejected) at some probability level
based on a statistical test. For example,
• The engineer formulates the null hypothesis that there is no difference in sensor
accuracy against the alternate hypothesis that there is; and
• The climate scientist formulates the null hypothesis that there is no increase in surface
temperature against the alternate hypothesis that there is.
This could be made more speciﬁc; for example, the test for the mean surface temperature μ
could specify the hypothesis in the form
H0: μ = 20C
H1: μ > 20C
Hypotheses can either be simple or composite, with the latter predominating in the real
world. This is best illustrated through examples. Let Xi
f
g be a set of random variables (rvs)
from a Gaussian distribution having a known variance. A null hypothesis is formulated that
speciﬁes a mean of μ1 against an alternate hypothesis that the mean is μ2. Since σ2 is
known a priori, either hypothesis completely speciﬁes the outcome. This is an example of a
simple hypothesis.
169
.007
13:35:08, subject to the Cambridge Core terms of use,

Alternately, let
Xi
f
g be a set of rvs from a discrete probability distribution. A null
hypothesis is formulated that the rvs are drawn from a Poisson distribution against an
alternate hypothesis that they are not Poisson. Neither hypothesis completely speciﬁes the
distribution (e.g., under H0, the parameter λ for the Poisson distribution is not given), so
this is an example of a composite hypothesis. Specifying λ in the null hypothesis would
convert it to a simple hypothesis. In general, if a sampling distribution depends on k
parameters and a hypothesis speciﬁes m of them, the hypothesis is simple if m = k and
composite if m < k. Composite hypotheses involve k  m nuisance parameters that must
be accounted for in some way.
In summary, the steps in testing a hypothesis under the Neyman-Pearson lemma are as
follows:
1. Formulate the null and alternate hypotheses using mathematical expressions. This
typically makes a statement about a characteristic or a descriptive measure of a
population.
2. Collect a random sample from the population of interest.
3. Calculate a statistic from the sample that provides information about the null hypothesis.
4. If the value of the statistic is consistent with the null hypothesis, then accept H0. This
requires knowledge of the probability distribution of the test statistic under the null
hypothesis.
5. If the value of the statistic is not consistent with the null hypothesis, then reject H0 and
accept H1. This is an application of the 0–1 decision rule of Section 5.2.6.
Example 6.1 Returning to the climate example, suppose that the scientist takes a random
sample of 100 evenly distributed points on Earth and obtains historical temperature meas-
urements at each over time. She uses the areal and temporal sample means to determine
whether there is sufﬁcient evidence to reject the null hypothesis and hence conclude that the
mean temperature has increased. The sample mean that she obtains is 22C, which is
slightly larger than the 20C value for the null hypothesis. However, the sample mean is
an rv and has some variability associated with it; if the variance of the sample mean under
the null hypothesis is large, then the difference between 22 and 20 might not be
meaningful. If the scientist assumes that the surface temperature is normally distributed
with a standard deviation of 6 based on prior experience, then she knows that the sample
mean is normally distributed with mean 20C and standard error 6/
ﬃﬃﬃﬃﬃﬃﬃﬃ
100
p
= 0.6C under the
null hypothesis. Standardizing yields the test statistic ^z ¼ 22  20
ð
Þ=0:6 ¼ 3:33. The test
statistic is 3.33 standard deviations from the mean if the null hypothesis is true, and only
~0.3% of normally distributed rvs fall outside 3 standard deviations of the mean. Thus 22C
is not consistent with the null hypothesis except at an unreasonable probability level, and the
alternate hypothesis is accepted.
This description illustrates the basic ideas behind hypothesis testing in statistics that pertain
whether the approach is parametric (meaning that the distributions of the data and hence
the test statistic are known a priori, as covered in this chapter), nonparametric (meaning
170
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

that distributional assumptions are relaxed), or based on resampling methods (see
Chapter 8). While there were earlier approaches to hypothesis testing, the Neyman-Pearson
lemma constitutes the basis for testing statistical conjectures in modern works. Nearly
concurrently with its presentation, Pitman (1937a) devised exact permutation methods
based on the Neyman-Pearson approach that entailed only the assumption of exchange-
ability (see Section 2.6) but also required a level of computation that was not feasible until
decades later. This led to the substitution of ranks for data in the 1940s and 1950s such that
the permutation calculations need be completed only once and tabulated, resulting in many
of the nonparametric methods described in Chapter 7. As computing power increased
dramatically from the 1970s to present, the direct application of permutation methods
became more widespread, and in addition, bootstrap resampling methods were invented by
Efron (1979). These are covered in Chapter 8 and are typically preferred when dealing with
real-world data.
6.2 Theory of Hypothesis Tests I
To formalize these statements in terms of set theory, hypothesis testing is a decision about
whether a parameter λ lies in one subset of the parameter space S0 or in its complement S1.
Consider a parameter λ whose value is unknown but that lies in the parameter space
S ¼ S0 [ S1. Assume that
S0 \ S1 ¼ ∅
H0 : λ 2 S0
H1 : λ 2 S1
(6.1)
The statistical algorithm to accept H0 is called the test procedure, and the statistic used to
make that decision is called the test statistic, denoted by ^λ. The distribution of the test
statistic under the null hypothesis is called the null distribution. The critical region
(sometimes called the rejection region) is a region for the test statistic over which the null
hypothesis would reject. The critical value is the locus for the test statistic that divides its
domain into regions where H0 will be rejected or accepted. The critical region depends on
the distribution of the statistic under the null hypothesis, the alternate hypothesis, and the
amount of error that can be tolerated. Typically, the critical region is in the tails of the
distribution of the test statistic when H0 is true. Consequently, there are three standard cases:
1. If a large value of the test statistic would reject the null hypothesis, then the critical
region is in the upper tail of the null distribution. This is called an upper tail test and
would be speciﬁed as H1: λ > ^λ.
2. If a small value of the test statistic would reject the null hypothesis, then the critical
region is in the lower tail of the null distribution. This is called a lower tail test and
would be speciﬁed as H1: λ < ^λ.
3. If either small or large values for the test statistic would reject the null hypothesis, then
the critical region is in both the lower and upper tails. This is called a two-tail test and
would be speciﬁed as H1: λ 6¼ ^λ.
171
6.2 Theory of Hypothesis Tests I
.007
13:35:08, subject to the Cambridge Core terms of use,

There are two kinds of error that can occur when a statistical hypothesis testing decision
is made, and there is an inherent asymmetry between them. The ﬁrst is called a Type 1
error, or a false positive, or the error caused by rejecting the null hypothesis when it is true.
Its probability is denoted by α. Mathematically, this is the conditional probability statement
α ¼ Pr
^λ 2 S1jH0

(6.2)
The second type of error is called a Type 2 error, or a false negative, or the error caused by
accepting the null hypothesis when it is false. Its probability is denoted by β. Mathematic-
ally, this is
β ¼ Pr
^λ 2 S0jH1

(6.3)
Table 6.1 summarizes the possible outcomes and their probabilities from a single
hypothesis test.
It is standard practice to search for signiﬁcant evidence that the alternate hypothesis is
true and not to reject the null hypothesis unless there is sufﬁcient data-driven evidence to
lead to that conclusion. The probability of making a Type 1 error is called the signiﬁcance
level of the test when the hypothesis is simple. If the null hypothesis is composite, the
probability of a Type 1 error will depend on which particular part of the null hypothesis is
true. In this case, the signiﬁcance level is usually taken as the supremum of all the
probabilities. The signiﬁcance level is a free parameter chosen by the analyst and repre-
sents the maximum acceptable probability for a Type 1 error. Typical values for α lie
between 0.01 and 0.05, although larger values might be used in special circumstances. The
critical value is the quantile of the null distribution that gives a signiﬁcance level of α.
Example 6.2 Returning to the climate example, the test statistic is ^z = 3.33. A signiﬁcance
level α of 0.05 is chosen. Since the alternate hypothesis holds that the temperature has
increased, a large value of the test statistic supports H1. The critical value using the
MATLAB function norminv(0.95) is 1.645. Thus, if ^z  1.645, H0 can be rejected at
the 0.05 probability level. This is an upper-tail test.
The probability of making a Type 2 error depends on β, which, in turn, depends on the
sample size and the alternate hypothesis. The probability of not detecting a departure from
the null hypothesis depends on the distribution of the test statistic under the alternate
hypothesis, in contrast to Type 1 error, which depends on the distribution under the null
hypothesis. However, there are many possible alternate hypotheses, so there are typically
many distributions to consider. This makes consideration of Type 2 errors inherently more
Table 6.1 Outcomes from a Single Hypothesis Test
Declared true
Declared false
True null
Correct (1  α)
Type 1 error (α)
False null
Type 2 error (β)
Correct (1 β)
172
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

complicated. In most cases, the null distribution is one of the common sampling distribu-
tions of Section 4.9, whereas the alternate hypothesis is the noncentral form of the same
distribution, with the noncentrality parameter describing the range of the alternate hypoth-
eses that can be tested.
Rather than specifying or computing β, it is standard practice to determine the probabil-
ity that H0 is rejected when it is false, which is called the power of the test and is given by
βC ¼ 1  β. Mathematically, this is
βC ¼ Pr
^λ 2 S1jH1

(6.4)
The power is a measure of the ability of the hypothesis test to detect a false null hypothesis.
The power depends on the magnitude of the difference between the population and
hypothesized means, with power increasing as this rises. Since a larger value of α makes
it easier to reject the null hypothesis, it increases the power of the test. In addition, a larger
sample size increases the precision of the test statistic and hence increases the power of the
test. It is standard practice in experimental design to choose α and β (typically, by placing a
lower bound on it, such as requiring it to be at least 0.8) and determine the number of data
required to achieve that signiﬁcance level and power. Finally, for a given α; β
ð
Þ, a one-tail
test is more powerful than a two-tail test and is preferred if it is appropriate for a given
statistical problem.
Figure 6.1 illustrates the concepts of signiﬁcance level and power. The solid line denotes
the null distribution, and the dotted line shows an instance of the alternate distribution. The
tail area for a two-sided hypothesis is the area beneath the gray portion of the null
distribution, each part of which contains α=2 of the tail probability. The probability of a
Type 2 error β is the area under the alternate distribution extending from the lower α=2 tail
of the null distribution to its upper α=2 tail. Consequently, the power is
βC ¼ 1  FH1 x1α=2; δ


þ FH1 xα=2; δ


(6.5)
where FH1 is the alternate cdf, xp is the p-quantile of the null distribution, and δ is
a measure of the distance between the test statistic and its hypothesized value. For an
β
α/2
α/2
Figure 6.1
The null distribution (solid line) and an alternate distribution (dashed line) illustrating the role of α and β in
hypothesis testing. The gray part of the null distribution denotes the tail area for a two-sided hypothesis, so a
probability of α=2 is given by the area beneath each gray segment. The Type 2 error probability β is the area
beneath the alternate distribution extending between the α=2 and 1  α=2 quantiles of the null distribution.
173
6.2 Theory of Hypothesis Tests I
.007
13:35:08, subject to the Cambridge Core terms of use,

upper-tail test, the last term in (6.5) is omitted, and α=2 is replaced by α. A power curve can
be constructed by computing (6.5) for a range of alternate hypotheses (i.e., by varying δ).
An ideal test would have α ¼ β ¼ 0 or α ¼ 0, βC ¼ 1, but this is impossible to achieve
in practice. Further, for a ﬁxed sample size, in order to decrease α, β must be increased, or
vice versa. Under the Neyman-Pearson paradigm, this conﬂict is resolved by accepting the
asymmetry between Type 1 and Type 2 errors. The signiﬁcance level α is a free parameter
that is ﬁxed in advance, and then a test and sample size are sought that maximize the
power. The “best” test is deﬁned to be the one that maximizes βC for a given α out of all
possible tests and is called the most powerful test. However, it is possible for a test to be
most powerful for some but not all alternates. If a given test can be shown to be more
powerful against all alternates, it is uniformly most powerful (or UMP). A test is admissible
(Section 5.2.6) if it is UMP and no other test is more powerful against all alternates.
For a composite hypothesis, a test is exact if the probability of a Type 1 error is α for all
the possibilities that make up the hypothesis. Exact tests are very desirable but often are
difﬁcult to devise. The permutation tests described in Section 8.3 are a notable exception.
A test is conservative if the Type 1 error never exceeds α. An exact test is conservative, but
the reverse may or may not hold. Finally, a test at a given signiﬁcance level is unbiased if
the power satisﬁes two conditions:
1. βC  α for all distributions that satisfy the hypothesis (i.e., βC is conservative); and
2. βC  α for every possible alternate hypothesis.
An unbiased test is more likely to reject a false hypothesis than a true one. A UMP test
that is unbiased is called UMPU, and devising such a test is desirable but often unachiev-
able, in which case a test that is most powerful against the most interesting alternates is
typically sought.
Example 6.3 Returning yet again to the climate example to evaluate Type 2 errors, these
values depend on the true mean, and hence the errors must be evaluated over a range of
trial values. The following MATLAB script illustrates the approach:
mualt = 10:0.1:30;
cv = 1.645;
sigma = 0.6;
ct = cv + 20;
ctv = ct*ones(size(mualt)); %ﬁnd the area under the curve to
the left of the critical value for each value of the true mean
beta = normcdf(ctv, mualt, sigma); %beta is the probability
of Type 2 error
power = 1 - beta;
p = 0.05*ones(size(mualt));
plot(mualt, power, mualt, p)
Figure 6.2 shows the result. As the true mean rises above the nominal value of 20, the
power (which is the likelihood that the alternate hypothesis can be detected) rises rapidly.
174
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

At the βC ¼ 0:05 level, the test is unable to distinguish the nominal mean from a value
below about 20.6 but then rapidly detects the alternate hypothesis. For a hypothesized
mean of 22, the power is 0.723, so there is about a 72% chance that the null hypothesis will
be rejected when it is false.
Example 6.4 Consider the problem of testing the value of the parameter p for a binomial rv
with N = 10 trials. The hypotheses are H0: p = 0.5 against H1: p > 0.5. The number of
successes ^X will be the test statistic, and the rejection region will be large values of ^X that
are unlikely under H0. A table of cumulative binomial probabilities Pr X  N
ð
Þ
½
 can be
constructed using the MATLAB binocdf(0:10,10, p) function for p = 0.5 to 0.7.
The signiﬁcance level of the test is the probability α of rejecting H0 when it is true.
Suppose that the rejection region consists of the values lying between 8 and 10. From the
last row of Table 6.2 (p = 0.5), α ¼ Pr X > 7
ð
Þ ¼ 1  Pr X  7
ð
Þ ¼ 0:0547. If the rejection
region consists of the values lying between 7 and 10, the signiﬁcance level is 0.1719. In
both cases, the null distribution is binomial with p = 0.5, N = 10.
Under the Neyman-Pearson paradigm, the value of α is chosen before a test is per-
formed, and the test result is subsequently examined for Type 2 errors. Choose α ¼ 0:0547.
If the true value of p is 0.6, the power of the test is Pr X  8
ð
Þ ¼ 0:1673. If the true value of
p is 0.7, then the power is 0.3828. Thus the power is a function of p, and it is obvious that it
approaches α as p ! 0:5 and 1 as p ! 1.
10
15
20
25
30
True Mean
0
0.2
0.4
0.6
0.8
1
Power
Figure 6.2
Power curve for the climate example. The horizontal dotted line denotes the 0.05 level.
Table 6.2 Cumulative Binomial Probabilities
0
1
2
3
4
5
6
7
8
9
10
0.7
0.0000
0.0001
0.0016
0.0106
0.0473
0.1503
0.3504
0.6172
0.8507
0.9718
1.0000
0.6
0.0001
0.0017
0.0123
0.0548
0.1662
0.3669
0.6177
0.8327
0.9536
0.9940
1.0000
0.5
0.0010
0.0107
0.0547
0.1719
0.3770
0.6230
0.8281
0.9453
0.9893
0.9990
1.0000
175
6.2 Theory of Hypothesis Tests I
.007
13:35:08, subject to the Cambridge Core terms of use,

Deﬁne the p-value to be the probability of observing a value of the test statistic as large
as or larger than the one that is observed for a given experiment. A small number for the
p-value is taken as evidence for the alternate hypothesis (or for rejection of the null
hypothesis). The deﬁnition of small is subjective and up to the analyst. As a rough guide
based largely on convention, a p-value that is (<0.01, 0.010.05, 0.050.10, >0.10) is
(very strong, strong, weak, no) evidence for the alternate hypothesis (or rejection of the
null hypothesis).
An important property of a p-value whose test statistic has a continuous distribution is
that under the null hypothesis, the p-value has a uniform distribution on [0,1]. Conse-
quently, if H0 is rejected for a p-value below a value α, the probability of a Type 1 error is α.
Caution: Do not confuse the p-value with α. The latter is a parameter chosen before a
given experiment is performed and hence applies to a range of experiments, whereas
the p-value is a random variable that will be different for distinct experiments.
Caution: A large p-value is not necessarily strong evidence for the null hypothesis
because this can also occur when the null hypothesis is false and the power of the test
is low. Understanding the power of a given test and choosing tests with high power
are important parts of hypothesis testing that are frequently neglected.
Caution: The p-value is the probability of observing data at least as far from the null
value given that the null hypothesis is true. It cannot simultaneously be a statement
about the probability that the null hypothesis is true given the observations.
The p-value concept is not without controversy, and the American Statistical Association
recently issued a statement on its context, process, and purpose (Wasserstein & Lazar
2016). The use in this book is consistent with this statement, and it deﬁnitely deserves a
read by all scientists who use hypothesis testing.
To summarize, a p-value hypothesis test proceeds as follows:
1. Determine the null and alternate hypotheses;
2. Find a test statistic ^λ that will provide evidence about H0;
3. Obtain a random sample from the population of interest, and compute the test statistic;
4. Calculate the p-value (where FH0 is the cdf under the null hypothesis):
a. Upper-tail test: ^p ¼ 1  FH0ð^λÞ,
b. Lower-tail test: ^p ¼ FH0ð^λÞ, and
c. Two-tail test: ^p ¼ 2  ½1  FH0ðj^λjÞ for a symmetric distribution and ^p ¼ 2
min ½1  FH0ð^λÞ, FH0ð^λÞ for an asymmetric distribution;
5. If the p-value is smaller than the speciﬁed Type 1 error α, then reject the null
hypothesis., and do not take the p-value to be the Type 1 error; and
6. Compute the power of the test to determine the probability of rejecting the null
hypothesis when it is false.
Example 6.5 For the climate example that has been beaten to a pulp,
mu = 20;
sig = 0.6;
xbar = 22;
176
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

zobs = (xbar - mu)/sig;
pval = 1 - normcdf(zobs, 0 , 1)
ans =
4.34e-04
This is a very small number that very strongly rejects the null hypothesis.
The asymmetry in the Neyman-Pearson paradigm between the null and alternate hypoth-
eses suggests that strategic choices of them can simplify the analysis and/or strengthen the
conclusions. This is not a mathematical issue. Some typical guidelines include
• Make the null hypothesis the simpler of the two;
• If the consequences of incorrectly rejecting one hypothesis over the other are more
serious, then this hypothesis should be made null because the probability of falsely
rejecting it can be controlled by the choice of α; and
• In most scientiﬁc applications, the null hypothesis is a simple explanation that must be
discredited to demonstrate the existence of some physical effect. This makes the choice
natural.
6.3 Parametric Hypothesis Tests
6.3.1 The z Test
The z test pertains when the distribution of the test statistic under the null distribution is
Gaussian. It is usually used to test for location (i.e., comparing the sample mean to a given
value or comparing two sample means) in a situation where there are no nuisance param-
eters. In particular, the population variance must either be known a priori or be estimated
with high precision. Because of the central limit theorem, for a sufﬁciently large sample
(a rule of thumb is N > 50), many statistical tests can be cast as z tests. However, when the
sample size is small and the variance is estimated from the data, the t test is preferred.
The most common application of the z test is comparing the mean estimated from data to
a speciﬁed constant μ* when the population variance σ2 is known. The test statistic or
z-score is
^z ¼
ﬃﬃﬃﬃ
N
p
XN  μ∗


σ
(6.6)
The null hypothesis is H0: μ = μ* versus any of three possible alternate hypotheses:
1. If H1 is two tailed (H1:μ 6¼ μ∗), H0 can be rejected if ^zj j  2  N1 1  α=2
ð
Þ;
2. If H1 is one tailed and predicts a population mean larger than that in the null hypothesis
(upper-tail test), H0 can be rejected if ^z  N1 1  α
ð
Þ; and
3. If H1 is one tailed and predicts a population mean smaller than that in the null
hypothesis (lower-tail test), H0 can be rejected if ^z < 0 and ^zj j  N1 α
ð Þ.
177
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

Example 6.6 Suppose that it is known that the population mean for the velocity of mantle
rocks is 8 km/s with a population standard deviation of 2 km/s. The sample mean obtained
from 30 rock specimens is 7.4 km/s. Can it be concluded that the sample came from a
population with mean μ = 8?
The hypotheses are H0: μ ¼ 8 versus H1: μ 6¼ 8. The test statistic is ^z = 1.64. Since
^zj j ¼ 1:64 and the two-tailed critical value is 1.96 [norminv(0.975)], the null hypothesis
must be accepted at the 0.05 level. The p-value is 2  [1  normcdf(1.64)], or 0.101,
which is no evidence for the alternate hypothesis.
The power of the test for a hypothesized value of the mean μ1 is
βC ¼ 1  Pr
ﬃﬃﬃﬃ
N
p
μ∗ μ1
ð
Þ=σ  zα=2  ^z0 
ﬃﬃﬃﬃ
N
p
μ∗ μ1
ð
Þ=σ þ zα=2
h
i
where ^z0 ¼
ﬃﬃﬃﬃ
N
p
XN  μ1


=σ is the hypothesized z-score, and zα=2 ¼ 1:96 for α ¼ 0:05.
This expression may appear to be conﬂating the concepts of conﬁdence intervals with
hypothesis testing, but Section 6.4 will demonstrate their equivalence. Suppose that μ1 is
7 km/s, so βC ¼ 1  Pr 0:8646  ^z0  3:0554
ð
Þ = 1  normcdf(3.0554) + normcdf
(0.8646) = 1  0.8052 = 0.1948, and there is a ~19% chance that the null hypothesis
will be rejected when it is false. The power is low because the number of samples is small.
Suppose that the power is required to be 0.9, and the number of samples needed to achieve
this value is to be computed before the experiment is completed for a Type 1 error
α ¼ 0:05. By trial and error, the result is N  260.
Example 6.7 A meteorologist determines that during the current year there were 80 major
storms in the northern hemisphere. He claims that this represents a signiﬁcant increase over
the long-term average. Based on data obtained over the past 100 years, it is known that
there have been 70 	 2 storms per year. Does 80 storms represent a signiﬁcant increase
from 70?
In this case, N = 1 for the single year with 80 storms. This gives ^z = 5. The null and
alternate hypotheses are H0: μ ¼ 70 versus H1: μ 6¼ 70. Since ^z > 3.92 using a two-sided
test, the null hypothesis is rejected at the 0.05 level. The p-value is 5.7  10–7.
MATLAB provides the function [h, p] = ztest(x, m, sigma) to perform a z test of the
hypothesis that the data vector x comes from a normal distribution with mean m and
standard deviation sigma (σ). It returns h = 1 to indicate that the null hypothesis is rejected
at the 0.05 level and the p-value p. There are many additional options and, in particular, the
keyword-value pair “Tail” with either “both” (default), “right,” or “left.”
6.3.2 The t Tests
The t test applies when the distribution for the test statistic follows Student’s t distribu-
tion under the null hypothesis. It is undoubtedly the most widely used hypothesis test in
178
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

statistics. There are a number of variants of the t test. The one-sample t test obtains when
the mean is compared with a speciﬁed value, and both the population mean and the
variance are estimated from the sample. The two-sample t test applies when the means of
two independent populations are compared, and both the population means and the
variances are estimated from the sample. It further requires that the variances of the
two populations be identical, a condition that is called homogeneity of variance or
homoskedasticity. In practice, this condition should always be veriﬁed using an F test
(Section 6.3.4) or (preferably) Bartlett’s M test for homogeneity of variance (Section
6.3.5), although the two-sample t test is remarkable insensitive to heteroskedasticity.
Two-sample t tests may further be divided into unpaired and paired cases. The unpaired t
test applies when iid samples are available from two populations. The paired t test is used
in designed experiments where samples are matched in some way and is widely used in
the biomedical ﬁelds.
The test statistic for the one-sample t test is
^t ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
XN  μ∗


^sN
(6.7)
where ^sN is the sample standard error given by the square root of (4.14). Test evaluation is
the same as for the single-sample z test, except that the critical value is taken from
Student’s t distribution and hence depends on the sample size.
Caution: The test statistic can also be expressed as
^t ¼
ﬃﬃﬃﬃ
N
p
XN  μ∗


^s0
N
(6.8)
where ^s0N is the unbiased standard deviation given by the square root of (4.15). Equations
(6.7) and (6.8) are identical.
Example 6.8 A scientist claims that the average number of days per year that an
oceanographer spends at sea is 30. In order to evaluate this statement, 20 oceanographers
are randomly selected and state the following values: 54, 55, 60, 42, 48, 62, 24, 46, 48, 28,
18, 8, 0, 10, 60, 82, 90, 88, 2, and 54. Do the data support the claim?
The hypotheses are H0: μ ¼ 30 versus H1: μ 6¼ 30. The sample mean is 43.95, and the
sample standard deviation is 26.62. The test statistic is 2.28. The two-tailed critical value is
obtained as tinv(0.975, 19) = 2.09 at the 0.05 level. The null hypothesis is rejected. The
p-value is 2  (1 – tcdf(2.28, 19)) = 0.0343, which is strong evidence for the alternate
hypothesis. One-sided tests provide no evidence for the alternate hypothesis for the lower-
tail version (p-value of 0.9828) but strongly accept it for the upper-tail test (p-value of
0.0172).
It is instructive to examine the power of the t test for this example. When the population
mean is μ1 6¼ μ∗, then (6.7) has the noncentral t distribution with noncentrality parameter
δ ¼
ﬃﬃﬃﬃ
N
p
μ1  μ∗
ð
Þ=σ, where σ is the population standard deviation that must be replaced
with a sample estimate. From (6.5), the power of the single-sample t test is given by
179
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

βC ¼ 1  NcteeN1 τN1 1  α=2
ð
Þ; δ
ð
Þ þ NcteeN1 τN1 1  α=2
ð
Þ; δ
ð
Þ
where τN1 1  α=2
ð
Þ is the 1  α=2 quantile of the central t distribution with N  1
degrees-of-freedom. A MATLAB script to evaluate the power is
x = [54 55 60
42 48 62 24 46 48 28 18 8 0 10 60 82 90 88 2 54];
mustar = 30;
sigma = std(x);
n = length(x);
tau = tinv(.975, n - 1);
mualt = 10:.1:50;
effect = (mualt - mustar)/sigma;
delta = sqrt(n)*effect;
power = 1 – nctcdf(tau, n - 1, delta) + nctcdf(-tau, n - 1, delta);
plot(effect, power)
Figure 6.3 shows the result plotted as effect size against power. The effect size is a
measure of the difference between the population and postulated means normalized by the
standard error. The power has a minimum value of 0.05 when μ1 ¼ μ∗or effect size is zero
and rises symmetrically on either side. A test whose power is no smaller than the
signiﬁcance level is unbiased, and in fact, the t test is a uniformly most powerful unbiased
(UMPU) test. The power is about 0.58 for the observed effect size of 0.51, so there is an
~60% chance of rejecting the null hypothesis when it is false.
MATLAB gives a function [h, p] = ttest(x) to test the hypothesis that the data in x come
from a normal distribution with mean zero and a Type 1 error of 0.05. The null hypothesis
(mean is zero) is rejected if h = 1, where p is the p-value. The call [h, p] = ttest(x, ‘Alpha’,
alpha) performs the test at the alpha probability level. The call [h, p] = ttest(x, ‘Alpha’,
alpha, ‘Tail’, tail) allows the alternate hypotheses of “both” (default), “right” or “left” to
be tested.
The t test can also be used to compare two paired samples. Pairing is frequently used in
designed experiments in which a population is sorted into groups according to some
–0.5
0
0.5
Effect Size
0
0.2
0.4
0.6
0.8
1
Power
Figure 6.3
Power curve for the single-sample t test of Example 6.8.
180
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

characteristic and is especially important in biomedical testing. In this case, the t test
evaluates the null hypothesis that the pairwise differences between two data sets have a
mean of zero.
Pairing can be an effective experimental technique under some circumstances. To see
this, consider N paired data
Xi; Yi
f
g, and assume that the means are
μX ; μY
f
g and the
variances are
σ2
X ; σ2
Y


. Further assume that distinct pairs are independent, and
cov Xi; Yj


¼ σXY. Consider the differences between the paired data Di ¼ Xi  Yi that
are independent with expected value μX  μY and variance σ2
X þ σ2
Y  2σ2
XY. A natural
estimator for Di is the difference between the sample means of Xi and Yi whose variance is
σ2
X þ σ2
Y  2σ2
XY


=N. However, if Xi and Yi were not paired but rather independent, then
the variance of the difference would be σ2
X þ σ2
Y


=N. The paired variance is smaller if the
correlation between the pairs is positive, leading to a better experimental design.
Example 6.9 In his seminal 1908 paper, W. S. Gossett (aka Student) presented the paired
results of yield from test plots of wheat, where half the plots were planted with air-dried
seed and the remaining plots were planted with kiln-dried seed. The paired t test can be
used to compare the two populations for a common mean.
gosset = importdata('gossett.dat');
[h, p, ci, stats] = ttest(gossett(:, 1), gossett(:, 2))
h =
0
p =
0.1218
ci =
-78.1816
10.7271
stats =
tstat: -1.6905
df: 10
sd: 66.1711
The null hypothesis that the means are the same is accepted at the 0.05 level, and the
p-value is 0.1218, which is no evidence for the alternate hypothesis.
The test statistic for the two-sample t test is
^t ¼
X1  X2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N 1^s2
1 þ N2^s2
2
N 1 þ N2  2


1
N 1
þ 1
N2


s
(6.9)
where the two samples have sizes N1 and N2, respectively, and Xi and ^s2
i are the sample
mean and variance for each. When N1 = N2 = N, the test statistic reduces to
181
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

^t ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
X1  X2


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^s2
1 þ ^s2
2
p
(6.10)
Test evaluation is the same as for the single-sample case, except that the null and alternate
hypotheses are H0: μ1 ¼ μ2 versus H1: μ1 6¼ μ2 (or H1: μ1  μ2 or . . .), and the test
degrees-of-freedom are N 1 þ N2  2. The reduction of two in the degrees-of-freedom
reﬂects the computation of the sample means from the data, which are in fact a pair of
linear constraints on them.
Example 6.10 A. A. Michelson made his famous measurements of the speed of light in
1879 and repeated a smaller number of measurements in 1882. These data are available in
Stigler (1977, tables 6 and 7). Because the experiments were conducted over disjoint time
intervals, they are presumed to be independent. Test the data to see if the sample means are
different, and estimate the power of the test.
The data are in ﬁles michelson1.dat and michelson2.dat for the 1879 and 1882 experi-
ments, respectively. The initial step in analyzing them is creating Gaussian q-q plots, as
shown in Figure 6.4. These show two weak outliers for the 1879 data at the left side of the
distribution but more systematic departures from normality for the 1882 data. The
1879 data show a stairstep pattern that is characteristic of numerical rounding that
sometimes happens when data units are changed.
The null hypothesis to be tested is H0: μ1 ¼ μ2 versus H1: μ1 6¼ μ2, where μ1 and μ2 are
the 1879 and 1882 means, respectively. There are 100 data in the 1879 data set and 24 data
in the 1882 data set. Using MATLAB, X1 = 852.40, X2 = 763.88, ^s1 = 78.61, ^s2 = 108.93,
and ^t = 4.53. The critical value is tinv(0.975, 122) = 1.98, and the null hypothesis is
rejected at the 0.05 level. The p-value is 1.40  10–5, which is very strong evidence for the
alternate hypothesis. Either the speed of light changed substantially over three years or
systematic errors are present. However, the condition that the variances of the two
populations are the same, as well as the test power, needs to be checked before drawing
this conclusion too strongly.
Under the alternate hypothesis, (6.9) is distributed as the noncentral t distribution with
noncentrality parameter
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
600
700
800
900
1000
1100
Quantiles of Input Sample
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
500
600
700
800
900
1000
1100
Quantiles of Input Sample
Figure 6.4
Quantile-quantile plots of the Michelson speed of light data from 1879 (left) and 1882 (right).
182
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

δ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N1N2
p
μ1  μ2
ð
Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N1σ2
1 þ N 2σ2
2


= N 1 þ N2
ð
Þ
q
=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N1 þ N 2
p
A power curve for the Michelson data is easily constructed.
michelson1 = importdata('michelson1.dat');
michelson2 = importdata('michelson2.dat');
n1 = length(michelson1);
n2 = length(michelson2);
s1 = std(michelson1);
s2 = std(michelson2);
sp = sqrt((n1*s1^2 + n2*s2^2)/(n1 + n2 - 2));
tcrit = tinv(.975, n1 + n2 - 2);
mudiff = -100:.1:100;
effect = mudiff/sp;
delta = sqrt(n1*n2/(n1 + n2))*effect;
power
=
1
-
nctcdf(tcrit,
n1
+
n2
-
2,
delta)
+
nctcdf
(-tcrit, n1 + n2 - 2, delta);
plot(effect, power)
Figure 6.5 shows the result. The power rises rapidly on either side of μ1  μ2 ¼ 0. For
the observed difference of the sample means of about 88.5 (effect size of 1.02), the power
is 0.99, so there is a 99% chance of rejecting the null hypothesis when it is false. This is a
high value for the power, and hence there can be considerable conﬁdence in the test,
presuming that the variances of the two populations are comparable.
MATLAB includes the function [h, p] = ttest2(x, y) that performs a two-sample t test on
the data in vectors x and y. It returns h = 1 if the null hypothesis that the means are equal
can be rejected at the 0.05 level, and p is the p-value. The call [h, p] = ttest2(x, y, ‘alpha’,
Effect Size
–1.5
–1
–0.5
0
0.5
1
1.5
Power
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 6.5
Power curve for the Michelson speed of light data.
183
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

alpha) gives the test at the alpha probability level. The call [h, p] = ttest2(x, y, ‘Alpha’,
alpha, ‘Tail’, tail) implements the test type when tail equals “both” (default), “right,”
or “left.”
It is also possible to formulate an approximate two-sample t test when the variances of
the two populations are not the same by using the test statistic
^t ¼
X1  X2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^s2
1= N 1  1
ð
Þ þ ^s2
2= N 2  1
ð
Þ
p
(6.11)
and an approximation to the degrees-of-freedom. MATLAB uses the Satterthwaite (1946)
value of
ν ¼
^s2
1= N1  1
ð
Þ þ ^s2
2= N2  1
ð
Þ
	

2
^s2
1= N1  1
ð
Þ
	

2
N 1
þ ^s2
2= N2  1
ð
Þ
	

2
N 2
 2
(6.12)
This is implemented in MATLAB as [h, p] = ttest2(x, y, ‘Vartype’, ‘unequal’).
However, the t test for unequal population variances does not perform as well as some
of the nonparametric tests described in Chapter 7 and hence is not recommended for
general use.
Example 6.11 Apply the unequal variance version of the t test to the Michelson speed of
light data.
[h, p] = ttest2(michelson1, michelson2, 'Vartype', 'unequal')
h =
1
p =
9.5099e-04
The conclusion is unchanged, and either there is not a disparity between the variances of
the two data sets or the unequal variance t test does not do a good job of correction.
Example 6.12 In the 1950s and 1960s, experiments were carried out in several countries to
see if cloud seeding would increase the amount of rain. A particular experiment randomly
selected half of 52 clouds for seeding and measured the amount of rain from each cloud in
acre-feet. These data are taken from Simpson, Olsen, & Eden (1975). The data are
contained in the ﬁle cloudseed.dat, with the ﬁrst and second columns corresponding to
unseeded and seeded clouds.
As a start, Gaussian q-q plots of the data are produced and appear in Figure 6.6. The
result shows marked departures from Gaussianity, being short tailed at the lower and long
tailed at the upper ends of the distribution. Consequently, it is reasonable to question
whether a t test will perform well.
184
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

A two-sample t test on the data yields
clouds = importdata('cloudseed.dat');
[h, p] = ttest2(clouds(:, 1), clouds(:, 2))
h =
0
p =
0.0511
The null hypothesis that the seeded and unseeded clouds yield different amounts of rain is
weakly accepted. The test statistic is 1.9982 against a 0.05 critical value of 2.06, so the
result is close to the critical region.
However, q-q plots of the logs of the data yield a result that is very close to being
Gaussian (Figure 6.7). This suggests that a two-sample t test on the logs might yield a more
accurate result.
[h, p] = ttest2(log(clouds(:, 1)),log(clouds(:, 2)))
h =
1
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
–500
0
500
1000
1500
Quantiles of Input Sample
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
–500
0
500
1000
1500
2000
2500
3000
Quantiles of Input Sample
Figure 6.6
Quantile-quantile plots for the cloud seeding data. The unseeded data are shown on the left and the seeded data
on the right.
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
0
2
4
6
8
Quantiles of Input Sample
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
0
2
4
6
8
Quantiles of Input Sample
Figure 6.7
Quantile-quantile plots of the logs of the cloud seeding data. The unseeded data are shown on the left and the
seeded data on the right.
185
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

p =
0.0141
The null hypothesis is now strongly rejected by the t test. The lesson from this example is
that the distribution of the data must be characterized before blindly applying a
statistical test.
6.3.3 The χ2 Test
The χ2 test applies when the distribution for the test statistic follows the chi square
distribution under the null hypothesis. In its simplest form, the χ2 test is used to determine
whether the variance computed from a sample has a speciﬁed value when the underlying
distribution of the sample is Gaussian. Confusingly, there are other tests with the same
name that are used in goodness-of-ﬁt testing (Section 7.2.2) and other areas.
The test statistic for the χ2 test is
^χ2 ¼ N ^sN
2
σ2
(6.13)
Test evaluation proceeds as follows:
1. If the alternate hypothesis is two tailed, H0 can be rejected if ^χ2  Chi1 1  α=2; N  1
ð
Þ
or ^χ2  Chi1 α=2; N  1
ð
Þ;
2. If the alternate hypothesis is one tailed and predicts a population variance larger than in
the null hypothesis, H0 can be rejected if ^χ2  Chi1 1  α; N  1
ð
Þ; and
3. If the alternate hypothesis is one tailed and predicts a population variance smaller than
that in the null hypothesis, H0 can be rejected if ^χ2  Chi1 α; N  1
ð
Þ.
Example 6.13 A company claims that a certain type of battery used in an instrument has a
lifetime of 9 months with a standard deviation of 2.24 months (or variance of 5 months
squared). It is believed that the stated standard deviation is too low. To test this hypothesis,
the lives of 30 batteries are measured, as contained in the ﬁle battery.dat. Do the data
indicate that the standard deviation is other than 2.24 months?
The hypotheses to be compared are H0: σ2 ¼ 5 versus H1: σ2 6¼ 5. From the data, ^s2
N=
7.40, and N = 30. The test statistic is ^χ2 = 44.40. From MATLAB, chi2inv(0.975, 29) =
45.72. Because ^χ2 < Chi1 1  α=2; N  1
ð
Þ, the null hypothesis is accepted, although the
test statistic is very close to the critical region. The p-value is 0.0674, so there is weak
evidence for the alternate hypothesis. However, if H1: σ2 > 5, the null hypothesis is
rejected because chi2inv(0.95, 9) = 42.56. The upper-tail p-value is 0.0337, which is
strong evidence for the alternate hypothesis.
When the random variables used to compute the sample variance do not have
zero expected value, the test statistic (6.13) has the noncentral chi square distribution with
186
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

N  1
degrees-of-freedom
and
noncentrality
parameter
δ ¼ PN
i¼1μ2
i =σ2,
where
μi ¼ E Xi
ð
Þ. A more reasonable way to measure the effect size is as the variance
ratio difference
σ2
0  σ2


=σ2, where σ2
0 is the target value of the variance, and
δ ¼ N σ2
0  σ2


=σ2. An effect size of zero corresponds to a variance ratio of unity and
occurs when the power of the test is the Type 1 error α because the test is unbiased; the
power rises asymmetrically on either side of this value due to the skewed form of the
distribution. The power curve of the chi square test is
βC ¼ 1  Ncchi x1α=2; N  1; δ


þ Ncchi xα=2; N  1; δ


(6.14)
where Ncchi is the cdf for the noncentral chi square distribution.
Example 6.14 Compute a power curve for Example 6.13.
The observed value for the variance ratio is 1.48. The power calculation is easily
implemented using MATLAB.
x = importdata('battery.dat');
sigma2 = 5;
n = length(x);
xlo = chi2inv(.025, n - 1);
xhi = chi2inv(.975, n - 1);
xhi1 = chi2inv(.95, n - 1);
varrat = 1:.01:2;
power = 1 - ncx2cdf(xhi, n - 1, n*(varrat - 1)) + . . .
ncx2cdf(xlo, n - 1, n*(varrat - 1));
power1 = 1 - ncx2cdf(xhi1, n - 1, n*(varrat - 1));
plot(varrat, power, varrat, power1, 'k--')
Figure 6.8 shows the power curves for the two-tailed and upper-tail tests against the
effect size. At the observed variance ratio of 1.48, the powers for the two-tailed and upper-
tail tests are only 0.37 and 0.48, respectively.
1
1.2
1.4
1.6
1.8
2
Variance Ratio
0
0.2
0.4
0.6
0.8
1
Power
Figure 6.8
Power curves for a two-sided (solid line) and upper-tail (dashed line) test for the battery data.
187
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

MATLAB includes [h, p] = vartest(x, v), which performs a chi square test of the
hypothesis that the data in x come from a normal distribution with variance v. It returns
h = 1 if the null hypothesis can be rejected at the 0.05 level, and p is the p-value.
6.3.4 The F Test
The F test applies when the distribution for the test statistic follows the F distribution under
the null hypothesis. In its simplest form, the F test is used to compare two variances
computed from independent samples for which the underlying distributions are Gaussian.
Consequently, it serves as a test for homogeneity of variance, although it is very sensitive
to departures from normality and hence should be avoided for this purpose in favor of
Bartlett’s M test. The F test also ﬁnds extensive use in linear regression and modeling.
To test for homogeneity of variance, the test statistic is
^F ¼ ^s2
max
^s2
min
(6.15)
where ^s2
max ¼ max ^s2
1;^s2
2


and ^s2
min ¼ min ^s2
1;^s2
2


, and ^s2
1 and ^s2
2 are the sample variances
for the two populations. The test statistic ^F is assessed against critical values of the F
distribution with Nmax  1, Nmin  1 degrees-of-freedom.
Example 6.15 For the Michelson speed of light data in Example 6.10, ^F = 11,866/6180.2 =
1.92 and F1
23,99 0:975
ð
Þ ¼ 1:80, so the null hypothesis H0: σ2
1 ¼ σ2
2 is rejected in favor of
the alternate H1: σ2
1 6¼ σ2
2 at the 0.05 level. The p-value is 0.0292, which is strong evidence
for the alternate hypothesis. This also holds for the upper-tail test H1: σ2
1 > σ2
2, where
F1
23,99 0:95
ð
Þ ¼ 1:64. The p-value for the upper-tail test is 0.0146. Because the data fail a
test for homogeneity of variance, the results from the t test in Section 6.3.2 are in doubt.
The power of the test is easily computed in a manner analogous to the t and χ2 tests, with
the result shown in Figure 6.9. At the observed variance ratio of 1.92, the power of the F
test is only about 0.55 for the Michelson data, so one cannot place a large degree of
conﬁdence in the outcome.
1
1.5
2
2.5
3
Variance Ratio
0
0.2
0.4
0.6
0.8
1
Power
Figure 6.9
Power of the F test for variance homogeneity applied to the Michelson data.
188
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

xlo = ﬁnv(.025, 23, 99);
xhi = ﬁnv(.975, 23, 99);
varrat = 1:.01:3;
plot(varrat,
1
-
ncfcdf(xhi,
23,
99,
24*(varrat
-
1))
+
ncfcdf(xlo, 23, 99, 24*(varrat - 1)))
MATLAB includes the function [h, p] = vartest2(x, y), which performs an F test for the
hypothesis that x and y come from normal distributions with the same variance. It returns
h = 1 if the null hypothesis that the variances are equal can be rejected at the 0.05 level, and
the p-value is p.
6.3.5 Bartlett’s M Test for Homogeneity of Variance
This test was introduced by Bartlett (1937) and is a more powerful test than the F test that
applies to a population of independent variance estimates (i.e., to more than two estimates).
It is also more robust to heteroskedasticity than the F test and is an unbiased test. The test
statistic is
^M ¼
N  k
ð
Þ log^s2
p  P
k
i¼1
N i log ^s02
i
1 þ
1
3 k þ 1
ð
Þ
X
k
i¼1
1
Ni  1 
1
N  1
 
!
(6.16)
where N ¼ Pk
i¼1Ni, and N i is the degrees-of-freedom for the variance estimate ^s02
i , and the
pooled variance is
^s2
p ¼
1
N  k
X
k
i¼1
Ni  1
ð
Þ^s02
i
(6.17)
For small samples, Bartlett showed that (6.16) is distributed according to χ2
k1 and can be
tested for signiﬁcance in the usual way. Because the null and alternate distributions cannot
be fully characterized, power calculations are not straightforward. However, the asymptotic
power can be obtained directly using the noncentral chi square distribution.
Example 6.16 For the speed of light data in Example 6.10, ^M = 4.97 using (6.16). To test
the null hypothesis H0: σ2
1 ¼ σ2
2 against the alternate H1: σ2
1 6¼ σ2
2, compute the p-value
2  min(1  chi2cdf(4.97, 1), chi2cdf(4.97, 1)) = 0.0516, which is weak evidence for the
alternate hypothesis. This suggests that the data are nearly homoskedastic, in contrast to
the F test.
MATLAB implements Bartlett’s M test, although it is a bit difﬁcult to use for comparing
two data sets with different numbers of data. The function vartestn(x) computes Bartlett’s
189
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

test on the columns of x. Because it ignores NaN entries, it is necessary to add NaNs to the
second Michelson data set to make it the same size as the ﬁrst one. The following script
accomplishes this:
michelson2 = [michelson2' NaN(1, 76)]';
x = [michelson1 michelson2];
[p, stats ]= vartestn(x)
p =
0.0265
stats =
chisqstat: 4.9245
df: 1
Note that the MATLAB function does not return the variable h, unlike the other tests
described in this section. The difference between the directly calculated p-value in Example
6.16 and that from the MATLAB function is that the latter returns the upper-tail test, unlike
the default for all of its other parametric tests, and no option is provided to change it, which
is most unfortunate. However, it is unclear why the test statistics are slightly different
because they do not depend on the nature of the test. The function vartestn also implements
a series of variants on the M test that are more robust to heteroskedasticity.
6.3.6 The Correlation Coefﬁcient
The correlation coefﬁcient (sometimes called Pearson’s product moment correlation coef-
ﬁcient) allows two populations drawn from a bivariate normal distribution to be compared.
The test statistic is given by (4.79). The null hypothesis under test holds that there is no
correlation between two data sets against the alternate hypothesis that there is correlation.
The null distribution is given by (4.83), and the alternate distribution is given by (4.80).
The p-value can be computed by integrating (4.83) to get the null cdf. The result is
Corr x; 0; N
ð
Þ ¼ 1
2 þ
Γ N  1
ð
Þ=2
½
x 2F1
1
2 ; 2  N
2 ; 3
2 ; x2


ﬃﬃﬃπ
p
Γ N=2  1
ð
Þ
(6.18)
The hypergeometric series (4.81) terminates when either of the ﬁrst two arguments are
integers [or when the number of data is odd in (6.18)], and this must be accounted for in
computing the hypergeometric function. The alternate cdf must be obtained by numerical
integration of (4.80). These steps will be demonstrated by example.
The MATLAB corr function computes the correlation and the test p-value using
a Student’s t approximation rather than the exact result in (6.18). The function call is
[rho, p] = corr(x1, x2), where rho is the correlation matrix, and p is the p-value.
Example 6.17 Data from a British government survey of household spending in the ﬁle
alctobacc.dat taken from Moore & McCabe (1989) may be used to examine the relation-
ship between household spending on tobacco products and alcoholic beverages.
190
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

A scatterplot of spending on alcohol versus spending on tobacco in the 11 regions of Great
Britain (Figure 6.10) shows an overall positive linear relationship, although there is a
single outlying value at the lower right. The goal is assessing whether there is signiﬁcant
correlation between these two data sets and investigating whether the result is unduly
inﬂuenced by a single datum. The results using the MATLAB function corr will be
compared with those from (6.18).
alctobacc = importdata('alctobacc.dat')
[rhat, p] = corr(alctobacc(:, 1), alctobacc(:, 2))
rhat =
0.2236
p =
0.5087
N = length(alctobacc);
Corr = 0.5 + gamma((N - 1)/2)*rhat* . . .
Hypergeometric2f1(1/2, 2 - N/2, 3/2, rhat^2)/(sqrt(pi)*gamma
(N/2 - 1));
pval = 2*min(Corr, 1 - Corr)
pval =
0.5087
The null hypothesis that there is no correlation in these data is accepted, and the results
from the MATLAB function corr and (6.18) are the same.
However, if the point at the lower right is eliminated (Northern Ireland, where they
either smoke a lot and drink comparatively little or else ﬁddle with the numbers), the
sample correlation coefﬁcient changes to ^r = 0.7843, the p-value changes to 0.0072,
and the null hypothesis is rejected. The correlation between alcohol and tobacco
consumption in Great Britain becomes real. To be strictly correct, after deleting the
outlying value, the null distribution should be taken as the truncated form of (4.82), as
described in Section 4.8.4. Due to its complexity, this will be neglected for pedagogical
purposes.
2.5
3
3.5
4
4.5
5
Tobacco Spending
3.5
4
4.5
5
5.5
6
6.5
Alcohol Spending
Figure 6.10
Alcohol and tobacco spending by British households in the 11 regions of the United Kingdom.
191
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

6.3.7 Analysis of Variance
One-way analysis of variance or ANOVA is a method for comparing the means of two or
more independent groups and is effectively an extension of the t test to more than two
samples. The term analysis of variance may cause some confusion because the method
uses variance to compare the means of the groups as opposed to comparing the variances
directly. One-way ANOVA is balanced if each group has the same number of variables and
otherwise is unbalanced.
Let M denote the number of groups to be compared. Let the population means be
μ1; . . . ; μM
f
g. ANOVA is a test of the null hypothesis H0 : μ1 ¼ μ2 ¼ 
 
 
 ¼ μM against the
alternate hypothesis H1: not H0, meaning that at least two of the population means are
unequal. The assumptions for the test are
1. The distribution of the rvs for each of the M groups is Gaussian;
2. The data in each group are homoskedastic; and
3. The samples are independent.
ANOVA uses the between (sometimes called hypothesis) and within (sometimes called
error) variances of the data. The between variance is a comparison of the sample mean of
each group with the grand mean of all the data. The within variance is a comparison of the
data in each group with its corresponding sample mean. The test statistic is the ratio of the
between and within variances, and its null distribution is the F distribution. The p-value is
the upper-tail probability that the test statistic exceeds the observed F value.
Let ni denote the number of data in the ith group, Xi denote the corresponding sample
mean, and X be the grand mean of all the data. The unbiased between variance is
^s02
b ¼
1
M  1
X
M
i¼1
ni Xi  X

2
(6.19)
Since (6.19) measures the variability among M means, it has M  1 degrees-of-freedom.
In the balanced situation where all the ni are identical, the data can be represented by the
N  M matrix X
$, and the between sum of squares can be written succinctly using matrix
notation as
w2
b ¼ 1
N jN
 X
$ 
I
$
M  1
M J
$
M



 X
$T 
 jT
N
(6.20)
where jN is a column N-vector of ones, I
$
M is the M  M identity matrix, J
$
M is an M 
M matrix of ones, and ^s02
b ¼ w2
b= M  1
ð
Þ.
The within variance is
^s02
w ¼
1
N  M
X
M
i¼1
ni  1
ð
Þ^s02
j
(6.21)
where N ¼ Pm
j¼1ni,
^s02
j ¼
1
nj  1
X
nj
i¼1
xi,j  Xj

2
(6.22)
192
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

and xi,j denotes the ith datum in the jth group. In the balanced situation, the sum of squares
can be written as
w2
w ¼ tr X
$ 
X
$T


 1
N jN
 X
$ 
X
$T 
 jT
N
(6.23)
where tr denotes the matrix trace (sum of the diagonal elements), and s02
w ¼ w2
w= M N  1
ð
Þ
½
.
The test statistic is
^F ¼ ^s02
b
^s02
w
(6.24)
and is distributed as FM1,M N1
ð
Þ under the null hypothesis. The results of an ANOVA are
usually presented using a table, with the ﬁrst and second rows denoting the between and
within groups, respectively, and the last row presenting the total. The columns show the
sum of squares, degrees-of-freedom, mean square, and F statistic.
MATLAB supports balanced one-way analysis of variance through the function
[p, anovatab] = anova1(x), where anovatab is optional. The input variable x is a matrix
whose columns are the data from each group. The returned variables are the p-value p and
the ANOVA table anovatab. By default, anova1 also produces a boxplot of the data.
A more easily understood boxplot can be generated directly using the function boxplot
(x, ‘medianstyle’, ‘target’) and is shown in Example 6.18. A boxplot consists of represen-
tations of the columns of x in which a target symbol is the median, the edges of the box are
the 0.25 and 0.75 quantiles, whiskers extend to the limits of the data that the function
considers to not be outliers, and outliers are marked with the symbol x. It is unclear how
outliers are detected, and hence those markers should not be emphasized.
Example 6.18 Measurements have been made by seven laboratories of the content of a
certain drug in tablet form in the ﬁle tablet.dat. There are 10 replicate measurements per
laboratory. The goal is to determine whether there are systematic differences between the
laboratory results.
[p, anovatab] = anova1(tablet);
The anova table is:
Source
SS
DF
MS
F
Between
0.15343
6
0.02556
11.9
Within
0.13536
63
0.00215
Total
0.28879
69
The p-value is 2  min(1 - fcdf(f, 6, 63), fcdf(f, 6, 63)) = 1.40  10–8, so the null
hypothesis that the means are the same is rejected, which is also apparent in Figure 6.11.
Much of the terminology that underlies analysis of variance comes from devising designed
experiments that are typically not feasible in the earth sciences but are common in the
social and biological sciences. There is an extensive literature on designing experiments
193
6.3 Parametric Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

that is omitted in this book. The design of experiments typically leads to more complex
situations where each group is divided into two or more categories, leading to two-way
analysis of variance or a higher-order version. More information can be found in the classic
text of Scheffé (1959) and in the more recent work of Hoaglin, Mosteller, & Tukey (1991).
The power of an ANOVA test can be computed but is not as straightforward as in prior
examples; see Murphy, Myors, & Wolach (2014) for details.
6.3.8 Sample Size and Power
There is an inherent tradeoff between the choice of the Type 1 error α, the power of a given
statistical test βC, and the number of data that are required to achieve a given α; βC


. It is
good statistical practice to estimate the required number of data to yield α; βC


prior to
initiating data collection as an element of experimental design. Indeed, this is standard
practice in the biomedical ﬁelds, where experimental designs are typically thorough and
aimed at fulﬁlling regulatory requirements.
MATLAB provides a function n = sampsizepwr(testtype, p0, p1) that returns the
number of values n required to achieve a power of 0.90 at a signiﬁcance level of 0.05
for a two-sided test of types “z,” “t,” “var,” and “p” for, respectively, the z, t, χ2, and
binomial proportion tests. The parameters p0 and p1 are the parameters under the null and
alternate hypotheses that are test speciﬁc; for example, for the t test, p0 is [μ, σ] under
the null hypothesis, and p1 is the mean under the alternate hypothesis. An alternate call
power = sampsizepwr(testtype, p0, p1, [ ], n) returns the power that is realized for n data
samples, and other variants are supported. It is possible to change the signiﬁcance level and
test type in the usual ways.
Caution: There are numerous power analysis applications available on the Internet; the
most widely used appears to be G*Power, which is available at http://gpower.hhu.de.
Laboratory Index
1
Composition
4
4.1
4.2
2
3
4
5
6
7
Figure 6.11
Boxplot for the tablet data from the seven laboratories of Example 6.17 shown on the x-axis with analysis results on
the y-axis. The targets are the population medians, the solid boxes encompass the 0.25 to 0.75 quantiles, and the
dotted whiskers show the full range of each data set. Two outliers in the ﬁfth and sixth laboratory results are shown
as + signs.
194
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

However, many of these applications use approximations to the power whose accur-
acy can be limited unless the degrees-of-freedom are large. It seems that the
MATLAB function sampsizepwr is not an exception. It is strongly recommended
that the data analyst understand how to compute the power of a test directly and use
that result instead of relying on black-box functions.
Example 6.19 For the number of days at sea data of Example 6.8, determine the number of
data with the observed mean and standard deviation that would be required to achieve a
power of 0.90 at a signiﬁcance level of 0.05 for a two-sided test with population means of
30 and 40. Determine the power of a t test with the observed parameters for 10, 20, and
100 data and a population mean of 30.
x = [54 55 60
42 48 62 24 46 48 28 18 8 0 10 60 82 90 88 2 54];
sampsizepwr('t', [mean(x) std(x)], 30)
ans =
43
sampsizepwr('t', [mean(x) std(x)], 40)
ans =
505
Recalling that the mean of the data is 43.95 days, it is reasonable to expect that a much
larger sample size will be required to discern a smaller effect size, as is observed.
sampsizepwr('t', [mean(x) std(x)], 30, [], 20)
ans =
0.5976
sampsizepwr('t', [mean(x) std(x)], 30, [], 30)
ans =
0.7714
sampsizepwr('t', [mean(x) std(x)], 30, [], 100)
ans =
0.9993
Clearly, a much better hypothesis test would have ensued by asking 30 rather than
20 oceanographers for the number of days per year that they spend at sea because the
power is nearly 0.8 for the larger sample.
6.4 Hypothesis Tests and Conﬁdence Intervals
Let
Xi
f
g be a random sample from a normal distribution having a known variance.
Consider H0: μ =
μ0 versus H1: μ 6¼ μ0. This is the z test described in Section 6.3.1,
where the test statistic is ^z ¼
ﬃﬃﬃﬃ
N
p
XN  μ0


=σ. The test accepts the null hypothesis when
195
6.4 Hypothesis Tests and Conﬁdence Intervals
.007
13:35:08, subject to the Cambridge Core terms of use,

XN  μ0

 < σ zα=2=
ﬃﬃﬃﬃ
N
p
, where zα=2 is the critical value from the Gaussian distribution.
This is equivalent to XN  σ zα=2=
ﬃﬃﬃﬃ
N
p
 μ0  XN þ σ zα=2=
ﬃﬃﬃﬃ
N
p
, which is the 1  α
conﬁdence interval for μ0. Comparing the acceptance region of the test with the conﬁdence
interval, it is clear that μ0 lies within the conﬁdence interval if and only if the hypothesis
test accepts or, alternately, the conﬁdence interval consists precisely of the values of μ0 for
which the null hypothesis is true. The example uses the two-tailed test, but equivalent
statements can be made for the upper- or lower-tail test where the conﬁdence intervals
become XN  σ zα=
ﬃﬃﬃﬃ
N
p
 μ0  ∞or ∞ μ 0  XN þ σ zα=
ﬃﬃﬃﬃ
N
p
. In addition, the out-
come applies generally for other types of hypothesis tests.
As a result, it is common practice to assess estimates of the mean (or another
statistic) for different samples with their conﬁdence intervals to evaluate the null
hypothesis that they are the same, with the seemingly obvious conclusion that when
there is no overlap of the conﬁdence intervals, then the null hypothesis is rejected.
However, this is not correct in general: rejection of the null hypothesis by examining
overlap of the conﬁdence intervals implies rejection by the hypothesis test, but failure
to reject by examining overlap does not imply failure to reject by the hypothesis test.
Consequently, the method of examining overlap is conservative in the sense that it
rejects the null hypothesis less frequently. Schenker & Gentleman (2001) provide a
detailed explanation, whereas Lanzante (2005) provides a simpliﬁed but lucid version.
The fundamental problem is that comparison of the means of two samples uses their
individual standard errors, whereas a two-sample test such as the two-sample t test
of Section 6.3.2 uses the pooled standard error given by the square root of the sum
of the squares of the individual standard errors. For equal-sized samples, it is not
correct to evaluate X
1
N  X
2
N  zα=2^s1 þ zα=2^s2, but it would be correct to evaluate
X
1
N  X
2
N  zα=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
bs2
1 þ bs2
2
p
. However, the latter is not typically presented in plots of
the means of different samples with their error bars. If ^s1 and ^s2 are comparable, then
the discrepancy between the sum of the standard errors and the pooled standard error
(hence the chance of drawing an incorrect conclusion) is large, whereas if one of the
individual standard errors is much larger than the other, then the difference is negli-
gible. It is much safer to use two sample tests to compare two populations than to rely
on graphical comparisons.
6.5 Theory of Hypothesis Tests II
Section 6.2 provides a summary description of parametric hypothesis testing under the
Neyman-Pearson lemma, and Section 6.3 contains a summary of the most commonly used
parametric hypothesis tests. The underlying theory for these two sections is usually
covered in more advanced statistical works, and only a summary of the key points is
provided here. A very readable elementary treatment of this material is contained in Hogg
& Craig (1995). A basic understanding of the principles underlying likelihood ratio tests
has considerable value in applying them using modern resampling techniques.
196
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

6.5.1 Likelihood Ratio Tests for Simple Hypotheses
There are typically many tests at a given signiﬁcance level with which the null hypothesis
can be evaluated. It is desirable to select the “best” ones based on some set of objective
criteria, where best means the test that has the correct signiﬁcance level and is as or more
powerful than any other test at that level.
Let Φ denote a subset of the sample space Ω. Φ is called the best critical region of size α
for testing the simple hypotheses H0: λ ¼ λ 0 versus H1: λ ¼ λ 1 if, for every subset A of the
sample space, the following properties hold:
1. Pr Xi 2 ΦjH0
ð
Þ ¼ α; and
2. Pr Xi 2 ΦjH1
ð
Þ  Pr Xi 2 ΑjH1
ð
Þ.
This deﬁnition states that if the null hypothesis holds, there will be a plurality of subsets
A of Ω such that Pr Xi 2 AjH0
ð
Þ ¼ α. However, there will be one subset Φ such that when
the alternate hypothesis is true, the power of the test is at least as great as the power of the
test for any other subset A. Then Φ speciﬁes the best critical region to test the null against
the alternate hypothesis.
Consider null and alternate hypotheses for parameters λ 0 and λ 1 that are both simple.
Suppose that H0 speciﬁes a null pdf f 0 x
ð Þ and H1 speciﬁes an alternate pdf f 1 x
ð Þ. Given a
speciﬁc random sample
Xi
f
g, the relative probabilities of H0 and H1 are measured by the
likelihood ratio L0 λ 0jx
ð
Þ=L1 λ 1jx
ð
Þ, where Li λijx
ð
Þ is the likelihood function for the null or
alternate hypotheses. Let Φ be a subset of the sample space, and let κ be a positive number
such that:
1. L0 λ 0jx
ð
Þ=L1 λ 1jx
ð
Þ  κ for each point x 2 Φ;
2. L0 λ 0jx
ð
Þ=L1 λ 1jx
ð
Þ  κ for each point x 2 ΦC; and
3. Pr Xi 2 ΦjH0
ð
Þ ¼ α.
Then conditions 13 are necessary and sufﬁcient for Φ to be a best critical region of size α
for testing the two simple hypotheses. This is the Neyman-Pearson lemma of Neyman &
Pearson (1933), which states that among all tests with a given Type 1 error probability α,
the likelihood ratio test minimizes the probability of a Type 2 error β or else maximizes the
power βC.
Example 6.20 Let
Xi
f
g be a random sample from a Gaussian distribution having unit
variance. Consider H0: μ ¼ 0 against H1: μ ¼ 1. Let the signiﬁcance level be α. By the
Neyman-Pearson lemma, among all tests with signiﬁcance α, the test that rejects for small
values of the likelihood ratio is the most powerful test. The likelihood ratio can be easily
computed because the distributions under the null and alternate hypotheses are both
Gaussian
^Λ ¼ L0 x j μ ¼ 0
ð
Þ
L1 x j μ ¼ 1
ð
Þ ¼
exp  P
N
i¼1
x2
i =2


exp  P
N
i¼1
xi  1
ð
Þ2=2


197
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

Taking the log and simplifying yields
log ^Λ ¼ 
X
N
i¼1
x2
i =2 þ
X
N
i¼1
xi  1
ð
Þ2=2 ¼ 
X
N
i¼1
xi þ N
2  log κ
The best critical region is given by
X
N
i¼1
xi  N
2  log κ ¼ ς
and is equivalent to XN  ς=N, where ς is determined such that the size of the critical
region is α. The distribution of the sample mean under the null hypothesis is N 0; 1=N
ð
Þ.
Consequently, ς can be estimated in the usual way from the Gaussian distribution. This
shows that the z test of Section 6.3.1 is a likelihood ratio test.
Example 6.21 Let
Xi
f
g be a random sample from a Cauchy distribution with location
parameter λ. The hypotheses to be evaluated are H0: λ = 0 against H1: λ ¼ λ 1 6¼ 0. The
null distribution is the Cauchy distribution with parameter λ = 0, whereas the alternate
distribution is the same distribution with arbitrary nonzero location parameter. The likeli-
hood ratio is
^Λ ¼
Q
N
i¼1
1
1 þ x2
i
Q
N
i¼1
1
1 þ xi  λ1
ð
Þ2
¼
Y
N
i¼1
1 þ xi  λ1
ð
Þ2
1 þ x2
i
 κ α
ð Þ
This is equivalent to a 2N-order polynomial whose coefﬁcients depend on κ and λ 1, and
hence the form of the likelihood ratio test depends in a complex way on the choice of α.
6.5.2 Uniformly Most Powerful Tests
Consider the more complicated situation of a simple null hypothesis and a composite
alternate hypothesis that is an ensemble of simple hypotheses, such as H0: λ ¼ λ0 against
H1: λ 6¼ λ0, where the signiﬁcance level is α. The critical region will be different for each
possible member of the ensemble of alternate hypotheses. However, it may be possible
to establish that there is a best critical region for all possible alternate hypotheses, which
is called the uniformly most powerful critical region, and the corresponding test is then
uniformly most powerful
(UMP) at a signiﬁcance level α. UMP tests do not always
exist, but if they do, the Neyman-Pearson lemma provides the formalism for their
construction.
198
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

Example 6.22 Let
Xi
f
g be a random sample from a Gaussian distribution having unit
variance, and consider H0: μ ¼ μ 0 against H1: μ > μ 0, where μ 0 is a constant. Let μ 1 be
another constant that is different from μ 0. The Neyman-Pearson lemma gives
log ^Λ ¼ 
X
N
i¼1
xi  μ0
ð
Þ2=2 þ
X
N
i¼1
xi  μ1
ð
Þ2=2 ¼ μ0  μ1
ð
Þ
X
N
i¼1
xi þ N
2 μ2
1  μ2
0


 logκ
where κ > 0. The inequality simpliﬁes to
X
N
i¼1
xi  N
2 μ 0 þ μ 1
ð
Þ 
log κ
μ 1  μ 0
provided that μ 1 > μ 0, and deﬁnes the critical region for the upper-tail test that is of
interest. Setting the right side to a constant ς, it has been demonstrated that the upper-tail z
test is UMP.
However, if the alternate hypothesis is changed to H1: μ 6¼ μ 0, the log likelihood ratio
reduces to
X
N
i¼1
xi  N
2 μ 0 þ μ 1
ð
Þ 
log κ
μ 1  μ 0
when μ 1 < μ 0, which is in conﬂict with the result when μ 1 > μ 0, and hence there is no
UMP two-tailed test for the Gaussian distribution. In fact, two-tailed UMP tests do not
exist in general.
If both the null and alternate hypotheses depend on a parameter λ, and there is a single
sufﬁcient statistic ^λk for λ, then it follows from the factorization theorem for sufﬁcient
statistics (5.16) that
^Λ ¼ V
^λk, λ0

V
^λk, λ1

(6.25)
so the most powerful test is a function only of the sufﬁcient statistic. This result further
underlines the importance of sufﬁciency because tests using any statistic that is not
sufﬁcient will not be UMP.
Suppose that the random sample Xi
f
g is drawn from a member of the exponential family
of distributions whose joint distribution is deﬁned in (5.17). Let k = 1, so the joint
distribution simpliﬁes to
f x1; . . . ; xNjλ
ð
Þ ¼ eA λ
ð ÞB xi
ð ÞþC xi
ð ÞþD λ
ð Þ
(6.26)
provided that xi 2 Ω and λ =2 Ω. If A λ
ð Þ is an increasing function of λ, then the likelihood
ratio is
199
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

^Λ ¼ L0 λ 0; x
ð
Þ
L1 λ 1; x
ð
Þ ¼ e
A λ0
ð
ÞP
N
i¼1
B xi
ð ÞþP
N
i¼1
C xi
ð ÞþN D λ0
ð
Þ
e
A λ1
ð
ÞP
N
i¼1
B xi
ð ÞþP
N
i¼1
C xi
ð ÞþN D λ1
ð
Þ
¼ e
A λ0
ð
ÞA λ1
ð
Þ
½
P
N
i¼1
B xi
ð ÞþN D λ0
ð
ÞD λ1
ð
Þ
½

(6.27)
If λ0 > λ1, the condition that A λ
ð Þ is increasing means that the likelihood ratio is an
increasing function of ^τ ¼ PN
i¼1B xi
ð Þ. Consequently, the likelihood ratio ^Λ is a monotone
function of the sufﬁcient statistic ^τ, and a test of H0: λ ¼ λ 0 versus H1: λ < λ 0 using ^Λ  κ
reduces to one involving only the sufﬁcient statistic ^τ  c for every λ 1 < λ 0 and is UMP.
A similar statement can be applied to the upper-tail alternate hypothesis. This provides a
method to devise UMP tests that are based on sufﬁcient statistics for the exponential family
of distributions.
6.5.3 Likelihood Ratio Tests for Composite Hypotheses
A general approach to likelihood ratio testing will be deﬁned that does not result in a UMP
test but that typically does yield a better test than can be constructed by other methods.
Such tests play the same role in hypothesis testing that the mle does for parameter
estimation. Suppose that there is a random sample
Xi
f
g that has a likelihood function
L λjxi
ð
Þ and that λ is a vector of parameters (λs, λc), where s þ c ¼ p is the total number of
parameters. The null hypothesis is H0: λs ¼ λ∗
s
and is composite unless c ¼ 0. The
alternate hypothesis is H1: λs 6¼ λ∗
s . The unconditional maximum of the likelihood function
is obtained using the mles for λs and λc, denoted by ^λs and ^λc. The conditional maximum of
the likelihood function is obtained using the mles for λc when H0 holds and will in general
differ from ^λc. Denote these mles by ^^λc. The likelihood ratio is
^Λ ¼
L0 λ∗
s ; ^^λcjxi


L1 ^λs; ^λcjxi


(6.28)
Equation (6.28) is the maximum of the likelihood under the null hypothesis divided by the
largest possible value for the likelihood, and ^Λ ! 1 indicates that H0 is acceptable. Thus
there exists a critical value κ for ^Λ such that ^Λ  κ rejects the null hypothesis. The critical
value is determined from the distribution of ^Λ at a speciﬁed value of α if that distribution is
available.
Example 6.23 Let
Xi
f
g be a set of N random variables from N μ; σ2
ð
Þ, where μ and σ2 are
unknown. The composite null hypothesis H0: μ ¼ 1, σ2 > 0 will be tested against the
composite alternate hypothesis H1: μ 6¼ 1, σ2 > 0. The likelihood function for the uncon-
ditional alternate hypothesis is
L1 μ; σ2jxi


¼ 2π σ2

N=2e
P
N
i¼1
xiμ
ð
Þ2= 2σ2
ð
Þ
whereas for the null hypothesis it is the same expression with μ ¼ 1. Under the null
hypothesis, the mle for the variance is ^s2
0 ¼ 1=N
ð
ÞPN
i¼1 xi  1
ð
Þ2, whereas under the
200
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

alternate hypothesis, the mles are ^μ ¼ XN and ^s2
1 ¼ 1=N
ð
ÞPN
i¼1 xi  XN

2 ¼ ^s2
N. Substi-
tuting into the likelihood functions for the numerator and denominator and simplifying
yields
^Λ ¼
P
N
i¼1
xi  XN

2
P
N
i¼1
xi  1
ð
Þ2
2
6664
3
7775
N=2
¼
1
1 þ^t2= N  1
ð
Þ
½
N=2
where ^t ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N  1
p
XN  1


=^sN is the one-sample t test statistic (6.7) for a postulated mean
of 1. The null hypothesis is rejected if ^Λ  κ  1 corresponding to ^t 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N 1
ð
Þ κ2=N 1
ð
Þ
p
.
Because ^t has Student’s t distribution with N  1 degrees-of-freedom, it has been shown
that the likelihood ratio test is the one-sample t test. It rejects if the test statistic ^t exceeds a
critical value for Student’s t distribution.
Example 6.24 Let
Xi
f
g and
Yj


be sets of N and M independent rvs from Gaussian
distributions with different means μ 1 and μ 2 and a common but unknown variance σ2.
A likelihood ratio test will be devised for H0: μ 1 ¼ μ 2, σ2 > 0 versus H1: μ 1 6¼ μ 2,
σ2 > 0, where neither μ1 nor μ2 is speciﬁed. Both the null and alternate hypotheses are
composite. The likelihood for the ﬁrst set of samples is
L μ1; σ2jx


¼ 2πσ2

N=2e
P
N
i¼1
xiμ1
ð
Þ2= 2σ2
ð
Þ
and similarly for the second set with M, yi, and μ2 substituting for N, xi, and μ1. The total
likelihood is the product of the x and y likelihood functions. The total log likelihood is
log L ¼  N þ M
2
log 2π  N þ M
2
log σ2 
X
N
i¼1
xi  μ1
ð
Þ2 þ
X
M
i¼1
yi  μ2
ð
Þ2
"
#
= 2σ2


Under the null hypothesis, there exists a sample of size M þ N from a normal distribu-
tion with unknown mean and variance. The mles for the mean and variance under the null
hypothesis are obtained by maximizing the likelihood ratio with μ2 ¼ μ1, yielding
^μ 0 ¼ N XN þ M YM


= N þ M
ð
Þ
^s2
0 ¼
X
N
i¼1
xi  ^μ0
ð
Þ2 þ
X
M
i¼1
yi  ^μ0

2
"
#
= N þ M
ð
Þ
and a maximum for the likelihood of
L0 ¼ 2πe^s2
0

 NþM
ð
Þ=2
The unconditional log likelihood yields the unconstrained mles of ^μ1 ¼ XN, ^μ2 ¼ YM,
and
201
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

^s2
1 ¼
X
N
i¼1
xi  ^μ 1
ð
Þ2 þ
X
M
i¼1
yi  ^μ 2

2
"
#
= N þ M
ð
Þ
The unconditional likelihood is given by
L1 ¼ 2πe^s2
1

 NþM
ð
Þ=2
The likelihood ratio is
^Λ ¼ L0
L1
¼
^s2
1
^s2
0

NþM=2
and rejects for ^Λ  κ. Substituting for ^s2
0 and ^s2
1 and simplifying yields
^s2
1
^s2
0
¼
1
1 þ
NM
N þ M
XN  YM

2
P
N
i¼1
xi  XN

2 þ P
M
i¼1
yi  YM

2
¼
1
1 þ
^t2
N þ M  2
where
^t ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
NM
N þ M
r
XN  YM


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N ^s2
x þ M ^s2
y


= N þ M  2
ð
Þ
r
is the two-sample t statistic (6.9). Because the test rejects for large values of ^t2, it also
rejects for large values of ^t, and hence it has been shown that the likelihood ratio test is
equivalent to the two-sample t test.
If the sampling distribution for ^Λ under H0 is known, as in the preceding
examples, then critical values and p-values can be obtained directly. However, in
many instances, the sampling distribution is unknown, and recourse must be made to
its asymptotic distribution. It can be shown that the asymptotic distribution of the
likelihood
ratio
statistic
(6.28)
is
noncentral
chi
square
with
noncentrality
parameter δ ¼ λs  λo
s

T 
 I
$
F 
 λs  λo
s


, where I
$
F is the Fisher information matrix
for the simple null hypothesis parameters. This reduces to the central chi square
distribution when H0 holds (i.e., when λs ¼ λ∗
s ). The distribution degrees-of-freedom
is the difference between the number of free parameters under H0 [ H1 minus the
number of free parameters under H0. This result is called Wilks’ theorem and was
introduced by Wilks (1938). It has an error that is typically O 1=N
ð
Þ. For (6.28),
the number of free parameters under the null hypothesis is c because the data
sample determines the parameters under the simple hypothesis. Under H0 [ H1, there
are s þ c free parameters, and consequently, the asymptotic distribution for (6.28)
202
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

has s degrees-of-freedom. The asymptotic power of the likelihood ratio test follows
directly from (6.5) using the noncentral chi square distribution.
Example 6.25 The mean monthly wind direction for a high-latitude region using the
National Center for Environmental Prediction product is contained in the ﬁle wind.dat
for calendar year 2004–5. The monthly means will average out periodic phenomena such
as the tides to a substantial degree, simplifying the data statistics. The postulated mean
over the 2-year interval is 240. Derive the likelihood ratio test for the null hypothesis H0:
ν ¼ ν∗, κ > 0 versus H1: ν 6¼ ν∗, κ > 0 for the von Mises distribution, and apply it to
the data.
Figure 6.12 is a rose diagram of the data showing a lot of scatter but with some tendency
to cluster around ~140 and ~315. The sample mean is 239.6.
The mles for ν and κ that are needed for the unconstrained alternate hypothesis were
derived in Example 5.23
^ν ¼ tan 1 ^S=^C


I1 ^κ
ð Þ
I0 ^κ
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^S
2 þ ^C
2
q
N
where ^S ¼ PN
i¼1 sin θi and ^C ¼ PN
i¼1 cos θi. The second equation must be solved numer-
ically. The denominator of the likelihood ratio is
L1 ¼ e
^κ P
N
i¼1
cos θi^ν
ð
Þ
2π
ð
ÞNIN
0 ^κ
ð Þ
  1
  2
  3
30
210
60
240
90
270
120
300
150
330
180
0
Figure 6.12
Rose diagram for the wind direction data.
203
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

The numerator of the likelihood ratio is
L0 ¼ e
^^κ P
N
i¼1
cos θiν∗
ð
Þ
2π
ð
ÞNIN
0
^^κ
 
where ^^κ satisﬁes
I1 ^^κ
 
I0 ^^κ
  ¼
^C cos ν∗þ ^S sin ν∗
N
The test statistic is
2 log ^Λ ¼ 2^κ
X
N
i¼1
cos θi  ^ν
ð
Þ þ 2N log I0 ^^κ
 
 2^^κ
X
N
i¼1
cos θi  ν∗
ð
Þ  2N log I0 ^κ
ð Þ
and is asymptotically distributed as chi square with one degree-of-freedom.
Solution of the equations for the precision parameters requires use of the MATLAB
nonlinear equation solver fzero(x, start), where x is a function handle and start is an initial
guess for the solution. A MATLAB script implementing the test is
wind = importdata('wind.dat');
s = sum(sind(wind));
c = sum(cosd(wind));
r = sqrt(s^2 + c^2);
nuhat = atan2d(s, c);
nustar = 240;
n = length(wind);
khat = fzero(@(x) besseli(1, x)/besseli(0, x) - r/n, 1);
khathat = fzero(@(x) besseli(1, x)/besseli(0, x) - (c*cosd
(nustar) + s*sind(nustar))/n, 1);
t1 = 2*khat*sum(cosd(wind - nuhat));
t2 = 2*n*log(besseli(0, khathat));
t3 = 2*khathat*sum(cosd(wind - nustar));
t4 = 2*n*log(besseli(0, khat));
test = t1 + t2 - t3 - t4
test =
0.0024
1 - chi2cdf(test, 1)
ans =
0.9609
The critical value is 5.02, and neither it nor the p-value rejects the null hypothesis.
However, the data are not tightly clustered (see Figure 6.12) given that the mle for κ is
only 3.1, so this is not surprising. If the test is repeated with a hypothesized mean of 150,
the test statistic is 41.6706, the p-value is 1.08e–10, and the null hypothesis is rejected.
204
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

The asymptotic power can be computed in the standard way. The variance can be
estimated as ^σ2 ¼ 1= ^R ^κ


, where ^R ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^S
2 þ ^C
2
q
, so δ ¼ sin 2 ^ν  ν∗
ð
Þ=^σ2.
wind = importdata('wind.dat');
s = sum(sind(wind));
c = sum(cosd(wind));
nuhat = atan2d(s, c);
n = length(wind);
r = sqrt(c^2 + s^2);
khat = fzero(@(x) besseli(1, x)/besseli(0, x) - r/n, 1);
sigma2 = 1./(r*khat);
nustar = 230:.1:250;
delta = sind(nuhat - nustar).^2/sigma2;
xhi = chi2inv(.975, 1);
xlo = chi2inv(.025, 1);
power = 1 - ncx2cdf(xhi, 1, delta) + ncx2cdf(xlo, 1, delta);
plot(nustar, power)
The test has low power (Figure 6.13) and hence a low probability of rejecting the null
hypothesis when it is false.
Likelihood ratio tests are usually carried out with respect to the general alternate
hypothesis that the parameters are free. If a speciﬁc alternate hypothesis can be formu-
lated, then better power generally can be obtained. For example, consider testing the null
hypothesis that a set of rvs
Xi
f
g is Poisson with parameter λ against the alternate
hypothesis that the data are Poisson but with different parameters (i.e., rates) for each
data point. Under the null hypothesis, the mle ^λ for λ is the sample mean of the data.
Under the alternate hypothesis, the mles ~λi are the corresponding data values xi. The
likelihood ratio is given by
230
235
240
245
250
Hypothesized Mean
0.05
0.1
0.15
0.2
0.25
Power
Figure 6.13
The power of the wind direction likelihood ratio test for the von Mises distribution.
205
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

^Λ ¼
Q
N
i¼1
^λxie^λ=xi!
Q
N
i¼1
~λi
xie~λi=xi!
¼
Y
N
i¼1
XN
xi

xi
e xiXN
ð
Þ
(6.29)
The statistic 2 log ^Λ ¼ 2PN
i¼1xi log xi=XN


may be tested using the chi square approxi-
mation if N is large. There is one degree-of-freedom under the null hypothesis, and
there are N degrees-of-freedom under the alternate, so the degrees-of-freedom for the test
are N  1. This test is called the Poisson dispersion test. It has high power against
alternatives that are overdispersed compared with a Poisson process.
Example 6.26 A data set in the ﬁle tornados.dat contains the number of tornados in a
single state from 1959 to 1990. Consider the null hypothesis that the data are Poisson
against the alternate that they are too dispersed to be Poisson. The data are plotted in
Figure 6.14.
There is some suggestion of an upward trend with time, although the data scatter is large,
as would be expected for a Poisson process. Form the test
x = importdata('tornado.dat');
test = 2*sum(x.*log(x/mean(x)))
test =
32.9726
There are 30 data, so the test has 29 degrees-of-freedom. The critical value is 45.72, and the
p-value for the test is
1 - chi2cdf(test, 29)
ans =
0.2788
The null hypothesis is accepted, and the data appear to be Poisson.
1950
1960
1970
1980
1990
Year
0
2
4
6
8
10
Number of Tornados
Figure 6.14
The number of tornados as a function of time.
206
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

6.5.4 The Wald Test
A simple approximation to the likelihood ratio test is the Wald test due to Wald
(1943). This test is asymptotically efﬁcient but often remains analytically approach-
able when the likelihood ratio is complicated. The Wald test is asymptotically
equivalent to the likelihood ratio test but differs in performance with a ﬁnite
sample.
Consider the likelihood ratio (6.28) for a single parameter λ under the null and alternate
hypotheses H0: λ ¼ λ∗versus H1 : λ 6¼ λ∗, and expand log L0 λ∗jxi
ð
Þ in a second-order
Taylor series about log L1ð^λjxiÞ, yielding
2 log ^Λ ¼ log L0 λ∗jxi
ð
Þ
L1ð^λjxiÞ
"
#
 2
^λ  λ∗
U
^λ

 ð^λ  λ∗Þ2U 0^λ

(6.30)
where Uð^λÞ is the score function (5.5) evaluated at ^λ. The score function is zero at ^λ, and
consequently,
2 log ^Λ  Nð^λ  λ∗Þ2 U 0ð^λÞ=N
h
i
(6.31)
The estimator ^λ for λ under the null hypothesis is consistent, so U0^λ

=N !
U 0 λ∗
ð
Þ=N ! I λ∗
ð
Þ=N, where I λ
ð Þ is the Fisher information (5.7). Substituting into
(6.31) gives
2 log ^Λ  ^W ¼ ð^λ  λ∗Þ2I λ∗
ð
Þ
(6.32)
where ^W is the Wald test statistic that is asymptotically distributed as χ2
1.
The Wald test easily generalizes to multiple parameters and to composite hypotheses.
Let ^λ be a p-vector that is partitioned into s simple and c composite elements, where
s þ c ¼ p, as in Section 6.5.3. For the composite hypotheses H0 : ^λs ¼ λ∗
s
versus
H1 : ^λs 6¼ λ∗
s , the Wald test statistic is
^W ¼ ^λs  λ∗
s

T 
 I
$ssð^λÞ
h
i1

 ^λs  λ∗
s


(6.33)
where ^λs is the unconditional mle, and the Fisher information matrix is partitioned as
I
$ ^λ
 
¼
I
$
ss
I
$
sc
I
$
cs
I
$
cc
"
#
(6.34)
and I
$ss ^λ
 
is the upper left element of the inverse of (6.34). By the block matrix inversion
lemma:
I
$ss ^λ
 
h
i1
¼ I
$
ss  I
$
sc 
 I
$
cc
1

 I
$
cs
(6.35)
The Wald test statistic is asymptotically distributed as χ2
s.
207
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

Example 6.27 Evaluate the Wald test for equivalence of the Michelson speed of light data in
Example 6.10.
The test for equivalence of the means is the same as testing that the difference of the
means is zero. The mles for the mean and standard deviation of the 1879 and 1881 data
are given by the sample means and standard deviations, respectively. Consequently, an
estimator for their difference is obtained by subtracting the sample means, yielding
88.52, and an approximate pooled variance is bs2
1=N2 þ bs2
2=N1 = 376.14. The Wald test
statistic (6.32) is the square of the difference of the means divided by the pooled
variance, or 20.83. The critical value from the χ2
1 distribution for a two-tail test is 5.02,
and hence the null hypothesis that the difference of the means is zero is rejected. The
asymptotic p-value is 1.00  10–5. This result is very close to that from the two-sample t
test in Example 6.10.
6.5.5 The Score Test
The score test is an alternative to the Wald and likelihood ratio tests and was introduced by
Rao (1947). All three of these tests are asymptotically equivalent but will behave differ-
ently for a ﬁnite-size sample.
The derivation of the score test closely parallels that of the Wald test. Consider a single
parameter λ and a simple hypothesis under the null and alternate hypotheses H0: λ ¼ λ∗
versus H1: λ 6¼ λ∗. Compute the score function Uð^λÞ as in (6.30), and let Ið^λÞ be the Fisher
information. The score test statistic is
^W
0 ¼ U 2 λ∗
ð
Þ
I λ∗
ð
Þ
(6.36)
and is asymptotically distributed as χ2
1. The score test statistic depends only on the null
value for the test statistic, unlike the Wald statistic (6.32).
The score test easily generalizes to multiple parameters and composite hypotheses. Let ^λ
be a p-vector that is partitioned into s simple and c composite elements, where s þ c ¼ p as
in Section 6.5.4. For the composite hypotheses H0: ^λs ¼ λ∗
s versus H1: ^λs 6¼ λ∗
s , the score
test statistic is
^W
0 ¼ UTð^λsÞ 
 I
$1ð^λsÞ 
 Uð^λsÞ
(6.37)
where ^λs is the mle under the null hypothesis. The score test statistic (6.37) is distributed as
χ2
s when the null hypothesis is true.
Score tests are frequently used in directional statistics. The simplest example is the
Rayleigh test for uniformity, which is the score test for uniformity under a von Mises
model. The log likelihood for the von Mises distribution is
208
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

log L ν; κjθ
ð
Þ ¼ κ
X
N
i¼1
cos θi  ν
ð
Þ  N log I0 κ
ð Þ
¼ κ cos ν; κ sin ν
ð
Þ 
X
N
i¼1
cos θi
X
N
i¼1
sin θi
0
B
B
B
B
@
1
C
C
C
C
A
 N log I0 κ
ð Þ
(6.38)
The von Mises parameters can be taken as κ cos ν; κ sin ν
ð
Þ instead of ν; κ
ð
Þ. The score
function is
U ¼
X
N
i¼1
cos θi
X
N
i¼1
sin θi
0
B
B
B
B
B
@
1
C
C
C
C
C
A
 N I1 κ
ð Þ
I0 κ
ð Þ
cos ν
sin ν


(6.39)
In the uniform limit when κ ! 0, the score becomes the ﬁrst term in (6.39). From the
central limit theorem for directional data in Section 4.7.3, the asymptotic covariance matrix
(4.44) in the uniform limit is I
$
2=2. The score statistic is
^W
0 ¼ 2N R
2
(6.40)
where R is the mean resultant length. The score statistic (6.40) is asymptotically
distributed as χ2
2. The Rayleigh test is most powerful against von Mises alternatives,
and because R is invariant under rotation and reﬂection of data, the test shares these
properties.
Example 6.28 Apply the Rayleigh test to the wind direction data of Example 6.25.
wind = importdata('wind.dat');
s = sum(sind(wind));
c = sum(cosd(wind));
rbar = sqrt(s^2 + c^2)/length(wind);
rayl = 2*length(wind)*rbar^2
rayl =
32.0933
1 - chi2cdf(rayl, 2)
ans =
1.0740e-07
The null hypothesis that the data are uniformly distributed is rejected, which is no surprise
in light of Figure 6.12.
209
6.5 Theory of Hypothesis Tests II
.007
13:35:08, subject to the Cambridge Core terms of use,

6.6 Multiple Hypothesis Tests
There are situations where it is necessary to compute many simultaneous independent
hypothesis tests. This topic has become prominent because of recent advances in ﬁelds
such as genomics, where microarrays make it possible to measure thousands to many
hundreds of thousands of genes at the same time, or in image processing, where many
pixels in a scene must be tested together. In geophysics, an example occurs in the inversion
of multiple data sets from distinct sites. It is customary to lump all the misﬁt across all data
sets and sites into a single misﬁt measure such as the chi square statistic of Section 6.3.3
and then test the result using just the single statistic. This approach loses information about
the misﬁt at each site and for each data set type. Instead, it would make more sense to test
each data set and site separately and merge the results into a multiple hypothesis test.
However, there is a problem in multiple hypothesis testing because if a particular test is
carried out at a signiﬁcance level α ¼ 0:05 for rejecting the null hypothesis when it is true,
then for an ensemble of tests, about one of twenty will be false positives. The problem is
that there is no control on the false positive rate for the ensemble.
It is easy to compute the probability of observing a false positive for N independent tests,
each of which has a signiﬁcance level α. The distribution of false positives is binomial with
probability α and number of trials N. The probability of observing n false positives is
bin n; α; N
ð
Þ. Consequently, it is straightforward to assess whether the observed number of
signiﬁcant tests in an ensemble is reasonable or not.
Example 6.29 For 100 simultaneous independent tests with each test having a signiﬁcance
level of 0.05, what is the probability of observing 1, 10, or 20 signiﬁcant tests?
The probability is PN
i¼nbin i; α; N
ð
Þ ¼ 1  Bin n  1; α; N
ð
Þ with N ¼ 100. Using the
MATLAB binocdf function, the answers are 0.994, 0.028, and 1.05e–07. For 10 or 20
observations, there clearly must be some real positives. The remaining issue is which ones
are they?
The simplest way to address this issue is the Bonferroni method introduced in Section 5.5,
which will now be formalized. Let the ensemble false positive probability be Α. In the
statistics literature, A is often called the family-wide error rate (FWER), or the probability
that at least one Type 1 error occurs, and the signiﬁcance level for a particular test α is the
comparison-wise error rate (CWER). The probability of not making a Type 1 error in a
particular test is 1  α, and for N such independent tests, the probability becomes
1  α
ð
ÞN. Consequently, the probability of at least one false positive is
A ¼ 1  1  α
ð
ÞN
(6.41)
Equation (6.41) can be easily inverted to yield the CWER required to achieve a speciﬁed
FWER for a given number of tests:
210
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

α ¼ 1  1  Α
ð
Þ1=N
(6.42)
A simpler version obtains by approximating
1  A
ð
Þ1=N  1  A=N, yielding the
Bonferroni CWER α ¼ A=N. The probability of the false rejection of any of the null
hypotheses is no more than α. However, the result is very conservative because either
(6.42) or the Bonferroni approximation to it makes it unlikely that even a single null
hypothesis will be falsely rejected. In addition, the power of each test is reduced by the use
of very small signiﬁcance levels.
Table 6.3 summarizes the outcomes from simultaneously performing N independent
hypothesis tests, whereas Table 6.1 covered the outcomes from a single test. Each of the N
hypotheses is presumed to have a p-value pi, and the ith hypothesis is deemed signiﬁcant if
the corresponding null hypothesis is rejected through a small p-value. The unobservable
random variables v, s, u, and t are, respectively, the number of false rejections, the number
of true rejections, the number of true acceptances, and the number of false rejections. The
observable random variables r ¼ s þ v and w ¼ u þ t are, respectively, the number of
signiﬁcant and insigniﬁcant tests. Similarly, the total number of true and false null
hypotheses N0 and N1 are unknown but ﬁxed. Finally, the proportion of false rejections
is v=r for r > 0, and the proportion of false acceptances is t=w for w > 0. Consequently,
the FWER is simply Pr v  1
ð
Þ.
Neither (6.42) nor the Bonferroni approximation that controls the FWER accounts
for the fact that rejection of a given hypothesis reduces the number of hypothesis tests
for the ensemble. This led to the development of a number of sequential approaches to
multiple hypothesis testing, culminating in the seminal work of Benjamini & Hochberg
(1995), who introduced the concept of false discovery rate (FDR), which is the
expected proportion of false rejections of the null hypothesis. From Table 6.3, the
FDR is E v=r
ð
Þ, with the understanding that this quantity is zero when r ¼ 0.
The motivation for FDR is that multiple hypothesis testing is usually a screening step
in a data analysis algorithm, and the data underlying tests that accept will often be
subjected to further analysis. In this case, it is more important to ensure that all the true
tests are included, accepting that a fraction of false positives will be included but
controlling their quantity.
The Benjamini-Hochberg one-stage step-up algorithm is as follows:
1. Compute the order statistics p k
ð Þ from the p-values obtained by applying N individual
hypothesis tests;
2. For independent tests, deﬁne βk ¼ kq=N, where k ¼ 1, :::, N, and q is an a priori
probability level (typically 0.05). For dependent tests, divide βk by PN
k¼1 1=k
ð
Þ;
Table 6.3 Outcomes from N Simultaneous Hypothesis Tests
Signiﬁcant
Not signiﬁcant
Total
True null
v
u
N0
False null
s
t
N1
Total
r
w
N
211
6.6 Multiple Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

3. Deﬁne the rejection threshold R ¼ max p k
ð Þ < βk


; and
4. If R is nonzero, reject all null hypotheses for which pk  R, and otherwise retain all
hypotheses.
Benjamini & Hochberg (1995) proved that the FDR from this procedure is always less than
or equal to N0 q=N, and more generally, they proved that the algorithm is a solution to the
constrained maximization problem of choosing α that maximizes the number of rejections
subject to the constraint αN= N  N 0
ð
Þ  q.
The following MATLAB script implements the Benjamini-Hochberg algorithm:
function [RejTh, Index] = Benjamini(pval, varargin)
%computes the rejection threshold ReJTh and the index Index
of p-values
%pval that reject the null hypothesis using the Benjamini-
Hochberg method
%input variables
%pval – vector of pvalues (required)
%q – FWER probability level (optional), default is 0.05
%cm
=
turn
on
dependence
factor
(optional),
default
is
1 for independence
[n p] = size(pval);
switch nargin
case 1
q = 0.05;
cm = 1;
case 2
q = varargin{1};
cm = 1;
case 3
q = varargin{1};
i = 1:max(n, p);
cm = sum(1./i);
end
if n < p
pval1 = pval';
else
pval1 = pval;
end
beta = (1:n)'*q/(cm*n);
pvalo = sort(pval1);
i = ﬁnd(pvalo - beta <= 0);
if isempty(i)
RejTh = 0;
else
RejTh = pvalo(i(length(i)));
212
Hypothesis Testing
.007
13:35:08, subject to the Cambridge Core terms of use,

end
Index = ﬁnd(pval1 <= RejTh);
End
Example 6.30 Suppose that ten independent hypothesis tests are computed, leading to the
following set of ordered p-values:
0.00013
0.00333
0.00579
0.00956
0.01345
0.26489
0.35487
0.55698 0.65345 0.99111
Apply a Bonferroni and Benjamini-Hochberg multiple hypothesis test at the 0.05
level.
The Bonferroni threshold with α ¼ 0:05 is 0.005, and hence the ﬁrst two tests are
rejected. For the Benjamini-Hochberg method, βi is given by the following:
0.0050
0.0100
0.0150
0.0200
0.0250
0.0300
0.0350
0.0400
0.0450
0.0500
Comparing the two shows that R = 0.01345 and r = 5. The ﬁrst ﬁve tests are rejected. The
FDR has an upper bound of 0.025. Note that the data suggest the presence of two
populations, with the ﬁrst ﬁve having small and the second ﬁve much larger p-values.
Multiple hypothesis testing continues to be an area of active research. Genovese &
Wasserman (2002) showed that the Benjamini-Hochberg algorithm minimizes the
expected proportion of unrejected hypotheses for which the alternate holds, or in other
words, it minimizes the false nondiscovery rate. Benjamini, Krieger, & Yekutieli (2006)
introduced a two-stage multiple hypothesis estimator in which the initial step is the original
algorithm that is used to estimate R, and the second stage uses a corrected value of q based
on that estimate. The result is a more powerful multiple hypothesis testing procedure.
213
6.6 Multiple Hypothesis Tests
.007
13:35:08, subject to the Cambridge Core terms of use,

7
Nonparametric Methods
7.1 Overview
The hypothesis tests discussed to this point have been based on the premise that the data
come from a known distribution in which some or all of the parameters are unknown.
These are called parametric tests.
This chapter covers nonparametric tests of two classes: goodness-of-ﬁt tests and rank-
based hypothesis tests. The former begins with the little-used likelihood ratio test for a
multinomial distribution that is more general than its widely used asymptotic approxima-
tion, the Pearson’s χ2 test, and deserves greater attention. The Kolmogorov-Smirnov and
Anderson-Darling tests for data distribution assessment are then reviewed, and the Jarque-
Bera test for Gaussianity is summarized.
Rank-based nonparametric tests are typically permutation tests (see Section 8.3) applied
to the data ranks and were devised during the 1940s and 1950s before computers were
powerful enough to apply permutation methods directly to data. Instead, it made sense to
tabulate their results for data ranks. Such tests are less sensitive to distributional assumptions
and work better than parametric tests with small samples. They typically lack power in
comparison with parametric tests but are more robust to departures from parametric test
assumptions. The sign, signed rank, and rank sum tests are described. The Ansari-Bradley
test for dispersion is then summarized. A pair of nonparametric tests for correlation, the
Spearman rank correlation and Kendall’s τ, are then described. The chapter closes with a
description of meta-analysis that allows p-values from separate studies to be combined.
7.2 Goodness-of-Fit Tests
Let
Xi
f
g be a random sample from an unknown cdf F x
ð Þ. The null hypothesis is H0:
F x
ð Þ ¼ F∗x
ð Þ, where F∗x
ð Þ is a speciﬁed target distribution that is to be tested against a
suitable alternate hypothesis, which is often H1: F x
ð Þ 6¼ F∗x
ð Þ. This is called a goodness-
of-ﬁt test. It is a simple hypothesis if all the parameters in the target distribution are
speciﬁed and a composite hypothesis if they are not.
7.2.1 Likelihood Ratio Test for the Multinomial Distribution
Partition the sample space S for a random variable X into k subspaces
Si
f
g that exhaust
S. Assume that N independent observations of the random variable (rv) are available, and
214
.008
13:35:01, subject to the Cambridge Core terms of use,

let
Yi
f
g be the number of rvs in Si. The joint distribution of
Yi
f
g is multinomial with
parameters N and
πi
f g, where πi ¼ Pr X 2 Si
ð
Þ. A likelihood ratio test for the goodness-
of-ﬁt to a multinomial model can be devised in the usual way. The hypothesis under test
is H0: πi ¼ pi, for i = 1, . . ., k, versus H1: πi 6¼ pi, for at least one value of i where pi
is known.
The likelihood ratio will be formulated relative to an alternate hypothesis where the k
class probabilities are free but subject to the constraint that they sum to 1, so the
alternate hypothesis has k  1 degrees-of-freedom. If m additional parameters are
estimated from the data for the null hypothesis, the test degrees-of-freedom are reduced
to k  m  1. Typically, at least one parameter will be needed for the null hypothesis,
so m  1.
The numerator of the likelihood ratio is
L N1; . . . ; N k; p1; . . . ; pkjyi
ð
Þ ¼
N!
y1!    yk! p1 λ
ð Þy1    pk λ
ð Þyk
(7.1)
and is maximized when λ is the multinomial maximum likelihood estimator (mle) ^λ.
The corresponding probabilities will be denoted as pi ^λ
 
. The denominator of the likeli-
hood ratio is the multinomial pdf with pi set to the multinomial mle ^pi ¼ yi=N. The
likelihood ratio is
^Λ ¼
N!
y1!    yk! p1 ^λ
 y1    pk ^λ
 yk
N!
y1!    yk! ^p1
y1    ^pk
yk
¼
Y
k
i¼1
pi ^λ
 
^pi
2
4
3
5
yi
¼
Y
k
i¼1
Npi
yi

yi
(7.2)
Taking logs and using yi ¼ N^pi yields
2 log ^Λ ¼ 2N
X
k
i¼1
^pi log
pi ^λ
 
^pi
2
4
3
5 ¼ 2
X
k
i¼1
oi log
oi
ei
 
(7.3)
where the observed number of occurrences is oi ¼ N^pi ¼ yi, and the expected number of
occurrences is ei ¼ Npi ^λ
 
. The null hypothesis is rejected if ^Λ or its logarithm is small
enough. The exact distribution for ^Λ is unknown, and hence the asymptotic χ2
km1
approximation must be used.
A signiﬁcant weakness of the multinomial likelihood ratio test (as well as the Pearson
chi square test covered in the next section) is that the subdivision of the sample space S,
and hence choice of the boundaries between the k classes, is arbitrary, yet different
parameterizations will result in different values for the likelihood ratio ^Λ. Further, if the
class boundaries are chosen from the data, they become rvs and hence should be incorpor-
ated into the model. A general rule of thumb is to choose the classes such that they have
equal probability under the null hypothesis and choose k ~ 10, although often neither
criterion is fulﬁlled in practice.
The multinomial likelihood ratio test is consistent (and hence asymptotically unbiased).
It is unbiased if the classes are chosen so that their corresponding probabilities are
approximately equal and will have the best power when the classes are approximately
215
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

the same size. The test is somewhat robust to the presence of a small number of extreme
values because of the logarithm in (7.3).
Example 7.1 Rutherford, Geiger, & Bateman (1910) present some of the ﬁrst statistical data
for radioactive decay. Their experiments measured α decay in polonium over 2608 disjoint
72-second intervals. The columns in the Table 7.1 give the number of counts and the
number of occurrences.
Based on physics, the data should be Poisson, and the sample mean constitutes the mle
for the parameter in the distribution.
rutherford = [57 203 383 525 532 408 273 139 45 27 10 4 1 1];
c = [0 1 2 3 4 5 6 7 8 9 10 11 13 14];
param = sum(rutherford.*c)/sum(rutherford)
param =
3.8715
n = sum(rutherford);
p = poisspdf(c, param);
loglambda = 2*sum(rutherford.*log(rutherford./(n*p)))
loglambda =
20.0336
1 - chi2cdf(loglambda, length(c) – 2)
ans =
0.0665
The null hypothesis that the data are Poisson is accepted, although not strongly.
Table 7.1 Radioactive Decay Data
Counts
Number
0
57
1
203
2
383
3
525
4
532
5
408
6
273
7
139
8
45
9
27
10
10
11
4
13
1
14
1
216
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

Example 7.2 Table 7.2 presents the number of V-1 ﬂying bomb hits in a 36 square kilometer
part of South London during World War II. The area was divided into 0.25-km squares, and
the number of hits per square is contained in Table 7.2. Assess whether the hits were
random or not.
If the hits were random, then a Poisson distribution should ﬁt the data. The observed
occurrences are given in the second column of the table. The expected occurrences are
given by the Poisson probabilities scaled by the total number of hits.
bomb = importdata('bomb.dat');
o = bomb(:, 2);
e
=
poisspdf(bomb(:,
1),
sum(bomb(:,
1).*bomb(:,
2))/sum
(bomb(:, 2)))* . . .
sum(bomb(:, 2));
loglambda = 2*sum(o.*log(o./e))
loglambda =
1.4995
1 - chi2cdf(loglambda, length(bomb) - 2)
ans =
0.8267
The null hypothesis is accepted.
Table 7.2 V-1 Bomb Hits in London
Hits
Number
0
229
1
211
2
93
3
35
4
7
5 and over
1
Source: Clarke (1946).
Example 7.3 A data set in the ﬁle quakes.dat consists of 62 measurements of the time in
days between successive earthquakes taken from Hand et al. (1994). Assess the goodness-
of-ﬁt of the data to the exponential distribution.
quakes = importdata('quakes.dat');
[f, x] = ecdf(quakes);
plot(x, f, x, expcdf(x,mean(quakes)))
217
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

The empirical and exponential cdfs with the mle for the distribution parameter are
compared in Figure 7.1.
This problem is most easily solved by using the histogram function to compute the
counts in each of a given number of bins, then integrate the pdf between the histogram bin
boundaries to get the theoretical probabilities, and then convert to theoretical counts. The
difﬁculty with an exponentially distributed variate is that the counts in some of the bins
may be zero and will not likely be approximately equal without carefully choosing the bin
edges.
h = histogram(quakes, 7, 'Normalization', 'count');
o = h.Values;
p = [];
for i = 1:7
p(i) = integral(@(x) exppdf(x, mean(quakes)), h.BinEdges
(i), h.BinEdges(i) + h.BinWidth);
end
e = p*length(quakes);
loglambda = 2*sum(o.*log(o./e))
loglambda =
5.9478
1 - chi2cdf(loglambda, 5)
ans =
0.3113
The null hypothesis that the data are exponential is accepted. However, the number of data
per bin is far from even. They will be adjusted to make that more true at the cost of fewer
bins.
edges = [0 75 200 400 600 1000 1960];
h = histogram(quakes, edges)
o = h.Values;
p = [];
for i = 1:6
0
500
1000
1500
2000
Time between Earthquakes
0
0.2
0.4
0.6
0.8
1
Cumulative Probability
Figure 7.1
The empirical (solid line) and exponential (dashed line) cdfs for the earthquake interval time data.
218
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

7.2.2 Pearson’s χ2 Test for Goodness-of-Fit
The Pearson (1900) goodness-of-ﬁt test differs from the multinomial likelihood ratio (LR)
test in that the former uses the asymptotic normal approximation rather than the exact form
of the multinomial distribution. This is one of the most widely used statistical tests in the
earth sciences and perhaps in science overall. The LR and Pearson tests are asymptotically
equivalent but will give different results for small samples. Cochran (1952) provides a
thorough introductory exposition on the chi square test that is as relevant today as it was
over sixty years ago and should be required reading for entering graduate students in
geophysics.
The Pearson test of goodness-of-ﬁt can be derived from (7.3) using a Taylor series
approximation. If the null hypothesis is true and the number of data is large, oi  ei. The
Taylor series about ei is
oi log
oi
ei
 
¼ oi  ei
ð
Þ þ 1
2ei
oi  ei
ð
Þ2 þ   
(7.4)
so that
2 log ^Λ  2
X
k
i¼1
oi  ei
ð
Þ þ
X
k
i¼1
oi  ei
ð
Þ2
ei
(7.5)
The ﬁrst term on the right is zero, and hence the LR test reduces to the Pearson test.
However, because it is an approximation, the Pearson test will typically be less accurate
than the multinomial LR test.
Consequently, the Pearson test for H0: πi ¼ pi,
i ¼ 1, :::, k, has the test statistic
^QN ¼
X
k
i¼1
yi  Npi
ð
Þ2
Npi
¼
X
k
i¼1
oi  ei
ð
Þ2
ei
(7.6)
p(i) = integral(@(x) exppdf(x, mean(quakes)), h.BinEdges
(i), h.BinEdges(i + 1));
end
e = p*length(quakes);
loglambda = 2*sum(o.*log(o./e))
loglambda =
3.3545
1 - chi2cdf(loglambda, 4)
ans =
0.5003
The conclusion is unchanged.
219
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

If the null hypothesis is true, then ^QN is asymptotically χ2
k1 for large samples. If m
parameters are estimated from the data, the degrees-of-freedom decrease to k  m  1. The
null hypothesis rejects when ^QN > κ, where Pr ^QN > κ


¼ α is chosen from the chi
square distribution.
While they are asymptotically equivalent, for ﬁnite numbers of data, the multinomial LR
test offers better performance if for no other reason than that fewer approximations and
assumptions are needed to derive it. It is typically more robust because the logarithm of a
ratio replaces the square of a difference. However, the Pearson test is far more widely used,
presumably because historically the use of logarithms required a slide rule or table lookup
rather than simple arithmetic. This is hardly a valid reason today, and the multinomial LR
test is recommended for standard use over the Pearson test.
Computing the power of either the multinomial LR or the Pearson test is complicated
because there are a large number of possible alternates and is typically not of much interest
because the tests are usually used to assess ﬁt in the absence of a well-deﬁned alternate.
Guenther (1977) gives an example of a power calculation for a single speciﬁc alternate
involving loaded die. The simple case where the class probabilities are completely speci-
ﬁed by the null hypothesis in the large sample limit was treated by Patnaik (1949). Power
computation becomes much more complicated when the class probabilities are estimated
from the data using, for example, maximum likelihood; see Mitra (1958) and Brofﬁtt &
Randles (1977) for elaboration.
Example 7.4 Returning to the Rutherford α decay data of Example 7.1, the test statistic is
q = sum((rutherford - n*p).^2./(n*p))
q =
21.4973
1 - chi2cdf(q, length(c) - 2)
ans =
0.0436
The null hypothesis that the data are Poisson is rejected.
However, as an illustration of the effect that class boundaries can have on the result,
lump the last four together and repeat the test.
c = [c(1:10) 10];
rutherford = [rutherford(1:10) 16];
param = sum(rutherford.*c)/sum(rutherford)
param =
3.8673
n = sum(rutherford);
p = poisspdf(c, param);
q = sum((rutherford - n*p).^2./(n*p))
q =
14.8620
220
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

1 - chi2cdf(q, length(c) - 2)
ans =
0.0948
The null hypothesis that the data are Poisson is accepted. A similar result is obtained when
the last six classes are combined.
Example 7.5 Returning to the V-1 ﬂying bomb data of Example 7.2
bomb = importdata('bomb.dat');
o = bomb(:, 2);
e = poisspdf(bomb(:,1), sum(bomb(:,1).*bomb(:,2))/sum(bomb
(:,2)))*sum(bomb(:,2));
q = sum((o - e).^2./e)
q =
1.0543
1 - chi2cdf(q, length(bomb) - 2)
ans =
0.9014
This result is consistent with the one obtained using the multinomial LR test. However, the
classes have a widely divergent numbers of hits. Merging the last four classes yields a more
even distribution of hits, although it reduces the degrees-of-freedom for the test to 1. The
result is a test statistic of 28.4423 and a p-value of 9.65  10–8. The null hypothesis that the
data are Poisson is rejected with this class structure. The problem with this data set is that it
was not structured for statistical analysis because the bin sizes are too small to give
adequate sampling.
In MATLAB, [h, p] = chi2gof(x) performs a Pearson chi square test of the data in x to a
normal distribution with mean and variance estimated from the data. If h = 1, the null
hypothesis that x is a random sample from a normal distribution can be rejected at the 5%
level, and p contains the p-value. For other distributions, the cdf must be provided as a
function handle, as shown by the following example.
Example 7.6 Apply the Pearson chi square test to the earthquake interval data in Example 7.3.
[h, p, stats] = chi2gof(quakes, 'cdf',{@expcdf, expﬁt(quakes)})
h =
0
p =
0.3206
chi2stat: 3.5014
221
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

df: 3
edges: [9.0000 198.2000 387.4000 576.6000 765.8000 1.9010e+03]
O: [21 14 8 10 9]
E: [22.5985 13.8407 8.9788 5.8248 10.7572]
The null hypothesis that the data are drawn from an exponential distribution with a mean
given by that of the data is accepted at the 0.05 level. However, the test only has three
degrees-of-freedom. The degrees-of-freedom can be increased by writing a MATLAB
script, as in Example 7.3.
7.2.3 Kolmogorov-Smirnov Test
This is not an exotic drink made from vodka and prune juice but rather a nonparametric test
for the ﬁt of a set of data to a speciﬁed distribution. Consider a random sample of size N
drawn from some unknown continuous distribution. An empirical cdf ^F N x
ð Þ can be
constructed from the order statistics, as shown in Section 4.8.2. This is essentially what
the MATLAB functions ecdf and histogram produce. The result is a monotonically
increasing set of step functions with step size 1=N assuming the data are unique and larger
steps if they are not. Recall from Section 4.8.2 that ^F N x
ð Þ !
p
F x
ð Þ, where F x
ð Þ is the
true
cdf
for
the
sample,
and
further,
by
the
Glivenko-Cantelli
theorem,
supx ^F N x
ð Þ  F x
ð Þ

 !
as
0.
Deﬁne the Kolmogorov-Smirnov (K-S) test statistic (Kolmogorov 1933; English trans-
lation is given in Kotz & Johnson 1992)
^DN ¼ supx ^F N x
ð Þ  F x
ð Þ


(7.7)
By the Glivenko-Cantelli theorem, lim N!∞^DN ¼ 0. The statistic ^DN is used to evaluate
the hypotheses
H0 : F x
ð Þ ¼ F∗x
ð Þ
H1 : F x
ð Þ 6¼ F∗x
ð Þ
(7.8)
The K-S test also can be applied to the two one-sided alternate hypotheses F x
ð Þ 
F∗x
ð Þ and F x
ð Þ  F∗x
ð Þ. The K-S statistic for these two alternates may be easily derived
from the order statistics as
^D
N ¼ max maxi F∗x ið Þ  i  1
ð
Þ=N


; 0


	

(7.9)
^Dþ
N ¼ max maxi i=N  F∗x ið Þ


; 0


	

(7.10)
where
^DN ¼ max ^D

N; ^Dþ
N


(7.11)
222
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

The distribution of ^DN when H0 is true is available in asymptotic form (Feller 1948,
1950)
lim
N!∞Pr
^DN 
zﬃﬃﬃﬃ
N
p


¼ 1  2
X
∞
i¼1
1
ð
Þi1e2i2z2
(7.12)
Two approximate critical values of the asymptotic distribution are c0:05 = 1.3581 and
c0:01 = 1.6726 for the two-sided alternate. These are reasonably accurate for N > 75.
Stephens (1970) gives approximations to the K-S statistic that are suitable for computation.
Another version of the K-S test can be used to compare two empirical cdfs (Smirnov
1939). Let
Xi
f
g and
Yj


be N and M independent rvs from two different populations,
respectively, and let their empirical cdfs be ^F N x
ð Þ and ^GM x
ð Þ. Testing the null hypothesis
that the population cdfs are the same against the alternate that they are not uses a K-S
statistic in the form
^DMN ¼ maxi
^F N ti
ð Þ  ^GM ti
ð Þ




(7.13)
where ti is an element of the merged rvs
Xi; Yj


. The asymptotic distribution is given by
(7.12) if N is replaced by MN= M þ N
ð
Þ under some continuity conditions (Feller 1948).
The K-S test offers some advantages over the multinomial LR test or Pearson test. The
K-S test does not depend on the target cdf or on data grouping, and it is typically more
powerful. However, there are a few caveats. First, the K-S test applies only to continuous
distributions. Second, the K-S test is relatively insensitive to the tails of the distribution, yet
it is in the tails that differences are typically largest (but also where outliers are a problem).
The K-S test is most sensitive near the distribution mode. Third, if the data are used to
estimate the distribution parameters, then the asymptotic distribution is not fully valid, and
the p-value will be biased. This is typically not an issue for large numbers of data but may
be for a limited sample size and can be overcome using the Lilliefors test (described later)
or resampling (as described in Section 8.2.5).
In MATLAB, the function [h, p, ksstat, cv] = kstest(x, cdf, alpha, type) rejects the null
hypothesis at the 0.05 level if h = 1, where x is a data vector, p is the asymptotic p-value,
and the target cdf is the standardized normal if cdf is not supplied. The parameters alpha
and type are the signiﬁcance level and the test type, respectively. The returned variables
ksstat and cv are the test statistic and the critical value.
Caution: Test rejection is based on a comparison of the asymptotic p-value with alpha.
It is strongly recommended that the actual critical value cv and the test statistic in ksstat be
compared as well.
The function kstest2(x1, x2) performs the two-sample K-S test to evaluate whether two data
samples are drawn from the same distribution. The remaining parameters are as for kstest.
A modiﬁcation of the K-S test is the Lilliefors test (Lilliefors 1967), which pertains to a
target distribution where the mean and variance are derived from the data. The disadvan-
tage of the Lilliefors test is that it depends on the target cdf, unlike the K-S test. In
MATLAB, this test is [h, p, kstat, cv] = lillietest(x, alpha, dist). The parameters are as for
kstest, but dist may only be “norm,” “exp,” or “ev,” where the latter refers to the extreme
value distribution.
223
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

Example 7.8 The K-S test will be applied to a set of random draws from a standardized
Gaussian distribution before and after it is contaminated with four outliers at the upper end
of the distribution.
Example 7.7 Estimate the ﬁt of the earthquake data of Example 7.3 to a Weibull distribution
using a two-sided K-S test.
This requires ﬁrst computing the parameters in the Weibull cdf
Weib xjα; c
ð
Þ ¼ 1  e x=α
ð
Þc
The mles for α and c are obtained by solving
^α ¼
1
N
X
N
i¼1
x^c
i
 
!1=^c
^c
¼
X
N
i¼1
x^c
i log xi
 
! X
N
i¼1
x^c
i
 
!1
 1
N
X
N
i¼1
log xi
2
4
3
5
1
The second equation may be easily solved iteratively for ^c using fzero with 1 as a starting
value, and then the ﬁrst equation may be solved for ^α. However, MATLAB makes this even
simpler because the wblﬁt function solves for the two mle parameters directly.
parmhat = wblﬁt(quakes)
parmhat =
450.2379
1.0798
quakes = sort(quakes);
cdf = [quakes' wblcdf(quakes', parmhat(1), parmhat(2))];
[h, p, ksstat, cv] = kstest(quakes, cdf)
h =
0
p =
0.9006
ksstat =
0.0700
cv =
0.1696
The null hypothesis that the data are drawn from a Weibull distribution with the computed
parameters is accepted at the 0.05 level, and the p-value is large. Figure 7.2 compares the
empirical and model cdfs and is substantially the same as Figure 7.1.
224
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

rng default;
data = normrnd(0, 1, 1000, 1);
[h, p, kstat, cv] = kstest(data)
h =
0
p =
0.3975
kstat =
0.0282
cv =
0.0428
data(1000) = 6.5;
data(999) = 6.0;
data(998) = 5.5;
data(997) = 5;
qqplot(data)
[h, p, kstat, cv] = kstest(data)
h =
0
p =
0.5415
kstat =
0.0252
cv =
0.0428
The K-S test result is almost unchanged when the outliers are present, and in fact, the
p-value has increased slightly. This illustrates the insensitivity of the K-S test in the tails of
the distribution. Figure 7.3 is a q-q plot that shows the four outliers that depart substantially
from a Gaussian expectation.
0
500
1000
1500
2000
Time between Earthquakes
0
0.2
0.4
0.6
0.8
1
Cumulative Probability
Figure 7.2
Empirical and Weibull cdfs for the earthquake interval data.
225
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

Because the absolute difference between the empirical and target cdf is measured, the K-S
statistic also provides a way to place conﬁdence limits on the entire cdf and on probability
plots. Regardless of what the true cdf F x
ð Þ is,
Pr ^DN ¼ supx ^F N x
ð Þ  F x
ð Þ

 > cαﬃﬃﬃﬃ
N
p


¼ α
(7.14)
which may be inverted to yield
Pr ^F N x
ð Þ  cαﬃﬃﬃﬃ
N
p
 F x
ð Þ  ^F N x
ð Þ þ cαﬃﬃﬃﬃ
N
p


¼ 1  α
(7.15)
There is a probability 1  α that the true cdf F x
ð Þ lies entirely within the band cα=
ﬃﬃﬃﬃ
N
p
around the empirical cdf ^F N x
ð Þ.
Example 7.9 Place a 95% conﬁdence band on the Weibull cdf obtained for the earthquake
data in Example 7.7.
Figure 7.4 shows the result. Bounds have been placed at 0 and 1 to keep the probabilities
meaningful.
–4
–2
0
Standard Normal Quantiles
–4
–2
0
2
2
4
4
6
8
Quantiles of Input Sample
Figure 7.3
Quantile-quantile plot for the contaminated Gaussian data.
0
500
1000
1500
2000
Time between Earthquakes
0
0.2
0.4
0.6
0.8
1
Cumulative Probability
Figure 7.4
The 95% conﬁdence interval for the earthquake interval data compared with the Weibull cdf.
226
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

cv = 1.3581;
plot(x,
wblcdf(x,
parmhat(1),
parmhat(2)),
x,
max(0,
f
-
cv/sqrt(length(quakes))), . . .
x, min(1, f + cv/sqrt(length(quakes))))
The K-S critical value also may be used to put a 1  α conﬁdence interval on p-p,
variance-stabilized p-p, and q-q plots (see Sections 4.8.6 and 4.8.7), as described
in Michael (1983). Let cα denote the critical value for the K-S test at probability
level α, and assume that the order statistics have been standardized to zero mean and
unit variance. Then the 1  α conﬁdence band is obtained by plotting the abscissa for
each probability plot type against (p-p plot) ui  cα, where
ui ¼ i  0:5
ð
Þ=N is a
uniform
quantile
(variance-stabilized
p-p
plot)
2 sin 1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
sin 2 ri
ð Þ  cα
p
h
i
=π,
where
ri ¼ 2 sin 1
ﬃﬃﬃﬃui
p


=π, and (q-q plot) F∗1 ui  cα
ð
Þ, where F∗x
ð Þ is the target cdf.
Example 7.10 Assess the ﬁt of the Gaussian distribution for the contaminated data of
Example 7.7, and place conﬁdence bounds on variance-stabilized p-p and q-q plots of
the data.
u = ((1:1000) - .5)/1000;
r = 2/pi*asin(sqrt(u));
plot(r, 2/pi*asin(sqrt(normcdf(sort(data), mean(data), std
(data)))), 'r+')
hold on
plot(r,2/pi*asin(sqrt(min(1,sin(pi*r/2).^2+cv))),r,2/pi*. . .
asin(sqrt(max(0, sin(pi*r/2).^2 - cv))))
hold off
x = norminv(u, mean(data), std(data));
plot(x, sort((data - mean(data))/std(data)), 'r+')
hold on
plot(x, norminv(u + cv, mean(data),std(data)), x, norminv
(u - cv, mean(data), std(data)))
The outliers appear as a slight downturn at the top of the distribution in the p-p plot of
Figure 7.5, and the conﬁdence band is too wide to enable detection of the outliers. The
outliers are much more apparent in the q-q plot of Figure 7.6, but the widening of the
conﬁdence band in the distribution tails prevents their direct detection. However, the slope
of the q-q plot, which gives the standard deviation of the data, has been torqued sufﬁciently
that the q-q plot crosses the lower conﬁdence bound at around 1 on the x-axis, and the null
hypothesis that the data are Gaussian would be rejected, or at least suspicion about its
validity would be raised.
227
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

7.2.4 Cramér–von Mises Tests
Cramér–von Mises tests provide a more general quadratic criterion for evaluating
goodness-of-ﬁt. Let ^F N x
ð Þ and F∗x
ð Þ be the empirical cdf for a set of N rvs and the target
distribution, respectively. The generalized Cramér–von Mises statistic is given by
^ω2 ¼
ð∞
∞
^F N x
ð Þ  F∗x
ð Þ
	

2w x
ð Þf ∗x
ð Þdx
(7.16)
where w x
ð Þ is a weight function. When w x
ð Þ ¼ 1, (7.16) is the Cramér–von Mises statistic,
for which the exact distribution is presented by Csörgő & Faraway (1996). When
w x
ð Þ ¼ F∗x
ð Þ 1  F∗x
ð Þ
½
	
f
g1, (7.16) is the Anderson-Darling (A-D) test ﬁrst proposed
by Anderson & Darling (1952, 1954), which is more powerful than the K-S test. The A-D
0
0.2
0.4
0.6
0.8
1
Sine Quantiles
0
0.2
0.4
0.6
0.8
1
Transformed Data
Figure 7.5
Variance-stabilized p-p plot for the contaminated Gaussian data of Example 7.7, along with the 95% conﬁdence
band derived from the K-S critical value.
–4
–2
Normal Quantiles
–4
–2
0
0
2
2
4
4
6
8
Standardized Data
Figure 7.6
Quantile-quantile plot for the contaminated Gaussian data of Example 7.7, along with the 95% conﬁdence band
derived from the K-S critical value.
228
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

test is most sensitive in the distribution tails but has the disadvantage that the critical values
depend on the target distribution. The critical values also depend on whether the distribu-
tion parameters are known or have been estimated because they are used in standardizing
the data.
The A-D test statistic is
^A
2 ¼ N  1
N
X
N
i¼1
2i  1
ð
Þ log F∗x ið Þ


þ log 1  F∗x Niþ1
ð
Þ


	



(7.17)
For the Gaussian distribution, Stephens (1976) gives asymptotic two-sided test critical
values at α = 0.05 (0.01) of 2.492 (3.857) when μ and σ2 are known, 1.088 (1.541) when
only σ2 is known, 2.315 (3.682) when only μ is known, and 0.751 (1.029) when both μ and
σ2 must be estimated, where the mles are used for the unknown parameters. For the
exponential distribution, the critical values are 1.326 (1.943) when the parameters are
unknown. Other critical points have been calculated and appear in the statistics literature.
MATLAB implements the A-D test as [h, p, adstat, cv] = adtest(x) and supports the
Gaussian, exponential, extreme value (Gumbel), lognormal, and Weibull distributions. It
defaults to the Gaussian, and alternate distributions may be selected using the keyword
value pair “Distribution” followed by “norm,” “exp,” “ev,” “logn,” or “weibull.” It also
allows control of the level of signiﬁcance and how the A-D critical value is calculated.
Example 7.11 For the earthquake data of Example 7.3, the A-D test for the null hypothesis
that the data are exponential against the alternate that they are not may be tested with the
following MATLAB script:
quakes = importdata('quakes.dat');
quakes = sort(quakes/mean(quakes));
n = length(quakes);
i = 1:n;
u1 = expcdf(quakes);
u2 = 1 - expcdf(quakes(n – i + 1));
a2 = -n - sum((2*i - 1).*(log(u1) + log(u2)))/n
a2 =
0.3666
This is much smaller than the critical value of 1.326 at the 0.05 level, and hence the null
hypothesis is accepted. Alternately, this can be done directly with the MATLAB function
[h, p, adstat, cv] = adtest(quakes, 'Distribution', 'exp')
h =
0
p =
0.7066
adstat =
0.3666
229
7.2 Goodness-of-Fit Tests
.008
13:35:01, subject to the Cambridge Core terms of use,

cv =
1.3130
The p-value is 0.7066, and hence the null hypothesis is strongly supported, and the data are
exponential.
Example 7.12 Returning to the contaminated Gaussian example of Example 7.7, the A-D
test is applied to the data after adding four outliers.
[h, p, adstat, cv] = adtest(data)
h =
1
p =
0.0027
adstat =
1.2665
cv =
0.7513
The test statistic is 1.27, which substantially exceeds the critical value, and the null
hypothesis is rejected. This illustrates the sensitivity of the A-D test to the distribution
tails, in contrast to the K-S test.
7.2.5 Jarque-Bera Test
The Jarque-Bera (J-B) or skewness-kurtosis score test for normality was introduced by
Jarque & Bera (1987). The test statistic is
^τ ¼ N
6
^s3
ð
Þ2 þ ^s4  3
ð
Þ2=4
h
i
(7.18)
and is a test of the joint null hypothesis that the skewness and the excess kurtosis are zero
against the alternate that they are not. For Gaussian data, the J-B statistic is asymptotically
chi square with two degrees-of-freedom and can be tested in the usual way, whereas
Monte-Carlo simulation can also be used to get more accurate p-values. MATLAB
implements the Jarque-Bera test as [h, p, jbstat, cv] = jbtest(x).
Example 7.13 Reanalyze the Gaussian simulation of Example 7.7 using the J-B test without
and with the four upper outliers.
rng default
data = normrnd(0, 1, 1000, 1);
230
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

[h, p] = jbtest(data)
h =
0
p =
0.0850
data(1000) = 6.5;
data(999) = 6.0;
data(998) = 5.5;
data(997) = 5;
[h, p] = jbtest(data)
Warning:
p
is
less
than
the
smallest
tabulated
value,
returning 0.001.
h =
1
p =
1.0000e-03
The test accepts the null hypothesis for the Gaussian random samples, although the p-value
of 0.0850 means that the support is weak. It easily detected the presence of the outliers.
7.3 Tests Based on Ranks
7.3.1 Properties of Ranks
Section 6.3 presented methods for testing single samples (e.g., the z test, single-sample t
test, and chi square test) and for comparing the properties of two samples (e.g., two-sample
t test, F test, and correlation coefﬁcient). There are a wide variety of nonparametric tests for
similar situations that replace the data with their ranks, yielding results that are invariant
under any monotone transformation of the data and moderating the inﬂuence of unusual
data because the most extreme values are pulled toward the center. However, this can be a
disadvantage at the same time because closely spaced observations are spread apart. An
excellent introduction to rank-based statistical tests that includes MATLAB code is given
by Kvam & Vidakovic (2007), and this section relies on that book. A more advanced
treatment of the theory is given by Hettmansperger & McKean (1998).
The ranks of an N-fold random sample Xi
f
g are easily obtained from its order statistics
rank X ið Þ


¼ i
(7.19)
or directly from the data values as
rank xi
ð Þ ¼
X
N
j¼i
1 xi  xj


(7.20)
231
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

Assuming that a random sample is exchangeable, it follows that Xi
f
g !
d
Xςi


, where ςi
represents a ﬁnite permutation of the indices. As a result,
Pr rank X ið Þ


¼ i
	

¼ 1
N
(7.21)
meaning that the ranks are distributed as discrete uniform rvs. Let ϒi ¼ rank X ið Þ


denote
the ranks of the rvs
Xi
f
g. It follows that
E ϒi
ð
Þ ¼
X
N
i¼1
i
N ¼ N þ 1
2
var ϒi
ð
Þ ¼
X
N
i¼1
i2
N  N þ 1
ð
Þ2
4
¼ N2  1
12
(7.22)
MATLAB supports ranks through the function r = tiedranks(x). This returns the ranks
in r, and if any values in x are tied, it gives their average rank.
7.3.2 Sign Test
The sign test is a simple nonparametric procedure that pertains to continuous distributions
for the null hypothesis H0: ~μ ¼ ~μ∗, where ~μ is the population median, and ~μ∗is a speciﬁed
value, against either one- or two-sided alternates. It substantially predates most of statistical
theory, originating in Arbuthnott (1710), who used a sign test to show that male births
exceeded female births for most of the prior century. For a random sample
Xi
f
g, assign
different categorical variables (such as + or ) when xi > ~μ∗and when xi < ~μ∗. If the null
hypothesis holds, then Pr Xi > ~μ∗
ð
Þ ¼ Pr Xi < ~μ∗
ð
Þ ¼ 1=2 by the deﬁnition of the median.
Let the test statistic ^Ξ denote the total number of samples where xi > ~μ∗given by
^Ξ ¼
X
N
i¼1
1 xi > ~μ∗
ð
Þ
(7.23)
Under the null hypothesis, the test statistic is a binomial variable with probability 1/2, or
^Ξebin N; 1=2
ð
Þ.
At a signiﬁcance level α, if the test is upper tail or lower tail, then the critical values are
given, respectively, by integers that are larger than or equal to ξα or smaller than or equal to
ξ0
α. These are given, respectively, by the smallest or largest integer for which
Pr ^Ξ  ξαjH0


¼ 1
2N
X
N
i¼ξa
N
i


< α
(7.24)
Pr ^Ξ  ξ0
αjH0


¼ 1
2N
X
ξ0
α
i¼0
N
i


< α
(7.25)
If the test is two tailed, then the critical values are integers either less than or equal to ξ0
α=2
or greater than or equal to ξα=2 and may be obtained as the union of (7.24) and (7.25) after
replacing α with α/2.
232
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

Similarly, for an observed value of the test statistic ^Ξ, the p-values for an upper- or
lower-tail test are given, respectively, by
p ¼ 1
2N
X
N
i¼^Ξ
N
i


¼ 1
2N
X
N^Ξ
i¼0
N
i


(7.26)
and
p ¼ 1
2N
X
^Ξ
i¼0
N
i


(7.27)
The two-tailed p-value follows by taking ^Ξ0 ¼ min ^Ξ; N  ^Ξ


and computing
p ¼
1
2N1
X
^Ξ0
i¼0
N
i


(7.28)
MATLAB implements the two-sided sign test by default as the function [p, h, stats] =
signtest(x, mustar), where p is the p-value, h is a logical variable (1 for rejection and 0 for
acceptance), and stats is a structure that contains details about the test. Note that MATLAB
reverses the order of the p-value and accept/reject ﬂag for the nonparametric tests com-
pared with the parametric tests, which is an unnecessary source of confusion. A tail
probability of 0.05 is assumed. The variable mustar is the postulated median that defaults
to zero if omitted. One-sided versions of the test may be computed using the keyword “tail”
with either the value “left” or “right.”
Example 7.14 Returning to Example 6.8, the sign test for the median (rather than the mean)
number of days an oceanographer spends at sea per year being 30 is
x = [54 55 60
42 48 62 24 46 48 28 18 8 0 10 60 82 90 88 2 54];
[p, h, stats] = signtest(x, 30)
p =
0.2632
h =
0
stats =
zval: NaN
sign: 13
The null hypothesis that the median number of days a scientist spends at sea per year is
30 is accepted, in contrast to the t test on the mean value. This result uses the default exact
statistical distribution that returns a z-value of NaN due to the small size of the sample. The
value of the test statistic is given by stats.sign. The sign test also accepts both single-sided
versions of the test, in contrast to the t test in Example 6.8, which rejects for the upper-
tail test.
233
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

In some experimental designs (especially in the biomedical ﬁelds), the data are paired.
For example, subjects might be grouped according to their age or weight or medical
condition and then randomly assigned to test and control groups. The pairing causes the
samples to be dependent. However, the sign test can be applied in this situation, in which
case the null hypothesis is that the median of the difference between the two groups is zero,
which is not the same as stating that their medians are identical.
Example 7.15 A data set consisting of the remission times for 42 leukemia patients is
contained in the ﬁle remiss.dat and is taken from Hand et al. (1994). The patients were
divided into two equal-sized groups: a test group that was treated with a new drug and a
control group that was not, with the latter contained in the ﬁrst 21 rows of the data. Test the
null hypothesis that the median value of the differences between the test and control groups
is zero.
remiss = importdata('remiss.dat');
x = remiss(1:21, 2);
y = remiss(22:42, 2);
[p, h, stats] = signtest(x, y)
p =
0.0015
h =
1
stats =
zval: NaN
sign: 3
The null hypothesis is strongly rejected. Figure 7.7 shows the two data sets against patient
number, clearly indicating that the test group typically is in remission for much longer than
the control group.
10
15
20
Patient Number
00
5
10
20
30
Remission Time
Figure 7.7
The remission time for the test group (x) and control group (ﬁlled circles) in Example 7.15.
234
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

7.3.3 Signed Rank Test
A signiﬁcant limitation of the sign test is that only the sign of the difference between a
sample and the postulated median enters it. This led Wilcoxon (1945) to suggest that the
absolute difference should be included with the sign in the test, and that turned into the test
that bears his name. Let ^Di ¼ xi  ~μ∗denote the difference between a speciﬁc element in
the random sample and a postulated value for the median ~μ∗. The key assumption behind
the Wilcoxon signed rank test is that the
^Di


are symmetric about zero if the null
hypothesis is true, so the probability of observing a positive or negative Di is about the
same. This is equivalent to assuming that the underlying distribution is symmetric about its
median. Hodges & Lehmann (1956) showed that the efﬁciency of the signed rank test is
3/π  0.95 for Gaussian data and never lower than 0.864 for any location distribution
alternate. Consequently, the penalty for using this nonparametric test over a parametric test
is not large. Further, including the absolute difference in the test raises the efﬁciency over
that of the sign test.
The Wilcoxon signed rank test statistic is calculated by:
1. Ranking the absolute values of the differences ^Di

;
2. Restoring the signs of the differences to the ranks, yielding the signed ranks; and
3. Calculating the sum of those ranks that have positive signs.
In effect, the ranked absolute differences are added to the sign test as weights.
Under the null hypothesis H0: ~μ ¼ ~μ∗, the expected value of the sum of the positive and
negative differences should be the same. Following on (7.23), the test statistics are
Example 7.16 Returning to Gossett’s wheat data from Example 6.9, use the sign test to
evaluate the null hypothesis that the difference in yield between air- and kiln-dried wheat
seeds has a median value of zero.
gossett = importdata('gossett.dat');
[p, h, stats] = signtest(gossett(:, 1), gossett(:, 2))
p =
0.5488
h =
0
stats =
zval: NaN
sign: 4
The null hypothesis is accepted at the 0.05 level, which also pertains to the t test in
Example 6.9.
235
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

^Ξ
þ ¼
X
N
i¼1
rank
^Di




1 ^Di > 0


(7.29)
^Ξ
 ¼
X
N
i¼1
rank
^Di




1  1 ^Di > 0


	

(7.30)
Under the null hypothesis, the signs
1 ^Di > 0




are iid Bernoulli variables with prob-
ability ½ and are independent of the
^Di




. Consequently, the expected value of ^Ξ
þ is
N N þ 1
ð
Þ=4, and its quantiles can be computed, and similarly for ^Ξ
. These results are
exact and are pertinent for small samples.
It follows from (7.29) to (7.30) that
^Ξ
þ þ ^Ξ
 ¼
X
N
i¼1
i ¼ N N þ 1
ð
Þ
2
(7.31)
and hence
^Ξ ¼ ^Ξ
þ  ^Ξ
 ¼ 2
X
N
i¼1
rank
^Di




1 ^Di > 0


 N N þ 1
ð
Þ
2
(7.32)
For large samples, a Gaussian limiting form for ^Ξ is appropriate, and hence the usual
Gaussian quantiles pertain.
MATLAB contains the function [p, h, stats] = signrank(x, mustar), which performs a
two-sided test of the hypothesis that the sample in x comes from a distribution whose median
is mustar. The structure stats contains the signed rank statistic. The function signrank uses
the exact distribution or the normal approximation depending on the size of the sample.
Example 7.17 Returning to Examples 6.8 and 7.14, use the Wilcoxon signed rank test to
evaluate the null hypothesis that the number of days per year that an oceanographer spends
at sea is 30.
x = [54 55 60
42 48 62 24 46 48 28 18 8 0 10 60 82 90 88 2 54];
[p, h, stats] = signrank(x, 30)
p =
0.0457
h =
0
stats =
zval: 1.9985
signedrank: 158.5
The null hypothesis is weakly rejected at the 0.05 level, as was also the case for the t test,
but not for the sign test. However, an upper-tail test fails weakly (p-value of 0.0239), and a
lower-tail test accepts (p-value of 0.9782), which is very similar to the t test result in
Example 6.8.
236
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

The Wilcoxon signed rank test also can be applied to the differences between two paired
samples, as shown by Wilcoxon (1946). In this case, the test determines whether the
median of the difference is zero and not whether their medians are the same.
Example 7.18 Returning to the leukemia data from Example 7.15, test the null hypothesis
that the median of the difference between the test and control groups is zero.
remiss = importdata('remiss.dat');
x = remiss(1:21, 2);
y = remiss(22:42, 2);
[p, h, stats] = signrank(x, y)
p =
0.0051
h =
1
stats =
zval: -2.8018
signedrank: 35
The null hypothesis is rejected at the 0.05 level, as it was for the sign test.
Example 7.19 Returning to Student’s wheat data from Examples 6.9 and 7.16, use the sign
rank test to evaluate the null hypothesis that the difference in yield between air- and kiln-
dried wheat seeds has a median value of zero.
gossett = importdata('gossett.dat');
[p, h, stats] = signrank(gossett(:, 1), gossett(:, 2))
p =
0.1230
h =
0
stats =
signedrank: 15
The null hypothesis is accepted. Note that the exact distribution is used for this small
sample, whereas the asymptotic normal approximation was used in Examples 7.17 and
7.18. The keyword value pair “method,” “exact” could have been used to override the
defaults for those two examples.
7.3.4 Rank Sum Test
The Wilcoxon (1945) rank sum test (which was introduced along with the signed rank test
in a single paper) and the Mann & Whitney (1947) U test are equivalent, although they
237
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

were derived from different assumptions and were long thought to be distinct. They are
sometimes known as the Wilcoxon-Mann-Whitney test, but the term rank sum test is
simpler. They are the nonparametric equivalent of the two-sample t test, but they do not
require the assumption of a Gaussian population. The rank sum test also can be used with
ordinal data, in contrast to the t test. Two key distinctions between the rank sum and the
sign or signed rank tests are that the former directly tests whether the medians of two data
sets are the same rather than testing the median of the difference, and there is no pairing
requirement for the rank sum test, so different sized samples can be accommodated.
Lehmann (1953) investigated the power of two-sample rank tests, which is a complex
subject unless speciﬁc alternates are considered.
Suppose that Xi
f
g are N samples from a distribution f x
ð Þ and Yj


are M samples from a
distribution g y
ð Þ, and further suppose that
Xi
f
g and
Yj


are independent. The null
hypothesis is that the two distributions are identical against a suitable alternate. A test statistic
can be obtained by grouping all M þ N observations together, ranking them and then
calculating the sum of the ranks of those samples that actually come from Xi
f
g, yielding
^Ω ¼
X
MþN
i¼1
i 1 Xi
ð
Þ
(7.33)
Equation (7.33) is the Wilcoxon rank sum statistic. Its exact distribution can be computed
through combinatoric arguments that are not simple. A Gaussian approximation can be
used for large samples. Additional corrections are needed in the event of ties in the ranks,
but the details will be omitted.
An alternate test statistic can be estimated by comparing all N values in Xi
f
g to all M values
in
Yj


and then computing the proportion of the comparison for which xi > yj, giving
^U ¼
X
N
i¼1
X
M
j¼1
1 Xi > Yj


(7.34)
Equation (7.34) is the Mann-Whitney test statistic. Its exact distribution is the same as for
the Wilcoxon rank sum statistic.
MATLAB provides the function [p, h, stats] = ranksum(x, y) that takes the original data
in the vectors x and y, computes the test statistic, and returns the two-sided p-value in p, a
0–1 value indicating acceptance or rejection in h and a statistical structure in stats.
Additional keyword value pairs can be added to change the test type and the method for
computing the p-value. The function ranksum uses the exact distribution rather than the
normal approximation for a small sample.
Example 7.20 Return to the Michelson speed of light data from Example 6.10.
michelson1 = importdata('michelson1.dat');
michelson2 = importdata('michelson2.dat');
[p, h, stats] = ranksum(michelson1, michelson2)
p =
4.4407e-05
238
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

h =
1
stats =
zval: 4.0833
ranksum: 6.8955e+03
The null hypothesis that the medians of the two populations are the same is rejected,
and the speed of light has evidently changed between 1879 and 1882 or else systematic
errors that were not accounted for are present. This result was computed using the
normal approximation to the distribution of the test statistic. The exact p-value can be
obtained, although this takes several minutes of computation
[p,h,stats]=ranksum(michelson1,michelson2,'method','exact')
p =
2.3681e-05
h =
1
stats =
ranksum: 6.8955e+03
The conclusionis unchangedand is consistentwith thetwo-sample t test result in Example6.10.
Example 7.21: Returning to the cloud seeding data from Example 6.12, compute the p-value
for the rank sum test to assess the null hypothesis that the medians of the rainfall from
unseeded and seeded clouds are the same.
clouds = importdata('cloudseed.dat');
[p, h] = ranksum(clouds(:, 1), clouds(:, 2))
p =
0.0138
h =
1
The null hypothesis is rejected. By contrast, the two-sample t test in Example 6.12 weakly
accepted the null hypothesis that the means of unseeded and seeded clouds were the same.
Carrying out the rank sum test on the logs of the data
[p, h] = ranksum(log(clouds(:, 1)), log(clouds(:, 2)))
p =
0.0138
h =
1
The p-value is unchanged from that with the original data because the ranks are not
affected by a monotone transformation such as the logarithm. The p-value is very close
239
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

to the two-sample t test result on the logs of the data. This example illustrates the
robustness to departures from normality for rank-based tests.
7.3.5 Ansari-Bradley Test
This test was proposed by Ansari & Bradley (1960) and is a two-sample nonparametric test
of dispersion for continuous populations having the same median and shape. The alternate
hypothesis holds that the two populations have the same median and shape but different
dispersions. Let two sets of rvs
Xi; Yi
f
g have M and N values and be drawn from
distributions with cdfs F and G, respectively. Under the null hypothesis, the two distribu-
tions differ at most by a scale factor on the argument, so F u
ð Þ ¼ G λu
ð
Þ. The Ansari-
Bradley (A-B) test evaluates H0: λ = 1 versus H1: λ 6¼ 1, or one-tailed alternates, based
on ranks.
The two data sets are merged and sorted into ascending values, and ranks are assigned
from both ends working toward the center, beginning with unity and incrementing by one
each time, so that for M þ N an even number, the array of ranks is
1, 2, :::, M þ N
ð
Þ=2, M þ N
ð
Þ=2, :::, 2, 1
(7.35)
whereas if the number of data is odd, the center value is unique. The test statistic is the sum
of the ranks associated with the x data and can be written as
^w ¼
X
MþNþ1
ð
Þ=2
i¼1
i 1 Xi
ð
Þ þ
X
MþN
i¼ MþNþ1
ð
Þ=2
M þ N þ 1  i
ð
Þ 1 Xi
ð
Þ
(7.36)
Small values of the test statistic indicate a larger dispersion for the x data, and large values
indicate the reverse. The null distribution can be determined exactly based on combinatoric
arguments and is asymptotically Gaussian.
MATLAB implements the A-B test as [h, p, stats] = ansaribradley(x, y) (note that the
order of p and h is reversed compared with all other nonparametric tests) and defaults to a
two-tailed test, with the computation method for the null distribution determined from the
number of data. One-tailed tests and the computation method can be controlled in the usual
way. The exact test can take considerable computation with a large number of data.
Example 7.22 Returning to the Michelson speed of light data from Examples 6.10 and 6.15,
test the null hypothesis that the dispersion of the data sets from different years is the same.
[h, p] = ansaribradley(michelson1, michelson2)
h =
1
p =
0.0109
240
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

The test rejects the null hypothesis. However, the medians are 850 and 776 for the
1879 and 1882 data. After adjusting so that the medians are identical, the result is
michelson1 = michelson1 - median(michelson1);
michelson2 = michelson2 - median(michelson2);
[h, p] = ansaribradley(michelson1, michelson2)
h =
0
p =
0.5764
The test now accepts the null hypothesis with a p-value of 0.58, which is no evidence for the
alternate hypothesis. This should be contrasted with the F test in Section 6.3.4 that rejected
the null hypothesis and the Bartlett’s M test result of Section 6.3.5 that barely accepted it.
7.3.6 Spearman Rank Correlation Test
Spearman (1904) proposed the rank correlation statistic for bivariate data that bears his
name long before most of the other work in nonparametric statistics was published in the
1940s and beyond. Suppose that
Xi
f
g and
Yi
f
g are N random samples, and let
ri
f g and
si
f g denote their ranks obtained from (7.20). The Spearman correlation coefﬁcient is just
the Pearson correlation coefﬁcient from Section 6.3.6 computed using the ranks:
^rs ¼
P
N
i¼1
ri  r
ð
Þ si  s
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
ri  r
ð
Þ2 P
N
i¼1
si  s
ð
Þ2
s
(7.37)
This expression can be simpliﬁed considerably by recognizing that r ¼ s ¼ N þ 1
ð
Þ=2 and
that the two terms in the denominator are each equal to N N 2  1


=12. Parameterizing the
expression in terms of the differences between the ranks ^Di ¼ ri  si yields
^rs ¼ 1 
6 P
N
i¼1
^D
2
i
N N 2  1


(7.38)
The exact permutation distribution for the Spearman statistic has been derived and can be
approximated as a Gaussian for large data sets. These are used to test the null hypothesis
that the data are uncorrelated against the alternate hypothesis that they are not
MATLAB implements the Spearman correlation coefﬁcient within the function corr that
also provides the Pearson correlation coefﬁcient, although this requires the input of
additional parameters. The command [rho, p] = corr(x, y, ‘type’, ‘spearman’) returns the
correlation coefﬁcient rho and p-value p given data in x and y.
241
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

7.3.7 Kendall’s Tau
This statistic uses the probability of concordance and discordance:
ρc ¼ Pr
Xi  Xj


Yi  Yj


> 0
	

ρd ¼ Pr
Xi  Xj


Yi  Yj


< 0
	

(7.39)
to deﬁne the Kendall τ statistic
τ ¼ ρc  ρd
(7.40)
When the marginal distributions of X and Y are continuous, then ρc þ ρd ¼ 1, and hence
τ ¼ 2ρc  1 ¼ 1  2ρd, so
ρc
ρd
¼ 1 þ τ
1  τ
(7.41)
To apply the Kendall τ estimator to data, it is necessary to use estimates for the
probability of concordance and discordance based on relative frequency. For N data
xi; yi
f
g, deﬁne
^υij ¼ sgn xi  xj


^ωij ¼ sgn yi  yj


^αij ¼ ^υij ^ωij
(7.42)
where sgn x
ð Þ is 1, 0, 1 as x is negative, zero, or positive. It is obvious that
Pr ^αij ¼ 1


¼ ρc and Pr ^αij ¼ 1


¼ ρd, and hence E ^αij


¼ τ. An estimator for τ is
Example 7.23 Returning to the tobacco and alcohol consumption data from Example 6.17:
alctobacc = importdata('alctobacc.dat');
[rhat,p]=corr(alctobacc(:,1),alctobacc(:,2),'type','spearman')
rhat =
0.3727
p =
0.2606
The null hypothesis that the data are uncorrelated is accepted at the 0.05 level. For
comparison, the results in Example 6.17 were ^r = 0.2236 and a p-value of 0.5087. The
effect of the Northern Ireland outlier is still present, although the correlation has risen and
the p-value has fallen. After removing the Northern Ireland data point, the correlation rises
to 0.8303 and the p-value drops to 0.0056, resulting in rejection of the null hypothesis, just
as happened in Example 6.17.
242
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

^τN ¼
1
N
2


X
N
i¼1
X
N
j¼iþ1
^αij
(7.43)
and is unbiased. Equation (7.43) is Kendall’s sample τ. It can easily be shown that (7.43) is
equivalent to
^τN ¼
P
N
i¼1
P
N
j¼1
^υij ^ωij
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
P
N
j¼1
^υ2
ij
 
!
P
N
i¼1
P
N
j¼1
^ω2
ij
 
!
v
u
u
t
(7.44)
which is the Pearson correlation coefﬁcient with the data replaced by signed differences.
A test of the null hypothesis that the two sets of rvs have zero correlation follows from
the fact that τ ¼ 0 in that instance, and hence the test should reject for large values of
^τN
j
j. The null distribution can be derived based on combinatorics and is asymptotically
Gaussian.
MATLAB implements the Kendall τ within the corr function as a different value for the
keyword “type.”
Example 7.24 Returning to the tobacco and alcohol consumption data from Examples 6.17
and 7.23:
alctobacc = importdata('alctobacc.dat');
[rho,p]=corr(alctobacc(:,1),alctobacc(:,2),'type','kendall')
rho =
0.3455
p =
0.1646
This is comparable to the result using the Spearman statistic in Example 7.23. The
Northern Ireland outlier is still producing zero correlation. After removing that data point,
the correlation rises to 0.6444 and the p-value is 0.0091, resulting in rejection of the null
hypothesis that the data are uncorrelated.
7.3.8 Nonparametric ANOVA
Conventional ANOVA (Section 6.3.7) will fail in the presence of heteroskedasticity or
non-Gaussian variates, just as the t test or (especially) the F test breaks down under
these conditions. Nonparametric multivariate tests have been developed to address
these problems. For one-way ANOVA, the Kruskal-Wallis test requires only that the
samples be independent and that their underlying distributions be identically shaped
243
7.3 Tests Based on Ranks
.008
13:35:01, subject to the Cambridge Core terms of use,

and
scaled.
The
hypotheses
tested
by
the
Kruskal-Wallis
test
are
H0:
~μ1 ¼ ~μ2 ¼    ¼ ~μM, where ~μi is the population median of the ith group, versus H1:
not H0, meaning that at least two of the medians are not the same. The Kruskal-Wallis
test procedure is:
1. Rank all the data, disregarding their grouping and assigning the average of the ranks
that would have been received to any tied values.
2. Form the test statistic
^K ¼ N  1
ð
Þ
P
M
i¼1
ni ri  r
ð
Þ2
P
M
i¼1
P
ni
j¼1
rij  r

2
(7.45)
where rij is the global rank of the jth observation from the ith group, r ¼ N þ 1
ð
Þ=2 and
ri ¼ 1=ni
ð
ÞPni
j¼1rij.
3. Since the denominator of (7.45) is N  1
ð
ÞN N þ 1
ð
Þ=12, the test statistic reduces to
^K ¼
12
N N þ 1
ð
Þ
X
M
i¼1
niri  3 N þ 1
ð
Þ
(7.46)
4. The test statistic is corrected for ties by dividing (7.46) by 1  PT
i¼1 t3
i  ti


=
N N 2  1


, where T is the number of groupings of different tied ranks, and ti is the
number of tied data with a particular value in the ith group. The correction is unimport-
ant unless the number of ties is very large.
5. Assess the test statistic against the Kruskal-Wallis distribution or, for a sufﬁciently large
sample, the chi square approximation χ2
M1 α
ð Þ, rejecting if the test statistic is larger than
the critical value (or computing the chi square p-value in the usual way).
MATLAB supports the Kruskal-Wallis test through the function [p, anovatab] = krus-
kalwallis(x), where the argument and returned values are identical to those for anova1.
Example 7.25 Return to the laboratory tablet data in Example 6.17.
[p, anovatab] = kruskalwallis(tablet)
The anova table is
Source
SS
DF
MS
chi-sq
Between
14756.4
6
2459.4
35.76
Within
13713.6
63
217.68
Total
28470
69
The p-value is 3.06  10–6, so the null hypothesis that the medians are the same is rejected
at the 0.05 level. This is the same conclusion obtained with ANOVA in Example 6.17.
244
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

7.4 Meta-analysis
Meta-analysis provides ways to combine the inferences from a number of studies con-
ducted under similar conditions. This is a common procedure in biomedical ﬁelds, where
p-values are always reported from trials, and might prove useful in the earth sciences once
reporting of p-values becomes more routine. The key remaining issue is the phrase “under
similar conditions,” which has to be approached with caution.
While parametric meta-analysis methods exist, these typically require additional infor-
mation that is not usually provided in the scientiﬁc literature. In many instances, all that is
available are the p-values from individual studies, in which case nonparametric methods
must be applied. Birnbaum (1954) summarizes the issues.
The simplest technique for combining p-values is the inverse chi square method of
Fisher (1932). Under the null hypothesis, the p-value is uniformly distributed on [0, 1] so
that 2 log pieχ2
2. By the additivity property of the chi square distribution, it follows for m
p-values that
2
X
m
i¼1
log pieχ2
2m
(7.47)
and hence the combined p-value satisﬁes
p ¼ 1  Chi 2
X
m
i¼1
log pi; 2m
 
!
(7.48)
The combined p-value can be assessed in standard ways.
An alternative approach uses the probability integral transformation of Section 4.8.1.
Implementing this for the Gaussian distribution, note that if
zi
f g are m standardized
Gaussian variables, then Pm
i¼1zi=
ﬃﬃﬃﬃm
p
is also N 0; 1
ð
Þ. Consequently, the combined p-value
can be computed as
p ¼ 1  Φ
1ﬃﬃﬃﬃm
p
X
m
i¼1
Φ1 1  pi
ð
Þ
"
#
(7.49)
This method is most suitable for upper-tail tests.
Example 7.26 Professors A and B are independently trying to evaluate a theory for
which they have formulated a hypothesis test based on Bernoulli trials. The hypotheses
to be tested are H0: p ¼ 0:25 versus H1: p > 0:25. Professor A teaches large classes
each semester and uses the attendees to conduct his experiments. He carries out two
trials and obtains p-values of 0.04 and 0.009. Professor B teaches small classes each
quarter and carries out ten experiments, yielding p-values of 0.22, 0.15, 0.23, 0.17,
0.20, 0.18, 0.14, 0.08, 0.31, and 0.21. None of Professor B’s experiments rejected the
null hypothesis at the 0.05 level, whereas both of Professor A’s do. However, the
245
7.4 Meta-analysis
.008
13:35:01, subject to the Cambridge Core terms of use,

combined p-values must be computed to compare their respective outcomes. This
example is taken from Kvam & Vidakovic (2007, page 108).
pvala = [.04 .009];
pvalb = [.22 .15 .23 .17 .20 .18 .14 .08 .31 .21];
1 - chi2cdf(-2*sum(log(pvala)), 4)
ans =
0.0032
1 - normcdf(sum(norminv(1 - pvala))/sqrt(2))
ans =
0.0018
1 - chi2cdf(-2*sum(log(pvalb)),20)
ans =
0.0235
1 - normcdf(sum(norminv(1 - pvalb))/sqrt(10))
ans =
0.0021
Despite the fact that none of her individual experiments rejected the null hypothesis, the
combination of Professor B’s ten experimental p-values does so at the 0.05 level. By
contrast, Professor A’s raw and combined p-values both reject the null hypothesis.
246
Nonparametric Methods
.008
13:35:01, subject to the Cambridge Core terms of use,

8
Resampling Methods
8.1 Overview
A variety of parametric estimators for the mean and variance (among other statistics) for iid
data and for testing hypotheses about them have been established in Chapters 4 through 6.
However, in many instances the data analyst will not have information on or want to make
assumptions about the data distribution and hence may not be certain of the applicability
of parametric methods. In the real world, it is often true that departures from expected
conditions occur, including:
• Mixtures of distributions,
• Outliers,
• Complex distributions for realistic problems (e.g., multivariate forms),
• Noncentrality, and
• Unknown degrees-of-freedom due to correlation and heteroskedasticity
This has led to the development of estimators that are based on resampling of the
available data that are covered in this chapter. The most widely used of these is the
bootstrap described in the next section, which was ﬁrst proposed by Efron (1979) and
has undergone considerable evolution since that time. In recent years, permutation hypoth-
esis tests that typically are superior to their bootstrap analogue have become more widely
used and are described in Section 8.3. A linear approximation to the bootstrap called the
jackknife is described in Section 8.4 and offers computational efﬁciency at a slight cost in
accuracy, but it is becoming less important as the power of computers grows.
8.2 The Bootstrap
8.2.1 The Bootstrap Distribution
Instead of assuming a particular parametric form for the sampling distribution, such as
Student’s t, the empirical distribution based on the available sample plays the central role in
the bootstrap method. The bootstrap is based on sampling with replacement, and as a
result, the bootstrap is typically not exact. Davison & Hinkley (1997) and Efron &
Tibshirani (1998) are very readable treatises on the bootstrap.
247
.009
13:32:30, subject to the Cambridge Core terms of use,

Example 8.1 The ﬁle geyser.dat contains 299 measurements of the waiting time in minutes
between eruptions of Old Faithful at Yellowstone National Park and was used to illustrate
the maximum likelihood estimator (mle) in Example 5.27. A kernel density estimator for
the data is shown in Figure 5.2. Characterize the empirical pdf of the data, and then
resample from it with replacement 30, 100, 300, 1000, 3000, and 10,000 times. Finally,
compute the sample mean for each replicate. Plot the resulting bootstrap distributions for
the sample mean. Repeat for the kurtosis.
Figure 5.2 shows that the data are bimodal, peaking at around 55 and 80 minutes.
Figure 8.1 shows a Gaussian q-q plot for the data, indicating that they are shorter tailed
than Gaussian data at the top and bottom of the distribution but with an excess of
probability near the distribution mode.
n = length(geyser);
b = [30 100 300 1000 3000 10000];
bins = [10 20 30 30 50 50];
rng default %initialize random number seed so result will
be reproducible
for i=1:6
ind = unidrnd(n, n, b(i)); %use unidrnd to get indices to
sample the data with replacement
boot = geyser(ind); %resample the data
meanb = mean(boot);
mean(meanb)
subplot(3, 2, i)
histogram(meanb, bins(i), 'Normalization', 'pdf')
end
ans =
72.5831
ans =
72.2098
ans =
72.3521
ans =
72.2766
ans =
72.3384
ans =
72.3201
For comparison purposes, the sample mean of the geyser data is 72.3144. Figure 8.2
shows the results. For a limited number of bootstrap replicates (in this case, under 300), the
resulting distribution does not resemble the N μ; σ2=N
ð
Þ sampling distribution of the mean.
As the number of replicates increases to 10,000, the bootstrap distribution looks increas-
ingly Gaussian and, in addition, is consistent as the spread of the distribution drops.
248
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

However, the bootstrap distribution is centered on the sample mean rather than the
unknown population mean.
Figure 8.3 shows the bootstrap distributions for the kurtosis of the Old Faithful data
whose parametric sampling distribution for Gaussian data is known but complicated.
However, Figure 8.1 shows that the geyser data are far from Gaussian. For reference, the
sample kurtosis of the geyser data is 1.9916. As with the bootstrap distributions for the
mean, the result is far from well deﬁned when the number of bootstrap replicates is small
but is increasingly clear as the number of replicates rises beyond 1000 or so. The result
appears to be slightly asymmetric, with a longer right than left tail.
It might seem as though something illegitimate is going on because resampling appears
to create data out of nothing. However, the resampled data are not used as if they are new
70
71
72
73
74
75
0
0.5
70
71
72
73
74
75
0
0.5
70
71
72
73
74
75
0
0.5
70
71
72
73
74
75
0
0.5
70
71
72
73
74
75
0
0.5
70
71
72
73
74
75
0
0.5
Figure 8.2
Bootstrap distributions for the sample mean of the Old Faithful geyser data computed using (top row) 30 and 100,
(middle row) 300 and 1000, and (bottom row) 3000 and 10,000 bootstrap replicates from the data.
–3
–2
–1
0
1
2
3
Standard Normal Quantiles
0
20
40
60
80
100
120
140
Quantiles of Input Sample
Figure 8.1
Gaussian q-q plot for the Old Faithful geyser data.
249
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

data. Rather, the bootstrap distribution of, for example, the sample mean is only used to
determine how the sample mean of the actual data varies through random sampling. It is
quite reasonable to use the same random sample to estimate a parameter such as the sample
mean and also to compute its variability. Indeed, this is what is done parametrically when
the sample mean is used to determine the population mean and the sample variance is used
to determine the dispersion about the sample mean. The difference with the bootstrap is
that the bootstrap distribution of the sample mean is used to determine the sample mean
and its variance. The other thing that distinguishes the bootstrap is that direct appeal to the
classical central limit theorem is not applied to determine whether a sampling distribution
is approximately Gaussian. It is what it is, as seen in Figures 8.2 and 8.3.
The approach of this section is a nonparametric bootstrap because no assumptions were
made about the distribution of the data being resampled, and replicates were simply drawn
at random to compute a statistic of interest. The only requirement that this entails is an iid
random sample for the data.
An alternate approach is the parametric bootstrap, in which there is belief that the
random sample is drawn from a particular distribution, and hence the model distribution is
ﬁt to the data using an estimator such as the mle. Subsequently, resampling from the ﬁtted
distribution is used to estimate statistical properties of the data set that typically are too
complicated to compute directly from the sample. This serves as an alternative to the delta
method of Section 4.7.4 that does not require a ﬁrst-order Taylor series approximation.
There are two types of error with the bootstrap: statistical and simulation errors. The ﬁrst
is due to the difference between the population cdf F and the empirical cdf ^F that decreases
with the size of the data set and based on the choice of the statistic λ∗
N. Whereas the former
cannot be controlled because F is unknown, the latter can be minimized by choosing λ∗
N so
that its empirical distribution is approximately Gaussian. This can sometimes be achieved
1.6
1.8
2
2.2
2.4
2.6
0
5
1.6
1.8
2
2.2
2.4
2.6
0
2
4
1.6
1.8
2
2.2
2.4
2.6
0
2
4
1.6
1.8
2
2.2
2.4
2.6
0
2
4
1.6
1.8
2
2.2
2.4
2.6
0
2
4
1.6
1.8
2
2.2
2.4
2.6
0
2
4
Figure 8.3
Bootstrap distributions for the sample kurtosis of the Old Faithful geyser data computed using (top row) 30 and 100,
(middle row) 300 and 1000, and (bottom row) 3000 and 10,000 bootstrap samples from the data.
250
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

through transformation, as described later. However, just as parametric estimators often
deliver poor performance with small samples, bootstrap estimators also suffer when the
sample size is small; there is no substitute for a large random sample. Simulation error is
caused by sampling from the empirical rather than population distribution and typically can
be minimized by using a large number of replicates. Given the power of modern computers,
simulation error ought not to be a signiﬁcant limitation in using the bootstrap any more.
8.2.2 Bootstrap Parameter Estimation
Suppose that the statistic of interest is ^λN, along with derived quantities such as its standard
error. Obtain B bootstrap samples from the original data by resampling with replacement.
The notation X∗
k will be used to denote the kth bootstrap sample, each of which contains N
data. Apply the estimator for ^λN to each bootstrap sample to get a set of bootstrap replicates
^λ∗
N X∗
k


for k = 1, . . ., B, from which estimates of derived statistics may be obtained. For
example, the standard error of ^λN follows from
SEB ^λN


¼
1
B
X
B
k¼1
^λ
∗
N X∗
k


 λ
∗
h
i2
(
)1=2
(8.1)
where the sample mean of the bootstrap replicates is
λ
∗¼ 1
B
X
B
k¼1
^λ∗
N X∗
k


(8.2)
While statistical lore holds that 50–200 replicates sufﬁce to estimate the standard error on
a statistic, the results in Section 8.2.1 suggest that more may be required to properly
simulate the sampling distribution, and it is always good practice to examine the bootstrap
distribution for accuracy.
A bootstrap estimate for the bias can also be obtained. Recall from Section 5.2.2 that the
bias is deﬁned as the difference between the expected and population values of a statistic.
The bootstrap analogue is
^B ^λN


¼ λ
∗ ^λN
(8.3)
from which the bias-corrected estimate obtains
λ
_
N ¼ ^λN  ^B ¼ 2^λN  λ
∗
(8.4)
Example 8.2 For the Old Faithful data set in Examples 5.27 and 8.1, estimate the skewness
along with its bias (8.3) and standard error (8.1) using the bootstrap.
n = length(geyser);
rng default
b = 1000; %number of bootstrap replicates
theta = skewness(geyser)
251
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

theta =
-0.3392
ind = unidrnd(n, n, b);
xboot = geyser(ind); %resample the data
thetab = skewness(xboot);
mean(thetab) %bootstrap mean
ans =
-0.3432
mean(thetab) - theta %bootstrap bias
ans =
-0.0040
std(thetab) %bootstrap standard error
ans =
0.1026
Note that slightly different answers are obtained from the data and from the replicates.
Further, if the script is run a second time without issuing the “rng default” command,
another slightly different answer results. The variability represents the effect of random
resampling.
MATLAB includes a function called boot = bootstrp(nboot, bootfun, data) that obtains
nboot replicates using the function handle bootfun and data in data and returns the function
applied to the data in each row of boot. With 10,000 bootstrap replicates using the geyser
data, this gives
rng default
bmat = bootstrp(b, @skewness, geyser);
mean(bmat)
ans =
-0.3421
mean(bmat) - skewness(geyser)
ans =
-0.0029
std(bmat)
ans =
0.0991
The function bootstrp will parallelize if a cluster is available, such as occurs after
executing the command parpool('local'). However, due to the vagaries of process sched-
uling on multiple cores, the “rng default” command will no longer remove variability from
the answer.
Example 8.3 Evaluate the effect of changing the number of bootstrap replicates for the
skewness statistic using the geyser data.
252
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

This can be achieved by examining the empirical distribution of the bootstrap skewness
estimates for 50, 500, 5000, and 50,000 replicates, as shown in Figure 8.4. The results
indicate that a large number of replicates results in a smoother, more symmetric
distribution.
rng default
b = [50 500 5000 50000];
for i=1:4
bmat = bootstrp(b(i), @skewness, geyser);
subplot(2, 2, i)
ksdensity(bmat)
end
Example 8.4 Apply the bootstrap to the interquartile range for the geyser data. Explain
the result.
The bootstrap can be used with the interquartile range simply by substituting iqr for
skewness in the preceding script. The mean of the interquartile range and its standard
deviation are (23.755, 1.7154), (23.7775, 1.7462), (23.6853, 1.7084), and (23.6642,
1.6943) for 50, 500, 5000, and 50,000 bootstrap replicates compared with the sample
interquartile applied to the data of 24.0000. Whereas the mean of the bootstrap
replicates for the interquartile range appears to be settling down as the number of
samples increases, the standard deviation is somewhat variable. Figure 8.5 shows
kernel density estimates for the four bootstrap simulation sizes. As the number of
bootstrap replicates increases, multiple peaks in the distribution are resolved. This
occurs because the order statistics are concentrated on the sample values that are quite
–0.8
–0.6
–0.4
–0.2
0
0.2
0
2
4
–0.8
–0.6
–0.4
–0.2
0
0.2
0
2
4
–0.8
–0.6
–0.4
–0.2
0
0.2
0
2
4
–0.8
–0.6
–0.4
–0.2
0
0.2
0
2
4
Figure 8.4
Kernel density pdfs of the bootstrap skewness for the Old Faithful data with 50 (upper left), 500 (upper right), 5000
(lower left), and 50,000 (lower right) replicates.
253
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

discrete unless the data set is very large. As a consequence, the bootstrap distribution
becomes increasingly discrete as the number of replicates rises, and a naive nonpara-
metric bootstrap approach fails.
The issue seen in Example 8.4 can be addressed using the smoothed bootstrap, which
replaces the discrete jumps in the empirical cdf from which the bootstrap replicates are
obtained with smoothed transitions. The smoothed bootstrap is easily implemented by ﬁrst
taking a bootstrap sample in the usual way and then adding random draws from the
Gaussian distribution with a variance h2 to each, where h is the smoothing bandwidth.
Example 8.5 Apply the smoothed bootstrap to the interquartile range of the geyser data.
This will be illustrated with the 50,000 bootstrap replicate example shown in the lower
right panel of Figure 8.5. The smoothing bandwidth must be chosen by trial and error.
rng default
h = [.707 .866 1 1.414];
for i = 1:4
x = bmat + normrnd(bmat, h(i).^2*ones(size(bmat)));
subplot(2,2,i)
ksdensity(x)
end
The result is shown in Figure 8.6. A smoothing bandwidth of about 1 appears
appropriate.
15
20
25
30
0
0.2
0.4
15
20
25
30
0
0.2
0.4
15
20
25
30
0
0.2
0.4
15
20
25
30
0
0.2
0.4
Figure 8.5
Kernel density pdfs of the bootstrap interquartile range for the Old Faithful data with 50 (upper left), 500 (upper
right), 5000 (lower left), and 50,000 (lower right) replicates.
254
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

8.2.3 Bootstrap Conﬁdence Intervals
The bootstrap is widely used to obtain conﬁdence intervals when the statistic of interest
is complicated so that its sampling distribution is hard to obtain or work with. Standard
bootstrap conﬁdence intervals can be found in the usual way by substituting the
bootstrap estimates for the statistic (or its bias-corrected version) and the standard
deviation for the sample mean and sample standard error in the usual t conﬁdence
interval formula, yielding
Pr λ
∗ tN1 1  α=2
ð
Þ SEB
^λN

 λ  λ
∗þ tN1 1  α=2
ð
Þ SEB
^λN

h
i
¼ 1  α
(8.5)
Where λ
∗and SEB
^λN

are given by (8.2) and (8.1), respectively.
Several enhancements to this straightforward approach have been proposed. The boot-
strap-t conﬁdence interval is based on the normalized statistic
z∗
k ¼
^λ
∗
N  λ
∗
SEB
^λN

(8.6)
If a formula for the standard error of the kth bootstrap replicate exists, then it can be used to
estimate z∗
k , but otherwise the bootstrap may be applied (in other words, bootstrap the
bootstrap replicates). The
z∗
k


are then ranked, and the α=2 and 1  α=2 quantiles are
located in the set for use in the standard formula in place of the tN1 quantile. To achieve
reasonable accuracy, the number of bootstrap replicates needs to be large, with a minimum
of 1000 recommended and preferably more.
20
30
40
50
60
70
0
0.05
0.1
0.15
20
30
40
50
60
70
0
0.05
0.1
0.15
20
30
40
50
60
70
0
0.05
0.1
0.15
20
30
40
50
60
70
0
0.05
0.1
0.15
Figure 8.6
Kernel density pdfs of 50,000 bootstrap replicates for the interquartile range for the Old Faithful data smoothed by a
Gaussian kernel with a bandwidth of (top row) 0.707 and 0.866 and (bottom row) 1.0 and 1.414.
255
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

The bootstrap percentile method directly uses the α=2 and 1  α=2 quantiles of the
bootstrap distribution as the conﬁdence interval and has better stability and convergence
properties than either the standard or bootstrap-t conﬁdence interval.
The bias-corrected and accelerated (BCa) method adjusts the conﬁdence limits
obtained from the percentile method to correct for bias and skewness. It is obtained
as the proportion of the bootstrap estimates ^λ∗
k that are less than ^λN (or λ
∗). The bias
factor is
z0 ¼ Φ1 p0
ð
Þ
(8.7)
p0 ¼ 1
B
X
B
i¼1
1 ^λ
∗
k < ^λN


(8.8)
Deﬁne the acceleration factor that measures the rate of change in the bootstrap standard
error to be
a0 ¼
P
B
i¼1
λ
∗ ^λ
∗
k

3
6 P
B
i¼1
λ
∗ ^λ
∗
k

2

	3=2
(8.9)
The BCa conﬁdence interval is obtained using the quantiles
q1 ¼ Φ z0 þ z0 þ zα=2


= 1  a0 z0 þ zα=2






(8.10)
q2 ¼ Φ z0 þ z0 þ z1α=2


= 1  a0 z0 þ z1α=2






(8.11)
This reduces to the bootstrap percentile interval when z0 ¼ a0 ¼ 0.
Example 8.6 Returning to the Old Faithful data of Example 5.27, apply the bootstrap to get
the double-sided 95% conﬁdence limits on skewness and kurtosis using the ﬁrst three
methods just described.
rng default
b = 10000;
bmat = bootstrp(b, @skewness, geyser);
xm = mean(bmat);
xs = std(bmat, 1);
t = tinv(.975, length(geyser) - 1);
[xm - t*xs xm + t*xs]
zhat = (bmat - xm)./xs;
zhat = sort(zhat);
[xm + zhat(round(.025*b))*xs xm + zhat(round(.975*b))*xs]
bmats = sort(bmat);
[bmats(round(b*.025)) bmats(round(b*.975))]
256
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

For the skewness, the results are
-0.5370
-0.1472
-0.5371
-0.1484
-0.5371
-0.1484
For the kurtosis, the results are
1.7117
2.2879
1.7433
2.3120
1.7433
2.3120
MATLAB provides the function ci = bootci(nboot, {bootfun, data}, 'type', 'xxx') that
returns the conﬁdence interval in ci using nboot replicates and bootstrap function handle
bootfun. The parameter after “type” speciﬁes the method used to get the conﬁdence interval
and may be “norm” for the normal interval with bootstrapped bias and standard error, “per”
for the basic percentile method, “cper” for the bias-corrected percentile method, “bca” for
the bias-corrected and accelerated percentile method (default), and “stud” for the studentized
conﬁdence interval. As for bootstrp, bootci will parallelize if a cluster pool is available.
Example 8.7 Apply the BCa method to the geyser data.
rng default
ci = bootci(10000, @skewness, geyser)
ci =
-0.5219
-0.1334
ci = bootci(10000, @kurtosis, geyser)
ci =
1.7575
2.3443
The results in Examples 8.12 and 8.13 are comparable because the bootstrap distribu-
tions for the skewness and kurtosis of these data are fairly symmetric. In general, the
standard and bootstrap-t intervals are obsolete and should not be used. The percentile
method works well when the number of bootstrap replicates is large, and the BCa is the
most consistent estimator for general applications.
Example 8.8 Returning once again to the earthquake interval data from Example 7.3,
compute the skewness and kurtosis along with bootstrap percentile and BCa conﬁdence
intervals.
257
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

skewness(quakes)
ans =
1.4992
kurtosis(quakes)
ans =
5.5230
rng default
bootci(10000, {@skewness, quakes}, 'type', 'per')
ans =
0.6487
2.0201
bootci(10000, @skewness, quakes)
ans =
0.9836
2.3748
bootci(10000, {@kurtosis, quakes}, 'type', 'per')
ans =
2.7588
8.4796
bootci(10000, @kurtosis, quakes)
ans =
3.5865
10.5179
The differences between the two types of bootstrap estimators is substantial and cannot
be accounted for by the fact that the BCa conﬁdence interval is bias corrected while the
percentile one is not. The cause can be found by examining the bootstrap distributions for
the skewness and kurtosis (Figure 8.7), both of which are skewed either to the left
–1
0
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
10
15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
1
2
3
5
Figure 8.7
Kernel density pdfs for the bootstrap distributions using 10,000 replicates for skewness (left) and kurtosis (right)
computed using the earthquake interval data.
258
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

(skewness) or to the right (kurtosis). In this circumstance, the percentile conﬁdence interval
will be biased, whereas the BCa interval will offer better performance.
8.2.4 Bootstrap Hypothesis Tests
Let an upper-tail hypothesis test on the statistic λ that is estimated by the estimator ^λ be
evaluated using the p-value p ¼ 1  Fð^λÞ. If the cdf F x
ð Þ is known, then this is
straightforward. More typically, the cdf is not known, and recourse is made to an
asymptotic result whose accuracy is unclear. The alternative approaches are to use a
nonparametric method (Section 7.3), a permutation test, or a bootstrap hypothesis test.
The latter differs from the permutation test in that sampling is done with replacement.
Implementing the bootstrap hypothesis test is straightforward: generate B bootstrap
replicates X∗
k of the random sample being tested, and then compute a bootstrap test
statistic ^λ∗
N X∗
k


from each using the same procedure as for ^λ. The bootstrap upper-tail
p-value is obtained by comparing the bootstrap test statistics with the test statistic based
on all the data
^p ¼ 1  ^F
^λ

¼ 1
B
X
B
i¼1
1 ^λ
∗
N X∗
k


> ^λ
h
i
(8.12)
A lower-tail test can be implemented by reversing the argument in (8.12), whereas a two-
tail test uses the equal-tail bootstrap p-value
^p ¼ 2
B min
X
B
i¼1
1 ^λ
∗
N X∗
k


 ^λ
h
i
;
X
B
i¼1
1 ^λ
∗
N X∗
k


< ^λ
h
i
(
)
(8.13)
In any instance, the bootstrap p-value is the proportion of the bootstrap test statistics that
are more extreme than the observed test statistic based on the entire sample.
The bootstrap test procedure outlined here is exact when the population test statistic
is a pivot (i.e., does not depend on any unknown parameters), so ^λ and ^λ∗
N have the
same distribution if the null hypothesis holds, and the number of bootstrap samples B
is such that α B þ 1
ð
Þ is an integer. The result is called a Monte Carlo test and actually
predates introduction of the bootstrap by two decades. When these conditions do not
hold, the resulting bootstrap hypothesis test will not be exact. However, it is good
practice to enforce the number of samples for a Monte Carlo test on all bootstrap
hypothesis tests.
It is important to ensure that the random samples are obtained from the null distribution
for the test, which is not the same as the empirical cdf of the data in most cases. The
appropriate approach for a one-sample location test is as follows:
1. Obtain B random draws from the empirical cdf of the data;
2. Compute a location parameter ^λ based on the entire sample;
3. Compute bootstrap test statistics that are proportional to ^λ∗
N  ^λ; and
4. Compute the p-value by comparing the bootstrap test statistics against a sample test
statistic that is proportional to ^λ  μ∗, such as the one-sample t statistic, where μ∗is the
postulated location parameter.
259
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

Example 8.9 Returning to the number of days an oceanographer spends at sea from
Example 6.8, evaluate the null hypothesis that it is 30 days against the alternate hypothesis
that it is not using 99, 999, 9999, and 99,999 bootstrap replicates.
rng default
b = [99 999 9999 99999];
data = [54 55 60 42 48 62 24 46 48 28 18 8 0 10 60 82 90 88 2 54];
that = (mean(data) - 30)./std(data, 1); %sample test statistic
against the hypothesized value
for i=1:4
bmat = bootstrp(b(i), @(t)(mean(t) - mean(data))./std
(t, 1), data);
pval = 2*min(sum(bmat >= that), sum(bmat < that))./b(i)
%two tail p-value
end
pval =
0.0404
pval =
0.0320
pval =
0.0402
pval =
0.0378
The test rejects the null hypothesis that an oceanographer spends 30 days per year at
sea, although not strongly. The distribution of the bootstrap test statistic is shown in
Figure 8.8 for 99,999 replicates. The value of the test statistic based on the original data
is 0.5240.
–1.5
–1
–0.5
0
0.5
1
1.5
0
0.5
1
1.5
2
Figure 8.8
Kernel density pdf for 99,999 bootstrap replicates in a test of the hypothesis that an oceanographer spends 30 days
per year at sea. The vertical dashed line is the test statistic based on all the data.
260
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

For a two-sample test for the equality of means by comparing M data Xi
f
g and N values
Yi
f
g, the following algorithm pertains:
1. Center the two data sets around their grand mean as ~Xi ¼ Xi  XN þ XN þ YN


=2
and similarly for Yi;
2. Sample separately with replacement from each centered data set;
3. Evaluate the two-sample t statistic for each sample; and
4. Compute the p-value by comparing the bootstrap statistics with the sample value of the
test statistic.
Example 8.10 Returning to the cloud seeding data of Example 6.12, compute a bootstrap
test for the null hypothesis that there is no effect from cloud seeding against the alternate
that there is.
rng default
b = [99 999 9999 99999];
that = (mean(clouds(:, 1)) - mean(clouds(:, 2)))./ . . .
sqrt(var(clouds(:, 1)) + var(clouds(:, 2))); %two
sample t statistic
gmean = (mean(clouds(:, 1)) + mean(clouds(:, 2)))/2; %grand
mean of the data
for i=1:4
bmat1 = bootstrp(b(i), @(x)[mean(x) var(x, 1)], clouds
(:, 1) - mean(clouds(:, 1)) + gmean);
bmat2 = bootstrp(b(i), @(x)[mean(x) var(x,1)], clouds(:,
2) - mean(clouds(:, 2)) + gmean);
boot = (bmat1(:, 1) - bmat2(:, 1))./sqrt(bmat1(:, 2) +
bmat2(:, 2));
pval = 2*min(sum(boot > that), sum(boot <= that))./b(i)
end
pval =
0.0606
pval =
0.0160
pval =
0.0174
pval =
0.0182
The null hypothesis is rejected, and there is a difference in rainfall between seeded and
unseeded clouds. This is in agreement with the rank sum test but in contrast with the two-
sample t test. Figure 8.9 shows the bootstrap distribution that is asymmetric due to the
logarithmic nature of the data. The sample test statistic is 0.3919. A nearly identical result
261
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

is obtained after taking logs of the data. This example demonstrates the robustness of
bootstrap tests against departures from Gaussianity.
To test whether two samples are drawn from the same distribution, use the following
algorithm:
1. Obtain B random draws from the data set obtained by pooling
Xi
f
g and
Yi
f
g.
The ﬁrst M observations in the bootstrap sample become X∗
i , and the last N
become Y∗
i .
2. Compute a bootstrap test statistic that measures the difference between the two samples.
This could be as simple as the difference of their means.
3. Compute the p-value by comparing the bootstrap test statistics with the sample test
statistic.
Example 8.11 Returning to the earthquake interval data from Example 7.3, evaluate the null
hypothesis that the data are exponential and Rayleigh by testing against random draws with
mle parameters for each.
rng default
rnd = exprnd(mean(quakes), size(quakes));
data = [quakes rnd]';
b = [99 999 9999 99999];
that = mean(quakes) - mean(rnd);
for i = 1:4
bmat = bootstrp(b(i), @(x)(mean(x(1:length(quakes))) -
mean(x(length(quakes)+1:2*length(quakes)))),data);
pval = 2*min(sum(bmat > that),sum(bmat <= that))./b(i)
end
–1
–0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
Figure 8.9
Kernel density pdf of the bootstrap distribution of the test statistic for the cloud seeding data. The vertical dashed
line is the test statistic based on all the data.
262
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

pval =
0.4848
pval =
0.4725
pval =
0.4992
pval =
0.4937
The result is quite unequivocal. The test statistic is 121.873. Figure 8.10 shows the
bootstrap test distribution for 99,999 replicates.
The test will be repeated using the difference of the medians as the test statistic. The
result is
pval =
0.1616
pval =
0.1441
pval =
0.1414
pval =
0.1376
The null hypothesis that the data are exponential is accepted, and the conclusion is
unchanged. Figure 8.11 shows the bootstrap distribution for 99,999 replicates, which is
somewhat long tailed and shows evidence for clustering around the order statistics, as in
Example 8.4. The test statistic is 135.8992.
Repeating using the Rayleigh distribution and the difference of the means as the metric
gives
–300
–200
–100
0
100
200
300
0
2
4
6
8 ×10–3
Figure 8.10
Kernel density pdf of the bootstrap distribution for the earthquake data using the difference between the means as
the bootstrap statistic and an exponential distribution as the target statistic. The vertical dashed line is the test
statistic based on all the data.
263
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

pval =
0
pval =
0.0040
pval =
0.0034
pval =
0.0030
The null hypothesis that the earthquake data are Rayleigh is strongly rejected. Figure 8.12
shows the bootstrap distribution, which is long tailed. The test statistic is 204.6115.
Bootstrap hypothesis tests are typically not exact because there is always a difference
between the true and empirical cdfs, and the best results obtain when the empirical cdf is
as close as possible to the true cdf in the vicinity of the critical value, although that is
–500
0
500
0
1
2
3
4
5 ×10–3
Figure 8.11
Kernel density pdf of the bootstrap distribution for the earthquake data using the difference of the medians as the
bootstrap statistic and an exponential distribution as the target statistic. The vertical dashed line is the test statistic
based on all the data.
–300
–200
–100
0
100
200
300
0
1
2
3
4
5
6 ×10–3
Figure 8.12
Kernel density pdf of the bootstrap distribution for the earthquake data using the difference of the medians as the
bootstrap statistic and a Rayleigh distribution as the target statistic. The vertical dashed line is the test statistic
based on all the data.
264
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

difﬁcult to ensure in practice without extensive simulation. However, when the test
statistic is asymptotically pivotal, a bootstrap hypothesis test has a lower probability of
error in terms of sample size N than any asymptotic test using the same test statistic. The
reverse is true when the test statistic is not asymptotically pivotal. Further, the power of a
bootstrap test is comparable to that of an asymptotic test when the test statistic is
asymptotically pivotal.
8.2.5 Bias Correction for Goodness-of-Fit Tests
It is well known that goodness-of-ﬁt tests such as the Kolmogorov-Smirnov (K-S) and
Anderson-Darling (A-D) procedures require that the parameters in the target distribution be
known a priori and hence will yield a biased test if the parameters are estimated from the
set of observations. This issue can be obviated by using the Lilliefors test, presuming that
the data distribution is known, which rarely holds in practice. However, it is possible to
remove the bias from goodness-of-ﬁt tests using a Monte Carlo approach. The steps in the
procedure are as follows:
1. Obtain the K-S test statistic for the N observations against a target distribution whose
parameters are estimated using the mle;
2. Obtain N random draws with replacement from the target distribution using the same
mle parameters;
3. Compute the K-S statistic for the random draw;
4. Repeat steps 2 and 3 a large number of times; and
5. Compute the p-value.
Example 8.12 Remove the bias from the Weibull ﬁt to earthquake data in Example 7.7.
parmhat = wblﬁt(quakes);
cdf
=
[sort(quakes)'
wblcdf(sort(quakes)',
parmhat(1),
parmhat(2))];
[h, p ,ksstat] = kstest(quakes, cdf);
n = length(quakes);
b = 999999;
sps = [];
parfor i = 1:b;
draw = sort(wblrnd(parmhat(1), parmhat(2), 1, n));
cdf = [draw' wblcdf(draw', parmhat(1), parmhat(2))];
[h, p, ks1] = kstest(draw, cdf);
sps = [sps ks1];
end
pval = 2*min(sum(sps >= ksstat) + 1, sum(sps < ksstat) + 1)/(b +1)
pval =
0.1995
The p-value used in this script differs slightly from the one usually used with bootstrap
hypothesis tests and is further discussed in Section 8.3.1. The sample p-value is 0.9006 and
265
8.2 The Bootstrap
.009
13:32:30, subject to the Cambridge Core terms of use,

is clearly upward biased, although the correction does not change the outcome of the
hypothesis test. The simulated distribution (Figure 8.13) is asymmetric, as would be
expected because it represents that of the K-S statistic.
Example 8.13. Remove the bias from the K-S test applied to contaminated Gaussian data in
Example 7.8.
data = normrnd(0, 1, 1000, 1);
data(1000) = 6.5;
data(999) = 6.0;
data(998) = 5.5;
data(997) = 5;
[h, p, kstat] = kstest(data);
par1 = mean(data);
par2 = std(data, 1);
b = 99999;
sps = [];
parfor i
= 1:b;
draw = sort(normrnd(par1, par2, 1000, 1));
cdf = [draw normcdf(draw, par1, par2)];
[h, p, ks1] = kstest(draw, cdf);
sps = [sps ks1];
end
pval = 2*min(sum(sps >= kstat) + 1, sum(sps < kstat) + 1)/(b + 1)
pval =
0.9192
This compares with the sample p-value of 0.5415. The conclusion is unchanged, but
downward bias of the sample K-S statistic is evident.
0
0.1
0.2
0.3
0.4
0
5
10
15
Monte Carlo Distribution
Figure 8.13
Monte Carlo distribution of the earthquake K-S statistic (solid line), along with the test statistic (dashed line).
266
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

8.3 Permutation Tests
8.3.1 Principles
Permutation tests were introduced by Pitman (1937a; 1937b; 1938) but were quite imprac-
tical for statistical purposes until modern computers became available. An overview is
provided by Good (2000, 2005), and a more advanced treatise is that of Pesarin & Salmaso
(2010) or Berry, Mielke, & Johnston (2016). Permutation tests have wide applicability and
can be used to analyze data that are Gaussian, close to Gaussian, or markedly non-
Gaussian. A permutation test is typically at least as powerful as the alternatives for large
samples (Hoeffding 1952). It is also nearly distribution free, requiring only very simple
assumptions about the population. Further, a key characteristic of the permutation test is its
accuracy. If the null hypothesis is true, then there is nearly exactly a probability α that the
permutation p-value will be less than α, and under particular circumstances, a permutation
test becomes exact. These are advantages to the permutation test over the bootstrap test
described in Section 8.2.4 because the latter is neither exact nor conservative and is
typically less powerful than a permutation test.
The key steps comprising the application of a permutation test are:
1. Choose a test statistic λ;
2. Compute the value of the test statistic for the original set(s) of data;
3. Combine the data into a pooled data set;
4. Obtain the permutation distribution of λ by randomly permuting the pooled data and
recomputing the test statistic;
5. Repeat step 4 many times; and
6. Accept (or not) the null hypothesis according to the p-value for the original test statistic
compared with the permutation distribution.
For a two sample test, if the null hypothesis holds, then a sample taken from the
pooled data set ensuing from the merger of X and Y will be same as that from either
sample taken alone. Consequently, if a sample is taken from the pooled data set, the
ﬁrst M values may be used to represent the ﬁrst data set, and the last N values may be
used to represent the second data set. If the difference between the sample means (or
any other statistic that measures the difference in location) of the original data does not
lie within the middle 95% of the permutation distribution, then a double-sided permu-
tation test rejects the null hypothesis. Similar arguments pertain for upper- and lower-
tail tests.
To express this more formally, let Z ið Þ denote the order statistics of the merged data set,
and deﬁne the indicator function 1 Z ið Þ 2 X


that determines whether the ith merged order
statistic came from the Xi. The combination of the order statistics and indicator function
contains all the information present in the original two samples. The vector Ψ that results
from applying the indicator function to all the merged data consists of M ones and N zeros,
and by combinatoric arguments, there are N þ M
ð
Þ!= M!N!
ð
Þ unique ways of dividing the
267
8.3 Permutation Tests
.009
13:32:30, subject to the Cambridge Core terms of use,

M þ N elements into two subsets of size M and N. Consequently, each realization of Ψ is a
permutation that has a probability of occurrence of M!N!= M þ N
ð
Þ! under the null
hypothesis. Deﬁne a test statistic ^λ Z ið Þ; Ψ


that measures the difference between the
sampled populations. Let Ψ∗
k denote one of the possible permutations, and let the permu-
tation replication of the test statistic be ^λ∗
k Z ið Þ; Ψ∗
k


. The distribution that results from a
set of permutation replications is the permutation distribution. The two-sided equal-tail
permutation p-value is
pperm ¼ 2  min
1
B þ 1
X
B
k¼1
1 ^λ
∗
k  ^λ


þ 1
"
#
;
1
B þ 1
X
B
k¼1
1 ^λ
∗
k < ^λ


þ 1
"
#
(
)
(8.14)
where B is the number of permutations and is evaluated in the usual way. One-tailed tests
also may be used. The choice of test statistic is usually not critical, although in some
circumstances a poor choice may result in reduced power.
Permutation tests are based on sampling without replacement, in contrast to a bootstrap
hypothesis test that uses sampling with replacement. A sufﬁcient condition for a permutation
test to be exact and unbiased is exchangeability of the observations in a combined sample.
The permutation p-value in (8.14) differs from
^p ¼ 2  min 1
B
X
B
k¼1
1 ^λ
∗
k  ^λ


;
1
B
X
B
k¼1
1 ^λ
∗
k < ^λ


"
#
(8.15)
used in Section 8.2.4. Equation (8.15) is unbiased, but its Type 1 error rate is given by
(Phipson & Smyth 2010)
Pr ^p  α
ð
Þ ¼ Bα
b
c þ 1
B þ 1
(8.16)
and may be larger or smaller than α as that parameter is near 0 or 1, respectively. Equation
(8.15) also can yield a zero p-value. Neither of these issues is likely to be important for
single hypothesis tests, but both can lead to problems when multiple hypotheses are
simultaneously tested, as described in Section 6.6. By contrast, while (8.14) is biased, it
yields an exact p-value presuming that there are no duplicate permutations or permutations
equal to the original data in the simulation, and cannot be smaller than 1= B þ 1
ð
Þ.
Consequently, it is preferred in practice. For sampling with replacement, (8.14) gives a
conservative upper bound on the p-value.
8.3.2 One-Sample Test for a Location Parameter
While permutation methods are usually applied to two or more samples, they can be used
for some types of one-sample tests. If the underlying distribution for a data set is symmetric
about the location parameter λ, then
Pr X  λ  x
ð
Þ ¼ F λ  x
ð
Þ ¼ Pr X  λ þ x
ð
Þ ¼ 1  F λ þ x
ð
Þ
or
268
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

F λ  x
ð
Þ þ F λ þ x
ð
Þ ¼ 1
For a test for a particular value of the location parameter λ0, a suitable test statistic is the
sum of the deviations of the data about λ0, which should be close to zero if the null
hypothesis holds. Under the alternate hypothesis, randomizing the signs of the deviations
of the data about λ0 will make the result either smaller or larger than zero. Consequently,
permuting the signs of the deviations and reattaching the results to their absolute value, and
repeating the process many times, will deﬁne a permutation distribution to which the sum
of the deviations of the original data can be compared to deﬁne a p-value. This works
because for a symmetric distribution the absolute values of the deviations are a sufﬁcient
statistic for the sample.
It is important to examine the permutation distribution for symmetry, which is most
easily accomplished by comparing it with a standard distribution such as the Gaussian.
This does not imply that the permutation distribution should correspond to some paramet-
ric form. Romano (1990) proved that a permutation test for a location parameter is
asymptotically exact if the data distribution has ﬁnite variance. If the permutation distribu-
tion is almost symmetric, the test will be almost exact even for small numbers of data.
Further, the permutation test has nearly the power of a Student t test for Gaussian data and
large samples.
Example 8.14 Returning to Example 6.8, devise a permutation test for the null
hypothesis that the number of days per year that an oceanographer spends at sea is
30 against an upper-tail alternate. The MATLAB function randperm(n) produces a
random permutation of the integers from 1 to n and will be used to obtain the
permutations.
x = [54 55 60
42 48 62 24 46 48 28 18 8 0 10 60 82 90 88 2 54];
x1 = sum(x - 30); %test statistic
n = length(x);
s = sign(x - 30); %signs of the differences
sps = [];
b = 10000;
perm = zeros(b, n);
m = [];
parfor i = 1:b
perm(i, :) = randperm(n);
if perm(i, :) == 1:n;
m = [m i];
end
end
if ~isempty(m)
perm(m', :) = []; % ensure original data are not included
end
perm = unique(perm, 'rows'); %ensure sampling without replacement
269
8.3 Permutation Tests
.009
13:32:30, subject to the Cambridge Core terms of use,

b = length(perm)
b =
10000
parfor i = 1:b
sp = s(perm(i, :)); % permute the signs
x2 = sum(sp.*abs(x - 30)); % permutation test statistic
sps = [sps x2];
end
pval = (sum(sps >= x1) + 1)/(b +1) %exact p-value
pval =
0.0281
The null hypothesis is rejected, but only weakly, and was also weakly rejected using a t
test in Example 6.8. The p-values for 100,000 and 1 million permutations are 0.0267 and
0.0262, respectively. In each case, there were no duplicate permutations in the array perm.
The permutation distribution for 1 million permutations is shown in Figure 8.14 and
appears symmetric.
8.3.3 Two-Sample Test for a Location Parameter
For a two sample test, a statistic is needed to measure the difference between two sets of
random variables, such as the difference of their sample means or, equivalently, the sum of
the data from one of the treatment groups. To demonstrate the validity of the latter as a test
statistic, write out the difference of the means of two sets of rvs:
X  Y ¼ M þ N
M N
X
M
i¼1
xi  1
N
X
M
i¼1
xi þ
X
M
i¼1
yi
 
!
(8.17)
–100
0
100
200
300
400
0
1
2
3
4
5
6
7
Permutation Distribution
× 10–3
Figure 8.14
Kernel density permutation distribution (solid line) for the oceanographer days at sea example compared with a
Gaussian pdf with mle parameters (dotted line). For comparison, the test statistic (dashed line) based on the
original data is 279. The rejection region is to its right for an upper-tail test.
270
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

The second term in (8.17) is a constant, so the difference of means is a monotone function
of the sum of the data in the ﬁrst treatment group. Provided that the distributions of X and
Y have ﬁnite second moments and the sample sizes are the same, the permutation test for
two location parameters based on the sum of the observations in X is asymptotically exact
(Romano 1990).
Example 8.15 Use a permutation test on the Michelson speed of light data to assess whether
the location parameters in each year are the same.
michelson1 = importdata('michelson1.dat');
michelson2 = importdata('michelson2.dat');
n1 = length(michelson1);
n2 = length(michelson2);
n = n1 + n2;
michelson = [michelson1' michelson2']';
s = sum(michelson1); %measure of the null hypothesis
sps = [];
b = 10000;
perm = zeros(b, n);
m = [];
parfor i = 1:b
perm(i, :) = randperm(n);
if perm(i, :) == 1:n;
m = [m i];
end
end
if ~isempty(m)
perm(m', :) = []; % ensure original data are not included
end
perm = unique(perm, 'rows'); %ensure sampling without replacement
b = length(perm)
b =
10000
parfor i = 1:b
michelsonp = michelson(perm(i, :)); %permuted version of
the merged data
sp = sum(michelsonp(1:n1));
sps = [sps sp];
end
pval = 2*min(sum(sps >= s) + 1, sum(sps < s) + 1)/(b + 1) %
permutation p-value
pval =
1.9998e-04
271
8.3 Permutation Tests
.009
13:32:30, subject to the Cambridge Core terms of use,

The result with 1 million permutation replicates is 1.8000  105. The two-sample t test
gives a two-sided p-value of 1.4  105 for comparison. Note that only a small fraction of
the possible permutations has been sampled even with 1 million replicates because the total
number of permutations is 26,010,968,307,696,038,491,182,501, so the sampling is effect-
ively without replacement even in the absence of enforcing that condition. The empirical
null distribution for 1 million permutations compared with a standardized Gaussian
(Figure 8.15) shows a strong resemblance. The Gaussian distribution is used as an
exemplar symmetric distribution and does not imply that the permutation distribution
should be Gaussian.
However, there are a signiﬁcant number of duplicate values in the 1879 data that are
presumably due to rounding during unit conversion and are apparent in the stairstep pattern
in the q-q plot of Figure 6.4. The duplicates can be removed using the MATLAB unique
function. This leaves only 30 of 100 values in the 1879 data set and 22 of 24 values in the
1882 data set. Repeating the permutation test yields a p-value of 0.0057, so the conclusion
is unchanged by removing the duplicates.
Example 8.16 Apply a permutation test to evaluate the null hypothesis that seeded and
unseeded clouds produce the same amount of rain using the data from Example 6.12.
data = importdata('cloudseed.dat');
n = length(data);
cloud = [data(:, 1)' data(:, 2)']';
s = mean(data(:, 1)) - mean(data(:, 2));
sps = [];
b = 1000000;
perm = zeros(b, 2*n);
8.2
8.3
8.4
8.5
0
0.2
0.4
0.6
0.8
1
Permutation Distribution
× 10–3
×104
Figure 8.15
The permutation null distribution with 1 million samples (solid line) for the Michelson speed of light data compared
with a standard Gaussian distribution (dotted line) with mle parameters. The vertical dashed line shows the value of
the test statistic.
272
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

m = [];
parfor i = 1:b
perm(i, :) = randperm(2*n);
if perm(i, :) == 1:2*n;
m = [m i];
end
end
if ~isempty(m)
perm(m', :) = []; % ensure original data are not included
end
perm = unique(perm, 'rows'); %ensure sampling without replacement
b = length(perm)
b =
1000000
parfor i = 1:b
cloudp = cloud(perm(i, :));
sp = mean(cloudp(1:n)) - mean(cloudp(n + 1:2*n);
sps = [sps sp];
end
pval = 2*min(sum(sps >= s) + 1, sum(sps < s) + 1)/(b +1)
pval =
0.0441
The null hypothesis that the seeded and unseeded clouds produce the same amount of
rain is rejected, although only weakly. A two-sample t test in Example 6.12 produced weak
acceptance of the alternate hypothesis with a p-value of 0.0511. The permutation distribu-
tion is approximately symmetric (Figure 8.16) but is platykurtic compared with a Gaussian.
The test statistic based on the original data is 277.3962.
Repeating after taking the logarithms of the data gives a p-value of 0.0143, so the
conclusion is unchanged, in contrast to the parametric test in Example 6.12. This example
shows how permutation tests are not dependent on distributional assumptions.
–500
0
500
0
0.5
1
1.5
2
2.5
3
Permutation Distribution
×10–3
Figure 8.16
Permutation distribution for 1,000,000 replicates of the cloud seeding data (solid) compared with a Gaussian
distribution with mle parameters (dotted line). The vertical dashed line shows the value of the test statistic.
273
8.3 Permutation Tests
.009
13:32:30, subject to the Cambridge Core terms of use,

8.3.4 Two-Sample Test for Paired Data
Rather than being based on random sampling from a population as in Section 8.3.3, a
permutation test applied to paired data requires the assumption that the assignment to
treatment groups is random. However, the permutation approach is very similar. The
permutation distribution (often called the randomization distribution in this application)
is then constructed to determine whether this statistic is large enough to establish that there
is a signiﬁcant difference between the treatments. Ernst (2004) describes estimation of the
additive treatment effect by inverting the permutation test.
Example 8.17 Returning to the leukemia remission data from Example 7.15, use a permu-
tation approach to assess whether a new drug improves survival.
remiss = importdata('remiss.dat');
x = remiss(1:21, 2);
y = remiss(22:42, 2);
s = sum(x);
data = [x' y'];
n = length(data);
sps = [];
b = 1000000;
perm = zeros(b, n);
m = [];
parfor i = 1:b
perm(i, :) = randperm(n);
if perm(i, :) == 1:n
m = [m i];
end
end
if ~isempty(m)
perm(m', :) = []; % ensure original data are not included
end
perm = unique(perm, 'rows'); %ensure sampling without replacement
b = length(perm)
b =
1000000
parfor i = 1:b
datap = data(perm(i, :));
sp = sum(datap(1:n/2);
sps = [sps sp];
end
pval = 2*min(sum(sps >= s) + 1,sum(sps < s) + 1)/(b + 1)
pval =
0.0021
274
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

This compares favorably with the sign test result in Example 7.15 and the signed rank test
result in Example 7.18. Figure 8.17 shows the permutation distribution for this example.
8.3.5 Two-Sample Test for Dispersion
On initial reﬂection, it would seem reasonable to compare the variances of two populations
by using the same procedure as for location parameters but using the squares of the
observations instead of the observations themselves. However, this will fail unless the
means of the two populations are known or are known to be the same. This follows directly
because E X2


¼ σ2 þ μ2.
The solution is to center each data set about its median and then use the difference of the
sum of the squares (or the absolute values) of the deviations from the median as a test
statistic. In essence, this centers each distribution about its middle. If the number of
samples is odd, then one of these values will be zero and hence not informative. It should
be discarded. In practice, this issue is important only for small sample sizes or when data
have undergone unit conversions with truncation.
Example 8.18 Use a permutation approach to test that the dispersion in the two Michelson
speed of light data sets are the same after removing duplicates.
x1 = importdata('michelson1.dat');
x2 = importdata('michelson2.dat');
x1 = unique(x1);
x1 = x1 - median(x1);
x2 = unique(x2);
x2 = x2 - median(x2);
x = [x1' x2']';
n1 = length(x1);
n2 = length(x2);
150
200
250
300
350
400
0
0.005
0.01
0.015
Permutation Distirbution
Figure 8.17
Permutation distribution for 1,000,000 replicates of the leukemia remission data (solid) compared with a Gaussian
distribution with mle parameters (dotted line). The vertical dashed line shows the value of the test statistic.
275
8.3 Permutation Tests
.009
13:32:30, subject to the Cambridge Core terms of use,

s = sum(x1.^2)/n1 - sum(x2.^2)/n2
sps = [];
b = 100000;
perm = zeros(b, n1 +n2 );
m = [];
parfor i = 1:b
perm(i, :) = randperm(n1 + n2);
if perm(i, :) == 1:n1 + n2;
m = [m i];
end
end
if ~isempty(m)
perm(m', :) = []; % ensure original data are not included
end
perm = unique(perm, 'rows'); %ensure sampling without replacement
b = length(perm)
b =
1000000
parfor i = 1:b
xp = x(perm(i, :));
sp = sum(xp(1:n1).^2)/n1 - sum(xp(n1 + 1:n1 + n2).^2)/n2;
sps = [sps sp];
end
pval=2*min(sum(sps>=s) + 1,sum(sps<s) + 1)/(b + 1)
pval =
0.5809
The null hypothesis is accepted. The permutation distribution is slightly asymmetric
(Figure 8.18). The test statistic computed from the original data is 2606.4 and is located
in the lower half of the permutation distribution.
–2
–1
× 104
0
0
1
2
0.2
0.4
0.6
0.8
1
Permutation Distribution
×10–4
Figure 8.18
Permutation distribution for the Michelson speed of light data dispersion test using the squared deviations
of the observations from their median as a test statistic (solid line) compared with a Gaussian distribution with
mle parameters (dotted line). There are 1 million replicates in the sample. The vertical dashed line is the test statistic.
276
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

An alternative measure of dispersion that is more robust is the difference of the sum of
the absolute values of the deviations from the median. Repeating the test using this metric
gives a p-value of 0.9956 for 1 million permutations, strongly accepting the null hypothesis
that the dispersion is the same in the two data sets. The test statistic based on the original
data is 0.9848. The permutation distribution is more symmetric (Figure 8.19), although
the statistical outcome is not changed.
8.4 The Jackknife
The jackknife is a linear approximation to the bootstrap that is computationally simple
and efﬁcient, making it suitable for large data sets where the bootstrap would be
prohibitive, although as computational power grows, this argument for the jackknife
gets weaker. Thomson & Chave (1991) provide a review of the jackknife. Let Xi
f
g, i =1,
..., N, be independent samples drawn from an unknown distribution or mixture of
distributions, and let λ be some parameter that is to be estimated using the estimator ^λ.
Let ^λN be the estimate of λ computed using all the data. Subdivide the data into N groups
of size N  1 by sampling with replacement. Denote the estimate of λ obtained from the
ith subset as ^λ

i X1; :::Xi1; Xiþ1; :::XN
ð
Þ, where the subscript \i denotes the delete the ith
datum estimate. The jackknife literature usually involves substitute data called pseudo-
values given by
pi ¼ N^λN  N  1
ð
Þ^λ

i
(8.18)
The jackknife estimate of λ is the mean of (8.18)
–2
–1
×104
0
0
1
2
0.2
0.4
0.6
0.8
1
Permutation Distribution
× 10–4
Figure 8.19
Permutation distribution for the Michelson speed of light dispersion test using the absolute values of the
deviations of the observations from their median as a test statistic (solid line) compared with a Gaussian pdf
with mle parameters (dotted line). The vertical dashed line shows the test statistic. There are 1 million replicates
in the sample.
277
8.4 The Jackknife
.009
13:32:30, subject to the Cambridge Core terms of use,

~λ ¼ 1
N
X
N
i¼1
pi
¼ N^λN  N  1
N
X
N
i¼1
^λ

i
(8.19)
If Eð^λNÞ ¼ λ þ a=N
ð
Þ þ O 1=N2


, where a is a constant, it can shown that the jackknife
estimator eliminates the 1=N term and hence reduces the bias. It was for this purpose that it
was originally introduced in the 1950s.
For a linear statistic such as the sample mean, where ^λN ¼ XN
~λ ¼
X
N
I¼1
xi  N  1
N
X
n
j¼1
1
N  1
X
i¼1
i 6¼ j
N
xi
0
B
@
1
C
A
¼
X
N
j¼1
xj  1
N
X
i¼1
i 6¼ j
N
xi
0
B
@
1
C
A
¼
X
N
j¼1
N þ 1
N
xj

1
N
X
N
i¼1
xi
 
!
¼ N þ 1
ð
ÞXN  N XN
¼ XN
(8.20)
which is the same as ^λN. Since the mean is a linear statistic, the jackknife will not improve
its calculation. However, when the estimator is not linear in the data, this will not generally
be true.
The most important application of the jackknife occurs in estimating the variance of ^λN.
Recalling that the standard or mle estimator is the unbiased sample variance divided by N
when ^λN ¼ XN, the jackknife formula follows from the deﬁnition of the pseudovalues
in (8.18).
^s2
J ¼
1
N N  1
ð
Þ
X
N
i¼1
pi  ~λ

2
¼ N  1
N
X
N
i¼1
^λ

i  1
N
X
N
j¼1
^λ

i
 
!2
¼ N  1
N
X
N
i¼1
^λ

i  λ

2
(8.21)
where λ is the mean of the delete-one estimates. This is computationally simple: compute the
delete-one estimates of the statistic, compute their mean, and compute the variance. While
one might expect that ^s2
J ¼ varð~λÞ, small sample simulations show that ^s2
J ¼ varð^λNÞ.
An important property of the jackknife is (Efron & Stein 1981)
E ^s2
J


> σ2
N
(8.22)
278
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

This holds even for data that are not identically distributed. Thus the jackknife yields a
conservative estimate for the variance.
Under general conditions, it can be shown that ð~λ  λÞ=^sj e tN1 and ð^λN  λÞ=^sj e tN1.
This allows the construction of approximate conﬁdence intervals using the jackknife in the
usual way:
Pr ^λN  tN1 1  α=2
ð
Þ ^sJ  λ  ^λN þ tN1 1  α=2
ð
Þ ^sJ
h
i
¼ 1  α
(8.23)
This holds for any statistic, not just the mean.
However, the jackknife should not be used blindly on statistics that are markedly non-
Gaussian. Based on simulations, the Student t model breaks down in that case unless
something is done to make the statistic more Gaussian. Two good examples are the
bounded statistics variance and correlation coefﬁcient, which must be nonnegative or lie
on [1, 1], respectively. The solution for the variance is to jackknife logs rather than raw
estimates. Let
^s2
k


be a set of raw delete-one variance estimates, so that
log^s2

i ¼ log
1
N
X
N
k ¼ 1
i 6¼ k
^s2
k
0
B
B
@
1
C
C
A
(8.24)
log s2 ¼ 1
N
X
N
j¼1
log^s2

j
(8.25)
^s2
J ¼ N  1
N
X
N
i¼1
log^s2

i

 log s22
(8.26)
Because of the log transformation,
log
ð
^s2

i  log s2
=^sJ e tN1, and approximate conﬁ-
dence intervals can be constructed in the usual way:
Pr setN1 1α=2
ð
Þ^sJ  λ  setN1 1α=2
ð
Þ^sJ


¼ 1  α
(8.27)
To jackknife the correlation coefﬁcient, an inverse hyperbolic tangent transformation is
made before proceeding in a like manner.
Example 8.19 Use the jackknife to estimate the skewness and its standard error for the Old
Faithful data set.
n = length(geyser);
skew = skewness(geyser)
skew =
-0.3392
skewt = zeros(1, n);
for i =1:n
geyt = geyser;
279
8.4 The Jackknife
.009
13:32:30, subject to the Cambridge Core terms of use,

geyt(i) = []; % leave i-th point out
skewt(i) = skewness(geyt);
end
mean(skewt)
ans =
-0.3392
sqrt((n - 1)/n*sum((skewt - mean(skewt)).^2))
ans =
0.1004
Note that the same result is obtained by directly computing the skewness from the data
and by averaging the delete-one estimates, and that a slightly different standard deviation
as compared with the bootstrap ensues.
MATLAB provides the function jackstat = jackknife(jackfun, x) to compute the jack-
knife from the N  p array x using the function handle jackfun and returns the N  p array
of jackknife statistics jackstat. Example 8.19 can be obtained more simply as
jackstat = jackknife(@skewness, geyser);
sqrt((n - 1)/n*sum((jackstat - mean(jackstat)).^2))
ans =
0.1004
Example 8.20 Compute the median and its standard error for the geyser data using the
jackknife and the bootstrap. The sample median of the data is 76.
Use the same scripts as in Example 8.8 and 8.19 but change the skewness function to the
median function. The result for the jackknife is zero, which does not make much sense on
initial glance. The delete-one estimates must be identical to account for a zero standard
error. The result with the bootstrap is 76.2390. The problem is that the median is not a
smooth statistic, and the jackknife breaks down for estimators based on order statistics.
However, the bootstrap continues to function satisfactorily.
280
Resampling Methods
.009
13:32:30, subject to the Cambridge Core terms of use,

10
Multivariate Statistics
10.1 Concepts and Notation
Multivariate statistics began to be investigated in the 1920s through the work of Wishart,
Hotelling, Wilks, and Mahalanobis, among others. The standard but somewhat advanced
textbook on the subject is Anderson (2003), which is now in its third edition. Two more
approachable general texts are Mardia, Kent, & Bibby (1979) and Rencher (1998). The
scope of multivariate statistics is vast, and this chapter covers only a subset of the available
techniques.
One of the major issues in multivariate statistics is notation because most statistical
entities are vectors or matrices rather than the scalars typical of univariate statistics.
Suppose that there are N random observation vectors of p variables. These may be written
as the N  p data matrix X
$. The 1  p sample mean vector is XN ¼ jN X
$ =N, where jN is
a 1  N vector of ones, and  denotes the inner product.
The population mean vector or expected value of the data matrix X
$ is deﬁned to be the
1  p vector of expected values of the p variables
EðX
$Þ ¼ E x1; . . . ; xp


¼

μ1, . . . , μp

¼ μ
(10.1)
where xi is N  1. It directly follows that E XN


¼ μ.
The symmetric matrix of sample variances and covariances is the sample covariance
matrix
S
$¼
s11
s12
:
s1p
s21
s22
:
s2p
:
:
:
:
sp1
sp2
:
spp
0
B
B
@
1
C
C
A
(10.2)
where the biased version of the covariance obtained by normalizing by 1=N is typically
used. The unbiased sample covariance is obtained by normalizing by N  1 because a
single linear constraint from estimating the mean applies to each column of X
$ and will be
denoted as S
$0. The diagonal elements of S
$ or S
$0 are the sample variances of the p variables,
whereas the off-diagonal element in the j; k
ð
Þ position is the sample covariance between
variable j and k.
A measure of overall variability of the data set is the generalized sample variance det ðS
$Þ
or the total sample variance trðS
$Þ. The generalized sample variance is a more complete
representation of the sample variance because it incorporates the off-diagonal terms,
whereas the total variance is simply the sum of all the variances.
344
.011
13:42:10, subject to the Cambridge Core terms of use,

The biased form of the sample covariance matrix may be written most succinctly in
terms of the data matrix
S
$¼ 1
N X
$T 
I
$
N  1
N J
$
N


 X
$
(10.3)
where I
$
N is the N  N identity matrix, and J
$
N is an N  N matrix of ones. It can be easily
shown that

I
$
N  J
$
N=N


eIN  J
$
N=N

¼

I
$
N  J
$
N=N

[i.e.,

I
$
N  J
$
N=N

is idem-
potent], and so

I
$
N  J
$
N=N

is the centering matrix that changes X
$ into X
$ XN. It can
also be shown that S
$ is at least positive semideﬁnite, meaning that all its eigenvalues are
nonnegative.
The population covariance matrix is
Σ
$ ¼ cov

X
$
¼ E

X
$ μ
T 

X
$ μ

h
i
¼
σ11
σ12
:
σ1p
σ21
σ22
:
σ2p
:
:
:
:
σp1
σp2
:
σpp
0
B
B
@
1
C
C
A
(10.4)
The population covariance matrix is symmetric and positive deﬁnite if there are no
linear relationships between the p variables. It follows that E

S
$
¼ N Σ
$ = N  1
ð
Þ or
E

S
$0
¼ Σ
$.
The sample correlation matrix is
R
$ ¼
1
r12
:
r1p
r21
1
:
r2p
:
:
:
:
rp1
rp2
:
1
0
B
B
@
1
C
C
A
(10.5)
where
rjk ¼ sjk=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
sjjskk
p
. R
$ may be related to S
$ by deﬁning the diagonal matrix
D
$
s ¼ diag

S
$
h
i1=2
so that S
$¼ D
$
s  R
$  D
$
s and R
$¼ D
$1
s
 S
$  D
$1
s . If the data matrix X
$
has been standardized to Z
$ by removing the sample mean XN from each entry and dividing
by the corresponding sample standard deviation, then S
$¼ R
$.
The population correlation matrix is
Ρ
$ ¼
1
ρ12
:
ρ1p
ρ21
1
:
ρ2p
:
:
:
:
ρp1
ρp2
:
1
0
B
B
@
1
C
C
A
(10.6)
where ρjk ¼ σjk=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σjjσkk
p
. A diagonal matrix D
$
σ analogous to D
$
s may be deﬁned such that
Σ
$¼ D
$
σ  Ρ
$  D
$
σ and Ρ
$¼ D
$1
σ  Σ
$  D
$1
σ
. However, R
$ is a biased estimator for Ρ
$. The
sample correlation matrix based on the unbiased sample covariance matrix will be denoted R
$0.
A multivariate measure of the distance between two vectors y1 and y2 must account for
the covariances as well as the variances of the variables, so the simple Euclidean distance
y1  y2
ð
ÞT  y1  y2
ð
Þ does not sufﬁce. The Mahalanobis distance standardizes distance
using the inverse of the unbiased covariance matrix
345
10.1 Concepts and Notation
.011
13:42:10, subject to the Cambridge Core terms of use,

d2 ¼ y1  y2
ð
ÞT  S
$01  y1  y2
ð
Þ
(10.7)
The inverse covariance matrix transforms the variables so that they are uncorrelated and
have the same variance, resulting in a rational distance measure.
10.2 The Multivariate Gaussian Distribution
10.2.1 Derivation of the Multivariate Gaussian Distribution
Most multivariate (mv) inferential procedures are based on the mv Gaussian distribution or
distributions derived from it. The mv Gaussian distribution is a direct generalization of the
univariate or bivariate Gaussian covered in Sections 3.4.1 and 3.4.10.
Suppose that there is a random vector z ¼ z1; . . . ; zp


, where each zi e N 0; 1
ð
Þ, and the
zi
f g are mutually independent. Their joint pdf is just the product of the marginal pdfs
Np 0; I
$
p


¼
1ﬃﬃﬃﬃﬃ
2π
p

p ezzT=2
(10.8)
A more general mv normal pdf with mean vector μ and covariance matrix Σ
$ may be
obtained in an analogous manner to the derivation of the bivariate normal distribution in
Section 3.4.10. Let x ¼ Σ
$1=2  zT þ μ, where the square root of the population covariance
matrix can be obtained by ﬁnding its p eigenvalues Λ
$ ¼ diag λi
ð Þ and eigenvector matrix
C
$, so Σ
$1=2 ¼ C
$ Λ
$1=2  C
$T, with Λ
$1=2 ¼ diag
ﬃﬃﬃﬃλi
p


. The expected value and variance for x
are μ and Σ
$, respectively.
Writing z in terms of x, substituting into the joint pdf, and performing the transformation
gives
N p μ; Σ
$


¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2π
ð
ÞpjΣ
$j
q
e xμ
ð
ÞΣ
$1 xμ
ð
ÞT=2
(10.9)
where the vertical bars in the denominator denote the determinant, not the absolute value.
Note that the exponent is the Mahalanobis distance between x and μ, where for the
univariate Gaussian it is the simple Euclidean distance divided by the variance, and the
generalized variance replaces the simple variance in the denominator. The p-variate
Gaussian distribution (10.9) contains p means, p variances, and
p
2


covariances, for a
total of p p þ 3
ð
Þ=2 parameters.
The central and noncentral moments of x are deﬁned analogously to those for the
univariate distribution. The ﬁrst moment is the mean vector μ. The second central moment
is E
	
ðxj  μjÞ  xk  μk
ð
Þ

¼ σjk, or the entries in the population covariance matrix. The
third central moment is zero, as are all further odd-order moments. The fourth central
moment
is
E
	
xi  μi
ð
Þ  ðxj  μjÞ  xk  μk
ð
Þ  xl  μl
ð
Þ

¼ σijσkl þ σikσjl þ σilσjk.
For
346
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

higher orders, even central moments become more complicated. Fortunately, they are
rarely needed.
MATLAB implements the cdf, pdf, and random number generation from the mv
Gaussian distribution as mvnpdf(x, mu, sigma) and so on, where mu is a vector and sigma
is a matrix. Neither distribution objects nor the quantile function are supported for the mv
Gaussian.
10.2.2 Properties of the MV Gaussian Distribution
Some key properties of the mv normal distribution will be stated and discussed without
proof (which can be found in standard texts). Suppose a linear transformation A
$ is applied
to a p-variate random Gaussian vector x. Further, suppose that A
$is q  p and has rank
q  p. The distribution of the linear transformation applied to x is
A
$ x  Nq A
$ μ; A
$  Σ
$ A
$T


(10.10)
Equation (10.10) holds when q ¼ 1 and A
$ is a p-vector a.
The linear transformation (10.10) can be used to standardize mv Gaussian variables in a
different way than the transformation x  μ
ð
Þ 

Σ
$1=21 that was applied to get (10.9).
Any symmetric positive deﬁnite matrix can be factored into the product of a lower
triangular matrix and its transpose using the Cholesky decomposition. Let Σ
$ ¼ E
$ E
$T be
the Cholesky decomposition of the population covariance matrix. Then a standardized
version of a Gaussian random vector x is given by z ¼ x  μ
ð
Þ  E
$1. In both cases, z is
distributed as N p 0; Ip


, meaning that each zi is independently distributed as N 0; 1
ð
Þ.
If x e Np μ; Σ
ð
Þ, then any subvector of x is mv normal with the corresponding means,
variances, and covariances. For example, if the ﬁrst r elements of x are the subvector, then
the result is N r

μr, Σ
$
r

, where μr contains the ﬁrst r elements in μ, and Σ
$
r is the upper left
r  r partition of Σ
$. This establishes that the marginal distributions for the mv Gaussian are
also mv Gaussian. It follows that each xi is distributed as N μi; σii
ð
Þ. The reverse is not true;
if each variable xi in a set of variables is distributed as N μi; σii
ð
Þ, it does not follow that x is
mv Gaussian.
If x and y are jointly mv Gaussian with Σ
$
xy ¼ 0, then x and y are independent. In
other words, zero covariance implies independence, and the converse also holds. The
mv Gaussian is the only mv distribution with this property. If x e Np

μ, Σ
$ 
, then any
two variables xi and xj are independent if σij ¼ 0. Further, when two Gaussian vectors
x and y are the same size with zero covariance, they can be added or subtracted
according to
x  y e Np μx  μy; Σ
$
xx þ Σ
$
yy


(10.11)
If x and y are jointly mv normal with Σ
$
xy 6¼ 0, then the conditional distribution of x given y
is mv normal with conditional mean vector and covariance matrix given by
347
10.2 The Multivariate Gaussian Distribution
.011
13:42:10, subject to the Cambridge Core terms of use,

E xjy
ð
Þ ¼ μx þ Σ
$
xy  Σ
$1
yy 

y  μy

cov xjy
ð
Þ ¼ Σ
$
xx  Σ
$
xy  Σ
$1
yy  Σ
$
yx
(10.12)
This deﬁnes the relationships of subvectors that are not independent.
The sum of the squares of p independent standardized Gaussian variables is chi square
with p degrees-of-freedom. For mv standardized Gaussian variables, if x e N p

μ, Σ
$
, then
the quadratic form x  μ
ð
Þ  Σ
$1  x  μ
ð
ÞT is χ2
p.
10.2.3 The Sample Mean Vector and Sample Covariance Matrix
The maximum likelihood estimators (mles) for the mean vector and population covariance
matrix may be derived in the same way as for the univariate normal distribution. If X
$
contains N samples from N p

μ, Σ
$
, then the mles for μ and Σ
$ are
^μ ¼ XN
^Σ
$ ¼ W
$ =N ¼ S
$
(10.13)
The total sum of squares matrix is
W
$ ¼

X
$ XN
T 

X
$ XN

(10.14)
Equations (10.13) and (10.14) can be proved in the same way as for the univariate
Gaussian, with the slight added complexity of dealing with matrices.
As for the univariate case, the mle is equivariant, so the mle of a function is given by
the function of the mle. For example, consider the mle for the population correlation
matrix Ρ
$¼ D
$1
σ  Σ
$ D
$1
σ , where D
$1
σ
¼ diag 1=σii
ð
Þ. The mle for 1=σii is 1=^σii, so
^Ρ
$ ¼ ^D
$1
σ
 ^Σ
$  ^D
$1
σ , which is the same as the mle for the sample correlation matrix R
$.
The properties of the sample mean vector and sample covariance matrix also resemble
those for the univariate case. For example, for an N  p data matrix X
$, the sample mean
vector XN e N p

μ, Σ
$=N

. Further, XN and S
$ are independent and jointly sufﬁcient for μ
and Σ
$. This can be shown by factoring the mv normal pdf and means that all the infor-
mation necessary to describe a random mv Gaussian sample X
$ is contained in XN and S
$.
The mv central limit theorem states that if XN is derived from a random sample X
$ from a
population with mean vector μ and covariance matrix Σ
$, then XN !
p N p

μ, Σ
$=N

. The
distribution of XN is asymptotically N p

μ, Σ
$=N

regardless of the distribution of X
$,
provided that the covariance matrix Σ
$ is ﬁnite.
The distribution of the sample covariance matrix S
$ is a multivariate analogue of the chi square
distribution called the Wishart distribution, as originally derived by Wishart (1928). If the
variables in X
$ are independent and distributed as Np

μ, Σ
$
, then the p p þ 1
ð
Þ=2 distinct
variables in

X
$ μ
T 

X
$ μ

are jointly distributed as the Wishart distribution W p

N, Σ
$
,
where
N
is
the
degrees-of-freedom.
Consequently,
the
p p þ 1
ð
Þ=2
elements
of
W
$ ¼

X
$ XN
T 

X
$ XN

¼ N S
$ are distributed as W p

N  1, Σ
$ 
, which reﬂects the
loss of one degree-of-freedom from estimating the sample mean of each column of X
$.
348
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

The Wishart distribution has a reproductive property like that for the chi square
distribution. If W
$
1 e W p

N1, Σ
$
and W
$
2 e W p

N2, Σ
$
and W
$
1 and W
$
2 are independent,
then W
$
1 þ W
$
2 e W p

N 1 þ N2, Σ
$ 
. Further, if W
$
e W p

N, Σ
$ 
and C
$ is a q  p constant
matrix of rank q  p, then C
$  W
$ C
$T e W q

N, C
$  Σ
$ C
$T
. The degrees-of-freedom for
the Wishart distribution have decreased, as evidenced by the fact that C
$  W
$ C
$T is q  q.
MATLAB supports the generation of random numbers from a Wishart distribution using
wishrnd(sigma, df), where sigma is the covariance matrix and df is the degrees-of-
freedom.
10.2.4 The Complex Multivariate Gaussian Distribution
The complex mv Gaussian distribution is important in spectral analysis and signal pro-
cessing. However, some of the concepts in Section 10.1 must be extended to handle
complex data before introducing the complex form of the Gaussian distribution.
A complex random vector x ¼ xr þ ixi has p dimensions, where xr and xi are, respect-
ively, its real and imaginary parts. The probability distribution of a complex random
vector is the joint distribution of the real and imaginary parts and hence is at least
bivariate. Its second-order statistics are described by the covariance and pseudocovar-
iance (sometimes called the complementary covariance) matrices (Van Den Bos 1995;
Picinbono 1996):
Γ
$¼ E
x  μx
ð
ÞH x  μx
ð
Þ
h
i
¼ Σ
$
xrxr þ Σ
$
xixi þ i Σ
$T
xrxi  Σ
$
xrxi


Γ
$
^
¼ E
x  μx
ð
ÞT x  μx
ð
Þ
h
i
¼ Σ
$
xrxr  Σ
$
xixi þ i Σ
$T
xrxi þ Σ
$
xrxi


(10.15)
where the superscript H denotes the complex conjugate transpose, and Σ
$ denotes the real
covariance matrix of the elements given by its subscript. The covariance matrix Γ
$ is
complex, Hermitian, and positive semideﬁnite, whereas the pseudocovariance matrix Γ
$
^
is
complex and symmetric.
A complex random vector x is proper if Γ
$
^
is identically zero and otherwise is improper.
The conditions on the covariance matrices of the real and imaginary parts for propriety
reduce to
Σ
$
xrxr ¼ Σ
$
xixi
Σ
$
xrxi ¼ Σ
$T
xrxi
(10.16)
where the second equation in (10.16) requires that the diagonal elements of Σ
$
xrxi vanish.
The complex covariance matrix for proper data is then given equivalently by the sum or
difference of the two equations in (10.15). However, in the improper case, both Γ
$ and Γ
$
^
are required for a complete description of the second-order statistics of a complex random
vector.
Extending statistical deﬁnitions from the real line to the complex plane is a topic that is
not without controversy. In the signal processing literature, where there has been extensive
recent work, the guiding approach has been that deﬁnitions and principles should be the
same in the real and complex domains. Schreier & Scharf (2010) provide a comprehensive
349
10.2 The Multivariate Gaussian Distribution
.011
13:42:10, subject to the Cambridge Core terms of use,

recent survey. This has led to the concept of so-called augmented variables and covariance
matrices. The augmented complex random variable for x is given by _x ¼ x x∗
ð
Þ, where
the superscript * denotes the complex conjugate and is obtained by adding the p variables
of x∗to those of x. Its elements x and x∗are clearly not independent, but augmented
variables simplify statistical algebra. The augmented covariance matrix is given by
_Γ
$ ¼
Γ
$
Γ
$
^
Γ
$
^
∗
Γ
$∗
2
4
3
5
(10.17)
where _Γ
$ is block structured, Hermitian, and positive semideﬁnite.
Using the augmented variable notation, the pdf of a complex Gaussian random vector is
_Cp
_μ; _Γ
$


¼ e
 1=2
ð
Þ
_x
_μ



_Γ
$1

_x
_μ

H
πp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
_Γ
$


r
(10.18)
and is viewed as the joint pdf of xr and xi pertaining to both proper and improper variables.
By the block matrix inversion lemma, the inverse of the augmented covariance matrix is
_Γ
$ 1
¼
Γ
$  Γ
$
^
 Γ
$∗

1
 Γ
$
^
∗
"
#1
 Γ
$  Γ
$
^
 Γ
$∗

1
 Γ
$
^
∗
"
#1
 Γ
$
^
 Γ
$∗

1
 Γ
$∗ Γ
$
^
∗ Γ
$1 Γ
$
^
"
#1
 Γ
$
^
∗ Γ
$1
Γ
$∗ Γ
$
^
∗ Γ
$1 Γ
$
^
"
#1
8
>
>
>
>
>
<
>
>
>
>
>
:
9
>
>
>
>
>
=
>
>
>
>
>
;
(10.19)
In older works, the complex Gaussian distribution is usually implicitly taken to be the
proper form, in which case Γ
$
^
¼ 0 and _Γ
$ is block diagonal. The proper complex Gaussian
distribution is given by
Cp μ; Γ
$


¼ e 1=2
ð
Þ xμ
ð
ÞΓ
$1 xμ
ð
ÞH
πpjΓ
$j
(10.20)
10.3 Hotelling’s T2 Tests
Multivariate hypothesis testing is more complicated than its univariate counterpart because
the number of hypotheses that can be posed is much larger and because it is generally
preferable to test p variables with a single test rather than performing p univariate tests. The
single test preserves the signiﬁcance level, generally has a higher power because multi-
variate tests include the effect of correlation between variables, and enables testing of the
contribution of each variable to the result.
350
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

Consider the test H0: μ ¼ μ0 versus H1: μ 6¼ μ0 when the covariance matrix Σ
$ is known.
The inequality in the alternate hypothesis implies that at least one of the parameters
μj 6¼ μ0j. A random sample of N observations X
$ is distributed as Np

μ, Σ
$
, from which
the sample mean vector XN is calculated. The test statistic is
^Z
2 ¼ N XN  μ0


 Σ
$1  XN  μ0

T
(10.21)
or the Mahalanobis distance between XN and μ0 scaled by N and is distributed as χ2
p if the
null hypothesis is true. The test rejects H0 if ^Z
2  χ2
p 1  α=2
ð
Þ \ ^Z
2 < χ2
p α=2
ð
Þ, and
p-values can be obtained in the usual way. For the univariate case using (10.21), the test
statistic is the square of the z statistic of Section 6.3.1 that is tested against the chi square
distribution with one degree-of-freedom. This test is based on a likelihood ratio and hence
has concomitant optimality properties.
When the covariance matrix Σ
$ is unknown, the multivariate counterpart to the one-
sample t test described in Section 6.3.2 pertains. Given a random sample of N observations
X
$ distributed as Np μ; Σ
$


, where Σ
$ is unknown, the test is again H0: μ ¼ μ0 against H1:
μ 6¼ μ0. The test statistic is
^T
2 ¼ N XN  μ0


 S
$01  XN  μ0

T
(10.22)
or, equivalently, may be obtained from (10.21) by replacing the population covariance
matrix with the unbiased sample covariance matrix S
$0. The test statistic is simply the
Mahalanobis distance between the observed and postulated means scaled by N.
The statistic in (10.22) has Hotelling’s T2 distribution with dimension p and degrees-of-
freedom N  1 if the null hypothesis is true and was ﬁrst derived by Hotelling (1931). The
test rejects H0 if ^T
2  T2
p,N1 1  α=2
ð
Þ \ ^T
2 < T2
p,N1 α=2
ð
Þ, and the two-sided p-value
follows in the usual way. When p = 1, the T 2 statistic reduces to the square of the univariate
t statistic t2
N1, which, in turn, is equivalent to F1,N1. As N ! ∞, Hotelling’s T2 becomes
χ2
p. Just as the univariate t statistic is derived from a likelihood ratio, Hotelling’s T2 is a
likelihood ratio test with concomitant optimality properties. The T2 statistic is invariant
under afﬁne transformations, and the test is UMP.
Let X
$ be distributed as Np

μ, Σ
$Þ, and let ^T 2 be given by (10.22). The distribution of T2
can be derived in an analogous manner to the univariate t distribution through the method
introduced in Section 2.9. A more useful approach in practice is to convert the distribution
of T2 to the F distribution using
N  p
N  1
ð
Þp T2
p,N1 e Fp,Np
(10.23)
This allows tests to be made using the F distribution with argument given by (10.22). It
also provides the null and alternate distribution for hypothesis tests and power calculations.
Matlab has no built-in functions to compute Hotelling’s T2 statistic, presumably for this
reason.
351
10.3 Hotelling’s T2 Tests
.011
13:42:10, subject to the Cambridge Core terms of use,

Since N XN  μ0


 S
$01  XN  μ0

T
e T2
p,
N1, the probability statement
Pr N XN  μ


 S
$01  XN  μ

T  T2
p,N1 α
ð Þ
h
i
¼ 1  α
(10.24)
can be used to derive a 1  α conﬁdence region on μ given by all vectors μ that satisfy
N XN  μ


 S
$01  XN  μ

T  T 2
p,N1 α
ð Þ, which describes a hyperellipsoid centered at
μ ¼ XN and is equivalent to mapping out those μ0 that are not rejected by the upper tail T 2
test of H0: μ ¼ μ0. However, this process is difﬁcult to visualize for p > 2.
The conﬁdence interval on a single linear combination a  μT for a ﬁxed vector a is given
by
a  X
T
N  tN1 α=2
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a  S
$0  aT
N
s
 a  μT  a  X
T
N þ tN1 α=2
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a  S
$0  aT
N
s
(10.25)
This encompasses a single mean value μj by making all the elements in a zero except for
the jth and also allows for linear combinations of the parameters. The outcome is identical
to the univariate case and is appropriate when the variables are independent, meaning that
S0
$
is diagonal or at least diagonally dominant.
Simultaneous conﬁdence intervals for all possible values of a  μT obtained by varying a
are given by
a  X
T
N  Tp,N1 α
ð Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a  S
$0  aT
N
s
 a  μT  a  X
T
N þ T p,N1 α
ð Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a  S
$0  aT
N
s
(10.26)
The probability that all such intervals generated by all choices of a will simultaneously
contain a  μT is 1  α. For example, simultaneous conﬁdence intervals on a set of p
sample mean estimates could be obtained by setting a in turn to 1; 0; . . . ; 0
ð
Þ, 0; 1; . . . ; 0
ð
Þ,
:::, 0; . . . ; 0; 1
ð
Þ. These intervals are much wider than would be obtained under univariate
reasoning [i.e., T p,N1 α
ð Þ >> tN1 α=2
ð
Þ except when p = 1]. For example, with N = 25
and p = 10, Tp,N1 α
ð Þ = 6.38 and tN1 α=2
ð
Þ = 1.71 when α = 0.05. Using a Bonferroni
approach where α is replaced by α=10 yields tN1 α=20
ð
Þ = 3.09, which is still much
smaller than the simultaneous conﬁdence interval. The Hotelling’s T 2 conﬁdence interval
is more conservative than the Bonferroni one because it accounts for correlations among
the variables.
Example 10.1 For the medical tablet data in Example 6.18, suppose that it is postulated that
the test value should be 4.1. This is very nearly the grand mean of all the data, or 4.0917.
The mean for each laboratory can be tested individually against this value with a univariate
t test to get the p-values, and the hypothesis that all the laboratory means simultaneously
have this value can be evaluated using Hotelling’s T 2 test.
tablet = importdata('tablet.dat');
[n, m] = size(tablet);
352
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

ybar = mean(tablet)
ybar =
4.0590
4.0180
4.0570
4.1210
4.1700
4.1150
4.1020
ystd = std(tablet);
t = sqrt(n)*(ybar - 4.1)./ystd
t =
-2.5487
-4.4646
-5.0952
1.2287
5.1542
1.6270
0.1206
2*(1 - tcdf(abs(t), n - 1))
ans =
0.0313
0.0016
0.0006
0.2503
0.0006
0.1382
0.9067
The t tests reject at the 0.05 level for the ﬁrst, third, and ﬁfth laboratories. Using the
Bonferroni method, the rejection threshold is 0.005, and the test rejects for the ﬁrst, second,
third, and ﬁfth laboratories. Using the Benjamini-Hochberg method from Section 6.6, the
rejection threshold is 0.0016, and the test rejects for the ﬁrst, second, third, and ﬁfth
laboratories, just as for the Bonferroni approach.
For the multivariate test using Hotelling’s T 2,
s = cov(tablet);
t2 = n*(ybar - 4.1*ones(size(ybar)))*inv(s)*(ybar - 4.1*ones
(size(ybar)))'
t2 =
422.2370
2*min(fcdf((n - m)*t2/(m*(n - 1)), m, n - m), 1 - fcdf((n -
m)*t2/(m*(n - 1)), m, n - m))
ans =
0.0317
The p-value is below the 0.05 threshold, and hence the null hypothesis that all the
laboratories simultaneously return a value of 4.1 is rejected, although not strongly.
Simultaneous conﬁdence intervals can be placed on the sample means of the data from
each of the seven laboratories.
t2 = (n - 1)*m/(n - m)*ﬁnv(.95, 7, 3)
t2 =
186.6216
For this example,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
T 2
7,3 0:95
ð
Þ
q
¼ 13:661, whereas t9 0:975
ð
Þ ¼ 2:2622, which is about
a factor of 6 difference, so the simultaneous conﬁdence intervals will be over an order of
magnitude wider than Bonferroni conﬁdence intervals.
for i = 1:7
a = zeros(size(ybar));
a(i) = 1;
[a*ybar' - sqrt(t2)*sqrt(a*s*a'/n), a*ybar '+ sqrt(t2)
*sqrt(a*s*a'/n)]
end
353
10.3 Hotelling’s T2 Tests
.011
13:42:10, subject to the Cambridge Core terms of use,

ans =
3.8392
4.2788
ans =
3.7671
4.2689
ans =
3.9417
4.1723
ans =
3.8875
4.3545
ans =
3.9845
4.3555
ans =
3.9891
4.2409
ans =
3.8754
4.3286
Two-sample T 2 tests can also be derived that are analogous to the two-sample t test. In this
case, the mean vectors from two populations are being compared. Assume that N1 samples
in X
$ are distributed as Np

μ1, Σ
$
and that N2 samples in Y
$ are distributed as Np

μ2, Σ
$
,
where Σ
$ is unknown but the same for both samples. The sample means XN1 and YN2 are
computed along with the sample sum of squared residuals W
$
1 ¼ N1S
$
1 and W
$
2 ¼ N2S
$
2.
A pooled estimator for the covariance matrix is S
$0
p ¼

W
$
1 þ W
$
2

= N1 þ N 2  2
ð
Þ, for
which E

S
$0
p

¼ Σ
$. Then a test of H0: μ1 ¼ μ2 versus H1: μ1 6¼ μ2 is based on the test
statistic
^T
2 ¼
N1 N 2
N 1 þ N2  2 XN1  YN2


 S
$01
p
 XN1  YN2

T
(10.27)
that is distributed as T 2
p,N1þN22 when the null hypothesis is true. It rejects if the test
statistic exceeds the critical value of T2
p,N1þN22 α
ð Þ.
The univariate two-sample t test is remarkably robust to departures from the model
assumptions of equal variance of the two populations and normality. The multivariate
T2test is also robust, although not to the same degree.
10.4 Multivariate Analysis of Variance
A data set contains N random samples of p-variate observations from each of M Gaussian
populations with equal covariance matrices. The one-way multivariate analysis of variance
(MANOVA) hypothesis is H0: μ1 ¼    ¼ μM versus H1: at least two μi are unequal. Note
that the test is against the populations rather than the variables, in contrast to Section 10.3,
and that there are MN observations in total. The distinction between ANOVA, as described
in Section 6.3.7, and MANOVA is that the samples are p-variate instead of univariate. The
test procedure is analogous to that for ANOVA, with additional complexity because the
354
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

between and within estimators are matrices rather than scalars, and the null distribution is a
multivariate generalization of the F distribution.
Let X
$ denote the N  M  p data matrix, which can be represented as a parallelepiped,
and denote the N  M face at the ith variable index by X
$
i. The i, j element of the between
or hypothesis matrix is
Hij ¼ 1
N jN  X
$
i 
I
$
M  1
M J
$
M


 X
$
j
T
 jT
N
(10.28)
whereas the corresponding element of the within or error matrix is
Eij ¼ tr X
$
i  X
$
j
T


 1
N jN  X
$
i  X
$
j
T
 jT
N
(10.29)
H
$ is p  p and has the sum of squares explained for each variable on its diagonal. The off-
diagonal components are analogous sums for all possible pairs of variables. E
$ is p  p and
has the sum of squared errors on its main diagonal with analogous sums for all possible
pairs of variables in the off-diagonal entries. There are nH ¼ M  1 degrees-of-freedom
for the between and nE ¼ M N  1
ð
Þdegrees-of-freedom for the within estimator. The rank
of H
$ is the smaller of M  1 and p, whereas the rank of E
$ must be p.
Four standard test statistics have been proposed to test the pair of MANOVA hypoth-
eses. Only the likelihood ratio test originally presented by Wilks (1932) will be considered
here, and it has test statistic
^Λ ¼
E
$


E
$ þ H
$


(10.30)
Equation (10.30) is known as Wilks’ Λ and has a range of (0, 1). Wilks’ Λ is distributed as
Wilks’ Λ distribution if (1) E
$ is distributed as the Wishart distribution W p

nE, Σ
$
, ( 2) H
$ is
distributed as W p

nH, Σ
$
under the null hypothesis, and (3) E
$ and H
$ are independent, all
of which hold if the data are multivariate normal with the same covariance matrix. The test
rejects if the test statistic ^Λ  Λp,nH,nE α
ð Þ. Rejection occurs when the test statistic is smaller
than the critical value, in contrast with Hotelling’s T2.
The Wilks’Λ distribution is analogous to the F distribution in univariate statistics and is
indexed by three parameters: the dimensionality p and the degrees-of-freedom for the
between and within estimators. The degrees-of-freedom parameters are the same as for
one-way ANOVA described in Section 6.3.7, so the addition of the dimensionality param-
eter distinguishes MANOVA from it. The Wilks’ Λ distribution has symmetries such that
Λp,nH,nE and ΛnH,p,nEþnHp are the same. Further, for ﬁxed values of nE and nH, the critical
values of Wilks’ Λ decrease as the dimensionality p rises. This means that the addition of
variables that do not improve the ability to reject the null hypothesis will lead to
decreased power.
The distribution can be written as a product of beta distributions, which is difﬁcult to
work with. However, exact relationships between Wilks’ Λ distribution and the F distribu-
tion exist when p = 1 or 2 or when nH ¼ 1 or 2, as given in Table 10.1.
355
10.4 Multivariate Analysis of Variance
.011
13:42:10, subject to the Cambridge Core terms of use,

For other values of p and M, an approximate relationship due to Rao (1951) is
^F ¼ 1  ^Λ1=t


n2
Λ1=tn1
(10.31)
w ¼ nH þ nE  p þ nH þ 1
ð
Þ=2
(10.32)
t ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2nH 2  4
ð
Þ= p2 þ n2
H  5
ð
Þ
q
(10.33)
n1 ¼ pnH
(10.34)
n2 ¼ wt  pnH  2
ð
Þ=2
(10.35)
^F is approximately distributed as Fn1,n2 and may be assessed against the F distribution in
the usual way. This approach is more accurate than using the χ2 approximation for a
likelihood ratio test. MATLAB does not provide support for Wilks’ Λ distribution, so an
approximate method must be applied directly. The following function provides that
support.
function [pval, lambda] = WilksLambda(h, e, p, n, m)
%Computes the Wilks lambda statistic and double sided p-value
%from the error and hypothesis matrices using Rao (1951) F
approximation
%Input arguments
% e error matrix
% h hypothesis matrix
% p dimensionality
% n hypothesis degrees of freedom
% m error degrees of freedom
lambda = det(e)/det(e + h);
if p == 1
f = m*(1 - lambda)/(n*lambda)
n1 = n;
n2 = m;
Table 10.1 Relation between ^ and F Statistics
Parameters
F
Degrees-of-freedom
Any p, nH ¼ 1
nE þ nH  p
ð
Þ 1  Λ
ð
Þ
pΛ
p, nE þ nH  p
Any p, nH ¼ 2
nE þ nH  p  1
ð
Þ 1 
ﬃﬃﬃ
Λ
p


p
ﬃﬃﬃ
Λ
p
2p, 2 nE þ nH  p  1
ð
Þ
Any nH, p = 1
nE 1  Λ
ð
Þ
nHΛ
nH, nE
Any nH, p = 2
nE  1
ð
Þ 1 
ﬃﬃﬃ
Λ
p


nH
ﬃﬃﬃ
Λ
p
2nH, 2 nE  1
ð
Þ
356
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

elseif p == 2
f = (m - 1)*(1 - sqrt(lambda))/(n*sqrt(lambda));
n1 = 2*n;
n2 = 2*(m - 1);
elseif n == 1
f = (n + m - p)*(1 - lambda)/(p*lambda);
n1 = p;
n2 = n + m - p;
elseif n == 2
f = (n + m - p - 1)*(1 - sqrt(lambda))/(p*sqrt(lambda));
n1 = 2*p;
n2 = 2*(n + m - p - 1);
else
n1 = p*n;
w = n + m - (p + n + 1)/2;
t = sqrt((p^2*n^2 - 4)/(p^2 + n^2 -5));
n2 = w*t - (p*n -2)/2;
f = n2*(1 - lambda^(1/t))/(n1*lambda^(1/t));
end
pval = 2*min(fcdf(f, n1, n2), 1 - fcdf(f, n1, n2));
end
MATLAB does provide support for one-way MANOVA through the function [d, p,
stats] = manova1 (x, groups). The matrix x is n  p, with each row corresponding to a
single observation of p variables. The matrix groups sorts the variables in x into
groups in a variety of ways. The returned variable d is the dimensionality of the space
containing the group means. If d = 0, there is no evidence to reject the null hypothesis
that the means of the groups are identical. If d = 1, the null hypothesis is rejected at
the 0.05 level, but the multivariate means may lie on a line. If d = 2, the null
hypothesis is rejected at the 0.05 level, and the means do not lie on a line, but they
may lie on a plane, and so on. The returned variables p are the corresponding p-values.
The returned variable stats is a structure that contains 14 additional variables. How-
ever, it appears that manova1 tests the result against a less accurate chi square
approximation to the Wilks’ ^ distribution rather than the F approximation (10.31)–
(10.35).
Example 10.2 Zonally averaged temperature data are available at http://data.giss.nasa.gov/
gistemp/ for various ranges of latitudes starting in 1880 and extending to date. The data
through 2015 will be used for this example. The data are available with and without the
inclusion of sea surface temperature (SST) and are presented as a temperature anomaly
relative to a base period of 1951–80. Such data have ﬁgured strongly in the current debate
about global warming. The data that include SST will be evaluated for signiﬁcance of
change over time by binning them into age groups and performing a MANOVA test. The
data are plotted in Figure 10.1.
357
10.4 Multivariate Analysis of Variance
.011
13:42:10, subject to the Cambridge Core terms of use,

The northern and southern hemisphere data are the most strongly averaged results.
Dividing the 136 years of data into 10 year groups, Wilks’ Λ with p = 2 measurements,
M = 13 groups, and N = 10 data per group is evaluated.
x
=
importdata('ZonAnnTs+dSST.dat');
%N
and
S
hemisphere
data in columns 3 and 4
p = 2;
m = 13;
n = 10;
xh = [];
i = 1:n;
for j = 1:m
xh(i, j, 1:p)= x((j - 1)*n + i, 3:4);
end
e = [];
h = [];
for i = 1:p
for j = 1:p
h(i, j) = sum(sum(xh(:, :, i)*(eye(m) - ones(m,m)/m)*
xh(:, :, j)'))/n;
e(i, j)=trace(xh(:, :, i)*xh(:, :, j)') - sum(sum
(xh(:, :, i)*xh(:, :, j)'))/n;
end
end
[pval, lambda] = WilksLambda(h, e, p, m - 1, n*(m - 1))
pval =
0
lambda =
0.0368
The p-value is ﬂoating-point zero; hence the null hypothesis that the means of the
northern and southern hemisphere data are the same over time is rejected. This ought not
1900
1950
2000
Year
–100
–50
0
50
100
150
Temperature Anomaly
Figure 10.1
Zonally averaged temperature anomaly data for the northern (solid) and southern (dashed) hemispheres.
358
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

to be a surprise because there is a long-term trend in the data, especially since 1980.
However, this result omits the data from 2010 to 2015. Repeating the result after skipping
the ﬁrst six years of data gives a p-value of 0 and a Wilks’ Λ statistic of 0.0396, yielding
the same conclusion.
The same analysis can be carried out using the MATLAB function, although the result is
not as easy to interpret. For the northern and southern hemisphere data,
groups = [];
for i = 1:13;
groups = [groups i*ones(1,10)];
end
x1 = x(7:136, 3:4);
[d, p, stats] = manova1(x1, groups)
d =
2
p =
1.0e-17 *
0.0000
0.7690
A value d = 0 would support the null hypothesis that the means are the same, and hence d =
2 rejects the null hypothesis.
The eight variable data set consisting of zonal averages extending from 9064, 6444,
4424, and 240 latitude on both sides of the equator will be considered next.
Figures 10.2 and 10.3 show the data. The variability is highest at polar latitudes, especially
for the southern hemisphere.
For the northern hemisphere data, p = 4, m = 13, and n = 10 for 10 year groups, yielding
^Λ = 0.0525. The p-value is 0, and hence the null hypothesis that the 10 year averages are
the same is rejected.
For both the northern and southern hemisphere data, the result is ^Λ = 0.0063. The
p-value is again 0, and hence the null hypothesis that the 10-year averages are the same is
rejected.
1900
1950
2000
Year
–200
–100
0
100
200
300
Temperature Anomaly
Figure 10.2
Northern hemisphere temperature anomaly estimates with a solid line indicating the 024, dashed line indicating
2444, dot-dash line indicating 4464, and dotted line indicating 6490 latitude bands.
359
10.4 Multivariate Analysis of Variance
.011
13:42:10, subject to the Cambridge Core terms of use,

The variability is much larger at polar latitudes, and in addition, the actual measurements
are sparsest at those locales. This is especially apparent for the most poleward southern
hemisphere data that are very smooth prior to 1904 because they are based only on SST
data and exhibit a lot of variability after that time as sparse land data became available. If
the most poleward data series for each hemisphere are excluded, for 10-year intervals the
result is ^Λ = 0.0140. The p-value is still 0, so the null hypothesis is rejected. The mean
temperature has changed over time based on this data set. However, the change is far from
just an increase and has spatial patterns that cannot be discerned from a simple comparison
of the means.
Applying the MATLAB manova1 function to the eight zonally averaged temperature
data from 1887 to 2015 gives a dimensionality of 4. MATLAB also provides the function
manovacluster(stats) to plot a dendrogram for the results. A dendrogram is a hierarchical
binary cluster tree. Figure 10.4 shows the result. The bottom of the diagram corresponds to
the individual decadal groups (1 is 1886–95, 2 is 1896–1905, and so on), and these are
joined together at a height that is proportional to the distance between the clusters. The
dendrogram clearly pulls out the four dimensions corresponding to the nearly constant
temperature from 1890 to 1930 (groups 1–5), the weak warming and cooling trend from
1900
1950
2000
Year
–300
–200
–100
0
100
200
Temperature Anomaly
Figure 10.3
Southern hemisphere temperature anomaly estimates with a solid line indicating the 024, a dashed line
indicating 2444, a dash-dot line indicating 4464, and a dotted line indicating 6490 latitude bands.
3 4 2 1 5 6 7 8 9 10 11 12 13
Decadal Group
4
6
8
10
12
Distance between Groups
Figure 10.4
Dendrogram for the MANOVA analysis of the NOAA temperature data.
360
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

1940 to 70, and the marked warming trend since that time in the northern hemisphere. The
latter has been broken into two clusters. This result is not changed much by omitting the
two poleward-most temperature anomalies.
Example 10.3 Four types of measurements were made on male Egyptian skulls recovered
from tombs from ﬁve different time periods ranging from 4000 BC to AD 150. The
measurements are maximal breadth of skull, basibregmatic (or maximum vertical) height
of skull, basialveolar (fore and aft) length of skull, and nasal height of skull. There are
30 measurements for each of the ﬁve time periods. The goal was to determine whether
there is any difference in skull size between the time periods that might indicate interbreed-
ing of the Egyptians with immigrant groups. This is a classic data set from Thomson &
Randall-Maciver (1905) that was made familiar to statisticians by Sir Ronald Fisher in the
1930s.
skulls = importdata('skulls.dat');
p = 4;
m = 5;
n = 30;
x = zeros(n, m, p);
for i = 1:n
for j = 1:m
x(i, j, 1:p) = skulls((j - 1)*n + i, 1:p);
end
end
e = [];
h = [];
for i = 1:p
for j = 1:p
h(i, j) = sum(sum(x(:, :, i)*(eye(m) - ones(m, m)/m)*x
(:, :, j)'))/n;
e(i, j)=trace(x(:, :, i)*x(:, :, j)') - sum(sum(x(:,
:, i)*x(:, :, j)'))/n;
end
end
[pval, lambda] = WilksLambda(h, e, p, m - 1, n*(m - 1))
pval =
1.4020e-06
lambda =
0.6636
The p-value is very small; hence the null hypothesis that there is no difference in skull size
between the time periods is rejected.
ANOVA tables for each variable over time will be examined to assess the univariate
relations among the data.
361
10.4 Multivariate Analysis of Variance
.011
13:42:10, subject to the Cambridge Core terms of use,

for j = 1:p
[pp, anovatab] = anova1(x(:, :, j))
end
pp =
1.8263e-04
anovatab =
'Source'
'SS'
'df'
'MS'
'F'
'Prob>F'
'Columns'
[502.8267]
[4]
[125.7067] [5.9546] [1.8263e-04]
'Error'
[3.0611e+03] [145] [21.1108]
[]
[]
'Total'
[3.5639e+03] [149] []
[]
[]
pp =
0.0490
anovatab =
'Source'
'SS'
'df'
'MS'
'F'
'Prob>F'
'Columns'
[229.9067]
[4]
[57.4767]
[2.4474] [0.0490]
'Error'
[3.4053e+03] [145] [23.4846]
[]
[]
'Total'
[3.6352e+03] [149] []
[]
[]
pp =
4.6364e-06
anovatab =
'Source'
'SS'
'df'
'MS'
'F'
'Prob>F'
'Columns' [803.2933]
[4]
[200.8233] [8.3057] [4.6364e-06]
'Error'
[3.5060e+03] [145] [ 24.1791] []
[]
'Total'
[4.3093e+03] [149] []
[]
[]
pp =
0.2032
anovatab =
'Source'
'SS'
'df'
'MS'
'F'
'Prob>F'
'Columns'
[
61.2000]
[ 4]
[15.3000]
[1.5070]
[0.2032]
'Error'
[1.4721e+03] [145] [10.1526]
[]
[]
'Total'
[1.5333e+03] [149] []
[]
[]
Only the last variable (nasal height of skull) accepts the univariate null hypothesis,
although the second one (fore and aft length of skull) only weakly rejects it. If a Bonferroni
approach is used, the second and last variables accept the null hypothesis. This remains
true using the Benjamini-Hochberg method. The remaining variables all demonstrate
signiﬁcant change through time.
10.5 Hypothesis Tests on the Covariance Matrix
Statistical testing of the structure of the covariance matrix is often used to verify assump-
tions for other tests, just as tests for the homogeneity of variance (either the F test or
362
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

Bartlett’s M test) were used to verify the assumptions behind a t test in Chapter 6. This
includes tests that the covariance matrix has a particular structure, tests for the equality of
two or more covariance matrices, and tests of independence.
10.5.1 Sphericity Test
The hypothesis that the p variables in a multivariate data set are independent and share a
common variance can be expressed as H0: Σ
$ ¼ σ2 I
$
p versus H1: Σ 6¼ σ2 I
$
p
$
, where σ2 is
the unknown common variance. This is called a sphericity test because the Mahalanobis
distance between the variables becomes independent of direction in p-space when the null
hypothesis is true. The test was introduced by Mauchly (1940).
The likelihood ratio for the sphericity test assuming multivariate Gaussian variables is
^λ ¼
S
$0


tr S
$0


=p
h
ip
8
>
<
>
:
9
>
=
>
;
N=2
(10.36)
The distribution of ^λ under the null hypothesis is unknown, and hence the Wilks’ chi
square approximation must be used. Consequently, the log likelihood
2 log ^λ ¼ N log
S
$0


tr S
$0


=p
h
ip
8
>
<
>
:
9
>
=
>
;
¼ N log ^U
(10.37)
is asymptotically chi square with p p þ 1
ð
Þ=2  1 degrees-of-freedom under the null
hypothesis. The degrees-of-freedom follow from the number of parameters in Σ
$, or
p p þ 1
ð
Þ=2, minus the number due to estimation of σ2.
One application of the sphericity test arises in determining whether a univariate or
multivariate ANOVA is appropriate. In general, one would expect p variables to be
correlated, and hence a multivariate test of the null hypothesis would be required. How-
ever, if the sphericity test accepts, then the variables are statistically uncorrelated, and
univariate ANOVA would sufﬁce.
Example 10.4 Returning to the Egyptian skull data in Example 10.3, apply the sphericity
test to determine if the variables are correlated.
p = 4;
m = 5;
n = 30;
s = cov(skulls);
u = p^p*det(s)/trace(s)^p;
test = -n*m*log(u)
test =
4.1000e+03
363
10.5 Hypothesis Tests on the Covariance Matrix
.011
13:42:10, subject to the Cambridge Core terms of use,

2*min(chi2cdf(test,p*(p+1)/2-1),1-chi2cdf(test,p*(p+1)/2-1))
ans =
0
The p-value is zero, and hence the sphericity test rejects. A MANOVA approach has to
be used, as was done in Example 10.3.
10.5.2 Comparing Covariance Matrices
An assumption behind the Hotelling’s T2 or Wilks’Λ tests is that the population covariance
matrices for the samples being compared are equal. This allows the sample covariance
matrices to be pooled to get an estimate of the population covariance matrix. If the
population covariance matrices are not the same, the Hotelling’s T 2 and Wilks’ Λ tests
are still reliable if the samples are both large and the same size. When these conditions
do not hold, it is useful to be able to test H0: Σ
$
1 ¼    ¼ Σ
$
k versus H1: not H0.
In the univariate case, the equality of two variances can be tested using the F test of
Section 6.3.4, and the equality of many variances can be tested using Bartlett’s M test, as
described in Section 6.3.5. The latter can be generalized to the multivariate case, as shown
in Rencher (1998, sec. 4.3). Assume that there are M independent samples of size ni from
multivariate normal distributions, where N ¼ PM
i¼1ni. The likelihood ratio test statistic is
^λ ¼
S
$0
1


n11
ð
Þ=2
   S
$0
M


nM1
ð
Þ=2
S
$0
p


NM
ð
Þ=2
(10.38)
Where S
$0
p is the pooled covariance matrix
S
$0
p ¼
P
M
i¼1
ni  1
ð
ÞS
$0
i
N  M
(10.39)
The test statistic lies between 0 and 1, with a larger value favoring the null hypothesis.
Further, each ni > p, or else (10.38) would be zero.
The statistic
^U ¼ 2 1  δ
ð
Þ log ^λ
¼ 1  δ
ð
Þ
N  M
ð
Þ log ^S
$0
p

 
X
M
i¼1
ni  1
ð
Þ log ^S
$0
i


"
#
(10.40)
where
δ ¼
2p2 þ 3p  1
6 p þ 1
ð
Þ M  1
ð
Þ
X
M
i¼1
1
ni  1 
1
P
M
i¼1
ni  1
ð
Þ
2
6664
3
7775
(10.41)
364
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

has a chi square distribution with M  1
ð
Þp p þ 1
ð
Þ=2 degrees-of-freedom. This result is
due to Box (1949). There is also an F approximation for the null distribution.
This test should be used with caution because it has been demonstrated that the Box M
test with equal sample sizes may reject when the heterogeneity of covariance is small, but
this does not affect the Hotelling’s or Wilks’ tests. The Box M test is also sensitive to
departures from normality that the T 2 and Λ tests are more robust against, notably kurtosis
of the distribution.
Example 10.5 Evaluate the equivalence of the covariance matrices for the global
temperature data in Example 10.2.
For the four northern hemisphere data sets,
p = 4;
m = 13;
n = 10;
i = 1:n;
for j = 1:m
xh(i, j, 1:p)= x((j - 1)*n + i, 8:8 + p - 1);
end
sp = 0;
s1 = 0;
for i = 1:m
s = cov(squeeze(xh(:, i, :)));
sp = sp + (n - 1)*s;
s1 = s1 + (n - 1)*log(det(s));
end
sp = sp/(m*(n
- 1));
delta = (2*p^2 + 3*p - 1)/(6*(p + 1)*(m - 1))*(m/(n - 1) - 1/(m*(n - 1)));
u = (1 - delta)*(m*(n - 1)*log(det(sp)) - s1)
u =
129.4492
The test accepts the null hypothesis that the covariance matrices for the 13 groups are the
same. Consequently, it appears that the 10-year means of the data are different, but their
covariance structures are similar, and hence the MANOVA analysis in Example 10.2
is solid.
10.5.3 Test of Independence
Suppose that an N  p matrix of observations X
$ is partitioned into an N  p1 submatrix
X
$
1 and an N  p2 submatrix X
$
2. The population covariance matrix then partitions into
365
10.5 Hypothesis Tests on the Covariance Matrix
.011
13:42:10, subject to the Cambridge Core terms of use,

Σ
$ ¼
Σ
$
11
Σ
$
12
Σ
$
21
Σ
$
22
"
#
(10.42)
and the sample covariance matrix has a similar form. The null hypothesis that X
$
1 and X
$
2
are independent is
H0 :
Σ
$ ¼
Σ
$
11
Σ
$
22
"
#
(10.43)
Equivalently, the test is H0: Σ
$
12 ¼ 0 versus H1: not H0. The null hypothesis holds that
every variable in X
$
1 is independent of every variable in X
$
2.
The likelihood ratio test statistic is
^λ ¼
jS
$0j
jS
$0
11j jS
$0
22j
(10.44)
and is distributed as Λp1,p2,Np21 if the null hypothesis is true. The test can be easily
extended to more than two partitions.
The test for independence of variables can also be adapted as a test for propriety of
complex Gaussian variables by applying it to (10.17). A generalized likelihood ratio test
for impropriety was proposed by Schreier, Scharf, & Hanssen (2006) and elaborated by
Walden & Rubin-Delanchy (2009). However, their impropriety hypothesis test is identical
to the present test of independence for multivariate data applied to the augmented covar-
iance matrix. The null hypothesis holds that the sample version of (10.17) is block diagonal
versus the alternate hypothesis that it is not or, equivalently, the null hypothesis holds that
Γ
$
^
¼ 0 versus the alternate hypothesis that it is not. The likelihood ratio test statistic for
impropriety is
^Λ ¼
^
_Γ
$


^Γ
$


2
(10.45)
Equation (10.45) is distributed as Wilks’ Λ distribution Λp,p,Np1 if the null hypothesis is
true. This test was used by Chave (2014) in evaluating the propriety of magnetotelluric
response function estimates.
10.6 Multivariate Regression
The linear relationship between a single response variable y and one or more predictor
variables X
$ was considered in Chapter 9. This can be generalized to the case of more than
one response variable Y
$ to give multivariate multiple linear regression. The extension is
straightforward, although statistical inference quickly gets messy. The regression problem is
366
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

Y
$¼ X
$  β
$ þ Ξ
$
(10.46)
Where Y
$ is N  q, X
$ is N  p, β
$ is p  q, and Ξ
$ is N  q. Each of the q columns of Y
$
depends on the predictor variables X
$ in its own way, resulting in the parameters in β
$
becoming a matrix rather than a vector. Under the same assumptions as for univariate
regression, the solution that minimizes Ξ
$T Ξ
$ is
^β
$ ¼
X
$T X
$

1
 X
$T Y
$
(10.47)
This is identical to solving q separate univariate regressions with the same predictor
variables but different response variables. The least squares solution minimizes both the
trace and the determinant of Ξ
$T Ξ
$. The parameters ^β
$ are the minimum variance unbiased
linear estimator if the conditions on the least squares model in Section 9.2 are met.
However, the column elements of ^β
$ will be correlated unless the columns of the predictor
matrix X
$ are orthogonal, as is also true of univariate linear regression. The row elements of
^β
$ will be correlated if the response data are correlated. As a result, hypothesis tests and
conﬁdence limit estimates typically must be multivariate.
The estimator ^β
$ will be partitioned so that the constant term (if present) is excluded.
This is necessary because otherwise multivariate tests would require that the columns of Y
$
have zero mean. Let the ﬁrst p  1 rows of ^β
$ be ^β
$
1. The hypotheses H0: ^β
$
1 ¼ 0 versus H1:
^β
$
1 6¼ 0 can be tested using Wilks’Λ. The alternate hypothesis holds that at least one of the
elements in ^β
$
1 is nonzero. The within E
$ and between H
$ matrices in the Wilks’ Λ test
become
E
$ ¼
Y
$  X
$  ^β
$

T
 Y
$
(10.48)
H
$ ¼ ^β
$T  X
$T Y
$  N Y
T
N  YN
(10.49)
so that
E
$ þ H
$ ¼ Y
$T Y
$  N Y
T
N  YN
(10.50)
The usual test statistic (10.30) is assessed against Λq,p,Np1 when the null hypothesis is
true. The test rejects if ^Λ  Λq,p,Np1 at the α level. This test is analogous to the F test for
regression that is described in Section 9.3.1.
Example 10.6 In a classic data set, Siotani et al. (1963) present data containing two response
variables (y1 = taste, y2 = odor) against eight predictor variables (x1 = pH, x2 = acidity 1,
x3 = acidity 2, x4 = sake meter, x5 = direct reducing sugar, x6 = total sugar, x7 = alcohol, x8 =
formyl nitrogen) for 30 brands of Japanese sake. The predictive power of the data will be
evaluated.
367
10.6 Multivariate Regression
.011
13:42:10, subject to the Cambridge Core terms of use,

sake = importdata('sake.dat');
y = sake(:, 1:2);
x = [sake(:, 3:10) ones(30, 1)];
[n, m] = size(x);
beta = x\y;
e = (y - x*beta)'*y;
h = beta'*x'*y - n*mean(y)'*mean(y);
[pval, lambda] = WilksLambda(h, e, 2, m - 1, n - m - 2)
pval =
0.8620
lambda =
0.4642
The p-value is large; hence the null hypothesis that all the regression coefﬁcients are
zero is accepted. There is no point in undertaking further analysis.
The signiﬁcance of a subset of the rows of ^β
$ can also be tested. Let ^β
$ be partitioned so that
the ﬁrst r rows are ^β
$
r and the last s rows are ^β
$
s, where r þ s ¼ p. The hypothesis to be
tested is H0: ^β
$
s ¼ 0 versus H1: ^β
$
s 6¼ 0. Let X
$
r denote the columns of X
$ corresponding to
^β
$
r so that Y
$¼ X
$
r  β
$
r þ Ξ
$0. The E
$ matrix is given by (10.48), whereas the H
$ matrix
becomes
H
$ ¼ ^β
$T
 X
$T Y
$  ^β
$T
r  X
$T
r  Y
$
(10.51)
The test statistic is the usual estimator for ^Λ (10.30) and is assessed against Λq,s,Np1.
When the number of predictor variables p or the number of response variables q is large,
it is often desirable to ﬁnd an optimal subset, often called the adequate set of variates.
While there is no bulletproof way to achieve this, a standard approach is backward
elimination beginning with all p of the predictor variables. The variables are deleted one
at a time with replacement, and each instance is assessed against a partial Wilks’Λ. For the
ﬁrst deletion, the partial Λ when the jth predictor is omitted is given by the conditional
distribution
Λ xj jx1 . . . ; xj1; xjþ1; . . . ; xp


¼
Λ Y
$; x1; . . . ; xp


Λ Y
$; x1; . . . ; xj1; xjþ1; . . . ; xp


(10.52)
and has Λq,1,Np1 as its distribution. The variable with the largest Λ is omitted, and the
process continues with the p  1 remaining variables. It terminates when the variable
corresponding to the largest Λ is signiﬁcant at a chosen probability level. A similar
procedure can be used to determine whether the reduced set of predictor variables predicts
all or only some of the response variables.
368
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

Example 10.7 A data set from Rencher (1995, p. 295) relates temperature, humidity, and
evaporation, and contains the following variables:
• Maximum daily temperature
• Minimum daily temperature
• Integrated daily temperature
• Maximum daily soil temperature
• Minimum daily soil temperature
• Integrated daily soil temperature
• Maximum daily relative humidity
• Minimum daily relative humidity
• Integrated daily relative humidity
• Total wind in miles per day
• Evaporation
The last two variables will be regressed on the ﬁrst nine, and backward stepwise selection
will be applied to cull the set of predictors.
the = importdata('the.dat');
x = [the(:, 1:9) ones(size(the(:, 1)))];
y = the(:, 10:11);
[n, m] = size(x);
m = m - 1;
beta = x\y;
e = (y - x*beta)'*y;
h = beta'*x'*y - length(y)*mean(y)'*mean(y);
[pval, lambda] = WilksLambda(h, e, 2, m, n - m - 1)
pval =
5.7607e-11
lambda =
0.1011
The p-value is very small; hence the null hypothesis that all the regression coefﬁcients
are zero is rejected.
The backward elimination procedure will be applied to determine if all the predictor
variables are required.
q = 2;
nh = 1;
p = 9;
ne = length(y) - p - 1;
lambda2 = [];
for i=1:p
if i == 1
x1 = x(:, 2:p + 1);
369
10.6 Multivariate Regression
.011
13:42:10, subject to the Cambridge Core terms of use,

elseif i < p
x1 = [x(:, 1:i) x(:, i + 2:p + 1)];
else
x1 = [x(:, 1:p - 1) x(:, p + 1)];
end
beta1 = x1\y;
e = (y - x1*beta1)'*y;
h = beta1'*x1'*y - length(y)*mean(y)'*mean(y);
lambda1 = lambda*det(e + h)/det(e);
lambda2 = [lambda2 lambda1];
end
f = (ne - p + 1)*(1 - lambda2)./(p*lambda2);
pval = 2*min(fcdf(f, p, ne - p + 1), 1 - fcdf(f, p, ne - p + 1));
lambda2
lambda2 =
0.9877
0.9197
0.9497
0.8988
0.8772
0.9738
0.9888
0.8498
0.8498
Eliminate the seventh variable and repeat.
lambda2 =
0.9651
0.8943
0.9286
0.8424
0.8474
0.9692
0.7955
0.7955
Eliminate the sixth variable and repeat.
lambda2 =
0.8405
0.7959
0.7436
0.6571
0.8405
0.6578
0.6578
Eliminate the ﬁrst variable (noting that there is a tie and hence the choice between the ﬁrst
and ﬁfth is arbitrary) and repeat.
lambda2 =
0.7874
0.7382
0.6559
0.8352
0.6303
0.6303
Eliminate the ﬁfth variable and repeat.
lambda2 =
0.5940
0.5639
0.5570
0.4442
0.4442
At this point, the p-values are all below 0.05, and the process terminates. The maximum
daily temperature, minimum daily soil temperature, integrated daily soil temperature, and
maximum daily relative humidity were eliminated from the predictors.
MATLAB supports multivariation regression through the function [b, sigma, r, covb] =
mvregress(y, x), where y is the response matrix and x is the predictor matrix. The returned
parameters are the regression coefﬁcients in b, the covariance matrix in sigma, the residuals
in r, and the regression coefﬁcient covariance matrix in covb.
370
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

10.7 Canonical Correlation
The correlation coefﬁcient ^R
2 introduced in Section 9.3.1 is a measure of the extent of a
linear relationship between a single response variable and one or more predictor variables.
Canonical correlation is a generalization to the case of more than one response and more
than one predictor variable. It differs from principal component analysis (Section 10.8) in
that the focus is on the correlation between two sets of random variables rather than on the
correlation within a single data set. Canonical correlation was ﬁrst proposed by Hotelling
(1936).
Consider a random sample of N observation vectors Y
$ and X
$, where the former is N  q
and the latter is N  p. The sample covariance matrix can be partitioned as
S
$ ¼
S
$
yy
S
$
yx
S
$
xy
S
$
xx
 
!
(10.53)
Just as the correlation coefﬁcient is the maximum squared correlation between a single
response and multiple predictor variables, the squared canonical correlation ^Ρ
2 is the
maximum squared correlation between a linear combination of response and a linear
combination of predictor variables. Let α be 1  q and β be 1  p vectors of coefﬁcients
chosen to maximize the sample squared correlation given by
^ραβ ¼
α  S
$
yx  βT
α  S
$
yy  αT

1=2
β  S
$
xx  βT

1=2
(10.54)
such that
^Ρ
2 ¼ maxα,β ^ρ2
α β
(10.55)
It can be shown that (10.55) is the largest eigenvalue of either S
$1
yy  S
$
yx  S
$1
xx  S
$
xy
or S
$1
xx  S
$
xy  S
$1
yy  S
$
yx, and the canonical coefﬁcients α and β are the corresponding
eigenvectors. When q ¼ 1, S
$1
yy  S
$
yx  S
$1
xx  S
$
xy reduces to syx  S
$1
xx  sT
yx=syy, which is
the multiple correlation coefﬁcient ^R
2. Note also that there are min p; q
ð
Þ distinct eigen-
values for either S
$1
yy  S
$
yx  S
$1
xx  S
$
xy or S
$1
xx  S
$
xy  S
$1
yy  S
$
yx that can be used to provide
additional information about the relationships between the response and predictor vari-
ables. The result is a simpliﬁcation of the covariance matrix structure: the qp covariances
in S
$
yx have been reduced to min p; q
ð
Þ canonical correlations. The min p; q
ð
Þ canonical
variates are given by Y
$  α and X
$  β, respectively. Because α and β are eigenvectors, they
are unique only up to a scale factor that can be chosen such that Y
$  α and X
$  β
are orthonormal. The canonical variable scores are deﬁned as u
$ ¼

X
$ XN

 α
and v
$ ¼

Y
$ YN

 β and are akin to the canonical variates after centering the variables.
MATLAB supports canonical correlation through the function [a, b, r] = canoncorr
(x, y), where x is n  p and y is n  q. The returned variables a and b are the canonical
371
10.7 Canonical Correlation
.011
13:42:10, subject to the Cambridge Core terms of use,

coefﬁcients α and β, whereas r contains the canonical correlations. [a, b, r, u, v, stats] =
canoncorr(x, y) also returns the canonical variates u and v and a structure containing test
information about the canonical correlations.
Example 10.8 Returning to the sake data of Example 10.6, compute the canonical correl-
ations among the variables.
sake = importdata('sake.dat');
y = sake(:, 1:2);
x = sake(:, 3:10);
[a, b, r, u, v, stats] = canoncorr(x, y);
r
r =
0.6208
0.4946
The canonical correlations are moderate, but they are not signiﬁcant, as can be shown by
examining the structure stats.
stats
stats =
Wilks: [0.4642 0.7553]
df1: [16 7]
df2: [40 21]
F: [1.1692 0.9718]
pF: [0.3321 0.4766]
chisq: [18.0330 7.0419]
pChisq: [0.3220 0.4245]
dfe: [16 7]
p: [0.3220 0.4245]
The ﬁfth entry is the p-value for the F approximation to Wilks’ Λ, whereas the seventh
entry is the p-value for a chi square approximation. In both cases, the p-values are large;
hence the null hypothesis that the data are uncorrelated is accepted. There is no discernible
correlation in these data.
Example 10.9 Returning to the temperature-humidity-evaporation data from Example 10.7,
compute and assess the canonical correlations among the data.
the = importdata('the.dat');
x = the(:,1:9);
y = the(:,10:11);
[a, b, r, u, v, stats] = canoncorr(x, y);
r
r =
0.9095
0.6442
372
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

stats
stats =
Wilks: [0.1011 0.5849]
df1: [18 8]
df2: [70 36]
F: [8.3438 3.1930]
pF: [2.8804e-11 0.0077]
chisq: [89.3869 21.0253]
pChisq: [1.8602e-11 0.0071]
dfe: [18 8]
p: [1.8602e-11 0.0071]
The canonical correlations are fairly high, and the p-values are all very small, rejecting
the null hypothesis that the correlations are zero. Figure 10.5 shows the canonical variable
scores. The higher canonical correlation of the ﬁrst canonical variable compared with the
second one is readily apparent.
10.8 Empirical Orthogonal Functions
A signiﬁcant weakness of linear regression in a multivariate context is that the distinction
between response and predictor variables is frequently arbitrary, yet their choice plays a
major role in the testing of hypotheses about the parameters. A second weakness is that the
sensitivity of least squares estimators to inﬂuential data in the response and predictor
variables is different, and hence the distinction between them can inﬂuence the effect of
outlying data. The lack of consistency when the predictors contain random noise is also an
issue. Linear regression makes sense when the underlying physics demands a linear
relationship between speciﬁc variables but is less tenable when the relationship between
the variables is unknown and hence a statistical parameter to be determined.
–3
–2
–1
U
–3
–2
–1
0
0
1
1
2
2
3
3
V
Figure 10.5
The ﬁrst (crosses) and second (circles) canonical variable scores for the temperature-humidity-wind-speed data.
373
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

There are several multivariate techniques available that elucidate the relationship
between different data sets. Discriminant analysis is used to describe the differences
between two or more groups of linear functions of the variables and is an extension of
MANOVA that is typically nonparametric. Classiﬁcation analysis uses discriminant func-
tions to reveal the dimensionality of separation and the contribution of the variables to each
group. Neither of these is completely data driven, and the grouping remains to some degree
arbitrary. Canonical correlation (Section 10.7) is a measure of the linear relationship
between several response and several predictor variables and hence is an extension of
the multiple correlation coefﬁcient. It is valuable in assessing the results from multivariate
multiple regression and encompasses much of MANOVA and discriminant/classiﬁcation
analysis, but the distinction between response and predictor variables remains subjective.
What are needed are approaches that treat all the variables identically and yield their linear
relationships using all possible correlations between all the variables. Two such tools are
empirical orthogonal function (eof)/principal components analysis and factor analysis.
The eof terminology has its origin in meteorology, where the technique is commonly
applied to spatial data sets or to frequency domain analysis of array data, whereas principal
components originated in the social sciences. The former uses terminology that is easily
parsed and will be emphasized in what follows. However, eofs and principal components
inherently are identical in concept. Principal component analysis was introduced by
Hotelling (1933). A key reference about principal components is Jolliffe (2002), and
Preisendorfer (1988) provides a thorough description of their use in meteorology and
physical oceanography.
10.8.1 Theory
In eof analysis, the original p variables are linearly transformed into combinations whose
variances are maximized while being orthogonal and hence mutually uncorrelated. Eof
analysis is a dimension reduction technique that simpliﬁes the p variables into a few linear
combinations that explain most of the data variance and that constitute the core structure of
the data set. In general, the more highly correlated that data are, the smaller is the number
of eofs required to describe them or, conversely, if the data are only weakly correlated, the
resulting eofs will largely reﬂect the original data, and limited parsimony will be achieved.
Eof analysis is a one-sample technique with no subjective groupings (unlike discriminant/
classiﬁcation analysis) and no partitioning of the data into response and predictor variables.
Consider N samples of p observation vectors. Each sample is a swarm in a p-dimen-
sional space. For example, there may be N time points (not necessarily equally spaced) and
p locations at which some variable is measured, although there is not a requirement that the
variables all be the same or that they be scalar quantities. For simplicity, scalar data will be
considered for the present. The N samples of p observation vectors constitute an N  p
data matrix X
$ that is assumed to be centered (i.e., have the sample mean removed from
each column by multiplying by the centering matrix I
$
N  J
$
N=N).
For a given sample (row in X
$), the p variables (columns of X
$) may be (and typically are)
correlated, and the swarm of points may not be oriented parallel to any of the p axes of the
data. The goal of eof analysis is to ﬁnd the natural axes of the data. This is accomplished by
374
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

rotating the axes in a p-dimensional space, after which the new axes become the natural
ones for the data, and the new variables (which are linear combinations of the original data)
will be mutually uncorrelated. In some cases, the eofs are directly interpretable as physical
phenomena, but there is no guarantee that the output will be physically meaningful entities.
The ﬁrst eof is the linear combination of the variables with maximum sample variance
among all possible linear combinations of the variables. An N-vector z ¼ X
$  aT is sought
whose sample variance s2
z ¼ a  S
$0  aT, where a is 1  p and S
$0 is the p  p unbiased
sample covariance matrix, is a maximum. The vector a cannot be arbitrary because s2
z has
no maximum if the elements in a increase without limit, and hence s2
z must be maximized
as the coefﬁcients in a vary relative to each other, or equivalently, the constraint a  aT ¼ 1
must be imposed. The quantity to be maximized is the Rayleigh quotient
t2 ¼ a  S
$0  aT
a  aT
(10.56)
According to Rayleigh’s principle, the vector a that maximizes (10.56) is given by a1, the
eigenvector of S
$0 that corresponds to its largest eigenvalue (usually called the ﬁrst
eigenvector). The maximum value of (10.56) is given by the corresponding eigenvalue
of S
$0. The resulting z1 ¼ X
$  aT
1 is the ﬁrst empirical orthogonal function amplitude of X
$.
S
$0 has p eigenvectors or eofs (originally called loadings in principal components
terminology), and hence there are p eof amplitudes (originally called scores) that can be
written as the N  p matrix
Z
$ ¼ X
$  A
$T
(10.57)
S
$0 is symmetric; hence the eigenvectors are mutually orthogonal and unitary
A
$T A
$¼

A
$  A
$T ¼ I
$
pÞ, and if S
$0 is positive deﬁnite, the eigenvalues are all positive. If S
$0 is only
positive semideﬁnite, then the number of positive eigenvalues corresponds to its rank.
By the spectral decomposition theorem,
S
$0 ¼ A
$T D
$  A
$
(10.58)
where D
$ ¼ diag λi
ð Þ is the diagonal matrix of eigenvalues that give the sample variances of
the empirical orthogonal function amplitudes. The sample covariance matrix is
S
$0 ¼ X
$T X
$ = N  1
ð
Þ because the variables are centered, and hence
S
$
Z ¼ Z
$T Z
$ =N ¼ A
$ X
$T X
$ A
$T= N  1
ð
Þ ¼ A
$ 
 S0 A
$T ¼ D
$
(10.59)
The empirical orthogonal function amplitudes are mutually orthogonal.
The eigenvalues and eigenvectors of the sample covariance matrix may be found by
solving the eigenvalue problem
S
$0 A
$ ¼ λ
$  A
$
(10.60)
where λ
$ is diagonal, and the columns of A
$ are the eigenvectors of S
$0. The eigenvectors
are uncertain by a sign because the result is unchanged if both sides of (10.60) are
multiplied by 1. The columns of A
$ are the eofs that constitute an orthogonal basis set
375
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

empirically determined from the data. The eof amplitudes are the projections of the data set
onto the eof basis functions.
The eigenvectors of
X
$T X
$ and S
$0 are identical, whereas the eigenvalues of X
$T X
$
are N  1 times the eigenvalues of S
$0. Consequently, it is often more convenient to work
directly with X
$T X
$ and correct for the scale factor N  1 at the conclusion.
Let D
$
k be the diagonal matrix of eigenvalues with all entries beyond i ¼ k set to zero.
Then S
$0
k ¼ A
$T  D
$
k A
$ is the best approximation of rank k to S
$0, in the sense that the
sum of squares k S
$0  S
$0
kk2 is minimized. Consequently, the vector subspace spanned by
the ﬁrst k eofs has a smaller mean square deviation from the sample variables than any
other k-dimensional subspace. Further, if S
$0 has rank k < p, then the total variation in the
data may be explained by the ﬁrst k eofs.
The sum of the ﬁrst k eigenvalues divided by the sum of all the eigenvalues is the
proportion of the variance represented by the ﬁrst k eof amplitudes. Consequently, if one
variable has a much larger variance than the remaining data, then this variable will
dominate the ﬁrst eof amplitude, which will, in turn, dominate the variance. Conversely,
if one variable is uncorrelated with the remaining variables, then its variance will be one of
the eigenvalues, and the corresponding eof will be all zeroes except for a one at the variable
index. In this case, the variable is itself an eof amplitude. Finally, because no inverse or
determinant is involved in extracting the eofs, the covariance matrix can be singular, in
which case some of the eigenvalues will be zero.
The eof amplitudes are not scale invariant. As a result, it is desirable for the data to be
commensurable (i.e., have similar measurement scales and variances). Eof analysis can be
carried out on the sample covariance matrix S
$0 or the corresponding sample correlation
matrix R
$0, and the latter may be preferred if the data are incommensurable because the
eof amplitudes extracted from R
$0 are scale invariant. The eof amplitudes from R
$0 will not
be the same as those from S
$0 because of the lack of scale invariance of the latter. In general,
the percent variance explained by a given eof amplitude will be different for R
$0 as
compared to S
$0.
As an example illustrating these issues, suppose that bivariate data have
S
$0 ¼
1
4
4
25


and
R
$0 ¼
1
0:8
0:8
1


The eigenvalues and eigenvectors for S
$0 are λ ¼ 25:6; 0:35
ð
Þ, a1 ¼ 0:160; 0:987
ð
Þ, and
a2 ¼ 0:987;
0:160
ð
Þ. The large variance of the second variable ensures that the ﬁrst eof
amplitude z1 ¼ 0:160x1 þ 0:987x2 nearly duplicates x2 and does not reﬂect the correlation
of the two variables. Note also that λ1= λ1 þ λ2
ð
Þ = 0.99, and hence the second eof
amplitude z2 is not very important. By contrast, the eigenvalues and eigenvectors for R
$0
are λ ¼ 1:8; 0:2
ð
Þ, a1 ¼ 0:707; 0:707
ð
Þ, and a2 ¼ 0:707; 0:707
ð
Þ. The variables are
equally weighted by the eofs. However, λ1= λ1 þ λ2
ð
Þ = 0.9, so the ﬁrst eof amplitude is
still dominant.
While eof analysis can be carried out from the sample covariance or correlation matrices,
there is an easier way using the singular value decomposition described in Section 9.3 to
directly estimate them from the data
376
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

X
$  B
$¼ U
$  ϒ
$ V
$T
(10.61)
Where B
$ is a p  p diagonal matrix that preweights the columns of X
$. If B
$ ¼ I
$
p, then
the result is equivalent to working with the sample covariance matrix, whereas
if B
$ ¼ diag 1=sii
ð
Þ, then the result is equivalent to working with the sample correlation matrix.
Note that X
$  B
$ contains the z-scores of the data in this instance because X
$ is centered. A third
way to preweight the columns of X
$ is to ﬁrst regress each column in turn on the remaining
p  1 columns and place the inverse of the residual standard deviation in the diagonal elements
of B
$. This equalizes the noise level, where noise is deﬁned to be the component of a given
sample that is uncorrelated with all the remaining samples across the data under the assumption
that the noise covariance matrix is diagonal, meaning that there is no correlated noise present.
The p  p matrix of right singular vectors V
$ and the N  N matrix of left singular vectors U
$
are both orthogonal and unitary and are ambiguous in sign because multiplying both by 1
leaves (10.61) unchanged. The N  p matrix ϒ
$ of singular values is zero except for the p
diagonal locations starting at the upper left corner and by convention ranked from largest to
smallest down the diagonal. The relationships between the matrices in (10.61) are
X
$  B
$  V
$ ¼ U
$  ϒ
$
(10.62)
B
$T  X
$T U
$ ¼ V
$ ϒ
$T
(10.63)
B
$T  X
$T X
$  B
$  V
$ ¼ V
$ ϒ
$T  ϒ
(10.64)
X
$  B
$  B
$T  X
$T U
$ ¼ U
$  ϒ
$ ϒ
$T
(10.65)
hence the columns of V
$ are the variable eofs, and the squared singular values are the
corresponding eigenvalues [i.e., D
$ ¼ ϒ
$T ϒ
$ = Ν  1
ð
Þin (10.59)]. The eof amplitudes are
given by (10.62). A second type of eof amplitude in observation rather than variable space
is obtained in the ﬁrst p columns of (10.63). Equations (10.64) and (10.65) deﬁne eof
analysis in terms of the covariance or correlation structure rather than the data structure,
recognizing that the squared singular values must be scaled by 1= N  1
ð
Þ.
MATLAB directly supports eof analysis through the functions pca and pcacov that
operate, respectively, on the raw data and the covariance matrix. The call [coeff, score,
latent] = pca(x) uses the svd by default on the n  p data matrix x and returns
the eigenvectors (V
$ from the svd) in coeff, the eof amplitudes in score, and the eigenvalues
or squared singular values divided by the degrees-of-freedom n  1 in latent. A variety of
name-value arguments can be added to control the algorithm, whether the data are centered
(default) or not, weighting, and how missing data are handled. The call [coeff, latent] =
pcacov(cov) provides fewer capabilities based on the sample covariance matrix cov and is
not recommended by comparison to pca.
10.8.2 Choosing the Number of Eofs
A key decision that the analyst has to make is how many eofs to retain in representing a
given data set. There are three main approaches that often produce contradictory results in
part because they are largely qualitative rather than quantitative.
377
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

The simplest but most arbitrary is retaining enough eofs to account for a ﬁxed percent-
age of the total variance (e.g., 90%, with typical values lying between 70% and 90%). The
choice of threshold is subjective.
A more objective approach is to retain those eofs whose eigenvalues are larger than the
average tr

S
$
=p of all the eigenvalues. This works well in general and is likely to err on the
side of retaining too many rather than too few eofs, which is a conservative approach. It is
the default in many software packages.
A third method is to plot the eigenvalues against their index (called a scree plot) and
look for the natural break between large and small eigenvalues. An alternative that is often
used in the atmospheric sciences is to plot the log of the eigenvalues rather than the
eigenvalues themselves. These methods work well with data that are highly correlated so
that a few eofs is sufﬁcient.
Once the number of eofs is determined, and presuming that this is two or three, the
pertinent empirical orthogonal function amplitudes and the variables can be plotted as a
biplot. MATLAB supports this through the function biplot(v(:, 1:p), 'scores', eofa(:, 1:p)),
where p is either 2 or 3. Additional name-value pairs can be used to label the eigenvector or
eof amplitude axes. Biplots feature prominently in compositional data analysis in Chapter 11.
Once a subset of the eofs has been selected as representative of the data, the subset eofs
can be rotated using one of many criteria to simplify the interpretation of the entire subset.
This is an integral part of factor analysis but is less commonly used in eof analysis.
Let V
$
k be a p  k submatrix of right singular vectors deﬁned by eliminating the last p  k
columns corresponding to the smallest eigenvalues in (10.62). Applying an orthogonal
rotation matrix T
$ yields a rotated subset of eigenvectors V
$
k T
$ that remain orthogonal
because T
$T  V
$T
k  V
$
k T
$¼ T
$T T
$¼ I
$
k. However, the rotated eigenvalue matrix T
$T  ϒ
$
k T
$
is not diagonal, and hence the rotated eof amplitudes are not uncorrelated.
The rotation matrix is chosen based on one of many (there are at least twenty) criteria.
The most widely used is the varimax rotation that maximizes the sum of the variances of
the eigenvectors (i.e., it simpliﬁes the columns of
V
$
k). The goal is to minimize the
complexity of the eofs by making the large eigenvectors larger and the small ones smaller.
The result is a set of principal components that are as independent of each other as possible.
An alternative is the quartimax rotation, which makes large eigenvectors larger within each
variable (i.e., it simpliﬁes the rows of V
$
k). The equimax rotation is a compromise between
varimax and quartimax. It is also possible to apply oblique (i.e., nonorthogonal) rotations,
although this is done less commonly.
MATLAB supports rotation through v1 = rotatefactors(v(:, 1:p)), where v is the
eigenvector matrix from svd or the coefﬁcient matrix from pca. By default, the varimax
rotation is applied. The keyword value pair “Method,” “quartimax,” or “equimax” can be
used to specify the quartimax or equimax methods.
10.8.3 Example
Example 10.10 Returning to the zonally averaged temperature data of Example 10.2, an eof
analysis will be applied to the eight zonal bands extending from the north to the south pole.
378
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

z = importdata('ZonAnnTs+dSST.dat');
x = z(:, 8:15);
var(x)
ans =
1.0e+03 *
6.4492 2.3541 1.0484 0.9533 1.0833 0.9371 0.6632 4.4149
The variance range covers an order of magnitude, with the largest values occurring toward
the poles, so the covariance eofs will probably be dominated by those variables.
x1 =
x - repmat(mean(x), length(x), 1);
[u, s, v] = svd(x1);
diag(s.^2)'/sum(diag(s.^2))
ans =
0.6318 0.2398 0.0696 0.0272 0.0158 0.0097 0.0042 0.0019
The ﬁrst two eofs account for 87.2% of the variance. By criteria 1 and 2 earlier, only
three and two eofs would be retained, respectively. A scree plot suggests that three or four
eofs are required (Figure 10.6). Consequently, four will be retained for further analysis.
A plot of the logarithm of the eigenvalues against the index is nearly a straight line and
hence uninformative.
Examine the eofs.
v
v =
0.697
0.345
0.568
-0.265
0.040
-0.013
0.006
0.020
0.416
0.089
-0.188
0.765
-0.108
0.428
-0.02
-0.052
0.269
0.010
-0.242
0.245
-0.061
-0.817
-0.297
0.221
0.239
-0.026
-0.385
-0.206
0.529
0.012
-0.255
-0.638
0.254
-0.042
-0.450
-0.251
0.366
0.202
0.249
0.657
0.247
-0.088
-0.289
-0.146
-0.429
-0.191
0.708
-0.325
0.165
-0.113
-0.264
-0.393
-0.615
0.269
-0.532
0.061
0.242
-0.922
0.283
0.057
0.088
0.004
-0.010
0.017
1
Eof
0
0.5
1
1.5
2
Squared Singular Value
×106
2
3
4
5
6
7
8
Figure 10.6
Scree plot for the zonally averaged temperature data.
379
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

The ﬁrst eof is dominated by the northern hemisphere polar and subpolar zonal bands. The
second eof is dominated by anticorrelation of the northern and southern polar zonal bands.
The third eof represents anticorrelation of the northern polar and two equatorial zonal
bands. The fourth eof represents anticorrelation of the northern and southern subpolar
bands. The ﬁrst two eof amplitudes are plotted in Figure 10.7.
eofa = u*s;
t = z(:, 1);
plot(t, eofa(:, 1), t, eofa(:, 2))
The ﬁrst eof amplitude reproduces the generally increasing temperature at northern polar
latitudes seen in Figure 10.2. However, the second eof amplitude is largest at the south pole
yet has the opposite sense to the south pole data (see Figure 10.3) until about 1960.
Reversing the sign of the second eof amplitude gives Figure 10.8. The ﬁrst eof amplitude
shows a generally increasing temperature anomaly, with a slight decrease from 1940 to
1970, and explains 64.5% of the data variance. The second eof amplitude reveals a weak
1880 1900 1920 1940 1960 1980 2000 2020
Year
–200
–100
0
100
200
300
Eof Amplitude
Figure 10.7
First two eof amplitudes for the unscaled zonal temperature data. The solid line is the ﬁrst and the dashed line is
the second eof amplitude.
1880 1900 1920 1940 1960 1980 2000 2020
Year
–300
–200
–100
0
100
200
300
Eof Amplitude
Figure 10.8
First two eof amplitudes for the unscaled zonal temperature data after reversing the sign of the second eof. The
solid line is the ﬁrst and the dashed line is the second eof amplitude.
380
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

decrease until approximately 1930, followed by nearly constant temperature, and explains
23.5% of the variance.
Apply a varimax rotation to the ﬁrst four eigenvectors.
v1 = rotatefactors(v(:,1:4))
v1 =
0.999
0.002
-0.005
-0.003
0.014
-0.006
0.115
0.888
-0.011
0.001
-0.153
0.410
-0.009
0.042
-0.491
0.077
-0.030
0.046
-0.569
0.068
0.015
-0.048
-0.402
0.094
0.027
-0.044
-0.487
-0.155
-0.002
-0.996
-0.007
0.004
The result maximizes the differences between the eigenvector columns, resulting in
dominance at the north and south poles in the ﬁrst two eigenvectors. A broad average
over the northern equatorial to southern subpolar bands is seen in the third eigen-
vector. The fourth eigenvector is dominated by the northern hemisphere midlatitude
and subpolar bands. A very similar result obtains using the quartimax or equimax
rotation.
Repeat the exercise using the correlation eofs rather than the covariance eofs.
b = diag(1./std(x));
[u, s, v ] = svd(x1*b);
diag(s.^2)/sum(diag(s.^2))
ans =
0.7025
0.1305
0.0666
0.0429
0.0276
0.0154
0.0105
0.0041
By criterion 1, three eofs would be retained. By criterion 2, only two eofs would be
retained. A scree plot (Figure 10.10) suggests that only two eofs should be retained.
1880 1900 1920 1940 1960 1980 2000 2020
Year
–300
–200
–100
0
100
200
Eof Amplitude
Figure 10.9
First four eof amplitudes obtained after varimax rotation of the ﬁrst four eigenvectors. The ﬁrst (solid line), second
(dashed line), third (dash-dot line), and fourth (dotted line) eof amplitudes are shown.
381
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

Examine the eofs.
v
v =
0.332
0.375
0.455
-0.307
-0.616
-0.253
0.022
-0.060
0.371
0.275
0.331
0.016
0.278
0.768
-0.018
0.098
0.386
0.163
0.155
0.114
0.579
-0.503
-0.387
-0.223
0.385
0.042
-0.296
0.496
-0.272
-0.070
-0.243
0.616
0.390
-0.005
-0.360
0.355
-0.215
0.132
0.249
-0.684
0.397
-0.130
-0.010
-0.245
0.269
-0.215
0.741
0.297
0.338
-0.330
-0.393
-0.644
-0.063
0.155
-0.422
-0.036
0.177
-0.794
0.527
0.216
-0.107
0.009
-0.035
-0.037
The ﬁrst eof represents a nearly global average of the temperature anomaly, with some
reduction of the weighting at south polar latitudes. The second eof is dominated by
anticorrelation of the northern and southern subpolar and polar bands. The third eof
represents a complicated global pattern. Plotting the eof amplitudes gives a ﬁrst one that
very closely resembles the global average temperature anomaly. The second eof amplitude
is similar in shape to the second covariance eof amplitude and hence has the wrong sign.
The correct sign of the third eof amplitude is not obvious. However, comparing the average
of the two northernmost and equatorial zonal averages suggests that the sign is correct. The
corrected eof amplitudes are shown in Figure 10.11. Note the much smaller ordinate by
comparison with Figure 10.8.
Apply a varimax rotation to the ﬁrst three eigenvectors.
v1 =
-0.107
0.016
0.668
0.020
-0.012
0.568
0.162
-0.017
0.415
0.465
0.116
0.086
0.519
0.109
0.026
0.390
-0.131
0.122
0.568
-0.135
-0.191
-0.023
-0.969
0.020
1
Eof
0
200
400
600
800
Squared Singular Value
2
3
4
5
6
7
8
Figure 10.10
Scree plot for the zonally averaged temperature data with the columns normalized by their standard deviation.
382
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

The ﬁrst rotated eof is dominated by the equatorial northern hemisphere and equatorial
through subpolar southern hemisphere data and is similar in shape to the third rotated eof
for the covariance eofs. The second rotated eof is dominated by the southern hemisphere
polar temperature anomaly and hence is similar in shape to the second rotated eof for the
covariance eofs. The third rotated eof represents the average of the northern hemisphere
temperature anomaly poleward of equatorial latitudes. The eof amplitudes after reversing
the sign of the second one are shown in Figure 10.12.
Apply the third version of preweighting based on regressing a given column of X
$
against the remaining ones.
res = [];
for i = 1:p
y = x1(:, i);
if i == 1
x2 = x1(:, 2:p);
elseif i < p
x2 = [x1(:, 1:i - 1) x1(:, i + 1:p)];
1880 1900 1920 1940 1960 1980 2000 2020
Year
–4
–2
0
2
4
6
8
Eof Amplitude
Figure 10.11
First three eof amplitudes for the zonally averaged temperature anomaly data with the data matrix columns
normalized by their standard deviations. The ﬁrst (solid line), second (dashed line), and third (dash-dot line) eof
amplitudes are shown.
1880 1900 1920 1940 1960 1980 2000 2020
Year
–4
–2
0
2
4
6
Eof Amplitude
Figure 10.12
First three varimax rotated eof amplitudes for the zonally averaged temperature anomaly data with the data matrix
columns normalized by their standard deviations. The ﬁrst (solid line), second (dashed line), and third (dash-dot
line) eof amplitudes are shown.
383
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

else
x2 = x1(:, 1:p - 1);
end
b1 = x2\y;
res = [res std(y - x2*b1)];
end
b = diag(1./res);
var(x1*b)
ans =
3.1738 5.7292 7.0055 13.5203 15.9300 10.1663 4.0031 1.3753
This form of weighting emphasizes the equatorward zones relative to the polar ones and
hence has the opposite sense to the covariance approach.
[u, s, v] = svd(x1*b);
eofa = u*s;
diag(s.^2/sum(diag(s.^2)))
ans =
0.8204
0.0636
0.0528
0.0188
0.0154
0.0125
0.0101
0.0065
By the ﬁrst criterion, three eofs would be used. By the second criterion, only a single eof
would be used. A scree plot (Figure 10.13) suggests that two or three eofs should be retained.
Examine the eofs.
v
v =
0.186
0.429
0.150
-0.541
0.312
-0.421 -0.385
-0.211
0.287
0.533
0.170
-0.138
0.208
0.532
0.438
0.259
0.336
0.442
0.052
0.577
-0.318
-0.266
0.122
-0.412
0.496 -0.337
0.3959
0.1537
0.1621 -0.421
0.1245
0.491
0.545 -0.422
0.179
-0.171
-0.077
0.433 -0.163
-0.494
0.418
0.130 -0.583
0.038
-0.227
0.116 -0.475
0.420
0.221 -0.152 -0.563
-0.354
-0.020
-0.294
0.610
-0.163
0.061 -0.067 -0.316
0.420
0.821
0.070 -0.074
-0.176
1
Eof
0
2000
4000
6000
8000
Squared Singular Values
2
3
4
5
6
7
8
Figure 10.13
Scree plot for the zonally averaged temperature data with the columns normalized by the noise variance.
384
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

The ﬁrst eof is an average over equatorial and subtropical latitudes. The second eof
represents anticorrelation of the northern hemisphere poleward of subtropical latitudes
and the equatorial data. The third eof is dominantly southern hemisphere poleward of
subtropical latitudes. The ﬁrst three eof amplitudes are plotted in Figure 10.14 after
comparing them with suitable averages of the data, suggesting that the ﬁrst two have the
right sign and the last one does not.
Apply a varimax rotation to the ﬁrst three eofs.
v1 =
-0.033
0.482
0.085
-0.005
0.625
0.067
0.035
0.552
-0.066
0.706
0.040
0.127
0.704
-0.046
-0.096
0.018
0.219
-0.695
0.029
-0.115
-0.613
-0.041
-0.082
-0.316
The ﬁrst rotated eof is dominated by the equatorial zonal bands. The second rotated eof
is dominated by midlatitude to polar northern hemisphere zonal bands. The third rotated
eof is dominated by the poleward southern hemisphere. After reversing the sign of the third
eof, the results are shown in Figure 10.15. An increasing temperature anomaly at equatorial
latitudes is quite apparent in the ﬁrst eof amplitude.
10.8.4 Empirical Orthogonal Function Regression
Empirical orthogonal function analysis is frequently used in conjunction with other statis-
tical techniques, most notably linear regression when the predictor variables are highly
correlated. This problem is called multicollinearity and occurs when two or more of the
1880 1900 1920 1940 1960 1980 2000 2020
Year
–20
–10
0
10
20
30
Eof Amplitude
Figure 10.14
First three eof amplitudes for the globally averaged temperature anomaly data normalized by their noise variance.
The ﬁrst (solid line), second (dashed line), and third (dash-dot line) eof amplitudes are shown.
385
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

predictor variables are nearly constant linear functions of the others. The outcome can be
regression parameters with very large variances.
Consider the linear regression
y ¼ X
$  β þ ε
(10.66)
where both the response variable and the predictor matrix are assumed to be centered. The
eof amplitudes for the observations are
Z
$ ¼ X
$  V
$
(10.67)
Because V
$ is orthogonal, X
$  β ¼ X
$  V
$ V
$T  β ¼ Z
$ γ, where γ ¼ V
$T  β. Equation
(10.66) becomes
y ¼ Z
$  γ þ ε
(10.68)
where the predictors are replaced by the eof amplitudes or principal component scores. Eof
regression is the solution of (10.68) or, more likely, a reduced-rank version obtained by
retaining only the ﬁrst k eofs in V
$. The least squares principles developed in Chapter 9 can
be used to solve (10.68) for ^γ from which an estimate for β is
^β ¼ V
$ ^γ
(10.69)
The advantage of the eof regression approach is that the columns of Z
$ in (10.68) are
orthogonal, simplifying the behavior in the presence of multicollinearity. The solution to
(10.68) is
^γ ¼
Z
$T Z
$

1
 Z
$T  y ¼
ϒ
$T ϒ
$

1
 Z
$T  y
(10.70)
using (10.64).
ϒ
$T ϒ
$

1
is a diagonal matrix of inverse squared singular values, elimin-
ating any problems from matrix inversion, assuming that the original data are full rank. The
regression coefﬁcient estimates follow from combining (10.69) and (10.70):
1880 1900 1920 1940 1960 1980 2000 2020
Year
–20
–10
0
10
20
Eof Amplitude
Figure 10.15
First three varimax rotated eof amplitudes for the zonally averaged temperature anomaly data normalized by their
noise standard deviations. The ﬁrst (solid line), second (dashed line), and third (dash-dot line) eof amplitudes
are shown.
386
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

^β ¼ V
$  ϒ
$T ϒ
$

1
 V
$T  X
$T  y
(10.71)
The product of the ﬁrst three terms on the right side of (10.71) replaces
X
$T X
$

1
and
does not involve inversion of the predictor matrix. Using the svd, (10.71) can be written
entirely in terms of the eigenvalue and eigenvector matrices as
^β ¼ V
$  ϒ
$T ϒ
$

1
 S
$T  U
$T  y
(10.72)
When multicollinearity is substantial, then a reduced-rank solution of (10.71) obtained by
deleting the eofs having small variances can yield more stable estimates of β. The
improved estimate is obtained by removing the eofs in V
$ corresponding to small singular
values in S
$, yielding
^β
0 ¼ V
$
1:k 
ϒ
$T
1:k  ϒ
$
1:k

1
 S
$T
1:k  U
$T  y
(10.73)
However, (10.73) is typically biased, as is evident from
^β
0^β ¼ V
$
kþ1:p 
ϒ
$T
kþ1:p  ϒ
$
kþ1:p

1
 S
$T
kþ1:p  U
$T  y
(10.74)
Since E ^β
 
¼ β, the bias is given by the right-hand side of (10.74) and is nonzero unless
the singular values at indices of k þ 1 to p are all zero, in which case the predictors are rank
deﬁcient.
By analogy to (9.19), the variance of the eof regression estimator is
var ^β
 
¼ σ2 V
$  ϒ
$T ϒ
$

1
 V
$T
(10.75)
whereas the variance of the reduced rank version is
var ^β
0


¼ σ2V
$
1:k 
ϒ
$T
1:k  ϒ
$
1:k

1
 V
$T
1:k
(10.76)
Since the entries in
ϒ
$T
kþ1:p  ϒ
$
kþ1:p

1
are all larger (and typically much more so) than
those in
ϒ
$T
1:k  ϒ
$
1:k

1
, it follows that var ^β
0


< var ^β
 
. Consequently, the reduced-
rank eof regression estimator has a lower variance than the ordinary one at the penalty of
increased bias. This may be an acceptable tradeoff if the bias is small while the variance
reduction is large.
Example 10.11 Returning to the sake data of Example 10.6, use eof regression to evaluate
the predictors for multicollinearity by applying (10.73) and using the Wilks’ Λ test for
multivariate regression to test the result.
sake = importdata('sake.dat');
y = sake(:, 1:2);
y = y - repmat(mean(y), length(y), 1);
x = sake(:, 3:10);
387
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

x = x - repmat(mean(x), length(x), 1);
[n, m] = size(x);
[u, s, v] = svd(x);
for i = m:-1:2
beta = v(:, 1:i)*inv(s(:, 1:i)'*s(:, 1:i))*s(:, 1:i)'*u'*y;
e = (y - u*s(:, 1:i)*v(:, 1:i)'*beta)'*y;
h = beta'*v(:, 1:i)*s(:, 1:i)'*u'*y;
p = WilksLambda(h, e, 2, i, n - i - 1)
end
p =
0.6642
p =
0.7940
p =
0.3109
p =
0.1352
p =
0.2453
p =
0.3027
p =
0.2173
All the p-values are well above 0.05, and hence there is no need for regression even
when only two eigenvalues are retained. The problem with these data is not multicolli-
nearity. Further, the result is not changed if the variables in x and y are standardized so that
the principal components are those from the correlation matrix.
Example 10.12 A standard data set to illustrate the importance of multicollinearity is due to
Longley (1967) and is available at www.itl.nist.gov/div898/strd/lls/data/LINKS/DATA/
Longley.dat. There are one response variable, six predictor variables, and 16 observations
in the seven columns of the data ﬁle. The predictors are highly incommensurable, and
hence the data will be standardized for analysis.
data = importdata('longley.dat');
y = data(:, 1);
y = y - repmat(mean(y), length(y), 1);
y = y./repmat(std(y), length(y), 1);
x = data(:, 2:7);
x = x - repmat(mean(x), length(x), 1);
x = x./repmat(std(x), length(x), 1);
[n p ] = size(x);
388
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

w = ones(size(y));
[b bse t pval ] = Treg(y, x, w);
b'
ans =
0.0463
-1.0137
-0.5375
-0.2047
-0.1012
2.4797
bse'
ans =
0.2475
0.8992
0.1233
0.0403
0.4248
0.5858
t'
ans =
0.1870
-1.1274
-4.3602
-5.0828
-0.2383
4.2331
pval'
ans =
0.8554
0.2859
0.0014
0.0005
0.8165
0.0017
The standard errors for the ﬁrst and ﬁfth predictors are substantially larger than the
coefﬁents, and the corresponding t statistics are very small. A test that the coefﬁcients
are zero rejects only for the third, fourth, and sixth predictors. The problem is that the
predictors are quite highly correlated.
corr(x)
ans =
1.0000
0.9916
0.6206
0.4647
0.9792
0.9911
0.9916
1.0000
0.6043
0.4464
0.9911
0.9953
0.6206
0.6043
1.0000
-0.1774
0.6866
0.6683
0.4647
0.4464
-0.1774
1.0000
0.3644
0.4172
0.9792
0.9911
0.6866
0.3644
1.0000
0.9940
0.9911
0.9953
0.6683
0.4172
0.9940
1.0000
[u s v ] = svd(x);
diag(s.^2)/sum(diag(s.^2))
ans =
0.7672
0.1959
0.0339
0.0025
0.0004
0.0001
The ﬁrst three eigenvalues account for 99.7% of the variance and will be retained for
further analysis. The ﬁrst three eigenvectors are
v(:, 1:3)
ans =
0.4618
-0.0578
0.1491
0.4615
-0.0532
0.2777
0.3213
0.5955
-0.7283
389
10.8 Empirical Orthogonal Functions
.011
13:42:10, subject to the Cambridge Core terms of use,

0.2015
-0.7982
-0.5616
0.4623
0.0455
0.1960
0.4649
-0.0006
0.1281
The solution from eof regression using the ﬁrst three eigenvectors is
bp = v(:, 1:3)*inv(s(:, 1:3)'*s(:, 1:3))*s(:, 1:3)'*u'*y;
yhat = u*s(:, 1:3)*v(:, 1:3)'*bp;
sigma2 = (y - yhat)'*(y - yhat)/(n - p);
bpse = sqrt(sigma2*diag(v(:, 1:3)*inv(s(:, 1:3)'*s(:, 1:3))
*v(:, 1:3)'));
tp = bp./bpse;
pvalp = 2*(1 - tcdf(abs(tp), n - p));
bp'
ans =
0.2913
0.3587
-0.3090
-0.1186
0.3048
0.2751
bpse'
ans =
0.0149
0.0245
0.0641
0.0543
0.0182
0.0134
tp'
ans =
19.5302
14.6412
-4.8183
-2.1848
16.7107
20.5575
pvalp'
ans =
0.0000
0.0000
0.0007
0.0538
0.0000
0.0000
The p-values indicate that the regression coefﬁcients for all but the fourth predictor are
required, and acceptance of the null hypothesis is weak for that variable. The regression
coefﬁcients have changed dramatically, indicating the inﬂuence of multicollinearity on
these data.
390
Multivariate Statistics
.011
13:42:10, subject to the Cambridge Core terms of use,

11
Compositional Data
11.1 Introduction
Compositional data are vectors whose elements are mutually exclusive and exhaustive
percentages or proportions of some quantity. They are statistically unusual because the
constraint that the sum of the components of an observation be a constant is explicitly
imposed; for example, percentages must sum to 100 and proportions to 1. On ﬁrst look, this
appears not to be a major issue, and compositional data give the appearance of being
ordinary statistical data. As a consequence, statistical tools that do not take account of the
constrained form of compositional data have frequently been applied to them over the past
century despite warnings from statisticians and domain scientists that this could lead to
incorrect inferences.
Compositional data are ubiquitous in the earth and ocean sciences. Some common
examples include:
• Major or trace element analyses of rocks (expressed as wt%, ppm, or ppb) or ﬂuids
(usually in molar concentration);
• Grain size analyses of sediments;
• Micropaleontological assemblages (e.g., taxa abundance);
• Proportions of oil, gas, brine, and solids in petroleum-bearing formations; and
• Surface composition of planetary bodies from remote spectral sensing.
In each case, a data set may be obtained from a suite of samples, either as a way to ensure
that real variability is adequately represented (e.g., in rock composition or trace element
data) or to measure the variability as a function of time (e.g., sediment or foraminifera
composition).
The most obvious problem in treating compositions using statistical tools meant for
unconstrained data occurs for the Pearson correlation coefﬁcient. Let the elements of a
D-part composition be xi
f g, where PD
i¼1xi ¼ κ. It follows that
cov xi;
X
D
i¼1
xi
 
!
¼ 0
(11.1)
because κ is invariant, so
cov xi;x1
ð
Þ þ  þ cov xi;xi1
ð
Þ þ cov xi;xiþ1
ð
Þ þ  þ cov xi;xD
ð
Þ ¼ var xi
ð Þ (11.2)
The right side of (11.2) is negative unless xi is constant; hence at least one of the
covariances on the left side must be negative. Consequently, there must be at least one
391
.012
13:42:12, subject to the Cambridge Core terms of use,

negative entry in each row or column of the D  D covariance matrix. The result is a
negative bias on the covariance entries because the variables are not free to wander over the
standard range constrained only by nonnegative deﬁniteness of the covariance matrix. This
occurs because a D-part composition has only D  1 degrees-of-freedom. Inevitable
problems in interpretation ensue. Compositional data analysis has moved through four
phases. In the ﬁrst phase, lasting until about 1960, many of the tools of multivariate data
analysis were under development and were often applied to compositional data without
regard to whether or not they were appropriate and, in addition, largely ignoring the
prescient warning of Karl Pearson (1896) just before the turn of the twentieth century:
“Beware of attempts to interpret correlations between ratios whose numerators and
denominators contain common parts.”
In the second phase, there at least was recognition of (primarily) the correlation problem,
particularly by the petrologist Felix Chayes, but ad hoc workarounds using multivariate
statistics continued to be the focus. The third phase was almost entirely the work of the
Scottish statistician John Aitchison in the 1980s and 1990s. Aitchison realized that
compositions provide only information about the relative rather than absolute values of
its components and that the constrained compositional sample space precluded the direct
application of multivariate statistical tools to compositions. Aitchison advocated the use of
the additive and centered log ratio transformations (alr and clr) to carry compositions into
an unconstrained real space where multivariate statistical tools could be applied. This
required care because the alr does not preserve distance and the clr yields singular
covariance matrices. Opposition to this approach was and is rampant, especially among
petrologists. The fourth phase is based on working within the constrained sample space of
compositional data, an evolution that has largely occurred in Europe, particularly the
University of Girona in Spain.
Given the recent rapid evolution of the statistics of compositional data, texts on the
subject are limited, and developments are being published on an ongoing basis. Aitchison
(1986) wrote the ﬁrst text on compositional data, and it remains relevant. Pawlowsky-
Glahn & Buccianti (2011) put together a collection of original contributions covering
theory and applications in a large number of domains. Pawlowsky-Glahn, Egozcue, &
Tolosana-Delgado (2015) produced a text covering the theory and implementation of
compositional data analysis written from a mathematical rather than application-oriented
perspective. This chapter draws on these sources.
11.2 Statistical Concepts for Compositions
11.2.1 Deﬁnitions and Principles
A composition is a sample that is broken down into D mutually exclusive and exhaustive
parts. Each part has a corresponding label stating what physical attribute it represents
(e.g., sand, silt, or clay for sediment). The numerical elements, or components, of the
392
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

composition are strictly positive and have a constant sum κ that depends on the measure-
ment units. For example, xi may be the weight percent of a particular oxide, so
PD
i¼1xi ¼ 100 for each sample. Samples with a known and constant sum are called closed
data. Two compositional vectors x, y are compositionally equivalent if
x ¼ αy for a
positive scalar α. As a consequence, any change of units for the closure constant κ is
compositionally equivalent.
The closure operation on a D-part compositional vector is
C x
ð Þ ¼
κx1
P
D
i¼1
xi
; . . . ; κxD
P
D
i¼1
xi
2
6664
3
7775
(11.3)
Closure rescales a compositional vector so that its sum is a positive constant κ that depends
on the units of measurement (e.g., proportions, percent, ppm, etc). An equivalent statement
to x, y being compositionally equivalent is that C x
ð Þ ¼ C y
ð Þ. Under closure, the vectors
(1, 3, 5) and (2, 6, 10) are equivalent. A MATLAB function to implement closure is
function [Result ] = Close(x, varargin)
%
closes
a
composition
x
to
the
constant
kappa
that
defaults to one if not provided
if nargin == 1
kappa = 1;
else
kappa = varargin{1};
end
[n D ] = size(x);
Result = kappa*x./repmat(sum(x')', 1, D)
end
The sample space of compositional data is the simplex deﬁned as
SD ¼
x ¼ x1; . . . ; xD
ð
Þ xi
j
> 0;
X
D
i¼1
xi ¼ κ
(
)
(11.4)
This deﬁnition explicitly excludes elements of zero size, although those can be handled
with some difﬁculty. The most basic example of a simplex is ternary because two-
component compositions are trivial.
An s-part subcomposition xs of a D-part composition x is C xs
ð
Þ. This operation is a
projection from one subsimplex to another. Conversely, compositions can be built by
combining subcompositions.
While compositions frequently contain many parts (e.g., the approximately 16 major
oxides for rock major element compositions), the focus in the earth sciences is often on
groups of three. These can be represented on a ternary diagram comprising an equilateral
triangle. For a ternary diagram with unit edges, let B ¼ 0; 0
ð
Þ denote the location of the
left lower vertex. Then the remaining vertices are at C ¼ 1; 0
ð
Þ and A ¼ 0:5;
ﬃﬃﬃ
3
p
=2


,
393
11.2 Statistical Concepts for Compositions
.012
13:42:12, subject to the Cambridge Core terms of use,

respectively. A three-part closed composition α; β; γ
ð
Þ maps onto Cartesian coordinates on
the unit ternary through the isometric transformation
x ¼
α þ 2β
2 α þ β þ γ
ð
Þ
y ¼
ﬃﬃﬃ
3
p
α
2 α þ β þ γ
ð
Þ
(11.5)
so the composition 1; 0; 0
ð
Þ corresponds to the top apex, the composition 0; 1; 0
ð
Þ corres-
ponds to the lower right apex, and the composition 0; 0; 1
ð
Þ corresponds to the lower left
apex. The α axis increases from the lower right apex to the top one, the β axis increases
from the lower left apex to the lower right one, and the γ axis increases from the top apex to
the lower left one. The barycenter of a ternary diagram is the point 1=3; 1=3; 1=3
ð
Þ and
comprises its natural center. A set of MATLAB functions to support ternary diagrams is
contained in Appendix 11A.
A less useful way to reduce the dimensionality of a composition is amalgamation, which
is the summation of a subset of the composition components into a single new part. Let a
D-component composition be separated into two groups comprising r and s parts, respect-
ively. The amalgamation of the parts contained in the s group is
xA ¼
X
i2s
xi
(11.6)
so that the new composition is xr; xA
f
g possessing a sample space on the simplex SDsþ1.
Information about the original composition is lost through the process of amalgamation.
Following Aitchison (1986), there are three conditions that must be fulﬁlled by any
statistical method used on compositions. The ﬁrst is scale invariance, and will be
illustrated by example. Suppose that marine sediment consists of sand, silt, and clay at
weight percentages (in g/100 g) of 50, 40, and 10, respectively. The sediment is known to
have been moved and sorted via water current activity, and the redeposited sediment
contains sand, silt, and clay in weight percentages of 40, 53, and 7, respectively. This
could have happened by one of three end-member changes or by a mixture of them. In
the ﬁrst end member, sand is unchanged, but silt increases by 26 g/100 g and clay
decreases by 1 g/100 g. In the second end member, silt is unchanged, but sand decreases
by 20 g/100 g and clay decreases by 4.5 g/100 g. In the third end member, clay is
unchanged, but sand increases by 7 g/100 g and silt increases by 36 g/100 g. There is no
way to distinguish these three end members (or mixtures of them) based purely on
compositional data because the ﬁnal composition is only available after closure, and
hence compositional data contain only relative information. As a corollary, a function is
scale invariant if it yields the same result for all compositionally equivalent vectors [i.e.,
f αx
ð
Þ ¼ f x
ð Þ]. This property is only possible if f x
ð Þ is a function of ratios of the
components in x.
To illustrate the last point, there is a one-to-one relationship between the components
xi 2 SD and the set of independent and exhaustive ratios
yi ¼
xi
x1 þ    þ xD
i ¼ 1; . . . ; D  1
ð
Þ
yD ¼
1
x1 þ    þ xD
(11.7)
394
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

and the inverse relationship
xi ¼
yi
y1 þ    þ yD1 þ 1
i ¼ 1; . . . ; D  1
ð
Þ
xD ¼
1
y1 þ    þ yD1 þ 1
(11.8)
Consequently, working with the compositions or the ratios yi=yD
f
g is equivalent.
The second condition on compositional statistical procedures is permutation invariance.
A function is permutation invariant if it yields equivalent results when the ordering of the
parts of a composition is changed. This property is closely tied to the requirement in most
of statistics that entities be exchangeable.
The ﬁnal condition is subcompositional coherence, where the ratio of two components
remains unchanged as a full composition is decomposed into subcompositions. As a
corollary, the distance between two compositions must be greater than or equal to the
distance between two subcompositions derived from them. This is called subcompositional
dominance. Ordinary Euclidean distance does not satisfy this condition, so it is not an
appropriate measure of the distance between compositions. A second consequence is that if
a noninformative part of a composition is removed (e.g., an oxide in a rock composition
that does not vary between samples), the results of compositional analysis will not change.
11.2.2 Compositional Geometry
In an unrestricted real vector space such as is used in classical statistics for either
nonnegative or unrestricted real random variables, operations such as addition, scaling,
orthogonality, and distance are intuitive due to the familiar unconstrained Euclidean
geometry. The restriction to the simplex for compositional data imposes constraints that
are not compatible with such a geometry. Consequently, it is necessary to deﬁne a set of
operations on the simplex that give it a linear vector structure analogous to the Euclidean
geometry of real space. The compositional geometry, or geometry on the simplex, has been
given the name Aitchison geometry on the simplex.
There are two operations that give a simplex the characteristics of a vector space.
Perturbation of a composition x 2 SD by a second composition y 2 SD is deﬁned by
x  y ¼ C x1y1; . . . ; xDyD
ð
Þ
(11.9)
Perturbation is commutative
x  y ¼ y  x
(11.10)
and associative
x  y
ð
Þ  z ¼ x  y  z
ð
Þ
(11.11)
The unique neutral element deﬁnes the barycenter of the simplex
n ¼ C 1; . . . ; 1
ð
Þ ¼
1
D ; . . . ; 1
D


(11.12)
395
11.2 Statistical Concepts for Compositions
.012
13:42:12, subject to the Cambridge Core terms of use,

so the inverse of x becomes
x1 ¼ C x1
1 ; . . . ; x1
D


(11.13)
and x  x1 ¼ n. A MATLAB function that implements perturbation is
function [Result] = Perturbation(x, y, varargin)
%Performs the compositional operation perturbation on two
compositions
if nargin == 2
kappa = 1;
else
kappa = varargin{1};
end
[nx Dx] = size(x);
[ny Dy] = size(y);
x1 = x;
y1 = y;
if Dy == 1
y1 = repmat(y, 1, Dx);
elseif nx == 1
x1 = repmat(x, ny, 1);
elseif ny == 1
y1 = repmat(y, nx, 1);
if Dx ~= Dy
warning('Perturbation: compositions must have the same length')
return
elseif nx ~= ny
warning('Perturbation: number of observations must be the same')
return
end
Result = Close(x1.*y1, kappa)
end
Power transformation or powering of a composition x 2 SD by a real scalar α is deﬁned by
α  x ¼ C xα
1; . . . ; xα
D


(11.14)
Powering has the unique neutral element 1  x ¼ x. Powering is associative
α  β  x
ð
Þ ¼ αβ
ð
Þ  x
½

(11.15)
and has two distributive properties
α  x  y
ð
Þ ¼ α  x
ð
Þ  α  y
ð
Þ
(11.16)
α þ β
ð
Þ  x ¼ α  x
ð
Þ  β  x
ð
Þ
(11.17)
396
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

In addition, power transformation has the inverse relationship
1
ð
Þ  x ¼ x1
(11.18)
A MATLAB function that implements powering is
function [Result] = Powering(alpha, x, vargin)
%Computes the power transformation of a compositional vector x
if nargin == 2
kappa = 1;
else
kappa = varagin{1};
end
Result = Close(x.^alpha, kappa);
end
The combination of the simplex with perturbation and power transformation deﬁnes a
vector space, and the two operations are analogous to the roles played by addition and
multiplication in an unconstrained space. Perturbation and power transformation are,
respectively, internal and external operations, and the standard rules of precedence in
vector spaces would place power transformation ﬁrst and perturbation second.
To complete the speciﬁcation of the Aitchison geometry, the Aitchison inner product of
x, y 2 SD is deﬁned by
x; y
h
ia ¼ 1
2D
X
D
i¼1
X
D
j¼1
log xi
xj
 
log yi
yj
 !
¼
X
D
i¼1
log xi
g x
ð Þ log yi
g y
ð Þ
(11.19)
where g x
ð Þ is the geometric mean of the parts of the composition x. The log ratios in the
second line of (11.19) are the centered log ratio, which will be described later, and the
subscript a differentiates
x; y
h
ia from the ordinary inner product
x; y
h
i. A MATLAB
function implementing the Aitchison inner product is
function [Result] = AInnerProduct(x, y)
%Computes the Aitchison inner product of the compositions x and y
[nx Dx ] = size(x);
[ny Dy ] = size(y);
if Dx ~= Dy
warning('Ainnerprod: compositions must have the same length')
return
elseif nx ~= ny
warning('AInnerProduct: number of observations must be the same')
return
end
Result = zeros(nx, 1);
397
11.2 Statistical Concepts for Compositions
.012
13:42:12, subject to the Cambridge Core terms of use,

x1 = log(x./repmat(geomean(x')', 1, Dx));
y1 = log(y./repmat(geomean(y')', 1, Dy));
for i = 1:nx
Result(i) = x1(i, :)*y1(i, :)';
end
end
It follows from (11.19) that the Aitchison norm of x is
x
k k a ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
2D
X
D
i¼1
X
D
j¼1
log xi
xj
 

2
v
u
u
t
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
D
i¼1
log xi
g x
ð Þ

2
v
u
u
t
(11.20)
A MATLAB function ANorm(x) follows directly from AInnerProd(x, y).
The Aitchison distance between x and y is given by
da x; y
ð
Þ ¼
x  y1
		
		
a ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
2D
X
D
i¼1
X
D
j¼1
log xi
xj
 
 log yi
yj
 !
"
#2
v
u
u
t
(11.21)
Note that the Aitchison distance is compatible with perturbation and power transformation
because
da s  x; s  y
ð
Þ ¼ da x; y
ð
Þ
(11.22)
da α  x; α  y
ð
Þ ¼ αj jda x; y
ð
Þ
(11.23)
In addition, the Aitchison distance is subcompositionally coherent and subcompositionally
dominant. A MATLAB function that implements the Aitchison distance is
function [Result ] = ADistance(x, y)
%Computes the Aitchison distance between compositions x and y
Result = ANorm(Perturbation(x, 1./y));
end
Within the Aitchison geometry on the simplex, a compositional line is deﬁned by
y ¼ x0  α  x
ð
Þ
(11.24)
where x0 is the starting point, and x is the leading vector. A set of parallel lines comprises
(11.24) with different starting points but identical leading vectors. A set of orthogonal lines
occurs when the Aitchison inner product of the leading vectors vanishes and intersects at
the starting point. Figure 11.1 shows two sets of parallel lines on S3 produced by the
following script
x = [(0:.01:1)' (0:.01:1)' .5*ones(101, 1)];
Ternary;
x0 = [1/3 1/3 1/3];
alpha = 10;
Result = Perturbation(x0, Powering(alpha, x));
398
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

TernaryLine(Result)
x0 = [1/6 1/2 1/3];
Result = Perturbation(x0, Powering(alpha, x));
TernaryLine(Result)
x0 = [1/2 1/6 1/3];
Result = Perturbation(x0, Powering(alpha, x));
TernaryLine(Result)
x = [(0:.01:1)' .5*ones(101,1) (1:-.01:0)'];
x0 = [1/3 1/3 1/3];
Result = Perturbation(x0, Powering(alpha, x));
TernaryLine(Result)
x0 = [1/6 1/2 1/3];
Result = Perturbation(x0, Powering(alpha, x));
TernaryLine(Result)
x0 = [1/2 1/6 1/3];
Result = Perturbation(x0, Powering(alpha, x));
TernaryLine(Result)
Perturbation of a straight line in a ternary diagram results in a straight line with different
beginning and ending points. Figure 11.2 compares ternary diagrams before and after
perturbation of the center by the composition [2 3 6].
subplot(1, 2, 1)
Ternary;
subplot(1, 2, 2)
Ternary(7, [2 3 6]);
0
0
0
0.1
0.1
0.1
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
1
1
1
Figure 11.1
Two sets of parallel lines on S3.
399
11.2 Statistical Concepts for Compositions
.012
13:42:12, subject to the Cambridge Core terms of use,

11.2.3 Compositional Transformations
Representation of compositional data using scale invariant log ratios or log contrasts was
introduced by Aitchison in the early 1980s and is deﬁned by
log
Y
D
i¼1
xαi
i
 
!
¼
X
D
i¼1
αi log xi
(11.25)
0
0
0
0.1
0.1
0.1
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
1
1
1
0
0
0
0.167
0.167
0.167
0.333
0.333
0.333
0.5
0.5
0.5
0.667
0.667
0.667
0.833
0.833
0.833
1
1
1
Figure 11.2
Ternary diagram showing grid lines before (left) and after perturbation by the composition [2 3 6].
400
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

X
D
i¼1
αi ¼ 0
(11.26)
The ﬁrst log contrast that was implemented is the additive log ratio transformation (alr). If
x 2 SD, the alr is given by
alr x
ð Þ ¼
log x1
xD
; . . . ; log xD1
xD


¼ Θ
(11.27)
and is easily inverted to yield the composition
x ¼ alr1 Θ
ð Þ ¼ C eΘ1; . . . ; eΘD


(11.28)
The alr changes perturbation and power transformation into standard operations in a
(D – 1)-dimensional real space (hence matching the dimensionality of SD) because
alr α  x
ð
Þ  β  y
ð
Þ
½
 ¼ αalrx þ βalry
(11.29)
The chief limitation to the alr is that it is not permutation invariant because the Dth part
(which can be chosen to be any part of the composition) plays a special role, and hence
many standard statistical procedures based on it fail due to the absence of exchangeability.
In addition, standard distance measures cannot be easily used on the alr. The alr is now
primarily of historical interest, but it serves to introduce the concept of compositional
transformations that move compositions from the simplex to a real vector space where
standard operations and statistical tools can be applied, following which the result can be
reverse transformed back to the simplex.
The problems with the alr led Aitchison to also introduce the centered log ratio
transformation (clr) from SD to RD given by
clrx ¼
log x1
g x
ð Þ ; . . . ; log xD
g x
ð Þ


 ξ
(11.30)
where PD
i¼1ξi ¼ 0. The D elements of the clr are log contrasts as in (11.25) but with a
natural rather than arbitrary reference. The inverse clr is
clr1 ξ
ð Þ ¼ C eξ1; . . . ; eξD


¼ x
(11.31)
The clr is symmetric in the components (unlike the alr) but imposes the constraint that the
sum of the transformed sample is zero. This leads to a singular covariance matrix for ξ. In
addition, clr coefﬁcients are not subcompositionally coherent because the geometric mean
of a subcomposition will not in general be the same as that of the entire composition, and
hence the clr coefﬁcients of a subcomposition will not be consistent with that of the whole.
The following functions implement the clr and its inverse in MATLAB:
function [Result] = Clr(x)
%Applies the centered log ratio transformation to the composition x
[n D ] = size(x);
Result = log(x./repmat(geomean(x')', 1, D));
end
401
11.2 Statistical Concepts for Compositions
.012
13:42:12, subject to the Cambridge Core terms of use,

function [Result] = ClrI(x, varargin)
%Computes the inverse clr of x
if nargin == 1
kappa = 1;
else
kappa = varargin{1};
end
Result = Close(exp(x), kappa);
end
The clr has several key properties that make it useful for many purposes:
clr α  x1  β  x2
ð
Þ ¼ αclrx1 þ βclrx2
x1; x2
h
ia ¼ clrx1; clrx2
h
i
x1
k
k a ¼
clrx1
k
k
da x1; x2
ð
Þ ¼ d clrx1; clrx2
ð
Þ
(11.32)
The ﬁrst equation in (11.32) shows that perturbation and power transformation on the
simplex correspond to addition and multiplication by a scalar in a D-dimensional real
space, whereas the remaining three equations show that the Aitchison inner product,
norm, and distance measure on the simplex correspond to the standard ones on the a real
vector space. However, the real vector space is not fully unconstrained because the sum
of the clr elements must be zero and hence in reality exist on a particular (D – 1)-
dimensional subspace of RD. In addition, the components of (11.32) apply to the entire
composition and not to pairs from it because the clr does not yield an orthonormal
basis set.
The Aitchison geometry and standard practice in a real vector space suggest that the
usual approach of deﬁning an orthonormal basis and coordinates would be useful. Let
e1; . . . ; eD1
f
g be an orthonormal basis on SD that must satisfy [using (11.32)]
ei; ej


a ¼ clrei; clrej


¼ δij
(11.33)
where
ei
k k a ¼ 1. Once an orthornormal basis has been constructed, a composition
x 2 SD may be written
x ¼ ilr1 x∗¼ x∗
1  e1


     x∗
D1  eD1


¼ 
D1
j¼1
x∗
j  ej


(11.34)
x∗¼ ilrx ¼
x; e1
h
ia; . . . ; x; eD1
h
ia


(11.35)
Equation (11.35) deﬁnes the coordinates of x relative to the orthonormal basis using the
Aitchison inner product, and (11.34) is an expansion of x in terms of the basis.
Equation (11.35) is the isometric log ratio transformation (ilr; where isometric means
transformation without change of shape or size) that assigns the coordinates x∗to x and is
a mapping from SD ! RD1, where clr is a constrained mapping from SD ! RD. The ilr
has the following properties that should be compared with (11.32)
402
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

ilr α  x1  β  x2
ð
Þ ¼ αilrx1 þ βilrx2 ¼ αx∗
1 þ βx∗
2
x1; x2
h
ia ¼ ilrx1; ilrx2
h
i ¼ x∗
1 ; x∗
2


x
k k a ¼
ilrx
k
k ¼
x∗
k
k
da x1; x2
ð
Þ ¼ d ilrx1; ilrx2
ð
Þ ¼ d x∗
1 ; x∗
2


(11.36)
Let Ψ be the D  1
ð
Þ  D contrast matrix whose rows are clrei so that
Ψ  ΨT ¼ ID1
ΨT  Ψ ¼ ID  JD=D
(11.37)
In a compact form, the ilr becomes
x∗¼ ilrx ¼ clrx  ΨT
(11.38)
so that the inverse is, using (11.31),
x ¼ ilr1x∗¼ C exp x∗ Ψ
ð
Þ
½

(11.39)
where the last term is just clr1 x∗ Ψ
ð
Þ from (11.31).
The remaining issue is how one computes the orthonormal basis ei
f g and contrast matrix
Ψ because there are an inﬁnite number of orthornormal bases in SD. This can be done using
empirical orthogonal functions (see Section 10.8), although the result may not have a clear
physical interpretation. The alternative is construction of a basis using a Gram-Schmidt
procedure starting from a partition of the data that is driven by domain knowledge.
A starting point is a sequential binary partition (SBP) of a composition vector, which is
a hierarchy obtained by splitting the parts into two groups and then dividing each group
into two groups and continuing until all groups have a single part. For a D-part compos-
ition, there are D  1 divisions to complete a SBP. Obviously, there are many ways to
apply a SBP, and it should be done in a manner that makes interpretation of the data easier,
requiring some degree of insight into the underlying physics or chemistry. For example,
chemical species may be divided into cations and anions, or oxides may be divided into
silicates and nonsilicates.
Once a sequential binary partition is deﬁned, an orthonormal basis can be constructed
using the Gram-Schmidt procedure. The Cartesian coordinates of a composition using such
a basis is called a balance, and the composition comprises balancing elements. A balance
differs from the clr only in that the composition is not centered and is given by
ϒ ¼ log x
ð Þ  ΨT
(11.40)
Let a sequential binary sequence be encoded for each part using 1 or 1, with 0 indicating
parts that are omitted. For a D-part composition, let there be r elements encoded with 1, s
elements encoded with 1, and t elements encoded with 0, where r þ s þ t ¼ D. Table 11.1
is an exemplar SBP for a ﬁve-part composition.
The contrast matrix Ψ is obtained by counting the number of +1 entries r and 1 entries
s in a given row and computing
403
11.2 Statistical Concepts for Compositions
.012
13:42:12, subject to the Cambridge Core terms of use,

ψþ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s
r r þ s
ð
Þ
r
ψ ¼ 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r
s r þ s
ð
Þ
r
(11.41)
The column entries are given by ψþ or ψ, where the SBP has +1 or 1 entries,
respectively, and 0 where the SBP has a corresponding 0 entry. The contrast matrix is
shown in Table 11.2 for the SBP of Table 11.1.
Caution: It is very easy to make a mistake when constructing an SBP. It is prudent to
check the contrast matrix for orthonormality by evaluating (11.37) before using it for
data analysis.
Once the contrast matrix has been constructed, the ilr follows from (11.38). The contrast
matrix can be constructed from an SBP using the following MATLAB function:
function [Result] = Contrast(sbp)
%Computes
the
contrast
matrix
from
the
sequential
binary
partition sbp
[Dm1 D ] = size(sbp);
if D ~= Dm1 + 1
warning('Contrast: sbp has wrong number of rows')
return
end
Result = zeros(Dm1, D);
for i = 1:Dm1
r = ﬁnd(sbp(i, :) == 1);
rs = sum(sbp(i, r));
Table 11.1 Exemplar Sequential Binary Partition
Order
x1
x2
x3
x4
x5
r
s
1
+1
+1
+1
1
1
3
2
2
+1
+1
1
0
0
2
1
3
+1
1
0
0
0
1
1
4
0
0
0
+1
1
1
1
Table 11.2 Exemplar Contrast Matrix
Order
x1
x2
x3
x4
x5
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2=15
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2=15
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2=15
p

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
3=10
p

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
3=10
p
2
ﬃﬃﬃﬃﬃﬃﬃﬃ
1=6
p
ﬃﬃﬃﬃﬃﬃﬃﬃ
1=6
p

ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
0
0
3
ﬃﬃﬃﬃﬃﬃﬃﬃ
1=2
p

ﬃﬃﬃﬃﬃﬃﬃﬃ
1=2
p
0
0
0
4
0
0
0
ﬃﬃﬃﬃﬃﬃﬃﬃ
1=2
p

ﬃﬃﬃﬃﬃﬃﬃﬃ
1=2
p
404
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

s = ﬁnd(sbp(i, :) == -1);
ss = -sum(sbp(i, s));
psip = sqrt(ss/(rs*(rs + ss)));
psim = -sqrt(rs/(ss*(rs + ss)));
Result(i, r) = psip;
Result(i, s) = psim;
end
end
The ilr and its inverse can be obtained using the following MATLAB functions:
function [Result] = Ilr(x, Psi)
%Computes the ilr of the composition x using the contrast
matrix Psi
Result = Clr(x)*Psi';
end
function [Result] = IlrI(x, Psi, varargin)
%Computes the inverse ilr using the contrast matrix Psi
if nargin == 2
kappa = 1
else
kappa = varargin{1};
end
Result = ClrI(x*Psi, kappa);
end
11.3 Exploratory Compositional Data Analysis
The initial steps in compositional data analysis comprise the exploratory ones: computing
descriptive statistics such as the center and variation matrix, centering the data so that
subcompositions can be viewed in ternary diagrams, identifying patterns of variability,
constructing an orthonormal coordinate system, and projecting the data into it and comput-
ing summary statistics. The remainder of this chapter covers these topics in some detail. At a
more advanced stage, distributions on the simplex need to be introduced so that statistical
tests (which are inevitably multivariate) can be devised. This leads to hypothesis testing and
conﬁdence interval estimation as well as linear regression. These topics are covered in the
last few chapters of Pawlowsky-Glahn, Egozcue, & Tolosana-Delgado (2015).
For the purposes of illustration, a data set consisting of the major oxides from basalts
from 23 to 35	N on the Mid-Atlantic Ridge (MAR) was downloaded from the PetDB data
archive (www.earthchem.org/petdb). This section of the ridge was selected because it is
known to display signiﬁcant compositional variability proximal to the Azores hotspot.
About 41 samples were chosen based on the absence of zeros in the 11 major element
405
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

oxides SiO2, TiO2, Al2O3, Fe2O3, FeO, MnO, MgO, CaO, Na2O, K2O, and P2O5. The parts
have that order. An exploratory compositional data analysis will be carried out on them.
Example 11.1 Using the MAR data, demonstrate their inconsistency with standard estima-
tors for correlation and the requirement to use compositional concepts.
Input and close the data.
mar = xlsread('MAR.xls');
mar = mar(2:length(mar), :);
%keep the major oxides
mar = mar(:, 3:13);
%keep only data that have all of the major oxides
mar = [mar(1:4, :)' mar(22, :)' mar(47:88, :)']';
%remove redundant outliers at 37-38 and 39-40
%remove redundant data at 24-26 and 30-36
mar = [mar(1:25, :)' mar(27:35, :)' mar(41:47, :)']';
mar = Close(mar, 100);
In classical statistics, the standard descriptive statistics are the sample mean and variance
(or covariance). However, these are not suitable as measures of central tendency and
dispersion for compositions, in particular, for the reason elaborated in (11.2), as can be
readily shown. The standard correlation coefﬁcients between (CaO, K2O), (Fe2O3, FeO),
(FeO, MgO), and (Fe2O3, MgO) are
[corr(mar(:,
8),
mar(:,
10))
corr(mar(:,
4),
mar(:,
5))
corr(mar(:, 5), mar(:, 7)) corr(mar (:, 4), mar(:, 7))]
ans =
0.2850
-0.5243
0.4712
-0.3587
Delete SiO2 and Al2O3 from the data set, reclose and recompute the correlation coefﬁ-
cients.
mar1=[mar(:, 2) mar(:, 4:11)];
mar1 = Close(mar1, 100);
[corr(mar1(:, 6), mar1(:, 8)) corr(mar1(:, 2), mar1(:, 3))
corr(mar1(:, 3), mar1(:, 5)) corr(mar1 (:, 2), mar1(:, 5))]
ans =
0.1889
-0.6507
0.2554
-0.4698
The correlation matrix between the maﬁc constituents (MgO, FeO, Fe2O3) before
closing the subcomposition is
corr([mar(:, 4:5) mar(:, 7)])
ans =
1.0000
-0.5243
-0.3587
-0.5243
1.0000
0.4712
-0.3587
0.4712
1.0000
406
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

and after closure
mar1 = Close([mar(:, 4:5) mar(:, 7)], 100);
corr(mar1)
ans =
1.0000
-0.6311
-0.4919
-0.6311
1.0000
-0.3649
-0.4919
-0.3649
1.0000
The differences are dramatic and illustrate the incompatibility of classical summary statis-
tics with the Aitchison geometry. Classical summary statistics for the entire data set would
be the sample mean (by columns) and the covariance matrix.
mean(mar)
ans =
Columns 1 through 7
50.1381
1.3846
16.1387
1.4690
7.9954
0.1662
8.1798
Columns 8 through 11
11.4451
2.7109
0.2264
0.1459
cov(mar)
ans =
Columns 1 through 7
0.9874
0.1222
-0.2330
0.1257
-0.3626
-0.0008
-0.8500
0.1222
0.0947
-0.1605
0.0953
-0.0272
0.0003
-0.1292
-0.2330
-0.1605
0.8230
-0.1259
-0.2958
-0.0040
-0.0273
0.1257
0.0953
-0.1259
0.3992
-0.3045
-0.0019
-0.2406
-0.3626
-0.0272
-0.2958
-0.3045
0.8447
0.0071
0.4599
-0.0008
0.0003
-0.0040
-0.0019
0.0071
0.0004
0.0036
-0.8500
-0.1292
-0.0273
-0.2406
0.4599
0.0036
1.1273
0.0388
-0.0950
0.0496
-0.0898
-0.1361
-0.0043
-0.0879
0.1311
0.0564
0.0666
0.0880
-0.1225
0.0010
-0.1815
0.0239
0.0307
-0.0609
0.0452
-0.0586
-0.0013
-0.0576
0.0173
0.0122
-0.0318
0.0092
-0.0044
-0.0001
-0.0168
Columns 8 through 11
0.0388
0.1311
0.0239
0.0173
-0.0950
0.0564
0.0307
0.0122
0.0496
0.0666
-0.0609
-0.0318
-0.0898
0.0880
0.0452
0.0092
-0.1361
-0.1225
-0.0586
-0.0044
-0.0043
0.0010
-0.0013
-0.0001
-0.0879
-0.1815
-0.0576
-0.0168
0.4385
-0.1540
0.0372
0.0031
-0.1540
0.1216
-0.0061
-0.0006
0.0372
-0.0061
0.0388
0.0088
0.0031
-0.0006
0.0088
0.0031
407
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

Note that each row or column of the covariance matrix has one or more negative entries,
illustrating (11.2). The data are quite multivariate, and the compositional constraint is
clearly playing a signiﬁcant statistical role.
The replacement for the sample mean in the Aitchison geometry is the center of the data
given by the closed geometric mean
g ¼ C g1; . . . ; gD
ð
Þ
gj ¼
Y
N
i¼1
xij
 
!1=N
(11.42)
The product in (11.42) is taken by rows (i.e., over all samples for a given component),
where in the deﬁnition of clr (11.30) it is taken by columns (i.e., over all components for a
given sample). The center (11.42) is an unbiased minimum variance estimator for the
expected value of a random composition.
Example 11.2 For the MAR data, evaluate the geometric mean.
Close(geomean(mar), 100)
ans =
Columns 1 through 7
50.3376
1.3533
16.1814
1.3658
7.9745
0.1655
8.1521
Columns 8 through 11
11.4750
2.6985
0.1617
0.1345
Dispersion of a compositional data set is obtained through the D  D variation matrix
T ¼
tij
 
tij ¼ var log xi
xj
 


(11.43)
This is sometimes given as the normalized variation matrix T∗, where
t∗
ij ¼ var log xi=xj


=
ﬃﬃﬃ
2
p
h
i
(11.44)
in which case the log ratio is a balance. In either instance, the entries are the variances
of the log ratio between the ith and jth parts. Note that T∗¼ T=2, where the normal-
ized entries have units of squared Aitchison distance. T and T∗are symmetric and have
zeros on the main diagonal. Further, the entries in either matrix are independent of the
units of the composition because it involves only their ratios, so rescaling has no
inﬂuence.
408
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

A summary statistic for overall dispersion is given by the total variance
totvar x
ð Þ ¼
1
2D
X
D
i¼1
X
D
j¼1
var log xi
xj
 


¼
1
2D
X
D
i¼1
X
D
j¼1
tij ¼ 1
D
X
D
i¼1
X
D
j¼1
t∗
ij
(11.45)
Example 11.3 For the MAR data, compute and analyze the normalized variation matrix and
total variance.
The following MATLAB function produces this matrix
function [T, totvar] = NVariation(x)
%computes the normalized variation matrix for the composition x.
[n D ] = size(x);
k = 1:D;
ratio = zeros(D, D, n);
for i = 1:n
for j = 1:D
ratio(j, k, i) = log(x(i, j)./x(i, k));
end
end
T = var(ratio/sqrt(2), 1, 3);
totvar = sum(sum(T))/D;
end
Only the upper triangle needs presentation and is as follows:
[T, totvar ] = NVariation(mar);
T
T =
SiO2
TiO2
Al2O3
Fe2O3
FeO
MnO
MgO
CaO
Na2O
K2O
P2O5
0
0.027
0.002
0.073
0.008
0.010
0.009
0.002
0.008
0.419
0.096
SiO2
0
0.038
0.054
0.040
0.037
0.049
0.036
0.020
0.291
0.040
TiO2
0
0.082
0.011
0.012
0.009
0.003
0.009
0.458
0.119
Al2O3
0
0.105
0.085
0.101
0.082
0.062
0.320
0.102
Fe2O3
0
0.010
0.007
0.010
0.021
0.483
0.118
FeO
0
0.013
0.013
0.016
0.456
0.113
MnO
0
0.010
0.024
0.498
0.133
MgO
0
0.016
0.428
0.103
CaO
0
0.416
0.101
Na2O
0
0.147
K2O
0
P2O5
The values are coded as <0.05 (dark gray), 0.05 to 0.3 (light gray), and >0.3 (white).
The highest values (>0.3) are K2O with the other oxides, and the intermediate values are
409
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

P2O5 with the other oxides save TiO2. Both of these may reﬂect the low concentrations of
K2O and P2O5. The other element with intermediate variability is Fe2O3.
totvar
totvar =
1.0823
A common way to plot data in a ternary diagram is to shift and rescale the data so that
they are approximately in the middle, and their range over the three coordinates is
approximately the same. This is a perturbation, although applied in an ad hoc manner.
Recall from (11.12) and (11.13) that x  x1 ¼ n, where n is the barycenter of the
simplex. A composition can be moved to the barycenter by computing the center of
the data from (11.42) and perturbing the data by g1. Such a transformation changes
gridlines on ternary diagrams by the same perturbation (see Figure 11.2). By analogy to
studentization of a classical statistical variable, a composition can be scaled by power
transformation with totvar x
ð Þ1=2. This results in data with the same relative contribution
from each log ratio.
Example 11.4 Plot ternary diagrams of the oxides (Fe2O3, K2O, P2O5) that exhibit the
highest covariation in Example 11.3 before and after centering.
mar1 = Close([mar(:, 4), mar(:, 10), mar(:, 11)]);
subplot(1, 2, 1)
Ternary;
TernaryPlot(mar1, 'd', 'k')
TernaryLabel('Fe2O3', 'K2O', 'P2O5')
subplot(1, 2, 2)
g = Close(geomean(mar1));
Ternary(0, 1./g, [0 .1 .5 .9 1]);
cmar1 = Perturbation(mar1, 1./g);
TernaryPlot(cmar1, 'd', 'k')
TernaryLabel('Fe2O3', 'K2O', 'P2O5')
The center and variation matrix provide summary information about compositions, and
perturbation and power transformation using the data center and total variance allow their
plotting in subcompositions of three, although without guidance from domain understand-
ing or statistical insight, this would be a time-consuming task because there are
D
3


unique ternary diagrams for a D-part composition. The next step in exploring compos-
itional data is use of a dendrogram to evaluate the relationships among the clrs of the data.
The dendrogram groups the clrs into a hierarchical binary cluster tree with the heights of
the U-shaped sections proportional to the distance between the data being connected.
410
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

Example 11.5 Present and describe a dendrogram for the clr of the centered MAR data.
g = Close(geomean(mar));
tree = linkage(Clr(Perturbation(mar, 1./g))');
dendrogram(tree)
0
0
0
0.1
0.1
0.1
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
1
1
1
Fe2O3
K2O
P2O5
0
0
0
0.1
0.1
0.1
0.5
0.5
0.5
0.9
0.9
0.9
1
1
1
Fe2O3
K2O
P2O5
Figure 11.3
A ternary diagram of Fe2O3-K2O-P2O5 before (left) and after (right) centering the data.
411
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

Figure 11.4 shows the result. It is interesting that the incompatible elements Fe3+, K, and
P cluster at the right and are together a large distance from the remaining ones but with Ti
the closest of them, followed by Mn. The remaining elements are close together in the
clusters (Fe+2, Mg, Na) and (Si, Al, Ca).
Empirical orthogonal functions summarize the interrelation of all the clrs of the data taken
together. These are obtained by using the singular value decomposition on the clr (11.30)
after centering the data about their geometric mean. First compute
ξ
$ ¼ clr x  g1


(11.46)
This quantity is decomposed as
ξ
$¼ U
$  S
$ V
$T
(11.47)
Example 11.6 Evaluate the svd of the MAR data.
g = Close(geomean(mar));
[u, s, v ] = svd(Clr(Perturbation(mar, 1./g)));
diag(s).^2./sum(diag(s).^2)'
ans =
0.8158
0.1018
0.0321
0.0218
0.0134
0.0081
0.0047
0.5
1
1.5
2
2.5
3
3.5
SiO2
CaO Al2O3 FeO
Na2O MnO
TiO2
Fe2O3
P2O5
MgO
K2O
Figure 11.4
Dendrogram of the clr of the centered MAR data.
412
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

0.0018
0.0004
0.0002
0.0000
The ﬁrst two eigenvalues explain 91.8% of the variance. The ﬁrst three eigenvalues
explain 95% of the variance. The last singular value is precisely zero, reﬂecting the fact
that the clr results in a singular covariance matrix. The ﬁrst 10 right singular vectors are
-0.143
0.066
-0.169
0.084
0.153
-0.130
0.099
-0.082
0.499
-0.737
0.042
-0.016
0.561
0.291
0.109
0.374
0.093
-0.568
-0.150
-0.050
-0.187
0.043
-0.295
0.240
0.209
-0.170
-0.123
0.106
-0.764
-0.214
0.064
-0.901
0.004
-0.289
0.051
-0.042
0.059
0.042
-0.021
0.003
-0.210
0.225
0.117
-0.247
-0.030
0.161
0.751
0.364
-0.083
0.100
-0.179
0.069
0.053
-0.103
-0.862
-0.246
-0.128
-0.185
-0.063
-0.034
-0.227
0.141
-0.170
-0.354
0.056
0.636
-0.502
0.110
0.090
0.059
-0.152
0.117
-0.371
-0.056
0.267
-0.297
0.096
-0.477
0.193
0.546
-0.128
-0.117
0.123
0.665
-0.052
-0.025
-0.151
0.460
0.298
0.309
0.830
0.115
-0.353
0.106
-0.166
0.184
0.103
0.031
-0.008
0.004
0.291
0.258
0.497
-0.338
0.264
-0.446
-0.297
0.199
0.008
0.014
The three largest elements in each are highlighted in gray. The eigenvectors in rank order
show (1) the largest value at K2O but correlation with P2O5 and anticorrelation with MgO,
(2) the largest value at Fe2O3 but anticorrelation with FeO and P2O5, (3) the largest value at
TiO2 but correlation with P2O5 and anticorrelation with CaO, (4) the largest value at
Na2O but anticorrelation with MgO and P2O5, (5) the largest value at MnO but antic-
orrelation with TiO2 and P2O5, (6) the largest value at MgO but correlation with TiO2 and
anticorrelation with P2O5, (7) the largest value at FeO but anticorrelation with MgO and
P2O5, (8) the largest value at TiO2 but correlation with Na2O and anticorrelation with CaO,
(9) the largest value at Al2O3 but anitcorrelation with SiO2 and Na2O, and (10) the largest
value at SiO2 but anticorrelation with CaO and Na2O.
A biplot can be used to evaluate the variability in the data and can be easily obtained with
MATLAB. A biplot of compositional data (Aitchison & Greenacre 2002) can take on two
end-member forms depending on whether the eigenvectors in V
$ are scaled by the corres-
ponding singular values or not. The former are called principal coordinates, and the latter
are called standard coordinates. The biplot in these two types of coordinates are called
covariance and form biplots, respectively. The length of the rays in a covariance biplot is
proportional to the standard deviation of the clr components of the centered data, whereas
the form biplot should have rays that are identical in length if all the data are represented.
There are corresponding observation biplots that will not be considered here.
In each case, the origin is the center of the compositional data set. The join of the origin to
each vertex is called a ray, and the join of two vertices is called a link (not shown in the
ﬁgures). The lengths of links and rays are a measure of relative variability of the composition,
whereas their orientation provides information about their correlation. The square of the
length of the i, j link is approximately var log xi=xj




, whereas the square of the ith ray
413
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

is approximately var clr xi
ð Þ
½
. If two links are at right angles, then the corresponding
pair of log ratios is uncorrelated. If two or more vertices coincide, then the ratio of
those two parts of the composition is constant, and hence the two parts are redundant.
Finally, if two or more rays are collinear, this suggests that their subcomposition
constitutes a compositional line.
Example 11.7 Compute covariance and form biplots for the ﬁrst two eigenvectors of the
MAR data, and present the key insights they provide as ternary diagrams before and after
centering.
The covariance biplot is given by the MATLAB script
labels = {'SiO2', 'TiO2', 'Al2O3', 'Fe2O3', 'FeO', 'MnO',
'MgO', 'CaO', 'Na2O', 'K2O', 'P2O5'};
biplot([s(1, 1)*v(:, 1) s(2, 2)*v(:, 2)], 'VarLabels', labels)
axis square
and is shown in Figure 11.5. The form biplot is shown in Figure 11.6.
The covariance biplot in Figure 11.5 is somewhat one-dimensional, reﬂecting the size of
the ﬁrst singular value. It is dominated by rays from K2O, Fe2O3, and P2O5, which are the
same pattern seen in the dendrogram in Figure 11.4. The rays for the oxides SiO2, Al2O3,
MnO, and CaO are nearly coincident and are not labeled in Figure 11.5. As a consequence,
they are redundant. They are coincident in direction but not magnitude with FeO and MgO.
The links for FeO-P2O5 and Fe2O3-P2O5 are approximately orthogonal, so their ratios are
independent. The same holds for FeO-Na2O and Na2O-K2O.
In Figure 11.6, the rays for the predominant oxides Fe2O3, K2O, and P2O5 are more
nearly the same size compared with Figure 11.5, but the entire data set is not evenly
represented, suggesting that more eigenvectors might be needed. It is possible to represent
a three-eigenvector biplot in MATLAB. The three-eigenvector covariance and form biplots
are shown in Figures 11.7 and 11.8, respectively. The covariance biplot is difﬁcult to
–5
0
–5
Component 1
–5
–4
–3
–2
–1
0
1
2
3
4
5
Component 2
TiO2
Fe2O3
FeO
MgO
Na2O
K2O
P2O5
Figure 11.5
Covariance biplot of the MAR oxide data.
414
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

–0.5
0
0.5
Component 1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
Component 2
TiO2
Al2O3
Fe2O3
FeO
MgO
Na2O
K2O
P2O5
CaO
Figure 11.6
Form biplot of the MAR oxide data.
Fe2O3 TiO2
FeO
P2O5
K2O
Na2O
MgO
–5
5
0
5
Component 3
Component 2
0
Component 1
5
0
–5
–5
Figure 11.7
Covariance biplot for the MAR data using three eigenvectors.
Al2O3
CaO
TiO2
–0.5
0.5
0
Component 3
0.5
0.5
Component 2
0
Component 1
0
–0.5
–0.5
Fe2O3
SiO2
P2O5
K2O
Na2O
MgO
Figure 11.8
Form biplot for the MAR data using three eigenvectors.
415
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

evaluate because so many of the components are nearly coincident, but the form biplot is
more evenly distributed. These ﬁgures can be rotated in 3D within MATLAB, and that is
necessary to understand the relationships in them.
Figure 11.9 shows the FeO-Na2O-P2O5 ternary that shows little correlation of the
data either before or after centering, consistent with what is seen in Figures 11.5
through 11.8.
0
0
0
0.1
0.1
0.1
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
1
1
1
FeO
Na2O
P2O5
0
0
0
0.1
0.1
0.1
0.3
0.3
0.7
0.7
0.7
0.3
0.9
0.9
0.9
1
1
1
FeO
Na2O
P2O5
Figure 11.9
Ternary diagrams for FeO-Na2O-P2O5 before (left) and after (right) centering.
416
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

Figure 11.10 shows a ternary diagram of the ferric minerals Fe2O3-FeO-MgO for which
the biplots suggest the existence of a compositional line that is readily apparent in the
centered data plot.
Figure 11.11 shows a ternary diagram of the feldspar minerals CaO-Na2O-K2O on which
the data plot as nearly constant proportions of 0.8 CaO to 0.2 Na2O and K2O.
0
0
0
0.1
0.1
0.1
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
1
1
1
Fe2O3
FeO
MgO
0
0
0
0.1
0.1
0.1
0.3
0.3
0.3
0.5
0.5
0.5
0.7
0.7
0.7
0.9
0.9
0.9
1
1
1
Fe2O3
FeO
MgO
Figure 11.10
Ternary diagram for Fe2O3-FeO-MgO before (left) and after (right) centering.
417
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

Example 11.8 Deﬁne a sequential binary partition, and apply it to the MAR data.
Insight into the underlying scientiﬁc problem is required to suggest a sequential binary
partition of the composition. This should be driven by an understanding of the chemistry
and mineralogy of basalt. A candidate set of criteria taken from Pawlosky-Glahn et al.
(2015) consists of
0
0
0
0.1
0.1
0.1
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
1
1
1
CaO
Na2O
K2O
0
0
0
0.2
0.2
0.2
0.8
0.8
0.8
1
1
1
CaO
Na2O
K2O
Figure 11.11
Ternary diagram for CaO-Na2O-K2O before (left) and after (right) centering.
418
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

1. Fe2O3 versus FeO (Fe3+ versus Fe2+ oxidation state proxy);
2. SiO2 versus Al2O3 (silica saturation proxy; when Si is lacking, Al takes its place);
3. Distribution within heavy minerals (rutile versus apatite);
4. Silicates versus heavy minerals;
5. Distribution within plagioclase (anorthite versus albite);
6. Plagioclase versus potassium feldspar;
7. Distribution within maﬁc nonferric minerals (MnO versus MgO);
8. Ferric (Fe2O3 and FeO) versus nonferrous (MnO and MgO) maﬁc minerals;
9. Maﬁc versus felsic minerals; and
10. Structure ﬁlling or cation oxides (those ﬁlling the crystalline structure of minerals)
versus frame-forming oxides (i.e., those forming the structure).
Such criteria will account for changes in composition related to variations in partial melting
of the mid-ocean ridge basalt (MORB) mantle source and for any effects of shallow level
fractionation. A MATLAB script to compute the ilr and present the results initially as a
boxplot is
sbp = [0 0 0 1 -1 0 0 0 0 0 0;
1 0 -1 0 0 0 0 0 0 0 0;
0 1 0 0 0 0 0 0 0 0 -1;
1 -1 1 0 0 0 0 0 0 0 -1;
0 0 0 0 0 0 0 1 -1 0 0;
0 0 0 0 0 0 0 1 1 -1 0;
0 0 0 0 0 1 -1 0 0 0 0;
0 0 0 1 1 -1 -1 0 0 0 0;
0 0 0 1 1 1 1 -1 -1 -1 0;
1 1 1 -1 -1 -1 -1 -1 -1 -1 1];
Psi = Contrast(sbp);
xstar = Ilr(mar, Psi);
boxplot(xstar)
The result is shown in Figure 11.12. The center of each boxplot element is very nearly
the ilr of the geometric mean of the observations in the composition. The larger the
distance a given boxplot center is from zero on the y-axis, the more one part of the balance
is larger than the other. Moving through the balances (1) Fe2O3 versus FeO is strongly
toward the ferrous lower oxidation state with some variability, (2) silica saturation is
toward silica with little variability, (3) distribution within heavy minerals is strongly toward
rutile with little variability, (4) silicates versus heavy minerals is very strongly toward
silicates with some variability, (5) anorthite versus albite is strongly toward the Ca end
member with little variability, (6) plagioclase versus K feldspar is strongly toward plagio-
clase with considerable variability, (7) MnO versus MgO is very strongly toward MgO
with little variability, (8) ferric versus nonferric maﬁcs is toward MnO and MgO with little
variability, (9) maﬁc versus nonmaﬁc minerals is neutral with considerable variability, and
(10) structure-ﬁlling versus frame-ﬁlling oxides is toward structure-ﬁlling with little
variability.
419
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

The boxplot deﬁnes the marginal (i.e., within balance) behavior. Understanding the
relationships between them requires the conventional covariance matrix and the
corresponding correlation matrix. These are obtained by transforming the normalized
form of the total variation matrix (11.45) in the standard way. The following list
shows the covariance matrix in the upper triangle including diagonal, whereas the
correlation matrix is below the main diagonal. Both are obtained from the MATLAB
script.
Cov = -Psi*NVariation(mar)*Psi';
ds = sqrt(diag(diag(Cov)));
Corr = inv(ds)*Cov*inv(ds);
1
2
3
4
5
6
7
8
9
10
0.105
0.003
-0.015
-0.049
-0.016
-0.127
0.010
0.059
-0.055
-0.019
0.226
0.002
-0.006
-0.012
0.000
-0.022
0.001
0.003
-0.009
0.001
-0.229
-0.630
0.040
0.053
-0.007
0.125
-0.004
-0.012
0.065
-0.004
-0.439
-0.780
0.767
0.119
0.011
0.228
-0.012
-0.038
0.114
-0.011
-0.388
0.032
-0.285
0.249
0.016
0.007
-0.006
-0.009
0.006
-0.001
-0.525
-0.655
0.835
0.887
0.075
0.557
-0.022
-0.085
0.293
0.002
0.257
0.258
-0.162
-0.314
-0.425
-0.263
0.013
0.005
-0.011
-0.002
0.89
0.37
-0.28
-0.53
-0.351
-0.550
0.198
0.043
-0.039
-0.007
-0.407
-0.494
0.776
0.797
0.108
0.942
-0.239
-0.449
0.174
-0.007
-0.485
0.133
-0.182
-0.266
-0.040
0.019
-0.113
-0.269
-0.140
0.015
Some observations about the correlation matrix in rank order:
1. Balance 1 (Fe2O3 versus FeO) and balance 8 (ferric versus nonferric oxides) are
strongly correlated.
2. Balance 4 (silicate versus heavy minerals) and balance 6 (plagioclase versus K feldspar)
are strongly correlated.
3. Balance 3 (distribution within heavy minerals) and balance 6 (plagioclase versus
K feldspar) are strongly correlated.
1
10
Balance
–3
–2
–1
0
1
2
3
4
5
2
3
4
5
6
7
8
9
Figure 11.12
Boxplot of the ilr computed from the MAR data.
420
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

4. Balance 4 (silicate versus heavy minerals) and balance 9 (maﬁc versus felsic minerals)
are strongly correlated.
5. Balance 2 (silica saturation proxy) and balance 4 (silicates versus heavy minerals) are
strongly anticorrelated.
6. Balance 3 (distribution within heavy minerals) and balance 9 (maﬁc versus felsic
minerals) are strongly correlated.
7. Balance 3 (distribution within heavy minerals) and balance 4 (silicates versus heavy
minerals) are strongly correlated.
8. Balance 2 (silica saturation proxy) and balance 6 (plagioclase versus K feldspar) are
correlated.
9. Balance 2 (silica saturation proxy) and balance 3 (distribution within heavy minerals)
are correlated.
Overall, the correlation structure is quite complex.
As a ﬁnal step, a biplot is computed from the ilr.
g = geomean(mar);
xstar = Ilr(Perturbation(mar, 1./g), Psi);
[u s v ] = svd(xstar);
labels = {'1', '2', '3', '4', '5', '6', '7', '8', '9', '10'};
biplot([s(1, 1)*v(:, 1) s(2, 2)*v(:, 2)], 'VarLabels', labels)
axis square
The eigenvalues from the ilr data are identical to those for the clr svd, but the
eigenvectors are different due to the use of Ψ
$ to weight the clr. The covariance biplot is
shown in Figure 11.13. Figure 11.14 shows the form biplot.
The covariance biplot is dominated by balances 1 (Fe oxidation state), 6 (plagioclase
versus K feldspar), and 9 (maﬁc minerals versus feldspar). Conversely, balances 2 (silica
saturation proxy), 5 (anorthite versus albite), 7 (MnO versus MgO), and 10 (structural
–5
0
Component 1
–5
–4
–3
–2
–1
0
1
2
3
4
5
Component 2
1
2
3
5
6
7
8
9
10
5
4
Figure 11.13
Covariance biplot for the ilr computed from the centered MAR data.
421
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

versus frame oxides) are very small. A compositional line relation is present between
balances 3 (distribution within heavy minerals), 4 (heavy minerals versus silicates), 9
(maﬁc minerals versus feldspar), and 10 (structural versus frame oxides), on the one hand,
and balances 1 (Fe oxidation state) and 8 (ferric versus nonferric maﬁc minerals), on the
other. None of the large links are close to orthogonal; hence there is dependence among the
balances.
The form biplot in Figure 11.14 is far from evenly distributed among the balances,
suggesting that more than two eigenvalues are needed to explain the data. This can be
addressed by producing 3D biplots, as in Figures 11.7 and 11.8. Finally, the ilr of the data
can be transformed back to the simplex using ilr1 and then plotted on ternary diagrams.
Example 11.9 The ﬁle komatiite.dat contains the major oxides from komatiites taken from
the Petdb database that were selected for Archaean age, the presence of NiO, the presence
of separate measurements for ferric and ferrous iron, and the absence of zeros in the
13 major oxides SiO2, TiO2, Al2O3, Cr2O3, NiO, Fe2O3, FeO, CaO, MgO, MnO, K2O,
Na2O, and P2O5. Komatiites are ultramaﬁc mantle-derived rocks of predominantly
Archaean age that exhibit compositions that are consistent with melting at temperatures
of over 1600	C, or at least 200	C higher than anything known today, reﬂecting the much
higher internal heat production in Earth at the time. Komatiites are characterized by low Si,
K, and Al and high Mg compared with modern day ultramaﬁcs and were formed as a result
of extensive (~50%) partial melting. Exploratory compositional data analysis will be
carried out on the data.
x = importdata('komatiite.dat');
komatiite = [x(:, 1:4) x(:, 10) x(:, 5:6) x(:, 9) x(:, 8) x
(:, 7) x(:, 12) x(:, 11) x(:, 13)];
komatiite = Close(komatiite);
–0.5
0
0.5
Component 1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
Component 2
1
2
3
4
5
6
7
8
9
10
Figure 11.14
Form biplot for the ilr computed from the centered MAR data.
422
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

center = Close(geomean(komatiite), 100)
center =
Columns 1 through 9
47.4147
0.3235
3.9727
0.4038
0.2039
2.8546
8.6417
0.1984
30.7611
Columns 10 through 13
5.0783
0.0997
0.0192
0.0283
The center of these data is decidedly different from that of MORB in Example 11.2.
[T totvar] = NVariation(komatiite);
SiO2
TiO2
Al2O3
Cr2O3
NiO
Fe2O3
FeO
MnO
MgO
CaO
0
0.034
0.061
0.065
0.077
0.254
0.027
0.017
0.021
0.170
0
0.049
0.107
0.151
0.352
0.032
0.051
0.087
0.110
0
0.154
0.242
0.374
0.064
0.090
0.132
0.110
0
0.094
0.234
0.087
0.082
0.053
0.352
0
0.307
0.123
0.099
0.030
0.369
0
0.397
0.204
0.206
0.718
0
0.051
0.068
0.129
0
0.035
0.216
0
0.286
0
Na2O
K2O
P2O5
0.386
0.312
0.250
0.335
0.327
0.262
0.287
0.260
0.279
0.595
0.417
0.242
0.658
0.445
0.337
0.897
0.558
0.165
0.371
0.355
0.342
0.430
0.340
0.212
0.535
0.364
0.246
0.217
0.421
0.557
0
0.415
0.752
0
0.501
0
The color code is <0.2 (dark gray), 0.2 to 0.4 (light gray), and >0.4 (white). The
highest value occurs for Na2O and Fe2O3, followed by Na2O and P2O5, CaO and
Fe2O3, Na2O and NiO, Na2O and Cr2O3, CaO and P2O5, and Na2O and MgO. This is
heavily tilted toward the felsic elements (Na, Ca, K) paired with maﬁc ones (Ni, Cr,
ferric iron, M), although felsic concentration in komatiites is very low. Conversely,
SiO2, TiO2, and Al2O3 are not highly correlated with any other oxide. The total
variation is 3.08.
423
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

The clr of the centered data is computed and decomposed using the svd. The eigenvalues
are
0.4849
0.2104
0.1285
0.0692
0.0401
0.0338
0.0123
0.0107
0.0059
0.0026
0.0011
0.0004
0.0000
The ﬁrst two eigenvalues explain 69.5%, the ﬁrst three eigenvalues explain 82.3%, and the
ﬁrst four eigenvalues explain 89.3% of the variance. A scree plot is shown in Figure 11.15
and suggests that at least ﬁve eigenvalues are needed to explain the data. This is substan-
tially different from that for the MORB data in Example 11.6.
The ﬁrst nine columns of the variable eigenvectors are
SiO2
0.025
-0.143
0.041
-0.065
-0.097
0.062
0.095
-0.158
0.016
TiO2
-0.070
-0.144
-0.110
0.198
-0.010
-0.013
-0.429
0.374
0.708
Al2O3
-0.133
0.023
-0.124
0.290
-0.322
-0.215
0.550
-0.331
0.329
Cr2O3
0.189
-0.178
0.110
-0.008
0.117
-0.747
0.031
0.360
-0.296
NiO
0.190
-0.304
0.369
-0.175
0.448
0.340
0.201
0.087
0.139
Fe2O3
0.475
0.312
-0.179
-0.394
-0.493
0.103
0.044
0.268
-0.003
FeO
-0.069
-0.275
0.052
0.111
-0.073
-0.214
-0.304
-0.385
-0.143
MnO
0.080
-0.091
-0.046
-0.107
-0.179
0.176
-0.515
-0.372
-0.205
MgO
0.156
-0.174
0.174
-0.154
0.030
0.136
0.266
-0.164
0.035
CaO
-0.389
-0.233
-0.194
0.296
-0.188
0.383
0.138
0.431
-0.449
Na2O
-0.589
0.224
-0.264
-0.598
0.277
-0.126
0.024
-0.034
0.042
K2O
-0.194
0.625
0.649
0.230
-0.038
0.032
-0.108
0.047
-0.060
P2O5
0.328
0.358
-0.477
0.375
0.528
0.082
0.007
-0.123
-0.114
It is likely that most of the variability described by the ﬁrst few eigenvectors is due
to alteration or metamorphism over the two billion years since the komatiites erupted,
and in fact, the ﬁrst mantle signal is the signature of Cr2O3 and NiO that does not
appear until the ﬁfth and sixth eigenvectors, representing 4% and 3% of the variance,
respectively. The clr eigenvectors do not appear to be a source of great insight into
these data.
Rather than examining biplots of the clr eofs, it makes more sense to go straight to a
sequential binary partition. A candidate set is
424
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

1. SiO2 versus Al2O3 (silica saturation proxy);
2. TiO2 versus NiO;
3. TiO2 and NiO versus P2O5 (speciﬁc types of heavy mineral);
4. Silicate versus heavy minerals;
5. Albite versus anorthite;
6. Plagioclase versus potassium feldspar;
7. Fe2O3 versus FeO (oxidation state proxy);
8. MnO versus MgO (distribution within maﬁc nonferric minerals);
9. Ferric versus nonferric maﬁc minerals;
10. Cr2O3 versus maﬁc minerals;
11. Maﬁc versus felsic minerals; and
12. Structure-ﬁlling (cation) oxides versus frame-forming oxides.
sbp = [1 0 -1 0 0 0 0 0 0 0 0 0 0;
0 1 0 0 -1 0 0 0 0 0 0 0 0;
0 1 0 0 1 0 0 0 0 0 0 0 -1;
1 -1 1 0 -1 0 0 0 0 0 0 0 -1;
0 0 0 0 0 0 0 0 0 1 -1 0 0;
0 0 0 0 0 0 0 0 0 1 1 -1 0;
0 0 0 0 0 1 -1 0 0 0 0 0 0;
0 0 0 0 0 0 0 1 -1 0 0 0 0;
0 0 0 0 0 1 1 -1 -1 0 0 0 0;
0 0 0 1 0 -1 -1 -1 -1 0 0 0 0;
0 0 0 1 0 1 1 1 1 -1 -1 -1 0;
1 1 1 -1 1 -1 -1 -1 -1 -1 -1 -1 1];
Psi = Contrast(sbp);
xstar = Ilr(komatiite, Psi);
boxplot(xstar)
Figure 11.16 shows a boxplot of the ilr of the centered komatiite data. Moving through
the results, balance 1 (SiO2 versus Al2O3) is toward silica with limited variability, balance
2 (TiO2 versus NiO) is approximately neutral, balance 3 (TiO2 and NiO versus P2O5) is
toward the metals, balance 4 (silicate versus heavy minerals) is very strongly toward
10
5
0
15
Index
0
0.1
0.2
0.3
0.4
0.5
Normalized Eigenvalue
Figure 11.15
Scree plot for the svd of the clr of the centered komatiite data.
425
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

silicates, balance 5 (albite versus anorthite) is strongly toward albite, balance 6 (plagioclase
versus K feldspar) is strongly toward plagioclase, balance 7 (iron oxidation state) is weakly
ferrous, balance 8 (MnO versus MgO) is very strongly toward MgO with little variability,
balance 9 (ferric versus nonferric maﬁc minerals) is weakly toward the ferric side, balance
10 (Cr2O3 versus maﬁc minerals) is strongly toward chromium, balance 11 (maﬁc versus
felsic minerals) is strongly toward maﬁc minerals with a large amount of variability, and
balance 12 (structure-ﬁlling versus frame-forming oxides) is neutral.
Cov = -Psi*NVariation(komatiite)*Psi';
ds = sqrt(diag(diag(Cov)));
Corr = inv(ds)*Cov*inv(ds);
The correlation matrix is
1
2
3
4
5
6
7
8
9
10
11
12
1.000
-0.783
0.241
-0.593
0.168
-0.092
0.263
-0.418
-0.179
0.029
0.618
0.203
1.000
-0.187
0.503
-0.174
0.369
-0.277
0.731
0.227
-0.126
-0.573
-0.256
1.000
0.467
0.130
0.279
-0.666
-0.260
-0.709
0.152
-0.221
-0.417
1.000
-0.176
0.302
-0.532
0.253
-0.207
-0.093
-0.613
-0.663
1.000
0.010
-0.106
-0.199
-0.147
0.106
0.475
0.597
1.000
-0.465
0.287
-0.308
-0.045
-0.182
-0.006
1.000
-0.065
0.745
-0.255
0.543
0.257
1.000
0.169
-0.395
-0.356
-0.267
1.000
-0.032
0.325
0.110
1.000
0.206
0.144
1.000
0.660
1.000
Some observations about the result in rank order are
1. Balance 1 (silica versus alumina) and balance 2 (TiO2 versus NiO) are strongly
inversely correlated.
2. Balance 7 (iron oxidation state) and balance 9 (ferric versus nonferrous maﬁc minerals)
are strongly correlated.
10
9
8
7
6
5
4
3
2
1
11 12
Balance
–4
–2
0
2
4
6
Figure 11.16
Boxplot of the ilr of the komatiite data.
426
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

3. Balance 2 (TiO2 versus NiO) and balance 8 (MnO versus MgO) are strongly correlated.
Because MgO is relatively constant, this suggest a tradeoff between TiO2 and MnO.
4. Balance 3 (TiO2 and NiO versus P2O5) and balance 9 (ferric versus nonferric maﬁc
minerals) are strongly inversely correlated.
5. Balance 3 (TiO2 and NiO versus P2O5) and balance 7 (oxidation state proxy) are
negatively correlated.
6. Balance 4 (silicate versus heavy minerals) and balance 12 (structure- versus frame-
ﬁlling oxides) are negatively correlated.
7. Balance 11 (maﬁc versus felsic minerals) and balance 12 (structure- versus frame-ﬁlling
oxides) are correlated.
8. Balance 1 (silica saturation proxy) and balance 11 (maﬁc versus felsic minerals) are
correlated.
9. Balance 4 (silicate versus heavy minerals) and balance 11 (maﬁc versus felsic minerals)
are negatively correlated.
g = geomean(komatiite);
xstar = Ilr(Perturbation(komatiite, 1./g), Psi);
[u s v] = svd(xstar);
labels = {'1','2','3','4','5','6','7','8','9','10','11','12'};
biplot([s(1,
1)*v(:,
1)
s(2,
2)*v(:,
2)
s(3,3)*v(:,3)],
'VarLabels', labels)
axis square
Figures 11.17 and 11.18 show covariance biplots corresponding to the ﬁrst and second
and second and third eigenvectors of the ilr of the centered komatiite data, respectively.
Given the scree plot in Figure 11.15, this is not an adequate description of this data set.
Figure 11.17 shows that the longest ray is for the eleventh balance (maﬁc versus felsic
minerals), followed by the sixth (plagioclase versus K feldspar) and seventh (iron oxidation
state). The rays for balances 3 (TiO2 and NiO versus P2O5) and 6 (plagioclase versus
–6
–4
–2
0
2
4
6
Component 1
–6
–4
–2
0
2
4
6
Component 2
1
2
3
4
5
6
7
8
9
10
11
12
Figure 11.17
Covariance biplot corresponding to the ﬁrst two eigenvectors of the ilr of the centered komatiite data.
427
11.3 Exploratory Compositional Data Analysis
.012
13:42:12, subject to the Cambridge Core terms of use,

K feldspar) are nearly coincident and hence redundant. There are compositional lines
between balance 2 (TiO2 versus NiO) and balance 11 (structure versus frame ﬁlling oxides)
and balances 3/6 and 7 (oxidation state proxy). It is likely that most of what is observed is
due to weathering.
Figure 11.18 shows the covariance biplot corresponding to the second and third eigen-
vectors of the ilr of the centered komatiite data. The longest rays occur for balance 6
(plagioclase versus K feldspar), followed by balances 3 (TiO2 and NiO versus P2O5) and 7
(oxidation state proxy). Balances 1 (silica saturation proxy) and 10 (Cr2O3 versus maﬁc
minerals) are nearly coincident, suggesting redundancy. Compositional lines exist between
balances 3 (TiO2 and NiO versus P2O5) and 9 (ferric versus nonferric maﬁc minerals) and
balances 5 (albite versus anorthite) and 7 (oxidation state proxy).
–3
–2
–1
0
1
2
3
Component 2
–3
–2
–1
0
1
2
3
Component 3
1
2
3
4
5
6
7
8
9
10
11
12
Figure 11.18
Covariance biplot corresponding to the second and third eigenvectors of the ilr of the centered komatiite data.
428
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

11A
Appendix 11A MATLAB Functions to Produce
Ternary Diagrams
The function Ternary is called initially to set up a ternary diagram, plot its frame, and place
gridlines through it. It optionally allows perturbation to facilitate centering. Section 11.2.2
and Example 7.4 illustrate its use.
function [h] = Ternary(varargin)
%sets up axes for ternary diagram
%arguments
%varargin{1}
=
ntick
-
number
of
tick
marks
on
the
axes
(default is 11); if set to zero,
%then tick values are contained in varargin{3}
%varargin{2} = composition - composition by which the grid-
lines are perturbed
if nargin == 0
ntick = 11;
else
ntick = varargin{1};
end
hold off
h = ﬁll([0 1 0.5 0], [0 0 sqrt(3)/2 0], 'w', 'linewidth', 2);
axis image
axis off
hold on
label = linspace(0, 1, ntick);
if nargin < 2
label = linspace(0, 1, ntick);
for i = 1:ntick
plot([label(i) 0.5*(1 + label(i))], [0 0.866*(1 - label
(i))], ':k', 'linewidth', 0.75)
plot([0.5*(1 - label(i)) 1 - label(i)], [0.866*(1 - label
(i)) 0], ':k', 'linewidth', 0.75)
plot([0.5*label(i) 1 - 0.5*label(i)], [0.866*label(i)
0.866*label(i)], . . .
':k', 'linewidth', 0.75)
text(1 - 0.5*label(i) + .025, sqrt(3)*label(i)/2 +
0.01, . . .
num2str(label(i), 3), 'FontSize', 14, 'FontWeight',
'bold');
429
.012
13:42:12, subject to the Cambridge Core terms of use,

str = num2str(label(i), 3);
text(label(i) - 0.02*length(str)/2, -.025, str, 'Font-
Size', 14, 'FontWeight', 'bold');
str = num2str(label(i), 3);
text(0.5*(1 - label(i)) - 0.015*(length(str) - 1) -
.035, . . .
sqrt(3)/2*(1 - label(i)) + .01, str, 'FontSize',
14, 'FontWeight', 'bold');
end
elseif nargin == 2
label = linspace(0, 1, ntick);
y = varargin{2};
for i = 1:ntick
x1 = Comp_to_Cart(Perturbation(Cart_to_Comp([label(i)
0]), y));
x2 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([0.5*(1 + label(i)) 0.866*(1 - label
(i))]), y));
plot([x1(1) x2(1)], [x1(2) x2(2)], ':k', 'linewidth',
0.75)
x1 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([0.5*(1 - label(i)) 0.866*(1 - label
(i))]), y));
x2 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([1 - label(i) 0]), y));
plot([x1(1) x2(1)], [x1(2) x2(2)], ':k', 'linewidth',
0.75)
x1 = Comp_to_Cart(Perturbation(Cart_to_Comp([0.5*label
(i) ...
0.866*label(i)]), y));
x2 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([1-0.5*label(i)0.866*label(i)]),y));
plot([x1(1) x2(1)], [x1(2) x2(2)], ':k', 'linewidth',
0.75)
x1 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([1 - 0.5*label(i) 0.866*label(i)]), y));
text(x1(1) + .025, x1(2) + 0.01, num2str(label(i),
3), . . .
'FontSize', 14, 'FontWeight', 'bold');
x1 = Comp_to_Cart(Perturbation(Cart_to_Comp([label(i)
0]), y));
str = num2str(label(i), 3);
text(x1(1) - 0.02*length(str)/2, x1(2) - .025, str,
'FontSize', . . .
430
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

14, 'FontWeight', 'bold');
x1 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([0.5*(1 - label(i)) 0.866*(1 - label
(i))]), y));
str = num2str(label(i), 3);
text(x1(1) - .015*(length(str) - 1) - .035, x1(2) +
.01, str, . . .
'FontSize', 14, 'FontWeight', 'bold');
end
else
label = varargin{3};
ntick = length(label);
for i = 1:ntick
y = varargin{2};
x1 = Comp_to_Cart(Perturbation(Cart_to_Comp([label(i)
0]), y));
x2 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([0.5*(1 + label(i)) 0.866*(1 - label
(i))]), y));
plot([x1(1) x2(1)], [x1(2) x2(2)], ':k', 'linewidth', 0.75)
x1 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([0.5*(1 - label(i)) 0.866*(1 - label
(i))]), y));
x2 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([1 - label(i) 0]), y));
plot([x1(1) x2(1)], [x1(2) x2(2)], ':k', 'linewidth',
0.75)
x1 = Comp_to_Cart(Perturbation(Cart_to_Comp([0.5*label
(i) ...
0.866*label(i)]), y));
x2 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([1 - 0.5*label(i) 0.866*label(i)]), y));
plot([x1(1) x2(1)], [x1(2) x2(2)], ':k', 'linewidth', 0.75)
x1 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([1 - 0.5*label(i) 0.866*label(i)]), y));
text(x1(1) + .025, x1(2) + 0.01, num2str(label(i),
3), . . .
'FontSize', 14, 'FontWeight', 'bold');
x1 = Comp_to_Cart(Perturbation(Cart_to_Comp([label(i)
0]), y));
str = num2str(label(i), 3);
text(x1(1) - 0.02*length(str)/2, x1(2) - .025, str,
'FontSize', . . .
14, 'FontWeight', 'bold');
431
Appendix 11A: MATLAB Functions to Produce Ternary Diagrams
.012
13:42:12, subject to the Cambridge Core terms of use,

x1 = Comp_to_Cart(Perturbation( . . .
Cart_to_Comp([0.5*(1 - label(i)) 0.866*(1 - label
(i))]), y));
str = num2str(label(i), 3);
text(x1(1) - .015*(length(str) - 1) - .035, x1(2) +
.01, str, . . .
'FontSize', 14, 'FontWeight', 'bold');
end
end
end
function [Result] = Comp_to_Cart(comp)
%Converts a closed composition comp to Cartesian components
Result on a
%ternary with unit edges
[n D] = size(comp);
if D ~= 3
warning('Comp_to_Cart: composition must have 3 parts')
return
end
Result = zeros(n, 2);
for i = 1:n
x = (comp(i, 1) + 2*comp(i, 2))/(2*(comp(i, 1) + comp(i,
2) + . . .
comp(i, 3)));
y = sqrt(3)*comp(i, 1)/(2*(comp(i, 1) + comp(i, 2) + comp
(i, 3)));
Result(i, :) = [x y];
end
end
function [Result] = Cart_to_Comp(xx)
%Converts Cartesian components xx on the ternary with unit
edges to a
%composition Result
n = size(xx, 1);
Result = zeros(n, 3);
for i = 1:n
Result(i, 1:3) = [2*xx(i, 2)/sqrt(3) xx(i, 1) - xx(i, 2)/sqrt
(3) ...
1 - xx(i, 1) - xx(i, 2)/sqrt(3)];
end
end
The function TernaryLabel places labels on the x, y, and z axes of a ternary diagram
created using Ternary.
432
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

function [h] = TernaryLabel(x, y, z)
%Places labels on the x, y and z axes of a ternary diagram
if nargout == 0
text(0.85, sqrt(3)/4 + 0.05, x, 'horizontalalignment', . . .
'center', 'rotation', -60, 'FontSize', 16, 'FontWeight',
'bold')
text(0.5,
-0.075,
y,
'horizontalalignment',
'center',
'FontSize', 16, . . .
'FontWeight', 'bold')
text(0.15, sqrt(3)/4 + 0.05, z, 'horizontalalignment', . . .
'center', 'rotation', 60, 'FontSize', 16, 'FontWeight',
'bold')
else
h = zeros(1, 3);
h(1) = text(0.85, sqrt(3)/4 + 0.05, x, 'horizontalalignment', ...
'center',
'rotation',
-60,
'FontSize',
16,
'Font-
Weight', 'bold');
h(2)
=
text(0.5,
-0.075,
y,
'horizontalalignment',
'center', . . .
'FontSize', 16, 'FontWeight', 'bold');
h(3) = text(0.15, sqrt(3)/4 + 0.05, z, 'horizontalalignment', ...
'center', 'rotation', 60, 'FontSize', 16, 'FontWeight',
'bold');
end
end
The function TernaryLine plots a compositional line given the three-part composition in
the argument comp.
function TernaryLine(comp)
%Plots a compositional line given the 3 part composition in
comp
xo = (comp(1, 1) + 2*comp(1, 2))/(2*sum(comp(1, :)));
yo = sqrt(3)*comp(1, 1)/(2*sum(comp(1, :)));
for i = 2:length(comp)
x = (comp(i, 1) + 2*comp(i, 2))/(2*sum(comp(i, :)));
y = sqrt(3)*comp(i, 1)/(2*sum(comp(i, :)));
plot([xo x], [yo y], 'k', 'linewidth', 1.0)
xo = x;
yo = y;
end
end
The function TernaryPlot plots compositional data in the three-vector comp using blue
plusses by default. Both the symbol type and color can be changed through optional input
arguments.
433
Appendix 11A: MATLAB Functions to Produce Ternary Diagrams
.012
13:42:12, subject to the Cambridge Core terms of use,

function TernaryPlot(comp, varargin)
%Plots
the
compositional
data
in
comp
using
blue
+
by
default
%varargin{1} speciﬁes alternate symbols (e.g.,'d') and var-
argin{2}
%speciﬁes alternate colors (e.g.,'k')
xx = Comp_to_Cart(comp);
if nargin == 1
mrk = '+';
mcol = 'b';
elseif nargin == 2
mrk = varargin{1};
mcol = 'b';
else
mrk = varargin{1};
mcol = varargin{2};
end
for i = 1:length(xx)
plot(xx(i, 1), xx(i, 2), 'Marker', mrk, 'MarkerSize', 8, . . .
'MarkerEdgeColor', mcol, 'MarkerFaceColor', mcol)
end
end
434
Compositional Data
.012
13:42:12, subject to the Cambridge Core terms of use,

References
Aitchison, J. (1986). The Statistical Analysis of Compositional Data. London: Chapman &
Hall.
Aitchison, J., & M. Greenacre (2002). Biplots of compositional data. Appl. Stat., 51,
375–92.
Anderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis, 3rd edn. New
York: Wiley.
Anderson, T. W., & D. Darling (1952). Asymptotic theory of certain goodness of ﬁt criteria
based on stochastic processes. Ann. Math. Stat., 23, 193–212.
Anderson, T. W., & D. Darling (1954). A test of goodness of ﬁt. J. Am. Stat. Assoc., 49,
765–9.
Andrews, D. F., & A. M. Herzberg (1985). Data: A Collection of Problems from Many
Fields for the Student and Research Worker. New York: Springer.
Ansari, A. R., & R. A. Bradley (1960). Rank-sum tests for dispersions. Ann. Math. Stat.,
31, 1174–89.
Arbuthnott, J. (1710). An argument for divine providence, taken from the constant
regularity in the births of both sexes. Philos. Trans. R. Soc. Lond., 27, 186–90.
Azzalini, A., & A. W. Bowman (1990). A look at some data on the Old Faithful geyser. J.
R. Stat. Soc., C39, 357–65.
Bartlett, M. S. (1937). Properties of sufﬁciency and statistical tests. Proc. R. Soc. Lond.,
A160, 268–82.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Philos.
Trans. R. Soc. Lond., 53, 370–418.
Benjamini, Y., & Y. Hochberg (1995). Controlling the false discovery rate: a practical and
powerful approach to multiple testing. J. R. Stat. Soc., B57, 289–300.
Benjamini, Y., A. M. Krieger, & D. Yekutieli (2006). Adaptive linear step-up procedures
that control the false discovery rate. Biometrika, 93, 491–507.
Berry, K. J., P. W. Mielke, Jr., & J. E. Johnston (2016). Permutation Statistical Methods:
An Integrated Approach. New York: Springer.
Birnbaum, A. (1954). Combining independent tests of signiﬁcance. J. Am. Stat. Assoc., 49,
559–74.
Blackwell, D. (1947). Conditional expectation and unbiased sequential estimation. Ann.
Math. Stat., 18, 105–10.
Bound, J., D. A. Jaeger, & R. M. Baker (1995). Problems with instrumental variables
estimation when the correlation between the instruments and the endogenous explana-
tory variable is weak. J. Am. Stat. Assoc., 90, 443–50.
435
.013
13:43:11, subject to the Cambridge Core terms of use,

Box, G. E. P. (1949). A general distribution theory for a class of likelihood criteria.
Biometrika, 36, 317–46.
Broﬁtt, J. D., & R. H. Randles (1977). A power approximation for the chi-square
goodness-of-ﬁt test: simple hypothesis case. J. Am. Stat. Assoc., 72, 604–7.
Carroll, R. J., & A. H. Welsh (1988). A note on asymmetry and robustness in linear
regression. Am. Stat., 42, 285–7.
Carter, M., & B. van Brunt (2000). The Lebesgue-Stieltjes Integral: A Practical Introduc-
tion. New York: Springer.
Chave, A. D. (2014). Magnetotelluric data, stable distributions and impropriety: an
existential combination. Geophys. J. Int., 198, 622–36.
Chave, A. D. (2015). A note about Gaussian statistics on a sphere. Geophys. J. Int., 203, 893–5.
Chave, A. D., & D. J. Thomson (2003). A bounded inﬂuence regression estimator based on
the statistics of the hat matrix. J. R. Stat. Soc., C52, 307–22.
Chave, A. D., & D. J. Thomson (2004). Bounded inﬂuence estimation of magnetotelluric
response functions. Geophys. J. Int., 157, 988–1006.
Chave, A. D., D. J. Thomson, & M. E. Ander (1987). On the robust estimation of power
spectra, coherences and transfer functions. J. Geophys. Res., 92, 633–48.
Clarke, R. D. (1946). An application of the Poisson distribution. J. Inst. Actuaries, 22, 32.
Cochran, W. G. (1934). The distribution of quadratic forms in a normal system, with
applications to the analysis of covariance. Math. Proc. Camb. Philos. Soc., 30, 178–91.
Cochran, W. G. (1952). The χ2 test of goodness of ﬁt. Ann. Math. Stat., 23, 35–45.
Cramér, H. (1945). Mathematical Methods of Statistics. Uppsala: Almqvist & Wiksell.
Csörgő, S., & J. J. Faraway (1996). The exact and asymptotic distributions of Cramér–von
Mises statistics. J. R. Stat. Soc., B58, 221–34.
David, H. A. (1981). Order Statistics, 2nd edn. New York: Wiley.
David, H. A. (2009). A historical note on zero correlation and independence. Am. Stat., 63,
185–6.
David, H. A., & H. N. Nagaraja (2003). Order Statistics, 3rd edn. New York: Wiley.
Davison, A. C., & D. V. Hinkley (1997). Bootstrap Methods and Their Application.
Cambridge University Press.
De Groot, M. H., & M. J. Schervish (2011). Probability and Statistics, 4th edn. London:
Pearson.
De Moivre, A. (1711). De mensura sortis. Philos. Trans. R. Soc. Lond., 27, 213–64.
Doob, J. L. (1993). Measure Theory. New York: Springer.
DuMouchel, W. H. (1975). Stable distributions in statistical inference. 2. Information from
stably distributed samples. J. Am. Stat. Assoc., 70, 386–93.
Durbin, J., & G. S. Watson (1950). Testing for serial correlation in least squares regression,
part I. Biometrika, 37, 409–28.
Durbin, J. & G. S. Watson (1951). Testing for serial correlation in least squares regression,
part II. Biometrika, 38, 159–77.
Durbin, J., & G. S. Watson (1971). Testing for serial correlation in least squares regression,
part III. Biometrika, 58, 1–19.
Efron, B. (1979). Bootstrap methods: another look at the jackknife. Ann. Stat., 7, 1–26.
Efron, B., & C. Stein (1981). The jackknife estimate of variance. Ann. Stat., 9, 586–96.
436
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Efron, B., & R. Tibshirani (1998). An Introduction to the Bootstrap. London: Chapman &
Hall.
Ernst, M. D. (2004). Permutation methods: a basis for exact inference. Stat. Sci., 19,
676–85.
Feller, W. (1948). On the Kolmogorov-Smirnov limit theorems for empirical distributions.
Ann. Math. Stat., 19, 177–89.
Feller, W. (1950). Errata: On the Kolmogorov-Smirnov limit theorems for empirical
distributions. Ann. Math. Stat., 21, 301–2.
Feller, W. (1971). An Introduction to Probability Theory and Its Applications, vol. 2. New
York: Wiley.
Fieller, E. C. (1940). The biological standardization of insulin. J. R. Stat. Soc., 7(Suppl.),
1–64.
Fieller, E. C. (1944). A fundamental formula in the statistics of biological assays and some
applications. Q. J. Pharm. Pharmacol., 17, 117–23.
Fieller, E. C. (1954). Some problems in interval estimation. J. R. Stat. Soc., 16, 175–85.
Fisher, N. I. (1995). Statistical Analysis of Circular Data. Cambridge University Press.
Fisher, R. A. (1915). Frequency distribution of the values of the correlation coefﬁcient in
samples from an indeﬁnitely large population. Biometrika, 10, 507–21.
Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philos.
Trans. R. Soc. Lond., A222, 309–68.
Fisher, R. A. (1928). The general sampling distribution of the multiple correlation coefﬁ-
cient. Proc. R. Stat. Soc., A121, 654–73.
Fisher, R. A. (1932). Statistical Methods for Research Workers, 4th edn. Edinburgh: Oliver
& Boyd.
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Ann.
Eugenics, 7, 179–88.
Fisher, R. A. (1953). Dispersion on a sphere. Proc. R. Soc. Lond., A217, 295–305.
Gamble, T. D., W. M. Goubau, & J. Clarke (1979). Magnetotellurics with a remote
reference. Geophysics, 44, 53–68.
Gauss, K. F. (1823). Theoria Combinationis Observationum Erroribus Minimis Oboxiae.
Göttingen: Dieterich.
Geary, R. C. (1949). Determination of linear relationships between systematic parts of
variables with errors of observation the variances of which are unknown. Econometrica,
17, 30–58.
Gelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, et al. (2013). Bayesian
Data Analysis, 3rd edn. Boca Raton, FL: Taylor & Francis.
Genovese, C., & L. Wasserman (2002). Operating characteristics and extension of the false
discovery rate procedure. J. R. Stat. Soc., B64, 499–517.
Gleser, L. J., & J. T. Hwang (1987). The nonexistence of 100(1α)% conﬁdence sets of
ﬁnite expected diameter in errors-in-variables and related models. Ann. Stat., 15,
1351–62.
Good, P. I. (2000). Permutation Methods, 2nd edn. New York: Springer.
Good, P. I. (2005). Permutation, Parametric, and Bootstrap Tests of Hypotheses, 3rd edn.
New York: Springer.
437
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Gossett, W. S. (1908). The probable error of a mean. Biometrika, 6, 1–25.
Gradshteyn, I. S., & I. M. Ryzhik (1980). Table of Integrals, Series and Products. San
Diego: Academic Press.
Guenther, W. C. (1977). Power and sample size for approximate chi-square tests. Am. Stat.,
31, 83–5.
Hampel, F. R., E. M. Ronchetti, P. J. Rousseeuw, & W. A. Stahel (1986). Robust Statistics.
New York: Wiley.
Hänggi, P., F. Roesel, & P. Trautmann (1978). Continued fraction expansion in scattering
theory and statistical non-equilibrium mechanics. Z. Naturforsch., 33a, 402–17.
Hand, D. J., F. Daly, K. McConway, D. Lunn, & E. Ostrowski (1994). A Handbook of
Small Data Sets. London: Chapman & Hall.
Handschin, E., F. C. Schweppe, J. Kohlas, & A. Fiechter (1975). Bad data analysis of
power system state analysis. IEEE Trans. Power Appar. Syst., PAS-94, 329–37.
Hanley, J. A., M. Julien, & E. E. M. Moodie (2008). Student’s z, t, and s: what if Gossett
had R?. Am. Stat., 62, 64–9.
Hastie, T., R. Tibshirani, & J. Friedman (2008). The Elements of Statistical Learning, 2nd
edn. New York: Springer.
Herschel, J. F. W. (1850). Quetelet on probabilities. Edinb. Rev., 92, 1–57.
Hettmansperger, T. P., & J. W. McKean (1998). Robust Nonparametric Statistical
Methods. London: Edward Arnold.
Hirschberg, J., & J. Lye (2010). A geometric comparison of the delta and Fieller conﬁdence
intervals. Am. Stat., 64, 234–41.
Hoaglin, D. C., F. Mosteller, & J. W. Tukey (1991). Fundamentals of Exploratory Analysis
of Variance. New York: Wiley.
Hodges, J. L., & E. L. Lehmann (1956). The efﬁciency of some nonparametric competitors
of the t-test. Ann. Math. Stat., 27, 324–35.
Hoeffding, W. (1952). The large-sample power of tests based on permutations of observa-
tions. Ann. Math. Stat., 23, 169–92.
Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. J.
Am. Stat. Assoc., 58, 13–30.
Hogg, R. V., & A. T. Craig (1995). Introduction to Mathematical Statistics, 5th edn. Saddle
River, NJ: Prentice-Hall.
Hotelling, H. (1931). The generalization of Student’s ratio. Ann. Math. Stat., 2, 360–78.
Hotelling, H. (1933). Analysis of a complex of statistical variables into principal compon-
ents. J. Educ. Psychol., 24, 417–41.
Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 28, 321–77.
Hotelling, H. (1953). New light on the correlation coefﬁcient and its transforms. Proc.
R. Stat. Soc., B15, 193–232.
Hubble, E. (1929). A relation among distance and radial velocity among extra-galactic
nebulae. Proc. Nat. Acad. Sci. USA, 15, 168–73.
Huber, P. (1964). Robust estimation of a location parameter. Ann. Math. Stat., 35,
73–101.
Huber, P. (2011). Data Analysis: What Can Be Learned from the Past 50 Years. New York:
Wiley.
438
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Jarque, C. M., & A. K. Bera (1987). A test for normality of observations and regression
residuals. Int. Stat. Rev., 55, 163–72.
Johnson, N. L., S. Kotz, & N. Balikrishnan (1994). Continuous Univariate Distributions,
vol. 1, 2nd edn. New York: Wiley.
Johnson, N. L., S. Kotz, & N. Balikrishnan (1995). Continuous Univariate Distributions,
vol. 2, 2nd edn. New York: Wiley.
Johnson, N. L., S. Kotz, & N. Balakrishnan (1997). Discrete Multivariate Distributions.
New York: Wiley.
Johnson, N. L., S. Kotz, & A. W. Kemp (1993). Univariate Discrete Distributions, 2nd
edn. New York: Wiley.
Jolliffe, I. T. (2002). Principal Component Analysis, 2nd edn. New York: Springer.
Kolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. Inst.
Ital. Attuarai, Giorn., 4, 1–11.
Kotz, S., & N. L. Johnson (eds.) (1992). Breakthroughs in Statistics, vol. II: Methodology
and Distributions. New York: Springer.
Kotz, S., N. Balakrishnan, & N. L. Johnson (2000). Continuous Multivariate Distributions,
vol 1: Models and Applications, 2nd edn. New York: Wiley.
Kvam, P. H., & B. Vidakovic (2007). Nonparametric Statistics with Applications to
Science and Engineering. Hoboken, NJ: Wiley.
Lagarias, J. C., J. A. Reeds, M. H. Wright, & P. E. Wright (1998). Convergence properties
of the Neider-Mead simplex method in low dimensions. SIAM J. Optim., 9, 112–47.
Langevin, P. (1905). Magnétisme et théorie des électrons. Ann. Chim. Phys., 5, 71–127.
Lanzante, J. R. (2005). A cautionary note on the use of error bars. J. Climate, 18,
3699–703.
Lehmann, E. L. (1953). The power of rank tests. Ann. Math. Stat., 24, 23–43.
Lehmann, E. L., & J. P. Schaffer (1988). Inverted distributions. Am. Stat., 42, 191–4.
Lévy, P. P. (1925). Calcul des Probabilités. Paris: Gauthier-Villars.
Lilliefors, H. W. (1967). On the Kolmogorov-Smirnov test for normality with mean and
variance unknown. J. Am. Stat. Assoc., 62, 399–402.
Longley, J. W. (1967). An appraisal of least squares programs for electronic computers
from the viewpoint of the user. J. Am. Stat. Assoc., 62, 819–41.
Love, J. J., & C. G. Constable (2003). Gaussian statistics for paleomagnetic vectors.
Geophys. J. Int., 152, 515–65.
Mallows, C. L. (1975). On some topics in robustness. Tech. Mem., Bell Telephone
Laboratories.
Mann, H. B., & D. R. Whitney (1947). On a test of whether one of two random variables is
stochastically larger than the other. Ann. Math. Stat., 18, 50–60.
Mardia, K. V., & P. E. Jupp (2000). Directional Statistics. New York: Wiley.
Mardia, K. V., J. T. Kent, & J. Bibby (1979). Multivariate Analysis. London: Academic
Press.
Mauchly, J. W. (1940). Signiﬁcance test for sphericity of a normal n-variate distribution.
Ann. Math. Stat., 11, 204–9.
Meerschaert, M. M. (2012). Fractional calculus, anomalous diffusion and probability. In
Fractional Dynamics, ed. S. C. Lim, J. Klafter, & R. Metler. Singapore: World Science Press.
439
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Michael, J. R. (1983). The stabilized probability plot. Biometrika, 70, 11–17.
Mitra, S. K. (1958). On the limiting power function of the frequency chi-square test. Ann.
Math. Stat., 29, 1221–33.
Moore, D. S., & G. P. McCabe (1989). Introduction to the Practice of Statistics. New York:
W.H. Freeman.
Mosteller, F. (1946). On some useful “inefﬁcient” statistics. Ann. Math. Stat., 17,
377–408.
Murphy, K. R., B. Myors, & A. Wolach (2014). Statistical Power Analysis. New York:
Routledge.
Nelder, J. A., & R. W. M. Wedderburn (1972). Generalized linear models. J. R. Stat. Soc.,
A135, 370–84.
Neyman, J., & E. S. Pearson (1933). On the problem of the most efﬁcient tests of statistical
hypotheses. Philos. Trans. R. Soc. Lond., A231, 289–337.
Nolan, J. P. (1997). Numerical calculation of stable densities and distribution functions.
Comm. Stat. Stoch. Mod., 13, 759–74.
Nolan, J. P. (1998). Parameterizations and modes of stable distributions. Stat. Prob. Lett.,
38, 187–95.
Nolan, J. P. (2001). Maximum likelihood estimation and diagnostics for stable distribu-
tions. In Lévy Processes: Theory and Applications, ed. O. E. Barnsdorff-Nielsen, T.
Mikosch, & S. I. Resnick. Basel, Birkhäuser.
Oldham, K. B., & J. Spanier (1974). The Fractional Calculus. San Diego: Academic
Press.
Patnaik, P. B. (1949). The non-central χ2- and F-distributions and their applications.
Biometrika, 36, 202–32.
Pawlowsky-Glahn, V., & A. Buccianti (eds.) (2011). Compositional Data Analysis: Theory
and Applications. New York: Wiley.
Pawlowsky-Glahn, V., J. J. Egozcue, & R. Tolosana-Delgado (2015). Modeling and
Analysis of Compositional Data. New York: Wiley.
Pearson, K. (1896). On a form of spurious correlation which may arise when indices are
used in the measurement of organs. Philos. Trans. R. Soc. Lond., 60, 489–98.
Pearson, K. (1900). On the criterion that a given system of deviations from the probable in
the case of a correlated system of variables is such that it can be reasonably supposed to
have arisen from random sampling. Philos. Mag., 50, 339–57.
Pesarin, F., & L. Salmaso (2010). Permutation Tests for Complex Data: Theory, Applica-
tions and Software. New York: Wiley.
Phipson, B., & G. K. Smyth (2010). Permutation p-values should never be zero: calculat-
ing exact p-values when permutations are randomly drawn. Stat. Appl. Genet. Mol. Biol.,
9(1), art. 39.
Picinbono, B. (1996). Second order complex random vectors and normal distributions.
IEEE Trans. Sig. Proc., 44, 2637–40.
Pitman, E. J. G. (1937a). Signiﬁcance tests which may be applied to samples from any
population. J. R. Stat. Soc. Suppl., 4, 119–30.
Pitman, E. J. G. (1937b). Signiﬁcance tests which may be applied to samples from any
population. II. The correlation coefﬁcient test. J. R. Stat. Soc. Suppl., 4, 225–32.
440
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Pitman, E. J. G. (1938). Signiﬁcance tests which may be applied to samples from any
population. III. The analysis of variance test. Biometrika, 29, 322–35.
Poisson, S. D. (1837). Recherches sur la Probabilité des Jugements en Matiére Criminelle
et en Matiére Civile, Précédées des Regles Générales du Calcul des Probabilités. Paris:
Bachelier.
Preisendorfer, R. W. (1988). Principal Component Analysis in Meteorology and Oceanog-
raphy. Amsterdam: Elsevier.
Rao, C. R. (1945). Information and the accuracy attainable in the estimation of statistical
parameters. Bull. Calcutta Math. Soc., 37, 81–91.
Rao, C. R. (1947). Large sample tests of statistical hypotheses concerning several param-
eters with applications to problems of estimation. Proc. Camb. Philos. Soc., 44, 50–7.
Rao, C. R. (1951). An asymptotic expansion of the distribution of Wilks’ criterion, Bull.
Inter. Stat. Inst., 33, 177–80.
Reiersøl, O. (1941). Conﬂuence analysis by means of lag moments and other methods of
conﬂuence analysis. Econometrica, 9, 1–24.
Reiersøl, O. (1945). Conﬂuence analysis by means of instrumental sets of variables. Ark.
Mat. Astron. Fys., 32, 1–119.
Rencher, A. C. (1995). Methods of Multivariate Analysis. New York: Wiley.
Rencher, A. C. (1998). Multivariate Statistical Inference and Applications. New York:
Wiley.
Rice, J. A. (2006). Mathematical Statistics and Data Analysis, 3rd edn. Independence, KY:
Cengage Learning.
Romano, J. P. (1990). On the behavior of randomization tests without a group invariance
assumption. J. Am. Stat. Assoc., 85, 686–92.
Rousseeuw, P. J. W. (1984). Least median of squares regression. J. Am. Stat. Assoc., 79,
871–80.
Rousseeuw, P. J. W., & A. M. Leroy (1987). Robust Regression and Outlier Detection.
New York: Wiley.
Rousseeuw, P. J. W., & A. M. Leroy (2005). Robust Regression and Outlier Detection, 2nd
edn. New York: Wiley.
Rutherford, E., H. Geiger, & H. Bateman (1910). The probability variations in the
distribution of α particles. Philos. Mag., 20, 698–707.
Ryan, T. P. (1997). Modern Regression Methods. New York: Wiley.
Samorodnitsky, G., & M. Taqqu (1994). Stable Non-Gaussian Random Processes.
London: Chapman & Hall.
Satterthwaite, F. E. (1946). An approximate distribution of estimates of variance compon-
ents. Biomed. Bull., 2, 110–14.
Schenker, J., & J. F. Gentleman (2001). On judging the signiﬁcance of differences by
examining the overlap between conﬁdence intervals. Am. Stat., 55, 182–6.
Scheffé, H. (1959). Analysis of Variance. New York: Wiley.
Schreier, P. J., & L. L. Scharf (2010). Statistical Signal Processing of Complex-Valued
Data. Cambridge University Press.
Schreier, P. J., L. L. Scharf, & A. Hanssen (2006). A generalized likelihood ratio test for
impropriety of complex signals. IEEE Sig. Proc. Lett., 13, 433–6.
441
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Shaffer, J. P. (1991). The Gauss-Markov theorem and random regressors. Am. Stat., 45,
269–73.
Simpson, J., A. Olsen, & J. C. Eden (1975). A Bayesian analysis of a multiplicative
treatment effect in weather modiﬁcations. Technometrics, 17, 161–6.
Siotani, M., K. Yoshida, H. Kawakami, K. Nojiro, K. Kawashima, et al. (1963). Statistical
research on the taste judgement: analysis of the preliminary experiment on sensory and
chemical characters of Seishu. Proc. Inst. Stat. Math., 10, 99–118.
Smirnov, N. V. (1939). On the estimation of the discrepancy between empirical curves of
distribution for two independent samples. Bull. Math. Univ. Moscow. Serie Int., 2, 3–16.
Spearman, C. E. (1904). The proof and measurement of association between two things.
Am. J. Psych., 15, 72–101.
Stephens, M. A. (1970). Use of the Kolmogorov-Smirnov, Cramer-Von Mises and related
statistics without extensive tables. J. R. Stat. Soc., B32, 115–22.
Stephens, M. A. (1976). Asymptotic results for goodness-of-ﬁt statistics with unknown
parameters. Ann. Stat., 4, 357–69.
Stigler, S. (1977). Do robust estimators work with real data? Ann. Stat., 5, 1055–98.
Stuart, A., & J. K. Ord (1994). Kendall’s Advanced Theory of Statistics, vol. 1: Distribution
Theory, 6th edn. London: Edward Arnold.
Stuart, A., J. K. Ord, & S. Arnold (1999). Kendall’s Advanced Theory of Statistics, vol. 2:
Classical Inference and the Linear Model, 6th edn. London: Edward Arnold.
Thomson, A., & R. Randall-Maciver (1905). Ancient Races of the Thebaid. Oxford
University Press.
Thomson, D. J. (1977). Spectrum estimation techniques for characterization and develop-
ment of WT4 waveguide, I. Bell. Syst. Tech. J., 56, 1769–815.
Thomson, D. J., & A. D. Chave (1991). Jackknifed error estimates for spectra, coherences
and transfer functions. In Advances in Spectrum Analysis and Array Processing, vol. 1,
ed. S. Haykin. Englewood Cliffs, NJ: Prentice-Hall, pp. 58–113.
Thompson, W. R. (1936). On conﬁdence ranges for the median and other expectation
distributions for populations of unknown distribution form. Ann. Math. Stat., 7, 122–8.
Tibshirani. R. (1996). Regression shrinkage and selection via the lasso. J. R. Stat. Soc.,
B58, 267–88.
Uchaikin, V. V., & V. M. Zolotarev (1999). Chance and Stability. Waterbury, VT: VSP
Press.
Van Den Bos, A. (1995). A multivariate complex normal distribution: a generalization.
IEEE Trans. Inform. Theory, 41, 537–9.
Von Bortkiewicz, L. J. (1898). Das Gesetz der kleinen Zahlen. Leipzig: B.G. Teubner.
Von Luxburg, U., & V. H. Franz (2009). A geometric approach to conﬁdence sets for
ratios: Fieller’s theorem and the general linear model. Stat. Sinica, 29, 1095–117.
Von Mises, R. (1918). Über die “Ganszzahligkeit” der Atomgewichte und verwandte
Fragen. Phys. Z., 19, 490–500.
Von Neumann, J. (1951). Various techniques used in connection with random digits:
Monte Carlo methods. App. Math. Ser. Nat. Bur. Stand., 12, 36–8.
Wald, A. (1940). The ﬁtting of straight lines if both variables are subject to error. Ann.
Math. Stat., 11, 284–300.
442
References
.013
13:43:11, subject to the Cambridge Core terms of use,

Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the
number of observations is large. Trans. Am. Math. Soc., 54, 426–82.
Walden, A. T., & P. Rubin-Delanchy (2009). On testing for impropriety of complex-valued
Gaussian vectors. IEEE Trans. Sig. Proc., 57, 825–34.
Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. New
York: Springer.
Wasserstein, R. L., & N. A. Lazar (2016). The ASA’s statement on p-values: context,
process and purpose. Am. Stat., 70, 129–33.
Wilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics Bull., 1,
80–3.
Wilcoxon, F. (1946). Individual comparisons of grouped data by ranking methods. J. Econ.
Entomol., 39, 269–70.
Wilks, S. S. (1932). Certain generalizations in the analysis of variance. Biometrika, 24,
471–94.
Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing
composite hypotheses. Ann. Math. Stat., 9, 60–2.
Wishart, J. (1928). The generalized product moment distribution in samples from a normal
multivariate population. Biometrika, 20, 32–52.
Yohai, V. J. (1987). High breakdown-point and high efﬁciency robust estimates for
regression. Ann. Stat., 15, 642–56.
443
References
.013
13:43:11, subject to the Cambridge Core terms of use,


Preface
Theophilos Cacoullos has made signiﬁcant contributions to many areas
of probability and statistics including Discriminant Analysis, Estimation
for Compound and Truncated Power Series Distributions, Characteriza-
tions, Diﬀerential Variance Bounds, Variational Type Inequalities, and
Limit Theorems. This is reﬂected in his lifetime publications list and
the numerous citations his publications have received over the past three
decades.
We have been associated with him on diﬀerent levels professionally and
personally. We have beneﬁted greatly from him on both these grounds.
We have enjoyed his poetry and his knowledge of history, and admired
his honesty, sincerity, and dedication.
This volume has been put together in order to (i) review some of the
recent developments in Statistical Science, (ii) highlight some of the new
noteworthy results and illustrate their applications, and (iii) point out
possible new directions to pursue. With these goals in mind, a number of
authors actively involved in theoretical and applied aspects of Statistical
Science were invited to write an article for this volume. The articles so
collected have been carefully organized into this volume in the form of
38 chapters. For the convenience of the readers, the volume has been
divided into following six parts:
• APPROXIMATIONS, BOUNDS, AND INEQUALITIES
• PROBABILITY AND STOCHASTIC PROCESSES
• DISTRIBUTIONS, CHARACTERIZATIONS, AND
APPLICATIONS
• TIME SERIES, LINEAR, AND NON-LINEAR MODELS
• INFERENCE AND APPLICATIONS
• APPLICATIONS TO BIOLOGY AND MEDICINE
From the above list, it should be clear that recent advances in both
theoretical and applied aspects of Statistical Science have received due
attention in this volume. Most of the authors who have contributed to
this volume were present at the conference in honor of Professor Theophi-
los Cacoullos that was organized by us at the University of Athens,
Greece during June 3–6, 1999. However, it should be stressed here that
this volume is not a proceedings of this conference, but rather a vol-
©2001 CRC Press LLC

ume comprised of carefully collected articles with speciﬁc editorial goals
(mentioned earlier) in mind. The articles in this volume were refereed,
and we express our sincere thanks to all the referees for their diligent
work and commitment.
It has been a very pleasant experience corresponding with all the au-
thors involved, and it is with great pleasure that we dedicated this vol-
ume to Theophilos Cacoullos. We sincerely hope that this work will
be of interest to mathematicians, theoretical and applied statisticians,
and graduate students.
Our sincere thanks go to all the authors who have contributed to this
volume, and provided great support and cooperation throughout the
course of this project. Special thanks go to Mrs. Debbie Iscoe for the
excellent typesetting of the entire volume. Our ﬁnal gratitude goes to
Mr. Robert Stern (Editor, CRC Press) for the invitation to take on this
project and providing constant encouragement.
Ch. A. Charalambides
Athens, Greece
Markos V. Koutras
Athens, Greece
N. Balakrishnan
Hamilton, Canada
April 2000
©2001 CRC Press LLC

Contents
Preface
List of Contributors
List of Tables
List of Figures
Theophilos N. Cacoullos — A View of his Career
Publications of Theophilos N. Cacoullos
The Ten Commandments for a Statistician
Part I. Approximation, Bounds, and Inequalities
1 Nonuniform Bounds in Probability Approximations
Using Stein’s Method
Louis H. Y. Chen
1.1 Introduction
1.2 Poisson Approximation
1.3 Binomial Approximation: Binary Expansion
of a Random Integer
1.4 Normal Approximation
1.5 Conclusion
References
2 Probability Inequalities for Multivariate Distributions
with Applications to Statistics
Joseph Glaz
2.1 Introduction and Summary
2.2 Positive Dependence and Product-Type Inequalities
2.3 Negative Dependence and Product-Type Inequalities
2.4 Bonferroni-Type Inequalities
2.5 Applications
2.5.1 Sequential Analysis
2.5.2 A Discrete Scan Statistic
2.5.3 An Approximation for a Multinomial
Distribution
©2001 CRC Press LLC

2.5.4 A Conditional Discrete Scan Statistic
2.5.5 Simultaneous Prediction in Time Series
Models
References
3 Applications of Compound Poisson Approximation
A. D. Barbour, O. Chryssaphinou, and E. Vaggelatou
3.1 Introduction
3.2 First Applications
3.2.1 Runs
3.2.2 Sequence Matching
3.3 Word Counts
3.4 Discussion and Numerical Examples
References
4 Compound Poisson Approximation for Sums of
Dependent Random Variables
Michael V. Boutsikas and Markos V. Koutras
4.1 Introduction
4.2 Preliminaries and Notations
4.3 Main Results
4.4 Examples of Applications
4.4.1 A Compound Poisson Approximation for
Truncated Moving Sum of i.i.d. r.v.s.
4.4.2 The Number of Overlapping Success Runs
in a Stationary Two-State Markov Chain
References
5 Uniﬁed Variance Bounds and a Stein-Type Identity
N. Papadatos and V. Papathanasiou
5.1 Introduction
5.2 Properties of the Transformation
5.3 Application to Variance Bounds
References
6 Probability Inequalities for U-Statistics
Tasos C. Christoﬁdes
6.1 Introduction
6.2 Preliminaries
6.3 Probability Inequalities
References
©2001 CRC Press LLC

II. Probability and Stochastic Processes
7 Theory and Applications of Decoupling
Victor de la Pe˜na and T. L. Lai
7.1 Complete Decoupling of Marginal Laws and
One-Sided Bounds
7.2 Tangent Sequences and Conditionally
Independent Variables
7.3 Basic Decoupling Inequalities for Tangent
Sequences
7.4 Applications to Martingale Inequalities and
Exponential Tail Probability Bounds
7.5 Decoupling of Multilinear Forms, U-Statistics
and U-Processes
7.6 Total Decoupling of Stopping Times
7.7 Principle of Conditioning in Weak
Convergence
7.8 Conclusion
References
8 A Note on the Probability of Rapid Extinction
of Alleles in a Wright-Fisher Process
F. Papangelou
8.1 Introduction
8.2 The Lower Bound for Boundary Sets
References
9 Stochastic Integral Functionals in an Asymptotic
Split State Space
V. S. Korolyuk and N. Limnios
9.1 Introduction
9.2 Preliminaries
9.3 Phase Merging Scheme for Markov Jump
Processes
9.4 Average of Stochastic Integral Functional
9.5 Diﬀusion Approximation of Stochastic
Integral Functional
9.5.1 Single Splitting State Space
9.5.2 Double Split State Space
9.6 Integral Functional with Perturbed Kernel
References
©2001 CRC Press LLC

10 Busy Periods for Some Queues with Deterministic
Interarrival or Service Times
Claude Lef`evre and Philippe Picard
10.1 Introduction
10.2 Preliminaries: A Basic Class of Polynomials
10.2.1 Construction of the Basic Polynomials
10.2.2 A Generalized Appell Structure
10.3 The Dg/M (Q)/1 Queue
10.3.1 Model and Notation
10.3.2 Exact Distribution of Nr
10.4 The M (Q)/Dg/1 Queue
10.4.1 Model and Notation
10.4.2 Exact Distribution of Nr
References
11 The Evolution of Population Structure of the
Perturbed Non-Homogeneous Semi-Markov
System
P.-C. G. Vassiliou and H. Tsakiridou
11.1 Introduction
11.2 The Perturbed Non-Homogeneous
Semi-Markov System
11.3 The Expected Population Structure with
Respect to the First Passage Time Probabilities
11.4 The Expected Population Structure with
Respect to the Duration of a Membership
in a State
11.5 The Expected Population Structure with
Respect to the State Occupancy of a Membership
11.6 The Expected Population Structure with
Respect to the Counting Transition Probabilities
References
III. Distributions, Characterizations, and Applications
12 Characterizations of Some Exponential Families
Based on Survival Distributions and Moments
M. Albassam, C. R. Rao, and D. N. Shanbhag
12.1 Introduction
12.2 An Auxiliary Lemma
12.3 Characterizations Based on Survival
Distributions
12.4 Characterizations Based on Moments
©2001 CRC Press LLC

References
13 Bivariate Distributions Compatible or Nearly
Compatible with Given Conditional Information
B. C. Arnold, E. Castillo, and J. M. Sarabia
13.1 Introduction
13.2 Imprecise Speciﬁcation
13.3 Precise Speciﬁcation
13.4 An Example
References
14 A Characterization of a Distribution Arising from
Absorption Sampling
Adrienne W. Kemp
14.1 Introduction
14.2 The Characterization Theorem
14.3 An Application
References
15 Reﬁnements of Inequalities for Symmetric Functions
Ingram Olkin
References
16 General Occupancy Distributions
Ch. A. Charalambides
16.1 Introduction
16.2 A General Random Occupancy Model
16.3 Special Occupancy Distributions
16.3.1 Geometric Probabilities
16.3.2 Bernoulli Probabilities
References
17 A Skew t Distribution
M. C. Jones
17.1 Introduction
17.2 Derivation of Skew t Density
17.3 Properties of Skew t Distribution
17.4 A First Bivariate Skew t Distribution
17.5 A Second Bivariate Skew t Distribution
References
18 On the Posterior Moments for Truncation Parameter
Distributions and Identiﬁability by Posterior Mean for
Exponential Distribution with Location Parameters
Y. Ma and N. Balakrishnan
18.1 Introduction
©2001 CRC Press LLC

18.2 Posterior Moments
18.3 Examples
18.4 Identiﬁability by Posterior Mean
18.5 An Illustrative Example
References
19 Distributions of Random Volumes without Using
Integral Geometry Techniques
A. M. Mathai
19.1 Introduction
19.2 Evaluation of Arbitrary Moments of the
Random Volumes
19.2.1 Matrix-Variate Distributions for X
19.2.2 Type-1 Beta Distribution for X
19.2.3 The Case when the Rows of X are
Independently Distributed
19.2.4 Type-1 Beta Distributed Independent
Rows of X
19.2.5 Type-2 Beta Distributed Independent
Rows of X
19.2.6 Independently Gaussian Distributed
Points
19.2.7 Distributions of the r-Contents
References
IV. Time Series, Linear, and Non-Linear Models
20 Cointegration of Economic Time Series
T. W. Anderson
20.1 Introduction
20.2 Regression Models
20.3 Simultaneous Equation Models
20.4 Canonical Analysis and the Reduced
Rank Regression Estimator
20.5 Autoregressive Processes
20.6 Nonstationary Models
20.7 Cointegrated Models
20.8 Asymptotic Distribution of Estimators
and Test Criterion
References
©2001 CRC Press LLC

21 On Some Power Properties of Goodness-of-Fit
Tests in Time Series Analysis
Efstathios Paparoditis
21.1 Testing Spectral Density Fits
21.2 Local Power Considerations
21.3 Comparison
References
22 Linear Constraints on a Linear Model
Somesh Das Gupta
22.1 Introduction
22.2 Geometric Interpretation of the Role of the
Linear Constraints
References
23 M-Methods in Generalized Nonlinear Models
Antonio I. Sanhueza and Pranab K. Sen
23.1 Introduction
23.2 Deﬁnitions and Assumptions
23.3 Asymptotic Results
23.4 Test of Signiﬁcance and Computational
Algorithm
23.4.1 Subhypothesis Testing
23.4.2 Nonlinear Hypothesis Testing
23.4.3 Computational Algorithm
References
V. Inference and Applications
24 Extensions of a Variation of the Isoperimetric
Problem
Herman Chernoﬀ
24.1 Introduction
24.2 Information Retrieval Problem
24.3 Information Retrieval without Measurement
Error
24.4 Useful Information in a Variable
24.5 Allocation of Storage Space
24.6 The Isoperimetric Problem
24.7 Extensions
References
©2001 CRC Press LLC

25 On Finding a Single Positive Unit in Group Testing
Milton Sobel
25.1 Introduction
25.2 Description of Properties, Numerical Results
25.3 Some Formulas for Procedure RDH
25.4 The Greedy Procedure RG
25.5 Conclusions
25.6 Changing the Prior with Procedure RDH
25.7 Robustness of Procedure RDH for q Known
References
26 Testing Hypotheses on Variances in the Presence
of Correlations
A. M. Mathai and P. G. Moschopoulos
26.1 Bivariate Normal Population
26.2 Modifying the Hypothesis
26.3 Nonnull Moments
26.4 Null Case
26.5 The Conditional Hypothesis
References
27 Estimating the Smallest Scale Parameter:
Universal Domination Results
Stavros Kourouklis
27.1 Introduction
27.2 Main Results
References
28 On Sensitivity of Exponential Rate of
Convergence for the Maximum Likelihood
Estimator
James C. Fu
28.1 Introduction
28.2 Main Results
28.3 Some Applications
28.3.1 Exponential Model
28.3.2 Families of t-distributions with
Location Parameter
28.4 Discussion
References
©2001 CRC Press LLC

29 A Closer Look at Weighted Likelihood in the
Context of Mixtures
Marianthi Markatou
29.1 Introduction
29.2 Background
29.3 Simulation Experiments and Results
29.3.1 Normal Mixture with Equal
Component Variance
29.3.2 Normal Mixtures with Unequal
Component Variance
29.3.3 Other Models
29.4 Model Selection
29.5 Conclusions
References
30 On Nonparametric Function Estimation with
Inﬁnite-Order Flat-Top Kernels
Dimitris N. Politis
30.1 Introduction: A General Family of Flat-Top
Kernels of Inﬁnite Order
30.2 Multivariate Density Estimation: A Review
30.3 Further Issues on Density Estimation
30.3.1 Case of Smooth Density over a
Finite Domain
30.3.2 Case of Inﬁnite Domain with Some
Discontinuities
References
31 Multipolishing Large Two-Way Tables
Kaye Basford, Stephan Morgenthaler, and
John W. Tukey
31.1 Introduction
31.2 Bilinear Multipolishers
31.2.1 Choosing the Auxiliary Variables
Equal to the Eﬀects of the Additive Fit
31.2.2 Robust Alternatives
31.3 Matrix Approximations
31.3.1 Approximations of Two-Way Tables
31.4 Displays
31.5 Example
31.6 Concluding Remarks
References
©2001 CRC Press LLC

32 On Distances and Measures of Information:
A Case of Diversity
Takis Papaioannou
32.1 Introduction
32.2 Measuring Information – Measures of
Information
32.3 Properties of Measures of Information
32.4 Measures of Information and Inference
32.5 Applications
32.6 Conclusions
References
33
Representation Formulae for Probabilities of
Correct Classiﬁcation
Wolf-Dieter Richter
33.1 Introduction
33.2 Vector Algebraic Preliminaries
33.3 Distributional Results
33.3.1 Representation Formulae Based upon
the Two-Dimensional Gaussian Law
33.3.2 Representation Formulae Based upon the
Doubly Noncentral F-Distribution
References
34 Estimation of Cycling Eﬀect on Reliability
Vilijandas Bagdonaviˇcius and Mikhail Nikulin
34.1 Models
34.2 Semiparametric Estimation
34.2.1 The First Model
34.2.2 The Second Model
References
VI. Applications to Biology and Medicine
35 A New Test for Treatment vs. Control in an
Ordered 2 × 3 Contingency Table
Arthur Cohen and H. B. Sackrowitz
35.1 Introduction
35.2 New Test, Implementation and Example
35.3 Simulation Study
35.4 Theoretical Properties
Appendix
References
©2001 CRC Press LLC

36 An Experimental Study of the Occurrence
Times of Rare Species
Marcel F. Neuts
36.1 Statement of the Problem
36.2 The Design of the Experiment
36.3 Stage 2 of the Experiment
36.4 Findings
References
37 A Distribution Functional Arising in Epidemic
Control
Niels G. Becker and Sergey Utev
37.1 Introduction
37.2 Properties of the Functional
37.3 Proof of the Theorem
37.4 Application to Epidemic Control
References
38 Birth and Death Urn for Ternary Outcomes:
Stochastic Processes Applied to Urn Models
A. Ivanova and N. Flournoy
38.1
Introduction
38.2
A Birth and Death Urn with Immigration
for Ternary Outcomes
38.3
Embedding the Urn Scheme in a
Continuous-Time Birth and Death Process
38.4
The Probability Generating Function for the
Number of Success on Treatment i in the
Continuous-Time Birth and Death Process
38.5
The Probability Generating Function for
the Number of Trials on Treatment i
in the Continuous-Time Birth and
Death Process
38.6
The Number of Trials on Treatment i in
the Continuous-Time Birth and Death Process
38.7
The Joint Probability Generating Function
for the Number of Successes and the Number
of Trials in the Continuous-Time Birth and
Death Process
38.8
Adopting a Stopping Rule to Convert
Continuous-Time Statistics to the
Urn Design
38.9
Limiting Results for the Proportion of
Trials on Treatment i
©2001 CRC Press LLC

38.10 Limiting Results for the Proportion of
Successes on Treatment i in the Urn
38.11 Asymptotic Inference Pertaining to the
Success Probabilities
38.12 Concluding Remarks
References
©2001 CRC Press LLC

List of Contributors
M. Albassam, School of Mathematics, Department of Probability and
Statistics, University of Sheﬃeld, Sheﬃeld, UK
M.Al-Bassam@sheffield.ac.uk
T. W. Anderson, Department of Statistics, Stanford University, Stan-
ford, CA 94305, USA
twa@stat.stanford.edu
Barry C. Arnold, Department of Statistics, University of California,
Riverside, CA 92521-0138, USA
barnold@ucrstat2.ucr.edu
Vilijandas Bagdonaviˇcius, MI2S, BP-69, Universit´e de Bordeaux-2,
Bordeaux 33076, France; Department of Statistics, University of Vilnius,
Vilnius, Lithuania
N. Balakrishnan, Department of Mathematics and Statistics, McMas-
ter University, 1280 Main Street West, Hamilton, Ontario L8S 4K1,
Canada
bala@mcmail.cis.mcmaster.ca
A. D. Barbour, Abt.
Angewandte Mathematik, University Z¨urich-
Irchel, Winterthurerstr. 190, 8057 Z¨urich, Switerland
adb@amath.unizh.ch
Kaye Basford, School of Land and Food, The University of Queensland,
Brisbane, Queensland 4304, Australia
k.e.basford@mailbox.uq.edu.au
Niels G. Becker, School of Mathematical Sciences, Australian National
University, Canberra ACT 0200, Australia
Michael V. Boutsikas, Department of Mathematics, University of
Athens, Panepistemiopolis 15784, Greece
mbouts@math.uoa.gr
Enrique Castillo, Departamento de Economia, Universidad de Cantabria,
Avda. de los Castros, 39005 Santander, Spain
©2001 CRC Press LLC

Ch. A. Charalambides, Department of Mathematics, University of
Athens, Panepistemiopolis 15784, Greece
ccharal@math.uoa.gr
Louis H. Y. Chen, Department of Mathematics, National University
of Singapore, Lower Kent Ridge Road, Singapore 0511, Singapore
matchyl@leonis.nus.sg
Herman Chernoﬀ, Department of Statistics, Science Center, Harvard
University, 1 Oxford Street, Cambridge, MA 02138, USA
chernoff@stat.harvard.edu
Tasos C. Christoﬁdes, Department of Mathematics and Statistics,
University of Cyprus, P.O. Box 537 1678, Nicosia, Cyprus
tasos@pythagoras.mas.ucy.ac.cy
O. Chryssaphinou, Department of Mathematics, University of Athens,
Panepistemiopolis 15784, Greece
ocrysaf@math.uoa.gr
Arthur Cohen, Department of Statistics, Rutgers University, New
Brunswick, NJ 08903, USA
artcohen@rci.rutgers.edu
Somesh Das Gupta, Division of Statistics/Mathematics, Indian Sta-
tistical Institute, 203 Barrackpore Trunk Road, Calcutta 700 035, India
sdg@isical.ernet.in
Victor H. de la Pe˜na, Department of Statistics, Columbia University,
New York, NY 10027, USA
vp@stat.columbia.edu
N. Flournoy, Department of Mathematics and Statistics, The Amer-
ican University, 4400 Massachusetts Avenue, Washington, D.C. 20016,
USA
flournoy@american.edu
J. C. Fu, Department of Statistics, University of Manitoba, Winnipeg,
Manitoba R3T 2N2, Canada
fu@ccm.umanitoba.ca
Joseph Glaz, Department of Statistics, The College of Liberal Arts and
Sciences, University of Connecticut, 196 Auditorium Road U-120 MSB
©2001 CRC Press LLC

428, Storrs, CT 06269-3120, USA
glaz@uconnvm.uconn.edu
Anastasia Ivanova, Department of Biostatistics, University of North
Carolina, Chapel Hill, NC 27599, USA
M. C. Jones, Department of Statistics, The Open University, Walton
Hall, Milton Keynes MK7 6AA, UK
m.c.jones@open.ac.uk
Adrienne W. Kemp, Mathematical Institute, University of St. An-
drews, North Haugh, St. Andrews KY16 9SS, Scotland
cdk@st-and.ac.uk
V. S. Korolyuk, Institute of Mathematices, National Academy of Sci-
ences of Ukraine, Tereschenkovska 3, 252601 Kiev, Ukraine
S. Kourouklis, Department of Mathematics, University of Patras, Pa-
tras 26110, Greece
stavros@math.upatras.gr
M. V. Koutras, Department of Mathematics, University of Athens,
Panepistemiopolis 15784, Greece
mkoutras@cc.uoa.gr
Tze Leung Lai, Department of Statistics, Stanford University, Stan-
ford, CA 94305, USA
Claude Lef`evre, Institute de Statistique, Universit´e Libre de Bruxelles,
Campus Plan, CP 210 B-1050, Bruxelles, Belgique
clefevre@ulb.ac.be
N. Limnios, Division Math´ematiques Appliqu´ees, Universit´e de Tech-
nologie de Compi`egne, Compi`egne Cedex, France
nikolaos.limnois@uct.fr
Yimin Ma, Department of Mathematics and Statistics, McMaster Uni-
versity, 1280 Main Street West, Hamilton, Ontario L8S 4K1, Canada
Marianthi Markatou, Columbia University, 615 Mathematics Bldg.,
New York, NY 10027, USA
markat@stat.columbia.edu
©2001 CRC Press LLC

A. M. Mathai, Department of Mathematics, McGill University, 805
Sherbrooke Street West, Montreal, Quebec H3A 2K6, Canada
mathai@math.utep.edu
S. Morgenthaler, D´epartement de Math´ematiques, Swiss Federal In-
stitute of Technology, Lausanne CH-1015, Switzerland
Stephan.Morgenthaler@epfl.ch
P. G. Moschopoulos, Department of Mathematical Sciences, Univer-
sity of Texas at El Paso, El Paso, TX 79968-0001, USA
peter@math.utep.edu
Marcel F. Neuts, Department of Mathematics, University of Arizona,
Tucson, AZ 85721-0001, USA
marcel@sie.arizona.edu
Mikhail Nikulin, Math´ematiques, Informatiques et Sciences Sociales,
University Victor Segalen, Bordeaux 2 U.F.R. M.I. 2S, 146 rue L´eo Saig-
nat, 33076 Bordeaux, France
M.S.Nikouline@u-bordeaux2.fr
I. Olkin, Department of Statistics and Education, Stanford University,
Sequoia Hall, Stanford, CA 94305, USA
iolkin@stat.stanford.edu
N. Papadatos, Department of Mathematics and Statistics, University
of Cyprus, P.O. Box 537 1678, Nicosia, Cyprus
npapadat@pythagoras.mas.ucy.ac.cy
Takis Papaioannou, Department of Mathematics, University of Ioan-
nina, Ioannina 45110, Greece
takpap@unipi..gr
Fredos Papangelou, Department of Mathematics, University of Manch-
ester, Manchester M13 9PL, UK
fredos.papangelou@man.ac.uk
Efstathios Paparoditis, Department of Mathematics and Statistics,
University of Cyprus, P.O. Box 537 1678, Nicosia, Cyprus
stathisp@pythagoras.mas.ucy.ac.cy
V. Papathanasiou, Department of Mathematics, University of Athens,
Panepistemiopolis 15784, Greece
bpapath@math.uoa.gr
©2001 CRC Press LLC

Philippe Picard, 14 Quai de Serbie, 69006 Lyon, France
picard@univ-lyon1.fr
Dimitris N. Politis, Department of Mathematics, University of Cali-
fornia at San Diego, La Jolla, CA 92093-0112, USA
politis@mathgrad.ucsd.edu
C. R. Rao, Department of Statistics, Pond Laboratory, Penn State
University, University Park, PA 16802, USA
crr1@psuvm.psu.edu
Wolf-Dieter D. Richter, Fachbereich Mathematik, Universit¨at Ros-
tock, Universit¨atsplatz 1 D-18051, Rostock, Germany
richter@likeli-uni-rostock.de
H. B. Sackrowitz, Department of Applied Mathematics and Statistics,
Rutgers University, Hill Center, Busch Campus, 110 Frelinghuysen Road,
Piscataway, NJ 08855, USA
sackrowi@rci.rutgers.edu
Antonio I. Sanhueza, Universidad de La Frontera, Temuco, Chile
Jos´e Maria Sarabia, Departamento de Economia, Universidad de Cantabria,
Avda. de los Castros, 39005 Santander, Spain
Pranab K. Sen, Department of Biostatistics, University of North Car-
olina, CB #7400 3101, McGavran-Greenberg Hall, Chapel Hill, NC
27599-7400, USA
pksen@bios.unc.edu
D. N. Shanbhag, School of Mathematics, Department of Probability
and Statistics, University of Sheﬃeld, Sheﬃeld, UK
D.Shanbhag@sheffield.ac.uk
Milton Sobel, Department of Mathematics, University of California at
Santa Barbara, Santa Barbara, CA 93017, USA
sobel@pstat.ucsb.edu
H. Tsakiridou, Department of Mathematics, Section of Statistics and
O.R., Aristotle University of Thessaloniki, 54006 Thessaloniki, Greece
heltsake@auth.gr
John W. Tukey Princeton University, Washington Road, Princeton,
NJ 08544-1000, USA
eo@math.princeton.edu
©2001 CRC Press LLC

Sergei Utev, Department of Mathematics and Statistics, School of Sta-
tistical Sciences, La Trobe Unviersity, Bundoora, VIC 3083, Australia
stasu@lure.latrobe.edu.au
E. Vaggelatou, Department of Mathematics, University of Athens,
Panepistemiopolis 15784, Greece
evagel@math.uoa.gr
P.-C. G. Vassiliou, Department of Mathematics, Section of Statis-
tics and O.R., Aristotle University of Thessaloniki, 54006 Thessaloniki,
Greece
Vasiliou@ccf.auth.gr
©2001 CRC Press LLC

List of Tables
2.1
Lower bounds and simulated values of P(τ > n)
α = β1 = .05 and θ∗= .5
2.2
Lower bounds and simulated values of
Eθ(τ)
2.3
Product-type bounds for P(Sn ≥k)
Bernoulli model, n = 500
2.4
Bounds for P-values for a test of equal cell probabilities
2.5
Bonferroni-type bounds for P(k; m, n, a)
2.6
Bonferroni-type bounds for E (Sm| n
i=1 Xi = a)
2.7
Evaluating the constant c using four methods
21.1
Proportion of rejections in 1000 samples of the
hypothesis of white noise
24.1
24.2
Break even values of n for shifting s
25.1
Comparison of procedure RDH with the optimal
procedure R′
1 for known q-values
25.2
New Bayesian procedure R(1)
1B for ﬁnd a single
positive unit
29.1
WLEE estimators with starting values MMEs
computed using the entire sample. The sample model
is p1N(0, 1) + p2N(8, 1) + p3N(12, 1), with p1 = p2.
The sample size is 100 and the number of Monte
Carlo replications is 100
29.2
Frequency of identiﬁed roots. The sample model is
p1N(0, 1) + p2N(8, 1) + p3N(12, 1), p1 = p2.
Model I is p1N(0, 1) + p2N(8, 1), II is
p1N(8, 1) + p2N(12, 1), III is p1N(0, 1) + p2N(12, 1)
and model IV stands for an MLE-like ﬁt. The + sign
indicates that the model was suggested, the −sign
indicates absence of it. The frequency is over 100
samples if size 100 with 50 bootstrap searches in
each sample
©2001 CRC Press LLC

29.3
Bootstrap estimates of the parameters of the mixture
p1N(0, 1) + p2N(8, 1), p1 + p2 = 1. The sampling
model is p1N(0, 1) + p2N(8, 1) + p3N(12, 1), p1 = p2.
The number of Monte Carlo replications is 100,
the sample size is 100 and the number of bootstrap
samples is 50
29.4
Huber’s estimators for the mean parameters and
covariance matrix Σ. The sampling model is
p1N((0, 0), I) + p2N((4, 4), I) + p3N((−3, 5), I),
p1 = p2. The sample size is 100 and the number of
Monte Carlo replications is 100
29.5
Means of MME, MLE and WLEE estimates and
their mean squared errors. The ﬁtted model is
p1N(µ1, σ2
1) + p2N(µ2, σ2
2). The notation
t(r)
3
means that the sample is rescaled to have
variance 1. Also t(r)
3 (2.56) means that the second
location is 2.56. In parenthesis the mean squared
error is reported. The sample size is 100 and the
number of Monte Carlo replications is 100
31.1
This table shows part of a 50 × 65 genotype by
environment dataset. The leftmost column and
the last row show the additive eﬀects as ﬁtted
by least squares
35.1
Body weight class
35.2
Simulated power functions
36.1
Parameters of the linear regression lines for the
logarithms of the times between occurrences of
rare indices in 50,000,000 trials using a geometric
prevalence distribution with success probability 0.05.
The ﬁrst column gives the number of the replication;
the second, the intercept, and the third the slope of
the linear regression. J is the number of rare indices
found in 50,000,000 trials in stage 2. The ﬁnal index
gives the time at which the simulations stopped, the
time variable having exceeded 50,000,000
©2001 CRC Press LLC

List of Figures
10.1
Arrivals (F) and services (τ) trajectories in the
Dg/M (Q)/1 queue
10.2
Arrivals (τ) and services (F) trajectories in the
M (Q)/Dg/1 queue
11.1
Stochastic evolutions at time t
11.2
Entrance for the ﬁrst time at state j at time s + n,
after one transition
11.3
Entrance for the ﬁrst time in state j at time s + n,
after two transition
11.4
Entrance for the ﬁrst time in state j at time s + n,
after ν −1 transitions
11.5
Duration in state i
11.6
Occupancy of state j for ν times in the time interval
(s, s + n)
17.1
Densities (17.2.1), standardised to have zero mean and
unit variance, for a = 2ib, i = 0, ..., 6, having increasing
amounts of skewness with i, in the case b = 2
17.2
The bivariate density (17.4.2) with a = 13, ν = 10
and c = 7
17.3
The bivariate density (17.5.1) with a = 1, b = 2
and c = 3
28.1
The expected scores S(θ|d) for d = 1, 6, 15, ∞,
and the empirical scores 1
nl(1)
n (θ) based on
Eq. (28.3.16) for n = 5 under model M1
31.1
Each curve corresponds to a genotype and shows
the ﬁtted value as a function of the environment eﬀects
31.2
The top panel shows the linear part of the ﬁt,
whereas the bottom contains the jagged part
31.3
Each curve corresponds to an environment and shows
the ﬁtted value as a function of the genotype eﬀects
31.4
The top panel shows the linear part of the ﬁt, whereas
the bottom contains the jagged part
©2001 CRC Press LLC

31.5
The individual slopes b are shown against the eﬀects
u of the purely additive ﬁt
31.6
This is the same as Fig. 31.1, with the three selected
genotypes highlighted
©2001 CRC Press LLC

Theophilos N. Cacoullos—A View of his
Career
Theophilos N. Cacoullos was born on April 5, 1932 in Pachna, a village
in the Limassol district of Cyprus. He is the eldest son of Nicolas and
Galatea Cacoullos. After his secondary education at Limassol Lanitio
Gymnasium, he studied mathematics at the University of Athens, receiv-
ing his diploma (B.Sc.) in 1954. He then returned to Cyprus and worked
for two years as a teacher of mathematics in his alma mater high school.
In 1957 he went to the USA, on a scholarship from the Greek Scholarship
Foundation, for graduate studies. He received his M.A. in 1960 and his
Ph.D. in 1962 in Mathematical Statistics from Columbia University, New
York. His M.A. thesis, with Alan Birnbaum, on an almost forgotten topic
of median-unbiased estimation, prompted his combinatorial derivation of
the distribution of the n-fold convolution of the zero truncated Poisson
distribution, published in the Annals of Mathematical Statistics of 1965.
His doctoral thesis on comparing Mahalanobis Distances between normal
populations with unknown means, published in two extensive papers in
Sankhy¯a of 1965, was written under the supervision of the late Professor
Rosedith Sitgreaves and Professor T. W. Anderson. Cacoullos says that
while many colleagues think he is a multivariate analysis expert, the
truth is “I am a multitopic visitor; as in everyday life I am a random
variable, a random walker; I am an actual stochastician.” He revisited
the topic of discriminant analysis with his paper in the Journal of Mul-
tivariate Analysis of 1992; he characterized normality under spherical
symmetry by the admissibility of Fisher’s linear discriminant functions
and showed that, within the family of spherical normal scale mixtures,
the normal maximizes the minimax probability of correct classiﬁcation
provided the Mahalanobis distance between the two populations is the
same.
While at Columbia University (1957–1961), Cacoullos met and mar-
ried Ann Rossettos, a graduate student in the Department of Philosophy.
They have three daughters, Rena, Nike, and Galatea, and a granddaugh-
ter, Annan Torres, all born in the USA. Ann Rossettos-Cacoullos is Pro-
fessor in the School of Philosophy of the University of Athens. Rena
Cacoullos-Torres is an Assistant Professor of Linguistics and Spanish at
the University of Florida; Nike, a biologist, is a researcher at the Greek
Cancer Institute in Athens; Galatea, also a Columbia alumna, works for
©2001 CRC Press LLC

the Alpha Credit Bank of Greece. Cacoullos himself has always been
interested in Greek linguistics, philosophy and stochasticity, as well as
poetry.
In 1961–1962 he worked as Research Associate at Stanford Univer-
sity, meeting Herman Chernoﬀ, Ingram Olkin, Lincoln Moses, Charles
Stein, Gerald Lieberman and Samuel Karlin, among others. He worked
with Olkin, producing their joint 1965 Biometrika paper on the bias of
the characteristic roots of a random matrix. In May 1962 he returned
to Cyprus accepting the position of the Director of the Department of
Statistics and Research of the new independent Republic of Cyprus.
Unable to adjust to the routine work of a small Government Statistics
Oﬃce, he returned to the States, accepting an Assistant Professorship
at the University of Minessota, Minneapolis. Yet, during his 15-month
bureaucratic work in Cyprus, he prepared his paper on nonparametric
multivariate density estimation, published in 1966 in the Annals of the
Institute of Statistical Mathematics. In Minneapolis he worked on char-
acterizations of normality by constant regression of quadratic and linear
forms; a paper by Geisser motivated the interesting relation between the
t and F distributions, published in 1965 in the Journal of the American
Statistical Association. He also worked with Milton Sobel on inverse-
sampling procedures for ranking multinomial distributions. In 1965 he
accepted an appointment as Associate Professor in the Department of
Industrial Engineering and Operations Research of New York University.
In April 1967, Cacoullos was elected Professor in the newly created
Chair of Probability and Statistics of the University of Athens, and in
1968 he returned to Greece, beginning his 31 year long career in the Fac-
ulty of Mathematical and Physical Sciences. An excellent teacher, never
using notes, with spontaneous humor, his inspired lectures inﬂuenced
many of his students. As the ﬁrst professor of Mathematical Statistics
in Greece, he deﬁnitely shaped the teaching of Statistics in Greek Uni-
versities.
In addition to standard courses of statistics, he introduced
combinatorics, linear programming, stochastic processes and recently
actuarial mathematics. Equally important and decisive for the future
of research in mathematical statistics in Greece was his supervising of
young mathematicians. Two of the editors of this volume (Ch. A. Char-
alambides and Markos V. Koutras) were honored to be among his ﬁrst
doctoral students. Several other people received their Ph.D. in Math-
ematical Statistics under his supervision or joint supervision. Among
them are Vassilis Papathanasiou, and Ourania Chryssaphinou, now as-
sociate professors of the Department of Mathematics of the University
of Athens. His guidance in widening his doctoral students’ research in-
terests and his encouragement to take advantage of a leave of absence to
do research in other Universities abroad were invaluable. The interna-
©2001 CRC Press LLC

tional recognition of his research work and his association with top rank
Statistics Departments were instrumental in obtaining positions for his
doctoral advisees.
Cacoullos himself visited several Institutions to teach, do research or
give seminar talks. He visited McGill University, Montreal, Canada, in
1972–1973 and 1975–1976, always enjoying the collaboration and friend-
ship of Harold Ruben, George Styan, and Arak Mathai. At Stanford
he taught in the summer of 1974. In the fall of 1983, at the invitation
of Chernoﬀ, he gave a series of lectures under Assorted Topics at the
Statistics Center of M.I.T.; in the spring of 1984 he visited the Uni-
versity of Arizona, Tucson. In 1986–1987 he spent the whole academic
year at Columbia University. In the spring of 1989 he visited the Center
for Multivariate Analysis under C. R. Rao, then at the University of
Pittsburgh.
Cacoullos has published about 60 papers in international journals. His
main research contribution since the early eighties has been the deriva-
tion of diﬀerential upper (Chernoﬀ-type) and lower variance bounds for
functions of random variables. Related characterizations of distributions
as well as elementary proofs of the classical central limit theorem have
also been obtained in a series of papers with his collaborators. Recently
he has been working on reducing tests of homoscedasticity to testing
independence under multinormality. He wrote seven textbooks on prob-
ability and statistics in Greek and edited the Discriminant Analysis and
Applications, published in 1973 by Academic Press. In 1989 Springer
published his book Exercises in Probability.
He also translated into
Greek, Stephenson’s Mathematical Methods for Science Students, Gass’
Linear Programming, and Formulae and Tables for Statistical Work by
C. R. Rao et al.
Ch. A. Charalambides
M. V. Koutras
N. Balakrishnan
©2001 CRC Press LLC

Publications of Theophilos N. Cacoullos
Books
1. Discriminant Analysis and Applications (Editor), Academic Press,
New York. 1973.
2. Exercises in Probability. Springer-Verlag, New York, 1989. Also
published by Narosa Publishing House, New Delhi as a Springer
International Student Edition, 1993.
Books translated into Greek
1. G. Stephenson: Mathematical Methods for Science Students, Long-
man, London, 1973.
2. S. Gass: Linear Programming, McGraw Hill, New York, 1974.
3. C. R. Rao, S. K. Mitra, A. Mathai, K. G. Ramamurthy, Formu-
lae and Tables for Statistical Work, Statistical Publishing Society
Calcutta, 1974.
Books in Greek
1. A Course in Probability Theory, Athens, 1969.
2. Elements of Stochastic Processes, Athens, 1970.
3. Exercises in Probability with Solutions and Elements of Theory,
Vol. 1, 2nd edition, Athens, 1986.
4. Probability Theory: Exercises and Problems with Solution and El-
ements of Theory, Vol. 2, 3rd edition, Athens, 1986.
5. Statistics, Theory and Applications, Athens, 1972.
6. Stochastic Processes, Athens, 1978.
7. Actuarial Science, Vol.
I: Risk Theory and Probability, Athens,
1994.
Articles
1. A combinatorial derivation of the distribution of the truncated
Poisson suﬃcient statistic. Annals of Mathematical Statistics 32
(1961), 904–905.
2. Comparing Mahalanobis Distances I: Comparing distances between
k known normal populations and another unknown. Sankhy¯a, Se-
ries A 27(1965), 1–22.
©2001 CRC Press LLC

3. Comparing Mahalanobis Distances II: Bayes procedures when the
mean vectors are unknown. Sankhy¯a, Series A 27(1965), 23–32.
4. On the bias of functions of characteristic roots of a random matrix
(with Ingram Olkin). Biometrika 52(1965), 87–94.
5. A relation between t- and F-distributions. Journal of the American
Statistical Association 60(1965), 179–182, 1249.
6. Estimation of a multivariate density.
Annals of the Institute of
Statistical Mathematics, Tokyo 18(1966), 179–189.
7. An inverse-sampling procedure for selecting the most probable event
in a multinomial distribution (with Milton Sobel). In Multivariate
Analysis (Ed., P. R. Krishnaiah) (1966), 423–455, Academic Press,
New York.
8. On a class of admissible partitions. Annals of Mathematical Statis-
tics 37(1966), 189–195.
9. Asymptotic distribution for a generalized Banach match-box prob-
lem.
Journal of the American Statistical Association 62(1967),
1252–57.
10. Characterizations of normality by constant regression of linear statis-
tics on another linear statistic. Annals of Mathematical Statistics
38(1967), 1894–1898.
11. On the distribution of the bivariate range (with Henry DeCicco).
Technometrics 9(1967), 476–480.
12. Some characterizations of normality. Sankhy¯a, Series A 29(1967),
399–404.
13. A sequential scheme for detecting outliers. Bull. de la Soc. Math.
de Grece, Nouvelle Serie 9(1968), 113–123.
14. Some properties of the incomplete beta-function ratio. Bull. de la
Soc. Math. de Grece, Nouvelle Serie 11(1970), 132–138.
15. Some remarks on topothetical procedures. Proceedings, ISI 38th
Session, Washington, DC, 70–73. Bull. Int. Statist. Inst. 1(1971),
128–131.
16. MVUE for a truncated Poisson process (with Ch. Charalambides).
Proceedings of the 4th Session of the Greek Mathematics Society.
Patras (1971), 73–77.
17. On ﬁxed-point solutions of operators and some reliability problems
(with N. Apostolatos). Research Chronicles, University of Athens,
Volume 3(1972), 306– 312.
18. Some remarks on the convolution of Poisson distributions with the
zero class missing. The American Statistician 83(1972).
©2001 CRC Press LLC

19. A bibliography of discriminant analysis (with G.P.H. Styan). In
Discriminant Analysis and Applications (Ed., T. Cacoullos), (1973),
375–435, Academic Press, New York.
20. Distance, discrimination and error. In Discriminant Analysis and
Applications (Ed., T. Cacoullos), (1973), 61–75, Academic Press,
New York.
21. MVUE for truncated discrete distributions (with Ch. Charalam-
bides). Invited paper, European IMS Meeting, Budapest, August
31-September 5, 1972, in Progress in Statistics (Eds., J. Gani et
al.), Vol. 1(1974), 133–144, Colloquia Mathematica Societatis Janos
Bolyai, North-Holland.
22. On minimum variance unbiased estimation for truncated binomial
and negative binomial distributions (with Ch.
Charalambides).
Annals of the Institute of Statistical Mathematics, 27(1975), 133–
144.
23. Multiparameter Stirling and C-type distributions.
In Statistical
Distributions in Scientiﬁc Work 1(1975): Models and Structures
(Eds., G.P. Patil et al.), pp. 19–30. Reidel, Dordrecht.
24. Best estimation for multiply truncated PSD’s. Scandinavian Jour-
nal of Statistics 4(1977), 159–164.
25. On some bivariate probability models applicable to traﬃc accidents
and fatalities (with H. Papageorgiou). International Statistical Re-
view 48(1980), 345–356.
26. On bivariate discrete distributions generated by compounding (with
H. Papageorgiou). In Statistical Distributions in Scientiﬁc Work
4(1981), Models, Structures and Characterizations (Eds., C. Taillie
et al.), pp. 197–212. Reidel, Dordrecht.
27. On upper and lower bounds for the variance of a function of a
random variable. Annals of Probability 10(1982), 799–809.
28. Bivariate
negative
binomial-Poisson
and
negative
binomial-
Bernoulli models with an application to accident data (with H.
Papageorgiou). In Statistics and Probability: Essays in honor of
C.R. Rao (Eds., G. Kallianpur et al.), (1982), pp. 155– 168, North-
Holland.
29. Characterizing the negative binomial distribution (with H. Pa-
pageorgiou), letter to the Editor. Journal of Applied Probability
19(1982), 742–743.
30. Characterizations of discrete distributions by a conditional distri-
bution and a regression function (with H. Papageorgiou). Annals
of the Institute of Statistical Mathematics, 35(1983), 95–103.
©2001 CRC Press LLC

31. Characterizations of mixtures of discrete distributions by a regres-
sion point. Statistics & Probability Letters 1(1983), 269–272.
32. Multiparameter Stirling and C-numbers; recurrences and applica-
tions (with H. Papageorgiou). The Fibonacci Quarterly 22(1984),
119–133.
33. Characterizations of mixtures of continuous distributions by their
posterior means (with H. Papageorgiou). Scandinavian Actuarial
Journal, (1984), 23–30.
34. Quadratic forms in spherical random variables: generalized noncen-
tral 2- distributions (with M. Koutras). Naval Research Logistics
Quarterly 31(1984), 447–461.
35. On upper bounds for the variance of functions of random variables
(with V. Papathanasiou). Statistics & Probability Letters 3(1985),
175–184.
36. Minimum distance discrimination for spherical distributions (with
M. Koutras). In Statistical Theory and Data Analysis, (Ed., K.
Matusita), (1985), pp. 91–102, North-Holland.
37. Bounds for the variance of functions of random variables by orthog-
onal polynomials and Bhattacharya bounds (with V. Papathana-
siou). Statistics & Probability Letters 4(1986), 21–23.
38. Characterizing priors by posterior expectations in multiparameter
exponential families. Annals of the Institute of Statistical Mathe-
matics, 39(1987), 399–405.
39. Characterization of generalized multivariate discrete distributions
by a regression point. Statistics & Probability Letters 5(1987), 39–
42.
40. On minimum-distance location discrimination for isotropic distri-
butions. Proceedings DIANA II, Praha, (1987) 1–16.
41. Characterization of distributions by variance bounds (with V. Pa-
pathanasiou). Statistics & Probability Letters 7(1989), 351–356.
42. Dual Poincar´e-type inequalities via the Cramer-Rao and the
Cauchy-Schwarz inequalities and related characterizations. In C.
R. Rao’s honorary 70th birthday festschrift Data Analysis and In-
ference (Eds., Y. Dodge), (1989), pp. 239–250, North-Holland.
43. Characterizations of distributions within the elliptical class by a
gamma distributed quadratic form (with C.G. Khatri). In Khatri’s
Memorial Volume. Gujarat Statistics Review, (1990), 89–98.
44. Correcting Remarks on “Characterization of normality within the
elliptical contoured class” (with C.G. Khatri). Statistics & Proba-
bility Letters 11(1990), 551–552.
©2001 CRC Press LLC

45. The optimality of linear discriminant functions under spherical
symmetry. Proceedings DIANA III, (1990), pp. 20–33, Czechoslo-
vak Academy of Sciences, Praha.
46. Another characterization of the normal law and a proof of the cen-
tral limit thoerem connected with it (with V. Papathanasiou and S.
Utev). Theory of Probability and its Applications 37(1992), 648–
657 (in Russian). Also in the English translation of the Russian
journal, pp. 581–588, 1993.
47. Two LDF Characterizations of the Normal as a Spherical Distri-
bution. Journal of Multivariate Analysis 40(1992), 205–212.
48. Lower variance bounds and a new proof of the multivariate central
limit theorem (with V. Papathanasiou). Journal of Multivariate
Analysis 43(1992), 173–184.
49. Variance Inequalities, Characterizations and a Proof of the Central
Limit Theorem, Festschrift in honor of F. Eicker. Data Analysis
and Statistical Inference (Eds., S. Schach and G. Trenkler), (1992),
pp. 27–32, Joseph Eul, Verlag.
50. Variational inequalities with examples and an application to the
central limit theorem (with V. Papathanasiou and S. Utev). Annals
of Probability 23(1994), 1607–1618.
51. A generalizattion of a covariance identity and related characteriza-
tions (with V. Papathanasiou). Mathematical Methods of Statistics
4(1995), 105–113.
52. On the Performance of Minimum-Distance Classiﬁcation Rules for
Kotz-type Elliptical Distributions (with M. Koutras). In Advances
in the Theory and Practice of Statistics: A Volume in Honor of
Samuel Kotz (Eds., Norman L. Johnson and N. Balakrishnan),
(1996), pp. 209–224, John Wiley & Sons, New York.
53. Characterizations of Distributions by Generalized Variance Bounds
and a Simple Proof of the CLT (with V. Papathanasiou). Statistical
Planning and Inference, Special Volume in honor of C. R. Rao’s
75th birthday, 61(1997), 157–171.
54. Variance inequalities for covariance kernels and applications to cen-
tral limit theorems (with N. Papadatos and V. Papathanasiou).
Theory of Probability and its Applications 42(1997), 195–201.
55. The Statisticity and Stochasticity of the Greeks, ISI Session 51, L
VII, Contributed Papers, Book 1(1997), 339–340.
56. Three elementary proofs of the central limit theorem with applica-
tions to random sums (with N. Papadatos and V. Papathanasiou).
In Stochastic Processes and Related Topics In Memory of S. Cam-
©2001 CRC Press LLC

banis, (Eds., I. Karatzas, B. Rajput, M. Taqqu), (1998), pp. 15–23,
Birkhauser, Boston.
57. Il statisticit`a et stochasticit`a dei Greci, Induzioni 16(1998), 23–31.
58. An application of an density transform, and the local limit theorem
(with N. Papadatos and V. Papathanasiou). (2000). Submitted for
publication.
59. Testing homoscedasticity via independence, under joint normality.
(2000). Submitted for publication.
60. The F-test of homoscedasticity for correlated normal variables.
(2000). Statistics & Probability Letters, to appear.
©2001 CRC Press LLC

The Ten Commandments for a Statistician
Theophilos Cacoullos
1. You must not have any other masters beside Chance.
2. You must not construct for yourself any model that is biased either
for your employer above or your client below or that is in the service
of the underworld.
You must not pay homage to them, nor serve them; for I,
Chance, your master am a source of error, propagating sample
bias to third and fourth phase subsamples of those who ignore me,
but showing consistency to the thousandth order of those that pay
homage to me and follow my distributions.
3. You must not invoke the name of Chance, your Master, to misin-
formation; for Chance will not hold him right that uses its name
in vain, by commission or omission.
4. Remember the seventh stage to stochasticize.
Six stages you are to labor and do all your work, but in the
seventh stage, a meditation on Chance, you must not do any work,
neither quantify, nor enumerate, nor sample, nor tabulate or cal-
culate, by hand or machine, nor analyse.
5. Adhere to the principles and code of your Society that you make
right inferences on the population that Chance is giving you.
6. You must not conceal nor distort information.
7. You must not abuse information acquired in conﬁdence.
8. You must not plagiarize.
9. You must not bring a false charge against your fellow.
10. You must not covet your colleague’s art; you must not covet your
colleague’s company, nor his software or hardware, nor his algo-
rithm, nor his program, nor anything that is your colleague’s.
Sources
1. Exodus, Chapter 20, The Complete Bible, The Old Testament (J.
M. Powis Smith Transl.) The University of Chicago Press.
2. JSI (1985) Declaration on Professional Ethics, International Sta-
tistical Review 1986, 54, pp. 227–242.
©2001 CRC Press LLC

3. Professional Code of Conduct, The Institute of Statisticians, Pre-
ston, Lancashire, England.
4. JCC/E.S.O.M.A.R., International Code of Marketing and Social
Research Practice, 1976.
©2001 CRC Press LLC
Theophilos Cacoullos

PART I
Approximation, Bounds, and Inequalities
©2001 CRC Press LLC

1
Nonuniform Bounds in Probability
Approximations Using Stein’s Method
Louis H. Y. Chen
National University of Singapore, Republic of Singapore
ABSTRACT Most of the work on Stein’s method deals with uniform
error bounds. In this paper, we discuss non-uniform error bounds using
Stein’s method in Poisson, binomial, and normal approximations.
Keywords and phrases Stein’s method, non-uniform bounds, proba-
bility approximations, Poisson approximation, binomial approximation,
normal approximation, concentration inequality approach, binary expan-
sion of a random integer
1.1
Introduction
In 1972 Stein introduced a method of normal approximation which does
not depend on Fourier analysis but involves solving a diﬀerential equa-
tion.
Although his method was for normal approximation, his ideas
are applicable to other probability approximations. The method also
works better than the Fourier analytic method for dependent random
variables, particularly if the dependence is local or of a combinatorial
nature. Since the publication of this seminal work of Stein, numerous
papers have been written and Stein’s ideas applied in many diﬀerent con-
texts of probability approximation. Most notable of these works are in
normal approximation, Poisson approximation, Poisson process approxi-
mation, compound Poisson approximation and binomial approximation.
An account of Stein’s method and a brief history of its developments can
be found in Chen (1998).
In this paper we discuss another aspect of the application of Stein’s
method, not in terms of the approximating distribution but in terms of
the nature of the error bound. Most of the papers on Stein’s method
deal with uniform error bounds. We show that Stein’s method can also
©2001 CRC Press LLC

be applied to obtain non-uniform error bounds and of the best possible
order. Roughly speaking, a uniform bound is one on a metric between
two distributions.
Whereas a non-uniform bound on the discrepancy
between two distributions, L(W) and L(Z), is one on |Eh(W)−Eh(Z)|,
which depends on h for every h in a separating class. We will consider
non-uniform bounds in three diﬀerent contexts, Poisson, binomial, and
normal. In the exposition below, we will focus more on ideas than on
technical details.
1.2
Poisson Approximation
Poisson approximation using Stein’s method was ﬁrst investigated by
Chen (1975a).
Since then many developments have taken place and
Poisson approximation has been applied to such diverse ﬁelds as ran-
dom graphs, molecular biology, computer science, probabilistic number
theory, extreme value theory, spatial statistics, and reliability theory,
where many problems can be phrased in terms of dependent events. See
for example Arratia, Goldstein, and Gordon (1990), Barbour, Holst, and
Janson (1992) and Chen (1993). All these results of Poisson approxima-
tion concern error bounds on the total variation distance between the
distribution of a sum of dependent indicator random variables and a
Poisson distribution. These bounds are therefore uniform bounds.
The possibility of nonuniform bounds in Poisson approximation using
Stein’s method was ﬁrst mentioned in Chen (1975b). For independent
indicator random variables, nonuniform bounds were ﬁrst obtained for
small and moderate λ by Chen and Choi (1992) and for unrestricted λ
with improved results by Barbour, Chen, and Choi (1995). To explain
the ideas behind obtaining nonuniform bounds, we ﬁrst illustrate how
a uniform bound is obtained in the context of independent indicator
random variables.
Let X1, . . . , Xn be independent indicator random variables with
P(Xi = 1) = 1 −P(Xi = 0) = pi, i = 1, ..., n. Deﬁne W = n
i=1 Xi,
W (i) = W −Xi, λ = n
i=1 pi and Z to be a Poisson random variable
with mean λ. Let fh be the solution (which is unique except at 0) of the
Stein equation
λf(w + 1) −wf(w) = h(w) −Eh(Z)
where h is a bounded real-valued function deﬁned on Z+ = {0, 1, 2, . . .}.
Then we have
©2001 CRC Press LLC

Eh(W) −Eh(Z) = E {h(W) −Eh(Z)}
= E {λfh(W + 1) −Wfh(W)}
=
n

i=1
p2
i E△fh(W (i) + 1)  
(1.2.1)
where △f(w) = f(w + 1) −f(w). A result of Barbour and Eagleson
(1983) states that ∥△fh∥∞≤2(1 ∧λ−1)∥h∥∞. Applying this result, we
obtain
dT V (L(W), L(Z)) = sup
A
|P(W ∈A) −P(Z ∈A)|
= (1/2) sup
|h|=1
|Eh(W) −Eh(Z)|
≤(1 ∧λ−1)
n

i=1
p2
i 
(1.2.2)
where dT V denotes the total variation distance. It is known that the
absolute constant 1 is best possible and the factor (1 ∧λ−1) has the
correct order for both small and large values of λ. The signiﬁcance of
the factor (1 ∧λ−1) is explained in Chapter 1 of Barbour, Holst, and
Janson (1992).
To obtain a nonuniform bound, we let
Ai(r) = P(W (i) = r)
P(Z = r) .
Then (1.2.1) can be rewritten as
Eh(W) −Eh(Z) =
n

i=1
p2
i EAi(Z)△fh(Z + 1)
(1.2.3)
where h is no longer assumed to be bounded.
Let C∗= sup1≤i≤n supr≥0 Ai(r). Then
|Eh(W) −Eh(Z)| ≤C∗
 n

i=1
p2
i

E|△fh(Z + 1)|.
What remains to be done is to calculate or bound C∗and E|△fh(Z+1)|.
In Barbour, Chen and Choi (1995), it is shown that for max1≤i≤n pi ≤
1/2, C∗≤4e13/12√π and the following theorem was proved.
©2001 CRC Press LLC

THEOREM 1.2.1
[Theorem 3.1 in Barbour, Chen, and Choi (1995)]
Let h be a real-valued function deﬁned on Z+ such that EZ2|h(Z)| < ∞.
We have
|Eh(W) −Eh(Z)|
≤C∗
n

i=1
p2
i [4(1 ∧λ−1)E|h(Z + 1)| + E|h(Z + 2)|
−2E|h(Z + 1)| + E|h(Z)|]/2.
(1.2.4)
If |h| = 1, then we have
dT V (L(W), L(Z)) = (1/2) sup
|h|=1
|Eh(W) −Eh(Z)| ≤C∗(1 ∧λ−1)
n

i=1
p2
i
where the upper bound has the same order as that of (2.2), but it has a
larger absolute constant. However, the bound in (2.4) allows a very wide
choice of possible functions h, and therefore contains more information
than the total variation distance bound in (2.2).
By iterating (2.1), we obtain
Eh(W) −Eh(Z) =
n

i=1
p2
i E△fh(Z + 1) + second order terms
= −1
2
n

i=1
p2
i E△2h(Z) + second order terms
where E∆fh(Z + 1) = −(1/2)E∆2h(Z) (see, for example, Chen and
Choi (1992), p.1871).
In Barbour, Chen, and Choi (1995), a more reﬁned result (Theorem
3.2) was obtained by bounding the second order error terms in the same
way the ﬁrst order error terms were bounded. From this theorem, a large
deviation result (Theorem 4.2) was proved which produces the following
corollary.
COROLLARY 1.2.2
Let z = λ + ξ
√
λ. Suppose max1≤i≤n pi →0 and ξ = o

[λ/ n
i=1 p2
i ]1/2
as n →∞. Then, as n, z and ξ →∞,
P(W ≥z)
P(Z ≥z) −1 ∼−ξ2
2λ
n

i=1
p2
i .
©2001 CRC Press LLC

The following asymptotic result was also deduced.
THEOREM 1.2.3
Let N be a standard normal random variable. Let h be a nonnegative
function deﬁned on R which is continuous almost everywhere and not
identically zero. Suppose that

( Z−λ
√
λ )4h( Z−λ
√
λ ) : λ ≥1

is uniformly in-
tegrable. Then as λ →∞such that max1≤i≤n pi →0,
∞

r=0
h(r −λ
√
λ
)|P(W = r) −P(Z = r)| ∼1
2λ(
n

i=1
p2
i )E|N 2 −1|h(N).
By letting h ≡1, E|N 2 −1|h(N) = E|N 2 −1| = 2
	
2/(πe), and
Theorem 2.3 yields a result of Barbour and Hall (1984a, p. 477) and
Theorem 1.2 of Deheuvels and Pfeifer (1986).
Nonuniform bounds in compound Poisson approximation on a group
for small and moderate λ were ﬁrst obtained by Chen (1975b) and later
generalized and reﬁned by Chen and Roos (1995). In these papers, the
techniques were inspired by Stein’s method. The ﬁrst paper on com-
pound Poisson approximation using Stein’s method directly was by Bar-
bour, Chen, and Loh (1992).
1.3
Binomial Approximation: Binary Expansion of
a Random Integer
In his monograph, Stein (1986) considered the following problem. Let
n be a natural number and let X denote a random variable uniformly
distributed over the set {0, 1, , n −1}.
Let W denote the number of
ones in the binary expansion of X and let Z be a binomial random
variable with parameters (k, 1/2), where k is the unique integer such
that 2k−1 < n ≤2k. If n = 2k, then W has the same distribution as Z,
otherwise it is a sum of dependent indicator random variables.
By using the solution of the Stein equation
(k −x)f(x) −xf(x −1) = h(x) −Eh(Z)
(1.3.1)
where h = I{r} and r = 0, 1, . . . , k, Stein (1986) proved that
sup
0≤r≤k
|P(W = r) −P(Z = r)| ≤4/k.
Diaconis (1977), jointly with Stein, proved a normal approximation re-
sult for W with an error bound of order 1/
√
k. A combination of this
©2001 CRC Press LLC

result with the normal approximation to the binomial distribution shows
that sup0≤r≤k |P(W ≤r) −P(Z ≤r)| is of the order of 1/
√
k.
Loh (1992) obtained a bound on the solution of a multivariate version
of (3.1) using the probabilistic approach of Barbour (1988). Using this
result of Loh and arguments in Stein (1986), we can obtain a bound of
order 1/
√
k on the total variation distance between L(W) and L(Z).
In an unpublished work of Chen and Soon (1994) which was based
on the Ph.D. dissertation of the latter, the method of obtaining nonuni-
form bounds in Poisson approximation was applied to the approximation
of L(W) by L(Z). Apart from proving other results, this work shows
that the total variation distance between L(W) and L(Z) is, in many
instances, of much small order than 1/
√
k.
Let X = k
i=1 Xi2k−i for the binary expansion of X and W =
k
i=1 Xi. In Stein (1986, pp. 44–45), it is shown that
Eh(W) −Eh(Z) = EQfh(W)
(1.3.2)
where Q = |{j : Xj = 0 or X +2k−j ≥n}| and fh is the solution of (3.1)
with h being a real-valued function deﬁned on {0, 1, . . . , k}. Deﬁne
ψ(r) = E[Q|W = r]
and
A(r) = P(W = r)
P(Z = r) .
Then (3.2) can be written as
Eh(W) −Eh(Z) = Eψ(Z)A(Z)fh(Z).
(1.3.3)
Let lk be the number of consecutive 1s, starting from the beginning in
the binary expansion of n −1. The relationship between n −1 and lk is
given by
n −1 =
lk

i=1
2k−i + m
where 0 ≤m < 2k−lk−1. It is shown in Chen and Soon (1994) that for
0 ≤r ≤k −1, lk/k ≤A(r) ≤2. By obtaining upper and lower bounds
on the right hand side of (3.3), the following theorem was proved.
THEOREM 1.3.1
Assume that 2k−1 < n < 2k.
(i) If limk→∞
lk
√
k = ∞, then
dT V (L(W), L(Z)) ≍2−lk.
(ii) If lim supk→∞
lk
√
k < ∞, then
dT V (L(W), L(Z)) ≍2−lk lk
√
k
©2001 CRC Press LLC

where xk ≍yk means that there exist positive constants a < b such that
a ≤xk/yk ≤b for suﬃciently large k.
From this theorem it follows that
dT V (L(W), L(Z)) ≍
1
√
k
if and only if
0 < lim inf
k→∞lk ≤lim sup
k→∞
lk < ∞.
The following theorems were also proved.
THEOREM 1.3.2
|Eh(W) −Eh(Z)|
≤13
√
k
E


Z −[k/2] −1
	
k/4
 (|h(Z)| + |h(Z + 1)| + 2|Eh(Z)|)

.
THEOREM 1.3.3
Let a = [k/2] + bk where bk/
√
k →∞and bk/k →0 as k →∞. If lk = l
for all suﬃciently large k, then
P(W ≥a)
P(Z ≥a) −1 ∼−2ψ
k
2
 bk
k
as k →∞, where l(1/2 −(l −1)/[2(k −l + 1)])l+1 < ψ([k/2]) ≤3.
Theorem 3.3 is in fact a corollary of a more general large deviation
theorem.
1.4
Normal Approximation
Let X1, . . . , Xn be independent random variables with EXi = 0, var(Xi)
= σ2
i , E|Xi|3 = γi < ∞and n
i=1 σ2
i = 1. Let F be the distribution
function of n
i=1 Xi and let Φ be the standard normal distribution func-
tion. The Berry-Esseen Theorem states that
sup
−∞<x<∞|F(x) −Φ(x)| ≤C
n

i=1
γi
©2001 CRC Press LLC

where C is an absolute constant. The smallest value of C, obtained so
far by Van Beek (1972) (without using computers), is 0.7975.
If X1, . . . , Xn are independent and identically distributed, then
sup
−∞<x<∞|F(x) −Φ(x)| ≤Cnγ
where γ = γi for i = 1, . . . , n. Nonuniform bounds were ﬁrst obtained
by Esseen (1945) who proved that for the i.i.d. case
|F(x) −Φ(x)| ≤
λ log n
√n(1 + x2)
and
|F(x) −Φ(x)| ≤λ log(2 + |x|)
√n(1 + x2)
where λ depends on n3/2γ. Nagaev (1965) improved the upper bounds
to Cnγ/(1+|x|3), also for the i.i.d. case. This was generalized by Bikelis
(1966) who proved that, for independent and not necessarily identically
distributed random variables,
|F(x) −Φ(x)| ≤C n
i=1 γi
1 + |x|3
where C is an absolute constant. Paditz (1977) calculated C to be 114.7
and Michel (1981) reduced it to 30.54 for the i.i.d. case. All the above
proofs used the Fourier analytic method. Chen and Shao (2000) used
Stein’s method to prove the following more general result:
|F(x)−Φ(x)| ≤C
n

i=1
EX2
i I(|Xi| > 1 + |x|)
(1 + |x|)2
+E|Xi|3I(|Xi| ≤1 + |x|)
(1 + |x|)3

where the existence of third moments is no longer assumed. Their proof
is based on truncation and the concentration inequality approach. The
concentration inequality approach was originally used by Stein for the
i.i.d. case (see Ho and Chen (1978)). It was extended by Chen (1986)
to dependent and non-identically distributed random variables with ar-
bitrary index set. A proof of the Berry-Esseen Theorem for independent
and non-identically distributed random variables using the concentration
inequality approach is given in Section 2 of Chen (1998).
The concentration inequality approach is not the only approach for
obtaining Berry-Esseen bounds using Stein’s method. Another approach
based on inductive arguments has been used by Barbour and Hall (1984b),
Bolthausen (1984) and Stroock (1993).
We would like to mention in passing that Stein’s method has also been
applied to obtain bounds on the total variation distances between the
standard normal distribution and distributions satisfying certain varia-
tional inequalities. See Utev (1989) and Cacoullos, Papathanasiou, and
Utev (1994).
©2001 CRC Press LLC

1.5
Conclusion
We would like to conclude by saying that there is much more to be done
in the direction of nonuniform bounds, particularly for dependent ran-
dom variables both in Poisson approximation and normal approximation.
The large deviation results referred to in the above sections are actually
those of moderate deviation. A related question therefore is how Stein’s
method can be applied to obtain results which cover both moderate and
really large deviations.
Acknowledgement
This work is partially supported by grant
RP3982719 at the National University of Singapore.
I would like to
thank K. P. Choi and Qi-Man Shao for their help in preparing the
manuscript and for their helpful comments.
References
1. Arratia, R., Goldstein, L., and Gordon, L. (1990). Poisson approxi-
mation and the Chen-Stein method. Statistical Science 5, 403–434.
2. Barbour, A. D. (1988). Stein’s method and Poisson process con-
vergence. Journal of Applied Probability 25 (A), 175–184.
3. Barbour, A. D., Chen, L. H. Y., and Choi, K. P. (1995). Poisson
approximation for unbounded functions, I: independent summands.
Statistica Sinica 5, 749–766.
4. Barbour, A. D., Chen, L. H. Y., and Loh, W. L. (1992). Com-
pound Poisson approximation for nonnegative random variables
via Stein’s method. Annals of Probability 20, 1843–1866.
5. Barbour, A. D. and Eagleson, G. (1983). Poisson approximation for
some statistics based on exchangeable trials. Advances in Applied
Probability 15, 585–600.
6. Barbour, A. D. and Hall, P. (1984a). On the rate of Poisson conver-
gence. Mathematical Proceedings of the Cambridge Philosophical
Society 95, 473–480.
7. Barbour, A. D. and Hall, P. (1984b). Stein’s method and the Berry-
Esseen theorem. The Australian Journal Statistics 26, 8–15.
8. Barbour, A. D., Holst, L., and Janson, S. (1992).
Poisson Ap-
proximation.
Oxford Studies in Probability 2, Clarendon Press,
Oxford.
9. Bikelis, A. (1966). Estimates of the remainder in the central limit
©2001 CRC Press LLC

theorem. Litovsk. Mat. Sb. 6(3), 323–346 (in Russian).
10. Bolthausen, E. (1984). An estimate of the remainder in a combina-
torial central limit theorem. Zeitschrift Wahrscheinlichkeitstheorie
und Verwandte Gebiete 66, 379–386.
11. Cacoullos, T., Papathanasiou, V. and Utev, S. A. (1994). Varia-
tional inequalities with examples and an application to the central
limit theorem. Annals of Probability 22, 1607–1618.
12. Chen, L. H. Y. (1975a). Poisson approximation for dependent tri-
als. Annals of Probability 3, 534–545.
13. Chen, L. H. Y. (1975b). An approximation theorem for convolu-
tions of probability measures. Annals of Probability 3, 992–999.
14. Chen, L. H. Y. (1986). The rate of convergence in a central limit
theorem for dependent random variables with arbitrary index set.
IMA Preprint Series #243, University of Minnesota.
15. Chen, L. H. Y. (1993). Extending the Poisson approximation. Sci-
ence 262, 379–380.
16. Chen, L. H. Y. (1998). Stein’s method: some perspectives with
applications. Probability Towards 2000 (Eds., L. Accardi and C.
Heyde), pp. 97–122. Lecture Notes in Statistics No. 128. Springer
Verlag.
17. Chen, L. H. Y. and Choi, K. P. (1992). Some asymptotic and large
deviation results in Poisson approximation. Annals of Probability
20, 1867–1876.
18. Chen, L. H. Y. and Roos, M. (1995). Compound Poisson approx-
imation for unbounded functions on a group, with application to
large deviations. Probability Theory and Related Fields 103, 515–
528.
19. Chen, L. H. Y. and Shao, Q. M. (2000). A non-uniform Berry-
Esseen bound via Stein’s method. Preprint.
20. Chen, L. H. Y. and Soon, S. Y. T. (1994).
On the number of
ones in the binary expansion of a random integer. Unpublished
manuscript.
21. Deheuvels, P. and Pfeifer, D. (1986). A semigroup approach to
Poisson approximation. Annals of Probability 14, 663–676.
22. Diaconis, P. (1977). The distribution of leading digits and uniform
distribution mod 1. Annals of Probability 5, 72–81.
23. Esseen, C.-G. (1945). Fourier analysis of distribution functions: a
mathematical study of the Laplace-Gaussian law. Acta Mathemat-
ica 77 1–125.
©2001 CRC Press LLC

24. Ho, S. T. and Chen, L. H. Y. (1978). An Lp bound for the remain-
der in a combinatorial central limit theorem. Annals of Probability
6, 231–249.
25. Loh, W. L. (1992). Stein’s method and multinomial approximation.
Annals of Applied Probability 2, 536–554.
26. Michel, R. (1981). On the constant in the non-uniform version of
the Berry-Esseen Theorem. Zeitschrift Wahrscheinlichkeitstheorie
und Verwandte Gebiete 55, 109–117.
27. Nagaev, S. V. (1965). Some limit theorems for large deviations.
Theory of Probability and its Applications 10, 214–235.
28. Paditz, L. (1977). ¨Uber die Ann¨aherung der Verteilungsfunktionen
von Summen unabh¨angiger Zufallsgr¨oben gegen unberrenzt teil-
bare
Verteilungsfunktionen
unter
besonderer
berchtung
der
Verteilungsfunktion der standarddisierten Normalverteilung. Dis-
sertation, A.TU Dresden.
29. Soon, S. Y. T. (1993). Some Problems in Binomial and Compound
Poisson Approximations. Ph.D. dissertation, National University
of Singapore.
30. Stein, C. (1972).
A bound for the error in the normal approx-
imation to the distribution of a sum of dependent random vari-
ables.
Proceedings of the Sixth Berkeley Symposium on Mathe-
matics, Statistics and Probability 2, 583–602, University California
Press. Berkeley, California.
31. Stein, C. (1986).
Approximation Computation of Expectations.
Lecture Notes 7, Institute of Mathematics and Statistics, Hayward,
California.
32. Stroock, D. W. (1993).
Probability Theory: An Analytic View.
Cambridge University Press, Cambridge, U.K.
33. Utev, S. A. (1989). Probability problems connected with a certain
integrodiﬀerential inequality. Siberian Mathematics Journal
30,
490–493.
34. Van Beek, P. (1972).
An approximation of Fourier methods to
the problem of sharpening the Berry-Esseen inequality. Zeitschrift
Wahrscheinlichkeitstheorie und Verwandte Gebiete 23, 187–196.
©2001 CRC Press LLC

2
Probability Inequalities for Multivariate
Distributions with Applications to
Statistics
Joseph Glaz
University of Connecticut, Storrs, CT
ABSTRACT In this article we review positive and negative depen-
dence structures for multivariate distributions. We present conditions
under which product-type inequalities hold for the cumulative distribu-
tion function and the survival function of a sequence of random variables.
When these conditions are not satisﬁed, we survey the Bonferroni-type
inequalities that are often used. Applications to sequential inference,
simultaneous inference, testing for randomness and outlier detection are
discussed. Numerical results are presented to evaluate the performance
of the inequalities discussed in this article.
Keywords and phrases Association,
Bonferroni-type
inequalities,
moving sums, negative dependence, product-type inequalities, scan statis-
tic
2.1
Introduction and Summary
In this article probability inequalities for multivariate distributions used
in statistical inference are discussed. In Section 2.2 several positive de-
pendence structures for multivariate distributions are deﬁned and con-
ditions are given for the validity of product-type inequalities of order
k ≥1 for the distribution and survival functions of a sequence of ran-
dom variables.
In Section 2.3 only negative dependence structures most relevant to
the development of product-type inequalities are mentioned.
In Section 2.4 a brief introduction to Bonferroni-type inequalities is
presented. The emphasis is on the inequalities that are used in the ap-
plications discussed in Section 2.5. For more information on this subject
©2001 CRC Press LLC

the reader is referred to the book by Galambos and Simonelli (1996).
In Section 2.5 we discuss ﬁve applications that use the inequalities that
have been presented in Sections 2.2–2.4. The ﬁrst application deals with
deriving an accurate upper bound for the tail probability of a stopping
time associated with a sequential test. Based on this bound an accurate
bound for the expected stopping time is obtained. Both bounds play an
important role in sequential inference [Glaz and Kenyon (1995)].
A discrete scan statistic is the topic of the second application. Discrete
scan statistics have been used extensively in various areas of science and
technology to test the null hypotheses that the observations are identi-
cally distributed against a clustering alternative. For current research in
this area and references for the applications see Glaz and Balakrishnan
(1999). In this article numerical results are given to evaluate the in-
equalities developed in Glaz and Naus (1991) and Chen and Glaz (1995)
for tail probabilities and expected values of a discrete scan statistic for
0-1 Bernoulli trials.
The third application discusses high order inequalities for the distri-
bution function of a multinomial random vector.
Both product-type
and Bonferroni-type inequalities are used. These inequalities are spe-
cialized to approximate the distribution of the largest order statistic.
Applications to testing for randomness and outlier detection are brieﬂy
discussed.
The fourth application is about approximating the distribution of a
conditional discrete scan statistic for 0-1 Bernoulli trials. Bonferroni-
type inequalities are used to accomplish this task. Based on these in-
equalities, quite accurate inequalities for the expected size of the scan
statistic are obtained.
The last application presents simultaneous prediction intervals in time
series models. To achieve a prescribed conﬁdence level for data modeled
by an AR(1) model both product-type and Bonferroni-type inequalities
are used. Simultaneous conﬁdence intervals for more general time series
models are discussed in Glaz and Ravishanker (1991) and Ravishanker,
Wu, and Glaz (1991).
©2001 CRC Press LLC

2.2
Positive Dependence and Product-Type
Inequalities
A sequence of random variables X1, ...., Xn or its distribution function
is said to be positive lower orthant dependent (PLOD) if for all xi
P(X1 ≤x1, ...., Xn ≤xn) ≥
n

i=1
P(Xi ≤xi).
(2.2.1)
It is said to be positive upper orthant dependent (PUOD) if for all xi
P(X1 > x1, ...., Xn > xn) ≥
n

i=1
P(Xi > xi).
(2.2.2)
It is known that for n = 2 PLOD and PUOD are equivalent, while for
n ≥3 this fact is not true anymore [Ahmed et al. (1978) and Lehmann
(1966)]. If X1, ...., Xn are both PLOD and PUOD we will say that they
are positive orthant dependent (POD). These inequalities have appeared
ﬁrst in the statistical literature in the context of simultaneous testing
[Kimball (1951)] and simultaneous conﬁdence interval estimation [Dunn
(1959), Khatri (1967), Sidak (1967, 1971), and Scott (1967)].
In general it is quite diﬃcult to verify the validity of PLOD or PUOD
for a given sequence of random variables. To accomplish this task, Esary
et al. (1967) introduced the following concept of positive dependence.
A sequence of random variables X1, ...., Xn or its distribution function
is said to be associated, if for all coordinatewise increasing real val-
ued functions of n variables f and g such that E[f(X1, ...., Xn)] and
E[g(X1, ...., Xn)] are ﬁnite
E[f(X1, ...., Xn)g(X1, ...., Xn)] ≥E[f(X1, ...., Xn)]E[g(X1, ...., Xn)].
(2.2.3)
The following theorem is of major importance.
THEOREM 2.2.1
[Esary et al. (1967)]
If X1, ...., Xn are associated then they are POD.
The class of distributions that are associated includes:
multivari-
ate normal distributions with nonnegative correlations [Pitt (1982) and
Joag-Dev et al. (1983)], multivariate exponential distribution [Olkin and
Tong (1994)], absolute value multivariate Cauchy distribution, negative
multinomial, and multivariate logistic [Karlin and Rinott (1980a)].
In some applications the inequalities given in Equations (2.2.1) and
(2.2.2) are not accurate, since they are based only on the univariate
©2001 CRC Press LLC

marginal distributions.
This is especially true if the dependence be-
tween the random variables is strong and n is moderate or large [Glaz
(1990, 1992) and Glaz and Johnson (1984, 1986)]. We introduce below
concepts of positive dependence that lead to more accurate product-type
inequalities that are based on k dimensional marginal distributions. We
will say that these product-type inequalities are of order k.
A sequence of random variables X1, ...., Xn is said to be sub-Markov
with respect to a sequence of intervals I1, ...., In if for any 1 ≤i ≤k ≤n
P

Xk ∈Ik|
k−1

j=1
(Xj ∈Ij)

≥P

Xk ∈Ik|
k−1

j=i
(Xj ∈Ij)

,
(2.2.4)
where k−1
j=k(Xj ∈Ij) is to be interpreted as the entire space [Glaz
(1990)].
It is easy to verify that if X1, ...., Xn are sub-Markov with
respect to inﬁnite intervals of the same type, i.e. Ij = (−∞, bj], 1 ≤j ≤
n, or Ij = [aj, ∞), 1 ≤j ≤n, (SM with respect to ∞intervals), then
they are associated. It turns out that the sub-Markov property is exactly
what we need to establish the product-type inequalities presented below
[Glaz (1990) and Glaz and Johnson (1984)]. Since it is quite diﬃcult to
establish the sub-Markov property for a sequence of random variables
the following concepts of positive dependence play a major role.
A nonnegative real-valued function of two variables, f(x, y), is totally
positive of order two, TP2, if for all x1 < x2 and y1 < y2
f(x1, y1)f(x2, y2) ≥f(x1, y2)f(x2, y1)
[Karlin (1968)].
A nonnegative real-valued function of n variables,
f(x1, ...., xn), is multivariate totally positive of order two, MTP2, if for
all x = (x1, ...., xn) and y = (y1, ...., yn)
f(x ∨y)f(x ∧y) ≥f(x)f(y),
where
x ∨y = (max{x1, y1}, ...., max{xn, yn})
and
x ∧y = ( min{x1, y1}, ...., min{xn, yn})
[Karlin and Rinott (1980a)]. A nonnegative real-valued function of n
variables, f(x1, ...., xn), is totally positive of order two in pairs, TP2 in
©2001 CRC Press LLC

pairs, if for any pair of arguments xi and xj, the function f viewed as a
function of xi and xj only, with the other arguments kept ﬁxed, is TP2
[Barlow and Proschan (1975)].
If f is MTP2 then it is also TP2 in pairs.
But, if the support of
f is a product space then f is MTP2 if and only if it is TP2 in pairs
[Kemperman (1977) and Block and Ting (1981)]. We say that a sequence
of random variables X1, ...., Xn is MTP2 (resp. TP2 in pairs) if its joint
density function is MTP2 (resp. TP2 in pairs). We will assume that the
support of the joint density functions is a product space.
The following results is often used to show that X1, ...., Xn are asso-
ciated.
THEOREM 2.2.2
[Sarkar (1969)]
If X1, ...., Xn are MTP2 then they are associated.
The above result is especially useful in determining when |X1|, ...., |Xn|
are associated, as it is often easier to verify that |X1|, ...., |Xn| are MTP2.
Remark It is still an open problem to characterize when |X1|, ...., |Xn|
are associated, if X1, ...., Xn have a multivariate normal distribution.
The following interesting examples of MTP2 random variables are
listed in Karlin and Rinott (1980a): multivariate normal with nonneg-
ative partial correlations, absolute value multivariate normal with only
nonnegative oﬀ-diagonal elements in −DΣ−1D, where Σ is the covari-
ance matrix and D is some diagonal matrix with elements ±1, multivari-
ate logistic, absolute value multivariate Cauchy, negative multinomial,
certain classes of multivariate t, certain classes of multivariate gamma,
certain classes of partial sums of i.i.d. random variables, and order statis-
tics of i.i.d. random variables.
The following result gives suﬃcient conditions for the validity of
product-type inequalities of order k.
THEOREM 2.2.3
[Glaz and Johnson (1984)]
Let X1, ...., Xn be MTP2 random variables and I1, ...., In be inﬁnite in-
tervals, all of the same type.
Then
X1, ...., Xn are sub-Markov with
respect to I1, ...., In and
P(X1 ∈I1, ...., Xn ∈In) ≥γk ≥γ1
where
©2001 CRC Press LLC

γk = P(X1 ∈I1, ...., Xk ∈Ik)
×
n

i=k+1

P (Xi−k+1 ∈Ii−k+1, ...., Xi ∈Ii)
P(Xi−k+1 ∈Ii−k+1, ...., Xi−1 ∈Ii−1)
	
(2.2.5)
is an increasing function in k.
Note that γ1 is the product-type inequality given by Equation (2.2.1)
or (2.2.2). If the conditions of Theorem 2.3 are satisﬁed, γk, the kth
order product-type inequality is tighter than γ1. For k = 2 we get from
Equation (2.2.5):
γ2 = P (X1 ∈I1, X2 ∈I2)
n

i=3
P (Xi−1 ∈Ii−1, Xi ∈Ii)
P (Xi−1 ∈Ii−1)
	
.
(2.2.6)
Also, if X1, ...., Xn are stationary and Ii = I then
γk = P(X1 ∈I, ...., Xk ∈I)
 P(X1 ∈I, ...., Xk ∈I)
P(X1 ∈I, ...., Xk−1 ∈I)
	n−k
.
(2.2.7)
In Section 5 the performance of these product-type bounds will be
evaluated for several applications.
Other concepts of positive dependence imply the validity of the product-
type bounds given in Equation (2.2.5). We will mention here two such
concepts.
A sequence of random variables X1, ...., Xn or its distribution func-
tion is said to be right corner set increasing (RCSI) or left corner set
decreasing (LCSD) if for all 1 ≤i ≤n and xi,
P(Xi > xi|Xj > xj, 1 ≤j ≤n, j ̸= i)
is an increasing function in all the xj’s, or for all 1 ≤i ≤n and xi, or
P(Xi ≤xi|Xj ≤xj, 1 ≤j ≤n, j ̸= i)
is a decreasing function in all the xjs, respectively [Harris (1970)]. We
will say that X1, ...., Xn are positive corner set dependent (PCSD) if they
are both RCSI and LCSD.
A sequence of random variables X1, ...., Xn or its distribution function
is said to be right tail increasing in sequence (RTIS) or left tail decreasing
in sequence (LTDS) if for all 1 ≤i ≤n and xi,
©2001 CRC Press LLC

P(Xi > xi|Xj > xj, 1 ≤j ≤i −1)
is an increasing function in x1, ...., xi−1 or for all 1 ≤i ≤n and xi, or
P(Xi ≤xi|Xj ≤xj, 1 ≤j ≤i −1)
is a decreasing function in x1, ...., xi−1, respectively [Esary and Proschan
(1972)]. We will say that X1, ...., Xn are positive tail sequence depen-
dent (PTSD) if they are both RTIS and LTDS. Properties and rela-
tionships among these and other related notions of positive dependence
have been investigated be many authors, including: Alam and Wallenius
(1976), Barlow and Proschan (1975), Block and Ting (1981), Brindley
and Thompson (1972), and Glaz (1990).
The following relationships among the positive dependence structures
presented above hold:
MTP2 =⇒PCSD =⇒PTSD =⇒SM∞intervals
=⇒Associated =⇒POD.
It will be interesting to study in more detail the diﬀerences among these
dependence structures and try to characterize them for certain classes of
multivariate distributions. Investigations of these dependence structures
for sequences of random vectors and stochastic processes are of interest
as well.
Let X1, ...., Xn be a sequence of independent random variables. The
sequence of moving sums {
m+i−1
j=i
Xj}n−m+1
i=1
is POD and therefore
the product-type inequalities given in Equations (2.2.1) and (2.2.2) hold.
The moving sums of independent random variables are not MTP2 and
one can show that the product-type inequalities of order k ≥2 given in
Equations (2.2.5) do not hold in this case [Glaz and Johnson (1988)]. For
a sequence of i.i.d. random variables Glaz and Naus (1991, 1996) have
used a diﬀerent approach to derive accurate product-type inequalities for
a sequence of moving sums. We present here only one of the results for a
sequence of i.i.d. integer valued random variables. For 2 ≤m ≤n1 ≤n,
let
G(n1) = P


max
1≤i≤n1−m+1
m+i−1

j=i
Xj ≤k −1

.
(2.2.8)
THEOREM 2.2.4
[Glaz and Naus (1991)]
Let X1, ...., Xn be a sequence of i.i.d. integer valued random variables.
Then for n ≥3m,
©2001 CRC Press LLC

G(3m)

1 + G(2m)−G(2m−1)
G(3m−1)
n−3m
≤G(n) ≤G(3m) [1 −G(3m) + G(3m −1)]n−3m .
(2.2.9)
An application of these inequalities to a discrete scan statistic is pre-
sented in Section 2.5.
2.3
Negative Dependence and Product-Type
Inequalities
In this section only the most relevant dependence structures for con-
structing high order product-type inequalities will be presented. In the
statistical literature many interesting negative dependence structures for
multivariate distributions have been discussed. In fact, most of these
negative dependence structures are deﬁned by reversing the inequalities
given in Section 2.2 [Block et al. (1982), Ebrahimi and Ghosh (1981,
1984), Glaz (1990), Kim and Seo (1995) and Lee (1985)].
A nonnegative real-valued function of two variables, f(x, y), is reverse
rule of order two, RR2, if for all x1 < x2 and y1 < y2
f(x1, y1)f(x2, y2) ≤f(x1, y2)f(x2, y1)
[Karlin (1968)].
A nonnegative real-valued function of n variables,
f(x1, ...., xn), is multivariate reverse rule of order two, MRR2, if for
all x = (x1, ...., xn) and y = (y1, ...., yn)
f(x ∨y)f(x ∧y) ≤f(x)f(y).
[Karlin and Rinott (1980b)]. If the domain of the function f is a product
space then MRR2 is equivalent to f being RR2 in each pair of the
arguments, while the rest of the arguments are kept ﬁxed. An MRR2
function has many nice properties, except that the MRR2 property is
not preserved under composition. Therefore if f is a density function of a
random vector and it is MRR2, the marginal densities are not necessarily
MRR2. In fact they can be even positively dependent [Karlin and Rinott
(1980b, p. 510)]. To eliminate this defect, the following stronger concept
of negative dependence is deﬁned in Karlin and Rinott (1980b).
©2001 CRC Press LLC

A density function f of a random vector X = (X1, ...., Xn) is said to
be strongly MRR2, S −MRR2, if it is MRR2 and for any set of PF2
functions {ϕm} (a function ϕ deﬁned on (−∞, ∞) is PF2 if ϕ(x −y) is
TP2 in the variables −∞< x, y < ∞), the marginal density functions
g(xm1, ...., xmk) = f(x1, ...., xn)
n−k

i=1
ϕi(xji)dxji
(2.3.1)
are MMR2 in the variables
{xm1, ...., xmk} = {x1, ...., xn} \

xj1, ...., xjn−k

.
The class of distributions with S −MRR2 density functions includes:
multinomial, multivariate hypergeometric, multivariate Hahn, Dirichlet
family of densities on a simplex, multinormal with special covariance
structure [Karlin and Rinott (1980b)].
A sequence of random variables X1, ...., Xn is said to be super-Markov
with respect to a sequence of intervals I1, ...., In if for any 1 ≤i ≤k ≤n
P

Xk ∈Ik|
k−1

j=1
(Xj ∈Ij)

≤P

Xk ∈Ik|
k−1

j=i
(Xj ∈Ij)

,
(2.3.2)
where k−1
j=k(Xj ∈Ij) is to be interpreted as the entire space [Glaz
(1990)]. For k ≥1 let γk be deﬁned in Equation (2.2.5). The following
result establishes the product-type inequalities.
THEOREM 2.3.1
[Glaz and Johnson (1984)]
Let X1, ...., Xn be S −MRR2 random variables and I1, ...., In be inﬁnite
intervals, all of the same type. Then X1, ...., Xn are super-Markov with
respect to I1, ...., In and
P(X1 ∈I1, ...., Xn ∈In) ≤γk ≤γ1,
where γk are decreasing in k.
In Section 2.5 the performance of these product-type bounds will be
evaluated for several applications.
2.4
Bonferroni-Type Inequalities
The following classical Bonferroni inequalities for the probability of a
union of n events have been derived in Bonferroni (1936). Let A1,....,
©2001 CRC Press LLC

An be a sequence of events and let A = ∪n
i=1Ai. Then for 2 ≤k ≤n,
k

j=1
(−1)j−1Uj ≤P(A) ≤
k−1

j=1
(−1)j−1Uj
(2.4.1)
where k is an even integer and for 1 ≤j ≤n,
Uj =

1≤i1<....<ij≤n
P

j
m=1
Aim

.
(2.4.2)
The orders of these inequalities is given by the upper index of the sums in
Equation (2.4.1). The ﬁrst order Bonferroni upper bound is referred to
in the statistical literature as Boole’s inequality, derived in 1854 (Boole
1984). These classical lower (upper) Bonferroni inequalities have a short-
coming of not necessarily increasing (decreasing) with the increase of
the order of the inequality.
Moreover, these inequalities can be wide
for a low order and the computational complexity increases rapidly with
the increase of the order of the inequalities. Therefore attempts have
been made to derive simpler inequalities that are more accurate than
the classical Bonferroni inequalities. We refer to these inequalities as
Bonferroni-type inequalities.
In this article we make use of the following class of upper Bonferroni-
type inequalities proposed in Hunter (1976). The basic idea of this ap-
proach is to represent the event A = ∪n
i=1Ai as a union of disjoint event
A = A1 ∪



n

i=2

Ai ∩


i−1

j=1
Ac
j






.
Therefore,
P(A) = P(A1) +
n

i=2
P

Ai ∩


i−1

j=1
Ac
j



.
For 2 ≤k ≤n −1,
P(A) ≤P(A1) +
k

i=2
P

Ai ∩


i−1

j=1
Ac
j




+
n

i=k+1
P

Ai ∩


i−1

j=i−k+1
Ac
j



.
©2001 CRC Press LLC

This inequality can be rewritten for k = 2 as
P(A) ≤U1 −
n

i=2
P(Ai ∩Ai−1)
(2.4.3)
and for k ≥3 as
P(A) ≤U1 −
n

i=2
P(Ai ∩Ai−1)
−
k−1

j=2
n−j

i=1
P


Ai ∩




i+j−1

j=i+1
Ac
j

∩Ai+j




.
(2.4.4)
Note that these upper inequalities are of order k and they decrease as
the order of the inequalities is increasing.
The inequality (2.2.3) is a member of a class of second order inequali-
ties derived in Hunter (1976). The following concepts from graph theory
have to be introduced. A graph G(V, E), or brieﬂy G, is a combinatorial
structure comprised of a set of vertices V and a set of edges E. Each
edge is associated with two vertices referred to as the end points of the
edge. The graph G is called undirected if both endpoints of an edge have
same relationship for each of the edges. A path is a sequence of edges
such that two consecutive edges in it share a common end point. A tree
is an undirected graph such that there is a unique path between every
pair of its vertices. Let v1, ...., vn be the vertices of a tree T, representing
the events A1,...., An , respectively. The vertices vi and vj are joined by
an edge eij if and only if Ai ∩Aj ̸= φ. The following inequality has been
derived in Hunter (1976):
P(A) ≤S1 −

{(i,j);eij∈T }
P(Ai ∩Aj).
(2.4.5)
The optimal inequality in the class of inequalities (2.2.3Opt) can be
obtained via an algorithm of Kruskal [Worsley (1982)]. The inequality
(2.4.3) is the most stringent within the class of inequalities (2.4.5) if the
events A1,...., An are exchangeable or if they are ordered in such a way
that for 1 ≤i1 < i2 < n, P(Ai1 ∩Ai2) is maximized for i1 −i2 = 1
(Worsley 1982, Examples 3.1 and 3.2).
Inequality (2.4.4) is a member of the following class of inequalities
investigated in Hoover (1990):
P(A) ≤P
 k
i=1
Ai

+
n

j=k+1
P


Aj ∩


m∈Tj
Ac
m




,
(2.4.6)
©2001 CRC Press LLC

where Tj is a subset of {1, ...., j −1} of size k−1 and j ≥k+1. For k ≥3
there is no eﬃcient algorithm to obtain the optimal inequality in the class
of inequalities (2.4.6). If the events A1,...., An are naturally ordered in
such a way that P(∩k−1
j=1Aij) is maximized for ij −ij−1 = 1, 2 ≤j ≤k−1
and 3 ≤k ≤n −1, the natural ordering with Tj = {j −1, ...., j −k + 1}
is recommended. In this case, inequalities (2.4.6) reduce to inequalities
(2.4.4).
If the events A1,...., An are exchangeable inequalities (2.4.4)
have the following simpliﬁed form
P(A) ≤nP(A1) −(n −1)P(A1 ∩A2)
−
k−1

j=2
(n −j)P

A1 ∩
 j
i=2
Ac
i

∩Aj+1
 
.
(2.4.7)
It is tedious but quite routine to verify that the Bonferroni-type in-
equalities (2.4.4) and (2.4.7) for P(∩n
i=1Ac
i) = 1 −P(∪n
i=1Ai) are given
by
P
 n

i=1
Ac
i

≥
n−k+1

i=1
αi,i+k−1 −
n−k

i=1
αi+1,i+k−1 = δk
(2.4.8)
and
P
 n

i=1
Ac
i

≥(n + k −1)α1,k −(n −k)α2,k,
(2.4.9)
respectively, where
αm,n = P
 n

i=m
Ac
i

.
In some examples it is easier to evaluate the terms αm,n, in which
case inequalities (2.4.8) or (2.4.9) will be used.
Many other interest-
ing Bonferroni-type inequalities for the union of n events are discussed
in a recent book by Galambos and Simonelli (1996).
The following lower Bonferroni-type inequality for P(∪n
i=1Ai) had
been derived in Dawson and Sankoﬀ(1967). Let U1 and U2 be given
in Equation (2.4.2) above. Then
P(A) ≥2U1
a
−
2U2
a(a −1),
(2.4.10)
where a is the integer part of 2 + 2U2/U1. Kwerel (1975), using a lin-
ear programming approach, has proved that inequality (2.4.10) is the
optimal inequality in the class of all linear inequalities of the form
©2001 CRC Press LLC

P(A) ≥b1U1 + b2U2.
In particular it outperforms the Bonferroni inequality P(A) ≥S1 −
S2, whenever a > 2. Higher order lower Bonferroni-type inequalities for
P(∪n
i=1Ai) are discussed in Galambos and Simonelli (1996).
2.5
Applications
2.5.1
Sequential Analysis
Let X1, ...., Xn, .... be a sequence of independent and identically dis-
tributed (i.i.d.) normal random variables with unknown mean θ and
known variance σ2. Without loss of generality we assume that σ2 = 1.
For θ0 > 0, we are interested in testing the following hypotheses
H0 : θ ≤θ0 vs H1 : θ > θ0.
Without loss of generality we assume θ0 = 0. The following stopping
time is used in the sequential probability ratio test (SPRT) for testing
the above hypotheses:
τ = inf{n ≥1; Tn /∈In},
where Tn = 
n
i=1 Xi is the sequence of partial sums of i.i.d. normal
random variables, In = (−a/θ1+nθ1/2, a/θ1+nθ1/2) is the continuation
region, and a and θ1 are the design parameters selected to control the
type I and type II probability errors:
β(0) = αandβ(θ1) = 1 −β1,
and β is the power function of the sequential test. [Glaz and Kenyon
(1995)]. It is convenient to transform the continuation region so that it
will be symmetric about the n axis of the (n, Tn) plane. This is accom-
plished by the transformation Yi = Xi −θ/2 and deﬁning
τ = inf{n ≥1; T ∗
n /∈I∗
n},
©2001 CRC Press LLC

where T ∗
n = 
n
i=1 Yi and I∗
n = (−a/θ1, a/θ1). The null hypothesis and
the error rates requirements are transformed to H0 : θ ≤−θ∗and
β(−θ∗) = α, 1 −β(θ∗) = β1, respectively, where θ∗= θ1/2
To sat-
isfy these error rates requirements one uses a = ln(γ/α) −0.583, where
γ is the Laplace transform of the asymptotic distribution of the excess
of the random walk θ1T ∗
n over the boundary evaluated at the value 1
[Woodroofe (1982, Section 3.1)]. We present bounds for P(τ ≥n) and
E(τ), which are of interest in evaluating the performance of sequen-
tial tests.
For references and a more extensive discussion of sequen-
tial tests see Glaz and Johnson (1986) and Glaz and Kenyon (1995).
The following result is central to deriving bounds for P(τ ≥n) and
E(τ) = 
∞
i=1 P(τ ≥n).
THEOREM 2.5.1
[Glaz and Johnson (1986)]
Let X1, ...., Xn, .... be a sequence of independent random variables from
a density function f(x), that has support on an interval. Then for any
n ≥2 the sequence of partial sums Ti = 
i
j=1 Xj, 1 ≤i ≤n, is MTP2
(or TP2 in pairs) if and only if
f(x) = e−ψ(x)
where ψ(x) is a convex function, or equivalently f(x) is a log concave
function.
It follows from this theorem that if Xi, i ≥1, are i.i.d. normal random
variables then the sequence of partial sums {T ∗
i }n
i=1 is MTP2. Therefore
the product-type bounds given in equation (2.2.5) are valid here. Glaz
and Kenyon (1995) developed eﬃcient algorithms to evaluate product-
type inequalities for P(τ ≥n). In Tables 2.1 and 2.2, numerical results
are presented for the SPRT for normal data for selected values of the
parameters.
There are many interesting open problems in this area of research.
For example, development of probability inequalities for stopping times
associated with sequential tests for a vector of parameters.
2.5.2
A Discrete Scan Statistic
Let X1, ...., Xn be a sequence of independent nonnegative integer valued
random variables. A discrete scan statistic, denoted by Sn, is deﬁned in
terms of moving sums of length 2 ≤m ≤n as follows:
Sn =
max
1≤i≤n−m+1
i+m−1

j=i
Xj.
(2.5.1)
©2001 CRC Press LLC

TABLE 2.1
Lower bounds and simulated values of P(τ > n)
α = β1 = .05andθ∗= .5
n
γ3
γ7
P(τ > n)
2
.8273∗
.8273∗
.8276
4
.5130
.5133∗
.5112
6
.3026
.3057∗
.3153
8
.1757
.1812
.1865
10
.1013
.1073
.1098
12
.0581
.0636
.0649
14
.0332
.0376
.0390
16
.0190
.0223
.0239
18
.0108
.0132
.0145
P(τ > n) is a simulated value of
P(τ > n) based on 10, 000 trials.
The starred values are exact.
TABLE 2.2
Lower bounds and simulated values of Eθ(τ)
θ∗= .25
θ∗= .50
α\θ
-.25
-.125
0
-.50
-.25
0
.01
31.39
46.22
59.46
9.05
14.13
19.04
34.55
53.87
72.04
9.28
15.21
21.16
36.16
61.31
84.90
9.23
15.25
21.54
.05
19.51
25.77
29.53
5.62
7.70
8.99
21.38
29.34
34.25
5.69
7.87
9.24
22.32
30.58
36.20
5.72
7.99
9.25
Upper and middle values are lower bounds based on
γ3 and γ7, respectively.
Lower value is simulated based on 10,000 trials.
©2001 CRC Press LLC

TABLE 2.3
Product-type bounds for P(Sn ≥k) Bernoulli model, n = 500
m
p
k
LB
UB
P(k; m, n)
10
.01
2
.3238
.3286
.3269
3
.0159
.0159
.0141
4
.0003
.0003
.0003
.05
2
.9967
.9996
.9992
3
.7274
.7479
.7328
4
.1534
.1544
.1467
5
.0134
.0134
.0141
20
.01
2
.5064
.5299
.5287
3
.0635
.0639
.0624
4
.0038
.0038
.0040
.05
3
.9523
.9857
.9758
4
.6263
.6509
.6476
5
.1964
.1996
.1962
6
.0352
.0353
.0315
7
.0045
.0045
.0058
P(k; m, n) is a simulated value of P(k; m, n)
based on 10,000 trials.
For the special case of i.i.d.
0 −1 Bernoulli trials this scan statistic
generalizes the notion of the longest run of 1s that has been investigated
extensively in the statistical literature. This discrete scan statistic is
used for testing the null hypothesis of the observations being identically
distributed against an alternative hypothesis of clustering, that speciﬁes
an increased occurrence of events in a consecutive subsequence of the
observed data. For an extensive discussion about this scan statistic and
the applications see Glaz and Naus (1991) and Chen and Glaz (1999).
Based on Theorem 2.4, in Table 2.3 we present bounds for P(Sn ≥k)
for a sequence of i.i.d. 0 −1 Bernoulli trials.
2.5.3
An Approximation for a Multinomial Distribution
Let X = (X1, ...., Xm) be a multinomial random vector with cell prob-
abilities p1, ...., pm and parameters n1, ...., nm, 0 < pi < 1, 
m
i=1 pi = 1
and 
m
i=1 ni = N. Since X is S −MRR2, it follows from Theorem 6
that for k ≥1,
P(X1 ≤x1, ...., Xm ≤xm) ≤γk.
(2.5.2)
A Bonferroni-type lower bound for P(X1 ≤x1, ...., Xm ≤xm) is ob-
tained from Equation (2.4.8):
P(X1 ≤x1, ...., Xm ≤xm) ≥δk.
(2.5.3)
©2001 CRC Press LLC

TABLE 2.4
Bounds for P-values for a test of equal cell probabilities
n
5
6
7
8
9
10
11
γ3
.9944
.8562
.4758
.1744
.0496
.0121
.0026
δ3
1
1
.6200
.1894
.0507
.0121
.0026
For 1 ≤i ≤m, xi = x, inequalities (2.5.2) and (2.5.3) yield inequalities
for the distribution function of X(m) = max(X1, ...., Xm). An approxi-
mation for the distribution of X(1) = min(X1, ...., Xm) is given in Glaz
(1990). For the special case of pi = 1/m, 1 ≤i ≤m, inequalities (2.5.2)
and (2.5.3) simplify to
(m + k −1)α1,k −(m −k)α1,k−1 ≤P(X(m) ≤x)
≤[α1,k]m−k+1
[α1,k−1]m−k ,
(2.5.4)
where for j ≥1
α1,j = P(X1 ≤x, ...., Xj ≤x).
The performance of inequalities (2.5.4) is evaluated for an example with
a roulette with m = 38 numbers.
Suppose we are interested to test
the null hypotheses that pi = 1/38, 1 ≤i ≤m. Consider the test that
rejects this null hypothesis for large values of X(38). Table 2.4 presents
inequalities (2.5.4) for k = 3 for P-values of this test for N = 100.
Inequalities for the distribution of X(1) and X(m) can be used to detect
outliers in a speciﬁed multinomial model.
2.5.4
A Conditional Discrete Scan Statistic
Let X1, ...., Xn be i.i.d.
0 −1 Bernoulli trials with P(Xi = 1) = p,
0 < p < 1.
Suppose that we know that a successes (1s) and n −a
failures (0s) have been observed. Then
P

X1 = x1, ...., Xn = xn|
n

i=1
Xi = a

= 1
na.
In this case the joint distribution of the observed 0 −1 trials assigns
equal probabilities to all the na arrangements of a 1’s and n −a 0s. We
©2001 CRC Press LLC

are interested in deriving tight bounds for the upper tail probabilities
for a conditional scan statistic denoted by
P(k; m, n, a) = P

Sm ≥k|
n

i=1
Xi = a

,
where Sm is deﬁned in Equation (2.5.1).
Let Y1, ...., Yn be the random variables denoting one of the na se-
quences of 0−1 trials that contain a 1s and n−a 0s. For n = Lm, L ≥4
and 1 ≤i ≤L −1 deﬁne the events
Di =
m+1

j=1
!
Y(i−1)m+j + .... + Yim+j−1 ≤k −1
"
.
It follows that
P(k; m, n, a) = P
L−1

i=1
Dc
i

.
Employing the second order Bonferroni-type inequality in Hunter (1976)
and Worsley (1982) given in Equation (2.2.3) and the fact that Di are
stationary events we get
P(k; m, n, a) ≤
L−1

i=1
P (Dc
i ) −
L−1

i=2
P
!
Dc
i ∩Dc
i−1
"
= 1 + (L −3)q2m(a) −(L −2)q3m(a),
where for 1 ≤i ≤L −1, q2m(a) = P(Di), q3m(a) = P(Di ∩Di−1) and
for r = 2, 3
qrm =
min(rk−r,a)

j=0
q(rm|j)rmjn −rma −j
na
where q(rm|j) = 1 −P(k; m, rm, j) are evaluated in Naus (1974, Theo-
rem 1).
To derive a Bonferroni-type lower bound for P(k; m, n, a) an inequality
in Kwerel (1975) presented in Equation (2.4.10) gives:
©2001 CRC Press LLC

P
L−1

i=1
Dc
i

≥2s1
b
−
2s2
b(b −1),
where b = [2s2/s1 + 2],
s1 =
L−1

i=1
P (Dc
i ) = (L −1)(1 −q2m(a))
and
s2 =

1≤i<j≤L−1
P
!
Dc
i ∩Dc
j
"
=
L−2

i=1
P
!
Dc
i ∩Dc
i+1
"
+

1≤i<j−1≤L−2
P
!
Dc
i ∩Dc
j
"
= (L −2)[1 −2q2m(a) + q3m(a)]
+.5(L −2)(L −3)[1 −2q2m(a) + q2m,2m(a)]
= (L −2)q3m(a) + .5(L −2)(L −3)q2m,2m(a)
+.5(L −1)(L −2)(1 −2q2m(a)),
where
q2m,2m(a)
=
min(2k−2,a)

j1=0
min(2k−2,a−j1)

j2=0
q(2m|j1)q(2m|j2)2mj12mj2n −4ma −j1 −j2
na
and q(rm|j), r = 2, 3 are given above. A diﬀerent approach in deriving
bounds for P(k; m, n, a) is presented in Chen et al. (1999). In Tables
2.5 and 2.6 the bounds for P(k; m, n, a) and
E

Sm|
n

i=1
Xi = a

=
m

k=1
P(k; m, n, a)
are evaluated for selected values of the parameters.
Probability inequalities for a conditional scan statistic for a sequence
of binomial or Poisson random variables has yet to be derived.
©2001 CRC Press LLC

TABLE 2.5
Bonferroni-type bounds for P(k; m, n, a)
n
m
a
k
BTLB
BTUB
P(k; m, n, a)
100
10
5
3
.1665
.1665
.1678
4
.0090
.0090
.0098
20
10
5
.3505
.3513
.3465
6
.0752
.0752
.0752
7
.0096
.0096
.0102
500
10
50
4
.6850
1.000
.8523
5
.2244
.2594
.2369
6
.0271
.0274
.0265
20
75
8
.3772
.4869
.4283
9
.1223
.1276
.1254
10
.0259
.0261
.0254
P(k; m, n, a) is a simulated value based on 10,000 trials.
TABLE 2.6
Bonferroni-type bounds for E

Sm| n
i=1 Xi = a

n
m
a
LB
UB
E

Sm| n
i=1 Xi = a

100
10
5
1.95
2.18
2.08
10
2.95
3.26
3.10
20
10
4.16
4.33
4.30
20
6.95
7.13
7.10
500
10
25
2.72
3.15
2.94
50
3.87
4.29
4.13
20
50
5.50
6.01
5.82
75
7.09
7.65
7.46
E

Sm| n
i=1 Xi = a

is a simulated value based on 10, 000 trials.
©2001 CRC Press LLC

2.5.5
Simultaneous Prediction in Time Series Models
Let X1, ...., Xn be the observed time series data. We are interested in
a rectangular simultaneous prediction region for the future observations
Xn+1, ...., Xn+m. The following region has been investigated in Glaz and
Ravishanker (1991) and Ravishanker, Wu and Glaz (1991):
Xn+i(i) ± c
#
V ar(en(i)), 1 ≤i ≤m,
where for 1 ≤i ≤m, Xn+i(i) are the forecasts for Xn+i and en(i) =
Xn+i −Xn+i(i) are the forecasts errors. The constant c is obtained from
the following equation:
P (|Un(i)| ≤c; 1 ≤i ≤m) = 1 −α,
where Un(i) = en(i)/
#
V ar(en(i)) are the standardized forecast errors
and 1 −α is the preassigned joint conﬁdence level of the rectangular
prediction region. Assume that the time series can be modeled by an
AR(1) process:
Xt = φXt−1+ ∈t, |φ| < 1,
and {∈t} is a sequence of i.i.d.
normal random variables with mean
0 and variance σ2. Then Un = (Un(1), ...., Un(m)) has a multivariate
normal distribution with mean vector 0 and correlation matrix Σ. It
follows from Glaz and Ravishanker (1991) that (|Un(1)|, ...., |Un(m)|) is
MTP2 and therefore, for 1 ≤k ≤5, product-type inequalities given in
Equation (2.2.5) can used to evaluate the constant c via an algorithm in
Schervish (1984). This algorithm can be used to evaluate the constant
c from the Bonferroni-type inequalities given in Equation (2.4.8).
In Table 2.7 the constant c is evaluated for selected joint conﬁdence
levels for a data set Series D in Box and Jenkins (1976). The data set
consists of n = 310 hourly readings of viscosity of a chemical process.
An AR(1) model is used for this data set.
There are many interesting open problems in the area of simultaneous
prediction regions in time series models.
For example, simultaneous
conﬁdence regions for future observed random vectors in multivariate
time series, using probability inequalities, have not been investigated
yet.
©2001 CRC Press LLC

TABLE 2.7
Evaluating the constant c using four methods
Conﬁdence level
Method
k
.99
.95
.90
.88
.80
.70
.60
1
3.29
2.80
2.56
2.41
2.29
2.11
1.96
2
3.21
2.68
2.41
2.24
2.11
1.90
1.73
Product-type
3
3.20
2.66
2.39
2.21
2.08
1.87
1.70
4
3.20
2.65
2.38
2.20
2.07
1.86
1.68
5
3.20
2.65
2.38
3.20
2.07
1.85
1.68
1
2.29
2.81
2.58
2.43
2.33
2.17
2.05
2
3.21
2.69
2.43
3.26
2.14
1.95
1.81
Bonferroni-type
3
3.20
2.67
2.40
2.23
2.10
1.91
1.75
4
3.20
2.66
2.39
2.22
2.09
1.89
1.73
5
3.20
2.65
2.38
2.21
2.08
1.87
1.71
Simulation
3.19
2.64
2.37
2.19
2.06
1.85
1.68
Marginal
2.58
1.96
1.64
1.44
1.28
1.04
.84
The simulation method is based on 10, 000 trials.
The marginal method is based on marginal conﬁdence levels.
©2001 CRC Press LLC

References
1. Ahmed, A. N., Langberg, N. A., Leon, R., and Proschan, F. (1978).
Two concepts of positive dependence with applications in multi-
variate analysis. Technical Report M486, Department of Statistics,
Florida State University.
2. Alam, K. and Wallenius, K. T. (1976). Positive dependence and
monotonicity in conditional distributions.
Communications in
Statistics — Theory and Methods 5, 525–534.
3. Barlow, R. E. and Proschan, F. (1975). Statistical Theory of Re-
liability and Life Testing. Holt, Reinhart, and Winston, Inc., New
York.
4. Block, H. W., Savits, T. H., and Shaked, M. (1982). Some concepts
of negative dependence. Annals of Probability 10, 765–772.
5. Block, H. W. and Ting, M. L. (1981). Some concepts of multi-
variate dependence. Communications in Statistics — Theory and
Methods 10, 749–762.
6. Bonferroni, C. E. (1936). Teoria statistica delle classi e calcolo delle
probabilita.
Pubblicazioni del R. Instituto Superiore di Scienze
Economiche e Commerciali di Firenze 8, 1–62.
7. Boole, G. (1984). Laws of Thought. American reprint of 1854 ed.,
Dover, New York.
8. Box, G. E. P. and Jenkins, G. M. (1976). Time Series Analysis:
Forecasting and Control. Holden Day, San Francisco.
9. Brindley, E. C. and Thompson, W. A. (1972). Dependence and
aging aspects of multivariate survival.
Journal of the American
Statistical Association 67, 822–830.
10. Chen, J. and Glaz, J. (1999). Approximations for the distribution
and the moments of discrete scan statistics.
In Scan Statistics
and Applications (Eds., J. Glaz and N. Balakrishnan). Birkh¨auser,
Boston.
11. Chen, J., Glaz, J., Naus, J., and Wallenstein, S. (1999). Bonferroni-
type inequalities for conditional scan statistics. Technical Report,
Department of Statistics, University of Connecticut.
12. Dawson, D. A. and Sankoﬀ, D. (1967). An inequality for prob-
abilities. Proceedings of the American Mathematical Society 18,
504–507.
13. Dunn, O. J. (1959).
Conﬁdence intervals for the means of de-
pendent normally distributed variables. Journal of the American
©2001 CRC Press LLC

Statistical Association 54, 613–621.
14. Ebrahimi, N. and Ghosh, M. (1981).
Multivariate negative de-
pendence. Communications in Statistics — Theory and Methods,
Series A 10, 307–337. (Correction, 1984 13, p. 3251).
15. Esary, J. D. and Proschan, F. (1972). Relationships among some
notions of bivariate dependence. Annals of Mathematical Statistics
43, 651–655.
16. Esary, J. D., Proschan, F., and Walkup, D. W. (1967). Association
of random variables with applications.
Annals of Mathematical
Statistics 38, 1466–1474.
17. Galambos, J. and Simonelli, I. (1996). Bonferroni-type Inequalities
with Applications. Springer, New-York.
18. Glaz, J. (1990).
A comparison of Bonferroni-type and product-
type inequalities in presence of dependence. Topics in Statistical
Dependence (Eds., H. W. Block, A. R. Sampson, and T. H. Savits).
IMS Lecture Notes – Monograph Series, Volume 16, pp. 223–235.
IMS, Hayward, CA.
19. Glaz, J. (1992). Extreme order statistics for a sequence of depen-
dent random variables. In Stochastic Inequalities (Eds., M. Shaked
and Y. L. Tong). IMS Lecture Notes – Monograph Series, Volume
22, 100–115. IMS, Hayward, CA.
20. Glaz, J. and Balakrishnan, N. (Eds.) (1999). Scan Statistics and
Applications. Birkh¨auser, Boston.
21. Glaz, J. and Johnson, B. McK. (1984). Probability inequalities for
multivariate distributions with dependence structures. Journal of
the American Statistical Association 79, 436–441.
22. Glaz, J. and Johnson, B. McK. (1986). Approximating boundary
crossing probabilities with applications to sequential tests. Sequen-
tial Analysis 5, 37–72.
23. Glaz, J. and Johnson, B. McK. (1988).
Boundary crossing for
moving sums. Journal of Applied Probability 25, 81–88.
24. Glaz, J. and Kenyon, J. R. (1995). Approximating the character-
istics of sequential tests. Probability and Mathematical Statistics
15, 311–325.
25. Glaz, J. and Naus, J. (1991). Tight bounds and approximations
for scan statistic probabilities for discrete data. Annals of Applied
Probability 1, 306–318.
26. Glaz, J. and Naus, J. (1996). Approximations and bounds for mov-
ing sums of iid continuous random variables. Preliminary Report.
27. Glaz, J. and Ravishanker, N. (1991). Simultaneus prediction inter-
©2001 CRC Press LLC

vals for multiple forecasts based on Bonferroni and product-type
inequalities. Statistics and Probability Letters 12, 57–63.
28. Harris, R. (1970). A multivariate deﬁnition for increasing hazard
rate distributions.
Annals of Mathematical Statistics 41, 1456–
1465.
29. Hoover, D. R. (1990). Subset complement addition upper bounds
– an improved inclusion/exclusion method. Journal of Statistical
Planning and Inference 24, 195–202.
30. Hunter, D. (1976). An upper bound for the probability of a union.
Journal of Applied Probability 13, 597–603.
31. Joag-Dev, K., Perlman, M. D., and Pitt, L. D. (1983). Associa-
tion of normal random variables and Slepian inequality. Annals of
Probability 11, 451–455.
32. Karlin, S. (1968).
Total Positivity.
Stanford University Press.
Stanford, CA.
33. Karlin, S. and Rinott, Y. (1980a). Classes of orderings of measures
and related correlations inequalities I: Multivariate totally positive
distributions. Journal of Multivariate Analysis 10, 467–498.
34. Karlin, S. and Rinott, Y. (1980b). Classes of orderings of measures
and related correlations inequalities II: Multivariate reverse rule
distributions. Journal of Multivariate Analysis 10, 499–516.
35. Kemperman, J. H. B. (1977). On the FKG inequality measures
on a partially ordered space.
Proc.
Kon.
Ned.
Akad.
Wet.,
Amsterdam, Series A 80(4), 313–331.
36. Khatri, C. G. (1967).
On certain inequalities for normal distri-
butions and their applications to simultaneous conﬁdence bounds.
Annals of Mathematical Statistics 38, 1853–1867.
37. Kim, T. S. and Seo, H. Y. (1995). A note on some negative de-
pendence notions.
Communications in Statistics — Theory and
Methods, Series A 24, 845-858.
38. Kimball, A. W. (1951). On dependent tests of signiﬁcance in the
analysis of variance. Annals of Mathematical Statistics 22, 600–
602.
39. Kwerel, S. M. (1975). Most stringent bounds on aggregated prob-
abilities of partially speciﬁed probability systems. Journal of the
American Statistical Association 70, 472–479.
40. Lee, M. L. T. (1985). Dependence by reverse regular rule. Annals
of Probability 13, 583–591.
41. Lehmann, E. L. (1966). Some concepts of dependence. Annals of
Mathematical Statistics 37, 1137–1153.
©2001 CRC Press LLC

42. Naus, J. I. (1974). Probabilities for a generalized birthday problem.
Journal of the American Statistical Association 69, 810–815.
43. Olkin, I. and Tong, Y. (1994). Positive dependence of a class of
multivariate exponential distributions. SIAM Journal of Comput-
ing 32, 965–974.
44. Pitt, L. D. (1982). Positively correlated normal random variables
are associated. Annals of Probability 10, 496–499.
45. Ravishanker, N., Wu, L. S. Y., and Glaz, J. (1991). Multiple pre-
diction intervals for time series: comparison of simultaneous and
marginal intervals. Journal of Forecasting 10, 445–463.
46. Sarkar, T. K. (1969). Some lower bounds of reliability. Technical
Report No. 124, Department of Operation Research and Statistics,
Stanford University.
47. Schervish, M. K. (1984). Algorithm AS 195, multivariate normal
probabilities with error bound. Applied Statistics 33, 81-94.
48. Scott, A. (1967). A note on conservative conﬁdence regions for the
mean of a multivariate normal distribution. Annals of Mathemat-
ical Statistics 38, 278–280. Correction: Annals of Mathematical
Statistics 39, 2161.
49. Sidak, Z. (1967). Rectangular conﬁdence regions for means of mul-
tivariate normal distributions. Journal of the American Statistical
Association 62, 626–633.
50. Sidak, Z. (1971).
On probabilities of rectangles in multivariate
student distributions: their dependence on correlations. Annals of
Mathematical Statistics 42, 169–175.
51. Woodroofe, M. (1982). Nonlinear Renewal Theory in Sequential
Analysis. CBMS-NSF Regional Conference Series in Applied Math-
ematics 39. SIAM, Philadelphia.
52. Worsley, K. J. (1982).
An improved Bonferroni inequality and
applications. Biometrika 69, 297–302.
©2001 CRC Press LLC

3
Applications of Compound Poisson
Approximation
A. D. Barbour, O. Chryssaphinou, and E. Vaggelatou
Universit¨at Z¨urich, Switzerland
University of Athens, Greece
University of Athens, Greece
ABSTRACT Stein’s method for compound Poisson approximation has
not been as widely applied as in the Poisson context. One reason for this
is that the bounds on the solutions of the Stein Equation are in general
by no means as good as for Poisson approximation. Here, we illustrate
how new bounds on the solutions have radically changed the situation,
provided that the parameters of the approximating compound Poisson
distribution satisfy certain conditions. The results obtained in this way
are every bit as satisfactory as those obtained in the Poisson context.
Keywords and phrases Compound
Poisson
approximation,
word
counts, sequence comparison, success runs, Stein’s method
3.1
Introduction
Stein’s method has proved to be one of the most successful methods
for determining the accuracy of Poisson approximation. Let W be any
nonnegative integer valued random variable, with the property that
|IE{λg(W + 1) −Wg(W)}| ≤ε0M0(g) + ε1M1(g)
(3.1.1)
for some λ > 0 and for all bounded g : N →R, where Ml(g) :=
supw∈N|∆lg(w)|, l ∈Z+, and ∆g(w) := g(w + 1) −g(w).
Then, if
the total variation distance between probability distributions on Z+ is
deﬁned by
dT V (P, Q) := sup
A⊂Z+
|P(A) −Q(A)|,
it follows that
dT V (L(W), Po(λ)) ≤ε0 sup
A⊂Z+
M0(gA) + ε1 sup
A⊂Z+
M1(gA),
(3.1.2)
©2001 CRC Press LLC

where gA is the solution of the Poisson Stein Equation
λg(j + 1) −jg(j) = f(j) −Po(λ){f},
(3.1.3)
for f = 1A. Bounds for the suprema are given in Barbour, Holst, and
Janson (1992, Lemma 1.1.1 and Remark 1.1.2):
sup
A⊂Z+
M0(gA) ≤min{1,

2/eλ};
sup
A⊂Z+
M1(gA) ≤λ−1(1 −e−λ).
(3.1.4)
The fact that both bounds decrease as λ increases is a very useful aspect
of the error estimate (3.1.2), because it often ensures that increasing
the size of the problem does not worsen the estimates. For instance, if
W = n
i=1 Ii is a sum of independent Bernoulli Be (pi) random variables,
we can take λ = IEW = n
i=1 pi, ε0 = 0 and ε1 = n
i=1 p2
i , resulting in
the simple bound [Barbour and Hall (1984); see also LeCam (1960)]
dT V (L(W), Po(λ)) ≤λ−1
n

i=1
p2
i ,
a quantity which typically does not grow with n: if all the pi are equal,
the bound is exactly p for all n. Establishing (3.1.1) for small ε0 and ε1 is
frequently a surprisingly tractable task; many examples are to be found
in Arratia, Goldstein, and Gordon (1990) and in Barbour, Holst, and
Janson (1992).
For compound Poisson approximation, there is an analogous approach.
Let W be any nonnegative integer valued random variable, and let µi ≥
0, i ∈N, satisfy 
i≥1 iµi < ∞and 
i≥1 µi = 1. If it can be shown
that

IE




i≥1
iλµig(W + i) −Wg(W)




≤ε0M0(g) + ε1M1(g)
(3.1.5)
for some λ > 0 and for all bounded g : N →R, then it follows that
dF(L(W), CP (λ, µ)) := sup
f∈F
|IEf(W) −CP (λ, µ){f}|
≤ε0 sup
f∈F
M0(gf) + ε1 sup
f∈F
M1(gf),
(3.1.6)
for any set F of test functions, where gf now solves the compound Pois-
son Stein Equation

i≥1
iλµig(j + i) −jg(j) = f(j) −CP (λ, µ){f},
j ≥0.
(3.1.7)
©2001 CRC Press LLC

Here, CP (λ, µ) denotes the compound Poisson distribution of 
i≥1 iZi,
where the Zi ∼Po(λµi) are independent.
As for (3.1.1), establish-
ing 3.(1.5) for small ε0 and ε1 may often be relatively simple. However,
the resulting distance estimates (3.1.6) were not as powerful as they could
have been, for lack of sharp bounds on the quantities supf∈F Ml(gf) for
the commonest choices of test functions F and for most CP (λ, µ).
For the test functions FT V = {1A, A ⊂Z+}, appropriate to total
variation approximation, the general bounds
HT V
l
(λ, µ) :=
sup
f∈FT V
Ml(gf) ≤min{1, (λµ1)−1}eλ,
l = 0, 1,
(3.1.8)
were proved in Barbour, Chen, and Loh (1992); because of the expo-
nential factor, these are only useful for small λ. However, under the
additional condition on the µi that
µ1 ≥2µ2 ≥3µ3 ≥. . . ;
(3.1.9)
they showed that
HT V
0
(λ, µ) ≤min

1,
2
√λν1

;
HT V
1
(λ, µ) ≤min

1, 1
λν1

1
4λν1
+ log+(2λν1)

,
(3.1.10)
where ν1 = µ1 −2µ2. These bounds are much better, but that on M1 is
still weaker than could be hoped for, because of the logarithmic factor.
This factor was removed in Barbour and Utev (1998, Theorem 3.1), in
the setting of approximation with respect to the Kolmogorov distance dK
deﬁned by
dK(P, Q) := sup
x |P[x, ∞) −Q[x, ∞)|,
by way of intricate Fourier arguments, but only at the expense of wors-
ening the constant factors to a point at which they became unattractive
for practical application.
In this paper, we illustrate the improvements in compound Poisson
estimates that can be obtained by using two new bounds. The ﬁrst of
these, proved in Barbour and Xia (1999b), requires a diﬀerent condition:
if
θ :=

i≥1
i(i −1)µi

m1 < 1/2,
(3.1.11)
where m1 := 
i≥1 iµi, then
HT V
0
(λ, µ) ≤
1
(1 −2θ)√λm1
;
HT V
1
(λ, µ) ≤
1
(1 −2θ)λm1
. (3.1.12)
©2001 CRC Press LLC

These bounds are of exactly the same order as the Poisson bounds (3.1.4),
and reduce to them if CP (λ, µ) is a Poisson distribution, so that µ1 = 1
and θ = 0; they were established by using an analytic perturbation
argument, starting from the Poisson solution.
The second bound is valid under the previous condition (3.1.9). It has
been shown in Barbour and Xia (1999a) that, for the set of test functions
FK := {fk, k ∈N :
fk(x) = 1[k,∞)(x)} appropriate to Kolmogorov
distance,
HK
0 (λ, µ)
:= supf∈FK M0(gf) ≤min

1,

2
eλµ1

,
HK
1 (λ, µ)
:= supf∈FK M1(gf) ≤min

1
2,
1
λµ1+1

.
(3.1.13)
Here, the proof is based on probabilistic arguments, using the asso-
ciated immigration–death process introduced in Barbour, Chen, and
Loh (1992).
Combining (3.1.12) and (3.1.13) with (3.1.6), we obtain
the following proposition.
PROPOSITION 3.1.1
Let W be a nonnegative integer valued random variable, and suppose
that (3.1.5) holds for all bounded g : N →R, for some λ > 0 and
µi ≥0, i ∈N, such that 
i≥1 iµi < ∞and 
i≥1 µi = 1.
(A)
If Condition 3.1.11) is satisﬁed, then
dT V (L(W), CP (λ, µ)) ≤(1 −2θ)−1{(λm1)−1/2ε0 + (λm1)−1ε1}.
(B)
If Condition (3.1.9) is satisﬁed, then
dK(L(W), CP (λ, µ))
≤ε0 min

1,

2
eλµ1

+ ε1 min
1
2,
1
λµ1 + 1

.
Neither pair of bounds has logarithmic factors, and both have small
constant factors, making them well suited for practical application; we
illustrate this in what follows, showing that a number of previously ob-
tained compound Poisson error estimates can be substantially improved.
©2001 CRC Press LLC

3.2
First Applications
3.2.1
Runs
As a ﬁrst, frequently studied illustration, we consider compound Pois-
son approximation to the number W of k-runs of 1’s in a sequence of
independent Bernoulli Be (p) random variables Xi, 1 ≤i ≤n: thus
W := n−k+1
i=1
Ii, where Ii := i+k−1
j=i
Xj and ψ := IEIi = pk. In order
to simplify the deﬁnition of the approximating distribution, we start by
adopting the ‘circle convention’, setting Xn+j = Xj for all j ∈Z, and
considering a new random variable W ′ := n
i=1 Ii, which counts the
number of consecutive k-runs of 1’s when the Xi are arranged around
a circle, rather than on the line. This is merely a way of avoiding edge
eﬀects, and is not essential to the method.
There have been a number of compound Poisson approximations to the
distributions of W and W ′, with the precise choices of λ and µ diﬀering
slightly from paper to paper. Arratia, Goldstein, and Gordon (1990)
used Poisson process approximation to derive a total variation estimate
of order O(nkψ2(1−p)) for the approximation error. Under the condition
p ≤1
3, so that the bound (3.1.10)) can be applied, Roos (1993) improved
the error estimate to order O(kψ log(nψ)); furthermore, Barbour and
Utev (1999, Theorem 1.10) can be used for any value of p to give an error
estimate of order O(kψ + exp(−cnψ)) for some c > 0 (see Eichelsbacher
and Roos (1998)), but with an uncomfortably large constant.
Here,
using the improved bounds on the Stein constants, we can establish
substantially smaller error estimates, for Kolmogorov distance if p ≤1/2
and for total variation distance if p < 1/5.
Roos (1993) addresses the problem using the ‘local approach’. For W
a sum of 0–1 random variables Ii, 1 ≤i ≤n, this involves writing W in
the form
W = Ii + Ui + Zi + Wi
(3.2.1)
for each 1 ≤i ≤n, where Ui, Zi and Wi are nonnegative, integer valued
random variables, with Ui typically heavily dependent on Ii, and Wi
almost independent of the pair (Ii, Ui). Then one can take
ε0 =
n

i=1

l≥1
IE|IP[Ii = 1, Ui = l −1 | Wi] −IP[Ii = 1, Ui = l −1]| (3.2.2)
and
ε1 =
n

i=1
{IEIi IE(Ii + Ui + Zi) + IE(IiZi)}
(3.2.3)
in (3.1.5), implying corresponding compound Poisson approximations by
©2001 CRC Press LLC

way of (3.1.6), if λ and µ have the canonical deﬁnitions
λµl := l−1
n

i=1
IE{IiI[Ui = l −1]},
l ≥1;
λ :=

l≥1
λµl.
(3.2.4)
Note that then λ 
l≥1 lµl = IEW, so that W and the compound Poisson
distribution CP (λ, µ) both have the same expectation.
For the example of runs and the random variable W ′, taking
Ui =

1≤|j−i|<k
Ij;
Zi =

k≤|j−i|≤2(k−1)
Ij,
(3.2.5)
it follows that Wi is independent of (Ii, Ui), so that ε0 = 0, and that Ii
and Zi are independent, which then easily gives ε1 = nψ2(6k −5). The
calculation of λ and µ is more complicated, eventually giving
λµl =























IEW ′ pl−1(1 −p)2
for l = 1, 2, ..., k −1,
IEW ′ l−1pl−1[2(1 −p) + (2k −l −2)(1 −p)2]
for l = k, ..., 2k −2,
IEW ′ (2k −1)−1p2k−2
for l = 2k −1,
(3.2.6)
with IEW ′ = nψ. For these parameters, Condition (3.1.9) is satisﬁed for
all k if p ≤1/3 and for k ≥4 if p ≤1/2, and Proposition 3.1.1(B) then
implies that
dK(L(W ′), CP (λ, µ)) ≤min
1
2,
1
λµ1 + 1

n(6k −5)ψ2
<
 p
for k = 1
(6k −5)ψ(1 −p)−2 for k ≥2.
(3.2.7)
For total variation distance, θ ≤2p/(1 −p) < 1/2 if p < 1/5, and
Proposition 3.1.1(A) gives the error estimate
dT V (L(W ′), CP (λ, µ)) ≤(6k −5)ψ(1 −p)/(1 −5p).
(3.2.8)
Both of these estimates are simple, explicit, and of better order than the
previous bounds in their domains of applicability.
The original runs problem is concerned with the distribution of W :=
n−k+1
i=1
Ii, omitting the extra random variables Ii, n −k + 2 ≤i ≤n,
©2001 CRC Press LLC

whose deﬁnition involved the circle convention.
The local approach
described above can still be carried out in the same way, yielding a
marginally smaller error bound; the drawback is that the canonical pa-
rameters obtained in this way are more complicated to specify, since they
also require edge corrections. An alternative solution is to observe that
W and W ′ diﬀer only by the sum n
i=n−k+2 Ii, so that
dT V (L(W), L(W ′)) ≤(k −1)ψ;
hence one can also approximate the distribution L(W) using the same
compound Poisson distribution CP (λ, µ), deﬁned in (3.2.6), as was used
for L(W ′), with an additional error of at most (k −1)ψ.
An even simpler approximating compound Poisson distribution is pro-
vided by the P´olya-Aeppli distribution PA (λ∗, p), deﬁned as CP (λ∗, µ∗)
with λ∗:= nψ(1 −p) and
µ∗
l := pl−1(1 −p),
l ≥1;
as can be seen from (3.2.6 λµl = λ∗µ∗
l for 1 ≤l ≤k−1, and furthermore
λ 
l≥1 lµl = nψ = λ∗
l≥1 lµ∗
l .
It then follows from Roos (1993, Lemma 2.4.2) that the distribution
CP (λ, µ) can be replaced by PA (λ∗, p) in the above approximations,
provided that ε1 is increased by an amount ε∗:= 
l≥1 l(l −1)|λµl −
λ∗µ∗
l |. Using the deﬁnition of λµl from (3.2.6), this gives
ε∗= nψ




k≤l≤2k−2
(l −1)pl−1(1 −p)|2(l −k)(1 −p) −2p|
+(2k −2)p2k−2|(2k −1)(1 −p)2 −1|
+

l≥2k
(l −1)lpl−1(1 −p)2



≤2nψ

l>k
(l −1)(l −k)pl−1(1 −p)2 + (k −1)pk(1 −p)
+(k −1)p2k−2

= 2nψ2{k + 2p(1 −p)−1 + (k −1)(1 −p + pk−2)}
≤2nψ2

2k + (3p −1)
(1 −p)

.
(3.2.9)
The last inequality of (3.2.9) holds for k ≥3 but it can be easily veriﬁed
that it still remains a bound for ε∗, when k = 2 and p ≤1/2. Thus using
©2001 CRC Press LLC

instead the P´olya-Aeppli distribution PA (nψ(1−p), p) as approximation,
requires the addition of at most 2(2k + 1)ψ(1 −p)−2 to the Kolmogorov
estimate (3.2.7), and, what is more, the estimate is then valid irrespective
of k for all p ≤1/2, since Condition (3.1.9) holds for PA (λ∗, p) whenever
p ≤1/2. For the total variation estimate (3.2.8), valid in p < 1/5, one
needs to add at most 4kψ(1 −p)/(1 −5p).
The problem of k-runs can also be treated as one of counting the
number of visits to a rare set in a Markov chain.
From this stand-
point, the general theorems of Erhardsson (1999) can be applied, in
conjunction with the improved bounds on the Stein constants given by
Proposition 3.1.1: see also Theorem (3.3.2) below. Using his approach,
the approximating compound Poisson distribution is automatically the
P´olya-Aeppli distribution PA (λ∗, p), and the error estimates obtained
using the bounds given in (3.1.13) and (3.1.12) are somewhat smaller,
being a little better than 4(k + 1)ψ(1 −p)−2 for Kolmogorov distance if
p ≤1/2, and a little better than (4k + 5)ψ/(1 −5p) for total variation
distance if p < 1/5.
3.2.2
Sequence Matching
Let ξ1, . . . , ξm and η1, . . . , ηn be two independent sequences of indepen-
dently chosen letters from a ﬁnite alphabet A, both uniformly distributed
over A. Fix k, and set
Iij = I[ξi = ηj, ξi+1 = ηj+1, . . . , ξi+k−1 = ηj+k−1],
so that
W :=
m−k+1

i=1
n−k+1

j=1
Iij
counts the number of times that pairs of matching strings of length k
can be found in the two sequences.
In molecular sequence applica-
tions, an observed value of W higher than that expected according to
the above model would indicate evolutionary relationship between the
two sequences. Previous work (Arratia, Goldstein, and Gordon (1990),
Neuhauser (1996)) has largely concentrated on approximating IP[W = 0],
which, by then varying k, translates into a statement about the length
of the longest matching run; with this in mind, the strategy is typically
to replace W by a random variable which counts distinct clumps of k–
runs, and to approximate its distribution by a Poisson random variable.
Here, as also in M
◦ansson (1999), who explores a more general setting,
we use compound Poisson approximation to treat the whole distribu-
tion of W, and to provide rather explicit bounds for the accuracy of the
approximations obtained.
©2001 CRC Press LLC

Once again, to simplify the canonical CP (λ, µ) approximation, we
work instead with the random variable
W ′ :=
m

i=1
n

j=1
Iij,
adopting the ‘torus convention’ that ξm+i = ξi and ηn+j = ηj for all
i, j ∈Z. The random variable W ′ then has expectation
IEW ′ = mnψ,
where ψ := IEIij = pk and p := 1/|A|, and we are typically interested
in values of k less than, say, 2 log(mn)/ log(1/p), so that IEW ′ is not
extremely small. In order to use the local approach, we require a decom-
position as in (3.3.1) of the form W ′ = Iij + Uij + Zij + Wij, for each
pair i, j. Noting that the indicators most strongly dependent on Iij are
those of the form Ii+l,j+l with |l| ≤k −1, we take
Uij =

1≤|l|≤k−1
Ii+l,j+l
and
Zij =



(r,s)∈Nij
Irs

−Iij −Uij,
where
Nij = {(r, s); min{|r −i|, |s −j|} ≤2(k −1)}.
This yields W ′ = Iij + Uij + Zij + Wij, in such a way that Wij is
independent of the pair (Iij, Uij), so that ε0 = 0.
The canonical parameters for the corresponding compound Poisson
approximation are the same as those in (3.2.6) for success runs, but
with the new deﬁnitions of p and IEW ′, and, as before, Condition (3.1.9)
is satisﬁed for all k if p ≤1/3, and Condition (3.1.11) is satisﬁed with
(1−2θ)−1 ≤(1−p)/(1−5p) if p < 1/5; a P´olya-Aeppli PA (mnψ(1−p), p)
approximation would add only at most an extra (4k + 2)ψ(1 −p) to ∆1
in the estimates given below. In bounding the expression corresponding
to (3.2.3) for ε1, it is immediate that
IEIijIE(Iij + Uij + Zij) ≤(4k −3)(m + n)ψ2.
(3.2.10)
For IE(IijZij), note that Iij is independent of each single Irs in the sum
deﬁning Zij, so that we have
IE(IijZij) ≤(4k −3)(m + n)ψ2
(3.2.11)
©2001 CRC Press LLC

as well. Hence ε1 ≤2mn(4k −3)(m + n)ψ2, and using Proposition 3.1.1
we deduce that
dK(L(W ′), CP (λ, µ))
≤{(1 −p)2 + (IEW ′)−1}−1 ∆1,
|A| ≥3;
dT V (L(W ′), CP (λ, µ))
≤{(1 −p)/(1 −5p)} ∆1,
|A| ≥6,
(3.2.12)
where λ and µ are as given in (3.2.6), but now with IEW ′ = mnψ, and
∆1 := 2(4k −3)(m + n)ψ.
(3.2.13)
Conversion from W ′ to W is once again straightforward, since it is im-
mediate that
dT V (L(W), L(W ′)) ≤(k −1)(m + n)ψ.
Most emphasis has previously been placed on asymptotics in which m
and n tend to inﬁnity in such a way that both log m/ log n and λ = λmn
converge to ﬁnite, nonzero limits. Using (3.11.4) and (3.11.5), we can
make more precise statements about how well the distribution of W ′ is
being approximated, and under less restrictive conditions. For m and n
given, m ≤n, set
k = kmn := log(mn)/ log(1/p) −c,
for any c = cmn ≥0, noting that then IEW ′ = p−c. From (3.11.5), we
immediately have
∆1 ≤8k(m + n)ψ ≤
2 log(mn)
log(1/p)

(m−1 + n−1)p−c,
which is small so long as log n ≪m and c is not too large. In asymptotic
terms, as m, n →∞for ﬁxed p, one would require that log n = o(m)
and in addition that cmn ≤δ log{m/ log n}/ log(1/p) for some δ < 1.
Previous asymptotics have mostly assumed that c is bounded above, so
that this last condition would then be automatically satisﬁed.
3.3
Word Counts
The next problem that we treat is counting the number of appearances W
of a word A = a1a2 . . . ak of length k, in a sequence of letters ξ1, . . . , ξn of
length n from an inﬁnite realization of an irreducible, aperiodic station-
ary Markov chain (ξi, i ∈Z) on a ﬁnite alphabet A, having transition
matrix Π and stationary distribution π. Thus W := n
i=k I[Bi
k], where
Bi
j := {ξi−j+1 = a1, . . . , ξi = aj},
1 ≤j ≤k,
(3.3.1)
©2001 CRC Press LLC

is the event that ξi−j+1 . . . ξi is a copy of the ﬁrst j letters of A, and
we set ψ := IEI[Bi
k], noting that IEW = (n −k + 1)ψ. We discuss this
problem in rather greater detail, making some numerical comparisons.
As a ﬁrst approach, we illustrate how the ‘declumping’ procedure of
Arratia, Goldstein, and Gordon (1990) can be used to derive accurate
bounds, when applied in conjunction with the compound Poisson Stein
equation, as in Barbour and Utev (1998, Section 5) and Barbour and
Chryssaphinou (1999, Section 2.2). Schbath (1995) employed the Ar-
ratia, Goldstein, and Gordon (1990) procedure directly, establishing an
estimate of order O(nψ2) for the error in compound Poisson approxi-
mation to L(W), though no fully explicit formula for the estimate was
given. As in the case of runs, this order of approximation is not accurate
when IEW is large. Here, we show how to modify her approach to obtain
an explicit bound of the improved order O(ψ log n).
In what follows, we use the term “period” of a word as deﬁned in
Guibas and Odlyzko (1981). In particular, an integer p is a period of a
word A = a1a2 . . . ak if ai = ai+p for all i = 1, ..., k −p. Let also P′(A)
denote the set of “principal” periods of A (Schbath (1995)), consisting of
the shortest period p0 of A together with those which are not multiples
of p0. For example, the word A = a1a1a2a1a1a2a1a1 with k = 8 has
periods 3, 6 and 7, and P′(A) = {3,7}.
To introduce the declumping, let Ii,l denote the indicator r.v. of the
appearance of an l-clump at position i. More precisely, let Cl be the set
of words consisting of exactly l overlapping appearances of A, U the set
of k −1 letter words that do not end with the p ﬁrst letters of A for any
p ∈P′(A) and V the set of k −1 letter words which do not start with
the p last letters of A for any p ∈P′(A). Then deﬁne
Ii,l =

U∈U,C∈Cl,V ∈V
(Ii−k+1(UCV )),
where Ii(S) denotes the indicator of the event that a copy of the word S
begins at index i, and note that
IEIi,l = (1 −L)2Ll−1ψ,
(3.3.2)
where L = 
p∈P′(A)
p
t=1 Π(atat+1).
The ‘declumped’ random variable W ′ := n−k+1
i=1

l≥1 lIi,l is easier
to approximate than W, and W = W ′ unless there is a copy of A which
overlaps either the beginning or the end of the interval {1, 2, . . . , n}, so
that
dT V (L(W), L(W ′)) ≤2(k −1)ψ,
(3.3.3)
a quantity no larger than the other terms appearing in the approxima-
tion error.
It follows from (3.3.2) that the P´olya-Aeppli distribution
©2001 CRC Press LLC

PA (IEW (1 −L), L) is the appropriate compound Poisson approxima-
tion, and Barbour and Chryssaphinou (1999, Section 2.2) shows that we
can take ε0 = b∗
3 and ε1 = b∗
1 + b∗
2 in (3.1.6), where
b∗
1 =
n−k+1

i=1

l≥1

(j,s)∈B(i,l)
lsIEIi,lIEIj,s;
b∗
2 =
n−k+1

i=1

l≥1

(j,s)∈B(i,l),j̸=i
lsIE(Ii,lIj,s);
(3.3.4)
b∗
3 =
n−k+1

i=1

l≥1
lIE|IE{Ii,l −IEIi,l | σ(Ij,s : (j, s) ∈Bc
(i,l))}|;
here, we specify
B(i,l) = {(j, s) : −(s + 2)(k −1) −r + 1 ≤j −i
≤(l + 2)(k −1) + r −1}.
(3.3.5)
This latter deﬁnition is appropriate, because the indicators Ii,l and Ij,s
are measurable with respect to σ(ξi−k+1, . . . , ξi+(l+1)(k−1)) and σ(ξj−k+1,
. . . , ξj+(s+1)(k−1)), respectively, so that Ii,l and Ij,s are only weakly de-
pendent whenever j −k + 1 ≥i + (l + 1)(k −1) + r or i −k + 1 ≥
j + (s + 1)(k −1) + r, for r ≥1. This gives the conclusion of Theo-
rem (3.3.1), for which we need some extra notation.
Let Π(m)(a, a′) denote the m-step transition probabilities in the ξ-
chain, and ΠR(a, a′) := Π(a′, a)π(a′)/π(a) the transition probabilities in
the reversed ξ-chain. Deﬁne
χ(A) := π(a1)−1 max
m≥1 Π(m)(ak, a1) ≥1.
Then let K ≥1 and 0 ≤ρ ≤1 be numbers such that
max
a∈A max
 
a′∈A
|Π(m)(a, a′) −π(a′)| ,

a′∈A
|Π(m)
R (a, a′) −π(a′)|

≤Kρm
(3.3.6)
for all m ≥1. It follows from an elementary coupling argument (see, for
example, Lindvall (1992), paragraph 2 on page 96, for a rather dismissive
description) that one can take K = 2 and ρ = 1 −min{δ(Π), δ(ΠR)},
where, for an A × A matrix P, δ(P) := 
a′∈A mina∈A P(a, a′). There
are usually better choices, particularly if δ(Π) = 0. Note also that (3.3.6)
is actually only needed for m ≥r, where r ≥1 is as above, and may be
chosen to suit.
©2001 CRC Press LLC

THEOREM 3.3.1
If L ≤1/2, then
dK

L(W), PA (IEW (1 −L), L)

≤∆1

1
(1 −L)2 + {IEW}−1

+ ∆0

2
eIEW (1 −L)2
1/2
+2(k −1)ψ;
if L < 1/5, then
dT V

L(W), PA (IEW (1 −L), L)

≤B(3.3.1)
:= {(1 −L)/(1 −5L)}(∆1 + {IEW}−1/2∆0) + 2(k −1)ψ,
where IEW = (n −k + 1)ψ,
∆0 := KρrIEW

2 + ρ3(k−1)+r + Kρr
;
∆1 := 2ψ

3(k −1) + r + 2(k −1)
L
(1 −L)
+χ(A)
2L(k −1 −p0)
(1 −L)3
+ 2(k −1) + r −1
(1 −L)2
 
,
and r ≥1 may be chosen to minimize the error estimates.
REMARK If the sequence ξi, i ∈Z, is independent, the estimates of
Theorem (3.3.1) hold, with r and K replaced by zero.
PROOF In order to prove Theorem (3.3.1), we just need to bound the
terms b∗
1, b∗
2 and b∗
3 of (3.3.4), and use (3.3.3). Then, as observed above,
Proposition 3.1.1 can be applied with ε0 = b∗
3 and ε1 = b∗
1 + b∗
2.
The computation of
b∗
1 =
n−k+1

i=1

l≥1

s≥1
i+(l+2)(k−1)+r−1

j=i−(s+2)(k−1)−r+1
lsIEIi,lIEIj,s
≤2
n−k+1

i=1

l≥1

s≥1
i+(l+2)(k−1)+r−1

j=i
lsIEIi,lIEIj,s
(3.3.7)
©2001 CRC Press LLC

is a simple matter, using (3.3.2), with (3.2.9-) yielding exactly
b∗
1 ≤2ψIEW

3(k −1) + r + 2(k −1)L
1 −L

.
(3.3.8)
For the term b∗
2, we write
b∗
2 =
n−k+1

i=1

l≥1

s≥1
i+(l+2)(k−1)+r−1

j=i−(s+2)(k−1)−r+1
j̸=i
lsIE(Ii,lIj,s)
= 2
n−k+1

i=1

l≥1

s≥1
ls
i+(l+2)(k−1)+r−1

j=i+1

C∈Cl
IE(Ii,l(C)Ij,s),
where
Ii,l(C) :=

U∈U,V ∈V
Ii−k+1(UCV ).
We now distinguish two cases,
when bounding the expectations
IE(Ii,l(C)Ij,s), i + 1 ≤j ≤i + (l + 2)(k −1) + r −1.
If i + 1 ≤j ≤i + |C| −1, then IE(Ii,l(C)Ij,s) = 0, in view of the
deﬁnition of Ij,s. If i + |C| ≤j ≤i + (l + 2)(k −1) + r −1, observe that
IE(Ii,l(C)Ij,s)
=

U∈U,V ∈V

U ′∈U,C′∈Cs,V ′∈V
IE{Ii−k+1(UCV )Ij−k+1(U ′C′V ′)}
≤

U∈U

C′∈Cs,V ′∈V
IE{Ii−k+1(UC)Ij(C′V ′)},
(3.3.9)
since

V ∈V
Ii−k+1(UCV ) ≤Ii−k+1(UC)
and

U ′∈U
Ij−k+1(U ′C′V ′) ≤Ij(C′V ′).
Moreover, the copies of the words UC and C′V ′ appearing in (3.3.9) do
not overlap, which implies that
IE(Ii−k+1(UC)Ij(C′V ′)) ≤π(UC)Π(j−i−|C|+1)(ak, a1)π(C′V ′)/π(a1)
≤χ(A)π(UC)π(C′V ′).
Hence, for C ∈Cl and i + |C| ≤j ≤i + (l + 2)(k −1) + r −1, we have
©2001 CRC Press LLC

IE(Ii,l(C)Ij,s) ≤χ(A)

U∈U

C′∈Cs,V ′∈V
π(UC)π(C′V ′)
≤χ(A)

U∈U
π(UC)(1 −L)Ls−1ψ,
(3.3.10)
and since |C| ≥k+(l−1)p0 for all C ∈Cl, where p0 is the shortest period
of A, the number of indices j for which the bound (3.3.10) is needed is
no greater than (l −1)(k −1 −p0) + 2(k −1) + (r −1). Combining these
observations, we ﬁnd that
b∗
2 ≤2
n−k+1

i=1

l≥1

s≥1
ls{(l −1)(k −1 −p0) + 2(k −1) + (r −1)}

C∈Cl
χ(A)

U∈U
π(UC)(1 −L)Ls−1ψ
= 2χ(A)(n −k + 1)ψ2(1 −L)2

l≥1
l{(l −1)(k −1 −p0) + 2(k −1) + (r −1)}Ll−1 
s≥1
sLs−1
= 2χ(A)ψIEW
2L(k −1 −p0)
(1 −L)3
+ 2(k −1) + r −1
(1 −L)2

.
(3.3.11)
It remains to examine the term b∗
3. Using the Markovian property and
the fact that σ(Ij,s
:
(j, s)
∈
Bc
(i,l)) is contained in σ(ξ1, . . . ,
ξi−k−r+1, ξi+(l+1)(k−1)+r, . . . , ξn), we obtain
b∗
3 =
n−k+1

i=1

l≥1
lIE|IE{Ii,l −IEIi,l | σ(Ij,s : (j, s) ∈Bc
(i,l))}|
≤
n−k+1

i=1

l≥1
l

U∈U,C∈Cl,V ∈V
IE|IE{Ii−k+1(UCV ) −IEIi−k+1(UCV ) | σ(Ij,s : (j, s) ∈Bc
(i,l))}|
≤
n−k+1

i=1

l≥1
l

U∈U,C∈Cl,V ∈V

x,y∈A
|IE{Ii−k+1(UCV ) | ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y} −π(UCV )|
IP(ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y)
©2001 CRC Press LLC

=
n−k+1

i=1

l≥1
l

U∈U,C∈Cl,V ∈V

x,y∈A
|IP(Ii−k+1(UCV ) = 1, ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y)
−π(UCV )IP(ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y)|.
(3.3.12)
Now
IP(Ii−k+1(UCV ) = 1, ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y)
= π(UCV )Π(r)
R (u1, x)Π(l(k−1)+r−|C|+1)(vk−1, y),
where u1 and vk−1 denote the ﬁrst and the last letters of the words U
and V , respectively; furthermore,
IP(ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y) = π(x)Π((l+2)(k−1)+2r)(x, y).
Writing
|Π(s)(a, a′) −π(a′)| := η(s)(a, a′);
|Π(s)
R (a, a′) −π(a′)| := η(s)
R (a, a′),
where, from (3.3.6), for each a ∈A,

a′∈A
η(s)(a, a′) ≤Kρs;

a′∈A
η(s)
R (a, a′) ≤Kρs,
(3.3.13)
we thus have
|IP(Ii−k+1(UCV ) = 1, ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y)
−π(UCV )IP(ξi−k−r+1 = x, ξi+(l+1)(k−1)+r = y)|
≤π(UCV )

π(y)η(r)
R (u1, x) + π(x)η(l(k−1)+r−|C|+1)(vk−1, y)
+η(r)
R (u1, x)η(l(k−1)+r−|C|+1)(vk−1, y)
+π(x)η((l+2)(k−1)+2r)(x, y)

.
Substituting this into (3.3.12), and using (3.3.13), it then follows that
b∗
3 ≤(n −k + 1)

l≥1
l

U∈U,C∈Cl,V ∈V
π(UCV )
×K

2ρr + Kρ2r + ρ3(k−1)+2r
= KρrIEW {2 + Kρr + ρ3(k−1)+r},
(3.3.14)
©2001 CRC Press LLC

since 
l≥1 l 
U∈U,C∈Cl,V ∈V π(UCV ) = ψ. The bounds (3.3.8), (3.3.11)
and (3.3.14), used in the procedure given at the start of the proof, es-
tablish the theorem.
The distribution of W can also be treated using the Markov chain ap-
proach. To do so, we introduce the achievement chain X = (Xi, i ∈Z),
also Markovian, with state space A∗= ((A \ {a1}) × {0}) ∪{1, 2, . . . , k},
deﬁned by
{Xi = j} := Bi
j \
k
l=j+1
Bi
l,
j = 1, ..., k −1
{Xi = k} := Bi
k,
{Xi = (a, 0)} := {ξi = a} \
k
j=1
Bi
j,
a ̸= a1,
(3.3.15)
where the Bi
j are as in (3.3.1). Denote its transition matrix by P and
its stationary distribution by ν.
Then the number of visits of Xi to
the state k between times k and n is also the number of copies of A
in the sequence ξ1, ξ2, . . . , ξn, and ν(k) = ψ.
We then use Erhards-
son (1999, Theorem 4.3) and the improved bounds (3.1.13) and (3.1.12)
to approximate L(W) by a compound Poisson distribution. Erhardsson
also considers the more complicated problem of counting the number of
times that a copy of any one of a set of words appears.
For x ∈A∗, let τx := inf{i ≥0 : Xi = x} and τ R
x
:= inf{i ≥
0 : X−i = x} denote the ﬁrst hitting times of the chain X and its
time reversal XR on the state x, and let τ +
x := inf{i > 0 : Xi = x}.
Let IEk denote expectation conditional on X0 = k and IEν expectation
conditional on X0 ∼ν. Then from Erhardsson (1999, Theorem 4.3 and
Remark 4.2), and from the bounds (3.1.13) and (3.1.12), we have the
following theorem.
THEOREM 3.3.2
Choose any a0 ̸= a1, and deﬁne L∗:= IPk(τ +
k
< τ(a0,0)).
Then, if
L∗≤1/2,
dK(L(W), PA (IEW (1 −L∗), L∗)) ≤{(1 −L∗)2 + (IEW)−1}−1∆′
1 + ∆3;
if L∗< 1/5,
dT V (L(W), PA (IEW (1 −L∗), L∗))
≤B(3.3.2) := {(1 −L∗)/(1 −5L∗)}∆′
1 + ∆3,
©2001 CRC Press LLC

where IEW = (n −k + 1)ψ, ∆3 := 2ψIEk(τ R
(a0,0)) and
∆′
1 := 2ψ{IEk(τ(a0,0)) + IEk(τ R
(a0,0)) + IEν(τ(a0,0))/ν(a0, 0)}.
The statement of this theorem is a little less explicit than that of The-
orem (3.3.1), in particular because L∗̸= L needs to be calculated. It
is actually possible to prove rough bounds for the quantities IEk(τ R
(a0,0))
and IEz(τ(a0,0)), z ∈A∗, and for the diﬀerence between L∗and L, in
terms of the elements of the transition matrix Π. However, all these
quantities can be computed simply by solving small sets of linear equa-
tions if |A| is small, as is the case for the four letter DNA alphabet or
the twenty letter amino acid alphabet, and this is the procedure that we
adopt in the numerical comparisons. The bound in Theorem (3.3.2) is
asymptotically better than that of Theorem (3.3.1), improving the order
O(ψ log n) to O(ψ). However, for moderate n, the logarithmic factor
may not dominate the contributions from the constant elements in the
bounds, so that numerical comparison for such n is also of interest.
3.4
Discussion and Numerical Examples
We compare the estimates B(3.3.1) of Theorem (3.3.1) and B(3.3.2) of
Theorem (3.3.2) with the estimate BS of Schbath (1995). Her estimate
is not explicit, as far as the long range dependence is concerned, so we
treat her b3 much as we have treated our b∗
3, giving in total
dT V (L(W), CP (λ, µ)) ≤BS
:= 2ψIEW

(k −1)(1 −L)(
1
πmin
+ 1)
+(2k + r −2 + r −k
πmin
)(1 −L)2 + 2(k −1)
π(a1)

+KρrIEW (1 −L){2 + Kρr + ρ3(k−1)+r} + 2(k −1)ψ,
(3.4.1)
where πmin := mina∈A π(a).
In BS and B(3.3.1), the value of r can be chosen to minimize the
error estimate. Given the values of k, ψ, L and ρ, it only requires simple
calculus to ﬁnd the real value r which minimizes the expressions, and the
best integer value of r can then be determined by experiment. In B(3.3.2),
the choice of a0 can also be made so as to obtain the best estimate. In
the examples that we present, for the word ACGACG (k = 6) from the
©2001 CRC Press LLC

DNA alphabet A = {A,C,G,T} with three diﬀerent matrices Π, we take
a0 := T. We tabulate the estimates for total variation distance; those
for Kolmogorov distance are slightly smaller.
1. The following transition matrix corresponds to the Lamda virus
(Reinert and Schbath (1998))
Π =




0.2994
0.2086
0.2215
0.2705
0.2830
0.2198
0.2740
0.2232
0.2540
0.2820
0.2480
0.2160
0.1813
0.2232
0.3164
0.2791



,
and has stationary distribution (0.254374, 0.234228, 0.264254, 0.247145).
The inequality (3.3.6) holds for K = 2 and ρ = 0.1726. The values of L
and L∗are 0.0145177 and 0.0152569 respectively.
Word
n
r
IEW
B(3.3.2)
B(3.3.1)
BS
14500
6
3
0.0159630
0.0188487
0.1068091
47500
6
10
0.0159630
0.0190079
0.3451702
ACGACG
95000
6
20
0.0159630
0.0191553
0.6882658
142500
6
30
0.0159630
0.0192684
1.0313614
237000
6
50
0.0159630
0.0194469
1.7139410
474000
6
100
0.0159630
0.0197762
3.4258074
2. For the transition probability matrix
Π =




0.5
0.125
0.125
0.25
0.125
0.5
0.25
0.125
0.125
0.25
0.5
0.125
0.25
0.125
0.125
0.5



,
the stationary distribution is (0.25, 0.25, 0.25, 0.25) and the inequal-
ity (3.3.6) holds for K = 2 and ρ = 0.5. The values of L and L∗are
0.0039063 and 0.0040525 respectively.
Word
n
r
IEW
B(3.3.2)
B(3.3.1)
BS
98500
15
3
0.0028436
0.0038838
0.0236823
330000
16
10
0.0028436
0.0039900
0.0810643
ACGACG
655500
17
20
0.0028436
0.0040564
0.1661725
985000
17
30
0.0028436
0.0040878
0.2495496
1640000
17
50
0.0028436
0.0041371
0.4152917
3280000
18
100
0.0028436
0.0041974
0.8590681
3. For the transition probability matrix
Π =




0.05
0.3
0.4
0.25
0.35
0.3
0.05
0.3
0.25
0.25
0.25
0.25
0.1
0.3
0.4
0.2



,
©2001 CRC Press LLC

the stationary distribution is (0.200778, 0.286976, 0,260486, 0.251761)
and the inequality (3.3.6) holds for K = 2 and ρ = 0.5426. The values
of L and L∗are 0.003750 and 0.003785 respectively.
Word
n
r
IEW
B(3.3.2)
B(3.3.1)
BS
267000
19
3
0.0007571
0.0017632
0.0119805
890000
20
10
0.0007571
0.0018143
0.0408546
ACGACG
1771000
20
20
0.0007571
0.0018401
0.0811845
2660000
20
30
0.0007571
0.0018602
0.1218807
4430000
21
50
0.0007571
0.0018792
0.2091687
8860000
21
100
0.0007571
0.0019108
0.4182248
We note that the bound B(3.3.2) remains stable as n increases, whereas
B(3.3.1) increases slowly. In all the cases considered here, B(3.3.2) per-
forms better. However, its advantage may be less in a larger alphabet,
for moderate n, because the factor 1/ν(a0, 0) may then be expected to
be bigger in comparison with log n. For the calculations, we used Math-
ematica 2.2.
Acknowledgements. This work was supported by University of Athens
Research Grant 70-4-2558 (A. D. Barbour and O. Chryssaphinou), by
Swiss Nationalfonds Projekt Nr. 20-50686.97 (A. D. Barbour), by the
grant Pened95 692 (O. Chryssaphinou and E. Vaggelatou), of the Gen-
eral Secretariat of Research and Technology of Greece and by the Na-
tional Scholarship Foundation of Greece (E. Vaggelatou).
References
1. Arratia, R., Goldstein, L., and Gordon, L. (1990). Poisson approx-
imation and the Chen-Stein method. Statistical Science 5, 403-434.
2. Barbour, A. D. and Chryssaphinou, O. (1999). Compound Poisson
approximation: a user’s guide. preprint.
3. Barbour, A. D., Chen, L. H. Y., and Loh, W. (1992).
Com-
pound Poisson approximation for nonnegative random variables
via Stein’s method. Annals of Probability 20, 1843-1866.
4. Barbour, A. D. and Hall, P. (1984). On the rate of Poisson conver-
gence. Mathematical Proceedings of the Cambridge Philosophical
Society 95, 473-480.
5. Barbour, A. D., Holst, L., and Janson, S. (1992). Poisson Approx-
imation., Oxford University Press.
©2001 CRC Press LLC

6. Barbour, A. D. and Utev, S. (1998). Solving the Stein equation in
compound Poisson approximation. Advances in Applied Probability
30, 449-475.
7. Barbour, A. D. and Utev, S. (1999). Compound Poisson approx-
imation in total variation. Stochastic Processes and Applications
82, 89-125.
8. Barbour, A. D. and Xia, A. (1999a). Estimating Stein’s constants
for compound Poisson approximation. Bernoulli, to appear.
9. Barbour, A. D. and Xia, A. (1999b).
Poisson perturbations.
ESAIM Probability and Statistics 3, 131-150.
10. Chryssaphinou, O., Papastavridis, S., and Vaggelatou, E. (1999).
On the number of appearances of a word in a sequence of i.i.d.
trials. Methodology and Computing in Applied Probability 3, 329-
348.
11. Eichelsbacher, P. and Roos, M. (1998) Compound Poisson approx-
imation for dissociated random variables via Stein’s method. Com-
binatorics, Probability and Computing 8, 335-346.
12. Erhardsson, T. (1999).
Compound Poisson approximation for
Markov chains using Stein’s method.
Annals of Probability 27,
565-596.
13. Guibas, L. J. and Odlyzko, A. M. (1981). Periods in strings. Jour-
nal of Combinatorial Theory A 30, 19-42.
14. LeCam, L. (1960).
An approximation theorem for the Poisson
binomial theorem. Paciﬁc Journal of Mathematics 10, 1181-1197.
15. Lindvall, T. (1992). Lectures on the coupling method. Wiley, New
York.
16. M
◦ansson, M. (1999).
On compound Poisson approximation for
sequence matching. preprint.
17. Neuhauser, C. (1996). A phase transition for the distribution of
matching blocks.
Combinatorics, Probability and Computing 5,
139-159.
18. Reinert, G., Schbath, S. (1998).
Compound Poisson and Pois-
son process approximations for occurrences of multiple words in
Markov chains. Journal of Computational Biology 5, 223-253.
19. Roos, M. (1993). Stein-Chen method for compound Poisson ap-
proximation. PhD thesis, University of Z¨urich, Switzerland.
20. Roos, M. (1994). Stein’s method for compound Poisson approxi-
mation: the local approach. Annals of Applied Probability 4, 1177-
1187.
©2001 CRC Press LLC

21. Schbath, S. (1995).
Compound Poisson approximation of word
counts in DNA sequences.
ESAIM Probability and Statistics 1,
1-16.
22. Wolfram, S. (1988). Mathematica, Addison-Wesley, Reading, MA.
©2001 CRC Press LLC

4
Compound Poisson Approximation for
Sums of Dependent Random Variables
Michael V. Boutsikas and Markos V. Koutras
University of Athens, Athens, Greece
ABSTRACT In the present article an upper bound is obtained for the
Kolmogorov distance between a sum of (dependent) nonnegative r.v.s
and an appropriate compound Poisson distribution. Two applications
pertaining to moving sums distributions and success runs in a sequence
of Markov dependent trials are considered and certain upper bounds for
their distance to a compound Poisson distribution are established, along
with asymptotic results (compound Poisson distribution convergence).
Keywords and phrases Compound Poisson approximation, sums of
dependent variables, Kolmogorov distance, rate of convergence, moving
sums, success runs in a Markov chain
4.1
Introduction
Let X1, X2, ..., Xn be a sequence of real-valued, nonnegative r.v.s. The
subject of the present paper is the approximation of the distribution of
the sum of Xi when the masses of L(Xi), i = 1, 2, ..., n are concentrated
around 0, and Xi, i = 1, 2, ..., n are “weakly” dependent. Such problems
arise quite naturally when we are dealing with a set of “rare” and “al-
most unrelated” events. The simplest case when Xi are binary i.i.d. r.v.s
was initially treated in the fundamental work of Poisson as early as 1837
when he established the classical Poisson approximation of the binomial
distribution. Since then a huge variety of generalizations and extensions
of this result has appeared in the literature. In some of them the assump-
tion of identical Xi was relaxed while in others the assumption of inde-
pendence was replaced by a condition of “local” or “weak dependence”.
The latter generalization was thoroughly investigated after the introduc-
tion of the much acclaimed Stein-Chen method (c.f. Chen (1975); the
©2001 CRC Press LLC

deﬁnite reference for this method, is the elegant monograph by Barbour,
Holst, and Janson (1992)). For more recent developments on Poisson or
compound Poisson approximation using Stein-Chen method the reader
may refer to Barbour, Chen, and Loh (1992), Roos (1994), Xia (1997),
Barbour and Utev (1998) and Erhardsson (1999). A completely diﬀerent
approach of Compound Poisson approximations for sums of dependent
r.v.s was taken by Vellaisamy and Chaudhuri (1999) (c.f. the references
therein).
The Stein-Chen method provides quite tight error bounds for Pois-
son or compound Poisson approximations of sums of weakly dependent
binary r.v.s. In a very recent work, Boutsikas and Koutras (2000a) re-
placed the assumption of binary r.v.s by that of integer valued ones and
proceeded to the investigation of error bounds for compound Poisson ap-
proximations by taking a completely diﬀerent approach than that of the
Stein-Chen method. This was accomplished at the expense of restrict-
ing the nature of the dependence between Xi to that of associated or
negatively associated (NA) r.v.s. More, speciﬁcally, the authors proved
that if X1, X2, . . . , Xn are nonnegative, integer-valued, associated or NA
r.v.’s with E(Xi), E(XiXj) < ∞for i, j = 1, 2, . . . , n, i ̸= j, then
dW (L(
n

i=1
Xi), CP(λ, F)) ≤2


i<j
Cov(Xi, Xj)

+
n

i=1
E(Xi)2.
(4.1.1)
Here dW stands for the Wasserstein distance, CP denotes a compound
Poisson distribution, λ = n
i=1 P(Xi > 0), and F(x) = 1
λ
n
i=1 P(0 <
Xi ≤x) (the latter can also be viewed as a mixture of the conditional
distributions of Xi given that Xi > 0 with weights P(Xi > 0), i =
1, 2, . . . , n). Clearly, if the nonnegative, integer-valued, associated or NA
r.v.s Xi, i = 1, 2, . . . , n are almost uncorrelated and their distributions
are concentrated around zero, then the above result states that their sum
can be approximated by an appropriate compound Poisson distribution.
As a matter of fact, (4.1.1) is a by-product of a more general result which
provides error estimates of the distance between n
i=1 Xi and a sum of
independent r.v.s n
i=1 X′
i having the same marginals as the original
ones (L(Xi) = L(X′
i)).
It is worth stressing that, when Xi are binary associated or NA r.v.s,
the bound provided by (4.1.1) has almost the same form as the respective
Poisson approximation Stein-Chen bound (created by exploiting an ap-
propriate coupling argument); the main diﬀerence in the ﬁnal bounds is
in the so-called “magic factor” which is known to improve substantially
the error bound provided by the Stein-Chen method (especially when λ
becomes large).
Should a compound Poisson approximation be pursued for sums of
©2001 CRC Press LLC

binary “weakly dependent” indicators, inequality (4.1.1) could be fruit-
fully applied to gain an error bound (e.g. by deﬁning Xi as the size
of clump clustered at the point where the i-th occurrence of the event
we are interested in took place).
In general this bound is simpler in
form than the corresponding Stein-Chen bound which engages neigh-
borhoods of strong/weak dependence for the binary indicators involved
[see Arratia et al. (1989, 1990), Roos (1994), Barbour and Utev (1998)].
As demonstrated in many recent versions of the compound Poisson ap-
proximation Stein-Chen method, the incorporation of a factor similar
to the ordinary Poisson approximation magic factor leads occasionally
to tighter bounds than (4.1.1) especially when λ is large. Nevertheless,
if λ is kept ﬁxed (which is quite common in many asymptotic results),
(4.1.1) is of the same order as the Stein-Chen bound.
When Xi are associated or NA, inequality (4.1.1) can be viewed as
an extension of the Stein-Chen-type bound to integer-valued r.v.s. It
is therefore reasonable to ask whether we could establish a similar ex-
tension, by rejecting the assumption of association or NA between the
Xis.
The main purpose of the present paper is to provide a bound for
the distance between L(n
i=1 Xi), and an appropriate compound Pois-
son distribution where Xi are arbitrary nonnegative real-valued r.v.s.
This is accomplished by using neighborhoods of strong and weak depen-
dence similar in structure to the ones employed in Arratia’s et al. (1989,
1990), ingenious approach. The core of our proof is very similar to that
used in Boutsikas and Koutras (2000a), although the starting point of
the two approaches is quite diﬀerent. Note also that here the metric
distance used for measuring the discrepancy between distributions and
establishing asymptotic results is the Kolmogorov distance (instead of
the Wasserstein or the total variation distance).
The new bound goes to 0 when the masses of L(Xi), i = 1, 2, ..., n
are concentrated on 0, and Xi, i = 1, 2, ..., n are “weakly” dependent. It
is worth stressing though that, in contrast to inequality (4.1.1) the new
result cannot be eﬀectively used when we deal with “globally” dependent
r.v.s [e.g. see Boutsikas and Koutras (2000b)].
The organization of the paper is as follows: In Section 4.2 we in-
troduce the necessary notations and review the basic properties of the
Kolmogorov distance. Section 4.3 provides our main result which oﬀers
an upper bound for the distance between the distribution of n
a=1 Xa
and an appropriate compound Poisson distribution.
Two interesting
intermediate results, pertaining to the distance between a) a sum of
dependent r.v.s n
a=1 Xa and the sum of their independent duplicates
n
a=1 X′
a (L(Xa) = L(X′
a), a = 1, 2, ..., n), b) a sum of independent r.v.s
and a compound Poisson distribution, are also presented. Finally, in
©2001 CRC Press LLC

Section 4.4 two speciﬁc examples are considered: the ﬁrst one deals with
the distribution of the total exceedance amount (above a prespeciﬁed
threshold) of moving sums in a sequence of i.i.d. r.v.s, and the second
with the distribution of the number of overlapping success runs in a
sequence of Markov dependent trials. In both cases, a bound for the
distance between the distribution of interest and a proper compound
Poisson distribution is obtained and an asymptotic result (compound
Poisson convergence) is established.
4.2
Preliminaries and Notations
In the sequel CP(λ, F) will denote the compound Poisson distribution
with characteristic function (c.f.) e−λ(1−φF (z)), where φF (z) is the c.f. of
the distribution F. Obviously, CP(λ, F) coincides with the distribution
of the sum N
i=1 Yi with N being a Poisson r.v. with parameter λ and
Yi independent r.v.s with distribution function F. We shall also use the
notation Po(λ) for the ordinary Poisson distribution with parameter λ.
Manifestly, Po(λ) = CP(λ, L(1)).
In order to bound the error incurred when approximating the sum
of Xi by an appropriate CP(λ, F) we shall be using the Kolmogorov
distance
d(L(X), L(Y )) = sup
w |P(X ≤w) −P(Y ≤w)|.
For typographical convenience, we shall allow in the sequel an abuse
of the notation and use d(X, Y ) instead of d(L(X), L(Y )). Manifestly,
a sequence of random variables {Xn} converges in distribution to Y if
d(Xn, Y ) converges to 0.
It can be easily veriﬁed that d(aX, aY ) =
d(X, Y ). Moreover, if Y is a r.v. independent of X, Z, we have
d(X + Y, Z + Y ) = sup
w


(P(X ≤w −y) −P(Z ≤w −y))dFY (y)

≤d(X, Z)
(4.2.1)
Using the last inequality in conjunction with the triangle inequality we
may easily verify that, if (X1, Y1) and (X2, Y2) are independent, then
d(X1 + X2, Y1 + Y2) ≤d(X1 + X2, Y1 + X2) + d(Y1 + X2, Y1 + Y2)
≤d(X1, Y1) + d(X2, Y2)
(4.2.2)
A repeated application of (4.2.2) captures the subadditivity property of
©2001 CRC Press LLC

the Kolmogorov distance for independent summands, i.e. if (X1, Y1),
(X2, Y2), ..., (Xn, Yn) are independent, then
d(
n

i=1
Xi,
n

i=1
Yi) ≤
n

i=1
d(Xi, Yi).
(4.2.3)
Let now X1, X2, ..., Xn be (independent) Bernoulli r.v.s such that
P(Xi = 1) = pi.
If Y1, Y2, . . . , Yn are independent r.v.s such that
L(Yi) = Po(pi), then, recalling (4.2.3), we get
d(
n

i=1
Xi, Po(
n

i=1
pi)) = d(
n

i=1
Xi,
n

i=1
Yi) ≤
n

i=1
d(Xi, Yi)
=
n

i=1
sup
w=0,1,...
|P(Xi > w) −P(Yi > w)|
=
n

i=1
max

pi −1 + e−pi, 1 −e−pi −pie−pi
≤1
2
n

i=1
p2
i .
(4.2.4)
We ﬁnally mention that d (X, Y )
≤
max{P (X < Y ) , P (X > Y )}
≤P (X ̸= Y ) for every coupling (X, Y ) of L(X), L(Y ).
For an early
review on some of the elementary properties of the Kolmogorov distance
refer also to Serﬂing (1978).
4.3
Main Results
Consider a sequence of nonnegative r.v.s Xa, a ∈In = {1, 2, ..., n}. For
each a ∈In, a ̸= 1, we introduce a subset Ba of Ia−1 so that Xa is
independent or weakly dependent to Xb, b ∈B′
a = Ia−1 \Ba. The set Ba
will be referred in the sequel as the left neighborhood of strong dependence
of Xa.
We shall now proceed to the statement of the main result of our paper
which presents an upper bound for the Kolmogorov distance between
the distribution of n
i=1 Xi and an appropriate compound Poisson dis-
tribution. In what follows, X′
a, a = 1, 2, ..., n will denote independent
r.v.’s having the same marginals as Xa, a = 1, 2, ..., n, i.e.
L(Xa) =
L(X′
a); X′
a, a = 1, 2, ..., n will also be assumed as being independent of
Xa, a = 1, 2, ..., n.
©2001 CRC Press LLC

THEOREM 4.3.1
If Xa, a = 1, 2, ..., n is a sequence of nonnegative r.v.s, then
d
 n

a=1
Xa, CP(λ, F)

≤
n

a=2
(P(Xa > 0,

b∈Ba
Xb > 0) + P(Xa > 0)P(

b∈Ba
Xb > 0))(4.3.1)
+1
2
n

i=1
P (Xi > 0)2 +
n

a=2
d(

b∈B′
a
Xb + Xa,

b∈B′
a
Xb + X′
a)
where λ = n
i=1 pi, and F(x) =
1
λ
n
i=1 piP(Xi ≤x|Xi > 0), x ∈R,
pi = P(Xi > 0).
The proof is carried out in two steps: ﬁrstly we obtain an upper bound
for the distance between n
a=1 Xa and n
a=1 X′
a, and an upper bound
for the distance between  X′
a and CP(λ, F). Secondly, the triangle
inequality is exploited to arrive at a bound between  Xa and CP(λ, F).
Due to its independent interest the development of the upper bounds
for d( Xa,  X′
a) and d( X′
a, CP(λ, F)) will be stated as separate
theorems (see Theorems 4.3.3 and 4.3.4 below). Before proceeding to
the statement and proof of the intermediate steps, let us discuss the
applicability of it to some interesting special cases.
Note ﬁrst that an upper bound for the ﬁrst term of the RHS of (4.3.1)
is oﬀered by the inequality
d
 n

a=1
Xa, CP(λ, F)

≤
n

a=2

b∈Ba
(P(Xa > 0, Xb > 0) + P(Xa > 0)P(Xb > 0))
(4.3.2)
+1
2
n

i=1
P (Xi > 0)2 +
n

a=2
d(

b∈B′
a
Xb + Xa,

b∈B′
a
Xb + X′
a)
In most applications the last bound is more convenient for performing the
necessary calculations to establish upper bounds for d ( Xa, CP(λ, F)).
It is clear that, should Xa be independent of Xb, b ∈B′
a for every
a ∈In the last summand in the RHS of inequality (4.3.1) would vanish.
The family of m-dependent r.vs oﬀers a case where this last remark
applies; under this condition the upper bound in (4.3.1) takes on a much
©2001 CRC Press LLC

simpler form. A similar bound for the total variation distance has been
mentioned by Barbour, Holst, and Janson (1992) (see Corollary 10.L.1,
page 239).
If B′
a is chosen so that Xa is not independent but “weakly” dependent
of Xb, b ∈B′
a we can ultimately bound the last term of (4.3.1) by an
appropriate quantity. For example, consider a sequence of nonnegative
r.v.s X1, X2, ..., Xn taking values in {0, 1, 2, ..., r −1} and assume that
the sequence {Xn} is a-mixing, i.e.
there exist am, m = 1, 2, .. such
that |P(A ∩B) −P(A)P(B)| ≤am for every A ∈σ(X1, ..., Xi), B ∈
σ(Xi+m, Xi+m+1, ...), i ≥1, m ≥1, and am →0 (see e.g. Billingsley
(1986), p. 375). Then
d(
i−m

b=1
Xb + Xi,
i−m

b=1
Xb + X′
i)
= sup
w
P
i−m

b=1
Xb + Xi ≤w

−P
i−m

b=1
Xb + X′
i ≤w

= sup
w

r−1

x=0

P
i−m

b=1
Xb ≤w −x, Xi = x

−P
i−m

b=1
Xb ≤w −x

P (Xi = x)

≤sup
w
r−1

x=0
am = ram
and hence
d(
n

a=1
Xi, CP(λ, F))
≤
n

a=2
i−1

b=max{1,i−m+1}
(P(Xa > 0, Xb > 0) + P(Xa > 0)P(Xb > 0))
+1
2
n

i=1
P (Xi > 0)2 + nram.
Therefore, if {Xn} is a stationary Markov chain with state space {0, 1, ...,
r −1}, then [see Billingsley (1986, p. 375, 128)] the above inequality
is valid for am = arρm, for some a ≥0 and ρ ∈(0, 1).
Assuming
©2001 CRC Press LLC

furthermore that the Markov chain has positive transition probabilities
pij then am = rρm where ρ = 1 −r minij pij.
A ﬁnal case where the last summand in (4.3.1) can be easily con-
trolled (upper bounded) arises when the r.v.s X1, X2, ..., Xn exhibit cer-
tain types of positive or negative dependence. Thus, when we deal with
integer valued r.v.s and Xa, Xb, b ∈B′
a, a = 1, 2, ..., n are associated or
negatively associated then one could exploit Theorem 1 in Boutsikas and
Koutras (2000a) to deduce the upper bound
n

a=2
d(

b∈B′
a
Xb + Xa,

b∈B′
a
Xb + X′
a) ≤
n

a=2

Cov(

b∈B′
a
Xb, Xa)

=
n

a=2


b∈B′
a
Cov(Xb, Xa)

.
It is worth mentioning that when the Xa are binary r.v.s, then the
upper bound in (4.3.2) is of the form (b1 + b2 + b∗
3)/2 where b1 and
b2 coincide to the quantities involved in the Chen-Stein upper bound,
Arratia et al. (1989, 1990).
The following lemma can be considered as the main ingredient of the
proofs to follow.
LEMMA 4.3.2
For any nonnegative random variables X, Y, Z, U we have
|d(X + Y + Z, X + Y + U) −d(X + Z, X + U)|
≤P(Y > 0, Z > 0) + P(Y > 0, U > 0).
PROOF For every w ≥0 it is obvious that the following equality holds
true
P (X + Y + Z > w) −P (X + Z > w)
= P(X + Y + Z > w, X + Z ≤w)
= P( X + Z ≤w, X + Y > w)
+P(X + Y + Z > w, X + Z ≤w, X + Y ≤w)
= P(X + Y > w) −P(X + Z > w, X + Y > w, X ≤w) −P(X > w)
+P (X + Y + Z > w, X + Z ≤w, X + Y ≤w) .
©2001 CRC Press LLC

Applying the above equality for U instead of Z we deduce that
(P(X + Y + Z > w) −P(X + Z > w))
−(P(X + Y + U > w) −P(X + U > w))
= −P(X + Z > w, X + Y > w, X ≤w)
+P(X + Y + Z > w, X + Z ≤w, X + Y ≤w)
+P (X + U > w, X + Y > w, X ≤w)
−P (X + Y + U > w, X + U ≤w, X + Y ≤w) .
Hence
|P(X + Y + Z > w) −P(X + Y + U > w)|
≤|P(X + Z > w) −P(X + U > w)|
+P (X + Z > w, X + Y > w, X ≤w)
+P (X + Y + Z > w, X + Z ≤w, X + Y ≤w)
+P (X + U > w, X + Y > w, X ≤w)
+P (X + Y + U > w, X + U ≤w, X + Y ≤w)
≤|P(X + Z > w) −P(X + U > w)|
+P(Y > 0, Z > 0) + P(Y > 0, U > 0)
and considering the supremum with respect to w in both sides of the
inequality we deduce
d(X + Y + Z, X + Y + U) ≤d(X + Z, X + U) + P(Y > 0, Z > 0)
+P(Y > 0, U > 0).
By the same token we may get,
d(X + Z, X + U) ≤d(X + Y + Z, X + Y + U) + P(Y > 0, Z > 0)
+P(Y > 0, U > 0)
and the proof is complete.
THEOREM 4.3.3
The distance between the sum of nonnegative dependent variables Xi
and the sum of their independent duplicates X′
i (L(Xi) = L(X′
i), i =
1, 2, ..., n) is bounded from above by
©2001 CRC Press LLC

d(

a∈In
Xa,

a∈In
X′
a) ≤
n

a=2
(P(Xa > 0,

b∈Ba
Xb > 0)
+P(Xa > 0)P(

b∈Ba
Xb > 0))
+
n

a=2
d(

b∈B′
a
Xb + Xa,

b∈B′
a
Xb + X′
a).(4.3.3)
PROOF Applying Lemma 4.3.2 for the r.v.s 
b∈B′
n Xb, 
b∈Bn Xb, Xn, X′
n
we deduce
d(

b∈B′
n
Xb +

b∈Bn
Xb + Xn,

b∈B′
n
Xb +

b∈Bn
Xb + X′
n)
≤P(

b∈Bn
Xb > 0, Xn > 0) + P(

b∈Bn
Xb > 0)P(Xn > 0)
+d(

b∈B′
n
Xb + Xn,

b∈B′
n
Xb + X′
n)
(4.3.4)
Accordingly, for the r.v.s 
b∈B′
n−1 Xb + X′
n, 
b∈Bn−1 Xb, Xn−1, X′
n−1,
we have that
d((

b∈B′
n−1
Xb + X′
n) +

b∈Bn−1
Xb + Xn−1, (

b∈B′
n−1
Xb + X′
n)
+

b∈Bn−1
Xb + X′
n−1)
≤P(

b∈Bn−1
Xb > 0, Xn−1 > 0) + P(

b∈Bn−1
Xb > 0)P(Xn−1 > 0)
+d(

b∈B′
n−1
Xb + X′
n + Xn−1,

b∈B′
n−1
Xb + X′
n + X′
n−1).
(4.3.5)
By virtue of inequalities (4.3.5) and (4.2.1) we may write
d(
n−1

b=1
Xb + X′
n,
n−2

b=1
Xb +
n

a=n−1
X′
a)
≤(P(

b∈Bn−1
Xb > 0, Xn−1 > 0)
©2001 CRC Press LLC

+P(

b∈Bn−1
Xb > 0)P (Xn−1 > 0))
+d(

b∈B′
n−1
Xb + Xn−1,

b∈B′
n−1
Xb + X′
n−1)
(4.3.6)
and applying the triangle inequality (recall also relations (4.3.4),(4.3.6))
we obtain
d
 n

b=1
Xb,
n−2

b=1
Xb +
n

a=n−1
X′
a

≤
n

a=n−1
(P(Xa > 0,

b∈Ba
Xb > 0) + P(Xa > 0)P(

b∈Ba
Xb > 0))
+
n

a=n−1
d(

b∈B′
a
Xb + Xa,

b∈B′
a
Xb + X′
a).
The proof is easily completed on using induction
Should we wish to secure that the upper bound in (4.3.3) converges
to 0 and 
a∈In X′
a < ∞a.s.
as n →∞, it is necessary to impose
the conditions that P(Xa > 0) ≡pa tends to 0 for every a ∈In and
that 
a∈In pa →λ ∈(0, ∞) respectively.
Under these conditions it
is not hard to see that the distribution of 
a∈In X′
a converges to a
compound Poisson distribution. Wang (1989) proved an analogous result
for integer-valued r.v.s (his approach is based on characteristic function),
while Vellaisamy and Chaudhuri (1999) proposed a bound on the rate
of convergence. Let us now proceed to the next theorem which provides
a bound for the error (in terms of Kolmogorov distance) incurred when
approximating the distribution of a sum of independent nonnegative r.v.s
by a compound Poisson distribution.
THEOREM 4.3.4
If Xa, a = 1, 2, ..., n is a sequence of independent nonnegative r.v.s, then
d
 n

a=1
Xa, CP(λ, F)

≤1
2
n

i=1
p2
i
(4.3.7)
where pi = P(Xi > 0), λ = n
i=1 pi, and F(x) =
1
λ
n
i=1 piP(Xi ≤
x|Xi > 0), x ∈R.
©2001 CRC Press LLC

PROOF At ﬁrst assume that Xi are simple r.v.s i.e. Xi ∈{0, a1, a2, ...,
am}, ai > 0, ai ̸= aj for i ̸= j.
The sum of Xi can be alternatively
expressed as
n

i=1
Xi =
n

i=1
m

j=1
ajI[Xi=aj] =
n

i=1
m

j=1
ajYji
and employing Theorem 4.3.3 for the r.v.s ajYji, j = 1, 2, ..., m,
i =
1, 2, ..., n, and neighborhoods of strong dependence B(j,i) = {(s, i) : s <
j} we deduce
d(
n

i=1
m

j=1
ajYji,
n

i=1
m

j=1
ajY ′
ji)
≤
n

i=1
m

j=2
j−1

s=1
(P(ajI[Xi=aj] > 0, asI[Xi=as] > 0)
+P(ajI[Xi=aj] > 0)P(asI[Xi=aj] > 0))
=
n

i=1
m

j=2
j−1

s=1
P(Xi = aj)P(Xi = as)
= 1
2
n

i=1
P(Xi > 0)2 −1
2
n

i=1
m

j=1
P(Xi = aj)2.
Moreover, n
i=1
m
j=1 ajY ′
ji = m
j=1 aj
n
i=1 Y ′
ji = m
j=1 ajZj , where
Zj = n
i=1 Y ′
ji are independent r.v.s. following a generalized binomial
distribution with parameters paj = P(Xi = aj), i = 1, 2, ..., n. Therefore,
(see also (4.2.1)–(4.2.4))
d(
m

j=1
ajZj,
m

j=1
ajWj) ≤
m

j=1
d(ajZj, ajWj)
=
m

j=1
d(Zj, Wj) ≤1
2
m

j=1
n

i=1
P(Xi = aj)2
with Wj, j = 1, 2, ..., m being independent Poisson r.v.s such that λaj =
E(Wj) = n
i=1 P(Xi = aj). Combining the above inequalities we con-
clude that
d(
n

i=1
Xi,
m

j=1
ajWj) ≤1
2
n

i=1
P (Xi > 0)2
©2001 CRC Press LLC

The proof for the case of simple r.v.s Xi, i = 1, 2, ..., n is completed by
observing that the r.v. m
j=1 ajWj follows a compound Poisson distri-
bution. More speciﬁcally, the characteristic function of m
j=1 ajWj is
E(eit  m
j=1 ajWj) =
m
	
j=1
E(eitajWj) =
m
	
j=1
e−λaj (1−eitaj )
= exp

−λ(1 −
m

j=1
eitaj λaj
λ )


(λ = m
j=1 λaj = n
i=1 P(Xi > 0)) which can be identiﬁed as the char-
acteristic function of a compound Poisson distribution with parameter
λ and compounding distribution
F(x) =

j:aj≤x
λaj
λ
= 1
λ

j:aj≤x
n

i=1
P(Xi = aj) = 1
λ
n

i=1

j:aj≤x
P(Xi = aj)
= 1
λ
n

i=1
P(0 < Xi ≤x) =
1
n
i=1 pi
n

i=1
piP(Xi ≤x|Xi > 0)
(pi = P(Xi > 0)). Note that according to the last expression F(x) can
be interpreted as a weighted mixture of the distributions of
Xi|Xi > 0.
We shall now treat the general case, Xi > 0. Deﬁne
Ψs(x) =
 (k −1)2−s,
if (k −1)2−s ≤x < k2−s, 1 ≤k < s2s, k ∈Z
s,
ifx ≥s,
for s = 1, 2, ....
For each i = 1, 2, ..., n, the sequence of simple non-
negative r.v.s Ψs(Xi) converges (as s →∞) to Xi almost surely (a.s.)
[e.g. see Billingsley (1986, pp. 185, 259)]. From the ﬁrst part of the
proof it follows that, for each s = 1, 2, ...
d
 n

a=1
Ψs(Xa), CP(µs, Fs)

≤1
2
n

i=1
P (Ψs(Xi) > 0)2
≤1
2
n

i=1
P (Xi > 0)2
(4.3.8)
where µs = n
i=1 P(Ψs(Xi) > 0) and Fs(x) =
1
µs
n
i=1 P(0 < Ψs(Xi) ≤
x), x ∈R. It can be easily veriﬁed that
©2001 CRC Press LLC

P (Xi ≤w) ≤P(Ψs(Xi) ≤w) ≤P(Xi ≤w + 1
2s ), 0 ≤w < s
and the right continuity of P(Xi ≤w) guarantee that, as s →∞,
P(Ψs(Xi) ≤w) →P(Xi ≤w) foreveryw ≥0.
Thus, for every w ≥0, we have µss →∞→λ and
Fs(w) = 1
µs
n

i=1
P(0 < Ψs(Xi) ≤w)s →∞→1
λ
n

i=1
P(0 < Xi ≤w) = F(w)
which ascertain the convergence of the c.f. of CP(µs, Fs) to the c.f. of
CP(λ, F). Hence, by virtue of the continuity theorem,
CP(µs, Fs)(−∞, w] →CP(λ, F)(−∞, w]
at least for every continuity point w of H(w) = CP(λ, F)(−∞, w].
Since Ψs(Xi) s →∞→Xi a.s., it follows that n
a=1 Ψs(Xa)s →∞→n
a=1 Xa
a.s. and thus it converges in distribution. Hence
P(
n

a=1
Ψs(Xa) ≤w) →P(
n

a=1
Xa ≤w)
at least for every continuity point w of G(w) = P(n
a=1 Xa ≤w). Com-
bining the above results with (4.3.8) we conclude that
P(
n

a=1
Xa ≤w) −CP(λ, F)(−∞, w]
 ≤1
2
n

i=1
P (Xi > 0)2
(4.3.9)
for every w ∈CG ∩CH. (CF ⊂R denotes the set of continuity points of
F, C′
F = R\CF ). Since H, G are nondecreasing, it follows that the sets
C′
G, C′
H are countable. Let now w0 ∈(CG ∩CH)′. Since (CG ∩CH)′ =
C′
G ∪C′
H is countable, there exists a sequence wj ∈CG ∩CH, j = 1, 2, ...
such that wj ↓w0. Inequality (4.3.9) holds true for all wj and due to the
right-continuity of G, H it will hold true for w0 as well. Hence (4.3.9) is
valid for every w ∈R and the proof is complete.
©2001 CRC Press LLC

4.4
Examples of Applications
4.4.1
A Compound Poisson Approximation for Truncated
Moving Sums of i.i.d. r.v.s
Let X1, X2, ..., Xn be a sequence of i.i.d. nonnegative unbounded random
variables with a common distribution function F.
The moving sums
Si = i+r−1
j=i
Xj, i = 1, 2, ..., n −r + 1 will be referred to as the r -scan
process. Denote by Fm the distribution function of m
j=1 Xj, the m-fold
convolution of F. Fixing a threshold b, we can measure the exceeding
amount of the moving sums above b, by
A+
n (b) =
n−r+1

i=1
max{0, Si −b} =
n−r+1

i=1
I[Si>b](Si −b).
Rootzen, Leadbetter and De Haan (1998) studied the asymptotic dis-
tribution of tail array sums for strongly mixing stationary sequences.
Their results pertain to compound Poisson and Normal convergence the-
orems for A+
n (b) for very high and moderate levels of b respectively. Note,
though, that their approach does not provide any bounds or convergence
rates and in the high b level case the parameters (Poisson intensity λ,
compounding distribution G) of the limiting compound Poisson distri-
bution CP(λ, G) can not be explicitly described. The approach based on
Theorem 4.3.1 oﬀers an upper bound for the distance d(A+
n (b), CP(λ, G))
and an explicit description of λ, G for the case of very high levels of the
threshold b.
Dembo and Karlin (1992) studied the number of exceedances N +
n (b) =
n−r+1
i=1
I[Si>b] by the aid of the Stein-Chen method, and proved that,
under certain conditions, N +
n (b) converges to a Poisson distribution. In
order to exploit Theorem 4.3.1 to study the variable A+
n (b), let us intro-
duce the nonnegative r.v.s Yi = I[Si>b](Si −b), i = 1, 2, ..., n −r + 1 and
choose Bi as Bi = {max{i −r + 1, 1}, ..., i −1}. Then Yi are independent
of Yj, j ∈B′
i and a direct application of Theorem 4.3.1 in conjunction
with inequality (4.3.2) yields (note that in this case the last sum in the
RHS of (4.3.2) vanishes)
d

A+
n (b), CP(λ, G)

≤
n−r+1

i=2
i−1

j=max{i−r+1,1}

P(Yi > 0, Yj > 0) + P(Yi > 0)2
+1
2
n−r+1

i=1
P (Yi > 0)2
©2001 CRC Press LLC

≤
n−r+1

i=2
i−1

j=max{i−r+1,1}
P(Si > b, Sj > b) + (n −r + 1)rP(S1 > b)2
≤λ
r
m=2
P(Sm > b|S1 > b) + λr(1 −Fr(b)) := λϵ(r, b)                    (4.4.1)
where λ = n−r+1
i=1
P(Si > b) = (n−r+1)(1−Fr(b)). The compounding
distribution G is given by the expression
G(x) = P(Si ≤x + b|Si > b) = 1 −1 −Fr(b + x)
1 −Fr(b)
, x ≥0.
Observe that the upper bound in (4.4.1) has almost the same form as the
bound developed by Dembo and Karlin (1992) for the variable N +
n (b).
Consequently, many of their results pertaining to limiting behavior of
their upper bound can also be exploited here for the investigation on
A+
n (b)’s asymptotic distribution. Thus, resorting to Theorem 3 in Dembo
and Karlin (1992) we can easily deduce the following lemma.
LEMMA 4.4.1
If for each constant K > 0,
1 −F(b −K)
1 −F2(b)
→0 asb →∞,
(4.4.2)
then the term ϵ(r, b) appearing in the upper bound of ( 4.4.1) converges
to 0 for any ﬁxed r.
We can now state the next proposition which elucidates the asymptotic
behavior of A+
n (b).
PROPOSITION 4.4.2
Let X1, X2, ... be a sequence of i.i.d. nonnegative and unbounded r.v.s
with common distribution function F satisfying condition ( 4.4.2). If r
is ﬁxed and n, b →∞so that n(1 −Fr(b)) →λ ∈(0, ∞) and
1 −1 −Fr(b + x)
1 −Fr(b)
b →∞→G(x)
then
L(A+
n (b)) = L(
n−r+1

i=1
max{0, Si −b}) →CP(λ, G)
with the convergence rate given by (4.4.1).
©2001 CRC Press LLC

According to Dembo and Karlin (1992), condition (4.4.2) holds true
for any distribution function F which is a ﬁnite or inﬁnite convolution
of exponentials of any scale parameters or has a logconcave density.
As an example we shall examine the simplest case where Xi are ex-
ponentially distributed with parameter µ, i.e. F(x) = 1 −e−µx, x ≥0.
Then
1 −Fm(x) = e−µx
m−1

i=0
(µx)i
i!
, m = 1, 2, ...
and it is not diﬃcult to verify that condition (4.4.2) holds true. Indeed,
1 −F(b −K)
1 −F2(b)
=
e−µ(b−K)
e−µb(1 + µb) =
eµK
(1 + µb)b →∞→0
and applying Proposition 4.4.2 we conclude that if n, b →∞so that
ne−µb
r−1

i=0
(µb)i
i!
∼ne−µb (µb)r−1
(r −1)! →λ
(the symbol ∼indicates that the ratio of the two sides tends to 1) then
A+
n (b) converges to a compound Poisson distribution CP(λ, G) with
G(x) = 1 −lim
b→∞
1 −Fr(b + x)
1 −Fr(b)
= 1 −lim
b→∞
e−µ(b+x) r−1
i=0
(µ(b+x))i
i!
e−µb r−1
i=0
(µb)i
i!
= 1 −e−µx, x ≥0
(such a result can be easily interpreted by recalling the lack of memory
property of the exponential law). Should one be interested on an upper
bound for d (A+
n (b), CP(λ, G)) he could condition on r
i=m Xi in the
upper bound of (4.4.1) to deduce
(n −r + 1)
r

m=2
 b
0
(1 −Fm−1(b −x))2dFr−m+1(x) + 1 −Fr−m+1(b)

+(n −r + 1)r(1 −Fr(b))2
and apply Lemma 4.1 of Dembo and Karlin (1992) to get the upper
bound
©2001 CRC Press LLC

n(r-1)
 b
0 (1 −F(b −x))2dFr−1(x) + 1 −Fr−1(b)

+ nr(1 −Fr(b))2
=n(r-1)

µr−1e−µb
(r−2)!
 b
0 e−µ(b−x)xr−2dx + e−µb r−2
i=0
(µb)i
i!

+nr

e−µb r−1
i=0
(µb)i
i!
2
∼λ(r−1)2
br−1
 b
0 e−µ(b−x)xr−2dx + λ(r−1)2
µb
+ r
nλ2.
Since
1
br−2
 b
0
e−µ(b−x)xr−2dx
=
r−2

i=0
r −2i(−1)i
bi
 b
0
e−µyyidy
=
r−2

i=0
r −2i (−1)i
µi+1bi
 µb
0
e−zzidy ≤
r−2

i=0
r −2i
i!
µi+1bi
the upper bound in (4.4.1) tends to 0, its rate of convergence being at
least
λ(r −1)2
b
r−2

i=0
r −2i
i!
µi+1bi + λ(r −1)2
µb
+ r
nλ2
∼2λ(r −1)2
µb
+ r
nλ2 = O(b−1).
4.4.2
The Number of Overlapping Success Runs in a
Stationary Two-State Markov Chain
Let {Xi}i=0,1,...be a stationary Markov chain with state space {0, 1}
and denote by W the number of overlapping success runs of length k
(k consecutive 1’s) in the sequence X1, X2, ..., Xn, i.e.
W =
n−k+1

i=1
k
	
j=1
Xi+j−1
The problem of approximating the distribution of W has been recently
addressed by Geske et al. (1995). The approach considered there was
based on the classical Stein-Chen method. We shall give here an alterna-
tive approach based on Theorem 4.3.1 which oﬀers slightly better results.
©2001 CRC Press LLC

The one step transition matrix P =(P(Xi = j|Xi−1 = k))kj = (pkj)kj of
the Markov chain equals
P =
 p00
p01
p10
p11

,
p00 + p01 = p10 + p11 = 1, pij ∈(0, 1)
with its i-step transition matrix given by
P(i) =

p(i)
00 p(i)
01
p(i)
10 p(i)
11

= Pi =
1
p01 + p10
p10 p01
p10 p01

+ (p11 −p01)i
p01 + p10
 p01 −p01
−p10 p10

.
The stationary distribution of the Markov chain (which, due to the sta-
tionarity assumption, coincides with the initial distribution) reads
p0 =
p10
p01 + p10
,
p1 =
p01
p01 + p10
.
In order to exploit Theorem 4.3.1 let us denote by W0 the sum of
Yi = (1 −Xi−1)
k−1

r=0
i+k+r−1
	
j=i
Xj,
i = 1, 2, ..., n −k + 1
which represent the truncated number of success runs within a clump
starting at position i. Manifestly, if there exist j ≥k consecutive suc-
cesses stretching from position i to position i + j −1, bordered at each
end by failures, then Yi = min{j −k + 1, k}.
The distance between W and an appropriate compound Poisson distri-
bution will be assessed by establishing ﬁrst upper bounds for d(W, W0),
d(W0, CP) and applying next the triangle inequality
d(W, CP) ≤d(W, W0) + d(W0, CP)
(4.4.3)
An upper bound for d(W, W0) is oﬀered by
d (W0, W) ≤P
n−k+1

i=1
Yi ̸= W

≤P
n−2k+1

i=1
[Xi−1 = 0, Xi = ... = Xi+2k−1 = 1]

+P (X0 = ... = Xk = 1)
©2001 CRC Press LLC

+P

n−k+1

i=n−2k+3
[Xi−1 = 0, Xi = ... = Xn+1 = 1]

≤(n −2k + 1)p0p01p2k−1
11
+ p1pk
11 + p0p01pk
11
1 −pk−1
11
1 −p11
≤(n −2k + 1)p0p01p2k−1
11
+ 2pk
11p1.
(4.4.4)
On the other hand, we could resort to Theorem 4.3.1 to establish an
upper bound for d(W0, CP).
Note ﬁrst that in this case it is quite
intricate to determine the left neighborhoods of strong dependence for
Yi, i = 1, 2, ..., n −k + 1 in an optimal way, i.e. so that the upper bound
is minimized. Nonetheless, it seems plausible to consider
Bi = {max{1, i −s}, ..., i −1} for an arbitrary s ≥2k −1 and apply
Theorem 4.3.1 to gain the inequality (recall also inequality (4.3.2))
d (W0, CP(λ, F))
≤
n−k+1

i=2
i−1

b=max{1,i−s}
(P(Yi > 0, Yb > 0) + P(Yi > 0)P(Yb > 0))
+
n−k+1

i=s+2
d(
i−s−1

b=1
Yb + Yi,
i−s−1

b=1
Yb + Y ′
i ) +
n−k+1

i=1
P (Yi > 0)2
(4.4.5)
where λ = n−k+1
i=1
P(Yi > 0), and F(x) = P(Yi ≤x|Yi > 0), x ∈R.
Now, it is clear that
P(Yi > 0) = P(Xi−1 = 0,
i+k−1
	
j=i
Xj = 1) = p0p01pk−1
11 ,
i = 1, 2, ..., n −k + 1,
P (Yi > 0, Yb > 0) = 0,
b = i −k, ..., i −1
P (Yi > 0, Yb > 0) ≤P(Xb−1 = 0,
b+k−1
	
j=b
Xj = 1, Xi−1 = 0,
i+k−1
	
j=i
Xj = 1)
= p0p01pk−1
11 p(i−b−k)
10
p01pk−1
11
≤p0p2
01p2k−2
11
,
b = i −s, ..., i −k −1
(4.4.6)
and
©2001 CRC Press LLC

λ = (n −k + 1)p0p01pk−1
11 ,
F(x) = 1 −px
11, x = 1, 2, ..., k −1. (4.4.7)
The mid term of the RHS of inequality (4.4.5) could be bounded by a
method similar to the ones described after the statement of Theorem
4.3.1. However, since in this example we are dealing with the simple
case of a two-state Markov chain, we chose to use a direct approach that
yields tighter bounds. Note ﬁrst that
P
i−s−1

b=1
Yb + Yi ≤w

−P
i−s−1

b=1
Yb + Y ′
i ≤w

≤
min{w,k}

y=0
P(
i−s−1

b=1
Yb ≤w −y, Yi = y)
−P(
i−s−1

b=1
Yb ≤w −y)P(Y ′
i = y)

≤
min{w,k}

y=0
1

x=0
|P(
i−s−1

b=1
Yb ≤w −y, Yi = y, Xi−s+2k−3 = x)
−P(
i−s−1

b=1
Yb ≤w −y, Xi−s+2k−3 = x)P(Yi = y)|
and use the Markov property to write the RHS as
min{w,k}

y=0
1

x=0
|P(Yi = y|Xi−s+2k−3 = x) −P(Yi = y)|
×P(
i−s−1

b=1
Yb ≤w −y, Xi−s+2k−3 = x)
≤
k

y=0
1

x=0
|P(Yi = y|Xi−s+2k−3 = x) −P(Yi = y)| P(Xi−s+2k−3 = x)
where
|P(Yi = 0|Xi−s+2k−3 = x) −P(Yi = 0)|
= |p(s−2k+2)
x0
−p0|p01pk−1
11
©2001 CRC Press LLC

= p1−x p01pk−1
11 |p11 −p01|s−2k+2,
|P(Yi = y|Xi−s+2k−3 = x) −P(Yi = y)|
= |p(s−2k+2)
x0
−p0|p01pk+y−2
11
p10
= p1−x p01pk+y−2
11
p10|p11 −p01|s−2k+2,
|P(Yi = k|Xi−s+2k−3 = x) −P(Yi = k)|
= |p(s−2k+2)
x0
−p0|p01p2k−2
11
= p1−x p01p2k−2
11
|p11 −p01|s−2k+2,
for x = 0, 1 and y = 1, ..., k −1. The validity of the next inequality is
now evident
d(
i−s−1

b=1
Yb + Yi,
i−s−1

b=1
Yb + Y ′
i )
≤
k

y=0
1

x=0
|P(Yi = y|Xi−s+2k−3 = x) −P(Yi = y)| px
= 2p0p1p01pk−1
11
|p11 −p01|s−2k+2 (1 + p10
k−1

y=1
py−1
11
+ pk−1
11 )
= 4p0p1p01pk−1
11
|p11 −p01|s−2k+2 .
(4.4.8)
A combined use of (4.4.3)-(4.4.8) produces the next inequality, for
every s ≥2k −1,
d (W, CP(λ, F))
≤(n −k)(s −k)p0p2
01p2k−2
11
+ (n −k)sp2
0p2
01p2k−2
11
+(n −k −s)4p0p1p01pk−1
11
|p11 −p01|s−2k+2 + (n −k + 1)p2
0p2
01p2k−2
11
+(n −2k + 1)p0p01p2k−1
11
+ 2pk
11p1
≤λ((s −k)p01pk−1
11
+ (s + 1)p0p01pk−1
11
+ pk
11 + 4p1 |p11 −p01|s−2k+2)
+2pk
11p1.
Several simpler upper bounds can be conferred by considering certain
special values for s. For example if s = 3k −1 we get
d (W, CP(λ, F)) ≤λ((2k −1)p01pk−1
11
+ 3kp0p01pk−1
11
+ pk
11
+4p1 |p11 −p01|k+1) + 2pk
11p1
©2001 CRC Press LLC

which oﬀers an upper bound of the same order but of better quality
than Geske’s et al.
(1995) corresponding bound (the latter performs
better only when p01 →0). Obviously, if n, k →∞so that (n −k +
1)p0p01pk−1
11
→λ0 then the upper bound vanishes and W converges to a
compound Poisson distribution at a convergence rate of order O(kpk
11).
Since F(x) is now describing a geometric distribution, W will follow
(asymptotically) a P´olya-Aeppli distribution.
Acknowledgements
Research of the ﬁrst author was supported by
the National Scholarship Foundation of Greece.
References
1. Arratia, R., Goldstein, L., and Gordon, L. (1989). Two moments
suﬃce for Poisson approximations: The Chen-Stein method. An-
nals of Probability 17, 9–25.
2. Arratia, R., Goldstein, L., and Gordon, L. (1990). Poisson Ap-
proximation and the Chen-Stein method.
Statistical Science 5,
403–434.
3. Barbour, A. D., Chen, L. H. Y., and Loh, W. L. (1992). Com-
pound Poisson approximation for nonnegative random variables
via Stein’s method. The Annals of Probability 20, 1843–1866.
4. Barbour, A. D., Holst, L., and Janson, S. (1992). Poisson Approx-
imation. Clarendon Press, Oxford.
5. Barbour, A. D. and Utev, S. (1998). Solving the Stein equation in
compound Poisson approximation. Advances in Applied Probability
30, 449–475.
6. Billingsley, P. (1986).
Probability and Measure.
John Wiley &
Sons, New York .
7. Boutsikas, M. V. and Koutras, M. V. (2000a). A bound for the
distribution of the sum of discrete associated or NA r.v.’s. The
Annals of Applied Probability (to appear).
8. Boutsikas, M. V. and Koutras, M. V. (2000b). On the number of
overﬂown urns and excess balls in an allocation model with limited
urn capacity.
Journal of Statistical Planning and Inference (to
appear).
9. Chen, L. H. Y. (1975). Poisson approximation for dependent trials.
The Annals of Probability 3, 534–545.
10. Dembo, A. and Karlin, S. (1992). Poisson approximations for r-
©2001 CRC Press LLC

scan processes. The Annals of Applied Probability 2, 329–357.
11. Erhardsson, T. (1999).
Compound Poisson approximations for
Markov chains using Steins method. Annals of Probability 27, 565–
595.
12. Geske, M. X., Godbole, A. P., Schaﬀner, A. A., Skolnick, A. M.,
and Wallstrom, G. L. (1995). Compound Poisson approximations
for word patterns under Markovian hypotheses. Journal of Applied
Probability 32, 877–892.
13. Roos, M. (1994). Stein’s method for compound Poisson approxi-
mation: the local approach. The Annals of Applied Probability 4,
1177–1187.
14. Rootzen, H., Leadbetter, M. R., and De Haan, L. (1998).
On
the distribution of tail array sums for strongly mixing stationary
sequences. Annals of Applied Probability 8, 868–885.
15. Serﬂing, R. J. (1978).
Some elementary results on Poisson ap-
proximations in a sequence of Bernoulli trials. SIAM Review 20,
567–579.
16. Vellaisamy, P. and Chaudhuri, B. (1999). On compound Poisson
approximation for sums of random variables. Statistics & Proba-
bility Letters 41, 179–189.
17. Wang, Y. H. (1989). From Poisson to compound Poisson approxi-
mations. The Mathematical Scientist 14, 38–49.
18. Xia, A. (1997).
On using the ﬁrst diﬀerence in the Stein-Chen
method. The Annals of Applied Probability 4, 899–916.
©2001 CRC Press LLC

5
Uniﬁed Variance Bounds and a
Stein-Type Identity
N. Papadatos and V. Papathanasiou
University of Cyprus, Nicosia, Cyprus
University of Athens, Athens, Greece
ABSTRACT Let X be an absolutely continuous (a.c.) random vari-
able (r.v.) with ﬁnite variance σ2. Then, there exists a new r.v. X∗
(which can be viewed as a transform on X) with a unimodal density,
satisfying the extended Stein-type covariance identity
Cov[X, g(X)] = σ2IE[g′(X∗)]
for any a.c. function g with derivative g′, provided that IE|g′(X∗)| < ∞.
Properties of X∗are discussed and, also, the corresponding uniﬁed upper
and lower bounds for the variance of g(X) are derived.
Keywords and phrases Stein-type identity, variance bounds, trans-
formation of random variables
5.1
Introduction
The well-known Stein’s identity, Stein (1972, 1981), for the standard
normal r.v. Z is formulated as follows. For every absolutely continuous
(a.c.) function g with derivative g′ such that IE|g′(Z)| < ∞,
IE[Zg(Z)] = IE[g′(Z)]
(5.1.1)
(throughout this paper, the term ‘a.c.’=‘absolutely continuous’ will be
used either to describe an r.v. having a density with respect to Lebesgue
measure on IR, or to denote an ordinary a.c. function; in any case, the
meaning will be clear from the context). This identity has had many
important applications in several areas of Probability and Statistics; see
for example Stein (1972), Hudson (1978) and Liu (1994). Several gen-
eralizations to other r.v.s can be found in Cacoullos and Papathanasiou
(1989, 1995) and Hudson (1978).
©2001 CRC Press LLC

Another interesting result from the point of view of upper variance
bounds is the inequality of Chernoﬀ, [Chernoﬀ(1981)],
Var[g(Z)] ≤IE[(g′(Z))2]
(5.1.2)
with equality iﬀ(if and only if) g is linear. This and its multivariate nor-
mal analogue, Chen (1982), motivated the generalizations to arbitrary
discrete and continuous r.v.s as well as corresponding lower variance
bounds, Cacoullos (1982) and Cacoullos and Papathanasiou (1985).
An important role of the general covariance identity given in Cacoullos
and Papathanasiou (1989) (see (5.2.4), below) is in the derivation of sim-
ple and elementary proofs of the Central Limit Theorem [see Cacoullos,
Papathanasiou and Utev (1992, 1994)]. Other applications concerning
the rate of convergence in the Local Limit Theorem for sums of indepen-
dent r.v.s are given in Cacoullos, Papadatos and Papathanasiou (1997).
However, all the preceding results [and, in particular, the validity of
the generalized covariance identity given in Cacoullos and Papathanasiou
(1989) and the variance bounds given in Cacoullos and Papathanasiou
(1985, 1989)] require an interval support of the basic r.v. X, which is
rather restrictive.
In the present paper we avoid the restriction of an interval support,
introducing an appropriate (smooth) transformation X∗, which is in fact
a new r.v. corresponding to X. The r.v. X∗is always uniquely deﬁned
(provided that X is a.c. with ﬁnite variance), having itself an absolutely
continuous unimodal density. Thus, for any a.c. r.v. X with ﬁnite second
moment, there exists a new smooth r.v. X∗with unimodal a.c. density
satisfying the generalized Stein covariance identity. This transformation
behaves well to convolutions of independent r.v.s. Moreover, it appears
in the upper and lower bounds for the variance of any a.c. function g of
X (and it is, in fact, the only r.v. with this property; Theorem 5.3.3).
It should be noted that in a recent paper, independently of our results,
Goldstein and Reinert (1997) used a similar approach (the so-called zero
bias transformation), which, in fact, is based on the same covariance
identity when IE[X] = 0. They also fruitfully applied this identity to
estimate the rate of convergence in the CLT, obtaining an O(n−1) bound
for smooth functions [see Goldstein and Reinert (1997, Corollary 3.1)].
Furthermore, they presented a nice application for dependent samples.
However, except of the deﬁnition, the results of the present paper are
completely diﬀerent; our main interest is on uniﬁed variance bounds and
their connection with Stein’s identity.
We are also interested on the
behavior of the inverse transform X∗→X; in fact we show that, under
general conditions, the distribution of X∗uniquely determines that of
X (Theorems 5.2.2 and 5.3.4). Finally, we also include some illustrative
examples.
©2001 CRC Press LLC

5.2
Properties of the Transformation
Let X be an a.c. r.v. with density f, mean µ, variance σ2 and support
S(X) (for the sake of simplicity, we will always mean the support of an
a.c. r.v. X with density f to be the set S(X) = {x : f(x) > 0}). We
simply deﬁne X∗to be a random variable with density f ∗, given by the
relation
f ∗(x) = 1
σ2
 x
−∞
(µ −t)f(t) dt = 1
σ2
 ∞
x
(t −µ)f(t) dt.
(5.2.1)
Obviously the right hand sides of (5.2.1) are equal and thus, f ∗is non-
negative. An application of Tonelli’s Theorem shows that f ∗integrates
to 1, and therefore it is indeed a probability density [c.f. Cacoullos and
Papathanasiou (1989) and Cacoullos and Papathanasiou (1989)]. The
following Lemma summarizes the properties of X∗and shows the gen-
eralized Stein’s identity (5.2.2).
LEMMA 5.2.1
(i)
f ∗is a unimodal a.c. density with mode µ and maximal value
f ∗(µ) = IE|X −µ|
2σ2
.
Furthermore, the function (f ∗(x))′/(µ −x) (deﬁned almost every-
where) is nonnegative and integrable.
(ii)
For each a.c. function g with IE|g′(X∗)| < ∞,
Cov[X, g(X)] = σ2IE[g′(X∗)].
(5.2.2)
(iii)
If the r.v. Y satisﬁes the identity
Cov[X, g(X)] = σ2IE[g′(Y )]
for every a.c. g with IE|g′(Y )| < ∞, then Y
d= X∗(in the sense
that Y and X∗have the same distribution).
(iv)
S(X∗) = (ess inf S(X), ess sup S(X)).
(v)
For arbitrary scalars a ̸= 0 and b,
(aX + b)∗d= aX∗+ b.
(vi)
For independent a.c. r.v.s X1, X2 with means µ1, µ2, variances
σ2
1, σ2
2 and arbitrary scalars a1 and a2 with a1a2 ̸= 0,
(a1X1+a2X2)∗d= B(a1X∗
1 +a2X2)+(1−B)(a1X1+a2X∗
2), (5.2.3)
©2001 CRC Press LLC

where X1, X2, X∗
1, X∗
2 and B are mutually independent with
Pr[B = 1] =
a2
1σ2
1
a2
1σ2
1 + a2
2σ2
2
= 1 −Pr[B = 0].
PROOF (i), (iv) and (v) are obvious. (ii) follows from the deﬁnition of
X∗using Fubini’s theorem, since by the assumption that IE|g′(X∗)| <
∞, the nonnegative functions g1(x, t) = |g′(x)|(µ −t)f(t)I(t < x) and
g2(x, t) = |g′(x)|(t−µ)f(t)I(t > x) are integrable over (−∞, µ]×(−∞, µ]
and [µ, ∞) × [µ, ∞), respectively [see also Lemma 3.1 in Cacoullos and
Papathanasiou (1989)]. (iii) follows from (ii) and the fact that
IE[G(X∗)] = IE[G(Y )]
for every bounded (measurable) function G.
Regarding (vi), observe
that for S = a1X1 + a2X2 and g′ bounded, we have from (ii):
Cov[S, g(S)] = σ2IE[g′(S∗)],
where σ2 = a2
1σ2
1 + a2
2σ2
2. On the other hand,
Cov[S, g(S)] = a1Cov[X1, g(S)] + a2Cov[X2, g(S)]
= a2
1σ2
1 IE[g′(a1X∗
1 + a2X2)] + a2
2σ2
2 IE[g′(a1X1 + a2X∗
2)].
It follows that for any bounded function G,
IE[G(S∗)] = a2
1σ2
1
σ2 IE[G(a1X∗
1 + a2X2)] + a2
2σ2
2
σ2 IE[G(a1X1 + a2X∗
2)]
= IE [G (B(a1X∗
1 + a2X2) + (1 −B)(a1X1 + a2X∗
2))] ,
which completes the proof.
It should be noted that in the previous Lemma, and elsewhere in this
paper, the term ‘unimodal’ is reserved to denote a function h : IR →IR
with the property that there exists some m ∈IR such that h(x) is
nondecreasing for x ≤m and is nonincreasing for x ≥m; each m with
this property is called a ‘mode’ of h. Of course, the assertion that h is a
unimodal and a.c. function implies that the mode(s) of h form a compact
interval [a, b] with −∞< a ≤b < ∞(which, in the case a = b, reduces
to single point).
The known identity of Cacoullos and Papathanasiou (1989), (5.2.4) be-
low, requiring an interval support of X, follows immediately from (5.2.2).
©2001 CRC Press LLC

Indeed, when S(X) is a (ﬁnite or inﬁnite) interval, S(X) can always be
taken to be open, and then S(X) = S(X∗). Thus, for the nonnegative
function w(x) = f ∗(x)/f(x) (deﬁned on S(X)), (5.2.2) becomes
Cov[X, g(X)] = σ2IE[w(X)g′(X)].
(5.2.4)
The identity (5.2.2) remains valid for any nondecreasing (or nonin-
creasing) a.c. function g, even in the case where IE[g′(X∗)] = ∞(or −∞
if g is nonincreasing), as it follows by an application of Tonelli’s (instead
of Fubini’s) theorem. In this case, IE[(X −µ)(g(X) −g(µ))] = ∞(or
−∞). If, however, IE|g′(X∗)| = ∞and g is arbitrary, it may happen
that IE|(X −µ)(g(X) −g(µ))| < ∞, as the following example shows.
Example 1
Assume that X has density f(x) = (3/8) min{1, x−4}, −∞< x < ∞
and
g(x) =
∞

n=1
(2n −1)(|x| −a2n−2)I (a2n−2 ≤|x| < a2n−1)
+
∞

n=1
2n(a2n −|x|)I (a2n−1 ≤|x| < a2n) ,
where a0 = 0 and an = 1 + 1/2 + · · · + 1/n for n ≥1. It follows that
0 ≤g(x) ≤1 and g(−x) = g(x) for all x. Moreover, g is a.c. with deriva-
tive g′(x) (outside the set {0, ±a1, ±a2, . . .}) satisfying |g′(x)| ≥|x| ≥
|g(x)| = g(x) for almost all x. Since IE[X] = 0, IE|X| = 3/4, Var[X] = 1
and S(X) = (−∞, ∞), we conclude that X∗has the density f ∗(x) =
(3/16)

(2 −x2)I(|x| ≤1) + x−2I(|x| > 1)

supported by the entire real
line, and IE|g′(X∗)| ≥IE|X∗| = ∞, while IE|Xg(X)| ≤IE|X| = 3/4.
We have shown in Lemma 5.2.1(iii) that X∗is the only r.v. satisfying
the identity (5.2.2), and thus, an equivalent deﬁnition of X∗could be
given via the covariance identity; the latter approach is due to Goldstein
and Reinert [Goldstein and Reinert (1997, Deﬁnition 1.1)], who proved
that such a transformation is uniquely deﬁned by this identity. Moreover,
their approach extends to r.v.s that are not necessarily a.c., e.g., for the
symmetric Bernoulli r.v. X taking the values ±1 with probability 1/2,
X∗is uniformly distributed over the interval (−1, 1).
Our approach,
however, is restricted to a.c. r.v.s; for this reason, the analytic deﬁnition
(2.1) is possible and, moreover, the density f ∗itself turns out to be an
a.c. unimodal function.
©2001 CRC Press LLC

Our results also go to the opposite direction; the following Theorem
shows that, in general, the distribution of X∗uniquely determines that
of X.
THEOREM 5.2.2
Assume that the r.v. Y has a unimodal a.c. density h.
(i)
If the mode m of h is unique (i.e., h(x) < h(m) for all x ̸= m), there
exists an r.v. Xm such that X∗
m
d= Y iﬀthe function h′(x)/(m−x)
is integrable. Moreover, X∗
1
d= Y and X∗
2
d= Y implies X1
d= X2
(and thus, Xm is unique).
(ii)
If {x : x is a mode of h} = [a, b] with a < b, then for each µ ∈
(a, b), there always exists a unique r.v. Xµ such that IE[Xµ] = µ
and X∗
µ
d= Y . Moreover, for µ = a or µ = b, there exists an r.v. Xµ
such that IE[Xµ] = µ and X∗
µ
d= Y iﬀthe function h′(x)/(µ −x) is
integrable. Finally, if X∗
1
d= Y , X∗
2
d= Y and IE[X1] = IE[X2], then
X1
d= X2.
PROOF (i) If X is an r.v. with mean µ variance σ2 and density f such
that X∗d= Y , it follows from Lemma 5.2.1(i) that µ must be a mode of
f ∗= h and that h′(x)/(µ−x) is integrable. Thus, µ = m and h′(x)/(m−
x) is integrable. Assume now that the function h′(x)/(m −x) (deﬁned
almost everywhere) is integrable. Observe that it is also nonnegative
(because m is the mode of h) and deﬁne the r.v. Xm with density
fm(x) =
h′(x)
c(m −x), where c =
 ∞
−∞
h′(x)
m −x dx > 0.
Since lim±∞h(x) = 0 (because h is a unimodal density), we have IE[m−
Xm] = 0. Applying Tonelli’s Theorem we have
 m
−∞
(m −x)h′(x) dx =
 m
−∞
 m
x
h′(x) du dx =
 m
−∞
h(u) du,
and similarly,
 ∞
m
(x −m)(−h′(x)) dx =
 ∞
m
h(u) du,
yielding Var[Xm] = 1/c. Therefore, X∗
m
d= Y . We now show that Xm
is unique. Indeed, if X∗d= Y for an r.v. X with mean µ, variance σ2
and density f, it follows from Lemma 5.2.1(i) that µ must be a mode of
©2001 CRC Press LLC

f ∗= h and therefore µ = m. Hence,
f(x) = σ2h′(x)
m −x = cσ2fm(x)
for almost all x, and thus X d= Xm.
(ii) Since h is constant in [a, b], h′ ≡0 in (a, b). Hence, for all µ ∈(a, b),
 ∞
−∞
h′(x)
µ −x dx =
 a
−∞
h′(x)
µ −x dx +
 ∞
b
−h′(x)
x −µ dx ≤h(a)
µ −a + h(b)
b −µ < ∞.
Consider the r.v. Xµ with density
fµ(x) =
h′(x)
cµ(µ −x), where cµ =
 ∞
−∞
h′(x)
µ −x dx.
Then IE[Xµ] = µ, Var[Xµ] = 1/cµ, and therefore X∗
µ
d= Y . It is easy to
see that if X∗d= Y and IE[X] = µ for some r.v. X with density f and
variance σ2, then X d= Xµ. Indeed, it follows from (5.2.1) and Lemma
5.2.1(i) that
f(x) = σ2h′(x)
µ −x
= cµσ2fµ(x)
for almost all x.
Therefore, since the mean of any r.v. X satisfying
X∗
d= Y must be a mode of h, either IE[X] = µ ∈(a, b) (and thus
X d= Xµ) or IE[X] = a (and h′(x)/(a −x) is integrable) or IE[X] = b
(and h′(x)/(b −x) is integrable).
The following example shows that all the cases described by Theorem
5.2.2 are possible.
Example 2
(i) Assume that Y has the unimodal a.c. density
h(x) = 1
3 (min {2 −|x|, 1})+
with derivative h′(x) = −(1/3)sign(x)I(1 < |x| < 2) for almost all x.
Then, for all µ ∈(−1, 1), the r.v. Xµ with density
fµ(x) = (3cµ|x −µ|)−1 I(1 < |x| < 2), where cµ = 1
3 log
4 −µ2
1 −µ2

,
satisﬁes IE[Xµ] = µ and X∗
µ
d= Y . Moreover, h′(x)/(µ −x) is not inte-
grable for µ = ±1.
©2001 CRC Press LLC

(ii) Assume that Y has the unimodal a.c. density
h(x) = 6
19 (min {x + 2, 1} I(−2 < x < 1) + x(2 −x)I(1 ≤x < 2)) .
Then, for any µ ∈(−1, 1], the r.v. Xµ with density
fµ(x) =
6
19cµ|x −µ| (I(−2 < x < −1) + 2(x −1)I(1 < x < 2)) ,
where
cµ = (6/19) (2 + log [(µ + 2)/(µ + 1)] −2(1 −µ) log [(2 −µ)/(1 −µ)]) ,
satisﬁes IE[Xµ] = µ and X∗
µ
d= Y , while h′(x)/(−1−x) is not integrable.
Similarly, for the r.v. W = −Y with density h(−x), there exists an r.v.
Rµ such that IE[Rµ] = µ and R∗
µ
d= W iﬀµ ∈[−1, 1).
(iii) For the r.v. Y with density
h(x) = 3
10 (|x|(2 −|x|)I(1 < |x| < 2) + I(|x| ≤1)) ,
there exists an r.v. Xµ such that IE[Xµ] = µ and X∗
µ
d= Y for any mode
µ ∈[−1, 1].
5.3
Application to Variance Bounds
Upper bounds for the variance of a function g(X) of a normal r.v. X
in terms of g′ are known as the inequality of Chernoﬀ[Chernoﬀ(1981)]
[see also Chen (1982) and Vitale (1989)].
Upper and lower variance
bounds of g(X) for an arbitrary r.v. X were considered in Cacoullos
(1982) and Cacoullos and Papathanasiou (1985) [see also Cacoullos and
Papathanasiou (1989), Cacoullos and Papathanasiou (1995) and refer-
ences therein]. Both upper and lower variance bounds may be obtained
as by-products of the Cauchy-Schwarz inequality. The following Lemma
summarizes and uniﬁes these bounds in terms of the r.v. X∗; in ef-
fect, (5.3.1) is a Chernoﬀ-type, Chen (1982), upper bound; (5.3.2) is a
Cacoullos-type, Papathanasiou (1990, 1993), lower bound as obtained in
Cacoullos (1982) and Cacoullos and Papathanasiou (1985), in terms of
a function w (see also (5.3.4) below).
LEMMA 5.3.1
Let X be an a.c. r.v. with mean µ and variance σ2. Then, for every a.c.
function g with derivative g′, we have the following bounds.
©2001 CRC Press LLC

(i)
IE[(g(X) −g(µ))2] ≤σ2IE[(g′(X∗))2],
(5.3.1)
with equality iﬀeither IE[g2(X)] = ∞or
g(x) −g(µ) =
 a1(x −µ)
if x ≤µ,
a2(x −µ)
if x ≥µ,
for some constants a1, a2 and for all x ∈S(X∗).
(ii)
If IE|g′(X∗)| < ∞,
Var[g(X)] ≥σ2IE2[g′(X∗)],
(5.3.2)
with equality iﬀPr[g(X) = aX + b] = 1 for some constants a and
b.
PROOF (i) Let f be a density of X. We then have
σ2IE[(g′(X∗))2] =
 µ
−∞
(g′(x))2
 x
−∞
(µ −t)f(t) dt dx
+
 ∞
µ
(g′(x))2
 ∞
x
(t −µ)f(t) dt dx
=
 µ
−∞
f(t)(µ −t)
 µ
t
(g′(x))2 dx dt
+
 ∞
µ
f(t)(t −µ)
 t
µ
(g′(x))2 dx dt
≥
 µ
−∞
f(t) (g(µ) −g(t))2 dt
+
 ∞
µ
f(t) (g(t) −g(µ))2 dt
= IE[(g(X) −g(µ))2],
from Tonelli’s theorem and the Cauchy-Schwarz inequality for integrals.
Observe that if IE[g2(X)] = ∞, the equality holds in a trivial way (∞=
∞); otherwise, the equality holds iﬀthere exist constants a1 and a2 such
that g′(x) = a1 + (a2 −a1)I(x ≥µ) for almost all x ∈S(X∗), which
completes the proof.
(ii) We have from (5.2.2),
σ4IE2[g′(X∗)] = Cov2[X, g(X)] ≤σ2Var[g(X)]
by the Cauchy-Schwarz inequality for r.v.s, and the proof is complete.
©2001 CRC Press LLC

COROLLARY 5.3.2
For every a.c. function g,
Var[g(X)] ≤σ2IE[(g′(X∗))2],
(5.3.3)
with equality iﬀeither IE[g2(X)] = ∞or g is linear on S(X∗).
Note that (5.3.2) continues to hold for any nondecreasing (nonincreas-
ing) a.c. function g, even in the case where IE[g′(X∗)] = ±∞(in this
case, IE[g2(X)] = ∞). Moreover, since Pr[X ∈S(X∗)] = 1 (because the
measure produced by X is absolutely continuous with respect to that
produced by X∗), equality in (5.3.3) implies the equality in (5.3.2). The
converse is not always true, as the following example shows.
Example 3
Let X be uniformly distributed over (−2, −1) ∪(1, 2) and
g(x) = xI(|x| > 1) + x3I(|x| ≤1).
Then µ = 0, σ2 = 7/3 and X∗has the density
f ∗(x) = (3/28)

min{4 −x2, 3}
	+ .
It follows that Var[g(X)] = IE[(g(X)−g(µ))2] = σ2 and σ2IE2[g′(X∗)] =
Var[g(X)] < σ2IE[(g′(X∗))2] = 53/15. This shows that g is ‘linear’ with
respect to the measure produced by X, and it is ‘nonlinear’ with respect
to the measure produced by X∗.
This example hinges on the fact that S(X) fails to be an interval.
If, however, S(X) is a (ﬁnite or inﬁnite) interval, the known upper and
lower bounds for the variance of g(X) take the form [see Cacoullos and
Papathanasiou (1985, 1989)]
σ2IE2[w(X)g′(X)] ≤Var[g(X)] ≤σ2IE[w(X)(g′(X))2],
(5.3.4)
for some nonnegative function w deﬁned on S(X) (in fact, w = f ∗/f),
where both equalities hold iﬀg is linear on S(X), provided that
IE[w(X)(g′(X))2] < ∞.
The following result shows the equivalence between the variance bounds
and the covariance identity.
©2001 CRC Press LLC

THEOREM 5.3.3
Assume that for the a.c. r.v. X with ﬁnite variance σ2, the r.v. Y satisﬁes
one of the following.
(i)
For every a.c. function g with derivative g′,
Var[g(X)] ≤σ2IE[(g′(Y ))2].
(ii)
For every a.c. function g with derivative g′ such that IE|g′(Y )| <
∞,
Var[g(X)] ≥σ2IE2[g′(Y )].
Then Y
d= X∗.
PROOF Assume that (i) holds. Let h′ be any (measurable) bounded
function and consider the a.c. function g(x) = x + λh(x), where λ is an
arbitrary constant and h an indeﬁnite integral of h′. It follows that g
is a.c. with bounded derivative g′ = 1 + λh′. Thus, Var[g(X)] < ∞,
Var[h(X)] < ∞and
Var[g(X)] = σ2 + λ2Var[h(X)] + 2λCov[X, h(X)]
≤σ2 
1 + λ2IE[(h′(Y ))2] + 2λIE[h′(Y )]
	
.
Therefore, by using standard arguments [see Cacoullos and Papathana-
siou (1989)], the quadratic θλ2 + 2δλ (where θ = σ2IE[(h′(Y ))2] −
Var[h(X)] ≥0, δ = σ2IE[h′(Y )] −Cov[X, h(X)]) is nonnegative for
all λ, and thus δ = 0. Hence, taking into account (5.2.2) we conclude
that for any bounded function H,
IE[H(Y )] = IE[H(X∗)]
and the result follows. The same arguments apply to (ii).
Finally, by using similar arguments and the results of Theorem 5.2.2, a
converse of Theorem 5.3.3 and Lemma 5.2.1(iii) can be easily established:
THEOREM 5.3.4
Let Y be an arbitrary r.v. Then, Y has a unimodal a.c. density h such
that the function h′(x)/(m −x) is integrable for some mode m of h iﬀ
there exists some a.c. r.v. X with ﬁnite second moment such that any
one of the following holds.
(i)
For every a.c. function g with derivative g′ such that IE|g′(Y )| <
∞,
Cov[X, g(X)] = Var[X]IE[g′(Y )].
©2001 CRC Press LLC

(ii)
For every a.c. function g with derivative g′,
Var[g(X)] ≤Var[X]IE[(g′(Y ))2].
(iii)
For every a.c. function g with derivative g′ such that IE|g′(Y )| <
∞,
Var[g(X)] ≥Var[X]IE2[g′(Y )].
(iv)
X∗d= Y .
Furthermore, X is unique iﬀthe mode of h is unique.
Theorems 5.3.3 and 5.3.4 characterize the r.v.s X and Y which admit
Poincar´e type inequalities or diﬀerential inequalities [cf Chen (1988) and
Vitale (1989)]. It should be noted that in the preceding inequalities the
constant equals Var[X], and this implies that the equality is attained for
‘linear’ g. There are, however, other kinds of Poincar´e type inequalities
where the constant does not equal Var[X]. The following example is
relative to this subject.
Example 4
Let X be uniformly distributed over (0, 1) and Y have the density h(x) =
(3/4)(1 −x2)I(|x| < 1). It follows that for every a.c. function g,
Var[g(X)] ≤1
12 IE[(g′(X∗))2] = 1
2
 1
0
x(1 −x)(g′(x))2 dx
≤cIE[(g′(Y ))2],
for some constant c ≤1/3, since (1/2)x(1 −x)I(0 < x < 1) ≤(1/3)h(x)
for x ∈(−1, 1).
On the other hand, an application to the function
g(x) = x2 shows that c ≥1/9 > 1/12 = Var[X].
Acknowledgements Research partially supported by the Ministry of
Industry, Energy and Technology of Greece under grant 1369. Part of
this work was done when the second author was visiting the University
of Bristol.
References
1. Cacoullos, T. (1982). On upper and lower bounds for the variance
of a function of a random variable. Annals of Probability 10, 799–
809.
©2001 CRC Press LLC

2. Cacoullos, T., Papadatos, N., and Papathanasiou, V. (1997). Vari-
ance inequalities for covariance kernels and applications to central
limit theorems. Theory of Probability and its Applications 42, 195–
201.
3. Cacoullos, T. and Papathanasiou, V. (1985). On upper bounds for
the variance of functions of random variables. Statistics & Proba-
bility Letters 3, 175–184.
4. Cacoullos, T. and Papathanasiou, V. (1989). Characterizations of
distributions by variance bounds. Statistics & Probability Letters
7, 351–356.
5. Cacoullos, T. and Papathanasiou, V. (1995). A generalization of
covariance identity and related characterizations.
Mathematical
Methods of Statistics 4, 106–113.
6. Cacoullos, T., Papathanasiou, V., and Utev, S. (1992). Another
characterization of the normal law and a proof of the central limit
theorem connected with it. Theory of Probability and its Applica-
tions 37, 648–657 (in Russian).
7. Cacoullos, T., Papathanasiou, V., and Utev, S. (1994). Variational
inequalities with examples and an application to the central limit
theorem. Annals of Probability 22, 1607–1618.
8. Chen, L. H. Y. (1982). An inequality for the multivariate normal
distribution. Journal of Multivariate Analysis 12, 306–315.
9. Chen, L. H. Y. (1988). The central limit theorem and Poincar´e
type inequalities. Annals of Probability 16, 300–304.
10. Chernoﬀ, H. (1981). A note on an inequality involving the normal
distribution. Annals of Probability 9, 533–535.
11. Goldstein, L. and Reinert, G. (1997). Stein’s method and the Zero
Bias Transformation with application to simple random sampling.
Annals of Applied Probability 7, 935–952.
12. Hudson, H. M. (1978). A natural identity for exponential families
with applications to multiparameter estimation. Annals of Statis-
tics 6, 473–484.
13. Liu, J. S. (1994). Siegel’s formula via Stein’s identities. Statistics
& Probability Letters 21, 247–251.
14. Papathanasiou, V. (1990). Characterizations of multidimensional
exponential families by Cacoullos-type inequalities.
Journal of
Multivariate Analysis 35, 102–107.
15. Papathanasiou, V. (1993). Some characteristic properties of the
Fisher information matrix via Cacoullos-type inequalities. Journal
of Multivariate Analysis 44, 256–265.
©2001 CRC Press LLC

16. Stein, C. (1972). A bound for the error in the normal approxima-
tion to the distribution of a sum of dependent random variables.
In Proceedings of the Sixth Berkeley Symposium on Mathematics,
Statistics and Probability 2, 583–602. University California Press,
Berkeley.
17. Stein, C. (1981). Estimation of the mean of a multivariate normal
distribution. Annals of Statistics 9, 1135–1151.
18. Vitale, R. A. (1989). A diﬀerential version of the Efron-Stein in-
equality: Bounding the variance of an inﬁnitely divisible variable.
Statistics & Probability Letters 7, 105–112.
©2001 CRC Press LLC

6
Probability Inequalities for U-statistics
Tasos C. Christoﬁdes
University of Cyprus, Nicosia, Cyprus
ABSTRACT Probability inequalities with exponential bounds are very
central both in probability and statistics. In particular, such inequali-
ties can be used in statistical (especially nonparametric) inference to
provide rates of convergence for various estimates. In this paper, max-
imal inequalities and probability inequalities with exponential bounds
are presented for various statistics including sums of i.i.d. random vari-
ables, sums of independent but not necessarily identically distributed
random variables, and U-statistics.
These inequalities generalize and
extend results of Hoeﬀding (1963), Turner, Young, and Seaman (1992),
Christoﬁdes (1994) and Qiying (1996).
Keywords and phrases U-statistics, maximal inequalities, exponen-
tial bounds
6.1
Introduction
Let X1, . . . , Xn be independent observations of some space X. For in-
teger m ≥1, let h(x1, . . . , xm) be a “kernel” mapping X m to R, and,
without loss of generality, assume that h is symmetric in its arguments
(i.e., invariant under permutations of arguments). To any such kernel
h and sample X1, . . . , Xn with n ≥m, we associate the average of the
kernel over the sample observations taken m at a time, i.e., we deﬁne
the corresponding U-statistic
Un =
n
m
−1 
c
h(Xi1, . . . , Xim),
where 
c denotes summation over the
 n
m

combinations of m distinct
elements {i1, . . . , im} from {1, . . . , n}.
©2001 CRC Press LLC

The class of U-statistics was ﬁrst introduced and studied by Hoeﬀd-
ing (1948), as a generalization of the notion of sample mean. This class
includes as special cases useful statistics such as the arithmetic mean
and sample variance. They have a variety of applications including non-
parametric estimation, hypothesis testing, and approximations to more
complicated statistics. For an extensive treatment of U-statistics includ-
ing examples, useful representations and applications see Serﬂing (1980)
or Lee (1990).
A U-statistic by construction is an average of dependent observations
except of course in the trivial case where m = 1. However, a U-statistic
can be represented as an average of averages of independent (and iden-
tically distributed if X1, . . . , Xn are i.i.d.) random variables. The repre-
sentation, due to Hoeﬀding (1963) is the following: For r = [n/m] where
[x] denotes the integer part of the real number x, put
W(x1, . . . , xn) = 1
r {h(x1, . . . , xm) + h(xm+1, . . . , x2m) + . . .
+ h(xrm−m+1, . . . , xrm)}.
Clearly W(X1, . . . , Xn) is an average of independent observations. One
can easily verify that Un can be expressed as
Un = 1
n!

p
W(Xi1, . . . , Xin),
where 
p denotes summation taken over all permutations (i1, . . . , in) of
{1, . . . , n}. The previous representation along with the results of Section
6.2 will be used in Section 6.3 to prove the main results of this paper.
Many U-statistics of interest are constructed based on bounded ker-
nels. This class of U-statistics includes as a special case statistics con-
structed using variables which are indicator functions such as the empir-
ical distribution function and the Wilcoxon one-sample statistic.
A very useful tool for various purposes is the following exponential
probability inequality due to Hoeﬀding (1963).
THEOREM 6.1.1
For a U-statistic Un based on the kernel h(x1, . . . , xm) with a ≤h ≤b
and ϵ > 0 we have that
P(Un −E(Un) ≥ϵ) ≤exp(−2rϵ2(b −a)−2),
where r = [n/m].
©2001 CRC Press LLC

Hoeﬀding’s inequality remains a powerful result having many applica-
tions not only in theoretical statistics but in other areas as well. See, for
example, Clayton (1994) and Srivastar and Stangier (1996). Extensions
of Hoeﬀding‘s result were given by various authors, including Roussas
(1996) for negatively associated random variables and Devroye (1991)
for kernel estimates of density functions.
The main objective of this paper is to establish maximal inequali-
ties for U-statistics based on bounded kernels. Such inequalities can be
used, among other things, to provide rates of convergence of various es-
timates. The results obtained in this paper improve and generalize the
corresponding bounds of Hoeﬀding (1963), Turner et al. 
(1992), and
Qiying (1996).
6.2 
Preliminaries
For the development of the maximal inequalities in Section 6.3 we will
make use of the following preliminary results.
LEMMA 6.2.1
Let Un be a U-statistic and g a nondecreasing positive convex function.
Then for ϵ > 0 and t > 0
P(supk≥nUk ≥ϵ) ≤{g(tϵ)}−1E{g(tUn)}.
PROOF Let Fn = σ{X(n), Xn+1, Xn+2, . . .}, where X(n) denotes the
vector of order statistics (Xn1, . . . , Xnn). Then {Uk, Fk, k ≥m} is a
reverse martingale.
By the convexity of g, {g(tUk), Fk, k ≥m} is a
reverse submartingale. Therefore,
P(supk≥nUk ≥ϵ) = P(supk≥ng(tUk) ≥g(tϵ))
≤{g(tϵ)}−1E{g(tUn)}
where the inequality follows from a well-known maximal inequality for
reverse submartingales [see, for example, Theorem 3.4.3 in Borovskikh
and Korolyuk (1997) reference].
©2001 CRC Press LLC

LEMMA 6.2.2
Let X be a random variable with a ≤X ≤b and E(X) = µ. If φX is
the moment generating function of X then
φX(t) ≤b −µ
b −a eat + µ −a
b −a ebt.
PROOF By the convexity of the exponential function
etX ≤b −X
b −a eat + X −a
b −a ebt
and by taking expectations the result follows.
Two useful (determinist) inequalities are presented next.
Lemma
6.2.3 combines results which can be found in Christoﬁdes (1991) and
Christoﬁdes (1994).
LEMMA 6.2.3
Let x, y ∈R such that y = αx for α ≥0.
(i) If 0 ≤α < 1
4 then
x2p −2pxy2p−1 + (2p −1)y2p ≥(y −x)2p,
p = 1, 2, . . .
(ii) If α ≥1
4 then
x2p −2pxy2p−1 + (2p −1)y2p ≥(2p −1)(y −x)2p,
p = 1, 2, . . .
The next lemma provides lower bounds under diﬀerent conditions for
a quantity which appears very frequently in proofs of results concerning
exponential inequalities.
LEMMA 6.2.4
Let
g(c, d) = (c + d) ln(c + d
d
) + (1 −c −d) ln(1 −c −d
1 −d
)
for c > 0, d > 0 and c + d < 1.
(i) If 1
2 < d < 1
2 + 1
3c then
g(c, d) ≥(c + 1
2) ln(1 + 2c) + (1
2 −c) ln(1 −2c).
(ii) If c + d < 1
2 or 1
2 + 1
3c ≤d then
g(c, d) ≥−1
2 ln(1 −4c2).
©2001 CRC Press LLC

(iii) If 1
2 −c ≤d ≤1
2 then
g(c, d) ≥2c2 + 4
3c4.
PROOF By Kambo and Kotz (1966) g(c, d) can be expressed as
g(c, d) =
∞

p=1
1
2p(2p −1){x2p −2pxy2p−1 + (2p −1)y2p}
where x = 1 −2c −2d and y = 1 −2d.
First assume that 1
2 < d < 1
2 + 1
3c. Then y = αx for 0 < α < 1
4. By
Lemma 6.2.3
g(c, d) ≥
∞

p=1
1
2p(2p −1)(y −x)2p
=
∞

p=1
1
2p(2p −1)(2c)2p
=
∞

p=1
1
2p −1(2c)2p −
∞

p=1
1
2p(2c)2p
= 2c
∞

p=1
1
2p −1(2c)2p−1 −1
2
∞

p=1
1
p(4c2)
p
= c ln(1 + 2c
1 −2c) + 1
2 ln(1 −4c2)
= (c + 1
2) ln(1 + 2c) + (1
2 −c) ln(1 −2c),
i.e., part (i) of the statement is established.
Now assume that c+d < 1
2 or 1
2 + 1
3c ≤d. Then y = αx for some α ≥1
4.
By Lemma 6.2.3
g(c, d) ≥
∞

p=1
1
2p(y −x)2p = 1
2
∞

p=1
(4c2)
p
p
= −1
2 ln(1 −4c2),
and part (ii) is established as well. Part (iii) follows directly from Lemma
3 of Kambo and Kotz (1966).
©2001 CRC Press LLC

6.3
Probability Inequalities
In this section which is the main one of this paper we present maximal
inequalities for U-statistics based on bounded kernels.
For the next theorem and its associated results we assume that the U-
statistic Un is constructed using i.i.d. random variables. However, with
minor modiﬁcations in the proof the same results hold true for the case
of random variables which are not necessarily identically distributed,
as long as the kernel h(Xi1, . . . , Xim) is uniformly bounded below and
above and has the same expectation for every choice of {i1, . . . , im} from
{1, . . . , n}.
THEOREM 6.3.1
Let Un be a U-statistic based on the kernel h with a ≤h ≤b.
Let
E(h) = µ, r = [n/m] and ϵ > 0.
(i) If either ϵ + µ < 1
2(b −a) or ϵ ≤3(µ −1
2(b −a)) then
P(supk≥n(Uk −µ) ≥ϵ) ≤(1 −4ϵ2(b −a)−2)
r/2.
(ii) If 1
2(b −a) < µ < 1
2(b −a) + 1
3ϵ then
P(supk≥n(Uk −µ) ≥ϵ) ≤(1 −2ϵ(b −a)−1)
r(ϵ(b−a)−1−1
2 )
×(1 + 2ϵ(b −a)−1)
−r(ϵ(b−a)−1+ 1
2 ).
(iii) If 1
2(b −a) −ϵ ≤µ ≤1
2(b −a) then
P(supk≥n(Uk −µ) ≥ϵ) ≤exp{−r(2ϵ2(b −a)−2 + 4
3ϵ4(b −a)−4)}.
(iv) If ϵ + µ = b then
P(supk≥n(Uk −µ) ≥ϵ) ≤(1 −ϵ(b −a)−1)
r.
PROOF If ϵ+µ > b then P(supk≥n(Uk−µ) ≥ϵ) = 0 and there is nothing
to prove. First assume that ϵ + µ < b. Let t > 0. Using Hoeﬀding’s
representation of Un as an average of averages of independent random
variables and Lemma 6.2.1 we can write
©2001 CRC Press LLC

P(supk≥n(Uk −µ) ≥ϵ) ≤e−t(ϵ+µ)E(etUn)
= e−t(ϵ+µ)E(e
t
n!

pW (.))
≤e−t(ϵ+µ) 1
n!

pE(etW (.))
= e−t(ϵ+µ) 1
n!

pE(et/r 
h(.))
= e−t(ϵ+µ) 1
n!

p(φh(t/r))r
= e−t(ϵ+µ)(φh(t/r))r
≤e−t(ϵ+µ)(b −µ
b −a eat/r + µ −a
b −a ebt/r)
r
= e−f(t)
(6.3.1)
where φh is the moment generating function of h, f is the function
f(t) = t(ϵ + µ) −r log(b −µ
b −a eat/r + µ −a
b −a ebt/r)
and the last inequality follows from Lemma 6.2.2. The function f is
maximized at
t⋆=
r
b −a ln{(ϵ + µ −a)(b −µ)
(b −ϵ −µ)(µ −a)}
and
f(t⋆) = r{ϵ + µ −a
b −a
ln(ϵ + µ −a
µ −a
) + b −ϵ −µ
b −a
ln(b −ϵ −µ
b −µ
)}.
Let ϵ⋆= ϵ(b −a)−1 and µ⋆= (µ −a)(b −a)−1. Then f(t⋆) = rg(ϵ⋆, µ⋆)
where
g(ϵ⋆, µ⋆) = (ϵ⋆+ µ⋆) ln(ϵ⋆+ µ⋆
µ⋆
) + (1 −ϵ⋆−µ⋆) ln(1 −ϵ⋆−µ⋆
1 −µ⋆
).
(i) If either ϵ+µ < 1
2(b−a) or ϵ ≤3(µ−1
2(b−a)) then by the assumptions
of the theorem ϵ⋆and µ⋆satisfy the conditions of Lemma 6.2.4 (ii) and
thus f(t⋆) ≥−r
2 ln(1 −4ϵ⋆2). Therefore by (6.3.1)
P(supk≥n(Uk −µ) ≥ϵ) ≤(1 −4ϵ2(b −a)−2)
r/2.
©2001 CRC Press LLC

(ii) If 1
2(b −a) < µ <  1
2(b −a) +  1
3ϵ then the conditions of Lemma 6.2.4
(i) are satisﬁed and thus
f(t⋆) ≥r(ϵ⋆+ 1
2) ln(1 + 2ϵ⋆) + r(1
2 −ϵ⋆) ln(1 −2ϵ⋆).
Therefore, again by (6.3.1)
P(supk≥n(Uk −µ) ≥ϵ) ≤(1 −2ϵ⋆)r (ϵ⋆− 1
2 )(1 + 2ϵ⋆)−r (ϵ⋆ + 1
2 ).
(iii) If 1
2(b−a)−ϵ ≤µ ≤ 1
2(b−a) then  1
2 −ϵ∗≤µ∗≤ 1
2 and by applying
(iii) of Lemma 6.2.4 we have that
f(t∗) ≥r(2ϵ∗ 2 + 4
3ϵ∗ 4).
The result follows from (6.3.1).
(iv) If ϵ + µ = b then
P(supk≥n(Uk −µ) ≥ϵ)
= P(supk≥nUk = b)
= P(h(Xi1, . . . , Xim) = b ∀(i1, . . . , im) ⊂{1, . . . , n})
≤P(h(Xjm+1, . . . , Xjm+m) = b, j = 0, . . . , r −1)
=
r−1
j =0P(h(Xjm+1, . . . , Xjm+m) = b) 
(6.3.2)
≤
r−1
j =0(E(h) −a
b −a
)
= (µ −a
b −a )
r
= (1 −ϵ⋆)r
where (6.3.2) follows from independence.
REMARK Simple algebraic manipulation shows that under its condi-
tions Theorem 6.3.1 provides a better bound than the one of Theorem
6.1.1. In addition, part (i) of Theorem 6.3.1 generalizes Theorem 3.1 of
Christoﬁdes (1994) in the sense that the result is established for a gen-
eral bounded kernel h rather than a kernel which is a Bernoulli random
variable.
©2001 CRC Press LLC

In many situations a two-sided version of the theorem might be more
appropriate, if not necessary. Such a version is provided by the next
corollary under appropriate conditions on the parameters.
COROLLARY 6.3.2
Let Un be a U-statistic based on the kernel h with a ≤h ≤b.
Let
E(h) = µ, r = [n/m] and ϵ > 0. Assume that | µ | +ϵ < 1
2(b −a) or
ϵ + µ < 1
2(b −a) ≤−µ −1
3ϵ or ϵ −µ < 1
2(b −a) ≤µ −1
3ϵ. Then
P(supk≥n | Uk −µ |≥ϵ) ≤2(1 −4ϵ2(b −a)−2)
r/2.
PROOF We can write
P(supk≥n | Uk −µ |≥ϵ) ≤P(supk≥n(Uk −µ) ≥ϵ)
+ P(supk≥n(−Uk + µ) ≥ϵ).
(6.3.3)
Observe that −Un is a U-statistic based on the kernel −h with E(−h) =
−µ and −b ≤−h ≤−a. The conditions on the parameters ϵ, µ, a and
b are such that the assumptions of the theorem are satisﬁed for both Un
and −Un. Thus, each term of the right hand side of (6.3.3) is bounded
by (1 −4ϵ2(b −a)−2)
r/2 and the result follows.
REMARK Corollary 6.3.2 improves and generalizes Corollary 2 of Qiy-
ing (1996) in several ways. First, the result is stated for a U-statistic
rather than the sample mean of i.i.d. random variables. In addition, in
the special case where the U-statistic is the sample mean of i.i.d. random
variables the bound provided is sharper than the one of Corollary 2 of
Qiying (1996). To see this, just expand the logarithm of (1 −4ϵ2) to get
ln(1 −4ϵ2) = −4ϵ2 −8ϵ4 −64
3 ϵ6 . . .
Clearly
n
2 ln(1 −4ϵ2) < −2nϵ2 −4
3nϵ4,
i.e.,
(1 −4ϵ2)
n/2 < exp(−2nϵ2 −4
3nϵ4),
where the right hand side of the previous inequality is the sharpest of
the three upper bounds of Corollary 2 of Qiying (1996).
©2001 CRC Press LLC

Assume that m = 1, h(x) = x and X1, . . . , Xn are Bernoulli random
variables with probability of success p, 0 < p < 1. Then as a special
case of Theorem 6.3.1 we have the following inequality for the Binomial
distribution.
COROLLARY 6.3.3
Le t Yk be a binomial random variable with parameters k and p, 0 < p <
1. Let ϵ > 0.
(i) If either ϵ + p <  1
2 or ϵ ≤3(p − 1
2) then
P(supk≥n(Yk
k −p) ≥ϵ) ≤(1 −4ϵ2)
n
2 .
(ii) If 1
2 < p <  1
2 + 1
3ϵ then
P(supk≥n(Yk
k −p) ≥ϵ) ≤(1 −2ϵ2)
n(ϵ− 1
2 )(1 + 2ϵ2)
−n(ϵ+ 1
2 ).
(iii) If 1
2 −ϵ ≤p ≤ 1
2 then
P(supk≥n(Yk
k −p) ≥ϵ) ≤exp{−n(2ϵ2 + 4
3ϵ4)}.
(iv) If p + ϵ <  1
2 or 0 < ϵ −p <  1
2 ≤p − 1
3ϵ then
P(supk≥n | Yk
k −p |≥ϵ) ≤2(1 −4ϵ2)
n
2 .
REMARK Corollary 6.3.3 improves the result of Turner et al. (1992)
who proved the inequality
P(supk≥n(Yk
k −p) ≥ϵ) ≤(1 −2ϵ2)
n(ϵ− 1
2 )(1 + 2ϵ2)
−n(ϵ+ 1
2 )
under the condition p + ϵ <  1
2 or p ≥ 1
2. As in the case of the previous
remark to see this we can expand the logarithm of (1−4ϵ2). Then simple
algebraic manipulation shows that
(1 −4ϵ2)
n
2 < (1 −2ϵ2)
n(ϵ− 1
2 )(1 + 2ϵ2)
−n(ϵ+ 1
2 ).
The next theorem is essentially a generalization of Theorem 6.1.1 to
the case where the observations X1, . . . , Xn are not necessarily identi-
cally distributed and the kernel h is not uniformly bounded.
©2001 CRC Press LLC

THEOREM 6.3.4
Let X1, . . . , Xn be independent random variables and Un a U-statistic
based on the kernel h.
Assume that for {i1, . . . , im} ⊆{1, . . . , n},
E{h(Xi1, . . . , Xim)}
=
θi1,...,im
and ai1,...,im
≤
h(Xi1, . . . , Xim)
≤bi1,...,im. Let ¯θn =
 n
m
−1 
c θi1,...,im, r = [n/m] and ϵ > 0. Then
P(supk≥n | Uk −¯θk |≥ϵ) ≤2 exp{−2r2ϵ2/max(i1,...,in)Di1,...,in}
where
Di1,...,in =
r−1
j=0(bijm+1,...,i(j+1)m −aijm+1,...,i(j+1)m)2
and the maximum is taken over all permutations (i1, . . . , in) of {1, . . . , n}.
PROOF Let t > 0 and ˜h(Xi1, . . . , Xim) = h(Xi1, . . . , Xim) −θi1,...,im.
For notational simplicity let ij,m = (ijm+1, . . . , i(j+1)m). Following the
proof of Theorem 6.3.1 we arrive at the following
P(supk≥n(Uk −¯θk) ≥ϵ) ≤e−tϵ 1
n!

p
r−1
j=0E(e
t
r
 ˜h(·))
≤e−tϵ 1
n!

p
r−1
j=0Gij,m(t),
(6.3.4)
where
Gij,m(t) = exp(−t
rθij,m)
 bij,m −θij,m
bij,m −aij,m
exp( t
raij,m)
+ θij,m −aij,m
bij,m −aij,m
exp( t
rbij,m)
	
,
and (6.3.4) is a consequence of Lemma 6.2.2. Let uij,m = (θij,m −aij,m)
(bij,m −aij,m)
and s = t/r. Then
Gij,m(t) = exp{−s(θij,m −aij,m)}{1−uij,m +uij,m exp{s(bij,m −aij,m)}}.
Following Hoeﬀding (1963) let
fij,m(s) = −s(θij,m −aij,m) + ln{1 −uij,m + uij,m exp{s(bij,m −aij,m)}},
so that
Gij,m = exp{fij,m(s)}.
(6.3.5)
©2001 CRC Press LLC

One can verify that fij,m(0) = 0 and f ′
ij,m(0) = 0. In addition,
f ′′
ij,m(s) = (bij,m −aij,m)2gij,m(s),
(6.3.6)
where
gij,m(s) =
uij,m(1 −uij,m) exp{−s(bij,m −aij,m)}
{uij,m + (1 −uij,m) exp{−s(bij,m −aij,m)}}2 .
The function gij,m(s) is of the form x(1−x) with 0 < x < 1 and therefore
gij,m(s) ≤1
4. From (6.3.6)
f ′′
ij,m(s) ≤1
4(bij,m −aij,m)2.
Using Taylor’s formula up to terms involving the second derivative we
have that
fij,m(s) ≤1
8s2(bij,m −aij,m)2.
Using the above inequality in (6.3.5) we have that
Gij,m(t) ≤exp{ t2
8r2 (bij,m −aij,m)2}.
Then from (6.3.4) we obtain the inequalities
P(supk≥n(Uk −¯θk) ≥ϵ) ≤e−tϵ 1
n!

p
r−1
j=0 exp{ t2
8r2 (bij,m −aij,m)2}
= e−tϵ 1
n!

p
exp{ t2
8r2
r−1

j=0
(bij,m −aij,m)2}
≤exp{−tϵ + t2
8r2 max(i1,...,in)Di1,...,in}.
Optimizing for t we get
P(supk≥n(Uk −¯θk) ≥ϵ) ≤exp{−2r2ϵ2/max(i1,...,in)Di1,...,in}.
The theorem follows using an analog of (6.3.3).
References
1. Christoﬁdes, T. C. (1991). Probability inequalities with exponen-
tial bounds for U-statistics.
Statistics & Probability Letters 12,
257–261.
©2001 CRC Press LLC

2. Christoﬁdes, T. C. (1994). A Kolmogorov inequality for U-statistics
based on Bernoulli kernels. Statistics & Probability Letters 21, 357–
362.
3. Clayton, H. R. (1994).
A combined bound for errors in audit-
ing based on Hoeﬀding‘s inequality and the bootstrap. Journal of
Business and Economic Statistics 12, 437–448.
4. Devroye, L. (1991). Exponential inequalities in nonparametric es-
timation, Nonparametric functional estimation and related topics
(Spetses 1990), pp. 31–44, NATO Adv. Sci. Inst. Ser. C Math.
Phys. Sci., 335, Kluwer Academic Publishers, Dordrecht.
5. Hoeﬀding, W. (1948).
A class of statistics with asymptotically
normal distributions. Annals of Mathematical Statistics 19, 293–
325.
6. Hoeﬀding, W. (1963). Probability inequalities for sums of bounded
random variables. Journal of the American Statistical Association
58, 13–30.
7. Kambo, N. S. and Kotz, S. (1966).
On exponential bounds for
binomial probabilities. Annals of the Institute of Statistical Math-
ematics 18, 277–287.
8. Lee, A. J. (1990). U-statistics: Theory and Practice, Marcel Dekker,
New York.
9. Qiying, W. (1996). On the maximal inequality. Statistics & Prob-
ability Letters 31, 85–89.
10. Roussas, G. G. (1996). Exponential probability inequalities with
some applications. Statistics, Probability and Game Theory, 303–
319, IMS Lecture Notes Monogr. Ser., 30, Inst. Math. Statist.,
Hayward, CA.
11. Serﬂing, R. (1980). Approximation Theorems of Mathematical Statis-
tics. John Wiley & Sons, New York.
12. Srivastar, A. and Stangier, P. (1996). Chernoﬀ-Hoeﬀding inequal-
ities in integer programming.
Random Structures Algorithms 8,
27–58.
13. Turner, D. W., Young, D. M., and Seaman, J. W. (1992). Improved
Kolmogorov inequalities for the Binomial distribution. Statistics &
Probability Letters 13 223–227.
©2001 CRC Press LLC

PART II
Probability and Stochastic Processes
©2001 CRC Press LLC

7
Theory and Applications of Decoupling
Victor H. de la Pe ˜na and Tze Leung Lai
Columbia University, New York, NY
Stanford University, Stanford, CA
ABSTRACT In this paper we provide a survey of the theory of decou-
pling, emphasizing its wide range of applications. Decoupling was born
out of a need to extend the known martingale inequalities for real and
Hilbert-space valued random variables to variables taking values in more
general spaces like Banach spaces. Among its many uses is that it enables
one to symmetrize highly dependent random variables. In fact, some of
the basic results in decoupling theory were motivated by symmetrization
techniques for U-processes which consist of U-statistics indexed by a class
of functions. Other applications include sequential analysis, martingale
theory, stochastic integration and weak convergence. We consider three
types of decoupling. The ﬁrst type, which we call complete decoupling,
completely replaces the summands in a sum of dependent random vari-
ables by independent ones with the same marginal distributions. The
second type of decoupling is decoupling of tangent sequences. In this
approach, two sequences adapted to the same ﬁltration and having the
same conditional distributions (given the past) are compared. The eﬀec-
tiveness of this approach is realized through the use of a sequence with
a more tractable dependence structure than the original one. The third
type of decoupling is called “total decoupling of stopping times”. In this
approach, problems involving stopping times are handled by establish-
ing inequalities that replace the original process by an independent copy
which is therefore independent of the stopping time.
Keywords and phrases Conditional independence, decoupling inequ-
alities, exponential bounds, martingales, stopping times, symmetriza-
tion, tangent sequences, U-statistics, weak convergence
©2001 CRC Press LLC

7.1
Complete Decoupling of Marginal Laws and
One-Sided Bounds
Let (X1, ..., Xn) be a vector of possibly dependent random variables.
Let (Y (i)
1 , ..., Y (i)
n ), i = 1, ..., n, be n independent copies of the origi-
nal random vector. To do this formally we might need to enlarge our
probability space. Then all sequences have exactly the same joint dis-
tributions.
Moreover, observe that Y (i)
i
, 1 ≤i ≤n, are independent
random variables with Y (i)
i
having the same distribution as Xi. With
this construction we can now analyze the decoupling property obtained
through the use of the linearity of expectation. That is,
E

n

i=1
Xi

= E

n

i=1
Y (i)
i

,
(7.1.1)
where the ﬁrst expectation involves a sum of dependent variables and the
second a sum of independent random variables. Indeed, E(n
i=1 Xi) =
n
i=1 EXi = n
i=1 EY (i)
i
= E(n
i=1 Y (i)
i
). This elementary identity has
been extended to the following one-sided bound by de la Pe˜na (1990).
THEOREM 7.1.1
Let {Xi} be a sequence of (arbitrarily dependent) random variables with
Xi ≥0 a.s. Let the sequence {Y (i)
i
} be deﬁned as above. Then, for all
concave nondecreasing functions Ψ on [0, ∞) with Ψ(0) = 0,
EΨ
 n

i=1
Xi

≤CEΨ
 n

i=1
Y (i)
i

,
(7.1.2)
for some universal constant C.
No reverse bound is possible in general for concave functions. However,
the reverse bound holds for a class of convex functions. The following
notation will be needed not only in the reverse bound but also in many
subsequent inequalities. For α > 0, let
Aα = {Φ : Φ is a nondecreasing, continuous function on [0, ∞) with
Φ(0) = 0 and Φ(cx) ≤cαΦ(x) for all c ≥2 and x ≥0}.
(7.1.3)
Then there exists a positive ﬁnite constant Cα depending only on α such
that for all convex Φ ∈Aα,
©2001 CRC Press LLC

EΦ
 n

i=1
Xi

≥CαEΦ
 n

i=1
Y (i)
i

.
(7.1.4)
Results of this kind, which appear in de la Pe˜na (1990), have obvious
martingale analogues through the use of the square function inequalities
of Burkholder, Davis and Gundy.
THEOREM 7.1.2
Let {di} be a martingale diﬀerence sequence. Let {Y (i)
i
} be a sequence
of independent random variables with Y (i)
i
having the same distribution
as di (deﬁned above). Let α > 0 be ﬁxed. Then, for all Φ ∈Aα with
Φ(x) convex and Φ(√x) concave on [0, ∞), there exists a positive ﬁnite
constant Cα depending only on α such that
EΦ

max
j≤n |
j

i=1
di|

≤CαEΦ

|
n

i=1
Y (i)
i
|

.
(7.1.5)
Moreover, if both Φ ∈Aα and Ψ(√x) are convex functions on [0, ∞),
then there exists a positive ﬁnite constant cα depending only on α such
that
EΦ

max
j≤n |
j

i=1
di|

≥cαEΦ

|
n

i=1
Y (i)
i
|

.
(7.1.6)
We can replace Σn
i=1 by max1≤i≤n in Theorem 7.1.1 in view of the
following.
LEMMA 7.1.3
Let {Ui} be a sequence of dependent sets.
Let {Vi} be a sequence of
independent sets such that P(Ui) ≤P(Vi) for all i. Then P(∪n
i=1Ui) ≤
2P(∪n
i=1Vi).
To prove Lemma 7.1.3, ﬁrst note that
P(∪n
j=1Uj) ≤P(∪n
j=1Uj ∩(∪n
i=1Vi)c) + P(∪n
j=1Vj).
(7.1.7)
The ﬁrst term in the right hand side of (7.1.7) is less than or equal to
n

j=1
P(Uj ∩(∩j−1
i=1 V c
i )) =
n

j=1
P(Uj)P(∩j−1
i=1 V c
i ) ≤
n

j=1
P(Vj)P(∩j−1
i=1 V c
i ),
©2001 CRC Press LLC

where the equality follows from independence. Note that the last sum
above is equal to
P(∪n
j=1(Vj ∩(∩j−1
i=1 V c
i ))) = P(∪n
j=1Vj).
From Lemma 7.1.3 it follows that
P( max
1≤i≤n Xi > x) ≤2P( max
1≤i≤n Y (i)
i
> x)
for all
x.
(7.1.8)
Therefore, if Xi ≥0 a.s. and Ψ is a nondecreasing, continuous function
on [0, ∞) with Ψ(0) = 0, then
EΨ( max
1≤i≤n Xi) ≤2EΨ( max
1≤i≤n Y (i)
i
).
(7.1.9)
In particular, if the Xi have a common distribution function F, then the
right hand side of (7.1.8) is equal to 2(1 −F n(x)), while the left hand
side of (7.1.8) is majorized by min{1, n(1 −F(x))} and there exists a
joint distribution of X1, . . . , Xn with F as marginal such that
P( max
1≤i≤n Xi > x) = min{1, n(1 −F(x))}
for all
x,
(7.1.10)
cf. Lai and Robbins (1978) who call such random variables Xi “maxi-
mally dependent.” Letting p = 1 −F(x), we obtain from (7.1.8) and
(7.1.10) the inequality min{1, np} ≤2[1 −(1 −p)n] for 0 ≤p ≤1.
We next mention two recent results of the type in Theorem 7.1.1 con-
cerning associated variables and stochastic domination by independent
random variables.
DEFINITION 7.1.4
A ﬁnite family of random variables {Xi, 1 ≤i ≤n} is said to be
negatively associated if for every pair of disjoint subsets A1 and A2 of
{1, 2, ..., n},
Cov{f1(Xi, i ∈A1), f2(Xj, j ∈A2)} ≤0,
(7.1.11)
whenever f1 and f2 are coordinatewise nondecreasing and the covariance
exists. The random variables Xi are said to be positively associated if
the inequality in (1.11) is reversed.
As is well known, the following analogue of (7.1.8) holds for positively
associated random variables:
©2001 CRC Press LLC

P( max
1≤i≤n Xi > x) ≤P( max
1≤i≤n Y (i)
i
> x)
for all
x.
(7.1.12)
Examples of negatively associated random variables include the multino-
mial distribution, random sampling without replacement and the joint
distribution of ranks; see Joag-Dev and Proschan (1983). Shao (1998)
has obtained the following theorem, which generalizes Hoeﬀding’s (1963)
results for sampling without replacement.
THEOREM 7.1.5
Let {Xi, 1 ≤i ≤n} be a negatively associated sequence, and let {Y (i)
i
}
be a sequence of independent random variables such that Y (i)
i
has the
same distribution as Xi for all i = 1, ..., n. Then
Ef(
n

i=1
Xi) ≤Ef(
n

i=1
Y (i)
i
)
(7.1.13)
for any convex function f, whenever the expectation on the right hand
side of (7.1.13) exists. Moreover, if f is a nondecreasing convex function,
then
Ef( max
1≤k≤n
k

i=1
Xi) ≤Ef( max
1≤k≤n
k

i=1
Y (i)
i
)
(7.1.14)
whenever the expectation on the right hand side of (7.1.14) exists.
THEOREM 7.1.6
Let {Xi} be a sequence of random variables.
Assume that for all n,
P(Xn > x|X1, .., Xn−1) ≤P(Yn > x) for all real x, where {Yi} is a
sequence of independent random variables. Then
P(
n

i=1
Xn > x) ≤P(
n

i=1
Yi > x),
(7.1.15)
for all real x.
Theorem 7.1.6 was proved by Huang (1999) in her study of stochastic
networks.
It turns out to be a special case of a general domination
inequality of Kwapien and Woyczynski (1992, Theorem 5.1.1).
©2001 CRC Press LLC

THEOREM 7.1.7
On the probability space (Ω, F, P) let {Xi} and {Yi} be two sequences
of random variables with values in a separable Banach space (B, || · ||),
adapted to a ﬁltration Fi increasing to F and such that Yi is independent
of Fi−1 for all i.
If u : B →[0, ∞) satisﬁes E(u(x + Xi)|Fi−1) ≤
Eu(x + Yi) for all x ∈B and all i then,
Eu

x +
n

i=1
Xi

≤Eu

x +
n

i=1
Yi

,
for all x ∈B.
A related result involving domination of variables from Kwapien and
Woyczynski (1992, Theorem 5.2.1) helps to bridge the way between com-
plete decoupling and decoupling of tangent sequences which will be in-
troduced in the next two sections.
THEOREM 7.1.8
Let {Xi} and {Yi} be two sequences of random variables adapted to Fi,
with {Xi} taking values in a separable Banach space and {Yi} taking
values in [0, ∞). Assume that for all i and x > 0,
P(||Xi|| > x|Fi−1) ≤P(Yi > x|Fi−1).
(7.1.16)
Then for all x > 0 and concave Ψ : [0, ∞) →[0, ∞),
P( max
i≤i≤n ||Xi|| > x) ≤2P( max
1≤i≤n Yi > x),
(7.1.17)
EΨ( max
1≤j≤n ||
j

i=1
Xi||) ≤3EΨ

max
1≤j≤n
j

i=1
Yi

.
(7.1.18)
Moreover, for every continuous Ψ such that Ψ(x + y) ≤C(Ψ(x) + Ψ(y))
for some C > 0 and all x, y ∈[0, ∞), there exists C′ such that
EΨ

max
1≤j≤n ||
j

i=1
Xi||

≤C′EΨ

max
1≤j≤n
j

i=1
Yi

.
(7.1.19)
©2001 CRC Press LLC

7.2
Tangent Sequences and Conditionally
Independent Variables
DEFINITION 7.2.1
Let {di} and {ei} be two sequences of random variables adapted to the
same increasing sequence of σ-ﬁelds {Fi}. Assume that the conditional
distribution of di given Fi−1 is the same as the conditional distribution
of ei given Fi−1. Then the sequences {di} and {ei} are said to be Fi-
tangent.
DEFINITION 7.2.2
Let {ei} be a sequence of variables adapted to an increasing sequence of
σ-ﬁelds {Fi} contained in the σ-ﬁeld F. Then {ei} is said to satisfy the
CI condition (conditional independence) if there exists a σ-ﬁeld G con-
tained in F such that {ei} is conditionally independent given G and the
conditional distribution of ei given Fi−1 is the same as the conditional
distribution of ei given G.
DEFINITION 7.2.3
A sequence {ei} which satisﬁes the CI condition and which is also tangent
to di is said to be a decoupled tangent sequence to {di}.
Let T be a stopping time. It is easy to see that the sequences {Xi1(T ≥
i)} and { ˜Xi1(T ≥i)} are tangent with respect to Fi = σ(X1, ..., Xi;
˜X1, ..., ˜Xi).
Moreover, { ˜Xi1(T ≥i)} satisﬁes the CI condition given
G = σ(X1, X2, ...). In the context of sampling schemes, the theory of
decoupling provides a sampling scheme that lies between sampling with
replacement and sampling without replacement. We call this sequence
a CI (conditionally independent) sequence.
Example 1
Let {b1, ..., bN} be a set of N balls inside a box.
The sequence {di}
will represent a sample without replacement from the box and {ei} a CI
(conditionally independent) sample, which can be generated as follows.
At the ith stage, we ﬁrst draw ei, return the ball, and then draw di and
put the ball aside. It is easy to see that the above procedure will make
{ei} tangent to {di} with Fi = σ(d1, ..., di; e1, ..., ei). Moreover, {ei}
satisﬁes the CI condition given G = σ(d1, ..., dN).
A rather fortunate fact, which makes the theory of decoupling widely
applicable, is closely linked to the above example. Broadly speaking,
©2001 CRC Press LLC

it states that decoupled tangent sequences always exist. This result is
summarized in the following proposition.
PROPOSITION 7.2.4
For any sequence of random variables {di} adapted to an increasing se-
quence of σ-ﬁelds {Fi}, there always exists a decoupled sequence {ei}
(on a possibly enlarged space) which is tangent to the original sequence
and in addition satisﬁes the CI condition given a σ-ﬁeld G.
A constructive approach to obtaining the CI sequence proceeds by
induction. First we take e1 and d1 to be two independent copies of the
same random mechanism. Having constructed d1, . . . , di−1, e1, . . . , ei−1,
the ith pair of variables di and ei comes from i.i.d. copies of the same
random mechanism, given d1, ..., di−1. It is easy to see that using this
construction and taking
Fi = σ(d1, ..., di; e1, ..., ei),
the sequences {di} and {ei} are Fi-tangent and the sequence {ei} is
conditionally independent given the σ-ﬁeld G generated by d1, d2, . . . and
is a decoupled version of {di}; see de la Pe˜na (1994). This construction
also provides the following reﬁnement of Proposition 7.2.5 which will be
used in Section 7.6.
PROPOSITION 7.2.5
For any sequence of random variables {di} adapted to an increasing se-
quence of σ-ﬁelds {Fi} and for any stopping time T of {Fi}, there exist
a decoupled sequence {ei} and a σ-ﬁeld G such that T is measurable with
respect to G, and {ei} is tangent to {di} and is CI given G.
7.3
Basic Decoupling Inequalities for Tangent
Sequences
The following results constitute the backbone of the theory of decoupling
for tangent sequences. We divide them into two classes. The ﬁrst class
concerns comparisons between two tangent sequences {di} and {ei} and
the second concerns inequalities between these two tangent sequences
when one of them is decoupled.
The ﬁrst general result for tangent
sequences involves the probabilities of unions, analogous to Lemma 7.1.3;
see Kwapien and Woyczynski (1992) and Hitczenko (1988).
©2001 CRC Press LLC

LEMMA 7.3.1
Let {Di} and {Ei} be two sequences of events adapted to {Fi}. Assume
that these two sequences are Fi-tangent (their indicator variables are
tangent). Then for all n ≥1,
P(∪n
i=1Di) ≤2P(∪n
i=1Ei).
(7.3.1)
Putting Di = 1(di > t) and Ei = 1(ei > t) in (7.3.1) yields
P( max
1≤i≤n di > t) ≤2P( max
1≤i≤n ei > t).
(7.3.2)
We remark that the above inequalities are two-sided as we can replace
the role of the D′s and E′s.
If the random variables are assumed to be either nonnegative or con-
ditionally symmetric, Kwapien and Woyczynski (1992) give a detailed
account of inequalities (7.3.3) and (7.3.6), while (7.3.5) and (7.3.8) (for
variables in R1) are from Hitczenko (1988) and (7.3.4) and (7.3.7) come
from de la Pe˜na (1993); see also de la Pe˜na and Gin´e (1999). The proof
of (7.3.8) for Hilbert-space valued random variables (an easy extension)
can be found in Kwapien and Woyczynski (1992) and de la Pe˜na (1993).
Two sequences of random variables {di}, {ei} adapted to Fi are said
to be conditionally symmetric if L(di|Fi−1) = L(−di|Fi−1), i.e., if the
conditional law of di is the same as that of −di. Deﬁne Aα as in (7.1.3).
THEOREM 7.3.2
Let {di}, {ei} be two sequences of Fi-adapted nonnegative random vari-
ables. Assume that the sequences are Fi-tangent. Then for all x, y > 0,
P
 n

i=1
di > x

≤y
x + 2P
 n

i=1
ei > y

,
(7.3.3)
P(
j

i=1
di > x) ≤2

E[(n
i=1 ei) ∧y]
x
+ 2P(
j

i=1
ei > y)

. (7.3.4)
Moreover, for every α > 0, there exists Cα such that for all Φ ∈Aα,
EΦ
 n

i=1
di

≤CαEΦ
 n

i=1
ei

.
(7.3.5)
©2001 CRC Press LLC

THEOREM 7.3.3
Let {di}, {ei} be two sequences of Fi-adapted Hilbert-space valued ran-
dom variables. Assume that the sequences are Fi-tangent and condition-
ally symmetric. Then for all x, y > 0
P

max
j≤n ||
j

i=1
di|| > x

≤3{y
x + P

max
j≤n ||
j

i=1
ei|| > y

}, (7.3.6)
P

max
j≤n ||
j

i=1
di||2 > x

≤4

6
E(maxj≤n
			j
i=1 ei
			
2
∧y)
x
+ P(2 max
j≤n
					
j

i=1
ei
					
2
> y)

.
(7.3.7)
Moreover, for every α > 0, there exists Cα such that for all Φ ∈Aα,
EΦ

max
j≤n
					
j

i=1
di
					

≤CαEΦ

max
j≤n
					
j

i=1
ei
					

. 
(7.3.8)
Inequality (7.3.8) was extended to the case of UMD spaces by Mc-
Connell (1989). Assuming that one of the two tangent sequences satisﬁes
the CI condition, de la Pe˜na (1994) proved part (i) (with g = 1) while
Hitczenko (1994) proved part (ii) of the following theorem. The case of
(i) for general g comes from de la Pe˜na (1999). See Section 6.2 of de la
Pe˜na and Gin´e (1999) for a detailed account of Theorem 7.3.4(i) and its
corollaries.
THEOREM 7.3.4
Let {di}, {ei} be two sequences of Fi-adapted random variables. Assume
that the sequences are Fi-tangent.
Furthermore, assume that {ei} is
conditionally independent given G.
(i)
For all G-measurable random variables g ≥0,
Eg exp

λ
n

i=1
di

≤E1/2

g2 exp(2λ
n

i=1
ei)

.
(7.3.9)
©2001 CRC Press LLC

(ii)
There exists a universal constant C such that for all p ≥1,
E|
n

i=1
di|p ≤CpE|
n

i=1
ei|p.
COROLLARY 7.3.5
Let {di} be a mean-zero martingale diﬀerence sequence. Then there exist
a σ-ﬁeld G and a G-conditionally independent sequence {ei}, tangent to
{di}, such that for all λ,
Eg exp{λΣn
i=1di} ≤E1/2{g2 exp(4λΣn
i=1eiri)},
(7.3.10)
where {ri} is a sequence of i.i.d. random variables independent of all the
variables involved and with P(ri = 1) = P(ri = −1) = 1
2.
COROLLARY 7.3.6
Let {Di}, {Ei} be two sequences of Fi-adapted events. Assume that the
sequences are Fi-tangent (that is, their indicator variables are tangent).
Furthermore, assume that {Ei} is conditionally independent given G.
Then for all G-measurable events G,
P(∩n
i=1Di ∩G) ≤P(∩n
i=1Ei ∩G| ∩n
i=1 Di ∩G).
(7.3.11)
Taking G = Ωin (7.3.11) yields
P(∩n
i=1Di) ≤P(∩n
i=1Ei| ∩n
i=1 Di),
(7.3.12)
and hence
P(∩n
i=1Di) ≤P 1/2(∩n
i=1Ei).
(7.3.13)
Setting Di = {di ≤t} and Ei = {ei ≤t}, or alternatively Di = (di ≥t)
and Ei = (ei ≥t), gives
P( max
1≤i≤n di ≤t) ≤P 1/2( max
1≤i≤n ei ≤t),
(7.3.14)
P( min
1≤i≤n di ≥t) ≤P 1/2( min
1≤i≤n ei ≥t).
(7.3.15)
From Theorem 7.3.4(ii) it follows that for all p ≥1,
©2001 CRC Press LLC

E exp

|
n

i=1
di|p

≤E exp

Cp|
n

i=1
ei|p

,
(7.3.16)
in which C is a universal constant. Hitczenko and Montgomery-Smith
(1996) have extended Theorem 7.3.4(ii) to functions Φ ∈Aα. Speciﬁ-
cally, under the assumptions of the theorem, for all Φ ∈Aα,
EΦ





n

i=1
di






≤C1+αEΦ





n

i=1
ei






.
(7.3.17)
A generalization of (7.3.9) by de la Pe˜na (1999) yields that for p−1 +
q−1 = 1 (p, q ≥1),
E exp

λ
n

i=1
di

≤E1/p

Ep/q

exp

qλ
n

i=1
ei

| G

. (7.3.18)
In particular, consider the stochastic regression model zn = βT un +
ϵn, where ϵ1, ϵ2 . . . are independent random variables with zero means,
representing unobservable disturbances, and the regressors un are Fn−1-
measurable random vectors. The sum Sn = n
1 uiϵi plays a basic role
in the analysis of the least squares estimate
ˆβ =
 n

1
uiuT
i
−1
n

i=1
uizi = β +
 n

1
uiuT
i
−1
n

1
uiϵi.
Let ψi(t) = log(Eetϵi) be the cumulant generating function of ϵi. Then
(3.18) yields
E exp

λT
n

i=1
uiϵi

≤E1/p

exp

pq−1
n

i=1
ψi(qλT ui)

,
(7.3.19)
which can be used to derive tail probability bounds n
1 uiϵi.
7.4
Applications to Martingale Inequalities and
Exponential Tail Probability Bounds
Let {di, Fi, i ≥1} be a mean-zero martingale diﬀerence sequence. Let
{ei} be CI (conditionally independent) given G and such that {di} and
©2001 CRC Press LLC

{ei} are tangent sequences. By Theorem 7.3.4(ii), for all p ≥1,
					
n

i=1
di
					
p
≤C
					
n

1
ei
					
p
for some universal constant C. Since the ei are independent zero-mean
random variables given G, it then follows from Rosenthal’s inequality
that

E





n

i=1
ei





p
| G
1/p
≤Cp

n

i=1
E(e2
i |G)
1/2
+ [E(sup
k≤n
|ei|p|G)]1/p

.
(7.4.1)
Hence
					
n

1
ei
					
p
=
						

E





n

1
ei





p
| G
1/p						
p
≤Bp(∥sn∥p + ∥e∗
n∥p),
where s2
n = n
1 E(d2
i |Fi−1) = n
1 E(e2
i |Fi−1) = n
1 E(e2
i |G) and e∗
n =
supk≤n |ek|. Moreover, ∥e∗
n∥p ≤21/p∥d∗
n∥p by (7.3.2). Hence, decoupling
inequalities for tangent sequences and Rosenthal’s inequality for sums of
independent zero-mean random variables yield the martingale inequality
					
n

i=1
di
					
p
≤Bp{∥sn∥p + ∥d∗
n∥p},
(7.4.2)
which was derived by Burkholder (1973) using distribution function in-
equalities. Hitczenko (1990) used this approach to derive the “best pos-
sible” constant Bp, depending only on p, with the slowest growth rate
p/ log p as p →∞. In the case p ≥2, Hitczenko (1994) also derived by
a similar decoupling argument the following variant of (7.4.2):
					
n

i=1
di
					
p
≤B{√p ∥sn∥p + p∥d∗
n∥p}
(7.4.3)
for some universal constant B.
Decoupling inequalities for moment generating functions and variants
thereof (such as Theorem 7.3.4 and Corollary 7.3.5) are particularly use-
ful for developing extensions of typical exponential inequalities for sums
©2001 CRC Press LLC

of zero-mean independent random variables to the case of the ratio of a
martingale over its conditional variance. Let {di, Fi, i ≥1} be a martin-
gale diﬀerence sequence. Let Mn = n
i=1 di and V 2
n = n
i=1 E(d2
i |Fi−1).
Consider the problem of bounding the tail probability of P(Mn/V 2
n ≥x).
Following the standard approach to such problems, one obtains
P
 n

i=1
di ≥xV 2
n

= P

exp(λ
n

i=1
di) ≥exp(λxV 2
n )

≤inf
λ>0 E

exp(−λxV 2
n ) exp

λ
n

i=1
di

.
Corollary 7.3.5 can be used here (with g = exp(λxV 2
n )) to bound the
last expression by
inf
λ>0 E1/2

exp(−2λxV 2
n ) exp

2λ
n

i=1
ei

,
where {ei} is a decoupled version of {di} and hence conditionally inde-
pendent given G = σ({di}). Conditional on G one can proceed as if the
variables were independent, and de la Pe˜na (1999) used this method to
derive new exponential tail probability bounds for Mn/V 2
n , including the
following result extending sharply Prokhorov’s inequality.
THEOREM 7.4.1
Let {di, Fi} be a martingale diﬀerence sequence. Let Mn = n
i=1 di,
V 2
n = n
i=1 E(d2
i |Fi−1) or V 2
n = || n
i=1 E(d2
i |Fi−1)||∞. Assume that
for some c > 0, |dj| ≤c a.s. for all j. Then for all x ≥0
P
Mn
V 2
n
> x, 1
V 2
n
≤y for some n

≤exp

−cx
2y arc sinh cx
2

.
(7.4.4)
7.5
Decoupling of Multilinear Forms, U-Statistics
and U-Processes
In this section we let X1, X2, . . . be independent random variables tak-
ing values in some measurable space S. Motivated in part by the de-
sire to extend the theory of multiple stochastic integration, the work of
©2001 CRC Press LLC

McConnell and Taqqu (1986) provided decoupling inequalities for mul-
tilinear forms of independent symmetric variables. After their paper, a
large amount of work has been done, providing extensions of this result
in many directions, including Kwapien (1987), Bourgain and Tzafriri
(1987), de Acosta (1987), Krakowiak and Szulga (1988), Kwapien and
Woyczynski (1992), de la Pe˜na (1992), de la Pe˜na, Montgomery-Smith,
and Szulga (1994), de la Pe˜na and Montgomery-Smith (1994, 1995), and
Szulga (1998). In particular, de la Pe˜na (1992) provided an extension to
a general class of statistics that include both U-statistics and multilinear
forms, while de la Pe˜na and Montgomery-Smith (1994, 1995) established
the following tail probability comparison for such statistics.
THEOREM 7.5.1
Let {Xi} be a sequence of independent random variables with values in
a measurable space (S, S). Let {X(1)
i
, X(2)
i
, ..., X(k)
i
} be k independent
copies of {Xi}. Let fi1,...,ik : Sk →B, where (B, ||·||) is a Banach space.
Then for all t ≥0,
P



						

1≤i1̸=i2̸=....̸=ik≤n
fi1...ik(Xi1, ..., Xik)


≥t



≤CkP


Ck
						

1≤i1̸=i2̸=....̸=ik≤n
fi1...ik(X(1)
i1 , ..., X(k)
ik )


≥t


,
where Ck > 0 is a constant depending only on k and 1 ̸= i1 ̸= i2 ̸= ...
̸= ik ≤n means ir ̸= is for r ̸= s.
The reverse inequality in Theorem 7.5.1 holds whenever
fi1...ik(xi1, ..., xik) = fi1...ik(xiσ(1), ..., xiσ(k))
for all permutations σ of (1, 2, ..., k). To better understand this result,
focus on the case k = 2. One can easily see that fij(Xi, Xj) = aijXiXj
reduces to quadratic forms while fij(Xi, Xj) = f(Xi, Xj)/
n
2

corre-
sponds to U-statistics. An important U-statistic is the sample variance
n
i=1(Xi −¯X)2
(n −1)
=
n

j=1
n

i=1;i̸=j
(Xi −Xj)2
n
2

,
whose tail probability is related by Theorem 7.5.1 to that of
n
j=1
Gj(X(2)
j
),
which is a sum of independent random variables when we condition on
{X(1)
i
}, where Gj(y) = n
i=1;i̸=j(X(1)
i
−y)2/
n
2

.
©2001 CRC Press LLC

There are potential applications of results like Theorem 7.5.1 to the
study of random graphs.
The following example, taken from Janson
and Nowicki (1991) who study the properties of random graphs by in-
troducing a generalized form of U-statistics, will help illustrate such
applications,
Example 2
Let {Xi} be a sequence of i.i.d. Bernoulli random variables with P(Xi =
1) = p1, P(Xi = 0) = 1 −p1. Let Yij be i.i.d. with P(Yij = 1) = p2,
P(Yij = 0) = 1 −p2. In the context of random graphs, Yij = 1 indicates
that there is an edge between vertices i and j, while Xi = 1 or 0 according
as vertex i is colored red or blue. Letting f(x1, x2, y) = y1(x1 ̸= x2),
Sn(f) =

1≤i<j≤n
f(Xi, Xj, Yij)
is the number of edges with diﬀerently colored vertices. It is easy to see
how to extend Theorem 7.5.1 to this type of statistics because in view
of the assumed independence between {Xi} and {Yij}, conditioning on
{Yij} yields a random variable to which Theorem 7.5.1can be applied.
An application of this theorem therefore reduces the problem to one
involving sums of conditionally independent random variables.
Decoupling inequalities like the one presented in Theorem 7.5.1 are
also important in the study of multiple stochastic integrals of the form

. . .

0≤t1<...<tk≤t
g(t1, . . . , tk)dYt1 . . . dYtk,
where Yt is a process with independent increments. The reason for this
is that such stochastic integrals are limits of the multilinear forms of the
type

1≤i1<...<ik≤n
wi1,...,ikXi1 . . . Xik,
where the Xi are independent random variables; see Kwapien and Woy-
czynski (1992).
An important application of Theorem 7.5.1 and its variants is to de-
rive symmetrization inequalities by using the fact that the decoupled
variables are conditionally independent. In particular, for U-statistics,
Nolan and Pollard (1987) made use of such symmetrization inequalities
to prove weak convergence of a functional version of U-statistics known
as a U-process. Arcones and Gin´e (1993) extended this work to more
general U-processes indexed by families of functions. A detailed account
©2001 CRC Press LLC

of these and subsequent developments and the role of symmetrization
inequalities in the theory of U-processes can be found in Chapter 5 of de
la Pe˜na and Gin´e (1999).
In Theorem 7.5.5 below we present a key symmetrization inequal-
ity that can be derived by using decoupling and the following classical
symmetrization inequality for sums of independent zero-mean random
variables.
LEMMA 7.5.2
Let {Xi} be a sequence of independent zero-mean random variables with
values in a separable Banach space (B, || ·|| ). Let {εi} be a sequence of
independent Bernoulli random variables, independent of {Xi} and with
P(εi = 1) = P(εi = −1) = 1
2. Then for all convex increasing functions
Φ,
EΦ(||1
2
n

i=1
Xiεi||) ≤EΦ(||
n

i=1
Xi||) ≤EΦ(||2
n

i=1
Xiεi||).
DEFINITION 7.5.3
Let {Xi} be a sequence of i.i.d. variables with values in a measurable
space (S, S). For k ≥1 let f : Sk →R be an integrable function such that
f(xσ(1), . . . , xσ(k)) = f(x1, . . . , xk) for all permutations σ of (1, . . . , k).
Then f is said to be P-degenerate of order r−1 (1 < r ≤k), with respect
to the probability law P of X1, if
E{f(X1, .., Xk)|X1, ..., Xr−1} = Ef(X1, ..., Xk)
and E(f(X1, .., Xk)|X1, ..., Xr) is not a constant function.
THEOREM 7.5.4
Let 1 < r ≤k and p ≥1. Let {Xi} be a sequence of i.i.d. random
variables with values in a measurable space (S, S). Let f : Sk →R be
a P-degenerate function of order r −1 with Ef(X1, . . . , Xk) = 0 and
E|f(X1, ..., Xk)|p < ∞. Then
E







1≤i1̸=i2̸=...̸=ik≤n
f(Xi1, ..., Xik)






p
≈p E







1≤i1̸=i2̸=...̸=ik≤n
f(Xi1, ..., Xik)εi1...εir






p
,
where a ≈p b denote two-sided inequalities between a and b possibly
multiplied by constants that depend only on p.
©2001 CRC Press LLC

In fact, due to the Banach-space nature of Theorem 7.5.1, the above
result is still valid if one replaces f by a class of functions and the abso-
lute value by the supremum this class. Moreover, Theorem 7.5.5 can also
be stated for convex increasing functions instead of pth powers. Obvi-
ously, by conditioning on the Xi’s, Theorem 7.5.5 permits the reduction
of problems involving functions of the random variables Xi’s to problems
involving multilinear forms of the form 
1≤i1 ̸=i2 ̸=...̸=ik≤n ai1 ...ikεi1...εir
for constants ai1 ...ik and hence permits the use of Khintchine-type in-
equalities for such random variables. Another application of this sym-
metrization and decoupling inequalities is an extension of Bernstein’s
inequality for sums of independent random variables to the case of U-
statistics, see Section 4.1 of de la Pe˜na and Gin´e (1999).
7.6
Total Decoupling of Stopping Times
We begin by stating a decoupling reformulation of Wald’s equation in
sequential analysis.
Let X1, X2, . . . be i.i.d.
random variables with
EXi = µ. Then for any stopping time T with ET < ∞,
E
 T

i=1
Xi

= µET = E
 T

i=1

Xi

,
(7.6.1)
where { 
Xi} is an independent copy of {Xi} and hence independent of T.
Note that conditional on T, T
i=1 
Xi is a sum of independent random
variables. Let Sn = n
i=1 Xi, Sn = n
i=1 
X1. The following result of
Klass (1988, 1990) provides basic decoupling inequalities for randomly
stopped sums.
THEOREM 7.6.1
Let {Xi} be a sequence of independent random variables taking values in
a Banach space (B, || · ||). Let T be a stopping time adapted to this se-
quence. Let { ˜Xi} be an independent copy of {Xi} and hence independent
of T. Then, for all Φ ∈Aα (see Deﬁnition 7.1.4), there exist constants
0 < bα, Bα < ∞depending only on α for which
bαE max
1≤n≤T Φ(|| ˜Sn||) ≤E max
1≤n≤T Φ(||Sn||) ≤BαE max
1≤n≤T Φ(|| ˜Sn||).
(7.6.2)
©2001 CRC Press LLC

Making use of the decoupling inequality (7.6.2) with Φ(x) = x and
the fact that E(max1≤k≤n ∥Sk −ESk∥) ≤4E∥Sn −ESn∥, Klass (1988)
established the following generalization of Wald’s equation (7.6.1).
THEOREM 7.6.2
Let {Xi} be a sequence of independent random variables with values in
a Banach space (B, ∥· ∥) such that EXi = µi.
Let T be a stopping
time.
Then E(T
i=1 Xi) = E(T
i=1 µi) whenever EaT < ∞, where
an = E∥n
i=1 Xi∥.
In particular, if the Xi are i.i.d. real-valued symmetric stable ran-
dom variables with index 1 < q ≤2, then Theorem 7.6.2 yields that
E(T
i=1 Xi) = 0 whenever ET 1/q < ∞, since in this case
an = E|
n

i=1
Xi| = n1/qE|X1|.
By making use of tangent sequence decoupling, we can generalize this
result far beyond the case of i.i.d. symmetric stable Xi. This is the
content of the following theorem.
THEOREM 7.6.3
Let {Xn, Fn, n ≥1} be a martingale diﬀerence sequence and let Sn =
n
i=1 Xi. Let 1 ≤p < q < 2. Suppose there exist nonrandom constants
κ and τ such that
P(|Xn| > t|Fn−1) ≤κt−q a.s. for all n ≥1 and t ≥τ.
(7.6.3)
Then there exists a constant Cp,q,κ,τ depending only on p, q, κ, and τ
such that for any stopping time T,
E( max
1≤n≤T |Sn|p) ≤Cp,q,κ,τET p/q.
PROOF Apply (7.3.5) with Φ(x) = xp/2 to di = X2
i and ei such that
{di} and {ei} are tangent sequences and {ei} is conditionally indepen-
dent given G with respect to which T is measurable; see Proposition
7.2.6 for the existence of such G and ei. Since {ei1{T ≥i}) is tangent
to (di1{T ≥i}), it follows from (7.3.5) that
E
 T

i=1
X2
i
p/2
≤Cp/2E
 T

i=1
ei
p/2
©2001 CRC Press LLC

= Cp/2E


E


 T

i=1
ei
p/2
|G




.
(7.6.4)
Recall that T is measurable with respect to G and that {en} is condi-
tionally independent given G with L(en|G) = L(X2
n|Fn−1) a.s. Noting
that 0 < α := q/2 < 1, let Gα be the distribution function of the positive
stable random variable with Laplace transform exp(−λα), λ > 0.
Take b > 0 such that bα/Γ(1−α) > κ. Since limx→∞xα(1−Gα(x)) =
1/Γ(1 −α) [cf. Feller (1971, p. 448)], (7.6.3) implies that there exists
η ≥τ such that with probability 1,
P(en > x|G) ≤κx−q/2 < 1 −Gα(x/b) for all n ≥1 and rational x ≥η.
(7.6.5)
Let Fn be the conditional distribution of en given G. In view of the
right continuity of distribution functions, it follows from (7.6.5) that with
probability 1, 1−Fn(x) ≤1−Gα(x/b) for all n ≥1 and x ≥η. Therefore,
letting F −1(u) = sup{x : F(x) ≤u} for 0 ≤u < 1 and θ = bG−1
α (η), we
have F −1
n (u) ≤bG−1
α (u) + θ for all n ≥1 and 0 ≤u < 1. Replacing u
in the preceding inequality by i.i.d. uniform random variables U1, U2, . . .
on [0, 1) and noting that conditional on G (with respect to which T is
measurable) the ei are independent random variables with distribution
functions Fi, we obtain that
P
 T

i=1
ei > x|G

≤P
 T

i=1
(bYi + θ) > x

a.s.,
(7.6.6)
where Y1, Y2, . . . are i.i.d. random variables with common distribution
function Gα and independent of T. Since n
i=1 Yi has the same distri-
bution as n1/αY1, (7.6.6) implies that
E


E


 T

i=1
ei
p/2
| G




≤E(bY1T 1/α + θT)p/2.
(7.6.7)
Noting that T ≤T 1/α and that EY p/2
1
< ∞since α = q/2 > p/2, the de-
sired conclusion follows from (7.6.4), (7.6.7) and Burkholder’s inequality
E( max
1≤n≤T |Sn|p) ≤BpE{(
T

i=1
X2
i )p/2}.
©2001 CRC Press LLC

One direction of extending total decoupling of a stopping time in Theo-
rem 7.6.1 deals with randomly stopped multilinear forms, and more gen-
erally, de-normalized U-statistics of i.i.d. random variables X1, X2, . . .
taking values in a measurable space (S, S) such that E|f(X1, . . . , Xk)|p <
∞for some f : Sk →R. This has been undertaken by Chow, de la
Pe˜na and Teicher (1993), de la Pe˜na and Lai (1997a,b) and Borovskikh
and Weber (1998).
Let {X(j)
i
}, j = 1, ..., k, be k independent copies
of {Xi}. Then by combining the decoupling inequality (7.3.8) for tan-
gent sequences with Burkholder’s inequality for martingales, and using
the bound E| n
i=1 Zi|p ≤Apnp/nE|Z1|p for i.i.d. zero-mean random
variables Z1, Z2, . . . together with H¨older’s inequality to bound
E


T p/2






T

ik−1=k−1
ik−1−1

ik−2=k−2
. . .
i2−1

i1=1
f(Xi1, . . . , Xik−1, ˜X(k)
1 )








,
de la Pe˜na and Lai (1997b) proved the following decoupling inequality
for randomly stopped de-normalized U-statistics in the case p ≥2:
E max
k≤n≤T







1≤i1<...<ik≤n
f(Xi1, . . . , Xik)






p
≤Ck,p(ET kp/2)(E|f( ˜X(1)
1 , . . . , ˜X(k)
1 )|kp)1/k.
(7.6.8)
Moreover, for the case 1 ≤p < 2, assuming that f(xσ(1), . . . , xσ(k)) =
f(x1, . . . , xk) for any permutation of (1, . . . , k), de la Pe˜na and Lai
(1997b) also proved the decoupling inequality
E max
k≤n≤T







1≤i1<...<ik≤n
f(Xi1, . . . , Xik)






p
≤Ck,p
k−1

h=0
k−h

t=1
E




T

ih+1=1
. . .
T

ih+t=1
fT,k,h( ˜X(h+1)
ih+1
. . . ,
˜X(h+t)
ih+1 , . . . , ˜X(k)
ih+1)




p
,
(7.6.9)
where fT,k,0 = f and for h ≥1, fT,k,h : Sk−h →R is given by
fT,k,h(yh+1, . . . , yk) =

1≤i1<...<ih≤T
f(Xi1, . . . , Xih, yh+1, . . . , yk).
Another direction of extending inequality (7.6.2) for total decoupling
of a stopping time deals with continuous-time processes with indepen-
dent increments. In particular, the following extension of Theorems 7.6.1
©2001 CRC Press LLC

and 7.6.2 is given by de la Pe˜na and Eisenbaum (1997) [see also de la
Pe˜na and Gin´e (1999)].
THEOREM 7.6.4
Let {Nt} be a continuous-time process, taking values in a Banach space
(B, ∥·∥), with independent increments and right-continuous sample paths
that have left-hand limits. Let T be a stopping time.
(i)
Let { ˜Nt} be an independent copy of {Nt} and hence independent
of T. Then, for all Φ ∈Aα, there exist constants 0 < bα, Bα < ∞
depending only on α for which
bαE sup
0≤s≤T
Φ(|| ˜Ns||) ≤E sup
0≤s≤T
Φ(||Ns||) ≤BαE max
0≤s≤T Φ(|| ˜Ns||).
(ii)
Let mt = ENt and bt = E sups≤t ||Ns −ms||. Then ENT = EmT
whenever EbT < ∞.
Concerning the decoupling of randomly stopped stochastic integrals,
de la Pe˜na and Eisenbaum (1994) introduced an approach based on the
development of decoupling inequalities for randomly stopped local times.
The fact that local times can be decoupled follows from the inequalities of
Barlow and Yor (1981), leading to part (i) of the following theorem. Part
(ii) is a further reﬁnement that can be applied to prove the decoupling
inequality in Theorem 7.6.6 for stochastic integrals; see de la Pe˜na and
Eisenbaum (1994) and Yang (1999) for a recent extension of Theorem
7.6.6.
THEOREM 7.6.5
Let Bt, t ≥0 be a standard Brownian motion and let {Lx
t (B) : x ∈R, t ≥
0} denote the family of local times of {Bt}. Let ˜Bt, t ≥0, be another
standard Brownian motion independent of Bt, t ≥0. Let T be a stopping
time adapted to σ(Bs, s ≥0).
(i)
For all p > 0, there exist universal constants 0 < cp, Cp < ∞
depending only on p such that
cpE sup
x∈R
[Lx
T ( ˜B)]p ≤E sup
x∈R
[Lx
T (B)]p ≤CpE sup
x∈R
[Lx
T ( ˜B)]p.
(ii)
For all x ∈R and all p ≥1, there exists a universal constant
Cp < ∞depending only on p such that
E sup
y∈R
[Ly
T (B)]p ≤Cp{E[Lx
T ( ˜B)]p + |x|p}.
©2001 CRC Press LLC

THEOREM 7.6.6
Let f be a Borel function such that
 ∞
0
f 2(t)dt < ∞. Let Bt, t ≥0, be
standard Brownian motion, and let Mt =
 t
0 f(Bs)dBs. Let { ˜
Mt, t ≥0}
be an independent copy of {Mt, t ≥0}. Then for all p ≥1, there exists a
constant Cp(f) such that for every stopping time T adapted to σ(Bs, s ≥
0),
E sup
s≤T
|Ms|p ≤Cp(f){E sup
s≤T
| ˜
Ms|p + 1}. 
(7.6.10)
7.7 
Principle of Conditioning in Weak Convergence
Decoupled tangent sequences are also useful for proving convergence in
distribution of sums of dependent random variables. Jakubowski (1986)
introduced a “principle of conditioning” for deriving such limit laws.
This principle starts with a limit theorem for sums of independent ran-
dom variables and extends it to sums of dependent random variables by
replacing
(a) expectations of functions of summands by conditional expectations
with respect to the past history,
(b) nonrandom number of summands by a random number, deﬁned by
a stopping time, of summands,
(c) convergence of nonrandom numbers by convergence in probability
of the random variables.
Let {Xnk; 1 ≤k ≤τn, n ≥1} be a double array of random vari-
ables adapted to a sequence of ﬁltrations {Fnk, k ≥1}, of which τn
is a stopping time. The corresponding independent random variables
Ynk in the principle of conditioning are such that (i) Ynk1{τn ≥k} has
the same conditional distribution given Fn,k−1 as Xnk1{τn ≥k} and
(ii) Yn1, Yn2, . . . are conditionally independent given the σ-ﬁeld Gn =
σ(∞
k =1 Fnk). The construction of such Ynk has been discussed in Sec-
tion 7.2. Jakubowski (1986) has proved the following result relating the
limit law of τn
k=1 Xnk to the conditional limit law of τn
k=1 Ynk given
Gn, justifying the preceding principle of conditioning.
THEOREM 7.7.1
Let µn(τn) denote the conditional law of τn
k=1 Ynk given Gn.
(i)
Suppose µn(τn) converges weakly in probability to some nonrandom
©2001 CRC Press LLC

probability measure µ whose characteristic function is nowhere 0.
Then τn
k=1 Xnk converges weakly to µ as n →∞.
(ii)
Suppose {µn(τn), n ≥1} is tight with probability 1. Then {
τn

k=1
Xnk,
n ≥1} is tight.
(iii)
Suppose µn(τn) converges weakly in probability to some nonrandom
probability measure µ. Then {τn
k=1 Xnk, n ≥1} is tight. If fur-
thermore the equation ν ∗µ = µ ∗µ has a unique solution µ, then
τn
k=1 Xnk converges weakly to µ as n →∞.
(iv)
In the case Xnk = B−1
n Xk and Fnk = Fk, where Bn are positive
constants such that limn→∞Bn = ∞, suppose µn(τn) converges
weakly in probability to a random measure µ such that ˆµ(t) ̸= 0
a.s. for t belonging to a dense subset of the real line, where ˆµ is
the characteristic function of µ. Then τn
k=1 Xnk converges weakly
to Eµ as n →∞.
Jakubowski (1986) also provides a functional version of Theorem 7.7.1
which is used to derive a functional central limit theorem for martingales.
Theorem 7.7.1 can be used to derive nonnormal limiting distributions of
martingales, as illustrated by the following.
Example 3
Let {Xnk, Fnk, 1 ≤k ≤n} be a martingale diﬀerence array (i.e.,
E(Xnk|Fn,k−1) = 0).
Suppose max1≤k≤n E(X2
nk|Fn,k−1)
P→0 and there exists a bounded non-
decreasing function H for which
n

k=1
E{X2
nk1(a < Xnk ≤b)|Fn,k−1}
P→H(b) −H(a)
as
n →∞,
for all continuity points a, b of H. From Theorem 7.7.1 and the limit-
ing inﬁnitely divisible law of the sum n
k=1 Ynk of independent random
variables Ynk (conditional on Gn), it follows that n
k=1 Xnk converges
weakly to an inﬁnitely divisible limit law whose characteristic function
φ(t) is given by
log φ(t) =
 ∞
−∞
(eitx −1 −itx)x−2dH(x).
This result was proved by Brown and Eagleson (1971) by a diﬀerent
argument which uses an elaborate analysis of conditional characteristic
functions.
©2001 CRC Press LLC

Example 4
Let {Xk, Fk, k ≥1} be a martingale diﬀerence sequence such that there
exist nonrandom constants Bn →∞for which B−2
n
n
k =1 E(X 2
k|Fk−1)
converges in probability to a random variable V with distribution func-
tion F. Assume also the conditional Lindeberg condition
B−2
n Σn
k =1E{X 2
k1(|Xk| ≥ϵ)|Fk−1 }
P→0
for every
ϵ > 0.
Let Xnk = B−1
n Xk and apply Theorem 7.7.1(iv) in conjunction with
Proposition 7.2.5.
Conditional on G, the Ynk are independent zero-
mean random variables satisfying the Lindeberg condition, so by the
central limit theorem, n
k =1 Ynk has a limiting normal distribution with
mean 0 and variance V (which is G-measurable). Hence by Theorem
7.7.1(iv), n
k =1 Xnk has a limiting distribution function of the form
 ∞
0
N(x/√v)dF(v), where N(x) denotes the standard normal distri-
bution function.
7.8 
Conclusion
The basic idea behind decoupling is to reduce problems concerning de-
pendent random variables to corresponding ones for independent ran-
dom variables.
It was introduced by Burkholder and McConnell to
extend martingale inequalities to Banach-space valued martingales, for
which diﬃculties in generalizing the concept of the square function have
been circumvented by using a decoupled (conditionally independent) ver-
sion that can be handled by symmetrization techniques; see Burkholder
(1983), Zinn (1985), McConnell (1989). Since then it has found many
diﬀerent applications which have in turn inspired its extensive develop-
ment. The relatively simple derivations of the new results in Theorem
7.6.3 and Example 4 illustrate the elegance and power of this idea, whose
important applications to U-processes, multiple stochastic integration
and randomly stopped sums or integrals are brieﬂy reviewed in Sections
7.5 and 7.6. Section 7.2 describes the basic concept of decoupled tan-
gent sequences, and the decoupling inequalities for tangent sequences in
Sections 7.3 and 7.4 can be regarded as a “second generation” of martin-
gale inequalities. This decoupling approach is particularly useful for the
development of exponential inequalities for martingales, as illustrated in
Theorems 7.3.4 and 7.4.1. Without appealing to conditioning arguments,
Section 7.1 shows that it is also possible to obtain one-sided inequali-
ties by using only the marginal distributions of the random variables to
decouple.
©2001 CRC Press LLC

Acknowledgement The research of the ﬁrst author was partially sup-
ported by the National Science Foundation grant DMS-9626236 and
DMS-99-72237, and that of the second author by the National Science
Foundation grant DMS-9704324 and the National Security Agency grant
MDA-904-98-1-0017.
References
1. Arcones, M. A. and Gin´e, E. (1993). Limit theorems for U-processes.
Annals of Probability 21, 1494–1542.
2. Barlow, M. T. and Yor, M. (1981). (Semi-)martingale inequalities
and local time.
Zeitschrift Wahrscheinlichkeitstheorie und Ver-
wandte Gebiete 55, 237–254.
3. Borovskikh, Y. V. and Weber, N. C. (1998). On Wald’s equation
for U-statistical sums. Technical Report. School of Mathematics
and Statistics, University of Sydney.
4. Bourgain, J. and Tzafriri, L. (1987). Invertibility of “large” sub-
matrices with applications to the geometry of Banach spaces and
harmonic analysis. Israel Journal of Mathematics 57, 137–224.
5. Brown, B. M. and Eagleson, G. K. (1971). Martingale convergence
to inﬁnitely divisible laws with ﬁnite variances. Transactions of
the American Mathematical Society 162, 449–453.
6. Burkholder, D. L. (1973).
Distribution function inequalities for
martingales. Annals of Probability 1, 19–42.
7. Burkholder, D. L. (1983). A geometric condition that implies the
existence of certain singular integrals of Banach-space-valued func-
tions. In Conference on Harmonic Analysis in Honor of Antoni
Zygmund (Eds., W. Beckner, A. P. Calder´on, R. Feﬀerman, and P.
W. Jones). Wadsworth, Belmont, California.
8. Chow, Y. S., de la Pe˜na, V. H., and Teicher, H. (1993). Wald’s
lemma for a class of de-normalized U-statistics. Annals of Proba-
bility 21, 1151–1158.
9. de Acosta, A. (1987). A decoupling inequality for multilinear forms
of stable vectors. Probability and Mathematical Statistics 8, 71–76.
10. de la Pe˜na, V. H. (1990). Bounds for the expectation of functions of
martingales and sums of positive rv’s in terms of norms of sums of
independent random variables. Proceedings of the American Math-
ematical Society 108, 233–239.
11. de la Pe˜na, V. H. (1992). Decoupling and Khintchine’s inequalities
©2001 CRC Press LLC

for U-statistics. Annals of Probability 20, 1877–1892.
12. de la Pe˜na, V. H. (1993). Inequalities for tails of adapted processes
with an application to Wald’s lemma. Journal of Theoretical Prob-
ability 6, 285–302.
13. de la Pe˜na, V. H. (1994). A bound on the moment generating func-
tion of a sum of dependent variables with an application to simple
random sampling without replacement. Annals of the Institute of
H. Poincar´e. Probability and Statistics 30, 197–211. Correction
note in 31, 703–704.
14. de la Pe˜na, V. H. (1996). From dependence to complete indepen-
dence: The decoupling approach. Proceedings of the IV Simposio
de Probabilidad y Procesos Estocasticos, Guanajuato, Mexico. No-
tas de Investigaci´on No. 12, Sociedad Matem´atica Mexicana.
15. de la Pe˜na, V. H. (1999). A general class of exponential inequalities
for martingales and ratios. Annals of Probability 27, 537–564.
16. de la Pe˜na, V. H. and Eisenbaum, N. (1994). Decoupling inequal-
ities for the local times of a linear Brownian motion. Technical
Report, Department of Statistics, Columbia University.
17. de la Pe˜na, V. H. and Eisenbaum, N. (1997). Exponential Burkholder-
Davis-Gundy inequalities.
Bulletin of the London Mathematical
Society 29, 239–242.
18. de la Pe˜na, V. H. and Gin´e, E. (1999). Decoupling: From Depen-
dence to Independence. Springer-Verlag, New York.
19. de la Pe˜na, V. H. and Lai, T. L. (1997a) Wald’s equation and
asymptotic bias of randomly stopped U-statistics. Proceedings of
the American Mathematical Society 125, 917–925.
20. de la Pe˜na, V. H. and Lai, T. L. (1997b) Moments of randomly
stopped U-statistics. Annals of Probability 25, 2055–2081.
21. de la Pe˜na, V. H. and Montgomery-Smith, S. J. (1994). Bounds on
the tail probability of U-statistics and quadratic forms. Bulletin of
the American Mathematical Society 31, 223–227.
22. de la Pe˜na, V. H. and Montgomery-Smith S. J. (1995). Decoupling
inequalities for the tail probabilities of multivariate U-statistics.
Annals of Probability 23, 806–816.
23. de la Pe˜na, V. H., Montgomery-Smith S. J., and Szulga, J. (1994).
Contraction and decoupling inequalities for multilinear forms and
U-statistics. Annals of Probability 22, 1745–1765.
24. Feller, W. (1971). An Introduction to Probability Theory and Its
Applications, Volume II. Second Edition. John Wiley & Sons, New
York.
©2001 CRC Press LLC

25. Hitczenko, P. (1988).
Comparison of moments for tangent se-
quences of random variables. Probability Theory and Related Fields
78, 223–230.
26. Hitczenko, P. (1990). Best constants in martingale version of Rosen-
thal’s inequality. Annals of Probability 18, 1656–1668.
27. Hitczenko, P. (1994). On a domination of sums of random variables
by sums of conditionally independent ones. Annals of Probability
22, 453–468.
28. Hitczenko, P. and Montgomery-Smith, S. J. (1996). Tangent se-
quences in Orlicz and rearrangement invariant spaces. Proceedings
of the Cambridge Philosophical Society 119, 91–101.
29. Hoeﬀding, W. (1963). Probability inequalities for sums of bounded
random variables. Journal of the American Statistical Association
58, 13–30.
30. Huang, T. (1999). Steady-state asymptotics for queuing networks
with heavy tailed service.
Ph.D. thesis.
Department of IEOR,
Columbia University.
31. Jakubowski, A. (1986). Principle of conditioning in limit theorems
for sums of random variables. Annals of Probability 14, 902–915.
32. Janson, S. and Nowicki, K. (1991). The asymptotic distributions of
generalized U-statistics with applications to random graphs. Prob-
ability Theory and Related Fields 90, 341–375.
33. Joag-Dev, K. and Proschan, F. (1983).
Negative association of
random variables with applications. Annals of Statistics 11, 268–
295.
34. Klass, M. J. (1988). A best possible improvement of Wald’s equa-
tion. Annals of Probability 16, 840–863.
35. Klass, M. J. (1990). Uniform lower bounds for randomly stopped
Banach space valued random sums. Annals of Probability 18, 780–
809.
36. Krakowiak, W. and Szulga, J. (1988). Hypercontraction principle
and multilinear forms in Banach spaces. Probability Theory and
Related Fields 77, 325–342.
37. Kwapien, S. (1987). Decoupling for polynomial chaos. Annals of
Probability 15, 1062–1071.
38. Kwapien, S. and Woyczynski, W. (1992).
Random Series and
Stochastic Integrals: Single and Multiple. Birh¨auser, New York.
39. Lai, T. L. and Robbins, H. (1978). A class of dependent variables
and their maxima. Zeitschrift Wahrscheinlichkeitstheorie und Ver-
wandte Gebiete 42, 89–111.
©2001 CRC Press LLC

40. McConnell, T. (1989). Decoupling and stochastic integration in
UMD Banach spaces. Probability and Mathematical Statistics 10,
283–295.
41. McConnell, T. and Taqqu, M. (1986). Decoupling inequalities for
multilinear forms in independent symmetric Banach valued random
variables. Probability Theory and Related Fields 75, 499–507.
42. Nolan, D. and Pollard, D. (1987). U-processes: rates of conver-
gence. Annals of Statistics 15, 780–799.
43. Shao, Q. M. (1998). A comparison theorem on maximal inequal-
ities between negatively associated and independent random vari-
ables. Technical Report, Department of Mathematics, University
of Oregon.
44. Szulga, J. (1998). Introduction to Random Chaos. Chapman &
Hall, New York.
45. Yang, M. (1999). On the order of ET γ
r – the boundary crossing
problem. Ph.D. thesis, Department of IEOR, Columbia University.
46. Zinn, J. (1985). Comparison of martingale diﬀerence sequences, In
Probability in Banach Spaces V, 453–457. Lecture Notes in Math-
ematics 1153. Springer-Verlag, New York.
©2001 CRC Press LLC

8
A Note on the Probability of Rapid
Extinction of Alleles in a Wright-Fisher
Process
F. Papangelou
University of Manchester, Manchester, UK
University of Athens, Athens, Greece
ABSTRACT The standard formulation of a large deviation principle
involves an upper bound for closed sets and a lower bound for open sets.
Such a principle was established in Papangelou (2000) for a multiallele
Wright-Fisher process. Here we extend under more restrictive assump-
tions the lower bound to boundary sets, thereby determining the expo-
nential asymptotics of the probabilities of actually reaching the boundary
within a short time.
Keywords and phrases Wright-Fisher process, Legendre transform,
large deviation bounds, exponential asymptotics
8.1
Introduction
A multiallele Wright-Fisher process models the way in which the pro-
portions of the various alleles of a gene in a population change from
generation to generation. If the size of the population, N say, is large
then the eﬀects of random genetic drift are noticeable over a time span of
N generations and if mutation and selection act on the same or a longer
time scale then, according to a classical result, the continual changes
of state of the suitably scaled process are approximated by a diﬀusion
process in a sense that we will not make precise here.
Over a much
smaller number n of generations the changes are, with high probability,
imperceptible but there is always a small probability that a large devia-
tion may occur. It is shown in Papangelou (2000) that the appropriate
space for studying the large deviations of the process is the d-sphere, if
there are d + 1 alleles of the gene in the population. More precisely, if
p1, . . . , pd+1 are the proportions of the alleles, then p1 + . . . + pd+1 = 1
©2001 CRC Press LLC

and therefore (√p1, √p2, . . . , √pd+1) is a point on the orthant of the d-
sphere in (d+1)-dimensionsal Euclidean space. Now if there is a change
from the point (√p1, . . . , √pd+1) to the point (√q1, . . . , √qd+1) over a
span of n generations (where both n and N
n are very large), then we
can be nearly certain that the process has followed closely the arc of
the great circle joining (√p1, . . . , √pd+1) with (√q1, . . . , √qd+1) on the
sphere, at constant speed. The “probability” of such a deviation is of
exponential order
exp

−4N
n θ2

where θ is the angle between (√p1, . . . , √pd+1) and (√q1, . . . , √qd+1)
subtended at the centre of the sphere. See Theorems 1 and 2 in Papan-
gelou (2000).
In order to make the above statements precise and explain the aim
of the present note we need to give a full and exact statement of the
large deviation result proved in Papangelou (2000). For a diploid pop-
ulation let 2N be the number of genes in the population and assume
that the proportions of the d + 1 alleles are
i1
2N , i2
2N , . . . , id+1
2N
where
i1 + . . . + id+1 = 2N.
The state of our process is then the vector
y = (y1, . . . , yd), where yk = ik
2N , k = 1, 2, . . . , d. In the Wright-Fisher
process the next generation of 2N genes is produced from the current
generation “multinomially”, i.e. the probability of a transition from state
y = (y1, . . . , yd) to state ˜y = (˜y1, . . . , ˜yd) =
 j1
2N , . . . , jd
2N

is
[(2N)!]
d+1

k=1
(jk!)
−1
πj1
1 πj2
2 . . . πjd+1
d+1
where πk is yk (k = 1, . . . , d) if there is no mutation or selection but
in general πk is yk “perturbed” by mutation or selection eﬀects, and
πd+1 = 1 −
d

k=1
πk.
The large deviation result will be stated for a sequence Y (n)
t
, t ≥0; n =
1, 2, . . . of scaled Wright-Fisher processes deﬁned as follows. For each
n = 1, 2, . . . let Nn be a positive integer such that Nn
n →∞as n →∞.
We stipulate that Y (n)
0
, Y (n)
1
n
, Y (n)
2
n
, . . . is a discrete Wright-Fisher process
as deﬁned above with
πk = πk(y, n) = yk + gk(y) + ok(1)
Nn
©2001 CRC Press LLC

where g1(y), . . . , gd(y) are continuous functions of the vector y = (y1, . . . ,
yd) on the simplex

=
	
y :
d

k=1
yk ≤1, yk ≥0, k = 1, . . . , d

and
ok(1) →0 uniformly in y as n →∞. We further stipulate that Y (n)
t
remains constant on
ν
n ≤t < ν + 1
n
for each ν = 0, 1, 2, . . . and that
Y (n)
0
= p for all n, where p is an interior point of . By Theorem 1 in
Papangelou (2000), if F is a closed subset of  and G a subset of 
open in the relative topology of , then for any T > 0
lim sup
n→∞
n
2Nn
log P(Y (n)
T
∈F) ≤−inf
q∈F Jp,T (q)
(8.1.1)
lim inf
n→∞
n
2Nn
log P(Y (n)
T
∈G) ≥−inf
q∈G Jp,T (q)
(8.1.2)
where
Jp,T = 2
T

cos−1
d+1

k=1
√pkqk
2
(pd+1 = 1 −
d

k=1
pk, qd+1 =
1 −
d

k=1
qk).
These two inequalities embody the large deviation prop-
erty for the sequence Y (n)
T
, n = 1, 2, . . ..
The situation is more complicated in the case of faster-acting mutation
or selection, as shown for d = 1 in Papangelou (1998a,b).
The above large deviation principle raises an interesting problem,
namely that of determining the rough asymptotics of the probabilities
of actually reaching given subsets of the boundary of  in the case of
a large deviation. Such subsets are not open in the relative topology of
 and hence the large deviation lower bound can only tell us something
about the probability of getting “close” to boundary sets. However, the
event of actually reaching the boundary is of great importance. In the
absence of mutation, for instance, the phenomenon of ﬁxation whereby
all but one of the alleles eventually become extinct is of primary interest.
In the present paper we consider the problem just mentioned for the
case of a Wright-Fisher process subject only to random genetic drift,
i.e. without mutation or selection. For such a process, the large devia-
tion upper bound for boundary sets follows trivially from (8.1.1). Our
purpose is to establish the lower bound for boundary sets under the
assumption that n2 = o(Nn).
©2001 CRC Press LLC

8.2
The Lower Bound for Boundary Sets
We base our calculations on the inequality stated in the lemma below,
which is a variant of classical estimates: it is related for instance to one
of the inequalities of Theorem 1.3.13 in Stroock (1993), which however
we take a step further in our case by using the connections between the
derivatives involved. More elaborate calculations based on an alternative
approach lead to a weakening of the condition n2 = o(Nn) but the
technicalities involved are out of proportion with the “improvement”
obtained.
To state the lemma suppose X1, X2, . . . are independent, identically
distributed random variables such that the cumulant generating function
G(z) = log EezX1 is ﬁnite for all z ∈R and let H(u) = sup
z {zu −G(z)},
u ∈R, be the Legendre transform of G. The derivative G′(z) exists for
all z and is increasing, with G′(0) = µ, where µ is the mean of X1.
LEMMA 8.2.1
Suppose that u > µ and that u is in the interior of the range of G′.
Then, for any δ > 0 and any N ≥1,
P(u −δ < 1
N
N

i=1
Xi < u + δ)
≥exp [−NH(u + δ)] ·

1 −

NH′′(u)δ2−1
.
The proof is brief. By assumption there is a z0 such that G′(z0) = u.
It is then clear that H(u) = z0u −G(z0) = z0G′(z0) −G(z0), hence
H′(u) = dH
dz0
· dz0
du = z0G′′(z0) · [G′′(z0)]−1 = z0. If Q denotes the dis-
tribution of X1, introduce the distribution
˜Q(dx) = exp{z0x −G(z0)}Q(dx)
and denote by ˜QN the distribution of 1
N
N

i=1
Xi under the convolution
power ˜QN. If δ > 0, then [cf. Stroock (1993)]
P(u −δ < 1
N
N

i=1
Xi < u + δ)
≥exp [−N(z0(u + δ) −G(z0))] · ˜QN(u −δ, u + δ).
©2001 CRC Press LLC

The right-hand side is greater than or equal to
exp [−N(H(u) + H′(u)δ)] ·

1 −
1
Nδ2 : Variance : ( ˜Q)

by Chebyshev’s inequality.
Now Variance( ˜Q) = G′′(z0) = [H′′(u)]−1
since the inverse of the function u = G′(z0) is the function z0 = H′(u).
Also, H(u) + H′(u)δ ≤H(u + δ) by the convexity of H. This proves the
lemma.
Returning to our process, assume initially that Y (n)
t
, t ≥0 is a scaled
one-dimensional Wright-Fisher process (d = 1) as deﬁned in the intro-
duction, with no mutation or selection. For the sake of convenience we
will write N for Nn. Suppose T > 0 and ϵ ∈

0, 1
2

and consider the
curve φ(t) = 1 −ϵ

1 −t
T
2 , 0 ≤t ≤T. Then
P(Y (n)
T
= 1 | Y (n)
0
= 1 −ϵ)
≥
nT −2

k=0
P

Y (n)
k+1
n
> φ
k + 1
n
 Y (n)
k
n
= φ
k
n

×P

Y (n)
T
= 1
Y (n)
T −1
n = φ

T −1
n

(8.2.1)
assuming for convenience that T is an integral multiple of 1
n.
With
a view to applying the lemma to each factor in the bracketed product
we set, for 0 < y < 1, G(z) = G(y, z) = log(1 + y(ez −1)).
Then
H(u) = H(y, u) = u log u
y + (1 −u)·log 1 −u
1 −y and hence H′′(u) = [u(1−
u)]−1. Note that if u > y > 1
2 then
H(y, u) ≤u
u
y −1

+ (1 −u)
1 −u
1 −y −1

= (u −y)2
y(1 −y) ≤2(u −y)2
1 −y
.
Taking
y = φ
k
n

,
u = φ
k
n

+ (1 + β)

φ
k + 1
n

−φ
k
n

and
δ = β

φ
k + 1
n

−φ
k
n

,
where β is suitably small, we obtain from the lemma the following, in
which H′′(y, u) denotes ∂2H
∂u2 .
©2001 CRC Press LLC

P

Y (n)
k+1
n
> φ
k + 1
n
Y (n)
k
n
= φ
k
n

= P

Y (n)
k+1
n
> u −δ
Y (n)
k
n
= y

≥exp [−2NH(y, u + δ)] ·

1 −(2NH′′(y, u)δ2)−1
≥exp

−16(1 + 2β)2 Nϵ
n2T 2

·

1 −n2T 2
8Nϵβ2

since H(y, u + δ) ≤2(u + δ −y)2
1 −y
≤8(1 + 2β)2
ϵ
n2T 2 as can be checked,
and
2NH′′(y, u)δ2 = 2Nδ2 [u(1 −u)]−1
≥2Nδ2 [(u −δ) (1 −u + δ)]−1
≥2N

1 −φ
k + 1
n
−1 2ϵβ
nT

1 −k + 1
nT
2
= 8Nϵβ2
n2T 2 .
Also,
P

Y (n)
T
= 1
 Y (n)
T −1
n = 1 −
ϵ
n2T 2

=

1 −
ϵ
n2T 2
2N
.
Now (8.2.1) implies that if n2 = o(N) as n →∞then, for suﬃciently
large n,
log P(Y (n)
T
= 1 | |Y (n)
0
= 1 −ϵ)
≥(nT −1)

−16 (1 + 2β)2 Nϵ
n2T 2 + log

1 −n2T 2
8Nϵβ2

+2N log

1 −
ϵ
n2T 2

≥nT
2

−16(1 + 2β)2 Nϵ
n2T 2 −n2T 2
4Nϵβ2

−4Nϵ
n2T 2
= −8N
n

(1 + 2β)2 ϵ
T + o(1)

as n →∞. Hence
lim inf
n→∞
n
2N log P(Y (n)
T
= 1
 Y (n)
0
= 1 −ϵ) ≥−4(1 + 2β)2 ϵ
T .
©2001 CRC Press LLC

In particular, if T = η and ϵ = η2 where η is a small positive number,
then
lim inf
n→∞
n
2N log P(Y (n)
η
= 1
 Y (n)
0
= 1 −η2) ≥−4(1 + 2β)2η.
The inequality
P(Y (n)
T
= 1 | Y (n)
0
= p)
≥P(Y (n)
T −η > 1 −η2 | Y (n)
0
= p)P(Y (n)
T
= 1 | Y (n)
T −η = 1 −η2)
combined with Theorem 8.2.2 implies
lim inf
n→∞
n
2N log P(Y (n)
T
= 1 | Y (n)
0
= p)
≥−
2
T −η

cos−1 
p(1 −η2) +

(1 −p)η2
2
−4(1 + 2β)2η
and since this is true for all small η, we obtain
lim inf
n→∞
n
2N log P(Y (n)
T
= 1
 Y (n)
0
= p) ≥−2
T

cos−1 √p
2 .
(8.2.2)
We can now establish the following theorem, which was stated in Pa-
pangelou (2000) without proof. We drop the assumption that Y (n)
t
is
one-dimensional and return to the set-up of the introduction where Y (n)
t
is d-dimensional.
THEOREM 8.2.2
Let Y (n)
t
, 0 ≤t ≤T; n = 1, 2, . . . be the sequence of d-dimensional pro-
cesses deﬁned in the introduction, with no mutation or selection eﬀects.
Set
m = {q = (q1, . . . , qd) ∈ : q1 = q2 = . . . = qm = 0} where m ≤d,
and suppose that G is a subset of 
m which is open in the relative topol-
ogy of 
m. If n2 = o(Nn) as n →∞, then for p (= Y (n)
0
) in the interior
of ,
lim inf
n→∞
n
2Nn
log P(Y (n)
T
∈G) ≥−inf
q∈G Jp,T (q).
We sketch the argument. Brieﬂy, enlarge G to an open subset G∗of
 (in the relative topology) by including points (q1, . . . , qd) such that
q1, . . . , qm are small rather than zero and (0, . . . , 0, qm+1, . . . , qd) ∈G.
The large deviation lower bound (8.1.2) implies that
P(Y (n)
T −ϵ ∈G∗) ≥exp


−2N
n ·
2
T −ϵ inf
q∈G∗

cos−1
d+1

k=1
√pkqk

+ β
2


©2001 CRC Press LLC

eventually (β > 0).
Once Y (n)
T −ϵ is in G∗, (8.2.2) applied to the one-
dimensional process 1 −Z(n)
t
where Z(n)
t
is the sum of the ﬁrst m co-
ordinates of Y (n)
t
, implies that the conditional probability of the event

Y (n)
T
∈G

given Y (n)
T −ϵ ∈G∗is greater than or equal to
exp

−2N
n · 2
ϵ

cos−1 
1 −η
2
,
where η is small. The proof can be completed by letting β →0, η →0,
ϵ →0 in that order in the resulting lower bound of
lim inf
n→∞
n
2N log P(Y (n)
T
∈G).
The theorem naturally holds for the other components of the boundary
of  as well.
References
1. Papangelou, F. (1998a). Tracing the path of a Wright-Fisher pro-
cess with one-way mutation in the case of a large deviation. In
Stochastic Processes and Related Topics – A Volume in Memory
of Stamatis Cambanis (Eds., I. Karatzas, B. Rajput, and M. S.
Taqqu), pp. 315–330. Birkh¨auser, Boston.
2. Papangelou, F. (1998b). Elliptic and other functions in the large
deviations behavior of the Wright-Fisher process. Annals of Ap-
plied Probability 8, 182–192.
3. Papangelou, F. (2000). The large deviations of a multi-allele Wright-
Fisher process mapped on the sphere. Annals of Applied Probabil-
ity, to appear.
4. Stroock, D. W. (1993).
Probability Theory, An Analytic View.
Cambridge University Press, Cambridge.
©2001 CRC Press LLC

9
Stochastic Integral Functionals in an
Asymptotic Split State Space
V. S. Korolyuk and N. Limnios
Ukrainian Academy of Science, Kiev, Ukraine
Universit´e de Technologie de Compi`egne, Compi`egne, France
ABSTRACT Let xε(t), t ≥0, ε > 0 be a family of Markov jump
processes on a measurable state space (X, X), and consider a ﬁnite
split of its state space.
Under some additional conditions we obtain
by a singular perturbation technique, averaging and diﬀusion approx-
imation results in single and double merging scheme for the integral
functional ζε(t) :=
 t
0 a(xε(s))ds, where a is an X-measurable function.
We present here recent results given essentially in Korolyuk and Limnios
(1998, 1999a,b), under a uniﬁed approach.
Keywords and phrases Markov process, Integral functional, averag-
ing principle, diﬀusion approximation, merging state space, double merg-
ing
9.1
Introduction
Asymptotic behaviour of stochastic systems in series scheme and stability
constitutes one of the major ﬁelds of investigation as well in theoretical
as in applied ﬁelds of research [see, for example, Anisimov (1995) and
Liptser (1984)]. In this paper we investigate the asymptotic behaviour
of integral functionals of Markov jump processes with asymptotic state
space merging.
In the study of complex systems we have very often to consider be-
haviour of diﬀerent scales of time, one slow and one fast and in some
cases even of more than two levels. On the other hand, the local char-
acteristics of the systems (transition rates, probabilities, etc.) can be
dependent on the current values of some other stochastic processes (nat-
ural environment, discrete event systems, etc.).
The study of complex systems with high dimensionality is not possible
©2001 CRC Press LLC

in most of the cases. Thus we need a simpliﬁcation of the system. Many
techniques were developed to face this problem.
The most useful is
the diﬀusion approximation and the asymptotic state space merging or
consolidation. If the state space can be split in several regions and if
the transition probabilities between them are small in some sense and
if, in each region, states communicate asymptotically, then we can apply
asymptotic consolidation, i.e., we simplify the state space of system by
assuming that each region can be represented asymptotically by a single
state.
In fact, in the merging scheme, we have three systems: the initial
system, the supporting one and the merged one. The supporting (or
intermediate) one is supposed to be ergodic in each region. In the merged
system, each region is reduced to a single state, thus the number of the
states is equal to the number of regions of the initial system. It is obvious
that the study of the merged system is much more simple than the initial
one. The initial system is considered as a perturbation of the supporting
system. More precisely, we consider a sequence of systems including the
initial system, converging to the supporting system. This constitutes the
series scheme.
The underlying mathematical tools for results obtained here are based
on the theory of singular perturbed reducible invertible operators [Ko-
rolyuk and Korolyuk (1999) and Korolyuk and Limnios (1999b)] and on
the martingale characterization of Markov processes [Ethier and Kurtz
(1986), Korolyuk and Korolyuk (1999) and Liptser and Shiryaev (1989)].
In Section 9.2, we give a more precise deﬁnition of the merging scheme
of a Markov system and of the integral functional that will be studied. In
Section 9.3, we give results for single and double merging of the Markov
processes. In Section 9.4, we give results for the average convergence of
the sequence of integral functionals. In Section 9.5, we give a diﬀusion
approximation of it in single and double merging scheme. Finally, in
Section 9.6, we give diﬀusion approximation results for a sequence of
integral functionals with a perturbed kernel.
9.2
Preliminaries
Let (X, X) be a measurable space with a countably generated σ-algebra
X. Consider now an ergodic irreducible Markov chain xn, n ≥0, with
state space (X, X) and transition probability function P(x, B), x ∈
X, B ∈X, and stationary distribution ρ(B), B ∈X, i.e., ρ(B) =

X ρ(dx)P(x, B) and ρ(X) = 1.
Let B be a Banach space of measurable real-valued bounded func-
©2001 CRC Press LLC

tions deﬁned on X, with the sup-norm ∥f∥:= supx∈X |f(x)|. Let us
denote by P too the operator of transition probabilities on B deﬁned
by : Pf(x) =

X P(x, dy)f(y). Let us denote by P n(x, B) the n-step
transition probability and P n the corresponding operator.
The Markov chain xn is called uniformly ergodic if sup∥f∥≤1 ∥(P n −Π)f∥
converges to zero as n →∞, where Π is the stationary projector in B
deﬁned from the stationary distribution ρ(B) of the Markov chain xn,
as follows [Korolyuk and Korolyuk (1999)]
Πf(x) :=

X
ρ(dy)f(y)1(x),
(9.2.1)
where 1(x) = 1 for all x ∈X. Note that uniform ergodicity implies
Harris recurrence.
For a uniformly ergodic Markov chain, the operator Q := P −I (where
I is the identity operator), is reducible invertible, i.e., B = NQ ⊕RQ,
where NQ = {ϕ ∈B : Qϕ = 0} is the null-space of the operator Q, with
dimNQ ≥1, and RQ = {ψ ∈B : Qf = ψ} is the space of values of the
operator Q.
Let xε(t), t ≥0, ε > 0, be a family of Markov jump processes on a
measurable state space (X, X) deﬁned by the generators
Qεϕ(x) = q(x)

X
P ε(x, dy)[ϕ(y) −ϕ(x)].
(9.2.2)
The integral functional in a merging and averaging scheme is consid-
ered in the following form:
ζε(t) =
 t
0
a(xε(s))ds
(9.2.3)
where a is an X-measurable function, such that
 t
0
|a(xε(s)| ds < ∞,
(a.s.), t > 0, ε > 0.
(9.2.4)
Concerning the above integral functional (9.2.3), we consider several
averaging and diﬀusion approximation problems in combination with
single and double asymptotic splitting with a singular perturbation ap-
proach.
Firstly, we consider the ﬁnite single splitting
X =
N

k=1
Xk,
Xk

Xk′ = φ,
k ̸= k′.
(9.2.5)
©2001 CRC Press LLC

The stochastic kernel P ε(x, dy) is then represented by
P ε(x, dy) = P(x, dy) + εP1(x, dy),
(9.2.6)
where the stochastic kernel P(x, dy) deﬁnes the support imbedded Markov
chain xn, n ≥0, uniformly ergodic in every class Xk, 1 ≤k ≤N, with
the stationary distributions ρk(dx), 1 ≤k ≤N. This kernel is related
to the state space splitting (9.2.5), namely,
P(x, Xk) = 1k(x) =
 1,
x ∈Xk
0,
x ̸∈Xk
P1(x, dy) is a perturbing kernel.
Thus the support Markov process
x(t), t ≥0, deﬁned by the generator
Qϕ(x) = q(x)

X
P(x, dy)[ϕ(y) −ϕ(x)],
is uniformly ergodic too with the stationary distributions πk(dx), 1 ≤
k ≤N, represented as follows
πk(dx)q(x) = qkρk(dx),
qk =

Xk
πk(dx)q(x).
Secondly, we consider the following double ﬁnite splitting:
X =
N

k=1
Xk,
Xk =
Nk

r=1
Xr
k,
1 ≤k ≤N,
Xr
k

Xr′
k′ = ∅,
k ̸= k′ or r ̸= r′.
(9.2.7)
The stochastic kernel P ε(x, dy), in this second case, has the following
representation
P ε(x, dy) = P(x, dy) + εP1(x, dy) + ε2P2(x, dy).
(9.2.8)
Here the stochastic kernel P(x, dy) deﬁnes the support Markov chain
xn, n ≥0, which is uniformly ergodic in every class Xr
k, 1 ≤r ≤
Nk, 1 ≤k ≤N, and P1(x, dy) and P2(x, dy) are perturbing kernels. The
former concerns transitions between classes Xr
k and the latter between
classes Xk.
Of course, these kernels are not stochastic, in particular
P1(x, X) = P2(x, X) = 0, for every x ∈X. The supporting Markov
process x(t), t ≥0, is uniformly ergodic too with generator Q.
©2001 CRC Press LLC

9.3
Phase Merging Scheme for Markov Jump
Processes
In the ergodic single split state space, deﬁne the merging function v as
follows
v(x) = v,
if x ∈Xk
Here we suppose that the perturbing kernel veriﬁes the merging condition
[Korolyuk and Limnios (1999a)], i.e.,
v(xε(t/ϵ)) =⇒ˆx(t),
as ε →0,
(9.3.1)
where ˆx(t), t ≥0, is the limit Markov process with state space ˆX =
{1, ..., N} and generator the contracted operator ˆQ1, given by the fol-
lowing relation
ΠQ1Π = ˆQ1Π,
Q1ϕ(x) = q(x)

X
P1(x, dy)ϕ(y).
(9.3.2)
The projector Π into the null space NQ of the operator Q is deﬁned by
Πϕ(x) =
N

k=1
ˆϕk1k(x),
ˆϕk :=

Xk
πk(dx)ϕ(x).
In the double split state space, let us assume that the perturbing
kernels P1 and P2 satisfy the merging condition, that is, the following
weak convergences take place:
ˆv(xε(t/ε)) =⇒ˆx(t)
(9.3.3)
ˆˆv(xε(t/ε2)) =⇒ˆˆx(t)
(9.3.4)
where ˆv and ˆˆv are the merging functions deﬁned as follows
ˆv(x) = ˆv,
x ∈Xr
k
and
ˆˆv(x) = ˆˆv,
x ∈Xk.
The limit process ˆx(t) has the state space ˆX = ∪N
k=1 ˆXk, ˆXk = {vr
k : 1 ≤r
≤Nk}, and ˆˆx(t) has the state space ˆˆX = {1, 2, ..., N}. The generators
of processes ˆx(t) and ˆˆx(t) are respectively ˆQ1 and ˆˆQ2, which are deﬁned
below.
The stationary distributions of the supporting uniformly ergodic
Markov process x(t), t ≥0, of the merged Markov process ˆx(t), t ≥0,
and of the twice merged Markov process ˆˆx(t), t ≥0, are (πr
k(dx), 1 ≤
r ≤Nk, 1 ≤k ≤N), (ˆπr
k, 1 ≤r ≤Nk, 1 ≤k ≤N) and (ˆˆπk, 1 ≤k ≤
N) respectively.
©2001 CRC Press LLC

The contracted operator ˆQ1 is given by (9.3.2). The double contracted
operator ˆˆQ2 is deﬁned as follows:
ΠQ2Π = ˆQ2Π,
Q2ϕ(x) = q(x)

X
P2(x, dy)ϕ(y)
ˆΠ ˆQ2 ˆΠ = ˆˆQ2 ˆΠ.
The projectors Π and ˆΠ are deﬁned as follows:
Πϕ(x) =
N

k=1
Nk

r=1

Xr
k
πr
k(dy)ϕ(y)1r
k(x),
1 ≤r ≤Nk, 1 ≤k ≤N
ˆΠϕ(x) =
N

k=1
Nk

r=1
ˆπr
kϕr
k1r
k(x)
with
1r
k(x) =
 1,
x ∈Xr
k
0,
x ̸∈Xr
k
,
1k(x) =
 1,
x ∈Xk
0,
x ̸∈Xk
9.4
Average of Stochastic Integral Functional
Consider now the integral functional ζε(t), t ≥0, given by (9.2.3), in
double merging and averaging scheme.
Let us deﬁne
ˆa(ˆv(x)) =
N

k=1
Nk

r=1
ˆar
k1r
k(x),
ˆˆa(ˆˆv(x)) =
N

k=1
ˆˆa(vk)1k(x),
and
ˆˆˆa =
N

k=1
ˆˆakˆˆπk
where
ˆar
k =

Xr
k
πr
k(dx)a(x),
1 ≤r ≤Nk, 1 ≤N ≤N,
ˆˆak =
Nk

r=1
ˆπr
kˆar
k,
1 ≤k ≤N.
The following average limit results take place.
©2001 CRC Press LLC

THEOREM 9.4.1
Under the above conditions, the following weak convergence holds, as
ε →0, in the sense of the Skorokhod topology on the space DIR[0, ∞)1:
1.
ε3ζε(t/ε3) =⇒tˆˆˆa,
2.
ε2ζε(t/ε2) =⇒ˆˆζ(t) :=
 t
0
ˆˆa(ˆˆx(s))ds.
3.
εζε(t/ε) =⇒ˆζk(t) :=
 t
0
ˆak(ˆx(s))ds,
1 ≤k ≤N,
with respect to IPx, x ∈Xk.
THEOREM 9.4.2
Under the above merging conditions, the following weak convergence
holds, as ε →0, in the sense of the Skorokhod topology on the space
DIR[0, ∞):
εˆˆζ(t/ε) =⇒tˆˆˆa
REMARK The following weak convergence, with respect to IPx, x ∈Xk,
εˆζ(t/ε) =⇒tˆˆak,
1 ≤k ≤N
takes place.
9.5
Diﬀusion Approximation of Stochastic Integral
Functional
9.5.1
Single Splitting State Space
Let us deﬁne the potential matrix,
ˆR0 = [rkl; 0 ≤k, l ≤N],
by the following relations [Korolyuk and Korolyuk (1999)]:
ˆQ ˆR0 = ˆR0 ˆQ = ˆΠ −I = [πlk = πk −δlk; 0 ≤l, k ≤N].
1Continuity of sample paths of the limit process provides the weak convergence in
the space CIR[0, ∞).
©2001 CRC Press LLC

The centering shift-coeﬃcient ˆa is deﬁned by the relation
ˆa =
N

k=0
πkak,
ak :=

Xk
πk(dx)a(x), 0 ≤k ≤N;
or, in an equivalent form, by
ˆa = q
N

k=0
ρkak,
ak :=

Xk
ρk(dx)a(x), 0 ≤k ≤N,
where
q =
N

k=0
πkqk,
and
ρk = πkqk/q,
0 ≤k ≤N,
is the stationary distribution of the imbedded Markov chain ˆxn, n ≥0.
The vector ˜a is deﬁned as ˜a = (˜ak := ak −ˆa, 0 ≤k ≤N).
THEOREM 9.5.1
Let the merging condition (9.3.1) be valid and the limit merged Markov
process ˆx(t), t ≥0, be ergodic with the stationary distribution π =
(πk, 1 ≤k ≤N). Then the normalized centered integral functional
ξε(t) = ε2ζε(t/ε3) −ε−1tˆa
converges weakly, as ε →0, to the diﬀusion process ξ(t), t ≥0, with
zero mean and variance
σ2 = 2
N

k,l=1
πk˜akrkl˜al = 2
N

i=1
πiqib2
i −
N

i̸=j≥1
bibj[πiqij + πjqji].(9.5.1)
REMARK The second equality in (9.5.1) is Liptser’s formula [Liptser
(1984) and Korolyuk and Limnios (1998)].
The generator of the coupled process ξε(t), xε(t/ε3), t ≥0, has the
following representation
Lε = ε−3Q + ε−2Q1 + ε−1A(x)
with
Q1ϕ(x) = q(x)

X
P1(x, dy)ϕ(y),
©2001 CRC Press LLC

and
A(x)ϕ(u) = [a(x) −ˆa]ϕ′(u).
The proof of the above theorem is based on the following singular
perturbation result.
PROPOSITION 9.5.2
[Korolyuk (1998)]
Let us assume that:
1.
the operator Q is reducible invertible;
2.
the contracted (on NQ) operator ˆQ1 is reducible invertible with null-
space ˆN ˆ
Q1 ⊂ˆNQ;
3.
the twice contracted operator ˆˆQ2 which is deﬁned by the relations
ˆˆQ2 ˆΠ = ˆΠ ˆQ2 ˆΠ,
ˆQ2Π = ΠQ2Π
is a zero-operator, i.e., ˆˆQ2ϕ = 0, for all ϕ ∈NQ.
Then the asymptotic representation

ε−3Q + ε−2Q1 + ε−1Q2 + Q3
 
ϕ + εϕ1 + ε2ϕ2 + ε3ϕ3

= ψ + θε
(9.5.2)
can be realized by the following relations:
ˆˆQ0ϕ = ˆˆψ
(9.5.3)
ϕ1 = −ˆR0 ˆQ2ϕ
(9.5.4)
ϕ2 = ˆR0( ˆψ −ˆQ0ϕ)
(9.5.5)
ϕ3 = R0[ψ −Q1ϕ2 −Q2ϕ1 −Q3ϕ]
(9.5.6)
where ˆˆQ0 is the contraction of the operator ˆQ0 := ˆQ3 −ˆQ2 ˆR0 ˆQ2 on
the null-space of Q, and ˆR0 is the potential operator of the contracted
operator ˆQ1.
From this proposition, the asymptotic representation of the above gen-
erator Lε is realized in the following form :
Lεϕε = L0ϕ + θε,
(9.5.7)
where the limit operator L0, by Proposition 9.5.2, is the twice contracted
operator, deﬁned by the following relations:
ˆAΠ = ΠA(x)Π
(9.5.8)
©2001 CRC Press LLC

L0 ˆΠ = ˆΠ ˆ
L0 ˆΠ,
(9.5.9)
where
ˆ
L0 = ˆAˆR0 ˆA.
(9.5.10)
Hence
L0ϕ(u) = σ2
2 ϕ
′′(u),
(9.5.11)
is the operator of the limit diﬀusion process ξ(t), t ≥0.
Now, the proof of weak convergence is based on the martingale char-
acterization of the coupled Markov process ξε(t), xε(t/ε3), t ≥0:
M ε(t) = ϕε(ξε(t), xε(t/ε3)) −ϕε(u, x) −
 t
0
Lεϕε(ξε(s), xε(s/ε3))ds.
(9.5.12)
By relation (9.5.7), the above martingale representation is transformed
into the following form
M ε(t) = ϕ(ξε(t)) −ϕε(t) −
 t
0
L0ϕε(ξε(s))ds + ψε(t)
(9.5.13)
where the negligible term ψε(t) satisﬁes assumptions of the pattern limit
theorem [Korolyuk and Korolyuk (1999)].
9.5.2
Double Split State Space
Let ˆˆR0 = (ˆˆrkℓ, 1 ≤k, ℓ≤N) be the potential matrix of operator ˆˆQ2
deﬁned by relations:
ˆˆQ2 ˆˆR0 = ˆˆR0 ˆˆQ2 = ˆˆΠ −I = (πkℓ= πk −δkℓ, 1 ≤k, ℓ≤N).
Note that ˆR0 is the potential matrix of operator ˆQ1.
Let W(t), t ≥0, be the standard Wiener process. Then the following
result takes place.
THEOREM 9.5.3
If the merging condition (9.3.3) takes place and if the limit merged
Markov process ˆx(t), t ≥0, has a stationary distribution, ˆπ = (πr
k, 1 ≤
©2001 CRC Press LLC

r ≤Nk, 1 ≤k ≤N) say, then, under the balance condition ˆˆˆA = 0, the
following weak convergence holds,
αε(t) := ε−1
 t
0
a(xε(s/ε4))ds =⇒σW(t),
as ε →0
in the sense of the Skorokhod topology on the space DIR[0, ∞), with vari-
ance
σ2 = −2
N

k=1
N

ℓ=1
ˆˆakˆˆrkℓˆˆaℓ,
(9.5.14)
where ˆˆak = 	Nk
r=1 ˆπr
k

Xr
k πr
k(dx)a(x), 1 ≤k ≤N.
REMARK For positiveness of the variance σ2 deﬁned by the above for-
mula see Korolyuk and Limnios (1998, 1999a).
THEOREM 9.5.4
If the merging condition (9.3.4) takes place and if the limit merged
Markov process ˆˆx(t), t ≥0, has a stationary distribution, ˆˆπ = (ˆˆπk, 1 ≤
k ≤N) say, then, under the balance condition ˆˆˆA = 0, the following weak
convergence holds,
βε(t) := ε−1
 t
0
ˆˆa(ˆˆx(s/ε2))ds =⇒σW(t),
as ε →0
in the sense of the Skorokhod topology on the space DIR[0, ∞), with vari-
ance σ2 given by relation (9.5.14).
THEOREM 9.5.5
If the merging condition (9.3.3) takes place and the support Markov pro-
cess x(t), t ≥0, has a stationary distribution, π(dx) = (πr
k(dx), 1 ≤
r ≤Nk, 1 ≤k ≤N) say, then the following weak convergence holds,
ηε(t) := ε3ζε(t/ε4) −ε−1tˆˆˆa =⇒σW(t),
as ε →0
in the sense of the Skorokhod topology on the space DIR[0, ∞), with vari-
ance σ2 given by
σ2 = −2
N

k=1
N

ℓ=1
˜˜ak˜˜rkℓ˜˜aℓ,
where ˜˜ak = ˆˆak −ˆˆˆa, 1 ≤k ≤N.
©2001 CRC Press LLC

9.6
Integral Functional with Perturbed Kernel
An analogous limit result can be obtained for the integral functional
with a perturbed kernel aε(x) = a(x) + εa1(x). In this case, the gen-
erator of the coupled process ξε(t), xε(t/ε3), t ≥0, has the following
representation:
Lε = ε−3Q + ε−2Q1 + ε−1A(x) + A1(x),
where the operator A1 acts as follows: A1(x)ϕ(u) = a1(x)ϕ′(u).
Consequently, we have the following singular perturbation problem:
[ε−3Q + ε−2Q1 + ε−1A(x) + A1(x)](ϕ + εϕ1 + ε2ϕ2 + ε3ϕ3) = ψ + θε,
where θε is a negligible function, i.e., ∥θε∥→0 as ε →0; ψ is given by
limiting operator, i.e., L0ϕ = ψ. From Proposition 9.5.2, this operator
L0 is a twice contracted operator deﬁned by
L0 ˆΠ = ˆΠ ˆ
L0 ˆΠ.
Then
L0 ˆΠ = ˆ
A1 −ˆA ˆR0 ˆA,
where ˆΠ,
ˆA,
ˆR0 are deﬁned as in previous section and ˆA1(k)ϕ(u) =
a1kϕ′(u), 0 ≤k ≤N.
From this, we obtain the limit operator of the limit diﬀusion process.
L0ϕ(u) = ˆa1ϕ′(u) + σ2
2 ϕ′′(u),
where the shift coeﬃcient is twice averaged:
ˆa1 =
N

k=0
πka1k,
a1k :=

Xk
πk(dx)a1(x), 1 ≤k ≤N.
Thus we have the following result.
THEOREM 9.6.1
Let the merging condition (9.3.3) be valid and let the limit merged Markov
process ˆx(t), t ≥0, be ergodic with the stationary distribution π = (πk, 1 ≤
k ≤N). Then the normalized centered integral functional
ξε(t) = ε2ζε(t/ε3) −ε−1tˆa
©2001 CRC Press LLC

converges weakly, in the sense of the Skorokhod topology on the space
DIR[0, ∞), as ε →0, to a diﬀusion process ξ(t),
t ≥0, with drift ˆa1
and variance
σ2 := 2
N

k,l=0
πk˜akrkl˜al.
References
1. Anisimov, V. V. (1995). Switching processes: averaging principle,
diﬀusion approximation and applications. Acta Aplicandae Math-
ematica 40, 95–141.
2. Anisimov, V. V. (1999). Diﬀusion approximation for processes with
semi-Markov switches, in Semi-Markov Models and Applications
(Eds., J. Janssen and N. Limnios), pp. 77–101, Kluwer Academic
Publishers, Dordrecht, The Netherlands.
3. Billingsley, P. (1968). Convergence of Probability Measures. John
Wiley & Sons, New York.
4. Ethier, S. N. and Kurtz, T. G. (1986). Markov Processes: Charac-
terization and convergence, John Wiley & Sons, New York.
5. Jacod, J. and Shiryaev, A. N. (1987). Limit Theorems for Stochas-
tic Processes. Springer-Verlang, Berlin.
6. Korolyuk, V. S. and Korolyuk, V. V. (1999). Stochastic Models
of Systems. Kluwer Academic Publishers, Dordrecht, The Nether-
lands.
7. Korolyuk, V. S. (1998). Stability of stochastic systems in diﬀusion
approximation scheme. Ukrainian Mathematics Journal 50, N 1,
36–47.
8. Korolyuk, V. S. and Limnios, N. (1998). A singular perturbation
approach for Liptser’s functional limit theorem and some exten-
sions. Theory of Probability and Mathematical Statistics 58, 76–80.
9. Korolyuk, V. S. and Limnios, N. (1999). Diﬀusion approximation
of integral functionals in merging and averaging scheme. Theory
of Probability and Mathematical Statistics 59.
10. Korolyuk, V. S. and Limnios, N. (1999). Diﬀusion approximation
of integral functionals in double merging and averaging scheme.
Theory of Probability and Mathematical Statistics 60.
11. Liptser, R. Sh. (1984). On a functional limit theorem for ﬁnite
state space Markov processes, in Steklov Seminar on Statistics and
©2001 CRC Press LLC

Control of Stochastic Processes, pp. 305–316, Optimization Soft-
ware, Inc., NY.
12. Liptser, R. Sh. and Shiryaev, A. N. (1989). Theory of Martingales,
Kluwer Academic Publishers, Dordrecht, The Netherlands.
©2001 CRC Press LLC

10
Busy Periods for Some Queues with
Deterministic Interarrival or Service
Times
Claude Lef`evre and Philippe Picard
Universit´e Libre de Bruxelles, Bruxelles, Belgique
Universit´e de Lyon 1, Villeurbanne, France
ABSTRACT The queueing systems considered in this paper are the
Dg/M (Q)/1 queue with deterministic, not necessarily equidistant, in-
terarrival times, and with exponential service times where customers are
served in batches of random size, and its dual the M (Q)/Dg/1 queue with
Poisson arrival process where customers arrive in batches of random size,
and with deterministic, not necessarily equidistant, service times. Our
purpose is to determine the exact distribution of the statistic Nr that
represents the number of customers served during some busy period ini-
tiated by r customers. The problem is analyzed as the ﬁrst crossing of a
compound Poisson trajectory with a ﬁxed nonlinear boundary, upper or
lower respectively. For the Dg/M (Q)/1 queue, a simple explicit formula
is derived for the law of Nr that is expressed in terms of a generalization
of Appell polynomials. For the M (Q)/Dg/1 queue, the law of Nr is now
written using a generalization of Abel-Gontcharoﬀpolynomials.
Keywords and phrases Single server queue, busy period, determin-
istic interarrival or service times, bulk queue, ﬁrst crossing problem,
compound Poisson process, generalized Appell polynomials, generalized
Abel-Gontcharoﬀpolynomials
10.1
Introduction
In queueing theory, the analysis of busy periods is an important prob-
lem, from a theoretical point of view and for practical applications. The
literature on this subject, however, is relatively little abundant, espe-
cially in view of the large number of papers on the asymptotic behavior
©2001 CRC Press LLC

of various queueing systems. A number of results can be found in the
books by Tak´acs (1962), Bhat (1968) and Prabhu (1998), for instance.
Quite often studies rely on the use of Laplace transforms.
Results
in distributional terms are generally more convenient and meaningful
but for many models, their derivation is quite diﬃcult or even impossi-
ble. Nevertheless, there exist some well-known exceptions (see, e.g., the
references above).
Such exceptions, which have partly motivated our present research, are
the classical M/D/1 queue (Poisson arrival process with rate λ, service
times equal to a constant a) and its dual D/M/1 (interarrival times equal
to a constant a, exponential service times with rate λ). Our attention
will be mainly focused on the statistic Nr that represents the number of
customers served during the busy period initiated by r (≥1) customers;
obviously, Nr ≥r a.s. For the M/D/1 queue, Borel (1942), and others
later, proved that the distribution of Nr is given by
P(Nr = n) = r
n e−λan (λan)n−r
(n −r)! ,
n ≥r.
(10.1.1)
For the D/M/1 queue, Stadje (1995) showed (inter alia) that for r = 1,
the law of N1 is provided by
P(N1 ≥n) = e−λa(n−1)
n−2

k=0
[λa(n −1)]k
k!
n −1 −k
n −1
,
n ≥1.
(10.1.2)
Now the formulae (10.1.1) and (10.1.2), quite simple and explicit, have
been extended by Stadje (1995) to the cases where the service times or
the interarrival times are deterministic but not necessarily equidistant.
For clarity, we denote these models by M/Dg/1 and Dg/M/1, respec-
tively.
The approach followed by Stadje consists in investigating the
question as some ﬁrst-passage problem. The formulae obtained, how-
ever, are rather intricated and of determinantal form.
In this paper, we are going to consider bulk queueing systems of both
kinds. Speciﬁcally, we will examine in Section 10.2 the queue, denoted by
Dg/M (Q)/1, in which customers are served in batches of random size Q,
and in Section 10.3 its dual, denoted by M (Q)/Dg/1, in which customers
arrive in batches of random size Q.
Our purpose is to derive for these two bulk queues a new and easily
tractable expression for the exact distribution of Nr (r ≥1), the number
of customers served during the busy period initiated by r customers. It
is worth underlining here that the busy period is not deﬁned in the same
way for both models. For the Dg/M (Q)/1 queue, the busy periods stops
when the queue becomes empty, that is when all the customers, initial
and subsequent, have been served or at least begin to be served. For
©2001 CRC Press LLC

the M (Q)/Dg/1 queue, the busy period stops when the queueing system
becomes empty, that is when the service of the last customer has been
completed.
With respect to the work by Stadje (1995), a ﬁrst diﬀerence is thus
that we will discuss the case of bulk queues. A second diﬀerence, also
important, is that the formulae obtained will be much simpler and with
an enlightened algebraic structure.
The methodology followed is that developed in Picard and Lef`evre
(1994, 1996, 1997) for the analysis of the ﬁrst crossing of some counting
processes in upper and lower nonlinear boundaries. For the Dg/M (Q)/1
queue, we start by observing that Nr corresponds to the level of ﬁrst
crossing of a compound Poisson trajectory with random initial level
through a given (increasing) upper nonlinear boundary. We then show
that the distribution of Nr can be expressed in terms of a generalization
of the classical Appell polynomials. For the M (Q)/Dg/1 queue, we see
that Nr corresponds, up to r, to the level of ﬁrst crossing of a com-
pound Poisson trajectory (starting at level 0) through a given (increas-
ing) lower nonlinear boundary. We then establish that the distribution
of Nr can be written in terms of a generalization of the less known Abel-
Gontcharoﬀpolynomials.
The two families of polynomials used have
remarkable mathematical structures which generate useful operational
properties and make easy their numerical evaluation.
10.2
Preliminaries: A Basic Class of Polynomials
We start by introducing a basic class of polynomials {en, n ≥0}, of
degrees n, that will play a central role in the analysis of both queueing
models. For further details, we refer the reader to Picard and Lef`evre
(1997).
10.2.1
Construction of the Basic Polynomials
Let us consider a compound Poisson process, with Poisson parameter λ
and jump sizes distributed as Q. For t ≥0, let N(t) be the number of
jumps by time t, and let S(t) =
N(t)

k=1
Qk be the total height of all the
jump sizes by time t. We have
©2001 CRC Press LLC


P[S(t) = 0] = e−λt,
P[S(t) = n] = e−λt n
k=1
(λt)k
k! P(Q1 + . . . + Qk = n),
n ≥1.
(10.2.1)
Now let us put, for t ≥0,

e0(t) = 1
en(t) = n
k=1
(λt)k
k! P(Q1 + . . . + Qk = n),
n ≥1,
(10.2.2)
yielding
P[S(t) = n] = e−λten(t),
n ≥0.
(10.2.3)
By construction, en(t), n ≥0, is a polynomial of degree n in t, t ≥0.
Note that en(0) = δn,0, n ≥0. Obviously, we may deﬁne the polynomials
en(t), n ≥0, by (10.2.2) for any t ∈IR, not necessarily positive.
This deﬁnition by (10.2.2), however, is not very convenient for the
sequel. It will be often preferable to work with the generating function
of the ens, which is given below.
DEFINITION 10.2.1
The polynomials en(t), n ≥0, are deﬁned by their formal generating
function
∞

n=0
en(t)sn = eλtq(s),
(10.2.4)
where q(s) =
∞

j=1
qjsj is the probability generating function of Q.
REMARK The ens can be determined explicitly for various speciﬁc dis-
tributions of Q. For example, in the logarithmic case where qj = ραj/j,
j ≥1, with 0 < α < 1 and ρ = −1/ ln(1 −α), then
eλtq(s) = (1 −αs)−λρt,
so that from (10.2.4),
en(t) =
 −λρt
n

(−α)n,
n ≥0.
Other illustrations are given in Picard and Lef`evre (1997).
A noteworthy particular situation is when the compound Poisson pro-
cess reduces to a Poisson process, that is when Q = 1 a.s. From (10.2.1),
we get the following simple expression for the ens.
©2001 CRC Press LLC

SPECIAL CASE 10.2.2
In the Poisson case,
en(t) = (λt)n/n!,
n ≥0.
(10.2.5)
10.2.2
A Generalized Appell Structure
With the class of polynomials {en, n ≥0}, we now associate the operator
∆deﬁned by
 ∆e0 = 0,
∆en = en−1,
n ≥1,
(10.2.6)
the powers of ∆being built recursively from ∆i+1 = ∆(∆i), i ≥0, with
∆0 as the identity operator.
In that way, {en, n ≥0} constitutes a
family of generalized Appell polynomials (or Sheﬀer polynomials) with
respect to the operator ∆(see, e.g., Sheﬀer (1937, 1939)).
The generalized Appell structure allows us to write a generalized Tay-
lor expansion for any polynomial.
PROPERTY 10.2.3
Any polynomial R(t) of degree k admits a generalized Taylor expansion
with respect to the operator ∆and the class {en, n ≥0}, namely for any
real b,
R(t) =
k

i=0
[∆iR(b)]ei(t −b).
(10.2.7)
PROOF We may write
R(t) =
k

i=0
αiei(t −b),
for some coeﬃcients αi. By (10.2.6), we then get, for 0 ≤j ≤k,
∆jR(b) =
k

i=0
αi∆jei(0) =
k

i=j
αiei−j(0)
=
k

i=j
αiδi,j = αj,
hence (10.2.7).
Let us go back to the particular situation where Q = 1 a.s. Inserting
(10.2.5) in (10.2.6) and (10.2.7) yields the following corollary.
©2001 CRC Press LLC

SPECIAL CASE 10.2.4
In the Poisson case, λ∆corresponds to the usual diﬀerentiation operator
D, and (10.2.7) reduces to a classical Taylor expansion.
The pair (en, n ≥0; ∆) enjoys various other nice properties. Some of
them are used in certain proofs that will not be developed hereafter (see
Special cases 10.3.4 and 10.4.3).
10.3
The Dg/M (Q)/1 Queue
10.3.1
Model and Notation
The arrival process is deterministic in the sense that the customers ar-
rive at ﬁxed points of time, not necessarily equidistant. Given r initial
customers, we denote by vr, vr+1, vr+2, . . . the arrival time of the 1st,
2nd, 3rd,. . . new customer; we put v0 = v1 = . . . = vr−1 = 0 for the r
initial customers.
The service process is a compound Poisson process. More precisely,
the customers are served according to a Poisson process with parameter
λ and in batches of random size Q (with qj ≡P(Q = j), j ≥1). If at
some epoch the queue length is less than the service capacity, the server
takes the available customers into service.
The system stops being busy at the instant Tr when the queue becomes
empty, that is when all the customers, initial and subsequent, have been
served or at least begin to be served (in other words, as soon as the server
realizes that the service capacity is not being fully utilized). The total
number of customers served during the time period [0, Tr] is the statistic
Nr under interest. Note that Nr includes also the number of customers
in the last batch service, which often will not be ﬁlled up. We mention
that in the literature, this last batch service is not always counted (see,
e.g., Prabhu (1998)).
By deﬁnition of the busy period and since Q ≥1, we have for r = 1
that N1 = 1. In what follows, we will thus assume that r ≥2.
In Figure 10.1 are drawn the deterministic curve F giving the cumu-
lative number of customers arrived in the queue, including the r initial
ones, and the trajectory τ giving the total number of services that are
susceptible to have begun.
Clearly, τ is the trajectory of the above
compound Poisson process S(t) but starting here at a random level dis-
tributed as Q, that is the trajectory of the process Q + S(t) say.
Now, we observe that Tr and Nr correspond to the ﬁrst crossing time
and level of the random trajectory τ with the ﬁxed boundary F. We
©2001 CRC Press LLC

FIGURE 10.1
Arrivals (F) and services (τ) trajectories in the Dg/M (Q)/1 queue
emphasize that F is an upper boundary for τ. Note that Tr is a contin-
uous random variable valued in IR+. Moreover, the distribution of Nr is
connected with the law of Tr through the identity
P(Nr = n) = P(vn−1 ≤Tr < vn),
n ≥r
(10.3.1)
In order to derive the law of Tr, we are going to determine the prob-
abilities
Pn(t)
= P[τ remains below F during (0, t), and its height at time t
is equal to n]
= P[Tr > t and Q + S(t) = n],
n ≥1, t ∈IR+.
(10.3.2)
10.3.2
Exact Distribution of Nr
When 1 ≤n ≤r −1, we have by (10.2.3) that
Pn(t)
= P[Q + S(t) = n]
= e−λt n
j=1 qjen−j(t)
≡e−λtfn−1(t),
say .
(10.3.3)
Note that fn−1(t) is a polynomial of degree n−1 in t, with, in particular,
Pn(0) = fn−1(0) = qn, as expected.
Furthermore, for all n ≥r, by
©2001 CRC Press LLC

considering the instant vn when F reaches the level n + 1, we get that
if t ≤vn,
Pn(t) = 0,
(10.3.4)
while
if t ≥vn,
Pn(t) =
n−1

k=0
Pn−k(vn)e−λ(t−vn)ek(t −vn).
(10.3.5)
Indeed, (10.3.4) is straightforward, and (10.3.5) means that at time vn,
[Tr > vn and Q+S(vn) = n−k] for some 0 ≤k ≤n−1, and afterward k
customers are served during (vn, t] (which occurs with probability given
by (10.2.3)).
Let us look for an expression of Pn(t) with the form
Pn(t) = e−λtHn−1(t)I(t ≥vn),
n ≥1.
(10.3.6)
To satisfy (10.3.3) and (10.3.4), we need
Hn−1(t) = fn−1(t),
1 ≤n ≤r −1,
(10.3.7)
Hn−1(vn) = 0,
n ≥r.
(10.3.8)
Concerning (10.3.5), when t ≥vn (≥vn−k), we should have
Hn−1(t) =
n−1

k=0
Hn−1−k(vk)ek(t −vn),
n ≥r.
(10.3.9)
Now, we get that the three relations (10.3.7), (10.3.8) and (10.3.9) can
be rewritten equivalently as
Hn−1(0) = qn,
1 ≤n ≤r −1,
(10.3.10)
Hn−1(vn) = 0,
n ≥r,
(10.3.11)
Hn−1(t) =
n−1

k=0
Hn−1−k(vn)ek(t −vn),
n ≥1.
(10.3.12)
Indeed, for 1 ≤n ≤r −1, we have vn = 0 so that (10.3.12), using
(10.3.10), reduces to (10.3.7). In particular, we then see that Hn−1(t),
n ≥1, is a polynomial of degree n−1. Note that by (10.3.9), the relation
(10.3.12) is restricted to t ≥vn, but Hn(t) being a polynomial, we may
deﬁne it for any real t.
It remains to identify the class of polynomials {Hn−1, n ≥1}. By
Property 10.2.3, we observe that (10.3.12) will be satisﬁed if one has
∆Hn−1(t) = Hn−2(t),
n ≥2,
(10.3.13)
©2001 CRC Press LLC

since (10.3.12) will then be simply the generalized Taylor expansion of
Hn(t) with respect to the operator ∆and the class {en, n ≥0}. As in-
dicated before, the condition (10.3.13) is the characteristic property of a
family of generalized Appell polynomials, { ¯An−1, n ≥1} say. Obviously,
(10.3.10) and (10.3.11) guarantee the unicity of the family.
Some useful complements on these polynomials ¯An−1 can be found in
Picard and Lef`evre (1996, 1997). We recall, inter alia, that they can be
expressed as
¯An−1(t) =
n−1

k=0
bken−1−k(t),
n ≥1,
(10.3.14)
for appropriate coeﬃcients bk that are independent of n. To sum up, we
have proved the following result (see (10.3.6), (10.3.10), (10.3.11) and
(10.3.14)).
PROPOSITION 10.3.1
Pn(t) = e−λt ¯An−1(t)I(t ≥vn),
n ≥1,
(10.3.15)
where the generalized Appell polynomials ¯An−1 are evaluated by (10.3.14),
the polynomials en being given by (10.2.4) and the coeﬃcients bk being
computed recursively from
n−1

k=0
bken−1−k(0) = qn,
1 ≤n ≤r −1,
(10.3.16)
n−1

k=0
bken−1−k(vn) = 0,
n ≥r.
(10.3.17)
This result, combined with (10.3.1), yields directly the distributions
of Tr and Nr.
PROPOSITION 10.3.2
P(Tr > t) = e−λt
∞

n=1
¯An−1(t)I(t ≥vn),
(10.3.18)
that is, when vj ≤t < vj+1, j ≥r −1,
P(Tr > t) = e−λt
j

n=1
¯An−1(t).
(10.3.19)
©2001 CRC Press LLC

Moreover,
P(Nr = n) = e−λvn−1
n−1

k=1
¯Ak−1(vn−1) −e−λvn
n−1

k=1
¯Ak−1(vn),
n ≥r.
(10.3.20)
Let us examine the particular situation when Q = 1 a.s. Using the
conclusions of Special case 10.2.4, we deduce from (10.3.13) that the
polynomials ¯An(t) are here given as follows
SPECIAL CASE 10.3.3
In the Poisson case,
¯An−1(t) = λn−1An−1(t),
n ≥1,
(10.3.21)
where {An−1, n ≥1} is a classical family of Appell polynomials.
Now, for illustration, let us assume that the interarrival times are
equal to a constant a, i.e. vr −vr−1 = vr+1 −vr = . . . = a. This is the
situation which is usually discussed in the literature. We underline that
the boundary F becomes here a straight line. A more explicit formula
for the ¯An−1’s is available; the proof is omitted for brevity reasons.
SPECIAL CASE 10.3.4
When the interarrival times are equal to a,
¯An−1(t) = fn−1(t), for 1 ≤n ≤r −1,
(10.3.22)
=
r−2

j=0
t −a(n −r + 1)
t −a(j −r + 2) fj[a(j −r + 2)]en−1−j[t −a(j −r + 2)],
for n ≥r.
(10.3.23)
In the Poisson case,
An−1(t) = tn−1/(n −1)!, for 1 ≤n ≤r −1,
(10.3.24)
= [t −a(n −r + 1)]
(n −1)!
r−2

j=0
n −1
j

[a(j −r + 2)]j
× [t −a(j −r + 2)]n−j−2, for n ≥r.
(10.3.25)
©2001 CRC Press LLC

REMARK In the Poisson case, (10.3.24) and (10.3.25) give, for example,
when r = 2,
An−1(t) = [t −a(n −1)]tn−2/(n −1)!,
n ≥1,
(10.3.26)
and when r = 3,
An−1(t) = [t −a(n −2)](t + a)n−2/(n −1)!,
n ≥1.
Now, in this case, τ is the trajectory of the compound Poisson process
S(t) starting at level 1, and which represents the total number of services
that can begin. On the other hand, the total number of services that
can be completed yields the trajectory ˜τ of the same compound Poisson
process, but starting now at level 0. Let ˜Tr and ˜Nr be the ﬁrst crossing
time and level of the trajectory ˜τ through the boundary F. In particular,
for r = 1, ˜N1 is the variable examined by Stadje (1995) and referred to
in (10.1.2) above. Now, we observe that by construction,
Tr = ˜Tr−1
and
Nr = 1 + ˜Nr−1.
(10.3.27)
For r = 2, we have shown that the distribution of N2 is provided explic-
itly from (10.3.20), (10.3.21) and (10.3.26). By (10.3.27), the distribution
of ˜N1 then follows. It is easily checked that it corresponds precisely to
the formula (10.1.2) obtained by Stadje (1995).
10.4
The M (Q)/Dg/1 Queue
10.4.1
Model and Notation
The arrival process is a compound Poisson process. More precisely. the
customers arrive according to a Poisson process with parameter λ and
in batches of random size Q (with qj ≡P(Q = j), j ≥1).
The service process is deterministic in the sense that the customers
are served at ﬁxed points of time, not necessarily equidistant. Given
r initial customers, we denote by −u0 the service time of the r initial
customers, and by u0 −u1, u1 −u2, u2 −u3 . . . the service times of the
1st, 2nd, 3rd . . . new customer; obviously, 0 < −u0 < −u1 < −u2 < . . .
This unusual notation will be justiﬁed later.
It is convenient to assume for a while that the arrival process starts
only at time −x, with −x ≤−u0. Ultimately, we will take x = 0.
The system stops being busy at the instant Tr when the queueing
system becomes empty, that is when the service of the last customer has
been completed. We are interested in the statistic Nr that counts the
number of customers served during [0, Tr].
©2001 CRC Press LLC

FIGURE 10.2
Arrivals (τ) and services (F) trajectories in the M (Q)/Dg/1 queue
In Figure 10.2 are drawn the trajectory τ giving the cumulative num-
ber of new arrivals, and the deterministic curve F giving the total num-
ber of services of new customers that are susceptible to be completed
(after the r initial ones). Clearly, τ is the trajectory of the above com-
pound Poisson process S(t), but starting now at level 0.
We observe here that Tr corresponds to the ﬁrst crossing time of the
random trajectory τ with the ﬁxed boundary F. The associated ﬁrst
crossing level, denoted by Lr, represents the number of new customers
served during [0, Tr]; thus, Nr = r + Lr, and Lr ≥0 a.s. We point out
that this time, F is a lower boundary for τ. Note that Tr is a discrete
random variable valued in {−ui, i ≥0}. Moreover, Lr is connected with
Tr through the relation
Tr = −uLr.
(10.4.1)
The law of Nr (or Lr) depends, of course, on the initial value x and
the family U ≡{ui, i ≥0}. To stress this dependence, we will use the
notation P(Lr = n|x, U), n ≥0, when necessary.
10.4.2
Exact Distribution of Nr
We have
P(Lr = 0|x, U) = P[S(−u0) = 0] = e−λ(−u0+x).
(10.4.2)
Furthermore, for all n ≥1, by considering the instant −u0 when the
service of the r initial customers has been completed, we get that for
−x = −u0,
P(Lr = n|u0, U) = 0,
(10.4.3)
©2001 CRC Press LLC

and for −x < −u0, by (10.2.3),
P(Lr = n|x, U) =
n

k=0
e−λ(−u0+x)ek(−u0 + x)P(Lr = n −k|u0, EkU),
(10.4.4)
where EkU, k ≥0, is the shifted family
EkU ≡{uk+i, i ≥0}.
(10.4.5)
Indeed, (10.4.3) is obvious, and (10.4.4) means that k customers arrived
during (−x, −u0], for some 0 ≤k ≤n, and the ﬁrst crossing will then
occur at level n −k, for the trajectory τ starting now at −u0 (instead of
−x) through the boundary F starting now at −uk (instead of −u0).
Let us look for an expression of P(Lr = n|x, U) with the form
P(Lr = n|x, U) = e−λ(−un+x)Kn(x|U),
n ≥0.
(10.4.6)
The relations (10.4.2), (10.4.3) and (10.4.4) require that for −x ≤−u0,
K0(x|U) = 1,
(10.4.7)
Kn(u0|U) = 0,
n ≥1,
(10.4.8)
Kn(x|U) =
n

k=0
Kn−k(u0|EkU)ek(x −u0),
n ≥1.
(10.4.9)
Clearly, these three conditions can be rewritten equivalently as
Kn(u0|U) = δn,0,
(10.4.10)
Kn(x|U) =
n

k=0
Kn−k(u0|EkU)ek(x −u0),
n ≥0.
(10.4.11)
In particular, we see that Kn(x|U), n ≥0, is a polynomial of degree n.
Note that (10.4.11) may be deﬁned for any real x, so that the restriction
−x ≤−u0 is superﬂuous.
Now, by Property 10.2.3, we observe that (10.4.11) will be satisﬁed if
∆Kn(x|U) = Kn−1(x|EU),
n ≥1,
(10.4.12)
since (10.4.11) will then be the generalized Taylor expansion of Kn(x|U)
with respect to the operator ∆and the class {en, n ≥0}. As developed
in Picard and Lef`evre (1996), the condition (10.4.12) is the character-
istic property of a family of generalized Abel-Gontcharoﬀpolynomials,
{ ¯Gn, n ≥0} say. The unicity of the family is guarateed by (10.4.10).
©2001 CRC Press LLC

We recall that these polynomials ¯Gn can be determined recursively by





¯G0(x|U) = 1,
¯Gn(x|U) = en(x) −
n−1

k=0
en−k(uk) ¯Gk(x|U),
n ≥1.
(10.4.13)
Other useful properties are given in Picard and Lef`evre (1996). To obtain
the law of Lr, it suﬃces to take x = 0 as announced, which yields the
following result (see (10.4.6) and (10.4.13)).
PROPOSITION 10.4.1
P(Nr = r + n) = eλun ¯Gn(0|U),
n ≥0,
(10.4.14)
where the generalized Abel-Gontcharoﬀpolynomials ¯Gn are evaluated by
(10.4.13), the polynomials en being given by (10.2.4).
In the particular situation when Q = 1 a.s., we obtain from (10.4.12)
that the polynomials ¯Gn(x|U) reduce, up to λn, to the more standard
Abel-Gontcharoﬀpolynomials (see, e.g., Lef`evre and Picard (1990)).
SPECIAL CASE 10.4.2
In the Poisson case,
¯Gn(x|U) = λnGn(x|U),
n ≥0,
(10.4.15)
where {Gn(x|U), n ≥0} is a classical family of Abel-Gontcharoﬀpoly-
nomials.
Finally, for illustration, let us assume, as generally made so far, that
the service times are equal to a constant a, i.e.
−u0 = ra, −u1 =
(r + 1)a, . . . Here thus, the boundary F is a straight line. The ¯Gn’s can
then be determined quite explicitly; the proof is omitted.
SPECIAL CASE 10.4.3
¯Gn(x|U) = x −u0
x −un
en(x −un),
n ≥0.
(10.4.16)
In the Poisson case,
Gn(x|U) = (x −u0)(x −un)n−1/n!,
n ≥0,
(10.4.17)
that is, the Gn’s become the classical Abel polynomials.
REMARK For the Poisson case, inserting (10.4.15) and (10.4.17) in
(10.4.14) yields the formula (10.1.1) derived by Borel (1942).
©2001 CRC Press LLC

References
1. Bhat, U. N. (1968).
A Study of Queueing Systems M/G/1 and
GI/M/1. Lectures Notes, Springer-Verlag, NewYork.
2. Borel, E. (1942).
Sur l’emploi du th´eor`eme de Bernoulli pour
faciliter le calcul d’une inﬁnit´e de coeﬃcients.
Application au
probl`eme de l’attente `a un guichet. Comptes Rendus de l’Acad´emie
des Sciences, Paris 214, 452–456.
3. Kaz’min, Y. A. (1988). Appell polynomials. In Encyclopedia of
Mathematics 1, Reidel Kluwer, Dordrecht, 209–210.
4. Lef`evre, C. and Picard, P. (1990). A non standard family of poly-
nomials and the ﬁnal size distribution of Reed-Frost epidemic pro-
cesses. Advances in Probability 22, 25–48.
5. Picard, P. and Lef`evre, C. (1994). On the ﬁrst crossing of the sur-
plus process with a given upper barrier. Insurance: Mathematics
and Economics 14, 163–179.
6. Picard, P. and Lef`evre, C. (1996). First crossing of basic count-
ing processes with lower non-linear boundaries: a uniﬁed approach
through pseudopolynomials (I). Advances in Applied Probability
28, 853–876.
7. Picard, P. and Lef`evre, C. (1997). The probability of ruin in inﬁnite
time with discrete claim size distribution. Scandinavuan Actuarial
Journal 1, 58–69.
8. Prabhu, N. U. (1998). Stochastic Storage Processes: Queues, In-
surance Risk, Dams and Data Communication. Springer-Verlag,
New York (second edition).
9. Sheﬀer, I. M. (1937). Concerning Appell sets and associated linear
functional equations. Duke Mathematics Journal 3, 593–609.
10. Sheﬀer, I. M. (1939). Some properties of polynomial sets of type
zero. Duke Mathematics Journal 5, 590–622.
11. Stadje, W. (1995). The busy periods of some queueing systems.
Stochastic Processes and their Applications 55, 159–167.
12. Tak´acs, L. (1962) Introduction to the Theory of Queues. Oxford
University Press, New York.
©2001 CRC Press LLC

11
The Evolution of Population Structure of
the Perturb ed Non-Homogeneous
Semi-Markov System
P.-C. G. Vassiliou and H. Tsakiridou
A r i s t o t l e U n i v e r s i t y o f T h e s s a l o n i k i , T h e s s a l o n i k i , G reece
ABSTRACT In the present we study the evolutions of various pop-
ulation structures in a Perturbed Non-Homogeneous Semi-Markov Sys-
tem (P-NHSMS) with diﬀerent goals in mind.
Firstly we start with
the expected population structure with respect to the ﬁrst passage time
probabilities and we follow with the study of the expected population
structures with respect to the duration of a membership in a state, the
state occupancy of a membership and the counting transition probabili-
ties.
Keywords and phrases Semi-Markov,
population
models,
non-
homogeneous Markov system
11.1
Introduction
The concept of the Perturbed Non-Homogeneous Semi-Markov System
(P-NHSMS) was introduced and deﬁned for the ﬁrst time in Vassiliou
and Tsakiridou (1998). A general theory for semi-Markov process is in-
cluded in the book by Howard (1971). The theory of semi-Markov models
for the evolution of population structures derives its motives from the
use of semi-Markov models in manpower systems. In this respect a basic
reference useful also for practical purposes is the book by Bartholomew,
Forbes and McClean (1991). Elegant theory for semi-Markov models
in manpower systems is included in Bartholomew (1982).
Moreover
an interesting account of theoretical results and important applications
of semi-Markov models can be found in Bartholomew (1986), McClean
(1980, 1986, 1993), McClean, Montgomery, and Ugwuowo (1997). The
evolution of the theory of non-homogeneous Markov systems is described
©2001 CRC Press LLC

in Vassiliou (1997). The concept of a non-homogeneous semi-Markov
system (NHSMS) was introduced and deﬁned for the ﬁrst time in Vassil-
iou and Papadopoulou (1992). This provided also a general framework
for a number of semi-Markov chain models in manpower systems and
a great variety of applied probability models. The asymptotic behavior
of the NHSMS was found in analytic form by Papadopoulou and Vas-
siliou (1994) and Papadopoulou (1997) studied the concepts of counting
transitions and entrance probabilities.
The idea of a perturbation of
a stochastic matrix was introduced in Meyer (1975) and an interesting
general theory is included in Campell and Meyer (1979). The concept
of a perturbed non-homogeneous Markov system was introduced in Vas-
siliou and Symeonaki (1997, 1999). The non-homogeneous semi-Markov
system in a stochastic environment was studied in Vassiliou (1996). In
Section 11.2 of the present we introduce the concept of a perturbed
non-homogeneous semi-Markov system (P-NHSMS). In Section 11.3 we
introduce for a P-NHSMS the ﬁrst passage times probabilities and we
ﬁnd them in closed analytic form. Then we establish the expected pop-
ulation structure in relation with the ﬁrst passage time from a state. In
Section 11.4 we introduce for a P-NHSMS the concept of the duration of
a membership in a state. In what follows in a matrix form the probabil-
ities of a membership of the system which entered the system at time s
to remain in the same state up to time n having made ν transitions are
calculated in closed analytic form. This form is useful for many purposes
and as an example we could refer to the asymptotic behaviour of the
system. The section is closed with the study of the evolution of the ex-
pected population structure in the system with respect to the duration
of a membership in a state. In Section 11.5 we introduce the concept of
the state occupancy of a membership. In this respect the probabilities of
a membership of the system which entered in a state at time s that will
enter the same state on ν occasions in n-steps are calculated in closed
analytic form useful for many purposes. The section terminates with the
study of the evolution of the expected population structure in the system
with respect to the state occupancies of a membership. Finally in Section
11.6 we study counting transition probabilities for a P-NHSMS a concept
ﬁrst introduced for a NHSMS by Papadopoulou (1998) and calculated
with the use of geometric transformations. In the present we study the
counting transition probabilities with pure probabilistic methods and
we conclude with ﬁnding the evolution of the population structure with
respect to the counting transition probabilities. The study of the evo-
lutions of the various population structures in a P-NHSMS and ﬁnding
them in closed analytic form apart from providing the only predictive
tools and much deeper understanding of the population also provides the
tools for controlling the system locally and asymptotically in time and
©2001 CRC Press LLC

means for the study of the asymptotic behavior of  a P-NHSMS. 
11.2 
The Perturb ed Non-Homogeneous
Semi-Markov System
Consider a population (system) which is stratiﬁed into classes (states)
according to various characteristics. The members of the system could
be sections of human societies, animals, biological microorganisms, par-
ticles in a physical phenomenon, various types of machines etc.
As-
sume that the sequence {T(t)}∞
t=0 of all members of the system is known
or that it is a known realization of a known stochastic process.
Let
S = {1, 2, ..., k} be the set of states that are assumed to be exclusive so
that each member of the system may be in one and only one state at
any given time. We consider that initially there are T(0) members in
the system and a member entering the system holds a particular mem-
bership which moves within the states with the member.
When the
system is expanding (∆T(t) = T(t) −T(t −1) > 0) then new member-
ships are created in the system which behave like the initial ones. Let
N(t) = [N1(t), N2(t), ..., Nk(t)] where Ni(t) is the number of members of
the system in the i-th state at time t.
Now let fij(t) = Pr{a member of the system who entered state i at
time t to choose to move to state j at its next transition}. Let F(t) =
{fij(t)}i,j∈S and assume that F(t) is of the form
F(t) = F −Ef(t)
for t = 1, 2, ...
with F(t)1′ ≤1′ ,
F1′ ≤1′ and F(t) ≥0 , F ≥0
for t = 1, 2, ...
(11.2.1)
where 1′ = [1, 1, ..., 1]′.
Moreover assume that the matrix Ef(t) is chosen randomly from a
ﬁnite set Ef = {Ef(1), Ef(2), ..., Ef(ν)} with
Pr{Ef(t) = Ef(i)} = ci(t) > 0 for i = 1, 2, ..., ν. 
(11.2.2)
Let pi,k +1(t) = Pr{a member of the system who entered state i at
time t to choose to leave the system at its next transition}; then if we
deﬁne by p′
k +1(t) = [ p1,k +1(t), p2,k +1(t), ..., pk,k+1(t)]′ we have
p′
k +1(t) = [I −F + Ef(t) ]1′. 
(11.2.3)
©2001 CRC Press LLC

From (11.2.1),(11.2.2),(11.2.3) we get that
p′
k +1(t) = p′
k +1 −ε′
k +1(t) 
(11.2.4)
where
p′
k +1 = [I −F]1′ and ε′
k+1(t) = −Ef(t) 1′ 
(11.2.5)
and consequently ε′
k +1(t) is randomly chosen with probabilities
Pr{ε′
k +1(t) = ε′
k +1(i)} = ci(t) > 0
for t = 1, 2, ...
i = 1, 2, ..., ν.
(11.2.6)
Let p0i(t) = Pr{a new member who enters the system in state i as
a replacement of a member who entered its last state at time t} and
p0(t) =[p01(t), p02(t), ..., p0k(t)]. Assume that for every t the vector
p0(t) is of the form
p0(t) = p0 −ε0(t)with p0(t)1′ = 1′andp01′ = 1′foreveryt. 
(11.2.7)
Let Mn,m be the set of all n×m matrices with elements from R. Also let
S Mn be the set of n×n stochastic matrices. If p ∈S M1,n then the vector
ε ∈ M1,n is a perturbation vector for p if the vector ˜p = p −ε is a stochas-
tic vector. Assume that in (11.2.7) ε0(t) is a perturbation vector for p0
and is randomly chosen from a ﬁnite set E0 = {ε0(1), ε0(2), ..., ε0(µ)}
with Pr{ε0(t) = ε0(i) } = c0i(t) > 0 for every t and i=1, 2, ..., µ.
Assume that the stochastic evolutions of selecting Ef(t) and ε0(t) are
independent.
Now let pij(t) = Pr{a membership of the system which entered state
i at time t to choose to move in state j at its next transition} = fij(t) +
pi,k +1(t)p0j(t).
Let P(t) = { pij(t)}i,j∈S; then
P(t) = [F −Ef(t)]+[pk +1 −εk +1(t) ]′[p0 −ε0(t) ] = P −Ep(t) (11.2.8)
with
Ep(t) = Ef(t) + p′
k +1ε0(t) + ε′
k +1(t) p0 −ε′
k +1(t)ε0(t)
for t = 0, 1, 2, ...
and let
Pr{Ep(t) = Ep(i)} = cpi(t) > 0
for
t = 1, 2, ...,
i = 1, 2, ..., µν.
(11.2.9)
From Meyer (1975) we get that if Q ∈S Mn and fully regular, then the
matrix E ∈Mn is a perturbation matrix for Q if the matrix ˜Q = Q−E
is also fully regular. In (11.2.8) we assume that Ep(t) is a perturbation
matrix for P.
©2001 CRC Press LLC

Deﬁne by τij (i, j = 1, 2, ..., k) to be the time that a membership
“holds” in state i after j has been selected before the actual transition to
j takes place. The holding times τij are positive integer-valued random
variables with probability mass function hij(m, t) = Pr{τij = m / the
membership entered state i at time t and state j has been selected};
where hij(0, t) = 0 for every i, j, t. Now let H(m, t) = { hij(m, t)}i,j∈S
and we assume that it is of the form H(m, t) = H(m) −EH(m, t) where
EH(m, t) is a perturbation matrix for H(m, t) i.e. the following properties
hold:
H(m, t) ≥0
and
∞

m=0
H(m, t) = U
with
H(m) ≥0
and
∞

m=0
H(m) = U 
(11.2.10)
where U is a matrix of 1’s. Moreover we assume that EH(m, t) is ran-
domly selected from the set of perturbation matrices
EH = {EH(m, 1), EH(m, 2), ..., EH(m, l)}
with probability
Pr{EH(m, t) = EH(m, j)} = cHj(t) > 0,
j = 1, 2, ..., l. 
(11.2.11)
We assume that the stochastic evolutions of selecting Ep(t), ε0(t) and
EH(m, t) are independent.
For the new memberships created by expansion at time t let r0i(t) =
Pr{a new membership to enter state i given that it enters the system
at time t} and r0(t) = [r01(t), r02(t), ..., r0k(t)]. Assume that for every t
the vector r0(t) is of the form
r0(t) = r0 −εr(t)
with
r0(t)1′ = 1′
and
r01′ = 1′. 
(11.2.12)
The vector εr(t) is assumed to be a perturbation vector for r0 and is ran-
domly selected from a ﬁnite set Er = {εr(1), εr(2), ..., εr(θ)} with prob-
abilities Pr{εr(t) = εr(i)} = cri(t) > 0. We assume that the stochastic
evolution of selecting εr(t) is independent of the selection of Ep(t)ε0(t)
and
EH(m, t) which in fact seems quite realistic and is a well established
inherent assumption (see Figure 11.1).
A population whose evolution is adequately described by a model as
above is called a perturbed non-homogeneous semi-Markov system.
©2001 CRC Press LLC

FIGURE 11.1
Sto chastic evolutions at time t
11.3 
The Exp ected Population Structure with
Resp ect to the First Passage Time
Probabilities
In a P-NHSMS the sequences {P(t)}∞
t=0 and {H(m, t)}∞
t=0 deﬁne uniquely
the inherent perturbed semi-Markov process. First passage time proba-
bilities for semi-Markov process play the same important role that they
played for the basic Markov process [Howard (1971)]. The perspective
has obvious practical implications for P-NHSMS especially when the
members are human beings and the system is a manpower system. Thus,
let {F(ν, s, n)}∞
ν,s,n=0 be the sequence of matrices the (i, j)-element of
which is the probability
fij(ν, s, n)
= Pr{
©2001 CRC Press LLC
a membership of the system which entered state i at time s

to make v - 1 transitions in the time interval (s, s + n) and
enters at time s + n for the first time in state j}
It is necessary to ﬁnd {F(ν, s, n)}∞
ν,s,n=0 as a function of the basic
parameters of the system which for the inherent perturbed semi-Markov
process are the sequences {P(t)}∞
t=0, {H(m, t)}∞
t=0. When a member-
ship enters a state at anytime, let us say t for example, then as described
in the previous section four independent stochastic evolutions are taken
place for the selection of P(t) = P −Ep(t), H(m, t) = H(m)−EH(m, t)
and r0(t) = r0 −εr(t) respectively. This fact constitutes the elements of
{P(t)}∞
t=0, {H(m, t)}∞
t=0 and {F(ν, s, n)}∞
ν,s,n=0 random variables and
thus for many reasons it is natural to seek for {E[F(ν, s, n)]}∞
ν,s,n=0 as a
function of {E[P(t)]}∞
t=0 and {E[H(m, t)]}∞
t=0. It could be proved that
E[P(t)] = P −ˆEp(t) 
(11.3.1)
where
ˆEp(t) = E[Ep(t)] =
ν

h=1
Ep(h)ch(t) + {
ν

h=1
ε′
k +1(h)ch(t)}p0
+ p′
k +1
µ

b=1
ε0(b)c0b(t) −
ν

h=1
µ

b=1
ε′
k +1(h)ε0(b)ch(t)c0b(t)
(11.3.2)
E[H(m, t)] = H(m) −
l

α=1
EH(m, α)cHα(t) = H(m) −ˆEH(t) 
(11.3.3)
E[r0(t)] = r0 −
θ

h=1
εr(h)crh(t) = r0 −ˆEr(t). 
(11.3.4)
Let A = {aij} and B = {bij} ∈Mn,m then the Hadamard product of
these matrices is the matrix A ⋄B = {aijbij ∈Mn,m.
We will now ﬁrstly ﬁnd E[F(1, s, n)]. It can be proved that
E[F(1, s, n)] = E[P(s)⋄H(n, s)] = [P−ˆEp(s)]⋄[H(n)−ˆEH(s)] (11.3.5)
where P(s) ⋄H(n, s) is the Hadamard product of the two matrices.
Now let us ﬁnd E[F(2, s, n)]. Consider the probabilities fij(2, s, n) for
i, j ∈S then (see Figure 11.2) taking all the mutually exclusive events
where the membership will move at time s+m1 to a state x where x ̸= j
and then from x to state j at time s + n we can prove that
fij(2, s, n) =
n−1

m1 =1

x̸=j
pix(s)hix(m1, s)pxj(s + m1)hxj(n −m1, s + m1).
(11.3.6)
©2001 CRC Press LLC

FIGURE 11.2
Entrance for the ﬁrst time at state j at time s + n, after one
transition
Taking expectations in both sides due to the independency of the
stochastic evolutions we get
E[fij(2, s, n)]
=
n−1

m1 =1

x̸=j
E[pix(s)]E[hix(m1, s)]E[pxj(s + m1)]E[hxj(n −m1, s + m1)].
(11.3.7)
Deﬁne the matrix ˆH to be a k × k matrix of the form
ˆH =




0 1 ... 1
1 0 ... 1
... ... ... ...
1 1 ... 0



.
Then from (11.3.1) and (11.3.3) equation (11.3.7) it can be proved
that takes the form
E[F(2, s, n)] =
n−1

m1 =1
{[P −ˆEp(s)] ⋄[H(m1) −ˆEH(s)]}
×{([P −ˆEp(s + m1)] ⋄[H(n −m1) −ˆEH(s + m1)]) ⋄ˆH}.
(11.3.8)
We will now ﬁnd E[F(3, s, n)] as a function of the basic parameters
of the system. Following the steps with which we arrived in equation
(11.3.8) we arrive at the following (see also Figure 11.3)
©2001 CRC Press LLC

FIGURE 11.3
Entrance for the ﬁrst time in state j at time s + n, after two
transitions
F(3, s, n) =
n−2

m1 =1
{P(s) ⋄H(m1, s)}{F(2, s + m1, n) ⋄ˆH}. 
(11.3.9)
From the equation (11.3.8) we can prove that
E[F(3, s, n)]
=
n−2

m1 =1
n−1

m2 =1+ m1
[(P −ˆEp(s)) ⋄(H(m1) −ˆEH(s))]
×{[(P −ˆEp(s + m1)) ⋄(H(m2 −m1) −ˆEH(s + m1))]
×{[(P −ˆEp(s + m2)) ⋄(H(n −m2) −ˆEH(s + m2))] ⋄ˆH} ⋄ˆH}.
(11.3.10)
Consider the probability which is the (i, j) element of the matrix
[P(s) ⋄H(m1 −m0, s)]{[P(s + m1) ⋄H(m2 −m1, s + m1)]{[P(s + m2)
⋄H(m3 −m2, s + m2)]{...{[P(s + mν−2)
⋄H(mν−1 −mν−2, s + mν−2)]
×{[P(s + mν−1) ⋄H(mν −mν−1, s + mν−1)] ⋄ˆH} ⋄ˆH}... ⋄ˆH} ⋄ˆH}
(11.3.11)
with m0 = 0 and mν = n.
Then this is (see Figure 11.4) Pr{a membership of the system which
entered state i at time s makes ν −1 transitions in the time interval
©2001 CRC Press LLC

FIGURE 11.4
Entrance for the ﬁrst time in state j at time s + n, after ν − 1
transitions
(s, s + n) at exactly the times s + m1, s + m2, ..., s + mν−1 and enters
state j for the ﬁrst time at time s + n}.
We can write the above product as
ν−2

r =0
{ ˆH}[P(s + mr) ⋄H(mr+1 −mr, s + mr)]
×{[P(s + mr +1) ⋄H(mr +2 −mr +1, s + mr +1)] ⋄ˆH}
(11.3.12)
with m0 = 0 and mν = n.
Now if we sum the above product for all possible values of m1, m2, ...,
mν−1 the holding times we get
F(ν, s, n)
=
n−ν +1

m1 =1
{
n−ν +2

m2 =1+ m1
{...{
n−1

mν −1 =1+ mν −2
ν−2

r =0
{ ˆH}[P(s + mr)
⋄H(mr +1 −mr, s + mr)]
×{[P(s + mr +1) ⋄H(mr +2 −mr +1, s + mr +1)] ⋄ˆH}}...}}
for ν = 4, 5, ..., n.
(11.3.13)
Using the fact that E[E[X/Y, Z]] = E[X] and the independence of
the various stochastic evolutions we get that the expected values of the
above probabilities are
©2001 CRC Press LLC

E[F(ν, s, n)]
=
n−ν +1

m1 =1
{
n−ν +2

m2 =1+ m1
{...{
n−1

mν −1 =1+ mν −2
ν−2

r =0
{ ˆH}
× [(P −ˆEp(s + mr)) ⋄(H(mr +1 −mr) −ˆEH(s + mr))]
× {[(P −ˆEp(s + mr +1)) ⋄(H(mr +2 −mr +1) −ˆEH(s + mr +1))]
⋄ˆH}}...}} . 
(11.3.14)
Now let us deﬁne by
N (f )
i
(ν, t)
= Pr{the number of memberships which enter state i for the first
time at time t after v transitions in the system}.
Let N(f )(ν, t) = [N(f )
1 (ν, t), N(f )
2 (ν, t), ..., N(f )
k (ν, t)] then a classical
problem
in
population
system
theory
[Bartholomew
(1982)
and
Bartholomew, Forbes and McClean (1991)] is to ﬁnd E[N(f )(ν, t)] as
a function of the basic parameters of the P-NHSMS i.e. {P−Ep(t)}∞
t=0,
{H(m) −EH(m, t)}∞
t=0, {r0 −εr(t)}∞
t=0 and {T(t)}∞
t=0.
We consider the population divided into two classes. In the ﬁrst belong
the initial memberships that survived and they enter for the ﬁrst time
in state j at time t. In the second we have all the new memberships that
are created due to expansion of the population after t = 0 and which
enter at time t for the ﬁrst time in state j. With careful probabilistic
analysis we can prove that
E[N (f )
j
(ν, t)] =
k

i=1
E[fij(ν, 0, t)]Ni(0)
+
t

m=1
k

i=1
∆T(m)E[r0i(m)]E[fij(ν, m, t −m)]
(11.3.15)
for
t = 1, 2, ...,
ν = 0, 1, 2, ..
and
fij(0, m, 0) = 1 for every i, j and
m = 0, 1, 2, ...
The above equation in matrix notation takes the form
©2001 CRC Press LLC

E[N(f )(ν, t)] = N(0)E[F(ν, 0, t)]
+
t

m=1
∆T(m)[r0 −ˆEr(t)]E[F(ν, m, t −m)].
(11.3.16)
Where the expected value of the matrices E[F(ν, 0, t)], E[F(ν, m, t −
m)] are given in (11.3.14). The relation apart from being the only one to
predict the memberships that visit for the ﬁrst time a state is important
in solving a large number of problems in the theory of the P-NHSMS.
11.4
The Expected Population Structure with
Respect to the Duration of a Membership in a
State
In a P-NHSMS a membership is allowed to make a transition in the
same state. As a distinction between real transitions, which require an
actual change of state indices as a result of a transition we call virtual
transitions the situation where the state indices could be the same after
the transition. The distinction between real and virtual transitions is
useful in the theory of the semi-Markov process and as a consequence
to the P-NHSMS. Some physical processes require that only real tran-
sitions be allowed, in other processes the virtual transitions are most
important. For example when the state of the process represents the
last brand purchased by the customer in a marketing model, a virtual
transition represents a repeat purchase of a brand, an event of frequent
importance to the analyst. As another example in a manpower system,
a virtual transition represents a consideration for promotion for a mem-
ber which eventually fails to be promoted. Thus there are important
reasons to preserve our ability to speak of virtual transitions. We are
now in a position to introduce the aspect of the duration in a state for
a membership in a P-NHSMS. In this respect let us deﬁne by
di(ν, s, n)
= Pr{a membership which entered state i at time s remains in
state i up to time n having made v transitions}
We deﬁne by D(ν, ∫, \) the diagonal matrix with di(ν, s, n) in the (i, i)-
position. Obviously due to the stochastic evolutions taken place at any
©2001 CRC Press LLC

time in a P-NHSMS the probabilities D(ν, s, n) are random variables and
thus we seek E[D(ν, s, n)].
Now deﬁne
wi(n, s)
= Pr{a membership of the system which entered state i at time s
to stay n time units in state i before its next transition}
Let w(n, s) the k × k matrix which has zeros everywhere apart from the
diagonal which has in position i the element wi(n, s).
It can be proved that the matrix which in the (i, i)-position contains
the probabilities that a membership which entered state i at time s stays
n time units in state i before making a real transition is given by
w(n, s)−P(s)⋄H(n, s)⋄I = I⋄{[P(s)⋄H(n, s)]U}−[P(s)⋄H(n, s)⋄I]
For ν = 2, 3, ... consider the probability which is the (i, j)-element of
the matrix
[P(s) ⋄H(m1, s) ⋄I][P(s + m1) ⋄H(m2 −m1, s + m1) ⋄I...]
...[P(s + mν−2) ⋄H(mν−1 −mν−2, s + mν−2) ⋄I]
×[w(n −mν−1, s + mν−1) −P(s + mν−1)
⋄H(n −mν−1, s + mν−1) ⋄I] 
(11.4.1)
then it can be proved that this is the probability (see Figure 11.5) Pr{a
membership of the system which entered state i at time s makes ν −1
virtual transitions in the time interval (s, s + n) at exactly the times
s + m1, s + m2, ... , s + mν−1 and makes a real transition at time s + n}
FIGURE 11.5
Duration in state i
©2001 CRC Press LLC

We can write the above product as
{
ν−2

r =1
[P(s + mr) ⋄H(mr +1 −mr, s + mr) ⋄I]}
×[w(n −mν−1, s + mν−1) −P(s + mν−1) ⋄H(n −mν−1, s + mν−1) ⋄I].
(11.4.2)
Deﬁne by
δ(ν) =
	 1,
if ν = 0
0,
otherwise .
Then if among other steps we sum the above product for all possible
values of the holding times m1, m2, ..., mν−1 then we can prove that
D(ν, s, n)
=
n−ν +1

m1 =1
{
n−ν +2

m2 =1+ m1
{...{
n−1

mν −1 =1+ mν −2
{
ν−2

r =1
[P(s + mr)
⋄H(mr +1 −mr, s + mr) ⋄I]}
×[w(n −mν−1, s + mν−1) −P(s + mν−1)
⋄H(n −mν−1, s + mν−1) ⋄I]}...}}
+ δ(ν −1)[w(n, s) −P(s) ⋄H(n, s) ⋄I]} 
(11.4.3)
where obviously the ﬁrst part in the second side of the above equation
is zero for ν = 1.
We can prove that
E[w(n, s)] = I ⋄{[(P −ˆEp(s)) ⋄(H(m) −ˆEH(s))]U}. 
(11.4.4)
Using the fact that E[E[X/Y, Z]] = E[X] and the independence of the
various stochastic evolutions of selections we can prove that the expected
values of the above probabilities are
E[D(ν, s, n)]
=
n−ν+1

m1=1
{
n−ν+2

m2=1+m1
{...{
n−1

mν−1=1+mν−2
{
ν−2

r=1
{[P −ˆEp(s + mr)]
⋄[H(mr+1 −mr) −ˆEH(s + mr)] ⋄I}}
×{I ⋄{[(P −ˆEp(s + mν−1)) ⋄(H(n −mν−1) −ˆEH(s + mν−1))]U}
©2001 CRC Press LLC

−1) −ˆEH(s + mν−1))] ⋄I}}...
+ δ(ν −1){I ⋄{[(P −ˆEp(s)) ⋄(H(m) −ˆEH(s))]U}
−[P −ˆEp(s)] ⋄[H(m) −ˆEH(s)] ⋄I}. 
(11.4.5)
Now let us deﬁne by N (d)
i
(ν, t) = {the number of memberships which
entered in state i in the system and remain in i up to time t having made
ν transitions} and N(d)(ν, t) = [N (d)
1
(ν, t), N (d)
2
(ν, t), ..., N (d)
k (ν, t)]. With
probabilistic argument we can prove that
E[N(d)(ν, t)] = N(0)E[D(ν, ′, ⊔)]
+
t

m=1
∆T(m)[r0 −ˆEr(t)]E[D(ν, m, t −m)].(11.4.6)
Relations (11.4.5), (11.4.6) provide a complete description of the evo-
lution of the expected population structure of the system in relation with
their duration in the state.
11.5
The Expected Population Structure with
Respect to the State Occupancy of a
Membership
In this section we will study the number of times a membership occupies
a speciﬁc state in a time interval. In many applications this aspect is
important in practice. In this respect let us deﬁne by
ωij(ν, s, n)
= Pr{a membership of the system which entered in state i at time
s willenter state jon ν occasions in the time period (s, s + n)}
Let ⊗(ν, ∫, \) be the matrix the (i, j) element of which is the above
probability.
Now deﬁne by
fij(s, n)
= Pr{a membership of the system which entered in state i at time s
will enter state j for the first time at time n}
©2001 CRC Press LLC

Let F(s, n) = {fij(s, n)}i,j∈S then it is clear that
E[⊗(′, ∫, \)] =
∞

⟨=\+∞
E[F(∫, ⟨)] = U −
n

h=0
E[F(∫, ⟨)] = E[F∗(∫, ⟨)].
(11.5.1)
Consider the expected value of the probability (see Figure 11.6) Pr{a
membership of the system which entered in state i at time s will enter
for the ﬁrst time in state j at time s + m1, then will enter again for the
ﬁrst time in state j at time s + m2, ..., will enter again for the ﬁrst time
since s + mν−1 in state j at time s + mν which is the ν-th time it enters
since time s and then will not enter again up to time s + n}.
FIGURE 11.6
Occupancy of state j for ν times in the time interval (s, s + n)
It can be proved that it is the (i, j)-element of the matrix
E[F(s, m1)]{E[F(s + m1, m2 −m1)]{E[F(s + m2, m3 −m2)]{...
...{E[F(s + mν−1, mν −mν−1)][E[F∗(s + mν, n −mν)] ⋄I]
⋄I}... ⋄I} ⋄I}
= E[

(F(s, m1, m2, ..., mν))].
(11.5.2)
Now if we sum the above product for all possible values of the holding
times then we get
E[⊗(ν, s, n)] =
n−ν+1

m1=1
n−ν+2

m2=1+m1
...
n

mν=1+mν−1
E[

(F(s, m1, m2, ..., mν))].
(11.5.3)
In order to complete the calculation of E[⊗(ν, s, n)] we have to ﬁnd
E[F(s, n)] for any value of n and s. However it can be easily seen that
E[F(s, n)] =
n

ν=1
E[F(ν, s, n)]
(11.5.4)
©2001 CRC Press LLC

and the probabilities E[F(ν, s, n)] are calculated in Section 11.3.
In order to ﬁnd the population structure with respect the number of
times a membership has visited its present state we deﬁne N (ω)
j
(ν, t) =
Pr{the number of memberships which entered state j on ν occasions in
the time period (0, t)}.
Let N(ω)(ν, t) = [N (ω)
1
(ν, t), N (ω)
2
(ν, t), ..., N (ω)
k
(ν, t)] then with proba-
bilistic arguments we can prove that
E[N(ω)(ν, t)] = N(0)E[⊗(′, ∫, \)]
+
t

m=1
∆T(m)[r0 −ˆEr(m)]E[⊗(ν, m, t −m)](11.5.5)
where the expected values of the matrices E[⊗(0, s, n)], E[⊗(ν, m, t −
m)] and E[F(ν, s, n)], are given through relations (11.5.1), (11.5.3) and
(11.3.14), which completes the study of the evolution of the expected
structure of the system in relation with the number of times a member-
ship visited a speciﬁc state.
11.6
The Expected Population Structure with
Respect to the Counting Transition
Probabilities
The semi-Markov processes allows a distinction between the number of
time units that have passed and the number of transitions that have
occurred.
Thus it is important to study not only the probability of
a membership being in each state at time t but also the probability
distribution of the number of transitions made by that time. In this
respect let us deﬁne by
φij(ν, s, n)
= Pr{amembershipwhichenteredstateiattimeswillmakeν
transitionsinthetimeinterval(s, s + n)andbeinstatej
attimes + n}
Now we deﬁne by Φ(ν, s, n) to be the matrix the (i, j)-element of
which is the probability φij(ν, s, n). We call the probabilities Φ(ν, s, n)
the counting transition probabilities of the P-NHSMS. Counting tran-
sition probabilities for NHSMS were studied by Papadopoulou (1997)
©2001 CRC Press LLC

using geometric transformations. In the present we will be interested in
ﬁnding the expected values E[Φ(ν, s, n)] which are necessary in ﬁnding
the expected population structure in respect with counting transitions.
Consider the probability Pr{a membership of the system which entered
in state i at time s will make ν transitions at times s+m1, s+m2, ..., s+n
and enter in state j at time s + n}.
Then it can be proved that the expected value of the above probability
is the (i, j)-element of the matrix
{[P −ˆEp(s)] ⋄[H(m1) −ˆEH(s)]}
× {[P −ˆEp(s + m1)] ⋄[H(m2 −m1) −ˆEH(s + m1)]}...
...{[P −ˆEp(s + mν−1)] ⋄[H(mν −mν−1) −ˆEH(s + mν−1)]}
=
ν

r=1
{[P −ˆEp(s + mr−1)] ⋄[H(mr −mr−1) −ˆEH(s + mr−1)]}
(11.6.1)
with m0 = 0 and mν = n.
With the above result as basis of our analysis of events we can prove
that
E[Φ(ν, s, n)]
=
n−ν+1

m1=1
{
n−ν+2

m2=1+m1
{...{
n−1

mν−1=1+mν−2
{
ν

r=1
{[P −ˆEp(s + mr−1)]
⋄[H(mr −mr−1)
−ˆEH(s + mr−1)]} · E[W(s + mr, n −mr)]
+
n−ν+1

m1=1
{
n−ν+2

m2=1+m1
{...{
n−1

mν−1=1+mν−2
× {
ν

r=1
{[P −ˆEp(s + mr−1)] ⋄[H(mr −mr−1) −ˆEH(s + mr−1)]}
(11.6.2)
where m0 = 0 and mν = n, and from Vassiliou and Tsakiridou (1999)
we get that
©2001 CRC Press LLC

E[W(n, s)] =
∞

m=n+1
I ⋄{E[P(s) ⋄H(m, s)]U}
= I ⋄{([P −ˆEp(s)] ⋄
∞

m=n+1
[H(m) −ˆEH(s)])U}
Now deﬁne by N (φ)
i
(ν, t) = {the number of memberships of the P-
NHSMS that are in state i at time t having made ν transitions from their
entrance into the system} and N(φ)(ν, t) = [N (φ)
1
(ν, t), N (φ)
2
(ν, t), ...,
N (φ)
k
(ν, t)]. Then the expected population structure with respect the
number of transitions made up to time t it can be proved that it is
given by
E[N(φ)(ν, t)] = N(0)E[Φ(ν, 0, n)]
+
t

m=1
∆T(m)[r0 −ˆEr(m)]E[Φ(ν, m, t −m)]
(11.6.3)
where the expected value of the matrices E[Φ(ν, 0, n)], E[Φ(ν, m, t−m)]
are given in (11.6.2).
The relation apart from being the only one to
predict the memberships in each state in relation with the number of
transitions made up to time t is important in solving a large number of
problems in the theory of the P-NHSMS.
It is easy to see that
E[N(t)] =
t

ν=0
E[N(φ)(ν, t)].
References
1. Bartholomew, D. J. (1982). Stochastic models for social processes,
Third Edition. John Wiley & Sons, Chichester.
2. Bartholomew, D. J. (1986).
Social applications of semi-Markov
processes. In Semi-Markov Models: Theory and Applications (Ed.,
J. Janssen). Plenum Press, New York.
3. Bartholomew, D. J., Forbes, A. F., and McClean, S. I. (1991).
Statistical Techniques for Manpower Planning. John Wiley & Sons,
Chichester.
©2001 CRC Press LLC

4. Campbell, S. L. and Meyer, C. D. (1979). Generalized inverses of
Linear Transformations. Pitman, London.
5. Howard, R. A. (1971).
Dynamic Probabilistic Systems, Vol.
II.
John Wiley & Sons, Chichester.
6. Janssen, J. (1986) (Ed.). Semi-Markov Models: Theory and Appli-
cations. Plenum Press, New York.
7. McClean, S. I. (1980) A semi-Markov model for a multigrade pop-
ulation with Poisson recruitment. Journal of Applied Probability
17, 846–852.
8. McClean, S. I. (1986). Semi-Markov models for manpower plan-
ning. In Semi-Markov Models: Theory and Applications (Ed., J.
Janssen). Plenum Press, New York.
9. McClean, S. I. (1993). Semi-Markov models for human resource
modelling. IMA Journal of Mathematics Applied in Business and
Industry 4, 307–315.
10. McClean, S. I., Montgomery, E., and Ugwuowo, F. (1997). Non-
homogeneous continuous time and semi-Markov manpower models.
Applied Stochastic Models and Data Analysis 13, 191–198.
11. Mehlman, A. (1979). Semi-Markovian manpower models in con-
tinuous time. Journal of Applied Probability 6, 416–422.
12. Meyer, C. D. (1975). The role of the group generalized inverse in
the theory of ﬁnite Markov chains. SIAM Review 17, 443–464.
13. Papadopoulou, A. A. (1997). Counting transitions-entrance proba-
bilities in non-homogeneous semi-Markov systems. Applied Stochas-
tic Models and Data Analysis 13, 199–206.
14. Papadopoulou, A. A. and Vassiliou, P.-C. G. (1994). Asymptotic
behavior of non-homogeneous semi-Markov systems. Linear Alge-
bra and its Applications 210, 153–198.
15. Vassiliou, P.-C. G. (1996).
The non-homogeneous semi-Markov
system in a stochastic environment. In Applied Probability in Honor
of J. M. Gani (Eds., C. C. Heyde, Yu. V. Parohorov, R. Pyke, and
S. T. Rachev). Springer-Verlag.
16. Vassiliou, P.-C. G. (1998). The evolution of the theory of Non-
homogeneous Markov systems. Applied Stochastic Models and Data
Analysis 13, 159–176.
17. Vassiliou, P.-C. G. and Papadopoulou, A. A. (1992). Non-homogen-
eous semi-Markov systems and maintainability of the state sizes.
Journal of Applied Probability 29, 519–534.
18. Vassiliou, P.-C. G. and Symeonaki, M. A. (1998). The perturbed
non-homogeneous Markov System in continuous time.
Applied
©2001 CRC Press LLC

Stochastic Models and Data Analysis 13, 207–216.
19. Vassiliou, P.-C. G. and Symeonaki, M. A. (1999). The perturbed
non-homogeneous Markov system. Linear Algebra and its Applica-
tions 289, 319–332.
20. Vassiliou, P.-C. G. and Tsakiridou, H. (1999). The perturbed non-
homogeneous semi-Markov system. To appear.
©2001 CRC Press LLC

PART III
Distributions, Characterizations, and
Applications
©2001 CRC Press LLC

12
Characterizations of Some Exponential
Families Based on Survival Distributions
and Moments
M. Albassam, C. R. Rao, and D. N. Shanbhag
University of Sheﬃeld, Sheﬃeld, UK
The Pennsylvania State University, University Park, PA
University of Sheﬃeld, Sheﬃeld, UK
ABSTRACT Shanbhag (1972b) characterized a family of exponential
distributions via unimodality. With a minor modiﬁcation in Shanbhag’s
argument, one can arrive at a characterization of a family of gamma
distributions, considering α-unimodality in place of unimodality. A dis-
crete version of the latter result has recently been obtained by Sapatinas
(1993, 1999). In the present paper, we give a uniﬁed approach to arrive
at the aforementioned results and certain of their variations. In the pro-
cess of doing this, we also show as to how these results are linked with
the results of Laha and Lukacs (1960), Shanbhag (1979), and Morris
(1982), as well as with a damage model introduced by Rao (1965).
Keywords and phrases Damage model, unimodality, α-unimodality,
α-monotonicity, Laha-Lukacs theorem, Shanbhag-Morris theorem, Rao-
Rubin theorem, Poisson, negative binomial distributions, power series
distribution, exponential family
12.1
Introduction
Khintcthine (1938) [also see Lukacs (1970, p. 92)] showed that a distri-
bution function F on R is unimodal with vertex 0 if and only if a random
variable X with distribution function F satisﬁes the relation
X
d= U Z ,
(12.1.1)
where U and Z are independent random variables such that U is uni-
formly distributed on (0, 1). Olshen and Savage (1970) gave an extended
version of the characterization showing that (in the notation as above) F
©2001 CRC Press LLC

is α-unimodal, with α > 0, if and only if (12.1.1) with U 1/α in place of U
and U and Z satisfying the stated conditions holds. A discrete analogue
relative to distributions concentrated on {0, 1, . . .} of α-unimodality has
recently been introduced by Steutel (1988); he deﬁned an N⊬-valued r.v.
X to be α-montone (discrete α-unimodal) where α > 0, if
X
d= U 1/α ◦Z ,
where U is as deﬁned above, Z is a nonnegative integer-valued r.v. in-
dependent of U, and the operator ◦is in the sense of Steutel and van
Harn (1979). The latter deﬁnition can easily be seen to be linked with a
damage model introduced by Rao (1965). Indeed, it follows that a dis-
tribution concentrated on {0, 1, ...} is α-unimodal in the sense of Steutel
if and only if it denotes, in a damage model, the distribution of the
resulting random variable when the original random variable (which is
nonnegative integer-valued) is subjected to a destructive process accord-
ing to the survival distribution
S(x | z) =
−α
x
 −1
z−x

−α−1
z

, x = 0, 1, 2, ..., z ; z = 0, 1, ...
. 
(12.1.2)
[Refer to Rao (1965) and M. B. Rao and Shanbhag (1982) for a precise
deﬁnition of a damage model]. The survival distribution in (12.1.2) is a
specialized version of the survival distribution considered by Shanbhag
(1977) to give a generalization of the celebrated Rao-Rubin theorem
(1964).
Shanbhag (1972b) showed, among other things, that for all θ (in an
open interval)
Xθ
d= U Zθ , 
(12.1.3)
with U as uniformly distributed on (0, 1) independently of Zθ and the
distributions of Xθ and Zθ forming respectively certain exponential fam-
ilies, if and only if Xθ or −Xθ is exponentially distributed, with mean
of a speciﬁc form, for each θ.
With a minor modiﬁcation in the ar-
gument of Shanbhag, it follows that replacing, in (12.1.3), U by U 1/α
with α positive and independent of θ, one gets a characterization of a
gamma distribution with index α in place of that of an exponential dis-
tribution. Essentially a discrete analogue of the latter result, following
implicitly Shanbhag’s idea, has recently been given by Sapatinas (1993,
1999). [Incidentally, Alamastaz (1985) has extended yet another result
in Shanbhag (1972b), which is not directly linked with the results in
our paper; for a bibliography of the literature relevant to the Alamastaz
result, see Pakes (1992, 1994) and Rao and Shanbhag (1994, Chapter
6).]
©2001 CRC Press LLC

The purpose of the present paper is to unify and extend the aforemen-
tioned results of Shanbhag and Sapatinas, and show that these results
are linked with certain results of Laha and Lukacs (1960), Shanbhag
(1972a, 1979), and Morris (1982). In view of the observation that we
have made above concerning the α-unimodality in the discrete case, it
follows that the result of Sapatinas referred to is linked with a certain
result on damage models; we also arrive at variations of this latter result
of the type in Shanbhag and Clark (1972) and Patil and Ratnaparkhi
(1975, 1977).
12.2
An Auxiliary Lemma
The following lemma plays a crucial role in the present investigation.
LEMMA 12.2.1
Let {(α(θ), β(θ)) : θ ∈Θ} be a collection of 2-component real-vectors
such that there exist at least three points, θ0, θ1 and θ2, in Θ such that
β(θ0), β(θ1) and β(θ2) are distinct. Also, let ci1, ci2, ci3, i = 1, 2, be real
numbers with c11 and c21 as positive and distinct. Then
exp{ci1α(θ)} = ci2 + ci3β(θ) ,
i = 1, 2; θ ∈Θ
(12.2.1)
if and only if c13 = c23 = 0, α(θ) ≡α0 on Θ for some number α0, and
exp{ci1α0} = ci2, i = 1, 2.
PROOF The “if” part of the lemma is obvious. To prove the “only if”
part of the lemma, it is suﬃcient if we show that under the assumptions
in the lemma,
(12.1) holds only if c13 = c23 = 0.
We do this, by
showing that if we assume that at least one of ci3s is nonzero, then we
are led to a contradiction. Indeed, If we have the assumption, then (12.1)
implies that α(θ) has distinct values at θ = θ0, θ1 and θ2, where θ0, θ1
and θ2 as in the statement of the lemma, and hence that both c13 and
c23 are nonzero. Assume then this is so. Without loss of generality, we
can assume that c21 < c11, c13 > 0, and that β(θ0) is the smallest of
β(θ0), β(θ1) and β(θ2). Then (12.1) implies that
exp{ci1(α(θ) −α(θ0))} −1 = ci3 (β(θ) −β(θ0)) exp{−ci1α(θ0)} ,
i = 1, 2; θ0, θ1, θ2.
(12.2.2)
Eq. (12.2), in turn, implies that α(θ0) is the smallest of α(θ0), α(θ1) and
α(θ2) and that
©2001 CRC Press LLC

exp{c21(α(θ) −α(θ0))} −1
= c23
c13
exp{−(c21 −c11)α(θ0)} (exp{c11(α(θ) −α(θ0))} −1) , θ
= θ0, θ1, θ2.
(12.2.3)
The function f deﬁned by
f(x) = (1 + x)c21/c11 −1
x
,
x > 0 ,
is strictly decreasing under our assumption that c21 < c11. (One of the
approaches to see this is that based on the binomial theorem because for
each x,
f(x) =

(1 + x)

1 −
x
1 + x
1−c21
c11
−1

/x.
)
As (12.3) implies that f(x) at x = exp{c11(α(θi) −α(θ0)} −1 , i = 1, 2,
are identical, we have a contradiction. Hence the lemma follows.
12.3
Characterizations Based on Survival
Distributions
Let {(Xθ, Zθ) : θ ∈Θ} be a family of random vectors with Θ having at
least three points, such that for each θ, Xθ and Zθ have respectively the
distributions of the form
Fθ(x) ∝

(−∞,x]
eλ1(θ)y µ1(dy) ,
x ∈R ,
and
Gθ(z) ∝

(−∞,z]
eλ2(θ)y µ2(dy) ,
z ∈R ,
where µ1 and µ2 are σ-ﬁnite measures deﬁned on the Borel σ-ﬁeld of
R and λ1 and λ2 are real functions on Θ with λ1 also as a one-to-one
function. Motivated by the link between the α-unimodality property and
the survival distribution of a certain type in a damage model, we now
establish the following theorems. These theorems extend the existing
characterizations of exponential families based on α-unimodality.
©2001 CRC Press LLC

THEOREM 12.3.1
Let {(Xθ, Zθ) : θ ∈Θ} be a family of random vectors with nonnegative
integer-valued components such that both µ1 and µ2 are concentrated on
{0, 1, ...} with µ2({0, 1}c) > 0. Also, let {ax : x = 0, 1, ...} be a sequence
of positive real numbers. Then for all z with µ2({z}) > 0 and θ ∈Θ,
Pθ{Xθ = x | Zθ = z} = ax
Az
, x = 0, 1, ..., z,
(12.3.1)
where Az = z
i=0 ai , z = 0, 1, ..., if and only if, for all θ ∈Θ, Pθ{Xθ ≤
Zθ} = 1, and, for all θ ∈Θ and some β > 0,
Pθ{Xθ = x, Zθ = z} ∝ax
	
eλ1(θ)β

z
,
x = 0, 1, ..., z; z = 0, 1, ... .
(12.3.2)
PROOF The “if” part follows easily. To prove the “only if” part, assume
that for all θ ∈Θ the conditional distribution of Xθ given Zθ is as in the
statement of the theorem. Then there exists a function C : Θ −→(0, ∞)
such that
∞

z=x
eλ2(θ)z µ2({z})
Az
= C(θ) eλ1(θ)x µ1({x})
ax
, x = 0, 1, ...,
(12.3.3)
where Az = z
x=0 ax. From (12.4), we get that
eλ2(θ)x µ2({x})
Ax
= C(θ) eλ1(θ)x
µ1({x})
ax
−eλ1(θ) µ1({x + 1})
ax+1

,
x = 0, 1, ...
,
and hence that
e(λ2(θ)−λ1(θ))x µ2({x})
Ax
= C(θ)
µ1({x})
ax
−eλ1(θ) µ1({x + 1})
ax+1

,
x = 0, 1, ...
.
(12.3.4)
Also, in view of the assumptions in the theorem including especially
that µ2({0, 1}c) > 0, it follows from (12.4) that µ1({x}) > 0 at least for
x = 0, 1, 2. (12.5), for x = 0, then implies that µ1({0})
a0
−eλ1(θ) µ1({1})
a1
> 0
on Θ and µ2({0}) > 0. In view of this, it follows from (12.5) that
©2001 CRC Press LLC

e(λ2(θ)−λ1(θ))x µ2({x})
Ax
= (µ2({0})
a0
)
µ1({x})
ax
−eλ1(θ) µ1({x + 1})
ax+1

×
µ1({0})
a0
−eλ1(θ) µ1({1})
a1
−1
, x = 0, 1, ...; θ ∈Θ. (12.3.5)
For x = 1, 2, and inductively for x = k, k +1 with k as a positive integer,
(12.6) can be reduced to the form of (12.1) with α(θ) = λ2(θ)−λ1(θ) and
β(θ) =
	
µ1({0})
a0
−eλ1(θ) µ1({1})
a1

−1
. Hence it follows from the lemma
that λ2(θ) = λ1(θ) + c , θ ∈Θ, for some constant c, and that
ecx µ2({x})
Ax
=
µ2({0})
axµ1({0}) µ1({x}) , x = 0, 1, ...
(12.3.6)
with { µ1({x})
ax
: x = 0, 1, ...} as a geometric sequence.
From (12.11)
and (12.7), it is immediate that, for each θ ∈Θ, the joint distribution of
Xθ and Zθ is of the form in (12.12) with Pθ(Xθ ≤Zθ) = 1.
THEOREM 12.3.2
Let {(Xθ, Zθ) : θ ∈Θ} be such that µ2 has at least one positive support
point and µ1 and µ2 are concentrated on R+, and let a be a positive
real-valued continuous function on (0, ∞) such that it is integrable with
respect to Lebesgue measure on (0, x) for some, and hence all, x ∈(0, ∞).
For all θ ∈Θ, there exist versions of the conditional distributions of Xθ
given Zθ satisfying
Pθ{Xθ ≤x | Zθ = z} =

1
for all x ∈R+ if z = 0
A(x)
A(z)
for all x ∈(0, z] if z > 0,
(12.3.7)
where A(x) =
 x
0 a(y)dy (and A(z) is deﬁned in obvious way) if and only
if, for all θ ∈Θ, Pθ(0 < Xθ < Zθ) = 1, and, for some positive number
β and all θ ∈Θ, (Xθ, Zθ) has an absolutely continuous distribution
with probability density function of the following form (with respect to
Lebesgue measure on R⊭
+)
fθ(x, z) ∝a(x)
	
eλ1(θ)β

z
,
x ∈(0, z], z ∈(0, ∞).
PROOF The “if” part of the assertion is obvious and to prove the “only
if” part of the assertion proceed as follows. (12.c1) implies that the re-
striction to (0, ∞) of µ1 is absolutely continuous with respect to Lebesgue
©2001 CRC Press LLC

measure with left continuous Radon-Nikodym derivative m1 such that,
for each θ ∈Θ,
m1(x) = (K(θ))−1

a(x)

[x,∞) (A(y))−1 eλ2(θ)y µ2(dy)
eλ1(θ)x

,
x ∈(0, ∞).
(12.3.8)
for some positive real-valued function K on Θ. (12.c2) implies that for
each x0 > 0 such that µ2((x0, ∞)) > 0, the hazard measure relative to
the survival function

[x,∞)
(A(y))−1 eλ2(θ)y µ2(dy)

×

[x0,∞)
(A(y))−1 eλ2(θ)y µ2(dy)
−1
,
x ∈(x0, ∞),
where the measure is as deﬁned in Kotz and Shanbhag (1980), is such
that it is not independent of θ while its value for each Borel set B of
Lebesgue measure zero is, for θ ∈Θ. This is impossible unless the restric-
tion to (0, γ), where γ = sup{x : x ∈supp[µ2]} (in standard notation),
of µ2 is absolutely continuous with respect to Lebesgue measure, with
its Radon-Nikodym derivative, m2, satisfying, for each θ ∈Θ, for almost
all x ∈(0, γ),
−(A(x))−1 eλ2(θ)x m2(x)
=

[x,∞)
(A(y))−1 eλ2(θ)y µ2(dy)

(λ1(θ) + g(x))
(12.3.9)
for some Borel measurable function g that is independent of θ. Appealing
to (12.c2) once more, one can see that, for each θ ∈Θ, for almost all
x ∈(0, γ),
−(A(x))−1 e(λ2(θ)−λ1(θ))x m2(x) = K(θ) m1(x) (a(x))−1 (λ1(θ) + g(x)).
(12.3.10)
(In the last two statements by “almost all”, we mean “almost all with
respect to Lebesgue measure”.)
Consequently, it follows that given θ0, θ1 ∈Θ with θ0 ̸= θ1, we can
take m2 in (12.c3) and (12.c4) to be such that
©2001 CRC Press LLC

m2(x) (A(x))−1 	
(K(θ1))−1 e(λ2(θ1)−λ1(θ1))x
−(K(θ0))−1 e(λ2(θ0)−λ1(θ0))x
= (λ1(θ0) −λ1(θ1)) m1(x)(a(x))−1.
This, in turn, implies that we can choose m2 and g in (12.c3) and (12.c4)
to be left continuous, and that, with this choice, the equations referred
to hold for all θ ∈Θ and x ∈(0, γ). Understand by (12.c3) and (12.c4)
henceforth their new versions with g and m2 left continuous.
Given
any x ∈(0, γ), we can ﬁnd x0, x1 such that 0 < x0 < x1 < x and
obtain from (12.c4), the expressions for exp{(λ2(θ)−λ1(θ))(x−x0)} and
exp{(λ2(θ)−λ1(θ))(x1−x0)} as linear functions of (λ1(θ)+g(x0))−1 (on
noting that λ1(θ) + g(x0) < 0) on Θ. In view of the lemma, we get then
that λ2(θ) = λ1(θ) + c for all θ ∈Θ and some constant c. This, in turn,
implies, because of the validity of (12.c4) for all θ ∈Θ and x ∈(0, γ),
that, for some constant ξ, K(θ)(λ1(θ) + ξ) is independent of θ on Θ and
g(x) = ξ for all x in (0, γ). We have hence that (12.c3) with g ≡ξ holds
for all x ∈(0, γ) and θ ∈Θ; this implies that γ = ∞and
(A(x))−1 eλ2(θ)x m2(x) = η(θ) e(λ1(θ)+ξ)x ,
x ∈(0, γ), θ ∈Θ,
(12.3.11)
for some function η on Θ. (The relation between λi’s implies then that
η is independent of θ on Θ). Also, as K in (12.c2) is not identically
equal to a constant on Θ, it is clear that the ratio of the normalizing
functions of the two exponential families cannot be identically equal to
a constant on Θ; this implies that µ1({0}) = µ2({0}) = 0. In view of
what we have proved, we can then claim that for each θ ∈Θ, (Xθ, Zθ)
has an absolutely continuous distribution (w.r.t. Lebesgue measure) on
R⊭
+ such that Pθ(0 < Xθ < Zθ) = 1 and it has a probability density
function of the form
fθ(x, z) ∝a(x)
	
eλ1(θ)β

z
,
x ∈(0, z], z ∈(0, ∞),
with β as a positive constant.
(β here equals exp{ξ} where ξ is as
in (12.c5).) Hence, we have the theorem.
REMARK If we take in Theorem 12.1, ax =
−α
x

(−1)x, x = 0, 1, ...,
then the result reduces to that corresponding to α-unimodal distribu-
tions, extending the result of Sapatinas (1993, 1999). We can obtain
characterizations of several other discrete distribution families via the
property in the theorem, taking speciﬁc {ax}. In particular, if we take
©2001 CRC Press LLC

ax = 1
x!, x = 0, 1, ..., we get the result with
Pθ{Xθ = x, Zθ = x} ∝(eλ1(θ) β)z
x!
,
x = 0, 1, ..., z; z = 0, 1, ...
.
Note that in this latter case, the marginal distribution of Xθ is P0(eλ1(θ) β)
and that of Zθ is such that
Pθ{Zθ = z} ∝
 z

x=0
1
x!
 	
eλ1(θ) β

z
,
z = 0, 1, ...
REMARK In the case of a(x) = xα−1, x ∈(0, ∞) with, α > 0, The-
orem 12.2 provides us with a characterization of a gamma distribution
family based on α-unimodality. In the notation used in the theorem, the
family that is characterized in this case has Xθ ∼Ga(|λ1(θ) + log β|, α)
and Zθ ∼Ga(|λ1(θ) + log β|, α + 1) where λ1(θ) + log β < 0. We can
obviously use other forms of the function a(·) to characterize various
other distribution families with Xθ having a well known distribution on
R+.
REMARK In view of Theorems 12.1 and 12.2, it follows that it is not
possible to have {(Xθ, Zθ) : θ ∈Θ} with µi(R+) > ⊬, µℶ(−R+) > ⊬
for i = 1, 2 and the families of the conditional distributions {Pθ{Xθ ≤
x, Zθ ≤z | Xθ ≥0, Zθ ≥0}, x, z ∈R+ : θ ∈≰} and of the conditional
distributions {Pθ{−Xθ ≤x, −Zθ ≤z | Xθ ≤0, Zθ ≤0}, x, z ∈R+ :
θ ∈≰} are both of the form characterized in Theorems 12.1 or Theo-
rems 12.2. In particular, this implies that the characterization of gamma
distributions based on α-unimodality, mentioned in the introduction fol-
lows essentially as a corollary to Theorem 12.2.
REMARK The characterization of gamma distributions referred to in
the introduction does not hold if (in the notation of the present section)
we take λi(θ)|y| in place of λi(θ)y for each of i = 1, 2 in the deﬁnitions
of Fθ and Gθ. This is illustrated by the following example:
EXAMPLE
Let {(Xθ, Zθ) : θ ∈(0, ∞)} be a family of random vectors with absolutely
continuous distributions (w.r.t. Lebesgue measure) on R⊭such that for
each θ, the corresponding density is given by
fθ(x, z) =

θα+1
2 Γ(α) |x|α−1 e−θ |z| if 0 < x < z or z < x < 0
0
otherwise,
©2001 CRC Press LLC

with α > 0 (and ﬁxed). Note that we have here for each θ ∈(0, ∞),
a random variable Uθ independent of Zθ such that it is uniformly dis-
tributed on (0, 1) and
Xθ
d= U 1/α
θ
Zθ.
Moreover, we have in the present case, for each θ ∈(0, ∞), with the
notation used above,
Fθ(x) =
 x
−∞
θα
2 Γ(α) |y|α−1 e−θ |y| dy ,
x ∈R
and
Gθ(z) =
 z
−∞
θα+1
2 Γ(α + 1) |y|α e−θ |y| dy ,
z ∈R.
REMARK The counter example given in the paper of Sapatinas (1999)
(i.e. in his Remark 2) is not valid. This is so because (in his notation)
for the P of the example, we have ρ = ∞, contradicting the parameter
space of G that is given in the paper.
REMARK Instead of stating Sapatinas’s (1999) result in terms of power
series and modiﬁed power series families, one can restate it, with repa-
rameterization, in terms of exponential families concentrated on {0, 1, . . .}.
(Note that the point ′0′ may be excluded from the parameter space, as it
leads only to a degenerate member.) Also, the way Sapatinas’s theorem
is stated, it holds with “bi > 0 for some i ≥2” deleted provided we
understand that the case of Pθ{Xθ = 0} ≡1 is excluded from consider-
ation.
REMARK Both Theorem 12.1 and 12.2 hold even when in place of the
condition that λ1 is a one-to-one function, we take only that it has three
distinct values. However, this does not apply to the results given in the
next section (i.e. in Section 12.4).
12.4
Characterizations Based on Moments
The relations of the type
Xθ
d= V Zθ ,
θ ∈Θ,
(12.4.1)
and
Xθ
d= V ◦Zθ ,
θ ∈Θ,
(12.4.2)
©2001 CRC Press LLC

where “◦” is the operator referred to in the introduction, with V ∈(0, 1)
and as a random variable independent of Zθ, lead us to some versions
of the problems addressed by the next two theorems. Note that (12.42)
is equivalent to stating that for each θ, (Xθ, Zθ) is as in Rao’s (1965)
damage model with the survival distribution as mixed binomial with
the mixing distribution ﬁxed. These results are in the spirit of those
given by Morris (1982) and Shanbhag (1979), and by Shanbhag and
Clark (1972), Patil and Ratnaparkhi (1975, 1977) and Sapatinas and
Aly (1994), respectively.
THEOREM 12.4.1
Let {(Xθ, Zθ) : θ ∈Θ} be as mentioned in Section 12.3 before the state-
ment of Theorems 12.1, but, with at least one µi as nondegenerate, Θ as
an open interval, λ1 as continuous and λ2 such that
λ2(θ) = λ1(θ) + c ,
θ ∈Θ
with c as a constant. Further let 0 < c2 < c1. Then for all θ ∈Θ,
Eθ(Xθ) = c1 Eθ(Zθ)
(12.4.3)
and
Eθ(X2
θ) = c2 Eθ(Z2
θ) ,
(12.4.4)
if and only if c2
1 < c2, and, for a given θ0 ∈Θ, (Xθ0, Zθ0) or (−Xθ0, −Zθ0)
has its components to be gamma distributed with the same scale param-
eter and the index parameters as c1(c1−c2)
c2−c2
1
and (c1−c2)
c2−c2
1
respectively.
PROOF The “if” part follows easily because of the form of the distribu-
tions of Xθ and Zθ for each θ ∈Θ. To prove the “only if” part of the
theorem, assume that (12.43) and (12.44) are valid for all θ ∈Θ. There
is no loss of generality in assuming that λ1(θ) ≡θ. Deﬁne now
βi(θ) =

R
exp{θx} µi(dx) , i = 1, 2,
θ ∈Θ.
In view of the validity of (12.43) and (12.44) for all θ ∈Θ, it follows (in
the standard notation for the ﬁrst and second derivatives) that
β
′
1(θ)
β1(θ) = c1
β
′
2(θ)
β2(θ) ,
θ ∈Θ
(12.4.5)
and that
β
′′
1 (θ)
β1(θ) = c2
β
′′
2 (θ)
β2(θ) ,
θ ∈Θ
.
(12.4.6)
Eq. (12.45) implies, in view of (12.46), that
©2001 CRC Press LLC

β
′′
1 (θ)
β1(θ) −

β
′
1(θ)
β1(θ)
2
= c1

β
′′
2 (θ)
β2(θ) −

β
′
2(θ)
β2(θ)
2

= c1

1
c2
β
′′
1 (θ)
β1(θ) −1
c2
1

β
′
1(θ)
β1(θ)
2
, θ ∈Θ.
(12.4.7)
On simplifying (12.47) gives that
V arθ(Xθ) = β
′′
1 (θ)
β1(θ) −

β
′
1(θ)
β1(θ)
2
=
c2 −c2
1
c1(c1 −c2)

β
′
1(θ)
β1(θ)
2
( =
c2 −c2
1
c1(c1 −c2)

(Eθ(Xθ))2
),
θ ∈Θ.
(12.4.8)
Eq. (12.45) implies also that
β1(θ) ∝(β2(θ))c1 ,
θ ∈Θ.
(12.4.9)
Essentially, in view of Morris (1982) or Rao and Shanbhag (1994; Corol-
lary 9.2.4) and the fact that one of the µis is nondegenerate, the assertion
follows from (12.48) and (12.49).
THEOREM 12.4.2
Let {(Xθ, Zθ) : θ ∈Θ}, c1 and c2 be as in Theorems 12.3. Then for all
θ ∈Θ,
Eθ(Xθ) = c1Eθ(Zθ)
Eθ(Xθ(Xθ −1)) = c2Eθ(Zθ(Zθ −1))
if and only if one of the following conditions holds:
(i)
c2 > c2
1 and, for a given θ0 ∈Θ, (Xθ0, Zθ0) or

−Xθ −c1(c1 −c2)
c2 −c2
1
, −Zθ −c1 −c2
c2 −c2
1

has its components to be negative binomially distributed with pa-
rameter vectors
	
c1(c1−c2)
c2−c2
1
, p

and
	
c1−c2
c2−c2
1 , p

respectively, for some
p ∈(0, 1).
©2001 CRC Press LLC

(ii)
c2 < c2
1 with
c1(c1−c2)
c2
1−c2
and
c1−c2
c2
1−c2 as integers, and, for a given
θ0 ∈Θ, Xθ0 and Zθ0 are binomially distributed with parameter
vectors
	
c1(c1−c2)
c2
1−c2
, p

and
	
c1−c2
c2
1−c2 , p

for some p ∈(0, 1).
(iii)
c2 = c2
1 and, for a given θ0 ∈Θ, Xθ0 and Zθ0 are Poisson random
variables with expected values α and α/c1 respectively for some
α > 0.
PROOF The result follows essentially via the argument used to prove
Theorem 12.3. In this case, in place of (12.46) in the proof of the previous
theorem, we get
β
′′
1 (θ)
β1(θ) −β
′
1(θ)
β1(θ) = c2

β
′′
2 (θ)
β2(θ) −β
′
2(θ)
β2(θ)

,
θ ∈Θ.
Essentially the argument that led to (12.48), leads us in the present case
to
V arθ(Xθ) =
c2 −c2
1
c1(c1 −c2)

β
′
1(θ)
β1(θ)
2
+ β
′
1(θ)
β1(θ) ,
θ ∈Θ.
The result then follows using the remainder of the argument of the proof
of the previous theorem.
REMARK That the Morris (1982) result follows via the Laha-Lukacs
(1960) result is clear essentially from what appears in Shanbhag (1979).
REMARK In the light of Morris (1982) and Shanbhag (1972a, 1979),
some of the steps in the proof of Theorem 12.3 are routine. However,
for the sake of clarity, we have reproduced these here. Also, now it is
an exercise to identify the exponential families for which, for all θ ∈
Θ, (12.43) in conjunction with
Eθ(X2
θ) = c2Eθ(Z2
θ) + c3Eθ(Zθ) + c4,
where c2, c3 and c4 are real numbers, holds; one is then essentially led
to the identiﬁcation of the six exponential families (but for scale and
location changes) met in the characterizations given by Morris (1982)
and Shanbhag (1972a, 1979) respectively.
REMARK In the case where Xθ is a weighted random variable (i.e. a
random variable with a weighted distribution) relative to Zθ for each θ ∈
©2001 CRC Press LLC

Θ, with weight function independent of θ, Theorems 12.3 and 12.4 imply
certain characterization results involving weighted distributions; these
results subsume some of the variations of the Rao-Rubin theorem (1964)
appearing in Shanbhag and Clark (1972) and Patil and Ratnaparkhi
(1975, 1977).
References
1. Alamatsaz, M. H. (1985). A note on an article by Artikis. Acta.
Math. Acad. Sci. Hungar. 45, 159–162.
2. Khintchine, A. Y. (1938). On unimodal distributions. Izv. Nauchno-
Issled. Inst. Mat. Mech. Tonsk. Gos. Univ. 2, 1–7 (in Russian).
3. Kotz, S. and Shanbhag, D. N. (1980). Some new approaches to
probability distributions. Advances in Applied Probability 12, 903–
921.
4. Laha, R. and Lukacs, E. (1960). On a problem connected with
quadratic regression. Biometrika 47, 335–343.
5. Lukacs, E. (1970). Characterstic Functions, Second Edition. Grif-
ﬁn, London.
6. Morris, C. (1982).
Natural exponential families with quadratic
variance functions. Annals of Statistics 10, 65–80.
7. Olshen, R. A. and Savage, L. J. (1970). A generalized unimodality.
Journal of Applied Probability 7, 21–34.
8. Pakes, A. G. (1992).
A characterization of gamma mixtures of
stable laws motivated by limit theorems.
Statistica Neerlandica
46, 209-218.
9. Pakes, A. G. (1994).
Necessary conditions for the characteriza-
tion of laws via mixed sums. Annals of the Institute of Statistical
Mathematics 46, 797-802.
10. Patil, G. P. and Ratnaparkhi, M. V. (1975). Problems of damaged
random variables and related characterizations. In Statistical Dis-
tributions in Scientiﬁc Work, Vol. 3 (Eds., G. P. Patil, S. Kotz,
and J. K. Ord), pp. 255–270. Dordrecht, Reidel.
11. Patil, G. P. and Ratnaparkhi, M. V. (1977). Characterizations of
certain statistical distributions based on additive damage models
involving Rao-Rubin condition and some of its variants. Sankhy¯a,
Series B 39, 65–75.
12. Rao, C. R. (1965). On discrete distributions arising out of methods
©2001 CRC Press LLC

of ascertainment. Sankhy¯a, Series A 27, 311–324 .
13. Rao, C. R. and Rubin, H. (1964). On a characterization of the
Poisson distribution. Sankhy¯a, Series A 26, 295–298.
14. Rao, C. R. and Shanbhag, D. N. (1994). Choquet-Deny Type Func-
tional Equations with Applications to Stochastic Models. John Wi-
ley & Sons, New York.
15. Rao, M. B. and Shanbhag, D. N. (1982).
Damage models.
In
Encyclopedia of Statistical Science, Vol.2 (Eds., N. L. Johnson and
S. Kotz), pp. 262-265, John Wiley & Sons, New York.
16. Sapatinas, T. (1993). Characterization and identiﬁbility results in
some stochastic models. Ph.D. thesis, University of Sheﬃeld.
17. Sapatinas, T. (1999). A characterization of the negative binomial
distribution via α-monotonicity.
Statistics & Probability Letters
45, 49–53.
18. Sapatinas, T. and Aly, M. A. H. (1994). Characterizations of some
well-known discrete distributions based on variants of the Rao-
Rubin condition. Sankhy¯a, Series A 56, 335–346.
19. Shanbhag, D. N. (1972a), Some characterizations based on the
Bhattacharyya matrix. Journal of Applied Probability 9, 580–587.
20. Shanbhag, D. N. (1972b), Characterizations under unimodality for
exponential distributions.
Research Report 99/DNS3, Sheﬃeld
University.
21. Shanbhag, D. N. (1977). An extension of the Rao-Rubin character-
ization of the Poisson distribution. Journal of Applied Probability
14, 640–646.
22. Shanbhag, D. N. (1979), Diagonality of the Bhattacharyya matrix
as a characterization. Theory of Probability and its Applications
24, 430–433.
23. Shanbhag, D. N. and Clark, R. M. (1972). Some characterizations
of the Poisson distribution starting with a power series distribution.
Proceedings of the Cambridge Philosophical Soceity 71, 517–522.
24. Steutel, F. W. (1988). Note on discrete α-unimodality. Statistica
Neerlandica 48, 137–140.
25. Steutel, F. W. and van Harn, K. (1979). Discrete analogues of self-
decomposability and stability. Annals of Probability 7, 893–899.
©2001 CRC Press LLC

13
Bivariate Distributions Compatible or
Nearly Compatible with Given
Conditional Information
Barry C. Arnold, Enrique Castillo, and Jos´e Mar´ıa Sarabia
University of California, Riverside, CA
University of Cantabria, Santander, Spain
University of Cantabria, Santander, Spain
ABSTRACT Let (X, Y ) denote a two dimensional discrete random
variable. Assume that exact or approximate statements are provided
with regard to the conditional distributions of X given Y and of Y given
X. This list of statements may be incompatible in the sense that no
joint distribution for (X, Y ) will satisfy all the statements in the list.
Measurement of the degree of incompatibility and identiﬁcation of most
nearly compatible distributions is discussed in the present paper.
Keywords and phrases Conditional speciﬁcation, imprecise probabil-
ities, ϵ-compatibility, Kullback-Leibler information, Markov chains
13.1
Introduction
Consider a two dimensional discrete random variable (X, Y ) with pos-
sible values x1, x2, . . . , xI and y1, y2, . . . , yJ for X and Y respectively.
First, suppose that we are given two putative families of conditional
distributions: One of X given Y , namely
P(X = xi|Y = yj) = aij,
i = 1, 2, . . . , I,
j = 1, 2, . . . , J
(13.1.1)
and one for Y given X, namely
P(Y = yj|X = xi) = bij,
i = 1, 2, . . . , I,
j = 1, 2, . . . , J.
(13.1.2)
The matrices A = (aij) and B = (bij) will be said to be compatible if
there exists a joint distribution P = (pij) for (X, Y ) whose corresponding
conditional distributions are given by A and B [i.e., by (13.1.1) and
(13.1.2)]. In the absence of compatibility, we wish to measure the degree
©2001 CRC Press LLC

of incompatibility and to produce an appropriate most-nearly compatible
distribution.
More generally we might be provided with imprecise speciﬁcations of
the conditional distributions of the form
a′
ij ≤P(X = xi|Y = yj) ≤a′′
ij, i = 1, 2, . . . , I, j = 1, 2, . . . , J  (13.1.3)
and
b′
ij ≤P(Y = yj|X = xi) ≤b′′
ij, i = 1, 2, . . . , I, j = 1, 2, . . . , J .  (13.1.4)
Here again we will ask if the information given, i.e., (13.1.3) and (13.1.4),
is compatible.
Speciﬁcally, does there exist a joint distribution P =
(pij) for (X, Y ) satisfying (13.3) and (13.4)? Again, in the absence of
compatibility we will endeavor to measure the degree of incompatibility
and identify a suitable nearly compatible distribution.
Problems of this nature can be expected to arise from attempts to
elicit subjective probabilities by asking a series of questions about ac-
ceptability of bets. A fertile source of such examples is the arena of prior
identiﬁcation in Bayesian analysis.
The precise speciﬁcation problem, (13.1.1) and (13.1.2), has received
most attention in the literature. A broad spectrum of possible solutions
are available. Some will extend readily to cover the imprecise speciﬁca-
tion scenario, others will not. We will brieﬂy outline the nature of the
imprecise speciﬁcation problem and a variety of possible approaches to
solution in Section 13.2. Section 13.3 will focus in more detail on the
precise speciﬁcation case with appropriate indications regarding whether
and how the techniques can be modiﬁed to account for imprecise informa-
tion. In Section 13.4, a simple precise speciﬁcation example is analysed
in detail to provide a sense of the diﬀerent results that can be expected
from diﬀerent approaches to the problem.
13.2
Imprecise Speciﬁcation
Suppose that P = (pij) is the joint distribution matrix for the random
variable (X, Y ). Each constraint in (13.1.3) and (13.1.4) can be rewritten
as a linear constraint involving the elements of P.
For example the
constraint a′
12 ≤P(X = x1|Y = y2) is equivalent to:
a12
I

i=1
pi2 −p12 ≤0 .
(13.2.1)
©2001 CRC Press LLC

Let us arrange the elements of P in a long column vector (by stacking
the columns of P) denoted by p. Rewriting the 4 × I × J constraints in
a form analogous to (13.2.1), we seek a vector p such that
Ap ≤0
(13.2.2)
where A is a known matrix of dimension (4 × I × J) by (I × J). In
addition to (13.2.2) we require that p have nonnegative elements which
sum to 1. We do not include the conditions p ≥0 and p′1 = 1 in our list
(13.2.2) because these conditions will be insisted on. We will however,
in many cases, have to be content with ﬁnding a vector p which almost
satisﬁes (13.2.2), to be called an almost compatible distribution.
The search for an almost compatible distribution can be viewed as a
problem of selecting p to minimize some measure of the degree to which
the constraints (13.2.2) are violated. Candidate objective functions in-
clude
D1(p) =
4×I×J

i=1
(Aip)+,
(13.2.3)
D2(p) = max
i (Aip)+,
(13.2.4)
and
D3(p) =
4×I×J

i=1
[(Aip)+]2,
(13.2.5)
where Ai denotes the i-th row of the coeﬃcient matrix A. The attained
value of the objective function can, for each case, be viewed as a measure
of incompatibility of the given information.
Another approach to this problem involves the concept of ϵ-compatibi-
lity introduced in Arnold, Castillo, and Sarabia (1999) in the context of
precise speciﬁcation. The concept remains viable in the current context
and provides an alternative interpretation of the discrepancy measure
deﬁned in terms of (13.2.4).
We say that information of the form Ap ≤0 is ϵ∗-compatible if ϵ∗is
the smallest value of ϵ such that the system
Ap −ϵ1 ≤0
(13.2.6)
has a solution p with p ≥0 and p′1 = 1.
In this formulation the
determination of ϵ∗can be viewed as a linear programming problem with
ϵ as the objective function and linear constraints (13.2.6) plus p ≥0 and
p′1 = 1. Of course, if ϵ∗turns out to be 0, the information Ap ≤0 is
actually compatible.
Arnold, Castillo, and Sarabia (1999) also discussed a concept of
weighted ϵ-compatibility which might be used if violations of the in-
equalities in (13.2.2) are judged to be of unequal importance. In this
©2001 CRC Press LLC

case our constraints (13.2.6) will be replaced by
Ap −ϵw′ 1 ≤0, 
(13.2.7)
where w is a known vector of suitable weights. Again, a simple linear
programming problem is to be solved. In practice, speciﬁcation of an
appropriate weight vector may be too diﬃcult and one would expect to
usually resort to the use of the unweighted constraints (13.2.6).
13.3 
Precise Sp eciﬁcation
First we remark that the material in Section 13.2 can be readily ad-
justed to handle the precise speciﬁcation case by merely assuming that
a′
ij = a′′
ij = aij and b′
ij = b′′
ij = bij, ∀i, j. However in the special case of
precise speciﬁcation, a variety of compatibility approaches have been dis-
cussed in the literature. There are thus many competitors to the distance
based approaches (13.2.3), (13.2.4), (13.2.5) and the ϵ-compatibility ap-
proach, (13.2.6). Which compatibility measure is to be appropriately
used clearly depends on the proposed use of the most compatible dis-
tribution so determined. Another criterion which deserves attention is
ease of interpretability.
Many of the competitors will be seen to be
mathematically attractive but not generally easy to interpret.
Some review of background material about compatible conditional dis-
tributions will be helpful.
Conditions for compatibility were provided in Arnold and Press (1989).
To be compatible, A and B, deﬁned in (13.1.1) and (13.1.2) must have
the same incidence matrix, i.e., we must have aij ̸= 0 if and only if
bij ̸= 0. A and B are then compatible if and only if there exist vectors
τ (of dimension I) and η (of dimension J) with nonnegative elements
such that
τibij = ηjaij,
∀i, j .
(13.3.1)
If τ and η are both normalized to have their coordinates sum to 1, then
we can identify them as representing the marginal distributions of X
and Y respectively, i.e., τi = P(X = i) and ηj = P(Y = j).
If we
consider A′B as the transition matrix of a Markov chain, a suﬃcient
condition for uniqueness of the solution to (13.3.1) is that the Markov
chain with transition matrix A′B be irreducible. Arnold and Gokhale
(1998b) discuss Markovian measures of incompatibility in some detail.
They observe that, whether A and B are compatible or not, two simple
Markov chains can be deﬁned in terms of them. First an I-state chain
with transition matrix BA′ and second a J-state chain with transition
©2001 CRC Press LLC

matrix A′B. Denote the long run distribution of BA′ by µ and the long
run distribution of A′B by ν. The vectors µ and ν then satisfy
µ = µBA′
(13.3.2)
and
ν = νA′B .
(13.3.3)
If A and B are compatible then the solutions to (13.3.2) and (13.3.3)
will satisfy
µibij = νjaij ∀i, j ,
(13.3.4)
i.e., they will provide solutions to (13.3.1). If A and B are not com-
patible, the diﬀerence between the left-hand and right-hand sides of the
equation (13.3.4), can be used to develop a measure of incompatibility.
Observe that the solutions to (13.3.2) and (13.3.3) are intimately re-
lated. It is always true that if µ and ν satisfy (13.3.2) and (13.3.3) then
µ = νA′, so that only one of the systems needs to be solved. Arnold and
Gokhale (1998b) suggest that a most nearly compatible distribution cor-
responding to A and B be deﬁned as one with elements 1
2(µibij +νjaij),
and that the degree of incompatibility of A and B be measured by a
measure such as:
M1 = 1
IJ
I

i=1
J

j=1
(µibij −νjaij)2
(13.3.5)
M2 = 1
IJ
I

i=1
J

j=1
|µibij −νjaij|
(13.3.6)
M3 =




I

i=1
J

j=1
(µibij −νjaij)2 .
(13.3.7)
Not included in their list, but worthy of consideration is
M4 = max
i,j |µibij −νjaij| .
(13.3.8)
If we deﬁne Pµ = (µibij) and Pν = (νjaij), then there is an attractive
alternative to 1
2(Pµ+Pν) as a most-nearly compatible distribution. It can
be veriﬁed that Pµ and Pν have identical marginals and we can search
for a matrix P ∗with those marginals such that the Kullback-Leibler
information distance between P ∗and Pµ equals that between P ∗and
Pν. An iterative procedure for obtaining such a P ∗may be found in
Gokhale and Kullback (1978, p. 182).
©2001 CRC Press LLC

Several other measures of incompatibility, not involving the Markov
chains BA′ and A′B, were suggested by Arnold and Gokhale (1998a).
The following quadratic incompatibility measure was suggested.
Q = min
P
I

i=1
J

j=1
[(pij −aijp·j)2 + (pij −bijpi·)2] .
(13.3.9)
The corresponding most nearly compatible matrix P would be PQ, the
choice of P for which the expression in (13.3.9) is minimized. The three
other measures suggested in the Arnold and Gokhale (1998a) paper were
Q′ = min
P
I

i=1
J

j=1
[|pij −aijp·j| + |pij −bijpi·|]
(13.3.10)
Q′′ = min
P
I

i=1
J

j=1
[(aij −pij
p·j
)2 + (bij −pij
pi·
)2]
(13.3.11)
and
Q′′′ = min
P
I

i=1
J

j=1
[|aij −pij
p·j
| + |bij −pij
pi·
|] .
(13.3.12)
Corresponding most nearly compatible distributions PQ′, PQ′′ and PQ′′′
are deﬁned as values of P that minimize the respective right-hand sides
of (13.3.10)–(13.3.12).
A plausible approach would involve selection of a distribution P whose
conditional distributions are minimally distant from the distributions
given by A and B using some convenient measure of distance between
distributions. Criteria (13.3.10) and (13.3.11) are actually of this form.
An attractive alternative is to use the Kullback-Leibler information to
measure distance between distributions. From this viewpoint Arnold and
Gokhale (1998a) suggest choosing P to minimize the following objective
function
I

i=1
J

j=1
[aij log(aijp·j
pij
) + bij log(bijpi·
pij
)] .
(13.3.13)
The distribution P ∗which minimizes (13.3.13) can be obtained by an
application of the Darroch-Ratcliﬀ(1972) iterative scaling algorithm.
As remarked earlier, the concept of ϵ-compatibility can be used in the
precise speciﬁcation context. A clear advantage of ϵ-compatibility is the
fact that it is readily adapted to handle imprecise formulations of A and
B, such is not the case for many of the other measures discussed in the
current section.
©2001 CRC Press LLC

In fact, in the precise speciﬁcation context, Arnold, Castillo and Sara-
bia (1999) suggested 3 variant concepts of ϵ-compatibility of putative
conditional distribution matrices A and B.
They described weighted
ϵ-compatibility, but for simplicity we only consider unweighted versions
here. The 3 options are:
Option 1: Minimize ϵ subject to
|pij −aijp·j| ≤ϵ ∀i, j
|pij −bijpi·| ≤ϵ ∀i, j
pij ≥0
∀i, j 
(13.3.14)
and 
i

j pij = 1 .
Option 2: Minimize ϵ subject to
|aijηj −bijτi| ≤ϵ ∀i, j
ηj ≥0
∀j
τi ≥0
∀i 
(13.3.15)

j ηj = 1
and

i τi = 1 .
Option 3: Minimize ϵ subject to
|aij
I

k =1
bkjτk −bijτi| ≤ϵ ∀i, j
τi ≥0 ∀i 
(13.3.16)
and

i
τi = 1 .
Option 1 will be recognizable as the version of ϵ-compatibility introduced
in Section 13.2 of the present paper. Using Option 1, if the objective
function ϵ is minimized by the choice (P1, ϵ∗
1) then we will say that A
and B are ϵ∗
1-compatible and a most nearly compatible matrix is given
by P1. Using Option 2, if the objective function ϵ is minimized by the
choice (τ (2), η(2), ϵ∗
2) then we will say that A and B are ϵ∗
2-compatible
and a most nearly compatible matrix is given by P2 with elements equal
to 1
2(aijη(2)
j
+ bijτ (2)
i
). Finally, using Option 3, if the objective function
ϵ is minimized by the choice (τ (3), ϵ∗
3) then we will say that A and B are
ϵ∗
3-compatible and a most nearly compatible matrix is given by P3 with
elements equal to 1
2(aij
I
k=1 bkjτ (3)
k
+ bijτ (3)
i
).
©2001 CRC Press LLC

All of the measures of compatibility thus far discussed may be used to
quantify the compatibility of any A (with columns summing to 1) and
any B (with rows summing to 1). If every element of A and every element
of B is assumed to be strictly positive (the “positivity” assumption), then
several more measures of compatibility merit consideration. A and B will
be compatible (under a positivity assumption) if and only if their uniform
marginal representations [Mosteller (1968)], UMR(A) and UMR(B),
are the same.
Consequently d(UMR(A), UMR(B)) for any distance
function d can be used as a compatibility measure. Under the positivity
assumption, A and B will also be compatible if and only if their cross
product ratios are equal. Following Arnold and Gokhale (1998b) we can
deﬁne a cross product ratio matrix corresponding to A to be a matrix
with elements
aijaIJ/aiJaIj
(13.3.17)
Analogously we may deﬁne CPR(B) and a measure of compatibility is
provided by d(CPR(A), CPR(B)) for any distance function d.
The ﬁnal criterion for measuring compatibility between A > 0 and
B > 0 is based on the compatibility condition introduced by Arnold
and Press (1989) [equation (13.3.1)]. If we deﬁne a matrix C1 to have
elements bij/aij and row normalize C1 to make the rows sum to 1, then
the resulting matrix N (1) will have identical rows if and only if A and B
are compatible. Analogously if we deﬁne C2 to have elements aij/bij and
column normalize C2 to have columns summing to 1, the resulting matrix
N (2) will have identical columns if and only if A and B are compatible.
A related Kullback-Leibler measure of compatibility is then provided by
D =
I

i=1
J

j=1
¯n(1)
·j log(¯n(1)
·j /n(1)
ij )
+
I

i=1
J

j=1
¯n(2)
i· log(¯n(2)
i· /n(2)
ij ) .
(13.3.18)
where ¯n(1)
·j
=
1
I
I
i=1 n(1)
ij
and ¯n(2)
i·
=
1
J
J
j=1 n(2)
ij .
Of course other
measures of inhomogeneity of the rows of N1 and the columns of N2 can
be used if desired.
©2001 CRC Press LLC

13.4
An Example
Consider conditional distributional matrices A and B as follows:
A =






1
4
1
3
1
4
1
3
1
2
1
3






,
B =






1
2
1
2
1
3
2
3
1
4
3
4






.
(13.4.1)
It is not diﬃcult to verify that these are incompatible. We will use this
pair A, B to illustrate the spectrum of compatibility measures described
in the present paper and to illustrate the various most-nearly compatible
matrices P obtained using the diﬀerent compatibility measures.
The long run distributions for the Markov chains BA′ and A′B are
respectively given by
ν =
13
37, 24
37

(13.4.2)
and
µ =
 45
148, 45
148, 58
148

.
(13.4.3)
Consequently
(µibij) =






45
296
45
296
30
296
60
296
29
296
87
296






(13.4.4)
and
(νjaij) =






26
296
64
296
26
296
64
296
52
296
64
296






.
(13.4.5)
Since (µibij) ̸≡(νjaij) we conﬁrm the incompatibility of A and B. The
most nearly compatible distribution using the Markov chain approach is
then
1
2(µibij + νjaij)

=






71
592
109
592
56
592
124
592
81
592
151
592






.
(13.4.6)
©2001 CRC Press LLC

An alternative, chosen to have the same marginals as (13.4.4) and (13.4.5)
but to be equally distant from (13.4.4) and (13.4.5) in terms of Kullback-
Leibler distance is
P ∗= (1/296)


35.40
54.60
28.33
61.67
40.27
75.73

.
(13.4.7)
The incompatibility measures (13.3.5)–(13.3) when applied to our exam-
ple (13.4.1) yield values of
M1 = 0.004136
(13.4.8)
M2 = 0.062162
(13.4.9)
M3 = 0.143809
(13.4.10)
M4 = 0.077703
(13.4.11)
If we use incompatibility measures (13.3.9)–(13.3.12), which do not in-
volve the Markov chains A′B and BA′, the corresponding values for
example (13.4.1) are:
Q = 0.010
(13.4.12)
Q′ = 0.241
(13.4.13)
Q′′ = 0.061
(13.4.14)
Q′′′ = 1.157
(13.4.15)
The corresponding most nearly compatible matrices P are given by
PQ =


0.119
0.194
0.096
0.224
0.122
0.246


(13.4.16)
PQ′ =


0.207
0.207
0.103
0.207
0.069
0.207


(13.4.17)
PQ′′ =


0.117
0.148
0.097
0.212
0.142
0.283


(13.4.18)
and
PQ′′′ =


0.001
0.001
0.130
0.261
0.152
0.455

,
(13.4.19)
©2001 CRC Press LLC

where in the last two it has been assumed that pij > 0.001, otherwise,
Q′′ and Q′′′ are not well deﬁned (division by zero). The Kullback-Leibler
measure of compatibility (13.3.13) yields a value of 0.0791, attained when
P is given by
PK =


0.11806
0.16282
0.09507
0.20403
0.14497
0.27505

.
(13.4.20)
Next we turn to the 3 variant forms of ϵ-compatibility, (13.3.14)–(13.3.16).
Using Option 1 we ﬁnd
ϵ∗
1 = 0.03472
(13.4.21)
and
P1 =






17
144
3
16
1
12
2
9
19
144
37
144






=


0.11806
0.18750
0.08333
0.22222
0.13194
0.25694

.
(13.4.22)
Option 2 yields
ϵ∗
2 = 0.069444
(13.4.23)
and
P2 =






17
144
3
16
5
54
23
108
19
144
37
144






=


0.11806
0.18750
0.09259
0.21296
0.13194
0.25694

,
(13.4.24)
while Option 3 results in
ϵ∗= 0.076087
(13.4.25)
and
P3 =






13
92
13
92
5
46
5
23
9
92
27
92






=


0.14130
0.14130
0.10870
0.21739
0.09783
0.29348

.
(13.4.26)
For completeness we will measure the compatibility of A and B using
the measures which involved a positivity assumption. First we need the
uniform marginal representations of A and B [as in (13.4.1)]. They are
UMR(A) =


0.1477
0.1857
0.1477
0.1857
0.2047
0.1287


(13.4.27)
©2001 CRC Press LLC

and
UMR(B) =


0.2152
0.1181
0.1589
0.1745
0.1259
0.2074

.
(13.4.28)
Incompatibility is once more conﬁrmed, since UMR(A) ̸= UMR(B).
A nearly compatible distribution P say PUMR is obtained by averaging
(13.4.27) and (13.4.28) and then adjusting this average to have marginals
ν and µ [given in (13.4.2) and (13.4.3)]. In this way we get
PUMR =


0.1194
0.1847
0.0959
0.2082
0.1361
0.2558

.
(13.4.29)
Using a simple Euclidean distance measure between matrices, i.e.,
d(C, D) =




I

i=1
J

j=1
(cij −dij)2,
we obtain as our measure of incompatibility
d(UMR(A), UMR(B)) = 0.147547
(13.4.30)
The cross-product ratio matrices corresponding to A and B are [recall
(13.3.17)]
CPR(A) =


0.5
1
0.5
1
1
1


(13.4.31)
and
CPR(B) =


3
1
1.5
1
1
1

.
(13.4.32)
Our nearly compatible distribution PCP R is obtained by averaging
(13.4.31) and (13.4.32) and adjusting this average to have marginals
ν and µ [given by (13.4.2) and (13.4.3)]. Thus we obtain
PCP R =


0.1344
0.1696
0.0948
0.2093
0.1221
0.2697

.
(13.4.33)
The corresponding measure of incompatibility, again using Euclidean
distance between matrices, is
d(CPR(A), CPR(B)) = 2.69
(13.4.34)
©2001 CRC Press LLC

Turning to the ﬁnal incompatibility measure (13.3.18) we ﬁnd
N1 =


4/7
3/7
2/5
3/5
2/11
9/11


(13.4.35)
and
N2 =


2/13
12/29
3/13
9/29
8/13
8/29


(13.4.36)
and consequently
D = 0.1837 + 0.1426 = 0.3263 .
Note that this last approach does not provide us with a most nearly
compatible P, it just provides a measure of compatibility.
References
1. Arnold, B. C., Castillo, E., and Sarabia, J. M. (1999). Conditional
Speciﬁcation of Statistical Models. Springer, New York.
2. Arnold, B. C. and Gokhale, D. V. (1998a).
Distributions most
nearly compatible with given families of conditional distributions.
Test 7, 377–390.
3. Arnold, B. C. and Gokhale, D. V. (1998b). Remarks on incompat-
ible conditional distributions. Technical Report #260, Department
of Statistics, University of California, Riverside.
4. Arnold, B. C. and Press, S. J. (1989).
Compatible conditional
distributions. Journal of the American Statistical Association 84,
152–156.
5. Darroch, J. and Ratcliﬀ, D. (1972). Generalized iterative scaling
for log-linear models. Annals of Mathematical Statistics 43, 1470–
1480.
6. Gokhale, D. V. and Kullback, S. (1978). The Information in Con-
tingency Tables. Marcel Dekker, New York.
7. Mosteller, F. (1968). Association and estimation in contingency
tables. Journal of the American Statistical Association 63, 1–28.
©2001 CRC Press LLC

14
A Characterization of a Distribution
Arising from Absorption Sampling
Adrienne W. Kemp
University of St. Andrews, St. Andrews, Scotland
ABSTRACT The paper ﬁnds that the distribution arising from direct
absorption sampling is characterized as the distribution of U|U +V = m
where m is constant and U and V have respectively a q-binomial and a
Heine distribution with the same argument parameter. An application is
given concerning the total number of cases of employee misbehaviour in
two workplaces, given that there is underreporting in the ﬁrst workplace.
Keywords and phrases Absorption distribution, q-binomial distribu-
tion, Heine distribution, basic hypergeometric series, q-series, visibility
bias, weighted distribution, weight function
14.1
Introduction
Absorption sampling and the absorption distribution have a long history
that dates back to the post 1939-1945 problem studied by Blomqvist
(1952), Borenius (1953), and Zacks and Goldfarb (1966). It concerns the
number of individuals Y out of n who fail to cross a mineﬁeld containing
m mines, i.e. the number of particles absorbed when n particles pass
through a medium containing m absorption points. Dunkl (1981) studied
the equivalent problem of the number of typos Y that are found in n
scans of a manuscript containing m typos; see also Johnson and Kotz
(1969) and Johnson, Kotz, and Kemp (1992). Dunkl was the ﬁrst person
to recognize the connection between the distribution and the theory of
q-hypergeometric functions, i.e. q-series.
Kemp (1998) recast the problem in terms of a closed population of m
endangered animals for which a captive breeding programme is planned.
She supposed that a ﬁxed amount of eﬀort is available each day for
searching for the animals, but that when an animal is found then the
©2001 CRC Press LLC

search is abandoned for the rest of that day.
She assumed that the
animals have constant and independent probabilities of evading capture
such that the probability that a particular animal evades capture on a
particular day is q. If a ﬁxed number k of individuals is needed and
the variable of interest, X, is the number of search days in excess of k
that are required, then we have inverse absorption sampling. If a ﬁxed
number n of search days is available and the variable of interest, Y , is the
number of individuals found, then we have direct absorption sampling,
as in the Blomqvist and Dunkl scenarios.
Other applications of absorption sampling include the ﬂight of planes
past a number of potentially fatal missile launchers, scans of a computer
listing of commercial transactions for fraudulant transactions, and the
search by parents for the latest electronic toy when only a limited number
of such toys is produced. Rawlings (1997) has generalized the particle
absorption model in such a way that the integer restriction on m is
removed.
Kemp (1998) realized that the relationship between direct and inverse
absorption sampling is analogous to that between direct and inverse bi-
nomial sampling.
She found that it is straightforward to obtain the
probability mass function (pmf) for inverse absorption sampling and
therefrom the pmf for direct absorption sampling. Initially the proba-
bility that the searcher locates one of the required items is 1 −qm, but
with each subsequent successful search the probability of ﬁnding an item
is reduced, 1−qm−1, 1−qm−2, etc. The probability generating function
(pgf) for X, the number of searches in excess of k that are required in
order to ﬁnd a ﬁxed number k of items, is therefore a convolution of k
geometric pgfs, each with support 0, 1, . . .,
GX(z) =
k

i=1
(1 −qm−i+1)
(1 −qm−i+1z).
(14.1.1)
Expansion via the q-binomial theorem gives
Pr[X = x] = qx(m−k+1) (qm; q−1)k(qk; q)x
(q; q)x
,
x = 0, 1, . . . ,
(14.1.2)
where (a; q)0 = 1, (a; q)x = (1 −a)(1 −aq) · · · (1 −aqx−1), 0 < q < 1.
As shown by Kemp (1998), the pmf for Y , the number of items found
given a ﬁxed number n of searches, is then
Pr[Y = y] = Pr[X + k = n + 1|k = y + 1]
Pr[last search is successful]
= q(n−y)(m−y) (1 −qm) · · · (1 −qm−y)(1 −qy+1) · · · (1 −qn)
(1 −q) · · · (1 −qn−y)(1 −qm−y)
©2001 CRC Press LLC

= q (n−y )( m−y ) (q m−y +1; q)y(q n−y +1; q)y
(q; q)y
,
y = 0, 1, . . . , min(m, n). 
(14.1.3)
Kemp found that the (direct) absorption distribution is log-concave
and unimodal and that it has an increasing failure rate. From (14.1.3)
the pgf is
GY (s) = q mn 
2φ1(q−n, q−m; 0; q, qz), 
(14.1.4)
where AφB(·) is Gasper and Rahman’s (1990) (G/R) deﬁnition of a basic
hypergeometric series (q-series):
AφB(a1, . . . , aA; b1, . . . , bB; q, z)
=
∞

j =0
(a1; q)j . . . (aA; q)jz j
(b1; q)j . . . (bB; q)j(q; q)j

(−1)jq( 
j
2)B −A+1 
.
The symmetry in m and n is an unexpected feature; we assume from
here onwards that m ≤n.
The purpose of this paper is to show that when m ≤n, m constant,
and U and V are independent, then Y ∼(U|U + V = m) has the
absorption distribution with pgf (14.1.4) if and only if U has a q-binomial
distribution with parameters (λ, q) and V has a Heine distribution with
parameters (n, λ, q). The q-binomial distribution was introduced into the
literature by Kemp and Newton (1990) and Kemp and Kemp (1991). It
has the pgf
GB(z) =  1φ0(q−n; −; q, −θz)
1φ0(q−n; −; q, −θ) 
(14.1.5)
and pmf
Pr[U = u] =
1
(−θq−n; q)n
· (q−n; q)u(−θ)u
(q; q)u
,
u = 0, 1, . . . , n.  (14.1.6)
The Heine distribution was ﬁrst studied by Benkherouf and Bather
(1988); see also Kemp (1992a,b). Its pgf is
GH(z) =  0φ0(−; −; q, −ψz)
0φ0(−; −; q, −ψ) 
(14.1.7)
and its pmf is
Pr[V = v] =
1
(−ψ; q)∞
· q v (v −1) /2ψ v
(q; q)v
,
v = 0, 1, . . . .  
(14.1.8)
An application of the characterization theorem is given in section 3.
It concerns the total number of cases of employee misbehaviour (e.g.
thefts, misuses of the Internet) in two workplaces belonging to the same
company, where the number of cases in each workplace has a Heine (λ, q)
distribution but there is underreporting of cases in the ﬁrst workplace.
©2001 CRC Press LLC

14.2
The Characterization Theorem
THEOREM 14.2.1
The distribution with pgf
GY (z) = qmn
2φ1(q−n, q−m; 0; q, qz)
(i.e. the absorption distribution with pgf (14.1.4)) is the distribution of
Y ∼(U|U + V = m), where m is a constant, m ≤n, and U and V are
independent, iﬀU has a q-binomial distribution with pgf (14.1.6) and V
has a Heine distribution with pgf (14.1.7), where θ = ψ = λ.
PROOF If U and V are independent and have distributions (14.1.6) and
(8) respectively, with θ = ψ = λ, then
P[U = u|U + V = m]
= K−1 (q−n; q)u(−λ)u
(q; q)u
· q(m−u)(m−u−1)/2λm−u
(q; q)m−u
= K−1 (q−n; q)u(−1)u
(q; q)u
· λmqm(m−1)/2+u(u−1)/2+(1−m)u
1
·(q−m; q)u(−1)uqmu−u(u−1)/2
(q; q)m
= K−1 λmqm(m−1)/2(q−m; q)u(q−n; q)uqu
(q; q)m(q; q)u
,
u = 0, 1, . . . , m,
where K−1 is the normalizing coeﬃcient. The pgf for Y ∼(U|U + V =
m) is therefore
GY (z) = 2φ1(q−n, q−m; 0; q, qz)
2φ1(q−n, q−m; 0; q, q)
= qmn
2φ1(q−n, q−m; 0; q, qz),
i.e. (14.1.4), by the q-Vandermonde theorem, G/R (1.5.3).
Patil and Seshadri’s (1964) theorem, as amended by Menon (1966),
provides a proof of the converse; see also Kagan, Linnik, and Rao (1973).
Let U and V be independent discrete random variables and set c(u, u +
v) = Pr[U = u|U + V = u + v]. If
γ(u, v) = c(u + v, u + v)c(0, v)
c(u, u + v)c(v, v)
= h(u + v)
h(u)h(v)
(14.2.1)
©2001 CRC Press LLC

where h(·) is a nonnegative function, then
f(u) = f(0)h(u)eau,
and
g(v) = g(0)k(v)eav
where
0 ≤f(u) = Pr[U = u], 0 ≤g(v) = Pr[V = v], k(v) = h(v)c(0, v)/c(v, v),
and a is a constant. The proof of the theorem depends on the Cauchy
functional equation.
Suppose that Y ∼(U|U + V = m) has the pmf (14.1.3) for the ab-
sorption distribution and that m = min(m, n). Then
c(u, u + v) = Pr[U = u|U + V = u + v = m]
= q(n−u)v(qv+1; q)u(qn−u+1; q)u
(q; q)u
,
c(u + v, u + v) = Pr[U = u + v|U + V = u + v = m]
= (qn−u−v+1; q)u+v,
c(0, v) = Pr[U = 0|U + V = v = m] = qnv,
c(v, v) = Pr[U = v|U + V = v = m] = (qn−v+1; q)v
and hence
γ(u, v) =
(qn−u−v+1; q)u+v × qnv
q(n−u)v(qv+1; q)u(qn−u+1; q)u{(q; q)u}−1 × (qn−v+1; q)v
= (q−n; q)u+v(−1)u+v
(q; q)u+v
·
(q; q)u
(q−n; q)u(−1)u ·
(q; q)v
(q−n; q)v(−1)v ·
From (14.2.1),
h(u) = (q−n; q)u(−1)u
(q; q)u
;
also
c(0, v)
c(v, v) =
qnv
(qn−v+1; q)v
= (−1)vqv(v−1)/2
(q−n; q)v
·
Hence
Pr[U = u] = Pr[U = 0](q−n; q)u(−1)ueau
(q; q)u
Pr[V = v] = Pr[V = 0]qv(v−1)/2eav
(q; q)v
·
©2001 CRC Press LLC

The pgf’s for U and V are therefore (14.1.6) and (8), respectively, with
θ = ψ = ea = λ, i.e. U has a q-binomial distribution with parameters
(n, λ, q) and support 0, 1, . . . , n whilst V has a Heine distribution with
parameters (λ, q) and support 0, 1, . . ..
14.3
An Application
Consider the number of events per employee (e.g. accidents, absen-
teeisms, thefts, misuses of the Internet), U ∗and V , in two diﬀerent
workplaces that operate independently although they belong to the same
company. Let the distributions of the number of events per person in
the two workplaces have Heine distributions with the same parameters
λ, q. Suppose now that events are underreported in the ﬁrst workplace
with weight function
w(u) = (1 −qn)(1 −qn−1) · · · (1 −qn−u+1)qn(n−u),
u ≥0,
i.e.
w(0) = qn2, w(1) = (1 −qn)qn(n−1), . . ., w(n) = (1 −qn)(1 −
qn−1) . . . (1−q), w(u) = 0 for u > n. The outcome is that low counts are
sometimes ignored and high counts greater than n are ignored altogether.
This is a visibility bias process; it gives rise to the distribution with pmf
Pr[U = u] = w(u) Pr[U ∗= u]/

u≥0
w(u) Pr[U ∗= u],
u = 0, 1, . . . , n.
When the original distribution in the ﬁrst workplace is Heine, then the
outcome distribution has the pmf
Pr[U = u] = C(qn−u+1; q)uqn(n−u)
λuqu(u−1)/2
(q; q)u(−λ; q)∞
,
u = 0, 1, . . . , n,
where
C−1 =
n

u=0
(qn−u+1; q)uqn(n−u)
λuqu(u−1)/2
(q; q)u(−λ; q)∞
=
n

u=0
(q−n; q)u(−λ)uqn2
(q; q)u(−λ; q)∞
= qn2(−λq−n; q)n
(−λ; q)∞
by Heine’s theorem, G/R (1.3.2), giving
Pr[U = u] = λuqu(u−1)/2(qn−u+1; q)uq−nu
(q; q)u(−λq−n; q)n
=
(−λ)x(q−n; q)u
(q; q)u 1φ0(q−n; −; q, −λ),
©2001 CRC Press LLC

where u = 0, 1, . . . , n. The number of reported events in the ﬁrst work-
place, U, therefore has a q-binomial distribution.
If we assume that underreporting occurs only in the ﬁrst workplace,
then the number of reported events in the second workplace, V , has the
Heine distribution with pmf
Pr[V = v] =
qv(v−1)/2λv
(−λ; q)∞(q; q)v
,
v = 0, 1, . . . .
(14.3.1)
Conditional on a given total number of reported events, m, in the two
workplaces, where m ≤n, it follows that the number of reported events
in the ﬁrst workplace, U|U + V = m, has the absorption distribution
with pgf (14.1.4). Note that this is true even when m > n where n is
the parameter of the weight function.
References
1. Benkherouf, L. and Bather, J. A. (1988).
Oil exploration: se-
quential decisions in the face of uncertainty. Journal of Applied
Probability 25, 529–543.
2. Blomqvist, N. (1952).
On an exhaustion process.
Skandinavisk
Aktuarietidskrift 35, 201–210.
3. Borenius, G. (1953). On the statistical distribution of mine explo-
sions. Skandinavisk Aktuarietidskrift 36, 151–157.
4. Dunkl, C. F. (1981). The absorption distribution and the q-binomial
theorem.
Communications in Statistics — Theory and Methods
A10, 1915–1920.
5. Gasper, G. and Rahman, M. (1990). Basic Hypergeometric Series.
Cambridge University Press, Cambridge, England.
6. Johnson, N. L. and Kotz, S. (1969). Discrete Distributions. Hough-
ton Miﬄin, New York.
7. Johnson, N. L., Kotz, S., and Kemp, A. W. (1992). Univariate
Discrete Distributions, Second edition. John Wiley & Sons, New
York.
8. Kagan, A. M., Linnik, Yu. V., and Rao, C. R. (1973). Character-
ization Problems in Mathematical Statistics. John Wiley & Sons,
New York.
9. Kemp, A. W. (1992a). Heine-Euler extensions of the Poisson dis-
tribution. Communications in Statistics — Theory and Methods
21, 571–588.
©2001 CRC Press LLC

10. Kemp, A. W. (1992b). Steady State Markov chain models for the
Heine and Euler distributions. Journa of Applied Probability 29,
869–876.
11. Kemp, A. W. (1998).
Absorption sampling and the absorption
distribution. Journa of Applied Probability, 35, 1–6.
12. Kemp, A. W. and Kemp, C. D. (1991). Weldon’s dice data revis-
ited, American Statistician, 45, 216–222.
13. Kemp, A. W. and Newton, J. (1990). Certain state-dependent pro-
cesses for dichotomised parasite populations. Journal of Applied
Probability 27, 251–258.
14. Menon, M. V. (1966).
Characterization theorems for some uni-
variate probability distributions. Journal of the Royal Statistical
Society, Series B 28, 143–145.
15. Patil, G. P. and Seshadri, V. (1964). Characterization theorems
for some univariate probability distributions. Journal of the Royal
Statistical Society, Series B 26, 286–292.
16. Rawlings, D. (1997). Absorption processes: models for q-identities.
Advances in Applied Mathematics 18, 133–148.
17. Zacks, S. and Goldfarb, D. (1966). Survival probabilities in cross-
ing a ﬁeld containing absorption points. Naval Research Logistics
Quarterly 13, 35–48.
©2001 CRC Press LLC

15
Reﬁnements of Inequalities for Symmetric
Functions
Ingram Olkin
Stanford University, Stanford, CA
ABSTRACT The arithmetic-geometric mean inequality has motivated
many variations and generalizations. One extension, due to Ky Fan, is
an inequality of ratios of arithmetic means versus geometric means. Us-
ing a majorization argument we show that this inequality is a special
case of a general monotonicity result.
Keywords and phrases Majorization, Schur-convex functions, mono-
tonicity of means moments
The arithmetic-geometric mean inequality has perhaps motivated more
variations and extensions than any other inequality. One of these, due
to Ky Fan and reported in Beckenbach and Bellman (1961, p. 5), has
generated a life of its own.
Let x1, . . . , xn be a sequence of positive real numbers in the interval
(0, 1
2], ˜xi = 1 −xi, i = 1, . . . , n, and let the arithmetic, geometric and
harmonic means be denoted by
An =
 xi
n
,
Gn = Πx1/n
i
,
Hn =
n
 x−1
i
,
A′
n =
 ˜xi
n
,
G′
n = Π˜x1/n
i
,
H′
n =
n
 ˜x−1
i
.
The Ky Fan inequality asserts that
Gn
G′n
≤An
A′n
,
(15.1.1)
with equality if and only if x1 = · · · = xn. There are many proofs of
(15.1.1) since it was ﬁrst introduced. Beckenbach and Bellman (1961)
©2001 CRC Press LLC

indicate that (15.1.1) is a good candidate for a proof by forward or
backward induction. Wang and Wang (1984) show further that
Hn
H′n
≤Gn
G′n
,
(15.1.2)
with equality if and only if x1 = · · · = xn. For another extension and
further references see S´andor and Trif (1999).
Inequalities (15.1.1) and (15.1.2) are suggestive of a hierarchy of in-
equalities. Deﬁne the generalized means
M(p) =
 xp
i
n
1/p
,
M ′(p) =
 ˜xp
i
n
1/p
,
(15.1.3)
then (15.1.1) and (15.1.2) state that
M(−1)
M ′(−1) ≤M(0)
M ′(0) ≤M(1)
M ′(1),
(15.1.4)
with equality if and only if x1 = · · · = xn.
Inequalities (15.1.4) are suggestive of the stronger result that
M(p)
M ′(p) is
increasing in p, and we show below that this is indeed the case. In order
to motivate the proof we ﬁrst discuss the proof of the simpler inequality
(15.1.1), as provided in Marshall and Olkin (1979, p. 97–98). The key
ingredients are the majorization ordering and the characterization of the
order-preserving functions, called Schur-convex functions.
DEFINITION 15.1.1
For x, y ∈Rn, x[1] ≥· · · ≥x[n], y[1] ≥· · · ≥y[n], x ≺y (read y
majorizes x) if k
1 x[i] ≤k
1 y[i], k = 1, . . . , n −1, n
1 x[i] = n
1 y[i].
There are a number of characterizations of Schur-convex functions.
One due to Schur in 1923 and enhanced by Ostrowski in 1952 [see Mar-
shall and Olkin (1979, p. 57)] is the following.
PROPOSITION 15.1.2
Let I ⊂R be an open interval and let ϕ : In →R be continuously dif-
ferentiable. Necessary and suﬃcient conditions for ϕ to be Schur-convex
on In are (i) ϕ is symmetric on In, and (ii) ∂ϕ(z)/∂zi is decreasing in
i = 1, . . . , n for all zi ≥· · · ≥zn, z ∈In.
To prove (15.1.1) we ﬁrst show that on the set {z : 0 < zi ≤1
2, i =
1, . . . , n},
©2001 CRC Press LLC

ϕ(x) = Π(1 −xi)1/n
(1 −xi)
 xi
Πx1/n
i
≡An
A′n
G′n
Gn
(15.1.5)
is symmetric and Schur-convex.
The symmetry is immediate. Schur-convexity is obtained by showing
that
∂log ϕ(x)
∂x1
−∂log ϕ(x)
∂x2
= 1
n

1
x2(1 −x2) −
1
x1(1 −x1)

≥0
because [z(1 −z)]−1 is decreasing in z ∈(0, 1
2].
Because Schur-convex functions preserve the majorization ordering,
and because
(x1, . . . , xn) ≻(¯x, . . . , ¯x),
¯x =
 xi
n
,
it follows from (15.1.5) that
ϕ(xi, . . . , xn) ≥ϕ(¯x, . . . , ¯x) = 1,
which is inequality (15.1.1).
The following proposition is a result of Marshall, Olkin, and Proschan
(1967) on the monotonicity of the ratio of means [see also Marshall and
Olkin (1979, p. 130)].
PROPOSITION 15.1.3
If x1 ≥. . . ≥xn > 0, yi > 0 and yi/xi is decreasing in i = 1, . . . , n, and
 xi =  yi, then
x ≡(x1, . . . , xn) ≺(y1, . . . , yn) ≡y.
Furthermore, if wi ≥, 0,  wi = 1, then
gw(r) =
 wixr
i
 wiyr
i
1−r
,
r ̸= 0,
=
Πxwi
i
Πywi
i ,
r = 0
(15.1.6)
is increasing in r.
In the above context if x1 ≥· · · ≥xn > 0 and yi = ˜xi, then yi/xi is
decreasing in i, so that gw(r) is increasing in r. The cases r = −1, 0, 1,
wi = · · · = wn = 1/n yields (15.1.4). The result of Wang and Wang
(1984) with weights also follows from (15.1.6).
©2001 CRC Press LLC

We review the essence of the proof of Proposition 15.1.3. A ﬁrst step is
a characterization of majorization, that if xi > 0 and yi/xi is decreasing
in i = 1, . . . , n, and  xi =  yi, that x is majorized by y:
x = (xi, . . . , xn) ≺(yi, . . . , yn) = y.
If yi/xi is decreasing, then

1
 xr
j

(xr
1, . . . , xr
n) ≺
1
( yr
j) (yr
1, . . . , yr
n) .
(15.1.7)
Because ϕ(x) =  xt
i is Schur-convex for t ≥1, it follows from (15.1.7)
that
 xrt
i
( sr
j)t ≤
 yrt
i
( yr
j)t ,
t ≥1.
(15.1.8)
Let t = s
r, ﬁx r and s such that |s| ≥|r|, rs > 0, then from (15.1.8)
with w1 = . . . = wn, that g(r) ≥g(s) if s ≥r > 0 and g(s) ≥g(r)
if s ≤r < 0. Because g is continuous at 0, we obtain (15.1.6). The
insertion of weights requires an additional argument [see Marshall and
Olkin (1979, p. 131)].
The majorization (15.1.1) shows that other functions could be used to
generate new inequalities. This follows from the fact that for convex g,
1
 g(xi) (g(x1), . . . , g(xn)) ≺
1
 g(yi) (g(y1), . . . , g(yn)) .
A more general continuous version of (15.1.5) can be obtained as fol-
lows.
If F and G are probability distribution functions, ¯F = 1 −F
and ¯G = 1 −G are survival functions, such that F(0) = G(0) = 0 and
¯F −1(p)/ ¯G−1(p) is increasing in p, then
	
xrdG(x)
	
xrdF(x)

1/r
is increasing in r.
Acknowledgements. The author gratefully acknowledges support of
the Alexander von Humboldt Foundation.
References
1. Beckenbach, E. F. and Bellman, R. (1961). Inequalities. Springer-
Verlag, Berlin.
©2001 CRC Press LLC

2. Marshall, A. W. and Olkin, I. (1979). Inequalities: Theory of Ma-
jorization and Its Applications. Academic Press, New York.
3. Marshall, A. W., Olkin, I., and Proschan, F. (1967). Monotonic-
ity of ratios of means and other applications of majorization. In
Inequalities (Ed., O. Shisha), pp. 177–190. Academic Press, New
York.
4. S´andor, J. and Trif, T. (1999). A new reﬁnement of the Ky Fan
inequality. Mathematical Inequalities and Applications 2, 529–533.
5. Wang, W.-L. and Wang, P.-F. (1984). A class of inequalities for
symmetric functions. Acta Mathematica Sinica 27, 485–497. In
Chinese.
©2001 CRC Press LLC

16
General Occupancy Distributions
Ch. A. Charalambides
University of Athens, Athens, Greece
ABSTRACT Consider a supply of balls randomly distributed in n
distinguishable urns and assume that the number R of diﬀerent kinds
of balls allocated in the n urns is a Poisson random variable. Further,
suppose that the number X of balls of any speciﬁc kind distributed in any
speciﬁc urn is a random variable with probability function qx = P(X =
x), x = 0, 1, 2, .... The probability function and factorial moments of the
number K of occupied urns (by at least one ball each), given that R = r
diﬀerent kinds of balls, among which Ry = ry are of multiplicity y for
y = 1, 2, ..., m, with r1+r2+· · ·+rm = r, are distributed in the n urns, are
deduced. They are expressed in terms of ﬁnite diﬀerences of the u-fold
convolution of qx, x = 0, 1, 2, .... Illustrating these results, the cases with
qx, x = 0, 1, 2, ..., zero-one Bernoulli and geometric distributions, or more
generally binomial and negative binomial distributions, are presented.
Keywords and phrases Finite diﬀerences, random occupancy model,
committee problems
16.1
Introduction
In discrete probability theory many random phenomena can be described
in terms of urn models. This is an advantageous approach since urn
models can be easily visualized and are very ﬂexible. In occupancy the-
ory, a collection of m balls of a general speciﬁcation (1r12r2 · · · mrm),
where rj ≥0 is the number of balls of multiplicity j, j = 1, 2, ..., m, with
r1+2r2+· · ·+mrm = m, are randomly distributed in n urns of a general
speciﬁcation (1u12u2 · · · nun), where ui ≥0 is the number of urns of mul-
tiplicity i, i = 1, 2, ..., n, with u1 +2u2 +· · ·+nun = n. Further, the urns
may be of limited or unlimited capacity and the balls in the urns may
be ordered or unordered [cf. MacMahon (1960) and Riordan (1958)].
©2001 CRC Press LLC

Consider the particular case of n distinguishable urns (u1 = n, ui = 0,
i = 2, 3, ..., n) and let Zj be the number of balls distributed in the jth
urn, j = 1, 2, ..., n. The random variables Zj, j = 1, 2, ..., n are called
random occupancy numbers.
The reduction of the joint distribution
of the random occupancy numbers Z1, Z2, ..., Zn to a joint conditional
distribution of independent random variables X1, X2, ..., Xn given that
Sn = X1 + X2 + · · · + Xn = m is a powerful technique in the derivation
and study of the distribution of the number Ki of urns occupied by i balls
each. Barton and David (1959), considering a supply of balls randomly
distributed in n distinguishable urns and assuming that the number X of
balls distributed in any speciﬁc urn (Xj = X, j = 1, 2, ..., n) is a random
variable obeying a Poisson, binomial or negative binomial law, derived
the probability function and the factorial moments of the number K0 of
empty urns, in these three cases, given that Sn = m; these distributions
are the classical, the restricted and the pseudo-contagious occupancy
distributions, respectively. It is worth noticing that the assumption that
X obeys a Poisson law is equivalent to the assumption that the m balls
distributed in the urns are distinguishable. Charalambides (1986, 1997)
derived, in the general case P(X = x) = qx, x = 0, 1, 2, ..., the prob-
ability function and factorial moments of the number K = n −K0 of
occupied urns and more generally of the number Ki of urns occupied by
i balls each, given that Sn = m. Holst (1980), considering a supply of
r diﬀerent kinds of balls randomly distributed in n distinguishable urns
and assuming that the number Xi,j of balls of the ith kind distributed
in the jth urn is a random variable obeying a zero-one Bernoulli law
independent of i, derived a representation of the characteristic function
of the number K0 of empty urns, given that Yi = yi balls of the ith kind
are distributed in the n urns, i = 1, 2, ..., r. This representation is used
to study the asymptotic distribution of K0 when n →∞. Further, the
exact probability function and the factorial moments of K0 given that
Yi = yi, i = 1, 2, ..., r, were deduced. The assumption that Xi,j obeys a
zero-one Bernoulli law is equivalent to the assumption that the capacity
of each urn is limited to one ball from each kind.
In the present paper a general occupancy distribution is derived. More
precisely, a supply of balls randomly distributed in n distinguishable urns
is considered and the number R of diﬀerent kinds of balls distributed in
the n urns is assumed to be a Poisson random variable. Further, it is
assumed that the number X of balls of any speciﬁc kind distributed in
any speciﬁc urn is a random variable with a general probability function
P(X = x) = qx, x = 0, 1, 2, .... Then, the probability function and the
factorial moments of the number K of occupied urns, given that R = r
diﬀerent kinds of balls, among which Ry = ry are of multiplicity y for
y = 1, 2, ..., m, with r1+r2+· · · rm = r, are distributed in the n urns, are
©2001 CRC Press LLC

deduced (Section 16.2). Further, some special occupancy distributions
are presented (Section 16.3).
16.2
A General Random Occupancy Model
Consider a supply of balls randomly distributed in n distinguishable
urns. Assume that the number R of diﬀerent kinds of balls distributed
in the n urns is a Poisson random variable with probability function
P(R = r) = e−λ λr
r! , r = 0, 1, 2, ...(0 < λ < ∞).
(16.2.1)
Further, suppose that the number Xi,j of balls of the ith kind dis-
tributed in the jth urn is a random variable with known probability
function
P(Xi,j = x) = qx, x = 0, 1, 2, ..., j = 1, 2, ..., n.
(16.2.2)
Assuming that the occupancy of each urn is independent of the others,
the probability function of the number Yi = Xi,1 + Xi,2 + · · · + Xi,n
of balls of the ith kind distributed in the n urns, P(Yi = y) = qy(n),
y = 0, 1, 2, ..., i = 1, 2, ..., is given by the sum
qyi(n) =

qxi,1qxi,2 · · · qxi,n, i = 1, 2, ...,
(16.2.3)
where the summation is extended over all integers xi,j ≥0, j = 1, 2, ..., n
such that xi,1 + xi,2 + · · · + xi,n = yi. Also, the total number of balls
distributed in the n urns SR = Y1+Y2+· · ·+YR has a compound Poisson
distribution with probability generating function
PSR(t) = e−λ[1−PY (t)], PY (t) =
∞

y=0
qy(n)ty.
Consider, in addition, the number Ry of those variables among Y1, Y2, ...,
YR that are equal to y, for y = 0, 1, 2, .... Clearly, Ry is the number of
diﬀerent kinds of balls of multiplicity y distributed in the n urns and
∞
y=0 Ry = R. Further, Ry, y = 0, 1, 2, ... are stochastically independent
random variables with Poisson probability function
P(Ry = r) = e−λqy(n) [λqy(n)]r
r!
, r = 0, 1, 2, ..., y = 0, 1, 2, ...
(16.2.4)
and ∞
y=1 yRy = SR [cf Feller (1968, pp. 291–292)].
Note that the
assumption that R is the number of diﬀerent kinds of balls distributed in
©2001 CRC Press LLC

the n urns implies R0 = 0. Also, if the total number of balls distributed
in the n urns is assumed to be SR = m, then Rm+1 = 0, Rm+2 = 0, ....
The inverse is not necessarily true. In the sequel, this stochastic model
is referred as the general random occupancy model.
Under this stochastic model, let K be the number of occupied urns
(by at least one ball each) and
pk ≡pk(r1, r2, ..., rm; r, n)
= P(K = k|R = r,R1 = r1, R2 = r2, ..., Rm = rm),
k = 1, 2, ..., n
(16.2.5)
with r1 + r2 + · · · + rm = r. Also, with (u)j = u(u −1) · · · (u −j + 1)
the (descending) factorial of u of degree j, let
µ(j) ≡µ(j)(r1, r2, ..., rm; r, n)
= E[(K)j|R = r, R1 = r1, R2 = r2, ..., Rm = rm],
j = 1, 2, ....
(16.2.6)
The probability function (16.2.5) and its factorial moments (16.2.6) may
be expressed in terms of ﬁnite diﬀerences of the convolutions (16.2.1).
The derivation of these expressions is facilitated by the following lemma
LEMMA 16.2.1
Consider the general random occupancy model and assume that R = r
diﬀerent kinds of balls, among which Ry = ry are of multiplicity y for
y = 1, 2, ..., m, with r1 + r2 + · · · + rm = r, are randomly distributed
in n distinguishable urns. Let Aj be the event that the jth urn remains
empty, j = 1, 2, ..., n. Then the events A1, A2, ..., An are exchangeable
with
P(Aj1Aj2 · · · Ajk) = qrk
0 [q1(n −k)]r1[q2(n −k)]r2 · · · [qm(n −k)]rm
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
(16.2.7)
for every k-combination {j1, j2, ..., jk} of the n indices {1, 2, ..., n} and
k = 1, 2, ..., n.
PROOF Note ﬁrst that the event {R1 = r1, R2 = r2, ..., Rm = rm} given
the event R = r, with r1 + r2 + · · · + rm = r, in addition to R0 = 0,
implies Rm+1 = 0, Rm+2 = 0, .... Thus, on using (16.2.1) and (16.2.4)
and the independence of Ry, y = 0, 1, 2, ..., it follows that
©2001 CRC Press LLC

P(R1 = r1, R2 = r2, ..., Rm = rm|R = r)
= P(R0 = 0)
m

y=1
P(Ry = ry)
∞

y=m+1
P(Ry = 0)/P(R = r)
=
r!
r1!r2! · · · rm![q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm.
Further, consider the number Zi = Xi,jk+1 + Xi,jk+2 + · · · + Xi,jn of
balls of the ith kind distributed in the n −k urns {jk+1, jk+2, ..., jn},
i = 1, 2, ..., where {jk+1, jk+2, ..., jn} ⊆{1, 2, ..., n}−{j1, j2, ..., jk}. Then
P(Zi = z) = qz(n −k), z = 0, 1, 2, ..., i = 1, 2, .... Assume that Xi,j1 =
Xi,j2 = · · · = Xi,jk = 0, i = 1, 2, ... and let Uz be the number of those
variables among Z1, Z2, ..., ZR that are equal to z, for z = 0, 1, 2, .... The
random variables Uz, z = 0, 1, 2, ... are stochastically independent with
Poisson probability function
P(Uz = u) = e−λqz(n−k) [λqz(n −k)]u
u!
,u = 0, 1, 2, ..., z = 0, 1, 2, ...
and ∞
z=0 Uz = R, ∞
z=1 zUz = SR. Then the probability P(Aj1Aj2 · · ·
Ajk) is equal to the conditional probability of the event
{Xi,js = 0, s = 1, 2, ..., k, i = 1, 2, ..., R, U1 = u1,
U2 = u2, ..., Um = um},
with u1 + u2 + · · · + um = r, given the occurrence of the event
{R1 = r1, R2 = r2, ..., Rm = rm, R = r},
with r1 + r2 + · · · + rm = r, which, in turn, is equal to
P(Xi,js = 0,s = 1, 2, ..., k, i = 1, 2, ..., r,U1 = r1, ..., Um = rm|R = r)
P(R1 = r1, R2 = r2, ..., Rm = rm|R = r)
= qrk
0 [q1(n −k)]r1[q2(n −k)]r2 · · · [qm(n −k)]rm
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
.
Hence (16.2.7) is established.
THEOREM 16.2.2
(a) Under the general random occupancy model, the conditional probabil-
ity pk ≡pk(r1, r2, ..., rm; r, n) that k urns are occupied, given that R = r
©2001 CRC Press LLC

diﬀerent kinds of balls, among which Ry = ry are of multiplicity y for
y = 1, 2, ..., m, with r1 + r2 + · · · + rm = r, are randomly distributed in
n distinguishable urns, k = 1, 2, ..., n, is given by
pk =
 n
k
 [∆k
uqr(n−u)
0
[q1(u)]r1[q2(u)]r2 · · · [qm(u)]rm]u=0
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
.
(16.2.8)
(b) The jth factorial moment µ(j) ≡µ(j)(r1, r2, ..., rm; r, n), j = 1, 2, ...,
of the probability function pk, k = 1, 2, ..., n, is given by
µ(j) = (n)j
[∆j
ugr(n−u)
0
[q1(u)]r1[q2(u)]r2 · · · [qm(u)]rm]u=n−j
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
.
(16.2.9)
PROOF (a) Consider the general random occupancy model and assume
that R = r diﬀerent kinds of balls, among which Ry = ry are of mul-
tiplicity y for y = 1, 2, ..., m, with r1 + r2 + · · · + rm = r, are ran-
domly distributed in n distinguishable urns. Let Aj be the event that
the jth urn remains empty, j = 1, 2, ..., n. The conditional probability
pk ≡pk(r1, r2, ..., rm; r, n) that k urns are occupied is equal to the prob-
ability that exactly n−k among the n events A1, A2, ..., An occur. Thus,
on using the inclusion and exclusion principle and since, by Lemma 2.1,
the events A1, A2, ..., An are exchangeable, it follows that
pk =

n
n −k

n

s=n−k
(−1)s−n+k

k
n −s

P(Aj1Aj2 · · · Ajs)
=
 n
k

k

j=0
(−1)k−j
k
j
 qr(n−j)
0
[q1(j)]r1[q2(j)]r2 · · · [qm(j)]rm
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
.
The last expression implies (16.2.8).
(b) The ith binomial moment
ν(i)
i!
= E
 K0
i

|R = r, R1 = r1, R2 = r2, ..., Rm = rm

of the number K0 of empty urns is equal to the conditional probability
that any i urns remain empty given that R = r diﬀerent kinds of balls,
among which Ry = ry are of multiplicity y for y = 1, 2, ..., m, with
r1 + r2 + · · · + rm = r, are randomly distributed in the n urns. Thus, by
(16.2.7),
ν(i)
i!
=
 n
i
 qri
0 [q1(n −i)]r1[q2(n −i)]r2 · · · [qm(n −i)]rm
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
.
Since
©2001 CRC Press LLC

µ(j) = E[(K)j|R = r, R1 = r1, R2 = r2, ..., Rm = rm]
= E[(n −K0)j|R = r, R1 = r1, R2 = r2, ..., Rm = rm]
= (−1)jE([K0 −n]j|R = r, R1 = r1, R2 = r2, ..., Rm = rm)
= (−1)jν[j](n),
on using the expression [Charalambides (1986)]
ν[j](n) =
j

i=0
(−1)j−i j!
i!
 n −i
j −i

ν(i)
it follows that
µ(j) = (n)j
j

i=0
(−1)i
 j
i
 qri
0 [q1(n −i)]r1[q2(n −i)]r2 · · · [qm(n −i)]rm
[q1(n)]r1[q2(n)]r2 · · · [qm(n)]rm
.
The last expression implies (16.2.9).
An interesting corollary of Theorem 16.2.1 is deduced when it is as-
sumed that R = r and Rs = r, Ry = 0, y ̸= s.
COROLLARY 16.2.3
(a) Under the general random occupancy model, the conditional proba-
bility pk(s; r, n) that k urns are occupied, given that s balls from each of
r kinds are distributed in the n distinguishable urns, k = 1, 2, ..., n, is
given by
pk(s; r, n) =
 n
k
 [∆k
uqr(n−u)
0
[qs(u)]r]u=0
[qs(n)]r
.
(16.2.10)
(b) The jth factorial moment µ(j)(s; r, n), j = 1, 2, ..., of the probability
function pk(s; r, n), k = 1, 2, ..., n, is given by
µ(j)(s; r, n) = (n)j
[∆j
ugr(n−u)
0
[qs(u)]r]u=n−j
[qs(n)]r
.
©2001 CRC Press LLC

16.3 
Sp ecial Occupancy Distributions
16.3.1 
Geometric Probabilities
Assume that the number X of balls of any speciﬁc kind distributed in
any speciﬁc urn obeys a geometric distribution with
qx = P(X = x) = pqx, x = 0, 1, 2, ... (q = 1 −p, 0 < p < 1).
Its u-fold convolution obeys a negative binomial distribution with
qy(u) = P(Y = y) =
 u + y −1
y

puqy, y = 0, 1, 2, ....
Then, according to Lemma 16.2.1, the conditional probability that k
speciﬁed urns remain empty given that R = r diﬀerent kinds of balls,
among which Ry = ry are of multiplicity y for y = 1, 2, ..., m, with
r1 + r2 + · · · + rm = r, are randomly distributed in n distinguishable
urns, k = 1, 2, ..., n, is given by
P(Aj1Aj2 · · · Ajk) =
m

y =1
 n −k + y −1
y
ry
m

y =1
 n + y −1
y
ry
.
Therefore, the general random occupancy model, with the assumption
of geometric probabilities, is equivalent to the occupancy model, where
a ﬁxed number m of balls of the general speciﬁcation (1r12r2 · · · mrm),
with r1 + 2r2 + · · · + mrm = m, r1 + r2 + · · · + rm = r, are randomly
distributed in n distinguishable urns [cf Riordan (1958, Chapter 5)].
Thus, by Theorem 16.2.2, the occupancy probability function
pk = P(K = k|R = r, R1 = r1, R2 = r2, ..., Rm = rm), k = 1, 2, ..., n
is given by
pk =
 n
k

	
∆k
u
m

y=1
u + y −1
y
ry
u=0
m

y=1
n + y −1
y
ry
=
 n
k

k

j=0
(−1)k−j
k
j

m

y=1
j + y −1
y
ry
m

y=1
n + y −1
y
ry .
(16.3.1)
©2001 CRC Press LLC

The closely related occupancy probability function of the number K0 of
empty urns, under the occupancy model where a ﬁxed number m of balls
of the general speciﬁcation (1r12r2 · · · mrm), with r1 +2r2 +· · ·+mrm =
m, r1 + r2 + · · · + rm = r, are randomly distributed in n distinguishable
urns, was derived by Riordan (1958, p. 96).
The jth factorial moment
µ(j) = E[(K)j|R = r, R1 = r1, R2 = r2, ..., Rm = rm],j = 1, 2, ...
is given by
µ(j) = (n)j
	
∆j
u
m

y=1
u + y −1
y
ry
u=n−j
m

y=1
n + y −1
y
ry
= (n)j
j

i=0
(−1)j−i
j
i

m

y=1
n −j + i + y −1
y
ry
m

y=1
n + y −1
y
ry
. (16.3.2)
In particular, by Corollary 16.2.3, the conditional probability pk(s; r, n),
k = 1, 2, ..., n, that k urns are occupied, given that s balls from each of
r kinds are distributed in n distinguishable urns, is given by
pk(s; r, n) =
n
k


∆k
u
u + s −1
s
r
u=0
n + s −1
s
r
=
n
k

k

j=0
(−1)k−j
k
j

j + s −1
s
r
n + s −1
s
r .
(16.3.3)
The jth factorial moment µ(j)(s; r, n), j = 1, 2, ..., n, of this distribution
is
µ(j)(s; r, n) = (n)j

∆j
u
u + s −1
s
r
u=n−j
n + s −1
s
r
,
©2001 CRC Press LLC

= (n)j
j

i=0
(−1)j−i
j
i

n −j + i + s −1
s
r
n + s −1
s
r
.
(16.3.4)
The following two remarks concerning extensions of and/or imposition
of restrictions on this random occupancy model are worth noting. The
replacement of the assumption that the number X obeys a geometric
distribution by the assumption that it obeys the more general negative
binomial distribution with
qx = P(X = x) =
 s + x −1
x

psqx, x = 0, 1, 2, ...,
divides each urn into s distinguishable compartments. In this case
qy(u) = P(Y = y) =
 su + y −1
y

psuqy, y = 0, 1, 2, ...
and
pk =
 n
k

	
∆k
u
m

y=1
su + y −1
y
ry
u=0
m

y=1
sn + y −1
y
ry
=
 n
k

k

j=0
(−1)k−j
k
j

m

y=1
su + y −1
y
ry
m

y=1
sn + y −1
y
ry
(16.3.5)
with
µ(j) = (n)j
	
∆j
u
m

y=1
su + y −1
y
ry
u=n−j
m

y=1
sn + y −1
y
ry
= (n)j
j

i=0
(−1)j−i

j
i

m

y=1
s(n −j + i) + y −1
y
ry
m

y=1
sn + y −1
y
ry
.
(16.3.6)
©2001 CRC Press LLC

Further, a restriction on the capacity of each urn or each compartment
can be expressed by choosing a suitable truncated version of the distri-
bution of X. Thus, the assumption of geometric probabilities truncated
to the right at the point s, that is, with
qx = P(X = x) = (1 −qs+1)−1pqx, x = 0, 1, 2, ..., s,
whence
qy(u) = P(Y = y) = L(y, u, s)(1 −qs+1)−upuqy, y = 0, 1, 2, ..., su
where
L(y, u, s) =
u

j =0
(−1)j
 u
j
  u + y −j(s + 1) −1
u −1

,
imposes the restriction that the capacity of each urn is limited to s
balls from each kind. The corresponding occupancy distribution can be
similarly deduced.
Example 1
Formation of committees from grouped candidates. Assume that there
are m vacant positions on r committees. More speciﬁcally ry ≥0 com-
mittees have y vacant positions, y = 1, 2, ..., m, so that r1 + 2r2 + · · · +
mrm = m and r1 + r2 + · · · + rm = r. Further, assume that for these m
positions there are m priority-ordered nominations by each of n diﬀerent
groups. In a random selection of the m members of the r committees,
ﬁnd the probability that k of the n groups are represented on at least
one committee [cf. Johnson and Kotz (1977, Chapter 3)].
The m vacant positions on the r committees may be considered as m
balls of the speciﬁcation (1r12r2 · · · mrm), with r1 + 2r2 + · · · + mrm =
m, r1 + r2 + · · · + rm = r, and the n diﬀerent groups of candidates
as n distinguishable urns.
Thus, the placement of a ball of the ith
kind to the jth urn corresponds to the selection of the jth group to
occupy a position on the ith committee. Since it is assumed that the
nominations are priority-ordered, the selection of the jth group uniquely
determines the candidate who occupies a position on the ith committee.
Further, there is no restriction on the number of balls of same kind that
each urn may accommodate. Therefore the probability that k of the n
groups are represented on at least one committee is given by (16.3.1).
In the particular case of the formation of r committees each of size s,
the probability that k of the n groups are represented on at least one
committee is given by (16.3.3).
©2001 CRC Press LLC

16.3.2
Bernoulli Probabilities
Assume that the number X of balls of any speciﬁc kind distributed in
any speciﬁc urn obeys a zero-one Bernoulli distribution with
qx = P(X = x) = pxq1−x, x = 0, 1(q = 1 −p, 0 < p < 1).
Its u-fold convolution obeys a binomial distribution with
qy(u) = P(Y = y) =
 u
y

pyqu−y, y = 0, 1, ..., u.
Then, according to Lemma 16.2.1, the conditional probability that k
speciﬁed urns remain empty given that R = r diﬀerent kinds of balls,
among which Ry = ry are of multiplicity y for y = 1, 2, ..., m, with
r1 + r2 + · · · + rm = r, are randomly distributed in n distinguishable
urns, k = 1, 2, ..., n, is given by
P(Aj1Aj2 · · · Ajk) =
m

y=1
 n −k
y
ry
m

y=1
 n
y
ry
.
Therefore, the general random occupancy model, with the assumption
of Bernoulli probabilities, is equivalent to the occupancy model where
a ﬁxed number m of balls of the general speciﬁcation (1r12r2 · · · mrm),
with r1 + 2r2 + · · · + mrm = m, r1 + r2 + · · · + rm = r, are randomly
distributed in n distinguishable urns, each of capacity limited to one ball
from each kind.
Thus, by Theorem 16.2.2, the occupancy probability function is given
by
pk =
 n
k

	
∆k
u
m

y=1
u
y
ry
u=0
m

y=1
n
y
ry
=
 n
k

k

j=0
(−1)k−j
k
j

m

y=1
 j
y
ry
m

y=1
n
y
ry .
(16.3.7)
Its jth factorial moment is given by
©2001 CRC Press LLC

µ(j) = (n)j
	
∆j
u
m

y=1
u
y
ry
u=n−j
m

y=1
n
y
ry
= (n)j
j

i=0
(−1)j−i
j
i

m

y=1
n −j + i
y
ry
m

y=1
n
y
ry
.
(16.3.8)
In particular, by Corollary 16.2.3, the conditional probability pk(s; r, n)
that k urns are occupied, given that s balls from each of r kinds are dis-
tributed in n distinguishable urns, each of capacity limited to one ball
from each kind, is given by
pk(s; r, n) =
n
k


∆k
u
u
s
r
u=0
n
s
r
=
n
k

k

j=0
(−1)k−j
k
j

j
s
r
n
s
r
(16.3.9)
and
µ(j)(s; r, n) = (n)j

∆j
u
u
s
r
u=n−j
n
s
r
= (n)j
j

i=0
(−1)j−i
j
i

n −j + i
s
r
n
s
r
.
(16.3.10)
As in the case of the random occupancy model with geometric prob-
abilities, extensions of and/or imposition of restrictions on this random
occupancy model can be achieved by assuming that X obeys a binomial
©2001 CRC Press LLC

or a suitable truncated binomial distribution. Notice that the assump-
tion that X obeys the binomial distribution
qx =
 s
x

pxqs−x, x = 0, 1, 2, ..., s(q = 1 −p, 0 < p < 1)
corresponds to the assumption that each urn is divided into s distin-
guishable compartments each with capacity limited to one ball. In this
case
qy(u) =
 su
y

pyqsu−y, y = 0, 1, 2, ..., su
and
pk =
 n
k

	
∆k
u
m

y=1
su
y
ry
u=0
m

y=1
sn
y
ry
=
 n
k

k

j=0
(−1)k−j
k
j

m

y=1
sj
y
ry
m

y=1
sn
y
ry ,
(16.3.11)
with
µ(j) = (n)j
	
∆j
u
m

y=1
su
y
ry
u=n−j
m

y=1
sn
y
ry
= (n)j
j

i=0
(−1)j−i
j
i

m

y=1
s(n −j + i)
y
ry
m

y=1
sn
y
ry
.
(16.3.12)
Example 2
Formation of committees from individual candidates.
As in Example
16.3.1, assume that there are m vacant positions on r committees, among
which ry ≥0 committees have y vacant positions, y = 1, 2, ..., m, so that
r1+2r2+· · ·+mrm = m and r1+r2+· · ·+rm = r. Further, assume that
for these m positions there are n individual nominations. In a random
©2001 CRC Press LLC

selection of the m members of the r committees, ﬁnd the probability
that k of the n candidates will participate to at least one committee [cf
Johnson and Kotz (1977, Chapter 3)].
As in Example 16.3.1, the m vacant positions on the r committees
may be considered as m balls of the speciﬁcation (1r12r2 · · · mrm), with
r1 + 2r2 + · · · + mrm = m, r1 + r2 + · · · + rm = r, and the n individual
candidates as n distinguishable urns. Thus, the placement of a ball of the
ith kind to the jth urn corresponds to the selection of the jth candidate
for a position on the ith committee. We assume that no candidate can
be selected to more than one positions of the same committee, while it
can participate in more than one committee. This assumption implies
that each cell may accommodate at most one object from each kind.
Consequently, the probability that k of the n candidates will participate
to at least one committee is given by (16.3.7). In the particular case
of the formation of r committees each of size s, the probability that k
of the n candidates are represented on at least one committee is given
by (16.3.9). It is worth noticing that this committee problem can be
rephrased as a multiple capture-recapture problem [cf Holst (1980)]. The
probability function of the number of diﬀerent individuals observed in
the census, which was also derived by Berg (1974), is given by (16.3.7).
Example 3
Formation of committees from grouped candidates with unordered nom-
inations. As in Example 16.3.1, assume that there are m vacant po-
sitions on r committees, among which ry ≥0 committees have y va-
cant positions, y = 1, 2, ..., m, so that r1 + 2r2 + · · · + mrm = m and
r1 + r2 + · · · + rm = r.
Further, suppose that for the m positions there are s unordered nomi-
nations by each of n diﬀerent groups. In a random selection of the m
members of the r committees, ﬁnd the probability that k of the n groups
are represented on at least one committee.
In this case the n diﬀerent groups each with s nominations may be
considered as n distinguishable urns each with s distinguishable com-
partments of capacity limited to one ball. Then the probability that k
of the n groups are represented on at least one committee is given by
(16.3.11).
Acknowledgement This research was partially supported by the Uni-
versity of Athens Research Special Account under grant 70/4/3406.
©2001 CRC Press LLC

References
1. Barton, D. E. and David, F. N. (1959). Contagious occupancy.
Journal of the Royal Statistical Society, Series B 21, 120–130.
2. Berg, S. (1974). Factorial series distributions with applications to
capture-recapture problems. Scandinavian Journal of Statistics 1,
145–152.
3. Charalambides, Ch. A. (1986). Derivation of probabilities and mo-
ments of certain generalized discrete distributions via urn models.
Communications in Statistics — Theory and Methods 15, 677–696.
4. Charalambides, Ch. A. (1997). A uniﬁed derivation of occupancy
and sequential occupancy distributions. In Advances in Combina-
torial Methods and Applications to Probability and Statistics (Ed.
N. Balakrishnan), pp. 259–273. Birkh¨auser, Boston.
5. Feller, W. (1968). An Introduction to Probability Theory and its
Applications, Volume 1, Third Edition. John Wiley & Sons, New
York.
6. Holst, L. (1980). On matrix occupancy, committee, and capture-
recapture problems. Scandinavian Journal of Statistics 7, 139–146.
7. Johnson, N. L. and Kotz, S. (1977). Urn Models and their Appli-
cation. John Wiley & Sons, New York.
8. MacMahon, P. A. (1960). Combinatory Analysis, Volumes 1 and
2. Chelsea, New York.
9. Riordan, J. (1958).
An Introduction to Combinatorial Analysis.
John Wiley & Sons, New York.
©2001 CRC Press LLC

17
A Skew t Distribution
M. C. Jones
The Open University, Milton Keynes, UK
ABSTRACT I ﬁrst consider the formulation and properties of a new
skew family of distributions on the real line which includes the (symmet-
ric) Student t distributions as special cases. The family can be obtained
through a simple generalization of a representation of the t distribution
in Cacoullos (1965). I go on to explore brieﬂy two ways in which bivariate
distributions with skew t marginals can be developed.
Keywords and phrases Beta distribution, Dirichlet distribution, mul-
tivariate t and beta distributions, skewness, Student t distribution
17.1
Introduction
Before I embark on the paper proper, I must record my pleasure at being
able to attend and enjoy the conference and to contribute to this volume
in honour of Professor Cacoullos. Quite a lot of my own work has built
on seminal papers of Professor Cacoullos from the 1960s. In particular,
given my own emphasis over the years on kernel smoothing methodol-
ogy [e.g. Wand and Jones (1995)], the reader might have expected me
to take as my starting point Cacoullos (1966), the ﬁrst paper on multi-
variate kernel density estimation. However, the current paper actually
has links with a quite diﬀerent source, namely Cacoullos (1965). (I am
pleased that Professor Politis spoke at the conference on multivariate
kernel density estimation!).
A fuller title for this paper might be “A New Univariate Skew t Dis-
tribution and Some Bivariate Extensions”. Its primary purpose is to
provide a family of distributions with support the whole real line that
includes members with nonzero skewness but also includes the symmet-
ric Student t distributions as special cases. The reasons for this exercise
are twofold: to provide distributions with both skewness and heavy tails
©2001 CRC Press LLC

for use in robustness studies (both frequentist and Bayesian); and to
provide further models for data. The second purpose of the paper is to
outline brieﬂy two extensions of the skew t distribution to the bivariate
case.
At the time of the conference, a fuller account of the new skew t family
could be found in my unpublished manuscript “A Skew Extension of the
t Distribution”. Since then, it has been decided — in collaboration with
a colleague — to add a number of things to that paper, particularly to
do with inference and a number of data applications of the distribution,
and the new, in preparation, version will be referred to as Jones and
Faddy (2000).
A plan of the current paper is as follows. The new (univariate) skew
t distribution will be derived in Section 17.2. Some of its properties will
be presented in Section 17.3. A ﬁrst bivariate extension of the skew t
distribution will be outlined in Section 17.4 and a second, quite diﬀerent,
bivariate extension will be outlined in Section 17.5. The ﬁgures in the
paper were prepared using the uniras graphics system in fortran.
17.2
Derviation of Skew t Density
Let us proceed by analogy with the beta distribution. Consider as a
starting point the symmetric beta distribution on [−1, 1]. (Readers more
familiar with the beta distribution residing on [0, 1] simply need to apply
a linear transformation to change support.) The symmetric beta distri-
bution is one in which the two parameters of the Beta(a, b) distribution
are equal (to a, say). The distribution is symmetric about zero, with
density proportional to (1−x2)a−1. (In fact, it forms a family of kernels
for univariate kernel estimation!)
Now, factorise 1 −x2 into (1 + x)(1 −x).
Power each factor up.
But why should each factor have the same power? Why not give each
factor a diﬀerent power? Doing so gives a density proportional to (1 +
x)a−1(1 −x)b−1 which is precisely the usual beta distribution on [−1, 1].
We have succeeded in going from a symmetric starting point to a more
general skew family encompassing the symmetric distribution by a little
mathematical sleight of hand! But we have not yet succeeded in coming
up with something new.
However, we can apply much the same trick to the t distribution.
In our teaching, we tend to shy away from giving the t density, and
this prompted one Open University student to state that it must be so
horriﬁc that we don’t write it down! However, as the beta distribution
is essentially based on 1 −x2, so the horriﬁc t distribution is essentially
©2001 CRC Press LLC

based on 1+x2. In fact, I will write the t density on 2a degrees of freedom
(for convenience) as being proportional to (1 + x2/(2a))−(a+1/2), x ∈ℜ.
Now factorise this. No, I shan’t venture into complex numbers. Instead,
write

1 + x2
2a
−1
= 1 −
x2
2a + x2
which factorises into

1 +
x
√
2a + x2
 
1 −
x
√
2a + x2

.
Each of these can be given diﬀerent powers a + 1/2 and b + 1/2 so that,
with a simple rescaling taking 2a to a + b for clarity, we have a density
f(x; a, b) proportional to

1 +
x
√
a + b + x2
a+1/2 
1 −
x
√
a + b + x2
b+1/2
.
(17.2.1)
This is the proposal for a skew t distribution. Its normalising factor is
tractable and turns out to be
{B(a, b)
√
a + b 2a+b−1}−1
where B(·, ·) is the beta function. When a = b, f reduces to the t distri-
bution on 2a degrees of freedom. Note also that f(x; b, a) = f(−x; a, b).
Again, this all seems to be just so much mathematical legerdemain.
Well, there is an alternative way of deriving distribution (17.2.1). It is
by transformation of a beta random variable. To this end, if B has the
Beta(a, b) distribution on [−1, 1], then
T =
√
a + b B
√
1 −B2
(17.2.2)
has the skew t distribution (17.2.1).
And this is the link with Cacoullos (1965) because formula (17.2.2)
can also be written in terms of a pair (X, Y ) of independent χ2 random
variables with 2a and 2b degrees of freedom, respectively:
T =
√
a + b (X −Y )
2
√
XY
.
(17.2.3)
Cacoullos (1965) is the paper setting out the special case of this relation-
ship between a symmetric t random variable and a pair of independent
©2001 CRC Press LLC

chi-squares with the same degrees of freedom, say 2a, or equivalently
by writing F = X/Y which has the F distribution on 2a, 2a degrees of
freedom and of which the right-hand side of (17.2.3) is a simple func-
tion. Cacoullos (1965) remains very important as the paper setting out
the relationship in the symmetric case, although it may well have been
known to others beforehand. In a corrigendum to the paper, Professor
Pratt, the journal editor, remarks “The editor is beginning to think he
and the six or so experts he consulted are alone in not having been aware
of this relation”. I hope that I and the few people who have heard me
talk on this subject are not alone in not being aware of the generalised
relationship (17.2.3) and hence the skew t distribution!
Cacoullos (1999) generalises his 1965 paper in a diﬀerent way.
17.3 
Prop erties of Skew t Distribution
Figure 17.1 shows a variety of skew t densities each with b = 2. These
densities have been standardised to have zero mean and unit variance.
The symmetric density belongs to the Student t distribution on 2b =
4 degrees of freedom.
The other six densities, displaying increasing
amounts of skewness, correspond to a doubling of a (which was initially
2) each time.
The moments of the skew t distribution are available in closed form
and a general formula for E(T r), r = 1, 2, ..., is given in Jones and Faddy
(2000). The rth moment exists provided both a and b exceed r/2. The
mean and variance (used in standardisation above) are
E(T) = (a + b)1/2(a −b)
2
Γ(a −1/2)Γ(b −1/2)
Γ(a)Γ(b)
,
where Γ(·) is the gamma function, and
V (T) = (a + b)
4
{(a −b)2 + (a −1) + (b −1)}
(a −1)(b −1)
−
(a −b)Γ(a −1/2)Γ(b −1/2)
Γ(a)Γ(b)
2
.
An expression for the skewness can also be written down.
Straightforward diﬀerentiation shows that the skew t density is always
unimodal with mode at
(a −b)
√
a + b
√2a + 1
√
2b + 1.
©2001 CRC Press LLC

FIGURE 17.1
Densities (17.2.1), standardised to have zero mean and unit
variance, for a = 2i b, i = 0, ...,  6, having increasing amounts of
skewness with i, in the case b = 2
Many (but not all) aspects of the properties of the skew t distribution
depend on a and b through ν = a + b and λ = a −b. It is tempting to
reparametrise the skew t distribution in terms of ν and λ which have ap-
pealing interpretations as ‘essential’ degrees of freedom and a parameter
controlling skewness, respectively, although I shall not do so here.
It is suggested by Figure 17.1 that the densities converge to a limiting
case as a →∞with b held ﬁxed. In fact, this obtains provided the mean
and variance, which behave as a and a2, respectively, as a increases,
are standardised away. Jones and Faddy (2000) show that this limiting
distribution is that of

2/Z where Z follows the chi-squared distribution
on 2b degrees of freedom. It is reasonable to think of this distribution as
displaying the greatest skewness that a skew t distribution can possess:
see Jones and Faddy (2000) for more on this skewness. If b is allowed to
grow to ∞with a, apparently in any fashion, the limiting distribution is
the normal (as is clearly the case when a = b →∞).
Further properties of the skew t distribution are immediate because of
its link to the beta distribution. For example, random variates can read-
ily be produced, the skew t distribution function can be written in terms
of the beta distribution function, and there are further transformation
links with other distributions. See Jones and Faddy (2000, Section 17.5).
©2001 CRC Press LLC

17.4
A First Bivariate Skew t Distribution
This section concerns the ﬁrst of two suggestions for bivariate distri-
butions with skew t marginals which I shall brieﬂy pursue. This ﬁrst
bivariate extension actually works in the general multivariate case (al-
though it is not clear whether it is of much practical importance in three
or more dimensions) and is part of a wider approach to building multi-
variate distributions with given marginals that I explore elsewhere, Jones
(2000).
The general, very simple, idea is that of replacing a marginal distri-
bution of a given multivariate distribution with a desired marginal by
multiplication i.e. really, of replacing the marginal distribution when rep-
resenting the multivariate distribution by the product of marginal and
conditional distributions. Specialising to the bivariate case, if f(x, y) is
a bivariate density with X-marginal density fX, say, then
f1(x, y) = g(x)f(x, y)/fX(x)
(17.4.1)
will have X-marginal g instead of fX (provided the support of g is a
subset of the support of fX).
Now take f to be the density of the spherically symmetric bivariate t
distribution on ν degrees of freedom, say; f ∝(1+ν−1(x2+y2))−(ν/2+1).
Replace its X-marginal fX, which, like its Y -marginal, is the ordinary
t distribution on ν degrees of freedom, by multiplying by g taken to be
the density of the skew t distribution with parameters a and c, say, and
dividing by fX. The result is the distribution with density
Γ((ν + 2)/2)
Γ((ν+1)/2)B(a,c)(a+c)1/22a+c−1√νπ
×
(1 + ν−1x2)(ν+1)/2 
1 +
x
(a+c+x2)1/2
	a+1/2 
1 −
x
(a+c+x2)1/2
	c+1/2
(1 + ν−1(x2 + y2))ν/2+1
.
(17.4.2)
Here, a, c and ν are all positive.
Distribution (17.4.2) has a number of interesting properties. First,
it has a skew t X-marginal distribution by construction. Second, the
conditional distribution of Y given X = x is the symmetric t distribution
on ν + 1 degrees of freedom, rescaled by a factor of {(ν + 1)−1(x2 +
ν)}1/2; this property is shared with the original symmetric bivariate
t distribution.
Third, the Y -marginal of distribution (17.4.2) is also
symmetric. Note that it is not itself a t distribution. However, in many
©2001 CRC Press LLC

FIGURE 17.2
The bivariate density (17.4.2) with a = 13, ν = 10 and c = 7
cases, this marginal is well approximated by a t distribution with degrees
of freedom given by matching variances with the Y -marginal distribution
[Jones (2000)]. Fourth, distribution (17.4.2) retains the zero correlation
of the underlying symmetric bivariate t distribution.
In fact, it also
retains the same local dependence function [Holland and Wang (1987)
and Jones (1996)] as the underlying bivariate t distribution. [All these
properties are special instances of properties consequent on applying
(17.4.1) to a general spherically symmetric f, Jones (2000).]
An example of density (17.4.2) is shown in Figure 17.2. It has a = 13,
ν = 10 and c = 7.
This means it has a t(13, 7) X-marginal, scaled
symmetric t on 11 degrees of freedom Y |X conditionals, and a symmetric
Y -marginal well approximated by a t distribution on about 6 degrees of
freedom.
17.5
A Second Bivariate Skew t Distribution
An alternative suggestion for a bivariate skew t distribution, this time
with both marginals skew t, is the subject of this ﬁnal section.
The
paper containing fuller details on this topic is not yet in a very advanced
state of preparation and so does not merit a proper citation.
The starting point here is the bivariate Dirichlet distribution which has
three nonnegative parameters a, b, and c and sample space the triangle
©2001 CRC Press LLC

FIGURE 17.3
The bivariate density (17.5.2) with a = 1, b = 2 and c = 3
u > 0, v > 0, u + v < 1. This has beta marginals, speciﬁcally U ∼
Beta(a, b + c) and V ∼Beta(b, a + c), here dwelling on the more usual
space [0, 1]. Note that at most one of these two beta marginals can be
symmetric.
Now make the “usual” transformations to the marginals of this distri-
bution to make them into skew t distributions! In fact, by setting B in
(17.2.2) equal to 2U −1 and −(2V −1), in turn (recall that B has support
[−1, 1] and U, V have support [0, 1]), things are set up in a particularly
convenient way:
X =
√
d(2U −1)
2

U(1 −U)
,
Y =
√
d(1 −2V )
2

V (1 −V )
.
(17.5.1)
Here, d = a + b + c. X and Y have joint density
dΓ(d + 1)
2d−1Γ(a)Γ(b)Γ(c)

1 +
x
√
d + x2
a−1 
1 −
y

d + y2
b−1
×
1
(d + x2)3/2(d + y2)3/2

y

d + y2 −
x
√
d + x2
c−1
.(17.5.2)
This has support — and this is not immediately obvious — the interest-
ing sample space y > x.
©2001 CRC Press LLC

In direct correspondence with the beta marginals of the Dirichlet dis-
tribution, the skew t marginals of distribution (17.5.2) are t(a, b+c) and
t(b, a + c), respectively, and again at most one of these two marginals
can be symmetric. An example of distribution (17.5.2) is given in Figure
17.3. This has a = 1, b = 2 and c = 3; its marginals are therefore t(1, 5)
and t(2, 4), respectively. (Actually, the distribution’s location has been
shifted for convenience.)
Fuller properties of this skew t-Dirichlet distribution are being inves-
tigated for publication elsewhere.
References
1. Cacoullos, T. (1965). A relation between t and F-distributions.
Journal of the American Statistical Association 60, 528-531; Cor-
rigendum, 60, 1249.
2. Cacoullos, T. (1966). Estimation of a multivariate density. Annals
of the Institute of Statistical Mathematics 18, 179-189.
3. Cacoullos, T. (1999). On the Pitman-Morgan-Wilks tests of ho-
moscedasticity for the bivariate normal distribution. To appear.
4. Holland, P. W. and Wang, Y. J. (1987). Dependence function for
continuous bivariate densities.
Communications in Statistics —
Theory and Methods 16, 863-876.
5. Jones, M. C. (1996). The local dependence function. Biometrika
83, 899-904.
6. Jones, M. C. (2000). Marginal replacement in multivariate den-
sities, with application to skewing spherically symmetric distribu-
tions. To appear.
7. Jones, M. C. and Faddy, M. J. (2000). A skew extension of the t
distribution, with applications. In preparation.
8. Wand, M. P. and Jones, M. C. (1995). Kernel Smoothing. Chap-
man and Hall, London.
©2001 CRC Press LLC

18
On the Posterior Moments for Truncation
Parameter Distributions and
Identiﬁability by Posterior Mean for
Exponential Distribution with Location
Parameter
Yimin Ma and N. Balakrishnan
McMaster University, Hamilton, ON
ABSTRACT The exact analytical expressions of posterior moments
for the two general truncation parameter likelihood functions with ar-
bitrary prior distributions are obtained by using the suﬃcient statistics
for these truncation parameter distributions. In particular, the explicit
forms for posterior mean and variance are given. Some examples are also
discussed to illustrate the obtained results. Next, an explicit expression
for mixing distribution in terms of posterior mean for exponential distri-
bution with location parameter is obtained and then the identiﬁability
by posterior mean is established. As an example, the gamma mixing
distribution family is used to illustrate the results obtained.
Keywords and phrases Posterior moments, prior distributions, suﬃ-
cient statistics, truncation parameters, exponential distribution, identi-
ﬁability, location parameter, mixing distribution, posterior mean
18.1
Introduction
Let ℓ(y|θ) be the likelihood function of an independent and identically
distributed sample y = (x1, . . . , xn) from distribution p(x|θ), then the
Bayes estimator and posterior risk, under squared error loss, are poste-
rior mean E(θ|y) and posterior variance var(θ|y) respectively. For the
normal likelihood function with known variance and an arbitrary prior
distribution, the explicit expressions for the posterior mean and variance
©2001 CRC Press LLC

are derived by Pericchi and Smith (1992). For an arbitrary location pa-
rameter likelihood function and the normal prior distribution, the exact
form of posterior mean is given by Polson (1991). Pericchi, Sans´o, and
Smith (1993) also discussed the posterior cumulant relations in Bayesian
inference assuming the exponential family forms either on the likelihood
or on the prior. They all mentioned that analytical Bayesian computa-
tions without the assumption of normality, either on likelihood function
or on prior distribution, are very diﬃcult and needed investigation.
In this paper, we consider the likelihood functions of random samples
from the following two diﬀerent types of truncation parameter distribu-
tions:
Type I truncation parameter density
p1(x1 |θ1) = h1(x1)/k1(θ1),
a < θ1 ≤x1 < b; 
(18.1.1)
Type II truncation parameter density
p2(x2 |θ2) = h2(x2)/k2(θ2),
a < x2 ≤θ2 < b,  
(18.1.2)
where h1(x1) and h2(x2) are positive, continuous and integrable over
(θ1, b) and (a, θ2), respectively, for θi, i = 1, 2, in the interval (a, b),
−∞≤a < b ≤∞.
In the ﬁrst part of this paper, we ﬁnd the exact analytical expressions
of posterior moments for the two diﬀerent truncation parameter likeli-
hood functions with arbitrary prior distributions. In Section 18.2, the
explicit forms for posterior moments are derived by using the suﬃcient
statistics for these truncation parameter distributions.
In particular,
the explicit expressions of posterior mean and variance are given, re-
spectively, for these two truncation parameter distribution models. In
Section 18.3, two examples representing the two diﬀerent truncation pa-
rameter distribution models are discussed to illustrate the obtained re-
sults.
Next, suppose that (X, θ) is a random vector in which the conditional
distribution of X given θ is f(x|θ) and θ has a mixing (prior) distribution
G(θ), then the mixture (marginal) distribution of X is given by
fG(x) =

f(x|θ)dG(θ).
The original concept of identiﬁability means that there is a one-to-one
correspondence between the mixing distribution G and the mixture dis-
tribution fG; see Teicher (1961, 1963) for a general introduction.
In
recent years, however, there are some other research works about the
©2001 CRC Press LLC

one-to-one correspondence between the mixing distribution G and the
posterior mean E(θ|x), instead of the mixture distribution fG; most of
these papers have dealt with diﬀerent distributions from the exponen-
tial distribution family. For example, see Korwar (1975), Cacoullos and
Papageorgiou (1984), Papageorgiou (1985), Cacoullos (1987), Kyriak-
oussis and Papageorgiou (1991), Arnold, Castillo, and Sarabia (1993),
Papageorgiou and Wesolowski (1997), and some others. Exception is the
paper by Gupta and Wesolowski (1997), who have established not only
the identiﬁability but also an explicit relationship between the mixing
distribution and the posterior mean for the uniform distribution.
In the second part of this paper, we consider the exponential distribu-
tion with location parameter θ given by
f(x|θ) = exp{−(x −θ)}
0 < θ < x < ∞. 
(18.1.3)
This exponential distribution with location (or threshold) parameter θ
arises in many areas of applications including reliability and life-testing,
survival analysis, and engineering problems; for example, see Balakrish-
nan and Basu (1995). In the literature, the location parameter is often
interpreted to be the minimum guaranteed life time.
We assume that the location parameter θ has a continuous mixing
distribution dG(θ) = g(θ)dθ on (0, ∞), so that the mixture distribution
of X is given by
fG(x) = f(x) =
 x
0
e−(x−θ )g(θ)dθ. 
(18.1.4)
We then consider the identiﬁability of mixtures by posterior mean for the
exponential distribution in (18.1.3) with continuous mixing distributions.
In Section 18.4, an explicit relation between the mixing distribution G
and the general posterior mean E(w(θ)|x), where w(θ) is a diﬀerentiable
function, is derived and then the identiﬁability of the continuous mix-
ture by general posterior mean E(w(θ)|x) is established. In particular,
an explicit expression for the mixing distribution in terms of the pos-
terior mean E(θ|x) is presented. In Section 18.5, the gamma mixing
distribution family is used to demonstrate the identiﬁability results for
the exponential distribution (18.1.3) established in this paper.
18.2
Posterior Moments
Let yi = (xi1, . . . , xin) denote the independent and identically distributed
samples of size n from truncation parameter distributions pi(xi|θi), i =
©2001 CRC Press LLC

1, 2, given by (18.1.1) and (18.1.2), respectively, then T1 = x1(1) and
T2 = x2(n) are suﬃcient statistics for θi, i = 1, 2, respectively, where
x1(1) is the smallest order statistic from y1 = (x11, . . . , x1n) and x2(n)
the largest order statistic from y2 = (x21, . . . , x2n).
The conditional
density functions of T1 and T2 can be easily derived as follows [Arnold,
Balakrishnan, and Nagaraja (1992, p. 12)]:
fT1(t1|θ1) = n{1 −P1(t1|θ1)}n−1p1(t1|θ1)
= n{k1(t1)}n−1h1(t1)/{k1(θ1)}n
= H1(t1)/K1(θ1),
a < θ1 ≤t1 < b,
(18.2.1)
fT2(t2|θ2) = n{P2(t2|θ1)}n−1p2(t2|θ2)
= n{k2(t2)}n−1h2(t2)/{k2(θ2)}n
= H2(t2)/K2(θ2),
a < t2 ≤θ2 < b,
(18.2.2)
where Hi(ti) = n{ki(ti)}n−1hi(ti) and Ki(θi) = {ki(θi)}n, i = 1, 2,
and the second equalities in (18.2.1) and (18.2.2) are obtained by using
relations
k1(θ1) =
 b
θ1
h1(x1)dx1
and
k2(θ2) =
 θ2
a
h2(x2)dx2,
respectively. Note that the conditional density functions of T1 and T2
are still type I and type II truncation parameter densities respectively.
It is assumed in this paper the truncation parameters θi, i = 1, 2, have
arbitrary prior distributions πi(θi) on (a, b) with πi(a) = 0 and πi(b) = 1,
respectively, then the marginal distributions of Ti, i = 1, 2, are given by
fπ1(t1) = f1(t1) =
 t1
a
{H1(t1)/K1(θ1)}dπ1(θ1),
(18.2.3)
fπ2(t2) = f2(t2) =
 b
t2
{H2(t2)/K2(θ2)}dπ2(θ2).
(18.2.4)
From these density functions (18.2.1)–(18.2.4), we are able to demon-
strate the explicit expressions of posterior moments in terms of the
marginal distributions of Ti, i = 1, 2, respectively, for the two diﬀerent
©2001 CRC Press LLC

truncation parameter likelihood functions with arbitrary prior distribu-
tions.
We ﬁrst demonstrate the exact relations between the posterior means
of general functions gi(θi) and the marginal distributions fi(ti), i =
1, 2, respectively, in Theorems 18.2.1 and 18.2.2, then we can easily give
the explicit expressions for posterior moments and posterior means and
variances in Corollaries 18.2.3–18.2.6.
THEOREM 18.2.1
For the type I truncation parameter distribution model, if g1(·) is diﬀer-
entiable and
 t1
a
|g′
1(s1)|{H1(t1)/H1(s1)}dF1(s1) < ∞,
then we have
E(g1(θ1) | y1)
= E(g1(θ1) | t1) = g1(t1) −
 t1
a g′
1(s1){H1(t1)/H1(s1)}dF1(s1)
f1(t1)
= g1(t1) −
 t1
a g′
1(s1)[{k1(t1)}n−1h1(t1)/{k1(s1)}n−1h1(s1)]dF1(s1)
f1(t1)
,
where f1(·) is given by (18.2.3) and F1(·) the corresponding cumulative
distribution function.
PROOF By Fubini’s theorem
 t1
a
g′
1(s1) H1(t1)
H1(s1) dF1(s1)
=
 t1
a
g′
1(s1) H1(t1)
H1(s1) f1(s1)ds1
=
 t1
a
g′
1(s1) H1(t1)
H1(s1)
 s1
a
H1(s1)
K1(θ1) dπ1(θ1)

ds1
=
 t1
a
 t1
θ1
g′
1(s1) H1(t1)
K1(θ1) ds1dπ1(θ1)
= g1(t1)f1(t1) −
 t1
a
g1(θ1)f1(t1 | θ1)dπ1(θ1),
then we obtain
©2001 CRC Press LLC

E(g1(θ1) | t1) =
 t1
a g1(θ1)f1(t1 | θ1)dπ1(θ1)
f1(t1)
= g1(t1) −
 t1
a g′
1(s1){H1(t1)/H1(s1)}dF1(s1)
f1(t1)
.
THEOREM 18.2.2
For the type II truncation parameter distribution model, if g2(·) is dif-
ferentiable and
 b
t2
|g′
2(s2)| | {H2(t2)/H2(s2)}dF2(s2) < ∞,
then we have
E(g2(θ2) | y2)
= E(g2(θ2) | t2) = g2(t2) +
 b
t2 g′
2(s2){H2(t2)/H2(s2)}dF2(s2)
f2(t2)
= g2(t2) +
 b
t2 g′
2(s2)[{k2(t2)}n−1h2(t2)/{k2(s2)}n−1h2(s2)]dF2(s2)
f2(t2)
,
where f2(·) is given by (18.2.4) and F2(·) the corresponding cumulative
distribution function.
PROOF It is similar as the proof of Theorem 18.2.1 and omitted.
COROLLARY 18.2.3
For the type I truncation parameter distribution model, we have
E(θr
1|y1) = E(θr|t1) = tr
1 −
 t1
a rsr−1
1
{H1(t1)/H1(s1)}dF1(s1)
f1(t1)
= tr
1 −
 t1
a rsr−1
1
[{k1(t1)}n−1h1(t1)/{k1(s1)}n−1h1(s1)]dF1(s1)
f1(t1)
,
where f1(·) is given by (18.2.3) and F1(·) the corresponding cumulative
distribution function.
©2001 CRC Press LLC

COROLLARY 18.2.4
For the type II truncation parameter distribution model, we have
E(θr
2|y2) = E(θr
2|t2) = tr
2 +
 b
t2 rsr−1
2
{H2(t2)/H2(s2)}dF2(s2)
f2(t2)
= tr
2 +
 b
t2 rsr−1
2
[{k2(t2)}n−1h2(t2)/{k2(s2)}n−1h2(s2)]dF2(s2)
f2(t2)
,
where f2(·) is given by (18.2.4) and F2(·) the corresponding cumulative
distribution function.
COROLLARY 18.2.5
For the type I truncation parameter distribution model, the posterior
mean and variance are given by, respectively,
E(θ1|y1) = E(θ1|t1) = t1 −u1(t1)
f1(t1) ,
var(θ1|y1) = var(θ1|t1) = 2t1u1(t1) −v1(t1)
f1(t1)
−u2
1(t1)
f 2
1 (t1) ,
where
u1(t1) =
 t1
a
{H1(t1)/H1(s1)}dF1(s1),
v1(t1) =
 t1
a
2s1{H1(t1)/H1(s1)}dF1(s1).
COROLLARY 18.2.6
For the type II truncation parameter distribution model, the posterior
mean and variance are given by, respectively,
E(θ2|y2) = E(θ2|t2) = t2 + u2(t2)
f2(t2) ,
var(θ2|y2) = var(θ2|t2) = v2(t2) −2t2u2(t2)
f2(t2)
−u2
2(t2)
f 2
2 (t2) ,
where
u2(t2) =
 b
t2
{H2(t2)/H2(s2)}dF2(s2),
©2001 CRC Press LLC

v2(t2) =
 b
t2
2s2{H2(t2)/H2(s2)}dF2(s2).
18.3
Examples
Example 1
Let x11, . . . , x1n be independent and identically distributed according to
the truncated exponential distribution as follows
p1(x1|θ1) = λ exp{−λ(x1 −θ1)},
−∞< θ1 ≤x1 < ∞, λ > 0,
it is type I truncation parameter density with h1(x1) = λe−λx1, k1(θ1) =
e−λθ1 and T1 = min(x11, . . . , x1n) is the suﬃcient statistic for θ1. Then
we have by Corollary 18.2.3,
E(θr
1|t1) = tr
1 −
 t1
−∞rsr−1
1
{e−nλ(t1−s1)}dF1(s1)
f1(t1)
and when r = 1, the posterior mean is given by
E(θ1|t1) = t1 −
 t1
−∞e−nλ(t1−s1)dF1(s1)
f1(t1)
;
(18.3.1)
for the special case n = 1, λ = 1, this result of (18.3.1) is exactly the
same as that given by Fox (1978).
Example 2
Let x21, . . . , x2n be independent and identically distributed according to
the power function distribution as follows
p2(x2|θ2) = αxα−1
2
/θα
2 ,
0 < x2 ≤θ2 < ∞, α > 0,
it is type II truncation parameter density with h2(x2) = αxα−1
2
, k2(θ2) =
θα
2 and T2 = max(x21, . . . , x2n) is the suﬃcient statistic for θ2. Then we
have by Corollary 18.2.4,
E(θr
2|t2) = tr
2 +
 ∞
t2 rsr−1
2
(tnα−1
2
/snα−1
2
)dF2(s2)
f2(t2)
and when r = 1, the posterior mean is given by
E(θ2|t2) = t2 +
 ∞
t2 (tnα−1
2
/snα−1
2
)dF2(s2)
f2(t2)
;
(18.3.2)
©2001 CRC Press LLC

for the special case n = 1, α = 1, this result of (18.3.2) is exactly the
same as that given by Fox (1978) for uniform distribution.
18.4
Identiﬁability by Posterior Mean
Before we present the main results about the identiﬁability by posterior
mean for the exponential distribution with location parameter, we will
introduce some results existing already in the literature which will be
very useful for the developments here.
LEMMA 18.4.1
For the exponential distribution in (18.1.3), we have
G(t) = F(t) + f(t),
(18.4.1)
where f(t) is given by (18.1.4) and F(t) is the corresponding cumulative
distribution function.
PROOF This relation was obtained by Blum and Susarla (1981) and by
Prasad and Singh (1990).
LEMMA 18.4.2
For the exponential distribution in (18.1.3), if w(θ) is a diﬀerentiable
function, then
M(x) = E(w(θ)|x) = w(x) −
 x
0 w′(s)e−xesf(s)ds
f(x)
.
(18.4.2)
In particular, when w(θ) = θ,
m(x) = E(θ|x) = x −
 x
0 e−xesf(s)ds
f(x)
,
(18.4.3)
where f(x) is given by (18.1.4).
PROOF The expression in (18.4.2) is presented in Theorem 18.1a, and
the special case (18.4.3) was given earlier by Fox (1978).
Next, we will present an explicit expression for the mixing distribu-
tion in terms of the general posterior mean M(x) for the exponential
distribution in the following theorem, and then the identiﬁability by the
general posterior mean is established.
©2001 CRC Press LLC

THEOREM 18.4.3
For the exponential distribution in (18.1.3), if the general posterior mean
M(x) is diﬀerentiable, then the mixing density function is given by
g(t) = C
M ′(t)
w(t) −M(t) exp

−t +

M ′(t)dt
w(t) −M(t)

,
(18.4.4)
where C is the norming constant.
Therefore, the mixing density is
uniquely determined by the general posterior mean.
PROOF From Lemma 18.4.2, we have
exf(x){w(x) −M(x)} =
 x
0
w′(s)esf(s)ds.
By diﬀerentiating both sides, we obtain
f ′(x){w(x) −M(x)} = f(x){M ′(x) −w(x) + M(x)}
and
 f ′(x)
f(x) dx =
 
M ′(x)
w(x) −M(x) −1

dx
which yields
f(x) = C exp

−x +

M ′(x)dx
w(x) −M(x)

where C is the norming constant. Then by Lemma 18.4.1, the mixing
density function is given by
g(t) = f(t) + f ′(t)
= C
M ′(t)
w(t) −M(t) exp

t +

M ′(t)dt
w(t) −M(t)

.
When w(θ) = θ, M(x) = E(θ|x) = m(x), we have the following
corollary.
COROLLARY 18.4.4
For the exponential distribution in (18.1.3), if the posterior mean m(x)
is diﬀerentiable, then the mixing density is given by
g(t) = C
m′(t)
t −m(t) exp

t +

m′(t)dt
t −m(t)

,
(18.4.5)
where C is the norming constant.
©2001 CRC Press LLC

Note that Gupta and Wesolowski (1997) obtained a similar expression
for the uniform distribution by considering the deﬁnition of posterior
mean directly. Our expressions are obtained by using the relationship
between the mixing distribution and the mixture distribution and the
relationship between the posterior mean and the mixture distribution.
Certainly, the procedure used here is easier to understand and also easy
to go through.
18.5
An Illustrative Example
Finally, we consider here discuss an example to illustrate the results
obtained in the last section. Suppose that the posterior mean m(x) is
given by
m(x) =

1 −
1
1 + α

x,
x > 0, α > 0.
Then by (18.4.5), we have
g(t) = C

1 −
1
1+α

1
1+α t
exp

−t +
 α dt
t

= C e−t tα−1
= tα−1 e−t
Γ(α)
,
t > 0, α > 0.
This is the gamma distribution which simply means that there is a
one-to-one relationship between the gamma mixing distribution family
and the posterior mean m(x) =

1 −
1
1+α

x.
References
1. Arnold, B. C., Balakrishnan, N., and Nagaraja, H. N. (1992). A
First Course in Order Statistics. John Wiley & Sons, New York.
2. Arnold, B. C., Castillo, E., and Sarabia, J. M. (1993).
Condi-
tionally speciﬁed models: Structure and inference. In Multivariate
Analysis: Future Directions 2 (Eds., C. M. Cuadras and C. R.
Rao), 441–450. Elsevier, Amsterdam.
©2001 CRC Press LLC

3. Balakrishnan, N. and Basu, A. P. (Eds.) (1995). The Exponen-
tial Distribution: Theory, Methods and Applications. Gordon and
Breach, Langhorne, PA.
4. Blum, J. R. and Susarla, V. (1981). Maximal derivation theory
of some estimators of prior distribution functions. Annals of the
Institute of Statistical Mathematics 33, 425–436.
5. Cacoullos, T. (1987). Characterizing priors by posterior expecta-
tions in multiparameter exponential families. Annals of the Insti-
tute of Statistical Mathematics 39, 399–405.
6. Cacoullos, T. and Papageorgiou, H. (1984). Characterizations of
mixtures of continuous distributions by their posterior means. Scan-
dinavian Actuarial Journal 8, 23–30.
7. Fox, R. (1978).
Solutions to empirical Bayes squared error loss
estimation problems. Annals of Statistics 6, 846–853.
8. Gupta, A. K. and Wesolowski, J. (1997). Uniform mixtures via
posterior means. Annals of the Institute of Statistical Mathematics
49, 171–180.
9. Korwar, R. M. (1975). On characterizing some discrete distribu-
tions by linear regression. Communications in Statistics 4, 1133–
1147.
10. Kyriakoussis, A. and Papageorgiou, H. (1991). Characterizations
of logarithmic series distributions. Statistica Neerlandica 45, 1–8.
11. Papageorgiou, H. (1985). On characterizing some discrete distri-
butions by a conditional distribution and a regression function.
Biometrical Journal 27, 473–479.
12. Papageorgiou, H. and Wesolowski, J. (1997). Posterior mean iden-
tiﬁes the prior distribution in NB and related models. Statistics &
Probability Letters 36, 127–134.
13. Pericchi, L. R., Sans´o, B., and Smith, A. F. M. (1993). Posterior
cumulant relationships in Bayesian inference involving the expo-
nential family. Journal of the American Statistical Association 88,
1419–1426.
14. Pericchi, L. R. and Smith, A. F. M. (1992). Exact and approximate
posterior moments for a normal location parameter. Journal of the
Royal Statistical Society, Series B 54, 793–804.
15. Polson, N. G. (1991). A representation of the posterior mean for a
location model. Biometrika 78, 426–430.
16. Prasad, B. and Singh, R. S. (1990). Estimation of prior distribu-
tion and empirical Bayes estimation in a non-exponential family.
Journal of Statistical Planning and Inference 24, 81–86.
©2001 CRC Press LLC

17. Teicher, H. (1961). Identiﬁability of mixtures. Annals of Mathe-
matical Statistics 32, 244–248.
18. Teicher, H. (1963).
Identiﬁability of ﬁnite mixtures.
Annals of
Mathematical Statistics 34, 1265–1269.
©2001 CRC Press LLC

19
Distributions of Random Volumes without
Using Integral Geometry Techniques
A. M. Mathai
McGill University, Montreal, Quebec, Canada
ABSTRACT The usual techniques available in the literature in de-
riving integer moments and distributions of random volumes of random
geometrical conﬁgurations are the integral geometry techniques.
The
author has earlier introduced a method based on functions of matrix ar-
gument and Jacobians of matrix transformations and derived arbitrary
moments, not just integer moments, and the exact distributions of the
volume contents of random p-parallelotopes and p-simplices in Euclidean
n-space, p ≤n + 1, for the following situations: (1) all the points are
either inside a hypersphere of unit radius or have general distributions
associated with them such as generalized type-1 beta, type-2 beta or
Gaussian, (2) some points are uniformly distributed over the surface of
a hypersphere of radius 1 and the remaining have general distributions
as in (1). In all these derivations no result from integral geometry is
used. In the present paper the case when the points have some general
matrix-variate distributions as well as the case when some points are
restricted to the surface of a hypersphere and the remaining points have
general classes of distributions as in (1) will be considered. The exact
arbitrary moments and the exact distributions of the volume content of
a random parallelotope will be derived without using any result from in-
tegral geometry. The densities of random distances, areas and volumes
in several particular cases will be written in terms of Bessel, Whittaker,
and Gauss’ hypergeometric functions.
Keywords and phrases Random parallelotope, random simplex, ran-
dom content, moments, exact distribution, Jacobians, matrix transfor-
mations, uniform random points, type-1, type-2 and Gaussian random
points, matrix-variate distributions
©2001 CRC Press LLC

19.1
Introduction
Let O be the origin of a rectangular coordinate system in the Euclidean
n-space Rn. Let P1, . . . , Pr be r points in this system. Consider the
vectors
⃗
OP1, . . . , ⃗
OPr for r ≤n. Let these vectors be linearly indepen-
dent. Then these ordered vectors can generate an r-parallelotope or an
r-simplex. If we consider r ≤n + 1 then the origin can be shifted to
one of the points and then the situation reduces to the one above. Let
∇r,n and ∆r,n denote the r-contents or the volume contents of these
r-parallelotope and r-simplex respectively. Then ∇r,n = r! ∆r,n. In-
teger moments of ∇r,n, thereby those of ∆r,n, are available in Miles
(1971) and the many references therein.
Many results from integral
geometry are used in those papers in deriving these results.
In the
present paper we will consider a method based on Jacobians of ma-
trix transformations and functions of matrix argument. For the vari-
ous types of matrix transformations and the associated Jacobians see
the recent book: Mathai (1997). By using the technique of Jacobians
of matrix transformations Mathai (1998) derived several results when
all the points are inside a hypersphere of radius 1 or have some gen-
eral matrix-variate distributions or independently and identically dis-
tributed with some general distributions associated with each point. In
a subsequent work the author has looked into the problem when some
of the points are uniformly distributed onver the surface of a hyper-
sphere thereby some of the vectors
⃗
OPi, i = 1, . . . , q, q ≤r have unit
length, that is, ∥⃗
OPi∥= 1, i = 1, . . . , q, and the remaining points have
general distributions associated with them. In the present paper these
results will be extended to cover the situation when the ﬁrst q vectors
⃗
OP1, . . . , ⃗
OPq, q ≤r have lengths unities, these q points are distributed
over the surface of the n-sphere but all points together have some gen-
eral distributions and the situations when the points are independently
distributed and having some general distributions.
Let X be a r×n, r ≤n, matrix of real variables where let Xj, the j-th
row of X represent the point Pj, j = 1, ..., r. When Y = XX′, where
X′ denotes the transpose of X, has a mtrix-variate distribution one can
evaluate the moments and distributions of the volume content ∇r,n of
the r-parallelotope. In terms of a determinant
∇r,n = |XX′|1/2 = |Y |1/2,
(19.1.1)
where Y = Y ′ > 0 almost surely.
In this case the rows of X need
not be independently distributed as long as Y has a tractable matrix-
variate distribution. For such an approach see the last chapters of Mathai
©2001 CRC Press LLC

(1997). Several classes of matrix-variate distributions are listed in the
book. Then when Y has a general matrix-variate distribution one can
evaluate arbitrary moments and the exact density of ∇r,n. In the present
paper we consider general matrix-variate distributions as well as the
situation where the Xjs are independently distributed, subject to the
restriction that some of the points are on the surface of an n-sphere of
radius 1. In order to tackle the situation when some of the random points
are restricted to the surface of an n-sphere various results on Jacobians
available in the literature are to be extended to cover such a situation.
Some of the results that we need, and which can be proved easily, will
be listed here as lemmas without proofs. All the variables appearing are
real unless stated otherwise. The parameters could be complex. Let
Ωj = {−1 ≤tjk ≤1, k = 1, . . . , j, 0 ≤t2
j1 + . . . + t2
jj ≤1}
wj = {0 ≤zk ≤1, k = 1, . . . , j, 0 ≤z1 + . . . + zj ≤1}
and
w∗
j = {−1 ≤tjk ≤1, k = 1, . . . , j, t2
j1 + . . . + t2
jj = 1}.
(19.1.2)
w∗
j describes the surface of a j-sphere of radius unity. That is,
tjj = ±

1 −t2
j1 −. . . −t2
jj−1.
The integral over w∗
j is the inegral over the surface of a j-sphere of radius
unity and the associated diﬀerential element is
2[1 −t2
j1 −. . . −t2
jj−1]−1/2dtj1 . . . dtjj−1.
(19.1.3)
Type–1 Dirichlet integral is given in Lemma 19.1.1 and Lemmas 19.1.2,
19.1.3, 19.1.4 and 19.1.5 can be established with the help of the type–1
Dirichlet integral.
LEMMA 19.1.1

wj
zα1−1
1
. . . zαj−1
j
(1 −z1 −. . . −zj)αj+1−1dz1 . . . dzj
= Γ(α1) . . . Γ(αj+1)
Γ(α1 + . . . + αj+1)
for ℜ(αk > 0, k = 1, . . . , j + 1
where ℜ(·) denotes the real part of (·).
©2001 CRC Press LLC

LEMMA 19.1.2

Ωj
(t2
jj)γjdtj1 . . . dtjj
=

ωj
z
1
2 −1
1
. . . z
1
2 −1
j−1 z
γj+ 1
2 −1
j
(1 −z1 −. . . −zj)1−1dz1 . . . dzj
=

Γ( 1
2)
j−1 Γ

γj + 1
2

Γ
 j
2 + 1 + γj

for ℜ(γj) > −1
2.
LEMMA 19.1.3
The volume of an n-sphere of unit radius is πn/2/Γ
 n
2 + 1

. This follows
from Lemma 19.1.2 by taking γj = 0 and replacing j by n.
LEMMA 19.1.4
(t2
j1)δj1 . . . (t2
jj)δjj integrated over the surface of a j-sphere of unit radius
is given by

ω∗
j
(t2
j1)δj1 . . . (t2
jj)δjjdtj1 . . . dtjj−1
= 2

ω∗
j
(t2
j1)δj1 . . . (t2
jj−1)δjj−1[1 −t2
j1 −. . . −t2
jj−1]δjj−1
2 dtj1 . . . dtjj−1
=
2
j
k=1 Γ
 1
2 + δjk
	
Γ
 j
2 + δj1 + . . . + δjj
 for ℜ(δjk) > −1
2, k = 1, . . . , j.
The result follows from ﬁrst multiplying by the diﬀerential element over
the surface of a j-sphere and then from using Lemmas 19.1.2 and 19.1.1.
LEMMA 19.1.5
The surface area of an n-sphere of unit radius, 2πn/2/Γ(n/2), is available
from Lemma 19.1.4 by putting δjk = 0, k = 1, . . . , j and then replacing
j by n.
LEMMA 19.1.6
Type–2 Dirichlet integral is given by

z1>0,...,zn>0
zα1−1
1
. . . zαn−1
n
(1 + z1 + . . . + zn)−(α1+...+αn+1)dz1 . . . dzn
= Γ(α1) . . . Γ(αn+1)
Γ(α1 + . . . + αn+1)
for ℜ(αk) > 0, k = 1, . . . , n + 1.
©2001 CRC Press LLC

LEMMA 19.1.7

−∞<tjk<∞, k=1,...,j
(t2
j1)γj1 . . . (t2
jj)γjj(1 + t2
j1 + . . . + t2
jj)−δj
dtj1 . . . dtjj
=
j
k=1 Γ
 1
2 + γjk
	
Γ

δj −γj1 −. . . −γjj −j
2

Γ(δj)
for ℜ(δj) > ℜ(
j
k=1 γjk) + j
2, ℜ(δj) > 0.
If Y = XX′ where some of the rows of X have Euclidean lengths
unity, the corresponding points are on the surface of an n-sphere, and
further the corresponding diagonal elements in Y are unities. Note that
Y is r × r and X is r × n, r ≤n. This requires modiﬁcations of the
Jacobians of matrix transformations when some of the diagonal elements
of the matrix are unities. These results are established by the author
and these will be stated as lemmas here.
LEMMA 19.1.8
Let Y = (yij) be an r × r symmetric positive deﬁnite matrix of func-
tionally independent real variables with y11 = 1 = . . . = yqq, q ≤r. Let
T be a lower triangular matrix with positive diagonal elements and of
functionally independent real variables. Then
Y = TT ′ ⇒
dY = 2r−q



q

j=2
tr−j
jj






r

j=q+1
tr−j+1
jj


dT
where dY and dT denote the wedge products of the diﬀerentials in Y and
T respectively, with tjj =

1 −t2
j1 −. . . −t2
jj−1, j = 2, . . . , q and if the
last q diagonal elements of Y are unities then
dY = 2r−q



r−q

j=1
tr−j+1
jj






r−1

j=r−q+1
tr−j
jj


dT
with
tjj =

1 −t2
j1 −. . . −t2
jj−1, j = r −q + 1, . . . , r −1.
For the proof of the next result see Mathai (1997).
©2001 CRC Press LLC

LEMMA 19.1.9
Let X = (xij) be an r×n, n ≥r, matrix of functionally independent real
variables, T be an r × r lower triangular matrix with positive diagonal
elements, U1 be an r × n semiorthonormal matrix, U1U ′
1 = Ir, U2 be an
(n −r) × n semiorthonormal matrix, U2U ′
2 = In−r, U1U ′
2 = 0 and let
U =

U1
U2

be an n × n full orthonormal matrix with uk denoting the
k-th column of U. Then
X = TU1 ⇒dX =



r

j=1
tn−j
jj


dT dU1
where
dU1 = ∧r
j=1 ∧n
k=j+1 (du′
j)uk
and (du′
j) denoting the vector of diﬀerentials in u′
j.
The following lemma can be established by integrating e−tr(XX′) di-
rectly over X as well as by using Lemma 19.1.9 and then equating the
two quantities, where tr(·) denotes the trace of the matrix (·).
LEMMA 19.1.10

dU1 = 2rπnr/2
Γr
 n
2

where, for example,
Γr(α) = π
r(r−1)
4
Γ(α)Γ

α −1
2

. . . Γ

α −r −1
2

,
ℜ(α) > r −1
2
.
(19.1.4)
In Lemma 19.1.9 let the ﬁrst q rows of X be such that 
n
k=1 x2
ik =
1, i = 1, . . . , q so that t11 = 1, tjj =

1 −t2
j1 −. . . −t2
jj−1, j = 2, . . . , q.
Then we have the following result:
LEMMA 19.1.11
In Lemma 9 let the ﬁrst q rows of X be such that 
n
k=1 x2
ik = 1, i =
1, . . . , q so that t11 = 1, tjj =

1 −t2
j1 −. . . −t2
jj−1, j = 2, . . . , q then
X = TU1 ⇒dX = c



r

j=1
tn−j
jj


dTdU1
©2001 CRC Press LLC

where TiT ′
i = 1, i = 1, . . . , q, Ti = (ti1, ti2, . . . , tii, 0, . . . , 0) and c is a
constant.
By integrating e−tr(XX′) over X, subject to the condition 
n
k=1 x2
ik =
1, i = 1, . . . , q, directly over X as well as by using Lemma 19.1.11 we
can establish the following result:
LEMMA 19.1.12
The constant c in Lemma 19.1.11 is unity.
19.2
Evaluation of Arbitrary Moments of the
Random Volumes
With the help of the above lemmas we are in a position to evaluate
arbitrary moments and the exact density of ∇r,n for various types of
situations. As mentioned earlier when the r × r real symmetric positive
deﬁnite matrix Y has a matrix-variate distribution the problem becomes
simpler and the necessary mathematics is available in Mathai (1997). For
the sake of illustration we will consider two results on the distribution
of the volume content, |Y | = |XX′| = ∇2
r,n, of the r-parallelotope in
n-space for the case when all the points are freely varying and for the
case when some of the points are restricted to be on the surface of an
n-sphere.
Then we will focus at the situation where the rows of X,
where Y = XX′, are independently distributed and having some general
distributions, and at the same time the ﬁrst q rows are restricted to be
of lengths unities.
19.2.1
Matrix-Variate Distributions for X
Let the joint density of X, where Y = XX′, be given by
f(Y ) = f(XX′) = c |XX′|αe−tr(XX′),
Y = Y ′ > 0, ℜ(α) > −n
2 + r −1
2
(19.2.1)
where c is the normalizing constant. Consider the transformation from
Lemma 19.1.9
X = TU1 ⇒dX =


r

j=1
tn−j
jj

dT dU1,
©2001 CRC Press LLC

and from Lemma 19.1.10 
dU1 = 2rπnr/2
Γr
 n
2
 .
From the fact that the total probability is 1, that is ,

X f(XX′)dX = 1,
we have
1 =

X
f(XX′)dX
= c2rπnr/2
Γr
 n
2


T
|TT ′|α+ n−j
2 e−tr(T T ′)dT
= c2rπnr/2
Γr
 n
2





i<j
 ∞
−∞
e−t2
ijdtij






r

j=1
 ∞
0
(t2
jj)α+ n−j
2 e−t2
jjdtjj


.
But
r

j=1
 ∞
0
(t2
jj)α+ n−j
2 e−t2
jjdtjj = 1
2r
r

j=1
Γ

α + n −j + 1
2

,
for ℜ(α) > −n
2 + r−1
2
and

i<j
 ∞
−∞
e−t2
ijdtij = πr(r−1)/4.
Substituting these back we have
c =
Γr
 n
2

πnr/2Γr

α + n
2
.
The general hth moment of the volume content of the r-parallelotope
in n-space, that is E|XX′|h/2, is available by replacing α by α + h
2 and
then taking the ratio of the normalizing constants. This will be stated
as a therem
THEOREM 19.2.1
When the r × n matrix X representing the r random points P1, . . . , Pr
in n-space have a matrix-variate density as given in (19.2.1) then the
hth moment of the volume content of the r-parallelotope created by the
linearly independent vectors
⃗
OP1, . . . , ⃗
OPr, where O denotes the origin,
is given by
E(∇r,n)h = E|XX′|
h
2 = Γr

α + n+h
2

Γr

α + n
2

for ℜ(h) > −n −2ℜ(α).
(19.2.2)
©2001 CRC Press LLC

Now, let us examine the situation when the ﬁrst q, q ≤r, points are
on the surface of a hypersphere of radius 1. In this case the ﬁrst q vectors
have lengths unities. That is,
∥⃗
OPj∥= 1, j = 1, . . . , q.
(19.2.3)
Suppose that the r × n, n ≥r, matrix, representing the r random
points, still has the matrix-variate density given in (19.2.1) subject to
the conditions in (19.2.3). Let the normalizing constant in this case be
denoted by c1. Then from Lemmas 19.1.9, 19.1.10 and 19.1.4 we have
the following: Let Tj = (tj1, . . . , tjj, 0, . . . , 0) be the jth row of T. Then
1 =

X
f(XX′)dX
= c1(

dU1)e−q



q

j=2

TjT ′
j=1
(t2
jj)α+ n−j
2 dTj



×



r

j=q+1
 ∞
0
(t2
jj)α+ n−j
2 e−t2
jjdtjj






r

i<j=q+1
 ∞
−∞
e−t2
ijdtij



= c1
2rπnr/2
Γr
 n
2
 e−qπ
q(q−1)
4
q
j=1 Γ

α + n−j+1
2

Γq 
α + n
2


×
1
2r−q



r

j=q+1
Γ

α + n −j + 1
2


π(r−q)(r+q−1)/4
= c12qe−q πnr/2
Γr
 n
2
 Γr

α + n
2

Γq 
α + n
2
 for ℜ(α) > −n
2 + r −1
2
, n ≥r.
Now, the h-th moment of the volume content of the r-parallelotope in
n-space is available by replacing α by α + h
2 and then taking the ratios
of the normalizing constants c1. This will be stated as a theorem.
THEOREM 19.2.2
Let the r random points P1, . . . , Pr in n-space represented by the r × n
matrix X, n ≥r, have a joint density as given in (19.2.1) subject to
the condition that the ﬁrst q points are on the surface of a hypersphere
of radius 1 or subject to the condition in (19.2.3) and let the points
be linearly independent. Then the hth moment of the volume content
of the r-parallelotope determined by the vectors
⃗
OP1, . . . , ⃗
OPr, where O
indicates the origin, is given by
©2001 CRC Press LLC

E(∇r,n)h = E|XX′|
h
2 = Γr

α + n+h
2

Γr

α + n
2

Γq 
α + n
2

Γq 
α + n+h
2
,
ℜ(h) > −n −2ℜ(α).
(19.2.4)
Similar procedure can be applied to derive the h-th arbitrary moments
of the volume contents of the r-parallelotopes when X, the matrix repre-
senting the r random points, has other types of matrix-variate distribu-
tions in the case of unrestricted points or in the case where some of the
points are restricted to be on a hypersphere. Observe that the volume
content of the r-simplex determined by the r points P1, . . . , Pr, denoted
by ∆r,n, is connected to the volume content of the r-parallelotope by the
relation
∆r,n = 1
r!∇r,n.
Thus the moments of the volume contents of the r-simplices are available
from those of the r-parallelotopes and hence a separate discussion is not
necessary.
Before concluding this section let us examine the distribution of ∇2
r,n
coming from Theorems 19.2.1 and 19.2.2. From (19.2.2), by replacing h
by 2h,
E(∇2
r,n)h = Γr

α + n
2 + h

Γr

α + n
2

=
r

j=1
Γ

α + n−j+1
2
+ h

Γ

α + n−j+1
2

= E(yh
1 )E(yh
2 ) . . . E(yh
r )
where y1, . . . , yr are independent real gamma random variables with the
parameters

α + n−j+1
2
, 1

, j = 1, . . . , r.
Hence, structurally, ∇2
r,n is
a product of r independent real gamma random variables. The exact
density of such a structure can be written in terms of a G-function of
the type Gr,0
0,r(·), for the details see Mathai (1993). Let x = ∇2
r,n and let
the density be denoted by fr(x). Then writing h = s −1,
fr(x) =
x−1
r
j=1 Γ

α + n−j+1
2
Gr,0
0,r

x

α+ n−j+1
2
, j=1,2,...,r

. (19.2.5)
Particular cases can be written in terms of known elementary functions.
For example, for r = 1,
f1(x) =
1
Γ

α + n
2
xα+ n
2 −1e−x,
0 < x < ∞.
(19.2.6)
©2001 CRC Press LLC

For r = 2 one can combine the gammas with the help of the duplication
formula for gamma functions, namely
Γ(2z) = π−1
2 22z−1Γ(z)Γ

z + 1
2

.
(19.2.7)
Then for r = 2
E(xh) = Γ(2α + n −1 + 2h)
22hΓ(2α + n −1) .
Thus u = 2x
1
2 has a gamma density, denoted by f2(u) where
f2(u) =
1
Γ(2α + n −1)u2α+n−2e−u,
u > 0.
(19.2.8)
For r = 4 also one can combine the gammas two by two by using the
formula (19.2.7). Then
E(4x
1
2 )h = Γ(2α + n −1 + h)Γ(2α + n −3 + h)
Γ(2α + n −1)Γ(2α + n −3)
.
Let v = 4x
1
2 . Then the density of v, denoted by f4(v), is given by the
following:
f4(v) =
v−1
Γ(2α + n −1)Γ(2α + n −3)G2,0
0,2

v

2α+n−1,2α+n−3

.
This G-function can be simpliﬁed in terms of a Bessel function as follows:
f4(v) =
2 v2α+n−3
Γ(2α + n −1)Γ(2α + n −3)K2

2v
1
2

,
v > 0 (19.2.9)
where K2(·) is a Bessel function, see Mathai (1993, p. 130).
In the case when the ﬁrst q points are restricted to be on the surface
of a hypersphere of unit radius then the hth moment is coming from
(19.2.4). That is,
E(∇2
r,n)h = d
r
j=2 Γ

α + n−j+1
2
+ h

Γq−1 
α + n
2 + h

= d



q

j=2
Γ

α + n−j+1
2
+ h

Γ

α + n
2 + h




×



r

j=q+1
Γ

α + n −j + 1
2
+ h



= E(uh
2) . . . E(uh
q )E(vh
1 ) . . . E(vh
r−q)
©2001 CRC Press LLC

where d is the normalizing constant such that the h-th moment is 1 when
h = 0, u2, . . . , uq are independent real type-1 beta random variables with
the parameters (α + n−j+1
2
, j−1
2 ), j = 2, . . . , q and v1, . . . , vr−q are inde-
pendent
real
gamma
random
variables
with
the
parameters
(α + n−j+1
2
, 1), j = q + 1, . . . , r. Then structurally ∇2
r,n in this case
is a product of the form u2 . . . uqv1 . . . vr−q. The exact density can be
written as a G-function of the type Gr−1,0
q−1,r−1(·). Let y = ∇2
r,n in (19.2.4)
and let the density of y be denoted by fr,q(y). Then for the general r
and q, q ≤r, the exact density of y is the following:
fr,q(y) =
Γq−1 
α + n
2

r
j=2 Γ

α + n−j+1
2
	y−1 Gr−1,0
q−1,r−1

y
α+ n
2 ,...,α+ n
2
α+ n−j+1
2
, j=2,...,r

,
y > 0.
(19.2.10)
Particular cases can be written up in terms of elementary functions. For
example, for q = 1, r = 2, y has a gamma density with the parameters

α + n−1
2 , 1

. That is,
f2,1(y) =
1
Γ

α + n−1
2
xα+ n−3
2 e−y,
y > 0.
(19.2.11)
For q = 2, r = 2, y has a type–1 beta density with the parameters

α + n−1
2 , 1
2

. That is,
f2,2(y) =
Γ

α + n
2

Γ

α + n−1
2

Γ
 1
2
yα+ n−3
2 (1 −y)−1
2 ,
0 < y < 1.
(19.2.12)
For q = 2, r = 3 the density of y is equivalent to that of the density of a
product of a real type-1 beta random variable and a real gamma random
variable where the variables are independent. This can be worked out
by using the transformation of variables technique. This is also directly
available from the G-function representation in (19.2.10). A G-function
of the type G2,0
1,2(·) can be written in terms of a Whittaker function W.,.(·)
by using the formula on page 130 of Mathai (1993), that is,
G2,0
1,2

z
α
β,γ

= z
β+γ−1
2
e−z/2W 1+β+γ−2α
2
, β−γ
2 (z).
In our case the parameters are α →α + n
2 , β →α + n−1
2 , γ →α + n−2
2 .
Substituting these the density is the following:
©2001 CRC Press LLC

f3,2(y) =
Γ

α + n
2

Γ

α + n−1
2

Γ

α + n−2
2
yα+ n
2 −7
4 e−y
2 W−1
4 , 1
4 (y),
y > 0.
(19.2.13)
For r = 3, q = 3, that is, when all the three points are restricted to be on
the surface of a hypersphere then the density is equivalent to that of a
product of two independent real type-1 beta random variables and from
(19.2.10) the density reduces to the following:
f3,3(y) =
Γ2 
α + n
2

Γ

α + n−1
2

Γ

α + n−1
2
y−1G2,0
2,2

y
α+ n
2 ,α+ n
2
α+ n−1
2
,α+ n−2
2

.
This G-function can be written in terms of a Gauss’ hypergeometric
function by using the formula on page 130 of Mathai (1993), namely
G2,0
2,2

z
β,β
β−1
2 ,β−1

= zβ−1(1 −z)
1
2
Γ
 3
2

× 2F1
1
2, 1
2; 3
2; 1 −z

, |z| < 1.
Therefore the density in this case reduces to the following:
f3,3(y) =
Γ2 
α + n
2

Γ

α + n−1
2

Γ

α + n−2
2

Γ
 3
2
yα+ n
2 −2(1 −y)
1
2
× 2F1
1
2, 1
2; 3
2; 1 −y

,
0 < y < 1.
(19.2.14)
When r = 3 and q = 1 the G-function in (19.2.10) reduces to the form
G2,0
0,2(·). This form is already mentioned earlier, and this G-function can
be written in terms of a Bessel function by using the formula
G2,0
0,2

z

α,β

= 2z(α+β)/2Kα−β

2z
1
2

.
19.2.2
Type–1 Beta Distribution for X
Here we consider another matrix-variate distribution of the beta type.
Let the r × n, r ≤n matrix X have a matrix-variate density of the type
fX(X) = k|XX′|α|I −XX′|β−r+1
2
(19.2.15)
©2001 CRC Press LLC

for O < XX′ < I where k is the normalizing constant, I is the identity
matrix and all the eigenvalues of XX′ are between 0 and 1. Let X = TU 1
as in Section 19.2.1. Then the joint density of T and U1, denoted by
f(T, U1), is given by
f(T, U1)dTdU1 = k|TT ′|α|I −TT ′|β−r+1
2



r

j=1
(t2
jj)
n−j
2


dTdU1.
Integrating out U1 the density of T, denoted by fT (T), is therefore
fT (T)dT = k 2rπ
n
2
Γr
 n
2
|TT ′|α|I −TT ′|β−r+1
2



r

j=1
(t2
jj)
n−j
2


dT.
From the general result corresponding to Lemma 19.1.8 we have
Y = TT ′ ⇒dY = 2r



r

j=1
(t2
jj)
r−j+1
2


dT.
Then the density of Y , denoted by fY (Y ), is available as
fY (Y ) = k π
nr
2
Γr
 n
2
|Y |α+ n
2 −r+1
2 |I −Y |β−r+1
2 , O < Y < I.
Integrating Y with the help of a real matrix-variate type-1 beta integral
and then substituting for k we have the density of Y given by
fY (Y ) = Γr

α + β + n
2

Γr(β)Γr

α + n
2
|Y |α+ n
2 −r+1
2 |I −Y |β−r+1
2 ,
for O < Y < I, ℜ(α) > −n
2 + r −1
2
, ℜ(β) > r −1
2
.
The h-th moment of the r-content of the r-parallelotope, ∇r,n, is avail-
able by integrating out over the density fX(X).
THEOREM 19.2.3
Let the r × n, r ≤n matrix representing the r linearly independent
random points in n-space have a matrix-variate density as in (19.2.15).
Then the hth moment of ∇r,n is given by
E(∇r,n)h = E|XX′|h/2 = Γr

α + n+h
2

Γr

α + n
2

Γr

α + β + n
2

Γr

α + β + n+h
2
,
ℜ(h) > −2ℜ(α) −n + p −1.
(19.2.16)
©2001 CRC Press LLC

If β =
r+1
2
then the density in (19.2.15) is a matrix-variate Pareto
type. In this case also if all the points are freely varying then the hth
moment of the r-content of the r-parallelotope is available from (19.2.16)
by putting β = r+1
2 . But if some of the points are restricted to be on the
surface of the n-sphere then the range of integration for the individual
variables seems to be diﬃcult to determine. From Lemma 19.1.8 note
that the Jacobian cannot be rewritten in terms of the determinant of Y
and hence integration with the help of matrix-variate integrals seems to
fail here.
Now we will consider a few cases when the rows of X are statisti-
cally independently distributed and having spherically symmetric distri-
butions. These cases in the general forms are discussed by the author
previously and hence the results will be listed here with the minimum
number of steps for the sake of completeness. Some special cases will be
enumerated in terms of hypergeometric functions which were not done
earlier.
19.2.3
The Case when the Rows of X are Independently
Distributed
Let X be the r × n, n ≥r matrix of real variables, T the lower triangu-
lar matrix with positive diagonal elements and U1 the semiorthonormal
matrix as deﬁned before. Let Xj = (xj1, . . . ., xjn) be the jth row of X.
Then the representation
X = TU1,
gives
Xj = tj1u(1) + tj2u(2) + . . . + tjju(j)
where u(i) is the i-th row of U1, and therefore
XjX′
j = t2
j1 + . . . + t2
jj, j = 1, . . . , r,
since u(i)u′
(i) = 1, u(i)u′
(k) = 0, i ̸= k. If Xj, j = 1, . . . , r are statistically
independently distributed with the n-variate density of Xj a function of
XjX′
j then the density is a function of t2
j1+. . .+t2
jj and {tj1, . . . , tjj}, j =
1, . . . , r are independently distributed, and
E(∇h
r,n) = E|XX′|h/2 = E|TT ′|h/2
= kr
2rπnr/2
Γr
 n
2


T



r

j=1
(t2
jj)
h+n−j
2


gj(t2
j1 + . . . + t2
jj)dT
(19.2.17)
©2001 CRC Press LLC

where kr is the normalizing constant in the density of X and gj(XjX′
j)
denotes the density of Xj. If the ﬁrst q points are distributed indepen-
dently and uniformly over the surface of an n-sphere of unit radius then
gj(·) is a constant which gets absorbed in kr for j = 1, . . . , q. Then for
j = 1, . . . , q the integrals over the tij’s are such that t11 = 1, tjj > 0,
tjj =

1 −(t2
j1 + . . . + t2
jj−1). Then from Lemma 19.1.4 the integrals
over the j-sphere for j = 2, . . . , q gives
q

j=2


Γj−1  1
2

Γ

h+n−j+1
2

Γ
 h+n
2


= πq(q−1)/4
q
j=1 Γ

h+n−j+1
2
	
Γq  h+n
2

.
(19.2.18)
The remaining integrals and kr depend upon gj(·), j = q + 1, . . . , r. We
will consider a few cases here.
Let the n-sphere be of radius unity. Out of the r independently dis-
tributed random points let the ﬁrst q be uniform over the surface and the
remaining r −q, q ≤r be uniform inside the sphere. From the volume
and surface contents we have
kr = Γq  n
2

Γr−q  n
2 + 1

2qπnr/2
.
(19.2.19)
From Lemma 19.1.2 the integral inside the j-sphere, j = q +1, . . . , r and
for tjj > 0, gives
r

j=q+1



Γj−1  1
2

Γ

n−j+h+1
2

2Γ
 n+h
2
+ 1




=
π(r−q)(r+q−1)/4 r
j=q+1 Γ

n+h−j+1
2

2r−qΓr−q  n+h
2
+ 1

(19.2.20)
Combining the results from (19.2.17) to (19.2.20) we have the following:
THEOREM 19.2.4
When the r points P1, . . . , Pr in Rn are independently and uniformly
distributed in an n-sphere of unit radius such that P1, . . . , Pq are uniform
over the surface and Pq+1, . . . , Pr are uniform inside the sphere, q ≤
r, then the h-th moment of the r-content ∇r,n of the r-parallelotope
generated by the vectors
⃗
OP1, . . . , ⃗
OPr, where O indicates the center of
the sphere, is given by
©2001 CRC Press LLC

E(∇h
r,n) = E|XX′|h/2
=

n
n + h
r−q Γr
 n+h
2

Γr
 n
2

Γr  n
2

Γr  n+h
2

for ℜ(h) > −n + r −1.
19.2.4
Type-1 Beta Distributed Independent Rows of X
Let Xj, j = 1, . . . , q be uniform on the surface of an n-sphere of radius
unity and let Xj, j = q + 1, . . . , p have the densities
fj(Xj) = dj(1 −XjX′
j)βj, XjX′
j < 1,
ℜ(βj) > −1. (19.2.21)
Then from Lemma 19.1.1
dj = Γ

βj + n
2 + 1

πn/2Γ(βj + 1) ,
ℜ(βj) > −1.
(19.2.22)
From Lemma 19.1.2 the integrals over tij’s, i ≥j, tjj > 0, i = q +
1, . . . , r, in the h-th moment of |XX′|1/2 gives the product
1
2r−q
r

j=q+1



Γj−1  1
2

Γ

n−j+h+1
2

Γ(βj + 1)
Γ
 n+h
2
+ 1 + βj




= π(r−q)(r+q−1)/4
2r−q
r

j=q+1



Γ

n−j+h+1
2

Γ(βj + 1)
Γ
 n+h
2
+ 1 + βj



.
(19.2.23)
The normalizing constant in this case, again denoted by kr, is
kr = Γq  n
2

2qπqn/2



r

j=q+1
Γ

βj + n
2 + 1

πn/2Γ(βj + 1)


.
(19.2.24)
From (19.2.17) to (19.2.24) we have the following result:
THEOREM 19.2.5
When the r independent random points P1, . . . , Pr are such that the ﬁrst
q of them are uniform over the surface of an n-sphere of radius unity
©2001 CRC Press LLC

and the remaining r −q points are type-1 beta distributed as in (19.2.21)
then the hth moment of the r content of the r-parallelotope created by
the vectors
⃗
OP1, . . . , ⃗
OPr, where O indicates the center of the sphere, is
given by
E(∇h
r,n) = E|XX′|h/2
= Γr
 n+h
2

Γr
 n
2

Γq  n
2

Γq  n+h
2




r

j=q+1
Γ

βj + n
2 + 1

Γ

βj + n+h
2
+ 1




for ℜ(h) > −n + r −1, −n −2 −2ℜ(βj), ℜ(βj) > −1, j = q + 1, . . . , r.
19.2.5
Type-2 Beta Distributed Independent Rows of X
Assume that the ﬁrst q points out of the r independently distributed
points are uniform over the surface of an n-sphere and the remaining
r −q have the densities
gj(Xj) = ηj(1 + XjX′
j)−(αj+ n
2 ),
ℜ(αj) > 0, j = q + 1, . . . , r
XjX′
j = x2
j1 + . . . + x2
jn.
From Lemma 19.1.7
ηj = Γ

αj + n
2

πn/2Γ (αj) .
(19.2.25)
The integral over the tij’s, in the h-th moment of |XX′|1/2, for j =
q+1, . . . , r is also available from Lemma 19.1.7 by replacing γj by n−j+h
2
and δj by αj + n
2 for j = q + 1, . . . , r and then taking the product,
multiplied by the normalizing constant ηj and observing that tjj > 0 for
all j. That is,
r

j=q+1
ηj

Tj
(t2
jj)
n−j+h
2
(1 + t2
j1 + . . . + t2
jj)−(αj+ n
2 )dTj
= π(r−q)(r+q−1)/4
π(r−q)n/22r−q



r

j=q+1
Γ

αj −h
2

Γ

n−j+h+1
2

Γ(αj + n
2 )


.
(19.2.26)
Then from (19.2.17)–(19.2.19) and (19.2.24)–(19.2.26) we have the fol-
lowing result:
©2001 CRC Press LLC

THEOREM 19.2.6
Out of the r independently distributed random points P1, . . . , Pr in Rn if
the ﬁrst q are uniform over the surface of an n-sphere of unit radius and
the remaining r−q have the densities gj(Xj), j = q+1, . . . , r of (19.2.25)
then the hth moment of the r-content ∇r,n of the r-parallelotope created
by the r vectors
⃗
OP1, . . . , ⃗
OPr, where O indicates the center of the sphere,
is given by
E(∇h
r,n) = E|XX′|h/2
= Γr
 n+h
2

Γr
 n
2

Γq  n
2

Γq  n+h
2




r

j=q+1
Γ

αj −h
2

Γ(αj)



for −n + r −1 < ℜ(h) < ℜ(2αj), ℜ(αj) > 0, j = q + 1, . . . , r.
19.2.6
Independently Gaussian Distributed Points
Out of the r independently distributed random points in Rn if the ﬁrst
q are uniform over an n-sphere of unit radius and the remaining r −q
have the densities
hj(Xj) = λn/2
πn/2 e−(λXjX′
j),
Xj = (xj1, . . . , xjn), −∞< xjk < ∞,
j = q + 1, . . . , r, r ≤n
(19.2.27)
then the integral over the tijs, with tjj > 0, in the hth moment of
|XX′|1/2 for i = q + 1, . . . , r, j = 1, . . . , n gives
r

j=q+1
λn/2
πn/2

. . .

(t2
jj)
n−j+h
2
e−λ(t2
j1+...+t2
jj)dtj1 . . . dtjj
=
λ
π
(r−q)n/2
λ−(r−q)(n+h)/2π(r−q)(r+q−1)/4
×
1
2r−q



r

j=q+1
Γ
n −j + h + 1
2


.
(19.2.28)
The integral over the surface of the n-sphere for the ﬁrst q points yields
Γq(n/2)
2qπqn/2
πq(q−1)/4
Γq  n+h
2




q

j=1
Γ
n + h −j + 1
2



(19.2.29)
©2001 CRC Press LLC

and

dU1 = 2rπnr/2
Γr
 n
2
 .
(19.2.30)
From (19.2.28)–(19.2.30) we have the following:
THEOREM 19.2.7
Consider r independently distributed random points P1, . . . , Pr in Rn
where the ﬁrst q of them are uniform over the surface of an n-sphere with
center O and radius unity and the remaining r −q have the Gaussian
densities given in (19.2.27). Then the hth moment of the r-content ∇r,n
of the r-parallelotope created by the vectors
⃗
OP1, . . . , ⃗
OPr is given by
E(∇h
r,n) = E|XX′|h/2
=
1
λ(r−q)h/2
Γq  n
2

Γq  n+h
2
 Γr
 n+h
2

Γr
 n
2
 , ℜ(h) > −n + r −1.
Note that two gammas can be cancelled from E(∇h
r,n) above. Also ob-
serve that in the density of (19.2.27) we could have replaced λ by λj for
each j still the procedure goes through.
19.2.7
Distributions of the r-Contents
The moment expression in Theorem 19.2.4 can be written in the following
form for E(∇2
r,n)h = E(∇r,n)2h.
E(∇2
r,n)h = a



r

j=2
Γ(γj + h)
Γ(γj + δj + h)


,
γj = n
2 −j −1
2
, j = 2, . . . , r
(19.2.31)
where δj, j = 1, . . . , r is the sequence
"1
2, 2
2, . . . , q −1
2
, q
2 + 1, . . . , r −1
2
+ 1
#
and a is a normalizing constant which is available by putting h = 0 in
the gamma product and taking the reciprocal of the resulting quantity.
Hence the density of ∇2
r,n can be written as a G-function of the form
Gr−1,0
r−1,r−1 from where series representation is available, see the details
©2001 CRC Press LLC

of the procedure from Mathai (1993). The density can also be looked
upon as the density of a product of independent real type–1 beta random
variables.
In Theorem 19.2.5, that is the type-1 beta distributed case, if the
parameters βj’s are replaced by zeros then the hth moment of ∇2
r,n
agrees with that in (19.2.31). Hence the hth moment of ∇2
r,n in this case
can also be written as (19.2.31) with γj, j = 1, . . . , r remaining the same
whereas δj, j = 2, . . . , q remain the same but for j = q + 1, . . . , r add
βq+1, . . . , βr to the parameters in (19.2.31). Hence the density of ∇2
r,n
in this case also is a G-function of the form Gr−1,0
r−1,r−1.
In Theorem 19.2.6 the hth moment of ∇2
r,n can be written in the
following form:
E(∇2
r,n)h = b
r−1
j=2 Γ(γj + h)
	 r
j=q+1 Γ(1 −δj −h)
	
q−1
j=1 Γ(ρj + h)
	
(19.2.32)
where b is the normalizing constant and γj =
n
2 −j−1
2 , j = 2, . . . , r,
δj = 1 −αj, j = q + 1, . . . , r, ρj = n
2 , j = 1, . . . , q −1. The density can
be evaluated as a G-function of the form Gr−1,r−q
r−1,r−1. When evaluating
the series form, part of the function is coming from the residues corre-
sponding to the gammas Γ(γj + h) and the remaining from the analytic
continuation or from the gammas Γ(1 −δj −h), see the details from
Mathai (1993). Since the particular cases of this situation do not seem
to be available in the literature we will list a few cases here. Let z = ∇2
r,n
in Theorem 19.2.6 and let the density of z be denoted by gr,q(z).
For r = 2, q = 2
E(zh) =
Γ
 n
2

Γ
 n−1
2
 Γ
 n−1
2
+ h

Γ
 n
2 + h
 .
Here z has a type-1 beta density with the parameters
 n−1
2 , 1
2

. In this
case there is no type-2 distributed point. The density is given by
g2,2(z) =
Γ
 n
2

Γ
 n−1
2

Γ
 1
2
z
n−3
2 (1 −z)−1
2 , 0 < z < 1.
(19.2.33)
For r = 2, q = 1
E(zh) =
1
Γ
 n−1
2

Γ(α2)Γ
n −1
2
+ h

Γ(α2 −h).
©2001 CRC Press LLC

The density is equivalent to that of the ratio z1/z2 where z1 and z2
are independently distributed real gamma variables with the parameters
n−1
2
and α2 respectively or the moment can also be looked upon as the
h-th moment of a type–2 beta random variable. Hence the density is the
following:
g2,1(z) = Γ

α2 + n−1
2

Γ
 n−1
2

Γ(α2)z
n−3
2 (1 + z)−(α2+ n−1
2 ),
0 < z < ∞.
(19.2.34)
For r = 3, q = 3 the density is equivalent to the density of a product of
two independent real type-1 beta random variables with the parameters
 n−1
2 , 1
2

,
 n−2
2 , 1

and the density coincides with the f3,3(y) given in
(2.14). For r = 3, q = 2 the hth moment is of the form
E(zh) =
Γ
 n
2

Γ
 n−1
2

Γ
 n−2
2

Γ(α3)
Γ
 n−1
2
+ h

Γ
 n
2 + h
 Γ
n −2
2
+ h

Γ(α3 −h).
This is equivalent to the hth moment of a product of independent real
random variables where one is a type-1 beta and the other is a type-2
beta. The hth moment can be simpliﬁed to the form
E(zh) =
 n
2 −1

Γ
 n−1
2

Γ(α3)
Γ
 n−1
2
+ h

Γ(α3 −h)
 n
2 −1 + h

.
There is one pole at h = −n
2 +1 and poles at h = −n−1
2
−ν, ν = 0, 1, . . .
for 0 < z < 1 and poles at h = α3 +ν, ν = 0, 1, . . . for z > 1. Evaluating
as the sum of the residues at these poles and then simplifying we have the
following density where 2F1(·) denotes a Gauss’ hypergeometric function.
g3,2(z) = γ







√πΓ

α3 + n
2 −1

z
n−4
2
−2Γ

α3 + n−1
2

z
n−3
2 γ1(z), 0 < z < 1
Γ(α3+ n−1
2 )Γ(α3+ n−2
2 )
Γ(α3+ n
2 )
z−α3−1γ2(z), z > 1
(19.2.35)
for ℜ(α3) > −n
2 + 1 where
γ =
 n
2 −1

Γ
 n−1
2

Γ(α3),
γ1(z) = 2F1

α3 + n −1
2
, 1
2; 3
2; −z

,
©2001 CRC Press LLC

and
γ2(z) = 2F1

α3 + n −1
2
, α3 + n −2
2
; α3 + n
2 ; −1
z

.
For r = 3, q = 1 the hth moment has the form
E(zh) =
%
Γ
n −1
2

Γ
n −2
2

Γ(α2)Γ(α3)
&−1
×Γ
n −1
2
+ h

Γ
n −2
2
+ h

Γ(α2 −h)Γ(α3 −h).
This can also be looked upon as the hth moment of a product of two
independent real type-2 random variables. Evaluating by the residue
calculus the density when the poles are simple, that is, for α2 −α3 ̸=
±ν, ν = 0, 1, . . ., is the following:
g3,1(z)
= δ







√πΓ

α2 + n−2
2

Γ

α3 + n−2
2

z
n−4
2 δ1(z)
−2√πΓ

α2 + n−1
2

Γ

α3 + n−1
2

z
n−3
2 δ2(z), 0 < z < 1
Γ

α2 + n−1
2

Γ

α2 + n−2
2

Γ(α3 −α2)z−α2−1δ3(z)
+Γ

α3 + n−1
2

Γ

α3 + n−2
2

Γ(α2 −α3)z−α3−1δ4(z), z > 1
(19.2.36)
where
δ−1 = Γ
n −1
2

Γ
n −2
2

Γ(α2)Γ(α3),
δ1(z) = 2F1

α2 + n −2
2
, α3 + n −2
2
; 1
2; z

δ2(z) = 2F1

α2 + n −1
2
, α3 + n −1
2
; 3
2; z

,
δ3(z) = 2F1

α2 + n −1
2
, α2 + n −2
2
; α2 −α3 + 1; 1
z

,
and
δ4(z) = 2F1

α3 + n −1
2
, α3 + n −2
2
; α3 −α2 + 1; 1
z

.
For r > 3 the poles of the integrand will be of higher order and hence
the density will involve psi and zeta functions. Hence more special cases
will not be listed here.
©2001 CRC Press LLC

In Theorem 19.2.7 the h-th moment of λ(r−q)∇2
r,n is of the form
E(λr−q∇2
r,n)h = c
r
j=2 Γ(γj + h)
q−1
j=1 Γ(δj + h)
(19.2.37)
where c is the normalizing constant, γj = n
2 −j−1
2 , j = 2, . . . , r and
δj =
n
2 , j = 1, . . . , q −1.
The density corresponding to (19.2.37) is
a G-function of the form Gr−1,0
q−1,r−1. Series representation of such a G-
function is also available from Mathai (1993). Several special cases of
this structure are already done in Section 19.2.1.
In all the above cases some of the special cases can be evaluated in
terms of elementary special functions as done in Section 19.2.1.
For
general parameters the series forms of the densities will be of logarithmic
type involving psi and generalized zeta functions due to the presence of
poles of higher orders in the integrand.
References
1. Mathai, A. M. (1998). Random p-content of a p-parallelopote in
Euclidean n-space. Advances in Applied Probability 32(2), (to ap-
pear).
2. Mathai, A. M. (1997). Jacobians of Matrix Transformations and
Functions of Matrix Argument. World Scientiﬁc Publishing, New
York.
3. Mathai, A. M. (1993). A Handbook of Generalized Special Func-
tions for Statistical and Physical Sciences. Oxford University Press,
Oxford.
4. Miles, R. E. (1971). Isotropic random simplices. Advances in Ap-
plied Probability 3, 353–382.
©2001 CRC Press LLC

PA RT I V
Time Series, Linear, and Non-Linear
Models
©2001 CRC Press LLC

20
Cointegration of Economic Time Series
T. W. Anderson
Stanford University, Stanford, CA
ABSTRACT Regression models and simultaneous equations models
are reviewed. The Limited Information Maximum Likelihood estimator
of a structural equation is a vector annihilated by a reduced rank re-
gression estimator of the reduced form. This estimator is based on the
canonical correlations and vectors of the dependent and independnt vari-
ables. Cointegated models are introduced as nonstationary autoregessive
processes having some stationary linear functions; the error-correction
form of the model has a coeﬃcient matrix of lower rank. The reduced
rank regression estimator of this matrix is in part a supereﬃcient estima-
tor. Its large-sample behavior and that of the likelihood ratio criterion
for testing rank are described.
Keywords and phrases Regression models, autoregressive processes,
simultaneous equations, reduced rank regression estimator, nonstation-
ary processes, error-correction form
20.1
Introduction
The purpose of this paper is to introduce the econometric techniques
suitable for analyzing cointegrated economic time series. The relevant
series may be nonstationary with some linear combinations being sta-
tionary. The observed series may display trends, inﬂation, and/or in-
creasing volatility, but some relations between them may hold constant.
Among the objectives is the estimation of these relations. The method-
ology is an extension of the techniques used in regression analysis and
for simultaneous equations models.
For background we review the basic ideas of regression analysis and the
single-equation methods for simultaneous equation models. The latter
are examples of “reduced rank regression,” which forms the basis for
©2001 CRC Press LLC

cointegration analysis. The models we treat are the simplest that show
the essential characteristics of cointegration; they are autoregressions of
order 1. The means are supposed to have been eliminated; no intercept
is included.
A method of detrending some nonstationary series is to diﬀerence the
series [Box and Jenkins (1971)]. A linear trend is eliminated by a single
diﬀerencing, for instance. The kind of nonstationary series considered in
this paper can be reduced to stationarity by one diﬀerencing. We look
for linear combinations of the observed variables which are stationary
without diﬀerencing.
A more detailed treatment is given in Anderson (1999). For a fuller
study see Johansen (1995) and Dhrymes (1998). The idea of cointegra-
tion was introduced by Granger (1981).
20.2
Regression Models
Possibly the most frequent statistical analysis in econometrics is regres-
sion analysis. The data for which this methodology is appropriate con-
sists of a set of p economic or dependent or endogenous variables observed
over time constituting a sequence of vectors Yt = (Y1t, . . . , Ypt)′ and a
set of q explanatory or independent or predetermined variables consti-
tuting a sequence of vectors Xt = (X1t, . . . , Xqt)′, t = 1, 2, . . . , T. The
explanatory variables may be noneconomic or exogenous variables or the
economic variables observed earlier in time.
A linear regression model is
Yt = BXt + Zt,
(20.2.1)
where Zt = (Z1t, . . . , Zpt)′ is a vector of unobserved random variables or
disturbances that are uncorrelated with Xt, Xt−1, . . .. In formal terms
the assumptions are EZt = 0,
EZtZ′
t = ΣZZ,
EZtX′
t = 0.
(20.2.2)
The statistical analysis concerns B, the matrix of regression.
We observe Yt, Xt,
t = 1, . . . , T.
We shall assume EXt = 0.
In
practice observations would be replaced by the observations minus the
respective sample means. For the sake of simplifying the exposition we
assume EXt = 0.
Deﬁne the sample covariance matrices
SY Y = 1
T
T

t=1
YtY′
t,
p × p,
(20.2.3)
©2001 CRC Press LLC

SY X = 1
T
T

t=1
YtX′
t,
p × q,
(20.2.4)
SXX = 1
T
T

t=1
XtX′
t,
q × q.
(20.2.5)
The Least Squares Estimator of the matrix of regression coeﬃcients is
ˆB = SY XS−1
XX.
(20.2.6)
When the Xt’s are nonstochastic, an element of ˆB is a minimum variance
unbiased estimator of the corresponding element of B.
The residuals
ˆZt = Yt −ˆBXt
(20.2.7)
are approximations to the unobserved disturbances Zt. The matrix
S ˆ
Z ˆ
Z = 1
T
T

t=1
(Yt −ˆBXt)(Yt −ˆBXt)′
(20.2.8)
is an estimator of the covariance matrix ΣZZ.
Statistical inference is carried out in terms of ˆB and S ˆ
Z ˆ
Z. For example,
if the Xt’s are nonstochastic, the variance of an element ˆβij of ˆB is
1
T σii
zz(S−1
XX)jj, where (S−1
XX)jj is the j, j-th element of the inverse of
SXX. This variance is estimated by 1
T sii
ˆ
Z ˆ
Z(S−1
XX)jj.
Whether X1, . . . , XT are stochastic or nonstochastic, the estimators
ˆB and S ˆ
Z ˆ
Z can be used. If SXX →ΣXX, a positive deﬁnite matrix, the
asymptotic theory holds.
20.3
Simultaneous Equation Models
For many econometric purposes the regression model is too simple. An
example is the elementary market situation in which the observed price
and quantity of a good is a result of the behaviors of consumers and
producers; that is, the observed price and quantity are the equilibrium
solution of a demand schedule and a supply function.
The general (linear) simultaneous equations model is
AYt = ΞXt + Ut,
|A| ̸= 0,
(20.3.1)
where as before Yt is a vector of endogenous variables, Xt a vector of
predetermined variables, and Ut is a vector of unobservable disturbances
©2001 CRC Press LLC

[Koopmans (1950)]. A component equation may describe the behavior
of some set of economic agents. For example, it might be the demand
function of consumers. The solution of (3.1) for Yt yields a regression
model (20.2.1), known as the reduced form. The relations between the
quantities in (20.2.1) and in (20.3.1) is
Ξ = AB,
Ut = AZt.
(20.3.2)
To make inferences about the parameters of the simultaneous equation
model, A, Ξ, and ΣUU = EUtU′
t we could ﬁrst estimate the parameters
of the regression model and than solve ˆΞ = ˆAˆB for ˆΞ and ˆA. However,
to carry out this program requires some a prior knowledge of Ξ and A;
that is, each structural equation must be identiﬁed.
As an over-simpliﬁed example, suppose that in an agricultural market
the demand equation for hogs, the ﬁrst component equation of (20.3.1),
does not depend on the other endogenous variables and does not depend
on the predetermined variables; that is, the ﬁrst row of Ξ consists of 0s
and the ﬁrst row of A has p −2 0s. The structural equation is
α11(hog price) + α12(hog quantity) = u1t.
(20.3.3)
The ﬁrst row of Ω= AB can be written as
ξ(1) = (0, 0, . . . , 0) = α(1)B
= (α11, α12, 0 . . . 0)


β11
· · ·
β1q
...
βp1
· · ·
βpq


= (α11, α12)

β11 · · · β1q
β21 · · · β2q

.
(20.3.4)
To solve (20.3.4) for (α11, α12) we need
rank

β11, . . . , β1q
β21, . . . , β2q

= 1.
(20.3.5)
To use the sample equivalent of (20.3.4) for estimation of (α11, α12) we
want the estimator of

β11, . . . , β1q
β21, . . . , β2q

(20.3.6)
to be of rank 1, say
 ¯β11, . . . , ¯β1q
¯β21, . . . , ¯β2q

.
(20.3.7)
©2001 CRC Press LLC

Then the estimator of (α11, α12) is the solution to
(¯α11, ¯α12)
 ¯β11, . . . , ¯β1q
¯β21, . . . , ¯β2q

= 0.
(20.3.8)
The solution (¯α11, ¯α12) is the Limited Information Maximum Likelihood
(LIML) Estimator [Anderson-Rubin (1949)]. It is the maximum likeli-
hood estimator using only the information in (20.3.4) when the Uts are
normally distributed. The estimator of the matrix (20.3.6) is a special
case of the reduced rank regression estimator, which will be explained
below.
20.4
Canonical Analysis and the Reduced Rank
Regression Estimator
It is enlightening to put the reduced rank regression estimator in terms
of a canonical analysis based on the regression model (20.2.1). We can
write an analysis of variance table for the random vector Yt, the sample
vector Yt, and a linear combination of the sample vector φ′Yt.
Analysis of Variance
R.V.
Sample
φ′Yt
Eﬀect
BΣXXB′
ˆBSXX ˆB′
φ′BΣXXB′φ
Error
ΣZZ
S ˆ
Z ˆ
Z
φ′ΣZZφ
Total
ΣY Y
SY Y
φ′ΣY Y φ
The sum of an eﬀect variance and the corresponding error variance is
the total variance. The eﬀect variance can be interpreted as the signal
variance and the error variance as the noise variance. Then
φ′BΣXXB′φ
φ′Σ ˆ
Z ˆ
Zφ
= signal
noise of φ′Yt.
(20.4.1)
We may ask for those linear combinations with the largest signal-to-
noise ratios; that is,
max
φ
φ′BΣXXB′φ
φ′ΣZZφ
,
φ′Σ ˆ
Z ˆ
Zφ = 1.
(20.4.2)
The maximizing φ satisﬁes
BΣXXB′φ = θΣZZφ,
(20.4.3)
where θ satisﬁes
|BΣXXB′ −θΣZZ| = 0.
(20.4.4)
©2001 CRC Press LLC

Let the solutions to (20.4.4) be
θ1 ≥θ2 ≥· · · ≥θp ≥0
(20.4.5)
and the corresponding solutions to (20.4.3) and φ′ΣZZφ = 1 be
φ1, φ2, . . . , φp,
φii > 0.
(20.4.6)
Then
φiBΣXXB′φj = φ′
iΣZZφjθj = θi,
i = j,
= 0,
i ̸= j.
(20.4.7)
The rank of B is k if and only if θk > 0 and
θk+1 = · · · = θp = 0.
(20.4.8)
In the sample a similar algebra can be carried out. The sample signal-
to-noise ratio is maximized by f ′Yt for f satisfying
ˆBSXX ˆB′f = tS ˆ
Z ˆ
Zf,
f ′S ˆ
Z ˆ
Zf = 1,
(20.4.9)
for t satisfying
|ˆBSXX ˆB′ −tS ˆ
Z ˆ
Z| = 0.
(20.4.10)
Let the solutions to (20.4.11) be
t1 > t2 > · · · > tp
(p + q ≤N),
(20.4.11)
f1, f2, . . . , fp,
fii > 0.
(20.4.12)
The Reduced Rank Regression estimator of B for prescribed rank k is
ˆBk = S ˆ
Z ˆ
ZF1F′
1 ˆB,
(20.4.13)
where
F1 = (f1, . . . , fk)
(20.4.14)
Anderson (1951). The estimator ˆBk is a maximum likelihood estimator
if the disturbances are normally distributed.
The equation (20.4.10) can be transformed to
|SXY S−1
Y Y SY X −r2SXX| = 0,
(20.4.15)
where r2 = t/(1 + t), and (20.4.9) can be transformed to
SXY S−1
Y Y SY Xˆγ = r2SXXˆγ.
(20.4.16)
©2001 CRC Press LLC

Then the reduced rank regression estimator of B can be written as
ˆBk = SY X ˆΓ1ˆΓ
′
1,
(20.4.17)
where ˆΓ1 = (ˆγ1, . . . , ˆγk) and ˆγi is normalized as ˆγiSXXˆγi = 1 for
i = 1, . . . , k (r2
1 > · · · > r2
p).
If the rank of B is k, then
φ′
jB = 0,
j = k + 1, . . . , p.
(20.4.18)
In particular, if k = p −1,
φ′
pB = 0, and the maximum likelihood
estimator of φp is fp (or a multiple of it). The LIML estimator of a
structural equation is fp when the relevant part of the reduced form is
(20.2.1).
The likelihood ratio criterion for testing the hypothesis that rank B
is k is
2 log λ = N
p

j=k+1
log(1 + tj) ∼N
p

j=k+1
tj
(20.4.19)
[Anderson (1951)]. If the null hypothesis is true, 2 log λ
d→χ2
(p−k)(q−k).
20.5
Autoregressive Processes
Many sets of economic variables constitute autoregressive processes. In
(20.2.1) replace Xt by Yt−1 to obtain
Yt = BYt−1 + Zt.
(20.5.1)
We have suppressed a constant term (intercept) in (20.5.1) as well as
exogenous variables. The model (20.5.1) deﬁnes a stationary process if
the roots λi,
i = 1, . . . , p, of
|B −λI| = 0
(20.5.2)
satisfy |λi| < 1. Then
Yt =
∞

s=0
BsZt−s,
(20.5.3)
and the covariance matrix of Yt is
ΣY Y =
∞

s=0
BsΣZZB′s.
(20.5.4)
©2001 CRC Press LLC

Given a sample Y0, Y1, . . . , YT , the least squares estimator of B is
(20.2.6) with SY X
and SXX
replaced by T −1 T
t=1 YtY′
t−1 and
T −1 T
t=1 Yt−1Y′
t−1, respectively. The residuals and the estimator of
ΣZZ are given by (20.2.7) and (20.2.8) with Xt replaced by Yt−1. The
canonical analysis and reduced rank regression estimator ˆBk are as de-
scribed in Section 20.4 with the obvious changes.
20.6
Nonstationary Models
A particular nonstationary model occurs when all the roots of |B−λI| =
0 are equal to 1. This is the case when B = I. Then
Yt = Yt−1 + Zt,
t = 1, 2, . . . .
(20.6.1)
The initial vector Y0 can be random or nonstochastic. For convenience
we assume that Y0 = 0. Then
Yt =
t−1

s=0
Zt−s,
t = 1, 2, . . . ,
(20.6.2)
EYtY′
t = E
t−1

r,s=0
Zt−rZ′
t−s = tΣZZ.
(20.6.3)
The model (20.6.1) deﬁnes a random walk. In another terminology we
say the {Yt} is integrated of order 1, denoted {Yt} ∈I(1). The sequence
{Zt} is integrated of order 0, denoted {Zt} ∈I(0). Note that the ﬁrst
diﬀerence of Yt
∆Yt = Yt −Yt−1 = Zt;
(20.6.4)
hence {∆Yt} ∈I(0).
Diﬀerencing has been used in time series analysis for another model –
a model with a linear trend
Yt = α + βt + Zt;
(20.6.5)
that is, EYt = α + βt. Here
E∆Yt = α + βt −[α + β(t −1)]
= β.
(20.6.6)
©2001 CRC Press LLC

Since ∆Yt has no trend, the diﬀerencing operation can be considered
as a detrending device. This is an important feature of the Box-Jenkins
time series analysis packages. The “error-correction form” of (20.5.1) is
∆Yt = ΠYt−1 + Zt,
(20.6.7)
where
Π = B −I
(20.6.8)
In the pure I(1) model Π = 0.
20.7
Cointegrated Models
Granger (1981) suggested that although many economic time series ap-
pear as I(1), linear combinations of these variables may be I(0); that
is, some relations between variables are stationary. He called models for
such variables “cointegrated.”
Suppose {Yt} ∈I(1). Consider a linear combination
ω′Yt = ω′BYt−1 + ω′Zt.
(20.7.1)
If ω′B = ω′, then
ω′Yt = ω′Yt−1 + ω′Zt ∈I(1).
(20.7.2)
If ω′B = λω,
|λ| < 1, then
ω′Yt = λω′Yt−1 + ω′Zt ∈I(0).
(20.7.3)
Let
ω′
iB = λiω′
i;
(20.7.4)
that is
ω′
i(B −λiI) = 0,
(20.7.5)
where λi is a root of |B −λI| = 0. Suppose |λi| < 1, i = 1, . . . , k, λi =
1,
i = k + 1, . . . , p. We can write |B −λI| = 0 as
0 = |B −I −(λ −1)I| = |Π −(λ −1)I|.
(20.7.6)
Thus 0 is a characteristic root of Π = B −I of multiplicity n = p −k
and there are k roots of Π diﬀerent from 0. The rank of Π is k.
The model (20.5.1) in the error-correction form (20.6.7) is of the form
(20.2.1) with Yt replaced by ∆Yt,
B replaced by Π, and Xt replaced
by Yt−1. The least squares estimators of Π and ΣZZ are
ˆΠ = 1
T
T

t=1
∆YtY′
t−1
	
1
T
T

t=1
Yt−1Y′
t−1

−1
,
(20.7.7)
©2001 CRC Press LLC

S ˆ
Z ˆ
Z = 1
T
T

t−1
ˆZtˆZ′
t = 1
T
T

t=1
∆Yt∆Y′
t −ˆΠ 1
T
T

t=1
Yt−1Y′
t−1 ˆΠ′, (20.7.8)
respectively, where ˆZt = Yt −ˆBYt−1 = ∆Yt −ˆΠYt−1,
t = 1, . . . , T.
The reduced rank regression estimator of Π of rank k is
ˆΠk = S ˆ
Z ˆ
ZF1F′
1 ˆΠ = 1
T
T

t=1
∆YtY′
t−1ˆΓ1ˆΓ′
1,
(20.7.9)
where F1 is deﬁned by (20.4.14) and f1, . . . , fk are the solutions to
(20.4.9) with Xt replaced by Yt−1 and B by Π and ˆΓ1 = (ˆγ1, . . . , ˆγk)
and ˆγ1, . . . , ˆγk are the solutions to (20.4.16) with Yt replaced by ∆Yt
and Xt by Yt−1 [Johansen (1988)]. Note that because we number the
roots in descending order, the n smaller roots of (20.4.10) estimate the
n characteristic roots of Π of 0, which correspond to the n characteristic
roots of B of 1.
The likelihood ratio criterion to test the null hypothesis that the rank
of Π is k is (20.4.9).
20.8
Asymptotic Distribution of Estimators and
Test Criterion
The behavior of the statistics in the cointegrated model is diﬀerent from
that in the stationary model because of the random walk aspect. To
study the behavior we want to distinguish between the stationary and
nonstationary dimensions. Let
Ω1 = (ω1, . . . , ωk),
Ω2 = (ωk+1, . . . , ωp),
(20.8.1)
Ω= (Ω1, Ω2). The vectors ωk+1, . . . , ωp correspond to the characteris-
tic roots 0 of Π (and 1 of B). Then
Ω′
1Π = (Λ1 −I)Ω′
1,
Ω′
2Π = 0.
(20.8.2)
Deﬁne
X1t = Ω′
1Yt,
W1t = Ω′
1Zt,
(20.8.3)
X2t = Ω′
2Yt,
W2t = Ω′
2Zt,
(20.8.4)
Xt = (X′
1t, X′
2t)′, Wt = (W′
1t, W′
2t)′. Then
Xt = ΨXt−1 + Wt,
(20.8.5)
∆Xt = ΥXt−1 + Wt,
(20.8.6)
©2001 CRC Press LLC

where Ψ = Ω′BΩ′−1 and Υ = Ω′ΠΩ′−1 = Ψ −I.
Deﬁne
S∆∆= 1
T
T

t=1
∆Xt∆X′
t,
(20.8.7)
S∆−= 1
T
T

t=1
∆XtXt−1,
(20.8.8)
S−−= 1
T
T

t=1
Xt−1X′
t−1.
(20.8.9)
The least squares estimator of Υ is ˆΥ = S∆−S−1
−−and of Ψ is ˆΨ = ˆΥ+I.
The reduced rank regression estimator of Υ is ˆΥk = S∆−G1G′
1, where
G1 = (g1, . . . , gk) and gj is a solution to
S−∆S−1
∆∆S∆−g = r2S−−g,
g′S−−g = 1.
(20.8.10)
and r2 is a solution to
|S−∆S−1
∆∆S∆−−r2S−−| = 0.
(20.8.11)
To describe the asymptotic distributions of the estimators we intro-
duce the vector Brownian motion process.
Let V1, V2, . . . be inde-
pendently identically distributed with EVt = 0,
EVtV′
t = Σ.
For
0 ≤u ≤1 consider
1
√
T
[T u]

t=1
Vt = √u
1
√
Tu
[T u]

t=1
Vt,
(20.8.12)
which converges in distribution to N(0, uΣ) (Central Limit Theorem).
([Tu] denotes the integer part of Tu.)
Then for 0 ≤w ≤u ≤1
(1/
√
T) [T u]
t=1 Vt −(1/
√
T) [T w]
t=1 Vt
converges
in
distribution
to
N(0, (u −w)Σ) and is asymptotically independent of (20.8.12).
The
sequences satisfy a “tightness condition.” We write
1
√
T
[T u]

t=1
Vt
w→V(u),
0 ≤u ≤1.
(20.8.13)
(The sequence {T −1
2 Vt} converges weakly to {V(u)}.) If Σ = I, we
term V(u) the standard Brownian motion. For more detail see Johansen
(1995), Appendix B.7, for example.
Consider the least squares estimator of Ψ. Let Ψ = (Ψ1, Ψ2), where
Ψ1 has k columns, and similarly partition ˆΨ, Υ, and ˆΥ. Then
©2001 CRC Press LLC

T( ˆΨ2 −Ψ2)
= T( ˆΥ2 −Υ2)
d→
 1
0
dW(u)W′
2(u)
 1
0
W2(u)W′
2(u) du
−1
,
(20.8.14)
where W(u) = [W′
1(u), W′
2(u)]′ is the Brownian motion process for
Σ = ΣW W . Also
√
T( ˆΨ1 −Ψ1) =
√
T( ˆΥ1 −Υ1) has a limiting normal
distribution with the covariance of the limiting distribution of
√
T( ˆψij −
ψij) and
√
T( ˆψkl −ψkl) being σw
ik[(Σ22
−−)−1]jl. Note that the elements of
ˆΨ−Ψ = ˆΥ−Υ that involves the stationary part of X(t) are normalized
by
√
T and have the usual kind of asymptotic normal distribution, but
the part corresponding to the nonstationary part is normalized by T; ˆΨ2
is a supereﬃcient estimator. The eﬀect of the covariance of X2t growing
is to make the estimator more accurate.
The least squares estimators of B and Π are ˆB = (Ω′)−1 ˆΨΩ′ and
ˆΠ = (Ω′)−1ΥΩ′, respectively. The asymptotic behavior of ˆB and ˆΠ
can be derived from that of ˆΨ and ˆΥ.
If the rank of Υ is k, then r2
i
p→0,
i = k + 1, . . . , p.
In fact
dk+1 = Tr2
k+1, . . . , dp = Tr2
p have a limiting distribution, which is the
distribution of the p −k roots of

 1
0
B2(u) dB′
2(u)
 1
0
dB2(v)B′
2(v) −d
 1
0
B2(u)B′
2(u) du
 = 0
(20.8.15)
[Johansen (1988)]. Then the limiting distribution of the test criterion
−2 log λ is the distribution of
tr
 1
0
dB2(v)B′
2(v)
 1
0
B2(u)B′
2(u) du
−1  1
0
B2(u) dB′
2(u). (20.8.16)
This distribution may be compared to the limiting χ2
(p−k)2 for the sta-
tionary process.
The asymptotic behavior of the larger roots of (20.8.11) depends mainly
on the stationary part of Xt, which satisﬁes
∆X1t = Υ11X1,t−1 + W1t.
(20.8.17)
The k larger roots of (20.8.11) converge in probability to the roots of
|Υ11Σ11
−−Υ′
11 −ρ2(Υ11Σ11
−−Υ′
11 + Σ11.2
W W )| = 0,
(20.8.18)
where Σ11
−−= ∞
s=0 Ψs
11Σ11
W W Ψ′
11 and
Σ11.2
W W = Σ11
W W −Σ12
W W (Σ22
W W )−1Σ21
W W .
(20.8.19)
©2001 CRC Press LLC

Let the roots of (20.8.18) be ρ2
1, . . . , ρ2
k. Then limiting distribution of
√
T(s2
1 −ρ2
1), . . . ,
√
T(s2
k −ρ2
k) has been given by Anderson (1999); it is
too complicated to give here.
Let the solutions to
Υ11Σ11
−−Υ11γ1 = ρ2(Υ11Σ11
−−Υ11 + Σ11.2
W W )γ1,
γ′
1Σ11
−−γ1 = 1
(20.8.20)
be γ11, . . . , γ1k, and deﬁne Γ11 = (γ11, . . . , γ1k). Then the solution to
(20.8.10) for r2 = r2
i is an estimator of (γ′
1i, 0)′,
i = 1, . . . , k.
To describe the statistical behavior of the characteristic vectors corre-
sponding to the larger roots it will be convenient to normalize them
diﬀerently.
Note that the reduced rank regression estimator ˆΥk =
S∆−G1G′
1 can be written equivalently as
ˆΥk = S∆−G1G′
11(G1G−1
11 )′,
(20.8.21)
where G1 = (G′
11, G′
21)′. Then G1G−1
11 estimates (I, 0)′, and
TG21G−1
11
d→
 1
0
W2(u)W′
2(u) du
−1
×
 1
0
W2(u) dW′
1.2(u),
(20.8.22)
where
W(u) =

W1(u)
W2(u)

,
k
p −k
(20.8.23)
W1.2(u) = W1(u) −Σ12
W W (Σ22
W W )−1W2(u).
(20.8.24)
Note that this estimator is super-eﬃcient; the error is of the order of
1/T.
References
1. Anderson, T. W. (1951). Estimating linear restrictions on regres-
sion coeﬃcients for multivariate normal distributions. Annals of
Mathematical Statistics 22, 327–351. [Correction, Annals of Statis-
tics 8, (1980), p. 1400.]
2. Anderson, T. W. (1999). Canonical analysis, reduced rank regres-
sion, and cointegration of time series.
3. Anderson, T. W. and Rubin, H. (1949).
Estimation of the pa-
rameters of a single equation in a complete system of stochastic
equations. Annals of Mathematical Statistics 20, 46–63.
4. Box, G. E. P. and Jenkins, G. M. (1976). Time Series: Forecasting
and Control. Holden-Day, San Francisco.
©2001 CRC Press LLC

5. Dhrymes, P. (1988). Time Series, Unit Roots, and Cointegration.
Academic Press, San Diego.
6. Granger, C. W. J. (1981). Some properties of time series data and
their use in econometric model speciﬁcation. Journal of Economet-
rics 16, 121–130.
7. Johansen, S. (1988). Statistical analysis of cointegration vectors.
Journal of Economic Dynamics and Control 12, 231–254.
8. Johansen, S. (1995).
Likelihood-based Inference in Cointegrated
Vector Autoregressive Models. Oxford University Press, Oxford.
9. Koopmans, T. C. (1950). Statistical Inference in Dynamic Eco-
nomic Models. John Wiley & Sons, New York.
©2001 CRC Press LLC

21
On Some Power Properties of
Goodness-of-Fit Tests in Time Series
Analysis
Efstathios Paparoditis
University of Cyprus, Nicosia, Cyprus
ABSTRACT The power properties of a goodness-of-ﬁt test for time
series models are investigated. The test statistic is an L2 functional of the
smoothed diﬀerence between the sample spectral density (periodogram)
rescaled by the hypothesized spectral density and the expected value of
this ratio under the null hypothesis. Diﬀerent classes of local alternatives
are considered and comparisons of the test proposed to some known
goodness-of-ﬁt tests in time series analysis are made.
Keywords and phrases Goodness-of-ﬁt, local alternatives, periodo-
gram, spectral density, time series models
21.1
Testing Spectral Density Fits
Testing the ﬁt of a time series model is an important diagnostic step
in time series analysis. In this paper some power properties of general
goodness-of-ﬁt tests are investigated, i.e., of tests which can be applied
when no a priori information is available about the kind of departure of
the true correlation structure from that postulated under the null.
The class of stochastic processes {Xt, t ∈Z} considered satisﬁes the
following assumption.
(A1)
Xt = ∞
j=−∞ψjεt−j where ψ0 = 1, ∞
j=−∞|j|1/2+β|ψj| <
∞for some β > 0 and {εt} is a sequence of independent identically
distributed random variables with mean zero, positive variance σ2
ε and
Eε8
1 < ∞.
A way to describe the autocorrelation structure of such a process is
©2001 CRC Press LLC

by means of its spectral density function f deﬁned by
f(λ) = σ2
ε(2π)−1|
∞

j=−∞
ψje−iλj|2
for λ ∈[−π, π].
Let F be the set of spectral densities of the class of stochastic processes
described in (A1) and let F ⊂F such that
F =

f :
f is ﬁrst order Lipschitz continuous and
inf
λ∈[0,π] f(λ) > 0

.
Assume that observations X1, X2, . . . , XT of such a process are available
and that a model f0 from this class has been selected to describe the
dependence structure of {Xt}.
We are then interested in testing the
hypothesis
H0 : f = f0
against
H1 : f ̸= f0.
In order to make the presentation self-contained we initially brieﬂy
discuss the idea underlying the test proposed by Paparoditis (1997) and
summarize some basic results. Consider the space (L2[−π, π], < ·, · >),
with the deﬁnition < f, g >= (2π)−1  π
−π f(x)g(x)dx of the inner prod-
uct. Denote by I(λ) the sample spectral density (periodogram) given
by
I(λ) =
1
2πT

T

t=1
Xte−iλt
2
.
(21.1.1)
Usually the periodogram is calculated at the Fourier frequencies λj =
2πj/T, j = −N, −N + 1, . . . , N −1, N where N = [(T −1)/2] and [ · ]
denotes integer part.
Consider the rescaled periodogram statistic QT (·) given by
QT (λ) =
I(λ)
f0(λ) −1.
It is well known (cf.
Priestley (1981), p.
418) that if f ∈F then
E[QT (λ)] = q(λ) + O(log(T)T −1) uniformly in λ where
q(·) :=
f(·)
f0(·) −1



= 0
under H0
̸= 0
under H1.
(21.1.2)
The idea used to test the goodness-of-ﬁt of the spectral density function
f0 is to estimate the function q(·) nonparametrically using the observ-
able random variables QT (λj), j = −N, −N + 1, . . . , N and to compare
©2001 CRC Press LLC

the result obtained with the zero function, i.e., the (asymptotically) ex-
pected form of q(·) under H0. To be more speciﬁc, denote by ˆqh(·) the
nonparametric (kernel) estimator of q(·) given by
ˆqh(λ) = 1
T
N

j=−N
Kh(λ −λj)QT (λj)
(21.1.3)
where Kh(·) = h−1K(·/h). Here
(A2)
K(·) is a bounded, symmetric and nonnegative kernel with
compact support [−π, π] satisfying (2π)−1 
K(x)dx = 1.
Furthermore, we assume that
(A3) The bandwidth h satisﬁes hT δ →1 for some 0 < δ < 1.
The statistic used to test the null hypothesis that f = f0 is then based
on the L2 norm of ˆqh(·), i.e.,
ST,h = Th1/2
 π
−π

ˆqh(λ)
2
dλ.
(21.1.4)
Note that ˆqh(λ) is a mean square consistent estimator of q(λ) since un-
der the assumption made and by standard arguments (cf. for instance,
Priestley (1981( we have that
E[ˆqh(λ)] = q(λ) + o(1)
and
Var[ˆqh(λ)] = O(T −1h−1).
Thus we expect under H0 a small value of ˆqh(λ), i.e., a large value of
ST,h will argue against the null hypothesis. More precisely, the following
theorem has been established in Paparoditis (1997).
THEOREM 21.1.1
Assume (A1)-(A3) and let εt ∼N(0, σ2).
(i)
Under H0 and as T →∞,
L
ST,h −µh(K)
σ(K)

→N(0, 1),
where
µh(K) = 2πh−1/2∥K∥2,
(21.1.5)
©2001 CRC Press LLC

σ(K) =

σ2(K) and
σ2(K) = π−1
 2π
−2π
  π
−π
K(u)K(u + x)du
2
dx.
(21.1.6)
(ii)
Under H1 and if f is the true spectral density, then as T →∞,
T −1h−1/2ST,h →2π
 f
f0
−1

2
in probability,
where ∥· ∥denotes the L2 norm in L2[−π, π].
Note that the asymptotic null distribution of ST,h does not depend on
any unknown parameters or functionals of the process. Modiﬁed appro-
priately, the above test can be also applied to test the hypothesis that
f belongs to a ﬁnite dimensional parametric class of spectral densities,
i.e., that
H0 : f ∈FΘ
against
H1 : f ̸∈FΘ
where
FΘ = {f(·, θ ) and θ ∈Θ ⊂Rp}.
Brieﬂy, let ˆθ be an estimator of θ . The test statistic used to test the
composite hypothesis is given by
ST,h(ˆθ ) = Th1/2
 π
−π

ˆqh(λ, ˆθ )
2
dλ
(21.1.7)
where
ˆqh(λ, ˆθ ) = 1
T
N

j=−N
Kh(λ −λj)
 I(λj)
f(λj, ˆθ )
−1

.
(21.1.8)
It has been shown that if ˆθ is a
√
T-consistent estimator of θ , like
the maximum likelihood or the Whittle estimator, then under the null
hypothesis ST,h(ˆθ ) = ST,h(θ 0) + oP (1) where θ 0 ∈Θ is the true pa-
rameter vector; cf. Paparoditis (1997). That is, the (asymptotic) null
distribution of the test statistic ST,h(ˆθ ) is not aﬀected if the unknown
parameter vector θ is replaced by a
√
T-consistent estimator, i.e., this
distribution is identical to that of testing a simple hypothesis given in
Theorem (21.1.1) (i). However, the limiting behavior of ST,h(ˆθ ) under
©2001 CRC Press LLC

the alternative depends on the particular method applied to obtain the
parameter estimator ˆθ , i.e., in this case we have
T −1h−1/2ST,h(ˆθ ) →2π

f(·)
f(·, θ ⋆) −1

2
in probability
as T →∞, where θ ⋆∈Θ is the minimizer of the distance function
used to obtain the estimator ˆθ . To give an example, if ˆθ is the Whittle
estimator then
θ ⋆= arg min
θ ∈Θ
1
4π
 π
−π

f(λ, θ ) +
f(λ)
f(λ, θ )

dλ;
cf. Dzhaparidze (1986). In the following we conﬁne ourselves to the case
of testing a full speciﬁed hypothesis in order to simplify calculations and
to discuss the power properties of the ST,h test. For more details on
testing a composite hypothesis see Paparoditis (1997).
21.2
Local Power Considerations
As Theorem (21.1.1) shows the test based on ST,h is a consistent test,
i.e., a test which has power against any alternative having spectral den-
sity f ∈F such that f ̸= f0. Furthermore, the power of the test is a
function of the L2 distance between the true spectral density f and the
hypothesized density f0 and approaches unity as T →∞. Apart from
these general properties, however, a more informative analysis of the
power behavior of ST,h can be made by means of local power consider-
ations. The idea is to consider a sequence of stochastic process denoted
by {XT,t, t ∈Z}T ∈N with a corresponding sequence of spectral densities
{fT (·)}T ∈N, fT ∈F that converge to the null density f0 as the sample
size T increases. The interest here is focused on the maximal rate at
which fT is allowed to converge to f0 such that the power of the test is
bounded away from the signiﬁcance level and from unity.
To be more speciﬁc, let {X0,t, t ∈Z} be the process under the null
hypothesis with spectral density f0. Consider the sequence of stochastic
processes {XT,t, t ∈Z}T∈N deﬁned by
XT,t =
∞

j=−∞
ψT,jX0,t−j
(21.2.1)
where the sequence {ψT,j, j ∈Z}T∈N satisﬁes
©2001 CRC Press LLC

∞

j=−∞
|j|1/2+β|ψT,j| < C
and the constant C does not depend on T. Since the process {XT,t}
is obtained by applying a linear ﬁlter to the process {X0,t} its spectral
density is given by
fT (λ) = f0(λ) |ΨT (λ)|2
(21.2.2)
where
|ΨT (λ)|2 =

∞

j=−∞
ψT,j exp{−iλj}

2
.
(21.2.3)
In particular, if ψT,j = δ0j with δ0j Kronecker’s delta, we have XT,t =
X0,t and, therefore, the case of the null hypothesis. Assume that the
sequence {ψT,j, j ∈Z} is such that
ΨT (λ)

2
= 1 + cT ω
λ −λ⋆
bT

+ o(cT bT )
(21.2.4)
where cT and bT are real valued and positive sequences approaching zero
as T →∞, λ⋆is a ﬁxed frequency and the o(cT bT ) term is uniformly in
λ. Furthermore, ω(·) is assumed to be a twice continuously diﬀerentiable
and bounded function. Under these assumptions supλ ||ΨT (λ)|2−1| →0
as T →∞, the spectral density fT (·) of the process {XT,t} is given by
fT (λ) = f0(λ)

1 + cT ω
λ −λ⋆
bT

+ o(cT bT )
(21.2.5)
and
sup
λ∈[−π,π]
fT (λ)
f0(λ) −1
 →0
as T →∞.
Using this approach diﬀerent classes of local alternatives can be con-
sidered.
For instance if we choose bT ≡1 we are in the case of the
so-called Pitman alternatives where the local alternatives approach f0(·)
at the rate cT . Depending on the particular form of ω(·) these depar-
tures can be thought as being of more global nature in the sense that
they not become centered around any particular frequency as the sample
increases. On the contrary, if the sequence bT is also allowed to approach
zero as T →∞then the departure of fT from the hypothesized spectral
density f0 becomes more and more centered around the frequency λ⋆as
T increases. This is the class of sharp peak local alternatives introduced
by Rosenblatt (1975) in the context of density testing based on a sample
of i.i.d. data; see also Ghosh and Huang (1991). The following theorem
characterizes the behavior of the ST,h test statistic for the class of local
alternatives given in (21.2.5).
©2001 CRC Press LLC

THEOREM 21.2.1
Assume that the true spectral density fT satisﬁes (21.2.5( and that the
assumptions of Theorem (21.1.1) are fulﬁlled.
(i)
If bT ≡1 and cT = T −1/2+δ/4, then as T →∞,
L
ST,h −µh(K)
σ(K)

⇒N

ν
σ(K) 1

,
where ν = 2π∥ω∥2 and µh(K) and σ2(K) are given in (21.1.5( and
(21.1.6( respectively.
(ii)
If cT = T −η and bT = T −γ for some η > 0 and γ > 0 such that
δ/2 = 1 −2η −γ
and
γ < δ,
then the conclusion of Part (i) of the theorem is true.
PROOF Note ﬁrst that by deﬁnition we have that XT,j =
∞

j=−∞
aT,jεt−j
where {aT,j, j ∈Z} is an absolute summable sequence. By Theorem
10.3.1 of Brockwell and Davis (1991) we then have for j = 1, 2, . . . , N
that IT (λj) = fT (λj)Uj + RT (λj) where IT (·) denotes the periodogram
of a realization of length T of the process {Xt,T }, Uj is a sequence of
independent standard exponential distributed random variables and the
remainder satisﬁes maxλj∈[0,π] E|RT (λj)|2 = O(T −1). Substituting this
expression for IT (λj) in ST,h we get after some algebra that
ST,h = T −1h1/2
 π
−π

N

j=−N
Kh(λ −λj)fT (λj)
f0(λj) (Uj −1)
2
dλ
+T −1h1/2
 π
−π

N

j=−N
Kh(λ −λj)
fT (λj)
f0(λj) −1
2
dλ + oP (1)
= L1,T + L2,T + oP (1)
with an obvious notation fot L1,T and L2,T . Since supλ |fT (λ)/f0(λ) −
1| →0 we get that
L1,T = T −1h1/2
 π
−π

N

j=−N
Kh(λ −λj)(Uj −1)
2
dλ + oP (1)
and by Lemma 8.3 of Paparoditis (1997) we have that
©2001 CRC Press LLC

L

T −1h1/2
 π
−π

N

j=−N
Kh(λ −λj)(Uj −1)
2
dλ −µh

⇒N(0, σ2
K).
Furthermore, using (21.2.5( we get for the term L2,T that
L2,T = c2
T Th1/2
 π
−π
  π
−π
Kh(λ −x)ω(x −λ⋆)/bT )dx
2
dλ
+o(bT c2
T Th1/2)
= bT c2
T Th1/2
 π
−π
  π
−π
K(u)ω(y −uhb−1
T )du
2
dy
+o(bT c2
T Th1/2).
Now, if bT ≡1 then L2,T →
 π
π ω2(λ)dλ in probability provided cT =
T −1/2+δ/4. For bT = T −γ the conditions δ/2 = 1 −2η −γ and hb−1
T
→0
should be satisﬁed in order for the same convergence to be true.
21.3
Comparisons
Recall that the ST,h test is based on the property that if the postu-
lated model is correct then the (asymptotic) expected value of the ratio
between the sample spectral density (periodogram) and the postulated
spectral density equals one. Thus the proposed test statistic measures
how close is the rescaled periodogram I(λ)/f0(λ) to the (sample) spec-
tral density of a white noise process. An alternative way to evaluate the
basic statistic I(λ)/f0(λ) for testing purposes is by using the cumulative
ratio
GT (x) =
 x
0
I(λ)
f0(λ)dλ.
(21.3.1)
The null hypothesis that f = f0 can then be tested using the normed
diﬀerence GT (τπ) −τGT (π) where the variable τ varies in the interval
[0, 1]. In particular, a statistic useful for testing this null hypothesis is
given by
CT = (2π2)−1T
 1
0

GT (τπ) −τGT (π)
2
dτ,
(21.3.2)
see Dzhaparidze (1986). Note that if {Xt} satisﬁes assumption (A1) then
the process ζT (τ) = (π
√
2)−1√
T(GT (τπ) −τGT (π( converges weakly to
©2001 CRC Press LLC

the Brownian Bridge U(τ), 0 ≤τ ≤1, Dzhaparidze (1986), Proposition
5.1. Thus
L(CT ) ⇒L
  1
0
U 2(τ)dτ

.
(21.3.3)
Note that the asymptotic null distribution of the CT test is identical to
that of the Cram´er-von Mises test based on the integrated squared dif-
ference between the standardized sample spectral distribution function
and the standardized spectral distribution function of the model under
H0; cf. Anderson (1993).
For the class of local alternatives discussed in this paper the asymp-
totic behavior of the CT test is established in the following theorem; cf.
Dzhaparidze (1986).
THEOREM 21.3.1
Assume that (A1) is fulﬁlled and that the true spectral density fT satisﬁes
(21.2.5(.
(i)
If bT ≡1 and cT = T −1/2, then as T →∞,
L(CT ) ⇒L
  1
0
(U(τ) + Ω(τ(2dτ

,
where for τ ∈[0, 1]
Ω(τ) =
1
√
2π
  τπ
o
ω(λ)dλ −τ
 π
0
ω(λ)dλ

.
(ii)
If bT = T −γ for some γ > 0 and cT = T −η, η > 0 such that
η + γ = 1/2 then the concluson of Part (i) of the theorem is true.
Part (i) of the above theorem has been established by Dzhaparidze
(1986), Proposition 5.2, while part (ii) is proved along the same lines.
Theorem (21.3.1) shows that for both classes of local alternatives con-
sidered the CT test is
√
T-consistent, i.e., it has a non trivial power for
local deviations approaching the null at a rate as fast as T −1/2. Now,
compare this with the asymptotic power of the ST,h test stated in The-
orem (21.2.1).
Consider ﬁrst the class of sharp peak alternatives. For this class of
local alternatives and for a suitable choice of δ, the ST,h test can be
made more powerful than the CT test. To see this consider the points
(δ, γ, η) in the set
∆= {(δ, η, γ) : δ > 0, η > 0, γ > 0, δ/2 = 1 −γ −2η,
γ < δ and γ > δ/2}.
©2001 CRC Press LLC

For (δ, η, γ) ∈∆we then have
η + γ = 1
2 + 1
2(γ −δ
2) > 1
2,
i.e., the ST,h test is more powerful than the CT test since it detects local
alternatives converging to the null at a rate faster that T −1/2.
Consider next the class of Pitman alternatives. Since for δ > 0 we
have T −1/2+δ/4 > T −1/2 the ST,h test detects alternatives approaching
the spectral density f0 at a rate smaller than the ‘parametric’ rate T −1/2.
Note that for δ = 1/5 which is the asymptotically optimal rate for choos-
ing δ in the sense of minimizing the mean square error of estimating q(·),
we have T −1/2+δ/4 = T −9/20 which is slightly smaller than T −1/2. Thus
it seems that a price have to be paid for using a nonparametric kernel es-
timator in ST,h, i.e., for smoothing locally the ratio I(λ)/f0(λ) to obtain
ˆqh(λ) instead of integrating this ratio as this is the case of the statistic
GT (τπ). However, this is not entirely true. As we will see, despite its
√
T-consistency the CT test can have even for the class of Pitman alter-
natives very low power in detecting deviations from the null if they are
due to nonvanishing autocorrelation at high lags. In this case the ST,h
test appears to be more powerful. To understand this statement consider
ﬁrst the following proposition which gives a comparable representation
of both statistics under the null hypothesis.
PROPOSITION 21.3.2
Assume that the process {Xt} satisﬁes assumption (A1) and that the null
hypothesis is true. Then
CT =
T
π2
T −1

s=1
1
s2
γ2
ε(s)
σ4ε
+ oP (1).
(21.3.4)
If (A2) is also fulﬁlled, then
ST,h = 4πTh1/2
T −1

s=1
 1
2π
 π
−π
K(u) cos(uhs)du
2 γ2
ε(s)
σ4ε
+ oP (1),
(21.3.5)
where γε(s) = T −1 T −s
t=1 εtεt+s is the sample autocovariance at lag s
based on the i.i.d. series ε1, ε2, . . . , εT .
PROOF Suppose that f = f0.
By Lemma 1 of Dzhaparidze (1986),
p 301, we have that I(λ)/f0(λ) = 2πIε(λ)/σ2
ε + RT (λ)/f0(λ) where
√
T max0≤λ≤π |RT (λ)| = oP (1) and Iε(λ) = (2πT)−1 T
t=1 εt exp{iλt}
is the periodogram of the series ε1, ε2, . . . , εT . Since f0 ∈F it follows for
©2001 CRC Press LLC

every τ ∈[0, 1] that GT (τπ) = 2πσ−2
ε
 τπ
0
Iε(λ)dλ + oP (T −1/2). Substi-
tuting Iε(λ) = (2π)−1 T −1
s=−T +1 ˆγε(s) cos(λh) we get
GT (τπ) =
1
σ2
ε

ˆγε(0)τπ + 2
T −1

s=1
sin(τπs)
s
ˆγε(h)

+ oP (T −1/2).
Therefore,
CT =
T
2π2
 1
0

2
T −1

s=1
sin(τπs)
s σ2ε
ˆγε(s)
2
dτ + oP (1)
= T
π2
T −1

s=1
1
s2
ˆγ2
ε(s)
σ4
ε
+ oP (1).
To obtain the asymptotic expression for the ST,h statistic note ﬁrst that
by substituting the relation I(λj)/f0(λj) = 2πIε(λj)/σ2
ε+RT (λj)/f0(λj)
we get
ST,h =
√
h
T
 π
−π

N

j=−N
Kh(λ −λj)

T −1

s=−T +1
ˆγε(s)
σ2
ε
−1
2
dλ
+
√
h
T
 π
−π

N

j=−N
Kh(λ −λj)RT (λj)
f0(λj)
2
dλ
+2
√
h
T
 π
−π

N

j=−N
Kh(λ −λj)
2πIε(λj)
σ2
ε
−1

×

N

i=−N
Kh(λ −λi)RT (λi)
f0(λi)

dλ
= M1,T + M2,T + M3,T
with an obvious notation for Mi,T , i = 1, 2, 3. Now, since max
0≤λ≤π|RT (λ)| =
oP (T −1/2) we get M2,T = oP (h1/2) while using Lemma 8.4 of Paparodi-
tis (1997) we conclude that M3,T = oP (1). Furthermore,
M1,T =
√
h
T
 π
−π

2
N

j=−N
Kh(λ −λj)
T −1

s=1
ˆγε(s)
σ2
ε
cos(λjh)
2
dλ + oP (1)
where the second term on the right hand side of the above expression is
due to the fact that ˆγε(0)−σ2
ε = O(T −1/2) and that for an i.i.d. sequence
©2001 CRC Press LLC

Cov(ˆγε(s1), ˆγε(s2(= δs1,s2O(T −1); cf. Brockwell and Davis (1991). In
particular, and using these relations we have
√
h
T
 π
−π

N

j=−N
Kh(λ −λj)
 ˆγε(0)
σ2
ε
−1
2
dλ = OP (h1/2)
and
√
h
T
 π
−π

2
N

j=−N
Kh(λ −λj)
T −1

s=1
ˆγε(s)
σ2ε
cos(λjh)

×

N

j=−N
Kh(λ −λj)
 ˆγε(0)
σ2ε
−1

dλ
= 2T −1h1/2OP (T 1/2)OP (T 1/2)
= OP (h1/2).
Approximating the Riemann sum by the corresponding integral and us-
ing the symmetry of K(·) and a trigonometric identity for cos(sλ −suh)
we ﬁnally get that
M1,T = 4Th1/2
 π
−π
 T −1

s=1
 1
2π
 π
−π
Kh(λ −x) cos(xs)dx
 ˆγε(s)
σ2
ε
2
dλ
+oP (1)
= 4Th1/2
 π
−π
 T −1

s=1
 1
2π
 π
−π
K(u) cos(sλ −suh)du
 ˆγε(s)
σ2
ε
2
dλ
+oP (1)
= 4Th1/2
 π
−π
 T −1

s=1
 1
2π
 π
−π
K(u) cos(suh)du

cos(sλ) ˆγε(s)
σ2
ε
2
dλ
+oP (1)
= 4πTh1/2
T −1

s=1
 1
2π
 π
−π
K(u) cos(suh)du
2 ˆγ2
ε(s)
σ4
ε
+ oP (1)
where the last equality follows by the orthogonality properties of cos(·).
The important aspect of Proposition (21.3.2) is that both statistics
considered can be expressed (asymptotically) as a weighted sum of the
©2001 CRC Press LLC

squared error autocorrelations γε(s)/σ2
ε, s = 1, 2, . . . , T −1. (Note that
γε(s)/σ2
ε is not exactly the sample autocorrelation based on ε1, ε2, . . . , εT
since in the denominator the true variance of the error process σ2
ε ≡γε(0)
is used instead of its estimator γε(0)). Now, the weighting sequence of
the CT test is just 1/s2 while that of ST,h is < K(·), cos(sh·) >2, i.e.,
it depends on the particular kernel K and the smoothing bandwidth h
used.
By the severe downweight of the squared autocorrelations ˆγε(s)/σ2
ε at
high lags appearing in the series representation of the CT statistic we
might expect that this test will have low power if the departure from the
null is due to signiﬁcant autocorrelation at high lags.
To proceed with the Pitman alternatives, and if the true spectral den-
sity equals
fT (λ) = f0(λ)

1 + T −1/2ω(λ)

+ o(T −1/2)
we have by Theorem (21.3.1) that L(CT ) →L(
 1
0 (U(τ) + Ω(τ(2dτ). It
follows then that in this case
L(CT ) →L
 ∞

j=1
(Zj + µj)2
(jπ)2

,
(21.3.6)
where the Zj are independent N(0, 1) distributed and µj =
√
2
 1
0 Ω(τ)
sin(jπτ)dτ, Shorack and Wellner (1986). Thus local deviations to the
null contribute to the test statistic by means of the components µj,
j = 1, 2, . . ..
Note that the components µj are related as follows to
the function ω(·) appearing in the local alternatives considered. Using
integration by parts and the deﬁnition of Ω(τ) we get
µj =
√
2
 1
0
Ω(τ) sin(jπτ)dτ
=
√
2
jπ
 1
0
cos(jπτ)dΩ(τ)
=
1
j√π
 1
0
cos(jπτ)ω(πτ)dτ.
Recall that
κ(j) = 2π
 1
0
cos(jπτ)ω(πτ)dτ = 2
 π
0
cos(jλ)ω(λ)dλ
©2001 CRC Press LLC

is the autocavariance at lag j of ω(·) if the real-valued function ω(·) is
a spectral density, i.e., if ω(·) is nonnegative and symmetric on [−π, π]
and

[−π,π] ω(λ)dλ < ∞; cf. Brockwell and Davis (1991).
Now, if the deviations from the null are due to signiﬁcant values of µj
for j > 2 we expect a lower power of the CT test due to the downweight-
ing of these components in the above series representation of this test.
In fact equation (21.3.6( enable us to relate the power behavior of the
CT test to that of the Cram´er-von Mises test in testing hypothesis on
the distribution function in the context of independent samples and to
reproduce several of the results obtained there, see Eubank and LaRic-
cia (1992). For instance, and using arguments similar to those presented
there, we get
inf
∥w∥2 =1 lim
T →∞P

CT ≥cT,α
fT (·) = f0(·)(1 + T −1/2ω(·(

= α
where cT,α denotes the α-quantile of the distribution of CT and
P(A|fT (·) = f0(·)(1 + T −1/2ω(·(
the probability of the event A under the assumption that the true spec-
tral density is given by fT (·) = f0(·)(1 + T −1/2ω(·(. Losely speaking,
this shows that if the deviation from the null is placed at a suﬃciently
high value of µj, the power of the CT test can degenerate to its level.
In contrast to the ST,h test the power of the CT test is, therefore, not a
monotone increasing function of ∥ω∥.
We conclude the comparison between both test by means of a small
simulation experiment. Series of length T = 128 from the process Xt =
εt + θεt−q have been generated where εt ∼N(0, σ2
ε) with σ 2
ε = 1/(1 +
θ 2) and θ = 0.4.
The null hypothesis is H0 : f = 1/(2π), i.e., the
hypothesis that the observed series is a white noise sequence. Several
values of q have been used in order to study the power behavior of both
tests for diﬀerent alternatives.
Note that Var(Xt) = 1 and that the
autocorrelation function of the process under the alternative is given by
E(XtXt+s) = δs,qθ/(1+θ 2) for s ∈N, i.e., the value of q corresponds to
the lag of the autocorrelation which causes the departure from the null.
The proportion of rejections in 1000 samples are reported in Table
21.1 for two diﬀerent values of the level α. The critical values of the CT
test were taken from Shorack and Wellner (1986) while those of the ST,h
test have been calculated using the bootstrap procedure proposed by
Paparoditis (1997) based on 1000 bootstrap samples. The later test has
been applied using the Bartlett-Priestley kernel and two diﬀerent values
of the smoothing bandwidth h.
The ST,h test has been also applied
using a cross-validation criterion to select the smoothing parameter h.
©2001 CRC Press LLC

TABLE 21.1
Proportion of rejections in 1000 samples of the hypothesis of white
noise
α = 0.05
α = 0.10
q = 0  
q = 1  
q = 3  
q = 6
q = 0  
q = 1  
q = 3  
q = 6
CT
0.046 
0.984 
0.205 
0.101
0.088 
0.993 
0.397 
0.187
ST,h∗
0.051 
0.932 
0.612 
0.362
0.110 
0.983 
0.821 
0.592
ST,h
h = 0.08
0.045 
0.622 
0.511 
0.323
0.095 
0.830 
0.734 
0.540
h = 0.12
0.047 
0.815 
0.639 
0.241
0.091 
0.940 
0.839 
0.435
In particular, and following Beltr˜ao and Bloomﬁeld (1987) we select h
as the minimizer of the function
CV (h) = 1
N
N

j =1

log ˆr−j(λj) + J(λj)
ˆr−j(λj)

where J(λj) = I(λj)/f0(λj),
ˆr−j(λj) = T −1 
s∈Nj
Kh(λj −λs)I(λs)/f0(λs)
and Nj = {s : −N ≤s ≤N and j −s ̸= ±jmodN}. That is, ˆr−j(·)
is the kernel estimator of f(·)/f0(·) when we delete the jth point. The
ST,h test applied with a data driven bandwidth selection rule is denoted
by ST,h∗.
The results of Table 21.1 conﬁrm our asymptotic analysis. As it is
seen the tests maintain the level and the CT test outperforms the ST,h
test for q = 1. Recall that for this value of q the departure from the
null is caused by a nonvanishing ﬁrst order autocorrelation. However,
the power of the CT test drops oﬀdrastically for the values q = 3 and
q = 6 since for these values of q the departure from the null is due to a
signiﬁcant autocorrelation at higher lags, e.g., lag 3 and 6 respectively.
For these values of q the ST,h test clearly outperforms the CT test and
its power drops more gradually as q increases.
Acknowledgements The author is grateful to the referee for his helpful
comments.
©2001 CRC Press LLC

References
1. Anderson, T. W. (1993). Goodness of ﬁt tests for spectral distri-
butions. Annals of Statistics 21, 830–847.
2. Beltr˜ao, K. I. and Bloomﬁeld, P. (1987). Determining the band-
width of a kernel spectrum estimate. Journal of Time Series Anal-
ysis 8, 21–38.
3. Brockwell, P. and Davis, R. (1991).
Time Series: Theory and
Methods, Second Edition. Springer-Verlag, New York.
4. Dzhaparidze, K. (1986).
Parameter Estimation and Hypothesis
Testing in Spectral Analysis of Stationary Time Series. Springer-
Verlag, New York.
5. Eubank, R. L. and LaRiccia, V. N. (1992). Asymptotic comparison
of Cram´er-von Mises and nonparametric function estimation tech-
niques for testing goodness-of-ﬁt. Annals of Statistics 20, 2071–
2086.
6. Ghosh, B. K. and Huang, W.-M. (1991). The power and optimal
kernel of the Bickel-Rosenblatt test for goodness of ﬁt. Annals of
Statistics 19, 999–1009.
7. Paparoditis, E. (1997). Spectral density based goodness-of-ﬁt tests
in time series analysis. Scandinavian Journal of Statistics, forth-
coming.
8. Priestley, M. B. (1981). Spectral Analysis and Time Series. Aca-
demic Press, New York.
9. Rosenblatt, M. (1975). A quadratic measure of deviation of two-
dimensional density estimates and a test of independence. Annals
of Statistics 3, 1–14.
10. Shorack, G. R. and Wellner, J. A. (1986). Empirical Processes with
Applications to Statistics, John Wiley & Sons, New York.
©2001 CRC Press LLC

22
Linear Constraints in a Linear Model
Somesh Das Gupta
Indian Statistical Institute, Calcutta, India
ABSTRACT It is shown by geometric arguments that the reduction
of a linear model µ = E(Y ) = Aθ by the linear constraints L′θ = 0 can
be equivalently obtained by considering the linear constraints L′
1θ = 0,
where the columns of L1 span the space M ∩L, M and L being the
column spaces of A′ and L, respectively. If t = dim(M∩L) is zero, then
the model is unchanged by the linear constraints. It is not necessary to
assume that the rows of L′θ are linearly estimable, to start with. It may
be noted that even when all the rows of L′θ are not linearly estimable,
there may exist some linear combinations of the rows of L′θ which are
linearly estimable.
Keywords and phrases Linear models, linear constraints, eﬀective-
ness of linear constraints
22.1
Introduction
Consider the linear model
EY = µ = Aθ,
(22.1.1)
where A : n × m is a known matrix of rank r, and θ ∈Rm, along with
the linear constraints given by
L′θ = 0,
(22.1.2)
where L : m × s is a known matrix of rank s.
The constrained linear model has been considered by Rao (1973), and
Wang and Chao (1994), in particular. The algebraic treatment presented
©2001 CRC Press LLC

by the above authors fails to clarify the role or impact of the constraints
adequately. Rao (1973) has posed the reduced model, combining (22.1.1)
and (22.1.2), as
µ = AMτ, τ ∈Rq,
(22.1.3)
where M : m × q is a matrix of rank q = m −s such that L′M = 0.
Although the problem of identiﬁability of θ can be resolved by consider-
ing A-equivalence (i.e. θ1 and θ2 are equivalent, if Aθ1 = Aθ2), it is not
clear how the impact of (22.1.2) is reﬂected in (22.1.3).
Usually the rows of L′θ are assumed to be linearly estimable.
An
equivalent assumption is N ⊂D, or the inverse image of A(D) on Rm is
D, where
D = {θ ∈Rm : L′θ = 0} = {θ ∈Rm : θ = Mτ, τ ∈Rq} , (22.1.4)
N = {θ ∈Rm : Aθ = 0}
(22.1.5)
As a consequence of this assumption, it is possible to express the reduced
model given by
µ ∈V0 = {µ ∈Rn : µ = Aθ, L′θ = 0}
(22.1.6)
as µ ∈V = C(A), the column space of A, along with µ⊥V1, where V1
is a subspace of V satisfying V1 = C(B), E(B′Y ) = L′θ.
The above
assumption ensures the existence of such a matrix B.
In case all or some of the rows of L′θ are not linearly estimable, it is not
apparently possible to get such a simple reduction. It is seen that (22.1.3)
leads to a similar speciﬁcation; however, the relationship between V1
and L cannot be clearly revealed through (22.1.3). We shall present a
geometric description of the role or impact of the linear constraints, and
relate V1 to L geometrically.
©2001 CRC Press LLC

22.2
Geometric Interpretation of the Role of the
Linear Constraints
Let
M = C(A′), L = C(L), M1 = M ∩L,
(22.2.1)
t = dim(M ∩L),
(22.2.2)
and M0 be the orthogonal complement of M1 in M. It is clear that
0 ≤t ≤min(s, r). We assume r > 0, s > 0 excluding the trivial cases.
Our main result is given as follows.
THEOREM 22.2.1
(a) If t = 0,
{µ ∈Rn : µ = Aθ, θ⊥L} = {µ ∈Rn : µ = Aθ, θ ∈M}
= {µ ∈Rn : µ = Aθ, θ ∈Rm} (22.2.3)
(b) If 0 < t < s, then dim (V0) = r −t > r −s, and
{µ ∈Rn : µ = Aθ, θ⊥L} = {µ ∈Rn : µ = Aθ, θ ∈M0}
= {µ ∈Rn : µ = Aθ, θ⊥M1}
(22.2.4)
(c) If t = s, then dim (V0) = r −s, M1 = L, and
{µ ∈Rn : µ = Aθ, θ⊥L} = {µ ∈Rn : µ = Aθ, θ ∈M0} (22.2.5)
Note 1
If t = 0, the model µ = Aθ, θ ∈Rm, is unchanged by the linear con-
straints L′θ = 0. We call this case “ineﬀective”. In this case, no non-
trivial linear combination of L′θ is linearly estimable.
If 0 < t < s,
some of the rows of L′θ are not linearly estimable. But there exist some
linear combinations of the rows of L′θ which are linearly estimable. We
call this case “partially eﬀective”, since the reduction in the model by
the constraints θ⊥L is also accomplished by the constraint θ⊥M1, M1
being a proper vector subspace of L; note that dim (V0) = r −t > r −s,
s being the rank of L. If t = s, the resulting reduction in the model
cannot be accomplished by the constraint θ⊥L1 for any proper vector
subspace L1 of L; note that dim (V0) = r −s. We call this case “fully
eﬀective”.
©2001 CRC Press LLC

Following Rao (1973) it may be seen that
t = rank (A) + rank (L) −rank (A′...L),
(22.2.6)
and
rank(AM) = r −t,
(22.2.7)
where M is deﬁned in (22.1.3). Note that t > 0 if m < r + s
The above theorem is proved using the following lemmas.
LEMMA 22.2.2
M ∩L = {θ ∈Rm : θ ∈M, θ⊥PM(D)} ,
(22.2.8)
where PM is the operator (or the symmetric idempotent matrix) corre-
sponding to the orthogonal projection on M, and D is given in (22.1.4).
PROOF First note that any vector d ∈Rm can be expressed as d = α+γ,
α ∈M, γ ∈N. Moreover, PM(d) = α. For a non-zero vector α0
α0 ∈M ∩L ⇔α0 ∈M, α0⊥D ⇔α0 ∈M, α0⊥PM(d) for all d ∈D,
(22.2.9)
since d −PM(d) ∈N and α0 ∈M ⇔α0⊥N.
Lemma 22.2.2 simply states that PM(D) = M0, the orthogonal com-
plement of M1 in M. The following lemma is an easy consequence of
the fact that d −PM(d) ∈N for any d ∈Rm.
LEMMA 22.2.3
For any vector subspace D in Rm
{µ ∈Rn : µ = Aθ, θ ∈D} = {µ ∈Rn : µ = Aθ, θ ∈PM(D)}
(22.2.10)
Under the linear constraints L′θ = 0 the eﬀective domain of θ is D.
The following lemmas give descriptions of the structure of the set D,
which are of interest on their own. The vector space spanned by a set of
vectors Ui’s is denoted by S{U ′
is}.
©2001 CRC Press LLC

LEMMA 22.2.4
Suppose M ∪L = L, i.e L ⊂M. Then
D = M0 ⊕N,
(22.2.11)
where M0 is the orthogonal complement of L (or M1) in M.
PROOF First note that any vector θ ∈N or any vector θ ∈M0 satisﬁes
L′θ = 0. Next note that dim(D) = m −s, and dim(M0 ⊕N) = (r −
s) + (m −r) = (m −s).
LEMMA 22.2.5
Suppose 0 < t ≤min(r, s). Then
D = S{α∗
t+1 + γ∗
t+1, . . . , α∗
r + γ∗
r} ⊕N2
(22.2.12)
where {α∗
t+1, . . . , α∗
r} is an orthonormal basis of M0, γi’s are speciﬁc
vectors in a vector subspace N1 of N of dimension s −t, and N2 is the
orthogonal complement of N1 in N.
PROOF Let {α∗
1, . . . , α∗
t , . . . , α∗
r} be an orthonormal basis of M so that
{α∗
1, . . . , α∗
t } is a basis of M1 = M∩L, and {α∗
1, . . . , α∗
t , αt+1+γt+1, . . . ,
αs + γs} is a basis of L, where αi’s are in M and γi’s are in N.
Suppose
s

t+1
diγi = 0 for some di’s. Then
s

t+1
di(αi + γi) =
s

t+1
diαi,
which belongs to M1. By the choice of the above basis of L, it follows
that
s

t+1
diαi = 0. Consequently,
s

t+1
di(αi + γi) = 0, which implies that
di’s are all 0. Hence γi’s are linearly independent.
Write
U1 = (α∗
1, . . . , α∗
t ), U2 = (α∗
t+1, . . . , α∗
r), V = (γt+1, . . . , γs).
Then
(αt+1, . . . , αs) = U1C1 + U2C2
for some matrices C1 and C2. It can be checked that the columns of
U2 −V (V ′V )−1C′
2
are orthogonal to each of the vectors in the above basis of L. Write
−V (V ′V )−1C′
2 = (γ∗
t+1, . . . , γ∗
r)
©2001 CRC Press LLC

It is easy to see that the vectors {α∗
t+1 + γ∗
t+1, . . . , α∗
r + γ∗
r} are linearly
independent.
Let N2 be the orthogonal complement of S{γt+1, . . . , γs} ≡N1 in N.
Then dim(N2) = (m −r) −(s −t), and N2 is orthogonal to L, as well
as to S{α∗
t+1 + γ∗
t+1, . . . , α∗
r + γ∗
r}. This yields the desired result, since
dim (D) = m −s = {(m −r) −(s −t)} + (r −t).
Proof of the theorem The theorem essentially follows from Lemma
22.2.2 and Lemma 22.2.3 or from Lemmas 22.2.3-22.2.5, as shown below.
(a) It is clear from Lemma 22.2.3 that the reduced model can be
expressed as
{µ ∈Rn : µ = Aθ, θ ∈PM(D)} ,
where PM(D) = M0, the orthogonal complement of M ∩L in M. By
Lemma 22.2.2, dimPM(D) = r. Thus M0 = M. Lemma 22.2.3 now
yields the result.
(b) 0 < t < s. Let {l1, . . . , ls} be a basis of L such that {l1, . . . , lt} is
a basis of M1 = M ∩L. Write
L1 = (l1 . . . lt), L2 = (lt+1 . . . ls).
Then
L′θ = 0 ⇔L′
1θ = 0, L′
2θ = 0.
Let D1 and D2 be the sets of θ ∈Rm for which L′
1θ = 0 and L′
2θ = 0,
respectively. Note that C(L1) ⊂M and C(L2) ∩M = {0}. By Lemmas
22.2.4 and 22.2.5, we ﬁnd that
PM(D1) = M0, PM(D2) = M.
It is easy to check that
PM(D1 ∩D2) = PM(D1) ∩PM(D2) = M0.
To see the above, note that it follows from the structure of D1 and D2,
described in (22.2.11) and (22.2.12), that given α ∈PM(D1) ∩PM(D2)
there exists a θ ∈D1 ∩D2 for which PM(θ) = α.
This yields the result.
(c) If t = s, M1 = L. Lemma 22.2.3 yields the result. It can also be
seen from Lemma 22.2.4 that PM(D) = M0.
©2001 CRC Press LLC

Note 2
Rao (1973) and Wang and Chao (1994) have considered the problem
of minimizing (Y −Aθ)′(Y −Aθ) using Lagrange multipliers for the
constraints L′θ = 0.
It is possible to ﬁnd constraints L′
1θ = 0 and
L′
2θ = 0 such that C(L1) ∈M, C(L2) ∩M = {0}, and
L′θ = 0 ⇔L′
1θ = 0, L′
2θ = 0.
Using the structure of L2, as given in the proof of Lemma 4, it can be
seen that the value of Aθ, θ being a solution of the normal equations
with the constraints L′
1θ = 0, is the same as Aˆθ where ˆθ is a solution of
normal equations with constraints L′
1θ = 0 and L′
2θ = 0, or equivalently
L′θ = 0. This is the algebraic aspect of our result.
Note 3
Roy (1958) considered only the linear hypothesis H0 : L′θ = 0 such that
the rows of L′θ are all linearly estimable. He called such a hypothesis
“testable”. However, it is not necessary to make this distinction. If the
constraints L′θ = 0 are ineﬀective then the hypothesis H0 is vacuous.
On the other hand, if L′θ = 0 is partially eﬀective, then H0 is equivalent
to L′
1θ = 0 where C(L1) = M ∩L.
Note 4
The results presented in this note merely gives geometric description
of the role of the linear constraints, and apparently provide no help
in the calculations needed for relevant statistical inference.
Given a
linear parametric function l′θ, we generally check whether l′θ is linearly
estimable from the simple or patterned structure of the design matrix
A. Otherwise, one has to resort to the sweep-out method on (A′...l) to
ﬁnd out whether l ∈C(A′). We can also use the structure of A or the
sweep-out method to ﬁnd the value of t, and in case 0 < t ≤s, we can
also get a basis of M ∩L. This, in turn, will avoid solving the normal
equations with Lagrange multipliers.
Note 5
Eaton et al. (1969) have considered the following model
{µ ∈Rn : Tµ = Sβ, β ∈Rp},
(22.2.13)
where T : m × n and S : m × p are known matrices; they have intro-
duced this model as a generalization of the classical linear model. With
©2001 CRC Press LLC

reference to the constrained linear model considered in this note, it can
be seen that there exist linear transformations
T : µ ∈Rn →θ ∈M, S : β ∈Rr−t →θ ∈M0
such that (22.2.15) holds. As a matter of fact, (22.2.15) is equivalent to
µ ∈V1 ⊕V2, where V1 is a known vector subspace of Rn and V2 is a
known proper vector subspace of the orthogonal complement of V1 in Rn.
It can be seen that any linear model can be expressed in the above form,
and (22.2.15) cannot be considered as a generalization of the classical
linear model.
Note 6
Suppose E(Y ) = Aθ and Cov(Y ) = Σ, which is singular of rank q, and
known except for a scale multiple. Without any loss of generality we
may assume that

=

σ2Iq
0
0
0

Let
Y =

Y1
Y2

q
n −q , A =

A1
A2

q
n −q .
then the model reduces to E(Y1) = A1θ, Cov(Y1) = σ2Iq, Y2 = A2θ. A
solution of Y2 = A2θ can be expressed as θ0 + τ, where θ0 is a particular
solution and A2τ = 0. Write Y ∗
1 = Y1 −A1θ0. Then the model reduces
further to
E(Y ∗
1 ) = A1τ, Cov(Y ∗
1 ) = σ2Iq, A2τ = 0.
This is a linear model with linear constraints.
Lastly, we present three simple examples to illustrate our result.
Example 1
µ1 = θ1+θ2, µ2 = 2(θ1+θ2); θ1, θ2 in R. Let the constraint be θ1−θ2 = 0.
The model is unaﬀected by the constraint.
Example 2
µ1 = θ1 + θ2, µ2 = θ3 + θ4; θi’s are in R.
Let the constraints be
θ1 = θ2 = θ3 = 0. Note that θ1, θ2, θ3 are not linearly estimable, but
θ1 + θ2 is. We get the some reduction of the model by the constraint
θ1 + θ2 = 0.
©2001 CRC Press LLC

Example 3
Consider the block-treatment experiment with the additive model
µij = γ + τi + βj; i = 1, . . . , t; j = 1, . . . , b.
Then it is well known that the constraints

i
τi = 0,

j
βj = 0 do not
change the model. One can check that no non-trivial linear combination
of

i
τi and

j
βj is linearly estimable.
References
1. Eaton, M. L., Eckles, J. E., and Morris, C. N. (1969). Estimation
for a generalization of the usual linear statistical model. The Rand
Corporation Memo. RM-6078-PR, Santa Monica, California.
2. Rao, C. R. (1973). Linear Statistical Inference and Its Applica-
tions. John Wiley & Sons, New York.
3. Roy, S. N. (1958). Some Aspects of Multivariate Analysis. Asia
Publishing House, Calcutta.
4. Wang, S. G. and Chao, S. C. (1994).
Advanced Linear Models.
Marcel Dekker, New York.
©2001 CRC Press LLC

23
M-methods in Generalized Nonlinear
Models
Antonio I. Sanhueza and Pranab K. Sen
Universidad de La Frontera, Temuco, Chile
University of North Carolina, Chapel Hill, NC
ABSTRACT In generalized nonlinear (regression) models (GNLM),
maximum likelihood (ML) or quasi-likelihood (QL) methods based sta-
tistical inference procedures are generally not robust against outliers or
error contamination. In fact, nonrobustness is more likely to be com-
pounded by the choice of link functions as needed for GNLM’s. For this
reason, robust procedures based on appropriate M-statistics are consid-
ered here. Our proposed M-estimators and M-tests, formulated along the
lines of generalized least squares procedures, are relatively more robust
and their (asymptotic) properties are studied.
Keywords and phrases Asymptotic normality, eﬃciency, estimating
equations, generalized additive models, M-estimators, M-tests, robust-
ness, uniform asymptotic linearity
23.1
Introduction
Let us consider the (univariate) nonlinear regression model
Yi = f(xi, β) + ei, i = 1, ..., n
(23.1.1)
where Yi are the observable random variables (r.v.), xi = (x1i, x2i, ...,
xmi)t are known regression constants, β = (β1, β2, ..., βp)t is a vector
of unknown parameters, f( , ) is a (nonlinear) function (of β) of spec-
iﬁed form; and the errors ei are assumed to be independent r.v.s with
mean 0, but not necessarily identically distributed. We assume that the
distribution of Yi is in the exponential family with density:
gYi(y, θi, φ) = c(y, θi) exp{(y θi −b(θi))/a(φ)}, i = 1, ..., n,
(23.1.2)
©2001 CRC Press LLC

where φ (> 0) is a nuisance scale parameter, the θi are the parameters
of interest, and the functional forms of a(·), b(·) and c(·) are assumed to
be known. In this context, the mean and variance of Yi are easily shown
[see McCullagh and Nelder (1989, pp. 28–29)] to be:
µi = E Yi = b′(θi) = µi(θi)
(23.1.3)
V ar Yi = a(φ) (∂/∂θi)µi(θi) = a(φ) vi(µi(θi)).
(23.1.4)
In that way, we let:
b′(θi) = f(xi, β)
and say that Yi follows a generalized nonlinear model (GNLM). Hence,
the variance of Yi is given by:
V ar Yi = a(φ) vi(f(xi, β)),
which remains the same for all i = 1, ..., n.
If we let f be a monotone diﬀerentiable function, such that:
f(xi, β) = f(xt
iβ),
the r.v. Yi follows the generalized linear model (GLM) introduced by
Nelder and Wedderburn (1972). Also, when
f(xi, β, Zi) = f(xt
iβ) +
q

j=1
hj(Zij)
where the hj are smooth (possibly nonlinear) functions dependent only
upon the coordinate Zij of the auxiliary explanatory vector Zi, we note
that Yi follows the generalized additive model (GAM) [Hastie and Tib-
shirani (1990)].
In the context of the exponential family, the estimating equation for
the maximum likelihood estimator (MLE) ˆβn in model (23.1.1) is the
form:
n

i=1
1
vi(f(xi, ˆβn))
(Yi −f(xi, ˆβn)) fβ(xi, β) = 0,
(23.1.5)
where fβ(xi, β) = (∂/∂β)f(xi, β), which are exactly of the form of the
weighted least square equations for model (23.1.1). Thus, we can use
the iteratively reweighted least square (IRWLS) procedure to implement
maximum likelihood estimation of β.
We may deﬁne the expected value and variance of Yi as in (23.1.2)
and (23.1.3), respectively, where vi(·) in (23.1.3) have known form and φ
is a possibly unknown positive scalar constant, but we make no speciﬁc
©2001 CRC Press LLC

assumption on the distribution of Yi. In this context, we can make use of
the quasilikelihood (QL) approach [Wedderburn (1974) and McCullagh
(1983)], and see that the QL equations for estimating β in (23.1.1) are
the same as given in (23.1.4). This allows us to incorporate those cases
where the functional form of the variance of the response variable is
known, though it may depend on the parameters of interest in the model.
Incorporating the ML and QL equations, we formulate suitable M-
estimators of β, and study their (asymptotic) properties (including con-
sistency and asymptotic normality).
M-tests are also considered for
testing suitable hypotheses on β. Two computational algorithms for M-
estimators based on Newton-Rapson and Fisher’s scoring methods are
presented. In Section 2, preliminary deﬁnitions and regularity conditions
are presented. Section 3 deals with the asymptotic distribution theory
of M-estimators of β, and in this respect, a uniform asymptotic linearity
result on M-statistics (in the regression parameter) is presented in detail.
Section 4 is devoted to related M-tests, and two iterative computational
methods are considered.
23.2
Deﬁnitions and Assumptions
Considering that the density function of Yi in (23.1.1) given by (23.1.2)
leads to weighted least square, we deﬁne an M-estimator of β as the
minimization:
ˆβn = Arg · min
 n

i=1
1
h[(vi(f(xi, β))] h2(Yi −f(xi, β)) : β ∈Θ ⊆ℜp
(23.2.1)
where h(·) is a real valued function, and Θ is a compact subset of ℜp.
Assuming that the variance function in the equation (23.2.1) is ﬁxed
and ψ(z) = (∂/∂z)h2(z), we have that the estimating equation for the
minimization in (23.2.1) is given by:
n

i=1
λ(xi, Yi, ˆβn) = 0
(23.2.2)
where
λ(xi, Yi, β) =
1
h[v(f(xi, β))] ψ(Yi −f(xi, β)) fβ(xi, β).
(23.2.3)
In particular, if we let h(z) = z, we have the ML and QL equations for
estimating β as given by (23.1.4). In the conventional setup of robust
methods [Huber (1981), Hampel et al. (1985), Jureˆckov´a and Sen (1996),
©2001 CRC Press LLC

and others], we will primarily use bounded and monotone functions h(·);
the so called Huber-score function corresponds to
h(z) =

1
√
2 z
if |z| ≤k
{k[|z| −k
2]}
1
2
if |z| > k
(23.2.4)
for suitable chosen k (0 < k < ∞). Whenever the errors ei in (23.1.1)
have a symmetric distribution, we may choose k as a suitable percentile
point of this law (say the 90th or 95th percentile), and let h be as in
(23.2.4). However, whenever the errors may not have a symmetric law,
we choose two values k1 and k2 (k1 < 0 < k2), and let
h(z) =





1
√
2 z
if k1 ≤z ≤k2
{k1[z −k1
2 ]}
1
2
if z ≤k1 (< 0)
{k2[z −k2
2 ]}
1
2
if z ≥k2 (> 0),
where k1 and k2 are chosen so that Eψ(Yi −f(xi, β)) = 0. This can
always be made if the distribution of ei is known up to a scalar factor
(that may depend on xt
iβ).
We make the following sets of regularity assumptions concerning (A)
the score function ψ, (B) the function f(·), and (C) the functions k1(·) and
k2(·), where
k1(xi, β) = 1/h[v(f(xi, β))],
(23.2.5)
and
k2(xi, β) = h′[v(f(xi, β))] v′(f(xi, β)
h2[v(f(xi, β))]
.
(23.2.6)
[A1]:
ψ is nonconstant, absolutely continuous and diﬀerentiable with respect
to β.
[A2]: Let e = Y −f(x, β),
(i) E ψ2(e) < ∞, and E ψ(e) = 0.
(ii) E
		ψ′(e)
		1+δ < ∞for some 0 < δ ≤1, and Eψ′(e) = γ (̸= 0)
[A3]:
(i) limδ→0 E

Sup

∆


≤δ
			ψ(Y −f(x, β + ∆)) −ψ(Y −f(x, β))
			

= 0
(ii) limδ→0 E

Sup

∆


≤δ
			ψ′(Y −f(x, β + ∆)) −ψ′(Y −f(x, β))
			

= 0
©2001 CRC Press LLC

[B1]:
f(x, β) is continuous and twice diﬀerentiable with respect to β ∈Θ,
where Θ is a compact subset of ℜp.
[B2]:
(i) limn→∞1
n Γ1n(β) = Γ1(β), where
Γ1n(β) =
n

i=1
{
1
h[v(f(xi, β))]fβ(xi, β) f t
β(xi, β)},
and Γ1(β) is a positive deﬁnite matrix.
(ii) limn→∞1
n Γ2n(β) = Γ2(β), where
Γ2n(β) =
n

i=1
{
u(xi)
h2[v(f(xi, β))]fβ(xi, β) f t
β(xi, β)},
and Γ2(β) is a positive deﬁnite matrix.
(iii) max

u(xi)
h2[v(f(xi,β))] f t
β(xi, β) (Γ2n(β))−1 fβ(xi, β)

−→0, as n →
∞
[B3]:
(i) limδ→0 Sup

∆


≤δ
			∂/∂βj )f(x, β + ∆) (∂/∂βk)f(x, β + ∆)−
(∂/∂βj )f(x, β) ∂/∂βk)f(x, β)
			 = 0; k = 1, ..., p.
(ii) limδ→0 Sup

∆


≤δ
			(∂2/∂βj ∂βk)f(x, β+∆)−(∂2/∂βj ∂βk)f(x, β)
			 =
0 for j, k = 1, ..., p.
[C]:
(i) limδ→0 Sup

∆


≤δ
			 k1(x, β + ∆) −k1(x, β)
			 = 0, uniformly in x.
(ii) limδ→0 Sup

∆


≤δ
			k2(x, β + ∆) −k2(x, β)
			 = 0, uniformly in x.
23.3
Asymptotic Results
In the ﬁrst result we shall prove the uniform asymptotic linearity on
M-statistics given in the following theorem.
©2001 CRC Press LLC

THEOREM 23.3.1
Under the conditions [A1]-[A3], [B1]-[B3], and [C] we have that:
Sup

t


≤C



 1
√n
n

i=1
{λ(xi, Yi, β + n−1
2 t) −λ(xi, Yi, β)} + γ
n Γ1n(β) t



= op(1)
(23.3.1)
as n →∞, where λ(xi, Yi, β) was deﬁned in (23.2.3).
PROOF We consider the jth element of the vector λ(xi, Yi, β) denoted
for
λj(xi, Yi, β) =
1
h[v(f(xi, β))]ψ(Yi −f(xi, β)) fβj(xi, β), j = 1, ..., p,
where fβj(xi, β) = (∂/∂βj)f(xi, β).
By using linear Taylor expansion, we have:
λj(xi, Yi, β + n−1
2 t) −λj(xi, Yi, β)
=
1
√n
p

k=1
tk{(∂/∂βk)λj(xi, Yi, β)}
+ 1
√n
p

k=1
tk{(∂/∂βk)λj(xi, Yi, β + ht
√n) −(∂/∂βk)λj(xi, Yi, β)},
where
(∂/∂βk)λj(xi, Yi, β)
= k1(xi, β) {ψ(Yi −f(xi, β)) (∂2/∂βk ∂βj)f(xi, β)
−ψ′(Yi −f(xi, β)) fβj(xi, β)fβk(xi, β)}
−k2(xi, β) ψ(Yi −f(xi, β)) fβj(xi, β) fβk(xi, β)
and k1(xi, β) and k2(xi, β) are given in (23.2.4) and (23.2.5), respec-
tively.
Then for each j = 1, ..., p we have that:
©2001 CRC Press LLC

Sup

t


≤C
				
1
√n
n

i=1
{λj(xi, Yi, β + h t
√n) −λj(xi, Yi, β)}
+ γ
n
n

i=1
p

k=1
{tk
1
h[v(f(xi, β))] fβj(xi, β)fβk(xi, β)
				
≤Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk{(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)}
			
+Sup

t


≤C
			 1
n
n

i=1
p

k=1
{tk(∂/∂βk)λj(xi, Yi, β)}
+γ
n
n

i=1
p

k=1
tk
1
h[v(f(xi, β))] fβj(xi, β)fβk(xi, β)
			,
where
Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk{(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)}
			
≤1
nC
n

i=1
p

k=1
Sup

t


≤C
			(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)
			.
Now, based on conditons [A3] (i)–(ii), [B3] (i)–(ii), and [C3] (i)–(ii)
we may prove that:
E

Sup

t


≤C
			(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)
			

−→0,
∀i ≤n,
and
©2001 CRC Press LLC

E

Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk {(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)}
			

−→0.
Also,
V ar

Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk{(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)}
			

≤1
n2 C2
n

i=1

V ar
 p

k=1
Sup

t


≤C
			(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)
			

≤C2 K/n −→0.
Therefore:
Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk{(∂/∂βk)λj(xi, Yi, β + h t
√n)
−(∂/∂βk)λj(xi, Yi, β)}
			 = op(1).
(23.3.2)
In addition, we may have,
Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk(∂/∂βk)λj(xi, Yi, β)
+γ
n
n

i=1
p

k=1
tk
1
h[v(f(xi, β))] fβj(xi, β)fβk(xi, β)
			
= Sup

t


≤C
			 1
n
n

i=1
p

k=1
{tk ψ(Yi −f(xi, β)) [k1(xi, β)
×(∂2/∂βk ∂βj)f(xi, β)
−k2(xi, β) f(xi, β)) fβj(xi, β) fβk(xi, β)]}
©2001 CRC Press LLC

−1
n
n

i=1
p

k=1
{tk k1(xi, β) [ψ′(Yi −f(xi, β)) −γ ]
×fβj(xi, β) fβk(xi, β)}
			
≤C
p

k=1
			 1
n
n

i=1
ψ(Yi −f(xi, β)) [k1(xi, β) (∂2/∂βk ∂βj)f(xi, β)
−k2(xi, β) fβj(xi, β) fβk(xi, β)]
			
+C
p

k=1
			 1
n
n

i=1
k1(xi, β) [ψ′(Yi −f(xi, β)) −γ ]
×fβj(xi, β) fβk(xi, β)
			
which by using the Markov WLNN and conditions [A2] (i)–(ii) yields:
1
n
n

i=1
{ψ(Yi −f(xi, β)) [k1(xi, β) (∂2/∂βk ∂βj)f(xi, β)
−k2(xi, β) fβj(xi, β) fβk(xi, β)]} = op(1),
and
1
n
n

i=1
{ k1(xi, β) [ψ′(Yi −f(xi, β)) −γ ] fβj(xi, β) fβk(xi, β)} = op(1).
Thus:
Sup

t


≤C
			 1
n
n

i=1
p

k=1
tk(∂/∂βk)λj(xi, Yi, β)
+γ
n
n

i=1
p

k=1
tk
1
h[v(f(xi, β))] fβj(xi, β)fβk(xi, β)
			 = op(1).
(23.3.3)
Therefore, from (23.3.2) and (23.3.3) we may conclude that:
©2001 CRC Press LLC

Sup

t


≤C
			 1
n
n

i=1
{λj(xi, Yi, β + h t
√n) −λj(xi, Yi, β) }
+γ
n
n

i=1
p

k=1
tk
1
h[v(f(xi, β))] fβj(xi, β)fβk(xi, β)
			 = op(1), j = 1, ..., p.
We may now consider the following theorem:
THEOREM 23.3.2
Under the conditions [A1]-[A3], [B1]-[B3], and [C] there exists a se-
quence ˆβn of solutions of (23.2.2) such that:
n
1
2 

ˆβn −β


 = Op(1)
(23.3.4)
ˆβn = β + 1
n γ ( 1
n Γ1n(β))−1
n

i=1
λ(xi, Yi, β) + op(n−1
2 ).
(23.3.5)
PROOF From Theorem 1.3.1 we have that the system of equations:
n

i=1
λj(xi, Yi, β + n−1
2 t) = 0
has a root tn that lies in


t


 ≤C with probability exceeding 1 −ϵ for
n ≥n0. Then ˆβn = β + n−1
2 tn is a solution of the equations in (23.2.2)
satisfying:
P(


n
1
2 (ˆβn −β)


 ≤C) ≥1 −ϵ for n ≥n0.
Inserting t −→n
1
2 (ˆβn −β) in (23.3.1), we have that the expression in
(23.3.4).
In the following theorem we shall prove the asymptotic normality.
THEOREM 23.3.3
Under the conditions [A1], [A2] (i)–(ii), [B1], [B2] (i)–(ii), we have
that:
1
√n
n

i=1
λ(xi, Yi, β) −→Np(0, σ2
ψ Γ2(β)), as n →∞
(23.3.6)
©2001 CRC Press LLC

PROOF We consider an arbitrary linear compound:
Z∗
n = ηt
1
√n
n

i=1
λ(xi, Yi, β), η ∈ℜp,
and have that:
Z∗
n =
1
√n
n

i=1
1
h[v(f(xi, β))] ψ(Yi −f(xi, β)) ηtfβ(xi, β) =
n

i=1
cni Zi,
where
cni = σψ
√n
u(xi)
h[v(f(xi, β))] ηtfβ(xi, β),
and
Zi = ψ(Yi −f(xi, β))/(σψ

u(xi) ).
Then by using the H´ajek-ˇSidak Central Limit Theorem, we may show
that Z∗
n converges in law to a normal distribution, as n →∞. In order
to use this Theorem we need to verify the regularity condition about cni,
which is given by
max
1≤i≤n c2
ni/
n

i=1
c2
ni −→0,
as n →∞, and it can be reformulated by requiring that as n →∞,
sup
η∈ℜp

max
1≤i≤n ηt
u(xi)
h2[v(f(xi, β))] fβ(xi, β) f t
β(xi, β) η / ηt Γ2n(β) η

−→0.
Now, in view of the Courant’s Theorem, we have that:
sup
η∈ℜp

ηt
u(xi)
h2[v(f(xi, β))] fβ(xi, β) f t
β(xi, β) η / ηt Γ2n(β) η

= ch1

u(xi)
h2[v(f(xi, β))] fβ(xi, β) f t
β(xi, β) (Γ2n(β))−1
=
u(xi)
h2[v(f(xi, β))] f t
β(xi, β) (Γ2n(β))−1 fβ(xi, β),
so this condition is reduced to the condition [B2] (iii) (Noether’s condi-
tion). Thus, we conclude that:
Z∗
n/[
n

i=1
c2
ni]
1
2 −→N(0, 1), as n →∞
and by using the Cramer-Wold Theorem and condition [B2] (ii) we may
prove the expression in (23.3.6).
©2001 CRC Press LLC

COROLLARY 23.3.4
Under the conditions [A1]-[A3], [B1]-[B3],
√n (ˆβn −β) −→Np

0, γ−2 σ2
ψ Γ−1
1 (β) Γ2(β) Γ−1
1 (β)

(23.3.7)
PROOF From Theorem 1.3.2 we have that:
√n (ˆβn −β) = ( 1
n Γn(β))−1
1
√n
n

i=1
λ(xi, Yi, β) + op(1)
Then from Theorem 1.3.3 and the Slutsky Theorem we may have the
expression in (23.3.7).
COROLLARY 23.3.5
Under the conditions [A1]-[A3], [B1]-[B3],
ˆΓ−1
2 √n (ˆβn −β) −→Np

0, Ip

,
(23.3.8)
where
ˆΓ = ˆγ−2 ˆσ2
ψ ( 1
n Γ1n(ˆβn))−1 ( 1
n Γ2n(ˆβn)) ( 1
n Γ1n(ˆβn))−1,
(23.3.9)
and ˆγ and ˆσ2
ψ are consistent estimators of γ and σ2
ψ, respectively.
PROOF Using expression (23.3.7) and the Slutsky Theorem we have the
expression in (23.3.8).
COROLLARY 23.3.6
Under the conditions [A1]-[A3], [B1]-[B3],
n (ˆβn −β)t ˆΓ−1 (ˆβn −β) −→χ2
p
(23.3.10)
PROOF By using expression (23.3.8) and the Cochran Theorem, we
prove the expression in (23.3.10).
We may use (23.3.10) to provide a conﬁdence set for β. Let Bn(α) =
{β : n (ˆβn −β)t ˆΓ−1 (ˆβn −β) ≤χ2
p,α}, where χ2
p,α is the upper 100%α
point of the chi-square distribution with p degrees of freedom. Then,
P{β ∈Bn(α) / β} −→1 −α, as n increases.
©2001 CRC Press LLC

23.4
Test of Signiﬁcance and Computational
Algorithm
23.4.1
Subhypothesis Testing
Let us consider the partition of β as
 β1
β2

where β1 is a vector of r × 1 and β2 is a vector of (p −r) × 1. We want
to test the hypothesis Ho : β1 = 0 vs H1 : β1 ̸= 0.
From (23.3.9) we can write ˆΓ = ˆγ−2
n
ˆσ2
ψ ˆΣ, where
ˆΣ = ( 1
n Γ1n(ˆβn))−1 ( 1
n Γ2n(ˆβn)) ( 1
n Γ1n(ˆβn))−1,
ˆγn = 1
n
n

i=1
ψ′(Yi −f(xi, ˆβn)),
and
ˆσ2
ψ =
1
n −p
n

i=1
ψ2(Yi −f(xi, ˆβ)).
We can also partition ˆΣ−1 as:
ˆΣ−1 =
 V11
V12
V21
V22

and use the Cochran theorem to prove that:
n (ˆβn1 −β1)t (ˆγ−2
n
ˆσ2
ψ)−1 V11 (ˆβn1 −β1) −→χ2
r.
Then, for testing the null hypothesis we can deﬁne the Wald-type M-test
as:
W = n (ˆβn1)t (ˆγ−2
n
ˆσ2
ψ)−1 V11 ˆβn1
which under Ho follows (asymptotically) a χ2
r distribution.
23.4.2
Nonlinear Hypothesis Testing
Let us consider the nonlinear hypothesis
Ho : a(β) = 0 vs H1 : a(β) ̸= 0
where a is a real valued (nonlinear) function (of β). By using Corollary
1.3.5 and the delta method we have that:
{at
β(ˆβn) ˆΓ aβ(ˆβn)}−1
2 n
1
2 
a(ˆβn) −a(β)

−→N

0, 1

,
©2001 CRC Press LLC

where aβ(β) = (∂/∂β)a(β). Also, by using the Cochran theorem:
n

a(ˆβn) −a(β)
t {at
β(ˆβn) ˆΓ aβ(ˆβn)}−1 
a(ˆβn) −a(β)

−→χ2
1
Then, for testing the null hypothesis we can use the Wald-type M-test:
W = n

a(ˆβn)
t {at
β(ˆβn) ˆΓ aβ(ˆβn)}−1 
a(ˆβn)

which under Ho follows a χ2
1.
23.4.3
Computational Algorithm
In order to solve the equations in (23.2.2), we propose two iterative
methods based on the Taylor expansion around some initial guess ˆβ
(0)
n .
We deﬁne the following matrices:
A(β) =
n

i=1
ψ(Yi −f(xi, β)) k1(xi, β) fβ(xi, β),
W(β) = Diag

k1(x1, β), ..., k1(xn, β)

,
W1(β) = Diag

ψ((Y1 −f(x1, β))
×k2(x1, β), ..., ψ((Yn −f(xn, β))k2(xn, β)

,
W2(β) = Diag

ψ′((Y1 −f(x1, β))
×k1(x1, β), ..., ψ′((Yn −f(xn, β)) k1(xn, β)

,
X(β) =

fβ(x1, β), ..., fβ(xn, β)
t,
Ψ(Y −f(x, β)) =

ψ(Y1 −f(x1, β)), ..., ψ(Yn −f(xn, β))
t
U(β) = A(β) −Xt(β) [W1(β) −W2(β)] X(β)
and
V (β) = Xt(β) W(β) X(β).
The ﬁrst method is similar to the Newton-Raphson procedure, which is
given by:





ˆβ
(0)
n
= ˆβML
ˆβ
(l+1)
n
= ˆβ
(l)
n −

U(ˆβ
(l)
n )
−1 Xt(ˆβ
(l)
n ) W(ˆβ
(l)
n ) Ψ(Y −f(x, ˆβ
(l)
n )).
We can replace U(β) by its expected value,
EU(β) = −γ V (β),
©2001 CRC Press LLC

and propose the following algorithm:











ˆβ
(0)
n
= ˆβML
ˆβ
(l+1)
n
= ˆβ
(l)
n + (ˆγl
n)−1
V (ˆβ
(l)
n )
−1 Xt(ˆβ
(l)
n ) W(ˆβ
(l)
n )
×Ψ(Y −f(x, ˆβ
(l)
n )),
where
ˆγ(l)
n = 1
n
n

i=1
ψ′(Yi −f(xi, ˆβ(l)
n )).
This procedure is similar to the Fisher-scoring method.
Acknowledgements Thanks are due to the reviewers for their helpful
comments on the manuscript. The ﬁrst author was partially supported
by Grant #97036 from the International Clinical Epidemiology Network
(INCLEN).
References
1. Beal, S. L. and Sheiner, L. B. (1988). Heteroscedastic nonlinear
regression. Technometrics 30, 327–338.
2. Bickel, P. J. (1973). On some analogues to linear combinations of
order statistics in the linear model. Annals of Statistics 1, 597–616.
3. Bickel, P. J. (1975). One-step Huber estimates in the linear model.
Journal of the American Statistical Association 70, 428–434.
4. Carroll, R. J. and Ruppert, D. (1982). Robust estimation in het-
eroscedastic linear models. Annals of Statistics 10, 429–441.
5. Dutter, R. (1975). Robust regression: diﬀerent approaches to nu-
merical solutions and algorithms. Research report 6, Fachgruppe
fuer statistik, ETH, Zurich.
6. Gallant, A. R. (1975). Seemingly unrelated nonlinear regressions.
Journal of Econometrics 3, 35–50.
7. Gallant, A. R. (1987). Nonlinear Statistical Models. John Wiley &
Sons, New York.
8. Genning, C., Chinchilli, V. M., and Carter, W. H. (1989). Response
surface analysis with correlated data: A nonlinear model approach.
Journal of the American Statistical Association 84, 805–809.
9. Giltinan, D. M. and Ruppert, D. (1989). Fitting heteroscedastic re-
gression models to individual pharmacokinetic data using standard
©2001 CRC Press LLC

statistical software. Journal of Pharmacokinetics and Biopharma-
ceutics 17, 601-614.
10. Hampel, F. R., Rousseeuw, P. J., Ronchetti, E. and Stabel, W.
(1986).
Robust Statistics — The Approach Based on Inﬂuence
Functions. John Wiley & Sons, New York.
11. Hartley, H. O. (1961). The modiﬁed Gauss-Newton method for the
ﬁtting of nonlinear regression functions by least squares. Techno-
metrics 3, 269–280.
12. Hastie, T. J. and Tibshirani, R. J. (1990). Generalized Additive
Models. Chapman and Hall, London.
13. Hill, R. W. and Holland, P. W. (1977). Two robust alternatives
to least squares regression.
Journal of the American Statistical
Association 72, 828–833.
14. Holland, P. W. and Welsch, R. E. (1977). Robust regression using
iteratively reweighted least squares. Communications in Statistics
A6, 813–827.
15. Huber, P. J. (1964). Robust estimator of a location parameter.
Annals of Mathematical Statistics 35, 73–101.
16. Huber, P. J. (1973). Robust regression: asymptotics, conjetures
and Monte-Carlo. Annals of Statistics 1, 799–821.
17. Huber, P. J. (1981). Robust Statistics. John Wiley & Sons, New
York.
18. Jaeckel, L. A. (1972). Estimating regression coeﬃcients by min-
imizing the dispersion of the residuals.
Annals of Mathematical
Statistics 43, 1449–1458.
19. Jureˆckov´a, J. and Sen, P. K. (1996). Robust Statistical Procedures,
Asymtotics and Interrelations. John Wiley & Sons, New York.
20. Klein, R. and Yohai, V. J. (1981). Asymptotic behavior of iterative
M-estimators for the linear model. Communications in Statistics
A10, 2373–2388.
21. Maronna, R. A. (1976). Robust M-estimators of multivariate loca-
tion and scatter. Annals of Statistics 4, 163–169.
22. Marquardt, D. W. (1963). An algorithm for least-squares estima-
tion of nonlinear parameters. Journal of the Society for Industrial
and Applied Mathematics 11, 431–441.
23. McCullagh, P. (1983). Quasilikelihood functions. Annals of Statis-
tics 11, 59–60.
24. McCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models
Second Edition. Chapman and Hall, UK.
©2001 CRC Press LLC

25. Nelder, J. A. and Wedderburn, R. W. M. (1972).
Generalized
linear models. Journal of the Royal Statistical Society, Series A
135, 370–384.
26. Schrader, R. M. and Hettmansperger, T. P. (1980). Robust analy-
sis of variance based upon a likelihood ratio criterion. Biometrika
67, 93–101.
27. Sen, P. K. (1982). On M-tests in linear models. Biometrika 69,
245–248.
28. Sen, P. K. (1998). Generalized linear and additive models: Robust-
ness perspectives. Brazilian Journal of Probability and Statistics
12, 91–112.
29. Sen, P. K. and Singer, J. M. (1993).
Large Sample Methods in
Statistics: An Introduction with Applications. Chapman and Hall,
New York.
30. Singer, J. M. and Sen, P. K. (1985). M-methods in multivariate
linear models. Journal of Multivariate Analysis 17, 168–184.
31. van Houwelingen, J. C. (1988). Use and abuse of variance models
in regression. Biometrics 43, 1073–1081.
32. Wedderburn, R. W. M. (1974). Quasi-likelihood functions, gener-
alized linear models, and the Gauss-Newton method. Biometrika
61, 439–447.
33. Yohai, V. J. and Maronna, R. A. (1979). Asymptotic behaviour of
M-estimators for the linear model. Annals of Statistics 7, 258-268.
34. Zeger, S. L., Liang, K. Y., and Albert, P. S. (1988). Models for
longitudinal data: A general estimating equation approach. Bio-
metrics 44, 1049–1060.
©2001 CRC Press LLC

PART V
Inference and Applications
©2001 CRC Press LLC

24
Extensions of a Variation of the
Isoperimetric Problem
Herman Chernoﬀ
Harvard University, Cambridge, MA
ABSTRACT A problem in the eﬃcient storage of noisy information
in two dimensional space leads to a gaussian variation of the classical
Isoperimetric Problem.
In this variation the area and perimeter are
modiﬁed by the bivariate normal distribution. This problem corresponds
to the eﬃcient storage of the noisy information with one bit; i.e. dividing
the plane into two sets. The extension considered here corresponds to
dividing the plane into n sets. For Example 3 bits of storage corresponds
to n = 8.
Keywords and phrases Bivariate
normal
distribution,
inequality,
isoperimetric problem, information retrieval, information storage
24.1
Introduction
Since this symposium was organized in honor of Professor Theophilos
Cacoullos, it seemed appropriate to resurrect a dormant problem which
gave rise to an inequality which he generalized, and he and his students
studied in considerable detail.
This inequality, which I published in Chernoﬀ(1981), and was an
example of a more general result published independently by Brascomb
and Lieb by Lieb in 1976, states that if X is normally distributed with
mean 0 and variance 1, g(x) is absolutely continuous, and the variance
of g(X) is ﬁnite, then E[g′(X)]2 ≥Var[g(X)]. The proof I oﬀered was
rather trivial after considerable eﬀort was expended in demonstrating
that g(X) could be expanded in Hermite polynomials.
At the time I had derived the inequality, I had two visitors at MIT.
They were Louis H. Y. Chen and Theo Cacoullos. I complained to them
that the derivation did not seem appropriate and that there must be a
©2001 CRC Press LLC

better approach. Both Chen and Cacoullos accepted my challenge after
leaving MIT and independently derived very interesting diﬀerent gen-
eralizations using diﬀerent approaches. The bibliography lists a couple
of the earlier papers by Cacoullos and Chen. Incidentally, I am almost
convinced, that had I the patience to do the scholarly labor, some form
of the inequality could be found in the nineteenth century literature.
My object here is to describe the original context from which I came
across the inequality, a problem in information retrieval, and how it
leads to a variation of the classical isoperimetric problem.
Since my
retirement, I have had the urge to clean up some problems that I came
across in the past and never seemed to have the time to address. One of
these is an extension of the variation of the isoperimetric problem.
24.2
Information Retrieval Problem
Around 1973 I attended a lecture on information retrieval in which a
problem posed by the FBI ﬁngerprint ﬁles was mentioned. It was claimed
that the FBI has a library of about 40 million sets of ﬁngerprints, that
30 thousand sets of ﬁngerprints arrive daily, and that each set must be
compared with those in the ﬁle to see if the subject is already repre-
sented there, possibly under another name. The task was said to be
accomplished with the help of 15 thousand clerks.
At that time attempts were being made to use computers to automate
the process of storing and retrieving the relevant information in the
ﬁngerprints. I spoke with some of the people at NIST who were supposed
to be leaders in the research eﬀort leading to automation and found
that their attempts were rather ad hoc and that a serious theoretical
framework was lacking. In particular, the role of measurement error was
not properly factored into any theoretical approach.
When I discovered that the FBI had ﬁles on 5 thousand subjects where
two sets of ﬁngerprints were kept for each subject, that seemed to be an
excellent source of information on measurement error. My attempts to
gain access to those ﬁles were discouraged. This led me to terminate my
research in this area where the major issues seemed to be measurement
error and the high dimensionality of the data. My major publication in
this area is Chernoﬀ(1980)
©2001 CRC Press LLC

24.3
Information Retrieval without Measurement
Error
In the case where there is no measurement error, there is an eﬀective
method of storing large amounts of high dimensional data called hash
coding. To explain this valuable method, consider the problem of setting
up a library of n = 50 million distinct entries, each represented by a
variable X consisting of m = 100 binary digits. The object is to store
this information eﬃciently so as to be able to determine whether a target
Y coincides with an X in the library.
One possible approach is to set up a storage space consisting of 2m =
2100 locations, each identiﬁed by a distinct 100 digit address. Than we
put each X in the location with address X. When Y is presented one
need only go to the location with address Y to see if it is occupied or
not. The problem with this approach is that a storage device with 2100
addresses is much too large to be practical.
The alternative hash coding approach is to use a storage device with
2n = 100 million locations, each of which can hold a hundred digit num-
ber. Select some random uniformly distributed real numbers, r1, r2, . . . ,
on (0, 1). Construct a hashing function f(x, r) which assigns a location
from 1 to 2n for each x and r. The hashing function should be selected
to be sensitive to changes in the components of x, and more or less
uniformly distributed.
The library is constructed as follows.
First X1 goes into location
f(X1, r1). Having assigned locations for X1, X2, . . . , Xk, put xk+1 into
location f(Xk+1, r1) if it is empty. If it is not empty, try f(Xk+1, r2),
f(Xk+1, r3), . . . until an empty location is found to hold Xk+1.
To determine whether Y is in the library,we need to compare Y with
the contents of f(Y, r1), f(Y, r2), . . . until a match or an empty location
is found.
When the library is constructed, most of the locations f(Xi, r1) will
be empty at ﬁrst, but as the library ﬁlls up, one would expect to ﬁnd
occupied locations about half the time. Then we would have, on the
average, about 2 tries to ﬁnd an empty location. Similarly in matching
the target Y , we will have to search, on the average, about 2 times before
ﬁnding an empty spot if Y is not in the library. If it should turn out
that the cost of storage is very high, we need not use 2n locations. Using
1.5n locations would be adequate, but our method would require more
comparisons in constructing the library and searching for matches to the
target. Thus the relative costs of computing and storage space determine
what is a good balance.
The main shortcoming with this approach, and which makes it inap-
©2001 CRC Press LLC

plicable to the ﬁngerprint problem, is that if X consists of many com-
ponents subject to measurement error, hash coding will not work.
24.4
Useful Information in a Variable
Chernoﬀintroduced some relevant measures of the useful information in
a variable. The speciﬁc choices need not be discussed here, but some
characteristics should be mentioned in connection with a few examples.
Example 1
Sex
If half of the subjects are of each sex, and the determination of sex is
accurate, then knowledge of the sex contributes one useful bit of informa-
tion. This is in the sense that if the ﬁles were divided according to sex,
then the knowledge of the sex would reduce the search by a factor of 2.
If, however, the probability of error in determining sex is 0.02, the listing
of sex would contribute only 1/4 of a bit of useful information according
to one of the measures. This substantial loss of useful information is an
important indication of the potential danger of using ad hoc approaches
without the beneﬁt of theory.
Example 2
Normal Errors
Suppose that X is an attribute measured by Z = X + Y where Y is
the measurement error. First we compare two observations on Z for the
same X.
Let X be normally distributed with mean 0 and variance 1, i.e. L(X) =
N(0, 1). Let Z1 = X + Y1 and Z2 = X + Y2 where X, Y1 and Y2 are
independently distributed with means 0 and variances 1, σ2 and σ2.
Then
L(Z1 −Z2) = N(0, 2σ2)
(24.4.1)
indicating how close two measurements on the same subject will diﬀer.
Now let us compare two observation Z for diﬀerent subjects. Here
Z1 = X1 + Y1 and Z2 = X2 + Y2 for independent X1, X2, Y1 and Y2, and
L(Z1 −Z2) = N(0, 2 + 2σ2)
(24.4.2)
It is clear that for small σ Z carries a great deal of information for
determining matches.
©2001 CRC Press LLC

24.5
Allocation of Storage Space
If the measurement of X in the example of the preceeding section does
not carry much useful information, we may not wish to devote much
storage space for it. In particular, suppose that we wish to allocate only
one bit of storage space in Example 2. Then symmetry considerations
suggest the use of the sign of Z as the stored indicator of Z. In other
words, we use Z∗= 1 for Z < 0, and Z∗= 0 for Z ≥0.
Suppose now that we have two diﬀerent and independent variables, X1
and X2 which are normally distributed with means 0 and variances 1,
and subject to independent measurement errors Y1 and Y2 with variance
σ2.
If we want to devote two bits of storage space to the resulting
observations Z1 and Z2, it would be sensible to use the signs of Z1 and
Z2.
Going one step further in conservatism, suppose we wish to use only
one bit of storage space for the vector Z = (Z1, Z2). How should we
divide the two dimensional Euclidean space R2 into two parts where
Z∗= 1 and 0 respectively so as to maximize the useful information in
the reduced variable Z∗?
My ﬁrst reaction to this question, conditioned by experience as a
statistician, was to conjecture that with two observations, we should be
able to squeeze out more useful information than with one observation.
However, several attempts at improvement failed, and I changed my con-
jecture to the reverse, i.e. that one could not improve over discarding
Z2 and letting Z∗= 1 or 0 depending on the sign of Z1.
24.6
The Isoperimetric Problem
Stated formally, the optimization problem of the last section involves
two, observations, Z(1) = X + Y(1) and Z(2) = X + Y(2) where X =
(X1, X2), Y(1) = (Y (1)
1
, Y (1)
2
) and Y(2) = (Y (2)
1
, Y (2)
2
). The components
of X,Y(1), and Y(2) are all independent and normally distributed with
means 0 and variances 1, σ2 and σ2 respectively. For various measures
of useful information, the optimality condition for small σ2 requires
P(X ∈R) = 1/2
(24.6.1)
as a ﬁrst order eﬀect and that P(Z(1) ∈R, Z(2) ∋R) be minimal as a
second order eﬀect.
©2001 CRC Press LLC

Since
P(Z(1) ∈R, Z(2) ∋R) ≈2√πσ

∂R
φ(x1)φ(x2)

dx2
1 + dx2
2
(24.6.2)
where ∂R is the boundary of R, and φ is the standard normal density,
our optimization problem is that of minimizing
L =

∂R
φ(x1)φ(x2)

dx2
1 + dx2
2
(24.6.3)
subject to the condition
A =

R
φ(x1)φ(x2)dx1dx2 = 1/2.
(24.6.4)
This is a variation of the standard isoperimetric problem where both
area and perimeter are modiﬁed by the bivariate normal density.
An attempt, to prove that the region R which minimizes L subject to
a given value of A between 0 and 1 consists of a half space, required the
derivation of the inequality generalized by Cacoullos and Chen. Later, a
more basic, and as yet unpublished, derivation was obtained by Beckner
and Jerison, using an inequality due to Borell.
As indicated earlier, this solution of the isoperimetric problem im-
plies that the asymptotic (as σ →0) solution of our storage problem is
equivalent to discarding Z2 and reporting the sign of Z1.
24.7
Extensions
Suppose now that we are allowed more storage space for recording Z. It
seems perfectly reasonable to use two bits of storage space by reporting
the signs of Z1 and Z2. But what if we are allowed still more storage
space? For three bits of storage space we have to divide R into 8 regions
of equal probability, so that the integrated density along the boundaries
is minimal.
We might even consider using a fractional number of bits of storage
space by dividing R into 3 regions of equal probability with minimal
integrated density along the boundaries. So let us consider how to mini-
mize L, the integrated density along the boundaries of n regions of equal
probability for n greater than 2. We introduce several conjectures of in-
creasing complexity which are supported on the grounds of the symmetry
inherent in the standard bivariate normal distribution.
©2001 CRC Press LLC

Conjecture 1
n rays.
Let the n regions consist of those divided by the equally spaced rays
from the origin. Here
L = n
 ∞
0
φ(0)φ(x)dx = nφ(0)/2 = 0.1997n.
(24.7.1)
Conjecture 2
(n −1) circles.
If we have n regions separated by n −1 circles centered at the origin,
we note that the probability outside a circle of radius r is
P(X2
1 + X2
2 ≥r2) = e−r2/2
(24.7.2)
and the integrated density along the circle is
2πr 1
2π e−r2/2.
(24.7.3)
Thus, if the radii are r1, r2, . . . , rn−1, we need
e−r2
i /2 = 1 −i
n
(24.7.4)
and
L =
n−1

i=1
(1 −i
n)

−2 log(1 −i
n)
L = 1
n
n−1

j=1

2 log(n/j)
≈0.443n
Calculations show that n rays is preferable to (n −1) circles for all
values of n.
Conjecture 3
Circle with m of n regions inside.
In more detail, this third conjecture involves m partial rays from the
origin to a circle of radius r centered at the origin, and n −m partial
rays from the circle to inﬁnity outside. Two exceptional versions of this
conjecture are when m = 1 and n −1. If m = 1 there is no partial ray
©2001 CRC Press LLC

inside the circle. If m = n −1 there is no partial ray outside the circle.
For this conjecture we have
e−r2/2 = 1 −m/n
(24.7.5)
and
L = re−r2/2+m1(m1)φ(0)(Φ(r)−1/2)+(n−m)1(m < n−1)φ(0)(1−Φ(r))
(24.7.6)
where Φ is the standard normal c.d.f.
and 1(·) is the characteristic
function.
Calculations for low values of n indicate that the best choice of m
varies with n and does better than Conjecture 1 for n4. In fact, for
1 < m < n −1, we may write
L = A(r) + nB(r)
(24.7.7)
where
A(r) = re−r2/2
(24.7.8)
and
B(r) = φ(0){(1 −e−r2/2)(Φ(r) −1/2) + e−r2/2(1 −Φ(r))}.
(24.7.9)
Since B(r) achieves its minimum at r = 0.913 where exp(−r2/2) =
0.659, A(r) = 0.602 and B(r) = 0.0909, it follows that for large n we
would like to have m ≈0.341n, and we would get
L ≈0.602 + 0.0909n
(24.7.10)
which crosses nφ(0)/2 at n = 5.5. However, for n = 5, a circle with
m = 1 does slightly better than 5 rays.
A more general conjecture which depends on symmetry is the following
one.
Conjecture 4
Several circles with several regions in each ring.
More precisely, we take s circles with centers at the origin and radii
r1 < r2 < . . . < rs. We select positive integers m1, m2, . . . , ms+1 and
have m1 regions in the interior of the ﬁrst circle, mi regions in the
annulus between the (i −1)-st and the i-th circles for 2 ≤i ≤s, and
ﬁnally ms+1 regions outside the i-th circle.
Taking r0 = 0 and rs+1 = ∞, we have, for 1 ≤i ≤s,
©2001 CRC Press LLC

e−r 2
i /2 = mi+1 + mi+2 + . . . + ms+1
n
L =
s

i=1
rie−r 2
i /2 + φ(0)
s+1

i=1
mi1(mi1)[Φ(ri) −Φ(ri−1)].
If all mi1,
L = A(r) + nB(r) 
(24.7.11)
where
A(r) =

rie−r 2
i /2 
(24.7.12)
and
B(r) =
s+1

i=1
[φ(ri−1) −φ(ri)][Φ(ri) −Φ(ri−1)]. 
(24.7.13)
Minimizing B(r) for several values of s, we obtain the following Table
24.1.
This table, presenting approximations, ignoring the constraints
that the mi are integers not necessarily exceeding one, suggests that the
break even choices of n, when one should shift from one value of s to the
next, are approximately those given by Table 24.2.
In conclusion, we have not presented a formal proof that Conjecture
4 resolves the extension of the variation of the isoperimetric problem to
the case of n regions for n2. We have not addressed the more general
problem where the observed data are multivariate of dimension greater
than 2. However, we now have some appreciation of what there is to
gain in increased eﬃciency of information storage for the bivariate case.
References
1. Beckner, W. and Jerison, D., Gaussian Symmetry and an Optimal
Information Problem. Unpublished Draft, 1–17.
2. Brascamp, H. J. and Lieb, E. H. (1976).
On extensions of the
Brunn-Minkowski and Prekopa-Leindler theorems including inequal-
ities for log concave functions, and with an application to the dif-
fusion equation. Journal of Functional Analysis 22, 366–389.
3. Cacoullos, T. (1982). On upper and lower bounds for the variance
of a function of a random variable. Annals of Probability 10, 799–
809.
©2001 CRC Press LLC

4. Cacoullos, T. (1989).
Dual Poincare-type inequalities via the
Cramer-Rao and the Cauchy-Schwarz inequalities and related char-
acterizations. In Statistical Data Analysis and Inferences pp. 239–
250. North-Holland, Amsterdam.
5. Chen, L. H. Y. (1982). An inequality for the multivariate normal
distribution. Journal of Multivariate Analysis 12, 306–315.
6. Chen, L. H. Y. (1985). Poincare-type inequalities via stochastic in-
tegrals. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und Verwandte
Gebiete 69, 251–277.
7. Chernoﬀ, H. (1980). The identiﬁcation of an element of a large
population in the presence of noise. Annals of Statistics 8, 1179–
1197.
8. Chernoﬀ, H. (1981). A note on an inequality involving the normal
distribution. Annals of Probability 9, 533–535.
©2001 CRC Press LLC

TABLE 24.1
s = 0
s = 1
s = 2
s = 3
s = 4
s = 8
A(r)
0.0000
0.6018
1.1036
1.5756
2.0364
3.8370
B(r)
0.1994
0.0909
0.0590
o.o437
0.0347
0.0191
r1
0.9133
0.6501
0.5221
0.4432
0.2923
r2
1.2340
0.9314
0.7706
0.4919
r3
1.4247
1.1116
0.6721
r4
1.5583
0.8500
r5
1.0367
r6
1.2453
r7
1.4998
r8
1.8682
m1/n
0.3410
0.1905
0.1274
0.0935
0.0418
m2/n
0.6590
0.3425
0.2245
0.1633
0.0721
m3/n
0.4670
0.2856
0.2040
0.0882
m4/n
0.3625
0.2422
0.1010
m5/n
0.2970
0.1125
m6/n
0.1238
m7/n
0.1358
m8/n
0.1501
m9/n
0.1747
TABLE 24.2
Break even values of n for shifting s
s
0 −1
1 −2
2 −3
3 −4
4 −8
ns
5.5
15.7
30.8
51.2
114.9
©2001 CRC Press LLC

25
On Finding A Single Positive Unit In
Group Testing
Milton Sobel
University of California, Santa Barbara, CA
ABSTRACT For the group-testing situation with the total number N
of units being inﬁnite, a simple non-optimal procedure is proposed and
is shown to be highly eﬃcient compared to the optimal procedure.
Keywords and phrases Group testing, DH procedure, eﬃciency, ro-
bustness, Bayesian procedure
25.1
Introduction
In group testing, the total number N of units may be ﬁnite or inﬁnite, but
it is usually assumed that the units all have a common probability p > 0
of being positive (previously called defective or unsatisfactory) and q =
1 −p of being negative (previously called nondefective or satisfactory).
Any number x of units can be included in a single test but there are only
two possible outcomes:
• all x units are negative,
• at least 1 of the x limits is positive.
We do not know which units are positive or how many are positive
unless x = 1. This positive/negative terminology gives us a wider scope
in applications since the positive units may be the ones we would rather
not ﬁnd or they may be the ones we are trying to ﬁnd. The problem in
this paper is to ﬁnd a single positive unit, assuming N = ∞, so that (with
p > 0) the probability is unity that it exists. For the two settings we
consider: (1) q is known and (2) q is given by the Uniform(0,1) prior, the
ﬁrst one has already appeared in the literature, but not the second. We
also wish to deﬁne a very simple (nonyoptimal) procedure (called DH
for doubling and halving), and see how it compares with the optimal
©2001 CRC Press LLC

procedure in each of these two settings. The eﬃciency of a procedure is
deﬁned by comparing its expected number of tests required to that of
the optimal procedure. The DH procedure does quite well in the ﬁrst
setting, reaching 94.22% eﬃciency (for some q values), but in the second
setting, it does much better with a (Bayesian) eﬃciency of 99.6%. This
is a remarkable result for a procedure so easy to implement.
25.2 
Description of Pro cedures, Numerical Results
For the ﬁrst setting with (q < 1 known) a procedure (denoted by R′
1) was
conjectured in Kumar and Sobel (1971) to be optimal and this conjecture
was proved for N = ∞in Hwang (1974) and then generalized for a ﬁnite
N problem in Garey and Hwang (1973) and Hwang (1974). In Table
25.1 below we give the expectation and variance (of the number of tests
required) for the optimal procedure for q = .50(.05).95, .99. The DH
procedure (also given in Table 25.1) is deﬁned as follows:
Procedure DH: Start, by testing 1 unit and keep on doubling the size
of the next test as long as the results are negative. Then, test half of the
positive set. Then, select the half that is positive either by direct test or
by inference and continue to use halving until a positive unit is found.
The Procedure DH is easy to implement and does not use q for carrying
it out, only for evaluating its performance. At q = .618034 = (
√
5−1)/2
the eﬃciency of the DH procedure reaches a maximum of over 94%.
For the second setting with q < 1 unknown, we propose a Bayesian
procedure denoted by R(1)
1B. It also starts by testing 1 unit and we use
the posterior estimate ˆq after each negative test result together with the
value of x from Kumar and Sobel (1971, p. 825), assuming that ˆq is
the true value of q. The same table, extended to x = 100, appears in
Sobel and Groll (1959, Table VII). The reduction part of the procedure
R(1)
1B is based on the current values of m (size of the positive set) and s
(the number of units shown to be negative); the current value of u (the
number of units shown to be positive) is of course zero. Hence, except for
the fact that u = 0 we have the same (F-situation) as in the procedure
R(1) of Sobel and Groll (1966) and we use the tables and diagrams of
that paper to complete the problem. The result is the nested procedure
R(1)
1B.
Obviously once we have a positive set, there is no need to do any
“mixing” of binomial units with subsets of a positive set, since we only
want to ﬁnd one positive unit. Hence, we restrict our attention to nested
procedures.
Since the procedure R′
1 in Kumar and Sobel (1971) and
R(1) in Sobel and Groll (1966) are both optimal, we claim that the
©2001 CRC Press LLC

new procedure R(1)
1B is also optimal in the Bayesian sense, i.e., that it
minimizes the Bayes average of the expected number of tests required
to ﬁnd a single positive unit. Since the ﬁrst part of the procedure R(1)
1B
is open-ended, it was necessary to cut it oﬀafter 20 attempts to ﬁnd a
positive subset; at this point, the probability is .999982 that a positive
set would have been found and the contribution to the expected total
number of tests is only in the fourth decimal place (as seen by the last
column of Table 25.2).
Table 25.2 gives numerical results only for procedure R(1)
1B; comparisons
with procedure RDH will be made later after some formulas are derived
for the latter procedure in the next section. In Table 25.1, the symbol
T1 (resp., T) denotes the number of tests from the ﬁrst part (resp., from
both parts), of procedure RDH. The optimal procedure R′
1 described in
Kumar and Sobel (1971) takes a common test size x1 until a positive
set (of size x1) is found. After that, it follows the F-algorithm described
in Sobel and Groll (1959) and Kumar and Sobel (1971) with boundary
condition F(1) = 0. For q = .99, x1 = 69 by Sobel and Groll (1959,
Table VII). For m = 69, x2 = 32; for m = 32 and m = 37, x2 = 16. All
other values can be obtained front Table IVA or Figure 4 of Sobel and
Groll (1959). In Table 25.2 the symbol Tj denotes the test on which the
ﬁrst positive set was found, ˆq1j is the posterior estimate of q just before
Tj, and ˆq2j is the same just after Tj. It is important to note that the
results in the last column get close to 3 but still remain under 3; for an
explanation, see Section 25.4 below. The entry in the j-th row of the
last column of Table 25.2 is formed by the accumulation
(col. 10, entry j) −
j

α=1
[α + (col. 9, entry α)](col. 5, entry α)
(25.2.1)
and the limiting result in col. 10 is the desired E{T|R(1)
1B}. Its exact
value (less than 3) is not known.
One property that was seen to hold in Sobel and Groll (1966) was that
for any positive set of size m = 2α + β with 0 ≤β < 2α and u = 0,
the maximum size (col. 8 in Table 25.2) of the next test set is 2α−1 for
2α ≤m ≤3.2α−1 and it is m −2α for the remaining values of m. Thus,
for µ = 0 and m = 10, 11, 12, 13, 14 and 15, the maximum test size for
the next test is 4, 4, 4, 5, 6 and 7, respectively. This property was used in
the second part of Procedure R(1)
1B since it is inherent in procedure R(1)
in Sobel and Groll (1966). To illustrate how this was used in calcula-
tions for Table 25.2, consider the row j = 11 where m = 208, α = 7,
β = 80. Since m > 3·26 = 192, the maximum x-value for the next test is
©2001 CRC Press LLC

Comparison of Procedure RDH with the optimal procedure R'1 for known q-values
* For X1 = 1 (res. 2) the variance is 4.236068 (resp. 1.000000). Discontinuities in the Var(T|R'1) can occur 
whenever the common X1 changes, but they do not occur for E(T|R'1).
TABLE 25.1
©2001 CRC Press LLC

©2001 CRC Press LLC
TABLE 25.2
New Bayesian Procedure R1B for finding a single positive unit
(1)

m −2α = 80. For the contribution to the 2d part (col. 9 of Table 25.2),
the values of x = 77, 78, 79, and 80 give respectively 7.655462, 7.651099,
7.66280, and 7.663130. The minimum of these is at x = 78 and that was
used in Table 25.2.
25.3
Some Formulas for Procedure RDH
Using T1 for the number of tests needed to ﬁnd a positive set under
Procedure RDH, we note that (before using the prior)
E{T1|RDH}
= 1 −q + 2(q −q3) + 3(q3 −q7) + · · ·
=
∞

j=0
q(2j−1).
(25.3.1)
For the total number of tests under Procedure RDH we have the simple
relation
T = 2T1 −1.
(25.3.2)
For the variance we used the fact that
E{T 2
1 |RDH}
= 1(1 −q) −q + 22(q −q3) + 32(q3 −q7) + · · ·
=
∞

j=0
(2j + 1)q(2j−1),
(25.3.3)
that σ2(T1) = ET 2
1 −[E(T1)]2 can be obtained from (25.3.1) and (25.3.3)
and hence by (25.3.2)
σ2(T|RDH) = 4σ2(T1|RDH).
(25.3.4)
Unlike the optimal procedure R(1)
1B, where we only approximated
E{T|R(1)
1B}, the procedure RDH now gives us two exact results. If we
integrate the right side of (25.3.1) with respect to the Uniform prior on
[0, 1], we obtain
©2001 CRC Press LLC

E{T1|R −DH} = 1 + 1/2 + 1/4 + 1/8 + · · · = 2
(25.3.5)
and from (25.3.2) we obtain our ﬁrst (exact) result for procedure RDH
E{T|RDH} = 3.
(25.3.6)
For the variance of T, we start by using (25.3.2) on each coeﬃcient in
(25.3.1), obtaining
E{T 2|RDH} = 12(1 −q) + 32(a −a3) + 52(q3 −q7)
+72(q7 −q15) + · · ·
(25.3.7)
= 1 + 8
∞

j=1
jq(2j−1)
(25.3.8)
and upon integration with respect to the Uniform prior
E{T 2|RDH} = 1 + 8
∞

j=1
j/2j.
(25.3.9)
Introducing a generating function parameter tj−1 in the summation
in (25.3.8) we obtain, after ﬁrst integrating from t = 0 to t = 1 and then
diﬀerentiating we respect to t (and setting t = 1)
E{T 2|RDH} = 1 + 8
 d
dt

t
2 −t

t=1
= 1 + 8

2
(2 −t)2

t=1
= 17.
(25.3.10)
This gives us our second (exact) result by using (25.3.6) for procedure
RDH
σ2(T|RDH) = 17 −32 = 8.
(25.3.11)
©2001 CRC Press LLC

25.4 
The Greedy Pro cedure RG
At the end of Section 25.2, we described the maximum test size and how
it was used in procedure R(1)
1B. It can be noted that this maximum was
used in 16 of the 20 rows of Table 25.2 (m is in Col. 3 and x is in Col. 8).
Suppose we used the maximum test size throughout the second part of
our procedure (without changing the ﬁrst part as in R(1)
1B), what results
are obtained; denote this new procedure as RG. Columns 1 through 6
of Table 25.2 are unchanged. Moreover since the value of x (in Col. 8)
changes only for rows 5, 6, 10 and 11, the ﬁrst four rows are exactly the
same. Starting with row 5, the entries for rows 5, 10, 15 and 20 in Col.
10 become
1
Col. 10 = E{T|RG}
5
2.400251
10
2.918631
15
2.982125
20
2.988644
Thus, we note that this gives an even better bound for the optimal
procedure R(1)
1B and if we compute its eﬃciency we obtain 99.98%. As
with the optimal procedure R(1)
1B, this procedure RG also does not give
an exact precise value for an answer.
25.5 
Conclusions
We now see why the value 3 was important to note in Table 25.2. If
Procedure R(1)
1B is (Bayesian) optimal, then E{T|R(1)
1B} has to be less
than the exact answer 3 for procedure R −DH, i.e., the value 3 is an
upper bound for the optimal Bayes procedure R(1)
1B. Comparing these
two expectations, we obtain the spectacular result
Eﬃciency (RDH) ≥2.988/3 = 99.6%,
(25.5.1)
a remarkably good result for a procedure that is easy to implement and
not fully adaptive.
The procedure RG has an even higher eﬃciency
(99.98%) but does not yield simple expressions to approximate E{T}
for the optimal procedure.
©2001 CRC Press LLC

25.6
Changing the Prior with Procedure RDH
In an attempt to obtain an explicit formula for E{T}, we restrict our
attention to procedure RDH and consider the beta distribution f(q) =
Cqα−1(1 −q)β−1 as a generalization of the Uniform(0,1) prior. By the
same methods as used above we have
E{T|RDH}
= C
∞

j=1
(2j −1)
 1
0
Pj{X ≥1}qα−1(1 −q)β−1dq
(25.6.1)
= C
∞

j=1
(2j −1)
 1
0
[q(2j−1−1) −q(2j−1)]qα−1(1 −qβ−1dq
(25.6.2)
= C
β−1

i=0
(−1)i
β −1
i
 ∞

j=1
(2j −1)

1
2j−1 + α + i −1 −
1
2j + α + i −1

(25.6.3)
= C
β−1

i=0
(−1)i
β −1
i
 ∞

j=1

2j −1
2j−1 + α + i −1 −
(2j + 1)
2j + α + i −1

(25.6.4)
= C
β−1

i=0
(−1)i
β −1
i
 ∞

j=1
2
2j + α + i −1.
(25.6.5)
The ﬁrst double sum in (25.6.4) telescopes and sums to C−1 = Γ(α)Γ(β)
Γ(α+β)
and we obtain
E{T|RDH}
= 1 + 2C
∞

j=1
β−1

i=0
(−1)i
β −1
i

1
2j + α + i −1
(25.6.6)
= 1 + 2C
∞

j=1
 1
0
x2j+α−1(1 −x)β−1dx
(25.6.7)
= 1 + 2Γ(α + β)
Γ(α)
∞

j=1
Γ(α + 2j)
Γ(α + β + 2j).
(25.6.8)
Computations based on (25.6.8) show that if we increase α (i.e., make the
positive units scarcer) then the value of E{T} increases, but if we keep
©2001 CRC Press LLC

the expectation α/(α + β) ﬁxed and increase the variance by replacing
(α, β) by say, (2α, 2β), then the value of E{T} decreases. The following
short table illustrates this pattern.
Known Prior Density 
Exp.
Var.
E {T |RDH }
α = β = 1, Uniform(0,1)
1/2
1/2
3.000000
α = β = 2,  6q (1 − q )
1/2
1/20
2.586995
α = β = 3,  30q 2 (1 − q )2
1/2
1/28
2.468845
α = 4,  β = 1,  4q 3
4/5
2/75
4.862089
α = 8,  β = 2,  72q 7 (1 − q )
4/5
4/275
4.127280
α = 9,  β = 1,  9q 8
9/10
9/1100
7.145056
α = 18, β = 2, 342 q 17 (1 − q )
9/10
9/2100
6.369355
α = 99, β = 1,  99q 98
99/100
99/101 10−4
12.394409
α = 198, β = 2, 39402 q 197 (1 − q )
99/100
99/201 10−4
12.522018
25.7 
Robustness of Pro cedure RDH for q Known
Suppose you assumed (or were told) that q = .80, but the correct value
is .90. Using the optimal procedure R′
1 for q known, your common test
group size would be x1 = 3 and the probability of a positive subset is
1 −(.9)3 = .271, so that E{T} = 1/.271 = 3.6900137. For the 2d part
of the procedure [using the table in Kumar and Sobel (1971)], you need
an average of
E{T2|R′
1} =
2 + q −2q3
1 −q3

q=.9
= 5.321033,
(25.7.1)
so that the total E{T|R′
1} = 9.011070.
For the procedure RDH from Table 25.1 with q = .90 we obtain 3.52652
for E{T1} and 5.705303 for E{T}. Note that the value of q is not used
in implementing the procedure, only for its evaluation.
Hence, with
an incorrect value of q you could be using an optimal procedure and
operating at 63.3% eﬃciency.
In other words, the procedure RDH is
robust against wrong information about the value of q.
Suppose now that q = .90 again but you assume (or are told) that
q = .99. Then the test group size under the optimal procedure is x1 = 69,
the probability of a positive set is 1 −(.9)69 = .999304 and E{T1|R′
1} =
1.000697. For the second part of the procedure, [using Kumar and Sobel
(1971)], you need an average
E{T2|R′
1} =

7 +
q59
1 −q69

q=.9
= 7.001998,
(25.7.2)
©2001 CRC Press LLC

so that the total E{T|R′
1} = 8.002695.
For procedure RDH, E{T} is the same as before, namely, 5.705303.
Here, with the optimal procedure R′
1 you operate with 71.3% eﬃciency.
Thus, from the viewpoint of robustness against wrong information about
the q-value, the procedure RDH is more robust “in both directions” than
procedure R′
1 for q known.
Since the optimal Bayes solution R(1)
1B is similar to RDH we don’t ex-
pect the same results when using a prior for q and the posterior estimates
of q after every test, i.e., we expect the optimal Bayes solution R(1)
1B to
also be robust in both direction.
Acknowledgements Thanks are due to Professor Anton Boneh (Tech-
nion, Haifa, Israel) for recently pointing out to me that he studied (in
a private communication) a modiﬁed and somewhat generalized form
of Procedure RDH above for the goal of ﬁnding all the defectives in a
group-testing model with N given items (with p and q = 1−p unknown);
he calls it the “DOD Policy” for Double or Divide. The results in his
study also show robustness and proximity to optimality as in the present
paper, where we look for only one single defective (or positive) unit.
References
1. Garey, M. and Hwang, F. K. (1973). Isolating a single defective
using group-testing. Preprint.
2. Hwang, F. K. (1974). On ﬁnding a single defective in binomial
group testing. Journal of the American Statistical Association 69,
151–153.
3. Hwang, F. K. (1980). Optimal group-testing procedures in iden-
tifying a single defective from a ﬁnite population. Bulletin of the
Institute of Mathematics, Academia Sinica 8, 129–140.
4. Kumar, S. and Sobel, M. (1971). Finding a single defective in bino-
mial group-testing. Journal of the American Statistical Association
66, 824–828.
5. Sobel, M. and Groll, P. A. (1959). Group-testing to eliminate eﬃ-
ciently all defectives in a binomial sample. Bell System Technical
Journal 38, 1179–1252.
6. Sobel, M. and Groll, P. A. (1966). Binomial group-testing with an
unknown proportion of defectives. Technometrics 8, 631–656.
©2001 CRC Press LLC

26
Testing Hypotheses on Variances in the
Presence of Correlations
A. M. Mathai and P. G. Moschopoulos
McGill University, Montreal, Quebec, Canada
The University of Texas at El Paso, El Paso, TX
ABSTRACT The classical technique of testing the hypothesis of equal-
ity of variances in a bivariate normal population, when the population
correlation is nonzero, is to construct linear functions of the variables so
that the correlation between these linear functions is zero under the null
hypothesis. Then the problem reduces to testing the hypothesis that the
correlation of these linear functions is zero. In this paper we show that
an extension of the technique to a p-variate normal when p ≥3 is not
possible. Thus we consider the likelihood ratio principle for the problem.
We also consider the likelihood ratio test for several variations of the hy-
pothesis. Both the exact null and nonnull moments and distributions of
these test statistics are discussed in this paper.
Keywords and phrases Likelihood ratio, bivariate normal, multivari-
ate normal, Lauricella function, Type-2 beta integral, gamma function
26.1
Bivariate Normal Population
Let X, with X′ = (x1, . . . , xp), have a p-variate nonsingular normal
distribution, that is, X ∼Np(µ, Σ), Σ > 0. For p = 2 let the covariance
matrix
Σ =

σ2
1
σ1σ2ρ
σ1σ2ρ
σ2
2

.
The hypothesis of interest is H0 : σ2
1 = σ2
2 where ρ ̸= 0. For conve-
nience let x1 = x and x2 = y. An ingeneous technique used by Pitman
(1939) is to consider the linear functions
u = x + y and v = x −y
©2001 CRC Press LLC

Then the covariance between u and v is
Cov(u, v) = Var(x) −Var(y) = σ2
1 −σ2
2
which is zero under H0 and thus the correlation between u and v is
zero under H0. Then, the hypothesis of equality of the two variances
is equivalent to the hypothesis that the correlation between u and v is
zero. Testing can be achieved by using the well known distribution of
the sample correlation between u and v.
As it has been shown by Morgan (1939), for p = 2 the likelihood
ratio principle also leads to u and v above. The maximum likelihood
estimate for the population mean vector µ is the corresponding sample
mean vector. Let L(ˆµ) be the likelihood function at this estimate ˆµ.
L(ˆµ) = (2π)−Np/2 | Σ |−N/2 e−1
2 tr(Σ−1S),
where S is the sample sum of products matrix
S = (sij), sij = ΣN
k=1(xik −¯xi)(xjk −¯xj)
with xik denoting the k-th observation on the i-th component of X. For
the general parameter space it is known that
max(L) = (2π)−Np/2e−Np/2
| S/N |N/2
.
For p = 2, under H0 let L be L0. Then,
Σ = Σ0 = σ2R = σ2

1
ρ
ρ
1

and
| Σ0 |= (σ2) | R |= (σ2)2(1 −ρ2)
∂ln L0
∂σ2
= 0
⇒
ˆσ0
2 = tr(ˆR−1S)
Np
= tr(ˆR−1S)
2N
(26.1.1)
where ˆR is the estimate of R. Taking the derivative of ln L0 with respect
to ρ, equating to zero and solving for ˆσ0
2 and ˆρ we have
ˆρ =
2s12
s11 + s22
and
ˆσo
2 = s11 + s22
2N
.
Substituting these back the λ-criterion is given by, denoting λ2/N = z,
z = λ2/N =
s11s22 −s2
12
(s11 + s22)2 −4s2
12
.
Here we reject H0 for small values of z.
©2001 CRC Press LLC

But note that if we had used the Pitman approach then we would
have started with u = x1 + x2 and v = x1 −x2. Let g denote the sample
correlation between u and v. Then
g =
N
i=1(ui −¯u)(vi −¯v)
N
i=1(ui −¯u)2 N
i=1(vi −¯v)2
=
N
i=1[(x1i −¯x1) + (x2i −¯x2)][(x1i −¯x1) −(x2i −¯x2)]
N
i=1[(x1i −¯x1) + (x2i −¯x2)]2 N
i=1[(x1i −¯x1) −(x2i −¯x2)]2
=
s11 −s22

(s11 + s22 + 2s12)(s11 + s22 −2s12)
=
s11 −s22

(s11 + s22)2 −4s2
12
.
Consider
1 −g2 = 1 −
(s11 −s22)2
[(s11 + s22)2 −4s2
12] =
4[s11s22 −s2
12]
[(s11 + s22)2 −4s2
12]
= 4z = 4λ2/N.
Thus both procedures lead to the same t-test, based on g; see also
Cacoullos (2000) for the corresponding F-test.
It is not possible to extend the Pitman-Morgan procedure to the case
p = 3. Consider the linear functions
u1 = a1x1 + a2x2 + a3x3 and u2 = b1x1 + b2x2 + b3x3
where X′ = (x1, x2, x3) and X ∼N3(µ, Σ), Σ > 0.
The covariance
between u1 and u2 is then
Cov(u1, u2) = a1b1V ar(x1) + a2b2V ar(x2) + a3b3V ar(x3)
+ a1b2Cov(x1, x2) + a1b3Cov(x1, x3)
+ a2b1Cov(x1, x2) + a2b3Cov(x2, x3)
+ a3b1Cov(x1, x3) + a3b2Cov(x2, x3).
Let H0 : σ2
1 = σ2
2 = σ2
3. Even under H0, since the various correlations are
arbitrary, if Cov(u1, u2) = 0, the following equations are to be satisﬁed.
a1b1 + a2b2 + a3b3 = 0
©2001 CRC Press LLC

a1b2 + a2b1 = 0
a1b3 + a3b1 = 0
a2b3 + a3b2 = 0.
The last three equations are in matrix notation
(a1, a2, a3)


b2
b3
0
b1
0
b3
0
b1
b2

= (0, 0, 0).
For a non-null solution for (a1, a2, a3) we must have the determinant






b2
b3
0
b1
0
b3
0
b1
b2






= 0.
Then at least one of b1, b2, b3 is zero. Thus at most two variables can
enter into u1 and u2 and hence the technique cannot work for p ≥3.
26.2
Modifying the Hypothesis
In the general p-variate normal the diﬃculties arise due to the fact that
the population correlations are unequal. If the population correlations
are equal, that is, ρij = ρ for all i ̸= j, then the problem becomes
simpler. Let us modify the hypothesis to the following form :
H0 : σ2
1 = · · · = σ2
p = σ2,
ρij = ρ for all i and j, i ̸= j
where σ2 and ρ are unknown. Then it is not diﬃcult to show that under
H0 we can construct two linear functions which are uncorrelated. For
example for p = 3, consider
u =
p

1
xi and v =
p−1

1
xi −(p −1)xp.
The correlation between u and v is zero under the above H0 and then
the sample correlation between u and v can be used as a test statistic.
Here the likelihood ratio procedure also works. Under H0 we have :
Σ = σ2R,
R =


1
ρ
· · ·
ρ
ρ
1
· · ·
ρ
...
...
...
...
ρ
· · ·
· · ·
1


©2001 CRC Press LLC

| Σ |= (σ2)p(1 −ρ)p−1[1 + (p −1)ρ]
and
Σ−1 =
1
σ2(1 −ρ)[1 + (p −1)ρ]
×


1 + (p −2)ρ
−ρ
· · ·
−ρ
−ρ
1 + (p −2)ρ
· · ·
−ρ
...
...
...
...
−ρ
−ρ
· · ·
1 + (p −2)ρ

.
The maximum likelihood estimate of σ2 is as given in (26.1.1), that is,
ˆσ2 = tr(ˆR−1S)
Np
.
(26.2.1)
Let
Σ−1 = A =


a
b
. . .
b
b
a
. . .
b
...
...
...
...
b
b
. . .
a

.
Then
|A| = (a −b)p−1[a + (p −1)b].
Diﬀerentiating the log-likelihood with respect to a and b and equating
to zero we have :
N(p −1)
ˆa −ˆb
+
N
ˆa + (p −1)ˆb
−tr(S) = 0,
(26.2.2)
−N(p −1)
ˆa −ˆb
+
N(p −1)
ˆa + (p −1)ˆb
−

i̸=j
sij = 0.
(26.2.3)
where N is the sample size. From (26.2.2) and (26.2.3) we get
ˆa + (p −1)ˆb =
Np
tr(S) + 
i̸=j sij
(26.2.4)
and
©2001 CRC Press LLC

ˆa −ˆb = Np(p −1
)
(p −1)tr(S) −

i̸=j
(sij).
(26.2.5)
Therefore,
|ˆΣ0|−1 = | ˆA| =
N p(p −1)p−1pp
[(p −1)tr(S) −
i̸=j sij](p−1)[tr(S) + 
i̸=j sij].
(26.2.6)
Then from (26.2.6), observing that the maximum of L in the whole
parameter space is proportional to | S | we have
u =
λ2/N
pp(p −1)p−1 =
| S |
[(p −1)tr(S) −Σi̸=jsij]p−1[tr(S) + Σi̸=jsij].
(26.2.7)
We note that the modiﬁed hypothesis agrees with the hypothesis of
equality of variances and equality of covariances considered by Wilks
(1946). The method used by Wilks to derive the null moments is quite
lengthy. Here we will consider a simple alternative method of deriving
the null and nonnull moments of u. The nonnull moments as well as the
following procedures do not seem to be available in the literature.
26.3
Nonnull Moments
Let
u =
|S|
[(p −1)tr(S) −Σi̸=j sij]p−1 [tr(S) + Σi̸=jsij]
.
E

uh
=

S>0
|S|
n
2 +h−p+1
2
e
−1
2tr(Σ−1S)
[(p−1)tr(S)−Σi̸=jsij]
(p−1)h [tr(S)+Σi̸=jsij]
h 2
np
2
Γp( n
2 ) |Σ|
n
2 dS
for a general Σ, n = N −1, where, for example,
Γp(α) = π
p(p−1)
4
Γ(α)Γ

α −1
2

· · · Γ

α −p −1
2

, ℜ(α) > p −1
2
,
©2001 CRC Press LLC

where ℜ(·) denotes the real part of (·). Replace S
2 by S which will get
rid of all factors containing 2. Replace two of the factors by equivalent
integrals.
1
[(p −1)tr(S) −Σi̸=j sij](p−1)h
=
1
Γ[(p −1)h]

x>0
x(p−1)h−1 e−x[(p−1)tr(S)−Σi̸=j sij] dx,
for ℜ(h) > 0.
1
[tr(S) + Σi̸=j sij]h
=
1
Γ(h)

y>0
yh−1 e−y[tr(S)+Σi̸=j sij] dy, for ℜ(h) > 0.
The exponent in E

uh
reduces to the following, excluding −1/2:
tr

Σ−1S

+ x [(p −1)tr(S) −Σi̸=j sij] + y [tr(S) + Σi̸=j sij]
=
tr

Σ−1S

+ [y + (p −1)x]tr(S) + (y −x)Σi̸=j sij
=
tr

Σ−1S

+ tr(AS) = tr

(Σ−1 + A

S),
where
A =


y + (p −1)x
y −x
· · ·
y −x
y −x
y + (p −1)x
...
...
...
y −x
y −x
· · ·
y −x
y + (p −1)x


.
Integral over S, along with Γp
 n
2

|Σ|
n
2 yields,


Σ−1 + A


−( n
2 +h) Γp
 n
2 + h

Γp
 n
2

|Σ|
n
2
= |Σ|h Γp
 n
2 + h

Γp
 n
2

|I + AΣ|−( n
2 +h).
(26.3.1)
©2001 CRC Press LLC

Now we examine the factor |I + AΣ|−( n
2 +h). One can write
A
=
px I + (y −x)


1
· · ·
1
...
1
· · ·
1


=
px I + (y −x)J, J =


1
· · ·
1
...
1
· · ·
1

.
Note that since J is symmetric there exists an orthonormal matrix Q
which will reduce J to its canonical form.
The eigenvalues of J are
p, 0, . . . , 0. Hence
Q′JQ =


p
0
· · ·
0
0
0
· · ·
0
...
...
...
0
0
· · ·
0

.
Then
pxI + (y −x)J
=
pxQQ′ + (y −x)QQ′JQQ′
and
|I + AΣ| =




I +
 py
O
O
O

V +
 O
O
O
pxI

V




 , V = Q′ΣQ.
Then
 py
O
O
O

V =
 py
O
O
O
  v11
V12
V21
V22

=
 v11py
pyV12
O
O

and
 O
O
O
pxI
  v11
V12
V21
V22

=

O
O
pxV21
pxV22

.
Now,




I +

py
O
O
O

V




 = 1 + v11py.
Therefore
|I + AΣ| = (1 + v11py)




I + px

V22 −
py
1 + v11py V21V12




 .
Note that by expanding the following determinant in two diﬀerent ways
we have
©2001 CRC Press LLC






1
(px)(py)
1+v11py V12
V21 I + pxV22





= 1




I + pxV22 −(px)(py)
1 + v11py V21V12




≡|I + pxV22|

1 −(px)(py)
1 + v11py V12 (I + pxV22)−1 V21

.
Hence
|I + AΣ| = |I + pxV22|

1 + y

pv11 −p2xV12 (I + pxV22)−1 V21

.
Now we can integrate out y. That is,
1
Γ(h)
 ∞
y=0
yh−1 
1 + y

pv11 −p2xV12 (I + pxV22)−1 V21
−( n
2 +h)
dy
=
Γ
 n
2

Γ
 n
2 + h
 (pv11)−h

1 −px V12
v11
(I + pxV22)−1 V21
−h
. (26.3.2)
Substituting
|I + pxV22|

1 −px V12
v11
(I + pxV22)−1 V21

=


I + px

V22 −V21v−1
11 V12


 .
The integral over x, denoted by Jx, is of the following form:
Jx =
1
Γ[(p −1)h]
 ∞
x=0
x(p−1)h−1
× |I + pxV22|−n
2 

I + px

V22 −V21v−1
11 V12


−h dx.
Let the eigenvalues of pV22 be λ1, . . . , λp−1 and that of p(V22−V21v−1
11 V12)
be µ1, . . . , µp−1 where λj, µj > 0,
j = 1, . . . , p −1 since pV22 and
p(V22 −V21v−1
11 V12) are symmetric positive deﬁnite matrices. Then the
integral reduces to the form
Jx =
1
Γ[(p −1)h]
 ∞
x=0
x(p−1)h−1 [(1 + λ1x) · · · (1 + λp−1x)]−n
2
× [(1 + µ1x) · · · (1 + µp−1x)]−h dx
(26.3.3)
©2001 CRC Press LLC

Put
x =
z
1 −z →dx =
dz
(1 −z)2 and 0 < z < 1.
Then
Jx =
1
Γ[(p −1)h]
 1
z=0
z(p−1)h−1(1 −z)(p−1) n
2 −1
×

(1 −λ′
1z) · · ·

1 −λ′
p−1z
−n
2 
(1 −µ′
1z) · · ·

1 −µ′
p−1z
−h dz,
λ′
j = 1 −λj, µ′
j = 1 −µj.
(26.3.4)
In order to write (26.3.4) as a Lauricella function we need a condition to
be satisﬁed by the coeﬃcient of z in the various factors. Take any large
enough number a such that
a > max {λ1, . . . , λp−1, µ1, . . . , µp−1} .
Replace x by x
a in (26.3.3). Then (26.3.4) becomes
Jx =
1
Γ[(p −1)h]a(p−1)h
 1
z=0
z(p−1)h−1(1 −z)(p−1) n
2 −1
×

(1 −λ∗
1z) · · ·

1 −λ∗
p−1z
−n
2 
(1 −µ∗
1z) · · ·

1 −µ∗
p−1z
−h dz,
λ∗
j = 1 −λj
a , µ∗
j = 1 −µj
a , 0 < λ∗
j < 1, 0 < µ∗
j < 1,
j = 1, . . . , p −1.
(26.3.5)
Then (26.3.5) can be written in terms of a Lauricella function FD. From
formula (4.8.14) of Mathai (1993) we have
Jx =
1
a(p−1)h
Γ

(p −1) n
2

Γ

(p −1)
 n
2 + h
 FD

(p −1)h, n
2 , . . . , n
2 , h, . . . , h;
(p −1)
n
2 + h

; λ∗
1, . . . , λ∗
p−1, µ∗
1, . . . , µ∗
p−1

.
(26.3.6)
Now collecting all the factors we have the nonnull moment of u,
E

uh
= |Σ|h Γp
 n
2 + h

Γp
 n
2

Γ
 n
2

Γ
 n
2 + h

Γ

(p −1) n
2

Γ

(p −1)
 n
2 + h

×
1
(pv11)h a(p−1)h
©2001 CRC Press LLC

× FD

(p −1)h, n
2 , . . . , n
2 , h, . . . , h; (p −1)
n
2 + h

;
λ∗
1, . . . , λ∗
p−1, µ∗
1, . . . , µ∗
p−1

for ℜ(h) > p−n−1
2
, where λ∗
j, µ∗
j are deﬁned in (26.3.5), FD is given in
(26.3.4) and v11 is available from (26.3.1).
26.4
Null Case
In the null case
|Σ| = (σ2)p(1 −ρ)p−1[1 + (p −1)ρ]
and I + AΣ is a matrix with the diagonal elements
1 + σ2[1 + (p −1)ρ]y + σ2(p −1)(1 −ρ)x
and the non-diagonal elements equal to −σ2(1 −ρ)x + σ2[1 + (p −1)ρ]y.
Then the determinant is given by
|I + AΣ| = {1 + σ2p(1 −ρ)x}p−1{1 + σ2p[1 + (p −1)ρ]y}.
Now, the integral over x, evaluating with the help of a type-2 beta inte-
gral, yields
1
Γ[(p −1)h]
 ∞
0
x(p−1)h−1{1 + σ2p(1 −ρ)x}−(p−1)( n
2 +h)dx
= [p(1 −ρ)σ2]−(p−1)h
Γ[(p −1) n
2 ]
Γ[(p −1)( n
2 + h)] for ℜ(h) > −n
2 .
Integrating y with the help of a type-2 beta integral yields
1
Γ(h)
 ∞
0
yh−1{1 + pσ2[1 + (p −1)ρ]y}−( n
2 +h)dy
= {pσ2[1 + (p −1)ρ]}−h
Γ( n
2 )
Γ( n
2 + h) for ℜ(h) > −n
2 .
Combining all the factors and observing that |Σ|h is cancelled we have
E[(ppu)h|H0] = Γp( n
2 + h)
Γp( n
2 )
Γ[(p −1) n
2 ]
Γ[(p −1)( n
2 + h)]
Γ( n
2 )
Γ( n
2 + h)
for ℜ(h) > −n
2 + p −1
2
.
©2001 CRC Press LLC

Expanding Γ[(p −1)( n
2 )] and Γ[(p −1)( n
2 + h)] with the help of the
multiplication formula for gamma functions, namely,
Γ(mz) = (2π)
1−m
2
mmz−1
2 Γ(z)Γ

z + 1
m

· · · Γ

z + m −1
m

,
m = 1, 2, · · · ,
(26.4.1)
and opening up Γp(.) we have,
E[vh|H0] = c
Γ( n−1
2
+ h)Γ( n−2
2
+ h) · · · Γ( n
2 −p−1
2
+ h)
Γ( n
2 + h)Γ( n
2 +
1
p−1 + h) · · · Γ( n
2 + p−2
p−1 + h)
(26.4.2)
where
v = (p −1)p−1ppu = λ2/N
and c is a normalizing constant such that E(vh) = 1 when h = 0. From
the gamma structure in (4.2) it is easy to observe that the density of v
in particular cases can be easily written down. For example when
p = 2, E(vh) = c Γ( n
2 −1
2 + h)
Γ( n
2 + h)
⇒v ∼type-1 beta
n −1
2
, 1
2

.
For p = 3,
E(vh) = c Γ( n−2
2
+ h)Γ( n−2
2
+ 1
2 + h)
Γ( n
2 + h)Γ( n
2 + 1
2 + h)
.
Combining the gammas with the help of the multiplication formula for
gamma functions of (4.1) with m = 2, we have
E(v
1
2 )h = c1
Γ(n −2 + h)
Γ(n + h)
,
where c1 is a normalizing constant. Then v
1
2 is a type-1 beta with the
parameters (n −2, 2).
For p = 4, two gammas in the numerator of
(4.2) diﬀer by an integer and hence the density is available in terms of
a psi function. For p = 5, combining the gammas with the help of the
multiplication formula we have
E(vh) = c2
Γ(n −2 + 2h)Γ(n −4 + 2h)
Γ(n + 2h)Γ(n −1
2 + 2h)
.
Thus the density of v
1
2 can be evaluated in terms of a psi function since
the numerator gammas diﬀer by an integer. For p ≥6 the density can be
evaluated in terms of series involving psi and generalized zeta functions.
For details see Mathai (1993).
©2001 CRC Press LLC

From the gamma structure in (26.4.2) we can easily derive some ap-
proximations. To this end, replace h by nh =
n
2 (2h) or consider the
h-th moment of vn. Then expand each gamma by using the asymptotic
expansion of a gamma function, namely,
Γ(z + a) ≈
√
2πzz+a−1
2 e−z,
(26.4.3)
where |z| →∞and a is bounded. In (26.4.2), with h replaced by nh,
take z = n(1 + 2h)/2 and the remaining part in each gamma as a. Now,
expanding and simplifying the gammas we have
E(vn)h ≈(1 + 2h)−1
4 (p2+p−4).
Consider the variable w = −ln(vn). Then the moment generating func-
tion of w, taking h as the parameter, namely,
E(ehw) = E

eh(−ln vn)
= E(vn)−h ≈(1 −2h)−1
4 (p2+p−4).
Since the right side is the moment generating function of a chi-square
with ν = 1
2(p2 + p −4) degrees of freedom we have
w ≈χ2
ν.
(26.4.4)
If better approximations are needed then we can use an extended version
of (26.4.3) which involves generalized Bernoulli polynomials. Then tak-
ing successive terms we can obtain w as a linear function of chi-square
variables with diﬀerent degrees of freedom where the leading term is
that in (26.4.4).
The approximation in (26.4.4) also agrees with the
large sample approximation for −2 ln λ.
26.5
The Conditional Hypothesis
We consider now the hypothesis
H0 : σ2
1 = σ2
2 = . . . = σ2
p
given that ρij = ρ. We call this the conditional hypothesis. Diﬀeren-
tiation with respect to σ2
i , i = 1, . . . , p yields the following likelihood
equations :
N + c(p, ρ)

−[1 + (p −2)ρ] s11
σ2
1
+ ρ
 s12
σ1σ2
+ · · · + s1p
σ1σp
 
= 0
©2001 CRC Press LLC

...
N + c(p, ρ)

−[1 + (p −2)ρ] spp
σ2p
+ ρ
 sp1
σpσ1
+ · · · + sp p−1
σpσp−1
 
= 0
where
c(p, ρ) =
1
(1 −ρ)[1 + (p −1)ρ].
(26.5.1)
Taking the sum of the above equations we have :
Np + c(p, ρ)


−[1 + (p −2)ρ]
s11
σ2
1
+ . . . + spp
σ2p

+ 2ρ

i<j
sij
σiσj


= 0.
(26.5.2)
Diﬀerentiating with respect to ρ we have:
Np(p −1)ρ −

(p −2)
s11
σ2
1
+ · · · + spp
σ2p

−2

i<j
sij
σiσj


+[(p −2) −2(p −1)ρ]
(1 −ρ)[1 + (p −1)ρ]
×

(1 + (p −2)ρ)
s11
σ2
1
+ · · · + spp
σ2p

−2ρ

i<j
sij
σiσj

= 0.
(26.5.3)
From (26.5.2) and (26.5.3) we have
ρ = p −2
p −1 −
1
Np(p −1)


(p −2)
s11
σ2
1
+ · · · + spp
σ2p

−2

i<j
sij
σiσj


.
(26.5.4)
The equations (26.5.2), (26.5.3) and (26.5.4) are satisﬁed for
ˆσ2
i = sii
N
and
ˆρ =
2
p(p −1)

i<j
rij,
rij =
sij
√sii sjj
.
©2001 CRC Press LLC

In the general space



ˆΣ



 = s11 · · · spp
N p
[p(p −1) −2 
i<j rij]p−1 [p + 2 
i<j rij]
pp (p −1)p−1
.
Under the null hypothesis, we had evaluated the estimate of |Σ| earlier.
That is,
| ˆ
Σ0| =

(p −1) tr(S) −2

i<j
sij


p−1
×

tr(S) + 2

i<j
sij

/

(Np)p(p −1)p−1
.
Let
ˆR =


1
r12
· · ·
r1p
r12
1
· · ·
r2p
...
rp1
rp2
· · ·
1

, D = diag
√s11, . . . , √spp

J =


1
· · ·
1
...
1
· · ·
1

, S = D ˆRD.
Then
p(p −1) −2

i<j
rij = p2 −tr(J ˆR) ⇒
p + 2

i<j
rij = tr(J ˆR).
p tr(S) = p tr(D2)
(p −1) tr(S) −2

i<j
sij = p tr(D2) −tr[(DJD) ˆR].
tr(S) + 2

i<j
sij = tr[(DJD) ˆR].
Let
u = N p pp (p −1)p−1 λ
2
N
(26.5.5)
where λ is the likelihood ratio criterion. Then
©2001 CRC Press LLC

u =
s11 · · · spp [p2 −tr(J ˆR)]p−1[tr(J ˆR)]
[p tr(D2) −tr(DJD) ˆR]p−1[tr[(DJD) ˆR]
.
(26.5.6)
In order to compute the h-th moment one can integrate over the density
of S. For convenience replace S
2 by S and take the integral. Then
E(uh|H0) =

S>0
(s11 · · · spp)h[p2 −tr(J ˆR)](p−1)h[tr(J ˆR)]h
[p tr(D2) −tr{(DJD) ˆR}](p−1)h[tr{(DJD) ˆR}]h
×|S|
n
2 −p+1
2
e−trΣ−1
0
S
|Σ0|
n
2 Γp( n
2 )
dS
where
|S|
n
2 −p+1
2
= |D2|
n
2 −p+1
2
| ˆR|
n
2 −p+1
2
= (s11 · · · spp)
n
2 −p+1
2
| ˆR|
n
2 −p+1
2 .
Then for large values of N we have −2 ln λ ≈χ2
p−1 where λ is available
from (26.5.5) and (26.5.6). The exact nonnull moments and the exact
distributions seem to be diﬃcult to evaluate.
References
1. Cacoullos, T. (2000). The F-test of homoscedasticity for correlated
normal variables. Statistics & Probability Letters, to appear.
2. Mathai, A. M. (1993). A Handbook of Generalized Special Func-
tions for Statistical and Physical Sciences. Oxford University Press,
Oxford.
3. Morgan, W. A. (1939). A test for the signiﬁcance of the diﬀerence
between the two variances in a sample from a normal bivariate
population. Biometrika 31, 13–19.
4. Pitman, E. J. G. (1939). A note on normal correlation. Biometrika
31, 9–12.
5. Wilks, S. S. (1946). Sample criteria for testing equality of means,
equality of variances, and equality of covariances in a normal mul-
tivariate distribution. Annals of Mathematical Statistics 17, 257–
281.
©2001 CRC Press LLC

27
Estimating the Smallest Scale Parameter:
Universal Domination Results
Stavros Kourouklis
U n i v e r s i t y o f P a t ra s , P a t ra s , G reece
ABSTRACT In this work universal domination results are obtained
for the problem of estimating the smallest of two scale parameters in
independent populations with monotone likelihood ratios. In particu-
lar, for estimating the smallest of two normal variances several estima-
tors universally dominating the minimum mean squared error estimator
based on the corresponding sample variance are derived.
Keywords and phrases Decision
theory,
universal
domination,
ordered restricted inference, scale parameter, monotone likelihood ra-
tio
27.1
Introduction
Wald’s (1950) theory of statistical decisions is based on the assumption
of a particular and fully speciﬁed loss. In practice, however, it is often
diﬃcult to specify the loss function exactly. Besides, if an estimator T2
dominates another estimator T1 with respect to a loss L1 there is no
guarantee in general that T2 will still be better than T1 under a diﬀerent
loss L2. Hwang (1985) proposed the criterion of universal domination
(u-domination) as a means of studying how robust the superiority of one
estimator over another is with respect to the loss function. This criterion
was further developed by Brown and Hwang (1989), whereas, earlier,
similar concepts were considered in a less formal manner by Cohen and
Sackrowitz (1970), Brown (1971) and Rukhin (1987).
For a scale parameter σ > 0 Hwang’s (1985) criterion is deﬁned as
follows. If
EL (|T2/σ −1|) ≤EL (|T1/σ −1|)
(27.1.1)
for every nondecreasing function L(·) and every σ > 0, and for a partic-
©2001 CRC Press LLC

ular L(·) the risk functions are not identical then T2 u-dominates T1 and
the latter is said to be u-inadmissible. If there does not exist estima-
tor that u-dominates T1, then T1 is called u-admissible. Hwang (1985)
showed that u-domination is equivalent to stochastic domination which
in our setup means that (27.1.1) holds iﬀ
P (|T2/σ −1| ≤c) ≥P (|T1/σ −1| ≤c) ∀c > 0, σ > 0 
(27.1.2)
and for some σ > 0 the distribution functions are not identical.
Recently Kushary (1998) dealt with u-admissibility and u-inadmis-
sibility of equivariant estimators of the scale parameter in the one pa-
rameter gamma and the two parameter normal and exponential distri-
butions. Subsequently, Kourouklis (1999) extended these results to a
scale family of distributions with monotone likelihood ratio and to two
normal or two exponential distributions for estimating the ratio of the
respective scale parameters.
In this article we focus on ordered scale parameters. We refer to the
book by Robertson, Wright, and Dykstra (1988) for an excellent account
on order restricted inference. When σ1, σ2 are the variances of two nor-
mal populations and σ1 ≤σ2, Kushary and Cohen (1989) demonstrated
the u-inadmissibility of the standard estimator of σ1, i.e. the minimum
mean squared error estimator of σ1 based only on the sample variance
from the ﬁrst population, in the equal sample sizes case by exhibiting a
u-dominating estimator which uses both sample variances.
In Section 27.2 we assume that σ1 ≤σ2 are scale parameters of two
general independent populations with monotone likelihood ratios and
derive suﬃcient conditions for the u-inadmissibility of an arbitrary scale
equivariant estimator of σ1. In particular, when σ1 ≤σ2 are normal
variances we establish u-inadmissibility of the standard estimator of σ1
for general sample sizes as well as u-inadmissibility of Kushary’s and Co-
hen’s (1989) estimator by deriving u-dominating estimators also based
on both sample variances. Then using also the sample means we de-
rive additional estimators of σ1 which u-dominate all the above ones.
Analogous results hold when σ1 ≤σ2 are the scale parameters of two
independent exponential populations but for the sake of brevity they will
not be presented here. Also, the results can easily be extended to more
than two populations.
27.2
Main Results
We start with some auxiliary results. Let X be a statistic such that
X/σ has density f(x)I(x > 0) where σ > 0 is an unknown scale param-
©2001 CRC Press LLC

eter. We assume that the family of distributions of X has the monotone
likelihood ratio property, i.e.,
f(c1x)
f(c2x) is strictly increasing in x > 0 for every 0 < c1 < c2.
(27.2.1)
Our main results are based on the following lemmas.
LEMMA 27.2.1
[Iliopoulos and Kourouklis (2000)]
The function f(x) is continuous on (0, ∞), xf(x) is strictly increasing
and then strictly decreasing, and
lim
x→0 xf(x) = lim
x→∞xf(x) = 0.
Lemma 27.2.1 ensures that xf(x) has a unique mode, which we denote
by a0.
LEMMA 27.2.2
[Kourouklis (1999)]
Let G(a) = P

aX
σ −1
 ≤c

, a > 0, c > 0. Then we have the follow-
ing.
(i) G(a) is strictly decreasing on (2/a0, ∞) for every c > 0.
(ii) If, in addition,
(x + y)f(x + y) > (x −y)f(x −y) whenever a0 ≥x > y > 0 (27.2.2)
then G(a) is strictly decreasing on (1/a0, ∞) for every c > 0.
PROOF (i) Let F(·) be the distribution function of X/σ. Then G(a) =
F
1 + c
a

−F
1 −c
a

.
Since X takes on positive values, for c ≥
1, G(a) = F
1 + c
a

which is obviously strictly decreasing in a. For
0 < c < 1 we obtain G′(a) = −1 + c
a2 f
1 + c
a

+ 1 −c
a2 f
1 −c
a

which
is negative provided
1 + c
a
f
1 + c
a

> 1 −c
a
f
1 −c
a

.
(27.2.3)
Now for a > 2/a0 we have 1 −c
a
< 1 + c
a
< a0 and thus (27.2.3) holds
by Lemma 27.1.1.
(ii) Applying (27.2.2) with x = 1
a, y = c
a, 0 < c < 1, a > 1/a0, we see
that (27.2.3) holds, and hence the result follows.
©2001 CRC Press LLC

LEMMA 27.2.3
If f1(x) and f2(x) are positive, unimodal functions on (0, ∞) with modes
x1 and x2 respectively (i.e., fi(x) is strictly increasing for 0 < x < xi
and strictly decreasing for x > xi, i = 1, 2) and
f2(x)
f1(x) is strictly increasing in x > 0
(27.2.4)
then x2 ≥x1.
PROOF Assume x2 < x1. Then on (x2, x1) f2(x) is decreasing and f1(x)
is increasing, so that f2(x)
f1(x) is decreasing on (x2, x1) which contradicts
(27.2.4).
Let now Si, i = 1, 2, be independent statistics such that Si/σi has
density gi(x)I(x > 0), where σ1 and σ2 are unknown positive parameters
with σ1 ≤σ2. We further assume that gi(x) satisﬁes (27.2.1). We will
study two classes of estimators of σ1, namely C = {aS1 : a > 0} and
D = {φ(V )S1 : φ is a positive function }, where V = S2/S1.
The
ﬁrst one is the class of scale equivariant estimators of σ1 based only on
S1. This class does not make use of the full data and, in eﬀect, ignores
the information σ1 ≤σ2.
The second one is the class of equivariant
estimators under the group of transformations (S1, S2) →(aS1, aS2),
a > 0. Clearly, C ⊂D.
For v > 0 let g1(x|v; σ1, σ2) be the density of the conditional distribu-
tion of S1/σ1 given V = v, i.e.
g1(x|v; σ1, σ2) ∝xg1(x)g2(σ1
σ2
xv).
When σ1 = σ2 we simply write g1(x|v).
We observe that since the
gi(x)’s satisfy (27.2.1) so does g1(x|v; σ1, σ2). We denote by m(v; σ1, σ2)
the mode of xg1(x|v; σ1, σ2) but when σ1 = σ2 we just write m(v) (which
does not depend on σi, i = 1, 2).
THEOREM 27.2.4
Let δ = φ(V )S1 be an estimator in the class D.
Then we have the
following.
(i) δ is u-dominated by δ1 = min{φ(V ), 2/m(V )}S1 provided δ1 ̸= δ with
positive probability.
(ii) If for every v > 0 and σ1 ≤σ2 g1(x|v; σ1, σ2) satisﬁes (27.2.2) then
both δ and δ1 are u-dominated by δ2 = min{φ(V ), 1/m(V )}S1 provided
δ2 ̸= δ with positive probability.
©2001 CRC Press LLC

PROOF (i) By Lemma 27.2.2(i), for every v > 0 the conditional proba-
bility
P
aS1
σ1
−1
 ≤c|V = v

is decreasing for a >
2
m(v; σ1, σ2). Further-
more,
xg1(x|v; σ1, σ2)
xg1(x|v)
∝
g2(σ1
σ2
xv)
g2(xv)
is strictly increasing in x > 0 (unless σ1 = σ2) since g2 satisﬁes (27.2.1)
and σ1 ≤σ2.
Thus, Lemma 27.2.3 implies m(v; σ1, σ2) ≥m(v) and
consequently
P

2
m(v)
S1
σ1
−1
 ≤c|V = v

> P
φ(v)S1
σ1
−1
 ≤c|V = v

(27.2.5)
if φ(v) >
2
m(v). Upon taking expectations on both sides of (27.2.5) we
conclude that
P

δ1
σ1
−1
 ≤c

> P

δ
σ1
−1
 ≤c

provided
P

φ(V ) >
2
m(V )

> 0.
By the equivalence of u-domination and stochastic domination (see
(27.1.1) and (27.1.2)) the result follows.
(ii) For the u-domination of δ repeat the argument of part (i) using
Lemma 27.2.2(ii). Since δ1 = φ1(V )S1 with φ1(v) = min{φ(v), 2/m(v)},
it is u-dominated by δ2 = min{φ1(v), 1/m(v)}S1 = δ1.
REMARK Theorem 27.2.4 can be applied, in particular, to any estima-
tor δ = aS1 in the class C.
We now consider independent random samples X1, . . . , Xn1∼N(µ1, σ1)
and Y1, . . . , Yn2 ∼N(µ2, σ2), ni ≥2, where all the parameters are un-
known and σ1 ≤σ2.
Set S1 =
n1

i=1

Xi −X
2 ∼σ1χ2
n1−1 and S2 =
n2

i=1

Yi −Y
2 ∼σ2χ2
n2 −1 and note that (27.2.1) is satisﬁed for the den-
sities of Si/σi, i = 1, 2. Also, in this case g1(x|v; σ1, σ2) is the density of
gamma distribution G
n1 + n2 −2
2
,
2
1 + (σ1/σ2)v

which, as shown in
Kushary (1998, Lemma 2.1), satisﬁes (27.2.2) with a0 = m(v; σ1, σ2) =
©2001 CRC Press LLC

n1 + n2 −2
1 + (σ1/σ2)v . The best estimator in the class C with respect to squared
error loss is
δ0 =
1
n1 + 1S1.
(27.2.6)
Then the following result follows directly from Theorem 27.2.4.
THEOREM 27.2.5
(i) For n2 > n1 + 4, δ0 is u-dominated by δ1 = min

δ0, 2(S1 + S2)
n1 + n2 −2

.
(ii) For n2 > 3, δ0 is u-dominated by
δ2 = min

δ0,
S1 + S2
n1 + n2 −2

.
(27.2.7)
REMARK δ3 =
S1 + S2
n1 + n2 −2 is the umvu estimator of σ1 when σ1 = σ2,
so that δ2 chooses between δ0 and δ3 depending on which is the smallest
of the two.
In the case that the samples sizes are equal, i.e. n1 = n2, Kushary
and Cohen (1989) have shown that δ0 is u-dominated by
δKC = min

δ0, 2(S1 + S2)
3(n1 + 1)

(27.2.8)
provided n1 > 6. The next theorem demonstrates that δKC is u-inadmis-
sible too.
THEOREM 27.2.6
Assume that n1 = n2 > 7. Then the estimator δKC in (27.2.8) is u-
dominated by the estimator δ2 in (27.2.7).
PROOF Writing δKC = φ(V )S, where φ(V ) = min

1
n1 + 1, 2(1 + V )
3(n1 + 1)

and applying Theorem 27.2.4 we have that δKC is u-dominated by
δ4 = min

1
n1 + 1, 2(1 + V )
3(n1 + 1),
1 + V
2(n1 −1)

S1.
Now for n1 > 7, 2(1 + V )
3(n1 + 1) >
1 + V
2(n1 −1) and thus
δ4 = min

1
n1 + 1,
1 + V
2(n1 −1)

S1 = δ1.
©2001 CRC Press LLC

In the above normal setup all the u-dominating estimators are based
only on the sample variances. We will next provide u-dominating es-
timators which, in addition, use the sample means. To this end , for
W1 = n1X
2/S1 and Z1 = n2Y
2/S1, we consider the broader class
D1 = {φ(V, W1, Z1)S1 : φ is a positive function} of equivariant estima-
tors of σ1 under the group of transformations
(X, Y , S1, S2) →(±aX, ±aY , a2S1, a2S2), a > 0.
THEOREM 27.2.7
An estimator δ = φ(V, W1, Z1)S1 in the class D1 is u-dominated by
δ∗= min
	
δ, S1 + n1X
2 + S2 + n2Y
2
n1 + n2

(which is also in D1) provided δ ̸= δ∗with positive probability.
PROOF Let L and K be independent Poisson random variables with
parameters n1µ2
1/2σ1 and n2µ2
2/2σ2 respectively. We may and will as-
sume that L and K are also independent of (X, S1, Y , S2). Then it is
easy to show that the conditional density of S1/σ1 given V = v, W1 =
w1, Z1 = z1, L = l, K = k is gamma
G
n1 + n2 + 2l + 2k
2
,
2
1 + w1 + (σ1/σ2)(v + z1)

which satisﬁes (27.2.2) with a0 =
n1 + n2 + 2l + 2k
1 + w1 + (σ1/σ2)(v + z1) (see Kushary
(1998, Lemma 2.1)). Hence, by Lemma 27.2.2(ii),
P
aS1
σ1
−1
 ≤c|V = v, W1 = w1, Z1 = z1, L = l, K = k

is strictly decreasing for a > 1 + w1 + (σ1/σ2)(v + z1)
n1 + n2 + 2l + 2k
. It follows that
P

1 + w1 + v + z1
n1 + n2
S1
σ1
−1
 ≤c|V = v, W1 = w1,
Z1 = z1, L = l, K = k

> P
φ(v, w1, z1)S1
σ1
−1
 ≤c|V = v, W1 = w1,
Z1 = z1, L = l, K = k

(27.2.9)
©2001 CRC Press LLC

if φ(v, w1, z1) > 1 + w1 + v + z1
n1 + n2
.
Upon taking expectations on both
sides of (27.2.9) we obtain that P

δ∗
σ1
−1
 ≤c

> P

δ
σ1
−1
 ≤c

provided
P

φ(V, W1, Z1) > 1 + W1 + V + Z1
n1 + n2

> 0. By virtue of (27.1.2) the
proof is now complete.
COROLLARY 27.2.8
(i) The estimator δ0 =
S1
n1 + 1  in (27.2.6) is u-dominated by
δ∗= min
	
S1
n1 + 1, S1 + n1X 
2 + S2 + n2Y 
2
n1 + n2

.
(ii) The estimator δ2 in (27.2.7) is u-dominated by
δ∗
1 = min
	
S1
n1 + 1,
S1 + S2
n1 + n2 −2, S1 + n1X 
2 + S2 + n2Y 
2
n1 + n2

.
(iii) For n1 = n2 and n1 > 3 the estimator δKC in (27.2.8) is u-
dominated by
δ∗
2 = min
	
S1
n1 + 1, 2(S1 + S2)
3(n1 + 1) , S1 + n1X 
2 + S2 + n1Y 
2
2n1

.
PROOF Immediate from Theorem 27.2.7.
REMARK We recognize that the estimator S1 + n1X 
2 + S2 + n2Y 
2
n1 + n2
that
appears in Theorem 27.2.7 is the umvue of σ1 when σ1 = σ2 and
µ1 = µ2 = 0.
REMARK The estimators δ∗
1 and δ∗
2 also u-dominate δ0.
References
1. Brown, L. D. (1971). Admissible estimators, recurrent diﬀusions,
and insoluble boundary value problems. Annals of Mathematical
Statistics 42, 855–903.
©2001 CRC Press LLC

2. Brown, L. D. and Hwang, J. T. (1989). Universal domination and
stochastic domination: U-admissibility and U-inadmissibility of the
least squares estimator. Annals of Statistics 17, 252–267.
3. Cohen, A. and Kushary, D. (1998). Universal admissibility of max-
imum likelihood estimators in constrained spaces. Statistics & De-
cisions 16, 131–146.
4. Cohen, A. and Sackrowitz, H. (1970). Estimation of the last mean
of a monotone sequence.
Annals of Mathematical Statistics 41,
2021–2034.
5. Hwang, J. T. (1985). Universal domination and stochastic domina-
tion: Estimation simultaneously under a broad class of loss func-
tions. Annals of Statistics 13, 295–314.
6. Iliopoulos, G. and Kourouklis, S. (2000). Interval estimation for
the ratio of scale parameters and for ordered scale parameters.
Statistics & Decisions, to appear.
7. Kourouklis, S. (1999). On universal admissibility of scale parame-
ter estimators. Unpublished manuscript.
8. Kushary, D. (1998).
A note on universal admissibility of scale
parameter estimators. Statistics & Probability Letters 38, 59–67.
9. Kushary, D. and Cohen, A. (1989). Estimating ordered location
and scale parameters. Statistics & Decisions 7, 201–213.
10. Robertson, T., Wright, F. T., and Dykstra, R. L. (1988). Order
Restricted Statistical Inference. John Wiley & Sons, New York.
11. Rukhin, A. L. (1987).
Universal Bayes estimators.
Annals of
Statistics 6, 1345–1351.
12. Wald, A. (1950). Statistical Decision Functions. John Wiley &
Sons, New York.
©2001 CRC Press LLC

28
On Sensitivity of Exponential Rate of
Convergence for the Maximum Likelihood
Estimator
James C. Fu
University of Manitoba, Winnipeg, Manitoba, Canada
ABSTRACT Maximum likelihood estimator derived from a paramet-
ric statistical model MO = {R, Fθ, θ ∈Θ} is often favored by statisticians
as estimating and collecting the information for the unknown parameter
θ. It has many large sample optimal properties. One of these optimal
properties is that the maximum likelihood estimator has an optimal ex-
ponential rate of convergence to the parameter θ when the underlying
distribution Q is a member of the model MO. Motivated by robust es-
timation, this article mainly studies the sensitivity of exponential rate
of convergence for the maximum likelihood estimator derived from the
parametric model MO when the true underlying distribution Q departs
slightly from the model MO. Applications of the results to exponential-
family and t-family of distributions are studied. As a byproduct, it shows
that the maximum likelihood estimator derived from the t-distribution
with 6 degrees of freedom is the best robust estimator in the sense of
local exponential rate with respect to all t-distributions.
Keywords and phrases t-distribution, robust estimation, maximum
likelihood estimate, rate of convergence, Bahadur bound, exponential
family, M-estimator, Cauchy distribution
28.1
Introduction
Let MO = {R, Fθ, θ ∈Θ} be a parametric statistical model, where
R = (−∞, ∞) is the sample space, Θ is parameter space, an open subset
of R, and for every θ ∈Θ, Fθ is a probability distribution deﬁned on
R. Let s = (x1, · · · , xn) be a sample of n independent identically dis-
tributed (i.i.d.) observations. The maximum likelihood estimator (mle)
©2001 CRC Press LLC

ˆθn(s) derived from the parametric model MO is often favored by many
statisticians for estimating and collecting the information of the unknown
parameter θ. If the true underlying distribution Q = Fθ is a member
of the statistical model MO (the model is correctly speciﬁed), then the
mle has many large sample optimal properties, for example consistency,
asymptotic normality, and exponential rate of convergence. The con-
sistency of the mle has been studied by Cram´er (1946), Wald (1949),
LeCam (1953, 1970), Bahadur (1967), and Pfanzagl (1973).
Under certain regularity conditions, the mle is asymptotically eﬃcient
in the sense that it is asymptotically normally distributed with variance
achieving the Cram´er-Rao lower bound; i.e.,
√n(ˆθn(s) −θ)
L→N(0, I−1(θ)),
(28.1.1)
where
L→stands for convergence in distribution, and I(θ) is the Fisher
information of the underlying distribution Fθ [see, for instance, Cram´er
(1946) and Rao (1963)].
Recently, Bahadur (1967, 1971, 1980), Fu (1973, 1975, 1982), Rukhin
(1983), Steinebach (1978), Jureckova (1981), Kester (1981), Kester and
Kallenberg (1986), and Rubin and Rukhin (1983) study the exponential
rates of convergence for consistent estimators. They show that for any
consistent estimator Tn(s),
lim inf
n→∞
1
n log P(|Tn −θ| ≥ε
Fθ) ≥−B(θ, ε)
(28.1.2)
and
lim inf
ε→0 lim inf
n→∞
1
nε2 log P(|Tn −θ| ≥ε
Fθ) ≥−I(θ)/2,
(28.1.3)
where B(θ, ε) is given by
B(θ, ε) = inf
θ′ {K(Fθ′, Fθ) : |θ
′ −θ| > ε},
(28.1.4)
and
K(Fθ′, Fθ) =



∞

−∞
(log
dFθ′
dFθ )dFθ′,
if Fθ′ << Fθ
∞,
otherwise,
(28.1.5)
is the Kullback-Leibler information of Fθ′ with respect to Fθ. The pos-
itive constant B(θ, ε) is usually referred as the Bahadur bound. The
estimator which achieves the lower bound of (28.1.3) is called asymptot-
ically eﬃcient in the Bahadur sense. It is well-known that for ﬁxed ε, the
mle achieves the Bahadur bound if and only if the underlying distribu-
tion is a member of an exponential family of distributions. The suﬃcient
©2001 CRC Press LLC

part was proved by Kester (1981) and the necessary part was proved by
Cheng and Fu (1986). For ε →0, under very general conditions, the mle
is always local ly optimal in the Bahadur sense that its exponential rate
achieves the lower bound of inequality (28.1.3).
All these large sample optimal properties of the mle require the as-
sumption that the true underlying distribution Q is a member of the
parametric model MO. When this basic assumption is false (Q /∈MO),
what will happen to the mle ˆθn derived from the wrongly speciﬁed model
MO? The following questions are often asked:
(i) If the true underlying distribution Q does not belong to MO, under
what conditions does the mle ˆθn derived from the wrongly speciﬁed
model MO still converge under Q? If it does converge, what value
does it converge to? Does this converge exponentially?
(ii) How sensitive is the exponential rate of convergence of the mle
when the underlying distribution Q departs slightly from the wrong-
ly speciﬁed model MO?
The main goal of this article is to answer some of the above questions.
It shows that if the underlying distribution Q is not too far away from
the wrongly speciﬁed model MO then the mle ˆθn(s) derived from the
model MO still converges to a value θ⋆whose corresponding distribution
Fθ⋆∈MO is the closest to the underlying distribution Q in terms of the
Kullback-Leibler information. Further if the moment generating function
of the score function of the speciﬁed model MO exists under Q (Q /∈
MO), then not only the mle ˆθn(s) converges to θ⋆but also converges
exponentially. This article is organized in the following way. Section 28.2
studies the main results mentioned above. In Section 28.3, exponential-
family and t-family of distributions are used to illustrate the main results.
28.2
Main Results
For each Fθ ∈MO, denote f(x|θ) as its density function, l(θ|x) =
log f(x|θ), and l(i)(θ|x) = (d/dθ)il(θ|x), i = 1, · · · , m.
For i = 1,
the function l(1)(θ|x) is referred as the score function.
Given data
s = (x1, · · · , xn), let ln(θ|s) =
n
i=1
log f(xi|θ) be the log-likelihood func-
tion.
For simplicity, if there is no special speciﬁcation, from here on
the mle ˆθn(s) derived from the model MO is assumed to be the unique
solution of the likelihood equation
l(1)
n (θ|s) = 0.
(28.2.1)
©2001 CRC Press LLC

Let q(x) be the density function of the underlying distribution Q. Fur-
ther, throughout this article, the underlying distribution Q is assumed to
be absolutely continuous with respect to (wrt) every Fθ ∈MO (Q ≪Fθ)
and the Kullback-Leibler information of Q wrt Fθ
K(Q, Fθ) =
∞

−∞
(log q(x)
f(x|θ))q(x)dx < ∞,
(28.2.2)
satisﬁes the following conditions:
(i) the Kullback-Leibler information K(Q, Fθ) is a convex function on
Θ.
(ii) θ⋆is the unique solution of the equation
K(1)(Q, Fθ) = d
dθK(Q, Fθ) = 0.
(28.2.3)
LEMMA 28.2.1
Given Q, if the Kullback-Leibler information of Q wrt Fθ, K(Q, Fθ),
satisﬁes (2.2) then
1
n
n

i=1
l(1)(θ|xi)
p
−→−K(1)(Q, Fθ),
(28.2.4)
in probability under Q as n →∞.
PROOF For given θ ∈Θ, it follows form the condition (i) that
EQl(1)(θ|x) = d
dθEQl(θ|x)
= d
dθ(−
∞

−∞
(log q(x)
f(x|θ))q(x)dx)
= −K(1)(Q, Fθ).
(28.2.5)
The result of (28.2.5) is an immediate consequence of (28.2.2) and the
weak law of large numbers. This completes the proof.
THEOREM 28.2.2
If the maximum likelihood estimator ˆθn(s) is the unique solution of the
likelihood
equation
(28.2.1)
and
the
Kullback-Leibler
information
K(Q, Fθ) satisﬁes conditions (i) and (ii), then
ˆθn(s)
p
−→θ⋆, under Q, as n →∞,
(28.2.6)
©2001 CRC Press LLC

where θ⋆is given by (ii) and
K(Q, Fθ⋆) = inf
θ∈Θ K(Q, Fθ).
(28.2.7)
PROOF Since the Kullback-Leibler information K(Q, Fθ) satisﬁes con-
ditions (i) and (ii), it follows that
K(1)(Q, Fθ)



> 0,
if θ > θ⋆,
= 0,
if θ = θ⋆,
< 0,
if θ < θ⋆.
(28.2.8)
Noting that the mle ˆθn(s) is the unique solution of the likelihood equation
(28.2.1). It follows that for every ε > 0,
P(ˆθn(s) −θ⋆> ε|Q) = P(l(1)
n (θ⋆+ ε|s) > 0|Q)
= P( 1
n
n

i=1
l(1)(θ⋆+ ε|xi) > 0
Q)
(28.2.9)
and
P(ˆθn(s) −θ⋆< −ε|Q) = P(l(1)
n (θ⋆−ε|s) < 0|Q)
= P( 1
n
n

i=1
l(1)(θ⋆−ε|xi) < 0
Q).
(28.2.10)
For every ε > 0, there exists a δ > 0 such that
K(1)(Q, Fθ⋆+ε) > δ > 0
and
P(ˆθn(s) −θ⋆> ε
Q)
= P( 1
n
n

i=1
l(1)(θ⋆+ ε|xi) > 0
Q)
= P( 1
n
n

i=1
l(1)(θ⋆+ ε|xi) + K(1)(Q, Fθ⋆+ε) > K(1)(Q, Fθ⋆+ε)
Q)
≤P( 1
n
n

i=1
l(1)(θ⋆+ ε|xi) + K(1)(Q, Fθ⋆+ε) > δ
Q).
(28.2.11)
©2001 CRC Press LLC

Lemma 28.2.1 and inequality (28.2.11) yield that
P(ˆθn(s) −θ⋆> ε
Q) →0, as n →∞.
(28.2.12)
By the same token, it follows that
P(ˆθn(s) −θ⋆< −ε
Q) →0, as n →∞.
(28.2.13)
Since ε is an arbitrary constant, the result that ˆθn(s) converges to θ⋆
is an immediate consequence of the inequalities (28.2.12) and (28.2.13).
Furthermore, the result (28.2.7) is a direct conclusion of condition (ii)
and inequality (28.2.8). This completes the proof.
If Q is not too far away from the speciﬁed model MO in the sense that,
for every θ ∈Θ, the moment generating function of the score function
l(1)(θ|x) wrt Q,
η(t|θ, Q) = EQ exp{tl(1)(θ|x)} < ∞,
(28.2.14)
exists in an interval t ∈(−δ, δ), then the following theorem shows that
the mle ˆθn(s) converges to θ⋆exponentially.
THEOREM 28.2.3
If the condition (28.2.14) holds for every θ ∈Θ and ˆθn(s) is the unique
solution of (28.2.1), the ˆθn(s) converges to θ⋆exponentially; i.e., for
every ε > 0, there exists a positive constant β such that
lim
n→∞
1
n log P(|ˆθn −θ⋆| > ε
Q) = −β,
(28.2.15)
where θ⋆satisﬁes (ii) and (28.2.7), and the exponential rate β is given
by
β = −log max(ρ(ε), ρ(−ε)), and ρ(±ε) = inf
t
>
<0
η(t|θ⋆± ε, Q). (28.2.16)
PROOF Without loss of generality, we assume θ⋆± ε ∈Θ. Since ˆθn(s)
is a unique solution of (28.2.1), it follows from the deﬁnition of θ⋆that
P(ˆθn(s) > θ⋆+ ε|Q) = P(l(1)
n (θ⋆+ ε|s) > 0|Q)
= P(
n

i=1
l(1)(θ⋆+ ε|xi) > 0
Q)
(28.2.17)
and
©2001 CRC Press LLC

P(ˆθn(s) < θ⋆−ε|Q) = P(l(1)
n (θ⋆−ε|s) < 0|Q)
= P(
n

i=1
l(1)(θ⋆−ε|xi) < 0
Q).
(28.2.18)
The results follow immediately from Chernoﬀ’s theorem [see Chernoﬀ
(1952), Fu (1975)]. This completes the proof.
Let ˜θn(s) be an M-estimator with respect to Ψ(x, θ). If exp{−Ψ(x, θ)}
is integrable and ˜θn(s) is the unique solution of the equation
n

i=1
Ψ(1)(xi, θ) =
n

i=1
d
dθΨ(xi, θ) = 0,
(28.2.19)
then the M-estimator ˜θn(s) can be viewed as the maximum likelihood
estimator derived from the model MO = {R, Fθ, θ ∈Θ} where Fθ has a
density function given by
f(x, θ) = C(θ) exp{−Ψ(x, θ)}
(28.2.20)
with
C−1(θ) =
∞

−∞
exp{−Ψ(x, θ)}dx.
Hence, all the above results for the mle can be extended to M-estimator.
It follows from Theorem 28.2.2 that
˜θn(s)
p
−→θ⋆,
(28.2.21)
as n →∞under Q, where θ⋆is the unique solution of
C(1)(θ)
C(θ)
−
∞

−∞
Ψ(1)(x, θ)dQ = 0.
(28.2.22)
Further, if
ξ(t|θ, Q) = EQ exp{−tΨ(1)(x, θ)} < ∞
(28.2.23)
exists in a neighborhood of zero, t ∈(−δ, δ), then it follows from Theo-
rem 28.2.2 that ˜θn(s) converges to θ⋆exponentially with an exponential
rate given by
β = −log max(ρ(ε), ρ(−ε))
(28.2.24)
where
ρ(±ε) = inf
t
>
<0
etC(1)(θ⋆±ε)/C(θ⋆±ε)ξ(t|θ⋆± ε, Q).
(28.2.25)
©2001 CRC Press LLC

It is worth mentioning that the least square estimator ˜θn(s) for θ can
also be viewed as mle wrt the statistical model MO = {X, Nθ, θ ∈R},
where Nθ is a normal distribution with location parameter θ. Under
very mild conditions, it can be shown that the least square estimator
˜θn(s) converges to θ⋆and is asymptotically normally distributed, but it
does not converge to θ⋆exponentially when the tail probability of the
underlying distribution Q tends to zero with a rate 1/xα (3 < α < ∞), as
x →∞. More discussions about these will be given in the next section.
The condition that the mle is the unique solution of the likelihood
equation (28.2.1) is often required in most literature of studying the large
sample properties of maximum likelihood estimator. The main reason
for requiring this condition, technically speaking, is that the uniqueness
will make all the proofs much simpler and mathematically tractable.
With some modiﬁcations of the above proofs, all the main results can
be proved under the much weaker condition that the expected score
function l(1)(x|θ) has a unique solution, i.e., mathematically it can be
stated that for every θ0 ∈Θ, the equation
∞

−∞
l(1)(x|θ)dFθ0
set
= 0,
(28.2.26)
has a unique solution in θ.
For example, in the case of Cauchy distribution with location param-
eter θ, the likelihood equation (2.1) has (2n−1) roots (real and complex
roots) but the equation
∞

−∞
l(1)(x|θ)dFθ0 = −
2(θ −θ0)
(θ −θ0)2 + 4
set
= 0
(28.2.27)
has unique root at θ = θ0. If the underlying distribution Q is a Cauchy
distribution, Perlman (1983) and Reeds (1985) proved that all the real
roots of the likelihood equation, except the mle (the global maximum),
tend to ±∞with probability one and the mle converges to θ0.
Bai
and Fu (1986) proved a stronger result that the mle, in fact, converges
to θ0 exponentially and is also locally eﬃcient in the Bahadur sense of
Eq. (28.1.3).
28.3
Some Applications
Motivated by robust estimation, this section mainly studies the exponen-
tial rate of the mle when the underlying distribution Q departs slightly
©2001 CRC Press LLC

from the speciﬁed model. The statistical models considered in the fol-
lowing examples are respectively exponential families and families of
t-distributions with location parameter θ.
28.3.1
Exponential Model
Let us consider the following exponential model
MO = {Fθ : dFθ = g(x) exp{xθ −C(θ)}dx, θ ∈R}.
(28.3.1)
The Kullback-Leibler information of the true underlying distribution Q
with respect to a distribution Fθ in the exponential model MO can be
written as
K(Q, Fθ) =

R
log dQ
dFθ
dQ = K(Q, F0) + C(θ) −C(0) −θEQX. (28.3.2)
The mle ˆθn(s) is a unique solution of the likelihood equation
¯xn −C(1)(θ) = 0,
(28.3.3)
and it can be written as
ˆθn(s) = h(¯xn),
(28.3.4)
where h(·) is the inverse function of C(1)(·). It is straightforward that
K(1)(Q, Fθ) has a unique maximum at θ⋆where θ⋆is a unique solution
of
EQX −C(1)(θ) = 0,
(28.3.5)
and it can be written as
θ⋆= h(EQX).
(28.3.6)
A straightforward application of our results, it yields that if
EQ exp{t(X −C(1)(θ))} < ∞
(28.3.7)
exists for t ∈(−δ, δ), then the mle ˆθn(s) converges to θ⋆exponentially.
This yields a stronger result than the result of McCulloch (1988).
In view of the mathematical form of the mle ˆθn(s) derived from (28.3.3),
it follows that
P(ˆθn(s) −θ⋆> ε|Q) = P( ¯Xn > C(1)(θ⋆+ ε)|Q),
(28.3.8)
hence, the mle ˆθn(s) converges to θ⋆exponentially if, and only if, the tail
probability of the underlying distribution Q tends to zero exponentially
©2001 CRC Press LLC

which is equivalent to that (28.3.7) exists in a neighborhood of t = 0. If
the tail probability of the underlying distribution Q tends to zero with
a rate of 1/xα (α > 3), then the generating function (28.3.7) does not
exist. If this is the case, then the mle ˆθn(s) remains convergence to θ⋆in
probability, and is asymptotically normally distributed. However, ˆθn(s)
does not converge to θ⋆exponentially. Summing up these evidences, we
could conclude that the mle ˆθn(s) derived from the exponential model
(28.3.1) is nonrobust in the sense of exponential rate of convergence with
respect to the usual metric topology. More importantly, it shows that
the criterion of selecting the optimal consistent estimator based on the
exponential rate is more reﬁned than the criterion based on the variance
of the asymptotic distribution.
28.3.2
Families of t-distributions with Location Parameter
For each d = 1, 2, · · · , ∞, deﬁne
Md = {dFθ,d(x) : all t-distributions with d degrees of freedom and
location parameter θ ∈R},
(28.3.9)
where dFθ,d has the form
dFθ,d(x) =
Γ( d+1
2 )
√
2πΓ( d
2)(1 + (x −θ)2
d
)−d+1
2 dx.
(28.3.10)
Further, let
M = {dFθ,d : all t-distributions with d = 1, 2, · · · , ∞, and θ ∈R}
(28.3.11)
be the family of all t-distributions with location parameter θ. The statis-
tical model M includes all the models of Md, d = 1, · · · , ∞. The model
M covers Cauchy (d = 1) and Normal (d = ∞) distributions, which are
of great interest both in theory and practice.
Let ˆθd(d = 1, · · · , ∞) be the mle derived from the model Md. Write
ˆθ∞= ˆθN and ˆθ1 = ˆθC as mles derived from Normal and Cauchy models,
respectively. In this example, our main interest is to study the expo-
nential rate of convergence of ˆθd when the true underlying distribution
Q does not belong to the model Md but it is a member of the larger
model M. This assumption is motivated by the robust estimation for
the location parameter.
The exponential rate of convergence of the mle ˆθd is deeply associated
with the expectation of the score function l(1)(θ|x, d) = (∂/∂θ) log dFθ,d
of speciﬁed model. Deﬁne
©2001 CRC Press LLC

S(θ|d)
= E(l(1)(θ|x, d)
F0,d)
=
∞

−∞
Γ( d+1
2 )
√
πdΓ( d
2)
(d + 1)(x −θ)
(d + (x −θ)2))(1 + x2/d)(d+1)/2 dx 
(28.3.12)
as the expected score function l(1)(θ|x, d) with respect to the underlying
distribution Q = F0,d. For the two extreme cases of Cauchy and Normal
distributions, we have, respectively,
S(θ|1) =
∞

−∞
2(x −θ)
π(1 + (x −θ)2)(1 + x2)dx = −
2θ
θ2 + 4, 
(28.3.13)
and
S(θ|∞) =
∞

−∞
(x −θ)
1
√
2π e−1/2x2dx = −θ. 
(28.3.14)
Note that for every d, S(θ|d) is a continuous function in θ and its graph
crosses the θ-axis only once at θ = 0. Further, for any degree of freedom
d (d < ∞), the function S(θ|d) is bounded and S(θ|d) →0, as |θ| →∞.
For the Normal distribution (d = ∞), the expected score function is
S(θ|∞) = −θ, which is continuous but unbounded.
By numerically
integrating (28.3.12), it yields the following Figure 28.1 which illustrates
the behaviors of the expected score functions S(θ|d), for d = 1, 6, 15, ∞.
In addition, it follows from the law of large numbers
1
nl(1)
n (θ) = 1
n
n

i=1
l(1)(θ|xi, d)
p
−→S(θ|d), 
(28.3.15)
as n →∞, under Q = F0,d.
For the Cauchy distribution, the empirical score function
1
nl(1)
n (θ) = 1
n
n

i=1
2(xi −θ)
1 + (xi −θ)2 
(28.3.16)
based on observed data (x1, · · · , xn) is also plotted against the expected
score function S(θ|1) in Figure 28.1. Perlman (1983) and Reeds (1985)
proved that, for Cauchy distribution, all the roots of likelihood equa-
tion except the mle (the global maximum) tend to ±∞with probability
one, and the mle converges to θ0 (θ0 = 0 in Figure 28.1). This conver-
gence behavior is also observed in Figure 28.1 for all t-distributions with
degrees of freedom d = 1, 2, · · ·.
©2001 CRC Press LLC

FIGURE 28.1
The expected scores S(θ|d) for d = 1, 6, 15, ∞, and the empirical
scores
1
nl(1)
n (θ) based on Eq. (28.3.16) for n = 5 under model M1
©2001 CRC Press LLC

THEOREM 28.3.1
Let M be the family of t-distributions deﬁned by equations (28.3.9) and
(28.3.10). Then the following results hold:
(i)
For any Q = Fθ,d ∈M, and d ≥3, the mle ˆθN = ¯xn derived from
the normal model M∞is consistent and asymptotically normally
distributed,
(ii)
the estimator ˆθN does not converge to θ(θ = θ⋆) exponentially for
any d = 1, 2, · · ·, except for d = ∞(nonrobust estimator),
(iii)
the mle ˆθC derived from the Cauchy model converges to θ exponen-
tially for every d including d = ∞(robust estimator).
PROOF For the maximum likelihood estimator ˆθN = ¯xn derived from
the Normal model, it follows from Theorem 28.2.2 that θ⋆= θ and ˆθN
converges to θ as n →∞for any d ≥3. The asymptotic normality of
ˆθN follows directly from the central limit theorem. This completes (i).
Note that for any d = 1, 2, · · · , (except d = ∞)
∞

−∞
e−(x−θ)t[1 + (x −θ
′)2
d
]−d+1
2 dx = ∞
(28.3.17)
does not exist for t in any neighborhood of zero. Hence ˆθN does not
converge to θ exponentially. This completes the proof of (ii). Contrary
to the notion of (28.3.17), for any θ and θ
′, and d = 1, 2, · · · , ∞, there
exists δ > 0, such that
∞

−∞
exp{−
t(x −θ)
1 + (x −θ)2 }dFθ′,d < ∞
(28.3.18)
for all t ∈(−δ, δ). It follows from Theorem 28.2.2 that the mle derived
from the Cauchy model M1 converges to θ exponentially for any under-
lying distribution Q = Fθ,d inside the model M. This completes the
proof.
REMARK In view of this theorem, the mle ˆθN has asymptotic variance
σ2
d/n (d ≥3) under the t-distribution, where σ2
d is the variance of the
t-distribution with d degrees of freedom but the mle ˆθn does not con-
verge to θ exponentially. This substantiates the criticisms of asymptotic
normal theory applying to large sample estimation problems by many
statisticians, for example Weiss and Wolfowitz (1966, 1967).
It also
shows that the asymptotic normal theory could not distinguish the rates
of convergence among the consistent estimators.
In general, roughly
speaking, ˆθN is a slow consistent estimator wrt heavy tail underlying
distribution.
©2001 CRC Press LLC

For any d, the mle ˆθd derived from the model Md (d = 1, 2, · · · , and
d ̸= ∞), the following general results hold.
THEOREM 28.3.2
For any d = 1, 2, · · · , and d ̸= ∞, the mle ˆθd converges to θ under
Q = Fθ,d′, d
′ = 1, · · · , ∞as n →∞, having an exponential rate given by
βd,d′(ε) = −log ρd,d′ ,
(28.3.19)
where
ρd,d′ = max(ρd,d′(ε), ρd,d′(−ε)),
(28.3.20)
ρd,d′(±ε) = inf
t
>
<0
ηd(t|θ ± ε, Fθ,d′),
(28.3.21)
and
ηd(t|θ ± ε, Fθ,d′) =
∞

−∞
(exp{−t(d + 1)(x −θ ± ε)
d + (x −θ ± ε)2 })dFθ,d′(x). (28.3.22)
PROOF For every d = 1, 2, · · · , the moment generating function ηd(t|θ±
ε, Fθ,d′) exists in a neighborhood of t = 0 for every d
′ = 1, 2, · · · , ∞,
hence the result follows immediately from Theorem 28.2.2.
28.4
Discussion
For ε small, the exponential rate βd,d′(ε) of the mle ˆθd(s), with respect
to the underlying distribution Q = Fθ,d′, admits the expansion [see Fu
(1975, 1982)]
βd,d′(ε) = αd,d′ ε2
2 + o(ε2),
(28.4.1)
where αd,d′ is called the local exponential rate. The local exponential
rate αd,1 of ˆθd with respect to the Cauchy underlying distribution is
a decreasing function of degree of freedom d having α1,1 = IC (Fisher
information of Cauchy distribution) and α∞,1 = 0. The local exponential
rate αd,∞of ˆθd with respect to the Normal underlying distribution is
an increasing function of d having α1,∞> 0, and α∞,∞= IN (Fisher
information of Normal). Hence the following inequalities
max(α1,1, α∞,∞) ≥max(αd,1, αd,∞)
≥min(αd,1, αd,∞) ≥min(α1,1, α∞,∞) (28.4.2)
©2001 CRC Press LLC

hold for every d = 1, 2, · · · , ∞.
Further, for every ﬁxed d, αd,d′ is a
convex function of d
′ and has a maximum at d = d
′, and
min
d′ αd,d′ = min(αd,1, αd,∞).
(28.4.3)
The local exponential rate αd,d′ can be used as a criterion for robust
estimation.
A consistent estimator is called a robust estimator with
respect to the model M if its local exponential rate is greater than zero
for every distribution in M.
Note that Theorem 28.3.1 yields
α∞,d′ ≡0,
for every d
′ = 1, 2, · · · except for d
′ = ∞
(28.4.4)
and
αd,d′ > 0,
for every d = 1, 2, · · · (d ̸= ∞), and every d
′ = 1, 2, · · · , ∞
(28.4.5)
Hence the mle ˆθN derived from the normal model is a non-robust esti-
mator, and on the contrary, the mle’s ˆθd derived from Md, d < ∞are
robust estimators with respect to M in the sense of exponential rate of
convergence.
The mle ˆθd⋆derived from the model Md⋆is called the best robust
estimator among all the mle’s ˆθd, d = 1, · · · , ∞, with respect to the
model M (maximum solution), if
αd⋆,1 = αd⋆,∞= max
d
min
d′ αd,d′ = max
d
min(αd,1, αd,∞).
(28.4.6)
Unfortunately, for given d and d
′, the exact analytical expression of
the local exponential rate αd,d′ could not be obtained. Hence d⋆could
not be obtained analytically. By using Theorem 28.3.2 and numerical
integration of evaluating the local exponential rate αd,d′ ∼βd,d′(ε)/ε2
at very small ε(ε = 0.0001), it yields that d⋆is approximately “6”. In
words, among all the maximum likelihood estimators ˆθd, d = 1, 2, · · · , ∞,
the mle ˆθ6 is the best robust estimator with respect to the family of all
t-distributions in the sense of local exponential rate.
Acknowledgments. This work was supported in part by the National
Sciences and Engineering Research Council of Canada under the grant
A-9216.
References
1. Bahadur, R. R. (1967). Rates of convergence of estimates and test
statistics. Annals of Mathematical Statistics 38, 303–324.
©2001 CRC Press LLC

2. Bahadur, R. R. (1971). Some limit theorems in statistics. Regional
Conference Series in Applied Mathematics, Society for Industry
and Applied Mathematics, Philadelphia.
3. Bahadur, R. R. (1980). On large deviations of maximum likelihood
and related estimates. Technical Report. Department of Statistics,
University of Chicago.
4. Bai, Z. D. and Fu, J. C. (1986). Likelihood principle and maximum
likelihood estimator of location parameter for Cauchy distribution.
Canadian Journal of Statistics 15, 137–146.
5. Cheng, P. and Fu, J. C. (1986). On a fundamental optimality of
the maximum likelihood estimator. Statistics & Probability Letters
4, 173–178.
6. Chernoﬀ, H. (1952). A measure of asymptotic eﬃciency for tests of
a hypothesis based on sum of observations. Annals of Mathematical
Statistics 23, 493–502.
7. Cram´er, H. (1946). Mathematical Methods of Statistics, Princeton
University Press, Princeton, New Jersey.
8. Fu, J. C. (1973). On a theorem of Bahadur on the rate of conver-
gence of point estimators. Annals of Statistics 1, 745–749.
9. Fu, J. C. (1975). The rate of convergence of consistent point esti-
mators. Annals of Statistics 3, 234–240.
10. Fu, J. C. (1982). Large sample point estimation: A large deviation
approach. Annals of Statistics 10, 762–771.
11. Jureckova, J. (1981). Tail-behavior of location estimators. Annals
of Statistics 9, 578–585.
12. Kester, A. (1981).
Large deviation optimality of MLE’s in ex-
ponential families. Technical Report No. 160, Free University of
Amsterdam.
13. Kester, A. and Kallenberg, W. C. M. (1986). Large deviation of
estimators. Annals of Statistics 14, 648–664.
14. LeCam, L. M. (1953). On some asymptotic properties of maximum
likelihood estimates and related Bayes estimates. Proceedings of
the Berkeley Symposium 1, 277–328.
15. LeCam, L. M. (1970). On the assumptions used to prove asymp-
totic normality of maximum likelihood estimates. Annals of Math-
ematical Statistics 41, 802-828.
16. McCulloch, R. E. (1988). Information function in exponential fam-
ilies. The American Statistician 42, 73–75.
17. Perlman, M. D. (1983). The limiting behavior of multiple roots
©2001 CRC Press LLC

of the likelihood equation. Recent Advances in Statistics, pp. 339–
370, Academic Press, New York.
18. Pfanzagl, J. (1973). Asymptotic optimum estimation and test pro-
cedures. Proceedings of Prague Symposium on Asymptotic Statis-
tics, 201–272, Charles University, Prague.
19. Rao, C. R. (1963). Criteria of estimation in large samples. Sankhy¯a
25, 189–206.
20. Reeds, J. A. (1985). Asymptotic number of roots of Cauchy loca-
tion likelihood equations. Annals of Statistics 13, 775–784.
21. Rubin, H. and Rukhin, A. L. (1983). Convergence rates of large
deviation probabilities for point estimators. Statistics & Probability
Letters 1, 197–202.
22. Rukhin, A. L. (1983). Convergence rates of estimators of ﬁnite pa-
rameter: how small can error probabilities be. Annals of Statistics
11, 202-206.
23. Steinebach, J. (1978). Convergence rates of large deviation prob-
abilities in the multidimensional case.
Annals of Probability 2,
751–759.
24. Wald, A. (1949). A note on the consistency of maximum likelihood
estimates. Annals of Mathematical Statistics 20, 595–601.
25. Weiss, L. and Wolfowitz, J. (1966). Generalized maximum likeli-
hood estimators. Teoriya Vyeroyatnostey 11, 68–93.
26. Weiss, L. and Wolfowitz, J. (1967). Maximum probability estima-
tors. Annals of the Institute of Statistical Mathematics 19, 193–
206.
©2001 CRC Press LLC

29
A Closer Look at Weighted Likelihood in
the Context of Mixtures
Marianthi Markatou
Columbia University, New York, NY
ABSTRACT The performance of the weighted likelihood methodol-
ogy in the context of mixtures is studied in detail. Speciﬁcally, we study
the behavior of the method under two types of misspeciﬁcation. Those
are (1) probability model misspeciﬁcation, which refers to both, compo-
nent misspeciﬁcation and probability distribution misspeciﬁcation, and
(2) variance structure misspeciﬁcation. We contrast the behavior of the
weighted likelihood estimates with that of Huber-type M-estimates and
maximum likelihood estimates. We present simulation results which ex-
emplify the role of the starting values in the convergence of the weighted
likelihood algorithm. We then discuss the relationship of these results
with the problem of model selection.
Keywords and phrases Estimating equations, model selection, mix-
tures, robustness, weighted likelihood
29.1
Introduction
The problem of parametric statistical inference in the presence of a ﬁnite
mixture of distributions has been studied extensively. An excellent sur-
vey of it was given by Redner and Walker (1984). Moreover, the recent
book by Lindsay (1995) gives a detailed account of problems that can
be formulated as mixture model problems.
It is also a common statistical practice to study the robustness of a
statistical procedure by constructing a class of alternative mixture mod-
els. For example, if the central model is N(µ, σ2) and we are interested
in estimating the parameter µ, we might assess the robustness of the es-
timator by measuring its performance, that is its bias and mean squared
error, on data that arise from the model (1 −ϵ)N(µ, σ2) + ϵN(µ, τ 2)
©2001 CRC Press LLC

where τ 2 > σ2, 0 < ϵ < 1.
One of the limitations of the mixture model theory is that it seems that
very little is known about the robustness aspect of this methodology. To
quote Lindsay (1995) “It is natural and desirable to ask the question:
what are the consequences of slight errors in the misspeciﬁcation of the
model? How stable are my parameters under contamination? When a
mixture model is being speciﬁed, and I specify a mixture of two normals
with diﬀerent means, what are the consequences if the mixture is actually
of 3 normals? Or of two t-distributions?”
A number of authors have tried to answer, at least a few of the above
questions, by trying to adopt the usual M-estimation methods in the
context of mixtures. Some of these attempts include a robustiﬁcation
of the EM algorithm [De Veaux and Krieger (1990)], the introduction
of a measure of typicality [Campbell (1984)] which reﬂects the contri-
bution of the mth observation to the mean of the kth component, and
the adoption of multivariate M-estimation techniques for multivariate
normal mixtures [McLachlan and Basford (1988)]. Additional work in-
cludes Aitkin and Tunnicliﬀe (1980), Clarke and Heathcote (1994), Gray
(1994), and Windham (1996).
A direction that has seen some development when the problem of
robustness is of interest, is to employ minimum distance ideas for pa-
rameter estimation.
This approach has been taken by Woodward et
al. (1984), Woodward, Whitney, and Eslinger (1995), and Cutler and
Cordero-Brana (1996). The problem with these procedures is that they
are computationally complex. Moreover, there is no natural deﬁnition
of the concept of outliers.
In this paper we take a closer look at the issue of robustness as it
relates to the mixture model. In particular, we address the eﬀects of
misspeciﬁcation of the variance structure and mixture model misspeciﬁ-
cation on the parameter estimates of the mixture model. We study the
performance of the weighted likelihood estimating equations (WLEE)
methodology in this setting and compare it with that of the usual like-
lihood methodology and with the performance of classical M-estimators
as they are adapted in the mixture model setting by McLachlan and
Basford (1988).
The paper is organized as follows. Section 29.2 brieﬂy discusses the
WLEE methodology in the context of mixtures. Section 29.3 presents
simulation results. Section 29.4 discusses the implications of the WLEE
as they relate to the model selection topic. Finally, Section 29.5 oﬀers
conclusions.
©2001 CRC Press LLC

29.2
Background
Let X1, X2, . . . , Xn be a random sample of completely unclassiﬁed ob-
servations from a ﬁnite mixture distribution with g components. The
probability density function of an observation X is of the form m(x; φ) =
g
i=1 pif(x; θi), where f(x; θi) is the probability density or mass func-
tion of the ith subpopulation and θi is the component parameter that
describes the speciﬁc attributes of the ith component population. Any
parameters that describe unknown characteristics common to the entire
population are assumed to be absent for the time being. However, if
those parameters are of interest their corresponding estimating equa-
tions can be easily incorporated into the estimating scheme. The vector
φ = (p1, p2, . . . , pg, θ1, . . . , θg)T of all unknown parameters belongs to a
parameter space Ωsubject to g
i=1 pi = 1, pi ≥0, i = 1, 2, . . . , g, and θi
belongs to Θ.
In this context, the weighted likelihood estimating equations are
n

j=1
w(δ(xj))∇φ[lnm(xj; φ)] = 0
(29.2.1)
where δ(xj) is the Pearson residual evaluated at xj and w(.) is a weight
function that downweights observations which have large residuals. If
m(x; φ) is a probability mass function the Pearson residual is deﬁned
as δ(t) = (d(t) −m(t; φ))/m(t; φ), [Lindsay (1994)], where d(t) is the
proportion of observations in the sample with value equal to t. If the
components are continuous, the Pearson residual is
δ(xj) = {f ∗(xj)/m∗(xj; φ)} −1,
where f ∗(.) is a kernel density estimator and m∗(.; φ) is the mixture
model smoothed with the same kernel used to obtain the density estima-
tor. Note that the range of the Pearson residuals is the interval [−1, ∞).
A value of δ(xi) close to zero indicates agreement between the data and
the hypothesized model in the neighborhood of the observation xi. On
the other hand, a large value of the residual indicates a discrepancy be-
tween the data and the model in the form of an excess of observations
relative to the model prediction. Thus, we call an observation an outlier
if it has a large Pearson residual.
The weight functions w(.) are unimodal in that they decline smoothly
as the residual δ departs from 0 towards -1 or +∞and take the maximal
value of 1 when the residual δ is 0. An observation that is consistent
with the assumed model receives a weight of approximately 1.
If it
is inconsistent with the model it receives a small weight; a weight of
©2001 CRC Press LLC

approximately 0 indicates that the observation is highly inconsistent
with the model.
The ﬁnal ﬁtted weights indicate which of the data
points were downweighted in the ﬁnal solution relative to the maximum
likelihood estimator.
The motivation for this weight construction comes from minimum dis-
parity estimation. In the discrete case, given any strictly convex function
G(.), a nonnegative disparity measure between the model mφ and the
data d is given by ρG(d, mφ) = 
t mφ(t)G(δ(t)). The value of φ that
minimizes the disparity is the minimum disparity estimator correspond-
ing to G [Lindsay (1994)]. In the continuous case , the disparity between
the data and the model is deﬁned as ρG(f ∗, m∗
φ) =

G(δ(x))m∗
φ(x)dx,
[Basu and Lindsay (1994)]. When G is selected appropriately a large
class of important distances can be developed. For example, if G(δ) =
(δ + 1)ln(δ + 1) a version of the Kullback-Leibler divergence is gener-
ated. We will focus attention on the robust disparity measure generated
by G(δ) = 2δ 2/(δ + 2) which corresponds to the symmetric chisquared
distance. The symmetric chisquared distance is deﬁned as

t
[d(t) −m(t; φ)]2
(1/2)d(t) + (1/2)m(t; φ)
in the discrete case, and

[f ∗(x) −m∗(x; φ)]2
(1/2)f ∗(x) + (1/2)m∗(x; φ)dx
in the continous case. The corresponding weight function is unimodal,
twice diﬀerentiable at δ = 0, with w
′(0) = 0 and w
′′(0) < 0, where prime
denotes diﬀerentiation with respect to δ. The weight function relates to
the function G via the equation (0.5[(δ + 1)G
′(δ) −G(δ)] + 1)/(δ + 1),
and G
′ is the derivative of G with respect to δ. An additional advantage
of the chisquared weight function is that in the continuous case we can
construct a pa ra l l e l disparity with G speciﬁed as above. This disparity
could be useful as it can form the basis for the construction of a goodness
of ﬁt test for model. How this can be done will be discussed in Section
29.4. Finally, we would like to note that Markatou, Basu, and Lindsay
(1997) suggested a general scheme for weight construction which can also
be adopted here.
From equation (29.2.1) the estimating equations for θi are
n

j=1
g

i=1
w(δ(xj))
m(xj; φ) pif(xj; θi)u(xj; θi) = 0
(29.2.2)
where u(xj; θi) = ∇θilnf(xj; θi).
©2001 CRC Press LLC

For the vector p = (p1, p2, . . . , pg)T the equations are
pi = pi
n

j=1
w(δ(xj))
n
j=1 w(δ(xj))
f(xj; θi)
m(xj; φ).
(29.2.3)
The solutions of the system (29.2.2), (29.2.3) are the weighted likelihood
estimating equations (WLEE) estimators of the parameter vector φ =
(p1, p2, . . . , pg, θ1, θ2, . . . , θg)T .
A key element to the success of this methodology is the appropri-
ate construction of the Pearson residuals in the continuous case, which
requires selection of a kernel k(x; t, h) and a degree of smoothing h in
order to create f ∗and m∗. The choice of kernel does not seem critical
and hence we recommend choosing it based on convenience [Markatou
(2000)]. The problem that arises next is that one would like to do the
smoothing so that the robustness properties are homogeneous through-
out the space of model parameters. This requirement suggests that the
smoothing parameter should be proportional to the variance of X un-
der the true model. When the model is a ﬁnite mixture of N(µi, σ2) a
natural kernel is the normal density with variance h2. The variance h2
serves as the bandwidth parameter and determines the robustness prop-
erties of the WLEE estimator. Markatou (2000), proposes to select the
smoothing parameter h2 at the (i+1)st step of the iteration as a multiple
of the estimate of model variance at the ith step of the iteration, that
is, h2
(i+1) = cˆσ2
(i), where c > 0. To complete the bandwidth selection we
need to have a rule to determine c. Markatou (2000) suggests obtaining
c by solving the equation
−A2
2 { (1 + c)3/2
c(3 + c)1/2 −1} = γ0,
where γ0 > 0 is the number of observations to be downweighted and it
is selected by the user, A2 = w
′′(0).
In the multivariate case the smoothing matrix is selected as H = cˆΣ,
where ˆΣ is the estimated covariance and the constant c is obtained by
solving the equation
−A2
2 {[ (1 + c)3/2
c(3 + c)1/2 ]m −1} = γ0
where m = dim(φ), [Markatou (2000)].
This equation assumes that
Σ = diag(σ2
i ). Note that, by selecting the smoothing parameter as a
function of the scale has the additional advantage of obtaining invariant
estimators under scale transformations.
When the model is a ﬁnite mixture of normal components with pa-
rameters (µi, σ2
i ), and the kernel is normal with variance h2, then select
©2001 CRC Press LLC

h2 = c·(g
i=1 ˆpiˆσ2
i ), ˆpi, ˆσ2
i are estimates of the proportion and variance of
the ith subpopulation and c is determined as before. In the multivariate
case we select H = c g
i=1 ˆpi ˆΣi [Markatou (2000)].
29.3
Simulation Experiments and Results
We now describe in detail a simulation study that was designed to oﬀer
insight in a number of issues associated with the performance of the
weighted likelihood methodology in the context of mixtures.
First, we oﬀer some general comments about the performance of the
WLEE.
The weighted likelihood estimating equations need not have a unique
solution. Veriﬁcation of the conditions of Markatou, Basu, and Lindsay
(1998) for the component densities of a mixture model together with
the additional assumption of parameter identiﬁability guarantees the
existence of a root in the neighborhood of the true parameter value.
Under these conditions, the consistency and asymptotic normality of
the estimates is ensured. The asymptotic distribution of the weighted
likelihood estimators is normal with mean 0 and covariance matrix the
inverse of the Fisher information. This result states that the estimates
are ﬁrst order eﬃcient. As such, their inﬂuence function is the same with
the inﬂuence function of the corresponding MLE. Therefore, in order to
study the robustness properties of these estimates we need to use tools
other than the inﬂuence function.
When one has an equation with multiple roots one might wish to know
if the presence of a root represents some underlying structure. This is
particularly relevant in the context of mixtures since the existence of dif-
ferent roots could indicate multiple potential mixture model ﬁts due to
the presence of more components than originally speciﬁed. The weighted
likelihood equations tend to have multiple roots when one attempts to ﬁt
a two component model to data from a three component density. When
the mixing proportion of the third component is suﬃciently small the
weighted likelihood equations mostly exhibit one root that corresponds
to the model comprised from the well represented components. As the
mixing proportion of the third component increases the weighted likeli-
hood equations have multiple roots that correspond to two component
mixture models that ﬁt diﬀerent portions of the data. In this case, the
robustness of the procedure is deﬁned by its ability to detect possible
data substructures in the form of multiple roots.
When an additional component that is not included in the model spec-
iﬁcation is close to a component of the speciﬁed mixture model, robust-
©2001 CRC Press LLC

ness, deﬁned as resistance of the statistical procedure to deviations from
the stipulated model, may be more relevant. The weighted likelihood
methodology downweights the data from the additional component, and
the power of discrimination goes down as the method tries to distinguish
between ﬁtting a model with a larger variance that encompasses all data
points or a more robust ﬁt.
This empirical property corresponds to a stability of the estimating
equation and its roots under contamination that can be investigated us-
ing an approach similar to that of Lindsay (1994). Let {ξj; j = 1, 2, . . .}
be a sequence of elements in the sample space. Let ˆFj(x) = (1−ϵ) ˆF(x)+
ϵ∆ξj(x) be the contaminated distribution and f ∗
j (x) =

k(x, t, h)d ˆFj(x)
the corresponding kernel smoothed data.
Then {ξj} is an outlier se-
quence for mφ(x) and data ˆF if δ(ξj) =
f ∗
j (ξj)
m∗
φ(ξj) −1 converge to inﬁnity
and at the same time m∗
φ(ξj) converges to zero.
The weighted likelihood score along this sequence is

w(x; ˆFj, Mφ)
u(x; φ)d ˆFj and the estimating equations will ignore the outliers if the
limit of the weighted likelihood sequence does not depend on the out-
liers. Veriﬁcation of the conditions given in Markatou, Basu, and Lindsay
(1998) shows that the weighted likelihood equations satisfy the relation-
ship

w(x; ˆFj, Mφ)u(x; φ)d ˆFj →

w(x; ˆFϵ, Mφ)u(x; φ)d ˆFϵ
where ˆFj(x) = (1 −ϵ) ˆF(x) + ϵ△ξj(x) is a contaminated distribution,
{ξj; j = 1, 2, . . .} is a sequence of elements of the sample space and
ˆFϵ(x) = (1 −ϵ) ˆF(x) is a subdistribution, 0 < ϵ < 1.
The stability of the set of roots in identifying important structures in
the data is of paramount importance as it could indicate the presence of
additional components than those originally speciﬁed in the model.
Because the estimating equations do not necessarily have a unique
solution, we need to use multiple random starts to ﬁnd all reasonable
solutions with ‘high probability’. Markatou (2000), suggests to use as
starting values the method of moment estimates (MME) of the param-
eters of the mixture [Lindsay (1989), Furman and Lindsay (1994), and
Lindsay and Basak (1993)]. These MME are calculated on subsamples
drawn randomly from the data.
There were several reasons that led
us to select the method of moment estimates as starting values. The
estimates are unique solutions of the moment equations, they can be
computed quickly using a simple bisection algorithm, and they are con-
sistent estimates of the true parameter values.
We used B = 50 bootstrap subsamples for our univariate case stud-
ies and B = 100 bootstrap subsamples for the bivariate case. However,
B = 50 is suﬃcient for the bivariate case as well, in that the important
©2001 CRC Press LLC

data substructures can be identiﬁed. The subsample size was equal to
the number of parameters to be estimated, consistent with the recom-
mendation of Markatou (2000). To terminate the algorithm we use the
stopping rule recommended in Markatou (2000) with α = 0.001.
To calculate the maximum likelihood estimates we used the EM algo-
rithm. It is often suggested to stop the iteration when |ℓi+1 −ℓi| ≤tol,
where tol is a small speciﬁed constant and ℓi+1, ℓi are the values of the
log-likelihood at steps (i + 1)st and ith. We used the implementation of
the EM algorithm given in McLachlan and Basford (1988, pages 213–
216). The stopping rule bounds the diﬀerence between the log-likelihood
at the ith step and the log-likelihood at the (i-10)th step by the number
10−4 multiplied by the log-likelihood at the (i-10)th step of the iteration.
If this bound is unreachable, the algorithm stops at the 150th iteration.
Bohning et al. (1994) discuss the use of the EM-algorithm with a diﬀer-
ent stopping rule. Their rule is based on the use of Aitken’s acceleration,
which is a device to exploit the regularity of the convergence process. In
complex practical problems this may be a better stopping rule to use.
For additional discussion about algorithms in the context of mixtures
see also Bohning (1999).
In what follows we describe the simulation experiments and discuss
our results.
29.3.1
Normal Mixtures with Equal Component Variance
In this section we describe the simulation experiments performed. We
ﬁrst concentrate in the case of normal mixtures with equal component
variance.
The nominal model was p1N(µ1, σ2) + p2N(µ2, σ2), p1 + p2 = 1. The
data were generated from the model p1N(0, 1) + p2N(8, 1) + p3N(12, 1),
where p1 = p2 and p3 = 0, 0.04, 0.10, 0.16, 0.20, 0.26.
In our calculations we used the chisquared weight function. It is de-
ﬁned as w(δ) = 1−δ2/(δ+2)2 [see Markatou, Basu, and Lindsay (1998)].
To compute the Pearson residuals we used normal kernel with h2 = cσ2
with two diﬀerent levels of c, either 0.050 or 0.010, which correspond
to downweighting on average 5 and 14 observations respectively.
We
considered samples of size 100.
At each sample model, we simulated
100 data sets. All samples were generated in S-plus and all programs
were written in FORTRAN77. All calculations were carried out on a
DEC5000/50 station.
For each data set we used the method of moment estimates calculated
on the entire sample to produce starting values; we also carried out
a bootstrap root search. Given a sample we took B = 50 bootstrap
subsamples of size 4 and use them to construct our starting values.
©2001 CRC Press LLC

TABLE 29.1
WLEE estimators with starting values MMEs computed using the
entire sample. The sample mo del is p1 N(0, 1) + p2 N(8, 1) + p3 N(12, 1),
with p1 = p2 . The sample size is 100 and the numb er of Monte
Carlo replications is 100
% of
Parameter Estimates
mixing
p3 
Estimates
p1 
µ1 
µ2 
σ
MLE
0.4998
8.0096
0.0281
0.9916
0%
WLEE (c = 0.050)
0.4998
8.0145
0.0267
0.9582
WLEE (c = 0.010)
0.4994
8.0124
0.0236
0.9121
MLE
0.5200
8.5102
0.2898
1.3834
4%
WLEE (c = 0.050)
0.5009
8.0258
0.0068
0.9685
WLEE (c = 0.010)
0.4997
8.0253
0.0070
0.9129
MLE
0.5511
8.7098
-0.0265
1.6377
10%
WLEE (c = 0.050)
0.5027
8.0156
-0.0238
0.9699
WLEE (c = 0.010)
0.5024
7.9969
-0.0246
0.9143
MLE
0.5813
9.0233
0.0028
1.7025
16%
WLEE (c = 0.050)
0.5105
8.0705
0.0137
1.0123
WLEE (c = 0.010)
0.5061
8.0165
0.0203
0.9255
MLE
0.6010
9.3029
-0.0256
1.7220
20%
WLEE (c = 0.050)
0.5206
8.2703
-0.0175
1.1383
WLEE (c = 0.010)
0.5028
8.0355
-0.0195
0.9561
MLE
0.6307
9.6287
-0.0243
1.7072
26%
WLEE (c = 0.050)
0.6021
9.3780
-0.0101
1.7127
WLEE (c = 0.010)
0.5603
8.8517
-0.0158
1.4084
Table 29.1 presents the WLEE estimators under the various three
component sampling models when the MME calculated on the entire
sample are used as starting values.
For p3 up to 0.16 and for c =
0.050, 0.010 the WLEE algorithm converges to a root similar to the model
p1N(0, 1) + p2N(8, 1) ignoring the third component. When p3 > 0.16
and depending on the value of c we observe an increase in the bias of
the WLEE estimators with a ﬁnal convergence to a root similar to that
suggested by the maximum likelihood. This root estimates fairly accu-
rately the location of the N(0, 1) component while the second location
is shifted to the right.
When bootstrap search is used several roots which correspond to dif-
ferent possibilities for ﬁtting diﬀerent two component mixture models
emerge. Depending on the mixing proportion of the third normal com-
©2001 CRC Press LLC

TABLE 29.2
Frequency of identiﬁed ro ots. The sample mo del is
p1 N(0, 1) + p2 N(8, 1) + p3 N(12, 1), p1 = p2 . Model I is
p1 N(0, 1) + p2 N(8, 1), II is p1 N(8, 1) + p2 N(12, 1), III is
p1 N(0, 1) + p2 N(12, 1) and mo del IV stands for an MLE-like ﬁt. The
+ sign indicates that the mo del was suggested, the - sign indicates
absence of it. The frequency is over 100 samples if size 100 with 50
b o otstrap searches in each sample
Suggested Models
% of Mixing p3
I
II
III
IV
4%
10%
16%
20%
26%
+
+
+
+
4
54
+
-
-
-
49
17
3
1
-
-
-
+
1
+
+
-
-
49
67
70
29
+
-
+
-
2
7
2
-
+
+
-
2
-
+
-
+
2
6
13
+
+
+
-
9
18
34
3
+
+
-
+
2
26
28
+
-
+
+
1
-
+
+
+
1
ponent some root patterns are favored over others.
Table 29.2 presents the diﬀerent root patterns found in our simulation
for c = 0.050.
When the mixing proportion of the third component
is 0.04 in approximately half of the Monte Carlo samples a mixture of
N(0, 1) and N(8, 1) was suggested. The remaining samples suggested a
mixture of N(0,1) and N(8, 1) as well as a mixture between N(8, 1) and
N(12, 1). In only two samples a mixture between N(0, 1) and N(12, 1)
was observed. For up to p3 = 0.10 the above described types of roots
are present in a large number of samples. Most of the samples exhibit
roots that stay close to ﬁtting a mixture between N(0, 1) and N(8, 1),
a mixture between N(8, 1) and N(12, 1) and a mixture between N(0, 1)
and N(12, 1), over the 50 bootstrap starting values. In all samples and
for all values of c the mixture model p1N(0, 1)+p2N(8, 1) was suggested.
©2001 CRC Press LLC

TABLE 29.3
Bootstrap estimates of the parameters of the mixture
p1N(0, 1) + p2N(8, 1), p1 + p2 = 1. The sampling model is
p1N(0, 1) + p2N(8, 1) + p3N(12, 1), p1 = p2. The number of Monte Carlo
replications is 100, the sample size is 100 and the number of
bootstrap samples is 50
% of mixing
p3
Estimates
ˆp1
ˆµ1
ˆµ2
ˆσ
WLEE (0.050)
0.4998
8.0145
0.0267
0.9582
0%
WLEE (0.010)
0.4994
8.0124
0.0236
0.9121
WLEE (0.050)
0.5009
8.0258
0.0068
0.9685
4%
WLEE (0.010)
0.4997
8.0253
0.0070
0.9129
WLEE (0.050)
0.5027
8.0156
-0.0238
0.9699
10%
WLEE (0.010)
0.5024
7.9969
-0.0246
0.9143
WLEE (0.050)
0.5105
8.0705
0.0137
1.0123
16%
WLEE (0.010)
0.5061
8.0165
0.0203
0.9255
WLEE (0.050)
0.5041
8.0045
-0.0122
0.9902
20%
WLEE (0.010)
0.5028
8.0355
-0.0195
0.9561
WLEE (0.050)
0.5024
7.9919
-0.0283
0.9688
26%
WLEE (0.010)
0.5019
8.0048
-0.0283
0.9367
©2001 CRC Press LLC

Table 29.3 shows the bootstrap estimates for proportion, location,
and scale of the mixture model comprised from the N(0, 1) and N(8,
1) components for various values of p3.
The estimates presented are
averages over the 100 simulated samples and the 50 bootstrap random
starts. A comparison between tables (1.1), (1.2), and (1.3) illustrates
the behaviour, in terms of convergence, of the random starts. When the
algorithm is started at the MMEs computed using the entire sample it
converges, for low values of p3, to a mixture model wich includes only
the well represented components in the sample. For larger values of p3 it
converges to an MLE-like root. On the other hand, when random starts
are used diﬀerent root conﬁgurations, and thus multiple model ﬁts, are
present.
The algorithm converged very fast. Our ﬁnding was that the mean,
median and the standard deviation of the number of iterations depend on
the mixing proportion of the third component and they increase with it.
For example, when the MMEs calculated on the entire sample are used as
starting values in the weighted likelihood algorithm with p3 = 0.10 and
smoothing constant c = 0.050 the average number of iterations is 8.78,
the median is 9 and the standard deviation is 1.508 over 100 samples.
When the smoothing constant is 0.010 the mean number of iterations
is 8.26, the median 8 and the standard deviation 1.260. However, when
the mixing proportion p3 = 0.20 and for c = 0.050 the average number
of iterations is 14.54, the median is 14 and the standard deviation is
6.090. When c = 0.010 the corresponding numbers are 15.2, 14, 4.575.
We also found that in our large simulation runs, each individual B=50
search averaged about 8 minutes in real time. Thus in no case were the
calculations excessive.
When the components are not well separated the number of iterations
needed to converge to a root increases. As an example, when the sam-
pling model is a mixture of equal proportions of N(0,1) and N(2,1) the
mean number of iterations for the weighted likelihood algorithm with
c = 0.050 was 14.52, the median was 10 and the standard deviation
16.455. The EM algorithm had a mean of 18.24, median of 16 and stan-
dard deviation 7.425 iterations.
29.3.2
Normal Mixtures with Unequal Component Variance
In this section we describe the simulation experiments and results ob-
tained for the case of normal mixtures with unequal component vari-
ances.
Moreover, we study the case of multivariate normal mixtures
with equal covariance matrices and compare our results with those ob-
tained from applying a Huber-type M-estimation procedure.
The sampling model is p1N(0, 1) + p2N(8, 4) + p3N(16, 1) where p1 =
©2001 CRC Press LLC

p2 and p3 is 0.04, 0.10, 0.14, 0.16, 0.20, 0.26. The nominal model is
assumed to be p1N(µ1, σ2
1) + p2N(µ2, σ2
2), p1 + p2 = 1. The starting
values are the MME calculated on the entire sample as well as bootstrap
sample based estimates. Notice that the starting values have been com-
puted under the assumption that the normal components have the same
variance. However, this variance misspeciﬁcation in the starting values
does not aﬀect the ﬁnal estimates. An interesting question that remains
yet unanswered is how can one construct method of moments estimates
when the component submodels have diﬀerent variances. The values of
the smoothing constant in the normal kernel were again 0.050, 0.010.
When the normal kernel is used for up to p3 = 0.14 the weighted like-
lihood algorithm started at the MMEs calculated on the entire sample,
converges to the root closer to the mixture that is comprised from the
N(0, 1) and N(8, 4) models. For p3 > 0.14 the algorithm suggests mul-
tiple mixture model ﬁts. This type of behavior is diﬀerent from the one
observed in the case of equal component variance where the MMEs com-
puted on the entire sample converge to an MLE-like root when the third
mixing proportion is increased. Convergence here is slightly slower than
in the case of equal component variances. For example, when p3 = 0.04
and c = 0.05 the mean number of iterations is 6.62, the median is 6.50
and the standard deviation is 2.058. When p3 = 0.10 and c = 0.050 the
mean, median and standard deviation are 9.15, 7 and 7.48 respectively.
When bootstrap is involved we always ﬁnd the interesting roots. When
the percentage of mixing of the third component becomes high we can
detect multiple roots as it is expected.
We also carried out a simulation study where the sampling model was
a three component mixture, with equal proportions from the normal
populations with means (0, 0)T and (4, 4)T and a varying proportion
from a third component. The third component is a normal with mean
(1 + ξ −5, 1 −(ξ −5))T .
This third component is thought of as an
outlying component.
All normal populations have the same identity
covariance matrix. We used values of ξ = 1, 3, 4. The mixing proportion
of this third component is 0.04, 0.10, 0.14, 0.16, 0.20, 0.26. As starting
values we use the method of moment estimates computed on the entire
sample. We also use bootstrap root search. Normal kernel smoothing
was used to compute the Pearson residuals with bandwidth parameter
H(i+1) = cˆΣ(i), where ˆΣ(i) is the estimated covariance at the ith step of
the iteration. The value of c = 0.3 corresponds to mean downweighting
of 1.599 or approximately 2% of the sample size.
We generated 100
samples of size 100 over the range of B.
We ﬁrst examine the behaviour with a ﬁxed starting value and hence
a single solution. When the mixing proportion of the outlying normal
component is less that 0.16 and the MME computed on the entire sam-
©2001 CRC Press LLC

ple were used as starting values the WLEE algorithm converged in every
case to the root that corresponds to ﬁtting a mixture of normals with
means (0, 0)T and (4, 4)T respectively, ignoring the third component.
When the mixing proportion of the third component is 0.16 in 89 out of
100 samples a root corresponding to ﬁtting the mixture of N((0, 0), I)
and N((4, 4), I) was identiﬁed. In the remaining 11 samples the WLEE
algorithm converged to a root ﬁtting one normal component with mean
near (−3, 5)T and a second component with mean similar to the mean
of the remaining data. When the mixing proportion of the normal com-
ponent with mean (−3, 5)T is 0.26, diﬀerent data samples resulted in
solutions which corresponded to ﬁtting one normal component among
means (0, 0)T , (4, 4)T and (−3, 5)T respectively and a second component
with mean similar to the one from the average of the remaining data.
This behaviour is diﬀerent from the one observed in the univariate case
in which the WLEE algorithm started with MMEs computed on the
entire sample converge to one type of root.
We next examine the value of the bootstrap root search. When boot-
strap is implemented depending on the mixing proportion of the normal
component with mean (−3, 5)T all interesting root types are present, as
it is expected. Therefore, the algorithm has the ability to unearth dif-
ferent data substructures when indeed those are present. On the other
hand, when data are generated from a mixture of equal proportions of
N((0, 0), I) and N((4, 4), I) observations then for all bootstrap starting
values and all samples one and only one root is present and that is the
root that corresponds to ﬁtting the model from which the data were
generated.
When the mean of the third component is (0, 2)T , one of the root
types, the MLE-like root is found in most cases for smoothing parameter
equal to 0.3. For values of the smoothing parameter less than 0.3 a root
corresponding closer to ﬁtting a mixture of the normal components with
means (0, 0)T and (4, 4)T is identiﬁed.
For the model p1N((0, 0)T , I) + p2N((4, 4)T , I) + p3N((−3, 5)T , I), p1
= p2 and p3 as above we also calculated Huber’s type M-estimators, as
proposed by McLachlan and Basford (1988, Chapter 2, Section 2.8).
These estimators replace the normal scores with a ψ function. We use
Huber’s psi-function deﬁned as
ψ(s) =

s
if
|s| ≤3.0034
3.0034sign(s)
otherwise
where the constant 3.0034 is calculated according to the formula k1(p) =
[p{1−(2/9)p +((2/9)p)1/21.645}3]1/2 recommended by Campbell (1984).
Their modiﬁed EM algorithm was used with the MLE as starting values.
The algorithm was terminated if the diﬀerence of the estimators between
two consecutive iterations was less than or equal to 10−5.
©2001 CRC Press LLC

TABLE 29.4
Huber’s estimators for the mean parameters and covariance
matrix Σ. The sampling model is
p1N((0, 0), I) + p2N((4, 4), I) + p3N((−3, 5), I), p1 = p2. The sample size
is 100 and the number of Monte Carlo replications is 100
% of mixing
Parameter Estimates
proportion
p3
ˆp1
ˆµ1
ˆµ2
ˆΣ
0%
0.5001
-0.007
3.9807
0.5278
0.0060
-0.003
3.9993
0.0060
0.5574
4%
0.5182
0.0185
3.9256
0.5847
0.0415
0.0423
3.9806
0.0415
0.6038
10%
0.5534
0.0153
3.6423
0.7140
0.1785
0.1807
4.0399
0.1785
0.7500
14%
0.5671
-0.007
3.6019
0.7423
0.1574
0.2413
4.0557
0.1574
0.7595
16%
0.5823
0.0914
3.3172
0.8404
0.3116
0.3420
4.0614
0.3116
0.9127
20%
0.6273
0.4023
2.2049
1.2717
0.7085
0.7222
4.2533
0.7085
1.3497
26%
0.6458
0.8686
0.1058
2.7084
1.1726
1.2912
4.5714
1.1726
2.2481
©2001 CRC Press LLC

Table 29.4 presents the modiﬁed Huber’s estimates for various values
of the mixing parameter p3. From p3 = 0, it is clear that the method does
not consistently estimate the normal covariance Σ, but it does recover
the correct shape up to a constant. However, the shape is distorted as
p3 increases. Notice that the location parameters are estimated fairly
accurately for low values of p3. As the value of p3 increases the bias
of the estimators increases. Moreover, the method does not have the
diagnostic value of the weighted likelihood, as it treats the observations
coming from the third component as contamination.
29.3.3
Other Models
We performed also simulations using mixtures of t-distributions with 2
and 3 degrees of freedom. The data were rescaled to have variance 1.
We generated samples of size 100 and randomly selected 50 data points
to which the constant 2.56 was added. The model ﬁtted was a mixture
of normals with diﬀerent means and variances.
In order to interpret the results it is instructive to bear in mind what
a scientist might want from an analysis of a mixture problem.
The
scientist might require good estimates of the mixing proportions and
good estimates of the central locations of the subpopulations, so as to
be able to describe these populations. The actual scale estimates are
probably of scientiﬁc interest only in describing the extend to which the
populations overlap.
Table 29.5 presents the estimates of the mixture parameters when the
MMEs computed using the entire sample initialized the algorithm. The
WLEE estimates of location have mean squared errors that are about ten
times smaller than the mean squared error of the corresponding MMEs.
It is no longer clear what the correct value for the scale parameters is, so
it is hard to deﬁne bias. Notice however, that the ratio of the two scale
estimates is approximately one. When bootstrap was implemented only
one root was identiﬁed. McLachlan and Peel (1998) discuss a robust ap-
proach to clustering using mixtures of multivariate t-distributions. This
t-mixture model can be ﬁtted using the EMMIX program of McLachlan
et al. (1998).
©2001 CRC Press LLC

TABLE 29.5
Means of MME, MLE, and WLEE estimates and their mean
squared errors. The ﬁtted model is p1N(µ1, σ2
1) + p2N(µ2, σ2
2). The
notation t(r)
3
means that the sample is rescaled to have variance 1.
Also t(r)
3 (2.56) means that the second location is 2.56. In
parenthesis the mean squared error is reported. The sample size is
100 and the number of Monte Carlo replications is 100
Sampling Model: 0.5t(r)
3
+ 0.5t(r)
3 (2.56)
Estimates
ˆπ
ˆµ1
ˆµ2
ˆσ1
ˆσ2
MME
0.5222
2.4763
-0.0149
0.9423
0.9423
(0.0172)
(0.1494)
(0.1330)
(0.0471)
(0.0471)
MLE
0.4934
2.6132
-0.0178
0.7614
0.8068
(0.0044)
(0.0310)
(0.0408)
(0.0900)
(0.0850)
WLE(0.300)
0.5067
2.5785
-0.0380
0.6997
0.6971
(0.0020)
(0.0254)
(0.0132)
(0.1147)
(0.1147)
WLE(0.100)
0.5023
2.5749
-0.0203
0.6540
0.6613
(0.0011)
(0.0127)
(0.0121)
(0.1324)
(0.1308)
WLE(0.050)
0.5025
2.5664
-0.0121
0.6387
0.6458
(0.0012)
(0.0127)
(0.0122)
(0.1431)
(0.1483)
WLE(0.010)
0.5019
2.5499
0.0112
0.6017
0.6078
(0.0015)
(0.0127)
(0.0127)
(0.1719)
(0.1717)
©2001 CRC Press LLC

29.4
Model Selection
A natural question associated with the existence of multiple roots is that
of selecting the model that describes adequately the true distribution.
This adequate description of the distribution is viewed as a goodness
of ﬁt problem, and the parallel disparity measure between the data as
expressed by f ∗(x) and the model m∗
φ(x) provides a natural avenue to
address this problem.
In what follows we will brieﬂy describe our framework.
We assume there exists a true probability distribution τ that has gen-
erated the data. The class M = {mβ : β ∈Θ} is a tentative family of
probability densities which we will call the model. The true model τ may
or may not belong to this class.
The goodness of ﬁt approach that we take here treats the question
of model adequacy, which is equivalent to the question of whether or
not τ belongs in M, as a null hypothesis. To select therefore among the
diﬀerent models suggested by the WLEE methodology we use
ρ(f ∗, m∗
φ) =

G(δ(x))m∗
φ(x)dx,
where G is a strictly convex, thrice diﬀerentiable function, such that
G(0) = 0 and G
′(0) = 0.
Assume now that the true probability model τ that generated the
data belongs to the class M and that the null hypothesis H0 : τ = mφ0
is simple, that is φ0 is completely speciﬁed. In this case, a reasonable
test statistic is
T1 = 2n[G
′′(0)]−1ρ(f ∗, m∗
φ0),
where G
′′(0) > 0, because the function G is strictly convex.
However, in practice, the parameters of the hypothesized model are
not known and need to be estimated from the data. Therefore, the null
hypothesis becomes H0 : τ ∈M , while the alternative states that τ
does not belong in the class M. In this case, we propose to use as a test
statistic the quantity
T2 = 2n[G
′′(0)]−1ρ(f ∗, m∗
ˆφ)
where ˆφ is an estimate of the parameter φ.
The development of the asymptotic distributions of these test statistics
under the null hypothesis and alternatives is work in progress and the
subject of a diﬀerent paper. Here, we would like to point out that evalu-
ation of the statistic T2 at the diﬀerent roots suggested by the weighted
likelihood methodology provides a way of selecting among the diﬀerent
proposed models.
©2001 CRC Press LLC

29.5
Conclusions
In this paper we take a closer look at the performance of weighted like-
lihood in the context of mixture models. It is shown that the weighted
likelihood methodology produces robust and ﬁrst order eﬃcient estima-
tors for the model parameters. When the number of components in the
true model is higher than the number of components speciﬁed in the hy-
pothesized model, the weighted likelihood equations have multiple roots.
In this case, the number of roots can be used as a heuristic to identify
multiple potential mixture model ﬁts.
The existence of multiple roots naturally leads to the model selection
question.
We have proposed two test statistics that may be used to
address this problem.
Acknowledgements
This work was supported by NSF grant DMS-
9973569. The author would like to thank a referee for constructive sug-
gestions that improve the structure of the paper.
References
1. Aitkin, M. and Tunnicliﬀe, W. G. (1980). Mixture models, outliers,
and the EM algorithm. Technometrics 22, 325–331.
2. Basu, A. and Lindsay, B. G. (1994). Minimum disparity estima-
tion for continuous models: Eﬃciency, distribution and robustness.
Annals of Institute of Statistical Mathematics 46, 683–705.
3. Bohning, D. (1999). Computer-Assisted Analysis of Mixtures and
Applications. Chapman and Hall, London, England.
4. Bohning, D., Dietz, E., Schaub, R., Schlattmann, P., and Lindsay,
B. G. (1994). The distribution of the likelihood ratio for mixtures
of densities from the one-parameter exponential family. Annals of
the Institute of Statistical Mathematics 46, 373–388.
5. Campbell, N. A. (1984).
Mixture models and atypical values.
Mathematical Geology 16, 465–477.
6. Clarke, B. R. and Heathcote, C. R. (1994). Robust estimation of
k-component univariate normal mixtures. Annals of the Institute
of Statistical Mathematics 46, 83–93.
7. Cutler, A. and Cordero-Brana, O. (1996). Minimum Hellinger dis-
tance estimation for ﬁnite mixture models. Journal of the Ameri-
can Statistical Association 91, 1716–1723.
©2001 CRC Press LLC

8. De Veaux, R. D. and Krieger, A. M. (1990). Robust estimation of
a normal mixture. Statistics & Probability Letters 10, 1–7.
9. Furman, D. W. and Lindsay, B. G. (1994). Measuring the relative
eﬀectiveness of moment estimators as starting values in maximizing
likelihoods. Computational Statistics and Data Analysis 17, 493–
507.
10. Gray, G. (1994). Bias in misspeciﬁed mixtures. Biometrics 50,
457–470.
11. Lindsay, B. G. (1989). Moment matrices: Applications in mixtures.
Annals of Statistics 17, 722–740.
12. Lindsay, B. G. (1994). Eﬃciency versus robustness: The case of
minimum Hellinger distance and related methods. Annals of Statis-
tics 22, 1018–1114.
13. Lindsay, B. G. (1995).
Mixture Models: Theory, Geometry and
Applications. NSF-CBMS Regional conference series in Probability
& Statistics, Vol. 5, Institute of Mathematical Statistics, Hayward,
CA.
14. Lindsay, B. G. and Basak, P. (1993). Multivariate normal mixtures:
A fast consistent method of moments. Journal of the American
Statistical Association 88, 468–476.
15. Markatou, M., Basu, A., and Lindsay, B. G. (1997). Weighted like-
lihood estimating equations: The discrete case with applications to
logistic regression. Journal of Statistical Planning and Inference
57, 215–232.
16. Markatou, M., Basu, A., and Lindsay, B. G. (1998). Weighted like-
lihood estimating equations with a bootstrap root search. Journal
of the American Statistical Association 93, 740–750.
17. Markatou, M. (1996). Robust statistical inference: Weighted like-
lihoods or usual M-estimation?
Communications in Statistics—
Theory and Methods 25, 2597–2613.
18. Markatou, M. (2000). Mixture models, robustness and the weighted
likelihood methodology. Biometrics (to appear).
19. McLachlan, G. J. and Basford, K. E. (1988).
Mixture Models:
Inference and Applications to Clustering. Marcel Dekker Inc., New
York.
20. McLachlan, G. J. and Peel, D. (1998).
Robust cluster analysis
via mixtures of multivariate t-distributions. In Lecture Notes in
Computer Science, Vol. 1451, (Eds., A. Amin, D. Dori, P. Pudil,
and H. Freeman), pp. 658–666. Spinger-Verlag, Berlin.
©2001 CRC Press LLC

21. McLachlan, G. J., Peel, D., Basford, K. E., and Adams, R. (1998).
EMMIX program.
http://www.maths.uq.edu.au/~gjm/emmix/emmix.html
22. Redner, R. A. and Walker, H. F. (1984). Mixture densities, maxi-
mum likelihood and the EM algorithm. SIAM Review 26, 195–239.
23. Windham, M. P. (1996). Robustizing mixture analysis using model
weighting. Preprint.
24. Woodward, W. A., Parr, W. C., Schucany, W. R., and Lindsey, H.
(1984). A comparison of minimum distance and maximum likeli-
hood estimation of the mixture proportion. Journal of the Ameri-
can Statistical Association 79, 590–598.
25. Woodward, W. A., Whitney, P., and Eslinger, P. W. (1995). Mini-
mum Hellinger distance estimation for mixture proportions. Jour-
nal of Statistical Planning and Inference 48, 303–319.
©2001 CRC Press LLC

30
On Nonparametric Function Estimation
with Inﬁnite-Order Flat-Top Kernels
Dimitris N. Politis
University of California at San Diego, La Jolla, CA
ABSTRACT The problem of nonparametric estimation of a smooth,
real-valued function of a vector argument is addressed. In particular, we
focus on a family of inﬁnite-order smoothing kernels that is characterized
by the ﬂatness near the origin of the Fourier transform of each member
of the family; hence, the term ‘ﬂat-top’ kernels. Smoothing with the
proposed inﬁnite-order ﬂat-top kernels has optimal Mean Squared Error
properties. We review some recent advances, as well as give two new
results on density estimation in two cases of interest: (i) case of a smooth
density over a ﬁnite domain, and (ii) case of inﬁnite domain with some
discontinuities.
Keywords and phrases Nonparametric estimation, ﬂat-top kernels,
density estimation, mean squared error
30.1
Introduction: A General Family of Flat-Top
Kernels of Inﬁnite Order
Let f : Rd →R be an unknown function to be estimated from data.
In the typical nonparametric set-up, nothing is assumed about f except
that it possesses a certain degree of smoothness. Usually, a preliminary
estimator of f can be easily calculated that, however, lacks the required
smoothness; e.g., in the case where f is a probability density. Often,
the preliminary estimator is even inconsistent; e.g., in the case where f
is a spectral density and the preliminary estimator is the periodogram.
Rosenblatt (1991) discusses these two cases in an integrated framework.
In order to obtain an estimator (denoted by ˆf) with good properties,
for example, large-sample consistency and smoothness, one can smooth
©2001 CRC Press LLC

the preliminary estimator by convolving it with a function Λ : Rd →R
called the ‘kernel’, and satisfying

Λ(x)dx = 1; unless otherwise noted,
integrals will be over the whole of Rd. It is convenient to also deﬁne
the Fourier transform of the kernel as λ(s) =

Λ(x)ei(s·x)dx, where
s = (s1, . . . , sd), x = (x1, . . . , xd) ∈Rd, (s · x) = 
k skxk is the inner
product between s and x.
Typically, as the sample size increases, the kernel Λ(·) becomes more
and more concentrated near the origin. To achieve this behavior, we let
Λ(·) and λ(·) depend on a real-valued, positive ‘bandwidth’ parameter
h, that is, we assume that Λ(x) = h−dΩ(x/h), and λ(s) = ω(hs), where
Ω(·) and ω(·) are some ﬁxed (not depending on h) bounded functions,
satisfying ω(s) =

Ω(x)ei(s·x)dx; the bandwidth h will be assumed to
be a decreasing function of the sample size.
If Ωhas ﬁnite moments up to qth order, and moments of order up to
q −1 equal to zero, then q is called the ‘order’ of the kernel Ω. If the
unknown function f has r bounded continuous derivatives, it typically
follows that
Bias( ˆf(x)) = E ˆf(x) −f(x) = cf,Ω(x)hk + o(hk),
(30.1.1)
where k = min(q, r), and cf(x) is a bounded function depending on Ω,
on f, and on f’s derivatives. Note that existence and boundedness of
derivatives up to order r includes existence and boundedness of mixed
derivatives of total order r; cf. Rosenblatt (1991, p. 8).
This idea of choosing a kernel of order q in order to get the Bias( ˆf(x))
to be O(hk) dates back to Parzen (1962) and Bartlett (1963); the paper
by Cacoullos (1966) seems to be the ﬁrst contribution on the multivari-
ate case. Some more recent references on ‘higher-order’ kernels include
the following: Devroye (1987), Gasser, M¨uller, and Mammitzsch (1985),
Granovsky and M¨uller (1991), Jones (1995), Jones and Foster (1993),
Marron (1994), Marron and Wand (1992), M¨uller (1988), Nadaraya
(1989), Silverman (1986), Scott (1992), and Wand and Jones (1993).
Note that the asymptotic order of the bias is limited by the order of
the kernel if the true density is very smooth, i.e., if r is large. To avoid
this limitation, one can deﬁne a ‘superkernel’ as a kernel whose order
can be any positive integer; Devroye (1992) contains a detailed analysis
of superkernels in the case of (univariate) probability density estimation.
Thus, if f has r bounded continuous derivatives, a superkernel will result
in an estimator with bias of order O(hr), no matter how large r may be;
so, we might say that a superkernel is a kernel with ‘inﬁnite order’.
However, it might be more appropriate to say that a kernel has ‘inﬁ-
nite order’ if it results in an estimator with bias of order O(hr) no matter
how large r may be regardless of whether the kernel has ﬁnite moments.
It seems that the ﬁnite-moment assumption for Ωis just a technical one,
©2001 CRC Press LLC

and that existence of the Lebesgue integrals used to calculate the mo-
ments is not necessarily required in order that a kernel has favorable bias
performance; rather, it seems that if the integrals deﬁning the moments
of Ωhave a Cauchy principal value of zero then the favorable bias per-
formance follows, and this is in turn ensured by setting ω to be constant
over an open neighborhood of the origin.
A preliminary report on a speciﬁc type of such inﬁnite order kernel
in the univariate case (that corresponds to an ω of ‘trapezoidal’ shape)
was given in Politis and Romano (1993). Consequently, in Politis and
Romano (1996, 1999) a general family of multivariate ﬂat-top kernels
of inﬁnite order was proposed, and the favorable large-sample bias (and
Mean Squared Error) properties of the resulting estimators were shown
in the cases of probability and spectral density estimation.
Presently, we will propose a slightly bigger, more general class of mul-
tivariate ﬂat-top kernels of inﬁnite order with similar optimality prop-
erties — as shown in Section 30.2. Similarly to the Politis and Romano
(1993, 1996, 1999) papers, our arguments here will focus on rates of con-
vergence without explicit calculation of the proportionality constants
involved; the reader should be aware of the increasing concern in the lit-
erature regarding those constants, and their corresponding ﬁnite-sample
eﬀects — see, for instance, Marron and Wand (1992). Finally, in Section
30.3 we will address the interesting case where the unknown function f
possesses the required smoothness only over a subset of the domain; in
particular, we will investigate to what extend the performance of ˆf(x) is
aﬀected by a discontinuity of f (or its derivatives) at points away from
x.
The general family of multivariate ﬂat-top kernels of inﬁnite order can
be deﬁned as follows.
DEFINITION 30.1.1
Let C be a compact, convex subset of Rd that contains an open neigh-
borhood of the origin; in other words, there is an ϵ > 0 such that
ϵD ⊂C ⊂ϵ−1D, where D is the Euclidean unit ball in Rd.
The kernel ΩC is said to be a member of the general family of multi-
variate ﬂat-top kernels of inﬁnite order if
ΩC(x) = (2π)−d

ωC(s)e−i(s·x)ds,
where the Fourier transform ωC(s) satisﬁes the following properties:
(i) ωC(s) = 1 for all s ∈C;
(ii)

|ωC(s)|2ds < ∞; and
©2001 CRC Press LLC

(iii) ωC(s) = ωC(−s), for any s ∈Rd.
Property (i) guarantees the favorable bias properties and the ‘inﬁ-
nite’ order, while property (ii) ensures a ﬁnite variance of the resulting
estimator ˆf; ﬁnally, property (iii) guarantees that ΩC is real-valued.
In practically working with such a ﬂat-top kernel, one must choose
C. A typical choice for C is the unit ball in lp, with some choice of p
satisfying 1 ≤p ≤∞; see Politis and Romano (1999). In addition, it is
natural to impose the condition that ωC be a continuous function with
the property |ωC(s)| ≤1, for any s ∈Rd. Nevertheless, only properties
(i), (ii), (iii) are required for our results.
30.2
Multivariate Density Estimation: A Review
Suppose X1, . . . , XN are independent, identically distributed (i.i.d.) ran-
dom vectors taking values in Rd, and possessing a probability density
function f; the assumption of independence is not crucial here. The ar-
guments apply equally well if the observations are stationary and weakly
dependent, where weak dependence can be quantiﬁed through the use of
mixing coeﬃcients — see, for example, Gy¨orﬁet al. (1989).
The objective is to estimate f(x) for some x ∈Rd, assuming f pos-
sesses a certain degree of smoothness. In particular, it will be assumed
that the characteristic function φ(s) =

ei(s·x)f(x)dx tends to zero suf-
ﬁciently fast as ||s||p →∞; here s = (s1, . . . , sd), x = (x1, . . . , xd) ∈Rd,
(s·x) = 
k skxk is the inner product between s and x, and ||·||p is the lp
norm, i.e., ||s||p = (
k |sk|p)1/p, if 1 ≤p < ∞, and ||s||∞= maxk |sk|.
We deﬁne the ﬂat-top kernel smoothed estimator of f(x), for some
x ∈Rd, by
ˆf(x) = 1
N
N

k=1
ΛC (x −Xk) =
1
(2π)d

λC(s)φN(s)e−i(s·x)ds,
(30.2.1)
where λC(s) =

ΛC(x)ei(s·x)dx, ΛC(x) = h−dΩC(x/h), for some chosen
bandwidth h > 0, and ΩC satisﬁes the properties of Deﬁnition 30.1; also
note that φN(s) is the sample characteristic function deﬁned by
φN(s) = 1
N
N

k=1
ei(s·Xk).
Now it is well known [cf. Rosenblatt (1991, p. 7)] that if f is contin-
©2001 CRC Press LLC

uous at x, and f(x) > 0, then
V ar( ˆf(x)) =
1
hdN f(x)

Ω2(x)dx + O(1/N).
(30.2.2)
Hence, the order of magnitude of the Mean Squared Error (MSE) of ˆf(x)
hinges on the order of magnitude of its bias. To quantify the bias (and
resulting MSE) of ˆf(x), we formulate three diﬀerent conditions based on
the rate of decay of φ that are in the same spirit as the conditions in
Watson and Leadbetter (1963).
Condition C1:
For some p ∈[1, ∞], there is an r > 0, such that

||s||r
p|φ(s)|ds < ∞
Condition C2: For some p ∈[1, ∞], there are positive constants B and
K such that |φ(s)| ≤Be−K||s||p for all s ∈Rd.
Condition C3: For some p ∈[1, ∞], there is a positive constant B such
that |φ(s)| = 0, if ||s||p ≥B.
Note that if one of Conditions C1 to C3 holds for some p ∈[1, ∞],
then, by the equivalence of lp norms for Rd, that same Condition would
hold for any p ∈[1, ∞], perhaps with a change in the constants B and
K.
Conditions C1 to C3 can be interpreted as diﬀerent conditions on the
smoothness of the density f(x) for x ∈Rd; cf. Katznelson (1968), Butzer
and Nessel (1971), Stein and Weiss (1971), and the references therein.
Note that they are given in increasing order of strength, i.e., if Condition
C2 holds, then Condition C1 holds as well, and if Condition C3 holds,
then Conditions C1 and C2 hold as well. Also note that if Condition
C1 holds, then f must necessarily have [r] bounded, continuous deriva-
tives over Rd, where [·] is the integer part; cf.
Katznelson (1968, p.
123). Obviously, if Condition C2 holds, then f has bounded, continuous
derivatives of any order over Rd.
The following theorem quantiﬁes the performance of the proposed fam-
ily of ﬂat-top estimators.
It was ﬁrst proved in Politis and Romano
(1999) in the case where C is the lp unit ball with 1 ≤p ≤∞; we restate
it—without proof—below in this slightly more general case.
THEOREM 30.2.1
[Politis and Romano (1999)]
Assume that N →∞.
(a) Under Condition C1, and letting h ∼AN −1/(2r+d), for some constant
©2001 CRC Press LLC

A > 0, it follows that
sup
x∈Rd MSE( ˆf(x)) = O(N −2r/(2r+d)).
(b) Under Condition C2, and letting h ∼A/ log N, where A is a constant
such that A < 2K, it follows that
sup
x∈Rd MSE( ˆf(x)) = O(logd N
N
).
(c) Under Condition C3, and letting h be some constant small enough
such that h ≤B−1, it follows that
sup
x∈Rd MSE( ˆf(x)) = O(1/N).
REMARK The special case where
ωC(s) = 0 for all s /∈C has been
considered by many authors in the literature, e.g. Parzen (1962), Davis
(1977), and Ibragimov and Hasminksii (1982). Nevertheless, the choice
ωC(s) = 0 for all s /∈C is not recommendable in practice; see Politis
and Romano (1999) for more details on such practical concerns, including
choosing the bandwidth h in practice.
REMARK A rather surprising observation is that smoothing with ﬂat-
top kernels does not seem to be plagued by the ‘curse of dimensionality’
in case the underlying density is ultra-smooth, possessing derivatives of
all orders, i.e., under Condition C2 (or C3). For example, in Theorem
30.2b under Condition C2, the MSE of estimation achieved by ﬂat-top
kernel smoothing is of order O( logd N
N
), i.e., depending on the dimension d
only through the slowly varying function logd N. A more extreme result
obtains under Condition C3: Theorem 30.2c shows that in that case the
MSE of estimation becomes exactly O(1/N) which is identical to the
parametric rate of estimation, and does not depend on the dimension d
at all.
REMARK It is also noteworthy that, even in the univariate case d =
1, the MSE of estimation is identical to the O(1/N) parametric rate
of estimation under Condition C3, and is very close to O(1/N) under
Condition C2. In other words, if a practitioner is to decide between
ﬁtting a particular parametric model to the data vs. assuming that the
unknown density has derivatives of all orders (i.e., Condition C2) and
©2001 CRC Press LLC

using our proposed ﬂat-top kernel smoothing, there is no real beneﬁt (in
terms of rate of convergence) in favor of the parametric model. As a
matter of fact, the smoothness Condition C2 may be viewed as deﬁning
a huge class of functions that includes all the usual parametric models;
the proposed ﬂat-top kernel smoothing can then proceed to estimate
the unknown function with accuracy comparable to the accuracy of a
parametric estimator.
REMARK It is well-known in the literature [see, for example, M¨uller
(1988) or Scott (1992)] that kernel density estimators corresponding to
kernels of order bigger than two are not necessarily nonnegative func-
tions; it goes without saying that the same applies for our estimators ˆf
that are obtained using kernels of inﬁnite order. Nevertheless, the non-
negativity is not a serious issue as there is a natural ﬁx-up, namely
using the modiﬁed estimator ˆf +(x) = max( ˆf(x), 0); see also Gajek
(1986) and Hall and Murison (1992). Note that the estimator ˆf +(x)
is not only nonnegative, but is more accurate as well, in the sense that
MSE( ˆf +(x)) ≤MSE( ˆf(x)), for all x; this fact follows from the obvious
inequality | ˆf +(x)−f(x)| ≤| ˆf(x)−f(x)|. In addition, if f(x) > 0, an ap-
plication of Chebychev’s inequality shows that Prob{ ˆf(x) = ˆf +(x)} →1
under the assumptions of our Theorem 30.2; on the other hand, if
f(x) = 0, then the large-sample distribution of either
√
hdN ˆf +(x), or
√
hdN ˆf(x), degenerates to a point mass at zero.
REMARK By the formal analogy between probability spectral density
estimation [see, for example, Rosenblatt (1991)] it should not be sur-
prising that ﬂat-top kernels might be applicable in a context of non-
parametric spectral density estimation. In Politis and Romano (1995,
1996), kernels belonging to a subset of the family of ﬂat-top kernels
are employed for the purpose of spectral density estimation using data
consisting of a realization of a stationary time series or a homogeneous
random ﬁeld. Incidentally, note a typo in the statement of Theorem 2
in Politis and Romano (1996): instead of Mi ∼dci log Ni it should read
mi ∼dci log Ni.
30.3
Further Issues on Density Estimation
In this section we will continue the discussion on probability density
estimation based on i.i.d. data X1, . . . , XN, and will investigate to what
extend the performance of ˆf(x) is aﬀected by a discontinuity of f (or its
derivatives) at points away from x.
©2001 CRC Press LLC

30.3.1 
Case of Smo oth Density over a Finite Domain
To ﬁx ideas, consider ﬁrst the univariate case d = 1; it is well-known
that, if the random variables X1, . . . , XN are bounded, that is, if the
density f has domain the ﬁnite interval [a, b] as opposed to R, then
the characteristic function φ will not satisfy the smoothness Conditions
C1, C2, or C3. The situation is exempliﬁed by the smoothest of such
densities, namely the uniform density over the interval [−θ, θ] whose
characteristic function is given by φ(s) = sin θs
θs ; cf. Rao (1973, p. 151).
In general, suppose ¯f is a very smooth density function (e.g., satisfy-
ing one of the smoothness Conditions C1, C2, or C3), and let f(x) =
c ¯f(x)1[−θ,θ](x), where 1[−θ,θ](x) is the indicator function, and c =
1/
 θ
−θ ¯f(x)dx. Then the characteristic function of f is given by φ(s) =
2θc ¯φ(s)∗sin θs
θs , where ¯φ is the characteristic function of ¯f, and ∗denotes
convolution. In other words, the term sin θs
θs
seems unavoidable, and is
due to the truncation of the random variables.
Nevertheless, it seems intuitive that for the ultra-smooth uniform den-
sity over the interval [−θ, θ], smoothing should give good results; this is
indeed true as the following discussion shows. First note that if θ = π,
then sin θs
θs
= 0 for all s ∈Z −{0}; this observation naturally brings us
to Fourier series on the circle deﬁned by ‘wrapping’ the interval [−π, π]
around on a circle, or — in high dimensions — Fourier series on the
d-dimensional torus.
So, without loss of generality (and possibly having to use a linear/aﬃne
transformation in pre-processing the data), assume that X1, . . . , XN are
i.i.d. with probability density f deﬁned on the torus T = [−π, π]d. Now
let the characteristic function φ(s) =

T ei(s·x)f(x)dx, and the sample
characteristic function φN(s) =
1
N
N
k=1 ei(s·Xk ). Recall the Fourier se-
ries formula
f(x) = (2π)−d 
s∈Z d
e−i(s·x)φ(s),
and deﬁne our estimator
ˆf(x) = (2π)−d 
s∈Z d
λC(s)e−i(s·x)φN(s),
where λC(s) was deﬁned in Section 30.2.
As before, we deﬁne some
smoothness conditions based on the characteristic function φ.
Condition K1:
For some p ∈[1, ∞], there is an r > 0, such that

s∈Zd ||s||r
p|φ(s)| < ∞
Condition K2: For some p ∈[1, ∞], there are positive constants B and
K such that |φ(s)| ≤Be−K||s||p for all s ∈Zd.
©2001 CRC Press LLC

Condition K3: For some p ∈[1, ∞], there is a positive constant B such
that |φ(s)| = 0, if ||s||p ≥B (with s ∈Zd).
The following theorem quantiﬁes the performance of the general family
of ﬂat-top estimators; its proof follows closely the proof in Politis and
Romano (1999) and is omitted.
THEOREM 30.3.1
Assume that N →∞.
(a) Under Condition K1, and letting h ∼AN −1/(2r+d), for some con-
stant A > 0, it follows that
sup
x∈T
MSE( ˆf(x)) = O(N −2r/(2r+d)).
(b) Under Condition K2, and letting h ∼A/ log N, where A is a constant
such that A < 2K, it follows that
sup
x∈T
MSE( ˆf(x)) = O(logd N
N
).
(c) Under Condition K3, and letting h be some constant small enough
such that h ≤B−1, it follows that
sup
x∈T
MSE( ˆf(x)) = O(1/N).
REMARK It is easy to see that the uniform density on T satisﬁes con-
dition K3, and thus smoothing with a ﬂat-top kernel achieves the para-
metric
√
N–rate in this case (with no dependence on the dimensionality
d) which is remarkable. Nevertheless, Conditions K1, K2, K3 are quite
stringent as they imply smoothness/diﬀerentiability of f over the whole
torus T; this is equivalent to assuming that a periodic extension of f
over Rd is smooth/diﬀerentiable over the whole of Rd.
To ﬁx ideas, we again return to the case d = 1, and note that Condition
K1 implies that f has [r] bounded, continuous derivatives over T, where
[·] is the positive part; this implies, in particular, that f(−π) = f(π),
f ′(−π) = f ′(π), f ′′(−π) = f ′′(π), and so forth up to the [r]-th derivative.
If f is smooth/diﬀerentiable over (−π, π)d but not over the whole torus
T then the Fourier series method is not appropriate; rather, a technique
of extension of f over the whole of Rd might be useful as elaborated
upon in the next subsection.
©2001 CRC Press LLC

30.3.2 
Case of Inﬁnite Domain with Some Discontinuities
We now return to the set-up of Section 30.2 where X1, . . . , XN are
i.i.d. random vectors taking values in Rd possessing a probability density
function f. The objective is to estimate f(x) for some x in the interior
of I, where I is a compact rectangle in Rd over which f possesses a
certain degree of smoothness. Again without loss of generality (and pos-
sibly having to use a linear/aﬃne transformation in pre-processing the
data), assume that I = [−a, a] × [−a, a] × · · · [−a, a] for some a > 0.
Outside the rectangle I, f and its derivatives might have discontinuities,
and f might even be zero (bringing us to the set-up of bounded random
variables as in the previous subsection). We now deﬁne the following
condition which is related to our previous Condition C1.
Condition C∗[r]: For some positive integer r, f has r bounded, con-
tinuous derivatives over the closed region I.
We again deﬁne the ﬂat-top kernel smoothed estimator of f(x) by
equation (30.2). It is intuitive that, if the tails of ΩC(x) were negligible,
the inﬂuence on ˆf(x) of some Xk observations that are far away from x
(and may even correspond to a region where f is not smooth) would be
insigniﬁcant; this is indeed a true observation, and leads to the following
result. To state it, we deﬁne the smaller rectangle J = [−b, b] × [−b, b] ×
· · · × [−b, b] where 0 < b < a; b should be thought to be close to a such
that the point x of interest will also belong to J.
THEOREM 30.3.2
Assume that
ΩC(x) = O((1 + max
i
|xi|)−q),
(30.3.1)
for some real number q > d. Let p = Prob{X1 ∈I} =

I f(x)dx be
strictly in (0, 1).
Under Condition C∗[r + 1], and letting h ∼AN −1/(2r∗+d), for some
constant A > 0, it follows that
sup
x∈J
MSE( ˆf(x)) = O(N −2r∗/(2r∗+d))
as N →∞, where r∗= min(r, q −d).
PROOF The order of MSE( ˆf(x)) again depends on the bias of ˆf(x)
since V ar( ˆf(x)) is of order 1/(hdN) as before. To estimate the bias,
consider the following argument.
Let ¯f be a probability density that has (at least) r+1 bounded deriva-
tives over the whole of Rd, and such that ¯f(x) = f(x) for all x ∈I; this
©2001 CRC Press LLC

extension of f over the whole of Rd can be done in many ways—see e.g.
Stein (1970). Re-order the Xis in such a way that X1, . . . , XK are in I,
whereas XK+1, . . . , XN are in Ic, i.e., the complement of I.
Construct a new sample Y1, . . . , YN with the property that Yi = Xi
for i = 1, . . . , K, and such that YK+1, . . . , YN are drawn i.i.d. from den-
sity ¯f(x)1Ic(x)/(1 −p), where p = Prob{X1 ∈I} =

I f(x)dx. It is
apparent now that the sample Y1, . . . , YN can be considered as a bona
ﬁde i.i.d. sample from density ¯f(x) for x ∈Rd.
Now recall that (together with the re-ordering) we have
ˆf(x) = 1
N
N

k=1
ΛC (x −Xk) = 1
N
K

k=1
ΛC (x −Xk) + (N −K
N
)u1.
Note that, due to assumption (30.3), and to the fact that an observation
outside I will be at a distance of at least a −b (in an l∞sense) from
the point x ∈J, we have u1 = O( hq−d
(a−b)q ) almost surely.
Finally, observe that
1
N
K

k=1
ΛC (x −Xk) = 1
N
K

k=1
ΛC (x −Yk)
= 1
N
N

k=1
ΛC (x −Yk) + (N −K
N
)u2,
where again u2 = O( hq−d
(a−b)q ) almost surely.
To summarize:
ˆf(x) = 1
N
N

k=1
ΛC (x −Yk) + (u1 + u2)(N −K
N
).
(30.3.2)
Taking expectations in equation (30.2), and recalling that E( N−K
N
) =
1 −p, and
E

1
N
N

k=1
ΛC (x −Yk)

= ¯f(x) + o(hr),
and that ¯f(x) = f(x) for all x ∈I (and thus for all x ∈J as well), the
theorem is proven.
REMARK Theorem 30.3 shows that, in the possible presence of discon-
tinuities in f or its derivatives at points away from the target region I, it
is important to use a ﬂat-top kernel ΩC(x) that is chosen to have small
tails. For example, the case where ωC(s) = 0 for all s /∈C that was con-
sidered by Parzen (1962), Davis (1977), and Ibragimov and Hasminksii
©2001 CRC Press LLC

(1982) satisﬁes equation (30.3) with q = 1 and is not recommendable.
By contrast, the simple kernel ΛP ROD
c
(x) of Politis and Romano (1999)
satisﬁes (30.3) with q = 2 as long as c > 1.
REMARK It is easy to construct ﬂat-top kernels satisfying equation
(30.3) with q > 2; all it takes is to make sure that
ωC(s) has a high
degree of smoothness (e.g., high number of derivatives) for all s.
To
do this, one must pay special attention at the boundary of the region
C since, inside C, ωC(s) is inﬁnitely diﬀerentiable with all derivatives
being zero. We now give an explicit construction of a ﬂat-top kernel
satisfying (30.3) with an arbitrary exponent q in the case where C is the
l∞unit ball. Let ω1(s1) be a member of the ﬂat-top family in the case
d = 1, and having the following properties:
ω1(s1) = ω1(−s1) for all
s1 ∈R; ω1(s1) = 1 for all |s1| ≤1; ω1(s1) = 0 for all |s1| ≥c for some
c > 1 (c = 2 is a useful practical choice); ω1(s1) is monotone decreasing
for 1 < s1 < c; and
ω1(s1) possesses n continuous derivatives for all
s1 ∈R.
Then, the d-dimensional ﬂat-top kernel ΩC(x) with Fourier
transform equal to ωC(s) = d
i=1 ω1(si) satisﬁes (30.3) with exponent
q > n + 1.
As a last remark, note that Condition C∗[r + 1] in Theorem 30.3 can
be relaxed to C∗[r] if ΩC(x) is chosen to have r ﬁnite moments which
in turn can be guaranteed by having q > r + 1 in equation (30.3). The
following corollary should be compared to Theorem 30.2(a).
COROLLARY 30.3.3
Assume Condition C∗[r], as well as equation (30.3) with some q > r + 1
and q ≥r + d. Let p = Prob{X1 ∈I} =

I f(x)dx be strictly in (0, 1).
Letting h ∼AN −1/(2r+d), for some constant A > 0, it follows that
sup
x∈J
MSE( ˆf(x)) = O(N −2r/(2r+d))
as N →∞.
Acknowledgement
This research was partially supported by NSF
grant DMS 97-03964. Many thanks are due to Prof. George Kyriazis of
the University of Cyprus for many helpful discussions, and to Prof. E.
Masry of the University of California at San Diego for pointing out a
mistake in Politis and Romano (1996).
©2001 CRC Press LLC

References
1. Bartlett, M. S. (1963). Statistical estimation of density functions.
Sankhy¯a, Series A 25, 245–254.
2. Butzer, P. and Nessel, R. (1971). Fourier Analysis and Approxi-
mation. Academic Press, New York.
3. Cacoullos, T. (1966). Estimation of a multivariate density. Annals
of the Institute of Statistical Mathematics 18, 178–189.
4. Davis, K. B. (1977). Mean integrated square error properties of
density estimates. Annals of Statistics 5, 530–535.
5. Devroye, L. (1987). A Course in Density Estimation. Birkh¨auser,
Boston.
6. Devroye, L. (1992). A note on the usefulness of superkernels in
density estimation. Annals of Statistics 20, 2037–2056.
7. Gajek, L. (1986). On improving density estimators which are not
bona ﬁde functions. Annals of Statistics 14, 1612–1618.
8. Gasser, T., M¨uller, H. G., and Mammitzsch, V. (1985). Kernels for
nonparametric curve estimation. Journal of the Royal Statistical
Society, Series B 47, 238–252.
9. Granovsky, B. L. and M¨uller, H. G. (1991). Optimal kernel meth-
ods: A unifying variational principle. International Statistical Re-
view 59, 373–388.
10. Gy¨orﬁ, L., H¨ardle, W., Sarda, P., and Vieu, P. (1989).
Non-
parametric Curve Estimation from Time Series.
Lecture Notes
in Statistics No. 60, Springer-Verlag, New York.
11. Hall, P. and Murison, R. D. (1992).
Correcting the negativity
of high-order kernel density estimators.
Journal of Multivariate
Analysis 47, 103–122.
12. Ibragimov, I. A. and Hasminksii, R. Z. (1982). Estimation of dis-
tribution density belonging to a class of entire functions. Theory
of Probability and Its Applications 27, 551–562.
13. Jones, M. C. (1995). On higher order kernels. Journal of Nonpara-
metric Statistics 5, 215–221.
14. Jones, M. C. and Foster, P. J. (1993). Generalized jackkniﬁng and
higher order kernels. Journal of Nonparametric Statistics 3, 81–94.
15. Katznelson, Y. (1968).
An Introduction to Harmonic Analysis.
Dover, New York.
16. Marron, J. S. (1994). Visual understanding of higher order kernels.
Journal Computational and Graphical Statistics 3, 447–458.
©2001 CRC Press LLC

17. Marron, J. S. and Wand, M. P. (1992). Exact mean integrated
squared error. Annals of Statistics 20, 712–736.
18. M¨uller, H. G. (1988). Nonparametric Regression Analysis of Lon-
gitudinal Data. Springer-Verlag, Berlin.
19. Nadaraya, E. A. (1989). Nonparametric Estimation of Probability
Densities and Regression Curves.
Kluwer Academic Publishers,
Dordrecht, The Netherlands.
20. Parzen, E. (1962). On estimation of a probability density function
and its mode. Annals of Mathematical Statistics 33, 1065–1076.
21. Politis, D. N. and Romano, J. P. (1993). On a family of smooth-
ing kernels of inﬁnite order. In Computing Science and Statistics,
Proceedings of the 25th Symposium on the Interface, San Diego,
California, April 14-17, 1993 (Eds., M. Tarter and M. Lock), pp.
141–145, The Interface Foundation of North America.
22. Politis, D. N. and Romano, J. P. (1995). Bias-corrected nonpara-
metric spectral estimation. Journal of Time Series Analysis 16,
67–104.
23. Politis, D. N. and Romano, J. P. (1996). On ﬂat-top kernel spec-
tral density estimators for homogeneous random ﬁelds. Journal of
Statistical Planning and Inference 51, 41–53.
24. Politis, D. N. and Romano, J. P. (1999). Multivariate density es-
timation with general ﬂat-top kernels of inﬁnite order. Journal of
Multivariate Analysis 68, 1–25.
25. Priestley, M. B. (1981), Spectral Analysis and Time Series. Aca-
demic Press, New York.
26. Rao, C. R. (1973). Linear Statistical Inference and its Applications,
Second Edition. John Wiley & Sons, New York.
27. Rosenblatt, M. (1991). Stochastic Curve Estimation. NSF-CBMS
Regional Conference Series Vol. 3, Institute of Mathematical Statis-
tics, Hayward, CA.
28. Scott, D. W. (1992).
Multivariate Density Estimation: Theory,
Practice, and Visualization. John Wiley & Sons, New York.
29. Silverman, B. W. (1986).
Density Estimation for Statistics and
Data Analysis. Chapman and Hall, London.
30. Stein, E. M. (1970). Singular Integrals and Diﬀerentiability Prop-
erties of Functions. Princeton University Press, Princeton, New
Jersey.
31. Stein, E. M. and Weiss, W. (1971). Introduction to Fourier Anal-
ysis on Euclidean Spaces. Princeton University Press, Princeton,
New Jersey.
©2001 CRC Press LLC

32. Wand, M. P. and Jones, M. C. (1993). Comparison of smoothing
parameterizations in bivariate kernel density estimation. Journal
of the American Statistical Association 88, 520–528.
33. Watson, G. S. and Leadbetter, M. R. (1963). On the estimation
of the probability density I. Annals of Mathematical Statistics 33,
480–491.
©2001 CRC Press LLC

31
Multipolishing Large Two-Way Tables
Kaye Basford, Stephan Morgenthaler and John W. Tukey
University of Queensland, Brisbane, Australia
Ecole Polytechnique F´ed´erale de Lausanne, Lausanne, Switzerland
Princeton University, Princeton, NJ
ABSTRACT This paper discusses tools for the more detailed analysis
of large two-way tables. Because of its simple structure, the additive ﬁt is
often inappropriate for large tables. To go further, multipolishing, where
the rows are ﬁtted as function of a vector representing the columns, pro-
vides a possible approach. The paper compares this technique with ideas
based on the singular value decomposition. The methods are illustrated
with a plant breeding example.
Keywords and phrases Analysis of variance, nonadditivity, interac-
tions, graphical displays of two-way tables, singular value decomposition,
plant breeding
31.1
Introduction
Plant breeding experiments involve the assessment of the quality of ge-
netically diﬀerent lines. If a single attribute is measured in a variety of
environments, a genotype by environment (G × E) table is determined.
The standard analysis of G × E tables decomposes the observed table
(yge) (g = 1, . . . , G, e = 1, . . . , E) into subtables of simple and inter-
pretable structure. Of course, this is but one of a multitude of classes
of examples involving two-way tables. When the table is large, such as
30 × 40 or even 50 × 200, ﬁtting the additive decomposition m + ug + ve,
using one common eﬀect, a vector u of individual genotype eﬀects, and
a vector v of individual environment eﬀects will typically not satisfy the
user. This class of ﬁts, which describes the eth column (y1e, . . . , yGe)
as
(m + ve) + u ,
(31.1.1)
is not rich enough to describe adequately the detailed aspects of the be-
havior of the response, which can be made visible because of the large
©2001 CRC Press LLC

amount of data. Note that we used boldfaced letters to denote vectors.
This will be the convention throughout the paper. Adding the whole
of the usual interaction to (31.1.1) is not helpful and would in any case
not result in an interpretable description. We should ask ourselves, in
what ways we can add to the simple additive model to arrive at a de-
composition that provides more insights in typical cases. The types of
decompositions we are led to use have been described by, among others,
Tukey (1962, Section VII), Gollob (1968), Hoaglin, Mosteller and Tukey
(1985, Ch. 3) and have been extensively discussed by Mandel (1995).
Suppose the genotype factor is associated with the numerical row vari-
able (u1, . . . , uG) and the environment factor is similarly associated with
the column variable (v1, . . . , vE). A variable such as u comes in handy in
the description of the columns of the table. We might, for example, con-
sider a description of the eth column (y1e, . . . , yGe) in terms of a linear
function of the form
pe + qe u ,
(31.1.2)
and similarly for the rows of the table. Examples with such external
variables occur frequently in physics and chemistry (and economics and
demographics). Suppose, for example, that the rows correspond to dif-
fering amounts of a catalyst.
A simple (conditional) model for each
column vector of responses might then postulate a linear dependence on
these amounts. In some other classes of examples, there is no useful ex-
ternal variable, in which case we propose to deﬁne one internally, based
on the observations. Various reasonable choices present themselves, for
example, the row and column eﬀects from an additive ﬁt to the table
or the two principal eigenvectors from a singular value decomposition of
the table.
In this paper we explore the use of such models and describe several
possible plots for visualizing the resulting ﬁt, reserving the use of one of
more (histograms) rootograms of residuals at one or more stages of the
ﬁt for a later paper.
31.2
Bilinear Multipolishes
The additive ﬁt to a two-way table (yge) with G rows and E columns
is of the form m + ug + ve, where ug are the row eﬀects and ve denotes
the column eﬀects. Finding least-squares estimates of m, ug and ve can
be achieved by polishing or row-and-column sweeping. This is an
algorithm one form of which works as follows:
[P1] Compute the row means and sweep them from the table:
©2001 CRC Press LLC

˜ug = E
e=1 yge/E
˜rge = yge −˜ug
[P2] Compute the column means and sweep them from the table, and
from the vector of preliminary eﬀects created in step [P1]:
m = G
g=1 ˜ug/G
ug = ˜ug −m
ve = G
g=1 ˜rge/G
rge = ˜rge −ve.
This method is inspired by Eq. (31.1.1), which shows that for centered u,
that is ¯u = 0, we can ﬁnd m + ve by taking the mean of the e-th column
thus formed. The above algorithm will thus produce eﬀects satisfying

g ug = 
e ve = 0.
In a multipolish, it is not simply the mean that is being swept from
a row or a column, but a more complex function involving an auxiliary
variable. Let u = (ug) be a centered variable for the rows and v = (ve)
a centered variable for the columns.
Centering means that 
g ug =

e ve = 0.
Inspired by Eq. (31.1.2) one can then modify the above
algorithm in the following way:
[MP1] Fit a linear function in v to each row and sweep it from the
table:
˜ag = E
e=1 yge/E
˜bg = E
e=1 ygeve/ E
e=1 v2
e
˜rge = yge −˜ag −˜bg ve;
[MP2] Sweep a linear function in u from each column of the table and
from the vectors of preliminary eﬀects created in step [MP1]:
m11 = G
g=1 ˜ag/G
m21 = G
g=1 ˜agug/ G
g=1 u2
g
ag = ˜ag −m11 −m21ug
m12 = G
g=1 ˜bg/G
m22 = G
g=1 ˜bgug/ G
g=1 u2
g
bg = ˜bg −m12 −m22ug
pe = G
g=1 ˜rge/G
qe = G
g=1 ˜rgeug/ G
g=1 u2
g
rge = ˜rge −pe −qeug
Of course, this algorithm produces eﬀects that are not only centered,
but are also orthogonal to the auxiliary variables.
To express these
conditions, Eq. (31.1.2) must be written as
(m11 + m12ve + pe) + (m21 + m22ve + qe) u ,
where the symbols correspond to the ones used in the multipolish algo-
rithm. The cycle [MP1], [MP2] also adds two terms for the eﬀects swept
©2001 CRC Press LLC

from the rows, which shows that the ﬁt to the e-th column is in fact
equal to
(m11 + m12ve + pe) + (m21 + m22ve + qe) u + a + veb .
(31.2.1)
31.2.1
Choosing the Auxiliary Variables Equal to the Eﬀects
of the Additive Fit
As we have indicated, it is quite natural to use [P1], [P2] in order to
create two variables u and v, and then to apply [MP1], [MP2].
To
describe this algorithm, it is helpful to put it into a matrix notation.
Let Y = (yge) be the data matrix of dimension G × E. The additive ﬁt,
obtained by [P1] and [P2], satisﬁes
– m = 1GT Y 1E/(EG)
– u = Y 1E/E −m1G
– v = Y T 1G/G −m1E
where 1n denotes a vector of length n whose components are all equal to
1. If we use these vectors as auxiliary variables, we obtain the following
ﬁt.
PROPOSITION 31.2.1
The bilinear multipolish ﬁt to the table Y ∈RG×E with the eﬀects of the
additive ﬁt as auxiliary variables is equal to
m1G1E
T + u1E
T + 1GvT + m22uvT + bvT + uqT ,
(31.2.2)
where
–
m = 1GT Y 1E/(EG)
–
m22 = uT Y v/((uT u)(vT v))
–
b = Y v/vT v −1G −m22u
–
q = Y T u/uT u −1E −m22v.
PROOF The step [MP1] leads to
– ˜a = Y 1E/E = u + m1G
– ˜b = Y v/vT v
– ˜R = (˜rge) = Y −˜a1ET −˜bvT
The sweeps in [MP2] will then result in
– m11 = m
– m21 = 1
©2001 CRC Press LLC

– a = 0G
– m12 = 1G T Y v /(Gv T v) = G(v T + m1E T )v /(Gv T v) = 1
– m22 = uT Y v /((uT u)(v T v))
– b = Y v /v T v −1G − m22 u
– p = ˜R T 1G /G = Y T 1G /G −1E˜aT 1G /G − v˜b
T 1G /G = (v+ m1E)−
m1E − v = 0E
– q = ˜R T u/uT u = Y T u/uT u −1E − m22 v
The ﬁrst three terms in (31.2.2) are equal to the additive ﬁt, adding
the forth term leads to Tukey’s (1949) odoffna, whereas the last two
terms are result of ﬁtting a straight line in the auxiliary variable to each
row and column.
31.2.2
Robust Alternatives
It is easy and straightforward to create a robust polish and multipol-
ish. Instead of using the least-square criterion when sweeping from the
rows and columns, one is in fact free to use any other method. If one
replaces the mean with the media in [P1] and in [P2], for example, the
median polish is obtained [Tukey (1977, Ch. 11)]. There is, however, one
important change in the algorithm. In general it does not converge in
two steps, [P1]–[P2], but has to be interated [P1]–[P2]–[P1]–[P2]–[P1]–
and so on until convergence or until the changes become small enough.
Similarly, in [MP1] and [MP2], the least-squares ﬁtting of a straight line
can be replaced by a robust line such as the biweight line.
31.3
Matrix Approximations
In this section we present an alternative way of thinking about the ﬁt
obtained in (31.2.2), based on the following result about rank-one ap-
proximations.
LEMMA 31.3.1
Let A ∈RG×E be an arbitrary matrix and let v ∈RE be an arbitrary
nonzero vector. The best least-squares approximation of the form uvT
to the matrix A is obtained by choosing
u = Av/vT v ∝Av .
(31.3.1)
©2001 CRC Press LLC

The corresponding approximation is
A ≈AvvT
vT v
(31.3.2)
and the sum of the squared elements of the diﬀerence between these ma-
trices is tr(AT A) −vT AT Av/vT v.
PROOF The sum of the squared entries of the diﬀerence A −uvT can
be computed as
tr
 
A −uvTT 
A −uvT 
,
where tr denotes the trace. Using elementary properties of the trace, we
can express this sum of squares as tr

ATA

−2uTAv + (uTu)(vTv).
This quadratic form in u is minimized when
2u(vT v) −2Av = 0G .
We could, of course, just as well have started with a given vector
u ∈RG and found the least-squares approximation based on u alone,
A ≈uuTA
uTu .
(31.3.3)
One can also iterate, for example, starting with a vector v0. The ﬁrst
approximation is then based on u0 = Av0/v0Tv0. If we use this vector
to compute an approximation, it will make use of v1 = ATu0/u0Tu0. If
iterating these two matrix multiplications converges, the limiting vectors
u∞and v∞must be ﬁxed points of the iteration. This implies
u∞+1 = Av∞/v∞
Tv∞
v∞+1 = ATu∞+1/u∞+1
Tu∞+1
= (ATAv∞)(v∞
Tv∞)/v∞
TATAv∞
= v∞
Thus
(ATA)
v∞
(v∞Tv∞)1/2 = (v∞TATAv∞)
(v∞Tv∞)
v∞
(v∞Tv∞)1/2 .
The resulting approximation is of the form
A ≈d
u∞
(u∞Tu∞)1/2
v∞T
(v∞Tv∞)1/2 ,
©2001 CRC Press LLC

with d =

v∞ T ATAv∞
1/2. This is the well-known rank-one singular
value approximation of the matrix A, with d being the largest singular
value. This shows that the approximation given in the above lemma
can be interpreted as a one-step version of the rank-one singular value
approximation, starting from the vector v.
31.3.1
Approximations of Two-Way Tables
By applying the above lemma one can derive the ﬁt described in (31.2.2).
The ﬁrst stage consists in centering the matrix,
Y = m 1G1E 
T +

Y −m 1G1E 
T
.
Next, we apply the rank-one approximation based on the vector v0 = 1E
to the centered matrix. Eq. (31.3.1) gives
u =

Y −m 1G1E 
T
1E /1E 
T1E = Y1E /E −m 1G ,
which is equal to the vector of row eﬀects (see Section 31.2.1).
The
approximation (31.3.2) is u1ET . If we add this to the previous ﬁt, we
have
Y = m 1G1E
T + u1E
T +

Y −m 1G1E
T −u1E
T
.
Next, one applies the rank-one approximation based on u0 = 1G to the
new remainder matrix. The lemma shows that
v =

Y −m 1G1E
T −u1E
TT 1G/G = Y1G/G −m1E
leads to an optimal approximation 1GvT . Adding this term into the ﬁt
gets us to the usual additive ﬁt
Y = m 1G1E
T + u1E
T + 1GvT +

Y −m 1G1E
T −u1E
T −1GvT
.
At this stage, several directions in which we might proceed further
present themselves. We could iterate the one-step procedure we have
used up to now, which would lead us to a one-rank singular value ap-
proximation of the centered data matrix. Or, we can proceed with the
method described in our lemma with the aim of extracting further terms
for the model from the residual matrix. Of course, we need to change
our starting vectors, but we have natural candidates in u and v. Start-
ing with the vector v, we would then like to approximate the residuals
from the additive ﬁt.
The corresponding term in the ﬁt is equal to

Y −m 1G1ET −u1ET −1GvT
vvT/vTv =

Yv −1GvTv

vT/vTv =
©2001 CRC Press LLC

YvvT/vTv −1GvT, where we made us of the fact that 1ET v = 0. After
this step, the ﬁt has become
Y = m 1G1E
T + u1E
T + YvvT/vTv + remainder matrix .
Next, we compute the rank-one approximation based on u of the new
remainder matrix. This term in the approximation is equal to
uuT 
Y −m 1G1E
T −u1E
T −YvvT/vTv

/uTu
= uuT Y/uTu −u1E
T −uuTYvvT/((vTv)(uTu)).
At this stage, the ﬁt is
Y = m 1G1E
T + uuT Y/uTu + YvvT/vTv
−uuT YvvT/((vTv)(uTu)) + R .
(31.3.4)
Since the ﬁt in (31.3.4) is equal to (31.2.2), multipolishing (by least
squares) with the vectors of eﬀects u and v is equal to the one-step
least-squares approximation of our lemma. Of course it would again be
possible to iterate and to replace the terms we added to the additive
model by the rank-one singular value approximation.
31.4
Displays
If we wish to display the ﬁtted values (31.3.4), we can decompose them
into rows. The ﬁtted gth row is equal to
m 1E + ugYTu
uT u
+ v

e ve yge
vT v
−ug
uT Yv
(vT v)(uT u)

,
which we can plot against the vector v. Doing this for 1 ≤g ≤G results
in a set of straight lines if the additional condition YTu ∝v is satisﬁed.
Note that this holds for u∞and v∞. However, replacing u and v by
these two vectors would not lead to the analogue of (31.3.4). In general,
this additional condition will not be true and the curves will be jagged
lines instead of straight ones. One could separate the second from the
other terms. In this case, what is left over is a family of straight lines
intersecting at the origin and a family of jagged lines. Looking from
the multipolish point of view (31.2.2) at these same terms, one notes
immediately, that the jagged part is due to the vectors b and q. So, an
©2001 CRC Press LLC

alternative way of plotting the rows would show b as a function of u. In
the resulting scatter plot, one would be able to spot unusual genotypes
(rows).
If one wants to study the dependence of the the ﬁtted eth column
against u, one is led to
m 1G + vgYv
vT v + u

g ug yge
uT u
−vg
uT Yu
(vT v)(uT u)

.
31.5
Example
The G×E data set under investigation comes from the International
Maize and Wheat Improvement Centre (CIMMYT), an internationally
funded, non-proﬁt research organization aimed at improving the produc-
tivity, proﬁtability, and sustainability of maize and wheat farmers in low-
income countries. CIMMYT disseminates most of its elite germplasm
through a system of international nurseries, where the term “nursery”
means a collection of wheat lines assembled to meet a breeding objective
(Fox, 1996).
One of the major CIMMYT nurseries was the Interna-
tional Spring Wheat Yield Nursery (ISWYN), with the ﬁrst, ISWYN1,
distributed in 1964 and the last, ISWYN30, distributed in 1994 to co-
operators around the world as pre-packaged seed, ready for sowing in an
international multi-environment trial.
The one chosen to analyse here was ISWYN11. It contains the yield (in
tonnes per hectare) of 50 genotypes grown in each of 65 environments.
The 50 genotypes comprised 49 entries that were chosen by the CIM-
MYT breeders (from their lines and nominations from diﬀerent parts of
the world) and a local check. The 65 environments comprised various
locations around the world, with co-operators sometimes using the same
location under diﬀerent conditions or time of planting (and hence label-
ing it as a diﬀerent environment). Over the full thirty ISWYNs, a total
of 605 lines (generally in subsets of 50) were tested in 407 environments
(of which 193 were used in three or more ISWYNs).
CIMMYT orientates breeding on the concept of mega-environments,
deﬁned as broad (not necessarily continuous and frequently transconti-
nental) areas, characterised by similar biotic and abiotic stresses, crop-
ping system requirements and consumer preferences. Germplasm may
accommodate major stresses through the mega-environment for which
it was developed, but perhaps not all the signiﬁcant secondary stresses.
Hence it was expected that there would be considerably less interaction
and that main eﬀects would not be suﬃcient to explain the variation in
©2001 CRC Press LLC

TABLE 31.1
This table shows part of a 50 × 65 genotype by environment
dataset. The rightmost column and the last row show the additive
eﬀects as ﬁtted by least squares
E1
E2
E3
E4
E5
E6
E7
E8
G1
5.1
1.3
4.0
5.5
3.9
2.1
2.5
3.0
G2
3.1
0.7
3.5
5.8
4.8
2.4
2.2
2.9
G3
3.5
1.6
3.8
3.7
4.4
2.2
2.0
2.3
G4
4.6
1.5
3.4
5.2
2.8
2.2
2.8
2.4
G5
5.9
1.3
4.1
5.7
3.3
1.0
2.5
3.0
G6
4.7
1.3
3.3
6.3
5.3
4.1
2.4
2.4
G7
5.0
0.9
2.1
5.7
2.4
3.0
2.3
2.6
G8
4.9
1.1
4.2
6.5
2.5
3.1
2.2
2.4
G9
4.6
1.0
4.2
6.6
3.0
1.2
3.0
3.1
G10
5.4
1.1
3.1
6.8
3.4
2.4
2.3
2.6
G11
4.6
1.5
4.3
6.3
3.8
2.3
2.7
2.9
G12
6.0
1.6
4.1
6.3
5.5
3.1
2.8
3.1
G13
5.6
1.3
2.2
6.5
2.8
2.8
2.4
2.7
G14
4.6
1.5
3.4
4.1
2.9
2.0
2.1
2.4
G15
4.3
0.9
3.5
5.1
1.9
2.7
1.8
2.2
G16
5.7
1.5
3.5
6.1
4.0
4.0
2.6
2.2
G17
6.4
1.4
3.7
5.4
5.2
2.2
2.4
3.2
E eﬀect
1.11
−2.55
−0.23
1.69
−0.17
−1.09
−1.40
−1.26
E9
E10
E11
E12
E13
E14
E15
G eﬀect
G1
3.0
4.7
6.4
4.8
5.2
4.1
3.8
0.15
G2
2.5
3.6
6.4
4.6
4.9
5.5
2.9
−0.47
G3
2.6
4.1
6.6
4.9
5.2
4.1
3.5
−0.15
G4
3.9
3.6
6.2
4.4
5.0
3.4
3.1
−0.28
G5
3.7
3.8
5.8
4.9
5.1
3.7
3.8
0.06
G6
3.6
4.7
6.9
5.1
5.5
3.3
4.1
0.12
G7
4.6
4.3
7.0
5.3
5.6
3.5
4.1
0.23
G8
3.5
4.7
6.7
5.2
5.7
4.4
3.9
0.03
G9
3.1
2.0
6.1
4.9
4.9
3.3
3.9
−0.14
G10
3.9
4.6
6.3
5.6
5.7
3.9
4.2
0.04
G11
3.9
5.4
7.0
5.4
5.7
4.1
4.5
0.23
G12
3.3
4.8
6.3
5.5
5.4
4.4
5.5
0.30
G13
3.7
3.1
8.0
5.6
6.2
4.1
3.9
0.16
G14
2.7
3.3
3.4
3.9
3.3
2.4
3.1
−0.67
G15
2.9
4.6
7.3
5.6
6.2
4.2
4.5
−0.04
G16
3.2
4.6
6.9
5.2
5.5
3.7
3.8
0.28
G17
3.4
4.1
6.1
4.1
6.0
5.1
4.7
0.38
E eﬀect
−0.69
0.46
2.62
1.27
1.72
−0.03
−0.09
3.81
©2001 CRC Press LLC

the data.
The four following ﬁgures show the genotypes in dependence of the
environments and vice versa.
The jagged part of the curves is quite
pronounced for some genotypes. Apart from a few exceptions, the plot
of the genotypes is quite encouraging in the sense that the good ones
are relatively good across all environments. Plotting the environments
as a function of genotypes would show ﬂat lines, if none of the genotypes
were well adapted to some class of environments. This is largely the case
in this example, but note that some of lines rise as one goes across the
genotypes.
Fig. 31.5 shows the individual eﬀects of the genotypes. On the hori-
zontal axis are the eﬀects from an additive model, whereas on the vertical
axis are the components of the vector b, that is the individual slopes,
when regression the responses of one genotype one the environments.
The genotypes G19, G28, and G45 are highlighted. The ﬁrst one has
the smallest ug and also a very small bg. The others have values of ug
slightly above average and are opposites with regard to bg. Fig. 31.6
shows that this has the expected eﬀects, namely a slower rise of the ﬁt-
ted response for G45 and a faster one for G28. The genotype G19 is
almost everywhere worst.
31.6
Concluding Remarks
We have discussed a special case of a method for ﬁtting large two-way
tables. With relatively few degrees of freedom, namely 1+(G−2)+(E−2)
additional parameters, it allows the user to explain important parts of
the interaction. A big advantage of the proposed method is the relative
ease of interpretation. We have seen that one can display the results and
use the ﬁtted eﬀects to identify remarkable environments or genotypes.
Acknowledgements.
The research by Stephen Morgenthaler has been
supported by a grant from the Swiss National Science Foundation.
©2001 CRC Press LLC

References
1. Fox, P. N. (1996). The CIMMYT Wheat Program’s International
Multi-environment Trials, Plant Adaptation and Crop Improve-
ment (Eds., M. Cooper and G. L. Hammer), pp. 139–164. CAB
International, Oxford, England.
2. Gollob, H. F. (1968). A statistical model which combines features
of factor analytic and analysis of variance techniques. Psychome-
trika 33, 73–116.
3. Hoaglin, D. C., Mosteller, F. and Tukey, J. W. (1985). Exploring
Data Tables, Trends, and Shapes. John Wiley & Sons, New York.
4. Mandel, J. (1995). Analysis of Two-Way Layouts. Chapman and
Hall, London.
5. Tukey, J. W. (1949). One degree of freedom for non-additivity.
Biometrics 5, 232–242.
6. Tukey, J. W. (1962). The future of data analysis. Annals of Math-
ematical Statistics 33, 1–67.
7. Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley,
Reading, Massachussetts.
8. Tukey, J. W. (1977).
The Collected Works of John W. Tukey:
Volume III, Philosophy and Principles of Data Analysis from 1949
to 1964. Wadsworth & Brooks/Cole, Monterey, California.
©2001 CRC Press LLC

FIGURE 31.1
Each curve corresponds to a genotype and shows the ﬁtted value
as a function of the environment eﬀects
©2001 CRC Press LLC

FIGURE 31.2
The top panel shows the G-linear part of the ﬁt, whereas the
bottom contains the rest of the ﬁt
©2001 CRC Press LLC

FIGURE 31.3
Each curve corresponds to an environment and shows the ﬁtted
value as a function of the genotype eﬀects
©2001 CRC Press LLC

FIGURE 31.4
The top panel shows the linear part of the ﬁt, whereas the bottom
contains the jagged part
©2001 CRC Press LLC

FIGURE 31.5
The individual slopes b are shown against the eﬀects u of the
purely additive ﬁt
©2001 CRC Press LLC

FIGURE 31.6
This is the same as Fig. 31.1, with the three selected genotypes
highlighted
©2001 CRC Press LLC

32
On Distances and Measures of
Information: A Case of Diversity
Takis Papaioannou
U n i v e r s i t y o f P i ra e u s , G reece
ABSTRACT We critically review the state of the art. The concept
of statistical information, measures of information, their properties and
their use, impact, and role in inference are discussed. New properties
are presented along with recent applications and interconnections. The
existing large number of measures of information or divergence, the ap-
parent failure of uniﬁcation, i.e., coming up with the measure for all
cases, settings, concepts and properties and the multitude of applica-
tions ranging from the theory of estimation to Bayesian analysis lead
to the conclusion that measures of information will remain a case of
diversity. The concept of information will continue to be intangible.
Keywords and phrases Measure of information, measure of diver-
gence, maximum entropy principle, measure of dependence
32.1
Introduction
Distances or divergences and measures of information (m.o.i.s) are of
fundamental importance in statistics. They have been in existence and
use for many years, but still their role in statistics is marginal, mysterious
or even magical. They deal with the concept of information, statistical
information, they are functionals of distributions or densities and, at
other occasions, they express the distance or divergence between two
distributions or probability measures or even the aﬃnity or similarity
between several distributions.
There are many m.o.i.s, places, and frameworks in which they appear.
Attempts have been made by the author, his coworkers, and other re-
searchers to present a uniﬁed theory of statistical information and the
result may be considered a partial success.
©2001 CRC Press LLC

While information is a basic and fundamental concept in statistics,
there is no universal agreement on how to deﬁne and measure it in a
unique way. In statistics we have Fisher information, Kullback–Leibler
information, entropy information, Lindley information, and Jaynes’s max-
imum entropy principle. We also have the Akaike information criterion.
There is prior and posterior information, weight of evidence, mutual
information, uncertainty etc. Since statistical information cannot be de-
ﬁned mathematically, several statements have been used to delimit the
concept. For details and related references see the review articles by
Kendall (1973), Csiszar (1977), Papaioannou (1985), Aczel (1986), Sooﬁ
(1994), and Pardo (1999) as well as Kullback (1959), Papaioannou and
Kempthorne (1970), and Ferentinos and Papaioannou (1981).
All these concepts and m.o.i.s are interrelated, one comes from or may
be considered a special case of the other, etc. and statisticians disagree
on the geometric ﬁgure representing these informations and their inter-
relationships. Even review papers diﬀer among themselves depending
on the author’s perspective. Sooﬁ(1994) presents information–theoretic
statistics in terms of a pyramid whose top vertex is Shannon information
and the vertices of its base are Lindley, Kullback and Jaynes informa-
tions. We disagree with this representation. Jaynes did not introduce a
new m.o.i but he put forward the maximum entropy principle. We be-
lieve statistical information theory should be represented by a triangle
whose top vertex is Fisher information and base vertices are Shannon
and Kullback. Sooﬁconsiders Fisher’s information as a byproduct of
the Kullback-Leibler information. Fisher’s information is a historically
leading concept in statistics and has played and is playing an important
and major role in developing statistics. Entropy is a specialized concept
more appropriate to communication theory.
In this paper we review the state of the art in this area. In Section 32.2
we give a summary of m.o.i.s presenting mainly but brieﬂy new mea-
sures and commenting on their interrelationships and interpretations.
The properties, mainly statistically oriented properties, are discussed in
Section 32.3. New properties are collected which give ideas for further
research. The role of m.o.i.s in statistical inference is discussed in Section
32.4 with applications in Section 32.5.
Due to the vastness of the subject we cite the most pertinent papers
and the reader is referred to the articles given in these references.
©2001 CRC Press LLC

32.2
Measuring Information – Measures of
Information
Measures of information have been classiﬁed in three categories: (i)
Fisher-type, (ii) divergence-type and (iii) entropy-type.
We shall fol-
low the notation of Papaioannou (1985). Sometimes measures in the
ﬁrst two classes are called parametric and nonparametric respectively.
The representative measure in the ﬁrst class is the classical m.o.i in-
troduced by Fisher and given by
IF
X(θ) =

f(x, θ)
∂ln f(x, θ)
∂θ
2
dx
based on a gpdf f(x, θ), where θ is a univariate parameter.
The representative measure of the second class (divergence-type) is
the Kullback-Leibler given by
IKL
X (f1, f1) =

f1(x) ln[f1(x)/f2(x)]dx
and based on two gpdfs f1, f2. It is the diﬀerence between Kerridge’s
inaccuracy and Shannon’s entropy and it is also called relative or cross-
entropy.
An important measure is this class which produced a high degree of
uniﬁcation is Csiszar’s measure or ϕ-divergence
IC
X(f1, f2) =

ϕ(f1/f2)f2dx,
where ϕ is a real valued convex function on [0, ∞) with 0ϕ(0/0) = 0,
0ϕ(a/0) = aϕ∞, ϕ∞= lim[ϕ(u)/u] as u →∞.
For additional measures in these two classes see Papaioannou (1985)
and Pardo (1999).
In the last twenty years, additional divergence-type measures of in-
formation have been introduced in the literature.
One class of these
measures are functions (s-power functions) of the Hellinger distance of
order r between f1 and f2 or f1 and (f1 +f2)/2 and, in a sense, are gen-
eralizations of Renyi’s distance between these two distributions. They
are usually called uniﬁed (r, s)-divergences. Special values or limiting
cases of the parameters or indices of these measures yield some of the
previous and simpler measures, obtaining in this way a uniﬁcation in
the study of divergence-type measures of information. These measures
cannot be obtained from Csizar’s ϕ-divergence by a special choice of ϕ
[cf. Taneja (1989)]. Other m.o.i’s are the λ divergence and λ-mutual
©2001 CRC Press LLC

information based on Fererri’s hypoentropy
(1 + λ−1) ln(1 + λ−1) −(1/λ)

(1 + λf) ln(1 + λf)dx
[cf. Morales et al. (1993), etc.].
A group of Spanish researches led by L. Pardo introduced a further
generalization of ϕ-divergence, the so-called (h, ϕ)-divergence, aiming at
obtaining a higher degree of uniﬁcation (i.e., to include more m.o.i.s than
the Csiszar or (r, s)-divergences). They took a nonnegative real valued
C2 function h(·) with h(0) = 0 and evaluated it at the ϕ-divergence.
This new measure shares some of the properties and results of statistical
information but has not received wide application because of its com-
plexity and the fact that the user has to select not only ϕ but h as well
[cf. Morales et al. (1994) and Salicru et al. (1994)].
Another general measure, which is very closely related with the m.o.i.s
in this class (Renyi), has been studied in detail and received eminence
particularly for multinomial populations and for reasons of goodness of
ﬁt, is the Cressie and Read power divergence statistic
ICR
X (P, Q) = 2[λ(λ + 1)]−1 
Pi[(Pi/Qi)λ −1],
λ ∈R
[cf. Read and Cressie (1988)].
The representative measure for the third class is Shannon’s entropy or
diﬀerential entropy
H[f] = −Σpi ln pi,
H[f] = −

f(x) ln f(x)dx
for discrete and continuous distributions, respectively. Some authors use
H[f] in statistics as a measure of dispersion or concentration. A distri-
bution with large H[f] is less concentrated, thus it is more diﬃcult to
predict an outcome of X, than a distribution with small H[f]. Entropy
itself, among other things, has been used in statistics to measure infor-
mation in a realization of a r.v. X with gpdf f. Large values of H[f]
correspond to small amounts of information contained in an observed
value of X, so information in X is given by – H[f] [cf. Ebrahimi and
Sooﬁ(1990)].
There are many entropy-type m.o.i.s which have been produced by
entropy characterizations, i.e., by ﬁnding the mathematical function of
the pis satisfying variations or relaxations or generalizations of Shannon’s
entropy axioms [see, for example, Aczel (1980)]. We have not followed
these developments since we believe that, these m.o.i.s do not have much
relevance in statistical inference.
Another well-known index is the Gini–Simpson index Σp2
i , also called
information energy, associated with distribution (p1, . . . , pk) and hav-
ing interesting statistical applications [cf.
Pardo and Taneja (1990)].
©2001 CRC Press LLC

Pardo and his coworkers introduced another divergence, the generalized
R-divergence based on the concavity property of the (h, ϕ)-entropy. For
details see Pardo et al. (1993). A further generalization can be obtained
using weights [cf. Pardo (1999)].
The previous categorization is not absolute in the sense that a mea-
sure may originate from one class but, depending on the distribution(s)
involved or the mathematical operator applied upon it, it may end up
into another. For example, Fisher’s measure is a limiting case of the
Kullback-Leibler measure as one distribution converges to the other
parametrically.
There is a fourth class of measures which are diﬃcult to classify
distinctly in the previous three classes since they originate from the
Bayesian perspective. In this perspective there are many distributions
involved.
E.g., with the obvious notation, ξ(θ), p(x|λ), p(x), ξ(θ|x)
and their number increases as we discriminate between two models or
consider Bayes hierachical models, i.e., models in which the parame-
ters of the prior distributions are r.v.s themselves depending on other
parameters (hyperparameters) and so on [Goel (1983)]. M.o.i.s can be
constructed in diﬀerent ways using the previous Fisher, divergence and
entropy type m.o.i.s and the distributions involved. The most represen-
tative m.o.i. in this category is Lindley’s (1956) m.o.i
IL
X(ξ) = H(ξ) −E[H(ξ|X)]
which is always ≥0 as being the divergence between p(x, θ) = p(x|θ)ξ(θ)
and p(x)ξ(θ).
Recently a new m.o.i. almost identical with Fisher information has
made its presence in statistical literature. It is Fisher information with
diﬀerentiation taken with respect to the argument x of f rather than the
parameter θ. It is usually called Fisher information number
IF
X = E[(∂ln f(X)/∂x]2.
This number coincides with Fisher’s m.o.i. when θ is a location param-
eter and there remains an open question to explore its role and intere-
lationship with statistical information theory (total variation) [see, for
example, Thelen (1989), Papathanasiou (1993) and Kagan and Lands-
man (1997)].
A standard feature of most of the newly introduced m.o.i.s is that they
have a close relationship with the older (classical) ones. They coincide
with them for special choices of their parameters or of the functions h
and ϕ.
Their usefulness lies in the idea of uniﬁcation but is limited
from the practical point of view barring some statistical applications to
be discussed below. Some of the new m.o.i.s are outside the scope of
statistical information theory since the do not satisfy some of its basic
properties.
©2001 CRC Press LLC

32.3
Properties of Measures of Information
As stated before there are many properties of m.o.i.s. For a list of these
properties see Papaioannou (1985). We simply mention by name ﬁve
basic ones: nonnegativity, additivity, maximal information, invariance
under suﬃcient transformations and appearance in Cramer-Rao inequal-
ities.
Statisticians believe that more data means more information. This
is justiﬁed by the additivity, nonnegativity and maximal information
properties.
As research progresses new interesting properties are being added up
or other neglected ones call to be investigated. The previous list may be
completed as follows:
1. Let (Z, δ) be the pair of variables associated with a randomly cen-
sored experiment, where we observe X or Y (the censoring variable)
and Z = min(X, Y ), δ = I(X < Y ). Then we should have
(i) E[Information(X)] ≥E[Information(Z, δ)], for every X, Y (a
kind of maximal information property).
(ii) E[Information(Z1, δ1)] ≤E[Information(Z2, δ2)] for every X
with Y1 ≤st Y2, [information decreases as censoring increases
or equivalently I(X, Y1) ≤I(X, Y2)] [Hollander, Proschan,
and Sconing (1987)].
2. If f(x, θ), θ real, has a monotone likelihood ratio in x then
θ1 < θ2 < θ3 →I(Fθ1, fθ2) < I(fθ1, fθ3)
[Ali and Silvey (1974)].
3. Additivity for hierarchical distributions: if f1 ⊂f2 ⊂f3 then
I(f1, f3) = I(f1, f2) + I(f2, f3).
4. Lindley’s concavity and monotonicity in n: Let X1, X2, . . . be i.i.d.
r.v.s; then IXn,Xn+1 is concave in n and increasing in n [Lindley
(1956)].
5. If f1 = p(x, θ) and f2 = p(x)ξ(θ), where p and ξ are gpdfs, then
I(f1, f2) is concave in ξ(θ) [Lindley (1956)].
6. Convexity of I(f1, f2) in either f1 or f2 [Csiszar (1975)].
7. Relationship with Type II error rate: log2 βn = −nI(f1, f2) for
large n, where βn is the Type II error rate in testing f1 versus f2.
8. Markov chain inequalities:
(i) if X1, X2, X3) is a Markov chain, then
I(f3(x3), f3|1(x3|x1)) ≤I(f2(x2), f2|1(x2|x1))
©2001 CRC Press LLC

[Goel (1983)],
(ii) if (X1, X2, X3, X4) is a Markov chain then
I(f1(x1), f4(x4)) ≤I(f2(x2), f3(x3))
[Csiszar (1977)].
9. The information generating function: a function whose derivatives
produce m.o.i.s [Guiasu and Reischer (1985)].
Do existing measures or which m.o.i.s satisfy the above or which of
the above properties and what are their implications to the foundations
and inference? This remains to be investigated.
32.4
Measures of Information and Inference
Measures of information have been and are extensively used in statistical
inference, mainly in testing statistical hypotheses developing primarily
tests of signiﬁcance based on asymptotic theory.
In the theory of point estimation, Fisher’s m.o.i. plays a dominant
role since it appears in Cramer-Rao type inequalities. Of course there
are some limitations since the Cramer-Rao lower bound is applicable for
regular families of distributions. We made some attempts to measure
information about θ contained in the data for broader families of distri-
butions including non regular ones but the results were of limited scope
since the new parametric measures lacked simplicity and were limiting
cases of divergence-type m.o.i.s [see Ferentinos and Papaioannou (1981)].
This led us to conjecture that Fisher’s m.o.i. is unique up to a constant.
Some years ago we thought to use m.o.i.s as loss functions and produce
information optimum (I-optimum) estimators, i.e., estimators minimiz-
ing the risk
E(I(fθ, fˆθ))
for all estimators ˆθ belonging to a certain class uniformly in θ.
We
obtained results analogous to the results of decision theory on point es-
timation, but these results relied heavily on the convexity of I(fθ, fθ∗)
in θ∗for all θ and we did not pursue the topic further. It is well known
that, for exponential families of distributions, maximum likelihood esti-
mators ˆθn of θ converge almost surely to θ∗, the value which minimizes
over Θ the Kullback-Leibler divergence IKL(f0, f(x, θ)) between f0, the
true distribution of the data, and f(x, θ) [cf. McCulloch (1988)]. Now
divergence-type m.o.i.s have been used in the general theory of estima-
tion as loss functions to introduce minimum distance/divegrence estima-
tiors and obtain generalizations of classical results such as consistency,
©2001 CRC Press LLC

asymptotic normality, etc. [see Papaioannou (1971), Morales, Pardo and
Vajda (1995), Barron and Hengartner (1998), Cutler and Cordero-Brana
(1996), and Hartigan (1998)].
A promising application of m.o.i.s in Bayesian estimation is to use
divergence-type measures to determine the prior needed in Bayesian
analysis so that the Bayes estimator is as close possible in terms of
divergence with the maximum likelihood estimator.
In testing statistical hypotheses, m.o.i.s have been used to develop
tests of signiﬁcance. The idea is to consider a sample estimator of the
m.o.i and derive its asymptotic distribution (usually normal or χ2).
Initially sample estimates of entropy both ﬁnite and diﬀerential, were
obtained and their asymptotic distributions were established [see, for ex-
ample, Nayak (1985)]. The asymptotic distribution of ˆH is normal. Then
sample estimates of entropy and divergence-type m.o.i.s were examined.
In all cases the sample statistics were evaluated exactly or approximately.
The asymptotic distribution of I(f1, f2) was considered in two sampling
cases: one random sample from f1 with f2 known and two independent
samples one from each fi. Also two model cases were considered: f1
and/or f2 to be either multinomial or general parametric distributions.
In all cases the asymptotic results lead to tests of signiﬁcance for good-
ness of ﬁt, tests of equality of divergence, tests of independence, tests of
homogeneity in contingency tables under random or stratiﬁed random
sampling with proportional allocation and independence among strata,
tests of homogeneity of variances, tests of partial homogeneity as, for
example, in two multivariate normal populations, etc. [cf. Pardo et al.
(1993), Morales et al. (1994), Salicru et al. (1994), and Pardo et al.
(1995)]. Analogous results for estimates of Fisher information do not
exist.
The results may representatively be summarized as follows. We shall
use Cziszar’s ϕ divergence as the most general and widely accepted uni-
fying m.o.i and multinomial or product multinomial populations.
1. For a single multinomial population Mult(N, p1, . . . , pk)
√
N[IC( ˆP, Q) −IC(P, Q)]/ˆσc
L→N(0, 1)
2N[IC( ˆP, Q) −ϕ(1)]/ϕ′′(1)
L→χ2
k−1
if P = Q
where ˆP is the sample estimate of P = (p1, . . . , pk), Q = (q1, . . . , qk)
is a known distribution and for ˆσ2
c, see Zografos, Ferentinos and Pa-
paioannou (1990).
2. In rxc multinomial tables with Π = (πij) and P = (pij) the cell
and sample probabilities and Π0 = (πi.π.j.), P0 = (pi.p.j)
©2001 CRC Press LLC

√
N[IC(P, P0) −IC(Π, Π0)]
L→N(0, σ2)
2N[IC(P, P0) −IC(Π, Π0)]
L→Linear combination of χ2
1
2N[IC(P, P0) −ϕ(1)]/ϕ′′(1)
L→χ2
(r−1)(c−1)|H0 : πij = πiπ.j,
provided that ϕ satisﬁes some smoothness conditions and for σ2
[cf. Zografos (1993)].
3. For two independent multinomial populations M(N, p1, . . . , pk),
M(M, q1, . . . , qk)
√
N + M[IC( ˆP, ˆQ) −IC(P, Q)]
L→N(0, σ2)
2MN(M + N)−1[IC( ˆP, ˆQ) −ϕ(1)]/ϕ′′(1)
L→χ2
k−1|P = Q,
i = 1, . . . , k, [cf. Zografos (1998)].
Similar results exist for general parametric distributions and for other
divergence-type m.o.i.s or dissimilarities and can be found in Salicru et
al. (1994). They are based on the old results by Kupperman (1957)
for the Kullback-Leibler divergence and have wide applications. More-
over similar asymptotic results hold for the estimated Rao distance or
geodesic or Riemannian distance between two parametric distributions
which is induced from the Fisher information matrix and is a generaliza-
tion of Mahalanobis distance [cf. Jensen (1993)]. Optimality of goodness
of ﬁt tests based on divergences in terms of Pitman and Bahadur eﬃcien-
cies has also been investigated [cf. Read and Cressie (1988), Zografos,
Ferentinos, and Papaioannou (1990), and Pardo (1999)].
32.5
Applications
As it is evident from the above discussion, the applications of m.o.i.s are
numerous. We shall only mention a few topics such as Bayesian infor-
mation theory, diﬀerential geometry and information, observed Fisher
information and EM algorithm, maximum entropy principle, measures
of dependence, etc. and refer the reader to the papers given below and
the references cited therein. In addition we mention selectively by ti-
tle two areas where we have used m.o.i.s to measure information and
to introduce and study new statistical models respectively: (i) random
censoring, quantal random censoring with random or not recording of un-
censored observations and total information [cf. Tsairidis et al. (1996,
©2001 CRC Press LLC

2000)] and (ii) association and symmetry/asymmetry models for contin-
gency tables based on f-divergence which are the closest to independence
or symmetry [cf. Kateri and Papaioannou (1994, 1997)].
32.6
Conclusions
To seek a measure satisfying all (reasonable) properties is unrealistic.
Statisticians have not agreed on the set of reasonable properties. M.o.i.s
will continue to play an important but diversiﬁed role in statistics and
the pattern of research is expected to continue as in the last ﬁfteen
years with special emphasis however in Bayesian applications. M.o.i.s
provide tools to prove results or give models for inference but the case to
yield a superior methodology than the classical one remains to be seen.
Information remains an intangible concept.
Acknowledgement Most of this work was done while the author was
at the University of Ioannina, Greece.
References
1. Aczel, J. (1986). Characterizing information measures: Approach-
ing the end of an era. In Uncertainty in Knowledge-Based Systems.
Lecture Notes in Computer Science (Eds., B. Bouchon and R. R.
Yager), pp. 359–384. Springer-Verlag, New York.
2. Ali, S. M. and Silvey, S. D. (1966). A general class of coeﬃcients of
divergence of one distribution from another. Journal of the Royal
Statistical Society, Series B 28, 131–142.
3. Barron, A. and Hengartner, N. (1998).
Information theory and
super eﬃciency. Annals of Statistics 26, 1800–1825.
4. Csiszar, I. (1977). Information measures: A critical review. Trans-
actions of the 7th Prague Conference on Information Theory, Sta-
tistical Decision Functions and Random Processes, pp. 73–86, Prague,
1974, Academia.
5. Cutler, A. and Cordero-Brana, O. I. (1996). Minimun Hellinger
distance estimation for ﬁnite mixture models. Journal of the Amer-
ican Statistical Association 91, 1716–1723.
6. Ebrahimi, N. and Sooﬁ, E. S. (1990). Relative information loss
under Type II censored experimental data. Biometrika 77, 429–
©2001 CRC Press LLC

435.
7. Ferentinos, K. and Papaioannou, T. (1981). New parametric mea-
sures of information. Information and Control 51, 193–208.
8. Guiasu, S. and Reischer, C. (1985). The relative information gen-
erating function. Information and Sciences 35, 235–241.
9. Hartigan, J. A. (1998). The maximum likelihood prior. Annals of
Statistics 26, 2083–2103.
10. Hollander, M., Proschan, F., and Sconing, J. (1987). Measuring
information in the right-censored models. Naval Research Logistics
34, 669–687.
11. Jensen, U. (1993). Derivation and calculation of Rao distances:
a review.
Technical Report 71/1993.
Institut fur Statistik und
Okonometrie der Universitat Kiel.
12. Kagan, A. and Landsman, Z. (1997). Statistical meaning of Carlen’s
superadditivity of the Fisher information. Statistics & Probability
Letters 32, 175–179.
13. Kateri, M. and Papaioannou, T. (1994). f-divergence association
models. International Journal of Mathematical and Statistical Sci-
ences 3, 179–203.
14. Kateri, M. and Papaioannou, T. (1997).
Symmetry and asym-
metry models for rectangular contingency tables. Journal of the
American Statistical Association 92, 1124–1131.
15. Kendall, M. G. (1973). Entropy, probability and information. In-
ternational Statistical Review 41, 59–68.
16. Kullback, S. (1959). Information Theory and Statistics. John Wi-
ley & Sons, New York.
17. Kupperman, M. (1957). Further Applications of Information The-
ory to Multivariate Analysis and Statistical Inference. Ph.D. dis-
sertation, George Washington University, Washington, D.C.
18. Lindley, D. V. (1956). On a measure of the information provided
by an experiment. Annals of Mathematical Statistics 27, 986–1005.
19. Morales, D., Pardo, L., Salicru, M., and Menendez, M. L. (1993).
The λ-divergence and the λ-mutual information: Estimation in
stratiﬁed sampling. Journal of Computational and Applied Math-
ematics 47, 1–10.
20. Morales, D., Pardo, L., Salicru, M., and Menendez, M. L. (1994).
Asymptotic properties of divergence statistics in stratiﬁed random
sampling and its application to test statistical hypotheses. Journal
of Statistical Planning and Inference 38, 201–223.
©2001 CRC Press LLC

21. Morales, D., Pardo, L., and Vajda, I. (1995). Asymptotic diver-
gence of estimates of discrete distributions. Journal of Statistical
Planning Inference 48, 347–369.
22. Nayak, T. K. (1985).
On diversity measures based on entropy
functions. Communications in Statistics — Theory and Methods
14, 203–215.
23. Papaioannou, T. (1971). A criterion of estimation based on mea-
sures of information, Unpublished manuscript, University of Geor-
gia. Abstract, Annals of Mathematical Statistics 42, 2179.
24. Papaioannou, T. (1985). Measures of information. In Encyclopedia
of Statistical Sciences 5 (Eds., S. Kotz and N. L. Johnson), pp.
391–397. John Wiley & Sons, New York.
25. Papaioannou, T. and Kempthorne, O. (1971). On Statistical In-
formation Theory and Related Measures of Information. Techni-
cal Report No. ARL. 71-0059, Aerospace Research Laboratories,
Wright-Patterson A.F.B., Ohio.
26. Papathanasiou, V. (1993). Some characteristic properties of the
Fisher information matrix via Cacoullos-type inequalities. Journal
of Multivariate Analysis, 44, 256-265.
27. Pardo, L. (1999).
Generalized divergence measures:
statistical
applications.
In Encyclopedia of Microcomputers, pp. 163–191.
Marcel-Dekker, New York.
28. Pardo, L., Morales, D., Salicru, M., and Menendez, M. L. (1993).
The ϕ-divergence statistic in bivariate multinomial populations in-
cluding stratiﬁcation. Metrika 40, 223–235.
29. Pardo, L., Salicru, M., Menendez, M. L., and Morales, D. (1995).
A test for homogeneity of variances based on Shannonss entropy.
Metron 53, 135–146.
30. Pardo, L. and Taneja, I. J. (1991).
Information energy and its
applications.
Advances in Electronics and Electron Physics 90,
165–241.
31. Read, T. R. C. and Cressie, N. A. C. (1988).
Goodness-up-Fit
Statistics for Discrete Multivariate Data.
Springer-Verlag, New
York.
32. Salicru, M., Morales, D., Menendez, M. L., and Pardo, L. (1994).
On the applications of divergence type measures in testing statis-
tical hypotheses. Journal of Multivariate Analysis 51, 372–391.
33. Sooﬁ, E. S. (1994). Capturing the intangible concept of informa-
tion. Journal of the American Statistical Association 89, 1243–
1254.
©2001 CRC Press LLC

34. Taneja, I. J. (1989).
On generalized information measures and
their applications. Advances in Electronics and Electron Physics
76, 327–413.
35. Thelen, B. J. (1989). Fisher information and dichotomies in equiv-
alence/contiguity. Annals of Statistics 17, 1664–1690.
36. Tsairidis, Ch., Ferentinos, K., and Papaioannou, T. (1996). Infor-
mation and random censoring. Information Sciences 92, 159–174.
37. Tsairidis, Ch., Zografos, K., Ferentinos, K., and Papaioannou, T.
(2000). Information in quantal response data and random censor-
ing. Annals of the Institute of Statistical Mathematics (to appear).
38. Zografos, K. (1993). Asymptotic properties of ϕ-divergence statis-
tic and its applications in contingency tables. International Journal
of Mathematical and Statistical Sciences 2, 5–21.
39. Zografos, K. (1998). f-dissimilarity of several distributions in test-
ing statistical hypotheses.
Annals of the Institute of Statistical
Mathematics 50, 295–310.
40. Zografos, K., Ferentinos, K., and Papaioannou, T. (1990).
ϕ-
diverg-ence statistics: sampling properties and multinomial good-
ness of ﬁt and divergence tests. Communications in Statistics —
Theory and Methods 19, 1785–1802.
©2001 CRC Press LLC

33
Representation Formulae for Probabilities
of Correct Classiﬁcation
Wolf-Dieter Richter
University of Rostock, Rostock, Germany
ABSTRACT Representation formulae for probabilities of correct clas-
siﬁcation are derived for a minimum-distance type decision rule. These
formulae are essentially based upon the two-dimensional Gaussian distri-
bution and allow new derivations of recent results in Krause and Richter
(1999).
Keywords and phrases Generalized minimum-distance rule, repeated
measurements, two-dimensional decision space, two-dimensional repre-
sentation formulae, doubly noncentral F-distribution, linear model ap-
proach
33.1
Introduction
Let an individual having a Gaussian distributed feature variable belong
to one of two distinct populations.
Assume that we are given mea-
surements from a sample of individuals giving rise to independent and
identically distributed Gaussian feature variables. Several methods of
allocating the individual or the whole sample to one of the two pop-
ulations have been studied in the literature.
For an introduction to
this area see, e.g., Anderson (1984) or McLachlan (1992).
A certain
subclass of classiﬁcation rules is given by the so-called distance rules.
Because of the great variety of distances existing in statistics, there are
several approaches to distance based classiﬁcation rules, as only to men-
tion Cacoullos and Koutras (1985), Cacoullos (1992), and Cacoullos and
Koutras (1996). When choosing a method for classifying an individual
or a sample of individuals, one has to distinguish between the cases of
known or unknown moments.
Certain sample-distance based classiﬁ-
cation rules, however, work without assumptions concerning the second
©2001 CRC Press LLC

order moments and probabilities of correct classiﬁcation can be described
explicitly in terms of these moments. This advantage has been exploited
to some extent recently in Krause and Richter (1999). The method devel-
oped there combines a geometric sample measure representation formula
for the multivariate Gaussian measure with a certain non classic linear
model approach due to Krause and Richter (1994). This linear model
type approach will be modiﬁed in the present paper to derive new rep-
resentation formulae for probabilities of correct classiﬁcation which are
based upon the two-dimensional Gaussian law. Further transformation
of these formulae yields expressions in terms of the doubly noncentral
F-distribution as has been derived recently in another way in Krause
and Richter (1999).
Let n1, n2 and n3 observations belong to the three populations Π1, Π2,
and Π3, respectively. We suppose that Π1 and Π2 are distinguishable
with respect to their expectations and that population Π3 can be under-
stood as a copy of one of Π1 or Π2. The overall sample vector
Y(n) =

Y 1T
(n1 ), Y 2T
(n2 ), Y 3T
(n3 )
T
,
n = n1 + n2 + n3,
consisting of the repeated measurement vectors Y i
(ni ) = (Yi1, . . . , Yini)T
from the three populations Πi, i = 1, 2, 3 will be assumed to deﬁne a
Gaussian statistical structure
Sn = (Rn, Bn, {Φµ,Σ , µ ∈M, Σ ∈Θ}).
Here, M denotes the range of EY(n) and will be called the model space,
although it is not a linear vector space. It satisﬁes the representation
M = {µ ∈Rn : µ = µ11+00 + µ210+0 + µ3100+,
µ3 ∈{µ1, µ2}, (µ1, µ2) ∈R2, µ1 ̸= µ2},
where
1+00 = (1T
n1OT
n2 +n3)T , 10+0 = (0T
n11T
n20T
n3)T , 100+ = (0T
n1 +n21T
n3)T ,
1ni = (1, . . . , 1)T ∈Rni, 0m = (0, . . . , 0)T ∈Rm.
Further,
Θ =


Σ =


σ2
1In1
σ2
2In2
σ2
3In3

, (σ2
1, σ2
2) ∈R+ × R+, σ2
3 ∈{σ2
1, σ2
2}



is a set of block diagonal matrices where Ini denotes a ni x ni unit
matrix.
The problem of interest here is, on the basis of the overall sample
vector Y(n), to decide between the hypotheses
H1/3 : µ3 = µ1 and H2/3 : µ3 = µ2.
©2001 CRC Press LLC

33.2
Vector Algebraic Preliminaries
Put
1+0+ = 1+00 + 100+, 10++ = 10+0 + 100+, 1+++ = 1+00 + 10+0 + 100+
and denote by
M1/3 = L(1+0+, 10+0) and M2/3 = L(10++, 1+00)
subspaces of the sample space spanned up by the vectors standing within
the brackets. These spaces can be understood as hypotheses spaces or
restricted model spaces under the hypotheses H1/3 or H2/3, respectively.
Second basis representations for these spaces are
M1/3 = L(1+++, 10+0),
M2/3 = L(1+++, 1+00).
Note that M1/3 and M2/3 are not orthogonal to each other,
M1/3 ∩M2/3 = L(1+++)
and
10+0⊥1+00.
While the dimensions of the hypotheses spaces satisfy the equations
dim Mi/3 = 2, i = 1, 2
the dimension of

M = L(1+00, 10+0, 100+),
i.e.
the smallest subspace of the sample space containing both M1/3
and M2/3, equals three. A second basis representation for this so called
extended model space is

M = L(1+++, 1+00, 10+0).
The spaces L(1+++) and L(1+00, 10+0) are linearly independent but not
orthogonal. A third basis representation for 
M is

M = L(1+++, 1−0+, 10−+),
where
1−0+ = −1
n1
1+00 + 1
n3
100+, 10−+ = −1
n2
10+0 + 1
n3
100+.
©2001 CRC Press LLC

The spaces L(1+++) and L(1−0+, 10−+) are orthogonal but
(1−0+, 10−+) = 1
n3
.
(33.2.1)
Since
Π1−0+µ = (µ3 −µ1)1−0+ and Π10−+µ = (µ3 −µ2)10−+,
the two-dimensional space
W = L(1−0+, 10−+)
will be called eﬀect space or decision space. These notations correspond
to the circumstances that changes of the diﬀerences (µ3−µi), i = 1, 2 are
immediately reﬂected in the space W and decisions concerning the mag-
nitude of these diﬀerences should be based upon considerations within
this space. This can be taken as motivation to deﬁne the decision rules
dc|Rn −→{1, 2}, c > 0
for deciding between the hypotheses H1/3 and H2/3 as
dc(y(n)) = 2 −I{∥Π1−0+y(n)∥< c∥Π10−+y(n)∥}
(33.2.2)
for arbitrary c > 0. Here, I(A) denotes the indicator of the random
event A. Notice that
dc(Y(n)) = 1
holds iﬀ
∥Π1−0+ΠW Y(n)∥< c∥Π10−+ΠW Y(n)∥.
Recognize further that
d1(Y(n)) = 2 −I{∥Y(n) −ΠM1/3Y(n)∥< ∥Y(n) −ΠM2/3Y(n)∥}.
Hence the geometrically motivated decision rule dc will be called, through-
out the present paper, a generalized minimum-distance classiﬁcation
rule.
Let us now consider the orthogonal projection of µ onto the eﬀect
space W, ΠW µ.
Note that µ ∈M ⊂
M = L(1+++, W) and W⊥1+++. Hence, ΠW µ =
µ −Π1+++µ, i.e.
ΠW µ = a(µ1, µ2, µ3)1+00+b(µ1, µ2, µ3)10+0+c(µ1, µ2, µ3)100+ (33.2.3)
with
©2001 CRC Press LLC

n · a(x, y, z) = n2(x −y) + n3(x −z),
n · b(x, y, z) = n1(y −x) + n3(y −z),
n · c(x, y, z) = n1(z −x) + n2(z −y).
It is easily seen from this 
M–basis representation of ΠW µ that the co-
eﬃcients must depend on each other. We shall therefore try to reduce
the number of parameters included in the model. To this end we start
with a ﬁrst reparametrisation. This reparametrisation is based upon a
certain partial orthogonalisation.
LEMMA 33.2.1
ΠW µ = −n1a(µ1, µ2, µ3)1−+0 −n2b(µ1, µ2, µ3)10−+,
Π1+++µ = n1µ1 + n2µ2 + n3µ3
n1 + n2 + n3
1+++.
The proof of the second assertion is obvious. The ﬁrst assertion follows
immediately from the following lemma. Notice that, e.g., the dimension-
depending new parameter
m(µ1, µ2, µ2) = 1
n
3

i=1
µini
does not allow an immediate interpretation in the original problem.
LEMMA 33.2.2
The 
M–vector
u = a1+00 + b10+0 + c100+
(33.2.4)
belongs to the subspace W and allows the representation
u = ϕ1−0+ + ψ10−+
(33.2.5)
for some (ϕ, ψ) ∈R2 if and only if
an1 + bn2 + cn3 = 0.
(33.2.6)
In this case,
ϕ = −n1a,
ψ = −n2b.
(33.2.7)
PROOF Replacing the two vectors in (33.2.5) by their deﬁnitions yields
u = −ϕ
n1
1+00 −ψ
n2
10+0 + ϕ + ψ
n3
100+.
©2001 CRC Press LLC

Equating coeﬃcients from the latter formula with corresponding coeﬃ-
cients from (33.2.4) gives (33.2.7) and
ϕ + ψ
n3
= −n1a + n2b
n3
.
The latter quantity coincides with the coeﬃcient c if and only if condition
(33.2.6) is fulﬁlled.
Let us deﬁne by
Y i· = 1
ni
ni

j=1
Yij
the mean in the i-th population, i = 1, 2, 3 and by m(Y 1·, Y 2·, Y 3·) the
overall mean. It follows then that
Π1+++Y = m(Y 1·, Y 2·, Y 3·)1+++
and
Π 
MY = Y 1·1+00 + Y 2·10+0 + Y 3·100+.
Hence, an

M–basis representation for ΠW Y , i.e. for Π 
MY −Π1+++Y
is
ΠW Y = a(Y 1·, Y 2·, Y 3·)1+00+b(Y 1·, Y 2·, Y 3·)10+0+c(Y 1·, Y 2·, Y 3·)100+.
The last three equations deﬁne 
M–basis representations of the least
squares estimations for the quantities Π1+++µ, Π 
Mµ and ΠW µ, respec-
tively.
COROLLARY 33.2.3
Using the above deﬁned functions a and b, a W-basis representation for-
mula for ΠW Y is given by
ΠW Y = −n1a(Y 1·, Y 2·, Y 3·)1−0+ −n2b(Y 1·, Y 2·, Y 3·)1−0+.
Note that
W = L(b1, b2)
with
b1 = 1−0+ and b2 =
1
n1 + n3
1+0+ −1
n2
10+0
(33.2.8)
deﬁnes an orthogonal basis representation for W where
b2 = 10−+ −Π1−0+10−+.
©2001 CRC Press LLC

From (33.2.1) and
∥1−0+∥2 = n1 + n3
n1n3
(33.2.9)
we get
Π1−0+10−+ =
n1
n1 + n3
1−0+.
Hence,
b2 = −1
n2
10+0 + 1
n3
100+ +
1
n1 + n3
1+00 −
n1
n3(n1 + n3)100+.
This yields the second assertion in (33.2.8). The following lemma presents
a reparametrisation which is based upon orthogonalisation.
LEMMA 33.2.4
The quantity ΠW µ can be written as
ΠW µ =
n1n3
n1 + n3
(µ3 −µ1)b1 + d(µ1, µ2, µ3)b2,
whereby the new parameter d satisﬁes the two representation formulae
nd(µ1, µ2, µ3) = n1n2(µ1 −µ2) + n2n3(µ3 −µ2),
(33.2.10)
and
nd(µ1, µ2, µ3) = n2(n1 + n3)

m(1/3)(µ1, µ3) −µ2

(33.2.11)
with
m(1/3)(x, y) = n1x + n3y
n1 + n3
.
PROOF Making use of equations (33.2.3) and (33.2.7) above as well as
(33.2.12) below one can see that
u = ΠW µ = ΠW (µ11+00 + µ210+0 + µ3100+)
allows the representation
u = ϑb1 + νb2
with
ϑ = −n1a(µ1, µ2, µ3) −
n1n2
n1 + n3
b(µ1, µ2, µ3)
= n1n2(µ2 −µ1) + n1n3(µ3 −µ1)
n
+n2
1n2(µ1 −µ2) + n1n2n3(µ3 −µ2)
n(n1 + n3)
©2001 CRC Press LLC

and
ν = ψ = −n2b(µ1, µ2, µ3) = n2n1(µ1 −µ2) + n2n3(µ3 −µ2)
n
.
Hence
n(n1 + n3)ϑ = (n1 + n3)[n1n2(µ2 −µ1) + n1n3(µ3 −µ1)]
+n2
1n2(µ1 −µ2) + n1n2n3(µ3 −µ2)
= n1n3n(µ3 −µ1)
and
nν = n2(n1 + n3)
n1µ1 + n3µ3
n1 + n3
−µ2

,
which proves the assertions of the lemma.
Observe that since the new parameters d and m(1/3)(µ1, µ3) depend
on the sample sizes they do not allow immediate interpretations with
respect to the original problem.
LEMMA 33.2.5
The 
M–vector u from (33.2.4) belongs to the subspace W and allows the
representation
u = ϑb1 + νb2
(33.2.12)
for certain values of (ϑ, ν) ∈R2 if and only if the condition (33.2.6) is
satisﬁed. In this case we have, with (ϕ, ψ) from (33.2.7),
ϑ = ϕ +
n1
n1 + n3
ψ, ν = ψ.
PROOF Equating coeﬃcients, (33.2.12) follows from
a1+00 + b10+0 + c100+ = u = ϑb1 + νb2
= ϑ

−1
n1
1+00 + 1
n3
100+

+ ν

1
n1 + n3
1+0+ −1
n2
10+0

=

−ϑ
n1
+
ν
n1 + n3

1+00 −ν
n2
10+0 +
 ϑ
n3
+
ν
n1 + n3

100+.
In the same way it follows that
c = ϑ
n3
+
ν
n1 + n3
= −n1
n3
a −
n1n2
n3(n1 + n3)b −
n2
n1 + n3
b,
which is equivalent to ( 33.2.6).
©2001 CRC Press LLC

By deﬁnition of b1,
Πb1Y =
n1n3
n1 + n3
(Y 3· −Y 1·)b1.
Using the notation
Y (1/3)
..
=
1
n1 + n3

i∈{1,3}
ni

j=1
Yij = m(1/3)(Y 1·, Y 3·)
for the pooled mean of the values from the union of the ﬁrst and third
subsamples Y 1
(n1) and Y 3
(n3), we get
Πb2Y = d(Y 1·, Y 2·, Y 3·)b2
= 1
n(n1n2(Y 1· −Y 2·) + n2n3(Y 3· −Y 2·))b2
= n2
n (n1 + n3)

Y (1/3)
..
−Y 2·

b2.
Hence we arrive at least squares estimates for Πb1µ and Πb2µ.
With
∥b1∥=
n1 + n3
n1 · n3
and ∥b2∥=
n1 + n2 + n3
n2(n1 + n3)
(33.2.13)
we get the following representation formula for the least squares estimate
of ΠW µ with respect to the normalized orthogonal basis {B1, B2} where
Bi = bi/∥bi∥:
ΠW Y =
 n1n3
n1 + n3
(Y 3· −Y 1·)B1 +

n2(n1 + n3)
n1 + n2 + n3

Y (1/3)
..
−Y 2·

B2 .
(33.2.14)
33.3
Distributional Results
33.3.1
Representation Formulae Based upon the
Two-Dimensional Gaussian Law
The random variables Y 3· −Y 1· and Y (1/3)
..
−Y 2· play an essential role
in the basic formula (33.2.14). They are coeﬃcients of the projections
of Y(n) onto the normalized orthogonal basis vectors B1 and B2 of the
decision space W and are therefore uncorrelated. They are linear com-
binations of the components of the Gaussian random vector Y(n) and
©2001 CRC Press LLC

are therefore jointly Gaussian distributed and consequently independent
from each other.
Let η1, η2, . . . denote independent standard Gaussian distributed ran-
dom variables. The symbol
Xi ∼Ni(ai, λ2
i )
will be used to indicate
that Xi follows the same distribution law as ai + λiηi . Thus
 n1n3
n1 + n3
(Y 3· −Y 1·) ∼N1
 n1n3
n1 + n3
(µ3 −µ1), n1σ2
3 + n3σ2
1
n1 + n3

.
(33.3.1)
The ﬁrst and second order moments of this distribution will be denoted
by ξ1 and δ2
1, respectively.
In a similar way as above, one can show
that the random variables Y (1/3)
..
und Y 2· are independent. Using the
notations
Y 2· ∼N3

µ2, σ2
2
n2

and
Y (1/3)
..
∼N4

m(1/3)(µ1, µ3), n1σ2
1 + n3σ3
2
(n1 + n3)2

,
we get

n2(n1 + n3)
n1 + n2 + n3
(Y (1/3)
..
−Y 2·) ∼N2(ξ2, δ2
2)
(33.3.2)
with
ξ2 =

n2(n1 + n3)
n1 + n2 + n3
(m(1/3)(µ1, µ3) −µ2)
= n1n2(µ1 −µ2) + n3n2(µ3 −µ2)

(n1 + n2 + n3)n2(n1 + n3)
and
δ2
2 = (n1 + n3)2σ2
2 + n2(n1σ2
1 + n3σ2
3)
(n1 + n2 + n3)(n1 + n3)
.
Recall that
dc(Y(n)) = 1
holds if and only if
N1B1 + N2B2 ∈{z ∈W : ∥Π1−0+z∥< c∥Π10−+z∥}.
(33.3.3)
The following lemma concerns reducing dimension and norming.
©2001 CRC Press LLC

LEMMA 33.3.1
The relation
N1B1 + N2B2 ∈{z ∈W : ∥Π1−0+z∥< c ∥Π10−+z∥}
is true iﬀ
N1
δ1
, N2
δ2
T
∈

(t1, t2)T ∈R2 :
|t1|
|t1 + κt2| < ζ · c

,
where
κ =

n3[(n1 + n3)2σ2
2 + n2(n1σ2
1 + n3σ2
3)]
n1n2(n1σ2
3 + n3σ2
1)
(33.3.4)
and
ζ =

n1n2
(n1 + n3)(n2 + n3) =
1

(1 + n3/n1)(1 + n3/n2)
.
(33.3.5)
PROOF Put A = {z ∈W : ∥Π1−0+z∥/∥Π10−+z∥< c}. Then
A =

z1B1 + z2B2 : ∥Π1−0+(z1B1 + z2B2)∥
∥Π10−+(z1B1 + z2B2)∥< c, (z1, z2)T ∈R2

.
With
(B1, 10−+) =

n1
n3(n1 + n3)
and
(B2, 10−+) =
n1 + n2 + n3
n2(n1 + n3)
it follows that
A = {z1B1 + z2B2 : |z1|/|z1 + χ · z2| < ζ · c, (z1, z2)T ∈R2},
where
χ =

n3(n1 + n2 + n3)/√n1n2.
Hence,
N1B1 + N2B2 ∈A
iﬀ
(N1, N2)T ∈

(z1, z2)T ∈R2 : |z1|/|z1 + χz2| < ζ · c

.
The assertion of the lemma follows now with κ = (δ2/δ1)χ.
©2001 CRC Press LLC

Put
ν1 = ξ1
δ1
=

n1n3
n1σ2
3 + n3σ2
1
(µ3 −µ1)
(33.3.6)
and
ν2 = ξ2
δ2
=
√n2(n1 + n3)[m(1/3)(µ1, µ3) −µ2]

(n1 + n3)2σ2
2 + n2(n1σ2
1 + n3σ2
3)
=
n1(µ1 −µ2) + n3(µ3 −µ2)

(n1 + n3)2σ2
2 + n2(n1σ2
1 + n3σ2
3)
.
(33.3.7)
If the hypothesis H1/3 is true, then m(1/3)(µ1, µ3) = µ1,
ν1 = 0
(33.3.8)
and
ν2 = √n2
µ1 −µ2
σ2

1 + n3/n1
1 + n3/n1 + (n2σ2
1)/(n1σ2
2)
=
√n1 + n3(µ1 −µ2)

(n1 + n3)σ2
2 + n2σ2
1
.
(33.3.9)
If the hypothesis H1/3 is true, then
κ =

n3
n1
+ (n1 + n3)n3σ2
2
n1n2σ2
1
,
(33.3.10)
but if H2/3 is true, then
κ =

n3σ2
2(n2n3 + (n1 + n3)2) + n1n2n3σ2
1
n1n2(n1σ2
2 + n3σ2
1)
.
(33.3.11)
If H1/3 is true, then the random event of correct classiﬁcation
CC1(c) = {dc(Y(n)) = 1},
in view of (33.3.3) and Lemma 33.3.1, obtains the representation
CC1(c) =

|N1(0, 1)|
|N1(0, 1) + κN2(ν2, 1)| < ζ · c

(33.3.12)
with independent random variables N1 and N2 , ν2 from (33.3.9), κ from
(33.3.10) and ζ from ( 33.3.5). This proves the following theorem.
©2001 CRC Press LLC

THEOREM 33.3.2
If the hypothesis H1/3 is true, then P1(CC1(c)), the probability of correct
classiﬁcation into the population Π1, satisﬁes the representation formula
P1(CC1(c)) = Φ
(0,ν2)T ,
 1
0
0
1
(CC∗
1(c))
(33.3.13)
for all c > 0, with
CC∗
1(c) =

(t1, t2)T ∈R2 :
|t1|
|t1 + κt2| < ζ · c

and ν2, κ, and ζ as in (33.3.9), (33.3.10), and (33.3.5),respectively.
If the hypothesis H2/3 is true, then the random event of correct classiﬁ-
cation into the population Π2,
CC2(c) = {dc(Y(m)) = 2},
satisﬁes for all c > 0 the representation
CC2(c) =

|N1(ν1, 1)|
|N1(ν1, 1) + κN2(ν2, 1)| > ζ · c

,
(33.3.14)
where κ and ζ are to be chosen according to (33.3.11) and (33.3.5),
respectively,
ν1 =
µ2 −µ1

σ2
2/n3 + σ2
1/n1
=
√n1n3(µ2 −µ1)

n1σ2
2 + n3σ2
1
(33.3.15)
and, since
m1/3(µ1, µ2) −µ2 =
µ1 −µ2
1 + n3/n1
,
ν2 is deﬁned as
ν2 = √n2
µ1 −µ2
σ2

1 + n3
n1
2
+ n2
n1
σ2
1
σ2
2
+ n2n3
n2
1
=
n1(µ1 −µ2)

σ2
2(n2n3 + (n1 + n3)2) + σ2
1n1n2
.
(33.3.16)
The following theorem is an immediate consequence.
©2001 CRC Press LLC

THEOREM 33.3.3
If H2/3 is true then the probability of correct classiﬁcation into the pop-
ulation Π2, P2(CC2(c)), satisﬁes the representation formula
P2(CC2(c)) = Φ
(ν1,ν2)T ,
 1
0
0
1
(CC∗
2(c))
(33.3.17)
for all c > 0, with
CC∗
2(c) =

(t1, t2)T ∈R2 :
|t1|
|t1 + κt2| > ζ · c

and ν1, ν2, κ, ζ as in (33.3.15), (33.3.16), (33.3.11), and (33.3.5), re-
spectively.
The representation formulae for P1(CC1(c)) in Theorem 33.3.2 and for
P2(CC2(c)) in Theorem 33.3.3 do not only reﬂect diﬀerent quantitative
situations but they are also of diﬀerent qualitative nature. The most
obvious diﬀerence between ν1 = 0 in (33.3.8) and ν1 ̸= 0 in (33.3.15) can
be easily detected. The following theorem presents a second represen-
tation formula for P2(CC2(c)) which corresponds in quality to that for
P1(CC1(c)) in Theorem 33.3.2. Its proof repeats that of Theorem 33.3.2
and will be suppressed therefore here.
THEOREM 33.3.4
If H2/3 is true then the probability of correct classiﬁcation into the pop-
ulation Π2 satisﬁes for all c > 0 the representation
P2(CC2(c)) = Φ
(0,˜ν2)T ,
 1
0
0
1
( 
CC2(c)),
(33.3.18)
where

CC2(c) =

(t1, t2)T ∈R2 :
|t1|
|t1 + ˜κt2| < ζ
c

with ζ as in (33.3.5),
˜κ =

n3
n2
+ n3(n2 + n3)
n1n2
σ2
1
σ2
2
(33.3.19)
and
˜ν2 =
√n2 + n3(µ2 −µ1)

(n2 + n3)σ2
1 + n1σ2
2
.
(33.3.20)
The next theorem follows by analogy.
©2001 CRC Press LLC

THEOREM 33.3.5
If H1/3 is true then it holds for all c > 0
P1(CC1(c)) = Φ
(˜ν1,˜ν2)T ,
 1
0
0
1
 
(CC1(c)),
(33.3.21)
where

CC1(c) =

(t1, t2)T ∈R2 :
|t1|
|t1 + ˜κt2| > ζ
c

with ζ as in (33.3.5),
˜κ =

n3σ2
1(n1n3 + (n2 + n3)2) + n1n2n3σ2
2
n1n2(n1σ2
1 + n3σ2
2)
(33.3.22)
and
˜ν1 =
√n2n3(µ1 −µ2)

n2σ2
1 + n3σ2
2
(33.3.23)
as well as
˜ν2 =
n2(µ2 −µ1)

σ2
1(n1n3 + (n2 + n3)2) + n1n2σ2
2
.
(33.3.24)
33.3.2
Representation Formulae Based upon the Doubly
Noncentral F-Distribution
It was shown in John (1961) and Moran (1975) that the probabilities of
correct classiﬁcation can be expressed in terms of the doubly noncentral
F-distribution if expectations are unknown but covariance matrices are
known and equal and the linear discriminant function is used for classi-
fying an individual into one of the populations Π1 and Π2. It has been
recently proved, in Krause and Richter (1999), that the probabilities of
correct classiﬁcation can be also expressed in terms of the doubly non-
central F-distribution if both expectations and covariance matrices are
unknown but a certain generalized minimum-distance rule is used for
making the decision. Here, a result will be derived which is equivalent
in content to the latter one but diﬀerent from it in form. The method
of proving this result developed here diﬀers from that in Krause and
Richter (1999) in using basically a two-dimensional representation for-
mula from the preceding section whereas the proof of the corresponding
result in Krause and Richter (1999) starts from a sample space measure
representation formula.
The aim of what follows is to determine the Gaussian measure of
CC∗
1(c) in accordance with Theorem 33.3.2. To this end we describe the
©2001 CRC Press LLC

boundary of the set of points satisfying the inequality
|t1|
|t1 + κt2| < ζc
(33.3.25)
with the help of the straight lines
gj : t2 = −1
κ

1 + (−1)j
ζc

t1, j = 1, 2.
(33.3.26)
It turns out that the set of solutions of (33.3.25) includes the t2–axis
in its inner part.
The straight line g2 belongs for all values of ζc to
the union of the second and third quadrants in a cartesian coordinate
system, i.e. it belongs to the union of the sets {t1 < 0, t2 > 0} and
{t1 > 0, t2 > 0}.
The straight line g1 belongs to the same set if ζc > 1 but to the union
of the ﬁrst and fourth quadrants if 0 < ζc < 1. The straight lines g1 and
g2 intersect within the set (33.3.25) under an angle α satisfying
α = π −arctan

−1
κ

1 + 1
ζc

+(−1)I{ζc<1} · arctan

−1
κ

1 −1
ζc

.
(33.3.27)
Notice that the set of points (t1, t2)T corresponding to (33.3.25) repre-
sents a cone. That is why there exists a vector (t10, t20)T ∈R2 and a
positive real number d = d(ζc) such that a vector (t1, t2)T ∈R2 satisﬁes
condition (33.3.25) if and only if it satisﬁes the condition
∥(t1, t2)T −Π(t10,t20)T (t1, t2)T ∥2
∥Π(t10,t20)T (t1, t2)T ∥2
< d2.
(33.3.28)
The latter condition is equivalent to
(t1t20 −t2t10)2
(t1t10 + t2t20)2 < d2
or
(q −t2/t1)2 < d2(1 + qt2/t1)2
(33.3.29)
where
q = t20/t10.
Let us determine now a solution (q, d). Recall that the boundary of
(33.3.25) can be described by the equations (33.3.26). The temporary
©2001 CRC Press LLC

assumption that we have in (33.3.25) and (33.3.29) equalities instead of
inequalities leads us to the equation systems
q −d
dq + 1 = −1
κ +
1
κζc
,
q + d
1 −dq = −1
κ −
1
κζc
(33.3.30)
and
q −d
dq + 1 = −1
κ −
1
κζc
,
q + d
1 −dq = −1
κ +
1
κζc.
(33.3.31)
The solution of (33.3.30) is given by
d = 1
2

κζc + ζc
κ −
1
κζc

+ 1
2

κζc + ζc
κ −
1
κζc
2
+ 4,
(33.3.32)
q =

d −1
κ +
1
κζc

/

1 + d
κ −
d
κζc

.
(33.3.33)
The solution of (33.3.31) is given by
d = −1
2

κζc + ζc
κ −
1
κζc

+ 1
2

κζc + ζc
κ −
1
κζc
2
+ 4,
(33.3.34)
q =

d −1
κ −
1
κζc

/

1 + d
κ +
d
κζc

.
(33.3.35)
Consequently, the representation (33.3.25) for the cone under considera-
tion is equivalent to the representation (33.3.29) if the quantities d and q
are chosen there either according to (33.3.32) and (33.3.33) or according
to (33.3.34) and (33.3.35), respectively. Hence, the following lemma has
been proved.
LEMMA 33.3.6
The probability of correct classiﬁcation into Π1 satisﬁes the representa-
tion
P1(CC1(c))
= Φ
(0,v2)T ,
 1 0
0 1


(t1, t2)T ∈R2 :
∥(t1,t2)T −Π(t10,t20)T (t1,t2)T ∥2
∥Π(t10,t20)T (t1,t2)T ∥2
<d2

.
Here, v2 is chosen as in (33.3.9), (t10, t20)T ∈R2 is an arbitrary vector
satisfying t20/t10 = q and d and q are to be chosen according to either
(33.3.32) and (33.3.33) or (33.3.34) and (33.3.35).
©2001 CRC Press LLC

It follows from the deﬁnition of the doubly noncentral F–distribution
with (1,1) degrees of freedom and from the invariance of the two-dimen-
sional standard Gaussian measure with respect to orthogonal transfor-
mations that P1(CC1(c)) can be expressed as a suitable value of the
cumulative distribution function F1,1,∆2
1,∆2
2. The noncentrality parame-
ters of this distribution are
∆2
2 =
Π(t10,t20)T (0, ν2)T 2 =
t2
20ν2
2
t2
10 + t2
20
and
∆2
1 =
(0, ν2)T −Π(t10,t20)T (0, ν2)T 2 =
t2
10ν2
2
t2
10 + t2
20
.
As a result, the following theorem has been proved.
THEOREM 33.3.7
If H1/3 is true, then
P1(CC1(c)) = F1,1,∆2
1,∆2
2(d2)
with ν2, d, q as in Lemma 33.3.6 and
∆2
1 =
ν22
1 + q2 , ∆2
2 =
ν2
2
1 + 1/q2 .
Notice that a corresponding result in Krause and Richter (1999) is dif-
ferent in form and it is not obvious how to transform one of the results
into the other in a direct way.
References
1. Anderson, T. W. (1984). Multivariate Statistical Analysis. John
Wiley & Sons, New York.
2. Cacoullos, T. (1992). Two LDF characterizations of the normal
as a spherical distribution. Journal of Multivariate Analysis 40,
205–212.
3. Cacoullos, T. and Koutras, M. (1985). Minimum distance discrim-
ination for spherical distributions. In Statistical Theory and Data
Analysis (Ed., K. Matusita), pp. 91–102, North-Holland, Amster-
dam.
©2001 CRC Press LLC

4. Cacoullos, T. and Koutras, M. (1996).
On the performance of
minimum-distance classiﬁcation rules for Kotz-type elliptical dis-
tributions. In Advances in the Theory and Practice of Statistics:
A Volume in Honor of Samuel Kotz (Eds., N. L. Johnson and
N. Balakrishnan), pp. 209–224. John Wiley & Sons, New York.
5. Hills, M. (1966). Allocation rules and their error rates. Journal of
the Royal Statistical Society, Series B 28, 1–31.
6. John, S. (1961). Errors in discrimination. Annals of Mathematical
Statistics 32, 1125–1144.
7. Krause, D. and Richter, W.-D. (1994).
Geometric approach to
evaluating probabilities of correct classiﬁcation into two Gaussian
or spherical categories. In Information Systems and Data Analy-
sis (Eds., H.-H. Bock, W. Lenski, and M. Richter), pp. 242–250.
Springer-Verlag, Berlin.
8. Krause, D. and Richter, W.-D. (1999). Exact probabilities of cor-
rect classiﬁcations for repeated measurements from elliptically con-
toured distributions. Submitted for publication.
9. McLachlan, G. (1992). Discriminant Analysis and Statistical Pat-
tern Recognition. John Wiley & Sons, New York.
10. Moran, M. A. (1975). On the expectation of errors of allocation
associated with a linear discrimination function. Biometrika 62,
141–148.
11. Schaafsma, W. and van Vark, G. (1977). Classiﬁcation and dis-
crimination problems with applications, Part I. Statistica Neer-
landica 31, 25–45.
©2001 CRC Press LLC

34
Estimation of Cycling Eﬀect on Reliability
Vilijandas Bagdonaviˇcius and Mikhail Nikulin
Vilnius University, Lithuania
University Bordeaux 2, Bordeaux, France
ABSTRACT Accelerated life models including cycling eﬀect are intro-
duced. These models are modiﬁcations of the additive accumulation of
damages and generalized proportional hazards models. Semiparametric
estimation procedures are proposed.
Keywords and phrases Accelerated life models, additive accumula-
tion of damages, cycling stress, generalized proportional hazards, semi-
parametric estimation
34.1
Models
If an item is functioning under a periodic stress, the number of cycles
has an eﬀect on reliability.
The greater the number of stress cycles,
the shorter is the life of items. The purpose of the paper is to formu-
late models including the eﬀect of cycling and to give semiparametric
estimation procedures for reliability characteristics corresponding to the
usual stress conditions using experiments under accelerated stresses with
greater numbers of cycles and possibly greater amplitudes then those of
the usual stress.
Consider modiﬁcation of the AAD [additive accumulation of damages,
Bagdonaviˇcius (1978)] model. Suppose that stresses are diﬀerentiable
periodic time functions x(·) : [0, ∞) →B ∈R and let Tx(·) be the
time-to-failure under the stress x(·).
Under the AAD model the stress changes locally only the scale: the
survival function Sx(·)(t) = P{Tx(·) > t} has the form
Sx(·)(t) = G
 t
0
r{x(u)} du

.
When cycling eﬀect is present, this model is not appropriate. Denote
©2001 CRC Press LLC

by ∆x the amplitude, by ∆t the period and by
¯x = 1
∆t
 ∆t
0
x(u)du
the mean value of x(·). The number of cycles in the interval [0, t] is
n(t) =
 t
0
| d1{x′(u) > 0} | .
Consider [cf. Schabe and Viertl (1995)] the following generalization of
the AAD model:
Sx(·)(t) = G

r1{¯x} t + r2{∆x}
 t
0
| d1{x′(u) > 0} |

.
(34.1.1)
The second term includes the eﬀect of cycling: if the number of cycles
and the amplitude are greater then the probability to survive until the
moment t is smaller (the function G is decreasing).
Consider also a modiﬁcation of the generalized proportional hazards
(GPH) model [Bagdonaviˇcius and Nikulin (1999b)]. Let
αx(·)(t) = lim
h↓0
1
h P{Tx(·) ∈(t, t + h] | Tx(·) > t} = −
S′
x(·)(t)
Sx(·)(t), t ≥0,
be the hazard rate function under the stress x(·). Denote by
Ax(·)(t) =
 t
0
αx(·)(u)du = −ln{Sx(·)(t)}, t ≥0,
the accumulated hazard rate under x(·).
The GPH model holds on E if for all x(·) ∈E
αx(·)(t) = r{x(t)} q{Ax(·)(t)} α0(t).
The particular cases of the GPH model are the proportional hazards
[PH, Cox (1972)] model (q(u) ≡1) and the AAD model (α0(t) ≡α0 =
const).
Examples of other particular cases are:
αx(·)(t) = r{x(t)}(1 + Ax(·)(t))γ+1α0(t),
αx(·)(t) = r(x(t)) eγAx(·)(t) α0(t),
αx(·)(t) = r(x(t))
1
1 + γAx(·)(t) α0(t).
©2001 CRC Press LLC

In terms of survival functions, the GPH model is written in the form:
Sx(·)(t) = G
 t
0
r{x(u)} dH(S0(u))

,
where the survival function G is such that it’s inverse function H has
the form
H(u) =
 −ln u
0
dv
q(v),
and the baseline function S0 doesn’t depend on the stress. Generalization
of the GPH model, including the eﬀect of cycling, has the form
Sx(·)(t) = G

r1{¯x} H(S0(t)) + r2{∆x}
 t
0
| d1{x′(u) > 0} |

.
(34.1.2)
34.2
Semiparametric Estimation
Semiparametric estimation for the AAD model was considered by Basu
and Ebrahimi (1982), Sethuraman and Singpurwalla (1982), Shaked and
Singpurwalla (1983), Schmoyer (1986, 1991), Robins and Tsiatis (1992),
Lin and Ying (1995), and Bagdonaviˇcius and Nikulin (1999a,b and 2000).
Estimation for the PH model was developed by Cox (1972), Tsiatis
(1981), and Andersen and Gill (1982).
Particular cases of the GPH model with parameterization of q was
considered by Andersen et al. (1993) and Bagdonaviˇcius and Nikulin
(1999b), with completely known q by Dabrowska and Doksum (1988a,b),
Cheng, Wei and Ying (1995), Murphy, Rossini and van der Vaart (1997),
and Bagdonaviˇcius and Nikulin (1997a,b,c).
34.2.1
The First Model
Suppose that the model (34.1.1) is considered, the function G is unknown
and the functions r1 and r2 are parameterized. The most common pa-
rameterizations for this type of models are
rj(u, βj) = exp{βj0 + βj1ϕj(u)}
(j = 1, 2),
where ϕj are speciﬁed and βj = (βj0, βj1)T .
We can take β10 = 0
because the function G is unknown. Set β = (β1, βT
2 )T .
Let x1(·), ..., xk(·) be periodic diﬀerentiable stresses with diﬀerent cy-
cling rates and possibly diﬀerent amplitudes.
©2001 CRC Press LLC

Suppose that k groups of items are observed. The ith group of ni
items is tested under the stress xi(·) time ti.
Denote by Ni(τ) the number of observed failures of the ith group in
the interval [0, τ] and by Yi(τ) the number of items at risk just before
the moment τ, by Ti1 ≤... ≤Timi the moments of failures of the ith
group, mi = Ni(ti).
The purpose is to estimate reliability of items under the usual stress
x0(·) with smaller cycling rate and possibly smaller amplitude than those
of the stresses xi(·).
The survival function under the stress xi is
Sxi(·)(t) = G

r1{¯xi, β1} t + r2{∆xi, β2}
 t
0
| d1{x′
i(u) > 0} |

.
In the particular case when ¯xi = ¯x0 for all i, we can take r1(u) ≡1
because G is unknown.
The idea of estimation procedure is similar to the idea of the EM
algorithm. At ﬁrst a pseudoestimator (depending on β) of the survival
function G is obtained. After the parameter β is estimated using the
maximum likelihood method.
The random variables
Rij = r1{¯xi, β1} Tij + r2{∆xi, β2}
 Tij
0
| d1{x′
i(u) > 0} |
can be considered as “observed” pseudofailures in the experiment where
n items, n = m
i=1 ni, with the survival function G are tested and ni−mi
among them are censored at the moment gi(ti, β), where
gi(t, β) = r1{xi(u), β1} t + r2{∆xi, β2}
 t
0
| d1{x′
i(u) > 0} | .
Denote by hi(t, β) the inverse function of gi with β being ﬁxed. Then
N R(τ, β) =
k

i=1
Ni(hi(τ, β))
is the number of “observed” failures in the interval [0, τ] and
Y R(τ, β) =
k

i=1
Yi(hi(τ, β))
is the number of items at risk just before the moment τ.
The survival function G can be “estimated” by the Kaplan-Meier es-
timator [see Andersen et al. (1993)]: for all s ≤maxi{gi(ti, β)} we have
˜G(s, β) =

τ≤s

1 −∆N R(τ, β)
Y R(τ, β)

=

τ≤s
	
1 −
k
l=1 ∆Nl(hl(τ, β))
k
l=1 Yl(hl(τ, β)

,
©2001 CRC Press LLC

where
∆N R(τ, β) = N R(τ, β) −N R(τ−, β).
If there is no ex aequo (ties) then the pseudoestimator ˜G can be written:
˜G(s, β) =

(i,j):Tij≤hi(s,β)
	
1 −
1
k
l=1 Yl(hl(gi(Tij, β), β)

.
The likelihood function is deﬁned as
L(β) =
k

i=1
mi

j=1
[ ˜G(gi(Tij−, β), β) −˜G(gi(Tij, β), β)] ˜Gni−mi(gi(ti, β), β),
where
˜G(u−, β) = lim
ε↓0
˜G(u −ε, β).
In parametric models the likelihood function is a product of densities
in the observed failure points and survival functions in the censoring
points.
In our case the density f = −G′ is unknown and the factor
which corresponds to a failure is replaced by a jump of the function ˜G,
therefore factors
˜G(gi(Tij−, β), β) −˜G(gi(Tij, β), β).
are included in L(β).
If there are ex aequo then denote by T ∗
1 (β) < ... < T ∗
q (β) the diﬀerent
moments among gi(Tij, β) , dj - the number of pseudofailures at the
moment T ∗
j (β). Then for any s ≤maxi{gi(ti, β)}
˜G(s, β) =

j:T ∗
j (β)≤s
	
1 −
dj
m
l=1 Yl(hl(T ∗
j , β)(β))

and
L(β) =
q

j=1
[ ˜G(T ∗
j−1(β), β) −˜G(T ∗
j (β), β)]dj
k

i=1
˜Gni−mi(gi(ti, β), β).
The maximum likelihood estimator maximizes L(β): ˆβ = ArgmaxβL(β).
The survival function under the usual stress x0(·) is estimated for
any s ≤maxi{gi(ti, β)} by replacing unknown parameters β by their
estimators ˆβ and the stresses x(·) by the usual stress x0(·) in the formula
(34.2.334.1.1):
ˆSx0(s) = ˜G

r1{¯x0, ˆβ1}t + r2{∆x0, ˆβ2}
 t
0
| d1{x′
0(u) > 0} |, ˆβ

.
©2001 CRC Press LLC

34.2.2
The Second Model
Consider the model (34.2.334.1.2) with pre-speciﬁed parameterization
ri = ri(u, βi) and q = q(u, γ) (or, equivalently, G = G(t, γ) = Gγ(t))
via the parameters θ = (β1, βT
2 , γ)T , β2 = (β20, β21) and an unknown
baseline function α0(t).
Suppose that the plan of experiments is as before. Put
Xij = Tij ∧ti, δij = 1{Tij≤ti}, Nij(t) = 1{Tij≤t,δij=1},
Yij(t) = 1{Xij≥t}, Ai = Axi(·).
Estimation is also a two-stage procedure. To obtain a pseudoestimator
of the baseline function S0 we note that the compensator Λi(t) of the
counting process Ni(t) is:
Λi(t) =
 t
0
Yi(u)ψ(Ai(u−))(r1(¯xi)d H(S0(u))
+r2(∆xi) | d1{x′
i(u) > 0} |),
where
ψ(u) = α(H(e−u)), α = −G′/G.
It indicates a pseudoestimator (still depending on θ) for S0:
˜S0(t, θ)
= Gγ
 t
0
dN(t)−k
i=1 r2{∆xi,β2}Yi(u) ψγ( ˆ
Ai(u−)))|d1{x′
i(u)>0}|
k
i=1 r1(¯xi,β1)Yi(u) ψγ( ˆ
Ai(u−))

,
where ˆAi is the Nelson-Aalen estimator [see Andersen et al. (1993)] of
Ai:
ˆAi(t) =
 t
0
dNi(u)
Yi(u) .
The likelihood function
L(θ) =
k

i=1
mi

j=1
[Ui(Tij−, θ) −Ui(Tij, θ)] U ni−mi
i
(ti, θ),
(34.2.1)
where
Ui(t, θ) = Gγ{r1(¯xi, β1)
 t
0
dN(u)−k
l=1 r2{∆xl,β2} Yl(u) ψγ( ˆ
Al(u−)) |d1{x′
l(u)>0}|
k
l=1 r1(¯xl,β1) Yl(u) ψγ(Al(u−))
+r2{∆xi, β2} | d1{x′
i(u) > 0} |}.
(34.2.2)
©2001 CRC Press LLC

If ki are small, the Nelson-Aalen estimators may become ineﬃcient and
the estimators ˆAi should be replaced by estimators which are functions of
all data:these estimators are determined recurrently [cf. Bagdonaviˇcius
and Nikulin (1999b)]:
˜Ai(t, θ) =
 t
0
ψγ( ˜Ai(u−, θ))(r1(¯xi, β1) dHγ( ˆS0(u, θ))
+r2(∆xi, β2) | d1{x′
i(u) > 0} |),
(34.2.3)
where
Hγ( ˆS0(u, θ))
=
 t
0
dN(t) −k
i=1 r2{∆xi, β2}Yi(u) ψγ( ˜Ai(u−, θ)) | d1{x′
i(u) > 0} |
k
i=1 r1(¯xi, β1)Yi(u) ψγ( ˜Ai(u−, θ))
.
The likelihood function has the form (34.2.334.2.1), where the functions
Ui(t, θ) are as in (34.2.334.2.2) with the only diﬀerence being that the
Nelson-Aalen estimators ˆAi(t) are replaced by ˜Ai(t, θ). Note that to ﬁnd
an initial estimator ˆθ(0) you need initial estimators ˜A(0)
i (t, θ) and vice
versa: to ﬁnd initial estimators ˜A(0)
i (t, θ) you need an initial estimator
ˆθ(0). This problem can be solved by taking the initial estimator ˆθ(0)
obtained by the ﬁrst method with Nelson-Aalen estimators. Then the
initial estimators ˜A(0)
i (t, θ(0)) are obtained recurrently from the equations
(34.2.334.2.3). The estimator ˆθ(1) is obtained by the likelihood function,
where Ui(t, θ) are obtained by the formula (34.2.334.2.2), replacing ˆAi(t)
by ˜A(0)
i (t, θ(0)) and so on.
The estimator of the survival function Sx0(t), t ≤τ, under the usual
stress is
ˆSx(·)(t)
= G

r1{¯x0, ˆβ1} Hˆγ( ˆS0(t, ˆθ)) + r2{∆x0, ˆβ2}
 t
0
| d1{x′
0(u) > 0} |

.
The asymptotic normality of estimators can be proved by arguments
similar to the ones used in Bagdonaviˇcius and Nikulin (1999b) for GPH
models.
©2001 CRC Press LLC

References
1. Andersen, P. K. and Gill, R. D. (1982). Cox’s regression model for
counting processes: A large sample study. Annals of Statistics 10,
1100–1120.
2. Andersen, P. K., Borgan, O., Gill, R. D., and Keiding, N. (1993).
Statistical Models Based on Counting Processes. Springer-Verlag,
New York.
3. Bagdonaviˇcius, V. (1978). Testing the hyphothesis of the additive
accumulation of damages. Probability Theory and its Applications.
23, 403–408.
4. Bagdonaviˇcius, V. and Nikulin, M. (1997a). Transfer functionals
and semiparametric regression models. Biometrika 84, 365–378.
5. Bagdonaviˇcius, V. and Nikulin, M. (1997b). Asymptotic analysis
of semiparametric models in survival analysis and accelerated life
testing. Statistics 29, 261–283.
6. Bagdonaviˇcius, V. and Nikulin, M. (1997c). Analysis of general
semiparametric models with random covariates. Romanian Journal
of Pure and Applied Mathematics 42, 351–369.
7. Bagdonaviˇcius, V. and Nikulin, M. (1999a).
On semiparamet-
ric estimation of reliability from accelerated data.
In Statistical
and Probabilistic Models in Reliability (Eds., D. C. Ionescu and N.
Limnios), pp. 75–89. Birkha¨user, Boston.
8. Bagdonaviˇcius, V. and Nikulin, M. (1999b). Generalized propor-
tional hazards models: estimation. Lifetime Data Analysis 5, 329–
350.
9. Bagdonaviˇcius, V. and Nikulin, M. (2000). On nonparametric es-
timation in accelerated experiments with step-stresses. Statistics
33, 349–365.
10. Basu, A. P. and Ebrahimi, N. (1982). Nonparametric accelerated
life testing. IEEE Transactions on Reliability 31, 432–435.
11. Cheng, S. C., Wei, L. J., and Ying, Z. (1995). Analysis of trans-
formation models with censored data. Biometrika 82, 835–846.
12. Cox, D. R. (1972). Regression models and life tables. Journal of
the Royal Statistical Society, Series B 34, 187–220.
13. Cox, D. R. and Oakes, D. (1984). Analysis of Survival Data, Chap-
man and Hall, London.
14. Dabrowska, D. M. and Doksum, K. A. (1988a). Estimation and
testing in a two-sample generalized odds-rate model. Journal of
©2001 CRC Press LLC

the American Statistical Association 83 744–749.
15. Dabrowska, D. M. and Doksum, K. A. (1988b). Partial likelihood
in transformation model with censored data. Scandinavian Journal
of Statistics 15, 1–23.
16. Lin, D. Y. and Ying, Z. (1995). Semiparametric inference for the
accelerated life model with time dependent covariates. Journal of
Statistical Planning and Inference 44, 47–63.
17. Murphy, S. A., Rossini, A. J., and van der Vaart, A. W. (1997).
Maximum likelihood estimation in the proportional odds model.
Journal of the American Statistical Association 92, 968–976.
18. Robins, J. M. and Tsiatis, A. A. (1992).
Semiparametric esti-
mation of an accelerated failure time model with time dependent
covariates. Biometrika 79, 311–319.
19. Schabe, H. and Viertl, R. (1995). An axiomatic approach to models
of accelerating life testing.
Engineering Fracture Mechanics 30,
203–217.
20. Schmoyer, R. (1986). An exact distribution-free analysis for accel-
erated life testing at several levels of a single stress. Technometrics
28, 165–175.
21. Schmoyer, R. ( 1991). Nonparametric analysis for two-level single-
stress accelerated life tests. Technometrics 33, 175–186.
22. Sethuraman, J. and Singpurwalla, N. D. (1982).
Testing of hy-
potheses for distributions in accelerated life tests. Journal of the
American Statistical Association 77, 204–208.
23. Shaked, M. and Singpurwalla, N. D. (1983). Inference for step-
stress accelerated life tests.
Journal of Statistical Planning and
Inference 7, 295–306.
24. Tsiatis, A. A. (1981). A large sample study of Cox’s regression
model. Annals of Statistics 9, 93–108.
©2001 CRC Press LLC

PART VI
Applications to Biology and Medicine
©2001 CRC Press LLC

35
A New Test for Treatment vs. Control in
an Ordered 2 × 3 Contingency Table
Arthur Cohen and H. B. Sackrowitz
Rutgers University, New Brunswick, NJ
ABSTRACT Suppose two treatments are to be compared where the
responses to the treatments fall into a 2 × k contingency table with or-
dered categories. Test the null hypothesis that the treatments have the
same eﬀect against the alternative that the second treatment is “better”
than the ﬁrst. “Better” means that the probability of falling into the
higher of the ordered categories is larger for the second treatment than
for the ﬁrst treatment. (In technical terms what we deﬁne as better is
stochastically larger.) For k = 3 we propose a new test that has very de-
sirable properties. The small sample version of the new test is unbiased,
conditionally unbiased, admissible, and has a monotone power function
property. It is not easily carried out. The large sample version of the
test is approximately unbiased, has desirable monotonicity properties,
and is easy to carry out. The new test seems preferable to the likelihood
ratio test and to the Wilcoxon-Mann-Whitney test.
Keywords and phrases Wilcoxon-Mann-Whitney test, likelihood ra-
tio test, unbiased test, likelihood ratio order, stochastic order, indepen-
dence
35.1
Introduction
A classical model in determining the eﬀectiveness of a treatment is to ad-
minister a placebo to n1 individuals and to administer the treatment to
n2 individuals. Suppose the responses are categorical and the categories
are ordered. For example, the categories oftentimes are no improvement,
some improvement, cured. For such a model one wishes to test the null
hypothesis that treatment and placebo eﬀects are the same against some
one sided alternative that the treatment is “better” than the placebo.
©2001 CRC Press LLC

There are various ways to formally deﬁne “better”. One way, which is
oftentimes convincing, is to say that the treatment is better than the
placebo if the proportion of individuals for the treatment group in the
higher categories of the ordering, is larger than the comparable propor-
tion in the placebo group. This way of describing “better” is formally
called stochastic ordering.
The situation above arises in connection with many 2 × k ordered
contingency tables. Each row of the table represents a treatment or a
condition while the ordered k levels of the other factor represent some
reaction or status that may very well be inﬂuenced and can be correlated
with the treatment or condition. There are an abundance of such tables.
In fact Moses, Emerson, and Hosseini (1984) identiﬁed 27 instances of
2 × k tables in a survey of articles in vol. 306, 1982 of the New England
Journal of Medicine. Many of these articles involve 2×3 tables. In some
of the 2×k tables the row totals are not predetermined as in our opening
model. Nevertheless, in the situation where only the total number in all
cells is predetermined a comparable testing problem is to test a null of
independence against a one sided alternative of stochastic ordering of
conditional probabilities.
A very popular approach to this testing problem is to use nonpara-
metric types of tests including the Wilcoxon-Mann-Whitney (WMW)
rank test.
See for example Rahlfs and Zimmerman (1993), Emerson
and Moses (1985), Agresti and Finlay (1997), and Moses, Emerson, and
Hosseini (1984). This latter paper is essentially reproduced in the book
Medical Uses of Statistics, second edition (1992). Reference in the ar-
ticle is made to Stat Xact, a software program based on the WMW
test. Grove (1980) derives and studies the likelihood ratio test (LRT) for
testing independence vs. stochastic ordering of conditional probabilities
assuming a 2×k ordered contingency table when the total number of ob-
servations is ﬁxed (full multinomial model). Bhattacharya and Dykstra
(1994) derive and study the LRT for testing equality of two multinomial
distributions against stochastic ordering when there are two indepen-
dent distributions (product multinomial model). More recently, Berger,
Permutt, and Ivanova (1998) oﬀers tests for a 2 × 3 table. Cohen and
Sackrowitz (1998) oﬀer a new testing methodology that can be applied
in this situation when samples are not large. See also Cohen and Sack-
rowitz (1999b).
In this paper, for 2×3 ordered contingency tables (under either the full
multinomial model or the product multinomial) we will propose a new
test. This new test will be decidedly preferable to the WMW test and
on an overall basis, preferable to the LRT. It is preferable to the tests
proposed by Berger et al. and for large samples, preferable to the test
methodology of Cohen and Sackrowitz (1998). The test recommended
©2001 CRC Press LLC

for the problem has very strong theoretical properties that the other
tests do not have. In fact the theory exposes serious weaknesses of the
WMW test implying that it should not be used. The LRT and Berger
et al. tests are shown to have shortcomings. The simulation study bears
out the theoretical support for the test we recommend.
In the next section we describe the test we recommend. Call it CST.
One additional advantage of CST is that it is easy to implement for
moderate samples sizes. We give a numerical example. In Section 35.3
we give a simulation study that compares the size and power of CST,
LRT, and WMW. In Section 35.4 we indicate some theoretical properties
of CST and the shortcomings of the other tests. All proofs are given in
the Appendix.
35.2
New Test, Implementation and Example
Consider the following 2 × k contingency table
X11
X12
· · ·
X1k
n1
X21
X22
· · ·
X2k
n2
T1
T2
Tk
n
The Xij, i = 1, 2; j = 1, 2, . . . , k are cell frequencies with k
j=1 Xij =
ni and n1 + n2 = n. The Tj, j = 1, 2, . . . , k are column totals. For
the product multinomial model the ni are ﬁxed sample sizes and Xi =
(Xi1, . . . , Xik) has a multinomial distribution with probability vector
p′
i = (pi1, . . . , pik) such that ni
j=1 pij = 1.
Test H0 : p1 = p2 vs.
H1 −H0 where
H1 :
k

j=ℓ
p1j ≤
k

j=ℓ
p2j,
for ℓ= 2, . . . , k.
(35.2.1)
This alternative is called stochastic ordering.
Now let k = 3. We ﬁrst describe our test procedure when n1 and
n2 have moderate to large samples sizes (ni ≥15).
Let Y1 = X11,
Y2 = X11 + X12, µy1 = n1T1/n, µy2 = n1(T1 + T2)/n, σ2
y1 = n1(n −
n1)T1(n−T1)/n2(n−1), σ2
y2 = n1(n−n1)(T1+T2)(n−T1−T2)/n2(n−1),
ρ =

T1(n −T1 −T2)/

(n −T1)(T1 + T2), B1 = (Y1 −µy1)/σy1, B2 =
(Y2 −µy2)/σy2, M = (σy1 −σy2ρ)/(σy2 −σy1ρ), and let R(ω) be the
decreasing function deﬁned as
R(ω) = ϕ(ω)/Φ(ω),
(35.2.2)
©2001 CRC Press LLC

TABLE 35.1
Bo dy weight class
Light 
Medium 
Heavy
condition absent (p1 )
22
 
28
 
14
condition present ( p2 ) 
6 
16 
18
where ϕ(ω) is the standard normal density function and Φ(ω) is the
standard normal cumulative distribution function. (Note R(−ω) is Mill’s
Ratio.) Our test procedure is carried out by determining a p-value. If
the p-value is to be used for testing at a particular signiﬁcance level,
clearly we reject if the p-value is less than the given level.
Determine the p-value as follows:
Consider whether
MR
B1 −B2ρ

1 −ρ2
 <
=
> R
B2 −B1ρ

1 −ρ2

. 
(35.2.3)
If the left hand side of (35.2.3) > right hand side ﬁx B2 and ﬁnd
B∗
1 > B1 such that equality holds in (35.2.3). Find the p-value from
tables of the bivariate normal distribution BV N((0, 0); (1, 1, ρ)) by ﬁnd-
ing 1−F(B∗
1, B2) where F is the bivariate normal cumulative distribution
function. If the left hand side of (35.2.3) < right hand side ﬁx B1 and
ﬁnd B∗
2 > B2 such that equality in (35.2.3) holds and proceed as before.
Note that tables of the bivariate normal BV N((0, 0); (1, 1, ρ)) appear
in Tables of the Bivariate Normal Distribution and Related Functions,
National Bureau of Standards (1959) and is also available in IMSL.
As an example we use some of the data in Zar (1996). Consider Table
35.1.
The data are the frequencies of occurrence among elderly women of a
skeletal condition, tabulated by body weight. The null hypothesis is that
the distribution for condition present is the same as for condition absent.
We note the weight categories are ordered and the alternative hypothesis
is that the condition present distribution is stochastically larger than the
condition absent distribution.
Find n1 = 64, n2 = 40, T1 = 28, T2 = 44, µ1 = 17.23, µ2 = 27.08,
σy1 = 2.21, σy2 = 2.30, ρ = .405, M = .91, B1 = 2.16, B2 = 2.47. Using
(2.3) and tables of the standard normal distribution ﬁnd B∗
1 = 2.44. Use
IMSL subroutines to ﬁnd a p-value of .014.
REMARK For the full multinomial model H0 : pij = pi.p.j and H1 :
k
j=ℓp1j/p1. ≤k
j=ℓp2j/p2., where pij is the cell probability, pi. =
k
j=1 pij, p.j = 2
i=1 pij. The test procedure and all results are exactly
©2001 CRC Press LLC

the same. This is true since the test is performed conditionally given all
marginal totals. Thus it doesn’t matter if n1 and n2 are treated as ﬁxed.
When the sample sizes are not large the new test can be described
but is more diﬃcult to implement. Let k = 3. The new test is per-
formed conditionally given n1, T1, T2. Let ψ(y1, y2) be the test function
i.e. ψ(y1, y2) is the probability of rejecting H0 when (y1, y2) is observed.
Then CST is as follows:
ψ(y1, y2; C1, C2, γ1, γ2) =











1
if y1 > C1 or y2 > C2
γ1
if 0 ≤y1 < C1, y2 = C2
γ2
if y1 = C1, 0 ≤y2 < C2
max(γ1, γ2) if y1 = C1, y2 = C2
0
otherwise,
(35.2.4)
where γ1, γ2, C1, C2 are chosen so that the test has size α and satisﬁes
EH0


(Y2 −Y1) −EH0(Y2 −Y1)

ψ(Y1, Y2)
n1, T1, T2

= 0. (35.2.5)
(EH0{·|n1, T1, T2} means conditional expected value under H0.)
REMARK When sample sizes are not large considerable randomization
may be required. In this case a method described in Berger and Sack-
rowitz (1997) can be used to improve the test described in (35.2.4).
35.3 
Simulation Study
We consider the product multinomial model.
In this section we oﬀer simulated power functions for the LRT, the
WMW rank test, and for CST. All tests in the simulation were carried
out at the .05 level of signiﬁcance.
The LRT was carried out using
Theorem 8.F.1 of Bhattacharya and Dykstra (1994). That is, the LRT
statistic is calculated and its p-value is determined using the probability
bound of Theorem 8.F.1. If the p-value ≤.05 the test rejects.
The WMW test was carried out using the limiting normal distribution
of the normalized test statistic.
Table 35.2 contains simulated power for the LRT, WMW rank test,
and the CST. The simulation was based on 10,000 iterations. The sample
©2001 CRC Press LLC

sizes for each multinomial was 25. From the table we note that samples
sizes of 25 seem adequate for using the normal approximation associated
with CST since the size of the test is close to .05. The size of the LRT
will always be less than .05. We also note that for those alternatives
considered, which represent a wide diversity of alternatives, CST is doing
better than LRT in the majority of cases and CST is decidedly better
than the WMW rank test.
Additional simulations for sample sizes (n1 = 15, n2 = 20), (n1 =
25, n2 = 15), (n1 = 50, n2 = 40) were done and the same pattern as
in the case (n1 = 25, n2 = 25) was observed. Thus we feel that the
simulation supports the theoretical ﬁndings to be discussed and leads us
strongly to recommend the CST.
In regard to the WMW test we can refer to Hilton and Mehta (1993).
In that paper it was noted that if the WMW test is performed uncondi-
tionally and without regard to randomization then its power will be poor.
Furthermore the alternative hypothesis for which WMW is considered
in that reference is not the stochastic order alternative we study. The
alternative which these authors and Mehta, Patel, and Tsiatis (1984)
study is the more stringent alternative called likelihood ratio order. See
Cohen and Sackrowitz (1999a) for a discussion of various alternatives.
Two further comments regarding CST and WMW. In our simulation
the power of CST is doing well relative to WMW even for likelihood ratio
order alternatives.
Second, our theory will demonstrate that WMW
must have very poor conditional power (less than α) for many stochastic
order alternatives. It follows that if the alternative is stochastic ordering
WMW should not be used.
35.4
Theoretical Properties
The test CST was developed so that it would have the desirable proper-
ties of being unbiased and it would lie in a nontrivial complete class of
tests. It has the further property of being conditionally unbiased given
the marginal totals are held ﬁxed. Still further it has a strictly increasing
power function in certain directions in the alternative space.
We describe the theoretical properties in terms of the product multino-
mial model and remark that the properties hold for the full multinomial
model. Recall that the product multinomial model entails Xi, i = 1, 2 as
independent multinomial vectors denoted by M(ni, k, p). Let the odds
ratio θj be deﬁned as θj = (p1jp2k/p2jp1k) and let νj = log θj.
Let
Tj = X1j + X2j. The joint density of X1, X2 is
©2001 CRC Press LLC

TABLE 35.2
Simulated power functions
POWER
Parameter
CST
LRT
WMW

p11
p21
p12
p22
p13
p23

.33 .33 .33
.0511
.0440
.0409
.33 .33 .33
.2
.4
.4
.0504
.0429
.0376
.2
.4
.4
.25 .25 .25
.0499
.0436
.0360
.25 .25 .25
.33 .33 .33
.2657
.2308
.2461
.25 .25 .5
.4
.4
.2
.8564
.8271
.8081
.2
.2
.6
.25 .5
.25
.6838
.7325
.5665
.2
.2
.6
.33 .33 .33
.2866
.2303
.2693
.2
.33 .47
.4
.1
.5
.4139
.3832
.2204
.2
.2
.6
.2
.4
.4
.2559
.3345
.2223
.2
.2
.6
.6
.2
.2
.7807
.8361
.5962
.2
.6
.2
.5
.1
.4
.1237
.1707
.0754
.4
.2
.4
.45 .1
.45
.0837
.0996
.0548
.40 .15 .45
.4
.1
.5
.4716
.3694
.3505
.2
.1
.7
.4
.1
.5
.8195
.7315
.7295
.1
.1
.8
.4
.2
.4
.8209
.7446
.7455
.1
.2
.7
.5
.3
.2
.4805
.3969
.4699
.3
.3
.4
.5
.2
.3
.6022
.5008
.5683
.25 .2
.55
©2001 CRC Press LLC

n1!n2!

i,j xij!

i,j
pxij
ij ,
k

j=1
xij = ni,
k

j=1
pij = 1.
(35.4.1)
Under H0 : p1 = p2, it follows that Tj, j = 1, 2, . . . , k are suﬃcient
and complete statistics. The conditional distribution of X1, X2 given
T1, . . . , Tk is the multivariate extended hypergeometric distribution. Its
density can be expressed as

k

j=1
 T1
x1j
 k−1

j=1
θx1j
j
 
x1∈A
k

j=1
 Tj
x1j
 k−1

j=1
θx1j
j

,
(35.4.2)
where A = {x(1)
1
: 0 ≤x1j ≤Tj, j = 1, . . . , k −1, x1k = n1 −k−1
j=1 x1j ≤
tk, k−1
j=1 x1j ≤n1}. From (35.4.2) we note that the conditional distri-
bution of X(1) given T is a multivariate exponential family distribution
of the form
f

x(1); νν

= CT(νν)h(x(1))ex(1)′ νν,
(35.4.3)
with νj, j = 1, 2, . . . , k −1 as the natural parameters. (The νj are log
odds ratios formed by the jth and kth columns of the 2 × k table.) Fur-
thermore, under H0, νν = 0. From Lemma 2.1 of Berger and Sackrowitz
(1997) it follows that given a value of νν with ν1 > 0, there exists a pair
(p1, p2) such that p1 ≤st p2 (p1 is stochastically less or equal to p2)
and (p1, p2) yield the given value of νν. (For that same value of νν there
could exist (p1, p2) such that p1̸≤stp2.) For purposes of determining a
complete class, an admissible test, or an unbiased test for H0 vs. H1,
we can ﬁnd a complete class, an admissible test and an unbiased test for
the conditional problem of testing H∗
0 : νν = 0 vs. H∗
1 : ν1 > 0. This
was noted in Berger and Sackrowitz (1997). Furthermore from (35.4.3)
and using arguments of Eaton (1970) it follows that a complete class of
tests, C, consists of test functions that are monotone nondecreasing as
a function of x11 when x12, . . . , x1(k−1) are ﬁxed. This too was noted in
Berger and Sackrowitz (1997).
We now concentrate on the special case k = 3. We ﬁrst oﬀer a set of
suﬃcient conditions for a test ψ(x11, x12) to be conditionally unbiased,
hence unconditionally unbiased, and to lie in the nontrivial complete
class noted earlier. Afterwards we will show that CST satisﬁes the con-
ditions.
©2001 CRC Press LLC

THEOREM 35.4.1
Suﬃcient conditions for a test ψ(x11, x12) to be conditionally unbiased
of size α and to lie in C are
ψ is nondecreasing in x11 for ﬁxed x11 + x12 
(35.4.4)
ψ is nondecreasing in x11 + x12 for ﬁxed x11 
(35.4.5)
Eνν=0ψ(X11, X12) = α 
(35.4.6)
Eνν=0[X12 −Eνν=0X12]ψ(X11, X12) = 0  
(35.4.7)
The function
g(X12) = Eνν=0[ψ(X11, X12)|X12] 
(35.4.8)
decreases and then increases.
PROOF See the Appendix.
REMARK The proof of Theorem 35.4.1 demonstrates a stronger result
than conditional unbiasedness of the test. The test ψ satisfying prop-
erties (35.4.4)–(35.4.8) has a conditional (unconditional) power function
that is increasing as a function of ν1 for every ﬁxed ν2.
Next we state
THEOREM 35.4.2
The test in (35.2.4) is conditionally unbiased and lies in C.
PROOF See the Appendix.
In Section 35.2 we indicated how to implement the test for moderate or
large samples. This implementation was based on some limiting results.
The implementation was possible because of
©2001 CRC Press LLC

THEOREM 35.4.3
The approximate p-value of the test procedure in (35.2.4) is as follows:
Consider whether
MR
B1 −B2ρ

1 −ρ2
 <
=
> R
B2 −B1ρ

1 −ρ2

.
(35.4.9)
If the left hand side of (35.4.9) > right hand side ﬁx B2 and ﬁnd B∗
1 > B1
such that equality holds in (35.4.9).
Find the p-value from tables of
the bivariate normal distribution BV N((0, 0); (1, 1, ρ)) by ﬁnding 1 −
F(B∗
1, B2). If the left hand side of (35.4.9) < right hand side ﬁx B1 and
ﬁnd B∗
2 > B2 such that equality (35.4.9) holds, and proceed similarly as
before.
PROOF See the Appendix.
We now make some comments regarding the LRT, the WMW test, and
the tests proposed by Berger et al. The LRT is not performed condi-
tionally given the marginal totals. The marginal totals form a suﬃcient
and complete statistic under H0, and therefore to achieve a similarity
property the test should be performed conditionally. Hence the LRT test
is not similar, meaning that its power properties on alternatives near the
null hypothesis space cannot be good. Furthermore the LRT as well as
the tests proposed by Berger et al. lack an intuitive monotonicity prop-
erty called concordance monotonicity. See Cohen and Sackrowitz (1998)
for a discussion of this property. Still further the asymptotic distribution
of the test statistic upon which the LRT is based is diﬃcult to deter-
mine. Bhattacharya and Dykstra (1994) oﬀer a bound on the probability
of rejecting the null hypothesis by the LRT. If one uses the bound the
test is conservative, rejecting less, and resulting in small power.
In this section we pose the testing problem in terms of ν1, ν2. It can be
shown that the power function of the WMW test increases as ν2 varies
from −∞to ∞when ν1 is zero. This implies that the power of the WMW
test will be below α (the power when ν1 = 0, ν2 = 0) for negative values
of ν2 when ν1 is close to zero. What’s more the power gets worse as ν2
becomes more negative. This is a very poor property and for this reason
the WMW test cannot be recommended for this testing problem.
©2001 CRC Press LLC

Appendix
Proof of Theorem 35.4.1
A complete class of tests for H∗
0 vs. H∗
1 are weak * limits of Bayes tests.
A Bayes test is
ψB(x11, x12) =



1
if

Ω1 C(νν)ex11ν1+x12ν2dξ(νν) > K
γ
if

Ω1 C(νν)ex11ν1+x12ν2dξ(νν) = K
0
otherwise,
(A.1)
where Ω1 = {νν : ν1 > 0} and ξ(νν) is a prior distribution on Ω1.
Clearly each Bayes test is nondecreasing in x11 for ﬁxed x12. It follows
that this property must also hold for ψ∗(x11, x12) which is a weak ∗limit
of a sequence of Bayes tests. Therefore the class of tests C, consisting of
tests which are nondecreasing in x11 for ﬁxed x12, is a complete class.
Now it can be shown that if ψ satisﬁes (35.4.4) and (35.4.5) it is
nondecreasing in x11 for ﬁxed x12. Hence ψ ∈C.
Next we prove conditional unbiasedness of size α. The size α property
is condition (35.4.6). Let βψ(ν1, ν2) denote the power function of the
test ψ. The partial derivative of the conditional power function with
respect to ν1 is
∂β
∂ν1
= EννX11ψ(X) −EννX11Eννψ(X).
(A.2)
Conditions (35.4.4) and (35.4.5) imply that ψ is monotone in the partial
sums, Y1 = X11, Y2 = X11 + X12. The partial sums of these variables
with the hypergeometric distribution of (35.4.2) are associated. See Co-
hen and Sackrowitz (1994). Hence (A.2) ≥0 for every (ν1, ν2).
To demonstrate conditional unbiasedness it suﬃces to show that the
conditional power function when ν1 = 0, βψ(0, ν2) is minimized at ν2 =
0. Note that the derivative of the function βψ(0, ν2) with respect to ν2
is
Eν1=0,ν2X12ψ(X11, X12) −Eν1=0,ν2X12Eν1=0,ν2ψ(X11, X12). (A.3)
Also note that
βψ(0, ν2) = Eν2

Eνν=0ψ(X11, X12)|X12

= Eν2g(X12).
(A.4)
Now we use condition (35.4.7) and (A.3) to conclude that βψ(0, ν2) has
an extremum at ν2 = 0. Furthermore condition (35.4.8), (A.4) and a
©2001 CRC Press LLC

sign change theorem for a one dimensional exponential family density
imply that the extremum is a minimum and βψ(0, ν2) is nondecreasing
as ν2 moves away from 0 in both directions. These properties of the
conditional power function of ψ along with (A.2) ≥0 imply conditional
and unconditional unbiasedness.
Proof of Theorem 35.4.2
We prove the theorem by showing there exists a test of the form (35.2.4)
satisfying (35.4.4)–(35.4.8). Clearly any test of the form (35.2.4) satisﬁes
(35.4.4) and (35.4.5). Next we show that any such test satisﬁes (35.4.8).
Note that for ﬁxed X12 = Y2−Y1 < C2−C1, the test rejects if X11 > C1.
However the conditional distribution of X11|X12 when νν = 0 is stochasti-
cally decreasing in X12. This implies that g(X12) increases as X12 varies
from C2 −C1 to smaller values. Furthermore when X12 > C2 −C1, the
test rejects when X11 + X12 > C2. But the conditional distribution of
X11 +X12|X12 is stochastically increasing as X12 increases. This implies
that g(X12) increases as X12 increases from C2 −C1 and establishes
condition (35.4.8).
To show that (35.2.4) satisﬁes (35.4.6) and 35.4.7) we note ﬁrst that
tests in (35.2.4) that are non-randomized in one of the two directions
can be written as
ψ(Y; C1, C2, 0, γ2) = ψ(Y; C1 + 1, C2, 1, γ2) and
ψ(Y; C1, C2, γ1, 1) = ψ(Y; C1, C2 −1, γ1, 0).
(A.5)
Now note that there exists a test ψ∗
1(Y1) deﬁned by
ψ∗
1(Y1) =



1
if Y1 > C∗
1
γ∗
1
if Y1 = C∗
1
0
if Y1 < C∗
1,
(A.6)
such that 0 ≤γ∗
1 < 1 and E0ψ∗
1(Y1) = α. Also there exists a test
ψ∗
2(Y2) =



1
if Y2 > C∗
2
γ∗
2
if Y2 = C∗
2
0
if Y2 < C∗
2,
(A.7)
such that 0 < γ∗
2 ≤1 and E0ψ∗
2(Y2) = α. In the above deﬁnitions of
ψ∗
1, ψ∗
2, if either test is non-randomized then γ∗
1 = 0 or γ∗
2 = 1 respec-
tively.
We next illustrate that there exists a collection of tests ψβ(Y) =
ψ

Y; C1(β), C2(β), γ1(β), γ2(β)

, indexed by β ∈[0, 1] such that
©2001 CRC Press LLC

ψβ(Y) is of the form (35.2.4)
(A.8)
ψ0(Y) = ψ∗
2(Y2) and ψ1(Y) = ψ∗
1(Y)
(A.9)
E0ψβ(Y) = α for all β ∈[0, 1]
(A.10)
E0(X12 −E0X12)ψβ = cov0(X12, ψβ(Y))
is a continuous function of β.
(A.11)
To see this note
ψ∗
1(Y1) = ψ(Y; C∗
1, U2, 0, γ∗
2) = ψ1(Y)
say
ψ∗
2(Y2) = ψ(Y; L1, C∗
2, γ∗
1, 1) = ψ0(Y)
say,
where U2 is the largest possible value of Y2 and L1 is the smallest possible
value of Y1.
Note that beginning with any test of the form (35.2.4) with 0 ≤γ1 < 1
and 0 < γ2 ≤1 for which E0ψ(Y) = α we can generate a collection of
tests having size α by letting γ1 decrease and γ2 increase (accordingly)
until either γ1 reaches 0 or γ2 reaches 1. When either of these occur we
use (A.4) to rewrite the test so that again γ1 > 0 and γ2 < 1. It can be
seen that this process will end at ψ0(Y) and yield a collection of tests
satisfying (A.7)–(A.10).
Since cov0

X12, ψ∗
1(Y)

≤0 and cov0

X12, ψ∗
2(Y)

≥0 it follows from
continuity that cov0

X12, ψβ(Y)

= 0 for some β ∈[0, 1]. Hence (35.4.6)
and (35.4.7) are satisﬁed.
The proof of Theorem 35.4.3 is based on a limiting result of Van Eeden
(1965). To state her result precisely let
x11ν
x12ν
x13ν
n1ν
x21ν
x22ν
x23ν
n2ν
t1ν
t2ν
t3ν
nν
be a sequence of 2 × 3 tables with limν→∞nν = ∞. Let µjν = EX1jν,
σ2
jν
=
VarX1jν,
ρX11ν,X12ν
≡
correlation (X11ν, X12ν).
Assume
©2001 CRC Press LLC

limν→∞µjν = µj = ∞, limν→∞njν = nj = ∞, limν→∞tjν = tj = ∞,
lim ρX11ν,X12ν = ρX11,X12 exists. Then Uν = (U1ν, U2ν)′, where Ujν =
(Xjν −µjν)/σjµ has a limiting bivariate normal distribution with mean
vector 0 and covariance matrix 
U =

1
ρX11,X12
ρX11,X12
1

.
Proof of Theorem 35.4.3
The normal approximation can be used so that the acceptance region of
(35.2.4) becomes {Z : Z1 ≤K1, Z2 ≤K2} where Zi = (Yi −µyi)/σyi and
K1, K2 are chosen so that (35.4.6) and (35.4.7) are satisﬁed. To deter-
mine the p-value corresponding to such a test procedure, the observed
value of (Z1, Z2), say (B1, B2) must be on the boundary of the accep-
tance region, in Z-space, of a unique unbiased test region. If it can be
established that (35.4.7) is approximated in (35.4.9) with equality, then
the size of the unique unbiased test region with (B1, B2) on its bound-
ary will be the p-value. This unique unbiased test region will either be
(B1, B∗
2) or (B∗
1, B2) as outlined in Theorem 35.4.3. Only one of these
pairs satisﬁes (35.4.9) with equality.
Hence the theorem will be proved if we show that (35.4.9) with equal-
ity, results by using the normal approximation in (35.4.7). Note (35.4.7)
can be expressed as cov

X12, ψ(X11, X12)

= 0. In terms of (Y1, Y2) this
condition becomes
Eνν=0


(Y2 −Y1) −E(Y2 −Y1)

ψ(Y1, Y2 −Y1)

= 0,
(A.12)
and in terms of (Z1, Z2) it becomes
Eνν=0

(Z2σy2 −Z1σy1)ψ∗(Z)

= 0.
(A.13)
Use the fact that
ψ∗(Z) =

0
if Z1 ≤K1, Z2 ≤K2
1
otherwise,
in (A.13) and the fact that EZi = 0 to rewrite A.13) as
 K1
−∞
 K2
−∞
z2σy2

2π(1 −ρ2)
e−(z2−ρz1)2/2(1−ρ2) · e−z2
1/2dz2dz1
−
 K1
−∞
 K2
−∞
z1σy1

2π(1 −ρ2)
e−(z2−ρz1)2/2(1−ρ2) · e−z2
1/2dz2dz1 = 0.
(A.14)
©2001 CRC Press LLC

Integrating in (A.14) and substituting (B1, B2) for (K1, K2) gives (35.4.9)
with equality.
Acknowledgements Research of the ﬁrst author (Arthur Cohen) was
supported by NSF Grant DMS-9400476. Research of the second author
(H. B. Sackrowitz) was supported by NSA Grant 904-901-1-0506.
References
1. Agresti, A. and Finlay, B. (1997). Statistical Methods in the Social
Sciences, Second edition, Dellen c/o Macmillan, San Francisco.
2. Berger, V., Permutt, T., and Ivanova, A. (1998). Convex hull tests
for ordered categorical data. Biometrics 54, 1541–1550.
3. Berger, V. and Sackrowitz, H. B. (1997). Improving tests of sto-
chastic order. Journal of the American Statistical Association 92,
700–705.
4. Bhattacharya, B. and Dykstra, R. L. (1994). Statistical inference
for stochastic ordering. In Stochastic Orders and Their Applica-
tions (Eds., M. Shaked and J. George Shantikumar), pp. 221–249.
Academic Press, New York.
5. Cohen, A. and Sackrowitz, H. B. (1994). Association and unbiased
tests in statistics.
In Stochastic Orders and Their Applications
(Eds., M. Shaked and J. George Shantikumar), pp. 251–274. Aca-
demic Press, New York.
6. Cohen, A. and Sackrowitz, H. B. (1998). Directional tests for one-
sided alternatives in multivariate models. Annals of Statistics 26,
2321–2338.
7. Cohen, A. and Sackrowitz, H. B. (1999a). Testing whether treat-
ment is “better” than control with ordered categorical data: Def-
initions and complete class theorems. Statistics and Decisions (to
appear).
8. Cohen, A. and Sackrowitz, H. B. (1999b). Testing whether treat-
ment is “better” than control with ordered categorical data: An
evaluation of new methodology. Submitted for publication.
9. Eaton, M. L. (1970). A complete class theorem for multidimen-
sional one sided alternatives.
Annals of Mathematical Statistics
41, 1884–1888.
10. Emerson, J. D. and Moses, L. E. (1985). A note on the Wilcoxon-
Mann-Whitney test for 2 × k ordered tables. Biometrics 41, 303–
©2001 CRC Press LLC

309.
11. Grove, D. M. (1980). A test of independence against a class of
ordered alternatives in a 2 × C contingency table. Journal of the
American Statistical Association 75, 454–459.
12. Hilton, J. F. and Mehta, C. R. (1993). Power and sample size cal-
culations for exact conditional tests with ordered categorical data.
Biometrics 49, 609–616.
13. Mehta, C. R., Patel, N. R., and Tsiatis, A. A. (1984). Exact sig-
niﬁcance testing to establish treatment equivalence with ordered
categorical data. Biometrics 40, 819–825.
14. Moses, L. E., Emerson, J. D., and Hosseini, H. (1984). Analyzing
data for ordered categories. New England Journal of Medicine 311,
442–448.
15. Moses, L. E., Emerson, J. D., and Hosseini, H. (1992). Analyzing
data for ordered categories. In Medical Uses of Statistics, Second
edition (Eds., J. C. Bailar III and F. Mosteller), NEJM Books,
Boston, MA.
16. National Bureau of Standards (1959). Tables of the Bivariate Nor-
mal Distribution and Related Functions, Applied Mathematics Se-
ries 50, U.S. Government Printing Oﬃce, Washington, D.C.
17. Rahlfs, V. W. and Zimmerman, H. (1993). Scores: Ordinal data
with few categories – how they should be analyzed. Drug Informa-
tion Journal 27, 1227–1240.
18. Van Eeden, C. (1965). Conditional limit distributions for the en-
tries in a 2 × k contingency table.
In Classical and Contagious
Discrete Distributions (Ed., G. P. Patil), pp. 123-126. Pergamon,
Oxford, England.
19. Zar, J. H. (1996). Biostatistical Analysis. Prentice Hall, New Jer-
sey.
©2001 CRC Press LLC

36
An Experimental Study of the Occurrence
Times of Rare Species
Marcel F. Neuts
The University of Arizona, Tucson, AZ
ABSTRACT We describe a computer experiment to study the wait-
ing times between the ﬁrst occurrences of rare values in a sequence of
independent, identically distributed random variables taking positive in-
teger values.
The experiment proceeds in two stages; the ﬁrst stage
requires detailed simulation and lasts only until all the alternatives that
have substantial probabilities have occurred. In the second stage, we
generate only the times when potentially new, rare values can occur.
That stage allows us to look eﬀectively and eﬃciently at millions of tri-
als. While the proposed methodology is general, we report some ﬁndings
only for prevalence distributions with a geometric tail.
Keywords and phrases Computer experimentation, coupon collec-
tor’s problem, waiting times between the occurrences of rare species
36.1
Statement of the Problem
We consider a sequence of independent, identically distributed random
variables {Xn} taking values in the set of natural numbers. The prob-
abilities P[Xn = j] = p(j), j ≥1, are assumed to be positive and,
without loss of generality, non-increasing in j. We are interested in the
occurrence times {Z(k)} of new values in the sequence {Xn}.
When the density {p(j)} concentrates only on the integers 1, . . . , m,
there is an extensive literature on various related waiting time problems
under the heading of as the Coupon Collector’s Problem. Boneh and
Hofri (1997) give a recent survey of that literature. An algorithm for the
computation of the probability density of Z(m) and related moments is
given in Neuts and Carson (1975) or in Neuts (1995).
Because of the involved dependence on the past of the successive new
©2001 CRC Press LLC

values in the observed sequence, very little about the coupon collector’s
problem is amenable to classical analysis. When many of the quantities
p(j) are small, when m is large and, a fortiori when it is inﬁnite, the
capabilities of exact algorithms are limited. The problem is therefore
well-suited to exploration by computer experiments.
In this paper, we focus on one experiment only. However, the proposed
method is also applicable to other aspects of the coupon collector’s prob-
lem. For example, in the case of ﬁnite m, when many p(j) are small,
even in a long string of (simulated) data, the corresponding j-values
typically occur only once. It would be interesting to see what can be
learned about the relative magnitudes of the corresponding p(j) from
the orders of appearance and the times between the occurrences of these
isolated rare values. The method proposed here is also useful in that
investigation.
Speciﬁcally, we experimentally study the relationship between the
asymptotic
behavior
of
the
sequence
{p(j)}
and
that
of
the
inter-occurrence times Y (k) = Z(k) −Z(k −1).
36.2
The Design of the Experiment
For operational purposes, we deﬁne two integers N0 and N, where
N0 = min

min

j :
j

ν=1
p(ν) ≥1 −β

, min {j : p(j) ≤β1}

,
(36.2.1)
and
N = min

j :
j

ν=1
p(j) ≥1 −γ

,
(36.2.2)
where β and β1 are small positive quantities with typically β = .05, .02,
or .01. The second quantity, β1, is much smaller than β and typically is
set to .001 or .0005. The constant γ is very small, say, γ = 10−12.
The indices j ≤N0 make up the set of common indices. The purpose
of β1 is to avoid including too many alternatives of small probability
among the common indices. The index N serves to eliminate alterna-
tives so rare that they are unlikely even to occur in millions of trials.
Provided that γ is of a low order of magnitude, our experimental results
©2001 CRC Press LLC

are insensitive to the speciﬁc choice of N. The indices j with N0 <
j ≤N make up the set of ra re i n d i ce s .
In the simulation, we use the sequence {p(j)} truncated at N. The
corresponding computer array is renormalized to sum to one, but the
eﬀect of doing so is negligible. An alternative is to place the neglected
mass (which is smaller than γ) at an extra alternative N +1 correspond-
ing to a s u pe r - ra re event – the sighting of a unicorn, perhaps? Again,
that is a matter of choice, but it is of little or no consequence.
The deﬁnitions of common and rare species are clearly operational and,
therefore, somewhat arbitrary. In some applications, these deﬁnitions
require revision. For many plausible probability densities {p(j)}, N0 is
rarely in excess of 75 while N may be a few hundreds (but, exceptionally,
can much larger).
In Stage 1 of the experiment, we generate and examine variates with
the (truncated) density {p(j)} until al l common indices have occurred
at least once. In Stage 2, we generate the ﬁrst occurrence times of rare
indices by an eﬃcient procedure described in Section 3.
Variates from a discrete probability density are generated by the clas-
sical Alias Method, see e.g., Devroye (1986). The eﬃcacy of that method
in generating a large set of variates from a discrete distribution more than
oﬀsets the modest overhead computation time needed to set up the alias
tables. In Stage 1, we generate variates from {p(j)} and we keep a list
of the indices that have already appeared. A next variate is generated
and checked against those in the list. If it is new, it is added to the list.
After a while, checking recurring common indices wastes too much
time. The redundant eﬀort in checking successive variates is precisely
the reason for the second stage. Running the code of Stage 1 to check,
say, 50,000,000 variates lacks elegance and takes a prohibitively long
processing time. In typical runs, Stage 1 requires around 1,000 variates,
rarely more. On a good PC, Stage 1 thus takes a negligible processing
time. Incidentally, the distribution of the waiting time to see all com-
mon events, that is here the number of variates required to complete
Stage 1, can be computed by the procedure in Neuts (1995). As dis-
cussed there, when the successive trials are performed at the events of
a Poisson process of rate one, we obtain a simpler form of the waiting
time distribution. That form is better suited to compute, say, an (ap-
proximate) high percentile of the waiting time distribution. Given the
rapidity of execution of Stage 1, we did not consider a further theoretical
study of its duration to be worthwhile.
Rare indices may also occur during Stage 1. We keep these indices in
a separate list, say, RI1, (rare indices in Stage 1). After completion of
Stage 1, the probability that a subsequent variate from {p(j)} already
occurred during Stage 1 is given by
©2001 CRC Press LLC

N0

j=1
p(j) +

RI1
p(j) = 1 −β∗.
(36.2.3)
Equivalently, β∗is the probability that an index has not shown up during
Stage 1.
Before starting Stage 2, we delete all indices up to N0 or belonging to
the set RI1. We normalize the resulting array of p(j) by dividing each
term by β∗. That is the conditional density of a random index given
that it did not appear in Stage 1. We compute the alias table of that
density.
36.3
Stage 2 of the Experiment
We denote the successive inter-occurrence times of new labels in Stage
2 by Y ∗(k) for k ≥1. In studying asymptotic behavior, we use only
these occurrence times. To make that clear, we add an asterisk to the
random variables related to Stage 2. The ﬁrst new label appears at the
ﬁrst success in a sequence on independent Bernoulli trials with success
probability β∗. It is therefore found after a geometrically distributed
waiting time Y ∗(1) with parameter β∗. We place that label in a list RI2
of new labels found in Stage 2. The waiting time to the next opportu-
nity for a new label has the same geometric distribution. We generate a
variate from the conditional density and, if it is not in RI2, it is added
to that list. The corresponding waiting time is then Y ∗(2). If the la-
bel belongs to RI2, we continue generating geometric waiting times and
variates until a new label is found and added to RI2. The waiting time
Y ∗(2) is then a random sum of independent geometric variables with
parameter β∗. Subsequent inter-occurrence times Y ∗(k) are constructed
in the same way. Since the list RI2 grows, the successive Y ∗(k) are ran-
dom sums of geometric variables in which the sequence of the numbers of
summands is stochastically increasing. The sequence of the interarrival
times {Y ∗(k)} is therefore also stochastically increasing.
Thus, the sequence Y ∗(k) is generated by drawing successive geometric
variates and, for each, a variate K from the conditional density of a
random index that did not appear in Stage 1. To see whether it is new,
the variate K needs to be checked against the short list RI2 only. Since
β∗is typically small, the geometric variates are large. The Y ∗(k) are
obtained without the onerous checking of repeating common labels. We
keep track of their successive partial sums Z∗(k) and we stop when the
sum exceeds a speciﬁed number such as 20,000,000 or 50,000,000.
©2001 CRC Press LLC

36.4 
Findings
So far, we have primarily examined the sequence of inter-occurrence
times Y ∗(k) for distributions with heavy geometric tails, that is with a
parameter smaller than 0.1. For these cases, we ﬁnd a suﬃciently large
number of new indices (typically on the order of two hundred) with the
stopping time set equal to 50,000,000.
We plotted the logarithms of
the Y ∗(k) and found that these consistently exhibit a close to linear
behavior. The parameters of the linear regression lines ﬁtted to these
simulation results show little variation in replications of the experiment.
That variation is consistent with the relatively small sample sizes, that
is the number of rare indices found even in as many as 50,000,000 tri-
als. The number of rare indices found is remarkably consistent across
replications. As an illustration, we display the parameters of the linear
ﬁts, the sample sizes, and the stopping times for 25 replications in Table
36.1. In that example, the density {p(j)} is geometric.
Our results exemplify the challenges in examining asymptotic behavior
by simulation. Even if the diﬃculties associated with simulating rare
events can be avoided, the asymptotic parameters must typically be
estimated from few data points. The type of sample variability seen in
Table 36.1 is representative.
Particularly for short-tailed prevalence distributions, such as Poisson,
the combination of rarity and small sample sizes adds to the diﬃculty of
experimentation. On the other hand, the asymptotic behavior may be
most interesting for heavy-tailed distributions. We explored cases of the
zeta density given by
p(j) = [ζ(λ)]−1j−λ, forj ≥1.
(36.4.1)
With λ = 3 and γ = 10−11, we ﬁnd that there are N = 9405 labels
in all of which only N0 = 20 are common. In a particular run, Stage
1 took 6,369 steps and that number does not vary signiﬁcantly across
replications. In that same run, only 7 rare labels showed up in Stage 1
and the smallness of that number is also typical. As is to be expected,
heavier tailed distributions are somewhat more generous in producing
rare labels. With a stopping time of 50,000,000, that run produced 418
labels during Stage 2 and that number again does not vary signiﬁcantly
across replications.
The logarithms of the Y ∗(k) now consistently exhibit a behavior that
is well-captured by a regression line of the form
y = a + bkc,
(36.4.2)
©2001 CRC Press LLC

TABLE 36.1
Parameters of the linear regression lines for the logarithms of the
times between occurrences of rare indices in 50,000,000 trials using
a geometric prevalence distribution with success probability 0.05.
The ﬁrst column gives the number of the replication; the second,
the intercept, and the third the slope of the linear regression. J is
the number of rare indices found in 50,000,000 trials in stage 2.
The ﬁnal index gives the time at which the simulations stopped,
the time variable having exceeded 50,000,000
Nr
a
b
J
Final index
1
-.279E-02
.706E-01
230
50000017
2
-.424E-02
.796E-01
218
50000009
3
-.421E-02
.788E-01
218
50000005
4
-.346E-02
.736E-01
227
50000000
5
-.382E-02
.746E-01
229
50000031
6
-.370E-02
.739E-01
226
50000009
7
-.292E-02
.693E-01
239
50000004
8
-.456E-02
.802E-01
215
50000050
9
-.359E-02
.725E-01
229
50000002
10
-.410E-02
.752E-01
224
50000011
11
-.348E-02
.721E-01
228
50000048
12
-.347E-02
.729E-01
230
50000005
13
-.396E-02
.757E-01
223
50000121
14
-.395E-02
.782E-01
216
50000017
15
-.329E-02
.706E-01
234
50000009
16
-.481E-02
.828E-01
211
50000002
17
-.250E-02
.667E-01
242
50000038
18
-.431E-02
.767E-01
222
50000021
19
-.592E-02
.847E-01
215
50000060
20
-.371E-02
.724E-01
231
50000002
21
-.305E-02
.703E-01
235
50000050
22
-.335E-02
.718E-01
226
50000018
23
-.416E-02
.794E-01
214
50000000
24
-.340E-02
.720E-01
229
50000001
25
-.427E-02
.750E-01
228
50000037
©2001 CRC Press LLC

where the variability of the parameter estimates is similar to that in
Table 36.1. However, in the interest of brevity, we do not display results
for replications of the experiment with the zeta density.
Acknowledgements The author thanks Professor Michael Rosenzweig
for stimulating discussions on possible applications. This research was
supported in part by NSF Grant Nr. DMI-9306828.
References
1. Boneh, A. and Hofri, M. (1997). The coupon-collector problem re-
visited – engineering problems and computational methods. Com-
munications in Statistics — Stochastic Models 13, 36–66.
2. Devroye, L. (1986).
Non-Uniform Random Variate Generation.
Springer-Verlag, New York, Berlin.
3. Neuts, M. F. and Carson, C. C. (1975). Some computational prob-
lems related to multinomial trials. Canadian Journal of Statistics
3, 235–248.
4. Neuts, M. F. (1995). Algorithmic Probability: A Collection of Prob-
lems. Chapman and Hall, London.
©2001 CRC Press LLC

37
A Distribution Functional Arising in
Epidemic Control
Niels G. Becker and Sergey Utev
Australian National University
Institute of Mathematics, Novosibirsk University, Russia
ABSTRACT The properties of a relatively new functional, [Becker
and Utev (1998)], are investigated, with emphasis on extreme values
with respect to a probability distribution. Scale invariance is established
and degenerate distributions are shown to give extreme values.
The
functional gives, for a certain strategy of vaccination, the fraction of
the households that must be immunized to prevent epidemics within
a community consisting of a large number of households. Interest lies
in seeing how the distribution of household size aﬀects the estimate of
the critical immunity coverage. The functional also gives the fraction of
individuals that must be immunized to prevent epidemics in a community
of individuals who mix uniformly but whose susceptibility is age-speciﬁc.
Interest then lies in seeing how the distribution of susceptibility aﬀects
the estimate of the critical immunity coverage.
Keywords and phrases Epidemic control, functional of distributions,
inequalities for functionals
37.1
Introduction
Certain functionals of probability distributions have an important role in
probability theory and mathematical statistics. Classical examples are
the Fisher information I(X) =

[(ln f)′]2fdx and the Shannon entropy
H(X) = −

f ln fdx.
Their study is of interest both for their own
sake and for the contribution they make to vital practical problems.
See Huber (1981) for applications of Fisher information in statistics,
Ellis (1985) for applications of Shannon entropy in statistical mechanics,
Barron (1986), Chen (1988), and Cacoullos, Papathanasiou, and Utev
(1994) for applications in limit theorems of probability theory.
©2001 CRC Press LLC

Here we study a distribution functional that is important as an es-
timate of the immunity coverage required to prevent epidemics in a
community of households [see Becker and Utev (1998)].
In part our
motivation is to see how the distribution of household size in the com-
munity aﬀects this estimate. The same functional is an estimate of the
immunity coverage required to prevent epidemics in a community of in-
dividuals with varying susceptibility to infection. Our interest then lies
in seeing how the distribution of susceptibility over community members
aﬀects the estimate.
We ﬁrst introduce the functional of interest, then establish some of its
properties and ﬁnally discuss some insights these results provide in the
context of epidemic control.
The Functional of Interest
Let X be any nonnegative random variable with ﬁnite expectation EX >
0, and denote its distribution function by F. Fix a real a, with 0 < a < 1,
and for convenience let a = 1 −a. The functional of main interest is
v(X) := 1 −
1
θ(X) EX ,
(37.1.1)
where the functional θ(X) is deﬁned by
θ(X) :
such that
a = E exp[−a X θ(X)] .
(37.1.2)
It is important to remember that the argument X in θ(X) and v(X)
only reﬂects their dependence on F. That is, they are not random vari-
ables. Sometimes the notation θ(F) and v(F) is more convenient than
θ(X) and v(X).
Note that the functional θ(X) exists and is unique when P(X = 0) <
a. For P(X = 0) ≥a, it is convenient to take θ(X) = ∞.
37.2
Properties of the Functional
A substantial part of the theorem below deals with local behaviour with
respect to a perturbation of the form X + δhY , for suﬃciently small
positive h > 0, where δh is a stochastic process which is independent of
the pair (X, Y ) and for which we assume that
Eδh = h
for any h > 0
and
h−1(1 −E exp[−tδh]) →Z(t) as h →0, for any t ≥0. (37.2.1)
©2001 CRC Press LLC

We consider Y ≥0 and 0 ≤δh ≤1. The following parameters will be
useful:
V (h) = v(X + δhY ),
ZX = E[X exp(−θaX)]
EX E exp(−θaX),
pX = E2X
EX2
and
∆X = aVar(X)
E2X
.
The main properties of the functional v(X) are as follows:
THEOREM 37.2.1
Using the above notation we have:
1.
For any c > 0 we have v(cX) = v(X) . In particular, v(c) = v(1) =
1 + a/ ln a.
2.
v(X) ≥v(EX), with equality if and only if X has a degenerate
distribution.
3.
v(X) ≤v(Bin[1, pX]) = 1 −(a + ∆X)/ ln(a −∆X)+, where x+ =
max(x, 0).
4.
V ′(0) has constant sign for all Y ≥0 if and only if X has a degen-
erate distribution.
5.
The following statements are equivalent:
S1. V ′(0) has constant sign for all non-negative Y that are inde-
pendent of X ;
S2. X is degenerate or Z(t)/t ≥ZX for all t > 0
Discussion
Assertion 1 indicates scale invariance, while Assertion 2 implies that the
distribution functional v(X) assumes its smallest value for degenerate
distributions. Assertions 2 and 3 indicate that v(X) can be approxi-
mated by v(EX) when the relative variability of X is small; that is,
when Var(X)/E2X is small.
To obtain a richer result about extremes of the functional v(X), we
consider its local behaviour with respect to a perturbation of the form
X + δhY. Assertion 4 states that the only stable points of V (h) with
respect to perturbations in the direction Y , where Y is an arbitrary
positive random variable, are degenerate distributions. That is, for each
strictly positive random variable X with a nondegenerate distribution
there exist strictly positive random variables Y1 and Y2 such that
v(X + δhY1) < v(X) < v(X + δhY2)
for all 0 < h < h0.
(37.2.2)
©2001 CRC Press LLC

Two particular types of perturbation promote understanding: the non-
random δh ≡h, for which Z(t) ≡t, and Bernoulli chaos with P(δh =
1) = h = 1 −P(δh = 0), for which Z(t) ≡1 −e−t. Note that the
Bernoulli type perturbation is equivalent to a perturbation in the distri-
bution. That is, given distributions FX and GU let Y = U −X. Then
the distribution of X + δhY is (1 −h)FX + hGU and (37.2.2) is read as:
for each nondegenerate distributions F there exist distributions H1 and
H2 such that
v[(1 −h)F + hH1] < v(F) < v[(1 −h)F + hH2]
for all 0 < h < h0.
Convolutions play a central role in the study of functionals. For ex-
ample, the inequalities
I(X + Y ) ≥I(X)
and
H(X + Y ) ≤H(X)
for all independent X and Y
(37.2.3)
are major features of the Fisher information I and Shannon entropy H;
see for example Barron (1986). Assertion 5 of the theorem provides a
local analogue of (37.2.3) for the functional v(X). In particular, we show
that when only perturbations along independent directions Y are con-
sidered, the statement (37.2.2) is no longer true in general. For example,
for a nonrandom perturbation, we have
v(X + εY ) < v(X)
for all ﬁxed ε such that 0 < ε < ε0(Y ) ,
(37.2.4)
for each non-negative Y that is independent of X. In contrast, for a
Bernoulli perturbation the local behaviour along independent positive
directions Y is approximately the same as local behaviour along arbitrary
positive directions Y .
Note that the functional v(X) does not satisfy the property (37.2.4)
globally, that is, for all Y independent of X and all ε > 0. Suppose, to
the contrary, that
v(X + Y ) ≤v(X)
(37.2.5)
were true for all independent random variables X and Y, such that X
has a nondegenerate distribution.
Choose a Y with a nondegenerate
distribution and consider a sequence of random variables X1, X2, . . . such
that Xn →1. From (37.2.5) it would follow that v(1 + Y ) ≤v(1) which
contradicts Assertion 2.
©2001 CRC Press LLC

37.3
Proof of the Theorem
Proof of Assertion 1.
By direct substitution of X′ = cX into
(37.1.1).
Proof of Assertion 2. From (37.1.1) it follows that we need to show
θ(X) ≥θ(EX) .
(37.3.1)
Jensen’s inequality gives
exp[−a EX θ(EX)] = a = E exp[−a X θ(X)] ≥exp[−a EX θ(X)] ,
with equality if and only if X has a degenerate distribution. This implies
(37.3.1), and hence the result.
Proof of Assertion 3.
By Assertion 1 we can assume EX = 1
without loss of generality.
Then pX = 1/EX2 ≤1 and the random
variable X0 = Bin(1, pX)/pX is well deﬁned.
Assume that pX > a. Then, for X0 equation (37.1.2) has the unique
solution
θ0 = −ln(a −∆X)
a + ∆X
= −ln[a −a(1/pX −1)]
a + a(1/pX −1)
.
Applying the sharp upper bound E exp(−tX) ≤E exp(−tX0), t ≥0,
see Stoyan (1983), we obtain
E exp[−a Xθ0] ≤E exp[−a X0θ0] = a = E exp[−a Xθ(X)],
which implies θ0 ≥θ(X) and thus the result for the case pX > a.
For the case pX ≤a we show that the only possible upper bound is
v(X) = 1. Deﬁne a sequence of random variable Xη = η + wBin(1, u),
with u and w chosen so that wu = 1 −η and w2u(1 −u) = Var(X),
and consider this sequence as η ↓0. Then equation (37.1.2) implies that
θ(Xη) →∞and therefore v(Xη) = 1 −1/θ(Xη) →1 as η ↓0.
Proof of Assertion 4. Deﬁne the functions
T(h) = θ(X + δhY )
and
V (h) = v(X + δhY ) ,
where the domains of T and V contain an open interval around the
origin. Both T and V are functions of a real variable, and both depend
on X and Y .
Applying deﬁnition (37.1.2) to X +δhY , replacing θ(X +δhY ) by T(h)
and diﬀerentiating at h = 0 yields
0 = −E{[aX T ′(0) + Z(θaY )] exp(−θaX)} .
©2001 CRC Press LLC

This gives
T ′(0) = −E[exp(−θaX)Z(θaY )]
E[aX exp(−θaX)]
,
and hence
V ′(0) = θaEY E[X exp(−θaX)] −EX E[Z(θaY ) exp(−θaX)]
aθ2 E2X E[X exp(−θaX)]
. (37.3.2)
When X is a constant, almost surely, equation (37.3.2) and Z(y) ≤y
give
V ′(0) ≥θaE[Y ] −EZ(θaY )]
aθ2E2X
≥0
for all Y ≥0.
This establishes the necessary condition of Assertion 4.
Now suppose that V ′ has constant sign. Without loss of generality
let θa = 1 and let H(X, Y ) denote the numerator of (37.3.2).
Take
Y = yID, for a set D, and deﬁne U = E[X exp(−X)] −exp(−X) EX.
Since Z(y)/y →1 as y ↓0, it follows that
lim
y↓0 H(X, yID)/y = E(IDU)
has constant sign for every set D.
Substituting the particular choices D = {U ≥0} and D = {U ≤0},
in turn, shows that either Pr(U ≥0) = 1 or Pr(−U ≥0) = 1 . Each
of these implies a degenerate distribution for X. For example, suppose
Pr(U ≥0) = 1. That is,
E[X exp(−X)] ≥exp(−X) EX,
almost surely.
Then
E[X exp(−X)] ≥exp(−xEI) EX ,
(37.3.3)
where
xEI = inf

x :
 x
0
ydF(y) > 0

is the essential inﬁmum with respect to the ﬁnite positive measure given
by dµ(x) = xdF(x).
Equation (37.3.3) can be written
 ∞
0
[exp(−x) −exp(−xEI)] x dF(x) ≥0 ,
which implies that F is degenerate.
A similar proof shows that Pr(−U ≥0) = 1 also implies that F is
degenerate.
Proof of Assertion 5. By Assertion 4, Statement S1 is valid when X
is degenerate. Therefore, without loss of generality, we assume that X
©2001 CRC Press LLC

is nondegenerate. Since Y is independent of X, it follows from (37.3.2)
that Statement S1 is equivalent to the statement
S3.
[EZ(θaY )/E(θaY )] −ZX has constant sign for all positive Y.
Note that ZX ≤1, with equality if and only if X is degenerate. We
assumed X to be nondegenerate, so that ZX < 1. Since Z(y)/y →1 as
y →0, it now follows that S3 is equivalent to the statement
S4. EZ(θaY ) ≥E(θaY )ZX for all positive Y
Since Statement S4 is equivalent to Statement S2 the assertion is
proved.
37.4
Application to Epidemic Control
We now point out some insights provided by these results in the context
of epidemic control.
Consider an epidemic model describing the spread of infection to sus-
ceptible units of a community. By a unit we might mean an individual
or a household, depending on the application. Assume that all infectious
individuals have the same potential to transmit the disease to susceptible
units, but susceptibility to infection may diﬀer among units. In the case
of individuals the susceptibility may depend on their age, for example,
while in the case of a household the susceptibility to infection depends
on its size.
Let i(t) and r(t) denote the proportion of infective and removed in-
dividuals at time t, respectively. We classify susceptible units according
to type w ∈A. The proportion of type-w units that are still susceptible
at time t, sw(t), is related to i(t) and r(t) by
dsw(t)
dt
= −βf(w)sw(t)i(t)
and
dr(t)
dt
= γi(t) ,
(37.4.1)
where f(w) indicates the susceptibility of units of type w.
Standard
arguments show that there exist limits satisfying
sw(∞) = sw(0) exp[−θf(w)r(∞)] ,
where θ = β/γ. Let µ(w) denote a measure for the structure variable w
such that s(t), the proportion of all individuals who are susceptible at
time t, is given by
s(t) =

sw(t)dµ(w) .
Then s(∞), the eventual proportion of susceptibles, is given by the so-
©2001 CRC Press LLC

lution of
s(∞) =

sw(0) exp{−θf(w)[1 −s(∞)]}dµ(w) .
This equation can be written in the form of equation (37.1.2) by setting
a = s(∞), provided µ is such that
Pr{X = f(w)} = sw(0)dµ(w)
(37.4.2)
deﬁnes a proper probability distribution for the random variable X.
We now introduce two particular settings, to which the model (37.4.1)
applies, and associated control strategies for which the critical immunity
level is given by (37.1.1).
A Household Structure
Consider a community consisting of a large number of households, in
which the vaccination strategy is to select households at random and to
immunize every member of each selected household against the infectious
disease. Let f(w) = w, w ∈A = {1, . . . , n}, and let the measure µ be
given by µ(w) = w/µ
H, where µ
H is the mean household size. In this
setting sw(t) of (37.4.1) denotes the proportion of households of size w
that are still susceptible at time t.
Interest is in knowing how high the immunity coverage needs to be to
make the probability of a major epidemic zero. Becker and Dietz (1995)
showed that the critical immunity level for a disease that is highly infec-
tious within households is given by (37.1.1), where θ is the mean number
of individuals from other households that an infective would infect if ev-
eryone were susceptible and the random variable X is the size of the
household of an individual selected randomly from the community. The
parameter θ needs to be estimated. Suppose that the eventual propor-
tion s(∞) is observed for an epidemic arising from an introduction of
the disease into a totally susceptible community. Then θ can be esti-
mated from estimating equation (37.1.2). Furthermore, the distribution
(37.4.2) is proper.
In this setting, Assertion 1 states that the estimate of the critical im-
munity level v is scale invariant with respect to household size. This
means, for example, that we would get the same estimate of v if every
household were double its actual size. By Assertion 2 the estimate of
v is smallest when every household has the same size. The inequality
of Assertion 3 shows that the critical immunity coverage for a commu-
nity with only two diﬀerent household sizes may be quite diﬀerent from
that for a community of uniformly-mixing individuals. This conclusion
supports some examples considered in Becker and Utev (1998). Asser-
tion 4 considers the eﬀect of perturbations of the household distribution
©2001 CRC Press LLC

on the functional v(X). Speciﬁcally, it indicates that there are no other
household distributions with extreme estimates.
An Age Structure
Consider now a community of individuals who mix uniformly, but their
susceptibility depends on some characteristic, such as age. Letting w
refer to age, f(w) reﬂects the susceptibility of individuals aged w and
sw(t) of (37.4.1) is the proportion of individuals aged w that are still
susceptible at time t. Suppose that we estimate the parameter θ using
data from a major epidemic that occurred, over a relatively short period
of time, as a result of a few infectives joining a completely susceptible
population.
Then an estimating equation for θ is (37.1.2), where X = f(W), the
random variable W is the age of an individual selected randomly from
the community and µ(w) gives the age distribution for the community.
Suppose the vaccination strategy is to immunize each individual, in-
dependently, with probability v.
Then the critical immunity level is
estimated by (37.1.1).
Assertion 2 states that the estimate of v is smallest when the suscep-
tibility f(w) is constant over age. This implies that it is wise to take the
age structure into account when estimating the critical immunity cov-
erage. The upper bound of Assertion 3 suggests that the simple lower
bound of Assertion 2 is useful when susceptibility does not change very
much with age. Assertions 4 and 5 investigate the local behaviour of
the estimate with respect to perturbations in susceptibility of the form
fε(W) = f(W) + εY ≡X + εY with both ε and Y positive. According
to Assertion 4 an extreme estimate occurs only when all age groups are
equally susceptible. In contrast, Assertion 5 states that each distribu-
tion for the susceptibility is stable with respect to perturbations in any
“independent” direction.
Acknowledgements Support from the Australian Research Council is
gratefully acknowledged.
References
1. Barron, A. (1986). Entropy and central limit theorem. The Annals
of Probability 14, 336–342.
2. Becker, N. G. and Dietz, K. (1995). The eﬀect of the household dis-
tribution on transmission and control of highly infectious diseases.
©2001 CRC Press LLC

Mathematical Biosciences 127, 207–219.
3. Becker, N. G. and Utev, S. (1998).
The eﬀect of community
structure on the immunity coverage required to prevent epidemics.
Mathematical Biosciences 147, 23–39.
4. Cacoullos, T., Papathanasiou, V., and Utev, S. A. ( 1994). Varia-
tional inequalities with examples and an application to the central
limit theorem. The Annals of Probability 22, 1607–1618.
5. Chen, L. H. Y. (1988). The central limit theorem and Poincare
type inequalities. The Annals of Probability 16, 300–304.
6. Ellis, R. S. (1985). Entropy, Large Deviations and Statistical Me-
chanics. Springer-Verlag, Berlin.
7. Huber, P. J. (1981). Robust Statistics. John Wiley & Sons, New
York.
8. Stoyan, D. (1983).
Comparison Methods for Queues and Other
Stochastic Models. John Wiley & Sons, Chichester, England.
©2001 CRC Press LLC

38
A Birth and Death Urn for Ternary
Outcomes: Stochastic Processes Applied
to Urn Models
Anastasia Ivanova and Nancy Flournoy
University of North Carolina, Chapel Hill, NC
The American University, Washington, D.C.
ABSTRACT
Consider the situation in which subjects arrive sequen-
tially in a clinical trial.
There are K possibly unrelated treatments.
Suppose balls in an urn are labeled with treatments.
When a subject
arrives, a ball is drawn randomly from the urn, the subject receives the
treatment indicated on the ball and ball is then returned to the urn.
There are three levels of response: success, no response, and failure.
If
the treatment is successful, one ball of the same type is added to the
urn. If a failure is observed, one ball of the same type is taken out from
the urn.
Otherwise, nothing is done.
We describe how the proposed
urn process can be embedded in a collection of independent continuous-
time birth and death processes.
We demonstrate the usefulness of this
tool in obtaining limiting results for the continuous-time process that
are equivalent to analogous ones for the urn design.
Furthermore, im-
portant exact statistics are obtained by using a stopping rule to transfer
statistics derived from the continuous-time birth and death processes to
the urn design. Finally, we give a likelihood ratio test for the comparison
of the K treatments.
Keywords and phrases Adaptive designs, clinical trials, experimen-
tal design, birth and death process with immigration
38.1
Introduction
An urn model is a useful model for treatment allocation.
See Kotz
and Balakrishnan (1997) for recent review.
Several adaptive designs
based on an urn models have been studied [see Ivanova and Rosenberger
(2000) for a review]. We consider the situation in which subjects arrive
©2001 CRC Press LLC

sequentially for a clinical trial in which there are K possibly unrelated
treatments with ternary outcomes.
Label each ball in the urn with a
treatment.
A ball labeled i is called a type i ball, i = 1, . . . , K.
The
collection of all balls in the urn is called the urn composition.
In gen-
eral, when a subject arrives, a ball is drawn randomly from the urn; the
subject receives the treatment indicated on the ball, and the ball is then
returned to the urn.
To facilitate the theory, this discrete time urn
process can be embedded into a continuous-time process. The technical
justiﬁcation for this can be found in Athreya and Ney (1972).
In this
paper, the urn process can be imbedded into a family of continuous-
time Markov linear birth and death processes. We emphasize how such
embedding permits the use of well-known results for continuous-time
Markov processes for characterizing an urn composition and for obtain-
ing other statistics of importance in evaluating the results of a clinical
trial.
We present a new adaptive design which is a generalized version of
the birth and death urn design for binary outcomes proposed by Ivanova
et al. (2000).
Both exact and asymptotic results are presented.
In
Section 38.2, we deﬁne the birth and death urn design.
In Section
38.3, we describe how the urn can be embedded in a continuous-time
birth and death process.
In Sections 38.4–38.7, we characterize the
number of successes and trials in the continuous-time birth and death
process. In Section 38.8, we introduce a stopping rule and show how it
can be used to transfer statistics derived from the continuous-time birth
and death process to the urn design.
In Sections 38.9 and 38.10, we
use the continuous-time birth and death process to obtain the limiting
proportion of trials on each treatment and the limiting proportion of
successes on each treatment, respectively. Important asymptotic results
are stated speciﬁcally for the birth and death urn with ternary responses.
However, the proofs of these results are analogous to ones previously
reported, and so are not given here. Section 38.11 gives a likelihood ratio
test for equal success probabilities.
Finally, we make some concluding
remarks in Section 38.12.
38.2
A Birth and Death Urn with Immigration for
Ternary Outcomes
The term birth refers to the addition of a ball to the urn, whereas the
term death refers to the removal of a ball from the urn. Having an urn
process in which balls can be removed from the urn creates the possibility
that even ball type(s) with high success probability will become depleted
©2001 CRC Press LLC

over the course of the experiment. Hence, we deﬁne the urn process to
include the possibility of randomly replenishing the urn through immi-
gration.
A ball is called an immigration ball if, when it is drawn, it is
replaced together with additional balls of other types and no subject is
treated.
We deﬁne a birth and death urn design with immigration for ternary
responses as follows:
An urn contains balls of K types representing K treatments,
and aK immigration balls. When a subject arrives, a ball is
drawn at random and then replaced. If it is an immigration
ball, one ball is added to the urn, with each ball type having
probability 1/K of being the one added; then the next ball is
drawn. The procedure is repeated until the ball representing
an actual treatment is drawn (i.e., not an immigration ball).
If it is a type i ball, the subject receives treatment i.
If a
success is observed, a type i ball is added to the urn; if no
response is observed the urn composition remains unchanged;
if a failure is observed, a type i ball is removed. The process
continues sequentially with the arrival of the next subject.
The parameter a is called the rate of immigration.
Given treatment
i, let pi be the probability of adding a type i ball to the urn; let ri be
the probability that the urn composition remains unchanged entirely,
and let qi be the probability of removing a type i ball from the urn;
qi + ri + pi = 1.
If all qi = 0, i = 1, . . . , K, and a = 0, we have the
pure birth urn design (called the generalized P´olya urn) considered in
Durham, Flournoy and Li (1998).
If ri = 0, i = 1, . . . , K, and a = 0,
we have the birth and death urn design considered in Ivanova (1998). If
ri = 0, i = 1, . . . , K, and a > 0, we have the birth and death urn design
with immigration considered in Ivanova et al. (2000).
For all these
designs, and for the one presented herein as well, a type i ball is added
to the urn following a success on treatment i; so pi = P{success|i}.
In
Ivanova (1998), Ivanova et al. (2000), and here as well, a type i ball
is removed from the urn following a failure on treatment i; so qi =
P{failure|i}.
Here we admit the possibility of no response, and given
this lack of response the urn composition will remain unchanged entirely,
so ri = P{no response|i}. We also permit immigration with rate a.
©2001 CRC Press LLC

38.3 
Emb edding the Urn Scheme in a
Continuous-Time Birth and Death Pro cess
To assist in the characterization of urn process deﬁned in Section 38.2, we
employ the technique of embedding this urn process into a continuous-
time K-type Markov process, which in the present context is a collection
of K independent linear birth and death processes with immigration
(given by Theorem 38.3.1).
Conventionally, continuous-time birth and
death processes describe the behavior of particles which split (produce
or die) at certain time points.
In our context, “particles” correspond
to balls in the urn, and the “splitting” of a type i ball corresponds
to the addition of a type i ball following a success on treatment i or
the immigration of a type i ball, as well as to the removal of a type i
ball following a failure on treatment i.
We call these events instead of
splits.
When a nonimmigration ball is drawn, a trial is initiated with
a treatment being selected for a subject.
If we start the experiment
with the urn having Zi0 balls of type i, then Z0 = {Z10, . . . , ZK0} is
the initial urn composition.
We assume that the vector  Z0 is ﬁxed.
Let Zω = {Z1ω, . . . , ZKω} denote the composition of the urn after
ω consecutive events, where Ziω is the number of type i balls.
Then
{Zω, ω = 0, 1, 2, . . .} is a stochastic process deﬁned on K dimensional
integer lattice.
Denote the continuous-time analog of the urn process by {Z(t) =
(Z1(t), . . . , ZK(t)) | t ≥0}, where Zi(t), i = 1, . . . , K, is a linear birth
and death processes with birth rate pi, death rate qi and immigration
rate a as for the urn process; and Zi(t) are independent of each other.
Consider the event times, τω, ω = 0, 1, . . . , τ0 = 0, of Z(t), and the
discrete process {Z(τω), ω = 0, 1, . . .} deﬁned at the event times.
The
following theorem is key to characterizing the urn design.
THEOREM 38.3.1
Given that Z(0) = Z0, the stochastic processes Zω and {Z(τ ω), ω =
0, 1, . . .} are equivalent.
PROOF 
The proof is similar to one in Athreya and Ney (1972, p. 221).
Characteristics of the number of successes, failures, and trials for the
continuous-time process at time t can be obtained through generating
functions.
In Sections 38.4–38.7, we present results from an artiﬁcial
experiment conducted according to a continuous-time birth and death
process.
But time is a construct that has no meaning in terms of
the urn process, and hence in terms of the experiment.
Therefore,
©2001 CRC Press LLC

using a stopping rule in Section 38.8, we show how to translate statistics
obtained from the continuous-time process back to the urn process at
the time the experiment is stopped.
Furthermore, we give asymptotic
results in Section 38.9 that are the same for both the urn process and
its continuous-time analog.
In clinical trials we are interested in estimating the success probability
for each treatment.
For making inferences concerning the proportion
of successes across the various treatments, joint characterization of the
numbers of successes and trials is required (Section 38.7). However, we
start by obtaining the probability generating function for the number of
successes on treatment i.
38.4
The Probability Generating Function for the
Number of Successes on Treatment i in the
Continuous-Time Birth and Death Process
Because the number of successes at any time depends on the number of
type i balls in the urn at that time, to obtain the probability generating
function for the number of successes on treatment i, we ﬁrst derive a
diﬀerential equation that is satisﬁed by the joint probability generating
function for the number of type i successes and the number of type i balls
in the urn.
Then the marginal solution yields the generating function
for the number of type i successes.
Let Zi(t) be a linear birth and death process with immigration, that
is, Zi(t) denote the number of balls of type i at time t; Xi(t) denotes
the number of successes on treatment i by or at time t; Yi(t) denotes the
number of failures on treatment i, and Ni(t) denotes the number of trials
on treatment i. Because the processes corresponding to each ball type in
continuous-time are independent, we only consider type i. Also we often
suppress the subscript i for notational simplicity when it is not needed
for understanding. Let P Z
z0→z(t) (P X
x0→x(t), P Y
y0→y(t), P N
n0→n(t)) be the
probability of getting z (x, y, n) type i balls (successes, failures, trials)
by time t starting with z0 (x0, y0, n0) type i balls (successes, failures,
trials).
Zi(t) is a Markov process on states 0, 1, . . . with stationary probabil-
ities.
The postulates deﬁning a linear birth and death process with
immigration rate a are [Karlin and Taylor (1974, p. 189)]:
1.
P Z
z→z+1(∆t) = (piz + a)∆t + o(∆t) as ∆t →0, z ≥0
2.
P Z
z→z−1(∆t) = qiz∆t + o(∆t) as ∆t →0, z ≥1
3.
P Z
z→z(∆t) = 1 −(piz + qiz + a)∆t + o(∆t) as ∆t →0, z ≥0
©2001 CRC Press LLC

4.
P Z
z→z(0) = δij.
Note that the process Zi(t) is Markovian, but the processes Xi(t),
Yi(t), Ni(t) are non-Markovian because the transition rates at t depend
on Zi(t). To deal with the dependency of Xi on Zi, we expand on a hint
by Cox and Miller (1965, page 265) and consider the two-dimensional
process of the number of successes on treatment i and the number of type
i balls, (Xi(t), Zi(t)), having state space the set of nonnegative integer
pairs (x, z).
Let P XZ
x0,z0→x,z(t) be the probability of having x successes
and z balls by time t given there were x0 successes and z0 balls at time
0.
Deﬁne also the joint probability generating function for the number of
balls and the number of successes,
GXZ
i
(v, s, t | Zi0) =
∞
x = x0

∞
z = 0

P XZ
x0,z0→x,z(t)vxsz,
where |v| ≤1, |s| ≤1. To obtain forward diﬀerential equations, assume
now that the joint process is at (x, z), z > 0, x > 0, at time t + ∆t.
The following theorem gives the diﬀerential equation for the probability
generating function.
This equation is solved for the case in which the
urn is initialized by the immigration process, i.e., Zi0 = 0.
This result
is useful for any initial urn composition because with immigration, the
eﬀect of the initial urn composition is only substantial in an experiment
with a very small sample size.
The limiting results do not depend
on the initial urn composition, but they do depend on the immigration
rate.
Note that if the urn initially is not empty so that Zi0 ̸= 0, the
process Zi(t) is the sum of two independent processes: one is initiated
by the balls which were in the urn from the beginning; another process
is initiated by the balls constantly immigrating into the urn.
The ﬁrst
process is a birth and death process without immigration.
THEOREM 38.4.1
Given the initial urn composition Zi(0) = Zi0, the joint probability gener-
ating function for the number of successes and balls satisﬁes the following
relation:
∂GXZ
i
(v, s, t | Zi0)∂t = (pivs2 −(1 −ri)s + qi)∂GXZ
i
(v, s, t | Zi0)∂s
+a(s −1)GXZ
i
(v, s, t | Zi0).
(38.4.1)
With Xi(0) = 0, the initial condition is
GXZ
i
(v, s, 0 | Zi0) = sZi0.
©2001 CRC Press LLC

Furthermore, when the urn is initialized by the immigration process so
that Zi0 = 0, the probability generating function for the number of suc-
cesses at time t is
GX
i (v, t | Zi0 = 0)
=

2ρe−ρt(2piv −1 + ri + ρ) −(2piv −1 + ri −ρ)

a
piv
× e−
ta
2piv (2piv−1+ri+ρ),
where ρ =

(1 −ri)2 −4piqiv, |v| ≤1.
PROOF Consider the possible transitions which are composite events
involving changes in the number of the type i balls and the number of
successes on treatment i.
There are three that can occur in the time
interval ∆t and result in x successes and z balls. First is an immigration,
i.e., (x, z −1) →(x, z), with probability of occurrence a△t. The second
transition is a trial resulting in a success, i.e., (x −1, z −1) →(x, z).
The probability of a success conditional on a type i ball being drawn
and on Zi(t) = z −1 is (z −1)pi△t.
The third transition is a trial
resulting in a failure, i.e., (x, z + 1) →(x, z).
The probability of a
failure conditional on a type i ball being drawn and on Zi(t) = z + 1 is
(z + 1)qi△t. In addition to these composite events, a trial can result in
no response, i.e., (x, z) →(x, z), with probability zri△t conditional on
a type i ball being drawn and Zi(t) = z.
Finally, no composite events
occur, i.e., (x, z) →(x, z), with probability [1 −(zpi + zqi + zri + a)∆t]
conditional on Zi(t) = z.
From the postulates deﬁning a birth and death process with immi-
gration, it can be shown that P XZ
m,k→x,z(△t)/△t = o(1) for k /∈{z −
1, z, z + 1}, where the o(1) term, apart from tending to zero, is uni-
formly bounded with respect to k and m, and x for ﬁxed z as △t →0.
Then it follows for z > 0, x ≥x0, that
P XZ
x0,z0→x,z(t + △t) = P XZ
x0,z0→x,z(t)(1 −(zpi + zqi + a)△t)
+P XZ
x0,z0→x−1,z−1(t)(z −1)pi△t
+P XZ
x0,z0→x,z+1(t)(z + 1)qi△t
+P XZ
x0,z0→x,z−1(t)a△t + o(△t);
whereas for z = 0,
P XZ
x0,z0→x,0(t+△t) = P XZ
x0,z0→x,0(t)(1−a△t)+P XZ
x0,z0→x,1(t)qi△t+o(△t).
©2001 CRC Press LLC

Taking the derivative and letting △t →0 for z > 0, we obtain
∂P XZ
x0,z0→x,z(t + △t)∂t = −(zpi + zqi + a)P XZ
x0,z0→x,z(t)
+[(z −1)pi + a]P XZ
x0,z0→x−1,z−1(t)
+(z + 1)qiP XZ
x0,z0→x,z+1(t).
(38.4.2)
For z = 0, we obtain
∂P XZ
x0,z0→x,0(t)∂t = −aP XZ
x0,z0→x,0(t) + qiP XZ
x0,z0→x,1(t).
(38.4.3)
The initial conditions are given by Xi(0) = x0, Zi(0) = z0 and
P XZ
x0,z0→x,z(0) = 1
ifx = x0, z = z0
P XZ
x0,z0→x,z(0) = 0
otherwise.
Multiply (38.4.2) and (38.4.3) by vx and by sz and sum over all values
of x and z. Then rearrange terms and use the fact that
s∂∂sGXZ
i
(v, s, t | Zi0) =
∞
x = x0

∞
z = 0

P XZ
x0,z0→x,z(t)vxzsz,
to yield the result (38.4.1).
Assuming Zi0 = 0, apply the algorithm in Anderson (1991) for solving
a Lagrange diﬀerential equation.
The probability generating function
for the number of type i successes is thereby obtained from the solution
to equation (38.4.1) by putting s = 1. It is
GX
i (v, t | Zi0 = 0)
=

2ρ
e−ρt(2piv −1 + ri + ρ) −(2piv −1 + ri −ρ)

a
piv
× e−
ta
2piv (2piv−1+ri+ρ),
where ρ =

(1 −ri)2 −4piqiv, |v| ≤1.
38.5
The Probability Generating Function for the
Number of Trials on Treatment i in the
Continuous-Time Birth and Death Process
Just as we obtained the probability generating function for the number of
successes from the joint probability generating function for the number
©2001 CRC Press LLC

of successes and the number of balls, the probability generating function
for the number of trials can be obtained from the joint probability gen-
erating function for the number of trials and the number of balls.
Let
GNZ
i
(w, s, t | Zi0) denote the joint probability generating function for
the numbers of successes and trials.
THEOREM 38.5.1
If the urn is initialized by the immigration process, the probability gen-
erating function for the numbers of trials is
GN
i (w, t | Zi0 = 0)
=

2ρ
e−ρt(2piw −1 + riw + ρ) −(2piw −1 + riw −ρ)

a
piw
× e−
ta
2piw (2piw−1+riw+ρ),
(38.5.1)
where ρ =

(1 −riw)2 −4piqiw2, |w| ≤1.
PROOF Using the same logic as in the proof of Theorem 38.4.1, the
probability generating function for the number of trials on treatment i
by time t, Ni(t), can be shown to satisfy the diﬀerential equation
∂GNZ
i
(w, s, t | Zi0)∂t
= (piws2 −(1 −riw)s + qiw)∂GNZ
i
(w, s, t | Zi0)∂s
+a(s −1)GNZ
i
(w, s, t | Zi0).
The probability generating function for the number of successes is found
by solving this equation and setting s = 1. The solution for the case in
which Zi0 = 0 is the result.
38.6
The Number of Trials on Treatment i in the
Continuous-Time Birth and Death Process
When Zi0 = 0, the expected number of trials by time t can be obtained
from the generating function (38.5.1) as
E(Ni(t)) =
∂GN(w, t)
∂w
				
w=1
.
©2001 CRC Press LLC

Evaluating the derivative yields
E(Ni(t)) =

a(e(pi −qi )t −(pi −qi)t −1)/(pi −qi)2 whenpi ̸= qi
at2/2
whenpi = qi.
(38.6.1)
The variance can be found easily by evaluating
V ar(Ni(t)) =
∂
∂w

w∂GN(w, t)
∂w
				
w=1
−E(Ni(t))2.
We show how to translate these moments, from the continuous-time
birth and death process to the urn process, in Section 38.8 with the
introduction of a stopping rule.
Now we give a limiting result for the (appropriately standardized)
number type i balls in the continuous-time birth and death process. In
particular, we have
THEOREM 38.6.1
For a > 0,
Ni(t)/E∗(Ni(t))
P→Wi(pi),
where
Wi(pi)
=



Gamma(a/pi, pi/a),
when pi > qi
has characteristic function

cosh

2pi
√−1τ/a
−
a
2pi
when pi = qi
1
when pi < qi
and
E∗(Ni(t)) =



ae(pi−qi)t/(pi −qi)2
when pi > qi
at2/2
when pi = qi
at/(qi −pi)
when pi < qi
is the dominant term in E(Ni(t)).
PROOF The characteristic function of the random variable
t →∞lim [Ni(t)/E∗(Ni(t))]
can be obtained by substituting w = exp[iτ/E∗(Ni(t))] in Gi(w, s, t | 0)
and calculating the limit when t goes to inﬁnity. The rest of the proof is
similar to the one for binary responses with a > 0 [Ivanova et al. (2000)].
©2001 CRC Press LLC

For the pure birth process with a = 0, Durham, Flournoy and Li (1998)
apply results from Athreya and Ney (1972, pp. 127–130) to show that
Ni(t)/E(Ni(t)) converges to a gamma distribution with shape parameter
zi0 and scale parameter 1/zi0.
38.7
The Joint Probability Generating Function for
the Number of Successes and the Number of
Trials in the Continuous-Time Birth and
Death Process
The observed proportion of successes on treatment i, Xi(t)/Ni(t), is a
natural estimator of the success probability of the ith treatment in the
continuous-time birth and death process.
Therefore, we seek the joint
distribution of successes and trials in order to characterize the propor-
tion of successes.
But again, in order to obtain this joint distribution
function, because the successes and trials depend upon the urn compo-
sition, we must ﬁrst obtain the joint probability generating function for
the total numbers of trials, successes and balls, GXNZ
i
(s, w, t | Zi0). We
obtain this generating function for the case in which the urn is initialized
by the immigration process.
THEOREM 38.7.1
Given the initial urn composition Zi(0) = Zi0,
∂GXNZ
i
(v, w, s, t | Zi0)
∂t
= (pivws2 −(1 −riw)s + qiw)∂GXNZ
i
(v, w, s, t | Zi0)∂s
+a(s −1)GXNZ
i
(v, w, s, t | Zi0).
With Xi(0) = Ni(0) = 0, the initial condition is
GXNZ
i
(v, w, s, 0 | Zi0) = sZi0.
When the urn is initialized by the immigration processes, the joint gen-
erating function for the numbers of successes and trials is
©2001 CRC Press LLC

GXN
i
(v, w, t | Zi0 = 0)
=

2ρe−ρt(2pivw −1 + riw + ρ) −(2pivw −1 + riw −ρ)

a
piw
× e−
ta
2pivw (2pivw−1+riw+ρ),
where ρ =

(1 −riw)2 −4piqivw2, |w| ≤1, |v| ≤1.
PROOF Proceed in an analogous way to the proof of Theorem 38.4.1 to
obtain the diﬀerential equation.
Then the joint probability generating
function for the number of successes and trials, GXN
i
(v, w, t | Zi0 = 0), is
the solution of the partial diﬀerential equation evaluated at s = 1.
The joint distribution of successes and trials is obtained by taking a
Taylor series expansion of GXN
i
(v, w, t | Zi0 = 0) around w = 0 and
v = 0. We have been unable to ﬁnd a closed form solution for the terms
in this expansion. A numerical solution can be obtained pointwise and
used to evaluate a speciﬁc urn design.
38.8
Adopting a Stopping Rule to Convert
Continuous-Time Statistics to the Urn Design
Recall that the continuous-time birth and death process is constructed
to assist in the analysis of the urn process; time is an artiﬁcial construct
and is not related to the treatment of subjects.
The next subject will
receive treatment i with probability proportional to the number of type
i balls in the urn. Therefore, we are interested in the number of balls in
the urn, Zin, rather than in Zi(t). One way to relate subjects and time
is to introduce a stopping rule. We use the stopping rule introduced by
Durham, Flournoy and Li (1998) for the pure birth urn design.
We introduce a ﬁctitious treatment labeled ‘K + 1’ having success
probability pK+1 = 1.
Balls labeled with this hypothetical treatment
are called control balls.
Thus each time a control ball is drawn and
returned to the urn another control ball is added to the urn. Since the
control balls are drawn randomly, they mark time, so to speak. The urn
starts with one control ball.
Use control balls to stop the experiment
as follows:
stop the sequence of trials when η, η ≥1, control balls have
been drawn from the urn.
©2001 CRC Press LLC

We now show how to derive the expected number of subjects receiving
treatment i when the ηth control ball is drawn.
Let Tη = min{t : Z(t) = η + 1} denote the time the ηth control ball
is drawn.
The density of Tη was derived by Durham, Flournoy and Li
(1998). It is
fη(t) = d
dtP{Tη < t} = η(1 −e−t)η−1eη−1.
(38.8.1)
Taking an expectation from the continuous-time birth and death process
conditional on Tη = t and integrating with respect to dfη(t) yields the
unconditional expectation when the ηth ball is drawn from the urn. For
example, using (38.6.1) and (38.8.1), the expected number of trials on
ith treatment when the ηth immigration ball is drawn for pi ̸= qi is
E{N(Tη)} = 0
∞

E{N(Tη)|Tη = t}fη(t)dt
= 0
∞

a
(pi−qi)2 (e(pi−qi)t −(pi −qi)t −1)η(1 −e−t)η−1eη−1dt
= api + qi
pi −qi
 Γ(1 −(pi −qi))η!
Γ(η −(pi −qi) + 1) −1

.
The expected number of successes on treatment i follows from the rela-
tionship E{X(Tη)} = piE{N(Tη)}.
In an analogous manner, time can be integrated out of all generating
functions and expectations that are obtained for the continuous-time
birth and death process.
The observed proportion of successes on treatment i, X(Tη)/N(Tη), is
a natural (but not necessarily unbiased) estimator of the success proba-
bility of the ith treatment at the time the experiment is stopped. How-
ever, we have to worry about dividing by zero because there is posi-
tive probability that N(Tη) = 0.
Therefore, consider the estimator
X(Tη)/(N(Tη) + 1) which will be essentially equivalent for large sam-
ples.
It is straightforward to write the expectation of this estimator,
X(Tη)/(N(Tη) + 1) from the joint generating function as
E

X(Tη)
N(Tη) + 1

= 0
∞

E

X(Tη)
N(Tη) + 1
				 Tη = t

fη(t)dt
= 0
∞
 
0
1
d
dη {GXN
i
(η, w, t|0)}dw


						
η=1
fη(t)dt.
©2001 CRC Press LLC

However, we have been unable to ﬁnd a closed form solution for the con-
ditional expectation E (X(Tη)/(N(Tη) + 1)|Tη = t). It can be obtained
pointwise using numerical integration for evaluating a particular design.
38.9 
Limiting Results for the Prop ortion of Trials
on Treatment i
For the continuous-time birth and death process, let τ1, . . . , τn denote
the trial times (as opposed to the event times considered in Section 38.3).
Now for the urn process, deﬁne the sample sizes on each treatment after
n subjects to be Nin, where, in terms of the continuous-time birth and
death process, Nin = Ni(τn).
Note that, by construction, N1(τn) +
· · · + NK(τn) = n.
Consider the proportion of subjects assigned to the
ith treatment for each i, i = 1, . . . , K, after n subjects, Nin/n.
We are
interested in the limit of Nin/n as n →∞.
It follows from Theorem
38.3.1 that
n →∞limNinn = t →∞limNi(t)j = 1
K

Nj(t),
that is, the limiting proportion of trials on treatment i that is obtained
from the continuous-time analog is equivalent to the limiting proportion
for the urn design. Therefore, we evaluate this limit in the continuous-
time model.
These limits depend on the uniqueness, or lack thereof, of the diﬀer-
ences p1 −q1, p2 −q2, . . . , pK −qK. Without loss of generality assume
that these diﬀerences are ordered: p1 −q1 ≥p2 −q2 ≥· · · ≥pK −qK.
Theorem 6 gives the limiting proportion of subjects assigned to treat-
ment i.
In Case 1, p1 ≥q1; in Case 2, p1 < q1.
Let h denote the
number of treatments having maximum success probability, and for the
treatments with maximum success probability, let ∆denote the diﬀer-
ence between the probability of success and the probability of failure,
i.e., ∆= p1 −q1 = p2 −q2 = · · · = ph −qh.
THEOREM 38.9.1
Case 1 If ∆= p1 −q1 = p2 −q2 = · · · = ph −qh > ph+1 −qh+1 ≥· · · ≥
pK −qK; with ∆≥0, then
Nin
n
P
n →∞→Di =

Wi(∆)
W1(∆)+···+Wh(∆)
when i ≤h
0
when i > h.
©2001 CRC Press LLC

where Wi(∆), i = 1, . . . , h, are random variables deﬁned in Theorem
38.6.1.
Case 2 If ∆= p1 −q1 ≥p2 −q2 ≥· · · ≥pK −qK; with ∆< 0, then
Nin
n
P
n →∞→Di =
1
qi−pi
1
q1−p1 + · · · +
1
qK−pK
.
PROOF The proof is analogous to that of Theorem 2 of Ivanova et al.
(2000).
Note in Case 1 that the joint distribution of (D1, . . . , Dh−1) is a (h−1)-
variate Dirichlet distribution with all parameters equal to a/∆.
Fur-
thermore, if h = 1, the limiting proportion of subjects receiving the
(unique) best treatment is one, and the limiting proportion of subjects
on all other treatments is zero.
In both cases, more precise rates of divergence for the treatments
subsamples can be derived analogous to those given for binary outcomes
by Ivanova et al. (2000).
38.10
Limiting Results for the Proportion of
Successes on Treatment i in the Urn
Again, limiting results for the proportion of success on treatment i using
the urn experiment will be identical to the limiting proportion in the
analogous continuous-time process.
Hence, we use the continuous-time
process to obtain these results.
The maximum likelihood estimator of
the proportion of successes on treatment i in the continuous-time birth
and death model is denoted by ˆpin = Xi(t)/Ni(t). Theorem 38.10.1 gives
the limiting distribution of ˆpin, appropriately standardized. Let p = p1,
and recall that ∆= p1 −q1 = p2 −q2 = · · · = ph −qh > ph+1 −qh+1.
THEOREM 38.10.1
Case 1.
p1 ≥q1.
Consider the set A, A = {1, 2, . . . , h} of treatments with maximum suc-
cess probability.
The vector with components √Nin(ˆpin −p), i ∈A, is
asymptotically multivariate normal with mean zero, variances with com-
ponents piqi, and all covariances zero.
Case 2.
p1 < q1.
The vector with components √Nin(ˆpin −pi), i = 1, . . . , K, is asymptot-
ically multivariate normal with mean zero, variances with components
©2001 CRC Press LLC

piqi, and all covariances zero. Furthermore, the vector with components
√n(ˆpin −pi), i = 1, . . . , K, is asymptotically a multivariate normal with
mean zero, variances with components piqi/Di, and all cross-covariances
equal to zero. The constants, Di, are given in Case 2 of Theorem 38.9.1.
PROOF The proof is analogous to that of Theorem 4.2 in Ivanova et al.
(2000).
38.11
Asymptotic Inference Pertaining to the
Success Probabilities
When the outcome of an experiment is ternary several hypothesis can
be tested. One might be interested in testing whether all pi −qi are the
same. In this paper, we only consider the simplest testing problem that
is of interest:
H0 : p1 = p2 = · · · = pK = p
H1 : notallpiareequal.
The likelihood ratio test provides a general method for hypothesis test-
ing.
Let S =
K
i = 1 Xin, where Xin = Xi(τn).
The likelihood ratio
test statistic is
λn = n ((S/n) log(S/n) + (1 −S/n) log(1 −S/n))
−
K
i = 1

Ni [(Xi/Ni) log(Xi/Ni) + (1 −Xi/Ni) log(1 −Xi/Ni)] .
THEOREM 38.11.1
Under H0,
−2logλn
D
→χ2
K−1.
PROOF The proof is analogous that of Theorem 5.1 in Ivanova et al.
(2000).
THEOREM 38.11.2
For ∆< 0,
−2logλn
D
→χ2
K−1(ϕ),
©2001 CRC Press LLC

where the noncentrality parameter ϕ is
ϕ =
K−1
i = 1

Diδ2
i piqi) −

K
i = 1

Dipiqi)
−1 
K−1
i = 1

Diδipiqi)
2
,
where δ = √n(p1 −pK, . . . , pK−1 −pK, 0).
PROOF The proof is analogous to that of Corollary 5.1 of Ivanova et al.
(2000).
The limiting distribution for the likelihood ratio statistic in the case
∆> 0 is not known.
38.12
Concluding Remarks
The urn model is useful for treatment allocation in clinical trials.
As
a randomized procedure, it provides protection from selection bias and
from confounding eﬀects of unknown covariates. Strategies for changing
the urn composition in response to treatment results can improve the
success rate of future subjects.
This was the motivation behind the
two arm randomized play-the-winner rule [Wei and Durham (1978)] and
its generalization to K treatments [Wei (1979)].
The urn composition
in these treatment allocation procedures is a K-variate Markov chain,
and because the urn composition is Markovian, it can be embedded in
a K-variate continuous-time Markov process. However, for these rules,
there are dependencies among the K processes in the continuous-time
analog of the urn process.
If the urn composition is a K-variate Markov chain, but it is modiﬁed
after observing a treatment outcome in a way such that only the number
of type i balls changes following treatment i, then it can be embedded
in a collection of K independent Markov processes.
This is the case
with the birth and death urn described herein, as well as with those urn
models described by Durham, Flournoy, and Li (1998), Ivanova (1998),
and Ivanova et al. (2000).
The advantages are great, in that much
existing theory facilitates their analysis.
It is easy to obtain limiting
results.
Also we have shown how a stopping rule can be invoked that
permits exact theoretical results to be obtained.
Just as the availability of theoretical results for the continuous-time
linear birth and death processes helped to suggest the birth and death
urn with immigration, other well-known continuous-time Markov proces-
ses can be used to analyze their analogous urn designs, which may be
©2001 CRC Press LLC

more suitable for a particular application.
This paper, used with the
references contained herein, provides a template for such an analysis.
References
1. Anderson, W. J. (1991). Continuous-Time Markov Chains. Spring-
er Verlag, New York.
2. Athreya, K. B. and Ney, P. E. (1972). Branching Processes. Spring-
er Verlag, Berlin.
3. Cox, D. R. and Miller, H. D. (1965). The Theory of Stochastic
Processes. John Wiley & Sons, New York.
4. Durham, S. D., Flournoy, N., and Li, W. (1998). A sequential de-
sign for maximizing the probability of a favorable response. Cana-
dian Journal of Statistics 26, 479–495.
5. Ivanova, A. (1998). A birth and death urn for randomized clinical
trials. Unpublished Doctoral Dissertation, University of Maryland
Graduate School, Baltimore.
6. Ivanova, A. and Rosenberger, W. F. (2000). A comparison of urn
designs for randomized clinical trials of K > 2 treatments. Journal
of Biopharmaceutical Statistics 10 93–107.
7. Ivanova, A., Rosenberger, W. F., Durham, S. D., and Flournoy,
N. (2000). A birth and death urn for randomized clinical trials:
Asymptotic methods. Sankhy¯a, Series B (to appear).
8. Karlin, S. and Taylor, H. M. (1974). A First Course in Stochastic
Processes. Academic Press, New York.
9. Kotz, S. and Balakrishnan, N. (1997). Advances in urn models dur-
ing the past two decades. In Advances in Combinatorial Methods
and Applications to Probability and Statistics (Ed., N. Balakrish-
nan), pp. 203–257. Birkha¨user, Boston.
10. Wei, L. J. (1979). The generalized P´olya’s urn design for sequential
medical trials. Annals of Statistics 7, 291–296.
11. Wei, L. J. and Durham, S. D. (1978). The randomized play-the-
winner rule in medical trials. Journal of the American Statistical
Association 73, 840–843.
©2001 CRC Press LLC


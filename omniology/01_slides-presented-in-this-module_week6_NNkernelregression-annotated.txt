Machine	  Learning	  Specializa0on	  
Going  
nonparametric: 
Nearest neighbor and kernel regression 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Fit globally vs. ﬁt locally 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
3	  
Parametric models of f(x) 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
4	  
Parametric models of f(x) 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
5	  
Parametric models of f(x) 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
6	  
Parametric models of f(x) 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
7	  
f(x) is not really a polynomial 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
8	  
What alternative do we have? 
If we: 
-  Want to allow ﬂexibility in f(x) having  
local structure 
-  Don’t want to infer “structural breaks” 
 
What’s a simple option we have? 
-  Assuming we have plenty of data… 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Simplest approach: 
Nearest neighbor regression 
9 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
10	  
Fit locally to each data point 
Predicted value = “closest” yi 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Here, this is 
the closest 
datapoint 
y 
sq.ft. 
price ($) 
x 
1 nearest neighbor  
(1-NN) 
 regression 

Machine	  Learning	  Specializa0on	  
11	  
What people do naturally… 
Real estate agent assesses value by 
ﬁnding sale of most similar house 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
$ = ??? 
$ = 850k 

Machine	  Learning	  Specializa0on	  
12	  
1-NN regression more formally 
Dataset of (    ,$) pairs: (x1,y1), (x2,y2),…,(xN,yN)  
Query point: xq 
 
1.  Find “closest” xi in dataset 
2.  Predict 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Here, this is 
the closest 
datapoint 
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
13	  
Visualizing 1-NN in  
multiple dimensions 
Voronoi tesselation 
(or diagram): 
-  Divide space into N 
regions, each 
containing 1 datapoint 
-  Deﬁned such that any 
x in region is “closest” 
to region’s datapoint 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Don’t explicitly form! 

Machine	  Learning	  Specializa0on	  
14	  
Distance metrics: 
Deﬁning notion of “closest” 
In 1D, just Euclidean distance: 
  
 distance(xj,xq) = |xj-xq| 
In multiple dimensions: 
-  can deﬁne many interesting distance functions 
-  most straightforwardly, might want to weight 
diﬀerent dimensions diﬀerently 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
15	  
Weighting housing inputs 
Some inputs are more relevant than others 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
# bedrooms 
# bathrooms 
sq.ft. living 
sq.ft. lot 
ﬂoors 
year built 
year renovated 
waterfront 

Machine	  Learning	  Specializa0on	  
16	  
Scaled Euclidean distance 
Formally, this is achieved via 
 
 distance(xj, xq) = 
             a1(xj[1]-xq[1])2 + … + ad(xj[d]-xq[d])2 
Other example distance metrics: 
-  Mahalanobis, rank-based, correlation-based,  
cosine similarity, Manhattan, Hamming, … 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
p
weight on each input 
(deﬁning relative importance) 

Machine	  Learning	  Specializa0on	  
17	  
Diﬀerent distance metrics 
lead to diﬀerent predictive surfaces 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Euclidean distance 
Manhattan distance 

Machine	  Learning	  Specializa0on	  
1-NN algorithm 
18 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Performing 1-NN search 
•  Query house: 
•  Dataset: 
•  Specify: Distance metric 
•  Output: Most similar house 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
1-NN algorithm 
Initialize Dist2NN = ∞,	  	  	     = Ø	  
For i=1,2,…,N 
Compute: δ = distance(        ,         ) 
 If δ < Dist2NN 
 set        =          
 set Dist2NN = δ 
Return most similar house   
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
i 
query house 
closest house 
i 
closest house 
to query house 
q 

Machine	  Learning	  Specializa0on	  
21	  
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Nearest Neighbors Kernel (K = 1)
1-NN in practice 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Fit looks good for data 
dense in x and low noise 

Machine	  Learning	  Specializa0on	  
22	  
Sensitive to regions with little data  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Nearest Neighbors Kernel (K = 1)
Not great at interpolating 
over large regions… 

Machine	  Learning	  Specializa0on	  
23	  
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Nearest Neighbors Kernel (K = 1)
Also sensitive to noise in data 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Fits can look quite wild… 
Overﬁtting? 

Machine	  Learning	  Specializa0on	  
k-Nearest neighbors 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
25	  
Get more “comps” 
More reliable estimate if you base estimate 
oﬀ of a larger set of comparable homes 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
$ = ??? 
$ = 850k 
$ = 749k 
$ = 833k 
$ = 901k 

Machine	  Learning	  Specializa0on	  
26	  
k-NN regression more formally 
Dataset of (    ,$) pairs: (x1,y1), (x2,y2),…,(xN,yN)  
Query point: xq 
 
1.  Find k closest xi in dataset 
2.  Predict 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Performing k-NN search 
•  Query house: 
•  Dataset: 
•  Specify: Distance metric 
•  Output: Most similar houses 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
k-NN algorithm 
Initialize Dist2kNN = sort(δ1,…,δk)         
  
  
  
  = sort(      ,…,      ) 
For i=k+1,…,N 
Compute: δ = distance(        ,         ) 
 If δ < Dist2kNN[k] 
  ﬁnd j such that δ > Dist2kNN[j-1] but δ < Dist2kNN[j] 
  remove furthest house and shift queue: 
  
    [j+1:k] =      [j:k-1]      
  
   Dist2kNN[j+1:k] = Dist2kNN[j:k-1]  
  set Dist2kNN[j] = δ and       [j] =  
Return k most similar houses  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
i 
query house 
closest houses 
to query house 
q 
i 
1 
k 
sort ﬁrst k houses 
by distance to query house 
list of sorted distances 
list of sorted houses 

Machine	  Learning	  Specializa0on	  
29	  
k-NN in practice 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Nearest Neighbors Kernel (K = 30)
Much more reasonable ﬁt 
in the presence of noise 
Boundary & sparse region 
issues 

Machine	  Learning	  Specializa0on	  
30	  
k-NN in practice 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Nearest Neighbors Kernel (K = 30)
Discontinuities! 
Neighbor either in or out 

Machine	  Learning	  Specializa0on	  
31	  
Issues with discontinuities 
Overall predictive accuracy might be okay, 
but… 
 
For example, in housing application: 
-  If you are a buyer or seller, this matters 
-  Can be a jump in estimated value of house going just 
from 2640 sq.ft. to 2641 sq.ft. 
-  Don’t really believe this type of ﬁt 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Weighted k-nearest neighbors 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
33	  
Weighted k-NN 
Weigh more similar houses more than 
those less similar in list of k-NN 
 
Predict:   
 
 
  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
cqNNj 
cqNN1yNN1 + cqNN2yNN2 + cqNN3yNN3 +…+ cqNNkyNNk 
k
X
j=1
ŷq = 
weights on NN 

Machine	  Learning	  Specializa0on	  
34	  
How to deﬁne weights? 
Want weight cqNNj to be small when 
 distance(xNNj,xq) large 
 
and cqNNj to be large when 
 distance(xNNj,xq) small 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
35	  
Kernel weights for d=1 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Deﬁne: cqNNj = Kernelλ(|xNNj-xq|) 
0 
-λ 
λ 
Gaussian kernel: 
Kernelλ(|xi-­‐xq|) =  
          exp(-(xi-xq)2/λ) 
 Note: never exactly 0! 
simple 
isotropic case 

Machine	  Learning	  Specializa0on	  
36	  
Kernel weights for d≥1 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Deﬁne: cqNNj = Kernelλ(distance(xNNj,xq)) 
0 
-λ 
λ 

Machine	  Learning	  Specializa0on	  
Kernel regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
38	  
Weighted k-NN 
Weigh more similar houses more than 
those less similar in list of k-NN 
 
Predict:   
 
 
  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
cqNNj 
cqNN1yNN1 + cqNN2yNN2 + cqNN3yNN3 +…+ cqNNkyNNk 
k
X
j=1
ŷq = 
weights on NN 

Machine	  Learning	  Specializa0on	  
39	  
Kernel regression 
Instead of just weighting NN, weight all points 
 
Predict:   
 
 
  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
ŷq = 
weight on each datapoint 
cqi 
cqiyi 
N
X
i=1
N
X
i=1
Nadaraya-Watson  
kernel weighted average  
Kernelλ(distance(xi,xq)) 
Kernelλ(distance(xi,xq)) * yi 
N
X
i=1
N
X
i=1
=  

Machine	  Learning	  Specializa0on	  
40	  
Kernel regression in practice 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Epanechnikov Kernel (lambda = 0.2)
Kernel has bounded 
support…Only subset 
of data needed to 
compute local ﬁt 

Machine	  Learning	  Specializa0on	  
41	  
Choice of bandwidth λ 
Often, choice of kernel matters  
much less than choice of λ 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Epanechnikov Kernel (lambda = 0.4)
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Epanechnikov Kernel (lambda = 0.04)
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Epanechnikov Kernel (lambda = 0.2)
λ = 0.04 
λ = 0.4 
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Boxcar Kernel (lambda = 0.2)
λ = 0.2 
Boxcar  
kernel 

Machine	  Learning	  Specializa0on	  
42	  
Choosing λ (or k in k-NN) 
How to choose?  Same story as always… 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Cross Validation 

Machine	  Learning	  Specializa0on	  
Formalizing the idea of local ﬁts 
43 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
44	  
Contrasting with global average 
A globally constant ﬁt weights all points equally 
 
 
 
 
 
 
 
ŷq =  
equal weight on each datapoint 
c 
c yi 
N
X
i=1
N
X
i=1
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Boxcar Kernel (lambda = 1)
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
yi 
N
X
i=1
1 
N 
=  

Machine	  Learning	  Specializa0on	  
45	  
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Boxcar Kernel (lambda = 0.2)
0
0.1
0.2
0.3
0.4
x0
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
 f(x0)
Epanechnikov Kernel (lambda = 0.2)
Contrasting with global average 
Kernel regression leads to locally constant ﬁt 
-  slowly add in some points and 
and let others gradually die oﬀ 
 
 
 ŷq =   
Kernelλ(distance(xi,xq)) 
Kernelλ(distance(xi,xq)) * yi 
N
X
i=1
N
X
i=1
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
46	  
Local linear regression 
So far, discussed ﬁtting constant function 
locally at each point 
à “locally weighted averages” 
 
Can instead ﬁt a line or polynomial locally 
at each point 
à “locally weighted linear regression” 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
47	  
Local regression rules of thumb 
-  Local linear ﬁt reduces bias at boundaries with minimum 
increase in variance 
-  Local quadratic ﬁt doesn’t help at boundaries and increases 
variance, but does help capture curvature in the interior 
-  With suﬃcient data, local polynomials of odd degree 
dominate those of even degree 
 
Recommended default choice:  
 local linear regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Discussion on k-NN 
and kernel regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
49	  
Nonparametric approaches 
k-NN and kernel regression are examples 
of nonparametric regression 
 
General goals of nonparametrics: 
-  Flexibility 
-  Make few assumptions about f(x) 
-  Complexity can grow with the number of observations N 
 
Lots of other choices: 
-  Splines, trees, locally weighted structured regression models… 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
50	  
Limiting behavior of NN: 
Noiseless setting (εi=0) 
In the limit of getting an inﬁnite amount of  
noiseless data, the MSE of 1-NN ﬁt goes to 0 
 
 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
51	  
Limiting behavior of NN: 
Noiseless setting (εi=0) 
In the limit of getting an inﬁnite amount of  
noiseless data, the MSE of 1-NN ﬁt goes to 0 
 
 
 
 
 
 
 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Not true for parametric models! 
Quadratic ﬁt 
1-NN ﬁt 

Machine	  Learning	  Specializa0on	  
52	  
Error vs. amount of data 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
# data points in 
training set 
Error 

Machine	  Learning	  Specializa0on	  
53	  
Limiting behavior of NN: 
Noisy data setting 
In the limit of getting an inﬁnite amount of data, 
the MSE of NN ﬁt goes to 0 if k grows, too 
 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
1-NN ﬁt 
200-NN ﬁt 
Quadratic ﬁt 

Machine	  Learning	  Specializa0on	  
54	  
NN and kernel methods  
for large d or small N 
NN and kernel methods work well when the data 
cover the space, but… 
-  the more dimensions d you have, the more 
points N you need to cover the space 
-  need N = O(exp(d)) data points for good performance 
 
 
This is where parametric models become useful… 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
55	  
Complexity of NN search 
Naïve approach: Brute force search 
-  Given a query point xq 
-  Scan through each point x1,x2,…, xN 
-  O(N) distance computations per 1-NN query! 
-  O(Nlogk) per k-NN query! 
 
What if N is huge???  
(and many queries) 
 
 
Will talk more about eﬃcient methods in  
Clustering & Retrieval course 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
k-NN for classiﬁcation 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
57	  
Recall classiﬁcation task: 
Spam ﬁltering example 
Input:  x 
Output:  y  
Not spam 
Spam 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Text of email, 
sender, IP,… 

Machine	  Learning	  Specializa0on	  
58	  
Using k-NN for classiﬁcation 
Space of labeled emails (not spam vs. spam), 
organized by similarity of text 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
not spam vs. spam 
decide via majority 
vote of k-NN 
query email 

Machine	  Learning	  Specializa0on	  
59	  
Using k-NN for classiﬁcation 
Space of labeled emails (not spam vs. spam), 
organized by similarity of text 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
not spam vs. spam 
decide via majority 
vote of k-NN 
query email 

Machine	  Learning	  Specializa0on	  
Summary for nearest neighbor 
and kernel regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
61	  
What you can do now… 
• 
Motivate the use of nearest neighbor (NN) regression 
• 
Deﬁne distance metrics in 1D and multiple dimensions 
• 
Perform NN and k-NN regression 
• 
Analyze computational costs of these algorithms 
• 
Discuss sensitivity of NN to lack of data, dimensionality, and noise 
• 
Perform weighted k-NN and deﬁne weights using a kernel 
• 
Deﬁne and implement kernel regression 
• 
Describe the eﬀect of varying the kernel bandwidth λ or # of 
nearest neighbors k 
• 
Select λ or k using cross validation 
• 
Compare and contrast kernel regression with a global average ﬁt 
• 
Deﬁne what makes an approach nonparametric and why NN and 
kernel regression are considered nonparametric methods 
• 
Analyze the limiting behavior of NN regression 
• 
Use NN for classiﬁcation 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  



Digital Signal Processing
with Examples in Matlab
Samuel D. Stearns
CRC PRESS

Library of Congress Cataloging-in-Publication Data
Steams, Samuel D.
Digital signal processing with examples in MAitAB/Samuel Steams.
p. cm.-(ElectricaI engineering and applied signal processing series)
Includes bibliographical references and index.
ISBN 0-8493-1091-1 (alk. paper)
I. Signal processing-Digital techniques-Computer simulation. 2. MATLAB. I. Title 
II. Series.
TK51O2 -9.S7I9 2002
651.382'2—dc21 
2002067480
This book contains information obtained from authentic and highly regarded sources. Reprinted material 
is quoted with permission, and sources are indicated. A wide variety of references are listed. Reasonable 
efforts have been made to publish reliable data and information, but the authors and the publisher cannot 
assume responsibility for the validity of all materials or for the consequences of their use.
Neither this book nor any part may be reproduced or transmitted in any form or by any means, electronic 
or mechanical, including photocopying, microfilming, and recording, or by any information storage or 
retrieval system, without prior permission in writing from the publisher.
The consent of CRC Press LLC does not extend to copying for general distribution, for promotion, for 
creating new works, or for resale. Specific permission must be obtained in writing from CRC Press LLC 
for such copying.
Direct all inquiries to CRC Press LLC, 2000 N.W. Corporate Blvd.. Boca Raton. Florida 33431.
Trademark Notice; Product or corporate names may be trademarks or registered trademarks, and are 
used only for identification and explanation, without intent to infringe
Visit the CRC Press Web site at www.crcpress.com
© 2003 by CRC Press LLC
No claim to original U.S. Government works 
International Standard Book Number 0-8493-1091-1
Library of Congress Card Number 2002067480 
Printed in the United States of America 34567890
Printed on acid-free paper

THE ELECTRICAL ENGINEERING 
AND APPLIED SIGNAL PROCESSING SERIES 
Edited by Alexander Poidarikas
The Advanced Signal Processing Handbook: 
Theory and Implementation for Radar, Sonar, 
and Medical Imaging Real-Time Systems
Stergios Stergiopoulos
The Transform and Data Compression Handbook
K.R. Rao and P.C. Yip
Handbook of Multisensor Data Fusion 
David Hall and James Llinas
Handbook of Neural Network Signal Processing 
Yu Hen Hu and Jenq-Neng Hwang
Handbook of Antennas in Wireless Communications
Lal Chand Godara
Noise Reduction in Speech Applications
Gillian M. Davis
Signal Processing Noise
Vyacheslav P. Tuzlukov
Digital Signal Processing with Examples in MATLAB®
Samuel Stearns
Forthcoming Titles
Propagation Data Handbook for Wireless Communications 
Robert Crane
The Digital Color Imaging Handbook 
Guarav Sharma
Applications in Time Frequency Signal Processing 
Antonia Papandreou-Suppappola
Smart Antennas 
Lal Chand Godara
Pattern Recognition in Speech and Language Processing
Wu Chou and Bing Huang Juang
Nonlinear Signal and Image Processing: Theory, Methods, and Applications 
Kenneth Barner and Gonzalo R. Arce

Dedication
An intelligent man acquires knowledge, 
and the ear of the wise seeks knowledge.
—Proverbs 18:15


Foreword in Memory of Rich ord 
W. Hamming (1915-1998)
The information age in which we are living has underlined the importance 
of signal processing, while the development of solid-state integrated circuits, 
especially in the form of general-purpose minicomputers, has made practical 
the many theoretical advantages of digital signal processing. It is for these 
reasons that a good book on digital signal processing is welcomed by a wide 
range of people, including engineers, scientists, computer experts, and 
applied mathematicians.
Although most signal processing is now done digitally, much of the data 
originates as continuous, analog signals, and often, the result of the digital 
signal processing is converted back to analog form before it is finally used. 
Thus, the complex and often difficult to understand relationships between 
the digital and analog forms of signals need to be examined carefully. These 
relationships form a recurring theme throughout this book.
The theory of digital signal processing involves a considerable amount of 
mathematics. The nonmathematically inclined reader should not be put off 
by the number of formulas and equations in the book, because the author 
has been careful to motivate and explain the physical basis of what is going 
on and, at the same time, avoid unnecessarily fancy mathematics and arti­
ficial abstractions. It is a pleasure, therefore, to recommend this book to the 
serious student of digital signal processing. It is carefully written and illus­
trated by many useful examples and exercises, and the material is selected 
to cover the relevant topics in this rapidly developing field of knowledge.
This foreword is included in memory of Professor Richard W. Hamming, one of the 
world's great mathematicians and a pioneer in the development of digital signal 
processing. The fact that his comments on signal processing, written in 1975 for 
the progenitor of this text, are just as relevant now as they were when they were 
written, is a testimony to Dr. Hamming's foresight and genius.
Samuel D. Steams


Foreword by Delores M-. Etter
The combination of digital signal processing techniques with today's com­
puting capabilities has allowed us to address many difficult problems. For 
example, computers can "understand" human speech, translate it to another 
language, and then synthesize the translated speech in a new voice. Satellites 
can transmit information from one part of our world to another in the blink 
of an eye. Medical doctors can diagnose, and surgically correct, heart valve 
problems in a baby before it is born. Biometric signals, such as fingerprints 
and iris scans, can be stored in a "smart card" to allow positive identification 
of the person using the card.
The capabilities that allow solutions such as the ones mentioned above all 
use digital signal processing algorithms to extract information from signals 
collected from the environment around us. These algorithms are based on 
fundamental principles from mathematics, linear systems, and signal anal­
ysis. This text will guide you through the mathematics and electrical engi­
neering theory using real-world applications. It will also use Matlab, a 
software tool that allows you to easily implement the signal processing 
techniques using the computer, and to view the signals graphically. Digital 
signal processing (DSP) and Matlab together open up a fascinating new 
world of possibilities.
The author of this text, Sam Steams, brings a unique perspective from a career 
of solving difficult problems at one of our country's finest laboratories — Sandia 
National Laboratory. He also brings a lifelong passion for education. Sam 
has been a teacher, a guide, a mentor, and a friend to me for over twenty 
years. I shall always be in his debt for introducing me to the wonders of 
digital signal processing.
Dr. Delores M. Etter, Professor, Electrical Engineering, United States Naval Acad­
emy at Annapolis, MD, is a former Deputy Under Secretary of Defense for Science 
and Technology. She is also the author of a number of engineering textbooks, includ­
ing several on Matlab.


Preface
This book is intended to be a text for courses in digital signal processing. It 
is written primarily for senior or first-year graduate engineering students, 
and it is designed to teach digital signal processing and how it is applied in 
engineering.
A progenitor of this text, Digital Signal Analysis, was published in 1975. At 
that time, there were only a handful of texts in the signal processing area, but 
now there are many texts, and we must give reasons why still another is needed.
One reason is that there is always room for another text in an area like 
digital signal processing, which is still expanding and finding new applica­
tions, provided the text is up to date and covers the fundamental areas, yet 
does not leave gaps in its development that must somehow be bridged by 
the reader. We endeavored in this text to provide a complete development 
of the main subjects in modern digital signal processing. These subjects, we 
believe, have changed over time in nature and relative importance, and this 
book is meant to respond to these changes and present the main subjects, 
or "basics," applicable in today's technology.
In a way, it is much more challenging now than it used to be to produce 
a text on DSP, because the field has grown in so many directions. It is more 
difficult now in a single book to cover the relevant theory and applications, 
and yet present the whole subject in a constructive manner, that is, in a 
manner conducive to teaching that proceeds logically from one topic to the 
next without omitting derivations and proofs.
A second and perhaps more important reason for this text, in particular, 
lies in our understanding of current applications of digital signal processing 
to engineering problems and systems. In most applications, digital signals, 
which are simply vectors or arrays with finite numbers of discrete elements, 
are worth processing and analyzing only because they represent discrete 
samples of continuous phenomena. That is, the engineer who applies the 
techniques described in a text like this is normally working with at least two, 
and usually all three, of the following operations:
Continuous 
signals
Continuous 
signals
Our premise is that the reader will find the text more useful, because we 
do not treat the central operation above as a subject in isolation. We try 
always to relate digital signal processing to continuous signal processing 
and to treat digital signals as samples of physical phenomena, just as the 

engineer must do when he or she applies digital signal processing to solve 
real problems.
Other important features of this text include the use of Matlab® to provide 
examples of signal processing, digital system design, solutions to exercises, 
coding and compression algorithms, etc. The Matlab language has become 
a standard in signal processing, because it is so easy to understand and use. 
Its plotting and graphics functions make it ideally suited to signal process­
ing applications. We provide a brief Matlab tutorial in Chapter 1, so that 
even if the reader does not use Matlab, he or she can easily read and 
understand the algorithms and examples in the text. All of the examples 
and functions presented in the text, as well as answers to exercises, are 
included with the software provided for the reader on the CRC PRESS LLC 
website (www.crcpress.Corh).
As indicated in the table of contents, other features include areas that are 
now considered basic but are not always covered in signal processing texts, 
including an introduction to statistical signal processing, the discrete cosine 
transform, an introduction to time-frequency and wavelet transforms, coding 
and compression of signals and other digital data, least-squares system 
design, and an introduction to adaptive signal processing.
The author of this text is greatly indebted to a number of students, col­
leagues, and friends, who have been patient and interested enough to com­
ment on large portions of the manuscript. Several have reviewed the entire 
manuscript and suggested many changes and improvements, in particular, 
Professors Chaouki Abdallah, Scott Acton, Majid Ahmadi, Victor DeBrunner, 
Dr. Robert Ives, Jeff Kern, Chip Stearns, and Dr. Li Zhe Tan. The students 
are those who have attended classes where the manuscript was first used, 
at the University of New Mexico, and in short courses taught in industry. 
Other friends and colleagues who have encouraged the author and com­
mented on the text's contents include Dr. Nasir Ahmed, Professors James 
Matthews, Wasfy Mikhael, Marios Pattichis, Balu Santhanam, Michael 
Soderstrand, and Dr. Stanley White.
The author is also thankful for his children, who kept asking, "How's the 
book coming?” (and were really interested in the answer), and to his won­
derful wife, Mary, who has patiently read and helped correct the entire 
manuscript.
Finally, in writing this text, the author has tried to adhere to the descrip­
tions and ideals implied in the two Forewords: First, we hope this project 
will honor the memory of Richard W. Hamming, one of the world's great 
mathematicians, who made so many contributions, who inspired so many 
through his teaching, and who laid so much of the foundation on which 
signal processing is built today. Second, in this book, we have strived to 
meet the high standards of clarity and logic in teaching and research set 
by Professor Delores M. Etter, formerly Deputy Director of Defense 
Research and Engineering and now Office of Naval Research Distin­
guished Chair in Science and Technology at the United States Naval 
Academy.

For Matlab product information, please contact:
The Math Works, Inc.
3 Apple Hill Drive
Natick, MA 01760-2098 USA
Phone: 508-647-7000
Fax: 508-647-7001
E-mail: info@mathworks.com
Web: www.mathworks.com


Riovratihu
— 
~ O t-------J
Samuel D. Stearns has been a Distinguished Member of the Technical Staff 
at Sandia National Laboratories, Albuquerque, New Mexico, and is now a 
consultant. His principal technical areas are digital signal processing and 
adaptive signal processing. His most recent work has been in teaching these 
subjects at the University of New Mexico and in industrial short courses, as 
well as in consulting.
Dr. Stearns has taught and advised dissertation research at several univer­
sities throughout the United States, including the Universities of Central 
Florida, Colorado, and New Mexico, and Kansas State and Stanford Univer­
sities. He is a Fellow of the IEEE. His Fellow citation reads, "For contributions 
to education in digital and adaptive signal processing systems and algo­
rithms." He has served in various IEEE activities and has published a number 
of papers in signal processing, adaptive signal processing, and related areas. 
He is author or coauthor of the following texts:
Digital Signal Processing with Examples in Matlab (2003)
Signal Processing Algorithms in Matlab (1996)
Signal Processing Algorithms in Fortran and C (1993)
Digital Signal Analysis, 2nd Ed. (1990)
Signal Processing Algorithms (1987)
Adaptive Signal Processing (1985)
Digital Signal Analysis (1975)


Contents
1. 
Introduction.................................................................................................... 1
1.1 
Digital Signal Processing................................................................... 1
1.2 
How to Read this Text........................................................................ 2
1.3 
Introduction to Matlab...................................................................... 2
1.4 
Signals, Vectors, and Arrays..............................................................3
1.5 
Review of Vector and Matrix Algebra
Using Matlab Notation......................................................................5
1.6 
Geometric Series and Other Formulas......................................... 12
1.7 
Matlab Functions in DSP................................................................ 14
1.8 
The Chapters Ahead..........................................................................15
References............................................................................................16
2. 
Least Squares, Orthogonality, and the Fourier Series................19
2.1 
Introduction..........................................................................................19
2.2 
Least Squares...................................................................................... 19
2.3 
Orthogonality...................................................................................... 24
2.4 
The Discrete Fourier Series.............................................................. 26
2.5 
Exercises................................................................................................33
References............................................................................................37
3. 
Correlation, Fourier Spectra, and the Sampling Theorem......39
3.1 
Introduction......................................................................................... 39
3.2 
Correlation........................................................................................... 40
3.3 
The Discrete Fourier Transform (DFT)..........................................42
3.4 
Redundancy in the DFT................................................................... 43
3.5 
The FFT algorithm............................................................................ 45
3.6 
Amplitude and Phase Spectra.........................................................48
3.7 
The Inverse DFT.................................................................................51
3.8 
Properties of the DFT........................................................................52
3.9 
Continuous Transforms.....................................................................58
3.10 
The Sampling Theorem.....................................................................60
3.11 
Waveform Reconstruction and Aliasing........................................62
3.12 
Exercises................................................................................................66
References............................................................................................69
4. 
Linear Systems and Transfer Functions........................................... 71
4.1 
Continuous and Discrete Linear Systems.....................................71
4.2 
Properties of Discrete Linear Systems...........................................71

4.3 
Discrete Convolution......................................................................... 74
4.4 
The z-Transform and Linear Transfer Functions........................ 75
4.5 
Poles and Zeros...................................................................................77
4.6 
Transient Response and Stability.....................................................81
4.7 
System Response via the Inverse z-Transform........................... 83
4.8 
Cascade, Parallel, and Feedback Structures................................ 84
4.9 
Direct Algorithms................................................................................87
4.10 
State-Space Algorithms......................................................................89
4.11 
Lattice Algorithms and Structures................................................. 91
4.12 
FFT Algorithms....................................................................................99
4.13 
Discrete Linear Systems and Digital Filters...............................103
4.14 
Exercises.............................................................................................. 105
References...-...:................................................................................... 108
5. 
FIR Filter Design.....................................................................................Ill
5.1 
Introduction.........................................................................................Ill
5.2 
An Ideal Lowpass Filter...................................................................112
5.3 
The Realizable Version.................................................................... 113
5.4 
Improving an FIR Filter with Window Functions.....................116
5.5 
Highpass, Bandpass, and Bandstop Filters................................ 121
5.6 
A Complete FIR Filtering Example.............................................. 125
5.7 
Other Types of FIR Filters............................................................... 126
5.8 
Exercises.............................................................................................. 130
References...........................................................................................133
6. 
HR Filter Design......................................................................................135
6.1 
Introduction........................................................................................ 135
6.2 
Linear Phase.......................................................................................136
6.3 
Butterworth Filters............................................................................137
6.4 
Chebyshev Filters..............................................................................141
6.5 
Frequency Translations.................................................................... 147
6.6 
Tire Bilinear Transformation...........................................................149
6.7 
HR Digital Filters...............................................................................153
6.8 
Other Types of HR Filters...............................................................157
6.9 
Exercises.............................................................................................. 162
References...........................................................................................165
7. 
Random Signals and Spectral Estimation.......................................167
7.1 
Introduction........................................................................................167
7.2 
Amplitude Distributions................................................................. 168
7.3 
Uniform, Gaussian, and Other Distributions.............................171
7.4 
Power and Power Density Spectra.............................................. 177
7.5 
Properties of the Power Spectrum................................................181
7.6 
Power Spectral Estimation.............................................................. 184
7.7 
Data Windows in Spectral Estimation......................................... 188
7.8 
The Cross-Power Spectrum............................................................ 190

7.9 
Algorithms......................................................................................... 193
7.10 
Exercises............................................................................................. 194
References..........................................................................................197
8. 
Least-Squares System Design............................................................199
8.1 
Introduction....................................................................................... 199
8.2 
Applications of Least-Squares Design........................................ 200
8.3 
System Design via the Mean-Squared Error.............................203
8.4 
A Design Example............................................................................207
8.5 
Least-Squares Design with Finite Signal Vectors......................210
8.6 
Correlation and Covariance Computation................................. 212
8.7 
Channel Equalization..................................................................... 215
8.8 
System Identification.......................................................................217
8.9 
Interference Canceling.................................................................... 220
8.10 
Linear Prediction and Recovery...................................................223
8.11 
Effects of Independent Broadband Noise.................................. 228
8.12 
Exercises............................................................................................. 230
References..........................................................................................238
9. 
Adaptive Signal Processing................................................................ 241
9.1 
Introduction.......................................................................................241
9.2 
The Mean-Squared Error Performance Surface........................242
9.3 
Searching the Performance Surface..............................................244
9.4 
Steepest Descent and the LMS Algorithm................................. 249
9.5 
LMS Examples..................................................................................255
9.6 
Direct Descent and the RLS Algorithm.......................................258
9.7 
Measures of Adaptive System Performance..............................263
9.8 
Other Adaptive Structures and Algorithms..............................267
9.9 
Exercises............................................................................................. 268
References..........................................................................................273
10. 
Signal Information, Coding, and Compression........................... 277
10.1 
Introduction.......................................................................................277
10.2 
Measuring Information.................................................................. 278
10.3 
Two Ways to Compress Signals....................................................280
10.4 
Entropy Coding................................................................................282
10.5 
Transform Coding and the Discrete Cosine Transform......... 291
10.6 
Multirate Signal Decomposition and Subband Coding........ 298
10.7 
Time-Frequency Analysis and Wavelet Transforms................ 308
10.8 
Exercises............................................................................................. 312
References..........................................................................................317
Index.........................................................................................................................319


Introduction
1.1 
Digital Signal Processing
Digital signa] processing (DSP.) has become an established area of electrical 
and computer engineering in relatively recent times. In fact, because all types 
of signals, when they are processed, are now most often processed in digital 
form, scientists and engineers in all disciplines have come to at least a 
nodding acquaintance with the subject in order to understand what their 
instruments and displays are trying to tell them.
Compared with other areas of electrical engineering that share their ana­
lytic tools with DSP, like fields and waves or communication theory or circuits 
or control theory, DSP. has a relatively short history. The author of this text 
has many colleagues and friends who have built the foundations of DSP 
from the ground up, and most of them are still active contributors at the 
time of this writing. The list is too long to print here, but some of them can 
be found in the lists of "early" references at the end of this chapter1’15 and 
subsequent chapters.
This book is the result of an attempt to organize the major contributions of 
these pioneers and describe subjects that are basic to DSP as it exists currently, 
that is, subjects that one would find essential to or at least useful in the 
understanding of most current applications of DSP. There are now many 
special topics within DSP, such as image processing, digital communications, 
signal coding and compression, and so on, so that no one is any longer an 
expert in all areas. But, hopefully, in this text, the reader will find subjects that 
are basic to (and useful in) all these different areas.
In other words, there is so much diversity in types of signals and types of 
processing and types of processors that we really only have two choices. One 
choice is to provide a complete text on a single subject, such as one of those 
just mentioned or wavelet transforms or digital filters, for example. The second 
choice is to provide a connected knowledge base consisting of subjects basic 
to all kinds of digital signal processing, so that the reader may acquire at least 
the fundamentals but not all of the details.
i

2 
Digital Signal Processing with Examples in Matlab
It is this second option that we have chosen here. We hope that this text will 
introduce the reader to the basic areas of signal processing and also provide 
a reference for the reader who wishes to review the fundamentals of DSP.
1.2 
How to Read this Text
There are two main ways to read this text. One way is to read it more or 
less in the order in which it is written and work some of the exercises as 
you go. Each chapter, especially in the first part of the book, depends on 
subjects discussed in previous chapters. If the subjects are read and learned 
in order, the reader should end with a good foundation for work in most 
areas of signal processing.
The second way is to look in the text for a particular subject, such as filters, 
coding, spectral estimation, etc. The best way to do this is to look in the 
index. If your subject is not listed there, look in the table of contents for a 
related area. If you have at least some familiarity with DSP as well as the 
basic mathematics needed for analysis, there is no need to read all the text. 
The treatment of each individual subject is meant to be self-contained, 
although the text may refer to previous, more basic subjects.
With these two approaches in mind, the reader may now decide whether 
to read the remainder of this chapter or skip to a subject of interest. The rest 
of this chapter consists mostly of reviews of some basic mathematics and 
formulas useful in signal and system analysis, as well as the Matlab lan­
guage, which has become a standard in signal processing analysis and system 
design. If you are familiar with the mathematics and are already a Matlab 
user, these reviews may be skipped and treated as reference material.
1.3 
Introduction to Matlab
Matlab, a product of The Math Works, Inc., is currently in use by a large 
fraction of the DSP community. Three principal reasons for this are (1) the 
syntax allows the user to do most DSP operations with very simple lines of 
code, (2) with Matlab's graphics support, one can produce publishable plots 
with minimal effort, and (3) most importantly, you do not have to be an 
expert in Matlab to use Matlab. The syntax is easy to read and learn and 
use with almost no prior groundwork.
Most of the DSP examples in this text use the Matlab language. This does 
not mean that the reader needs to own or operate Matlab software, although 
having access to a computer running Matlab gives a definite advantage. But 
the language is useful as a standard for describing DSP operations and 
algorithms. For readers unfamiliar with Matlab notation, we begin here with 
some basics. If you do not own or operate Matlab, you may wish to view 
the use of Matlab in this text as a convenient and easy-to-read standard 

Introduction
3
system for describing signal processing procedures. For more depth in Matlab, 
the reference by Etter19 at the end of this chapter not only describes Matlab 
but also addresses several of the topics in this text, with applications.
Matlab uses single expressions called commands, which may be assembled 
into sets of commands called m-files (because the file extension is "m") or 
functions, which may be called by other m-files or functions. In this text, 
when you see lines inside a text box, these will usually be Matlab expres­
sions. For example,
» x=4 ;
»y=[2,3,4];
»z=[l 2 3; 4 5 6];
(1-1)
Each of these lines is an individual expression (command) as indicated by 
the command prompt at the beginning of the line. Each line ends with a 
semicolon; if not, then Matlab would echo the results of the line as in (1.2) 
below. Row elements of an array may be separated by commas (as in y) or by 
spaces (as in z). Rows are separated by semicolons (as in z). The results of 
(1.1) are seen in (1.2) when the Matlab expression "r,y,z" is entered without 
a semicolon at the end.
(1-2)
From basic expressions like these, we can proceed rapidly and simply in 
the following sections to expressions that accomplish complicated DSP 
operations.
1.4 
Signals, Vectors, and Arrays
High-level computer languages such as Matlab are designed to process 
ordered sequences of elements, that is, variables, vectors, and matrices. As used 
in this text, these three terms form a hierarchy in the sense that:
• A variable is a single element (integer, real, or complex) [likex in (1.1)].
• A vector is an ordered sequence of variables [like y in (1.1)].
0 A matrix is an ordered sequence of vectors [like z in (1.1)].

4 
Digital Signal Processing with Examples in Matlab
We usually use array as an inclusive term to designate a vector or a matrix. 
Sometimes our use of vector here is confusing at first, because we are used to 
vectors in electromagnetics or mechanics—in three-dimensional space—with 
exactly three components. But these are really just ordered sequences of three 
variables and are thus vectors in the sense used here, and we must now allow 
more dimensions in order to have vectors that represent signals in a "signal 
space," which we define as follows.
Figure 1.1 shows a sampled waveform and its corresponding signal vector. 
When we say a waveform is sampled, we mean that its amplitude is measured 
and recorded at different times. Usually, these different times are equally 
spaced in the time domain, but not always. We assume that the samples are 
equally spaced. The interval between samples is called tire sampling interval 
or time step. In Figure 1.1; the time step is At = T =1. Tn this example, there 
are ten sample values, and the signal vector consists of the ordered sequence 
of the ten integer samples. Because each sample is a variable, we may say 
that the signal space of the sampled waveform in Figure 1.1 has ten dimensions, 
one for each sample. Thus, in this sense, the signal space of a sampled wave­
form has as many dimensions, or degrees of freedom, as there are samples in 
the waveform.
Sample vector
[0 1 -1 2 56 4 1 -2-1]
FIGURE 1.1
Sampled waveform and sample vector with time step 7 = 1

Introduction
5
Sample array
0 12 3 1
2 3 0 1 2
3 2 10 1
0 12 3 1
2 3 0 2 0
FIGURE 1.2
Sampled gray-scale image and sample array.
Figure 1.2 illustrates a (very small) grey-scale image and its corresponding 
sample array. Each element of the image is called a pixel (picture element), 
and in this example, an element can take on only one of four values, 0, 1, 2, 
or 3, and these values cover the range from black to white in the order given; 
that is, 0 = black, and 3 = white. The digital image array is shown next to the 
image, and in this case, we may say that the signal space associated with 
the image has 25 dimensions, one for each sample (variable) in the image.
Thus, through processes of sampling and analog-to-digital conversion that 
depend on the quantities being measured, signals and images in the real, 
continuous world are converted to vectors and matrices of integers in the 
discrete world of DSP. It is with these kinds of digital data that the subject 
of DSP begins. We should note that the examples in Figures 1.1 and 1.2 are 
simple for the sake of illustration. In real situations, waveform vectors and 
image arrays have large numbers of elements, as may be seen in some of 
the examples in this text. Real images, which consist of thousands or millions 
of pixels, typically have 256 grey levels, not just four as in Figure 1.2, and 
color images have many more levels to a pixel. Moreover, video signals, with 
the most data of all, consist of sequences of images.
1.5 Review of Vector and Matrix Algebra Using 
Matlab Notation
In the rest of this chapter, before we begin discussing DSP fundamentals in 
Chapter 2, we review some of the basic mathematics required to understand 
DSP systems, beginning in this section with vector and matrix algebra. We 
have seen how vectors and matrices are used to represent digitized signals 

6 
Digital Signal Processing with Examples in Matlar
and images. Vectors and matrices also appear in other ways in DSP, as we 
will see later. In modern DSP, and especially in languages like Matlab, vector 
and matrix sums, products, etc. are the basic processing operations. These 
operations are mostly just demonstrated here. The development is not as 
formal as it would be in a text on linear algebra or matrix theory. A summary 
of all operations is presented in Table 1.1. If the reader is already familiar
TABLE 1.1
Examples of Array Operations
Operation
Symbol
Example
Remarks
Sum or 
difference
Matrix product
Array product
Transposes
+ or -
*
.*
[1
1
3
2
2
1
4
1
4
1 2
3 4
2
2
4
*[1
21
3 4]
.* [1
2
5
[1 + 2
2
5
+
3]
)L
2 
.*
2
i 
ij,
0
1
-2
3
0
1
3
3-
-1
2
=
-1
2
] =
1
2
3
4/
i
2
3
[~
-1
2
1
[
[
4
5
6
4
5
6
1
2
1
4
0
j2
[2
4
3
3
4
1­
3 +
t
8
2
4
1]
3
7
]
i
L
Dimensions must be 
the same unless 
one variable is a 
scalar.
Inner dimensions 
must be the same 
unless one variable 
is a scalar.
Dimensions must be 
the same unless 
one variable is a 
scalar.
j - 7-1 • Rows 
become columns. 
Elements are 
conjugated with' 
but not with /
Exponentiation
Backslash
a * b
2
n
0
u3 4
= c.
[1+2
•A[l
2
2
5
wht
i
2
31
>re
3-4/
3] =
A 2 =
1
1
« =
•'
I
[
2
0
5
2
2
1
3
2
1+2/ 
3-4/
4 8}
4 9]
0 1
9 4
and
4
1
c
Dimensions mustbe 
the same unless 
one variable is a 
scalar.
• t Solves sets of linear
1 equations.
2J
= a\c
5
2
3
2
\
1
2
-1
2

Introduction
7
with the operations, the table alone should suffice as a convenient reference.
To begin, we note that a vector is either an array with one row (row vector) 
or an array with one column (column vector). Thus, rules and statements 
about arrays are generally true for vectors.
The sum or difference of two arrays is obtained simply by adding or 
subtracting the individual elements. Thus,
[1 
2 3] + [0 -1 4] = [1 1 7]
1 
2 0 + [-1 1 2 „ 0 3 2
_3 
4 1J 
[2 -2 -1J |_5 2 0
(1-3)
The Matlab expressions for these operations are quite similar. For example,
» [1,2,0; 3,4,l]+[-l,l,2; 2, -2,-1] 
ans =
0 
3 
2
5 2 0
(1-4)
But, we do not really need the Matlab versions here, where conventional 
expressions are more acceptable. Notice that in order for a sum or differ­
ence operation to make sense, the two arrays must have the same dimen­
sions, that is, 3 x 1, 1 x 3, and 2 x 3 in (1.3). An exception occurs when 
one of the arrays is a scalar or single variable. In this case, the scalar or 
variable is added to or subtracted from each element of the second array. 
Thus,
In this text, we use letters without subscripts to represent single vectors 
and arrays like those in (1.3) and (1.5) and letters with subscripts to represent 
the elements. For example,
a = a" °12 013 
(1.6)
fl2l ^22 a23
The first index of each element designates the row, and the second the 
column. Indices generally begin at one. If a is a vector, then only one index 
is required.
Two kinds of array products are used most often in DSP. The first is the 
ordinary matrix product with symbol *, formed by summing products of 

8 
Digital Signal Processing with Examples in Matlab
row elements in the first array times column elements in the second. If a is 
an array with M rows and N columns, and if b is an array with N rows and 
K columns, the elements of the array product c = a * b are given by the 
following:
N
ctj = ^ainb„t-, 1<i<M and l<j<K 
(1.7)
n=i
For example, suppose a, b, and c are defined as follows:
Then we can see from the matrix product definition in (1.7) that the array 
equation a * b = c is an expression of three simultaneous equations. That is,
bt - b2 = -1
-b2 + b3 = 1 and a * b = c 
(1.9)
2bi +b3 = 2
are equivalent expressions of three linear equations in three unknowns. 
Notice how the matrix product a * b is obtained by summing products along 
rows of a and down columns of b.
In DSP, it is often necessary to solve simultaneous linear equations like 
(1.9), and we will return to this subject shortly.
The second kind of product, used just as often in DSP as the matrix 
product defined in (1.7), is obtained simply by multiplying corresponding 
elements together. To do this, the two arrays must have the same dimen­
sions. This kind of operation is designated with the symbol .* (dot star) to 
distinguish it from the matrix product (*). Thus, if a and b are M x N arrays, 
an element of the array product (as opposed to matrix product), c = a * b, 
is given by
- a^bij; l<i<M and l<j<N 
(1-10)
To illustrate the array product, suppose we wish to "fade" the image in 
Figure 1.2 by making the rows lighter from the bottom of the image to the 
top. Then we could construct a "fading matrix," Q, and use the array product 
to produce the faded version of the original sample array (S) in Figure 1.2.

Introduction 
9
For example, let
The original and faded versions of the image in Figure 1.2 are shown in 
Figure 1.3. Note that there are 16 gray levels in the faded version, compared 
with four in the original.
The array product may be used in the same way to impose an "envelope" 
on a waveform. For example, the vector f consisting of 1000 samples of a 
unit sine wave with 50 samples per cycle is produced by the following 
Matlab expressions:
»n=[1:1000];
»f=sin(2*pi*n/50);
(1.13)
FIGURE 1.3
Original and faded images in equation (1.12).

10
Digital Signal Processing with Examples in Matlab
FIGURE 1.4
Plots of undamped and damped sine waves resulting from (1.13) and (1.14).
Notice how the row vector n causes the sine function to produce/, also a 
row vector with 1000 elements. The sine wave is then "dampened" to pro­
duce the vector g by imposing a decaying exponential envelope using the 
array product (.*), and then both/and g are plotted, as follows:
»g=exp(-n/2 5 0). * f ; 
»plot(n,f, n, g)
(1-14)
In (1.14),'each element (/„) of/is multiplied by the exponential function 
e-n/25o produce an eiement (g„) of g. The results of the Matlab plot function 
are shown in Figure 1.4, illustrating the ease of producing such a plot. It 
is also easy to add grid lines, labels, etc., as we will later see in examples.
Having reviewed array sums and products, we turn to two operations that 
work with single arrays. First is the transpose with symbol' (prime), which 
changes rows to columns. Thus, each element aif in array a becomes a.j in a’. 
The transpose is illustrated in Table 1.1. An important feature of the Matlab 
transpose applies when the elements of the array are complex. Then, in a', 
the transposed elements are conjugated. If the elements are complex and 
conjugation is not desired, then (dot prime) is used in place of' (prime). 
Note the examples in Table 1.1.
The second operation is the matrix inverse with symbol 1 or inv. If a is a 
square matrix, inv(a') or n'1 is another square matrix such that a * a'1 = I, 
where I is the identity matrix with ones on the diagonal and zeros elsewhere. 
The inverse of any 2x2 matrix, which is needed often in DSP, is
a
rtin
/21
a22
a22
~a21
- flJ2
= I (1.15)
1
alla22 ~ a12a2l
; a * a 1 =
1
0
0
1

Introduction 
11
For a matrix to have an inverse, it must be square, because only then will 
the matrix product a * o'1 be valid. The matrix must also be nonsingular, no 
row may be a linear combination of other rows. This, in turn, means that 
the determinant1617 of the matrix must not vanish. In the case of (1.15), the 
determinant of a is anfl2z _ ana2i- In this text, we will prefer the backslash 
operation, which is described next after exponentiation, to the inverse.
Array exponentiation is also useful in DSP and may be done either with scalars 
or arrays. In this text, we use only the Matlab ,a (dot hat) operation, which is 
similar to .* (dot star) but produces powers instead of products. Thus,
3 2 1 A2 = 9 4 1
-10 2’ 
104
3 2 1 A 2 -10
-1 0 2_f [-1 1 2_
9 0.5 1
-1 0 4
(1.16)
On the left, each element of the first array is squared; and on the right, 
each element of the first array is raised to the corresponding power in the 
second array.
The final array operation we review here is the backslash (\) operation, 
which is useful in DSP for solving linear equations like those in (1.9). We saw 
in connection with (1.9) that a set of linear equations may be expressed in the 
form a * b = c. The solution is found by multiplying on the left by fl’1, that is, 
b = fl”1 * c. The backslash notation, b = a\c, is equivalent, but in Matlab, the 
latter is preferred, because it uses methods that produce the smallest roundoff 
errors. Thus, in summary,
If a * b = c, then b = a\c 
(1-17)
For example, the system in (1.9) is solved as follows:
(1-18)
The operations described in this section are the main algebraic operations 
we use in DSP. We will also need relational and logical operations, but they 
will be introduced in terms of the Matlab language as they are used. This 
is the painless way to learn Matlab—learning necessary operations and 
functions as they are needed and memorizing those most often used. 
Table 1.1, consisting of examples of the operations discussed in this section, 
is provided as a reference.

12
Digital Signal Processing with Examples in Matlab
1.6 Geometric Series and Other Formulas
The geometric series is used repeatedly to simplify expressions in DSP. The 
basic form of the series with N terms is
N-l£xu = l + x+x2+ ••• + XN”1
n=0
1
= L~.x„ 
(1.19)
1 - x
The reader can prove this result by induction: It is obviously true for N = 1. 
And, you can easily show that if it is true for N, it is true for N + 1, and 
therefore true for all N. Moreover, if the magnitude of x is less than one, the 
infinite version of (1.19) is as follows:
= w<i 
(L20)
1 -x
>1=0
In DSP, x in the geometric series is often a complex exponential variable 
of the form e'\ where j = 7-1 and k is a real constant. Thus, for example,
N-l .2nn 
j2n
“ - 77= - ° 
(12,)
n=0
Expressions in DSP are often simplified using the geometric series in this 
way. Furthermore, the complex plane is a useful aid to understanding why 
sums like this vanish. On the complex plane, e’6 becomes a unit vector at 
angle 0 measured counterclockwise from the positive real axis. For example, 
all the terms in (1.21) with N = 8 are plotted as unit vectors in Figure 1.5.
FIGURE 1.5
All the terms in (1.21) with N = 8 on the complex plane.

Introduction
13
TABLE 1.2
Trigonometric Identities
sin© =
e'“ = cos8+ j sin©
sin(0+ct) = sin©cos<jt+cosfisinct
sin 6 sin a = |(cos(0-a)-cos(©-t- a)) 
sin 6 cos a = |(sin{© + «) + sin(©-a)) 
sin18 + cos2 0=1
sin1© = ^(1 - cos20)
cos© = 
+
cos(©+a) = cos ©cos a-sin ©sin a
cos0cosa = cosf© + a) + cos(©- a))
cos2©-sin1© = cos 2©
cos1© = ^(1 + COS20)
That is, the vector at 3 o'clock is the term for n = 0, the vector at 12 o'clock 
is for n = 2, etc. We can see from the symmetry of the plot that the sum of 
all eight vectors is zero as in (1.21).
Some of the most useful trigonometric identities are provided in Table 1.2 
as a reference for later chapters. Trigonometric functions, especially sine 
and cosine functions, appear in different combinations in all kinds of har­
monic analysis—Fourier series, Fourier transforms, etc. The identities that 
give sine and cosine functions in terms of exponentials are important, 
because they allow us to find sums of sines and cosines using the geometric 
series as in the example above. For example, with the exponential form of 
the sine or cosine function, we can easily show from (1.21) that when K is 
an integer,
kn-i , 
kn-1 .
Xsinf"^} = 0 and £cosHr) = 0 
(L22)
n.-0 
n=0
That is, a sum of equally spaced samples of any sine or cosine function is 
zero, provided the sum is over an integral number of cycles, or periods, of 
the function.
In addition to the trigonometric formulas in Table 1.2, some additional 
mathematical formulas and operations are summarized in Table 1.3. These 
are used frequently in DSP design and analysis as well as in later chapters 
of this text.
The last two lines in Table 1.3, in which w and v are functions of an inde­
pendent variable (say t) are especially useful with continuous transforms and 
transfer functions, because it is often necessary to differentiate or integrate 
products or quotients of functions when deriving transforms or transfer func­
tions. For example, suppose we wish to integrate the product sinaf over

14 
Digital Signal Processing with Examples in Matlab
TABLE 1.3
Additional Identities and Operations
if x = a", then log„x = y
logic* = log X login e,
where log s log.
• K b * 
-'A
sinhx = -(p -e )
i 
b« -W
coshx = -(e 
)
sinh"'(z) = log(.x + Jx2 + 1)
cosh"1 (x) = logCx+V-ir2-l); x>l
ri(»v) 
dv 
du
dx 
dx 
dx
d(u/v) _ r du 
dv'
dx 
L dx 
dx_I/-’
Jii dv = uv - du
an indefinite interval of t. If we set u = sin at and dv = e'Mdt, then v = {l/ja)eial 
and du = a cosat dt, and the last line in Table 1.3 becomes the following:
jsinate'“fdt = 4-^ sin ate'1"1 - ajcos ate;a>!rif) (1.23)
At first, this result seems only to make the problem worse, but if we use the 
same method on the right-hand integral in (1.23), this time with w = cos at, 
the result is as follows:
Jcosafe;wtdf = j^^cosateJ<uf + ajsinafe,io,dt^ (1-24)
Now the integral on the right in (1.24) is the same as the original integral 
on the left in (1.23), and we can combine the two equations to give the desired 
result:
j (1)1
fsinafe'atdt ~ —----- -(/’tusinaf-acosat) 
(1-25)
J 
a - co
Thus, applications of the last line in Table 1.3, which is called integration by 
parts, are often useful for integrating products of functions.
1.7 Matlab Functions in DSP
Matlab fund ions, as we mentioned in Section 1.3, are m-files that can be called 
by other m-files. In the Matlab system, these files are implemented in a way 
that is especially advantageous in the development of DSP algorithms and 
systems. As you go through this text, there will be opportunities to write your 
own functions to use in doing exercises and then (hopefully) to apply in your 
own occupation. The Matlab language thus helps people in DSP and other 
research and development areas to work efficiently by stockpiling low-level 
operations, rather than having to re-develop them for each new project.

Introduction
15
The row_vec function is a simple example. Suppose your Matlab programs 
often require the conversion of unknown arrays to row vectors. Then, even 
in 
...nln lL,-. 
C.
LUOLL^ll LA LV LW1 LV C1O1UH tO 
J V/ VI LVUIU VV AAlV LA AV 1V11U W 11 Atj 1 L4.1 IV LI V/1 I V» 1 LV<-
and avoid doing the task repeatedly:
function v=row_vec(A)
% v=row_vec(A) 
%
% Converts any array A into row vector v.
% If A is 1XN, v=A. If A is Nxl, v = A.'.
% If A is a matrix, v=A scanned by rows. 
AT=A';
v=AT ( :
Matlab allows you to store functions like this in your own system and then 
use them in other functions. If you are like most of us, you may eventually 
forget how to use the function, hence the comment (%) lines in (1.26). The 
comments allow one to request help at the Matlab command prompt as in 
the following example:
» help row_vec 
v=row_vec (A)
Converts any array A into row vector v.
If A is 1XN, v=A. If A is NXl, v=A.'.
If A is a matrix, v=A scanned by rows.
(1-27)
That is, the help request at the command prompt causes the initial comment 
lines of the function to be printed. If you are new to Matlab, it is a good 
idea to try the help command (you can even enter “help help") and get 
instructions for some of the commonly used functions. It is also a good idea, 
as you read this text and work exercises, to form the habit of developing 
functions to do tasks that require repeating, and thus build your own per­
sonal library. In DSP, there is a real advantage in making a practice of doing 
this.
1.8 The Chapters Ahead
In the chapters ahead, we develop the fundamentals of DSP more or less 
from the ground up. As indicated in the Preface, we are attempting to cover 
the subject from an applied standpoint, assuming that the reader is planning 
to process digitized samples of real, continuous signals.

16 
Digital Signal Processing with Examples in Matlab
Chapter 2 is a review of least-squares analysis, orthogonality, and the 
Fourier series, all of which are needed for later chapters but may not be 
necessary for readers familiar with these subjects. Chapters 3 through 4 
proceed through Fourier spectra, sampling and measurement of signals, the 
sampling theorem, linear algorithms, structures, and transfer functions. Chap­
ters 2 through 4 are meant to be read successively, unless the reader is already 
familiar with the material.
The remaining chapters, from Chapter 5 on, cover different subjects basic 
to DSP—digital filters, statistical signal processing, least-squares system 
design, and so on. These chapters depend on Chapters 2-4 but are relatively 
independent of each other, and therefore not meant to be read in a particular 
order. The reader may choose to look only at chapters of current interest.
All of the chapters except this chapter have exercises at the end. This 
chapter is meant primarily for background and reference, but in each of the 
rest of the chapters, the exercises serve the purpose not only of providing 
examples of the topics covered in the chapter, but also of introducing topics 
and applications not covered in the chapter. In this sense, the exercises are 
an essential component of the text.
References
Digital Signal Processing
1. Shannon, C.E., The Mathematical Theory of Communication, University of Illinois 
Press, Urbana, IL, 1949.
2. Blackman, R.B. and Tukey, J.W., The Measurement of Power Spectra, Dover, New 
York, 1958. 
'
3. Hamming, R.W., Numerical Methods for Scientists and Engineers, McGraw-Hill, 
New York, 1962.
4. Kuo, F.K. and Kaiser, J. E, Eds., System Analysis by Digital Computer, John Wiley 
& Sons, New York, 1967, chap. 7.
5. Gold, B., et al., Digital Processing of Signals, McGraw-Hill, New York, 1969.
6. Rabiner, L.R. and Rader, C.M., Eds., Digital Signal Processing, IEEE Press, New 
York, 1972. 
"
7. Oppenheim, A.V. and Schafer, R.W., Digital Signal Processing, Prentice Hall, En­
glewood Cliffs, NJ, 1975.
8. Rabiner, L.R. and Gold, B., Theory and Application of Digital Signal Processing, 
Prentice Hall, Englewood Cliffs, NJ, 1975.
9. Peled, A. and Liu, B., Digital Signal Processing: Theory, Design, and Implementation, 
John Wiley & Sons, New York, 1976.
10. Tretter, S.A., Introduction to Discrete-Time Signal Processing, John Wiley & Sons, 
New York, 1976. 
' 
'
11. Bellanger, M., Digital Processing of Signals, John Wiley & Sons, New York, 1984.
12. Orfanidis, S., Optimum Signal Processing: An Introduction, MacMillan, New York, 
1985.
13. Ludeman, L.E., Fundamentals of Digital Signal Processing, Harper & Row, New 
York, 1986.

Introduction 
17
14. Roberts, R.A. and Mullis, C.T., Digital Signal Processing, Addison-Wesley, Read­
ing, MA, 1987.
15. Oppenheim, A.V, and Schafer, R.W., Discrete-Time Signal Processing, Prentice Hall, 
Englewood Cliffs, NJ, 1989, chaps. 4, 5.
Vectors, Matrices, and Linear Algebra
16. Strang, G., Linear Algebra and Its Applications, 3rd ed„ Harcourt Brace, Orlando, 
1988. 
’
17. Lipshutz, S., Schaum's Outline of Theory and Problems of Linear Algebra, 2nd ed., 
Schaum's Outline Series, McGraw-Hill, New York, 1991.
18. Lyons, L., All You Wanted to Know About Mathematics but Were Afraid to Ask, vols.
1 and 2, Cambridge University Press, Cambridge, 1998.
Matiab
19. Etter, D.M., Engineering Problem Solving with Matlab, Prentice Hall, Englewood 
Cliffs, NJ, 1993. 
'
20. Moler, C., Ed., The Student Edition o/MATLAB,The MathWorks, Inc. (latest edition).


■ ~~~ ~
Least Squares, Orthogonality, 
and the Fourier Series
2.1 
Introduction
The three topics reviewed in this chapter have been fundamental to DSP 
since its beginning. An understanding of these subjects provides insight into 
almost every area of spectral analysis and DSP system design. The three 
topics are related to each other. The use of orthogonal functions greatly 
simplifies the process of finding aleast-squares fit of a linear function to a set 
of data. The Fourier series is an important example of a linear least-squares 
function, and the Fourier series is a series made up of orthogonal functions.
In this chapter, we provide a brief review of these topics as they are applied 
in DSP. The list of references at the end of the chapter includes more general 
texts on these subjects.
------------------
2.2 
Least Squares
The principle of least squares is used often in DSP with signals and other 
functions of one or more variables. In this discussion, we will assume func­
tions of just one variable (time), that is, waveforms, because they are sim­
plest, but the concepts apply to images as well.
To begin, suppose we have two continuous functions,/(f) and /(c, f), 
where c is a constant or an array of constants (that is, c is not a function of t). 
The elements of c may then be selected to make /(c, t) a least-squares approx­
imation to /(f) with respect to a specified range of f, say from f, to f2. If c is 
selected in this manner, then the total squared error,
TSE = [ (/(f)-/(c, f)) di 
(2.1)
is as small as possible.
19

20 
Digital Signal Processing with Examples in Matlab
-5---------------------------------------------------- i------------- ‘—v-------- '
Q 
200 
400 
600 
800 
1000
Time (1)
FIGURE 2.1
fit) and continuous least-squares approximation.
An example of a continuous least-squares fit is illustrated in Figure 2.1. 
The first function, f(t), is the triangular wave with period 500, plotted from 
f! = 0 to f2 = 1000. In the second function, J(c, t) = c sin(2?rt/500), c is adjusted 
to minimize the TSE in (2.1), and so f(t) is a least-squares fit to/(f) in the 
interval [fa, f2] = [0, 1000]. Note that there is only one constant to adjust in 
this example, that is, c is a scalar in this case, adjusted to minimize the TSE.
In DSP, least-squares approximations are made more often to discrete (sam­
pled) data rather than to continuous data. Instead of starting with a continuous 
function,/(t), we start with a sample vector with N elements, f = [J\ f2 ... /kJ- 
If the approximating function is again /(c, f), the total squared error, similar 
to (2.1), is now given by the following:
N
TSE = £(/„-/(c, nT))2 
(2.2)
17 = 1
where f„ is the nth element of f, and T is the time step (interval between 
samples) described in Chapter 1, Section 1.4.
An example of a discrete least-squares fit is illustrated in Figure 2.2. The 
functions are the same as in Figure 2.1, but here /(f) is sampled, and c is 
adjusted to minimize the TSE in (2.2). The time step (sampling interval) in 
Figure 2.2 is T = 35, and the data vector, f, has N - 28 elements.
A linear least-squares approximation occurs when f(c, f) is a linear func­
tion of the elements of c. A least-squares polynomial of the form
f (c, t) = Cj + c2t 4-c3t + ■■■ + cMt 
iZ(2.3)
is an example of a linear least-squares function. In this case, there are M 
functions of the form t"‘ with m beginning at zero. More generally, suppose 
gm(t) is the mth function of f, g„,„ = gm(nT) is the value of g„,(t) at t - nT, and 
' ~~---- “ — --- ------- —

Least Squares, Orthogonality, and the Fourier Series
21
FIGURE 2.2
Least-squares approximation to sampled data with time step T = 35.
there are Ad functions in all. Then the linear least-squares function, beginning 
this time with m = 1, may be written
/(c, nT) = c;gln + c2g2n + c3g3n + ■■■ + cMgMn 
(2.4)
and the TSE in (2.2) becomes
N _
TSE = £(/„-/(c,«T))
n=l
n r 
m 
\2
~ S ^” ^1 Cm%mn I 
(2.5)
n=l \ 
ni=l 
)
The TSE, viewed as a function of the vector c, is minimized by setting its 
gradient vector to zero; that is,
V(TSE) = [2151 iJSE ... 9T5E1 = [0 0 ... 0] 
(2.6)
L oCj 0C2 
J
Using (2.5) in (2.6) produces M equations of the following form:
<?TSE
= 0;m = l,2,
(2.7)

22
Digital Signal Processing with Examples in Matlab
Dividing both sides of (2.7) by -2 and exchanging the summation order in 
the second term, we obtain
MN 
N
= ^fngmn, m = 1, 
V(2.8)
k>l n=l 
n»l
This is the set of linear equations to be solved for.the ieas.t-,sfluares..coefficient 
vector, c. To simplify the notation, we use the following vector notation for 
the N samples of each function in (2.8):
G,„ =[#„,! g„,2 ■■■ 
/ = l/i fz ■■■ /n]; C = ki c2 ... cM]' (2.9) x
Note that c is defined as a column vector. Then, using the Matlab operators 
described in Chapter 1, Table 1.1, all M equations in (2.8) are given by
This expression of (2.8) provides an example of the use of the vector 
product (*) and transpose (') operations in Table 1.1. An element G,„ * G’k 
of the left matrix is seen to be 
in (2.8), an element f * G'„ of
the right vector is seen to be T,n.if„g„m, and therefore, the mth row of 
(2.10) is the same as (2.8) for each value of m. Once we see that (2.10) 
expresses the M equations in (2.8), we can go one step further by defining
G = [G; G'2 ... GmI
Vz(2.11)
so that there is a row of G for each sample («) and a column for each function 
(Af). Then, we may express (2.10) and, therefore, (2.8), as follows:
G' * G * c = (f * G)' 
(2.12)
This result is a general expression of the discrete linear least-squares equa­
tions. In accordance with Table 1.1 in Chapter 1, the solution is then given by
c = (G' * G)\(f * G)' 
^(2.13)
The best way to see how these results work is to do an example.
Suppose we have a signal for which the elements of the sample vector f= 
Ifi fz /n] with N = 12 are the first 12 samples in Figure 2.2 (also shown


24
Digital Signal Processing with Examples in Matlab
The key operation here is the vector product inside the sine argument that 
causes G to be a 12x3 matrix. Having found G, we now have all the elements 
needed to express the least-squares equations in (2.12). If you are using 
Matlab, the following expressions, along with (2.16) and row vector f, will 
solve the equations as in (2.13) and plot the data and the continuous approx­
imation as shown in Figure 2.3.
c=(G'*G)\(f*G)’
n=[0:12*35-1]';
arg=2 *pi*n/500;
fhat=[sin(arg) sin(3*arg) sin(5*arg)]*c;
plot (35* [0.: 11], f,'o’,n,fhat); grid
(2.17)
Note in (2.17) that we have changed the time step from 35 to 1 to produce 
the continuous plot.
One other point may be made concerning the least-squares equations, (2.8) 
and (2.12): The equations may not be solvable. As with any set of M linear 
equations with M variables, a unique solution does not exist unless the 
equations are independent. This will not be thejzase if the number qf_data 
points, N, is less than M.1 Hence, N > M is always assumed in our discus­
sions of least-squares. Furthermore, if N = M, the function and the data have 
the same degrees of freedom, the fit is exact, and the TSE is zero. We will 
not discuss solvability further here, but the points just discussed are illus­
trated in Exercises 1(a) and 2 at the end of this chapter.
In summary, the discrete least-squares process is used in many areas of DSP. 
It consists of fitting a linear combination of continuous functions with adjust­
able coefficients to a set of data samples in a way such that the total squared 
error (TSE) defined in (2.5) is minimized. This, in turn, involves solving the 
linear equations in (2.8), or in matrix form in (2.12), for the vector c of least­
squares coefficients.
In the next section, we will see that when the continuous functions are 
orthogonal with respect to the set of data samples (we will explain what this 
means), the process of finding the vector c is greatly simplified.
2.3 Orthogonality
In addition to the principle of least squares, orthogonality is another impor­
tant concept used often in DSP. Tire word comes from a Greek word (orthos) 
implying a perpendicular or right-angled relationship. In mathematics, two 
row vectors a and b are said to be orthogonal if their inner product given by 
a * b' is equal to zero. When a and b have just two or three elements, then the 
original meaning of orthogonality holds, and the vectors are perpendicular to

Least Squares, Orthogonality, and the Fourier Series 
25
FIGURE 2.4
Orthogonal vectors A and B form a right angle at the origin.
each other, as illustrated in Figure 2.4. When vectors have more than three 
elements, we lose the spacial picture of orthogonality, but the definition, that 
is, a * b' = 0, holds for any number of elements, provided only that a and 
b have the same number of elements.
Ln Chapter 1, Section 1.4, we discussed the concept of waveform vectors in 
signal space. We say two such waveform vectors are orthogonal when their 
product vanishes as just described. Sometimes these vectors occur from 
measurements, that is, analog-to-digital conversion, and sometimes they 
occur from the evaluation of functions. In the latter case, orthogonality must 
be evaluated with respect to a specified set of samples. That is, the vectors in 
this case consist of specified samples of the functions. For example, suppose 
we have N samples of two functions of f, a(t), and b(f), sampled at t = 0, 
T,...,(N- IfT to produce elements 
and b0,...,bN_v These are then the
elements of two vectors, a and b, and the condition for orthogonality with 
respect to the N samples is a * b' = 0; that is,
N-l
= 0 
(2.18) -
>1=0
Suppose, for example, that a„ = sin(2rot/N) and b„ - cos(2rot/N). Then, using 
the result in Chapter I, Equation (1.22), we could say that the two functions 
are orthogonal with respect to equally spaced samples over an integral 
number of cycles of the functions. But we could also show that the two 
functions are, in general, not orthogonal with respect to other sets of equally 
spaced samples.
Knowing when functions are orthogonal is important in the least-squares 
process, because when they are, the process is simplified. Look again at (2.4), 
in which the approximating function, f(c, nT), consists of a linear combina­
tion of M functions, 
g„„..., gM- As in (2.9), each of these functions
has a sample vector given by G„, = [g,nl gm2 ... gmNJ. Now, suppose the M 

26 
Digital Signal Processing with Examples in Matlab
functions are mutually orthogonal with respect to the N samples, that is,
G,„ *Gt = 0; m*k 
l^(2.19)
Then all the off-diagonal matrix elements in (2.10) are zero, and each of the 
equations in (2.10) becomes a simple expression of the form
G,„ * G', * c,„ = f * G',; m - 1,2,...,M 
/ (2.20)
Noting that c,„ is a scalar and that the two vector products are also scalars, 
we see that the solutioh for c„, in (2.20) is
n 
n
cm = 
m = 1,2......M 
(2.21)
n=i 
«=i
Thus, when the least-squares functions are orthogonal with respect to a given 
set of samples, we do not need to solve simultaneous equations to get the 
leasFsquarS'coefficient vector. The vector elements can be determined one 
by one as in (2.21).
In the next section, we discuss the discrete Fourier series as an example 
of least-squares with orthogonal functions.
2.4 The Discrete Fourier Series
A discrete Fourier series consists of combinations of sampled sine and cosine 
functions like those we have been using as examples in this chapter. The 
Fourier series is named after J. B. J. Fourier (1768-1830). It forms the basis 
of a branch of mathematics called harmonic analysis, which is applicable to 
the study of all kinds of natural phenomena, including the motion of stars 
and planets and atoms, acoustic waves, radio waves, etc. The contributions 
of Fourier are a principal part of the foundation of DSP. Fourier's basic 
theorem, put into the DSP terms we are using here, states that any sample 
vector, no matter what its size or origin, can be exactly reproduced by 
summing a sufficiently long series of harmonic functions.
We must first describe what we mean by a harmonic function. To do this, 
we must first review a few basic properties of signals. Suppose we begin 
with a sample vector like the one we used earlier in this chapter, x = 
[x0,x1,...,xN_1]. (Notice that here the indexes range from 0 to N - 1 instead of 
1 to N. This is admittedly annoying, but it cannot be helped, because

Least Squares, Orthogonality, and the Fourier Series 
27
FIGURE 2.5
Sample vector and periodic extension; N = 50.
sometimes we want the array index to start at 1, and sometimes we want 
the time of the first sample to be 0. We often need this convention in using 
Matlab for DSP—for example, whenever the first element in an array is the 
sample at t = 0.)
We say the fundamental period of x is N samples. When we say this, we are 
really imagining that the samples of x repeat, over and over again, in the 
time domain. In other words, we are really thinking of a periodic extension of 
x, illustrated in Figure 2.5. The elements of x are plotted on the left in Figure 2.5. 
They are shown connected with line segments to show approximately the 
original continuous function, x(t), before sampling. The periodic extension 
of x on the right is formed by repeating x every N samples endlessly in both 
directions as shown.
Any function that repeats like the one on the right in Figure 2.5 is called 
a periodic function. The fundamental period is N samples, or NT seconds, where 
T is the time step in seconds. The fundamental frequency is the reciprocal of 
the fundamental period, or f0 = 1/NT Hertz (Hz). This, as we can see, is the 
number of times the function repeats (number of periods) in one second; 
that is, "Hertz" means "cycles per second." For example, suppose T = 1 ms 
(0.001 s) in Figure 2.5. Then the fundamental period is NT - 50 ms, and the 
fundamental frequency is/= 20 Hz.
We also use two other measures of frequency besides/in this text. One of 
them is a> = 2?$^rad/s (radians per second). There are 2x radians in one cycle 
(rotation on the complex plane), and the factor 2n converts cycles/s to rad/s. 
We will introduce the third measure of frequency later.
We now introduce the discrete Fourier series as a least-squares approximation 
in the form of (2.4). We use two equivalent forms of the Fourier series in this 
text. These are given on the first and last lines of the following:

28 
Digital Signal Processing with Examples in Matlab
M-l .
x,,-x(c,nT) = 
£ 
k (2.22)
M-l
, jmn<D0T -jmna^T
= Co + X c"’e + c-"<e 
m=l
M-l
- c0+ Y (cM + c_IH)cosmn co0T + j(cm - c_„Jsinmno)0T 
»i=i 
M-l
- + T amcosmna>()T + b,usinmncoQT 
(2.23)
2
m=l
In these expressions of the Fourier series, a>0 is the fundamental frequency 
of the series in rad/s, t = nT is the continuous time variable, and b„„ and 
c,„ are coefficients similar to those in (2.4). Notice that for c„„ m ranges from 
-(M - 1) to Al - 1 instead of 0 to M - 1, and that b0 = 0. The relationships of 
the coefficients in the two forms of the series are summarized in Table 2.1. 
They can be derived from (2.23) and the line just above (2.23). The prime (') 
in Table 2.1 indicates the complex conjugate.
Neglecting the constant terms, a0 and c0, the Fourier series in its second 
form (2.23) is easily seen to be periodic with period 2m'cq], because each sine 
and cosine function goes through m cycles during this period.
Furthermore, a Fourier series is a sum of harmonic functions. The terms 
for m = 1 in (2.22) and (2.23) are called fun da mental terms. The terms for 
m = 2 are second harmonic terms, and so on. For each value of m, the 
frequency of the with harmonic is mco0 rad/s, that is, m times the funda­
mental frequency. 
........ '
Because the Fourier series is a least-squares approximation, the coefficients 
in (2.22) and (2.23) must be least-squares coefficigntsTlf1we derive one set of 
these, we can find the other set using Table 2.1, so let us derive the a and b 
coefficients in (2.23). These may be determined using (2.21), because the 
harmonic functions are mutually orthogonal with respect to any set of reg­
ularly spaced samples over one period of the fundamental function. As in 
Figure 2.5, we may be thinking of a single sample vector, x = [xo,...,^^], or 
we may be analyzing a periodic function with repetitions of x. In either case,
TABLE 2.1
Equivalence of Fourier Series Coefficients
[rt, b] in terms ofc = 2c0; a,„ = c,„ + c_m; m > 0
t’o = O; b„ =j(c„ — m > 0
c in terms of [a, b] c0 = a0/2; c„ = (a,„ + jb.„,)f2; m <0
C„ = cl„ = (a„ - jb.„)/2; m > 0 

Least Squares, Orthogonality, and the Fourier Series
29
if the fundamental period, 2k/c£>0, covers N samples or NT seconds, then the 
fundamental frequency must be
(Oo = ~rad/s 
(2.24)
With this substitution to indicate sampling over exactly one fundamental 
period, (2.22) and (2.23) are now expressed as follows:
M-I
xa = x(c, nT)= cnie}2n,un,N 
(2.25)
M-l
=—+X a”,cos (2Ttnm 7 +b'"sin 1(226)
m=l
In this form, the harmonic functions in (2.26) are orthogonal with respect to 
the N samples of x. That is,
N-l
£cos(27Cffl«/N)sin(27rto/W) = 0; m, k>0
n-0
N-l
£ cos {2Kmn / N) cos {2k kn/N) = 0; m,k>0 and m*k (2.27) 
n=0
N-l
£ sin(27t?wn/N)sin(2ickn/N) = 0; m, k > 0 and m * k 
n-0
These results may be proved using the trigonometric identities in Table 1.2 
and the geometric series application in (1.21).
Because the harmonic functions in (2.23) are all mutually orthogonal, we 
can apply (2.21) to solve for the least-squares coefficients. In the denominator 
of (2.21), if gmn is one of the sine or cosine terms in (2.27), we can again use 
the identities in Table 1.2 for sin2© and cos2© and the geometric series formula 
to show that the denominator of (2.21) is N/2. For example,
N-l 
N-l 
N-l
£sin2(2rt:mmN) = £(l-2cos(2nmn/N))/2 = £1/2-0 = N/2 
(2.28)
n=0 
n=0 
n=0
Thus, using x„ for the sample data in (2.21), the least-squares Fourier coef­
ficients in (2.26) are
N-l
a„, = ^'^lx<i<tos{2nmn/N');
n=i 
(2’29^
b,„ = — £xnsin(2nmn/N); l<m<M-l
n=0

(2.30)
30 
Digital Signal Processing with Examples in Matlab
In using (2.21) to derive (2.29), we note that c in (2.21) stands for any coef­
ficient so long as the orthogonality condition holds. When we are talking 
about the Fourier coefficients in (2.25), the elements of c, which are not all 
mutually orthogonal, are found using Table 2.1:
Cm
C-,K
A summary of the discrete Fourier series in (2.25) and (2.26) and the coeffi­
cient formulas we have just derived is presented in Table 2.2. We emphasize 
again that these formulas work only because the fundamental frequency of 
the series is cq, = 2n/NT rad/s. If we wish to apply the Fourier series to 
samples where this is not the case, as we did in the example using the 
samples in (2.14), then we must use the general least-squares solution in 
(2.13). We now consider an example where the formulas in Table 2.2 can be 
applied.
Suppose a continuous signal is sampled, and the result is the vector with 
samples shown on the left in Figure 2.5. The upper equations in Table 2.2 
are applied to these samples with M = 5 in the following Matlab file:
% Chapter 2: Discrete Fourier Series.
% Index vectors, n and m.
N=50;
n=[0:N-l];
M=5;
ltl= [0 :M-1] ;
% Sample vector, x.
x=[zeros(1,28),ones(1,12), zeros(1,N-40)];
% a and b coefficients. Note: b0=0 and aO/2 replaces aO.
a=(2/N) *x*cos(2*pi*(n')*m/N);
a(l)=a(l)/2;
b=(2/N)*x*sin(2*pi*(n‘)*m/N);
xhat=a*cos(2*pi*(m’)*n/N)+b*sin(2*pi*(m’)*n/N);
(2.31)
There are several things to notice in the Matlab code, especially in the 
array products. First, the index vectors (row vectors) are set to begin at 
zero. Then the sample vector x in Figure 2.5 is specified as a row of zeros 
followed by ones followed by zeros, with a total of N elements. Next, in 
the computations of a and b, note how the product («') * m produces an 
IV by M array of sine or cosine values which, when multiplied by x, 
produce the a and b row vectors. Note that a(l), which is really a0, is 
divided by two to satisfy the form of x„ in Table 2.2. Finally, note that x

Least Squares, Orthogonality, and the Fourier Series 
31
TABLE 2.2
Discrete Fourier Series and Coefficient Formulas
A4-1 
2*V-1
x„ “ + Xi,|,r cos(2/n?in/N) +!?,„ sin(2ffmn/N) == j^^.vHcos(2ffwrn/N); 0 < m < M - 1
WJ=1 
»-0
b,„ ~ ]\r X xMsin(2^j«n/N);
u=Q
N-J
M-1 
1 V -i2<»wir'N r. , 
. i g
i„ = Y c,Z™;N 
' 0<m<M-l
it =0 
jrr = -(M-t i
c„, = C,'„
FIGURE 2.6
Discrete Fourier series approximations with increasing values of the series size, M. 
Waveforms are plotted versus the sample number.
[xhat in (2.31)] is produced in a similar way, this time using the product 
(m') * n, which produces an M by N array of the indexes. The result, x, 
is plotted in Figure 2.6 for increasing values of M to show how the least­
squares approximation improves with M.
As with any signal vector x, we may consider the vector elements, xn and 
x„ in Table 2.2, as samples of a continuous waveform. If we do this, it is 
reasonable to make two substitutions in Table 2.2 to obtain an approximation, 
x(t), for the continuous waveform. First, we substitute the fundamental 
frequency, tq in (2.24), and second, we substitute continuous time, f, for nT 

32 
Digital Signal Processing with Examples in Matlab
as in (2.4). The result is a continuous version of the discrete formulas in 
Table 2.2:
a M~1
x(t) = 
+ ^a,„cos(m(u0f) + &„,sin(mtt^f) 
(2.32)
m-1
M-l
x(t)= 
£ 
(2.33)
Because the coefficients here are derived from, samples as in Table 2.2, these 
results suggest a method for reconstructing a continuous function from a set 
of samples. We will discuss this idea further in the next chapter.
The results in (2.32) and (2.33) also suggest a continuous form of the Fourier 
series, that is, a computation of the coefficients such that the reconstruction 
of x(f) is exact. Instead of a formal development of the continuous form, we 
offer the following intuitive approach. In the three coefficient formulas in 
Table 2.2, we allow m to be unlimited and substitute the fundamental fre­
quency, ey = 2tPNT rad/s, so that the formula for c,„, for example, may be 
written as follows:
c„, = TT 
0<m<oo; c_„, = c; 
(2.34)
Z 7C ■*—* 
n«0
With c,„ in this form, we can imagine decreasing the time step, T, toward 
zero, and at the same time increasing N in a way such that the period, NT, 
remains constant. The samples (x„) of x(f) are thus packed more densely, so 
that, in the limit, we have the Fourier series for a continuous periodic func­
tion, x(f):
x(t) = £ cme ; c„, = — [ x(f)e dt (2.35)
Z.7C JQ
Sometimes, for the sake of symmetry, cm is given by an integral around t - 0 
as follows:
c« = xWe dt 
(2.36)
By separating the integral in (2.35) into two halves and substituting u = 
t - 2^/cq, in the second integral, one can see that the two forms of cm are equiv­
alent. Derivation of formulas for am and bm is left for Exercise 13 at the end of 
this chapter. In DSP, we are more generally concerned with sampled signals 
rather than continuous signals. The continuous forms of the Fourier series are, 
nevertheless, applicable to a wide range of natural periodic phenomena.

Least Squares, Orthogonality, and the Fourier Series 
33
In this section, we have introduced two forms of the discrete Fourier series 
and shown how to calculate the coefficients when the samples are taken over 
one fundamental period of the data, which is the usual case. In the next 
chapters, we will see that harmonic analysis, and hence the Fourier series, 
forms the basis for the discrete Fourier transform and for spectral analysis 
in general.
2.5 Exercises
1. Find the parabola, that is, Equation (2.3) with M = 3, that forms a 
least-squares fit to each set of data points given in (a) through (d) 
below. In each case, assume that the time step, T, is one, so the 
samples in/are taken at t = 0, 1, ...,N - 1, where N is the length 
off. Write a Matlab file to perform the following steps:
• Compute the vectors Glz G2, and G3 in (2.9); that is, the columns 
of G in (2.11).
• Form the array G in (2.11).
• Solve for the least-squares coefficients, clz c2, and c3, using (2.13).
• Make a continuous plot of the least-squares parabola, /(t), in the 
range t = [0, N- 1]. Use 1000 values to produce a smooth plot.
• Finally, on the same graph, plot the elements of / as discrete 
points. Label the plot using functions xlabel, ylabel, and title and 
observe whether the least-squares fit seems to be correct.
a. /= [1 3 2]
b. f= [1 3 4 2]
c. /= [1.1 2.5 3.2 3.8 3.7 3.1 2.0]
d. /= [8 6 4 2.5 1 0 -0.5 -0.5 -0.5 0 1.5 2.5 4 5 7 8]
2. Try to execute the program in Exercise 1 using a vector /= [-1 3], 
that is, a vector with N < M.
a. Print the coefficient matrix and the right-hand vector in (2.10).
b. Show from (a) why the three linear equations are not indepen­
dent, and why therefore, a unique solution for c does not exist.
c. Show what happens when the Matlab backslash, (matrix)) 
(vector), is attempted.
d. Explain in simple geometric terms why the three equations do 
not have a single unique solution.

34 
Digital Signal Processing with Examples in Matlab
3. Some dates and closing values of the Dow-Jones Industrial Average 
during 1960 were as follows:
Yr Mo Da Close Yr Mo Da Close Yr Mo Da Close
1960
1
4
679.060
1960
5
3
607.730
1960
8
31
625.990
1960
1
22
645.850
1960
5
23
623.660
1960
9
21
594.260
1960
2
11
618.570
1960
6
13
655.850
1960
10
11
588.750
1960
3
3
612.050
1960
7
1
641.300
1960
10
31
580.360
1960
3
23
622.060
1960
7
22
609.870
1960
11
21
604.540
1960
4
12
626.500
1960
8
11
622.880
1960
12
12
611.940
a. Using steps similar to those in Exercise 1, find and print the 
coefficients of a least-squares straight line ("trend") using this 
data. (Hint: change the dates to fractions of a year using a 
reasonable approximation.)
b. Plot the data points and the trend line on the same graph. Make 
the ordinate range [0, 1000] and extend the trend line beyond 
1960 through the first half of 1961.
c. Also (on the same graph) plot the following closing values. WaS 
the 1960 trend valid in the first half of 1961?
Yr Mo Da Close Yr Mo Da Close
1961
1
3
610.250
1961
4
13
692.020
1961
I
23
639.820
1961
5
3
688.900
1961
2
10
639.670
1961
5
23
700.590
1961
3
3
671.570
1961
6
14
695.810
1961
3
23
675.450
4. The intensity, I, of a certain extraterrestrial source is known to vary 
sinusoidally around a constant value, c0, with a period of 1000 
years. We have the following history:
Date Intensity (D Date Intensity (I) Date Intensity (I)
1/1/1900
167
1/1/1940
105
1/1/1980
42
1/1/1910
152
1/1/1950
89
1/1/1990
27
1/1/1920
136
1/1/1960
74
1/1/2000
11
1/1/1930
121
1/1/1970
58
a. If time (f) is measured in years, what is the fundamental fre­
quency (<Ub) of I in rad/year?
b. Using steps similar to those in Exercise 1, find, c0, cv and c2, so 
that the function
/(f) = c0 + CiSinoV + Cjcoscoof
is a least-squares fit to the data.
c. Plot /(f) from year 1900 through year 2999, and plot the sample 
values on the same graph.

Least Squares, Orthogonality, and the Fourier Series 
35
d. Explain why the Fourier coefficient formulas in Table 2.2 are not 
valid in this exercise.
5. Suppose we have two sample vectors, x and y, each with 
N + 1 samples taken at t = 0, T,2T,..., NT from the functions 
x(t) = a(t - NT/2) and y(f) = x2(t), where a and b are constants.
a. Write a program to show whether or not x and y are orthogonal 
for N = 100 and N = 101. For convenience, use a = b = T = 1.
b. Prove that x and y are orthogonal for any N > 0.
6. Prove that two different harmonic functions, anc[ jn 
the complex Fourier series are orthogonal with respect to the set 
of samples at n = 0, 1,...,N - 1.
7. Two theorems are important and helpful when applying the 
Fourier series to a sampled periodic function, say x(t), when the 
samples are at f = 0, T,...,(N - 1)T. The first theorem is
If x(t) is an even function, that is, x(nT) - x(-nT) = x((N - n)T), 
then the b„, coefficients are all zero, C„, = c_,„ is real, and the Fourier 
series is a cosine series.
a. Assume N is even. In the formula for b„, in Table 2.2, show that 
the terms in the sum for n = 0 and n = N/2 are both zero.
b. Show that if x(t) is even, x„ - for any value of n.
c. Using (b), separate the nonzero terms in the sum into two sums 
that cancel, thus proving the theorem.
8. The second theorem is similar to the first theorem in Exercise 7:
If x(f) is an odd function, that is, x(nT) = -x(-nT) = -x((N - n)T), 
then the am coefficients are all zero, c„, = -c_m is imaginary, and 
the Fourier series is a sine series.
a. Assume N is even. In the formula for a,„ in Table 2.2, show that, 
since xn = x_„, the terms in the sum for n = 0 and n = N/2 must 
both be zero.
b. Show that if x(t) is odd, x„ = -xN_„ for any value of n.
c. Using (b), separate the nonzero terms in the sum into two sums 
that cancel, thus proving the theorem.
9. The triangular waveform in Figure 2.1 is seen to have amplitude 
5.0 and period 500.
a. Write a Matlab expression to construct a row vector, x, of sam­
ples of the waveform x(t) over exactly one period beginning at 
t = 0 and using sampling interval T = 0.5.
b. Which of the theorems in Exercises 7 and 8 applies to this 
waveform?

36 
Digital Signal Processing with Examples in Matlab
c. Using the appropriate theorem, find and print the Fourier series 
coefficients for the first form of the series in Table 2.2 for M = 6.
d. Using the first form of the series in Table 2.2, plot exactly one 
cycle of the original waveform, x(f), and the Fourier series 
approximation, x„ with M - 6, versus t on the same graph.
10. Repeat Exercise 9, but this time use T~ 25 and do part (d) differently. 
Instead of plotting the original waveform, plot x as a set of discrete 
points. Then plot the continuous approximation, x(f) in (2.32), 
using 2000 samples of t.
11. A periodic binary (two-valued) waveform has a fundamental 
period equal to 1 ps (microsecond). One hundred samples are taken 
at t = 0.00, 0.01,....,0.99 ps. The sample vector x consists of 20 ones 
followed by 61 zeros followed by 19 ones.
a. Do either of the theorems in Exercises 7 and 8 apply to this 
vector? If so, which?
b. Using M = 16, compute the complex Fourier series coefficients, 
c„, in Table 2.2, for m = 0,1,...,M - 1. Do the computed values 
agree with your answer to (a)?
c. Plot x as a set of discrete points, and plot the continuous 
approximation, x(f) in (2.32), using 1000 samples of t.
12. The average monthly high and low temperatures (°F) in Albuquer­
que, NM, are as follows:
Months 
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
Avg; High: 
46 53 
61 
70 
79 
90 
92 
89 
81 71 
57 i7
Avg. Low: 
21 26 
32 
39 
48 
58 
64 
62 
55 43 
31 
23
a. Do either of the theorems in Exercises 7 and 8 
apply to this
vector? If so, which?
b. Compute the Fourier series coefficients aa, ar, and bi in Table 2.2.
c. Plot each temperature vector as a set of discrete points, and plot 
each continuous approximation using (2.32) with 1000 samples 
of t. Make all plots on the same graph with the abscissa labeled 
in months, that is, January = 1, February = 2, etc.
d. Comment on why the least-squares fit is as good as it is with 
M = 2. In other words, why is the average annual temperature 
variation nearly sinusoidal?
13. Complete the following:
a. Derive formulas similar to those in (2.35) for the continuous 
Fourier series in terms of sine and cosine functions. (Hint: Use 
the formula involving e’e in Chapter 1, Table 1.2.)
b. Give formulas similar to (2.36) for the Fourier series coefficients 
found in part (a).

Least Squares, Orthogonality, and the Fourier Series 
37
References
Least Squares
1. Giordano, A.A. and Hsu, F.M., Least Square Estimation with Applications to Digital 
Signal Processing, John Wiley & Sons, New York, 1985.
Fourier Series
2. Bellanger, M., Digital Processing of Signals, 2nd ed., John Wiley & Sons, New York, 
1989, chap. 1.
3. Lyons, L., All You Wanted to Know About Mathematics But Were Afraid to Ask, 
Cambridge Press, Cambridge, U.K., 1998, chap. 12.


Correlation, Fourier Spectra, 
and the Sampling Theorem
3.1 
Introduction
The word spectrum has several meanings. In signal processing, the spec­
trum of a signal is a particular kind of mapping function. In the case of a 
waveform, the spectrum maps the signal from the time domain into the 
frequency domain, where we can see exactly how the signal content is dis­
tributed over frequency. In the case of an image, the spectrum maps the 
signal from the spatial domain of the image into a two-dimensional fre­
quency domain. Sometimes the spectrum of a waveform or an image shows 
us things we can only see less clearly or even not see at all, when we look 
at the signal in the time domain. We will see some examples of this later 
in this chapter.
In this chapter, we discuss the Fourier spectrum of a signal in terms of the 
discrete Fourier transform and the continuous Fourier transform. (Later, we will 
discuss other transforms as well.) These two transforms allow us to obtain 
the spectral properties of continuous and discrete (sampled) signals.
The two Fourier transforms also provide a means to prove the Sampling 
Theorem, w’hich is basic to all DSP. The sampling theorem tells us how often 
a given continuous signal must be sampled in order to preserve all of the 
information in the signal, so that we are able to compute the spectrum 
correctly and even reconstruct the continuous signal from the samples. The 
sampling theorem then allows us to discuss aliasing, which is something that 
happens to the spectrum when a signal is not sampled often enough.
The final subject in this chapter is waveform reconstruction, that is, methods 
for obtaining or estimating the original continuous waveform from a set of 
discrete samples.
Before we begin with these subjects, we introduce another operation 
known as correlation. Correlation has many uses in signal processing, but it 
is introduced here for one main reason: the discrete Fourier spectrum of a 
signal, x(t), can be understood as a correlation of x(t) with a set of sine and 
cosine functions.
39

40 
Digital Signal Processing with Examples in Matlab
3.2 
Correlation
We begin the discussion of spectra by reviewing correlation as it applies to 
signal processing. Basically, correlation is a measure we use to tell how much 
two waveforms or two images are like each other. We also use autocorrelation 
to tell how much a waveform at time t is like itself at some other time, t + r, 
or how much an image at location (x, y) is like itself at some other location, 
(x + g, y + r).
Suppose we have two waveforms, x(t) and y(t), defined in the range t = 
[0, “>). (This notation means the signals are defined from the lower limit to 
just before the upper limit.) Then, the correlation function of x and y is
1 rL
<pxy(r) = E[x(f)y(f + r)J = lim - J x(t)y(f + r) dt; ?>0 
(3.1)
where E[»] stands for the expected value. The independent variable rhas the 
effect of shifting y forward in time relative to x, thus making the correlation 
function a continuous function. For sampled waveforms, the correlation 
function is the discrete form of (3.1), that is,
1 £
= E[x„y„+m] = hm 2jc„yn+m; m > 0 
(3.2)
n=0
In this case, the time shift, m, is measured in samples.
We can observe in either (3.1) or (3.2) that the correlation function, as 
defined here, is the average product of two signals. The product is averaged 
without an upper bound so that, as long as the time shift is positive, we do 
not need to be concerned with end effects. If the waveforms are defined 
only in the finite interval [0, NT), then we usually make one of two assump­
tions:
The waveform values are zero outside the interval.
The waveform is periodic with period NT, as illustrated on the right 
in Figure 2.5.
An example of the correlation of two waveforms is given in Figure 3.1, 
in which x(t) is the rectangular signal, and y(f) is the triangular signal. Both 
signals are defined in the expressions for vectors x and y in the Matlab code 
(3.3). The Matlab code computes the correlation function (pxv(m) using (3.2) 
with periodic extension, recognizing that the average product in (3.2) can 
be computed by averaging over one period. Note how the use of yy, the 
one-period extension of y, is used to prevent y„+„, in (3.2) from going beyond 
the end of the vector.

Correlation, Fourier Spectra, and. the Sampling Theorem
41
Sample no. (n)
Time shift (m)
FIGURE 3.1
Signals x(t) and y(f + mT), with m - 0,10, and 20, and correlation function <p,v(mT). Each circle 
on the correlation plot corresponds with one of the signal plots.
n=0:127;
% Waveform values with periodic extension.
x=[ones(1,25),-ones(1,25) , zeros(1,78)];
y= [0:24,25:-1:1,zeros(1,78) 1/25;
yy=[y,y];
% Correlation function.
for m=0:127
phi (m+1) = (1/12 8) *x*yy (m+1: m+12 8) ' ;
end
(3.3)
Figure 3.1 illustrates the operation by showing the two signals aligned first 
with m - 0, next with m = 10, and finally with m = 20. In each picture, notice 
how x„ remains stationary, while y„+,„ (with periodic extension) moves to the 
left. These three cases are circled on the plot of in the lower right 
corner of Figure 3.1. In each case, you can see how the value of is the 
sum of the product of the two curves on the corresponding plot.
Correlation has important uses of its own in signal processing. For exam­
ple, a correlation detector is a system that correlates an incoming signal in real 
time with a stored "target," the objective being to detect the presence of the 
target in the incoming signal. An example of correlation detection is given 
in Figure 3.2. The target signal is shown in the upper plot. The target signal 
appears again in additive random noise at sample numbers 1000 and 3700 
in the center plot. The signal can be seen in the center plot but is more easily

42 
Digital Signal Processing with Examples in Matlab
Example of Correlation Detection
Sample number
FIGURE 3.2
An example of correlation detection. The signal is seen more clearly in the correlation function 
(lower plot) than in the raw data (center plot).
detected in the lower plot, which is the correlation of the target signal with 
the center waveform.
3.3 The Discrete Fourier Transform (DFT)
The discrete Fourier transform (DFT) transforms sampled waveform or 
image data into spectral information about the respective waveform or 
image. We begin our discussion of the DFT with the complex Fourier series 
coefficients in Table 2.2:
c„. = 
~ ^xne'llnmn,N; 0 < m < M-1 
(3.4)
11=0
Because = cosilnmnlN) -jsin(2rcmn/N), and because the sum in (3.4) 
goes over n, we can see that the complex Fourier coefficient in (3.4) resembles 
the finite correlation function in (3.2). In fact, if we define w„ = cosQnmn/N) 
and v„ - sin(2nmn/N), then, using the definition in (3.2), c,„ is the sum of two 
correlation functions:
= q>x„(0) + /tp„(0) 
(3.5)

Correlation, Fourier Spectra, and the Sampling Theorem
43
Thus, recalling from (2.24) that the frequency corresponding with index tn is
2 mu 
,.
co„, = 
= — 
(3.6)
we see that c„, is a measure of correlation of the vector x = [x0 x,... Xn.J with 
unshifted sine and cosine vectors at the frequency a)„,. In this sense, c,„ is a 
measure of the spectral content of x at com rad/s.
The DFT is a scaled version of c,„, and is, therefore, also a measure of the 
spectral content of x. However, the DFT has exactly N components; that is, 
M in (3.4) is equal to N. The N components of the DFT are
N-l
X,„ = '^x,le~l2m“‘l'N; m = 0,1,...,N-l
(3.7)
Thus,
X,„ = Nc,„; m = 0,1,...,M-1 
(3.8)
We will discuss the relative values of M and N later in the section on the 
sampling theorem.
As we have done previously, we use X without a subscript to represent 
the DFT vector with components Xo, X,,. ..,XN_j. Thus,
X = [Xo Xj ... XN_,] is the DFT of x = [x0 Xj ... xN_t] (3.9)
3.4 
Redundancy in the DFT
It is easy to show that the indices m = 0, 1,...,N - 1 in (3.7) are sufficient 
to supply all possible values of the DFT, that is, that values of the DFT 
outside this range of m are repetitive, het K be any integer. Then, because 
g±j2xnK _ 
■
X,
N-l 
N-l
• 
_  v-1 
-/2/t(m±KN)n7N _ V'"’ -jlmnlN ijlitnK
■m±KN “ 
~ 
'
n=0 
H=0
(3.10)
Haus, the DFT is a periodic complex function of tn with period N, and its 
values simply repeat outside the foregoing range of m.
Furthermore, when all the elements of the sample vector are real, essentially 
only half of the DFT components are independent. Again, because e”'2*" = 1 
for any value on n, we have
N-l 
N-l
Y - V v 
_ V v 
_ V'
/ . XHC 
/ , V
,1-0 
>1=0
(3-11)

44
Digital Signal Processing with Examples in Matlab
1
0.3
0.6 
c
X
0.4
0.2
FIGURE 3.3
Signal and DFT
where X,'„ is the complex conjugate of X„, as in Chapter 2, Table 2.1. Thus, 
when the samples (x„) are real, all the independent values of X,„ are in the 
following range:
m = 0, 1,...,^ 
(3.12)
To illustrate the DFT and the properties we have discussed so far, we use 
for our next example the sample vector x in the Fourier series example (2.31) 
of Chapter 2, which is shown on the left in Figure 2.5 and again on the left 
in Figure 3.3. The waveform vector, x, and the DFT vector, X, are computed 
as follows:
% Chapter 3 Example: Discrete Fourier Transform.
% Index vectors, n and m.
N=5 0 ;
n=[0:N-l];
m=[0:N—1];
% Sample vector, x.
x=[zeros(1,28),ones(1,12),zeros (1,N-40) ] ;
% DFT, X.
X=x*exp (—j*2*pi*m1*n/N);
(3.13)
Notice again, as in Chapter 2, (2.31), how the index vectors (n and m) are 
multiplied to produce a square matrix, so that the "X" computation produces 
all N values of X,„ in (3.7).
The DFT magnitude, |Xra|, is plotted on the right in Figure 3.3. As shown 
in (3.10), the DFT is periodic outside the range shown. Furthermore, the 
result in (3.11) is illustrated, because we are plotting the magnitude of the 
DFT, |XN_„,| = |X,n| = |X,„], and so the left and right halves of the plot are 
mirror images.

Correlation, Fourier Spectra, and the Sampling Theorem 
45
A final item to note in the plot of |X„,| in Figure 3.3 is the value of Xo, which 
is seen in (3.7) to be the sum of all the elements in the sample vector x. Because 
there are 12 ones in x and the rest of the elements are zeros, we have Xo = 12.
;-----------------
3.5 
The FFT Algorithm
The fast Fourier transform (FFT) is not a new transform. It is an algorithm for 
producing the DFT exactly as expressed in (3.7) but faster. The FFT appears in 
different forms in DSP hardware and software. In the literature,5-12 the operation 
and structure of the FFT, as well as the interesting history of its development 
are described. Our purpose here is only to observe that the DFT computation, 
as expressed in (3.7), contains redundant complex products (products that must 
be essentially repeated in the computation of the DFT) and that repetitions of 
these products may be eliminated to produce a faster computation.
If (3.7) is used to compute the DFT in the form shown, we can see that 
there are N complex products in the sum and N sums in the transform, so 
there are N2 products in all. Let us now express the DFT in the following form:
N-l
X„, = £x„W'n', m = 0,1,..., N-l, where WN = e~'2x,N (3.14) 
n=0
We can see that WN is a unit vector on the complex plane, and that WkN is 
periodic over k with period N; that is, in Matlab notation, 
where rem(fc, N) is the remainder after dividing k by N.
Redundant values of are illustrated with N = 8 in Figure 3.4. With N = 8, 
the product k = m in the DFT (3.7) ranges from 0 through 49. However, only
FIGURE 3.4
Repeating values of WN with N = 8, showing that only N values are unique.

46 
Digital Signal Processing with Examples in Matlab
eight of these products are unique, as shown in the figure. Using this redun­
dancy in Wjj, the construction of a fast DFT algorithm begins by decompos­
ing the sample vector. Suppose N is a multiple of 2. Then, we decompose 
the samples into two vectors containing even- and odd-numbered samples, 
and we write (3.14) as follows:
NI2-1 
N/2-1
X„, = £ x2„W2^ + Z 
m = 0, 1,...,N- 1 
(3.15)
>1=0 
>1=0
In the definition of WkN in (3.14), we can see that Wj,,2 = for any value 
of k. Thus, (3.15) becomes
N/2-1 
. . 
N/2-1
x,„ = £ x2„W'"';2+ W” % x2,l+1W^2r m = 0,1.......N-l (3.16)
;/-0 
»=0
Although this result seems like a rather complicated way to express the DFT, 
it gives the basis for fast algorithms, because each sum in (3.16) is seen to be 
a DFT, but of size N/2 rather than N, requiring (N/2)2 instead of N products. 
Each smaller DFT is periodic in m with period N/2, and so two periods are 
contained in the range 0 < m < N. Also, there are N/2 unique outer products 
in the right-hand term, because, as illustrated in Figure 3.4, WkN = -W^'w2 
when k > N/2. Thus, (3.16) expresses a total of 2(N/2)2 + N/2 = N2/2 + N/2 
complex products, which are fewer than the N‘products in (3.14).
Suppose further that N is not only a multiple of 2 but also a power of 2, 
say N = 2f\ Then each of the sums in (3.16) can be decomposed as we have 
just described, and (3.16) can be written as the sum of four sums, and so on, 
until after K such steps, there are N sums with only one term in each sum. 
In this process, we have added N/2 complex products at each stage and 
finally produced N sums that contribute N/2 additional nonredundant prod­
ucts. Thus, if N ~ 2k and (3.14) is decomposed iteratively using (3.16) to 
produce the FFT, the DFT computation is reduced as follows:
Complex products in DFT = N2
N N 
(3.17)
Complex products in FFT = — K = — log2N
FFT algorithms are most efficient when N is a power of 2, which is assumed 
in (3.17), because then the number of decompositions (logzN) is maximum. 
However, the process just described works when N has any prime factors 
other than itself. For example, suppose N is a multiple of 3. Then it is not 
hard to see that the equivalent version of (3.16) is as follows:
N/3-1 
N/3-1 
N/3-1
+ x3n+X3 + W2Nm £ X3n+2W™,
n=0 
n-0 
»1=O
m = 0,1,...,N-1 
(3.18)

Correlation, Fourier Spectra, and the Sampling Theorem 
47
Here, we see that there are (N/3)2 products in each reduced DFT, plus N 
outer products in the second and third terms. Therefore, the original N2 
products are reduced to 3(i\73)- + 2/\T (that is, N~I3 -r 2Af) products in this case.
FFT algorithms have different specific forms in software and hardware, 
but they are all based on decomposing the DFT iteratively into successively 
smaller DFTs as illustrated in (3.16) and (3.18). Robust FFT algorithms, such 
as the Matlab fft function, take advantage of all prime factors of the signal 
vector size (N) to reduce the number of complex products.
In Matlab there is a function called flops that allows one to keep track of 
the number of floating-point operations. The flops function may be used in 
the following Matlab code to compute f(N), the number of floating-point 
operations needed in the FFT computation of a vector x of length N. The 
first expression, flops(0), resets the counter, and the second gives/(N).
flops (0);
f f t (x, N) ;
f(N)=flops
Figure 3.5 illustrates the results of computing/(N) for a range of values of 
N. All values of f(N) for 4 < N < 2500 are plotted as circles. In addition, two 
boundary curves are plotted. The upper plot is of a constant times N2, and 
the lower is of a constant times (N/2)log2N. The flops function counts more 
than just complex products, but in Figure 3.5, we can see, nevertheless,
FIGURE 3.5
Floating-point operations (flops) as a function of FFT size (N). Lower boundary plot is propor­
tional to (fV/2)log2N; upper boundary is proportional to

48 
Digital Signal Processing with Examples in Matlab
that the values of/(N) conform to the theory. Notice that when N is a power 
of 2, f(N) lies close to the lower bound. On the other hand, when N is a 
prime number and has no factors other than itself, f(N) is on the upper 
bound.
3.6 
Amplitude and Phase Spectra
We turn now from the computation of the DFT to its use in spectral analysis. 
We introduce the subject in this chapter and continue it in Chapter 7. We 
begin with a discussion of the amplitude spectrum and the phase spectrum of 
a signal vector. These are easily defined. The amplitude spectrum is the 
vector with elements that are DFT component amplitudes, and the phase 
spectrum is the vector of DFT component phase angles in radian units. 
Thus, if X = 
is the DFT of a signal vector, x =
then
Amplitude spectrum = abs(X) = [|X0| |Xj| ... |XN,t|]
Phase spectrum = angle(X) = [/-Xo ZXj ... ZXN_i]
When these DFT functions are applied to measured data, that is, samples of 
a continuous signal, each component is associated with a corresponding 
frequency. Frequency is usually measured in one of four systems of units. It 
is important to get these units straight in one's mind, because they are all 
used in the literature, and they cause a surprising amount of confusion among 
students and even engineers who should know them.
All of the four frequency scales are illustrated below the DFT index (m) in 
the example in Figure 3.6, which is a plot of the amplitude spectrum of
x = r.slPJg.?(.n..Z5P-.5^5J; „ = o, 1 99^1 
(3.20)
L n - 50.5 
J
It is interesting to note that the spectral content of x is mostly below index 
m = 20; however, we leave this point for a later discussion and concentrate 
on the frequency scales below the figure.
First, note that the range of the DFT index (m) is [0,N/2]. As shown in 
(3.10) and (3.11), the amplitude spectrum simply repeats outside this range. 
That is, from m = N/2 to N, the amplitude spectrum is the mirror image of 
Figure 3.6, and outside the range m = [0,N - 1], the DFT is periodic. Thus, 
in the range m = [0,N/2], we have the complete amplitude spectrum.
The first two frequency scales below the index are dimensionless. These 
are based on one rotation of the unit vector e'2lr”,/Non the complex plane, as 
shown previously. The Hertz-second (Hz-s) scale is defined as m/N, and the 
radian (rad) scale is defined as 2?rm/N.

Correlation, Fourier Spectra, and the Sampling Theorem 
49
To
TABLE 3.1
Frequency Conversion Factors
Example: fl (Frequency in rad) — nT*f (Frequency in Hz)
From
Index (m)
Hz-s (v)
rad (£2)
Hz (/)
rad/s (co)
Index(m)
m
mlN
2 71 m/N
ml(NT)
2irm/(NT)
Hz-s (v)
Nv
V
. 2kv
v/T
2 k v/T
rad (Q)
N£2/(2rc)
£2/(2ti)
£2
Q/(2itT)
£2/T
Hz(/)
NfT
/T
2it/T
f
2it/
rad/s (cd)
N«77(2lt)
wT/(2it)
oT
co/(2it)
CD
Amplitude spectrum; N=100
£2
or
o1 ■ 
I
pl---------------
1---------------
0 
0.1 N 0.2N 
0.3N 
0.4N 
0.5N index (m)
6 
03 
5(2 
03 
04 
05 Hz-s (v)
0 
rt/4 
k/2 
3"/4 
it rad (£2)
6 
O.i/T 
0.2/T 
0.3/T 
0.4/T 
05/T Hz (f)
0 
rt/(4T) 
rt/(2T) 
3re(4T) 
rt/T rad/s (co)
FIGURE 3.6
Amplitude spectrum of X = sin[2ic(n - 50.5)/5]/(n - 50.5); n = 0, 1,...,99, illustrating different 
frequency scales.
The last two frequency scales in Figure 3.6 are used with real data, where 
the time step (T) between samples is given in seconds (s). The Hertz (Hz) 
scale is defined as m/(NT), and the radian-per-second (rad/s) scale is defined 
as 2nm/(NT).
The symbol used commonly to represent each frequency measure is 
shown at the right of each scale in Figure 3.6: v for Hz-s, fl for rad, etc. 
All five scales are used in practice to describe frequency in whatever units 
seem convenient to the situation, so the DSP engineer must be able to move 
easily from one scale to another. Table 3.1 relates the scales by providing 
factors for all possible conversions. Note that each pair of symmetric table 
elements above and below the main diagonal represents a reverse conver­
sion, and the factors are, therefore, inverses.

50 
Digital Signal Processing with Examples in Matlab
TABLE 3.2
Sampling Frequencies and DFT Frequency Increments
Index On) Hz-s (v) rad (£1) Hz (f) rad/s (<U)
Sampling frequency 
N 
1 
2ft 1/T 
2 ft/T
Frequency increment 1 
1/N 
1/(NT) 2it/(NT)
Frequency (Hz-s)
FIGURE 3.7
Amplitude and phase spectra of X = sin[2ft(n - 50.5)/5]/(rz - 50.5); n = 0,1. ...,99.
Two other important features of the frequency scales—the sampling fre­
quency and the DFT frequency increment—are given in Table 3.2 for the 
frequency index and for each frequency scale. The DFT index N designates 
the sampling frequency in the sense that, with m = N in the DFT formula 
(3.7), the complex sinusoid gOes through a cycle each time the sample 
number (n) is increased by one. Given m = N at the sampling frequency, the 
rest of the sampling frequencies are found by using the top row of conver­
sions in Table 3.1. The frequency increments in Table 3.2 are determined 
simply by dividing each sampling frequency by N, the number of frequency 
increments from zero to the sampling frequency. Because the index increment 
is one, these are identical with the conversion factors on the first row of 
Table 3.1.
Finally, Figure 3.7 is provided to illustrate the phase spectrum of the signal 
vector in (3.20) together with the amplitude spectrum. This time, the com­
plete frequency range from -0.5 to 0.5 Hz-s (one period of the DFT) is shown 

Correlation, Fourier Spectra, and the Sampling Theorem
51
to illustrate the redundancy in (3.11), that is, XN_,„ = X'„, which implies that 
the amplitude spectrum is evenly symmetric about the origin and the phase 
spectrum has odd symmetry about the origin. Amplitude and phase plots 
are often shown in this form.
The amplitude and phase spectra in Figure 3.7 may be computed with 
the Matlab code shown below. The expressions are self-explanatory, with the 
exception of the fftshift and unwrap functions. The fftshift function swaps 
the upper and lower halves of the DFT vector (X). Thus (assuming N is 
even), the first N/2 DFT components cover [0.5,1) Hz-s, or, equivalently, 
[-0.5,0] Hz-s [see (3.10)], and the complete frequency range is therefore 
[-0.5,0.5), as shown.
The unwrap function is used to "unwrap" the output of the angle function. 
Angle computes the phase in radians of each DFT component using the 
"ATAN2" arctangent formula, which resolves all angles into the range [-.t, tt] 
radians. The umvrap function adds multiples of to eliminate "jumps" from 
~K to 7t or vice versa in the progression of phase elements.
n=0:99;
x=sin(2*pi*(n-50.5)/5)./(n-50.5);
X=fftshift(fft(x));
amplitude=abs(X);
phase=unwrap(angle(X));
(3.21)
3.7 The Inverse DFT
The DFT in the form of (3.7) may be viewed as a mapping in N dimensions 
of a signal vector, x, into a DFT vector, X. Each vector has N components. 
Only half plus one of the DFT components are independent, as we have 
seen, but half minus one are complex and have two independent components 
each (Xa and XN,2 are the exceptions). Therefore, x and X have equal degrees 
of freedom, and an inverse mapping is possible.
The inverse mapping may be found by multiplying X,„ in (3.7) by 
and summing over m. [Note that because n appears in the left-hand sum, 
we use k instead of n on the right when (3.7) is substituted for X,„ ]
N-l 
N-l N-t 
N-l N-l
XX„,e'2“”'"/N = X 
= NXi (3 22)
w=0 
ih=0 £=0 
k=0 nt=0
Once again, this result is an application of (1.21), where the geometric series 
on the right in (3.22) sums to zero for all values of k except k = n, where the 

52 
Digital Signal Processing with Examples in Matlab
sum is N. So, we can now present the DFT as a symmetric pair:
Xm = XV/2M"'N; m = 0,1.......N-l
n=0
z„ = L£X,Z™"'N; n = °'l.......
nt=O
(3.23)
Thus, the inverse transformation from X back to x is the same as the forward 
transform, except for a change of sign in the exponent and the scaling factor 
1/N. The Matlab ifft function implements the inverse transformation as 
shown, again using the fast algorithm described in Section 3.5, with the 
scaling factor included, so that, for example, the expression "y = ifft(fft(x))" 
would (neglecting roundoff errors) be the same as "y = x."
------------------
3.8 Properties of the DFT
In addition to the inverse transform, the DFT has other properties worth 
mentioning. Along with properties described in Sections 3.4 and 3.6, four 
additional properties are summarized in Table 3.3. In this section, we discuss 
these properties. If the reader is already familiar with them, he or she may 
wish to skip the discussion.
The periodicity and redundancy properties of the DFT are given by (3.10) 
and (3.11). Note that if the signal vector x is complex, the redundancy prop­
erty does not hold.
The linearity property follows from the DFT definition (3.7). If ax„ + by„ is 
substituted for x„ in (3.7), we can write the sum in (3.7) as the sum of a times 
the DFT of x plus b times the DFT of y, thus proving the property. We can
TABLE 3.3
Properties of the DFT (x = [x0, xv...,xM]; X = DFT(x) - [X0,XI,...,XN I])
1. Periodicity
2. Redundancy
3. Linearity
4. Two real elements
Xnt* KN Xrpj
XN_W = X'm, that is, XMr, = X'„ if x is real
If s = ax + by, then S =aX + bY
Xo = 
+ xN_2 and Xw/2 = xD- x, + x2-----± are real
if x is real
5. Zero extension
6. Zero insertion
If K zeros are appended to x, Av decreases to Av - 1/(N + X) Hz-s 
If K zeros are inserted after each sample in x, the DFT repeats
K times
7. Resampling
By inserting zeros properly into X, the time step in x can be 
changed
8. Linear phase shift
If is substituted for x„, 2nkm/N rad are subtracted from 
angle(X,„)

Correlation, Fourier Spectra, and the SamplingTheorem 
53
apply the linearity property to the DFT of a complex signal vector as follows. 
If x = u + jv, then the DFT of x must be the complex sum of two DFTs of real 
vectors, that is, X = U + jV. Because this is true, it is usually sufficient to use 
real signal vectors in our discussions of the DFT.
Property 4 states that when x is real, Xo is the sum of the elements of x, and 
XNI2 is the alternating sum of the elements as shown in the table. As discussed 
in Section 3.7, this property results in the DFT of a real signal vector having 
exactly N degrees of freedom in the inclusive index range [0,N/2].
Properties 5 through 7 in Table 3.3 are interesting and useful characteristics 
of the DFT. Properties 5 and 7 are in a sense duals of each other. Each causes 
a form of interpolation in its corresponding transform domain.
First, we consider zero extension in the time domain (Property 5). Suppose we 
form a vector y by appending K zeros to a row vector, x. The Matlab expression 
for doing this would be y = [x,zeros(l,K)]. [The zeros function in this case 
creates a row of K zeros as in the example (3.3), etc.] We now find the DFT of 
y using (3.7). Noting that the last K elements of y are zeros, the result is
N+K-l 
N-l
Y„, = Y y„e-'lm"',{N+K) = 
m - 0,1....... N + K-l
n=Q 
n=0
(3-24)
One way to interpret this result is to say that the DFT of y is the same as the 
DFT of the original vector x, but interpolated in frequency, that is, given at 
frequencies that are more closely spaced. For example, suppose K = N so 
that the length of y is twice the length of x. Then, it is easy to see in (3.24) 
that Y2„, = X„„ that is, every other value in Y is an original DFT component, 
and the in-between values are "interpolated" DFT components.
An example of this with K - 3N is given in Figure 3.8. The two signals and 
DFTs are produced with the following Matlab expressions:
Nl=32; N2=128;
x= [ones (1, N 1/2 ) , -ones (1, N1/2 ) ]
X=fft(x,Nl);
Y=fft(x,N2);
The original signal is shown at top left with Nj - 32, and again with 96 zeros 
appended (N2 = 128) at top right. The original amplitude spectrum is plotted 
at bottom left, and the interpolated version is plotted at bottom right. In the 
latter, note that the original points (circled) coincide with the left-hand spec­
trum, and that between each pair of original points, there are three interpo­
lated points. Notice also the use of the "Hz-s" frequency scale to emphasize 
that frequency range does not change.
This effect of zero-extension in the time domain leads to a continuous version 
of the DFT, because there is theoretically no limit to the number of appended 
zeros and, therefore, no limit to the amount of interpolation. That is, if we

54
Digital Signal Processing with Examples in Matlab
FIGURE 3.8
Zero-extension in time resulting in interpolation in frequency. With N2 = 4N, there are four times 
as many points in the interpolated spectrum.
substitute a> (rad/s) into (3.7) in accordance with Table 3.1, we obtain a 
continuous form of the DFT:
N-l
Continuous DFT: X„, = 
0<g><2zf/T; m - 
(3.25)
>1=0
The question of whether the continuous DFT gives a true representation of 
the spectrum of a sampled continuous signal, x(t), must remain until we 
discuss the sampling theorem later in this chapter.
Zero insertion between samples of a signal vector in the time domain also 
has an interesting effect on the DFT. Suppose K zeros are inserted after each 
sample in x to produce y, so that yt = x„ when k = (K + l)n, and yk = 0 when 
k is not a multiple of K + 1. Then, the DFT of y is
(K+1JN-1 
■ 2 Kink 
N-l 
.2 kwh
y„, = Y y/'<Ktl,N = N = X- m = 0,l,...,(K+l)N-l
fc=0 
J1-0
(3.26)
We get the second sum by using ~ xk, when k = (K + l)n and yk~ 0 otherwise. 
Thus, the DFT of x with zeros inserted is a succession of K +1 DFTs of x. Now 
suppose x is a sample vector with Tj = T seconds between samples. With zeros 
inserted in x, the interval is then reduced to T2 = T/ (K +1), and the corresponding 
sampling frequencies are /j = 1/T and f2 = (K + 1)/T Hz. This, together with

Correlation, Fourier Spectra, and the Sampling Theorem
55
FIGURE 3.9
Interpolation between samples of vector x by inserting K zeros between samples and then 
removing all spectral content above 1/(27) Hz.
the result in (3.26), suggests the time-domain interpolation scheme illustrated 
in Figure 3.9, which will in turn take us to Property 7 in Table 3.3.
In Figure 3.9, an ideal lowpass filter is used following the insertion of X 
zeros after each sample in x. The filter has the effect of limiting the DFT 
of y, so it contains only the components of the first of the sequence of DFTs 
in (3.26), thus creating an interpolated version of x, that is, a vector that has 
K + 1 times as many samples of x and a proportionately smaller time step 
(T2), and yet it has the same spectrum as the original x and the same values 
at the original sample points of x.
The entire process in Figure 3.9 can be accomplished in the frequency 
domain simply by expanding X, the DFT of x, with zeros, and effectively 
creating Yin (3.26) without any frequencies above 1/(2T) Hz, that is, without 
any nonzero components at indices above m - N/2. This we do by inserting 
zeros in the middle of X, as in the following Matlab expressions:
X=fft(x);
Y=[X(1: (N+l)/2) , zeros (1,K*N) ,X( (N+l)/2+1:N) ] ; 
y=(K+l)*ifft(Y);
(3.27)
Here, we assumed that N (the number of elements in x) is odd, so X(2:N) 
can be split into equal halves, and there is no component at m = N/2. (If N 
is even, XNI2 must be split into two halves on either side of the zeros and the 
number of zeros reduced to KN - 1.) Notice that Y, the DFT of y, now has (K 
+ 1)N components and no frequencies above 1/(2T) Hz. Thus, when the 
inverse transform is taken, y contains K interpolated samples after each 
original sample in x, no frequencies above 1/(2T) Hz, and a smaller time step, 
T2 = T/(K + 1). The inverse transform is multiplied by K + 1, because the factor 
in front of the sum in (3.23) has changed from 1/N to 1/((K + 1)N).
An example of this interpolation process with N = 17 and K = 2 is shown 
in Figure 3.10. The original signal vector (x) is at the upper left with time 
step at Tj = 3 ms. The amplitude spectrum, |X|, is shown on the upper right 
and on the lower left with zeros inserted. The spectra are shown using the 
fftshift function described previously to emphasize the frequency content, 
but the computations are done exactly as in (3.27). Finally, the interpolated 
vector (y) is plotted on the lower right, with x overplotted to show that the

FIGURE 3.10
Interpolation in the time domain by expanding the DFT with zeros in the frequency domain. 
N = 17, and K = 2 in this example. The process is equivalent to that of Figure 3.9.
samples of x and y coincide. Again, its properties are that it occupies essen­
tially the same time frame as the original vector, the time step has been 
decreased to T2 = 1 ms, and the spectral content has not been changed from 
the original.
At this point, it is natural to ask whether increasing K indefinitely in 
the process just described would allow one to recover the original con­
tinuous waveform from which the samples in x were taken. And the 
answer, as we shall see, is "yes," provided the sampling theorem holds for 
the vector x.
The final DFT property, Property 8 in Table 3.3, is called linear phase, shift. 
The simplest way to illustrate this property is to assume there is a signal 
component given by
x = [x„; 0 < n < N]
within a long vector of length L » N, and also a time-shifted version of x 
given by
y = [y„; 0<n<N] = [x„_t; k<n<N + k],

Correlation, Fourier Spectra, and the Sampling Theorem 
57
such that x and y are entirely within the long vector. We now examine the 
contribution of y to the DFT of the long vector using (3.7):
L-l 
L-l
Y„, = 
= £x,e’'2,I”'; V'2™1'1- = 
0<m<L (3.28)
»=0 
1=0
The second sum is found by substituting i - n - k and keeping the same 
limits, because the indices of x and y are, by definition, within the limits 
0 < i < L. Thus, we have shown that delaying a signal component k samples 
causes no change in the component's amplitude spectrum and subtracts a 
linear phase component given by 2nmk/L, where L is the size of the transform, 
from the phase spectrum.
Linear phase shift is illustrated in Figure 3.11 using transform size L = 50 
and delay k = 4 samples..The signal component, x, is on the upper left, and 
its shifted version, y, is on the lower left. The respective phase spectra are 
on the right. In this example, the linear phase component, plotted at the 
lower right as a dashed line, is ~2nkm/L - -0.50m rad.
FIGURE 3.11
Illustration of linear phase shift. Signal samples [x„] are delayed to produce [y„] with y„ = . k; 
0 < n <N. The delay does not alter the amplitude spectrum but subtracts 2mnk/N rad from the 
phase spectrum of x. The dashed line shows this linear phase component. N = 50, and k = 4 in 
this illustration.

58
Digital Signal Processing with Examples in Matlab
3.9 Continuous Transforms
The focus of DSP is on vectors and arrays with elements taken from discrete 
sample spaces. But to keep in touch with reality, engineers must have ways 
to relate these vectors and arrays of discrete measured samples to their origin 
in the continuous real world, that is, to their corresponding continuous wave­
forms and images. This leads first to a discussion of the Fourier Transform 
(FT), which we introduce as a continuous, limiting version of the DFT. The 
Fourier transform of a continuous waveform, x(i), is
Fourier Transform: FT{x} = X(/o)) = f x(t)e~‘latdt
* —oo
(3.29)
where a> is continuous frequency in rad/s, and t is time in s. [The use of X 
here for the FT raises the possibility of confusion with its use to represent 
the DFT vector, but this is conventional, so we must remember that the 
argument (/w) implies the FT rather than the DFT. Usually, the implication 
comes from the context in which X is used anyway.]
To relate the FT to the DFT, we define another continuous function of time, 
x (t), as a semi-infinite sequence of impulses equal to x(t) at t = 0, T, 2T,... 
given by
x(t) = £x(f)<5(f-nT); 0<t<« 
(3.30)
H=0
where 8(t - nT) is the unit impulse at t ~ riT. Then the FT of x (t) is
FT{x} = J"^x(t)S(t-nT)e’/®,dt = £x(HT)e’y‘",'T 
(3.31)
° >i=0 
n=0
The sum on the right results from exchanging the summation and integration 
operations and noting that in general J2,/(t)3(t- r)df is equal to/(r) for 
any value of t. Now, if we assume x(nT) - x„ is a discrete sample of x(t) and 
is nonzero only for 0 < n < N, (3.31) is equivalent to
N-l
FT{i}=£xlie-'wT 
(3.32)
n=0
We recognize this as the DFT with continuous frequency, ®, that is, Xm in 
(3.25). Thus, we have the following interesting relationship between the FT 
and the DFT:
The Fourier transform (FT) of a sequence of impulse functions, 
each of which is an element of the sample vector x, is equal to 
the DFT of x at each frequency at which the DFT is measured.
(3.33)

Correlation, Fourier Spectra, and the Sampling Theorem
In addition, the FT has many other applications. The FT of a continuous 
signal provides the amplitude and phase spectra of the signal, just as the 
DFT provides the amplitude and phase in (3.19). That is, the amplitude 
spectrum is the FT magnitude, and the phase spectrum is the angle of the FT.
There is also an inverse FT, just as there is an inverse DFT. The inverse FT is
Inverse FT: x(f) = — J X(jco)e'“f rfco
(3.34)
The inverse FT is verified when we substitute (3.29) for X(/w) and switch 
the order of integration. Then, in the following expression, the term in 
square brackets is seen to equal one when t = t and zero when t t, that 
is, 8(f - r):
x(t) = j 
j’ e'"(/ 
= j x(x)8(f — T) dr = x(f) (3.35)
In some applications used later in this text, the use of the FT is limited, 
because even though the integral in (3.29) is infinite, the energy in the 
waveform must, in general, be finite in order for the integral to converge; 
that is, for convergence of the FT,
j |x2(t)|dt <oo
(3.36)
Thus, for example, the FT of any periodic function does not exist according 
to the definition in (3.29), although functions of co that "work" as transforms 
of periodic functions in some sense are sometimes included in transform 
tables.
The Laplace transform is a modification of the FT that converges with any 
finite-valued waveform. The Laplace transform is the FT with frequency (co) 
replaced by a complex variable, s = a + /a). In this case, we can use X(s) 
without ambiguity to represent the Laplace transform of x(f), and thus define 
the Laplace transform as
Laplace transform: X(s) = f x(t)e st dt
(3.37)
Like the Fourier transform, the Laplace transform has an inverse similar to 
(3.34), but the integration is on the complex plane, being over s instead of 
jco. Extensive tables of the Laplace transform and its inverse are available in 
the literature.14'15
The continuous transforms are useful mainly with continuous signals, and 
so they have limited use in a text like this when the main subject is DSP. 
However, the Fourier and Laplace transforms are useful in the discussion of 
the sampling theorem in the next section and also in the design of optimal

60 
Digital Signal Processing with Examples in Matlab
HR filters, which we discuss later. The main purpose of this section has been 
to introduce the two transforms and to show how they are related to each 
other and to the DFT. The principal relationships are in (3.29), (3.33), (3.34), 
and (3.37).
3.10 The Sampling Theorem
Having discussed discrete and continuous transforms, which are used to 
map time-varying waveform and image data into the frequency domain, we 
are now in a position to discuss and apply the well-known sampling theorem. 
The sampling theorem can be stated in different ways, but its main use in 
DSP is to tell us the rate at which we must sample a continuous waveform 
in order to convey all the information in the waveform, that is, in order to 
be able to reconstruct the waveform from its samples.
Such a reconstruction is, of course, impossible, in general, because any 
set of samples "belongs" to an infinite set of continuous waveforms, as 
illustrated in Figure 3.12, which shows three different waveforms with the 
same sample values at t = 0,2,4,.... In other words, given a finite set of 
sample points, it is always possible to construct an infinite number of 
continuous functions through these points. If this is the case, then how can 
any continuous waveform ever be reconstructed uniquely from a set of 
sample points?
The answer lies in allowing only certain frequency components to exist 
in the continuous waveform. The sampling theorem may be stated simply
10r
T=2
0'---------------------- 1----------------------- ।----------------------- 1----------------------- 1------------------------ 1----------------------- 1----------------------- 1
0 
1 
2 
3 
4 
5 
6 
7
t
FIGURE 3.12
Different waveforms with the same samples at f = 0, 2, 4, 6, ....

Correlation, Fourier Spectra, and the Sampling Theorem
61
as follows:
If a continuous signal is sampled at a rate greater 
than twice its highest frequency component, then it 
is possible to recover the signal from its samples.
(3.38)
Notice that if the signal can be recovered from the samples, then all of 
information in the signal can be recovered from the samples. For example, 
if the signal is a speech signal conveying news about events, then all of the 
news is also conveyed in the samples, provided the sampling theorem is 
satisfied. Also, the samples in (3.38) are assumed to be accurate. In some 
cases, they are noticeably inaccurate, and quantizing errors must be taken into 
account. Quantizing errors are treated as statistical or random errors and are 
discussed in Chapter 10.
The sampling theorem is easy to prove, and in fact, might be anticipated 
from previous results in Sections 3.3 and 3.4. Suppose we have accurate 
samples of x(f) taken at f = 0, 
- 1)7, and we wish to recover x(t) in
the range t = [0,N7). Let x (t) represent the corresponding set of impulse 
samples as in (3.30). Then, according to (3.33), the Fourier transform of x(f) 
and the DFT of the vector x = [x0 x3 ... xN_;] are the same; that is, DFT(x} = 
FT(x | - X (jco). Now suppose we express the impulse sequence in (3.30) as 
x (f) = x(t)d(t), where
d(t) = ^5(t-nT) 
(3.39)
The Fourier series coefficients for this function, which is obviously periodic with 
period T and fundamental frequency cq = 2.T/T, may be computed using (2.35):
c„, = | 
'£j8(t-nT)e-'2:mt'Tdt = | Xf' 3(t - nT)e-’2m,lTdt = 1 (3.40)
1 J-T12 
1 
1
In this result, the integrand on the right equals one at f = 0 and zero else­
where. Using (2.34), the Fourier series for x(f) = x(t)d(t) is
x(t) = - X -^(0e'2TO'"T 
(3-41)
Hi =
The final step is to take the Fourier transform of this version of x(t), as 
follows:
X(jm) = |f X x(t)ei2xm,lTe-i,0,dt 
m=-°°
4 X f' 
= | X X(;((d-2^/T)) (3.42)

62 
Digital Signal Processing with Examples in Matlas
Thus, X(y'a>), which is the Fourier transform of the sequence of impulse 
samples of x(t), is 1/T times an infinite superposition of Fourier transforms 
of x(f). But, if X(ja>), the Fourier transform of x(t), is zero at and beyond half 
the sampling rate, that is, for |<w| > zr/T, then the terms in the sum in (3.42) 
do not overlap, and X(/tD) = TX (jaS) in this frequency range.
Thus, the sampling theorem is proved, because if we can compute X(jco) 
from the DFT [that is, by transforming the sample sequence to produce 
X(y<w)], then we can recover x(f) as the inverse transform of X(jO)). Two useful 
recovery methods that effectively accomplish these operations when the 
sampling theorem is satisfied are described in the next section.
3.11 Waveform Reconstruction and Aliasing
Our first waveform reconstruction method is based on the inverse DFT in 
(3.23) and on the equivalence of the DFT components with the Fourier series 
coefficients in (3.8). These suggest that writing the inverse DFT as a contin­
uous function of t provides a continuous Fourier series for a function that 
passes through all the sample points and has no frequencies beyond half the 
sampling rate. Because (via the sampling theorem) the only function having 
these properties is x(t), we conclude that the reconstruction, which we may 
call the Fourier series reconstruction, is exact when the sampling theorem 
holds. Furthermore, even when the sampling theorem does not hold, the 
Fourier series reconstruction is exact at all the sample points and is guaran­
teed to contain no frequencies at or above half the sampling rate.
Using the redundancy in (3.11), we write the inverse DFT (3.23) as follows, 
so that the frequency index stays at or below half the sampling rate index, 
that is, < N/2:
N-l
. V y ^fZxmnlN
n ~~ N Z-i 
m=0
. / wi<N/2 
x 
_ 
\
1 v . n v rv i (2nmn\ T v . (2nmn\
= - X0 + 2 2, Re{X„,}cosl —— I - Im{X,„}sinl—I (3.43) 
X n«=l 
. J
The second line is obtained from the first by separating the first sum into 
two halves and applying (3.11) with (3.10). If N is even, XN,2must be zero 
for the sampling theorem to hold. Next, we substitute <x>0 = 2rr/NT as in (2.24), 
and we let t = nT to obtain from (3.43) the Fourier series reconstruction at 
any point in time:
X0 + 2 7 Re{X,„}cos mo)0t - Im{Xnl]sin 
(3.44)
m=l 
J

Correlation, Fourier Spectra, and the SamplingTheorem
63
Sampling rate=2 samples/s
FIGURE 3.13
Two examples of Fourier series reconstruction using (3.44). In the center plot, the sampling 
theorem is satisfied, and reconstruction is exact. In the lower plot, the sampling rate is too low, 
and the reconstruction is not exact. 
'
The following Matlab function implements (3.44):
y = reconst(x, t) 
(3.45)
In this function, t is any vector of time points, and y is the reconstructed 
waveform, x(t), computed as in (3.44).
In Figure 3.13, there are two examples of the use of (3.44). The original 
waveform, x(i), is shown in the upper plot. The spectrum of x(f) is zero at 
and above 1.0 Hz. In the center plot, x(t) is sampled at 2 samples/s, and 
x(f) is reconstructed using (3.44). Because the sampling theorem is satisfied 
in this case, the reconstruction is exact. In the lower plot, x(t) is sampled at 
1.5 samples/s, and x(t) is again reconstructed using (3.44). Because the 
sampling theorem is not satisfied in this case, the reconstruction (solid 
curve) differs from x(i) (dashed curve), both of which are shown. However, 
the reconstruction is seen to pass through the sample points and to inter­
polate smoothly between the sample points. One would expect the latter, 
because, in this case, the reconstruction (3.44) contains only frequencies 
below 0.75 Hz.
The second reconstruction method was developed previously in Section 
3.8. It is based on the resampling method on line 7 of Table 3.3, in which zeros 
are inserted into the DFT, resulting in interpolation between samples in the 
time domain. Because this method produces a more densely packed sample

Digital Signal Processing with Examples in Matlab
t (s)
FIGURE 3.14
Samples and reconstruction of a continuous signal, x(t), using the "resampling" method of 
interpolation between samples. The reconstructed signal has no spectra! content above half the 
sampling rate (166 Hz in this example).
sequence that includes the original sample points and has an unchanged 
spectrum, we can now use the sampling theorem to say that, if the sampling 
theorem is satisfied and if an increasingly large number of zeros is inserted 
into the DFT in the manner described [see (3.27), for example], the resampled 
function will, in the limit, become the original continuous function.
The resampling method is especially easy to implement using Matlab 
expressions. The resamp function in the software that comes with this text is 
one implementation. The expressions in resamp are essentially similar to the 
expressions in (3.27), except even as well as odd sample vector lengths are 
allowed. An example of the use of resamp is provided in Figure 3.14. The 
implementing Matlab expression is
y = resamp (x, Ly); 
(3.46)
in which x is the sample vector (with length Lx = 18 in Figure 3.14), y is the 
interpolated version of x, and Ly is the specified length of y, which was set at 
500 for this example. With 500 points, the reconstruction appears continuous.
Both reconstruction methods—Fourier series and resampling—produce 
band-limited approximations of the original signal from which the samples 
were taken. That is, the spectra of the reconstructions are limited to frequencies 
below half the sampling rate. These methods differ from other interpolation 
methods such as linear interpolation, splines, etc., which are not band limited.
A method used often in the design of DSP systems is shown in Figure 3.15. 
To prevent errors in the reconstruction (as well as other kinds of processing 
such as filtering), a signal is "prefiltered" to remove all frequency compo­
nents over half the sampling rate. Then, as suggested in the illustration, the 
prefiltered signal can be sampled, stored, and later reconstructed in its orig­
inal form.

Correlation, Fourier Spectra, and the Sampling Theorem
FIGURE 3J6
Illustration of aliasing in accordance with (3.40); sampling rate = 500 Hz. Upper: Spectrum of 
original signal. Center: Components of sum in (3.40) and sum (dashed curve). Lower: Spectrum 
of reconstructed signal.
Sampling 
and 
Digital' 
Storage
FIGURE 3.15 
Reconstruction of a 
half the sampling rate.
The concept in Figure 3.15, although often used, cannot be done ideally 
as shown. Ideal filters do not exist, as we will see later in this text. So the 
question arises: How is a signal distorted, or aliased, when it is reconstructed 
after having been sampled at a rate that is somewhat less than twice its 
highest spectral content? The answer lies essentially in (3.33) and (3.42). Each 
of the reconstruction methods produces a signal with a spectrum that is the 
continuous DFT of the samples. But, from (3.33) and (3.42), the spectrum of 
the samples is identical with 1/T times the superposition of shifted Fourier 
transforms of the original signal in the range |to| < n/T, and is zero outside 
this range.
This phenomenon, called aliasing, is illustrated in Figure 3.16. The upper 
plot shows the spectrum of the original signal, x(f). The center plot shows 
the two essential components in the sum in (3.42) (with m = 0 and m = 1) 

66 
Digital Signal Processing with Examples in Matlab
and the sum, which is X(ja>) in (3.42), assuming that x(t) is sampled at 500 
samples/s. The lower plot shows the reconstructed spectrum, which is dis­
torted above about 150 Hz and is zero above 250 Hz, or half the sampling 
rate.
3.12 Exercises
General Instructions: (1) Whenever you make a plot in any of the exercises, 
be sure to label both axes so we can tell exactly what the units mean. (2) 
Make continuous plots unless otherwise specified. "Discrete plot" means to 
plot discrete symbols, unconnected unless otherwise specified. (3) In plots, 
and especially in subplots, use the axis function for best use of the plotting 
area.
1. Write a program similar to (3.13), and use the result to produce a 
figure similar to Figure 3.3, illustrating the signal and the magni­
tude of its DFT. In place of the signal in (3.13), use
x„ = 2sinl-TQ-l; n - 0,1,...,49
2. Do Exercise 1, this time using the following signal with N = 500:
- . (2nn\ -niioo „ d
X„ = 2sm -=— e ; „ = 0,1,..., 499 
\ 50 )
3. Create a 2 x 2 array of plots using subplot{2,2,*). On the upper left, 
plot the signal vector x, where
x„ = sin^~); n = 0,l,...,99
versus time, assuming time step T = 0.05 s. On the lower left, make 
a discrete plot of the DFT magnitude, |X,„|, versus m for m = 0, 1, 
..., 50. On the upper right, make a discrete plot of the DFT mag­
nitude versus frequency in the range [0,0.5] Hz-s. On the lower 
right, make a discrete plot of the DFT magnitude versus frequency 
in Hz from zero to half the sampling rate.
4. In this exercise, we examine the phenomenon known as leakage. The 
term refers to the spectrum as represented by the DFT, in which the 
"true" spectrum of a periodic signal or component is spread out as 
a consequence of transforming a signal vector of finite length and not 
containing an integral number of cycles of the periodic component.

Correlation, Fourier Spectra, and the Sampling Theorem 
C7
Create a 2 x 2 array of subplots. On the upper left, plot the signal 
vector with N = 92:
Xn = sin(^F) n = 0/
Below this, make a discrete plot of the amplitude spectrum, X,„|, 
versus frequency in the range [0,0.5] Hz-s, and note the leakage 
around the signal frequency, 0.05 Hz-s. Repeat these two plots on 
the right with the signal vector length increased to N = 292, and 
note the effect on leakage of the increased vector length.
5. Make two subplots, side by side. On the left, plot the following 
sample vector;
On the right, make a connected discrete plot of the amplitude 
spectrum in terms of the frequency in rad. Use the correct symbol 
in Table 3.1 to label the frequency axis.
6. Do Exercise 5, assuming an interval of 0.1 ps between samples, and 
label the time and frequency axes appropriately. Use frequency 
units/ (MHZ).
7. Complete the following:
a. Do Exercise 5, assuming an interval of 0.1 ps between samples, 
but instead of the amplitude spectrum, plot the unwrapped 
phase spectrum in terms of degrees versus frequency in MHZ.
b. Prove that for any real vector x, the DFT at a>T = 0 and coT = n 
must be real, and therefore, the phase must be either zero or 
±180°.
8. Beginning at t = 0, 2048 samples of /(f) = tU!a are collected with 
time step T = 0.015 s. In the upper of two subplots, make a connected 
discrete plot of the amplitude spectrum of/(f) over the range [0,0.6] 
Hz. In the lower subplot, make a connected discrete plot over the 
same frequency range of the interpolated amplitude spectrum 
using Property 5 in Table 3.3, such that the density of points is four 
times the density in the upper plot.
9. A function given by
x(t) — e’(/4sin(2^t/5); t>0
is sampled once per second from t - 0 through t = 16 s. In the 
upper of two subplots, make a discrete plot of x, the sample vector. 
In the lower subplot, make a discrete plot of the resampled version

68 
Digital Signal Processing with Examples in Matlab
of x, with three samples between each pair of original samples. 
Use the resampling method (Table 3.3, Property 7) illustrated in 
(3.27). 
' 
'
10. Starting with the definition of the Fourier transform in (3.29), derive 
the Fourier transform of the continuous signal, x(t), in Exercise 9. 
(You may wish to use the example of integration by parts in 
Chapter 1.) Plot the amplitude spectrum, |X(/cd)| versus to in the 
range [0,10] rad/s. Is the sampling interval in Exercise 9 adequate 
to prevent aliasing? Why or why not? How would you select an 
"adequate" sampling rate for x(t)?
11. The spectrum of x(f) is given by X(/a>) = 
= lOe1^2, where/
is in Hz. Note that the spectrum is real in this example. In each of 
two subplots, plot the continuous DFT of x(t), that is, X (/cd) in 
(3.42), over the frequency range [0,40] Hz. Use sampling intervals 
T = 0.04 s and 0.07 s in the upper and lower subplots, respectively. 
Use as many terms in the sum in (3.42) as necessary to complete 
the plot.
12. A continuous waveform is described for t > 0 s as follows:
x(t) = 100te-150|!‘O(K|sin(200rtf)
Create a sample vector, x, of x(t) in the range t = [0,0.07] s using 
time step T = 0.1 ms. In the upper of two subplots, plot x(f) as a 
continuous curve through the points in x. In the lower subplot, 
make a connected discrete plot of the amplitude spectrum in terms 
of the complete DFT of x versus frequency in the range [0,10] KHz. 
Based on the lower plot, estimate the lowest sampling frequency 
for x(f) that would not violate the sampling theorem.
13. Compare the DFTs of two sample vectors taken from x(f) in Exercise 
12, both over the same range t - [0,0.07] s, but this time, with T - 3.0 
and? = 2.0 ms. Plot the DFT amplitudes in two subplots, one above 
the other, both versus frequency in KHz over the range from zero 
to the sampling frequency. Observe the values of the DFT ampli­
tudes around half the sampling rate and comment on whether 
either or both of the two time steps are small enough to satisfy the 
conditions of the sampling theorem.
14. This is a reconstruction exercise using the resampling method 
(Table 3.3, Property 7). In the center of three subplots, plot the sample 
vector of the waveform in Exercise 12 with T = 0.1 ms. For the upper 
subplot, create a sample vector over the same 70-ms time interval 
using T = 3.0 ms, and plot the reconstructed continuous x(t) using 
Property 7. Finally, do the same in the lower plot with T = 2.0 ms. 
Make all plots with time range [0,70] ms and amplitude range [­
2,2], Comment on the adequacy of the sampling frequencies for 
subplots 1 and 3 by comparing both with subplot 2.

Correlation, Fourier Spectra, and the Sampling Theorem 
69
15. Do Exercise 14 using the Fourier series reconstruction instead of 
the resampling method. Comment on which reconstruction 
method is preferable for this particular signal, and why the method 
is preferable.
References
1. Lyons, R.G., Understanding Digital Signal Processing, Addison-Wesley, Reading, 
MA, 1997, chaps. 3 and 4.
2. Cartinhour, J., Digital Signal Processing: An Overview of Basic Principles, Prentice 
Hall, Upper Saddle River, NJ, 2000, chap. 8.
3. Oppenheim, A.V. and Schafer, R.W., Discrete-Time Signal Processing, Prentice Hall, 
Englewood Cliffs, NJ, 1989, chap. 8.
4. Orfanidis, S.J., Introduction to Digital Signal Processing, Prentice Hall, Upper Saddle 
River, NJ, 1996, chap. 9.
5. Proakis, J.G., Rader, C.M., Ling, E, Nikias, C.L., Moonen, M., and Proudler, I.K., 
Algorithms for Statistical Signal Processing, Prentice Hall, Upper Saddle River, NJ, 
2002, chap. 2.
6. Brigham, E.O., The Fast Fourier Transform, Prentice Hall, Englewood Cliffs, NJ, 
1974, chap. 10.
7. Ahmed, N. and Rao, K.R., Orthogonal Transforms for Digital Signal Processing, 
Springer-Verlag, New York, 1975.
8. Elliott, D.F. and Rao, K.R., Fast Transforms, Academic Press, New York, 1983.
9. Bergland, G.D., A guided tour of the fast Fourier transform, IEEE Spectrum, (>, 
41, July 1969.
10. IEEE Transactions on Audio and Electroacoustics (Special Issues on the Fast Fourier 
Transform), AU-15, June 1967 and AU-17, June 1969.
11. Cooley, J.W. and Tukey, J.W., An algorithm for the machine calculation of com­
plex Fourier series, Math. Comput., 19, 297, Apr. 1965.
12. Good, I.J., The interaction algorithm and practical Fourier series, J. Roy Statist. 
Soc., sen B„ 20, 361, 1958 and 22, 372, 1960.
13. Whittaker, E.T., Expansions of the interpolation-theory, Proc. Roy. Soc. Edinburgh, 
35, 181,1915. 
'
14. Nixon, F.E., Handbook of Laplace Transformation, Prentice Hall, Englewood Cliffs, 
NJ, 1965. 
'
15. Holbrook, J.G., Laplace Transforms for Electronic Engineers, Pergamon Press, Oxford, 
1966.


Linear Systems and Transfer Functions
4.1 
Continuous and Discrete Linear Systems
Digital signal processing technology has its roots in the analysis of continu­
ous linear systems. We saw in the previous chapter how continuous signals 
may be regarded as limiting forms of corresponding discrete sample vectors, 
and the same idea holds for continuous and discrete processing systems. Con­
tinuous linear systems1011 are described with linear differential equations, and 
discrete linear systems are described with discrete linear equations that are 
similar to linear difference equations.
There are many reasons to keep these "continuous roots" in our thoughts as 
we proceed to develop the subject of DSP. In this chapter particularly, the time 
and frequency domains are similar for digital and continuous systems. Convo­
lution in one domain (time, for example) is equivalent to multiplication in the 
other domain (frequency) in the case of either discrete or continuous signals. 
Most of the other notions in continuous linear systems, such as resonance, 
stability, impulse response, step response, transfer function, filtering, etc., are 
carried over and applied in the analysis of discrete systems.
Our discussion of linear systems begins with a description of the discrete 
linear equation and goes on to develop the linear transfer function as well as 
some of the rest of the concepts just mentioned. The development will be 
complete and (we hope) understandable in the terms given; nevertheless, it will 
be helpful, as it has been helpful in previous chapters, always to think of the 
sample vectors being derived from continuous signals.
4.2 
Properties of Discrete Linear Systems
A linear equation describing the behavior of a discrete linear system is an equa­
tion that describes a straight line in a Cartesian coordinate system when the 
dependent variable in the equation is plotted versus the independent variable(s). 
Take for example the following equation:
y,. = bxk + ayk^ 
(4.1)
71

72 
Digital Signal Processing with Examples in Matlab
In this example, discrete values of the dependent variable, y, are related to 
discrete values of the independent variable, x. In the terminology of signal 
processing, when (4.1) describes a linear system, x is called the input signal, 
and y is called the output signal.
The two constants in (4.1), b and a, are called coefficients, or weights. We 
can tell the equation is linear, because the values of x and y appear only as 
first powers. But in what sense does (4.1) represent a straight line? To answer, 
we must "solve" the equation so that y is given explicitly in terms of x.
Before we solve (4.1), we should note that the subscript k denotes the 
position in an ordered sequence (vector) as in previous chapters, and this, 
in turn, implies an order of processing in equations like (4.1). Beginning at 
k = 0 (for convenience), yQ is computed (usually with y_x assumed to be 
zero) as y0 = bx0, then, yx - bxt + ay0, y2 = bx2 + ay2, and so on. We also 
associate continuous time with the order of processing, that is, t = kT as 
in previous chapters, T being the time step between any two adjacent 
samples.
Several other properties of linear equations are related to the association 
of k with the time domain. The first is realizability. The linear operation in 
(4.1) is realizable, because the current value, yk, of the output signal is com­
puted in terms of its own past values (yt_p in this case), and not in terms of 
any future values of y, such as yM, yk+2, etc.
The second property is causality. The linear operation in (4.1) is causal, 
because yk is computed in terms of present and past input signal values (xk, 
in this case), and not in terms of any future values of x. The term comes 
from the idea that in nature, results in the present are not affected by events 
that have not yet happened. For example, if xk in (4.1) were changed to xt+1, 
then (4.1) would be a noncausal operation. Examples of causality and realiz­
ability are given at the end of this section.
The linear equation (4.1) is not hard to solve by induction, that is, by 
substituting repeatedly for the sample of y on the right. The result (assuming 
xk = 0 for k < 0) becomes
y* = bxk + a(_bxk_x+ayk.2)
= bxk+ a(bxt.2 + a(bxk.2 + a(bxk_3 + •••))•••) 
k
= b^a"xk_„ 
(4.2)
n~0
Now, we see how (4.1) can be viewed as the equation of a straight line. The 
Cartesian signal space has k + 2 dimensions, and yk is a function of x0, x.,...,xk 
in this space.
A linear equation such as (4.1), in which the output, y, is given in terms 
of its own past values as well as values of the input, is called recursive. If 
past values of y do not appear on the right, then the linear equation is 

Linear Systems and Transfer Functions 
73
nonrecursive, that is, recursion is not needed to express yk in terms of 
samples of x. Note that (4.1), which is recursive, becomes nonrecursive 
when expressed in the form of (4.2). However, (4.2) indicates that yk, in 
response to a single nonzero input sample ,r0, will continue forever, because 
«kx0 is always the final term in the sum. Thus, a recursive linear system is 
also known generally as an infinite impulse response (UR) system. Con­
versely, a nonrecursive system is sometimes called a finite impulse response 
(FIR) system.
Another property related to the time-domain behavior of (4.1) is stability. 
A linear system is stable if the magnitude of its response to a finite input 
does not grow without bound. In the case of (4.1), we can see from its 
nonrecursive form in (4.2), that (4.1) describes a stable system when |a| < 1 
and an unstable system when |a| > 1, because, with |rz| > 1, the final term in 
the sum in (4.2), which is akx0, increases in magnitude with k for any nonzero 
value of x0. Stability is an important property of linear systems and is dis­
cussed later in terms of poles of the transfer function.
In most of our discussions, the weights in the linear equation, that is, b 
and a in (4.1), are constant real numbers. However, sometimes these 
weights are equal to, or related to, measured properties of a system, that 
is, parameters of the system, and it is possible that these parameters may 
change or drift with time, so that the weights are not constant but are 
functions of k instead. In this case, the linear equation is describing a time­
varying or adaptive signal processing system. Adaptive signal processing, 
in which the weights change with time and adapt according to a specified 
purpose, has become an important part of DSP and is introduced in 
Chapter 9.
The foregoing properties of discrete linear equations are summarized 
in Table 4.1. For each property, there is a simple example where the 
property exists and a simple example where the property does not exist.
With these properties in mind, we are ready to discuss linear transfer 
functions, which, as we shall see, lead to better ways to solve recursive 
linear equations and, more importantly, provide useful insights into the 
behavior of linear systems.
TABLE 4.1
Properties of Discrete Linear Systems
Property
Property Exists
Does not Exist
Linear
Vt = bxk + nyw
'jk = bM +
Realizable
= bxk +
J/i = bxk + ayM
Causal
]/k = bxk +
th, = bxM +
Recursive (HR)
i/t = bxk +
i/i = bxt +
Stable
i/t = fcxt + 0.9yM
J/l = bXt + l.ll/j.;
Adaptive
= bkxk +
ft = bxk + ai/i-j

74 
Digital Signal Processing with Examples in Matlab
4.3 
Discrete Convolution
We begin this section with the general form of (4.1), known as the direct form, 
that describes any causal, realizable linear system with constant weights:
N-l 
M-l
i/i = V*-,. - £ fc = o,i,... 
(4.3)
»=0 
»<=l
Causality (n > 0) is not essential in this discussion, but it simplifies the index 
notation, and it also enables the system to operate in “real time," which is 
often required. Also, we can usually define the time index, k, so that the 
signals are zero for k < 0, and we will assume this unless we state otherwise. 
Thus, we have a startup sequence that begins as follows:
Vo = Vo,
J/i = boXj + b^o-ait/o, 
(4.4)
y2 = b0x2 + b}x1 + b2x0-(a1y1 + a2y0),... etc.
The starhip sequence continues untilA: = M - 1 orA = N — 1, whichever is later, 
that is, until the final terms in the sums in (4.3) involve samples of x and y at 
and above k = 0. From this point, the computation in (4.3) involves a constant 
number of (N + M - 1) products.
Each of the sums in (4.3) is a convolution of two vectors, a weight vector 
and a signal vector. Convolution is a fundamental operation in continuous 
as well as digital signal processing. An illustration of the left-hand convo­
lution in (4.3), with N = 4, is given in Figure 4.1. There is nothing really 
new in the illustration, but it is often helpful to picture the convolution as
FIGURE 4.1
Discrete convolution of signal vector x with weight vector b, showing how the vectors are 
aligned at each time step to produce the sum of products on the right.
bo
xt x3 x2 xt x0
i’ll bt
b3 b, b3
b2 b3
J/o = bax0
lh = Mi + M0
X4 X3 X2 X! Xa
t’o by b2
Xi x3 x2 xt x0
b3
y? = Mi+bi-xi + Mo
b0 b, b
Xl-H x* xt_, xt
b3
-2
y* = b„xk + bjXt-j + b,xk_2 + M*-v k > 3

Linear Systems and Transfer Functions 
75
shown, with the signal vector, x, reversed and aligned with the weight 
vector, b, in accordance with the time index, k, so that each product in the 
sum is evident.
4.4 
The z-Transform and Linear Transfer Functions
Just as we used the discrete Fourier transform (DFT) to transform signals 
from the time domain to the frequency domain, we may also use the DFT 
to transform the convolutions in (4.3) from the time domain to the frequency 
domain. To do this, we define the z-transform, which is really just the DFT 
expressed with modified notation using the substitution
jtf)T 
z = e
(4.5)
in which <j)T is frequency in radians as described in Section 3.6. The z-transform 
of a vector x, which we denote X(z), is the continuous DFT (3.25) with the 
substitution (4.5), that is,
N-l
z-transform: X(z) = Y x„z
»=0
(4.6)
The limits of the sum in this definition are not fixed; they are set, in each 
case, to include all the samples in the vector x, or whatever set of samples 
we may wish to include, in the transform. Either limit may be infinite if so 
desired. For example, suppose xk - e~ak for k - [0:“>). Then
■ 
jaiT
z-transform: X(z) = 
DFT: X(e'aT) = —|—- (4.7)
The closed expression for X(z) is found using the geometric series formula 
(1.20), which, together with (1.19), are the key formulas used for expressing 
z-transforms of functions of this sort in closed form. In modern DSP, we deal 
mostly with sample vectors that are not usually expressible as simple discrete 
functions such as xk in (4.7). Thus, in this text, we do not (except in a few 
special cases) discuss the theory of forward and inverse z-transforms of 
functions. References1'3 are recommended for this purpose.
We now consider an application of z-transforms, namely, the z-transform 
of a convolution. The transform of a convolution such as those in (4.3) is 

76
Digital Signal Processing with Examples in Matlab
found as follows:
N-l
Let uk = 
0<k<°°
J»=O
oo N-l 
N-l 
«
Then U(z) = £ £ b„xk_Hz'k = £ b„z~n ^xk_„z~<k^ 
(4.8)
fc—0 h=0 
«=0 
ic=0
N-l 
«
= ^b„z"'^x;z“' = B(z)X(z) 
,1=0 
1=0
In the final sum, we nqte.that i = k - n, and i can begin at zero, because k begins 
at zero, and xk is zero for k < 0. Thus, the z-transform of the convolution shown 
here is the product of the z-transforms of the two vectors being convolved. This 
is true in all cases in which one of the vectors has infinite extent, as x has in 
this case. Cases in which both vectors are finite will be discussed later.
With the result in (4.8), we are now able to transform the direct-form 
equation in (4.3). Let a0 = 1. Then (4.3) and its transform become
M-l 
N-l
y, = y bnxk.„; 0<k<°°
>11=0 
n=0
A(z)Y(z) = B(z)X(z) 
(4.9)
Y(z) = B(z) = bo + b1z-1+- + bN.1z-<w-l) =
X(z) A(z) i+£!12-' + ...+CIm_12-<m-1>
When the transforms are written in this form, the quotient H(z) = B(z)M(z) 
is called a transfer function, because it contains all the information on how 
to relate the input signal to the output signal, that is, how to "transfer" the 
signal through the system. Furthermore, the transfer function supplies this 
information in the frequency domain because of the association of z with 
frequency in (4.5). In other words, equivalent descriptions of a discrete linear 
system in the time and frequency domains are as follows:
N-l 
M-l
Time domain: yk = ^b„xk_„-^a,„yk_m 
(4.10)
n=0 
m=l
t c ( urx Y(z) bo + feiZ’1 + - - + bN_1z"lN'1)
Transfer function: H(z = —f =---------------------------- (4.11)
A(z) l+a12 '+ ... +aM_jZ tM 11
A । li 
। 
> li
Frequency response: H(e'mT) = 
(M_1)fl>T 
(4.12)
1 +a}e ’ + +aM_ke 7 
’
Now, H(eJ<“r) is, as we have said, the ratio of the DFTs of the vectors a and b. 
Thus, just as the amplitude and phase spectra of a signal vector are given

Linear Systems and Transfer Functions
77
in (3.18) as the magnitude and angle of the DFT of the vector, so the amplitude 
response (or amplitude gain) and phase response (or phase shift) of a linear system 
are given by the magnitude and angle of H(c;"r) in (4.12); that is,
Amplitude gain = abs(H(e;wT)) -
8 
|A(e"“r)| 
(4.13)
Phase shift = angle(H(r7"'T)) = ZB(e'uT)-ZA(el<aT)
An example expressed in Matlab notation of the computation of (4.13) is 
given next in (4.14). The weight vectors, a and b, with M - N = 5 in (4.3), are 
copied into the first two expressions. These were generated by a function to 
be discussed in Chapter 6. So, for the present, we take b and a as given:
b=[0.2021 0.3328 0.4517 0.3328 0.2021];
a=[l.0000 -0.2251 0.6568 0.0349 0.0548];
H=fft(b,1000)./fft(a,1000);
subplot (1,2,1);
plot(1inspace (0 , .5,501), abs(H(1:501) ));
subplot (1,2,2);
plot(1inspace(0,.5,501),unwrap(angle(H(1:501))));
(4.14)
Next, 
is computed as the ratio of DFTs in (4.12). Both DFTs are
extended with zeros to length 1000 to produce a more densely-packed 
response vector, and thus allow continuous amplitude gain and phase 
plots. (See "Zero Extension" Chapter 3, Table 3.3.) Amplitude and phase 
response vectors are then computed as in (4.13). [The operation of unwrap 
was explained in connection with (3.21) in Chapter 3.] The response curves 
are plotted in the two subplots in Figure 4.2. Because, as explained in 
Chapter 3, the second half of a DFT is a mirror image if the first half, it is 
conventional to plot responses only up to half the sampling rate, that is, 
only for to = [0, rr/T] rad/s, or v= [0,0.5] Hz-s. Expressions for labels, and 
so forth, in the figure are not shown in (4.14) for the sake of simplicity. The 
complete m-file is included in the software that comes with this text.
As a matter of interest, the amplitude gain curve in Figure 4.2 is called 
a "lowpass" characteristic, because it describes a system that passes low- 
frequency components and blocks high-frequency components of a signal.
4.5 Poles and Zeros
There is a geometrical interpretation of the transfer function, H(z), which is a 
useful aid in understanding the steady-state frequency response and also the 
transient response characteristics of a linear system. The interpretation 
involves the z-plane, with Cartesian coordinates that are the real and

78 
Digital Signal Processing with Examples in Matlab
2
v (Hz-s) 
v (Hz-s)
FIGURE 4.2
Amplitude gain and phase response of the linear system with weights given in (4.14). The 
system is called a "lowpass filter," because it passes the low-frequency components of a signal 
and blocks the high-frequency components.
imaginary parts of z, and the poles and zeros of H(z). The latter are the roots, 
respectively, of the denominator and numerator of H(z) in (4.11).
We can best explain this transfer-function geometry with a simple example. 
Suppose the linear transfer function, corresponding with yk = xk + 
+ 0.9yt_lz
is as follows:
H(z) = 
= JL±X 
(4.15)
1 - 0.9z 1 z — 0.9
The pole at z = 0.9 and the zero at z = -1 are plotted on the z-plane in the 
left half of Figure 4.3. The numerator and denominator of the complex gain, 
Hie'"'1), are also shown as vectors on the z-plane. First, we see that the 
amplitude gain is a ratio of distances, a/P; that is,
? 
(4.16)
|ez T-0.9| 
0
The point on the unit circle labeled etaT designates the frequency at which the 
gain is being observed and is called the operating point. As the frequency, 
co rad/s, moves from zero to half the sampling rate, the operating point moves 
from 0 to 180° around the upper half of the unit circle. As it moves, we can 
see, in this case, that a decreases from 2.0 to 0, and p increases from 0.1 to 1.9. 
Thus, we are able to see, from the locations of the pole and zero, the form of

Linear Systems and Transfer functions 
79
20.
5
-1---------. 
-----------
-1 
-0.5 
0 
0.5 
1 
.
Re (z) 
0_______ ' 
-
0 
0.2 
0.4
Frequency v=<nT/2n (Hz-s
0
FIGURE 4.3
(Left) Pole-zero plot of H(z) = (z + l)/(z - 0.9), showing how the amplitude gain at co rad/s may 
be visualized as a quotient of lengths; that is, |H(e'“T)| = a/p. (Right) Plot of |H(e'“,T)| versus 
frequency in Hz-s.
Re(z)
-100------------ '-------------- ■------
0 
0.2 
0.4
Frequency v=a.T/2rt (Hz-s)
FIGURE 4.4
(Left) Pole-zero plot of H(z) = (z + l)/(z - 0.9), showing how the phase shift at <u rad/s may be 
visualized as a difference of angles; that is, Zfffe7'"7) = 6 - q>. (Right) Plot ot ZH(ef'r) versus 
frequency in Hz-s.
the amplitude gain plot, without actually calculating values in (4.16). The 
amplitude gain plot is shown in the right half of Figure 4.3.
A similar plot of the phase response of the linear system in (4.15) is shown 
on the left in Figure 4.4. As in (4.13), the phase response is the algebraic sum 
of angles of the poles and zeros of H(e,a: ). In Figure 4.4, the zero angle is 
labeled 6, and the pole angle is labeled <p, so the phase response is as follows:
ZH(e'<"7') = 6-tp 
(4.17)

80
Digital Signal Processing with Examples in Matlab
Notice that as co increases from 0 to it, [0,cp] increase from [0,0] to [ir/2,it]. 
Thus, the phase response in (4.17) decreases from 0 to -tt/2, as seen on the 
right in Figure 4.4.
More generally, to find the poles and zeros of a linear system in the form 
of (4.11), the first step is to write H(z) as a ratio of polynomials in z:
tr, x _ 
1 + ■■■ + bN iZ (W 11 _ M_w bozN 1 + b1z"' ~+ ■■■ + bN-i
Then, the poles and zeros are the roots of the denominator and numerator 
polynomials in (4.18). Notice that, if N * M, there is also a pole or zero of 
order |M - N] at z = 0. . .
In Matlab, the roots may be determined with the roots function. The syntax 
is r = roots(w), in which w is a vector of weights, and r is a vector of roots. 
The corresponding polynomial form for r - roots(w) is as follows:
p(z) = W]ZN-1 +w2zn~2+ ••• + wN - a'i(z-r1)(z-r2)---(z - rw_]) (4.19)
The roots function allows us to make pole-zero plots easily using Matlab. 
For example, the system in (4.14) resulted in the gain and phase plots in 
Figure 4.2. The poles and zeros of the transfer function may be obtained with 
the following expressions:
b=[0.2021 0.3328 0.4517 0.3328 0.20211; 
a=[1.0000 -0.2251 0.6568 0.0349 0.0548]; 
zeros=roots (b) 
poles=roots(a)
When these expressions are executed, the zeros and poles are printed in the 
Matlab command window as follows:
zeros=
poles=
-0.7444 + 0.6677i
0.1668 + 0.7577i
-0.7444 - 0.66771
0.1668 - 0.75771
-0.0789 + 0.9969i
-0.0542 + 0.2968i
-0.0789 - 0.9969i
-0.0542 - 0.2968i
When roots are complex, as these are, they always appear in conjugate 
pairs. Otherwise, as one can easily see by reconstructing the polynomial 
from the roots, some of the weights would have to be complex.
The poles and zeros in (4.21) are plotted on the z-plane in Figure 4.5. 
Imagine moving the operating point around the unit circle from coT = 0 
to a>T = it, and note how the ratio of distances from the operating point 
to all the poles and zeros produces the amplitude gain plot in Figure 4.2.

Linear Systems and Transfer Functions
81
FIGURE 4.5
Poles (x) and zeros (o) of the linear transfer function with coefficients given in (4.14) and 
amplitude and phase responses plotted in Figure 4.2.
Also note that the 180°-phase jumps produced by the zeros on the unit 
circle in Figure 4.5 appear on the phase plot in Figure 4.2. In this way, 
the pole-zero plot illustrates the response of a linear system at all fre­
quencies.
The function pz_plot{b, a) is included with this text. It uses the roots 
function internally to produce the actual plot in a Matlab figure window. 
The plot in Figure 4.5 was produced by executing pz_plot(b, a), where b 
and a were specified as in (4.20).
4.6 
Transient Response and Stability
Having the poles and zeros of a linear system allows us to express the direct- 
form transfer function in (4.11) as a sum of partial fractions. Even with the 
poles and zeros given, the partial-fraction form is not easy to construct. 
Another Matlab function called residue is useful in the construction, and it 
is, in fact, used in the function part_frac_exp provided with this text, which 
computes the pole-zero expansion of the direct-form transfer function (4.11). 
The operation of these functions is complicated when there are multiple poles 
[a multiple pole is due to a term of the form (z - p)”, with n > 1 in the 
denominator of H(z)]. We discuss the partial fraction form only briefly in 
this section, because normally, in DSP, partial-fraction expansions of transfer 
functions are needed only in the special situations discussed here.

82 
Digital Signal Processing with Examples in Matlab
For an example of the use of part ~frac _exp, we again use the linear system 
with weights given in (4.14) as follows:
b=[0.2021 0.3328 0.4517 0.3328 0.2021]; 
a=[l.0000 -0.2251 0.6568 0.0349 0.0548]; 
[r,p,c]=part_frac_exp(b,a)
When these expressions are executed, the result is 
(4.22)
(4.23)
(4.24) 
refer-
r =
c =
1.9720 + 5.3690i
-0.0542 - 0.2968i
3.6880
1.9720 - 5.3690i
-0.0542 + 0.2968i
-0.1099 + 0:3623i
0.1668 - 0.75771
-0.1099 - 0.3623i
0.1668 + 0.75771
And, the corresponding partial-fraction form of H(z) is
- r + ''1P1Z + riP1Z + 
+
J X i Zi | — U I 
1 
I " ■ ■ T
Z-Pl z-p2 z-p3 z-pi
Further aspects of partial fraction expansions may be found in
ences,1,2,12 but we wish to turn to a more important topic, namely, transient 
response and the question of stability.
The transient response of a system is the output signal resulting from a 
single transient input signal, such as an impulse or step function. In partic­
ular, the impulse response of a linear system is the response to a single unit 
sample at t = 0, that is, the response to the unit impulse:
Unit impulse: i - [1 0 0 ...] 
(4.25)
In definition (4.6), we can see that the z-transform of the unit impulse is 
I(z) = 1, and therefore, the impulse response of any linear system is just the 
inverse transform of the transfer function, that is, Z-1(H(z)j.
Consider now the impulse response of H(z) in (4.24), which is a constant, 
c, plus the sum of responses contributed by each additional term. Any of 
these terms describes a simple linear system of the form given in (4.1), with 
b = rnp„, and a = p„ and the corresponding "solution" for the output in (4.2). 
When applying this solution with x = i in (4.25), the impulse response 
contribution from this nth term is found to be of the following form:
Vk = r„p„(p„)k = r„p*+1; k = 0,1,... 
(4.26)
Here, we can see that the impulse response will increase with k, provided 
\p\ > 1, that is, provided the pole is outside the unit circle on the z-plane,

Linear Systems and Transfer Functions 
83
|z| = 1. From this result, we can easily reach the following general conclu­
sion concerning the stability of linear systems:
A system is unstable if its response to a transient input 
increases without bound. The linear system described by 
H(z) is stable if and only if all poles of H(z) are inside the 
unit circle on the z-plane.
This is the usual criterion for stability. If H(z) has pole(s) on and inside the 
unit circle, the system is said to be conditionally stable.
4.7 
System Response via the Inverse z-Transform
We have just seen a situation where the inverse z-transform was useful in 
determining the response of a linear system. In general, given analytic 
descriptions of the input signal, x, and the transfer function, H(z), we can 
obtain the output signal, y, as an inverse z-transform, that is,
y = Z-1(X(z)H(z)| 
(4.28)
The uses of this approach are limited, because signals are not usually known 
in analytic form, but sometimes, the impulse or step-function response of a 
system, or the transient response to a sinusoidal input, is required, and the 
inverse transform approach is useful in these cases.
In general, inverse z-transforms and inverse Laplace transforms are 
derived with the help of the Residue Theorem, which is part of complex 
variable theory.4 Because this subject is not a normal prerequisite for a first 
course in DSP, we will not pursue it here. Instead, the list of z-transforms in 
Table 4.2 is provided. The table is arranged in conventional form, with the 
transforms on the right.
By combining lines in the table, one can find transforms and inverse 
transforms of a wide variety of functions. The lettered lines, A through 
E, are functional relations that may be proved from the definition of the 
z-transform, which appears on line A. Notice that lines B and C are equiv­
alent to the linearity and phase shift properties (lines 2 and 8) of the DFT in 
Table 3.3.
The numbered lines, 1 through 7, then provide the transforms of specific 
functions. These lines may be combined or modified using the relations on 
the lettered lines to produce other transform pairs. For example, we would 
produce the transform of e^Sinfak) using lines D and 4 of Table 4.2 as follows:
Z(e srn(odr)} = ——------- -------------- (4.29)
ze - 2z e cos a + 1

84 
Digital Signal Processing with Examples in Matlab
Table 4.2 may be used in a number of ways to determine transient response 
characteristics of linear systems. For example, we saw in the previous section 
that a linear transfer function may be expressed as a sum of partial fractions, 
as in (4.24). The conjugate pairs of fractions may then be combined to pro­
duce rational quadratic forms with real coefficients. We will discuss this form 
of the transfer function (called the parallel form) in a later section, but here 
we note that with H(z) written in the parallel form, lines 6 and 7 of Table 4.2 
may be used to determine the impulse response of each term, and thus (using 
line B), the impulse response of H(z). There are exercises on this and other 
topics relating to Table 4.2 at the end of this chapter.
TABLE 4.2
A Short Table of z-Transforms
Line
Function of k
Function of z
A
X = [XO,A,-1
x(2) = sr.oxtz-‘
B
ax + Py
aX(z) + p Y(z)
C
f 0; k < A
!/‘ = 1xe-a; k > /L
Y(z) = z-iX(z)
D
yt = e~aiv, k > 0
Y(z) = X(ze°)
E
JA = kxt; k > 0
Y(z) = -z^X(z)
1
[1,0,0,...] (impulse function)
1
2
[1,1,1,...] (step function)
z 
z- 1
3
a'; k > 0 and |a| < 1
z
z-a
4
sin(alc); k > 0
zsina
z2-2zcosa + 1
5
cos(a/c); k > 0
z(z-cosa)
z2-2zcosct +1
6
z
(z - a)(z-P)
isin(k£); 1? = J Ct2 + ft2, 0 =
P
7
(z -a)2 + P2
4.8 
Cascade, Parallel, and Feedback Structures
We have seen how the linear transfer function, H(z) in (4.11), may be factored 
so that its numerator and denominator are in the form of (4.19) or are written 
as a sum of partial fractions in the form of (4.24). The terms in the factored 
version may be combined to form a cascade filter structure, and the terms in 
the partial-fraction version may be combined to form a parallel structure.

Linear Systems and Transfer Functions
FIGURE 4.6
Cascade (left) and parallel (right) filter structures.
FIGURE 4.7
Linear feedback. The output, y, is filtered by H2(z), and the result is added to the input, x. Then 
the sum is filtered by HL(z) to produce y. For realizability, the path through H;(z) and H2(z) must 
include a delay.
Both structures are illustrated in Figure 4.6. The corresponding relationships 
are as follows:
Cascade: H(z) = 
Parallel: H(z) = 
(4.30)
When the two forms are used to implement a given linear transfer function, 
H(z), the overall transfer function is the same, but the individual terms, H,.(z), 
are not the same. They imply different computations in the time domain, and 
sometimes, especially in DSP hardware design, one form may be preferred.
A third form, the feedback structure, is illustrated in Figure 4.7 The output 
signal, y, is processed by H2(z) and added to the input signal, x. The sum is 
then processed by H/z) to produce y. For realizability, element y, obviously 
cannot be computed in terms of itself. Therefore, we must include a delay in 
the path through Hfz) and H2(z). Assuming this is true and the structure is 
realizable, we can apply the transform relation (4.11) and obtain
Y(z) = (X(z) + Y(z)H2(z))H1(z)

86
Digital Signal Processing with Examples in Matlab
FIGURE 4.8
Inverse operation to recover x from y.
Solving this for the overall transfer function, H(z), we obtain
Hz) Ht(z)
H(2) X(z) 1-H^zWz) 
(4‘3 }
The stability of the linear feedback form does not depend on the individual 
stabilities of Hj(z) and W2(z), but instead relies on the location of the poles 
in (4.31). Notice also, that for H(z) to be realizable, the combination Hl(z')H2{z) 
must delay the input by at least one sample.
The final form we wish to discuss in this section, illustrated in Figure 4.8, 
involves a simple concept—the inversion of a transfer function. In the figure, 
we can see that if the second system is the inverse of the first, then, except 
for delays and possible roundoff errors, vectors x and .r should be identical. 
Two questions arise about such a system: (1) is it ever useful, and (2) when 
is the inverse operation realizable?
Regarding (1), there are at least two applications—encryption and com­
pression. If x is a secret signal, then H(z) could amount to an "encryption 
key" that allows one to send y in the clear and allows a receiver with 
knowledge of H(z) to recover x from y. In the compression application, H(z) 
could be a filter that reduces the amount of information in x in a manner 
such that y requires a shorter description and is thus a compressed version of 
x. There is an example of signal compression and recovery in Chapter 8.
The answer to the second question is that, at least with signal vectors of 
finite length, inversion is essentially always possible. This can be seen simply 
by solving (4.9) for xk:
(4.32)
In accordance with Table 4.1, this result is both realizable and causal. One 
may argue further that because y was computed originally from x, and y is 
finite, instability cannot become a problem in this case.
As a final point, (4.32) is obviously not valid if b0 = 0. If such is the case, 
then (4.9) can be solved for xk_j, where b. is the first nonzero weight. Then, 
the inverse operation involves a delay but is still realizable.
The structures reviewed in this section are representative of a variety of 
electromechanical systems and processes, as well as other physical phenom­
ena. The feedback concept is applicable to feedback control systems, that is, 
control systems that measure their own performance. Many control applications 

Linear Systems and Transfer Functions 
87
that were formerly accomplished with continuous electromechanical sys­
tems are now implemented with digital processors, essentially in the form 
of Figure 4.7.
------------ —------------------------------------------—--------- '—*----------‘—““———'—   -------------- —*——
4.9 
Direct Algorithms
Each of the structures just described is an assembly of linear transfer func­
tions; each is in the direct form of (4.11). The direct form of the transfer 
function implies a computational procedure, or algorithm, in the time domain, 
just as (4.11) implies (4.10). But, (4.10) represents only one of several algo­
rithms, all of which produce (4.11) as the transfer function. The algorithms 
are all equivalent in that they produce the same output signal in response 
to a given input signal, but each implies a different set of computations, or, 
if implemented in hardware, a different chip design.
First, three direct forms of (4.10) are shown in Figures 4.9, 4.10, and 4.11. 
These are all equivalent and produce the same output for a given input, 
but the algorithms differ and imply different realizations in hardware. For 
example, the first diagram would require twice as many storage units 
compared with the other two. In languages like Matlab, any of the three 
forms may easily be implemented. The Matlab filter function accepts data 
in the form of Figure 4.9.
The diagram in Figure 4.9 is seen to represent explicitly the direct form in 
(4.10). The "it" symbols in Figures 4.10 and 4.11 each stand for intermediate 
signal vectors like x and y. To prove the form in Figure 4.10 is equivalent to
FIGURE 4.9
Linear system diagram 1. The diagram illustrates the direct form in (4.10), with N = M. Any of 
the weights may be zero.

88
Digital Signal Processing with Examples in Matlab
FIGURE 4.10
Linear system diagram 2. The diagram illustrates the direct form with N = M, but the form of 
the algorithm differs from (4.11). Any of the weights may be zero.
FIGURE 4.11
Linear system diagram 3. The diagram illustrates the direct form with N = M, in a form that 
differs from the first two diagrams. Any of the weights may be zero.
(4.10), 
substitute the second line of the equation into the first line recursively 
for n = 1, 2,...,N - 2, and substitute the last line for n = N - 1, as follows:
?k = Vk+ (Mt-l - «lVk-l + (b2Xk-2 -«21/k-2 + • • • + 
- itN-iJ/k-N+i) ■ • •))
N-l 
N-l
= X bnxk.„ - Y 
(4.33)
«=0 
iii=l
Proving that Figure 4.11 is equivalent to (4.10) is more difficult. The first 
equation recurses by self-substitution to tq = u°_„. Substituting this result

Linear Systems and Transfer Functions 
89
into the second equation, we obtain the following: 
,V-1 
M -1
u°k = xt- 
or xk = 
(4-34)
n=l 
H-0
Similarly, substituting = uj^n into the third equation produces
N-l 
y* = 
<4-35)
it-0
Next, as in (4.8), we take the z-transforms of the convolutions in (4.34) and 
(4.35) and eliminate the transform of u° as follows: 
•
X(z) = A(z)U°(z); Y(z) = BU°(z); 
/. 
(4.36)
This is the transfer function in (4.11), and therefore, Figures 4.11 and 4.9 are 
equivalent.
4.10 State-Space Algorithms
In addition to the three direct forms in Figures 4.9 through 4.11, we have the 
state-space or state-vector form, which is ciosely related to the third form in 
Figure 4.11. In this form, the internal state of the system at time step k is 
specified by the state vector,
uk = [u^ u?-2 ... 
(4.37)
Each of the elements of the state vector is called a state variable of the system. 
Note that the state vector is a column vector and does not include the input 
state variable, u°. Next, we define the following arrays, each having N-1 rows:
With these definitions, the state-space equations for Figure 4.11 are as follows:
A =
0
0
0
~aN-l
1
0
0
0
1
0
~aN-3
0
0
0
.. 0
.. 0
.. 0
.. -a2
0
0
1
B =
0
0
0
1
c =
- ^0aN-1
&N-2 -
b2-b0a2
(4.38)
Mid ~ A*uk + Bxk 
yk = C'*uk + boxk
(4.39)

90
Digital Signal Processing with Examples in Matlab
These are the state-space equations for a linear discrete system, with array 
products indicated with The state-space form is used often in control 
literature and with feedback systems in general.
To show that the state-space equations represent the diagram in Figure 4.11 
and therefore any linear system, we provide the following demonstration with 
N ~ 4, which should be easier to follow and understand than a formal proof. 
With N = 4 and the definitions just given, we have the following arrays:
Uk =
(4.40)
Substituting (4.40) into the first equation in (4.39), we obtain
uk+l ~
3 
«k+l
2
1
0
-n2
Wk
*k =
2
»*•
Uk
0 
Uk
(4.41)
0
0
0
1
_W*+1_
Wt
0 
0
1
2
two equations in Figure 4.11 with N = 4.
Thus, in (4.41), we have the first 
Substituting (4.40) into the second equation in (4.39), we obtain
Vk = [b3-bQa3 b2-b0a2 b^-boa^*
3 
»k
2 
Wt + boxk
_“k
/ 
3 
‘
= bo
3
+ b1u} + b2u2k + b3uk = ^b„uk • (4.42)
This completes the demonstration with N = 4, because the result in (4.42) is 
the last line in Figure 4.11. The demonstration works in the same way for 
any value of N.
The state-space equations (4.39) may also be written in a form that allows 
an output signal element, t/j, to be computed at any time step nonrecursiveiy, 
that is, in terms only of the initial state vector and the history of the input 
signal. This nonrecursive form of (4.39) is as follows:
k-l
uk = Ak * uQ + 
* Bx,
ni=Q
= c’*uk + boxk
(4.43)

Linear Systems and Transfer Functions 
91
The second lines in (4.39) and (4.43) are identical. The validity of the first line 
of (4.43) may be proved by induction. First, with k = 1, and writing the first 
line of (4.39) for k instead of k + 1, the two equations for uk are the same. Now, 
suppose (4.43) is true for time step k. Then, at step k + 1, the first line of (4.43) 
is as follows:
/ 
k 
\
llk+I = A * Ak * a0 +
k 
H = 1 
/
/ 
k-1 
X
= A* Ak * u0 + y, A"' * B.rfc.,,,-! I + 
(4.44)
\ 
»i=0 
/
But, if (4.43) is true for k, then the quantity in brackets in (4.44) must be uk, 
and so (4.44) is the same as the first line in (4.39). Thus, we have shown that 
(4.43) is true for k = 1. Also, we have shown that if (4.43) is true for k, it must 
be true for k + 1. So, the validity of (4.43) is proved by induction.
4.11 Lattice Algorithms and Structures
The dictionary defines a lattice as an open framework of strips of wood or 
metal interwoven to form a regular pattern. The lattice form of a linear 
system gets its name, because if you substitute signal paths for the wood 
and metal strips, its diagram fits the definition.
Lattice algorithms are like direct algorithms in that both provide imple­
mentations of any linear system. As we shall see, converting from direct to 
lattice form is complicated, and the reader may ask whether it is worth the 
effort, considering that the lattice does nothing really new. The answer is 
that it may not be worth the effort, and you may wish to skip this section, 
unless (1) you may be interested in DSP operations that use the lattice 
structure for greater stability or a preferable hardware layout or (2) you are 
a student and your instructor thinks this subject is important. In either case, 
the result of your study may be beneficial.
A single symmetric two-multiplier lattice stage is shown in Figure 4.12. These 
stages are connected to form lattice structures. The signal vectors, u'\ v", etc. 
are indicated at each stage, with the stages being numbered from right to 
left. (Note that the superscripts are not exponents.) In z-transform notation, 
we can see the following relationships in Figure 4.12:
lT(z) =
(4.45)
V',+1(z) = k„U’\z) + z~1V"(z')

N-2
N-2
N-2,
A form of (4.45) more useful for analysis allows us to move from stage n to 
stage n + 1;
A lattice may be formed by connecting stages like this in cascade. Asymmetric 
recursive lattice equivalent to the direct form is shown in Figure 4.13, follow­
ing Gray and Market.5 There are N - 1 "k" weights and N “X" weights; 
hence, there are the same degrees of freedom as in Figures 4.9 through 4.11. 
To prove that Figure 4.13 is an implementation of Figure 4.9, and to show 
how to convert the weights in Figure 4.13 to direct weights, we first normalize 
(4.46) as follows:
Digital Signal Processing with Examples in Matlab
U"+1(z)
y,+1(z)
U"(z) 
Vn(z).
(4.46)
P”(z) = ^ 
U°(z)
V”(z). 
V°(z)'
P“(z)
Q”(Z).
P”-1(Z) 
_Q”-,(z)_
(4.47)
K-„Z^
K’n-lZ'1
yn+1
FIGURE 4.12
Symmetric two-multiplier lattice stage.
un
1
1
^-1
FIGURE 4.13
Symmetric recursive lattice structure.


94
Digital Signal Processing with Examples in Matlab
In (4.50), p and q are coefficient vectors of P'7(z) and Q"(z) in (4.49), and these 
keep changing in the "for" loop in (4.50). At the nth stage, the vectors are p' 
and q", and is added to the b vector, that is, the numerator of H(z) in (4.49). 
At the final stage, A(z) is set equal to PN-1(z), the denominator of H(z) in (4.49).
Having seen equivalent lattice and direct forms, it is reasonable to ask 
how stability translates from direct to lattice. A stable direct system has all 
poles inside the unit circle, and in (4.49), we can see that the poles of P‘v-1(z) 
must also lie inside the unit circle for stability. Suppose N = 2, that is, the 
lattice in Figure 4.13 has only a single stage. Then, in (4.49), PN-1(z) would 
become P’(2)(z) = 1 + k^z-1, and the requirement for stability would be | kJ < 1. 
Because the lattice stages in Figure 4.13 are cascaded, it is not surprising 
that, as Jury9 has proved, the weight in each stage must satisfy | kJ < 1 for 
stability. Thus, 
'
For stablility in a symmetric lattice: 
|fc„| < 1 in each stage of the lattice
We turn now to the question of an inverse for (4.49), that is, a direct-to- 
lattice conversion. First, we prove an interesting property of the symmetric 
lattice. By repeated use of the second equation in (4.49), we can write
(4.51)
P"(z) = 
1
P"(z)
_Q"(z).
_Q”(2)_ 
k;,., 
z’1 _
[Here, we can see that = k!:_} and q": = 1 are the coefficients of z " in P"(z) 
and Q”(z), respectively. We will use this property later.] In the first equation 
in (4.51), we substitute z-1 for z and multiply by z~" to obtain
z^P^z-1) 
. Q"(2) .
2’’ 
*...* Z’ *0*1
Z’1 Ko Z"1 Ll
(4.52)
If we carry out the product in (4.52) from right to left, the elements of the 
right-hand column vector are always equal. Thus, we have this important 
relationship at each stage of a symmetric lattice: The coefficient vector q" is 
the reverse of the coefficient vector p”; that is,
Q"(z) = z-"P"(^j] 
(4-53)
With this relationship, we can construct a direct-to-lattice algorithm. We 
rewrite the center equation in (4.49) as follows:
^-1 * P"~\z)
i
1J L2'■’q"”1(z).
(4.54)

Linear Systems and Transfer Functions 
95
Next, we invert the weight matrix in accordance with (1.15) and rewrite 
(4.54) as follows:
P"-\z)
The first of these two equations gives us the formula for P"’ (z) in terms of 
P"(Z): 
'
P"’l(z) = P 
(z); H = N-l, N-2,.,.,1 
(4.56)
1 - Kh_i
Because, as seen in (4.51), p" = K,^ is the coefficient of z~" in P"(z), we have in 
(4.56) a method for regressing from left to right in Figure 4.13 and computing 
k;,_i, p", and q" at each stage. Furthermore, in the numerator of H(z) in (4.49), 
we can see that AN_t must be the coefficient of z'(N’’ in the sum, because, again 
as in (4.51), the coefficient equals one. Therefore, Aw_] = bN_v and we can 
remove the final term in the sum and thus move to the left in Figure 4.13, 
computing A„ similarly at each stage. All of this is summarized in the algo­
rithm (4.57), in which S’'(z) is the reduced numerator of (4.49) at the nth stage.
As before, the conversion in (4.57) from direct to lattice weights is easy to 
accomplish in Matlab. The function [kappa, lambda] = dir Jo Jaffa), which is 
the inverse of latjojlir, is included with this text. Within the function, the 
conversion is accomplished with the expressions in (4.58). The expressions 
are essentially an implementation of (4.57). The p, q, and s vectors are coef­
ficient vectors of P“(z), Q\z), and S"(z), and these change during execution. 
Tire only difference is that in the m-file, array indices must begin at one 
instead of zero.
Direct -to-Lattice Conversion:
PN-1(z) = A(z); SN-1(z) = B(z); An_i = bN_A
(4.57)
Q"(z) = z-P’’(z)

96
Digital Signal Processing with Examples in Matlab
% Initialize p, s, kappa, and lambda .
p=a;
s=b;
kappa=zeros(1,N-l);
lambda=[zeros (1,N-1) , b(N)] ;
% Updatekappa, q, p, s, and lambda recursively, 
for n=N:-l:2
kappa(n-l}=p (n);
q=p (n: -1:1) ;
p=(p(1:n)- kappa(n-l)*q) / (1- kappa(n-l)A2);
s=s(1:n)-lambda (n)*q;
lambda (n-l.) =s (n-l) ;
end
(4.58)
Thus, we have algorithms for transforming from direct to lattice form 
and vice versa. Lattice filtering is accomplished simply by implementing 
Figure 4.13, by setting the appropriate initial conditions including uN-1 = x, 
where x is the input signal vector, computing successive values of U"(z) and 
V”(z) at each time step in accordance with (4.45) going from left to right in 
Figure 4.13, and computing at each time step as the sum shown in 
Figure 4.13. The lattice algorithm may be summarized in z-transform form 
as follows, with zero initial conditions assumed. The input and output signal 
vectors in Figure 4.13 are xk and yk with k = 0,1,...,K - 1:
Lattice Filtering: 
LI^Cz) = X(z)
U"(z) = U”+1(z) - /c„V"(2) 1
V”+1(z) = k„U” + V"(z) J'
(4.59)
N-l
Y(z) = £A„V’(z) 
n=0
The lat_ filter function is an implementation of (4.59). The reader may exam­
ine the m-file, which is quite short, to see the procedure in detail.
We have seen that a nonrecursive direct form could be constructed simply 
by setting all the "a" weights in (4.10) to zero, so H(z) = B(z), a polynomial 
in z. A nonrecursive symmetric lattice form is also possible but is not attained 
quite as simply. In (4.49), if we set A; = 1 and the rest of the A's to zero, then, 
because Q°(z) = 1, we have
= Y(z) = 
1 
= U°(z)
X(z) 
LfN-1(z)
(4.60)

Linear Systems and Transfer Functions
97
FIGURE 4.14
Symmetric nonrecursive lattice structure.
Now suppose in (4.60) that we could reverse the signal flow and thus invert 
the transfer function. Then, we would have a nonrecursive filter with transfer 
function:
H(z) = P^'fz) = U (z) = B(z) = 14-&1z'1 + -+bN_1z’(N’1’ 
(4.61)
U (z)
That is, a polynomial in z with b0 - 1. But, this requires X(z) = (J°(z) and Y(z) = 
Uv-1(z), which means that the signal in each lattice stage must flow from it" 
to n”+1, without changing the relationships in (4.45). Thus, the operation of the 
lattice stages must be described by (4.46) instead of (4.45). A lattice of this 
form is shown in Figure 4.14. The lattice-to-direct conversion is the same as 
in (4.49), except now H(z) = Pw-1(z):
Similarly, the direct-to-lattice conversion algorithm follows from (4.57) with
Pwi(z) initialized to B(z) to conform with (4.61):
Nonrecursive Direct-to-Lattice Conversion:
PN-1(z) = B(z) = l + b1z’1 + --- + hN_1z’(N’” 
«■„-! = p"
L = N-l,N-2, 
. = Pfz)-Kn 1Qntz)
(4.63)

98
Digital Signal Processing with Examples in Matlab
FIGURE 4.15
Conversion to nonrecursive lattice when ba * 1. The overall transfer function is Y(z)/X(z) = B(z).
Note that the direct weight b0 must equal one. If this is not so, that is, b0 * 1 
in (4.63), one might first think of dividing B(z) by h0, but if bN_t = b0 (and this 
is often the case as we will see in the next chapter), then dividing by b0 would 
result in kn_t = 1, and again, (4.63) would not work. So, the preferred 
approach when &0 1 is to convert H(z) - 1 + B(z) to a lattice using (4.63), 
then subtract one from the transfer function by subtracting the input from 
the output as in Figure 4.15.
A Matlab function called nr_dir_to_lat converts any causal nonrecursive 
system from direct to lattice form by implementing (4.63) and, in cases where 
b0 * 1, modifies the system as in Figure 4.15 and issues a warning that the 
input signal must be subtracted from the lattice output. The reader may wish 
to compare the expressions in nr_dir_to_lat with (4.63).
A companion function, nr_lat_tojdir, converts a nonrecursive lattice to 
direct form in accordance with (4.62). The output of nr_lat_to_dir is a direct 
weight vector with length one greater than the length of the input lattice 
weight vector, and with weight b0 always equal to one.
Another function, nr_lat_filter, implements the nonrecursive lattice filter 
in the form of Figure 4.14. The reader may examine the m-file to see the 
procedure, which follows essentially from (4.46), in detail.
In this section, we discussed recursive and nonrecursive lattice algorithms 
and structures. The development was rather long, so we omitted examples, 
and the reader is encouraged to look at some of the exercises at the end of this 
chapter on conversion to lattice forms and lattice filtering. One of the best ways 
to begin with lattices is to convert a simple linear system from direct to lattice 
form, then process a given signal vector with both forms and see that, although 
the structures are very different, they produce the same output signal vector.
One final point concerning lattice structures is worth considering: We have 
seen that in the recursive lattice, the " k " weights must be less than one for 
stability. In the nonrecursive lattice, if the k weights are all less than one, the 
zeros of H(z) will all be inside the unit circle, resulting in a minimum-phase 
system,1 which is often desirable. Thus, generally speaking, lattice structures 
have k weights less than one; that is, each k weight is an attenuator rather 
than an amplifier of the signal samples. This property has been, and probably 
will be, an advantage in DSP systems that operate at the highest possible 
sampling rates, because, with proper scaling, "hybrid" lattice filters can be 
made with attenuators that perform analog rather than digital weight mul­
tiplication. Hybrid applications continue to exist, because as processor 

Linear Systems and Transfer Functions 
99
speeds increase, requirements for greater speeds somehow manage to stay 
ahead, just out of reach.
4.12 FFT Algorithms
The previous remarks on processor speed requirements bring us to this 
discussion of FFT algorithms, which may also increase the processing speed 
of a linear system. FFT algorithms are based essentially on (4.8), that is, on 
the fact that the DFT (or FFT) of the convolution of two vectors is the product 
of the two individual DFTs (or FFTs).
In any discrete linear system, we have the equivalent relationships in 
(4.10)—(4.12), that is, Y(z) = H(z)X(z). Also, in connection with (4.25), we have 
seen that the impulse response, h, is the inverse transform of H(z). If H(z) is 
nonrecursive, then h is a finite vector equal to b, the weight vector. If H(z) is 
recursive, h is an infinite vector with elements equal to coefficients of the 
polynomial resulting from the division of B(z) by A(z). The point here is that, 
in any case, if H(z) is expressed as a polynomial in z, and if Y(z) = H(z)X(z) 
in which x = [x0 
... ,xK_1] is a signal vector, then
yk = ^h„xk_„-, k = 0,1......K-l 
(4.64)
n=0
Furthermore, if we assume as before that xk is 0 for k < 0, then the products 
in the sum are all zero for n > K, and
K-l
yk=Yh-x^"' * = 
(4.65)
>1=0
Thus, we have the convolution of two finite vectors of equal length (X). (In the 
nonrecursive system with h = [b0 
... hN_J, we simply append K-N zeros
to b in order to obtain h with length K.) We must now ask whether Y(z) can be 
expressed as a product of transforms in this finite case, as it was for the infinite 
case in (4.8). To answer, we examine the inverse DFT of the product of two 
finite DFTs. We use the DFT now instead of the z-transform, because the DFT 
has discrete components, and the inverse DFT is given in terms of these com­
ponents. From definition (3.23), the inverse of the DFT product is as follows:
DFT-'(H„,XJ =
A "
?H=0
K-IK-1 
K-l
- 
e-j2^l1^K^x.e'i2^,lllKej2^uk,K
nt^Qn-Q 
i=0
= | Y Z h "X‘ X 
k = 0,1,..., K -1 (4.66)
rt=0 i-O

100
Digital Signal Processing with Examples in Matlab
hi
h2
hj
J/o = hoxo + h,x3 + h2x2 + h2x}
L-v<>
x3
x2
■T1
hq
h.
h2
h3
y, = hax2+hlx(j + h2x3 + h3x2
Lx>
*0
x3
*2
h0
hi
hi
h3
y2 = AoXj + h^i + ^Xo + JljXj
*1
*0
XZ
ho
h.
h.
hi
y3 = h0Xj + h2x2 + h2x2 + h3x0
A
X2
*1 V
FIGURE 4.16 
•
Illustration of the periodic convolution of vectors h and x of length K = 4, showing the compu­
tation of yQ through y3. Compare with Figure 4.1.
In the final expression, the sum on the right is equal to K when i = k - n + vK, 
where vis any integer, and is equal to zero otherwise. Therefore, the expression 
becomes
K-l
DFT-'(HmXm| = 
k = 0,l,...,K-l; v=0,l,...«
n=0
(4.67)
Thus, the inverse DFT of a DFT product is essentially the same as in (4.8), 
except that when the signal vectors h and x are finite, as they are here, h is 
convolved with the periodic extension of x, that is, an infinite vector with the 
elements of x repeating with period K. [Naturally, in (4.8), we did not notice 
the periodicity of x, because the period was infinite.] An example of (4.67) 
similar to Figure 4.1, showing the alignment of h and x with K = 4, is shown 
in Figure 4.16. Alignments are shown in the figure for the computation of yt 
with all K values of k.
Comparing Figures 4.16 and 4.1, it is obvious that periodic convolution 
differs from ordinary convolution, and in this sense, does not represent a 
linear transfer function. To apply the FFT, we must make periodic convo­
lution look like ordinary convolution. This may be done by appending K 
zeros to the h and x vectors. Then, again using vectors of length four as 
an illustration and extending to K - 8, the periodic convolution looks like 
Figure 4.17 instead of Figure 4.16, and the values of yk in Figure 4.17 now 
represent the operation of a linear system.
Thus, in general, linear processing may be done using products of FFTs, 
provided the signal vectors are extended with zeros. In Matlab, the process 
takes the following form, in which x(l:K) is the signal vector, and li(l :K) is 
the system impulse response vector.
y = IFFT[FFT(/i, 2 * K). * FFT(x, 2 * K)] 
y = y(l:K)
(4.68)

Linear Systems and Transfer Functions
h„ 
ht h2 h3 0 
0 0 
0
.v0 
0 
0 0 
0 
x3 x2 
-ti
h0 
hj h, h3 0 
0 0 
0
-f! 
x0 
0 0 
0 
0 x3 
x2
h0 
k, h2 h3 0 
0 0 
0
x, 
x, x, 0 
0 
0 0 
x3
ha hl h2 h, 0 
0 0 
0i
x3 x2 Xi x0 0 
0 0 
oj
FIGURE 4.17
Illustration of the periodic convolution of vec 
extension, so K is increased from four to eight.
101
l/o = ^0
J/i = /loXj + Z^Xo
y3 = hox2 + h3xt + FijXj
Vi = hax3 + hJx1 + h2x3 + ft3*o
s It and x as in Figure 4.15, but with zero
Before going further with this, we may ask whether the FFT method 
will really save time, because the vectors are all twice as large. The answer 
depends on the size and value of K, as suggested in Chapter 3, Figure 3.5. 
Using the number of complex products as a measure of computing time, 
we can see in (4.65) that there are K products per sum and K sums; 
therefore, K2 products in the convolution. Assuming the best case where 
K (and hence, 2K) is a power of two, (3.17) with N = 2K gives Klog2(2K) 
complex products per FFT. A complex product in Cartesian form requires 
four products, so we use 4Klog2(2K) as the number of products per FFT. 
In (4.68), there are two FFTs and one inverse FFT, for a total of 12Klog2(2K) 
products. Therefore,
# products using (4.65) _ K2 _ K 
.. ,q.
# products using (4.68) " 12Klog2(2K) ~ 12(1 + log2K) ( 
'
This ratio is plotted in Figure 4.18 for cases where K is a power of 2. When 
the ratio exceeds one, the FFT method becomes profitable in terms of com­
puting time. Thus, the FFT method is worth considering for K > 128, and 
definitely preferable for very large values of K.
Seeing that the FFT method can be useful for large values of K, we look 
in more detail first at the nonrecursive case, where (4.65) describes the linear 
system as a single convolution with h = b, the nonrecursive weight vector. 
In practical linear operations where K is large, which we assume here, the 
number of weights is usually less than K. Suppose this is true. To see the effect 
on periodic convolution, suppose h2 = h3 - 0 in Figure 4.17. Then, we would 
not need as many zeros to avoid circular convolution. In fact, we only need 
to extend x with N zeros, where N is the number of nonrecursive weights. 
So, assuming we still require the FFT size to be a power of two, we can

FIGURE 4.18
Ratio of products required for convolution using the direct method (4.65) to products required 
using the FFT method (4.68), when the vector length, K, is a power of two. The FFT method 
becomes profitable as K increases from 64 to 128.
modify (4.68) as follows:
Nonrecursive FFT Filtering; K » N
= length (b)
= length (x)
- next power of two >K + N
= IFFT[FFT(b,L).*FFT(x,L))]
(4.70)
N 
K
L
y
y = real(y(l:K))
hi (4.70), the FFTs are smaller than in (4.68), but the convolution in (4.65) 
now has N products per sum instead of K products per sum. So Figure 4.18, 
while no longer valid for this case, is still an approximation. The last line of 
(4.70) is written to adjust y to its correct length and to remove small imagi­
nary components that are the result of rounding errors.
In the recursive case, direct processing as in (4.10) consists of two convo­
lutions. Assume M - N so there are 2N products per sum, or a total of 2NK 
products in the direct form. In the FFT version, we must first find h as the 
inverse transform of B(z)/A(z). Division of FFTs is not a good idea, because 
even assuming the system is stable, A(z) could have zeros near the unit 
circle at points where the FFTs are computed, resulting in an inaccurate 
computation of h.

Linear Systems and Transfer Functions 
103
The preferred method for finding h with a recursive system is polynomial 
division, in which the division of B(z) by A(z) is simply carried out by long 
division to the desired length of h. The process is easy to encode in languages 
like Matlab that operate on vectors. A function called imp_resp is included 
to illustrate the process. Once li(l:K) is found, it may be used as the nonre­
cursive weight vector in place of b in (4.70) to implement recursive filtering 
using FFTs.
If Matlab is being used to do the processing, the Matlab conv, deconv, and 
filter functions, which use compiled algorithms, should be used, because, 
with long signal vectors, they are at least an order of magnitude faster than 
imp_resp and the expressions in (4.70). (See also the filters function described 
at the end of Section 6.7 in Chapter 6.)
On the other hand, if a processor is using a primitive language, and 
particularly if a 2 -element FFT chip is available, then block FFTfiltering using 
the algorithm in (4.70) should allow processing at the highest possible sample 
rates. In block FFT filtering, as the name suggests, the input signal is pro­
cessed in successive blocks (vectors) of length 2* in order to use the FFT chip 
most efficiently.
The exercises include applications of FFT filtering with recursive and non­
recursive filters. The Matlab language provides an excellent set of tools to 
test such applications, as we saw in (4.70), which is almost a set of Matlab 
expressions. The main purpose of this section has been to show that, essen­
tially as the result of the relationship in (4.8) (that is, convolution in the time 
domain implies multiplication in the frequency domain), the efficiency of 
the FFT algorithm may be applied to digital filtering.
4.13 Discrete Linear Systems and Digital Filters
The names digital filter and discrete linear system refer to signal processing 
systems like those we have been discussing in this chapter. But when we 
use filter, we usually are thinking of a process analogous to filtering in 
chemistry, where some of the ingredients of a mixture are removed by a filter 
to produce a mixture of greater purity. In signal processing, the mixture is 
a signal mixed with noise. The filter is a discrete processor, and the desired 
output is the signal with less noise.
In the next two chapters, we discuss filters in which the discrete processor 
is a linear nonrecursive (FIR) or recursive (HR) system of the type discussed 
in this chapter. When a linear processor of this type is used as a filter, its 
effect is most evident in the frequency domain—particularly in the amplitude 
response characteristic. Certain frequency components of the input signal 
are allowed to pass through the filter, while other components are attenuated 
by the filter. Power gain characteristics of the four types of filters used most

■2m'
2m
FIGURE 4.19
Power gain characteristics of four common types of digital filters. The filter weights were 
determined using algorithms described in Chapter 6.
commonly in DSP are illustrated in Figure 4.19. Power gain is the square of 
amplitude gain and is usually measured in decibels (dB), that is,
where |H(yto)| is the amplitude gain in (4.13), and v= coT/2k Hz-s. Power 
gain plots are commonly used in the literature to illustrate filter gain prop­
erties. In each case, the name of the filter in Figure 4.19 describes the gain 
property (the processing effect of the filter).
The filter weights used for the plots in Figure 4.19 were computed using 
methods described in Chapter 6; thus, the plots are for typical recursive 
digital filters. From (4.71), we can see that a power gain equal to 0 dB 
translates to an amplitude gain equal to one; a power gain equal to -40 dB 
translates to an amplitude gain equal to 0.01.
There are also digital filters that are not linear processors. Nonlinear filters 
such as neural networks and time-varying or adaptive filters are examples. 
The discussion of these is left to Chapter 9 in the case of adaptive filters and 
to texts more specialized in specific areas of DSP.
Digital Signal Processing with Examples in Matlab
s
Power gain in dB = 10 log10
= 20 log10
(4.71)
ra 
cn
IB 5 o 
0.
0.2 
0.4
Frequency (Hz-s)
0.2 
0.4
Frequency (Hz-s)
Bandpass
■30
Lowpass
-10
■30
Bandstop
Highpass

Linear Systems and Transfer Functions 
105
4.14 Exercises
General Instructions: (1) Whenever you make a plot in any of the exercises, 
be sure to label both axes so we can tell exactly what the units mean. (2) Make 
continuous plots unless otherwise specified. “Discrete plot" means to plot 
discrete symbols, unconnected unless otherwise specified. (3) In plots, and 
especially in subplots, use the axis function for best use of the plotting area.
1. Find a nonrecursive expression of the linear system described by 
yk - xk + + yk_j. Assume zero initial conditions at k = 0. Then
present a clear inductive proof of your result.
2. Decide whether the first four properties in Table 4.1 exist for the 
following systems:
a. yk = b sinxM
b. yk = bxk_, - ayk_2
c- yk = 
+ b2xk + b3xM
d. yk = xk + bxk.iyk_}
3. Suppose the sample vector x, which begins at k = 0, is 
x = [1 1 -10 0 ...]. Show the startup sequences for linear 
systems (c) and (d) in Exercise 2.
4. Write the transfer function, H(z), for linear systems (b) and (c) in 
Exercise 2. For (b), express the transfer function as a ratio of poly­
nomials in z.
5. The transfer function of a digital filter is given by
„„ , 
0.0675 + 0.1350z'1 + 0.0675z’2
H(z) = ------------------- J----------- ■ 2- •
1-1.1430Z ’ + 0.4128z 2
a. Using (4.12), show that the filter gain is one at a>T = 0 and zero 
at coT = 7T.
b. Plot the amplitude gain versus frequency in rad, and verify that 
your answer to (a) is correct. Use zero extension with an FFT 
size of 1000 in a manner similar to (4.14).
6. For the filter in Exercise 5, plot phase shift in degrees versus fre­
quency in Hz-s again in a manner similar to (4.14), but this time, 
make a plot of connected discrete points using an FFT size of 100. 
Note that the phase shift at 0.5 Hz-s is indeterminate, because the 
gain at this frequency is zero.
7. Make a connected discrete plot of the complete impulse response 
of the filter in Exercise 5, from its beginning until it becomes neg­
ligible. Normalize the plot so its maximum value is 1.0. Also, print 

106 
Digital Signal Processing with Examples in Matlab
the normalized impulse response and identify the point where its 
magnitude stays below 1%.
8. The weights of a digital filter in the form of (4.11) are as follows:
b=[0.0675 
0.0000 -0.1349 
0.0000 0.0675];
a=[1.0000 -1.9425 
2.1192 -1.2167 0.4128],
Plot the continuous amplitude response versus frequency in terms 
of KHz, assuming a sampling frequency of 40 KHz.
9. Make a continuous plot of the phase shift of the filter in Exercise 
8, in terms of degrees versus frequency in KHz, assuming the filter 
is working with -a time step of 0.025 ms.
10. Make a connected discrete plot of the complete impulse response 
of the filter in Exercise 8, from its beginning until it becomes neg­
ligible. Normalize the plot so its maximum magnitude is 1.0. Also, 
print the normalized impulse response and identify the point 
where its magnitude stays below 1%.
11. Plot the poles and zeros of the digital filter in Exercise 8 using the 
pz_plot function. Explain the following:
a. How did the pole and zero locations indicate a bandpass gain 
characteristic?
b. What is the approximate frequency range of the passband?
c. Why is the phase shift zero at co - 0 and then jumps to 180° 
when co is just above zero?
12. Write a Matlab function [H, mi] - my_gam{b, a) to compute the com­
plex gain of any linear system, recursive or nonrecursive, in terms of 
the b and a vectors in (4.12). Have the function return N values of H 
and v, evenly spaced from zero through half the sampling rate, 
including the endpoints, with N set internally to 1024. Verify that the 
function works by duplicating the left-hand plot in Figure 4.2. If the 
function works, you may use it as desired in subsequent applications.
13. Write a Matlab function [dB,mi] - my-power_gain(b, a) to compute 
power gain in dB versus frequency in Hz-s for any linear system. 
Have the function return N values of dB and v, with N set internally 
to 1024. Write the function so that it limits the range of dB to 200. 
Test the function on the following vectors:
b=[0.0675 
0.1350 0.0675]
a=[1.0000 -1.1430 0.4128]
Plot dB versus v, and use tms([0 0.5 -80 0]) to set the v and dB axis 
limits. If your plot shows 0 dB at v - 0 and -25 dB at v = 0.3, your 
function is probably correct, and you may use it to make power 
gain plots in subsequent chapters.

Linear Systems and Transfer Functions 
107
14. 
A linear transfer function is given as H(z) = (z2 + l)/(z2 - 1.93z + 
1.21). Make two subplots. On the left, plot the poles and zeros of 
H(z). On the right, plot the first 60 samples of the impulse response. 
Discuss the stability of H(z) in terms of each plot.
15. 
Using lines B, 4, and 5 in Table 4.2, find the z-transform, X(z), 
of xk - 2sin(<yt + ?r/6).
16. 
Two linear systems, Hfz) = 1 + 0.4z'1 and H2(z) = -&z-1, are connected 
together to form H(z) in the feedback configuration of Figure 4.7.
a. 
On the complex z-plane, make a pole-zero plot showing the 
trajectory of the poles as b varies from 0 to 1.7.
b. 
For what range of b is H(z) stable?
c. 
For what range of b are the poles of H(z) real?
17. 
The direct form of a particular linear system is given by the following:
„. . 
2 + l.lz-1-0.5z“2
= ------------- --------------------------------------
1 +0.1z-1-0.3z
a. 
Express H(z) in cascade form with two sections.
b. 
Express H(z) in parallel form with two sections.
18. 
A linear system has the transfer function H(z) = (0.5 + 0.6z-2)/(l -0.4z~2).
a. 
Is the system stable?
b. 
Write the difference equation for the system, and make a dia­
gram similar to Figure 4.9.
c. 
Make a diagram similar to Figure 4.10, including the difference 
equations.
d. 
Make a diagram similar to Figure 4.11, including the difference 
equations.
19. 
Define the necessary arrays and write the state-space equations for 
the linear system in Exercise 18.
20. 
Do the following:
a. 
Find the weights of the lattice form of H(z) = (1 + z'')(l - 0.8z’*).
b. 
Make diagrams of the direct and lattice forms.
c. 
Check for equivalence by computing the output vectors of each 
form when the input signal vector is [0 1 3 -1 0 0 0 0].
21. 
Complete the following:
a. 
Convert
. 
1 + l.lz’1 - 0.5z“2
H(z) = -------------- T--------- r
6(1 + O.lz"1 - 0.8z-2)

108 
Digital Signal Processing with Examples in Matlab
to a lattice filter, and draw the lattice diagram, showing all 
weights.
b. Make connected discrete plots of the direct and lattice impulse 
responses in two subplots, one above the other, to show the 
equivalence of the two forms.
22. Complete the following;
a. Convert Ji(z) = l+ z')-z4toa nonrecursive lattice filter, and 
draw the direct and lattice diagrams, showing all weights.
b. Compute and print the direct and lattice impulse responses to 
show the equivalence of the two forms.
23. Plot the power gain in dB versus Hz-s of H(z) in Exercise 21, which 
is the transfer function of a certain cable. Describe the effect of the 
cable when it is used to transmit signals.
24. The impulse response of a filter is given as follows:
h, . 
2?»; 
... 50
a. In the upper plot of three subplots, plot the following signal:
xk - 10sin(0.5rtk) + cos(0.1tt/c); k = 0, 1,...,499
b. Use the FFT filtering algorithm in (4.70) to produce y, the result 
of filtering x with the filter just described. Plot y below x in the 
center plot.
c. Plot the amplitude gain of the filter in the lower plot, and 
explain what the filter did to x.
25. Do Exercise 24 again, but this time use the following IIR filter:
, 
(0.2533z2 +0.5066z + 0.2533)(0.1839z2 + 0.3678z + 0.1839)
lx
(z - 0.4531 z + 0.4663)(z2 - 0.3290z + 0.0646)
In the FFT product, use an impulse response having the same 
length as x.
References
1. Oppenheim, A.V. and Schafer, R.W., Discrete-Time Signal Processing, Prentice Hall, 
Englewood Cliffs, NJ, 1989, chaps. 4, 5.
2. Orfanidis, S.J., Introduction to Digital Signal Processing, Prentice Hall, Upper Sad­
dle River, NJ, 1996, chap. 5.

Linear Systems and Transfer Functions 
109
3. Stearns, S.D. and Hush, D.R., Digital Signal Analysis, Prentice Hall, Englewood 
Cliffs, NJ, 1989, chap. 7.
4. Churchill, R.V., Brown, J.W., and Verhey, R.F., Complex Variables and Applications, 
McGraw-Hill, New York, 1976.
5. Gray, A.H. and Market J.D., Digital lattice filter synthesis, IEEE Trans. Audio 
Electroacoust., AU-21, 491, Dec. 1973.
6. Itakura, F. and Saito, S., Digital filtering techniques for speech analysis and 
synthesis, Proc. 7"‘ Int. Cong. Acoust., Budapest, 261, 1971.
7. Makhoul, J., Stable and efficient lattice methods for linear prediction, IEEE Trans. 
Acoust. Speech Signal Process., ASSP-25, 423, Oct. 1977.
8. Mitra, S.K., Kamat, P.S., and Huey, D.C., Cascaded lattice realization of digital 
filters, IEEE Trans. Circuit Theory Appl., 5, 3,1977.
9. Jury, E.I., A note on the reciprocal zeros of a real polynomial with respect to the 
unit circle, IEEE Trans. Commun. Tech., CT-11, 292, June 1964.
10. Oppenheim, A.V. and Willsky, A.S., Signals and Systems, Prentice Hall, Englewood 
Cliffs, NJ, 1983.
11. Schwarz, R.J. and Friedland, B., Linear Systems, McGraw-Hill, New York, 1965.
12. Moon, T.K. and Stirling, W.C., Mathematical Methods and Algorithms for Signal 
Processing, Prentice Hall, Upper Saddle River, NJ, 2000.


FIR Filter Design
5.1 Introduction
In Chapter 4, we saw that the transfer function of a linear nonrecursive or 
FIR filter is the transform of the filter's impulse response vector, the latter 
being equal to the filter's weight vector. In this chapter, we discuss the 
simplest FIR design techniques, which are based simply on starting with a 
desired transfer function in the form of a discrete Fourier transform (DFT) 
and then obtaining the filter weights via the inverse DFT. Because many 
different applications take the form of one of the four filters in Figure 4.19 — 
lowpass, highpass, bandpass, and bandstop—we focus in this chapter on 
these four basic types of filters.
Compared with IIR filters, FIR filters have advantages and disadvantages. 
The major disadvantage is the result of the FIR transfer function being a poly­
nomial in z and z"1 rather than a ratio of polynomials, and thus having zeros 
anywhere but poles only at the origin. Without poles away from the origin, 
more weights are generally required to provide a transition with given steep­
ness from passband to stopband.
One major advantage of FIR filters comes in a property called linear phase, 
which we defined in Chapter 3 in terms of a shift in the time domain. We 
showed that when a signal x is delayed n time steps, na>T radians are added 
to the phase of x, that is, added to the angle of the transform of x. This means 
that if x is expressed as a Fourier series, each component of x is adjusted in 
phase to produce a delay of exactly n time steps. So then, if the transfer 
function of a linear digital filter is given by
H(e'"7) = \H(e'aT)\e~,naT 
(5.1)
then, in a like manner, the effect of the filter will be to shift each component 
of the input signal by n time steps while modifying its amplitude. Thus, a 
linear-phase digital filter always has a transfer function in the form of (5.1). 
As we shall see, it is easy to impose this requirement on the design of FIR 
filters. We will also see in the next chapter that linear phase shift is not 
possible in stable HR filters that operate in real time; therefore, linear phase 
is a major consideration in choosing the FIR construction.
Ill

112 
Digital Signal Processing with Examples in Matlab
FIR filters have other advantages as well. Having only zeros, they are 
not unstable as long as the weights are finite. Also, the finite impulse 
response property is often an advantage, because we can always specify 
the exact startup time, that is, the duration of the filter's response to a 
transient signal.
5.2 An Ideal Lowpass Filter
To derive the filter weights in the manner just described, as the inverse 
transform of the filter transfer function, we begin with the ideal transfer 
function shown in Figure 5.1, which is ideal in two ways: first, the gain is 
exactly one at frequencies |v| < vc Hz-s and exactly zero for |v| > vc Hz-s, 
and second, the transfer function is real, implying zero phase shift.
The units in Figure 5.1 require some discussion. The frequency units are 
Hz-s. (It is helpful here to recall the frequency conversions in Table 3.1, 
especially co = 2nv/T.) The filter gain, which is real, is expressed (for conve­
nience) as a continuous Fourier transform. The cutoff frequency, which is 
illustrated here at y. ~ 0.1 Hz-s, is below half the sampling rate, that is, y. < 0.5, 
and as long as this is true, the sampling theorem holds, and (3.42) gives a 
simple relationship between the Fourier transform and the DFT of the 
impulse samples, that is,
1' --------------
p 0.8-
"S:
CM
0.6- 
tu 
■o
±
I 0.4 -
0.2-
{)l--------------1--------------1------ ------- 1--------------1--------------I--------------1
-0.4 
-0.2 
0 
0.2 
0.4 
0.6
Frequency v (Hz-s)
FIGURE 5.1
Gain of an ideal lowpass filter. The filter passes all signal components below vc = 0.1 Hz-s 
without alteration, and stops all components above ve Hz-s.

FIR Filter Design
113
Accordingly, because we wish Hidmi rather than HM to be one at frequen­
cies below a>c, the discrete samples of the impulse response of this ideal filter 
must be T times the inverse Fourier transform of the transfer function; that 
is, according to (3.34),
T 
T -2XVJT . ._
hM(kT) = 4- 
± J (Dc'^dw
27TJ^ 
2tIJ-2xvcIT
2vc; k = 0 
(5.2)
= lsin(2ffvci) 
...........’
The central part of the ideal weight vector corresponding with Figure 5.1, 
= hideiAkT), is plotted in Figure 5.2. Note that the maximum weight is 
b0 = 2vc, and that the weights extend infinitely in both directions.
5.3 The Realizable Version
The simplest way to make a realizable FIR filter out of the infinite weight vector 
in (5.2) is simply to truncate the vector at, say, ±L, so there are N = 2L + 1 
weights in the realizable version. We can see from (3.42) that the phase of the 
truncated vector is still zero as long as the weights remain symmetric about 

114
Digital Signal Processing with Examples in Matlab
zero, because each term in the vector's DFT sum is added to its complex 
conjugate, resulting in a real transfer function. Furthermore, if the entire 
weight vector is shifted L steps so that h_L becomes h0, the result is a causal 
digital filter with linear phase shift, in accordance with Table 3.3, Property 8. 
In other words,
Weights of any causal FIR filter with linear phase shift: 
b = [i>0 ... b2L] with bL~„ = bL+„; n = 1,2,...,L
Total number of weights: N = 2L + 1.
(5.3)
Using (5.2), the impulse response (or weights) of our realizable lowpass 
causal, linear-phase filter may be given by
2vc; k = L
sin(2»,(t-L)l 
j..... L J ......2L
%(K - L)
(5-4)
For weights computed in this maimer, with vc = 0.1 Hz-s, the power gain 
characteristic is plotted in Figure 5.3 for L = 4, 8, and 128. The plots illustrate 
the effects of truncating the weight vector. First, the gain curve becomes less 
rectangular as L is decreased. Second, there is significant passband ripple and 
Frequency v (Hz-s)
FIGURE 5.3
Power gain of three lowpass FIR filters, each with cutoff at 0.1 Hz-s. The number of weights 
in each case is N = 2L + 1.

FIR Filter Design 
115
stopband ripple, both of which are undesirable. Third, the power gain at cutoff 
is ideally -6 dB, corresponding with ideal amplitude gain 0.5, that is, halfway 
from one to zero in Figure 5.1. This also may be altered by truncation, that 
is, by the undesirable ripple in the gain characteristic.
The first effect, the nonrectangular gain curve, can obviously be improved 
by increasing L, but a larger L means more hardware and more computation 
per sample.
There is also another less obvious way to improve the shape of the gain 
curve, if the sampling rate can be adjusted. Suppose you are examining a 
digital processing system and notice that the analog-digital converter is set 
to convert at 108 samples per second; the sampling frequency is fs = 100 MHZ. 
Suppose you also know there is never any content above 5 MHZ in the 
analog input signal. Then, obviously, fs is about 10 times higher than 
required, and the filter gain, which is the DFT of the weights, is spread over 
10 times the necessary range. Thus, the gain may be made more rectangular in 
cases like this by reducing the sampling frequency to a more reasonable value, 
say 10 MHZ, which still avoids aliasing. An example is given in Figure 5.4. 
The upper plot is the power gain with/s = 100 MHZ, and the lower plot is 
the power gain with/ = 10 MHZ.
Even if the analog-to-digital conversion rate cannot be changed,/ can be 
reduced simply by decimation, or down-sampling, provided that aliasing is 
not introduced. In the example just discussed, the process is illustrated in
FIGURE 5.4
Power gain of two lowpass FIR filters, each with N = 257 weights and cutoff frequency fc = 
2 MHZ. The upper filter is designed for sampling frequency fs = 100 MHZ and the lower for 
fs = 10 MHZ.

116 
Digital Signal Processing with Examples in Matlab
FIGURE 5.S
Down sampling. The sampling rate is reduced from 100 MHZ to 10 MHZ.
Figure 5.5. Down-sampling is represented by a box with a down-arrow and 
a number indicating the output/input time-step ratio. Aliasing is not introduced 
in this case, and the result is a waveform that we can filter successfully with 
fewer weights, as suggested in Figure 5.4.
When down-sampling with time-step ratio K is accomplished by decima­
tion, K - 1 out of K samples of the input are ignored in each output time 
step. An obvious alternative to decimation is averaging, in which each output 
sample is the average of K input samples. We will see later that averaging 
is preferable to decimation, because it usually reduces the quantizing error 
of the analog-to-digital converter.
The second adverse effect caused by truncating the weight vector (the 
passband and stopband ripple evident in Figure 5.3) cannot be improved by 
the methods just discussed but may be improved and almost eliminated 
with the use of window functions, which is the subject of the next section.
5,4 Improving the Lowpass FIR Filter 
with Window Functions
The name window function comes from the idea of taking an infinite impulse 
response function such as in Figure 5.2 and superimposing a rectangular 
frame, like a window frame, through which you can only see impulse 
response values from index k = -L to L. A rectangular window with length 
2L + 1 has the effect of multiplying h(-L:L) by one and the rest of the h's by 
zero. There are also nonrectangular windows, and these have the effect of 
scaling the weights within the window frame.
We have already seen how the rectangular window is used to obtain a 
realizable lowpass FIR filter with noticeable passband and stopband ripple. 
Our goal in this section is to apply nonrectangular windows to the ideal 
impulse response function and obtain a smoother gain characteristic with 
less ripple.
The theory we need for this application is similar to the convolution 
result in (4.67), that is, the DFT of the convolution of periodic functions in 
the time domain is the product of the two individual DFTs. Suppose we 
now take the DFT of the product of the truncated ideal impulse response,

FIR Filter Design 
117
[/Zj; k = 0, 1,...,N - 1], and any window function, k = 0, 1,...,N - 1]:
DFTf^wJ = ^hkTvke~'2m"k,N
k=Q
N-l N-l 
N-l
= —^ y, H ei2jzik,N £ W ei2ank/Ne‘i2linik!N
N J=o l=o 
„=0
iV-1 N-l 
N-l
N ,-,Q 
„ = 0 
t,Q
N-I
= 
’« = 0-l......N-l 
(5.5)
On the second line, inverse DFTs are substituted for the time functions. On 
the third line, the final sum is N when n = ni - i and zero otherwise. Thus, 
the DFT of a time-domain product is the convolution of individual DFTs, 
and taken together, (4.67) and (5.5) illustrate the duality of convolutions and 
transforms in the time and frequency domains. If x and y are vectors of 
length N,
Convolution in the time and frequency domains:
N-l
DFT-^X,,,?,,,} = X^-,i±vn; fc = 0, 1......N-l; v = 0, 1,... 
(5.6)
>1=0
N-!
DFT{x*yJ = 
= 0,1......N-l 
(5.7)
n=0
Notice that both convolutions involve periodic functions. In the convolution 
in (5.7), the DFT is always periodic as stated in Table 3.3, line 1.
Thus, we see in (5.5) that the gain of the FIR filter is the convolution of 
the DFT of the truncated ideal impulse response, h(0:N - 1), with the DFT 
of the window vector, w(0:N- 1). The DFT of the rectangular window has 
high side lobes, hence, the ripple in the filter gain function.
We illustrate this effect on the FIR lowpass power gain function using the 
four window functions illustrated with N = 31 in Figures 5.6 and 5.7. The 
relatively low value of N is used so the structures of the power spectra may 
be observed easily. The windows are in Figure 5.6, and their power spectra are 
in Figure 5.7. We use boxcar, the Matlab term which is descriptive and easy to 
remember, for rectangular in Figure 5.6. In Figure 5.7, each of the corresponding 
window functions was scaled so the power gain was 0 dB at v = 0.


FIR Filter Design 
119
TABLE 5.1
Effects of Window Spectrum Properties on FIR Filter Power Gain
Window Spectrum Property 
Effect on Power Gain of FIR Filter
Wider main lobe 
Wider transition from passband to stopband
Higher side lobes 
Higher ripple in passband and stopband
Formulas for Weights: k = 0,- 1 (Algebraic), or
Window 
k = [0:N-l] (Matlab)
TABLE 5.2
Window Formulas — Algebraic and Matlab
Hamming
w, = 0.54-0.46 cosf—^j
w = 0.54 - 0.46*cos(2*pi‘k/(N - 1))
Blackman
wt = 0.42 -0.5 cosl —- 1 + 0.08 cos( jq—j!
w = 0.42 - 0.5*cos(2‘pi*k/(N - 1))+
Kaiser
0.08*cos(4*pi*k/(N - 1) )
Wt 
w
w = bessel_0(beta*sqrt{1-(2*k/(N - 1)
-1) . ^2) ) /bessel_0 (beta)
Before discussing the other three windows in detail, imagine convolutions 
in (5.7) using the power spectra shown in Figure 5.7, convolving each of 
these from right to left over the ideal lowpass function in Figure 5.1 (with 
the gain in dB instead of amplitude units). As the main lobe at the left end 
of the window spectrum passes over the cutoff point of the ideal filter 
spectrum, the convolution causes the resulting gain to spread or "roll off" 
around the cutoff frequency. Then, as the window spectrum moves further 
to the left, the side lobes interact with the ideal spectrum to produce the 
ripple we have seen in the resulting passband and stopband. In brief, we 
may summarize these effects as in Table 5.1.
Having seen how windows operate to smooth the ripple in the FIR power 
gain, we turn now to formulas for the windows. The subject of windows is 
extensive in the literature on signal processing and statistical analysis of time 
series.1 7 Here we present, besides the boxcar, just the three w’indows in 
Figure 5.6 that are well known and have certain optimal properties. Algebraic 
and Matlab expressions for these windows are given in Table 5.2. The expres­
sions are easy to understand and implement, except for the Kaiser expres­
sions, which require explanation.

120
Digital Signal Processing mith Examples in Matlab
The Hamming window is the raised cosine function illustrated in Figure 5.6 
and is usually the preferred "two-term" function in filter design. Another 
window, the Harming window, named after J. von Hann, is similar but is zero 
at the end points. The Blackman window is a modulated version of the 
Hanning window. As seen in Figure 5.7, the Blackman window has a relatively 
wide main spectral lobe and lower side lobes, resulting in a smoother FIR 
power gain function with longer transition between passband and stopband.
The Kaiser window requires more discussion, but it is worth the trouble, 
because its properties lead to superior FIR filter designs. Unlike the other 
windows, it has a parameter (j8), which has been set in Figures 5.6 and 5.7 
to produce a main spectral lobe about the same as that of the Blackman 
window. Typically, the value of p is in the range from four to nine. Values 
of P that give main lobe widths comparable to those of the Hamming and 
Blackman windows are
p = n Kaiser and Hamming main lobe widths about equal.
P = 272/r Kaiser and Blackman main lobe widths about equal.
(5-8)
Power spectra of windows with these values of p are illustrated in Figure 5.8 
with N = 127 weights. In the upper half of the figure, for example, with 
P= J37i, the Kaiser and Hamming windows have similar main lobes. However, 
FIGURE 5.8
Power spectra of Kaiser windows with j3 set to give main lobe widths approximately the same 
as Hamming and Blackman main lobe widths.

FIR Filter Design 
121
the Kaiser window has lower side lobes, resulting in less ripple in an FIR 
filter gain function.
The Kaiser window as presented in Table 5.2 also requires computation of 
the zero-order modified Bessel function of the first kind, given by
I0(x) = 1 + W^'f 
(5.9)
1 \ ni ) 
ii=i '
Fortunately, this sum converges rapidly due to the factorial in the denom­
inator and is easily approximated in various ways. The Matlab function 
besseli (0, x) may be used to compute (5.9). If Matlab is not available, a simple 
algorithm called besselJD is included in the software. The algorithm is in the 
Matlab language but would be easy to translate. The use of besselj) is illus­
trated in Table 5.2.
There is an important point about using windows in the manner just 
described to design lowpass FIR filters: due to the truncation of the original 
ideal impulse response followed by the application of a window, the win­
dowed weights may require scaling to adjust the lowpass gain to 0 dB at v = 0. 
(We noted that this was done for the plots in Figure 5.7.) For lowpass filters, 
because the amplitude gain at v = 0 is just the sum of the weights, the 
adjustment (in Matlab notation) after the weights are computed is simply
b = b/sum(b')
For highpass, bandpass, and bandstop filters, the gain may be adjusted to one 
at v= 0.5, (Vj + vz)/2, and v= 0. These adjustments are made in the fir_weights 
function described in the next section.
To conclude this section, Figure 5.9 illustrates power gain spectra for four 
lowpass FIR filters, each with N = 127 weights and cutoff at vc = 0.1 Hz-s. In 
each case, the lowpass power gain results from the use of the window with 
a power spectrum in the corresponding position in Figure 5.8. The relative 
effects on passband ripple and passband-to-stopband transition are not easy 
to see in Figure 5.9, but the relative effects on stopband ripple are evident.
5.5 
Highpass, Bandpass, and Bandstop Filters
In the preceding section, we saw how to apply a window to a truncated ideal 
impulse response to achieve a lowpass FIR filter. This concept applies equally 
well to impulse responses of other "ideal" FIR filters with rectangular gain 
functions, and in particular, to highpass, bandpass, and bandstop FIR filters. 
Hence, all we need for these filters is the impulse response functions, or weight 
vectors, of the filter types to use in place of the lowpass vector, h, in (5.4).

FIGURE 5.9
Lowpass FIR filter power gain; L — 63 and vc = 0.1 Hz-s. Each power gain function results from 
the use of the window with corresponding power gain in Figure 5.8.
FIGURE 5.10
Allpass gain minus lowpass gain equals highpass gain, assuming the HJf,'s are of equal 
amplitude.
Weight vectors for the other filter types can be derived by subtracting 
lowpass weight vectors in the form of (5.4). For example, Figure 5.10 illus­
trates how to produce an ideal highpass filter gain by taking the difference 
between two ideal lowpass filter gains. We use the notation Hlp(v, vc) to 
represent the gain of an ideal lowpass filter with passband from zero to vc 
Hz-s. The filter on the left in Figure 5.10 with gain (v, 0.5) is a lowpass 
filter with passband extending to half the sampling frequency, that is, an 
allpass filter. The second filter is an ideal lowpass filter with cutoff at v, Hz-s. 
The difference between these two gains, which are both one in the passband, 
is the ideal highpass gain, Hhp(v, vc), on the right.
Ideal bandpass and bandstop gains are produced similarly. These are 
denoted Hbp(v, vlx v2) and Hbs(v, vv v2), respectively, where Vj and v2 are the 

FIR Filter Design 
123
band-edge frequencies. We summarize all three conversions as follows:
Conversion of ideal lowpass FIR filter gain to
Highpass: Hhp( v, vf) = H(j,( v, 0.5) - HZp( v, vr)
Bandpass: Hbp(y, vlz v2) = Hip(y, v2) -Hlp(v, vj 
'
Bandstop: Hfe( v, vu v2) = Hlp( v, 0.5) - Hbp( v, v„ v2)
Because the DFT is a linear operation (Table 3.3, Property 3), the implica­
tion of (5.10) is that we may construct highpass, bandpass, and bandstop 
FIR filters using lowpass weight vectors given by (5.4). First, we note that 
the impulse response of the allpass filter is the discrete impulse function. 
Using vc = 0.5 in (5.4) and denoting the allpass weight vector bap, we have 
the following:
b„p = [zerosfl, L), l,zeros(l, L)] 
(5.11)
Using similar notation, that is, using fe/,f]( vc) for the weight vector correspond­
ing with Hbp(v, vc), etc., and the relationships in (5.10), the highpass, band­
pass, and bandstop weight vectors may be formulated as follows:
Conversion of ideal lowpass FIR filter weights to
Highpass: b,,p(vc) = bap - blp( vc)
Bandpass: bbp( v,, v2) = b,p( v2) - blp( v2)
Bandstop: bhs(v1,v2) = bap-bbp(v1,v2)
Thus, in (5.4), (5.11), and (5.12), we have weight-vector formulas for the 
four basic types of FIR digital filters. A Matlab function b = fir_weights(N, 
band,windo,vl, v2), in which N is the number of weights, band designates 
the band (lowpass, highpass, etc.), windo designates the window, and [vl, 
v2] are the critical frequencies, is provided so the reader can see the equations 
in algorithmic form.
Any of the windows discussed in the previous section may be applied to 
the weight vectors in (5.12), either before or after they are combined, with 
the same effect. Another function, w = windmv(N, type,beta), generates several 
different nonnormalized windows and is used internally by fir_weights. For 
example, the weight vector for the FIR filter with N = 127 weights, band = 1 
for lowpass, windo = [7, 8.89] for the Kaiser window with ft = 8.89, and vr = 0.1 
Hz-s on the lower right in Figure 5.9 is produced by the following expression:
b = fir_weights(127,1, [7,8.89] , .1) ; 
(5.13)
To conclude this section, four examples of FIR filter power gain are presented 
in Figure 5.11. In each case, the number of weights is IV = 127. Tire four

124
Digital Signal Processing with Examples in Matlab
FIGURE 5.11
Power gain of four typical FIR filters, each with N = 127 weights and using the Kaiser window 
with (3 = 8.89.
windowed weight vectors were computed with the following expressions:
bl = fir_weights(127,1, [7,2*sqrt(2)*pi], .1) ;
bh = fir_weights(127,2, [7,2*sqrt(2)*pi], .4) ;
(5.14) 
bp = fir_weights(127,3,[7,2*sqrt(2)*pi],.1,.2);
bs = fir_weights(127,4, [7,2*sqrt(2)*pi], .1,.2) ;
Let b stand for any of the four weight vectors above. The filter gain in each 
case in Figure 5.11 is as follows:
H = fft(b,1000) ,- 
(5.15)
Hopefully, the reader is familiar by now with the effect of appending zeros 
to the windowed weight vector before taking the FFT. The result in this case 
is a DFT with 1000 points and a 501-point gain function in the range [0, 0.5] 
Hz-s, that is, from zero through half the sampling rate. The final step is to 
convert the amplitude of H to dB:
dB = 20*logl0 (max(abs (H(l: 500)) , 1 .e-6)) ; 
(5.16)
The resulting power gain in dB is plotted in Figure 5.11 for each of the four 
filters.

FIR Filter Design
125
5.6 
A Complete FIR Filtering Example
We now have all the tools necessary for the application of any of the four 
common types of FIR filters to a digitized signal vector. There are a few 
practical matters worth noting when this is done, such as conversion of units, 
phase shift, etc. To illustrate some of these, we provide the following example 
in which an input vector consisting of a signal with additive high-frequency 
random noise is filtered to improve the signal-to-noise ratio. We should note 
at the outset that this is a typical application of filtering, but rarely do the 
results look this good in practice. The reader should not be discouraged 
when the results do not look as good with real data.
Suppose you are looking for electromagnetic pulses emitted by a distant 
source known to be sending in a band between 100 and 200 MHZ. A sensitive 
antenna is aimed at the source, and the receiver output is digitized at 109 
samples per second. One microsecond of data (that is, 1000 samples) of the 
receiver output are plotted in the upper part of Figure 5.12.
Suppose also that most of the noise power is known to be concentrated at 
frequencies above 300 MHZ. Having this information, you put the receiver 
output through a lowpass FIR filter cutting off at 250 MHZ. The filter you 
choose is similar to the upper left filter in Figure 5.9. It uses the Hamming 
window and has N = 127 weights. The filter output is plotted in the lower 
half of Figure 5.12 and gives evidence of a single pulse at about 650 ns after 
the beginning of the 1-us interval.
FIGURE 5.12
Receiver output consisting of a transient signal near 650 ns plus high-frequency random noise 
(upper plot), and filtered version (lower plot).

126 
Digital Signal Processing with Examples in Matlab
There are several things to note about the lower plot. First, the amplitude 
range is 1/10 of the upper plot range. The waveform in the vicinity of 650 ns is 
in exactly the same position in the upper plot but is obscured by the high- 
frequency noise. Second, to align the two signals, the lower plot has been shifted 
left 63 ns to account for the filter delay of L = (N -1 )/2 = 63 ns, that is, to account 
for the linear phase shift of the filter. [See the discussion with (5.1)]. And third, 
the filter output component at 0 ns looks similar to the signal at 650 ns and 
might be mistaken for another pulse from the source, but it is only the filter 
startup transient, that is, the transient response of the filter with zero initial 
conditions to the onset of the upper waveform.
There are other things to consider as well, such as the choice of the number 
of filter weights and the cutoff frequencies, etc., in addition to assuring that 
the filter output is beirig correctly interpreted.
5.7 
Other Types of FIR Filters
Besides the four principal types of FIR filters (lowpass, highpass, bandpass, 
and bandstop), other FIR filter types have found use in various special 
applications. It is easy to generalize the procedure we have been using to 
obtain the design of an FIR filter with any specified gain characteristic. We 
simply begin with the specified continuous gain function, find the ideal 
impulse response as in (5.2), translate to a realizable version as we did in 
(5.4), and apply a window to the realizable version. In other words,
To find the weights of an FIR filter:
1. Specify a real continuous gain, H(jai), in the range -nff < a>< n/T 
such that
2. Ideal impulse response: hilkal(kT) = H(ja))e,,utT da;
—oo < k < <*> 
~ "
3. Realizable, causal version: hk = hiuKk-LjT); 0<k<N-l; 
N = 2L + 1
4. FIR filter weights: bk - wkhk; 0 < k < N - 1
(5.17)
As an example of this procedure, we design an FIR differentiator, that is, a 
filter that differentiates the input signal. The Fourier transform of the deriv­
ative of x(t) with respect to t is 
which may be proved by taking the
inverse transform as follows:
. 4(x(t» (5.18)

Therefore, in step 1, we specify
Hf.jd)') = jar, \(d\<ti/T 
(5.19)
In step 2, we determine the ideal impulse response (after some effort):
In step 3, we choose a filter length, N = 2L+ 1, and determine the realizable 
impulse response:
Finally, in step 4, we apply a window function to obtain the weight vector, 
which in Matlab notation is b = w .* h. The vector length (number of weights) 
is the overall range of A: in (5.21), that is, N = 2L + 1.
An example of a differentiator with N = 63 weights using the Hamming 
window is presented in Figure 5.13. The amplitude gain, |H(Jo>)|, is plotted
FIGURE S.13
Amplitude gain of FIR differentiator with N = 63 weights and Hamming window (left). The 
on the gain plot indicates the frequency (1 Hz) of an input unit sine wave (upper right). 
The differentiator output (lower right), after startup, is a cosine wave with amplitude a> = 2tt. 
The time step in this example is T = 0.05 s.
FIR Filter Design
k,M(kT) = Tf ja>elkmTdco
0; k = L
(-D .
kT ’
0; k = 0
(5.20)
(5.21)
(-1)*~L
k = 0, 1,..., L-l, L + 1,...,2L 
(k-L)T
5 
10
Frequency (Hz)
0 
1 
2
Time (s)
co
40

128
Digital Signal Processing with Examples in Matlab
versus frequency in Hz on the left. The input signal on the upper right is a 
unit sine wave at 1 Hz, sampled at 20 sampies/s. The output, which has 
been shifted left L = 31 samples to account for linear phase shift, is the 
derivative of the input with respect to time, that is, a cosine wave with 
amplitude equal to a), or 2zr in this case, as shown by the marker (*) on the 
amplitude gain plot. Notice also the startup transient on the output (y) plot, 
with a duration of L — 31 samples.
Differentiators inherently amplify high frequencies. If a signal is known 
to have no components above, say, a>c, the differentiator may be designed to 
cut off at a>c and thus eliminate the amplification of high-frequency noise in 
the output. This would be done simply by changing the limits of integration 
in (5.20) from ±n!T to ±tyc. The result, in place of (5.21) is as follows:
0; k = L
‘ O)ccos((k-~L)(DCT) sm((k-L)a)eT) 
(5.22)
(k-LfxT '
An example of this with a>c = 4zr rad/s is shown in Figure 5.14. Because the 
input signal at cq = 2zr rad/s is entirely below 0)c, the results after startup 
are the same as in Figure 5.13.
Another example of the design procedure in (5.17), and another example 
of a filter with special use in signal processing, is the FIR Hilbert transformer,
FIGURE 5.14
Repetition of Figure 5.13 with differentiator cutoff reduced from k/T to/. = 2 Hz.

FIR Filter Design 
129
which is a digital filter designed ideally to provide a 90° phase shift at all 
frequencies below half the sampling rate. Its ideal transfer function is simply
J j; -n/T < a< 0 
[-/; 0 < <y< rr/T
(5.23)
The Hilbert transformer is used typically with complex signals with samples 
that have real and imaginary parts. The output of a radar detector, for 
example, consists of real and imaginary or quadrature components on sepa­
rate channels, and taken together, these become a complex signal with real 
and imaginary parts.
The operation of a Hilbert transformer on complex signals brings up an 
interesting spectral property of these signals. Suppose we have a complex 
signal vector, s = x + yy. Because the DFT is a linear operation, its DFT 
components must be S„t = Xm + jY„,. We recall from (3.11) and Table 3.3, 
Property 2, that, at negative frequencies, X_„, = X'„, when x is real. The result 
is S_m = X'„, + j Y'„ for the spectrum of a complex signal at negative frequen­
cies. Now suppose the imaginary component, y, is the Hilbert transform of 
the real component, x. This implies Y„, = -jX„, for m > 0 and Y,„ = jXm for 
m < 0, and therefore, S„, = X„, + j(-;X,„) = 2X„, and S_„, = X'„, + y(;X'„) = 0 for 
m > 0. Thus, we have the following result:
If s is the complex signal s = x + jy, and if y is the Hilbert 
transform of x, then the spectrum of s is one-sided, that is,
j2Xm; m>0
| 
0; m < 0
(5.24)
The FIR Hilbert transformer is easy to design using the procedure in (5.17). 
The ideal impulse response in step 2 of the procedure is
haAkT') =
0; k even
k odd 
kn
(5.25)
The rest of the procedure, as well as the application of the Hilbert transformer 
to complex signals in communications and other related areas, is described 
in references 2, 3, and 9.

130 
Digital Signal Processing with Examples in Matlab
5.8 Exercises
General Instructions: (1) Whenever you make a plot in any of the exercises, 
be sure to label both axes so we can tell exactly what the units mean. (2) 
Make continuous plots unless otherwise specified. "Discrete plot" means to 
plot discrete symbols, unconnected unless otherwise specified. (3) In plots, 
and especially in subplots, use the axis function for best use of the plotting area.
1. Plot the unweighted, linear-phase weights given by (5.4) in two 
subplots, one above the other. Set L = 200 for both plots. Set the 
cutoff frequency (vc) at 0.03 Hz-s in the upper plot and 0.3 Hz-s in 
the lower plot.
2. Complete the following:
a. Derive an expression for the "upper envelope" of (5.4), that is, 
an expression for a smooth curve passing through all the posi­
tive peaks beyond the second peak at k - L = 5/(4vc).
b. For vc = 0.03, how far from the peak impulse response (bL) does 
this envelope drop below 5% of the peak?
c. Verify your answers to (a) and (b) by plotting bklbL as well as 
the upper envelope of bk/bL for 240 < k < 400. On your plot, use 
abscissa limits [L, 2L] and ordinate limits [0, 0.5].
3. Make a plot similar to Figure 5.3 of the power gains of two unwin­
dowed FIR lowpass filters with weights given by (5.4). Use the 
following data:
Sampling frequency: 20 samples/s Power gain from -60 to 5 dB 
Cutoff frequency: 1 Hz 
Frequency from 0 to 10 Hz
Number of weights: 33 and 129
4. Plot the power gains of the two filters described in Exercise 1 in a 
single figure with no subplots. Plot dB in the range [-80, 5] versus 
frequency in Hz-s in the range from zero to half the sampling rate.
5. Complete the following:
a. Show that the amplitude gain of any linear-phase filter with 
weights in the form of (5.3) may be expressed in the following 
form:
L
|H(wT)| = |^ + 2^COS(b*T)| 
t=i
[Hint: Use the weight vector in (5.3) centered at zero, noting 
that the time shift has no effect on the amplitude spectrum.]

FIR Filter Design 
131
Compare this result with (2.32), and note that here we have a 
continuous Fourier series for a function of frequency (not time).
b. Express the amplitude gain at half the sampling rate as a simple, 
nontrigonometric sum. Use this expression to compute the power 
gains of the two filters in Exercise 1 at half the sampling rate. 
If you have worked Exercise 4, use this result to verify the ends 
of the two dB plots.
6. Write a Matlab function, b = my_fir_lowpass_weights(N,nuc) that 
will compute unwindowed weights of any linear-phase lowpass 
FIR filter, where N is the number of weights, and nuc is vc, the 
cutoff frequency in Hz-s. Write the function so that if N is even, it 
will be increased by one with a warning.
a. Test the function with N = 7 and IV = 8 to see that it prints the 
correct number of weights and issues a warning when N = 8.
b. Use the function to plot the unweighted impulse response of a 
lowpass filter with vc = 0.3 and IV = 401. Compare the plot with 
your answer to Exercise 1, or verify it by computing (5.2) and 
plotting the result. If the plots are the same, you may use the 
function in subsequent exercises.
7. Write a Matlab function called w = my_window(N,type,beta) to create 
w, a row vector of N window samples. Let type designate any of 
the three windows in Table 5.2 plus the boxcar window, and let 
beta designate the parameter for the Kaiser window. (Note: You 
may use the Matlab function nargin to determine whether there 
are two or three input arguments.) Plot all four windows in a single 
figure for /V = 15, using fl = 7 for the Kaiser window. Make con­
nected discrete plots using a different symbol for each window, 
and print a legend on your plot.
8. Make connected discrete plots of the Hamming, Blackman, and 
Kaiser windows with IV = 63 weights in three vertically stacked 
subplots. Make the Kaiser window look like the Blackman window. 
Label the plots to show which is which.
9. Design a lowpass FIR filter to cut off at 12.5 MHZ, assuming a 
sampling frequency of 100 MHZ. Use the Hamming window, and 
use L = 12. Plot the power gain in terms of dB vs. frequency in MHZ.
10. The specifications of an FIR lowpass filter are as follows:
Time step: 100 ps; Cutoff: 1.0 KHz; Power gain < -40 dB for/> 2 KHz; 
Window: Blackman
Design the filter by making 11 plots in the same graph of the power 
gain in terms of dB versus frequency in KHz using L = [5:15]. 
Indicate the smallest value of L that meets the specifications.
11. In a single graph, plot the power gains of two lowpass FIR filters 
with T and vf the same as in Exercise 10. Use the Blackman window 

132 
Digital Signal Processing with Examples in Matlab
for one filter and the Kaiser window for the other, with ft set to 
match the Blackman window. Use N = 31 weights in both filters.
Plot dB in the range [-100, 0] versus frequency in KHz, and identify 
each plot. Briefly compare the two filters.
12. Design a highpass FIR filter with 41 weights to operate with 106 
samples/s, with passband beginning at 100 KHz. Use the Kaiser 
window with p = 6. Plot power gain in dB in the range [-100,0] 
versus frequency in KHz. What is the amplitude gain at cutoff?
13. Design a bandpass FIR filter with L = 30 to operate with time step 
T = 0.1 s and a passband from 3 to 4 Hz. Plot power gain in dB in 
the range [-100,0] versus frequency in Hz.
14. Complete the'following:
a. Create a Gaussian noise vector using the following expressions:
N = 200; randn('state0); x = 10*randn(1,N);
Treat this vector as a waveform sampled at 100 samples/s and 
beginning at t = 0. Next, create vector y by passing x through a 
lowpass FIR filter designed with 61 weights, the Hamming 
window, and cutoff at 30 Hz. Finally, add to y a unit 40-Hz sine 
wave beginning at t — 1.0 s and extending to the end of y. Plot 
y versus t in the upper of two subplots.
b. Create vector u by passing y through a highpass FIR filter with 
61 weights, the Hamming window, and cutoff at 35 Hz. Plot u 
versus t in the lower subplot, and explain the result.
15. Complete the following:
a. Create vector x consisting of 200 samples of the sum of five unit 
sine waves with frequencies 10,15, 20,25, and 30 Hz, using time 
step T = 10 ms. (For full credit, create x with a single Matlab 
expression.) Plot x versus t in seconds in the upper of two 
subplots.
b. Design a bandpass filter with 63 weights to select the component 
of x at 15 Hz. Use one of the windows in Table 5.2. Plot the 
filtered version of x in the lower subplot, and comment on the 
effectiveness of the filter.
16. Complete the following:
a. Create the vector x in Exercise 15 with N - 300 samples. Plot 
x(f) in the range t = [0, 2] s in the first of three subplots.
b. Filter x as in Exercise 15, but this time with a bandstop filter to 
remove the component at 15 Hz. Plot the filter output in the 
center subplot.
c. Shift the filter output to account for the delay caused by the 
filter. Then subtract it from x. Plot the result in the lower subplot 

FIR Filter Design 
133
over the same range of t. What is the frequency of the final 
waveform?
17. Derive the impulse response of the ideal differentiator, beginning 
with H(ja)) = j(D and using integration by parts.
18. Complete the following:
a. Create the vector p = [0:2 * T:l, 1 - T: - T:0], Let x consist of three 
cycles of p. Make a continuous plot of x(f) versus f using T - 0.01s 
in the upper of three subplots.
b. In the center subplot, make a connected discrete plot of the im­
pulse response of a differentiator using L = 10 and the Hamming 
window. Plot the response vs. time t.
c. In the lower subplot, show the differentiator output when the 
input is x. Shift the output so it is correctly aligned with x in 
the upper subplot.
19. Make two subplots. On the left, plot N = 101 samples of the sum 
of the impulse responses of an ideal allpass filter with gain equal 
to 2.0 and an ideal lowpass filter with gain equal to 1.0 and vc = 
0.2 Hz-s. On the right, plot the amplitude gain of this combined 
filter after applying the Hamming window.
References
1. Harris, FJ., On the use of windows for harmonic analysis with the discrete 
Fourier transform. Proc. IEEE, 66, 51, Jan. 1978.
2. Oppenheim, A.V. and Schafer, R.W., Discrete-Time Signal Processing, Prentice Hall, 
Englewood Cliffs, NJ, 1989, chap. 7.
3. Orfanidis, S.J., Introduction to Digital Signal Processing, Prentice Hall, Upper Saddle 
River, NJ, 1996, chap. 10.
4. Kaiser, J.F., Design methods for sampled data filters, Proc. First Allerton Conf, on 
Circuit and System Theory, 221, Nov. 1963.
5. Kaiser, J.E, Digital filters, in System Analysis by Digital Computer, Kuo, F.F. and 
Kaiser, J.F., Eds., John Wiley & Sons, Inc., New York, 1966, chap. 7.
6. Blackman, R.B. and Tukey, J.W., The Measurement of Power Spectra, Dover Publi­
cations, New York, 1958.
7. Helms, H.D., Nonrecursive digital filters: design methods for achieving specifi­
cations on frequency response, IEEE Trans. Audio and Electroacoust., 16, 336, Sept. 
1968. 
'
8. Mitra, S.K. and Kaiser, J.F., Eds., Handbook of Digital Signal Processing, Wiley, New 
York, 1993.
9. Parks, T.W. and Burrus, C.S., Digital Filter Design, Wiley, New York, 1987.
10. Antoniou, A., Digital Filters: Analysis and Design, 2nd ed., McGraw-Hill, New York, 
1993. 
'
11. Elliott, D.E, Ed., Handbook of Digital Signal Processing, Academic Press, New York, 
1987. 
'

134 
Digital Signal Processing with Examples in Matlab
12. Rabiner, L.R. and Rader, C.M., Eds., Digital Signal Processing, IEEE Press, New 
York, 1972.
13. Roberts, R.A. and Mullis, C.T., Digital Signal Processing, Addison-Wesley, Reading, 
MA, 1987.
14. Jackson, L.B., Digital Filters and Signal Processing, Kluwer Academic Publishers, 
Norwell, MA, 1989.
15. Lutovac, M.D., Tosic, D.V., and Evans, B.L., Filter Design for Signal Processing 
Using Matlab and Mathematics, Prentice Hall, Upper Saddle River, NJ, 2001.

HR Filter Design
6.1 Introduction
In Chapter 4, we saw that the transfer function of a linear recursive or IIR 
filter is a ratio of polynomials in z, rather than just a single polynomial that 
describes an FIR filter. The direct-form algorithm, transfer function, and gain 
of an IIR filter was given in (4.10) through (4.12):
(6.1)
Algorithm: 
yk =
N-l 
M-l
u=0 
»i=I
Y(z) _
Filter gain:
H(e'“T) =
X(z) 1 4-rjjZ-1 + ••• + 
ba+b1e-'aiT+- + bN_Ie-l(N-^T
1+n1e~'l“'r+
Thus, the IIR transfer function has poles as well as (usually) zeros on the 
z-plane, and, as stated in (4.27), the poles must be inside the unit circle for 
stability. As we shall see in the next section, this restriction on the location 
of the poles results in the impossibility of a stable, linear-phase IIR filter 
operating in real time.
On the other hand, as noted in Section 5.1, the presence of poles in H(z) 
away from the origin can also result in a sharper transition from the passband 
to the stopband of an IIR filter, compared with an FIR filter with the same 
total number of weights. With this advantage, IIR filters are still used 
in situations where at least a small amount of phase distortion is allowable.
As in Chapter 5 for FIR filters, the primary purpose of this chapter is to 
develop weight-vector formulas for the standard designs of lowpass, high- 
pass, bandpass, and bandstop IIR filters. Our approach will also be similar 
to the development in Chapter 5, that is, beginning with an "ideal" lowpass 
design, creating from this a realizable IIR lowpass filter, and finally trans­
forming the latter into the other three filter types.
135

136 
Digital Signal Processing with Examples in Matlab
6.2 Linear Phase
As we saw in Chapter 5, the key to understanding linear phase is DFT 
Property 8 in Table 3.3, that is, a uniform shift in time transforms to a phase 
angle which is a linear function of frequency. This, in turn, led to the con­
clusion in (5.1), that is, if H(eJ“T) is the gain function of a linear-phase filter, 
the following condition must hold:
H(e,uT) = 
= R(e"ur)N,',,MT 
(6.2)
where R(e^“T) is a real function of co and therefore equal to its own conjugate, 
R(e~'). In terms of the z-transform, (6.2) becomes
H(z) = z“"R(z); R(z) = R(z“’) 
(6.3)
Now suppose R(z) is a ratio of polynomials in z, as it must be for an IIR 
filter. Then, if R(z) has a pole at z0, the condition in (6.3) implies R(z) must 
also have a pole at z'1 = z0, that is, a pole at z = l/z0. Thus, R(z) and therefore 
H(z) cannot be stable, because if z0 is inside the unit circle, l/z0 must be 
outside the unit circle. Thus, any stable IIR filter in the form of (6.1) cannot 
have a linear phase characteristic.
Even though linear phase is not possible in (6.1), there are two points worth 
noting. First, the filter designs introduced in this chapter have, as we shall see, 
phase characteristics that are nearly linear through most of the passband. The 
stopband phase is usually not of concern, and so the nearly linear phase in 
portions of the passband results in many useful applications of IIR digital filters.
Second, if a digital filter (IIR or FIR) is not operating in real time, there is 
a way to operate the filter without phase shift, that is, a way to modify the 
operation of the filter so that the resulting gain is real. The method involves 
reversing the input or the output signal vectors.
In (4.6), we defined X(z) as the z-transform of a signal vector x, that is, the 
z-transform of a sequence of samples beginning at t = 0 and ending at t = 
(N - 1)T. Now suppose we create a vector xr by reversing this sequence so 
that it begins at t = 0 and ends at t = -(N - 1)T. Obviously, we could not 
reverse x in real time, but if x were recorded, we could reverse the elements 
of the signal vector. Thus,
x = [Xo,...,*^]; x = 
(6.4)
To transform xr, we must extend the transform definition in (4.6) to include 
negative index values; that is,
o 
N-l
Xr(z) = X Xnz" = £x.mzm = X(z-’) 
(6.5)
n=-(N-l) 
m=0

HR Filter Design 
137
FIGURE 6.1
Filtering without phase shift via time series reversal. In both cases, the overall gain is real and 
equal to 
.
Thus, reversing a signal vector is equivalent to substituting z for z”1 in its 
z-transform. An obvious corollary to this result is that reversal is its own 
inverse, that is, (X'(z))r = X(z). Furthermore, reversal is distributive, that is, 
(X(z)H(z)|r = Xr(z)H'(z).
Now consider the two equivalent operations in Figure 6.1. In each case, 
the "Reverse" labels mean that the output of the box is the reversed input 
vector. When we apply (6.5) and its corollaries to either of the two operations, 
the result is the same:
Y(z) = {Xr(z)H(z)}rH(z) = X(z)H(z-1)H(z) = X(z)|H(z)|2
(6.6)
Y(z) = {{X(z)H(z)}'H(z)}r = X(z)H(z)H(z-1) = X(z)|H(z)|2
Thus, the overall gain in Figure 6.1, |ff(e<<“7')|‘, is real and equal to the squared 
magnitude of the gain of the filter used twice in each operation. In this 
limited sense, linear and zero phase shift are realizable with all digital filters, 
including HR filters.
6.3 Butterworth Filters
In this and the next section, we introduce two of the most commonly used 
HR filters, the Butterworth and Chebyshev filters. These filters were invented 
in the form of continuous filters long before the advent of DSP. Each is 
optimal in a certain sense. Our purpose here is first to describe the lowpass 
version of each type of filter and then to show how to convert these to digital 
lowpass HR filters that are optimal in the same sense.
We begin with the Butterworth filter. The power gain of the ideal, contin­
uous, lowpass version of this filter is given by
|H(/te)|2 =
1
l + (jco/ja)D2L 
(6-7)

138
Digital Signal Processing with Examples in Matlab
where co is frequency in rad /s, and a>c is the cutoff frequency at the end of 
the passband, also in rad/s. The maximum power gain of this ideal filter is 
H(/0)|‘ = l,and the power gain at cutoff is by definition |H(/cuf)|2 = 1/2 (unlike 
the FIR filters, with amplitude gain = 1/2 at cutoff).
The filter is named for S. Butterworth.14 Butterworth was able to show 
that, of all possible rational functions of co of order L, (6.7) has "maximum 
flatness" in the passband, that is, a maximum number of vanishing deriva­
tives of the power gain function at Ct) = 0, and is thus optimal in this sense. 
Using eg. = 1, examples of (6.7) for the first three even values of L, each 
illustrating the maximum flatness property, are shown in Figure 6.2.
When we use s in place of jo) to get the Laplace transfer function, (6.7) 
takes one of two forms depending on whether L, the order of the filter, is 
odd or even. To simplify (6.7) as well as the rest of our discussion, we will 
assume L is even at this point. With this assumption, we have
|H(s)|2
1
1+(s/M)2L
1 .
1 + (s/0)c)
L even
(6.8)
A unique set of poles of this expression on the s-plane is given by
(6-9)
These are spaced evenly around the circle with radius cq. on the s-plane. The 
placement of poles for the first three even values of L, corresponding with 
the gain curves in Figure 6.2, is shown in Figure 6.3.
Frequency (rad/s)
FIGURE 6.2
Butterworth power gain curves with ci>c = 1 for the first three even values of L.

FIGURE 6.3
Butterworth power gain poles on the s-plane for the first three even values of L; (oc = 1.
For continuous filters, we will see shortly that the stability requirement 
for continuous systems implies that the s-plane poles must all be to the left 
of the imaginary axis, that is, on the left half-plane. Because |H(s)|" in (6.8) 
is equal to H(s)H(-s), we may therefore specify the stable Butterworth trans­
fer function by using only the left half-plane poles in (6.9), that is, poles with 
angles that lie in the range
71 (2n-l)zr 3 zr
2 < 2L < T'
or L < 2h - 1 < 3L
(6.10)
Because we have assumed L is even, this means that the L values of n used 
in (6.9) to give the left-plane poles of H(s) must be given by
n = | + 1, | + 2,...,y, or m = n - | = 1,2,...,L (6.11)
Thus, using m in place of n in (6.9), the poles of the stable lowpass Butter­
worth filter are at
s,„ = o)ce 2L ; m = 1,2,.,.,L 
(6-12)
For L = 2, for example, the pole angles of H(s) are 3zr/4 and 5zr/4, in agreement 
with Figure 6.3.
When L is even, as we have been assuming, we can see that the poles of 
H(s) are in conjugate pairs; that is, poles sm and sL+1_m are conjugates. There­
fore, we can express H(s) in cascade form as follows:
H(s) = H,(s)-H2(s)........HL/2(s); 
(6.13) 

140
Digital Signal Processing with Examples in Matlab
in which the transfer function of each section, normalized so that H(0) - 
1, is given by
Thus, in (6.13) and (6.14), we have the transfer function of a continuous 
lowpass Butterworth filter of order L with cutoff at O)c rad /s. We will see 
that this form with (£>c = 1 is convenient when we transform H(s) to construct 
an equivalent digital Butterworth filter.
As we noticed in Figure 6.2, the Butterworth power gain curve becomes 
more rectangular as L increases. It is not difficult, in fact, to determine L 
based on this sort of criterion. The traditional parameters described in the 
literature1 11 specify two power gain limits. A simplified method that assumes 
the power gain is 0.5 (-3 dB) at the cutoff frequency, a>c, is based on one's 
choice of the parameter A, which is illustrated in Figure 6.4. Just as the 
passband is defined to end at mc, the stopband is defined to begin at <ys, where 
the power gain of the lowpass filter is specified to be at or below 1/(1 + A2) 
at all frequencies above os. The steepness (and hence the rectangularity) of
Frequency <b (rad/s)
FIGURE 6.4
Passband and stopband specifications in terms of power gain.

HR Filter Design
141
the gain curve then depends on the interval from 0)c to cy, and, when this 
interval is specified, on one's choice of A. Larger values of A result in greater 
steepness.
If we now substitute a>5 for a> in (6.7) and use the specifications just 
described for the stopband, the result is
 
5 =  
or L = ; —- 
(6-15) 
1+A------l+(/<os//wc)--------------------logw(ios/CD£)
Furthermore, practical values of A are usually 10 or more, so we may approx­
imate the maximum stopband gain with 1/(1 + A2) ~ 1/A , that is, -20 logwA 
dB. Using this approximation in (6.15), we can specify the filter order (L), 
that is, the number of poles, needed to meet given passband and stopband 
specifications:
Necessary number of lowpass analog Butterworth poles: 
p > ^maxlmum stopband gain in dB)
' 
-20 log1D(tas/<oc)
(6.16)
6.4 Chebyshev Filters
Chebyshev filters are named for P. L. Chebyshev, who discovered a unique 
set of polynomials that may be described in either of two ways:13
^o(x) = 1
V^x) = x
i (6.17) 
: 
VL(x) = cos(Lcos x)
V\(x) = 2xIZl_j(x) - Vt_2(x); L > 2 
= cosh(Lcosh-1x)
The equivalence of these two forms of VL(x) may be shown13 by using the 
series expansions for cos(LfT) and cosh(Ld) in the right-hand formulas.
Chebyshev was able to show that, of all polynomials of degree L with 
leading coefficient equal to one, the polynomial VL(x) has the smallest max­
imum magnitude in the interval |x| < 1. This magnitude is, in fact, one. It 
happens that there is a simple way to map the interval |x| < 1 to either the 
passband or the stopband of the Butterworth power gain function, and 
thereby create a power gain curve with this maximum-magnitude or equal­
ripple property in either of these two bands. Without going into detail, the 

142
Digital Signal Processing with Examples in Matlab
result for the two types of lowpass Chebyshev filters, each of which is a 
simple modification of (6.7), is
Typel: 
r-r—: 
(6.18)
1 + e VL(j(0/)mc)
Type 2: |H(/a>)|2 = ----------------- 1----- ------------ (6.19)
1 + V?(;a>s/;c9c)/VE(;<»s//®)
The equal-ripple property of the Chebyshev polynomial is translated to the 
passband in the type 1 filter and into the stopband in the type 2 filter. In type 
1, £ determines the amplitude of the passband ripple. In type 2, as for the 
Butterworth filter, the power gain is by definition equal to 1/2 at coe rad/s, 
and there are no parameters other than or and <»s.
Tire varying passband gain of the type 1 filter is undesirable in most 
applications, and thus, the type 2 filter is usually chosen. We will therefore 
focus on the design of the type 2 filter, calling it simply the Chebyshev filter 
from here on, and beginning with the derivation of its poles. (The derivation 
of the type 1 poles is similar and actually easier.)
The s-plane poles of the Chebyshev filter are similar to the Butterworth 
poles, but they are located on an ellipse rather than a circle. To find the poles, 
it is convenient to substitute s = -tosa>c/(7into (6.19), thus transforming from 
the s-plane to the cr-plane, and also to define the constant f = 1/VL(<os/tq.). 
With these definitions, (6.19) may be written
|H(;w)|2 =
fvlWj(oc)
i + ^vzL(a/jac)
(6.20)
To derive the pole locations on the o-plane, we set the denominator in (6.20) 
equal to zero and substitute the "cosine" definition in (6.17). The result is
VL(cr/;wc) = cos(Lcos’1(a/;to(.)) = ±j/£ 
(6-21)
Now the inner term, cos_1(<7//t»c), is, in general, complex. Let us represent 
this term with p + ja and then solve for cr:
cos^fo/ytoj = p+ja;
cr = jcoccos(j8 + ja) 
(6.22)
= ct>c (sinha sin j9+/ cosh o! cos/3)
If we substitute the first line of (6.22) into (6.21), we will be able to solve for 
a and p, and thus for the pole locations in terms of cr:
VL(cr//ci)c) = cos(L()3 + ja)) = cosLP coshLa - js'mLp sinhLcc - +j/(j
(6.23)

HR Filter Design 
143
Because the real part of this expression is zero, we must have cos L/3 = 0, and 
therefore,
A, ~ {2’l2L1)E' n = l,2,...,2L 
(6.24)
Note that these values of /3 are the same as the Butterworth pole angles in 
(6.9). Next, using these values of p and equating the imaginary components 
in (6.23), we have sinL/3 = ±1, and therefore,
sinhLa=±l/£, or a = ±isinh-1(l/£) 
(6.25)
Using (6.24) and (6.25) in (6.22), and switching to index m in a manner similar 
to (6.11), we can express the right-heclf cy-plane poles of the lowpass Chebyshev 
filter as follows:
e>„, - cn-fsbaha cos ft,, + j cosh a sin/lj;
1 . ,-i 1 
o (2ffl-L-l)7T 1 
. 
(6.26)
a = -sinh 
p„, = ±m = 1,2,...,L
Now we may return to the s-plane, again using s = -0),xoJo and noting that 
the minus sign will map the right half of the enplane to the left half of the 
s-plane. Thus, we have a complete definition of the Chebyshev poles. The 
poles of the power gain function, that is, with m = I through 2L, are plotted 
in Figure 6.5 with, <nr = 1, 
— 2, and L = 2, 4, and 6 for comparison with the
Butterworth poles in Figure 6.3. In this case, the poles are on an ellipse with 
a shape that depends on a, and ®,.
The power gain function (6.20) also has zeros on the cr-plane that must 
be mapped to the s-plane. These are easier than the poles to find, because 
the inverse cosine is not complex. Setting the numerator of (6.20) equal to 
zero, we have
VL(cr/;a»c) = cos(Lcos’I(o,//<i)c)) = 0 
(6.27)
L=2 
L=4 
L=6
Refs)
FIGURE 6.5
Chebyshev power gain poles on the s-plane with ojc -1 and co. = 2 for the first three even values 
of L

144 
Digital Signal Processing with Examples in Matlab
In this case, the cos argument may be any odd multiple of rc/2. The result is 
the following expression for the roots of (6.27), all of which are on the 
imaginary axis of the cr-plane:
<r„, = ;Tjccos^2,,,'2-1 — j; n = l,2,...,L 
(6.28)
As before, we transform these using s = -a>scq./o, and thus obtain the zeros 
of the lowpass Chebyshev filter, which, we note, are ail on the imaginary 
axis of the s-plane.
Thus, beginning with (6.19) and ending writh (6.28), we have derived the 
poles and zeros of the type 2 Chebyshev analog lowpass filter. Although the 
derivation is complicated compared with the Butterworth derivation, the 
result is easily summarized in Matlab language:
% Poles.
zeta=l/cosh(L*acosh(ws/wc) ) ;
alpha=(1/L)*asinh(l/zeta);
beta=(2*(1:L)-L-l)*pi/(2*L);
sigma=wc*(sinh(alpha)*cos(beta)+j *cosh(alpha)*sin(beta)); 
poles=-wc*ws./sigma;
% Zeros.
sigma=j *wc*cos((2*(1:L)-1)*pi/(2*L));
zeros- wc’w::. /sigma;
(6.29)
Using these expressions, which locate poles and zeros in the left-half plane, 
all poles for a>c - 1, a>s = 2, and L = 2, 4, and 6 may be calculated for the plot 
in Figure 6.5. The zeros, which are on the imaginary axis above and below 
the unit circle, are not plotted in Figure 6.5.
When L is even, as we assumed for the Butterworth filter, the poles and 
zeros are in complex pairs, and the lowpass Chebyshev transfer function may 
be expressed in cascade form as follows, in a manner similar to (6.13) and 
(6.14), in which pm and z„, are the poles and zeros just described.
ti(s) = tids) H2(s).......
J-} (s) — ^f>|(5 ~~ 
~
zm(s~Pm) z'm(S-p'n) 
(6.30)
= NV + N2) 
. m = j 2 
-
|zm|2(s2-2sRe{pm} + |p„,|2)' 
' ' '2
Note that the transfer function is normalized in this case so that Hra(0), and 
therefore H(0), is equal to one, that is, the filter gain is one at <a = 0.

HR Filter Design
145
On account of the stopband ripple, it is important to have a formula like 
(6.16) for the Chebyshev filter so the maximum stopband gain may be spec­
ified. Using the same parameter, A, illustrated in Figure 6.4, we can write a 
Chebyshev power gain expression similar to (6.15):
1
(6-31)
1
1+A2 ~ 1 + vl(jas/j(Dc)/V2LW
Noting from (6.17) that V2(l) = 1 and using the "cosh" representation of 
we conclude
A = Vl(®s/<dc) = cosh(Lcosh l(fi)s/tt)c)), or L = —— (6.32) 
cosh (<o5/<oc)
To manipulate this into a rule like (6.16), we may use the identity cosh *A = 
loge( A + ./A2 - 1) with stopband gain dB = 10 log10(l/(l + A2)). If we again 
assume A is large (say 10 or more), these become cosh A = logt,(2A) and dB ~ 
-20 logwA; that is,
-1 5 ~ loS102 + 1(W - lo8’02 - dB/2° - 6-02 - dB m W 
log10e log10e 8.69
With this approximation in (6.32), we approximate the required number of 
Chebyshev poles:
Necessary number of lowpass analog Chebyshev poles: 
> 6.02 - (maximum stopband gain in dB) 
8.69 cosh1!
(6.34)
There are times when one may need to solve this expression for a)—for example, 
when the poles and zeros are found using (6.29) for a fixed value of L—in order 
to have a specified stopband gain. This is easily done and the result is
a>s = cosh 6.02 - (maximum stopband gain in dB)
8.69L
(6.35)
This result was used in Figure 6.6 to plot lowpass Chebyshev power gain 
curves for comparison with the Butterworth curves in Figure 6.2. In each 
case, the maximum stopband gain was set to -40 dB, and 0)s was then 
determined with (6.35) and used in (6.19) to find the power gain.
Another comparison of Chebyshev and Butterworth gains is shown in 
Figure 6.7. Here, the maximum stopband gain was set to the bottom of the 
plot, and in each case, we can note that the Chebyshev gain is more rectangular.
With the analog lowpass Butterworth and Chebyshev filters described, our 
next steps are to describe the translation from lowpass to other analog pass­
bands, and then the conversion to digital filters with optimal characteristics.

FIGURE 6.6
Chebyshev power gain curves with a>c = 1 for the first three even values of L. In each case, the 
maximum stopband gain was set to -40 dB and (6.35) was used to find a>s.
FIGURE 6.7
Comparison of Butterworth (B) and Chebyshev (C) power gain characteristics for the first three 
even values of L.

HR Filter Design 
147
b.t> trequency Translations
Jn the previous chapter, we transformed lowpass FIR filters into highpass, 
bandpass, and bandstop filters by adding and subtracting lowpass impulse 
response vectors to obtain the appropriate weight vectors. To transform the 
analog lowpass filters in this chapter, we use a different technique that 
accomplishes the transformation in the frequency domain.
The three frequency translations from lowpass to highpass, bandpass, and 
bandstop are accomplished by substituting three different functions of s for 
s in the lowpass Butterworth or Chebyshev transfer function, H(s). The three 
substitutions for s, along with the corresponding substitutions for a> (found 
by setting s = jco) are as follows:
Lowpass to highpass:
2 
.2
a>. 
~o)c 
.
s<-—; 
-------- (6.36
SO)
Lowpass to bandpass:
2 
2
s +at1w2 (0-0)10)1 
..
s <----------—■; ax--------------- - with co. = a)2 - ®i 
(6.37)
s 
o> 
'
Lowpass to bandstop:
2 
2
SO). 
0)0). 
,
s<— ------—; ox---------------- with 0)c = ah-o)i 
(6.38)
s + cij 0)2 o)t ox 0)
Each of these is illustrated in Figure 6.8 using ideal amplitude gain functions. 
In each case, without much effort, you can see how the substitutions (6.36) 
through (6.38) translate the lowpass passband from -O)c to 0)c into the pass­
bands of the transformed gain functions. For example, the lowpass frequency 
sequence from -cq. to 0 to ex is translated to the highpass range from O)c to 
oo to -oo to -O)c. Notice especially how the lowpass cutoff frequency, ac, 
becomes the bandwidth of the bandpass and bandstop filters.
Now suppose a lowpass transfer function, H(s), is in the cascade form of 
either (6.14) for the Butterworth filter or (6.30) for the Chebyshev filter. This 
will turn out to be the most useful form to transform from analog to digital. 
Then, if we apply the substitutions (6.36) through (6.38) to one of the factors 
of H(s), we get the terms shown in Table 6.1 for each of the other three filter 
types. In Table 6.1, we omit the index (m) in (6.14) and (6.30) for brevity.
In each of these translations, there is an inherent conversion of lowpass zeros 
and poles into highpass, bandpass, or bandstop poles and zeros. The translated 
zeros and poles may be derived by finding the roots of the numerators and 
denominators, respectively, in Table 6.1. This task is accomplished by two 
Matlab functions to be discussed in Section 6.7. The function outputs are the 
weights of the target analog filter. We may note here, however, that the

148
Digital Signal Processing with Examples in Matlab
Frequency Translation of One Component of H(s) in Cascade Form
TABLE 6.1
Butterworth
Chebyshev
Lowpass
p 
S-p
P(s -z) 
z(s-p)
Highpass
s
2, 
S - (i)c/Z
s - afcfp
s - o?c!p
Bandpass
ps
p(_S! - ZS + (0,01,)
s2 - ps + atxo)2
z(s2- ps +
Bandstop
S2 + (Wq
s! - a^s/z + 6j:<u,
s2 - cu’s/p + a>i<u2
s2 - o2s/p +
FIGURE 6.8
Illustration of the analog frequency translations in (6.36) through (6.38).
quadratic forms in the translations, where they appear in the bandpass and 
bandstop translations, cause the numbers of poles and zeros to double in the 
target filter. This and other translation features are illustrated in Figures 6.9 
and 6.10, using specific examples of all six transformations in Table 6.1.
At this point, we have complete descriptions of analog Butterworth and 
Chebyshev filters with all four passbands. Our next step is to convert these

FIGURE 6.9
Butterworth poles on the s-plane translated from lowpass to highpass, bandpass, and bandstop 
configurations using (6.36) through (6.38). Parameters for the lowpass poles at upper left are 
L = 10 and a>c = 0.32. Also, = 0.32 (and hence, <o, = 0.64) for the bandpass and bandstop 
translations.
to comparable digital filters. For this operation, we need one last translation. 
This is the bilinear transformation discussed in the next section. After that, we 
will be able to form a complete picture of HR digital filter design.
6.6 The Bilinear Transformation
The bilinear transformation, described by Kaiser and others,11'12 is used to 
transform a continuous analog transfer function, H(s), into a discrete digital 
transfer function, H(z), and thus transform an analog filter into a digital 
filter. We will see that the power gain of the resulting digital filter approx­
imates, and in one sense actually improves, the power gain of its analog 
counterpart, that is, any of the filters discussed in Section 6.5.
The bilinear transformation is accomplished simply by substituting a func­
tion of the discrete transform variable, z, for the continuous transform variable, 
s, to map points on the z-plane to points on the s-plane, and vice versa, that 
is, substituting a function of s for z to map from the s-plane to the z-plane.

FIGURE 6.10
Chebyshev poles and zeros on the s-plane translated from lowpass to highpass, bandpass, and 
bandstop configurations using (6.36) through {6.38). Parameters are the same as for Figure 6.9.
The mappings are as follows:
z-plane to s-plane: 
s-plane to z-plane:
(6.39)
take the
(6.40)
Hie geometry of this transformation is easily understood if we 
squared magnitude of the right-hand expression with s = a + jo:
|zp - 11 + g + 
_ (1 + a)1 2 + <u2
1. Tire left half of the s-plane is mapped to the interior of the unit 
circle on the z-plane.
2. The ja> axis on the s-plane is mapped to the unit circle on the 
z-plane.
|l-ct-/®|2 
(l-a2) + w2
Here, we can see that if a, the real part of s, is negative, then |z| is less than 
one, and also if a is zero, then |z| = 1. In other words, in the bilinear trans­
formation from the s-plane to the z-plane,

HR Filter Design
151
These are the essential properties of the bilinear transformation, which is 
also the simplest mapping function that produces these results.
Now suppose we convert a lowpass analog filter with transfer function 
Ha(s) to a lowpass digital filter with transfer function H(z) using the bilinear 
transformation with the properties just described. What happens to the filter 
gain in the transformation? The answer is found simply by expressing the 
gain function. First, from the definition (6.39), we have
H(z) = 
(6.41)
From this, we can write the frequency response (gain) by substituting z = e’mT = 
e,7nv into (6.41). We will use the latter form, e7 , in order to emphasize that 
H(e'2"v) is the digital gain:
z /2rtV q \ 
z JKV
- MW; = MWW = ^O'tan(Tiv)) (6.42) 
V + V '■e + e' '
Thus, the gain of the digital filter is the same as the gain of the analog filter, 
but at different frequencies. We also have from (6.42) a more explicit state­
ment of rule 2 above; that is,
Under the bilinear transformation, points given by e'21" in the range 
0 < v < 0.5 on the unit circle on the z-plane map to corresponding points 
given by co = tan(itv) in the range 0 < co < °° on the/co-axis on the s-plane.
We recall from Chapter 4 that, with digital filters, the usual frequency range 
is from zero to half the sampling rate, that is, 0 < v < 0.5. But now when the 
frequency goes through this range in the digital filter, we see in (6.42) that 
the frequency in the analog filter, that is, tanrcv, goes through the range [0, <>=].
The effect of this is illustrated in Figure 6.11, in which power gain rather than 
dB is plotted in order to illustrate the effect clearly. In the case of a lowpass 
filter, the critical frequency is the cutoff frequency, vc. [The power gain at cutoff 
for the Butterworth and type 2 Chebyshev filters is |H(;ioc)p - 1/2, as shown 
in (6.7) and (6.19), respectively.] In Figure 6.11, we see that if the analog lowpass 
filter is designed to cut off at oy = tanfirv,.) rad/s, and if this analog filter is 
converted to a digital lowpass filter using the bilinear transformation, then 
the digital filter will cut off at vc Hz-s. Figure 6.11 simply illustrates the prop­
erty of the bilinear transformation shown in (6.42), that is, the gain of the 
analog filter at tan(jrvc) rad/s is the same as the gain of the digital filter at v, 
Hz-s, where vc is any critical frequency in the range [0, 0.5] Hz-s.
Another important effect of the bilinear transformation, which is also 
illustrated in Figure 6.11, is the mapping of analog filter gain in the range 
tan(nvc) < cd < co to digital filter gain in the range vc < v < 0.5. As we can 
see, this part of the mapping warps the analog filter gain in a way that

In summary, we can see that the design of an HR lowpass digital filter with 
cutoff at a>c rad/s may be accomplished in three simple steps:
Digital Signal Processing with Examples in Matlab
Analog
Digital
FIGURE 6.11
Illustrating the effect of the bilinear transformation. The analog filter on the left is transformed 
to the digital filter on the right.
improves the digital filter gain, because the digital filter gain goes to zero 
at v = 0.5.
Mapping frequencies in this way also affects the rules in (6.16) and (6.34) 
for the necessary number of lowpass poles. Each of the critical frequencies 
must be mapped as just described, and so the equivalent rules for lowpass 
digital filters are
(6.43)
(6.44)
1.
2.
3.
Determine the number of poles using (6.43) or (6.44).
Design the analog lowpass filter, Hx(s), to cut off at tan(>TVc) rad/s. 
Transform HA(s) into H(z) with the bilinear transformation, that is,
(6.45)
0 
vc n/T
Frequency v (Hz-s)
tan (rwc) 
Frequency w (rad/s)
0.5
Necessary number of lowpass digital Butterworth poles:
> (maximum stopband gain in dB)
-20 log10(tan(ffvs)/tan(n:vc))
Necessary number of lowpass digital Chebyshev poles:
> 6.02 - (maximum stopband gain in dB)
8.69 cosh-1(tan(7rv's)/tan(7rv.))

HR Filter Design 
153
Lowpass analog filters may be transformed into highpass, bandpass, or 
bandstop filters as described in the preceding section. Therefore, these same 
rules are applicable to the design of all four types of IIR digital filters, 
provided all critical frequencies of the digital filter are transformed in accor­
dance with (6.42), that is,
<oc = tan(rrvc)
to, = tan (/tv,) 
(6.46)
to, - tan(Jtv2)
Thus, the rules in (6.45) may be expanded to include the design of all four 
types of filters:
To design an IIR digital filter:
1. Determine a>c, to,, and tOj, as needed using (6.46).
2. Determine the number of lowpass poles using (6.43) or (6.44).
3. Design the analog lowpass filter, Hx(s), to cut off at toc rad/s.
4. Translate Hx(s) if necessary, as shown in Table 6.1.
5. Transform Hx(s) into H(z) with the bilinear transformation, that is,
H(z) =
(6-47)
It is also possible, and just as easy, to exchange steps 5 and 4 and perform 
the frequency translations in the z-plane.16 In the next section, we describe 
the process (6.47) in more detail. 
----------------------------------------------------
6.7 IIR Digital Filters
The ability described in Chapter 1, Section 1.7, to write functions that 
perform routine low-level tasks and to use these functions in higher-level 
functions, is a feature that gives Matlab and similar high-level program­
ming languages a great advantage over older programming languages, 
where defining and using functions were more difficult and restricted. We 
can take advantage of this capability to develop modules for the design 
of IIR digital filters. For example, expressions (6.43) and (6.44) for the 
necessary number of poles are easy enough to calculate without writing 
a program, but on the other hand, a Matlab function also is easy to 
implement and use in other programs and has the advantage of one-time 
debugging. For example, the function may easily be written to make L 
even, so poles will be in conjugate pairs, and disallow obvious errors in

154 
Digital Signal Processing with Examples in Matlab
TABLE 6.2
Designing an HR Digital Filter
Computation
Matlab Function
Equation
# poles
L - bw_lowpass_size(vc, vs, dB)
L = chjowpass_size(vc, vs, dB)
(6.43)
(6.44)
Analog weights 
(single-pole 
sections)
> 
[d, c] = bw_analog_weights(band, L, ivl, iv2)
[d, c] = ch_analog_weights(band, L, dB, ivl, 
w2)
(6.12), (6.14)
(6.29-30), (6.36-38)
Digital weights 
(quadratic 
sections)
[b, fl] - bilin(d, c)
(6.41)
the parameters such as specifying a critical frequency greater than half 
the sampling rate, etc. The function
L = bw_lowpass_size(vc, vs, dB)
solves (6.43) with these constraints. All functions in this text are available 
with the text.
The process of designing a lowpass HR digital filter typically begins with 
the selection of the filter type, which for now we assume is either Butterworth 
or Chebyshev type 2. Then, the process follows the course we have already 
described, which is summarized in Table 6.2.
There are many different ways to proceed from a lowpass analog filter to 
a desired digital filter, and Table 6.2 represents only one. The details of each 
algorithm (how it implements the corresponding equations) may be seen by 
examining the expressions in its complete listing. The reader may prefer to 
find or invent another set of algorithms. But, if the procedure in the table 
and the accompanying Matlab functions are used, some remarks will be 
helpful. First, the frequency arguments for the first set of algorithms are in 
Hz-s. The frequency arguments for the second set, which are meant primarily 
for internal use (by other functions), are in rad, and for our purposes, are 
usually bilinear translations using tanfirv) as in (6.42). Also, for the second 
set of functions, the analog weights are coefficients of single-pole analog 
cascade sections and are, in general, complex. Only L/2 of these are generated, 
because the other L/2 are complex conjugates. It is simpler to use single-pole 
sections this way and pass the weights on to the final operation, which applies 
the bilinear translation and generates the weights of the digital filter in 
cascade form with quadratic sections and real coefficients. Thus, b and a are 
real arrays dimensioned L/2 x 3, each row containing the coefficients of the 
corresponding quadratic filter section given in (6.1) as b(z)/a(z) with M-N= 3.
In the procedure in Table 6.2, and others like it, with bandpass and bandstop 
filters, the frequency translations and bilinear transformations combine to 
warp the skirts of the filter gain function, so that only the gain at the critical 
frequencies is accurate. Thus, it is a good idea to plot the power gain and 
decide whether to accept the design or change one or more of the parameters.

HR Filter Design
155
FIGURE 6.12
Lowpass Butterworth digital filter designed with the procedure in (6.47). Poles and zeros are 
plotted on the left and power gain on the right.
A couple of examples will illustrate this. In each case, the results of the 
Matlab expressions are the weights of the digital filter in cascade form with 
L/2 quadratic sections, each section having two zeros and two poles. Also, 
for each example, we assume that the first algorithm in Table 6.2 has pro­
vided L = 6 for the number of lowpass poles.
In our first example, we design a lowpass Butterworth digital filter to cut 
off at vc = 0.3 Hz-s using the following expressions (see Table 6.2):
[d, c] = bw_analog_weights(l, 6, tan(pi*0.3));
[b, a] = bilin(d, c);
The results are shown in Figure 6.12 in terms of a pole-zero plot and a power 
gain plot. Note that the bilinear transformation creates a zero of order L at 
z = -1, even though the lowpass analog filter has only poles. Notice also 
how the distribution of poles causes the gain to be uniformly high until the 
frequency v approaches 0.3 Hz-s, that is, e/2lrv approaches where the 
polar angle is 108°. Above this frequency, the operating point moves toward 
the six zeros at z = -1, and the gain decreases rapidly toward zero.
In the second example, with results shown in Figure 6.13, we produce the 
weights of a digital Butterworth bandpass filter with passband from = 0.3 
to v2 = 0.4 using the following expressions:
[d, c] = bio_analog_weights(3, 6, tan(pi*0.3), tan(pi*0.4));
[b, a] = bilin(d, c);

156 
Digital Signal Processing with Examples in Matlab
Hz-s
FIGURE 6.13
Bandpass Butterworth digital filter designed with the procedures in (6.47). Poles and zeros are 
plotted on the left and power gain on the right.
On account of the quadratic substitution for s in (6.37), which causes L to 
increase from 6 to 12, the bandpass filter has six quadratic sections—twice 
as many as the lowpass filter. On the pole-zero plot in Figure 6.13, notice 
how the poles cover the passband from Q; = O.6tt to <12 = 0.8^ rad, and the 
-3 dB points on the power gain curve are at v, = 0.3 and v2 = 0.4. In this case, 
there are also 12 zeros, six at z = 1 and six at z = -1.
Phase characteristics for these two examples are illustrated in Figure 6.14, 
with the passband phases signified by heavy lines. The passband phases are 
seen to be nearly linear and would be acceptable in many filtering applica­
tions. But if absolutely linear phase is required, we must revert either to an 
FIR filter or the time-reversal technique described in Section 6.2.
Instead of the two expressions in (6.48) and (6.49), one may wish to write 
a single function that computes analog weights and transforms these to 
digital weights using the bilinear transformation. Two such functions are 
provided along with the other functions in Table 6.2. These are
[b, 
= bw_weights(band, L, vl, v2)
[b, a] = ch weights(band,L,dB,vl,v2) 
(6.50)
If you examine the listings of either of these functions, you will see that most 
of the effort in composing the function is in the comments. But if your 
memory is not perfect or if the function is for use by others as well as yourself, 
it is justified.
Techniques for HR filtering, that is, using the HR digital filters in this 
chapter to filter sample vectors, have been described in Chapter 4. Any of

HR Filter Design 
157
Lowpass 
0.------
0 
0.1 
0.2 
0.3 
0.4 
0.5
Bandpass
0 
0.1 
0.2 
0.3 
0.4 
0.5
Hz-s
FIGURE 6.14
Phase characteristics of the lowpass and bandpass filters used for Figures 6.12 and 6.13. Phase 
in the passband of each filter is shown as a heavier line.
the algorithms discussed in Chapter 4, including the Matlab filter function, 
may be applied using the filters in this chapter. The Matlab filter function is 
the best choice if Matlab is being used; however, the current version of filter 
requires the direct form of H(z) rather than the cascade form preferred in 
this chapter. To implement the cascade form, the function
y = filtersfb, a, x)
may be used. This function allows b and a to be arrays, with each row 
containing the weights of one section of the cascade structure. In other words, 
filters accept weights as computed by the functions in (6.50). Internally,/liters 
simply uses filter to implement each section of the cascade structure.
The exercises contain additional examples of designing and applying HR 
filters and filtering using the techniques described in this section.
6.8 
Other Types of HR Filters
Besides the four standard types of HR filters, there is an unlimited variety 
of different transfer functions that one may wish to invent for special pur­
poses. We will give only two examples here, but in general, a useful way to 
think about obtaining a given amplitude gain or phase characteristic is to 

158 
Digital Signal Processing with Examples in Matlab
imagine how poles and zeros might be placed on the z-plane to achieve the 
desired results. The first example illustrates this process.
Suppose we need a digital resonator, that is, a digital filter that has high 
gain at a specified frequency and low gain at all other frequencies. We saw 
in Chapter 4, Section 4.5, how gain is affected by distances from the operating 
point, e,a,T on the unit circle, to poles and zeros on the z-plane. That is, the 
gain is high when the operating point is close to a pole and low when the 
operating point is near a zero. Now suppose we want a resonator that 
resonates at a>= ey. Then, it is reasonable to place a pole just inside the unit 
circle (for stability), say at radius r = 1 - £. This pole, along with its conjugate, 
would give us the transfer function
' ' z2 
z2
------------ =- = ---------- -------------; 
(6.51) 
(z-re 0 )(z-re’7“° ) z -2rzcosta0T +r
Now suppose we wish to place zeros all around on the unit circle to suppress 
the gain at frequencies away from (OqT. We could do this with
Hz(z) = l-z-N 
(6.52)
which has zeros at (l)1/w; that is, in a manner similar to (6.9),
zeros at z — ^2mlN- n = 0, 1,...,N-1 
(6.53)
We then construct the digital resonator as the cascade combination Hz(z)Hr(z) 
shown in Figure 6.15, with a constant gain A to make the gain equal to one 
at resonance, that is,
Thus, the digital resonator is a simple structure consisting of the delay section 
(6.52) followed by the two-pole IIR filter section (6.51). An example with 
N = 16, r — 0.99, and a^T = 2.5(2n/N), is given in Figure 6.16, which shows 
pole-zero and power gain plots for the resonator. Note that the pole is placed 
halfway between two zeros and just inside the unit circle at r = 0.99, so the 
gain is nearly uniform on either side of ft>0.
FIGURE 6.15
Digital resonator: cascade combination of H,(z) in (6.52), H,(z) in (6.51), and constant gain A 
in (6.54).

HR Filter Design
FIGURE 6.16
Pole-zero and power gain plots for the configuration in Figure 6.15 with N" = 16 zeros, pole 
radius r = 0.99, and resonant frequency 2.5(2rc'N) rad.
FIGURE 6.17
Comb filter configuration described by (6.51) through (6.54).
The comb filter shown in Figure 6.17 is another type of HR resonator, in 
which several units in the form of in (6.51) and (6.54) are connected to 
a single FIR delay:
H;(z) = l-rV 
(6.55)
The factor r causes the zeros to move in from the unit circle to a circle of 
radius r. Each pole of the comb filter is placed directly on a zero, and

160
Digital Signal Processing with Examples in Matlab
Hz-s
FIGURE 6.18
Pole-zero and power gain plots for the configuration in Figtire 6.17 with N = 16 zeros, pole and 
zero radii r - 0.99, and resonant frequencies at [2:5] * (2 * rad.
therefore, cancels its effect on gain, thus producing a resonant peak at the 
corresponding frequency. The result is shown in Figure 6.18~a "comb" filter 
with outputs in adjacent frequency bands.
The spectrogram is an interesting application of the comb filter or any 
similar set of bandpass filters. It is useful with nonstationary signals (sig­
nals with spectral properties that change with time), which are discussed 
in the next chapter. An example of a nonstationary signal, in this case, a 
"chirping" signal that increases in both amplitude and frequency, is 
shown in Figure 6.19. A spectrogram of the signal, consisting of the 
outputs of a comb filter with eight bands, is shown in Figure 6.20. With 
a properly constructed spectrogram like this, we can observe the arrival 
of different frequency components of a signal at different times, which 
may be very useful with certain types of data, such as acoustic or seismic 
waves.
Our second example of a different type of HR filter is the all-pass filter, 
which has constant gain at all frequencies. Suppose we place poles at p and 
p' (the conjugate of p) on the z-plane, and then, to compensate for the 
contributions of these to the amplitude gain, we place zeros at 1/p and 1/p' 
and construct the following filter:
H(z) = 
(6.56)
(z-p)(z-p) 
(z-p)(z-p)

HR Filter Design 
161
FIGURE 6.19
Plot of 300 samples of a nonstationary signal increasing in both frequency and amplitude.
Sample number
FIGURE 6.20
Spectrogram of the signal in Figure 6.19 produced using a comb filter with N = 100, r = 0.99, 
and resonances at [1:2:16]*(2 * tr/N).

162 
Digital Signal Processing with Examples in Matlab
We can see that the amplitude response is one at all frequencies:
i„. 7W.I _ Ip^-lllp'e^-ll
~ |Z1 
, 
/«TVI 1 
(6.57)
1(1 -pe )||(l-pe' )|
The operations in (6.57) were first to multiply each denominator term by 
|e“i‘aT| - J, and 
use the distributive property of the conjugate plus the
property |+^| = |£| for any complex variable or function Thus, the all­
pass filter is a filter with unit gain, and phase shift depending on the value 
of p. Allpass filter sections, each with its own value of p, may be connected 
in cascade to produce different phase characteristics. For further analysis 
of allpass phase and group-delay characteristics, the reader is referred to 
more advanced DSP texts, such as Oppenheim and Schafer.5
6.9 Exercises
General Instructions: (1) Whenever you make a plot in any of the exercises, 
be sure to label both axes so we can tell exactly what the units mean. (2) Make 
continuous plots unless otherwise specified. "Discrete plot" means to plot 
discrete symbols, unconnected unless otherwise specified. (3) In plots, and 
especially in subplots, use the axis function for best use of the plotting area.
1. Make two subplots. On the left subplot, plot the s-plane poles of 
an analog lowpass Butterworth filter having L -12 poles and cutoff 
frequency equal to 2.0 rad/s. On the right, plot the power gain 
from -200 to 0 dB vs rad/s from 0 to 10.
2. Do Exercise 1 using a Chebyshev filter. The stopband should begin 
at 5 rad/s. Plot zeros as well as poles on the s-plane.
3. Make two subplots. On the left, plot the power gains of four digital 
lowpass Butterworth filters with cutoff at 0.3 Hz-s and L = 2, 4, 8, 
and 16. Plot dB in the range [-120, 0] vs. Hz-s in the range [0, 0.5]. 
On the right, plot the same curves for the corresponding Chebyshev 
filters with stopband gain set to -100 dB.
4. Make two subplots. On the left, plot the poles and zeros of a digital 
Butterworth filter with L = 10 poles and passband from 80 to 100 KHz, 
assuming a time step of 0.005 ms. On the right, make the same plot 
for a Chebyshev filter, assuming stopband gain = -80 dB.

HR Filter Design 
163
5. Complete the following:
a. Make two subplots. On the left, plot the amplitude gain vs. 
frequency in KHz for a digital Butterworth filter with L = 10 
poles and passband from 80 to 100 KHz, assuming a time step 
of 0.005 ms. On the right, make the same plot for a Chebyshev 
filter, assuming stopband gain = -80 dB.
b. Explain the forms of the plots in terms of the pole and zero 
locations in Exercise 4.
c. Comment on the differences between the two amplitude gain 
plots.
6. Make two subplots. On the left, plot the amplitude gain vs. fre­
quency in MHz for a digital Butterworth filter with 16 poles and 
passband from 70 to 90 MHz, assuming a sampling frequency of 
300 MHz. On the right, make the same plot for a Chebyshev filter, 
assuming stopband gain = -80 dB.
7. Repeat Exercise 6, plotting power gain in the range [—100, 0] dB 
instead of amplitude gain.
8. Make two subplots. Plot the phases of the two filters in Exercise 6 
over just the passband from 70 to 90 MHz. In both plots, plot phase 
shift in the range [-10, 20] rad.
9. An analog filter has s-plane poles at -4 and —2 ± j4, zeros at +j, and 
a maximum gain of 1. Make two subplots. On the left, plot the 
amplitude gain vs. frequency in the range [0, 50] rad/s. On the 
right, plot amplitude gain vs. frequency in the range [0, 0.5] Hz-s 
for the digital filter obtained via the bilinear transformation. Explain 
how the analog gain peak at 4.0 rad/s is mapped to the digital gain 
peak at 0.42 Hz-s.
10. Generate a signal vector x with 200 samples of x(f), which consists 
of a unit sine wave at 5 KHz plus another sine wave with amplitude 
10 at 15 KHz, assuming a time step of 0.025 ms. Design a Butter­
worth digital filter with cutoff at 10 KHz and power gain down at 
least 60 dB at 15 KHz.
a. Find L, the necessary number of poles.
b. Design the filter and plot power gain in the range [-100, 0] dB 
vs. frequency in KHz. Write your prediction of the effect of the 
filter on x.
c. In a separate figure, make two subplots. In the upper subplot, 
plot x vs. time in ms. In the lower subplot, plot the filtered 
version of x.
11. Test a digital highpass Chebyshev filter by filtering a sinusoid with 
frequency equal to the cutoff frequency. Use L = 8, dB = -100, and 
vc = 0.05 in your test, and filter 200 samples of a unit sine wave. 
Plot the input and output signals together.

164 
Digital Signal Processing with Examples in Matlab
a. Comment on whether or not the filter gain is correct at the cutoff 
frequency.
b. Why are the two signals out of phase?
c. How would you estimate the approximate duration of the fil­
ter's impulse response?
12. A signal x(t) has emit sinusoidal components at 10, 20, 30, 40, and 
50 Hz. A two-second segment of x(t) is sampled at 400 samples/s. 
Make three subplots, each with frequency range [0, 80] Hz.
a. On the left, plot the amplitude spectrum, |X|.
b. In the middle, plot the amplitude gain of a digital Chebyshev 
bandstop filter designed to eliminate just the component at 40 Hz.
c. On the right, plot the amplitude spectrum of the filtered version 
of x, indicating the effect of the filter.
13. Make three subplots. On the upper left, plot the power gain in the 
range [-80, 0] dB of a highpass digital Butterworth filter with cutoff 
at 0.3 Hz-s. On the upper right, plot the poles and zeros of this 
filter. Construct a sample vector with 200 samples of the sum of 
two sine waves: x = 10 sin(27r(.l)k) + sin(2^(.35)fc); 0 < k < 199. Filter 
x to produce y. In the lower subplot, plot x and y together.
14. Construct a comb filter in the configuration of Figure 6.17. Use 
N = 100 zeros and pole/zero radius r = 0.99. Set the poles for 
resonance at frequencies [1:2:16]/N Hz-s. Make a plot similar to 
Figure 6.18, with a pole-zero plot on the left and a set of power 
gain plots on the right. Plot power gain in the range [-40, 0] dB vs. 
frequency in the range [0, 0.5] Hz-s.
15. Generate 300 samples of the chirping signal in Figure 6.19. with the 
following expressions:
k=0:299; x=(l+3*k/300).*sin(2*pi*k.*(.05+(k/300)*.06));
a. Plot the signal and observe that it is the same as Figure 6.19.
b. In a separate plot, reproduce the spectrogram in Figure 6.20 
using the comb filter in Exercise 14. Plot each waveform with 
an offset corresponding with the resonant frequency of the comb 
filter section.
16. Design a Butterworth lowpass digital filter with L = 10 poles and 
cutoff at 0.4 Hz-s. Plot the power gain in dB vs. frequency in FIz-s. 
Set the axis ranges to [-80, 0] dB and [0, 0.5] Hz-s, respectively. Then 
on the same plot, overlay the power gain of an FIR lowpass filter 
using the Kaiser window with ft = 8.0 that matches the Butterworth 
gain as closely as possible. Make the FIR plot with 50 connected 
discrete points. Choose the FIR cutoff frequency (vc) and filter length 
(N) to produce tire best match. Print your choices in the plot title.

HR Filter Design 
165
References
1. Orfanidis, S.J., Introduction to Digital Signal Processing, Prentice Hall, Upper Sad­
dle River, NJ, 1996, chap. 9.
2. Mitra, S.K. and Kaiser, J.F., Eds., Handbook of Digital Signal Processing, Wiley, New 
York, 1993.
3. Antoniou, A., Digital Filters: Analysis and Design, 2nd ed., McGraw-Hill, New York, 
1993.
4. Jackson, L.B., Digital Filters and Signal Processing, Kluwer Academic Publishers, 
Nor well, MA, 1989.
5. Oppenheim, A.V. and Schafer, R.W., Discrete-Time Signal Processing, 2nd ed., Pren­
tice Hall, Upper Saddle River, NJ, 1998, chap. 5.
6. Stearns, S.D. and Hush, D.R., Digital Signal Analysis, Prentice Hall, Englewood 
Cliffs, NJ, 1989, chap. 12.
7. Parks, T.W. and Burrus, C.S., Digital Filter Design, Wiley, New York, 1987.
8. Roberts, R.A. and Mullis, C.T., Digital Signal Processing, Addison-Wesley, Read­
ing, MA, 1987.
9. Elliott, D.F., Ed., Handbook of Digital Signal Processing, Academic Press, New York, 
1987.
10. Rabiner, L.R. and Rader, C.M., Eds., Digital Signal Processing, IEEE Press, New 
York, 1972.
11. Kaiser, J.F., Digital filters, in System Analysis by Digital Computer, Kuo, F.F. and 
Kaiser, J.F., Eds., John Wiley & Sons, Inc., New York, 1966, chap. 7.
12. Kaiser, J.F., Some practical considerations in the realization of linear digital filters, 
Proc. 3>d Ann. Conf. Circuits System Theory, 621, 1965.
13. Storer, J.E., Passive Network Synthesis, McGraw-Hill, New York, 1957, chap. 30.
14. Butterworth, S., On the theory of filter amplifiers, Wireless Engr., 1, 536, 1930.
15. Lutovac, M.D., Tosic, D.V., and Evans, B.L., Filter Design for Signal Processing 
Using Matlab and Mathematica, Prentice Hall, Upper Saddle River, NJ, 2001.
16. Proakis, J. and Manolakis, D., Digital Signal Processing: Principles, Algorithms, and 
Applications (3"* ed.}, Prentice Hall, Upper Saddle River, NJ, 1996, chap. 8.


Random Signals and Spectral Estimation
7.1 Introduction
Random signals are signals with elements that cannot be predicted or derived 
exactly from other elements. In our previous examples of discrete signals, we 
used finite data vectors or analytic signals that are either periodic or transient; 
that is, we used signals that can be described completely in simple terms.
When a signal has random components and cannot be described analyti­
cally, we may be able to describe it in terms of its statistical properties, which 
we discuss in this chapter. There are two basic sources of the most commonly 
used statistical properties. One is the amplitude distribution of the random 
signal, and the other is the autocorrelation function or, equivalently, the power 
density spectrum. The amplitude distribution is something we have not yet 
considered. The autocorrelation function is an application of the correlation 
function in Chapter 3, Section 3.2. The power density spectrum is like the 
power spectra we have been using to analyze filters, but not quite the same 
thing, as we shall see.
Before we discuss these representations of the signal, we should consider 
the assumption of stationarity that is made or implied when statistical signal 
processing techniques are applied in DSP. A stationary signal is a signal with 
local statistics that are invariant over the entire duration of the signal. Thus, 
a periodic signal is a stationary signal, but a transient signal that occurs 
locally in a long time domain is not stationary. A random signal is stationary 
if average local estimates of the amplitude distribution and the autocorrela­
tion function do not change over the duration of the signal. Figure 7.1 shows 
an example of a stationary signal with two nonstationary signals. The upper 
signal is nonstationary because its average power is steadily increasing. The 
middle signal is nonstationary because one of its components is steadily 
increasing in frequency, thus we could say that its power spectrum is changing 
with time. Only the lower signal is stationary, with unchanging distributions 
of amplitude and power.
One could argue that no process in the natural world is truly stationary. But, 
there are many situations in engineering where the assumption of stationarity
167

168 
Digital Signal Processing with Examples in Matlab
NonstfitionArv
k
FIGURE 7.1
Vectors taken from nonstationary and stationary signals. The upper signal is nonstationary, 
because its average power is increasing steadily. The center signal is nonstationary, because its 
frequency is increasing steadily. The lower signal is stationary.
can be very useful. For example, if a long recording of the signal from a noise 
source is obtained, the estimated power density spectrum, which we describe in 
this chapter, is usually the best way we have to characterize the signal.
In this chapter, we begin by discussing the amplitude distributions of 
random signals and then go on to discuss power spectral estimates.
7.2 Amplitude Distributions
Suppose we have a long recording of a signal, such as a measurement of 
temperature, pressure, radiation from a particular source, pulse rate (there 
are many examples like these), and suppose the recorded signal is stationary 
and includes random components. If the signal is digitized and processed 
using DSP techniques, we may consider two kinds of amplitude distributions,15 
that is, functions that give the relative likelihood, or frequency, of different 
signal amplitudes. (This is a different use than we have had for "frequency," 
but also a common use of the word in statistical analysis.) To analyze the 
continuous signal before it is digitized, we use the continuous amplitude 
distribution. To analyze the signal after it has been digitized and can only 
have discrete values, we use the discrete amplitude distribution.
These two cases are shown in Figure 7.2. For the continuous signal, all 
values of x(t) in the range [-7, 7] are theoretically possible. But, the digitized

Random Signals and Spectral Estimation
169
signal can only have integer values in this range; that is, each xk is the integer 
nearest x(kT). The continuous and discrete amplitudes are distributed in 
accordance with the following functions:
Continuous: p(x); J p(x)dx = 1
- 
(7.1)
Discrete: P„ - Pr{x = n}; y P„ = 1 
n =-eo
Thus, p(x)dx is the probability that x(t) is in the range [x, x + dx], and P„ is the 
probability that x takes the integer value n. The function p(x) is called the 
probability density function of x. Thus, assuming the analog-to-digital conver­
sion is unbiased, we may say that
r>i+l/2
P>. = p(x)dx 
(7.2)
In signal processing applications, the discrete probability function is often 
estimated as the frequency function, fn> which is simply the relative frequency 
of occurrence of integer n in a digitized sample vector; that is, for a digitized 
sample vector of length N,
Pn^fn
# samples equal to n 
N
(7.3)

170 
Digital Signal Processing with Examples in Matlab
Before examining important amplitude distributions, specific examples of 
p(x) and P„, a few fundamental properties may be defined in general terms. 
First, the mean value (expected value, average value') of any function of x, say 
y(x), is defined as follows:
Continuous: E[y] = j y(x)p(x)dx
- 
(7.4)
Discrete: E[y] = £ y(x„)P„
Thus, E[y] is essentially the sum of all values of y(x), each value being 
weighted by p(x). If the integral or sum in (7.4) does not converge, then y(x) 
has no mean value. Two particular mean-value functions are so important 
that they are given special symbols. First is the mean value of x, given the 
symbol Applying the continuous form of (7.4) with y(x) = x, the mean 
value of x is
/zx = E[x] = | xp(x)dx 
(7.5)
Thus, is essentially the sum of weighted values of x, that is, the “center 
of mass" coordinate of the distribution given by p(x).
The second important expected value function is the variance of x, which 
is given the symbol <T2. The variance is the most common measure of the 
variability of x(f) about its mean value, fir. It is defined as the expected 
squared deviation of x from its mean value, i.e., <r2 is the expected value of 
y(x) = (x - fix)2. Accordingly,
<TX = E[(x-/zx)2] = J (x-pt)2p(x)rfx
= x p(x)dx-2/zJ xp(x)dx + p.~\ p(x)dx 
J —og 
* — OS 
J —<x>
= E [x2] - 2JuI/< + £ = E [ x2] - px 
(7.6)
Thus, the variance turns out to be the difference between the mean squared 
value of x and the square of qT. The square root of, <TV called the standard 
deviation of x, is the standard measure of the deviation of x from its mean 
value, the measure having the same units as x.
An important related problem arising often in signal analysis is that of 
determining the probability density of a function y(x), given p(x). This prob­
lem is solved in general1 by partitioning x into segments over which y(x) is 
monotonic and viewing the product p(x)dx defined in (7.1) as the probability 
that x(t) lies between x and x + dx, that is, viewing p(x)dx as a vanishingly 
small increment of area above the sample space of x. In the simplest case,

Random Signals and Spectral Estimation
171
FIGURE 7.3
Equal probabilities shown as equal areas.
which is illustrated in Figure 7.3, y(x) is a unique, one-to-one mapping 
between the x and y sample spaces, so that p(x) and p(y) are different func­
tions, but p(x)dx and p(y)dy are the same probabilities. That is, assuming x 
and y are differentiable,
Equal areas: p(y)\dy\ = p(x)|d.r|, or p(y) = 
(7.7)
where \dyldx\ is the absolute value of the derivative of y(x). Exercises 1 and 
2 provide illustrations of this relationship.
The case where y is a linear function of x, say y - ax + b, is another useful 
example. In this case [dy/dx[ - |a|, and so p(y) - p(x)/ |a| . Two useful corollaries 
follow from this result:
Pa^b = 
+ b
J. 
1 7
(7.8)
These both follow from (7.7) and the definitions of mean and variance in 
(7.5) and (7.6), and they are the subject of Exercise 1 at the end of this chapter.
7.3 Uniform, Gaussian, and Other Distributions
Turning now to specific examples of amplitude distributions, two distribu­
tions are of particular interest in digital signal analysis. First is the uniform 
distribution illustrated in Figure 7.4. The uniform probability density func­
tion expresses the assumption that all values of x in the interval [a,b] are 
"equally likely." (This notion has real meaning only in the discrete case;

172
Digital Signal Processing with Examples in Matlab
FIGURE 7.4
Continuous and discrete uniform amplitude distribution functions.
nevertheless, it has heuristic appeal in the continuous case.) In the continu­
ous case, the sample space is the horizontal (x) axis, and the integral of p(x) 
satisfies (7.1). In the discrete case, the sample space consists of N integer values 
of x in the range |n,b], and the sum of P„ over n again satisfies (7.1). In either 
case, if the amplitudes of x(t) and its samples are uniformly distributed, x(t) 
is called a uniform variate.
Using the continuous probability function, the mean value of the uniform 
variate in Figure 7.4 is found as follows:
= £xp(x)dx = ^-£xdx = b~r 
(7-9)
Using the result in (7.6), the variance of the uniform variate is
2 nr 2, 
2 
1 2j (b + a}2 (b - a)2 
..
= E[x ] - px = —a£ x rfx- 
_ 
(7.10)
Thus, the standard deviation, ax, is 1/712 times the range of values of x, that 
is, b - a.
The second important example of an amplitude distribution is the Gaussian 
or normal probability function, illustrated in Figure 7.5. This function is of 
interest in DSP primarily, because, with many types of data (electromagnetic, 
acoustic, seismic, vibration, etc.), the amplitude distributions of random 
noise are Gaussian, or approximately so. The general form of the Gaussian 
probability density function is
p{x)=N(pr<j)- -^=e 2o? 
(7.11)
In this expression, u is the mean value, px, and o’is the standard deviation, 
m. The discrete form, shown on the right in Figure 7.5, is a discrete distri­
bution function, P„, having an envelope given by (7.11) and satisfying the 
condition (7.1).

plx) dx
rdrdG
(7.12)
o
0
-o+p = u.
(7.14)
FIGURE 7.5
Continuous and discrete Gaussian amplitude distribution functions.
I" re r dr = 1 
'o
e 'J e v dy dv =
Ml 
x
In this form, the integral can be evaluated simply by evaluating its square 
with the use of polar coordinates:
r _oa 
*12 q c<. 
2 
2 
1 .9jr 
2 
*<x> 
2
(7.13)
The demonstration that p is the mean value of x is made by placing the 
Gaussian function into the definition (7.5) and again making the substitution 
in (7.12) for x, that is, x = G t/,/2 + p:
To see that N(p, o) has a unit integral as in (7.1), we may substitute y - 
(x - p)/(o72), so the integral of p(x) becomes
f p(x)dx = —f 
oV2kj-
■^'^dx = -1 f" e~y2dy
Random Signals and Spectral Estimation
0.2/c
p-G p p+G
Continuous 
f P(x)
0.4/cs-
Thus, the use of p in (7.11) is justified. Similarly, for the variance as given in 
(7.6), we can let p = 0 and obtain a2 as the expected value of x" as follows, 
doing the integration by parts (Chapter 1, Table 1.3):
Thus, because p = px, o2 is the variance of the Gaussian distribution.
2 
x
2 rr 2, 
(°° A' 2a2 j _2
ox = E[x ] = 
—— e dx - a
J-“O.V2tt
(7.15)
.2
P
Discrete
t
I

174 
Digital Signal Processing with Examples in Matlab
Unfortunately, when the distribution of x(t) is given by N(p, o) as in (7.11), 
the probability that x lies between some pair of values [as in (7.2)] cannot 
be easily expressed as it obviously can be, for example, in the case of the 
uniform distribution. Therefore, normalized versions of this probability, 
which is called the error function (erf) are approximated in various ways. In 
Matlab, the computation is done by erf(v), which computes the normalized 
error function of each element of a vector v as follows:
erf(u) = -== f e'!/ dy. 
(7.16)
To apply erf(u) to the form of p(x) shown in (7.11) and in Figure 7.5, we again 
use the substitution used to obtain (7.12), in this case, y = (x - p)laJ2:
2 
dx „
erf(u) = — e 
—— = 2 N(u, <r)dx; or,
o\/2 
(7.17)
J" N(/z, <r)dx = | erff-^-1
2 
Vo-,/2/
Assuming x > 0, the second result gives us the probability that a normal 
variate lies in the range (jl, p + x) in terms of the Matlab function, erf.
An example is shown in Figure 7.6, which illustrates the approximately 
Gaussian amplitude distribution N(2000, 600) of a signal (x) from a 12- 
bit analog-to-digital converter. The distribution of x is discrete, but with 
212 - 4096 possible values, it may be viewed as continuous. In this case, 
because the values of x are separated by unit distance, the integral of p(x) 
and the sum of discrete probabilities (P„) must be the same. To compute
oxl04
8 r
X
FIGURE 7.6
Amplitude distribution of signal sampled with a 12-bit A-D converter. The dark area, which is 
Pr{2000 < x < 2500}z is computed as shown in (7.17) and the text.

Random Signals and Spectral Estimation
175
the dark area shown in the figure, we would use (in Matlab notation) 
darkarea - .5 * erf((2500-mit)/(sigma * st/rf (2))) with mu - 2000 and sigma = 600.
Two Matlab functions are used to produce random sequences and arrays. 
(Other computing languages have similar functions.) rand(M,N) produces an 
M by N array of uniform random variates with each element in the range 
(0,1). Similarly, randn(M, N) produces an array of Gaussian variates from the 
distribution N(0,1). These functions actually generate pseudo-random sequences 
and arrays using number-generating algorithms with very long cycles. To use 
the same random sequence, say in a test or simulation, the expressions 
rand ('seed' ,K) and randn(‘$eed',K) may be used to reset the starting point.
Given (7.8), which allows us to make a linear treansformation of any 
random variate, we can deduce the following rules for generating zero-mean 
random sequences with variance g~:
Random vectors: length = N, mean = 0, and standard deviation - g 
Uniform: x = g * 712 * (rand(l,N) - 0.5)
Gaussian: x = g * randn(l,N)
(7.18)
Two examples are shown in Figure 7.7. In each case, N = 10,000 samples, 
and each element of x is rounded to the nearest integer to simulate the output 
of a digital device such as an analog-to-digital converter. On the left is the 
frequency function of a uniform sequence with g = 4.33, which translates
FIGURE 7.7
Amplitude distributions of digitized uniform and Gaussian random sequences. In each case, 
N ~ 10,000 samples, p = 0, and <7 is shown in the figure.

176 
Digital Signal Processing with Examples in Matlab
FIGURE 7.8
Amplitude distributions that are neither uniform nor Gaussian. The amplitude distribution of a 
digitized sine wave is on the left, and the distribution of gray-scale image pixel values is on the right.
via (7.18) to equally likely integer values in the range [-7,7], On the right is 
the frequency function of a Gaussian sequence with <j= 2. In both cases, we 
note that, due to the finite value of N, the distributions are only approxi­
mately the same as the theoretical distributions.
Obviously; a sequence does not need to be random or even have random 
components in order to have an amplitude distribution. Essentially all 
sequences and arrays of samples have amplitude distributions. And 
although the amplitude distribution is used mainly in the analysis of random 
functions, it is also used with all types of functions in the analysis of coding 
and compression techniques, which are important areas of DSP discussed 
in Chapter 10. So to conclude our discussion here, we offer also Figure 7.8, 
which consists of two nonrandom examples, both with N = 216 samples. On 
the left is a sine wave with amplitude 7.5, with samples rounded to the 
nearest integer before compiling the amplitude distribution, [/„]. Note how 
the amplitude distribution is symmetric and indicates that values off(t) near 
the minimum and maximum are more likely than values near zero. The 
amplitude distribution of a continuous sine wave y(x) may be found using 
(7.7), beginning with a uniform distribution of x and transforming p(x) into 
p(y). See Exercise 2 at the end of this chapter.
On the right in Figure 7.8 is the image known as “Lena," which has been 
a standard gray-scale image in DSP literature for decades at the time of this 
publication. (We pause here to thank the subject for her notable contribution 

Random Signals and Spectral Estimation 
177
to image processing and hope the years have been good to her.) In this 
version, the image is 256 x 256 pixels, and each pixel value is on a black-to- 
white scale from 0 to 255 (.although not all pixel values are present in the 
image). The amplitude distribution in this case shows the relative frequencies 
of different pixel values occurring in the image. In both cases, the frequency 
functions sum to one in accordance with (7.1) and (7.3).
7.4 Power and Power Density Spectra
In Chapter 4, Section 4.13, we measured the power gain of a linear system 
in terms of the square of the transfer function magnitude, which in turn is 
measured by the DFT of the impulse response. Thus, we could say that the 
square of the DFT magnitude of any function x(f) is a measure of the distri­
bution of the power in x(f) over the frequency domain. Now we wish to 
pursue this idea and arrive at precise definitions of power and poiuer density.
The instantaneous power of x(f) at any time t is the squared signal magni­
tude, |x(f)|2. We assume the signal is real so that x2(t) may be used, but the 
general definition holds even for complex signals. It follows that the average 
or expected power of a stationary function x(t) is
Average power - E[x (t)] = lim — [ x (f)dt 
(7.19)
If x is a vector consisting of N samples of x(t), we say the average power in x is
Average power = 
= x VW 
(7-20)
1V 
IV i J 0
n=0
The average power in this sense is an estimate of the true average power in 
(7.19). 
'
To express the average power in terms of the DFT, we substitute the inverse 
DFT formula in (3.23) for x„ in (7.20) and write the result as follows:
(7.21)
In this result, we may substitute k = N - i, use the redundancy in (3.10) to 
note that Xo = XN, and use the redundancy in (3.11) to let XN_( = X'. Then the 
average power expression becomes
Avg. power = A £ £xmX:£e^”'-'>"'N = 
|X,/ 
(7.22)
™ 
1-0 
n«0 
™ m-0

178
Digital Signal Processing with Examples in Matlab
The result on the right follows, because the inner sum in the center equals 
N when m = i and is zero otherwise. Thus, in combination with (7.20), we 
have the result
(7.23)
This result presents average signal power in terms of the power spectrum. 
Known as Parseval's theorem, it provides an important insight and link between 
the time and frequency domains.
Our final step in the development of the notion of a power density spectrum 
is to define the periodogram. The periodogram has N components given by
Periodogram: Pxx(m) = 
m = 0,1,...,N-1
(7.24)
The periodogram is thus a real function of m, periodic in accordance with 
(3.10) and with PIX(N - m) — Pxx(m) in accordance with (3.11). Furthermore, 
using the results obtained above, we may write
N-l 
N-l
Average power = 
= 
(7-25)
H=0 
ni=0
Thus, in the same way that x2„ gives a measure of signal power at a point in 
the time domain, Pxx(m) gives a measure of signal power at a point in the 
frequency domain. Therefore, the vector P„ = [P„(nt)] is said to be a measure 
of the power density spectrum of r(f), because its average value in (7.25) is 
another expression of the average power estimate.
Power density in this form is best applied to stationary signals and images 
from which segments are extracted for analysis, as described in the next 
section. If a signal component is considered to have a definite beginning and 
end, and if the sampling process covers the entire range, then we use energy 
measures instead of power. Energy is the integral of power over time or space. 
In terms of samples, the measure of signal energy is
N-l 
N-l 
,
_ 
„ , -NT ,
Energy = T^Pxx(m) = T^x„ = x (P)dt 
(7.26)
nt=O 
11=0
Thus, the periodogram average is our measure of signal power, and the 
total periodogram is our measure of signal energy.
Examples of power and energy spectra in terms of the periodogram are 
shown in Figures 7.9 and 7.10. Figure 7.9 shows one cycle of a periodic

Random Signals and Spectral Estimation 
179
FIGURE 7.9
A sampled periodic signal and its periodogram. One hundred samples of a single cycle of x(t) 
are shown in the upper plot, and the periodogram is shown below.
FIGURE 7.10
A transient signal and its periodogram. One hundred samples of x(t) are shown in the upper 
plot, and the periodogram is shown below'.

180 
Digital Signal Processing with Examples in Matlab
function and its periodogram. The latter represents power density in the 
sense that (7.25) holds, with the average power being about 0.09 in this 
example. Figure 7.10 shows a transient signal and its periodogram. The 
periodogram in this case represents energy density in the sense that (7.26) 
holds, with the energy being approximately 11.3 in the example, assuming 
T = 1. The two periodograms are computed in the same manner, but there 
is a difference in how they are used to represent power or energy. To make 
the discussion easier, from here on, we will discuss the periodogram as 
representing power density, because this is how it is used with random signals.
When x(t) represents a time-varying physical quantity, the power spectrum 
is sometimes plotted vs. the frequency index as it is in Figure 7.9, but more 
often, it is plotted in terms of power per unit of frequency—usually power 
per Hz or power per Hz-s. In this case, a bar graph is used in place of the discrete 
plot in Figure 7.9. The bar graph is made such that, in the power spectrum,
Power in range (|m|+0.5)/N Hz-s = (area of barat m) + (area of bar at-m)
= 2(area of bar at nt) 
(7.27)
(and similarly for the energy spectrum). Figure 7.11 shows the periodogram 
in Figure 7.9 modified in this manner. That is, the power represented by each 
line in Figure 7.9 is represented by the area of the corresponding bar in 
Figure 7.11. Thus, the total power, or average power, is the integral of the 
power spectrum, and (7.25) holds in either case, because the bar width in 
Figure 7.11 is 1/N. Note, however, that if the frequency units are changed 
from Hz-s to Hz, the bar width is now 1/NT, and so the power density must
FIGURE 7.11
Periodogram in Figure 7.9 redrawn as a power density spectrum. A bar centered over v = m/N 
represents power in the range (m ± 0.5)/N, and the integral of the entire spectrum is equal to 
the average squared signal value.

Random Signals and Spectral Estimation
181
be scaled by T for the integral to agree with (7.25). The general version of 
(7.25) accommodating different familiar frequency notations is
(7.28)
To see how each spectral measure must be scaled, assume Pxx(m) is constant 
over m and equal to P. Then we can see that PrT(v) = P, Pxx(f) = TP, and P„(co) 
= TP/2n will all produce the same average power, that is, P.
7.5 Properties of the Power Spectrum
There are many examples in DSP where the source of a signal, x(f), is such 
that x(f) is stationary over a long period and contains random components. 
The power spectrum, estimated from recorded segments of x(f), often reveals 
facts about the source that we would not notice in the raw data. The power 
spectrum and the amplitude distribution together contain the basic signal 
statistics that are normally required for analysis, filtering, signal compression, 
and other types of processing. In the next section, we discuss a method for 
estimating the power spectrum of a stationary random sequence. Here we 
review its properties, the most important being its relationship to the auto­
correlation function.
Suppose a signal vector x is acquired in accordance with Figure 7.12. In 
practice, aliasing may be prevented by analog pre filtering, or simply by 
sampling at a rate greater than twice the maximum frequency response of 
the sensor. The frequency-limited signal, x(t), is sampled to produce the 
vector x consisting of integer samples. For the following analysis, we assume 
a periodic extension of x, which is reasonable when the signal is stationary. 
Then, in accordance with (3.2) in Chapter 3, the autocorrelation function of
Analog
FIGURE 7.12
Acquiring the sample vector, x, from the measurement of a continuous signal.
x=[xQ,x

182
Digital Signal Processing with Examples in Matlab
x (extended periodically) is
N-l
<M*) = 
k = 
(7.29)
i:=0
From this definition and (7.25), we notice that <pYY(k) is also periodic, and 
furthermore,
N-l 
Average power = 
= tp”(0) 
(7-30)
,i=0
We can show by the following that the periodogram is the DFT of <plt(k). 
First, using the definition (3.7) of the DFT,
DFT{<f>„(k)} = 
m = 0,1,...,N-l (7.31)
4=0 n=0
Now substitute the inverse DFT (3.23) for x„ and xn+k, using the linear phase 
shift property (Table 3.3, Property 8) on the latter (the range of m remains 
the same throughout):
DFT{<p„(k)| = 
^^^xae'2"a''/N^fXpe'2,lP(n+'r)Ne-y2’*"‘/N
N 4=0 n=0 a=0 
P-0
(7.32)
Into this equation, we substitute a - N - y. The reduction is then similar to 
the reduction of (7.21), and we obtain the final result:
N-1N-1 
N-l 
N-l
DFT{<p„(k)} = ±3£^Xfix;^e/2K(|J-w>WN£^’"0-lfl"/N
p=0 ?=0 
k~0 
n=0
= ^|xm|2 = p„(m); ffl = 0, 1......N-l (7.33)
The final result follows because, similar to (7.22), the sums over k and n are 
nonzero and equal to N only when a = y = m. Thus, we have the following 
important result:
The periodogram of a vector x with periodic extension is the 
DFT of the autocorrelation function of x; that is, 
DFT{(p„(k)} = Prr(m); m - 0,1,...,N-1
(7.34)
Our usual view of the autocorrelation function is that (p„ gives us infor­
mation on the dependence of a given sample, x„, on nearby samples, r„±1,

Random Signals and Spectral Estimation 
183
FIGURE 7.13
Signals, autocorrelation functions, and periodograms for a unit sinusoid (left) and white Gaus­
sian noise (right). The average power, is 0.5 for the sinusoid and 0.1 for the noise.
x,1±2, and so on. The periodogram, as we have seen, tells us the distribution 
of signal power, xn, over frequency. Thus, these two kinds of information 
about the signal are equivalent via (7.34). Notice also that neither <pxx nor Pxx 
contain any phase information about x; that is, the functions are not affected 
by shifting x in the time domain.
Two special examples of (7.34) are worth mentioning. First is the case 
where x is any sinusoidal component of a waveform, sampled over K cycles. 
It is easy to show in this case that «prv(/<) is a cosine function, regardless of 
the phase of x, and Pxx(m) is zero everywhere except at m = +K. In the second 
case, x is a segment of white noise, that is, a random signal with the same 
power density at all frequencies. In this case, <prr(Jc) is an impulse at k = 0, 
showing that the signal elements are statistically independent. Both of these 
cases are illustrated in Figure 7.13.
On the left in Figure 7.13, x(t) is sampled with T = 1. The autocorrelation 
function <p„(/c), plotted below x(t), is a cosine function. Note that <pxx(0') = 0.5, 
which is the average squared value of x(t). The power spectrum, plotted at 
the bottom, shows that all the power is at v = 0.1, which is the frequency of 
x(t) in Hz-s. The integral of the power spectrum, in accordance with (7.28), 
is the average power, or 0.5.
On the right in Figure 7.13, the random signal x(t) is again sampled with 
T = 1. The functions <p„(/c) and Pxr(v) are the statistical properties of the 
stationary signal from which the segment x(f) was taken, not the measured 
properties of x(t), in order to better illustrate the properties of white noise.

184
Digital Signal Processing zuith Examples in Matlab
7.6 Power Spectral Estimation
As in the preceding section, let us assume that x(t) is stationary and contains 
random components. Let us also assume that we have a recording of x(t) in 
terms of a long sample vector obtained as in Figure 7.12. From this vector, 
we wish to estimate the power spectrum of x(t). There are many applications 
like this in statistical analysis, where the characteristics of a population are 
estimated using samples taken from the population.
Having the sample vector, intuition would suggest at first that we use its 
periodogram as the estimate of the power spectrum, but this approach turns 
out to be not useful. .The reason has to do with the number of degrees of 
freedom (and thus the variance) of the periodogram [as if we had a large 
collection of periodograms of x(t), and could measure the variance of each 
component, PTy(m)]. Because, by definition, Pxx(m) is found from the DFT 
magnitude, the periodogram must always have N/2 degrees of freedom. 
Therefore, the degrees of freedom of the periodogram increase linearly with 
the segment length, and the periodogram of a long sample vector taken from 
a random function, x(t), is essentially no more reliable than the periodogram 
of a short sample vector as an estimate of the true power spectrum.
This concept is illustrated in the two upper power spectra in Figure 7.14. 
(Here, we plot only the positive half of the spectra, using connected points
FIGURE 7.14
Power spectral estimates. The first two, based on single periodograms, are not good estimates. 
The third is the average of 128 periodograms and is a good estimate of the true power density, 
which is 100 at all frequencies.

Random Signals and Spectral Estimation 
185
instead of bars on account of the large values of N.) Referring to Figure 7.12, 
for this illustration, we simulated the sampling, 12-bit conversion, and scaling 
to units of volts (v) of a white random signal with average power equal to 
100 v2. The upper periodogram is based on 216 samples, and the center period­
ogram on 29 samples of the signal. Both periodograms are essentially useless 
estimators of the true power spectrum.
On the other hand, the periodogram at the bottom of Figure 7.14 gives an 
estimate of the true power spectrum that is quite accurate. It was constructed 
by partitioning the same segment with 216 samples into 27 successive, non­
overlapping segments, each of length 29, and by then averaging these to 
produce an average periodogram. We can see in this case that the power density 
is approximately 100 v /Hz-s at all frequencies.
Thus, if we have a recorded segment of a stationary function with random 
components, our method for obtaining a useful estimate of the power spec­
trum consists of partitioning the recorded segment into smaller segments 
and averaging the periodograms of the smaller segments. It even helps to 
overlap the smaller segments. At first, it may appear that overlapping does 
not improve the estimate, because samples are re-used. But although the 
samples are re-used, overlapping improves the estimate, because it allows 
more segments in a given recording, and even though the segments overlap, 
each segment is different.
This partitioning concept is illustrated in Figure 7.15. At the top, x(JcT) is 
a plot of N = 1024 samples of a stationary random signal. The segments
x(kT) 
-
X2 -
x, ■
x3 -
0 
128 
256 
384 
512 
640 
768 
896 
1024
k
F1GURE 7.15
Partitioning N = 1024 samples of a recorded random waveform into segments of length NI2 = 512. 
Segments Xj-Xj illustrate two nonoverlapping segments, and segments X]-x3 illustrate five segments 
with 75% overlap.

186 
Digital Signal Processing with Examples in Matlab
and x2 in the middle of the figure, each with 512 samples, are obtained 
simply by partitioning x into two halves. The segments xt through x5 at the 
bottom, each again with 512 samples, are the result of applying a rectangu­
lar, 512-sample window to x, with the window shifting 128 samples for each 
successive segment. Thus, the figure represents three choices for power 
spectra] estimation using the same recorded data, and these choices may 
be rated as follows:
Worst: Periodogram of x
Better: Average periodogram of two independent segments 
Best: Average periodogram of five overlapping segments.
Even though the overlapping segments contain redundant data, no two seg­
ments are alike, and a larger averaged number of periodograms means a 
better estimate.
The average periodogram method for estimating the power spectrum of a 
stationary waveform has been described by Welch.16 Welch suggested using 
half-overlapping segments. One reason for this choice is that more overlap, 
although it helps, does not produce proportionately less variance in the 
spectral estimate, and half-overlapping is a good compromise. A second 
reason is that an FFT is required for each segment, so more overlap translates 
into more computing. However, the second reason is not as important as it 
was in the "old days" when computers were orders of magnitude slower 
than they are now. More overlap, even to the point where the window moves 
only one sample at a time, generally produces a better spectral estimate. The 
accuracy of spectral estimation is discussed in the literature.11-21
This principle is illustrated using N = 1024 samples of filtered white Gaus­
sian noise, generated as shown in Figure 7.16. The input signal vector, x, is 
a record of white Gaussian noise with zero mean and unit standard devia­
tion, in other words, a recording from a signal having unit power and unit 
power density in accordance with (7.6). The true power spectrum of the 
bandpass filter output, y, is therefore the same as the power gain of the filter.
Three power spectral computations are shown in Figure 7.17. The true band­
pass power spectrum is plotted as the heavy line in each case. Each spectral
White Gaussian noise 
Bandpass
x = N(0,l) 
filter
FIGURE 7.16
Illustration showing how the signal (y) is produced for the examples of power spectra in Figure 
7.17. The input (x) is white Gaussian noise with /i = 0 and <7=1, that is, unit power density. 
The power spectrum of y is therefore the squared amplitude gain of the bandpass filter.

Random Signals and Spectral Estimation 
187
FIGURE 7.17
Power density, Py(v), computed from 1024 samples of the signal y in Figure 7.16. In each plot, 
the segment size was 64 samples, and the window was rectangular. The segment overlap, and 
hence, the number of segments (Ns), is the only change from one spectrum to the next. The 
heavy line in each case shows the true power spectrum.
computation was made with the same record (t/) having length N -1024, 
using 64-sample segments and a rectangular data window. Only the overlap 
is changed, and we can see that the accuracy seems to improve with the 
degree of overlap. But we also note that the number of segments, and thus 
the number of FFTs, only approximately doubles as we go to 50% overlap, 
whereas this number increases by another order of magnitude as we go to 
95% overlap. Thus, the choice of overlap becomes a matter of judgment, 
depending on the factors we mentioned.
We should also note that the foregoing is based on the idea of getting the 
most we can out of a fixed signal record of fixed length, such as the recording 
from an instrument, or telemetry, of a source that has finite duration. If an 
unlimited length of the stationary random signal is available, then it is always 
preferable to use independent, nonoverlapping segments.
Another matter of judgment concerns the selection of the segment size, 
again assuming a recording of fixed length. Obviously, a shorter segment 
length means more segments and a more accurate spectral estimate. On the 
other hand, if the segment length is K, then the frequency resolution is 1/K 
Hz-s, and so a shorter segment length also means less resolution. On account 
of this tradeoff, the value of K becomes a matter of choice. The effect of 
reducing K in order to get a better spectral estimate is illustrated in Figure 7.18, 
which again is produced using a signal y as produced in Figure 7.16. In this

188 
Digital Signal Processing with Examples in Matlab
FIGURE 7.18
Power density, Ps{v), computed from 1024 samples of the signal y in Figure 7.16. fn each plot, 
the segment overlap was 95%, and the window was rectangular. The segment size (K) is the 
only change from one spectrum to the next. The heavy line in each case shows the true power 
spectrum.
example, the overlap was 95% in all three cases, K was reduced from 64 to 
32 to 16, and we can see the improvement in the spectral estimate.
The illustrations in Figures 7.17 and 7.18 are random in the sense that 
examples like these will be quite different for different random sequences 
due to the short record length of only N = 210 samples (compared with 216 
samples used for the lower plot in Figure 7.14). In other words, the examples 
in Figures 7.17 and 7.18 illustrate the concepts of improving spectral esti­
mates, but they are not "typical" in any other sense.
7.7 Data Windows in Spectral Estimation
In Chapter 5, Section 5.4, we used window functions to modify the impulse 
response of an ideal FIR filter in order to obtain a smoothed version of the 
filter's power gain function. In the same way, the application of a window 
function to a random signal segment should, in general, result in a smoothed 
periodogram. Windows are often used for this purpose in spectral estimation.
The effect of the data window is governed by the relationship (5.7), which 
states in effect that when any signal vector is multiplied by a window vector, 
the spectral result is the periodic convolution of the DFTs of the two vectors.

Random Signals and Spectral Estimation
189
That is,
DFT{wtxk} = 
W„X,„_„; m = 0,1......N-l (7.35)
n=0
where w and x are the window and signal vectors of length N. The convolution 
of the window spectrum with the signal spectrum has the effect shown in the 
examples of Chapter 5. In the present case, it improves the spectral estimate 
by smoothing it, but also smears abrupt changes in the estimated power density, 
making the estimated spectrum somewhat less accurate in this respect.
When we use a window vector (w) to modify a data segment (x) in power 
spectral estimation, it is also important to remember that the window alters 
the total squared value of the data. That is, unless we are using the boxcar 
window, (w .* x) .A2 is not equal to x ,A2. Therefore, to preserve (7.28) and 
have the integral of the periodogram equal to the average power, we must 
remove this effect of the window by dividing each periodogram component 
by w * w', that is, we must divide each periodogram by the total squared 
window value.
An illustration of the use of data windows (scaled as just described) is 
given in Figure 7.19, which again shows power spectral estimates of the 
signal i/ generated as in Figure 7.16. In this illustration, the record size was 
increased to 212 samples to give a better estimate for segment length 64. The
FIGURE 7.19
Power density, Py(v), computed from 4096 samples of the signal y in Figure 7.16. In each plot, 
the segment overlap was 95%, and the segment size was 64. The plots show the effect of using 
different data windows. The heavy line in each case shows the true power spectrum.

190
Digital Signal Processing with Examples in Matlab
FIGURE 7.20
Data windows generated by the windows function; N = 32.
overlap was again set at 95%. We can note that the Hanning and Blackman 
windows produce smoothing along the top of the spectrum, but (on close 
inspection) they have the adverse effect of causing the edges at 0.1 and 
0.4 Hz-s to be less defined.
The use of windows in power spectral estimation is generally recommended 
with stationary random signals, especially signals with broad spectra. The 
windows generated by the function window, described in Chapter 5, (5.13), 
are illustrated in Figure 7.20. Any of these may be used to smooth the 
periodograms that are averaged to obtain an estimated power spectrum.
If a signal is not stationary or has a true power spectrum with sharp dis­
continuities, data windows must be used with caution. In this case, the boxcar 
and the tapered rectangular window, which are among the choices illustrated 
in Figure 7.20, are generally preferable. The tapered rectangular window is 
almost rectangular, having a cosine taper over 10% of the length at each end.
7.8 The Cross-Power Spectrum
The cross-power spectrum, or cross spectrum for short, is like the power spec­
trum we have been discussing, except it involves two time series instead 
of just one. Suppose x and y are vectors of length N taken from two wave­
forms, x(t) and y(t). As before, we assume for this analysis that both x and

Random Signals and Spectral Estimation
191
y extend periodically. Then, analogous to the periodogram in (7.24), we have 
the cross-periodogram:
Cross-periodogram: Pxy(m) = —m = 0,l,...,N-l
(7.36)
Just as PIX(m) was a measure of power density, so also Pxv(m) is a measure 
of cross-power density, in the following sense. Similar to (7.27) and (7.28), a 
single component Pilin') gives the cross-power in range (m ± 0.5)/N Hz-s. 
And as illustrated in Figure 7.11, we add the components at positive and 
negative frequencies, that is, Pxy(m) and P (-m) = Pxy(N - m), to get the total 
power in this frequency range. But in this case, these components are com­
plex conjugates in accordance with (3.11). The development that led from 
(7.20) to (7.22) in this case leads to the following:
n-i
Average cross-power = xny„ = x(t)y(t)rff 
iV *""* 
IN I Jo
>1 = 0
- <pTry(O) = 
= 
(7.37)
™ m=0 
m=0
The sum on the right is real, because Pty(0) and P (N/2) are real, and P„J(m) 
and P^N - m) are conjugates for the other values of m, and also, of course, 
because q\7(0) is real.
Thus, only the real part of the cross-periodogram contributes to the cross­
power. In physical terms, if v(t) and i(f) are voltage and current waveforms, 
the average cross-power given by (7.37) is the actual power in watts. For 
example, if v(f) and i(t) are both unit sine waves, the average power is the 
average product, or 0.5 watt; if v(t) is a sine wave and i(t) is a cosine wave, 
the average power is again the average product, or zero.
Another important use of the cross-power spectrum comes from the fol­
lowing relationship, which may be derived just as (7.34) was derived. The 
cross-periodogram of vectors x and y with periodic extension is the DFT of 
the correlation function q>vy(k); that is,
DFT{(pvs,(fc)} = Pxjf(m); m = 0,1,...,N-1 
(7.38)
Thus, Pxy(ffl) tells us how the cross-correlation function is distributed over 
frequency. In this sense, the magnitude of Pxy(m), that is, JP (m)|, is a 
measure of the coherence of signals x and y at different frequencies. A 
function called the Magnitude-Squared Coherence function (MSC)19'22,23 is 
defined as
Magnitude-squared coherence: T2y(v) = 
) 
(7-39)

192 
Digital Signal Processing with Examples in Matlab
noise (n()
noise (np
FIGURE 7.21
Generation of signals x and y, which were used to produce the power spectra in Figure 7.22.
The MSC is a normalized measure of the coherence of signals x and y at v 
Hz-s in the sense that it indicates whether the cross-correlation function, 
<pxJk~), has a component at that frequency.
The MSC is useful in situations where the source of a signal is weak and 
inaccessible, and only noisy versions of the signal can be recorded by 
placing sensors away from the source. Medical situations where invasive 
procedures are ruled out, or geophysical applications where the source 
cannot be reached, are examples. Then, one may use multiple sensors and 
look for coherence between pairs of sensor outputs using the MSC. (The 
concept also extends to more than two sensors using multidimensional 
spectra.)
The MSC concept is illustrated in the situation shown in Figure 7.21, where 
two signals, x and y, are recorded by placing sensors where signals from an 
unknown source (s) are able to be received. In this example, white Gaussian 
noise (nJ is added to s to produce x, and independent white Gaussian noise 
(n2) is added to a delayed version of s to produce y. The power spectrum of s, 
Pss(v), is shown on the left in Figure 7.22, with Pxx(v) and Pw(v) in the center 
of Figure 7.22. In situations like this, the signal quality is expressed in terms 
of the signal-to-noise ratio (SNR), which is
SNR = si5nalPower 
(7.40)
noise power
In this example, s is limited to the frequency range [0.1,0.3] Hz-s, and in this 
range, the SNR is 0.03, which is representative of a very noisy signal.
The magnitude of the cross-spectrum, |Pty(v)|, is shown on the right in 
Figure 7.22. All four spectra are based on a record length of 105 samples and 
periodograms of length 20 with 50% segment overlap. The segment window 
in each case was a tapered boxcar. The cross-spectrum magnitude, which is 
the nonnormalized version of the MSC in (7.39), illustrates the "recovery" 
of the signal spectrum, Pss(v), in this situation. Note that all four spectra are

Random Signals and Spectral Estimation 
193
FIGURE 7.22
Power density spectra for the signals in Figure 7.21. Each spectrum is based on a recording of 
105 samples of the signal(s). The SNR in this case was 0.03.
plotted on the same scale, and |( v)| is (theoretically) the same as Pss(v), 
because and n2 are statistically independent and, therefore, uncorrelated. 
Other illustrations of MSC may be found at the end of this chapter in Exercises 
14 and 15. 
'
Thus, we have seen that the cross-power spectrum has meaning in terms 
of electrical power (and similarly in terms of mechanical power), and that 
it is also useful in various ways as a measure of the distribution over fre­
quency of the cross-correlation of two signal vectors.
7.9 
Algorithms
Besides the various Matlab functions that form the basis for statistical signal 
analysis, three additional functions included with the text were used to help 
produce examples in this chapter. First is the function
[f, xmin, xmc.x] = freq (r) 
(7.41)
This function computes the frequency function (/„) given by (7.3), as well as 
the minimum and maximum elements, of any integer array or vector, x. (If 
the elements of x are not integers, they are rounded.)

194 
Digital Signal Processing with Examples in Matlab
The second function is
bar2(v,p,color) 
(7.42)
This is a simple modification of the Matlab function bar, which may be used 
to plot continuous amplitude distributions or power spectra.
The third function is
[P, nsgmts] = pds(x, y, N, windo, overlap) 
(7-43)
which may be used to compute a power spectral estimate as an average of 
periodograms. The segment size is N. When y = x, P(1 :N) is P„(v). When x 
and y are two different signal vectors, P(1:N) is Pxv(v). The second output 
(nsgmts), if desired, gives the number of segments the function was able to 
use, given N and the amount of overlap, the latter being specified within 
the range (0,1).
7.10 Exercises
General Instructions: (1) Whenever you make a plot in any of the exercises, 
be sure to label both axes so we can tell exactly what the units mean. (2) 
Make continuous plots unless otherwise specified. "Discrete plot" means to 
plot discrete symbols, unconnected unless otherwise specified. (3) In plots, 
and especially in subplots, use the axis function for best use of the plotting 
area. (4) In exercises that require random numbers, initialize the seed to 
"123."
1. Complete the following:
a. Given y(t) = ax(t) + b, begin with the definition of the mean in 
(7.4) and prove the formula for in (7.8).
b. Begin with the definition of variance in (7.5) and prove the 
formula for in (7.8).
2. Let y = sinx. Assuming x is uniformly distributed in the range (0, 
2ii), use (7.7) over a range of x, where y increases monotonically, 
say x = (0, n/2), to derive the probability density function of y. Make 
a plot of p(y) vs. y over the range y = (-1,1).
3. Generate 5000 samples of a continuous uniform random variate with 
zero mean and average power equal to 27. Make a single plot of the 
theoretical continuous probability density, and on the same graph 
make a histogram plot of the sample amplitude distribution, using 
nine bars of equal width that exactly span the range of the variate.

Random Signals and Spectral Estimation 
195
4. Generate 5000 samples of a continuous Gaussian random variate 
with mean equal to 6 and average power equal to 4. Make a single 
plot of the theoretical continuous probability density and on the 
same graph, make a histogram plot of the sample amplitude dis­
tribution, using bars centered at integers ranging from fix - 3<7X to 
ip + 3<jx. Comment on the integral of the histogram plot in this case.
5. Is the amplitude distribution of x + y equal to p(x) + p(y)? Prove 
your answer.
6. Suppose x(f) is a Gaussian signal with mean equal to zero and 
power equal to 10. At any time t, determine the following: 
a. What is the probability that x(i) lies in the range [0,5]?
b. What is the probability that |x(f)| lies in the range [1,2]?
7. For the signal vector (x), use the 5000-sample sequence in Exercise 3. 
Make three subplots. On the left, make a continuous plot of the peri­
odogram of x over the range [0,0.5] Hz-s. In the center, make a bar 
plot similar to Figure 7.11, but only over [0,0.5] Hz-s, using half­
overlapping 100-sample segments. On the right, make a similar bar 
plot using 40-sample segments. Use [0,100] for the periodogram 
range on all three plots.
8. Generate 1200 samples of uniform white noise with zero mean and 
average power equal to 3.
a. Plot the amplitude distribution as a histogram with each bar 
width equal to 0.5.
b. Make two subplots, each with a power density spectrum like 
Figure 7.11. For the left-hand plot, use nonoverlapping seg­
ments of length 80. For the right-hand plot, use segments of 
length 80 that overlap 50%. Use the tapered boxcar window in 
both cases.
c. How does the integral of each plot compare with the theoretical 
average power? How do the two spectra compare with each 
other and with the theoretical power spectrum?
9. Generate 5000 samples of
x* = sin(2ftA750) + */2 cos(2?dc/20)
Test the operation of function pds by making two subplots. On the 
left, plot power density vs. Hz-s using segment size 100, a boxcar 
window, and no overlap. On the right, make a similar plot using 
the Hamming window and 50% overlap. Make the plots over the 
range [0,0.5] Hz-s as in Figure 7.17.
a. Explain the differences between the two plots.
b. Equate the average power in x to the integral of each plot.

196 
Digital Signal Processing with Examples in Matlab
10. Generate 8192 samples of uniform white noise with mean zero 
and average power equal to 3 in vector x. Filter x to produce vector 
y. Use an FIR lowpass filter with Hamming window, 31 weights, 
and cutoff at 12.5 Hz. The time step between samples is 0.02 s.
a. Explain how you would compute the theoretical power density 
spectrum of y.
b. Estimate the power density spectrum using 64-sample seg­
ments, 50% overlap, and the Hamming window. Make a bar 
plot of the spectrum over the range [0,0.5] Hz-s, and overlay a 
continuous plot of the theoretical spectrum.
11. Describe how a window is normalized. In a 2 x 2 array of subplots, 
plot nonnormalized and normalized versions of the tapered rect­
angular, Hamming, Tent, and Blackman windows. Begin on the 
upper left with normalized and nonnormalized plots of the 
tapered rectangular window.
12. Generate N = 8000 samples of uniform white noise with mean = 0 
and average power = 3.
a. Plot the amplitude distribution as a histogram with each bar 
width equal to 0.5.
b. Make three subplots. On the left, make a continuous plot of the 
periodogram of all N samples vs. frequency from -0.5 to 0.5 Hz-s. 
In the middle, make a bar plot of the power density using 
nonoverlapping segments of length 80. On the right, make a 
similar plot of 75%-overlapping segments of length 80. Use the 
Hamming window.
13. Generate vector x having 8192 uniform white noise samples with 
average power equal to one. Generate y as a similar vector of 
Gaussian white noise. Make four subplots. On the upper left, plot 
the amplitude distribution of x using a histogram with 15 bars. 
On the lower left, plot the power density of x over [0,0.5] Hz-s 
using half-overlapping segments of length 32 and the Hamming 
window. Do the same on the right for y, using amplitude range 
±3cr.
14. Let k = [0:3999]. Generate vector x - sin(O.lrfc). Then, generate 
vector y — cosfO.lrtfc) + n, where n is a Gaussian white noise vector 
with zero mean and average power 100.
a. Assuming the "signal" is the cosine wave, what is the SNR of 
y in dB?
b. Make two subplots. On the left, make a bar plot of the power 
density spectrum of y over [0,0.5] Hz-z. Use 75% overlapping 
segments of length 40 and the boxcar window. On the right, 
make a similar plot of | Pxy( v)|, the magnitude of the cross-power 
density of x and y. Observe the differences in the two plots.

Random Signals and Spectral Estimation 
197
15. Generate N = 8000 samples of signals x and y in Figure 7.21, using 
s = sin(0.1zr[0: N- 1]), d- 2, and independent white Gaussian noise 
with power 50 for and n2. Now let x and y simulate signals from 
two sensors, in which the signal from the unknown source, s, may 
or may not be present.
a. Plot x and y, one above the other, in two subplots. Is there any 
sign of the source in these plots?
b. Make three subplots. Use segment length 20, the boxcar window, 
and 75% overlap. From left to right, plot, P„(v), P,„,(v), and the 
mean-squared coherence, F"y (v), vs. v over range [0,0.5], Is there 
any sign of the source in these plots? If so, explain.
References
1. Shammugan, K.S., Random Signals: Detection, Estimation, and Data Analysis, Wiley, 
New York, 1988, chap. 2.
2. Douglas, C.M. and Runger, G.C., Applied Statistics and Probability for Engineers, 
2nd ed., John Wiley. & Sons, New York, 1999.
3. Derman, C, Gleser, L.J., and Olkin, I., A Guide to Probability Theory and Application, 
Holt, Rinehart and Winston, New York, 1973.
4. Dwass, M., Probability Theory and Applications, W.A. Benjamin, New York, 1970.
5. Feller, W, An Introduction to Probability Theory and Its Applications, 2nd ed., vol. 1, 
Wiley, New York, 1957.
6. Blanc-LaPierre, A. and Fortet, R., Theory of Random Functions, vol. 1, Gordon and 
Breach, New York, 1965.
7. Jenkins, G.M. and Watts, D.G., Spectral Analysis and Its Applications, Holden-Day, 
San Francisco, 1968.
8. Bendat, J.S. and Piersol, A.G., Random Data: Analysis and Measurement Procedure, 
John Wiley & Sons, New York, 1971, chap. 9.
9. Otnes, R.K. and Enochson, L., Digital Time Series Analysis, John Wiley, New York, 
1972.
10. Koopmans, L.H., The Spectral Analysis of Time Series, Academic Press, New York, 
1974.
11. Kay, S., Modern Spectral Estimation: Theory and Application, Prentice Hall, Engle­
wood Cliffs, NJ, 1987.
12. Marple, S.L., Digital Spectral Analysis with Applications, Prentice Hall, Englewood 
Cliffs, NJ, 1987. 
'
13. Oppenheim, A.V. and Schafer, R.W., Discrete-Time Signal Processing, Prentice Hall, 
Englewood Cliffs, NJ, 1989, chaps. 8, 11.
14. Rabiner, L.R. and Gold, B., Theory and Application of Digital Signal Processing, 
Prentice Hall, Englewood Cliffs, NJ, 1975, chap. 6.
15. Richards, P.I., Computing reliable power spectra, IEEE Spectrum, 4,83, Jan. 1967.
16. Welch, P.D., The use of the fast Fourier transform for the estimation of power 
spectra, IEEE Trans., AU-15, 70, June 1967.
17. Bingham, C., Godfrey, M.D., and Tukey, J.W., Modern techniques of power 
spectrum estimation, IEEE Trans., AU-15, 56, June 1967.

198 
Digital Signal Processing with Examples in Matlab
18. Yuen, C.K., A comparison of five methods for computing the power spectrum 
of a random process using data segmentation, Proc. IEEE, 65, 984, June 1977.
19. Hinich, M.J. and Clay, C.S., The application of the discrete Fourier transform in 
the estimation of power spectra, coherence, and bispectra of geophysical data, 
Reviews of Geophysics, 6, 347, Aug. 1968.
20. Carter, G.C. and Nuttall, A.H., On the weighted overlapped segment averaging 
method for power spectral estimation, Proc. IEEE, 68, 1352, 1980.
21. Nuttall, A.H. and Carter, G.C., A generalized framework for power spectral 
estimation, IEEE Trans., ASSP-28, 334, June 1980.
22. Carter, G.C., Knapp, C.H., and Nuttall, A.H., Estimation of the magnitude- 
squared coherence function via overlapped FFT processing, IEEE Trans., AU-21, 
337, Aug. 1973.
23. Benignus, V.A., Estimation of the coherence spectrum and its confidence interval 
using the fast Fourier transform, IEEE Trans., AU-17, 145, June 1969.

Q0
Least-Squares System Design
8.1 Introduction
The least-squares principle is widely applicable to the design of digital signal 
processing systems. In Chapter 2, we saw how to fit a general linear combi­
nation of functions to a desired function or sequence of samples, and how 
the finite Fourier series, as an example, provides a least-squares fit to a 
sequence of data. In Chapter 5, Exercise 5, we saw that the FIR filter gain 
may be expressed as a Fourier series in frequency, and is therefore a least­
squares approximation to the ideal rectangular gain function. In this chapter, 
we wish to extend the least-squares concept to the design of other kinds of 
signal processing systems. We will see how several kinds of DSP systems 
may be realized using a common linear least-squares approach.
First, we describe briefly a variety of tasks involving prediction, modeling, 
equalization, and interference canceling, where least-squares design is useful. 
We show that all these tasks lead to the same least-squares design problem, 
which is to match a particular signal to a desired signal so that the difference 
between the two signals is minimal in the least-squares sense.
Next, we discuss the solution to the least-squares system design problem, 
which, for nonrecursive systems, amounts to the inversion of a matrix of 
correlation coefficients, similar to the symmetric coefficient matrix in Chapter 
2, (2.10). We show that finding this solution is equivalent to finding the 
minimum point on a quadratic mean-squared-error performance surface 
(a concept that will become useful when we discuss adaptive signal processing 
in Chapter 9).
We also include various examples of least-squares design. The examples 
illustrate a variety of system configurations used in different applications, 
and also different ways to estimate correlation coefficients. However, the 
examples do not even begin to cover all of the many applications of this topic. 
A number of additional applications are included in the exercises, and there­
fore, the exercises are an important part of this particular chapter. Further­
more, this chapter forms the basis for adaptive signal processing, which is 
the subject of Chapter 9.
199

200 
Digital Signal Processing with Examples in Matlab
8.2 Applications of Least-Squares Design
In this section, we describe system configurations where the least-squares 
design is applicable. The first configuration, illustrated in Figure 8.1, is the 
linear predictor. The prediction concept is illustrated in its simplest form in 
the upper diagram. In the least-squares design, the coefficients, or weights, of 
a causal linear system, H(z), are adjusted to minimize the mean-squared error, 
E[e*], thereby making the system output, gk, the least-squares approximation 
to a desired signal, dk. In this case, dk is the same as the input, sk, and must 
be "predicted" using the past history of sk. That is, dk must be predicted in 
terms of sk delayed .by m samples and processed through H(z). Again, the 
least-squares design process consists of adjusting the weights of H(z) to make 
this processed version of the history of the input the best "prediction" of the 
present sample value, sk.
The upper diagram in Figure 8.1 with a unit delay, that is, with m = 1, is 
the most common form of the linear predictor, even though it does not 
actually predict a future value of the input. In most signal processing appli­
cations, the error, ek, rather than a future value of sk is needed. If an actual 
prediction of a signal is needed, the augmented form in the lower diagram
sk
X
Copy of 
H(z)
FIGURE 8.1
Least-squares linear prediction. In the upper diagram, gt is the prediction of the current input, 
sk, given the past history of s. In the lower diagram, st is the prediction of a future input, st4„„ 
based on the history of s.

Least-Squares System Design 
201
FIGURE 8.2
Least-squares modeling or system identification. When the noise is independent of the input, 
f, H(z) becomes a "model" of the unknown system in the sense that its output, g(, is a least­
squares approximation to the output of the unknown system.
of Figure 8.1 may be implemented. Here, sk is processed through a copy of 
the predictor, H(z), which produces the predicted future value, st+,„.
The linear predictor is useful in waveform encoding and data compres­
sion,12'19 spectral estimation,9'10'15’16 spectral line enhancement,14 event detec­
tion,13 and other areas. Its effect in all these applications, as one might guess 
from the previous discussion, is to produce a signal which is a decorrelated 
or whitened version of sk. The input is a signal that may be predictable in 
terms of its past values, and the output, one might say, is everything that is 
left after the predictable part is taken out.
The second configuration in which least-squares design is applicable is 
illustrated in Figure 8.2 and is called modeling or system identification. Here, 
a linear system, H(z), models or identifies an unknown "plant" (see Section 8.8) 
consisting of an unknown system with internal noise. The least-squares 
design forces the linear system output, gk, to be a least-squares approximation 
to the desired plant output, dk, for a particular input signal, fk. When fk has 
spectral content at all frequencies and when the plant noise contributes at 
most a small part of the power in dk, we expect H(z) to be similar to the 
transfer function of the plant's unknown system. Note, however, that H(z) 
is not necessarily a least-squares approximation, as it was, for example, in 
Chapter 5. Thus, the modeling concept is applicable where the best approx­
imation to a signal, rather than to a transfer function, is the objective. The 
type of modeling illustrated in Figure 8.2 has a wide range of applications, 
including modeling in the biological, social, and economic sciences,22 in 
adaptive control systems,3'8 in digital filter design,23 and in geophysics.3
Another application of least-squares system design, known as inverse mod­
eling or equalization, is illustrated in Figure 8.3. Here, the desired output of 
H(z), again labeled dk, is a delayed version of the input signal. To cause the

202 
Digital Signal Processing with Examples in Matlab
FIGURE 8.3
Least-squares inverse modeling, or equalization. When the noise is independent of the input, 
H(z) becomes the inverse of the channel, and thus "equalizes" the effect of the channel on the 
input, producing a delayed least-squares approximation to the input.
output to approximate dk, H(z) is adjusted to model the inverse of, or 
equalize, the unknown system with internal noise. When the unknown system 
and the linear system are both causal, the delay, z~"', is used to compensate 
for the propagation delay through the two systems in cascade. When the 
internal noise of the unknown system is only a small part of /A, the output 
of the equalizer, gk, is a good approximation to the delayed input, sk_,„. Hence, 
the linear system is used here to invert, or equalize, the effect of the unknown 
system on the input signal.
Equalization is used in communication systems to remove distortion by 
compensating for nonuniform channel gain and multipath effects and to 
improve the signal-to-noise ratio if the channel introduces band-limited 
noise.3'17,18 Equalization and inverse modeling are also used in adaptive control,3 
speech analysis,7,15'10 deconvolution,20 digital filter design,3,23 and other areas.
Our final example of a configuration where least-squares design is used, 
interference canceling, is illustrated in Figure 8.4. The interference-canceling 
principle1,3,21 is applicable in cases where there is a signal, sk, with additive 
noise, nk, and also a source of correlated noise, nk. Ideally, nk and n'k are corre­
lated with each other but not with sk, although the principle is applicable even 
if the signal and noise are correlated. The least-squares design objective is 
to adjust H(z) so that its output, gk, is a least-squares approximation to nk, 
thereby canceling, by subtraction, the noise from the incoming waveform. 
When the signal and noise are independent, this is equivalent to minimizing 
the mean-squared value of the error, E[e2], because the independent noise 
cannot be made to cancel the signal, sk. The delay is placed in the configu­
ration to compensate for propagation through the causal linear system and 
allow the noise sequences, nk and n'k, to be aligned in time.
Interference canceling is an interesting and sometimes preferable alterna­
tive to bandpass filtering for improving the signal-to-noise ratio. For example, 
suppose we have an underground seismic sensor that receives, in addition 
to a seismic signal component, sk, an acoustic noise component, nk, coupled

Least-Squares System Design 
203
Signal + noise
FIGURE 8.4
Least-squares noise cancellation. When the noise (ha.) is independent of the signal, and correlated 
noise (nJ) can be measured independently, H(z) produces a least-squares approximation to nk, 
and thus, improves the SNR, even though $k and nk may have overlapping spectra.
into the ground from the atmosphere above, and the two components have 
similar spectra so that nk cannot be removed from (sk + nk) using bandpass 
filtering. We could then add an above-ground microphone to the system to 
receive nk, an acoustic component correlated with (but not exactly the same 
as) nk, and process the signals as in Figure 8.4 to reduce the acoustic noise and 
increase the signal-to-noise ratio. The number of examples of this type is 
limited only by one's imagination.
Although the applications illustrated in Figures 8.1 through 8.4 are dis­
tinctly different, they all involve the same least-squares design problem. The 
important features may be summarized as follows. There is a linear system, 
H(z), with adjustable coefficients. These coefficients are adjusted to cause the 
output, gk, of the linear system to be a least-squares approximation to the 
desired signal, dk, and thereby minimize the mean-squared error, E [ ]. We now 
proceed to examine the nature of the resulting least-squares design problem, 
using a geometrical interpretation.
8.3 System Design via the Mean-Squared Error
If we compare Figures 8.1 through 8.4, we can see a common least-squares 
design problem, which is illustrated in Figure 8.5. The parameters of a causal 
linear system, H(z), are to be adjusted or selected to minimize a mean- 
squared error, E[e^]. [Actually, there is no need in what follows to assume 
that H(z) is causal, but we assume causality for convenience, and to include 
real-time applications.] For example, if H(z) is a linear system of the form 
B(z)/A(z), the parameters to be selected are b and a, the coefficient vectors 
with transforms of B(z) and A(z). The error, ek, is the difference between the 
desired signal, dkl and the linear system output, gk:
ek = dk-gk 
(8-1)

204
Digital Signal Processing with Examples in Matlab
Causal 
linear system 
H(z)
FIGURE 8.5
Essential ingredients common to Figures 8.1 through 8.4. The weights of a causal linear system, 
H(z), are adjusted to minimize the MSE, that is, the mean value of e2k, and thereby make a 
least-squares approximation to dk.
Let us now assume that the signals in Figure 8.5 are stationary so that expected 
values and correlation functions are defined. Then the mean-squared error 
(MSE) is
MSE = E[e2] = E[(d^gJ2]
= E[dk + g2- 2d(yJ
= <p(llJ(0) + E[g2]-2E[d|igjt] 
(8.2)
The last result follows from the definition of the correlation function in
Chapter 3, (3.2). For the causal linear system in Figure 8.5, we have
N-l
gf=p.A-.; 0<k<~ 
(8.3)
>1=0
where vector h is the impulse response of H(z). Using this in (8.2), we obtain
N-1N-1 
N-l
= 
+ 
(8-4)
n=0 nt-0 
it=Q
In the center term, and are displaced n - m time steps from each 
other; hence, the expected product is <f>/y(n - tn). Similarly, the expected 
value of dkfk^ is <pdf(-n).
Our next objective is to formulate the minimization of (8.4) as a linear 
least-squares problem so we can apply the methods in Chapter 2 to its

Least-Squares System Design 
205
solution for the optimal impulse response. Because the MSE in (8.4) is a 
quadratic function of the N samples in h, we can see that the gradient of the 
MSE with respect to these samples is a linear function of the samples; that is,
k = Q, 
(8.5)
k 
n=0
[To obtain this, we used the fact that (pfr(n) is an even function of n, and also 
substituted <p[An) in place of 
Setting this gradient vector equal to zero,
we have the N equations we need to solve for the impulse response of the 
"optimal" H(z) that minimizes the MSE in a stationary signal environment:
(8.6)
<pff(0)
<Pff(2) 
.
• <P#(N-1)
110
<M°)
(PffW
<P//(D 
■
• <P//(N-2)
^(i)
<PffW
/i2
<M2>
<pff(N- 1)
<Pff(N-2)
¥>//(N-3) .
jN-i.
_<p/1((N-l)
Thus, we formulated the problem of minimizing the MSE as a linear least­
squares problem. The coefficient matrix, called the autocorrelation matrix of 
the signal f, is known as a Toeplitz matrix on account of its special form. 
Notice that each column (or row) is a rotation of the autocorrelation vector, (pff, 
which is N x 1. Let denote the coefficient matrix. In Matlab, to create 
from the elements of <pff, we create an N x N index array. Matlab indices must 
begin at one, so the elements of the index array are one plus the indices of 
<pff in (8.6), that is, 1 through N on the first row, 2 followed by 1 through 
N - 1 in the second row, etc., and finally N, N - 1,...,1 on the last row. The 
autocorrelation matrix, is then created using the index array. For example, 
the following expressions, in which index_mat is the index matrix, will pro­
duce from the autocorrelation vector, phi:
N=length(phi);
q=[phi(N: -1:2);phi];
ixl=[0:N-1]'*ones[1,N];
ix2=ones(N, 1)*[N:-1:1];
i ndex_ma t=ixl + ix2 
Phi_f f=q(index_mat)
By running this algorithm with "phi" equal to a column vector of length 
N = 3 or 4 and allowing q and index_mat to be printed, one can observe how 

206 
Digital Signal Processing with Examples in Matlab
the index matrix is used to create 0^ from the correlation vector, phi. For 
example, with phi equal [90, 80, 70]' and therefore q equal [70, 80, 90, 80, 70]',
index_mat = 
■321
4 3 2
5 4 3 
Phi_ff =
90 80 70
80 90 80 
70 80 90
(8-8)
The solution of (8.6) is the impulse response vector, h, of the optimal causal 
linear system that minimizes the MSE. If the system is nonrecursive, then 
h = b; that is, the elements of h are the weights of the optimal FIR filter. If 
the system is recursive, then h is the inverse transform of B(z)/A(z), and the 
solution for a and b is, in general, a nonlinear problem. Therefore, we will 
restrict our discussion to the causal FIR configuration in Figure 8.6 with 
impulse response h = b.
We can write (8.4) and (8.6) using vector and array notation, again using 
the notation where <pxi/(n) stands for the wth of N elements of column vector 
<px;l, and assuming b = h is also a column vector of length N. The two vector 
expressions are, respectively,
(8.4): MSE = <prfl,(0) + b'<bffb - 2b'<pfll
(8.6): <S>f/b = (pfd 
" '
An algorithm due to Levinson25 6 uses the special properties of 0ff to solve 
for b in (8.6). If Levinson's algorithm is not available, the methods of Chapter 
2 may be used. The solution without the use of Levinson's algorithm and 
the resulting minimum MSE are given in Table 8.1, where we use bopt to 
represent the solution to (8.6), that is, the optimal weight vector.
The result for the minimum MSE is proved by substituting the optimal 
weight expression into the MSE and noting that = 0^-, [0jj ipjd]' = <p^0)p 
and also <p^bopl = b'opl<p/d.
H(z)=B(z)
FIGURE 8.6
The same essential ingredients as in Figure 8.5, but now with H(z) in the form of an FIR filter 
with weights b =

Least-Squares System Design 
207
TABLE 8.1
Least-Squares Equations Using Correlation: Algebraic and Mati.ab
Mean-squared error (MSE) MSE = <p,u(0) + b'^gb - 2b'(prj
MSE=phi_dd(1)+b'*Phi_f f *b-2 *b'*phi_fd
Optimal weights 
bopt = <pld
bopt=Phi„ff\phi_fd
Minimum MSE 
MSEmin = <p(M(0) - qj^hopt
MSEmin=phi_dd{1)-phi_fd’*bopt
We note in these equations that the MSE is, by its nature, always positive, 
and therefore, because it is a quadratic function of the weights, it describes 
a bowl-shaped surface in (N + 1) dimensional Cartesian space. Thus, the 
MSE must have the single global minimum given by the optimal weight 
vector. This minimum might be distributed due to a nonsingular correlation 
matrix, that is, the bowl might have a flat bottom, but local minima cannot 
exist.
For some system design problems, the correlation vectors are known or 
assumed exactly. In other applications, they must be estimated. In still other 
applications, they may drift slowly with time. This leads to the design of 
adaptive signal-processing systems, which is the subject of the next chapter. 
Adaptive systems are systems that adjust the weight vector, b, continually 
as they continuously seek the solution to (8.6).
8.4 A Design Example
We now consider a simple example illustrating system design via the min­
imum MSE. Perhaps the easiest type of system to understand and to solve 
is the least-squares predictor with a simple periodic input, an example of which 
is shown in Figure 8.7. The predictor is an example of the upper diagram in 
Figure 8.1, with the delay (m) set to one time step. It is called a one-step 
predictor. For the signal sk, we use a sine wave:
sk = T2sin(27rk/12); -<» < k < <» 
(8.10)
The correlation functions in (8.6) are found by averaging over one cycle 
(12 samples) of sk. Using the fourth line of Table 1.2 in Chapter 1, we have
, . 
2 
. (2n(k + n)\ 
l'2nn'\
= i2XSmlirM“^2 J = C0Sl^2-J
(8.U)
. . 
2 v’ ■ f2ff(A:-m)'\ . (2n(k + ti)\ (2n(m + n)j
<M”) = i2^sml 12 JSint 12 
= C°V 12
Jr=O V 
7 V 
7 
V 
7

208
Digital Signal Processing with Examples in Matlab
FIGURE 8.7
A simple example of the linear predictor in Figure 8.1, known as a "one-step predictor with 
two weights."
With the delay (m) set at one, these computations give us the following for 
the present example:'
_ 1.0000 0.8660*1. 
_ 0.8660*1 
raim
- 
/ <Pfd ~
” LO-8660 l.OOOOj L°-5000J
We also note that, in this example, (pdd = (pff, which is the first row or column 
of <hjry. Thus, the MSE in Table 8.1 for this example is
MSE = 1 + k ■ I10000 °'8660l ■ N -2k frJ ■ M (8'13)
L 0 1J [0.8660 l.OOOOj [bj L 0 1J [0-5000
The MSE, a quadratic function of b, is plotted in Figure 8.8. The optimal weight 
values and the minimum MSE are, in accordance with Table 8.1,
MSEmin = tpdd(O) -<p^b = 0
(8.14)
The surface in Figure 8.8 reaches zero at the point where the heavy lines 
cross, that is, where the weights are at their optimal values. Thus, the one- 
step predictor with optimal weights exactly cancels the signal, sk, and the 
error, ek, is zero.
The MSE is illustrated in Figure 8.8 as a three-dimensional bowl-shaped 
surface. A contour plot of the MSE is illustrated in Figure 8.9. Most of the 
time, the contour plot is preferable for analysis. It shows clearly how the 
MSE decreases to a minimum at the optimal value of b and gives a better 
picture of the gradient of the MSE surface. The four contours in Figure 8.9 
were plotted with the following Matlab instructions:
v=[.O5 .15 .4 .8 1.8];
[c , h] =contour (bO ,bl, mse, v, ' k 1 ) ; grid; (8.15) 
clabel(c);

Least-Squares System Design 
209
FIGURE 8.8
Mean-squared-error plot for the example in Figure 8.7 with Sj. = Ji sin(2?rlr/12).
0 
12 
3
FIGURE 8.9
Contour plots, as if viewing a portion of the MSE in Figure 8.8 from directly above.
For the plot, the elements of vectors fe0 and were spaced evenly over the 
ranges shown, and mse was a matrix of MSE values computed (in this case) 
on a grid of 1502 evenly spaced pairs of elements of b0 and bv The clabel 
function was used to label the contours.
This example is artificial, because the signal is simple and is known exactly, 
and also because the filter has only two weights. It is meant to illustrate the 

210
Digital Signal Processing with Examples in Matias
quadratic error surface in three dimensions and to show a simple derivation 
of the optimal weights.
As a final step in this simple example, let us observe the effect of adding 
a third weight, b2, to the filter in Figure 8.7. With the third weight, the arrays 
in (8.12) are now
=
1.0000 0.8660 0.5000
0.8660 1.0000 0.8660
0.5000 0.8660 1.0000
; <Pfd =
0.8660
0.5000
0.0000
(8.16)
In this case, is singular and has no inverse. That is, (8.16) imposes con­
straints on the weight vector b, but does not provide one specific vector. The 
constraints may be expressed as
bo — b2 — a/3 
bg + ~/3b1 + b2 = 0
(8-17)
They imply a distributed minimum of the error surface, that is, a subspace 
in which the MSE is everywhere equal to zero. The solution for the optimal 
vector with b2 = 0 is seen to agree with the two-weight solution in (8.14).
8.5 Least-Squares Design with Finite Signal Vectors
In the preceding two sections, we dealt with stationary signals described in 
terms of correlation functions, and derived least-squares design equations 
from this basis. These we summarized in Table 8.1. Now, we turn our atten­
tion to the design of FIR systems that are optimal with respect to finite signal 
vectors. There are many ways in which these systems may be applicable to 
the tasks of prediction, noise cancellation, etc., described in Section 8.2. Even 
in real-time applications, the high processing rates of DSP chips may be used 
in a block processing mode, in which the system is optimized for each block 
of time and processes the blocks of signals in order as they occur in real time.
When the signals in Figure 8.5 are finite vectors, each with K elements, the 
least-squares equations are quite similar to the equations in Table 8.1. Instead 
of the MSE, it is easier to use the total squared error (TSE) and avoid having 
to divide by K; that is, similar to (8.4),
K-l 
K-1N-1N-1 
K-1N-1
tse = 
+
k=Q 
k=Q n=0 m=0 
Jc=O n=0
N-1N-1 
N-l
= rdd(0,0)+2, 
n)-2^fe„rrf/(-n) 
(8.18)
n-0 m=0 
n=0

Least-Squares System Design 
211
Here, in place of the correlation functions of the stationary signals in (8.4), 
we substituted the covariance functions of the finite signal vectors, and these 
now completely determine the geometry of the least-squares design problem, 
just as the correlation functions did previously. The covariance functions in 
(8.18) are as follows:
K-l
Autocovariance: rtr(m, n) = 
= r„(?7,m); 0 < (m, n) < N - 1
k=0
K-t
Cross-covariance: rXIJ(n) = 
= r,jr(-n); 0<n<N-1
*=o
(8.19)
We note that covariance is defined in different ways. These definitions are 
convenient here, because they apply directly to the TSE formula (8.18).
Notice that the covariance functions, as defined in (8.19), require knowl­
edge of the signal sequences beyond the range 0 < k < K - 1. Specifically, 
from the way the functions in (8.19) are used in (8.18), the following 
sequences are required:
f - 1/-N+1 f-N+2 "'fo •” /k-i]
(8.20) 
d = [do di djc-i]
That is, these sequences are required to produce the error vector, e, on which 
the TSE is based. Depending on the application, as will be seen, elements/_K+1 
through/^ may be set to zeros, or they may be set equal to past values of/.
With the TSE expression (8.18), the principal results for the error surface 
and the least-squares weights in Table 8.1 are applicable, provided that the 
covariance functions in (8.19) are substituted for the correlation functions 
used previously. Table 8.2 is the covariance-equivalent of Table 8.1, with bop, 
again representing the optimal weight vector.
In Table 8.2, as in Table 8.1, notice that the algebraic indices begin at zero 
and the Matlab indices at one, and also that rdj in (8.18) is replaced with 
here in order to make the range of the index (n) positive. Also, the autocova­
riance matrix, R#, replaces the autocorrelation matrix in Table 8.1. Notice that
TABLE 8.2
Least-Squares Equations Using Covariance: Algebraic and Matlab
Total-squared error (TSE) TSE = rAI(0,0) + b'Rgb - 2b'rfJ
TSE=d'*d + b'*R_ff*b - 2*b'*r_fd
Optimal weights 
feopt = Rfru
bopt=R_f f\r_fd
Minimum TSE 
TSE^ rJd(0,0) - r'^b^,
TSEmin=d'*d - r_fd1*bopt 

212
Digital Signal Processing with Examples in Matlab
the autocovariance matrix is symmetric, that is, rf. (m, n) = rfl {n, m), but is not 
Toeplitz, that is, the rows are, generally, not permutations of each other.
In summary, least-squares design using covariance is almost the same as 
least-squares design using correlation, the only essential difference being in 
the way we extend the ends of the signal vectors. Before considering more 
applications of these methods, we describe methods for computing the cor­
relation and covariance functions for given signal vectors.
8.6 Correlation and Covariance Computation
In this section, we consider two simple methods for estimating correlation 
or computing covariance that are useful in a variety of applications but do 
not cover all cases. Here, we assume the functions are estimated from sam­
ples of the signals; however, especially in the case of correlation, this may 
not be true. The correlation functions may be derived theoretically from 
assumed properties of the signals, or they may be computed using the 
inverse DFT of the power spectrum as described in Chapter 7.
Suppose the cross-correlation function (pxv(n) is to be estimated in terms of 
two signal vectors, r and y. Suppose we allow these vectors to be extended 
periodically. Then, we may compute <ply(n) as follows:
<P*v(n) = 
0<|n|<K-l 
(8.21)
The computation of <pXIJ(n) in this form requires K2 products, and would be 
lengthy if (1) all/(products were required and (2) K were large. Let us assume 
that both (1) and (2) are true and examine the computation in terms of a 
DFT product. From (8.21), we may conclude:
<PxVW = <ptJx(-n) = 
0<|«|<K-l (8.22) 
where r is the operation in (6.4) that reverses the order of the elements of a 
vector. This result gives <pIJ((H) in the form of a convolution, and thus, as the 
inverse DFT of a DFT product in accordance with (4.67). Furthermore, using 
z - eia>T in (6.5), we can see that the DFT of a reversed vector x' is X', that is, 
the conjugate of the DFT of x. Therefore, when (4.67) is applied to (8.22), the 
result is
Correlation vector <pXIJ = j~DFT ^X'Y}
(8.23)

Least-Squares System Design
213
Although this result, which is analogous to (4.67), but for correlation 
instead of convolution, is of general interest, it is only useful if the entire 
autocorrelation vector is required. If K is the length of x and y, then a 
computation of N elements of requires NX products. In Chapter 4, (4.69), 
we argued that the computation of (8.23) requires 12K log2(2K) real products. 
Therefore, the computation of <pIy(l :N) becomes more efficient using (8.23) 
only when
NK> 12Klog,(2K), or N>121og,(2K)
(8.24)
In least-squares applications, this condition is not very likely when K is large. 
In typical applications, K is at least an order of magnitude greater than N. 
Therefore, we usually prefer to compute correlation and covariance compu­
tations in the form of (8.21) rather than (8.23).
Three correlation functions are coded in the form of (8.21). The first is 
autocorr, which is illustrated in (8.25). The arguments are x, the signal vector, 
type, which is set to zero to extend x with zeros and one to extend x period­
ically, and N, the number of values of %xto compute.
(8.25)
Autocorrelation example: 
x=[Xj x2 x3 x4J; phi=autocorr(x,type,3)
type=0
type=l
phi(l)
(x3x1'tx2x2+x3x3+x4x4) /4
(x1x1+x2x2+x3x3+x4x4) / 4
phi(2)
(x1x2+x2x3+x3x4) /4
(x1x2+x2x3+x3x4+x4x1) /4
phi(3 )
(x1x3+x2x4)/4
(x1x3+x2x4+x3xl+x4x2) /4
The second correlation function is crosscorr, which has similar arguments 
and is illustrated in (8.26).
(8.26)
Cross-correlation example: 
x=[xx x2 x3 xj; y=[yx y2 y3 y4J;
phi=crosscorr(x,y,type,3)
type=0
type=l
phi(l)
(xxy1+x2y2+x3y3+x4y4)/4
(xiyI+x2y2+x3y3+x4y4) /4
phi(2)
(X!y2+x2y3+x,y4)/4
(x1y2+x2y3+x3y4+x4y1) /4
Phi(3)
(x1Y3+x2y4)/4
(x1y3+x2y4+x3y1+x4y2) / 4
The third correlation function is Phi = autocorr _mat{x, type, N), which com­
putes the N-element autocorrelation vector (pxx using autocorr and then pro­
duces Phi, the NxN autocorrelation matrix using code similar to (8.7).
In the case of covariance as it is used in least-squares design, there is not 
much reason to compute an autocovariance vector. Therefore, there are just 

214
Digital Signal Processing with Examples in Matlab
two Matlab functions, the first being R - autocovar_mat(x,N'), which imple­
ments the computation of the elements r1T(m,n) in (8.19). For covariance 
computation, the signals (x in this case) are always extended with zeros. If 
nonzero values of the startup elements in (8.20) are necessary in the compu­
tation, then all signal vectors may be extended to the left with N -1 elements, 
and K increased to K + N - 1. That is, in (8.20), we would append N - 1 
startup values to the beginning of f, and N - 1 zeros to the beginning of d. 
With this understanding, the autocovariance equation tn (8.19) becomes
K-l 
K-™-I
= ^xk_„,xk_„ = y x,xi+(l
k-Q 
x=0
(8.27)
Comparing (8.21) with the second form of rxx(m,n), we note that as the signal 
vector becomes long enough to neglect end effects, the covariance function 
approaches K times the autocorrelation function. [The same is true for the 
cross-covariance function in (8.19).]
Because R is symmetric, we only need to compute elements for m > n. An 
example of the covariance computation with N = 2 is given in (8.28). As in 
the previous examples, the indices begin at one instead of zero.
X=[X1 x2
Autocovariance example:
x3 x4] ; R=autocovar_mat(x,N)
r(l,l)
X1X1+X2X2 + X3X3+X4X4
r(2,l)
x1x2+x2x3+x3x4
r(2,2)
X1X1+X2X2+X3X3
(8.28)
The second cross-covariance function is r = crosscovar(x, y, N). The com­
putation is made as in the cross-covariance formula in (8.19). An example is 
shown in (8.29).
(8.29)
Cross-covariance example: 
x= [x4 x2 x3 x4l ; y=[yi y2 y3 y4] ; r=crosscovar (x, y, 3 )
r(l)
x1y1+x2y2+x3y3+x4y4
r (2)
xiy2+x2y3+x3y4
r (3)
x1y3+x2y4
With the functions described in this section, and with the power of Matlab 
in expressions such as those in Tables 8.1 and 8.2, it is easy to design least­
squares systems for the kinds of applications described in Section 8.2. Exam­
ples are described in the following sections.

Least-Squares System Design
215
8.7 Channel Equalization
In this section, we consider another example of nonrecursive least-squares 
design in order to illustrate the use of the functions just described, and also 
to design a system different from the predictor designed previously. In this 
example, illustrated in Figure 8.10, we are to design a least-squares equalizer 
for an “unknown channel," which, in this case, is represented by an all-pole 
transfer function given by
U(z) = ______e 01 sintO.I ,t):: 1
1 - 2e"°'1cos(0.17r)z*1 + e
(8.30)
Unlike Figure 8.3, which is more realistic, we are not including noise in 
Figure 8.10. This description of the unknown channel assures us a simple 
design of the all-zero equalizer, B(z), with zeros that must cancel the poles 
of U(z). To obtain the equalizer, we assume U(z) is unknown, but that we 
have sequences of the signals sk and fk, the latter being response of U(z) to 
the input signal, st. These are shown in Figure 8.11, where sk is seen to be an 
impulse at k = 0, and fk is the impulse response, that is, the inverse transform 
of U(z), which is a decaying sinusoid. (See Table 4.2, lines D and 4.) In Matlab, 
we would express the signal vectors as follows:
(8.31)
FIGURE 8.10
Equalizer design example. The '"unknown" channel is specified by Ll(z) in (8.30). The problem 
is to adjust the weights of B(z) so that the squared difference between and sk is minimal.

216 
Digital Signal Processing with Examples in Matlab
FIGURE 8.11
Signals in the equalizer example of Figure 8.10. The input signal, st, is a unit impulse, and ft is 
the response of the unknown channel to st.
Notice that d is created by appending m zeros to the beginning of s, thus 
effecting the delay of m samples in Figure 8.10.
Thus, we are designing an equalizer that corrects the impulse response of 
the unknown channel so that its output, sk_m, is a delayed least-squares 
approximation to the signal sk. Using the functions described in the preceding 
section, the optimal weights and minimum TSE for given values of N and 
m are computed as follows (see Table 8.2):
Rff=autocovar_mat(f,N); 
rfd=crosscovar(f,d,N); 
b=Rff\rfd;
TSEmin(N+l)=d*d1+b'*Rff*b-2*b'*rfd
(8.32)
It is interesting to consider the effect of the delay (m) on the required 
number of weights, which is illustrated in Figure 8.12. The latter is a plot 
of the minimum TSE vs. the number of FIR weights for different values of 
the delay, m. It shows first that if the delay (m) is at least one, an equalizer 
that exactly compensates for the channel and drives the TSE to zero is 
realizable. Second, the plot shows that there is also an optimal delay in 
systems like this, in the sense that the number of equalizer weights is 
minimized.

Least-Squares System Design 
217
8.8 System Identification
The next example in Figure 8.13 is one in which we are trying to identify 
(model) an unknown system or "plant". In this case, the plant has poles, 
so we cannot exactly match the desired signal with an FIR filter, that is, 
we cannot drive the TSE to zero using B(z) to model the plant. In this 
example, as in the previous example, the transfer functions Lf(z) and B(z) 
in Figure 8.13 are
.. 
_ g~olsin(O.17r)z-1
1 - 2e~t>1cos(0.17r)z~1 + e“°'2z'2
N-t 
(8.33)
B(z) = ^b„z'"
The input signal, fk in Figure 8.13, is a random white signal with unit power. 
The input sequence is given by
f=randn(l,K) 
(8.34)
That is,/is a white Gaussian noise sequence with K elements and unit power.
Number of weights (N)
FIGURE 8.12
Minimum total squared error (TSEmut) versus number of optimal weights (IV) in the equalizer 
example of Figure 8.10 for delay values 1, 2, and 3, illustrating the effect of the delay value.

218
Digital Signal Processing with Examples in Matlab
FIGURE 8.13
System identification example,.in which B(z) becomes a model of the unknown system when 
the total squared error is minimized.
As noted in Section 8.6, the covariance elements in the matrix R should 
approach scaled values of the correlation coefficients, as K increases. 
Because fk is a white random sequence with independent samples and unit 
power, we have
f 0; n * 0
«>//(") = t n 
(8.35)
[ 1; n = 0
Thus, the theoretical autocorrelation matrix is diagonal, that is, <6,7 - I the­
oretically, and from Table 8.1, the optimal weights are given by the cross­
correlation vector; that is,
b = <pf. 
(8.36)
Now let us multiply the transfer-function relationship for U(z) in Figure 8.13 
by the conjugate of the transform of f to obtain
F'(z)D(z) = F'(z)F(z)U(z) 
(8.37)
Using <hyrf(z) and Oy/z) to represent the z-transforms of the corresponding 
correlation functions, we can apply (8.23) to (8.37) and, noting that <p,j is a 
unit impulse, and therefore, d-h/z) = 1, obtain
®/a(z) = <t»#(z)U(z) = U(z) 
(8.38)
Therefore, (p^, and thus the theoretical value of b, is the impulse response of 
U(z), which, as we saw in (8.31), is
^(theoretical) = e’’1" Sin (0.1 K ff) 
(8.39)

Least-Squares System Design
219
FIGURE 8.14
Signal vectors/ and d in the system identification example.
Thus, with a broadband input signal, the least-squares weight vector b 
tends to match a finite portion of the impulse response of the unknown 
system. One might suppose that a very large length (K) of the random input 
sequence would be required for the covariance solution to match the corre­
lation solution, but in fact, a short sequence suffices. As an example, the 
sequences of length K = 64 illustrated in Figure 8.14 were used to derive a 
least-squares model of size N - 50. These were produced with the following 
expressions:
K=64;
randn('seed' , 0) ;
f=randn(1,K);
c = exp(-.1);
d=filter[oc*sin(.l*pi)],[1 -2*c‘cos(.l*pi) <T2] , f);
(8.40)
The least-squares weight vector, b, is computed using the first three expres­
sions in (8.32), with N = 50, and compared in Figure 8.15 with samples of 
the continuous plant impulse response, that is, the theoretical weights in 
(8.39). The two plots are nearly identical, and the minimum TSE in this 
example is on the order of 10, indicating a nearly perfect model. Notice that 
the FIR filter length N = 50 covers essentially all of the nonzero portion of 
the impulse response of the unknown system.

220 
Digital Signal Processing with Examples in Matlab
FIGURE B.1S
Impulse responses of the model, B(z), and the unknown system. In this noiseless example, the 
model is nearly perfect in the sense that the TSE is nearly zero.
8.9 Interference Canceling
Our next example illustrates the interference-canceling concept in Figure 8 4 
in the communication situation shown in Figure 8.16. A caller is speaking 
in a construction environment, where the acoustic noise is so loud that, when 
it is picked up by his telephone and added to his speech, the combined signal 
becomes unintelligible to the listener. Luckily, a DSP engineer is on hand to 
design the least-squares noise canceler, which operates by processing corre­
lated noise picked up on a microphone in the same environment. The noise 
canceler works by subtracting the processed correlated noise, gk, from the 
combined signal, sk + nk.
The reason for the necessity to subtract the noise instead of using a noise­
canceling filter is shown in Figure 8.17. A 1-second sample of the speech 
signal, s(f), is shown at the left along with the power density spectrum of 
the entire signal. The construction noise, n(t), and its spectrum are on the 
right. The sampling rate in both cases is 40 KHz. We can see that filtering 
the combined signal, s(f) + n(t), to improve the signal-to-noise ratio (SNR) 
might be possible to some degree but would also result in the loss of some 
of the signal. If s(f) and n(t) are independent, the configuration in Figure 8.16 
should be a better choice for improving the SNR.

Least-Squares System Design 
221
FIGURE 8.16
Least-squares interference canceling. Loud shop noise is added to the speaker's voice. Using a 
correlated version of the noise without the speech, the interference-canceling filter, B(z), pro­
duces an output that cancels most of the noise. The filter weights are chosen to minimize the 
total squared value of the output, ek.
FIGURE 8.17
Speech signal, s(t), and noise, n(t), with power density spectra, for the interference-canceling 
example. The signal in this case is around 20 dB below the noise level.

222 
Digital Signal Processing with Examples in Matlab
500 .■
500 
--------- >--------------- i---------------->--------------------------------1---------------J
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5
t(s)
FIGURE 8.18
A segment of signal plus noise, illustrating a poor signal-to-noise ratio.
t(s)
FIGURE 8.19
Segments of noise and correlated noise.
The combined signal is shown in Figure 8.18, in which we can observe 
that the low SNR in this case tends to obscure the speech component almost 
completely to the eye. The human ear is more adept than the eye at sepa­
rating the two components, but even a good listener would lose most of the 
speech when this much noise is added.
Short (0.1-second) segments of the noise, n(f), and correlated noise,/(f), 
are shown in Figure 8.19. In this idealized example, the correlated noise is

Least-Squares System Design
223
FIGURE 8.20
Minimum TSE versus number of weights (N) in the interference-canceling example, indicating 
that the SNR increases only slightly as N increases beyond 4 or 5.
a linear transformation of the noise. In reality, we must usually accept at 
least a small degree of independence between the measured noise and the 
actual noise added to the signal.
As in previous examples, the optimal weight vector computation is 
described in Table 8.2. The required Matlab expressions, in which N is the 
number of weights, are
Rf f=autocovar_mat(f,N);
Rfd=crosscovar(f,s+n, N) ; 
b=Rff\Rfd;
(8.41)
The TSE, computed as in Table 8.2 for increasing values of N, is plotted in 
dB in Figure 8.20. The TSE is seen to decrease slowly after N - 4 or 5. In 
practice, because quantities such as speech quality or image quality require 
subjective judgments, the choice of N may best be made on the basis of "best 
listening" or "best looking," rather than on the basis of the TSE.
In any case, we can see that least-squares interference canceling is poten­
tially capable of producing a large increase in the SNR. The improvement is 
illustrated in the final figure, Figure 8.21, which shows first the noisy signal, 
s(t) + „(t), then the system output, e(t) = s(i), for N - 5 and N = 12, and finally, 
the original speech signal, s(t). Thanks to DSP, the listener in Figure 8.16 is 
able to get the message.
8.10 Linear Prediction and Recovery
In Chapter 4 (Figure 4.8), we discussed how inverse filtering could be used 
to recover a filtered signal. In Figure 8.22, we illustrate an application of 
inverse filtering together with linear prediction in Figure 8.1. In this appli­
cation, when B(z) is chosen to minimize the total squared error, the predictor 
removes predictable (correlated) components from the input d, leaving a

FIGURE 8.21
Final plot for the interference-canceling example, showing (1) the received signal without 
canceling, (2,3) the received signal with canceling using N = 5 and 12 weights, and (4) the 
original speech signal without noise.
FIGURE 8.22
Waveform compression and recovery using least-squares prediction and inverse prediction. In 
this application, the weight vector, b, and the prediction error, e, contain ail the information 
needed to reconstruct the original signal, d, except possibly for roundoff errors,
smaller residue, e. The residue becomes the compressed or encoded version 
of d, and the operation is called linear predictive coding.
The recovery of the original signal, d, from e is shown on the right in 
Figure 8.22. The recovery follows the description of inverse filtering in Chap­
ter 4, and (4.32) in particular. Recovery requires only knowledge of the 
weight vector, b, and the prediction error vector, e. With the exception of 
roundoff errors, d and d should be identical.
An example of predictive compression and recovery is given in Figures 8.23 
through 8.25. The upper part of Figure 8.23 is a plot of 40,000 samples of a 
seismic event recorded by the USGS Seismological Laboratory at Albuquer­
que, NM. The sampling rate was 20 samples/s, so the plot spans 2000 s. The 
lower part of Figure 8.23 is a plot of the prediction error using a predictor

Least-Squares System Design
225
FIGURE 8.23
Short-period seismic event recorded by the USGS Albuquerque Seismological Laboratory, NM, 
and prediction error using a least-squares predictor with N = 6 weights. The entire prediction 
error, e, is shown with the exception of e(l: N). Note the different vertical scales on the two plots.
FIGURE 8.24
Power density spectra of the seismic waveform (upper plot) and the prediction error without 
startup values (lower plot). The vertical range is the same in both plots to emphasize the 
whitening effect of least-squares linear prediction.

226 
Digital Signal Processing with Examples in Matlab
■5--------------------- ---------------------- ,--------------------- 1----------------------,
0 
500 
1000 
1500 
2000
“ 05'
j
tn
3-0.5-
q
cd
-1L 
0
500 
1000 
1500 
2000
t(s)
FIGURE 8.25
Reconstructed signal, df and reconstruction error, d - d. In this example, even though compu­
tations are made with a floating-point processor, the accuracy is sufficient to produce a perfect 
reconstruction.
with N — 6 weights. Notice the difference in vertical scales on the two plots. 
The prediction error is seen to be much smaller and gives the appearance of 
being more random than the upper plot.
It is remarkable, but not unusual, that a such a small linear predictor is 
capable of subtracting so much content in this way from a recorded signal. 
The reason for prediction working so well on many instrumentation and 
other waveforms is not hard to find. In Section 8.4, we saw that two weights 
were required to cancel a sinusoidal component at a given frequency with 
arbitrary amplitude and phase. Thus, it is reasonable that, when the input 
signal is nonwhite and is concentrated in one or a few narrow frequency 
bands, a small linear predictor can remove a large part of the signal and 
leave a small, white residue.
This whitening effect is seen in the power spectra of d and e in Figure 8.24. 
The spectrum of d is seen to be concentrated at low frequencies and to have 
a large total power. The spectrum of e, on the other hand, has a relatively 
small total power and, when plotted over the same range of 150 dB, looks 
almost flat (white).
As indicated in Figure 8.22, recovery requires knowledge of the weight 
vector, b, and the prediction error, e. The recovery process is an application 
of inverse filtering described in Chapter 4, (4.32). The reconstructed signal, 
d, and the reconstruction error, d-d, are plotted in Figure 8.25. In this case, 
the reconstruction is exact, and the error is zero.

Least-Squares System Design 
227
The recovery operation and the predictive coding operation are both accom­
plished easily using Matlab functions we have already described, as follows, 
in this case with K - 40,000 and N = 6:
Predictive Coding
Recovery
f=[0; d(l:K-l)];
Rff=autocovar_mat(f,N); 
rfd=crosscovar(f,d,N);
b=Rff\rfd;
e=d-filter(b,l,f);
dhat=filter(1,[1 -b‘1,e)
Without going into too much detail, we can easily see the application to 
signal compression in this example. If integer samples of the original wave­
form, d, are stored or transmitted without the use of any other coding 
scheme, we can see in Figure 8.23 that a range of 106 is required, that is, 
20 bits/sample. Because d consists of 40,000 samples, 800,000 bits are 
required. If the prediction data is used instead, then we need to store or 
transmit the weight vector, b, and the prediction error, e. The first N values 
of e (where N is the number of weights) require 20 bits each, but afterwards, 
as seen in Figure 8.23, a range of only 200, or 8 bits/sample, is required. 
Thus, the total number of compressed data bits with N = 6 in this example is
Compressed bits = 6(20) + 6(20) + 39994(8) = 320192 
(8.42)
The compression ratio in this case is about 2.5, meaning that 2.5 times as many 
bits are required to describe the signal in its original form, compared with 
the compressed version.
Predictive coding has been applied in this manner to many kinds of signals 
and data, including speech and image data, telemetry, etc., and applied in 
many areas besides compression. Encryption was mentioned in Chapter 4, 
for example, where the weight vector becomes the encryption key, and the 
prediction error is sent in the clear.
Another application,.used in spectral estimation, is called prewhitening. If 
a power spectrum is estimated using the methods described in Chapter 7, 
and if the signal contains one or more large narrowband components, these 
may bias the power density estimates at other frequencies. If so, the pre­
whitening operation may be used to reduce the bias. Suppose x is the signal 
vector, and y is the prediction error after passing x through a least-squares 
linear predictor. Then the transform relationship is
Y(z) = (l-B(z))X(z) 
(8.43)
Taking the squared magnitude of both sides of (8.43) and then applying 
(8.23), we have 
.
Y'(z)Y(z) = |l-B(z)|2X'(z)X(z), or ®yy(z) = |l-B(z)|23>„(z) 
(8.44)

228 
Digital Signal Processing with Examples in Matlab
From this, we conclude
= 
(8-45)
Thus, the power spectrum of x may be estimated from the spectrum of the 
whitened signal, y, and the power gain of the least-squares predictor.
8.11 Effects of Independent Broadband Noise
In many practical design cases, noise is added to the signals with properties 
that are used to design least-squares systems, that is, to the vectors d and f 
in Figure 8.6. The noise could be plant noise added to d as shown in Figure 8.2, 
or independent channel noise associated with the unknown channel and 
added to/as shown in Figure 8.3, or uncorrelated noise added to either s or 
/in Figures 8.4 and 8.16, and so on. Our purpose, in this section, is to derive 
the general effects of noise in least-squares systems, assuming the noise is 
independent, has a broad spectrum, and is additive, that is, adds to the signals. 
These assumptions are somewhat restrictive, but nevertheless, cover many 
cases of interest, and the approach used here is applicable with modifications 
if the noise is not independent or does not have a flat power spectrum.
Tire noisy least-squares design configuration is illustrated in Figure 8.26.
It is the same as Figure 8.6 with noise added to the signals/and d. Either of 
the two noise sequences, n or m, or both, may be present depending on the 
application of Figure 8.26. If the noises are independent of the signals, and
mk
FIGURE 8.26
The essential components of the least-squares system with independent broadband noise, 
and nt, added to the signals 
and /t, respectively.

Least-Squares System Design 
229
of each other, the cross-correlation and cross-spectral terms are zero. That is, 
for example,
= E[(/#+ «*)(/*+,-+ n*+,)]
= E[AAJ + E[n^,] = <pff(L) + <p,m(i) 
(8.46)
The second line in (8.46) follows, because, if/and n are independent, the 
expected cross-products in the first line are zero. Furthermore, when the 
noise is white, as we will assume here, <p„„(i) is zero except at i - 0; hence 
the corresponding autocorrelation matrix, is diagonal.
Therefore, if we use the noisy version in Figure 8.26 in place of Figure 8.6, 
we replace the autocorrelation functions with noisy versions as in (8.46), 
leaving the cross-correlation functions unchanged, because n and m are 
independent. The result is the following modification of the formula (8.4) 
for the MSE in terms of the weight vector b:
N-l N-l 
N-l 
N-l
MSE = %rf(0) + -p,„,„(0)+ y 
- 7) +
i-0 j-0 
i=0 
1=0
(8.47)
Note that the third term in this expression is the same as the second term 
in (8.4), and the fourth term results from diagonal autocorrelation matrix, 
<!>„„, of the white noise.
The result in (8.47), and all that follows here, is essentially the same using 
either correlation or covariance functions. In terms of covariance, Table 8.3 
shows how the covariance equations in Table 8.2 are modified to include 
additive white noise.
From the formulas in Table 8.3, we reach the following conclusions:
• Adding independent white noise to the input signal (/) is equiv­
alent to adding a constant to the diagonal of Rff, the autocovariance 
matrix.
* Adding independent noise to the desired signal (d) simply adds a 
constant to the total mean-squared error and to its minimum value, 
TSFm:,.
TABLE 8.3
Least-Squares Covariance Equations under Noisy Conditions (Figure 8.26)
Total-squared error (TSE) TSE = tJO.O) + r„„„(0,0) + b'(Rf) + r„„(0,0)I)b - 2b'r/d
TSE=d'*dtm'*m+b’*(R_ff+n'*n*I)*b-2 *b1*r_fd
Optimal weights 
b = (Rl{ + ^(O.O)!)"1^
b=ff+n'*n*I)\r_fd
Minimum TSE 
TSEm,„ = r*,(0,0) + r„„„(O,0) - r'fib
TSEmin=d'*d+ra*m'-r_fd’*b

230 
Digital Signal Processing with Examples in Matlab
If Rff, the autocovariance matrix, is a diagonal matrix, then cross terms of 
the form bjbj with i do not appear in the MSE and TSE formulas, as seen, 
for example, in (8.47) with <Pff(n - m) = 0 for n a m, and hence, the error 
contours (in Figure 8.9, for example) are circular. Furthermore, the <pn„(0) 
term in (8.47) increases the b2 terms in the MSE formula, thereby causing the 
sides of the bowl-shaped error surface to become steeper. Therefore, we also 
conclude the following:
• Adding independent white noise to the input signal tends to 
steepen the sides of the performance function and to make its 
contours more circular.
In Table 8.3, we also note that the optimal coefficients with input noise are 
modified by having the increased diagonal elements in the autocovariance 
matrix. We note that if the input is dominated by the noise sequence n, then 
the optimal coefficients are just scaled values of the cross-covariance terms; 
that is,
b = rfd/(p„„(£) 
(8.48)
Furthermore, in the limiting case where the input is entirely independent 
noise, the cross-correlation terms are zero, and therefore, the weight vector, 
b, is zero. We also observe in Table 8.3 that the solution for optimal coeffi­
cients is not affected by the noise m added to the desired signal, which only 
affects the TSE by adding a constant, r„„„(0, 0).
Thus, in these results, we have expressed the effects of independent broad­
band noise on least-squares system design. The results in Table 8.3 for least­
squares design using covariance functions, and similar results with correla­
tion functions, indicate that least-squares system design is a valid and useful 
process, even if random noise is added to the signals involved.
8.12 Exercises
1. In a least-squares interference-canceling system, suppose the filter 
B(z) has two weights, b0 and bv that the total power in the desired 
signal power is cr), and that the correlation functions are 
and (pdf{n).
a. Express the mean-squared error as a function of and bv
b. Express the optimal weights, b0 and bv in terms of the correlation 
functions.
c. Express the minimum mean-squared error in terms of the signal 
statistics.

Least-Squares System Design 
231
2. In Exercise 1, suppose = ^(1) = 0.
a. What are the optimal weights?
b. What relationship between the signal statistics must hold in 
order for the minimum MSE to equal zero?
3. Consider a general sinusoid, xk =■ A sin(a>kT + a), sampled at K sam­
ples per cycle.
a. What is the average power in xk?
b. Express the autocorrelation function of xk, and show that it is 
independent of the phase, a.
c. Show that the autocorrelation function of xk has a period equal 
to that of xt.
4. A long sequence, xk, with a period equal to eight samples is
xk = [..., 1,1,1, 1, -1, -1, -1, -1, 1, 1,...]
Plot xk and the autocorrelation function, for -15 < n < 15.
5. If the samples of two sequences xk and yk are complex, the correlation 
function (pxy(n) is defined to be <pxy(n) = E[x[ylT„]; that is, <A,,(n) is 
the average product of the conjugate of xk times yu„.
a. Show that (pxy(n) agrees with the previous definition when xt is 
a real sequence.
b. Show that <p'xy(n) = <pyx(-n).
c. Express <p„(n) when xk is a sinusoidal function, xk = Ae'{2lA’N+ai. 
Show that <pxx(n) does not depend on the phase of xk and has a 
period equal to that of xk.
6. A sequence x has samples given by xk = A cos(®fc + a) + rk, where 
rk is a uniform white random variate with variance cfr. Express the 
autocorrelation function, (pxx(n).
7. Suppose xk and yk are sinusoidal signals, either real or complex, 
with frequencies a)T and (i)y. What condition must hold in order for 
<pXIJ(n) to be zero for all values of n?
8. The predictor in Figure 8.27 is a specific example of Figure 8.1 with 
delay m = 2.
a. Let sk = «/2 cos(2^c/15 + n/A). Determine matrix O, and vector 
<p,s for this example.
b. Express the mean-squared error as a quadratic function of b0 
and by.
c. What are the optimal values of b0 and fry?
d. What is the minimum mean-squared error, E[ej]?
9. Do Exercise 8 with sk = sin(2rfc/15) + rk, where rk is a uniform 
white random sequence in the range [-1, 1]. Then replace rk with 
artand plot the MSEmin vs. a in the range [0,10].

FIGURE 8.27
Two-step predictor.
10. In Exercise 8, how are matrix ft>and vector <pf, altered if we replace 
(bQ + f^z"1) with (b0 + bkz ”!), where i is a positive integer greater than 
one?
11. Give a simple expression for the MSE in (8.4) when the input signal, 
f, is a white random sequence not necessarily independent of d.
12. A least-squares system has an adjustable "bias weight," which 
causes the constant c to be added to the filter output, as shown in 
Figure 8.28. The bias weight is used in cases where one of the 
signals has a nonzero mean value and the other does not. Suppose 
E[/J = ty’ = 0, B(z) is a highpass filter, and the weight vector, 
b, is optimized. What is the optimal value of c in terms of /.(,■?
13. The noise-canceling system in Figure 8.29 is used to cancel sinuso­
idal interference from a broadband signal. The signal is represented 
as shown by a filtered version of rk, where rk is a uniform white 
random sequence with <7, = 2.0. The noise is nk = sin(2?tfc/31), and 
reference signal (correlated noise) is fk = 2cos(2^fc/31).
a. Find the power density spectrum of sk and the total signal power, 
o?-
b. Write an expression for the MSE, E[e*], assuming sk and nk are 
independent.
c. Find the optimal weights, b0 and bt.
d. Show that the minimum MSE equals <fs.

Least-Squares System Design
233
Noise
FIGURE 8.29
Noise-canceling system.
FIGURE 8.30
System with two weights.
14. For the situation described in Exercise 8, draw contours for MSE = 
1, 0.8, 0.6, 0.4, and 0.2 on the b0 - b, plane. (Note: the Matlab contour 
function is useful for this purpose.)
15. A least-squares filter is to be used to adjust the amplitude and 
phase of fk to match the amplitude and phase of d*. as shown in 
Figure 8.30, where f. and dk are described as follows:
fk = 5sin(2fffc/32 + ^/4); dk = sin(2^/32)
a. Find b0 and bt assuming the signals are unbounded.
b. Using correlation functions with sequence lengths K = 8, 16, 32, 
and 64 and periodic extension, print b0, blr and TSEmin for each 
value of K. Compare the results with part (a) and explain.
16. Do Exercise 13 using covariance functions. Find b0, by and TSEmin 
for K = 10, 100, 1000, and 10,000. Compare these results with the 
answers to Exercise 13(c) and (d), and explain.
17. Solve the following sets of linear equations:
a. b0 + 2bj + 3b2 = 8; 4bp 4- bj + 6b2 = 17; 9b0 + b1 + b2 ~ 19.
b. 4bg 4* 3bj 4- 2b2 4- b2 — 2; 3bg + 4b] + 3b2 + 2b2 = 0; 2bg 4- 3b\ 4- 4b2 4- 
3b2 = 0; 4- 2b, 4- 3b2 4- 4b2 ~ —2.

234 
Digital Signal Processing with Examples in Matlab
FIGURE 8.31
Least-squares FIR filter design.
c. Five equations of the form B = 
in which the correlation
functions are (pff(n) ~ cos(2zrn/15) and tpyrf(n) = 2sin(2?rn/15).
d. bc + b1 = 1; b0 + b2 = 4; b0 + b3 = 0; bt + b4 = 2; bt + b2 = 3.
18. In this exercise, we use the modeling configuration in Figure 8.13 to 
design a lowpass FIR filter. The concept is illustrated in Figure 8.31. 
The input signal, fk, is composed of N sine waves at frequencies 
evenly distributed between zero to half the sampling rate, that is,
N
ft = > sin ——- ; k - 0, 1,...,2N 
<2N + 1J
(=1
The desired (real) gain characteristic, H(v), is applied to each of 
these components, and the filtered version of fk is then delayed to 
produce dk.
a. Write an expression for dk similar to the expression for/. above.
b. Write an expression for
c. With M = 20 and N = 199, find the optimal weight vector, b, the 
minimum TSE, and the ratio of TSEmin to the maximum TSE.
d. In the upper of two subplots, plot the first halves of the DFTs 
of/and d vs. v in Hz-s.

Least-Squares System Design
235
e. In the lower subplot, make a continuous plot of the filter 
amplitude gain, |B( v)|, vs. v in Hz-s.
19. Do Exercise 18 using the covariance method with signal vectors 
just long enough to cover one fundamental period. Comment on 
any differences in the results, especially in the minimum TSE.
20. Suppose the least-squares FIR filter in Figure 8.6 is a linear-phase 
filter of the following form:
N-l
B(z) = b0 + £fe„(z" + z~")
a. Express the error signal, ek.
b. Express the MSE, E[ej;], in terms of correlation functions. Is the 
MSE a quadratic function of the weights?
c. Derive a set of linear equations to solve for the optimal weights.
21. Design the least-squares equalizer in Figure 8.10 with U(z) given by
1
U(z) = ------ i -g~ 2
1-1.82Z + 0.81Z 2
For sk, use the impulse function shown in Figure 8.11, and use K = 50 
samples. Make a plot of the minimum TSE vs. the number of 
weights similar to Figure 8.12, and explain the results.
22. Design the equalizer in Exercise 21 with N = 3 weights and delay 
m = 0. For the input signal vector s(l:K), use a white Gaussian 
sequence with cr2 = 1 and K = 50. Compare your solutions for b f 
and TSEmin with the corresponding solutions in Exercise 21.
23. The equation error method for least-squares HR system identification 
is illustrated in Figure 8.32. Note that the method produces a com­
bined least-squares system consisting of B(z), a forward model of 
P(z), and 1 - A(z), an inverse model of P(z). The method could be 
applied to model a plant with poles and zeros, to design an HR 
filter using the method described in Exercise 18, to equalize a 
channel with zeros as well as poles, and so on.
a. Assume/is a finite signal vector. In terms of z-transforms, show that 
the overall transfer function from/to e„ is HJz) = P(z) (1 - A(z)) - B(z).
b. Now suppose the same plant, P(z), is used in Figure 8.13, and 
the model in that figure, B(z), is replaced with a recursive model 
given by
H(z) =
() l-A(z)

236 
Digital Signal Processing with Examples in Matlab
FIGURE 8.32 
' ‘
HR least-squares system identification.
FIGURE 8.33
Direction finder.
Express E,(z) in Figure 8.32 in terms of E(z) in Figure 8.13, and 
prove the following:
E,(z) = (l-A(z))E(z)
Thus, if the model in Figure 8.32 is perfect in the sense that 
e k = 0, then ek = 0 in Figure 8.13 as well. Therefore, as the error 
approaches zero, Ff(z) above becomes a valid least-squares HR 
model of the plant in Figure 8.32.
c. Show how to minimize the MSE, first with a fixed and then with 
b fixed.
d. Let P(z) = H(z) in Chapter 4, Exercise 5. Iterate, solving for [b0, bj 
with a fixed and then [av a2l with b fixed, until B(z)/(1 - A(z)) 
converages to P(z). Plot all five weights versus the iteration 
number as they converge to the weights in P(z).

Least-Squares System Design
237
FIGURE 8.34
Least-squares nonlinear modeling.
24. A simple direction-finding system is shown in Figure 8.33. A plane 
wave traveling at velocity c m/s arrives with direction angle 0 at 
two receptors spaced xfl meters apart, such that
sok = A cos
su = A cos
where A = (xosin ff)!c s, c being the speed of light. The second signal, 
Sjk, is sent through a quadrature filter, in which there is a 90-degree 
(nine-sample) phase delay from b0 to bv
a. Express the optimal weights in terms of the signal direction, 0.
b. Describe how the system could be used as a direction finder, 
that is, to acquire a signal and determine the direction 0.
c. Comment on the effects of adding mutually independent white 
noise at the two inputs.
25. A nonlinear modeling system is shown in Figure 8.34. The plant 
being modeled is nonlinear, and the system attempts to construct 
a least-squares polynomial model of the plant. In this case, the degree 
is 2, but it is easy to see how higher degrees of/) could be appended 
to the bottom of the figure and added to gk. A bias weight could 
also be added as in Exercise 12 to complete the polynomial.
a. Derive an expression for the TSE similar to (8.18) for Figure 8.34, 
and show that the error surface is a quadratic function of the 
weights having a single global minimum.
b. Prove that the solution for optimal weights amounts to solving 
a set of linear equations.
26. In Exercise 13, suppose uniform white noise with power equal to 
3 is added to the reference input.
a. Write the revised expression for the MSE.

238 
Digital Signal Processing with Examples in Matlab
b. Find the optimal weights.
c. Compute the minimum MSE and compare the result with of.
References
1. Haykin, S.S., Adaptive Filter Theory, 2nd ed., Prentice Hall, Englewood Cliffs, NJ, 
1991. 
'
2. Orfanidis, S., Optimum Signal Processing: An Introduction, Macmillan, New York, 
1985. 
'
3. Widrow, B. and Stearns, S.D., Adaptive Signal Processing, Prentice Hall, Englewood 
Cliffs, NJ, 1985.
4. Blahut, R., Fast Algorithms for Digital Signal Processing, Addison-Wesley, Reading, 
MA, 1985. 
... 
.
5. Saywood, K., Introduction to Data Compression, Morgan Kaufman, San Francisco, 
CA, 1996.
6. Jain, A., Fundamentals of Image Processing, Prentice Hall, Englewood Cliffs, NJ, 1989.
7. Rabiner, L.R. and Schafer, R.W., Digital Processing of Speech Signals, Prentice Hall, 
Englewood Cliffs, NJ, 1978, chap. 8.
8. Franklin, G.F. and Powell, J.D., Digital Control of Dynamic Systems, Addison- 
Wesley, Reading, MA, 1980.
9. Marple, S.L., Digital Spectral Analysis with Applications, Prentice Hall, Englewood 
Cliffs, NJ, 1987.
10. Kay, S., Modern Spectral Estimation: Theory and Application, Prentice Hall, Engle­
wood Cliffs, NJ, 1987. 
' '
11. Atal, B.S. and Hanauer, S.L., Speech analysis and synthesis by linear prediction 
of the speech wave, J. Acoust. Soc. Amer., 50, 637, 1971.
12. Bordley, T.E., Linear predictive coding of marine seismic data, IEEE Trans. Acoust. 
Speech Signal Process., ASSP-31, 828, Aug. 1983.
13. Clark, G.A. and Rodgers, P.W., Adaptive prediction applied to seismic event 
detection, Proc. IEEE, 69, 1166, Sep. 1981.
14. Zeidler, J.R. et al., Adaptive enhancement of multiple sinusoids in uncorrelated 
noise, IEEE Trans. Acoust. Speech Signal Process., ASSP-26, 240, June 1978.
15. Itakura, F. and Saito, S., Digital filtering techniques for speech analysis and 
synthesis, Proc. 711' International Congress on Acoustics, 261, 1971.
16. Makhoul, J., Spectral analysis of speech by linear prediction, IEEE Trans. Audio 
Electroacoust., AU-21, 140, June 1973.
17. Lucky, R.W., Techniques for adaptive equalization of digital communication 
systems, Bell System Tech. J., 255, Feb. 1966.
18. Gersho, A., Adaptive equalization of highly dispersive channels for data trans­
mission, Bell System Tech. J., 55, Jan. 1969.
19. Ives, R., Near lossless compression of SAR imagery using gradient adaptive 
lattice filters, Proc. 35lk Asilomar Conf, on Signals, Systems, and Computers, Nov. 
2001. 
'
20. Griffiths, L.J., Smolka, F.R., and Trembly, L.D., Adaptive deconvolution: a new 
technique for processing time-varying seismic data, Geophysics, 742, June 1977.
21. Widrow, B. et al., Adaptive noise canceling: principles and applications, Proc. 
IEEE, 63, 1692, Dec. 1975.

Least-Squares System Design 
239
22. Kailath, T, Ed., Special Issue on System Identification and Time Series Analysis, 
IEEE Trans. Automatic Control, AC-19, Dec. 1974.
23. Widrow, B., Titchener, P.P., and Gooch, R.P, Adaptive design of digital filters, 
Proc. ICASSP'81, 243, Mar. 1981.
24. Wiener, N., Extrapolation, Interpolation and Smoothing of Stationary Time Series with 
Engineering Applications, Wiley, New York, 1949.
25. Levinson, N., The Wiener RMS error criterion in filter design and prediction, 
J. Math. Phys., 25, 261, 1946.
26. Zohar, S., Toeplitz matrix inversion: the algorithm of W. F. Trench, J. Assoc. 
Comput. Mach., 16, 592, Oct. 1969.


Adaptive Signal Processing
9.1 Introduction
In this chapter, which introduces a type of system we have not yet consid­
ered, we will be relying on the discussions of linear least-squares systems 
in Chapter 8. Adaptive signal processing systems, as introduced here, are 
time-varying versions of the systems in Chapter 8, in which the weights are 
varied to allow the systems to adapt to trends or variations in the statistical 
properties of nonstationary signals.
Adaptive signal processing systems appear in many different forms, 
including all of the different applications mentioned in Chapter 8. The diver­
sity has been promoted by developments in microelectronics that have 
greatly increased the amount of computing possible in real-time DSP sys­
tems. Seismic signals at 102 Hz and below, speech and acoustic signals at 102 
to 105 Hz, electromagnetic signals at 10s Hz and beyond, signals in control 
systems at all frequencies, as well as other similar signals appearing in time 
and space are all reasonable candidates for adaptive signal processing.
The adaptive notion derives from the human desire to emulate living systems 
found in nature, which adapt to their environments using various remarkable 
methods that we can copy only in very limited ways. In the development of 
adaptive systems, the journey from concept to reality has taken us away from 
direct emulation, somewhat like the development of manned aircraft. An 
adaptive signal processor usually resembles its natural counterpart about as 
much as, or if anything less than, an airplane resembles a bird.
Adaptive signal processing has its roots in adaptive control and in the 
mathematics of iterative processes, where the first attempts were made to 
design systems that adapt to their environments. In this chapter, we focus 
on the basic theory and applications of modem adaptive signal processing. 
Much of what we describe here is based on the work of Bernard Widrow 
and his colleagues,1"3'5'6'12'13'18'24'30'35'37'40 which began around 1960. We use 
Widrow's development of the geometry of adaptation,5 and we introduce 
the least-mean-square (LMS) algorithm as the simplest and most widely 
applicable adaptive processing scheme.
241

242
Digital Signal Processing with Examples in Matlab
►
Filter with
variable weights
FIGURE 9.1
Essential elements in adaptive signa] processing. These are the same as the essential elements in 
least-squares design, except.that here the filter weights are adjusted continuously to minimize the 
squared error, <?(.
Adaptation involves a process known as performance feedback, in which the 
performance of a system, measured in terms of the same squared error we 
used in Chapter 8, is used to adjust the adaptive weights. The weights may 
be adjusted at each step in the sampling process, or after N steps in what is 
known as block adaptive processing.
In any adaptive signal processing system, we are usually able to identify 
the basic structure and signals in Figure 8.5 or Figure 8.6, depending on 
whether the adaptive filter is recursive or not. Thus, we begin with the struc­
ture and signals in Figure 9.1. The variable weights are adjusted repeatedly, 
at regular intervals, in accordance with an adaptive algorithm. The adaptive 
algorithm usually uses, either explicitly or implicitly, the signals shown in 
Figure 9.1, that is, the input signal, fk, the desired signal, dk, and the error 
signal, ek, which is the difference between dk and gk, the filtered version of/fc. 
The details of the adaptive algorithms are given in the following sections. 
The main point here is that the weights are adjusted continually during 
adaptation to reduce the mean-square error, E[c^], toward its minimum 
value. Thus, the adaptive system seeks continually to reduce the difference 
between the desired response, dk, and its own response, gk. In this way, as 
we shall see, the system "adapts" to its prescribed signal environment.
In this chapter, we can only hope to introduce some of the basic ideas and 
applications of adaptive signal processing, because the subject is sobroad. Texts 
devoted to this subject, some of which are listed in the References, are recom­
mended for better coverage. The text by Haykin4 has an especially comprehen­
sive and complete treatment of adaptive signal processing and filter theory.
9.2 
The Mean-Squared Error Performance Surface
In Chapter 8, Section 8.3, we saw that the mean squared error (MSE) was 
a quadratic function of the samples, [hk], of the impulse response of the 
least-squares filter. The function was derived in (8.4), which is repeated

Adaptive Signal Processing 
243
here:
N-1N-1 
N-l
MSE = 
(9.1)
ns=O m=0 
n=0
To keep the quadratic form and have a finite filter, in Chapter 8, we set the 
impulse response equal to b, the weight vector of an FIR filter, as we have 
done here in (9.1). Later in this chapter, we will consider the consequences 
of using an adaptive HR filter.
In adaptive systems, (9.1) describes what is called a quadratic performance 
surface,5 meaning that the performance of an adaptive system may be 
described in terms of the geometry of this surface, which turns out to be a 
helpful vehicle for visualizing the adaptive process. Examples of two ways 
to view the performance surface with two weights were given in Chapter 8, 
Figures 8.8 and 8.9.
The key equations describing the performance surface were given in Table 
8.2 in terms of correlation functions, that is,
Mean-squared error: MSE = <pM(0) + b'&ffb - 2b'<pfd 
(9.2)
Optimal weights: b = <Pfd 
(9.3)
Minimum MSE: MSEmin = <Parf(0) - (fab 
(9.4)
Under stationary conditions, the correlation functions are constant properties 
of the signals. If, as we assume in this chapter, the signals are nonstationary, 
then we may view the correlation functions as changing with time. In (9.2), 
we can see that this implies an MSE, that is, a performance surface that 
moves and changes shape as the correlation functions change. The goal of 
the adaptive signal processor is then to track these changes and continuously 
adjust the elements of the weight vector (b) so the MSE is kept near its 
minimum value.
In the sections that follow, we will consider methods for searching the 
performance surface and tracking the movement of the minimum MSE. The 
most general adaptive algorithms for searching performance surfaces under 
unknown signaling conditions are random search algorithms. One type of 
random search algorithm24 selects a random direction in the performance 
space in which to look at each iteration and then moves in directions that 
happen to reduce the MSE. An even more general random search algorithm25 
selects random locations on the weight plane (or hyperplane), estimates the 
MSE at these points, and retains points with the lowest MSE from one 
iteration to the next.
In this chapter, we will focus on the two most common methods for 
accomplishing such adjustments in real-time adaptive systems; namely, 
steepest descent and sequential regression. These methods work best with 

244 
Digital Signal Processing with Examples in Matlab
quadratic error surfaces, and therefore, are applicable to FIR adaptive filters, 
including the equation-error structure described in Chapter 8, Exercise 23. 
However, these are by no means the only methods that work, especially with 
other types of filters and nonquadratic error surfaces.
9.3 Searching the Performance Surface
With the types of systems and the quadratic performance surfaces we have 
been discussing, as well as with similar performance functions that are 
continuous and have continuous derivatives, we can implement powerful, 
deterministic (that is, nonrandom) adaptive search algorithms. In this section, 
we provide a basis for these algorithms in terms of the parabolic MSE in 
(9.2), viewing the MSE as a function of the N elements of the weight vector, b, 
that is, viewing the MSE as a performance surface in (N + l)-dimensional 
space. As discussed previously, in adaptive processing, we also view the 
performance surface as moving and changing shape, so the algorithms must 
continually adjust b to track the minimum MSE.
We will see that to track the minimum MSE in this manner, deterministic 
search algorithms, in one way or another, must employ an estimate of the 
gradient of the MSE performance surface. The gradient of the MSE is a vector 
with elements that are derivatives with respect to the weights; that is, similar 
to (8.5),
y „ <9 MSE _ a MSE MSE 
a MSE 
(95)
db0 dbY dbN_Y
We can write the MSE gradient as the vector form of (9.5) by differentiating 
the MSE in (9.2) with respect to b. To differentiate the second term in (9.2), 
let <5(n) represent a vector of length N, with the rath element equal to one 
and all other elements equal to zero. Then, we have
N-i
= 
+ 
(9.6)
w-0
The final result is due to the symmetry of that is, - m) - ®^(ra: - ra). 
Thus, the gradient vector, which is the derivative of (9.2) with respect to b, 
is given by
V = 2<I>/-2^ 
(9.7)
Setting V = 0 produces the solution in 9.3 for optimal weights.

Adaptive Signal Processing 
245
Now we consider a weight vector, b(k), as a function of its location in the 
sample space. We will assume sampling takes place in time, so b(k) is the 
weight vector and V(k) the gradient vector at time kT in the time domain of 
the adaptive processor. We denote the optimal weight vector in (9.3) as bopl. 
If we multiply (9.7) by and add the result to (9.3), we may obtain the 
following "one-step" algorithm for moving from an arbitrary point on the 
performance surface to the minimum MSE in a single step:
bopl = 
(9.8)
This result is essentially Newton's root-finding method4' applied to find the 
zero of a linear function (V in this case). It can also be derived by writing 
the MSE in the form of a Taylor series, as in the literature.43 Given any weight 
vector b(k) along with Cr.and the corresponding gradient V(k), we can move 
from b(k) to the optimum weight vector bopl in a single step by adjusting b 
in accordance with (9.8).
If we could apply (9.8) in practical situations such as those illustrated in 
Figures 8.1 to 8.4 in the previous chapter, we would always be able to adapt 
in one iteration to the optimal weight vector. As with natural systems, how­
ever, practical adaptive signal processing systems do not have enough infor­
mation to adapt perfectly in just one iteration. There are two specific 
problems with (9.8) in practice, under nonstationary conditions:
1. The correlation functions change and can, at best, only be estimated.
2. The gradient must be estimated at each adaptive iteration, using 
local statistics.
In other words, we must work with estimated values of <I>j and V(/c) in (9.8).
To anticipate the use of noisy estimates of these values, we modify (9.8) 
to obtain an algorithm that adjusts b in smaller increments and converges 
to &opt after many iterations. The small increments have the effect of smooth­
ing the noise in the estimates of and V(Zc), thus allowing the adaptive 
system to have stable behavior. The modified version of (9.8), which allows 
us to compute b(k + 1) in terms of b(k) and a convergence parameter, it, is
b(k + 1) = b(k) - ii^V(k) 
(9.9)
We can examine the convergence of (9.9) by first subtracting 2u times (9.8) 
from (9.9) to obtain
b(k+l) = (1-2it)b(k) + 2u&opt 
(9-10)

246
Digital Signal Processing with Examples in Matlab
Taking the z-transform of (9.10) and solving for B(z), we have
zB(z) = (l-2ti)B(z) + ^^;
" <9-n)
= (z - l)(z - (1 - 2m))
Taking the inverse transform (Table 4.2, line 6), we have
b(k) = [1 - (1 - 2u/]Bopl 
(9.12)
Here we have a formula that begins at b(0) = 0 and converges (conditionally) 
to bopt at k = =». Because the formula follows a linear path on the b-plane, we 
may move the starting point to &(0) by substituting b(k) - b(0) for b(k) and 
bopt - &(0) for bopt. Then (9.12) becomes
b(k) = &Qpt + (1 - 2U/(b(0) - feopt) 
(9.13)
Here we have a description of the relaxation of the weight vector from b(0) 
to bopt under ideal conditions, where <1^ and V(k) are known at each step.
In (9.13), we can see that b(k) will converge to i’op, only if the geometric 
ratio 1 - 2m is less than 1 in magnitude; that is,
For convergence under ideal conditions: 0 < u < 1 
(9.14)
In (9.13), the one-step solution is found at k = 1 with u = 1/2. With u > 1/2 
tire convergence is oscillatory, with b jumping back and forth across the bowl 
and converging toward i?opt. In practical adaptive systems with noise, values 
of u well below 1/2, typically on the order of 0.01, are used, giving a con­
vergence time constant of many iterations. We define tire convergence time 
constant to be r iterations, where the relaxation toward bopt goes in proportion 
to (1 - e~klT). Then, using the linear approximation (1 - 1/r) for e”1/r when r is 
large, and comparing it with (9.13), we have
1 
1 - (1 -1/t)1
= 1-(1-2m/ 
(9.15)
Thus, we conclude that 1/t = 2w, and we have the following rule:
Time constant for convergence of weights under ideal conditions: 
Tr„ = i iterations; 0 < u « 1/2
(9.16)
In addition to weight convergence in (9.13), the convergence of the MSE to 
its minimum value is also often used as a performance measure in adaptive 

Adaptive Signal Processing 
247
systems. A plot of the MSE vs. the iteration number Ic is euphemistically 
called a learning curve.5 To derive a formula similar to (9.13) for the learning 
curve, it is convenient to use a translated weight vector, c, that allows the 
minimum MSE to occur at copl = 0:
c = b-bopt 
(9.17)
Then, noting that = <p'fllb, the MSE in (9.2) becomes
MSE = <pdd(0) + (c + bopl)'<I>jf(c + bopl)-2<pj;d(c + bopt)
= <PmW + ^opt^^opt - 2^(bopt + c'<bffc + c'<S#bopt + 
- 2<p'fJc
(9.18)
We can simplify this result in two steps. First, if we substitute (9.3) for the 
first occurrence in the second line of b'op, in (9.18), the first three terms become 
equal to MSEmin in (9.4). Second, again using (9.3) in the last three terms, we 
can substitute ®fbopf - and also, because is symmetric, Kpi®,.. = <?«• 
Therefore, the last three terms sum to zero, and (9.18) reduces to
MSE = MSEmin + c'<t>ffC 
(9.19)
This is the simplest expression for the quadratic performance surface, given 
in terms of coordinates normalized so that the optimal weight vector, copt, is 
zero.
If we substitute (9.17) into (9.13), we have a simpler formula for the relax­
ation of the weight vector from a starting value, c0, to its optimum value, 
that is, to zero:
c(k) = (1 - 2w/c(0) 
(9.20)
Substituting (9.20) for c in (9.19), we have a formula for the learning curve, 
that is, the relaxation of the MSE to its minimum, under ideal conditions:
MSEk = MSEmin + (1 - 2u)V(0)<I>#c(0) 
(9.21)
Comparing this with (9.20) or (9.13), we see that the convergence ratio is 
now (1 - 2u)2 instead of (1 - 2u). Thus, similar to (9.16), the time constant for 
the learning curve is
Time constant for convergence of the MSE under ideal conditions:
rMSE = 
= iterations; 0 < u « 1 / 2
wise 2 
4»

248 
Digital Signal Processing with Examples in Matlab
50 iterations; u=0.05
0 
0.5 
1 
1.5 
2 
2.5 
3 
3.5
FIGURE 9.2
Weight convergence in the predictor described in Chapter 8, Section 8.4, with error contours 
illustrated in Figure 8.9. Convergence is in accordance with (9.13), with u = 0.05, beginning at 
b(0) = [1.00, -2.75], and continuing to b(49).
Examples of weight convergence and a learning curve are given in Figures 9.2 
and 9.3. These examples illustrate ideal convergence of the weights as 
described by (9.9), beginning at b(0) = [1.00; -2.75] and using u — 0.05, the 
latter being well below the one-step convergence value, u = 0.5. The exam­
ples are for the prediction example in Chapter 8, Section 8.4, with perfor­
mance surface illustrated in Figures 8.8 and 8.9. In Figure 9.2, note the 
straight, noise-free track from !t(0) to b(49), which is close to feopl, and also 
the translated coordinates of c with origin at i?opr With u = 0.05, the weight 
convergence time constant is approximately 10 iterations in accordance with 
(9.16), and we note in Figure 9.2 that there is convergence after five time 
constants, or 50 iterations.
In Figure 9.3, we observe the corresponding exponential decrease of the 
MSE from its starting value of 5.8 toward the minimum value, which is zero 
in this case. The MSE time constant, tmse, is approximately five iterations in 
accordance with (9.22), and we observe in Figure 9.3 that the approximation 
is valid. After four time constants, or 20 iterations in this example, the MSE 
has relaxed to about 1.1% of its original value, which is close to 5.8e"4.
In this section, we discussed the process of searching the quadratic per­
formance surface. The principal formulas, (9.9) and (9.13), describe how,

Adaptive Signal Processing 
249
6<f
50 iterations; u=0.05.
0 
10 
20 
30 
40 
50
k
FIGURE 9.3
Convergence of the MSE ("learning curve") corresponding with weight convergence in Figure 9.2. 
under ideal conditions, the weight vector changes at each step and con­
verges finally to its optimum value.
We must now examine how adaptive systems reach convergence under 
realistic conditions, with imperfect knowledge of the gradient and correla­
tion properties of the performance surface, which may change from one 
iteration to the next. First, we will assume that we have little knowledge of 
the correlation properties and can only estimate the input signal power, that 
is, This assumption leads us to the steepest-descent class of algorithms 
and, in turn, to the widely used Widrow-Hoff LMS algorithm, which is the 
simplest and most useful algorithm in adaptive signal processing.
9.4 
Steepest Descent and the LMS Algorithm
A steepest-descent adaptive algorithm is a formula like (9.9) for step-by-step 
adjustment of the weight vector, but it is unlike (9.9) in that it does not move 
along a straight track in the weight plane (or hyperplane) as in Figure 9.2, 
but rather follows the path of steepest descent down the error surface, 
perpendicular to the MSE contours in Figure 9.2.
One of the goals in developing such an algorithm is to revise (9.9) by 
removing the correlation information in and yet be able to control con­
vergence as we did in (9.9) by restricting u to the range (0,1/2). To accomplish 
this, we recall that is a square matrix with rows and columns that are 

250 
Digital Signal Processing with Examples in Matlab
permutations of the autocorrelation vector, <pff. If <t>T is nonsingular so that 
•TC,1 exists, which is the case in all our applications, the eigenvalues of ‘by are 
developed from the following equation,5 in which A, is a scalar, I is the identity 
matrix, and q„ is a column vector (not a single element) with N elements, q„0, 
Clnlr---' Cjn.N-V
= [0...0]' 
(9.23)
This equation has nontrivial solutions for and if the following deter­
minant vanishes:
, . detA„I] = 0 
(9.24)
This is called the characteristic equation of <bp. Its N solutions, designated 
Ao,^,...,/^!, are the eigenvalues of <Py. Corresponding with each distinct 
eigenvalue, A,„ there is an eigenvector, q„, satisfying (9.23). In (9.23), we note 
that q„ may be normalized (multiplied by a scalar) so that
q'„q„ = 1; n = 0, 1,...,N-1 
(9.25)
We will assume the eigenvalues are distinct, and the eigenvectors are nor­
malized in this manner.
For each eigenvalue we may express the equality in (9.23) by writing
<&ffqn = 
n = 0, 1,..., N - 1 
(9.26)
From this expression, we can show that the N eigenvectors are mutually 
orthogonal. We take two versions of (9.26), one for q„, and the other for q„:
We multiply the first expression by q'„ and the second by q'„,:
qm^ffqn = ^nq>nqn
We transpose the first of these expressions, recalling that 
The result
is the same as the second expression; that is,
— 
~ ^mqnqm
Because the expressions are equal, and because z,„ and are different, we 
conclude that
q'mq>, = q'nqm = 0; n * m 
(9.27)
and thus, the eigenvectors are mutually orthogonal.

Adaptive Signal Processing 
251
Next, we form the eigenvector matrix, Q, as a square matrix with q„ as the 
nth column, and also A, a diagonal matrix with as the nth diagonal 
element:
Q = [<7o fli ■■■ 
A = diag[Z0 ^ ... AN_J (9.28)
With these definitions, (9.26) may be applied N times, once for each column 
of Q, to give
0>#Q = QA; or = QAQ’1 
(9.29)
This is called the normal form of the autocorrelation matrix in which the 
eigenvalues appear explicitly. Furthermore, because (9.25) is true for each 
column of Q, and because the columns of Q are mutually orthogonal, it 
follows that
QQ' = Z; that is, Q' = Q’1 
(9.30)
and therefore, another expression of the normal form of A>f/is
= QAQ', or A - Q'<H#Q 
(9.31)
The inverse of the normal form is easy to find, because A is diagonal, and 
so A 1 is also diagonal with elements 1/A,,; that is,
A’1 = diagfl/Ao l/A, ... l/A^.J 
(9.32)
Furthermore, for any two matrices A and B with inverses, (AB)”’ = B_1A-1. 
Therefore, using (9.29), the inverse of (9.31) is
= (QT’(QA)-1 = QA-1Q' 
(9.33)
With the normal form and its inverse, we may now discuss the convergence 
of a steepest-descent algorithm starting with a revised version of the ideal 
weight recursion formula (9.9), which contains no information about the 
input correlation function, tpff:
b(k+ 1) - &(k)-qV(fc) 
(9.34)
First, we translate (9.34) using (9.17) so the optimal weight vector is copt = 0:
c(k + 1) = c(k) 
(9.35)

252
Digital Sigrtal Processing with Examples in Matlab
where Vc is the gradient vector translated so the gradient is zero at c - 0. 
Next, we substitute (9.7) for the translated gradient (noting that (p^ = <fybopt):
c(k+l) = c(k')-2p<t>ffc(k') = (I - 2p&ff)c(k) 
(9.36)
Our next step is to rotate the c-coordinates to new coordinates, v, using the 
eigenvector matrix, as follows:
c = Qv, or Q'c = v 
(9.37)
The latter version follows from (9.30). Now, (9.36) becomes
Qv(k + 1) = (I -2p<5//)Qu(k) = (Q- 2p<fyQ)v(k)
Multiplying by Q' and substituting (9.31), we have the normalized form of 
the ideal convergence formula:
v(k + l) = (Q'Q - 2pQ'<t>ffQ)v(k)
= (I-2/zA)u(k) 
(9.38)
In this normal form, because the off-diagonal elements of A are all zero, we 
now have N independent convergence formulas:
p„(k+l) = (1-2/d,,.)^); « = O,1,...,N-1 
(9.39)
Here, similar to (9.10) or (9.13), the geometric convergence ratio is (1 - 2/1A,,), 
and so we see that the convergence criterion is now
For steepest-descent convergence: 0 < p < 1//^ 
(9.40)
where is the maximum eigenvalue (corresponding with fastest conver­
gence). Similarly, the time constants for convergence under ideal conditions, 
in (9.16) and r..,BE in (9.22), may be modified as follows to reflect conver­
gence along the principal axes:
Steepest descent: tu,„ = z—z-;
tmse» - d 3 '■ 0<n<N 
(9.41)
Note that the slowest descent (longest time constant) corresponds with the 
smallest eigenvalue, ArrlJJ1, which tends to dominate the latter part of the 
convergence process, as will be seen in examples that follow.
Concerning the range of p in (9.40), we do not yet have a practical conver­
gence criterion, because 2^ in (9.40) is known only if g^is known. Hence, our 

Adaptive Signal Processing 
253
final step in the development of a steepest-descent algorithm is to express 
the sum of the eigenvalues in terms of a local estimate of signal power and 
use this in place of /Lmax in (9.40) to provide an upper limit on p.
Now the sum of the diagonal elements of any square matrix A is called 
the trace of A, or Tr(/1). Thus, the trace of A, which is a diagonal matrix, is 
the sum of the eigenvalues, and the trace of ©y is the sum of the N diagonal 
elements; that is,
N-l 
N-l
Tr(A) = 
Tr(^) = XW<0) = N«M°) 
(9.42)
n=0 
;i=0
These two traces are equal. The demonstration of equality can be made using 
an interesting property of the trace, namely, that the trace of the product of 
two square matrices is the trace of the reverse product; that is, if A and B 
are square matrices, then, using the definition of the product AB in (1.7) with 
the definition of the trace in (9.42),
N-l N-l 
N-l N-l
Tr(AB) = X = X = Tr(BA) <9-43) 
1=0 n=0 15=0
The equality of Tr(A) and Tr(Oy) follows when we use (9.31) in Tr(A) and 
apply (9.43) along with (9.30):
Tr(A) = Tr(Q'O#Q) = Tr(©z/Q'Q) = Tr(©#) 
(9.44)
Thus, the trace of A—the sum of the eigenvalues—is equal to the trace of 
©y—the sum of the diagonal elements of the autocorrelation matrix, each of 
which is <jt>y(O) as in (8.6); that is,
N-l
Tr(A) = X4 = W#(0) = N(72f 
(9.45)
n-0
Here, we use cr) to represent the variance, or average power, of the signal/, 
which is the same as <p^(0) in (7.30). If Oy changes during the adaptive process, 
then it must be adjusted along with the weights. But at least <7y(fc) is easy 
to estimate locally in time as the average squared signal amplitude (one 
estimation method is described in the next section), and so we now have a 
practical version of the convergence criterion in (9.40) for steepest-descent 
algorithms. To be consistent with the ideal weight convergence described by 
(9.9), we set
p = u/N<fyk) 
(9.46)

254
Digital Signal Processing with Examples in Matlab
Then, the steepest-descent algorithm in (9.34) and the convergence criterion 
in (9.40) produce the following result:
Steepest descent: b(k+l) = b(k~)-------—V(fc); 0<u<l
Ncfyk)
(9.47)
With this result, we are ready to derive the LMS algorithm, which was first 
published by Widrow and Hoff’ and has since become the most widely applied 
algorithm in adaptive signal processing. All we need is a local estimate of 
the gradient vector, which the LMS algorithm supplies. We recall that the 
ideal gradient in (9.5) and (9.9) is the derivative of the MSE with respect to 
the weight vector, b. In the Widrow LMS algorithm, the local estimate of the 
MSE is the squared error, e*. First, we define the input signal vector of length 
N, consisting of the most recent N samples of the input signal, f, in reverse 
order:
= A-i ••• A-n+i]' 
(9.48)
Using (8.1) and (8.3) in Chapter 8, with b(k) as the current weight vector, our 
expression for ek is
e, = dk-b(k)'f(k) 
(9.49)
(Remember here that subscript k denotes the kth sample, and argument k 
denotes the kth vector.) Using (9.49), Widrow's estimate of the gradient vector 
for use in the LMS algorithm is
V(fc) = ^ = 
= -2eJ(fc) 
(9.50)
Using this gradient estimate in place of V(k) in (9.47) produces the LMS 
algorithm:
LMS algorithm: b(k + 1) = b(k) + —0<m«1 
’ 
Nof(k)
(9.51)
The condition w « 1 is in (9.51) to emphasize that w is small in practice 
(sometimes on the order of 0.1 and often much less) depending on how fast 
the signal environment is changing. The LMS gradient estimate causes the 
weight vector to converge on a noisy version of the steepest descent path, 
as we will see in the first example in the following section.
Convergence of the LMS weights and mean squared error may be approxi­
mated by the steepest-descent time constants in (9.41), withfa. specified in (9.46).

Adaptive Signal Processing
255
In general, as stated previously, the convergence of the LMS algorithm is 
governed increasingly as the MSE approaches MSEmin by the minimum eigen­
value, 2^, which corresponds with the longest convergence time constant. 
For a conservative description of LMS convergence, we substitute 2min for /L„ 
and (9.46) for u into (9.41) and obtain these LMS convergence time constants:
LMS Convergence: rm = N°f ; rMSE = 
; 0 < w « 1 
(9.52)
Thus, convergence of the LMS algorithm is slower than ideal convergence 
when 2min is less than the average eigenvalue. If the adaptive system is 
designed with unknown, which is usually true, one must plan for the 
worst case. For example, suppose the input to an adaptive filter with two 
weights has a sinusoidal component at v Hz-s. Then, it is easy to show 
(Exercise 9.13) that the ratio of Amln to the average eigenvalue is
- 1 - cos(2rrv)
Thus, other factors being equal, convergence times for the LMS algorithm 
are longer at lower frequencies.
9.5 LMS Examples
Our first example of adaptive weight convergence using the LMS algorithm 
is illustrated in Figure 9.4, which repeats Figure 9.2 for one-step prediction of 
a sine wave with two weights and ideal convergence, and also includes the 
weight track for LMS convergence. The adaptive gain is u = 0.05 for both weight 
tracks; however, convergence requires about 50 iterations on the ideal track 
and 350 iterations on the LMS track. On the LMS track, we can observe erratic 
direction and step size due to inaccuracy in the gradient estimate (9.50) and 
the steepest-descent weight track, which is orthogonal to the MSE contours.
A plot of e2k vs. k in this example is shown in Figure 9.5. The plot is 
equivalent to the learning curve described in Section 9.3, which is a plot of 
E[e* ] vs. k. It covers 51 iterations of the LMS algorithm and is seen to 
converge toward the minimum MSE (zero) over this range. Convergence is 
comparable to the ideal convergence plotted in Figure 9.3. The latter is plotted 
as a dashed line for comparison with LMS convergence in this example.
The error signals used for the LMS track in Figure 9.4 and the learning 
curve in Figure 9.5 were generated using a Matlab function supplied with

256 
Digital Signal Processing with Examples in Matlab
Ideal and LMS covergence; u=0.05.
FIGURE 9.4
Comparison of ideal and LMS weight convergence for the predictor with two weights (Figure 9.2). 
The number of iterations is K — 50 for ideal convergence and 350 for LMS convergence.
this text called lms_ filter:
[g,b] - Ims_filter(b,u,d, f)
Referring to Figure 9.1, this function produces vector g as the result of 
filtering/and adapting at each step using the error, ek = dk - gk. The weight 
vector b(k) is updated and stored at each step, so that the entire weight track 
is stored as an output. This is a simple implementation of the LMS algorithm 
in which cf is held fixed. It is useful for producing results like those in 
Figures 9.4 and 9.5 but may need to be modified for other uses.
Our next example is an illustration of why adaptive signal processing may be 
preferred with nonstationary signals. We will keep the same, simple two-weight 
predictor described in Chapter 8, Section 8.4, and illustrated in Figure 8.7. 
To have a simple nonstationary signal, we use a single "frame" of signal dk, 
which is shown in the upper plot in Figure 9.6. The frame consists of 1000 
samples of a unit sine wave with frequency changing from v = 0.05 to v = 
0.10 Hz-s at sample k = 750. We assume that, prior to the time of this frame, 
the frequency of dk has been constant at v = 0.05 Hz-s. Now, suppose we are 
using a two-weight predictor to compress this signal and wish, therefore, to 
produce a small residue, et. If fixed optimal weights are computed using the

Adaptive Signal Processing
257
FIGURE 9.5
2
Comparison of convergence of (connected points) using the LMS algorithm with ideal 
convergence of the MSE (dashed line) for the prediction example in Figure 9.3. The adaptive 
gain is u = 0.05 in both cases.
FIGURE 9.6
Nonadaptive and adaptive signal compression using a one-step predictor with two weights. 
The frequency of the signal, doubles at k = 750. The fixed (nonadaptive) predictor has weights 
optimized for the entire frame. The adaptive predictor has adapted previously to the first 
frequency, so its average prediction error (residue) is much smaller.

258 
Digital Signal Processing with Examples in Matlab
method of Chapter 8, the residue, shown in the center plot in Figure 9.6, 
cannot be reduced to zero due to the frequency change in dk. On the other 
hand, if an LMS adaptive predictor has previously adapted to the initial sine 
wave and again adapts to the frequency change in this frame, the residue 
remains close to zero over most of the frame, as shown in the lower plot in 
Figure 9.6. The only noticeable residue is where the change occurs at k = 750. 
In this particular example, the nonadaptive total squared error (TSE) is 7.6, 
and the adaptive TSE is only 0.2, indicating a much smaller average residue 
and, hence, significant compression of the signal.
If an adaptive filter is used in signal compression, the question of recovery 
arises. Can the original signal be recovered from the residue? The answer is 
yes, and with no additional information except the adaptive gain constant, 
u. Recovery proceeds as described in Chapter 4, (4.32), and as discussed in 
Chapter 8, Section 8.10. The only difference is that the LMS algorithm must 
be applied at each step to adjust the weight vector during recovery. It is easy 
to see in the formulation (9.51) of the LMS algorithm that the adjustment is 
realizable, because at each step, the weights for the next step are computed 
in terms of quantities that are available during recovery.
9.6 Direct Descent and the RLS Algorithm
Comparing the steepest-descent formula (9.47) with the ideal formula (9.9), 
and comparing the weight tracks in Figure 9.4, we may note that the presence 
of <5^ in (9.9) has the effect of steering the weights away from the direction 
of the gradient to a direct path leading to the minimum MSE. Thus, it is 
reasonable to look for a formula like (9.51) that, in addition to estimating 
the gradient at each step, also estimates at each step. The Recursive Least 
Squares (RLS) algorithm is such a formula.
The RLS algorithm exists in different forms.4'6'45 The version we use here 
is based on an approximation to (9.9) of the form
b(k + l) = b(k) - u^’Vf/c)
The simplest local gradient estimate is the LMS gradient estimate (9.50). 
Substituting this estimate for V(k) and using for the current estimate 
of , we have
b(k + 1) = b(k) + 2u^ff\k)ekf(k) 
(9.53)
Let us now consider the problem of estimating the inverse autocorrelation 
matrix, <!>?. The information in Of is contained in N correlation elements, 
<Pff(0’.N - 1). With the exception of <%(0), which is the input power Op a 

Adaptive Signal Processing 
259
correlation value <Pff(n) = E[ fkfk.„] cannot be estimated without using a history 
of at least n samples. Thus, the simplest local estimate of <pr/»i) is the sample 
product/*/,.,,, and, using the definition of the fcth signal vector, fik) in (9.48), 
it follows that the simplest local estimate of <bff at the Jcth step is
M) = /(*)/'(*) 
(9.54)
In Chapter 8, where stationary signals were assumed, we used the avail­
able history, that is, the entire signal vector/, to estimate <^(n) as the average 
(or total) product. If/is nonstationary such that changes slowly, then it 
is reasonable to use only the recent history of / to estimate <^. If the signal 
/(f) is locally periodic, it is reasonable to average over one or more periods. 
If the signal statistics are changing slowly with time, we use the following 
modification of (9.54), which gradually "forgets" the past history of the 
estimate:
= (1 - 
1) + af(k)f'(k); 0<a<l 
(9.55)
When a is close to zero, current events in/(f) have little effect on the estimate, 
and conversely, when a is close to one, current events in /(f) have a large 
effect on the estimate.
The recursive estimate (9.55) produces the correlation estimate as essen­
tially a lowpass-filtered version of the instantaneous estimate, f(k)f'(k). The 
lowpass transfer function is seen to be
H(z) = ---------------- j 
(9.56)
1 -(l-«)z 1
The corresponding impulse response is
hk = cr(l - a)‘; k>0 
(9.57)
and the correlation estimate is the convolution of hk with f(k)f\k); that is,
k
= '£hnf(k-n)fr(k-n)
n«0
k
= a£(l - a)"f(k-n)f(k-n) 
(9.58)
n^O
Here again, as in (9.13), we can see that when cris small, there is an exponential 
decay in the memory of the past correlation estimate, and the "forgetting 

260
Digital Signal Processing with Examples in Matlab
time constant" is
Time constant for forgetting the correlation estimate:
1 .. 
.. 
n _ q 
(9.59)
r. = - iterations; 0 < a« 1 
' 
'
if a
To eliminate the need for a matrix inversion at each step in (9.53), we now 
proceed to change (9.55) from a recursive estimate of 4>;/, to a recursive estimate 
of 4>y. First, we pre-multiply (9.55) by (k) and post-multiply by 6" (k - 1) 
to obtain
-1) = "(1 - a)fyf\k) + o%’(k)/(k)/'(k)<I>y (k -1) 
(9.60)
Next we post-multiply this result by/(k):
= 
(k)/(k)(l - a + (k - l)/(k)) 
(9.61)
To make this less complicated, we define
S(k) = <t^’(k-l)/(k); 
S'(k) = f'(k^ff(k-l) (9.62)
(The second form follows because <f>/ is symmetric and equal to its trans­
pose.) Now (9.61) becomes
S(k) = <t^(k)/(k)(l - a + of (k)S(k)) 
(9.63)
Note that the factor in parentheses here is a scalar. Dividing by this factor 
and post-multiplying by S'(k) in (9.62), we have
(’■«)
Our final step is to substitute a times this result for the last term in (9.60) 
and rearrange the terms to obtain
- dSJ (9-65)
J- 
\ 
J. 
i 
yi\ J J \ r\ J /
In this result, with the definition of S(k) in (9.62), we have an iterative 
algorithm for computing (k) at each step k. It is reasonable also to use 

Adaptive Signal Processing
261
the same smoothed estimate of total power, using either <p^(0) or the instanta­
neous squared value, fl, as the input to H(z) in (9.56). In either case, the equiv­
alent of (9.55) for the power estimate is
o^(k) = (1 - 
1) + afl 
(9.66)
Note that we could, if desired, replace fl, with the average diagonal element 
off(k)f'(k) as a current estimate of q^-(O).
Initial conditions must be specified for the functions in both of these recur­
sive estimation formulas. For cf)(O) in (9.66), we assign a value, say CT;-O, as an 
a priori estimate of the input signal power. Then o\c is also the a priori estimate 
of and, in the absence of any other correlation information, we may use 
<r^o I as the initial estimate of Gty. Thus, the initial conditions for (9.65) and 
(9.66) are
Initial conditions:
CT;(0) = (Tp
^(0) = -^1 
&f0
(9.67)
With these initial conditions, we also require starting values b(0) and/fO) for 
the weight vector and signal vector, respectively, as we required with the 
LMS algorithm. After startup, the RLS algorithm then consists of (9.62), 
(9.65), and (9.53) and may be summarized as follows:
RLS Algorithm:
S(fc) = 4>7/(fc-l)/(fc)
&#<*) = 0<u<<1
T Ca \ 
X v-v I UCJ A. J L? ( 
/ J
ek = dk-b'(k)f(k)
b(k+l) = b(k)+2u&ff(k)ekf(k); 0<u«l
(9.68)
Compared with the LMS algorithm (9.51), the RLS algorithm requires much 
more computation at each step and also requires the specification of two 
parameters, ctand u. However, it has the potential, provided 4^ (fc) is a good 
estimate of the inverse autocorrelation matrix, of producing faster conver­
gence to bopt, the optimal weight vector.
An example of RLS weight convergence is provided in Figure 9.7, which 
is like Figure 9.4 with the RLS weight track in place of the ideal weight track. 
Compared with the ideal and LMS tracks in Figure 9.4, the RLS track is more 
like the ideal track. It follows a more or less direct path to the optimal point, 
which, we recall, is at = [73,-1] in this example. The LMS track

262 
Digital Signal Processing with Examples in Matlab
RLS (o) and LMS (*) convergence
bo
FIGURE 9.7
Comparison of RLS and LMS weight convergence for the predictor with two weights (Figure 
9.2). The number of iterations was K = 150 for RLS convergence and 300 for LMS convergence. 
RLS parameters were [n, al = [0.02, 0.2], LMS adaptive gain was n = 0.05.
contains twice as may iterations, and, following the path of steepest descent, 
converges slowly as the gradient approaches zero.
A learning curve for the RLS algorithm is shown in Figure 9.8. The adaptive 
gain was set to u = 0.05 so the result could be compared with the LMS 
learning curve in Figure 9.5, and we can see that convergence is improved 
with the RLS algorithm in this example.
As in the case of Figures 9.4 and 9.5, the plots in Figures 9.7 and 9.8 were 
made using a Matlab function supplied with this text. The function is called 
rls_ filter and has arguments as follows:
[g,b] = rls_ filter (&, u, alpha, d,f)
This function is the same as the lms_ filter function described previously, 
except that it implements the RLS algorithm, and its arguments include the 
forgetting factor, a, used to adjust the correlation and power estimates.
Having seen these examples and having discussed and compared the LMS 
and RLS adaptive algorithms, the reader may wonder why, if one or the 
other of the algorithms does not converge as fast as desired, the adaptive 
gain cannot be increased until it does so. The answer is that the behavior of 

Adaptive Signal Processing 
263
50 iterations; 11=0.05, a=0.2.
9r
8-
7 ■
FIGURE 9.8
Comparison of convergence of (connected points) using the RLS algorithm with ideal con­
vergence of the MSE (dashed line) for the prediction example in Figure 9.3. The adaptive gain 
is u = 0.05 in both cases.
the system after convergence may suffer if u is too close to its stability limit 
(one). After convergence under stationary conditions, the weight vector, b(k), 
continues to "adapt," and wanders around under the error surface in the vicinity 
of its optimum, bopt. That is, due to inaccuracies in local estimates, the MSE 
climbs up the bowl-shaped quadratic error surface and is then driven back 
down toward MSEmin. The continuation of this process results in a deviation 
of the MSE from its minimum value, MSEmin, which is undesirable. Thus, 
there is a tradeoff between convergence speed and convergence accuracy, 
which we discuss in the next section.
9.7 Measures of Adaptive System Performance
The two principal measures of adaptive system performance are those we 
discussed in the preceding sections. First are the time constants for conver­
gence of the weights and the MSE, which measure the speed of adaptation. 
Second is a measure called misadjustment, which is a normalized measure of 
the average deviation of the MSE from its minimum value.
For convergence time constants, we have the weight and MSE time con­
stants, r„. and rMSE, which we derived in Section 9.3, (9.16) and (9.22), and in

264
Digital Signal Processing with Examples in Matlab
Section 9.4, (9.52):
Convergence time constants; 0 < u « 1:
1 1
Ideal conditions: t.=iterations; 
2u 
4m
r. 
j 
N(J2f . 
.
Steepest descent: tw = =—tMSE = -—iterations 
4MAmjn
(9.69)
(9.70)
Because these time constants are inversely proportional to u, why not 
increase it toward one and have a more responsive, faster-converging adap­
tive system? As we have seen, the answer is that in practical systems with 
noisy estimates of the gradient and/or the autocorrelation matrix, increasing 
u means increasing the excess M.SE after convergence. The excess MSE is the 
increase in the MSE above MSEmin after convergence due to these noisy 
estimates. From (9.19), the excess MSE may be written as
Excess MSE = MSE - MSEmin after convergence
= E[c'O#c]
For example, the noisy LMS gradient estimate in (9.50) causes the weight 
vector to be adjusted in the vicinity of MSE^ at each iteration after conver­
gence, and thus contributes to the excess MSE through variations in the 
translated weight vector, c, in accordance with (9.70).
To derive a simple and practical estimation of the excess MSE for the LMS 
algorithm, it is necessary to assume that these variations in c consist of small, 
independent jumps away from the optimal point at c = 0. With this approx­
imation, under certain conditions, the excess MSE is
Excess MSE = nN MSEmin; 0 < u « 1 
(9.71)
The conditions are that u must be small, the eigenvalues of 4>y must be 
approximately equal, and the excess MSE is small (on the order of 0.1 or 
less). The derivation of this useful approximation, which is lengthy and 
involves a number of assumptions that must be supported, is not included 
here. Derivations are provided in Widrow et al.18 and in Chapters 5 and 6 
of Widrow and Stearns.5
Instead of the excess MSE, we use a normalized measure of the excess 
MSE called misadjustment (M), defined as follows:
Misadjustment: M = 
= mN
MSE^
(9.72)
The misadjustment and the convergence time constants (9.69) constitute the 
principal performance measures for adaptive system performance.
The approximation in (9.72) is said to be good for values of M up to 0.144.
This assumption is investigated in Exercises 15 and 16. Both exercises

Adaptive Signal Processing 
265
FIGURE 9.9
White noise (nt) is added to dk in the adaptive predictor used in previous examples in order to 
produce a nonzero minimum MSE.
FIGURE 9.10
Result of Exercise 15, illustrating how the LMS algorithm drives the converged weight vector 
in the vicinity of the minimum MSE and causes misadjustment.
investigate (9.72) using the example we have been using throughout this 
chapter, which is the one-step predictor in Figure 8.7 with two weights and 
a sinusoidal input. The example is modified as shown in Figure 9.9 by adding 
white noise to dk in order to produce a nonzero minimum MSE. Notice that 
the adaptive filter can cancel the sine wave but is not able to cancel the noise, 
because the noise does not appear in fk.
Exercise 15 consists of starting the weight vector at bapt and running 2000 
iterations of the LMS algorithm with adaptive gain u = 0.05. The result is 
illustrated in Figure 9.10, where 2000 iterations of the weight vector are

266
Digital Signal Processing with Examples in Matlab
FIGURE 9.11
Result of Exercise 16, in which misadjustment is measured experimentally under stationary 
conditions with different adaptive gain values. The dashed line represents the theoretical 
approximation in (9.72).
plotted as points on the weight plane. The plot illustrates the typical random 
excursions of the converged weight vector in the vicinity of the minimum 
MSE, which are the cause of misadjustment.
Exercise 16 consists of starting the weight vector at fropl and making several 
independent runs of the LMS algorithm for each of several values of u, 
measuring the misadjustment in each case by averaging the squared predic­
tion error, e2k. The result is plotted in Figure 9.11, which shows the experi­
mental points with the approximation in (9.72). Even though the eigenvalues 
of which are [0.1340, 1.8660] in this example, are quite different and do 
not fulfill the second condition for the validity of (9.72), the result verifies 
(9.72) as a useful approximation. However, as the misadjustment increases 
above 0.1, M becomes increasingly larger than the approximation in (9.72).
If we assume the validity of (9.72) as well as the ideal time constants (9.69), 
which are only approximations for LMS and RLS convergence as we have 
seen, we may combine the formulas for M and rMSE to obtain
M=—; 0<w«l, l<M<0.1 
4fMSE
(9.73)
Although this formula does not apply in all situations, it nevertheless serves 
to summarize adaptive system performance. It demonstrates that, in adaptive 
signal processing, low misadjustment and fast convergence are desirable but 
conflicting requirements. Faster convergence means noisier performance with 

Adaptive Signal Processing 
267
a greater excess MSE. In fact, noting that approximately four time constants 
(4tmse) are required for convergence from an arbitrary MSE to MSEmin, we can 
restate (9.73) as follows?8
... ,. 
,, Number of weights(N) [0<u«l i .
Misadiustment (M) = ------------------* ,,—< 
(9.74)
Convergence time(4rM5E) [0<M<0.1
This is perhaps the simplest and most general statement one can make about 
the performance of linear adaptive systems that use algorithms like those 
we have been discussing in this chapter.
Beyond these considerations, at least at the time of this writing, the design 
of adaptive systems is less than an exact science. The number of weights 
(N), the adaptive gain (»), and the forgetting factor («) must be chosen with 
the performance requirements in mind, but once they are chosen, perfor­
mance in nonstationary conditions also is affected by unpredictable changes 
in signal statistics. The fact that adaptive signal processing systems work 
and survive in so many different applications, like the fact that biological 
systems do the same, is quite remarkable.
9.8 
Other Adaptive Structures and Algorithms
In this chapter, we introduced adaptive signal processing using an FIR filter 
as the adaptive filter and discussed the LMS and RLS adaptive algorithms. 
These today are the simplest and most widely applicable components of 
adaptive systems, but there are other kinds of components and applications 
discussed in the literature.
One type of application using the FIR filter we have not mentioned is the 
adaptive array,3/5,16 a form of which is illustrated in Figure 9.12. (A simple form
FIGURE 9.12
An adaptive array, consisting of a multiple-input adaptive system. To achieve a common goal, 
the adaptive filters are adjusted using a single error, ek.

268 
Digital Signal Processing with Examples in Matlab
of this was used in Chapter 8, Exercise 24.) The array consists of M. sensors, 
Sj through sM, each s,„ feeding its adaptive filter, B,„(z). The desired signal, 
dk, is derived in a manner that points the array at a signal source, or away 
from an interfering source, or in some other manner, in a process known as 
adaptive beamforming35 Systems like this, with more than one adaptive filter 
driven by a single error, are known as multiple-input adaptive systems.
When each of the adaptive filters in Figure 9.12 consists of just a single 
weight, the system is called an adaptive linear combiner.5'6 The adaptive linear 
combiner adapts in the same manner as the FIR filter, except that the elements 
of the input vector/are available simultaneously, that is, at the same time step. 
For example, the adaptive linear combiner could take the form of a linear array 
of photo sensors that moves relative to (scans) a visual image, or it could take 
the form of a planar array of sensors viewing a changing scene.
Other types of adaptive filter structures have been proposed. One is the ad­
aptive lattice,33~3:>'38'3M4 which is developed by allowing the weights, k and A, 
of one of the lattice structures described in Chapter 4, Section 11, to be adapted 
with a version of the LMS algorithm.
Recursive filters as well as nonlinear filter structures have been applied in 
adaptive signal processing.5'6'21 23 Some of the nonlinear structures, for exam­
ple, use neural networks consisting of networks of threshold elements that act 
like neurons. Recursive adaptive filters have not found a large number of 
applications, because, as discussed previously, the associated MSE functions 
are generally not quadratic and may contain local minima. But, recursive 
filters have poles, and their operations are, in general, reversible, so they 
remain candidates for adaptive systems in compression applications and 
system identification.
Other adaptive algorithms include special algorithms for adapting recur­
sive and nonlinear systems,6 including genetic algorithms,5'25 which use the 
evolutionary or "survival of the fittest" approach to minimize the MSE.
There is also a class of algorithms known as blind algorithms used in 
communications systems that adapt to changing signal or channel condi­
tions. In blind equalization,6 the desired signal, d, which is known at the source 
but not at the receiver, is approximated (guessed) at the receiver using 
portions or properties of the received signal, and is then used to modify the 
received signal. Blind equalization is used in adaptive communications 
receivers to reduce channel distortion as well as interfering noise and other 
signals, including echos of the desired signal.
9.9 
Exercises
1. In Section 3, the weight convergence formula (9.12) was proved by 
taking the z-transform of the recursion formula (9.10). Instead of 
using the z-transform, prove (9.12) by induction.

Adaptive Signal Processing 
269
A
FIGURE 9.13
Adaptive filter with two weights.
2. Examine the validity of the weight-convergence time constant (9.16) 
by making four subplots in the same figure with u = 0.05 in the 
first plot, and then 0.1, 0.2, and 0.4 in the rest of the subplots. In 
each subplot, complete the following:
t
a. Plot 1 e vs. k for k = [0, 5tro] using a dashed line.
b. Plot symbols at k - 
2t,„,..., 5t„,.
c. Plot the exact convergence, 1 - (1 - 2u) as a connected discrete 
plot. Comment on the validity of (9.16) with respect to each plot.
3. A simple adaptive filter with two weights is illustrated in Figure 9.13. 
The signals/and d are:
fk = sin(2nfc/N); dk = 2cos(2^fc/N)
a. Express the autocorrelation matrix as a function of N, the 
number of samples per cycle.
b. Express the cross-correlation vector, (fy, as a function of N.
c. Express the mean squared error (MSE) as a quadratic function 
of bQ and bk.
4. For the adaptive filter described in Exercise 3, complete the follow­
ing:
a. Write an expression for the gradient of the error surface.
b. Write an expression for the optimal weight vector.
5. For the adaptive filter described in Exercise 3, assume N = 10 and 
complete the following:
a. Calculate the optimal weight vector, bQpt.
b. On the b0 - bk plane, plot contours (curves of constant MSE) for 
N - 10 and the following MSE values: 0.0001, 1.0, 5.0, and 10.0. 
Use b ± 6 to define the limits of the b-plane. (Note: This plot is 
most easily done using the Matlab contour function.)

270 
Digital Signal Processing with Examples in Matlab
FIGURE 9.14
Adaptive linear combiner.
6. Suppose we modify Figure 9.13 to produce the adaptive linear com­
biner in Figure 9.14. Now there are two input signals instead of 
one, and no delays are involved. We assume stationary signals with 
the following statistics:
E[^] = 36, E[/tl = 2, E[/t] = 3, E[/u/n] = 1, 
E[/Udt] = 6, E[/2A] = 4
a. Express fIy and <p!d for this case.
b. Write an expression for MSE as a function of b0 and
C. Find the optimal weight vector, bopt.
d. Determine the minimum MSE.
7. For the adaptive linear combiner in Figure 9.14,
a. Find the eigenvalues, A, and Aj.
b. Find the eigenvector matrix, Q.
c. Plot MSE contours on the b0 - plane using MSE values [10, 
50, 100, 200],
d. In the same figure, plot translated (c) axes and rotated (v) axes. 
Note that the v axes are the principal axes of the elliptical MSE 
contours and thus indicate the steepest and least-steep descent 
paths on the MSE surface.
8. For the adaptive linear combiner in Figure 9.14,
a. Write an expression for the gradient vector as a function of the 
weight vector.
b. Starting with initial weight vector b(0) = [10; 10] and using param­
eters u = 0.1 and cf, equal the average input signal power in 
Exercise 6, compute the first five weight values on a path that 
converges directly to bopt.
c. Repeat part (b) using the path of steepest descent instead of the 
direct path.

Adaptive Signal Processing 
271
9. For the adaptive linear combiner in Exercise 3, complete the fol­
lowing:
a. Write an expression for the gradient vector as a function of the 
weight vector.
b. Starting with initial weight vector b(O) = [2; -8] and using param­
eters u - 0.1 and equal <pff (0,0) compute the first five weight 
values on a path that converges directly to i>opt.
c. Repeat part (b) using the path of steepest descent instead of the 
direct path.
10. In the prediction example in Section 9.5, Figure 9.6, the nonstation- 
ary input is described as a unit sine wave that is stationary at 0.05 
Hz-s for a long time and then doubles in frequency.
a. What is the optimal weight vector, bop„ for predicting the initial 
part of the signal?
b. What is the minimum MSE associated with (a)?
c. In the upper of three subplots, plot the complete data frame as 
it appears in the upper plot in Figure 9.6.
d. Operate the LMS algorithm on the data frame with 1000 samples 
with b initialized to bop[ and u = 0.2. Plot the residue after k = 0 
in the center subplot and observe that your plot looks the same 
as the lower plot in Figure 9.6.
e. Operate the RLS algorithm on the data frame with 1000 samples 
with b initialized to bo t, u = 0.2, and a = 0.001. Plot the residue 
after k = 0 in the lower subplot, and compare the result with the 
center plot.
11. An adaptive filter with two weights is used as in Figure 9.15 to 
minimize the squared error, e2k, with the following inputs at fre­
quency v — 0.1 Hz-s:
fk = cos(2?rvk); dk = 3cos(27rvk + tr/5); 0<k<°°
a. Find the autocorrelation matrix <I>^.
b. Find the eigenvalues of <6^ by hand using (9.24).
FIGURE 9.15
Adaptive filter with two weights.

272 
Digital Signal Processing with Examples in Matlab
c. Find the time constant for convergence of the mean squared 
error using the LMS algorithm with n - 0.1.
d. Plot a learning curve (MSE vs. k) for the LMS algorithm that 
begins at the mean squared value of dk and converges to zero 
over k = 0:99.
e. On the same graph, plot a learning curve using the average 
eigenvalue in place of Amin.
f. Run the LMS algorithm with signals d and f, beginning with 
weight vector b(0) = [0,0]', and plot e2 vs. k, again on the same 
graph. Note how a running average of this plot would run 
between the two theoretical curves.
12. Repeat Exercise 11, but this time, use the RLS algorithm with 
u = a = 0.1 and b(O') = [0,0]'. Plot theoretical learning curves for 
ideal convergence and convergence using the LMS algorithm. This 
time, use the range k — 0:50. Note the faster convergence of the RLS 
algorithm, and comment on the comparison of the squared error 
plot with the learning curves.
13. Suppose, as in the two preceding exercises, that the input to an 
adaptive filter with two weights is an arbitrary sinusoid at fre­
quency v Hz-s, say
fk = Acos(2ji:vk + y); 0<k<^>
a. Write an expression for the autocorrelation matrix <W.
b. Write a quadratic expression and solve for the eigenvalues of 
®ff
c. Prove that the ratio of the minimum to the average eigenvalue is
Amin/iU = l-cos(2sv)
d. Show (in terms of time constants) why the above expression 
implies increasingly nonideal convergence of the LMS algo­
rithm as v decreases.
14. Verify the result of Exercise 13 by plotting LMS learning curves for 
frequencies v = 0.1 and v = 0.2 Hz-s and k = 0:99, using the signals 
in Exercise 11 in the configuration of Figure 9.15.
15. This exercise illustrates misadjustment using the LMS algorithm 
with u ~ 0.05 and the configuration in Figure 9.9 with the following 
signals:
sk = V2sin(2^[0:2000]/12)'; nk = white Gaussian noise; a2 = 1
a. Set the random number seed to 123, and find the optimum 
weight vector using actual signals. Compare with the theoretical 
optimum in Figure 9.2, which was b t = [75/ -1].

Adaptive Signal Processing 
273
b. Plot the error contours on the weight plane as shown in Figure 9.10.
c. Run 2000 iterations of the LMS algorithm with u = 0.05, using 
bopl for the initial weight vector. Plot a point for the weight vector 
at each iteration, and complete the plot in Figure 9.10.
d. Compute the theoretical misadjustment for this case.
e. Compute the misadjustment using the error signal, ek, in this 
example, and compare the result with (d).
16. This exercise tests the theoretical misadjustment formula (9.72) 
using the LMS algorithm in a manner similar to Exercise 15, that 
is, with the configuration in Figure 9.9 and with the following 
signals:
sk = 72sin(240:2000]/12)';
nk = white uniform noise; cr* = 0.05, p.,, = 0
(Note: This exercise involves a total of 1.5 million LMS iterations. 
If your computer is too slow, use a compiled program to obtain the 
16 data points, and then plot these using Matlab.)
The objective is to start the LMS algorithm at bopt and obtain 
misadjustments experimentally at each of 16 values of u:
The experimental misadjustment at u = 0 is zero, because no adjust­
ment is allowed in this case. At each value of u greater than zero, 
make 10 runs, each with 10* LMS iterations. At the beginning of 
the nth run, set the weights to bop[ = [73,-1], and add a phase shift 
of 2m,v/10 to the argument of sk so the signal begins at a different 
point in its cycle. After all 10 runs, compute the MSE and the 
misadjustment (M) based on the theoretical minimum MSE. Finally, 
plot M vs. u, and obtain a result similar to Figure 9.11.
References
1. Widrow, B. and Hoff, M.E., Jr., Adaptive switching circuits, I960 IRE WESCON 
Convention Record, Part 4, 96, 1960.
2. Widrow, B. et al., Adaptive noise canceling: principles and applications. Proc. 
IEEE, 63, 1692, Dec. 1975.
3. Widrow, B. et al., Adaptive antenna systems, Proc. IEEE, 55, 2143, Dec. 1967.
4. Haykin, S.S., Adaptive Filter Theory, 2nd ed., Prentice Hall, Englewood Cliffs, NJ,
1991.

274 
Digital Signal Processing with Examples in Matlab
5. Widrow, B. and Steams, S.D., Adaptive Signal Processing, Prentice Hall, Englewood 
Cliffs, NJ, 1985.
6. Treichler, J. R., Johnson, C.R., Jr., and Larimore, M.G., Theory and Design of Adaptive 
Filters, Texas Instruments, Inc. and John Wiley & Sons, New York, 1987.
7. Honig, M.L. and Messerschmitt, D.G., Adaptive Filters, Kluwer Academic Pub­
lishers, Norwell, MA, 1984.
8. Atal, B.S. and Hanauer, S.L., Speech analysis and synthesis by linear prediction 
of the speech wave, J. Acoustical Soc. Amer., 50, 637, 1971.
9. Makhoul, J., Spectral analysis of speech by linear prediction, IEEE Trans. Audio 
and Electroacoustics, 21, 140, June 1973.
10. Landau, A.I., Adaptive Control: The Model Reference Approach, Dekker, 1979.
11. Youn, D.H., Ahmed, N., and Carter, G.C., Magnitude-squared coherence function 
estimation: an adaptive approach, IEEE Trans. Acoustics, Speech, and Signal Pro­
cessing, 31, 137, Feb'. 1983.
12. Widrow, B., McCool, J.M., and Med off, B., Adaptive control by inverse modeling, 
12th Asilomar Conference on Circuits, Systems, and Computers, 90, Nov. 1978.
13. Widrow, B., Shur, D., and Shaffer, S., On adaptive inverse control, 15th Asilomar 
Conference on Circuits, Systems, and Computers, 185, Nov. 1981.
14. Gersho, A., Adaptive equalization of highly dispersive channels for data trans­
mission, Bell System Tech. ]., 55, Jan. 1969.
15. Lucky, R.W., Techniques for adaptive equalization of digital communication 
systems, Bell System Tech. J., 255, Feb. 1966.
16. Monzingo, R.A. and Miller, T.W., Introduction to Adaptive Arrays, John Wiley & 
Sons, New York, 1980.
17. Cowan, C.F.N. and Grant, P.M., Eds., Adaptive Filters, Prentice Hall, Englewood 
Cliffs, NJ, 1985.
18. Widrow, B. et al., Stationary and nonstationary learning characteristics of the 
LMS adaptive filter, Proc. IEEE, 64, 1151, Aug. 1976.
19. White, S.A., A nonlinear digital adaptive filter, 14th Asilomar Conference on Cir­
cuits, Systems and Computers, 350, Nov. 1980.
20. David, R.A., IIR Adaptive Algorithms Based on Gradient Search Techniques, Ph.D. 
Thesis, Stanford University, Stanford, CA, Aug. 1981.
21. Feintuch, P.L., An adaptive recursive LMS filter, Proc. IEEE, 1622, Nov. 1976.
22. Larimore, M.G., Treichler, J.R., and Johnson, C.R., Jr., SHARP: an algorithm for 
adapting IIR digital filters, IEEE Trans. Acoustics, Speech, and Signal Processing, 
28, 428, Aug. 1980.
23. White, S.A., An adaptive recursive digital filter, 9th Asilomar Conference on Circuits, 
Systems, and Computers, 21, Nov. 1975.
24. Widrow, B. and McCool, J.M., A comparison of adaptive algorithms based on 
the methods of steepest descent and random search, IEEE Trans. Antennas and 
Propagation, 24, 615, Sep. 1976.
25. Etter, D.M. and Masakawa, M.M., A comparison of algorithms for adaptive 
estimation of the time delay between sampled signals, Proc. 1CASSP-81, 1253, 
Mar. 1981.
26. Ahmed, N. et al., A short-term sequential regression algorithm, IEEE Trans. 
Acoustics, Speech, and Signal Processing, 27, 453, Oct. 1979.
27. Bershad, N.J. and Feintuch, P.L., Analysis of the frequency domain adaptive filter, 
Proc. IEEE, 67, 1658, Dec. 1979.
28. Clark, G.A., Mitra, S.K., and Parker, S.R., Block implementation of adaptive 
digital filters, IEEE Trans. Circuits and Systems, 28, 584, June 1981.

Adaptive Signal Processing 
275
29. Clark, G.A., Parker, S.R., and Mitra, S.K., A unified approach to time- and 
frequency-domain realization of FIR adaptive digital filters, IEEE Trans. Acoustics, 
Speech, and Signal Processing, 31, 1073, Oct. 1983.
30. Dentino, M.J., McCool, J., and Widrow, B., Adaptive filtering in the frequency 
domain, Proc. IEEE, 66, 1658, Dec. 1978.
31. Ferrara, E.R., Fast implementation of LMS adaptive filters, IEEE Trans. Acoustics, 
Speech, and Signal Processing, 28, 474, Aug. 1980.
32. Itakura, F. and Saito, S., Digital filtering techniques for speech analysis and 
synthesis, Proc. 7th International Congress on Acoustics, 261, 1971.
33. Makhoul, J., A class of all-zero lattice digital filters: properties and applications, 
IEEE Trans. Acoustics, Speech, and Signal Processing, 26, 304, Aug. 1978.
34. Makhoul, J., Stable and efficient lattice methods for linear prediction, IEEE Trans. 
Acoustics, Speech, and Signal Processing, 25, 423, Oct. 1977.
35. Griffiths, L.J., An adaptive lattice structure for noise-canceling applications, Proc. 
ICASSP-78, 87, Apr. 1978. 
' ' '
36. Clark, G.A. and Rodgers, P.W., Adaptive prediction applied to seismic event 
detection, Proc. IEEE, 69, 1166, Sep. 1981.
37. Griffiths, L.J., Smolka, F.R., and Trembly, L.D., Adaptive deconvolution: a new 
technique for processing time-varying seismic data, Geophysics, 742, June 1977.
38. Makhoul, J. and Viswanathan, R., Adaptive lattice methods for linear prediction, 
Proc. ICASSP-78, 83, May 1978.
39. Makhoul, J.L. and Cosell, L.K., Adaptive lattice analysis of speech, IEEE Trans. 
Acoust. Speech Signal Process., ASSP-29, 654, June 1981.
40. Widrow, B. and Walach, E., On the statistical efficiency of the LMS algorithm 
with nonstationary inputs, IEEE Trans. Inform. Theory, IT-30, 211, Mar. 1984.
41. Zeidler, J.R. et al., Adaptive enhancement of multiple sinusoids in uncorrelated 
noise, IEEE Trans. Acoust. Speech Signal Process., ASSP-26, 240, June 1978.
42. Thomas, G.B., Jr, Calculus and Analytic Geometry, 4th ed., Addison-Wesley, Reading, 
MA, 1968, chap. 10. 
’
43. Luenberger, D.G., Introduction to Linear and Nonlinear Programming, Addison- 
Wesley, Reading, MA, 1973, chap. 7.
44. Lim, J.S. and Oppenheim, A.V., Eds., Advanced Topics in Signal Processing, Prentice 
Hall, Englewood Cliffs, NJ, 1988, chap. 5.
45. Johnson, C.R., Jr., Lectures on Adaptive Parameter Estimation, Prentice Hall, 
Englewood Cliffs, NJ, 1988.


Signal Information, Coding, 
and Compression
10.1 Introduction
In this chapter, we first discuss the measurement of signal information content 
as a basis for the rest of the subjects in the chapter. Then, we will examine 
topics applicable to the encoding and compression of signals, that is, wave­
forms and images. Encoding and compression1'’7 are broad subjects, and our 
goal here is to introduce some of the basic techniques and show how they 
apply.
Signal coding and compression seem destined always to be necessary parts 
of the world of DSP. Every time the technology offers twice the amount of 
storage capacity, users discover they have more than twice the amount of 
information to store. When the capacity of a communication link is increased, 
the demand for channel capacity seems to increase in proportion. Thus, there 
is always the requirement to render signals in ways that reduce the related 
storage capacity or communication bandwidth.
The objective here is to reduce or compress the number of bits (binary digits) 
in the representation of a signal or image, in other words, to remove unnec­
essary or redundant information from a signal in ways that reduce its size 
and yet allow its restoration.
Note that just the opposite is done when signals are encoded for error 
detection and error correction. In these cases, redundant information is delib­
erately added to the signal in order to allow its verification. For example, you 
could append the sum of all the samples in a frame of waveform data before 
sending the frame, thus allowing the receiver to detect an error in one of the 
samples. Simple and practical coding methods for error detection and cor­
rection such as Hamming and cyclic codes3,7 are well known, but these are 
subjects usually not covered in a course on DSP, and again, our interest here 
is in removing redundant information from a signal rather than adding it.
Another reason for our interest in coding and compression lies in the close 
relationship between these subjects and the encryption of signals and other data. 
Encryption is a process where data is transformed in ways such that recovery
277

278 
Digital Signal Processing with Examples in Matlab
is (ideally) not possible except for those who know the transformation. 
Generally, a transformation is easier to discover when the data contains 
redundant information, such as a repeated sequence of samples. Thus, 
although not always done, it is generally best to remove redundant infor­
mation from data before encrypting it.
When we discuss coding and compression, special terms are commonly 
used. In coding theory, a sample of a waveform or a pixel in an image 
becomes a symbol, and in some kinds of coding, there is a symbol code asso­
ciated with each symbol. Furthermore, the waveform or image must be 
discrete, with a finite range of values. The symbol set for a continuous wave­
form, however small its range of values, would be infinite. Symbol codes 
are introduced in Section 10.4.
10.2 Measuring Information
The bit (binary digit) is the basic unit we use to measure information. The 
bit has two possible states (zero and one). Other familiar measures are the 
byte (eight bits, 28 = 256 states), which is the basic element in many storage 
and computing systems, and the computer word, which may consist of 16, 
32, or 64 bits.
The common measure of the amount of information in a waveform or image 
(or in any similar set of discrete data) is essentially the log base 2 (log2) of its 
a priori probability of occurrence. Consider the example in Figure 10.1, which 
illustrates two possible discrete waveforms in a particular time frame. At each
sample (k)
FIGURE 10.1
Two possible waveform vectors out of a large ensemble of different vectors. Each sample in a 
vector has eight possible values, and there are 32 samples in a vector, so the ensemble consists 
of 83‘ vectors.

Signal Information, Coding, and Compression
279
time step (fc) in the frame, a given waveform may have any of eight different 
amplitudes (states), that is, any sample value from 0 through 7. It follows 
that any pair of samples may exist in 8" = 64 different states, any vector of 
three samples in 83 states, etc. Therefore, the number of different waveform 
vectors that could occupy the frame in Figure 10.1 with 32 states is
Number of possible vectors - 832 
(10.1)
Suppose all 832 waveforms are equally likely. Then, the probability of each 
possible waveform is 8~ 2, and as soon as we are given one waveform vector, 
such as one of the two shown in Figure 10.1, we have gained the amount of 
information in the vector, which is
Information in a vector = -log2(8 32) = 96 bits 
(10.2)
The term used for this information measure is entropy (H). It is used in the 
sense that, in this example, the identification of a particular waveform 
results in the entropy (uncertainty) being reduced by 96 bits. Thus, our basic 
measure of information, or entropy, which is due to C.E. Shannon,1 is
Entropy of event with probability P: 
H = -log2P
(10.3)
Now suppose, instead of defining an "event" to be an occurrence of a 
complete vector, we define an event to be the selection of one of the K = 8 
possible symbols (values of xk) in Figure 10.1, and let P„ represent the prob­
ability of this event, as discussed in Chapter 7, Section 7.2. Then, it is rea­
sonable to extend (10.3) and define the average or expected symbol entropy:
Average entropy (bits/symbol): 
N-l
H = -£P„log2P„ 
n=0
(10.4)
If all waveform vectors are equally likely, as we have assumed so far, then all 
eight symbols in Figure 10.1 must also be equally likely with P„ = 1/8, and 
hence, H = 3 bits/symbol. Thus, the entropy of a vector with 32 equally likely 
symbols is 3(32) = 96 bits, and we see that the entropy measure (10.4) agrees 
with our idea of accumulating "information." As each successive symbol 
becomes known, our uncertainty about the waveform decreases by 3 bits until, 
after 32 symbols are known, we have a total of 96 bits of information.
There are different ways to apply the entropy measure (10.4). First, the 
measure is applicable to all kinds of symbol sets (letters, numbers, etc.), and 
all kinds of information besides just waveforms and images, which are our 
primary interest here.

280
Digital Signal Processing with Examples in Matlab
Second, when the entropy measure is applied generally to the measure­
ment of information and specifically to the design of codes for signal com­
pression, the determination of the symbol probabilities (P„) depends on the 
application. For example, suppose we are measuring the information in the 
single sentence you are now reading, and PW] stands for the probability of 
the ASCII symbol e. We could estimate P101 in two ways: first, as the relative 
occurrence of e in this book or in all literature, and second, as the relative 
occurrence of e in the sentence. If we wanted a valid measure of information, 
we would choose the first way, but if we were interested in compressing just 
the sentence as an isolated message, we would choose the second, as 
explained further in the next two sections.
10.3 Two Ways to Compress Signals
In this section, we discuss briefly two different and essentially indepen­
dent ways to compress a signal or image. For this discussion, we take one 
of the waveforms in Figure 10.1, say the waveform beginning with x0 = 1, 
and assume this waveform is to be encoded with fewer than 96 bits and 
stored in a database. Exact retrieval must be possible knowing only the 
general rules used for encoding. The waveform is plotted by itself in 
Figure 10.2.
We first notice that the symbols, that is, waveform values in Figure 10.2, 
are not equally likely. Using the freq function defined in Chapter 7 (7.3), we 
have the following:
Frequency (/r(n)):
5 6 7
ill
32 32 32
Symbol (n) in x:
Amplitude spectnim

Signal Information, Coding, and Compression 
281
Using fx(n) in place of P„ in (10.4), the average symbol entropy in this case is
7
Hx = ~^/v(n) logj/jfn) = 2.8553 bits/symbol (10.6)
Fl=0
Because the entropy in bits per symbol is less than three, tire number of bits 
per symbol required for the ordinary binary codes (000 through 111), we 
know that entropy coding, which is discussed in the next section, may provide 
a way to store the signal with fewer than 32(3) = 96 bits.
Second, we notice in the amplitude spectrum of x, which is also plotted 
in Figure 10.2, that not all spectral elements are equal. As we noted in 
Chapter 7, Section 7.5, this implies that the signal is correlated, that is, the 
autocorrelation function, <plr(m), is not just an impulse at m = 0. In other 
words, the samples of x are not independent, and therefore, linear predictive 
coding as it is described in Chapter 8, Section 8.10, could also be applied to 
compress the waveform. In Figure 10.3, we have a plot of the original vector, 
x, with the discrete prediction error vector, e. In this case, the symbol fre­
quencies, similar to (10.5), are as follows:
Symbol (n) in e: -1 
0
Frequency (/,(«)): 
£
1
15
32
(10.7)
The corresponding entropy, H„, is less than Hx in (10.6):
2
Hc = -^/t.(n)log2/t,(n) = 1.4948 bits/symbol (10.8) 
n=0
Signal 
Prediction error
Sample (k) 
Sample (k)
FIGURE 10.3
The signal vector in Figure 10.2 and its prediction error rounded to the nearest integer. The 
one-step predictor had two weights derived using covariance functions.

282 
Digital Signal Processing with Examples in Matlab
_________  Compressed 
e___ k Entropy---------------- data*,
coding
Linear 
predictive 
coding
X
FIGURE 10.4
Compression of a signal (x) in two stages. The first stage reduces intersymbol dependence, and 
the second stage applies coding to reduce the number of bits per symbol in e, the quantized 
prediction error.
Thus, in comparison with (10.6), further compression is possible due to the 
lowered signal entropy. Note that the predictor weights (two weights in this 
case) must be stored with the error vector as described in Chapter 8, but this 
would not be significant with a long signal vector.
Thus, we see that there are two ways to compress a signal. One aims at 
encoding the signal in a way such that the number of bits is close to the 
signal entropy times the signal length. The second aims at reducing the range 
of the signal amplitude, and thus the number of bits per symbol, by removing 
intersymbol dependence and whitening the spectrum of the signal.
Regarding the second method, predictive coding is not the only way to 
reduce intersymbol dependence. Other methods include string coding,5,8~i0 
which is particularly applicable to text and certain kinds of images, vector 
quantization,11 and some types of transform coding.7 Transform coding methods 
are described in Sections 10.5 through 10.7.
A final important point is that the two methods may be used together in a 
single compression operation. In the example we have been considering, a 
symbol code could be applied to vector e to replace the binary code of two 
bits per symbol and approach the entropy limit in (10.8), and we would have 
the scheme shown in Figure 10.4, that is, decorrelation followed by entropy 
coding of the quantized prediction error. In the illustration we have given 
here, the prediction error is simply rounded to the nearest integer, which 
allows exact recovery of the original vector as demonstrated in Chapter 8. 
One could produce further compression by increasing or varying the size of 
the quantizer steps, but then exact recovery would no longer be possible. 
This is known as lossy compression, as opposed to lossless compression, and is 
applicable in situations such as narrow-band telephony, or image and video 
compression, where exact recovery is not a requirement.
10.4 Entropy Coding
The term entropy coding refers to the first kind of compression we discussed 
in the previous section, where we encode a vector or image in a way such 
that the number of bits in the encoded version approaches the average 
entropy in bits/symbol times the number of symbols. In this section, we

Signal Information, Coding, and Compression 
283
FIGURE 10.5
(Upper) A segment, x, of the seismic waveform in Figure 8.23. (Center) Amplitude distribution 
of x, showing that entropy coding will not work. (Lower) Amplitude distribution of categories, 
which may be encoded as described in the text.
look at two coding schemes that are practical and also come close to being 
optimal in the sense just described.
Entropy coding begins with a set of symbols such as the eight possible 
waveform values in Figures 10.1 through 10.3. The coding schemes work well 
when the number of symbols is small but are not directly applicable when 
the symbol set is large. Consider the example in Figure 10.5. The upper plot 
is a segment consisting of 1000 samples of the seismic signal in Figure 8.23, 
and we note that the range of symbol values in this segment is over 20,000. 
The distribution of signal amplitudes in the segment is shown in the center 
plot of Figure 10.5. Entropy coding affords essentially no improvement in 
this case, because even the most frequent samples appear only twice (with 
frequency 0.002) in the entire segment. This leads us to the lower plot, in 
which ffn) is the frequency of the nth category of symbols, each category 
having a range of 2048. Entropy coding then consists of encoding the cate­
gory sequence rather than the sample sequence and, for each symbol, 
appending an 11-bit offset to locate the symbol within the category. Using 
this approach, entropy coding is applicable to nearly all kinds of waveforms 
and images.13
The first coding scheme is called Huffman coding after D.A. Huffman.2,5'12 
Huffman coding is a type of fixed-symbol coding, in which a specific code is

284 
Digital Signal Processing with Examples in Matlab
Signa!
Sample (k)
FIGURE 10.6
One of the signal vectors in Figure 10.1.
x=[4 445432101232323
456545676545655 4]
symbols: 01234567
32*fx=[l 2 4 4 8 8 4 1]
Entropy ofx: H.=2.6875 bits/symbol
assigned to each possible symbol. Our intent here is only to give a simple 
example of Huffman coding that shows how to construct Huffman codes. 
The formal theoretical background is left to texts on coding and information 
theory,2'5 which are recommended for further study.
For our illustration, we use the second waveform in Figure 10.1, that is, the 
waveform shown in Figure 10.6. Next to the waveform plot in Figure 10.6 
are the waveform vector (x), the frequency vector multiplied by K = 32 (which 
shows the number of times each symbol appears in x), and the average 
entropy in bits/symbol computed using (10.4) with Pk
The rules for Huffman code production are simple. They involve the 
production of a binary code tree, the structure of which contains the codes 
for all the possible symbols. An algorithm that produces a Huffman code 
tree is shown next, along with an illustration of its use with the data in 
Figure 10.6:
HUFFMAN CODE PRODUCTION
Assign a symbol and its frequency in terms of 
(symbol, frequency) to each of N nodes. Then,
1. List the N nodes in order of decreasing frequency.
2. Combine nodes N and N - 1 into a new node.
3. Record the link from these nodes to new node.
4. There are now i\' - 1 nodes, so decrease N by one.
5. Repeat steps 1-4 until N = 1.
Tree information is now complete.

Signal Information, Coding, and Compression 
285
(10.10)
Iteration
Initial
1
2
3
4
5
6
7
8
(0,1)
(4,8)
(4,8)
(4,8)
(10,8)
(11,8)
(12,16)
(13,16)u
(14,32)
(1,2)
(5,8)
(5,8)
(5,8)
(4,8)
(10,8)
(H,8)b (12,16)14
(2,4)
(2,4)
(2,4)
(9,4)
(5,8)
(4,8)12
(10,8)]3
(3,4)
(3,4)
(3,4)
(2,4)
(9,4)n
(5,8)12
(4,8)
(6,4)
(6,4)
(3,4)10
(2,4)n
(5,8)
(1,2)
(8,2),
(6,4)10
(6,4)
(0,l)s
(1,2),
(7,1)
(7,1)8
Initially, in the first column in (10.10), each symbol is assigned to a node, and 
the nodes are listed with corresponding symbol frequencies as shown. For 
convenience, we use 32/, instead of /., so the frequencies in (10.10) sum to 
32 instead of one.
Tree production begins in the second column (column 1), where the two 
least-frequent nodes, 0 and 7, are attached to node 8. The attachment is 
denoted by the subscript (8) on these two nodes in column 1, thus satisfying 
step 3 in the algorithm (10.9). In this manner, we move from column 1 to 
the second iteration in column 2, which has N - 1 = 7 nodes, and so on, until 
after eight iterations, there is a single node, which represents the top of the 
code tree. In the fourth iteration, notice that step 1 is necessary, that is, node 
10 is not at the bottom of column 4, because there are less-frequent nodes.
All the Huffman code information for the signal in Figure 10.6 is contained 
in (10.10), but it is best illustrated in the drawing of the tree in Figure 10.7. 
The tree is formed by connecting the nodes as indicated in (10.10), and then 
the codes are formed assigning binary labels (arbitrarily) to the branches 
emerging from each node. The resulting Huffman codes are listed in the figure.
The first thing to note about codes generated in this manner is the assign­
ment of shorter codes to more-frequent symbols, which of course will lead to a 
shorter representation of the waveform in Figure 10.6. Second, the tree 
structure implies an important property called the prefix property, without 
which the codes would be useless for signal compression:
Prefix property: No code begins with another code (10.11)
We can easily see why Huffman codes have this property. If, say, 100 were 
the prefix of the code for any symbol besides 6, then node 6 would have 
emerging branches, which is impossible, because node 6 represents a symbol 
and not an internal node in the tree.
The prefix property implies that Huffman codes can be concatenated 
(strung together) to represent the complete signal vector in Figure 10.6, 
thereby producing a representation shorter than the original 96-bit binary 
representation, and yet allowing recovery of the original data. The recovery

286 
Digital Signal Processing with Examples in Matlab
FIGURE 10.7
Huffman codes and tree for the vector in Figure 10.6.
process consists (in principal) of beginning with the first bit of the binary 
string and proceeding down from the top of the tree (node 14 in Figure 10.7) 
until a symbol node is reached, recording the symbol, and repeating the 
process until all symbols are decoded.
The binary and Huffman encoded bit streams for the vector in Figure 10.6 
are as follows, the Huffman string being encoded with the codes in Figure 10.7:
symbol :444 5432101232323
binary: 100100100101100011010001000001010011010011010011
symbol: 456 
54 56765456 554
binary: 100101110101100101110111110101100101110101101100
Huffman: 010101000110111011101111111101101011101011101010 
10010000010010011110100000100100000001
(10.12)
Note how the Huffman string would be decoded without ambiguity using 
the tree or the table in Figure 10.7.
Due to the small number (32) of samples, which is not realistic, the Huffman 
vector with 86 bits is only 10 bits shorter than the binary vector with 96 bits, 
but it illustrates the principal. Furthermore, the Huffman vector in this case 
is optimal. It matches the entropy of the waveform vector in Figure 10.6, that is,
Total entropy = 32HX = 32(2.6875) = 86 bits 
(10.13) 

Signal Information, Coding, and Compression 
287
This fortuitous result is not true in general. It is true, as one may observe 
from the nature of the binary tree, only when the symbol frequencies are all 
negative powers of two. Otherwise, some of the possible codes in the tree 
will not be used. See Exercise 7, for example, which provides such a case. 
The type of entropy coding discussed next gets around this constraint on 
the symbol probabilities.
Two final points on Huffman coding: first, there is not just one unique set 
of Huffman codes for a given signal, that is, for a given set of symbols and 
symbol frequencies. This is easily seen in Figure 10.7, where we could 
exchange the "0" and "A" emerging from any node and change the code 
without changing any of the code lengths, thus producing a comparable but 
different Huffman code. In fact, given an ordered set of symbols and a 
generally understood rule for assigning zeros and ones to branches, we could 
construct the code tree knowing only the lengths of the symbol codes. Thus, 
in Huffman coding applications, the only data we must store or communicate 
with the encoded signal vector consists of the ordered set of code lengths. Because 
of this, Huffman coding is used widely in signal coding and compression.
Second, Matlab functions for Huffman coding are available from many 
sources, including the Internet at the time of this writing. A simple function 
included with this text that generates Huffman codes is
[H,L] = h_codes(x) 
(10.14)
Given a signal x, H is a vector of Huffman codes for the symbols in x, and 
L is a vector giving the length of each code. (Note that L is necessary, because 
some codes begin with zero.) The signal vector is assumed to consist of 
integers in the range [0,,rmaJ. The function calls two other functions. One is 
code_length(x), which does as its name implies, and the other is freq(x), which 
computes a symbol frequency table. The latter is useful for computing ampli­
tude distributions in general. Its description is
[f,xmin, xmax] = freq(x) 
(10.15)
When freq is executed, / becomes a vector of symbol frequencies, that is, the 
amplitude distribution of x, in the range [xmin, xmax].
The next type of entropy coding we discuss is called arithmetic coding5"'3''5 
Arithmetic coding differs from Huffman coding in that the encoded version 
of the signal vector or array does not consist of a sequence of codes for 
individual signal elements (symbols). In arithmetic coding, the signal is 
encoded by processing the symbols one at a time in order, and the end result 
is a single long binary fraction rather than a sequence of symbol codes.
To illustrate the production of an arithmetic code, we again use the wave­
form vector in Figure 10.6, which is shown again in Figure 10.8 with its table 
of symbol frequencies. The frequencies sum to one and represent the symbol 
probabilities for this particular vector.
The concept (but not exactly the method, as we shall see) of arithmetic coding 
is illustrated in Figure 10.9. The encoding process begins by establishing a line

288
Digital Signal Processing with Examples in Matlab
Signal
symbol
0
1
2
3
4
5
6
7
frequency 
0.03125 
0.0625 
0.125 
0.125 
0.25 
0.25 
0.125 
0.03125
. . Sample (k)
FIGURE 10.8
Signal in Figure 10.6 with frequency table.
32768-=t— 
31744-7-1—
27648—I—
4258------
14896-----
7168-----
2
FIGURE 10.9
Arithmetic encoding of the first five symbols of the signal in Figure 10.8. Each symbol is used 
in turn to select its interval in an overall range of integers, and thus produce a smaller interval 
to be used by the following symbol.
14960-----
3
15164—4—'
3072-----
1
!024 o-
0-U1—

Signal Information, Coding, and Compression 
289
from zero to (in this case) 215 = 32,768, with eight partitions with lengths that 
are proportional to the eight symbol frequencies. This is the vertical line at 
the left of the diagram. Notice that any one of the eight possible symbols 
specifies a unique piece of the line, with the length of the piece being pro­
portional to the symbol frequency. Thus, for example, the interval from 
27,648 to 31,744 stands for the symbol 6, and so on. Now instead of the entire 
interval, we agree (arbitrarily) to represent the interval by its lower bound. 
Thus, the symbol 6 would be represented by 27,648, and so on.
The idea in arithmetic coding is to represent the entire message as a small 
interval on this line. The encoding process begins in this illustration with 
the interval [0,32768) and proceeds with the definition of an interval that 
grows smaller with each symbol. As the sequence of symbols increases in 
length, it requires a diminishing fraction of the line to represent it, that is, 
to distinguish it from other sequences of the same length.
The process is illustrated in Figure 10.9, which shows the encoding of just 
the first four symbols in the vector in Figure 10.8, that is, [4, 4, 4, 5, 4], With 
the first symbol (4), the range is reduced from [0,32768) to the interval 
[11264,19456), which is represented by the second line in from the left. With 
the second symbol (4 again), the interval is reduced to [14080,16128). With 
the third (4), the interval is reduced again to [14784,15296). The next symbol 
(5) selects a different part of the line and reduces the interval to [15088,15216). 
Notice that the number of significant bits required to distinguish the upper 
and lower limits of the interval (and thus the final symbol from other sym­
bols) has increased at each stage. The final symbol in Figure 10.9 (4) causes 
the interval to shrink again to [15132,15164), and thus, the number of bits 
needed to distinguish the interval grows with each symbol.
In fact, we can see in the example of Figure 10.9 that to process all 32 symbols 
in the vector and still represent the intervals with integers, we would have 
to begin with a range much larger than 2. Because the total signal entropy 
is 86 bits, we would require an initial integer range of 286 ~ 1026 for the entire 
signal. However, the range is not necessary, because the most significant bits 
of the interval limits can be stripped (and saved) as encoding proceeds. For 
example, in Figure 10.9, the binary representations of the range limits on the 
third line are
(14O8O)lo = (011011100000000), 
(16128)10 = (011111100000000)2
We could represent the interval just as well without the two leading bits in 
each binary number. Practical arithmetic coding algorithms work by strip­
ping and saving these unnecessary leading bits as symbols are processed, 
thus maintaining a practical integer word size. Matlab arithmetic encoding 
and decoding functions that use this technique are included with this text:
[i/, Nbits] = a_encode(x) 
,, n,,,
x = a_decode{y,f ,K)

290 
Digital Signal Processing loith Examples in Matlab
The functions are useful with the exercises, but probably will need modifi­
cation for other applications. The encoding function, a_encode, encodes an 
integer vector or array, x, into another vector, y, with 8 bits per element of 
y, and also supplies the total number of output bits. If a_encode is used with 
the signal in Figure 10.8, the output bit stream is
Arithmetic coding of signal in Figure 10.8:
01110110 01000110 
11000001 
01011010 10110011 11100111
01010100 11011000 
01001111 
11011110 011101
. 
(10.17)
Comparing this with (10.12), we see that the bit stream length (86 bits) is the 
same as the Huffman length in this example, but that there are no individual 
symbol codes in this case. Instead, there is a single code representing the 
entire signal.
The a_encode function makes use internally of the symbol frequencies 
computed with the freq function described previously. The frequency data 
must be included with the code in (10.17), or in some way made available 
to the decoder. The decoding function, a_decode, uses the encoding algorithm; 
that is, enough bits are read to discern the first symbol, which in turn is used 
to compute the interval for the second symbol just as in encoding, and so 
on. Tire arguments/and K of a_decode in (10.16) are the frequency table and 
the length of x, respectively.
As we have seen, Huffman and arithmetic encoding both result in an 
encoded vector length that approaches or matches the average symbol 
entropy times the number of symbols. Both encoding methods also have the 
overhead associated with including the symbol lengths (Huffman) or symbol 
frequencies (arithmetic).
Our final point regarding these entropy coding methods is that either 
method can be made adaptive. Adaptive arithmetic coding is especially easy, 
assuming that the range of symbols is known globally. We begin with the 
assumption that all symbol probabilities are equal, or with any similar glo­
bally understood assumption about the symbol frequencies. After the first 
symbol, say xt, is encoded with this assumption, the frequency table is 
modified by incrementing the frequency of and so on. As encoding pro­
ceeds, in general, the frequencies become more accurate, and thus, the 
entropy limit is approached.
Adaptive coding has two advantages. First, it eliminates the overhead. 
There is no need to include code lengths or frequencies with the encoded 
signal. Second, in cases where the symbol frequencies change in a long signal, 
the encoder can adapt to these changes by continuously modifying the 
frequency table.
As we leave this section, we note that the entropy limit, that is, bits per 
symbol times number of symbols, is not usually the limit of compressibility. 
We saw in the previous section that there are two kinds of redundancy in 

Signal Information, Coding, and Compression 
291
signals usually encountered in engineering, and entropy coding addresses 
only one of these. In Exercise 24, we apply the process in Figure 10.4 to the 
signal in Figure 10.8 and find that the encoded message is thereby reduced 
from the previous entropy limit (86 bits) to a lower entropy limit (24 bits).
................ ...........................................................................................................................
10.5 Transform Coding and the Discrete
Cosine Transform
In Section 10.3, we discussed the distinction between lossless and lossy signal 
compression. Lossy compression is appropriate in applications where some 
kinds of errors in the reconstructed signal do not matter to the end user. In 
speech communications, for example, audio signals are routinely encoded 
in a lossy manner in order to conserve bandwidth.16 In image and video 
processing, the JPEG and MPEG standards7 involve lossy compression tech­
niques. These also involve a form of encoding known as transform coding.7'13 
The objective in transform coding is to transform the signal vector or array 
into another domain, such as the frequency domain, in which most of the 
essential information in the signal is contained in a relatively small part of 
the transform domain, that is, in a relatively few elements of the transform 
vector or array. Almost any kind of transform that provides mapping of 
vectors or arrays to another domain and allows recovery via an inverse 
transform is a possible candidate for accomplishing this objective, depending 
on the data being transformed. In this section, we discuss the Discrete Cosine 
Transform (DCT), which has turned out to be especially useful in waveform 
and image compression. In the next section, we discuss multirate processing, 
which produces transforms used for similar purposes.
These transforms are applied to signal compression essentially in the man­
ner shown in Figure 10.4; that is, transform coding followed by entropy 
coding. The concept is shown with more detail in Figure 10.10. The signal is 
processed in segments, with each segment being transformed independently. 
In current image processing practice, for example, 8x8 pixel segments are
Signal 
.........
segment v Transform 
coding 
FIGURE 10.10
Signal compression using transform coding. Depending on relative importance, transform 
components may be quantized with different quantizing step sizes. The step sizes, along with 
the information needed for entropy decoding, must be included with the compressed signal 
data.

292
Digital Signal Processing with Examples in Matlab
processed, one at a time, until the entire image is processed. Then the 
transform components are (or may be) quantized selectively, so that infor­
mation in the more important components is kept in more detail. The 
definition of "more important" is both signal-dependent and subjective, and 
therefore, the selective quantizing scheme that works best for all wave­
forms or all images does not exist. Different kinds of data, and different 
uses of data, require different designs of this stage of the operation.
Next, in Figure 10.10, the quantized transform data is encoded, and other 
essential data, such as the Huffman code lengths or the arithmetic code 
frequencies, are attached to the compressed data. Unless it is known globally, 
the quantizing information, that is, the number of steps used to quantize 
each transform value, must also be contained in the compressed data.
Transform coding and predictive coding are competing candidates for 
waveform and image compression in the sense that both remove the same 
kinds of redundancy in a signal. For example, a predictor reduces the infor­
mation in narrow-band waveform components by subtraction, and a trans­
form coder reduces the information in these same components by 
concentrating them into fewer transform components.
With this idea in mind, it is natural to think first of the DFT as a candidate 
for transform coding. The Discrete Cosine Transform (DCT)7'17'18 is related to 
the DFT, and in fact, may be written as a function of the DFT, as we shall 
see. The DCT and the DFT are similar, because they transform a signal 
component at a given frequency into essentially the same transform compo­
nent, and also because both transforms have unique, nearly symmetric 
inverses. However, unlike the DFT, the DCT has real components, which are 
easier to use and quantize.
The one-dimensional DCT of a vector x with N elements is defined as follows:
Discrete Cosine Transform (DCT): 
N-t 
.
v-dct 
((2n+l )mn\ 
n.. 
,. .
X,„ = c„,2_,x„cos^—'« =0,1,...,N-l 
n=0
[ Vl/N ; m = 0
= j ,______
I 72/N ; in > 0
(10.18)
We use X°CT to represent the DCT here and distinguish the DCT from 
the DFT. There are variations of the form in (10.18),7,17,18 but this form is 
used especially in compression, because, as we shall see, the scaling 
factors (c„,) produce transform components generally in a range of values 
similar to that of the signal components. Note that unlike the DFT, the 
DCT produces real components, which may be positive or negative, but 
not complex.
As with the DFT, there is a unique inverse DCT that recovers the original 
vector or array. When the DCT is expressed as in (10.18), the inverse has the 

Signal information, Coding, and Compression
293
following symmetric form:
(10.19)
Because the scaling factors are the same in (10.18) and (10.19), we conclude that 
a signal and transform elements will have comparable ranges of values. The 
demonstration that (10.19) is the inverse of (10.18) is similar to demonstration 
(3.22) for the DFT inverse and is left for Exercise 25 at the end of this chapter.
In fact, the DCT may be expressed as a function of the DFT. We do this by 
constructing a vector consisting of any signal vector, x, appended to itself. 
To begin, we use XU1 to represent the DFT of x as defined in Chapter 3, 
(3.7); that is,
N-l 
,2 7rntJi
X^ = 
m = 0,l,...,N-l 
(10.20)
»=o
Now X°n is in general complex. But, we have seen [Chapter 8, (8.22) for 
example] that if x is reversed, the DFT of x is the conjugate of X^. 
Therefore, we should be able to combine x and xr and produce a vector with 
imaginary DFT components that cancel; that is, a vector with a real DFT. 
One such vector is the following vector with length 2N:
it = [«o ... u2N^] = 
... Xi x0 x0 xA ... xN_j] (10.21)
The DFT of u is then
(In the final sums, the indices are k = N - n - 1 and i = n - N.)
To satisfy the conjugate relationship between vector x and its reversal, w 
must now be centered at the origin, that is, halfway between the x0 samples

which the indices are
(10.22)
N-l
294 
Digital Signal Processing with Examples in Matlab
in (10.21). Thus, we imagine a discrete domain in 
aligned as follows:
element of u: xN.t ... x0 x0 ... 
. , 1 11 
mdex: -N + - ... -- - ...
We recall from Chapter 3, Table 3.3, Property 8, that the time shift described 
in (10.22), that is, the left shift of N - 1/2 steps, causes the DFT, , to be 
multiplied by the factor 
_ The trans{orm, y^, of the shifted
time series is given by
17DFT itDFT 1 2N
V,„ = U,„ e
N'-l ,2mn(k+l/2) N-l 
,2gin( 1 + 1/2)
= £xte' ZN + ^xfi' 2N ; m = 0,1,...,2N-1 
(10.23)
k=0 
i=0
The two sums in the last line are conjugates of each other, so their imaginary 
parts cancel. Therefore, by limiting m in (10.23) to its first N values, we can 
write an expression that contains all of the DCT components in (10.18):
., . 
,.2«m(f+V2)
V°FT = 2Reb>
I 1=0
N“* 
zzn 
x 0VDCT
((2 n +1) TH Tt \ 
t-A-m 
n xt , cm
= 22j.y„cos -----yr-y---- = --------; m = 0,1,..., N - 1 
(10.24)
\ XZv / Cm 
n=0
Thus, we have a derivation of the DCT in terms of a related DFT. The 
components in (10.18) are scaled to produce a symmetric inverse, as we shall 
see. But first, we note that the upper sum in (10.24) is the DFT of x appended 
with N zeros and delayed (shifted right) by 1/2 sample. There are two 
consequences of this. First, we see that the components of the DCT of a vector, 
x, are nearly the same as the components of the DFT of the vector [xr x], and 
thus, we would expect the DCT and the DFT to produce similar spectra. The 
similarity is illustrated in Figure 10.11, where DFT and DCT amplitudes are 
plotted for comparison. The signal, in this case, is a portion of the speech 
waveform in Figure 8.17. Note that, although the DCT has twice as many 
components, the two amplitude spectra are similar in form.
Second, we have in the first line of (10.24) a way to compute the DCT using 
the FFT algorithm; that is, an equivalent expression for X, the DCT of x, is
y = [x0 Xj ... xN_! 0 0 ... 0]
t 
(10.25)
Xm = cmRe^FFT{y}e 2N}; m = 0,1,...,N- 1

Signal Information, Coding, and Compression 
295
0 
0-05 
0.1 
0-15 
0.2 
0.25 
0.3 
0.35
t(s)
Frequency (KHz) 
Frequency (KHz)
FIGURE 10.11
A segment of the speech signal, s(t), in Figure 8.17 with DFT and DCT amplitudes, showing 
the similarity between the two transforms.
The zeros in (10.25) increase the FFT size from N to 2N, and there are more 
efficient algorithms for computing the DCT/ but the form in (10.25) is useful 
if only the FFT is available.
By the same line of reasoning, a formula equivalent to (10.25) for the 
inverse DCT in (10.19) is
mir
Y„, = c,„e2NX^T; m = 0,l......N-l;
x„ = 2HRe{IFFT{[Y0 Yt ... Yn_j 0 0 ... 0]}}; n = O,1,...,N-1
(10.26)
Two Matlab functions, y = sp__dct(x) ar|d spjdct, that compute the DCT and 
its inverse using (10.25) and (10.26) are included to demonstrate these meth­
ods. The derivation of (10.26) is similar to the derivation of (10.25) and is 
left for Exercise 26.
In image coding and spectral analysis, we use a two-dimensional transform. 
In general, the two-dimensional transform of an array consists of transform­
ing the columns of the array one at a time, and then transforming the rows 
of the result. Thus, the two-dimensional transform is quite different from 
the one-dimensional transform. Recall that the DFT of an array, for example, 
is given as the DFT of each column. Thus, the two-dimensional DFT, which

296 
Digital Signal Processing with Examples in Matlab
Boats
71 
86 
102 l 
16 
32
FIGURE 10.12
Two-dimensional DCT of a segment of the "Boats" image. The segment includes the top of the 
tower in the upper left quadrant of the complete image.
we call DFT2, is produced as follows, using " to indicate the transpose:
(Two-dimensional) DFT2(.r) = DFT{DFT{x}.'}.' 
(10.27)
The Matlab fft2 function accomplishes this computation.
An illustration of the two-dimensional DCT in image coding is given in 
Figure 10.12. The two-dimensional DCT is computed as in (10.27) with the 
DCT in place of the DFT. In the illustration, we use the image known as 
"Boats," which appears often in DSP literature. The complete image, a 256 x 
256 array of 8-bit gray-scale pixels, is shown on the upper left. On the lower 
left, we extracted a 32 x 32-pixel segment consisting of the top of the tower 
in the upper left quadrant of the complete image. The DCT of this segment 
is shown on the right in Figure 10.12. To plot the DCT segment, the DCT 
components were scaled to the image pixel range, that is, [0,255]. In this 
representation of the DCT, low-frequency components appear at the upper 
left, high-frequency components are at lower right, and component values 
increase from black to white.
Compared with the original image, the rounded DFT and the rounded 
DCT in this case have lower entropies per element, as seen in the following 
data. (For DFT entropy, we used a comparable array consisting of separate 
real and imaginary parts with eight bits per element.) These entropies are

Signal Information, Coding, and Compression
297
FIGURE 10.13
Original image and reconstruction after 4:1 compression using two-dimensional DCT quantization.
for the complete image, not the segment, in Figure 10.12:
Entropy of Boat's image:
Entropy of DFT components:
Entropy of DCT components:
7.19 bits/pixel
1.50 bits/component (10.28)
1.76 bits/component
Thus, transform coding reduces the image entropy. Furthermore, as seen 
here, the choice of the DCT over the DFT is due not to entropy, but to the 
real component values in the DCT, which are more amenable to quantization 
as well as to other procedures that lead to lossy image compression.
To produce the entropies above, the mean value of the image was subtracted 
before transforming, the transforms were scaled to the range [0,255], and the 
transform components were rounded to the nearest integer. In image compres­
sion, the information needed to undo these steps must be included with the data.
The rounding operation is a simple example of the quantizing step in 
Figure 10.10 which, in this case, results in a compression in terms of the entropy 
ratio of 7.19/1.76, or about 4:1. With this degree of quantization, the quality of 
the reconstruction via the inverse DCT, which is shown in Figure 10.13, is good.
When the image transform is scanned to convert the array to a vector, as 
it is in some compression schemes, the zig-zag scan (Figure 10.14) is prefer­
able, because it places components at similar frequencies near each other in 
the scanning sequence. A Matlab function called y = image _scan(x), for which 
x is any rectangular array, and y is the zig-zag scan vector, is included to 
accomplish this type of scan.
Because the DCT components are likely to be correlated along the scan 
line, linear predictive coding of these components prior to entropy coding, 
illustrated in Figure 10.15, may be advantageous. The result of adding this 
step, that is, zig-zag scanning followed by linear predictive coding, to the 
compression process for the Boats image reduces the DCT entropy slightly 
from the result in (10.28).

298 
Digital Signal Processing with Examples in Matlab
FIGURE 10.14
Zig-zag scanning of two-dimensional pixels or transform components.
FIGURE 10.15
A linear prediction stage added to the compression scheme in Figure 10.10.
10.6 Multirate Signal Decomposition
and Subband Coding
Another method used to analyze and compress waveforms and images 
involves a process known as multirate signal processing, or multirate signal decom­
position, combined with subband coding.19’22 These terms are more or less self- 
descriptive, as we shall see. Multirate processing is also related to signal 
transformation using wavelets. These are all broad subjects, and there are sev­
eral good reference texts for the reader wishing to pursue them.19-25 Our goal 
is to introduce the concepts and show how they apply to signal and image 
analysis and compression.
Because filtering is a familiar subject in DSP, it is easiest to describe these 
concepts in terms of filtering operations. The key process in this approach

FIGURE 10.16
Single stage of a multirate signal processing system.
FIGURE 10.17
Encoding and recovery involving a wavelet transform with two components. Symbols Q, EC, 
and ED stand for quantization, entropy coding, and entropy decoding, respectively
is known as decimation, or down-sampling. A simple filtering and decimation 
process is illustrated in Figure 10.16. The principle is the same, as we shall see, 
for both waveforms and images, and, although only two filters and down­
sampling by a factor of 2 are shown in Figure 10.16, the same analysis holds 
for any number (M) of filters and down-sampling by the factor M.
The down-sampling operations in Figure 10.16 are similar to down-sampling 
discussed in Chapter 5, Section 5.3. For our discussion, the signals in the 
lowpass channel are labeled as follows:
x = [x0 Xj x2 ...]
1 rlll , ID r 1 
1 
1
u = [u0 u, u2 ...J; u = [Mo u2 ...J
(10.29)
Thus, m1d, the down-sampled or decimated version of it1, is a sequence con­
sisting of every other sample of u.
If the decimated sequences, ii'D and u2D, are defined in this manner, then 
clearly these sequences together have the same number of samples as their 
parent vector, x. (We assume for this discussion that the length of x is even.) 
Furthermore, if, say, m2D contains only a small amount of useful information 
compared with u1D, then it is useful to be able to subject the two signals 
separately to quantization and entropy coding, that is, subband coding. An 
encoding and recovery concept is illustrated in Figure 10.17. L and H stand 

300 
Digital Signal Processing with Examples in Matlab
for the lowpass and highpass filters in Figure 10.16, Q stands for quantizing, 
EC stands for entropy coding, and ED stands for entropy decoding. We 
assume EC-ED is a lossless process. We view this concept as encoding and 
recovery in a multirate processor with two components produced by the filters, 
L and H, each component the result of convolving x with the impulse 
response of the respective filter, as described in Chapter 4.
Notice that multirate processing differs from transforms like the DCT in that 
it involves digital filters that may be used to process a signal or image con­
tinuously rather than one vector at a time. This continuous type of processing 
in lossy compression eliminates blocking effects of the DCT in the recovered 
signal, which may be objectionable in waveform and image compression.
Multirate processing.with more components involves the application of the 
same type of filtering and down-sampling in cascade arrangements, as we will 
discuss shortly. The main subject in multirate theory, however, lies in the design 
of lowpass and highpass filters, which produce useful transformations and 
allow exact or at least acceptable recovery of the original signal.
The first question we might ask is whether the decimated sequences, ii1D 
and m2D in (10.29), which contain the same number of samples as x, contain 
enough information to reconstruct x, even with ideal filters. Let us assume 
small (lossless) quantizing steps and lossless entropy coding, resulting in 
the version shown in Figure 10.18. Then the answer to our question lies in 
the relationship between the spectrum of a continuous signal and the spec­
trum of its sample vector. This relationship is given in Chapter 3, (3.42), 
which gives the spectrum of the samples as a superposition of shifted 
spectra of the continuous signal.
Suppose now that we have a waveform, x(t), sampled with time step T, 
that is small enough to prevent aliasing. Let X(co) represent the continuous 
DFT of x(t) [as in (3.25), for example]. We use this representation here instead 
of the more correct notation, X(eimT), for the sake of brevity. Because the 
sampling theorem is satisfied, we note from (3.42) that TX(cd) in the range 
|®| < n/T also represents the Fourier transform of r(t). Suppose the Fourier 
transform of x(t) has the form shown at the upper left in Figure 10.19. Note 
that the spectrum of x(t) occupies the entire range from zero to half the 
sampling rate, so any attempt to down-sample x would result in an aliased 
reconstruction, as described in Chapter 3, Section 3.10, and particularly in 
(3.42). 
'
FIGURE 10.18
Equivalent of Figure 10.17 when the quantizing and entropy coding operations are lossless.

Signal Information, Coding, and Compression 
301
FIGURE 10.19
Upper left: continuous transform of .v(f) scaled by 1/T. Upper right: DFT of sample vector x 
with time step T. Middle left: DFT of u, which is ,r after lowpass filtering. Middle right: DFT 
of m’ after down-sampling with T2 = 2T. Lower plots: same as middle plots for u2, which is X 
after highpass filtering.
The DFT of a long vector, x, consisting of samples of x(t) with time step 
T, is shown at the upper right in Figure 10.19. This is one cycle of the DFT, 
which we designate X(cu). The DFT consists of repetitions of X(tt>) over all 
frequencies. We also note that these transforms are usually complex, and 
that we are using real transforms here for the sake of the illustration.
For this discussion, we assume the filters in Figure 10.18 are ideal, and 
produce w1 and u with DFTs U'fcu) and LF(w) shown at the middle and lower 
left in Figure 10.19. That is, the filters extract the low- and high-frequency 
portions of X(o>) without overlap. Because the spectra of u and u2 are (ideally) 
band-limited, the DFTs of these down-sampled signals, U1D(ft>) and LT2 (to), 
which are shown at the middle and lower right, are not aliased. Note that 
the time step after down-sampling is
T2 = 2T 
(10.30)
and therefore, the frequency range in the plots of U1D(<i)) and U"D(<w) is half 
the range in the plots of Ll'fcw) and li2(co). Also, the spectral values are only 
half as large for the decimated sequences, because these have only half as 
many samples. Thus, all the information in X(tt>) (and thus in x) is contained 

302
Digital Signal Processing with Examples in Matlab
unambiguously in the down-sampled signals, u1D and u2D, and recovery of x 
is at least theoretically possible.
We now consider the question of recovery. As shown in Figure 10.18, 
recovery is preceded by upsampling, or expansion. In general, expansion by 
a factor M consists of inserting M - 1 zeros between each pair of samples 
and is thus an application of Property 6 in Chapter 3, Table 3.3, which states 
that the DFT of the expanded sequence consists of M repetitions of the DFT 
of the sequence before expansion.
In the case of Figure 10.18 where the factor is M = 2, we have the following 
sequences:
Expansion; M = 2:
IE 
u
2E 
H
r ID 
q ID 
zx .ID -•
- [u0 
U ut 
(J u2 ...]
r 2D 
n 2D 
n 
2D 1
= [u0 
0 u1 
0 u2 ...]
(10.31)
In accordance with Property 6, then, u1E has a DFT, U1£, consisting of two 
repetitions of U1D, and likewise for m2 . This result is illustrated in Figure 
10.20. Again, because T2 = 2T, the frequency range on the right in Figure 10.20 
is twice that on the left. We can also see that an ideal lowpass filter would 
succeed in extracting the portion of U1E from 0 to n!2T rad/s, and an ideal 
highpass filter would succeed in extracting the portion of Li from K/2T to 
nff rad/s. Thus, the ideal recovery of x from its decimated components 
would be accomplished as in Figure 10.21, and we may view Figure 10.21 
as a simple example of a transform (in the left half of the figure) and its 
inverse (in the right half).
Practical multirate processors, however, use real digital filters (both FIR and 
HR) in place of the ideal filters (L and H) and actually allow aliasing to occur 
during the transforma tion and recovery. Aliasing causes signal spectra to be
FIGURE 10.20
DFTs of the down-sampled sequences on the left; DFTs of the expanded sequences on the right. 
With T2 = 2Tt expansion consists of inserting a zero between each pair of samples in the down­
sampled sequences. Frequency ranges on the right are twice those on the left.

Signal Information, Coding, and Compression 
303
FIGURE 10.21
Ideal decomposition and recovery of a signal using decimation with M = 2.
FIGURE 10.22
Decomposition and recovery of a signal using real filters that allow aliasing.
modified as described in Chapter 3, Sections 3.10 and 3.11, and illustrated, for 
example, in Figure 3.16. In the present case, aliasing would cause the spectral 
components in Figure 10.20 to overlap and add together in the vicinity of 
co = 7t/T2 in the left-hand plots, and in the vicinity of co = ntlT and co = 3it!2T 
in the right-hand plots, as described by (3.42) in Chapter 3.
With aliasing present, we now consider the configuration in Figure 10.22, 
in which the input and recovery filters (I? and If, or H1 and Hz) are not 
necessarily the same. In this configuration, we have the following DFT rela­
tionships for the forward transform:
U\co) = X(co)L1(co); U1D(co) =
~ 
(10.32)
LI2(ffl) = X(co)H\cof U2D(co) = |(u2( cd) + U2(w-
In each line, the first expression describes the input filtering operation. The 
second expression is due to aliasing in the vicinity of 0)= rtlTf and amounts 
to an application of (3.42) with two terms (converted to DFTs), one for m = 0 
and the second for m = 1.
For the inverse transform, that is, the second half of Figure 10.22, the fol­
lowing relationships are applicable in the range |<u| < n/T:
U1E(CO) = LI1D(<D);1 „ 
2 IE 2 2E
' 
1 h X(tB) = L2(£0)U1E(®) + HXw)U2E«0) (10.33)
II2E(®) = LTD(t»);J
The first two equations here are illustrated in Figure 10.20, and the third is 
evident from Figure 10.22.

304 
Digital Signal Processing with Examples in Matlab
Before proceeding to see whether X (co) can be made equal to X(co) in the 
presence of aliasing, we express the DFTs in (10.32) and (10.33) using z- 
transform notation, that is, with
z = ei<oT 
(10.34)
as in Chapter 4, (4.5). Then each DFT of the form X(co), or more precisely, 
X(e'“r), becomes X(z), and each DFT of the form U(co - n!T) becomes Lf(-z) 
via (10.34). If we make these substitutions in (10.33) and (10.34) and combine 
terms, we obtain the following result:
2X(z) = L2(z)[X(z)L'(z) + Xt-zlL^-z)] + H2(z)[X(z)Ji'(z) + X(-z)H1(-z)]
= X(z)[L1(z)L2(z) + H’(z)H2(z)J + X(-z)[L1(-z)L2(z) + H’(-z)H2(z)]
(10.35)
With X(z) expressed in this form, it is now easy to proceed.
To prevent aliasing in the output, we must not allow X(-z) to be part of 
its construction. Thus, in the second line of (10.35),
To prevent aliasing; L1(-z)L2(z) + H1(-z)H2(z) - 0 
(10.36)
With this constraint on the filters, the second term on the second line in 
(10.35) vanishes, and we can see the additional requirement for exact recovery 
of the input signal:
For exact recovery: L1(z)L:(z) + Hl(z)H2(z) = 2 
(10.37)
We consider now the choice of the input filters, L1 and H1. These are often 
chosen by first designing I?(z) as a linear-phase FIR filter, and then specifying
H (z) = l'(-z); that is, Jf'fco) = I? (co-tt/T) (10.38)
With this specification, we create (in terms of the DFT) H’(co) as the mirror 
image of L (a>), as illustrated in Figure 10.23. Again, note that the plots
m (rad/s) 
a> (rad/s)
FIGURE 10.23
Amplitude gains of lowpass and highpass FIR mirror-image filters, with H’(tu) equal to 
L’(o> - rr/T).

Signal Information, Coding, and Compression 
305
FIGURE 10.24
Signal decomposition and exact recovery with quadrature mirror filters. L(z) is any lowpass 
filter with cutoff at a>c = nUT, and the other filters are determined from L(z).
show DFT amplitudes over the range 0 < Ci)< ZrtT, rather than the usual range, 
0 < co < rtT, in order to emphasize the frequency shift in (10.38). Thus, with 
(10.38), we assure that if L\z) is a good lowpass filter, then h’(z) is a good 
highpass filter.
If we now impose the constraint in (10.38), we can eliminate H:(z) in (10.36) 
and (10.37), with the following result:
Prevent aliasing: L\z)L\-z) + H2(z)L\z) = 0
Exact recovery: L2(z)l?(z) + H2(z)I?(-z) = 0
With these two constraints, we have now specified the recovery filters in 
terms of the single filter, L'(z), that is,
L2(z) = L\z); H2(z) = -L1(-z) = -H1(z) 
(10.40)
The result is shown in Figure 10.24, where there is now only a single choice 
of a lowpass filter, L(z). Because the input filter gains are mirror images with 
respect to the quadrature frequency n/4T, the filters in this system are known 
as quadrature mirror filters.
Figure 10.24 contains the basic elements of this introductory discussion of 
multirate processing. The first half of the figure represents a signal decompo­
sition, and the second half represents an expansion, or recovery, of the signal. 
There are other considerations in the design and use of filters for multirate 
processing, the selection of the factor M, etc., including the use of adaptive 
filters in multirate systems, which are beyond our scope here. The reference by 
P.P. Vaidyanathan20 is particularly recommended for further study.
We can, however, make some observations about Figure 10.24 and its 
use in signal analysis and compression, as discussed, for example, with 
Figure 10.17. First, each filter in Figure 10.24 introduces a phase shift that 
we have ignored by plotting real spectra in Figures 10.19 and 10.20. If the 
filters are linear-phase FIR filters such as those described in Chapter 5, the 
phase shift from x to x in Figure 10.24 amounts to a signal delay of N - 1 
samples, where N is the number of weights in each filter. Thus, the entire

306 
Digital Signal Processing with Examples in Matlab
process can be made causal without causing distortion. Exercises 22 and 23 
illustrate this point.
Second, as we noted before, any transform that processes a waveform 
vector can be made two-dimensional, as discussed previously in connection 
with the FFT and the DCT. Let TR(x) represent any transformation of the 
signal vector x. When x is an array, TR(x) is defined to operate on the columns 
of x. Then the two-dimensional transform, TR2, is defined always as follows:
TR2{x} = TR{TR{x}.'}.' 
(10.41)
In the case of the multirate processing, the "transform" may be formed by 
ordering the output sequences from low to high frequency to form the output 
vector in each column ’of the transformed array.
Third, we also observe that the configuration in Figure 10.24 may be 
expanded in different ways to form a complete transform. Two of the most 
common ways are illustrated in Figures 10.25 and 10.26. In Figure 10.25, we 
see the first two stages of a decomposition with equal bands. Each successive 
stage decomposes its input vectors into twice as many output vectors, each 
output vector having half the number of samples in one of the input vectors. 
Thus, the frequency domain of x is partitioned into twice as many equal
FIGURE 10.25
Decomposition with equal bands.
FIGURE 10.26
Decomposition with octave bands.

Signal Information, Coding, and Compression 
307
FIGURE 10.27
Decomposition of a short speech segment into equal bands in accordance with Figure 10.25.
parts with each stage, and a transform with N stages has 2N components, that 
is, decimated waveform vectors. Note the superscript labeling of the down­
sampled vectors in Figure 10.25 (and 10.26 as well). These indicate the 
relative frequency bands, and the number of superscripts tells us how many 
times the length of the input has been reduced by a factor of two due to 
down-sampling.
Decomposition with octave bands is illustrated in Figure 10.26. In this case, 
only the low-frequency component is decomposed at each stage. The appli­
cability of this approach may be seen by examining the speech spectra in 
Figure 10.11 and the image spectrum in Figure 10.12. In general, the spectra 
of naturally occurring waveforms and images tend to be concentrated at low 
frequencies, and the amplitude spectrum tends to change at a faster rate at 
lower frequencies, thus justifying the uneven bandwidths in the transform. 
There is an advantage with the octave bands, because decomposition with 
N stages produces N + 1 components instead of the 2N components produced 
in the equal-band transform.
Using the speech signal in Figure 10.11, the equal- and octave-band decom­
positions are illustrated in Figure 10.27, which shows all the signals in the 
system, and Figure 10.28, which shows just the input and output signals. In 
the octave-band decomposition, we can observe that the signal energy is 
distributed more uniformly in the three high-frequency bands, and note that 
more stages in the system would decompose xLLL without changing the other 
three signals.

308 
Digital Signal Processing with Examples in Matlab
150
1501------------ ..1 . ■ ■ .U.----- ■ ■---------------■-------------- '■ ■ ------ ' ----  J
0 
0.05 
0 1 
0 15 
0.2 
0.25 
0.3 
0.35
t(s) 
t(s) 
t(s) 
t(s)
FIGURE 10.28
Decomposition of a short speech segment into octave bands in accordance with Figure 10.26
10.7 Time-Frequency Analysis and Wavelet Transforms
In this final section, we introduce the general concept known as time-fre­
quency analysis as a method for analyzing and transforming signals. Time­
frequency analysis is closely related to the transforms we discussed, that is, 
the DFT and the DCT, and also to multirate processing and filtering. To 
begin, let us reconsider the spectrogram, which was introduced in Chapter 6. 
In the illustration in Figure 6.20, we had an example of partitioning the time­
frequency plane as might be represented by Figure 10.29. The plane is 
divided into segments such that the time increment is the time step, T, and 
the frequency increment is n/NT, where N is the number of bandpass filters, 
that is, teeth in the comb filter. The height of each rectangle in Figure 10.29, 
which is k/NT rad/s, then represents the frequency resolution, that is, the pass­
band of the corresponding tooth in the comb filter. The width represents the 
time resolution, that is, the single step from kT to (k + 1)T in the output of the filter.
Now instead of the comb filter in Chapter 6, suppose we define a short-time 
discrete transform using a scheme similar to that used for spectral analysis in 
Chapter 7. That is, we use a sliding window, wk_„N, of length N and take (say) 
the DFT or the DCT at intervals of NT seconds (s) in time, resulting in a different 
set of N transform components every N time steps and partitioning the time­
frequency plane as illustrated in Figure 10.30. (In this case, the transform

Signal Information, Coding, and Compression 
309
FIGURE 10.29
Representation of how the spectrogram resolves time and frequency. Each waveform in the 
spectrogram is within a bandwidth of itlNT rad/s, and the points on each waveform are spaced 
T s apart.
FIGURE 10.30
A short-time transform, such as the short-time DCT, partitions the time-frequency plane into 
equal parts. The frequency resolution is n/NT rad/s, and the time resolution is NT s, where N 
is the length of the time-domain window.
components are saved rather than averaged as they were in Chapter 7.) The 
short-time DCT may be described by modifying (10.18) as follows:
X,“CT = 
m = O,1,...,N- 1; n = 0,l,...,«
k=0
(10.42)

310 
Digital Signal Processing with Examples in Matlab
FIGURE 10.31
Time-frequency resolution for the equal-band multirate filter with three stages and N = 8 output 
signals. Frequency resolution is nINT, and with down-sampling, the time resolution becomes 
NTs. 
'
In this description, wk.„N is the window that covers the time domain from 
nNT to (n + 1)NT s and selects the sequence in the range [x,,NT.. .x(n+1)NT). We 
now have a frequency resolution equal to n/NT rad/s and a time resolution 
equal to NT s, as shown in Figure 10.30. Note that the implied constant area 
(zr) of each rectangle illustrates the uncertainty principle. If we increase N in 
order to increase frequency resolution, we must accept a proportionate 
decrease in time resolution.
In the case of the short-time discrete transform, we have something quite 
similar to the equal-band multirate filter described previously. Down-sampling 
by the factor N, as illustrated, for example, for N = 4 in Figure 10.25, increases 
the time step from T to NT, as we have seen, and produces signals with 
frequencies at multiples of n/NT rad/s. Thus, the time-frequency resolution 
for equal-band multirate processing is also depicted in Figure 10.30 and is 
the same as that for the short-time DCT. To emphasize the time resolution 
produced by down-sampling, we use Figure 10.31, which illustrates the 
same resolution in frequency and time with three stages, that is N - 8.
Short-time transforms and equal-band coding do not, however, have the 
advantage of octave-band coding with its nonuniform frequency resolution. 
This is an important advantage to have with the encoding of data from 
natural sources, because, as we mentioned previously, the spectra of these 
data generally tend to decrease in proportion to frequency, that is, as 1/co. In 
the example used in Figure 10.26, octave-band coding partitions the time­
frequency plane as shown in Figure 10.32. Note that the area of each rectangle 
(time-frequency resolution) is constant Ct).
Wavelet transforms ' may be viewed as a generalization of the octave­
band coding concept. Suppose we view the three-stage system in Figure 10.26

Signal Information, Coding, and Compression
311
FIGURE 10.32
Time-frequency resolution for the octave-band multirate filter with three stages and four output 
signals. Frequency resolution decreases with frequency, time resolution increases with frequency, 
and the product of the two remains constant and equal to tr.
FIGURE 10.33
Filter structure for the wavelet transform, which is the same as the octave-band structure in 
Figure 10.26 with filtering and down-sampling combined into single operations. The wavelet 
transform components are y, through y4.
as a single filter bank with varying bandwidths, as in Figure 10.33. Each 
transfer function, H"'(z), is the overall transfer function of the corresponding 
channel in Figure 10.26, with down-sampling occurring all at once instead of 
one stage at a time. For convenience, we assume the signal is unbounded and 
(as before) the filters are noncausal, with impulse responses centered around 

312
Digital Signal Processing with Examples in Matlab
zero and also unbounded, so in the time domain, we may express each filter 
output as follows:
lT”(z) = X(z)H"'(z); u™ = 
x„h"'_„; m = 0,l,2,3; 
(10.43)
n =— °°
Each output, y", is down-sampled according to its position in Figure 10.33, 
and so we have
111 XT' i Ml
Vfc ~~ z f ^n”-ik-nf
J! =-oo
m = 0,1,2,3 
i = 8,8,4,2
-OO < k < oo
(10.44)
It is easy to see how to extend this and add stages to Figure 26; for example, 
m = [0 1 2 3 4] and i = [16 16 8 4 2], and so on. The decimated signals, yf 
are called wavelet coefficients, and together they comprise the wavelet transform 
of x. The wavelets, h™, may be derived in different ways, and their derivation 
is the subject of texts on wavelet transforms, several of which are listed in 
the references. At the time of this writing, Matlab also offers a special toolbox 
of wavelet transforms.
10.8 Exercises
General Instructions: A few of the exercises in this section involve processing 
a signal from the dsp data folder, which is part of the software with this text. 
When this is the case, you may use the Matlab file display jdata to display all 
of the signals in a single figure, and then use the expressions in display_data 
to read a specific file. In doing so, you may need to modify the path to the 
folder. Executing display_data should result in a color version of the two 
signals and the image in Figure 10.34.
1. The gray-scale image in Figure 10.13 has 256 pixels in each of 256 
rows. Each pixel has a range of 256 levels of gray, ranging from 
black (0) to white (255).
a. How many different images are possible?
b. If all possible images are equally likely, how much information 
is contained in a single image?
2. You receive the following binary message from a source which 
sends one or zero with equal probability:
x = [0 100001101101000011010010111000 0]
a. What is the entropy of the message, based on the source prob­
abilities?

Signal Information, Coding, and Compression
313
Speech 
1501--------------------------------
x 104
FIGURE 10.34
Speech waveform, seismic waveform, and image used in several of the exercises.
b. What is the entropy measure that would be used for encoding 
x, based on its content?
c. The message actually consists of consecutive 8-bit ASCH symbols 
for the word "Chip." Compute the information received in these 
four symbols, assuming all ASCII symbols are equally likely.
3. Suppose we wish to encode the following message using a Huffman 
or arithmetic code:
Hatred stirs up strife, but love covers all offenses.
a. How many symbols are in the message, including spaces and 
punctuation?
b. How many different symbols are in the message, including spaces 
and punctuation?
c. Write all the different symbols in a row vector, s. Underneath s, 
write a vector/ containing the symbol frequencies expressed as 
fractions. Assume that s and / are to be included with the en­
coded information.
d. Show how to compute the average entropy of the symbols in 
the message, and compute it.
4. Suppose the sample vector, x, plotted with the circular (o) symbol 
in Figure 10.1, is to be entropy-encoded using its own statistics. What 
is the smallest number of bits you could have in the encoded version, 
not counting the frequency table and other similar extra information?

314 
Digital Signal Processing with Examples in Matlab
5. What is the average entropy in bits per symbol of the two wave­
forms in Figure 10.1, taken together?
6. Given the sample vector, x, plotted with the circular (o) symbol in 
Figure 10.1, complete the following:
a. Describe the symbol set.
b. Develop a frequency table in terms of [N/x(h)].
c. Develop a Huffman code tree following the procedure in (10.9), 
and draw the tree.
d. Using the code tree, tabulate the set of symbol codes and code 
lengths.
e. Encode the entire message, and express the result as a binary 
string.
f. Compute the total entropy of x, and compare the result with 
the length of (e).
7. (See the note above Exercise 1.) Quantize the speech signal in Figure 
10.34 to integers in the range [0,7], plot the result, and do parts (a) 
through (f) in the previous exercise.
8. (See the note above Exercise 1.) For the speech signal in Figure 10.34, 
complete the following:
a. Compute the minimum number of bits required for entropy 
coding.
b. Use the hjcodes function to produce the complete set of binary 
symbol codes. Print the codes in binary. Find the shortest code, 
and verify its prefix property.
c. Compute the length of the encoded signal and compare with 
(a). (You do not need to construct the encoded signal.)
9. Given the sample vector, x, plotted with the circular (o) symbol in 
Figure 10.1, complete the following:
a. Describe the symbol set.
b. Develop and print a frequency table.
c. Use function a_encode to encode the entire message. Express the 
result as a binary string.
d. Compute the total entropy of x and compare the result with the 
length of (c).
10. Given the message vector in Exercise 3, assume the symbol set 
consists only of the ASCII symbols actually used in the message, 
and do parts (a) through (d) in the previous exercise.
11. (See the note above Exercise 1.) Letx(l:30000) represent the seismic 
signal in Figure 10.34.
a. Compute the minimum number of bits required for entropy 
coding of x(9000:9999).

Signal Information, Coding, and Compression 
315
b. Use the a_encode function to encode *(9000:9999). Print the first 
32 bits of the output.
c. Compute the length of the encoded signal and compare with (a).
12. (See the note above Exercise 1.) Let x represent the seismic signal 
in Figure 10.34.
a. Filter* using a least-squares, one-step predictor with six weights. 
Plot x and prediction error, e, in separate subplots. Do not include 
startup values ea-e5.
b. Encode e with e0-es excluded. Compare the length of the Huffman- 
coded version with the entropy of x and the entropy of e.
13. (See the note above Exercise 1.) Let x represent the seismic signal in 
Figure 10.34.
a. Compute the DFT vector, X. Append the imaginary part of X 
to the real part. Scale the resulting real vector (Y) to the range 
[0,255], and plot the result.
b. Using Huffman coding, compare the length of the encoded ver­
sion of Y with the entropy of Y and the entropy of X.
c. Do part (b) using arithmetic coding. Comment on the results of 
(b) and (c).
Exercises 14 through 19 are specified in the table below. In each case, a 
waveform is processed in the manner described. The note above Exercise 1 
explains how to obtain and plot the signals. In each exercise, do the following:
a. Process the signal before coding as described, except in Exercises 
14 and 15.
b. Plot the signals (and processed signals, except in Exercises 14 
and 15).
c. Shift the values so the minimum is zero and the range is 
[0,255]. Construct a Huffman code list consisting of [symbol, 
code length] for each symbol, and print the list.
d. Without actually creating the encoded message, compute the 
message length and the resulting compression ratio assuming 
eight bits per symbol in the original signal, and compare it with 
the results of other exercises you have worked in the same 
column of the table below.
Speech 
Signal
Seismic 
Signal
Huffman coding only
Ex. 14
Ex. 15
Predictive coding quantized to integers; six weights 
Huffman coding
Ex. 16
Ex. 17
DCT of (signal - mean value), quantized to eight bits 
Huffman coding
Ex. 18
Ex. 19

316 
Digital Signal Processing with Examples in Matlab
20. Process the speech signal as in Exercise 18, but in place of the DCT, 
use a four-stage octave-band encoder with FIR filters having seven 
weights each and using the Hamming window.
a. Plot the concatenated encoder output, from low to high frequency.
b. Translate the output to integers in the range [0,255]. Find and 
print a set of Huffman codes.
c. Compute the length of the encoded message and the compres­
sion ratio.
21. (See the note above Exercise 1.) For the color version of the image 
in Figure 10.34, note that the array dimensions are [256,256,3]; that 
is, there are three arrays containing the red, green, and blue pixels 
of the image. Compute the two-dimensional DCT of each of the 
three arrays and combine these into the array X with dimensions 
[256,256,3], Delete the d. c. component, that is, X(l,l,:). Create a 
color figure with two images, each about 3.5" on a side. Put the 
original color image on the left, and on the right, display the seg­
ment X(l:10,l:10,:) as a color image. Comment on how the two­
dimensional DCT components change with the spacial frequency.
22. Refer to the diagram in Figure 10.24, which illustrates a quadrature­
mirror filter (qmf) stage with recovery. Make a figure with the 
following eight subplots (4 by 2), in order:
1. Chirping sinusoidal signal, xn vs. n, with 128 samples given by
x=sin(2*pi*[0:127] .* 1inspace(.1, . 4,128)/4) .
2. Amplitude gain of L(z), a lowpass, linear-phase FIR filter designed 
with N = 13 weights and the Hamming window, and also the 
amplitude gain of the mirror filter, L(-z).
3. Signal u',D vs. n.
4. Signal uJD vs. n.
5. Signal u1^ vs. n.
6. Signal t^E vs. n.
7. Reconstructed signal x„vs. n.
8. Signals x„ and x„_N together vs. n. These two signals should be 
nearly the same. Explain why the shift of N samples is necessary 
to align the two signals.
23. Refer to the three-stage, octave-band filter in Figure 10.26. In this 
exercise, we will use a similar filter with M = 4 stages to partition 
the speech signal, x, described in the note above.
a. First, truncate x so its length, K, is the largest possible multiple 
of 2m. What is the revised value of X?

Signal Information, Coding, and Compression 
317
b. Filter x through four stages with each stage using N - 21 weights 
and the Hamming window, producing eight signals similar to 
the four signals in Figure 10.27. Concatenate the eight signals 
into a single vector, e, with K samples.
c. Process e through the mirror image of the octave-band filter, 
and sum the result to produce the reconstructed signal, x .
24. In Section 10.3 we saw how predictive coding could be applied to 
reduce the entropy of a signal prior to entropy coding, following 
the scheme in Figure 10.4. Apply this scheme to the waveform in 
Figure 10.8.
a. Apply the one-step predictor used in Figure 10.3, and make a 
plot similar to Figure 10.3 of the signal and the prediction 
error.
b. Compute the entropy of the original signal.
c. Compute and compare the entropy of the prediction error, e, 
excluding e0.
25. Prove by substitution into (10.18) that the inverse DCT formula in 
(10.19) is correct.
26. By using the inverse DFT formula, (3.23), prove that the fast version 
of the inverse DCT in (10.26) is identically the same as the original 
formula in (10.19).
References
1. Shannon, C.E., A Mathematical Theory of Communication, Bell System Tech J., 
27, 379 and 623, 1948.
2. Ingels, P.M., Information and Coding Theory, Intext, Scranton, PA, 1971.
3. Hamming, R.W., Coding and Information Theory, Prentice Hall, Englewood Cliffs, 
NJ, 1980.
4. Held, G. and Marshall, T., Data Compression: Techniques and Applications, Hardware 
and Software Considerations, John Wiley & Sons, New York, 1991.
5. Golomb, S.W., Peile, R.E., and Schotz, R.A., Basic Concepts in Information Theory 
and Coding, Plenum Press, New York, 1994.
6. Sayood, K., Introduction to Data Compression, Morgan Kaufman Publishers, Inc., 
1996.
7. Rao, K.R. and Hwang, J.J., Techniques and Standards for Image, Video, and Audio 
Coding, Prentice Hall, Upper Saddle River, NJ, 1996, chap. 5.
8. Ziv, J. and Lempel, A., A Universal Algorithm for Sequential Data Compression, 
IEEE Trans, on Information Theory (23)3, p 337, May 1977.
9. Nelson, M.R., LZW Data Compression, Dr. Dobb's Journal, 29, 86, Oct. 1989.
10. Regan, S.M., LZW Revisited, Dr. Dobb's Journal, 126, 167, June 1990.
11. Gersho, A. and Gray, R.M., Vector Quantization and Signal Compression, Kluwer 
Academic Publishers, Norwell, MA, 1992.

318 
Digital Signal Processing with Examples in Matlab
12. Huffman, D.A., A method for the construction of minimum-redundancy codes, 
Proc. IRE 40, 1098, 1952.
13. Rissanen, J. and Langdon, G.G., Jr., Arithmetic coding, IBM J. of Research and Dev., 
23, 149, Mar. 1979.
14. Witten, I.H., Neal, R.M., and Cleary, J.G., Arithmetic coding for data compres­
sion, Comm. ACM., 30, 6, 520, June 1987.
15. Stearns, S.D., Arithmetic coding in lossless waveform compression, IEEE Trans, 
on Signal Processing, 43, 8, 1874, Aug. 1995.
16. Jayant, N.S. and Noll, P., Digital Coding of Waveforms—Principles and Applications 
to Speech and Video, Prentice Hall, Englewood Cliffs, NJ, 1984.
17. Ahmed, N. and Rao, K.R., Orthogonal Transforms for Digital Signal Processing, 
Springer-Verlag, New York, 1975.
18. Rao, K.R. and Yip, P., Discrete Cosine Transform: Algorithms, Advantages, Applica­
tions, Academic Press) Boston, MA, 1990.
19. Crochiere, R.E. and Rabiner, L.R., Mulltirate Digital Signal Processing, Prentice 
Hall, Englewood Cliffs, NJ, 1983.
20. Vaidyanathan, P.P., Multirate Systems and Filter Banks, Prentice Hall, Englewood 
Cliffs, NJ, 1993.
21. Shynk, J., Frequency-domain and multirate adaptive filtering, IEEE SP Magazine, 
14,'Jan. 1992.
22. Vetterli, M. and Kovacevic, J., Wavelets and Subband Coding, Prentice Hall, Engle­
wood Cliffs, NJ, 1995.
23. Burrus, C.S., Gopinath, R.A., and Guo, H., Introduction to Wavelets and Wavelet 
Transforms, Prentice Hall, Upper Saddle River, NJ, 1998.
24. Boggess, A. and Narcowich, F.J., A First Course in Wavelets with Fourier Analysis, 
Prentice Hall, Upper Saddle River, NJ, 2001.
25. Strang, G. and Nguyen, T., Wavelets and Filter Banks, Wellesley-Cambridge Press, 
Wellesley, MA, 1996.
26. Saha, S., Image compression—from DCT to wavelets: A review, ACM Crossroads 
(ACM electronic publication), 
s, Feb. 2002.
www.acm.org/crossroad

Index
Adaptation
geometry of, 241, 246
as process, 241-242
speed of, 263-264
Adaptive algorithm, 242
Adaptive arithmetic coding, 290
Adaptive array, 267-268
Adaptive beamforming, 268
Adaptive filter, 242, 267
Adaptive linear combiner, 268, 270
Adaptive predictor, least-mean-square, 
258
Adaptive signal compression, 256-258 
gradient in, 244, 252, 264
Adaptive signal processing, 241-275
algorithms for
direct descent, 258-263
least-mean-square, 241, 249, 
254-258
one-step, 245
other, 267-268
random search, 243
recursive least squares, 258-263
steepest-descent, 249-254, 258
applications of, 73, 199
convergence in
under ideal conditions, 246-249, 
252
learning curve for weights, 
247-248, 255
parameter for, 245-246
as performance measure, 263-264, 
266-267
recursive least squares, 261-263
steepest-descent algorithm, 249, 
251-254, 258
time constant for forgetting 
correlation estimate, 260 
time constant for LMS, 247-248 
time constant for MSE, 247-248 
time constant for weights, 246-247 
time constant under ideal
conditions, 252
weight factors, 246-247, 251-252, 
255, 258, 263
eigenvalues in, 250-253, 264, 266 
exercises for, 268-273
finite impulse response filters in, 
243-244, 267-268
introduction to, 241-242 
mean-squared error in
algorithm for, 241-242
gradient of, 244
least-mean-square convergence 
with, 255-257 
'
as performance measure, 263-267 
performance surface of, 242-244 
search algorithms for, 243-249 
vector form of, 244
performance measures of, 263-267 
misadjustment as, 263-267 
time constants as, 263-264, 266-267
performance surface of
mean-squared error and, 242-244 
quadratic, 243, 247-248
search algorithms for, 243-249
prediction in, 255-258
in real time, 241, 243
recovery factors, 258
time constants in
for forgetting correlation estimate, 
260
for mean-squared error, 247-248 
as system performance measure, 
263-264, 266-267
for weight convergence, 246-247, 
252, 255
weights in, 243-249
319

320 
Digital Signal Processing with Examples in Matlab
convergence factors, 246-247, 
251-252, 255, 258, 263
learning curve for, 247-249, 255
LMS examples of, 255-258 
recursive least squares and, 258, 
261-263
system performance measures, 
263-267
Additive noise, in least-squares system 
design, 228
Algebra
in linear systems, 79-80
vector and matrix, using Matlab 
notation, 5-11 
.
in window formulas, for finite
impulse response filters, 119-121 
Algorithm(s)
for adaptive signal processing 
direct descent, 258-263 
least-mean-square, 241, 249, 
254-258
one-step, 245
other, 267-268
random search, 243
recursive least squares, 258-263
steepest-descent, 249-254, 258
for Fast Fourier transform, 45-48, 
99-103
Levinson's, 206
for linear systems, 87-103
direct, 87-89, 91
fast Fourier transform, 99-103
lattice, 91-99
state-space, 89-91
for mean-squared error minimization, 
205-206
Aliasing
in least-squares system design, 201-203 
with multirate signal decomposition,
300, 302-304, 306
of signal spectrum, 39, 65-66
cross-power, 192-193
prevention of, 181-182
Allpass filter
in finite impulse response, 122
in infinite impulse response, 160, 162
Amplitude distribution(s)
continuous vs. discrete, 168-170
of random signals
introduction to, 167-168
principles of, 168-171
Amplitude response (gain)
of finite impulse response 
differentiator, 127-128
of infinite impulse response filters, 
138, 157-162
in linear systems, 77-79
Amplitude spectrum
estimation of, see Spectral estimation 
of signal vector, 48-51
Analog-digital conversion
in finite impulse response filters, 
115-116
in infinite impulse response filters, 
147-149
bilinear transformation for, 
149-153
of random signals, 169
Analog prefiltering, 181-182
Arithmetic coding, 287-291
adaptive, 290
Array
adaptive, 267-268
image, 5
index, in mean-squared error 
minimization, 205
in Matlab language, 3-5
Array operations
in algebra notation, 10-11
summary of, 6
Array products
in fading operation, 8-9
in linear discrete system, 90
types of, 7-8
Autocorrelation, 40
in cross-power spectrum, 191-192
in least-squares system design, 213, 
218, 229 
’
of signal power spectrum, 181-183 
Autocorrelation matrix
in least-squares equations, 205 
inverse, in adaptive signal processing, 
258-260 
' 
’
normal form of, in adaptive signal 
processing, 251
Autocorrelation vector, in mean- 
squared error minimization, 205
Autocovariance matrix, in least-squares 
system design, 211-213 
with broadband noise, 229-230
Average cross-power, 191
Average periodogram, in power spectral 
estimation, 185
Average periodogram method, of power 
spectral estimation, 186-187
Average power, of random stationary 
signals, 177-178, 181 
'
example of, 181-182
Average product, of two signals, 40

321
index
B
Backslash, as algebraic operation, 11 
Bandpass filter
finite impulse response, 121-124
infinite impulse response, 135, 
147-148, 153-155
Bandstop filter
finite impulse response, 121-124
infinite impulse response, 135, 
147-148, 153-154
Bandwidth, in infinite impulse response 
frequency translation, 147-149
Bar graph, of power density, 180 
Beamforming, adaptive, in signal 
processing, 268
Bessel function, in Kaiser window, for 
finite impulse response filters, 
121
Bias weight, 232
Bilinear transformation, for analog­
digital conversion, in infinite 
impulse response filters, 149-153
Binary code tree, in Huffman coding, 
284-286
Binary digit, see Bits
Bits
coding of, 282; see also Coding
compression of, 277; see also 
Compression
as information unit, 278
two states of, 278
Blackman window formula
for finite impulse response filters, 
119-120
in spectral estimation, 190
Blind equalization, in adaptive signal 
processing, 268
Block adaptive process, 242
Block fast Fourier transform filtering, in 
linear systems, 103
Block processing, in least-squares 
system design, 210
"Boats," as image compression example, 
296-297
Boxcar window
for lowpass finite impulse response 
filters, 116, 118
in spectral estimation, 189-190 
Broadband noise, effects on least­
squares system design, 228-230 
Butterworth filters, as infinite impulse 
response filters, 137-141
Chebyshev vs., 145-146
Byte, as information unit, 278
Cascade filter structure
in infinite impulse response filters, 
139-140, 144, 148, 155-158, 162
of linear systems, 84-85
Causality
of discrete linear systems, 72, 86
in finite impulse response filters, 114
Channel noise, in least-squares system 
design, 228
Characteristic equation, in steepest- 
descent algorithm, 250
Chebyshev filters, as infinite impulse 
response filters, 141-146
Butterworth vs., 145-146
Code lengths, in Huffman coding, 287
Coding, of signals
adaptive, 290
amplitude distribution and, 176
arithmetic, 287-291
entropy, 281-291
equal-band, 306-307, 310
exercises for, 312-317
fixed-symbol, 283-287, 290
Huffman, 283-287, 290
linear predictive, 201, 224, 281, 298
multirate processing for, 291, 298-308
objective of, 277, 291
octave-band, 306-307, 310-312
predictive, 281-282, 292, 298
string, 282
subband, 298-299, 306-308
terms used with, 278
transform, 282, 291-298
vector quantization as, 282
Coefficient(s)
in adaptive signal processing, 243-249 
convergence factors, 246-247,
251-252, 255, 258, 263
learning curve for, 247-249, 255
LMS examples of, 255-258 
recursive least squares and, 258, 
261-263
in system performance measures, 
263-267
in discrete linear systems, 72-73, 
98-99, 101
in finite impulse response filters, 
111-114, 121-124
formula for finding, 126 
in infinite impulse response filters, 
135, 141
analog-to-digital conversion of, 
154-157

322 
Digital Signal Processing with Examples in Matlab
in least-squares system design 
as bias, 232
identification example, 218-219 
for interference cancellation, 221, 
223, 230, 232
for optimal mean-squared error 
minimization, 205-210
Column vector
in algebra notation, 7
in least squares, 22
Comb filter, infinite impulse response, 
159-160
Commands, in Matlab language, 3 
Complex plane, description of, 12-13 
Complex products, in Fourier 
transforms, discrete vs. fast, 46
Complex signals
finite impulse response filter 
processing of, 129
instantaneous power of, 177 
Components
low-frequency vs. high-frequency, in 
image compression, 296-298 
in multirate signal decomposition, 
307 '
Compression, of signals 
adaptive vs. nonadaptive, 256-258 
amplitude distribution and, 176 
entropy coding as, 281-291 
exercises for, 312-317 
with inverse transfer functions, 86,224 
linear predictor for, 201, 224-225, 227 
multirate processing for, 291, 298-308 
objective of, 277, 291 
power spectrum and, 181 
prediction error in, 281-282 
transform coding as, 282, 291-298 
two stages for, 280-282
Compression ratio, in least-squares 
system design, 227
Conjugate pairs
of fractions, in linear systems, 80, 84 
of poles, in infinite impulse response 
filters, 139, 153-154
Continuous amplitude distributions, 
168-170
Continuous form, of Fourier series, 
30-32
Continuous Fourier transform
in ideal lowpass filter, 112 
inverse, 59, 113
of signal spectrum, 39, 58-60
Continuous least-squares fit, 20
Continuous probability function, in 
uniform random signals, 172
Contour plot, in mean-squared error 
minimization, 208-209, 233
Convergence, in adaptive signal 
processing
under ideal conditions, 246-249, 252 
learning curve for weights, 247-248, 
255 
'
parameter for, 245-246
as performance measure, 263-264, 
266-267
recursive least squares and, 261-263 
steepest-descent algorithm for, 249, 
251-254, 258
time constant for forgetting 
correlation estimate, 260
time constant for LMS, 254-255 
time constant for MSE, 247-248 
time constant for weights, 246-247 
time constant under ideal conditions, 
252
weight factors, 246-247,251-252, 255, 
258, 263
Convolution
in least-squares system design, 
212-213
of linear equations 
discrete, 74-75 
fast Fourier transform algorithms 
and, 99-103
periodic, 100-101
z-transform of, 75-76
in lowpass finite impulse response 
filters, 116-119
Correlation
in adaptive signal processing, 
recursive estimate of, 258-260
in cross-power spectrum, 192 
exercises for, 66-69
in least-squares equations, 205-207 
broadband noise effects on, 229-230 
computation of, 212-214 
system identification example, 
218-219
in signal processing, 39-42 
Correlation detector, 41-42 
Correlation estimate, recursive, in 
adaptive signal processing, 
258-260
Cosine(s)
basic applications of, 13
in Fourier series, 26, 28-29, 39
in infinite impulse response filters, 
141-145
Covariance function, in least-squares 
equations, 211-212 

Index
323
computation of, 212-214 
under noisy conditions, 229-230 
system identification example, 
218-219
Cross-correlation function, 192 
least-squares computations with, 
212-213
broadband noise effects on, 
229-230
Cross-covariance, in least-squares 
system design
broadband noise effects on, 229-230 
computations with, 213-214 
equations for, 211
Cross-periodogram, 191
Cross-power density, 191
Cross-power spectrum, 190-193
Cross spectrum, 190-193
Cutoff frequency, in infinite impulse 
response filters, 140
D
Data files, in Matlab, 3, 14, 312
Data transformation, encryption as, 
277-278 
'
Data windows, see Window entries
DCT, see Discrete cosine transform 
(DCT)
Decimation
in finite impulse response filters, 
115-116
in multirate signal decomposition, 
299-302, 312
Decoding, entropy, with multirate signal 
decomposition, 299-300
Decomposition, see Signal 
decomposition
Decorrelation, in least-squares system 
design, 201
Degrees of freedom
in power spectral estimation, 184
in sampled waveforms, 4
Dependent variable, in discrete linear 
systems, 71-72
Determinant, in algebra notation, 11
DFT, see Discrete Fourier transform 
(DFT)
Diagonal matrix, in adaptive signal 
processing, 250-253, 261
Difference equations, 71
Digital filters
discrete linear systems and, 103-104 
as finite impulse response filters, 111, 
129
as infinite impulse response filters, 
137, 145
designing, 153-154
transformation to, 149-153 
writing functions for, 153-157
Digital resonator, for infinite impulse 
response filters, 158
Digital signal processing (DSP) 
adaptive, see Adaptive signal 
processing
algebra review for, 5-11 
geometry review for, 12-14 
introduction to, 1-2, 6 
language for, see Matlab
Matlab functions in, 3,14-15; see also 
Functions
text review techniques, 2, 15-16 
Direct algorithms, for linear systems, 
87-89,91 
'
conversion to lattice, 94-95 
lattice conversion to, 93-94
Direct descent algorithm, for adaptive 
signal processing, 258-263
Direct form, of linear systems, 87-89 
Direct-to-lattice conversion, in linear 
systems, 94-95 
nonrecursive, 97-98
Direction-finding system, 236-237
Discrete amplitude distributions, 
168-170
Discrete convolution, of linear 
equations, 74-75
Discrete cosine transform (DCT) 
short-time, in signal analysis, 308-310 
as signal compression, 292-298
discrete Fourier transform function 
with, 292-296
fast Fourier transform computation 
of, 294-295
inverse, 292-293, 295
multirate processing vs., 300, 306 
two-dimensional, 295-298
Discrete Fourier series 
continuous form of, 30-32 
exercises for, 33-36 
fundamentals of, 26-27 
as least-squares approximation, 27-30 
Matlab code implications, 30-31 
summary of, 30-31
Discrete Fourier transform (DFT) 
complex products in, 46 
continuous, 54, 58-60, 112 
discrete cosine transform function of, 
292-296
frequency increments in, 50

324
Digital Signal Processing with Examples in Matlab
interpolation of time-domain in, 53, 
55, 63-64
inverse, 51-52, 62 
in multirate signal decompression, 
300-305
principles of, 42-43 
properties of, 52-57 
redundancy in, 43-45, 52, 62 
resampling of, 52, 68
in spectral analysis, 39, 43-44, 48-51 
z-transforms vs., 75-77
Discrete least-squares process, 20, 22, 24
Discrete linear equations 
continuous systems vs., 71 
convolution of, 74—75 
■ '
properties of, 71-73 
state-space, 89-91
Discrete linear systems 
causality of, 72, 86 
coefficients in, 72-73, 98-99, 101 
digital filters and, 103-104 
realizability of, 72, 85-86 
state-space equations for, 89-91 
time domain in, 71, 73-75, 85 
variables in, 71-72, 75-76
Distortion, see Aliasing
Distributed minimum of error surface, 
210
Down-sampling 
in finite impulse response filters, 
115-116
with multirate signal decomposition, 
299-302, 310-312
DSP, see Digital signal processing 
(DSP) ' 
'
E
EC, see Entropy coding (EC)
ED (entropy decoding), with multirate 
signal decomposition, 299-300
Eigenvalues, in adaptive signal 
processing, 250-253, 264, 266
Eigenvector, in adaptive signal 
processing, 250
Eigenvector matrix, in adaptive signal 
processing, 251
Encoding, of signals, see Coding 
Encryption
as data transformation, 277-278 
with inverse transfer functions, 86, 
224
in least-squares system design, 224, 
227 
'
Energy density, of signals, 178, 180
Entropy, as information measure, 279 
Entropy coding (EC), as signal 
compression, 281-283 
arithmetic scheme for, 287-291 
Huffman scheme for, 283-287 
in multirate decomposition, 299-300
Entropy decoding (ED), with multirate 
signal decomposition, 299-300
Envelope, exponential, imposed by 
array products, 9-10
Equal-band coding, 306-307, 310 
Equal bands, in multirate signal 
decomposition, 306-307, 310
Equalization
blind, in adaptive signal processing, 
268
in least-squares system design, 199, 
201-202
methods for, 215-217
Equalizer, in least-squares system 
design, 215-216
Equal-ripple property, of infinite 
impulse response filters, 141-142
Equation error, for least-squares HR 
system identification, 235-236
Error(s)
quantizing, in sampling theorem, 61 
total squared, in least squares, 19-21, 
24
in waveform reconstruction, 64
Error correction, in signals, 277
Error detection, in signals, 277
Error funebon, in Gaussian amplitude 
distributions, 174
Error surface, distributed minimum of, 
210
Event, in information measurement, 279 
Expansion, with multirate signal 
decomposition, 302, 305
Expected power, of stationary function, 
177
Expected value, in correlation, 40 
Exponential envelope, imposed by array 
products, 9-10
Exponentiation, as algebraic operation, 
11 '
Expression simplification, geometric 
formulas for, 12-13
Extension
periodic, in linear systems, 100 
zero, of discrete Fourier transform, 
52-54

Index
325
F
Fading operation, for array products, 
8-9
Fast Fourier transform (FFT) 
algorithms for, 45-48
in linear systems, 99-103
complex products in, 46
discrete cosine transform
computation with, 294-295
filtering in linear systems
block, 103
nonrecursive, 101-102
in multirate signal decompression, 
300, 306
Feedback control systems, 86-87 
Feedback structure, of linear systems, 
85-86
FFT, see Fast Fourier transform (FFT) 
Filter(s)
in adaptive signal processing, 
242-244, 267-268
in signal processing, 103,157, 235, 268
Filter gain, see Power gain
Filtering
in linear systems
block fast Fourier transform, 103
cascade, 84-85
digital, 103-104, 111, 129
finite, see Finite impulse response
(FIR) filters
infinite, see Infinite impulse 
response (HR) filters
inverse, 223-224
lattice, 96, 98
nonrecursive fast Fourier
transform, 101-102
power gain of, 103-104
recursive vs. nonrecursive, 73 
multirate signal decomposition as, 
298-305
Finite impulse response (FIR) 
differentiator, 126-128
Finite impulse response (FIR) filters, 
111-134
in adaptive signal processing, 
242-244, 267-268
advantages of, 111-112
allpass, 122
bandpass, 121-124
bandstop, 121-124
complete example of, 125-126
digital, 111, 129
disadvantages of, 111
exercises for, 130-133
Hibert transformer as, 128-129 
highpass, 121-124 
introduction to, 73, 111-112 
least-squares design with, 210-212, 
235
linear phase of, 111-112, 114 
lowpass
conversion to other types, 121-124 
ideal characteristics of, 112-113 
window functions for, 116-121
in multirate signal decomposition, 
302, 304-305
other types of, 126-129 
passband ripple in, 114-115, 119, 121 
power gain factors, 112, 114-115 
window spectrum as, 117-124
realizable version of, 113-116 
stopband ripple in, 115, 119, 121 
truncation of, 115, 117, 121
weights of, 111-114, 121-124
formula for finding, 126
Finite signal vectors, in least-squares 
system design, 210-212
FIR (finite impulse response) 
differentiator, 126-128
FIR filters, see Finite impulse response 
(FIR) filters
Fixed-symbol coding, 283-287, 290 
Floating-point operations, in fast
Fourier transforms, 47-48 
Flops, in fast Fourier transforms, 
47-48
Forward transform, in multirate signal 
decomposition, 303
Fourier coefficients
in correlation function, 42—43 
principles of, 28-31
Fourier Series
discrete, see Discrete Fourier series 
harmonic analysis in, 13, 26, 28 
introduction to, 19
reconstruction of signals, 62, 69
Fourier spectra, 39 
analysis of, 43, 48-51 
exercises for, 66-69 
frequency principles in, 43, 48-51
Fourier transforms (FT), of signal 
spectrum, 39
analytic applications of, 48-51 
continuous, 54, 58-60, 112 
discrete, see Discrete Fourier
transform (DFT)
fast, see Fast Fourier transform (FFT) 
in ideal lowpass filter, 112-113 
inverse, 51-52, 59, 62, 113

326 
Digital Signal Processing with Examples in Matlab
Fractions, in linear systems, 80-81, 
84-85 '
Frequency and frequency domain 
of amplitude distributions, 168, 
175-176
in discrete linear systems, 71, 75-76 
fundamental, in Fourier series, 27-29 
as image compression factor, 296-297 
of lowpass finite impulse response
filters, 117
in multirate signal decomposition, 
306-312
signal power density and, 177-178, 
180-181
in signal processing, 39 • • 
Frequency conversion factors, in
spectral analysis, 49
Frequency function, of digitized
random signals, 169
Frequency increments, in spectral 
analysis, 50
Frequency resolution, of signal 
information, 308-310
Frequency scales, in spectral analysis, 
48-50
Frequency translations, analog-to-digital 
in finite impulse response filters,
115-116
in infinite impulse response filters, 
147-149
bilinear transformation for, 149-153 
Frequency units
in finite impulse response filters, 112
in signal power analysis, 180
FT, see Fourier transforms (FT)
Functions, Matlab
a_decode, 289-290
a_eitcode, 289-290
angle, 51
autocorr, 213
autocorrjnat, 213
autocovar_mat, 214
bar2,194
bessel_O, 121
bilin, 154-155
bw_analog_weights, 154-155
bw_lowpass_size, 154
bw_weights, 156
ch_analog_weights, 154
ch_lowpass_size, 154
ch_weights, 156
codejength, 287
contour, 208-209, 233
com, 103
crosscorr, 213
crosscovar, 214
deconv, 103
dirjvjat, 95
display _data, 312 
in DSP, 3, 14-15 
erf, 174
#6 47
Jft2, 296
Jftshift, 51, 55
filters, 87, 103, 157
fir_weights, 121, 123-124
flops, 47-48
freq, 193, 280, 287
h_codes, 287
ifft, 52
image_scan, 297
imp_resp, 103
index_mat, 205
lat_filter, 96 
lat_to_dir, 93 
lms_ filter, 256, 262
nr_dir_tojat, 98
nr_lat_filter, 98 
nr_lat_to_dir, 98 
part_frac_exp, 81-82 
pds, 194 
pz_plot, 81 
rand, 175
randn, 175
reconst, 63
resamp, 64
residue, 81
rls_filter, 262 
roots, 80-81 
row_vec, 7, 15 
sp_dct, 295 
sp_idct, 295 
unwrap, 51 
window, 123
zig-zag scan, 297
Fundamental frequency, in Fourier 
series, 27-29
Fundamental period, in Fourier series, 
27, 29
Fundamental terms, in Fourier series, 28
G
Gain curve, see Power gain
Gaussian distribution, of random signal 
amplitude, 172-175
white noise in, 183, 186, 192, 217 
Geometry
of adaptation, 241, 246
for expression simplification, 12-14

Index
327
Gradient, in adaptive signal processing, 
244, 252, 264 
'
Gray-scale image, of amplitude 
distributions, 175-176
Hanning window
for finite impulse response 
differentiator, 127-128
for finite impulse response filters, 
119-120
in spectral estimation, 190
Harmonic analysis, in Fourier series, 13, 
26, 28
Harmonic functions, in Fourier series, 
26-29
Help command, in Matlab, 15
Hertz scale, in spectral analysis, 48-49
Hibert transformer, as finite impulse 
response filters, 128-129
Highpass filters
with finite impulse response, 121-124 
with infinite impulse response, 135, 
147-149
in multirate signal decomposition, 
299-302, 304-305
Huffman coding, 283-287, 290
Ideal conditions, in adaptive signal 
processing, convergence under, 
246-249, 252, 255
Identity matrix, definition of, 10
HR filters, see Infinite impulse response 
(HR) filters
Image(s)
amplitude distribution of, 175-176
in Matlab language, 5
mirror, in multirate signal 
decomposition, 304-305
transform coding of, 291, 295-297 
Image array, in Matlab language, 5 
Impulse response, in linear systems, 
82-83, 100
Independent noise, effects on least­
squares system design, 228-230
Independent variable, in discrete linear 
systems, 71-72
Index array, in mean-squared error 
minimization, 205
Infinite impulse response (HR) filters, 
135-165
in adaptive signal processing, 243
allpass, 160, 162
bandpass, 135, 147-148, 153-155 
bandstop, 135, 147-148, 153-154 
bilinear transformation of, 149-153 
Butterworth, 137-141
Chebyshev, 141-146
comb, 159-160
digital, 137, 145
designing, 153-154
transformation to, 149-153
writing functions for, 153-157 
exercises for, 162-164, 235-236 
highpass, 135, 147-149 
introduction to, 73, 135 
least-squares system identification
and, 235-236
linear phase of, 136-137
lowpass, 135, 137-141, 144
conversion to other types, 147-149, 
153
other types of, 157-162
passband ripple in, 135-136, 140-142, 
148, 156-157
power gain of
with analog-to-digital conversion, 
151-156
Chebyshev vs. Butterworth, 
145-146, 152
type-specific, 137-140, 143-145, 
151-152, 159
spectrogram and, 160, 162
stopband ripple in, maximum gain in, 
151-153
Infinite impulse response transfer 
function, 135
linear phase of, 136-137 
Information, see Signal information 
Input signal, in linear systems, 72, 85,
136
Instantaneous power, of random 
signals, 177
Integration by parts, 14
Interference canceling; see also Noise 
in least-squares system design
broadband considerations for, 
228-230
example of, 220-223, 232
principles of, 199, 202-203
Interpolation (time-domain), of discrete 
Fourier transform, 53, 55, 63-64
Inverse, as algebraic operation, 10-11 
Inverse autocorrelation, in adaptive 
signal processing, 258-260
Inverse discrete cosine transform, 
292-293, 295

328 
Digital Signal Processing with Examples in Matlab
Inverse filtering, in least-squares system 
design, 223-224
Inverse Fourier transforms
continuous, 59, 113 
discrete, 51-52, 62
Inverse modeling, least-squares system 
design for, 201-202
Inverse transfer functions, signal 
compression with, 86, 224
Inverse transform, in multirate signal 
decomposition, 303-304
Inverse z-transform, linear systems and, 
83-84 
'
Iteration, in Huffman coding, 285
K
Kaiser window formula, for finite 
impulse response filters, 119-121, 
123-124
K weights, of lattice structure, 98-99
L
Laplace transfer function, in infinite 
impulse response filters, 138
Laplace transform, 59, 83
Lattice algorithms, for linear systems, 
91-99
Lattice filtering, in linear systems, 96,98 
Lattice stage, symmetric two-multiplier, 
91-92
Lattice structure(s) 
adaptive, 268 
k weights of, 98-99 
symmetric
nonrecursive, 96-98
recursive, 91-92
Lattice-to-direct conversion, in linear 
systems, 93-94 
nonrecursive, 97-98
Learning curve 
for recursive least squares algorithm, 
262-263
for weight convergence, in adaptive 
signal processing, 247-248, 255
Least-mean-square adaptive predictor, 
258
Least-mean-square (LMS) algorithm, for 
adaptive signal processing
examples of, 255-258 
performance measure for, 264-266 
principles of, 241, 249, 254-255 
recursive least squares algorithm vs.,
261-262 
'
Least squares, 19-37
applications of, 20, 24, 26-30
in digital signal processing systems, 
199-230 
'
exercises for, 33-36
introduction to, 19
principle of, 19-24, 199
Least-squares approximation, 20
discrete, 20, 22, 24
Fourier series as, 27-30
Least-squares coefficients, in Fourier 
series, 28-29
Least-squares coefficient vector, 24, 26
Least-squares equalizer, for unknown 
channels, 215-216
Least-squares equations
error identification in, 235-236
using correlation, 207
broadband noise effects on, 229-230
computation of, 212-214
system identification example, 
218-219
using covariance, 211
Least-squares polynomial, 20
Least-squares predictor, for mean- 
squared error minimization, 
207-208
Least-squares system design, 199-239 
applications of, 200-203 
correlation in, 205-207
computation of, 212-214
covariance in, 211-212
computation of, 212-214 
equalization in, 199, 201-202
of unknown channels, 215-216
example of, 207-210
exercises for, 230-237
with finite signal vectors, 210-212
interference canceling in
example of, 220-223, 232
principles of, 199, 202-203
introduction to, 19-24, 199
linear prediction and, 199-201
example of, 223-228
for mean-squared error 
minimization, 207-208 
mean-squared error minimization in, 
200, 203-207
illustration of, 207-210
modeling in, 199,201-202,217,234,237 
noise in, 199, 201-203
effects of independent broadband, 
228-230
improvement methods for, 220-223 
system identification, 217-220

Index
329
"Lena," as amplitude distribution 
image, 175-176
Levinson's algorithm, for mean-squared 
error minimization, 206
Linear differential equations, 71
Linear equations
discrete vs. continuous, 71-72
solving simultaneous, 8
transfer functions and, 75-77 
Linearity, of discrete Fourier transform, 
52-53
Linear least-squares approximation, 
20-22
Linear phase
of finite impulse response filters, 
111-112, 114
of infinite impulse response filters, 
136-137
Linear phase filter, 235
Linear phase shift
of discrete Fourier transform, 52, 
56-57
by finite impulse response filters, 
111-112, 114, 129
by infinite impulse response filters, 
111, 136-137, 157, 162
in multirate signal decomposition, 
305-306
principles of, 77, 79-80
Linear predictive coding, 201, 224, 281, 
298 
"
Linear predictor
compression with, 201, 224-225
encoding with, 201, 224 
in least-squares system design,
199-201
example of, 223-228
for mean-squared error 
minimization, 207-208
recovery and, 223-228
Linear systems, 71-109
algorithms for, 87-103
direct, 87-89, 91
fast Fourier transform, 99-103
lattice, 91-99
state-space, 89-91
convolution of, 74-75
discrete, 74-75
fast Fourier transform algorithms 
and, 99-103
periodic, 100-101
z-transform of, 75-76
discrete vs. continuous, 71
exercises for, 105-108
poles and zeros in, 77-81, 86
processing speed of, 98-99
properties of, 71-72
structural illustrations of, 84-87 
transient vs. stable, 81-84, 86, 112 
z-transform and, 75-77
inverse, 83-84
Linear transfer functions, 75-77 
exercises for, 105-108 
fractions in, 80-81, 84-85 
structure options for, 84-87
LMS algorithm, see Least-mean-square 
(LMS) algorithm
Log base 2, as information measure, 
278-279
Lossless signal compression, 282, 291 
with multirate decomposition, 300
Lossy signal compression, 282, 291
Lowpass filters
design of, 234-235
in finite impulse response
conversion to other types, 121-124 
convolution of, 116-119
effects on power gain, 117-124 
formulas for, 119-121
ideal characteristics of, 112-113
window functions for, 116-124
in infinite impulse response 
conversion to other tvpes, 147-149, 
153
ideal characteristics of, 135, 
137-141, 144
in multirate signal decomposition, 
299-300, 302, 304-305
M
Magnitude, of discrete Fourier 
transform, 44
Magnitude-squared coherence (MSC), 
191-193
Mathematical identities/operations; see 
also specific type
summary of, 13-14
Matlab
in algebra notation, 5-11
basic language of, 2-3
element sequences processed by, 3-5 
Fourier series implications for, 30-31 
functions in DSP, see Functions 
help command, 15 
introduction to, 2-3
utility of, 2
window formulas, for finite impulse 
response filters, 119-121
Matlab data files, 3, 14, 312

330 
Digital Signal Processing with Examples in Matlab
Matlab functions, see Functions 
Matrix(ces)
algebra review of, 5-11
in least squares, 22-24
in Matlab language, 3-5
Mean-squared error (MSE)
in adaptive signal processing 
algorithm for, 241-242 
gradient of, 244 
least-mean-square convergence 
with, 255-257
as performance measure, 263-267 
performance surface of, 242-244 
search algorithms for, 243-249 
vector form of, 244 
' '
of least-squares process 
applications of, 200, 203 
broadband noise effects on, 229-230 
equations for, 207
minimization algorithms, 205-206 
minimization example, 207-210 
system design via, 203-207
Mean-squared error gradient, as 
adaptive signal processing 
vector, 244
Mean-squared error plot, 208-209
Mean value, in amplitude distributions, 
170-171
of Gaussian function, 173
of uniform variate, 172
m-files, in Matlab language, 3, 14
Mirror image, in multirate signal 
decomposition, 304-305
Modeling, in least-squares system
design, 199,201-202,217,234,237 
MSC (magnitude-squared coherence), 
191-193
MSE, see Mean-squared error (MSE) 
Multiple-input adaptive systems, 268 
Multirate processors, for signal coding 
and compression, 302-303
Multirate signal decomposition, see 
Multirate signal processing
Multirate signal processing, for coding 
and compression
aliasing with, 300, 302-304 
decimation/down-sampling with, 
299-302,310-312 '
decomposition with, 305-308
discrete cosine transform vs., 300,306 
discrete Fourier transform in, 300-305 
as filtering process, 298-305 
forward, 303
Fourier transform in, 300, 306 
inverse, 303-304 
principles of, 291, 298-299 
recovery with, 299, 302-305 
subband coding with, 298-299, 
306-308
time-frequency resolution of, 310-311 
wavelets in, 298-299
N
Neural networks, in adaptive signal 
processing, 268
Newton's method, in adaptive signal 
processing, 245
Node(s), in Huffman coding, 285
Noise
adaptive system performance and, 
264-267
in cross-power spectrum, 192-193 
in least-squares system design, 199, 
201-203 
’
effects of independent broadband, 
228-230
improvement methods for, 220-223 
Noise cancellation, with least-squares 
design
example of, 220-223, 232 
principles of, 199, 202-203
Noncausality, of discrete linear systems, 
72 
’
Nonlinear modeling, in least-squares 
system, 237
Nonrectangular window, for lowpass 
finite impulse response filters, 116
Nonrecursive fast Fourier transform 
filtering, in linear systems, 
101-102
Nonrecursive lattice, symmetric, 96-98
Nonrecursive linear equations, 73, 
90-91, 101, 103; see also Finite 
impulse response (FIR) filters
Nonsingular matrix, in algebra notation, 
11
Nonstationary signal, 168
in adaptive signal processing, 243,256 
Normalized error function, in Gaussian 
amplitude distributions, 174
Normal probability function, in 
amplitude distributions, 172
o
Octave-band coding, 306-307, 310 
wavelet transforms as, 310-312
Octave bands, in multirate signal 
decomposition, 306-307, 311

Index
331
One, as bit state, 278 
coding of, see Coding
One-step algorithm, for adaptive signal 
processing, 245
One-step predictor, for mean-squared 
error minimization, 207-208 
in adaptive signal processing, 
255-258
Orthogonality, 19-37 
exercises for, 33-36 
in Fourier series, 28-30 
introduction to, 19 
principles of, 24-26 
in steepest-descent algorithm, 
250-251
Output signal, in linear systems, 72, 85, 
136
Overlapping segments, in power 
spectral estimation, 185-189
Parallel form, of transfer function, in 
linear systems, 84
Parallel structure, of linear systems, 
84-85
Parameters
convergence, in adaptive signal 
processing, 245-246
for infinite impulse response filters, 
140, 145, 149, 154
in linear systems
discrete, 73
for mean-squared error 
minimization, 203-204 
Parseval's theorem, 178
Partial fractions, in linear systems, 81,84 
Partitioning, in power spectral 
estimation, 185-186
Passband ripple
in finite impulse response filters, 
114-115, 119, 121
in infinite impulse response filters, 
135-136, 140-142, 148, 156-157 
Performance feedback, in adaptation 
process, 242
Performance surface
of mean-squared error
in adaptive signal processing, 
242-244
search algorithms for, 243-249
quadratic, in adaptive signal 
processing, 243, 247-248
Periodic convolution, of linear 
equations, 100-101
Periodic extension, in linear systems, 
100
Periodic function, in Fourier series, 27 
Periodicity, of discrete Fourier 
transform, 52
Periodogram
in cross-power spectrum, 191
power density and, 178-180
example of, 181-182
in power spectral estimation, 181-186 
Phase response (shift), in linear systems,
77, 79-80; see also Linear phase 
shift
Phase spectrum, of signal vector, 48-51 
Phi, in least-squares system design, 
205-206, 213
Pixel
coding and compression of, 278, 291, 
296-298
in Matlab language, 5
Plant noise, in least-squares system 
design, 228
Poles
in infinite impulse response filters, 
135, 147, 158-159
with analog-to-digital conversion, 
151-156 
'
Butterworth, 138-139, 141, 149, 152
Chebyshev, 142-145, 152
conjugate pairs of, 139, 153-154
in linear systems, 77-81, 86 
Polynomial ratios/division
in filter systems, 111, 135-136, 
141-142, 237
least-squares, 20
in linear systems, 99, 103
Power density spectrum
of random signals
algorithms for, 193-194
cross-power, 190-193
data windows of, 187-190
estimation of, 184-188 
introduction to, 167-168 
principles of, 178-181 
properties of, 181-183
white noise in, 183, 186, 192
Power gain
of digital filters, 103-104, 151
of finite impulse response filters, 112, 
114-115
window spectrum effects on, 
117-124
of infinite impulse response filters 
with analog-to-digital conversion, 
151-156

332 
Digital Signal Processing with Examples in Matlab
Chebyshev vs. Butterworth, 
145-146, 152
individual types, 137-140,143-145, 
151-152, 159
Power spectral estimation .
in adaptive signal processing, 261-262
principles of, 177-183
random signals and, 184-188
Power spectrum, see Power density 
spectrum
Prediction
in adaptive signal processing, 255-258
in least-squares system design, 
199-201
for mean-square error ■ • 
minimization, 207-208
recovery and, 223-228
linear, of recovery, 224-228
of seismic events, 224—225
Prediction error
in adaptive signal processing, 257
in least-squares system design, 226
in signal compression, 281-282
Predictive coding, 281-282, 292, 298
linear, 201, 224, 281, 298
Prefiltered signals
for aliasing prevention, 181
in waveform reconstruction, 64-65 
Prefix property, in Huffman coding, 
285-286
Prewhitening, in least-squares system 
design, 227-228
Probability, in amplitude distributions, 
169
Probability density function, of
amplitude distributions, 169-171
Gaussian form of, 172-175 
Probability function
continuous, in uniform random 
signals, 172
normal, in amplitude distributions, 
172
Processing speed, of linear systems, 
98-99
Q
Quadratic error surface, in mean- 
squared error minimization, 210
Quadratic performance surface, in 
adaptive signal processing, 243, 
247-248
Quadrature filter, 237
Quadrature mirror filters, 305 
Quantization
with multirate signal decomposition, 
299-300
vector, as coding, 282
Quantizing errors, in sampling theorem, 
61 
’
Radian-per-second scale, in spectral 
analysis, 48-49
Random search algorithm, for adaptive 
signal processing, 243
Random signals, 167-198 
amplitude distribution of 
definition of, 167-168 
principles of, 168-171 
exercises for, 194-197
Gaussian distribution of, 172-175 
introduction to, 167-168 
other distributions of, 175-177 
power density spectra of 
algorithms for, 193-194 
cross-power, 190-193 
data windows of, 187-190 
definition of, 167-168, 178 
estimation of, 184-188 
principles of, 178-181 
properties of, 181-183
power of, 177-178 
stationarity of, 167-169 
statistical properties of, 167, 
193-194
uniform distribution of, 171-172 
Realizability
of discrete linear systems, 72, 85-86 
of finite impulse response filters, 
113-116
Real time
adaptive signal processing in, 241,243 
digital filters and phase shift in, 136
Reconstruction error, in least-squares 
system design, 226
Recovery
with compression
in adaptive signal processing, 258 
in multirate signal decomposition, 
299, 302-305
linear prediction of, 224-225 
in least-squares system design, 
223-228
Rectangular window 
for lowpass filters 
finite impulse response, 116, 118, 
121

Index
333
infinite impulse response, 140, 
145-146
in spectral estimation, 187-188, 190
Recursive adaptive filters, in signal 
processing, 268
Recursive estimate, in adaptive signal 
processing, 259
Recursive lattice, symmetric, 91-92
Recursive least squares (RLS) algorithm, 
for adaptive signal processing, 
258-263
convergence with, 261 -263 
correlation estimates with, 258-260 
forms of, 258, 261
learning curve for, 262-263 
least-mean-square algorithm vs., 
261-262
Recursive linear equations, 72,102-103; 
see also Infinite impulse response 
(HR) filters 
'
Redundancy
of discrete Fourier transform, 43-45, 
52, 62
removal of, 277-278
Resampling, of discrete Fourier 
transform, 52, 68
Residue Theorem, 83
Reversal
of signal vectors, in digital filters, 
136-137 
'
time series, in infinite impulse 
response filters, 137
RLS algorithm, see Recursive least 
squares (RLS) algorithm
Row vector, in algebra notation, 7, 15
s
Sampled waveform(s), in Matlab 
language, 4-5
Sample vector, in least squares, 20
Sampling frequency, in spectral 
analysis, 50
Sampling interval, in Matlab language, 
4
Sampling Theorem 
exercises for, 66-69
Fourier transforms as proof of, 39, 54, 
56, 61-62
simply stated, 60-61
in waveform reconstruction, 62-64
Scanning, in image compression, 
297-298
Second harmonic terms, in Fourier 
series, 28
Segments
in multirate signal decomposition, 
307-308
in power spectral estimation, 185 
overlapping, 185-189
Seismic event(s)
adaptive signal processing of, 241
prediction and recovery of, 224-225
Short-time discrete transform, of signal 
information, 308-310
Signal component, in discrete Fourier 
transform, 56-57
Signal decomposition, with multirate 
signal processing, 305-308
Signal energy, 178,180
Signal information, 277-318
coding, 282-308; see also Coding
compressing, 280-308; see also
Compression
exercises for, 312-317
introduction to, 277-278
measuring, 278-280
time-frequency analysis of, 308-312
wavelet transforms of, 310-312
Signal power, see Power entries
Signal processing
adaptive, see Adaptive signal 
processing
digital, see Digital signal processing 
(DSP)
multirate, see Multirate signal 
processing
speed of
in adaptive systems, 263-264
in linear systems, 98-99
Signals
in linear systems, 72, 100
in Matlab language, 4-5
reversal of, in digital filters, 136-137
Signal space
in Matlab language, 4
orthogonal vectors in, 25
Signal-to-noise ratio (SNR)
in cross-power spectrum, 192-193
in least-squares system design, 
201-203
effects of broadband, 228-230
improvement methods for, 220, 
222-223
Signal vectors, see Vector(s)
Sine(s)
basic applications of, 10, 13-14
in Fourier series, 26, 28-29, 39
in least squares, 23
SNR, see Signal-to-noise ratio (SNR)

334 
Digital Signal Processing with Examples in Matlab
Spatial domain, spectrum mapping of, 39
Spectral analysis
discrete Fourier transform in, 39,
43-44, 48-51
exercises for, 66-69
frequency principles in, 49-50
transform coding of, 295-298
white noise in, 183, 186, 192
Spectral estimation, 167-198
in adaptive signal processing, 261-262
algorithms for, 193-194
cross-power, 190-193
data windows in, 187-190
exercises for, 194-197
introduction to, 167-168' '
power principles of, 177-183, 261-262
techniques for, 184-188
Spectrogram
of infinite impulse response, 160-161
of signal information, 308-309
Spectrum, in signal processing, 39
Speech signal
compression of, 291, 295
in interference canceling, 221-222
multirate signal decomposition of, 
307-308
Speed, of processing
in adaptive systems, 263-264
in linear systems, 98-99
s-plane, in infinite impulse response 
filters, 138-139
transformation to z-plane, 149-150
Stability, of linear systems, 81-83, 86
discrete, 73
infinite impulse response filters and, 
136, 139
symmetric, 94
Standard deviation, in amplitude 
distributions, 170
of uniform variate, 172
Startup sequence, for discrete 
convolution, 74-75
Startup time, of finite impulse response 
filters, 112
State-space algorithms, for linear
systems, 89-91
State-space equations, for discrete linear
system, 89-91
State-space form, of linear systems, 
89-91
State variable, of linear systems, 89
Stationarity, of signals, 167-169
Stationary function, instantaneous
power of, 177-178
Stationary signal, 167-169
mean-squared error in, 204-205 
total squared error in, 211
Statistical properties, of random signals, 
167, 193-194
Steepest-descent algorithm, for adaptive 
signal processing, 249-254 
convergence criterion for, 249, 
251-254
eigenvalues of, 250-253, 264, 266
goals of, 249-250, 258
Stopband ripple 
in finite impulse response filters, 115, 
119, 121 '
in infinite impulse response filters, 
135, 140-142, 145-146
maximum gain in, 151-153
String coding, 282
Subband coding, with multirate signal 
decomposition, 298-299, 
306-308
Superscripts, in multirate signal 
decomposition, 307
Symbol(s)
as coding term, 278
in entropy coding, 279-281, 283
Symbol code, 278; see also Coding
Symbol entropy, 279
Symmetric nonrecursive lattice, 96-98
Symmetric recursive lattice, 91-92
Symmetric two-multiplier lattice stage, 
91-92 
' 
"
System identification, least-squares 
system design for, 201
T
Tapered rectangular window, in spectral 
estimation, 190
Time constant, in adaptive signal 
processing
for forgetting correlation estimate, 260 
for mean-squared error, 247-248 
as system performance measure, 
263-264, 266-267
for weight convergence, 246-247,252, 
255
Time domain
of discrete Fourier transform 
interpolation of, 53, 55, 63-64 
zero extension in, 52-54
in discrete linear systems, 71, 73-75, 
85
signal power density and, 178, 180 
in signal processing, 39, 312
Download more eBooks here: http://avaxhm.com/blogs/ChrisRedfield

Electrical Engineering
Copyrighted Material
Digital Signal Processing
with Examples in Matlab'
Samuel D. Stearns
In a field as rapidly expanding as digital signal processing, even the basic 
topics change over time both in nature and relative importance. It is 
important, therefore, to have an up-to-date text that not only covers the 
fundamentals, but also follows a logical development that leaves no gaps 
that readers must somehow bridge by themselves.
Digital Signal Processing with Examples in Matlab* is just such a 
text. The presentation does not focus on DSP in isolation, but relates it 
to continuous signal processing and treats digital signals as samples of 
physical phenomena. The author introduces important topics not usually 
addressed in signal processing texts, including the discrete cosine transform, 
multirate signal processing, signal coding and compression, least squares 
systems design, and adaptive signal processing. He also uses the industry­
standard software Matlab to provide DSP functions and examples.
Designed for a one-semester upper-level course but also ideal for self­
study and reference, Digital Signal Processing with Examples in Matlab* 
is complete, self-contained, and rigorous. For basic DSP, it is quite simply 
the only book you need.
FEATURES
• 
Offers an outstanding balance of DSP theory, techniques, and applications
• 
Provides a concise Matlab tutorial and describes dozens of useful Matlab 
functions
• 
Describes digital transforms and algorithms
• 
Covers digital filter analysis and design and least-squares system design
• 
Introduces statistical signal processing and spectral analysis
• 
Explores adaptive signal processing systems
• 
Furnishes all of the examples and functions 
used in the text, as well as exercise
1011
solutions, online at www.crcpress.com.
CRC PRESS
www.crcpress.com
CopyrighteHINaterial


UNITEXT 126 
 A Compact 
Course on 
 Linear PDEs
Alberto Valli

UNITEXT – La Matematica per il 3+2
Volume 126
Editor-in-Chief
Alﬁo Quarteroni, Politecnico di Milano, Milan, Italy; École Polytechnique
Fédérale de Lausanne (EPFL), Lausanne, Switzerland
Series Editors
Luigi Ambrosio, Scuola Normale Superiore, Pisa, Italy
Paolo Biscari, Politecnico di Milano, Milan, Italy
Ciro Ciliberto, Università di Roma “Tor Vergata”, Rome, Italy
Camillo De Lellis, Institute for Advanced Study, Princeton, NJ, USA
Massimiliano Gubinelli, Hausdorff Center for Mathematics, Rheinische Friedrich-
Wilhelms-Universität, Bonn, Germany
Victor Panaretos, Institute of Mathematics, EPFL, Lausanne, Switzerland

The UNITEXT – La Matematica per il 3+2 series is designed for undergraduate
and graduate academic courses, and also includes advanced textbooks at a research
level.
Originally released in Italian, the series now publishes textbooks in English
addressed to students in mathematics worldwide.
Some of the most successful books in the series have evolved through several
editions, adapting to the evolution of teaching curricula.
Submissions must include at least 3 sample chapters, a table of contents, and a
preface outlining the aims and scope of the book, how the book ﬁts in with the
current literature, and which courses the book is suitable for.
For any further information, please contact the Editor at Springer:
francesca.bonadei@springer.com
THE SERIES IS INDEXED IN SCOPUS
More information about this subseries at http://www.springer.com/series/5418

Alberto Valli
A Compact Course on Linear
PDEs

Alberto Valli
Department of Mathematics
University of Trento
Trento, Italy
ISSN 2038-5714
ISSN 2532-3318
(electronic)
UNITEXT
ISSN 2038-5722
ISSN 2038-5757
(electronic)
La Matematica per il 3+2
ISBN 978-3-030-58204-3
ISBN 978-3-030-58205-0
(eBook)
https://doi.org/10.1007/978-3-030-58205-0
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland
AG 2020
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
Cover illustration: Daniele Salvalai “Alveare-Omaggio a La Ruche de Montparnasse”. Reproduced with
permission. ©Arte Sella 2009. Photo by Giacomo Bianchi.
This Springer imprint is published by the registered company Springer Nature Switzerland AG.
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To Jarno and Beatrice, the future

Preface
It don’t mean a thing
(if it ain’t got that swing)1
Edward “Duke” Ellington
This book stems from a 45-h course that I delivered for the Master degree at the
Department of Mathematics of the University of Trento.
Partial differential equations (PDEs) are an extremely wide topic, and it is not
possible to include them in a single course, no matter how many lessons are assigned
to it. Thus, the ﬁrst question I had to face was the viewpoint I wanted to adopt and
choice of the arguments.
I decided to focus on linear equations. It is well known to everyone that
the mathematical description of natural phenomena is mainly based on nonlinear
models; however, in many cases, a reasonable approximation is obtained by a linear
formulation, and, moreover, the knowledge of linear problems is the ﬁrst step for
dealing with more complex nonlinear cases.
The second choice I made is to limit the presentation to the so-called weak
formulation of partial differential equations. This means that our point of view is the
following: solving a linear partial differential equation is interpreted as the solution
of a problem associated with a linear operator acting between suitable inﬁnite-
dimensional vector spaces.
The path for arriving at this abstract formulation needs some tools that were not
available in the classical theory. In a nutshell, the four main missing ingredients are
the following:
•
weak derivatives,
•
weak solutions,
•
Sobolev spaces, and
•
a bit of functional analysis.
1If you are curious take a look on the web: you can ﬁnd nice videos on YouTube with this title.
vii

viii
Preface
The ﬁrst results in this direction date back to the thirties of the last century,
with the pioneering works of Jean Leray [13],2 Sergei L. Sobolev [19],3 and
others. In the same period, the study of inﬁnite dimensional vector spaces and of
functional analysis attracted the attention of many researchers: let us only mention
the milestone book by Stefan Banach [2].
Still speaking about concepts not present in the classical theory, I decided not to
introduce the distributions and the distributional derivatives, as they are not essential
for the presentation. In fact, as it is well known, the distributional derivative of a
function essentially coincides with its weak derivative, and dealing with spaces of
functions permits to avoid further generalizations.
The determination of the weak formulation is essentially performed by trans-
forming the original problem into a set of inﬁnitely many integral equations, one
for each “test” function belonging to a suitable vector space. At several points of
the book, I have tried to motivate the various steps of this approach starting from
the analysis of ﬁnite dimensional linear systems, then enlightening analogies and
differences when passing to the inﬁnite dimensional case. In particular, Chap. 3
is devoted to the results of functional analysis that show some typical differences
between a ﬁnite dimensional and an inﬁnite dimensional vector space. Another
section of that chapter aims to clarify that suitable spaces for the new approach are
those endowed with a scalar product, more precisely those for which the orthogonal
projection on a closed subspace is well deﬁned: in other words, this means Hilbert
spaces.
This recurring comparison between algebraic linear systems and weak formula-
tions of linear PDEs has the aim of making clear that for the latter subject functional
analysis plays the role of linear algebra, namely, it is a basic tool for its study;
however, as it has been observed, this does not mean that it is a good idea to
transform the whole topic into a too abstract branch of functional analysis itself.
When I started to teach the course, I suggested a couple of books to the students:
those by Evans [6] and Salsa [18]. For this reason, I cannot hide that the structure
of these books has inﬂuenced what I presented then to my students and what is
included now in this book. However, I hope that the reader can ﬁnd here at least a
different ﬂavor (together with some new topics).
The book is organized as follows. Chapter 1 is a very brief introduction to the
subject, in which some deﬁnitions are given and a list of examples are presented.
In Chap. 2, many important items already appear: second-order elliptic equations
and related boundary value problems, weak solutions, and ﬁnally also the Lax–
Milgram theorem. However, the functional analysis framework is not made clear,
and for that, the reader is referred to the following results included in Chaps. 3 and 4.
2It seems that Leray has been the ﬁrst one to speak about weak solutions (“solutions turbulentes”)
and weak derivatives (“quasi-dérivées”).
3The functional framework where we describe and analyze the problems is given by Sobolev
spaces, a name on which there is agreement since the middle of the last century.

Preface
ix
Chapter 3 is devoted to analogies and differences between ﬁnite dimensional
and inﬁnite dimensional vector spaces and to the motivation that makes useful the
introduction of Hilbert spaces.
In Chap. 4, some core topics are introduced and analyzed: weak derivatives and
Sobolev spaces.
Chapter 5 is a central part of the book: a systematic presentation of weak formu-
lations of elliptic boundary value problems is included. Moreover, the properties of
the bilinear forms that describe the problems are presented in full detail.
Chapter 6 is devoted to several technical results that have been used in the
previous chapters: approximation in Sobolev spaces, Poincaré and trace inequalities,
Rellich compactness theorem, and du Bois–Reymond lemma.
In Chap. 7, a rich variety of additional results is presented: Fredholm alternative,
spectral theory for elliptic operators, maximum principle, regularity results and
Sobolev embedding theorems, and ﬁnally Galerkin numerical approximation.
Chapter 8 deals with constrained minimization and Lagrange multipliers in the
inﬁnite dimensional case. A general theory for saddle point problems is presented,
and two speciﬁc examples are described: second-order elliptic equations rewritten
as a ﬁrst-order system of two equations and the Stokes problem. The Galerkin
approximation of saddle point problems is also described and analyzed.
Chapter 9 is focused on parabolic problems, starting from the abstract evolution
theory in Hilbert spaces and then arriving to its application to speciﬁc problems.
The proof of maximum principle is also included.
A similar presentation is given in Chap. 10 for hyperbolic problems, ending with
the proof of the property of ﬁnite propagation speed.
The book ﬁnishes with some appendices, devoted to the technical results: a
detailed construction of a partition of unity, the precise deﬁnition of the regularity
of the boundary of a domain, integration by parts formulas, the Reynolds transport
theorem, the Gronwall lemma, and a general well-posedness theorem for weak
problems.
Each chapter of the book is complemented by some exercises: they have different
difﬁculty, and in some case, they could be more properly intended as an additional
in-depth analysis. For the ease of the reader, I decided to present the complete
solution of all of them.
At the end, a few words about the sentence by “Duke” Ellington that I chose as
an incipit: a book is not a course, even though the title seems to suggest it. Thus, for
the delight of the students, a colleague who will decide to follow this presentation
should ﬁnd the way to add some swing to these barren pages: I tried my best, but it
is never enough.
This book would not have been written without my former Master students Fede-
rico Bertacco and Laura Galvagni, who a day (but after the exam!) entered my ofﬁce
with the Latex ﬁle of my unreﬁned handwritten notes. This has been the irresistible
push for rearranging everything into a better structured textbook. I am also grateful
to Gabriele Dalla Torre, who suggested the best way for drawing the ﬁgures.

x
Preface
Finally, I want to thank the Editors Luigi Ambrosio, Paolo Biscari, Ciro Ciliberto,
Camillo De Lellis, and Victor Panaretos and the Editor-in-Chief Alﬁo Quarteroni
for having accepted to publish this book in the Springer Series “UNITEXT: La
Matematica per il 3 + 2.” Special thanks to Francesca Bonadei from Springer, who
encouraged me to undertake this project and with great experience and enthusiasm
has followed me along its realization.
Trento, Italy
Alberto Valli
June 2020

Contents
1
Introduction .................................................................
1
1.1
Examples of Linear Equations.......................................
2
1.2
Examples of Non-linear Equations ..................................
3
1.3
Examples of Systems.................................................
3
1.4
Exercises ..............................................................
4
2
Second Order Linear Elliptic Equations.................................
9
2.1
Elliptic Equations ....................................................
9
2.2
Weak Solutions .......................................................
11
2.2.1
Two Classical Approaches .................................
12
2.2.2
The Weak Approach ........................................
16
2.3
Lax–Milgram Theorem ..............................................
21
2.4
Exercises ..............................................................
23
3
A Bit of Functional Analysis...............................................
31
3.1
Why Is Life in an Inﬁnite Dimensional Normed Vector
Space V Harder Than in a Finite Dimensional One? ...............
31
3.2
Why Is Life in a Hilbert Space Better Than
in a Pre-Hilbertian Space? ...........................................
34
3.3
Exercises ..............................................................
37
4
Weak Derivatives and Sobolev Spaces....................................
39
4.1
Weak Derivatives .....................................................
39
4.2
Sobolev Spaces .......................................................
44
4.3
Exercises ..............................................................
50
5
Weak Formulation of Elliptic PDEs ......................................
53
5.1
Weak Formulation of Boundary Value Problems ...................
53
5.2
Boundedness of the Bilinear Form B(·, ·) and the Linear
Functional F(·) .......................................................
59
5.3
Weak Coerciveness of the Bilinear Form B(·, ·)....................
60
5.4
Coerciveness of the Bilinear Form B(·, ·)...........................
64
xi

xii
Contents
5.5
Interpretation of the Weak Problems ................................
68
5.6
Exercises ..............................................................
72
6
Technical Results ...........................................................
83
6.1
Approximation Results...............................................
83
6.2
Poincaré Inequality in H 1
0 (D) .......................................
87
6.3
Trace Inequality ......................................................
89
6.4
Compactness and Rellich Theorem..................................
94
6.5
Other Poincaré Inequalities ..........................................
96
6.6
du Bois-Reymond Lemma ...........................................
99
6.7
∇f = 0 Implies f = const ..........................................
99
6.8
Exercises .............................................................. 100
7
Additional Results .......................................................... 109
7.1
Fredholm Alternative................................................. 109
7.2
Spectral Theory....................................................... 115
7.3
Maximum Principle .................................................. 120
7.4
Regularity Issues and Sobolev Embedding Theorems.............. 125
7.4.1
Regularity Issues............................................ 125
7.4.2
Sobolev Embedding Theorems ............................ 131
7.5
Galerkin Numerical Approximation................................. 135
7.6
Exercises .............................................................. 137
8
Saddle Points Problems .................................................... 151
8.1
Constrained Minimization ........................................... 151
8.1.1
The Finite Dimensional Case .............................. 152
8.1.2
The Inﬁnite Dimensional Case............................. 157
8.2
Galerkin Numerical Approximation................................. 168
8.2.1
Error Estimates ............................................. 169
8.2.2
Finite Element Approximation............................. 172
8.3
Exercises .............................................................. 173
9
Parabolic PDEs ............................................................. 177
9.1
Variational Theory.................................................... 177
9.2
Abstract Problem ..................................................... 180
9.2.1
Application to Parabolic PDEs............................. 186
9.3
Maximum Principle for Parabolic Problems ........................ 188
9.4
Exercises .............................................................. 190
10
Hyperbolic PDEs ........................................................... 195
10.1
Abstract Problem ..................................................... 195
10.1.1
Application to Hyperbolic PDEs........................... 205
10.2
Finite Propagation Speed ............................................ 207
10.3
Exercises .............................................................. 208
A
Partition of Unity ........................................................... 213
B
Lipschitz Continuous Domains and Smooth Domains ................. 215

Contents
xiii
C
Integration by Parts for Smooth Functions and Vector Fields ........ 217
D
Reynolds Transport Theorem ............................................. 221
E
Gronwall Lemma ........................................................... 225
F
Necessary and Sufﬁcient Conditions for the Well-Posedness of
the Variational Problem ................................................... 229
References......................................................................... 231
Index ............................................................................... 233

Chapter 1
Introduction
Very often the description that we give of natural phenomena is based on physical
laws that express the conservation of some quantity (mass, momentum, energy,
...). In addition, some experimental relations are also taken into account (how the
pressure is related to the density, how the heat ﬂux is related to the variation of
temperature, ...).
Conservation and variation are thus basic ingredients: in mathematical words, the
latter one means derivatives. More precisely, very often the description we want to
devise involves many variables: therefore we have to play with partial derivatives
and with equations involving unknown quantities and their partial derivatives.
Deﬁnition 1.1 A partial differential equation (PDE) is an equation involving an
unknown function u = u(x) of two or more variables x = (x1, . . . , xn), n ≥2, and
certain of its partial derivatives. An expression of the form
F(x1, . . . , xn, u, Du, D2u, . . . , Dku) = 0
is called a kth order PDE, where k ≥1 is an integer and we have denoted by Dku a
generic partial derivative of order k.
Equivalently, keeping on the left all the terms involving the unknown u and putting
on the right all the other terms, we can write a PDE in the form
L(x, u) = f ,
where L is called partial differential operator and f turns out to be a given datum.
Deﬁnition 1.2 A PDE is said to be non-linear if it is not linear.
The reason of this apparently meaningless deﬁnition is that we want to enlighten
the fact that the crucial point is to understand the deﬁnition of what is a linear PDE.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_1
1

2
1
Introduction
Deﬁnition 1.3 A PDE in the form L(x, u) = f is said to be linear if the operator
L is linear, i.e., L(x, α1w1 + α2w2) = α1L(x, w1) + α2L(x, w2) for all α1, α2 ∈R
and all functions w1, w2.
This deﬁnition is a little bit inaccurate, as the operator L has not a meaning for
all functions w: it is necessary that the derivatives appearing in L do exist for these
functions.
Deﬁnition 1.4 Let the operator L be linear; then the linear equation Lu = f ̸= 0
is said to be non-homogeneous, while the linear equation Lu = 0 it is said to be
homogeneous.
We use the notation Diu for indicating the partial derivative ∂u
∂xi . Other equivalent
notations are uxi, Dxiu, ∂xiu.
Remark 1.1 The general form of a linear operator of ﬁrst order (k = 1) is:
L(x, w) =
n

i=1
ˆbi(x)Diw + a0(x)w .
The general form of a linear operator of second order (k = 2) is:
L(x, w) =
n

i,j=1
ˆaij(x)DiDjw +
n

i=1
ˆbi(x)Diw + a0(x)w .
We will see in the sequel that very often a second order linear operator will be
written in the variational form
L(x, w) = −
n

i,j=1
Di(aij(x)Djw) +
n

i=1
bi(x)Diw + a0(x)w .
Clearly, for smooth coefﬁcients aij it is easy to return to the previous form.
1.1
Examples of Linear Equations
Transport equation:
ut + b · ∇u = f , where ∇= (D1, . . . , Dn).
Laplace equation / Poisson equation:
−u = 0 / −u = f , where  =
n
i=1 D2
i is the Laplace operator. A solution u of the Laplace equation is called
harmonic function.
Helmholtz equation:
−u −ω2u = 0, with ω ̸= 0.
Heat equation:
ut −ku = f , with k > 0 (thermal conductivity). A solution u
has an inﬁnite speed of propagation.

1.3
Examples of Systems
3
Schrödinger equation:
−i ¯hut −
¯h
2mu + V u = 0, with ¯h > 0 (reduced Planck
constant), m > 0 (mass).
Wave equation:
utt −c2u = f , with c > 0 (speed of propagation). A solution
u has the ﬁnite speed of propagation c.
Damped wave equation:
utt −c2u + σut = f , with c > 0, σ > 0.
Klein–Gordon equation:
utt −c2u + m2c4
¯h2 u = 0, with c > 0, ¯h > 0, m > 0.
Telegraph equation:
utt −τ 2uxx + d1ut + d2u = 0, with τ > 0, d1 > 0,
d2 > 0 (the three constants being related to resistance, inductance, capacitance,
conductance).
Plate equation:
utt + 2u = f .
1.2
Examples of Non-linear Equations
Burgers equation:
ut + uux = εuxx (viscous: ε > 0; inviscid: ε = 0).
Korteweg–de Vries equation:
ut + cuux + uxxx = 0, with c ̸= 0.
Cahn–Hilliard equation:
ut + ν2u −(βu3 −αu) = 0, with ν > 0, α > 0,
β > 0.
Minimal surface equation:
div

∇u
√
1+|∇u|2

= 0, where divw = ∇· w =
n
i=1 Diwi.
Monge–Ampere equation:
det(Hu) = f (x, u, ∇u), where H is the Hessian
matrix of second order derivatives.
1.3
Examples of Systems
Elasticity system:
−μu −ν∇divu = f , where μ > 0, ν
> 0 (Lamé
coefﬁcients).
Incompressible Navier–Stokes/Euler system:

∂u
∂t + (u · ∇)u −νu + ∇p = f
div u = 0
(incompressibility condition) ,
where (u · ∇)u is the vector with components [(u · ∇)u]i = n
j=1 ujDjui, and
ν > 0 (viscosity per unit density) for Navier–Stokes, ν = 0 for Euler.
Compressible Navier–Stokes/Euler system (barotropic case):
⎧
⎪⎪⎨
⎪⎪⎩
∂ρ
∂t + div(ρu) = 0
ρ
 ∂u
∂t + (u · ∇)u

−μu −(ζ + n−2
n μ)∇div u + ∇P = ρf
P = p∗(ρ)
(barotropic condition) ,

4
1
Introduction
where μ > 0 (kinematic viscosity) and ζ > 0 (bulk viscosity) for Navier–Stokes,
μ = 0 and ζ = 0 for Euler.
Maxwell system:
⎧
⎪⎪⎨
⎪⎪⎩
∂tB + curlE = 0
,
divB = 0
∂tD −curlH = −Je ,
divD = 0
B = μH
D = ϵE ,
where μ > 0 (magnetic permeability), ϵ > 0 (electric permittivity),
curlE = ∇× E = det
⎡
⎣
i
j
k
D1 D2 D3
E1 E2 E3
⎤
⎦.
Eddy current system:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂tB + curlE = 0 ,
divB = 0
curlH = J
,
χIdivD = 0
B = μH
D = ϵE
J = χCσE + Je ,
where σ > 0 (electric conductivity), χI and χC are the characteristic functions
of QI and QC, respectively, and QI and QC are two subsets which furnish a
splitting of the whole domain. This is an approximation of Maxwell system for
slow varying electromagnetic ﬁelds.
1.4
Exercises
Exercise 1.1 Write the Poisson equation −u = f as a ﬁrst order system in terms
of u and q = −∇u.
Solution Since u = div∇u, we have
 q + ∇u = 0
div q = f .
Exercise 1.2
(i) Determine the second order system that is obtained for the electric ﬁeld E by
applying the backward Euler scheme to the Maxwell system (assume that μ
and ϵ are constants).

1.4
Exercises
5
(ii) Determine the second order system that is obtained for the magnetic ﬁeld H
by applying the backward Euler scheme to the Maxwell system (assume that
μ and ϵ are constants).
(iii) Note that the two systems have the same structure curl curl + αI, with α > 0.
Solution
(i) Approximating the time derivatives by the difference quotients
∂tB ≈B −Bold
t
, ∂tD ≈D −Dold
t
and remembering that B = μH and D = ϵE we ﬁnd

μH + t curlE = Bold
ϵE −t curlH = −t J + Dold .
(1.1)
Applying the curl operator to the ﬁrst equation and using the second equation
for expressing curlH we easily ﬁnd
curl curlE +
μϵ
(t)2 E = 1
t curlBold +
μ
(t)2 Dold −μ
t J .
(ii) Applying the curl operator to the second equation in (1.1) and using the ﬁrst
equation in (1.1) for expressing curlE we have
curl curlH +
μϵ
(t)2 H = −1
t curlDold +
ϵ
(t)2 Bold + curlJ .
(iii) Evident from (i) and (ii).
Exercise 1.3 Let u be a smooth solution in R3 of the equation u −∇divu = f .
(i) Show that divu is a solution in R3 of the equation p −p = divf .
(ii) If curlf = 0 in R3, show that u = ∇ψ for a suitable function ψ.
(iii) If curlf = 0 in R3, divf = 0 in R3 and the derivatives of u decay fast enough
at inﬁnity, say, |divu| + |∇divu| ≤C∗|x|−α for α > 3
2 and |x| ≥q∗large
enough, then u = ∇ψ for a suitable harmonic function ψ.
Solution
(i) Taking into account that div∇div = div, the result follows at once by
applying the div operator to the equation.
(ii) Taking into account that curl ∇= 0, applying the curl operator to the equation
we ﬁnd curlu = curlf = 0. Since R3 is a simply-connected domain, we deduce
that there exists a function ψ such that u = ∇ψ in R3.

6
1
Introduction
(iii) Multiply the equation divu −divu = divf = 0 by divu and integrate over
the ball Bs = {x ∈R3 | |x| < s}, s > q∗. It holds
0 =

Bs
[(divu)2 −(divu)divu]dx
=

Bs
[(divu)2 + ∇divu · ∇divu]dx −

∂Bs
∇divu · n divu dSx ,
(1.2)
where we have used the integration by parts formula (C.5). The boundary
integral can be estimated as follows


∂Bs
∇divu · n divu dSx
 ≤C∗s−2α4πs2 ,
and moreover

Bs(divu)2dx =

Bq∗
(divu)2dx



= C0
+

Bs\Bq∗
(divu)2dx
≤C0 + C∗

Bs\Bq∗
|x|−2αdx = C0 + 4πC∗
 s
q∗
r2r−2αdr ≤Q0 ,
where Q0 is independent of s > q∗, as α > 3
2. Similarly,

Bs
|∇divu|2dx ≤Q1 .
Passing to the limit as s →+∞in (1.2) we ﬁnd

R3[(divu)2 + |∇divu|2]dx = 0 ,
therefore divu = 0 in R3. Since from (ii) we already know that u = ∇ψ, it
follows that div∇ψ = ψ = 0 in R3.
Exercise 1.4 Let u be a smooth solution in R3 of the equation u + curl curlu = f .
(i) Show that curlu is a solution in R3 of the equation q + curl curlq = curlf .
(ii) If divf = 0 in R3, show that u = curl for a suitable function .
(iii) If curlf = 0 in R3, divf = 0 in R3 and the derivatives of u decay fast enough
at inﬁnity, say, |curlu| + |curlcurlu| ≤C∗|x|−α for α > 3
2 and |x| ≥q∗large
enough, then u = curl for a suitable function  that satisﬁes curl curl = 0.

1.4
Exercises
7
Solution
(i) This a sort of “curl” version of the previous exercise. The ﬁrst result follows at
once by applying the curl operator to the equation.
(ii) Taking into account that div curl = 0, applying the div operator to the equation
we ﬁnd divu = divf = 0. It is well-known that this condition R3 is equivalent
to the fact that there exists a function  such that u = curl in R3.
Note that, if we know that the vector potential  decays sufﬁciently fast at
inﬁnity, we can apply the classical Helmholtz decomposition and write  =
∇φ +curlQ. Thus ⋆=  −∇φ satisﬁes curl⋆= u and div⋆= 0: in other
words, we have found a divergence free vector potential ⋆.
(iii) Take the scalar product of the equation curlu + curl curl curlu = curl f = 0 by
curlu and integrate over the ball Bs = {x ∈R3 | |x| < s}, s > q∗. It holds
0 =

Bs
[|curlu|2 + curlcurl curlu · curlu]dx
=

Bs
[|curlu|2 + curlcurlu · curl curlu]dx
−

∂Bs
n × curl curlu · curlu dSx ,
(1.3)
where we have used the integration by parts formula (C.8). The boundary
integral can be estimated as follows


∂Bs
n × curl curlu · curlu dSx
 ≤C∗|s|−2α4πs2 ,
and moreover

Bs |curlu|2dx =

Bq∗
|curlu|2dx



= C0
+

Bs\Bq∗
|curlu|2dx
≤C0 + C∗

Bs\Bq∗
|x|−2αdx = C0 + 4πC∗
 s
q∗
r2r−2αdr ≤Q0 ,
where Q0 is independent of s > q∗, as α > 3
2. Similarly,

Bs
|curl curlu|2dx ≤Q1 .
Passing to the limit as s →+∞in (1.3) we ﬁnd

R3(|curlu|2 + |curlcurlu|2)dx = 0 ,

8
1
Introduction
therefore curlu = 0 in R3. Since from (ii) we already know that u = curl, it
follows that curl curl = 0 in R3.
As in case (ii), if we know that the vector potential  decays sufﬁciently fast
at inﬁnity, we can modify it and ﬁnd a vector potential ⋆such that curl⋆= u
and div⋆= 0. Thus
0 = curl curl⋆= −⋆+ ∇div⋆= −⋆;
in other words, all the components of ⋆are harmonic functions.

Chapter 2
Second Order Linear Elliptic Equations
This chapter is concerned with a general presentation of second order linear elliptic
equations and of some of the most popular boundary value problems associated to
them (Dirichlet, Neumann, mixed, Robin).
Before introducing the concept of weak solution and of weak formulation we
brieﬂy describe the general ideas behind two quite classical methods for ﬁnding the
solution of partial differential equations: the Fourier series expansion in terms of an
orthonormal basis given by the eigenvectors of the operator, and the representation
of the solution by integral formulas, using the fundamental solution of the operator
as integral kernel.
The approach leading to the weak formulation is then described without giving all
the technical details, but only trying to specify which steps are needed for obtaining
the desired result. Though the complete functional framework is not yet clariﬁed,
nonetheless we end the chapter with the proof of the fundamental existence and
uniqueness result: the Lax–Milgram theorem.
2.1
Elliptic Equations
In this chapter we will study the boundary value problem

Lu = f
in D
BC
on ∂D ,
(2.1)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_2
9

10
2
Second Order Linear Elliptic Equations
where D is an open, connected and bounded subset of Rn, u : D →R is the
unknown, and BC stands for “boundary condition”. Here f : D →R is given and
L denotes a second order partial differential operator having the form
Lw = −
n

i,j=1
Di(aijDjw) +
n

i=1
biDiw + a0w .
(2.2)
The second order term −n
i,j=1 Di(aijDjw) is called the principal part of L. The
reason of the (mysterious) minus sign will be clear in the sequel (see Remark 2.3).
Remark 2.1 In physical models, u in general represents the density of some
quantity, for instance a chemical concentration. In the operator L, the principal part
represents the diffusion of u within D. The ﬁrst order term represents advection
(transport) of u within D. The term of order zero describes the local reactions that
occur in D.
We will focus on four different types of boundary condition:
Dirichlet BC
: u = 0 on ∂D [homogeneous case].
Neumann BC
:
n

i,j=1
niaijDju = g on ∂D.
Mixed BC
: u = 0 on D and
n

i,j=1
niaijDju = g on N, where ∂D = D ∪N,
D ∩N = ∅[homogeneous case on D].
Robin BC
:
n

i,j=1
niaijDju + κu = g on ∂D, where κ ≥0 a.e. on ∂D and

∂D κ ̸= 0.
Remark 2.2 In the case of a non-homogeneous Dirichlet boundary condition
u = u♯
on ∂D
(and, similarly, of the non-homogeneous mixed boundary condition u = u♯on D)
we proceed as follows:
1. ﬁnd u : D →R such that u|∂D = u♯;
2. setting ω = u−u, we see that ω|∂D = 0 and Lω = Lu−Lu = f −Lu. Then the
second step is: ﬁnd ω, a solution of the homogeneous Dirichlet boundary value
problem Lω = f −Lu, ω|∂D = 0;
3. ﬁnally deﬁne u = ω + u.
For arriving at the deﬁnition of elliptic equation we need now to give a deeper
look at the matrix {aij(x)}n
i,j=1 of the coefﬁcients of the principal part of L.
Deﬁnition 2.1 A (real) matrix A is said to be positive deﬁnite if Av · v > 0 for
every v ∈Rn, v ̸= 0.

2.2
Weak Solutions
11
Exercise 2.1 A matrix A is positive deﬁnite if and only if it exists α > 0 such that
Av · v ≥α|v|2 for every v ∈Rn.
Exercise 2.2 Consider a positive deﬁnite matrix A (thus satisfying Av · v ≥α|v|2
for every v ∈Rn, for a suitable α > 0). Then the real part of an eigenvalue of A is
greater than or equal to α; in particular, a positive deﬁnite matrix is non-singular.
Exercise 2.3
(i) A matrix A is positive deﬁnite if and only if A+AT
2
is positive deﬁnite.
(ii) A matrix A is positive deﬁnite if and only if all the eigenvalues λi of A+AT
2
are
strictly positive.
Deﬁnition 2.2 The partial differential operator L is said to be (uniformly) elliptic
in D if the matrix {aij(x)}n
i,j=1 is (uniformly) positive deﬁnite, i.e., if there exists a
constant α0 > 0 such that
n

i,j=1
aij(x)ηjηi ≥α0|η|2
for a.e. x ∈D, for every η ∈Rn.
Exercise 2.4
(i) Show that the operator
Lw = −D1

(1 + x1x2)D1w

−D1(x1D2w) −D2(x2D1w) −D2D2w ,
is uniformly elliptic in D = {x ∈R2 | 0 < x1 < 1/2, 0 < x2 < 1}.
(ii) Show that the operator Lw = −3
i,j=1 Di(aijDjw), with
{aij} =
⎛
⎝
1
−x3
x2
x3 1 + x2
1
x1
−x2
x2
1 + x2
3
⎞
⎠
is uniformly elliptic in D = {x ∈R3 | |x| < 1}.
2.2
Weak Solutions
Before speaking about a different idea of what is the solution of a partial differential
equation, let us spend a few words about a couple of “classical” approaches
concerning this question (say, in use throughout nineteenth century and after).

12
2
Second Order Linear Elliptic Equations
2.2.1
Two Classical Approaches
A ﬁrst approach is based on series expansion. Suppose we want to solve the problem
 −u = f
in D
u|∂D = 0
on ∂D ,
(2.3)
and we have a countable basis {ωk}∞
k=1, with ωk : D →R and ωk|∂D = 0. We can
expand u and f as u = ∞
k=1 ukωk and f = ∞
k=1 fkωk, with uk, fk ∈R, and
impose Eq. (2.3)1. This formally gives
∞

k=1
fkωk = f = −u =
∞

k=1
uk(−ωk) .
(2.4)
Expanding also −ωk (and admitting that this is possible...) we ﬁnd
−ωk =
∞

j=1
qk
j ωj ,
and inserting this result in (2.4) we obtain
∞

j=1
fjωj =
∞

k=1
uk
 ∞

j=1
qk
j ωj

=
∞

j=1
 ∞

k=1
qk
j uk

ωj .
(2.5)
Thus we have to solve the inﬁnite dimensional linear system
∞

k=1
qk
j uk = fj , j = 1, 2, . . .
(2.6)
This simpliﬁes a lot if ωk are eigenvectors of the − operator: −ωk = λkωk, with
λk ∈R the associated eigenvalues. In this case the coefﬁcients qk
j have to satisfy
λkωk =
∞

j=1
qk
j ωj ,
hence we infer
qk
j = λkδkj , k, j = 1, 2, . . .

2.2
Weak Solutions
13
where δkj is the Kronecker symbol, deﬁned by δkj = 0 if k ̸= j, δkj = 1 if k = j.
Then (2.6) can be easily solved by setting
uj = fj
λj
, j = 1, 2, . . .
provided that λj ̸= 0. In particular, if the eigenvectors ωj are an orthonormal basis
with respect to some scalar product (·, ·), one has fj = (f, ωj), the classical Fourier
coefﬁcients.
We have thus solved the problem via Fourier series expansion. This procedure
requires that we are able to ﬁnd an orthonormal basis given by eigenvectors of
the operator which satisfy the boundary condition. Clearly, one has to check that
the formal procedure we have described can be rigorously justiﬁed: the series
expansions hold, the series can be differentiated term by term, the eigenvalues λj are
different from 0. Some answers concerning these points can be found in Sect. 7.2.
The following exercise furnishes an example of orthonormal system of eigenvec-
tors in L2(D) (the proof that it is a orthonormal basis, namely, that any function f ∈
L2(D) can be expressed by a convergent Fourier series requires some additional
work: for this, see Theorem 7.7):
Exercise 2.5 Consider D = (0, a) × (0, b). Determine the eigenvalues and the
eigenvectors associated to the operator − with homogeneous Dirichlet boundary
condition, and verify that, after a suitable normalization, the eigenvectors are an
orthonormal system in L2(D). [Hint: use the method of separation of variables.]
Still referring to problem (2.3), a second approach we want to describe is the
following: suppose we know a function K(x, ξ) : D × D →R satisfying, for
x ∈D,

D
(−xK)(x, ξ)f (ξ)dξ = f (x) .
(2.7)
Before proceeding, let us see in which way such a function K could be determined.
Fix x ∈D and for m ≥1 set
ρm(ξ; x) =
1
meas(B(x, 1
m))
χB(x, 1
m )(ξ) ,
where B(x, 1
m) = {ξ ∈Rn | |x −ξ| < 1
m} and χB(x, 1
m ) is the characteristic function
of B(x, 1
m). It is readily veriﬁed that

B(x,t) ρm(ξ; x)dξ = 1 for each t > 0 and
m > 1/t. Moreover, it is well-known that, if f is continuous at x, then
lim
m→∞

D
ρm(ξ; x)f (ξ)dξ = f (x) .

14
2
Second Order Linear Elliptic Equations
Thus one could try to ﬁnd a function K(x, ξ) such that −(xK)(x, ξ)
=
−(ξK)(ξ, x) and
−(ξK)(ξ, x) = lim
m→∞ρm(ξ; x) .
Clearly, the weak point here is that lim
m→∞ρm(ξ; x) = 0, in the pointwise sense for
all ξ ̸= x, and moreover in the limit the condition saying that the average on B(x, t)
is equal to 1 is lost. A surrogate of this choice can be to look for K(x, ξ) such that
−(xK)(x, ξ) = −(ξK)(ξ, x) = 0 for ξ ̸= x and satisfying
−

∂B(x,t)
(∇ξK)(ξ, x) · n(ξ)dSξ = 1 .
The reason of this condition is that by the divergence theorem (see Theorem C.3)
we have −

B(x,t) g(ξ)dξ = −

∂B(x,t) ∇g(ξ) · n(ξ)dSξ for a smooth function g.
This procedure is indeed feasible (in Exercise 2.6 we give an example of the
construction of a function with these two properties: which however is just the
starting point for saying that (2.7) is satisﬁed in some suitable sense).
Exercise 2.6
(i) Find a function K0 = K0(ξ) deﬁned in R2 \ {0} and such that
−K0 = 0 in R2 \ {0} and −

∂B(0,t)
∇K0 · ndSξ = 1
for any t > 0. [Hint: look for a radial function K0 = K0(|ξ|).]
(ii) Verify that a function K(x, ξ) satisfying −(xK)(x, ξ) = −(ξK)(ξ, x) = 0
for ξ ̸= x and −

∂B(x,t)(∇ξK)(ξ, x) · n(ξ)dSξ = 1 for each t > 0 is given by
K(x, ξ) = K0(|x −ξ|).
Let us go back to (2.7). Being K(x, ξ) available, we set
u(x) =

D
K(x, ξ)f (ξ)dξ
(2.8)
and proceeding formally from (2.7) we have
−u(x) =

D
(−xK)(x, ξ)f (ξ)dξ = f (x) .
What is missing is the fact that u satisﬁes the boundary condition. This difﬁculty
can be overcome if we know a function G(x, ξ) : D × D →R satisfying (2.7) and

2.2
Weak Solutions
15
also G(x, ξ)|x∈∂D = 0 for each ξ ∈D. Then setting
u(x) =

D
G(x, ξ)f (ξ)dξ
furnishes a solution of (2.3).
The possibility of ﬁnding the function K introduced above depends on the
properties of the operator −, while the possibility of ﬁnding G also depends on
the properties of the domain D. Therefore, it could be useful to devise a procedure
only based on the knowledge of K. Given a function v : D →R, by integration by
parts (see Theorem C.2) we obtain

D
(−ξv)(ξ)K(ξ, x)dξ −

D
v(ξ)(−ξK)(ξ, x)dξ
=

∂D

−∇ξv(ξ) · n(ξ)K(ξ, x) + v(ξ)∇ξK(ξ, x) · n(ξ)

dSξ .
(2.9)
If K(x, ξ) = K(ξ, x), so that (ξK)(ξ, x) = (xK)(x, ξ), and we select v = u,
where u satisﬁes −u = f in D, from (2.9) and (2.7) we ﬁnd for x ∈D

D
f (ξ)K(ξ, x)dξ −u(x)
=

∂D

−∇ξu(ξ) · n(ξ)K(ξ, x) + u(ξ)∇ξK(ξ, x) · n(ξ)

dSξ .
(2.10)
This is a representation formula for u(x), x ∈D, in terms of K, f and the values
of ∇u · n and u on the boundary ∂D. If we are considering the Dirichlet or the
Neumann boundary value problems, on the boundary ∂D we know only one of the
two functions ∇u · n and u: thus we cannot conclude our argument. But if a similar
formula can be obtained for x ∈∂D (to be more precise, what it is known to hold
is the same formula with the only modiﬁcation given by the replacement at the left
hand side of u(x) with p(x)u(x), for a suitable function p), and we assume that u
is a solution of the Dirichlet boundary value problem with boundary datum u♯, then
we ﬁnally obtain

∂D
∇ξu(ξ) · n(ξ)K(ξ, x)dSξ = −

D
f (ξ)K(ξ, x)dξ + p(x)u♯(x)
+

∂D
u♯(ξ)∇ξK(ξ, x) · n(ξ)dSξ , x ∈∂D .
(2.11)
This is a boundary integral equation for the boundary unknown ∇u·n. If we are able
to solve it, we can put the obtained value of ∇u · n in (2.10) and we have found a
representation formula for the solution u(x), x ∈D. Note that a similar dual result is
obtained if we assume that u satisﬁes the Neumann boundary condition: in that case
the unknown function of the boundary integral equation is u|∂D, while (∇u · n)|∂D
becomes a known datum.

16
2
Second Order Linear Elliptic Equations
With this procedure we have thus transformed the original boundary value
problem into a boundary integral equation. Also in this case we need to show that
this formal process gives indeed the solution we are looking for. This means that we
have to show that all the integrals appearing in (2.10) and (2.11) have a meaning,
that the function given by (2.10) is differentiable as many times as we need and
satisﬁes the equation, and that as x →ˆx ∈∂D the given boundary condition is
achieved at ˆx.
The theory related to this method is called potential theory: indeed, the function
x →K(x, ξ), up to a normalization, is the potential of the electric ﬁeld generated
by a point charge placed at ξ. The function K(x, ξ) satisfying (2.7) is called the
fundamental solution of the partial differential operator (in our presentation, of the
operator −). A classical (and a little bit old fashioned) reference on this topic is
the textbook by Kellogg [9] (originally printed in 1929, and several times reprinted);
for a more recent one see McLean [15].
2.2.2
The Weak Approach
After the two examples in Sect. 2.2.1, the aim now is to describe a different point of
view, based on the deﬁnition of what is called a weak solution u of (2.1). We will
be driven by the form of a linear problem in a ﬁnite dimensional vector space, say
Rm. It can always be associated to a square matrix and it takes the form of a linear
system
Qq = p ,
(2.12)
with q, p ∈Rm. Let us remind that this problem is well-posed if and only if the map
r →Qr, r ∈Rm, is one-to-one and onto (indeed, in the ﬁnite dimensional case the
two properties are equivalent, thus it is enough that only one of them is satisﬁed).
System (2.12) is equivalent to
(Qq, r) = (p, r)
∀r ∈Rm ,
(2.13)
where we have denoted by (·, ·) a scalar product in Rm. In fact, from (2.13) we have
(Qq −p, r) = 0 for each r ∈Rm, and taking r = Qq −p the result follows.
We can also remark that the same holds true if (2.13) is valid for all r in a set V
that is dense in Rm: it is enough to recall the continuity of the scalar product due to
Cauchy–Schwarz inequality.
Noting that the new form (2.13) of problem (2.12) has at the left hand side a
bilinear form and at the right hand side a linear functional, one is led to analyze
the problems that can be written in this form: suppose you have to ﬁnd the solution
q ∈Rm of
b(q, r) = F(r)
∀r ∈Rm ,
(2.14)

2.2
Weak Solutions
17
where b(·, ·) is a bilinear form on Rm×Rm and F(·) is a linear functional on Rm. It is
straightforward to check that this can be easily rewritten in the matrix form Qq = p,
by setting Qij = b(ωj, ωi) and pi = F(ωi), where ωi are basis vectors of Rm, i =
1, . . . , m. Going a little bit further, a more abstract approach, which will be easily
extended to the inﬁnite dimensional case, is to apply the ﬁnite dimensional Riesz
representation theorem. We know that, for each ﬁxed w ∈Rm, we can represent the
linear functional r →b(w, r) by means of the scalar product of a unique element
ωw ∈Rm and r, namely, b(w, r) = (ωw, r) for each r ∈Rm. The same happens
for r →F(r), say, F(r) = (gF , r) for each r ∈Rm. The map w →ωw is clearly
linear, thus ωw can be represented as Mw for a suitable m × m matrix M. Then
solving (2.14) is equivalent to ﬁnding the solution q ∈Rm of the linear system
Mq = gF ; in particular, well-posedness of (2.14) is satisﬁed if and only if the map
r →Mr is one-to-one and onto from Rm to Rm.
Having clariﬁed this correspondence between the matrix formulation (2.12) and
formulation (2.14), let us come back to our elliptic boundary value problem. We
assume in the following that
aij, bi, a0 ∈L∞(D)
(i, j = 1, . . . , n)
(2.15)
and
f ∈L2(D) ,
(2.16)
and, for the sake of deﬁniteness, in the rest of this section we will consider the
Dirichlet boundary value problem.
When solving (2.1), we are looking for an element in an inﬁnite dimensional
vector space (loosely speaking, functions are elements of a vector space, as we can
add them and we can multiply them by a real number; moreover, for identifying
each one of them we need inﬁnitely many informations, namely, its value in all the
points of the domain D: thus they live in a inﬁnite dimensional vector space). If we
can play with a scalar product, we could repeat what has been done here above for
a ﬁnite dimensional linear system.
We know that in an inﬁnite dimensional vector space we can have inﬁnitely many
scalar products, and they are not equivalent to each other. Thus we must choose the
scalar product to be employed for mimicking the ﬁnite dimensional case, and the
natural choice is the simplest scalar product we use when dealing with functions:
the L2(D)-scalar product, i.e.,
(w, v)L2(D) =

D
wvdx .
(2.17)

18
2
Second Order Linear Elliptic Equations
Let us start now from (2.1). We know that the space of smooth functions with
compact support C∞
0 (D) is dense in L2(D), thus it could play the role of the dense
subspace V. With this in mind, Eq. (2.1) could be rewritten as
(Lu, v)L2(D = (f, v)L2(D
∀v ∈C∞
0 (D)
(we are admitting, for the moment, that u ∈C2(D) and the coefﬁcients aij ∈
C1(D), so that all the three terms deﬁning Lu belong to L2(D)). This reads

D
−
n

i,j=1
Di(aijDju)vdx +

D
n

i=1
biDiuvdx +

D
a0uvdx =

D
f vdx .
The term associated to the principal part can be balanced in a better way. In fact,
integrating it by parts and remembering that v|∂D = 0, we obtain

D
−
n

i,j=1
Di(aijDju)vdx =

D
n

i,j=1
aijDjuDivdx −

∂D
n

i,j=1
niaijDjuv|∂DdSx



= 0
and so

D
n

i,j=1
aijDjuDivdx+

D
n

i=1
biDiuvdx+

D
a0uvdx =

D
f vdx
∀v ∈C∞
0 (D) .
Deﬁnition 2.3 The bilinear form BL(· , ·) associated with the elliptic operator L
introduced in (2.2) is deﬁned by
BL(w, v) =

D
n

i,j=1
aijDjwDivdx+

D
n

i=1
biDiwvdx+

D
a0wvdx .
(2.18)
Remark 2.3 Having chosen the minus sign in (2.2) has as a consequence that in the
deﬁnition of the bilinear form (2.18) we have the plus sign!
We indicate by FD( · ) the linear functional associated to the right hand side f ,
namely, we set
FD(v) =

D
f vdx .
(2.19)
With this notation, problem (2.1) has been rephrased as follows: ﬁnd u (in which
space?) such that
BL(u, v) = FD(v)
∀v ∈C∞
0 (D) .
(2.20)

2.2
Weak Solutions
19
Remark 2.4 Let us note, from the very beginning, that the weak problem is the
right problem to face and we can focus on it without being afraid of considering
something that is not meaningful. In fact, suppose we have a classical solution u
to problem (2.1). We have just seen that u is also a solution to problem (2.20).
If we know that for problem (2.20) a uniqueness result holds, then solving (2.20)
furnishes the solution to (2.1). Furthermore, if the classical problem (2.1) has not a
solution (for instance, the right hand side f has a jump discontinuity, so that a twice
differentiable solution u cannot exist), it is still possible that the solution to (2.20)
does exist (for example, the deﬁnition of the right hand side just need f ∈L2(D)),
and that it has a correct physical meaning. In this respect, remember that physical
models are based on conservation principles, where the balance between integral
quantities is required, and the process leading to pointwise partial differential
equations is a limit process as volumes shrink at a point.
As we have remarked, the missing point in (2.20) is that we have to devise a
suitable inﬁnite dimensional vector space V where looking for u. The analogy with
the ﬁnite dimensional matrix problem suggests that V should enjoy the following
properties:
1. V is a subspace of L2(D) and is endowed with a scalar product (possibly,
stronger than the L2(D)-scalar product);
2. the bilinear form BL(·, ·) and the linear functional FD(·) are deﬁned and bounded
in V × V and V , respectively;
3. the (inﬁnite dimensional) Riesz representation theorem holds in V . This essen-
tially says that V must be a Hilbert space: namely, any Cauchy sequence in V is
convergent to an element of V . (See Sect. 3.2 for the proof of Riesz theorem and
also for some other interesting remarks.)
4. C∞
0 (D) is a subspace of V , and it is dense in V with respect to the convergence
in V . (We will see that relaxing the assumption that C∞
0 (D) is a subspace of V
is possible, but one must be careful: see Sect. 5.5.)
Let us note that in a ﬁnite dimensional vector space a linear functional is always
bounded, while this is not true in the inﬁnite dimensional case (see Sect. 3.1).
Therefore in property 2 we have explicitly assumed boundedness. Note also that
in property 4 we have taken into account that we are considering the homogeneous
Dirichlet boundary value problem; we will see that for the other boundary value
problems this assumption could refer to other subspaces of C∞(D).
The following exercise can be useful for understanding better which is the
meaning of boundedness for a linear functional.
Exercise 2.7 Let V be a Hilbert space (indeed, a normed space would be enough),
and F : V →R a linear functional. Then F is bounded if and only if it is continuous.
An inspection of the terms in BL(u, v) shows that the principal part of it is deﬁned
if ∇u, ∇v belong to (L2(D))n (and the assumption aij ∈L∞(D) is sufﬁcient);
for the lower order terms we must add the assumption u, v ∈L2(D). Thus we
could choose V = {v ∈C1(D) | v|∂D = 0}, but the choice of the scalar product

20
2
Second Order Linear Elliptic Equations
(w, v)L2(D) would not be enough (there is not a control of the integrals where ﬁrst
order derivatives appear). Therefore, we could endow V with the scalar product
(w, v)1 =

D
(wv + ∇w · ∇v)dx .
(2.21)
However, it is easy to check that with these choices of V and (·, ·)1 property 3 here
above is not satisﬁed. In fact, let us consider this exercise:
Exercise 2.8
(i) Consider D = (−1, 1) and for x ∈D deﬁne f (x) = 1−|x|, g(x) = −sign(x).
Show that there exists a sequence vk ∈V = {v ∈C1(D) | v|∂D = 0} such that
vk →f in L2(D) and v′
k →g in L2(D).
(ii) Show that V is not a Hilbert space with respect to the scalar product (·, ·)1
deﬁned in (2.21).
Thus a new problem is enlightened: on one side, the scalar product (·, ·)1, that
seems to be quite reasonable, requires that the gradient is deﬁned (and square-
summable); on the other side, the sequence vn constructed in Exercise 2.8, part
(i), is a Cauchy sequence with respect to the scalar product (·, ·)1, and, if we could
obtain that f ′(x) = g(x) (which is deﬁnitely not true in the standard sense, but also
does not seem to be completely meaningless), then we would have that vn converges
to f with respect to the scalar product (·, ·)1.
Summing up, here there is something to do: we need derivatives (and that they
belong to L2(D)), and we also need that a “corner” function admits a derivative
(belonging to L2). Therefore a natural question arises: is it the time to introduce a
different deﬁnition of derivative?
We will see: for the moment, assume that we will be able to overcome these
difﬁculties, and let us analyze how to solve a general problem of the form:
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(2.22)
where V is a Hilbert space, endowed with the scalar product (·, ·)V and the norm
∥· ∥V , and the bilinear form B(·, ·) and the linear functional F(·) are deﬁned and
bounded in V × V and V , respectively.
A particular interesting and at the same time simple situation arises when B(·, ·)
satisﬁes |B(w, v)| ≤γ ∥w∥V ∥v∥V for each w, v ∈V (boundedness), B(v, v) ≥
α∥v∥2
V for each v ∈V (coerciveness) and is symmetric, i.e., B(w, v) = B(v, w) for
each w, v ∈V (for the bilinear form BL(·, ·) introduced in (2.18) this means that the
coefﬁcients of the operator L satisfy aij = aji and bi = 0 for each i, j = 1, . . . , n).
In this case B(·, ·) is a scalar product in V , and the induced norm is equivalent to
the original one: in fact from boundedness and coerciveness we have
α∥v∥2
V ≤B(v, v)) ≤γ ∥v∥2
V .

2.3
Lax–Milgram Theorem
21
Thus solving problem (2.22) is a direct consequence of the (inﬁnite dimensional)
Riesz representation theorem (see Theorem 3.1).
Let us note, however, that in the ﬁnite dimensional case the linear system Qq =
p has a unique solution if and only if det Q ̸= 0. Hence, as shown in Exercise 2.2,
a sufﬁcient condition to have a unique solution is that Q is positive deﬁnite, i.e.,
(Qr, r) ≥α|r|2
∀r ∈Rm
for some α > 0. Therefore symmetry does not seem to be essential: we could hope
that the well-posedness of (2.22) is true even if B(·, ·) is not symmetric, but still
bounded and such that B(v, v) ≥α∥v∥2
V for each v ∈V .
The answer is in the quite important result presented in next section.
2.3
Lax–Milgram Theorem
In this section we assume V is a (real) Hilbert space, with norm ∥· ∥V and inner
product (·, ·)V (note however that the result below, with easy modiﬁcation, is also
true for a complex Hilbert space).
Theorem 2.1 (Lax–Milgram Theorem) Let B : V × V →R and F : V →R be
a bilinear form and a linear functional, respectively. Assume that B(·, ·) is bounded
and coercive in V × V , i.e., there exist constants γ > 0, α > 0 such that
|B(w, v)| ≤γ ∥w∥V ∥v∥V
∀w, v ∈V
(2.23)
and
B(v, v) ≥α∥v∥2
V
∀v ∈V ,
(2.24)
and that F : V →R is bounded in V , i.e., there exists a constant M > 0 such that
|F(v)| ≤M∥v∥V
∀v ∈V .
(2.25)
Then there exists a unique element u ∈V such that
B(u, v) = F(v)
∀v ∈V .
Moreover the stability estimate ∥u∥V ≤M
α holds true.

22
2
Second Order Linear Elliptic Equations
Proof The proof is divided into 6 steps.
1. For each ﬁxed element w ∈V , the mapping v →B(w, v) is a bounded
linear functional on V ; hence the Riesz representation Theorem 3.1 asserts the
existence of a unique element ωw ∈V satisfying
B(w, v) = (ωw, v)V
∀v ∈V .
Let us write Aw = ωw, so that for w, v ∈V it holds
B(w, v) = (Aw, v)V .
2. Similarly, once more from the Riesz representation Theorem 3.1 we observe that
we can write
F(v) = (gF , v)V
∀v ∈V
for a unique element gF ∈V . Then problem (2.22) reduces to ﬁnding a unique
u ∈V satisfying Au = gF , namely, to show that A : V →V is one-to-one and
onto.
3. We ﬁrst claim A is a bounded linear operator. Indeed if λ1, λ2 ∈R and w1, w2 ∈
V , for each v ∈V we see that
(A(λ1w1 + λ2w2), v)V = B(λ1w1 + λ2w2, v)
= λ1B(w1, v) + λ2B(w2, v)
= λ1(Aw1, v)V + λ2(Aw2, v)V
= (λ1Aw1 + λ2Aw2, v)V .
This equality is true for each v ∈V , thus we have proved that A is linear.
Furthermore
∥Av∥2
V = (Av, Av)V = B(v, Av) ≤γ ∥v∥V ∥Av∥V .
Consequently ∥Av∥V ≤γ ∥v∥V for all v ∈V and A is bounded.
4. Next we assert
⎧
⎪⎪⎨
⎪⎪⎩
A is one-to-one
and
R(A), the range of A, is closed in V.
(2.26)
To prove this, let us compute
α∥v∥2
V ≤B(v, v) = (Av, v)V ≤∥Av∥V ∥v∥V .

2.4
Exercises
23
Hence α∥v∥V ≤∥Av∥V . This inequality easily implies that A is one-to-one.
Moreover, take a sequence Avn ∈R(A) such that Avn →ω0 ∈V . Since
Avn is convergent, it is a Cauchy sequence; using the linearity of A and the
last inequality we also have ∥vn −vm∥V ≤α∥Avn −Avm∥V , thus vn is a
Cauchy sequence, too. Being V a Hilbert space we have that vn →w0 ∈V ,
and since A is bounded it follows Avn →Aw0. The uniqueness of the limit
yields ω0 = Aw0, thus R(A) is a closed subspace.
5. We prove now that
R(A) = V .
(2.27)
By the projection theorem (see Yosida [23, Theorem 1, p. 82]), it is enough to
prove that R(A)⊥= {0}. Let us take w ∈R(A)⊥; then
α∥w∥2
V ≤B(w, w) = (Aw, w)V = 0 ,
hence w = 0. In conclusion, A is onto.
6. Finally we have that
α∥u∥2
V ≤B(u, u) = F(u) ≤M∥u∥V ,
thus ∥u∥V ≤M
α .
⊓⊔
Remark 2.5 The dual space of V (i.e., the space of linear and bounded functionals
from V to R) will be denoted by V ′. Following this notation, in Lax–Milgram
theorem we have assumed F ∈V ′.
Remark 2.6 Necessary and sufﬁcient conditions for a general existence and unique-
ness result are presented in Theorem F.1.
Remark 2.7 For the sake of simplicity, in the sequel we will often say that a bilinear
form B(·, ·) : V × V →R is bounded or coercive in V , instead of in V × V .
2.4
Exercises
Exercise 2.1 A matrix A is positive deﬁnite if and only if it exists α > 0 such that
Av · v ≥α|v|2 for every v ∈Rn.
Solution
(⇐)
Trivial.
(⇒)
The map v →Av · v is positive for all v ̸= 0 and it is continuous. On the
subset |v| = 1, which is bounded and closed, it has a minimum α > 0 and a

24
2
Second Order Linear Elliptic Equations
minimum point v∗such that Av∗· v∗= α. Now take v ̸= 0 and let v♯=
v
|v|,
|v♯| = 1. Therefore we have that Av♯· v♯≥α > 0, that is
α ≤A v
|v| · v
|v| =
1
|v|2 Av · v ⇒Av · v ≥α|v|2 .
Exercise 2.2 Consider a positive deﬁnite matrix A (thus satisfying Av · v ≥α|v|2
for every v ∈Rn, for a suitable α > 0). Then the real part of an eigenvalue of A is
greater than or equal to α; in particular, a positive deﬁnite matrix is non-singular.
Solution Let λ ∈C be an eigenvalue of A, with (unit) eigenvector ω = v+iw ∈Cn,
v, w ∈Rn. We have
λ = λ|ω|2
Cn = (λω, ω)Cn = (Aω, ω)Cn = Av · v −iAv · w + iAw · v + Aw · w ,
thus
Re λ = Av · v + Aw · w ≥α(|v|2 + |w|2) = α .
As a consequence, all the eigenvalues of A are different from 0 and det A ̸= 0, thus
A is non-singular.
Exercise 2.3
(i) A matrix A is positive deﬁnite if and only if A+AT
2
is positive deﬁnite.
(ii) A matrix A is positive deﬁnite if and only if all the eigenvalues λi of A+AT
2
are
strictly positive.
Solution
(i) We have
A + AT
2
v · v = 1
2(Av · v + AT v · v) = Av · v ,
thus (i) is proved.
(ii) It is enough to note that A+AT
2
is a symmetric matrix, thus being positive
deﬁnite is equivalent to say that its minimum eigenvalue is strictly positive.
Exercise 2.4
(i) Show that the operator
Lw = −D1((1 + x1x2)D1w) −D1(x1D2w) −D2(x2D1w) −D2D2w ,
is uniformly elliptic in D = {x ∈R2 | 0 < x1 < 1/2, 0 < x2 < 1}.

2.4
Exercises
25
(ii) Show that the operator Lw = −3
i,j=1 Di(aijDjw), with
{aij} =
⎛
⎝
1
−x3
x2
x3 1 + x2
1
x1
−x2
x2
1 + x2
3
⎞
⎠
is uniformly elliptic in D = {x ∈R3 | |x| < 1}.
Solution Using Exercise 2.3, it is enough to show that the minimum eigenvalue
λ1(x) of the matrix { 1
2(aij + aji)} satisﬁes infD λ1(x) > 0.
(i) Writing A = {aij} we have
A =
1 + x1x2 x1
x2
1

and
1
2(A + AT ) =
 1 + x1x2
1
2(x1 + x2)
1
2(x1 + x2)
1

.
A simple calculation shows that
λ1(x) = 1
2

2 + x1x2 −
 
x2
1x2
2 + (x1 + x2)2

.
Since for a ≥0, b ≥0 we have
√
a + b ≤√a +
√
b, it follows that λ1(x) ≥
1
2

2 −x1 −x2) ≥1
4 for x ∈D.
(ii) We have
1
2(A + AT ) =
⎛
⎝
1
0
0
0
1 + x2
1
1
2(x1 + x2)
0 1
2(x1 + x2)
1 + x2
3
⎞
⎠.
Clearly one of the eigenvalues is equal to 1, while the minimum of the other
two is given by
λ1(x) = 1
2

2 + x2
1 + x2
3 −
 
(x2
1 −x2
3)2 + (x1 + x2)2

.
Using again the inequality √a + b ≤√a +
√
b, we ﬁnd
λ1(x) ≥1
2

2+x2
1 +x2
3 −|x2
1 −x2
3|−|x1+x2|

≥1
2

2−|x1+x2|

≥1−
√
2
2
for x ∈D.

26
2
Second Order Linear Elliptic Equations
Exercise 2.5 Consider D = (0, a) × (0, b). Determine the eigenvalues and the
eigenvectors associated to the operator − with homogeneous Dirichlet boundary
condition, and verify that, after a suitable normalization, the eigenvectors are an
orthonormal system in L2(D). [Hint: use the method of separation of variables.]
Solution We must ﬁnd functions ω = ω(x, y) and numbers λ such that −ω = λω
in (0, a) × (0, b) and ω|∂D = 0. Using the technique of separation of variables we
look for ω(x, y) = p(x)q(y), with p(0) = p(a) = 0 and q(0) = q(b) = 0.
Imposing the equation we ﬁnd
−ω = −p′′q −pq′′ = λpq = λω
in (0, a) × (0, b) ,
and dividing by pq (this is justiﬁed for p ̸= 0 and q ̸= 0, but let us go on...) we
obtain
−p′′
p −q′′
q = λ .
Since p′′
p is a function of the variable x only and q′′
q is a function of the variable
y only, this equation can be satisﬁed if and only if p′′
p and q′′
q are both equal to a
constant.
Let us write p′′
p = −μ (thus q′′
q = μ−λ). The ordinary differential equation p′′+
μp = 0 has a general solution given by p(x) = c1 exp(√−μx) + c2 exp(−√−μx)
for μ < 0, by p(x) = c1+c2x for μ = 0 and by p(x) = c1 sin(√μx)+c2 cos(√μx)
for μ > 0. In the ﬁrst two cases imposing the boundary conditions p(0) = p(a) = 0
readily yields c1 = c2 = 0, thus p is vanishing and it is not an eigenvector; in the
third case from p(0) = 0 it follows c2 = 0, thus we have to impose p(a) =
c1 sin(√μa) = 0 without setting c1 = 0. The condition to be satisﬁed is therefore
sin(√μa) = 0 ⇒√μa = mπ for m ≥1.
We have thus found the sequence μm =
m2π2
a2 , m ≥1, and the corresponding
functions pm(x) = sin( mπ
a x). Setting ν = λ −μ, a similar computation for the
other factor q yields νl = l2π2
b2 and ql(y) = sin( lπ
b y), for l ≥1.
We have thus determined
λml = m2π2
a2
+ l2π2
b2
, ωml(x, y) = sin
mπ
a x

sin
lπ
b y

, m ≥1 , l ≥1 .
From
 a
0 sin( mπ
a x) sin( m′π
a x)dx = 0 for m ̸= m′ and
 a
0 sin2( mπ
a x)dx = a
2 it is
readily seen that ωml =
2
√
abωml is an orthonormal system in L2((0, a) × (0, b)).

2.4
Exercises
27
Exercise 2.6
(i) Find a function K0 = K0(ξ) deﬁned in R2 \ {0} and such that
−K0 = 0 in R2 \ {0} and −

∂B(0,t)
∇K0 · ndSξ = 1
for any t > 0. [Hint: look for a radial function K0 = K0(|ξ|).]
(ii) Verify that a function K(x, ξ) satisfying −(xK)(x, ξ) = −(ξK)(ξ, x) = 0
for ξ ̸= x and −

∂B(x,t)(∇ξK)(ξ, x) · n(ξ)dSξ = 1 for each t > 0 is given by
K(x, ξ) = K0(|x −ξ|).
Solution
(i) Let us write |ξ| = r and look for K0(r). The Laplace operator in polar
coordinates is given by
 = ∂2
r + 1
r ∂r + 1
r2 ∂2
θ
(see Exercise 7.14). Therefore we have to solve, for r > 0,
0 = K′′
0 (r) + 1
r K′
0(r) = 1
r (rK′
0(r))′ ,
thus we have rK′
0(r) = c0, a constant. Consequently we ﬁnd K0(r) = c0 log r+
c1, and, for simplicity, we can choose c1 = 0. Then let us compute ∇K0. We
obtain
DiK0(r) = c0Di log r = c0
1
r Dir = c0
1
r
ξi
r .
On the other hand, on ∂B(0, t) we have ni = ξi
t . Thus on ∂B(0, t) we obtain
∇K0·n = c0 1
t
ξ
t · ξ
t = c0 1
t3 |ξ|2 = c0 1
t . Let us integrate this function on ∂B(0, t):

∂B(0,t)
∇K0 · ndSξ = c0
1
t meas(∂B(0, t)) = c0
1
t 2πt = 2πc0 .
In conclusion we have found c0 = −1
2π and K0(ξ) = −1
2π log |ξ|.
(ii) The result is straightforward as K(x, ξ) given by K0(|x −ξ|) is symmetric with
respect to x and ξ, and then radial with center at x.
Exercise 2.7 Let V be a Hilbert space (indeed, a normed space would be enough)
and F : V →R a linear operator. Then F is bounded if and only if it is continuous.
Solution If F is bounded, namely, |F(v)| ≤γ ∥v∥V for a suitable γ > 0, from
linearity we readily obtain that F(vk) →F(v0) if vk →v0 in V .

28
2
Second Order Linear Elliptic Equations
Conversely, assume that F is continuous. Since F is linear we have F(0) = 0; then
there exists δ > 0 such that |F(v)| ≤1 for ∥v∥V ≤δ. Take now v ∈V , v ̸= 0.
Deﬁne w = δ
v
∥v∥V , so that ∥w∥V = δ. We have |F(w)| ≤1, hence |F(v)| ≤
1
δ ∥v∥V .
Exercise 2.8
(i) Consider D = (−1, 1) and for x ∈D deﬁne f (x) = 1−|x|, g(x) = −sign(x).
Show that there exists a sequence vk ∈V = {v ∈C1(D) | v|∂D = 0} such that
vk →f in L2(D) and v′
k →g in L2(D).
(ii) Show that V is not a Hilbert space with respect to the scalar product (v, w)1
deﬁned in (2.21).
Solution
(i) Take vk deﬁned as follows:
vk(x) =
⎧
⎨
⎩
1 −|x|
for −1 < x < −1
k
1 −1
2k −k
2x2
for −1
k ≤x ≤1
k
1 −|x|
for 1
k < x < 1 .
It is easily seen that vk ∈V and that
v′
k(x) =
⎧
⎨
⎩
1
for −1 < x < −1
k
−kx
for −1
k ≤x ≤1
k
−1
for 1
k < x < 1 .
Then
 1
−1
(v′
k(x) + sign(x))2dx =
 0
−1
k
(−kx −1)2dx +

1
k
0
(−kx + 1)2dx
= (kx + 1)3
3k
0
−1
k + (kx −1)3
3k

1
k
0 = 2
3k .
On the other hand
 1
−1
(vk(x) −1 + |x|)2dx =
 0
−1
k
(1 −1
2k −k
2x2 −1 + x)2dx
+

1
k
0
(1 −1
2k −k
2x2 −1 −x)2dx
= 2

1
k
0
( 1
2k + k
2x2 + x)2dx ≤2 1
k
4
k2 = 8
k3 .

2.4
Exercises
29
(ii) Part (i) says that vk and v′
k are convergent sequences, therefore Cauchy
sequences in L2(D). Thus vk is a Cauchy sequence with respect to norm
induced by the scalar product (·, ·)1. Assume, by contradiction, that vk con-
verges with respect to this norm to a function v0 ∈V . Since the scalar
product (·, ·)1 is stronger than the scalar product (·, ·)L2(D, one also has that
vk converges to v0 in L2(D), therefore v0 = f . Since f ̸∈V , a contradiction is
produced.

Chapter 3
A Bit of Functional Analysis
For the ease of the reader, in this chapter we present some results of functional
analysis: in particular, we show how a ﬁnite dimensional normed vector space and a
inﬁnite dimensional normed vector space enjoy different properties, and which are
some basic points that make a Hilbert space different from a pre-hilbertian space.
3.1
Why Is Life in an Inﬁnite Dimensional Normed Vector
Space V Harder Than in a Finite Dimensional One?
1. The boundedness (continuity) of a linear functional must be explicitly required.
In fact:
If dim V < +∞a linear functional is bounded.
If dim V = +∞this is not true anymore.
Example 3.1 Let’s take the space of trigonometric polynomials
V =
!
v : [0, 2π] →R | ∃N ≥0, ∃{ak, bk}N
k=0 such that
v =
N

k=0
ak cos(kx) + bk sin(kx)"
,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_3
31

32
3
A Bit of Functional Analysis
endowed with the scalar product (v, w)V =
 2π
0
vwdx. Set Lv = v′ and take
vm = sin(mx), m ≥1, then
 2π
0
v2
mdx =
 2π
0
(sin(mx))2dx = π
 2π
0
(Lvm)2dx =
 2π
0
(m cos(mx))2dx = m2π
and
∥Lvm∥V
∥vm∥V
= m√π
√π
= m →∞.
Hence the functional L is linear but not bounded.
2. The precompactness of a bounded set must be explicitly proved. In fact:
If dim V
< +∞from a bounded sequence you can extract a convergent
subsequence (Bolzano–Weierstrass Theorem).
If dim V = +∞this is not true anymore.
Example 3.2 Let’s take wm an orthonormal system in L2(0, 2π) = V (for
instance wm(x) =
1
√π sin(mx)). Then
∥wm∥V = 1
and, for k ̸= m,
∥wm −wk∥2
V = (wm −wk, wm −wk)V = ∥wm∥2
V + ∥wk∥2
V −2(wm, wk)V



=0
= 2 .
Thus any subsequence extracted by wm is not convergent, as it is not a Cauchy
sequence.
3. The convergence of Cauchy sequences must be explicitly proved. In fact:
If dim V < +∞any Cauchy sequence in V is convergent to an element in V .
[Indeed a Cauchy sequence is bounded (see Exercise 3.1 (i)) and from point 2.
you can extract a convergent subsequence; if a Cauchy sequence has a convergent
subsequence then the whole sequence is convergent (see Exercise 3.1 (ii)).]
If dim V = +∞this is not true anymore.
Example 3.3 Let us take V = C0([−1, 1]) endowed with the scalar product
(v, w)V =
 1
−1 vwdx and consider
vm(x) =
⎧
⎪⎪⎨
⎪⎪⎩
0
x ∈[−1, 0]
mx
x ∈(0, 1/m)
1
x ∈[1/m, 1]
(3.1)
(see Fig. 3.1).

3.1
Why Is Life in an Inﬁnite Dimensional Normed Vector Space V Harder. . .
33
−1
1
0.5
1
x
y
−1
1
0.5
1
x
y
Fig. 3.1 The graph of the function vm in (3.1) for m = 2 (left) and m = 4 (right)
Then setting
v(x) =

0
x ∈[−1, 0]
1
x ∈(0, 1] ,
we have that
 1
−1
|vm −v|2dx =
 1/m
0
(1 −mx)2dx ≤1
m →0 .
Therefore vm is a Cauchy sequence in V , but it is not convergent to an element
in V , as v /∈C0([−1, 1]).
4. The closure of a vector subspace must be explicitly proved. In fact;
If dim V < +∞a subspace is always closed.
If dim V = +∞this is not true anymore.
Example 3.4 Let us take V = L2(−1, 1) with (v, w)V
=
 1
−1 vwdx. As a
subspace of V take W = C0([−1, 1]) and choose vm as in the previous example.
Then vm ∈W, vm →v in V but v /∈W.
5. The closure of the range of a linear and bounded operator A : V →W, V and
W Hilbert spaces, must be explicitly proved. In fact;
If dim V < +∞the range of A, being a subspace, is always closed.
If dim V = +∞this is not true anymore.
Example 3.5 Take V = W = L2(−1, 1) and A : v →Av where (Av)(x) =
 x
−1 v(t)dt. Clearly A is a linear operator, Av ∈V and ﬁnally A is a bounded
operator, as by Cauchy–Schwarz inequality
 1
−1
(Av)2(x)dx =
 1
−1
 x
−1
v(t)dt
2
dx ≤
 1
−1
(x + 1)
 x
−1
v(t)2dt

dx
≤
 1
−1
v(t)2dt
 (x + 1)2
2

1
−1 = 2
 1
−1
v(t)2dt .

34
3
A Bit of Functional Analysis
Indeed, we have a further regularity result, as any Av ∈R(A) is uniformly
continuous in [−1, 1]. In fact, for x1, x2 ∈[−1, 1], x1 < x2 it holds
|(Av)(x2) −(Av)(x1)| =

 x2
x1
v(t)dt

C−S

≤√x2 −x1
 1
−1
v(t)2dt
1/2
.
Choose now ωm ∈V as follows:
ωm(x) =
⎧
⎨
⎩
0 for −1 ≤x ≤0
m for 0 < x < 1/m
0 for 1/m ≤x ≤1 .
As a consequence we have that Aωm is given by
(Aωm)(x) =
⎧
⎨
⎩
0
for −1 ≤x ≤0
mx for 0 < x < 1/m
1
for 1/m ≤x ≤1 ,
thus Aωm are equal to the functions vm in Example 3.3, (3.1). There we have
seen that Aωm = vm converges to
v(x) =

0
x ∈[−1, 0]
1
x ∈(0, 1] .
Since v is discontinuous, it follows that v ̸∈R(A) and therefore the range of A
is not closed.
3.2
Why Is Life in a Hilbert Space Better Than in a
Pre-Hilbertian Space?
Deﬁnition 3.1 A pre-hilbertian space is a space endowed with a scalar product.
It is clearly difﬁcult to express which is the main basic difference between a
pre-hilbertian space and a Hilbert space. A possible answer, the one on which we
ﬁrst focus here, is that in a Hilbert space we have the Riesz representation theorem,
whereas in a pre-hilbertian space that is not true. We will see later that we can make
more precise this assertion.
Theorem 3.1 (Riesz Representation) Let V be a Hilbert space, and let F : V →
R be a linear and bounded functional. Then there exists a unique ω ∈V such that
F(v) = (ω, v)V for each v ∈V .

3.2
Why Is Life in a Hilbert Space Better Than in a Pre-Hilbertian Space?
35
Let us give a proof of Riesz theorem. If an element ω ∈V satisﬁes F(v) =
(ω, v)V for all v ∈V , then ω ∈N⊥= {w ∈V | (w, v)V = 0 ∀v ∈N}, where
N = {v ∈V | F(v) = 0}. If F(v) = 0 for all v ∈V , take ω = 0. Otherwise
take ˆω ̸= 0, ˆω ∈N⊥, and look for ω in the form ω = α ˆω for a suitable α ∈R.
Imposing that the representation formula is true for v = ˆω, namely, that we have
F( ˆω) = (α ˆω, ˆω)V , it follows
α = F( ˆω)
∥ˆω∥2
V
.
We claim that
ω = F( ˆω)
∥ˆω∥2
V
ˆω .
We have to prove that such ω satisﬁes F(v) = (ω, v)V for each v ∈V . It holds
F(v) ?=
F( ˆω)
( ˆω, ˆω)V
( ˆω, v)V ⇐⇒F(v)( ˆω, ˆω)V −F( ˆω)( ˆω, v)V
?= 0
⇐⇒(F(v) ˆω −F( ˆω)v, ˆω)V
?= 0 ,
thus it is sufﬁcient to prove that (F(v) ˆω −F( ˆω)v) ∈N. Indeed by linearity we have
F( ˆωF(v) −vF( ˆω)) = F( ˆωF(v)) −F(vF( ˆω))
= F( ˆω)F(v) −F(v)F( ˆω) = 0 .
We have thus completed the proof of the Riesz representation theorem. But where
did we use the assumption that V is a Hilbert space and not simply a pre-hilbertian
space? At a ﬁrst look it is not so evident...
The point is that we have assumed that there exists ˆω ̸= 0, ˆω ∈N⊥. But we only
know that there exists ω∗̸= 0 such that F(ω∗) ̸= 0, namely, ω∗̸= 0, ω∗/∈N.
In a pre-hilbertian space this does not mean that we can ﬁnd ˆω ̸= 0, ˆω ∈N⊥. It
is possible that N⊥= {0} even if N ̸= V ! On the contrary this is not possible for
a Hilbert space, as we have the projection theorem (see Yosida [23, Theorem 1, p.
82]) and therefore if N ̸= V we know that N⊥is not trivial, because we can split
V = N ⊕N⊥, writing ω∗̸= 0 as
ω∗= PNω∗
  
∈N
+ PN⊥ω∗
  
∈N⊥
with PN⊥ω∗̸= 0 if ω∗/∈N.
Example 3.6 Let us give an example of N ̸= V , N⊥= {0} for a pre-hilbertian
space V . Take V = C∞
0 (D) with D an open, connected, bounded set, and endow

36
3
A Bit of Functional Analysis
V with the scalar product (v, w)V =

D vwdx. Consider F(v) =

D vdx and note
that F is linear and continuous, as by the Cauchy–Schwarz inequality
|F(v)| =


D
vdx
 ≤

D
|v|dx ≤(meas(D))1/2

D
v2dx
1/2
∀v ∈V .
It is also clear that
N =

v ∈C∞
0 (D)


D
vdx = 0
#
(3.2)
is a subspace with N ̸= V , as there are C∞
0 (D) functions that are positive and
not identically 0, thus satisfying

D vdx > 0. It is also easy to show that N is a
closed subspace, namely, if a sequence vm ∈N converges to v⋆∈V with respect
to the norm associated to (·, ·)V , then

D v⋆dx = 0, thus v⋆∈N. If ω ∈N⊥
(orthogonality in V , thus ω ∈C∞
0 (D)...), for each v ∈N it follows
0 =

D
ωvdx =

D
(ω −ωD)vdx + ωD

D
vdx
  
=0
=

D
(ω −ωD)vdx ,
where
ωD =
1
meas(D)

D
ωdx .
If we prove that N is dense in
L2
∗(D) =

v ∈L2(D)


D
vdx = 0
#
(see below, Exercise 3.2), then by a density argument we can also write
0 =

D
(ω −ωD)vdx
∀v ∈L2
∗(D) .
Taking v = ω −ωD, which satisﬁes v ∈C∞(D) with

D v = 0, therefore belongs
to L2
∗(D), it follows that

D
(ω −ωD)2dx = 0 ⇒ω −ωD = 0 in D .
As a consequence ω is constant in D, and from ω ∈C∞
0 (D) it follows ω = 0.

3.3
Exercises
37
Example 3.7 In particular, we can also see that in V = C∞
0 (D), endowed with the
scalar product (v, w)V =

D vwdx, the Riesz theorem is false. If we had ω ∈V
such that
F(v) =

D
vdx = (ω, v)V
∀v ∈V ,
then we would have
(ω, v)V = 0
∀v ∈N ,
hence ω ∈N⊥. From what we have seen above we would obtain ω = 0, and this is
a contradiction as there exists v ∈V with F(v) =

D v ̸= 0.
As a ﬁnal comment, let us come back to the main basic difference between a
pre-hilbertian space and a Hilbert space. We can conclude that, in our context, it is
the fact that for a Hilbert space the projection theorem holds, and, as a consequence,
the Riesz theorem is valid.
3.3
Exercises
Exercise 3.1
(i) A Cauchy sequence vk ∈V is bounded.
(ii) A Cauchy sequence vk ∈V with a convergent subsequence is convergent.
Solution
(i) Fix ϵ0 > 0 and consider N∗∈N such that ∥vk −vs∥V ≤ϵ0 for k, s ≥N∗.
Then for k ≥N∗it holds
∥vk∥V ≤∥vk −vN∗∥V + ∥vN∗∥V ≤ϵ0 + ∥vN∗∥V ,
thus vk is bounded as there are only a ﬁnite number of terms vk for k < N∗.
(ii) Let vks be a subsequence convergent to v∗∈V . Fix ϵ > 0: we know that there
exists Nϵ ∈N such that
∥vkm −v∗∥V ≤ϵ , ∥vs −vr∥V ≤ϵ
for m ≥Nϵ and s, r ≥Nϵ. Since the sequence of integers km is strictly
increasing (deﬁnition of a subsequence...) it holds km ≥m; thus taking
m ≥Nϵ it follows
∥vm −v∗∥V ≤∥vm −vkm∥V + ∥vkm −v∗∥V ≤2ϵ .

38
3
A Bit of Functional Analysis
Exercise 3.2 N, deﬁned as in (3.2), is dense in L2
∗(D).
Solution Take v ∈L2
∗(D). Since C∞
0 (D) is dense in L2(D), we have ϕm ∈C∞
0 (D)
with ∥ϕm −v∥V →0 as m →∞. Take ψ ∈C∞
0 (D) with 
D ψdx ̸= 0 and deﬁne
ˆψ =
ψ

D ψdx ;
thus ˆψ ∈C∞
0 (D) and

D ˆψdx = 1. Deﬁne Im =

D ϕmdx and take
˜ϕm = ϕm −Im ˆψ .
We have ˜ϕm ∈C∞
0 (D) and

D
˜ϕmdx =

D
ϕmdx −Im

D
ˆψdx = 0 ,
thus ˜ϕm ∈N. Moreover
∥˜ϕm −v∥V = ∥ϕm −Im ˆψ −v∥V ≤∥ϕm −v∥V + |Im| ∥ˆψ∥V .
Since
|Im| =


D
ϕmdx
 =


D
(ϕm −v)dx
 ≤(meas(D))1/2∥ϕm −v∥V ,
the result follows.

Chapter 4
Weak Derivatives and Sobolev Spaces
The functional spaces deﬁned in terms of classical derivatives are unfortunately not
a suitable setting for a PDEs theory based on weak formulations, as we are not
usually able to prove that weak solutions actually belong to such spaces. Therefore
other kind of spaces are needed: we must weaken the requirement of smoothness for
the functions belonging to them. On the other hand, the bilinear form determined
in (2.18) contains derivatives. Summing up, we need to speak about derivatives, but
this is not possible in the classical sense: we have to introduce a new concept.
The aim of the next section is to extend the meaning of partial derivative. On the
basis of this new idea, in Sect. 4.2 we deﬁne the functional spaces that will be used
for the variational formulation of the boundary value problems we are interested in.
4.1
Weak Derivatives
Let us start with some preliminaries.
Remark 4.1 (Motivation for Deﬁnition of Weak Derivatives) Assume we are given
a function u ∈C1(D). Then if ϕ ∈C∞
0 (D) (we will call a function ϕ belonging
to C∞
0 (D) a test function), we see from the integration by parts formula (see
Theorem C.2) that

D
uDiϕdx = −

D
Diuϕdx
∀i = 1, . . ., n .
(4.1)
There are no boundary terms, since ϕ has a compact support in D and thus vanishes
near ∂D. More generally, if k is a positive integer, u ∈Ck(D) and α = (α1, . . . , αn)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_4
39

40
4
Weak Derivatives and Sobolev Spaces
is a multi-index of order |α| = α1 + · · · + αn = k, then

D
uDαϕdx = (−1)|α|

D
Dαuϕdx
(4.2)
This equality holds since
Dαϕ = ∂α1
∂xα1
1
. . . ∂αn
∂xαn
n
ϕ
and we can apply (4.1) |α| times.
We next examine if (4.2) can be generalized to functions u that are not k times
continuously differentiable. The left hand side of (4.2) makes sense if u is only
locally summable: the problem is rather that if u is not Ck, then the expression Dαu
on the right hand side of (4.2) has no obvious meaning. We overcome this difﬁculty
by asking that there exists a locally summable function ωα for which formula (4.2) is
valid, with ωα replacing Dαu. (We remember that a function v is locally summable,
written v ∈L1
loc(D), if for every measurable subset E that is bounded and satisﬁes
E ⊂D, written E ⊂⊂D, we have that v ∈L1(E).)
Deﬁnition 4.1 Let D ⊂Rn be an open set. Suppose u, ωα ∈L1
loc(D), and α is a
multi-index. We say that ωα is the αth-weak partial derivative of u, written
Dαu = ωα ,
if

D
uDαϕdx = (−1)|α|

D
ωαϕdx
(4.3)
for all test functions ϕ ∈C∞
0 (D).
Remark 4.2 Note that, for the sake of simplicity, we are using the same notation
Dαu for weak derivatives and for classical derivatives. However, we believe that in
the sequel it will be easy to understand from the context which type of derivative we
refer at.
Remark 4.3 Let us not that in the classical sense differentiation is a local concept:
we deﬁne the derivative of a function u at a point x0 ∈D, and we say that u is
differentiable in D if its derivative exists at each point x ∈D. Here the concept of
weak derivative is global: the weak derivative is a function deﬁned in D.
Proposition 4.1 (Uniqueness of Weak Derivatives) A weak αth-partial deriva-
tives of u, if it exists, is uniquely deﬁned up to a set of measure zero.

4.1
Weak Derivatives
41
Proof Assume that ωα, ˜ωα ∈L1
loc(D) satisfy

D
uDαϕdx = (−1)|α|

D
ωαϕdx = (−1)|α|

D
˜ωαϕdx
for all ϕ ∈C∞
0 (D). Then

D
(ωα −˜ωα)ϕdx = 0
for all ϕ ∈C∞
0 (D); whence, since ωα −˜ωα ∈L1
loc(D), we have that ωα −˜ωα = 0
almost everywhere by du Bois-Reymond lemma (see Lemma 6.1).
⊓⊔
Remark 4.4 Note that if a function u is differentiable in D, then its classical
derivative Diu coincides with its weak derivative, as it is a function which belongs to
L1
loc(D) and satisﬁes (4.3). Hence the concept of weak derivative is a generalization
of the concept of classical derivative.
Proposition 4.2 The map u →ωα, where ωα is the αth-weak partial derivatives of
u, is linear.
Proof Straightforward from the deﬁnition.
⊓⊔
Exercise 4.1 Set Xα = {v ∈L2(D) | Dαv ∈L2(D)}, where α is a multi-index.
The operator Dα : u →Dαu deﬁned in Xα is a closed operator from L2(D) to
L2(D), namely, if for um ∈Xα one has um →u in L2(D) and Dαum →wα in
L2(D) then it follows wα = Dαu.
Example 4.1 Let n = 1, D = (0, 2), and
u(x) =

1 −x
if 0 < x ≤1
x −1
if 1 < x < 2
(4.4)
(see Fig. 4.1).
Fig. 4.1 The graph of the
function u in (4.4)
1
2
0.5
1
x
y

42
4
Weak Derivatives and Sobolev Spaces
Deﬁne
ω(x) =

−1
if 0 < x ≤1
1
if 1 < x < 2 .
(4.5)
Let us show that u′ = ω in the weak sense. To see this, we must prove that
 2
0
uϕ′dx = −
 2
0
ωϕdx
for each ϕ ∈C∞
0 (D). We easily compute, integrating by parts in (0, 1) and in (1, 2),
 2
0
uϕ′dx =
 1
0
(1 −x)ϕ′dx +
 2
1
(x −1)ϕ′dx
=
 1
0
ϕdx −ϕ(0)

=0
−
 2
1
ϕdx + ϕ(2)

=0
= −
 2
0
ωϕdx ,
as required.
Example 4.2 Let n = 1, D = (0, 2), and
u(x) =

1
if 0 < x ≤1
2
if 1 < x < 2
(4.6)
(see Fig. 4.2). We claim that u′ does not exist in the weak sense. To check this, we
must show that it is not possible to ﬁnd any function ω ∈L1
loc(D) satisfying
 2
0
uϕ′dx = −
 2
0
ωϕdx
(4.7)
for all ϕ ∈C∞
0 (D). Suppose, by contradiction, that (4.7) is valid for some ω ∈
L1
loc(D) and all ϕ ∈C∞
0 (D). Then, taking into account that ϕ(0) = ϕ(2) = 0,
−
 2
0
ωϕdx =
 2
0
uϕ′dx =
 1
0
ϕ′dx + 2
 2
1
ϕ′dx
= ϕ(1) −2ϕ(1) = −ϕ(1) .
(4.8)

4.1
Weak Derivatives
43
Fig. 4.2 The graph of the
function u in (4.6)
1
2
1
2
x
y
Choose in C∞
0 (D) a sequence {ϕm}∞
m=1 satisfying
0 ≤ϕm ≤1,
ϕm(1) = 1, ϕm(x) →0 for all x ̸= 1,
suppϕm ⊂K ⊂⊂(0, 2).
Replacing ϕ by ϕm in (4.8) and sending m →∞, we discover, by the Lebesgue
dominated convergence theorem,
1 = lim
m→∞ϕm(1) = lim
m→∞
 2
0
ωϕmdx = 0 ,
a contradiction. Note that we can apply the Lebesgue dominated convergence
theorem, as
 2
0 ωϕmdx =

K ωϕmdx and |ωϕm| ≤|ω|, with ω ∈L1(K).
Remark 4.5 The computations in Example 4.2 in particular show that the functional
ϕ →ϕ(1), ϕ ∈C∞
0 (0, 2), cannot be represented by
 2
0 ωϕdx for a function ω ∈
L1
loc(0, 2). In other words, the Dirac δ “function” is not a function.
An example of sequence ϕm ∈C∞
0 (0, 2) with the required properties is given by
ϕm(x) =
⎧
⎨
⎩
e
1−
1
1−4m2|x−1|2
if |x −1| <
1
2m
0
if |x −1| ≥
1
2m
(4.9)
(see Fig. 4.3).
Exercise 4.2 Let ϕm as in (4.9) and set ψm(x) = I −1
m ϕm(x), x ∈(0, 2), where
Im =
 2
0 ϕmdx. Show that
 2
0 ψmϕdx →ϕ(1) for each ϕ ∈C∞
0 (0, 2). Repeat the
proof for each ϕ ∈C0(0, 2) (Fig. 4.3).

44
4
Weak Derivatives and Sobolev Spaces
1
2
0.5
1
x
y
1
2
0.5
1
x
y
Fig. 4.3 The graph of the function ϕm in (4.9) for m = 1 (left) and m = 2 (right)
4.2
Sobolev Spaces
In this section we ﬁnally introduce the inﬁnite dimensional vector spaces that furnish
the “right” framework for the weak formulation of partial differential equations.
In some particular case, these spaces had been considered since the beginning of
the last century, but their systematic deﬁnition and use dates back to the thirties,
especially in the papers by Sergei L. Sobolev [20, 21].
Take 1 ≤p ≤+∞and let k be a non-negative integer. Now we deﬁne certain
functional spaces, whose elements have weak derivatives of some order lying in Lp.
Deﬁnition 4.2 Let D ⊂Rn be an open set. The Sobolev space
W k,p(D)
consists of all locally summable function u : D →R such that for each multi-index
α with |α| ≤k the derivative Dαu exists in the weak sense and belongs to Lp(D).
Remark 4.6
(i) If p = 2, we usually write
W k,2(D) = H k(D) .
In particular, W 0,2(D) = H 0(D) = L2(D).
(ii) From the deﬁnition it is clear that we identify functions in W k,p(D) if they
agree almost everywhere.
Deﬁnition 4.3 If v ∈W k,p(D), with 1 ≤p < +∞we deﬁne its norm to be
∥v∥W k,p(D) :=
 
|a|≤k

D
|Dαv|pdx
1/p
=
 
|a|≤k
∥Dαv∥p
Lp(D)
1/p
.

4.2
Sobolev Spaces
45
If p = +∞the norm is deﬁned as
∥v∥W k,∞(D) := max
|α|≤k ∥Dαv∥L∞(D) .
For 1 ≤p ≤+∞we may also use the equivalent norm deﬁned as

|α|≤k
∥Dαv∥Lp(D) .
Deﬁnition 4.4 We denote by
W k,p
0
(D)
the closure of C∞
0 (D) in W k,p(D).
Thus v ∈W k,p
0
(D) if and only if there exist functions vm ∈C∞
0 (D) such
that vm →v in W k,p(D). We will se later (see Remark 6.5) that we can interpret
W k,p
0
(D) as the space of those functions v ∈W k,p(D) such that
“Dαv = 0 on ∂D” for all |α| ≤k −1 .
It is customary to write
W k,2
0
(D) = H k
0 (D) .
Remark 4.7 The norm ∥· ∥W k,p(D) is actually a norm. Indeed
1. ∥v∥W k,p(D) =
 
|a|≤k ∥Dαv∥p
Lp(D)



≥0
1/p
≥0.
2. If v = 0 then trivially ∥v∥W k,p(D) = 0. On the other hand, if ∥v∥W k,p(D) = 0
we have
 
|a|≤k ∥Dαv∥p
Lp(D)
1/p
= 0, thus in particular ∥v∥Lp(D) = 0 which
implies v = 0 almost everywhere in D.
3. Take λ ∈R: then
∥λv∥W k,p(D) =
 
|a|≤k
∥Dα(λv)∥p
Lp(D)
1/p
= |λ|
 
|a|≤k
∥Dαv∥p
Lp(D)
1/p
= |λ| ∥v∥W k,p(D) .

46
4
Weak Derivatives and Sobolev Spaces
4. We have ﬁnally to verify that the triangular inequality ∥w + v∥W k,p(D) ≤
∥w∥W k,p(D) + ∥v∥W k,p(D) holds true. Indeed, if 1 ≤p < +∞, the discrete
Minkowski’s inequality implies
∥w+v∥W k,p(D) =
 
|a|≤k
∥Dα(w + v)∥p
Lp(D)
1/p
=
 
|a|≤k
∥Dαw + Dαv∥p
Lp(D)
1/p
≤
 
|a|≤k
(∥Dαw∥Lp(D) + ∥Dαv∥Lp(D))p1/p
≤
 
|a|≤k
∥Dαw∥p
Lp(D)
1/p
+
 
|a|≤k
∥Dαv∥p
Lp(D)
1/p
= ∥w∥W k,p(D) + ∥v∥W k,p(D) .
The case p = +∞is trivial.
Theorem 4.1 The space W k,p(D) is a Banach space.
Proof We have already proved that W k,p(D) is a normed space. It remains to prove
that each Cauchy sequence {vn}∞
n=1 is convergent in W k,p(D). Assume that for each
ε > 0 it exists Mε ∈N such that for all n, m > Mε
∥vn −vm∥W k,p(D) =
 
|α|≤k
∥Dα(vn −vm)∥p
Lp(D)
1/p
≤ε
for 1 ≤p < +∞or
∥vn −vm∥W k,∞(D) = max
|α|≤k ∥Dα(vn −vm)∥L∞(D) ≤ε .
In particular we have that for all α with |α| ≤k
∥Dα(vn −vm)∥Lp(D) ≤ε ,
i.e., {Dαvn}∞
n=1 is a Cauchy sequence in Lp(D). Since Lp(D) is a Banach space,
for any α with |α| ≤k there exits vα ∈Lp(D), such that
Dαvn
Lp
→vα as n →∞.

4.2
Sobolev Spaces
47
In particular with α = (0, . . . , 0) we have that vn
Lp
→v(0,...,0) (which we denote by
v0). We now claim that
v0 ∈W k,p(D) and Dαv0 = vα .
To verify this assertion, ﬁx ϕ ∈C∞
0 (D). Then

D
v0Dαϕdx = lim
n→∞

D
vnDαϕdx =
= lim
n→∞(−1)|α|

D
Dαvnϕdx =
= (−1)|α|

D
vαϕdx .
Thus we have Dαv0 = vα and consequently Dαvn
Lp
→Dαv0 for all |α| ≤k, which
means vn →v0 in W k,p(D), as required.
⊓⊔
Remark 4.8 The Sobolev space W k,2(D) = H k(D) is a Hilbert space. In fact, it is
easy to prove that the norm
∥v∥2
H k(D) =

|α|≤k

D
|Dαv|2 dx =

|α|≤k
∥Dαv∥2
L2(D)
is induced by the scalar product
(w, v)H k(D) =

|α|≤k

D
DαwDαv dx .
In particular, if k = 1 we have that
(w, v)H 1(D) =

D
wv dx +

D
∇w · ∇v dx
and therefore
∥v∥H 1(D) =
 
D
v2dx +

D
|∇v|2dx
1/2
.
Remark 4.9 It is proved that W k,p(D) is a reﬂexive Banach space when 1 <
p < +∞and is a separable Banach space when 1 ≤p < +∞(see Adams [1,
Theorem 3.5]).

48
4
Weak Derivatives and Sobolev Spaces
−1
1
5
10
x
y
−1
1
5
10
x
y
Fig. 4.4 The graph of the function |x|−α for α = 1/2 (left) and α = 1/4 (right) (The graph is
drawn for 0.01 ≤|x| ≤1)
Example 4.3 Take D = B1, the open unit ball in Rn centered at 0, and
u(x) = |x|−α
(x ∈D, x ̸= 0)
(see Fig. 4.4). We notice that u /∈L∞(D) and we want to ﬁnd for which α > 0,
p ∈[1, +∞), n ≥1 the function u belongs to W 1,p(D).
To answer, note ﬁrst that u is smooth away from 0, i.e., for x with |x| > 0 we
have that x →u(x) ∈C∞; thus in this set we can compute the derivatives in the
classical sense. We have
Diu = (−α) |x|−α−1 Di(|x|) = (−α) |x|−α−1 Di

n

j=1
x2
j
1/2
= (−α) |x|−α−1 1
2
1
|x| 2xi = −αxi
|x|α+2 ;
therefore for x ̸= 0 it holds
|∇u(x)| = | −α| |x|
|x|α+2
=
α
|x|α+1 .
For i = 1, . . . , n let us deﬁne
ωi(x) =
Diu(x) for x ̸= 0
0
for x = 0 .
Let us determine for which values of α we have u ∈Lp(D) and ωi ∈Lp(D). We
can employ polar coordinates (in dimension n), and we ﬁnd

D
|u|pdx = κn
 1
0
ρ−αpρn−1dρ = κn
 1
0
ρ−αp+n−1dρ ,

4.2
Sobolev Spaces
49
where κn is the (n −1)-measure of the set {x ∈Rn | |x| = 1}. Thus u ∈Lp(D)
if and only if αp < n (in particular, u ∈L1(D) if and only if α < n). A similar
calculation shows that

D

n

1=1
ω2
i
p/2
dx = αpκn
 1
0
ρ−(α+1)p+n−1dρ ,
thus ωi ∈Lp(D) if and only if (α + 1)p < n (and ωi ∈L1(D) if and only if
α + 1 < n).
Assume therefore n ≥2 and α < n −1, so that u, ωi ∈L1(D) and we are
allowed to consider weak derivatives of u. We want to show that the weak derivative
Diu is equal to ωi. Let ϕ ∈C∞
0 (D) and ﬁx ε > 0. Then, denoting by Bε the ball
centered at 0 with radius ε > 0,

D\Bε uDiϕdx = −

D\Bε Diuϕdx −

∂Bε uϕnidSx
= −

D\Bε ωiϕdx −

∂Bε uϕnidSx ,
where n denotes the unit normal on ∂Bε, external to Bε. It holds


∂Bε
uϕnidSx
 ≤∥ϕ∥L∞(D)

∂Bε
ε−αdSx = Cn,ϕεn−1−α →0 ,
as α < n −1. Thus passing to the limit as ε →0+ and taking into account that
uDiϕ ∈L1(D) and ωiϕ ∈L1(D) one ﬁnds

D
uDiϕdx = −

D
ωiϕdx
for all ϕ ∈C∞
0 (D). We have thus proved that Diu = ωi, and in conclusion u ∈
W 1,p(D) if and only if α < (n −p)/p; in particular u /∈W 1,p(D) for each p ≥n.
This example seems to show that unbounded functions are not allowed to belong
to W 1,p(D) when p ≥n: we will see later on that this in fact true, but for the
stronger restriction p > n.
Exercise 4.3 Let 1 ≤p ≤+∞, u ∈W 1,p(D), ϕ ∈C∞
0 (D). Then uϕ ∈W 1,p(D)
and Di(uϕ) = ϕDiu + uDiϕ.
Exercise 4.4 Let u ∈H 1
0 (D) and v ∈H 1(D) (or viceversa). Then

D
vDiudx = −

D
uDivdx .

50
4
Weak Derivatives and Sobolev Spaces
4.3
Exercises
Exercise 4.1 Set Xα = {v ∈L2(D) | Dαv ∈L2(D)}, where α is a multi-index.
The operator Dα : u →Dαu deﬁned in Xα is a closed operator from L2(D) to
L2(D), namely, if for um ∈Xα one has um →u in L2(D) and Dαum →wα in
L2(D) then it follows wα = Dαu.
Solution The deﬁnition of Dαum reads

D
um Dαϕdx = (−1)|α|

D
Dαum ϕdx
for each ϕ ∈C∞
0 (D). Then passing to the limit in this equality we ﬁnd

D
u Dαϕdx = (−1)|α|

D
wα ϕdx ,
hence wα = Dαu.
Exercise 4.2 Let ϕm as in (4.9) and set ψm(x) = I −1
m ϕm(x), x ∈(0, 2), where
Im =
 2
0 ϕmdx. Show that
 2
0 ψmϕdx →ϕ(1) for each ϕ ∈C∞
0 (0, 2). Repeat the
proof for each ϕ ∈C0(0, 2).
Solution Since
 2
0 ψm(x)dx = 1, we have

 2
0
ψm(x)ϕ(x)dx −ϕ(1)
 =

 2
0
ψm(x)

ϕ(x) −ϕ(1)

dx

=

 1+ 1
2m
1−1
2m
ψm(x)

ϕ(x) −ϕ(1)

dx

≤
max
|x−1|≤1
2m
|ϕ(x) −ϕ(1)|

 1+ 1
2m
1−1
2m
ψm(x)dx

=
max
|x−1|≤1
2m
|ϕ(x) −ϕ(1)| .
Since in both cases ϕ ∈C∞
0 (0, 2) and ϕ ∈C0(0, 2) we have that ϕ is uniformly
continuous in each compact subset K of (0, 2), the thesis follows with the same
argument.
Exercise 4.3 Let 1 ≤p ≤+∞, u ∈W 1,p(D), ϕ ∈C∞
0 (D). Then uϕ ∈W 1,p(D)
and Di(uϕ) = ϕDiu + uDiϕ.

4.3
Exercises
51
Solution Clearly uϕ, ϕDiu, uDiϕ ∈Lp(D) (u and Diu belong to Lp(D), and ϕ
is smooth...). Thus it is enough to show that Di(uϕ) = ϕDiu + uDiϕ. We have,
for ψ ∈C∞
0 (D)

D
uϕDiψdx =

D
uDi(ϕψ)dx −

D
uψDiϕdx
= −

D
(Diu)ϕψdx −

D
uDiϕψdx
= −

D
[ϕDiu + uDiϕ] ψdx ,
as ϕψ ∈C∞
0 (D).
Exercise 4.4 Let u ∈H 1
0 (D) and v ∈H 1(D) (or viceversa). Then

D
vDiudx = −

D
uDivdx .
Solution Take uk →u in H 1(D) with uk ∈C∞
0 (D). The result is true for uk, v
and then we just pass to the limit to conclude the proof.

Chapter 5
Weak Formulation of Elliptic PDEs
In this chapter we want to derive and analyze the weak formulation of the boundary
value problems associated to the (uniformly) elliptic operator
Lw = −
n

i,j=1
Di(aijDjw) +
n

i=1
biDiw + a0w ,
(5.1)
where, as done in Sects. 2.1 and 2.2, we assume that D ⊂Rn is a bounded,
connected, open set, aij ∈L∞(D) for i, j = 1, . . . , n, bi ∈L∞(D) for i =
1, . . . , n, a0 ∈L∞(D). When considering the Robin problem, the assumptions on
the coefﬁcient are κ ∈L∞(∂D), κ ≥0 a.e. on ∂D and

∂D κdSx ̸= 0. On the
data we assume that f ∈L2(D) and g ∈L2(∂D) (Neumann and Robin problems),
g ∈L2(N) (mixed problem).
5.1
Weak Formulation of Boundary Value Problems
We have seen in Chap. 2 that a standard way for rewriting the boundary value
problem

Lu = f
in D
BC
on ∂D
is:
1. multiply the equation by a test function;
2. integrate in D;
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_5
53

54
5
Weak Formulation of Elliptic PDEs
3. reduce the problem to a more suitable form (we could say: a more balanced form)
by integrating by parts the term stemming from the principal part (using in this
computation the information given by the boundary condition).
This typically leads to a problem of the form
u ∈V : B(u, v) = F(v)
∀v ∈V
(see (2.22); see also (2.20), which has been speciﬁcally obtained taking into account
the homogeneous Dirichlet boundary condition). In order to analyze this problem by
means of tools from functional analysis, we have also clariﬁed in Chap. 2 that the
inﬁnite dimensional vector space V must be a Hilbert space.
Our aim now is to make precise this procedure for all the boundary value
problems we are interested in: Dirichlet (homogeneous case), Neumann, mixed
(homogeneous case on D), Robin.
Dirichlet BC.
In this case the problem is

Lu = f
in D
u = 0
on ∂D .
(5.2)
For the ease of the reader, we repeat here the procedure presented in Chap. 2.
This procedure is formal, namely, we are implicitly assuming that all the terms
we are going to write have a meaning. We start choosing a function v ∈C∞
0 (D),
thus satisfying v|∂D = 0, and we multiply the equation by v. Integrating over D
we obtain
−

D
n

i,j=1
Di(aijDju)vdx +

D
n

i=1
biDiuvdx +

D
a0uvdx =

D
f vdx .
Integrating by parts, we obtain
−

D
n

i,j=1
Di(aijDju)vdx =

D
n

i,j=1
aijDjuDivdx −

∂D
n

i,j=1
niaijDjuv|∂DdSx



= 0, as v|∂D= 0
=

D
n

i,j=1
aijDjuDivdx .
Thus we are left with

D
n

i,j=1
aijDjuDivdx +

D
n

i=1
biDiuvdx +

D
a0uvdx =

D
f vdx .

5.1
Weak Formulation of Boundary Value Problems
55
Up to here, as we said, this is just a formal procedure; the aim now is to check
for which choice of the space V this equation has a meaning for u, v ∈V .
If u ∈H 1(D) (thus the derivatives appearing in the equation above have to be
considered as weak derivatives) all the terms are well-deﬁned. Moreover, since
the space of test functions C∞
0 (D) is dense in the Sobolev space H 1
0 (D), it is
easy to check that by continuity we can extend this equation to test functions v ∈
H 1
0 (D). Finally, a reasonable interpretation of the boundary condition u|∂D = 0
is that u can be approximated by functions vanishing near the boundary: thus we
can require u ∈H 1
0 (D). Our last step now is clear: the Hilbert space we choose
is V = H 1
0 (D).
We observe that the original problem (5.2) has been transformed into a set of
inﬁnitely many integral equations, or, equivalently, into an equation in the inﬁnite
dimensional vector space V = H 1
0 (D).
We recall the deﬁnitions of the bilinear form
BL(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx
and the linear functional
FD(v) =

D
f vdx
(see (2.18) and (2.19)). Problem (5.2) has been therefore rewritten in the weak
form:
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(5.3)
where
B(w, v) = BL(w, v) , F(v) =

D
f vdx , V = H 1
0 (D) .
(5.4)
Neumann BC.
In this case the problem is
⎧
⎪⎪⎨
⎪⎪⎩
Lu = f
in D
n

i,j=1
niaijDju = g
on ∂D .
(5.5)
Besides conditions (2.15) on the coefﬁcients and (2.16) on the right hand side of
the equation, as already told here we also assume g ∈L2(∂D).
In this case the structure of the boundary condition is qualitatively different from
that of the Dirichlet problem. In particular, there is no longer reason to impose
to the test function v to vanish on ∂D. Thus we choose v ∈C∞(D) and we

56
5
Weak Formulation of Elliptic PDEs
multiply the differential equation by v. Proceeding formally, we integrate over D
and obtain

D
−
n

i,j=1
Di(aijDju)vdx +

D
n

i=1
biDiuvdx +

D
a0uvdx =

D
f vdx .
Integrating by parts the ﬁrst term, the following boundary integral appears:
−

∂D
n

i,j=1
niaijDjuv|∂DdSx .
(5.6)
Using the Neumann condition it can be rewritten as −

∂D gv|∂DdSx; thus we
have ﬁnally obtained

D
n

i,j=1
aijDjuDivdx +

D
n

i=1
biDiuvdx +

D
a0uvdx
=

D
f vdx +

∂D
gv|∂DdSx .
Proceeding similarly to the Dirichlet case, we can choose V equal to the closure
of C∞(D) with respect to the H 1(D)-norm. We will see in Theorem 6.3 that,
if D has a Lipschitz continuous boundary ∂D, the subspace C∞(D) is dense
in H 1(D). Thus we choose V = H 1(D), and assume that the boundary ∂D is
Lipschitz continuous (see Appendix B for a precise deﬁnition of this regularity
assumption).
Let us now give a look at the equation we have obtained. Four of its terms were
also present in the Dirichlet case, thus we already know that they have a meaning
for u ∈H 1(). The new one is 
∂D gv|∂DdSx: this needs some additional
attention. In fact, ﬁrst of all we have to show that it is possible to give a meaning
to v|∂D for v ∈H 1(D) (remember that ∂D is a set whose measure is equal to
zero...), and moreover show that it belongs to L2(∂D); secondly, if we want that
the right hand side of the equation above is bounded for v ∈H 1(D), we need
that the following inequality holds true:

∂D
v2
|∂DdSx ≤C∗

D
(v2 + |∇v|2)dx
∀v ∈H 1(D)
(5.7)
for a suitable C∗> 0. We will see in Theorem 6.5 that, for v ∈H 1(D), both
these issues have a positive answer: the value v|∂D will be called the trace of v
and (5.7) will be called the trace inequality.
Problem (5.5) has been therefore rewritten in the weak form:
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(5.8)

5.1
Weak Formulation of Boundary Value Problems
57
where
B(w, v) = BL(w, v) , F(v) =

D
f vdx +

∂D
gv|∂DdSx , V = H 1(D) .
(5.9)
Remark 5.1 The “thumb rule” for identifying which are the Dirichlet boundary
condition and the Neumann boundary condition associated to a general second order
partial differential operator L (not necessarily the elliptic operator L in (5.1)) is the
following. Multiply Lu by v, integrate in D and integrate by parts the principal
(namely, second order) terms. Some terms given by integrals on the boundary ∂D
will appear (for the operator L they are shown in (5.6)): they can be canceled either
by putting to 0 the ﬁrst order terms related to u or by putting to 0 the zero order
terms related to v. The Neumann boundary condition is expressed by the ﬁrst order
terms related to u, the Dirichlet boundary condition is expressed by the zero order
terms related to v. For the homogeneous Dirichlet boundary value problem the
boundary condition is inserted as a constraint in the deﬁnition of the variational
space V , whereas for the (non-homogeneous) Neumann boundary value problem
the boundary condition is used to give a boundary contribution to the linear and
bounded functional F(·) at the right hand side of the variational problem.
Mixed BC.
In this case the problem is
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Lu = f
in D
u = 0
on D
n

i,j=1
niaijDju = g
on N ,
(5.10)
where ∂D = D ∪N, D ∩N = ∅, and, besides (2.15) and (2.16), we assume
g ∈L2(N).
Choose as space of test functions
C∞
D(D) = {v ∈C∞(D) | v = 0 in a neighborhhood of D} .
Multiplying the differential equation by v ∈C∞
D(D) and integrating over D we
obtain

D
−
n

i,j=1
Di(aijDju)vdx +

D
n

i=1
biDiuvdx +

D
a0uvdx =

D
f vdx .

58
5
Weak Formulation of Elliptic PDEs
By proceeding as in the previous cases, integrating by parts the ﬁrst term and
using the boundary conditions we obtain

D
n

i,j=1
aijDjuDivdx +

D
n

i=1
biDiuvdx +

D
a0uvdx
=

D
f vdx +

N
gv|N dSx .
We take the space V equal to the closure in H 1(D) of C∞
D(D). It will be shown
that, if D is a Lipschitz continuous boundary, this closed subspace is H 1
D(D)
(see Sect. 6.5 for a precise deﬁnition and further details). Moreover, it will be
also possible to deﬁne the trace of v on D and on N, to show that v|D = 0,
that v|N ∈L2(N) and ﬁnally that the map from v ∈H 1
D(D) to its trace
v|N ∈L2(N) is continuous, namely, that the following trace inequality holds:

N
v2
|N dSx ≤C∗

D
(v2 + |∇v|2)dx
∀v ∈H 1
D(D)
(5.11)
for a suitable C∗> 0 (see Remark 6.7).
Problem (5.10) has been therefore rewritten in the weak form:
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(5.12)
where
B(w, v) = BL(w, v) , F(v) =

D
f vdx +

N
gv|N dSx , V = H 1
D(D) .
(5.13)
Robin BC.
In this case the problem is
⎧
⎪⎪⎨
⎪⎪⎩
Lu = f
in D
n

i,j=1
niaijDju + κu = g
on ∂D ,
(5.14)
where, besides (2.15) and (2.16), we also assume g ∈L2(∂D), κ ∈L∞(∂D),
κ ≥0 a.e. in ∂D and

∂D κdSx ̸= 0.
We choose C∞(D) as space of test functions. Multiplying the differential
equation by v ∈C∞(D) and integrating over D we obtain

D
−
n

i,j=1
Di(aijDju)vdx +

D
n

i=1
biDiuvdx +

D
a0uvdx =

D
f vdx .

5.2
Boundedness of the Bilinear Form B(·, ·) and the Linear Functional F(·)
59
Integrating by parts the ﬁrst term, the following boundary integral appears:
−

∂D
n

i,j=1
niaijDjuv|∂DdSx .
Using the Robin condition it can be written as
−

∂D
(g −κu|∂D)v|∂DdSx .
Thus we have obtained

D
n

i,j=1
aijDjuDivdx +

D
n

i=1
biDiuvdx +

D
a0uvdx +

∂D
κu|∂Dv|∂DdSx
=

D
f vdx +

∂D
gv|∂DdSx .
The results that have been used for giving a meaning to the Neumann problem
are employed also here: thus we assume that ∂D is a Lipschitz continuous
boundary, so that the trace v|∂D of v ∈H 1(D) is deﬁned in L2(∂D) and depends
continuously on v.
Problem (5.14) has been therefore rewritten in the weak form:
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(5.15)
where
B(w, v) = BL(w, v) +

∂D κw|∂Dv|∂DdSx
F(v) =

D f vdx +

∂D gv|∂DdSx , V = H 1(D) .
(5.16)
5.2
Boundedness of the Bilinear Form B(·, ·) and the Linear
Functional F(·)
For the analysis of the boundary value problems we have derived in the previous
section we want to apply the Lax–Milgram theorem 2.1. Thus, as a ﬁrst step,
we have to verify that B(·, ·) and F(·) are bounded in H 1(D). Let us remind
the assumptions on the coefﬁcients and the right hand side: aij ∈L∞(D) for
i, j = 1, . . . , n, bi ∈L∞(D) for i = 1, . . . , n, a0 ∈L∞(D), f ∈L2(D) for all the
problems, then g ∈L2(∂D) (for the Neumann and Robin problems) or g ∈L2(N)
(for the mixed problem), and ﬁnally κ ∈L∞(∂D), κ ≥0 almost everywhere on ∂D
and

∂D κdSx ̸= 0 (for the Robin problem). Finally, we have assumed that D has a
Lipschitz continuous boundary ∂D.

60
5
Weak Formulation of Elliptic PDEs
Let us denote by A = {aij}n
i,j=1 the coefﬁcient matrix of the principal part, by
∥A∥=
 n
i,j=1 a2
ij its norm, and by b = {bi}n
i=1 the vector ﬁeld describing the
ﬁrst order part of the operator L. We readily check, using the Cauchy–Schwarz
inequality in L2(D),
|BL(w, v)| =


D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx

≤sup
D
∥A∥

D
|∇w||∇v|dx + sup
D
|b|

D
|∇w||v|dx
+ sup
D
|a0|

D
|w||v|dx
≤γ ∥w∥H 1(D) ∥v∥H 1(D)
for a suitable constant γ > 0 depending on the L∞-norms of A, b and a0. Moreover,


∂D
κw|∂Dv|∂DdSx
 ≤∥κ∥L∞(∂D)∥w|∂D∥L2(∂D)∥v|∂D∥L2(∂D) ,
by the Cauchy–Schwarz inequality in L2(∂D). The trace inequality (5.7) permits
to estimate ∥w|∂D∥L2(∂D) and ∥v|∂D∥L2(∂D) in terms of ∥w∥H 1(D) and ∥v∥H 1(D),
respectively, and the boundedness of B(·, ·) is therefore proved.
Remark 5.2 Other conditions assuring boundedness of the bilinear form BL(·, ·)
can be found in Exercise 7.16, (i).
Let us come to the boundedness of the linear functional F. We have, again by the
Cauchy–Schwarz inequality,

D f vdx
 ≤∥f ∥L2(D)∥v∥L2(D) ,

∂D gv|∂DdSx
 ≤∥g∥L2(∂D)∥v|∂D∥L2(∂D)


N gv|N dSx
 ≤∥g∥L2(N)∥v|N ∥L2(N) .
The trace inequalities (5.7) and (5.11) give an estimate of ∥v|∂D∥L2(∂D) and
∥v|N ∥L2(N) in terms of ∥v∥H 1(D), and the boundedness of F(·) thus follows at
once.
5.3
Weak Coerciveness of the Bilinear Form B(·, ·)
First of all we need a new deﬁnition. Assume that V ⊂H 1(D) is a Hilbert space
with respect to the H 1(D)-scalar product.

5.3
Weak Coerciveness of the Bilinear Form B(·, ·)
61
Deﬁnition 5.1 A bilinear form B(·, ·) : V × V →R is said to be weakly coercive
in V if there exist two constants α > 0 and σ ≥0 such that
B(v, v) + σ∥v∥2
L2(D) ≥α∥v∥2
H 1(D)
∀v ∈V .
Remark 5.3 It is clearly seen that, if it possible to choose σ = 0 in this deﬁnition,
then the bilinear form B(·, ·) is coercive in H 1(D).
We consider the bilinear forms BL(·, ·) and B(·, ·) deﬁned as
BL(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx
and
B(w, v) =
⎧
⎪⎪⎨
⎪⎪⎩
BL(w, v)
for the Dirichlet, Neumann,
mixed problems
BL(w, v) +

∂D
κw|∂Dv|∂DdSx
for the Robin problem,
under the same assumptions of Sect. 5.2. Having assumed κ ≥0 it follows

∂D κv2
|∂DdSx ≥0, thus we can limit our analysis to BL(v, v). We have
BL(v, v) =

D
n

i,j=1
aijDjvDivdx



[1]
+

D
n

i=1
biDivvdx



[2]
+

D
a0v2dx .



[3]
(1) By ellipticity, for almost all x ∈D and for all η ∈Rn we have that
n

i,j=1
aij(x)ηjηi ≥α0|η|2
for some α0 > 0 .
Thus, setting η = ∇v(x) and integrating in D it follows that

D
n

i,j=1
aijDjvDivdx ≥α0

D
|∇v|2dx .

62
5
Weak Formulation of Elliptic PDEs
(2) Using the Cauchy–Schwarz inequality we ﬁnd that


D
n

i=1
biDivvdx
 ≤

D
n

i=1
|bi| |Div| |v|dx ≤∥b∥L∞(D)

D
|∇v||v|dx
≤∥b∥L∞(D)

D
|∇v|2dx
1/2 
D
v2dx
1/2
=

D
|∇v|2dx
1/2 
∥b∥2
L∞(D)

D
v2dx
1/2
.
Consider now the elementary inequality |2AB| ≤A2 +B2: from this, replacing
A by √εA and B by B/√ε, where ε > 0, we can easily derive the following
inequality
|AB| ≤ε
2A2 + B2
2ε .
Applying this we obtain


D
n

i=1
biDivvdx
 ≤ε
2

D
|∇v|2dx + 1
2ε ∥b∥2
L∞(D)

D
v2dx
and so

D
n

i=1
biDivvdx ≥−ε
2

D
|∇v|2dx −1
2ε ∥b∥2
L∞(D)

D
v2dx .
(3) We have that

D
a0v2dx ≥inf
D a0

D
v2dx .
Putting everything together and choosing ε = α0 we have
BL(v, v) ≥α0
2

D
|∇v|2dx +

inf
D a0 −
1
2α0
∥b∥2
L∞(D)
 
D
v2dx .
Therefore the following inequality holds
BL(v, v)+σ

D
v2dx ≥α0
2

D
|∇v|2dx+

σ + inf
D a0 −
1
2α0
∥b∥2
L∞(D)
 
D
v2dx .

5.3
Weak Coerciveness of the Bilinear Form B(·, ·)
63
Set μ = infD a0 −
1
2α0 ∥b∥2
L∞(D). Choosing σ as follows:
 σ = 0
if μ > 0
σ > −μ ≥0 if μ ≤0 ,
and denoting by ρ = σ + μ > 0 we ﬁnd the desired result:
BL(v, v) + σ

D
v2dx ≥α0
2

D
|∇v|2dx + ρ

D
v2dx
≥min
α0
2 , ρ
 
D

|∇v|2 + v2
dx .
(5.17)
Remark 5.4 Weak coerciveness with σ > 0 is not enough to apply the Lax–
Milgram theorem 2.1. Therefore, in this respect the result just proved is satisfactory
only when we can choose σ = 0, namely, when μ = infD a0 −
1
2α0 ∥b∥2
L∞(D) > 0.
This requires infD a0 > 0 and ∥b∥2
L∞(D) small enough. The following example
shows that for the “queen” of our operator, the Laplace operator −, this is not
satisﬁed.
Example 5.1 Consider the (homogeneous) Dirichlet boundary value problem

−u = f
in D
u = 0
on ∂D .
In this case we have b = 0 and a0 = 0, thus the condition infD a0 −
1
2α0 ∥b∥2
L∞(D) >
0 is not satisﬁed. Since
B(v, v) =

D
∇v · ∇vdx =

D
|∇v|2dx ,
to prove coerciveness we have to ﬁnd a constant α satisfying 0 < α < 1 such that
B(v, v) =

D
|∇v|2dx ≥α

D

|∇v|2 + v2
dx
∀v ∈H 1
0 (D)
or, equivalently, we have to prove that
there exists a constant CD > 0 :

D
v2dx ≤CD

D
|∇v|2dx
∀v ∈H 1
0 (D)) .
(5.18)

64
5
Weak Formulation of Elliptic PDEs
Assuming that such a constant exists, we observe that
B(v, v) =

D
|∇v|2dx = 1
2

D
|∇v|2dx + 1
2

D
|∇v|2dx
≥1
2

D
|∇v|2dx +
1
2CD

D
v2dx
≥min
1
2,
1
2CD
 
D
(v2 + |∇v|2)dx .
Inequality (5.18) is called Poincaré inequality in H 1
0 (D): we will present its proof
in Sect. 6.2. For the moment, let us note that this inequality is surely false if we can
select as function v a non-zero constant. The fact that the only constant in H 1
0 (D) is
0 opens the possibility of showing that (5.18) is indeed true.
5.4
Coerciveness of the Bilinear Form B(·, ·)
Assuming more regularity on the vector ﬁeld b and some other qualitative relations,
we want now to show that the bilinear form B(·, ·) is coercive for all the boundary
value problems we have presented.
The starting point for this analysis is the remark that in some cases we succeed
in proving the Poincaré inequality

D
v2dx ≤C∗

D
|∇v|2dx ;
this tells us that the principal part of the bilinear form can be bounded from below
by ∥v∥2
H 1(D), namely, it is coercive. Thus we have only to be careful that the other
terms, coming from b and a0, do not destroy this property.
Let us consider the term coming from the vector ﬁeld b. Assume that b ∈
W 1,∞(D) so that by the Sobolev immersion theorem 7.15 we also have b|∂D ∈
L∞(∂D) for the Neumann and Robin problems or b|N ∈L∞(N) for the mixed
problem (it is possible to require less restrictive assumptions, but the proof would
become more technical). We proceed by analyzing each boundary condition.
Dirichlet BC.
The choice of the Hilbert space is V
= H 1
0 (D), and in this
case Poincaré inequality holds (see Theorem 6.4). Since C∞
0 (D) is dense in
H 1
0 (D) we can ﬁrst suppose that v ∈C∞
0 (D). We have, by integrating by parts

5.4
Coerciveness of the Bilinear Form B(·, ·)
65
(see Exercise 4.4)

D
n

i=1
biDiv vdx =
n

i=1

D
biDi
v2
2 dx
= −

i=1

D
Dibi
v2
2 dx = −

D
1
2 div b v2dx .
By a density argument we see that this relation is also true for v ∈H 1
0 (D). Hence
we have
B(v, v) =

D
n

i,j=1
aijDjv Divdx +

D
n

i=1
biDiv vdx +

D
a0v2dx
≥α0

D
|∇v|2dx +

D

a0 −1
2 div b

v2dx
and coerciveness in H 1
0 (D) is guaranteed by the Poincaré inequality and
assuming
a0 −1
2 div b ≥0
in D .
Neumann BC.
The Hilbert space in this case is V = H 1(D). Since in this space
Poincaré inequality doesn’t hold (e.g., consider v = 1), we could be led to modify
this choice. Let us start, as before, by looking at the term coming from the ﬁrst
order part of the operator. We want to perform an integration by parts, which
will show up an integral on ∂D involving the trace v|∂D of v on ∂D. To give a
meaning at this term we assume that ∂D is a Lipschitz continuous boundary, thus
the space C∞(D) is dense in H 1(D) and the trace is deﬁned (see Theorem 6.5).
We can ﬁrst assume that v ∈C∞(D). By integration by parts (see Exercise 6.7)
we have

D
n

i=1
biDiv vdx =
n

i=1

D
biDi
v2
2 dx
= −

i=1

D
Dibi
v2
2 dx +
n

i=1

∂D
bi|∂Dni
1
2v2
|∂DdSx
= −

D
1
2 div b v2dx +

∂D
1
2 b|∂D · nv2
|∂DdSx .

66
5
Weak Formulation of Elliptic PDEs
By a density argument this relation is true also for v ∈H 1(D). In conclusion,
we easily see that sufﬁcient conditions for coerciveness are
a0 −1
2 div b ≥δ > 0 in D , b|∂D · n ≥0 on ∂D .
However, these conditions are not satisfactory, as, for instance, the Laplace
operator − does not satisfy them. On the other hand this is not a surprise,
as for the Neumann problem associated to the Laplace operator we cannot have
a unique solution, as, if u is a solution, also u + c with c ∈R is a solution, and
therefore the assumptions in the Lax–Milgram theorem 2.1 cannot be satisﬁed.
(Remember that Lax–Milgram theorem guarantees the existence and uniqueness
of the solution.)
In order to devise a weak problem for which the associated bilinear form is
coercive, the idea is to deﬁne a new Hilbert space that doesn’t contain constants
different from 0. A space with this property is given by
H 1
∗(D) =

v ∈H 1(D)


D
vdx = 0
#
.
(5.19)
This is a closed subspace of H 1(D) (indeed if vk →v in H 1(D) and

D vkdx =
0, then

D vdx = 0: the quantity |

D(vk −v)dx| is estimated by ∥vk −v∥L2(D)
by the Cauchy–Schwarz inequality), therefore it is a Hilbert space with respect
to the same scalar product. In this space the Poincaré inequality holds (see
Theorem 6.10) and therefore we can prove the coerciveness of B(·, ·) in H 1
∗(D)
by following the same procedure we have employed in the case of the Dirichlet
boundary condition. More precisely, sufﬁcient conditions that guarantee the
coerciveness of B(·, ·) are
a0 −1
2 div b ≥0 in D , b|∂D · n ≥0 on ∂D .
Mixed BC.
The Hilbert space in this case is V = H 1
D(D), and we will see that
in this space the Poincaré inequality holds (see Theorem 6.11). Therefore we can
proceed exactly as in the case of the Neumann condition with the space H 1
∗(D)
and we conclude that sufﬁcient conditions that guarantee the coerciveness of
B(·, ·) are
a0 −1
2 div b ≥0 in D , b|N · n ≥0 on N .

5.4
Coerciveness of the Bilinear Form B(·, ·)
67
Robin BC.
The Hilbert space in this case is V = H 1(D), and the bilinear form
is given by
B(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx
+

D
a0wvdx +

∂D
κw|∂Dv|∂DdSx ,
where κ is a non-negative function deﬁned on ∂D. By performing an integration
by parts in the ﬁrst order term as in the Neumann case we have that
B(v, v) ≥α0

D
|∇v|2dx +

D

a0 −1
2 div b

v2dx
+

∂D
1
2 b|∂D · n + κ

v2
|∂DdSx
= α0

D
|∇v|2dx +

∂D
α−1
0 κv2
|∂DdSx

+

D

a0 −1
2 div b

v2dx
+

∂D
1
2 b|∂D · nv2
|∂DdSx .
We assume that
a0 −1
2 div b ≥0 in D , b|∂D · n ≥0 on ∂D ,
and we note that the function q = α−1
0 κ satisﬁes q ≥0 on ∂D and

∂D qdSx ̸=
0, thus we can apply the Poincaré-type inequality (see Theorem 6.12). In
conclusion we are left with
B(v, v) ≥α0

D
|∇v|2dx +

∂D
α−1
0 κv2
|∂DdSx

= α0
2

D
|∇v|2dx +

∂D
α−1
0 κv2
|∂DdSx

+ α0
2

D
|∇v|2dx +

∂D
α−1
0 κv2
|∂DdSx

≥α0
2

D
|∇v|2dx +

∂D
α−1
0 κv2
|∂DdSx

+ α0
2C∗

D
v2dx

68
5
Weak Formulation of Elliptic PDEs
≥α0
2

D
|∇v|2dx + α0
2C∗

D
v2dx
≥min
α0
2 , α0
2C∗
 
D
v2dx +

D
|∇v|2dx

.
Exercise 5.1 Show that in all cases coerciveness is satisﬁed even if the assumption
a0 −1
2 div b ≥0 in D is weakened to a0 −1
2 div b ≥−ν in D for a constant ν > 0
small enough.
Remark 5.5 Other conditions assuring coerciveness of the bilinear form BL(·, ·)
can be found in Exercise 7.16, (ii).
5.5
Interpretation of the Weak Problems
We want to clarify which is the “strong” interpretation of the weak problems we
have presented up to now. To this aim, we ﬁrst need a deﬁnition.
Deﬁnition 5.2 If we have qi ∈L1
loc(D), i = 1, . . ., n, we say that w ∈L1
loc(D) is
the weak divergence of q = (q1, . . . , qn) if

D
n

i=1
qiDiϕdx = −

D
wϕdx
∀ϕ ∈C∞
0 (D) .
Remark 5.6 If we know that the weak derivatives Diqi exist, for each i = 1, . . . , n,
then clearly w = n
i=1 Diqi.
Let us start our discussion from a simple example.
Example 5.2 Suppose we have found the solution u ∈H 1
0 (D) of

D
∇u · ∇vdx =

D
f vdx
∀v ∈H 1
0 (D) ,
where f ∈L2(D). What have we solved?
We can take ϕ ∈C∞
0 (D) ⊂H 1
0 (D) and we get

D
n

i=1
DiuDiϕdx =

D
f ϕdx ,
thus from the deﬁnition above, with qi = Diu ∈L2(D), we obtain that
−div∇u = f
in D ,

5.5
Interpretation of the Weak Problems
69
where div is the weak divergence and ∇is the weak gradient. Thus, in this weak
sense, −u = f in D, where  is the weak Laplace operator.
This interpretation is based on the fact that C∞
0 (D) ⊂V
= H 1
0 (D), the
variational space where we have solved the problem. When considering the mixed
problem, we have V = H 1
D(D), and again C∞
0 (D) ⊂V . For the Robin problem,
we have V = H 1(D), and C∞
0 (D) ⊂V .
A difference comes for the Neumann problem for the Laplace operator, for the
weak formulation in which we have chosen
V = H 1
∗(D) =

v ∈H 1(D)


D
vdx = 0
#
,
with the aim of obtaining the Poincaré inequality in this space.
This time C∞
0 (D) ̸⊂V , thus the interpretation in this case needs some care. Let
us write the weak problem:
u ∈H 1
∗(D) :

D
∇u · ∇vdx =

D
f vdx +

∂D
gv|∂DdSx
∀v ∈H 1
∗(D) .
Take a test function w ∈H 1(D), namely, without the restriction

D wdx = 0. Then
we deﬁne
v = w −wD ,
wD =
1
meas(D)

D
wdx .
Then v ∈H 1
∗(D), and we can use it as a test function. We have ∇w = ∇v, thus for
each w ∈H 1(D) we have

D
∇u · ∇wdx =

D
∇u · ∇vdx =

D
f vdx +

∂D
gv|∂DdSx
=

D
f (w −wD)dx +

∂D
g(w|∂D −wD)dSx
=

D
f wdx −wD

D
f dx +

∂D
gw|∂DdSx −wD

∂D
gdSx
=

D
f wdx +

∂D
gw|∂DdSx
−

D
f dx +

∂D
gdSx

1
meas(D)

D
wdx
=

D
$
f −
1
meas(D)

D
f dx +

∂D
gdSx
%
wdx
+

∂D
gw|∂DdSx .
(5.20)

70
5
Weak Formulation of Elliptic PDEs
Taking in particular w ∈C∞
0 (D), it follows
−u = f −
1
meas(D)

D
f dx +

∂D
gdSx

in D .
If we have p ∈H 1(D), q ∈H 1(D) with −q ∈L2(D), by approximation we
have the integration by parts formula

D
∇q · ∇pdx = −

D
q pdx +

∂D
(∇q · n)p|∂DdSx .
The last term should be clariﬁed, indeed it is not obvious that there is a trace for
∇q · n. However, we do not deal here with this question, and we go on somehow
formally. Let us come back now to the choice of a generic w ∈H 1(D): taking
p = w and q = u in (5.20) we thus ﬁnd

∂D ∇u · n w|∂DdSx +

D 

(−u)wdx =

D ∇u · ∇wdx
=

D
$
(((((((((((((((
f −
1
meas(D)

D f dx +

∂D gdSx
%
wdx +

∂D gw|∂DdSx .
As a consequence

∂D
(∇u · n −g)w|∂DdSx = 0
∀w ∈H 1(D) ,
which is a weak form of ∇u · n = g on ∂D. In conclusion, the “strong” form of the
weak problem we have solved reads

−u = f −
1
meas(D)

D f dx + 
∂D gdSx

in D
∇u · n = g
on ∂D .
(5.21)
This problem has been solved for any f ∈L2(D) and g ∈L2(∂D); but it is not the
Neumann problem we had in mind, namely

−u = f
in D
∇u · n = g
on ∂D .
(5.22)
On the other hand, we know by the divergence theorem that this last problem cannot
be solved unless the following compatibility condition is satisﬁed:

D
f dx +

∂D
gdSx = 0 .

5.5
Interpretation of the Weak Problems
71
In fact

D
f dx = −

D
udx = −

D
div∇udx = −

∂D
∇u · ndSx = −

∂D
gdSx .
In conclusion, if

D f dx +

∂D gdSx = 0 problem (5.21) becomes our original
problem, and we have found a unique solution in H 1
∗(D), namely, with

D udx = 0.
Remark 5.7 Why is problem (5.21) always solvable? It is a Neumann problem,
therefore the compatibility condition on the data at the right hand side must be
satisﬁed. The new right hand side in D is
˜f = f −
1
meas(D)

D
f dx +

∂D
gdSx

.
Take its integral in D: it holds

D
$
f −
1
meas(D)

D
f dx +

∂D
gdSx
%
dx
=

D
f dx −
$
D
f dx +

∂D
gdSx
%
= −

∂D
gdSx .
Thus

D
˜f dx +

∂D
gdSx = 0 ,
and the compatibility condition for the Neumann problem (5.21) is satisﬁed.
Exercise 5.2 Taking hint from the deﬁnition of the weak divergence in Deﬁni-
tion 5.2, give the deﬁnition of the weak curl of a vector ﬁeld q ∈(L1
loc(D))3,
D ⊂R3.
Exercise 5.3
(i) Show that there exists a unique solution of the weak problem
ﬁnd u ∈H 1
∗(D) :

D
∇u · ∇vdx +

∂D
u|∂Dv|∂DdSx
=

D
f vdx +

∂D
gv|∂DdSx
∀v ∈H 1
∗(D) ,
where H 1
∗(D) is deﬁned in (5.19).
(ii) Devise the “strong” interpretation of the weak problem above.

72
5
Weak Formulation of Elliptic PDEs
5.6
Exercises
Exercise 5.1 Show that in all cases coerciveness is satisﬁed even if the assumption
a0 −1
2 div b ≥0 in D is weakened to a0 −1
2 div b ≥−ν in D for a constant ν > 0
small enough.
Solution Let us consider the case of the Dirichlet boundary condition. We have, by
using the Poincaré inequality 5.18 and proceeding as before,
B(v, v) ≥α0

D
|∇v|2dx +

D
(a0 −1
2 div b)v2dx
≥α0
2

D
|∇v|2dx + α0
2CD

D
v2dx −ν

D
v2dx
= α0
2

D
|∇v|2dx +
 α0
2CD
−ν
 
D
v2dx ,
therefore coerciveness holds provided that ν <
α0
2CD . The proof in the other cases
is similar, using the result provided by the Poincaré inequality in Theorem 6.10
(Neumann problem) or in Theorem 6.11 (mixed problem), or the Poincaré-type
inequality in Theorem 6.12 (Robin problem).
Exercise 5.2 Taking hint from the deﬁnition of the weak divergence in Deﬁni-
tion 5.2, give the deﬁnition of the weak curl of a vector ﬁeld q ∈(L1
loc(D))3,
D ⊂R3.
Solution Having in mind the integration-by-parts formula (see Theorem C.7)

D
curl q · vdx =

D
q · curl vdx
valid for q ∈C1(D), v ∈C∞
0 (D), the weak curl of q is a vector ﬁeld ω ∈
(L1
loc(D))3 such that

D
ω · vdx =

D
w · curl vdx
for each v ∈C∞
0 (D).
Exercise 5.3
(i) Show that there exists a unique solution of the weak problem
ﬁnd u ∈H 1
∗(D) :

D
∇u · ∇vdx +

∂D
u|∂Dv|∂DdSx
=

D
f vdx +

∂D
gv|∂DdSx
∀v ∈H 1
∗(D) ,
where H 1
∗(D) is deﬁned in (5.19).
(ii) Devise the “strong” interpretation of the weak problem above.

5.6
Exercises
73
Solution
(i) The bilinear form

D
∇w · ∇vdx
is coercive in H 1
∗(D) (see Theorem 6.10), and

∂D v2
|∂DdSx ≥0. Thus Lax–
Milgram theorem 2.1 guarantees existence and uniqueness of the weak solution.
(ii) As in Sect. 5.5, take a test function w ∈H 1(D) and deﬁne v = w −wD, where
wD =
1
meas(D)

D wdx. Then v ∈H 1
∗(D), and we can use it as a test function,
obtaining

D
∇u · ∇wdx +

∂D
u|∂D(w|∂D −wD)dSx
=

D
f (w −wD)dx +

∂D
g(w|∂D −wD)dSx ,
which can be rewritten as

D
∇u · ∇wdx −
1
meas(D)

D

∂D
u|∂DdSx

wdx +

∂D
u|∂Dw|∂DdSx
=

D
f wdx +

∂D
gw|∂DdSx −
1
meas(D)

D

D
f dx +

∂D
gdSx

wdx .
Thus, following the procedure in Sect. 5.5, we obtain the equation
−u −
1
meas(D)

∂D
u|∂DdSx = f −
1
meas(D)

D
f dx +

∂D
gdSx

in D ,
and the boundary condition
∂u
∂n + u|∂D = g
on ∂D ;
clearly, the solution u also satisﬁes the constraint

D udx = 0.
Exercise 5.4
(i) Find ω ∈N⊥, ω ̸= 0, where N ⊂V = L2(D) is deﬁned as in (3.2) and ⊥
means orthogonality with respect to the scalar product in (w, v)V =

D wvdx.
Compare with Example 3.6.
(ii) Find ω ∈N⊥, ω ̸= 0, where N ⊂V = H 1(D) is deﬁned as in (3.2) and ⊥
means orthogonality with respect to the scalar product in ((w, v))V =

D(wv +
∇w · ∇v)dx. Compare with Example 3.6.

74
5
Weak Formulation of Elliptic PDEs
Solution
(i) We simply take ω = 1. From an abstract point of view, it is the solution ω ∈
L2(D) of the problem
(ω, v)V =

D
vdx
∀v ∈L2(D) ,
whose existence is assured by the Riesz representation theorem . The difference
with Example 3.6 is that now we are working in the Hilbert space L2(D), so
that the Riesz representation theorem holds.
(ii) Similarly to what done in (i), we take the solution ω ∈H 1(D) of the problem
((ω, v))V =

D
vdx
∀v ∈H 1(D) .
The well-posedness follows from the Riesz representation Theorem 3.1, and
ω ∈N⊥. Again, the difference with Example 3.6 is that H 1(D) is a Hilbert
space, thus the Riesz representation theorem holds.
Exercise 5.5
(i) Devise a variational formulation for the homogeneous Dirichlet boundary
value problem associated to the operator Lw = −n
i,j=1 Di(aijDjw) +
n
i=1 biDiw + n
i=1 Di(ciw) + a0w, where ci ∈L∞(D), i = 1, . . . , n.
(ii) Determine a sufﬁcient condition on the coefﬁcients ci ensuring existence and
uniqueness of the solution.
Solution
(i) Assuming w, v ∈H 1
0 (D), a formal integration by parts yields the bilinear form
ˆBL(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiw vdx
−

D
n

i=1
ciwDivdx +

D
a0wvdx ,
that is deﬁned and bounded in H 1
0 (D) × H 1
0 (D) under the sole assumption
ci ∈L∞(D), i = 1, . . . , n. The variational formulation is thus
u ∈H 1
0 (D) :
ˆBL(u, v) =

D
f vdx
∀v ∈H 1
0 (D) .

5.6
Exercises
75
(ii) Taking w = v, the two terms coming from the ﬁrst order terms of the operator
become

D
n

i=1
biDiv vdx −

D
n

i=1
civDivdx =

D
n

i=1
(bi −ci)Div vdx .
Therefore, proceeding as in Sect. 5.4, coerciveness is achieved provided that
a0 −1
2div(b −c) ≥0 in D.
Exercise 5.6 The physical conservation principles used to derive the time-indepen-
dent linear Stokes system lead to the problem
 −ν n
i=1 Di(Diuj + Djui) + Djp = fj in D
div u = 0
in D ,
(5.23)
for ν > 0 (viscosity).
(i) Show that for a smooth solution u this problem can be rewritten as
 −νu + ∇p = f in D
div u = 0
in D .
(5.24)
(ii) Devise a variational formulation for the homogeneous Dirichlet boundary
value problems associated to (5.23) and associated to (5.24), and show that
these two variational formulations are equivalent.
(iii) Devise the variational formulation for the Neumann boundary value problem
associated to (5.23), and determine the strong form of the Neumann boundary
condition.
(iv) Devise the variational formulation for the Neumann boundary value problem
associated to (5.24), and determine the strong form of the Neumann boundary
condition.
(v) Compare the two Neumann boundary conditions in (iii) and (iv), and show that
they are not equivalent.
Solution
(i) From the relation DiDjui = DjDiui (that is valid for smooth functions) it
follows
−ν
n

i=1
Di(Diuj + Djui) = −νuj −νDjdiv u ,
thus using the second equation in (5.23) the result follows.

76
5
Weak Formulation of Elliptic PDEs
(ii) Taking the scalar product of (5.23) by a vector ﬁeld v, integrating in D and
integrating by parts we readily ﬁnd

D
n

j=1
fjvjdx =

D
n

j=1
&
−ν
n

i=1
Di(Diuj + Djui) + Djp
'
vjdx
= ν

D
n

i,j=1
(Diuj + Djui)Divjdx −

D
p div vdx
−ν

∂D
n

i,j=1
(Diuj + Djui)nivj|∂DdSx +

∂D
p v|∂D · ndSx .
(5.25)
For the homogeneous Dirichlet boundary value problem we assume v ∈V =
{v ∈H 1
0 (D))n | div v = 0 in D}, thus the term

D p div vdx and the boundary
terms disappear and we are left with
u ∈V : ν

D
n

i,j=1
(Diuj + Djui)Divjdx =

D
n

j=1
fjvjdx
∀v ∈V .
Repeating the same procedure for problem (5.24) we ﬁnd

D
n

j=1
fjvjdx =

D
n

j=1
&
−ν
n

i=1
DiDiuj + Djp
'
vjdx
= ν

D
n

i,j=1
DiujDivjdx −

D
p div vdx
−ν

∂D
n

i,j=1
Diujnivj|∂DdSx +

∂D
p v|∂D · ndSx ,
(5.26)
and the variational formulation
u ∈V : ν

D
n

i,j=1
DiujDivjdx =

D
n

j=1
fjvjdx
∀v ∈V .
The
two
formulations are
equivalent as

D
n
i,j=1 DjuiDivjdx
=

D div u div vdx. In fact, by a density argument we can suppose v ∈C∞
0 (D):
thus

D
n

i,j=1
DjuiDivjdx = −

D
n

i,j=1
uiDjDivjdx
= −

D
n

i,j=1
uiDiDjvjdx =

D
n

i,j=1
DiuiDjvjdx .

5.6
Exercises
77
(iii) Proceeding as in (ii) we obtain (5.25). The boundary terms
−ν

∂D
n

i,j=1
(Diuj + Djui)nivj|∂DdSx +

∂D
p v|∂D · ndSx
can be rewritten as

∂D
n

i,j=1
[−ν(Diuj + Djui) + pδij]nivj|∂DdSx ,
where δij is the Kronecker symbol, and imposing the condition
ν
n

i=1
(Diuj + Djui)ni −pnj = gj , j = 1, . . . , n ,
(5.27)
leads to the variational formulation
u ∈W : ν

D
n

i,j=1
(Diuj + Djui)Divjdx
=

D
n

j=1
fjvjdx +

∂D
n

j=1
gjvj|∂DdSx
∀v ∈W,
where W = {v ∈(H 1(D))n | div v = 0 in D}.
The strong form of the Neumann boundary condition (see Remark 5.1) is
thus given by (5.27).
(iv) Proceeding as in (ii) we obtain (5.26). The boundary terms
−ν

∂D
n

i,j=1
Diujnivj|∂DdSx +

∂D
p v|∂D · ndSx
can be rewritten as

∂D
n

i,j=1
(−νDiuj + pδij)nivj|∂DdSx ,
where δij is the Kronecker symbol, and imposing the condition
ν
n

i=1
Diujni −pnj = gj , j = 1, . . . , n .
(5.28)

78
5
Weak Formulation of Elliptic PDEs
leads to the variational formulation
u ∈W : ν

D
n

i,j=1
DiujDivjdx =

D
n

j=1
fjvjdx+

∂D
n

j=1
gjvj|∂DdSx
∀v ∈W ,
where W = {v ∈(H 1(D))n | div v = 0 in D}.
The strong form of the Neumann boundary condition (see Remark 5.1) is
thus given by (5.28).
(v) The two Neumann boundary conditions are different due to the term
n
i=1 Djuini, which is not present in (5.28). Anyway, there are divergence
free vector ﬁelds for which this term is not vanishing, as, for instance,
v(x1, x2) = (x1, −x2) on the ﬂat boundary {(x1, x2) ∈R2 | x2 = 0}. In
this case we have n = (0, 1) and
n

i=1
D1uini = 0 ,
n

i=1
D2uini = −1 .
Exercise 5.7
(i) Devise a variational formulation for the homogeneous Dirichlet boundary value
problem associated to the linear elasticity operator −μ−ν∇div, μ > 0, ν > 0
(Lamé coefﬁcients).
(ii) Show its well-posedness.
Solution
(i) In components, the equation −μu −ν∇div u = f can be rewritten as
−μ
n

i=1
DiDiuj −νDjdiv u = fj , j = 1, . . ., n ;
thus, multiplying by vj ∈H 1
0 (D), adding over j = 1, . . . , n, integrating in D
and integrating by parts we ﬁnd:

D
n

j=1
fjvjdx =

D
n

j=1

−μ
n

i=1
DiDiuj −νDjdiv u

vjdx
=

D
μ
n

i,j=1
DiujDivj + ν div u div vdx ,

5.6
Exercises
79
which leads to the variational formulation
u ∈(H 1
0 (D))n :

D
μ
n

i,j=1
DiujDivj + ν div u div vdx
=

D
n

j=1
fjvjdx
∀v ∈(H 1
0 (D))n .
(ii) Since

D ν(div v)2dx ≥0, well-posedness follows at once by the Poincaré
inequality in H 1
0 (D) (see Theorem 6.4)) and Lax–Milgram theorem 2.1.
Exercise 5.8
(i) Devise a variational formulation for the homogeneous Dirichlet boundary
value problem and for the Neumann boundary value problem associated to the
operator curl curl + αI.
(ii) Show their well-posedness.
Solution
(i) Take the scalar product of the equation curl curl u + αu = f by v, integrate in
D and integrate by parts: taking into account Theorem C.7 we ﬁnd

D
f · vdx =

D
(curl curl u + αu) · vdx
=

D
(curlu · curl v + αu · v)dx +

∂D
n × curl u · vdSx .
Since on the boundary it holds v = (v · n)n + n × v × n, the boundary term

∂D n × curl u · vdSx can be rewritten as

∂D n × curl u · (n × v × n)dSx. As
explained in Remark 5.1, the Neumann boundary condition is thus given by
curl u × n = g, with g · n = 0, while the homogeneous Dirichlet boundary
condition is given by n × v × n = 0, or, equivalently, v × n = 0.
The variational formulations are the following: for the Neumann problem
u ∈H(curl; D) :

D
(curl u · curl v + αu · v)dx
=

D
f · vdx +

∂D
g · (n × v × n)dSx
∀v ∈H(curl; D) ,
where H(curl; D) = {v ∈(L2(D))3 | curl v ∈(L2(D))3}, endowed with the
scalar product
(w, v)curl =

D
(curlw · curl v + w · v)dx

80
5
Weak Formulation of Elliptic PDEs
(the curl being intended in the weak sense), and for the homogeneous Dirichlet
problem
u ∈H0(curl; D) :

D
(curl u·curl v+αu·v)dx =

D
f ·vdx
∀v ∈H0(curl; D) ,
where H0(curl; D) = {v ∈H(curl; D) | v × n = 0 on ∂D}.
(ii) The well-posedness of the two problems is easily proved, as the bilinear form

D(curl u · curl v + αu · v)dx deﬁnes a scalar product which is equivalent to
(w, v)curl. Thus it is enough to apply the Riesz representation Theorem 3.1.
[Indeed, here we are putting under the carpet some technical problems (that have
a similar structure with those we had to face for the elliptic operator L):
•
Is H(curl; D), endowed with the scalar product (·, ·)curl, a Hilbert space? (For
this, an easy proof of the positive answer can be obtained by mimicking what is
done in Exercise 8.4.)
•
Have the tangential component n×v×n and the tangential trace v×n a meaning
on ∂D for v ∈H(curl; D)?
•
Is the linear map v →v × n bounded from H(curl; D) to a suitable tangential
trace space (so that H0(curl; D) is a closed subspace of H(curl; D), therefore a
Hilbert space)?
•
What is the real meaning of the term

∂D g · (n × v × n)dSx? Namely, is it an
integral?
•
Which is the required regularity of the Neumann datum g?
We know all the answers (and for the ﬁrst three questions they are positive),
but it is not completely straightforward to obtain them... for these issues, see, e.g.,
Monk [16, Chapters 3 and 5].]
Exercise 5.9
(i) Devise a variational formulation for the homogeneous Dirichlet boundary
value problem and for the Neumann boundary value problem associated to the
operator −∇div + αI.
(ii) Show their well-posedness.
Solution
(i) As in the previous exercise, take the scalar product of the equation −∇div u +
αu = f by v, integrate in D and integrate by parts: taking into account
Theorem C.6 we ﬁnd

D
f ·vdx =

D
(−∇div u+αu)·vdx =

D
(div u div v+αu·v)dx−

∂D
div u n·vdSx .
As explained in Remark 5.1, the Neumann boundary condition is thus given by
div u = g, while the homogeneous Dirichlet boundary condition is given by
v · n = 0.

5.6
Exercises
81
The variational formulations are the following: for the Neumann problem
u ∈H(div; D) :

D
(div u div v + αu · v)dx
=

D
f · vdx +

∂D
g v · ndSx
∀v ∈H(div; D) ,
where H(div; D) = {v ∈(L2(D))n | div v ∈L2(D)}, endowed with the scalar
product
(w, v)div =

D
(div w div v + w · v)dx
(the divergence being intended in the weak sense), and for the homogeneous
Dirichlet problem
u ∈H0(div; D) :

D
(div u div v+αu·v)dx =

D
f ·vdx
∀v ∈H0(div; D) ,
where H0(div; D) = {v ∈H(div; D) | v · n = 0 on ∂D}.
(ii) The well-posedness of the two problems is trivial, as the bilinear form

D(div u div v + αu · v)dx deﬁnes a scalar product which is equivalent to
(w, v)div. Thus it is enough to apply the Riesz representation Theorem 3.1.
[As in the previous exercise, here there are some technical problems:
•
Is H(div; D), endowed with the scalar product (·, ·)div, a Hilbert space? (The
positive answer to this question is in Exercise 8.4.)
•
Has the normal component v · n a meaning on ∂D for v ∈H(div; D)?
•
Is the linear map v →v·n bounded from H(div; D) to a suitable tangential trace
space (so that H0(div; D) is a closed subspace of H(div; D), therefore a Hilbert
space)?
•
What is the real meaning of the term

∂D g v · ndSx? Namely, is it an integral?
•
Which is the required regularity of the Neumann datum g?
Again, we know all the answers (and for the ﬁrst three questions they are
positive): see, e.g., Monk [16, Chapters 3 and 5].]

Chapter 6
Technical Results
This chapter contains some technical results that have been frequently used in
the previous sections: strictly speaking, if we had followed a “chronological”
presentation, we should have proved these results before. We preferred to adopt
a description without lateral interruptions, though it is quite clear that without these
technical results the general ideas behind weak formulations would not have reached
the desired end.
The following sections are devoted to approximation in Sobolev spaces, to
the Poincaré and trace inequalities, to compactness results in H 1(D) (the Rellich
theorem), and to the du Bois-Reymond lemma. An “obvious” result assuring that
if in a connected open set D the weak gradient of a function f vanishes then f is
constant is also presented.
6.1
Approximation Results
Let D ⊂Rn be a bounded, connected, open set.
Theorem 6.1 Take u ∈W k,p(D), where k is a non-negative integer and 1 ≤p <
+∞. Set
Dε = {x ∈D | dist(x, ∂D) > ε} .
Then there exists a sequence uε ∈C∞(Dε) with uε →u in W k,p
loc (D) as ε →0.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_6
83

84
6
Technical Results
Fig. 6.1 The graph of the
function η in (6.1)
−2
−1
1
2
0.5
x
y
Proof We use the so-called molliﬁers, introduced and named by Kurt O. Friedrichs
[7] (earlier versions of them can be found in some seminal papers by Jean Leray
[13] and Sergei L. Sobolev [19]). To deﬁne them let us consider the function
η(x) =
⎧
⎨
⎩
c0 exp

−
1
1−|x|2

if |x| < 1
0
if |x| ≥1 ,
(6.1)
where c0 is such that

Rn ηdx = 1. In the one-dimensional case the graph of η is
drawn in Fig. 6.1.
For every ε > 0 set
ηε(x) = 1
εn η
x
ε

.
It is known that if u ∈Lp
loc(D) then the molliﬁer uε deﬁned in Dε as
uε(x) = (ηε ∗u)(x) =

D
ηε(x −y)u(y)dy
belongs to C∞(Dε) and converges to u in Lp
loc(D) (see, e.g., Evans [6, Theorem 6,
pp. 630–631]). We need to prove that Dαuε →Dαu in Lp
loc(D). To this aim, it is
sufﬁcient to show that
Dαuε = ηε ∗Dαu ,
that is, the ordinary αth-partial derivative of the smooth function uε is the ε-
molliﬁcation of the αth-weak partial derivative of u. To conﬁrm this, we compute
for x ∈Dε
Dαuε(x) =

D
Dα
xηε(x −y)u(y)dy = (−1)|α|

D
Dα
yηε(x −y)u(y)dy ,

6.1
Approximation Results
85
where the results comes from the fact that any derivative with respect to x is the
opposite of the correspondent derivative with respect to y. For ﬁxed x ∈Dε, the
function φ(y) = ηε(x −y) belongs to C∞
0 (D), because its support is given by {y ∈
Rn | |y −x| ≤ε}. Consequently, the deﬁnition of the αth-weak partial derivative
implies:

D
Dα
yηε(x −y)u(y)dy = (−1)|α|

D
ηε(x −y)Dαu(y)dy .
The proof is thus complete, as (−1)|α|(−1)|α| = 1.
⊓⊔
Exercise 6.1 Prove that H 1
0 (Rn) = H 1(Rn).
We now ask when it is possible to approximate a given function u ∈W k,p(D) by
functions belonging to C∞(D). Such an approximation requires some conditions
on the regularity of the boundary ∂D. We start with an extension result.
Theorem 6.2 (Extension Result in W 1,p(D)) Let D be a bounded, connected,
open subset of Rn with Lipschitz continuous boundary ∂D. Let 1 ≤p < +∞
and k ≥1, and let Q a bounded, connected, open subset with D ⊂⊂Q. Then there
exists a linear and bounded operator
E : W k,p(D) →W k,p(Rn)
such that
(i) Eu|D = u a.e. in D;
(ii) supp(Eu) ⊂⊂Q.
Proof We only present an idea of the proof, in the case k = 1. As a ﬁrst step we
consider a ﬂat boundary. Set BR,+ = {ξ ∈Rn | |ξ| < R, ξn > 0} and BR,−= {ξ ∈
Rn | |ξ| < R, ξn < 0}, and consider w ∈W 1,p(BR,+). We set, by reﬂection,
E−w(x′, xn) =

w(x′, xn)
if x ∈BR,+
w(x′, −xn)
if x ∈BR,−,
having set x = (x′, xn), x′ = (x1, . . . , xn−1). As shown in Exercise 6.8, we see that
E−w ∈W 1,p(BR).
Let us consider now a general domain D and u ∈W 1,p(D). As in Theorem 6.7
we can cover the domain D by a ﬁnite union of open balls Bs, s = 1, . . . , M,
each one centered at a point xs ∈∂D, plus an internal open set B0 (the covering is
ﬁnite as D is a closed and bounded set, therefore a compact set in Rn). Consider a
partition of unity ζs associated to the covering Bs of D (in particular, the support
of ζs is a compact set in Bs: see Appendix A). The assumption on the regularity
of the boundary tells us that there is a ﬁnite set of local charts ψs, s = 1, . . . , M,
bijective Lipschitz continuous maps from Bs onto BR = {ξ ∈Rn | |ξ| < R}, with

86
6
Technical Results
the inverse map ψ−1
s
that is Lipschitz continuous, and such that Bs ∩D is mapped
onto BR,+. The functions (ζsu)◦ψ−1
s
belong W 1,p(BR,+) (with compact support in
BR,+∩BR). We can thus apply the reﬂection result obtained above, and we construct
the function E−((ζsu) ◦ψ−1
s
) belonging to W 1,p(BR) (with compact support in
BR). Then we have to go back to the domain D by deﬁning in Bs the extension
us = E−((ζsu)◦ψ−1
s
)◦ψs; since it has a compact support in Bs, we can extend it by
0 outside Bs, obtaining Eus ∈W 1,p(Rn). It can be noted that (Eus)|D = (ζsu)|D.
We ﬁnally set Eu = M
s=0 Eus (having simply set Eu0 the extension by 0 outside
B0 of ζ0u). Now it is not difﬁcult to check that Eu has the property listed in the
statement of the theorem.
For more details on this proof see, e.g., Salsa [18, Section 7.8.2]. A similar proof
for the general case k ≥1 would need the introduction of higher order “reﬂections”
and, due to the use of local charts, a Ck-regularity of the boundary ∂D. The result
for a Lipschitz continuous boundary is proved in Stein [22, Section VI.3], by means
of a different approach.
⊓⊔
Remark 6.1 It is also easily checked that the “extension-by-reﬂection” Eu con-
structed in the proof of the theorem satisﬁes Eu ∈W 1,p(Rn) ∩C0(Rn) if u ∈
W 1,p(D) ∩C0(D).
The following approximation result is now an easy consequence.
Theorem 6.3 Let D be a bounded, connected, open subset of Rn with Lipschitz
continuous boundary ∂D. Let u ∈W k,p(D), 1 ≤p < +∞. Then there exists a
sequence uε ∈C∞(D) with uε →u in W k,p(D).
Proof We consider the extension Eu ∈W k,p(Rn) of u, with supp(Eu) ⊂⊂Q.
Then, by Theorem 6.1 we can construct a sequence of molliﬁers (uε ∈C∞(Qε)
with (uε →Eu in W k,p
loc (Q) as ε →0. Taking uε = (uε|D we have the desired result.
⊓⊔
Remark 6.2 We also obtain that, if u ∈W 1,p(D) ∩C0(D), then the sequence
uε ∈C∞(D) constructed in Theorem 6.3 converges to u not only in W 1,p(D)
but also in C0(D). In fact, from Remark 6.1 we know that in this case the extension
Eu ∈C0(Q), and it is well-known that the molliﬁers of a continuous function in Q
converge uniformly on compact subsets of Q, thus on D ⊂Q.
Exercise 6.2 Let 1 ≤p ≤+∞and let p′ be given by 1
p + 1
p′ = 1 (with p′ = +∞
for p = 1 and viceversa). If fk →f in Lp(D) and gk →g in Lp′(D), then

D fkgkdx →

D fgdx.
Exercise 6.3
(i) Let u ∈H 1(D), v ∈H 1(D). Then uv ∈W 1,1(D) and
Di(uv) = (Diu)v + u(Div) .

6.2
Poincaré Inequality in H 1
0 (D)
87
(ii) The same result holds for u ∈W 1,p(D), v ∈W 1,p′(D), 1 < p < +∞,
1
p + 1
p′ = 1.
6.2
Poincaré Inequality in H 1
0 (D)
Theorem 6.4 (Poincaré Inequality in H 1
0 (D)) Let D be a bounded, connected,
open subset of Rn. Then there exists a constant CD > 0 such that

D
v2dx ≤CD

D
|∇v|2dx
∀v ∈H 1
0 (D) .
Proof (1st Way) Since H 1
0 (D) is the closure of C∞
0 (D), we can proceed by
approximation. Indeed, if we assume that the inequality holds in C∞
0 (D) it can
be easily extended to H 1
0 (D) by the following continuity procedure: consider
v ∈H 1
0 (D), then there exists a sequence {vk} in C∞
0 (D) such that vk →v in
H 1(D); in particular we have that

D
v2
kdx →

D
v2dx ,

D
|∇vk|2dx →

D
|∇v|2dx
(see Exercise 6.4), and therefore the inequality holds for v by passing to the limit in

D
v2
kdx ≤CD

D
|∇vk|2dx .
We thus need now to prove the inequality in C∞
0 (D); let v ∈C∞
0 (D), and choose
a ball large enough to contain the bounded set D, say D ⊂B(x0, R) with x0 ∈D.
Note that div(x −x0) = n, then integrating by parts and using the Cauchy–Schwarz
inequality

D
v2dx = n−1

D
nv2dx = n−1

D
div(x −x0)v2dx
= −n−1

D
(x −x0) · ∇(v2)dx = −n−1

D
(x −x0) · 2v∇vdx
≤2n−1 sup
x∈D
|x −x0|



≤R

D
v2dx
1/2 
D
|∇v|2dx
1/2
.

88
6
Technical Results
We simplify

D v2dx
1/2 and deﬁning
CD =
2R
n
2
= 4R2
n2
we obtain the estimate.
⊓⊔
Exercise 6.4 Prove that if vk →v in H 1(D) then

D
v2
kdx →

D
v2dx ,

D
|∇vk|2dx →

D
|∇v|2dx .
Exercise 6.5 Using an approach similar to the one presented in the ﬁrst proof of
Theorem 6.4, prove the Poincaré inequality for D bounded in one direction, with
constant S2 (S being the dimension of the strip containing D).
Proof (of the Poincaré Inequality, 2nd Way) We have already noted that, since
H 1
0 (D) is the closure of C∞
0 (D), we can proceed by approximation. Take v ∈
C∞
0 (D) and extend it by 0 outside D. Since D is bounded, it is bounded in all
directions; let us say that, having set x = (x′, xn), x′ = (x1, . . . , xn−1), for each
x ∈D we have a ≤xn ≤b. Thus we have v(x′, a) = 0 for all x′ such that
(x′, xn) ∈D and therefore
v(x′, xn) =
 xn
a
Dnv(x′, ξ)dξ + v(x′, a)
  
=0
=
 xn
a
Dnv(x′, ξ)dξ .
Consequently,
v2(x′, xn) =
 xn
a
1 · Dnv(x′, ξ)dξ
2
≤
) xn
a
12dξ
 1
2  xn
a
(Dnv(x′, ξ))2dξ
 1
2
*2
≤(xn −a)
 xn
a
(Dnv(x′, ξ))2dξ .
Integrating in dx′ we obtain

Rn−1 v2(x′, xn)dx′ ≤(xn −a)

Rn−1
 xn
a
(Dnv(x′, ξ))2dξdx′
≤(xn −a)

Rn (Dnv(x′, ξ))2dξdx′ .

6.3
Trace Inequality
89
Thus
 b
a

Rn−1 v2(x′, xn)dx′dxn ≤
 b
a
(xn −a)
$
Rn (Dnv(x′, ξ))2dξdx′
%
dxn
= 1
2(b −a)2

Rn (Dnv(x))2dx
= 1
2(b −a)2

D
(Dnv(x))2dx
(v = 0 outside D)
and
 b
a

Rn−1 v2(x′, xn)dx′dxn =

Rn v(x)2dx
(v = 0 for xn /∈(a, b))
=

D
v(x)2dx
(v = 0 outside D) .
In conclusion

D
v2dx ≤1
2(b −a)2

D
(Dnv)2dx ≤1
2(b −a)2

D
|∇v|2dx ,
thus the stated estimate with CD = 1
2(b −a)2.
⊓⊔
Exercise 6.6 The Poincaré inequality still holds in W 1,p
0
(D), 1 ≤p < +∞: there
exists a constant CD > 0 such that

D
|v|pdx ≤CD

D
|∇v|pdx
∀v ∈W 1,p
0
(D) .
6.3
Trace Inequality
Next we discuss the possibility of assigning “boundary values” on ∂D to a function
v ∈H 1(D), assuming that ∂D is Lipschitz continuous. When we deal with v ∈
C(D), clearly it has values on ∂D in the usual sense. The problem is that a typical
function v ∈H 1(D) is not in general continuous and, even worse, is only deﬁned
almost everywhere in D. Since ∂D can have n-dimensional Lebesgue measure equal
to zero, it seems that we cannot give a clear meaning to the expression “v restricted
to ∂D”. The notion of a trace on the boundary solves this problem.
Theorem 6.5 (Trace on ∂D and Trace Inequality) Let D be a bounded, con-
nected, open set with a Lipschitz continuous boundary ∂D. Then for v ∈H 1(D)
there is a way to determine a function γ0v ∈L2(∂D) such that
γ0v = v|∂D
for v ∈C∞(D)

90
6
Technical Results
and

∂D
(γ0v)2dx ≤C∗

D
(v2 + |∇v|2)dx
for a suitable C∗> 0 (independent of v). Moreover, the map v →γ0v is linear and,
from the inequality above, continuous from H 1(D) to L2(∂D).
Deﬁnition 6.1 We call γ0v the trace of v on ∂D, and, even if this can lead to some
confusion, very often in the sequel we will continue to write v|∂D instead of γ0v.
The proof of this theorem needs some steps. We start by proving it for smooth
functions deﬁned in a half-space. To clarify this point, we need some notation.
Suppose we have v ∈C1(Rn
+), where Rn
+ = {x ∈Rn | xn > 0}, with v = 0
out of
BR,+ = {x ∈Rn | xn ≥0 , |x| ≤R} .
Then we have
Theorem 6.6 (Trace Inequality in Rn
+ for C1-Functions) For any v ∈C1(Rn
+)
vanishing outside BR,+ it holds

Rn−1 v2(x′, 0)dx′ ≤R

Rn
+
(Dnv)2dx .
Proof For (x′, 0) ∈BR,+ we have
v(x′, 0) = −
 R
0
Dnv(x′, ξ)dξ + v(x′, R)
  
=0
= −
 R
0
Dnv(x′, ξ)dξ .
Thus, as in the second proof of the Poincaré inequality:

Rn−1 v2(x′, 0)dx′ ≤R

Rn−1
 R
0
(Dnv(x′, ξ))2dξ

dx′ = R

Rn
+
(Dnv)2dx ,
where the last equality is justiﬁed since v = 0 outside BR,+.
⊓⊔
Now we can obtain the following theorem:
Theorem 6.7 (Trace Inequality in D for C1-Functions) Let D be a bounded,
connected, open set with a Lipschitz continuous boundary ∂D. There exists a
constant C∗> 0 such that

∂D
v2
|∂Ddx ≤C∗

D
(v2 + |∇v|2)dx
∀v ∈C1(D) .

6.3
Trace Inequality
91
Proof The proof is rather technical and we will only enlighten some essential ideas.
To simplify a little the procedure, let us also suppose that the regularity of the
boundary is C1; the proof for the Lipschitz case is just a little bit more complicate,
as in that case we have to deal with almost everywhere differentiable functions with
bounded derivatives (this is the case of Lipschitz functions, by the Rademacher
theorem).
We can cover the boundary ∂D by a ﬁnite union of open balls Bs, s = 1, . . . , M,
each one centered at a point xs ∈∂D (the covering is ﬁnite as ∂D is a closed
and bounded set, therefore a compact set in Rn). Consider a partition of unity ζs
associated to the covering Bs of ∂D (in particular, the support of ζs is a compact set
in Bs: see Appendix A). The assumption on the regularity of the boundary tells us
that there is a ﬁnite set of local charts ψs, bijective C1-maps from Bs onto BR =
{ξ ∈Rn | |ξ| < R}, with the inverse map ψ−1
s
that is C1, and such that Bs ∩D
is mapped onto BR,+ = {ξ ∈Rn | |ξ| < R, ξn > 0}. The functions (ζsv) ◦ψ−1
s
are C1-functions in Rn
+, vanishing outside BR,+. Therefore we can apply to each of
them the result of Theorem 6.6, and we get

Rn−1 ((ζsv) ◦ψ−1
s
)2(x′, 0)dx′ ≤R

Rn
+
(Dn((ζsv) ◦ψ−1
s
))2dx .
Transforming these integrals into integrals in Bs ∩∂D and Bs ∩D we ﬁnd, by the
chain rule and some straightforward estimates,

Bs∩∂D
(ζsv)2dSx ≤C

Bs∩D
(|∇v|2 + v2)dx .
Now we can add for s = 1, . . . , M, and using the fact that ζs is a partition of unity
of the covering Bs of ∂D we obtain the ﬁnal result.
⊓⊔
We can now give the proof of the trace theorem (Theorem 6.5).
Proof (of Theorem 6.5) We proceed by approximation. Consider vk ∈C∞(D)
such that vk →v in H 1(D). By the trace theorem for C1-functions we have that

∂D
v2
k|∂D ≤C∗

D
(v2
k + |∇vk|2)dx
∀k ≥1
(6.2)
and

∂D
(vk|∂D −vs|∂D)2 ≤C∗

D
[(vk −vs)2 + |∇(vk −vs)|2]dx
∀k, s ≥1 .
(6.3)
Since vk is convergent, it is a Cauchy sequence in H 1(D). Therefore

∂D
(vk|∂D −vs|∂D)2 ≤C∗

D
[(vk −vs)2 + |∇(vk −vs)|2]dx ≤C∗ϵ

92
6
Technical Results
for k, s large enough, and thus we see that vk|∂D is a Cauchy sequence in L2(∂D).
Since L2(∂D) is a Hilbert space, we ﬁnd q ∈L2(∂D) such that vk|∂D →q in
L2(∂D). Taking the limit in (6.2) we have

∂D
q2dx ≤C∗

D
(v2 + |∇v|2)dx .
This value q does not depend on the approximating sequence vk, but only on v. In
fact, if wk is another approximating sequence of v, and p is the limit in L2(∂D) of
wk|∂D, it follows

∂D
|q −p|2dx =

∂D
|q −vk|∂D + vk|∂D −wk|∂D + wk|∂D −p|2dx
≤3
+ 
∂D
(q −vk|∂D)2dx +

∂D
(p −wk|∂D)2dx
+

∂D
(vk|∂D −wk|∂D)2dx
,
≤3
+ 
∂D
(q −vk|∂D)2dx +

∂D
(p −wk|∂D)2dx
+ C∗

D
+
(vk −wk)2 + |∇(vk −wk)|2,
dx
,
,
and all the terms go to zero, as vk →v in H 1(D) and wk →v in H 1(D).
In conclusion, we deﬁne the trace γ0v as the unique value q ∈L2(∂D) obtained
with the above procedure. Clearly the map v →q is linear; moreover,if v ∈C∞(D)
we can choose vk = v for all k ≥1, therefore
vk|∂D = v|∂D →γ0v ,
showing that the trace of a smooth function v (the limit of vk|∂D...) is coincident
with its restriction on the boundary.
⊓⊔
Remark 6.3 As we have seen the proof of the trace inequality is based on an
elementary argument that we have already met many times. Indeed, if we consider
a continuous function f : Q →R and we want to extend this function to all R,
how can we do? Let x be an irrational number; since Q is dense in R, we can take a
sequence {rk} ⊂Q such that rk →x. Then the natural step is to deﬁne f (x) as the
limit of f (rk). To led this argument to its end we have to verify that the limit exists,
proving for example that {f (rk)} is a Cauchy sequence, and that its limit does not
depend on the sequence {rk} we have chosen.

6.3
Trace Inequality
93
Remark 6.4 If v ∈H 1(D) ∩C0(D), we know from Remark 6.2 we can ﬁnd a
sequence vk ∈C∞(D) that converges to v in H 1(D) and in C0(D) (namely,
uniformly in D). Then on one side
vk|∂D →γ0v
in L2(∂D)
(deﬁnition of the trace γ0v)
and on the other side
vk|∂D →v|∂D
in C0(∂D)
(uniform convergence in D) ,
in particular vk|∂D →v|∂D in L2(∂D). Thus the trace γ0v on ∂D is equal to the
restriction v|∂D on ∂D for all functions v ∈H 1(D) ∩C0(D).
Remark 6.5 It can be proved that H 1
0 (D) is equal to the space {v ∈H 1(D) | v|∂D =
0 on ∂D}. The proof of the inclusion H 1
0 (D) ⊂{v ∈H 1(D) | v|∂D = 0 on ∂D}
is easy. In fact, an element v ∈H 1
0 (D) can be approximated by a sequence vk ∈
C∞
0 (D); since vk|∂D = 0, it follows that the trace v|∂D satisﬁes v|∂D = 0. The
opposite inclusion is also true, but the proof is a little bit technical, therefore we do
not present it here (see Evans [6, Theorem 2, pp. 259–261]).
Remark 6.6 Let us note that the trace inequality still holds in W 1,p(D) (1 ≤
p < +∞). The proof of the basic estimate for smooth functions in Theorem 6.6
is essentially the same of the similar estimate for the Poincaré inequality (see
Exercise 6.6).
Remark 6.7 A result similar to that presented in Theorem 6.5 can be proved for the
trace on , a (non-empty) open and Lipschitz continuous subset of ∂D.
Having deﬁned the trace, we can prove an integration by parts formula. We state
it as an exercise.
Exercise 6.7 Let D a bounded, connected, open set with a Lipschitz continuous
boundary ∂D, and take u ∈H 1(D), v ∈H 1(D). Then the integration by parts
formula

D
(Diu)vdx = −

D
uDivdx +

∂D
niu|∂Dv|∂DdSx
holds.
Another couple of exercises are the following:
Exercise 6.8 Let us assume that D is a bounded, connected, open set with a
Lipschitz continuous boundary ∂D, and that D = D1 ∪D2, D1 ∩D2 = ∅, where
D1 and D2 are (non-empty) open sets with a Lipschitz continuous boundary. Set
 = ∂D1 ∩∂D2 and take v ∈Lp(D), 1 ≤p < +∞. Then v ∈W 1,p(D) if and
only if v|D1 ∈W 1,p(D1), v|D2 ∈W 1,p(D2) and the trace of v|D1 and v|D2 on  is
the same.

94
6
Technical Results
Exercise 6.9 Let D a bounded, connected, open set with a Lipschitz continuous
boundary ∂D. The statement “there exists a constant C > 0 such that

∂D
|v|pdSx ≤C

D
|v|pdx
∀v ∈C0(D)”
is false for 1 ≤p < +∞.
6.4
Compactness and Rellich Theorem
First of all, we see a compactness criterion (similar to Ascoli-Arzelà theorem, and
due to Kolmogorov and M. Riesz).
Theorem 6.8 (Precompactness) Let D ⊂Rn be a bounded, connected, open set.
Consider 1 ≤p < +∞and X ⊂Lp(D). Then X is precompact if and only if
(i) there exists M > 0 such that
∥v∥Lp(D) ≤M
∀v ∈X ;
(ii) extending v by 0 outside D, it holds
lim
h→0 ∥v(· + h) −v(·)∥Lp(D) = 0 ,
uniformly with respect to v ∈X.
Remark 6.8 Remember that a subset X of a Banach space Y is said to be
precompact if its closure is compact, i.e., from any sequence in X we can extract a
subsequence convergent in Y to an element that does not necessarily belong to X.
The principal compactness result in Sobolev spaces is the following:
Theorem 6.9 (Rellich Theorem) Let D a bounded, connected, open subset of Rn,
with a Lipschitz continuous boundary ∂D, and let 1 ≤p < +∞. Then W 1,p(D)
is compactly immersed in Lp(D): from any bounded sequence vk ∈W 1,p(D) it is
possible to extract a subsequence vks that converges in Lp(D) to a limit v ∈Lp(D).
Proof We use the precompactness theorem, and we limit ourselves to the case
p = 2. Let us start with an estimate that is valid for smooth functions. Taking
v ∈C∞
0 (Rn) it follows
v(x + h) −v(x) =
 1
0
d
dt [v(x + th)]dt =
 1
0
∇v(x + th) · hdt ,

6.4
Compactness and Rellich Theorem
95
hence
|v(x + h) −v(x)|2 =

 1
0
∇v(x + th) · hdt

2
≤|h|2

 1
0
∇v(x + th)dt

2
≤|h|2
 1
0
|∇v(x + th)|2dt
by the Cauchy–Schwarz inequality. Integrating in Rn

Rn |v(x + h) −v(x)|2dx ≤|h|2

Rn
 1
0
|∇v(x + th)|2dt

dx
= |h|2
 1
0

Rn |∇v(x + th)|2dx

dt = |h|2

Rn |∇v|2dx ,
having performed the change of variable x + th = y (and then replaced dy with
dx...). By approximation, since C∞
0 (Rn) is dense in H 1
0 (Rn), we have that this
inequality is true for v ∈H 1
0 (Rn):

Rn |v(x + h) −v(x)|2dx ≤|h|2

Rn |∇v|2dx .
(6.4)
Now we want to prove that a bounded set X ⊂H 1(D) is precompact in L2(D).
Consider
X = {v ∈H 1(D) | ∥v∥H 1(D) ≤M} .
By the extension theorem (Theorem 6.2) we know that, for v ∈X, Ev ∈H 1
0 (Rn),
supp(Ev) ⊂⊂Q. Thus Ev ∈H 1
0 (Q) and is vanishing outside Q; moreover, from
the continuity of the extension operator we have
∥Ev∥H 1(Q) = ∥Ev∥H 1(Rn) ≤C∗∥v∥H 1(D) ≤C∗M
∀v ∈X .
Let us denote by EX the set of the extensions of elements of X
EX = {w ∈H 1
0 (Q) | ∃v ∈X such that w = Ev} .
We have just shown that EX is bounded in L2(Q). Furthermore we know that (6.4)
is satisﬁed for all w ∈EX, thus

Q
|(Ev)(x + h) −(Ev)(x)|2dx ≤

Rn |(Ev)(x + h) −(Ev)(x)|2dx
≤
(6.4)
|h|2

Rn |∇Ev|2dx ≤C2
∗M2|h|2
∀v ∈X .

96
6
Technical Results
Applying Theorem 6.7 we obtain that EX is precompact in L2(Q). Take now a
sequence vk ∈X: since EX is precompact in L2(Q), we can select a subsequence
Evks convergent to w0 in L2(Q). Then vks = Evks|D converges to w0|D in L2(D),
and the proof is complete.
⊓⊔
Exercise 6.10 Let D a bounded, connected, open set with a Lipschitz continuous
boundary ∂D. Let vk be a bounded sequence in W 1,p(D), 1 < p < +∞, and
consider a subsequence vks which converges to v in Lp(D) by the Rellich theorem.
Prove that the limit v indeed belongs to W 1,p(D).
6.5
Other Poincaré Inequalities
We are now in a condition to prove other Poincaré inequalities that are useful in
the proof of the coerciveness of the bilinear form BL(·, ·) introduced in (2.18) (see
Sect. 5.4 for these coerciveness results).
Theorem 6.10 Let D be bounded, connected, open subset of Rn with a Lipschitz
continuous boundary ∂D. Denote by
H 1
∗(D) =

v ∈H 1(D)


D
vdx = 0
#
.
Then there exists C∗> 0 such that

D
v2dx ≤C∗

D
|∇v|2dx
∀v ∈H 1
∗(D) .
Proof Assume, by contradiction, that for each k ∈N, k ̸= 0, we can ﬁnd vk ∈
H 1
∗(D) such that

D
v2
kdx > k

D
|∇vk|2dx .
Thus

D v2
k > 0, and we can consider
wk =
vk

D v2
kdx
1/2 ∈H 1
∗(D) ,
which satisﬁes

D w2
kdx = 1. We clearly have that
1 =

D
w2
kdx > k

D
|∇wk|2dx ⇒

D
|∇wk|2dx < 1
k ,
(6.5)

6.5
Other Poincaré Inequalities
97
in particular
∥wk∥H 1(D) =

D
w2
kdx +

D
|∇wk|2dx
1/2
≤
√
2 .
From Rellich theorem we can extract a subsequence wks which converges to w0 in
L2(D), therefore

D
w2
0dx = lim
s→∞

D
w2
ksdx = 1 .
From (6.5) we have ∇wks →0 in (L2(D))n; therefore for each ϕ ∈C∞
0 (D) and
for each i = 1, . . ., n it holds

D
w0Diϕdx = lim
s→∞

D
wksDiϕdx = −lim
s→∞

D
(Diwks)ϕdx = 0 .
As a consequence ∇w0 = 0 and w0 ∈H 1(D). From wks →w0 in L2(D) we also
have that

D
w0dx = lim
s→∞

D
wksdx = 0 ,
thus w0 ∈H 1
∗(D). From Diw0 = 0 for each i = 1, . . . , n we can infer w0 = const
(see Sect. 6.7) and thus we have a contradiction, as the only constant belonging to
H 1
∗(D) is the null constant, but then

D w2
0dx = 1 is impossible.
⊓⊔
Let us continue by presenting other similar results. We start with this remark:
Remark 6.9 Let D be bounded, connected, open subset of Rn with a Lipschitz
continuous boundary ∂D, and let D ⊂∂D be a non-empty, open Lipschitz
continuous subset. It can be proved that H 1
D(D), the closure of C∞
D(D) in H 1(D),
is equal to the space
-
v ∈H 1(D) | v|D = 0
.
, where v|D is the trace on D
(see Remark 6.7). As already seen in Remark 6.5, the easy part is the inclusion
H 1
D(D) ⊂{v ∈H 1(D) | v|D = 0}; the inverse inclusion is more technical.
Theorem 6.11 Let D be bounded, connected, open subset of Rn with a Lipschitz
continuous boundary ∂D. Denote by
H 1
D(D) =
!
v ∈H 1(D) | v|D = 0
"
,
where D ⊂∂D is a non-empty, open Lipschitz continuous subset. Then there exists
C∗> 0 such that

D
v2dx ≤C∗

D
|∇v|2dx
∀v ∈H 1
D(D) .

98
6
Technical Results
Proof It is essentially the same as before. The only change is a consequence of
the remark that, having found wks →w0 in L2(D) with ∇wks →0 = ∇w0 in
(L2(D))n, we have indeed obtained wks →w0 in H 1(D). Thus by the continuity of
the trace operator we ﬁnd 0 = wks|D →w0|D in L2(D), hence w0 ∈H 1
D(D).
Since we also know that w0 = const and that the only constant belonging to H 1
D(D)
is the null constant, again we obtain a contradiction from

D w2
0dx = 1.
⊓⊔
For the Robin problem this Poincaré-type inequality is important.
Theorem 6.12 Let D be bounded, connected, open subset of Rn with a Lipschitz
continuous boundary ∂D. Let q : ∂D →R be a non-negative and bounded function,
not identically vanishing, namely, such that

∂D qdSx > 0. Then there exists C∗> 0
such that

D
v2dx ≤C∗

D
|∇v|2dx +

∂D
qv2dSx

∀v ∈H 1(D) .
(6.6)
Proof The result is proved as before. We arrive at wks →w0 in H 1(D), with

D w2
0dx = 1 and w0 = const. By the continuity of the trace operator we obtain
that wks|∂D →w0|∂D in L2(∂D), thus also √qwks|∂D →√qw0|∂D in L2(∂D). As
a consequence,

∂D
qw2
ksdSx →

∂D
qw2
0dSx ,
by applying Exercise 6.2 in L2(∂D). On the other hand, from the assumption that
inequality (6.6) does not hold we have

D
|∇wks|2dx +

∂D
qw2
ksdSx < 1
ks
,
hence

∂D qw2
ksdSx
→
0. The contradiction comes from the fact that

∂D qw2
0dSx = 0 implies w0 = 0, as w0 is constant and

∂D qdSx > 0.
⊓⊔
We conclude with the following theorem:
Theorem 6.13 Let D be bounded, connected, open subset of Rn with a Lipschitz
continuous boundary ∂D. Then there exists C∗> 0 such that

D
(v −vD)2dx ≤C∗

D
|∇v|2dx
∀v ∈H 1(D) ,
where vD =
1
meas(D)

D vdx.
Proof The proof is trivial. Indeed it is sufﬁcient to consider w = v −vD, which is
average free and satisﬁes ∇w = ∇v. Thus we can apply Theorem 6.10.
⊓⊔

6.7
∇f = 0 Implies f = const
99
6.6
du Bois-Reymond Lemma
Lemma 6.1 Let D be an open set in Rn. If f ∈L1
loc(D) satisﬁes

D
f ϕdx = 0
∀ϕ ∈C∞
0 (D)
(6.7)
then f = 0 a.e. in D.
Proof For r > 0 and ε > 0 denote by Br = {x ∈Rn | |x| < r} and by Dε = {x ∈
D | dist(x, ∂D) > ε}. Take k0 large enough to have D1/k0 ∩Bk0 ̸= ∅. For a ﬁxed
k ∈N, k ≥k0 and for 0 < δ < 1/k consider the molliﬁer fδ = f ∗ηδ deﬁned in
Dδ ⊃D1/k.
For any ﬁxed x ∈D1/k the map y →ηδ(x −y) ∈C∞
0 (D), thus by (6.7) we
obtain
fδ(x) =

D
f (y)ηδ(x −y)dy = 0 .
We also know that fδ →f in L1
loc(D), in particular fδ →f in L1(D1/k ∩Bk).
Therefore, for a suitable subsequence we ﬁnd fδs →f a.e. in D1/k ∩Bk.
Putting together the two results it follows f (x) = 0 a.e. in D1/k ∩Bk. Since
D = ∪∞
k=k0(D1/k ∩Bk), the thesis is proved.
⊓⊔
6.7
∇f = 0 Implies f = const
Proposition 6.1 Let D be an open and connected set in Rn. Suppose that f ∈
L1
loc(D) satisﬁes Dif = 0 for each i = 1, . . . , n. Then f = const a.e. in D.
Proof It is enough to prove that there exists c0 ∈R such that

D
f ϕdx = c0

D
ϕdx
∀ϕ ∈C∞
0 (D) .
In fact, from this it follows

D (f −c0)ϕdx = 0 for each ϕ ∈C∞
0 (D), thus from du
Bois-Reymond Lemma 6.1 we obtain f = c0 a.e. in D. Consider now ϕ ∈C∞
0 (D):
the assumption says that the weak gradient of f is vanishing, namely,
0 =

D
f Diϕdx
for each i = 1, . . . , n .

100
6
Technical Results
Take Q ⊂⊂D, Q open and connected. Consider the molliﬁer fε = ηε ∗f , deﬁned
in Q for ε < εQ. We already know that
Difε = ηε ∗Dif
(see the proof of Theorem 6.1), thus
Difε = 0
in Q .
Therefore we have
fε = cε,Q
in Q ,
and for any ϕ ∈C∞
0 (Q) it follows, for ε < εQ,

Q
fεϕdx = cε,Q

Q
ϕdx .
(6.8)
Selecting ˆϕQ ∈C∞
0 (Q) such that

Q ˆϕQdx ̸= 0, for ε < εQ we have from (6.8)
cε,Q =

Q fε ˆϕQdx

Q ˆϕQdx
.
Since fε →f in L1
loc(D), we get

Q fε ˆϕQdx →

Q f ˆϕQdx, hence
cε,Q →

Q f ˆϕQdx

Q ˆϕQdx
= c0,Q .
On the other hand, we also have

Q fεϕdx →

Q f ϕdx for any ϕ ∈C∞
0 (Q), thus
from (6.8) we obtain

Q
f ϕdx = c0,Q

Q
ϕdx
∀ϕ ∈C∞
0 (Q) .
In conclusion, we have f = c0,Q a.e. in Q. Since when Q1 ∩Q2 ̸= ∅it follows
c0,Q1 = c0,Q2, the proof is completed by “invading” D by a sequence of open and
connected sets Qm ⊂⊂D.
⊓⊔
6.8
Exercises
Exercise 6.1 Prove that H 1
0 (Rn) = H 1(Rn).

6.8
Exercises
101
Solution We only need to show that a function v ∈H 1(Rn) can be approximated
in H 1(Rn) by functions belonging to C∞
0 (Rn). For this aim, the keywords are:
“truncate” and “mollify”. In fact, adapting the proof of Theorem 6.1, one sees that
the molliﬁers vε ∈C∞(Rn) converge to v in H 1(Rn), but vε have not a compact
support, unless v itself has a compact support.
Then let us ﬁrst suppose that v ∈H 1(Rn) and has a compact support. We
take vε = ηε ∗v, where ηε is the Friedrichs molliﬁer introduced in the proof
of Theorem 6.1. It is known that vε ∈C∞
0 (Rn) (here it is used that v has a
compact support) and that vε →v in L2(Rn) (here it is used that v ∈L2(Rn)).
Moreover, adapting the proof of Theorem 6.1 to the whole space Rn, we see that
Divε = (Div)ε in Rn, thus Divε →Div in L2(Rn).
Now we have to show that each function v ∈H 1(Rn) can be approximated by
a function belonging to H 1(Rn) with compact support. It is enough to “truncate”
v out of a compact set. Precisely, we take a function ζ ∈C∞
0 (Rn) such that 0 ≤
ζ(x) ≤1, ζ(x) = 1 for |x| ≤1 and ζ(x) = 0 for |x| ≥2 and for t > 0 we deﬁne
vt(x) = v(x)ζ(x/t). Clearly vt ∈H 1(Rn) and has a compact support. Then
∇vt(x) = ∇v(x)ζ(x/t) + 1
t v(x)∇ζ(x/t) .
We have

Rn(v(x) −vt(x))2dx =

Rn v2(x)(1 −ζ(x/t))2dx ≤

|x|≥t
v2(x)dx
and

Rn |∇v(x) −∇vt(x)|2dx =

Rn
∇v(x)(1 −ζ(x/t)) −1
t v(x)∇ζ(x/t)
2dx
≤2

Rn |∇v(x)|2(1 −ζ(x/t))2dx + 2
t2

Rn v2(x)|∇ζ(x/t)|2dx
≤2

|x|≥t |∇v(x)|2dx + 2M2
t2

Rn v2(x)dx ,
where M = sup
x∈Rn |∇ζ(x)|. Taking the limit for t →+∞we obtain the result.
Exercise 6.2 Let 1 ≤p ≤+∞and let p′ be given by 1
p + 1
p′ = 1 (with p′ = +∞
for p = 1 and viceversa). If fk →f in Lp(D) and gk →g in Lp′(D), then

D fkgkdx →

D fgdx.

102
6
Technical Results
Solution Indeed, by Hölder inequality,


D
(fkgk −fg)dx
 =


D
(fkgk −fkg + fkg −fg)dx

≤


D
fk(gk −g)dx
 +


D
g(fk −f )dx

≤∥fk∥Lp(D)∥gk −g∥Lp′ (D) + ∥g∥Lp′ (D)∥fk −f ∥Lp(D) →0 ,
as ∥fk∥Lp(D) →∥f ∥Lp(D) (by the triangular inequality).
Exercise 6.3
(i) Let u ∈H 1(D), v ∈H 1(D). Then uv ∈W 1,1(D) and
Di(uv) = (Diu)v + u(Div) .
(ii) The same result holds for u ∈W 1,p(D), v ∈W 1,p′(D), 1 < p < +∞,
1
p + 1
p′ = 1.
Solution
(i) The proof is similar to that of Exercise 4.3. First of all, we know that
uv ∈L1(D). Moreover (Diu)v and u(Div) belong to L1(D), as products of
functions in L2(D). Thus it is enough to prove Di(uv) = (Diu)v+u(Div). We
choose ϕ ∈C∞
0 (D) and we set Q = supp(ϕ). Then we take an open set 
Q such
that Q ⊂⊂ˆQ ⊂⊂D. By Theorem 6.1 we ﬁnd uk ∈C∞(
Q), vk ∈C∞(
Q)
such that uk →u in H 1(
Q), vk →v in H 1(
Q). Since ϕ ∈C∞
0 (
Q) we have


Q
ukvkDiϕdx = −


Q
Di(ukvk)ϕdx
= −


Q
[(Diuk)vk + uk(Divk)] ϕdx .
Taking into account Exercise 6.2, the result follows passing to the limit for
k →∞, as we obtain

D
uvDiϕdx = 

Q uvDiϕdx
= −


Q
[(Diu)v + u(Div)]ϕ dx = −

D
[(Diu)v + u(Div)]ϕ dx .
(ii) The proof is the same, just noting that uv, (Diu)v and u(Div) belong to
L1(D), as products of functions in Lp(D) and Lp′(D), and using the approxi-
mation results given by Theorem 6.1 for functions belonging to W 1,p(D) and
W 1,p′(D).

6.8
Exercises
103
Exercise 6.4 Prove that if vk →v in H 1(D) then

D
v2
kdx →

D
v2dx ,

D
|∇vk|2dx →

D
|∇v|2dx .
Solution Notice that since vk →v in H 1(D) we have in particular that vk →v in
L2(D) and ∇vk →∇v in L2(D). Therefore, by the triangular inequality


D
v2
kdx −

D
v2dx
 =
∥vk∥2
L2(D) −∥v∥2
L2(D)
 ≤∥vk −v∥2
L2(D) →0 .
Similarly we prove that

D |∇vk|2dx →

D |∇v|2dx in L2(D).
Exercise 6.5 Using an approach similar to the one presented in the ﬁrst proof of
Theorem 6.4, prove the Poincaré inequality for D bounded in one direction, with
constant S2 (S being the dimension of the strip containing D).
Solution By proceeding as in Theorem 6.4 it is enough to prove the inequality for
v ∈C∞
0 (D). Suppose that D is contained in the strip {x ∈Rn | |xn −x0
n| ≤S/2}.
Since Dnxn = 1, we have

D
v2dx =

D
Dn(xn −x0
n)v2dx
= −

D
(xn −x0
n) Dn(v2)dx = −

D
(xn −x0
n) 2vDnvdx
≤2 S
2

D
v2dx
1/2 
D
|Dnv|2dx
1/2
,
thus the Poincaré inequality holds with CD = S2.
Exercise 6.6 The Poincaré inequality still holds in W 1,p
0
(D), 1 ≤p < +∞: there
exists a constant CD > 0 such that

D
|v|pdx ≤CD

D
|∇v|pdx
∀v ∈W 1,p
0
(D) .
Solution As in the proof of Theorem 6.4, 2nd way, we assume that a ≤xn ≤b and
we start writing, for v ∈C∞
0 (D),
v(x′, xn) =
 xn
a
Dnv(x′, ξ)dξ .

104
6
Technical Results
For 1 ≤p < +∞it follows
|v(x′, xn)|p =

 xn
a
Dnv(x′, ξ)dξ

p
≤
 xn
a
1 · |Dnv(x′, ξ)|dξ
p
.
By Hölder inequality, for 1 < p < +∞and 1
p + 1
p′ = 1 (for p = 1 you do not even
need the Hölder inequality...) it follows
 xn
a
1 · |Dnv(x′, ξ)|dξ
p
≤
) xn
a
1p′dξ
 1
p′  xn
a
|Dnv(x′, ξ)|pdξ
 1
p
*p
≤(xn −a)p/p′  xn
a
|Dnv(x′, ξ)|pdξ .
Since p
p′ = p −1, integrating in dx′ we obtain

Rn−1 |v(x′, xn)|pdx′ ≤(xn −a)p−1

Rn−1
 xn
a
|Dnv(x′, ξ)|pdξdx′
≤(xn −a)p−1

Rn |Dnv(x′, ξ)|pdξdx′ .
Thus
 b
a

Rn−1 |v(x′, xn)|pdx′dxn ≤
 b
a
(xn −a)p−1
$
Rn |Dnv(x′, ξ)|pdξdx′
%
dxn
= 1
p (b −a)p

Rn |Dnv(x)|pdx .
In conclusion, taking into account that v = 0 outside D,

D
|v|pdx ≤1
p(b −a)p

D
|∇v|pdx .
Exercise 6.7 Let D a bounded, connected, open set with a Lipschitz continuous
boundary ∂D, and take u ∈H 1(D), v ∈H 1(D). Then the integration by parts
formula

D
(Diu)vdx = −

D
uDivdx +

∂D
niu|∂Dv|∂DdSx
holds.

6.8
Exercises
105
Solution We proceed by approximation. We have uk ∈C∞(D), uk →u in H 1(D),
vk ∈C∞(D), vk →v in H 1(D). Then

D
(Diuk)vkdx



[1]
= −

D
ukDivkdx



[2]
+

∂D
niuk|∂Dvk|∂DdSx



[3]
.
By Exercise 6.2 we have these ﬁrst two results:
(1) As k →∞we have

D
(Diuk)vkdx →

D
(Diu)vdx .
(2) As k →∞we have

D
ukDivkdx →

D
uDivdx .
(3) The ﬁnal step is to check that

∂D
niukvkdSx →

∂D
niu|∂Dv|∂DdSx .
We know that the map v →v|∂D is continuous from H 1(D) to L2(∂D), thus
uk|∂D →u|∂D
in L2(∂D)
and
vk|∂D →v|∂D
in L2(∂D) .
Since n is a bounded vector ﬁeld,
niuk|∂D →niu|∂D
in L2(∂D) ,
which ends the proof, applying the result of Exercise 6.2 in L2(∂D).
Exercise 6.8 Let us assume that D is a bounded, connected, open set with a
Lipschitz continuous boundary ∂D, and that D = D1 ∪D2, D1 ∩D2 = ∅, where
D1 and D2 are (non-empty) open sets with a Lipschitz continuous boundary. Set
 = ∂D1 ∩∂D2 and take v ∈Lp(D), 1 ≤p < +∞. Then v ∈W 1,p(D) if and
only if v|D1 ∈W 1,p(D1), v|D2 ∈W 1,p(D2) and the trace of v|D1 and v|D2 on  is
the same.

106
6
Technical Results
Solution
(⇒)
The proof that v|D1 ∈W 1,p(D1) and v|D2 ∈W 1,p(D2) is straightforward.
Then consider a sequence vk ∈C∞(D) which converges to v in W 1,p(D) (see
Theorem 6.3); in particular, w1,k = vk|D1 ∈C∞(D1) converges to v|D1 in
W 1,p(D1) and w2,k = vk|D2 ∈C∞(D2) converges to v|D2 in W 1,p(D2). Hence
w1,k| converges in Lp() to the trace of v|D1 on  and w2,k| converges in
Lp() to the trace of v|D2 on . Since w1,k| = w2,k|, the thesis follows.
(⇐)
For the sake of simplicity, let us write v1 and v2 for v|D1 and v|D2. Take a test
function ϕ ∈C∞
0 (D) (and thus not necessarily vanishing on the interface ) and
deﬁne ωi ∈Lp(D) by setting ωi|D1 = Div1 and ωi|D2 = Div2, i = 1, . . . , n.
We ﬁnd, by integration by parts as in Exercise 6.7,

D ωiϕdx =

D1 Div1ϕdx +

D2 Div2ϕdx
= −

D1 v1Diϕdx +

 n1,iv1|ϕ|dSx
−
D2 v2Diϕdx + 
 n2,iv2|ϕ|dSx ,
where nj is the unit normal vector on  directed outside Dj, j = 1, 2. Since
v1| = v2| and n1,i = −n2,i, it follows

D
ωiϕdx = −

D1
v1Diϕdx −

D2
v2Diϕdx = −

D
vDiϕdx ,
hence Div = ωi ∈Lp(D).
Exercise 6.9 Let D a bounded, connected, open set with a Lipschitz continuous
boundary ∂D. The statement “there exists a constant C > 0 such that

∂D
|v|pdSx ≤C

D
|v|pdx
∀v ∈C0(D)”
(6.9)
is false for 1 ≤p < +∞.
Solution Consider the sequence vk ∈C0(D) satisfying 0 ≤vk(x) ≤1 and deﬁned
as follows:
vk(x) =
⎧
⎨
⎩
1
for x ∈D \ D1/k
continuous for x ∈D1/k \ D2/k
0
for x ∈D2/k ,
where Dϵ is as in Theorem 6.1. Then

∂D
|vk|pdSx = meas(∂D) > 0

6.8
Exercises
107
and

D
|vk|pdx ≤meas(D \ D2/k) ≤C 1
k ,
thus (6.9) cannot hold.
Exercise 6.10 Let D a bounded, connected, open set with a Lipschitz continuous
boundary ∂D. Let vk be a bounded sequence in W 1,p(D), 1 < p < +∞, and
consider a subsequence vks which converges to v in Lp(D) by the Rellich theorem.
Prove that the limit v indeed belongs to W 1,p(D).
Solution Since W 1,p(D) is a reﬂexive Banach space (see Remark 4.9), from the
bounded sequence vks we can extract a subsequence, still denoted by vks , which
converges weakly to w ∈W 1,p(D). In particular, vks converges weakly to w in
Lp(D), and since it converges to v in Lp(D), it follows v = w by the uniqueness
of the weak limit and thus v ∈W 1,p(D).

Chapter 7
Additional Results
In this chapter a series of additional results are described and analyzed: the
Fredholm alternative theory applied to second order elliptic problems; the spectral
theory for an elliptic operator (in the general case and in the symmetric case);
the maximum principle for weak subsolution of elliptic equations; some results
concerning further regularity of weak solutions, together with higher summability
or regularity results in the classical sense for functions belonging to Sobolev spaces;
and ﬁnally the Galerkin approximation method.
7.1
Fredholm Alternative
We can employ the Fredholm theory for a compact perturbation of the identity
operator to glean more detailed information regarding the solvability of second order
elliptic PDE.
We start by brieﬂy analyzing the ﬁnite dimensional case. Let A be a n×m matrix,
associated to the linear map v →Av, v ∈Rm, Av ∈Rn. From linear algebra it is
known that dimN(A) + dimR(A) = m, where N(A) = {v ∈Rm | Av = 0} is the
kernel of A and R(A) = {Av ∈Rn | v ∈Rm} its range. Therefore, if n = m it
follows that N(A) = {0} implies R(A) = Rn and viceversa: in other words, from
uniqueness one obtains existence and viceversa.
Another interesting and well-known result is a characterization of the range of
A, given by R(A) = N(AT )⊥(see Exercise 7.2).
We want to understand if something of this type is also true in a Hilbert space
V whose dimension is inﬁnite. The answer is provided by the Fredholm alternative.
Before stating the result, we need a deﬁnition.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_7
109

110
7
Additional Results
Deﬁnition 7.1 A linear operator K : X →X, X a Banach space, is said to be
compact if it is bounded and it maps bounded sets into precompact sets (namely,
sets whose topological closure is a compact set).
The following result is the core of Fredholm theory (see, e.g., Evans [6, Theorem 5,
pp. 641–643]).
Theorem 7.1 (Fredholm Alternative) Let V be a Hilbert space and K : V →V
be a compact linear operator. Then:
1. N(I −K) = {0} if and only if R(I −K) = V ;
2. N(I −K) is a ﬁnite dimensional subspace;
3. dimN(I −K) = dimN(I −KT );
4. R(I −K) is closed and therefore R(I −K) = N(I −KT )⊥(see Exercise 7.3).
Let us recall that, if A : V →V is a bounded linear operator, its adjoint operator
AT : V →V is deﬁned as
(AT w, u) = (w, Au)
∀w, v ∈V .
Let us consider the elliptic operator
Lw = −
n

i,j=1
Di(aijDjw) +
n

i=1
biDiw + a0w ,
with aij ∈L∞(D) for i, j = 1, . . . , n, bi ∈L∞(D) for i = 1, . . ., n, a0 ∈L∞(D).
The formal adjoint LT is deﬁned by
LT w = −
n

i,j=1
Di(ajiDjw) −
n

i=1
Di(biw) + a0w .
The bilinear form BL(·, ·) is deﬁned as
BL(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx ,
while the adjoint bilinear form is deﬁned by
BLT (w, v) =

D
n

i,j=1
ajiDjwDivdx +

D
n

i=1
biwDivdx +

D
a0wvdx ,
where integration by parts has been applied not only to the second order term but
also to −
D Di(biw)vdx. Consequently,
BLT (w, v) = BL(v, w)
∀v, w ∈H 1(D) .

7.1
Fredholm Alternative
111
Let us start focusing on the homogeneousDirichlet boundary condition. As usual,
we will say that u is a weak solution of Lu = f with homogeneous Dirichlet
boundary value if u ∈H 1
0 (D) is a solution of
BL(u, v) =

D
f vdx
∀v ∈H 1
0 (D) .
Similarly, we will say that w is a weak solution of LT w = p with homogeneous
Dirichlet boundary value if w ∈H 1
0 (D) is a solution of
BLT (w, v) =

D
pvdx
∀v ∈H 1
0 (D) .
Theorem 7.2 (Existence and Uniqueness Theorem Based on Fredholm Alterna-
tive) Let D ⊂Rn be a bounded, connected, open set.
(i) Precisely one of the following statements holds:
(α) either for each f ∈L2(D) there exists a unique solution u ∈H 1
0 (D) of
BL(u, v) =

D
f vdx
∀v ∈H 1
0 (D) ,
(7.1)
(β) or else there exists a solution w ∈H 1
0 (D), w ̸= 0, of
BL(w, v) = 0
∀v ∈H 1
0 (D) .
(7.2)
The dichotomy (α), (β) is called the Fredholm alternative.
(ii) Furthermore, when assertion (β) holds, the dimension of N(L), the space of
the solution of the problem (7.2) is ﬁnite, and it is equal to the dimension of
N(LT ), the space of the solutions of the problem
BLT (w, v) = 0
∀v ∈H 1
0 (D) .
(iii) Finally, when assertion (β) holds, problem (7.1) has a solution if and only if

D
f v∗dx = 0
∀v∗∈N(LT ) .
Proof
(i) Choose τ > 0 in a such a way that
Bτ (w, v) = BL(w, v) + τ

D
wvdx

112
7
Additional Results
is coercive in H 1
0 (D). We have seen in Sect. 5.3 that this is possible choosing
τ > max(0, −μ) ,
where μ = infD a0 −
1
2α0 ∥b∥2
L∞(D). Then for each q ∈L2(D) there exists a
unique solution u⋆∈H 1
0 (D) of
Bτ (u⋆, v) =

D
qvdx
∀v ∈H 1
0 (D) .
(7.3)
Let us write u⋆= (L + τI)−1q whenever (7.3) holds. Indeed (7.3) is the weak
form of Lu⋆+ τu⋆= q.
Now observe that u ∈H 1
0 (D) is a solution of (7.1) if and only if
Bτ(u, v) =

D
(τu + f )vdx
∀v ∈H 1
0 (D) ,
namely, if and only if
u = (L + τI)−1(τu + f ) = τ(L + τI)−1u + (L + τI)−1f .
Let us write this as
u −Ku = (L + τI)−1f ,
where K = τ(L + τI)−1. We have thus found that a solution u ∈H 1
0 (D) ⊂
L2(D) to (7.1) is a solution of u −Ku = h, with a right hand side h =
(L + τI)−1f ∈H 1
0 (D) ⊂L2(D).
On the other hand, let us take a solution ˆu ∈L2(D) of ˆu −K ˆu = h with
h ∈L2(D), namely, we have
ˆu −τ(L + τI)−1 ˆu = h .
If we know the additional information that h ∈H 1
0 (D), then ˆu = τ(L +
τI)−1 ˆu + h ∈H 1
0 (D). Moreover, we can rewrite the problem as (L + τI)ˆu −
τ ˆu = (L + τI)h or simply Lˆu = (L + τI)h. Therefore, choosing h = (L +
τI)−1f = 1
τ Kf with f ∈L2(D), the two problems Lu = f with u ∈H 1
0 (D)
and (I −K)u = h with u ∈L2(D) are equivalent.
We claim that K : L2(D) →L2(D) is a linear and compact operator. In
fact, from the coerciveness of Bτ (·, ·) for the solution u⋆of (7.3) we have
α∥u⋆∥2
H 1(D) ≤Bτ(u⋆, u⋆) =

D
qu⋆dx ≤∥q∥L2(D)∥u⋆∥L2(D) ≤∥q∥L2(D)∥u⋆∥H 1(D) ,

7.1
Fredholm Alternative
113
hence, being u⋆= (L + τI)−1q, K = τ(L + τI)−1 and Kq = τu⋆,
∥Kq∥H 1(D) ≤τ
α ∥q∥L2(D) .
(7.4)
In particular we have
∥Kq∥L2(D) ≤∥Kq∥H 1(D) ≤τ
α ∥q∥L2(D) ,
that proves the boundedness of K. Moreover estimate (7.4) and Rellich
Theorem 6.9, (that in H 1
0 (D) is valid without assumptions on ∂D, as we can
freely use the trivial extension by 0 outside D) tell us that K is compact.
We now apply the Fredholm alternative that states that
N(I −K) = {0} if and only if R(I −K) = L2(D) .
In other words
(α) we always ﬁnd u ∈L2(D), solution of u −Ku = h ∈L2(D), and u is
unique
or
(β) N(I −K) is not trivial and has ﬁnite positive dimension.
We have already seen that case (α) can be rephrased as follows: choosing h =
(L + τI)−1f , f ∈L2(D), we always ﬁnd u ∈H 1
0 (D) solution of Lu = f .
In case (β) we have that there exists w ∈N(I −K), w ̸= 0; this means
w = Kw, namely,
w = τ(L + τI)−1w ⇐⇒(L + τI)w = τw ⇐⇒Lw = 0 ,
thus w ∈N(L).
(ii) In case (β) we know that dimN(I −K) = dimN(I −KT ) and also that
dimN(I −K) is ﬁnite; since we have just seen that dimN(I −K) = dimN(L),
we obtain that dimN(L) is ﬁnite. Moreover, it is easy to check that KT =
τ(LT + τI)−1 (see Exercise 7.1). Thus, similarly to what proved for the
operator L, we deduce that v ∈N(I −KT ) is equivalent to v ∈N(LT ), and
consequently dimN(LT ) = dimN(I −KT ) = dimN(I −K) = dimN(L).
(iii) Finally, we know that R(I −K) = N(I −KT )⊥. Thus u −Ku = h has a
solution if and only if h ∈N(I −KT )⊥. Let us make explicit this condition:
take v∗∈N(I −KT ), i.e., v∗= KT v∗, and remember that we are interested
in solving the problem for h = 1
τ Kf . Then we can solve the problem if and
only if h satisﬁes
0 =

D
hv∗dx =

D
1
τ Kf v∗dx =

D
1
τ f KT v∗dx =

D
1
τ f v∗dx .

114
7
Additional Results
Thus h = 1
τ Kf ∈N(I −KT )⊥is equivalent to f ∈N(I −KT )⊥, which
means f ∈N(LT )⊥or, explicitly,

D f v∗dx = 0 for all v∗∈N(LT ).
⊓⊔
Exercise 7.1 Prove that in Theorem 7.2 one has KT = τ(LT + τI)−1.
Similar arguments can be used for other boundary value problems. Let us present
how the result can be adapted to the Neumann problem for Laplace operator −.
Let us restrict our attention to the homogeneous case ∇u · n = 0, namely, g = 0.
The weak problem reads:
ﬁnd u ∈H 1(D) :

D
∇u · ∇vdx =

D
f vdx
∀v ∈H 1(D) .
(7.5)
Theorem 7.3 (Existence and Uniqueness Theory for the Neumann Problem)
Assume that D ⊂Rn is a bounded, connected and open set, with a Lipschitz
continuous boundary ∂D. There exists a weak solution w ∈H 1(D), w ̸= 0, of

D
∇w · ∇vdx = 0
∀v ∈H 1(D) .
(7.6)
The dimension of the space of such solutions is 1, and problem (7.5) has a solution
if and only if

D
f dx = 0 .
Proof We can repeat the procedure used for the homogeneous Dirichlet boundary
value problem. We can introduce the operator K = τ(L + τI)−1, from L2(D)
to H 1(D), and prove that K is compact from L2(D) into itself (the regularity
of the boundary ∂D assures that the Rellich theorem is valid in H 1(D)). Then
Fredholm alternative can be applied, and in this case we see that there are non-
trivial solution of the homogeneous problem. In fact, a weak solution w of (7.6)
must satisfy

D |∇w|2dx = 0, hence w is a constant. Note now that the bilinear
form

D ∇w·∇vdx is symmetric, thus the adjoint problem coincides with the given
problem, and therefore the solutions of the homogeneous adjoint problem are only
constants. Then from the Fredholm alternative theorem applied to this problem we
have that (7.5) has a solution if and only if

D
f ωdx = 0
for all the solutions ω of the homogeneous adjoint problem, thus for all the constants
ω ∈R. This is equivalent to

D f dx = 0.
⊓⊔
As ﬁnal remark, let us note that for a weak solution w of (7.6) the conclusion
w = 0 follows if we require 
D wdx = 0; thus with this additional condition the

7.2
Spectral Theory
115
solution of problem (7.5) is unique. We have already proved this result: if

D f dx =
0 there is a solution of (7.5) and it is unique in H 1
∗(D) = {v ∈H 1(D) |

D v = 0}
(see Sect. 5.4).
Exercise 7.2 Let A be a n × m matrix, associated to the linear map v →Av,
v ∈Rm, Av ∈Rn. Prove that R(A) = N(AT )⊥.
Exercise 7.3 Let A : X →Y be a linear and bounded operator, X and Y Hilbert
spaces. Deﬁne the adjoint operator AT : Y →X as (AT y, x)X = (y, Ax)Y for all
y ∈Y, x ∈X. Prove that
(i) R(A) = N(AT )⊥
(ii) R(A)⊥= N(AT ).
7.2
Spectral Theory
Deﬁnition 7.2 Let V be a Banach space and A : V
→V a bounded linear
operator.
(i) The resolvent set of A is
ρ(A) = {η ∈R | A −ηI is one-to-one and onto} .
(ii) The spectrum of A is
σ(A) = R \ ρ(A) .
(iii) η ∈σ(A) is an eigenvalue of A if N(A −ηI) ̸= {0}.
(iv) If η is an eigenvalue of A, any w ∈V , w ̸= 0, satisfying
Aw = ηw
is an associated eigenvector.
Theorem 7.4 (Spectrum of a Compact Operator) Let V be a Hilbert space and
assume that dim V = +∞. Let K : V →V be a linear and compact operator.
Then
(i) 0 ∈σ(K).
(ii) If η ̸= 0 belongs to σ(K), then η is an eigenvalue of K.
(iii) The eigenvalues η ̸= 0 are either the empty set, or a ﬁnite set, or a sequence
tending to 0.
(iv) If η ̸= 0 is an eigenvalue, then dim N(K −ηI) < +∞.
We now apply this general theorem to a boundary value problem. We focus on
the homogeneous Dirichlet boundary condition.

116
7
Additional Results
Theorem 7.5 Let D be a bounded, connected and open set in Rn. There exists an
at most countable set  ⊂R such that the problem
u ∈H 1
0 (D) : BL(u, v) = λ

D
uvdx +

D
f vdx
∀v ∈H 1
0 (D)
(7.7)
has a unique solution for each f ∈L2(D) if and only if λ /∈. Moreover, if  is
inﬁnite, then  = {λk}∞
k=1 with λk →+∞. In particular, λk can be reordered in a
non-decreasing way, with λ1 ≤λ2 ≤λ3 ≤. . . .
Proof Choose τ > 0 in such a way that
Bτ (w, v) = BL(w, v) + τ

D
wvdx
is coercive in H 1
0 (D). We have seen in Sect. 5.3 that this is possible choosing
τ > max(0, −μ) ,
where μ = infD a0 −
1
2α0 ∥b∥2
L∞(D) (let us note that there we wrote σ instead of τ,
but now σ denotes the spectrum... ).
For λ = −τ we know that (7.7) has a unique solution, as Bτ is coercive. Thus
let us assume from now on that λ ̸= −τ. According to the Fredholm alternative, we
know that problem (7.7) has a unique solution for each f ∈L2(D) if and only if
the only solution of
BL(u, v) = λ

D
uvdx
∀v ∈H 1
0 (D)
is u = 0 (see Theorem 7.2). This means that u = 0 is the only solution to
BL(u, v) + τ

D
uvdx = (τ + λ)

D
uvdx
∀v ∈H 1
0 (D) .
We can rewrite this relation as
u = (L + τI)−1(τ + λ)u = τ + λ
τ
Ku ,
having set K = τ(L + τI)−1. We have already proved that K : L2(D) →L2(D)
is linear and compact. Thus its spectrum is given by 0 (in fact K is not onto form
L2(D) to L2(D): H 1
0 (D) is a subspace of L2(D), strictly contained in it) and by
eigenvalues. Thus u = 0 is the only solution to Ku =
τ
τ+λu if and only if
τ
τ+λ is
not an eigenvalue of K. The eigenvalues ηk ̸= 0 of K are either the empty set, or a
ﬁnite set or a sequence convergent to zero. In the last case, from
ηk =
τ
τ + λk

7.2
Spectral Theory
117
we get
λk = τ 1 −ηk
ηk
.
We want now to show that ηk > 0. Being ηk an eigenvalue of K, we have
Kwk = ηkwk , wk ̸= 0 ,
which is equivalent to
τ(L + τI)−1wk = ηkwk ⇐⇒τwk = ηk(L + τI)wk ⇐⇒(L + τI)wk = τ
ηk
wk .
This means
Bτ (wk, v) = τ
ηk

D
wkvdx
and from the coerciveness of Bτ (· , ·) we get
α∥wk∥2
H 1(D) ≤Bτ(wk, wk) = τ
ηk

D
w2
kdx ≤τ
ηk
∥wk∥2
H 1(D) .
Thus τ
ηk ≥α > 0, hence ηk > 0 (and consequently λk > −τ). In conclusion
ηk →0+
and
λk = τ 1 −ηk
ηk
→+∞,
which is the stated result.
⊓⊔
Exercise 7.4 Under the assumptions of Theorem 7.5, take λ ̸∈ and for each
f ∈L2(D) let u ∈H 1
0 (D) be the unique solution of (7.7). Prove that the solution
operator Sλ : f →u is a bounded operator in L2(D), namely, there exists a constant
C > 0 such that
∥u∥L2(D) ≤C∥f ∥L2(D) .
Exercise 7.5 Under the assumptions of Theorem 7.5, take λ ̸∈ and for each
f ∈L2(D) let u ∈H 1
0 (D) be the unique solution of (7.7). Prove that the solution
operator Sλ : f →u is a bounded operator from L2(D) to H 1
0 (D), namely, there
exists a constant C > 0 such that
∥u∥H 1(D) ≤C∥f ∥L2(D) .
Another important result is the following.

118
7
Additional Results
Theorem 7.6 (Spectrum of a Compact and Self-Adjoint Operator) Let V be a
separable Hilbert space and let K : V →V be a linear, compact and self-adjoint
operator. Then there exists an (at most) countable orthonormal basis of V consisting
of eigenvectors of K; in particular, if dim V = +∞the eigenvectors of K are an
inﬁnite sequence, and if moreover dim N(K) < +∞the eigenvalues of K are an
inﬁnite sequence.
As a consequence, it holds:
Theorem 7.7 (Spectrum of a Symmetric Elliptic Operator) Let D be a bounded,
connected and open subset of Rn. Let the coefﬁcients of the operator L be bounded
and satisfy aij = aji for i, j = 1, . . ., n, bi = 0 for i = 1, . . . , n. Then there
exist an inﬁnite sequence {λk}∞
k=1 of eigenvalues of L and a countable L2(D)-
orthonormal basis {wk}∞
k=1 given by eigenvectors of L with homogeneous Dirichlet
boundary condition, namely, solutions wk ∈H 1
0 (D) of
BL(wk, v) = λk

D
wkvdx
∀v ∈H 1
0 (D) .
The eigenvectors
ωk =
wk
√λk + τ
are an orthonormal basis of H 1
0 (D) with respect to the scalar product given by
Bτ (w, v) = BL(w, v) + τ

D
wvdx ,
where τ > 0 is such that Bτ (w, v) is coercive in H 1
0 (D).
Proof We know that L2(D) is a separable Hilbert space; furthermore, we have
already seen that the operator K = τ(L + τI)−1 is compact in L2(D), whose
dimension is inﬁnite, and we trivially see that N(K) = {0}. Moreover, from
aij = aji and bi = 0 we see that K is also self-adjoint. Indeed we have that
Lv = −
n

i,j=1
Di(aijDjv) +
n

i=1
biDiv + a0v
LT v = −
n

i,j=1
Di(ajiDjv) −
n

i=1
Di(biv) + a0v

7.2
Spectral Theory
119
and so L = LT . Thus there exists a sequence of eigenvalues ηk and eigenfunctions
wk of K such that wk are an orthonormal basis in L2(D). Let us see what is the
meaning of this statement. We have wk ∈L2(D), wk ̸= 0, such that Kwk = ηkwk;
this is equivalent to
τ(L + τI)−1wk = ηkwk ⇐⇒τwk = ηk(L + τI)wk
⇐⇒(L + τI)wk = τ
ηk
wk ⇐⇒Lwk = τ 1 −ηk
ηk
wk ,
thus wk are the eigenvectors of L corresponding to the eigenvalues λk = τ 1−ηk
ηk
>
−τ. Coming back to the bilinear forms, we see that
Bτ(wk, v) = BL(wk, v) + τ

D
wkvdx = (λk + τ)

D
wkvdx
∀v ∈H 1
0 (D) .
Thus
Bτ(wk, wj) = (λk + τ)

D
wkwjdx = (λk + τ)δkj .
In conclusion,
ωk =
wk
√λk + τ
is an orthonormal system with respect to the scalar product Bτ (· , ·) in H 1
0 (D).
For verifying that it is a basis, it is sufﬁcient to see that if v ∈H 1
0 (D) satisﬁes
Bτ(v, ωk) = 0 for every k ≥1, then it follows v = 0. This is true as
0 = Bτ (v, ωk) = BL(v, ωk) + τ

D
vωkdx = (λk + τ)

D
vωkdx ,
thus

D vwkdx = 0 for every k ≥1. Since wk is an orthonormal basis in L2(D) it
follows that v = 0.
⊓⊔
Exercise 7.6 Prove that the minimum eigenvalue λ1 of the Laplace operator −
associated to the homogeneous Dirichlet boundary condition is equal to
1
CD , where
CD =
sup
v∈H 1
0 (D),v̸=0

D v2dx

D |∇v|2dx
is the “best” Poincaré constant (see Sect. 6.2).

120
7
Additional Results
Exercise 7.7
(i) Consider the elliptic operator
Lw = −
n

i,j=1
Di(aijDjw) + a0w ,
with aij = aji and a0 ≥0. If λ⋆is an eigenvalue of L associated to anyone
of the boundary conditions of Dirichlet, Neumann, mixed or Robin type, then
λ⋆≥0.
(ii) The case λ⋆= 0 is possible if and only if the boundary condition is of Neumann
type and a0 = 0. In that case the corresponding eigenvector w⋆is a constant
(different from 0).
7.3
Maximum Principle
A peculiar property of a solution of an elliptic problem is that, under suitable
assumptions, its values on the boundary ∂D are a bound for its values in the interior
D. Just to propose a simple physical example, one can think to an elastic membrane
ﬁxed on the boundary: looking for the position u in the vertical direction, the
simplest model is given by the solution of the Poisson equation −u = f , where
f is the external force. When the membrane is charged by a load (thus f ≤0), the
values of u on the boundary are higher than its values inside (or viceversa, if you
pushes it from below, with f ≥0).
We start by underlying a clear fact: for a function v ∈H 1(D) the meaning of
v ≥0 on ∂D is that its trace v|∂D ∈L2(∂D) satisﬁes v|∂D ≥0 (since we are
considering the trace v|∂D, we have to assume that the boundary ∂D is Lipschitz
continuous).
Another remark is that it is possible to see that v+ = max(v, 0) and v−=
max(−v, 0) belong to H 1(D) for v ∈H 1(D) (see Exercise 7.8); moreover, v ≤v+
and v ≥−v−almost everywhere in D. A consequence is the fact that the statement
v ≥0 on ∂D can to be interpreted as v−∈H 1
0 (D); similarly v ≤0 on ∂D means
v+ ∈H 1
0 (D).
Exercise 7.8 Prove that v+ = max(v, 0) and v−= max(−v, 0) belong to H 1(D)
for v ∈H 1(D). More precisely, deﬁning
w+
i =

Div
where v > 0
0
where v ≤0
, w−
i =

Div
where v < 0
0
where v ≥0
,
one has Div+ = w+
i and Div−= w−
i , i = 1, . . ., n.

7.3
Maximum Principle
121
We need now a deﬁnition. We say that u ∈H 1(D) satisﬁes Lu ≤0 on D if
BL(u, v) ≤0
∀v ∈H 1
0 (D) , v ≥0 a.e. in D .
Deﬁnition 7.3 If u ∈H 1(D) satisﬁes Lu ≤0 in D, then it is called subsolution of
L. A function u ∈H 1(D) is called supersolution of L if −u is a subsolution of L
(namely, if BL(u, v) ≥0 for all v ∈H 1
0 (D), v ≥0 a.e. in D).
Theorem 7.8 Let D ⊂Rn be a bounded, connected and open set with a Lipschitz
continuous boundary ∂D. Let L be the elliptic operator
Lv = −
n

i,j=1
Di(aijDjv) +
n

i=1
biDiv + a0v ,
with bounded coefﬁcients aij, bi and a0. Assume that a0 ≥0 a.e. in D. Then:
(i) if u is a subsolution of L we have
sup
D
u ≤sup
∂D
u+ ;
in particular, if u ≤0 on ∂D (thus u+ ∈H 1
0 (D)) it follows u ≤0 a.e. in D;
(ii) if u is a supersolution of L we have
inf
D u ≥inf
∂D(−u−) ;
in particular, if u ≥0 on ∂D (thus u−∈H 1
0 (D)) it follows u ≥0 a.e. in D.
Proof Let us give the proof under the assumption that the weak divergence divb
exists and satisﬁes div b ≤0 a.e. in D. The proof for the general case can be found
in Gilbarg and Trudinger [8, Theorem 8.1, p. 168].
(i) Let u be a subsolution of L. Then

D
n

i,j=1
aijDjuDivdx ≤−

D
n

i=1
biDiu vdx −

D
a0uvdx
for each v ∈H 1
0 (D), v ≥0 a.e. in D. Set M = sup∂D u+, which clearly
is ≥0 (this is an important point in the proof). We can suppose M < +∞,
otherwise we would have sup∂D u+ = +∞and nothing has to be proved. Take
v = max(u −M, 0); clearly v ≥0 a.e. in D and from u ≤M on ∂D we have
v ∈H 1
0 (D). Moreover, note that in the set {u > M} we have v = u −M, thus

122
7
Additional Results
∇v = ∇u; instead, where {u ≤M} one has v = 0 and ∇v = 0. Then we have

D
n

i,j=1
aijDjuDivdx =

{u>M}
n

i,j=1
aijDjuDivdx +

{u≤M}
n

i,j=1
aijDjuDivdx
=

{u>M}
n

i,j=1
aijDjvDivdx =

D
n

i,j=1
aijDjvDivdx
≥α0

D
|∇v|2dx ,
where α0 > 0 is the ellipticity constant. Moreover
−

D
n

i=1
biDiu vdx = −

{u>M}
n

i=1
biDiv vdx = −

D
n

i=1
1
2bi Di(v2)dx
=

D
1
2 div b

≤0
v2dx ≤0
and
−

D
a0uvdx = −

{u>M}
a0uvdx −

{u≤M}
a0uvdx = −

{u>M}
a0uvdx
= −

{u>M}
a0

≥0
u

≥M≥0
(u −M)



≥0
dx ≤0 .
Thus

D
|∇v|2dx ≤0 ,
hence ∇v = 0 in D. Since v ∈H 1
0 (D), it follows v = 0 a.e. in D, hence u ≤M
a.e. in D.
(ii) The proof in the case of u supersolution comes from the fact that −u is a
subsolution and (−u)+ = u−.
⊓⊔
Exercise 7.9 Prove that
sup
∂D
u+ = max(sup
∂D
u, 0)
and
inf
∂D(−u−) = min(inf
∂D u, 0)
(so that the conclusion of Theorem 7.8 can be written as supD u ≤max(sup∂D u, 0)
for a subsolution and infD u ≥min(inf∂D u, 0) for a supersolution).

7.3
Maximum Principle
123
Remark 7.1 Note that in the Theorem 7.8 we cannot substitute sup∂D u+ with
sup∂D u or inf∂D u+ with inf∂D u. The following example can clarify the point:
consider the one dimensional elliptic problem

−u′′ + u = 0
u(−1) = 1 , u(1) = 1 .
(7.8)
To ﬁnd the solution, consider the associated polynomial −r2 + 1, whose roots are
r = 1, r = −1. The general solution of −u′′ + u = 0 is thus given by
u(x) = c1ex + c2e−x .
Imposing the boundary conditions, it follows
c1e−1 + c2e = 1 , c1e + c2e−1 = 1 ,
thus c1 = c2 =
1
e+e−1 , and we ﬁnally obtain
u(x) =
1
e + e−1 (ex + e−x)
(see Fig. 7.1).
Taking the derivative we see that u′(x) =
1
e+e−1 (ex−e−x), which satisﬁes u′ > 0
for x > 0 and u′ < 0 for x < 0, therefore u has its minimum for x = 0 (as it is
also clear from Fig. 7.1). This minimum value is
2
e+e−1 , which is larger than 0 and
smaller than 1. Thus
inf
(−1,1) u =
2
e + e−1 < 1 =
inf
∂(−1,1) u ,
but, as the theorem says,
inf
(−1,1) u =
2
e + e−1 > 0 =
inf
∂(−1,1)(−u−) .
Fig. 7.1 The graph of the
solution
u(x) =
1
e+e−1 (ex + e−x) of
problem (7.8)
−1
−0.5
0.5
1
0.5
1
x
y

124
7
Additional Results
One can revisit this example noting that the solution u satisﬁes u ≥0. Therefore
−u′′ = −u ≤0, and u is a subsolution of the elliptic operator Lv = −v′′.
Therefore the theorem assures that the (positive) maximum is on the boundary, as it
is reasonable for a charged elastic membrane.
Remark 7.2 Instead, if a0 = 0 we can substitute sup∂D u+ with sup∂D u and
inf∂D u+ with inf∂D u. In fact, in this case one can repeat the same proof (again,
for simplicity, with div b ≤0), but now setting M = sup∂D u (which is no longer
assured to be non-negative). Choosing v = max(u −M, 0), the assumptions that u
is a subsolution, that div b ≤0 and that a0 = 0 still yield

D
n

i,j=1
aijDjuDivdx ≤0 ,
and everything goes on as in the previous case.
An interesting consequence is the following result.
Theorem 7.9 (Existence Theorem via Fredholm Alternative) Let D ⊂Rn be a
bounded, connected and open set, with a Lipschitz continuous boundary ∂D. Let L
be an elliptic operator with bounded coefﬁcients aij, bi, a0. Assume that a0 ≥0
almost everywhere in D. Then there exists a unique solution u ∈H 1
0 (D) of the
homogeneous Dirichlet boundary value problem
BL(u, v) =

D
f vdx
∀v ∈H 1
0 (D) .
Proof Let w ∈H 1
0 (D) be a solution with f = 0. Then it is both a subsolution and
a supersolution, thus
0 = inf
∂D(−w−) ≤inf
D w ≤sup
D
w ≤sup
∂D
w+ = 0 ,
hence w = 0 in D. Thus the thesis follows from the Fredholm alternative, see
Theorem 7.2.
⊓⊔
Remark 7.3 The existence and uniqueness of a solution for the homogeneous
Dirichlet boundary value problem has been proved, via coerciveness, if b ∈
W 1,∞(D) and a0 −1
2divb ≥−ν, with ν > 0 and small enough (precisely, such
that α0 −2CDν > 0, with α0 > 0 the ellipticity constant and CD > 0 the Poincaré
constant; see Exercise 5.1). Therefore the two results are not comparable. In one
case b is only assumed to be bounded, but one needs a0 ≥0 in D. In the other
case b is assumed to belong to W 1,∞(D) and to satisfy div b ≤2(a0 + ν), but no
assumption on the sign of a0 in D is required.

7.4
Regularity Issues and Sobolev Embedding Theorems
125
7.4
Regularity Issues and Sobolev Embedding Theorems
7.4.1
Regularity Issues
Let us look back at the existence theorems for the four boundary value problems we
have considered. In all cases, we have found a weak solution u ∈V of
B(u, v) =

D
f vdx
∀v ∈V ,
where V is a inﬁnite dimensional, closed subspace of H 1(D).
Since this is the weak form of the second order elliptic equation
Lu = −
n

i,j=1
Di(aijDju) +
n

i=1
biDiu + a0u = f ,
and the right hand side f belongs to L2(D), we could expect u ∈H 2(D).
Let us show with a formal example that this is reasonable. Suppose that u is a
solution to −u = f in D, and assume that u ∈C∞
0 (D). Then we have

D
(−u)2dx =

D
f 2dx .
Integrating by parts we obtain

D
f 2dx =

D
(−u)2dx =

D
n

i,j
DiDiuDjDjudx
= −

D
n

i,j=1
=DiDj
  
DjDi DiuDjudx =

D
n

i,j=1
DjDiuDiDju
=
n

i,j=1

D
(DiDju)2dx ≥

D
(DkDlu)2dx ,
(7.9)
for any ﬁxed couple of indices k, l = 1, . . . , n. Hence the L2(D)-norm of all the
second order derivatives is bounded by the L2(D)-norm of the right-hand side f .
For a general operator L it is necessary to take into account the regularity of the
coefﬁcients. Rewriting the second order term we have
−
n

i,j=1
Di(aijDju) = −
n

i,j=1
aijDiDju −
n

i,j=1
(Diaij)Dju ,

126
7
Additional Results
thus
−
n

i,j=1
aijDiDju =
n

i,j=1
(Diaij)Dju −
n

i=1
biDiu −a0u + f .
(7.10)
Already knowing that u ∈H 1(D), this suggests that we have to assume
aij ∈C1(D) for i, j = 1, . . . , n
(or simply aij ∈W 1,∞(D)). With this choice the right-hand side in (7.10) belongs
to L2(D), because only products between L∞(D)-functions and L2(D)-functions
appear.
Theorem 7.10 (Interior Regularity) Assume that D ⊂Rn is a bounded, con-
nected and open set. Let u ∈H 1(D) be a weak solution of Lu = f in D,
with f ∈L2(D). Assume that aij ∈C1(D), bi ∈L∞(D), a0 ∈L∞(D) for
i, j = 1, . . . , n. Then u ∈H 2
loc(D) and for each subset Q ⊂⊂D it holds
∥u∥H 2(Q) ≤C(∥f ∥L2(D) + ∥u∥L2(D)) ,
where the constant C > 0 only depends on D, Q and aij, bi, a0.
Proof We only give a brief description of the ideas. There are three steps:
1. To localize the problem into Q use a cut-off function ζ, namely, a C∞-function
with ζ = 1 in Q, ζ = 0 on Rn \ T , 0 ≤ζ(x) ≤1 (here Q ⊂⊂T ⊂⊂D).
2. For w ∈L2(D), k = 1, . . . , n and h ̸= 0 consider the difference quotients
Dh
kw(x) = w(x + hek) −w(x)
h
,
(7.11)
deﬁned in Q ⊂⊂D for 0 < |h| < dist(Q, ∂D).
3. Take as test function in the weak formulation
v = −D−h
k (ζ 2Dh
ku)
and proceed to estimate all the terms.
Two important properties of difference quotients are used: see Exercises 7.10 and
7.11.
⊓⊔
Exercise 7.10 Take v ∈L2(D), ϕ ∈L2(D) with Φ = suppϕ ⊂D, and consider
the difference quotients deﬁned in (7.11). Then we have the integration by parts
formula

D
v Dh
kϕdx = −

D
D−h
k v ϕdx ,
for all h with 0 < |h| < dist(Φ, ∂D), k = 1, . . . , n.

7.4
Regularity Issues and Sobolev Embedding Theorems
127
Exercise 7.11
(i) Take v ∈H 1(D) and consider Q ⊂⊂D. Then the difference quotient Dhv =
(Dh
1v, . . . , Dh
nv) deﬁned in (7.11) satisﬁes
∥Dhv∥L2(Q) ≤∥∇v∥L2(D)
for each h with 0 < |h| < dist(Q, ∂D).
(ii) Take k with 1 ≤k ≤n, v ∈L2(D) and Q ⊂⊂D. Suppose that there exists a
constant C∗> 0 such that
∥Dh
kv∥L2(Q) ≤C∗
for each h with 0 < |h| < dist(Q, ∂D). Then Dkv ∈L2(Q).
(iii) Take k with 1 ≤k ≤n, v ∈L2(D) and suppose there exists a constant C♯> 0
such that
∥Dh
kv∥L2(D|h|) ≤C♯
for each h ̸= 0, where D|h| = {x ∈D | dist(x, ∂D) > |h|}. Then Dkv ∈
L2(D) and ∥Dkv∥L2(D) ≤C♯.
An inductive argument gives:
Theorem 7.11 (Higher Interior Regularity) Assume that D ⊂Rn is a bounded,
connected and open set. Let u ∈H 1(D) be a weak solution of Lu = f in D, with
f ∈H m(D), m ≥1. Assume that aij ∈Cm+1(D), bi ∈Cm(D), a0 ∈Cm(D) for
i, j = 1, . . ., n. Then u ∈H m+2
loc (D), and for each Q ⊂⊂D we have the estimate
∥u∥H m+2(Q) ≤C

∥f ∥H m(D) + ∥u∥L2(D)

,
where the constant C > 0 only depends on m, D, Q and aij, bi, a0.
These regularity results can be extended up to the boundary ∂D. For simplicity,
let us focus on the homogeneous Dirichlet boundary value problem.
Theorem 7.12 (Regularity up to the Boundary) Let the assumptions of the
interior regularity Theorem 7.10 be satisﬁed. Assume moreover that aij ∈C1(D)
and that ∂D is of class C2. Assume that u ∈H 1
0 (D) is a weak solution of Lu = f ,
u|∂D = 0. Then u ∈H 2(D) and it holds
∥u∥H 2(D) ≤C

∥f ∥L2(D) + ∥u∥L2(D)

,
where the constant C > 0 only depends on D and aij, bi, a0.

128
7
Additional Results
Proof As for the interior regularity result, there are some steps.
1. Reduce the problem to a ﬂat boundary by local charts (here the fact that the
boundary ∂D is of class C2 is used).
2. To localize the problem into BR,+ = {x ∈Rn | |x| < R, xn > 0} use a cut-
off function ζ ∈C∞
0 (BR), a function with ζ = 1 in Br, ζ = 0 on Rn \ Bρ,
0 ≤ζ(x) ≤1 (here Br ⊂⊂Bρ ⊂⊂BR).
3. Rewrite the elliptic problem in the half-ball BR,+ and use as test function v the
difference quotient
v = −D−h
k (ζ 2Dh
ku) , k = 1, . . . , n −1 ,
namely, in the directions tangential to the boundary {xn = 0} (this will give a
control on all the second order derivatives in which at least one is tangential).
4. Use the ellipticity of the operator L for estimating the second order normal
derivative DnDnu in terms of the other derivatives (see also Exercise 7.12).
5. Use a partition of unity associated to the constructed covering of D for gluing all
the estimates together.
⊓⊔
Exercise 7.12 Prove that all the terms aii(x) on the diagonal of a uniformly positive
deﬁnite matrix in D (namely, a matrix {aij(x)} such that 
ij aij(x)ηjηi ≥α0|η|2
for all η ∈Rn and almost every x ∈D) satisfy aii(x) ≥α0 for almost every in
x ∈D.
Exercise 7.13 Under the assumptions of Theorem 7.12, the stronger estimate
∥u∥H 2(D) ≤C∥f ∥L2(D)
holds, provided that we know that for each f ∈L2(D) there exists a unique weak
solution u ∈H 1
0 (D).
By induction, we obtain:
Theorem 7.13 (Higher Regularity up to the Boundary) Let the assumption of
Theorem 7.11 be satisﬁed. Assume moreover that aij ∈Cm+1(D), bi ∈Cm(D),
a0 ∈Cm(D) for i, j = 1, . . . , n and that ∂D is of class Cm+2. Assume that u ∈
H 1
0 (D) is a weak solution of Lu = f , u|∂D = 0. Then u ∈H m+2(D) and it holds
∥u∥H m+2(D) ≤C

∥f ∥H m(D) + ∥u∥L2(D)

,
where the constant C > 0 only depends on m, D and aij, bi, a0.
Remark 7.4 Similar results hold for the Neumann and Robin problems, having
assumed a boundary datum g = 0. In the case g ̸= 0 the trace theory for the
derivatives of u and for higher order Sobolev spaces is needed.

7.4
Regularity Issues and Sobolev Embedding Theorems
129
−1
1
−1
1
x
y
−1
1
−1
1
x
y
Fig. 7.2 The sectors Sα for α = 2π
3 (left) and α = 5π
3 (right)
As we have seen, the regularity results require some assumptions on the
smoothness of the boundary and have been stated for Dirichlet, Neumann and Robin
problems. It is interesting to give a couple of examples on the regularity of the
solution in domains with corners and for the mixed problem.
Example 7.1 (Domains with Corners) Consider Sα = {(r, θ) | 0 < r < 1, −α/2 <
θ < α/2} with 0 < α < 2π and α ̸= π (for α = π there are no corners; see Fig. 7.2
for the cases α = 2π
3 and α = 5π
3 ).
Consider
u(r, θ) = r
π
α cos
π
α θ

.
Remember that the Laplace operator in polar coordinates is given by
 = ∂2
r + 1
r ∂r + 1
r2 ∂2
θ
and that the length of the gradient is given by
|∇v|2 = (∂rv)2 + 1
r2 (∂θv)2
(see Exercise 7.14). Thus it is easy to check that

u = 0
in Sα
|∇u|2 = π2
α2 r2( π
α −1)
in Sα .
Moreover for θ = −α/2 and θ = α/2 we have u = 0, and for r = 1 we have u =
cos( π
α θ). Thus u is the solution in Sα of a (non-homogeneous) Dirichlet boundary

130
7
Additional Results
value problem for the Laplace operator, and the boundary datum is a continuous
function on the boundary. Moreover,

Sα
|∇u|2dx =
 α/2
−α/2
dθ
 1
0
π2
α2 r2( π
α −1)rdr = α π2
α2
α
2π = π
2 ,
thus u ∈H 1(Sα). On the other hand |D2u| ∼r
π
α −2 as r ∼0, therefore

Sα
|D2u|2dx ∼
 1
0
r2( π
α −2)rdr =
 1
0
r2 π
α −3dr ,
and this integral is convergent if and only if 3 −2π/α < 1, namely if α < π.
In conclusion, if Sα is convex we have u ∈H 2(Sα); if Sα is not convex we have
u /∈H 2(Sα). Re-entrant corners are a threshold for regularity.
Exercise 7.14 Prove that the Laplace operator in polar is given by
 = ∂2
r + 1
r ∂r + 1
r2 ∂2
θ ,
and that the gradient is given by
Dx1 = cos θ∂r −1
r sin θ∂θ , Dx2 = sin θ∂r + 1
r cos θ∂θ .
Example 7.2 (The Mixed Problem) Consider u = r1/2 sin(θ/2) in S = {(r, θ) | 0 <
r < 1, 0 < θ < π} (see Fig. 7.3). As before, we have u = 0 in S, u|r=1 =
sin(θ/2), u|θ=0 = 0. We have seen in Exercise 7.14 that Dx2u is given by Dx2 =
sin θ∂r + 1
r cos θ∂θ, thus
Dx2u = sin θ
1
2r1/2 sin
θ
2

+ 1
r cos θ r1/2 1
2 cos
θ
2

=
=
1
2r1/2

sin θ sin θ
2 + cos θ cos θ
2

,
Fig. 7.3 The domain S: the
homogeneous Neumann
condition is imposed on the
part of the boundary
represented by a thicker line,
while the Dirichlet condition
is imposed on the remaining
part of the boundary
−1
−0.5
0.5
1
0.5
1
x
y

7.4
Regularity Issues and Sobolev Embedding Theorems
131
which vanishes for θ = π. Therefore u is the solution in S of the mixed problem
for the Laplace operator, with homogeneous Neumann boundary datum on θ = π,
homogeneous Dirichlet boundary datum on θ = 0 and non-homogeneous Dirichlet
boundary datum for r = 1 (note however that the Dirichlet boundary datum is
continuous on the boundary).
We have |∇u|2 = (∂ru)2 + 1/r2(∂θu)2 = 1
4r , thus

S
|∇u|2dx =
 π
0
dθ
 1
0
1
4r rdr = π
4
and u ∈H 1(S). On the other hand, we have
|D2u| ∼r−3/2 as r ∼0 ,
thus

S
|D2u|2dx ∼
 1
0
r−3rdr =
 1
0
r−2dr = +∞
and u /∈H 2(S).
In conclusion, the mixed boundary value problem can have solutions that are
not regular. Note that the singularity has nothing to do with the corners at the points
(1, 0) and (−1, 0). In fact, we can modify S in such a way that it becomes as smooth
as we want at those points, and we can then reconsider this same example in that
smooth domain.
7.4.2
Sobolev Embedding Theorems
An element in the Sobolev space W 1,p(D) has additional “summability” or
“regularity” properties. These properties are usually stated as “Sobolev embedding
theorems”. We will not present here the proofs (for that, see Evans [6, Section 5.6]),
which are not so difﬁcult but present some technicalities: we only underline that the
idea is to prove suitable inequalities for smooth functions, and then use the fact that
smooth functions are dense in W 1,p(D). We divide the ﬁnal statement in two cases:
1 ≤p < n and n < p < +∞.
Theorem 7.14 Let D ⊂Rn be a bounded, connected and open set. Suppose that
∂D is Lipschitz continuous. Assume 1 ≤p < n. Then if u ∈W 1,p(D) it follows
u ∈Lp∗(D), where
1
p∗= 1
p −1
n

132
7
Additional Results
and the estimate
∥u∥Lp∗(D) ≤C∥u∥W 1,p(D)
holds with a constant C > 0 only depending on p, n and D.
Note that p∗> p and p∗< +∞(with p∗→+∞for p →n−).
Example 7.3 Take n = 2 and u ∈W 1,2(D): then u ∈Lq(D) for all q < +∞.
Indeed u ∈W 1,p(D) for an arbitrary p < 2 = n, so that u ∈Lp∗(D) for p∗
converging to +∞as p →2−.
Example 7.4 Take n = 3 and u ∈W 1,2(D): then u ∈L6(D), as
1
p∗= 1
2 −1
3 = 1
6 .
Remark 7.5 We have already seen that |x|−α belongs to W 1,p(B1) (B1 being the
ball centered at 0 with radius 1), provided that p < n and 0 < α < n−p
p . Thus for
p < n, unbounded functions are admitted in W 1,p(D). This is also true for p = n >
1. Consider in fact u(x) = (−log |x|)α for α > 0 and B1/2 = {x ∈Rn | |x| < 1/2}.
We have, writing |x| = r:
|∇u| = α(−log r)α−1|∇log r| = α(−log r)α−1 1
r ,
thus

B1/2
|∇u|ndx ∼
 1/2
0
αn(−log r)(α−1)n 1
rn rn−1dr
= αn
 1/2
0
(−log r)(α−1)n 1
r dr .
Changing variable with t = −log r, dt = −1
r dr, we have

B1/2
|∇u|ndx ∼αn
 +∞
log 2
t(α−1)ndt ,
which is convergent for (α −1)n < −1, namely 0 < α < n−1
n . For these values of
α the unbounded function u(x) = (−log |x|)α belongs to W 1,n(B1/2).

7.4
Regularity Issues and Sobolev Embedding Theorems
133
Let us come now to the second result we want to present. We ﬁrst introduce the
Hölder space Cm,λ(D), with m ≥0, 0 < λ < 1. This is given by the functions
u ∈Cm(D) such that

|α|=m
|Dαu(x1) −Dαu(x2)| ≤K|x1 −x2|λ
∀x1, x2 ∈D ,
where the constant K does not depend on x1 and x2.
Theorem 7.15 Let D ⊂Rn be a bounded, connected and open set. Suppose that
∂D is Lipschitz continuous. Assume n < p < +∞. Then if u ∈W 1,p(D), possibly
modifying it on a set of measure equal to 0 we have u ∈C0,λ(D) with λ = 1 −n
p
and the estimate
∥u∥C0,λ(D) ≤C∥u∥W 1,p(D)
holds with a constant C > 0 only depending on p, n and D.
The norm ∥u∥Cm,λ(D), m ≥0, is given by the sum of ∥u∥Cm(D) and
[u]Cm,λ(D) =
sup
x1,x2∈D, x1̸=x2

|α|=m
|Dαu(x1) −Dαu(x2)|
|x1 −x2|λ
.
Example 7.5 Take n = 2 and u ∈W 1,3(D): then u ∈C0,λ(D) with λ = 1−2
3 = 1
3.
Example 7.6 Take n = 3 and u ∈W 1,6(D): then u ∈C0,λ(D) with λ = 1−3
6 = 1
2.
Clearly, by a simple induction argument one can also obtain immersion theorems
for higher order Sobolev spaces.
Theorem 7.16 Let D ⊂Rn be a bounded, connected and open set. Suppose that
∂D is Lipschitz continuous. Assume u ∈W k,p(D), k ≥2, 1 ≤p < +∞.
1. If pk < n, then u ∈Lq(D), where
1
q = 1
p −k
n
and
∥u∥Lq(D) ≤C∥u∥W k,p(D) ,
with a constant C > 0 only depending on k, p, n and D.
2. If pk > n, then u ∈Ck−[n/p]−1,λ(D), where
λ =

[n/p] + 1 −n/p
if n/p is not an integer
any positive number < 1
if n/p is an integer

134
7
Additional Results
and
∥u∥Ck−[n/p]−1,λ(D) ≤C∥u∥W k,p(D) ,
with a constant C > 0 only depending on k, p, n and D.
Example 7.7 Take n = 3 and u ∈H 2(D) = W 2,2(D): then u ∈C0,λ(D), with
λ = [3/2] + 1 −3/2 = 1/2.
Exercise 7.15 Let D ⊂R3 be a bounded, connected and open set, with a Lipschitz
continuous boundary ∂D. Show that the immersion W 2,2(D) !→C0,1/2(D) holds,
using Theorems 7.14 and 7.15.
Remark 7.6 (About Compactness)
(i) Let p < n. We have seen that
W 1,p(D) !→Lp∗(D)
for p∗=
np
n−p; thus, since D is bounded, we also have
W 1,p(D) !→Lq(D)
for q satisfying p ≤q ≤p∗. It can be proved that this immersion is compact
for p ≤q < p∗(note the strict inequality between q and p∗).
(ii) Let p > n. We have seen that
W 1,p(D) !→C0,λ(D)
for λ = 1 −n/p; thus, since D is bounded, we also have
W 1,p(D) !→C0,μ(D)
for μ satisfying 0 < μ ≤λ. It can be proved that this immersion is compact for
0 < μ < λ (note the strict inequality between μ and λ).
Exercise 7.16
(i) Let D ⊂R3 be a bounded, connected and open set, with a Lipschitz continuous
boundary ∂D. Show that the bilinear form
BL(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx
is bounded provided that the coefﬁcients satisfy aij ∈L∞(D), bi ∈L3(D) and
a0 ∈L3/2(D).

7.5
Galerkin Numerical Approximation
135
(ii) Prove that BL(w, v) is coercive in H 1
0 (D), H 1
∗(D) and H 1
D(D), provided that
∥bi∥L3(D), i = 1, . . . , n, and ∥a0∥L3/2(D) are small enough.
Exercise 7.17 Show that the solution u of the homogeneous Dirichlet boundary
value problem
−u = 1 in D
u|∂D = 0 on ∂D ,
where D = {x ∈Rn | |x| < 1}, belongs to C∞(D).
7.5
Galerkin Numerical Approximation
The general form of the variational problem we have dealt with is:
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(7.12)
where V is an inﬁnite dimensional Hilbert space.
This is a problem with inﬁnitely many “degrees of freedom” (as we need
inﬁnitely many informations for determining a function in an inﬁnite dimensional
Hilbert space). Moreover, very often we have not an explicit formula for represent-
ing the solution. Therefore, in concrete applications it is important to devise an
approximation method to compute a suitable approximate solution.
To this aim, a very popular and efﬁcient idea is to discretize the problem by
projecting it onto a ﬁnite dimensional subspace of V , say VN ⊂V , such that
dimVN = N < +∞. Notice that VN is a Hilbert space because it is a ﬁnite
dimensional subspace.
The approximate problem in VN can be simply formulated as follows:
ﬁnd uN ∈VN : B(uN, vN) = F(vN)
∀vN ∈VN .
(7.13)
Let us assume that ψ1, . . . , ψN is basis of VN: as a consequence of the linearity of
B(·, ·) and F(·) this problem is equivalent to
ﬁnd uN ∈VN : B(uN, ψj) = F(ψj)
∀j = 1, . . . , N .
This is the so-called Galerkin method. Note that it corresponds to the solution of the
linear system
AU = F ,
with uN = N
j=1 Ujψj, Uj ∈R, U = (U1, . . . , UN), A = {Ajl} with Ajl =
B(ψl, ψj) and F = (F(ψ1), . . . , F(ψN)).

136
7
Additional Results
The convergence analysis is very easy, and it is based on the following important
result.
Theorem 7.17 (Céa Theorem) Assume that bilinear form B and the linear func-
tional F satisfy to hypotheses of the Lax-Milgram theorem, i.e., that the following
conditions hold
(i) |B(w, v)| ≤γ ∥w∥V ∥v∥V for γ > 0 [boundedness of B(·, ·)]
(ii) B(v, v) ≥α∥v∥2
V for α > 0 [coerciveness of B(·, ·)]
(iii) |F(v)| ≤M∥v∥V for M > 0 [boundedness of F(·)].
Then by Lax-Milgram theorem in V there exists a unique u ∈V , solution of
the inﬁnite dimensional problem (7.12), and by Lax-Milgram theorem in VN there
exists a unique uN ∈VN, solution of the approximated problem (7.13). Moreover,
the following error estimate holds
∥u −uN∥V ≤γ
α
inf
vN ∈VN
∥u −vN∥V = γ
α dist (u, VN) .
Therefore, the convergence of the Galerkin method follows at once, provided that
for all w ∈V we have that dist (w, VN) →0 as N →∞.
Proof Since B(u, v) = F(v) for all v ∈V , in particular we have that B(u, vN ) =
F(vN) for all vN ∈VN ⊂V . Moreover B(uN, vN) = F(vN) for all vN ∈VN.
Therefore B(u−uN, vN) = 0 for all vN ∈VN. Employing this consistency property,
we easily have that
α∥u −uN∥2
V ≤B(u −uN, u −uN) =
as B(u−uN,uN)=0



B(u −uN, u)
=
as B(u−uN,vN)=0



B(u −uN, u −vN) ≤γ ∥u −uN∥V ∥u −vN∥V
∀vN ∈VN ,
and so we have obtained that
∥u −uN∥V ≤γ
α
inf
vN∈VN
∥u −vN∥V ,
the desired estimate.
⊓⊔
Exercise 7.18 Let D ⊂R3 be a bounded, connected and open set, with a Lipschitz
continuous boundary ∂D. Let V be a closed subspace of H 1(D), and let the
assumptions of Theorem 7.17 be satisﬁed. Suppose moreover that for each w ∈
C0(D) one can ﬁnd πN(w) ∈VN such that ∥w −πN(w)∥V →0 as N →∞. Then
show that the Galerkin method is convergent.
Remark 7.7 One of the most important examples of Galerkin approximation is that
based on ﬁnite elements. For the variational problems described in Chap. 5 the ﬁnite
dimensional subspace VN is given by piecewise-polynomialand globally continuous

7.6
Exercises
137
functions (see Exercise 6.8 for the proof that this is indeed a subspace of H 1(D)).
Here it is assumed that the domain D is the union of (non-overlapping) subsets of
simple shape T , the elements: say, for n = 3, tetrahedra or hexahedra. Denoting by
h the maximum diameter of the elements, let Nh be the dimension of the space
VNh = {v : D →R | v ∈C0(D), v|T ∈Pr ∀T } ,
where Pr is the space of polynomials of degree less than or equal to r, r ≥1.
Thus when h →0 the number of elements T goes to inﬁnity, and therefore one has
Nh →+∞.
For this type of ﬁnite elements one has an error estimate between the exact
solution u and the approximate solution uh that satisﬁes ∥u −uh∥H 1(D) = O(hr)
(having assumed that the hypotheses of Theorem 7.17 are satisﬁed and provided that
the solution u is smooth enough).
7.6
Exercises
Exercise 7.1 Prove that in Theorem 7.2 one has KT = τ(LT + τI)−1.
Solution Let us ﬁrst observe that this result is clearly reasonable, as this would be
the case for a matrix K = τ(L + τI)−1.
Let us write for simplicity (·, ·) instead of (·, ·)L2(D), and for w, v ∈L2(D)
compute (Kw, v): deﬁning by q ∈H 1
0 (D) the solution of (L + τI)q = w (in the
weak sense, Bτ (q, ψ) = (w, ψ) for each ψ ∈H 1
0 (D)), we have
(Kw, v) = (τ(L + τI)−1w, v) = (τq, v) = τ(q, v) .
Then deﬁne by p ∈H 1
0 (D) the solution of (LT + τI)p = v (namely, BLT (p, ψ) +
τ(p, ψ) = (v, ψ) for each ψ ∈H 1
0 (D)) and compute (τ(LT +τI)−1v, w): it holds
(τ(LT + τI)−1v, w) = (τp, w) = τ(p, w) .
Thus we must prove that (q, v) = (p, w). We have
(q, v) =
BLT (p,q)+τ(p,q)



(q, (LT + τI)p) = τ(q, p) +
BLT (p,q)



(q, LT p) = τ(p, q) +
B(q,p)
  
(p, Lq)
and
(p, w) = (p, (L + τI)q)



Bτ (q,p)
= τ(p, q) + (p, Lq)
  
B(q,p)
,

138
7
Additional Results
thus the result
(Kw, v) = (τ(LT + τI)−1v, w)
is proved.
Exercise 7.2 Let A be a n × m matrix, associated to the linear map v →Av,
v ∈Rm, Av ∈Rn. Prove that R(A) = N(AT )⊥.
Solution (⊂) y ∈R(A) means that exists x ∈Rm such that Ax = y. Taking now
w ∈N(AT ), namely, AT w = 0, it is easily checked that (y, w) = (Ax, w) =
(x, AT w) = 0.
(⊃) y ∈N(AT )⊥can be written (as any vector in Rn) as
y = ˆy + Ax,
ˆy ∈R(A)⊥,
x ∈Rm .
Then taking w ∈N(AT ) it follows
( ˆy, w) = (y −Ax, w) = (y, w) −(x, AT w) = 0 ⇒ˆy ∈N(AT )⊥.
Also
(AT ˆy, x) = ( ˆy, Ax) = 0
∀x ∈Rm ⇒AT ˆy = 0 ⇒ˆy ∈N(AT ) .
Since ˆy ∈N(AT ) ∩N(AT )⊥, it follows ˆy = 0 and y = Ax ∈R(A).
Exercise 7.3 Let A : X →Y be a linear and bounded operator, X and Y Hilbert
spaces. Deﬁne the adjoint operator AT : Y →X as (AT y, x)X = (y, Ax)Y for all
y ∈Y, x ∈X. Prove that
(i) R(A) = N(AT )⊥
(ii) R(A)⊥= N(AT ).
Solution
(i) The proof that R(A) ⊂N(AT )⊥is as in Exercise 7.2; since N(AT )⊥is closed,
we have R(A) ⊂N(AT )⊥. On the other hand, let us ﬁrst verify that for a
subspace W ⊂Y it holds W ⊥= W
⊥. In fact, a vector v orthogonal to all the
elements of W is clearly orthogonal to all the elements of W; viceversa, suppose
we have (v, w)Y = 0 for all w ∈W and take w∗∈W: then w∗= limk wk,
wk ∈W, and therefore (v, w∗)Y = limk(v, wk)Y = 0. As a second step,
consider the orthogonal decomposition given by Y = R(A) ⊕R(A)
⊥and take
y ∈N(AT )⊥. We can write y = ˆy + q, where ˆy ∈R(A)
⊥= R(A)⊥and
q ∈R(A). Then we have q = limk qk, qk = Axk ∈R(A). Now the proof is
similar to that of Exercise 7.2: taking z ∈N(AT ) it follows
( ˆy, z)Y = (y −q, z)Y = (y, z)Y −lim
k (Axk, z)Y = −lim
k (xk, AT z)X = 0 ,

7.6
Exercises
139
hence ˆy ∈N(AT )⊥; moreover
(AT ˆy, x)X = ( ˆy, Ax)Y = 0
∀x ∈X ,
thus ˆy ∈N(AT ) and therefore ˆy = 0. In conclusion, y = q ∈R(A).
(ii) Follows at once from (i) by passing to the orthogonal.
Exercise 7.4 Under the assumptions of Theorem 7.5, take λ ̸∈ and for each
f ∈L2(D) let u ∈H 1
0 (D) be the unique solution of (7.7). Prove that the solution
operator Sλ : f →u is a bounded operator in L2(D), namely, there exists a constant
C > 0 such that
∥u∥L2(D) ≤C∥f ∥L2(D) .
Solution We prove that the operator Sλ is closed, thus, being deﬁned on the whole
space L2(D), it is bounded as a consequence of the closed graph theorem (see
Yosida [23, Theorem 1, p. 79]). Take fk →f in L2(D) and uk = Sλfk →q
in L2(D). For a suitable τ > 0 we know that uk is the solution of the coercive
problem
BL(uk, v) + τ

D
ukvdx = (τ + λ)

D
ukvdx +

D
fkvdx
∀v ∈H 1
0 (D) .
(7.14)
Thus by Lax–Milgram theorem we have the estimate
∥uk∥H 1(D) ≤C(∥uk∥L2(D) + ∥fk∥L2(D)) .
Therefore uk is bounded in H 1(D), and since H 1(D) is a Hilbert space we
can extract a subsequence uks which is weakly convergent to w ∈H 1(D) (see
Yosida [23, Theorem 1, p. 126, and Theorem of Eberlein–Shmulyan, p. 141]), in
particular is weakly convergent to w in L2(D). As a consequence of the uniqueness
of the weak limit we obtain q = w, and passing to the limit in (7.14) we ﬁnd
BL(q, v) + τ

D
qvdx = (τ + λ)

D
qvdx +

D
f vdx
∀v ∈H 1
0 (D) .
This shows that q = Sλf , thus Sλ is closed.
Exercise 7.5 Under the assumptions of Theorem 7.5, take λ ̸∈ and for each
f ∈L2(D) let u ∈H 1
0 (D) be the unique solution of (7.7). Prove that the solution
operator Sλ : f →u is a bounded operator from L2(D) to H 1
0 (D), namely, there
exists a constant C > 0 such that
∥u∥H 1(D) ≤C∥f ∥L2(D) .

140
7
Additional Results
Solution In Exercise 7.4 we have seen that u is the solution of the coercive problem
BL(u, v) + τ

D
uvdx = (τ + λ)

D
uvdx +

D
f vdx
∀v ∈H 1
0 (D) ,
τ > 0 being a suitable constant, and that by Lax–Milgram theorem u satisﬁes the
estimate
∥u∥H 1(D) ≤C(∥u∥L2(D) + ∥f ∥L2(D)) .
Thus the result follows from Exercise 7.4.
Exercise 7.6 Prove that the minimum eigenvalue λ1 of the Laplace operator −
associated to the homogeneous Dirichlet boundary condition is equal to
1
CD , where
CD =
sup
v∈H 1
0 (D),v̸=0

D v2dx

D |∇v|2dx
is the “best” Poincaré constant (see Sect. 6.2).
Solution The eigenvalues λk and their related eigenvectors wk ∈H 1
0 (D), wk ̸= 0,
k = 1, 2, . . ., satisfy

D
∇wk · ∇vdx = λk

D
wkvdx
∀v ∈H 1
0 (D) ,
(7.15)
thus λ1 can be represented by the Rayleigh quotient
λ1 =

D |∇w1|2dx

D w2
1dx
and we have at once
λ1 ≥
inf
v∈H 1
0 (D),v̸=0

D |∇v|2dx

D v2dx
=
1
CD
.
On the other hand, knowing that the sequence of eigenvectors wk is an L2(D)-
orthonormal basis (see Theorem 7.7), we can write v = ∞
k=1 vkwk, where vk =

D vwkdx, so that

D
v2dx =

D
 ∞

k=1
vkwk
  ∞

j=1
vjwj

dx =
∞

k=1
v2
k

7.6
Exercises
141
and, using (7.15),

D
|∇v|2dx =

D
 ∞

k=1
vk∇wk

·
 ∞

j=1
vj∇wj

dx
=
∞

k,j=1
vkvj

D
∇wk · ∇wjdx =
∞

k,j=1
vkvjλk

D
wk wjdx =
∞

k=1
v2
kλk
≥λ1
∞

k=1
v2
k .
In conclusion, for any v ∈H 1
0 (D), v ̸= 0,

D
|∇v|2dx

D
v2dx
≥
λ1
∞

k=1
v2
k
∞

k=1
v2
k
= λ1
thus
1
CD
=
inf
v∈H 1
0 (D),v̸=0

D |∇v|2dx

D v2dx
≥λ1 ,
and the thesis is proved.
Exercise 7.7
(i) Consider the elliptic operator
Lw = −
n

i,j=1
Di(aijDjw) + a0w ,
with aij = aji and a0 ≥0. If λ⋆is an eigenvalue of L associated to anyone
of the boundary conditions of Dirichlet, Neumann, mixed or Robin type, then
λ⋆≥0.
(ii) The case λ⋆= 0 is possible if and only if the boundary condition is of Neumann
type and a0 = 0. In that case the corresponding eigenvector w⋆is a constant
(different from 0).
Solution
(i) The eigenvalue λ⋆and the correspondent eigenvector w⋆∈V , w⋆̸= 0, satisfy
B(w⋆, v) = λ⋆

D
w⋆vdx
∀v ∈V ,

142
7
Additional Results
where V and B(·, ·) are the Hilbert space and the bilinear form associated to
the different boundary value problems (see Sect. 5.1). In particular, we have
λ⋆= B(w⋆, w⋆)

D w2⋆dx
and, by the ellipticity assumption (and the assumption that the coefﬁcient κ for
the Robin problem is non-negative) we obtain
B(w⋆, w⋆) ≥BL(w⋆, w⋆) =

D
n
i,j=1 aijDjw⋆Diw⋆dx +

D a0w2
⋆dx
≥α0

D |∇w⋆|2dx +

D a0w2
⋆dx ≥0 .
(ii) When we have λ⋆= 0, from the arguments in (i) we deduce B(w⋆, w⋆) = 0.
Therefore coerciveness and the assumption a0 ≥0 imply w⋆= const. For the
Dirichlet, mixed and Robin boundary value problems this would give w⋆= 0,
a contradiction. (Note that for the Robin problem this follows from the fact that
0 = B(w⋆, w⋆) = BL(w⋆, w⋆) +

∂D
κw2
⋆dSx ,
thus

∂D κw2
⋆dSx = w2
⋆

∂D κdSx = 0, only possible for w⋆= 0.) For the
Neumann boundary condition knowing that w⋆= const has as a consequence

D a0dx = 0, which gives a0 = 0. Finally, it is trivial to show that the Neumann
problem with a0 = 0 has a vanishing eigenvalue correspondent to a constant
eigenvector (different from 0).
Exercise 7.8 Prove that v+ = max(v, 0) and v−= max(−v, 0) belong to H 1(D)
for v ∈H 1(D). More precisely, deﬁning
w+
i =

Div
where v > 0
0
where v ≤0
, w−
i =

−Div
where v < 0
0
where v ≥0
,
one has Div+ = w+
i and Div−= w−
i , i = 1, . . ., n.
Solution Nothing has to be proved if either v > 0 a.e. in D or v ≤0 a.e. in D.
Thus we can consider the case in which both sets {v > 0} and {v ≤0} have positive
measure. Since (v+)2 ≤v2 and (v−)2 ≤v2 it is clear that v+ and v−belong to
L2(D). Let us focus on v+. Deﬁning w+
i ∈L2(D) as above and taking ϕ ∈C∞
0 (D)
we formally have, by integration by parts,

D
w+
i ϕdx =

{v>0}
w+
i ϕdx +

{v≤0}
w+
i ϕdx =

{v>0}
Divϕdx
= −

{v>0}
vDiϕdx +

∂{v>0}
nivϕdSx = −

D
v+Diϕdx ,

7.6
Exercises
143
where

∂{v>0} nivϕdSx = 0 as ∂{v > 0} = (∂{v > 0} ∩D) ∪(∂{v > 0} ∩∂D),
ϕ = 0 on ∂D and we expect that v = 0 on ∂{v > 0} ∩D. However, this formal
proof is not rigorous, as when v ∈H 1(D) is not smooth the set {v > 0} is only
a measurable set, and an integration by parts formula like the one here above is
not necessarily valid. Even assuming that v ∈H 1(D) is smooth does not solve the
problem, as in this situation it is true that the set {v > 0} is an open set and that
v = 0 on ∂{v > 0}, but still this boundary ∂{v > 0} can be as wild as you (do not)
like. Thus we have to change strategy, and for the proof of this exercise and other
related results we refer, e.g., to Kinderlehrer and Stampacchia [10, Theorem A.1, p.
50] or Gilbarg and Trudinger [8, Lemma 7.6, p. 145]. Take into account that it is not
even trivial to prove the following “trivial” result: for v ∈H 1(D) it holds ∇v = 0
a.e. in E = {x ∈D | v(x) = 0}. Its proof is indeed a consequence of the results
provided by this exercise, as v = v+ −v−.
Exercise 7.9 Prove that
sup
∂D
u+ = max(sup
∂D
u, 0)
and
inf
∂D(−u−) = min(inf
∂D u, 0)
(so that the conclusion of Theorem 7.8 can be written as supD u ≤max(sup∂D u, 0)
for a subsolution and infD u ≥min(inf∂D u, 0) for a supersolution).
Solution For the sake of simplicity let us write B
=
sup∂D u+ and A
=
max(sup∂D u, 0). Suppose that sup∂D u > 0 and deﬁne Q = {x ∈∂D | u(x) > 0}:
we have u+ = u in Q and u+ = 0 in ∂D \ Q, thus B = sup∂D u+ = supQ u+ =
supQ u = sup∂D u = A. On the other hand, if sup∂D u ≤0 we have A = 0
and u ≤0 on ∂D, thus u+ = 0 on ∂D and ﬁnally B = 0 = A. The proof of
inf∂D(−u−) = min(inf∂D u, 0) is similar.
Exercise 7.10 Take v ∈L2(D), ϕ ∈L2(D) with Φ = suppϕ ⊂D, and consider
the difference quotients deﬁned in (7.11). Then we have the integration by parts
formula

D
v Dh
kϕdx = −

D
D−h
k v ϕdx ,
for each h with 0 < |h| < dist(Φ, ∂D), k = 1, . . . , n.
Solution Set " = supp ϕ and deﬁne "k
h = {y ∈D | y = x −hek, x ∈"}. Then we
have

D
v(x −hek)ϕ(x)dx =

"
v(x −hek)ϕ(x)dx
=

"k
h
v(y)ϕ(y + hek)dy =

D
v(y)ϕ(y + hek)dy ,

144
7
Additional Results
having used the change of variable y = x −hek. Then it easily follows

D
v(x)ϕ(x + hek) −ϕ(x)
h
dx = −

D
v(x −hek) −v(x)
−h
ϕ(x)dx ,
which is the stated result.
Exercise 7.11
(i) Take v ∈H 1(D) and consider Q ⊂⊂D. Then the difference quotient Dhv =
(Dh
1v, . . . , Dh
nv) deﬁned in (7.11) satisﬁes
∥Dhv∥L2(Q) ≤∥∇v∥L2(D)
for each h with 0 < |h| < dist(Q, ∂D).
(ii) Take k with 1 ≤k ≤n, v ∈L2(D) and Q ⊂⊂D. Suppose that there exists a
constant C∗> 0 such that
∥Dh
kv∥L2(Q) ≤C∗
for each h with 0 < |h| < dist(Q, ∂D). Then Dkv ∈L2(Q).
(iii) Take k with 1 ≤k ≤n, v ∈L2(D) and suppose there exists a constant C♯> 0
such that
∥Dh
kv∥L2(D|h|) ≤C♯
for each h ̸= 0, where D|h| = {x ∈D | dist(x, ∂D) > |h|}. Then Dkv ∈
L2(D) and ∥Dkv∥L2(D) ≤C♯.
Solution
(i) By approximation, we can assume that v is smooth. Take x ∈Q and let ek the
unit vector in the k-th direction. Since
d
dt v(x + thek) =
n

j=1
(Djv)(x + thek) d
dt (xj + thδkj) = h (Dkv)(x + thek) ,
we have
v(x + hek) −v(x) = h
 1
0
(Dkv)(x + thek)dt

7.6
Exercises
145
and consequently

Q(Dh
kv)2(x)dx =

Q
|v(x + hek) −v(x)|2
h2
dx =

Q
 1
0
(Dkv)(x + thek)dt
2
dx
≤

Q
 1
0
(Dkv)2(x + thek)dt

dx =
 1
0

Q
(Dkv)2(x + thek)dx

dt
≤
 1
0

D
(Dkv)2(y)dy

dt =

D
(Dkv)2(x)dx ,
having used the change of variable x + thek = y.
(ii) The idea is to pass to the limit in the integration by parts formula in
Exercise 7.10:

Q
D−1/m
k
v ϕdx = −

Q
v D1/m
k
ϕdx ,
(7.16)
where ϕ ∈C∞
0 (Q) and m is such that 1/m < dist(suppϕ, ∂Q). Since L2(Q)
is a Hilbert space, the estimate ∥Dhv∥L2(Q) ≤C∗for h = −1/m (and m
large enough to have 1/m < dist(Q, ∂D)) has as a consequence that from
the sequence D−1/m
k
v we can extract a subsequence, still denote by D−1/m
k
v,
which converges weakly to wk in L2(Q) (see Yosida [23, Theorem 1, p. 126,
and Theorem of Eberlein–Shmulyan, p. 141]). On the other hand, it is easily
seen that D1/m
k
ϕ converges to Dkϕ in L2(Q): in fact, by Taylor expansion
ϕ(x + hek) −ϕ(x)
h
−Dkϕ(x) = h
2 D2
kϕ(ˆx) ,
where ˆx is between x and x + hek. Thus

Q

ϕ(x + hek) −ϕ(x)
h
−Dkϕ(x)

2
dx ≤(max
D |D2
kϕ|)2 meas(Q) h2
4 .
Passing to the limit in (7.16) we obtain

Q
wk ϕdx = −

Q
v Dkϕdx ,
namely, Dkv = wk ∈L2(Q).

146
7
Additional Results
(iii) From part (ii) we know that the weak derivative Dkv exists in each subset Q
with Q ⊂⊂D and that D−1/m
k
v converges weakly to Dkv in L2(Q). Since the
weak derivatives are unique, by the arbitrariness of Q we deduce that the weak
derivative Dkv exists in D and moreover it satisﬁes
∥Dkv∥L2(Q) ≤lim inf
m→+∞∥D−1/m
k
v∥L2(Q) ≤C♯,
(see Yosida [23, Theorem 1, p. 120]). If we deﬁne
qk,m =
(Dkv)|D1/m in D1/m
0
in D \ D1/m ,
we readily see that q2
k,m →(Dkv)2 pointwise in D as m goes to +∞and q2
k,m
is an increasing sequence with respect to m. Then by the Beppo Levi monotone
convergence theorem it follows that

D1/m(Dkv)2dx
=

D q2
k,mdx
→

D(Dkv)2dx, thus Dkv ∈L2(D) and ∥Dkv∥L2(D) ≤C♯.
Exercise 7.12 Prove that all the terms aii(x) on the diagonal of a uniformly positive
deﬁnite matrix in D (namely, a matrix {aij(x)} such that 
ij aij(x)ηjηi ≥α0|η|2
for all η ∈Rn and almost every x ∈D) satisfy aii(x) ≥α0 for almost every in
x ∈D.
Solution Take η = e(k), the k-th element of the euclidean basis, k = 1, . . . , n. Then
α0 = α0|e(k)|2 ≤

ij
aij(x)e(k)
j e(k)
i
= akk(x) .
Exercise 7.13 Under the assumptions of Theorem 7.12, the stronger estimate
∥u∥H 2(D) ≤C∥f ∥L2(D)
holds, provided that we know that for each f ∈L2(D) there exists a unique weak
solution u ∈H 1
0 (D).
Solution Knowing that for each f ∈L2(D) there exists a unique weak solution
u ∈H 1
0 (D) means that the solution operator S0 : f →u is well-deﬁned and thus
0 is not an eigenvalue. Then, looking at Exercise 7.4, we know that ∥u∥L2(D) ≤
C∥f ∥L2(D) and therefore from Theorem 7.12 we ﬁnd
∥u∥H 2(D) ≤C∥f ∥L2(D) .
Exercise 7.14 Prove that the Laplace operator in polar coordinates is given by
 = ∂2
r + 1
r ∂r + 1
r2 ∂2
θ ,

7.6
Exercises
147
and that the gradient is given by
Dx1 = cos θ∂r −1
r sin θ∂θ , Dx2 = sin θ∂r + 1
r cos θ∂θ .
Solution Polar coordinates are given by x1 = r cos θ, x2 = r sin θ. Setting

f (r, θ) = f (r cos θ, r sin θ), we have
∂
f
∂r
= ∂f
∂x1
cos θ + ∂f
∂x2
sin θ
1
r
∂
f
∂θ = −∂f
∂x1
sin θ + ∂f
∂x2
cos θ
(here and in the sequel, for the sake of simplicity and with abuse of notation,
we are not writing that the derivatives of f have to be computed at (x, y) =
(r cos θ, r sin θ)). For determining ∂f
∂x1 , multiply the ﬁrst equation by cos θ and the
second one by −sin θ, and add the equations; for determining ∂f
∂x2 , multiply the ﬁrst
equation by sin θ and the second one by cos θ, and add the equations. The ﬁnal result
is
∂f
∂x1
= cos θ ∂
f
∂r −sin θ
r
∂
f
∂θ
∂f
∂x2
= sin θ ∂
f
∂r + cos θ
r
∂
f
∂θ ,
hence D1 = cos θ ∂r −sinθ
r
∂θ and D2 = sin θ ∂r + cosθ
r
∂θ. This permits to compute
the second order derivatives, yielding
∂2f
∂x2
1
= cos θ ∂
∂r

cos θ ∂
f
∂r −sin θ
r
∂
f
∂θ

−sin θ
r
∂
∂θ

cos θ ∂
f
∂r −sin θ
r
∂
f
∂θ

∂2f
∂x2
2
= sin θ ∂
∂r

sin θ ∂
f
∂r + cos θ
r
∂
f
∂θ

+ cos θ
r
∂
∂θ

sin θ ∂
f
∂r + cos θ
r
∂
f
∂θ

.
By straightforward computations we obtain the representation of the Laplace
operator in polar coordinates:
f = ∂2 
f
∂r2 + 1
r
∂
f
∂r + 1
r2
∂2 
f
∂θ2 .
Exercise 7.15 Let D ⊂R3 be a bounded, connected and open set, with a Lipschitz
continuous boundary ∂D. Show that the immersion W 2,2(D) !→C0,1/2(D) holds,
using Theorems 7.14 and 7.15.

148
7
Additional Results
Solution We have that ∇u ∈W 1,2(D), thus, by Theorem 7.14, ∇u ∈L6(D). The
same holds for u, therefore we have u ∈W 1,6(D). Since p = 6 > 3 = n, from
Theorem 7.15 it follows that the Hölder exponent is λ = 1 −3
6 = 1
2, thus u ∈
C0,1/2(D).
Exercise 7.16
(i) Let D ⊂R3 be a bounded, connected and open set, with a Lipschitz continuous
boundary ∂D. Show that the bilinear form
BL(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx
is bounded provided that the coefﬁcients satisfy aij ∈L∞(D), bi ∈L3(D) and
a0 ∈L3/2(D).
(ii) Prove that BL(w, v) is coercive in H 1
0 (D), H 1
∗(D) and H 1
D(D), provided that
∥bi∥L3(D), i = 1, . . . , n, and ∥a0∥L3/2(D) are small enough.
Solution
(i) We have, using Hölder inequality,


D
n

i=1
biDiwvdx
 ≤
n

i=1

D
|bi||Diw||v|dx ≤
n

i=1
∥bi∥L3(D)∥Diw∥L2(D)∥v∥L6(D)
and


D
a0wvdx
 ≤

D
|a0||w||v|dx ≤∥a0∥L3/2(D)∥w∥L6(D)∥v∥L6(D) .
The result follows from the Sobolev embedding Theorem 7.14.
(ii) From the Sobolev embedding Theorem 7.14 we have ∥v∥L6(D) ≤C∥v∥H 1(D);
from the Poincaré inequality, that holds in all the spaces H 1
0 (D), H 1
∗(D) and
H 1
D(D), we have ∥v∥L2(D) ≤
√
CD∥∇v∥L2(D). Therefore it holds ∥v∥L6(D) ≤
C∗∥∇v∥L2(D). Then we have found
BL(v, v) ≥
+
α0 −C∗

n

i=1
∥bi∥2
L3(D)
1/2 −C2
∗∥a0∥L3/2(D)
,
∥∇v∥2
L2(D) ,
and the result follows.

7.6
Exercises
149
Exercise 7.17 Show that the solution u of the homogeneous Dirichlet boundary
value problem
−u = 1 in D
u|∂D = 0 on ∂D ,
where D = {x ∈Rn | |x| < 1}, belongs to C∞(D).
Solution The coefﬁcients of the operator and the right hand side are constant and
the boundary is a C∞-manifold, thus by the regularity result in Theorem 7.13 we
see that u ∈H m+2(D) for any m ≥0. Therefore by the Sobolev embedding
Theorem 7.16 we deduce u ∈Cm+1−[n/2](D) for any m ≥[n/2] −1, hence
u ∈C∞(D).
Exercise 7.18 Let D ⊂R3 be a bounded, connected and open set, with a Lipschitz
continuous boundary ∂D. Let V be a closed subspace of H 1(D), and let the
assumptions of Theorem 7.17 be satisﬁed. Suppose moreover that for each w ∈
C0(D) one can ﬁnd πN(w) ∈VN such that ∥w −πN(w)∥V →0 as N →∞. Then
show that the Galerkin method is convergent.
Solution Let u ∈V be the exact solution of the problem. By the approximation
Theorem 6.3 for each ϵ > 0 we can ﬁnd u∗∈C∞(D) such that ∥u −u∗∥V ≤ϵ.
Thus, using Theorem 7.17, we have
∥u −uN∥V ≤γ
α
inf
vN∈VN
∥u −vN∥V ≤γ
α ∥u −πN(u∗)∥V
≤γ
α (∥u −u∗∥V + ∥u∗−πN(u∗)∥V ) ≤2 γ
α ϵ
for N large enough.

Chapter 8
Saddle Points Problems
This chapter is devoted to the solution of saddle point problems that can be written
in the abstract form

Au + BT λ = F
Bu = G
for some linear operators A and B, λ having the role of a Lagrangian multiplier
associated to the constraint Bu = G.
The ﬁrst section, concerned with constrained minimization, is divided into two
parts: the ﬁnite dimensional case and the inﬁnite dimensional case. Then we
describe and analyze the Galerkin approximation method for saddle point problems,
and ﬁnally we present some issues of the Galerkin method based on ﬁnite elements.
8.1
Constrained Minimization
This section is divided into two parts, regarding the ﬁnite dimensional and the
inﬁnite dimensional case, respectively. We chose this approach as we believe that
the leading ideas are more easily caught when dealing with vectors. In this way we
hope that the process of extending known results of ﬁnite dimensional linear algebra
to the inﬁnite dimensional case can become an easier task.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_8
151

152
8
Saddle Points Problems
8.1.1
The Finite Dimensional Case
Let us start from a problem in Rn. We have a function f : Rn →R and we want to
minimize it subject to a set of constraints, expressed by g(x) = 0, with g : Rn →
Rm, with m < n. If m = 1, we know that at a minimum point ˆx we must have
∇f (ˆx) = λ∇g(ˆx) ,
where λ ∈R is a Lagrange multiplier. If 1 < m < n, we know that at a minimum
point ˆx we must have
∇f (ˆx) =
m

k=1
λk∇gk(ˆx) ,
where λk ∈R, k = 1, . . . , m, are Lagrange multipliers.
In other words, we can look for the stationary points (i.e., the points where the
gradient vanishes) of the Lagrangian
L(w, μ) = f (w) +
m

k=1
μkgk(w) ;
clearly, we mean stationary points related to derivatives with respect to all the
components of w and μ.
Suppose now we have a quadratic function
f (w) = 1
2(Aw, w) −(F, w) ,
where A is a n × n matrix and F ∈Rn and we denote by (·, ·) the scalar product in
Rn. Let us also consider linear (indeed, afﬁne) constraints
g(w) = Bw −G ,
where B is an m × n matrix and G ∈Rm. Assuming that A is symmetric, it is
well-known that the problem
min
w∈Rn, g(w)=0 f (w)
(8.1)
can be rewritten in the following equivalent matrix form.

8.1
Constrained Minimization
153
Theorem 8.1 Suppose that A is a symmetric matrix. Let u ∈Rn be a solution of
problem (8.1). Then there exists λ ∈Rm such that the couple (u, λ) is a solution to

Au −F + BT λ = 0
Bu −G = 0 .
(8.2)
Proof As explained above, it is enough to take the derivatives of the Lagrangian
L(w, μ) = 1
2(Aw, w) −(F, w) +
m

k=1
μk(Bw −G)k .
Taking the derivative with respect to wj we obtain
∂
∂wj
⎡
⎣1
2
n

i,s=1
Aiswswi −
n

s=1
Fsws
⎤
⎦= 1
2
n

i,s=1
Ais
∂
∂wj
(wswi) −
n

s=1
Fs
∂ws
∂wj
= 1
2
n

i,s=1
Ais(δsjwi + wsδij) −
n

s=1
Fsδsj
= 1
2

n

i=1
Aij

=AT
ji
wi +
n

s=1
Ajsws

−Fj =
AT + A
2
w −F

j
and
∂
∂wj
/ m

k=1
μk(Bw −G)k
0
=
∂
∂wj
/ m

k=1
μk
) n

s=1
Bksws −Gk
*0
=
m

k=1
μk
n

s=1
Bks
∂ws
∂wj
=
m

k=1
μkBkj = (BT μ)j .
Differentiating with respect to μl, l = 1, . . . , m, it easily follows
∂L
∂μl
(w, μ) =
∂
∂μl
) m

k=1
μk(Bw −G)k
*
= (Bw −G)l .
Therefore the Euler equations of the Lagrangian L are

AT +A
2
w −F + BT μ = 0
Bw −G = 0 ,
(8.3)

154
8
Saddle Points Problems
and, having assumed that the matrix A is symmetric, a stationary point (u, λ) of L
satisﬁes problem (8.2).
⊓⊔
We can also show that problems (8.2) and (8.1) are indeed equivalent (provided
that A is not only symmetric but also non-negative deﬁnite). In fact, it holds:
Theorem 8.2 Suppose that A is a symmetric and non-negative deﬁnite matrix. A
solution (u, λ) to (8.2) furnishes a solution u of the minimization problem (8.1).
Proof Take v such that g(v) = 0, namely Bv = G. Then it can be written as
v = u + w, with Bw = 0. We have
1
2(Av, v) −(F, v) = 1
2(A(u + w), u + w) −(F, u + w)
= 1
2(Au, u) + (Au, w) + 1
2(Aw, w) −(F, u) −(F, w)
(A is symmetric)
= 1
2(Au, u) −(F, u) −( BT λ

=F−Au
, w) + 1
2(Aw, w)
= 1
2(Au, u) −(F, u) −(λ, Bw

=0
) + 1
2 (Aw, w)



≥0
≥1
2(Au, u) −(F, u) ,
thus u solves the minimization problem (8.1).
⊓⊔
We can give some additional information on the stationary point (u, λ) of the
Lagrangian L. In fact we have:
Proposition 8.1 Suppose that A is a symmetric and non-negative deﬁnite matrix. A
solution (u, λ) of (8.2) is a saddle point of the Lagrangian
L(w, μ) = 1
2(Aw, w) −(F, w) +
m

k=1
μk(Bw −G)k ,
i.e., it satisﬁes
L(u, η) ≤L(u, λ) ≤L(v, λ)
(8.4)
for each v ∈Rn and η ∈Rm.

8.1
Constrained Minimization
155
Proof Writing v = u + w, we have for each w ∈Rn,
L(u + w, λ) = 1
2(A(u + w), u + w) −(F, u + w) +
m

k=1
λk(B(u + w) −G)k
= 1
2(Au, u)



⋆
+ (Au, w) + 1
2(Aw, w) −(F, u)
  
⋆
−(F, w)
+
m

k=1
λk(Bu −G)k



⋆
+
m

k=1
λk(Bw)k
(A is symmetric)
= L(u, λ)
  
⋆
+(Au −F, w) + 1
2(Aw, w) +
m

k=1
λk
n

s=1
Bksws
= L(u, λ) + (Au −F, w) + 1
2(Aw, w) +
n

s=1
ws
m

k=1
Bks

=BT
sk
λk
= L(u, λ) + (Au −F + BT λ



=0
, w) + 1
2 (Aw, w)



≥0
≥L(u, λ) .
Moreover, for each η ∈Rm
L(u, η) = 1
2(Au, u) −(F, u) +
m

k=1
ηk (Bu −G)k



=0
= 1
2(Au, u) −(F, u) = L(u, λ) ,
and (8.4) is completely proved.
⊓⊔
Example 8.1 In order to show, by means of a ﬁgure, the saddle point structure of a
constrained minimization problem like those we are considering, let us take n = 1,
m = 1, A = 1, B = 2, F = 3 and G = 4. This leads to the Lagrangian L(w, μ) =
1
2w2 −3w + μ(2w −4). The graph of this function is drawn in Fig. 8.1, where it
can be possible to recognize that (2, 1
2) is a saddle point, and that w →L(w, 1
2) has
a minimum at w = 2, while μ →L(2, μ) is constant.
We are now in a position to prove the well-posedness of problem (8.2).
Theorem 8.3 Suppose that A is a positive deﬁnite matrix and that N(BT ) = {0}.
Then (8.2) has a unique solution.

156
8
Saddle Points Problems
0
1
2
3
4 0
0.5
1
−6
−4
−2
0
Fig. 8.1 The graph of the Lagrangian L(w, μ) = 1
2w2 −3w + μ(2w −4)
Proof For a ﬁnite dimensional linear problem existence and uniqueness are equiv-
alent. Let us prove the uniqueness, namely, let us show that if F = 0 and G = 0 in
(8.2) we obtain u = 0 and λ = 0. Take the scalar product of the ﬁrst equation by u:
0 = (Au, u) + (BT λ, u) = (Au, u) + (λ, Bu

=0
)
= (Au, u) ⇒u = 0
(as A is positive deﬁnite).
Since u = 0, we have BT λ = 0, then the assumption N(BT ) = {0} gives λ = 0.
⊓⊔
Remark 8.1 The condition N(BT ) = {0} is necessary for uniqueness. If we had
BT η∗= 0 for η∗̸= 0, from a solution (u, λ) of (8.2) we could construct another
solution (u, λ + η∗).
Remark 8.2 The symmetry of A is not needed in this theorem. On the other hand,
it has been used to show that the solution of the minimization problem (8.1) is a
solution to (8.2) and viceversa (see Theorems 8.1 and 8.2).
Remark 8.3 Giving a deeper look at the proof, we see that it is possible to weaken
a little bit the assumption on A. In fact, the proof of the theorem also works if we
only assume that
(Aw, w) = 0 for w with Bw = 0 implies w = 0 .

8.1
Constrained Minimization
157
8.1.2
The Inﬁnite Dimensional Case
Before entering the problem of how we can extend Theorem 8.3 to Hilbert
spaces having inﬁnite dimension, let pose the following question: in the inﬁnite
dimensional case, do we encounter problems with a structure like (8.2)?
Example 8.2 Consider the Stokes problem
⎧
⎪⎪⎨
⎪⎪⎩
−νu + ∇p = f
in D
div u = 0
in D
u = 0
on ∂D ,
(8.5)
where u is the velocity of a ﬂuid, p is the pressure (indeed, the pressure divided by
the density), ν > 0 a constant (the kinematic viscosity) and f is the acceleration of
the external forces. The constraint div u = 0 represents the incompressibility of the
ﬂuid.
We know that formally ∇is the adjoint operator of −div:

D
∇ϕ · v = −

D
ϕ div v
for ϕ ∈C∞
0 (D) , v ∈C∞
0 (D) .
Then if we call A = −ν ( being the Laplace operator acting on vector functions,
associated with the homogeneous Dirichlet boundary condition) and B = −div (so
that BT = ∇), we rewrite the Stokes problem as

Au + BT p = f
Bu = 0 .
Example 8.3 Consider the elliptic operator (without the ﬁrst order and zero order
terms)
Lϕ = −
n

i,j=1
Di(aijDjϕ)
and deﬁne
qi = −
n

j=1
aijDjϕ , i = 1, . . . , n .
Then the problem

Lϕ = g
in D
ϕ = 0
on ∂D

158
8
Saddle Points Problems
can be rewritten
⎧
⎪⎪⎨
⎪⎪⎩
qi + n
j=1 aijDjϕ = 0
in D , i = 1, . . . , n
n
i=1 Diqi = g
in D
ϕ = 0
on ∂D .
Due to the ellipticity assumption we know that the matrix {aij} is (uniformly)
positive deﬁnite, hence non-singular. If we deﬁne Z = {zij} its inverse matrix,
which is also positive deﬁnite, we have, since n
j=1 zijajs = δis,
n

j=1
zijqj + Diϕ = 0
in D , i = 1, . . . , n .
Thus we have ﬁnally rewritten the problem as a ﬁrst order elliptic system:
⎧
⎪⎪⎨
⎪⎪⎩
Z q + ∇ϕ = 0
in D
−div q = −g
in D
ϕ = 0
on ∂D .
(8.6)
In this case the operator A is not a differential operator, but simply Aq = Zq,
where the matrix Z has entries {zij}. Instead, as before, the operator B is −div and
BT = ∇.
We want to extend to inﬁnite dimensional Hilbert spaces the results in Theo-
rem 8.3; in particular we want to devise which sufﬁcient conditions will take the
place of those appearing there.
Let us present the abstract theory that covers both cases (8.5) and (8.6). It can be
described in two equivalent ways. In the ﬁrst one we are given with two bounded
bilinear forms a : V × V →R and b : V × M →R, where V and M are two
Hilbert spaces. Clearly, these two forms deﬁne two linear and bounded operators
A : V →V ′, B : V →M′, where V ′ and M′ are the dual spaces of V and M,
respectively, namely, the space of linear and bounded operators from V to R and
from M to R, respectively. This is done as follows: for each w ∈V we deﬁne
Aw is the map v →a(w, v)
∀v ∈V
Bw is the map ψ →b(w, ψ)
∀ψ ∈M ;
in this way BT : M →V ′ is deﬁned by saying that, for each μ ∈M, BT μ is the
map v →b(v, μ) for all v ∈V .

8.1
Constrained Minimization
159
The other way around is described by starting from two linear and bounded
operators A : V →V ′ and B : V →M′, and introducing two bilinear and bounded
forms a : V × V →R and b : V × M →R by setting
a(w, v) = ⟨Aw, v⟩
∀w, v ∈V
b(w, ψ) = ⟨Bw, ψ⟩
∀w ∈V, ψ ∈M ,
where ⟨·, ·⟩are the duality pairings between V and V ′ and M and M′ (we use the
same notation for both of them, and the speciﬁc context will permit to identify which
duality pairing is considered). As a consequence, one can also see that BT : M →
V ′ is deﬁned as
⟨BT μ, v⟩= b(v, μ) = ⟨Bv, μ⟩
∀μ ∈M, v ∈V .
We will present and analyze the problem in terms of the operators A, B and BT .
Before going on, a clearer picture of the situation in the inﬁnite dimensional case
can come from a more direct proof of the existence of a solution to problem (8.2).
We can devise a procedure that have three steps, as described here below.
1. Find a solution uG ∈Rn of BuG = G: this requires that the range of B, namely,
the space R(B) = {μ ∈Rm | ∃v ∈Rn such that μ = Bv}, satisﬁes R(B) = Rm.
2. Find ˆu ∈Rn solution to

Aˆu = −BT λ + F −AuG
B ˆu = 0 .
This would require the knowledge of λ. However, if we project the ﬁrst equation
on the kernel N(B) we ﬁnd that
ˆu ∈N(B) : (Aˆu, v) = −(BT λ, v)



=−(λ,Bv)=0
+(F −AuG, v)
∀v ∈N(B) ,
a problem where λ is no longer present. For solvability, here a sufﬁcient
assumption is that A is positive deﬁnite on N(B).
3. Find a solution λ ∈Rm to
BT λ = F −AuG −Aˆu .
Here we have, by the second step, (F −AuG −Aˆu, v) = 0 for all v ∈N(B),
therefore the needed property is that R(BT ) = N(B)⊥.
In the ﬁnite dimensional case we know that the property R(BT ) = N(B)⊥is
always satisﬁed, as well as R(B) = N(BT )⊥(see Exercise 7.2). Thus the existence
of a solution to problem (8.2) follows by assuming that A is positive deﬁnite on
N(B) and that N(BT ) = {0}, so that R(B) = N(BT )⊥= Rm.

160
8
Saddle Points Problems
In this respect, the situation at the inﬁnite dimensional level is somehow different.
First, for a linear and bounded operator K : X →Y, X and Y Hilbert spaces, it is
no longer true that R(K) = N(KT )⊥, as in general the range R(K) is not a closed
subspace in Y (see Sect. 3.1, item 5, and Exercise 7.3; in particular, in the latter it is
proved that R(K)⊥= N(KT ) and R(K) ⊂R(K) = (R(K)⊥)⊥= N(KT )⊥, thus
the equality in this last relation is true if and only if R(K) is closed in Y). Moreover,
here we have to deal with operators B : V →M′ and BT : M →V ′, V ′ and M′
being the dual spaces of V and M, respectively, and it is more suitable to focus in a
more precise way on this speciﬁc situation.
Thus we start with a deﬁnition.
Deﬁnition 8.1 The polar set of N(B) is
N(B)♯= {g ∈V ′ | ⟨g, v⟩= 0
∀v ∈N(B)} .
As seen in Exercise 8.1, N(B)♯can be identiﬁed with a suitable dual space.
Exercise 8.1 N(B)♯can be isometrically identiﬁed with the dual of N(B)⊥.
We are now in a position to “translate” conditions 1, 2 and 3 for the inﬁnite
dimensional case. With respect to condition 2, when considering the Lax–Milgram
theorem 2.1 we have already seen that a natural extension of the assumption that the
matrix A is positive deﬁnite is that the operator A : V →V ′ is coercive, namely,
there exists α > 0 such that ⟨Av, v⟩≥α∥v∥2
V for all v ∈V . However, we have
seen in Remark 8.3 that in the present case it could be sufﬁcient to assume that
coerciveness is satisﬁed only in the kernel of B, namely, it holds ⟨Av, v⟩≥α∥v∥2
V
for all v ∈N(B) = {v ∈V | Bv = 0}.
A remark is in order about condition 3: since the operator A takes values in
the dual space V ′, the relation R(BT ) = N(B)⊥clearly has to be replaced by
R(BT ) = N(B)♯.
Conditions 1 and 3 are strictly related. In fact, by a suitable version of the closed
range theorem (see Yosida [23, Theorem 1, p. 205]) we know that
Theorem 8.4 (Closed Range) Let B : V →M′ be a linear and bounded operator,
where V and M are Hilbert spaces and M′ is the dual space of M. Denote by
BT : M →V ′ the adjoint operator of B, V ′ being the dual space of V . Then
(i) The range R(B) is closed in M′ if and only if the range R(BT ) is closed in V ′.
(ii) The range R(B) is closed in M′ if and only if R(B) = N(BT )♯.
(iii) The range R(BT ) is closed in V ′ if and only if R(BT ) = N(B)♯.
It is now easy to see that, for repeating the ﬁnite dimensional existence procedure,
it is sufﬁcient to assume that A is coercive on N(B), N(BT ) = {0} and R(BT )
is closed in V ′. In fact, in this case from (i) we have that R(B) is closed in M′,
hence from (ii) we see that R(B) = N(BT )♯= M′ and ﬁnally from (iii) we obtain
R(BT ) = N(B)♯. Moreover, from the coerciveness of A in N(B) and N(BT ) = {0}
it follows that the solution is unique.
To this end, the key point is the following result.

8.1
Constrained Minimization
161
Proposition 8.2 Suppose that there exists β > 0 such that
∀μ ∈M ∃vμ ∈V, vμ ̸= 0 : ⟨BT μ, vμ⟩≥β∥μ∥M∥vμ∥V .
(8.7)
Then N(BT ) = {0} and R(BT ) is closed in V ′.
Proof Condition (8.7) clearly says that N(BT ) = {0}. Moreover, in Theorem 2.1
we have already presented an argument that shows that R(BT ) is closed in V ′. Let
us repeat it here for the ease of the reader. From (8.7) we see that for all μ ∈M it
holds
∥BT μ∥V ′ =
sup
v∈V,v̸=0
⟨BT μ, v⟩
∥v∥V
≥⟨BT μ, vμ⟩
∥vμ∥V
≥β∥μ∥M .
(8.8)
Suppose that BT μk →ϕ in V ′, thus BT μk is a Cauchy sequence in V ′ and by
condition (8.8) μk is a Cauchy sequence in M. Since M is a Hilbert space we ﬁnd
μk →μ0 in M and by the continuity of BT it follows BT μk →BT μ0, hence
ϕ = BT μ0.
⊓⊔
Remark 8.4 Condition (8.7) is called inf–sup condition since it can be rewritten as
inf
μ∈M,μ̸=0
)
1
∥μ∥M
sup
v∈V,v̸=0
⟨BT μ, v⟩
∥v∥V
*
=
inf
μ∈M,μ̸=0
sup
v∈V,v̸=0
⟨BT μ, v⟩
∥μ∥M∥v∥V
≥β > 0 .
Exercise 8.2 The inf–sup condition (8.7) is equivalent to each one of the following
conditions:
(a) The operator BT is an isomorphism from M onto N(B)♯and
∃β > 0 : ∥BT μ∥V ′ ≥β∥μ∥M
∀μ ∈M .
(b) The operator B is an isomorphism from N(B)⊥onto M′ and
∃β > 0 : ∥Bv∥M′ ≥β∥v∥V
∀v ∈N(B)⊥.
For the solution of Exercise 8.2 it is useful to use the following result:
Exercise 8.3 Let V be a Hilbert space and F ∈V ′. Show that the norm ∥F∥V ′
deﬁned as
∥F∥V ′ =
sup
v∈V,v̸=0
⟨F, v⟩
∥v∥V

162
8
Saddle Points Problems
is indeed equal to
∥F∥V ′ =
max
v∈V,v̸=0
⟨F, v⟩
∥v∥V
,
namely, there is vF ∈V , vF ̸= 0, such that
∥F∥V ′ = ⟨F, vF ⟩
∥vF ∥V
.
We are now in a position to prove the existence and uniqueness theorem we are
interested in. The problem reads: for each F ∈V ′, G ∈M′, ﬁnd a unique solution
(u, ϕ) ∈V × M of

Au + BT ϕ = F
Bu = G .
(8.9)
Theorem 8.5 Let A be a linear and bounded operator from V to V ′, with ∥A∥= γ .
Let B be a linear and bounded operator from V in M′. Assume that the operator A
is coercive over the kernel of the operator B, namely,
∃α > 0 such that ⟨Av, v⟩≥α∥v∥2
V
∀v ∈N(B) ,
(8.10)
and that the inf–sup condition (8.7) is satisﬁed, namely,
∃β > 0 such that ∀μ ∈M ∃vμ ∈V, vμ ̸= 0 : ⟨BT μ, vμ⟩≥β∥μ∥M∥vμ∥V .
(8.11)
Then there exists a unique solution (u, ϕ) to (8.9). Moreover
∥u∥V ≤1
α ∥F∥V ′ + 1
β

1 + γ
α

∥G∥M′
∥ϕ∥M ≤1
β

1 + γ
α

∥F∥V ′ + γ
β2

1 + γ
α

∥G∥M′ .
Proof Uniqueness is easy: from F = 0 and G = 0 it follows Bu = 0 and from the
ﬁrst equation we get
0 = ⟨Au, u⟩+ ⟨BT ϕ, u⟩= ⟨Au, u⟩+ ⟨ϕ, Bu

=0
⟩,
thus u = 0 from condition (8.10), as u ∈N(B). Hence it follows BT ϕ = 0 and,
taking μ = ϕ in condition (8.11), we obtain ∥ϕ∥M∥vϕ∥V = 0 for vϕ ̸= 0, thus
ϕ = 0.

8.1
Constrained Minimization
163
Now, from Proposition 8.2 and Theorem 8.4 we know that R(B) = M′, thus we
ﬁnd uG ∈N(B)⊥such that BuG = G and moreover
∥uG∥V ≤1
β ∥G∥M′
(see Exercise 8.2(b)). Then we rewrite problem (8.9) as

Aˆu + BT ϕ = F −AuG
B ˆu = 0 ,
(8.12)
with ˆu = u −uG. Taking the pairing with v ∈N(B), we can eliminate ϕ: we ﬁnd
⟨F −AuG, v⟩= ⟨Aˆu, v⟩+ ⟨BT ϕ, v⟩= ⟨Aˆu, v⟩+ ⟨ϕ, Bv

= 0
⟩= ⟨Aˆu, v⟩.
Since we look for ˆu ∈N(B), we can apply Lax–Milgram theorem 2.1 in N(B),
where A is coercive by condition (8.10). Then we have a unique solution ˆu ∈N(B)
of
⟨Aˆu + AuG −F, v⟩= 0
∀v ∈N(B) ,
satisfying
∥ˆu∥V ≤1
α ∥F −AuG∥V ′ .
Setting u = ˆu + uG, we have that
⟨Au −F, v⟩= 0
∀v ∈N(B) ,
thus (Au −F) ∈N(B)♯. From Proposition 8.2 and Theorem 8.4 there exists a
unique ϕ ∈M such that
BT ϕ = F −Au ,
and estimate (8.8) holds, i.e.,
∥ϕ∥M ≤1
β ∥BT ϕ∥V ′s = 1
β ∥Au −F∥V ′ ≤1
β (∥Au∥V ′ + ∥F∥V ′)
≤γ
β ∥u∥V + 1
β ∥F∥V ′ .

164
8
Saddle Points Problems
Thus (u, ϕ) is a solution to problem (8.9). Moreover we have
∥u∥V ≤∥ˆu∥V + ∥uG∥V ≤1
α ∥F −AuG∥V ′ + ∥uG∥V
≤1
α ∥F∥V ′ +

1 + γ
α

∥uG∥V ≤1
α ∥F∥V ′ + 1
β

1 + γ
α

∥G∥M′ .
Concerning ϕ, we easily obtain
∥ϕ∥M ≤1
β ∥F∥V ′ + γ
β ∥u∥V ≤1
β

1 + γ
α

∥F∥V ′ + γ
β2

1 + γ
α

∥G∥M′ ,
which ends the proof.
⊓⊔
Let us come back now to our examples 8.2 and 8.3. We want to show that they can
be written in the general form we have described in Theorem 8.5. The ﬁrst step is the
identiﬁcation of the variational spaces: in case (8.5) we take u ∈V = (H 1
0 (D))n,
so that each component of the velocity vector u belongs to H 1
0 (D), and p ∈M ⊂
L2(D) (M yet to be determined). The reason of this choice is that integrating by
parts we obtain

D
(−νu) · vdx =

D
−ν
n

k,s=1
(DsDsuk)vkdx
=

D
ν
n

k,s=1
DsukDsvkdx −

∂D
ν
n

k,s=1
Dsukns vk

=0
dSx ,
and the last integral vanishes if v ∈(H 1
0 (D))n. Moreover

D
∇p · vdx = −

D
p div vdx +

∂D
pn ·
v

=0
dSx ,
and again the last integral vanishes if v ∈(H 1
0 (D))n, while the ﬁrst integral has a
meaning for p ∈L2(D).
Concerning the second equation div u = 0 in D, it is easily seen that it can be
simply written in weak form as

D
(div u)r = 0
for each r ∈L2(D) .

8.1
Constrained Minimization
165
However, here it is worthy to note that, by the divergencetheorem C.3,

D div vdx =

∂D v · ndSx = 0 for each v ∈(H 1
0 (D))n; namely, div v is orthogonal to the
constants. Therefore, it is sufﬁcient to require that the equation above is satisﬁed
for each r ∈L2
∗(D) =
-
r ∈L2(D) |

D rdx = 0
.
. In conclusion, the right choice
of the pressure space is M = L2
∗(D). Let us note that in (8.5) the pressure p is
determined up to an additive constant: thus this choice permits to select a unique
pressure.
Let us see now which are the variational spaces in case (8.6). Take the scalar
product of the ﬁrst equation in (8.6) by m: integrating in D and integrating by parts
we obtain
0 =

D
Zq · mdx +

D
∇ϕ · mdx
=

D
Zq · mdx −

D
ϕ div mdx +

∂D
ϕ

=0
n · mdSx
=

D
Zq · mdx −

D
ϕ div mdx .
From the second equation in (8.6) we get, for any ψ,

D
(−div q)ψdx = −

D
gψdx .
Thus we need q, m ∈(L2(D))n with div q, div m ∈L2(D), and ϕ, ψ ∈L2(D).
Summing up, in this second case (8.6) we have
V = H(div; D) = {m ∈(L2(D))n | div m ∈L2(D)}
and M = L2(D). It is easy to see that H(div; D) is a Hilbert space with respect to
the scalar product
(q, m)H(div;D) =

D
(q · m + div q div m)dx .
(8.13)
Exercise 8.4 Prove that H(div; D) is a Hilbert space with respect to the scalar
product (8.13).

166
8
Saddle Points Problems
In order to apply Theorem 8.5, let us check if the operator A is coercive over the
kernel of the operator B. In the ﬁrst case (8.5) we have V = (H 1
0 (D))n and
⟨Av, v⟩= ν

D
n

k=1
∇vk · ∇vkdx = ν
n

k=1

D
|∇vk|2dx
= ν
2
n

k=1

D
|∇vk|2dx + ν
2
n

k=1

D
|∇vk|2dx
≥ν
2
n

k=1

D
|∇vk|2dx +
ν
2CD
n

k=1

D
v2
kdx
(Poincaré inequality in H 1
0 (D))
≥α∥v∥2
H 1(D)
where α = min

ν
2,
ν
2CD

, and CD is the Poincaré constant in H 1
0 (D).
We have thus seen that for problem (8.5) the operator A is indeed coercive in V ,
and not only on the kernel of B. A natural question then arises: are there interesting
cases for which the “strong” assumption
(Av, v) ≥α∥v∥2
V ,
α > 0
is not satisﬁed and we really need a weaker assumption? The answer is yes, as the
second example 8.3 shows.
In fact, in case (8.6) we have V = H(div; D) and
⟨Am, m⟩=

D
Zm · mdx
≥α∥m∥2
L2(D)
(Z is positive deﬁnite, uniformly in x ∈D) ,
(8.14)
but this is not enough as the control on

D (div m)2dx is missing. However, we note
that in this case Bm = 0 means

D
div m ψ = 0
for each ψ ∈L2(D); thus it follows at once div m = 0 in D. Summing up, for m
satisfying div m = 0 in D we can rewrite (8.14) as
⟨Am, m⟩≥α∥m∥2
V = α

∥m∥2
L2(D) + ∥div m∥2
L2(D)



=0

,

8.1
Constrained Minimization
167
and we have a control from below in terms of the norm of the space V , namely,
coerciveness is restored in the closed subspace of V given by N(B).
Let us now verify that the condition (8.11) is fulﬁlled for the Stokes problem (8.5)
and the ﬁrst order elliptic system (8.6). Let us start from problem (8.5). We have to
check that for each q ∈L2
∗(D), q ̸= 0 , we can ﬁnd vq ∈(H 1
0 (D))n, vq ̸= 0, such
that
⟨BT q, vq⟩= −

D
q div vq dx ≥β∥q∥L2(D)∥vq∥H 1(D) ,
with a positive constant β not depending on q. Since q is average-free, i.e.,

D qdx = 0, it is known that there exists vq ∈(H 1
0 (D))n such that div vq = −q in
D (with vq ̸= 0, as q ̸= 0) and
∥vq∥H 1(D) ≤c∗∥q∥L2(D)
(see Remark 8.5 here below).
Remark 8.5 There are many ways to prove the result here above, and all of them
require some work. Just to quote a classical result, it is possible to furnish an explicit
formula, at least for a (connected) bounded open set that is star-shaped with respect
to all the points of a ball B0 = B(x0, r0), x0 ∈D, r0 > 0. In this geometrical case,
take w ∈C∞
0 (B0) with 
B0 w dx = 1. For q ∈C∞
0 (D) with 
D q dx = 0, deﬁne
for i = 1, . . . , n
(vq)i(x) = −

D
q(y)
$ xi −yi
|x −y|3
 +∞
0
w

x + t x −y
|x −y|

(|x −y| + t)2 dt
%
dy .
In 1979 Mikhail E. Bogovskii [3] has proved that vq ∈(H 1
0 (D))n and div vq = −q
in D, with ∥vq∥H 1(D) ≤c∗∥q∥L2(D). Since a bounded, connected, open set D with
Lipschitz continuous boundary ∂D is the ﬁnite union of domains that are star-shaped
with respect to all the points of a ball, the result for this general geometrical situation
is obtained by localization. Then by a density argument the result is also extended
to all q ∈L2(D) with 
D q dx = 0.
Let us use the function vq thus determined for checking condition (8.11). We
have
−

D
q div vqdx =

D
q2dx = ∥q∥L2(D)∥q∥L2(D) ≥∥q∥L2(D)
1
c∗
∥vq∥H 1(D) ,
thus we get β = 1/c∗, independent of q.

168
8
Saddle Points Problems
Let us come now to problem (8.6). For any q ∈L2(D), take the solution ϕq ∈
H 1
0 (D) of the weak form of the homogeneous Dirichlet problem

−ϕq = q
in D
ϕq = 0
on ∂D ,
and set vq = ∇ϕq. We have
−div vq = −ϕq = q
in D
and
∥ϕq∥H 1(D) ≤c∗∥q∥L2(D)
by the Lax–Milgram theorem 2.1. Thus
∥vq∥2
H(div;D) = ∥vq∥2
L2(D) + ∥div vq
  
−q
∥2
L2(D) ≤c2
∗∥q∥2
L2(D) + ∥q∥2
L2(D) .
Hence
∥vq∥H(div;D) ≤
1
c∗+ 1 ∥q∥L2(D) ,
and the thesis now follows as in the previous case.
8.2
Galerkin Numerical Approximation
Let us now give a look at the Galerkin numerical approximation. In the present case
we change the notation used in Sect. 7.5, and we take Vh ⊂V , Mh ⊂M, two ﬁnite
dimensional subspaces of dimension NV
h and NM
h , respectively, where h > 0 is a
parameter; for h →0+ one has NV
h →+∞and NM
h →+∞.
Writing the saddle point problem in terms of the bilinear forms, we want to solve
the ﬁnite dimensional problem
uh ∈Vh , ϕh ∈Mh :

a(uh, vh) + b(vh, ϕh) = ⟨F, vh⟩
∀vh ∈Vh
b(uh, ψh) = ⟨G, ψh⟩
∀ψh ∈Mh .
(8.15)
The assumptions assuring well-posedness are:
∃αh > 0 : a(vh, vh) ≥αh∥vh∥2
V
∀vh ∈Nh
(8.16)

8.2
Galerkin Numerical Approximation
169
where Nh = {vh ∈Vh | b(vh, ψh) = 0 ∀ψh ∈Mh} (coerciveness of a(·, ·) on the
discrete kernel of b(·, ·)) and
∃βh > 0 : ∀μh ∈Mh, ∃ˆvh ∈Vh , ˆvh ̸= 0 : b(ˆvh, μh) ≥βh∥μh∥M∥ˆvh∥V
(8.17)
(discrete inf–sup condition for b(·, ·)). In this case, in fact, we can repeat the
procedure that has led to determine the solution (u, ϕ) to problem (8.9).
Note that these two assumptions are not a consequence of conditions (8.10) and
(8.11). Indeed in general Nh ̸⊂N(B) (as Mh is a proper closed subspace of M).
Moreover, from condition (8.11) we know that for each μh ∈Mh ⊂M we can ﬁnd
ˆv ∈V , ˆv ̸= 0, satisfying the desired estimate, but not ˆvh ∈Vh, ˆvh ̸= 0.
8.2.1
Error Estimates
Under assumptions (8.16) and (8.17) it is possible to prove the convergence of the
Galerkin approximation method. This can be done as follows. The ﬁrst step is a
consistency property: since Vh ⊂V , we can take a test function vh ∈Vh in (8.9).
Thus the ﬁrst equations in (8.15) and (8.9) give
a(uh, vh) + b(vh, ϕh) = ⟨F, vh⟩= a(u, vh) + b(vh, ϕ) .
(8.18)
Now we want to make appearing a difference between the approximate solution ϕh
and a test function μh ∈Mh: subtracting from (8.18) b(vh, μh) we ﬁnd
a(uh, vh) + b(vh, ϕh −μh) = a(u, vh) + b(vh, ϕ −μh) .
(8.19)
A similar procedure is in order for the approximate solution uh: take v∗
h ∈Vh such
that b(v∗
h, ψh) = ⟨G, ψh⟩for each ψh ∈Mh. Note that any element of the form
uh + wh, wh ∈Nh, has this property. We will denote by NG
h the afﬁne subspace
{ω∗
h ∈Vh | ω∗
h = uh + wh, wh ∈Nh}: we have thus selected v∗
h ∈NG
h . Subtracting
a(v∗
h, vh) we get
a(uh −v∗
h, vh) + b(vh, ϕh −μh) = a(u −v∗
h, vh) + b(vh, ϕ −μh) .
(8.20)
Taking now vh = uh −v∗
h, it follows
αh∥uh −v∗
h∥2
V ≤a(uh −v∗
h, uh −v∗
h)
= −b(uh −v∗
h, ϕh −μh) + a(u −v∗
h, uh −v∗
h) + b(uh −v∗
h, ϕ −μh) .

170
8
Saddle Points Problems
Since
b(uh −v∗
h, ψh) = ⟨G, ψh⟩−⟨G, ψh⟩= 0
∀ψh ∈Mh ,
the term b(uh −v∗
h, ϕh −μh) vanishes. Therefore we have found
∥uh −v∗
h∥2
V ≤1
αh
γ ∥u −v∗
h∥V 

∥uh −v∗
h∥V + ∥b∥

∥uh −v∗
h∥V ∥ϕ −μh∥M
 .
Thus
∥u −uh∥V ≤∥u −v∗
h∥V + ∥uh −v∗
h∥V
≤∥u −v∗
h∥V + γ
αh
∥u −v∗
h∥V + ∥b∥
αh
∥ϕ −μh∥M
≤

1 + γ
αh

∥u −v∗
h∥V + ∥b∥
αh
∥ϕ −μh∥M ,
(8.21)
for each v∗
h ∈NG
h and for each μh ∈Mh.
For a ﬁxed vh ∈Vh consider now the linear functional ψh →b(u −vh, ψh),
ψh ∈Mh . From condition (8.17) we know that there exists a unique zh ∈N⊥
h such
that
b(zh, ψh) = b(u −vh, ψh)
∀ψh ∈Mh ,
with
∥zh∥V ≤1
βh
sup
ψh∈Mh, ψh̸=0
b(u −vh, ψh)
∥ψh∥M
≤∥b∥
βh
∥u −vh∥V .
Setting w∗
h = zh + vh, we see that
b(w∗
h, ψh) = b(zh + vh, ψh) = b(u, ψh) = ⟨G, ψh⟩
∀ψh ∈Mh .
Thus w∗
h ∈NG
h and
inf
ω∗
h∈NG
h
∥u −ω∗
h∥V ≤∥u −w∗
h∥V ≤∥u −vh∥V + ∥zh∥V
≤

1 + ∥b∥
βh

∥u −vh∥V
∀vh ∈Vh .

8.2
Galerkin Numerical Approximation
171
In conclusion, inserting this estimate in (8.21) we have found the error estimate
∥u −uh∥V ≤

1 + γ
αh
 
1 + ∥b∥
βh

inf
vh∈Vh
∥u −vh∥V + ∥b∥
αh
inf
μh∈Mh
∥ϕ −μh∥M .
(8.22)
The estimate of the error ∥ϕ −ϕh∥M is obtained as follows: by condition (8.17),
in correspondence with ϕh −μh we can ﬁnd vh ∈Vh, vh ̸= 0, such that
b(vh, ϕh −μh) ≥βh∥vh∥V ∥ϕh −μh∥M.
(8.23)
On the other hand from (8.18) we have a(u −uh, vh) + b(vh, ϕ −ϕh) = 0 for each
vh ∈Vh, hence
b(vh, ϕh −μh) = b(vh, ϕh −ϕ) + b(vh, ϕ −μh)
= a(u −uh, vh) + b(vh, ϕ −μh) .
Thus from condition (8.23) we have
∥ϕh −μh∥M ≤1
βh
a(u −uh, vh) + b(vh, ϕ −μh)
∥vh∥V
≤γ
βh
∥u −uh∥V + ∥b∥
βh
∥ϕ −μh∥M .
Finally, we have found
∥ϕ −ϕh∥M ≤∥ϕ −μh∥M + ∥ϕh −μh∥M
≤

1 + ∥b∥
βh

∥ϕ −μh∥M + γ
βh
∥u −uh∥V
∀μh ∈Mh ,
hence
∥ϕ −ϕh∥M ≤

1 + ∥b∥
βh

inf
μh∈Mh
∥ϕ −μh∥M + γ
βh
∥u −uh∥V ,
(8.24)
which, together with (8.22), is the error estimate we wanted to prove.
Remark 8.6 It is evident that a speed of convergence that only depends on the
approximation properties of Vh in V and of Mh in M is achieved if αh ≥α > 0
and βh ≥β > 0, uniformly with respect to the parameter h. Thus the art of the
approximation here is to ﬁnd ﬁnite dimensional subspaces Vh and Mh such that
conditions (8.16) and (8.17) are satisﬁed uniformly with respect to h.

172
8
Saddle Points Problems
8.2.2
Finite Element Approximation
The uniform approximation of V and M by Vh and Mh is possible for many
interesting cases, for instance for V
= (H 1
0 (D))n and M = L2
∗(D) or V
=
H(div; D) and M = L2(D), the spaces related to Examples 8.2 and 8.3 that we
have considered here. To illustrate this fact, let us focus on a very important type of
Galerkin approximation: the ﬁnite element method.
As already noted in Remark 7.7, the main ingredients of a ﬁnite element
approximation are the facts that the domain D is the union of a ﬁnite number of
non-overlapping subsets T of simple shape (say, triangles or tetrahedra) and that the
ﬁnite dimensional spaces Vh and Mh are given by functions whose restrictions to
the elements T are polynomials. The parameter h represents the mesh size, namely,
the maximum diameter of the elements T .
Let us show some examples of ﬁnite elements that satisfy the two conditions
(8.16) and (8.17), focusing on the two-dimensional case. A ﬁrst example for the
Stokes problem described in Example 8.2 is the P2-P0 element, in which the two
components of the velocity are piecewise-quadratic polynomials and the pressure
is a piecewise-constant, therefore a discontinuous function; its degrees of freedom
are point values, at the nodes drawn in Fig. 8.2, left. A second example is the
“mini-element” (P1 ⊕B)-P1, in which the two components of the velocity uh are
linear combination of ﬁrst order polynomials and of a ﬁxed third order polynomial
vanishing on the sides (this is called “a bubble”), and the pressure ϕh is a continuous
piecewise-linear polynomial; its degrees of freedom are point values, at the nodes
drawn in Fig. 8.2, right.
For the ﬁrst order elliptic system presented in Example 8.3 a classical instance is
the Raviart–Thomas element, for which in each element T the vector ﬁeld uh is of
the form a+bx, with a ∈R2 and b ∈R, and the scalar ϕh is a piecewise constant; its
degrees of freedom are point values of the scalar ϕh, at the node drawn in Fig. 8.3,
and ﬂuxes of the vector uh across the sides of T , i.e., integrals of uh · n on the sides.
For all these elements it is proved that the convergence in V × M of the
approximate solutions to the exact solution is linear with respect to the mesh size h.
Fig. 8.2 The degrees of freedom of the P2-P0 element (left) and of the “mini-element” (P1⊕B)-P1
(right): point values for the velocity and for the pressure

8.3
Exercises
173
Fig. 8.3 The degrees of
freedom of the
Raviart–Thomas element:
ﬂuxes for the vector uh and
point values for scalar ϕh
8.3
Exercises
Exercise 8.1 N(B)♯can be isometrically identiﬁed with the dual of N(B)⊥.
Solution Take g ∈(N(B)⊥)′, we deﬁne ˆg ∈V ′ by setting
⟨ˆg, v⟩= ⟨g, P⊥v⟩
∀v ∈V,
where P⊥v is the orthogonal projection on N(B)⊥. Clearly ˆg ∈N(B)♯, as P⊥v =
0 for v ∈N(B). The map g →ˆg from (N(B)⊥)′ to N(B)♯is clearly one-to-one, as
ˆg = g on N(B)⊥. It is also onto: in fact, taking ˜g ∈N(B)♯, we need to verify that
there exists g∗∈(N(B)⊥)′ such that g∗= ˜g. Let us deﬁne g∗∈(N(B)⊥)′ by
⟨g∗, w⟩= ⟨˜g, w⟩
∀w ∈N(B)⊥.
Thus we have g∗= ˜g on N(B)⊥, and also 
g∗= g∗on N(B)⊥, thus 
g∗= ˜g on
N(B)⊥. On the other hand, 
g∗= 0 and ˜g = 0 on N(B), as both of them belong to
N(B)♯, thus 
g∗= ˜g on V .
Finally, for each v ∈V , v ̸= 0, one has ⟨ˆg, v⟩= 0 if v ∈N(B), while for
v ∈N(B)⊥
⟨ˆg, v⟩
∥v∥
= ⟨g, v⟩
∥v∥
≤
sup
w∈N(B)⊥,w̸=0
⟨g, w⟩
∥w∥
= ∥g∥,
thus ∥ˆg∥≤∥g∥. Moreover, for w ∈N(B)⊥, w ̸= 0, it holds
⟨g, w⟩
∥w∥
= ⟨ˆg, w⟩
∥w∥
≤
sup
v∈V,v̸=0
⟨ˆg, v⟩
∥v∥
= ∥ˆg∥.
Exercise 8.2 The inf–sup condition (8.7) is equivalent to each one of the following
conditions:
(a) The operator BT is an isomorphism from M onto N(B)♯and
∃β > 0 : ∥BT μ∥V ′ ≥β∥μ∥M
∀μ ∈M .

174
8
Saddle Points Problems
(b) The operator B is an isomorphism from N(B)⊥onto M′ and
∃β > 0 : ∥Bv∥M′ ≥β∥v∥V
∀v ∈N(B)⊥.
Solution
(b) ⇒(a).
From (b) we know that R(B) = M′ is closed, so that by the closed
range Theorem 8.4 R(BT ) is closed in V ′ and R(BT ) = N(B)♯, R(B) =
N(BT )♯= M′, thus N(BT ) = {0}. In conclusion, BT is an isomorphism from
M onto N(B)♯. The estimate in (b) says that ∥B−1∥L(M′;N(B)⊥) ≤1/β, while the
estimate in (a) says that ∥(BT )−1∥L(N(B)♯;M) ≤1/β. Thus they are equivalent,
since
∥B−1∥L(M′;N(B)⊥) = ∥(BT )−1∥L(N(B)♯;M) ,
as it can be easily veriﬁed by looking at the deﬁnition of adjoint operator and
taking into account that (B−1)T = (BT )−1 and the identiﬁcation N(B)♯=
(N(B)⊥)′.
(a) ⇒(8.7).
It is enough to note that
∥BT μ∥V ′ =
max
v∈V,v̸=0
⟨BT μ, v⟩
∥v∥V
(see Exercise 8.3).
(8.7) ⇒(b).
By Proposition 8.2 we know that (8.8) is satisﬁed, R(BT ) is closed
in V ′ and N(BT ) = {0}, so that, by the closed range Theorem 8.4, R(B) = M′.
By decomposing V into the two ortoghonal subspaces N(B) and N(B)⊥, it is
easy to check that also the restriction of B to N(B)⊥is onto M′. Therefore B is
an isomorphism from N(B)⊥onto M′. Finally, (8.8) is equivalent to the estimate
in (a), which, as already seen, is equivalent to the estimate in (b).
Exercise 8.3 Let V be a Hilbert space and F ∈V ′. Show that the norm ∥F∥V ′
deﬁned as
∥F∥V ′ =
sup
v∈V,v̸=0
⟨F, v⟩
∥v∥V
is indeed equal to
∥F∥V ′ =
max
v∈V,v̸=0
⟨F, v⟩
∥v∥V
,
namely, there is vF ∈V , vF ̸= 0, such that
∥F∥V ′ = ⟨F, vF ⟩
∥vF ∥V
.

8.3
Exercises
175
Solution We can assume that F ̸= 0, otherwise the result is trivial. By the Riesz
representation theorem 3.1 we know that there exists a unique vF ∈V such that
⟨F, v⟩= (vF , v)V for any v ∈V . Moreover, ∥F∥V ′ = ∥vF ∥V : in fact
⟨F, v⟩= (vF , v)V ≤∥vF ∥V ∥v∥V
∀v ∈V ,
which implies ∥F∥V ′ ≤∥vF ∥V . On the other hand
⟨F, vF ⟩
∥vF ∥V
= ∥vF ∥2
V
∥vF ∥V
= ∥vF ∥V ≤∥F∥V ′ .
Thus ∥F∥V ′ = ∥vF ∥V = ⟨F,vF ⟩
∥vF ∥V .
Exercise 8.4 Prove that H(div; D) is a Hilbert space with respect to the scalar
product (8.13).
Solution Take a Cauchy sequence qk in H(div; D): in particular qk and div qk
are Cauchy sequences in L2(D), thus we have that qk →q and div qk →w in
(L2(D))n and in L2(D), respectively. From the deﬁnition of weak divergence we
know that div qk satisﬁes

D
div qk ϕdx = −

D
qk · ∇ϕdx
∀ϕ ∈C∞
0 (D) .
Passing to the limit we ﬁnd

D
w ϕdx = −

D
q · ∇ϕdx
∀ϕ ∈C∞
0 (D) ,
which means that w ∈L2(D) is the weak divergence of q. As a consequence we
have proved that the sequence qk converges to q in H(div; D).

Chapter 9
Parabolic PDEs
Parabolic equations are equations of the form
∂u
∂t + Lu = f
in D × (0, T ) ,
where L is an elliptic operator, whose coefﬁcients can depend on t. The “prototype”
is the heat equation
∂u
∂t −u = f
in D × (0, T ) .
Since with respect to the space derivative the operator ∂
∂t + L is associated to an
elliptic operator, it is necessary to add boundary conditions (for instance, one of the
four types we have considered before: Dirichlet, Neumann, mixed, Robin). Since
with respect to the time derivative the operator ∂
∂t + L is a ﬁrst order operator, it is
necessary to add one initial condition on u, the value of u in D at t = 0.
In the ﬁrst two sections of this chapter we present the abstract variational theory
related to parabolic equations and its application to various examples of initial-
boundary value problems. The last section is devoted to an important property of
the solutions: the maximum principle.
9.1
Variational Theory
Before considering some speciﬁc problems, let us present an abstract theory for
ﬁrst order evolution equations in Hilbert spaces. First of all we need to clarify
some theoretical results concerning functions with values in an inﬁnite dimensional
Hilbert space. We will not enter in depth this topic, limiting ourselves to give some
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_9
177

178
9
Parabolic PDEs
general ideas. A complete description of the functional analysis framework can be
found in Dautray and Lions [5, Chapter XVIII, §1].
We start with some deﬁnitions. Let X be an Hilbert space: we set, for 1 ≤p ≤
+∞,
Lp(0, T ; X) = {v : D × (0, T ) →R | t →∥v(· , t)∥X is a Lp- function in (0, T )}
C0([0, T ]; X) = {v : D×[0, T ] →R | t →∥v(· , t)∥X is a C0- function in [0, T ]} .
Then we deﬁne the weak derivative with respect to t ∈[0, T ]. Denote as usual
by (·, ·)X the scalar product in X.
Deﬁnition 9.1 We say that q ∈L1
loc(0, T ; X) is the weak derivative of u ∈
L1
loc(0, T ; X) if, as elements of the space X,
 T
0
"(t)q(t)dt = −
 T
0
"′(t)u(t)dt
for each " ∈C∞
0 (0, T ), or, equivalently, if
 T
0
"(t)(q(t), v)Xdt = −
 T
0
"′(t)(u(t), v)Xdt
for each v ∈X and " ∈C∞
0 (0, T ). In this case we write u′ = q, as an element of
L1
loc(0, T ; X).
Now it is a standard task to deﬁne the Sobolev spaces W 1,p(0, T ; X). We write, as
usual, H 1(0, T ; X) = W 1,2(0, T ; X).
An important theorem is the following:
Theorem 9.1 If u ∈H 1(0, T ; X), then u ∈C0([0, T ]; X) and
∥u∥C0([0,T ];X) ≤CT ∥u∥H 1(0,T ;X) .
This is not enough for our needs, and we are going to present a similar
theorem which is even more important. Before giving its statement, we need some
preliminary considerations. First of all the following result holds:
Exercise 9.1 Suppose that V and H are two Hilbert spaces and that V is immersed
in H with continuity and that V is dense in H. Then, identifying H with its dual
H ′, it follows that H is immersed in V ′, the dual space of V , i.e.,
V !→H ≈H ′ !→V ′ .

9.1
Variational Theory
179
We can now furnish a deﬁnition of the derivative of u with respect to t which
is weaker than that given in 9.1. Suppose that u ∈L2(0, T ; H); we say that there
exists the derivative u′ ∈L2(0, T ; V ′) if there exists q ∈L2(0, T ; V ′) such that
 T
0
q(t)"(t)dt = −
 T
0
u(t)"′(t)dt
for each " ∈C∞
0 (0, T ). This equality has an element of V ′ at the left-hand side
and an element of H at the right-hand side; it can be more explicitly speciﬁed by
writing
2  T
0
q(t)"(t)dt, v
3
= −
2  T
0
u(t)"′(t)dt, v
3
= −
 T
0
⟨u(t), v⟩"′(t)dt
(•)
= −
 T
0
(u(t), v)H "′(t)dt
∀v ∈V ,
where ⟨·, ·⟩denotes the duality pairing between V and V ′ and (·, ·)H the scalar
product in H. Thus
 T
0
⟨q(t), v⟩"(t)dt = −
 T
0
(u(t), v)H "′(t)dt
∀v ∈V .
Therefore, if for u ∈L2(0, T ; H) we know that u′ ∈L2(0, T ; V ′), we have
 T
0
⟨u′(t), v⟩"(t)dt = −
 T
0
(u(t), v)H "′(t)dt
∀v ∈V ,
which can be also rewritten as
d
dt (u(t), v)H = ⟨u′(t), v⟩
(9.1)
for almost all t ∈[0, T ] and all v ∈V , where d
dt has to be intended as the weak
time derivative of the real valued function t →(u(t), v)H .
Remark 9.1 A remark on (•). Due to the identiﬁcation of H ′ with H, we have that
ω ∈H implies ω ∈V ′ and in particular
⟨ω, v⟩V ′,V = ⟨ω, v⟩H ′,H = (ω, v)H
∀v ∈V .
We are now ready to state the theorem we will often use in the sequel.

180
9
Parabolic PDEs
Theorem 9.2 Let H be a separable Hilbert space, V a separable Hilbert space
immersed with continuity and dense in H. Let u ∈L2(0, T ; V ) with u′
∈
L2(0, T ; V ′). Then u ∈C0([0, T ]; H) and
∥u∥C0([0,T ];H) ≤CT

∥u∥L2(0,T ;V ) + ∥u′∥L2(0,T ;V ′)

.
Moreover, if v ∈L2(0, T ; V ) with v′ ∈L2(0, T ; V ′) for each t, s ∈[0, T ] the
integration by parts formula holds
 t
s
⟨u′(τ), v(τ)⟩dτ = −
 t
s
⟨v′(τ), u(τ)⟩dτ + (u(t), v(t))H −(u(s), v(s))H .
Also, for almost all t ∈[0, T ]
d
dt (u(t), v(t))H = ⟨u′(t), v(t)⟩+ ⟨v′(t), u(t)⟩
and
1
2
d
dt ∥u(t)∥2
H = ⟨u′(t), u(t)⟩.
9.2
Abstract Problem
Let us formulate now the abstract problem we want to solve. Suppose we have a
separable Hilbert space H, a separable Hilbert space V such that V !→H with
continuous and dense immersion. Assume that we are given with u0 ∈H and F ∈
L2(0, T ; V ′) and with a family of bilinear forms a(t; · , ·), deﬁned in V × V and
valued in R for almost each t ∈[0, T ].
We want to ﬁnd u ∈L2(0, T ; V ) with u′ ∈L2(0, T ; V ′) such that u(0) = u0
(note that from Theorem 9.2 we know that u ∈C0([0, T ]; H), thus this equality has
a meaning) and
⟨u′(t), v⟩+ a(t; u(t), v) = ⟨F(t), v⟩
(9.2)
for almost all t ∈[0, T ] and for each v ∈V .
For showing the existence and uniqueness of such a solution we need some
assumptions on the family of bilinear forms a(t; ·, ·). We suppose that:
(i) a(t; ·, ·) is uniformly weakly coercive in V × V , namely, there exist a constant
α > 0 and a constant σ ≥0 (both not depending on t ∈[0, T ]) such that
a(t; v, v) + σ(v, v)H ≥α∥v∥2
V
∀v ∈V and a.e. t ∈[0, T ]

9.2
Abstract Problem
181
(ii) a(t; ·, ·) is uniformly bounded in V ×V , namely, there exists a constant γ > 0
(not depending on t ∈[0, T ]) such that
|a(t; w, v)| ≤γ ∥w∥V ∥v∥V
∀w, v ∈V and a.e. t ∈[0, T ]
(iii) the map t →a(t; w, v) is measurable for every w, v ∈V .
The existence and uniqueness theorem reads as follows:
Theorem 9.3 (Existence and Uniqueness) Let H and V be two separable Hilbert
spaces, with V !→H with continuous and dense immersion. Assume u0 ∈H
and F ∈L2(0, T ; V ′). Assume that the family of bilinear forms a(t; ·, ·) is deﬁned
in V × V and valued in R for almost each t ∈[0, T ] and satisﬁes (i), (ii) and
(iii). Then there exists a unique solution u ∈L2(0, T ; V ) of Eq. (9.2), satisfying
u′ ∈L2(0, T ; V ′) and u(0) = u0. Moreover, for each τ ∈[0, T ] the stability
estimate
∥u(τ)∥2
H + α
 τ
0
e2σ(τ−t)∥u(t)∥2
V dt ≤e2στ∥u0∥2
H + 1
α
 τ
0
e2σ(τ−t)∥F(t)∥2
V ′dt
(9.3)
holds.
Remark 9.2 In (i) we can always assume that σ = 0, namely, that a(t; ·, ·) is
uniformly coercive in V × V . In fact, if we set ˆu = e−σtu, we see that ˆu is a
solution to
⟨ˆu′(t), v⟩+ a(t; ˆu(t), v) + σ(ˆu(t), v)H = ⟨e−σtF(t), v⟩,
and now the bilinear forms a(t; ·, ·) + σ(·, ·)H are uniformly coercive in V × V .
Proof The proof of the theorem requires several steps. For the proof of uniqueness
and existence we assume σ = 0 in (i) (see Remark 9.2).
First Step Let us start from the uniqueness. It is enough to show that the only
solution for F = 0 and u0 = 0 is u = 0. Let t ∈[0, T ] be a value for which
Eq. (9.2) is satisﬁed. Take v = u(t). Then
⟨u′(t), u(t)⟩+ a(t; u(t), u(t)) = 0 .
On the other hand we have
d
dt ∥u(t)∥2
H = 2⟨u′(t), u(t)⟩
and
a(t; u(t), u(t)) ≥α∥u(t)∥2
V ,

182
9
Parabolic PDEs
thus
1
2
d
dt ∥u(t)∥2
H + α∥u(t)∥2
V ≤0
for a.e. t ∈[0, T ] .
As a consequence, integrating in [0, τ] we ﬁnd
∥u(τ)∥H ≤∥u(0)∥H = ∥u0∥H = 0
for all τ ∈[0, T ] .
Second Step The proof of the existence of a solution is based on an approxi-
mation procedure (Galerkin method for a time-dependent problem). Since V is
separable, we have a countable orthonormal basis {ϕm} ⊂V . Deﬁne VN
=
span{ϕ1, . . . , ϕN} ⊂V . We want to ﬁnd an approximate solution uN in VN. Since
V is dense in H, we can ﬁnd a sequence u0,N ∈VN such that u0,N converges to u0
in H. Then we look for an approximate solution uN of the form
uN(t) =
N

j=1
uN
j (t)ϕj
that has to satisfy uN(0) = u0,N (this means uN
j (0) = (u0,N, ϕj)V ) and
⟨(uN)′(t), ϕl⟩+ a(t; uN(t), ϕl) = ⟨F(t), ϕl⟩
for almost all t ∈[0, T ] and for all l = 1, . . . , N. Inserting the expression of uN,
we ﬁnd
N

j=1
⟨ϕj, ϕl⟩(uN
j )′(t) +
N

j=1
a(t; ϕj, ϕl)uN
j (t) = ⟨F(t), ϕl⟩
(9.4)
for each l = 1, . . . , N and almost all t ∈[0, T ].
Setting Mlj = ⟨ϕj, ϕl⟩, Alj(t) = a(t; ϕj, ϕl), UN(t) = (uN
1 (t), . . . , uN
N(t)),
U0,N = ((u0,N, ϕ1)V , . . . , (u0,N, ϕN)V ) and b(t) = (⟨F(t), ϕ1⟩, . . . , ⟨F(t), ϕN⟩)
we have obtained the linear system of ordinary differential equations

M(UN)′(t) + A(t)UN(t) = b(t)
UN(0) = U0,N .
(9.5)
The matrix Mlj = ⟨ϕj, ϕl⟩can be rewritten as (ϕj, ϕl)H (take into account that
ϕj ∈V and see Remark 9.1); it is clearly symmetric and moreover it is positive

9.2
Abstract Problem
183
deﬁnite. In fact, taking η ∈RN one has
N

j,l=1
(ϕj, ϕl)H ηjηl =
 N

j=1
ηjϕj,
N

l=1
ηlϕl

H =
444
N

j=1
ηjϕj
444
2
H ≥0
and the equality gives N
j=1 ηjϕj = 0 in H and thus in V , since V is immersed in
H. Since ϕj are linearly independent in V , it follows ηj = 0 for j = 1, . . ., N.
Thus the matrix Mlj = ⟨ϕj, ϕl⟩is non-singular, therefore there exists a unique
solution (uN
1 (t), . . . , uN
N(t)) of the linear system (9.5) and uN
j ∈C0([0, T ]) with
(uN
j )′ ∈L2(0, T ).
Third Step Now we want to pass to the limit in Eq. (9.4) as N →∞. We need
suitable a-priori estimates, in such a way that we can apply some known results
of functional analysis. Precisely, we want to ﬁnd a subsequence uNk such that uNk
converges weakly to u in L2(0, T ; V ). For this purpose, we need to ﬁnd uniform
estimates for uN in L2(0, T ; V ). Multiplying expression (9.4) by uN
l (t) and adding
over l we get
((uN)′(t), uN(t))H + a(t; uN(t), uN(t)) = ⟨F(t), uN(t)⟩.
Since
1
2
d
dt ∥uN(t)∥2
H = ((uN)′(t), uN(t))H ,
integrating on (0, τ), we have for each τ ∈[0, T ]
1
2∥uN(τ)∥2
H +
 τ
0
a(t; uN(t), uN(t))dt = 1
2∥u0,N∥2
H +
 τ
0
⟨F(t), uN(t)⟩dt .
By coerciveness we have
 τ
0
a(t; uN(t), uN(t))dt ≥α
 τ
0
∥uN(t)∥2
V dt ;
moreover, from the inequality ab ≤ε
2a2 + 1
2εb2, valid for any a ∈R, b ∈R and
ε > 0, we obtain, with ε = α,
 τ
0
⟨F(t), uN(t)⟩dt ≤
 τ
0
∥F(t)∥V ′∥uN(t)∥V dt
≤α
2
 τ
0
∥uN(t)∥2
V dt + 1
2α
 τ
0
∥F(t)∥2
V ′dt ,

184
9
Parabolic PDEs
and consequently
1
2∥uN(τ)∥2
H + α
2
 τ
0
∥uN(t)∥2
V dt ≤1
2∥u0,N∥2
H + 1
2α
 τ
0
∥F(t)∥2
V ′dt .
Since u0,N converges to u0 in H, we have obtained a uniform bound for uN in
L2(0, T ; V ). Since L2(0, T ; V ) is a Hilbert space, it exists a subsequence uNk (still
denoted uN) that converges weakly to an element u ∈L2(0, T ; V ) (see Yosida [23,
Theorem 1, p. 126, and Theorem of Eberlein–Shmulyan, p. 141]).
Take now " ∈C∞
0 (0, T ) and v ∈V . We have a sequence vN ∈VN such that
vN →v in V . If we deﬁne ψN : [0, T ] →V and ψ : [0, T ] →V by setting
ψN(t) = "(t)vN , ψ(t) = "(t)v ,
we have at once ψN →ψ in L2(0, T ; V ) and (ψN)′ →ψ′ in L2(0, T ; V ).
Rewriting equation (9.4) as
⟨(uN)′(t), wN ⟩+ a(t; uN(t), wN) = ⟨F(t), wN⟩
∀wN ∈VN
(9.6)
and taking wN = ψN(t), by integrating by parts in (0, T ) it follows
−
 T
0
(uN(t), (ψN )′(t))H dt +
 T
0
a(t; uN(t), ψN (t))dt =
 T
0
⟨F(t), ψN (t)⟩dt .
Since uN converges weakly to u ∈L2(0, T ; V ), ψN converges to ψ in L2(0, T ; V )
and (ψN)′ converges to ψ′ in L2(0, T ; V ), we can pass to the limit (see Exer-
cise 9.2) and obtain
−
 T
0
(u(t), v)H "′(t)dt +
 T
0
a(t; u(t), v)"(t)dt =
 T
0
⟨F(t), v⟩"(t)dt ,
hence u′ ∈L2(0, T ; V ′) and u satisﬁes equation (9.2), namely,
⟨u′(t), v⟩+ a(t; u(t), v) = ⟨F(t), v⟩
∀v ∈V
for almost all t ∈[0, T ].
Fourth Step It remains to show that u(0) = u0. Let " ∈C∞([0, T ]), with "(T ) =
0 and "(0) ̸= 0. First of all, by integration on (0, T ) from Eq. (9.2) it follows
 T
0
⟨u′(t), v⟩"(t)dt = −
 T
0
a(t; u(t), v)"(t)dt +
 T
0
⟨F(t), v⟩"(t)dt .

9.2
Abstract Problem
185
The integration by parts formula in Theorem 9.2 yields
 T
0
⟨u′(t), v⟩"(t)dt = −
 T
0
(u(t), v)H "′(t)dt −(u(0), v)H "(0) ,
thus
−
 T
0
(u(t), v)H "′(t)dt −(u(0), v)H "(0)
= −
 T
0
a(t; u(t), v)"(t)dt +
 T
0
⟨F(t), v⟩"(t)dt .
Now deﬁne as before ψN(t) = "(t)vN, ψ(t) = "(t)v, where v ∈V and vN ∈VN
with vN →v in V. Taking wN = ψN(t) in (9.6), integration by parts gives
−
 T
0
(uN(t), (ψN )′(t))H dt −(uN(0)
  
=u0,N
, ψN(0))H +
 T
0
a(t; uN(t), ψN (t))dt
=
 T
0
⟨F(t), ψN (t)⟩dt ,
and passing to the limit as N →∞one gets
−
 T
0
(u(t), v)H "′(t)dt −(u0, v)H "(0)
= −
 T
0
a(t; u(t), v)"(t)dt +
 T
0
⟨F(t), v⟩"(t)dt .
Hence for each v ∈V we have obtained
(u0, v)H "(0) = (u(0), v)H "(0) .
Since we have assumed "(0) ̸= 0 and V is dense in H, it follows u(0) = u0.
Fifth Step The last step is related to the stability result (9.3). For a while let us
assume again that σ = 0 in (i). Taking v = u(t) in Eq. (9.2) (we have u(t) ∈V for
almost all t ∈[0, T ]), it follows
⟨u′(t), u(t)⟩+ a(t; u(t), u(t)) = ⟨F(t), u(t)⟩,
thus proceeding as in the third step
1
2
d
dt ∥u(t)∥2
H + α∥u(t)∥2
V ≤∥F(t)∥V ′∥u(t)∥V ≤1
2α ∥F(t)∥2
V ′ + α
2 ∥u(t)∥2
V .

186
9
Parabolic PDEs
In conclusion, for each τ ∈[0, T ] an integration on (0, τ) gives
∥u(τ)∥2
H + α
 τ
0
∥u(t)∥2
V dt ≤∥u0∥2
H + 1
α
 τ
0
∥F(t)∥2
V ′dt ,
and when σ = 0 the proof is complete. For the case σ > 0 it is enough to replace
u(t) with e−σtu(t) and F(t) with e−σtF(t) and then (9.3) follows easily.
⊓⊔
Exercise 9.2 Let V be a Hilbert space, and suppose that vk ∈V converges to v in
V and that wk converges weakly to w in V . Then (vk, wk)V →(v, w)V .
9.2.1
Application to Parabolic PDEs
We are now in a position to present some examples that are covered by this abstract
theory. Let D ⊂Rn be a bounded, connected open set with a Lipschitz continuous
boundary ∂D. For the operator
Lv = −
n

i,j=1
Di(aijDjv) +
n

i=1
biDiv + a0v
in the elliptic case we have considered four boundary value problems: Dirichlet,
Neumann, mixed, Robin. The related variational spaces and bilinear forms are:
Dirichlet
V = H 1
0 (D), H = L2(D),
a(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx +

D
a0wvdx .
Neumann
V = H 1(D), H = L2(D), a(w, v) as in the Dirichlet case..
Mixed
V = H 1
D(D) = {v ∈H 1(D) | v|D = 0}, H = L2(D), a(w, v) as in the
Dirichlet case.
Robin
V = H 1(D), H = L2(D),
a(w, v) =

D
n

i,j=1
aijDjwDivdx +

D
n

i=1
biDiwvdx
+

D
a0wvdx +

∂D
κwvdSx .
In the present situation, we have also time dependence; therefore the bilinear forms
are more generally given by
a(t; w, v) =

D
n

i,j
aij(t)DjwDivdx +

D
n

i=1
bi(t)Diwvdx +

D
a0(t)wvdx
and similarly for the Robin problem.

9.2
Abstract Problem
187
We assume that aij, bi, a0 belong to L∞(D×(0, T )) and κ belongs to L∞(∂D×
(0, T )) (with κ(x, t) ≥0 for a.e. (x, t) ∈∂D × (0, T ) and 
∂ κ(t)dSx ̸= 0 for a.e.
t ∈[0, T ]), so that conditions (ii) and (iii) in Theorem 9.3 are satisﬁed. Moreover
we also assume that there exists a constant α0 > 0 such that
n

i,j=1
aij(x, t)ηjηi ≥α0|η|2
∀η ∈Rn
for a.e. (x, t) ∈D × (0, T ), i.e., on the operator L we assume ellipticity, uniformly
with respect to x and t.
Under these assumptions we have already seen in Sect. 5.3 that condition (i) in
the existence and uniqueness theorem is satisﬁed, with
σ > max(0, −μ) ,
where μ = infD×(0,T ) a0 −
1
2α0 ∥b∥2
L∞(D×(0,T )) and
α = min
α0
2 , σ + μ

.
Thus a(t; w, v) is uniformly weakly coercive in H 1(D).
Then we have to check that V and H satisfy the required properties. First of all,
it is well-known that L2(D) is a separable Hilbert space. Moreover, H 1
0 (D) and
H 1
D(D) are closed subspaces of H 1(D), which is a separable Hilbert space (see
Remark 4.9); thus they are separable Hilbert spaces. We also have that
C∞
0 (D) !→H 1
0 (D) !→H 1
D(D) !→H 1(D) !→L2(D)
and we know that C∞
0 (D) is dense in L2(D); therefore for all the boundary value
problems we have V !→H with continuous and dense immersion.
On the data, we assume that u0 ∈L2(D) and we remember that in the four cases
the linear and continuous functional F is deﬁned as follows:
Dirichlet
F(t) ∈V ′ is given by v →⟨F(t), v⟩=

D
f (t)vdx.
Neumann
F(t) ∈V ′ is given by v →⟨F(t), v⟩=

D
f (t)vdx +

∂D
g(t)vdSx.
Mixed
F(t) ∈V ′ is given by v →⟨F(t), v⟩=

D
f (t)vdx +

N
g(t)vdSx.
Robin
F(t) ∈V ′ is given by v →⟨F(t), v⟩=

D
f (t)vdx +

∂D
g(t)vdSx.
Thus we assume that f ∈L2(D × (0, T )), g ∈L2(∂D × (0, T )) (for the Neumann
and Robin cases) or g ∈L2(N × (0, T )) (for the mixed case), and we conclude
that Theorem 9.3 can be applied.

188
9
Parabolic PDEs
As a ﬁnal remark, one easily sees that in some case weaker assumptions would be
sufﬁcient, for instance f ∈L2(0, T ; V ′) for the Dirichlet boundary value problem.
9.3
Maximum Principle for Parabolic Problems
The maximum principle also holds in the case of parabolic problems. Let us start
with some deﬁnitions, that are similar to those given for elliptic problems.
Deﬁnition 9.2 We say that u ∈L2(0, T ; V ) with u′ ∈L2(0, T ; V ′) is a subsolution
for the operator
∂u
∂t + Lu
if the inequality
⟨u′(t), v⟩+ a(t; u(t), v) ≤0
(9.7)
holds for a.e. t ∈[0, T ] and for all v ∈H 1
0 (D) such that v ≥0 in D.
A similar deﬁnition is given for a supersolution: it is enough to say that −u is a
subsolution.
Theorem 9.4 Let D ⊂Rn be a bounded, connected, open set with a Lipschitz
continuous boundary ∂D. Let L be the elliptic operator
Lw = −
n

i,j=1
Di(aijDjw) +
n

i=1
biDiw + a0w ,
with bounded coefﬁcients aij = aij(x, t), bi = bi(x, t), a0 = a0(x, t). Assume that
a0(x, t) ≥0 a.e. in D × (0, T ). Then if u is a subsolution for L we have
sup
D×[0,T ]
u ≤sup
ST
u+ = max
)
sup
ST
u, 0
*
,
where ST = (∂D × [0, T ]) ∪(D × {0}). Similarly, if u is a supersolution for L we
have
inf
D×[0,T ] u ≥inf
ST
(−u−) = min

inf
ST
u, 0

.
Proof For the sake of simplicity, we present the proof under the assumption that the
weak divergence divb exists and satisﬁes divb ≤0 a.e. in D × (0, T ). The lines
of the proof for the general case can be found in Dautray and Lions [4, Theorem 1,

9.3
Maximum Principle for Parabolic Problems
189
p. 252] (indeed, under somehow different assumptions on the regularity of u and the
coefﬁcients; there a good exercise is also to ﬁnd out and correct some misprints...);
a complete presentation is in Ladyžhenskaja, Solonnikov and Ural’ceva [12, Chapter
III, §7].
Let us start from the case of the subsolution. Set M = supST u+; we can assume
M to be ﬁnite, otherwise we have nothing to prove, and clearly M ≥0. Choose
v(t) = max(u(t) −M, 0), so that v(t) ∈H 1
0 (D) and v(t) ≥0 for almost all
t ∈[0, T ]. When considering the maximum principle for the elliptic case, we have
already noted that ∇v(t) = ∇u(t) in {u(t) > M}, while v(t) = 0 and ∇v(t) = 0 in
{u(t) ≤M}. Thus

D
n

i,j=1
aij(t)Dju(t)Div(t)dx =

D
n

i,j=1
aij(t)Djv(t)Div(t)dx
≥α0

D
|∇v(t)|2dx .
Moreover, and similarly to what we have just seen
⟨u′(t), v(t)⟩= ⟨v′(t), v(t)⟩= 1
2
d
dt

D
v(t)2dx ,
as in {u(t) > M} we have v(t) = u(t) −M, while in {u(t) ≤M} it holds v(t) = 0
(here the argument is a little bit formal, but let us go on...; for a detailed proof see
Ladyžhenskaja, Solonnikov and Ural’ceva [12, Theorem 7.2, p. 188]).
Finally
−

D
n

i=1
bi(t)Diu(t) v(t)dx = −

{u(t)>M}
n

i=1
bi(t)Div(t) v(t)dx
= −

D
n

i=1
1
2bi(t) Di(v2(t))dx
=

D
1
2 div b(t)
  
≤0
v2(t)dx ≤0
and
−

D
a0(t)u(t)v(t)dx = −

{u(t)>M}
≥0

a0(t)
≥M≥0

u(t)
≥0



(u(t) −M) dx ≤0 .

190
9
Parabolic PDEs
From (9.7) we have thus obtained the following inequality
1
2
d
dt

D
v(t)2dx + α0

D
|∇v(t)|2dx ≤0 ,
(9.8)
so that v(t) is decreasing in t. Since v(0) = max(u(0) −M, 0) = 0, it follows
v(t) = 0 and therefore u(t) ≤M for t ∈[0, T ].
For the supersolution, just note that if u is a supersolution, then −u is a
subsolution, and (−u)+ = u−.
⊓⊔
Remark 9.3 If we have ∂u
∂t +Lu = f ≥0 in D×(0, T ), u(t)|∂D ≥0, u|t=0 ≥0, by
the change of variable ˆu(t) = e−ktu(t), k ≥−infD×(0,T ) a0, we can easily prove
that u(t) ≥0 for all t ∈[0, T ]. In fact, with respect to ˆu the problem is related
to a bilinear form with the coefﬁcient of the zero order term, say ˆa0, that satisﬁes
ˆa0 ≥0. Since ˆu(t)|∂D ≥0, ˆu|t=0 ≥0 and ˆf (t) = e−ktf (t) ≥0, it follows ˆu(t) ≥0
and consequently u(t) ≥0.
In other words, if you maintain a positive temperature on the walls of a room in
which the temperature was positive at the initial time and in which you are injecting
heat, then the temperature in the room will remain positive for all the subsequent
time. Do you see the power of mathematics?
Remark 9.4 If a0 = 0, one can substitute supST u+ with supST u (and infST (−u−)
with infST u). In fact, assuming again for simplicity that divb ≤0, the same proof
applies choosing M = supST u (which now is no longer non-negative) and v =
max(u −M, 0). This yields inequality (9.8) and the thesis follows.
9.4
Exercises
Exercise 9.1 Suppose that V and H are two Hilbert spaces and that V is immersed
in H with continuity and that V is dense in H. Then, identifying H with its dual
H ′, it follows that H is immersed in V ′, the dual space of V , i.e.,
V !→H ≈H ′ !→V ′ .
Solution Take an element F
∈H ′ ≈H, which by the Riesz representation
Theorem 3.1 can be written as F(h) = (kF, h)H for each h ∈H, with kF ∈H.
To this functional we can associate the element Q ∈V ′ given by Q(v) = (kF, v)H
for each v ∈V . We want to show that the map F ∈H ′ →Q ∈V ′ is one-
to-one. Thus suppose that there exists G ∈H ′ given by (kG, v)H and such that
(kG, v)H = (kF, v)H for each v ∈V . Take h ∈H: since V is dense in H there
exists a sequence vk ∈V such that vk →h in H. Therefore (kG, vk)H →(kG, h)H
and (kF, vk)H →(kF, h)H , and consequently (kG, h)H = (kF, h)H for each
h ∈H, namely, G = F in H ′.

9.4
Exercises
191
Exercise 9.2 Let V be a Hilbert space, and suppose that vk ∈V converges to v in
V and that wk converges weakly to w in V . Then (vk, wk)V →(v, w)V .
Solution First of all, let us note that a weakly convergent sequence in a Hilbert
space is bounded (see Yosida [23, Theorem 1, p. 120]) ). Then we have
|(vk, wk)V −(v, w)V | = |(vk −v, wk)V + (v, wk −w)V |
≤|(vk −v, wk)V | + |(v, wk −w)V |
≤∥vk −v∥V ∥wk∥V + |(v, wk −w)V | .
Being wk bounded, the ﬁrst term goes to 0; since for any v ∈V the linear functional
ψ →(v, ψ)V = Fv(ψ) is bounded, from the weak convergence of wk to w it
follows Fv(wk −w) →0, and the result is proved.
Exercise 9.3 Let D ⊂Rn be a bounded, connected open set. Consider the problem
⎧
⎪⎪⎨
⎪⎪⎩
∂u
∂t −u = 0
in D × (0, +∞)
u|∂D = 0
on ∂D × (0, +∞)
u|t=0 = u0
in D ,
where u0 ∈L2(D). Show that:
(i) there exists a unique solution u ∈L2(0, +∞; H 1
0 (D)) ∩C0([0, +∞); L2(D))
with u′ ∈L2(0, +∞; L2(D));
(ii)
lim
t→+∞∥u(t)∥L2(D) = 0.
Solution
(i) Looking at the proof of Theorem 9.3 we easily see that, for a right hand side
f = 0, it is possible to prove the existence of a solution u(t) for t ∈[0, +∞),
and moreover the estimate
∥u(τ)∥2
L2(D) +
 τ
0
∥∇u(t)∥2
L2(D)dt ≤∥u0∥2
L2(D)
(9.9)
holds for each τ ∈[0, +∞).
(ii) Using the Poincaré inequality (6.2) in (9.9) we ﬁnd
∥u(τ)∥2
L2(D) + σ
 τ
0
∥u(t)∥2
L2(D)dt ≤∥u0∥2
L2(D)
(9.10)
for each τ ∈[0, +∞), where σ =
1
CD . Now set w(t) = eσtu(t). We obtain at
once w′(t) = eσtu′(t) + σeσtu(t), thus
⟨w′(t), v⟩+ a(t; w(t), v) −σ(w(t), v)L2(D) = 0
∀v ∈H 1
0 (D) .
(9.11)

192
9
Parabolic PDEs
Since
a(t; w(t), w(t))−σ(w(t), w(t))L2(D) =

D
|∇w(t)|2dx−σ

D
w(t)2dx ≥0 ,
Equation (9.11) and the relation w(0) = u0 lead to the estimate
1
2
d
dt ∥w(t)∥2
L2(D) ≤0
for a.e. t ∈[0, T ] and thus
∥w(τ)∥2
L2(D) ≤∥u0∥2
L2(D)
for each τ ∈[0, +∞). In conclusion ∥u(τ)∥L2(D) ≤e−στ∥u0∥L2(D) →0 as
τ →+∞.
[From the physical point of view this result says that, if no heat is furnished and
the boundary temperature is kept to 0, then the internal temperature goes to 0 as
time becomes larger and larger: a well-known situation in our real life experience.]
Exercise 9.4 Propose a numerical scheme for ﬁnding the approximate solution
of a parabolic problem which is based on the Galerkin approximation and on the
backward Euler method for discretizing ∂u
∂t .
Solution Let VM be a ﬁnite dimensional subspace of V (not necessarily the space
generated by the ﬁrst M elements of an orthonormal basis of V ), whose basis is
denoted by {φ1, . . . , φM}. Choose a time-step t = T/K > 0, deﬁne tk = kt,
k = 0, 1, . . ., K, and consider the backward Euler approximation of the ﬁrst order
derivative:
uk+1 −uk
t
≈u′(tk+1) , k = 0, 1, . . . , K .
Then the parabolic equation
⟨u′(t), v⟩+ a(t; u(t), v) = ⟨F(t), v⟩
can be approximated by means of the following numerical scheme: being given
u0
M
∈VM, a suitable approximation of the initial datum u0, for each k
=
0, 1, . . ., K −1 ﬁnd uk+1
M
∈VM, solution of the problem
)
uk+1
M
−uk
M
t
, φi
*
H
+ a(tk+1; uk+1
M , φi) = ⟨F(tk+1), φi⟩, i = 1, . . . , M .

9.4
Exercises
193
More explicitly, at each time step tk+1, k = 0, 1, . . . , K −1, one has to solve the
discretized elliptic problem
1
t (uk+1
M , φi)H +a(tk+1; uk+1
M , φi) = 1
t (uk
M, φi)H +⟨F(tk+1), φi⟩, i = 1, . . . , M .
This linear system is associated to the matrix Ak+1
ij
=
1
t (φj, φi)H +a(tk+1; φj, φi).
Note that if a(t; ·, ·) is uniformly weakly coercive in V × V , then for t small
enough the bilinear form
1
t (·, ·)H + a(t; ·, ·) is uniformly coercive in V × V ,
hence the matrix Ak+1 is uniformly positive deﬁnite for k = 0, 1, . . . , K −1.

Chapter 10
Hyperbolic PDEs
Hyperbolic equations have the form
∂2u
∂t2 + Lu = f
in D × (0, T ) ,
where L is an elliptic operator, whose coefﬁcients can depend on t. The “prototype”
is the wave equation
∂2u
∂t2 −c2u = f
in D × (0, T ) ,
with speed c > 0.
As for the parabolic equations, we have to add a boundary condition (one of
those we have considered for elliptic problems: Dirichlet, Neumann, mixed, Robin).
Since with respect to time we have a second order derivative, we also need to add
two initial conditions, namely u|t=0 and ∂u
∂t |t=0 have to be assigned in D.
In the ﬁrst section of the chapter we present the abstract variational theory for
second order evolution equations in Hilbert spaces; then the application of this
theory to hyperbolic equations is described. The second section is concerned with
an important property of the solutions: the ﬁnite propagation speed.
10.1
Abstract Problem
We again assume that we are given with a separable Hilbert space H and a separable
Hilbert space V , with V !→H with continuous and dense immersion. Assume that
u0 ∈V , u1 ∈H and F ∈L2(0, T ; H). We look for a solution u ∈L2(0, T ; V ),
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0_10
195

196
10
Hyperbolic PDEs
with u′ ∈L2(0, T ; H) and u′′ ∈L2(0, T ; V ′) of the problem
⟨u′′(t), v⟩+ a(t; u(t), v) = (F(t), v)H
(10.1)
for each v
∈V and a.e. t
∈[0, T ], with u(0) = u0 and u′(0) = u1.
Since u ∈
L2(0, T ; V ) ⊂
L2(0, T ; H) and u′
∈
L2(0, T ; H), it follows
that u ∈C0([0, T ]; H), thus the value u(0) has a meaning; similarly, since
u′ ∈L2(0, T ; H) ⊂L2(0, T ; V ′) and u′′ ∈L2(0, T ; V ′), it follows that u′ ∈
C0([0, T ]; V ′), thus u′(0) has a meaning. Note that, similarly to the relation (9.1)
valid for the parabolic case, if u′ ∈L2(0, T ; H) and u′′ ∈L2(0, T ; V ′) it holds
d
dt (u′(t), v)H = ⟨u′′(t), v⟩
(10.2)
for almost all t ∈[0, T ] and all v ∈V , where d
dt has to be intended as the weak
time derivative of the real valued function t →(u′(t), v)H . Since under the present
assumptions (9.1) can be written as
d
dt (u(t), v)H = ⟨u′(t), v⟩= (u′(t), v)H ,
(10.3)
we also have
d2
dt2 (u(t), v)H = d
dt (u′(t), v)H = ⟨u′′(t), v⟩,
(10.4)
where d2
dt2 has to be intended as the second order weak time derivative of the real
valued function t →(u(t), v)H .
Let us now clarify the assumptions on the family of bilinear forms t →a(t; ·, ·).
We assume that
a(t; w, v) = a(t; w, v) + a1(t; w, v) ,
where a1(t; w, v), the “lower order part”, satisﬁes
(i) |a1(t; w, v)| ≤C1∥w∥V ∥v∥H for all w, v ∈V , with C1 > 0 independent of
t ∈[0, T ],
whereas a(t; w, v), in some sense the “principal part”, satisﬁes
(ii) t →a(t; w, v) is differentiable for t ∈[0, T ] and for all w, v ∈V . The
derivative of thus map will be denoted by a′(t; w, v)
(iii) |a′(t; w, v)| ≤
C1∥w∥V ∥v∥V for all w, v ∈V , with 
C1 > 0 independent of
t ∈[0, T ]
(iv) |a(t; w, v)| ≤
C0∥w∥V ∥v∥V for all w, v ∈V , with 
C0 > 0 independent of
t ∈[0, T ]

10.1
Abstract Problem
197
(v) a(t; v, v) + σ(v, v)H ≥α∥v∥2
V for all v ∈V , where α > 0 and σ ≥0 are
independent of t ∈[0, T ]
(vi) a(t; w, v) = a(t; v, w) for all w, v ∈V and for all t ∈[0, T ] (symmetry of
the principal part).
Let us underline from the very beginning that the symmetry of the principal part
is a crucial point. The abstract theorem reads as follows.
Theorem 10.1 (Existence and Uniqueness) Let H and V be two separable
Hilbert spaces, with V !→H with continuous and dense immersion. Assume u0 ∈
V , u1 ∈H and F ∈L2(0, T ; H). Assume that the family of bilinear forms a(t; ·, ·)
satisﬁes the hypothesis (i)–(vi) listed here above. Then there exists a solution
u ∈L2(0, T ; V ) of Eq. (10.1), with u′ ∈L2(0, T ; H), u′′ ∈L2(0, T ; V ′) and
u(0) = u0, u′(0) = u1. Uniqueness also holds, under the additional assumption
(vii) |a1(t; w, v)| ≤C2∥w∥H ∥v∥V for all w, v ∈V , with C2 > 0 independent of
t ∈[0, T ].
Remark 10.1 Note that one can obtain a better result, as it is true that u ∈
C0([0, T ]; V ) and u′ ∈C0([0, T ]; H). For this result see, e.g., Dautray and
Lions [5, Chapter XVIII, §5.5].
Proof The proof is obtained by approximation, by proceeding as in the parabolic
case.
First Step Since V is separable, we have a countable orthonormal basis ϕm ∈V .
Deﬁne VN = span{ϕ1 . . . ϕN} ⊂V . Since V is dense in H, we can select a sequence
u1,N ∈VN such that u1,N →u1 in H. Moreover, we also have u0,N ∈VN such
that u0,N →u0 in V . We look for
uN(t) =
N

j=1
uN
j (t)ϕj
such that uN(0) = u0,N (this means uN
j (0) = (u0,N, ϕj)V ), (uN)′(0) = u1,N (this
means (uN
j )′(0) = (u1,N, ϕj)V ) and moreover
⟨(uN)′′(t), ϕl⟩+ a(t; uN(t), ϕl) = (F(t), ϕl)H
for almost all t ∈[0, T ] and for all l = 1, . . ., N. Inserting the expression of uN(t),
we ﬁnd
N

j=1
⟨ϕj, ϕl⟩(uN
j )′′(t) +
N

j=1
a(t; ϕj, ϕl)uN
j (t) = (F(t), ϕl)H .
(10.5)

198
10
Hyperbolic PDEs
We have already veriﬁed in Theorem 9.3 that the matrix ⟨ϕj, ϕl⟩is non-singular
(it is symmetric and positive deﬁnite), thus this is a linear system of second order
ordinary differential equations. Setting qj(t) = (uN
j )′(t), it can be rewritten as a
standard linear system of ﬁrst order ordinary differential equations, thus we know
that there exists a unique solution (uN
1 (t), . . . , uN
N(t)), with uN
j ∈C1([0, T ]) and
(uN
j )′′ ∈L2(0, T ).
Second Step We must now ﬁnd suitable a-priori estimates for passing to the limit.
Multiply equation (10.5) by (uN
l )′(t) and add over l. It holds
((uN)′′(t), (uN)′(t))H +a(t; uN(t), (uN)′(t))
= −a1(t; uN(t), (uN)′(t)) + (F(t), (uN)′(t))H .
We know that
((uN)′′(t), (uN)′(t))H = 1
2
d
dt ∥(uN)′(t)∥2
H .
Moreover
a(t; uN(t), (uN)′(t)) = 1
2
d
dta(t; uN(t), uN(t)) −1
2a′(t; uN(t), uN(t)) ,
due to the symmetry of a(t; ·, ·). Finally, from assumption (i),
| −a1(t; uN(t), (uN)′(t))| ≤C1∥uN(t)∥V ∥(uN)′(t)∥H
and moreover
|(F(t), (uN)′(t))H | ≤∥F(t)∥H ∥(uN)′(t)∥H .
Summarizing, we have
1
2
d
dt ∥(uN)′(t)∥2
H + 1
2
d
dta(t; uN(t), uN(t)) ≤
≤1
2|a′(t; uN(t), uN(t))| + C1∥uN(t)∥V ∥(uN)′(t)∥H + ∥F(t)∥H ∥(uN)′(t)∥H
≤1
2

C1∥uN(t)∥2
V + C1∥uN(t)∥V ∥(uN)′(t)∥H + ∥F(t)∥H ∥(uN)′(t)∥H ,

10.1
Abstract Problem
199
having used assumption (iii). Integrating with respect to t on [0, τ] we have
1
2∥(uN)′(τ)∥2
H + 1
2a(τ; uN(τ), uN(τ))
≤1
2∥(uN)′(0)∥2
H + 1
2a(0; uN(0), uN(0)) + 1
2

C1
 τ
0
∥uN(t)∥2
V dt
+ C1
 τ
0
∥uN(t)∥V ∥(uN)′(t)∥H dt +
 τ
0
∥F(t)∥H ∥(uN)′(t)∥H dt .
Using the weak coerciveness of a(t; ·, ·) we ﬁnd
a(τ, uN(τ), uN(τ)) ≥α∥uN(τ)∥2
V −σ∥uN(τ)∥2
H .
From the inequality ab ≤1
2a2 + 1
2b2 and using assumption (iv) we get
α∥uN(τ)∥2
V + ∥(uN)′(τ)∥2
H
≤σ∥uN(τ)∥2
H + ∥u1,N∥2
H + 
C0∥u0,N∥2
V
+ C∗
+  T
0
∥F(t)∥2
H dt +
 τ
0

∥uN(t)∥2
V + ∥(uN)′(t)∥2
H

dt
,
.
Since u0,N →u0 in V and u1,N →u1 in H, we have ∥u0,N∥2
V +∥u1,N∥2
H ≤const.
Moreover, we have
uN(τ) =
 τ
0
(uN)′(t)dt + uN(0)
  
=u0,N
,
thus, noting that (a + b)2 ≤2(a2 + b2) and using the Cauchy–Schwarz inequality,
we obtain
∥uN(τ)∥2
H ≤
4444
 τ
0
(uN)′(t)dt
4444
H
+ ∥u0,N∥H
2
≤2
) τ
0
∥(uN)′(t)∥H dt
2
+ ∥u0,N∥2
H
*
C−S
≤2

τ
 τ
0
∥(uN)′(t)∥2
H dt + ∥u0,N∥2
H

.
Note that this last series of inequalities is not needed if σ = 0, namely, if the bilinear
form a(t; ·, ·) is coercive and not only weakly coercive.

200
10
Hyperbolic PDEs
In conclusion, setting Q(τ) = ∥uN(τ)∥2
V + ∥(uN)′(τ)∥2
H, we have found
Q(τ) ≤K1 + K2
 τ
0
Q(t)dt
for a.e. τ ∈[0, T ] .
From Gronwall lemma E.2 we have
Q(τ) ≤K1eK2τ
for a.e. τ ∈[0, T ] ,
therefore uN is bounded in L2(0, T ; V ) and (uN)′ is bounded in L2(0, T ; H),
respectively (more precisely, in L∞(0, T ; V ) and L∞(0, T ; H)). Since L2(0, T ; V )
and L2(0, T ; H) are Hilbert spaces, by known results in functional analysis we can
select a subsequence (still denoted by uN) such that uN →u weakly in L2(0, T ; V )
and (uN)′ →w weakly in L2(0, T ; H) (see Yosida [23, Theorem 1, p. 126, and
Theorem of Eberlein–Shmulyan, p. 141]). It is an easy task to show that w = u′; in
fact, for each v ∈H and η ∈C∞
0 (0, T ) by integration by parts we have
 T
0
((uN)′(t), v)H η(t)dt = −
 T
0
(uN(t), v)H η′(t)dt ,
and passing to the limit, using the weak convergence of uN and (uN)′ in
L2(0, T ; H), we obtain
 T
0
(w(t), v)H η(t)dt = −
 T
0
(u(t), v)H η′(t)dt ,
namely, u′ = w. Take now " ∈C∞(0, T ), v ∈V and vN ∈VN such that vN →v
in V (remember that VN = span{ϕ1, . . . , ϕN}, where ϕj is an orthonormal basis of
V ). Set
ψN(t) = "(t)vN , ψ(t) = "(t)v .
It is clear that ψN →ψ in L2(0, T ; V ) and (ψN)′ = "′vN converges to ψ′ = "′v
in L2(0, T ; V ).
Equation (10.5) can be rewritten as
⟨(uN)′′(t), wN⟩+ a(t; uN(t), wN) = (F(t), wN)H
(10.6)
for each wN ∈VN and a.e. t ∈[0, T ]; choosing wN = ψN(t) and integrating by
parts in (0, T ), it follows
−
 T
0

(uN)′(t), (ψN)′(t)

H dt +
 T
0
a(t; uN(t), ψN(t))dt =
 T
0
(F(t), ψN(t))H dt .

10.1
Abstract Problem
201
Passing to the limit we ﬁnd
−
 T
0
(u′(t), v)H "′(t)dt +
 T
0
a(t; u(t), v)"(t)dt =
 T
0
(F(t), v)H "(t)dt ,
thus u′′(t) ∈L2(0, T ; V ′) and Eq. (10.1) is satisﬁed.
Third Step The proof of the existence of a solution is completed if we show that
u(0) = u0 and u′(0) = u1. Take " ∈C∞([0, T ]) with "(T ) = 0 and "′(T ) = 0,
and deﬁne as before ψN(t) = "(t)vN, ψ(t) = "(t)v, with vN ∈VN and vN →v
in V . Integrating equation (10.1) on (0, T ) we ﬁnd
 T
0
⟨u′′(t), v⟩"(t)dt = −
 T
0
a(t; u(t), v)"(t)dt +
 T
0
(F(t), v)H "(t)dt .
On the other hand, integrating by parts twice on (0, T ) we obtain
 T
0
⟨u′′(t), v⟩"(t)dt
=
 T
0
(u(t), v)H "′′(t)dt −⟨u′(0), v⟩"(0) + (u(0), v)H "′(0) ,
thus
 T
0
(u(t), v)H "′′(t)dt −⟨u′(0), v⟩"(0) + (u(0), v)H "′(0)
= −
 T
0
a(t; u(t), v)"(t)dt +
 T
0
(F(t), v)H "(t)dt .
Inserting wN = ψN(t) in Eq. (10.6), it follows, by integration by parts on (0, T ),
 T
0
(uN(t), (ψN)′′(t))H dt −((uN)′(0)



=u1,N
, ψN(0))H + ((uN)(0)
  
=u0,N
, (ψN)′(0))H =
= −
 T
0
a(t; uN(t), ψN(t))dt +
 T
0
(F(t), ψN(t))H dt
Then passing to the limit as N →+∞, we obtain
 T
0
(u(t), v)H Φ′′(t)dt −(u1, v)H Φ(0) + (u0, v)H Φ′(0) =
= −
 T
0
a(t; u(t), v)Φ(t)dt +
 T
0
(F(t), v)H Φ(t)dt ,

202
10
Hyperbolic PDEs
and in conclusion
−⟨u′(0), v⟩"(0) + (u(0), v)H "′(0) = −(u1, v)H "(0) + (u0, v)H "′(0) .
Due to the arbitrariness of "(0) and "′(0) and v we conclude u′(0) = u1 and
u(0) = u0.
Fourth Step Let us come to the proof of the uniqueness of the solution. It is better
to divide the proof in two parts, and consider later the general case. In this step we
thus make two additional assumptions: ﬁrstly that a1(t; ·, ·) = 0, so that a(t; ·, ·)
coincides with a(t; ·, ·) and therefore is symmetric, and secondly that a(·, ·) does
not depend on t ∈[0, T ].
Let us assume F = 0, u0 = 0, u1 = 0; thus Eq. (10.1) reads
⟨u′′(t), v⟩+ a(u(t), v) = 0
∀v ∈V , for a.e. t ∈[0, T ] .
(10.7)
Here one would like to follow the same idea employed for the ﬁnite dimensional
approximation: select a value t among those for which (10.7) is satisﬁed, and choose
v = u′(t). However, this cannot be done since u′ does not belong to L2(0, T ; V )
but only to L2(0, T ; H). Thus we adopt a classical procedure proposed by Olga A.
Ladyzhenskaya [11] (see also Dautray and Lions [5, p. 572]), and we choose as a
test function an antiderivative of u: precisely, for a ﬁxed s ∈[0, T ] set
v(t) =
 s
t u(τ)dτ
if 0 ≤t ≤s
0
if s ≤t ≤T .
We have v(t) ∈V for every t ∈[0, T ] and v′(t) = −u(t) for 0 ≤t ≤s. Let us
choose this v = v(t) in Eq. (10.7); for 0 ≤t ≤s we have
⟨u′′(t), v(t)⟩= d
dt (u′(t), v(t))H −(u′(t), v′(t)

=−u(t)
)H
= d
dt (u′(t), v(t))H + 1
2
d
dt ∥u(t)∥2
H ,
(10.8)
and
a(u(t), v(t)) = −a(v′(t), v(t)) = −1
2
d
dt [a(v(t), v(t))] ,
(10.9)

10.1
Abstract Problem
203
where the last equality is due to the fact that a(·, ·) is symmetric and not depending
on t. Thus integrating (10.7) over (0, s) it follows
0 =
 s
0
$ d
dt (u′(t), v(t))H + 1
2
d
dt ∥u(t)∥2
H −1
2
d
dt a(v(t), v(t))
%
dt
= 1
2

∥u(s)∥2
H + a(v(0), v(0))

(since u(0) = 0, u′(0) = 0 and v(s) = 0)
≥1
2

∥u(s)∥2
H + α∥v(0)∥2
V −σ∥v(0)∥2
H

(since a(· , ·) is weakly coercive) .
We have v(0) =
 s
0 u(τ)dτ, thus
∥v(0)∥2
H ≤
 s
0
∥u(τ)∥Hdτ
2
≤

C-S
s
 s
0
∥u(τ)∥2
H dτ ,
and then
∥u(s)∥2
H + α∥v(0)∥2
V ≤σs
 s
0
∥u(τ)∥2
H dτ
≤σT
 s
0
∥u(τ)∥2
H dτ
∀s ∈[0, T ] .
From Gronwall lemma E.2 it follows ∥u(s)∥H = 0 for s ∈[0, T ] and uniqueness is
proved.
Fifth Step Repeat now the uniqueness result without assuming that a1(t; ·, ·) = 0
and a(t; ·, ·) is independent of t ∈[0, T ]. Instead of (10.7) we have the equation
⟨u′′(t), v(t)⟩+a(t; u(t), v(t)) = −a1(t; u(t), v(t)) ,
(10.10)
and instead of (10.9) we have
a(t; u(t), v(t)) = −a(t; v′(t), v(t))
= −1
2
d
dt [a(t; v(t), v(t))] + 1
2a′(t; v(t), v(t)) .
(10.11)

204
10
Hyperbolic PDEs
Therefore integrating (10.10) over (0, s) and taking into account (10.8) and (10.11)
it follows
−
 s
0
a1(t; u(t), v(t))dt −1
2
 s
0
a′(t; v(t), v(t))dt
=
 s
0
$ d
dt (u′(t), v(t))H + 1
2
d
dt ∥u(t)∥2
H −1
2
d
dta(t; v(t), v(t))
%
dt
= 1
2

∥u(s)∥2
H +a(0; v(0), v(0))

(since u(0) = 0, u′(0) = 0 and v(s) = 0)
≥1
2

∥u(s)∥2
H + α∥v(0)∥2
V −σ∥v(0)∥2
H

(since a(0; ·, ·) is weakly coercive) .
Using the boundedness of a′(t; ·, ·) in V × V and of a1(t; ·, ·) in H × V (see
assumptions (iii) and (vii)), we obtain
∥u(s)∥2
H + α∥v(0)∥2
V
≤σ∥v(0)∥2
H + 2C2
 s
0
∥u(t)∥H ∥v(t)∥V dt + 
C1
 s
0
∥v(t)∥2
V dt
≤

2ab≤a2+b2
σ∥v(0)∥2
H + C2
 s
0
∥u(t)∥2
H dt + (
C1 + C2)
 s
0
∥v(t)∥2
V dt .
For 0 ≤t ≤T set now w(t) =  t
0 u(τ)dτ. It holds v(0) = w(s) and v(t) =
w(s) −w(t), for 0 ≤t ≤s. Thus, using that (a + b)2 ≤2a2 + 2b2, we can rewrite
the last equation as
∥u(s)∥2
H + α∥w(s)∥2
V ≤σ∥w(s)∥2
H + C∗  s
0
∥u(t)∥2
Hdt +
 s
0
∥w(s) −w(t)∥2
V dt

≤σ∥w(s)∥2
H + C∗  s
0
∥u(t)∥2
Hdt + 2
 s
0
∥w(t)∥2
V dt + 2s∥w(s)∥2
V

,
where C∗= 
C1 + C2. We also have, for 0 ≤s ≤T1 ≤T ,
∥w(s)∥2
H =
444
 s
0
u(τ)dτ
444
2
H ≤
  s
0
∥u(τ)∥H dτ
2
≤

C-S
s
 s
0
∥u(τ∥2
H dτ ≤T1
 s
0
∥u(τ∥2
H dτ ,

10.1
Abstract Problem
205
thus
∥u(s)∥2
H + (α −2T1C∗)∥w(s)∥2
V
≤(σT1 + C∗)
 s
0
∥u(t)∥2
H dt + 2C∗
 s
0
∥w(t)∥2
V dt .
Choosing T1 > 0 so small that α −2T1C∗≥α
2, we can apply Gronwall lemma E.2
on the interval [0, T1] to the function η(s) = ∥u(s)∥2
H + α
2 ∥w(s)∥2
V , thus obtaining
η(s) = 0 for s ∈[0, T1]. Since T1 only depends on the data of the problem through
C∗and α, we can repeat the same argument on [T1, 2T1] and so on.
⊓⊔
10.1.1
Application to Hyperbolic PDEs
Let us show some examples of hyperbolic problems that are covered by this abstract
theory. Let D ⊂Rn be a bounded, connected open set with a Lipschitz continuous
boundary ∂D. The operator L will be as usual
Lv = −
n

i,j=1
Di(aij(t)Djv) +
n

i=1
bi(t)Div + a0(t)v ,
that we assume to be elliptic, uniformly with respect to x ∈D and t ∈[0, T ].
The associated bilinear form, depending on the boundary conditions we have to
consider, is
a(t; w, v) =

D
n

i,j=1
aij(t)DjwDivdx +

D
n

i=1
bi(t)Diwvdx
+

D
a0(t)wvdx
$
+

∂D
κ(t)wvdSx
%
,
where the integral inside the square brackets is present only in the case of the Robin
boundary condition.
We assume that aij(t), bi(t), a0(t) belong to L∞(D × (0, T )), and that κ(t)
belongs to L∞(∂D × (0, T )), with κ(x, t) ≥0 for a.e. (x, t) ∈∂D × (0, T ) and

∂D κ(t)dSx ̸= 0 for a.e. t ∈[0, T ]. We deﬁne
a(t; w, v) =

D
n

i,j=1
ai,j(t)DjwDivdx
$
+

∂D
κ(t)wvdSx
%
,
which is the bilinear form associated to the principal part. We assume that aij(x, t) is
differentiable with respect to t in [0, T ], for almost all x ∈D, and that ∂aij
∂t belongs

206
10
Hyperbolic PDEs
to L∞(D × (0, T )). Similarly we assume that κ(x, t) is differentiable with respect
to t in [0, T ] for almost all x ∈∂D, and that ∂κ
∂t belongs to L∞(∂D × (0, T )).
Finally, we assume that the coefﬁcient matrix of the principal part of the operator L
is symmetric, i.e., that
aij(x, t) = aji(x, t)
for a.e. (x, t) ∈D × [0, T ] .
With these hypotheses it is an easy task to verify that all the assumptions of the
abstract Theorem 10.1 are satisﬁed, choosing H and V as in the parabolic case: in
conclusion, the existence of a solution is assured.
Remark 10.2 Let us note that in the hyperbolic case, due to the presence of the
second order time derivative, it is not possible to rewrite the given problem as
a hyperbolic problem associated to a coercive bilinear form, by using a suitable
change of variable (see Remark 9.2 and Exercise 10.4). However, it is possible to
choose σ = 0 in the weak coerciveness assumption provided that the Poincaré
inequality is satisﬁed (or the generalized Poincaré inequality in the case of the Robin
problem); in other words, only in the case of the Neumann problem the principal part
of the bilinear form is weakly coercive and not coercive.
Concerning uniqueness, we need to check that
|a1(t; w, v)| ≤C2∥w∥H ∥v∥V
∀w, v ∈V ,
(10.12)
where ∥· ∥H = ∥· ∥L2(D), ∥· ∥V = ∥· ∥H 1(D). We have
a1(t; w, v) =

D
n

i=1
bi(t)Diwvdx +

D
a0(t)wvdx .
The second term satisﬁes (10.12), thus we only have to verify (10.12) for the
ﬁrst term. Let us integrate by parts formally (we will see here below when this
is possible):

D
n

i=1
bi(t)Diwvdx = −

D
w
n

i=1
Di(bi(t)v)dx +

∂D
w b(t) · n vdSx
= −

D
w div b(t) vdx −

D
w b(t) · ∇vdx +

∂D
w b(t) · n vdSx .
Therefore we can easily verify that estimate (10.12) holds if for example:
(i) div b ∈L∞(D × (0, T )), b · n = 0 a.e. on ∂D × (0, T ) (Neumann or Robin
problem)
(ii) div b ∈L∞(D × (0, T )), V = H 1
0 (D) (Dirichlet problem)

10.2
Finite Propagation Speed
207
(iii) div b ∈L∞(D × (0, T )), b · n = 0 a.e. on N × (0, T ), V = H 1
D(D) (mixed
problem).
Thus, concerning the regularity of b, we can simply assume b ∈L∞(0, T ; W 1,∞
(D)) (so that, by the Sobolev immersion theorem 7.15, b(t)|∂D and b(t)|ΓN have a
meaning). Clearly, all these conditions are satisﬁed if bi = 0 for i = 1, . . . , n.
10.2
Finite Propagation Speed
The hyperbolic equations have the property of ﬁnite propagation speed. This is a
general property, but we will give a proof of it only for the wave equation, with
velocity c > 0.
Consider a point (x0, t0), with x0 ∈Rn and t0 > 0, and for 0 ≤t < t0 deﬁne the
sets
Dt = {x ∈Rn | |x −x0| < c(t0 −t)}
W = {(x, t) ∈Rn × [0, t0) | x ∈Dt} .
Let us write for simplicity ut = ∂u
∂t and utt = ∂2u
∂t2 . The following result holds true:
Theorem 10.2 Suppose that u is a (smooth enough) solution of utt −c2u = 0
and that u = 0, ut = 0 on D0. Then u = 0 in W.
Proof Deﬁne
e(t) = 1
2

Dt
(u2
t + c2|∇u|2)dx .
We want to compute e′(t). We have, by the Reynolds transport theorem D.1,
e′(t) = 1
2

Dt
(u2
t + c2|∇u|2)tdx + 1
2

∂Dt
(u2
t + c2|∇u|2)V · ndSx ,
where V is the velocity of ∂Dt and n is the external unit normal on ∂Dt. Since ∂Dt
is the zero level-set of
Q(x, t) = |x −x0| −c(t0 −t)
and Dt = {x ∈Rn | Q(x, t) < 0}, we have
n = ∇Q
|∇Q| = x −x0
|x −x0| .

208
10
Hyperbolic PDEs
For a particle x = x(t) belonging to ∂Dt we have |x(t) −x0| = c(t0 −t); thus
differentiating with respect to t we have
−c = d
dt |x(t) −x0| = x(t) −x0
|x(t) −x0| · x′(t) = n · V .
Summing up, using the Cauchy–Schwarz inequality and the fact that for any a, b ∈
R it holds 2ab ≤a2 + b2, we obtain
e′(t) = 1
2

Dt
(2ut utt + 2c2
∇u · ∇ut



integrate by parts
)dx + 1
2

∂Dt
(u2
t + c2|∇u|2)(−c)dSx
=

Dt
ut uttdx −c2

Dt
u utdx
+ c2

∂Dt
∇u · n ut



C−S
dSx −c
2

∂Dt
(u2
t + c2|∇u|2)dSx
≤

Dt
ut (utt −c2u)



=0
dx + c
2

∂Dt
2c|∇u||ut|



2ab≤a2+b2
dSx −c
2

∂Dt
(u2
t + c2|∇u|2)dSx
≤c
2

∂Dt

c2|∇u|2 + u2
t

dSx −c
2

∂Dt
(u2
t + c2|∇u|2)dSx = 0 ,
so that e(t) ≤e(0) = 0 for each t ∈[0, t0]. Since e(t) ≥0, it follows e(t) = 0 for
each t ∈[0, t0]. In particular this gives ut = 0 in W and, since u = 0 on the basis
D0, it follows u = 0 in W.
⊓⊔
Remark 10.3 The real-life interpretation of this result looks clear: if you throw a
stone in a pond, the generated wave reaches the other side not immediately but after
a little time. Do you see how mathematics is powerful?
10.3
Exercises
Exercise 10.1 Suppose that u is a smooth solution in D × (0, T ) of the homoge-
neous Dirichlet boundary value problem associated to the wave equation
∂2u
∂t2 −c2u = 0
in D × (0, T ) .
Show that E(t) = ∥u′(t)∥2
L2(D) + c2∥∇u(t)∥2
L2(D) is constant for each t ∈[0, T ].

10.3
Exercises
209
Solution Fix t ∈(0, T ), and choose v = u′(t) as test function in the weak
formulation of the wave equation. We obtain
⟨u′′(t), u′(t)⟩+ c2

D
∇u(t) · ∇u′(t)dx = 0 .
This can be rewritten as
1
2
d
dt

D
u′(t)2dx + c2
2
d
dt

D
|∇u(t)|2dx = 0 ,
therefore

D u′(t)2dx + c2 
D |∇u(t)|2dx is constant for each t ∈[0, T ].
[The physical meaning of this equality is that for an event steered by the wave
equation the total energy (kinetic plus potential energy) is conserved.]
Exercise 10.2 Devise a variational formulation for the homogeneous Dirichlet
boundary value problem associated to the damped wave equation
∂2u
∂t2 + β ∂u
∂t −c2u = f
in D × (0, T ) ,
where β > 0 is a given parameter.
Solution The result is quite simple: by proceeding as for the wave equation,
we look for u ∈L2(0, T ; H 1
0 (D)), with u′
∈L2(0, T ; L2(D)) and u′′
∈
L2(0, T ; (H 1
0 (D))′), solution of
⟨u′′(t), v⟩+ β(u′(t), v)L2(D) + c2(∇u(t), ∇v)L2(D) = (f (t), v)L2(D)
∀v ∈H 1
0 (D) .
Exercise 10.3 Suppose that u is a smooth solution in D × (0, +∞) of the
homogeneous Dirichlet boundary value problem associated to the damped wave
equation described in the previous exercise, with f = 0. Show that the total energy
E(t) = ∥u′(t)∥2
L2(D) + c2∥∇u(t)∥2
L2(D) is decreasing.
Solution First of all, let us note that by proceeding as in Theorem 10.1 one could
prove the existence and uniqueness of a solution u ∈L2(0, T ; H 1
0 (D)) of the
damped wave equation, with u′ ∈L2(0, T ; L2(D)) and u′′ ∈L2(0, T ; (H 1
0 (D))′).
However, this would not permit us to use u′(t) as a test function in the weak
formulation, as it does not belong to H 1
0 (D) but only to L2(D). Thus let us proceed
formally and assume that u is a smooth solution and set u(0) = u0 and u′(0) = u1.
Fix t ∈(0, +∞), and choose v = u′(t) as test function in the weak formulation of
the damped wave equation. We have
⟨u′′(t), u′(t)⟩+ β

D
u′(t)2dx + c2

D
∇u(t) · ∇u′(t)dx = 0 .

210
10
Hyperbolic PDEs
This can be rewritten as
1
2
d
dt

D
u′(t)2dx + c2
2
d
dt

D
|∇u(t)|2dx + β

D
u′(t)2dx = 0 .
Therefore we have
E′(t) = −2β

D
u′(t)2dx ≤0 .
[The physical meaning of this equality is that for an event steered by the damped
wave equation the total energy (kinetic plus potential energy) is dissipated as time
increases.]
Exercise 10.4 Show that a suitable change of variable transforms the hyperbolic
problem
∂2u
∂t2 + Lu = f
in D × (0, T )
associated to a weakly coercive bilinear form BL(·, ·) into a damped hyperbolic
problem
∂2u
∂t2 + β ∂u
∂t + L♯u = ˆf
in D × (0, T )
associated to a coercive bilinear form BL♯(·, ·).
Solution Set w(t) = e−ηtu(t) where η = √σ > 0 and σ is the constant related to
weak coerciveness. Then
w′(t) = −ηe−ηtu(t) + e−ηtu′(t) = −ηw(t) + e−ηtu′(t)
w′′(t) = η2e−ηtu(t) −2ηe−ηtu′(t) + e−ηtu′′(t)
= η2w(t) −2η(w′(t) + ηw(t)) + e−ηtu′′(t)
= −η2w(t) −2ηw′(t) + e−ηtu′′(t) .
Thus from u′′ = f −Lu it follows
w′′(t) + 2√σw′(t) + Lw(t) + σw(t) = e−√σtf (t) ,
thus the desired result with L♯= L + σI, β = 2√σ and ˆf (t) = e−√σtf (t).
Exercise 10.5 Propose a numerical scheme for ﬁnding the approximate solution
of a hyperbolic problem which is based on the Galerkin approximation and on a
suitable ﬁnite difference scheme for discretizing ∂2u
∂t2 .

10.3
Exercises
211
Solution As in Exercise 9.4, let VM be a ﬁnite dimensional subspace of V (not
necessarily the space generated by the ﬁrst M element of an orthonormal basis of
V ), whose basis is denoted by {φ1, . . . , φM}. Choose a time-step t = T/K >
0, deﬁne tk = kt, k = 0, 1, . . ., K, and consider the (second order) centered
approximation of the second order derivative:
uk+1 −2uk + uk−1
(t)2
≈u′′(tk) , k = 1, . . . , K −1 .
Then the hyperbolic equation
⟨u′′(t), v⟩+ a(t; u(t), v) = ⟨F(t), v⟩
can be approximated by means of the following numerical scheme: being given
u0
M ∈VM, a suitable approximation of the initial datum u0, and u1
M ∈VM, a
suitable approximation of u(t1) constructed in terms of u0
M and of an approximation
u1,M of the initial datum u1 (for instance, u1
M = u0
M + t u1,M, or, better, a higher
order approximation), for each k = 1, . . . , K −1 ﬁnd uk+1
M
∈VM, solution of the
problem
)
uk+1
M
−2uk
M + uk−1
M
(t)2
, φi
*
H
+ a(tk; uk
M, φi) = ⟨F(tk), φi⟩, i = 1, . . . , M .
In the literature, this is often called the (second order) “explicit” Newmark method
(see, e.g., Raviart and Thomas [17, Sections 8.5 and 8.6]). Here the term “explicit”
is used though at each time step tk+1, k = 1, . . . , K −1, one has indeed to solve the
discretized linear problem
(uk+1
M , φi)H = −(t)2a(tk; uk
M, φi) + (2uk
M −uk−1
M , φi)H
+(t)2⟨F(tk), φi⟩, i = 1, . . . , M ;
this linear system is associated to the so-called mass matrix Mij = (φj, φi)H, where
the contribution of the bilinear form a(t; ·, ·) is not present, thus the operator L is
not playing any role.

Appendix A
Partition of Unity
A technical result that have been used in the previous chapters is that of partition of
unity. Let us explain which is its meaning.
Let K be a compact set in Rn, covered by a ﬁnite union of open sets, K ⊂
5M
i=1 Vi. Deﬁne
Vi,ε = {x ∈Vi | dist(x, ∂Vi) > ε} .
The ﬁrst result that we want to prove is the following one: we can ﬁnd other open
coverings 5M
i=1 Vi,ε0, 5M
i=1 Vi,2ε0, for a suitable ε0. Let us prove this assertion.
Proposition A.1 If a compact set K ⊂Rn is covered by a ﬁnite union of open sets,
K ⊂5M
i=1 Vi, then there exists ε0 > 0 such that K ⊂5M
i=1 Vi,ε0.
Proof We proceed by contradiction, and suppose that the statement is not true.
Then for each ε > 0 we can ﬁnd xε ∈K, xε /∈5M
i=1 Vi,ε. Since K is compact,
we can select a subsequence xεk →x0 ∈K, with εk →0. Then there exists
i0 ∈{1, . . ., M} such that x0 ∈Vi0. On the other hand, since xεk /∈5M
i=1 Vi,εk, in
particular xεk /∈Vi0,εk, and consequently we know that
dist(xεk, ∂Vi0) ≤εk −→0 .
Thus dist(x0, ∂Vi0) = 0, a contradiction as Vi0 is an open set.
⊓⊔
Now we can state the result concerning the partition of unity.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
213

214
A
Partition of Unity
Proposition A.2 Let K be a compact set in Rn, covered by a ﬁnite union of open
sets, K ⊂5M
i=1 Vi. Then there exist functions ωi : Rn →R, i = 1, . . ., M, with
the following properties:
(i) ωi ∈C∞
0 (Vi) for each i = 1, . . . , M;
(ii) 0 ≤ωi(x) ≤1 for each i = 1, . . . , M and for each x ∈Rn;
(iii) M
i=1 ωi(x) = 1 for each x ∈K.
Proof Take the characteristic function χi of Vi,2ε0 and for some ﬁxed ε < ε0
consider its molliﬁer ζi = χi ∗ηε deﬁned as
ζi(x) =

Rn χi(y)ηε(x −y)dy
,
x ∈Rn
(see Theorem 6.1). We know that ζi ∈C∞(Rn) and that ζi(x) ≥0 for all x ∈Rn,
as both χi and ηε are non-negative functions. Since the integral is indeed computed
on Vi,2ε0 ∩B(x, ε), where B(x, ε) = {y ∈Rn | |y −x| < ε}, we have ζi(x) = 0
for x /∈Vi,ε0, as in this case Vi,2ε0 ∩B(x, ε) = ∅; therefore ζi ∈C∞
0 (Vi). More
precisely, we can see that ζi(x) > 0 for x ∈Vi,2ε0−ε, ζi(x) = 0 for x /∈Vi,2ε0−ε,
namely supp ζi = Vi,2ε0−ε. We now deﬁne
ωi(x) =
⎧
⎨
⎩
ζi(x)
M
j=1 ζj (x)
if x ∈Vi,2ε0−ε
0
if x ∈Rn \ Vi,2ε0−ε .
Therefore ωi ∈C∞
0 (Rn), supp ωi = Vi,2ε0−ε ⊂Vi,ε0 ⊂⊂Vi, ωi(x) ≥0 for all x ∈
Rn and ωi(x) ≤1 for all x ∈Rn. Finally, for x ∈K ⊂5M
i=1 Vi,2ε0 ⊂5M
i=1 Vi,2ε0−ε
let us deﬁne
Ix = {i = 1, . . . , M | x ∈Vi,2ε0−ε} ;
then we have
M

i=1
ωi(x) =

s∈Ix
ωs(x) =

s∈Ix

ζs(x)

s∈Ix ζs(x)

= 1 .
⊓⊔

Appendix B
Lipschitz Continuous Domains
and Smooth Domains
In this appendix we clarify the meaning we give to the concept of “regularity” of
the boundary of a domain.
First of all we have:
Deﬁnition B.1 Let O ⊂Rn be an open set. We say that a function q : O →Rn is a
Lipschitz function in O, and we write q ∈Lip(O), if there exists a constant L > 0
such that
|q(x) −q(y)| ≤L|x −y|
for every x, y ∈O.
To give an example, it is easily veriﬁed that, if O is a bounded open set, then
a function q ∈C1(O) (namely, the restriction to O of a C1(Rn)-function) is a
Lipschitz function in O.
Consider now a bounded, connected, open set D ⊂Rn. Then the Lipschitz
continuous regularity of its boundary ∂D is deﬁned as follows:
Deﬁnition B.2 We say that D is a Lipschitz domain, or equivalently a domain with
a Lipschitz continuous boundary, if for every point p ∈∂D there exist an open ball
Bp centered at p, an open ball 
B0 centered at 0, a rigid body motion Rp : Bp →
B0
given by Rpx = Apx+bp, with Rpp = 0, Ap an orthogonal n×n-matrix, bp ∈Rn,
and a map ϕ : Q →R, where Q = {ξ ∈
B0 | ξn = 0}, such that
1. ϕ ∈Lip(Q) and ϕ(0) = 0
2. Rp(Bp ∩∂D) = {(ξ′, ξn) ∈
B0 | ξn = ϕ(ξ′), ξ′ ∈Q}
3. Rp(Bp ∩D) = {(ξ′, ξn) ∈
B0 | ξn > ϕ(ξ′), ξ′ ∈Q} .
The meaning of the second condition is that ∂D coincides locally with the graph of
a Lipschitz function; the third condition asserts that D is locally situated on one part
of its boundary ∂D.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
215

216
B
Lipschitz Continuous Domains and Smooth Domains
Fig. B.1 A (polyhedral) domain whose boundary is a Lipschitz manifold but it is not locally the
graph of a Lipschitz function. The “bad” points are the four vertices of the square that is the
interface between the two bricks (courtesy of Jarno and Beatrice)
In particular, this deﬁnition says that a Lipschitz domain is a domain whose
boundary is a manifold with a system of local charts that are invertible Lipschitz
functions, namely, a Lipschitz manifold.
It can be interesting to note the the opposite is not true: if you have a good
geometrical intuition you can verify that the boundary of the two-brick set described
in Fig. B.1 is an example of a surface that is not locally the graph of a Lipschitz
function. On the other hand, it is a Lipschitz manifold (for complete description of
this situation, see for instance a recent paper by Licht [14]).
For a Lipschitz domain at almost every point x ∈∂D a tangent (hyper)plane is
well deﬁned, together with the outward unit normal vector n.
Deﬁnition B.3 We say that D is a domain of class Ck, or equivalently a domain
with a Ck-boundary, k ≥1, and we write ∂D ∈Ck, if the function ϕ in
Deﬁnition B.2 belongs to Ck.

Appendix C
Integration by Parts for Smooth
Functions and Vector Fields
This appendix is devoted to various “integration by parts” formulas that have been
used several times in the previous chapters.
Let us start from the “fundamental theorem of calculus” (whose proof can be
found in any Calculus textbook): the integral of a derivative of a function f can be
explicitly expressed by an integral of f over a lower dimensional set.
Theorem C.1 (Fundamental Theorem of Calculus) Let D ⊂Rn be a bounded,
connected, open set with a Lipschitz continuous boundary, and let f : D →R be a
function of class C1(D). Then

D
Dif dx =

∂D
f nidSx ,
(C.1)
where n is the outward unit normal vector, deﬁned on ∂D for almost every x ∈∂D.
From this theorem we easily obtain many well-known results:
Theorem C.2 (Integration by Parts)
Let D ⊂Rn be a bounded, connected, open
set with a Lipschitz continuous boundary, and let f, g : D →R be two functions of
class C1(D). Then

D
Dif gdx = −

D
f Digdx +

∂D
fgnidSx .
(C.2)
Proof It is enough to remember that Di(fg) = Difg + f Dig and to apply
Theorem C.1.
⊓⊔
Theorem C.3 (Divergence or Gauss Theorem)
Let D ⊂Rn be a bounded,
connected, open set with a Lipschitz continuous boundary, and let F : D →Rn
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
217

218
C
Integration by Parts for Smooth Functions and Vector Fields
be a vector ﬁeld of class C1(D). Then

D
divFdx =

∂D
F · n dSx .
(C.3)
Proof Since div F = n
i=1 DiFi, one has only to apply Theorem C.2 for f = Fi,
g = 1 and to add over i = 1, . . . , n.
⊓⊔
Theorem C.4 Let D ⊂Rn be a bounded, connected, open set with a Lipschitz
continuous boundary, and let F : D →Rn be a vector ﬁeld of class C1(D), g :
D →R be a function of class C1(D). Then

D
divF gdx = −

D
F · ∇gdx +

∂D
F · n g dSx .
(C.4)
In particular, taking F ∈C∞
0 (D) and g ∈C∞
0 (D) one veriﬁes that −∇is the
(formal) transpose operator of div.
Proof It is enough to apply Theorem C.2 to f = Fi and to add over i = 1, . . ., n.
⊓⊔
Theorem C.5 Let D ⊂Rn be a bounded, connected, open set with a Lipschitz
continuous boundary, and let f : D →R be a function of class C2(D), g : D →R
be a function of class C1(D). Then

D
(−f ) gdx =

D
∇f · ∇gdx −

∂D
∇f · n g dSx .
(C.5)
In particular, taking g = 1 it follows

D
f dx =

∂D
∇f · n dSx .
(C.6)
Proof Recalling that −f = −div∇f , it is enough to apply Theorem C.4 to F =
−∇f .
⊓⊔
Theorem C.6 Let D ⊂Rn be a bounded, connected, open set with a Lipschitz
continuous boundary, and let F : D →Rn be a vector ﬁeld of class C1(D), G :
D →Rn be a vector ﬁeld of class C2(D). Then

D
(−∇divG) · Fdx =

D
divG divFdx −

∂D
divG F · n dSx .
(C.7)
Proof It is enough to apply Theorem C.4 to g = divG.
⊓⊔
Theorem C.7 Let D ⊂Rn be a bounded, connected, open set with a Lipschitz
continuous boundary, and let F, G : D →Rn be two vector ﬁelds of class C1(D).

C
Integration by Parts for Smooth Functions and Vector Fields
219
Then

D
curlF · Gdx =

D
F · curlGdx +

∂D
n × F · G dSx .
(C.8)
In particular, taking F ∈C∞
0 (D) and G ∈C∞
0 (D) one veriﬁes that curl is
(formally) equal to its transpose operator.
Proof Recalling that curlF can be formally computed as the vector product ∇× F,
one has only to apply Theorem C.2 to all the terms of the scalar product curlF · G
and to check that the result follows.
⊓⊔
Theorem C.8 Let D ⊂Rn be a bounded, connected, open set with a Lipschitz
continuous boundary, and let M : D →Rn be a vector ﬁeld of class C2(D),
G : D →Rn be a vector ﬁeld of class C1(D). Then

D
curlcurlM · Gdx =

D
curl M · curlGdx +

∂D
n × curlM · G dSx .
(C.9)
Proof Just take F = curlM in Theorem C.7.
⊓⊔

Appendix D
Reynolds Transport Theorem
In this appendix we are concerned with a well-known result of differential calculus,
which is often useful in continuum mechanics. In the literature we are not aware of
a reference presenting its proof in a detailed way (but surely it exists!). Anyway, for
the ease of the reader we decided to present the proof here.
We need a preliminary result. Let us denote by Lip(Rn) the space of Lipschitz
functions on Rn.
Lemma D.1 Consider v = v(t, X) ∈L1(0, +∞; Lip(Rn)), x ∈Rn, and let " =
"(t, x) be the solution of the Cauchy problem
⎧
⎪⎨
⎪⎩
d
dt "(t, x) = v(t, "(t, x)) , t > 0
"(0, x) = x .
(D.1)
Deﬁning j(t, x) = det Jacx "(t, x), it holds
dj
dt (t, x) = [(divXv) ◦"](t, x)j(t, x) .
(D.2)
Remark D.1 In ﬂuid dynamics one says that v is the velocity of the ﬂow ": in other
words, the position "(t, x) is determined by integrating the velocity v along the
trajectories of the ﬂuid particles. This means that "(t, x) is the position at time t of
a particle that at time 0 was at x: then X = "(t, x) is the Lagrangian coordinate,
whereas x is the Eulerian coordinate.
Proof Being j a determinant, its derivative is given by
dj
dt =
n

k=1
det Mk ,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
221

222
D
Reynolds Transport Theorem
where, for k = 2, . . . , n −1, the matrix Mk is given by
Mk =
⎛
⎜⎜⎜⎜⎜⎜⎝
Dx1"1
. . .
Dxn"1
...
...
...
DtDx1"k . . . DtDxn"k
...
...
...
Dx1"n
. . .
Dxn"n
⎞
⎟⎟⎟⎟⎟⎟⎠
,
with obvious modiﬁcation for the cases k = 1 and k = n. For k, j = 1, . . . , n from
(D.1) we have
DtDxj "k = Dxj Dt"k = Dxj (vk ◦") ,
where we have denoted by g ◦" the function (t, x) →g(t, "(t, x)). Moreover, by
means of the chain rule we also ﬁnd, for k, j = 1, . . . , n,
Dxj (vk ◦") =
n

s=1
 ∂vk
∂Xs
◦"

Dxj "s .
Take for a while k = 2, . . . , n −1. Using the two last results we obtain
Mk =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
Dx1"1
. . .
Dxn"1
...
...
...
n

s=1
 ∂vk
∂Xs
◦"

Dx1"s . . .
n

s=1
 ∂vk
∂Xs
◦"

Dxn"s
...
...
...
Dx1"n
. . .
Dxn"n
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
←k-th row .
Since the determinant is linear with respect to the rows we ﬁnd
det Mk =
n

s=1
 ∂vk
∂Xs
◦"

det
⎛
⎜⎜⎜⎜⎜⎜⎝
Dx1"1 . . . Dxn"1
...
...
...
Dx1"s . . . Dxn"s
...
...
...
Dx1"n . . . Dxn"n
⎞
⎟⎟⎟⎟⎟⎟⎠
←k-th row .

D
Reynolds Transport Theorem
223
When s ̸= k the matrix has two rows that are equal, thus its determinant vanishes;
therefore
det Mk =
 ∂vk
∂Xk
◦"

det
⎛
⎜⎜⎜⎜⎜⎜⎝
Dx1"1 . . . Dxn"1
...
...
...
Dx1"k . . . Dxn"k
...
...
...
Dx1"n . . . Dxn"n
⎞
⎟⎟⎟⎟⎟⎟⎠
=
 ∂vk
∂Xk
◦"

j .
For k = 1 and k = n we have the same result, with straightforward modiﬁcation.
Adding over k form 1 to n we ﬁnd (D.2).
⊓⊔
We are now ready for the main result. Let D0 ⊂Rn be a bounded, connected,
open set with a Lipschitz continuous boundary. For t > 0 deﬁne
Dt = {X ∈Rn | X = "(t, x) for some x ∈D0}
and
W = {(t, X) ∈(0, +∞) × Rn | X ∈Dt} .
Theorem D.1 (Reynolds Transport Theorem)
Let f : W →R be a (smooth
enough) scalar function. Then
d
dt

Dt
f dX

=

Dt
∂f
∂t dX +

∂Dt
v · n f dSX ,
where v is the velocity of the boundary ∂Dt.
Proof For any ﬁxed t consider the change of variables X = "(t, x), which yields

Dt
f (t, X) dX =

D0
f (t, "(t, x))| detJacx "(t, x)| dx .
(D.3)
Since j(0, x) = det Jacx "(0, x) = det Jac Id = 1, from (D.2) we ﬁnd
j(t, x) = exp
 t
0
(divXv)(s, "(s, x)) ds

> 0 .
Thus in (D.3) we can drop the absolute value of the determinant. Let us now
differentiate with respect to t. Since the integral in D0 is on a ﬁxed set, we can

224
D
Reynolds Transport Theorem
differentiate inside the integral and we ﬁnd
d
dt

Dt
f dX = d
dt

D0
f (t, "(t, x)) det Jacx "(t, x) dx
=

D0
d
dt [f (t, "(t, x))] det Jacx "(t, x) dx
+

D0
f (t, "(t, x)) d
dt [det Jacx "(t, x)] dx .
(D.4)
By the chain rule, and taking (D.1) into account, the ﬁrst factor in the ﬁrst term of
(D.4) can be rewritten as
d
dt [f (t, "(t, x))] = ∂f
∂t (t, "(t, x)) +
n

i=1
∂f
∂Xi
(t, "(t, x))d"i
dt (t, x)
= ∂f
∂t (t, "(t, x)) +
n

i=1
∂f
∂Xi
(t, "(t, x))vi(t, "(t, x))
=
∂f
∂t + v · ∇Xf

(t, "(t, x)) .
Using (D.2) in the second term of (D.4) we obtain
f (t, "(t, x)) d
dt [det Jacx "(t, x)] = (f divXv)(t, "(t, x)) det Jacx "(t, x) .
In conclusion, we have seen that
d
dt

Dt
f (t, X) dX =

D0
∂f
∂t + v · ∇Xf + f divX v

(t, "(t, x)) det Jacx "(t, x) dx
=

D0
∂f
∂t + divX(f v)

(t, "(t, x)) det Jacx "(t, x) dx .
Rewriting the integral at the right hand side by means of the change of variable
X = "(t, x) we have
d
dt

Dt
f (t, X) dX =

Dt
∂f
∂t + divX(f v)

(t, X) dX ,
hence the thesis by using the divergence theorem C.3.
⊓⊔

Appendix E
Gronwall Lemma
The Gronwall lemma is an useful tool in the analysis of evolution equations. Its
statement is the following.
Lemma E.1 (Gronwall Lemma) Let f ∈L1(0, T ) be a non-negative function, g
and ϕ be continuous functions in [0, T ]. If ϕ satisﬁes
ϕ(t) ≤g(t) +
 t
0
f (τ)ϕ(τ)dτ
∀t ∈[0, T ] ,
then
ϕ(t) ≤g(t) +
 t
0
f (s)g(s) exp
 t
s
f (τ)dτ

ds
∀t ∈[0, T ] .
(E.1)
The proof of this lemma will be given below. For the moment let us show some
consequences of it.
Corollary E.1 If g is a non-decreasing function, then
ϕ(t) ≤g(t) exp
 t
0
f (τ)dτ

∀t ∈[0, T ] .
Proof If g is non-decreasing, we have g(s) ≤g(t) for 0 ≤s ≤t, thus from
Eq. (E.1)
ϕ(t) ≤g(t)
$
1 +
 t
0
f (s) exp
 t
s
f (τ)dτ

ds
%
.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
225

226
E
Gronwall Lemma
Since
d
ds

exp
 t
s
f (τ)dτ

= −exp
 t
s
f (τ)dτ

f (s) ,
we have that
 t
0
f (s) exp
 t
s
f (τ)dτ

ds = −
 t
0
d
ds

exp
 t
s
f (τ)dτ

ds
= −

1 −exp
 t
0
f (τ)dτ

,
hence the result.
⊓⊔
Corollary E.2 If g(t) = k1 and f (t) = k2, then
ϕ(t) ≤k1ek2t
∀t ∈[0, T ] .
Proof Just apply Corollary E.1.
⊓⊔
Proof (of Lemma E.1) For s ∈[0, T ] set R(s) =
 s
0 f (τ)ϕ(τ)dτ. The assumption
yields
R′(s) = f (s)ϕ(s) ≤f (s)[g(s) + R(s)] .
Then
d
ds
$
R(s) exp

−
 s
0
f (τ)dτ
%
= R′(s) exp

−
 s
0
f (τ)dτ

−R(s)f (s) exp

−
 s
0
f (τ)dτ

= [R′(s) −R(s)f (s)] exp

−
 s
0
f (τ)dτ

≤f (s)g(s) exp

−
 s
0
f (τ)dτ

.
Integrating over [0, t], we ﬁnd, as R(0) = 0,
R(t) exp

−
 t
0
f (τ)dτ

≤
 t
0
f (s)g(s) exp

−
 s
0
f (τ)dτ

ds ,

E
Gronwall Lemma
227
thus
R(t) ≤
 t
0
f (s)g(s) exp
 t
s
f (τ)dτ

ds ,
which gives the stated result as a consequence of the assumption ϕ(t) ≤g(t)
+R(t).
⊓⊔

Appendix F
Necessary and Sufﬁcient Conditions
for the Well-Posedness of the Variational
Problem
We present here the well-posedness result for a general variational problem of the
form
ﬁnd u ∈V : B(u, v) = F(v)
∀v ∈V ,
(F.1)
where V is a Hilbert space, B(·, ·) : V × V →R is a bounded bilinear form and
F(·) : V →R is a bounded linear functional.
Theorem F.1 Problem (F.1) is well-posed (namely, it has one and only one solution
u for each bounded and linear functional F, and the solution map F →u is
bounded) if and only if the following conditions are satisﬁed:
(i) there exists α > 0 :
inf
w∈V,w̸=0
sup
v∈V,v̸=0
B(w, v)
∥w∥V ∥v∥V
≥α
(ii) if B(w, v) = 0 for all w ∈V then v = 0 .
Proof We introduce the linear and bounded functionals Q : V →V ′ and QT :
V →V ′ deﬁned as
⟨Qw, v⟩= B(w, v)
∀v ∈V , ⟨QT v, w⟩= B(w, v)
∀w ∈V .
The well-posedness statement is thus reformulated as: Q is an isomorphism from V
onto V ′.
(⇒)
Suppose that Q is an isomorphism from V onto V ′. Then N(Q) = {0} and
R(Q) = V ′, thus in particular R(Q) is closed. From the closed range theorem
(see Yosida [23, Theorem 1, p. 205]) R(Q) = N(QT )♯, thus N(QT )♯= V ′
and N(QT ) = {0}. This means that QT v = 0 implies v = 0, namely, that
⟨QT v, w⟩= B(w, v) = 0 for each w ∈V implies v = 0. This is condition (ii).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
229

230
F
Necessary and Sufﬁcient Conditions for the Well-Posedness of the Variational. . .
Moreover, since Q is an isomorphism from V onto V ′, its inverse is bounded,
namely, there exists α > 0 such that ∥Qw∥V ′ ≥α∥w∥V for each w ∈V . This
means
sup
v∈V,v̸=0
⟨Qw, v⟩
∥v∥V
=
sup
v∈V,v̸=0
B(w, v)
∥v∥V
≥α∥w∥V
∀w ∈V ,
thus condition (i).
(⇐)
Let us assume now that (i) and (ii) are satisﬁed. We can follow the lines of
the proof of the Lax–Milgram theorem 2.1. From condition (i) it follows
∥Qw∥V ′ =
sup
v∈V,v̸=0
⟨Qw, v⟩
∥v∥V
≥α∥w∥V ,
(F.2)
as a consequence we derive that Q is one-to-one, as from Qw = 0 it follows
at once w = 0, and that Q−1 is bounded (at the moment, from R(Q) to V ).
Moreover, we can also prove that R(Q) is closed. In fact, consider a sequence
Qvk ∈R(Q) such that Qvk →ω ∈V ′. In particular, Qvk is a Cauchy sequence
in V ′, and from (F.2) we have that vk is a Cauchy sequence in V . Therefore we
ﬁnd v0 ∈V such that vk →v0 in V , thus Qvk →Qv0 in V ′, which gives
Qv0 = ω.
Since R(Q) is closed, to prove that R(Q) = V ′ it is enough to show that
R(Q)♯= {0}, namely, that ⟨Qw, v⟩= 0 for all w ∈V implies v = 0: since
⟨Qw, v⟩= B(w, v), this is exactly condition (ii).
⊓⊔
Remark F.1 It is straightforward to verify that the coerciveness of B(·, ·) implies
both (i) and (ii).

References
1. Adams, R.A.: Sobolev Spaces. Academic Press, New York (1975)
2. Banach, S.: Théorie des opérations linéaires. Instytut Matematyczny Polskiej Akademii Nauk,
Warszawa (1932)
3. Bogovskii, M.E.: Solution of the ﬁrst boundary value problem for an equation of continuity
of an incompressible medium. Dokl. Akad. Nauk SSSR 248(5), 1037–1040 (1979) (Russian).
[English translation: Soviet Math. Dokl. 20, 1094–1098 (1979)]
4. Dautray, R., Lions, J.-L.: Mathematical Analysis and Numerical Methods for Science and
Technology, vol. 2. Springer, Berlin (1988)
5. Dautray, R., Lions, J.-L.: Mathematical Analysis and Numerical Methods for Science and
Technology, vol. 5. Springer, Berlin (1992)
6. Evans, L.C.:
Partial Differential Equations.
American Mathematical Society, Providence
(1998)
7. Friedrichs, K.O.: The identity of weak and strong extensions of differential operators. Trans.
Am. Math. Soc. 55(1), 132–151 (1944)
8. Gilbarg, D., Trudinger, N.S.: Elliptic Partial Differential Equations of Second Order. Springer,
Berlin (1977)
9. Kellogg, O.D.: Foundations of Potential Theory. Ungar, New York (1929)
10. Kinderlehrer, D., Stampacchia, G.:
An Introduction to Variational Inequalities and Their
Applications. SIAM, Philadelphia (2000)
11. Ladyženskaya, O.A.: On integral estimates, convergence, approximate methods, and solution
in functionals for elliptic operators. Vestnik Leningrad. Univ. 13(7), 60–69 (1958) (Russian)
12. Ladyženskaja, O.A., Solonnikov, V.A., Ural’ceva, N.N.: Linear and Quasilinear Equations of
Parabolic Type. American Mathematical Society, Providence (1968)
13. Leray, J.: Sur le mouvement d’un liquide visqueux emplissant l’espace. Acta Math. 63, 193–
248 (1934)
14. Licht, M.W.: Smoothed projections over weakly Lipschitz domains. Math. Comput. 88(315),
179–210 (2019)
15. McLean, W.:
Strongly Elliptic Systems and Boundary Integral Equations.
Cambridge
University Press, Cambridge (2000)
16. Monk, P.: Finite Element Methods for Maxwell’s Equations. Oxford University Press, Oxford
(2003)
17. Raviart, P.A., Thomas, J.M.: Introduction à l’Analyse Numérique des Équations aux Dérivées
Partielles. Masson, Paris (1983)
18. Salsa, S.: Partial Differential Equations in Action, 2nd edn. Springer, Milan (2015)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
231

232
References
19. Sobolev, S.L.: Méthode nouvelle à résoudre le problème de Cauchy pour les équations linéaires
hyperboliques normales. Mat. Sbornik 1(43), 39–72 (1936)
20. Sobolev, S.L.: On a boundary value problem for polyharmonic equations. Mat. Sbornik 2(44),
465–499 (1937) (Russian). [English translation: Am. Math. Soc. Transl. (2) 33, 1–40 (1963)]
21. Sobolev, S.L.: On a theorem in functional analysis. Mat. Sbornik 4(46), 471–497 (1938)
(Russian). [English translation: Am. Math. Soc. Transl. (2) 34, 39–68 (1963)]
22. Stein, E.M.:
Singular Integrals and Differentiability Properties of Functions.
Princeton
University Press, Princeton (1970)
23. Yosida, K.: Functional Analysis, 4th edn. Springer, Berlin (1974)

Index
B
Basis
orthonormal, 13, 118, 119, 182, 197
Bilinear form, 18, 20, 142, 180, 186, 197
adjoint, 110
bounded, 20, 21, 60, 134, 136, 148, 159,
181, 229
coercive, 20, 21, 64, 135, 136, 148, 181,
210, 230
weakly coercive, 61, 180, 187, 193
Boundary value problem
Dirichlet, 10, 13, 17, 54, 61, 64, 111, 115,
118, 119, 124, 127, 135, 140, 149,
168, 177, 186, 187, 195, 206
mixed, 10, 57, 61, 130, 177, 186, 187, 195,
207
Neumann, 10, 55, 61, 65, 69, 114, 128, 177,
186, 187, 195, 206
Robin, 10, 58, 61, 128, 177, 186, 187, 195,
206
C
Cauchy sequence, 19, 32, 46, 91, 175, 230
Compactness, 94, 134
Compatibility condition, 70, 71
Consistency, 136, 169
Constrained minimization, 151
Constraint, 151, 152
D
Difference quotient, 126, 127, 143, 144
Dirac δ “function”, 43
E
Eigenvalue, 11–13, 115, 118–120, 140, 141
Eigenvector, 12, 13, 115, 118, 119
Equation
boundary integral, 15
damped wave, 3, 209, 210
eddy current, 4
elasticity, 3, 78
elliptic, 9
evolution, 177
heat, 2, 177
hyperbolic, 195, 205
Laplace, 2
Maxwell, 4, 5
parabolic, 177
Poisson, 2, 4, 69, 120
Stokes, 75, 157, 167, 172
wave, 3
Error estimate, 136, 171
F
Finite elements, 136, 172
Finite propagation speed, 195, 207
Fourier expansion, 13
Fredholm alternative, 109–111, 113, 114, 116,
124
Function
locally summable, 40
I
Inequality
Poincaré, 64, 72, 79, 87–89, 96, 103, 166,
191
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2020
A. Valli, A Compact Course on Linear PDEs, UNITEXT 126,
https://doi.org/10.1007/978-3-030-58205-0
233

234
Index
Poincaré-type, 67, 72, 98
trace, 56, 58, 60, 89, 90, 93
Inf–sup condition, 161, 162, 169, 173
Integration by parts, 93, 104, 126, 143, 145,
180, 185, 217
K
Kernel, 109, 160, 162, 166, 169
L
Lagrange multiplier, 151, 152
Lagrangian, 152–155
Lemma
du Bois-Reymond, 41, 99
Gronwall, 200, 203, 205, 225
Linear functional, 18, 20
bounded, 21, 60, 136, 229
Lipschitz domain, 215, 216
Lipschitz manifold, 216
M
Matrix
positive deﬁnite, 10, 155, 158–160, 183,
193
Maximum principle, 120, 177, 188
Method
backward Euler, 192
Galerkin, 135, 136, 149, 168, 169, 182,
192, 210
Newmark, 211
separation of variables, 13, 26
Molliﬁer, 84, 101
O
Operator
adjoint, 110, 115, 138, 160, 174
bounded, 19, 27, 159, 160, 162
closed, 41, 139
coercive, 160, 162, 166
compact, 110, 112, 115, 118
self-adjoint, 118
solution, 117, 139
P
Partial differential operator
elliptic, 11, 18, 53, 120, 124, 141, 157, 177,
187, 188, 205
Laplace, 2, 27, 63, 66, 114, 119, 129, 130,
140, 146, 157
principal part, 10, 205
symmetric elliptic, 118, 206
Partition of unity, 85, 213
Polar coordinates, 129, 130, 146, 147
Polar set, 160
Potential theory, 16
Precompactness, 94–96, 110
R
Range, 22, 33, 109, 159, 160
Rayleigh quotient, 140
Regularity
interior, 126, 127
up to the boundary, 127, 128
Resolvent set, 115
S
Saddle point, 151, 154, 155
Space
Banach, 46
dual, 23, 158, 160, 173, 178, 190
Hilbert, 19–21, 34, 37, 47, 54, 60, 64–67,
74, 80, 81, 92, 115, 118, 135, 138,
139, 142, 145, 158, 160, 161, 165,
174, 175, 177, 178, 184, 186, 190,
191, 200, 229
pre-hilbertian, 35, 37
reﬂexive Banach, 47, 107
separable Banach, 47
separable Hilbert, 118, 180, 181, 187, 195,
197
Sobolev, 44, 55, 131, 178
Spectrum, 115, 118
Subsolution, 121, 124, 188, 189
Supersolution, 121, 122, 124, 188, 190
System
ﬁrst order elliptic, 158, 167, 172
T
Theorem
Céa, 136
closed graph, 139
closed range, 160, 174, 229
divergence, 14, 70, 165, 217, 224
extension, 85
Lax–Milgram, 59, 73, 79, 136, 139, 160,
163, 168, 230
projection, 23, 35, 37, 173
Rellich, 94, 97, 107, 114

Index
235
Reynolds transport, 207, 221, 223
Riesz representation, 17, 19, 21, 22, 34, 37,
74, 80, 81, 175, 190
Trace, 65, 89, 90, 93, 105
W
Weak
derivative, 39–41, 49, 178, 179, 196
formulation, 55, 56, 58, 59


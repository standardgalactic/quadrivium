Josef Bigun
Vision with Direction

Josef Bigun
Vision with Direction
A Systematic Introduction
to Image Processing and Computer Vision
With 146 Figures, including 130 in Color
123

Josef Bigun
IDE-Sektionen
Box 823
SE-30118, Halmstad
Sweden
josef.bigun@ide.hh.se
www.hh.se/staff/josef
Library of Congress Control Number: 2005934891
ACM Computing Classiﬁcation (1998): I.4, I.5, I.3, I.2.10
ISBN-10 3-540-27322-0 Springer Berlin Heidelberg New York
ISBN-13 978-3-540-27322-6 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material
is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broad-
casting, reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of
this publication or parts thereof is permitted only under the provisions of the German Copyright Law
of September 9, 1965, in its current version, and permission for use must always be obtained from
Springer. Violations are liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
© Springer-Verlag Berlin Heidelberg 2006
Printed in Germany
The use of general descriptive names, registered names, trademarks, etc. in this publication does not
imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant pro-
tective laws and regulations and therefore free for general use.
Typeset by the author using a Springer TEX macro package
Production: LE-TEX Jelonek, Schmidt & Vöckler GbR, Leipzig
Cover design: KünkelLopka Werbeagentur, Heidelberg
Printed on acid-free paper
45/3142/YL - 5 4 3 2 1 0

To my parents, H. and S. Bigun

Preface
Image analysis is a computational feat which humans show excellence in, in compar-
ison with computers. Yet the list of applications that rely on automatic processing of
images has been growing at a fast pace. Biometric authentication by face, ﬁngerprint,
and iris, online character recognition in cell phones as well as drug design tools are
but a few of its benefactors appearing on the headlines.
This is, of course, facilitated by the valuable output of the resarch community
in the past 30 years. The pattern recognition and computer vision communities that
study image analysis have large conferences, which regularly draw 1000 partici-
pants. In a way this is not surprising, because much of the human-speciﬁc activities
critically rely on intelligent use of vision. If routine parts of these activities can be
automated, much is to be gained in comfort and sustainable development. The re-
search ﬁeld could equally be called visual intelligence because it concerns nearly all
activities of awake humans. Humans use or rely on pictures or pictorial languages
to represent, analyze, and develop abstract metaphors related to nearly every aspect
of thinking and behaving, be it science, mathematics, philosopy, religion, music, or
emotions.
The present volume is an introductory textbook on signal analysis of visual com-
putation for senior-level undergraduates or for graduate students in science and en-
gineering. My modest goal has been to present the frequently used techniques to
analyze images in a common framework–directional image processing. In that, I am
certainly inﬂuenced by the massive evidence of intricate directional signal process-
ing being accumulated on human vision. My hope is that the contents of the present
text will be useful to a broad category of knowledge workers, not only those who
are technically oriented. To understand and reveal the secrets of, in my view, the
most advanced signal analysis “system” of the known universe, primate vision, is a
great challenge. It will predictably require cross-ﬁeld fertilizations of many sorts in
science, not the least among computer vision, neurobiology, and psychology.
The book has ﬁve parts, which can be studied fairly independently. These stud-
ies are most comfortable if the reader has the equivalent mathematical knowledge
acquired during the ﬁrst years of engineering studies. Otherwise, the lemmas and
theorems can be read to acquire a quick overview, even with a weaker theoretical

VIII
Preface
background. Part I presents brieﬂy a current account of the human vision system
with short notes to its parallels in computer vision. Part II treats the theory of lin-
ear systems, including the various versions of Fourier transform, with illustrations
from image signals. Part III treats single direction in images, including the ten-
sor theory for direction representation and estimation. Generalized beyond Carte-
sian coordinates, an abstraction of the direction concept to other coordinates is of-
fered. Here, the reader meets an important tool of computer vision, the Hough trans-
form and its generalized version, in a novel presentation. Part IV presents the con-
cept of group direction, which models increased shape complexities. Finally, Part
V presents the grouping tools that can be used in conjunction with directional pro-
cessing. These include clustering, feature dimension reduction, boundary estimation,
and elementary morphological operations. Information on downloadable laboratory
exercises (in Matlab) based on this book is available at the homepage of the author
(http://www.hh.se/staff/josef).
I am indebted to several people for their wisdom and the help that they gave me
while I was writing this book, and before. I came in contact with image analysis by
reading the publications of Prof. G¨osta H. Granlund as his PhD student and during
the beautiful discussions in his research group at Link¨oping University, not the least
with Prof. Hans Knutsson, in the mid-1980s. This heritage is unmistakenly recogniz-
able in my text. In the 1990s, during my employment at the Swiss Federal Institute
of Technology in Lausanne, I greatly enjoyed working with Prof. Hans du Buf on
textures. The traces of this collaboration are distinctly visible in the volume, too.
I have abundantly learned from my former and present PhD students, some of
their work and devotion is not only alive in my memory and daily work, but also in
the graphics and contents of this volume. I wish to mention, alphabetically, Yaregal
Assabie, Serge Ayer, Benoit Duc, Maycel Faraj, Stefan Fischer, Hartwig Fronthaler,
Ole Hansen, Klaus Kollreider, Kenneth Nilsson, Martin Persson, Lalith Premaratne,
Philippe Schroeter, and Fabrizio Smeraldi. As teachers in two image analysis courses
using drafts of this volume, Kenneth, Martin, and Fabrizio provided, additionally,
important feedback from students.
I was privileged to have other coworkers and students who have helped me out
along the “voyage” that writing a book is. I wish to name those whose contributions
have been most apparent, alphabetically, Markus B¨ckman, Kwok-wai Choy, Stefan
Karlsson, Nadeem Khan, Iivari Kunttu, Robert Lamprecht, Leena Lepist¨o, Madis
Listak, Henrik Olsson, Werner Pomwenger, Bernd Resch, Peter Romirer-Maierhofer,
Radakrishnan Poomari, Rene Schirninger, Derk Wesemann, Heike Walter, and Niklas
Zeiner.
At the ﬁnal port of this voyage, I wish to mention not the least my family, who
not only put up with me writing a book, often invading the private sphere, but who
also ﬁlled the breach and encouraged me with appreciated “kicks” that have taken
me out of local minima.
I thank you all for having enjoyed the writing of this book and I hope that the
reader will enjoy it too.
August 2005
J. Bigun

Contents
Part I Human and Computer Vision
1
Neuronal Pathways of Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Optics and Visual Fields of the Eye . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Photoreceptors of the Retina . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Ganglion Cells of the Retina and Receptive Fields . . . . . . . . . . . . . .
7
1.4
The Optic Chiasm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.5
Lateral Geniculate Nucleus (LGN) . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.6
The Primary Visual Cortex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.7
Spatial Direction, Velocity, and Frequency Preference . . . . . . . . . . .
13
1.8
Face Recognition in Humans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.9
Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2
Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.1
Lens and Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2
Retina and Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3
Neuronal Operations and Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.4
The 1931 CIE Chromaticity Diagram and Colorimetry . . . . . . . . . .
26
2.5
RGB: Red, Green, Blue Color Space . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.6
HSB: Hue, Saturation, Brightness Color Space . . . . . . . . . . . . . . . . .
31
Part II Linear Tools of Vision
3
Discrete Images and Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.1
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2
Discrete Image Types, Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3
Norms of Vectors and Distances Between Points . . . . . . . . . . . . . . .
40
3.4
Scalar Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.5
Orthogonal Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.6
Tensors as Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.7
Schwartz Inequality, Angles and Similarity of Images . . . . . . . . . . .
53

X
Contents
4
Continuous Functions and Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.1
Functions as a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.2
Addition and Scaling in Vector Spaces of Functions. . . . . . . . . . . . .
58
4.3
A Scalar Product for Vector Spaces of Functions . . . . . . . . . . . . . . .
59
4.4
Orthogonality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.5
Schwartz Inequality for Functions, Angles . . . . . . . . . . . . . . . . . . . .
60
5
Finite Extension or Periodic Functions—Fourier Coefﬁcients . . . . . . .
61
5.1
The Finite Extension Functions Versus Periodic Functions . . . . . . .
61
5.2
Fourier Coefﬁcients (FC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
5.3
(Parseval–Plancherel) Conservation of the Scalar Product . . . . . . . .
65
5.4
Hermitian Symmetry of the Fourier Coefﬁcients . . . . . . . . . . . . . . . .
67
6
Fourier Transform—Inﬁnite Extension Functions. . . . . . . . . . . . . . . . . .
69
6.1
The Fourier Transform (FT). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
6.2
Sampled Functions and the Fourier Transform . . . . . . . . . . . . . . . . .
72
6.3
Discrete Fourier Transform (DFT) . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
6.4
Circular Topology of DFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
7
Properties of the Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
7.1
The Dirac Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
7.2
Conservation of the Scalar Product . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
7.3
Convolution, FT, and the δ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
7.4
Convolution with Separable Filters . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
7.5
Poisson Summation Formula, the Comb . . . . . . . . . . . . . . . . . . . . . .
95
7.6
Hermitian Symmetry of the FT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
7.7
Correspondences Between FC, DFT, and FT . . . . . . . . . . . . . . . . . . .
99
8
Reconstruction and Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
8.1
Characteristic and Interpolation Functions in N Dimensions . . . . . 103
8.2
Sampling Band-Preserving Linear Operators. . . . . . . . . . . . . . . . . . . 109
8.3
Sampling Band-Enlarging Operators . . . . . . . . . . . . . . . . . . . . . . . . . 114
9
Scales and Frequency Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
9.1
Spectral Effects of Down- and Up-Sampling . . . . . . . . . . . . . . . . . . . 119
9.2
The Gaussian as Interpolator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
9.3
Optimizing the Gaussian Interpolator . . . . . . . . . . . . . . . . . . . . . . . . . 127
9.4
Extending Gaussians to Higher Dimensions . . . . . . . . . . . . . . . . . . . 130
9.5
Gaussian and Laplacian Pyramids . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
9.6
Discrete Local Spectrum, Gabor Filters . . . . . . . . . . . . . . . . . . . . . . . 136
9.7
Design of Gabor Filters on Nonregular Grids . . . . . . . . . . . . . . . . . . 142
9.8
Face Recognition by Gabor Filters, an Application. . . . . . . . . . . . . . 146

Contents
XI
Part III Vision of Single Direction
10
Direction in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
10.1
Linearly Symmetric Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
10.2
Real and Complex Moments in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
10.3
The Structure Tensor in 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
10.4
The Complex Representation of the Structure Tensor . . . . . . . . . . . . 168
10.5
Linear Symmetry Tensor: Directional Dominance . . . . . . . . . . . . . . 171
10.6
Balanced Direction Tensor: Directional Equilibrium . . . . . . . . . . . . 171
10.7
Decomposing the Complex Structure Tensor . . . . . . . . . . . . . . . . . . . 173
10.8
Decomposing the Real-Valued Structure Tensor . . . . . . . . . . . . . . . . 175
10.9
Conventional Corners and Balanced Directions. . . . . . . . . . . . . . . . . 176
10.10 The Total Least Squares Direction and Tensors . . . . . . . . . . . . . . . . . 177
10.11 Discrete Structure Tensor by Direct Tensor Sampling . . . . . . . . . . . 180
10.12 Application Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor) . . . . . . . . 187
10.14 Relationship of the Two Discrete Structure Tensors . . . . . . . . . . . . . 196
10.15 Hough Transform of Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
10.16 The Structure Tensor and the Hough Transform . . . . . . . . . . . . . . . . 202
10.17 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
11
Direction in Curvilinear Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
11.1
Curvilinear Coordinates by Harmonic Functions . . . . . . . . . . . . . . . 209
11.2
Lie Operators and Coordinate Transformations . . . . . . . . . . . . . . . . . 213
11.3
The Generalized Structure Tensor (GST) . . . . . . . . . . . . . . . . . . . . . . 215
11.4
Discrete Approximation of GST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.5
The Generalized Hough Transform (GHT) . . . . . . . . . . . . . . . . . . . . 224
11.6
Voting in GST and GHT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
11.7
Harmonic Monomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
11.8
“Steerability” of Harmonic Monomials . . . . . . . . . . . . . . . . . . . . . . . 230
11.9
Symmetry Derivatives and Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . 231
11.10 Discrete GST for Harmonic Monomials . . . . . . . . . . . . . . . . . . . . . . . 233
11.11 Examples of GST Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
11.12 Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
11.13 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
12
Direction in ND, Motion as Direction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.1
The Direction of Hyperplanes and the Inertia Tensor . . . . . . . . . . . . 245
12.2
The Direction of Lines and the Structure Tensor . . . . . . . . . . . . . . . . 249
12.3
The Decomposition of the Structure Tensor . . . . . . . . . . . . . . . . . . . . 252
12.4
Basic Concepts of Image Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
12.5
Translating Lines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
12.6
Translating Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
12.7
Discrete Structure Tensor by Tensor Sampling in ND . . . . . . . . . . . 263

XII
Contents
12.8
Afﬁne Motion by the Structure Tensor in 7D . . . . . . . . . . . . . . . . . . . 267
12.9
Motion Estimation by Differentials in Two Frames . . . . . . . . . . . . . 270
12.10 Motion Estimation by Spatial Correlation . . . . . . . . . . . . . . . . . . . . . 272
12.11 Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
12.12 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
13
World Geometry by Direction in N Dimensions . . . . . . . . . . . . . . . . . . . 277
13.1
Camera Coordinates and Intrinsic Parameters . . . . . . . . . . . . . . . . . . 277
13.2
World Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
13.3
Intrinsic and Extrinsic Matrices by Correspondence . . . . . . . . . . . . . 287
13.4
Reconstructing 3D by Stereo, Triangulation . . . . . . . . . . . . . . . . . . . 293
13.5
Searching for Corresponding Points in Stereo . . . . . . . . . . . . . . . . . . 300
13.6
The Fundamental Matrix by Correspondence . . . . . . . . . . . . . . . . . . 305
13.7
Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
13.8
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
Part IV Vision of Multiple Directions
14
Group Direction and N-Folded Symmetry . . . . . . . . . . . . . . . . . . . . . . . . 311
14.1
Group Direction of Repeating Line Patterns . . . . . . . . . . . . . . . . . . . 311
14.2
Test Images by Logarithmic Spirals . . . . . . . . . . . . . . . . . . . . . . . . . . 314
14.3
Group Direction Tensor by Complex Moments . . . . . . . . . . . . . . . . . 315
14.4
Group Direction and the Power Spectrum . . . . . . . . . . . . . . . . . . . . . 318
14.5
Discrete Group Direction Tensor by Tensor Sampling . . . . . . . . . . . 320
14.6
Group Direction Tensors as Texture Features . . . . . . . . . . . . . . . . . . 324
14.7
Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
Part V Grouping, Segmentation, and Region Description
15
Reducing the Dimension of Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
15.1
Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . . . . 329
15.2
PCA for Rare Observations in Large Dimensions . . . . . . . . . . . . . . . 335
15.3
Singular Value Decomposition (SVD) . . . . . . . . . . . . . . . . . . . . . . . . 338
16
Grouping and Unsupervised Region Segregation . . . . . . . . . . . . . . . . . . . 341
16.1
The Uncertainty Principle and Segmentation . . . . . . . . . . . . . . . . . . . 341
16.2
Pyramid Building . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
16.3
Clustering Image Features—Perceptual Grouping . . . . . . . . . . . . . . 345
16.4
Fuzzy C-Means Clustering Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 347
16.5
Establishing the Spatial Continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
16.6
Boundary Reﬁnement by Oriented Butterﬂy Filters . . . . . . . . . . . . . 351
16.7
Texture Grouping and Boundary Estimation Integration . . . . . . . . . 354
16.8
Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356

Contents
XIII
17
Region and Boundary Descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
17.1
Morphological Filtering of Regions . . . . . . . . . . . . . . . . . . . . . . . . . . 359
17.2
Connected Component Labelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
17.3
Elementary Shape Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
17.4
Moment-Based Description of Shape . . . . . . . . . . . . . . . . . . . . . . . . . 368
17.5
Fourier Descriptors and Shape of a Region . . . . . . . . . . . . . . . . . . . . 371
18
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391

Abbreviations and Symbols
ℸ(f)
(Dxf + iDyf)2
inﬁnitesimal linear symmetry tensor (ILST1)
δ(x)
Dirac delta distribution, if x is continuous
δ(m)
Kronecker delta function, if m is an integer
CN
N-dimensional complex vector space
BCC
brightness constancy constraint
EN
real vectors of dimension N; Euclidean space
∇f
(Dxf, Dyf, · · · )T
gradient operator1
-
(Dx + iDy)nf
symmetry derivative operator of order n
CT
coordinate transformation
DFD
displaced frame difference
DFT
discrete Fourier transform
FC
Fourier coefﬁcients
FD
Fourier descriptors
FE
ﬁnite extension functions
FF
ﬁnite frequency functions; band–limited functions
FIR
ﬁnite impulse response
FT
F
Fourier transform
GHT
generalized Hough transform
GST
S, or Z
generalized structure tensor
HFP
{ξ, η}
harmonic function pair
ILST
see ℸ(f)
KLT
KLT Karhunen–Lo`eve transform, see PCA
LGN
lateral geniculate nucleus
MS
mean squares
OCR
optical character recognition
ON
orthonormal
PCA
principal component analysis, see KLT
SC
superior colliculus
ST
S, or Z
structure tensor
SNR
signal-to-noise ratio
SVD
singular value decomposition
TLS
total least squares
V1
primary visual cortex, or striate cortex
WGSS
within-group sum of squared error
1 The symbols ℸand ∇are pronounced as “doleth” and “nabla”, respectively.

Part I
Human and Computer Vision
Enlighten the eyes of my mind
that I may understand my place
in Thine eternal design!
St. Ephrem (A.D. 303–373)

1
Neuronal Pathways of Vision
Humans and numerous animal species rely on their visual systems to plan or to
take actions in the world. Light photons reﬂected from objects form images that
are sensed and translated to multidimensional signals. These travel along the visual
pathways forward and backward, in parallel and serially, thanks to a fascinating chain
of chemical and electrical processes in the brain, in particular to, from, and within
the visual cortex. The visual signals do not just pass from one neuron or compart-
ment to the next, but they also undergo an incredible amount of signal processing
to ﬁnally support among others, planning, and decision–action mechanisms. So im-
portant is the visual sensory system, in humans, approximately 50% of the cerebral
cortex takes part in this intricate metamorphosis of the visual signals. Here we will
present the pathways of these signals along with a summary of the functional prop-
erties of the cells encountered on these. Although they are supported by the research
of reknowned scientists that include Nobel laurates, e.g., Santiago Ramon y Cajal
(1906), and David Hubel and Thorsten Wiesel (1983), much of the current neuro-
biological conclusions on human vision, including what follows, are extrapolations
based on lesions in human brains due to damages or surgical therapy, psychological
experiments, and experimental studies on animals, chieﬂy macaque monkeys, and
cats.
1.1 Optics and Visual Fields of the Eye
The eye is the outpost of the visual anatomy where the light is sensed and the 3D
spatio–temporal signal, which is called image, is formed. The “spatial” part of the
name refers to the 2D part of the signal that, at a “frozen” time instant, falls as a
picture on light-sensitive retinal cells, photoreceptors. This picture is a spatial signal
because its coordinates are in length units, e.g., millimeters, representing the distance
between the sensing cells. As time passes, however, the amount of light that falls on
a point in the picture may change for a variety of reasons, e.g., the eye moves, the
object in sight moves, or simply the light changes. Consequently the sensed amount
of photons at every point of the picture results in a 3D signal.

4
1 Neuronal Pathways of Vision
Primary  visual  cortex (V1)
Cornea
Optic radiations
Optic nerve
Optic tract
Lens
Nasal retina
Temporal retina
Lateral geniculate nucleus
LGN
Optic chiasm
Pupil
Nasal view
Nasal view
Temporal view
Temporal view
Nasal retina
V2
Fig. 1.1. The anatomic pathways of the visual signals
The spatial 2D image formed on the retina represents the light pattern reﬂected
from a thin1 plane in the 3D spatial world which the eye observes. This is so, thanks
to the deformable lens sitting behind the cornea, a transparent layer of cells that ﬁrst
receives the light. The thickness of the cornea does not change and can be likened
to a lens with ﬁxed focal length in a human-made optical system, such as a camera.
Because the lens in the eye can be contracted or decontracted by the muscles to which
it is attached, its focal length is variable. Its function can be likened to the zooming
of a telephoto objective. Just as the latter can change the distance of the plane to be
imaged, so can the eye focus on objects at varying distances. Functionally, even the
cornea is thus a lens, in the vocabulary of technically minded. Approximately 75%
of the refraction that the cornea and the eye together do is achieved by the cornea
(Fig. 1.1). The pupil, which can change the amount of light passing into the eye, can
be likened to a diaphram in a camera objective.
The light traverses the liquid ﬁlling the eye before it reaches the retinal surface
attached to the inner wall of the eyeball. The light rays are absorbed, but the sen-
sitivity to light amount, that is the light intensity,2 of the retinal cells is adapted in
various ways to the intensity of the light they usually receive so as to remain opera-
tional despite an overall decrease or increase of the light intensity, e.g., on a cloudy
or a sunny day. A ubiquous tool in this adaptation is the pupil, which can contract
or decontract, regulating the amount of light reaching the retina. There is also the
1 The thickness of the imaged 3D plane can be appreciated as thin in comparison with its
distance to the eye.
2 The light consists of photons, each having its own wavelength. The number of photons
determines the light intensity. Normally, light contains different amounts of photons from
each wavelength for chromatic light. If however there is only a narrow range of wavelengths
among its photons, the light is called monochromatic, e.g., laser light.

1.2 Photoreceptors of the Retina
5
night vision mechanism in which the light intensity demanding retinal cells (to be
discussed soon) are shut off in favor of others that can function at lower amounts of
light. Although two-dimensional, the retinal surface is not a ﬂat plane; rather, it is a
spherical surface. This is a difference in comparison to a human-made camera box,
where the sensing surface is usually a ﬂat plane. One can argue that the biological
image formed on the retina will in the average be better focused since the surfaces of
the natural objects the eye observes are mostly bent, like the trunks of trees, although
this may not be the main advantage. Presumably, the great advantage is that an eye
can be compactly rotated in a spherical socket, leaving only a small surface outside
of the socket. Protecting rotation-enabled rectangular cameras compactly is not an
easy mechanical feat.
1.2 Photoreceptors of the Retina
In psychophysical studies, it is customary that the closeness of a retinal point to the
center O′ is measured in degrees from the optical axis; this is called the eccentric-
ity (Fig. 1.2). Eccentricity is also known as the elevation. The eccentricity angle is
represented by ϵ in the shown graphs and every degree of eccentricity corresponds
to ≈0.35 mm in human eyes. The locus of the retinal points having the same ec-
centricity is a circle. Then there is the azimuth, which is the polar angle of a retinal
point, i.e., the angle relative the positive part of the horizon. This is shown as α in
the ﬁgure on the right, where the azimuth radii and the eccentricity circles are given
in dotted black and pink, respectively. Because the diameter O′O is a constant, the
two angles ϵ, α can then function as retinal coordinates. Separated by the vertical
meridian, which corresponds to α = ± π
2 , the left eye retina can roughly be divided
into two halves, the nasal retina, which is the one farthest away from the nose, and
the temporal retina, which is the one closest to the nose. The names are given after
their respective views. The nasal retina “sees” the nasal hemiﬁeld, which is the view
closest to the nose, and the temporal retina sees the temporal hemiﬁeld, which is the
view on the side farthest away from the nose. The analogous names exist for the right
eye.
In computer vision, the closest kinn of a photoreceptor is a pixel, a picture ele-
ment, because the geometry of the retina is not continuous as it is in a photographic
ﬁlm, but discrete. Furthermore, the grid of photoreceptors sampling the retinal sur-
face is not equidistant. Close to the optic axis of the eye, which is at 0◦eccentricity,
the retinal surface is sampled at the highest density. In macula lutea, the retinal region
inside the eccentricity of approximately 5◦on the retina, the highest concentration of
photoreceptors are found. The view corresponding to this area is also called central
vision or macular vision. The area corresponding to 1◦eccentricity is the fovea.
The photoreceptors come in two “ﬂavors”, the color-sensitive cones and light
intensity-sensitive rods. The cones are shut off in night vision because the intensity
at which they can operate exceeds those levels that are available at night. By contrast,
the rods can operate in the poorer light conditions of the night, albeit with little or no
sensitivity for color differences. In the fovea there are cones but no rods. This is one

6
1 Neuronal Pathways of Vision
Eccentricity
ε
Optic axis
P
P'
O'
O
O''
o
o
45
o
-45
0o
90
135
o
P'
ε
o
45
30
o 10
o
α
O'
Fig. 1.2. Given the diameter O′O, the eccentricty ϵ (left), and the azimuth α, one can deter-
mine the position of a point P ′ on the retina (right)
of the reasons why the spatial resolution, also called acuity, which determines the
picture quality for details that can be represented, is not very high in night vision. The
peak resolution is reserved for day vision, during which there is more light available
to those photoreceptors that can sense such data. The density of cones decreases with
high eccentricity, whereas that of rods increases rapidly. Accordingly, in many night-
active species, the decrease in rod concentration towards the fovea is not as dramatic
as day-active animals, e.g. in owl monkey [171]. In fovea there are approximately
150,000 cones per mm2 [176]. The concentration decreases sharply with increased
eccentricity. To switch to night vision requires time, which is called adaptation, and
takes a few minutes in humans. In human retinae there are three types of cones,
sensitive to long, medium, and short wavelengths of the received photons. These are
also known as “red”, “green”, and “blue” cones. We will come back to the discussion
of color sensitivity of cones in Chap. 2.
The retina consists of six layers, of which the photoreceptor layer containing
cones and rods is the ﬁrst, counted from the eye wall towards the lens. This is an-
other remarkable difference between natural and human-made imaging systems. In
a camera, the light-sensitive surface is turned towards the lens to be exposed to the
light directly, whereas the light-sensitive rods and cones of the retina are turned away
from the lens, towards the wall of the eye. The light rays pass ﬁrst the other ﬁve lay-
ers of the retina before they excite the photoreceptors! This is presumably because
the photoreceptors bleach under the light stimuli, but they can quickly regain their
light-sensitive operational state by intaking organic and chemical substances. By be-
ing turned towards the eye walls, their supply of such materials is facilitated while
their direct exposure to the light is reduced (Fig. 1.3). The light stimulus is translated
to electrical pulses by a photoreceptor, rod, or cone, thanks to an impressive chain of
electrochemical process that involves hyperpolarization [109]. The signal intensity
of the photoreceptors increases with increased light intensity, provided that the light
is within the operational range of the photoreceptor in terms of its photon amount
(intensity) as well as photon wavelength range (color).

1.3 Ganglion Cells of the Retina and Receptive Fields
7
1.3 Ganglion Cells of the Retina and Receptive Fields
The ganglion cells constitute the last layer of neurons in the retina. In between the
ganglion cells and photoreceptor layer, there are four other layers of neuronal cir-
cuitry that implement electro-chemical signal processing. The processing includes
photon ampliﬁcation and local neighborhood operation implementations. The net re-
sult is that ganglion cells outputs do not represent the intensity of light falling upon
photoreceptors, but they represent a signal that can be comparable to a bandpass-
ﬁltered version of the image captured by all photoreceptors. To be precise, the output
signal of a ganglion cell responds vigorously during the entire duration of the stimu-
lus only if the light distribution on and around its closest photoreceptor corresponds
to a certain light intensity pattern.
There are several types of ganglion cells, each having its own activation pattern.
Ganglion cells are center–surround cells, so called because they respond only if there
is a difference between the light intensity falling on the corresponding central and the
surround photoreceptors [143]. An example pattern called (+/−) is shown in Fig.
1.3, where the central light intensity must exceed that in the annulus around it. The
opposite ganglion cell type is (−/+), for the surround intensity must be larger than
the central intensity. The opposing patterns exist presumably because the neuronal
operations cannot implement differences that become negative.
There are ganglion cells that take inputs from different cone types in a speciﬁc
fashion that make them color sensitive. They include (r+g−)-type, reacting when the
intensities coming from the central L-cones are larger than the intensities provided
by the M-cones in the surround, and its opposite type (r−g+), reacting when the
intensities coming from the central L-cones are smaller than the intensities provided
by the M-cones in the surround. There are approximately 125 million rods and cones,
which should be contrasted to about 1 million ganglion cells, in each eye. After a
bandpass ﬁltering the sampling rate of a signal can be decreased (Sect. 6.2), which
in turn offers a signal theoretic justiﬁcation for the decrease of the sampling rate
at the ganglion cell layer. This local comparison scheme plays a signiﬁcant role in
color constancy perception, which allows humans to attach the same color label of
a certain surface seen under different light sources, e.g., daylight or indoor light.
Likewise, this helps humans to be contrast-sensitive rather than gray-sensitive at ﬁrst
place, e.g., we are able to recognize the same object in different black and white
photographs despite the fact that the object surface does not have the same grayness.
The output of a ganglion cell represents the result of computations on many pho-
toreceptor cells, which can be activated by a part of the visual ﬁeld. To be precise,
only a pattern within a speciﬁc region in the visual ﬁeld is projected to a circular
region on the retina, which in turn steers the output of a ganglion cell. This retinal
region is called the receptive ﬁeld of a ganglion cell. The same terminology is used
for other neurons in the brain as well, if the output of a neuron is steered by a local
region of the retina. The closest concept in computer vision is the local image or the
neighborhood on which certain computations are applied in parallel. Consequently,
the information on absolute values of light intensity, available at the rod and cone
level, never leaves the eye, i.e., gray or color intensity information is not available

8
1 Neuronal Pathways of Vision
Rod
Cone
Horizontal cell
Bipolar cell
Amacrine cell
Ganglion cell
Light direction
Optic nerve
Stimulus
Time
Fig. 1.3. The graph on left illustrates the retinal cells involved in imaging and visual signal
processing. On the right the response pattern of a (+/−)-type ganglion cell is shown
to the brain. All further processing in the brain takes place on “differential signals”,
representing local comparisons within and between the photoreceptor responses, not
on the intensity signals themselves.
The outputs of the ganglion cells converge to eventually form the optic nerve
that goes away from the eye. Because the ganglion layer is deep inside the eye and
farthest away from the eye wall, the outputs come out of the eye through a “hole”
in the retina that is well outside of the fovea. There are no photoreceptors there.
The visual ﬁeld region that projects on this hole is commonly known as the blind
spot. The hole itself is called the optic disc and is about 2 mm in diameter. Humans
actually do not see anyting at the blind spot, which is in the temporal hemiﬁeld, at
approximately 20◦elevation close to the horizontal meridian.
Exercise 1.1. Close your left eye, and with your right eye look at a spot far away,
preferably at a bright spot on a dark background. Hold your ﬁnger between the
spot and the eye with your arm stretched. Move your ﬁnger out slowly in a half
circle without changing your gaze ﬁxation on the spot. Do you experience that your
ﬁnger disappears and reappears? If so, explain why, and note at approximately what
elevation angle this happens. If not, retry when you are relaxed, because chances are
high that you will experience this phenomenon.
The ganglion cells are the only output cells of the eye reaching the rest of the
brain. There is a sizable number of retinal ganglion cell types [164], presumably to

1.4 The Optic Chiasm
9
equip the brain with a rich set of signal processing tools, for, among others, color,
texture, motion, depth, and shape analysis, when the rest of the brain has no access to
the original signal. The exact qualities that establish each type and the role of these
are still debated. The most commonly discussed types are the small midget cells, and
the large parasol cells. There is a less-studied third type, frequently referred to when
discussing the lateral geniculate nucleus connections, the koniocelullar cells.
The midget cells are presumed to process high spatial frequency and color. They
have, accordingly, small receptive ﬁelds and total about 80% of all retinal ganglion
cells. The large majority of midget cells are color-opponent, being excited by red
in the center and inhibited by green in the surround, or vice versa. Parasol cells,
on the other hand, are mainly responsible for motion analysis. Being color indif-
ferent, they total about 10% of ganglion cells, and have larger receptive ﬁelds than
the midget cells. There are few parasol cells in the fovea. The ratio of parasol to
midget cells increases with eccentricity. Parasol cells are insensitive to colour, i.e.,
they are luminance-opponent. This is a general tendency; the receptive ﬁelds of gan-
glion cells increase with eccentricity. This means that bandpass ﬁltering is achieved
at the level of retina. Accordingly, the number of ganglion cells decreases with ec-
centricity. Since ganglion cells are the only providers of signals to the brain, the
cerebral visual areas also follow such a spatial organization.
The koniocelullar cells are much fewer and more poorly understood than midget
and parasol cells. They are not as heterogenous as these either, although a few com-
mon properties have been identiﬁed. Their receptive ﬁelds lack surround and they are
color sensitive! In the center, they are excited by blue, whereas they are inhibited (in
the center) by red or green [104]. Presumably, they are involved in object/background
segregation.
1.4 The Optic Chiasm
The optic nerve is logically organized in two bundles of nerves, carrying visual sig-
nals responsible for the nasal and temporal views, respectively. The two optic nerves
coming from both eyes meet at the optic chiasm, where one bundle of each sort trav-
els farther towards the left and the right brain halves. The temporal retina bundle
crosses the midline, whereas the nasal retina bundle remains on the same side for
both eyes. The bundle pair leaving the chiasm is called the optic tract. Because of
the midline crossing arrangement of only the temporal retina outputs, the optical tract
that leaves the chiasm to travel to the left brain contains only visual signal carriers
that encode the patterns appearing on the right hemiﬁeld. Similarly, the one reach-
ing the right brain carries visual signals of the left hemiﬁeld. The optic tract travels
chieﬂy to reach the lateral geniculate nucleus, LGN to be discussed below. However,
some 10% of the connections in the bundle feed an area called superior colliculus,3
(SC). From the SC there are outputs feeding the primary visual cortex at the back of
the brain, which we will discuss further below. By contrast, SC will not be discussed
3 This area is involved in visual signal processing controlling the eye movements.

10
1 Neuronal Pathways of Vision
further here; see [41,223]. We do this to limit the scope but also because this path to
the visual cortex is much less studied than the one passing through the LGN.
1.5 Lateral Geniculate Nucleus (LGN)
The lateral geniculate4 nucleus (LGN) is a laminated structure in the thalamus. Its
inputs are received from the ganglion cells coming from each eye (Fig. 1.4). The
input to the layers of LGN is organized in an orderly fashion, but the different eyes
remain segregated. That is there are no LGN cells that react to both eyes, and each
layer contains cells that respond to stimuli from a single eye. The left eye (L) and the
right (R) eye inputs interlace when passing from one layer to the next, as the ﬁgure
illustrates. Being R,L,L,R,L,R for the left LGN, the left–right alternation reverses
between layers 2 and 3 for reasons that are not well understood. Layer 1 starts with
the inputs coming from the eye on the other side of the LGN, the so called contralat-
eral5 eye, so that for the right eye the sequence is L,R,R,L,R,L. Each LGN receives
signals representing a visual ﬁeld corresponding to the side opposite their own, that
is a contralateral view. Accordingly, the left and right LGNs cope only with, the
right and left visual ﬁelds, respectively.
Like nearly all of the neural visual signal processing structures, LGN also has a
topographic organization. This implies a continuity (in the mathematical sense) of
the mapping between the retina and the LGN, i.e., the responses of ganglion cells
that are close to each other feed into LGN cells that are located close to each other.6
The small ganglion cells (midget cells) project to the cells found in the parvocel-
lular layers of LGN. In Fig. 1.4 the parvocellular cells occupy the layers 3–6. The
larger cells (parasol cells) project onto the magnocellular layers of the LGN, layers
1–2 of the ﬁgure. The koniocellular outputs project onto the layers K1–K6. The ko-
niocellular cells, which are a type of cells found among the retinal ganglion cells,
have also been found scattered in the entire LGN. Besides the bottom–up feeding
from ganglion cells, the LGN receives signiﬁcant direct and indirect feedback from
the V1 area, to be discussed in Sect. 1.6. The feedback signals can radically inﬂuence
the visual signal processing in LGN as well as in the rest of the brain. Yet the func-
tional details of these connections are not well understood. Experiments on LGN
cells have shown that they are functionally similar to those of the retinal ganglion
cells that feed into them. Accordingly, the LGN is frequently qualiﬁed as a relay
station between the retina and visual cortex, and its cells are also called relay cells.
The outputs from LGN cells form a wide band called optic radiations and travel to
the primary visual cortex (Fig. 1.1).
4 Geniculate means kneelike, describing its appearance.
5 The terms contralateral and ipsilateral are frequently used in neurobiology. They mean,
respectively, the “other” and the “same” in relation to the current side.
6 Retrospectively, even the ganglion cells are topographically organized in the retina because
these are placed “behind” the photoreceptors from which they receive their inputs.

1.6 The Primary Visual Cortex
11
1
2
3
4
5
6
K3
K2
K6K5K4
K1
Parvocellular layers
Magnocellular layers
Ganglion (parasol) cells
Ganglion (midget) cells
R
L
R
L
L
R
L: Left eye
R: Right eye
K1-6:
Interlaminar
zones
Parvocellular-Left
Parvocellular-Right
Magnocelular-Left
Magnocelular-Right
Konicelular-Left
Konicelular-Right
1
2
3
4A
4B
5
6
Right
Left
Right
LGN
To superior colliculus
To  V2,  MT
To  V2
 Primary visual cortex  (V1)
Fig. 1.4. The left graph illustrates the left LGN of the macaque monkey with its six layers.
The right graph shows the left V1 and some of its connections, following Hassler labelling of
the layers [47,109].
1.6 The Primary Visual Cortex
Outputs from each of the three LGN neuron types feed via optic radiations into dif-
ferent layers of the primary visual cortex, also known as V1, or striate cortex. The V1
area has six layers totalling ≈2 mm on a few cm2. It contains the impressive ≈200
million cells. To compare its enormous packing density, we recall that the ganglion
cells total ≈1 million in an eye. The V1 area is by far the most complex area of the
brain, as regards layering of the cells and the richness of cell types.
A schematic illustration of its input–output connections is shown in Fig. 1.4 us-
ing Hassler notation [47]. Most of the outputs from magnocellular and parvocellular
layers of the LGN arrive at layer 4, but to different sublayers, 4A and 4B, respec-
tively. The cells in layer 4A and 4B have primarily receptive ﬁeld properties that are
similar to magnocellular and parvocellular neurons, which feed into the former. The
receptive ﬁeld properties of other cells will be discussed in Sect. 1.7. The koniocellu-
lar cell outputs feed narrow volumes of cells spanning layers 1–3, called blobs [155].
The blobs contain cells having the so-called double-opponent color property. These
are embedded in a center–surround receptive ﬁeld that is presumably responsible
for color perception, which operates fairly autonomously in relation to V1. We will
present this property in further detail in Sect. 2.3. Within V1, cells in layer 4 provide
inputs to layers 2 and 3, whereas cells in layers 2 and 3 project to layers 5 and 6.
Layers 2 and 3 also provide inputs to adjacent cortical areas. Cells in layer 5 pro-
vide inputs to adjacent cortical areas as well as nonadjacent areas, e.g., the superior
colliculus. Cells in layer 6 provide feedback to the LGN.
As to be expected from the compelling evidence coming from photoreceptor,
ganglion, and LGN cell topographic organizations, the visual system devotes the
largest amount of cortical cells to fovea even cortically. This is brilliant in the face

12
1 Neuronal Pathways of Vision
5
10
30
o
o
o
-45
o
45
o
45
10
45
30
o
o
o
5
o
L
eft
visual c
ort
e
x
o
0
o
Fig. 1.5. On the left, a model of the retinal topography is depicted. On the right, using the
same color code, a model of the topography of V1, on which the retinal cells are mapped, is
shown. Adapted after [217]
of the limited resources that the system has at its disposal, because there is a limited
amount of energy available to drive a limited number of cells that have to ﬁt a small
physical space. Because the visual ﬁeld, and hence the central vision, can be changed
mechanically and effectively, the resource-demanding analysis of images is mainly
performed in the fovea. For example, when reading these lines, the regions of interest
are shufﬂed in and out of the fovea through eye motions and, when necessary, by a
seamless combination of eye–head–body motions.
Half the ganglion cells in both eyes, are mapped to the V1 region. Geometrically,
the ganglion cells are on a quarter sphere, whereas V1 is more like the surface of a
pear [217], as illustrated by Fig. 1.5. This is essentially equivalent to a mathematical
deformation, modeled as a coordinate mapping. An approximation of this mapping is
discussed in Chap. 9. The net effect of this mapping is that more of the total available
resources (the cells) are devoted to the region of the central retina than the size of
the latter should command. The over-representation of the central retina is known
as cortical magniﬁcation. Furthermore, isoeccentricity half circles and isoazimuth
half-lines of the retina are mapped to half-lines that are approximately orthogonal.
Cortical magniﬁcation has also inspired computer vision studies to use log–polar
spatial-grids [196] to track and/or to recognize objects by robots with artiﬁcial vision
systems [20,187,205,216]. The log–polar mapping is justiﬁed because it effectively
models the mapping between the retina and V1, where circles and radial half-lines

1.7 Spatial Direction, Velocity, and Frequency Preference
13
On
Time
Off
Off
On
Off
Off
Time
Fig. 1.6. On the left, the direction sensitivity of a cell in V1 is illustrated. On the right, the
sensitivity of simple cells to position, which comes on top of their spatial direction sensitivity,
is shown
are mapped to orthogonal lines in addition to the fact that the central retina is mapped
to a relatively large area in V1.
1.7 Spatial Direction, Velocity, and Frequency Preference
Neurons in V1 have radically different receptive ﬁeld properties compared to the
center–surround response pattern of the LGN and the ganglion cells of the retina.
Apart from the input layer 4 and the blobs, the V1 neurons respond vigorously only
to edges or bars at a particular spatial direction, [114], as illustrated by Fig. 1.6. Each
cell has its own spatial direction that it prefers, and there are cells for (approximately)
each spatial direction. The receptive ﬁeld patterns that excite the V1 cells consist in
lines and edges as has been illustrated in Fig. 1.8. Area V1 contains two types of
direction-sensitive cells, simple cells and complex cells. These cells are insensitive
to the color of light falling in their receptive ﬁelds.
Simple cells respond to bars or edges having a speciﬁc direction at a speciﬁc po-
sition in their receptive ﬁelds, Fig. 1.6. If the receptive ﬁeld contains a bar or an edge
that has a different direction than the preferred direction, or the bar is not properly
positioned, the ﬁring rate of a simple cell decreases down to the biological zero ﬁring
rate, spontaneous and sporadic ﬁring. Also, the response is maintained for the entire
duration of the stimulus. The density of simple cells decreases with increased ec-
centricity of the retinal positions they are mapped to. Their receptive ﬁelds increase
in size with increased eccentricity. This behavior is in good agreement with that of

14
1 Neuronal Pathways of Vision
the receptive ﬁeld sizes of ganglion cells in the retina. Likewise, the density changes
of the simple cells reﬂect corresponding changes in ganglion cell density that occur
with increased eccentricity. The smallest receptive ﬁelds of simple cells, which map
to fovea, are approximately 0.25◦×0.25◦, measured in eccentricity and azimuth an-
gles. This is the same as those of ganglion cells, on which they topographically map.
The farthest retinal periphery commands the largest receptive ﬁeld sizes of ≈1◦×1◦
for simple cells. Furthermore, the simple cell responses appear to be linear, e.g. [6].
That is, if the stimulus is sinusoidal so is the output (albeit with different amplitude
and phase, but with the same spatial frequency). This is a further evidence that at
least a sampled local spectrum for all visual ﬁelds is routinely available for the brain
when it analyzes images. In Sect. 9.6, we will study the signal processing that is
afforded by local spectra in further detail.
Complex cells, which total about 75% of the cells in V1, respond to a critically
oriented bar, moving anywhere within their receptive ﬁelds (Fig. 1.7). They share
with simple cells the property of being sensitive to the spatial directions of lines, but
unlike them, stationary bars placed anywhere in their receptive ﬁelds will generate
vigorous responses. In simple cells, excitation is conditioned to the bar or edge with
the critical direction be precisely placed in the center of the receptive ﬁeld of the cell.
Complex cells have a tendency to have larger receptive ﬁelds than the comparable
simple cells, 0.5◦×0.5◦in the fovea. The bar widths that excite the complex cells,
however, are as thin as those of simple cells, ≈0.03◦. Some complex cells (as well
as some simple cells) have a sensitivity to the motion-direction of the bar, in addition
to the spatial direction of it. Also, the complex cell responses are nonlinear [6].
In neurobiology the term orientation is frequently used to mean what we here
called the spatial direction, whereas the term direction in these studies usually rep-
resents the motion-direction of a moving bar in a plane. Our use of the same term
for both is justiﬁed because, as will be detailed in Chap. 12, these concepts are tech-
nically the same. Spatial direction is a direction in 2D space, whereas velocity (di-
rection + absolute speed information) is a direction in the 3D spatio–temporal signal
space (see Fig. 12.2). Accordingly, the part of the human vision system that deter-
mines the spatial direction and the one that estimates the velocity mathematically
solve the same problem but in different dimensions, i.e., in 2D, and 3D, respectively.
The cells that are motion-direction sensitive in V1 are of lowpass type, i.e., they
respond as long as the amplitude of the motion (the speed) is low [174]. This is
in contrast to some motion-direction sensitive cells found in area V2, which are of
bandpass-type w.r.t. the speed of the bar, i.e., they respond as long as the bar speed
is within a narrow range. There is considerable specialization in the way the the cor-
tical cells are sensitive to motion parameters. Those serving the fovea appear to be
of lowpass character, hence they are maximally active during the eye ﬁxation, in all
visual areas of the cortex, although those in V2 have a clear superiority for coding
both the absolute speed and the motion-direction. Those cells serving peripherial
vision appear to have large receptive ﬁelds and are of high-pass type, i.e., they are
active when the moving bar is faster than a certain speed. Area V1 motion-direction
cells are presumably engaged in still image analysis (or smooth pursuit of objects
in motion), whereas those beyond V1, especially V2, are engaged in analysis and

1.7 Spatial Direction, Velocity, and Frequency Preference
15
On
Off
Off
Time
Time
Time
Fig. 1.7. The graph on the left illustrates that the complex cells of V1 are insensitive to bar
position. On the right, top and right, bottom the responses of motion-direction sensitive and a
motion-direction insensitive complex cell responses are shown
tracking of moving objects. Except for those which are of high-pass type, the opti-
mal velocity of velocity-tuned cells increases with visual eccentricity and appears to
range from 2◦to 90◦per second. To limit the scope of this book and also because
they are less studied, we will not discuss cells beyond area V1 further, and refer to
further readings, e.g., [173].
Complex cells are encountered later in the computational processing chain of
visual signals than are simple cells. Accordingly, to construct their outputs, the com-
plex cells presumably receive the outputs of many simple cells as inputs. As in the
case of simple cells, the exact architecture of input–output wiring of complex cells
has not been established experimentally, but there exist suggested schemes that are
being debated.
There is repeatedly convincing evidence, e.g., [4,6,45,46,159,165], suggesting
the existence of well-organized cells in V1 that exhibit a spatial frequency selec-
tivity to moving and/or still sinusoidal gratings, e.g., the top left of Fig. 10.2. The
cells serving fovea in V1, have optima in the range of 0.25–4 cycles/degree and have
bandwiths of approximately 1.5 octaves [165]. Although these limits vary somewhat
between the different studies that have reported on frequency selectivity, even their
very existence is important. It supports the view that the brain analyzes the visual
stimuli by exploding the original data via frequency, spatial direction, and spatio
temporal direction (velocity) channels in parallel before it actually reduces and sim-
pliﬁes them, e.g., to yield a recognition of an object or to generate motor responses
such as those of catching a fast ball.

16
1 Neuronal Pathways of Vision
Taken together, the central vision is well equipped to analyze sharp details be-
cause its cells in the cortex have receptive ﬁelds that are capable to quantify high
spatial frequencies isotropically, i.e., in all directions. This capability is gradually
replaced with spatial low-frequency sensitivity at peripherial vision where the cell
receptive ﬁelds are larger. In a parallel fashion, in the central vision we have cells
that are more suited to analyze slow moving patterns, whereas in the peripherial vi-
sion the fast moving patterns can be analyzed most efﬁciently. Combined, the central
vision has most of its resources to analyze high spatial frequencies moving slowly,
whereas the peripheral vision devotes its resources to analyze low spatial frequen-
cies moving fast. This is because any static image pattern is equivalent to sinusoidal
gratings, from a mathematical viewpoint, since it can be synthesized by means of
these.7
The spatial directional selectivity mechanism is a result of interaction of cells in
the visual pathway, presumably as a combination of the LGN outputs which, from
the signal processing point of view, are equivalent to time-delayed outputs of the
retinal ganglion cells. The exact mechanism of this wiring is still not well understood,
although the scheme suggested by Hubel and Wiesel, see [113], is a simple scheme
that can explain the simple cell recordings. It consists in an additive combination of
the LGN outputs that have overlapping receptive ﬁelds. In Fig. 1.8, this is illustrated
for a bar-type simple cell, which is synthesized by pooling outputs of LGN cells
having receptive ﬁelds along a line.
A detailed organization of the cells is not yet available, but it is fairly conclusive
that depthwise, i.e., a penetration perpendicular to the visual cortex, the cells are or-
ganized to prefer the same spatial direction, the same range of spatial frequencies,
and the same receptive ﬁeld. Such a group of cells is called an orientation column in
the neuroscience of vision. As one moves along the surface of the cortex, there is lo-
cally a very regular change of the spatial direction preference in one direction and oc-
ular dominance (left or right eye) in the other (orthogonal to the ﬁrst). However, this
orthogonality does not hold for long cortical distances. Accordingly, to account for
the spatial direction and ocular dominance changes as one moves along the surface,
a rectangular organization of the orientation columns in alternating stripes of ocular
dominance is not observed along the surface of the cortex. Instead, a structure of
stripes, reminiscent of the ridges and valleys of ﬁngerprints, is observed. Across the
stripes, ocular dominance and along the stripes, spatial direction preference changes
occur [222].
The direction, whether it represents the spatial direction or the motion, is an
important feature for the visual system because it can deﬁne the boundaries of objects
as well as encode texture properties and corners.8 Also, not only patterns of static
images but also motion patterns are important visual attributes of a scene because
object background segregation is tremendously simpliﬁed by motion information by
motion information compared to attempting to resolve this in static images. Likewise,
7 We will introduce this idea in further detail in Chap. 5.
8 The concept of direction can be used to represent texture and corners in addition to edges.
In Chaps. 10 and 14 a detailed account of this is given

1.8 Face Recognition in Humans
17
Fig. 1.8. The patterns that excite the cells of V1 are shown on the left. On the right, a plausible
additive wiring of LGN cell responses to obtain a directional sensitivity of a simple cell is
shown
the spatial frequency information is important because, on one hand, it encodes the
sizes of objects, while on the other hand, it encodes the granularity or the scale of
repetitive patterns (textures).
1.8 Face Recognition in Humans
Patients suffering from prosopagnosia have a particular difﬁculty in recognizing face
identities. They can recognize the identities of familiar persons, if they have access to
other modalities, e.g., voice, walking pattern, length, or hairstyle. Without nonfacial
cues, the sufferers may not even recognize family members, or even their own faces
may be foreign to them. They often have good ability to recognize other objects that
are nonfaces. In many cases, they become prosopagnosic after a stroke or a surgical
intervention.
There is signiﬁcant evidence, both from studies of prosopagnosia and from stud-
ies of brain damage, that face analysis engages special signal processing in visual
cortex that is different from processing of other objects [13, 68, 79]. There is a gen-
eral agreement that approximately at the age of 12, the performance of children in
face recognition reaches adult levels, that there is already an impressive face recog-
nition ability by the age of 5, and that measurable preferences for face stimuli exist

18
1 Neuronal Pathways of Vision
Fig. 1.9. Distribution of correct answers. Example reading: 11% of females and 5% of males
had 7 correct answers out of 8 they provided
in babies even younger than 10 minutes [66]. For example, human infants a few
minutes of age show a preference to track a human face farther than other moving
nonface objects [130]. While there is a reasonable rotation-invariance to recognize
objects though it takes longer times, turning a face upside down results usually in a
dramatic reduction of face identiﬁcation [40]. These and other ﬁndings indicate that
face recognition develops earlier than other object recognition skills, and that it is
much more direction sensitive than recognition of other objects.
Perhaps recognition of face identities is so complex that encoding the diversity of
faces demands much more from our general-purpose, local direction, and frequency-
based feature extraction system. If so, that would explain our extreme directional
sensitivity in face recognition. One could even speculate further that the problem is
not even possible to solve in real time with our general object recognition system,
that it has an additional area that is either specialized on faces or helps to speed-up
and/or to robustify face recognition performance. This is more than an experiment
of thought because there is mounting evidence that faces [13,99,181,232], just like
color (Chap. 2), disposes its own “brain center”. Face sensitive cells have been found
in several parts of the visual cortex of monkeys, although they are found in most
signiﬁcant numbers in a subdivision of inferotemporal cortex in the vicinity of the
superior temporal sulcus. Whether these cells are actually necessary and sufﬁcient to
establish the identity of a face, or if they are only needed for gaze-invariant general

1.9 Further Reading
19
human face recognition (without person identity) is not known to sufﬁcient accuracy.
In humans, by using magneto resonance studies, the face identity establishing system
engages a brain region called fusiform gyrus. However, it may not exclusively be
devoted to face identiﬁcation, as other sub-categorization of object tasks activate this
region too.
As pointed out above, humans are experts in face recognition, at an astonishing
maturity level, even in infancy. Our expertise is so far-reaching that we remember
hundreds of faces, often many years later, without intermediate contact. This is to
be contrasted to the difﬁculty in remembering their names many years later, and to
the hopeless task of remembering their telephone numbers. Yet this specialization
appears to have gone far in some respects and less so in others. We have difﬁculty
recognizing faces of another ethnic group versus own group [36, 38, 65, 158]. For
an african-american and a caucasian, it is easier to recognize people of their own
ethnicity as compared to cross-ethnic person identiﬁcation, in spite of the fact that
both groups are exposed to each other’s faces. Besides this cross-ethnic bias, hair
style/line is another distraction when humans decide on face similarities, [24, 39,
201]. Recently [24], another factor that biases recognition has been evidenced (Fig.
1.9). Women had systematically higher correct answer frequencies than men in a
series of face recognition tests (Fig. 1.10), taken by an excess of 4000 subjects. A
possible explanation is that face identiﬁcation skill is more crucial to women than
men in their social functioning.
1.9 Further Reading
Much of the retinal cell connections and cell types were revealed by Santiago Ra-
mon y Cajal [44] at the begining of the 1900s using a cell staining technique known
as Golgi staining. However, the current understanding of human vision is still de-
bated at various levels of details (and liveliness) as new experimental evidence ac-
cumulates. An introductory, yet rich description of this view is offered by [113,121]
whereas [173] offers discussions of the neuronal processing and organization w.r.t.
the experimental support. The study [109] offers a more recent view of mammalian
vision with extrapolations to human vision, whereas [235] presents the current view
on human color vision. The study in [189] provides support for Hubel and Wiesel’s
wiring suggestion to model simple cell responses from LGN responses, whereas that
of [206] offers an alternative view that also plausibly models the various types of
simple cells that have different decreases in sensitivity when stimulated with nonop-
timal directions. The reports in [40, 69], provide a broad overview of the human
facerecognition results. The study of [196] suggested a nonuniform resource allo-
cation to analyze static images in computer vision. In analogy with the cortical cell
responses to moving patterns, one could differentiate resource allocations in motion
image processing too. This can be done by designing ﬁlter banks containing elements
that can analyze high spatial frequencies moving slowly, as well as low spatial fre-
quencies moving fast at the cost of other combinations. A discussion of this is given
in [21].

20
1 Neuronal Pathways of Vision
F
7%
9%
9%
5%
2%
M
5%
7%
11%
6%
4%
A1
A2
A3
A4
A5
F
9%
40%
3%
15%
1%
M
8%
31%
2%
24%
1%
A6
A7
A8
A9
A10
Fig. 1.10. A question used in human face recognition test and the response distribution of the
subjects. The rows F and M represent the female and the male responses, respectively. The
rectangle indicates the alternative that actually matches the stimulus

2
Color
Color does not exist as a label inherent to a surface, but rather it is a result of our
cerebral activity, which constructs it from further processing of the photoreceptor
signals. However perceptional, the total system also relies on a sensing mechanism,
which must follow the strict laws of physics regulating the behavior of light in its
interaction with matter. These laws apply from the moment it is reﬂected from the
surface of an object, until the light photons excite the human photoreceptors after
having passed through the eye’s lens system. The light stimulates the photorecep-
tors, and after some signal processing both in the retina and in other parts of the
brain, the signals result in a code representing the color of the object surface. At the
intersection of physics, biology, psychology, and even philosophy, color has attracted
many brilliant minds of humanity: Newton, Young, Maxwell, and Goethe to name
but a few. Here we discuss the color sensation and generation along with the involved
physics, [166,168], and give a brief account of the signal processing involved in color
pathways, evidenced by studies in physiology and psychology [146,235].
2.1 Lens and Color
The role of the lens is to focus the light coming from a plane (a surface of an ob-
ject) at a ﬁxed distance on the retina which contains light-sensitive sensors. Without
a lens the retina would obtain reﬂected rays coming from different planes, at all
depths, thereby blurring the retinal image. For a given lens curvature, however, the
focal length varies slightly, depending on the wavelength of the light. The longer
wavelengths have longer focal lengths. A light ray having a wavelength interpreted
by humans as red has the longest focal length, whereas bluelight has the shortest
focal length. Humans and numerous other species have dynamically controlled lens
curvatures. If human eyes are exposed to a mixture of light having both red and blue
wavelengths, e.g., in a graph, the eyes are exposed to fatigue due to the frequent lens
shape changes. The lens absorbs light differently as a function of the wavelength.
It absorbs roughly twice as much the blue light as it does the red light. With aging
this absorption discrepancy with wavelengths is even more accentuated. As a result,

22
2 Color
400
450
500
550
600
650
700
0
0.2
0.4
0.6
0.8
1
Wavelength, nm
Cone sensitivity
440
S
540
M
570
L
440
S
540
M
570
L
440
S
540
M
570
L
440
S
540
M
570
L
440
S
540
M
570
L
Fig. 2.1. The graph illustrates the sensitivity curves of S-, M-, and L-cones of the retina to the
wavelength of the stimulating light
humans become more sensitive to longer wavelengths (yellow and orange) than short
wavelengths (cyan and blue). In addition to decreased sensitivity to blue, with aging
our general light sensitivity also decreases.
2.2 Retina and Color
The retina has sensor cells, called cones and rods, that react to photons. Rods require
very small amounts of photons to respond compared to cones. Rods also respond
to a wider range of wavelengths of photons. Within their range of received photon
amounts, that is the light intensity, both cone and rod cells respond more intensely
upon arrival of more light photons. Humans rely on cones for day vision, whereas
they use rod sensors, which are wavelength-insensitive in practice, for night vision.
This is the reason that we have difﬁculty perceiving the color of an object at dark,
even though we may be perfectly able to recognize the object. By contrast, the cones,
which greatly outnumber the rods in the retina, are not only sensitive to the amount
of light, but are also sensitive to the wavelength of the light. However, they also
require many more photons to operate, meaning that the cones are switched “off” for
nightvision, and they are “on” for dayvision.
Cones belong to either L-, M-, or S- types representing long, middle and short
wavelengths. These categories have also been called red, green and blue types mak-
ing allusion to the perceived colors of the respective wavelengths of the cells. How-
ever, studies in neurobiology and psychology have shown that the actual colors the
top sensitivity of the cones represent do not correspond to the perceptions of red,

2.2 Retina and Color
23
green, and blue but rather to perceptions of colors that could be called yellowish-
green, green and blue-violet, respectively. Figure 2.1 illustrates the average sensi-
tivity of the cones to photon wavelengths along with perceptions of colors upon re-
ception of photons with such wavelengths. Note that hues associated with pink/rose
are absent at the bottom of the diagram. This is because there are no photons with
such wavelengths in the nature. Pink is a sensation response of the brain to a mix-
ture of light composed of photons predominantly from short (blue) and long (red)
wavelengths.
The dotted sensitivity curves in the graph are published experimental data [80],
whereas the solid curves are Gaussians ﬁtted by the author.
Long
(570 nm):
(exp(−(ω−ω1)2
2σ2
) + exp(−(ω−ω2)2
2σ2
))/C,
where C = 1.32, ω1 = 540, ω2 = 595, and σ = 30.
Middle (540 nm):
exp(−(ω−ω1)2
2σ2
),
where ω1 = 544, and σ = 36.
Short
(440 nm):
(exp(−(ω−ω1)2
2σ2
1
) + CE exp(−(ω−ω2)2
2σ2
2
))/C,
where CE = 0.75, C = 1.48, ω1 = 435,
ω2 = 460, σ1 = 18 and σ1 = 23.
More than half of the cones are L-cones (64%). The remaining cones are predomi-
nantly M-cones (32%), whereas only a tiny fraction are S-cones (4%). It is in fovea,
within appproximately 1◦of eccentricity, where humans have the densest concen-
tration of cones. The fovea has no rods, and with increased density towards higher
eccentricities, the cone density decreases while the rod density increases. Even the
cones are unevenly distributed in the central part, with M-cones being the most fre-
quent at the very center, surrounded by a region dominated by L-cones. The S-cones
are mainly found at the periphery, where the rods are also found. The center of the
retina is impoverished in S-cones (and rods). The minimum amounts of photons re-
quired to activate rods, S-cones, M-cones, and L-cones are different, with the rods
demanding the least. Among the cones, our M-type need the least amount of photons
for activation, meaning that more intense blues and reds, compared to green-yellows,
are needed in order to be noticed by humans.
The coarseness of a viewed pattern matters to the photoreceptors too. A retinal
image with a very coarse pattern has small variations of light intensities in a given
area of the retina than does a ﬁne pattern that varies more. A repeating pattern is
also called texture. Coarse textures contain more low spatial-variations than ﬁne tex-
tures. Silhouettes of people viewed through bathroom glass belong to the coarse cat-
egory. A retinal image with “ﬁne” texture is characterized by rapid spatial changes
of the luminosity such as edge and line patterns. This type of patterns is respon-
sible for the rich details and high resolution of the viewed images. We will discuss
the coarseness and ﬁneness with further precision, when discussing the Fourier trans-
form and the spectrum (Chap. 9). Generally, the photoreceptors at high eccentricities,
i.e., basically S-cones and rods, respond to low spatial-variations (spatial frequen-

24
2 Color
cies), whereas those in the central area, i.e., basically M- and L-cones, respond best
to high spatial-variations. The ﬁneness (spatial frequency) at which a photoreceptor
has its peak sensitivity decreases with increased eccentricity of the receptors. At the
periphery, where we ﬁnd rods and S-cones, the photoreceptors respond to low spa-
tial variations (silhouettes) whereas the central vision dominated by M- and L-cones
responds better to high spatial variations.
2.3 Neuronal Operations and Color
Color perception is the result of comparisons, not direct sensor measurements. The
amount of photons with a narrow range of wavelengths reﬂected from a physical sur-
face changes greatly as a function of the time of the day, the viewing angle, the age of
the viewer,..., etc., and yet humans have developed a code that they attach to surfaces,
color. Human color encoding is formidable because, despite severe illumination vari-
ations (including photon wavelength composition), it is capable of responding with
constant color sensation for the viewed surface. This is known as color constancy.
It has been demonstrated by Land’s experiments [145] that the color of a viewed
patch is the result of a comparison between the dominant wavelength of the reﬂected
photons from the patch and those coming from its surrounding surface patches.
The signals coming from the L-, M-, and S-cones of the retina, represented by
L, M, and S here, arrive at the two lateral geniculate nucleus (LGN) areas of the
brain. At the LGN the signals stemming from the 3 cone types in the same retinal
proximity are presumably added and subtracted from each other as follows:
˜L + ˜
M
Lightness sensation
˜L −˜
M
Red–green sensation
˜L + ˜
M −˜S
Blue–yellow sensation
The ˜· represents a local, weighted spatial summation of the respective cone type
responses [113]. The local window weighting is qualitatively comparable to a 2D
probability distribution (summing to 1), e.g., a “bell”-like function (Gaussian). The
positive terms in the three expressions have weight distributions that are much larger
at the center than those of the negative terms. Accordingly, the net effect of ˜L−˜
M is
a center–surround antagonism between red and green, where red excites the center
as long as there is no green in the surround. If there is green in the surround the
response attenuates increasingly. This signal processing functionality is found among
parvocellular cells in layers 4–6 of the LGN, called (r + g−)-cells. However, the
mathematical expression ˜L −˜
M above can result in negative values if ˜L < ˜
M. In
that case another group of cells, the (g + r−)-cells which are also found among
the parvocellular cells, will deliver the negative part of the signal ˜L −˜
M, while
the (r + g−)-cells will be inactive. The (g + r−)-cells function in the same way
as (r + g−)-cells except that they are excited by green in the center and inhibited
by r in the surround. Accordingly, (r + g−)- and (g−r+)-cells together implement
˜L −˜
M. Likewise, ˜L + ˜
M −˜S results in an antagonism between blue and yellow.
This scheme is presumably implemented by two groups of parvocellular LGN cells,

2.3 Neuronal Operations and Color
25
(y + b−) and (b + y−), where y is a shorthand way of saying “red plus green”.
The latter is perceived as yellow light if the amount of light in the red wavelength
range is approximately the same as that of green. Together, the (r +g−)-, (g +r−)-,
(y + b−)-, and (b + y−)-cells populate the vast majority of the cells in layers 3–6 of
LGN. Albeit in minority, there is another signiﬁcant cell type in these layers that is of
the center–surround type. Cells of this type differ from the other cells in that they are
color-insensitive, and presumably implement the ˜L + ˜
M-scheme. Additionally, the
entire layers 1 layers 2 are populated by this type of cells, the magnocellular cells,
albeit these are larger than the parvocellular cells populating layers 3–6.
It is worth noting that in the perception of lightness, or luminosity, the blue color
does not play a signiﬁcant role. Details only differing in the amount of blue do not
show up very well because such changes do not contribute to the perception of edges
and lines.
In LGN, most of neurons are wavelength-selective while being center–surround.
They are excited by one wavelength pattern of the stimulus light falling in one re-
gion of their receptive ﬁeld and inhibited by another in the other. However, they do
not measure wavelength differences between the light falling into their center, and
surround. Merely, they express the difference in the amount of light quanta with
speciﬁc wavelengths captured by the center and surround regions. In a way, it is a
matter of spatial subtraction that these cells perform, not wavelength subtraction.
The blobs encountered in layers 1–3 of the V1 area contain the so-called double-
opponent color cells, which are sensitive to wavelength differences in the center and
surround regions [155]. They respond vigorously to one wavelength in the center
of their receptive ﬁeld, while they are inhibited by another (still in the center). The
same cells are excited by this second wavelength in the surround and depressed by
the ﬁrst. A double-opponent color cell can thus be excited by the wavelength of red
and inhibited by that of green in its center, while it will be excited by the wavelength
of green and inhibited by the wavelength of red in the surround. This behavior has
been denoted as (r + g −/g + r−) in neurobioligical studies. Consequently, a large
patch reﬂecting red will generate zero response from these cells because the wave-
length pattern in the center is “subtracted” from that of the surround. In fact, not only
red colored light, but any colored light, including white, that shines up a large patch
observed by an (r +g −/g +r−)-cell will generate zero response. An (r +g−)-cell
of LGN, by contrast, will be excited, if the wavelength pattern matches either the
one it prefers in the center or the one in the surround. The following types of double-
opponent color cells have been experimentally observed in blobs: (r +g −/r −g+),
(r −g + /r + g−), (b + y −/y + b−), (b −y + /y −b+), where b corresponds
to the light with wavelength patterns of S-cones (blue) and y is light with an addi-
tive combination of wavelength patterns represented by L-cones (red) and M-cones
(green). It is presumably the double-opponent color cells that are largely responsible
for color constancy observed in many ﬁsh species, macaque monkey and humans,
although these cells appear in the retina in ﬁsh.
Simpliﬁed, there are three color axes (lightness, red–green and yellow–blue)
along which color processing takes place in humans. However, there are only three
independent measurements, represented by the signals L, M, and S, that drive our

26
2 Color
color perception system. The comparisons are carried out on spatially ﬁltered L-,
M-, and S-signals combined linearly (additions or subtractions preceded by spatial
summation) rather than the original cone signals. One can therefore expect that a
color perception model can be built by a spatial summation ﬁltering of L, M, and S
signals combined with pointwise operations that probably include addition, subrac-
tion, and normalization to achieve color constancy. Next, we outline such a plausible
theory.
In Land’s retinex theory [146], which is in part found in that of Ewald Hering
(1834–1918) [105], the color sensation algorithm is suggested as
Ri(x, y) = log
fi(x, y)
g(x, y) ∗fi(x, y)
(2.1)
where g is a spatial lowpass ﬁlter that is used to average large areas of the retina, ∗is
the operation that performs local averaging (we will discuss such operations further
in Section 7.3), and fi is one of the cone signal response combinations, {˜L+ ˜
M, ˜L+
˜
M −˜S, ˜L −˜
M}, above. There exist simulation studies of this model, including on
how the order of convolution and log functions affects the result, and how a Gaussian
and other functions perform [127], conﬁrming a fairly accurate prediction of the
color constancy.
2.4 The 1931 CIE Chromaticity Diagram and Colorimetry
The chromaticity diagram, constructed in 1931 by the Committe International de
l’Eclairage,1 CIE, links the wavelength of light to perceived colors as an interna-
tional standard, (Fig. 2.2). It is used for a variety of purposes, including to compare
colors produced by color-producing devices, e.g., PC monitors, printers, and cam-
eras. The science of quantifying color is called colorimetry.
The CIE diagram is a projection of a 3D color space, called XYZ color space,
to 2D. The X, Y, Z coordinates are found as follows. The light emitted by a device,
or light reﬂected from a surface consists of photons with different wavelengths. The
amount of photons with a certain wavelength, λ, in a given light composition is rep-
resented by the function C(λ). The CIE diagram comprises three functions μX(λ),
μY (λ), μZ(λ) (Fig. 2.3). With these functions one can calculate three scalars, called
X, Y , Z,
X =

C(λ)μX(λ)dλ
Y =

C(λ)μY (λ)dλ
Z =

C(λ)μZ(λ)dλ
(2.2)
There are devices that can measure X, Y , and Z by use of ﬁlters and photosensors.
Because the functions C and μ. are positive, the scalars X, Y , Z are real and non-
negative. These measurements represent the color coordinates of the observed light
in the CIE–XYZ color system. The projection to the CIE diagram is obtained via
1 French for illumination

2.4 The 1931 CIE Chromaticity Diagram and Colorimetry
27
x
y
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
630
630
610
610
590
590
600
600
580
580
570
570
560
560
550
550
540
540
530
530
520
520
510
510
500
500
490
490
480
480
470
470
460
460
420
420
680
680
Fig. 2.2. The perceivable colors and their wavelengths, according to CIE (1931) standard on
color
x =
X
X + Y + Z
(2.3)
y =
Y
X + Y + Z
(2.4)
z =
Z
X + Y + Z
(2.5)
where x + y + z = 1, its so that only two of x, y, z are independent, making the
projection a planar surface. In Fig. 2.2 x and y are the coordinate axes. After having
projected its X, Y, Z values via Eqs. (2.3)–(2.5), each perceived color is a point on
the CIE diagram. The projection amounts to a normalization of the 3D XYZ space
with respect to luminosity, X + Y + Z. The 2D xy color space represents the colors
appearing in the CIE diagram.

28
2 Color
400
450
500
550
600
650
700
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
μX(λ)
μY(λ)
μZ(λ)
Wavelength λ
Sensitivity
Fig. 2.3. The functions used in projecting the wavelength distribution to CIE XY Z space
The aim of the CIE diagram is to model color, e.g., those generated by a TV set,
as if generated by mixing three types of light sources, each composed of photons
with different ranges of wavelengths. This is called additive2 color model. Each of
the threee light sources alone will produce a different color sensation, corresponding
to the three “primary” colors, i.e., three points in the CIE diagram. A new color
is produced by changing the relative amount of light emitted by the primary light
sources. An example of such a color triplet is marked as R, G and B in the copy
of the CIE diagram represented by Fig. 2.4. If these three colors are appropriately
placed by the manufacturer of the device, then most colors will be reproducable.
The points R, G, B on the diagram will deﬁne a triangle, so that any new color
made by mixing these three (primary) colors will be within the triangle. It should,
however, be emphasized that it is impossible to ﬁnd three such points so that all
perceivable colors of the CIE diagram can ﬁt into the corresponding triangle, since
the form of the diagram is not strictly triangular. In consequence, there will always
be a fraction not included in the triangle, if the CIE diagram is to be approximated
by three points. The colors included in the triangle are called the gamut of the three
primaries. As a special case, one can produce a limited range of “color” by mixing
only two primaries. In this case the produced colors will be limited to those to be
found on the line joining the two primaries, the gamut of them.
The point marked as W in Fig. 2.4 is the color white. Note that we have three
color components, XY Z, but these are normalized to yield xy coordinates.3 As a
result, the colors in the CIE diagram are normalized so that colors differing by only
2 The subtractive color model, e.g., used by painters, also exists and achieves the same result.
3 Because x + y + z = 1, computing z is not useful in practice.

2.4 The 1931 CIE Chromaticity Diagram and Colorimetry
29
H
W
X
X´
H´
R
G
B
630
630
610
610
590
590
600
600
580
580
570
570
560
560
550
550
540
540
530
530
520
520
510
510
500
500
490
490
480
480
470
470
460
460
420
420
680
680
Fig. 2.4. The deﬁnition of hue and purity
luminosity are represented by the same point. The pure hue is deﬁned via the normal-
ized color components x, y and is to be found at the boundaries, where the dominant
wavelength of photons producing this color is also read out. The point marked as H
in the ﬁgure is a pure hue, whereas less pure colors of the same hue are to be found
on the line segment between W and H. The point H has the purity 1, while the point
marked as X has the purity ∥W X∥
∥W H∥with ∥WX∥representing the distance from W
to X. The point X′ is the complementary color of X, and is found in the opposite
direction, but with the same purity, ∥W X′∥
∥W H′∥= ∥W X∥
∥W H∥. The point H′ is the boundary
point obtained by mirroring H through W.

30
2 Color
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
R
B
G
Green axis
Blue axis
Red axis
Fig. 2.5. The edges of the RGB color space and some important lines with the corresponding
colors. The edges behind the visible faces are marked as dotted
2.5 RGB: Red, Green, Blue Color Space
There exist other color spaces, each offering a desired convenience in applications
it is intended for, but all relate to CIE–XYZ space. The RGB color space is one
such space that is widely used by millions of devices, including nearly all TV sets,
computer displays, projectors. From experimentally determined XY Z values of the
three primaries, the RGB space is obtained via the linear (coordinate) transformation
⎛
⎝
R
G
B
⎞
⎠=
⎛
⎝
2.36461 −0.89654 −0.46807
−0.51517 1.42641
0.08876
0.00520 −0.01441 1.00920
⎞
⎠·
⎛
⎝
X
Y
Z
⎞
⎠
(2.6)
In the additive model, a mixture of colors using positive amounts from the primaries
R,G,B results in a new color. Assuming that the amount mixed from each primary
is in the interval [0, 1], the resulting color will be represented by a point in a cube,
having the red, green, blue basis as the orthogonal edges of a cube (Fig. 2.5). The
vertices marked as R, G and B deﬁne a triangle in the CIE color diagram above. The
triangle represents the gamut of the printer having its primary colors as those marked
with R, G and B. The gray line in one of the main diagonals represents the luminosity
variation available to the RGB space. It consists of colors having equal components
of red, green and blue. Two colors that are complementary have red, green and blue
components that sum to 1, 1, 1, respectively.

2.6 HSB: Hue, Saturation, Brightness Color Space
31
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Red
Green
Blue
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Red
Green
Blue
Blue
0.2
0.4
0.6
0.8
1
Fig. 2.6.
(Left) The isocurves of the HSB coordinates when the saturation is 0.5. The
nine twisted hexagons represent colors for different values of brightness. (Right) The same
isocurves viewed through the brightness axis, (1, 1, 1)T
2.6 HSB: Hue, Saturation, Brightness Color Space
Whereas the colors of the RGB space are usually produced by varying three colored
light emitters, it is not easy for humans to interpret the thus-obtained colors. Derived
from the RGB space, the hue, saturation and brightness space,4 HSB color space
is, by contrast, intuitive. It interprets the color in a way that resembles artists’ way of
describing or perceiving it. Assuming to have a color with the components (r, g, b)T ,
in the RGB space, the HSB representation of this color yields
H =
⎧
⎨
⎩
(
g−b
max −min) π
3 ,
if
r = max,
(2 +
b−r
max −min) π
3 ,
if
g = max,
(4 +
r−g
max −min
π
3 ,
if
b = max,
(2.7)
S = max −min
(2.8)
B = max
(2.9)
where max and min refer to the largest and smallest values of r, g, b, respectively.
The HSB coordinates resemble5 the cylindrical coordinates of the RGB color point
around the brightness axis, the main diagonal represented by the direction (1, 1, 1)T
of the RGB cube, although there are signiﬁcant differences. First, the coordinate
curves and sufaces of the HSB colors are contained within the RGB cube, but they are
neither cylindrical nor circular. By that we mean the curves and surfaces generated
when one or two of the variables H,S,B of the color is kept constant, i.e., the isocurves
and surfaces of the coordinate transformation above. They are curves that generate
4 Another name for this is hue, saturation, value (HSV) color space.
5 There are also studies in which HSB coordinates are presented as cylindrical coordinates,
although the “hexagon” version here appears to be the most common, because of its conver-
sion equations using only arithmetic operations, which result in simple graphics hardware.

32
2 Color
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Red
Green
Blue
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
Red
Green
Blue
Fig. 2.7. The same as in Fig. 2.6 except the saturation is now full, S = 1
cones around the brightness axis instead of cylinders. In Fig. 2.6 and Fig. 2.7 two
such cones, corresponding to S = 0.5 and S = 1, respectively, are sampled. Second,
on such hexagonal cones, the color curves corresponding to iso-brightness, i.e., when
both saturation and brightness are constant, consist of twisted hexagons i.e., each
such hexagon is a true 3D curve, does not lie in a single plane. The hexagons are
twisted in such a way that the resulting curve is continuous, but the curve zigzags
in 3D, while always being a straight line parallel to the edges of the RGB cube, as
depicted by the ﬁgures.
The hue component tells where on the twisted hexagon a color is in analogy with
the angle in cylindrical coordinates. The “zero” value of hue points at red, as does
the value 1, to be contrasted to 2π an angle of cylindrical coordinates. The saturation
component S is similar to the radial coordinate, i.e., it represents the “radius” of the
hexagon around the brightness axis. By changing it, we effectively change the size
of the twisted hexagon. The brightness component resembles the height component
of the cylindrical coordinates. Increasing it at the same saturation moves the twisted
hexagon towards the white color, (1, 1, 1)T in RGB, with the value 1 representing
the intersection of the cone with the three walls of the RGB cube neighboring the
white point.
All human labels for color are reachable by changing the hue variable, whereas
changing the other two variables for a ﬁxed hue does not change the human label for
the sensed color. Decreasing the saturation component amounts to adding more black
color in the minds of artists. A 100% saturated color of a given hue and brightness
is the most vivid color of that hue/brightness combination. Fully saturated colors,
S = 1, appear on any of the three boundary surfaces of the RGB cube (Fig. 2.7). A
0% saturated color is perceived as colorless, i.e., gray, regardless the hue. It is the
result of collapsing the hexagons to the brightness axis. Increasing the brightness
component amounts to adding white in artists’ terminology. This has the effect of
making the hue shine.

Part II
Linear Tools of Vision
Let books be your dining table
And you shall be full of delights
Let them be your mattress
And you shall sleep restful nights
St. Ephrem (A.D. 303–373)

3
Discrete Images and Hilbert Spaces
A Hilbert space is a mathematical space that is equipped with a scalar product. A
Hilbert space is also referred to as an inner product space. The elements of Hilbert
spaces are typically sequences of scalars or functions, also called vectors. Provided
that certain rules are respected, matrices, which are studied in linear algebra, can be
viewed as vectors, and their space as a vector space. A scalar product for matrices,
which can easily represent discrete images, can also be deﬁned. Most important,
however, is that the space of all images can be viewed as a Hilbert space. By using the
scalar product the angles between vector pairs of the Hilbert space can be determined.
This allows us to quantify how similar two images are. A Hilbert space contains
numerous orthonormal bases. Once a basis is given, every element of the Hilbert
space can be written uniquely as a sum of multiples of the basis elements. In this
way, it is feasible to represent an image as a sum of the basis vectors weighted with
appropriate coefﬁcients.
3.1 Vector Spaces
In linear algebra courses one studies vectors that represent points in spaces. A vector
is often visualized as an arrow from the center of the coordinate system, the origin,
to the point. The point itself can be seen as equivalent to the vector, or vice versa.
Although we will work with other vector spaces mostly in this book, we start with
a vector space with which we have the most familiarity, the environment in which
we move about, i.e., ordinary space. Without the time aspect, it is referred to as
a 3D Euclidean space, or simply as E3. The points in this space are numerically
represented as an ordered set of real numbers x(1), x(2), x(3) standing one on the
top of the other:
x =
⎛
⎝
x(1)
x(2)
x(3)
⎞
⎠
(3.1)
The elements, x(i) are to be interpreted relative to a set of abstract quantities, e1, e2,
and e3, called the basis, such that they are coordinates, i.e., x(i) act as coefﬁcients

36
3 Discrete Images and Hilbert Spaces
“resizing” the basis, to “add” up to the vector x. Assuming that we know how to
resize, and add vectors, this idea is expressed as follows.
x = x(1)e1 + x(2)e2 + x(3)e3
(3.2)
In related literature as well as later in this book, the basis set attached to an origin
is also called a coordinate frame, or just frame when there is no risk of interference
with other concepts. Since a basis set is the same for all points in the space, ei do
not have to be written each time one needs to represent a vector. This is why they
are omitted in (3.1). Because any point P in E3 can be represented by a triplet of
real numbers relative to an origin and a basis set, as in Eq. (3.2), all points of E3 are
“vector” quantities. In that, the point P’s being a vector depends on another point,
which is often not mentioned, the origin, O. Without O the point P would not have
been a vector. Indeed vectors are quantities with directions so that any two points A
and B in E3 deﬁne a vector, also written as −−→
AB or AB (or even as AB when there
is no risk of confusion) in an alternative notation to x.
The above concept does not need to be limited to our 3D space. The vectors ei,
and thereby x, do not need to be basis vectors in our ordinary 3D space. They can be
any abstract quantities or “points” as long as the set they are part of is a set that has
the vector space properties deﬁned below:
Deﬁnition 3.1. A set of quantities {x} is called a vector space if the members obey
two rules
1. Scaling with a scalar is deﬁned and the space is closed under this operation,
also called vector scaling;
2. Addition is deﬁned and the space is closed under this operation, also called
vector addition.
The term closed represents the fact that the result of an operation (vector scaling
or vector addition) never leaves the original space (the set {x}), no matter which
quantities are involved in the operation.
Scaling and addition in our 3D Euclidean space are visually meaningful. Scaling
represents making the vectors “longer” or “shorter” while adding two vectors means
that we concatenate them at their ends by translation (moving one of the vectors
without rotation). In a given basis of the E3, the mechanism of scaling is to multiply
all three vector components with the scalar α, while the mechanism performing addi-
tion is to add the three vector components. In other words, given a representation of
E3, there is a method of performing scaling and addition that obeys the scaling and
addition properties without leaving the original space. Accordingly, E3 is a vector
space.
Using the same idea, we can construct similar scaling and addition mechanisms
for N-tuple arrays with N > 3. The N-tuple of real numbers,
⎛
⎜
⎜
⎜
⎝
x(1)
x(2)
...
x(N)
⎞
⎟
⎟
⎟
⎠
(3.3)

3.2 Discrete Image Types, Examples
37
also constitutes a vector space, EN. This is called the N-dimensional Euclidean
space (or room) and behaves very much like E3 as far as addition and scaling is
concerned, albeit it is not easy to visualize its members.
This concept can be easily extended to comprise arrays with more than one index,
e.g., with four indices:
A = {A(k, l, m, n)}
(3.4)
where A(k, l, m, n) is a scalar (i.e., a real or a complex number). To ﬁx the ideas,
the indicies k, l, m, n would be typically positive integers that can at most take the
values K, L, M, N, respectively. An addition and scaling rule that can make the set
of all As having the same K, L, M, N, a vector space is as follows¡:
A + B = {A(k, l, m, n) + B(k, l, m, n)}
(3.5)
αA = {αA(k, l, m, n)}
(3.6)
Matrices, the double-indiced arrays studied in linear algebra courses, constitute a
special case of the above. Evidently, those having the same size are a vector space
too!
3.2 Discrete Image Types, Examples
There are many types of discrete images, and new types are being constructed as
the science studying their production from light, by sensors and other hardware ad-
vances. This science is also called imaging. The simplest image type is a 2D array,
where the array elements represent gray values
A = {A(k, l)}
(3.7)
where k, l take K, L different values. The size of the image is K × L, and typically
k, l corresponds to row (top–down) and column (left–right) counts. However, it is
also common that k, l represent the horizontal (left–right) and vertical (down–up)
counts. The pair k, l represents the coordinates of a point, also called pixel, on the
discrete 2D grid of size K × L. The scalar A(k, l) is typically an integer ranging
0–255 and represents the gray value or the pixel value of the pixel. For convenience
we can assume that A(k, l) is a scalar (real or complex-valued), although it is not
possible to interpret negative values as gray values. Other names for gray value, are
intensity, brightness, and luminance. Negative and complex pixel values are common
and can be obtained from computations in image processing, even from sensors. For
example, we can have an array with two indices representing a map of altitutes w.r.t.
the sea level. The points under the sea level will be negative, and those above will
be positive. There are examples of complex-valued images that can be obtained from
sensors as well as from computations. To limit the scope, we do not discuss the tax-
onomy of images at that level here. The space of discrete images of the same size
taking scalar values is thus a vector space. The necessary rules of vector addition
and scaling can be deﬁned via the general elementwise addition and scaling rules

38
3 Discrete Images and Hilbert Spaces
discussed in the previous section. Accordingly, image addition is achieved by adding
the image values at corresponding points, whereas image scaling is achieved by mul-
tiplying the image values with a scalar, e.g., as given by Eqs. (3.8) and (3.9) for gray
images:
A + B = {A(i, j) + B(i, j)}
(3.8)
αA = {αA(i, j)}
(3.9)
Exercise 3.1. Is the space of images having the same size but taking the gray values
0, 1, · · · , 255 a vector space?
Extending the number of indices to three in a discrete array
A = {A(k, l, m)}
(3.10)
has also counterparts in the world of images. The most commonly known example is
discrete color images, where the ﬁrst two indices of A represent the coordinate point
and the third index deﬁnes the color components. In other words, at every spatial
position, which is another common name for the coordinate pair k, l, we have an
M-dimensional array of scalars (instead of a single scalar). In the case of ordinary
color images, M is 3, representing the color components, typically in the RGB color
space. There are large banks of discrete image data of the earth where M is > 3; these
are multispectral images. Here each “color” represents a speciﬁc photon wavelength
range of the light, some visible, many invisible to humans. Such images with scalar
values of A(k, l, m) constitute a vector space with the addition and scaling rules:
A + B = {A(k, l, m) + B(k, l, m)}
(3.11)
αA = {αA(k, l, m)}
(3.12)
For certain image types, there is also a different interpretation of the third index.
In computer tomography images one also obtains an array of discrete data with three
indices, as discussed above. In this case the indices k, l, m represent horizontal, ver-
tical, and depth coordinates of a spatial point, commonly called a voxel. The scalar
A(k, l, m) typically represents absorption.
There is yet another interpretation of the third index for black and white motion
images. The ﬁrst two indices k, l correspond to the spatial position, whereas the
third index m corresponds to the temporal instant. Accordingly, k, l, m are called
the spatio–temporal coordinates.1 With the same size and scalar image values these
constitute vector spaces too.
Increasing the number of indices to four brings into focus images that are com-
monly produced by consumer electronics, color motion images. These are also called
image sequences. The ﬁrst three indices are the spatio–temporal coordinates, whereas
the fourth index encodes the color. Evidently, even these images constitute a vector
space, with the addition and scaling rules analogous to Eqs. (3.5)–(3.6).
The list of discrete image types can be made longer, but we stop here to proceed
with an example illustrating what one can do with vector spaces.
1 A suitable name for these coordinates would be stixels, making allusion to the spatio–
temporal nature of the data.

3.2 Discrete Image Types, Examples
39
Fig. 3.1. An image sequence and the average image (bottom) obtained by applying the matrix
vector space rules of addition and multiplication by a scalar according to Eq. (3.13)

40
3 Discrete Images and Hilbert Spaces
Example 3.1. The 2D discrete color images (of the same size) are vector spaces if
we treat them as matrices having elements consisting of arrays representing the three
color components, R, G, B. In the right–down direction, the digital pictures in Fig.
3.1 illustrate the frames of an image sequence except the last frame (bottom). The
last frame is the average of all frames observed by the camera (shown in the ﬁgure).
¯A = (A1 + A2 + · · · A15)/15
(3.13)
where Ai is the ith frame. The frames are summed and ﬁnally multiplied with a
scalar (1/15), according to the addition and scaling rules of Eqs. (3.11)–(3.12)
In the observed sequence the camera is static (immobile) and the passing human
is present in every frame, and yet nearly no human is present in the averaged frame.
This is because at any given point, the color is nearly unchanged over the time be-
cause the camera is ﬁxed. Though seldom, a change of the color does occur at that
point, due to the passage of the human. This happens in, roughly 1 out of 15 time
instants at the same pixel coordinate. Consequently, the time averages of color are
not sufﬁciently inﬂuenced by one or two outliers, to the effect that the means closely
approximate the background colors. Another effect of averaging is that the mean
represents the color more accurately than any of the constituent images. Similar uses
of matrix averaging, sometimes combined with robust statistics (e.g., clipped aver-
age, median, etc.) are at the heart of many image processing applications, e.g noise
reduction [85,192], super resolution, and the removal of moving objects, [9,119].
Exercise 3.2. Can we use the HSB space to represent the color arrays under the
rules of (3.11)–(3.12)?
HINT: Do the points on a circle represent a vector space? Is the numerical average
of two numbers representing hue always a hue (e.g., two complementary colors)?
3.3 Norms of Vectors and Distances Between Points
Adding or scaling vectors are tools that are not powerful enough for our needs. We
must be able to measure the “length” of the vectors as well. The length of a vector is
also known as the norm or the magnitude of a vector. The symbol for the norm of a
vector u is ∥u∥.
In analogy with the length concept in E3,
•
The norm should not be negative.
•
Only the null vector has the norm zero.
Despite its simplicity, the second requirement constitutes the backbone of nu-
merous impressive proofs in science and mathematics. It expresses when only the
knowledge of the norm is sufﬁcient to identify the vector itself fully. The vector
space properties guarantee that the null vector is the only vector which enjoys the
privilege that the knowledge of its norm automatically determines the vector itself.
Paraphrasing this, we have the right to “throw away” the norm symbol only when we
are sure that the norm is zero.

3.3 Norms of Vectors and Distances Between Points
41
Exercise 3.3. In two dimensions, how many points are there such that their coordi-
nate vectors have the norm (i) 5, (ii) 0?
HINT: Can we count them in both cases?
Once it is deﬁned on a vector space, an important usage of the norm is in the
computation of the distance between two points of that space. The difference
u −v
(3.14)
is a vector if the vectors u and v belong to the same vector space, because in such
a space addition and scaling are well deﬁned and the space is closed. The distance
between two points is deﬁned as
∥u −v∥
(3.15)
This is natural because when ∥u−v∥= 0, then we know that the two vectors are one
and the same. This follows from the properties of the norm, namely that if the norm
is the null then the vector is null vector, u −v = 0, which is the same as u = v.
There are two other properties that a norm must have before it can properly be
called a norm:
•
When a vector is scaled by a scalar, its norm must scale with the magnitude of
the scalar.
•
The vector space must yield the triangle inequality under the norm, i.e., the short-
est distance between two points is the norm of the vector joining them.
We summarize the norm properties that every norm, regardless of the vector
space on which it is deﬁned, must have as follows
0 ≤∥u∥,
Nonnegativity
(3.16)
∥u∥= 0 ⇔u = 0,
Nullness
(3.17)
∥αu∥= |α|∥u∥,
Scaling
(3.18)
∥u + v∥≤∥u∥+ ∥v∥.
Triangle inequality
(3.19)
We already know that discrete color images constitute a vector space. By using
the rules above, we can add a norm to a vector space that can be used to represent
discrete images having vector-valued pixels.
Exercise 3.4. Let A be an image with vector-valued pixels. Show that the expression
∥A∥= [

ijk
A∗(i, j, k)A(i, j, k)]1/2
(3.20)
where
∗is the complex conjugate, is a norm.
HINT: This rule obeys the triangle inequality under the addition and scaling rules of
Eq. (3.11)–(3.12).

42
3 Discrete Images and Hilbert Spaces
Fig. 3.2. Norms under scaling of 2D vector-valued images are illustrated by a digital color
image. On the left the original image A, having three color components at each image point,
with the norm ∥A∥= 874, is shown. On the right, 0.5A with the norm ∥0.5A∥= 437 is
displayed
Example 3.2. The left image in Fig. 3.2 has three real valued color components at
each pixel, i.e., RGB color values. Each color component varies between 0 and 1,
representing the lowest and highest color component value, respectively. The image
is thus a vector-valued image that we can call A. The darker right image in Fig. 3.2
is 0.5A, i.e., it is obtained by multiplying all color components by the scalar 0.5. The
norm of A is computed to be 874, by using the expression in Eq. (3.20), whereas the
norm of 0.5A using the same expression is computed to be 437. As expected from a
true norm, scaling all vector pixels by the scalar 0.5 results in a scaling of the image
norm with the same scalar.
Exercise 3.5. If the image A shown in Fig. 3.2 had been white, i.e., all three color
components were equal to 1.0, then one obtains ∥A∥= 1536, where the norm is in
the sense of Eq. (3.20). How many pixels are there in A? Can you ﬁnd how many
rows and columns there are in A?
HINT: Use a ruler.
The images in Fig. 3.3 illustrate the triangle inequality of the norm. They rep-
resent, clockwise from top left, A1, A2, A3, and A1 + A2 + A3, respectively.
At any image point the pixel is a vector consisting of three color components.
The norms of these images are ∥A1∥= 517, ∥A2∥= 527, ∥A3∥= 468, and
∥A1 + A2 + A3∥= 874, respectively. As expected from a norm, we obtain
∥A1 + A2 + A3∥≤∥A1∥+ ∥A2∥+ ∥A3∥.
To illustrate both the triangle inequality and the scaling property of the norms,
we study the quotient Q
Q(A1, A2) =
∥A1 + A2∥
∥A1∥+ ∥A2∥≤1
(3.21)
which equals 1 if both A1 and A2 are vectors that share the same direction. In other
words, if

3.3 Norms of Vectors and Distances Between Points
43
Fig. 3.3. Norms under addition of 2D vector-valued images are illustrated by adding three
digital color images. The bottom right is the addition of the other images. The norms, counter
clockwise from top left, are ∥A1∥= 517, ∥A2∥= 527, ∥A3∥= 468, ∥A1 + A2 + A3∥=
874, respectively
A2 = αA1
(3.22)
where α is a positive scalar, then the quotient Q will equal 1, which is a consequence
of the triangle inequality and the scaling property of the studied vector space. When
A1 and A2 are those displayed in Fig. 3.2, the quotient Q equals to 1, because by
computation we obtain ∥A1 +A2∥= 1311, ∥A1∥= 874, and ∥A2∥= 437. In view
of the norm represented by Eq. (3.20) and the triangle inequality, relation (3.19), this
is expected.
However, because of the continuity of Q w.r.t. A2, even when A2 approximately
equals to αA1, we may expect that Q will be close to 1. To see whether this is
true, we study the corresponding quotients for the four images in Fig. 3.4, which are
Q(A1, A2) = 0.960, Q(A1, A3) = 0.943, Q(A1, A4) = 0.955. These suggest that
image A1 was likely obtained by multiplying A2 with a positive constant, although,
in fact, this is not true. The images A1 and A2 are instead different views of the same
scene, and are not obtainable from each other by scaling the color components. As
suggested already by the example, i.e., that all three quotients are close to each other
and not far away from 1, the quotient Q as a means to establish general similarities of
images may not be reliable, although it can be used to test certain types of similarities

44
3 Discrete Images and Hilbert Spaces
Fig. 3.4. Four images A1, A2, A3, A4, clockwise from the top left, are to be compared with
each other by using the triangle inequality and the scaling rule of the norms. The pairwise
quotients Q suggest that A1 is most likely obtained from A2 by a positive multiplicative
constant
i.e., those that are obtained by positive scaling. Furthermore, in case α is negative
Q will not be reliable at all because it will not only be less than 1, but it might
even vanish, i.e., when α = −1. We stress therefore that Q illustrates the triangle
inequality and the scaling, but that it cannot be used as a means to test all types of
image similarities.
3.4 Scalar Products
The norms are useful to measure distances. Next, we present another tool that is
useful for “navigation” in abstract vector spaces. This is the scalar product which
will be used to measure the “angles” between vectors in vector spaces. A scalar
product is a particular operation between two vectors that results in a scalar (real or
complex) that “somehow” represents the angle ϕ between the two vectors involved
in the product. The symbol of this operation is written as:
⟨u, v⟩
(3.23)

3.4 Scalar Products
45
Any rule that operates on two vectors and produces a number out of them is not good
enough to be qualiﬁed to be a scalar product. To be called a scalar product, such an
operator must obey the scalar product rules:
1. ⟨u, v⟩
=
⟨v, u⟩∗
2. ⟨αu, v⟩
=
α∗⟨u, v⟩
3. ⟨u + v, z⟩
=
⟨u, z⟩+ ⟨v, z⟩
4. ⟨u, u⟩
> 0
if
u ̸= 0,
and
⟨u, u⟩
= 0
iff
u = 0
Here the star is the complex conjugate, and “iff” is a short way of writing “if and
only if”. Remembering the ﬁrst requirement, we note that the second is equivalent to
⟨u, αv⟩= α⟨u, v⟩
(3.24)
As a byproduct, the last relationship offers a natural way of producing a norm for
a vector space having a scalar product:
∥u∥=

⟨u, u⟩
(3.25)
Since the scalar product can be used to express the norm, the distance between
two vectors is easily expressed by scalar products as well:
∥u −v∥2 = ⟨u −v, u −v⟩
(3.26)
Deﬁnition 3.2. A vector space which has a scalar product deﬁned in itself is called
a Hilbert Space.
We use the term “scalar product” and not the term “real” since this allows us to
use the same concept of Hilbert Space for vector spaces having complex-valued ele-
ments. The scalar products can thus be complex-valued in general, but never the norm
associated with it. Such vector spaces are represented by the symbol CN. Hence a
scalar product for CN must be deﬁned in such a way that the auto–scalar product of
a vector represents the square of the length. The norm must be strictly positive or be
zero if and only if the vector is null. As a scalar product for EN would be a special
case of the scalar product for CN, it should be inherited from that of CN. For this
reason the deﬁnition of a scalar product on CN must be done with some ﬁnesse.
Lemma 3.1. A scalar product for vectors u, v ∈CN, i.e., vectors with complex
elements having the same ﬁnite dimension, yields
⟨u, v⟩=

i
u(i)∗v(i)
(3.27)
where
∗is the complex conjugate and u(i), v(i) are the elements of u, v.
♦
We show the lemma by ﬁrst observing that conditions 1–3 on scalar products are
fulﬁlled because these properties follow from the deﬁnition of u and v, and be-
cause u and v belong to the vector space. For any complex number u(i), the product

46
3 Discrete Images and Hilbert Spaces
u(i)∗u(i) is a real number which is strictly positive unless u(i) = 0, in which case
u(i)∗u(i) = 0. Accordingly,
∥u∥2 = ⟨u, u⟩=

i
|u(i)|2
(3.28)
is strictly positive except when u = 0. Conversely, when ∥u∥= 0, the vector u
equals the null vector so that even condition 4 on scalar products is fulﬁlled.
Evidently, this scalar product deﬁnition can be used also for EN because EN
is a subset of CN. Using this scalar product yields a natural generalization of the
customary length in E2 and E3. Because of this, the norm associated with the scalar
product is also called the Euclidean norm even if u is complex and has a dimension
higher than 3. Another common name for this norm is the L2 norm.2
In applied mathematics Hilbert spaces are used for approximation purposes. A
frequently used technique is to leave out some basis elements the space, that a priori
are known to have little impact on the problem solution. To do similar approxima-
tions in image analysis, we need to introduce the concept of orthogonality.
3.5 Orthogonal Expansion
In our E3 space, orthogonality is easy to imagine: Two vectors have the right an-
gle ( π
2 ) between them. Orthogonal vectors occur most frequently in human-made
objects or environments. In a ceiling we normally have four corners, each being an
intersection of three orthogonal lines. The books we read normally rectangular, and
at each corner there are orthogonal vectors. Fish-sticks do not swim around in the
ocean (perhaps because they have orthogonal vectors), but instead they are encoun-
tered in the human-made reality, etc.
In fact, orthogonality is even encountered in nonvisible spaces constructed by
man, the Hilbert spaces. In these spaces, which include nonﬁnite dimensional spaces
such as function spaces, one can express the concepts of distance and length. Ad-
ditionally, one can express the concepts of orthogonality and angle, as we discuss
below. First, let us introduce the deﬁnition of orthogonality.
Deﬁnition 3.3. Two vectors are orthogonal if their scalar product vanishes
⟨u, v⟩= 0
(3.29)
In E3, if we have three orthogonal vectors we can express all points in the space
easily by means of these vectors, the basis vectors. This is one of the reasons why
orthogonality appears in the human-made world, although it does not explain why
the ﬁsh are, post mortem, forced to appear as rectangular (frozen) blocks. The other
important reason, which does explain the ﬁsh-sticks, is that orthogonality reduces or
eliminates redundancy. Humans can store and transport more ﬁsh if ﬁsh have corners
with orthogonal vectors. For the same reason, pixels in images are quadratic and
2 It is pronounced as “L two norm”.

3.5 Orthogonal Expansion
47
orthogonal functions that eliminate redundancies in small rectangular image patches
have been constructed as one way to compress image data so that we can store and
transport images efﬁciently (e.g., JPEG images).
Orthogonal sets are often used as bases; that is, they are used to synthesize as well
as to analyze vector spaces. To illustrate this, we go back to our ordinary space of E3
and ask ourselves the following: How do we ﬁnd the coordinates of a point x if we
have three orthogonal vectors e1,e2,e3 that we use as a basis? We do this by the so-
called projection operation, which measures the “orthogonal shadow” of the vector
representing the point (which starts at the origin and ends, at the point itself) on the
three basis vectors. More formally, a projection is an operator that equals to identity
if it is repeated more than once. Paraphrased, the resulting vector of a projection does
not change by further projections. The point is projected orthogonally to deﬁne the
tips of the three vectors that lie on the basis vectors, which when added vectorially
end up with a result that is identical to the coordinate vector of the point, the one that
goes from the origin to the point.
To teach projection to a computer we should have a more precise mathematical
procedure of making projections than the term “orthogonal shadows”. To do that we
work backwards. We write the vector x as if we knew its coordinates or coefﬁcients,
c1, c2 and c2 in the basis {ei}, that is
x =

i
ciei
(3.30)
Now we take the scalar product of the left- as well as the right-hand side of this
equation with one of the bases, e.g., e1.
⟨x, e1⟩= ⟨[

i
ciei], e1⟩= c1⟨e1, e1⟩+ c2⟨e2, e1⟩+ c3⟨e3, e1⟩
(3.31)
Since our basis vectors are orthogonal, the scalar products between different bases
will vanish ⟨e1, e2⟩= ⟨e1, e3⟩= ⟨e2, e3⟩= 0. So we have
⟨x, e1⟩= c1⟨e1, e1⟩=⇒c1 = ⟨x, e1⟩/⟨e1, e1⟩
(3.32)
But ⟨e1, e1⟩, which is the square of the length of e1, can be computed regardless
of x since e1 is known. In other words, by using the scalar product operation, we
have achieved projecting x on e1 since we could ﬁnd c1, which was the unknown
coordinate scaling factor. If we could do it for e1, we should be able to do it for the
remaining two coefﬁcients as well. This is done without effort by just replacing the
index 1 with the desired index, that is:
ci = ⟨x, ei⟩/⟨ei, ei⟩
(3.33)
Eq. (3.33) is the reason for why the scalar product is often referred to as the projec-
tion operator because, except for the denominator, it is just a scalar product of any
member x of the space, with the known basis vectors {ei}i. The denominator does
not spoil the equivalence of the scalar product to projection since the denominator

48
3 Discrete Images and Hilbert Spaces
is simply the square of the length of the basis vectors that can be computed ahead
of the time, and in fact also be used to rescale {e}i. For this reason, it is justiﬁed to
assume that the length of basis vectors are normalized to 1,
ˆei = ei/

⟨ei, ei⟩
(3.34)
so that ⟨ˆei,ˆei⟩= 1 and the equation (3.33) reduces to
ci = x(i) = ⟨x,ˆei⟩
(3.35)
The hat on ˆei is a way to tell that this vector has the length 1. Often, the hat is even
omitted, when this fact is clear from the context.
Deﬁnition 3.4. The vectors um and un are orthonormal if
⟨um, un⟩= δ(m −n)
(3.36)
where δ is the Kronecker delta and is deﬁned as
δ(m) =
1, if m = 0;
0, otherwise.
(3.37)
In analogy with E3, even in EN (as well as in CN) there must be exactly N vectors
in order that they will be just sufﬁcient, neither too many nor too few, to represent
any point in EN. Also in EN, such a set of vectors is called a basis. If the basis
vectors are orthogonal the basis is called an orthogonal basis.
3.6 Tensors as Hilbert Spaces
Discussed in linear algebra textbooks (e.g., [147,210]) matrices are arrays with two
indices having scalar (real or complex) elements. Because addition and scaling are
well deﬁned for matrices, we already know from Sect. 3.2 that they constitute a
vector space. Furthermore, in Sect. 3.2 we saw that actually all arrays with multiple
indices are vector spaces, and many of them correspond to real images. We only
need a suitable scalar product to make such spaces Hilbert spaces. For simplicity we
assume four indices below, but the conclusions are readily generalizable.
Lemma 3.2. Let A and B be two arrays that have the same size. Then, with the
deﬁnition
⟨A, B⟩=

k,l,m,n
A(k, l, m, n)∗B(k, l, m, n)
(3.38)
the space of such arrays is a Hilbert space.
♦
To show that this fulﬁlls the conditions of a scalar product, one can proceed as in Eq.
(3.27). This scalar product prompts the following norm:
∥A∥2 = ⟨A, A⟩=

k,l,m,n
|A(k, l, m, n)|2
(3.39)

3.6 Tensors as Hilbert Spaces
49
U0
„ 1
1
1
1
«
U1
„ −1
1
−1
1
«
U2
„
1
1
−1 −1
«
U3
„ −1
1
1 −1
«
F
„ 1/2
1
0
1/2
«
˜F
„ 1/2
1
0
1/2
«
Fig. 3.5. The discrete image F can be expanded as the sum of the basis vectors U0, · · · , U3
weighted with the expansion coefﬁcients ci. The image intensities, representing matrix values,
vary uniformly from black to white as the matrix values change from −1 to 1. The matrix ˜F
is obtained by summing ciUi and should be identical to F
Example 3.3. Assume that we have a discrete image of size 2 × 2
F =

0.5 1
0 0.5

(3.40)
and the four orthogonal basis vectors as
U0 =

1 1
1 1

,
U1 =

−1 1
−1 1

,
U2 =

1
1
−1 −1

,
U3 =

−1
1
1 −1

which are illustrated by Fig. 3.5.
We calculate the coefﬁcients ci when F is expanded in the basis Ui as:
F =

i
ciUi
(3.41)
The space of 2 × 2 images is a Hilbert space with the scalar product given by Eq.
(3.38). The coefﬁcients are obtained according to
ci = ⟨Ui, F⟩
⟨Ui, Ui⟩
(3.42)

50
3 Discrete Images and Hilbert Spaces
yielding:
c = (2/4, 1/4, 1/4, 0)T
(3.43)
By summing the basis matrices Ui weighted with the coefﬁcients ci, F can be re-
constructed.
Tensors represent physical or geometric quantities, e.g., a force vector ﬁeld in a
solid, that are viewpoint-invariant in that they have invariant representations w.r.t. a
coordinate frame. For practical manipulations they need to be represented in a coor-
dinate system, however. The data corresponding to these representations are usually
stored as arrays having multiple indices, e.g., as ordinary vectors and matrices of lin-
ear algebra, although their representation does not critically depend on the choice of
the coordinate system more than a change of basis, the viewpoint transformation. In
other words, the numerical representation of a tensor ﬁeld given in a coordinate frame
should be recoverable from its representation in another known frame. For example,
the directions and the magnitudes of the internal forces in a body will be ﬁxed with
respect to the body, whereas the body itself and thereby the force ﬁeld can be viewed
in different coordinate systems. Once observed and measured in two different coordi-
nate systems, the two measurements of the force ﬁelds will only differ by a viewpoint
transformation and nothing else because the forces (which stayed ﬁxed relative the
body) are not inﬂuenced by the coordinate system. The principal property of the ten-
sors is that the inﬂuence of the coordinate system becomes nonessential. The tensors
look otherwise like ordinary arrays having multiple indices, e.g., matrices.
Geometric or physical quantities are characterized by the degrees of freedom they
have. For example, the mass and the temperature are scalars having zero degrees of
freedom. A velocity or a force is determined by an array of scalars that can be ac-
cessed with one index, a vector. Force and velocity accordingly have one degree of
freedom. The polarization and the inertia are quantities that are described by matri-
ces; therefore they require two indices.3 They have then two degrees of freedom. The
elements of arrays representing physical quantities are real or complex scalars and
are accessed by using 1, 2, 3, 4, etc. indices. The number of indices or the degree
of freedom is called the order of a tensor. Accordingly, the temperature, the veloc-
ity, and the inertia are tensors with the orders of 0, 1, and 2, respectively. First- and
second-order tensors are arrays with 1 and 2 indices, respectively, but an array with
1 or 2 indices is a tensor if the array elements are not inﬂuenced by the observing
coordinate system by more than a viewpoint transformation.
Tensors consitute Hilbert spaces too since their realizations are arrays. It is most
interesting to study the ﬁrst- and second-order tensors as being tensor valued pixels
in the scope of this book. That is to say, apart from the space–time indices, which
are the points where the tensor elements are either measured or computed, we will
have at most two indices representing a tensor as a pixel value. For example, in a
color image sequence there are four indices, the last of which represents the “color
vector”, which is encoded with one index. Further in the book, Sect. 9.6 and Chap.
3 There are quantities that would require more than two indices, i.e., they are to be viewed as
generalized matrices.

3.6 Tensors as Hilbert Spaces
51
U0
0
@
0
0
0
0
1
0
0
0
0
1
A
U1
0
@
0
0
0
1
0
1
0
0
0
1
A
U2
0
@
0
0
0
−1
0
1
0
0
0
1
A
U3
0
@
0
1
0
0
0
0
0
1
0
1
A
U4
0
@
1
0
1
0
0
0
1
0
1
1
A
U5
0
@
−1
0
1
0
0
0
−1
0
1
1
A
U6
0
@
0 −1
0
0
0
0
0
1
0
1
A
U7
0
@
−1
0 −1
0
0
0
1
0
1
1
A
U8
0
@
1
0 −1
0
0
0
−1
0
1
1
A
F
0
@
1
2
4
2
3
2
0
0
2
1
A
C
0
@
3
2
0
1
7/4
5/4
−1 −3/4 −1/4
1
A
˜F
0
@
1
2
4
2
3
2
0
0
2
1
A
Fig. 3.6. The discrete image F can be expanded as the sum of the basis vectors U0, U1,
· · · , and U8 weighted with the expansion coefﬁcients that are placed in the matrix C in a
row major fashion, i.e., c0 = C(1, 2), c1 = C(1, 3),c2 = C(1, 3), c3 = C(2, 1)
· · · ,
etc. The image intensities, representing matrix values, vary uniformly from black to white as
the matrix values change from −1 to 1, except in the bottom row, where the gray range from
black to white represents the interval [−4, 4]. The matrix ˜F is obtained by summing ciUi and
should be identical to F
10, we will discuss the discrete local spectrum and the structure tensor at each point
of the image. These pixels will have arrays with two indices as values, tensor values.
Tensors constitute Hilbert spaces for which addition, scaling, and scalar prod-
uct rules follow the same rules as arrays with multiple indices. The zeroth and the

52
3 Discrete Images and Hilbert Spaces
ﬁrst-order tensor scalar products are therefore the same as those of the scalars and
the vectors of linear algebra. Not surprisingly, the second-order tensors can also be
equipped with a scalar product. We restate these results for the ﬁrst- and second-order
tensors.
Lemma 3.3. With the following scalar products
⟨u, v⟩=

k
u(k)∗v(k)
(3.44)
⟨A, B⟩=

kl
A(k, l)∗B(kl)
(3.45)
where k, l run over the components of the tensor indices, the tensors up to the second-
order having the same dimension constitute Hilbert spaces.
♦
Example 3.4. We attempt to expand 3×3 images in an orthogonal basis with matrix
elements having ±1 or 0, in an analogous manner as in Example 3.3. We note, how-
ever, that it is now more difﬁcult to guess orthogonal matrices. We suggest “guess-
ing” but only three variables instead of 9, by using the following observation.
Consider the vectors u(i) in E3:
u0 = (
0, 1, 0)T
u1 = (−1, 0, 1)T
u2 = (
1, 0, 1)T
which are orthogonal in the sense of the ordinary scalar product of ﬁrst-order tensors,
i.e.,
⟨ui, uj⟩= uT
i uj = 0
for
i ̸= j
(3.46)
By using the following product based on the ordinary matrix product rule in linear
algebra, tensor product, we can construct 3 × 3 matrices Uk that are orthogonal
second-order tensors in the sense of the above scalar product (Eq. 3.45).
Uk = uiuT
j
(3.47)
That the matrices Uk are orthogonal to each other follows by direct examination:
⟨uiuT
j , ui′uT
j′⟩=

m

n
ui(m)uj(n)ui′(m)uj′(n)
(3.48)
= (

m
ui(m)ui′(m))(

n
uj(n)uj′(n))
(3.49)
= 0
(3.50)
when (i, j) ̸= (i′, j′). Consequently, we can expand all 3 × 3 real matrices by using
the scalar product in Eq. (3.45) and the basis Uk.

3.7 Schwartz Inequality, Angles and Similarity of Images
53
In Fig. 3.6, we show the nine basis vectors obtained via Eq. (3.47), and the re-
construction coefﬁcients of the example matrix:
F =
⎛
⎝
1
2
4
2
3
2
0
0
2
⎞
⎠
(3.51)
The matrix is expanded in this basis and is reconstructed by a weighted summation
of the same basis with the weighting coefﬁcients ci.
We generalize the result of the example as a theorem.
Theorem 3.1. Let ui ∈EN be orthogonal to each other, i.e., ⟨ui, uj⟩= uT
i uj = 0
for i ̸= j. Then the (tensor) products of these, Uk = uiuT
j , are also orthogonal to
each other, i.e., ⟨Uk, Ul⟩= 
mn Uk(m, n)Ul(m, n) = 0 when k ̸= l.
We can construct images where image values are not gray values but tensors.
A gray image can thus be viewed as a ﬁeld with zero-order tensors. A color image
can be viewed as a ﬁeld having ﬁrst-order tensors as values. In Sect. 10.3 we will
discuss the direction as a second-order tensor. An image depicting the local direction
will thus be a ﬁeld of second-order tensors. Evidently, even ﬁelds of tensors are
Hilbert spaces as they are multi-index arrays, which are Hilbert spaces. We restate
this property for the sake of completeness.
Lemma 3.4. With the following scalar products:
⟨A, B⟩=

ij

k
A(i, j, k)∗B(i, j, k)
(3.52)
⟨A, B⟩=

ij

kl
A(i, j, k, l)∗B(i, j, k, l)
(3.53)
(where k, l run over the components of the tensor indices, and i, j run over the dis-
crete points of the image) tensor ﬁelds up to the second degree having the same
dimension constitute Hilbert spaces.
♦
3.7 Schwartz Inequality, Angles and Similarity of Images
We explore here the concept of “angle” which we already met, at least verbally in
connection with the “orthogonality” which has visual conotations with the right an-
gles. For this too, we need the scalar product. First, we present an important inequal-
ity, known as the Schwartz inequality.
Theorem 3.2 (Schwartz inequality I). The Schwartz inequality,
|⟨u, v⟩| ≤∥u∥∥v∥
(3.54)
holds for Hilbert spaces.
♦

54
3 Discrete Images and Hilbert Spaces
Exercise 3.6. Prove that the Schwartz inequality holds for arrays with one or two
indices.
The theorem says
|⟨u, v⟩|
∥u∥∥v∥≤1
(3.55)
which in turn yields that
−1 ≤⟨u, v⟩
∥u∥∥v∥≤1
(3.56)
When the dimension of the vector space is large (N > 3) the concept of angle is
difﬁcult to imagine. The following deﬁnition helps to bridge this difﬁculty. The idea
is to use the ampliﬁcation factor that is needed to turn the Schwartz inequality into
an equality to pinpoint the angle between two vectors. This is possible because the
ampliﬁcation factor is always in the interval [−1, 1], as shown above.
Deﬁnition 3.5. The angle ϕ between the two vectors u and v is determined via
cos ϕ = ⟨u, v⟩
∥u∥∥v∥
(3.57)
Angle computation requires a scalar product. If we have a Hilbert space, however, it
comes with a method that produces the scalar product.
The Schwartz inequality, Eqs. (3.54-3.57), with a suitable scalar product e.g.,
(3.44)–(3.45) or (3.52)–(3.53) are extensively used in machine vision. In particular
Eq. (3.57) is known as the normalized correlation, where U is a template or an image
of an object that is searched for, and V is a test image which is usually a subpart of an
image that is matched with the template. The direction cosine between the template
and the test vector is measured by the expression in Eq. (3.57). When the two patterns
are vectors that share the same direction, then the system decides for a match. This
angular closeness is often tested by checking whether the absolute value of Eq. (3.57)
is above a threshold, which is empirically decided e.g., by inspecting the image data
manually or by using statistical tools such as a classiﬁer or neural network.
Example 3.5. Consider the photograph of the text shown in Fig. 3.7. The digital pho-
tographs of pages of books or manuscripts such as this can be conveniently stored
in computers. Yet, to let computers process the stored data, e.g., to search for a par-
ticular word in such images, demands that the letters and words be located and rec-
ognized automatically by computers themselves. The goal is to automatically locate
and identify all characters of the relevant alphabet in digital pictures of text. This is
known as the optical character recognition (OCR) problem in image analysis. We use
the Schwartz inequality to construct a simple OCR system for the displayed script as
summarized next.
Let U be the character to be searched for, and V be the test image that is any
subimage of Fig. 3.7 having the same size as U. The angle ϕ between U and V is
determined via the equation

3.7 Schwartz Inequality, Angles and Similarity of Images
55
Alaph
0
0.2
0.4
0.6
0.8
1
Alaph
Mim
Tau
Fig. 3.7. On the top, a digital photograph of a text (Aramaic), in which individual characters
are to be automatically identiﬁed, is displayed. In the middle, the directional cosines between
the template of a character (alaph) and the local images is shown in color. At bottom, the
identiﬁcation result is shown for 3 letters where the same color represents the same letter
cos(ϕ) =
⟨U, V⟩
∥U∥· ∥V∥
(3.58)
Because the values in the matrices are greater than or equal to zero, the interval
for cos(ϕ) will be [0, 1]. In the case when the scalar product ⟨U, V⟩equals zero,
the angle ϕ will be 90 degrees (orthogonal), i.e., least similar. By contrast, if they
share direction, then we will have V = αU, where α is a positive scalar. Because
⟨U, U⟩= ∥U∥2,

56
3 Discrete Images and Hilbert Spaces
cos(ϕ) = ⟨U, aU⟩/(∥U∥∗∥aU∥) = a⟨U, U⟩/(a∥U∥2) = 1,
(3.59)
to the effect that V is an ideal match if cos(ϕ) is 1.
Since all occurrences of U in the manuscript must be tested, the norm has to be
calculated for all (same size) subimages in the picture of the manuscript. The result
of Eq. (3.58) is illustrated in Fig. 3.7 by showing the automatically found positions of
three different characters, “alaph”, “mim”, and “tau”. The more “redish” an image,
the closer cos(ϕ) is to 1 at that location (middle). As example, two positions of
“Alaph” character are indicated.
With the same threshold of 0.925 applied to the three directional cosine images
(only one is shown in Fig. 3.7), all locations of the three individual characters could
be found and marked with respective colors. An “alaph” template painted the black
pixel values in the original image as red, if the directional cosine corresponding to the
template center was above the threshold. A similar procedure was applied to other
two templates to obtain the result at the bottom.
A2
A3
A2
A1 0.844 0.778 0.826
Exercise 3.7. We compared the four images in Fig. 3.4 by computing the directional
cosines Eq. (3.57) between the image A1 and the images A2-A4. We obtained the
directional cosines, listed above.Would you prefer the directional cosines (i.e., the
Schwartz inequality ratios) to the triangle inequality ratios to measure similarities
between images?
HINT: Use average relative similarties in your judgement.
In summary, if we have a Hilbert space then we have a vector space on which we
have a scalar product. The existence of a scalar product always allows us to derive
a useful norm, the L2 norm. In turn, this norm allows to measure distances between
points in the Hilbert space. Furthermore, we can also measure the angles between
vectors by a scalar product.

4
Continuous Functions and Hilbert Spaces
We can reconstruct as well as decompose a discrete signal by means of a set of dis-
crete signals that we called the basis in the previous chapter. Just as a vector in E3
can be expressed as a weighted sum of three orthogonal vectors, so can functions
deﬁned on a continuous space be expressed as a weighted sum of orthogonal basis
functions with some coefﬁcients. This will be instrumental when modeling nondis-
crete images. Examples of nondiscrete images are the images on photographic pa-
pers or ﬁlms. Tools manipulating functions deﬁned on a continium rather than on
a discrete set of points are necessary because without them, understanding and per-
forming many image processing operations would suffer, e.g., to enlarge or reduce
the size of a discrete image. In this chapter we deliberately chose to be generic about
functions as vector spaces. We will be more precise about the function families that
are of particular interest for image analysis in the subsequent chapters. First, we will
need to introduce functions, which are abstractions that deﬁne a rule, as points in
a vector space. Then, we deﬁne addition and scaling to establish that functions are
vector spaces. To make a Hilbert space of the function vector space we need a scalar
product, the generic form of which will be presented in the subsequent section. Fi-
nally, we present orthogonality and the angle concept for nondiscrete images.
4.1 Functions as a Vector Space
If one scrutinizes the theory of Hilbert space that we have studied so far, one con-
cludes that there are few difﬁcult assumptions hindering its extension to cover quan-
tities other than arrays of discrete scalars. Continuous functions are quantities that we
will study in the framework of vector spaces. A major difference between the thus
far discussed arrays and the function spaces is that the latter are abstract quantites
that are continuous. A photograph is a continuous function deﬁned on an ordinary
plane. In that sense it is called a continuous image. Its discrete version, a digital im-
age, is deﬁned on a set of points on the same 2D plane. The continuous photograph
assumes many more values because it is deﬁned even between the set of points where
the discrete image is undeﬁned.

58
4 Continuous Functions and Hilbert Spaces
As in the case of E3, we should get used to the idea that a function, e.g., f(t),
is a point in the function space. This is probably the hardest part of all, since many
students do not have experience other than to imagine functions as curves placed
over the x-axis or surfaces placed over an (x, y)-plane and so on. So, we need to
imagine that the entire graph is a point among its likes in a function space. This idea
is transmitted by writing a function as f, without its argument(s), which also serves
to keep from becoming distracted by the (t) or whatever function arguments that
usually follow f. If one has a cosine or arctan function, for example, these will be
points in an appropriate function space. Observe that we are not changing the fact
that a function is an abstract rule that tells how to produce a function value (e.g., the
number that a function delivers) when coordinate arguments are given to it. When
one has a unique rule (a function), then one has a unique point (in a vector space
of functions to be discussed below) that corresponds to that function. If every point
is a function then the origin of the function space must also be a function, which is
perfectly true. The origin of the function space, the null function, is a function that
represents a unique rule: “deliver the function value zero for all arguments”. What is
more, it is contained in all vector spaces of functions!
4.2 Addition and Scaling in Vector Spaces of Functions
The next step is to have a way to obtain a new “point” (function) by a rule of scaling,
function scaling. This will yield a different function (rule) than the one we started
with. Since we are looking for a rule, we have to deﬁne it as such and assign a new
function symbol to it. Let us call it g:
g = αf
(4.1)
We know g if we know a rule how to obtain its value when an argument is presented
to it. Since we know the rule for f, (this is f(t)), the rule to obtain the value of g can
be simply deﬁned as to multiply the function value of f at t with the scalar α:
g(t) = αf(t)
(4.2)
This is, of course, the way we are used to multiplying functions with scalars from
calculus. But now we have additionally the interpretation that this is an operation
making the corresponding vector “longer”.
We also need a way to add two vectors in the function spaces. Function addition
is also deﬁned in the old fashion: The new rule h,
h = f + g
(4.3)
is obtained by using the already known rules regarding f and g:
h(t) = f(t) + g(t)
(4.4)

4.4 Orthogonality
59
4.3 A Scalar Product for Vector Spaces of Functions
Finally, we need to introduce a scalar product of functions:
⟨f, g⟩=

f ∗g
(4.5)
where the ·∗on the top of g represents the complex conjugate operation. This deﬁ-
nition resembles the scalar product for CN, see Eq. (3.27). Instead of a summation
over a discrete set, we integrate now over a continuum. The integration is taken over
a suitable deﬁnition domain of the functions f and g, that is in practice a suitable
range of the arguments of f and g. Integral operation can be seen as a “degenerate”
form of summation when we have so many terms to sum that they are uncountably
inﬁnite. The idea of uncountably inﬁnite needs some more explanation. Integers that
run from 0 to ∞are, as an example, inﬁnite in number, yet they are countable (nam-
able). That is, there is no integer between two successive integers since all integers
can be named one after the other. By contrast, the real numbers are inﬁnite and un-
countable, because there is always another real number between two real numbers,
no matter how close they are chosen in any imaginable process that will attempt to
name them one after the other. So if we want to make summations over terms that
are generated by real number “indices” (instead of integer indices), we need integra-
tion to sum them up because these “indices” are uncountably inﬁnite. It is possible
to deﬁne other scalar products as well, but for most signal analysis applications Eq.
(4.5) will be a useful scalar product.
We can see that if an auto–scalar product of a vector in the Hilbert space is taken
then the result is real and nonnegative. To be more precise, ⟨f, f⟩≥0, and equality
occurs if and only if f = 0.
4.4 Orthogonality
With a scalar product we have the tool to test if functions are “orthogonal” in the
same way as before. Two functions are said to be orthogonal if
⟨f, g⟩= 0
(4.6)
We can reconstruct and analyze a signal by means of a set of orthogonal basis
signals. Just as a vector in E3 can be expressed as a weighted sum of three orthog-
onal vectors, so can a function be expressed as a weighted sum of orthogonal basis
functions with some “correctly” chosen coefﬁcients. But in order to do that, we will
need to be more precise about the Hilbert space, i.e., about our scalar product and
functions that are allowed to be in our function space. In Chap. 5 we will give further
details on these matters.

60
4 Continuous Functions and Hilbert Spaces
4.5 Schwartz Inequality for Functions, Angles
The Schwartz inequality in Sect. 3.7 was deﬁned for conventional matrices and vec-
tors. An analogue of this inequality for function spaces yields the following result:
Theorem 4.1 (Schwartz inequality II ). The Schwartz inequality
|⟨f, g⟩| ≤∥f∥∥g∥
(4.7)
where
⟨f, g⟩=

f ∗g
(4.8)
holds for functions.
♦
Exercise 4.1. Prove that the Schwartz inequality holds even for functions.
Dividing both sides of inequality (4.7) and subsequently removing the magnitude
operator yields
|⟨f, g⟩|
∥f∥∥g∥≤1
⇔
−1 ≤
⟨f, g⟩
∥f∥∥g∥≤1
(4.9)
Accordingly, we can deﬁne an “angle” ϕ between the functions f and g as follows:
cos(ϕ) =
⟨f, g⟩
∥f∥∥g∥
(4.10)
because the right-hand side of the equation varies continuously between −1 and 1 as
f/∥f∥changes. Notice that cos ϕ = 1 exactly when f/∥f∥equals g/∥g∥, whereas
it equals −1 when f/∥f∥equals −g/∥g∥.

5
Finite Extension or Periodic Functions—Fourier
Coefﬁcients
In this section we will make use of Hilbert spaces being a vector space in which a
scalar product is deﬁned. We will do this for a class of functions that are powerful
enough to include physically realizable signals. The scalar product for this space will
be made precise and utilized along with an orthogonal basis to represent the member
signals of the Hilbert space by means of a discrete set of coefﬁcients.
5.1 The Finite Extension Functions Versus Periodic Functions
We deﬁne the ﬁnite extension functions and the periodic functions as follows:
Deﬁnition 5.1. A function f is a ﬁnite extension FE function if there exists a ﬁnite
real constant T such that
t /∈[−T
2 , T
2 ]
⇒
f(t) = 0
(5.1)
A function f is a periodic function if there is a positive constant T, called a period,
such that f(x) = f(x + nT) for all integers n.
The deﬁnitions for higher dimensions are analogous. Examples of ﬁnite extension
signals include (i) a sound recording in which there is a time when the signal starts
and another when it stops; (ii) a video recording which has, additionally, a space lim-
itation representing the screen size; (iii) an (analog) photograph, which has a bound-
ary outside of which there is no image. A periodic signal is a signal, that repeats
copies of itself in a basic interval that is given by a ﬁxed interval T. Because of this
all real signals that are ﬁnite extension signals can be considered to be equivalent to
periodic signals as well. The FE signals can be repeated to yield a periodic function,
and periodic functions can be truncated such that all periods but one are set to zero.
Accordingly, a study of periodic functions is also a study of FE functions, and vice
versa. In this chapter we will use f to mean either ﬁnite extension or periodic.
Arguments of periodic functions are often measured or labelled in angles because
after one period the function is back at the same point where it started, like in the

62
5 Finite Extension or Periodic Functions—Fourier Coefﬁcients
trigonometric circle that represents the angles from 0 to 2π. This can always be
achieved by stretching or shrinking the argument of the periodic function f(t) to
yield t′ = 2π
T t. In terms of physics, t′ becomes a “dimensionless” quantity since it is
an angle. The quantity
ω1 = 2π
T
(5.2)
serves the goal that t′ obtains values in [0, 2π] when t is assigned values in [0, T].
Theorem 5.1. The space of periodic functions that share the same period is a Hilbert
space with the scalar product:
⟨f, g⟩=

T
2
−T
2
f ∗g
(5.3)
♦
This is the scalar product that we have seen before, except that it has been given a
precision with respect to function type and the integration domain. It is straightfor-
ward to extend it to higher dimensions where the integration domains will be squares,
cubes, and hypercubes.
5.2 Fourier Coefﬁcients (FC)
Since there are many types of orthogonal functions that are periodic, one speaks of
orthogonal function families. A very useful orthogonal function family is the com-
plex exponentials:
ψm(t) = exp(iωmt)
(5.4)
where
ωm = m2π
T
and,
m = 0, ±1, ±2, ±3 · · ·
(5.5)
In other words for each ωm, we have a different member of the same family. The
function family is then
{ψm}m = {· · · , e−i2ω1t, e−iω1t, 1, eiω1t, ei2ω1t, · · · }
(5.6)
This is also called the Fourier basis, because the set acts as a basis for function spaces
and deﬁnes the Fourier transform, Sect. 6.1. Here, we discuss the Fourier series as
an intermediary step.
We can verify that the members of the Fourier basis are orthogonal via their
mutual scalar products. Assuming that m ̸= n, we can ﬁnd the primitive function to
the exponential easily and obtain

5.2 Fourier Coefﬁcients (FC)
63
⟨ψn, ψm⟩=

T
2
−T
2
exp(−inω1t) exp(imω1t)dt
=

1
i(m −n)ω1
exp(i(m −n)ω1t)
t= T
2
t=−T
2
=
1
i(m −n)ω1
[exp(i(m −n)ω1T) −exp(i0)] = 0
(5.7)
where m −n ̸= 0. Now we also need to ﬁnd the norms of these basis vectors, and
therefore we assume that m = n.
⟨ψm, ψm⟩= ∥Ψm∥2 =

T
2
−T
2
exp(−imω1t) exp(imω1t)dt
=

T
2
−T
2
1dt = T
(5.8)
The norm of the complex exponential is consequently independent of m. That is, any
member of this orthogonal family has the same norm as the others. This result is not
obvious, but thanks to the scalar product we could reveal it conveniently. Accord-
ingly, we obtain:
⟨ψn, ψm⟩= Tδ(m −n)
(5.9)
where δ is the Kronecker delta, see Eq. (3.37). Since complex exponentials are or-
thogonal and we know their norms, any function f that has a limited extension T can
be reconstructed by means of them as:
f(t) =

m
cm exp(iωmt)
(5.10)
where cms are complex-valued scalars. An arbitrary coefﬁcient F(n) can be deter-
mined by taking the scalar product of both sides of Eq. (5.10) with exp(iωnt):
⟨exp(iωnt), f⟩= ⟨exp(iωnt),

m
cm exp(iωmt)⟩
=

m
cm⟨exp(iωnt), exp(iωmt)⟩
=

m
cmTδ(m −n) = cnT
(5.11)
yielding,
cn = 1
T ⟨exp(iωnt), f⟩= 1
T

T
2
−T
2
f(t) exp(−iωnt)dt
(5.12)
Consequently, cns are the projection coefﬁcients of f on the Fourier basis. They are
also known as the Fourier coefﬁcients (FCs). We summarize this result in a slightly
different way in the next theorem, to facilitate the derivation of the Fourier transform
in Sect. 6.1.

64
5 Finite Extension or Periodic Functions—Fourier Coefﬁcients
Theorem 5.2 (FC I). There exists a set of scalars F(ωm) which can synthesize a
function f(t) having the ﬁnite extension T,
F(ωm) = 1
2π

T
2
−T
2
f(t) exp(−iωmt)dt
(Analysis)
(5.13)
such that
f(t) = 2π
T

m
F(ωm) exp(iωmt)
(Synthesis)
(5.14)
using ωm = m 2π
T and m = 0, ±1, ±2, ±3 · · · .
♦
Exercise 5.1. Prove the theorem.
HINT: Expand the function T
2πf in the Fourier basis.
While this is nothing but a convenient way of computing projection coefﬁcients
in a function space where the functions are interpreted as points, few formulas have
had as much practical impact on science as these two, including signal analysis,
physics, and chemistry. Equations (5.13-5.14) are the famous Fourier1 series. They
will be generalized further below to yield the usual Fourier transform on [0, ∞].
These formulas can be seen as a transform pair. In that sense the coefﬁcients:
cm = 2π
T F(ωm)
(5.15)
constitute a unique representation of f, and vice versa.
It is worth noting that, per construction, the complex exponentials also constitute
a “basis” for sine and cosine functions whose periods exactly ﬁt the basic period
one or more (integer) times. We can sometimes ﬁnd the projection coefﬁcients even
without integration since FCs are unique. We can, for example, write the sine and
cosine as weighted sums of complex exponentials by using the Euler formulas:
cos(ω1t) = 1
2 exp(iω1t) + 1
2 exp(−iω1t) = 1
2ψ1 + 1
2ψ−1
(5.16)
sin(ω1t) = 1
2i exp(iω1t) −1
2i exp(−iω1t) = 1
2iψ1 −1
2iψ−1
(5.17)
The scalars in front of each orthogonal function ψ· are then the projection coef-
ﬁcients cms, very much like the coordinates of the ordinary vectors in E3. They are
also referred to as coordinates.
Exercise 5.2. Show that ⟨cos(ω1t), eiω1t⟩/T = 1
2
1 J. B. J. Fourier, 1768–1830, French mathematician and physicist.

5.3 (Parseval–Plancherel) Conservation of the Scalar Product
65
5.3 (Parseval–Plancherel) Conservation of the Scalar Product
We consider a scalar product between two members of our Hilbert space of periodic
functions, f = 2π
T

m F(ωm)ψm and g = 2π
T

m G(ωm)ψm :
⟨f, g⟩=
2π
T
2
⟨

m
F(ωm)ψm,

n
G(ωn)ψn⟩
=
2π
T
2 
mn
F ∗(ωm)G(ωn)⟨ψm, ψn⟩
=
2π
T
2 
mn
F ∗(ωm)G(ωn)Tδ(m −n)
= (2π)2
T

m
F ∗(ωm)G(ωm)
(5.18)
To obtain Eq. (5.18), we changed the order of the summation and the integration (of
the scalar product). This is allowed for all functions f and g that are physically real-
izable. The δ(m −n) is the Kronecker delta whereby we obtained the last equality.
Notice that the Kronecker delta reduced the double sum to a single sum by replac-
ing n with m everywhere before it disappeared, a much appreciated behavior of δ
under summation. We explain the reason for this “sum-annihilating” property of the
Kronecker delta. Assume that we have written down all the terms one after the other,
F ∗(ω1)G(ω1)δ(1 −1),
F ∗(ω1)G(ω2)δ(1 −2),
F ∗(ω1)G(ω3)δ(1 −3) · · ·
(5.19)
The only terms that are nonzero are those when both indices are equal, n = m.
Accordingly, we can reach all terms that are nonzero in the sum by using a single
index in a single sum. Behaviorally, this is the same as saying that δ “replaces” one
of its indices in every other terms such that the argument of the δ becomes zero and
it “erases” the sum corresponding to the disappeared index, before vanishing itself.
The set of the discrete Fourier coefﬁcients
{F(ωm)}m = (· · · F(ω−2), F(ω−1), F(ω0), F(ω1), F(ω2) · · · )
(5.20)
can be viewed as a vector with an inﬁnite number of elements. Members constitute a
faithful and unique representation of f because of the synthesis formula, Eq. (5.14).
Can such inﬁnite dimensional vectors be considered a Hilbert space on their own ac-
count? Yes, indeed. Scaling and addition are extended versions of their counterparts
in ﬁnite dimensional vector spaces:
(· · · αF(ω−1), αF(ω0), αF(ω1), · · · ) = α(· · · F(ω−1), F(ω0), F(ω1), · · · )
(5.21)
and
(· · · F(ω−1) + G(ω−1), F(ω0) + G(ω0), F(ω1) + G(ω1), · · · )
= (· · · F(ω−1), F(ω0), F(ω1), · · · ) + (· · · G(ω−1), G(ω0), G(ω1), · · · )

66
5 Finite Extension or Periodic Functions—Fourier Coefﬁcients
As scalar product, the sum
⟨{F(ωm)}m, {G(ωn)}n⟩=

m
F ∗(ωm)G(ωm)
(5.22)
which we already obtained from Eq. (5.18), can be utilized. That this is a scalar
product is readily veriﬁed, given the sum deﬁning the scalar product converges. In
other words, the more terms used in the sum, the closer it will get to the one and same
converged quantity. Convergence is not a problem for physically realizable signals.
Using Eq. (5.18), we state the Parseval–Plancherel theorem for the FC transform:
Theorem 5.3 (Parseval–Plancherel FC). The scalar products are conserved under
the FC transform
⟨f, g⟩= (2π)2
T
⟨{F(ωm}m, {G(ωn}n⟩
(5.23)
♦
As a special case, we obtain the conservation of the norms (Energies).
∥f∥2 = (2π)2
T

m
|F(ωm)|2 = (2π)2
T
∥{F(ωm)}m∥2
(5.24)
This is analogous to the 3D Euclidean space where the angle between two vectors is
independent of the particular orthogonal coordinate system in use. We will discuss
this further in Sect. 10.3, because in image analysis the interpretation of this theorem
has important applicatons.
It can be shown that the function series in Eq. (5.14) always converges for the
functions f that we will be interested in (physically realizable functions that have
ﬁnite extension) so that the sum on the right side always produces the function value
on the left side at all points t (strong convergence, or pointwise convergence). This
implies that the Fourier coefﬁcients constitute a decreasing sequence:
lim
m→∞F(ωm) = 0
(5.25)
Accordingly, the sum in equation Eq. (5.14) can be truncated after a ﬁnite number
of terms without much harm to the original signal. This trick is often used in ap-
plications such as image enhancement or image compression, since the high-index
Fourier coefﬁcients correspond to complex exponentials (sinusoids) with high spa-
tial frequencies that are likely to be noise. We will even use it to justify the theory of
a fully discrete version of the FC transform, DFT, in Sect. 6.3

5.4 Hermitian Symmetry of the Fourier Coefﬁcients
67
5.4 Hermitian Symmetry of the Fourier Coefﬁcients
The Fourier series equations in Eqs. (5.14) and Eq. (5.13) are also valid for functions
f that are complex-valued, since our scalar product can cope with complex functions
from the beginning. However, the signals of the real world are real, even if humans
put two real signals together and interpret them as the real and the imaginary parts
of a complex-valued signal, e.g., the electromagnetic signals. Yet the Fourier coefﬁ-
cients of real signals as well as complex signals are complex-valued. Since there is
much more information in a complex signal than in a real signal, there must be some
redundancy in the Fourier coefﬁcients of the real signals. Here we will bring further
precision to this redundancy.
A real signal is the complex conjugate of itself since the imaginary part is zero,
f = f ∗
(5.26)
The Fourier series reconstruction of the signal f yields
f(t) =

m
F(ωm) exp(imω1t)
(5.27)
so that its complex conjugate becomes
f(t)∗=

m
F ∗(ωm) exp(−imω1t)
(5.28)
Replacing the index m with −m, we obtain
f(t)∗=
−∞

m=∞
F ∗(ω−m) exp(imω1t)
(5.29)
In doing so, notice that the summation order of m has been reversed, i.e., m runs
now from positive to negative integers. But because the relative signs of m in the
summands are unchanged, i.e., m in F(ωm) with respect to the one in exp(imω1t),
we have the same terms in the sum as before. However, summing in one direction or
the other does not change the total sum, so we are allowed to reverse the direction of
the summation back to the conventional direction:
f(t)∗=
∞

m=−∞
F ∗(ω−m) exp(imω1t)
(5.30)
Accordingly, the Fourier coefﬁcients of f ∗will fulﬁll
f ∗= {F ∗(ω−m)}m
(5.31)
Now using Eq. (5.26) yields f = f ∗to the effect that
F(ωm) = F ∗(ω−m)
(5.32)

68
5 Finite Extension or Periodic Functions—Fourier Coefﬁcients
because the Fourier basis ψm is an orthogonal basis set yielding unique FCs. This
property of the FCs, which is only valid for real functions f, is called Hermitian sym-
metry. Because of this, the Fourier coefﬁcients of a real signal contains redundancy.
If we know a coefﬁcient, say F(ω17) = 0.5 + i0.3, then the mirrored coefﬁcient is
locked to its conjugate, i.e., F(ω−17) = 0.5 −i0.3 .
The coefﬁcient F(ωm) (as well as F ∗(ω−m) ) is complex and can be written in
terms of its real and imaginary parts
F(ωm) = ℜ(F(ωm)) + iℑ(F(ωm))
(5.33)
so that
F ∗(ω−m) = ℜ(F ∗(ω−m)) + iℑ(F ∗(ω−m)) = ℜ(F(ω−m)) −iℑ(F(ω−m))
Thus, Eq. (5.32) can be rewritten as
ℜ(F(ωm)) = ℜ(F(ω−m)),
and
ℑ(F(ωm)) = −ℑ(F(ω−m)).
(5.34)
We summarize our ﬁnding on the redundancy of the FC coefﬁcents of f as follows.
Theorem 5.4. The Fourier coefﬁcients of a real signal have Hermitian symmetry:
F(ωm) = F ∗(ω−m)
(5.35)
so that the real and imaginary parts of their FCs are even and odd, respectively,
whereas their magnitudes are even.
♦

6
Fourier Transform—Inﬁnite Extension Functions
In the previous chapter we worked with ﬁnite extension functions and arrived at
useful conclusions for signal analysis. In this chapter we will lift the restriction on
ﬁniteness on the functions allowed to be in the Hilbert space. As a result, we hope to
obtain similar tools for a wider category of signals.
6.1 The Fourier Transform (FT)
If f is not a ﬁnite extension signal, we still would like to express it in terms of the
complex exponentials basis via the synthesis Eq. (5.14), and the analysis Eq. (5.13)
formulas. To ﬁnd a workaround, we will express a part of such a function in a ﬁnite
interval and then let this interval grow beyond all bounds. To be precise, we will
start with the ﬁnite interval [−T/2, T/2], and then let T go to inﬁnity (Fig. 6.1)
Synthesizing the function f on a ﬁnite interval by complex exponentials amounts to
setting the function values to zero outside the interval, knowing that we will only
be able to reconstruct the signal inside of it. Outside of [−T/2, T/2], the synthesis
delivers a repeated version of the (synthesized) function inside the interval. We call
the function restricted to the ﬁnite interval fT , to put forward that it is constructed
from a subpart of f, and that it converges to f when T approaches, ∞:
f = lim
T →∞fT
(6.1)
The symmetric interval is a trick that will enable us to control both ends of the in-
tegration domain with a single variable T. By sending T to inﬁnity both ends of the
integration domain will approach inﬁnity while the function fT will be replaced by
f.
We ﬁrst restate our scalar product, Eq. (5.3), for convenience:
⟨f, g⟩=
 T/2
−T/2
f ∗g
(6.2)
The vector space of periodic functions or the space of FE functions having the ex-
tension T constitutes a Hilbert space, with the above deﬁnition of the scalar product.

70
6 Fourier Transform—Inﬁnite Extension Functions
T/2
−T/2
 T →∞
→
←
 fT→ f
Fig. 6.1. The graph drawn solid illustrates fT . As T grows beyond every bound, fT will
include the dashed graph and will equal f
Exercise 6.1. Show that (i) complex exponentials are orthogonal under any scalar
product taken over an interval with the length of the basic period. (ii) The norms of
the complex exponentials are not affected by the shift of the interval over which the
scalar product is taken. (iii) Show the results in (i) and (ii) for 2D functions, i.e.,
C exp(iωxx + iωyy).
Remembering that
ωm −ωm−1 = m2π
T −(m −1)2π
T = 2π
T
(6.3)
we restate the synthesis formula of Eq. (5.14) as
fT (t) =

m
F(ωm) exp(iωmt)(ωm −ωm−1)
(6.4)
and the analysis formula of Eq. (5.13) as
F(ωm) = 1
2π
 T/2
−T/2
fT (t) exp(−iωmt)dt
(6.5)
Passing to the limit with T →∞, we observe that ωm −ωm−1 = 2π
T , which is the
distance between two subsequent elements of the equidistant discrete grid:
· · · ω−2, ω−1, ω0, ω1, ω2 · · · ,
(6.6)
approaches to zero,
2π
T = ωm −ωm−1 = Δω →0
(6.7)

6.1 The Fourier Transform (FT)
71
Under the passage to the limit, the discrete sequence of F(ωm) will approach a func-
tion F(ω) that is continuous in ω almost everywhere1. This is because at any ﬁxed
ω, F(ω) can be approximated by F(ωm) as (i) for some m, the difference between
ω −ωm will be negligible, and (ii) the analysis formula, Eq. (5.13), exists almost
everywhere when ωm changes. Thus, the synthesis formula, Eq. (6.4), approximates
the “measure/area” (integral) of the limit function F(ω) exp(iωt) better and better,
to the effect that  will be replaced by

while Δω will be replaced by dω. The anal-
ysis formula remains as an integral, but the integration domain approaches the entire
real axis, [−∞, ∞]. In consequence, fT will approximate f, the function that we
started with, ever better. Because f is integrable, the analysis formula will converge
as T increases. As mentioned, the value ωm is a constant in the analysis integral and
it can therefore be replaced by the ﬁxed point ω, yielding F(ω). We formulate these
results in the following theorem:
Theorem 6.1 (FT). Provided that the integrals exist, the integral transform pair
F(f)(ω) = F(ω) = 1
2π
 ∞
−∞
f(t) exp(−iωt)dt
(Forward) FT
(6.8)
F−1(F)(t) = f(t) =
 ∞
−∞
F(ω) exp(iωt)dω
(Inverse) FT
(6.9)
deﬁnes the Fourier transform (FT) and the Inverse Fourier transform, which relate a
pair of complex-valued functions f(t) and F(ω), both deﬁned on the domain of the
real axis. The function F is called the Fourier Transform of the function f.
♦
As their counterparts in the FC transform, the FT and inverse FT equations are
also referred to as analysis formula and synthesis formula, respectively. The pair
(f, F) is commonly referred to as the FT pair. Because the forward FT is the same
as the inverse FT except for a reﬂection in the origin, i.e., F(f)(ω) = F−1(f)(−ω),
the forward FT can be used to implement the inverse FT in practice. Reﬂection can
be implemented by reordering the result. Notwithstanding its simplicity, this obser-
vation is useful when practicing the FT and results in the following convenience.
Lemma 6.1 (Symmetry of FT). If
(f(t), F(ω))
(6.10)
is an FT pair, so is
(F(ω), f(−t))
(6.11)
The conclusions and principles established assuming the forward direction are also
valid in the inverse direction.
♦
1 Almost everywhere means everywhere except for a set of points with zero measure. See
[226] for further reading on measure and integral.

72
6 Fourier Transform—Inﬁnite Extension Functions
Fig. 6.2. The image on the right is obtained by applying the forward FT to the image on the
left two times. The red lines are added for comparison convenience
Example 6.1. First, we illustrate the symmetry of FT by showing how one would
need a simple rearrangement to make the ordinary FT a replacer of the inverse FT.
Subsequently, we use the same lemma to establish a third version of the FC theorem.
•
Figure 6.2 represents a forward FT applied to an image twice. The second FT
acts as an inverse FT, except for a reﬂection. The FT has been applied to all three
color components, in RGB.
Because of the symmetry of FT, the conclusions on FT remain valid even if the roles
of f(t) and F(ω) are interchanged. The FC theorem can therefore be formulated for
limited frequency functions.
Theorem 6.2 (FC II). There exists a set of scalars f(tm) that can synthesize a func-
tion F(ω) having the ﬁnite extension of Ω,
F(ω) = 1
Ω

m
f(tm) exp(−itmω),
(Synthesis)
(6.12)
where
f(tm) =

Ω
2
−Ω
2
F(ω) exp(itmω)dω,
(Analysis)
(6.13)
using tm = m 2π
Ω .
♦
Exercise 6.2. Prove theorem 6.2.
HINT: Expand ΩF(ω) in exp −itmω.
6.2 Sampled Functions and the Fourier Transform
Via theorem 5.2, we established that there exists a unique set of values capable of
reconstructing any f(t) by means of complex exponentials, provided that f has an

6.2 Sampled Functions and the Fourier Transform
73
extension T that is ﬁnite. This technique relies heavily on the fact that T is ﬁnite,
and therefore we can make copies of f to obtain a periodic function. This function
can in turn be expanded in Fourier series by use of a scalar product deﬁned on a
period, e.g., [0, T]. In Eq. (5.13), we obtained the discrete set of values used in the
reconstruction. We viewed these values as an equidistant sampling of a continuous
function, although we did not speculate how this function of “imagination” behaved
between the grid points. As long as the function values turn out to be the correct
values on the grid, we can always take this view, by letting the values of F between
the discrete points ωm = m 2π
T vary according to some rule of our choice.
However, theorem 6.1 (FT) states that we can reconstruct f even if it is not a ﬁ-
nite extension signal, albeit that the reconstruction now cannot be achieved by using
a discrete sequence of values F(ωm), but instead a continuous function F(ω). By
using the FT theorem we are now, in fact, capable of reconstructing f even with-
out periodizing it. Assuming that the signal f is nonzero in the symmetric interval
[−T/2, T/2], the FT of f is:
F(ω) = 1
2π
 ∞
−∞
f(t) exp(−iωt)dt = 1
2π
 T/2
−T/2
f(t) exp(−iωt)dt
(6.14)
Eq. (6.14) tells us that not only the values of F are unique on the grid ωm, but also
in the continuum between the grid points! The conclusion must be that there exists a
continuous function F(ω) that can be represented via its discrete values, also referred
to as discrete samples or samples, to such an extent that even the function values
between the discrete grid points are uniquely determined by the simple knowledge
of its discrete values on the grid ωm. In other words, the discrete samples F(ωm)
constitute not only an exact representation of the periodized original signal f, but
also an exact representation of F(ω), which is in turn an exact representation of the
original (unperiodized) function f. Notice that F(ω) is the Fourier transform of the
limited extension function f (without periodization).
How do we estimate (interpolate) F(ω) when we only know its values at discrete
points? The answer to this is obtained by substituting Eq. (5.14) in Eq. (6.14).
F(ω) = 1
2π
 T/2
−T/2
f(t) exp(−iωt)dt
= 1
2π
 T/2
−T/2
(

m
F(ωm)2π
T exp(iωmt)) exp(−iωt)dt
=

m
F(ωm)2π
T
1
2π
 T/2
−T/2
exp(−i(ω −ωm)t)dt
=

m
F(ωm) 1
T
 T/2
−T/2
exp(−i(ω −ωm)t)dt
=

m
F(ωm)μ(ω −ωm)
(6.15)
where

74
6 Fourier Transform—Inﬁnite Extension Functions
μ(ω) = 1
T
 T/2
−T/2
exp(−iωt)dt
(6.16)
Clearly F(ω) is reconstructed from its samples F(ωm) by using these as weight
functions for a series of a displaced versions of μ, the interpolation function. We can
identify μ in Eq. (6.16) as the Fourier transform of a piecewise constant function
χT (t), which is constant inside the interval where f is nonzero, and zero elsewhere.
χT (t) =

1,
if t ∈[−T
2 , T
2 ];
0,
otherwise.
(6.17)
We will refer to χT as the characteristic function of the interval [−T
2 , T
2 ]. The rela-
tionship in Eq. (6.16) along with the deﬁnition in Eq. (6.17) reveals that
μ(ω) = 1
T
 ∞
−∞
χT (t) exp(−iωt)dt = 2π
T F(χT )(ω)
(6.18)
Computing the integral in Eq. (6.16) in a straightforward manner yields
μ(ω) = 1
T
exp(−iωt)
−iω
t=T/2
t=−T/2
(6.19)
= 1
T
1
ω 2 sin(ω T
2 )
(6.20)
In consequence, deﬁning the sinc function as
sinc(ω) = sin ω
ω
(6.21)
yields the interpolator function
μ(ω) = sinc
T
2 ω

(6.22)
which, using Eq. (6.18), results in
F(χT )(ω) = T
2π sinc
T
2 ω

(6.23)
A characteristic function and its corresponding interpolation function is illustrated in
Fig. 6.3. Before we discuss the properties of the sinc functions further, we present a
few observations and elucidate the concept of functions with ﬁnite frequency exten-
sion, also known as band-limited functions.
•
The sampling interval for F is given by ωm −ωm−1 = 2π
T , so that if the distance
between the samples deﬁnes the unit measurement, ωm −ωm−1 = 1, then T’s
value is locked to T = 2π. If another measurement unit is used to quantify the
length of the interval that samples F, alternatively the length of the period of f,
an ordinary scaling of the arguments will restore the correspondence. Because of
this, it is customary to assume that a ﬁnite extension 1D function is nonzero in
[−π, π], and the sampling interval of its FT is equal to unit length.

6.2 Sampled Functions and the Fourier Transform
75
−pi
−4pi/8
−pi/8
0
pi/8
4pi/8
−0.2
0
0.2
0.4
0.6
0.8
1
−64
−56
−48
−40
−32
−24
−16
−8
0
8
16
24
32
40
48
56
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 6.3. The graph shows the ﬁnite extension of one domain that matches a particular inter-
polation function in the other domain where the domains are, interchangeably, the time and
the frequency domains. Notice that the interpolator (sinc function) crosses zero only at every
eighth integer, while its total extension (in the other domain) is 2π
8
•
The function F(ω) can be represented exactly by means of its samples on an
integer grid, if f(t) vanishes outside an interval having length T = 2π.
Functions with Finite Frequencies or Band-limited Functions
By using the symmetry of the Fourier transform, we can switch the roles of F and
f while retaining the above results. In other words, we assume F to have a limited
extension Ω, outside of which it will vanish (and therefore can be periodized). We
will refer to such functions as ﬁnite frequency (FF) functions, which are also known
as band-limited functions. Consequently, the continuous function f(t) can be syn-
thesized faithfully by means of its samples. This combination, limited extension F
in the ω-domain and discrete f in the t-domain, is the most commonly encountered
case in signal analysis applications. Evidently, the roles of μ and χ are also switched
to the effect that μ now interpolates the samples of f whereas χ deﬁnes the extension
of F. As discussed previously, the sampling interval of f equals 1 if the extension of
F is Ω = 2π.
Properties of Sinc Functions
We note that the sinc function,
sinc(t) = sin t
t
(6.24)
is a continuous function, even at the origin, where it attains the maximum value
1. In fact, it is not only continuous at the origin, but it is also analytic there (and
everywhere), meaning that all orders of its derivatives exist continuously yielding a
“smooth” function. As has been observed already, a scaled version of sinc yields the
interpolation function μ

76
6 Fourier Transform—Inﬁnite Extension Functions
μ(t) = sinc(πt) = sin( Ω
2 t)
Ω
2 it
.
(6.25)
If Ω, the extension or the support of F(ω) where it is nonzero, is an integer share
of 2π, i.e., Ω = 2π/κ for some integer κ, then the interpolator μ is a sinc function
in the t-domain with zero crosses at every κth grid point. If κ = 1, the zero crosses
will occur at every integer (except at the origin). We illustrate a χ, μ pair with κ = 8
in Fig. 6.3. As a consequence of the previous discussion, shifting the above sinc
function yields a series of functions μm,
μm(t) = μ(t −tm) = sinc(Ω
2 (t −tm)) = sin( Ω
2 (t −tm))
Ω
2 (t −tm)
(6.26)
which are capable of reconstructing any band-limited function f via
f(t) =

m
f(tm)μm(t)
(6.27)
However, it can also be shown2 that these functions {μm}∞
m=−∞are orthogonal to
each other:
⟨μm, μn⟩= 2π
Ω δ(m −n)
(6.28)
under the scalar product,
⟨f, g⟩=
 ∞
−∞
f ∗g
(6.29)
Thus, the band-limited functions constitute a Hilbert space with the above scalar
product.3 Accordingly, any band-limited signal f can be reconstructed via an or-
thogonal expansion and the scalar product from Eq. (6.29)
f(t) =

m
F(ωm)μm(t)
(6.30)
where
F(ωm) = ⟨f, μm⟩/⟨μm, μm⟩
(6.31)
In consequence, the coefﬁcients F(ωm) in Eq. (6.30) must equal to f(tm) =
f(m 2π
Ω ) appearing in Eq. (6.27)
F(ωm) = ⟨μm, f⟩
⟨μm, μm⟩= ⟨μm, f⟩
( 2π
Ω )
= f(m2π
Ω )
(6.32)
In other words, the projection coefﬁcients of a band-limited signal f onto the sinc
functions basis are the samples of the signal on the grid:
2 See Exercise 7.2.
3 This is not too surprising because just another representation of the same functions, i.e.,
their FTs, constitutes the space of limited extension functions. We have already seen that
such functions constitute a Hilbert space with a scalar product enabling their reconstruction
via FCs and the Fourier series.

6.2 Sampled Functions and the Fourier Transform
77
⟨μm, f⟩= 2π
Ω f(tm)
(6.33)
which uses no integrals, this result is a fairly simple tool to ﬁnd projection coefﬁ-
cients.
Example 6.2. We illustrate sampling and periodization by Fig. 6.4.
•
In the top, left graph, we have drawn a band-limited signal f(t). The very def-
inition of f as band-limited implies that F(ω) has a limited extension in the
spectrum, which is drawn in the top, right graph. The width of F is Ω.
•
In the middle-left graph, we have drawn f after discretization. In the middle
right graph, we have drawn the ω-domain after discretization. The discretization
in one domain is equivalent to a periodization in the other domain. Also, the
discretization step is inversely proportional to the period of the other domain.
•
In the bottom, left graph we have shown the sampled f, when we periodize F,
with a larger period than the extension of F. This periodization of F, using twice
as large an Ω as compared to the extension of F, is shown in the bottom, right
graph.
We summarize the results of this section by the Nyquist theorem.
Theorem 6.3 (Nyquist). Let f and F be a Fourier transform pair. Then sampling of
either of the functions is equivalent to periodization of the other function. To be more
precise, the following two statements are valid:
•
(Time-limited signals) If f is nonzero only in the interval [−T
2 , T
2 ], then F can
be sampled without loss of information provided that the sampling period is 2π
T
or less.
•
(Band-limited signals) If F is nonzero only in the interval [−Ω
2 , Ω
2 ] then f can
be sampled without loss of information provided that the sampling period is 2π
Ω
or less.
♦
When working with band-limited signals, the distance between the samples in
the t-domain can be assumed to be equal to 1, for convenience. Then we obtain the
critical frequency domain parameter Ω = 2π, so that the critical frequency in the
theorem yields:
Ω
2 = π
⇔
−π < ω < π
(6.34)
In signal processing literature, Ω
2 is often referred to as the Nyquist frequency. Even
its normalized version, π, is called the Nyquist frequency, because a scaling of ω
and t-domains is always achievable. The basic interval −pi < ω < π is sometimes
called the Nyquist period, or the Nyquist block.

78
6 Fourier Transform—Inﬁnite Extension Functions
Ω0
2π
Ω0
Ω0
2π
Ω0
Ω0
Fig. 6.4. The effect of sampling on a band-limited signal is a repetition of its ﬁnite extension
spectrum. On the left, the graphs are in the t-domain, on the right, they are in the ω-domain

6.3 Discrete Fourier Transform (DFT)
79
6.3 Discrete Fourier Transform (DFT)
When f is a ﬁnite extension function with the extension T, Eq. (5.14) states that f
can be periodized and synthesized by means of FCs, F(n 2π
T ), which are samples of
an inﬁnite extension function F. Albeit theoretically inﬁnite in number, the discrete
function values, F(n 2π
T ), decrease since they are FCs. As such, they must decrease,
or else the synthesis formula will not converge to the periodized version of f. Ac-
cordingly,
lim
n→∞F(n2π
T ) →0
(6.35)
is a fact for real signals and from Fourier series we obtain a periodic function which
approximates the periodized f(t) well. Consequently, when synthesizing f from its
FCs as in Eq. (5.14), there exists N such that we can ignore the F(m 2π
T ) for which
m ≥N, and still obtain a reasonably good approximation of the periodized f(t).
In accordance with the above, we will attempt to bring the concept of the FT
closer to applications, by assuming in this section that not only f has a ﬁnite exten-
sion T, but that the corresponding FCs are also ﬁnite in number, i.e., only FCs
{F(nωn)}N−1
n=0
with
ωn = n2π
T
(6.36)
are nonzero. According to Eq. (5.14), we can synthesize f(t)
f(t) =
N−1

m=0
2π
T F(ωm) exp(iωmt)
(6.37)
by means of the following N coefﬁcients:
F(ωm) = 1
2π

T
2
−T
2
f(t) exp(−iωmt)dt
(6.38)
where m ∈0, · · · N −1. Because f has a ﬁnite extension, any sampling of it will
yield a ﬁnite number of function values. If we choose to sample f at a set of tn
in such a way that the sampling interval corresponds to what the Nyquist theorem
affords us, it should be possible not to lose information. The largest sampling step
suggested by the Nyquist theorem is 2π(N 2π
T )−1 = T
N , where N 2π
T represents the
extension of {F(nωn)}N−1
n=0 . We proceed accordingly, and substitute tn in Eq. (6.37)
f(tn) =
N−1

m=0
2π
T F(ωm) exp(iωmtn)
with
tn = n T
N
(6.39)
Notice that there are exactly N samples of f(tn). Thus, it should be possible to com-
pute F(ωm) appearing in Eq. (6.39) directly from f(tn), without passing through
a continuous reconstruction of F(ω) (and then sampling it) in Eq. (6.38). We ob-
serve that the sequence of complex numbers exp(itnωm) appearing in Eq. (6.39)

80
6 Fourier Transform—Inﬁnite Extension Functions
constitutes an N-element vector4 ψn, i.e.,
ψn = (1, exp(itnω1), exp(itnω2) · · · exp(itnωN−1))T
(6.40)
or
ψn(m) = exp(itnωm),
with
m ∈0 · · · N −1.
(6.41)
There are N such vectors ψn because n ∈0 · · · N −1. Furthermore, ψn are orthog-
onal to each other under the conventional vector scalar product of Eq. (3.27). That
is,
⟨ψn, ψm⟩=
N−1

l=0
exp(i(m −n) l
N ) = Nδ(m −n)
(6.42)
where δ is the Kronecker delta, holds.
Exercise 6.3. Show that ψm are orthogonal.
HINT: Identify ⟨ψm, ψn⟩as a geometric series.
To obtain the kth element of the vector F(ωk), we compose the scalar product be-
tween the vector (f(t0, · · · , f(tN−1))T and ψk, i.e., we multiply both sides of Eq.
(6.39) with exp(−iωktn) and sum over the index n:
N−1

n=0
f(tn) exp(−iωktn) =
N−1

n=0
exp(−iωktn)
N−1

m=0
2π
T F(ωm) exp(iωmtn)
=
N−1

n=0
N−1

m=0
2π
T F(ωm) exp(−iωktn) exp(iωmtn)
=
N−1

m=0
2π
T F(ωm)
N−1

n=0
exp(−iωktn) exp(iωmtn)
=
N−1

m=0
2π
T F(ωm)
N−1

n=0
exp

−ink 2π
N

exp

inm2π
N

=
N−1

m=0
2π
T F(ωm)Nδ(k −m)
= N 2π
T F(ωk)
(6.43)
where we have used tnωm = nm 2π
N and the orthogonality of ψm as given by Eq.
(6.42). Consequently, we obtain
F(ωm) = (N 2π
T )−1
N−1

n=0
f(tn) exp(−iωmtn)
(6.44)
4 Note the typographic difference that now ψn is in boldface because it is an array or a
conventional vector, in contrast to ψn, which represents a function.

6.3 Discrete Fourier Transform (DFT)
81
This equation, together with Eq. (6.39), establishes f and F as a discrete transform
pair when sampled on the grids of tm and ωn, respectively. We state the result as a
lemma.
Lemma 6.2. Let f(t) be a ﬁnite extension function with the extension T and have N
nonzero FCs. The samples of f
f(tn) =
N−1

m=0
2π
T F(ωm) exp(iωmtn),
with
tn = n T
N ,
(6.45)
and the samples of its FT,
F(ωm) = (N 2π
T )−1
N−1

n=0
f(tn) exp(−iωmtn),
with
ωm = m2π
T ,
(6.46)
constitute a discrete transform pair.
♦
Using an analogous reasoning and theorem 6.2 we can restate this result for band-
limited functions.
Lemma 6.3. Let f(t) be a band-limited function, i.e., F(ω) has the ﬁnite extension
Ω, that has at most N nonzero samples f(tn) with tn = n 2π
Ω . The samples of f
f(tn) =
N−1

m=0
2π
TN F(ωm) exp(iωmtn),
with
tn = n2π
Ω ,
(6.47)
and the samples of its FT,
F(ωm) = (2π
T )−1
N−1

n=0
f(tn) exp(−iωmtn),
with
ωm = m Ω
N ,
(6.48)
constitute a discrete transform pair.
♦
The two lemmas can be simpliﬁed further because we are free to deﬁne T =
2π, or Ω = 2π. In either case, the transform pair can be interpreted as a ﬁnite
discrete sequence, even without reference to the sampling distance between t and
ω variables. Accordingly, both lemmas can be reduced to a dimensionless form that
only differs with where we place the transform constant 1/N. We choose to reduce
the ﬁrst lemma and give it as a theorem.
Theorem 6.4 (DFT). The discrete Fourier transform (DFT) for arrays with N ele-
ments, deﬁned as
F(n) = 1
N
N−1

m=0
f(m) exp

−imn2π
N

(6.49)

82
6 Fourier Transform—Inﬁnite Extension Functions
0
1
-N+1
2
N+2
N-1
-1
N-2
-2
0
+
+
_
Fig. 6.5. The circular topology of DFT indices illustrated for N points. An index j represents
the same point as j + nN, where n is an arbitrary integer
has an inverse transform
f(m) =
N−1

n=0
F(n) exp

imn2π
N

(6.50)
♦
The DFT or its two variants in this section are special cases of the FT. In 1961
Cooley and Tukey, [48], published an algorithm that could quickly compute the dis-
crete Fourier transform. Since then this and other algorithms for quickly computing
DFTs are collectively called FFT (Fast Fourier Transform) algorithms although there
are several variants to choose from. The FFT has found many applications because
of its efﬁciency. In many cases, it is faster to do two FFTs, a multiplication of the
resulting functions, and then to do the inverse FFT, than doing shift-invariant com-
putations, e.g., convolutions with arbitrary ﬁlters.
6.4 Circular Topology of DFT
We note that m and n on the right-hand side of the equations in theorem 6.4, do not
necessarily have to be one of the integers in 0 · · · N −1. We will investigate this issue
along with a practical way of dealing with it next.
In arrays with N elements that are DFT pairs, there can be at most N different
f(m) or F(n) values, although m, n can be any integer according to the DFT lemma.
It can be easily shown that if for any integer m, n, and k the equation
n = m + kN
(6.51)
holds, then

6.4 Circular Topology of DFT
83
f(n) = f(m),
and
F(n) = F(m).
(6.52)
Hence, the translation of indices in the time, or frequency domain must be done by
means of a circular addition, which means that n+m and n+m+kN are equivalent.
This results in a circular topology of DFT where the ﬁrst array element labelled as
0 is the nearest neighbor of the one labelled as 1 and N −1.
Although it is feasible to ﬁnd neighbors using visual workarounds for discrete
functions deﬁned on 1D-domains (Fig. 6.5), it is deﬁnitely a challenge to “visually”
ﬁnd the neighbor of a point close to grid boundaries in higher dimensions. The arith-
metic modulo function, here abbreviated as Mod(m, N), is handy in practical appli-
cations of DFT to avoid the practical difﬁculties associated with its circular topology.
The function computes the unique remainder of m after division by N; that is, the
result is always one of the integers 0 · · · N −1, e.g.,
Mod(3, 17) = Mod(−14, 17) = Mod(20, 17) = 3
(6.53)
Ordinary addition using the Mod function, i.e., Mod(m + n, N), deﬁnes a so-
called ﬁnite group so that integers that differ with integer multiples of N are equiva-
lent to each other. This allows a seamless interpretation of all translations on rectan-
gular discrete grids, in all dimensions, which is precisely what is needed to interpret
the indices of DFT correctly.
For example, when N = 17, the integers 3, −14, 20 represent the same coefﬁ-
cient
f(3) = f(−14) = f(20)
and
F(3) = F(−14) = F(20)
(6.54)
although −14 and 20 are obviously not possible to use as the indices of an array
having 17 elements, which has index labels 0 · · · 16. After an application of the mod-
ulo function, e.g., Eq. (6.53), the resulting integers can be used as indices of arrays
holding the discrete values of f and F, Eq. (6.54).
A systematic use of the Mod function in all index references, additions, and sub-
traction, known also as modulo arithmetic, enables circular translation, i.e., transla-
tions that hit no grid boundaries. This can be used to “walk” around in the DFT arrays
conveniently, which is particularly important for a correct implementation and inter-
pretation of the convolution operation by DFT, as we will study in Chap. 7. The
circular interpretation of the DFT indices on 1D domains is illustrated by Fig. 6.5
using N points (taps). Note that the ﬁrst index is 0 and the last index is N −1, and
the indices are periodic with the period N in the modulo N arithmetic.

7
Properties of the Fourier Transform
In this chapter we study the FT to reveal some of its properties that are useful in
applications. As has been shown previously, the FT is a generalization of both FC and
DFT. There is an added value, to use FT indifferently both to mean FC and to mean
DFT. To do that, however, we need additional results. The ﬁrst section contributes to
that by introducing the Dirac distribution [212]. This allows us to Fourier transform
a sinusoid, or a complex exponential, which is important to virtually all applications
of signal analysis in science and economics. Fourier transforming a sinusoid is not a
trivial matter because a sinusoid never converges to zero. In Sect. 7.2, we establish
the invariance of scalar products under the FT. This has many implications in the
practice of image analysis, especially in the direction estimation and quantiﬁcation
of spectral properties. Finally, we study the concept of convolution in the light of
FT, and close the circle by suggesting the Comb tool to achieve formal sampling
of arbitrary functions. This will automatically yield a periodization in the frequency
domain via a convolution. With the rise of computers, convolution has become a
frequently utilized tool in signal analysis applications.
7.1 The Dirac Distribution
What is the area or the integral of the sinc signal that was deﬁned in Eq. (6.21)? The
answer to this question will soon lead us to construct a sequence of functions that
will produce a powerful tool when working with the Fourier transform.
We answer the question by identifying the characteristic function of an interval
as the inverse FT of a sinc function via Eq. (6.23):
χT (t) =

1,
if t ∈[−T
2 , T
2 ];
0,
otherwise.
= F−1
 T
2π sinc(ω T
2 )

(t) =

T
2π sinc

ω T
2

exp(iωt)dω
(7.1)
Evaluating χT (0) yields the integral of a sinc function:

86
7 Properties of the Fourier Transform
T=0.5
T=1.2
T=16
Fig. 7.1. The sinc function sequence BT given by Eq. (7.3) for some T values. In blue the
(capped) function indicates that BT approaches the Dirac distribution with “∞value” at the
origin

T
2π sinc

ω T
2

dω = 1
(7.2)
Obviously, the “area under the curve”,
BT (ω) = T
2π sinc

ω T
2

(7.3)
is 1, independent of T, as long as T is ﬁnite! In fact for every T we have a different
function BT , so that we can ﬁx a sequence of Ts and study BT . This is what we will
do next.
The BT s were obtained by Fourier transforming a function that is constant in
a ﬁnite and symmetric interval [ −T
2 , T
2 ]. What happens to sinc when T approaches
inﬁnity? In other words, what is the Fourier transform of an “eternal” constant? We
restate Eq. (6.23) and study it when T increases.
F(χT )(ω) = BT (ω) = 1
2π T · sinc

T ω
2

(7.4)
Any function contracts when we multiply its argument with a large constant T. The
function
sinc(T ω
2 )
(7.5)
is accordingly a contracted version of sinc(ω) as T increases.
The sinc function is a smooth function that is both continuous and has continuous
derivatives everywhere, including at the origin. When we increase T, the sinc func-
tion contracts in the horizontal direction only. In particular, the maximum of the sinc

7.1 The Dirac Distribution
87
function remains 1, and this maximum is attained at the origin for all Ts. However,
when T increases, the values of the functions {BT (ω)}T at ω = 0 increase with T,
since sinc(T · 0) = 1. As a result, BT (ω) approaches inﬁnity at ω = 0, whereas the
effective width of the BT functions shrink to zero. Thus, the BT (ω) approaches to
something which is zero everywhere except at the origin and yet has an area which
is always 1!
That something is the so-called Dirac distribution represented by δ
lim
T →∞BT (ω) = lim
T →∞
1
2π T · sinc(ω T
2 ) = δ(ω)
(7.6)
which is shown in Fig. 7.1. Other names for the Dirac distribution are the delta
function, unit impulse, and the Dirac function, although, δ(ω) is not very useful when
considered strictly as a function. This is because the δ(ω) interpreted as a function,
will only deliver the value zero, except at the origin, where it will not even deliver a
ﬁnite value, but delivers the value “inﬁnity”.
In mathematics, the limit of the BT function is not deﬁned in terms of what it
does to the points ω, its argument, but instead what it does to a class of other func-
tions under integration. That is also where its utility arises, namely, it consistently
delivers a value as a result of the integration. Such objects are called generalized
functions, or distributions. To obtain a hint on how we should deﬁne the limit of
BT , we should thus study the behavior of BT when it is integrated with arbitrary
(integrable) functions f:
⟨BT , f⟩=

BT f(ω)dω
=

( 1
2π

χT (t) exp(−iωt)dt)f(ω)dω
=

( 1
2π

exp(−iωt)f(ω)dω)χT (t)dt
=

F(t)χT (t)dt
Thus, in the limit we will obtain
lim
T →∞⟨BT , f⟩=

F(t)dt = f(0)
(7.7)
because χT approaches to the constant function 1 with increasing T. Also, changing
the order of the integration and the limit operator is not a problem for physically
realizable functions. This shows that as T grows beyond any bound, BT will consis-
tently “kick out” the value of its fellow integrand f at origin, no matter the choice of
f. Therefore, we give the following precision to δ, the Dirac distribution.
Deﬁnition 7.1. The Dirac-δ distribution is deﬁned as
⟨δ, f⟩=

f(ω)δ(ω)dω = f(0)
(7.8)

88
7 Properties of the Fourier Transform
The argument “ω” in δ(ω) is there to mark the “hot point” of the delta distri-
bution, not to tempt us to conclude that δ is a function delivering a useful value at
the argument. For example, δ(ω −ω′) becomes inﬁnity when ω = ω′ (whereas it
vanishes elsewhere). In fact, taking a close look at the rule that deﬁnes the Dirac dis-
tribution, we note that it is not based on real numbers as arguments but on functions,
represented by f. Whereas functions are rules that deliver numbers for arguments
that are numbers, the Dirac distribution is a rule that delivers numbers for “argu-
ments” that are functions appearing in scalar products, Eq. (7.1). In plain English,
this is what the Dirac distribution does. No matter what the integration domain is,
as long as the domain contains the “hot-point” of the δ-distribution, the distribution
kicks out the value of its “fellow integrand” at the hot point. Example behaviors
include

f(x)δ(x −y)dx = f(y)
(7.9)
and
 
f(x)δ(x −y)dxdy =

f(y)dy.
(7.10)
Theorem 7.1. The Fourier transform of the constant function “1” is a Dirac distri-
bution:
F(1)(ω) = 1
2π

exp(−iωt)dt = δ(ω)
(7.11)
♦
Exercise 7.1. The proof of the theorem is obtained by Egs. (7.4) and Eq. (7.6). In
particular, provide the following:
i) Prove that the Fourier transform of the complex exponential exp(iωt) is a shifted
delta distribution.
ii) Prove that the Fourier Transform of a Dirac-δ is a constant.
iii) Create a δ in a 2D image. Compute the real part of its Fourier transform (use
DFT on a large image). Compute the imaginary part of its Fourier transform.
What are the parameters that determine the direction and the frequencies of
planar waves?
7.2 Conservation of the Scalar Product
In the case of Fourier series, we were able to show that the scalar products were
conserved between the time domain and the Fourier coefﬁcients. Now we show that
scalar product conservation is valid even for the Fourier transform. The scalar prod-
ucts are deﬁned as
⟨f, g⟩=
 ∞
−∞
f(t)∗g(t)dt
(7.12)
and

7.2 Conservation of the Scalar Product
89
⟨F, G⟩=
 ∞
−∞
F(ω)∗G(ω)dω
(7.13)
We use the inverse FT to transfer the scalar product in the time domain to the
frequency domain:
⟨f, g⟩= ⟨F−1(F), F−1(G)⟩
=

[

F(ω) exp(iωt)dω][

G(ω′) exp(iω′t)dω′]∗dt
=
  
F(ω) exp(iωt)[G(ω′) exp(iω′t)]∗dωdω′dt
=
 
F(ω)G(ω′)∗[

exp(iωt) exp(−iω′t)dt]dωdω′
=
 
F(ω)G(ω′)∗[

exp(−i(ω −ω′)t)dt]dωdω′
=
 
F(ω)G(ω′)∗2πδ(ω −ω′)dωdω′
= 2π

F(ω)G(ω)dω = 2π⟨F, G⟩
(7.14)
This establishes the Parseval–Plancherel theorem for the FT.
Theorem 7.2 (Parseval–Plancherel). The FT conserves the scalar products:
⟨f, g⟩=
 ∞
−∞
f(t)∗g(t)dt = 2π
 ∞
−∞
F(ω)∗G(ω)dω = 2π⟨F, G⟩
(7.15)
♦
As a consequence of this theorem, we conclude that FT preserves the norms of
functions.
Exercise 7.2.
i) Show that the shifted sinc functions, see Eq. (6.26), that interpolate band-limited
signals constitute an orthonormal set under the scalar product, Eq. (6.29)
HINT: Apply the Parseval–Plancherel theorem to the scalar products.
ii) There are functions that are not band-limited. What does the projection of such
a signal on sinc functions correspond to?
HINT: Decompose the signal into a sum of two components corresponding to
portions of the signal inside and outside of Ω.
iii) Can the sinc functions of Eq. (6.26) be used as a basis for square integrable
functions?
HINT: Are the square integrable functions band-limited?

90
7 Properties of the Fourier Transform
7.3 Convolution, FT, and the δ
We deﬁne ﬁrst the convolution operation, which is bilinear.
Deﬁnition 7.2. Given two signals f and g, the convolution operation, denoted by ∗,
is deﬁned as
h(t) = (f ∗g)(t) =
 ∞
−∞
f(t −τ)g(τ)dτ
(7.16)
It takes an ordinary variable substitution to show that the convolution is commutative:
h(t) = (f ∗g)(t) =
 ∞
−∞
f(t −τ)g(τ)dτ =
 ∞
−∞
f(τ)g(t −τ)dτ = (g ∗f)(t)
(7.17)
Now we study the FT of the function h = f ∗g.
h(t) = F(f ∗g)(ω) = 1
2π

exp(−iωt)[

f(t −τ)g(τ)dτ]dt
= 1
2π

g(τ)[

f(t −τ) exp(−iωt)dt]dτ
= 1
2π

g(τ)[

f(t) exp(−iω(t + τ))dt]dτ
= 1
2π

g(τ) exp(−iωτ)dτ[2π
2π

f(t) exp(−iωt)dt]
= 2πF(ω)G(ω)
(7.18)
Consequently we have the following result, which establishes the behavior of convo-
lution under the Fourier transform.
Theorem 7.3. The bilinear operator * transforms as · under the FT
h = f ∗g
⇔
H = 2πF · G
(7.19)
♦
This is a useful property of FT in theory and applications. Below we bring further
precision as to how it relates to sampled functions. Before doing that, we present
the Fourier transform of δ(x), the Dirac distribution, as a theorem for its practical
importance.
Theorem 7.4. The Dirac-δ acts as the element of unity (one) for the convolution
operation:
δ ∗f = f ∗δ = f
(7.20)
♦

7.3 Convolution, FT, and the δ
91
Convolution and the FC
To work with discrete signals, we assume now f and g are two band-limited func-
tions, both with the same frequency extension of Ω. With this assumption, f(tm) and
g(tm) are discrete versions of f and g which can also synthesize them faithfully. To
obtain relation Eq. (7.19), we did not need to make a restrictive assumption on the
extensions of F and G. In consequence, relation (7.19) holds even for band-limited
functions. The right-hand side is easy to interpret because F and G are ﬁnite ex-
tension functions that get multiplied pointwise. However, the left-hand side requires
that we are more precise on the deﬁnition of the convolution. To obtain a discrete
deﬁnition of the convolution operator that is consistent with its continuous analogue,
we pose the question as follows. What is h(t), if H = 2πFG is given? To answer
this, we synthesize F and G according to Eq. (6.12)
F(ω) =
 1
Ω
 
m
f(tm) exp(−itmω)
(7.21)
G(ω) =
 1
Ω
 
m
g(tm) exp(−itmω)
(7.22)
(7.23)
Evidently, H will have the extension Ω because F and H have the extension Ω. We
can then proceed to compute the FCs of H according to Eq. (6.13)
h(tk) = 2π( 1
Ω )2

Ω
2
−Ω
2

mn
f(tm)g(tn) exp[−i(tm + tn)ω] exp(itkω)dω
= 2π( 1
Ω )2 
mn
f(tm)g(tn)

Ω
2
−Ω
2
exp[i(tk−tm −tn)ω]dω
= 2π( 1
Ω )2 
mn
f(tm)g(tn)δ(tk −tm −tn)Ω
= 2π
Ω

n
f(tk −tn)g(tn)
(7.24)
Using the standard assumption Ω = 2π results in tk −tk−1 = 1 for any k, and yields
the following deﬁnition:
Deﬁnition 7.3. Given two discrete signals f(k) and g(k), with k being integers, the
convolution operation for such signals is denoted by ∗, and is deﬁned by:
h(k) = (f ∗g)(k) =

n
f(k −n)g(n)
(7.25)
With this deﬁnition at hand, we summarize our ﬁnding in Eq. (7.24).
Lemma 7.1. The discrete convolution deﬁned by Eq. (7.25) is a special case of its
counterpart given by Eq. (7.16) in that theorem 7.3 holds when the discrete signals

92
7 Properties of the Fourier Transform
f(k) and g(k) are samples of band-limited signals f(t) and g(t). Likewise, theorem
7.4 also holds if δ is interpreted as the Kronecker delta.
♦
Convolution and DFT
It is logical to use the same deﬁnition of convolution on an array holding N scalars
as the one given for an inﬁnitely large grid containing samples of a function.
h(k) = (f ∗g)(k)
N−1

n=0
f(k −n)g(n)
(7.26)
However, the translated function f(k −n) needs to be more elaborately deﬁned. The
index k −n must refer to well-deﬁned array elements. This is because even if k
and n are within 0 · · · N −1, their difference k −n is not necesserily in that range,
which is a problem that an inﬁnite grid does not have. Remembering that the DFT
was derived from lemma 6.2 and that both f(t) and F(ω) have been extended to be
periodic, it is clear that the translation k −n on the grid is equivalent to the circular
translation discussed in Sect. 6.4. In other words,
f(m) ←f(Mod(m, N))
(7.27)
as well as
f(k −n) ←f(Mod(k −n, N)
(7.28)
should be used in practice. Using the above deﬁnition for convolution together with
circular translation is also called circular convolution.
Upon this interpretation of convolution, we now investigate how a circular con-
volution transforms under DFT. Let the symbol F represent the DFT, and let F, G
and H be DFTs of the discrete function values f, g, and h on a grid. The functions f,
g, and h are assumed to have been deﬁned on a ﬁnite grid of tm, while the functions
F, G, and H are deﬁned on a grid of ωm. Both grids have the same number of grid
points, N, as generated by integers m ∈0, 1, 2 · · · N −1.
F(f ∗g)(k) = 1
N

l
⎛
⎝
j
f(j)g(l −j)
⎞
⎠exp

−ik2πl
N

= 1
N

l
⎧
⎨
⎩

j

m
F(m) exp

ij 2πm
N

×

n
G(n) exp

i(l −j)2πn
N
⎫
⎬
⎭exp

−ik2πl
N

(7.29)
We change the order of summations so that we can obtain a “delta” function that
helps us to annihilate summations. However, we ﬁrst shift the order of the sums with
indices l, j with those with the indices n, m:

7.3 Convolution, FT, and the δ
93
F(f ∗g)(k) = 1
N

n

m
F(m)G(n) ×

l

j
exp

ij 2πm
N

exp

i(l −j)2πn
N

exp

−ik2πl
N

= 1
N

n

m
F(m)G(n)

l
exp

il2π(n −k)
N
 
j
exp

ij 2π(m −n)
N

= N 2
N

m

n
F(m)G(n)δ(n −k)δ(m −n)
= N 2
N

n
F(n)G(n)δ(n −k)
= NF(k)G(k)
(7.30)
We note here that the two Kronecker delta functions are obtained by recognizing
sums (the ones with l and j) as scalar products between complex exponentials, see
Eq. (6.42), and that these deltas annihilated1 two summations (the ones with m and
n) before they disappeared. Accordingly, we give the deﬁnition for circular convolu-
tions.
Deﬁnition 7.4. Given two ﬁnite discrete signals f(m) and g(m), with m being inte-
ger in 0, · · · N −1, the circular convolution operation for such signals is denoted by
∗, and is deﬁned as
h(k) = (f ∗g)(k) =
N−1

n=0
f(k −n)g(n)
(7.31)
All references to indices, including additions, are in ModN arithmetic.
Accordingly, h(m), the outcome of the discrete convolution, is periodic with the
period N, just as the component sequences f(m) and h(m). We summarize the result
on DFT of convolution as a lemma.
Lemma 7.2. Given the discrete circular convolution deﬁned by Eq. (7.31), it trans-
forms as multiplication under the DFT
h(m) = (f ∗g)(m)
⇔
H(k) = NF(k) · G(k)
(7.32)
Likewise, theorem 7.4 holds if δ is interpreted as the Kronecker delta.
♦
1 The δs are very efﬁcient “summation annihilators”. They erase summations twice: The ﬁrst
time when they are created, and the second time when they disappear. The Dirac distribu-
tions are the analogous “integration annihilators”.

94
7 Properties of the Fourier Transform
7.4 Convolution with Separable Filters
The 1D convolution, Eqs. (7.25) and (7.31), and the FT results are also valid in
multiple dimensions. Specially, when the ﬁlter size is large, it is more efﬁcient to
perform the convolution in the frequency domain for general ﬁlters. However, there
is an exception to this rule if ﬁlters have a special form.
Here we will discuss how to implement a special class of ﬁlters in multiple di-
mensions, separable ﬁlters, that can be implemented very quickly. This class covers a
frequently used type of ﬁlters used in bandpass or scale space ﬁltering, e.g., to change
the size of an image or to compress it. We discuss it for 2D continuous functions here,
but the results are also valid for both types of discrete convolutions discussed above,
and for even higher dimensions than 2.
Deﬁnition 7.5. A function g(x, y) is called a separable function if
g(x, y) = g1(x)g2(y)
(7.33)
Assume that g(x, y) is a separable ﬁlter function with which we wish to convolve the
function f(x, y). Using separability of g, we can rewrite the convolution:
h(x, y) = (g ∗f)(x, y) =
 
g(τx, τy)f(x −τx, y −τy)dτxdτy
(7.34)
=

g1(τx)

g2(τy)f(x −τx, y −τy)dτy

dτx
=

g1(τx)[˜h(x −τx, y)]dτx
(7.35)
= (g1 ∗˜h)(x, y)
(7.36)
where ˜h introduced in Eq. (7.35) is a function that represents the intermediary result:
˜h(x, y) =

g2(τy)f(x, y −τy)dτy = (g2 ∗˜h)(x, y)
(7.37)
This is in fact a convolution that involves only one of the argument dimensions of
f(x, y), i.e., y. It is computed by integrating along the y-axis of the image with the
1D ﬁlter g2(y). The operation that follows is also a pure 1D convolution, but applied
to the result of the ﬁrst convolution and along the remaining dimension, x, so that
we can write the entire operation as a cascade of 1D convolutions.
h(x, y) = (g1(x) · g2(y)) ∗f(x, y) = g1(x) ∗(g2(y) ∗f)
(7.38)
Assuming that, in practice, the ﬁlter as well as the image are discrete, and the integra-
tions have to be implemented as summations, we can estimate the cost of convolution
as the number of arithmetic operations2 per image point. If the ﬁlter g is m×m, then
2 One arithmetic operation in computational cost estimation corresponds to one multiplica-
tion plus one addition.

7.5 Poisson Summation Formula, the Comb
95
the ﬁlter g1 will be of size 1 × m, whereas the ﬁlter g2 will be of size m × 1. Im-
plementing the convolution directly via a 2D ﬁlter, as in Eq. (7.34), will require m2
operations/pixel. Implementing it as in Eq. (7.38) demands ﬁrst m operations/pixel
to compute ˜h = g2 ∗f, then another m operations/pixel to compute the ﬁnal h, total-
ing to 2m operations/pixel. It should be stressed that both ways of implementing the
ﬁltering will yield results that are identical. Assuming a typical ﬁlter with m = 20,
the gain in terms of arithmetic operations is, however, a factor of 10. This is an appre-
ciable difference for most applications because it translates to a speed-up of the same
amount in conventional computation environments. For this reason, separable ﬁlters
are highly attractive ﬁlters, in image and other multidimensional signal processing
applications.
7.5 Poisson Summation Formula, the Comb
We elaborate in this section on how to sample a function formally. We will use dis-
placed dirac distributions to achieve this. For a quick initiation, the theorems and
lemmas can be directly read without proofs.
Assume that we have an integrable function F(ω), which is not necessarily a
ﬁnite frequency function. Then we deﬁne the periodized version of it ˜F(ω) as
˜F(ω) =

n
F(ω + nΩ)
(7.39)
Evidently, ˜F(ω) is periodic with the period Ω, and we can expand it in terms of FCs,
see Eq. (6.12)
˜F(ω) = 1
Ω

m
˜f(tm) exp(−itmω)
(7.40)
where we obtain ˜f(tm) through Eq. (6.13), if we keep in mind that tm = m 2π
Ω ,
˜f(tm) =

Ω
2
−Ω
2
˜F(ω) exp(itmω)dω
=

Ω
2
−Ω
2

n
F(ω + nΩ) exp(itmω)dω
=

n

Ω
2
−Ω
2
F(ω + nΩ) exp(itmω)dω
Here, we changed the order of summation and integration which is permitted for
physically realizable functions. Accordingly, ˜f(tm) is obtained as:

96
7 Properties of the Fourier Transform
˜f(tm) =

n
 nΩ+ Ω
2
nΩ−Ω
2
F(ω) exp(itm(ω −nΩ))dω
= exp(−itmnΩ)

n
 nΩ+ Ω
2
nΩ−Ω
2
F(ω) exp(itmω)dω
= exp(−imn2π)
 ∞
−∞
F(ω) exp(itmω)dω
= f(tm)
(7.41)
Thus, ˜F(0) according to the formula in Eq. (7.40) yields
˜F(0) = 1
Ω

m
f(m2π
Ω )
(7.42)
On the other hand, using the deﬁnition of ˜F(ω) in Eq. (7.39) we obtain the same
˜F(0) as
˜F(0) =

n
F(nΩ)
(7.43)
so that we can write the Poisson summation as a theorem.
Theorem 7.5 (Poisson summation). If f and F are a FT pair, then their sample
sums fulﬁll

n
F(nΩ) = 1
Ω

m
f(m2π
Ω )
(7.44)
♦
The power of this formula is appreciable because we did not require F to have ﬁnite
frequencies nor f to have a ﬁnite extension. It says that if we know an FT pair, then
we can deduce their discrete sums from one another.
The Comb distribution is deﬁned as a train of δ distributions:
CombT (t) =

n
δ(t −nT)
(7.45)
It is a convenient analytic tool when sampling physical functions, as it relates sam-
pling in one domain with the periodization in the other domain, elegantly. However,
we need to derive its behavior under the FT before exploiting it.
The left side of Eq. (7.44) is

n
F(nΩ) = ⟨

n
δ(ω −nΩ), F⟩
(7.46)
whereas the right side of Eq. (7.44) yields

7.5 Poisson Summation Formula, the Comb
97
1
Ω

m
f(m2π
Ω ) = 1
Ω ⟨

m
δ(t −m2π
Ω ), f⟩
= 1
Ω
 
m
δ

t −m2π
Ω
 
F(ω) exp(itω)dω)

dt
= 1
Ω

F(ω)
 
m
δ

t −m2π
Ω

exp(itω)dt

dω
by changing the order of the integrations and the summation. Because δ(t) = δ(−t),
we can perform a variable substitution in the inner integral,
1
Ω

m
f(m2π
Ω ) = 1
Ω

F(ω)
 
m
δ

t −m2π
Ω

exp(−itω)dt

dω
= 1
Ω

F(ω)F

m
δ

t −m2π
Ω

(ω)dω
= ⟨1
Ω F

m
δ

t −m2π
Ω

, F⟩
(7.47)
and identify it formally as a FT of a Comb. In fact, because δ is a distribution and F
was an arbitrary function, the object
F

n
δ(ω −nΩ)

(7.48)
is also a distribution which, thanks to Eqs. (7.44), (7.46), and (7.47),

n
F(nΩ) = ⟨

n
δ(ω −nΩ), F⟩= ⟨1
Ω F

m
δ

t −m2π
Ω

, F⟩(7.49)
acts in a well-deﬁned manner on arbitrary functions under integration. Accordingly,
the last equation delivers the FT of a Comb, which we state in the following theorem.
Theorem 7.6 (FT of a Comb). The FT of a Comb distribution yields a Comb distri-
bution
F

m
δ

t −m2π
Ω

= Ω

n
δ(ω −nΩ)
(7.50)
♦
Up to now, we did not impose any restriction on F(ω). We now assume that F
is zero outside of −Ω
2 , Ω
2 . Then, we can, by applying the right-hand of the theorem,
see that the periodized F is obtained as a convolution:

98
7 Properties of the Fourier Transform
˜F(ω) = (Ω

n
δ(ω −nΩ)) ∗F
= Ω

n
δ(ω −nΩ) ∗F
= Ω

n
F(ω −nΩ)
(7.51)
As there is no overlap, the information relative to F is still intact after the periodiza-
tion of F. Since convolution in the ω-domain is equivalent to a multiplication in the
t-domain Eq. (7.51) is equivalent to a multiplication in t-domain,

m
δ

t −m2π
Ω

f(t) =

m
f

m2π
Ω

δ

t −m2π
Ω

(7.52)
where Eq. (7.50) has been used. However, a periodization of the ω-domain corre-
sponds to a sampling in the t-domain. Accordingly, we conclude that a multiplication
by a Comb distribution yields the mathematical representation of sampled function.
The following lemma, summarizes this and restates the Nyquist theorem:
Lemma 7.3. Let f be a ﬁnite frequency function with F vanishing outside of −Ω
2 , Ω
2 .
Then the sampled f is given by the distribution
CombT (t)f(t) =

m
f(mT)δ(t −mT)
(7.53)
with T < 2π
Ω . A lossless reconstruction of f is achieved by convolving this distribu-
tion with the inverse FT of the characteristic function χΩ.
7.6 Hermitian Symmetry of the FT
The Hermitian symmetry of FCs were discussed in Sect. 5.4. There is an analogous
result even for FT. We suggest the reader to study it in the frame of the following
exercises.
Exercise 7.3. Show that the F(ω) that is the Fourier transform of f(t) is Hermitian:
F(ω) = F(−ω)∗
(7.54)
Do the same in 2D, i.e., when F(ωx, ωy) is the FT of f(x, y), show that
F(ωx, ωy) = F(−ωx, −ωy)∗
(7.55)
Exercise 7.4. Show that even DFT coefﬁcients are Hermitian. Which coefﬁcients rep-
resent F(−2, −2) in a 256 × 256 image (the ﬁrst element is labelled as 0 in both
dimensions)?

7.7 Correspondences Between FC, DFT, and FT
99
Fig. 7.2. FT of a real function is Hermitian, i.e., the points mirrored through the origin have
the same absolute value, shown as the sizes of circles, but negative arguments, shown as the
red hue and its negative hue, cyan
Exercise 7.5. Take a digital image and apply the DFT to it. Put all strictly negative
horizontal frequencies to zero, i.e., half the frequency plane. Inverse Fourier trans-
form this, and display the real part and the imaginary part. The complex image you
obtained is called the analytic signal in signal processing literature. Comment on
your result.
Exercise 7.6. Now ﬁll the points in the half-plane with zero values with values from
the other half-plane according to Eq. (7.55). Comment on your result.
7.7 Correspondences Between FC, DFT, and FT
We started with the Fourier series and derived from it the FC transform, theorem 5.2.
By theorem 6.1, we showed that a generalization of the FCs leads to FT if the ﬁnite
extension restriction imposed on functions is removed. In that we also established
that not only the t-domain but also the ω-domain can be continuous. This led to
the idea of reformulating the FC theorem for the ﬁnite frequency functions, yielding
theorem 6.2. From both of the theorems FC I and FC II, we could derive a fully
discrete transform pair, DFT, where the number of FCs synthesizing the current ﬁnite
extension function is ﬁnite, theorem 6.4.

100
7 Properties of the Fourier Transform
Table 7.1. Frequently referenced Fourier transform pairs
Function
FT of function
f(t)
F(ω)
f(t + t′)
exp(−it′ω)F(ω)
df(x)
dx
iωF(ω)
(f ∗g)(x)
2πF(ω)G(ω)
δ(t)
1
2π
P
m δ(t −mT)
2π
T
P
n δ(ω −n 2π
T )
χT (t) =
(
1,
if t ∈[−T
2 , T
2 ];
0,
otherwise.
T
2π
sin( T
2 ω)
( T
2 ω)
1
√
2πσ2 exp(−t2
2σ2 )
exp(−
ω2
2( 1
σ )2 )
The same essential properties hold true for FTs, FCs and DFTs. For this reason
and to keep the clutter of variables and indices to a minimum, it is natural to work
with FTs, at least at the design stage of applications. We will summarize the impor-
tant aspects of FT pair interrelationship by using only the forward and inverse FT
integrals. In table 7.1, we list some commonly used properties of the transform, and
useful FT pairs in signal analysis studies. Here, we recall that lemma 6.1 is conve-
nient to use to obtain some other entries for the table.
Exercise 7.7. What is the FT of δ(t −t0) ?
HINT: Study the FT of f(t −t′).
Exercise 7.8. What is the FT of cos(t) ?
HINT: Use cos(t) = 1
2(exp(it) + exp(−it)) .
By interpreting the symbols according to the FT correspondence table (Table
7.2), the FT pair can represent both variants of the FC transform and the DFT. How-
ever, it should be noted that the table can not be used for translation of FT pairs. For
example, the FT of a Gaussian is a Gaussian, but the FC and DFTs of a Gaussian are
strictly speaking not sampled Gaussians, although for many purposes this is a good
approximation.
Exercise 7.9. What are the FT, FC and DFT of cos(t) on the appropriate parts of
the real axis?
HINT: Apply FT, FC, and DFT to translated versions of δ while interpreting the
latter as Dirac and Kronecker-δs.
Exercise 7.10. Plot the FT, FC and DFT of a Gaussian by using a numerical soft-
ware package. How should we proceed to approximate a true Gaussian by the results
better and better?

7.7 Correspondences Between FC, DFT, and FT
101
Table 7.2. Correspondence table for forward and inverse FT, FC, and DFT
F(ω) = F(f)(ω) = 1
P
Z
t∈D
f(t) exp(−iωt)dt
(Forward)
f(t) = F−1(F)(t) =
Z
ω∈D
F(ω) exp(itω)dω
(Inverse)
FT
FC I
FC II
DFT
FORWARD
Symbol
F
F
F
DFT
Constant
P = 2π
P = 2π
P = 1
P = N
exp
exp(−iωt)
exp
`
−in 2π
T t
´
1
Ω exp
`
−im 2π
Ω ω
´
exp(−in 2π
N m)
Sum rule
R
R
P
m
P
m
D
t ∈[−∞, ∞]
t ∈[ −T
2 , T
2 ]
m ∈Z
m ∈{0, ··, N −1}
Translation
t + t′
Mod(t + t′, T)
m + m′
Mod(m + m′, N)
δ
Dirac-δ
Dirac-δ
Kronecker-δ
Kronecker-δ
f
integrable f
FE f
FF f
FEF f
INVERSE
Symbol
F −1
F −1
F −1
IDFT
exp
exp(iωt)
2π
T exp
`
in 2π
T t
´
exp
`
im 2π
Ω ω
´
exp
`
in 2π
N m
´
Sum rule
R
P
n
R
P
n
D
ω ∈[−∞, ∞]
n ∈Z
ω ∈[ −Ω
2 , Ω
2 ]
n ∈{0, ··, N −1}
Translation
ω + ω′
n + n′
Mod(ω + ω′, Ω)
Mod(n + n′, N)
δ
Dirac-δ
Kronecker-δ
Dirac-δ
Kronecker-δ
FE:
Finite extension functions
FF:
Finite frequency functions (band-limited functions)
FEF: Finite extension and frequency functions

8
Reconstruction and Approximation
Often we only know gray values of an image on a discrete set of points. This is
increasingly the case due to the ever more affordable digital imaging equipment, e.g.,
consumer still and motion picture cameras. Even when the original image is on ﬁlm,
it must often be digitized for further processing on digital equipment, e.g., when
analog processing is not available. In this chapter we study the techniques used to
obtain some discrete image processing schemes for the most common mathematical
operations.
We discuss ﬁrst the interpolation function and the characteristic function as this
concept has a signiﬁcant impact on how one implements approximation of functions
and operators. Then we study the important class of linear operators and illustrate
them by computing the partial derivatives of images, and afﬁne coordinate transfor-
mations, e.g., rotation and zooming, of images. Subsequently, we contrast the linear
operators with nonlinear operators and discuss “square of an image”, and “multipli-
cation of two images”, which are nonlinear.
The basic approach to approximation of mathematical operators is to reconstruct
the continuous image from its samples and then to apply the operator followed by
an appropriate sampling of the result. While discussing the typical pitfalls, our goal
is to generate discrete signal processing schemes using discrete data to approximate
continuous operators.
8.1 Characteristic and Interpolation Functions in N Dimensions
Deﬁnition 8.1. The characteristic function on a limited volume D in N dimensions
(ND) is deﬁned as
χD(r) =

1,
if r ∈D;
0,
otherwise.
(8.1)
This function is also known as the indicator function because it is used to indicate
two volumes, one in which the function is zero, and the complementary volume in
which it is not zero.

104
8 Reconstruction and Approximation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 8.1. A characteristic function that corresponds to a square in 2D.
We now multiply f with the characteristic function χD(r) to obtain a limited ex-
tension function from a periodic function, χD(r)f(r). We assume that D ﬁts entirely
in a central hypercube having the vertices π(±1, · · · , ±1)T , the Nyquist block. The
Nyquist block deﬁnes one period of the function f which is repeated in all possi-
ble coordinate axis directions to cover the entire ND space. The boundaries of the
Nyquist block have the distance to the origin, π, implying that the distances between
samples are integer values in directions parallel to the coordinate axes. We wish to
study how the ﬁnite extension function χD(r)f(r) is Fourier transformed. This prob-
lem is the same as the one discussed previously, except that we now have multiple
dimensions.
F(ω) = 1
2π
 ∞
−∞
χD(r)f(r) exp(−iωT r)dr
(8.2)
= 1
2π
 ∞
−∞
χD(r)(

m
F(ωm) exp(iωT
mr) exp(−iωT r)dr
(8.3)
Here we have used the fact that a periodic function can be expanded in its Fourier
coefﬁcients, F(ωm), given by an ND version of theorem 5.2. By changing the order
of integration and summation we obtain
F(ω) =

m
F(ωm) 1
2π
 ∞
−∞
χD(r) exp(iωT
mr) exp(−iωT r)dr
(8.4)
so that

8.1 Characteristic and Interpolation Functions in N Dimensions
105
−0.2
0
0.2
0.4
0.6
0.8
Fig. 8.2. The interpolation function corresponding to a square region. The cross section of
this function is periodically negative, as is shown in Fig. 8.4
F(ω) =

m
F(ωm)2π
T
1
2π
 ∞
−∞
χD(r) exp(−i(ω −ωm)T r)dr
(8.5)
= 2π
T

m
F(ωm)F(χD)(ω −ωm)
(8.6)
Eq. (8.6) states that the Fourier transform of a limited extension function is a discrete
sum of shifted continuous functions weighted with some coefﬁcients. These contin-
uous functions can be identiﬁed as interpolation functions in ND. They are, as the
equation indicates, determined entirely by the characteristic function χD, and D is
in turn determined by the volume D, outside of which the function f vanishes.
The characteristic function on a (limited) symmetric interval in 1D is given as
χT (x) =

1,
if
x ∈[ −T
2 , T
2 ];
0,
otherwise.
(8.7)
The FT of this characteristic function yields
F(χT )(ω) = T
2π sinc(ω T
2 ),
(8.8)
a fact already established in Eq. (6.22).
The repetition of an ND volume in the r-domain, which is also called a tessella-
tion, is always achievable by use of the suggested Nyquist block. This corresponds
to a hypercubic sampling in the ω-domain. Conversely, we can switch the roles of
r and ω and arrive at the conclusion that a band-limited signal f(r) can always be

106
8 Reconstruction and Approximation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 8.3. The circular region deﬁning a characteristic function
sampled at the vertices or at the centers of a hypercube. This is done by using a suf-
ﬁciently large hypercube that entirely contains the volume D′ of the characteristic
function, the Nyquist block, which is now deﬁned in the ω-domain. Fortunately, hy-
percubic sampling is utilized to obtain discrete images from imaging devices in most
applications, e.g., a scanner, a digital camera, a digital video camera, magnetores-
onance camera, and so on. However, other repeatable volumes, such as repeating a
hexagonal region in a hexagonal manner in 2D, are also possible. The corresponding
interpolation functions can be identiﬁed by Fourier transformation of the repeated
volume containing the characteristic function.
The information that encodes the shape of the boundary of the characteristic
function is sufﬁcient to determine the interpolation function in the “other” domain
which can be represented accurately by sampling. In 1D we cannot talk about the
shape of the boundaries because the boundaries are points, which results in the sinc
functions as interpolators when D is a contiguous interval. If it consists of several
juxtaposed intervals, then the interpolation function is expressed as sums and differ-
ences of sinc functions.
In 2D, an extension of a central interval to a square interval results in a simple
characteristic function too. Assuming that it is the ω-domain that contains the limited
extension function F, this leads to the characteristic function
sinc(πx) · sinc(πy)
(8.9)
in the r-domain, which is illustrated by Fig. 8.1. Likewise, in 3D the analogous
interpolation function is deﬁned by
sinc(πx) · sinc(πy) · sinc(πz)
(8.10)

8.1 Characteristic and Interpolation Functions in N Dimensions
107
−64
−56
−48
−40
−32
−24
−16
−8
0
8
16
24
32
40
48
56
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 8.4. The cross sections (along the horizontal axes) of the interpolators shown in Fig. 8.5
and 8.2. Note that both of the latter 2D interpolators have bandwidth of π
8 (horizontally). The
circular region interpolator, the Besinc function, is shown in blue, whereas the quadratic region
interpolator, the sinc function, is shown in red
It is straightforward to show that both in 2D as well as higher dimensions, character-
istic functions deﬁned on such rectangular volumes always lead to products of sinc
functions of the cartesian variables in the “other” domain.
Admittedly, the sinc functions are suggested by the theory as the ideal interpola-
tion functions for 1D band-limited signals, and they can be easily extended to higher
dimensions. However, from this, one should not hasten to conclude that the sinc func-
tions are the ideal (and the best) interpolation functions in practice. This is far from
the truth because of the various shortcomings of the sinc functions. To start with,
the sinc functions are not ideal interpolators for 2D and higher dimensions even in
theory, since it is most probable that natural ND signals to be processed will have
characteristic functions (in the ω-domain) that do not have direction preference, e.g.,
the average camera images in 2D are best described by characteristic functions with
boundaries that are circles. In 2D, such a circular characteristic function results in an
interpolation function that is obtained by substituting π∥ω∥=

ω2x + ω2y into a 1D
function, as below:
Besinc(∥ω∥) = 2J1(π∥ω∥)
π∥ω∥
(8.11)
where J1 is a Bessel function (of the ﬁrst-order of the ﬁrst kind). Since it is the
analogue of the sinc function, we call Eq. (8.11) Besinc function. The interpolator
function is illustrated by Fig. 8.3, whereas its proﬁle through the origin, the Besinc
function, is given in Fig. 8.4.

108
8 Reconstruction and Approximation
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
9
0.
Fig. 8.5. The interpolation function corresponding to a circular characteristic function. A
cross section of this function is shown in Fig. 8.4
The most important reasons why sinc functions fall out of grace in practice are,
however, as follows:
i) They decrease as 1/∥r∥, which is too slow, yielding very large ﬁlters.
ii) They are not direction isotropic, meaning that in 2D and higher dimensions,
sharp changes of image intensities with well-deﬁned directions are not treated
by a sinc interpolator in the same manner for all directions, creating artifacts.
We show this in Sect. 9.4 via Example 9.3.
iii) They introduce ringing near sharp changes of image intensities, creating arti-
facts.
Point i) is a sufﬁcient reason to look for other interpolation functions in 1D. In 2D
and higher dimensions, the subsequent points are so imposing that sinc functions are
almost never the ﬁrst choice.
Consider the functions that look like tents with the maximum 1
μ(t) =
⎧
⎨
⎩
1 −t, if
0 < t ≤1
1 + t, if
−1 ≤t ≤0
0,
otherwise;
(8.12)
as illustrated by Fig. 8.6. This is the linear interpolation function, which, because
of its appearance, is sometimes called a tent function. Equation (8.15) suggests the
weighting of the shifted versions of the continuous interpolation function, μ, with the
known image values f on the grid rl. When these ampliﬁed functions are summed,
the continuous signal f(r) is obtained from its discrete values f(rl). This process

8.2 Sampling Band-Preserving Linear Operators
109
is called synthesis or reconstruction, and is illustrated by Fig. 8.6 for tent functions.
Since the sum of two linear functions is another linear function, and because the in-
terpolation functions are exact on the grid (integers of the x-axis, in the ﬁgure), the
approximation using Eq. (8.12) results in a piecewise linear function. Continuous
by construction, the approximating function is delivered by the weighted tent sum-
mation, which “automatically” joins the values of the original function on a set of
discrete points. Thus, the approximation is error free if the function to be approxi-
mated is piecewise linear. If the constant “1” is to be approximated, then the entire
set of interpolation functions is summed and the result is error free as shown in the
ﬁgure. However, the linear interpolator suffers from the following drawbacks:
i) Extending them to 2D via μ(x)μ(y) will make them anisotropic because this 2D
function has isocurves that are not circles (but biased by squares). This means
that certain directions of edges will be artiﬁcially favored over the others.
ii) It is not differentiable everywhere. As a result we cannot construct all partial
derivatives easily everywhere. We discuss the partial derivative operator in the
next section.
Another function family, called B-splines, can be obtained by successive convolution
of the linear interpolator by itself. The result is a piecewise polynomial interpolator
that is smooth. Because they do not suffer from the second point above, B-splines are
often preferred over the linear interpolator [123]. They have found many applications
because of their speed thanks to their separability in x and y. However, they too suffer
from being anisotropic.
A nonpolynomial interpolator, that we will study further in Sect. 9.2 is the Gaus-
sian interpolator, which can be shown to be the asymptotic limit of B-splines [221].
In Fig. 8.7 we show a set of shifted Gaussians that are used to illustrate a synthesis
process. A Gaussian is isotropic when extended to 2D via μ(x)μ(y), and it is in-
ﬁnitely differentiable, everywhere. In the next section, it sufﬁces to know that there
exists an interpolation function that has a localized support (it decreases rapidly to
zero), and that the choice is not restricted to the sinc functions.
8.2 Sampling Band-Preserving Linear Operators
We will be concerned here with continuous operators acting on vector spaces, in
particular, function spaces. Such operators act on functions and deliver elements that
stay in the same function space, as a result. The continuity of an operator means that
a small change of its argument (a function) results in a small change of its result (also
a function). Derivation, convolving with a particular ﬁlter, and taking the square root
of a function are examples of operators. We start by deﬁning linear operators.
Deﬁnition 8.2. An operator T is a linear operator if it satisﬁes
T (f + g) = T (f) + T (g)
(8.13)
T (λf) = λT (f)
(8.14)
where λ is a scalar.

110
8 Reconstruction and Approximation
−4
−3
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
−4
−3
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
−4
−3
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Fig. 8.6. (Top, left ) The graph illustrates the set of linear interpolators on a discrete set of
points. (Top, right) Summing up the interpolators, approximates the constant function 1. (Bot-
tom, left) A function to be approximated along with an interpolator ampliﬁed by its function
sample. (Bottom, right) The green curve is the result of the approximation, the sum of the
ampliﬁed interpolators
Assume that we are given a set of function values f(rl), where rl is a discrete set
of points that we will call grid. From this, the continuous f(r) can be synthesized by
using Eq. (8.6)
f(r) =

l
f(rl)μ(r −rl)
(8.15)
with μ being an interpolation function. This continuous reconstruction is merely
a mental process that is done to implement continuous linear operators discretely.
Ultimately, we wish to be able to implement every continuous operator discretely.
However, there is no effective formula that will ﬁt all types of operators. Instead, we
aim here to illustrate how to implement some frequently occurring linear continuous
operators, in particular, those that do not change the bandwidth of the functions. The
latter is an important restriction that raises the hope of success because we can always
represent every function with ﬁnite frequencies well enough by its samples. We illus-
trate the technique by showing how to implement partial derivation and noninteger
shifts on a discrete grid.

8.2 Sampling Band-Preserving Linear Operators
111
−4
−3
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
−4
−3
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
−4
−3
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Fig. 8.7. (Top, left) The graph illustrates a set of Gaussians, with σ = 0.65, that are used as
differentiable interpolators. (Top, right) Summing up the interpolators approximates a constant
function. (Bottom, left) A function to be approximated (the same as in Fig. 8.6) along with an
interpolator ampliﬁed by its function sample. (Bottom, right) The green curve is the result of
the approximation, the normalized sum of the ampliﬁed interpolators
Partial Derivatives
In image analysis, the partial derivative of a function, which is only discretely avail-
able, is a frequently demanded operation that ideally should also result in a discrete
image. Ideally, the computations performed on the discrete samples of the image
should result in a discrete version of the result delivered by the continuous opera-
tor applied to the continuous image. To take the partial derivative of a function is
evidently linear because
∂
∂x(f + g) = ∂
∂x(f) + ∂
∂x(g),
and
∂
∂x(λf) = λ ∂
∂x(f).
(8.16)
The partial derivative is one of the most frequently used operators in image pro-
cessing, e.g., to extract edges, lines, direction, curvature, and texture properties. In
particular, arbitrary partial derivatives of a differentiable scalar function f(r), where
r = (x1, x2, · · · xN)T , are represented by ∂f(r)
∂xj
with j = 1, 2, · · · N. The function

112
8 Reconstruction and Approximation
f can be seen as a scalar-valued image deﬁned on an N-dimensional space repre-
sented by r. If f has ﬁnite frequencies, i.e., F is zero outside of a limited volume,
∂f
∂xj will also have ﬁnite frequencies. This is because the latter is equivalent to iωjF
in the frequency domain, which will evidently be zero in the same places (volume)
as F is zero. Thus, the partial derivatives of the continuous function can be faithfully
represented by sampling these functions on the same grid that samples the original
image. To compute the partial derivatives, the operator
∂
∂xi is applied to Eq. (8.15),
and it is moved past the sum because partial derivation is a linear operator:
∂f(r)
∂xi
=

l
f(rl)∂μ(r −rl)
∂xi
,
i, j : 1 · · · N
(8.17)
At this point, we must require that the interpolation function μ is differentiable.
Assuming this, the linear operator can be absorbed by the interpolator function, and
the resulting continuous function can be discretized too. Because ∂f
∂x has the same
ﬁnite frequencies as f, we can also sample it on the same grid rk, yielding
∂f(rk)
∂xi
=

l
f(rl)∂μ(rk −rl)
∂xi
,
i, j : 1 · · · N
(8.18)
This is a discrete scalar product of two functions, the ﬁrst of which is the original
discrete image. The second function is the discretized interpolation function after it
has absorbed the partial derivation operator. As can be seen from the right-hand side
of Eq. (8.18), the approximation can be realized by a convolution when the values of
∂f
∂xi at all points of the grid have to be computed.
Arbitrary Translation or Shifting
Translating an image f(r) with Δr is another frequently needed operation. Here,
Δr does not have to be a multiple of the sampling period that is assumed to be 1.
For example the translation could be Δr = (0.25, 0.75)T in 2D. In the continuous
domain, this is equivalent to a coordinate transformation,
r′ = r + Δr
(8.19)
so that the translated version of f(r) is given by f(r −Δr). Because the FT of the
translated image is exp(−iΔrT ω)F(ω), translation is a band-preserving operator.
Without loss of generality, one can assume that all components of the vector Δr
are in the open interval ]0, 1[ because otherwise the integer translation is performed
ﬁrst. Integer translations are conveniently implemented as a permutation. Using Eq.
(8.15), the remaining noninteger translation is implemented by sampling the recon-
structed and shifted signal at the grid points as
f(rj −Δr) =

l
f(rl)μ(rj −Δr −rl)
(8.20)

8.2 Sampling Band-Preserving Linear Operators
113
Fig. 8.8. A 1D function (a line) containing slow and rapid gray changes is translated with
zero shifts (left) and 0.67 points (noninterpolated) shifts (right) between successive lines. Each
color represents a unique gray level of the original, which is given in color in Fig. 12.4 along
with the interpolated shifts. The vertical axis is time
The result is a scalar product between the shifted, sampled interpolation function,
which easily lends itself to be implemented as a convolution.
If this scalar product is not implemented, translation will result in aliasing effects.
This is illustrated for 1D functions by Fig. 8.8, where we show 1D functions as lines
of an image. The image on the left is obtained by repeating the same line (the 1D
function), i.e., the translation is zero between the successive lines. We applied 0.67
pixels (cyclical) translation to the same 1D function to obtain the successive lines,
by rounding off 0.67j where j is the horizontal index, of the image on the right.
Apart from the jaggedness of the lines, we also see erroneous gray levels in the
high-frequency parts, where both aliasing problems are due to the straightforward
implementation of the translation. This should be compared to Fig. 12.4 (top and
middle), where we show the translation performed according to Eq. (8.20), which
contains interpolation.
Note, however, when one produces a (higher dimensional) motion image by suc-
cessive application of a translation to a static image f, such as the one in the example,
the resulting image will potentially have a large temporal frequency extent as well.
This extent depends on the size of the shift as well as on the frequency content of
the static pattern f. Because the temporal frequencies must obey the law of sampling
to avoid the errors of discretization, for every temporal sampling frequency there is
a maximum speed/shift that should not be exceeded. The implications of motion on
the spectrum are discussed in further detail in Sect. 12.7.
In conclusion, a linear band-preserving continuous operator can be discretized by
discretizing the result of the operator applied to the interpolation function in analogy
with the partial derivative operator.

114
8 Reconstruction and Approximation
8.3 Sampling Band-Enlarging Operators
Here we discuss how to discretely carry out certain operations that will ﬁnitely en-
large the volume in which a band-limited function is non-zero, i.e., the result is also
a band-limited function, albeit with a different band limitation. There is no general
formula for nonlinear operators, although with some care many can be discretized
as long as they do not enlarge the frequency support to inﬁnity, i.e., the volume of
frequencies in which F is zero diminishes to zero after the operation.
Multiplication of two images. Assume that we know the discrete values of two
images f and g on the same grid rl and we need the discrete values of fg. This is
usually referred to as a (pointwise) multiplication of two images. An intuitive ap-
proximation is
(fg)(rl) = f(rl)g(rl).
(8.21)
However, the use of the original grid rl poses a problem because the resulting dis-
crete function on the right-hand side of Eq. (8.21) may not represent the continuous
(fg)(r) faithfully enough, although f(rl) and g(rl) may do so with their respec-
tive functions f(r) and g(r). The problem is traced to the fact that the continuous
fg has a Fourier spectrum that is wider than each of its constituent functions. The
multiplication fg in the spatial domain is equivalent to the convolution F ∗G in the
frequency domain to the effect that the result will be “wider” than each of F and G.
Particularly undesirable effects may result if the sum of the bandwidths of F and G
is larger than π in one or more of their dimensions, (x1, x2, · · · xN)T . Discretizing
such a product of images at the sampling rate of the original grid will yield aliasing
errors because of the violation of the fundamental sampling conditions given by the
Nyquist theorem. Thus, reconstructing fg from f(rl)g(rl) will contain unacceptable
errors.
To avoid a violation of the Nyquist theorem, discrete images containing very high
frequencies should not be multiplied with each other directly ﬁrst, but they should be
up-sampled with a factor 2 to force the highest frequency components to be less than
π/2. Multiplying such images is then risk-free if the product is sampled at the rate
of the new, ﬁner grid. Although the continuous product has a larger bandwidth than
its constituents, the bandwidth is not as large as it can violate the Nyquist theorem
upon sampling on the new grid. A lossy, but faster alternative to this procedure is,
prior to multiplication, to apply a lowpass ﬁltering to the image to make sure that
no signiﬁcant frequency components exist outside of the central square having the
width and height π in the spectrum. Squaring, being a special case of multiplication
of two digital images, is subject to the same reasoning, i.e., no signiﬁcant power
of high-frequency components above the central square with height and width of π
should exist upon squaring. The reasoning is extended to ND in a straightforward
fashion.
Because
∂f
∂xi has the same bandwidth as f, products and squares of the partial
derivative images must not contain components outside of the central square with
height and width of π.
Rotation of an image. The rotation of an image with the arbitrary angle of θ rep-
resents a linear operator. It can be achieved by replacing r in Eq. (8.6) or equivalently

8.3 Sampling Band-Enlarging Operators
115
in Eq. (8.15), with Qr′, where Q is a rotation matrix:
f(Qr′) =

l
f(rl)μ(Qr′ −rl)
(8.22)
As an example of rotation matrices, we mention
Q =

cos θ sin θ
−sin θ cos θ

(8.23)
which is the matrix used to rotate 2D images. The new coordinates r′ can be sampled
at a desired rate yielding r′
k. If the image f is band-limited so is f(Qr′) because a
rotation merely rotates the spectrum:
f(Qr′
k) =

l
f(rl)μ(Qr′
k −rl)
(8.24)
Inspecting this equation shows that rotation is achieved by a scalar product be-
tween the image values on the original grid and the interpolation function rotated
via Qr′
k to align the axes of the new coordinates r′, but sampled at the original
grid points. The result of the scalar product is placed in the new grid at the location
r′
k. In other words, here too, the rotation operation is absorbed by the interpolation
function. The interpolator is aligned to the new grid coordinates, but it is sampled
at the old grid points to generate the scalar product coefﬁcients. A scalar product
between the latter and the function samples on the old grid delivers the rotated func-
tion value on the new grid. The sampling rate on the new grid must be appropriately
chosen so it does not lose some spectral components. This is because a 2D image f
can be faithfully sampled on a rectangular grid only if F is conﬁned to the square
[−π, π] × [−π, π], (and to the hypercube [−π, π] × [−π, π] × ...[−π, π] for an ND
image), according to the Nyquist theorem. Rotating f, and thereby F, will require a
denser sampling rate on the new grid if all spectral components are to be retained.
In Fig. 8.9 we illustrate the frequency behavior of two example rotations. Using
the original density will result in a rotation aliasing caused by certain components,
see the “magenta” zones, which may not be acceptable to some applications. For ex-
ample, a rotation with θ = π/4 may require a sampling grid that is
√
2 times denser
than the old grid if all frequency components are to be retained. For square-shaped
2D images, a frequency band magniﬁcation with a factor
√
2 is the largest factor for
any rotation. Alternatively, the area between the blue circle and the boundaries of
the red square, representing the frequencies that risk causing aliasing, can be simply
suppressed by a lowpass ﬁltering as this is an area which could be accepted as noise
by the application at hand. Such a ﬁltering can be incorporated directly into the rota-
tion operator and amounts to having a larger kernel to reconstruct f from its samples.
Similarly, in ND images having the same size in all coordinate axes, the maximum
density magniﬁcation due to rotation is
√
N.
Afﬁne warping of an image. Evidently, the same signal processing principles
and reasoning used to discretize rotation apply to discretizing afﬁne coordinate trans-
formations, also known as afﬁne warping. The old coordinates are replaced by

116
8 Reconstruction and Approximation
−6
−4
−2
0
2
4
6
−6
−4
−2
0
2
4
6
Fig. 8.9. The red square shows the basic area [−π, π] ⊗[−π, π] that is preserved (and re-
peated) by a rectangular sampling grid. The two squares behind show the same frequency
content but rotated with π
16 and π
4 radians. The magenta regions show the areas that should be
suppressed if the original grid density is retained
r = Ar′ + r0
(8.25)
where A is a constant invertible matrix, and r0 is a constant translation vector. Even
afﬁne warping is a linear operator. Because a translation with the constant vector r0
corresponds to a multiplication with exp(−irT
0 k) in the frequency domain, the fre-
quency support of F is unchanged. Accordingly, only the effect of the matrix multi-
plication, i.e., r0 = 0 above, on the spectral support is of relevance when discussing
sampling of an afﬁne deformation of an image. Just like in the rotation transforma-
tion,
r = Ar′
⇒
f(Ar′)
(8.26)
the result of afﬁne warping is a band-limited function, albeit the boundary of the
characteristic function of F now undergoes a more general linear transformation
instead of a simple rotation,
r = Ar′
⇔
k′ = A−1k
(8.27)
An afﬁne coordinate transformation is therefore absorbed by the interpolation func-
tion, and the result is a value of the image at the new grid coordinates but computed

8.3 Sampling Band-Enlarging Operators
117
via a scalar product on the old grid. The translation part can be implemented accord-
ing to Eq. (8.20).

9
Scales and Frequency Channels
Blurring a function by a linear ﬁlter is equivalent to suppressing high-frequency com-
ponents of its spectrum. Having void information at high frequencies suggests in turn
that there might be a redundancy in the representation which can be avoided. This
observation can be utilized in several ways in signal analysis, notwithstanding image
analysis, including the following:
1. The amount of data representing an image can be reduced, e.g., image compres-
sion.
2. The image can be efﬁciently ﬁltered to contain only certain frequency compo-
nents, e.g., recognition of image contents.
3. The image can be resized, e.g., user interfaces and animation.
9.1 Spectral Effects of Down- and Up-Sampling
We consider ﬁrst the band-limited 1D function f(t) to investigate the effects of up-
and down-sampling on F(ω), the spectrum.
Let the maximum frequency of the band-limited signal f be Ω0/2 i.e., F(ω)
is effectively zero outside of the interval [−Ω0
2 , Ω0
2 ]. Then f can be sampled with
the distance T0 = 2π
Ω0 between the samples, without loss of information. What does
happen in the frequency domain when we sample f tighter and tighter using smaller
discretization steps than T0? If information is not lost with the discretization step T0,
then it will deﬁnitely not be lost when sampling f with a smaller sampling step, T,
with T < 2π
Ω0 . Sampling in one domain with the sampling period T corresponds to a
periodization with the basic period of Ω = 2π
T in the other domain.
Conversely, when periodizing F with a period Ω that is larger than the bandwidth
of the signal, we pad zeros after the highest frequency of the signal. This has the
effect that the part of the void band in which F is practically zero increases with
increased sampling rate, i.e., with smaller T. Both up- and down-sampling can be
achieved via continuous reconstruction as follows:

120
9 Scales and Frequency Channels
1. Conﬁne F to the basic period around the origin. This operation has the purpose
to hinder F from being periodic because if F is not periodic but limited, then f
is continuous. This is achieved by multiplying F with a suitable characteristic
function, χD(ω). Such a multiplication is equivalent to a convolution in the spa-
tial domain, leading to a reconstruction of the continuous signal from its sampled
signals, Eq. (8.6):
f(t) =

m
f(tm)μ0(t −tm)
(9.1)
where μ0 = F−1(χD).
2. Resample the continuous signal at the desired rate, which at all circumstances
should be done with a ﬁner step size than what the highest frequency of the
signal content allows (Nyquist theorem). Effectively, this sampling of Eq. (9.1)
amounts to, sampling the interpolation function at the desired points. The equa-
tion itself becomes an ordinary discrete scalar product between the ﬁlter samples
and the old function samples, f(tm):
f(t′
n) =

m
f(tm)μ0(t′
n −tm)
(9.2)
In Sects. 9.2 and 9.3 we will discuss further the choice of μ0 in practice. To establish
the principles of how the interpolation functions are used when performing up and
down-sampling of discrete signals, which we do next, it is sufﬁcient to imagine them
as functions that look like tents for now.
Down-sampling
Here we will down-sample a given discrete signal f(mT0) with an integer factor of
κ by destroying as little information as possible. In that, we follow the procedure
outlined in items and 1 and 2, above.
We reconstruct the continuous signal f(t) and obtain
f(t) =

m
f(mT0)μ0(t −mT0)
(9.3)
where μ0 is the interpolation function, the effective width of which is inversely pro-
portional to Ω0. The highest frequency content of the signal f(t) is Ω
2 and the grid
is a regular grid given by tm = mT0 where1 T0 = 2π
Ω0 .
Before sampling f(t) with the new step size T > T0, we must make sure that the
continuous f does not contain frequencies outside the new interval [−π
T , π
T ]. Some
of the high frequencies that were possible to represent with the smaller step size T0
are no longer possible to represent with the coarser step, T. This can be achieved ﬁrst
in the mind, by convolving the continuous f(t) with a ﬁlter, μ1 having a frequency
extension that is the same as the new frequency interval, yielding:
1 For the sake of simplicity we have made this choice, although the argumentation still holds
if we choose T0 as T0 ≤2π
Ω0 .

9.1 Spectral Effects of Down- and Up-Sampling
121
˜f(t) =

m
f(mT0)μ(t −mT0)
(9.4)
where μ is the new interpolation function obtained as a continuous convolution be-
tween two lowpass ﬁlters, μ = μ1 ∗μ0. The interpolation function μ can be assumed
to be equal to μ1 for the following reasons. In the frequency domain this convolution
will be realized as a multiplication between two characteristic functions. Ideal char-
acteristic functions2 assume, by construction, only values 1 and 0 to deﬁne intervals,
regions, volumes, and so on. In the case of down-sampling, μ will be equal to μ1
because a large step size implies a characteristic function with a pass region that is
narrower than that of a smaller step size. After the substitution t = nT we obtain:
˜f(nT) =

m
f(mT0)μ(nT −mT0)
(9.5)
We can now restate this result by the substitution T = κT0, where κ is a positive
integer:
˜f(nκT0) =

m
f(mT0)μ(nκT0 −mT0) =

m
f(mT0)μ((nκ −m)T0)
(9.6)
Here we used the values of f on the original ﬁne grid mT0 to form the scalar prod-
uct with the ﬁlter μ sampled on the same ﬁne grid. We note, however, that the values
˜f(nκT0) are to be computed at a coarser grid, i.e., at every κth point of the ﬁne grid.
As compared to a full convolution, a reduction of the number of arithmetic operations
by the factor κ is possible by building the scalar products only at the new grid points,
i.e., at every κth point of the original grid. In other words, the number of arithmetic
operations per new grid point is M/κ, where M is the size of the ﬁlter μ sampled at
the original grid positions. However, the size of the discrete ﬁlter is directly propor-
tional to the step size T = κT0. Consequently, the number of arithmetic operations
per grid point does not change when changing κ.
Up-sampling
Up-sampling with a positive integer factor of κ works in nearly the same way as
down-sampling.
First, the continuous signal f(t) is obtained as in Eq. (9.3):
f(t) =

m
f(mT0)μ(t −mT0)
(9.7)
where μ is the interpolation function associated with the original grid having the
period T0.
2 In practice where computational efﬁciency, numerical, and perceptional trade-offs must be
made simultaneously, the interpolation ﬁlters will be inverse FTs of smoothly decreasing
functions.

122
9 Scales and Frequency Channels
We will up-sample f(t) with the ﬁner step size of T, i.e., T = T0
κ . There is ideally
no risk of destroying information with a ﬁner step size, which should be contrasted
to down-sampling, cancelling the highest frequency contents that must be void for
the best ﬁdelity. As before, sampling is achieved by replacing t with nT:
f(nT) =

m
f(mT0)μ(nT −mT0) =

m
f(mT0)μ((n
κ −m)T0)
(9.8)
Here too we can identify the sum as a scalar product between the two discrete
sequences: the values of f on the original coarse grid mT0 and the continuous in-
terpolation function μ sampled with the original step size T0. This is because the
summation index m generates the interpolator samples, whereas the quantity nT is
an offset that remains unchanged as m changes. We note that the values f will be de-
livered at a ﬁner grid when changing n, i.e., at κ fractions of the original, coarser grid.
This will require as many arithmetic operations as the size of the ﬁlter μ, sampled at
the step of T0. This size, which we denote by M, can be determined by truncating
the ﬁlter when it reaches a sufﬁciently low value at the boundary as compared to its
maximum value, typically at 1%. The ﬁlter size of μ is proportional to the sampling
step T0. In up-sampling we make T smaller than T0, but we neither change μ nor
T0 so that the number of arithmetic operations per point in the up-sampled signal is
M and remains constant for any κ. We note in the last expression of Eq. (9.8) that a
change of n results in a shift of μ, but only as much as an integer multiple of T0/κ,
where κ itself is a positive integer. After κ consecutive changes of n, the original
grid is obtained cyclically.
The function f(nT) can be generated as κ convolutions, each using its own frac-
tionally shifted up-sampling ﬁlter, μ. This is because in Eq. (9.8) the ﬁlter coef-
ﬁcients used to generate f(nT) are the same as those generating f(n′T) only if
Mod (n −n′, κ) = 0. An alternative is to view μ(nT −mκT) as a continuous
function μ sampled with the step T, but every κth point is retained when forming the
scalar product. This is equivalent to ﬁlling κ −1 zeros between the available values
of the discrete signal to be up-sampled, i.e., f(nT0), while retaining all points of μ:
f(nT) =

m
f(mκT)μ(nT −mκT) =

l
˜f(nT)μ((n −l)T)
(9.9)
where
˜f(mT) =

f(mT), if m Modκ = 0;
0,
else.
(9.10)
Effectively, Eq. 9.9 implements up-sampling by ﬁrst generating a sparse se-
quence ˜f and then smoothing this by the ﬁlter μ, sampled with the step size T. This
process is simpler to remember and to implement, but one might argue that it is less
efﬁcient because it involves κ−1 multiplications with zeros. This is indeed the case,
if these are truly computed. On the other hand, multiplications with zeros occur cycli-
cally, meaning that they can be eliminated with some care in the implementation.
Example 9.1. We illustrate, in Fig. 9.1, the down-sampling process in 1D for κ = 2,
i.e., we wish to reduce the number of samples of a discrete signal by half keeping as
much descriptive power in the resulting samples as possible.

9.1 Spectral Effects of Down- and Up-Sampling
123
Fig. 9.1. The graphs in the top row illustrate the reconstruction of a 1D function and down-
sampling by the factor 2. The bottom row illustrates the data used that achieve this: the original
discrete signal and the sampled ﬁlter
1. Figure 9.1 top, left illustrates a sampled discrete signal (green stems) along with
the original (green solid). The dashed magenta curves show the interpolation
functions that can be generated by amplifying the interpolation function with
the corresponding function values. With the magenta solid curve we represent
the reconstructed signal, i.e., the signal that is obtained from the samples by sum-
ming up the dashed magenta curves. The reconstructed signal (magenta curve)
is a version of the original which lacks rapid variations. We wish to sample the
original at a larger step size than the distance between the shown samples. How-
ever, a larger step size means smaller repetition period in the frequency domain,
meaning that high frequencies must be deleted before the repetition takes place
(i.e., sampling in the time domain). If we do not suppress the high frequencies,
we will introduce undesired artifacts to the resulting discrete signal. To achieve
this implicit smoothing effect, the interpolation function must be chosen twice
as wide as it needs to be to reconstruct the original signal.
2. On the top, right of the same ﬁgure, we show with magenta samples the desired
values, which represent the samples of the lowpass ﬁltered reconstructed signal
using twice as large a discretization step as the original discretization.
3. On the bottom we show how this down-sampling is achieved by processing only
discrete signal samples. On the left we show the discrete signal to be down-
sampled. On the right we show the sampled interpolation function. The down-
sampled signal (shown as the magenta samples in top, right) is obtained by con-

124
9 Scales and Frequency Channels
Fig. 9.2. The top row illustrates the reconstruction of a 1D function (as in Fig. 9.1) and its
up-sampling by the factor 2. The bottom row illustrates the data used that achieves this: the
original discrete signal and the sampled ﬁlter
volving the two discrete signals at the bottom and deleting every second. The
deleted samples do not need to be computed if efﬁciency is cared for in the im-
plementation.
Example 9.2. In this example, Fig. 9.2, we illustrate the up-sampling of a 1D signal
with the factor 2. The goal is to keep the same descriptive power in the resulting
samples, because we will double the number of the samples compared to the original
sampling.
1. The top, left graph illustrates the same sampled discrete signal (green stems)
along with the original (green solid) as shown in Fig. 9.1. However, this time the
dashed magenta curves show that the interpolation functions that are narrower
(magenta curve) are able to reproduce the original signal more faithfully. We
wish to sample the original at a smaller step size than the distance between the
given samples, and we should therefore not lose the existing richness of the
variations.
2. On the top, right of the same ﬁgure we show with magenta samples the desired
values, which represent the samples of the reconstructed signal using twice as
small discretization step as the one shown on top, left.
3. On the bottom we show how this up-sampling can be implemented by process-
ing only discrete signal samples. On the left we show a slightly different version
of the discrete signal to be up-sampled. The difference consists in that midway
between the original signal sample positions we have inserted zeros. On the right

9.2 The Gaussian as Interpolator
125
we show the sampled interpolation function to be used in the up-sampling. The
up-sampled signal (shown as the magenta samples in top, right) is obtained by
convolving the two discrete signals shown at the bottom. Evidently, multipli-
cations with zeros do not need to be executed if efﬁciency is cared for in the
implementation.
9.2 The Gaussian as Interpolator
When resampling a discrete signal, its continuous version is resampled. The continu-
ous signal is in turn obtained by placing the interpolation functions at the grid points
and summing them up with weights which are the corresponding discrete signal val-
ues. Although this happens in the experimenter’s imagination, the implications of
it in practice are sizeable because this reasoning inﬂuences the discrete ﬁlters to be
used. Here it is important to note that we generally do not know the original signal
but only know its samples. Because of this, any reconstruction is a guess translates
to making an assumption on, and a motivation for the interpolation function.
The interpolation function that is commanded by the band-limited signal theory
in conjunction with Fourier transform theory is the sinc(t) function, Eq. (6.27). As
pointed out in Sect. 8.1, this choice brings us to the frontier where practical interest
conﬂicts with purely theoretical reasoning. A reconciliation can be reached by not
insisting on the strict “limitedness” in the frequency domain in exchange for smaller
and direction-isotropic ﬁlters (in 2D and higher dimensions). We keep the idea of
displaced tentlike functions as a way to reconstruct the original function, but these
functions are subjected to a different condition than strict band-limited requirement.
Instead, the latter will be replaced by smoothness and compactness in both domains,
i.e., we will require from the interpolation function μ0 that not only μ0 but also its
FT is “compact”, in addition to being “smooth”.
We will suggest a Gaussian to be used as an interpolator, primarily for reasons
called for by 2D and higher dimensional image analysis applications. However, to
elucidate why the Gaussian family is interesting, we will use 1D in the discussions,
for simplicity.
Deﬁnition 9.1. With 0 < σ2 < ∞, the functions :
g(x) =
1
√
2πσ2 exp(−x2
2σ2 )
(9.11)
and their ampliﬁed versions A · g(x), where A is a constant, constitute a family of
functions, called Gaussians.
The Gaussian in Eq. (9.11) is positive and has an area of 1, guaranteed by the constant
factor
1
√
2πσ2 . These qualities make it a probability distribution function, the Normal
distribution.
A Gaussian with a ﬁxed3 σ is a decreasing function that converges to zero with
increased x. A property of the Gaussian family is that it is invariant under the Fourier
3 With increasing σ a Gaussian converges functionally to the constant A.

126
9 Scales and Frequency Channels
transform,
F {g(x)} = G(ω) = exp

−
ω2
2(1/σ)2

(9.12)
meaning that the result is still a Gaussian, albeit with an inverted σ and a different
constant factor. It is a function that has a “minimal width”, even after Fourier trans-
formation. To clarify what this “minimal width” means, we need to be precise about
what width means. To that end we ﬁrst recall that the graph of any function (not just
Gaussians) f(x) shrinks compared to f(ax) with regard to the x-axis if a is real
and 1 < a. Conversely, it will dilate if 0 < a < 1. Second, if we have the Fourier
transform pair f(x), F(ω) then the function pair f(ax), F( 1
aω), with a being real
and nonzero, is also a Fourier transform pair. In other words, if f shrinks with regard
to the x-axis, its Fourier transform will dilate with regard to ω.
In case f is the uniform distribution, e.g., the characteristic function in Fig. 6.3
left, we know how to interpret the width; we just need to measure the length between
the two “ends” of the graph as we measure the length of a brick. The problem with
the widths of general functions is that they may not have clearly distinguishable ends.
The not-so-farfetched example is the Gaussian family. Therefore one needs to bring
further precision to what is meant by the width of a function.
Assuming that the function is integrable, one way to measure the width of a
function f(x), is via the square root of its variance
Δ(f) =
 ∞
−∞
(x −m0)2 ˜f(x)dx
 1
2
(9.13)
where
˜f(x) = |f(x)|/
 ∞
−∞
|f(x)|dx
(9.14)
and
m0 =
 ∞
−∞
x ˜f(x)dx
(9.15)
The “˜” applied to f sees to it that ˜f becomes a probability distribution function, by
taking the magnitude of f and normalizing its area to 1. Because f can be negative
or complex in some applications, the magnitude operator is needed to guarantee that
the result will be positive or zero. After this “conversion”, the integral measures the
ordinary variance of a probability distribution function.
We can see by inspection that Δ(f(αx)) and Δ(F( ω
α)) will be inversely propor-
tional to each other to the effect that their product will equal to a constant γ that is
independent of α:
γ = Δ[f(αx)] · Δ
 
F
ω
α
!
(9.16)
The exact value of γ depends on f, and thereby also on F. As a direct consequence
of Cauchy–Bunyakovski–Schwartz inequality, it can be shown that
1 ≤γ = Δ(f) · Δ(F)
(9.17)

9.3 Optimizing the Gaussian Interpolator
127
In 1920s, when investigating the nature of matter in quantum mechanics, it was
discovered via this inequality that joint measurement accuracies of certain physically
observable pairs, e.g., position and momentum, time and energy, are limited [103].
Later, Gabor used similar ideas in information science and showed that the joint
time–frequency plane is also granular and cannot be sampled with smaller areas
than the above inequality affords [77]. The principle obtained by interpreting the
inequality came to be known as Heisenberg uncertainty, the uncertainty principle or
even as the indeterminacy principle. The principle is a cornerstone of both quantum
mechanics and information science.
In the case of a Gaussian, we have Δ(g) = σ because Eq. (9.11) is already
a known probability distribution function having the variance σ2. Likewise, ˜G is a
normal distribution with variance 1/σ2, yielding Δ(g) = 1/σ. Consequently, the
uncertainty inequality is fulﬁlled with equality by the Gaussian family. For signal
analysis, this outcome means that the Gaussian pair is the most compact Fourier
transform pair. Because of this, extracting a local signal around a point by multiply-
ing with a “window” signal will always introduce high-frequency components to the
extracted signal. The Gaussian windows will produce the most compact local signal
with the least amount of contributions from the high-frequency components. This
is also useful when integrating (averaging) a local signal. Using another function
than Gaussian with the same width will yield an average that is more signiﬁcantly
inﬂuenced by the high frequencies.
9.3 Optimizing the Gaussian Interpolator
Although we have not been precise about the interpolator we used, in Sect. 9.1 we
showed the principles of how the continuous interpolators perform up- and down-
sampling of the discrete signals. Furthermore, we suggested using the Gaussians as
interpolators in the previous section, but we did not precisely state how the variance
σ2 should be chosen as a function of κ, which we will discuss next.
In down-sampling, the Gaussian will not only be used as an interpolator, but at
the same time also it will be used as a lowpass ﬁlter to reduce the high frequencies
while retaining the low frequencies as intactly as possible. Therefore, a Gaussian
must mimic or approximate the characteristic function:
χ(ω) =

1,
−B
2 < ω < B
2
0, otherwise.
To ﬁx the ideas, we choose B/2, determining the symmetric band of interest around
the DC component, as B
2 = π
2 . The quantity B is the bandwidth of interest that we
wish to retain, which includes also the negative frequencies. The particular choice of
B above allows a size reduction of the signal with the factor κ = 2π
B = 2. In Fig. 9.3
the correponding characteristic (box) function is shown. The Gaussian
G(ω) = exp

−
ω2
2(σ′)2

(9.18)

128
9 Scales and Frequency Channels
5
4
3
2
1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
a
Fig. 9.3. A Gaussian function (black), plotted with the χ function (red) with B
2 =
π
2 . Blue
lines indicate the distance a used when estimating the ||E||∞norm
that will approximate χ is given in the same graph. Both χ and G have ω as input
variables to show that they are in the frequency domain. The value at the origin
G(0) = 1 guarantees that subsampling of at least the constant signal will be perfectly
accomplished, in that the signal value will be unchanged. We will vary σ′ to obtain
the function G that is least dissimilar to χ.
To obtain a dissimilarity measure, we deﬁne ﬁrst the error signal:
e(ω) = G(ω) −χ(ω)
Because the width of the function G depends on the constant σ′, the norm of the
error signal will depend on σ′. We will use the Lk norm:
∥f∥k =
 ∞
−∞
|f(x)|kdx
1/k
to minimize the dissimilarity
min
σ′ ∥e∥k = min
σ′ ∥G −χ∥k
(9.19)
The higher the value of k the higher the impact of extremes will be on ∥f∥k. If
we take k towards the inﬁnity, then only the maximum of |f| will inﬂuence ∥f∥k,
yielding:
||f||∞= max |f|
(9.20)
For this reason ∥· ∥∞is also called the max norm.

9.3 Optimizing the Gaussian Interpolator
129
Using this norm to measure the amount of dissimilarity, we obtain
∥e∥∞= max
ω
|G(ω) −χ(ω)| = max(a, 1 −a)
(9.21)
where a is in the interval ]0, 1[ and is deﬁned geometrically as in Fig. 9.3. The ﬁgure
illustrates a Gaussian with a speciﬁc σ′, demonstrating that the ||e||∞is determined
by the intersection of the line ω = π
2 with the graph of G(ω) because this is where the
highest absolute values of the difference will occur. The norm equals the maximum
of the pair a and 1−a, as depicted in the ﬁgure. By varying σ′, the intersection point
can be varied with the expectation that we will ﬁnd a low value for max(a, 1 −a).
The lowest possible norm is evidently obtained for σ′
0, producing a = 0.5 i.e.,
G(π
2 ) = exp

−( π
2 )2
2(σ′)2

= 1
2
(9.22)
so that the analytic solution,
σ′
0 =
π

log(256)
≈1.3
(9.23)
is the solution for the nonlinear minimization problem given in Eq. (9.19) when k
approaches ∞. In the spatial domain this corresponds to a Gaussian ﬁlter with
σ0 =

log(256)
π
≈0.75
(9.24)
Accordingly, the Gaussian with maximum 1 that approximates the constant function
1, the best in the L∞norm, is the one placed in the middle and that attenuates 50%
at the characteristic function boundaries.
The used norm affects the optimal parameter selection. Had we chosen another
value for k in the Lk norm, we would obtain another value for σ′
0, yielding a different
Gaussian. This is one of the reasons why there is not a unique down-sampling (or up-
sampling) ﬁlter: because the outcome depends on what norm one chooses to measure
the dissimilarity with.
We study here just how much the choice of the error metrics inﬂuences the σ′
0
that determines the width of the Gaussian. The minimization of Eq. (9.19) can be
achieved for ﬁnite k numerically. The values of σ′
0 that yield minimal dissimilari-
ties measured in different norms are listed in Table 9.1 when the cutoff frequency is
B
2 = π
2 . Column σ0 represents the corresponding standard deviations of the ﬁlters
in the spatial domain, with σ0 = 1/σ′
0. The table shows that σ0 varies in the interval
[1, 1.3], where the upper bound is given by Eq. (9.24). This suggests that there is
a reasonably large interval from which σ′
0 can be picked up to depreciate the high
frequencies sufﬁciently while not depreciating the low frequencies too severely for
down-sampling. The σ′
0 determines the ﬁlter G in the frequency domain. The cor-
responding ﬁlter in the spatial domain is given by the g(x) in Eq. (9.11), where the
optimal σ0 is the inverse of the σ′
0. Table 9.1 also lists the σ0 values that determine

130
9 Scales and Frequency Channels
Table 9.1. Gaussians approximating an ideal characteristic function
Cutoff frequency: π
2
Norm σ0 [x-domain] σ′
0 [ω-domain]
L1
0.98
1.0
L2
0.82
1.2
L3
0.78
1.3
L∞
0.75
1.3
the spatial domain ﬁlters. Drawn in the frequency domain, Fig. 9.4 illustrates the
corresponding Gaussians and the characteristic function they approximate.
It can be shown that for cases when B has values other than π, corresponding to
a size reduction of 2π
B , we obtain the critical frequency domain standard deviation
parameter as
σ′
0 ∈

0.75
B

log(256)
,
B

log(256)

≈

0.65B
2 , 0.85B
2

⇒
σ0′ ≲0.85B
2
The interval above is an upper bound for σ′
0, for the loosest sampling afforded by the
bandwidth B. The critical upper bound on the right originates from the upper bound
of the interval, which is obtained by using the L∞norm and observing that one has
the option to sample denser than the loosest allowable sampling. Accordingly, the
lower bound for the spatial ﬁlter parameter for the same size reduction factor yields:
σ0 ∈

0.372π
B , 0.52π
B

=

0.75T
2 , T
2

⇒
0.75T
2 ≲σ0
Here T = 2π
B is the critical sampling period, which is also the critical size reduction
factor. This is because, throughout, we assumed that the original sampling period is
1, implying that the maximum frequency (Nyquist) is normalized to π.
9.4 Extending Gaussians to Higher Dimensions
Gaussians are valuable to image analysis for a number of reasons. In the previous
sections, we studied how well it is possible to ﬁt the Gaussian to the characteristic
function in the frequency domain and why the Gaussian family should be used in-
stead of the sinc function family. Although in 1D we suggested Gaussians, there are
many other function families that are used in practice for a myriad of reasons,4. To
appreciate the use of the Gaussian family of functions in signal analysis, it is more
appropriate to study them in higher dimensions than 1. We ﬁrst extend their deﬁnition
to include Gaussians in N dimensions:
4 Some frequently encountered 1D ﬁlter families include Chebychev, Butterworth, and co-
sine functions.

9.4 Extending Gaussians to Higher Dimensions
131
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
-B/2
0.5
B/2
1
2
3
Infinity
Fig. 9.4. The characteristic function (red) plotted with Gaussians optimally approximating it
using the norms L1 (blue), L2(cyan), L3 (green), and L∞(black). The bandwidth is B = π
g(x) = a exp

−1
2xT C−1x

where C is an N ×N symmetric, positive, deﬁnite5 (covariance) matrix, and x is the
N-dimensional coordinate vector. The one-dimensional Gaussian is a special case
of this function, with the major difference being that we now have more variance
parameters (one σ for each dimension and also covariances), which are encoded in C.
Two properties in particular speak in favor of Gaussians: separability and directional
indifference (isotropy).
Separability
The separability property stems from the fact that ND Gaussians can always be
factored out into N one-dimensional Gaussians, in the following manner:
g(x) = a
N
"
i=1
exp

−(xT vi)2
2σ2
i

= a
N
"
i=1
exp

−(yi)2
2σ2
i

(9.25)
where vi is the ith unit-length eigenvector of C, and (σ2
i ) is the ith eigenvalue of C.
The vector y = (y1, y2, · · · , yN)t is given by y = Qx. Convolving an ND image
f(x) with the ND Gaussian can therefore be achieved by ﬁrst rotating the image:
5 Positive deﬁnite symmetric matrices can always be decomposed as C = QΣQ, where Q
is an orthogonal matrix containing the unit-length eigenvectors of C in its columns, and
Σ is the diagonal matrix containing the eigenvalues, which are all positive. Orthogonal
matrices fulﬁll QT Q = I , which expresses the orthogonality of the eigenvectors.

132
9 Scales and Frequency Channels
Fig. 9.5. FMTEST image, which consists of frequency-modulated planar waves in all direc-
tions
f(x) = f(QT y) = ˜f(y)
(9.26)
and then convolving the rotated image separably. In this case, the 1D Gaussians
having the arguments yi, the right-hand side of Eq. (9.25), as discussed in Section
7.3.
Directional Isotropy
It is easy to equip the Gaussian ﬁlter with another desirable property, isotropy. From
Eq. (9.25) one can conclude that if σ1 = σ2 = · · · = σN = σ then we can write:
g(x) = a
N
"
i=1
exp

−(xT vi)2
2σ

= a
N
"
i=1
exp

−(yi)2
2σ2
i

= a exp

−∥y∥2
2σ2

(9.27)
Consequently, Gaussians can not only be made separable in ND, but they can also
be made fully invariant to rotations, i.e., the function values depend only on the
distance to the origin. Gaussians are the only functions that are both separable and
rotationally invariant, in 2D and higher dimensions. Used as ﬁlters, this property
allows us to identically weight all frequencies having the same distance to the origin
in the ω-domain. This is signiﬁcant to image analysis because there is no reason to
systematically disfavor a direction compared to the others.
Example 9.3. We illustrate isotropy in image analysis operators by using an omni-
directional test image consisting of frequency-modulated planar waves in Fig. 9.5.
We compare Gaussians with sinc functions when they are used as lowpass ﬁlters.

9.4 Extending Gaussians to Higher Dimensions
133
Fig. 9.6. Lowpass ﬁltering on a test image using both a Gaussian and a sinc. Left: ﬁltering
using a 2D Gaussian, with σ = 0.75, corresponding to the black graph in Fig. 9.4. Right:
ﬁltering using a 2D Sinc, corresponding to the red graph in Fig. 9.4
Such an operation is frequently needed in image analysis, e.g to change the size of
an image. Both ﬁlter families are separable in that they can be implemented as a
cascade of 1D convolutions. However, as illustrated by Fig. 9.6, the sinc ﬁlter does
not treat all directions equally when it suppresses high frequencies, introducing arti-
facts. By contrast, the Gaussian ﬁlter, being the only ﬁlter that is both separable and
rotation-invariant, does not suffer from this.
Example 9.4. Here we illustrate the process of down- and up-sampling of an image
by using Gaussians as interpolation functions.
•
In Fig. 9.7 we show a graph representing an image where red x-marks represent
pixels. The image will be reduced by a factor of 2. At every second row and col-
umn, a blue ring shows where the interpolation functions are placed and a scalar
is computed by the scalar product of the local image and the interpolation image.
These scalars represent the pixel values of the reduced image. The concentric
magenta rings show the isovalues of the interpolation function. The rings show
the loci where the (continuous) interpolation function, with σ = 0.8, reaches 0.5
(dashed) and 0.01 (solid) levels, respectively. The interpolation function can be
sampled at points marked with ×, and it can be truncated at the level of the solid
magenta ring.
•
In Fig. 9.8 we show another drawing representing an image that has been en-
larged by a factor 2. The red x-marks represent the pixels copied from the original
(smaller) image, whereas at every second row and column, the blue dots mark the
pixels that are initially put to zero. The interpolation function convolves this im-
age to deliver the ﬁnal enlarged image, via local scalar products. The concentric
magenta rings show the isovalues of the interpolation function. The rings show

134
9 Scales and Frequency Channels
Fig. 9.7. The graph represents the down-sampling of an image with the factor 2. The crosses
with rings show the placement of the interpolation function where the scalar products between
the ﬁlter and the image data are computed. Two isocurves of such an interpolation function
are shown by two concentric circles (magenta)
the loci where the (continuous) interpolation function with σ = 0.8 reaches 0.5
(dashed) and 0.01 (solid) levels, respectively.
9.5 Gaussian and Laplacian Pyramids
Useful shape properties describing size and granularity can be extracted for numer-
ous applications by scale space [128,141,152,221,230] techniques, of which image
pyramids [43,160] are among the most efﬁcient to compute. The Gaussian pyramid
of an image is a hierarchy of lowpass ﬁltered versions of the original image, such that
successive levels correspond to lower frequencies. Because of the effectuated low-
pass ﬁltering one can reduce the image size without loss of information. The lowpass
ﬁltering is done using convolution with either a Gaussian ﬁlter or similar. Where the
ﬁlter overhangs the image boundaries one can reﬂect the image about its boundary.
There are, however, other possibilities to avoid the artiﬁcial irregularities the bound-
aries cause, e.g., extension with zeros. The cut-off frequency of the Gaussian ﬁlters
can be controlled using the parameter σ, offering different size reduction rates, κ,
which refers to the reduction rate an image boundary, e.g., the image width in 2D. If
the reduction rate is κ = 2 then the pyramid is called an octave pyramid. One may

9.5 Gaussian and Laplacian Pyramids
135
Fig. 9.8. The up-sampling of an image with the factor 2. Dots and crosses show where the
interpolation functions are placed. The dots represent zero values, whereas crosses represent
the data of the original grid, which has fewer samples than the new grid consisting of crosses
and dots. Two isocurves of the interpolation function are shown by two concentric circles
(dotted and solid)
deﬁne the resizing operator that effectuates the down-sampling as R↓, which is a ﬁl-
tering followed by dropping appropriate number of pixels in each of the dimensions,
e.g., both rowwise and columnwise in 2D. For a Gaussian kernel g and an image f,
R↓is deﬁned as:
(R↓f)(r′
i) =

m
g(rm)f(r′
i + rm),
with
r′
i ∈NR↓⊂N.
(9.28)
Here N is the regular discrete grid on which the original image f is sampled and
from which rm is drawn. The points deﬁning NR↓are a subset of N obtained by
retaining every κ row and column of it. Consequently, rm tessellates a ﬁner grid
compared to r′
i. In Fig. 9.7, which shows the case with the down-sampling rate of
κ = 2, and in 2D, NR↓is illustrated by the encircled grid points, whereas the points
of N are marked with crosses. Applying the operator R↓to f k times:
Rk
↓f = R↓· · · R↓R↓f
(9.29)
yields the kth level in the pyramid, with k = 0 deﬁning the original image f.
The Laplacian pyramid of an image is obtained from the Gaussian pyramid. It
consists of a hierarchy of images such that successive levels of the Laplacian pyra-

136
9 Scales and Frequency Channels
mid correspond to the differences of two successive layers of the Gaussian pyramid.
Because the layers of the Gaussian pyramid have different sizes, one may deﬁne the
resizing operator that yields the up-sampling as R↑. This is a lowpass ﬁltering ap-
plied to an image after having injected an appropriate number of points in each of
the dimensions having zero image values. The kernel is the same kernel g used when
building the Gaussian pyramid. Assuming that the image to be up-sampled is f, then
the R↑is deﬁned as:
(R↑f)(r′
i) =

m
g(rm)f(r′
i + rm),
with
r′
i ∈NR↑⊃N.
(9.30)
As before, N is the regular discrete grid on which the image f is originally sampled
and from which rm is drawn. The NR↑is now a superset of N and is determined in
such a way that when every κ row and column of it is retained then N is obtained.
In other words, rm tessellates a sparser grid compared to r′
i. In Fig. 9.8, which illus-
trates the up-sampling rate of κ = 2 in 2D, NR↓is illustrated by all of the grid points
(blue dots as well as red crosses), whereas the points of N stand out as red crosses.
Applying the operator R↑to f k times:
Rk
↑f = R↑· · · R↑R↑f
(9.31)
yields the kth level in the pyramid, with k = 0 deﬁning the lowest level correspond-
ing to the highest resolution.
Example 9.5. In Fig. 9.9, we illustrate the Gaussian pyramid in 2D for κ = 2 as
layers of images, stacked on top of another.
•
The Gaussian pyramid is shown on the right of Fig. 9.9. The largest image of the
Gaussian pyramid containing all frequencies is marked as level 0 of the pyramid.
Each level is obtained by lowpass ﬁltering the level below and retaining every
second row and column of the result. The ﬁlter used was a Gaussian with σ =
0.75.
•
The Laplacian pyramid is shown on the left of Fig. 9.9 . The largest image of
the Laplacian pyramid containing the highest frequencies is level 0. Each level is
obtained by the difference of two levels of the Gaussian pyramid. Before subtrac-
tion, the smaller of the two images is up-sampled by injecting zeros and lowpass
ﬁltering using a Gaussian ﬁlter with σ = 0.75.
•
In Fig. 9.10 the concentric annuli show the effective frequency bands captured by
each level of the Laplacian pyramid. The color code matches the corresponding
level of the pyramid. The ﬁgure also shows which bands the Gaussian pyramid
retains. Level 0 of the Gaussian pyramid corresponds to the full circle precisely
containing the annulus marked with 0. Level 1 corresponds to the full circle pre-
cisely containing the annulus marked with 1, and so on.
9.6 Discrete Local Spectrum, Gabor Filters
For convenience, we start this section by discussing 1D time–frequency sampling.
Music scores, which are a way to encode music, illustrate time–frequency sampling.

9.6 Discrete Local Spectrum, Gabor Filters
137
f0 f1
1 f2
f2
3
f1
f
f2
f3
Laplacian pyramid
Gaussian pyramid
−f
−
−
f
0
Fig. 9.9. Gaussian and Laplacian pyramids applied to a texture patch image, shown in bottom,
right
Although forms of notational aids to remember music existed even in ancient Greek,
Judeo-Aramaic as well as Oriental traditions, western music notation offering elabo-
rate tone and duration representation came to dominate. This notation is traced back
to pre-medieval ages. In its modern version, a sequence of tones with duration on a
time axis represents the music, as illustrated by Fig. 9.11. The printed music score is
brought to life through musical instruments producing an air pressure variation that

138
9 Scales and Frequency Channels
L 0
L 1
L 2
L 3
Fig. 9.10. The concentric annuli illustrate the effective frequency bands captured by different
levels of a Laplacian pyramid
can be represented by f(t). There are symbols for each tone, and the time durations
of the tones are part of the symbols. Hence a given tone is played for a certain dura-
tion, followed by another tone with its duration, and so on. This way of bringing to
life a 1D function f is radically different than telling how much air pressure should
be produced at a given time, i.e., a straightforward time sampling of f(t). The sam-
pled joint time–frequency representations of functions have relatively recently been
given a formal mathematical frame [10,53,77,203,214]. This is remarkable because,
for several centuries humans have been synthesizing and analyzing certain music sig-
nals by using sequences of tones chosen from a limited set, differing from each other
either in their (basic) frequencies or durations. Below, we discuss time–frequency
sampling concept in further detail. We subsequently extend these results to 2D and
higher dimensional images.
Let f(t) be a 1D function and w(t) be a window function with limited support,
where the coordinate t varies continuously in ] −∞, ∞[ .
Deﬁnition 9.2. The local function around t0 is f ′(t0, t) = f(t −t0)w(t), and the
local spectrum is deﬁned as the Fourier transform of the local function f′:
F ′(t0, ω0) =

f(t −t0)w(t) exp(−iω0t)dt
= ⟨w(t) exp(iω0t), f(t −t0)⟩
(9.32)

9.6 Discrete Local Spectrum, Gabor Filters
139
G234 ˇ ÚÚ
ˇ ˘
ˇ ææ
ˇ ˘
ˇ Č ˇ
ˇ
ˇ
˘
ˇ
ˇ‌
ÈÈˇ ˇ
ÈÈ
ˇ
ˇ` ¨
-
ˇ ˇ
ˇ
ÈÈ
ˇ ˇł
ˇ
Fig. 9.11. Music scores are represented and interpreted as a sequence of time-limited musical
tones chosen from a ﬁnite set
As originally proven by Gabor [77], for a Gaussian window function w, it is possible
to discretize F ′(t0, ω0) w.r.t. t0 and ω0 such that f as well as F can be recovered, up
to a lowpass ﬁltering, from the samples. The samples corresponding to the same t0
constitute the discrete local spectrum. It is also possible to come to this conclusion
by using our results in the previous sections. From the deﬁnition, the local spectrum
around t0 equals:
F ′(t0, ω0) = ⟨w(t) exp(itω0), f(t −t0)⟩
= 2π⟨W(ω −ω0), F(ω) exp(−it0ω)⟩
(9.33)
where we have used theorem 7.2 (Parseval–Plancherel). In consequence, we can
write
F ′(t0, ω0) =

f(t −t0)w(t) exp(−iω0t)dt
(9.34)
= 2π

W(ω −ω0)F(ω) exp(−it0ω)dω
(9.35)
For t0 = 0, Eq. (9.35) represents a lowpass ﬁltering of the spectrum, F, with the
ﬁlter W. Accordingly, the smoothed spectrum can be sampled with the discretiza-
tion step Ω = 2π
T where T is the effective width of the window, w(t), in the time
domain. However, the smoothed spectrum, F ′(0, ω), continuous in ω, represents the
Fourier transform of the local function around the origin, i.e., f(t)w(t) in Eq. (9.34),
and it can be recovered from the samples, F ′(0, n 2π
T ) precisely because the func-
tion f(t)w(t) has limited extension due to windowing. The origin t = 0 is in no way
unique, because the function F(ω) exp(−it0ω) appearing in Eq. (9.35) is the Fourier
transform of f(t −t0), which is a shifted version of the function such that t0 is the
origin. In consequence, what we have in Eq. (9.35) is a smoothing of the Fourier
transform of the shifted function f(t −t0) with the ﬁlter W(ω) in the Fourier do-
main. Because of this, F ′(t0, ω) can be sampled w.r.t. ω, i.e., F ′(t0, n 2π
T ), where T
corresponds to the effective extension of w(t) and recovered from the samples. Using
the same arguments, and using the symmetry of the Fourier transform, we conclude
that F ′(t, ω0) can be sampled w.r.t. t, i.e., F ′(mT, ω0), which in turn can be recov-
ered from the samples of F ′(mT, n 2π
T ). The discrete local spectrum, also called the
Gabor spectrum, or Gabor decomposition, is computed as a projection onto a ﬁlter
bank either in the spatial domain, as in Eq. (9.36), or in the Fourier domain, as in Eq.
(9.37).

140
9 Scales and Frequency Channels
Fig. 9.12. The graphs of the real (green, solid) and the imaginary (green, dashed) parts of a
Gabor ﬁlter, g(t), in the time domain. The window function w(t) = |g(t)| is drawn in black
F ′(mT, n2π
T ) =

f(t −mT)w(t) exp(−in2π
T t)dt
(9.36)
= 2π

W(ω −n2π
T )F(ω) exp(−imTω)dω
(9.37)
The ﬁlters of the ﬁlter bank are called the Gabor ﬁlters and they are given as follows
in the spatial domain:
gn(t) = w(t) exp(−in2π
T t) =
1
(2πσ2)
1
2 exp(−t2
2σ2 ) exp(−in2π
T t)
(9.38)
with 2σ > T. In Fig. 9.12, the real and the imaginary part of one such Gabor ﬁlter,
gn(t) with n = 2, is shown. The black curve shows |gn(t)|, which is the Gaussian
window. The Fourier transform of the ﬁlter is shown in magenta in Fig. 9.13. The
higher the value of n, the more the ﬁlter function oscillates in the Gaussian window.
In the Fourier domain, Gabor ﬁlters are translated Gaussians:
Gn(ω) = W

ω −n2π
T

= exp

−(ω −n 2π
T )2
2
σ2

(9.39)
Figure 9.13 illustrates Gn(ω) for n = 1 · · · 7 for a set of Gabor ﬁlters uniformly
distributed in the angular frequency range [0.05π, 0.95π]. The ﬁlters are centered
in the seven equally sized frequency cells and attenuate 50% at the cell boundaries.
Note that this amount of overlap is motivated by our ﬁnding in Sect. 9.3, according to
which a Gaussian approximates the characteristic function of its frequency cell best
in the L∞norm when it attenuates at the characteristic function boundaries with
50%. The characteristic function of one frequency cell that is to be approximated by
its underlying Gaussian (magenta) is drawn in black in Fig. 9.13. Note that we only

9.6 Discrete Local Spectrum, Gabor Filters
141
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
0.05π
0.95π
Fig. 9.13. The graphs of a Gabor ﬁlter bank with minimum and maximum angular frequencies
0.1π and 0.9π, respectively. The ﬁlter corresponding to Fig. 9.12 is shown in magenta, where
the black constant illustrates the characteristic function of the corresponding frequency cell
need to have ﬁlters that cover the positive ω-axis in the Gabor ﬁlter bank because we
assumed that the function f is real-valued so that there is no additional information
in the negative axis, compared to the positive axis due to F(ω) = F ∗(−ω). Given
the attenuation at the cell boundaries, for a 1D Gabor ﬁlter bank we thus need the
minimum frequency, the maximum frequency, shown as vertical arrows, as well as
the total number of ﬁlters to determine the ﬁlter bank. The tip of a green arrow and
the closest yellow arrow is half the width of the angular frequency cells.
One can extend these results into 2D and higher dimension in a straightforward
manner. The window function is an ND Gaussian, which has certain advantages,
e.g., direction isotropy in image analysis applications. The resulting Gabor ﬁlters are
given in the spatial domain by
gn(r) = w(t) exp(−iωT
nr) =
1
(2πσ2)
N
2 exp(−∥r∥2
2σ2 ) exp(−iωT
nr)
(9.40)
and in the frequency domain, by
Gn(ω) = W(ω −ωn) = exp(−∥ω −ωn∥2
2σ2ω
)
(9.41)
where σω = 1/σ. Without loss of generality, we assumed that the frequency cells
are axis-parallel hypercubes with equal edge sizes in all directions and that the cell
centers are at ωn. We illustrate the frequency cells of one such Gabor ﬁlter bank in
Fig. 10.28 along with the isocurves of the ﬁlters (green). The DC extracting Gabor

142
9 Scales and Frequency Channels
ﬁlter is a pure Gaussian and has been omitted in the center of the illustration but can
be included in the ﬁlter bank. In certain applications, such as direction estimation,
the DC-ﬁlter is not needed. As before, the ﬁlters need tessellate only one half of the
frequency plane because the image f is assumed to be real.
9.7 Design of Gabor Filters on Nonregular Grids
It has been shown that both in audio and in visual signal processing pathways of
primates the sensitivity is not uniformly distributed across the perceivable frequen-
cies. We start to hear low-frequency audio at lower amplitudes than we do for high-
frequency audio. In images, we start to perceive low spatial-frequencies at lower
contrasts (amplitudes) than high spatial-frequencies. Therefore, noise in high fre-
quencies does not inﬂuence our audio or image quality assessment as much as it
does at the low frequencies of the respective spectra. The nonuniformity in sensitiv-
ity is attributable to the limited signal processing and communication resources, such
as the number of specialized cells, the connections between the cells and the statis-
tics of real-world signals that matter for the organism. Images that humans and other
primates encounter have a decreasing spectral power with increased frequencies. It
is therefore plausible that an organism devotes its limited processing resources in
proportion to their actual use.
As to human-made systems, these observations help to design more efﬁcient sys-
tems, such as in compression of audio and video, where the largest part of the limited
vocabulary represents the frequencies to which humans are most sensitive. Pattern
recognition in audio and video increasingly uses ﬁlters with bandwidths that increase
with tune-on frequencies.6 Applications such as biometric person authentication also
increasingly use such ﬁlter banks. Here, we will discuss Gabor ﬁlter design on mul-
tidimensional and nonuniform grids, [21], to yield higher sensitivies at certain bands
than others.
We illustrate the approach ﬁrst in 1D for convenience, and then we generalize the
approach by extending it to 2D. To reduce the amount of manual labor, we suggest
using an analytic coordinate transformation:
ξ = u(ω),
or
ω = u−1(ξ),
(9.42)
where u is yet to be speciﬁed. The transformation u is an injective reparametrization
of ω such that u is invertible and establishes a differentiable (continuous) map be-
tween ω ∈[ωmin, ωmax] and ξ[0, 1]. In consequence, we require that u satisﬁes the
boundary conditions:
ω(0) = u−1(0) = ωmin
and
ω(1) = u−1(1) = ωmax
(9.43)
Additionally, we demand that there will be higher sensitivity at low frequencies. We
must translate sensitivity to a mathematical concept by deﬁning it as a property of
6 The tune-on frequency of a bandpass ﬁlter is the frequency where the Fourier transform of
the ﬁlter has the maximum amplitude.

9.7 Design of Gabor Filters on Nonregular Grids
143
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
0.05π
0.95π
Fig. 9.14. The Gabor ﬁlters in the Fourier domain plotted against the linear frequency scale.
The green arrows show the used minimum and the maximum frequency parameters
a coordinate transformation, such that a small interval is mapped to a larger interval
through the transformation. To be precise, we want a mapping such that when we di-
vide ξ equally, where each interval has the width δξ, the widths of the corresponding
intervals in ω should be larger and larger in such a way that δω grows proportionally
with ω, i.e.,
δω = Cωδξ
(9.44)
where C is an unknown constant. Effectively, this leads to a differential equation
when we let δξ, and thereby δω (which depends on it), be ever smaller.
δω = Cωδξ
→
dω
dξ = Cω
(9.45)
The differential equation with the boundary condition yields the unique solution
ω = u(ξ) = ωmin exp

log ωmax
ωmin

ξ

(9.46)
or
ω = ωmin ·
ωmax
ωmin
ξ
(9.47)
with the inverse
ξ =

log
ωmax
ωmin
−1
· log
 ω
ωmin

(9.48)
We divide ξ into N equal cells and place the same translated Gaussian in each ξ
cell in analogy with Fig. 9.13 and Eq. (9.41):

144
9 Scales and Frequency Channels
Fig. 9.15. The isocurves of Gabor ﬁlters at cell boundaries. The orientation and frequency
resolution of the decomposition are explicitly controllable because the frequency cells are
designed in the log(z) coordinates
Gn(ξ) = W(ξ −ξn) = exp

−∥ξ −ξn∥2
2σ2
ξ

(9.49)
where ξn and σ2
ξ are tune-on and bandwidth parameters as before, but in the mapped
frequency, i.e., the ξ-domain. These ﬁlters can be remapped to the ω-domain via
Gn[ξ(ω)] = W[ξ(ω) −ξn] = exp

−∥ξ(ω) −ξn∥2
2σ2
ξ

(9.50)
where ξ(ω) is the log function given by Eq. (9.48). As illustrated by Fig. 9.14, the
only difference in this approach is the transformation Eq. (9.48) applied to the ω-
coordinate, which enables the ﬁlters to be designed in the ξ-domain such that they
all become identical and ordinary Gaussians translated to the centers of equally sized
cells. The function Gn(ξ(ω)) vanishes at ω = 0, which guarantees that these ﬁl-
ters have no DC bias, as opposed to Gn(ω) > 0 at ω = 0, discussed in the pre-
vious section. The ﬁlter bank uses the same parameters as the one illustrated by
Fig. 9.13. Note that there are more resources (ﬁlters) devoted to the low-frequency
range where humans are most sensitive. More precisely, the tune-on frequencies, the
cell-boundaries, as well as the bandwidths, deﬁned as the size of cells in ω-domain,
progress geometrically with the factor (ωmax/ωmin)Ωξ, with Ωξ being the cell size.

9.7 Design of Gabor Filters on Nonregular Grids
145
This should be contrasted to progressing arithmetically, with the increment of the cell
size Ω, see Fig. 9.13. The explanation lies in the coordinate transformation, which
does not introduce new ﬁlter values (heights), but rather stretches the ω-axis. The t-
domain versions of these ﬁlters are found by inverse Fourier transforming Gn(ξ(ω))
w.r.t. ω. It is possible to show that functions sampled on nonregular lattices can be
reconstructed from their samples [5]. In consequence, the discretization discussed
here is a representation of the local spectrum.
In 2D, we wish to have a geometric progression of the tune-on frequencies in
the radial direction, whereas we wish to have an arithmetic progression of them in
the angular direction. The latter is easily justiﬁable because, a priori, all directions
appear to have equal importance in human vision as well as in numerous applications
in machine vision. Assuming that ω is the 2D Cartesian coordinate vector in the
Fourier domain, the transformation that has the desired properties is given by the
log–polar coordinates:
ξ =

log
ωmax
ωmin
−1
· log
 ∥ω∥
ωmin

(9.51)
η = 1
π tan−1(ω)
(9.52)
where ξ varies between [0, 1] to the effect that ∥ω∥varies between [ωmin, ωmax], and
tan−1(ω) varies between [−π, π], yielding a variation between [−1, 1] for η.
We divide ξ, η uniformly as in Fig. 10.28 and place Gaussians in the marked cell
centers (ξn, ηn):
Gn(ξ, η) = exp

−|ξ −ξn|2
2σ2
ξ

· exp

−|η −ηn|2
2σ2η

(9.53)
The radial part of this ﬁlter function was previously suggested in [137] when design-
ing quadrature mirror ﬁlters. Note that only one half of the plane needs to be covered
because we assume that the ﬁlter bank will be used to analyze real-valued signals
having F(ω) = F ∗(−ω) only. The ﬁlters, Gn(ξ(ω), η(ω)) can be readily sam-
pled on the original Cartesian grid ωl because ξ(ω) and η(ω) are available via Eqs.
(9.51)–(9.52). We show the isocurves of the resulting ﬁlters at the cell-boundaries for
seven frequencies and seven directions in Fig. 9.15 and mark the tune-on frequen-
cies by dots. The radial cross-section of the ﬁlters through the tune-on frequencies is
shown in Fig. 9.14.
The extension of the approach to ND Gabor ﬁlter banks is possible, provided
that the type of resource allocation scheme is at least roughly known. The latter is
crucial because it determines the coordinate transformations, which are not always
intuitively derivable. For an example, we refer to [21], where Gabor ﬁlters on an
irregular grid in 3D are suggested. The purpose is to mimic the speed sensitivity
of humans to apparent motion in image sequences. For example, using coordinate
transformations one can obtain a decomposition that is specially sensitive to low
spatial frequencies moving quickly and high frequencies moving slowly as compared
to other speed, and spatial frequency combinations.

146
9 Scales and Frequency Channels
0
40
-40
0
40
-40
0
40
0
40
Fig. 9.16. Left: the retinotopic sampling grid with axes graded in pixels. Right: a few receptive
ﬁelds are represented as sets of concentric circles. Adapted from [205].
9.8 Face Recognition by Gabor Filters, an Application
We discuss here a saccadic search strategy, a general-purpose attentional mechanism
that identiﬁes semantically meaningful structures in images by performing “jumps”
(saccades) between relevant locations [205]. The saccade paths are chosen automat-
ically and rely on apriori knowledge of facial features that are modeled by means of
Gabor ﬁlters discussed in the previous sections. Additionally, here they are applied
to a log–polar grid, that is, together with Gabor ﬁlter responses, called the retino-
topic sensor, because each sampling point is not a gray value but is instead an array
of responses coming from Gabor ﬁlters designed in the log–polar frequency plane.
The usefulness of the concept to complex cognitive tasks is demonstrated by facial
landmark detection and identity authentication experiments over the M2VTS and
Extended M2VTS (XM2VTS) databases.
The saccadic search strategy and face recognition are based on a sparse retino-
topic grid obtained by log–polar mapping [196], also called log-z mapping :

ξ = log

x2 + y2
η = tan−1(y, x)
(9.54)
that we will return to in Chap. 11 in the context of other image processing tasks. The
log–polar grid, which is in the spatial domain, is used to sample the original image
but not for extracting the grayvalues. At each log–polar grid point in the image, a
local Gabor decomposition of the image is performed to the effect that they mimic the
simple cells of the primary visual cortex having the same receptive ﬁeld but different
spatial directions and frequencies [113].
The log–polar grid used in the example results discussed here is shown in Fig.
9.16. The simple cells are modeled by computing a vector of 30 Gabor ﬁlter re-

9.8 Face Recognition by Gabor Filters, an Application
147
Fig. 9.17. The local and the extended models of a facial landmark are obtained by centering
the retina on that landmark in the images of the training set. Adapted from [205]
sponses at each point of the retina. The ﬁlters are organised in ﬁve frequency chan-
nels and six equally spaced orientation channels as discussed in Sect. 9.7. Filter
wavelengths span the range from 4 to 16 pixels in half-octave intervals.
The same sensor is used both for facial landmark localization and for person
authentication. The sparse nature of the sensor and the relatively low number of
ﬁxations, thanks to the quick convergence of the saccades to the facial landmarks,
make the computation of Gabor responses by direct ﬁltering in the image domain
more advantageous [205] and feasible even in real time [27].
The saccadic search strategy requires the use of two levels of modeling for each
of the facial landmarks: left eye, right eye, mouth+nose. The latter is a metafeature
that consists in joining the mouth and the nose. The local model is obtained for each
facial landmark as the vector of Gabor responses extracted at the central grid point
at the location of that landmark in the images of a training set. The extended model
is obtained by placing the whole retina at the location of the facial landmark on the
training images (Fig. 9.17) and collecting the set of Gabor ﬁlter responses from all
of the retinal points. The local model and the global model are given to supervised
classiﬁers7 that learn a compact representation of the facial landmarks by examples,
to identify them later in other images.
Facial landmark detection is achieved by a saccadic search, starting with the
retinotopic sensor placed at random on the image. The search initially aims to ﬁnd
any of the three facial landmarks. Gabor response vectors are computed at all reti-
7 Examples of supervised classifers are neural networks [29,49], and Bayes classiﬁers [125,
153]. and support vector machines.

148
9 Scales and Frequency Channels
nal points at a random starting point. The response vectors at each log–polar grid
point on a circle are rated by the local model of the selected landmark according
to the output of the corresponding classiﬁer, floc(vcγ). The retina is subsequently
centered at the grid point in the image that maximizes the similarity provided by the
classiﬁer. This procedure is iterated until the retinotopic sensor is centered on a lo-
cal maximum. One advantage of this search strategy is that the search automatically
becomes ﬁner as a local maximum is approached, since the artiﬁcial retina is denser
at the center (fovea) than at the periphery. As this application demonstrates, the acu-
ity gradient existing between the peripheral and the foveal vision in the topology of
the human retina plays a plausible role in achieving fast convergence to targets by
use of the saccades, homing. After saccades have converged to a local optimum, the
retinotopic grid is displaced in a pixel-by-pixel fashion to maximize the output of the
more accurate, but computationally heavier, extended model for the detected facial
landmark. Matches that score less in classiﬁer-provided comparisons are discarded
at this stage.
Once a match for a facial landmark has been found, a saccade to the average
assumed location of one of the others is performed. An attempt at detection is made
directly with the corresponding extended model. If this fails, the search is restarted
at random to look for this feature.
A global conﬁguration score is computed based exclusively on the quality of
the matches detected. Saccadic search is continued until a complete set of facial
landmarks that has a very high conﬁguration score is found. In Fig. 9.18, the perfor-
mance of this technique is illustrated by way of examples. These images belong to
the XM2VTS database [162]. In 99.5% of the database images, at least two facial
features were correctly positioned. Following the facial feature detection, an authen-
tication of the found face against a reference or client person can be performed by
comparing the measured features of the global models for all three facial features
with those of the client. This can be done by the same classiﬁer that was used to
locate the eyes and mouth+nose, but adapted to the identity of the person, yielding
0.5% false acceptance at 0.5% false rejection threshold. The details of the system
and its performance are presented in [205].

9.8 Face Recognition by Gabor Filters, an Application
149
Fig. 9.18. Eye and mouth localization by retinotopic sensor and saccades. Performance in the
presence of eyeglasses (1st row), partial occlusion (2nd row), pose changes (3rd row), facial
hair (4th row), and ethnic diversity (5th row) are illustrated. Adapted from [205]

Part III
Vision of Single Direction
Humility is the barrier against all evils.
St. Isaac (died 7th century)

10
Direction in 2D
Directional processing of visual signals is the largest single analysis toolbox of mam-
malian visual system: it feeds other specialized visual processing areas [114, 173,
235], e.g., face recognition. Directional analysis is gaining increased traction even
in computer vision, as it moves from single-problem-solving systems towards multi-
problem-solving platforms. Nearly all applications of image analysis now have al-
ternatives using direction tensor ﬁelds. The necessary tools are more modern and
offer advanced low-level signal processing that was hitherto reserved to processing
of high-level tokens, such as binarized or skeletonized edge maps. In 2D, the earliest
solutions to the problem of ﬁnding the direction of an image patch, e.g., [51, 116],
consisted in projecting the image onto a number of ﬁxed orthogonal functions. The
projection coefﬁcients were then used to evaluate the orientation parameter of the
model. When the number of ﬁlters used is increased, the local image is described
better and better, but the inverse function, mapping the coefﬁcients to the optimal
orientation, increase greatly in complexity. A generalization of the inverse projec-
tion approach to higher dimensions becomes therefore computationally prohibitive.
Here we will follow a different approach by modeling the shapes of isocurves via
tensors.
10.1 Linearly Symmetric Images
We will refer to a small 2D image patch around a point as an image, to the effect
that we will treat the local image patches in the same way as the global image. Let
the scalar function f, taking a two-dimensional vector r = (x, y)T as argument,
represent an image. Assume that fr = (x, y)T is a two-dimensional real vector that
represents the coordinates of a point in a plane on which an image f(x, y) is deﬁned.
Furthermore, assume that k = (kx, ky)T is a two-dimensional unit vector represent-
ing a constant direction in the plane of the image.
Deﬁnition 10.1. The function f is called a linearly symmetric image if its isocurves
have a common direction, i.e., there exists a scalar function of one variable g such
that

154
10 Direction in 2D
−80
−60
−40
−20
0
20
40
60
80
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 10.1. The graph represents the 1D function g(t) = sin(ωt) that will be used to construct
a linearly symmetric 2D function
f(x, y) = g(kT r) = g(kxx + kyy)
(10.1)
The direction of the linear symmetry is ±k.
The term is justiﬁed in that F, the 2D Fourier transform of f, is concentrated to a line,
as will be shown below. In addition, all isocurves of linearly symmetric functions are
lines that have a common direction k, i.e., they are parallel to each other. Note that the
term isocurves refers to the fact that the values of g and thereby f are invariant when
one moves along certain curves in the argument domain. For linearly symmetric
images, these curves are lines.
It should be noted that while g(t) is a function of one free variable, g(ktr) is a
function of two free variables, (x, y)T since k is constant. In the rest of this section
we will assume that the argument domain of f is two-dimensional, whereas that of g
is one-dimensional and g is a “constructor” of f via Eq. (10.1) and k whenever f is
linearly symmetric. Therefore g(kT r) generates an image despite that g by itself is a
one-dimensional function. By deﬁnition, images with the linear symmetry property
have the same gray value at all points r satisfying kT r = C for a given value C.
Because kT r = C describes a line in the (x, y)-plane, it follows that along this line
the gray values of the image do not change and this gray value equals to g(C). In such
images, the only occasion when g can change is when the argument of g changes,
i.e., when the constant C assumes another value. However, the curves kT r = C1,
kT r = C2, ... kT r = Cn, with Ci being different constants, represent lines that
are shifted versions of each other, all having the same direction, k. Consequently,
the one-dimensional function g is a proﬁle of the two-dimensional function g(kT r)
along any line perpendicular to the line kT r = C.
Local images, which can be extracted by multiplying the original image with an
appropriate window function, are, from mathematical viewpoint, no different than
the larger original image from which these local images are “cut”. For notational

10.1 Linearly Symmetric Images
155
Fig. 10.2. In top, left, the image generated by g(t) = sin(ωt) with the argument t = kT r
is given. The solid and dashed vectors represent k and −k respectively. The 3D graph in
top, right represents g(kT r) as a surface. The FT magnitudes of g(t) and g(kT r) are shown
in bottom, left and bottom, right. The FT coordinates are in the angular frequencies ω and
(ωx, ωy)T
simplicity, we will therefore not make a distinction between an image and a local
neighborhood of it. Both variants will be referred to as an image here, unless an
ambiguity calls for further precision.
Below we detail the process of constructing linearly symmetric images from 1D
functions ﬁrst by three examples of 1D functions g, that are continuous. The last one
of these will model an ideal line. Then we will study a discontinuous g which will
be a model of ideal edges. Both ideal lines and ideal edges have been used to model
and to detect discontinuities in image processing.

156
10 Direction in 2D
−60
−40
−20
0
20
40
60
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 10.3. The graph represents the sinc function, Eq. (10.4)
Example 10.1. The function
g(t) = sin(ωt),
with
ω = 2π
12 ,
(10.2)
rendered in Fig. 10.1, oscillates around zero. This sinusoid is used to construct the
linearly symmetric 2D function, shown in Fig. 10.2 (top, left). To render the negative
values of g we added the constant 0.5 and rescaled the gray values to the effect
that white represents 1 and black represents −1 by using 256 gray tones and linear
mapping. The dashed green line through origin shows an example isocurve. We recall
that an isocurve is the curve that joins all points having a certain gray value. The
example isocurve in the constructed 2D image is clearly a line, along which the
gray value of the image is the same. In fact, every other isocurve is also a line and
all isocurves are parallel, just as they should be, because of the way the image is
constructed.
By construction, the gray value of the image cannot change unless the argument
of g changes. For any given image location r, the argument of g will equal a constant
value C
kT r = C
(10.3)
to the effect that it is possible to restrict the changes of r to a curve so that C is invari-
ant. By virtue of Eq. (10.3), this path is a line, and the equation represents parallel
lines when k is ﬁxed. We can move in the image, i.e., change r, along a line which
is perpendicular to the line shown as the dashed diagonal, i.e along k, and obtain
changes in the value of C, which in turn changes the values of g. In this path, the ob-
tained function or gray values are identical to the values of the original 1D function,
g(t) = cos(ωt). The solid green arrow illustrates the vector k = (cos( π
4 ), sin( π
4 ))T
used to build the image. The representation is not unique because −k (dashed) would
have generated the same image. In other words, the k used to construct the linearly
symmetric image is unique only up to a sign factor ±1. The color surface in 3D shows

10.1 Linearly Symmetric Images
157
the same image as a landscape, illustrating that the gray variations are identical to
those in Fig. 10.1 across the isocurves.
The magnitudes of the Fourier transforms of g(t) and g(kT x) are also given in
Fig. 10.2 (bottom). The red color represents the value zero in the image. The bright
(yellowish) spots represent the largest values. In the image, the 2D Fourier transform
magnitudes clearly equal zero outside of the two bright points.
Example 10.2. The 1D sinc function
g(t) = sin(ωt)
ωt
,
with
ω = 2π
12 ,
(10.4)
is plotted in Fig. 10.3. The synthetic image represented by the function g(kT r) is
linearly symmetric. Its image is illustrated by the gray image in Fig. 10.4 (top) which
differs from the one in Fig. 10.2 only by the choice of g. The function values are
scaled and shifted to be rendered by the available 256 gray tones. The solid and
the dashed vectors represent k and −k respectively. Both this image and the gray
image in Fig. 10.2 have the same vector k = (cos( π
4 ), sin( π
4 ))T , which represents
the direction orthogonal to the isocurve direction. In the direction of k, any cross
section of the image is identical to the sinc function of Fig. 10.3, as illustrated by the
3D graph in Fig. 10.4, which shows g(kT r) as a surface.
In the (2D) color image, we note that the magnitudes of the Fourier transformed
function equal zero (red color) outside of a line passing through the origin, indicated
by bright yellow in Fig. 10.4 (bottom, right). The line has the direction k. Along
the line, the Fourier transform magnitude has the same shape as the (1D) Fourier
transform magnitude (bottom, left).
We will bring further precision to the relationship between the 1D and 2D Fourier
transforms of the linearly symmetric functions below. For now we note the result of
this example, as illustrated by Fig. 10.5 bottom, right, is consistent with that of the
previous example, Fig. 10.2 bottom, right. Both Fourier transform magnitudes vanish
outside a central line having the direction k, whereas on the line itself both magni-
tudes have at least the same magnitude variations as their 1D counterparts shown on
the respective left. From the magnitudes of Fourier transformed functions, we can
in general not deduce the underlying complex values. However, there is one excep-
tion to that which is a result of the null property of norms, i.e., the magnitudes of
complex numbers are zero if and only if the complex numbers are zero. The Fourier
transforms of the two illustrated example images possessing linear symmetry must
consequently have not only magnitudes but also complex values that equal zero out-
side the referenced line.
The second example actually showed the same sinusoid as in the ﬁrst one, with
the difference that in the second example, the sinusoid attenuates gradually as 1/t.
The Fourier transform magnitude of the Sinc example is therefore more spread as
compared to that of the pure sinusoid, which consists of a pair of Dirac pulses.
The sinusoid is neither a pure line nor a pure edge, but yet it has a direction. The
classical edge and line detection techniques in image processing model and detect

158
10 Direction in 2D
Fig. 10.4. (Top) The gray image is generated by substituting t = kT r in Eq. (10.4). The 3D
graph shows g(kT r). (Bottom) The 1D and 2D FT magnitudes of g(t) and g(kT r), respec-
tively
pure lines and edges, without a provision for other types of patterns that have well-
deﬁned directions. In the next example, we show that pure lines can be modeled as a
linearly symmetric function generated by means of an analytic function, a Gaussian.
Example 10.3. The 1D Gaussian
g(t) = exp

−t2
2σ2

,
with
σ = 3,
(10.5)
is plotted as the green curve in Fig. 10.5. The synthetic image represented by the
function g(kT r) is linearly symmetric and is illustrated by the gray image in Fig.
10.5. The function values are scaled and linearly mapped to 256 gray tones, with 0
corresponding to black, and 1 corresponding to white. In the direction of k, any cross
section of the image is identical to the 1D Gaussian of Fig. 10.5.

10.1 Linearly Symmetric Images
159
−40
−30
−20
−10
0
10
20
30
40
0
0.2
0.4
0.6
0.8
1
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
Fig. 10.5. (Top) The graph represents the 1D Gaussian g(t) in Eq. (10.5) and the 2D function
generated by the substitution t = kT r. The solid and dashed vectors represent k and −k
respectively. (Bottom) The 1D FT magnitude of g(t) and the 2D FT magnitude of g(kT r) are
illustrated by the (left) graphics and the (right) image, respectively
When we study the 2D Fourier transform magnitudes of this linearly symmetric
image, Fig. 10.5 bottom, right, we note that it too equals to zero (red) outside of the
same central line (bright yellow) as in the previous two examples. The line has a
proﬁle matching the 1D version of the Fourier transform magnitude, shown in the
bottom, left graph.
Example 10.4. The 1D step function
g(t) =

1,
if t ≥0,
0,
otherwise,
(10.6)

160
10 Direction in 2D
−80
−60
−40
−20
0
20
40
60
80
0
0.2
0.4
0.6
0.8
1
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
Fig. 10.6. (Top) The graph represents the 1D step function g(t) in Eq. (10.6) and the 2D
function generated by the substitution t = kT r. The solid and dashed vectors represent k and
−k, respectively. (Bottom) The 1D FT magnitude of g(t) and the 2D FT magnitude of g(kT r)
are illustrated by the (left) graphics and the (right) image, respectively
is discontinuous. It is shown as the green curve in Fig. 10.6 with its corresponding
linearly symmetric gray image, which is obtained by sampling
g(kT r) =

1,
ifkxx + kyy ≥0,
0,
otherwise.
(10.7)
The gray image models what came to be known as the ideal edge in image process-
ing. Despite the fact that it is discontinuous, it too is a linearly symmetric function,
with Fourier transform magnitudes concentrated to the same central line as in the
previous examples.

10.1 Linearly Symmetric Images
161
Fig. 10.7. In real images, linear symmetries often resemble “lines” or “edges” but they appear
even as object or texture boundaries. Some of these are illustrated by colored lines in this
photograph
Example 10.5. The linearly symmetric images appear frequently as local images,
both in images of human-made environments and in images of the nature. They are
often perceived as lines or edges (Fig. 10.7), or as repetitive patterns called tex-
tures (Fig. 10.8). Although the cross sections of their isocurves are seldom like the
ideal lines and edges, they usually have a well-distinguishable direction, with Fourier
transform magnitudes concentrated to a line, as Fig. 10.8 shows.
We give precision to the example indications regarding the Fourier transform of
the linearly symmetric images in the following lemma. We recall that the argument
domain of g is one-dimensional, whereas that of f is two-dimensional by the adopted
convention.
Lemma 10.1. A linearly symmetric image f(r) = g(kT r) has a 2D Fourier trans-
form concentrated to a line through the origin:
F(ωx, ωy) = G(kT ω)δ(uT ω)
(10.8)
where k, u are orthogonal vectors and δ is the Dirac distribution. The vector ω is
the angular frequency vector ω = (ωx, ωy)T . The function G is the one-dimensional
Fourier transform of g.
♦

162
10 Direction in 2D
Fig. 10.8. Left: a real image that is linearly symmetric. It shows a close-up view of the blinds.
Right: The Fourier transform magnitude of a neighborhood in the central part of the original
(brightness) image. Notice that the power is concentrated to a line orthogonal to isocurves in
the original
Lemma 10.1 is proven in the Appendix of this chapter, Sect 10.17. It states that the
function g(kT r), which is in general a “spread” function such as a sinusoid or an
edge, is compressed to a line having the direction k in the Fourier domain. Even
more important, it says that as far as k is concerned the choice of G, and thereby
g, has no signiﬁcance. This is because, k, the angle at which all nonzero F reside,
remains the same no matter what G is. This is achieved by the Dirac pulse δ, which
becomes a line pulse along the inﬁnite line uT ω = 0 by the expression δ(uT ω).
Because u is the normal direction of this line and u is orthogonal to k, the direction
of the spectral line uT ω = 0 coincides with the vector k. We have already observed
this line in red-colored images of Examples 10.1–10.5, as a concentration of the
magnitudes to the central line, in the same direction as the same (green) k vector
shown in the gray images, regardless g.
Along this central spectral line, not only the magnitude but also the complex
values conform to that of the 1D Fourier transform. This is because G is the 1D
Fourier transform of g, and Eq. (10.8) is a formula for how to produce the 2D Fourier
transform of the linearly symmetric functions only from the 1D Fourier transform G
and the isocurve normal k. According to the lemma, the vector u can always be
deduced from k up to a sign factor, because it is orthogonal to k. Due to limitations
of the illustration methods, Examples 10.1–10.5 could only be indicative about this
more powerful result, and this only as far as the Fourier transform magnitudes are
concerned.
Consequently, to determine whether or not an image is linearly symmetric is the
same thing as to quantitate to what extent the Fourier transform vanishes outside of
a line, a property which will be exploited to construct computer algorithms below.
Such algorithms can be constructed conveniently to describe textures possessing a

10.2 Real and Complex Moments in 2D
163
direction, or by detecting the lack of linear symmetry, to describe textures lacking
direction. Measuring the lack of linear symmetries has been frequently used as a
way of detecting corners in image processing.
10.2 Real and Complex Moments in 2D
In image analysis there are a variety of occasions when we need to quantitate func-
tions by comparing them to other functions. Assuming that the integral exists, the
quantity
mpq(κ) = ⟨xpyq, κ⟩=
 
xpyqκ(x, y)dxdy
(10.9)
with p and q being nonnegative integers, is the real moment p, q of the function κ.
If κ has a ﬁnite extension, then the real moments deﬁned as above are projections
of an integrable function onto the vector space of polynomials. It follows from the
Weierstrass theorem [193,209], that the vector space of the polynomials is powerful
enough to approximate a ﬁnite extension function κ to a desired degree of accuracy.
In that, the approximation property of moments is comparable to the FCs, although
the polynomial basis of moments is not orthogonal, whereas the Fourier basis is.
Nonetheless, moments are widely used in applications. If κ is a positive function
then it is possible to view it as a probability distribution, after a normalization with
m00. Accordingly,
¯c = (¯x, ¯y)T =
1
m00
(m10, m01)T
(10.10)
represents the centroid or the mean vector of the function κ. The quantity
˜mpq(κ) = ⟨

x −m10
m00
p 
y −m01
m00
q
, κ⟩
=
  
x −m10
m00
p 
y −m01
m00
q
κ(x, y)dxdy
(10.11)
related to real moments, is called the central moment p, q of the function κ. Both real
moments and central moments have been utilized as tools to quantitate the shape of
a ﬁnite extension image region. We will discuss this further in Sect. 17.4.
Another type of moment, which we will favor over real moments in what follows,
is
Ipq(κ) = ⟨(x −iy)p(x + iy)q, κ⟩=
 
(x + iy)p(x −iy)qκ(x, y)dxdy (10.12)
with p and q being nonnegative integers. This is the complex moment p, q of the func-
tion κ. Notice that complex moments are linear combinations of the real moments,
e.g.,
I20 = m20 −m02 + i2m11
I11 = m20 + m02

164
10 Direction in 2D
The order number and the symmetry number of a complex moment refer to p+q and
p−q, respectively. The integrals above should be interpreted as summations when the
complex moments of discrete functions are to be computed. In the following sections
we will make use of (real and complex) moments as a spectral regression tool, i.e.,
to ﬁt a line to the FT of a function.
10.3 The Structure Tensor in 2D
The structure tensor1 or the direction tensor models linearly symmetric structures
that are frequently found in images. To represent certain geometric properties of
images, it associates 2 × 2 symmetric matrices that are tensors to them. This is not
different from the fact that in a color image there are several color components per
image point, e.g., HSB, to every point. Typically, however, the structure tensor is used
to quantify shape properties of local images. As such, structure tensors are assigned
to every image point to represent properties of neighborhoods.
Let the scalar function f(r), taking the two-dimensional vector r = (x, y)T as ar-
gument, represent an image, which is usually a neighborhood around an image point.
As before, the (capitalized) letter F is the Fourier transform of f. We denote with
|F(ω)| the magnitude spectrum of f, where ω = (ωx, ωy)T is the Fourier transform
coordinates in angular frequencies, and with |F(ω)|2 we denote the power spectrum
of f. We will use the power spectrum rather than |F| to measure the signiﬁcance of a
given frequency in the signal because it will turn out that the average values of |F|2
are easier to measure in practice than |F|.
The direction of a linearly symmetric function f(r) = g(kT r) is well-deﬁned by
the vector k, but only up to a sign factor. According to lemma 10.1, if and only if f is
linearly symmetric is its power spectrum, |F|2 concentrated to a central line with the
direction k. The direction of this line represents the direction of the linear symmetry.
We will approach estimating k by ﬁtting the image power spectrum, |F|2, a line in
the total least square TLS sense. Consequently, it will be possible to “measure” if
f is linearly symmetric by studying the error of the ﬁt. If the error is “small” in the
sense that has been deﬁned, then our method will take this as a provision that the ﬁt
was successful and that the image approximates a linearly symmetric image g(kT r)
well. It turns out that, in this procedure, g need not be known beforehand. It will be
automatically determined when the error of the ﬁt is near zero, because we will then
obtain a reliable direction along which to “cut” the image. In turn the 1D function
obtained by cutting the image is g only if f is linearly symmetric. Owing to the
continuity of the TLS error function, the decrease or the increase of the error will be
graceful when f approaches to a linearly symmetric function or departs from one.
We discuss the details of the line-ﬁtting next.
We wish to ﬁt an axis through the origin of the Fourier transform of an image, f,
which may or may not be linearly symmetric. Fitting an axis to a ﬁnite set of points
is classically performed by minimizing the error function:
1 Other names of this tensor include the “second order moment tensor”, “inertia tensor”,
“outer product tensor”, and “covariance matrix”.

10.3 The Structure Tensor in 2D
165
|F|2
y
ω
x
ω
ω
Fig. 10.9. The line-ﬁtting process is illustrated by the linear symmetry direction vector k,
the angular frequency vector ω, and the distance vector d. The function values |F(ω)|2 are
represented by color. The frequency coordinate vector is ω
e(k) =

ω
d2(ω, k)
(10.13)
where d(ω, k) is the shortest distance between a point ω and a candidate axis k.
This is the TLS error function for a discrete data set. Noting that ∥k∥= 1, then the
projection of ω on the vector k is (ωtk)k. As illustrated by Fig. 10.9, the vector d
represents the difference between ω and the projection of ω. This difference vector
is orthogonal to k
d = ω −(ωtk)k
(10.14)
with its norm being equal to the shortest distance, i.e., ∥d∥= d(ω, k). Consequently,
the square of the norm of d provides:
d2(ω, k) = ∥ω −(ωtk)k∥2
(10.15)
=
#
ω −(ωtk)k
$T #
ω −(ωtk)k
$
(10.16)
Since we have a Fourier transform function, F, deﬁned on dense angular fre-
quency coordinates in E2, instead of a sparse point set, Eq. (10.13) needs to be
modiﬁed. The following error function is a generalization of Eq. (10.13) to dense
point sets. It weights the squared distance contribution at an angular frequency point
ω with the energy |F(ω)|2 and integrates all error contributions
e(k) =

d2(ω, k)|F(ω)|2dω
(10.17)

166
10 Direction in 2D
where dω equal to dωxdωy, and the integral is a double integral over E2. The ex-
pression deﬁnes the TLS error function for a continuous data set. Interpreting the
integral as a summation, Eq. (10.13) is a special case of Eq. (10.17). By construc-
tion, the contribution of F(ω) to the error is zero for points ω along the line tk. With
increased distance of ω to the latter axis, the contribution of F(ω) will be ampliﬁed.
If |F(ω)|2, which is the spectral energy, is interpreted as the mass density, then
the error e(k) corresponds to the inertia of a mass with respect to the axis k in
mechanics [83]. Besides this function being continuous in k, it has also a screening
effect; that is, F is concentrated to a line if and only if e(k) vanishes for some k.
This is to be expected because the resulting quantity when substituting Eq. (10.16)
in Eq. (10.17) is the square norm of a vector-valued function, i.e.,
e(k) = ∥
#
ω −(ωT k)k
$
F∥2 =

∥
#
ω −(ωT k)k
$
∥2|F(ω)|2dω
(10.18)
Because e(k) is the square norm of a function in a Hilbert space,
e(k) = ∥
#
ω −(ωT k)k
$
F∥2 = 0
⇒
#
ω −(ωT k)k
$
F = 0
(10.19)
Consequently, when e(k) = 0, the expression
#
ω −(ωT k)k
$
F equals (the vector-
valued function) zero. Assuming F ̸= 0, this will happen if and only if F equals zero
outside the line tk. A nontrivial F can thus be nonzero only on the line tk, which,
according to lemma 10.1, means that f must be linearly symmetric. Conversely, if F
is zero except on the line tk, the corresponding e(k) vanishes. To summarize, if and
only if e(k) = 0, all spectral energy, |F|2, is concentrated to a line and f is linearly
symmetric with the direction ±k. Leaving the question whether or not ±k always
translates to an unambiguous direction aside (we will discuss this further below), we
proceed to the more urgent question of how to calculate k.
Noting that ωT k is a scalar that is indistinguishable from the scalar kT ω, and
∥k∥2 = kT k = 1, the square magnitude distance can be written in quadratic form:
d2(ω, k) = kT #
IωT ω −ωωT $
k
(10.20)
= kT
ω2
x + ω2
y
0
0
ω2
x + ω2
y

−
 ω2
x
ωxωy
ωxωy
ω2
y

k
(10.21)
Deﬁning the components of ω = (ωx, ωy)T as
ωx = ω1,
and
ωy = ω2,
(10.22)
for notational convenience, Eq. (10.17) is expressed as
e(k) = kT Jk = kT (I · Trace(S) −S)k
(10.23)
where I is the identity matrix,
J = I · Trace(S) −S
(10.24)
and

10.3 The Structure Tensor in 2D
167
S =

ωωT |F|2dω =

S(1, 1) S(1, 2)
S(2, 1) S(2, 2)

,
with
S(i, j) =

ωiωj|F(ω)|2dω
(10.25)
Deﬁnition 10.2. The matrix S in Eq. (10.25), which consists of the second-order
moments of the power spectrum, |F|2, is called the structure tensor of the image f.
The matrix J is the inertia tensor of the power spectrum using a term of mechan-
ics. The matrix S is also called the scatter tensor of the power spectrum in statistics.
The structure tensor can be readily obtained from J, and vice versa via Eq. (10.24).
There is another related tensor called the covariance tensor or the covariance matrix
in statistics, C = S −m · mT , where m =

ω|F|2dω. However, for real images
m = 0, since |F| is even when f is real. Because of the tight relationship between the
notions inertia, scatter, and covariance, they are used in an interchangeable manner
in many contexts. Since different notions of the structure tensor coexist, the follow-
ing lemma, which establishes the equivalence of J and S (and of C ), is useful to
remember.
Lemma 10.2. With eigenvalue, eigenvector pairs of J being {λ1, u1} and {λ2, u2},
and those of S being {λ′
1, u′
1} and {λ′
2, u′
2}, we have
{λ′
1, u′
1} = {λ1, u2},
and
{λ′
2, u′
2} = {λ2, u1}.
(10.26)
♦
The eigenvector with a certain eigenvalue in the ﬁrst matrix is an eigenvector with
the other eigenvalue in the second matrix. The lemma can be proven by utilizing Eq.
(10.24) and operating with J on u′
i, which is assumed to be an eigenvector of S:
Ju′
i = (I · Trace(S) −S)u′
i = (λ1 + λ2)u′
i −λiu′
i
i = 1, 2
(10.27)
The error minimization problem formulated in Eq. (10.17) is reduced to a min-
imization of a quadratic form, kT Jk with the matrix J given by Eq. (10.24). This
is in turn minimized by choosing k as the least eigenvector of the inertia matrix,
J [231]. All eigenvalues of J are real and nonnegative because the error expres-
sion Eq. (10.17) is real and nonnegative. Calling the eigenvalue and eigenvector
pairs of J {λmin, kmin} and {λmax, kmax}, the minimum of e(k) will occur at
e(kmin) = λmin. In other words, the matrix J, or equivalently S, contains sufﬁ-
cient information to allow the computation of the optimal k in the TLS error sense.
We will discuss the motivation behind this choice of error in some detail in Sect.
10.10.
The matrix S is deﬁned in the frequency domain, which is inconvenient, partic-
ularly if S must be estimated numerous times. For example, when computing the
direction for all local patches of an image, we would need to perform numerous
Fourier transformations if we attempt to directly estimate the structure tensor from
its deﬁnition. We can, however, eliminate the need for a Fourier transformation by
utilizing (Parseval–Plancherel) theorem 7.2 which states that the scalar products are

168
10 Direction in 2D
conserved under the Fourier transform. Applying it to Eq. (10.28), the computation
of the matrix elements will be lifted from the Fourier domain to the spatial domain:
S(i, j) =

ωiωj|F(ω)|2dω =
1
4π2

∂f
∂xi
∂f
∂xj
dx
i, j : 1, 2
(10.28)
where x1 =x, x2 =y, dx=dxdy, and the integral is a double integral over the entire
2D plane. This is rephrased in matrix form,
S =

ωωT |F|2dω =
1
4π2

(∇f)(∇Tf)dx
(10.29)
where
∇f =

Dxf, Dyf)T = (∂f(r)
∂x , ∂f(r)
∂y
T
.
(10.30)
We summarize our ﬁnding on 2D direction estimation via the following theorem,
where the integrals are double integrals taken over the 2D spatial domain.
Theorem 10.1 (Structure tensor I). The extremal inertia axes of the power spec-
trum, |F|2 are determined by the eigenvectors of the structure tensor:
S =
1
4π2

(∇f)(∇Tf)dx
(10.31)
=
1
4π2


(Dxf)2dx

(Dxf)(Dyf)dx

(Dxf)(Dyf)dx

(Dyf)2dx

(10.32)
The eigenvalues λmin, λmax and the corresponding eigenvectors kmin, kmax of the
tensor represent the minimum inertia, the maximum inertia, the axis of the maximum
inertia, and the axis of the minimum inertia of the power spectrum, respectively.
♦
We note that kmin is the least eigenvector, but it represents the axis of the maximum
inertia. This is because the inertia tensor J is tightly related to the scatter tensor S
according to lemma 10.2. The two tensors share eigenvalues in 2D, although the
correspondence between the eigenvalues and the eigenvectors is reversed.
While the major eigenvector of S ﬁts the minimum inertia axis to the power spec-
trum, the image itself does not need to be Fourier transformed according to the the-
orem. The eigenvalue λmax represents the largest inertia or error, which is achieved
with the inertia axis having the direction kmin. The worst error is useful too, because
it indicates the scale of the error when judging the size of the smallest error, λmin
(the range problem). By contrast, the axis of the maximum inertia provides no addi-
tional information, because it is always orthogonal to the minimum inertia axis as a
consequence of the matrix S being symmetric and positive semideﬁnite.
10.4 The Complex Representation of the Structure Tensor
Estimating the structure tensor, and thereby the direction of an image, can be sim-
pliﬁed further by utilizing the algebraic properties of the complex z-plane. Multi-
plication and division are well-deﬁned in complex numbers, whereas conventional

10.4 The Complex Representation of the Structure Tensor
169
vectors cannot be multiplied or divided with each other to yield new vectors. As a
result, an explicit computation of the matrix eigenvalues will become superﬂuous,
and the structure tensor will be automatically decomposed into a directional and a
nondirectional part.
Central to the structure tensor theory is the maximization of the scatter kT Sk.
Because S is a positive semideﬁnite matrix with real elements, i.e., S = ATA for
some A having real coefﬁcients, the scatter is (Ak)T (Ak) and is either zero or pos-
itive for any real vector k. By incorporating the complex conjugation into transposi-
tion, i.e., BH = (B∗)T , the Hermitian transposition, the expression (Ak)H(Ak) =
kHSk will be either zero or positive even if the vector k has been expressed in a ba-
sis that has complex elements. Here, we will maximize kHSk, assuming k may have
complex elements. First, we introduce a new basis that has complex vector elements,
using the unitary matrix: UH
k′ = UHk,
where
UH =
1
√
2

1 i
i 1

,
and
U =
1
√
2

1 −i
−i 1

. (10.33)
Unitary matrices generalize orthogonal matrices in that a unitary matrix has in gen-
eral complex elements, and it obeys the relationship UHU = UUH = I. Conse-
quently,
kHSk = (Uk′)HSUk′ = k′HZk′
(10.34)
where
Z = UHSU
(10.35)
= 1
2

S(1, 1) + S(2, 2)
−i(S(1, 1) −S(2, 2) + i2S(1, 2))
i(S(1, 1) −S(2, 2) + i2S(1, 2))∗
S(1, 1) + S(2, 2)

We call Z the complex structure tensor, and we can conclude that it represents the
same tensor as S, except for a basis change, and that both matrices have common
eigenvalues. They share also eigenvectors, but only up to the unitary transformation,
so that kZ representing an eigenvector of Z is given by kZ = UHkS, with kS being
an eigenvector of S. We deﬁne the elements of Z via the complex quantities I20 and
I11 as follows.
Deﬁnition 10.3. The matrix
Z = 1
2

I11 −iI20
iI∗
20
I11

,
where
I20 = S(1, 1) −S(2, 2) + i2S(1, 2)
I11 = S(1, 1) + S(2, 2)
(10.36)
is the complex representation of the structure tensor.
A matrix Z is called Hermitian if ZH = Z, and if additionally kHZk ≥0, which is
the case by deﬁnition for the complex structure tensor, is called Hermitian positive
semideﬁnite. With the above representation, the elements of Z encode the λmax,
λmin as well as kmax more explicitly than S. This is summarized in the following
theorem [28], the proof of which is found in Sect. 10.17.

170
10 Direction in 2D
Theorem 10.2 (Structure tensor II). The minimum and the maximum inertia as
well as the axis of minimum inertia of the power spectrum are given by
I20 = (λmax −λmin)ei2ϕmin =
1
4π2

ℸ(f)(x, y)dx
(10.37)
I11 = λmax + λmin =
1
4π2

|ℸ(f)(x, y)|dx
(10.38)
with the inﬁnitesimal linear symmetry tensor (ILST) deﬁned as2
ℸx,y(f)(x, y) =
∂f
∂x + i∂f
∂y
2
(10.39)
The quantities λmin, λmax, and ϕmin are, respectively, the minimum inertia, the max-
imum inertia, and the axis of the minimum inertia of the power spectrum.
♦
The eigenvalues of the tensor in theorem 10.1 and the λ’s appearing in this theorem
are identical. Likewise, the direction of the major eigenvector kmax and the ϕmin,
of theorem 10.2 coincide. Accordingly, the eigenvector information is encoded ex-
plicitly in an offdiagonal element of Z, i.e., I20 whereas the sum and the difference
of the eigenvalues are encoded in the diagonal element as I11 and in the offdiagonal
element as |I20|, respectively.
For completeness, we provide the eigenvectors of Z as well. Because the ar-
gument angle of I20 is twice the direction angle of kmax, the direction of latter is
obtained by the direction of the square root of I20. The eigenvectors of S are related
to the eigenvectors of Z via Eq. (10.33), so that we obtain
k′
max = γ(

I20, i

I∗
20)T
(10.40)
k′
min = γ(

−I20, i

−I∗
20)T
(10.41)
where γ is a scalar that normalizes the norms of the vectors to 1.
Summary of the complex structure tensor
•
Independence under merging. Averaging the “square” of the complex ﬁeld
(Dxf + iDyf)2 and its magnitude (scalar) ﬁeld |Dxf + iDyf|2, automatically
ﬁts an optimal axis to the spectrum,
•
Schwartz inequality. The inequality |I20| ≤I11 holds with equality if and only
if the image is linearly symmetric,
•
Rotation-invariance and covariance. If the image is rotated, the absolute values
of the elements of Z, i.e., |I20| as well as I11, will be invariant to the rotation,
while the argument of I20 will change linearly with the rotation.
In the next two sections we discuss two simpler tensors that will be used as basis
tensors for decomposing the structure tensor.
2 The symbol ℸis pronounced as “doleth” or “daleth”, which is intended to be a mnemonic
for the fact that it is not an ordinary gradient delivering a vector constisting of derivatives
but is a (complex) scalar comprised of derivatives.

10.6 Balanced Direction Tensor: Directional Equilibrium
171
10.5 Linear Symmetry Tensor: Directional Dominance
In this section we will mean the complex structure tensor when we refer to the struc-
ture tensor. An “ideal” linear symmetry is present in the image, when λmax >> 0
and λmin = 0. Such images have a directional dominance in that there is a single
and well-identiﬁed direction of isocurves. One way to quantitate this property is by
measuring λmax −λmin and kmax, which are jointly given by the complex scalar
I20 [28]. When λmax −λmin increases, so does the evidence for the image being lin-
early symmetric, and hence we have a crisp direction in the image. For Hermitian3
positive semi deﬁnite matrices, which includes the structure tensor Z, the dimension
of the eigenvector space is equal to the multiplicity of the corresponding eigenvalue,
which is the number of times the latter is repeated. The eigenvectors are orthogonal
if they belong to two different eigenvalues. Because there are at most two different
eigenvalues in 2D, there is no need to encode both eigenvectors. The linear symmetry
quality of a 2D image has also been called the “line” property [225], and the “stick”
property [161]. The linear symmetry tensor is a special type of structure tensor such
that
ZL = 1
2

|I20| −iI20
iI∗
20 |I20|

(10.42)
The tensor is fully equivalent to the scalar quantity I20, which determines ZL
uniquely, which is, in turn, a spatial average of ℸ(f):
10.6 Balanced Direction Tensor: Directional Equilibrium
Certain images lack direction, i.e., when a direction is attempted to be ﬁt to the
power spectrum there is not one optimal axis but there are an inﬁnite (uncountable)
number of them, e.g., the image of sand in Fig. 10.13 or the image in Fig. 10.10.
This property is captured by the structure tensor via the condition λmax = λmin, i.e.,
the smallest (or the largest) eigenvalue is repeated twice making its multiplicity 2.
The condition actually does not describe the presence of a property but the lack of
it. It describes the lack of linear symmetry. An image with a structure tensor having
λmax = λmin, has previously been called “perfectly balanced”, in analogy with the
terminology used in mechanics [28]. The term expresses that there is a directional
equilibrium in that no single direction dominates over the others. Such images lack a
single direction4 that is more signiﬁcant than other directions, a property that justiﬁes
the use of the term “balanced directions” or “balancedness”, both referring to an
equilibrium of directions. Balancedness is quantitated by λmin because, for a ﬁxed
λmax, the larger λmin, the closer it gets to λmax. When λmin reaches its upper bound,
which is λmax, then the structure tensor has one eigenvalue which has a multiplicity
2. The least eigenvalue λmin can be used to signal the presence of a balanced image,
3 We recall that a Hermitian matrix Z fulﬁlls Z = ZH.
4 The pattern may still have a group direction, although it may lack a single direction domi-
nating others, e.g., see Fig. 10.10.

172
10 Direction in 2D
−pi/12
0
pi/12
−pi/12
0
pi/12
−pi/12
0
pi/12
−pi/12
0
pi/12
Fig. 10.10. (Top) A perfectly balanced image f, and its power spectrum, |F|2. The red circles
represent a concentration of the power. (Bottom) The same as on top, except that the image is
rotated with the angle π
4
the state of equilibrium of directions when both eigenvalues are equal. Accordingly,
the balanced direction tensor is given by
ZB = 1
2
I11 0
0 I11

(10.43)
which is a special case of the complex structure tensor. The tensor is completely
equivalent to the scalar quantity I11, which determines ZB uniquely which is in turn
an average of the magnitude of ℸ(f). This tensor has also been called “isotropic”
[225] and being “ball-like” [161] in other studies. The term isotropic should not be
interpreted as an existence of all directions in the image is a necessity. Figure 10.10
shows two images that contain only two directions and yet they are qualiﬁed for the
term. Likewise the term ball tensor should not be interpreted too restrictively such

10.7 Decomposing the Complex Structure Tensor
173
as the image must look like a ball, or a junction. The images in Fig. 10.10 as well
as in Fig. 10.13 represent textures in which at every point there is a “ball” tensor
of approximately the same magnitude. We will discuss the use of balanced direction
tensor as a corner detector in Sect. 10.9.
10.7 Decomposing the Complex Structure Tensor
In general, an image is neither perfectly linearly symmetric, e.g., Figs. 10.2-10.6, nor
does it totally lack it, e.g., Fig. 10.13. Instead it has the qualities of both types. The
amount of evidence for the respective type can be obtained from the structure tensor.
The structure tensor decomposition can always be achieved into its linear symmetry
and balanced direction components easily using its complex form:
Z = 1
2
 I11 −iI20
iI∗
20
I11

= ZL + ZB
(10.44)
where
ZL = 1
2
 |I20| −iI20
iI∗
20 |I20|

,
and
ZB = 1
2
I11 −|I20|
0
0
I11 −|I20|

.
(10.45)
Conversely, we also wish to study what happens when joining regions having dif-
ferent structure tensors. Without loss of generality, we consider a composition of
a region consisting of two subregions, each having a different structure tensor, Z′
and Z′′, respectively. This is a realistic scenario since two neighboring regions in an
image might differ in their local structure tensors, and the local structure tensor at
a border point between the two regions is needed. Because the components of the
structure tensor are integrals, they can be computed as the sum of two integrals, each
taken over the respective regions. Accordingly, the structure tensor, Z, of a boundary
point is obtained by the addition
Z = pZ′ + qZ′′
(10.46)
where p, q are two real positive scalars, with p + q = 1, that are proportional to the
areas of the two constituent regions. Following the deﬁnition of Z, we obtain
I20 = pI′
20 + qI′′
20
(10.47)
I11 = pI′
11 + qI′′
11
(10.48)
where I′
··, I′′
··, and I·· are the structure tensor parameters of the ﬁrst, the second and
the joint patches.
Example 10.6. In Fig. 10.11 we have two regions labelled A, and B. There are four
local images, called images here, and these are marked as 1, · · · , 4 with their borders
shown as (color) circles. Let images 2 and 4 have the (complex) structure tensors Z′
and Z′′, respectively. The corresponding tensor components are therefore

174
10 Direction in 2D
Fig. 10.11. Illustration of addition using the complex structure tensor. The linear symmetry
tensor components I20 are shown as vectors for four local images. The balanced direction
components, I11, are shown as circles ﬁlled with black. The limits of images are marked by
color circles
I′
20 = k′2 = exp(−i π
2 )
I′′
20 = k′′2 = exp(i π
2 )
I′
11 = 1
I′′
11 = 1
In these images, the linear symmetry components point at directions given by k2,
where, k is the direction of the respective gradient (Fig. 10.12, left). The balanced
direction components are equal to zero, because both images are linearly symmetric
so that I11 = |I20|. In image 3 we have the structure tensor Z = 1
2Z′ + 1
2Z′′, having
the components
I′
20 = 1
2 exp(−i π
2 ) + 1
2 exp(i π
2 ) = 0
I′
11 = 1
2 + 1
2 = 1
The linear symmetry component is zero, as it should be. The image is a perfectly
balanced image because none of its constituent directions dominates the others. The
balanced direction tensor element is, by contrast, I11 −|I20| = 1, which indicates
that all spectral power is distributed in such a way that the directions balance each
other perfectly. Conceptually, balanced image phenomenon is present also when the
gradient directions are random (Fig. 10.12, right). Likewise in image 1 we have the
structure tensor Z = 1
4Z′ + 3
4Z′′ having the components
I′
20 = 1
4 exp(−i π
2 ) + 3
4 exp(i π
2 ) = 1
2 exp(i π
2 )
I′
11 = 1
4 + 3
4 = 1
In particular, the balanced direction component, I11−|I20| = 1
2, should be contrasted
to the magnitude of the linear symmetry component |I20| = 1
2. The argument of I20

10.8 Decomposing the Real-Valued Structure Tensor
175
M
M
Fig. 10.12. The linear symmetry component, I20, and the balanced directions component, I11,
of the structure tensor add as vectors and as scalars, independently. The addition of numerous
structure tensors (left) for a merging of local images sharing a common direction, (right) for a
merging that largely lacks such a common direction .
shows the dominant direction of the image. We paraphrase this result as follows.
When the account of directions is ﬁnalized there is an excess of a single direction
that is not balanced by other directions. The net excess of this dominant direction is
as signiﬁcant as the directions that are balanced.
10.8 Decomposing the Real-Valued Structure Tensor
Using its real form, S, the structure tensor decomposition in terms of linear symme-
try and balanced direction components is also possible [161, 225]. This is done by
the spectral decomposition of the tensor S
S = λmaxkmaxkT
max + λminkminkT
min
(10.49)
followed by the rearrangement:
S = (λmax −λmin)kmaxkT
max + λmin(kmaxkT
max + kminkT
min)
= (λmax −λmin)kmaxkT
max + λminI
(10.50)
where the fact that kmaxkT
max + kminkT
min = I, when kmax and kmin are orthogo-
nal, has been used. The ﬁrst term of Eq. (10.50) is the linear symmetry tensor, and
the second term is the balanced direction tensor in real matrix representation. When
merging two images, each consisting of balanced directions, the result is an image
that is perfectly balanced. Accordingly, adding the balanced direction components
of two arbitrary images will not result in a change of the linear symmetry compo-
nents. However, if two linearly symmetric images are merged the result usually has

176
10 Direction in 2D
Fig. 10.13. Left: a real image that is not linearly symmetric. It shows a close up view of sand.
Right: The Fourier transform magnitude of a neighborhood in the central part of the original
image. Notice that the power is isotropic i.e., far from being concentrated to a line
a structure tensor that has a balanced direction component unless both images have
the same direction. Accordingly, the linear symmetry components do not add in a
straightforward fashion in the real matrix representation, causing an “interaction”
contribution to the balanced direction component. In effect, when merging two arbi-
trary images already decomposed into their component tensors, the linear symmetry
components add (matrix addition) ﬁrst according to Eq. (10.50), possibly producing
a balanced direction term. This additional term should then be added to the sum of
the ordinary balanced direction tensor components of the two images.
The decomposition of the complex structure tensor is conserved and closed under
averaging whereas that of the real structure tensor is not. Paraphrasing, averaging lin-
ear symmetry tensors in their complex form yields linear symmetry tensors, whereas
averaging linear symmetry tensors in their real form may produce tensors that are
not linearly symmetric. This is because (i) the two components I20 and I11 add sep-
arately when merging or smoothing images, and (ii) these components are explicitly
linked to eigenvalues and optimal directions.
10.9 Conventional Corners and Balanced Directions
By using other algebraic functions of λmax and λmin, numerous measures to quanti-
tate the amount of linear symmetry can be obtained. Likewise, we can also measure
the lack of symmetry, which is the balanced directions property of an image.
Example measures include the energy invariant measure, Cf2 [28], for linear
symmetry,
Cf2 = |I20|
I11
= λmax −λmin
λmax + λmin
(linear symmetry)
(10.51)

10.10 The Total Least Squares Direction and Tensors
177
which also immediately deﬁnes the energy-invariant measure for the lack of linear
symmetry Cf3:
Cf3 = 1 −|I20|
I11
(balanced directions)
(10.52)
At the heart of such measures is how well the Schwartz inequality, |I20| ≤I11,
is fulﬁlled. The case of equality happens if and only if one has linear symmetry
(|I20| = I11). The left-hand side of it vanishes and the right-hand side becomes as
large as the energy permits it, if and only we have balanced directions (0 = |I20|).
Having this in mind, then other functionals that measure the distance I11 −|I20| can
be used to quantitate the balancedness of the directions in the image. The popular
detector of Harris and Stephen [97] (a similar measure is that of Forstner and Gulch
[74]) used to measure cornerness, quantitates this distance as well
Chs = λmaxλmin −0.04(λmax + λmin)2
(10.53)
= (I11 + |I20|)(I11 −|I20|)/4 −0.04I2
11
= (0.84I2
11 −|I20|2)/4
(10.54)
albeit in the quadratic scale, which is most obvious if the empirical constant 0.84 is
replaced by 1. Because of the constant, the measure Chs must be combined with a
threshold to reject the negative values. This will happen at (local) images that have
only the linear symmetry component (e.g., on lines and edges) where I11 = |I20|,
yielding Chs = −0.04|I20| < 0. The measure Chs responds strongly to many corner
types, including a corner that consists of the junction of two orthogonal directions, or
a corner that consists of the intersection of several lines. A word of caution is in place
because Chs will also respond strongly to other patterns, including at every point in a
texture image that lacks direction. This may be a desirable property for an application
at hand. However, it is also possible that the application is actually unintentionally
accepting (false acceptence) many patterns as corners by using Cf3 or Chs. The
texture images shown in Fig. 10.10 are perfectly balanced everywhere, meaning that
every point is a “balanced directions corner” or “Stephen–Harris corner”. Likewise,
all boundary points, except the boundary corners, between region A and region B
in Fig. 10.11 are the strongest corners in either of the two corner senses above. All
points of these four lines are, in fact, stronger “corners” than the four boundary corner
points, as discussed in Sect. 10.7!
10.10 The Total Least Squares Direction and Tensors
It is in place to ask what makes the matrices J, S, or even Z (second-order) tensors.
We recall that the basic difference between a second-order tensor and a matrix is
subtle and lies in that a tensor represents a physical quantity on which the coordinate
system has no real inﬂuence except for a numerical representation. The numerical
representation of a tensor is then a matrix that corresponds to physical measurements

178
10 Direction in 2D
in a speciﬁc basis. A representation of the same tensor in a different basis can only
be obtained by a similarity transformation using the basis transformation matrix.
For ﬁrst-order tensors and vectors, a similar subtle difference exists. The ﬁrst-order
tensor is represented as a vector in a speciﬁc basis. Another representation of it can be
obtained by a linear transformation corresponding to a basis change. The zero-order
tensors represent physical quantities that are scalars. Their numerical representations
do not change with basis transformations.
As illustrated by Fig. 10.9, the error function e(k) employed by the total least
square (TLS) error represents the spectral power weighted by its shortest (orthog-
onal) distance to the estimated axis k. This makes e a zero-order tensor and k a
ﬁrst-order tensor. If we apply a basis change e.g., rotate the coordinates of the power
spectrum, e(k) will not change at all and only the numerical representation of k will
change. The new direction vector, k′, will be coupled to the old k linearly using the
inverse of the matrix that caused the basis change.
To appreciate the TLS error in this context we compare it to the mean square
(MS) error which is extensively used in applications where one has a black box
controlled by known inputs resulting in a measurable output. In such applications
there is thus a response measurement, y, that may contain measurement errors and
that is to be explained by means of another set of known (error-free) variables, called
explanatory variables X, via a linear model
y = Xk
(10.55)
Here k is the unknown regression parameter, which will be estimated by minimizing
the following residual:
min
k ∥y −Xk∥2
(10.56)
Adapted to our 2D direction estimation problem, the MS error yields:
min
γ
e(γ) =

∥ωy −γωx∥2|F(ωx, ωy)|2dω
(10.57)
This is the classical regression problem. Here, the direction coefﬁcient γ is unknown
and will be estimated from the data F(ωx, ωy). The unknown γ is related to the di-
rection vector k = (cos θ, sin θ)T as γ = −cos θ
sin θ . Notice that the integrand measures
the distance between the data point ω and a point on the k-axis to be ﬁtted. This
distance is in general not the shortest, distance as illustrated by the vector d in Fig.
10.14. The MS error would accordingly depend on the coordinate axis directions to
the effect that after a basis change, the new error using the same data will be dif-
ferent. Likewise, the new direction k′ will not be given by multiplying the inverse
of the basis transformation matrix with k, the estimated direction before the basis
change. In consequence, neither the MS error nor γ are tensors. One can associate
k and kkT to every γ = −cos(θ)
sin(θ) . These quantites are not tensors, although they are
conventional vectors and matrices. A more detailed discussion of the TLS error can
be found in [115] and [59].
Through Taylor expansion a spatial interpretation of e(k), as an alternative to its
original interpretation, the spectral inertia, can be obtained. In this view, the structure

10.10 The Total Least Squares Direction and Tensors
179
|F|2
y
ω
x
ω
ω
Fig. 10.14. The error, ∥d∥2, used in the MS estimate. The error is not measured as the shortest
distance between a frequency coordinate ω and the k-axis. This should be contrasted with the
TLS error, which does measure the shortest distance, as shown in Fig. 10.9
tensor, via its minor eigenvector, encodes the direction in which a small translation of
the image causes it the least departure from the original. To see this, we perform the
expansion, i.e., we express the image f at r + ϵk, where the direction k = (kx, ky)T
has the unit length, by using the partial derivatives of f at r
f(r+ϵk) = f(r)+ϵ
#
kxDx+kyDy
$
f(r)+ ϵ2
2
#
kxDx+kyDy
$2f(r)+· · · (10.58)
Accordingly,
ϵ
#
kxDx + kyDy
$
f(r) = f(r + ϵk) −f(r)
(10.59)
is the linear approximation of the difference between the function f(r) and its trans-
lated version, f(r + ϵk). In consequence,
(
#
kxDx + kyDy
$
f(r))2
(10.60)
is the magnitude of the rate of the change in the direction of k, which can be viewed
as the error rate or resistance rate when translating the image in the direction k.
Integrating this function and using the (Parseval–Plancherel) theorem 7.2, yields
  #
kT ∇f(x, y)
$2 dxdy = kT
 
∇f(x, y)∇T f(x, y)dxdy

k = kT Sk
(10.61)
which is the dynamic part of our original error function, e(k), because

180
10 Direction in 2D
e(k) = Trace(S) −kT Sk
(10.62)
Accordingly, minimizing Eq. (10.62) yields the direction of minimum translation
error. Evidently maximizing Eq. (10.62) yields the direction of maximum translation
error. Both minimum and maximum error directions are given by the eigenvectors of
S. Paraphrased, the structure tensor encodes the minimum resistance direction in the
spatial domain, which is identical to the direction of the line ﬁt to the power spectrum
in the TLS sense.
10.11 Discrete Structure Tensor by Direct Tensor Sampling
Until this section, the theory for detection of the orientation of a scalar function in
2D space has been based on continuous signals. One such technique was summarized
by theorem 10.1 which we will attempt to approximate by use of discrete functions.
We call this approach direct tensor sampling since the suggested method examines
whether or not the spectrum of an image consists of a line, by directly estimating the
matrix S, with the elements given by Eq. (10.28) without ﬁrst estimating the power
spectrum by a discrete local spectrum.
We need to approximate the continuous integrand of Eq. (10.28) from a discrete
image. To that end, we need the approximation of
∂f(r)
∂xi
∂f(r)
∂xj,
with
i, j = 1, 2,
x1 = x,
and
x2 = y,
(10.63)
on a Cartesian grid i.e., r = rl, where rl is the coordinates of the grid nodes. In
analogy with the theory presented in Sects. 8.2, 8.3, and 9.2, we can do this by
ﬁltering the original image linearly:
∂f(rl)
∂xi
=

k
f(rl + rk)∂μ(rl)
∂xi
with
i = 1, 2
(10.64)
where ∂μ(rl)
∂xi
= ∂μ(r)
∂xi |r=rl, ∂f(rl)
∂xi
= ∂f(r)
∂xi |r=rl, and then applying pointwise multi-
plication between the two thus-obtained discrete partial derivative images:
∂f(rl)
∂xi
∂f(rl)
∂xj
,
with
i, j = 1, 2,
x1 = x,
and
x2 = y.
(10.65)
The latter is an estimate of Eq. (10.63) on a Cartesian grid. Note that the continuous
form of Eq. (10.63) is not known, but we estimated nevertheless its discrete version
by applying a linear discrete ﬁltering to the discrete f(rl), followed by a pointwise
multiplication on the grid rl.
To estimate the structure tensor elements, Eq. (10.28), we ﬁrst reconstruct (10.63)
from its samples (10.65):
∂f(r)
∂xi
∂f(r)
∂xj
=

l
∂f(rl)
∂xi
∂f(rl)
∂xj
μ(r −rl)
i, j : 1, 2
(10.66)

10.11 Discrete Structure Tensor by Direct Tensor Sampling
181
where x1 = x, x2 = y. The vectors rl represent points on a grid as before, and μ(r)
is the continuous interpolation function, assumed to be a Gaussian with the variance
σ2
p:
μ(r −rl) = exp
#
−
1
2σ2p
∥r −rl∥2$
(10.67)
We proceed by substituting (10.66) in Eq. (10.28)
S(i, j) =
1
4π2

l
∂f(rl)
∂xi
∂f(rl)
∂xj

E2
μ(r −rl)dxdy
(10.68)
=
C
4π2

l
∂f(rl)
∂xi
∂f(rl)
∂xj
(10.69)
Here the integral evaluates to a constant C

E2
μ(r −rl)dxdy = C
that is independent of rl because the area under a shifted Gaussian is the same re-
gardless of the amount of the shift. The summation in Eq. (10.69) is taken over all
image points, rl, on the grid.
However, the structure tensor is most frequently needed for a local image, rather
than the entire image. A simple way to achieve this goal is to approximate
∂f
∂xi
∂
∂xj
for the local image by multiplying its global version with a window function, w(r),
placed around the current point r0. Without loss of generality, we assume that the
local image for which the structure tensor is to be estimated is the one around the
origin. Using a Gaussian as a window function, this amounts to replacing μ in Eq.
(10.66) with
μ(r −rl)w(r) = exp

−1
2σ2p
∥r −rl∥2

exp

−1
2σ2w
∥r∥2

(10.70)
where σ2
w is a constant that controls the effective width of the second Gaussian, the
window deﬁning the local image around the origin. Substituting this into Eq. (10.66)
and then using it in Eq. (10.28) yields the local structure tensor elements:
S(i, j) =
1
4π2

l
∂f(rl)
∂xi
∂f(rl)
∂xj

E2
μ(r −rl)w(r)dxdy
(10.71)
=
1
4π2

l
∂f(rl)
∂xi
∂f(rl)
∂xj
μl
(10.72)
where μl is deﬁned as
μl =

E2
exp

−1
2σ2p
∥r −rl∥2

exp

−1
2σ2w
∥r∥2

dxdy
= (μ ∗w)(rl) = 2π
4
σ2
pσ2
w
σ2p + σ2w
exp
#
−
1
2(σ2p + σ2w)∥rl∥2$
.
(10.73)

182
10 Direction in 2D
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
                                    
         
         
         
         
         
         
         
         
         
         
         
                           
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
Fig. 10.15. On the left, the test image, the axes of which are marked with fractions of π
representing the spatial frequency. On the right is the color code representing the directions.
The marks on the axes are separated by 5 degrees when joined to the center. The colored dots
in both images deﬁne the curves from which 1D direction measurements will be sampled
The integral represents a continuous convolution, and Eq. (10.73) is obtained by
noting that both μ and w are Gaussians and that a convolution of them yields another
Gaussian, with a variance that is the sum of the variances of μ and w. An easy way
of seeing this is by applying the Fourier transform to μ ∗w. Eq. (10.72), which
computes the local tensor around the origin, is therefore a discrete convolution by
a Gaussian if S(i, j) needs to be computed for local patches around all points of
the original image grid. Since the values of μls decrease rapidly outside of a circle
with radius

σ2p + σ2w, we can truncate the inﬁnite ﬁlter when its coefﬁcients are
sufﬁciently small, typically when the coefﬁcients have decreased to about 1% of
the ﬁlter maximum. Thus, Eq. (10.72) implies that the local tensor (of the origin) is
obtained as a window-weighted average of the gradient outer products:
S =
1
4π2

l
(∇fl)(∇fl)T μl
(10.74)
where ∇fl is the gradient of f(r) at the discrete image position rl, and μl is a discrete
Gaussian. Deﬁning Dxfl and Dyfl, for convenience, as the components of ∇fl, at
the mesh point rl:
∇fl = (Dxfl, Dyfl)T = (∂f(rl)
∂x
, ∂f(rl)
∂y
)T ,
(10.75)
We summarize our ﬁnding on tensor discretization as a theorem:
Theorem 10.3 (Discrete structure tensor I). Assuming a Gaussian interpolator
with σp and a Gaussian window with σw, the optimal discrete structure tensor ap-
proximation is given by

10.11 Discrete Structure Tensor by Direct Tensor Sampling
183
S = C

l
(∇fl∇Tfl)μl
(10.76)
= C

l

(Dxfl)2
(Dxfl)(Dyfl)
(Dxfl)(Dyfl)
(Dyfl)2

μl
(10.77)
where μl is a discrete Gaussian with σ =

σ2p + σ2w.
♦
In analogy with Eqs. (10.63)–(10.65), the quantity
Dxf(rl) + iDyf(rl)
(10.78)
can be obtained by two convolutions using real ﬁlters, one for Dxf(rl) and one for
Dyf(rl). After that, the complex result depicted by Eq. (10.78) is squared to yield
the ILST image:
ℸ(f)(rl) = (Dxf(rl) + iDyf(rl))2
(10.79)
In consequence of theorem 10.2, the following theorem then holds true:
Theorem 10.4 (Discrete structure tensor II). Assuming a Gaussian interpolator
with σp and a Gaussian window with σw, the optimal discrete structure tensor com-
plex elements are given by
Z = 1
2

I11 −iI20
iI∗
20
I11

(10.80)
where
I20 = C

l
(Dxfl + iDy)2μl
(10.81)
I11 = C

l
|Dxfl + iDy|2μl
(10.82)
with μl being a discrete Gaussian with σ =

σ2p + σ2w.
♦
Figure 10.15 shows a frequency-modulated test (FM-test) image. The test im-
age has axes marked by the spatial frequencies of the waves in the horizontal and
vertical directions from the image center. The absolute frequency of the waves de-
creases exponentially radially, whereas the direction of the waves changes uniformly
angularly. The exponential decrease occurs between the spatial frequencies 0.4π and
0.9π. The image on the right represents the color code of the ideal orientation in
double-angle representation, i.e., exp 2ϕ, where ϕ is the polar angle coordinate of
a point in the image. In half of the image, spatially uncorrelated Gaussian noise X,
with mean 0.5 and variance 1/36, has been added to the image signal, f, according
to pf + (1 −p)X, where the weight coefﬁcient p = 0.3, and X is the noise. On

184
10 Direction in 2D
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
Fig. 10.16. The images illustrate the direction tensor, represented as I20 (left) and I11 (right),
for Fig. 10.15 and computed by using theorem 10.4. The hue encodes the direction, whereas
the brightness represents the magnitudes of complex numbers
the right the reference orientation is encoded as a color image. Using the HSB color
space, the hue is modulated by 2ϕ, whereas the brightness and the saturation are set
to the maximum at all points. The colored dots deﬁne curves along which the struc-
ture tensor measurements will be extracted and discussed in detail further below. The
color coded reference image has axes marked by angles separated in 5◦as seen from
the center.
The images in Fig. 10.16 illustrate the structure tensor computed for all local
images. The color image on the left represents the complex-valued I20. The hue of
an image point is modulated by the arg(I20) of its local neighborhood, whereas its
brightness is given by |I20|. The saturation of all points is set to the maximum. The
computations are implemented according to theorem 10.4 where the derivative Gaus-
sian ﬁlter had σp = 0.8 and the integrative Gaussian ﬁlter,

σ2p + σ2w = 2.5. The
hue of a point should be the same as the reference color given at the same point of the
color image in Fig. 10.16. Visually, it appears that the colors are in good conformity
with those of the reference image. The image on the right shows I11, which, being
nonnegative and real-valued, modulates the gray values. It is possible to verify that
the direction measurements adhere to the theoretical values, even in the noisy part of
the test image, where the signal-to-noise ratio, (SNR), was 2 log2( 0.3
0.7) ≈−2.4 dB.
The brightness of the points at the noisy part are lower in both images for two rea-
sons. First, the |I20| should be weak or ideally zero because the unique direction is
disturbed, and second, because the linear derivation and integration operations have
effectively a bandpass character and the noise components are suppressed by the lin-
ear process. In the clean part of the signal, the brightness of the corresponding points
in the left and the right images is the same. We will discuss these conclusions more
quantitatively next.

10.11 Discrete Structure Tensor by Direct Tensor Sampling
185
2
3
4
5
6
7
3
2
1
0
1
2
3
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
Fig. 10.17. The graphs in the left image represent the estimated (arg(Il
20), solid) as well as
the ideal direction angle (dashed) on a ring in the FM test image. The graphs on the right show
|Il
20| (solid) and Il
11 (dashed) on the same ring
In the left graph of Fig. 10.17 we show in solid green the direction measurements,
arg(I20), extracted along the circle passing through the points 1 to 4, see also Fig.
10.15. We show by the dotted black curve the reference direction measurements. The
direction measurements agree with the reference values nearly exactly in the noise-
free parts of the image, whereas they follow the reference quite well in the noisy parts
of the image. The estimated signiﬁcance of the direction measurements is given by
the graphs of the right image. The solid curve shows |I20|, whereas the dotted curve
shows I11. As predicted by the theory, we have 0 ≪|I20| = I11 in the noise-free
part, whereas 0 ≈|I20| < I11 in the noisy part. Both |I20| and I11 are invariant to
directional changes, which is manifested by the fact that they both equal a constant
(one) in the noise-free part of the test image.
Likewise, in the left graph of Fig. 10.18 we display in blue the analogous mea-
surements, but for arg(I20) extracted along the horizontal line joining point 6 to the
center, as marked in Fig. 10.15. This curve is reasonably horizontal even in the noisy
part. The constant represented by the green line shows the direction measurements
extracted along the line joining point 5 to the center. Being in the noise-free part, this
line adheres nearly perfectly to the reference measurements, shown as a dotted. The
estimated signiﬁcance of the direction measurements is given by the graphs of the
right image. Here, the solid, and the dashed curves at the top represent the measure-
ments of |I20|, and I11 respectively for the clean signal, (from point 5 to the center).
The corresponding measurements for the noisy part (from Point 6 to the center) are
given by the solid and the dashed curves at the bottom, respectively. The direction
estimation quality is assured by the condition that |I20| is high and is close to its up-
per bound, I11. By contrast, when the linear symmetry in the signal is disturbed poor
estimations of the direction are obtained. This is manifested by the fact that |I20| is
close to zero while I11 is weak.
The structure tensor measurements are implemented by using the results of linear
operators. First, the partial derivative operator Dx +iDy using a Gaussian derivative

186
10 Direction in 2D
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
−3
−2
−1
0
1
2
3
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0
0.2
0.4
0.6
0.8
1
Fig. 10.18. The graphs in the left image represent the estimated (arg(Il
20)) as well as the ideal
direction angle on the two radial lines shown in the FM test image. The graphs on the right
represent |Il
20| and Il
11 on the same lines
ﬁlter with variance σ2
p is applied. The result is squared pointwise and smoothed by
the Gaussian ﬁlter with the variance σ2
p+σ2
w. Despite the nonlinear squaring between
the two linear operators, the combined operator exhibits a bandpass character that is
tuned to a particular frequency. This is seen in the right graph of Fig. 10.18, where
the tune-on frequency, which the structure tensor is most sensitive to, has a well-
distinguished peak in the solid blue curve. The tune-on frequency of this particular
implementation at 0.62 radians, corresponding to a sinusoidal wave with a period
of 10 pixels, has been shown in dotted black in both graphs. For convenience, even
the points 1 through 4 deﬁning the sampling ring above correspond to 0.62 radians.
The tune-on frequency can be changed in a variety of ways, a straightforward one
of which is by changing the derivative Gaussian variance σ2
p. The arrows show the
limits of the frequency annulus to which the ﬁlters are most sensitive, as conﬁrmed
by the graphs of |I20| and I11.
10.12 Application Examples
We present two applications to illustrate the use of the structure tensor.
Fingerprint Recognition
In Fig. 10.19, a minutia detection process is visualized. A minutia in a ﬁngerprint
is a point that can be easily identiﬁed. Typically it is an end point of a ridge or a
bifurcation point. The ﬁrst two images visualize the original ﬁngerprint and its en-
hanced version, respectively. Image III represents I20/I11, where the numerator and
denominator at each point are obtained by direct tensor discretization as discussed
above. Even here the minutiae are discernable as dark spots, indicating lack of linear
symmetry. The hue represents local direction. Image IV represents the presence of
linear symmety in parabolic coordinates, a subject that we will discuss in detail in

10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor)
187
Fig. 10.19. Minutia detection process. Adapted after [76]
Chap. 11. Image V represents a fusion of the information in image III and IV by
pointwise multiplication according to (IV ) · (1 −|III|). Finally, image VI presents
the automatically found minutiae. The positions of the minutiae as well as the di-
rections between and around them are useful personal attributes in biometric person
authentication. The details of this minutiae detection approach are given in [76].
Image Enhancement
Image enhancement is a valuable tool to reinforce the quality of recoverable details.
In Fig. 10.20, we show the result of line reinforcement in color images [224]. The
processing performs a so-called shock ﬁlltering to bias lines by means of direction
estimation using the structure tensor.
10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor)
We will attempt to estimate the discrete structure tensor by spectrum sampling as fol-
lows. In Sect. 9.6 we concluded that the responses of a set of Gabor ﬁlters constitute
a sampled version of the local Fourier spectrum. Each ﬁlter is tuned to a particular
position in the spectral domain corresponding to a particular frequency and direction
with a certain support in the frequency domain. Assuming uniform sampling, the

188
10 Direction in 2D
Fig. 10.20. Image enhancement by reinforcing linear symmetry tensor directions, after [224]
number of ﬁlters determines how coarsely the Fourier spectrum is down-sampled.
The more ﬁlters there are in the ﬁlter ensemble, the more the Gabor ﬁlter responses
will approach an ordinary Fourier spectrum while the size of the neighborhood being
analyzed will approach the size of the original image. Figure 10.21 shows a Gabor
ﬁlter set which is (polar) separable in tuning directions and absolute frequencies. The
tuning frequencies of the ﬁlters (marked with ×) along with the isoamplitude curves
of the ﬁlters are given in the graph. For real images, half of the ﬁlters are not needed
e.g., those drawn in cyan, because the response of a ﬁlter equals the complex conju-
gate of the response of the ﬁlter at the mirror site through the origin. Increasing the
number of ﬁlters in the Gabor ﬁlter bank amounts to increasing the sampling resolu-
tion of the Fourier spectrum, since the ﬁlter bandwidths decrease when the frequency
resolution increases.
A linearly symmetric (local) image has a power spectrum that is concentrated
to a line. Depending on the direction of the image and the spectrum discretization,
this line may or may not pass exactly through the tune-on frequencies, the frequency
grid points, e.g., see the two drawn axes in Fig. 10.21. In the case when this axis
has the same direction as the direction of one or more frequency grid points (Gabor
ﬁlters), then the responses of these ﬁlters will account for a signiﬁcant share of the
total response power stemming from the ﬁlter ensemble. In Fig. 10.22, the Fourier
transform of such an image is shown as the dashed energy concentration passing ex-
actly through a subset of ﬁlter tuning sites. The ﬁlter tuning sites are marked with
× in the ﬁgure. The magnitudes of the ﬁlter responses are modulated by the sizes of
the circles placed at the corresponding frequency grid points. The responses repre-
sented by cyan circles can be deduced from their mirrored counterparts as mentioned
before.
To determine the dominant direction of a local image, one could identify the Ga-
bor ﬁlter yielding the largest magnitude and then use the tune-on direction of the
ﬁlter as the direction searched for. However, the disadvantage of this approach is

10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor)
189
Fig. 10.21. A Gabor ﬁlter set in the Fourier domain and the Fourier transforms of two linearly
symmetric images, overlayed. The dashed and solid axes represent the spectral energy con-
centrations of the two linearly symmetric images having gradient directions of 30◦and 45◦
direction, respectively
an unnecessarily rough quantization of the direction because when the linear sym-
metry axis direction falls between two ﬁlter directions, two directions will signal a
presence of signiﬁcant response power in their respective directions (Fig. 10.23). In
consequence, choosing the maximum power direction will not give better direction
resolution than the direction resolution in the Gabor ﬁlter tunings. Below, by using
direction tensors, we show that the limitation of the direction resolution attributable
to the ﬁlter tuning resolution can be reduced signiﬁcantly by utilizing the structure
tensor theory.
Let the coordinates of the ﬁlter tune-on frequencies, marked by × in the example
of Fig. 10.21, be given as complex numbers:
zkl = xkl + iykl
(10.83)
The indices k, l, running over all ﬁlters in the Gabor ﬁlter set, represent the direc-
tion and the absolute frequency of the ﬁlter tunings. Then the second-order complex
moments of the discrete power spectrum are given by:
I20 =

k,l
z2
kl(z∗
kl)2|F {k,l}|2 =

k,l
z2
kl|F {k,l}|2
(10.84)
and

190
10 Direction in 2D
Fig. 10.22. Gabor ﬁlter responses when an input image is linearly symmetric with a gradient
direction aligned with ﬁlter tuning orientations. The sizes of the circles represent the magni-
tudes of the complex ﬁlters at the respective site
I11 =

k,l
z1
kl(z∗
kl)1|F {k,l}|2 =

k,l
|zkl|2|F {k,l}|2
(10.85)
Using theorem 10.4, we conclude that these complex numbers are the elements of
the direction tensor constructed from the discrete local spectrum. Accordingly, the
direction tensor encodes the axis having the least square error when ﬁt to the discrete
power spectrum. In case the image is linearly symmetric, the error of the ﬁt, en-
coded by I11 −|I20| will vanish and the arg(I20) will deliver the direction in double-
angle representation as 2ϕ, with ϕ being the angle of the image gradient. Suggested
by [90,139], the double angle representation (to represent direction) maps an angle ϕ
and its mirror angle ϕ + π to the same angle. Previously, we arrived at this represen-
tation by a total least squares minimization. According to the above equations, the
direction 2ϕ can be interpreted as an angle interpolation between ﬁxed ﬁlter direc-
tions using the magnitude responses of a bank of ﬁlters. Because of the interpolation,
the result should not suffer from the direction quantization as much when compared
to using the maximum power direction. In Fig. 10.25, encoded as images, we show
the direction estimations as I20 and I11, for three tune-on directions in a log–polar
Gabor decomposition, the details of which are discussed further below. However, it
should be emphasized that the structure tensor estimation via the formulas given by
Eqs. (10.84)–(10.85) are equally valid for a Cartesian Gabor decomposition.

10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor)
191
Fig. 10.23. The 2D spectrum and the Gabor ﬁlter responses when an input image is linearly
symmetric with a direction between ﬁlter tune-on directions. The crosses represent the tune-on
frequencies. The arrows represent the coordinate vectors, zk, of some of these, whereas the
circles illustrate, via their radii, the magnitudes of the respective ﬁlter responses, |F {k,l}|
Structure Tensor by Log-Polar Gabor Decomposition
The sums in Eqs. (10.84)–(10.85) are taken over the entire range of Gabor ﬁlters.
Assuming log–polar separable ﬁlter tune-on frequencies discussed in Sect. 9.6, one
could, however, split the sum over the direction and the frequency components, as
shown for I20:
I20 =

l

k
z2
kl|F {k,l}|2 =

l
Il
20
(10.86)
with
Il
20 =

k
z2
kl|F {k,l}|2
(10.87)
being the complex moment contribution from ﬁlters on a “ring” of Gabor tune-on
frequency sites. Analogously, we can obtain
I11 =

l

k
|zkl|2|F {k,l}|2 =

l
Il
11
(10.88)
with

192
10 Direction in 2D
Il
11 =

k
|zkl|2|F {k,l}|2
(10.89)
In fact, the quantities Il
20 and Il
11 are equal to I20 and I11, respectively, if the
image is bandpass ﬁltered in such a way that the absolute frequency contents dif-
fering from |zkl| are suppressed. Accordingly, Il
20 and Il
11 represent the direction
tensor of a speciﬁc (absolute) frequency (which is a ring with a “small” width in the
spectrum) of the image. Apart from requiring less computational resources, restrict-
ing the Gabor ﬁlter–based direction tensor to a speciﬁc absolute frequency is also
desirable in many situations where multi-scale analysis is needed, e.g., texture and
ﬁngerprint image analysis, since this allows one to split the analysis over an array of
scales naturally. Originally suggested in [137], via a scheme that is a special case of
Eq. (10.84) applied to a frequency ring, such an orientation interpolation is sufﬁcient
for the purpose of estimating the local direction for many applications. The original
scheme employed quadrature mirror ﬁlters with the angular bandwidth of π i.e., the
angular deviation from the tune-on direction was measured by the function cos2(θ)
as compared to the Gaussians discussed here. Regardless of the ﬁlters shape5 and
how they tessellate the spectrum, it should be emphasized that Il
20 must be com-
pleted with Il
11 to make it a direction tensor. Otherwise I20 alone cannot encode both
the minimum and the maximum error, apart from their difference.
Although the interpolated direction offers better direction accuracy, there is ob-
viously a limit on the minimum number of ﬁlters one can have in the decomposition.
To begin with, it is not possible to compute Il
20, which consists of two real variables,
and Il
11, which consists of one nonnegative real variable, from just one Gabor ﬁlter,
(actually two ﬁlters when mirrored). The Il
20 would point in the same direction, re-
gardless of the direction of the image. Even two Gabor ﬁlter directions differing with
π
2 (actually 4 ﬁlter sites when mirrored) are not enough because this would result in
an Il
20 that could not possibly point in other than the two directions: 2ϕ and 2ϕ + π
where ϕ is the direction of one of the two ﬁlters. Using three ﬁlters with directions
separated by π
3 is the minimum requirement on the Gabor decomposition to yield a
meaningful direction tensor.
In Fig. 10.24, this is illustrated by investigating the Gabor ﬁlter responses to
an image composed of a planar sinusoid. The intersection point of three circles is
marked with a small circle, and it represents the frequency coordinates of one of
the two Dirac impulses composing the sinusoid image. The three circles show the
frequency coordinates (the direction and absolute frequencies) at which the Dirac
impulse can be placed, judging from the individual Gabor ﬁlter magnitudes. Provided
that it is on such a ring, the magnitude response of the same Gabor ﬁlter is invariant
to the position changes of the Dirac pulse. Only by investigating no less than three
Gabor ﬁlter magnitudes, is it possible to uniquely determine the position of a Dirac
impulse. This result is nonetheless not surprising, because there are three freedoms
in a 2D direction tensor and these cannot be ﬁxed by less than three independent
measurements.
5 Rotation invariant versions of quadrature ﬁlters have also been suggested [71].

10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor)
193
Fig. 10.24. Iso-magnitudes of three Gabor ﬁlters corresponding to an image that consists of a
sinusoid (planar) wave, having the direction and the absolute frequency indicated by the pair
of small circles.
In Fig. 10.25, we show on the left the quantity I20 computed according to Eq.
(10.87) where the Gabor decomposition had three ﬁlters differing only in tune-on
directions with increments of π
3 . The hue encodes arg(I20), whereas the brightness
encodes |I20|. The absolute frequency represented by the radius of the ring where
the tune-on frequencies of the ﬁlters are placed is 0.62π. The standard deviation of
the directional Gaussians was set to ση = π
6 in the log–polar mapped frequency do-
main discussed in Sect. 9.6. The hue appears to change continuously in the angular
direction, and the response is concentrated to a strict annulus, indicating a reasonable
approximation of the direction as well as the certainty in a narrow band of frequen-
cies. On the right we show I11 supporting the same visual conclusion. However, this
visual evaluation is too coarse to quantify the quality of the approximation.
To this end, on the left graph of Fig. 10.26 we show a plot of arg(I20) along the
ring passing through the points 1–4 marked in Fig. 10.15. The direction estimations
follow the true direction, the piecewise linear graph, reasonably well, although the
estimation accuracy is not as good as in Fig. 10.17. Likewise, the total error graphs
on the right representing |I20| and I11 do not perform nearly as well as compared
to Fig. 10.17. This is because on the clean part of the image |I20| ≈I11, and both
should be constant and large compared to the noisy part. However, this is only ap-
proximately true, as the oscillations show. The same result is conﬁrmed in the mea-
surements across the lines joining the point 5 and point 6 to the center (Fig. 10.27).
The image on the left shows the direction estimations in double angle representation

194
10 Direction in 2D
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
Fig. 10.25. The images illustrate the direction tensor, represented as I20 (left) and I11 (right),
for Fig. 10.15 and computed by using theorem 10.4 for Eqs. (10.87)–(10.89). The hue encodes
the direction, whereas the brightness represents the magnitudes of complex numbers. There
were three different tune-on directions in a set of three Gabor ﬁlters
(radians). The arrows show the limits of the frequency annulus to which the Gabor
ﬁlters are most sensitive, as conﬁrmed by the graphs of |I20| and I11 in the right. The
main disadvantage of this direction estimation compared to the one presented in Sect.
10.11 is due to the poor quality estimations, caused by the very coarse discretization
of the spectrum in three directions. The accuracy of the direction estimation itself
is, however, fully sufﬁcient for many applications. Both direction accuracy and error
estimation accuracy can be improved signiﬁcantly by a nonminimalist set-up of the
Gabor decomposition. This will be elaborated in the next section further. Here, we
wanted to show that even with three Gabor ﬁlters it is possible to obtain reasonably
accurate estimates of the direction, albeit the error estimates of the directions are not
nearly as good.
Structure Tensor by Cartesian Gabor Decomposition
It is worth noting that the local spectrum must not be sampled via a separable tessel-
lation in the orientation and the absolute frequency, although such a decomposition
has advantages for pattern recognition purposes. The algorithm represented by Eqs.
(10.84)–(10.85) will still hold for reasonably densely discretized power spectra, in-
cluding a Gabor ﬁlter bank that covers the frequency plane in a Cartesian-separable
fashion, as shown in Fig. 10.28. In this graph the tuning frequencies of the ﬁlters
(marked with ×) along with the iso-amplitude curves of the ﬁlters are shown (the red
and the cyan curves). For real images, half of the ﬁlters are not used, those drawn
with cyan in the graph, as before because they are deducible from the other half. The
dashed and solid axes show two example frequency responses at which the spectral
energy will ideally be concentrated if the image is linearly symmetric having a gra-

10.13 Discrete Structure Tensor by Spectrum Sampling (Gabor)
195
2
3
4
5
6
7
−3
−2
−1
0
1
2
3
π/2
3π/2
5π/2
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
Fig. 10.26. The graphs in the left image represent the estimated (arg(Il
20r), solid) as well as
the ideal direction angle (dashed) on the circle passing through 1 to 4 in Fig. 10.15 when using
three tune-on directions. The graphs on the right show |Il
20| (solid) and Il
11 (dashed) on the
same ring
dient in the 45◦and 30◦directions respectively. We show the frequency coordinates
zkl (marked by ×), representing the tune-on frequencies of the ﬁlters. Note that the
indices k and l now represent frequencies in the horizontal and vertical directions of
the image instead of explicitly encoding the direction and absolute frequencies of the
ﬁlters.
Marked explicitly by arrows in Fig. 10.29, we show the frequency coordinates
zkl. The sizes of the circles are modulated by the magnitudes of the ﬁlter responses,
|F k,l|. The magnitudes of the responses are larger when the corresponding ﬁlter tune-
on frequencies are close to the solid line because we assumed that we have a linearly
symmetric image, which has a spectral energy concentrated to the shown solid line.
To obtain a reasonably good estimate of the structure tensor, the ﬁlters should
cover all directions, not necessarily all frequencies. This is because most images
with directions consist of a wide range of frequencies. Some studies have used other
tessellations which are a mixture of Cartesian and log–polar tessellation [124,144].
Along the positive horizontal axis, they divided the frequency coordinate in expo-
nentially increasing cell sizes. In each such cell they then placed Cartesian-separable
Gaussians,
Gk(ωx, ωy) = exp
(ωx −ωk)2
2σ2ωx

exp

ω2
y
2σ2ωy

(10.90)
symmetric around the cell centers, (ωk, 0)T representing the horizontal tune-on di-
rections. This set of ﬁlters is then rotated with an angular increment to obtain the
Gabor ﬁlters for other directions, allowing an angular tessellation of the frequency
domain. However, we emphasize that the ﬁlter functions themselves are Cartesian
because they are fully symmetrical Gaussians in the frequency domain, although
they are placed at frequency sites that tessellate the spectrum in a log–polar fash-
ion. One advantage is implementation efﬁciency because all horizontal ﬁlters are
Cartesian-separable enabling a computation by cascades of 1D ﬁltering. Other direc-

196
10 Direction in 2D
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−4
−3
−2
−1
0
1
2
3
4
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0
0.2
0.4
0.6
0.8
1
Fig. 10.27. The same as in Fig. 10.26 but the graphs on the left represent angle estimations
on the line joining point 5 to the center (green) and on the line joining point 6 to the center
(blue) marked in Fig. 10.15. In the right image, the two curves at the top, and the two curves at
the bottom represent |Il
20| (solid curves), and Il
11 (dashed curves) for two parts of the image.
The measurement pair on the top originates from the line joining point 5 to the center (clean),
whereas the pair at the bottom represents the measurements for the line between point 6 and
the center (noisy)
tions are evidently not Cartesian-separable, but the image can be rotated with ﬁxed
increments, and the same horizontal set of ﬁlters can be applied. The result is rotated
back with the same amount. A disadvantage is that the ﬁlters, which are Gaussians,
give the same weight to low and high frequencies within the frequency cell they are
placed.
10.14 Relationship of the Two Discrete Structure Tensors
The structure tensor measures the second-order moment properties of the power
spectrum. It estimates the most prominent direction of the image in the TLS sense
and provides estimates on the quality of the ﬁt. As we saw in Sect. 10.11, the tensor
can be directly discretized or, as in Sect. 10.13, ﬁrst the spectrum can be discretized
via a Gabor ﬁltering and then the structure tensor is computed for the discrete spec-
trum. Either case yields a discrete structure tensor that represents how well a TLS
ﬁt of a line models the image spectrum. Below we discuss the relationship of both
techniques and the advantages of each.
The Gabor decomposition used in Sect. 10.13 to estimate the structure tensor
sampled the spectrum at just six points (only three ﬁlters were needed though!) along
a ring. According to the sampling theory discussed in Sect. 9.6, a sampled signal
must be lowpass ﬁltered before sampling to deter sampling artifacts. This is, in fact,
done by the Gabor decomposition by a smoothing of the spectrum before sampling.
Suppose that we have a perfectly linear symmetric image, for the sake of discussion
a sinusoid in the spatial domain, with a crisp direction. This has a spectrum that has a
pair of Dirac pulses. As the direction of the image changes in the spatial domain, the

10.14 Relationship of the Two Discrete Structure Tensors
197
Fig. 10.28. Cartesian-separable Gabor decomposition where the × mark the ﬁlter centers and
the circles show the iso-amplitudes of the ﬁlters midway between the grid sites. The two axes
(dashed and solid) illustrate, overlayed, the spectra of two linearly symmetric images
pair of Dirac pulses rotates on the same ring. Accordingly, it makes sense to attempt
to discretize the spectrum along a ring passing nearby the Dirac pulses. A log–polar
Gabor decomposition will do the sampling, but before sampling the spectrum along
the ring, the spectrum will be smeared. Thus, the Dirac pulses are smeared out in the
angular direction in an amount that corresponds to the size of the window for which
the Gabor ﬁlters are designed. In consequence, the more Gabor ﬁlters we use, the
more the structure tensor computed by Gabor decomposition will approach the one
discussed in Sect. 10.11.
In Fig. 10.30 we show on the left the quantity I20 which is encoded by the hue,
and the brightness as obtained for the test image in Fig. 10.15. We used 18 directions,
i.e., the tune-on directions differed by increments of
π
18. The band concentration of
the response is the same as before (cf. Fig. 10.25). This is to be expected because we
have not changed the radial widths of the log–polar frequency Gaussians. Instead we
see that the hue variation better follows the ideal hue variation shown in Fig. 10.15.
On the right in Fig. 10.30 we see I11 modulating the brightness holding itself into
an annulus, and following the brightness of the image on the left, |I20|, closely and
without oscillations. To assure ourselves, we also study the graph of these functions
along the same ring as before. This is shown in Fig. 10.31, where on the left we see
that the estimations follow the true directions nearly perfectly. On the right in the
ﬁrst half of the graph, representing the clean part of the test image, we see that the
certainty, |I20|, is nearly constant and follows I11 apart from a minor bias. The small
bias is not surprising because even with 18 directions, the sampling density of the
spectrum afforded by the Gabor decomposition is inferior to what is available in the
original spectrum. The result is a smearing of the perfect energy concentration of
the Dirac pulse. The dilation of the concentration is noticed by the structure tensor
and is quantiﬁed as a small error in its attempt of ﬁtting an ideal (inﬁnitesimally
narrow) line. In the noisy part of the graph, the quality of the ﬁt is lower as expected,

198
10 Direction in 2D
Fig. 10.29. Computing the second-order complex moments of the local power spectrum ob-
tained via a Cartesian separable Gabor decomposition according to Eqs. (10.84)–(10.85). The
sizes of the circles, representing the magnitudes of the ﬁlter responses become larger the closer
the corresponding ﬁlter sites get to the solid axis
but the direction estimations follow the true directions quite well, nevertheless. We
see a conﬁrmation of these conclusions in the graphs of Fig. 10.32, which represent
arg(I20) on the left, and |I20|, I11 on the right, corresponding to sites along the
lines joining points 5 and 6 to the center, marked in the test image of Fig. 10.15.
The direction estimates are constant, as they should be and the quality of the ﬁt as
represented by the magnitudes of |I20| and I11 is high. These are close to each other
in the clean signal, whereas they are much smaller and not very close to each other
(relative the amplitude of |I20|).
Which structure tensor decomposition should one then use? The direct approach
is a cascade of 1D Gaussian ﬁltering and directionally isotropic that is, free of
direction-dependent artifacts. Using this technique results in far fewer operations in
a sequential computer, such as a personal computer. Accordingly, if dense direction
tensor maps are needed, direct sampling of the structure tensor offers computational
advantages, while the accuracy of all elements of the structure tensor is virtually
unaffected by the minimum number of the ﬁlters needed (three ﬁlters are needed:
Dx, Dy, and a Gaussian). However, a direct steering of the absolute frequency to
which the structure tensor is sensitive and the bandwidth of the frequency sensitiv-
ity range are more conveniently achievable by a log–polar mapping of the spectrum
and discretization via Gabor ﬁltering. Such a steering is possible for direct sampling
of the structure tensor too, but indirectly. A Laplacian pyramidlike processing must
ﬁrst be applied to the image to make sure that the frequency ranges of interest are
sufﬁciently well isolated. Also, dense structure tensors are not always needed. Some
applications can successfully be developed on sparse grids, e.g., the square grid sug-
gested in [144] or a log–polar sampling of the world [196,216]. Tracking of humans
and their identiﬁcation is, for example, achievable on a doubly log–polar sampling of
the image and its spectrum, i.e., on an image grid which samples the image plane in a
log–polar fashion the local spectrum is sampled in a log–polar fashion too [27,205].

10.15 Hough Transform of Lines
199
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
1
2
3
4
5
6
 0.5
 0.7
 0.9
 0.7
 0.5
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
Fig. 10.30. The images illustrate the direction tensor, represented as I20 (left) and I11 (right),
for Fig. 10.15 and computed by using theorem 10.4 and Eqs. (10.87)–(10.89). The hue encodes
the direction, whereas the brightness represents the magnitudes of the complex numbers. There
were 18 tune-on directions in the ﬁlter set
In this case, the Cartesian separability of the ﬁlters is not a serious advantage because
the grid is sparse and the spectral sampling can be done by simple scalar products
with Gabor ﬁlters.
10.15 Hough Transform of Lines
The Hough transform is a nonlinear ﬁltering technique to estimate the position and
direction of certain curves in a discrete image [111]. Despite its name, it is not an
invertible transform in the sense of Fourier transform or the like.
The simplest Hough transform is the Hough transform of lines. It detects (in-
ﬁnitely) long lines, which we discuss in this section. To ﬁnd lines, it can be imagined
that a gradient ﬁltering would be sufﬁcient. Even in an ideal image free of noise,
signiﬁcantly large segments of lines may be missing for a variety of reasons. For
example, lines of an object may be occluded because another object is in front of it.
The Hough transform does not replace gradient ﬁltering but starts from the result of
it, to be precise the gradient image to the magnitude of which a threshold has been
applied, to group together the scattered segments of the same line. A summary of the
Hough transform for lines is given next.
1. The curve family of lines is modeled and parameterized. Because of its unbiased
properties w.r.t. directions, one commonly used line model is:
kT r = cos(φ) · x + sin(φ) · y = b
(10.91)
Here φ and b are the parameters representing the direction and the closest dis-
tance of the line to the origin, marked between the points O and B in Fig. 10.33.

200
10 Direction in 2D
2
3
4
5
6
7
−3
−2
−1
0
1
2
3
π/2
3π/2
5π/2
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
Fig. 10.31. The graphs in the left image represent the estimated directions (arg(Il
20), solid)
as well as the ideal direction angles (dashed) on the circle passing through 1 to 4 in Fig.
10.15 when using 18 tune-on directions. The graphs on the right show |Il
20| (green) and Il
11
(magenta) on the same ring
Every parameter pair (φ, b) represents a unique and inﬁnitely long line. Like-
wise, every (x, y) pair deﬁnes a curve in the (φ, b) space.
2. The local edge strengths along with their directions are extracted by a gradient
ﬁltering,
|∇f(x, y)| = |(Dx + iDy)f(x, y)|
(10.92)
θ(x, y) = arg(∇f) = arg[(Dx + iDy)f]
(10.93)
and a binary image is obtained by thresholding the edge strength, |∇f|. The
result is an edge image, α(x, y), which is quantized to either 0 (background
point), or 1 (edge point) with the local direction of the edge being θ(x, y).
3. Given an edge point (x0, y0) in the edge image α(x, y), it deﬁnes uniquely one
curve in the (φ, b)-plane:
cos(φ) · x0 + sin(φ) · y0 = b
(10.94)
because for every imaginable φ there is a uniquely deﬁned b. However, only a
discrete version of (φ, b)-plane is investigated in practice. A grid A(φk, bk) is
deﬁned by having an appropriate level of quantization for φ and b parameters.
Each point of the grid is assigned initially the value zero, A(φk, bk). This arti-
ﬁcially constructed grid is called the accumulator, because it will be used for a
voting procedure storing the votes at each cell node, A(φk, bk).
4. Assume that (x0, y0) is an edge point of the image found in α(x, y). Because
every point in the (x, y) plane deﬁnes a curve in the (φ, b) plane, one votes for
all points of the curve C corresponding to (x0, y0) in the (φ, b) plane. Adding
the value 1 (a vote) in each of the grid cells A(φm, bm) through which the curve
C passes, one accomplishes the voting by repeating the procedure for all edge
points of α(x, y). To know which cells the curve C passes in the (φ, b) plane,
one only needs to substitute φ = φk in Eq. (10.94), where φk is incremented

10.15 Hough Transform of Lines
201
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−3
−2
−1
0
1
2
3
4
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0
0.2
0.4
0.6
0.8
1
Fig. 10.32. The same as in Fig. 10.31 but the graphs on the left represent angle estimations
on the line joining point 5 to the center (green) and on the line joining point 6 to the center
(blue) marked in Fig. 10.15. The graphs on the right represent the corresponding |Il
20| (solid)
and Il
11 (dashed), respectively. The measurements on the noisy line are at the bottom.
through all discrete φ values of the grid A established in step 3. For each such
φk, one b is obtained, which is rounded off towards the closest discrete b value
of the grid cells. The ﬁnal A(φm, bm) obtained after the voting procedure is the
Hough transform for lines.
5. A long line causes a high peak in the (φ, b) plane, because every long line in
the (x, y)-plane generates a point in the (φ, b) plane. Accordingly, the position
of each such peak yields a speciﬁc (φ, b) parameter that represents an inﬁnitely
long line in the (x, y)-plane.
The Hough transform procedure above can be simpliﬁed further by voting only for
one parameter cell (φ, b) per edge point (x, y) in step 4, the most likely direction
φk ≈θ(x, y) and its corresponding b value closest to a grid node. The robustness can
be improved by smoothing the accumulator before ﬁnding the peaks. Accordingly,
the votes cast for the line (φj, bj) are
A(φj, bj) =

l
δ[θ(xj + xl, yj + yl) −θ(xj, yj)]
(10.95)
where (xl, yl) are the edge points along the line given φj, bj having the direction
θ(xj, yj). The sum represents a ﬁltering that counts the occurrence of the edges
encountered along the line. The occurrence is w.r.t. edges and edge directions, i.e.,
if there is a directional conﬂict the value of the cast vote is zero.
Nonuniqueness of Line Directions
In step 1 we have a model of a line for every ﬁxed pair of parameters of (ϕ, b).
However, the model parameters and the lines do not uniquely correspond to each
other because by substituting (φ+π, −b) in Eq. (10.91), one sees that this parameter

202
10 Direction in 2D
r=[x,y]T
k=[cos(φ), sin(φ)]T
φ
B
O
φj
bj
bj
φj
Fig. 10.33. (left) A common model of a line (green) to be used in the Hough transform of a
line is given by the vector k and the perpendicular distance of the origin to the line, b = |OB|.
(right) The discretized parameter space (φ, bj) is shown. Edges in the (x, y) space cast votes
to cells in the (φj, bj) space
point generates the same line as the one generated by (φ, b). The accumulator will
then have two peaks for every line in the (x, y)-plane if no measures are taken. One
technique is to add the votes of one half of the (φ, b) plane to the respective cells in
the other half, which is equivalent to forcing θ(x, y) to the range [0, π].
10.16 The Structure Tensor and the Hough Transform
In this section we develop the Hough voting process to see that the structure tensor
averaging is a voting process too. The votes have tensor values, or equivalently com-
plex values, jointly encoding the line strength and the line direction continuously.
In case of strong coherence of local edge directions with the line model, both tech-
niques yield identical results. However, they differ when the local edge directions
are not consistent with the line model, because the tensor voting allows voting to
other candidates too. In that respect, the structure tensor voting can be likened to a
multiparty election that extends the single-party election, the Hough transform vot-
ing. The electors of the structure tensor voting are allowed to cast a vote even to the
opposition party, whereas the electors of the Hough transform are only offered to be
absent in case they disagree with the single party.
In Sect. 10.15, the nonuniqueness of lines w.r.t. the line direction was observed.
At ﬁrst, this may appear as a technical problem that can be solved by using half of
the arc circle as angle parameter. However, there is a fundamental problem that is
not resolved by such an approach. This is because a numerical discontinuity at an
end of the interval [0, π] must be introduced since the angles 0 and π correspond
to the same line direction but differ maximally numerically. We suggest a different

10.16 The Structure Tensor and the Hough Transform
203
(2φj ,bj )
2
L exp(i2φ )
exp(i2θ(xl,yl))
L exp(i2φj)
exp(i2θ(xl,yl))
(2φj ,bj)
2
Fig. 10.34. The graphs illustrate the structure tensor voting along an inﬁnitely long line drawn
in magenta. (Left) The directions of the green edges are approximately consistent with the long
magenta line, (φj, bj). A few lines corresponding to the same φj but different bj are shown
in magenta. (Right) The coherent directions of the green lines are in maximal conﬂict with
the long line. The linear symmetry tensors of the edges are shown in green as the complex
numbers exp(i2θ(xl, yl))
parametrization that yields a unique representation of lines. Squaring both sides of
Eq. (10.91) yields:
(kT r)2 = rT kkT r = b2
(10.96)
where
kkT =

cos2 φ
sin φ cos φ
sin φ cos φ
cos2 φ

= 1
2

I +

cos 2φ sin 2φ
sin 2φ cos 2φ

(10.97)
The parameters (2φ, b2) will uniquely represent lines since (φ + π, −b) and (φ, b)
map to the same parameter, the one and the same line.
The problem is thus to ﬁnd how much support or votes there are along a presumed
line represented by (2φj, b2
j), given the observed edge points having the directions
{exp(i2θl)}l along that line. In other words, we wish to know if a “subimage”, which
only consists of the long narrow line (2φj, b2
j), is composed of edge elements having
the same direction. The subimage is illustrated by the magenta line in Fig. 10.34.
This is equivalent to investigating if the image is linearly symmetric in the subimage.
A few other subimages, i.e., lines, corresponding to the same 2φj but different b2
j
are shown in magenta. According to the structure tensor theory, the solution to this
problem is to measure I20 and I11 in the image, the long line. Accordingly, we can
compute I20:
I20 =

l
exp(i2θ(xl, yl))
(10.98)
where (xl, yl)T are edge points along the presumed line. Likewise, summing the
magnitudes | exp(i2θl)| = 1 yields:

204
10 Direction in 2D
I11 =

l
1 = L
(10.99)
where L is the length of the observed line segments along the presumed line. Ac-
cordingly, the inequality
|I20| ≤I11
(10.100)
holds with equality if all observed directions of the edges are are the same, or else
|I20| will be less than I11. Accordingly, the I20 values computed by Eq. (10.98) along
a given line (2φj, b2
j) are the total votes in the accumulator cell A(2φj, b2
j). Alter-
natively, a normalized vote I20/I11 can be cast if there are large contrast variations
in the image. For a given direction 2φj, the tensor summation in Eq. (10.98) can
be computed for all b2
js, yielding one column in the accumulator matrix A. Called
orientation radiograms, such columns can encode the shape information, useful for
various applications, e.g., image database retrievals [163].
In the ideal case, the inequality |I20| ≤I11 will hold with equality, and the count
of votes in the parameter cell A(2φj, b2
j) will be identical to that obtained by use of
the Hough transform. Both methods will thus deliver the same vote count in case all
edge elements have the same direction as the presumed direction.
What happens if equality is not reached, i.e., some of the directions of the edges
are in maximal conﬂict with each other? To illustrate this, we assume that half of the
edge elements have directions orthogonal to the other half, i.e., we have either the
direction θ0 or θ0 + π
2 , yielding the complex tensor elements:
I20 =

m
exp(i2θ0) +

n
exp(i2θ0 + iπ) = L
2 −L
2 = 0
(10.101)
The conﬂicting votes of the orthogonal directions are thus counted as negative votes,
reducing the strength of the total vote for (2φj, b2
j). By contrast, in the Hough trans-
form an observed edge element with a conﬂicting direction is only prevented from
voting. Such edge elements are hindered from reducing the accumulator votes.
In fact, the votes of the structure tensor can not only be negative, but can even
be complex-valued. The argument of the complex-valued vote tells which (direction)
candidate the current edge is supporting. Vote counting is a vectorial averaging, and
the result, |I20|, will be as large as I11 if the edge directions are collinear, or else the
total vote will be reduced, possibly until |I20| reaches 0. In contrast to Hough voting,
tensor voting allows one to ﬁt a line even to a dashed line where the edge elements
share the same direction consistently, but this common direction is different from the
direction of the presumed long line, drawn in magenta in Fig. 10.34. If this is not
desired, the total complex votes not agreeing with the corresponding cell labels, i.e.,
2φj, can be eliminated by applying a threshold to the total vote arguments (angles).

10.17 Appendix
205
10.17 Appendix
Proof of lemma 10.1 Noting that ∥k∥= 1, we can choose u so that it is orthogonal
to k. By juxtaposing these two column vectors side by side, we can construct the
orthogonal matrix U = [k, u], i.e., UUT = 1. Recalling that r = (x, y)T , and
ω = (ωx, ωy)T , the 2D Fourier transform of a linearly symmetric image then yields:
F(ωx, ωy) =
 
f(x, y) exp(−iωT r)dxdy
=
 
g(kT r) exp(−iωT r)dxdy
(10.102)
where we have substituted the 2D linearly symmetric function f(x, y) with its ver-
sion constructed from the 1D function via g(kT r). We can now perform a variable
substitution to the effect that r′ = UT r, where r′ = (x′, y′)T . The differential term
dxdy can be replaced by dx′dy′ without further consideration because U is orthog-
onal and causes the Jacobian determinant to become 1. Because of the orthogonality
of U, we also have r = Ur′. Consequently, the Fourier transform reduces to
F(ωx, ωy) =
 
g(x′) exp(−iωT Ur′)dx′dy′
=
 
g(x′) exp(−iωT [k, u]r′)dx′dy′
=
 
g(x′) exp(−i[ωTk, ωTu]r′)dx′dy′
=

g(x′) exp(−iωT kx′)dx′

exp(−iωT uy′)dy′
= G(kT ω)δ(uT ω)
(10.103)
■
Proof of theorem 10.2 We will ﬁrst express the error or the inertia given by Eq.
(10.17) by means of complex numbers instead of vectors. To be precise, the distance
deﬁned in Eq. (10.16) can be computed by replacing the scalar product in the 2D
plane with complex multiplications via
xT
1 x2 = ℜ(%x∗
1%x2) = 1
2[%x∗
1%x2 + (%x∗
1%x2)∗]
(10.104)
where%· is the operator that constructs a complex number from a real 2D vector:
%x = x + iy,
when
x = (x, y)T ,
(10.105)
and ℜ(%x) equals to x, the real part of the complex number %x. Since ∥k∥= 1 and
S is positive semideﬁnite , k maximizing kTSk is the eigenvector belonging to the
largest eigenvalue of S. By using Eqs. (10.23) and Eq. (10.29) we obtain:

206
10 Direction in 2D
kTSk =
1
4π2

kT (∇fj)(∇T f)kdx =
1
4π2

(kT ∇f)(kT ∇f)dx
=
1
4π2

(ℜ(%k∗%∇f))2dx
Now, with ordinary algebraic manipulations applied to complex numbers, we
obtain
kTSk =
1
4π2
 1
4
 
(%k2)∗(%∇f)2 + [(%k2)∗(%∇f)2]∗+ 2|%∇f|2
dx
=
1
4π2
 1
2|%∇f|2dx + 1
2ℜ[(%k2)∗(%∇f)2]dx
(10.106)
where
%∇= Dx + iDy = ∂
∂x + i ∂
∂y
(10.107)
By construction, %∇f and %k are the complex interpretations of the real 2D vectors
∇f and k. Since the ﬁrst term of the sum in Eq. (10.106) is free of %k, the complex
number %k2 that maximizes Eq. (10.106) is the same as the one that maximizes the
second term:
1
4π2
 1
2ℜ[(%k2)∗(%∇f)2]dx = 1
2ℜ

(%k2)∗1
4π2

(%∇f)2dx

(10.108)
= 1
2ℜ[(%k2)∗I20]
(10.109)
where we have used the fact that ℜ(·) and

· are linear operators that commute with
each other, and I20 represents the complex scalar:
I20 =
1
4π2

(%∇f)2dx =

(ωx + iωy)2|F|2dω.
(10.110)
The choice of the subscript in I20 is justiﬁed because the second integral in Eq.
(10.110) is a complex moment.
Identifying the scalar product in Eq. (10.109) via Eq. (10.104) and remembering
that |%k2| = 1, we interpret the expression geometrically. The vector that corresponds
to the complex scalar I20 is projected onto a line with the direction vector that cor-
responds to %k
2. The projection, a real scalar, is given by Eq. (10.109), which is to be
maximized. Evidently, the direction %k2
max that maximizes this projection is the same
as the direction of I20:
%k2
max = I20/|I20|.
(10.111)
and the projection result is the (positive real) scalar |I20|. Similarly, the complex
number %k2
min that minimizes Eq. (10.106) is given by the (negative real) scalar
−%k2
max yielding the negative scalar −|I20|. Calling the (positive real) scalar expres-
sion in the ﬁrst term of Eq. (10.106) as I11:

10.17 Appendix
207
I11 =
1
4π2

|%∇f|2dx
(10.112)
and substituting %k2
min and %k2
max in Eq. (10.106) yields:
λmin = e(kmin) = 1
2(I11 −|I20|)
(10.113)
λmax = e(kmax) = 1
2(I11 + |I20|).
(10.114)
That is:
|I20| = λmax −λmin
(10.115)
arg I20 = 2θ0
(10.116)
I11 = λmax + λmin
(10.117)
■

11
Direction in Curvilinear Coordinates
This chapter provides a general technique for detection of patterns possessing lin-
ear symmetry, with respect to curvilinear coordinates in 2D images. Curves given
by a harmonic function pair (HFP) will be discussed in detail. The idea is to “bend
and twist” the image by means of an HFP so that the patterns can be detected by
the same formalism that we developed for the structure tensor. This will lead us to
the concepts of coordinate transformations (CT) and generalized structure tensor
(GST), which will be discussed further from the viewpoint of pattern recognition.
Since very intricate patterns can be described by such CTs, the technique is a general
toolbox for pattern detection. We will also develop a unifying concept for geometric
shape quantitation and detection, to the effect that the generalized structure tensor
becomes an extension of the generalized Hough transform, with the additional prop-
erty that it is also capable of handling negative votes as well as complex-valued votes.
In the generalized structure tensor theory, the detection of intricate target objects is
equivalent to a problem of symmetry detection in the HFP coordinate system. The
generalized structure tensor does not necessitate explicit coordinate transformations.
Instead, via Lie operators, the “bending and twisting” occurs in the complex-valued
ﬁlters implicitly once for all, rather than being performed explicitly on every image
to be recognized.
11.1 Curvilinear Coordinates by Harmonic Functions
Let ξ(x, y) be a harmonic function, that is, its partial derivatives of the ﬁrst two
orders are continuous and it satisﬁes the Laplace equation:
Δξ = ∂2ξ
∂x2 + ∂2ξ
∂y2 = 0.
(11.1)
Due to the linearity of Laplace’s equation, linear combinations of harmonic functions
are also harmonic. If two harmonic functions ξ and η satisfy the Cauchy–Riemann
equations:

210
11 Direction in Curvilinear Coordinates
∂ξ
∂x = ∂η
∂y ,
∂ξ
∂y = −∂η
∂x
(11.2)
then η is said to be the conjugate harmonic function of ξ. Equivalently, the pair
(ξ, η) is said to be a harmonic function pair (HFP). Conversely, if two functions
with continuous second-order partial derivatives satisfy Eq. (11.2), then both are
harmonic, i.e., fulﬁll the Laplace equation. This is seen by applying
∂
∂x and
∂
∂y,
respectively, to the Cauchy–Riemann equations, which yield:
∂2ξ
∂x2 = ∂2η
∂x∂y ,
∂2ξ
∂y2 = −∂2η
∂y∂x
(11.3)
where we have
∂2η
∂x∂y =
∂2η
∂y∂x because of the continuity assumption on the second-
order derivatives. Accordingly, the conjugate harmonic function of a known har-
monic function, ξ, is found by solving the Laplace equation using the Cauchy–
Riemann equations as boundary conditions. These equations stipulate that the gradi-
ents of a harmonic function pair are orthogonal to each other at every point, i.e., they
are locally orthogonal.
An analytic function is generally a complex function and is characterized by the
fact that it has complex derivatives of all orders. It can be shown that the imaginary
part of any analytic function is the conjugate harmonic function of the real part. With-
out loss of generality, we can assume both ξ and η to be single-valued by imposing
proper restrictions. Then by deﬁnition Eq. (11.2), an HFP curve pair,
ξ0 = ξ(x, y)
(11.4)
η0 = η(x, y)
(11.5)
has orthogonal gradients at the same point. For nontrivial ξ(x, y) and η(x, y), Eqs.
(11.4) and (11.5) deﬁne a coordinate transformation (CT) which is invertible almost
everywhere.
Let an image be represented by the real function f1(x, y). Another representation
of the image f2 can be obtained by means of a CT using the HFP,
f1(x(ξ, η), y(ξ, η)) = f2(ξ, η)
(11.6)
As before, the term image will refer to a subimage.
Deﬁnition 11.1. The image f(ξ, η) is said to be linearly symmetric in the coordinates
(ξ, η) if there exists a one-dimensional function g such that
f(ξ, η) = g(aξ + bη)
(11.7)
for some real constants a and b. Here ξ(x, y), η(x, y) is a HFP, and the symmetry
direction vector, (a, b), has its length normalized to unity, i.e.,
√
a2 + b2 = 1.
The notion “linearly symmetric in (ξ, η)” is, in analogy with Chap. 10, motivated
by the fact that the isocurves of such images are parallel lines in the ξη coordinates.
Likewise, such images have a high concentration of their spectral power along a line
through the origin.
Starting with the trivial unity transformation we give examples of CTs for pattern
families that can be modeled and detected by the toolbox to be presented.

11.1 Curvilinear Coordinates by Harmonic Functions
211
Fig. 11.1. The HFPs used in Examples 11.1 (top) and 11.2 (bottom), respectively
Example 11.1. Let w(z) be deﬁned as the identity coordinate transformation:
w(z) = z = ξ(x, y) + iη(x, y) = x + iy
Since w is an analytic function in z, (x, y) is a HFP. For illustration, we let the one-
dimensional function g be
g(τ) = (1 + cos τ)/2
(11.8)
while bearing in mind that g can be any 1D function. The argument τ is replaced by
aξ + bη to generate the family of isocurves deﬁned by this transformation. Figure
11.1 (top) illustrates the two basis patterns, g(ξ) = constant, and g(η) = constant
respectively. The linear combinations of these two patterns generate new patterns, all
of which belong to the same family of curves, those that are linearly symmetric w.r.t.
the Cartesian x and y, which we studied in Chap. 10.
Example 11.2. By using the same g as in Example 11.1, we can illustrate the trans-
formation deﬁned by
w(z) = log z = ξ(x, y) + iη(x, y) = log

x2 + y2 + i tan−1(x, y)
which is analytic everywhere except at the origin. We assume the principal branch
as the value set of w to avoid multiple-valued functions. Figure 11.1 (bottom) shows

212
11 Direction in Curvilinear Coordinates
Fig. 11.2. The HFPs used in Examples 11.3 (top) and 11.4 (bottom), respectively
the basis pair of this transformation. The local orthogonality can be seen by super-
imposing the two ﬁgures so that the origins coincide. The linear combinations of the
basis pair, aξ + bη, generate the family of logarithmic spirals. Some members of this
family are displayed in Fig. 11.3. We note that the sign of a·b determines the chirality
of the spirals, i.e., whether they are twisted to the left or to the right. By measuring
the direction angle of the vector (a, b), it will be possible to tell apart a left-handed
pattern from a right-handed pattern, as well as whether a pattern is circular or star
shaped, without actually knowing the gray levels of the pattern (g) in advance.
Example 11.3. We use the analytic function z2 to obtain the HFP ξ, η
w(z) = z2 = ξ(x, y) + iη(x, y) = x2 −y2 + i2xy
(11.9)
which is illustrated in Fig. 11.2 (top). The generated pattern family, aξ + bη corre-
sponds to rotated versions of a basis pattern. The asymptotes of the generated hyper-
bolic patterns are orthogonal and the direction of the cross is given by the direction
of (a, b), which, as will be discussed further below, can be used to detect “crosslike”
junctions.
Example 11.4. The analytic function √z:

11.2 Lie Operators and Coordinate Transformations
213
w(z) = √z = ξ(x, y) + iη(x, y) = √r exp

iϕ
2

= √r cos
ϕ
2

+ i√r sin
ϕ
2

generates the basis patterns that are illustrated in Fig. 11.2 (bottom). The pattern
family generated by this pair consists of rotated versions of one of the basis patterns;
that is, the rotation angle corresponds to the direction of the vector (a, b).
11.2 Lie Operators and Coordinate Transformations
Here we summarize the essential parts of the coordinate transformation theory using
differential operators to detect symmetric pattern families. We will brieﬂy present
the Lie operators that perform small (inﬁnitesimal) CTs, which are all equivalent to
translations in HFP coordinates.
We start by performing translations of ξ and η, which are coordinates that an
arbitrary image f will later be represented by, beginning with ξ. The translated1
coordinates are marked with
′ and yield:
T1(ξ, η, ϵ1) =

ξ′ = ξ + ϵ1,
η′ = η.
(11.10)
Two successive translations are equivalent to a single translation which can be ob-
tained by using the parameter combination rule:
φ(ϵ1, δ) = ϵ1 + δ
(11.11)
where φ is analytic with respect to both of its arguments and fulﬁlls the group axioms
with ϵ = 0 being the identity element of the group. These properties make T1 a one-
parameter Lie group of transformations [31]. With each Lie group of transformation
an inﬁnitesimal generator is associated. In this case, this is2 Dξ =
∂
∂ξ. When applied
to f the CT results in a translation of the isocurves of f along the basis vector,
ˆξ, which is the curvilinear basis related to ξ. Dξ applied to any function whose
isocurves consist of ξ(x, y) = λ delivers the tangent ﬁelds of ξ. An arbitrary amount
of translations in the ˆξ direction can be obtained by applying the exponential form of
Dξ:
f(ξ′, η′) =

1 + ϵ1Dξ + ϵ2
1
2! D2
ξ + · · ·

f(ξ, η) = exp(ϵ1Dξ)f(ξ, η)
(11.12)
which is a Taylor expansion of f(ξ + ϵ1, η) around (ξ, η). The CT (ξ′, η′) is com-
pletely determined by Dξ’s actions on the ξ, η coordinates. As a special case, if the
isocurves of f are given by ξ = constant, i.e., f(ξ, η) = g(ξ) for some g, then we
1 The symbol ′ represents new coordinates after a CT is applied to original coordinates in
this chapter.
2 In studies of CTs, it is common to use the notation L1 for Dξ, and L2 for Dη, where L is
a mnemonic for the “Lie operator”.

214
11 Direction in Curvilinear Coordinates
π
4
2π
4
3π
4
4π
4
5π
4
6π
4
7π
4
8π
4
Fig. 11.3. The top row shows the rotated patterns generated by g(z) = √z, i.e., when n =
−1 in Eq. (11.87), for various angles between the parameters. The second row displays the
corresponding curves for g(z) = log(z), i.e., when n = −2. The third row is ϕ, which
represents the parameter ratios used to generate the patterns. The change of ϕ represents a
geometric rotation on the top row, whereas it represents a change of bending in the second
row
have Dξf = g′(ξ), leaving the isocurve families of f invariant. The isocurve fami-
lies obtained for various ﬁxed λs by the equation ξ(x, y) = λ are integral curves of
Dξ, that is an isocurve maps to another isocurve, ξ(x, y) + ϵ1 = λ, which is within
the same family. A more powerful invariance is obtained if f(ξ, η) = g(η) for some
g. This yields Dξf = 0, that is, the isocurves η(x, y) = λ, are invariant. In this case
one isocurve maps onto itself.
Similarly, T2(ξ, η, ϵ2), which translates the η coordinate, is a one-parameter Lie
group of transformation:
T2(ξ, η, ϵ2) =

ξ′ = ξ,
η′ = η + ϵ2,
(11.13)
with the corresponding inﬁnitesimal generator
Dη = ∂
∂η
(11.14)
As before, one can reconstruct T2 by means of the operator exp(ϵ2Dη).
We can formally deﬁne a new inﬁnitesimal operator:
Dζ = cos θDξ + sin θDη
(11.15)
which is linear in Dξ, Dη, and expect to have a one parameter group of transfor-
mations T(ξ, η, ϵ) corresponding to it. This expectation can be realized precisely
because Dξ and Dη are tangent vector ﬁelds of a coordinate basis by construction.3
A ﬁnite motion along the integral curves of Dζ can be obtained by successive expo-
nentiations (Taylor series):
3 For general vector ﬁelds represented by
∂
∂λ and
∂
∂μ, in order for a coordinate basis generat-
ing them via tangents to exist, the vector ﬁelds must commute and be independent. Luckily,
Dξ and Dη] always commute if ξ, η are HFPs.

11.3 The Generalized Structure Tensor (GST)
215
exp(ϵDζ) = exp(ϵ cos θDξ) exp(ϵ sin θDη)
= exp(ϵ sin θDη) exp(ϵ cos θDξ)
(11.16)
Paraphrasing, Dξ and Dη together act as an operator basis pair for any translation
along a “line” (which is a linear combination of ξ, η) [31], which in turn makes
Dζ a classical directional gradient, but in the curvilinear coordinates (ξ, η). The
corresponding one-parameter Lie group of transformation is given by
T(ξ, η, ϵ1) =

ξ′ = ξ + ϵ cos θ,
η′ = η + ϵ sin θ,
(11.17)
with θ being a “directional” constant, characterizing a unique family of curves, which
are also the invariants of the operator Dζ, Eq. (11.15).
−ξ sin θ + η cos θ = constant
(11.18)
The converse is also true, i.e., Eq. (11.18) uniquely represents Dζ. In Fig. 11.3 we
show two curve families generated by using Eq. (11.18) for various θs. The bottom
row shows log(x + iy) = ξ(x, y) + iη(x, y), while the top row shows √x + iy =
ξ(x, y) + iη(x, y). Notice that when changing θ we generate the members of the
same pattern family, whereas when changing the ξ, η pair we generate completely
new families of patterns.
11.3 The Generalized Structure Tensor (GST)
We wish to know how well an arbitrary image can be described by means of analytic
curves. In our approach ξ and η are known a priori and are conjugate harmonic pairs,
whereas neither θ nor the gray values are known. We will attempt to ﬁt a family of
curves as deﬁned by Eq. (11.18) to an arbitrary image by ﬁnding its “closest member”
to the image. We do this by minimizing the following error or energy:
e(θ) =

|Dζf(ξ, η)|2dξdη =

|(cos θDξ + sin θDη)f(ξ, η)|2dξdη
(11.19)
with respect to θ. Since we do not know if the image is really a member of the space
linearly spanned by the ξ, η pair, the next best thing is to ﬁnd a member of this family
closest to our image and see if the error is small enough. Notice that e(θ) is a norm
(in the sense of L2), and from this it follows that when e(θ) is zero for some θ, the
image f fulﬁlls Dζf(x, y) = 0, almost for all x, y. By using Eq. (11.12), Eq. (11.19)
can be seen as the total error in a small translation. Furthermore, the error is the total
least square error as in the ordinary linear symmetry error function discussed in Sect.
10.10 except that the coordinates are now the curve pair ξ, η.
Example 11.5. To ﬁx the ideas, we return to Example 11.2 and note that the opera-
tors
∂
∂ξ and
∂
∂η have the curves

216
11 Direction in Curvilinear Coordinates
η = tan−1(x, y) = constant
and
ξ = log(

x2 + y2) = constant (11.20)
as invariants. Because Dξ =
∂
∂ξ acts as a zoomer and Dη =
∂
∂η as a rotator, the di-
rection given by θmin represents the amount of scaling versus rotation leaving f un-
changed (in practice, least changed). The e(θmin) contains information as to whether
θmin, which represents the symmetry “direction”, is or is not signiﬁcant.
The basis of the recognition is that e(θ) is small for some θ. We deﬁne
k(θ)
△=(cos θ, sin θ)T
(11.21)
and we rewrite the error function in quadratic form:
e(θ) =

k(θ)T
 ∂f
∂ξ
∂f
∂η
 ∂f
∂ξ , ∂f
∂η

k(θ)dξdη = k(θ)T Sk(θ)
(11.22)
where S is a matrix deﬁned as follows:
Deﬁnition 11.2. The matrix
S =
 ∂f
∂ξ
∂f
∂ξ dξdη,
 ∂f
∂ξ
∂f
∂η dξdη
 ∂f
∂ξ
∂f
∂η dξdη,
 ∂f
∂η
∂f
∂η dξdη

(11.23)
is called the generalized structure tensor (GST).
Applying Parseval–Plancherel identity to the elements of matrix S gives
S =

μ20(|F(ωξ, ωη)|2), μ11(|F(ωξ, ωη)|2)
μ11(|F(ωξ, ωη)|2), μ02(|F(ωξ, ωη)|2)

(11.24)
Hence, elements of S correspond to the second-order moments of the power spec-
trum (|F(ωξ, ωη)|2), which is the Fourier transform of image f(ξ, η) taken in the ξη
coordinates. It should be emphasized that the Fourier transform of f w.r.t. the (ξ, η)
coordinates is not the same as when taken w.r.t. the (xy) coordinates.
We know from Chap. 10 that the second-order moments can also be represented
by the second-order complex moments, Ipq with p + q = 2, see Eq. 10.36, so that
we can conveniently use another matrix as deﬁned next.
Deﬁnition 11.3. The complex representation of the generalized structure tensor is
given as :
Z = 1
2

I11(|F(ξ, η)|2) −iI20(|F(ξ, η)|2)
iI20(|F(ξ, η)|2)∗
I11(|F(ξ, η)|2)

(11.25)
where
I20 = S(1, 1) −S(2, 2) + i2S(1, 2)
I11 = S(1, 1) + S(2, 2)
(11.26)
with S(k, l) being the elements of S according to Eq. (11.23).

11.3 The Generalized Structure Tensor (GST)
217
It follows from the above that the building blocks of Z are obtained as follows:
I20(|F(ωξ, ωη)|2) =

(Dξf(ξ, η) + iDηf(ξ, η))2dξdη
(11.27)
and
I11(|F(ωξ, ωη)|2) =

|Dξf(ξ, η) + iLηf(ξ, η)|2dξdη
(11.28)
Again, the real moments μ20, μ02, and μ11, as well as the complex moments I20 and
I11 are all taken w.r.t harmonic coordinates as in Eqs. (11.23), Eq. (11.27), and Eq.
(11.28). We must still ﬁnd a way to obtain them directly by using Cartesian coor-
dinates. In different coordinate systems the representation of the Lie operators and
the linear symmetry operator may look quite different despite the identical physi-
cal effect when applied to a function. The representation of the Lie operators in the
Cartesian coordinates can be obtained by using the chain rule:
Dξ = xξ
∂
∂x + yξ
∂
∂y =
ξx
ξ2x + ξ2y
∂
∂x +
ξy
ξ2x + ξ2y
∂
∂y =
∇T ξ
∥∇ξ∥2 ∇
(11.29)
Dη = xη
∂
∂x + yη
∂
∂y = −
ξy
ξ2x + ξ2y
∂
∂x +
ξx
ξ2x + ξ2y
∂
∂y = ∇T
⊥ξ
∥∇ξ∥2 ∇(11.30)
where the deﬁnitions ξx = ∂ξ(x,y)
∂x
and ∇T
⊥ξ = (−ξy, ξx) are used for simplicity.
Moreover, the partial derivatives of ξ and η are obtained by inverting the Jacobian4
of T, and using the Cauchy–Riemann equations, Eq. (11.2):
∂2(x, y)
∂ξ∂η
=
 xξ xη
yξ yη

=
∂2(ξ, η)
∂x∂y
−1
=
 ξx ξy
ηx ηy
−1
=
1
ξ2x + ξ2y
ξx −ξy
ξy ξx

We note that Dη does not explicitly depend on partial derivatives of η with respect
to x or y, which of course is the consequence of the HFP assumption that binds the
gradients of ξ and η together.
Example 11.6. For illustration we go back to Example 11.2, and see that the corre-
sponding inﬁnitesimal operators are found by using Eqs. (11.29) and (11.30):
Dξ = x ∂
∂x + y ∂
∂y
Dη = −y ∂
∂x + x ∂
∂y
These are well-known scaling and rotation operators from differential geometry. By
applying Dξ to the coordinate pair x, y we obtain an inﬁnitesimal increment which
4 Represented by the symbol ∂2(ξ,η
∂∂y , a Jacobian of a 2D CT (x, y)T →(ξ, η)T is deﬁned as
the matrix
 
∂ξ
∂x
∂ξ
∂y
∂η
∂x
∂η
∂y
!
.

218
11 Direction in Curvilinear Coordinates
we can add to x, y to obtain the new coordinates (x′, y′)T . This will yield an in-
ﬁnitesimal scaling:
 Dξx = x
Dξy = y
⇒
x′ = x + xdx
y′ = y + ydy
(11.31)
Similarly, a small rotation is obtained as
Dηx = −y
Dηy = x
⇒
x′ = x −ydx
y′ = y + xdy
(11.32)
Dηx = −y ⇒x′ = x −ydx,
and
Dηy = x ⇒y′ = y + xdy.
(11.33)
We extend the deﬁnition of the ILST introduced in Chap. 10 to harmonic coordi-
nates
Deﬁnition 11.4. The inﬁnitesimal linear symmetry tensor w.r.t. the harmonic coordi-
nates ξ, η is deﬁned as
ℸξ,η(f)(ξ, η) = [Dξf(ξ, η) + iDηf(ξ, η)]2
(11.34)
The subscripts in ℸξ,η represent the derivation variables, i.e., they remind that the
two derivations of the operator are taken with respect to ξ and η variables, respec-
tively.
Lie operators can be translated to Cartesian coordinates even if they were initially
formulated in canonical coordinates. Accordingly, we can also translate ℸξ,η, which
consists of the square of a Lie operator applied to an image, as follows:
ℸξ,η(f)(ξ, η) = (Dξf + iDηf)2 = [(∇ξ + i∇⊥ξ)T ∇f]2/∥∇ξ∥4.
(11.35)
However, since
∇ξ + i∇⊥ξ =
 ξx −iξy
i(ξx −iξy)

= (ξx −iξy)
 1
i

(11.36)
where ξx −iξy is complex-valued (scalar function), we have the result
ℸξ,η(f)(ξ, η) = (ξx −iξy)2
|ξx −iξy|4

(1, i)

fx
fy
2
= (ξx −iξy)2
|ξx −iξy|4 (fx + ify)2
=
ℸ∗
ξ,η(ξ)(x, y)
|ℸ∗x,y(ξ)(x, y)|2 ℸx,y(f)(x, y).
(11.37)
which we restate in the following lemma.
Lemma 11.1. Under a harmonic conjugate basis change given by ξ, η, the ILST
changes basis according to:
ℸξ,η(f)(ξ, η) =
ℸ∗
x,y(ξ)(x, y)
|ℸ∗x,y(ξ)(x, y)|2 · ℸx,y(f)(x, y)
(11.38)
♦

11.3 The Generalized Structure Tensor (GST)
219
Notice that the basis change resulted in a decoupling of the terms according to
their dependency on coordinates. The ﬁrst term depends only on ξ, computable with-
out a knowledge of f, whereas the second term is dependent only on the image f,
computable without a knowledge of ξ. For convenience, in what follows we drop the
mnemonic subscripts of ℸx,y when it is clear from the context that the derivations
are w.r.t. x, y. Consequently, I20 is given by
I20 =

ℸξ,η(f)(ξ, η)dξdη
(11.39)
=

ℸ∗(ξ)(x, y)
|ℸ∗(ξ)(x, y)|2 ℸ(f)(x, y) det(∂ξ(x, y)
∂x∂y )dxdy
(11.40)
=

ℸ∗(ξ)(x, y)
|ℸ∗(ξ)(x, y)|ℸ(f)(x, y)dxdy
(11.41)
=

w20∗(x, y)ℸ(f)(x, y)dxdy
(11.42)
with
w20(x, y) = ℸ(ξ)(x, y)
|ℸ(ξ)(x, y)|
(11.43)
Here the Cauchy–Riemann equations applied to the functional determinant:
det
∂ξ(x, y)
∂x∂y

=
&&&&
ξx ξy
−ξy ξx
&&&& = ξ2
x + ξ2
y = |ℸ∗(ξ)(x, y)|
(11.44)
have been used. Accordingly, we have the following result:
Lemma 11.2. The complex moment I20 of the power spectrum in harmonic coordi-
nates can be estimated in Cartesian coordinates as follows
I20 =

w20∗(x, y)ℸ(f)(x, y)dxdy
(11.45)
where
w20(x, y) = ℸ(ξ)(x, y)
|ℸ(ξ)(x, y)|
(11.46)
♦
In Eq. (11.45), we note that I20 is obtained by projecting the ILST of the image
in Cartesian coordinates on the kernel w20, which consists of the ILST of the target
curve pattern, ξ, except for a magnitude normalization. This is a scalar product be-
tween a function, which only depends on the image and is independent of the target
pattern family ξ, and a kernel that encodes the directional information of the pattern
family. Consequently, once the ILST of the image is available it can be tested for
matching a multitude of pattern families by changing the corresponding kernel w20
without recalculating the ILST of the image. This is not surprising as the ILST of
the image represents “universal” information for pattern matching, namely the local
edges and the double of their directions [90,137].
Similarly, I11 which is the average ILST magnitude, can be obtained. We state
this result as a lemma.

220
11 Direction in Curvilinear Coordinates
Lemma 11.3 (Energy conservation). The sum of the maximum and the minimum
error is independent of the coordinate system chosen for symmetry investigation of
the image f:
I11 = e(θmax) + e(θmin) =

|ℸξ,η(f)(ξ, η)|dξdη
=

|ℸ(f)(x, y)|dxdy =
 ∂f
∂x
2
+
∂f
∂y
2
dxdy.
(11.47)
♦
This suggests the interesting property5 that I11 can be computed even without knowl-
edge of ξ. This is because I11, which is the upper bound of |I20| such that |I20| ≤I11
attains the upper bound with equality if and only if the image has linear symmetry
w.r.t. ξ, η curve family, is independent of the ξ, η. Accordingly, it follows that the
upper bound of |I20| is the same for all harmonic CTs, I11 computed for f w.r.t.
the (ordinary Cartesian) linear symmetry. An image cannot be both linear symmetric
w.r.t. (ordinary Cartesian) lines obtained as the linear combinations of ξ = x and
η = y and w.r.t. another harmonic curve family at the same time. This means that it
is possible to construct independent shape properties of f by measuring I20s w.r.t.
different CTs, ξ, η.
In summary, the elements of the GST can be found by ﬁltering ℸ(f) as stated in
the following theorem [18]:
Theorem 11.1 (Generalized structure tensor). The structure tensor theorem holds
in harmonic coordinates. In particular, the second-order complex moments determin-
ing the minimum inertia axis of the power spectrum, |F(ωξ, ωη)|2, can be obtained
in the (Cartesian) spatial domain as:
I20= (λmax −λmin)ei2ϕmin =

(ωξ + iωη)2|F|2dωξdωη
(11.48)
=

ℸξ,η(ξ, η)dξdη =

ℸ∗
x,y(ξ)
|ℸ∗x,y(ξ)|ℸx,y(f)dxdy
(11.49)
=

exp(i arg ℸ∗
x,y(ξ))ℸx,y(f)dxdy
(11.50)
I11= λmax + λmin =

(ωξ + iωη)(ωξ −iωη)|F|2dωξdωη
(11.51)
=

|ℸξ,η(f)|dξdη=

|ℸx,y(f)|dxdy
(11.52)
The quantities λmin, ϕmin, and λmax are, respectively, the minimum inertia, the di-
rection of the minimum inertia axis, and the maximum inertia of the power spectrum
of the harmonic coordinates, |F(ωξ, ωη)|2.
♦
5 Note that this conclusion is valid for the continuous representation. In the discrete case the
integral still needs a ξ-dependent kernel as will be discussed further below.

11.4 Discrete Approximation of GST
221
We emphasize that it is the coordinate transformation that determines what I20
and I11 represent and detect. Central to the generalized structure tensor is the har-
monic function pair ξ(x, y) and η(x, y), which creates new coordinate curves to rep-
resent the points of the 2D plane. An image f(x, y) can always be expressed by such
a coordinate pair ξ(x, y) and η(x, y) as long as the transformation from (x, y) to
(ξ, η) is one-to-one and onto. The deformation by itself does not create new gray
tones, i.e., no new function values of f are created, but rather it is the isogray curves
of f that are deformed. The harmonic coordinate transformations deform the ap-
pearance of the target patterns to make the detection process mathematically more
tractable. In the principle suggested by theorem 11.1, these transformations are not
applied to an image because they are implicitly encoded in the utilized complex ﬁl-
ters. The deformations occur only in the idea, when designing the detection scheme
and computing the ﬁlters.
11.4 Discrete Approximation of GST
For computation of the generalized structure tensor parameters, the ILST of the im-
age f,
ℸ(f)(x, y) = (Dxf + iDyf)2
(11.53)
is needed. This is, however, the same complex image that is used to compute the
ordinary structure tensor, discussed in Eq. (10.79) and in theorem 10.4. Accordingly,
provided that the image is densely sampled to a sufﬁcient degree, the ILST image
can be obtained through:
ℸ(f)(x, y) = (Dxf(x, y) + iDyf(x, y))2|(x,y)=(xj,yj) = [(fxj + ifyj)]2 (11.54)
where (xj, yj) is a point on the grid on which the original discrete image f is deﬁned.
Below we discuss how to estimate the GST elements on the discrete Cartesian grid,
(xj, yj).
Case 1: Estimation of I20 with known analytic expression of ξ(x, y)
Assuming that an analytic expression of ξ(x, y) is known explicitly, it follows from
this that ℸ(ξ)(x, y) is known. An approximation of I20 can be obtained by substitut-
ing the reconstructed (from its samples) ℸ(f),
ℸ(f)(x, y) =

j
ψ(x −xj, y −yj)ℸ(f)(xj, yj)
(11.55)
into Eq. (11.45)
I20 =

j
ℸ(f)(xj, yj)

ψ(x −xj, y −yj)[ ℸ(ξ)(x, y)
|ℸ(ξ)(x, y)|]∗dxdy
=

j
ℸ(f)(xj, yj)(w20
j )∗
(11.56)

222
11 Direction in Curvilinear Coordinates
where w20
j is given by:
w20
j
=

ψ(x −xj, y −yj) exp(i2 tan−1(ξx, ξy))dxdy
(11.57)
Here, we observe that the discrete kernel of I20, which is w20
j , is obtained by pro-
jecting the continuous kernel, Eq. (11.45),
w20(x, y) = ℸ(ξ)(x, y)
|ℸ(ξ)(x, y)| = exp(i2 tan−1(ξx, ξy))
(11.58)
onto the space of band-limited signals. We note that the continuous kernel has mod-
ulus 1 except at ∇ξ(x, y) = 0, where ℸ(ξ) is undeﬁned. At these points, w20 can
safely be assumed to be 0, as the values of the integrand on a set of points with zero
measure do not affect I20. Technically, Eq. (11.57) is a lowpass ﬁltering followed by
discretization, which is also known as perfect sampling. Thus, Eq. (11.56) is essen-
tially a matching of the direction of the basis tangent vector ﬁeld with the tangent
vector ﬁeld of the image. This observation will prove to be useful in Sect. (11.4).
Naturally, the closed form of the integral in Eq. (11.57) is not possible to obtain for
most ξs. However, w20
j
can be computed numerically and off line, e.g. [19], for pat-
tern recognition purposes. In one important case, when ξ = x, though, Eq. (11.57)
can be derived analytically and reduces to w20
j
= 1 which gives the ordinary struc-
ture tensor kernel for the image. Assuming that we will need I20 for a local image,
w20
j will however, be a window function, e.g., a Gaussian. There are other nontrivial
ξ(x, y)s yielding analytically tractable kernels, those with polynomial derivatives.
We will discuss this class further in Sect. 11.7.
Is it really worth computing w20
j
exactly through Eq. (11.57) to obtain a useful
approximation of I20? The answer to this question depends on the application at
hand. The computation of w20
j
through Eq. (11.57) and then substituting it in Eq.
(11.56) yields robust approximations of I20 since the weight zero is automatically
given to the appropriate points of the kernel at the same time as all kernel coefﬁcients
vary smoothly. If the digitized image fj, represents a small neighborhood, it might be
worth computing w20
j
in the aforementioned “orthodox” fashion (i.e., by projection
on the band-limited functions), as this yields less biased estimates. However, this
may not be worth doing if the number of singularity points is negligible compared to
the total number of the image points, since the bias of the singularity points will be
negligible. In this case one can use the approximation:
wj =

exp[i2 tan−1(ξx, ξy)]|x=xj,y=yj,
if ∇ξ(xj, yj) ̸= 0 ;
0,
if ∇ξ(xj, yj) = 0.
(11.59)
Case 2: Estimation of I20 with unknown analytic expression of ξ
Often a digital image of a prototype pattern ˜ξj is all that is known, and one would
like to know whether ˜ξj occurs in a discrete image fj or not. One cannot assume
that isocurves of the prototype are sampled isocurves of harmonic functions, i.e., the

11.4 Discrete Approximation of GST
223
isocurves fulﬁll the Laplacian equation, as this is difﬁcult to verify for an arbitrary
digital prototype, and it is not very helpful when ˜ξ is not strictly harmonic. The tilde
in ˜ξj is used in order to emphasize that we do not know whether ˜ξ is harmonic or not.
We can exploit the fact that the computation of I20 is essentially a matching between
the ILST image and the sampled version of the normalized ILST of the prototype.
As an algorithm implementing the discrete ILST operator according to Eq. (11.54)
is assumed to exist, we can apply such an algorithm to ˜ξj to obtain:
ℸ(˜ξ)(xj, yj) = [(˜ξxj + i˜ξyj)]2,
(11.60)
We can then proceed as if ˜ξj is a sampled harmonic function and ﬁnd the kernel w20
j
as
w20
j
=

exp(i2 tan−1(˜ξxj, ˜ξyj))
if ∇ξ(xj, yj) ̸= 0 ;
0,
if ∇ξ(xj, yj) = 0.
(11.61)
Accordingly, in analogy with Eq. (11.56), I20 can be computed as
I20 =

j
ℸ(f)(xj, yj)(w20
j )∗
(11.62)
The classical alternative to the case in this subsection is to directly match (cor-
relate) the two digital images, fj and ˜ξj, without ﬁltering them through the ILST
operator. However, matching the ILST image with an appropriate kernel has certain
advantages. In the ILST approach it is the edges of fj and not the gray values which
are aligned in case of match. As a consequence we can expect a high localization.
However, this is only a byproduct; the main advantage is the complex voting process
and its rich interpretability, as will be discussed in Sect. 11.5.
Case 3: Estimation of I11
According to Eq. (11.47), I11 is obtained as
I11 =

|ℸ(f(x, y))| · 1dxdy =

|ℸ(f)(x, y)||w20(x, y)|dxdy
(11.63)
Consequently, the continuous kernel of I11 is
w11 = |w20|
(11.64)
By using the reconstructed |ℸ(f)|, and in analogy with Eq. (11.56),
¯I11 =

j
|ℸ(f)(xj, yj)| ¯w11
j
(11.65)
where the discrete kernel,
¯w11
j
=

ψ(x −xj, y −yj)|w20(x, y)|dxdy
(11.66)

224
11 Direction in Curvilinear Coordinates
is the projection of the continuous kernel w11(x, y) onto the space of band-limited
signals. Using the kernel ¯w11
j in Eq. (11.65) yields a biased estimate of I11 as shown
below. The discrete kernel ¯w11
j is the perfect sampling of w11(x, y). The magnitudes
of the two kernels fulﬁll the inequality |w20
j | ≤¯w11
j
which is a weaker relation-
ship than the equality relationship in the continuous case, Eq. (11.64). Consequently,
when |w20
j | < ¯w11
j even for one kernel coefﬁcient j, we get
|I20| = |

j
ℸ(f)(xj, yj)w20
j
∗| ≤

j
|ℸ(f)(xj, yj)||w20
j |
<

j
|ℸ(f)(xj, yj)| ¯w11
j
= ¯I11
(11.67)
This implies that |I20| will not attain the value ¯I11 even if the image is linearly
symmetric in harmonic coordinates. By using the triangle inequality, it can be shown
that |I20| will attain the upper bound only when the discrete ILST of the image and
the kernel coefﬁcients are collinear. This behavior is similar to the continuous case,
as can be seen by applying the triangle inequality to Eq. (11.45) and comparing the
result to (11.47). In order to avoid the bias introduced by the discretization process,
we will use the discrete kernel,
w11
j
= |w20
j |
(11.68)
instead of Eq. (11.66) to compute
I11 =

j
|ℸ(f)(xj, yj)||w20
j |
(11.69)
where w20 is assumed to be available through either of the processes described in
Eqs. (11.57), (11.59), and (11.61).
11.5 The Generalized Hough Transform (GHT)
When an analytic expression for a target curve or a collection of curves is not avail-
able, provided that there is a discrete version of the target, ˜ξ, it is still possible to
detect it in discrete images. However, in this section we consider the alternative ap-
proach, the generalized Hough transform (GHT). In the subsequent section, we will
establish that GST is a GHT, with the additional capability to recognize antitargets
during the target recognition. We will dwell only on the case when an analytic ex-
pression for the target curve is not available. The discussion when the target curve is
analytically available is analogous and is omitted.
The chief tool to achieve machine recognition of general curves in images is the
GHT [12,111] which has been extensively studied [54,118,126,177]. GHT is pop-
ular for its robustness, because even when the occurrence of a target curve is only
partial in an image, for some reason, including occlusion, the method can ﬁnd the

11.5 The Generalized Hough Transform (GHT)
225
φj
Rj
Fig. 11.4. A collection of edges describing an object to be identiﬁed is shown in green.
An edge with the direction φj and the distance Rj to the reference point, marked with a ×,
are shown in magenta. The latter information is used by GHT to encode the position of the
prototype. All edge matches occuring in the target and the image result in a vote cast to the
corresponding reference point in the image
presence of target curves. It consists of a table look-up indexed by the direction of
the target curve and a voting procedure, [12,111]. The table entries are discrete edge
directions of the edges of the prototype. To each tabulated direction corresponds a list
of edge positions, expressed relative to a unique reference point (origin) of the pro-
totype. There is a direction associated with each contour edge, both in the image and
in the prototype. Typically the contour points and their directions are obtained after
applying a threshold to the gradient-ﬁltered image and the quantized gradient direc-
tions. This is because edges participating in the GHT poll are binary, i.e., they either
exist with a unique direction, or they do not. Given the table, one can easily construct
the contours of the prototype, or inversely construct the table, given the contours of
the prototype. The contours of a transistor, an example prototype, its reference point,
an edge, and its (φj, Rj) parameters used in a GHT table are shown in Fig. 11.4.
Accordingly, the GHT direction table is equivalent to a 2D template containing the
contour points and their directions. The GHT procedure consists of a 2D nonlinear
ﬁltering identical to sliding a template over the contour-direction image and counting
the direction matches per template position. Thus, the vote accumulator, A, is two-
dimensional and has usually the same resolution as the image itself in this process.
However, it may be necessary to estimate more than two parameters, e.g., transla-
tion, rotation, and scale. This can be handled by the 2D accumulators too, provided
that one can ﬁx the other parameters by use of a numerical optimization procedure,
such as gradient descent. The peaks in the accumulator A will indicate the positions
where the template presence is most likely. The accumulator votes are given by

226
11 Direction in Curvilinear Coordinates
(a)
(b)
(c)
(d)
(e)
Fig. 11.5. Electronic circuit and transistor detection. a,b. the original and the prototype. c,d.
|I20| and I11. e. GHT accumulator with 2π/256 angle resolution (from [22]). The circle is
inserted to show the location of the peak
A(xj, yj) =

l
δ(θ(xj + xl, yj + yl) −φ(xl, yl))
(11.70)
where l runs over the edge elements of the prototype, and φ(xl, yl) is the direction
of the prototype edge. The θ(xj + xl, yj + yl) corresponds to the direction of the
edge at the position (xj + xl, yj + yl) in the contour image, whenever there is an
edge pixel, or i∞. That θ is deﬁned to be i∞is purely symbolic and serves to make
θ different from φ, because expected contours modeled by the prototype may be
missing in the contour image. In such instances this will generate a zero contribution
(vote) from the δ-function. The array A(xj, yj) represents the vote accumulator and
can be constructed on the same grid as the image itself. Figure 11.5 shows the result
of detection of a transistor in an electronic circuit picture.
11.6 Voting in GST and GHT
The parameter estimation of the generalized structure tensor is more than a correla-
tion of edge magnitudes. It carefully takes into account the directions of the edges,
too. Here we show that the GST detection extends GHT, to a complex GHT in which
the votes are allowed to assume complex values.
The GHT accumulator, A(xj, yj), can be compared to an estimation of I20 at
local images around all image points,

11.6 Voting in GST and GHT
227
I20(xj, yj) =

l
ℸ(f)(xj + xl, yj + yl)w20∗(xl, yl)
=

l
exp[i2θ(xj + xl, yj + yl) −i2φ(xl, yl)]
(11.71)
The latter assumes that the same unitary gradient of the image and the unitary gra-
dient of the prototype as those used in the computation of A are employed to obtain
ℸ(f)(xj, yj) and w20(xj, yj), via Eq. (11.61), and Eq. (11.62). Consistent with Eq.
(11.70), θ = i∞whenever it is not deﬁned, i.e., when a pixel position does not rep-
resent an edge pixel position, the vote contribution of that point to I20 is reduced
to 0. While A(xj, yj) is the count of the positive matches only, |I20| is the positive
matching score adjusted downwards with the amount of negative matches. At the
reference point, when the image is the same as the prototype itself, we obtain the
maximum match with both of the techniques:
A = L,
I20 = L,
(11.72)
where L represents the number of edge pixels in the prototype. When 100% of the
edge directions mismatch maximally i.e., when all prototype directions are orthogo-
nal to the image directions, we obtain:
A = 0
I20 = −L
(11.73)
while if 50% of the edge directions match perfectly and 50% mismatch maximally
we obtain,
A = L
2 ,
I20 = 0.
(11.74)
In computing I20(xj, yj) the scores of the positions with contradictory gradient di-
rections will be reduced compared to the GHT scores, A(xj, yj). The GHT is with-
out score reduction since a score of A(xj, yj) is only allowed to increase (in case
of gradient direction match) or it is unchanged (in case of mismatch). For GHT this
is a necessity, because negative scores would not be meaningful in case they were
given to empty accumulators, while in the case of I20 this is allowed as this sim-
ply corresponds to a vote for a pattern which is locally orthogonal to the prototype,
antiprototype. Clearly, the computation of I20 is a voting process in which not only
negative but also complex votes are allowed.
Formally, nonprototype ILST images can be generated by a phase shift of the
prototype ILST image:
exp[iϕ0) exp(i2 tan−1(˜ξxj, ˜ξyj)]
(11.75)
But to which real patterns ϕ0 ̸= 0 corresponds, as this new ILST image is a purely
synthetic construct, is not obvious. This is because a phase shift of the prototype
ILST may result in tangent ﬁelds that are not always imaginable or intelligible by
visual inspection of Eq. (11.75). To give every ϕ0 an exact meaning, i.e., to ﬁnd a
ξj that approximates ˜ξj, an estimation should be made by numerical methods. For

228
11 Direction in Curvilinear Coordinates
z−1
z−0.5
log(z)
z0.5
z1
z1.5
z2
z2.5
Γ {−4,σ2} Γ {−3,σ2} Γ {−2,σ2} Γ {−1,σ2}
Γ {0,σ2}
Γ {1,σ2}
Γ {2,σ2}
Γ {3,σ2}
n = −4
n = −3
n = −2
n = −1
n = 0
n = 1
n = 2
n = 3
Fig. 11.6. The top row shows the harmonic functions, Eqs. (11.87), that generate the patterns
in the second row. The isocurves of the images are given by a linear combination of the real
and the imaginary parts of the harmonic functions on the top according to Eq. (11.88) with a
constant parameter ratio, i.e., ϕ = tan−1(a, b) = π
4 . The third row shows the ﬁlters that are
tuned to detect these curves for any ϕ, while the last row shows the symmetry order of the
ﬁlters
pattern recognition purposes, however, this will not be necessary given the control
possibility the I11 estimation offers. If, for an image for which 0 ≪|I20| ≈I11,
ϕ0 = arg I20 ̸= 0 is obtained, the nonprototype is known in reality too (as the
gradients come from a real image). Thus, in practice only when |I20| ≪I11 may
pose interpretation difﬁculties of arg I20, in which case no member of this class is a
good ﬁt to the data anyway.
Both the Hough accumulator value A and the value of I20 will be maximal and
identical to each other when there is maximal match between the prototype and the
image edges (i.e in GST terms when arg(I20) = 0 and |I20| = I11 ). However,
because of the complex votes, the two GST measurements, I20 and I11, additionally
offer detection of other prototypes not detected by the Hough transform, e.g., the
antiprototype when arg(I20) = π and |I20| = I11. We summarize our ﬁndings in the
following lemma.
Lemma 11.4. The GST is GHT with complex votes. Except for a possible vote re-
duction due to edge direction mismatch, the GHT accumulator value A is equivalent
to I20 in the 0 radian direction, i.e., GST will only sharpen the accumulator peaks of
the GHT. The GST parameter I20 in other directions than 0 radian, along with I11,
can detect and identify other prototypes not detected by GHT.
♦
11.7 Harmonic Monomials
Here we will discuss a speciﬁc harmonic function class with member functions hav-
ing direction ﬁelds that are monomials of z. As will be shown, this class of harmonic
function families is easily found analytically while they constitute computationally
powerful models to process symmetric patterns in images.
Assuming z = x + iy, we will study those
g(z) = ξ(x, y) + iη(x, y)
(11.76)

11.7 Harmonic Monomials
229
such that
ℸ∗(ξ)(x, y) = (Dxξ −iDyξ)2 = zn,
with
n = 0, ±1, ±2, · · · . (11.77)
This study is motivated because, to estimate I20 via theorem 11.1, ℸ∗(ξ) is needed6
and it makes sense to know what kind of curve families the ILST ﬁelds are projected
on,
ei arg([(Dx−iDy)ξ]2) = ei arg([ dg
dz ]2) = ei arg(zn) = zn
|zn| = ein arg(x+iy)(11.78)
First, we establish a relationship between the operator7 Dx −iDy and complex
derivatives as follows:
(Dx −iDy)ℜ[g(z)] = Dx[ℜg(z)] −iDy[ℜg(z)]
(11.79)
= ℜ[Dxg(z)] −iℜ[Dyg(z)]
(11.80)
= ℜ
dg
dz
dz
dx

−iℜ
dg
dz
dz
dy

(11.81)
= ℜ
dg
dz

−iℜ
dg
dz i

(11.82)
= ℜ
dg
dz

−iℜ

iℜ
dg
dz

−ℑ
dg
dz

(11.83)
= ℜ
dg
dz

+ iℑ
dg
dz

= dg
dz
(11.84)
Thus, we obtain:
ℸ∗(ξ(x, y)) = (dg(z)
dz
)2 = zn,
with
n = 0, ±1, ±2, · · · .
(11.85)
and establish
g(z) = z
b
2 ,
with
n = 0, ±1, ±2 · · ·
(11.86)
as a solution. We integrate dg
dz = z
n
2 to obtain the real and imaginary parts of g,
g(z) =

1
n
2 +1z
n
2 +1, if n ̸= −2;
log(z),
if n = −2.
(11.87)
The scheme discussed in Sects. 11.3 and 11.4 detects the patterns that are generated
by real and imaginary parts of g(z). Such patterns are shown in Fig. 11.6 by gray
modulation:
s(aξ + bη) = cos(aℜ[g(z)] + bℑ[g(z)])
(11.88)
The 1D function s(t) = cos(t) is chosen for illustration purposes. The ﬁlters that are
tuned to detect the isocurves aξ + bη are not sensitive to s, but to the angle
ϕ = tan−1(a, b).
(11.89)
6 Since ξ is harmonic, to study η does not represent freedom of choice but is determined as
soon as ξ is given.
7 The properties of such linear operators will be discussed in further detail in Sect. 11.9

230
11 Direction in Curvilinear Coordinates
11.8 “Steerability” of Harmonic Monomials
In Fig. 11.6, the angle is ﬁxed to ϕ = π
4 , and n is varied between −4 and 3. Each
n represents a separate isocurve family. By changing ϕ and keeping n ﬁxed, the
parameter pair (a, b) is rotated to (a′, b′). Except for the patterns with n = −2,
which we will come back to next, this results in rotating the isocurves, since for
n ̸= −2 and g(z) = z
n
2 +1 we have:
a′ξ + b′η = ℜ[(a′ −ib′)(ξ + iη)] = ℜ[(a′ −ib′)g(z)]
(11.90)
= ℜ[(a −ib)eiϕz
n
2 +1] = ℜ[(a −ib)g(ze
i
1
n
2 +1 ϕ)]
(11.91)
= aξ′ + bη′
(11.92)
Here ξ′ and η′ are rotated versions of the harmonic pair ξ and η, so that g(z expiϕ0) =
ξ′ + iη′ for some ϕ0. The top row of Fig. 11.3 displays the curves generated by Eq.
(11.88), for increasing values of ϕ in (a, b) = (cos(ϕ), sin(ϕ)), to illustrate that a
coefﬁcient rotation results in a pattern rotation.
When n = −2, we obtain the isocurves via the function
g(x + iy) = log(|x + iy|) + i arg(x + iy)
which is special in that it represents the only case when a change of the ratio between
a and b does not result in a rotation of the image pattern. Instead, changing the angle
ϕ bends the isocurves,
cos(ϕ) log(|x + iy|) + sin(ϕ) arg(x + iy) = constant
(11.93)
That is, the spirals become “tighter” or “looser” until the limit patterns, circles,
and radial patterns, corresponding to inﬁnitely tight and inﬁnitely loose spirals, are
reached.
The relationships (11.90)-(11.92) show that the isocurves of the patterns that are
modeled by harmonic monomials are obtained as a linear combination of the non-
rotated isocurves, except g(z) = log(z). Yet half of these patterns fail to fulﬁll the
steerability condition [75, 180]. The steerability condition foresees that the angu-
lar Fourier series expansion function, f, must have a limited number of elements
to allow steering by linear weighting of basis elements. In our case, g(z) = z
n
2 +1
does not satisfy the steerability condition on angular band-limitedness when n is odd
since it is not possible to expand odd powers of a square root with a limited number
of (integer) angular frequencies. The same holds evidently for the isocurve family
represented by the CT log(z), the members of which cannot be rotated by changing
the linear coefﬁcients. Consequently, the patterns with odd n or with n = −2 can-
not be generated by weighted sums of a low number of steerable functions. In turn,
this makes it impossible to detect the mentioned patterns by correlating the original
gray images with steerable ﬁlters. Yet, these patterns can be accurately generated by
analytic functions.8 They can even be detected by steerable ﬁlters, as will be shown
8 Analytic functions are harmonic, but they do not necessarily meet the steerablity condition
of [75,180].

11.9 Symmetry Derivatives and Gaussians
231
in Sect. 11.10, though not by gray image correlations as these ﬁlters were originally
intended for, but via GST ﬁeld correlations. We can conclude that the angular band-
limitedness condition is a sufﬁcient but not a necessary condition for steering the
rotations of 2D patterns or for their detection.
11.9 Symmetry Derivatives and Gaussians
In this section we describe a set of tools that will be useful when recognizing intri-
cate patterns that certain harmonic curve families and the Lie operators are capable
representing.
Deﬁnition 11.5 (Symmetry derivative). We deﬁne the ﬁrst symmetry derivative as
the complex partial derivative operator:
Dx + iDy = ∂
∂x + i ∂
∂y
(11.94)
The symmetry derivative resembles the ordinary gradient in 2D. When it is applied
to a scalar function f(x, y), the result is a complex ﬁeld instead of a vector ﬁeld.
Consequently, the ﬁrst important difference is that it is possible to take the (positive
integer or zero) powers of the symmetry derivative, e.g.,,
(Dx + iDy)2 = (D2
x −D2
y) + i(2DxDy)
(11.95)
(Dx + iDy)3 = (D3
x −3DxD2
y) + i(3D2
xDy −D3
y)
(11.96)
· · ·
Second, being a complex scalar, it is even possible to exponentiate the result of the
symmetry derivative, i.e., (Dx + iDy)nf, to yield nonlinear functionals:
[(Dx + iDy)nf]m
Had it not been for the two mentioned exponentiation properties, there would not be
any real reason to introduce the symmetry derivatives concept, because the ordinary
gradient operator, ∇, would be sufﬁcient in practice. As will be seen, however, the
symmetry derivatives posses properties that make them elegant tools when modeling
and detecting intricate patterns.
The operator (Dx + iDy)n will be deﬁned as the nth order symmetry derivative
since its invariant patterns (those that vanish under the linear operator) are highly
symmetric. In an analogous manner, we deﬁne, for completeness, the ﬁrst conjugate
symmetry derivative as Dx −iDy =
∂
∂x −i ∂
∂y and the nth conjugate symmetry
derivative as (Dx −iDy)n. We will, however, only dwell on the properties of the
symmetry derivatives. The extension of the results to conjugate symmetry derivatives
are straightforward.

232
11 Direction in Curvilinear Coordinates
Deﬁnition 11.6. We apply the pth symmetry derivative to the Gaussian and deﬁne
the function Γ {p,σ2} as
Γ {p,σ2}(x, y) = (Dx + iDy)p
1
2πσ2 e−x2+y2
2σ2
(11.97)
with Γ {0,σ2} being the ordinary Gaussian.
Theorem 11.2. The differential operator Dx + iDy and the scalar −1
σ2 (x + iy) op-
erate on a Gaussian in an identical manner:
(Dx + iDy)pΓ {0,σ2} = (−1
σ2 )p(x + iy)pΓ {0,σ2}
(11.98)
♦
The theorem, proved in the Appendix (Sect. 11.13), reveals an invariance property of
the Gaussians w.r.t. symmetry derivatives. We compare the second-order symmetry
derivative with the classical Laplacian, also a second-order derivative operator, to
illustrate the analytical consequences of the theorem. The Laplacian of a Gaussian,
(D2
x + D2
y)Γ {0,σ2} = (−2
σ2 + x2 + y2
σ4
)Γ {0,σ2}
(11.99)
can obviously not be obtained by a mnemonic replacement of the derivative symbols
Dx with x and Dy with y in the Laplacian operator. As the Laplacian already hints,
with an increased order of derivatives, the resulting polynomial factor, e.g.,, the one
on the right-hand side of Eq. (11.99), will resemble less and less the polynomial form
of the derivation operator. Yet, it is such a form invariance that the theorem predicts
when symmetry derivatives are utilized. By using the linearity of the derivation op-
erator, the theorem can be generalized, see Sect. 11.13, to any polynomial as follows:
Lemma 11.5. Let the polynomial Q be deﬁned as Q(q) = N−1
n=0 anqn. Then
Q(Dx + iDy)Γ {0,σ2}(x, y) = Q(−1
σ2 (x + iy))Γ {0,σ2}(x, y)
(11.100)
♦
That the Fourier transformation of a Gaussian is also a Gaussian has been known
and exploited in information sciences. It turns out that a similar invariance is valid
for symmetry derivatives of Gaussians, too.
A proof of the following theorem is omitted because it follows by observing that
derivation w.r.t. x corresponds to multiplication with iωx in the Fourier domain and
applying Eq. (11.98). Alternatively, theorem 3.4 of [208] can be used to establish it.
Theorem 11.3. The symmetry derivatives of Gaussians are Fourier transformed on
themselves, i.e.,

11.10 Discrete GST for Harmonic Monomials
233
F[Γ {p,σ2}] =

Γ {p,σ2}(x, y)e−iωxx−iωyydxdy
= 2πσ2(−i
σ2 )pΓ {p, 1
σ2 }(ωx, ωy)
(11.101)
♦
We note that, in the context of prolate spheroidal functions [204], and when con-
structing rotation-invariant 2D ﬁlters [52], it has been observed that the (integer)
symmetry order n of the function h(ρ) exp(inθ), where h is a one-dimensional func-
tion, ρ and θ are polar coordinates, is preserved under the Fourier transform. Accord-
ingly, the Fourier transforms of such functions are: H0[h(ρ)](ωρ), where H0 is the
Hankel transform (of order 0) of h. However, a further precision is needed as to the
choice of the function family h, to render it invariant to Fourier transform.
Another analytic property that can be used to construct efﬁcient ﬁlters by cascad-
ing smaller ﬁlters or simply to gain further insight into steerable ﬁlters or rotation-
invariant ﬁlters is the addition rule under the convolution. This is stated in the fol-
lowing theorem and is proved in the Appendix.
Theorem 11.4. The symmetry derivatives of Gaussians are closed under the convo-
lution operator so that the order and the variance parameters add under convolution:
Γ {p1,σ2
1} ∗Γ {p2,σ2
2} = Γ {p1+p2,σ2
1+σ2
2}
(11.102)
♦
11.10 Discrete GST for Harmonic Monomials
We discussed above how analytic functions g(z) generate curve families via a linear
combination of their real, ξ = ℜ[g(z)], and imaginary, η = ℑ[g(z)] parts. The curve
families generated by the real part are locally orthogonal to those of the imaginary
part. Here we discuss a speciﬁc family. The next lemma, a proof of which is given in
the Appendix, makes use of the symmetry derivatives of Gaussians to represent and
to sample the generalized structure tensor. Sampled functions are denoted as fk, i.e.,
fk = f(xk, yk), as before.
Lemma 11.6. Consider the analytic function g(z) with dg
dz = z
n
2 , and let n be an
integer, 0, ±1, ±2 · · · . Then the discretized ﬁlter Γ {n,σ2
2}
k
is a detector for patterns
generated by the curves aℜ[g(z)] + bℑ[g(z)] = constant, provided that a shifted
Gaussian is assumed as an interpolator and the magnitude of a symmetry derivative
of a Gaussian acts as a window function. The discrete scheme
I20(|F(ωξ, ωη)|2) = CnΓ {n,σ2
2}
k
∗(Γ {1,σ2
1}
k
∗fk)2
(11.103)
I11(|F(ωξ, ωη)|2) = Cn|Γ {n,σ2
2}
k
| ∗|Γ {1,σ2
1}
k
∗fk|2
(11.104)

234
11 Direction in Curvilinear Coordinates
where 0 ≤n and Cn is a real constant, estimates the direction parameter tan−1(a, b)
as well as the error via I20 and I11 according to theorem 11.1. For n < 0 the follow-
ing scheme yields the analogous estimates:
I20(|F(ωξ, ωη)|2) = CnΓ ∗{n,σ2
2}
k
∗(Γ {1,σ2
1}
k
∗fk)2
(11.105)
I11(|F(ωξ, ωη)|2) = Cn|Γ ∗{n,σ2
2}
k
| ∗|Γ {1,σ2
1}
k
∗fk|2
(11.106)
where Γ ∗{n,σ2
2} = (Γ {n,σ2
2})∗.
♦
We note that the parameter Cn is constant w.r.t. to (xl, yl) and has no implications
to applications because it can be assumed to have been incorporated to the image the
ﬁlter is applied to. In turn, this amounts to a uniform scaling of the gray-value gamut
of the original image. There are two parameters employed by the suggested scheme
that control ﬁlter sizes: σ1, which is the same as in the ordinary structure tensor,
determining how much of the high frequencies are assumed to be noise; and σ2,
representing the size of the neighborhood.
Equations (11.103) and (11.105) can be implemented via separable convolutions
since the ﬁlters Γ {n,σ2
2}
k
are separable for all n. The same goes for Eqs. (11.104) and
(11.106), provided that n is even. Consequently, for even n, both I20 and I11 can
be computed with 1D ﬁlters. For odd n, only |Γ {n,σ2
2}
k
| is not separable. For such
patterns, while I20 can be computed by the use of 1D ﬁlters, the computation of I11
will need one true 2D convolution or an inexact approximation of it obtainable, e.g.,
by the singular value decomposition of the 2D ﬁlter (Sect. 15.3). Alternatively, the
computational costs can still be kept small by working with small σ2 and Gaussian
pyramids.
Lemma 11.6 assumes that there is a window function whose purpose is to limit
the estimation of I20 and I11 to a neighborhood around the current image point. Apart
from n = 0, straight line patterns, the local gradient direction arg

dg
dz

= arg(z
n
2 )
is not well-deﬁned in the origin for patterns generated by Eq. (11.87). This is visible
in Fig. 11.6. The factor |x + iy|n in the window function is consequently justiﬁed,
since it suppresses the origin as information provider for n ̸= 0. Figure 11.6 shows
that the ﬁlters that are suggested by the lemma for various patterns vanish at the
origin except for the (Gaussian) one used for straight line extraction.
As mentioned, for n ̸= 0 the origins of the target patterns are singular. Because of
this, with increased |n|, the continuous image of such a pattern becomes increasingly
difﬁcult to discretize in the vicinity of the origin, to the effect that their discrete
images will have an appearance less faithful to the underlying continuous image
near the origin. As a consequence, approximating such patterns with band-limited
or other regular functions, a necessity for accurate approximation of the integrals
representing I20 and I11, will be problematic because the singularity at the origin
is barely or not at all accounted for already at the original discrete image. This can
be achieved by signal theoretically correct sampling [63], e.g., when the square of
an image on a discrete grid is needed, then the discrete image must be assured to

11.10 Discrete GST for Harmonic Monomials
235
x
y
←
←
←
←
ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo..................................................................................................................................................................................................................
Point #10
Point #10
Point #9
Point #9
Point #7
Point #7
Point #4
Point #4
Point #5
Point #5
Point #3
Point #3
Point #2
Point #2
Point #1
Point #1
Point #6
Point #6
Point #8
Point #8
Fig. 11.7. Detecting cross markers and their direction in crash tests (from [23]). (Top, left)
The ideal model of a cross with ϕ =
π
8 . (Top, right) The gray tones change linearly with
the function sin(2ϕ)(x2 −y2) −cos(2ϕ)2xy, where ϕ = π
8 . (Bottom, left) The ﬁrst frame
of an image sequence with the tracked forward (◦, thick curve) and backward (·, thin curve)
trajectory of the cross attached to the head. (Bottom, right) The automatically identiﬁed crosses
have been oversampled, before pixelwise squaring is applied. Oversampling can be
effectively implemented by separable ﬁlters and pyramids [43].

236
11 Direction in Curvilinear Coordinates
11.11 Examples of GST Applications
Example 11.7. In vehicle crash tests, the test event is ﬁlmed with a high-speed cam-
era to quantify the impact of various parameters on human safety by tracking mark-
ers. A common marker is the “cross”, which allows, to quantitate the planar position
of an object as well as its planar rotation (Fig. 11.7). Markers have to be tracked
across numerous frames (in the order of hundreds to thousands). The tracking has to
be fast and robust in that the markers should not be lost from frame to frame. The
rotations and translations of the objects are not constant due to the large accelera-
tions/decelerations, while severe light conditions are common between two frames
(e.g., imperfect ﬂash synchronization). A symmetry tracker using hyperbolas, i.e.,
n = 2
⇒
g(z) = z2 = x2 −y2 + i2xy
(11.107)
has been used to model the family of cross markers. Because of this, the cross-
markers were detected and simultaneously their rotation angles were quantitated
automatically. The image at the bottom, left shows the original superimposed tra-
jectory of the head, whereas the image on the right shows the identiﬁed crosses [23].
Alternatively, the candidate cross markers could have been detected by lack of linear
symmetry in Cartesian coordinates, but then the rotation angles of the crosses would
have to be identiﬁed separately. This is because a lack of a symmetry in any coordi-
nate system does not provide the angle informationa since only the presence of an
explicitly modeled curve family has a well-deﬁned direction whereas the lack of a
modeled curve family cannot provide the direction information, e.g. [97].
Example 11.8. In biometric authentication, alignment of two ﬁngerprints without
extraction of minutiae9 has gained increased interest, e.g., [108], since this im-
proves the subsequent person authentication (minutiae-based or not) performance
substantially. Besides improved accuracy, this helps to reduce the costly combina-
torial match of ﬁngerprint minutiae. Recently, silicon-based imaging sensors have
become cheaply available. However, because sensor surfaces are decreasing, in or-
der to accommodate them to portable devices, e.g., mobile phones, the delivered
images of the ﬁngerprints are also small. In turn this results in fewer minutiae points
that are available to consumer applications, which is an additional reason for why
nonminutiae-based alignment techniques in biometric authentication have gained in-
terest. A high automation level of accurate ﬁngerprint alignment is desirable inde-
pendent of which matching technique is utilized. For robustness and precision, an
automatical identiﬁcation of two standard landmark types: core and delta (Fig. 11.8)
has been proposed [169]. These can be modeled and detected by symmetry deriva-
tive ﬁlters in a scheme based on lemma 11.6. Naturally, the used coordinate trans-
formations are different than the one modeling the cross markers. Furthermore, the
detection is performed within a Gaussian pyramid scheme to improve the SNR. The
real and imaginary parts of the analytic functions, i.e.,
9 Typically a minutia point is the end of a line or the bifurcation point of two lines.

11.11 Examples of GST Applications
237
O
Fig. 11.8. The “+” and the “□” represent the delta and core (parabolalike) points that have
been automatically extracted in several ﬁngerprint images. Top, left and bottom, left are two
ﬁngerprints of the same ﬁnger that differ signiﬁcantly in quality. The certainties are 0.84 (+)
and 0.64 (□) in top, left. The certainties are 0.73 (□), 0.22 (“+”) and 0.10 (O) in bottom, left
n = −1
⇒
g(z) = z
1
2
(11.108)
were used to model core, whereas
n = 1
⇒
g(z) = z
3
2
(11.109)
were used for delta, see also Figs. 11.6 and 11.8.
Example 11.9. Figure 11.9 shows the result of GST to locate letters for an optical
character recognition system [186], in Sinhala script. Letters of a script encoding
a natural language can be more effectively modeled by GST than direct gray values
because occlusion and noise affects the directional information less than gray values.
In this use of GST the analytic expressions of the pattern isocurves are not known,
but they are deducted from the structure tensor variations of the reference characters,

238
11 Direction in Curvilinear Coordinates
Fig. 11.9. The illustration of a segmentation-free OCR for Sinhala text (Sri Lanka), imple-
mented by use GST, after [186]. The points are painted colored at the places where a support
for a GST character template has been found
in analogy with Figs. 11.4 and 11.5. Similarly, Fig. 11.10 illustrates the recognition
of Ethiopian script.
Example 11.10. Two-dimensional gel electrophoresis is an important tool of pro-
teomics, e.g., in search of new drugs. The result is 2D data that can be viewed as
images (Fig. 11.11). These contain spots that are maps of a large number of proteins,
requiring automatic image processing for efﬁcient analysis. Quantiﬁcation of indi-
vidual proteins and tracing changes in expression between gels require accurate pro-
tein detection. One can approach the problem by modeling circles via the monomial
family corresponding to n = −2. A set of computationally cheap and robust symme-
try derivative features can implement a detector that is particularly tuned to circles.
In the same ﬁgure, the color image represents I20 using the CT implied by log z. The
red hue represents the presence of circles. Brightness represents the certainty. Using
this model as one of the features, an efﬁcient classiﬁcation and segmentation can be
achieved [182].
11.12 Further Reading
In computer vision, normally one does not locate an edge or compute the direction
of a line by correlation with multiple templates consisting of rotated edges, incre-
mented with a small angle. However, multiple correlations with the target pattern
rotated in increments are commonly used to detect other shapes. This approach is
also used to estimate the direction of such shapes. The number of rotations of the
template can be ﬁxed a priori or, as in [30], dynamically. The precision of the es-
timated direction is determined by the number of rotated templates used or by the
amount of computations allowed. Although such techniques yield a generally good
precision when estimating the afﬁne parameters, which include target translation and
rotation, in certain applications a beforehand undetermined number of iterations may
not be possible or be desirable due to imposed restrictions that include hardware and
software resources. Furthermore, the precision and/or convergence properties remain
satisfactory as long as the reference pattern and the test pattern do not violate the

11.12 Further Reading
239
Fig. 11.10. Recognition of characters in Ethiopian text. The colors encode the labels of some
identiﬁed target characters
image constancy hypothesis severely. In other words, if the image gray tones repre-
senting the same pattern differ nonlinearly and signiﬁcantly between the reference
and the test image, then a good precision or a convergence may not be achieved.
An early exception to the “rotate and correlate” approach is the pattern recog-
nition school initiated by Hu [112], who suggested the moment-invariant signatures
to be computed on the original image, which was assumed to be real-valued. Later,
Reddi [188] suggested the magnitudes of complex moments to efﬁciently implement
the moment invariants of the spatial image, mechanizing the derivation of them. The
complex moments contain the rotation angle information directly encoded in their
arguments as was shown in [25]. An advantage they offer is a simple separation of
the direction parameter from the model evidence, i.e., by taking the magnitudes of
the complex moments one obtains the moment invariants, which represent the evi-
dence. The linear rotation-invariant ﬁlters suggested by [51,228] or the steerable ﬁl-
ters [75,180] are similar to ﬁlters implementing complex moments. With appropriate
radial weighting functions, the rotation-invariant ﬁlters can be viewed as equivalent
to Reddi’s complex moment ﬁlters, which in turn are equivalent to ﬁlters implement-
ing Hu’s geometric invariants. From this viewpoint, the suggestions of [2, 199] are
also related to the computation of complex moments of a gray image, and hence de-
liver correlates of Hu’s geometric invariants. Despite their demonstrated advantage
in the context of real images, it is, however, not a trivial matter to model isocurve
families embedded in gray images by correlating complex moment ﬁlters with gray

240
11 Direction in Curvilinear Coordinates
Fig. 11.11. (Left) An electrophoresis image of protein expression. (Right) the linear symmetry
in log z coordinates (I20), where red represents presence of circles
images. By contrast, this is possible when (GST) tensor-valued images are correlated
with (GST) tensor valued ﬁlters.
Sharing similar integral curves with the generalized structure tensor, the Lie op-
erators of [72,107] should be mentioned. However, these studies have not provided
tools on how to estimate the parameters of the integral curves, e.g., the direction and
the estimation error.
Granlund and Knutsson ﬁrst showed that convolving complex images by com-
plex ﬁlters can result in detection of intricate patterns [140]. In the follow-up stud-
ies [17, 22, 23, 129] the GST theory, which achieved isocurve detection and iden-
tiﬁcation by use of analytic models and tensor ﬁelds, was introduced. Hansen [94]
studied the structure tensor when it is extended to curves and surfaces in three and
higher dimensional spaces.
We have used a biometric person authentication application to illustrate the the-
ory. A review of biometric recognition with brief summaries of the main schools are
given in [175].
11.13 Appendix
Proof of theorem 11.2 and lemma 11.5
We prove the theorem by induction.
1. Eq. (11.98) holds for p = 1.
2. Assume (induction) that Eq. (11.98) is true for p = p0, for some p0 ≥1.
3. Then

11.13 Appendix
241
Γ {p0+1,σ2}(x, y) =
=
1
2πσ2 (Dx + iDy)(Dx + iDy)p0 exp(−x2 + y2
2σ2
)
=
1
2πσ2 (Dx + iDy)
−x
σ2 + i−y
σ2
p0
exp

−x2 + y2
2σ2

(11.110)
where we indicated by brackets [ ] the terms on which the differential operators
act, is obtained by applying the induction assumption (point 2). Consequently,
and by using the chain rule as well as the linearity of the partial differential
operators, we obtain:
Γ {p0+1,σ2}(x, y)
=
1
2πσ2

(Dx + iDy)
−x
σ2 + i−y
σ2
p0
exp

−x2 + y2
2σ2

· · ·
+
1
2πσ2
−x
σ2 + i−y
σ2
p0 
(Dx + iDy) exp

−x2 + y2
2σ2

(11.111)
By repeated applications of the algebraic rules that govern the differential oper-
ators, we obtain:
Γ {p0+1,σ2}(x, y)
=
1
2πσ2

Dx
−x
σ2 + i−y
σ2

p0 + iDy
−x
σ2 + i−y
σ2

p0

exp

−x2 + y2
2σ2

· · ·
+σ2
−x
σ2 + i−y
σ2

p0
−x
σ2 + i−y
σ2

exp

−x2 + y2
2σ2

=
1
2πσ2

p0
−1
σ2
−x
σ2 + i−y
σ2

p0−1 + i2p0
−1
σ2
−x
σ2 + i−y
σ2

p0−1

· · ·
× exp

−x2 + y2
2σ2

+
1
2πσ2
−x
σ2 + i−y
σ2

p0+1 exp

−x2 + y2
2σ2

=
1
2πσ2
−x
σ2 + i−y
σ2

p0+1 exp

−x2 + y2
2σ2

(11.112)
Consequently, when it holds for p = p0, Eq. (11.98) will also hold for p = p0+1.
Now we turn to the general case, lemma 11.5.
Q(Dx + iDy)
1
2πσ2 exp(−x2 + y2
2σ2
) =
=
1
2πσ2 [
N−1

n=0
an(Dx + iDy)n] exp(−x2 + y2
2σ2
)
and obtain

242
11 Direction in Curvilinear Coordinates
Q(Dx + iDy)
1
2πσ2 exp(−x2 + y2
2σ2
) =
(11.113)
=
1
2πσ2
N−1

n=0
an(−x
σ2 + i−y
σ2 )n exp(−x2 + y2
2σ2
)
= Q(−x
σ2 + i−y
σ2 )
1
2πσ2 exp(−x2 + y2
2σ2
)
(11.114)
in which the linearity of derivative operators and Eq. (11.98) have been used.
■
Proof of theorem 11.4
We can ignore nearly all steps below to conclude that the theorem holds, provided
that the following conditions (to be proven) are granted: (i) convolution and sym-
metry derivatives are distributive, i.e., (Dx + iDy)[f ∗g] = [(Dx + iDy)f] ∗g =
f ∗[(Dx + iDy)g], and (ii) the Gaussian convolutions are variance-additive, i.e.,
Γ {0,σ2
1} ∗Γ {0,σ2
2} = Γ {0,σ2
1+σ2
2}.
Convolution in the Fourier transform domain reduces to an ordinary multiplica-
tion:
Γ {p1,σ2
1} ∗Γ {p2,σ2
2} ↔F[Γ {p1,σ2
1}]F[Γ {p2,σ2
2}]
(11.115)
By using Eq. (11.101) we note that
F[Γ {p1,σ2
1}](ωx, ωy) = 2π
−i
σ2
1
p1
σ2
1Γ
{p1, 1
σ2
1
}(ωx, ωy)
(11.116)
Due to Eq. (11.98), we can write:
F[Γ {p1,σ2
1}] · F[Γ {p2,σ2
2}]
(11.117)
= 2π
−i
σ2
1
p1
σ2
1Γ
{p1, 1
σ2
1
}(ωx, ωy) · 2π
−i
σ2
2
p2
σ2
2Γ
{p2, 1
σ2
2
}(ωx, ωy)
= 2πσ2
1
−i
σ2
1
p1
(−σ2
1)p1(ωx + iωy)p1
1
2πσ2
1
exp

−σ2
1
ω2
x + ω2
y
2

· · ·
×2πσ2
2
−i
σ2
2
p2
(−σ2
2)p2(ωx + iωy)p2
1
2πσ2
2
exp

−σ2
2
ω2
x + ω2
y
2

=
−i
σ2
1
p1 −i
σ2
2
p2
(−σ2
1)p1(−σ2
2)p2(ωx + iωy)p1+p2 exp

−ω2
x + ω2
y
2
1
σ2
1+σ2
2

Here, we have used the deﬁnition of Γ {p, 1
σ } to obtain the Gaussian terms. Cancelling
the redundant terms allows us to write the product as:

11.13 Appendix
243
F[Γ {p1,σ2
1}] · F[Γ {p2,σ2
2}] = (i)p1+p2(ωx + iωy)p1+p2 exp

−ω2
x + ω2
y
2
1
σ2
1+σ2
2

= 2π(σ2
1 + σ2
2)
2π(σ2
1 + σ2
2)(i)p1+p2(ωx + iωy)p1+p2 exp

−ω2
x + ω2
y
2
1
σ2
1+σ2
2

(−σ2
1 −σ2
2)p1+p2
(−σ2
1 −σ2
2)p1+p2
= 2π(σ2
1 + σ2
2)

i
−σ2
1 −σ2
2
p1+p2
Γ
{p1+p2,
1
σ2
1+σ2
2
}(ωx, ωy)
(11.118)
Remembering, Eq. (11.115) we now inverse Fourier transform Eq. (11.118) by using
Eq. (11.101) and obtain:
Γ {p1,σ2
1} ∗Γ {p2,σ2
2} = Γ {p1+p2,σ2
1+σ2
2}
(11.119)
■
Proof of lemma 11.6
We use theorem 11.1 to estimate I20, for which (Dx −iDy)ξ is needed.10 We write
the coordinates as a complex variable z = x+iy and remember their relationship be-
tween symmetry derivatives and complex derivatives that was derived in Eq. (11.84).
(Dx −iDy)ℜ[g(z)] = dg
dz
(11.120)
But dg
dz = z
n
2 , so that we can obtain the complex exponential as:
ei arg{[(Dx−iDy)ξ]2} = ei arg([ dg
dz ]2) = ei arg(zn) = zn
|zn| = ein arg(x+iy)
Consequently, the expression I20 in equation (11.50) of theorem 11.1 reduces to
I20=

((Dξ + iDη)f)2dξdη =

ein arg(x+iy)[(Dx + iDy)f]2dxdy
We assume that [(Dx+iDy)f]2 is discretized on a Cartesian grid and use a Gaussian
as interpolator11 to reconstruct it from its samples:
h(x, y) = 2πσ2
1

k
hkΓ {0,σ2
1}(x −xk, y −yk)
(11.121)
Here hk represents the samples of h(x, y), and the constant 2πσ2
1 normalizes the
maximum of Γ {0,σ2
1} to 1. We include the window function Kn|x+iy|nΓ {0,σ2
2}(x, y),
where Kn is the constant12 that normalizes the maximum of the window function to
10 Because ξ is harmonic, η does not represent freedom of choice, but is determined as soon
as ξ is given.
11 This is possible by using a variety of interpolation functions, not only Gaussians. For exam-
ple, the theory of band-limited functions allows such a reconstruction via the Sinc functions
but also other functions have been discussed along with Gaussians, see interpolation and
scale space reports of [141,152,221].
12 At |x + iy| = (nσ2
2)1/2, the window Kn|x + iy|nΓ {0,σ2
2}(x, y) attains the value 1 when
Kn = 2πσ2
2(
e
nσ2
2 )n/2.

244
11 Direction in Curvilinear Coordinates
1, into [(Dx + iDy)f]2, to estimate Eq. (11.121) in a neighborhood:
I20 =

Kn|x + iy|nein arg(x+iy)Γ {0,σ2
2}(x, y)[(Dx + iDy)f]2dxdy
(11.122)
By assuming 0 ≤n and substituting Eq. (11.121) in Eq. (11.122), we obtain
I20(x′, y′)
(2πσ2
1)Kn
=
(11.123)

|x + iy|nein arg(x+iy)Γ {0,σ2
2}(x, y)

k
hkΓ {0,σ2
1}(x −xk, y −yk)

dxdy
Noting that
|x + iy|nein arg(x+iy)Γ {0,σ2
2}(x, y)
= (x + iy)nΓ {0,σ2
2}(x, y) = (−σ2
2)nΓ {n,σ2
2}(x, y)
(11.124)
where we used the deﬁnition of Γ {n,σ2
2} in Eq. (11.97) and applied theorem 11.2, we
can estimate I20 on a Cartesian grid:
I20(x′, y′)
(2πσ2
1)Kn(−σ2
2)n
=

k
hk
 
Γ {n,σ2
2}(x, y)Γ {0,σ2
1}(x′ −x −xk, y′ −y −yk)dxdy
=

k
h(xk, yk)(Γ {n,σ2
2} ∗Γ {0,σ2
1})(x′ −xk, y′ −yk)
=

k
h(xk, yk)Γ {n,σ2
1+σ2
2}(x′ −xk, y′ −yk)
(11.125)
Here Eq. (11.125) is obtained13 by utilizing theorem 11.4. Equation (11.125) can be
computed on a Cartesian discrete grid by the substitution (x′, y′) = (xl, yl), yielding
an ordinary discrete convolution:
I20(xl, yl) = Cn

h ∗Γ {n,σ2
1+σ2
2}
(xl, yl)
(11.126)
with Cn = (2πσ2
1)Kn(−σ2
2)n.
The result for n < 0 is straightforward to deduce by following the steps after
Eq. (11.125) in an analogous manner. Likewise, the scheme of I11 is obtained by
following the same idea as for I20.
■
13 We note that in the proof of theorem 11.4, theorem 11.3 is needed, so that all of the theorems
of this paper are actually utilized in the proof of this lemma.

12
Direction in ND, Motion as Direction
In this chapter we extend our discussion of the direction estimation problem from a
2D setting in Cartesion and curvilinear coordinates to an estimation problem in N
dimensions (ND). We start the discussion in 3D. To the extent that we can recognize
the solutions from treatment of 2D, the solutions of 3D problems related to direction
estimation will be inherited from their 2D analogues. This will provide an oppor-
tunity to focus on speciﬁc issues of direction estimation not present in 2D. In turn,
the experience from 3D will help us draw conclusions on the problem of direction
estimation in ND.
12.1 The Direction of Hyperplanes and the Inertia Tensor
Whether or not a 3D image has a direction can be studied in the Fourier transform
domain1 in analogy with Chap. 10, although the computations will be carried out in
the spatial domain. Let f be a positive real function deﬁned on E3 with F being its
3D Fourier transform. As before, we will sometimes call f an image and its values as
gray values, although the values of f can represent a variety of physical properties in
applications. They can, for example, represent the light intensities as observed on a
video camera sensor to form image sequences, f(x, y, t), or the absorption of X-ray
in an organic tissue to form X-ray tomography images, f(x, y, z). The function f
will still be called an image, even if it represents a local image.
Intuitively, if f has a “direction” then this has to be related to the locus of f’s
isogray values, points which have the same function values. In 3D the loci of isogray
values are more complex to describe than in 2D because f’s being constant can be
achieved along a 1D curve, the simplest of which is a line, or along a 2D surface, the
simplest of which is a plane. First, we will discuss the simple manifolds, namely the
1 An equivalent formulation in the spatial domain utilizing the Lie operators [31] as is done
in Sect. 11.2, is possible in 3D Cartesian coordinates. We prefer the frequency-domain
derivation for its straightforward geometric interpretation in terms of the habitual line and
plane ﬁtting.

246
12 Direction in ND, Motion as Direction
lines and planes embedded in 3D such that f does not change value. Subsequently,
we will show that the problems of describing the direction when isovalues are planes
or lines are mathematically the same. We will ﬁrst be concerned with the case of
planar loci since these have some notational advantages. We deﬁne them directly for
the general ND case, from which the 3D case follows directly.
Deﬁnition 12.1. The image f(r), where r ∈EN, is called linearly symmetric if the
loci of its isogray points constitute hyperplanes that have a common direction, i.e.,
there exists a scalar function of one variable g such that
f(r) = g(kT r)
(12.1)
for some vector k ∈EN. The direction of the linear symmetry is ±k.
It should be noted that although g(x) is a one-dimensional function, g(kT r) is a
function deﬁned on ND. The way they are deﬁned assures that the linearly symmet-
ric images have the same gray values at all points r satisfying
kT r = constant
(12.2)
which is the equation of a hyperplane embedded in ND having the normal k. The
hyperplane itself is an (N −1)-dimensional surface (which is embedded in ND).
This corresponds to an ordinary plane (surface) in E3 when N = 3.
Example 12.1. A step edge, deﬁned in 3D as:
f(r) =

1, if kT
0 r ≥0;
0, otherwise;
(12.3)
is equal to χ(kT
0 r), where χ is the usual one-dimensional step function:
χ(x) =
 1, if x ≥0;
0, otherwise.
Thus f deﬁned in Eq. (12.3) is linearly symmetric.
We can Fourier transform a linearly symmetric image deﬁned in 3D, by following
steps analogous to Eqs. (10.102)–(10.103):
F(ω) = F(g(kT r))(ω) = G(kT ω)δ(u1
T ω)δ(u2
T ω)
(12.4)
In Fig. 12.1, the dashed triangular plane shows a linearly symmetric image in the
3D space having the coordinate axes represented by three black basis vectors. The
red basis vectors show the corresponding axes in the 3D Fourier transform domain,
whereas the vector in black shows k, the axis on which all the energy of the Fourier
transform of the image is concentrated. Note that k is orthogonal to the plane. Natu-
rally, this can be extended to ND, a result which we state as a lemma.

12.1 The Direction of Hyperplanes and the Inertia Tensor
247
Fig. 12.1. The green axes show the 3D Euclidean space in which a linearly symmetric image
is deﬁned, the dashed triangular plane. The red axes show the corresponding 3D Fourier
transform domain axes in which a concentration of the energy to the axis represented by the
vector k occurs
Lemma 12.1. A linearly symmetric image, f(r) = g(kT
0 r), has a Fourier transform
concentrated to a line through the origin:
F(ω) = G(ωT k0)δ(ωT u1)δ(ωT u2) · · · δ(ωT uN−1)
where k0, u1 · · · uN−1 are orthonormal vectors in EN, and δ is the Dirac distribu-
tion. G is the one-dimensional Fourier transform of g.
♦
The lemma states that the function g(kT
0 r), which is in general a “spread” func-
tion, is compressed to a line, even for functions deﬁned on spaces with higher di-
mension than two. To detect linearly symmetric functions is consequently the same
as to check whether or not the energy is concentrated to a line in the Fourier domain.
To simplify the discussion, we assume that f is deﬁned on E3 and we attempt to ﬁt
an axis to F, through the origin of the corresponding 3D Fourier transform domain.
min
∥k∥=1 eL(k) =

E3
(dL(ω, k))2|F(ω)|2dE3
(12.5)
where ω = (ω1, ω2, ω3)T represents the frequency coordinates corresponding to
the spatial coordinates r = (x1, x2, x3)T . The dE3 is equal to dω1dω2dω3, and the

248
12 Direction in ND, Motion as Direction
superscript L is a label reminding us that we are ﬁtting a line to the spectrum. The
line-ﬁtting process is the same as identifying iso-values of parallel planes in the r
domain. If the energy of the Fourier transform is interpreted as the mass density,
then eL(k) is the inertia of a mass with respect to the axis k. Because the integral
Eq. (12.5) is a norm, i.e.,
eL(k) = ⟨dLF, dLF⟩= ∥dLF∥2
(12.6)
eL(kmin) vanishes if and only if F is concentrated to a line.
The distance function is given by:
(dL(ω, k))2 = ∥ω −(ωT k)k∥2
=
#
ω −(ωT k)k
$T #
ω −(ωT k)k
$
Using matrix multiplication rules and remembering that ωT k is a scalar and identical
to kT ω and ∥k∥2 = kT k = 1, the quadratic form:
(dL(ω, k))2 = kT #
IωT ω −ωωT $
k
is obtained. Thus Eq. (12.5) is expressed as
eL(k) = kT Jk
(12.7)
with
J =
⎛
⎝
J(1, 1) J(1, 2) J(1, 3)
J(2, 1) J(2, 2) J(2, 3)
J(3, 1) J(3, 2) J(3, 3)
⎞
⎠
where J(i, j)’s are given by
J(i, i) =

E3

j̸=i
ω2
j |F(r)|2dE3
(12.8)
and
J(i, j) = −

E3
ωiωj|F(ω)|2dE3
when
i ̸= j.
(12.9)
Notice that the matrix J is symmetric per construction. The minimization problem
formulated in Eq. (12.5) is solved by k corresponding to the least eigenvalue of the
inertia matrix, J, of the Fourier domain [231]. All eigenvalues are real and non-
negative and the smallest eigenvalue is the minimum of eL. The matrix J contains
sufﬁcient information to allow computation of the optimal k in the TLS error sense
Eq. (12.5). As is its 2D counterpart, even this matrix is a tensor because it represents
a physical property, inertia.
The obtained direction will be unique if the least eigenvalue has the multipicity2
1. When the multiplicity of the least eigenvalue is 2, there is no unique axis k, but
2 An N ×N matrix has N eigenvalues. If two eigenvalues are equal then that eigenvalue has
the multiplicity 2. If three eigenvalues are equal then the multiplicity is 3, and so on.

12.2 The Direction of Lines and the Structure Tensor
249
plenty of them by which the image can be described as g(kT r). Representing an im-
portant “degenerate” case, the energy in the Fourier domain will then be distributed
in such a way that there is a 2D plane (instead of an axis) containing an inﬁnite
number of axes that give the (same) least square error. This plane is deﬁned by the
subspace generated by linear combinations of the eigenvectors belonging to the least
eigenvalue, which has the multiplicity 2. The dimension of eigenvector subspaces is
always equal to the multiplicity of the eigenvalue to which the eigenvectors belong,
whereas the eigenvectors belonging to different eigenvalues are always orthogonal.
This is due to the fact that J is positive semideﬁnite and symmetric by deﬁnition,
Eqs. (12.5) and (12.7). Accordingly, such a 3 × 3 matrix always has three orthogo-
nal eigenvectors, where those possibly belonging to the same eigenvalue will not be
unique. This conclusion can be naturally extended to N dimensions.
Lemma 12.2. An inertia tensor J in N-D has N orthogonal eigenvectors with the
reservation that those belonging to the same eigenvalue, with a multiplicity 2 or
larger, are not unique.
♦
12.2 The Direction of Lines and the Structure Tensor
Here, we will discuss what the degenerate solutions (the nonunique eigenvectors) of
the spectral line-ﬁtting problem corresponds to in the spatial domain. We investigate
the issue ﬁrst for 3D spectra. Assuming that the multiplicity of the least eigenvalue
is 2, the energy will be concentrated to a 2D plane, spanned by the eigenvectors of
the least eigenvalue. What sense will this plane make when the intention was to ﬁt a
line through the origin and what we found is a plane (instead of a line)?
Suppose that we were intending to ﬁt the (complex) spectral function F a plane
under the condition that it had to pass through the origin. Representing the normal of
the plane by k, we would then minimize
min
∥k∥=1 eP(k) =

E3
(dP(ω, k))2|F(ω)|2dE3
(12.10)
where the P reminds us that we are attempting to ﬁt a plane and dP is the Euclidean
distance between the point ω, and the plane with the normal k in the spectrum,
(dP(ω, k))2 = (ωT k)2 = kT ωωT k
(12.11)
Compared to line-ﬁtting, the main difference is in the distance function dP(ω, k),
which is now a projection of the point ω on the normal vector k, the shortest distance
to the plane. Thus Eq. (12.10) reduces to minimizing
eP(k) = kT Sk
(12.12)
where

250
12 Direction in ND, Motion as Direction
S(i, j) =

E3
ωiωj|F(ω)|2dE3
(12.13)
Comparing Eq. (12.13) with Eqs. (12.8) and (12.9) yields an invertible algebraic
relationship between the matrices S and J, arising from line-ﬁtting and plane-ﬁtting
problems, respectively.
J = Trace(S)I −S
(12.14)
Here “Trace(S)” is the trace of a matrix that is the sum of all eigenvalues of S. It
can be computed conveniently by summing up S’s diagonal elements. Like J, the
matrix S is also positive semideﬁnite, and the solution to the plane-ﬁtting problem is
given by the least eigenvalue of S and its corresponding eigenvector(s). The matrix
S deﬁned by Eq. (12.13) is the structure tensor in n dimensions. Both of the matri-
ces S and J are tensors because they encode physical qualities, i.e., the scatter and
the inertia along with the extremal axes of a mass distribution. The scatter and the
inertia remain the same, regardless the coordinate frames they are measured in. The
axes do not change relative the mass distribution. Relative to coordinate frames not
attached to the mass distribution, their representation varies up to a rotation, which
is a viewpoint transformation.
Lemma 12.3. The tensors J and S have common eigenvectors, that is,
Ju = λ′u
⇔
Su = λu
(12.15)
with
λ′ = Trace(S) −λ
(12.16)
♦
The lemma is a consequence of the fact that J and S commute but can also be proven
immediately by utilizing the relationship Eq. (12.14) and operating with J on u,
which is assumed to be an eigenvector of S.
According to the lemma, ﬁtting a line to F or ﬁtting a plane to F can be achieved
by a quadratic form using the same tensor, i.e., either of S or J. Then, the following
lemma holds, too.
Lemma 12.4. Let the spectrum be 3D and that the eigenvalues of the structure tensor
S are enumerated in ascending order, 0 ≤λ3 ≤λ2 ≤λ1. If and only if
0 = λ3 < λ2
(12.17)
the FT values are zero outside of a plane through the origin. The normal of this
plane is given by u3, the least signiﬁcant eigenvector of S. On the plane, the FT
values equal the 2D FT of the values of f collected from any plane with normal u3.
Similarly, if and only if
0 = λ3 = λ2 < λ1
(12.18)
the FT is concentrated to a line through origin. The direction of this line is given by
u1, the most signiﬁcant eigenvector of S. The FT values on the plane are given by
the 1D FT of f lying on any of the lines with the direction u1.
♦

12.2 The Direction of Lines and the Structure Tensor
251
The ﬁrst part of the lemma predicts that in case a 2D image g(x, y) is translated
as we move along the the third dimension t, the FT values are concentrated to a
plane. This will be discussed in detail in Sect. 12.6 as this lemma will be useful
to quantitate motion in image sequences, such as TV images. In such images the
direction of the plane will be interpreted as velocity. Mathematically, the reach of the
lemma is, however, not restricted only to the (x, y, t)-type images, where t is time,
but any type of 3D images such as magneto resonance images, or X-ray tomography,
which are images deﬁned on an xyz coordinate frame, there all coordinates have the
same physical interpretation, the length. The direction in such images will mean the
surface direction of anatomic structures, such as the surface of the gray matter in the
brain, or the direction of a blood vessel.
A generalization of lemma 12.4 to higher dimensions, where we have the func-
tion F(ω) with ω ∈EN and N > 3, can be done by ﬁtting a hyperplane to F. The
(weighted) average distance to the hyperplane having the normal k is then
∥kT ωF∥2 = kT (

ωωT |F|2dω)k = kT Sk
(12.19)
where the weights are given by the spectral energy |F|2. This is minimized by the
least signiﬁcant eigenvector of S, being the scatter tensor of F (or the structure tensor
of f). In the ideal situation of a perfect hyperplane ﬁt, the least eigenvalue must be
zero, whereas the remaining (N −1) eigenvalues must be nonzero. However, more
than 1 eigenvalue could be zero too, i.e., using the sorting convention
0 ≤λN ≤λN−1 · · · ≤λ1
(12.20)
for labelling the eigenvalues, and calling the multiplicity of the null eigenvalue n, we
will have
0 = λN = λN−1 · · · = λN−(n−1) < λN−n
(12.21)
Then, the (range) value space of S will be a hyperplane with the dimension N −n.
We summarize this in the following theorem:
Theorem 12.1. Let
S =

ωωT |F|2dω
(12.22)
be the scatter tensor of the function F(ω), where ω ∈EN, and 2 ≤N. Then, if
and only if the null space3 of S has the multiplicity n, with n being an integer, and
0 < n < N, F is concentrated to a hyperplane with dimension N −n, through the
origin. The hyperplane is given by the points satisfying Sω = 0, and the values of F
on this plane are given by the (N −n)-dimensional FT of f lying on any hyperplane
spanned by the null space of S.
♦
3 The space represented by the eigenvectors belonging to the eigenvalue zero is the null
space: S ω=0. Regardless of its dimension, the null space always includes the vector ω =
0.

252
12 Direction in ND, Motion as Direction
12.3 The Decomposition of the Structure Tensor
To study the decomposition in terms of its subspaces, we discuss the structure tensor
in 3D, i.e., S is 3 × 3. According to the spectral theorem of positive semideﬁnite
operators [136], the structure tensor in ND can be written as a weighted sum of its
eigenvector subspaces. In 3D this yields,
S = λ1u1uT
1 + λ2u2uT
2 + λ3u3uT
3
(12.23)
which is also called the spectral decomposition. As theorem 12.1 suggests, the di-
mension of the null space of S determines if the tensor has “succeeded” to ﬁt a plane,
a line, or none of these to the image. Accordingly, the following three cases can be
distinguished. The analogy to 2D, see Section 10.7, is discernable, but there are now
three basic cases in 3D, instead of 2.
The spectral line: This is the case when λ2 equals the least signiﬁcant eigenvalue:
0 = λ3 = λ2 < λ1
(12.24)
Accordingly, the null space and the line will be given in parametric form by
Sω = 0,
when
ω = αu3 + βu2,
(12.25)
Sω ̸= 0,
when
ω = γu1,
(12.26)
where α, β and γ are arbitrary real scalars. Alternatively, the line is given by solving
for ω in the underdetermined system of equations:
uT
3 ω = 0
(12.27)
uT
2 ω = 0
(12.28)
which is the same as searching for the space perpendicular to the null space.
The spectral plane: This is the case when
0 = λ3 < λ2
(12.29)
The null space and the plane will be given in parametric form by
Sω = 0,
when
ω = αu3,
(12.30)
Sω ̸= 0,
when
ω = βu2 + γ, u1
(12.31)
where α, β, and γ are arbitrary real scalars. The plane can also be obtained by writing
down the condition for ω to be perpendicular to the null space:
uT
3 ω = 0
(12.32)
The balanced directions: This is the case when all three eigenvalues are equal:
0 < λ3 = λ2 = λ1
(12.33)

12.3 The Decomposition of the Structure Tensor
253
Accordingly this case has a nullspace with dimension 0. There is a point symmetry
in the distribution of the energy to the effect that no direction is different than other
directions.
We deﬁne the tensors
U1 = u1uT
1 ,
U2 = u2uT
2 ,
U3 = u3uT
3 ,
(12.34)
and note that these are orthogonal in the sense of Eq. (3.45) and therefore constitute
a basis. Accordingly, Eq. (12.23) yields:
S = λ1U1 + λ2U2 + λ3U3
(12.35)
which can be interpreted as an orthogonal expansion of the structure tensor, and the
eigenvalues are the coordinates encoding the tensor S in terms of the basis. Inversely,
we could try to synthesize an S by varying the coordinates, but we would then need
to obey
0 ≤λ3 ≤λ2 ≤λ1
(12.36)
This means that the coordinates cannot be chosen independent of each other in a
synthesis process. To simplify the synthesis, we could deﬁne a new set of coordinates
that are independent of each other as follows:
0 ≤λ′
1 = λ1 −λ2
0 ≤λ′
2 = λ2 −λ3
0 ≤λ′
3 = λ3
⇔
0 ≤λ′ = Cλ
(12.37)
with
λ′ =
⎛
⎝
λ′
1
λ′
2
λ′
3
⎞
⎠,
C =
⎛
⎝
1 −1
0
0
1 −1
0
0
1
⎞
⎠,
λ =
⎛
⎝
λ1
λ2
λ3
⎞
⎠
(12.38)
The new coordinates λ′ simply encode the increments between the subsequent eigen-
values and can therefore be chosen freely, as long as they are positive or zero. The
structure tensors must thus be chosen in a cone bounded by a certain basis tensor
corresponding to the new coordinates. To ﬁnd the new basis we only need to follow
the standard rule of linear algebra. Given the coordinate transformation matrix C,
the basis tranformation matrix equals C−1:
[U′
1U′
2U′
3] = [U1U2U3]C−1
(12.39)
so that we have
C−1 =
⎛
⎝
1 1 1
0 1 1
0 0 1
⎞
⎠
⇒
U′
1 = U1
U′
2 = U1 + U2
U′
3 = U1 + U2 + U3
(12.40)
Consequently, the structure tensor decomposition, Eq. (12.23), can always be rear-
ranged in terms of independent coordinates as

254
12 Direction in ND, Motion as Direction
S = (λ1 −λ2)u1uT
1 + (λ2 −λ3)(u1uT
1 + u2uT
2 ) +
+ λ3(u1uT
1 + u2uT
2 + u3uT
3 )
(12.41)
(12.42)
This rearrangement allows a direct interpretation of the eigenvalues in terms of our
three fundamental cases. The line case is the dominating structure when 0 ≈λ2 ≪
λ1, i.e., the ﬁrst term is largest. The plane case is the dominating structure when
0 ≈λ3 ≪λ2, i.e., the second term is largest. The balanced direction case is the
dominating structure when 0 ≪λ3 ≈λ2 ≈λ1, i.e., the last term is largest. Accord-
ingly,
λ′
1 = CL = λ1 −λ2
λ′
2 = CP = λ2 −λ3
λ′
3 = CB = λ3
with
U′
1 = u1uT
1
U′
2 = u1uT
1 + u2uT
2
U′
3 = u1uT
1 + u2uT
2 + u3uT
3 = I
(12.43)
where the three coordinates, CL, CP, CB, can be used as a certainty or saliency
for S representing a line, a plane, and a balanced directions structure. That u1uT
1 +
u2uT
2 + u3uT
3 equals the identity matrix I in the last row follows from the fact that
I can be expanded in the orthonormal basis Uj = ujuT
j by using the scalar product,
Eq. (3.45):
⟨Uj, I⟩=

k,l
uj(k)uj(l)δ(k −l) =

k
|uj(k)|2 = 1
(12.44)
Following a similar procedure, these results can be generalized to the spectral
decomposition of the an N × N structure tensor as follows:
Lemma 12.5 (Structure tensor decomposition). Assuming that λj, uj constitute
the eigenvalue and eigenvector pairs of the symmetric positive deﬁnite matrix S, the
spectral decomposition of S yields
S =

j
λjujuT
j =

j
λ′
jU′
j
(12.45)
where
λ′
1 = λ1 −λ2,
λ′
2 = λ2 −λ3,
· · ·
λ′
N = λN−1 −λN,
λ′
N = λN,
with
U′
1 = u1uT
1 ,
U′
2 = u1uT
1 + u2uT
2 ,
· · ·
U′
N−1 = u1uT
1 + u2uT
2 + · · · uN−1uT
N−1,
U′
N = u1uT
1 + u2uT
2 + · · · uNuT
N = I.
(12.46)
For j < N, λ′
j, represents the certainty for S to represent a j-dimensional hyper-
plane, whereas λ′
N represents the certainty for S to represent a perfectly balanced
structure.
♦

12.4 Basic Concepts of Image Motion
255
It is worth noting that in image analysis applicatons, all directions may not have
the same physical meaning. This implies that distances in the N-dimensional space
on which f, and thereby S is deﬁned may lack physical relevance, although direc-
tions may still be meaningful. For example, in a TV sequence, the ﬁrst two dimen-
sions are length, whereas the third dimension is time, e.g., see Figs. 12.2 and 12.1.
Depending on what unit has been chosen for the time (seconds or hours) and the
length (meter or km) and whether or not the points are in the same image frame, the
interpretation of the Euclidean length between two points in the 3D spatio–temporal
sequence will be different. However, the direction of lines and planes will represent
the velocity, a physically meaningful quantity. We will discuss the spatio–temporal
direction below.
12.4 Basic Concepts of Image Motion
Visual motion analysis is a vital processing element of the mammalian visual sys-
tems, which include human vision. Even still image processing, which would nor-
mally be handled by still image analysis tools in computer vision, are handled via
motion analysis pathways of the brain. For example, even when analyzing a painting
on the wall, the human eyes perform saccades, whereby the motif is brought to an
artiﬁcial motion on the retina to the effect that the relevant visual ﬁeld is analyzed
by the cells of the brain that are motion-direction, and/or spatial-direction sensitive.
While the current understanding of mammalian vision still leaves room for discus-
sion on the reasons why biological vision has gone to 3D signal analysis to solve
2D problems, it is beyond doubt that motion is a very valuable and powerful feature.
By contrast, it is fair to say that much of the development efforts in computer vision
have been steered towards still image analysis tools essentially because of the limi-
tations in computational resources, even if the power of motion in image analysis is
not a disputed issue. However, because of the rapid developments in computer and
communication technologies, this argument is swiftly being suppressed as a raison
d’etre.
The motion of a body can be observed by an eye or by artiﬁcial imaging equip-
ment. In this section we will discuss the translational motion of a local patch, as this
is a good starting point to approximate even more complex types of motions, e.g.,
those that can bring an image into rotation. We discuss the motion of a surface patch
of a 3D object moving relative to a camera in a small time interval. The velocity
of the motion is a 3D vector. When the moving patch is observed by a camera (on
a 2D plane), the light reﬂections from the surface are projected to the image plane.
Also, the 3D velocity vector is projected to the image plane, now becoming a 2D
vector ﬁeld. The vector ﬁeld representing the translations of the moving object sur-
face patches is known as the motion ﬁeld. Whether or not the motion ﬁeld is actually
identiﬁable by a vision system from the observed projections of the object surface
depends on many factors, such as the surface color, its texture, the optical properties
of the material when interacting with light, and the illumination. The observable ver-
sion of the motion ﬁeld is known as optical ﬂow. In many circumstances the optical

256
12 Direction in ND, Motion as Direction
Fig. 12.2. (Left) The uniform motion of a line in the image plane generates the magenta plane
in the 3D spatio–temporal signal. (Right) The uniform motion of a set of points, shown in
black, generates a set of parallel lines, drawn in magenta
ﬂow ﬁeld will be a good approximation of the motion ﬁeld, although there are nu-
merous circumstances in which the approximation will be poor. In terms of a rotating
sphere, we will discuss the distinction between the two ﬁelds and the identiﬁcation of
the motion from images of the motion further in Sect. 12.12. Below we will summa-
rize the essentials of the translational motion assuming that the motion ﬁeld equals
to the optical ﬂow and vice versa.
We will study the motion by modeling it as a 2D image patch that has been
brought into motion according to a model, e.g., a translation or an afﬁne motion.
When doing this we also assume that the following constraint is satisﬁed by the
spatio–temporal image observations:
Deﬁnition 12.2 (BCC). The brightness constancy constraint (BCC) is satisﬁed if the
colors of the spatio–temporal image points representing the same 3D points, remain
unchanged throughout the spatio–temporal image.
The simplest motion model is the translational model. In the following we discuss
how the 2D content inﬂuences the computation of the translational motion. The ob-
servability of translation depends on the directional content of the 2D pattern brought
into such a motion. Leaving out the nonobservable translation of a constant (gray
value or color), there are two fundamental classes that can be discerned:
•
The translation of linearly symmetric 2D images, i.e., the linear symmetry com-
ponent of the 2D structure tensor is nonzero whereas the balanced direction com-
ponent is zero.
•
The translation of 2D images that have more than one directions, i.e., the bal-
anced direction component of the 2D structure tensor is nonzero.
Assuming a continuum of image frames, i.e., a continuous 3D volume, the motion
of a line generates isosurfaces4 that are equal to a tilted plane. If there are more
parallel lines in motion there will be more layers of planes that will be parallel to
each other, like a cheeseburger. The more the motion plane tilts, the faster the motion
4 An isosurface is the set of points for which f(x, y, t) = constant.

12.4 Basic Concepts of Image Motion
257
Fig. 12.3. (Left) The translational motion of a linearly symmetric patch. (Right) The trans-
lational motion of a patch containing two distinct directions. The actual and the observable
translations are drawn as magenta and black vectors, respectively
is perceived. This situation is shown in Fig. 12.2, left, where the normal vector of the
motion plane (magenta) is drawn in blue and the (normal) motion vector of the line
is drawn in black.
By contrast, the motion of a point generates a line in the 3D continuous image.
This situation is shown in Fig. 12.2, right, where dots translate upwards in the image
plane, as represented by the black arrow. The result is many parallel lines, remini-
scient of a bundle of spaghetti.
The two fundamental translation types are not new types of phenomena. They
are not even speciﬁc to the physics of motion. Indeed, these occur as a consequence
of physical quantities that happen to be equivalent to or deduced from the structure
tensor. In particular, the motion vectors in the image plane are uniquely determined
by the normal of the plane, k, if the 2D image patch is linearly symmetric (a 2D
property). Likewise, the 2D optical ﬂow is uniquely determined by the direction of
the generated 3D line, k, if the 2D structure tensor of the image patch has a balanced
direction tensor component that is nonzero.5 In both cases the vector k is a 3D vector
and the optical ﬂow is insensitive to its direction, i.e., −k is as good as k when it
comes to represent, the velocity. Accordingly, the same tensor,
kkT
(12.47)
can represent both types of translational motion conveniently.
5 This is equivalent to saying that the image patch is nontrivial (nonconstant) and it lacks
linear symmetry.

258
12 Direction in ND, Motion as Direction
12.5 Translating Lines
Assuming that the normal of the tilting plane is k, how is the 2D normal ﬂow ob-
tained from this 3D vector? In this case we have a linearly symmetric image in 2D,
g(cos(θ)x0 + sin(θ)y0)
(12.48)
where g(τ) is a one-dimensional function, the vectors
s0 = (x0, y0)T ,
a = (cos(θ), sin(θ))T
(12.49)
represent the coordinates of an arbitrary point in the image plane, and the normal
of the line(s) deﬁning the linearly symmetric image, respectively. The 2D image in
Eq. (12.48) is manufactured by replacing the argument of the 1D function with the
“equation” of a line:
cos(θ)x0 + sin(θ)y0 = τ
(12.50)
Clearly, the gray value does not change as long as we are on a certain line, i.e.,
the x0, y0 pair that satisﬁes Eq. (12.50), and therefore the notion of line is justiﬁed
when speaking about linearly symmetric images. We take this CT reasoning one step
further and translate one of the lines in the pattern g.
We can assume that the position vector s0 = (x0, y0)T represents a (spatial) point
in the image plane at the time instant t = 0 and we wish to move it with the velocity
va, where a is the direction of the velocity (∥a∥= 1) and v is the absolute speed.
A velocity in the direction orthogonal to a, i.e., the line moves “along itself”, will
not be observable. This problem is also known as the aperture problem. That is why
only in the direction a can a motion be observed in a linearly symmetric image, Eq.
(12.48). It may not be the true motion, but it is the only motion that we can observe.
Accordingly, after time t, a point on the line can be assumed to have moved to the
position
s(t) = (x(t), y(t))T = s0 + vt · a
(12.51)
so that s0 = s −vt · a, although we should be aware that the point may have actually
moved to any place along the line. The vector va is called the normal image velocity
or normal optical ﬂow. Substituting, the gray-values expression in Eq. (12.48) yields
a spatio–temporal image (sequence) in which the lines of g move with the same
velocity:
g(aT s0) = g(aT s(t) −vtaTa) = g(aT s −vt) = g(˜k
T r)
(12.52)
Here we have deﬁned the new variables ˜k and r as the spatial variables augmented
with the temporal variables −v and t, respectively.
˜k = [aT | −v]T ∈E3,
r = [sT |t]T ∈E3
(12.53)
However, Eq. (12.52) represents a linearly symmetric 3D image having paral-
lel planes as isosurfaces. The vector ˜k is thus equal to the normal of the plane

12.6 Translating Points
259
kT r = constant that will be ﬁt by the most signiﬁcant eigenvector of the 3 × 3
structure tensor of f(x, y, t). Notice that the ﬁrst two elements of ˜k are normalized
to have length 1. Accordingly, given that one of its ﬁrst two elements is nonnil, the
normal vector k is related to ˜k as
˜k =
k

k2x + k2y
(12.54)
Obtained via such a normalization from the most signiﬁcant eigenvector of the struc-
ture tensor, the ﬁrst two elements of ˜k will then equal to a :
a = (
kx

k2x + k2y
,
ky

k2x + k2y
)T
(12.55)
and the third element will be equal to the speed : v
v = −
kt

k2x + k2y
(12.56)
to the effect that the velocity or the normal optical ﬂow will be given by va
v = −va = −
kt
k2x + k2y
(kx, ky)T
(12.57)
12.6 Translating Points
It is possible to estimate an unambigious optical ﬂow for certain 2D image patterns
in motion. We discuss the necessary and sufﬁcient conditions further below, whereas
for now we will be served well enough if we assume the existence of isocurves in
the 3D spatio–temporal image that is parallel lines (not planes!). This should enable
us to track where a point moves to with higher precision than the translating lines.
Such images are obtained, for example, when we have images like a sand pattern that
translates. Because of lemma 12.4, we know that the 3D Fourier transform of such
translating 2D patterns is concentrated to a plane. How can the optical ﬂow be ob-
tained from the common direction of the 3D lines, k, that the 2D pattern undergoing
a group translation generates, see Fig. 12.2, right? We discuss an answer below.
We assume that g(x, y) is a 2D pattern that contains trackable points at the time
instant t = 0. Translating a point (x, y)T in the image plane x, y with the velocity
v = (vx, vy)T to another point (x′, y′)T is achieved by
x′ = x+ tvx
y′ = y+ tvy
t′ =
t
(12.58)
which is a linear CT, r →r′. Accordingly, a 2D image g(x, y) is translated by
substituting the above CT in the gray image,

260
12 Direction in ND, Motion as Direction
g(x, y) = g(x′ −t′vx, y′ −t′vy) = f(x′, y′, t′)
(12.59)
This yields a spatio–temporal image function f deﬁned on E3 that contains a volume
of image frames originating from the continuously varying time, t′. Accordingly, the
CT is written in matrix form as:
r′ = Ar,
with
r = (x, y, t),
A =
⎛
⎝
1 0 vx
0 1 vy
0 0 1
⎞
⎠·
(12.60)
Using s′ = (x′, y′)T , and s = (x, y)T for convenience, the inverse of this CT is
easily found by observing that s′ = s + tv, Eq. (12.58),
s = s′−tv = s′−t′v =
x = x′ −t′vx
y = y′ −t′vy
⇒
A−1 =
⎛
⎝
1 0 −vx
0 1 −vy
0 0
1
⎞
⎠(12.61)
Now, the FT of f(x′, y′, t′) is
F(kx, ky, kt) =

f(r′) exp(−ikT r′)dr′
=

g(x′ −t′vx, y′ −t′vy) exp(−ikT r′)dr′
=

g(x, y) exp(−ikT Ar)|J(r′, r)|dr
=

g(x, y) exp(−ik′T r)|J(r′, r)|dr
=

g(x, y) exp(−i(k′
xx + k′
yy)) exp(−ik′
tt)dsdt
= G(k′
x, k′
y)δ(k′
t) = G(kx, ky)δ(kxvx + kyvy + kt) (12.62)
where k′ = (k′
x, k′
y, k′
t)T and |J(r′, r)|, the determinant of the Jacobian, are
k′T = kT A,
and
|J(r, r′)| = det
⎡
⎢⎣
⎛
⎜
⎝
∂x′
∂x
∂x′
∂y
∂x′
∂t
∂y′
∂x
∂y′
∂y
∂y′
∂t
∂t′
∂x
∂t′
∂y
∂t′
∂t
⎞
⎟
⎠
⎤
⎥⎦= det(A) = 1
(12.63)
We can see from Eq. (12.62) that the 3D FT of a 2D pattern in translation is an
intersection of an oblique plane having the normal w = (vT , 1)T ,
kxvx + kyvy + kt = wT k = 0
(12.64)
and a cylinder,6
6 Note that the cylinder is given by the 2D FT of the pattern stacked in the depth, i.e., ˜G = G
for all values of kt.

12.6 Translating Points
261
˜G(kx, ky, kt) = G(kx, ky)
(12.65)
The inclination of the plane, Eq. (12.64), is steered by the velocity, v, in that it
controls the normal of the plane, w
w =
v
1

(12.66)
This normal vector, and thereby the velocity, can be estimated from the image data
by the least signiﬁcant eigenvector of the structure tensor, k3. By construction, the
third element of w in Eq. (12.66) must be 1 for its ﬁrst two elements to equal to v.
Accordingly, we obtain v from k3 as follows.
k3 = (kx, ky, kt)
⇒
v = (kx
kt
, ky
kt
)T
(12.67)
Some studies refer to Eq. (12.62) as “a tilting of G”. The product Gδ is thus a cut
of the cylinder ˜G consisting of the 2D FT of the still pattern by a thin oblique plane,
δ, representing the motion plane, Eq. (12.64). Evidently, such a cut is not a true tilting
of G(k) in the FT domain because the coordinates of G(k) would have undergone
an orthogonal CT, which Eq. (12.62) does not depict. Instead, the transformation is
an inverse projection, i.e., the FT of the still image is a projection of the motion
plane to the kx, ky plane. Even if the still image is a band-limited function, without
a limitation on the speed, a translation can tilt the motion plane to reach arbitrary
high values for kt. Accordingly, a translation is a band-enlarging operation for band-
limited functions.
The velocity in Eq. (12.67) requires that kt ̸= 0. When kt →0 the velocity risks
increasing beyond every bound, causing numerical instability in computations. The
question for what patterns kt becomes small, and thereby the unambiguous motion
estimation becomes unstable, is discussed in the next paragraph.
The Velocity of Two Nonparallel Lines and Sensitivity Analysis
We assume now that we have an image,
g(x0, y0)
(12.68)
deﬁned on E2, but that this image contains two sets of isocurves with distinct direc-
tions,7 e.g., two sets of nonparallel lines in the image g(x0, y0). Accordingly, the 2D
vectors a1, a2 represent the respective normal vectors of the assumed line sets,
a1 = (cos(θ1), sin(θ1))T ,
a2 = (cos(θ2), sin(θ2))T ,
where
θ1 ̸= θ2.
(12.69)
7 Such an image is not linearly symmetric in 2D since it contains more than one direction.
For this reason g is not a function originally deﬁned on 1D originally but instead on 2D.
Sums of two linearly symmetric functions with different directions are examples of such
images.

262
12 Direction in ND, Motion as Direction
When the line sets translate with a common velocity vector v so that a point at s0
moves to s:
v = (vx, vy)T
⇒
s = (x, y)T = s0 + tv = (x0 + vxt, y0 + vyt)T
(12.70)
We obtain by this CT an image f that is continuous in x, y, t
f(x, y, t) = g(x −vxt, y −vyt)
(12.71)
in analogy with the discussion in the previous section. Because (s −s0) = tv,
is what we are interested in, the orthogonal projections of the (common) velocity
v on the normal directions of the lines are the individual speeds that would have
been perceived by the observer, if there was only one direction in the image. In that
case, the components of the velocity in the directions of a1, and a2 and the lines
themselves would deﬁne two sets of oblique planes containing the lines. The normal
vectors of these planes generated by the motion of each line are given by
˜k1 = (cos(θ1), sin(θ1), −aT
1 v)T
and
˜k2 = (cos(θ2), sin(θ2), −aT
2 v)T
according to the discussion preceeding Eq. (12.53). The cross-product of these 3D
normals will then be orthogonal to both, yielding the vector pointing in the direction
of the intersection (lines) of the nonparallel plane sets.
˜k3 = ˜k1 × ˜k2 =
⎛
⎝
cos(θ1)
sin(θ1)
−vx cos(θ1) −vy sin(θ1)
⎞
⎠×
⎛
⎝
cos(θ2)
sin(θ2)
−vx cos(θ2) −vy sin(θ2)
⎞
⎠
=
⎛
⎝
sin(θ1)(−vx cos(θ2) −vy sin(θ2)) −sin(θ2)(−vx cos(θ1) −vy sin(θ1))
−cos(θ1)(−vxcos(θ2) −vysin(θ2)) + cos(θ2)(−vxcos(θ1) −vysin(θ1))
cos(θ1) sin(θ2) −sin(θ1) cos(θ2)
⎞
⎠
=
⎛
⎝
−vx cos(θ2) sin(θ1) + vx sin(θ2) cos(θ1)
vy sin(θ2) cos(θ1) −vy sin(θ1) cos(θ2)
sin(θ2 −θ1)
⎞
⎠
=
⎛
⎝
vx
vy
1
⎞
⎠sin(θ2 −θ1) =
v
1

sin(θ2 −θ1)
The vector ˜k3 can be estimated by the least signiﬁcant eigenvector of the structure
tensor of the function f(x, y, t), i.e., k3, up to a scale.
˜k3 = k3
⇔
v
1

sin(θ2 −θ1) =
⎛
⎝
kx
ky
kt
⎞
⎠
(12.72)
This reconﬁrms the velocity estimate computations suggested in the previous para-
graph, Eq. (12.67), as
v = (kx
kt
, ky
kt
)T
(12.73)

12.7 Discrete Structure Tensor by Tensor Sampling in ND
263
However, most important, Eq. (12.72) shows that the condition kt ̸= 0 will be best
fulﬁlled when the difference between the two directions is maximum, (θ2 −θ1) =
π/2. Ideally this happens when the 2D image has perfectly balanced directions, i.e.,
both eigenvalues of the 2D structure tensor of g are large and are equal to each other.
Ill-conditioned numerical computations can be avoided by assuring that: (i) there is
a nonzero motion, i.e., ∥∂f
∂t ∥> 0; (ii) the (2D spatial) gradient in the 2D image is
nonzero, i.e., ∥∇g∥> 0; (iii) the 2D image g lacks linear symmetry such that there
are at least two distinct directions in it, i.e., the most signiﬁcant eigenvalue of the
2D structure tensor of g has the multiplicity 2. We summarize these ﬁndings in the
following lemma.
Lemma 12.6 (Spatial directions constraint). The lack of linear symmetry is a sufﬁ-
cient and necessary condition for an image g(x, y) to satisfy for a translation of it be
computable from the corresponding spatiotemporal image f(x, y, t). The minimum
number of directions that must be contained in g to allow computation of an unam-
biguous translational motion is 2, provided that they are sufﬁciently distinct.
♦
12.7 Discrete Structure Tensor by Tensor Sampling in ND
In discretizing the ND tensor, we will follow an analogous approach to that devel-
oped in Sect. 10.11; for this reeason we only state the results without derivations or
proofs. The computation and discretization in the r-domain is done by utilizing the
Parseval–Plancherell theorem and by assuming that the interpolation function and
the multiplicative window deﬁning a local image, when need be, are two Gaussians
with σp and σw, respectively.
The structure tensor for E3
The structure tensor for the 3D Euclidean space was deﬁned in the spectral domain
and for continuous images via Eq. (12.13). In numerous situations, these quantities
need to be estimated on a discrete (Cartesian) 3D grid in the spatial domain, (the
r-domain). Furthermore, this should, in many applications, be done quite often, e.g.,
for local images around every point.
Lemma 12.7. The structure tensor is estimated in the TLS error sense, by averaged
tensor (outer) products
S =
1
4π2

l
(∇fl)(∇fl)T ml.
(12.74)
where ∇fl is the gradient of f(r) at the point rl, which belongs to a regular discrete
grid, and ml is an averaging kernel that consists of a discrete Gaussian. The discrete
gradients are estimated by ﬁltering the original image fl with a kernel consisting of
a discrete gradient of a Gaussian.
♦

264
12 Direction in ND, Motion as Direction
Eq. (12.74) is equivalent to a discrete convolution by a Gaussian if S is com-
puted for every point in the original discrete image. Since the values of ml, the ﬁlter
coefﬁcients, decrease rapidly outside of a circle with radius equal to a few standard
deviations of the Gaussians used, we can truncate the inﬁnite ﬁlter when its coefﬁ-
cients are sufﬁciently small. The advantages of Gaussians in 3D Euclidean spaces
are even more imposing than those in 2D: their separability, compactness, and be-
ing fully isotropic. The conclusions, including the lemma, are evidently valid also in
ND.
The structure tensor for sampled motion images
Both translational and afﬁne model-based motion parameter estimations developed
in the previous sections originally formulated the problem as a direction estimation
problem in the continuous domain. The optical ﬂow vectors, formally in 2D, are
encoded in the the 3D structure tensor, which in turn can be estimated by by use of
lemma 12.7. However, some issues raised in the context of sampling band-enlarging
operators in Sect. 8.3, need to be considered.
First, the structure tensor of any dimension is a band-enlarging operator because
it requires multiplications of functions having the same bandwidth, due to the tensor
product ∇f∇T f. The tensor image ∇f∇T f is also a band-limited function, but the
bandwidth is now larger than that of the bandwidth of the constituent vector image
∇f. To make sure that ∇f∇T f is sampled without aliasing, the components Drif,
where ri = x, y, t, must be oversampled with a factor 2. Alternatively, the maximum
frequencies of F should stay within half the Nyquist cube.8 This is typically done by
incorporating a lowpass ﬁltering directly into the kernel coefﬁcients of the operators
Dri. Thus, in each direction the Gaussian interpolation functions used to reconstruct
f from its samples must be at least σp ≈1.3, as discussed in Sect. 9.3. Furthermore,
since the physical dimensions of spatial coordinates x, y are different than that of
the temporal coordinate t, the used σp are the same for the spatial coordinates but
different than that of the spatial coordinate, σp. Accordingly, the derivation operator
is the sampling of the derivated interpolation function:
Driμ(x, y, t) = exp

−x2 + y2
2σ2
ps

exp

−t2
2σ2
pt

,
with
ri = x, y, t.
Second, translating a 2D band-limited image, g(x, y), to produce a 3D spatio–
temporal image, f(x, y, t) is a band-enlarging operation. This is concluded from Eq.
(12.62), which states that the 3D FT, F(kx, ky, kt), is given by the cylinder G(kx, ky)
cut by the motion plane, i.e., the plane with the normal (v, 1)T . Accordingly, if
g(x, y) is a band-limited function, i.e., G(kx, ky) vanishes when ks = (kx, ky)T is
outside of a compact region,
8 Recall that the Nyquist cube is the cube with surfaces intersecting the frequency coordinate
axes at π and surface normals coinciding with the frequency axes. Half the Nyquist cube is
similar, but the surface intersections are at π
2 on the frequency axes.

12.7 Discrete Structure Tensor by Tensor Sampling in ND
265
∥ks∥> Ks
⇒
G(kx, ky) = 0
(12.75)
Because of this |kt| will have an upper bound for the meaningful spatial frequencies
∥ks∥≤Ks
⇒
|kt| ≤vKs = Kt
(12.76)
Then, if the translation speed v is limited, the spatio–temporal image is band-limited,
too. Furthermore, if the temporal axis is sampled with the sampling period
2π
2Kt or
tighter, the speeds of points moving with speeds not greater than v will be recover-
able, because the motion planes generated by such translations will have a smaller
inclination angle with the kx, ky plane. In consequence of this, it is sufﬁcient that
vmax, Kt, and Ks satisfy
vmax ≤Kt
Ks
(12.77)
where vmax is the largest speed of any point in f(x, y, t), to recover the spatio–
temporal image, and thereby represent the motion without distortion, via the samples
f(xj, yj, tj). We summarize these results in the following lemma:
Lemma 12.8. Let g(x, y) be a band-limited function with the maximum spatial fre-
quency Ks and f be a translated version of it, f(x, y, t) = g(x −vxt, y −vyt), with
the velocity v = (vx, vy)T . Then,
Ks|v| = Kt
(12.78)
where Kt is the maximal temporal frequency that can occur in F. If the structure
tensor is to be sampled, then Ks and Kt must satisfy
Ks ≤π
2
Kt ≤π
2
(12.79)
assuming normalized spatial and temporal sampling periods.
♦
If we assume that both Ks = Kt = π, which normalizes the sampling period to
unity to the effect that the sampling points are one unit apart in x, y, and t directions,
then the maximal speed of the translation is bounded by
vmax ≤Kt
Ks
= 1
(12.80)
To recover larger speeds from sampled images, either Kt must be increased (the tem-
poral axis is sampled more densely) or Ks should be decreased (x, y axes are low-
pass ﬁltered or the image is enlarged). When the structure tensor is used to recover
the velocity, one must make sure that both Kt and Ks are less than π/2, assuming
normalized distances between the image pixels and image frames.

−π
−π
kt
kx
−π
−π
kt
kx
π/2
π/2
−π
−π
kt
kx
Fig. 12.4. (Left) A pattern undergoing a translation of (top) 0, (middle), 0.67, and (bottom) 2
pixels/frame. (Right) The schematic FTs are also shown

12.8 Afﬁne Motion by the Structure Tensor in 7D
267
We illustrate the signal discretization effects of translational motion by Fig. 12.4,
where a 1D pattern, containing the full range of the frequencies, is translated with
zero, 0.67, and 2 pixels per frame. In the color images, the vertical axis is the time
axis. The image is actually a gray image, see Fig. 8.8, but to illustrate the translation
effectively, every gray tone has been replaced by a unique color. The corresponding
schematic FTs of the signals are shown besides, with the red lines representing the
nonzero power. For small motions there is no aliasing wrap-around, because the tem-
poral sampling frequency is sufﬁcient to discretize the motion faithfully. However,
when the motion magnitude is increased above 1 pixel/frame, the sampling rate be-
comes too low for a satisfactory representation. The result is a distortion (aliasing),
or frequency wrap-around caused by the repetition of the Nyquist square in the kt
direction. Had the spatio–temporal signal been oversampled with a factor of 2 in the
x, as well as in the t directions, the extra red lines, caused by the repetition in the
kt direction, would not appear. This is because the signal would then be conﬁned
to the cyan-colored zone shown in the ﬁgure. In conclusion, the motion sequences
in the spatial as well as temporal domains must be oversampled with a factor 2, to
avoid sampling aliasing caused by high spatial frequency contents (quick variations)
moving fast. Independently, this is also what is required if the structure tensor is to
be used to compute the velocity.
12.8 Afﬁne Motion by the Structure Tensor in 7D
Whereas a translational model is often adequate to describe the motion in a local
image, it becomes sometimes insufﬁcient to describe complex motion when the im-
age patch to be analyzed is enlarged and thereby generally the complexity of the
motion is increased. A more elaborate model such as an afﬁne motion model will
generally be better placed to describe the motion of a rigid object, particularly if the
ﬁeld of view is small enough [132,141]. Here we only treat the case when all image
points move according to the same afﬁne motion model, parameters whose need to
be identiﬁed.
As before, we assume that BCC is valid, meaning that the spatio–temporal image
is generated from one frame of the image, i.e., the brightness distribution originating
from a certain instant. The generation of the rest of the spatio–temporal image will
be done by applying the afﬁne coordinate transformation to the spatial coordinates,
s = (x, y)T :
s∗= s + δt[A0s + v0]
⇒
f(x, y, t) = g(s + δt[A0s + v0])(12.81)
Consequently, the afﬁne model is characterized by a velocity ﬁeld v expressed in the
parametric form:
v(s) = A0s + v0
(12.82)
with v0, A0 being a 2D vector and 2×2 invertible matrix, respectively. There are two
real parameters in the constant translation v0, and four in the matrix A0, represent-
ing rotation, scaling, and the (two) shearing deformations, totaling to six degrees of

268
12 Direction in ND, Motion as Direction
Fig. 12.5. Afﬁne motion is estimated from 5 images (right) [61], using the eigenvectors of
(12.92). One of the original frames is shown on the left. On the right, the red car region, and
the cyan-labeled background region have different afﬁne motion parameters which were found
automatically
freedom in the afﬁne model. Naturally, more complex ﬂow ﬁelds can be modeled by
the afﬁne model than the translation model, which has only two degrees of freedom.
The afﬁne matrix A0 can be decomposed into [58,64,132],
A0 = As
0 + Ar
0 + Ah1
0 + Ah2
0
(12.83)
where the matrices on the right side control the amounts of rotation, scaling, shearing
of the ﬁrst type, and shearing of the second type, respectively.
As
0 =

s 0
0 s

,
Ar
0 =

0 −r
r 0

,
Ah1
0 =

h1
0
0 −h1

,
Ah2
0 =

0 h2
h2 0

The CT in Eq. (12.81) is a differential equation modeling the speed or the optical
ﬂow ﬁeld, because
s∗−s
δt
→ds
dt = v(x, y, t)
(12.84)
when δt approaches to zero. Accordingly, the trajectory of a point in the image plane,
s(t) = (x(t), y(t))T , can be solved analytically. The six parameters represented by
v0 and A0 steer the tangent curves of s. It can be shown that a small motion along
these curves can be achieved by an afﬁne inﬁnitesimal operator9 [58],
Dζ = k1Dξ1 + k2Dξ2 + · · · k7Dξ7 =
7

j=1
kjDξj
(12.85)
9 The CT generated by the afﬁne model is a Lie group of transformations of one parameter,
time.

12.8 Afﬁne Motion by the Structure Tensor in 7D
269
with
Dξ1 = Dx
Dξ2 = Dy
Dξ3 = xDx + yDy
Dξ4 = −yDx + xDy
Dξ5 = xDx −yDy
Dξ6 = yDx + xDy
Dξ7 = Dt
(12.86)
under which a spatio–temporal image generated by the afﬁne coordinate transforma-
tion is invariant:
Dζf(x, y, t) =

j
kjDξjf(x, y, t) = 0
(12.87)
From this equation we will attempt to solve the unknown parameter vector:
k = (k1, · · · k7)T ,
with
∥k∥= 1.
(12.88)
The solution is evidently not possible if we only know the 7D measurement vector
Dξf deﬁned as
Dξf = (Dξ1f, Dξ2f, · · · Dξ7f)T
(12.89)
at a single point r = (x, y, t)t. Equation (12.87) is effectively an equation of a hy-
perplane in E7.
kT Dξf(r) = 0
(12.90)
The equation is satisﬁed in many points ideally because f is in that case a spatio–
temporal image that is truly generated from a static image by use of an afﬁne coordi-
nate transformation. In that case F will be concentrated to a 6D plane, see theorem
12.1 and lemma 12.7. Accordingly, estimating the afﬁne motion parameters is a lin-
ear symmetry problem in E7. It can be solved in the TLS error sense by minimizing
the error
e(k) =

∥kT Dξf∥2dxdydt = kT [

(Dξf)(DT
ξ f)dxdydt]k = kT Sk (12.91)
under the constraint ∥k∥= 1. Here, the matrix S is the structure tensor deﬁned as
S =

(Dξf)(DT
ξ f)dxdydt
(12.92)
The TLS estimate of the parameter vector k is given by the least signiﬁcant eigen-
vector of S. The necessary differential operators and the integrals are possible to
implement by use of Eqs. (12.86), and (12.89) via separable convolutions with ker-
nels derived from Gaussians [59]. Estimating S by sampling the structure tensor in
ND is discussed in Sect. 12.7. Figure 12.5 illustrates a motion image sequence. The
car is moving to the left whereas a video camera tracks the car and keeps it approx-
imately at the center of the camera view. As a result there are two motion regions,
the car’s and the background’s. The eigenvectors of Eq. (12.92) are computed twice,

270
12 Direction in ND, Motion as Direction
to estimate both the background and the car motion using the afﬁne model which in
turn has helped to ﬁnd the motion boundaries [61]. The boundary estimation aided
by motion parameters is far more accurate than only using static image frames (e.g.,
see the antenna of the car which has also a cable, loosely attached to it), although not
perfect (e.g., the dark region behind the car is not well segmented).
12.9 Motion Estimation by Differentials in Two Frames
We begin with writing down the velocity of a particle, v = (vx, vy)T , moving in the
(x, y)-plane, as deﬁned in mechanics:
v = ds(t)
dt
= (dx(t)
dt , dy(t)
dt )T = (vx, vy)T
(12.93)
where s = (x, y)T is the coordinate of the particle and t is the time coordinate. Let
the function f(x, y, t) represent the spatio–temporal image of such moving parti-
cles,x where f is the gray intensity. A change of the gray intensity can be expected
when changing the coordinates s = (x, y)T and/or t. From calculus we can conclude
that a small intensity change in f can be achieved by small changes of all three vari-
ables of f. To be exact, the change df is controllable by the independent changes
dx, dy, dt via:
df = dt∂f
∂t + dx∂f
∂x + dy ∂f
∂y
(12.94)
or equivalently,
df
dt = ∂f
∂t + dx
dt
∂f
∂x + dy
dt
∂f
∂y
(12.95)
Here we assumed that f is differentiable, i.e., all of its partial derivatives exist con-
tinuously. It is of great interest to ﬁnd a path s(t) such that f does not change at all,
i.e.,
df
dt = 0
(12.96)
This path will yield the isocurves of f(x, y, t), which in turn offers an opportunity to
track points. Under the condition that the path is unique, this is the same as having
tracked a point if the BCC holds [110, 122], see Deﬁnition (12.2). This conclusion
is reasonable because the image of a moving 3D point is a moving 2D point, which
normally changes its gray value only insigniﬁcantly, at least during a sufﬁciently
short observation time:
BCC:
∂f
∂t + vx
∂f
∂x + vy
∂f
∂y = 0
(12.97)
Conversely, if an image f(x, y, t) satisﬁes this equation we can conclude that the
motion satisﬁes the BCC [110,157]. This is why the equation is frequently quoted as
the BCC equation in image analysis studies.

12.9 Motion Estimation by Differentials in Two Frames
271
If one attempts to solve v = (vx, vy)T from the BCC when the partial derivatives
of f are known at a given space–time x, y, t, one fails because v contains two real
variables, whereas we have only one equation. However, if the equation holds at
several image points s = (xk, yk)T at a given time t0, then we can write several
such equations. This happens typically for a local image pattern g(x, y) wherein all
points translate with the same velocity v. This situation is similar to the example in
Fig. 12.2 (right), where the coherent translation of points is depicted. Accordingly,
the BCC for a discrete 2D neighborhood f(xk, yk, t0) yields10
⎛
⎜
⎜
⎜
⎜
⎝
∂f(x1,y1,t0)
∂t
∂f(x2,y2,t0)
∂t...
∂f(xN,yN,t0)
∂t
⎞
⎟
⎟
⎟
⎟
⎠
= −
⎛
⎜
⎜
⎜
⎜
⎝
∂f(x1,y1,t0)
∂x
∂f(x1,y1,t0)
∂y
∂f(x2,y2,t0)
∂x
∂f(x2,y2,t0)
∂y
...
...
∂f(xN,yN,t0)
∂x
∂f(xN,yN,t0)
∂y
⎞
⎟
⎟
⎟
⎟
⎠

vx
vy

(12.98)
Here, f(xk, yk, t0) with k ∈{1 · · · N} represents f(x, y, t) evaluated at the kth point
of an image neighborhood. This is an overdetermined system of linear equations of
the form
d = −Dv
(12.99)
with v being unknown and
d =
⎛
⎜
⎜
⎜
⎜
⎝
∂f(x1,y1,t0)
∂t
∂f(x2,y2,t0)
∂t...
∂f(xN,yN,t0)
∂t
⎞
⎟
⎟
⎟
⎟
⎠
,
D =
⎛
⎜
⎜
⎜
⎜
⎝
∂f(x1,y1,t0)
∂x
∂f(x1,y1,t0)
∂y
∂f(x2,y2,t0)
∂x
∂f(x2,y2,t0)
∂y
...
...
∂f(xN,yN,t0)
∂x
∂f(xN,yN,t0)
∂y
⎞
⎟
⎟
⎟
⎟
⎠
(12.100)
are to be assumed known. Suggested by Lucas and Kanade [157], this is a linear re-
gression problem for optical ﬂow estimation. The standard solution of such a system
of equations is given by the MS estimate, obtained by multiplying the equation with
DT and solving for the 2 × 2 system of equations for the unknown v:
DT d = −DT Dv
(12.101)
The solution exists if the matrix
S = DT D =

k
(∇skf) · (∇T
skf)
(12.102)
where
∇skf =
 ∂f(xk,yk,t0)
∂x
∂f(xk,yk,t0)
∂y

(12.103)
However, S is the structure tensor for the 2D discrete image f(xk, yk, t0). A unique
solution exists if S is nonsingular, a situation that occurs if the image lacks linear
10 For a ﬁxed t0, f(x, k, yk, t0) is a 2D function.

272
12 Direction in ND, Motion as Direction
symmetry. Conversely, if S has an eigenvalue that vanishes, there is no unique veloc-
ity that can be estimated from the image measurements. Having an overdetermined
system of equations and yet not being able to solve for the velocity may appear coun-
terintuitive, but it is explained by the fact that the 2D pattern f(xk, yl, t0) is linearly
symmetric when an eigenvalue of S vanishes. Such images have parallel lines as
isocurves, that is, if translated along these lines no difference in gray values will be
noticed. Accordingly, this situation is the same as the one in the example of Fig. 12.2
(left), where a translating line has been depicted. Because lines and edges are com-
mon in real images, singular structure tensors are also common in image sequences.
In consequence, an appropriate nonsingularity test before solving Eq. (12.101) must
be applied.
The solution of Eq. (12.101) necesitates the estimation of the structure tensor for
a 2D image neighborhood, which we know how to do from Sect. 10.11. However,
the vector d is also needed. It is customary to estimate it via a temporal difference
∂f(xk, yk, t0)
∂t
= f(xk, yk, t1) −f(xk, yk, t0)
(12.104)
between two successive image frames [157]. Accordingly, the optical ﬂow technique
discussed above is possible to compute from just two frames.
The technique described in this section and the tensor approach discussed in
Sects. 12.5 and 12.6 are related since they are both gradient-based. The main dif-
ference is that the tensor approach solves the regression problem in the TLS sense,
whereas here it is solved in the MS sense, meaning that the regression error in the
time direction is assumed to be noise-free, see Sect. 10.10. Because of this, the es-
timation is not independent of the coordinate system, in contrast to the tensor aver-
aging approach. A second difference lies in that the tensor approach uses 3D gradi-
ents in which both types of translations are jointly represented, whereas Lucas and
Kanade’s approach uses 2D gradients and frame differences to estimate the transla-
tion of points, which decreases its noise tolerance. However, it should be pointed out
that the 2D approach is computationally less demanding and therefore faster.
12.10 Motion Estimation by Spatial Correlation
We assume that the discrete image frame f(xk, yk, t0), which is typically a local
image patch, satisﬁes the BCC constraint while it undergoes a translation with the
displacement vector v = (vx, vy)T . In other words, the 2D pattern f(xk, yk, t0) and
f(xk, yk, t1) are the same or change insigniﬁcantly except for a translational CT in
the spatial coordinates, sk = (xk, yk)T :
s(t1) = s(t0) + v(t1 −t0) = s(t0) −v
(12.105)
where we wrote the position of a point in a 2D frame as a function of the time, s(t),
and assumed that the temporal sampling period is normalized, (t1−t0) = 1. Because
the two image frames satisfy the BCC, one can write

12.10 Motion Estimation by Spatial Correlation
273
Δf = f(xk(t1), yk(t1), t1) −f(xk(t0), yk(t0), t0)) = 0
(12.106)
so that the substitution of Eq. (12.105) yields
DFD:
Δf = f(xk, yk, t1) −f(xk −vx, yk −vy, t0) = 0
(12.107)
The spatial coordinates xk, yk refer to those at time t1, i.e., xk(t1), yk(t1). The differ-
ence function Δf is also called the displaced frame difference DFD, in the literature
because one moves a frame towards another using a displacement. In practice, how-
ever, the DFD is not exactly zero because the BCC is satisﬁed only approximately.
The problem is instead reformulated so that the L2 norm of the DFD is minimized
over v:
∥Δf∥2 =

k
(f(xk, yk, t1)−f(xk −vx, yk −vy, t0))2 =

k
(fk −˜fk)2 (12.108)
where fk and ˜fk are the sampled image frame at time t1 and the sampled translated
image frame11 at time t0. The minimization is achieved by assuming that f is a local
patch, typically a 7 × 7 square, and the displacement is limited, |v| < C, typically
C = 7. Thus, a typical implementation would search for a point yielding the least
∥Δf∥2 by varying v for a local f with the size of 7 × 7 in a search window of a
15 × 15. Equation (12.108) measures the distance between two images after one is
translated towards the other. Altenatively, one can compare the frame to its translated
previous frame by using a similarity measure. Deﬁning the discrete gray values in the
2D image patches as vectors i.e.,
f = (· · · fk−1, fk, fk+1 · · · )T ,
and
˜f = (· · · ˜fk−1, ˜fk, ˜fk+1 · · · )T ,
(12.109)
the Schwartz inequality,
cos(θ) = |⟨f,˜f⟩|
∥f∥∥˜f∥
=
|f T˜f|
∥f∥∥˜f∥
≤1
(12.110)
can be used used as a similarity measure. One can search for the pattern ˜f that is most
parallel to f among patterns in its vicinity by maximizing cos(θ), which is insensitive
to multiplicative changes of ˜f. Seeking the optimal ˜f by maximizing cos(θ) requires
repetitive scalar products, which is also a correlation. When ∥f∥= ∥˜f∥holds, opti-
mizing the objective functions in Eq. (12.108) and Eq. (12.110) yield identical v as
is seen by the expansion
∥f −˜f∥2 = ⟨f −˜f, f −˜f⟩= ∥f∥2 + ∥˜f∥2 −2f T˜f
(12.111)
which is minimized at the same time as f T˜f is maximized. This is why using both
variants of the objective functions is known as correlation-based optical ﬂow. A ma-
jor difference is, however, that the ratio cos(θ) is less affected by brightness changes
because these are approximated well by multiplicative ampliﬁcations of images.
11 This is also referred to as the warped image.

274
12 Direction in ND, Motion as Direction
12.11 Further Reading
Roughly, the types of motion estimation can be categorized by whether they use
sparse local image data, called feature points, or all local image data. Those using
sparse data rely heavily upon existence of discriminatory local images that can be
identiﬁed from local information and their relative positions to other feature points.
Typically, in computer vision and cartography, various correlates of lack of linear
symmetry are utilized to identify these points [28, 74, 97]. An alternative approach
is to try to identify the presence of a speciﬁc symmetry of the local image, e.g., see
Sect. 11.11, which, in addition to detection, delivers the geometric orientation of the
pattern the target of tracking. By its nature, this approach is two-dimensional [198].
Combined with snakes and energy minimization techniques, it can cope with tracking
deformable objects as well as deformable boundaries, e.g. [148]. The problem of
correspondence, i.e., which sparse point corresponds to which between two frames,
if at all present in both, is, however, still a nontrivial issue along with the occlusion
of points by moving objects.
In this presentation, we have mainly dwelled on dense motion estimation tech-
niques, known as optical ﬂow ﬁelds. Such motion estimations are used in numerous
applications. Probably one of the most widely used application is in image com-
pression, where the displacements ﬁelds serve to reduce the redundancy by qualiﬁed
guesses as to where an image patch will likely go to in the next frame. This is called
prediction which of course is never perfect but it is sufﬁcient if the guess is roughly
correct, because then the error will have less variations than the original sequence
due to almost correct guesseses. Coding a function that has lower variations results
in compression of the data necessary to represent the image sequence [120].
A basic assumption in the dense translational motion ﬁeld estimation is that a 2D
image undergoes motion with its pattern basically intact, i.e., the gray levels and their
geometic distributions are conserved, leading to the BCC equation. This is, however,
not enough to solve for the velocity and must be regularized [15]. We have presented
two effective regularizations, tensor averaging in 3D and differential averaging in 2D.
However, other approaches to regularization exist, e.g., those, using the membrane
differential equation, [110], adding gradient propagation, [167], combining BCC and
local feature invariance, [200]. An alternative approach to the partial derivative-based
techniques is to estimate the translational motion in the local spectral domain. As has
been discussed, the motion of points and lines are concentrated to a plane and a line
in the spectrum. The 3D local spectrum can, however, be sampled by means of a
Gabor decomposition, which reveals whether or not there is a certain concentration
of the spectral energy, and hence the motion information. The study in [101] has
proposed an algorithm for detecting optical ﬂow using the magnitude of 3D Gabor
ﬁlters on the basis of the extracted spectral energy via a set of Gabor ﬁlters. A plane
is ﬁt to the power spectrum by a full search of the tilt parameters. Using an analogue
of the tensor approach discussed in Sect. 10.13, but in 3D, [138] does a similar ﬁt
to the power spectrum. The advantage is a full search is avoided. The report of [73]
solves the same tilt estimation problem by using the Gabor ﬁlter phase, where phase-

12.12 Appendix
275
Fig. 12.6. Does the apparent motion of the specular light represent the motion of the sphere?
unwrapping discontinuities are avoided by use of the derivatives of the phase that is
continuous in contrast to the phase itself.
The motion estimation using afﬁne model (six parameters) is also dense, although
its support region is generally much larger than the translational model (two param-
eters) to yield a reliable estimate. By contrast, it can describe more complex motion
ﬁelds than the translational model. However, it is also true that when the image sup-
port is large, even the afﬁne model will be quickly unsufﬁcient, typically because the
image will cover two or more regions moving independently according to different
motion models. The situation with several motions require an estimation of the mov-
ing regions, called layers, and the motion parameters, [3,9,30,60] simultaneously or
in alternation, where one is improved while the other is kept unchanged. In a layer
there is a single motion following a motion model with ﬁxed but unknown parame-
ters. This happens in TV like image sequences where the images will usually contain
several independently moving objects and/or regions, e.g., see Fig. 12.5.
12.12 Appendix
The Rotating Sphere Experiment
That optical ﬂow ﬁelds may not necessarily identify motion ﬁelds faithfully has been
known, as is often illustrated by the rotating sphere phenomenon, Fig. 12.6. In this
experiment, a hanging sphere with uniform color, illuminated by a light source, is
brought to rotation about the vertical axis. An image sequence generated by an ob-
serving camera that is ﬁxed in the room will identify this motion as a zero motion

276
12 Direction in ND, Motion as Direction
because there will not be a statistical difference between the observations of the
sphere at different time instances. In other words, nonzero motion ﬁeld can generate
a zero optical ﬂow ﬁeld as suggested by the experiment. By contrast, if the sphere
and the observer are immobile but the light source is brought into motion, the sphere
will have different light distributions falling on its surface, which will be observed
as a nonzero optical ﬂow ﬁeld by the observer. Accordingly, a zero motion ﬁeld can
generate a nonzero optical ﬂow ﬁeld.

13
World Geometry by Direction in N Dimensions
The origins of the concept ”perspective” as it came to be used in the ﬁeld of computer
vision can be traced to two key ﬁelds, photogrammetry and geometry. Although the
problems and the concepts studied in the latter since the early 1800s accounts for
a large portion of contemporary computer vision, these ﬁelds have a long history
that covers several schools of thought and art. Many of these concepts date back at
least to the Renaissaince or even earlier. In 1480, Leonardo da Vinci formulated a
deﬁnition of the perspective images [56] that was very close to our contemporary
understanding of it. Scientists continued the work of da Vinci on projections and
geometry. Albrecht Duerer created an instrument that could be used to create a true
perspective drawing in 1525 [93]. Girard Desargues contributed to the foundation of
projective geometry, Trait´e de la section perspective (1636). Other signiﬁcant contri-
butions were by Johan Heinrich Lambert, with the treatise “Perspectiva Liber” (The
Free Perspective, 1759), and the establishment of the relationship between projec-
tive geometry and photogrammetry (1883), by R. Sturms and G. Haick [56]. In the
subsequent sections, we will outline the basic principles of measurements of world
geometry from photographs, to help us understand the structure of a world scene.
13.1 Camera Coordinates and Intrinsic Parameters
We assume that we have a perspective camera. This can be imagined as a box with
a very small hole through which the light rays hit the image plane behind. For this
reason the perspective camera is also called the pinhole camera. It is an ideal camera
obscura of the kind that was carried on horses or on the back of the artist in the
medieval age. The pinhole camera is illustrated in Fig. 13.1 with the image plane
shown in green.
First, we make some notational precisons. Points are represented by capital let-
ters such as P, O, Q. Since the geometry of points and vectors between them in the
ordinary Euclidean space E3 is discussed in this chapter, an alternative notation will
be used for vector representation. Consequently, to represent a vector between the

278
13 World Geometry by Direction in N Dimensions
O′
P′
O
O′′
P
Fig. 13.1. An illustration of the pinhole camera, also known as the perspective camera
world points P and Q the notation −−→
PQ will be used. The primed points represent
mapped versions of the corresponding points to a plane.
The axis −−→
OO′ is known as the optical axis of the camera. The image is formed
in the plane orthogonal to the optical axis, shown in green in Fig. 13.1. Exploiting
the congruency between the triangles1 of the pinhole camera, an equivalent geometry
for points mapped through the camera can be obtained (Fig. 13.2). The points that
have the same labels correspond logically to each other when compared to those
in Fig. 13.1. The image plane is again shown in green. Note that the focal plane
or the image plane is now placed between “the pinhole” and the object, whereas
in its physical realization it is the pinhole that is between the object and the image
plane. Use of this geometry as a reference model for the projective camera is mostly
adopted in computer vision studies, and it is equivalent to the physical model shown
in Fig. 13.1. One advantage is that the image of an object is not turned upside down.
The parameters that depend on the hardware and the design of the camera are
called intrinsic parameters. Referring to Fig. 13.2, the distance
∥
−−→
OO′∥= f
(13.1)
called the focal length,2 is the foremost one. Because digital images are used in
the practice of computer vision, there are more parameters that are intrinsic. The
parameters sx, sy, representing the size of 1 pixel in the length unit, e.g., meter, are
the most obvious. In a CCD sensor, for example, these are determined by how many
pixels there are in horizontal and vertical directions and how large the total width and
1 If two triangles have the same angles, then they are said to be congruent.
2 The tiny hole in the pinhole camera is effectively replaced by a lens in practice. This has
the advantage that the camera can receive more light since the lens is larger than the tiny
hole, allowing imaging with less light. Also, a lens effectively captures one plane of the
world and focuses it on the image plane, allowing us to map objects that are at a certain
distance, only.

13.1 Camera Coordinates and Intrinsic Parameters
279
Fig. 13.2. The perspective camera model is illustrated by projection of a point P to its image
point P ′
the total height of the sensor is in the length unit. Next, is the less obvious parameter
set
−−→
O′C = (c0, r0)T
(13.2)
which is the translation vector between the digital image center and the analog im-
age center, the image of the optical axis. For technical reasons, such as limitations
and constraints of mechanical construction and lens imperfections, the digital image
rectangle, shown in magenta in Fig. 13.2 is not centered around the optical axis but
around a point that is marked as C in the ﬁgure.
We can represent the world point P by the vector
−−→
OP = (X, Y, Z)T
(13.3)
where the numerical values of the vector components are represented relative to the
camera basis, eX, eY , and eZ. Likewise we can represent an image point P ′ by the
vector
−−−→
O′P ′ = (x, y)T
(13.4)
where the vector components are given relative to the (analog) image coordinates,
ex, ey with the frame origin placed at the image of the optical axis, the point O′.
Using the congruency between the triangle OO′P ′ and the triangle OO′′P, we can
write

280
13 World Geometry by Direction in N Dimensions
f
Z = x
X
f
Z = y
Y
(13.5)
which yields the CT between the analog image frame and the camera frame
x = f
Z X,
y = f
Z Y
(13.6)
Before further discussion, it is necessary to present the homogeneous coordi-
nates. Any n dimensional vector represented in a certain frame can be written as an
n + 1-dimensional vector by adding a redundancy in terms of an extra dimension.
The coordinates in the augmented version are called the homogeneous coordinates,
including a possible common scale factor λ. These are obtained as follows for the
dimension n = 2. For dimensions higher than 2 the procedure is analogous.
If
−−−→
O′P ′ = (x, y)T
⇒
(
−−−→
O′P ′)H = λ (x, y, 1)T
(13.7)
The real scalar λ is arbitrary as long as it is not 0. We will mark homogenized vec-
tors here with (·)H to avoid confusion, although in matrix algebra such vectors are
treated in the same way as other vectors of the same dimension. An equality sign “=”
in expressions containing homogenized vectors should be interpreted in the sense of
equivalent classes, i.e., a homogenized vector that is scaled remains the same ho-
mogenized vector. If the homogeneous coordinates of a point in the image plane are
known, then the image coordinates are recoverable as
(
−−−→
O′P ′)H = (X, Y, Z)T ⇒
−−−→
O′P ′ = 1
Z (X, Y )T
(13.8)
We note that −−→
OP is not uniquely determined by its image, i.e., −−→
OP ′, or −−−→
O′P ′. In fact,
not only P, but any point P ′′ (not shown in Fig. 13.2) on the inﬁnite line represented
by the vector −−→
OP will have the image P ′. Accordingly, the coordinates of a point P
in the 3D world are homogeneous coordinates for the point P ′ if the focal length is
assumed to have the unit length. This assumption is not a loss of generality because
when length measurements are done in the focal length, or equivalently when an
appropriate image scaling is performed, then f = 1. However, to do this requires
an estimation of f, for reasons as follows. In this sense, a 3D homogeneous vector
derived from a point in the 2D image plane is thus the inﬁnite line represented by the
vector that joins the projection center, O, to the image point. The line, among others,
passes through all 3D world points whose image is given by the intersection between
the line and the image plane. These results are summarized by the following lemma:
Lemma 13.1. Let the analog picture in a projective camera be described by the basis
vectors ex, ey, placed at O′, the image of the projection center. Then a point P is
imaged at point P ′, with the coordinates
(
−−−→
O′P ′)AH = MA · (−−→
OP)C,
with
MA =
⎛
⎝
f 0 0
0 f 0
0 0 1
⎞
⎠,
(13.9)

13.1 Camera Coordinates and Intrinsic Parameters
281
where (−−−→
xOP)C = (X, Y, Z)T and (−−−→
O′P ′)AH = Z · (x, y, 1)T are the coordinates
of the point P in the camera frame, and the homogeneous coordinates of the point
P ′ in the analog image frame, respectively.
♦
Not surprisingly, the homogeneous coordinate concept is the result of geometry
studies in mathematics, to which A.F. M¨obius (1790–1868) contributed greatly. It
has turned out to be an important tool of computer vision as well as computer graph-
ics in modern times. Among others, it is used to make afﬁne transformations linear,
e.g., a 3D rotation and translation can be implemented as a single 4 × 4 matrix mul-
tiplication, thanks to the relationship between the projective spaces and Euclidean
spaces. Besides that, the representation allows a structured treatment of geometric
concepts such as points, inﬁnite lines, planes, parallelism, and bundles [70,91].
Above, the quantities x, and y were in length units, e.g., meters or focal length,
whereas one refers to a point in a digital image by its column count in the left–right
direction, c, and by its row count in the top–down direction, r. Having this, and the
geometric relationship
−−−→
O′P ′ =
−−→
O′C +
−−→
CP ′
(13.10)
in mind, the basis vectors ec, er placed at point C deﬁne the digital image frame,
represented by the column and row basis vectors, ec, er placed at the point C, which
is, in general, not the same as O′. The quantities c, r are then coordinates in this
basis. The coordinates of a point in the digital image frame can be transformed to the
analog image frame by the following equations:
x = −(c −c0)sx
y = −(r −r0)sy
(13.11)
The pixel counts (c, r)T are with reference to the central point, C. The two minus
signs in Eq. (13.11) are motivated as follows. For the x-direction one should note
that the delivered digital image is the one observed by an observer behind the “image
screen” at point O so that an increase in c count results in a decrease in x. For the
y-direction one needs a minus sign because the row count r grows in the opposite
direction of y. By using Eq. (13.11) one can then obtain

c = −x
sx + c0
r = −y
sy + r0
⇔
⎧
⎨
⎩
cZ = (−x
sx + c0)Z
rZ = (−y
sy + r0)Z
Z =
Z
(13.12)
In matrix form, this result is restated as follows:
Lemma 13.2. Let the frame representing the digital picture in a projective camera
be D with the basis vectors ec, er, placed at a point C in the image. Then the coor-
dinates of a point P ′ represented in the analog picture frame A are transformed to
frame D as follows:
(
−−→
CP ′)DH = MD(
−−−→
O′P ′)AH
(13.13)

282
13 World Geometry by Direction in N Dimensions
with
(
−−→
CP ′)DH = Z
⎛
⎝
c
r
1
⎞
⎠,
MD =
⎛
⎝
−1
sx
0
c0
0
−1
sy r0
0
0
1
⎞
⎠,
(
−−−→
O′P ′)AH = Z
⎛
⎝
x
y
1
⎞
⎠
(13.14)
♦
In the lemma, (−−→
CP ′)DH and (−−−→
O′P ′)AH are the homogeneous coordinates of the
point P ′ in frame D, and the homogeneous coordinates of the point P ′ in frame A,
respectively. By combining lemmas 13.1 and 13.2, one can thus obtain a single linear
transformation from frame C to frame D, without passing through A. This is made
explicit in lemma 13.3 .
Lemma 13.3. Given a point P and its camera frame coordinates, (−−→
OP)C, its per-
spective transformation P ′ has digital image frame coordinates that can be obtained
linearly from the former by
(
−−→
CP ′)DH = MI(−−→
OP)C
(13.15)
with
(
−−→
CP ′)DH =
⎛
⎝
cZ
rZ
Z
⎞
⎠,
(−−→
OP)C =
⎛
⎝
X
Y
Z
⎞
⎠
(13.16)
and
MI = MDMA =
⎛
⎝
−1
sx
0
c0
0
−1
sy r0
0
0
1
⎞
⎠
⎛
⎝
f 0 0
0 f 0
0 0 1
⎞
⎠=
⎛
⎝
−fx
0
c0
0
−fy r0
0
0
1
⎞
⎠
(13.17)
Here MI is the intrinsic matrix that encodes the hardware parameters of the camera
with fx =
f
sx , and fy =
f
sy whereas (−−→
OP)C, and (−−→
CP ′)DH are coordinates of a
point in the camera frame, C, and the homogeneous coordinates corresponding to
the perspective projected point in the digital image frame, D, respectively.
♦
The coordinates of the vector −−→
OP are relative the basis eX, eY , eZ, and the vec-
tor −−→
CP ′ is represented via its 3D homogeneous coordinates. From the concept of
homogeneous coordinates, it then follows that
if (
−−→
CP ′)DH =
#
P ′
x, P ′
y, P ′
z
$T
⇒
(
−−→
CP ′)D = 1
P ′z
#
P ′
x, P ′
y
$T = (c, r)T
(13.18)
Conversely,
if
(
−−→
CP ′)D =
 c
r

⇒
(
−−→
CP ′)DH = λ
⎛
⎝
c
r
1
⎞
⎠
(13.19)
with λ being an arbitrary real scalar.

13.2 World Coordinates
283
eY
C
eZ
C
eX
C
eY
W
eZ
W
eX
W
O
W
O
C
P
Fig. 13.3. The camera and the world coordinate systems are shown in yellow and green,
respectively
13.2 World Coordinates
In this section we place the camera frame, C, into another frame, that we call the
world frame, W (Fig. 13.3). We will study how points represented in the camera
frame can be transferred to the world frame. This is useful because the camera frame
and the world frame are often in motion w.r.t. each other while the camera observes
the world. The camera and the world frames do not, in general, have axis parallel
basis vectors, which means that there is a rotation matrix (a tensor) R having the
elements3
R =
⎛
⎝
R11 R12 R13
R21 R22 R23
R31 R32 R33
⎞
⎠
(13.20)
that can align them. Equivalently, the elements of R can be used to express the
camera basis by a linear combination of the world basis:
⎧
⎨
⎩
eC
X = R11eW
X + R12eW
Y + R13eW
Z
eC
Y = R21eW
X + R22eW
Y + R23eW
Z
eC
Z = R31eW
X + R32eW
Y + R33eW
Z
⇔
-
eC
X, eC
Y , eC
Z
.
=
-
eW
X , eW
Y , eW
Z
.
· RT
(13.21)
Here eW
X represents a world basis vector and eC
X represents a camera basis vector,
respectively. The interpretation of the other symbols is analogous. Note that entities
like eW
X are abstract vectors that do not require a particular frame to exist. Deﬁning
the camera and the world basis sets as
C =
-
eC
X, eC
Y , eC
Z
.
,
W =
-
eW
X , eW
Y , eW
Z
.
(13.22)
3 A rotation matrix R is an orthogonal matrix, i.e. RT R = I and R−1 = RT .

284
13 World Geometry by Direction in N Dimensions
one can write the basis change between the world and the coordinate frames, Eq.
(13.21), as follows.
C = WRT
(13.23)
The quantities C, W appear as row vectors in (13.22) in brackets. However, it is im-
portant to note that their elements are not scalars, but basis vectors! To mark that this
is a formalism to represent the basis vectors jointly, in Eq. (13.22) we used C, W,
which are in a different “vector” notation than the ordinary vector notation (boldface
letters). Accordingly, the multiplication in the basis change equation involves the
symbolic quantities such as eC
X, not scalars. The brackets are used throughout this
chapter to mark that we are constructing a matrix or a vector by juxtaposing other
entities, which can be other matrices or vectors in coordinate or abstract representa-
tion.
A vector
−−−→
OCP expressed in the camera frame can be written as
−−−→
OCP = XCeC
X + Y CeC
Y + ZCeC
Z
(13.24)
Here, XC, Y C, ZC are scalars, i.e., coordinates, that are the projections of
−−−→
OCP on
eC
X, eC
Z, eC
Z, respectively. Using the deﬁnitions for C, W in (13.22) we can write the
equation of coordinates, (13.24), as
−−−→
OCP = C · (
−−−→
OCP)C,
where
(
−−−→
OCP)C = (XC, Y C, ZC)T .
(13.25)
Note that we used the subscript (·)C to mark that the vector in question is no longer
an abstract vector, but it is expressed in the camera frame by means of a speciﬁc
triplet of scalars, the coordinates. Accordingly, the same vector will be represented
as a different triplet of scalars in the world frame. The new coordinates can be
obtained by substituting (13.22) in (13.25), yielding:
−−−→
OCP = C(
−−−→
OCP)C = WRT · (
−−−→
OCP)C
(13.26)
Equation (13.25) represents
−−−→
OCP in the camera frame. Using an analogous notation,
it can be represented in the world frame as well:
−−−→
OCP = W(
−−−→
OCP)W
(13.27)
Representing the same vector in the same frame, the right-hand sides of Eqs. (13.26)
and (13.27) can be compared to each other, establishing the following identity be-
tween the coordinate triplets of the camera and the world frames as:
(
−−−→
OCP)W = RT · (
−−−→
OCP)C
(13.28)
or
(
−−−→
OCP)C = R · (
−−−→
OCP)W
(13.29)
In the latter, we note that the coordinate transformation uses the matrix R, which
is the inverse of the matrix used in the basis transformation, Eq. (13.23), RT . We
summarize this result in lemma 13.4.

13.2 World Coordinates
285
Lemma 13.4. If the two bases C, W are related to each other as C = WRT , with R
being an invertible matrix and P being a point in the space E3, then
(
−−−→
OW P)C = R · (
−−−→
OW P)W
(13.30)
where (
−−−→
OW P)C and (
−−−→
OW P)W are the coordinates of
−−−→
OW P in the bases C and W,
respectively.
♦
To reach this conclusion, we used only fundamental principles of linear algebra.
It is worth pointing out that even if the CT matrix R had not been an orthogonal ma-
trix, it would still relate to the basis change matrix inversely. Also, because a vector
is equivalent to all its translated versions, the above result is not only applicable to a
point, but to any vector. Also, the two involved frames, C, W, and the vector space
which they describe do not need to be 3D. We summarize this generalization as a
lemma,
Lemma 13.5. If two bases C, W are related to each other as C = WR−1 with R
being an invertible matrix, and −−→
AB being a vector in the vector space, then
(−−→
AB)C = R · (−−→
AB)W
(13.31)
where (−−→
AB)C and (−−→
AB)W are the coordinates of −−→
AB in the bases C and W respec-
tively.
♦
Returning to the 3D vector space containing the world and camera frames, we
sum the vectors between the three points OW , OC, P (Fig. 13.3) to obtain the equa-
tion
−−−−→
OW OC +
−−−→
OCP +
−−−→
POW = 0
(13.32)
Evidently, the relationship holds in any frame of the vector space, provided that all
three vectors are represented in that same frame. Using the world frame, we obtain
(
−−−→
OW P)W = (
−−−−→
OW OC)W + (
−−−→
OCP)W
(13.33)
In practice, however, the vector
−−−→
OCP is available in the camera frame. The substitu-
tion of Eq. (13.28) in this equation yields
(
−−−→
OW P)W = (
−−−−→
OW OC)W + RT · (
−−−→
OCP)C
(13.34)
or
(
−−−→
OCP)C = R · (
−−−→
OW P)W −R · (
−−−−→
OW OC)W
(13.35)
The latter is an afﬁne relationship between the vectors (
−−−→
OCP)C, (
−−−→
OW P)W , i.e.,
the position vectors of the point P expressed in the camera and world frames.

286
13 World Geometry by Direction in N Dimensions
This is because the term −R · (
−−−−→
OW OC)W is constant for all points P as soon as
R, (
−−−−→
OW OC)W are ﬁxed. The parameters R, (
−−−−→
OW OC)W are called extrinsic param-
eters because they encode the direction and the position of the camera frame relative
to the world frame. Classically, we can make the relationship between the two po-
sition vectors linear by representing (
−−−→
OW P)W in its homogeneous coordinates. By
augmenting the matrix R, we thus obtain
(
−−−→
OCP)C = ME(
−−−→
OWP)WH
(13.36)
where4
ME =[R, −R(
−−−→
OWOC)W ]=[R, (
−−−−→
OCOW)C],
(
−−−→
OWP)WH =

(
−−−→
OWP)W
1

(13.37)
represent the extrinsic matrix and the homogeneous coordinates of the point P w.r.t.
the world frame, respectively. Here we have used lemma 13.4 to obtain (
−−−−→
OCOW)C =
−R(
−−−→
OWOC)W , which is the relationship between the two representations of the dis-
placement vector between the camera and the world frames. The extrinsic matrix is
accordingly a 3 × 4 matrix obtained by juxtaposing R with the column vector of the
interframe displacement, which we deﬁne as follows for convenience:
t = (tX, tY , tZ)T = (
−−−−→
OCOW )C = −R(OW OC)W
(13.38)
We summarize these results as follows:
Lemma 13.6. Given the camera and the world frames C, W at the different origins
OC, OW, and that they are related to each other as C = WRT, then the coordinates
of a point in one frame can be transformed linearly to those in the other as
(
−−−→
OCP)C = ME(
−−−→
OWP)WH
(13.39)
where
ME =[R, t],
(
−−−→
OWP)WH =

(
−−−→
OWP)W
1

(13.40)
with
t = (
−−−−→
OCOW )C
(13.41)
♦
4 Note that we construct matrices from other matrices by use of their symbols and brackets
[·, ·], and then multiply (!) these following the rules of block matrix operations in linear
algebra. A summary is given in the Appendix, Sect. 13.8.

13.3 Intrinsic and Extrinsic Matrices by Correspondence
287
We can now “transport” a point represented in the world coordinates to the perspec-
tive image, where we will be able to ﬁnd its coordinates in the digital image frame.
We do that ﬁrst by transferring the coordinates of the point in the world frame to the
camera frame via Eq. (13.36) and then taking these to the digital image frame via
(13.15), which is realized by matrix multiplications as stated in lemma 13.7.
Lemma 13.7. A point P represented in the world basis is projected to a point in the
digital image basis via a linear transformation if both coordinates are homogeneous:
(
−−→
CP ′)DH = M(
−−−→
OWP)WH = MIME(
−−−→
OWP)WH
(13.42)
where M = MIME is called the camera matrix.5
♦
In conclusion, for every perspective camera there exists a 3 × 4 camera matrix:
M =
⎛
⎝
M11 M12 M13 M14
M21 M22 M23 M24
M31 M32 M33 M34
⎞
⎠= MIME
(13.43)
where MI, ME are deﬁned by Eqs. (13.17) and (13.40), respectively. In the follow-
ing section, we outline the fundamentals of determining M, MI, ME.
13.3 Intrinsic and Extrinsic Matrices by Correspondence
In practice, M is not known to a sufﬁcient degree of accuracy for a variety of rea-
sons, e.g., either or both of MI, ME are noisy or unavailable. Because of this, M
will be assumed here to be unknown. We will estimate M, MI, ME from known
correspondences between a set of image and world points. Deﬁning the pair of ho-
mogeneous position vectors (with λ ̸= 0) that correspond to the same point in the
world frame and in the image frame, respectively, as
p = (X, Y, Z, 1)T = (
−−−→
OWP)WH
λp′ = λ(c, r, 1)T = (
−−→
CP ′)DH
(13.44)
we note that these must satify Eq. (13.42):
Mp −λp′ = 0
(13.45)
If one uses the deﬁnition of M given by Eq. (13.43), this produces three equations:
XM11+ YM12+ ZM13+M14 −cλ = 0
(13.46)
XM21+YM22+ZM23 + M24 −rλ = 0
(13.47)
XM31+YM32+ZM33 + M34 −
λ = 0
(13.48)
5 The matrix M is also known as the projection matrix.

288
13 World Geometry by Direction in N Dimensions
However, pulling λ from Eq. (13.48) and substituting it in Eqs. (13.46) and (13.47)
reduces the number of equations to 2.
XM11+ YM12+ ZM13+M14−x(XM31+YM32+ZM33 + M34)=0(13.49)
XM21+Y M22+ZM23+M24−y(XM31+YM32+ZM33 + M34)=0(13.50)
Now let
c(p, p′) = (X,Y,Z,1,0,0,0,0,−cX,−cY,−cZ,−c)T
(13.51)
r(p, p′) = (0,0,0,0,X,Y,Z,1,−rX,−rY,−rZ,−r)T
(13.52)
where c, r are 12−D vectors produced by the pair of vectors p, p′ representing a
point and its image according to Eq. (13.42). Furthermore, let
m = (M11,M12,M13,M14,M21,M22,M23,M24,M31,M32,M33,M34)T ,
(13.53)
which is a vector version of the unknown matrix M. Consequently, we can express
Eqs. (13.49) and (13.50) as the equation pair
cT m = 0
(13.54)
rT m = 0
(13.55)
where we have omitted displaying the dependency of c, r on p, p′ for convenience.
Geometrically, the homogeneous equation6
sT m = 0
(13.56)
represents the equation of a (hyper)plane in E12, where the constant vector m is the
normal of the plane. For a corresponding pair p, p′, we can obtain two points in this
12D space, i.e., s1 = c and s2 = r given by Eqs. (13.54) and (13.55), that satisfy
Eq. (13.56).
Let S = {c, r} be a set of correspondence vectors.7 Because of measurement
noise, members of S will only approximately satisfy Eq. (13.56). Then, a TLS solu-
tion is the preferable procedure to ﬁnd m. In that, one attempts to minimize
e(m) =

S
|sT m|2h(s)ds = mT

S
ssT h(s)ds

m,
with
∥m∥= 1,
(13.57)
which is the linear symmetry problem in E12, see theorem 12.1. Here h is a func-
tion that represents the certainty on the correspondence data s, which is equivalent
6 An equation is homogeneous if it is of the form Ax = 0, with x being the unknown vector.
Only if the null space of A is nonnil, does another solution than x = 0 exist.
7 The set can be a dense set, e.g., a jointly observed surface in the world frame, and in
the digital image frame. Although the term “digital” implies sampling, strictly speaking,
sampling is not necessary for the conclusions of this chapter. This is because even the
transformation from (x, y, 1)T to (c, r, 1)T is continuous!

13.3 Intrinsic and Extrinsic Matrices by Correspondence
289
to a probability density in statistics, and a mass density in mechanics. The integral
reduces to a summation for a discrete set S = {sj}. When the certainty of a corre-
spondence is available, which can be a computed conﬁdence on the position of the
points or the strength of the salient points being matched, it is used. When this is
not available, h can be approximated by the constant function 1. Accordingly, for a
discrete set of correspondences S
S = {c1, r1, c2, r2, · · · , cN, rN}
(13.58)
and assuming that no certainty data is available for simplicity, the problem consists
in minimizing
mT
⎛
⎝
j
cjcjT +

j
rjrjT
⎞
⎠m = mT BT Bm = 0,
with
B =
 Bc
Br

.
(13.59)
where
Bc =
⎛
⎜
⎜
⎜
⎜
⎝
c1T
c2T
...
cN T
⎞
⎟
⎟
⎟
⎟
⎠
,
Br =
⎛
⎜
⎜
⎜
⎜
⎝
r1T
r2T
...
rN T
.
⎞
⎟
⎟
⎟
⎟
⎠
(13.60)
Therefore,
B =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
X1 Y 1 Z1 1
0
0
0
0 −c1X1
−c1Y 1
−c1Z1
−c1
X2 Y 2 Z2 1
0
0
0
0 −c2X2
−c2Y 2
−c2Z2
−c2
...
...
...
...
...
...
...
...
...
...
...
...
XN Y N ZN 1
0
0
0
0 −cNXN −cNY N −cNZN −cN
0
0
0
0 X1 Y 1 Z1 1 −r1X1
−r1Y 1
−r1Z1
−r1
0
0
0
0 X2 Y 2 Z2 1 −r2X2
−r2Y 2
−r2Z2
−r2
...
...
...
...
...
...
...
...
...
...
...
...
0
0
0
0 XN Y N ZN 1 −rNXN −rNY N −rNZN −rN
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(13.61)
Equivalently, the problem is to solve the system of equations [1]:
Bm = 0
(13.62)
The solution can be determined only up to a multiplicative constant because of the
homogeneity of the underlying equation, i.e., if m is a solution then so is γm with
γ ̸= 0. The solution is given by the least signiﬁcant eigenvector8 of BT B. If the
solution is to be unique and nontrivial, then the least eigenvalue of BT B should
ideally be 0 (close to 0 in practice), and it should have multiplicity 1.
8 Equivalently, the solution can be found by the least singular vector of B as obtained from a
singular value decomposition. We will discuss this decomposition, which is used frequently
to solve homogeneous equations (i.e. those like Eq. (13.62), efﬁciently, in Sect. 15.3.

290
13 World Geometry by Direction in N Dimensions
Fig. 13.4. The checkerboard consists of black and white squares with side lengths of 28 mm.
Such patterns are frequently used to calibrate cameras, i.e., their intrinsic and extrinsic param-
eters are computed
When m, and thereby M, is estimated, however, MI and ME are yet to be
determined. To do that, we spell out M in terms of its physical parameters.
M =
⎛
⎝
−fx
0
c0
0
−fy r0
0
0
1
⎞
⎠
⎛
⎝
R11 R12 R13 tX
R21 R22 R23 tY
R31 R32 R33 tZ
⎞
⎠
(13.63)
=
⎛
⎝
−fxR11 + c0R31, −fxR12 + c0R32, −fxR13 + c0R33, −fxtx + c0tz
−fyR21 + r0R31, −fyR22 + r0R32, −fyR23 + r0R33, −fyty + r0tz
R31,
R32,
R33,
tz
⎞
⎠
Accordingly, and by observing that the ﬁrst three columns of the last row in M is a
row of an orthogonal matrix that should have the norm 1 [70], we can compute ﬁrst
γ

M 2
31 + M 2
32 + M 2
33 = γ

R2
31 + R2
32 + R2
33 = γ
(13.64)
and normalize M as
M ←1
γ M
(13.65)
Next we calculate c0, r0, fx, fy by building pairwise scalar products between the
subrows of Eq. (13.63) that contain the elements of R to conclude that

13.3 Intrinsic and Extrinsic Matrices by Correspondence
291
X
Y
O
Z
Fig. 13.5. A camera to be calibrated takes the picture of a known pattern
(M11, M12, M13)
⎛
⎝
M31
M32
M33
⎞
⎠= c0
. (M11, M12, M13)
⎛
⎝
M31
M32
M33
⎞
⎠= r0
M 2
11 + M 2
12 + M 2
13 = f 2
x + c2
x
.M 2
21 + M 2
22 + M 2
23 = f 2
y + c2
y.
where the observation that different rows of R are orthogonal has been utilized. In the
equations, Mij are the known parameters, to the effect that the intrinsic parameters
can be pulled out without effort. Once the intrinsic parameters are known, they can
be substituted into Eq. (13.63) so that t is identiﬁed from the last column, whereas
R is identiﬁed from the remaining columns, i.e., the ﬁrst 3 × 3 block matrix on the
left.
Using the above procedure, from every M, one can estimate MI, ME. However,
for every registered view of the calibration pattern, one can obtain an M and thereby
a pair of MI, ME. A series of views containing the pattern can thus be shot by mov-
ing and/or tilting it randomly, yielding a sequence of matrices MI and ME, where
MI is the estimate of a constant matrix, the intrinsic parameters. The intrinsic pa-
rameters of the camera should remain unchanged so that an average of the matrices
MI will be a better estimate of the intrinsic matrix as compared to that of an indi-
vidual calibration. The matrix ME will nevertheless be different for each calibration
because each estimated ME encodes the rotation and the distance of the calibra-

292
13 World Geometry by Direction in N Dimensions
100
0
100
0
200
400
600
800
1000
200
100
0
100
8
9
13
3
11
1
6
5
10
2
7
12
4
Xc
Z c
Yc
Oc
Fig. 13.6. The various placements of the calibration pattern relative to the camera as computed
from the extrinsic parameters, ME
tion pattern, which is the world frame, at different positions but relative to the ﬁxed
camera frame. Equivalently, each ME expresses the position and the rotation of the
camera frame with respect to the world frame, which can be assumed ﬁxed.
Example 13.1. The correspondences can be established by letting the camera ob-
serve a known pattern, the calibration pattern. Figure 13.4 illustrates a digital camera
calibration set-up. Two digital cameras of different brands will be calibrated, i.e., the
intrinsic and extrinsic parameters of each of the cameras w.r.t. a (world) coordinate
frame will be determined. Each camera images calibration pattern whose geometry
is fully known, e.g, in the case of the ﬁgure this means that the checkerboard consists
of black and white squares with side length of 28 mm.
The world coordinate frame is assumed to be attached to the pattern (Fig. 13.5).
Because the sizes of the squares are known and the world coordinate frame is at-
tached to the calibration pattern, the world point coordinates, Xj, Y j, Zj, of the
corners of the squares are therefore known. Their corresponding image points cj, rj
can be identiﬁed in the camera image either automatically, e.g., by the lack of linear
symmetry technique discussed in Sect. 10.9 or manually. Together, this knowledge
allows us to determine cj, rj corresponding to the rows of the matrix B. For each
view imaged by the camera one can thus compute the data matrix B and thereby the
matrix BT B, least eigenvector corresponds to the TLS estimate of m.
Figure 13.6 depicts the 3D placements of the calibration pattern from the cam-
era viewpoint. The graphics were drawn by using 13 estimated matrices ME cor-
responding to 13 different placements of the calibration pattern and using the left
camera in Fig. 13.4.

13.4 Reconstructing 3D by Stereo, Triangulation
293
Fig. 13.7. The system graph summarizes a stereo camera set-up with both cameras as well as
the corresponding epipoles, EL, ER, marked
13.4 Reconstructing 3D by Stereo, Triangulation
In this section we will discuss how to estimate the position of a point in the world
by means of two images. The ideas behind stereo processing can also be extended to
three or more cameras, although we will not elaborate on them to limit the scope. To
be precise, two images taken by two perspective cameras located at different places
but observing the point simultaneously will be assumed. The issue of taking pictures
simultaneously is evidently an approximation of the reality. When the points in the
view ﬁelds of the two cameras move much slower than a camera, even one camera
taking pictures at two different locations and two different times can be viewed as a
reasonable approximation of a stereo system. The points of interest in our system are
listed below and are also marked in Fig. 13.7:
P
An arbitrary point in the common view ﬁeld of the cameras
P ′L The image of P in the left camera
P ′R The image of P in the right camera
OL The projection center of the left camera
OR
The projection center of the right camera
EL The image of OR in the left camera, the left epipole
ER
The image of OL in the right camera, the right epipole
O′L The image of OL in the left camera
O′R The image of OR in the right camera
OW The projection centre of a ﬁctitious world frame
In Fig. 13.7 we have only shown the left and the right coordinate frames, whereas we
have only marked OW for simplicity because the world frame can be eliminated by
transferring the reference to one of the camera frames, here the left. We will elaborate
on this further below.

294
13 World Geometry by Direction in N Dimensions
Next we discuss triangulation, which is the process of reconstructing the posi-
tion vector of a 3D point in the reference (here left) coordinate frame. The name
triangulation refers to the triangle OLPOR because the sought
−−→
OLP is obtained by
summing up the involved vectors as
−−→
POL +
−−−→
OLOR +
−−→
ORP = 0
(13.66)
where coordinates of all vectors are represented in the same frame, the left camera
coordinates. Triangulation is evidently not possible when ∥−−→
OO′∥= 0. Joint de-
termination of the position vector corresponding to a world point and the camera
parameters is a difﬁcult problem in the presence of measurement noise. Unsurpris-
ingly, there have been numerous research studies [98] on the topic. We describe in
the following a basic technique that solves the triangulation problem after that the
camera parameters have been determined. We have chosen this technique more for
its simplicity and reasonable efﬁciency than for its superiority.
TLS Triangulation in a Projective Space
Assume that we know the positions of two points P ′L, P ′R in the left and right
camera images of the same world point P (Fig. 13.7). Then, according to lemma
13.7, we have
⎧
⎨
⎩
ML(
−−−→
OWP)WH = (
−−−−→
CLP ′L)DHL
MR(
−−−→
OWP)WH = (
−−−−→
CRP ′R)DHR
(13.67)
where ML, MR are the camera parameters obtained from calibrating the left and
the right cameras individually against a known world frame placed at the point OW .
Remembering that the cross-products of parallel lines vanish, we rewrite these as
⎧
⎨
⎩
(
−−−−→
CLP ′L)DHL × ML(
−−−→
OWP)WH = TLMLp = 0
(
−−−−→
CRP ′R)DHR × MR(
−−−→
OWP)WH = TRMRp = 0
(13.68)
Here p = (
−−−→
OWP)WH is the unknown, and we expressed the cross-product operation
as a matrix multiplication according to:
(
−−−−→
CLP ′L)DHL × · =
⎛
⎝
cL
rL
1
⎞
⎠× ·
⇒
TL =
⎛
⎝
0
−1 rL
1
0 −cL
−rL cL
0
⎞
⎠· (13.69)
(
−−−−→
CRP ′R)DHR × · =
⎛
⎝
cR
rR
1
⎞
⎠× ·
⇒
TR =
⎛
⎝
0
−1 rR
1
0 −cR
−rR cR
0
⎞
⎠· (13.70)
where the elements of the matrices are derived from known image coordinates of
correspondence points in respective cameras. Consequently, we obtain:

13.4 Reconstructing 3D by Stereo, Triangulation
295

TLMLp = 0
TRMRp = 0
(13.71)
This is equivalent to solving for p in the homogeneous equation
Ap = 0,
with
A =
 TLML
TRMR

,
(13.72)
which can be done by minimizing ∥pT AT Ap∥2. The solution is, however, in the
world coordinate system, which may not be desirable if there is a series of calibra-
tion parameter matrices of a ﬁxed stereo system that amenate from different world
coordinates. One might then wish to compute the position of P in the left camera
coordinates. We discuss this below.
By using Eq. (13.39) for (
−−−→
OLP)LC, the 3D position vector of the point P in the
left camera coordinates, one can obtain
(
−−−→
OLP)LC = ML
E(
−−−→
OWP)WH = ML
Ep
(13.73)
where RL, tL are the rotation and the translation matrices constituting ML
E, respec-
tively, being the extrinsic matrix of the left camera w.r.t. the world frame. A substi-
tution in the ﬁrst line of Eq. (13.71) then affords a formulation in the left camera
coordinates:
TLMLp = TLML
I ML
Ep = TLML
I (
−−−→
OLP)LC = 0
(13.74)
where we have used ML = ML
I ML
E. Deﬁng the unknown ˜p as the homogenized
(
−−−→
OLP)LC:
˜p = (
−−−→
OLP)LCH =

(
−−−→
OLP)LC :
1

(13.75)
and appending a null column to ML
I via ML
I [I, 0], we can thus rewrite Eq. (13.74)
and thereby express the left camera equation as
TLML
I [I, 0]˜p = 0
(13.76)
The vector ˜p is, as p is, homogenized and 4D except that it is in the left camera
coordinates.
We now turn to the right camera equation in Eq. (13.71) with the purpose
of rewriting the unknown in the left camera coordinates. Remembering from Eq.
(13.39) that
ML
E = [RL, tL],
and
p = (
−−−→
OWP)WH =

(
−−−→
OWP)W
1

,
(13.77)
we can rewrite Eq. (13.73):
(
−−−→
OLP)LC = ML
Ep = RL(
−−−→
OWP)W + tL
(13.78)

296
13 World Geometry by Direction in N Dimensions
9
3
6
1
5
10
7
X
Z
Y
Right
4
Z
X
Y
Left
2
0
200
400
600
200
400
600
800
1000
100
0
100
200
300
Fig. 13.8. This computed view of camera and checkerboard positions of Fig. 13.4 requires
knowledge of ME, ML
I , MR
I , which are in turn computed by using known correspondence
points
to pull out
(
−−−→
OWP)W = RLT (
−−−→
OLP)LC −RLT tL
(13.79)
and to obtain p by homogenization as
p = (
−−−→
OWP)W H =

(
−−−→
OWP)W
1

=

RLT (
−−−→
OLP)LC −RLT tL
1

(13.80)
By substituting this in Eq. (13.71) and using the decomposition of MR
E in analogy
with Eq. (13.77), we obtain:
TRMRp = TRMR
I MR
Ep = TRMR
I [RR, tR]

RLT (
−−−→
OLP)LC −RLT tL
1

= TRMR
I (RR(RLT (
−−−→
OLP)LC −RLT tL) + tR)
= TRMR
I (RRRLT (
−−−→
OLP)LC −RRRLT tL + tR)
(13.81)
We now deﬁne,
R = RRRLT ,
t = tR −RtL,
and
ME = [R, t],
(13.82)
and call the matrix ME the stereo extrinsic matrix because it represents the relative
rotation and displacement of the two cameras. Using Eq. (13.81) we then obtain the
equation of the right camera system as
TRMRp = TRMR
I (R(
−−−→
OLP)LC + t)
= TRMR
I [R, t]˜p = TRMR
I ME ˜p
(13.83)

13.4 Reconstructing 3D by Stereo, Triangulation
297
Fig. 13.9. The images registered by the left and right camera system shown in Fig. 13.4
Finally, we put together (13.76) and (13.83):
 TLML
I [I, 0]˜p = 0
TRMR
I ME ˜p = 0
(13.84)
to obtain:
˜A˜p = 0,
with
˜A =

TLML
I [I, 0]
TRMR
I ME

.
(13.85)
Equation (13.85) can be solved for ˜p by minimizing
∥˜A˜p∥2 = ˜pT ˜A
T ˜A˜p
(13.86)
The found ˜p is now in the left camera coordinate system and solves a 4D linear
symmetry direction estimation problem. Accordingly, ˜p is given by the least eigen-
vector of ˜A
T ˜A. An analogous solution and interpretation in terms of direction in a
4D space can evidently also be made for Eq. (13.72). We summarize our ﬁndings as
the following two lemmas.
Lemma 13.8. Given the intrinsic matrices ML
I , MR
I and the extrinsic matrices,
ML
E = [RL, tL],
MR
E = [RR, tR]
(13.87)
of two cameras, the stereo extrinsic matrix yields

298
13 World Geometry by Direction in N Dimensions
ME = [R, t]
(13.88)
where
R = RRRLT ,
and
t = tR −RtL.
(13.89)
The coordinates of vectors marked with L and R are in the left and right camera
coordinates, respectively, whereas the L and R marked matrices rotate the world
coordinates to the respective camera coordinates.
♦
Compared with Sect. 13.2, we note that the left camera frame (the reference) here
takes the role of the world frame there, (not to be confused with the world frame
attached to the calibration pattern, at OW in Fig. 13.7). Consistently with this, t
represents the position of the left camera center (the world center) in the right cam-
era coordinates, whereas the matrix R transfers the coordinates of a vector in the
left camera (the world) to coordinates in the Right camera, see Figs. 13.3, 13.7 in
connection with lemmas 13.6 and 13.8.
Lemma 13.9. Let the intrinsic matrices ML
I , MR
I of two cameras and the extrinsic
matrix of the corresponding stereo system ME = [R, t], having the left camera as
reference, be given. Then a TLS estimate of a homogenized world point position, ˜p,
in the reference frame is given by the solution of a 4D linear symmetry direction
determination problem,
min
∥˜p∥=1 ˜pT ˜A
T ˜A˜p,
with
˜A =

TLML
I [I, 0]
TRMR
I ME

,
(13.90)
where the image coordinates of a point P are encoded as
TL =
⎛
⎝
0
−1 rL
1
0 −cL
−rL cL
0
⎞
⎠,
TR =
⎛
⎝
0
−1 rR
1
0 −cR
−rR cR
0
⎞
⎠
(13.91)
for the left and in the right camera views, respectively.
♦
That ME should be independent of our (reasonable but yet) arbitrary placement
of the calibration pattern (the world center at OW ) is supported by lemma 13.8. The
extrinsic parameters of a stereo system can be obtained by combining the individual
calibrations of the two cameras using the lemma. Based on individual camera ro-
tation and translation estimations w.r.t. to the current world coordinates, a series of
such measurements can thus be computed from displaced and rotated calibration pat-
terns via Eq. (13.82). The obtained ME, as well as the intrinsic matrices ML
I , MR
I ,
can subsequently be averaged to reduce the random measurement errors.
Example 13.2. Using known patterns, such as a checkerboard pattern, and corre-
spondence between points, we can estimate the extrinsic and intrinsic matrices of a
stereo camera system which allows us to determine the positions of the cameras in

13.4 Reconstructing 3D by Stereo, Triangulation
299
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x xx
x
x
x x
x
x
x x x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x xx
x
x
x x
x
x
x x x
Fig. 13.10. (Left) The result of 3D position estimation for a sparse set of points. The planes
interpolate linearly between the estimated 3D points. (Right) The set of points in the right
camera view, for which the 3D positions have been estimated
the world. Fig. 13.8 illustrates a result of the camera calibration for the stereo cam-
eras shown in Fig. 13.4. The positions of the checkerboards as well as the positions
of the two cameras observing them are marked. The cameras were static all the time,
whereas the world coordinates attached to the checkerboard were displaced.
As in the linear symmetry direction estimate case discussed in Chap. 12, the
eigenvalues of the matrix ˜A
T ˜A can be sorted and used in various combinations to
provide an estimate for the quality of the ﬁtted plane, normal whose now represents
the sought position in the triangulation problem. Examples include λ4, λ3 −λ4, or
the dimensionless certainty measure:
Ct = λ3 −λ4
λ3 + λ4
(13.92)
where λ4 is the least signiﬁcant eigenvalue.
Example 13.3. Fig. 13.9 shows the images registered by the left and right cameras
shown in Fig. 13.4. The cameras were calibrated, i.e., the extrinsic as well as intrinsic
parameters are known. In Fig. 13.10 the triangulation is illustrated by using the stereo
image pair of Fig. 13.9. The result of triangulation for a set of points is shown on the
left. The surfaces are planes that interpolate between the 3D points, whereas color
modulates height. Because their 3D positions are known, the object deﬁned by the

300
13 World Geometry by Direction in N Dimensions
planes could be rotated and imaged from a different view than the camera views. On
the right we show the used set of points in the right camera view.
13.5 Searching for Corresponding Points in Stereo
Finding correspondence points in stereo images automatically is a difﬁcult problem.
In this section we discuss how the search for a point can at least be constrained even
if the problem remains a difﬁcult one. Simpliﬁed, the underlying logics consists in
translating the knowledge on the stereo system to limit the freedom of a correspond-
ing point, as to where it can be. Without stereo assumption, the corresponding point
can be anywhere in the other camera view, i.e., it has a degree of freedom of 2,
whereas we expect to reduce this freedom to 1 by use of knowledge on the stereo
set-up.
We will utilize an axiom of the 3D world, namely that three points deﬁne a plane.
We observe that the normal of a plane is orthogonal to all vectors lying in that plane.
The points OL, P, OR in a stereo system then deﬁne a plane, with its normal vector
given by the cross-product
−−−−→
OROL ×
−−−→
OLP, which is in turn orthogonal to
−−−→
ORP:
(
−−−→
ORP)T (
−−−−→
OROL ×
−−−→
OLP) = 0
(13.93)
All vectors must be represented in the same frame for this equation to hold. We keep
the same notation as in previous sections to characterize the stereo system, meaning
that
•
the relative displacement vector t = (
−−−−→
OROL)R is in the right camera frame,
whereas
•
the relative rotation matrix R transfers the coordinates of a vector represented in
the left camera to the right camera.
Accordingly, in Eq. (13.93) there is one vector,
−−−→
OLP, that can be expressed most nat-
urally in the left (reference) camera frame, whereas the remaining two are normally
expressed in the right camera coordinates. We assume then that the coordinates of all
vectors are w.r.t. the right camera basis
(
−−−→
ORP)T
R((
−−−−→
OROL)R × (
−−−→
OLP)R) = 0
(13.94)
so that by the replacement (
−−−→
OLP)R = R(
−−−→
OLP)L, we obtain:
(
−−−→
ORP)T (
−−−−→
OROL × (R(
−−−→
OLP)L)) = 0
(13.95)
where, and below, we have dropped marking the right camera frame because all vec-
tors are in the right camera coordinates unless otherwise mentioned. Note however,
that this does not mean a change of the world frame. The left camera still plays

13.5 Searching for Corresponding Points in Stereo
301
the role of the world frame in analogy with Sect. 13.2, and consequently, the inter-
pretation of t, R remains as listed above. We express the cross-product operation
−−−−→
OROL × q as a matrix multiplication with a vector q, that is:
−−−−→
OROL × q = T · q
(13.96)
where, given that
−−−−→
OROL = (X, Y, Z)T
(13.97)
the matrix T is determined by the elements of
−−−−→
OROL as
T =
⎛
⎝
0
−Z
Y
Z
0
−X
−Y
X
0
⎞
⎠
(13.98)
The coplanarity equation, (13.95), for points OLPOR then yields:
(
−−−→
ORP)T TR(
−−−→
OLP)L = 0
(13.99)
Consequently, the coplanarity equation can be expressed as
(
−−−→
ORP)T E(
−−−→
OLP)L = 0,
(13.100)
where
E = TR
(13.101)
Calling the third components of (
−−−→
OLP)L,
−−−→
ORP as ZL, ZR, and the focus length of
the two camera frames as f L, f R, respectively, we can, from Eq. (13.100) obtain
( f R
ZR
−−−→
ORP)T E( f L
ZL
−−−→
OLP)L = 0
(13.102)
so that
(
−−−−−→
O′RP ′R)T E (
−−−−→
O′LP ′L)L = 0
(13.103)
Note that both
−−−−−→
O′RP ′R and (
−−−−→
O′LP ′L)L are now image points expressed in homoge-
neous coordinates in the right and the left image planes, respectively. In what follows
we will not mark the homogenized position vectors in the two image planes, because
all such vectors are homogenized.
The matrix E, often named the essential matrix for the stereo system,9 is not
affected when the point P moves in the space. It changes only when the relative
position and gaze of the cameras change by a translation (via T ), and/or by a rotation
(via R), respectively. The equation can be utilized when determining correspondence
9 Note that E = TR and the stereo extrinsic matrix, ME = [R, t], discussed in Sect. 13.4
are obtained from the same information.

302
13 World Geometry by Direction in N Dimensions
because its validity is required if two points in stereo view correspond to the same
3D scene point, P.
We show next that the identity Eq. (13.103) can also be interpreted as two equa-
tions of two lines lying in the left and the right (analog) image frames, respectively.
First, we assume that we have a world point P that moves along the line repre-
sented by
−−−−→
ORP ′R. The right image of such a a point is always P ′R, to the effect that
(
−−−−−→
O′RP ′R)T remains unchanged, and one obtains
0 = (
−−−−−→
O′RP ′R)T E (
−−−−−→
O′LP ′L)L = (a, b, c)
⎛
⎝
x
y
1
⎞
⎠
(13.104)
= ax + by + c = 0
where
−−−−−→
O′LP ′L = (x, y, 1)T has been assumed. Naturally, a, b, and c are the scalars
that must be obtained via
(a, b, c) = (
−−−−−→
O′RP ′R)T E
(13.105)
which is a vector that has a constant direction,10 as long as P is a point along the
inﬁnite line represented by
−−−−→
ORP ′R. Accordingly, the direction of (a, b, c)T remains
unchanged, even for the point OR. In conclusion, EL must lie on the line ax + by +
c = 0 which passes through the point P ′L. This line is sometimes called the (left)
epipolar line (of point P). An important byproduct of this reasoning is that EL must
lie on all (left) epipolar lines (of all points P in the 3D space) as long as the extrinsic
parameters, E, encoding the relative position and gaze of the stereo cameras are
unchanged. In other words, all (left) epipolar lines intersect at the (left) epipole, EL.
Second, assuming that the world point P now moves along the line that corre-
sponds to
−−−−→
OLP ′L, we can ﬁnd analogous results for the right image frame. Accord-
ingly, it can be shown that the line
a′x′ + b′y′ + c′ = 0
(13.106)
with
(a′, b′, c′)T = E(
−−−−→
O′LP ′L)L and
−−−−→
O′RP ′R = (x′, y′, 1)T
(13.107)
is the right epipolar line with the Right epipole ER, both deﬁned in analogy with
their left counterparts.
The vector (
−−−→
OLEL)L is not in the image plane of the left camera, and it is not
homogenized, i.e., it is an ordinary 3D vector. Yet, it is in the null space of the matrix
E:
10 The vector
−−−−−→
O′RP ′R is represented in homogeneous coordinates, i.e., it can be known only
up to a scale factor. In turn this causes (a, b, c)T to change only up to a scale factor when
−−−−−→
O′RP ′R is scaled.

13.5 Searching for Corresponding Points in Stereo
303
E(
−−−→
OLEL)L = TR(
−−−→
OLEL)L =
−−−−→
OROL ×
−−−→
OLEL = 0
(13.108)
because it is parallel to
−−−−→
OROL, and multiplication by T is a cross-product that is
fully determined by
−−−−→
OROL, Eq. (13.98). Yet, (
−−−→
OLEL)L is a homogenized version of
(
−−−−→
O′LEL)L, so that the latter is also in the null space of E. Similarly, the homogenized
−−−−→
O′RER must be in the null space of ET because
ET −−−−→
ORER = RT TT −−−−→
ORER
(13.109)
which, noting that TT = −T, translates to:
ET −−−−→
ORER = −RT T
−−−−→
ORER = −RT −−−−→
OROL ×
−−−−→
ORER = 0
(13.110)
Evidently, the 3 × 3 matrices E, ET are rank-deﬁcient. We summarize these results
in the following lemma.
Lemma 13.10. Let a stereo system be characterized by a rotation matrix R such
that R(
−−−→
OLP)L = (
−−−→
OLP)R, and by a displacement vector t = (X, Y, Z)T =
(
−−−−→
OROL)R, such that the matrix T is determined by the elements of t as
T =
⎛
⎝
0
−Z
Y
Z
0
−X
−Y
X
0
⎞
⎠
(13.111)
Then, two points P ′L and P ′R in the left and the right (analog) image coordinates
are images of the same world point P if and only if
(
−−−−−→
O′RP ′R)T
R E (
−−−−→
O′LP ′L)L = 0,
with
E = TR.
(13.112)
Furthermore, the homogenized (analog) image coordinates of the left and the right
epipoles, EL, ER, are in the null spaces of the essential matrix, E, and its transpose,
ET , respectively,
E−→( O′LEL)L = 0,
ET (
−−−−→
O′RER)R = 0
(13.113)
♦
Now we show that the relative position and gaze of the cameras are not needed
to ﬁnd the epipoles, and that the latter implicitly encode the extrinsic parameters of
the stereo system. We note ﬁrst that (
−−−−−→
O′LP ′L)L is represented in the analog image
coordinates of the left frame. It can also be expressed in the digital image coordinates,
i.e., in column and row counts via

304
13 World Geometry by Direction in N Dimensions
(
−−−−−→
O′LP ′L)LD = ML
I (
−−−−−→
O′LP ′L)L
(13.114)
where ML
I is the matrix encoding the intrinsic parameters of the left camera. Simi-
larly, we obtain
(
−−−−−→
O′RP ′R)RD = MR
I (
−−−−−→
O′RP ′R)R
(13.115)
The epipolar equation (13.112) can then be denoted as
(
−−−−−→
O′RP ′R)T
RDF(
−−−−−→
O′LP ′L)LD = 0,
(13.116)
where
F = (MR
I )−T E (ML
I )−1
(13.117)
The matrix F is often called the fundamental matrix, which can be obtained via a set
of correspondences in digital image coordinates [156].
The coordinates of a vector q represented in the left analog image coordinates
can be transformed back and forth to left digital image coordinates. Evidently, the
same can be done for the left camera image. Because of this, identical reasoning
can be followed to reach analogous conclusions as those in lemma 13.10, but for the
fundamental matrix. This is given precision in the following lemma.
Lemma 13.11. Let a stereo system be characterized by a rotation matrix R such
that R(
−−−→
OLP)L = (
−−−→
OLP)R, and by a displacement vector t = (X, Y, Z)T =
(
−−−−→
OROL)R, such that the matrix T is determined by the elements of t as
T =
⎛
⎝
0
−Z
Y
Z
0
−X
−Y
X
0
⎞
⎠
(13.118)
Then, two points P ′L and P ′R in the left and the right (digital) image coordinates
are images of the same world point P if and only if
(
−−−−→
CRP ′R)T
RDF(
−−−−→
CLP ′L)LD =0,
with
F = (MR
I )−TE(ML
I )−1.
(13.119)
Furthermore, the homogenized (digital) image coordinates of the left and right
epipoles, EL, ER, are in the null spaces of the fundamental matrix, F, and its trans-
pose, FT , respectively:
F−→( CLEL)LD = 0,
FT (
−−−−→
CRF R)RD = 0
(13.120)
♦
The main points of interest with epipoles and the epipolar lines include the fol-
lowing:

13.6 The Fundamental Matrix by Correspondence
305
•
The corresponding points can be searched along lines that can be effectuated as
a 1D search instead of a search in two dimensions. For example, if the point
(
−−−−−→
O′LP ′L)LD is known, then by substituting it in Eq. (13.116) one obtains the
search line on which the corresponding unknown point (
−−−−−→
O′RP ′R) must lie.
•
The establishment of corresponding points in images is a crucial problem for 3D
geometry reconstruction from stereo.
•
The epipolar lines can be utilized to implement triangulation where the rays
−−−−→
OLP ′L and
−−−−→
ORP ′R are guaranteed to intersect.
13.6 The Fundamental Matrix by Correspondence
Assume that the correspondences of (at least) eight points in the digital image co-
ordinates are known. Then Eq. (13.116) can be written for a known pair of points,
expressed in homogeneous coordinates, as
0 = (pR)T F pL
= cRcLF11 + cRrLF12 + cRF13 +
+rRcLF21 + rRrLF22 + rRF23 +
+
cLF31 +
rLF32 +
F33 = 0
(13.121)
where the unknown matrix elements,
F =
⎛
⎝
F11 F12 F13
F21 F22 F23
F31 F32 F33
⎞
⎠
(13.122)
and the known pair of points,
pL = (cL, rL, 1)T
and
pR = (cR, rR, 1)T ,
(13.123)
have been utilized. Equation (13.121) is a scalar product between two 9D vectors
that reduces to the homogeneous equation
qT f = 0
(13.124)
with
q = ( cRcL, cRrL, cR, rRcL, rRrL, rR,
cL,
rL,
1 )T
f = ( F11, F12, F13, F21,
F22, F23, F31, F32, F33 )T
(13.125)
where the vectors q and f encode the known data and the unknown parameters,
respectively. Geometrically, Eq. (13.124) represents the equation of a hyperplane in
E9, and we are interested once again in ﬁnding the direction of this hyperplane, f,
when we know a set of correspondences, Q = {q}, of image points in a pair of stereo

306
13 World Geometry by Direction in N Dimensions
images.11 The equation also represents the distance of a point to the hyperplane so
that the problem is a linear symmetry direction–ﬁtting problem, see theorem 12.1, in
E9 where one attempts to minimize
e(f) =

Q
|qT f|2dq = f T (

Q
qqT h(q))dqf,
with
∥f∥= 1
(13.126)
Here h is the strength of the correspondence q, which is equivalent to a probabil-
ity density in statistics, and a mass density in mechanics. The integral reduces to a
summation for a discrete set Q = {qj}, and h reduces to a discrete certainty on
correspondence, or a constant if it is not available. Note that the constraint ∥f∥= 1
is deduced from the fact that we can determine f only up to a scale constant. This is
because (13.124) is homogeneous, i.e., it is satisﬁed by λf, where λ is a scalar, if it
is satisﬁed by f. For a discrete Q and no certainty data:
Q = {q1, q2, · · · , qN}
(13.127)
the problem is conﬁned to minimization of
f T (

j
qjqjT )f = f T QT Qf = 0,
with
Q =
⎛
⎜
⎜
⎜
⎜
⎝
q1T
q2T
...
qN T
⎞
⎟
⎟
⎟
⎟
⎠
.
(13.128)
that is, the elements of Q are given by
Q =
⎛
⎜
⎜
⎜
⎜
⎝
cR1cL1, cR1rL1, cR1, rR1cL1, rR1rL1, rR1, cL1, rL1, 1
cR2cL2, cR2rL2, cR2, rR2cL2, rR2rL2, rR2, cL2, rL2, 1
...
...
...
...
...
...
...
...
...
cRNcLN, cRNrLN, cRN, rRNcLN, rRNrLN, rRN, cLN, rLN, 1
⎞
⎟
⎟
⎟
⎟
⎠
(13.129)
Designating the least signiﬁcant eigenvalue of the 9 × 9 matrix QT Q as λ9, and
its corresponding eigenvector as v9, the solution of (13.128) is given by f = v9.
Assuming that the thus obtained vector f has the elements
f = (f1, f2, f3, f4, f5, f6, f7, f8, f9)T
(13.130)
and using (13.122) and (13.125), one can obtain
F =
⎛
⎝
f1 f2 f3
f4 f5 f6
f7 f8 f9
⎞
⎠
(13.131)
We recall from lemma 13.11 that if this F is to be a useful solution to our problem,
F must be rank-deﬁcient, i.e., its least eigenvalue must equal to zero. Ideally, if the
11 In principle, the correspondence set Q could be a dense set in this formalism.

13.7 Further Reading
307
measurement noise of the data is zero, then F has an eigenvalue that is zero. How-
ever, there is no guarantee for this to happen automatically in practice because the
measurements on which one bases the calculation of f are not noise-free. Accord-
ingly, a numerical correction method, guaranteeing the singularity of F, is applied.12
Naturally, the epipolar line represented by (
−−−−→
O′LEL)LD is given by the last row
of V, whereas (
−−−−→
O′RER)RD is given by the last row of U. The epipoles in (homoge-
neous) analog image coordinates x, y, 1 are given by:
(
−−−−→
O′LEL)L = ML
I
−1(
−−−−→
O′LEL)LD
(
−−−−→
O′RER)R = MR
I
−1(
−−−−→
O′RER)RD
when the intrinsic matrices ML
I and MR
I are known for both cameras. In that case,
E can be found too (up to a scale factor):
E = MR
I
T FML
I = TR
(13.133)
Here, the matrix T encodes the displacement between the stereo cameras, whereas
R represents their relative rotation, lemma 13.11.
13.7 Further Reading
An alternative introduction to camera calibration and stereo vision of world geometry
can be found [219]. In [70] a detailed discussion on projective geometry tools and
geometry reconstruction by images can be found, including reconstruction by use
of more than two views. Modern developments in the theory and practice of scene
reconstruction are described in detail and comprehensive background material is pro-
vided in the monograph of [98]. In [102] the essentials of camera calibration, includ-
ing an extended pinhole camera model with a nonlinear lens correction, is discussed.
Epipolar constraints are covered in detail in [8]. Studies where wide-baseline stereo
issues are discussed include [211, 215]. Even without camera calibration, scene un-
derstanding is possible from image views for numerous applications [14, 106, 170].
Robots controlled by active vision [50] or stereo from motion [194] are signiﬁcant
application ﬁelds of geometry studies in computer vision. Another signiﬁcant do-
main is architectural or similar scenes, where there is little or no motion [55,86].
12 This can be achieved by the singular value decomposition of F that we will discuss in Sect.
15.3. The decomposition will yield:
F = UΣVT
⇒
˜F = UΣ′VT
⇒
F ←˜F
(13.132)
where Σ′ is the same (diagonal) matrix as Σ except that the least signiﬁcant (diagonal)
element, Σ(3, 3) is now replaced by 0.

308
13 World Geometry by Direction in N Dimensions
13.8 Appendix
Below we illustrate matrix augmentation and multiplication by use of block matrices:
⎛
⎜
⎜
⎜
⎜
⎜
⎝
A
/ 01 2
1 2
4 5

B
/ 01 2
3
6

#
7 8
$
1 2/ 0
C
#
9
$
12/0
D
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎝
A′
/ 01 2
9 8
6 5

B′
/ 01 2
7
4

#
3 2
$
1 2/ 0
C′
#
1
$
12/0
D′
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=

A B
C D
 
A′ B′
C′ D′

=

AA′ + BC′ AB′ + BD′
CA′ + DC′ CB′ + DD′

(13.134)

Part IV
Vision in Multiple Directions
Why do you increase your bonds?
Take hold of your life before your light grows
dark and you seek help and do not ﬁnd it.
St. Isaac (died 7th century)

14
Group Direction and N-Folded Symmetry
Direction of a line or a set of lines displaced in a parallel fashion were discussed
in Chap. 10. Such images, characterized by linearly symmetric isocurves, contain a
single direction that is the common direction of the parallel lines. The linearly sym-
metric image model was helpful to explain many local image phenomena, including
edges, lines, simple textures, and a rich variety of corners. In its various extensions,
the single direction model could even be useful for describing motion, simple texture,
and even geometry. Here, we wish to go one step further and increase the number of
directions that can be allowed to be contained in an image. We do this mainly because
the linear model, which assumes a single direction in some coordinate system, is not
sufﬁciently powerful to explain certain image phenomena. Texture patterns consist-
ing of repetitive line constallations having multiple directions, even in the local scale,
constitute an important example.
14.1 Group Direction of Repeating Line Patterns
We describe the concept of group direction by illustrating it through line patterns.
Two linearly symmetric images that differ with π
4 or 5π
4 in their directions are shown
in the ﬁrst column of Fig. 14.1. The relative rotation between the two images can be
obtained from the relative directions between the constituent line directions. Using
the double-angle representation, the direction of the lines can be made unique,
θ = 2ϕ
(14.1)
where ϕ is the normal of a line direction. Accordingly, θ for the images on the top
and on the bottom would be θ = π and θ′ = 3π
2 , respectively. The angle θ of the
content can thus serve as the direction of the image itself, provided that it is known
that the image is linearly symmetric. The relative rotation between the two images
becomes Δθ = θ′ −θ = π
2 , which needs to be divided by 2 to yield Δϕ as π
4 or 5π
4 .
In the second column of Fig. 14.1, the rotated versions of an image containing
two sets of parallel lines are shown. The line sets differ directionwise maximally

312
14 Group Direction and N-Folded Symmetry
Fig. 14.1. Top row illustrates images containing 1, 2, and 3 directions. Bottom row shows the
same, but with different group directions
within the same image, i.e., the constituent line directions differ by π
2 . The amount
of relative rotation between the images is π
4 , 3π
4 , or 5π
4 . Can we deduce the relative
image rotation from the contents? Can we represent the direction of the line constel-
lation in each of the two images separately and uniquely as we did in the linearly
symmetric images? Yes, we can use a “quadruple-angle” representation:
θ = 4ϕ
(14.2)
where ϕ is any of the four possible normal directions, to achieve this. We will call
θ the group direction. The mapping is continuous w.r.t. any rotation and preserves
uniqueness of the rotation in that it continuously maps four different angles to one
and the same angle, which in turn represents an equivalence class of four angles
continuously. Employing the normal directions in the second column of images, we
obtain θ = 0 and θ = π as the group directions for the top and the bottom images,
respectively. While representing the “direction” of the image, the notion of “group
direction” also represents the direction of a constellation of lines that is invariant to
speciﬁc (group) rotations, justifying its name. In the case of linear symmetries, it was
not urgent to “invent” the notion of group direction because images were assumed to
contain only one direction. In this case, it was fairly simple to use this as a reference
direction of the image itself, allowing us to deduce the relative rotation between two
linearly symmetric images.
When there are many directions, the correspondence between the normal direc-
tion angles of individual lines and the rotation angle of a line constellation is more

14.1 Group Direction of Repeating Line Patterns
313
Fig. 14.2. The ﬁrst four images in reading direction are used to construct the last two test
charts, FMTEST2 and FMTEST3

314
14 Group Direction and N-Folded Symmetry
complex. Therefore a speciﬁc notion of group direction θ is needed to differentiate
the image direction from individual line normal directions.
The third column shows a more elaborate line conﬁguration. It consists of three
distinct directions, none of which is more dominant than the other two. The images
are rotated w.r.t. each other with the angle π
8 +l 2π
6 , with l being an integer. In analogy
with the above, we can obtain the group direction, uniquely and continuously deter-
mining the image and the line constellation directions, by hexa-angle representation,
θ = 6ϕ
(14.3)
The group directions of the images at the top and the bottom then become 0 and 6π
8 ,
respectively.
14.2 Test Images by Logarithmic Spirals
In case of single direction occuring in a local image, we previously discussed the
accuracy of direction estimation methods by using a frequency-modulated test chart,
see FMTEST in Fig. 10.15. The idea was to observe the output of a method to local
image inputs having all possible directions at all possible frequencies. Similarly, we
present here two test charts to evaluate methods estimating group directions: one
for group directions of local images containing two maximally distinct directions,
e.g., the second column of Fig. 14.1, and one for the same, but with third maximally
distinct directions, e.g., the 3rd column of Fig. 14.1.
To construct test charts with two distinct directions, we add wave patterns that
are orthogonal to each other, namely the ﬁrst two images of Fig. 14.2 in the reading
direction. Each constituent image is locally a sinusiod with isocurves that are ap-
proximately parallel. All directions and a wide range of frequencies are represented.
When added, the local result contains two wave patterns that are mutually orthogonal
with a unique group direction and granularity (frequency), see FMTEST2 shown as
the ﬁfth image of Fig. 14.2. The group direction and the granularity change indepen-
dently in the angular and in the radial directions, respectively.
Similarly, to construct test charts with three distinct directions we add wave pat-
terns that intersect with the angle π
3 with each other, namely the ﬁrst, the third, and
the fourth images of Fig. 14.2 in reading direction. Each of these spiral1 images
are sinusiod patterns in log-polar coordinates with isocurves that are approximately
parallel, locally. The result contains three sinusoid patterns with maximally distinct
wave front directions, i.e. the spirals intersect one another at π
3 . There is a contin-
uous change in the group direction (angularly) and the granularity (radially), see
FMTEST3 shown as the sixth image in Fig. 14.2.
In the test charts of FMTEST2 and FMTEST3, multiple wave patterns are added.
By contrast, in the test chart of FMTEST there is only one sinusoidal wave pattern,
locally. Although there are two gradient vectors with opposing directions present
in all neighborhoods of FMTEST, there is a unique group direction for each local
1 A circle is a special case of a spiral.

14.3 Group Direction Tensor by Complex Moments
315
Fig. 14.3. The gradient directions (left) in the local images of FMTEST and the corresponding
group directions (right). The two gradient direction types that can occur in local images, e.g.,
the colored region, are drawn at the same point, to avoid clutter
image. This is illustrated in Fig. 14.3, where the gradient directions and the group
directions are overlayed on the original pattern. The gradient and the group direc-
tion vectors are drawn for neighborhoods at the same distance from the center and
represent the various types of directions that can occur in the respective neighbor-
hoods. Whereas the gradient directions are periodic with 2π, the group directions are
periodic with the period π. Per neighborhood, there are two gradient vector types,
differing in their directions with π, whereas there is a unique group direction. Be-
cause the partial derivatives Dx and Dy are linear, the normal vector at a point, the
gradient, is the vectorial summation of the constituent normal vectors. Consequently
there are four and six types of gradient vector directions that can locally occur in
FMTEST2 and FMTEST3, respectively, but the local group directions are unique for
all local images in both test charts. We illustrate this only for the latter, in Fig. 14.4,
where we note that the group directions are unique and periodic with the period π
3 ,
whereas there are six gradient direction types that can occur in a neighborhood, e.g.,
the green region. As one walks around a circle, the local images rotate continuously,
and the group directions reﬂect this change by rotating continuously.
14.3 Group Direction Tensor by Complex Moments
In Chapter 10 we discussed a technique that allows detection of patterns having one
direction, to be precise, linearly symmetric images, while it could quantitate the di-
rection. Studying the FT of images that are linearly symmetric reveals that the power
spectrum is concentrated to a line passing through the spectral origin. Here we will
be interested in the same except that the number of directions that are jointly present

316
14 Group Direction and N-Folded Symmetry
Fig. 14.4. The gradient directions (left) in the local images of FMTEST3 and the correspond-
ing group directions (right). To avoid clutter, the six gradient direction types that can occur in
local images, e.g., the colored region, are drawn at the same point
in the image is higher than one. We will call such images n-folded symmetric, as the
following deﬁnition describes:
Deﬁnition 14.1. Let F be the Fourier transform of an image f. If F is zero except
on a set of lines, and any of these lines can be obtained from another by an integer
multiple of 2π
n rotations, then f is n-folded symmetric.
Because the FT is linear, the n-folded symmetric images can be viewed as the
superposition of linearly symmetric images that differ directionwise maximally. The
isocurves of linearly symmetric images are parallel lines. Accordingly, the deﬁnition
suggests an extension of the linear symmetry concept to high-order symmetries by
isocurve directions of ordinary lines that are jointly present in the image. Note that
this extension is different than the generalized structure tensor discussed in Chap. 11,
which measures the amount of linear symmetry in non-Cartesian coordinates, i.e. it
still represents the presence of a single direction, albeit in a different coordinate
system.
Because |F| is invariant to translations of f, a quantity that represents ﬁtting of
multiple lines to it will also be invariant to translations. Consequently, ﬁtting multiple
lines to |F|2, which is easier to do than to |F|, has useful consequences to image
analysis. If the ﬁtting error is reasonably low, the obtained directions of individual
lines are the gradient directions of the individual isocurves of f. Using the mappings
discussed in Sect. 14.1, it will then be straightforward to assign a meaningful and
unique group direction to f.
We will ﬁrst discuss ﬁtting a cross to F in the TLS sense. The error function
associated with this problem is not as straightforward to formulate as in Chapter 10.
The approach taken here is ﬁrst to map F to another function through a two-to-one
transformation, and then to view it as a line-ﬁtting problem, which we know how to

14.3 Group Direction Tensor by Complex Moments
317
T
T
Fig. 14.5. The two-to-one mapping of the spectrum, Eq. (14.5), creating an equivalence of
directions differing with π
2
solve. After presenting the solution to this problem, we will generalize these results
by means of a theorem which includes the cross-ﬁtting (and the line-ﬁtting) process
as a special case. The proof of the theorem will not explicitly be carried out since it
is a straightforward extension of the method described here. We will show that the
complex moment Ipq of |F|2, with p −q = 4, ﬁts a cross (4-folded symmetry) to the
function |F|2, and thereby to F in the TLS sense.
Assuming that the spectrum is expressed in polar coordinates, F(r, ϕ), the inte-
gral deﬁning the complex moment I40 of the power spectrum yields:
I40 =
 2π
0
 ∞
0
r4 exp(i4ϕ)|F(r, ϕ)|2rdrdϕ
(14.4)
Using the transformation
ϕ1 = 2ϕ
(14.5)
the integral can be transformed once more, so that it can be identiﬁed as I20 of a
remapped spectrum. The latter mapping is a two-to-one mapping, as illustrated in
Fig. 14.5. Consequently, points of the spectrum that have angular coordinates dif-
fering by π
2 are treated as equivalent, i.e. the spectral power |F|2 originating from
such points is added, to produce the folded spectral power |F1|2. The complex mo-
ments I20, I11 of |F1|2 are such that they equal I40, I22 of |F|2, respectively. Using
theorem 10.2, one can then conclude that I40, I22 ﬁt a cross to the power spectrum
in the TLS sense. The pairs I4,0, I2,2 also constitute a tensor because they together
represent the structure tensor of a (remapped) power spectrum. The cross-ﬁtting pro-
cess, a detailed discussion of which is found in [26], leads naturally to the following
generalization:
Theorem 14.1 (Group direction tensor I). A pair of complex moments In,0 and
I n
2 , n
2 of |F|2, with n ̸= 0, determines an optimal ﬁt of a set of lines possessing
n-folded symmetry to a function F in the TLS sense. The real quantities |In0|, I n
2 , n
2
depend on the minimum and the maximum errors of the ﬁt, e(kn
max), e(kn
min), respec-
tively,
|In0| = e(kn
max) −e(kn
min)
(14.6)
I n
2 , n
2 = e(kn
max) + e(kn
min)
(14.7)

318
14 Group Direction and N-Folded Symmetry
where kn
min, kn
max are complex representations of the extremal group directions,
θmin, θmax,
kn
min =
In,0
&&I n
2
n
2
&& = exp(iθmin),
and
kn
max = −kn
min.
(14.8)
♦
The quantities I n
2 , n
2 , In,0 represent a tensor because they measure a physical prop-
erty, unbiased by the observation coordinate system. For odd n, the complex mo-
ments In,0 vanish when f is real. This is because the power spectrum of real images
is even. Accordingly, to be useful, n must be even when f is real.
14.4 Group Direction and the Power Spectrum
In the theorem discussed above, F was the spectrum of an image without being spe-
ciﬁc about whether it is local or global, as it holds for both interpretations. However,
the case when f is a local image is more interesting for texture analysis applications,
therefore we assume this henceforth. The local image can be readily obtained by
multiplying the original image with a window concentrated to a small support. Ac-
cordingly, |F|2 will be the local power spectrum. This yields a useful interpretation
in texture analysis, because the local power spectrum measurements, e.g., In,0, will
then reﬂect the properties of the local texture, which should not change as long as
one moves the window within the same texture.
The view supported by psychology experiments suggests that [131] the gray
value of a local image in comparison with that of another point in the same neigh-
borhood is more signiﬁcant than the absolute gray values for human texture discrim-
ination. This is sometimes called the second-order statistics because the gray image
correlations of two points are measured. Some studies limit the deﬁnition of texture to
images consisting of regions having the same second-order statistics, which depends
on the distance and the direction between two points [81,96]. However, more gener-
ally a texture can be deﬁned as an image consisting of translation-invariant local im-
age properties. The local power spectrum, which is translation-invariant, is therefore
rich in texture features. Within homogeneous regions, local phase information which
allows us to discriminate between textures consisting of differently shaped lines and
edges is neglected, e.g. [57]. In other words, only the directional information of the
geometrical structures are taken into account, irrespective of whether, for example,
the component lines are caused by crests or valleys. Therefore, when there is a tex-
ture with one distinct orientation (linear or 2-folded symmetry) around an inspected
point, the power spectrum will be concentrated to a line. When there is a texture with
two mutually orthogonal directions (rectangular or 4-folded symmetry) the power
spectrum will be concentrated to a cross. It is similar for hexagonal/triangular struc-
tures (6-folded symmetry) and octagonal ones (8-folded symmetry). The arguments
of the complex moments give the orientation of the estimated n-folded symmetry,
whereas the magnitudes give measures of the estimation quality, that is, certainties.

14.4 Group Direction and the Power Spectrum
319
At this point we note that if the complex moments I40 and I60 of a local power
spectrum with an energy concentration having only 2-folded symmetry are com-
puted, the magnitude responses will be high. However, these magnitude responses
will be lower relative to the cases in which the spectrum shows 4- and 6-folded sym-
metries because 2 is a factor of 4 and 6; hence, such a concentration “leaks” to 4-
and 6-folded symmetries. The converse is not true, which allows complex moments
to discriminate between these symmetries. In the ideal case, a 6-folded symmetric en-
ergy concentration gives a zero response for complex moments with p −q = 4 and
2. Likewise, a 4-folded symmetric concentration yields a zero response for p−q = 2
and 6.
One might think that the arguments of the complex moments which represent
k2, k4, . . . according to theorem 14.1 need to be divided by 2, 4, etc., for a straight-
forward representation of the directions of the component lines or edges. Likewise,
the moduli of these moments do not provide for the minimum error directly. Al-
though it is simple to compute this by solving the minimum error in Eqs. (14.6)
and (14.7), we will argue that for pattern recognition purposes, the complex number
representation of Ipq has advantages that allow us to circumvent the following three
problems.
First, the continuity of the In,0 w.r.t. group directions is possible to achieve in the
sense that two such directions that differ a small amount also differ a small amount in
the numerical representations afforded by the real and imaginary parts of In,0. That
is, a number arbitrarily close to 0 is not arbitrarily close to 2π, whereas the corre-
sponding physical angles (as well as In,0) are [90]. In the Cartesian representation,
the complex numbers are continuous with respect to changes in their arguments, ex-
cept at the origin. Second, the factor n makes the representation of the direction of
the symmetry unique. This is because this factor makes the argument of In,0 a group
direction, as discussed in Sect. 14.1. Eliminating the factor n in the argument would
thus discard the equivalence of certain rotation angles which are the same because
they have exactly the same physical effect on n-folded symmetric images. Third,
knowing the value of e(kn
min) alone is not sufﬁcient to judge the quality of the esti-
mate; one must know whether this error is large or small (the range problem). The
comparison with the worst case, i.e., e(kn
max) −e(kn
min), provides a means to assess
the quality. The complex quantity I20 already represents the difference of the er-
rors through its magnitude, which is real. Alternative quality measures can be found
easily, e.g.,
e(kn
max) −e(kn
min)
e(knmax) + e(kn
min) =
&&&&
In,0
n
2 , n
2
&&&&
(14.9)
which is always in the interval [0,1] and attains the end points 0 and 1 if the quality
of the ﬁt is totally uncertain and totally certain, respectively.
The continuity, group direction representation, and range problems might easily
imperil the performance of e.g., a clustering method that could follow the extraction
of orientation and certainty features, if an adequate representation of these features
is lacking. By using complex moments in the Cartesian representation one can avoid
these three problems.

320
14 Group Direction and N-Folded Symmetry
14.5 Discrete Group Direction Tensor by Tensor Sampling
Theorem 14.1 suggests computing complex moments of the power spectrum |F|2 to
estimate the group direction. These computations can also be carried out in the spatial
domain by direct tensor sampling, in analogy with the discussion in Sect. 10.11. We
can write the integral of the complex moment In,0 as follows, if we assume that the
spectrum is represented in its Cartesian coordinates, F(ωx, ωy), and n is nonzero
and even:
In,0 =

E2
(ωx + iωy)n(ωx −iωy)0|F(ωx, ωy)|2dωxdωy
=

E2
[(ωx −iωy)
n
2 F]∗[(ωx + iωy)
n
2 F(ωx, ωy)]dωxdωy
(14.10)
By use of theorem 7.2, due to Parseval–Plancherel, the complex moments integral
can be computed in the spatial domain:
In,0 = ( 1
2π )2

E2
[(Dx −iDy)
n
2 f(x, y)]∗[(Dx + iDy)
n
2 f(x, y)]dxdy,
= ( 1
2π )2

E2
[(Dx + iDy)
n
2 f(x, y)]2dxdy,
(14.11)
where f is the inverse FT of F. Further, we can identify (Dx+iDy)
n
2 f as a symmetry
derivative of f, introduced in Section 11.9. Then, assuming that f is band-limited,
there exists an interpolation function ( μ1 below) by which we can reconstruct f via
its discrete samples
f(r) =

j
fjμ1(r −rj)
(14.12)
where r = (x, y)T and rj = (xj, yj)T . However, the function (Dx + iDy)
n
2 f is
band-limited too, and can be reconstructed from the samples of f:
(Dx + iDy)
n
2 f(r) =

j
fj(Dx + iDy)
n
2 μ1(r −rj)
(14.13)
by linear ﬁltering. Evidently, this function can also be sampled without loss of infor-
mation on the same grid as the samples fj are deﬁned, so that
(Dx + iDy)
n
2 f(rk) =

j
fj(Dx + iDy)
n
2 μ1(rk −rj)
(14.14)
Assuming that μ1 is a Gaussian with the variance σ2
p, the discrete symmetry deriva-
tives of f can be computed by an ordinary discrete linear ﬁltering with the ﬁlter
Γ { n
2 ,σ2
p}(rk −rj) = (Dx + iDy)
n
2
1
2πσ2p
exp
∥rk −rj∥2
2σ2p

(14.15)

14.5 Discrete Group Direction Tensor by Tensor Sampling
321
Ordinarily, one uses the same grid as the fj to compute its discrete symmetry deriva-
tives, so that the discrete symmetry derivative of f is obtained by convolving the
discrete fk with the following ﬁlter, theorem 11.2.
Γ { n
2 ,σ2
p}(rj) = (Dx + iDy)
n
2
1
2πσ2p
exp(∥rj∥2
2σ2p
)
= (−1
σ2p
)
n
2
1
2πσ2p
(x + iy)
n
2 exp(∥rj∥2
2σ2p
)
(14.16)
In analogy with Chapter 10, where we computed the elements of the structure
tensor (the group direction tensor for 2-folded symmetry), we conclude that even the
group direction tensor [(Dx + iDy)
n
2 f]2 is band-limited because (Dx + iDy)
n
2 f is
band-limited. Accordingly, there is an interpolation function μ2 that can reconstruct
the former from its discrete elements as
[(Dx + iDy)
n
2 f]2(r) =

j
[(Dx + iDy)
n
2 f]2(rj)μ2(r −rj)
(14.17)
Then, assuming a Gaussian as an interpolator, an element of the group direction
tensor for n-folded symmetry:
In,0 =

E2

j
[(Dx + iDy)
n
2 f]2(rj)μ2(r −rj)dxdy
=

j
[(Dx + iDy)
n
2 f]2(rj)

E2
μ2(r −rj)dxdy
=

j
[(Dx + iDy)
n
2 f]2(rj)μ2(rj)dxdy
(14.18)
is obtained by an ordinary Gaussian smoothing of the (pixelwise) square of the com-
plex image delivered by the computation scheme given in Eq. (14.14).
Following a similar reasoning, we can obtain the remaining tensor element, I n
2 , n
2 ,
which is always real and nonnegative. We summarize our ﬁndings as a theorem.
Theorem 14.2 (Group direction tensor II). The complex group direction number,
kn
min = exp(iθmin), associated with an n-folded symmetric line set ﬁtted to the
power spectrum in the TLS error sense and the extremal TLS errors of the ﬁt are
given by
In,0 = (e(kn
max) −e(kn
min)) expiθmin =
1
4π2

[(Dx + iDy)
n
2 f(r)]2dr
= ⟨[Γ { n
2 ,σ2
p} ∗f]2⟩
(14.19)
I n
2 , n
2 = (e(kn
max) + e(kn
min)) =
1
4π2

|(Dx + iDy)
n
2 f(r)|2dr
= ⟨|Γ { n
2 ,σ2
p} ∗f|2⟩
(14.20)
where Γ { n
2 ,σ2
p} is deﬁned according to (11.97).
♦

322
14 Group Direction and N-Folded Symmetry
Fig. 14.6. The test image FMTEST2 and I40 computed for all local images. The intensity and
the hue are modulated by the magnitude and the argument of I40
The structure tensor in theorems 10.2, and 10.4 can be obtained as special cases
of this theorem with n = 2. Furthermore, we ended up with the same basic computa-
tional scheme consisting of three consecutive operations—two linear-ﬁltering steps,
linked by an intermediary, nonlinear-mapping step:
First Linear-Filtering:
Apply a symmetry derivative ﬁltering,
Nonlinear-Mapping:
Apply a pointwise squaring,
Second Linear-Filtering: Apply a symmetry derivative ﬁltering.
The symmetry derivative ﬁlter in the ﬁrst step, (Dx + iDy)
n
2 , is more elaborate
as compared to Dx + iDy, which is used in linear symmetry detection. The last step
uses the zero-order symmetry derivative ﬁlter in both cases, average-ﬁltering.
Surprisingly, even the GST scheme, developed to detect nontexture patterns, e.g.,
parabolic patterns, log-spirals, crosses, etc., see lemma 11.6, shares the same compu-
tational steps as above. Additionally, the ﬁlters used in the ﬁrst step here are similar
to those derived for the last step of the GST scheme. By contrast, in the ﬁrst step of
the GST, the simple, ﬁrst-order symmetry derivative ﬁlter, Dx + iDy, is mandatory
whereas more elaborate ﬁlters are reserved for the last step. As will be discussed
below, and in Section 14.6, to allow high-order symmetries at the ﬁrst step ensures
that multiple directions can be seen as equivalent by the detection process. This will
in turn provide translation-invariant texture features.
It is worth noting that linearly symmetric patterns can conceptually belong to
both object-, and texture-type patterns. In isolation, they can on one hand be viewed
as being harmonic monomial objects with the trivial harmonic mapping (the identity
transformation in GST) whereas on the other hand, they are the simplest textures
containing line patterns, those having a single direction when repeated.
To visualise the effect of computing complex moments in the local power spec-
trum, we show I40 applied to FMTEST2 in Fig. 14.6. The hue and intensity represent

14.5 Discrete Group Direction Tensor by Tensor Sampling
323
Fig. 14.7. The test image FMTEST3 and I60 computed for all local images. In the color image
the intensity and the hue represent the magnitude and the argument of I60, respectively
the group direction θ and the certainty of the group direction, respectively. They are
obtained from the argument and the magnitude of I40 at each local image via the
three-step complex ﬁltering suggested by theorem 14.2. In the convolution of the
ﬁrst step, delivering symmetry derivatives of local images, the ﬁlter Γ{2, 0.64} was
used. After squaring, the image was ﬁltered through Γ{0, 5.76}, which is an ordinary
Gaussian with σ = 2.4 that deﬁnes the extent of a local image. The orginal image
is preprocessed by a bandpass ﬁlter to facilitate the specialization of the scheme to
a certain frequency range only. This means that the scheme with the above ﬁlters
is specialized to detect joint occurence of two directions (4-folded symmetric im-
ages) at a certain range around a tune-on frequency, which in the result is observed
as nonblack. In the ﬁgure the original is placed beside the result to facilitate visual
identiﬁcation of the effective frequency range of the scheme. We also observe that the
hue of the colors change continuously and uniformly in the angular direction with
the period π
2 . This is manifested by the fact that the same color appears regularly
four times circularly. Accordingly, the group direction has been extracted faithfully
by I40. A detailed discussion on this can be found in [134].
A similar processing of FMTEST3 to obtain I60 yields Fig. 14.7. The hue and
intensity represent the group direction θ and the certainty as before. The convolu-
tion of the ﬁrst step utilizes the complex ﬁlter Γ{3, 0.64}, whereas the last step uses
the same Gaussian ﬁlter as above, Γ{0, 5.76}. The orginal image is subjected to a
bandpass-ﬁltered prior I60 computation to allow the frequency range specialization
of the scheme to a certain frequency range only. The ﬁltering scheme with the above
ﬁlters detects joint occurrence of three directions (6-folded symmetry). As evidence
of accuracy of the group direction computation, the hue of the colors changes con-
tinuously and uniformly in the angular direction with the period π
3 . The latter is
observed by noting that the colors repeat regularly six times circularly.

324
14 Group Direction and N-Folded Symmetry
Fig. 14.8. The P1 texture patch composition consisting of real aerial images (left) and its
automatic segmentation using group direction tensors for 2, 4, and 6-folded symmetries
14.6 Group Direction Tensors as Texture Features
In the previous sections, we discussed group direction features that were shown to
be invariant to translation. This is a property one wishes to have in texture features
because textures are precisely deﬁned as images in which the same geometric struc-
ture repeats itself. The texture properties measured around any point should not vary
as long as the point is within the same texture. How powerful the group direction
features are and how they can be used as texture features will be studied in further
detail next.
The group direction of a texture for n-folded symmetry is determined by the two
complex moments,
In,0,
and
I n
2 , n
2 ,
(14.21)
of the power spectrum. This s pair represents three real scalars, the ﬁrst being a com-
plex scalar (two real scalars), the second being a (nonnegative) real scalar. They can
be applied to the original image directly or to a bandpass-ﬁltered version of the orig-
inal, forcing the quantities to be measured for only a certain frequency range (scale)
of the image. Assuming bandpass ﬁltering, the result of which is a number of im-
ages that differ in their absolute frequency contents, then one can extract the group
direction tensor elements for every image, increasing the description power. Band-
pass decompositions can be efﬁciently implemented by a variety of ways, e.g., the
Laplace pyramid discussed in Sect. 9.5. Therefore the two complex moments pro-
vide evidence for the joint presence of n
2 directions contained in a narrow frequency
ring and within a local support around a point in an image. We discuss the practical
issues by way of two examples below.
Example 14.1. In Fig. 14.8 we show the P1 texture patch that has to be segmented
based on local image features. The texture patches in the composition are cut from

14.6 Group Direction Tensors as Texture Features
325
aerial images. Before being put together they are also mean and variance normalized
photometrically in that all patches have the same mean and the same variance. This is
done to attempt that segmentation be done on elaborate texture measurements rather
than based on simple (nontexture) features such as gray level and variance, which
can be inﬂuenced by illumination. Since the boundaries are known and the patches
are photometrically normalized, an advantage of using such images is that one can
evaluate and compare the results with other segmentation techniques by sharing the
test image and its results only. The ground truth of the boundaries is shown in Fig.
16.8
The segmentation is to be done by some grouping, in unsupervised manner. How
to do this grouping and estimating of region boundaries (without training a neu-
ral network or a classiﬁer) is suggested in Chapter 16. To stay within the scope of
this chapter, however, we only study the texture features. First, ﬁve fully circular
(isotropic) subbands are obtained by convolution. The (absolute) frequency centers
of the subbands are in (octavelike) geometric progression with the factor 1.2 be-
tween successive subbands, see Fig. 9.14. The following real measurement for each
subband has been computed:
I11
I20
I40
I60
Total
#Real scalars
1
2
2
2
7
so that effectively 35 measurements per image point are available. The elements
I22, I33 are excluded because the respective measurements are well approximated
through I11 when the input image is a subband that contains only a narrow ring of
frequency range. In the latter case, I n
2 , n
2 for all n, estimate the variance within a nar-
row ring of frequencies. The mentioned grouping followed by a boundary estimation
using the above features have been applied. The result is shown as a color image
in the Fig. 14.8 where color represents the identity label of the found textures. Ac-
cordingly, the same color represents the same texture. All seven texture patches are
correctly identiﬁed, and the boundaries are reasonably well drawn without training.
Example 14.2. In Fig. 14.9 we show a more difﬁcult texture patch composition con-
sisting of seven different textures in a 4×4 arrangement as before. Again, the individ-
ual patches are photometrically normalized, and the same texture measures detailed
above have been utilized in the automatic grouping and boundary estimation process.
The seven texture patches are correctly identiﬁed, and the boundaries are reasonably
well drawn, although the boundary quality is somewhat worse.
Exercise 14.1. We have studied only In,0, I n
2 , n
2 above. What do other complex mo-
ments of the power spectrum with p ≥q, e.g., I4,2, I5,1, can you estimate? Does Iqp
contain new information compared to Ipq?
HINT: (ωx + iωy)p(ωx −iωy)q = (ωx + iωy)p−q|ω|2q.

326
14 Group Direction and N-Folded Symmetry
Fig. 14.9. The P2 texture patch composition consisting of real aerial images (left) and its
automatic segmentation using group direction tensors for 2, 4, and 6-folded symmetries.
14.7 Further Reading
Texture as a vision problem has been covered by a large body of publications in
psychology. Texture recognition in its various forms appears in many applications,
e.g., as diagnostic tools, multi-spectral image segmentation, and tracking in image se-
quences [78,82,233]. The prevailing view is that the second order statistics of images
is the only property that humans can discriminate in textures. The study in [178] sug-
gests certain Lie operators, which are differential operators, to deﬁne textures. While
the complex moments provide for optimal solutions to the line-ﬁtting, cross-ﬁtting,
etc., problems in the power spectrum, they correspond precisely to Lie operators that
can identify the direction(s) along which the image is translation invariant. As has
been discussed here, these complex moments can be computed by means of symme-
try derivatives applied to subbands of the original in the spatial domain. The scheme
was possible to implement via separable ﬁltering thanks to direct tensor sampling
and the theoretical results studied in Sect. 11.9. However, the group direction tensor
can also be obtained by spectrum sampling in analogy with Sect. 10.13. The theo-
retical details of this approach to implement the group direction tensor are studied
in [26], whereas experimental results can be found in [25]. Even if a texture contains
symmetries of high order when no subband decomposition is applied, it can still be
discriminated against another texture by use of only structure tensor features applied
at the subbands level [134], e.g., the P3 patch shown in Fig. 16.8. By contrast, some
textures cannot be discriminated by applying the structure tensor to subbands, but
need the descriptive power of higher order symmetry features, e.g., Figs. 14.9 and
16.8. Accordingly, as the number of textures involved in the discrimination increases,
and/or the complexity of the individual textures increases, the structure tensor fea-
tures need to be completed. This can be done by the group direction tensors.

Part V
Grouping, Segmentation, and
Region Description
Who has ever seen, that a breach became as a mirror?
Two parties looked thereinto;
it served for those without and those within.
They saw therein as with eyes,
the Power that breaks down and builds up:
They saw Him who made the breach
and again repaired it.
Those without saw His might;
they departed and tarried not till evening:
those within saw His help;
they gave thanks yet sufﬁced not.
The Nisibene hymns,
St. Ephrem (A.D. 303–373)

15
Reducing the Dimension of Features
For many applications, a dimensionality reduction results in improved signal sepa-
ration in the presence of noise. We will discuss the underlying concept in the prin-
cipal components analysis section below. However, dimension reduction is a general
problem that has been the subject of intensive study in different disciplines. While
principal components analysis is the earliest and simplest technique, and is widely
used for its efﬁciency, there are other approaches, such as independent component
analysis, neural networks, and self-organizing maps [29,117,142], that have proven
to be more powerful in many applications. For reason of limited scope, we must,
however, restrict our discussion to principal components analysis. An example of its
use in face recognition is discussed in Sect. 15.2, whereas another example for which
reduced dimension is a prerequisite, texture analysis, will be illustrated in Sect. 16.7
when discussing clustering and boundary estimation in textures.
15.1 Principal Component Analysis (PCA)
There are many names to principal component analysis (PCA), as it has been used in
numerous disciplines and applications. Examples include color representation, tex-
ture segmentation, multispectral image classiﬁcation, face recognition, source sepa-
ration, visualization, and image database queries [100,172,179,202,218,220]. Syn-
onyms of PCA include Karhunen–Lo´eve (KL) transform, [133], Hotelling transform,
eigenvalue analysis, eigenvector decomposition, and spectral decomposition. In im-
age analysis it is used to reduce dimensions, and to ﬁnd subspaces in which recog-
nition works better than taking the full space. Not only does PCA reduce the size of
the data for the sake of efﬁcient storage, transmission, and processing advantages.
Assume that we have observed a set of K vectors
O = {f k}
(15.1)
i.e., the observation set, in an M-dimensional vector space.
The coordinates of the observation set can be represented by M scalars in some
basis

330
15 Reducing the Dimension of Features
BM = {ψ1, ψ2, · · · , ψM}
(15.2)
yet to be speciﬁed, as
f 1=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
f 1(1)
f 1(2)
f 1(3)
...
f 1(M)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, f 2=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
f 2(1)
f 2(2)
f 2(3)
...
f 2(M)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, · · · , f k=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
f k(1)
f k(2)
f k(3)
...
f k(M)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, · · · , f K=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
f K(1)
f K(2)
f K(3)
...
f K(M)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(15.3)
where f k(m) is the mth component of the vector f k. Each vector f k can then be
written as
f k =
M

m=1
f k(m)ψm
(15.4)
By using all M basis vectors, we can thus represent any of the observed f k without
error. This remains true even if we choose another basis set containing M orthogonal
vectors, as long as we include all M basis vectors in the expansion in Eq. (15.4).
Does the basis we choose really matter? Yes, it does, because in applications we
cannot always afford to choose complete bases of M vectors for a variety of reasons,
including that M can be too large. One must then expand each of the observed f k by
using fewer vectors:
˜f k =
N

m=1
f k(m)ψm,
where
N < M
(15.5)
Note that the only difference between Eq. (15.5) and Eq. (15.4) is in the number of
the terms in the summation, N and M, respectively. All terms in Eq. (15.5) exist in
Eq. (15.4), but not vice versa. The vectors
˜f 1,˜f 2 · · · ,˜f k, · · · ,˜fK
(15.6)
only approximate the corresponding observation, because the approximation error
∥f k −˜f k∥
(15.7)
is usually not zero.
Here, we are interested in ﬁnding an orthonormal (ON) basis, BN:
BN = {ψ1, · · · , ψN},
with
⟨ψi, ψj⟩= δij,
(15.8)
that is “most economical” among all possible ON basis sets. Note that BN is a “trun-
cated” BM in that it has fewer basis vectors. Economical means that, despite the fact
that the basis BN has fewer basis vectors than the full set, it should still represent O
with a smaller basis truncation error,
1
K
K

k
∥f k −˜f k∥2
(15.9)

15.1 Principal Component Analysis (PCA)
331
than all alternative bases. This should be true on equal footing, i.e., when no more
than N basis vectors are used in each of the expansions ˜f k. Accordingly, we are
interested in minimizing the error, by varying BN for the same observation set O:
K

k
∥f k −˜f k∥2 =
K

k
⟨f k −˜f k, f k −˜f k⟩
=
K

k
⟨f k, f k⟩+ ⟨˜f k,˜f k⟩−2⟨f k,˜f k⟩
=
K

k
∥f k∥2 + ∥˜f k∥2 −2⟨f k,˜f k⟩
(15.10)
By substituting Eqs. (15.4) and (15.5) in the last term of this equation, we then obtain:
K

k
∥f k −˜f k∥2 =

k

∥f k∥2 + ∥˜f k∥2 −2⟨
M

m=1
f k(m)ψm,
N

m=1
f k(m)ψm⟩

= T +

k

∥˜f k∥2 −2⟨
N

m=1
f k(m)ψm +
M

m=N+1
f k(m)ψm,
N

m=1
f k(m)ψm⟩

= T +

k

∥˜f k∥2 −2⟨˜f k,˜f k⟩−2⟨
M

m=N+1
f k(m)ψm,
N

m=1
f k(m)ψm⟩

(15.11)
where
T =

k
∥f k∥2
(15.12)
is constant w.r.t. the changes of BM because the length of f k is the same (a zero-
order tensor) in every basis. In Eq. (15.11), the ﬁrst scalar product term ⟨˜f k,˜f k⟩, can
be identiﬁed as ∥˜f k∥2, whereas the second scalar product term vanishes because of
orthogonality of the involved ψm, to yield
1
K
K

k
∥f k −˜f k∥2 = T
K −1
K
K

k
∥˜f k∥2
(15.13)
Since T is a constant, minimizing this expression is equivalent to maximizing
K

k
∥˜f k∥2 =
K

k=1
⟨˜f k,˜f k⟩=
K

k=1
N

m=1
|f k(m)|2
=
K

k=1
N

m=0
|⟨ψm, f k⟩|2 =
K

k=1
N

m=0
⟨ψm, f k⟩⟨f k, ψm⟩
(15.14)
The scalar product of the vector space, the Euclidean EM, to which both the vectors
of O and BM belong, is given by ⟨ψm, f m⟩= ψT
mf k, so that the highest bound of

332
15 Reducing the Dimension of Features
K

k
∥˜f k∥2 =
K

k=1
N

m=0
⟨ψm, f k⟩⟨f k, ψm⟩=
K

k=1
N

m=0
ψT
mf kf T
k ψm
=
N

m=0
ψT
m
 K

k=1
f kf T
k

ψm =K
N

m=0
ψT
mSψm ≤K
N

m=1
λ(m) (15.15)
is reached when the ψm are the N most signiﬁcant1 eigenvectors of
S = 1
K
K

k=1
f kf T
k
(15.16)
which is the scatter matrix for the observation vectors O. This is because S is, by
construction, a positive semi-deﬁnite matrix, meaning that 0 ≤gT Sg for all g ∈
EM, including for g = ψm. Accordingly, substituting Eq. (15.15) in Eq. (15.13),
yields
1
K
K

k
∥f k −˜f k∥2 = T
K −1
K
K

k
∥˜f k∥2 ≥T
K −
N

m=1
λ(m) =
M

m=N+1
λ(m) (15.17)
and one can similarly conclude that the lowest bound of the approximation error is
reached if BN is chosen as the N most signiﬁcant eigenvectors of S. Here, we have
used
T
K = 1
K
K

k=1
∥f k∥2 = λ(1) + · · · + λ(M),
(15.18)
which can be obtained by remembering that the approximation is error-free, i.e.
˜f k = f k, if one uses all basis vectors, i.e. N = M, in Eqs. (15.15) and (15.12).
Expression (15.17) tells that if we decide to use N eigenvectors (and not the full set
of M) then the lowest possible error of approximation is the sum of the N −M
least signiﬁcant eigenvalues of S, and that this error is achievable if one chooses the
N most signiﬁcant eigenvectors of S. An estimation of S can be obtained from the
observed vectors.
For computational convenience, the scatter matrix computation can be achieved
as a pure matrix multiplication, since
S = 1
K

k
f kf T
k = 1
K [f 1, · · · , fK]
⎡
⎢⎣
f T
1
...
f T
K
⎤
⎥⎦= 1
K OOT
(15.19)
Because S is symmetric positive semideﬁnite, its eigenvectors are orthogonal, guar-
anteeing the optimal basis to be orthogonal. Once the new basis is found, the obser-
vation data can be projected onto the basis vectors as
˜O
T = OT BN
(15.20)
1 The notation λ(m) represents sorted eigenvalues, λ(M) ≤· · · ≤λ(1) .

15.1 Principal Component Analysis (PCA)
333
where
˜O = [˜f 1, · · · ,˜f N],
O = [f 1, · · · , f K],
BN = [ψ1, · · · , ψN]
(15.21)
are the juxtaposed vector coordinates corresponding to the data, a subset of the basis,
and the new data, respectively. The resulting data vectors have N coordinates for ˜f k,
down from the original M for f k, and yet achieve an approximation of the original
data with the minimal error of Eq. (15.9). Since BM has full rank with orthogonal
columns, using the full set in Eq. (15.20) would rotate the original coordinates. Con-
sequently, Eq. (15.20) is a truncated version of the rotated coordinates, where the
corresponding basis vectors are numbered according to their ability to represent the
observed data.
Before the rotation, it is often wise to translate the data to the mass center, es-
pecially in equally quantized feature spaces (e.g., images are usually quantized uni-
formly to yield 256 gray values), to avoid numerical problems stemming from the
limited dynamic range. Data translation can also be preferable from a pattern dis-
crimination viewpoint too, because the basis rotated at the mass center to yield the
minimal error of representation is not the same as a basis attached elsewhere and
rotated there. The rotation at the mass center will rank the directions according to the
maximal distance of the data to the mass center and the directional variation of the
data (which we discuss at the end of this section). If two classes are to be discrimi-
nated in a certain direction in the new basis, it is better that there is a great variation
in that direction than no variation, since the latter suggests that the classes leave the
same footprint, and hence cannot be separated in that particular direction.
Translating the observed vectors to the centroid:
f k ←(f k −f c),
where
f c = 1
K
K

k=1
f k,
(15.22)
and searching for a new basis minimizing the approximation error
1
K
K

k=1
∥f k −˜f k∥2
(15.23)
among the bases attached to the mass center can be conveniently achieved by pre-
processing. The scatter matrix obtained after shifting the data to the mass center is
also known as the covariance matrix. Alternatively, the covariance matrix can be
computed as
C = 1
K
K

k=1
(f k −f c)(f k −f c)T
(15.24)
where f k is the unshifted observation data. The eigenvectors of C are frequently
called the principal components, whereas the rotated coordinates ˜O, relative to the
mass center, are called the Karhunen–Lo´eve (KL) transform.
We could use the same arguments as above to ﬁnd analogous results for the di-
mension reduction of the complex vector space, CM, allowing us to formulate the
following lemma, which summarizes the conclusions so far.

334
15 Reducing the Dimension of Features
Lemma 15.1. Let O = [f 1, · · · , f K], where f k is a vector in CM. The ON basis
BN = [ψ1, · · · , ψN] that minimizes
1
K
K

k
∥f k −˜f k∥2,
with
˜f k =
N

m
f k(m)ψm,
(15.25)
for every integer N : N < M, is given by the ﬁrst N eigenvectors of the scatter
matrix:
S = 1
K
K

k=1
f kf H
k = 1
K OOH
(15.26)
where the eigenvalues are sorted as λ(1) ≥· · · ≥λ(M). The new coordinates are
given by
˜O
T = OT BN,
with
˜O = [˜f 1, · · · ,˜f N].
(15.27)
♦
Now we consider the following problem. Assuming that the data has its mass
center in the origin, we wish to search for N = M −1 basis vectors to approximate
the observations O in the TLS sense. The problem can be solved by application of the
lemma, evidently. Alternatively, we can conceive it as a hyperplane-ﬁtting problem:
f T ψ = 0
(15.28)
where ψ is the normal of an unknown (M −1)-dimensional hyperplane passing
through the origin in Em. Given the observations O = [f 1, · · · , f K], which contain
noise, the equation will not be satisﬁed exactly but can be solved in the TLS sense,
by searching for a ψ minimizing
1
K ∥OT ψ∥2 = 1
K ψT OOT ψ,
where
∥ψ∥= 1,
(15.29)
which is solved by the least signiﬁcant eigenvector of OOT . When the data are
projected onto this plane by
f −⟨ψ, f⟩ψ
(15.30)
we have thus reduced its dimension by 1. The error in the hyperplane approximation
is λM, the least eigenvalue of S =
1
K OOT . Naturally, one could ﬁt planes recur-
sively to eliminate more and more dimensions until reaching any desired dimension
N. Accordingly, we conclude that although conceptually different, the dimension
reduction and the direction estimation are equivalent mathematically, because both
are solved by an eigen-analysis of the same scatter matrix. We summarize this as a
lemma.
Lemma 15.2 (Direction and PCA). A solution ψ of a homogeneous equation
OT ψ = 0,
(15.31)

15.2 PCA for Rare Observations in Large Dimensions
335
where ∥ψ∥= 1 and O is an M × K matrix, that minimizes the TLS error ∥OT ψ∥2
is given by the least signiﬁcant eigenvector of OOT . The solution coincides with
the normal direction of the hyperplane having the least orthogonal distance to the
points given by the columns of O. If the multiplicity of the least eigenvalue is ν, the
dimension of the ﬁtted hyperplane is M −ν.
♦
15.2 PCA for Rare Observations in Large Dimensions
If the dimension of the observation vector space is very large, it is generally very
likely that S will be singular. The scatter matrix is guaranteed to be singular when
the number of observed vectors is less than the dimensions, K < M. This happens
often in image analysis.
A well-known example of large dimensionality in features is obtained when the
observed feature vectors consist of entire images, e.g., face images that we will
also illustrate in an example below. The image then becomes an observation vec-
tor by scanning it in a certain fashion, for example in the reading direction. For a
1000 × 1000 image, M is equal to 1 million, for a 100 × 100 image the dimension-
ality is 10000, etc. To obtain a reliable estimate of the scatter matrix one has to collect
tens of thousands of observations even for small, low-resolution images, whereas for
high-resolution images this is simply not practicable. The result is that one has to
use the available observations, despite the fact that they are almost never sufﬁcient,
to yield a good estimate of the scatter matrix. Another difﬁculty when dealing with
large-dimensional PCA is that building the scatter matrix grows from being a trivial
task in few dimensions, to an impractical task in large dimensions. This is because
every outer (tensor) product in the sum of Eq. (15.26) needs M(M + 1)/2 multipli-
cations, and there are K such matrices in the summation. In the ideal case, K should
furthermore be at least as many as the scatter matrix coefﬁcients, M(M + 1)/2. It
follows then that the number of multiplications is KM(M +1)/2, which approaches
O(M 4).
In this section we will discuss how to obtain the KL coefﬁcients when K ≪M
without performing large numbers of arithmetic operations. The symmetric matrix
OOT , and thereby S, has the size M×M, whereas the matrix OT O, also symmetric,
has the size K×K. Although different, these two matrices share eigenvalues because
OOT ψm = λmψm,
⇒
OT O(OT ψm) = λm(OT ψm),
(15.32)
where λm > 0. Also, inspecting (15.32), we see that if ψm is an eigenvector of
OOT , then
ψ′
m = OT ψm
(15.33)
is an eigenvector of OT O. Thus the two matrices have eigenvectors that differ only
by a ﬁxed projection. Multiplying this equation with O
Oψ′
m = OOT ψm = λmψm
(15.34)

336
15 Reducing the Dimension of Features
shows that the eigenvectors of OOT are related to those of OT O through a pro-
jection. Accordingly, given O = [f 1, · · · , f K], with f k ∈EM, the eigenvectors of
OOT can be obtained from those of OT O as follows:
1. Compute the sorted eigenvalues and eigenvectors of OT O as λm, ψ′
m. The
eigenvalues are also the eigenvalues of OOT .
2. Obtain the (unnormalized) eigenvectors ψm as Oψ′, (15.34).
Exercise 15.1. How is it that the M × M matrix OOT has M eigenvalues that are
the same as the K eigenvalues of the K × K matrix OT O, and yet K ≪M?
Example 15.1. In some face recognition techniques, recognition is done by compar-
ing the gray images, interpreted as (very) high dimensional vectors in a Hilbert space
with the scalar product, Eq. (3.27). A similarity or dissimilarity measure between two
face images can then be used, e.g. the directional difference, (3.56), or the Euclidean
distance, to decide whether or not the images represent the same person. Alterna-
tively, a trained classiﬁer, such as a neural network [29, 49], or the Mahalanobis
distance [62], can be used in this decision making. One assumes then the reference
images of the clients, O = [f 1, · · · , f K ] with f k ∈EM. In order for this to work,
the images must be scale- and position-normalized, so that little or no background
is present and the eyes are essentially in the same position in all images. It has been
shown, see [202,220] and others, that this recognition is improved if the dimension
of the face images is ﬁrst reduced to N by PCA, where typically N < K ≪M. The
resulting subspace is also called the face space. The similarity of two face images
are then measured in this subspace, spanned by the N most signiﬁcant eigenvectors,
also called eigenfaces.
In the top row of Fig. 15.1, which shows six images of two persons, some sam-
ples of a face training set O are illustrated. Representing 64 × 96 images, the cor-
responding face vectors have M = 6144 dimensions. The training set O consists
of K = 738 such face vectors. The mean of the training set is subtracted from the
training set, and also later from the test set, (15.22). The actual scatter matrix OOT
is 6144 × 6144, whereas the alternative scatter matrix OT O, by which the eigen-
vectors can be computed, is 738 × 738. The 24 most signiﬁcant eigenvectors are
shown as pseudo colored images in the same ﬁgure, in reading order. Using a test
set containing 369 images (different than those in O) and retaining the N = 30
most signiﬁcant eigenvectors, one could obtain 89% recognition in the senspe that
the person to be recognized was assigned the top rank. The recognition was 96% if
the correct image was found among the top 3 ranks [154].

Fig. 15.1. Top row shows samples of 64×96 face images of two persons [154]. The color
images show the 24 most signiﬁcant basis faces that are found by PCA

338
15 Reducing the Dimension of Features
15.3 Singular Value Decomposition (SVD)
Here we discuss a powerful tool, the singular value decomposition (SVD) to solve
homogeneous linear equations of the type
OT ψ = 0,
where
O is M × K,
(15.35)
in the TLS error sense, which appear in numerous image analysis problems. This is
an important class of problems for computer vision for many reasons, including the
following.
First, in the direction estimation formulation, the error ∥Oψ∥2 is to be made as
small as possible, possibly zero, by a certain solution ψ. The minimum achievable
error is λ(M), which can be zero in the ideal case or small in a successful hyperplane
ﬁtting, if and only if the solution satisﬁes
OOT ψ(M) = λ(M)ψ(M)
(15.36)
where λ(M) is the least (eigenvalue) of the scatter matrix OOT . With this construc-
tion, we can thus transfer the quadratic minimization problem to the homogeneous
equation problem:
min
ψ ψOOT ψ
⇔
OT ψ = 0
(15.37)
where we search for the TLS error solution with ∥ψ∥= 1.
Second, studying lemmas 15.2 and 15.1 shows that the quadratic minimization
ψT OOT ψ problem which we converged at, ﬁrst in the linear symmetry direction
problem, then in feature extraction including corner features, group direction fea-
tures, motion estimation, world geometry estimation, and dimension reduction prob-
lems, is a very fundamental problem for vision. One can therefore conclude that the
problems of vision can be effectively modeled¿ as a direction estimation problem,
which is in turn equivalent to a homogeneous equation problem. Third, because a
linear equation Aψ = b can be rewritten as
OT ψ = b
⇒
[OT , −b]

ψ
1

= 0
(15.38)
even non homogeneous linear equations can be easily treated within homogeneous
equation formalism. There are straightforward extensions of this idea employing
polynomials as well as other nonlinear transformations as the elements of O, af-
fording nonlinear modeling too. Typically, O, b represent the explanatory variables
and the response variables, respectively. Other names for these variables are in-
put and output, respectively. One searches for ψ, representing the model variables,
which are also known as the regression coefﬁcients. The advantage with a homoge-
neous equation resides in that it can model noise in both O and b in the TLS sense,
making the solution independent of measurement coordinates, that is, a tensor solu-
tion, see Sect. 10.10. In computer vision, homogenization is in itself an important

15.3 Singular Value Decomposition (SVD)
339
tool to transform large afﬁne and projective mappings, which are nonlinear, to linear
mappings, see Sect. 13.2, in addition to the tensor solution it affords.
To stay within the limits of our scope, the following lemma, which is extensively
discussed elsewhere, e.g. [84], is given without proof.
Lemma 15.3 (SVD). An M × K matrix O can be decomposed as
O = UΣVT
(15.39)
where Σ is an M × K diagonal matrix,2 and the matrices U, V are quadratic and
orthogonal.
Note that the matrix O can be, and in practice is, nonquadratic. If we have K > M,
we have the following decomposition format for O
⎛
⎝
O
M×K
⎞
⎠=
⎛
⎝
U
M×M
⎞
⎠
⎛
⎝
Σ
M×K
⎞
⎠
⎛
⎜
⎜
⎜
⎜
⎝
VT
K×K
⎞
⎟
⎟
⎟
⎟
⎠
(15.40)
whereas if we have K < M the format of the decomposition yields
⎛
⎜
⎜
⎜
⎜
⎝
O
M×K
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎝
U
M×M
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
Σ
M×K
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎝VT
K×K
⎞
⎠
(15.41)
In both cases, the matrices O, Σ have the same form, i.e. either they are both “sleep-
ing” as in Eq. (15.40), or they are both “standing” as in Eq. (15.41). Below, we will
qualify a matrix as standing if it has more rows than columns, and sleeping con-
versely.
Assuming that the diagonal elements of Σ are sorted in descending order, σ(11) ≥
· · · ≥σ(κκ), where κ = min(M, K), and the columns of the matrices O, U, V
are sorted accordingly, one can see that the symmetric, semipositive deﬁnite matrix
OOT can be diagonalized by means of SVD as
OOT = UΣVT VΣT UT = UΣΣT UT
= UΛUT
(15.42)
Because the matrix ΣΣT is diagonal and has a square form, the eigenvectors of
OOT are to be found in the columns of U, whereas its eigenvalues are the diagonal
elements ΣΣT . The latter elegantly shows that M −K eigenvalues are automatically
zero when O is a standing matrix because the corresponding ΣΣT will have zeros
2 A diagonal matrix is one that has zeros as offdiagonal elements, i.e., the elements of the
diagonal matrix Σ are σijδ(i −j).

340
15 Reducing the Dimension of Features
in its last M −K diagonal elements. By contrast, if O were a sleeping matrix, the
quadratic matrix OOT would not be singular in general since no zero values would
be automatically generated in the diagonal elements of ΣΣT . Evidently, the number
of such null eigenvalues is a consequence of whether or not the equation OT ψ = 0
is overdetermined, which is paraphrased by the term sleeping O. Similarly, one can
diagonalize OT O as
OT O = VΣT UUΣVT = VΣT ΣVT
= V ˜ΛVT
(15.43)
to the effect that the eigenvectors of OT O are to be found in the columns of V,
whereas its eigenvalues are in the diagonal elements ΣT Σ. The two diagonal matri-
ces ΣT Σ and ΣΣT contain the same nonzero diagonal elements.
There exist effective software implementations of SVD, and even some where
only the ﬁrst K columns of U are computed in case one has K ≪M. This is
valuable in image analysis, where one often ﬁts hyperplanes with low dimensions to
data in high-dimensional (Hilbert) vector spaces. The SVD is a ubiquous tool that
can be generously applied to numerous vision problems.

16
Grouping and Unsupervised Region Segregation
In this chapter we present the elements of unsupervised texture boundary estimation
to address automatic grouping of regions in images. The grouping is based on the
assumption that a set of feature vectors is densely available for the image. These
can be viewed as layers of images, each representing a property of the image. A
simple example is an HSV-type color image, where the ﬁrst layer represents the hue,
the second layer saturation, and the third layer the brightness. However, the feature
images need not come from imaging sensors. They could be local image properties,
densely computed from ordinary gray value images, such as the texture properties:
mean, variance, and direction. A set of N dense feature layers is equivalent to a 2D
grid on which the image takes N-dimensional vector values.
The presentation in this chapter focuses on group formation and boundary esti-
mation in 2D images having tensors as pixel values, but it does not presuppose or
prescribe a speciﬁc set of features, neither imaged nor computed, as this is applica-
tion dependent. The value of a pixel is then represented by an array, each element of
which over the entire image grid can be considered as a separate image, also called
a feature image, with real pixel values. First an issue critical to the grouping process
of images is discussed, the uncertainty principle. The presence of noise in the fea-
tures causes a class overlap that can be reduced in a multiresolution pyramid. The
uncertainties in boundaries can be reduced by means of butterﬂy-shaped smoothing
ﬁlters, adaptive to boundary directions. We give further precision to these matters
in the subsequent sections, where the discussions follow the same principles as the
studies in [195], [207], and [227].
16.1 The Uncertainty Principle and Segmentation
Uncertainty in computed image features is similar to the uncertainty principle in
physics. A wave describing a particle cannot be highly concentrated simultaneously
in position and frequency. In unsupervised image segmentation, both position (class
boundaries) and prototype features (local spectral properties) have to be determined.
Finding boundaries is dependent on prototypes and vice versa because:

342
16 Grouping and Unsupervised Region Segregation
Fig. 16.1. A Gaussian pyramid is shown for a gray image having two classes (bottom). The
graph on the top shows the histogram of the lowest resolution image whereas the graph in the
middle is ditto for the highest image resolution

16.1 The Uncertainty Principle and Segmentation
343
•
Accurate prototypes are needed to yield accurate boundaries, whereas
•
Accurate boundaries are precisely what is needed to compute accurate proto-
types.
Therefore, the prototypes and the boundaries cannot be simultaneously determined
[227] to yield high accuracy. The presence of noise in the feature space, mainly pro-
duced by modeling errors (inappropriate features), is another source of noise which
contributes to a class overlap.
The variance of a signal decreases when it is lowpass ﬁltered, i.e., averaged.
Because of this, multiple resolutions are crucial in reducing the uncertainty. At lower
resolutions the class prototype estimates are better deﬁned than high resolutions.
A Gaussian pyramid can be built, up to a predeﬁned level in which each level is
obtained by smoothing the preceding ﬁner level. Accordingly, each element of the
pixel value tensor, a feature image, will have its own pyramid. At the coarsest level,
the amount of noise in the features therefore decreases signiﬁcantly, allowing the
feature prototypes to be determined more accurately, but at the expense of the region
boundary resolution.
Class uncertainty reduction via pyramids is illustrated in Fig. 16.1 [195,228]. At
the ﬁnest level the histogram is unimodal, even if it is possible to see that distinct
classes are present. At a lower resolution the noise has been smoothed out signiﬁ-
cantly, making possible the detection of the two classes (the histogram is bimodal).
It is at a low resolution level that a good opportunity to partition the feature space
into its constituent Nc classes appears. Partitioning can be seen as ﬁnding Nc subsets
of feature vectors gathered around their respective prototypes (or class centers) au-
tomatically. This can be obtained by applying a clustering algorithm in the smoothed
feature space (obtained at the coarsest level). For most of these algorithms to work,
the number of classes Nc, or an equivalent information, has to be available a priori.
Image segmentation experiments using automatic clustering indicate that the results
do not critically depend on the choice of a clustering algorithm, provided that the
classes are separable by means of the provided features.
Next, isolated points as well as isolated and scattered small classes are eliminated
by reassigning them to a class in the spatial vicinity to obtain a spatial connectivity.
It is, however, now, when a segmentation at the coarsest level is available, that it
becomes evident that the cost of good class separation is bad boundaries.
The last but not the least step is a boundary estimation procedure that gradually
improves the class boundaries by traversing down the pyramid. First, at the resolution
level where the clustering is performed, the crude boundaries are identiﬁed. The
children of the boundary points deﬁne a boundary region at the next higher resolution
because every pixel at a certain level represents several pixels at the lower resolution.
The nonboundary nodes at the children level are given the same labels and properties
as their parents. The class uncertainty within the boundary region is high and has to
be reduced before reassignment of the boundary vectors. Now that the approximate
boundaries and thereby the boundary directions are available, orientation-adaptive
ﬁlters are used to smooth the boundary regions. For each dominant local direction,
a butterﬂylike averaging ﬁlter (see Fig. 16.2 and Sect. 16.6) reduces the inﬂuence of

344
16 Grouping and Unsupervised Region Segregation
feature-vectors across the boundary while decreasing the within-class variance along
the boundary. The two halves of the ﬁlter (applied separately) produce two responses.
The distances between the resulting vectors and the two prototypes, associated to
the classes deﬁning the boundary, are computed and a reclassiﬁcation is performed.
The procedure, which will be discussed further in the subsequent sections, can be
summarized as follows:
1. Build a multiresolution pyramid up to a certain level. The noise in the feature
space is reduced, increasing the separation between the classes at the expense of
the spatial resolution.
2. Cluster the data in the smoothed feature space by using algorithms at suitable
levels of the pyramid. Reassign isolated pixels as well as small and scattered
classes to enforce spatial continuity.
3. Gradually improve the spatial resolution by projecting down the labels and reﬁne
the boundaries using orientation-adaptive ﬁlters.
16.2 Pyramid Building
Noise reduction can be done by means of smoothing, which can efﬁciently be imple-
mented in image pyramids [43, 92]. Lower resolution levels are obtained by taking
the (weighted) average of small neighborhoods to the next coarser level, as discussed
in Sect. 9.5. In the discussion here, an octave pyramid, i.e., one in which the image
width and heights are halved between subsequent levels, is assumed. Let f(rk, l) be
the feature vector having N dimensions (layers) at image location rk and level l of a
feature pyramid constructed from the highest resolution (lowest level) as follows:
f(rk, l + 1) =

rp
g(rp)f(2rk −rp, l)
(16.1)
The value of a parent node is the weighted mean of its children within the support
of a ﬁlter g, such as a Gaussian. The different levels are computed using Eq. (16.1)
in a bottom–up manner, starting from level l = 0 up to a predeﬁned level l = L.
The height and width of the image decrease with a factor of 2l at level l. A pyramid
is constructed for each feature separately. Therefore, the data structure can be seen
as a set of pyramids or as a single one composed of M × 1 vectors at each image
point. The choice of the number of levels is important, as this determines the mini-
mum connected region size in the grouping. If L is too small, the uncertainty is not
reduced sufﬁciently, whereas if L is too high, small regions will disappear. Pyramids
also reduce the computational cost by progressively reducing the number of feature
vectors on which a clustering algorithm will be applied. Assume, for instance, that
the feature images have the size 256 × 256 and that L = 3. Then the number of fea-
tures is reduced from 65, 536 to 256 at the level L, a reduction with factor 4L = 64.
The level L should, however, be chosen as a function of the noise rather than a func-
tion of the size. If the number of feature vectors remains prohibitively large for a
clustering algorithm, it still can be reduced by taking them from random image sites.

16.3 Clustering Image Features—Perceptual Grouping
345
16.3 Clustering Image Features—Perceptual Grouping
At the coarsest level of the pyramid, a clustering algorithm is used to ﬁnd the differ-
ent classes and their prototypes. It is reasonable to assume that objects with similar
properties belong to the same group or cluster, i.e., they are gathered around the same
prototype. The problem is to ﬁnd a partition of the feature space into Nc homoge-
neous subsets. To solve this problem, a suitable clustering criterion and a similarity
(or dissimilarity) measure have to be deﬁned. Being a statistical tool for grouping,
clustering is also known as vector quantization. Advantages include:
•
No training is needed.
•
No statistical distribution of the data is to be estimated.
For the clustering to succeed, the features must be powerful enough to group the
data into compact volumes in the feature space, where compactness depends on the
choice of the used metrics in the feature space.
The choice of the number of classes Nc, required by most of the clustering al-
gorithms, is considered to be one of the most fundamental problems in cluster anal-
ysis [67]. Usually this information is not known, and partitions of the feature space
for different values of Nc are computed.1 An overview of cluster validity and studies
of this nontrivial problem can be found elsewhere [16]. In the following, we will as-
sume that clustering algorithms in which the number of classes Nc (or its equivalent)
has to be given. We will, however, discuss reducing the problem of an exact estima-
tion of the class number by overestimating Nc and reassigning small and scattered
classes to a class in the spatial neighborhood.
Using a schematic example, we illustrate the clustering ﬂow chart in the feature
space next. For completeness, in the next section we will summarize the fuzzy C-
means clustering algorithm proposed by Bezdek [16], which is an extension of the
idea discussed in the example [11], also known as the ISODATA algorithm. This
algorithm yields reasonable clustering results while being simple.
Example 16.1. We wish to cluster the data shown in the left graph of Fig. 16.3. We
follow the processing ﬂow illustrated by the chart on the right. The number of classes,
Nc, is assumed to be known. Here Nc = 3, and the class labels are, circle, rectangle,
and square, for convenience.
1. Start: At random choose Nc objects, e.g., 2, 3, 8. These will be the initial class
centers (prototypes); assign them a label each, e.g., circle, rectangle, and square.
2. Update partitions: Given the current class centers, assign all objects to one of
the circle, rectangle, or star classes by use of the least distance criterion. This
will yield the current partitioning of the data.
3. Update centers: Given the current partitioning, ﬁnd the class centers by com-
puting the class means. This will yield the current class centers.
1 Hierarchical clustering techniques do not require the number of classes to be known explic-
itly [28,191], but they require it implicitly, e.g., they need an input threshold for maximum
within class variance.

346
16 Grouping and Unsupervised Region Segregation
Fig. 16.2. Illustration of an oriented butterﬂy ﬁlter adapting to the normal direction of a
boundary point
4. Exit? If the current partitioning or the class centers have changed signiﬁcantly
compared to their previous values, go to step 2; else exit.
The trace of partitioning would be given by the following listing:
Circle
Rectangle
Star
{2, 10, 12, 7}
{3, 1, 9, 6, 13, 15} {8, 5, 14, 4, 11}
{2, 10, 12, 7, 15} {1, 3, 6, 9, 4, 13, 11}
{5, 8, 14}
{2, 10, 12, 7, 15}
{6, 9, 4, 13, 11}
{5, 8, 14, 1, 3}
{2, 10, 12, 7, 15}
{6, 9, 4, 13, 11}
{5, 8, 14, 1, 3}
(16.2)
Had we called the objects, 2, 3, 8, as star, rectangle, circle, respectively, the labels
of the result would be permuted too. Accordingly, the labels may change up to a
permutation, unless they are “learned” by the system separately.
If the clusters are not “ball”-shaped, e.g., elongated or engulfed, there is evidently a
risk of erroneous partitioning. This is also related to the distance metrics used in the
clustering. Assuming that there is a choice of metrics that can “cluster” the features
and the features themselves are powerful, then the clustering method will be able to
group the feature data meaningfully. The clustering then will implicitly or explicitly
compute a distance matrix D that describes the distances between all feature vectors
[f 1, · · · , f K]:
D =
⎛
⎜
⎜
⎜
⎝
d1,1 d1,2 · · · d1,K
d2,1 d2,2 · · · d2,K
...
...
dK,1 dK,2 · · · dK,K
⎞
⎟
⎟
⎟
⎠
(16.3)
Here and elsewhere in this section, we have used the notation f k to mean f(rk, L)
for notational simplicity, i.e., the clustering is assumed to be done on feature vectors
at the highest level of the pyramid. Thus, the distance does not need to be Euclidean

16.4 Fuzzy C-Means Clustering Algorithm
347
(but very well can be), e.g., L1, L∞. The number of distances to compute grows
quadratically. For 1000 feature vectors, one would need to compute over half a mil-
lion distances in a multidimensional space. This is one of the limitations making clus-
tering algorithms not so practical for large number of features in high-dimensional
spaces. The other and more serious limitation is that the convergence problem of
the partitioning, which once achieved, is usually not as meaningful as a partitioning
obtained in a (unkown) lower-dimensional subspace. This provides yet another mo-
tivation to dimension reduction followed by image feature smoothing in a pyramid,
before attempting to group image features automatically.
16.4 Fuzzy C-Means Clustering Algorithm
Let F = f 1, ..., f K be a ﬁnite set of feature vectors with f k ∈EM, and Nc be an
integer2 representing the number of classes to which the feature vectors can belong.
The Nc partitioning of the feature space is represented by the Nc × K matrix U =
[unk], consisting of the elements unk, which represent the membership of feature
vector vecfk to the class n. There are K feature vectors and Nc classes, so that k
and n are integer indices in the ranges 1, · · · , K and 1, · · · , Nc, respectively. In the
hard (or crisp) case, the degree of “belongingness” of feature vector f k to a class n
is either 1 or 0, i.e.,
unk =

1, f k ∈class n,
0, otherwise.
(16.4)
In the fuzzy case, unk gives the “strength” of the membership of feature vector f k to
class n (unk ∈[0, 1]). The degree of belongingness unk can be intuitively seen as a
distance from feature vector k to the class n normalized by the sum of distances to
all class centers. This representation is in many cases closer to the physical reality
in the sense that feature vectors almost never fully belong to one class i.e., there is
always a suspicion that an object could have been classiﬁed to a different class, albeit
with a lower certainty. The two following conditions have to be respected:
Nc
n=1 unk = 1,
for all f k,
(16.5)
0 < K
k=1 unk < K,
for all classes n.
(16.6)
In the crisp case, Eq. (16.5) simply means that feature vector k belongs to one class
only. The second condition Eq. (16.6) means that no class is empty and no class is
all of F. The fuzzy C-means algorithm belongs to the class of objective function
methods. Such methods minimize a clustering criterion which is, in this case, the
total within-group sum of squared error (WGSS). The fuzzy C-means is the fuzzy
extension of the hard C-mean. It minimizes the objective function
eκ(U, V) =
K

k=1
Nc

n=1
(unk)κ(dnk(vn, f k))2
(16.7)
2 Evidently we assume that Nc ∈2..K −1 because if Nc = 1 or Nc = K, then one would
obtain trivial partitionings.

348
16 Grouping and Unsupervised Region Segregation
where κ is an empirically chosen weighting exponent controlling the amount of
fuzziness κ ∈[1, ∞[. The scalars unk ∈[0, 1] are the elements of the matrix
U = [u1, · · · , uk, · · · , uK], which represent a fuzzy partition of the feature vec-
tors such that a column uk represents the degree of belongingness of the feature
vector f k to each of the Nc different classes. In such a partitioning, the class pro-
totypes are represented by the columns of the matrix V = [v1, · · · , vn, · · · vNc, ].
Thus U, V are updated by the partitioning process, whereas the feature set F =
[f 1, · · · , f k, · · · f K], which is the set to be categorized into Nc classes, is ﬁxed. The
elements dnk = ∥f k −vn∥where ∥· ∥is an inner product induced norm on the M-
dimensional Euclidean space, EM, of the features, represents the squared distance
between two vectors in the feature space. Restated, the problem is to ﬁnd the best
pair (U, V) that minimizes the error function eκ. It can be shown that [16] e(U, V)
may be globally minimal for (U, V) only if
unk =
⎛
⎝
Nc

j=1
dnk
djk
2/(κ−1)
⎞
⎠
−1
,
(16.8)
vn =
 K

k=1
(unk)κf k

/
K

k=1
(unk)κ,
for
n ∈1, · · · , Nc.
(16.9)
The fuzzy C-means algorithm, which approximates a solution of the minimization
problem, can be stated as follows:
1. Start: Fix Nc, choose a norm (based on an inner product) ∥· ∥for EM, ﬁx κ,
where 1 < κ ≤∞, and initialize U(0).
2. Update centers: Calculate the Nc fuzzy cluster centers V = [v1, · · · , vNc] with
Eq. (16.9) and the current fuzzy partitions, U.
3. Update partitions: Update U using Eq. (16.8) and the current class centers, V.
4. Exit ? If ∥U l+1 −U l∥≤ϵ, then stop; otherwise go to step 2.
As κ →1, the fuzzy C-means converge to a “generalized” hard C-means solution
(ISODATA) [11], discussed in example 16.1 and Eq. (16.7). This algorithm always
reaches a strict local minimum for different initializations of U.
16.5 Establishing the Spatial Continuity
First, we consider the spatially isolated points in the resulting image obtained from
clustering. Such points are likely to be obtained when the clustering terminates, for
the following reason. Let γn be the label associated to the class prototype of vn.
Then, the image position of a feature vector f k, inherits the class label γn if d(f k, vn)
is smallest compared to all Nc classes. However, because the clustering process does
not “know” from which image location the feature vectors come, the spatial (i.e., in
the label image, which is Γ ) continuity is not evident, although in practice most
pixels will cluster together even spatially (in the image). At the coarsest resolution

16.5 Establishing the Spatial Continuity
349
1
2
3
4
5
6
7
8
9
10
11
12
15
14
13
Update
centers
Update
partitions
Exit?
Start
Fig. 16.3. The graph on the left represents some feature vectors to be clustered. On the right
the processing ﬂow to achieve a partitioning is shown
level L, we can deﬁne a label image Γ(rk, L) in which each pixel receives the label
of the corresponding prototype vector in the feature space. Note that even the label
image Γ is also deﬁned as a pyramid, albeit the high-resolution levels are yet to
be determined. At the lowest resolution level, rough boundaries can be observed
between the different classes in Γ. Let N8(rk) be the 8-connected neighborhood of
a pixel at location rk composed of the 8-closest neighbors on a square grid.3 A pixel
rk is considered as spatially misclassiﬁed if Γ(rk, L) is different from all the labels
in its N8(rk). In that case, it is reassigned to the most common class in N8(rk).
Next, we consider insigniﬁcant classes, which are small and scattered classes
that need to be reassigned to their neighboring classes. This is because a class in the
feature space is distributed in one or more subregions in Γ(rk, L), even the largest of
which may not be more than a few pixels. A class is considered as “insigniﬁcant” if
its largest subregion contains no more than a threshold (nine in the discussions here).
Thus, the idea is to give a preference to classes that are spatially distributed into large
and compact subregions rather than the inverse. It is, however, clear that the mean-
ing of “insigniﬁcant classes” is closely related to the height of the pyramid, i.e., for
a low value of L, the actual size of an insigniﬁcant class is smaller than for higher
values of L. We discuss a possible implementation of this by the illustration in Fig.
16.4. Representing the largest regions of their respective classes, the hatched regions
are assumed to be insigniﬁcant classes on the grounds of their size (because they are
fewer than 9 pixels each) and have to be reassigned to either one of the surrounding
classes or, in the cases of (b) and (c), possibly even between themselves. Thus there
are two types of insigniﬁcant classes: isolated insigniﬁcant class, as exempliﬁed by
(a), and touching insigniﬁcant class, as exempliﬁed by (b) and (c). The isolated class
3 That is, N8(rk) represents the set of points in a 3 × 3 neighborhood around the current
point rk, excluding the latter.

350
16 Grouping and Unsupervised Region Segregation
Fig. 16.4. Three classes (white) that have to be reassigned to a neighbor class (colored)
can be reassigned in one pass, whereas multipasses are needed if the insigniﬁcant
classes are touching each other. First, one can reassign the isolated classes, like those
(a) in Fig. 16.4, by generating a set of candidate classes obtained from the neighbor-
ing classes. This is determined for each pixel of the region. If a label in the N8(rk)
neighborhood of a point of such a region is different from Γ(rk, L), then a new can-
didate class is obtained. Otherwise, a different point (outside of N8(rk), but in the
same direction) is examined until their corresponding class labels differ. These op-
erations are repeated for the eight possible directions of the N8(rk). For each pixel
rk, the Euclidean distances between the feature vector prototype of the region, e.g.,
that of (a), and the prototypes of the candidate classes are computed. Each pixel is
then assigned to the closest candidate class. To extend the search outside of the mask
is necessary because not all points share boundaries with a point of a different class.
This can be illustrated by region (a) in Fig. 16.4, where the central pixel near the bor-
der of the image lacks neighbors belonging to signiﬁcant classes. For these pixels,
an extended search beyond the N8(rk) neighborhood is necessary to obtain relevant
candidate classes.
Next, the remaining insigniﬁcant classes consisting of touching regions need to
be reassigned. However, the reassignment order leads to different results, and mul-
tipasses are needed to avoid this problem. The process can be illustrated by con-
sidering the classes (b) and (c) in the ﬁgure. In the ﬁrst pass, these two classes are
temporarily reassigned as if they were isolated insigniﬁcant classes. The same initial
state of Γ(rk, L) is used for both classes. The maximum number of pixels Kn max
reassigned to the same candidate class is determined for each class label γn equaling
to the class of (b), and ditto for class (c). Then a quotient qn between Kn max and
the respective population Kn of the classes is computed, i.e., qn = Kn max/Kn.
A value of qn = 1 means that the class n tends to be reassigned to a single candi-
date class. The reassignment of the region with the largest quotient qn is then taken
ﬁrst. Suppose that (b) is reassigned ﬁrst. Before reassigning (c) in a second pass, one
needs to check if (c) is still considered as an insigniﬁcant class. It might occur that
pixels of (b) were reassigned to (c), making it a signiﬁcant class. If this is not the
case, (c) is reassigned using the procedure described for isolated classes. A similar
procedure can be applied to reassign more than two touching insigniﬁcant classes,

16.6 Boundary Reﬁnement by Oriented Butterﬂy Filters
351
Fig. 16.5. The parent node at a boundary point of level l in the pyramid and its corresponding
children nodes at level l −1. Such children nodes deﬁne the boundary region
although it will take as many passes as the number of touching classes to terminate
the reassignment procedure.
16.6 Boundary Reﬁnement by Oriented Butterﬂy Filters
The ﬁnal step of the pyramidal unsupervised segmentation is a boundary reﬁnement
procedure that gradually improves the spatial resolution of the label image Γ(rk, L).
As stated by the spatial principle of uncertainty [227], accurate prototypes can be ob-
tained only at the expense of the spatial resolution. Therefore, at the coarsest level,
the class uncertainty at the boundaries is higher than other points. One can reduce
it by means of orientation-adaptive ﬁlters [195]. Other methods for boundary en-
hancements include [88, 149, 150]. The spatial resolution is gradually restored by
projecting down the class labels, smoothing around the boundaries, and reassigning
the boundary pixels to their closest neighboring class. We also assume that the pro-
totypes have constant values across the different levels of the pyramid. First, at the
coarsest level L the boundary pixels are determined. Each pixel rk is considered as
a boundary pixel if at least one label in N8(rk) is different from Γ(rk, L). Next,
Γ(rk, L −1) is obtained by projecting down the label of each nonboundary parent
node to its four respective children nodes, i.e., Γ(n, j, l) = Γ(n/2, j/2, l+1), where
/ is integer division.4 The children of the boundary nodes deﬁne a boundary region
β (see Fig. 16.5) in which the oriented smoothing will be performed. For smoothing
we need an estimate of local boundary directions. We use the linear symmetry algo-
rithm, discussed in Sect. 10.11, which deﬁnes the dominant orientation in the TLS
sense. The direction is determined by the complex number
4 Integer division is the integer part of ordinary division, e.g. 5/3 would yield 1.

352
16 Grouping and Unsupervised Region Segregation
Fig. 16.6. The coefﬁcients of a butterﬂy ﬁlter with orientation θ = 0 for a 3 × 3 (left) and
5 × 5 (right) neighborhood
z = ℸ(Γ) ∗g
(16.10)
where Γ is the image of labels, ℸ(Γ) is the squared complex image (DxΓ +iDyΓ)2,
and ∗g the convolution with an averaging ﬁlter. The argument of z obtained at every
pixel location represents
arg(z) = two arg(kmin)
(16.11)
where kmin is the complex number whose argument is the dominant boundary gra-
dient direction. In this case, the magnitude of the gradient of Γ is 1 at the transition
between two classes, and 0 within a class. The smoothing ﬁlter g is of size s×s and is
given by a Gaussian, which in this presentation is s = 7 induced by use of σ = 1.8.
The direction is computed for the boundary pixels at the parent node level and is
propagated to the children level. For each dominant local direction, a butterﬂy-like
ﬁlter is deﬁned. The butterﬂylike shape, Fig. 16.2, reduces the inﬂuence of vectors
along the boundaries that have high uncertainty. The shape and the weights of a 3×3
and 5×5 ﬁlter for the horizontal direction (θ = 0) are given in Fig. 16.6, where r is a
function of the dissimilarity d, described below, between the two classes that deﬁne
the boundary and rr = (1 −r)/Nν with Nν being the number of weights different
from 0 or r . The dissimilarity d is given by
d =
|μA
m −μB
m|

(σA
m)2 + (σB
m)2
(16.12)
where μA
m, μB
m and (σA
m)2, (σB
m)2 are the means and the variances of the two classes
on both sides of the boundary in the mth component image of the feature vector
f(rk, l). Note that the latter can be viewed as consisting of M layers of (scalar)
images. Then r = r(d) is deﬁned empirically as an increasing function, here as in
Fig. 16.7 [229]. If the dissimilarity d is large then r = 1 and no smoothing is applied,
whereas a stronger smoothing is performed (r = rr) for low values of d.

16.6 Boundary Reﬁnement by Oriented Butterﬂy Filters
353
Fig. 16.7. The function relating the dissimialrity to the central coefﬁcient of the butterﬂy ﬁlter
We still need to deﬁne the ﬁlters for different orientations θ. This is done by rotat-
ing the mask deﬁned for the horizontal direction θ = 0 and redistributing the weights
for matching the grid of an image. The ﬁlters are computed for a ﬁxed number of di-
rections (eight here) and are stored in the memory, i.e., a lookup table.Starting from
the coarsest level l = L, the boundary reﬁnement procedure can be enumerated as
follows:
1. Project down the labels, i.e., Γ(rk, l −1) = Γ(rk/2, l) and deﬁne the boundary
region β at level l −l. Compute the mean μn and variance (σn)2 for all classes.
2. For each boundary pixel at level l compute the dominant direction θ using Eqs.
(16.10) and (16.11). Determine the two classes A, and B on both sides of the
boundary. Choose the corresponding butterﬂy ﬁlter, and propagate its identity to
the corresponding pixels at level l −1.
3. For each pixel rk ∈β, apply the ﬁlter corresponding to the current position to
(all components of) the feature vectors f(rk, l−1). If a feature-vector participat-
ing to averaging is outside of β, take the vector of its corresponding prototype.
The left and right halves of the ﬁlters are applied separately, reducing the risk
of smoothing across the boundaries. Two responses are obtained, hA(rk, l −1)
and hB(rk, l −1). This smoothing is repeated a certain number of times found
empirically (four in the examples) using a small ﬁlter size (3 × 3 here). This
is equivalent to use a large ﬁlter size in one iteration, but it is computationally
faster.
4. For each pixel rk, compute the four distances between the two ﬁlter responses
hA, hB and the prototypes μA, μB corresponding to the classes A, B on each
side of the boundary region, i.e., ∥μA −hA∥, ∥μA −hB∥, ∥μB −hA∥, ∥μB −

354
16 Grouping and Unsupervised Region Segregation
T4           T7            T5           T1
T2           T1            T4
T6
T4
T2           T7
T2
T6          T5
T3
Fig. 16.8. A texture patch (P3) and the ground truth class identities (T1, T2,...,T7) represented
by color at each point
hB∥. Each boundary pixel receives the label of the class that gives the minimum
distance.
5. Decrease the value of l by one and repeat from step 1, until the bottom of the
pyramid is reached.
16.7 Texture Grouping and Boundary Estimation Integration
Here we integrate the unsupervised clustering and the boundary estimation process
discussed in the previous sections, by illustrating its effect on a patch image of tex-
tures (Fig. 16.8). It consists of regions corresponding to real texture images, as for
those in [37], put in a 256 × 256 image. Each texture patch was photometrically
normalized with respect to the mean and the variance, i.e., if the texture patch is rep-
resented by f, the normalized texture is obtained as f ′ = af + b for some constants
a and b to force the mean and the standard deviation f ′ to equal speciﬁc values (128
and 30 here, for all patches). This was done to illustrate the multidimensional group-
ing because, otherwise, the textures would be too easy to separate by gray values,
being 1D features. There are, in total, seven different textures tiled as a 4 × 4 matrix,
and the patches are repeated to obtain all texture combinations astride the boundaries
for a maximal diversity. Accordingly, every texture is a neighbor of any other at least
once.
Exercise 16.1. Is there an alternative way to construct Fig. 16.8, i.e., to obtain 4 × 4
patches of T-textures with T = 7, 8 · · · 16, so that all texture crossovers are present?
A rotation of the result or a change of texture identities is not counted.
The symmetry derivatives applied to a log–polar decomposition of the spatial
frequencies discussed in Chap. 14 have been used as features. To be precise, there

16.7 Texture Grouping and Boundary Estimation Integration
355
Fig. 16.9. (Reading direction) In pseudocolor, the images represent the four most signiﬁcant
texture features of P3 after dimension reduction
were three frequencies in the decomposition, at each of which I11 (real), I20 (com-
plex), I40 (complex) were computed. Taking the real and the imaginary parts of the
complex features, one thus obtains 15 real features. Furthermore, the dimension of
this space was reduced to ﬁve by projecting these features to the ﬁve most signiﬁcant
eigenvectors from a KL transform, see Chap. 15. Consequently, the clustering and
the boundary reﬁnement procedure here were fed with 5 dimensional features, of
which are 4 are shown in Fig. 16.9. We required Nc = 7−−9 classes from the clus-
tering with L = 3, i.e., the clustering had to be performed on a 16 × 16 image, and
the correct patch identities (up to a texture name permutation) as well as reasonably
accurate boundaries could be obtained in an unsupervised manner, i.e., there was
no training. The boundaries are nondeterministic i.e., they show small variations be-
tween the runs because the prototypes display small variations. This depends on the

356
16 Grouping and Unsupervised Region Segregation
T4           T7            T5           T1
T2           T1            T4           T6
T4           T3           T2           T7
T2             T6          T5           T3
Fig. 16.10. On the left the found texture groups are shown in color, i.e., points sharing the
same color belong to the same group. On the right, the boundaries and class labels implied by
this grouping are superimposed the original image
initialization of the clustering, which was random, as well as the required classes,
which was varied from 7 to 9. Figure 16.9 illustrates such a result, the boundaries
and the texture classes of which agree fairly well with the ground truth, shown in
Fig. 16.8.
16.8 Further Reading
A word of caution is in place at this point. A disadvantage of the simple structure of a
resolution pyramid, is its incapacity to keep track of small regions while building up
the pyramid. In other words, this scheme assumes that the regions to be segmented
do not have extreme size variations because very small regions will be considered
noise if one attempts to reduce the noise within large regions by using a too high
L. On the other hand, a more adequate solution would need general automatic scale
selection, an issue which has proven to be a signiﬁcant challenge [151].
In the vast clustering literature available for general feature vectors, it can be
noted that different criteria might lead to different results depending if the shape of
the metrics is not reasonable as compared to cluster shapes. It is clear that no cluster-
ing criterion or measure of dissimilarity is universally applicable. For instance, the
single link method is more suitable for elongated clusters while WGSS (within-group
sum of squares) criterion gives good results for ball-shaped structures [87].However,
it can be argued that clustering used to achieve perceptual grouping or image seg-
mentation should not critically depend on the choice of a clustering technique if
features are sufﬁciently strong to characterize groups. If they are not, it is often more
worthwhile to devote efforts to redesign the features by using human experts than to

16.8 Further Reading
357
the clustering scheme, because humans are formidable vision experts on their own.
The feature vectors discussed here are easy to interpret by a human expert, as they
encode the geometric properties. If human expertise is involved, features that are
easy to interpret by humans is preferable. In that case, it will likely be easier to use
or to devise novel features ﬁtting to the clustering schemes that are available, than to
design novel clustering schemes for the available features. An overview of classical
clustering techniques can be found in [87,135].
To restore the spatial continuity in Sect. 16.5, alternative approaches to the
scheme discussed above could be used. These include the use of stochastic relax-
ation and Markov random ﬁelds MRF [81].

17
Region and Boundary Descriptors
In this chapter we discuss images consisting of two colors (or gray values), so called
binary images. Such images are typically the result of a computer processing, e.g.,
applying a threshold, but they can also be semiautomatically or manually produced
by humans in artistic activities, e.g., graphics with regions having the same color,
such as logos or cartoon images. First, we introduce some general tools from mathe-
matical morphology that can be useful in binary image processing. Then we elaborate
on how to label the regions with the purpose to identify them for further processing.
This will be done within the framework of morphological ﬁltering. Subsequently,
we describe elementary shape features of a region, assuming that the regions have
been individually labelled. In the ﬁnal two sections we present a more systematic ap-
proach to describe the shape of a region with possibility to increase the description
power systematically, i.e., complex moments for regions and Fourier descriptors for
boundaries.
17.1 Morphological Filtering of Regions
A binary image is an image which has only two colors in it. Without loss of generality
we call these as 0 and 1 respectively.
We start with continuous morphological operations and then focus on binary ver-
sions of them, as this has some pedagogical beneﬁts. The functions max(f) and
min(f) correspond to the largest and smallest value of the image f within the re-
gion, where the image is deﬁned as the basic operators in this theory. In applications,
typically f is a local image, which means that max and min will be applied to all
points of a local image, producing a (nonlinearly) ﬁltered image. In consequence,
there is a neighborhood function, g(r), also called a morphological ﬁlter, or struc-
ture element, associated with such uses of max and min, delivering the result after
a pointwise product with the original function. To mark the ﬁltering aspect that ne-
cessitates the ﬁlter g in their use, these operators are called dilation and erosion,
respectively, and are deﬁned as:

360
17 Region and Boundary Descriptors
Fig. 17.1. (Left) A binary image and (right) a morphological ﬁlter. Red and blue represent 1
and 0 respectively
(f ⊕g)(r) = max
r′
f(r −r′)g(r′)
Dilation
(17.1)
(f ⊖g)(r) = min
r′ f(r −r′)g(r′)
Erosion
(17.2)
where r′ is varied through all admissible points of the image f. With this deﬁnition,
dilation and erosion differ from convolution only in that the integration/summation
is replaced by max and min functions. Each of these operations qualiﬁes to be called
a ﬁltering. These ﬁlterings are nonlinear and can be applied several times, one after
the other and in various orders, to achieve a particular purpose, e.g., to remove a
speciﬁc type of noise or to smooth boundaries of regions. Collectively, these opera-
tions are called morphological ﬁltering. Since max and min functions are nonlinear,
with scant knowledge on them compared to linear functions, more user experience
on morphological operators in discrete image representation is both necessary and
crucial for their effective application, compared to linear operators. Accordingly, we
ﬁrst assume that r obtains discrete values, i.e., we have a grid by rk, and we discuss
only images and ﬁlters that are binary, assuming that they take the values 1 or 0.
Examples of ﬁlters yield
G1 =
⎛
⎝
1 1 1
1 1 1
1 1 1
⎞
⎠
G2 =
⎛
⎝
1 1 1
1 0 1
1 1 1
⎞
⎠
G3 =
# 1 1 1$
G4 =
⎛
⎝
1
0
1
⎞
⎠
(17.3)
The deﬁnition of ⊕and ⊖for discrete, multidimensional functions follows from
Eqs. (17.1) and (17.2) except that now f and g are discrete binary images that we
choose to represent by F and G for convenience. In 2D, these functions are deﬁned
on a 2D grid, a matrix, whereas in 3D and higher dimensions the functions are de-
ﬁned on discrete arrays that are multidimensional grids. Given a discrete grid and
the binary image deﬁned on it, the morphological operations are then deﬁned as:
(F ⊕G)(rk) = max
rl
F(rk −rl)G(rl)
(17.4)
(F ⊖G)(rk) = min
rl F(rk −rl)G(rl)
(17.5)

17.1 Morphological Filtering of Regions
361
where rl is the index vector of the grid (row and column indices for a matrix) of an
element, G, with G(rl) representing the value of the element. When F(rk −rl) is
undeﬁned for a given rk −rl, e.g., because it is a point outside of an image, usually
an assumption such as that it is zero is made. The deﬁnition above has provisions
that morphological operations can be applied to the discrete gray image F. This is
because one can measure the local maximum and minimum after their elements are
modiﬁed due to a pointwise multiplication by G. However, we will refrain from
discussing gray images here to limit the scope only to images where both F as well
as G are binary. In all cases, the purpose of the neighborhood is twofold:
•
to mark the points that participate into max and min operations
•
to weight them within the allowed region
Some care should be taken when marking and computing the points of local images
because weighting with zero is not the same as excluding the point from the local
image, although this is normally what the user wishes.
Whether or not a zero of the ﬁlter is to be interpreted as a weight or a region
marker matters in morphological operations. For example, a weight of zero forces
the min operation to always produce zero on binary images, which consist of ones
and zeros, regardless of the other coefﬁcients of the ﬁlter and even regardless the
input image, whereas interpreting the same zeros as region excluders would mean
that the corresponding points do not participate in min calculations. The zero co-
efﬁcients of ﬁlters used under max, min thus behave differently in comparison to
ﬁlters used in summation operations of convolutions. This is because a sum does not
change, regardless of whether the zero coefﬁcients are markers, or that the respective
image points participate to summation with the weight zero. To avoid the interpre-
tation ambiguities, it is practical to assume that the morphological ﬁlter coefﬁcients
as markers rather than weights, i.e., a coefﬁcient zero is a region-excluding marker,
whereas a coefﬁcient one is a region-including marker.
What ⊕and ⊖really do to images becomes more intuitive when they are applied
to an image. Figure 17.1 shows a binary image F and a ﬁlter G, which is the ﬁlter
shown as G3 in Eq. (17.3). The color code is such that blue represents 0 and red
represents 1.
For convenience we used light red and light blue colors in illustrations. They
represent 1 and 0, respectively, too, but they are chosen so to mark points that have
changed their values as compared to the original. As seen from these results, the
dilation operator dilates the image in a way that is consistent with the direction of
the ﬁlter. There is an enlargement of the objects, but it is systematically in the direc-
tion of the ﬁlter. There is no dilation of horizontal edges. The erosion ﬁltering acts
analogously, but it reduces the objects. We can draw several conclusions from this:
1. Objects that are too close to each other are merged after a dilation, e.g., the letter
¨A and the dot “.” in the example.
2. If there is a dominant ﬁlter direction, then dilation has a directional preference
too, e.g., a horizontal ﬁlter dilates edges with horizontal direction.
3. Objects smaller than the ﬁlter disappear under erosion, e.g., the dot “.” in the
example.

362
17 Region and Boundary Descriptors
Fig. 17.2. (Left) The result of applying the dilation, F⊕G and (right) the erosion F ⊖G
operators when using the image F and the ﬁlter G shown in Fig. 17.1. Red and blue represent
1 and 0, respectively. The light red and the light blue represent 1 and 0, respectively, but they
are chosen so to mark points that have changed their values as compared to the original
4. If there is a dominant ﬁlter direction, then erosion has a directional preference
too, e.g., a horizontal ﬁlter erodes edges with horizontal direction.
5. If the background pixels are called 1 and the object pixels 0, then the roles of
dilation and erosion are reversed.
Hit–Miss Transform
To detect binary objects of a speciﬁc shape and size, the hit–miss transform can
be utilized. It exploits the fact that erosion removes only objects “smaller” than the
“target”. To ﬁx the ideas we illustrate it by:
G = (1, 1, 1)T
(17.6)
so that our goal is to erase every object that does not equal G. The center pixel is
marked as boldface for convenience to mark that it is the point that represents the
position of the object.The hit–miss transform is implemented as follows.
1. Delete all objects smaller than the target. This is achieved by
H = F ⊖G
(17.7)
where G is the target. This step is illustrated by Fig. 17.2 (right).
2. Delete all “objects” larger than the target. This is achieved by
R = F ⊖G
(17.8)
where · represents the conjugate operation F = 1 −F, having the effect that all
ones become zeros and zeros become ones. For the ﬁlter this is true too, but it
would result in trivial ﬁlters if certain rules were not observed. To avoid this, the
ﬁlter is assumed to be limited by a thin boundary of zeros that has no effect on

17.1 Morphological Filtering of Regions
363
Fig. 17.3. (Left) The complement of the original image, F and (right) the complement of the
target to be used as (an erosion) ﬁlter G
the operation of G it is important when computing the complement G = 1−G.
Accordingly, our example G yields
G = 1 −G =
⎛
⎝
1 1 1 1 1
1 0 0 0 1
1 1 1 1 1
⎞
⎠
(17.9)
The binary image F and the morphological ﬁlter G are illustrated in Fig. 17.3.
This image, eroded with the shown ﬁlter, results in the binary image R, which
is shown in Fig. 17.4 (left).
3. Multiply pointwise the images H and R obtained in the steps above:
S = H ⊙R
(17.10)
where ⊙represents a pointwise multiplication. Note that pointwise multiplica-
tion of two matrices (which must have the same size) is different than multipli-
cation of two matrices, in that the former is achieved by multiplying two entries
from the same row and column from the input matrices and assigning it to the
corresponding entry of the resulting matrix. Because only ordinary multiplica-
tion between ones and zeros are involved, this step is equivalent to a logical AND
operation1 between the pixel values of H and R. The step is illustrated by Fig.
17.4 (right), which is the result of applying ⊙between Fig. 17.2 (right) and Fig.
17.4 (left). It shows the result of the hit–miss transform, which marks the found
target object (as deﬁned by the ﬁlter) red.
For a detailed discussion of morphological operators and skeletonization we refer
to [32,33,35,183–185,197].
1 In some studies AND, alternatively ⊙, is represented by ∩.

364
17 Region and Boundary Descriptors
Fig. 17.4. (Left) The image R that is obtained by erosion between the image and the ﬁlter
shown in Fig. 17.3. The light red, representing 1, and the light blue representing 0, show
the changes introduced by the erosion. (Right) The image S that is obtained by a pointwise
multiplication between the left image and the right image of Fig. 17.2
17.2 Connected Component Labelling
For convenience, let 0 represent the color of the background. A connected compo-
nent is the collection (region) of all points having the color 1 such that they are
attached, i.e., if two points of the region are not already neighbors, there is a path
through neighboring points belonging to the region that can join any two points of
the region. We assume 8-connected neighbors, N8(rk), for convenience but a 4-
connected2 neighbors assumption is also possible. Evidently, in 3D binary images,
the neighborhood assumptions are more complex, because a point in a discrete 3D
image, called a voxel, can have face, edge, and vertex neighbors. We refer to [34,35]
for a detailed discussion of binary image processing in higher dimensions. To ﬁx
the ideas, a connected component is typically a region, also called an object, which
has one closed boundary with pixels having the same color. However, more than one
closed boundary is possible, e.g., the letter “e” in Fig. 17.5. Accordingly, it is pos-
sible to assign a unique label to each component. For simplicity one can use colors
to represent the labels e.g., as in Fig. 17.5. A method achieving this task is called
connected component labelling. This is not just an image-understanding problem but
also a computational physics problem, e.g., [7].
There are several techniques achieving connected component labelling [213] in
2D, differing in computational efﬁciency and computer architectures for which they
are intended. The differences are at the implementation level because there is, given
the neighborhood type, noroom for ambiguity on whether or not the labelling has
been achieved correctly. We outline the conventional method of doing such a la-
belling in 2D below because of its simplicity although there are other ways of achiev-
ing connected components labelling that may be more efﬁcient [213].
The algorithm in [95] scans through a binary image f(ri) in the forward and the
backward raster directions alternately. Assume that f(ri) is a 2D (discrete) image
2 In 4-connected neighbors, N4(rk), only points that share an edge with a pixel as neigh-
bors, i.e., top, down, left, and right pixels, are neighbors. An 8-connected neighborhood
assumption admits, additionally, the adjacent 4-diagonal pixels.

17.2 Connected Component Labelling
365
that consists of pixel values 1, indicating connected components (object regions),
and 0, indicating the background. The ﬁrst two labels, m = 0 and m = 1, are as-
signed to the background and to the ﬁrst object as soon as the ﬁrst 0-valued pixel and
the ﬁrst 1-valued pixel, respectively, are encountered in the forward raster direction
(reading direction). These pixels will obtain the labels 0 and 1, which is the same as
their pixel values. Then the following operations in the forward raster scan order are
performed using the neighborhood ﬁlter, as is shown in Eq. (17.11) for 8-connected
neighborhoods. The ﬁlter is to be interpreted as a mask, i.e., the 0 elements mark the
“don’t care” pixels, which do not participate into the min operation below:
L(ri) =
⎧
⎨
⎩
0,
if
F(ri) = 0,
m;
m = m + 1,
if
{for all k : L(ri−k)G(rk) = 0},
mink L(ri−k)G(rk),
otherwise.
(17.11)
where the mask G for the forward scan (right–down direction) yields:
G =
1 1 1
1 0 0

(17.12)
Here the 0 elements of the mask mark the “don’t care” pixels, that is, they are not
meant to be multiplied with the (label) values of L in Eq. (17.11). Conversely, the
ones of the mask mark the points to be formally multiplied by one, although this
may be replaced by other instructions for efﬁciency. The boldfaced zero element
represents the current point, ri when moving the mask over the label image L. Notice
that the mask is only moved over the label image being constructed. Consequently,
the current point as well as the future point in the scan direction are “don’t care”
values since they are marked with zero.
The top condition of Eq. (17.11) is usually the one that is most frequently entered.
This is because it can be activated by the current point ri being a background point,
without any regard to the values of the neighboring pixels within the G(ri−k). This
branch aims to keep a background pixel as a background pixel in the scan direction.
A necessity for the second condition to be active is that the current pixel value in
the binary image, F(ri), be nonzero, even if this is not tested in this branch. This
is because if the current pixel is zero then it is caught by the condition of the ﬁrst
branch. Accordingly, the second condition is active if and only if the current pixel
value is an object pixel, and all of the neighboring pixels in the mask marked with 1,
are already assigned to the background. When this happens the current point gets the
label m, which is immediately incremented so that every time this branch is entered,
the output label is a new label. The third condition is active when we have that the
current pixel is an object pixel and one of the neighboring pixels, marked with 1
by the mask, has an (old) object label, i.e., m > 1. In this case no new labels are
generated for the current point, but the minimum label of the neighboring pixels is
assigned to the current point. Accordingly, the smaller labels invade the connected
component.
After the forward pass, the following operations in the backward raster scan order
are applied:

366
17 Region and Boundary Descriptors
Fig. 17.5. (Left) A binary image with several regions. (Right) A connected component la-
belling where colors represent labels
L(ri) =
 0,
if
L(ri) = 0,
mink(L(ri−k)G(rk)),
otherwise,
(17.13)
where the mask G for the backward scan (right–down) is given by
G =
0 0 1
1 1 1

(17.14)
The forward and the backward scans are alternately applied until no labels change in
the label image L.
Figure 17.5 shows a binary image consisting of the word “Direction”. Each letter
consists of one connected component except the letter “i” which consists of two.
Accordingly, the connected component labelling comes out with 11 regions, each
having its own unique label, shown as color of pixels. Using connected components,
one can apply further processing to individual regions, e.g., measuring their shape
properties, as we will discuss in what follows.
17.3 Elementary Shape Features
In practice, there exist a number of adhoc measures that may be sufﬁcient to resolve
shape-related recognition issues. We list some of these descriptors in the following.
We assume that the image is f and that it contains one connected component D,
which is to be described w.r.t. its shape primitives. If there is more than one region
in the image, it is split up to several images so that there is no more than one re-
gion in each by connected component labelling. The points inside the region D are

17.3 Elementary Shape Features
367
collectively called the object whereas those outside are called the background. For
convenience, we also assume that f = 1 for object points and f = 0 for the back-
ground.
Area
Formally, this is given by
A =

D
f(x, y)dxdy
(17.15)
which in practice, translates to the count of points with f = 1.
Perimeter
The formal deﬁnition of the perimeter is
P =

∂D
f(x(s), y(s))ds
(17.16)
where ∂D is the boundary of the region D and (x(s), y(s)) is a parametrization of
the boundary curve. In practice, one decides on a neighborhood connectivity type,
e.g., 8- or 4- connectivity in 2D, then counts neighborhoods containing both points
with f = 1 and f = 0.
Bounding Box
This is the tightest “cuboid” that contains an image volume. For a 2D region, this is
given by the rectangle represented by a four-tuple:
(min
rk X(rk), min
rk Y (rk), max
rk X(rk), max
rk Y (rk))
(17.17)
where X(rk) and Y (rk) represent the coordinates of image points rk in the region.
Bounding boxes can be used for having a simple idea on the shape of a region, i.e.,
elongated versus compact shape. It is also used for avoiding collisions, i.e. if two
regions that move have nonoverlapping bounding boxes then they are not in collision.
Circularity/Compactness
The dimensionless ratio deﬁned via the perimeter and the area is called circularity or
compactness, C:
C = P 2
A
(17.18)
It has a lower bound. In 2D it satisﬁes 4π ≤C and reaches its minimum when the
region is a circle.

368
17 Region and Boundary Descriptors
17.4 Moment-Based Description of Shape
The region to be described here may contain varying gray shades as opposed to many
simple shape descriptors. In that case, the entire function f is assumed to represent
the region. Otherwise, the function takes the value f = 1 inside the region to be
described and f = 0 outside. A function f can be translated via
f

x −m10
m00
, y −m01
m00

(17.19)
where m10, m01, m00 are the real moments of f given by Eq. (10.9). The centroid of
the resulting image coincides with the new origin because ( m10
m00 , m01
m00 )T represents
the position of the centroid in the above equation. Accordingly, we assume that f’s
centroid is already brought to the origin, making f translation-invariant. Because of
this, the complex moments of f,
Ipq(f) =
 
(x + iy)p(x −iy)qf(x, y)dxdy
are translation-invariant, too. We investigate now how
Ipq(f) =
 
rp exp(ipθ)rq exp(−iqθ)f(x(r, θ), y(r, θ))rdrdθ
=
 
rp+q exp(i(p −q)θ)f(x(r, θ), y(r, θ))rdrdθ
(17.20)
where x(r, θ) = r cos(θ) and y(r, θ) = r sin(θ), will be affected by a rotation and
scaling. In consequence of Eq. (17.20), the complex moment of a function deﬁned in
polar coordinates, f(r, θ), will be given by
Ipq(f) =
 
rp+q exp(i(p −q)θ)f(r, θ)rdrdθ
(17.21)
For convenience, we assume below that f is given in polar representation as f(r, θ)
with its centroid at the origin. To discuss how complex moments transform under a
rotation and scaling transformation, we use primed variables for the variables after
the CT, i.e., r′ = αr where α > 0 and θ′ = θ + ϕ. The transformed function f ′, is
thus obtained from f via f ′(r′, θ′) = f( r′
α , θ′ −ϕ). Using Eq. (17.21), a complex
moment of the transformed image yields
Ipq(f ′) =
 
r′p+q exp(i(p −q)θ′)f ′(r′, θ′)r′dr′dθ′
=
 
r′p+q exp(i(p −q)θ′)f(r′
α , θ′ −ϕ)r′dr′dθ′
=
 
(αr)p+q exp(i(p −q)θ) exp(i(p −q)ϕ)f(r, θ)αrαdrdθ
= αp+q+2 exp(i(p −q)ϕ)
 
rp+q exp(i(p −q)θ)f(r, θ)rdrdθ
= αp+q+2 exp(i(p −q)ϕ)Ipq(f)
(17.22)

17.4 Moment-Based Description of Shape
369
We summarize this result as a theorem.
Theorem 17.1. When f(r, θ), having its centroid at the origin, is transformed to
f ′(r′, θ′) through a CT consisting of θ′ = ϕ + θ, and r′ = αr, the complex moments
are invariant up to a multiplication with a constant (complex) scalar,
Ipq(f ′) = αp+q+2 exp(i(p −q)θ)Ipq(f)
(17.23)
♦
This result, albeit with different notation, is due to Reddi [188] and simpliﬁes the
derivation of scale and rotation-invariant forms of moments [42, 89, 112, 190, 234]
discussed below. Most important, however, the theorem shows that the complex mo-
ments are nearly invariant to rotation and scaling because the effects of these are
no more than a multiplicative scalar on the complex moments. Furthermore, one
can always increase the descriptive power of the invariants systematically, by using
high-order complex moments.
Hu’s moment invariants
The following quantities
H1 = m20 + m02
H2 = (m20 −m02)2 + 4m2
11
H3 = (m30 −3m12)2 + (3m21 −m03)2
H4 = (m30 + m12)2 + (m21 + m03)2
H5 = (m30 −3m12)(m30 + m12) · [(m30 + m12)2 −3(m21 + m03)2] + · · ·
+(3m21 −m03)(m21 + m03) · [3(m30 + m12)2 −(m21 + m03)2]
H6 = (m20 −m02) · [(m30 + m12)2 −(m21 + m03)2] + · · ·
+4m11(m30 + m12)(m21 + m03)
H7 = (3m21 −m03)(m30 + m12) · [(m30 + m12)2 −3(m21 + m03)2] + · · ·
−(m30 −3m12)(m21 + m03) · [3(m30 + m12)2 −(m21 + m03)2]
where mpq represent the real (central) moments, were suggested by Hu [112] as
rotation-invariant measures. We will shortly see that this is indeed the case. We write
down the complex moments so as to express H1 · · · H7 via the complex moments:
I11 = m20 + m02 = d
I20 = m20 −m02 + i2m11 = c + ic′
I12 = m30 + m12 −i(m21 + m03) = b −ib′
I30 = m30 −3m12 + i(3m21 −m03) = a + ia′
yielding

370
17 Region and Boundary Descriptors
I2
12 = (b2 −b′2) −i2bb′
I20I2
12 = (c + ic′)I2
12 = c(b2 −b′2) + 2c′bb′ + i[c′(b2 −b′2) −i2cbb′]
I3
12 = (b3 −3bb′2) + i(b′3 −3b2b′)
I30I3
12 = (a + ia′)I3
12 = ab(b2 −3b′) −a′b′(b′2 −3b2) +
+i[a′b(b2 −3b′) −ab′(3b2 −b′2)]
We identify then H1 · · · H7
H1 = d = I11
H2 = c2 + c′2 = |I20|2
H3 = a2 + a′2 = |I30|2
H4 = b2 + b′2 = |I12|2
H5 = ab(b2 −3b′) −a′b′(b′2 −3b2) = ℜ(I30I3
12)
H6 = c(b2 −b′2) + 2c′bb′ = ℜ(I20I2
12)
H7 = a′b(b2 −3b′) −ab′(3b2 −b′2) = ℑ(I30I3
12)
so that
H1 = I11
(17.24)
H2 = |I20|2
(17.25)
H3 = |I30|2
(17.26)
H4 = |I12|2
(17.27)
H5 = ℜ(I30 · I3
12)
(17.28)
H6 = ℜ(I20 · I2
12)
(17.29)
H7 = ℑ(I30 · I3
12)
(17.30)
By inspection and using the above theorem, it then follows that these scalars are
rotation-invariant. Using the theorem again, the normalized quantities
H′
2 = H2
H2
1
(17.31)
H′
3 = H3
H2.5
1
(17.32)
H′
4 = H4
H2.5
1
(17.33)
H′
5 = H5
H5
1
(17.34)
H′
6 = H6
H3.5
1
(17.35)
H′
7 = H7
H5
1
(17.36)
will make these invariants also scale-invariant.

17.5 Fourier Descriptors and Shape of a Region
371
b(m)=x(m)+i⋅y(m)
Fig. 17.6. The circles mark the data points represented by b(m). They sample the boundary
of a region, a closed curve
17.5 Fourier Descriptors and Shape of a Region
Let
b(m) = x(m) + iy(m),
with
m ∈0, · · · N −1,
(17.37)
be a sequence of complex numbers representing coordinates of boundary points of a
region see Fig. 17.6. Regions have closed curves as boundaries and as such they are
periodic,
b(m) = b(m + kN),
with
k ∈0, ±1, ±2, · · · .
(17.38)
They can accordingly be expanded in a suitable Fourier basis. Notice that b(m) does
not necessarily sample the boundary in an equidistant manner. Since it is a discrete
sequence, the expansion coefﬁcients are easily found by DFT. Accordingly, we have
the DFT pair:
B(n) = 1
N
N−1

m=0
b(m) exp

−imn2π
N

(17.39)
with the inverse transform that can synthesize b(m) from B(n)
b(m) =
N−1

n=0
B(n) exp

imn2π
N

(17.40)
without loss. The coefﬁcients B(n) are called the Fourier descriptors3 (FD) of the
boundary b(m). Because, b is periodic with N, m follows a circular topology, al-
lowing us to write the synthesis formula symmetrically:
3 Because FDs are DFT coefﬁcients, the index n in B(n) is cyclical, i.e., it follows the rules
of modulo arithmetic when it is an integer other than 0, · · · , (N −1).

372
17 Region and Boundary Descriptors
b(m) =
N−1
2

n=−N−1
2
B(n) exp

imn2π
N

,
with
K ∈1, 2, · ≤N −1
2
,
(17.41)
where we assumed that N is odd, for simplicity. The synthesis can be truncated at
K ≪N/2, in which case we obtain a smoother approximation of the boundary curve
˜b(m) =
K

n=−K
B(n) exp

imn2π
N

,
with
K ∈1, 2, · ≪N −1
2
,
(17.42)
because ˜b lacks high-frequency terms. Such a truncation is often utilized in image
analysis, e.g., to reduce the number of FDs, which are in turn utilized in shape-based
recognition or discrimination. Smoothing via truncations is illustrated by Fig. 17.7
where we show the boundary approximation of the continents by FDs. The total
available FDs is equal to the number of boundary samples. Note that intricate curves
need more samples, for a faithful reconstruction.
The FDs have a number of desirable properties that we list below.
•
The centroid of the boundary is given by B(0)
B(0) = 1
N
N−1

m=0
b(m) exp

−im02π
N

= 1
N
N−1

m=0
b(m)
(17.43)
•
All FDs except B(0) are translation-invariant. To see this, we apply a translation
Δb = Δx + iΔy to the boundary and obtain the new coordinates as b′(m) =
b(m) + Δb having the FDs:
B′(n) = 1
N
N−1

m=0
b′(m) exp(−imn2π
N )
= 1
N
N−1

m=0
(b(m) + Δb) exp(−imn2π
N )
= 1
N
N−1

m=0
b(m) exp(−imn2π
N ) + Δb · 1
N
N−1

m=0
exp(−imn2π
N )
= 1
N
N−1

m=0
b(m) exp(−imn2π
N ) + Δbδ(n)
= B(n) + Δb · δ(n)
(17.44)
Accordingly, to obtain shift-invariant FDs one can ignore B(0), the centroid.
•
Assume that b′ is a scaled version of the boundary, so that b′ = αb with α being
a positive real scalar. Then

17.5 Fourier Descriptors and Shape of a Region
373
B′(n) = 1
N
N−1

m=0
b′(m) exp

−imn2π
N

= 1
N
N−1

m=0
αb(m) exp

−imn2π
N

= αB(n)
(17.45)
Provided that |B′(1)| > 0, one can compute
B′(n)
|B′(1)| = αB(n)
α|B(1)|
(17.46)
which is scale-invariant. If |B′(1)| ≈0 then another FD can be used in the
normalization, yielding more stable scale-invariant features.
•
Assume that b′ is a rotated version of the boundary, so that b′ = exp(iθ)b with θ
being an arbitrary angle. Then
B′(n) = 1
N
N−1

m=0
b′(m) exp

−imn2π
N

= 1
N
N−1

m=0
exp(iθ)b(m) exp

−imn2π
N

= exp(iθ)B(n)
(17.47)
From this it follows that |B′(1)| = |B(1)|, so that if |B′(1)| > 0, one can, by
division of both sides of Eq. (17.47), obtain for n = 1:
B′(1)
|B′(1)| = exp(iθ) B(1)
|B(1)|
(17.48)
Accordingly, we can use this ratio to normalize B′(n) for n > 1:
B′(n)
B′(1)
|B′(1)|
= exp(−iθ) B∗(1)
|B(1)| exp(iθ)B(n) = B∗(1)
|B(1)|B(n)
(17.49)
to obtain rotation-invariant FD features. As before, if |B′(1)| ≈0 then another
FD can be used in the normalization, yielding more stable rotation-invariant fea-
tures. One can also use |B′(n)| to achieve rotation-invariance, but these features
annihilate ≈N
2 freedoms, far too much description power of FDs than necessary
for many applications.
•
Because, B(1) contains both scale and rotation parameters, to achieve both scale
and rotation-invariance in FDs one can normalize the other FDs by this complex
scalar
B′(n)
B′(1) ,
with
n = 2, 3, · · · N −1,
(17.50)
provided that it is nonzero (else one can use another FD that has a large magni-
tude).

374
17 Region and Boundary Descriptors
To summarize, to achieve translation, rotation, and scale-invariance jointly, one
can ignore B(0) and normalize all other FDs by another B(n) that has a large
magnitude, usually B(1). In that one ignores B(0) and B(1), containing four free-
doms. This is economical because it corresponds precisely to translation- (two free-
doms), rotation- (1 freedom), and scale-invariance (one freedom). Note that the in-
dex n ∈0 · · · N −1 runs over the full range of the coefﬁcients so that the negative
frequencies have the indices N −n according to the cyclical translation rules of
DFT. Whereas |B(n)| ̸= |B(−n)| because b(k) are not real, in practical applications
|B(n)| still decays as the index n increases through 0, ±1, ±2, ±3, · · · . Accordingly,
when truncating B(n), the coefﬁcients are kept symmetrical w.r.t. n = 0, as is done
in Fig. 17.7.

17.5 Fourier Descriptors and Shape of a Region
375
100 %
50.0 %
25.0  %
12.5 %
100 %
50.0 %
25.0  %
12.5 %
100 %
50.0 %
25.0  %
12.5 %
100 %
50.0 %
25.0  %
12.5 %
100 %
50.0 %
25.0  %
12.5 %
100 %
50.0 %
25.0  %
12.5 %
Fig. 17.7. The images illustrate some region boundary curves synthesized by FDs truncated
at 100%, 50%, 25%, and 12.5% of the available FDs

18
Concluding Remarks
The term vision encompasses much more than registering the distribution of light by
our eyes or by a camera. It refers to a process that analyzes what has been captured,
so as to facilitate actions and inferences. In this book, we anatomized the observed
light distribution by using multidimensional signal analysis tools and provided some
illustrations of the latter in terms of applications, e.g., tracking, feature extraction,
segmentation, texture description, 3D geometry reconstruction, face recognition, and
ﬁngerprint recognition. In many of these applications humans are experts, almost by
birth, and yet we do not have detailed knowledge on how this amazing performance
is achieved. Biased by this performance, the uninitiated can therefore not be blamed
for underestimating the challenges of image analysis, whether the issues are to be
addressed by her/his own visual system or by a computer.
When a computing system has to solve the same vision problem for multiple ap-
plications in changing environments, the versatility and robustness demands quickly
become challenging. In this book, we discussed certain image analysis concepts and
tools in an attempt to make them useful in many scenarios. Intelligent vision systems
in changing environments are increasingly in demand not only because humans are
increasingly online while moving, but also because we want mobile machines to be
online and autonomous. Just as a hunting man must carefully consider what tools to
carry, so must a vision system that must function in changing environments be selec-
tive in its choice of processing tools because of the extreme resource limitations the
mobility demands.
In the face of undisputable evidence being accumulated, there is hardly a need to
argue about the existence of sophisticated signal processing in human vision, solely
devoted to directional processing. Such a mechanism, which is the sine qua non of
the most advanced mobile organisms, including the human, hunting or not, is at ﬁrst
counterintuitive due to its computational “heaviness”. This is because directional
processing amounts to increasing the dimension of the visual signal in a hefty man-
ner. It is almost an explosion of data that takes place—and that routinely, before
any simpliﬁcation, e.g., decision, takes place. Motivated by its imposing presence in
biological vision, this book attempts to simplify and unify the modern directional
signal processing. It attempted to reconcile some of the most utilized principles in

378
18 Concluding Remarks
machine vision, e.g., the generalized Hough transform, with directional processing.
In the text, the belief that most vision problems can be solved by systems mimick-
ing the known processing of human vision in their frontend is not concealed. In this
euphoria, it would nevertheless be a serious mistake to pretend that all methods and
principles to achieve human visual intelligence are covered, not least because there
are too many of these yet to be discovered.
The dimension explosion in the visual pathways is not an unknown phenomenon
in the mathematical treatment of signals. It goes under the general notion of feature
extraction. It is nearly routinely demanded in all advanced decision support systems
because each dimension brings a simpliﬁcation, not available in the original signal.
In comparison, it is worth noting that an image ﬁltered through a particular direc-
tional ﬁlter is more predictable than the original, and therefore less complex. The
ﬁltered image, in its essence 1D, e.g., see [163], is more likely constant in the tune-
on direction than the orthogonal direction because of the speciﬁc ﬁltering. Several
chapters of the book treat therefore principles that are also found in statistics liter-
ature. However, the visual signals have unique properties which can allow a vision
system to limit its feature extraction to directional processing as a resource manage-
ment strategy to cope with multiple environments.
In English and in numerous other languages, the notion of vision is also synony-
mous with qualiﬁed imagination about the future. However, predicting the future of
visual signal analysis as a science is a feat even larger than solving the problems of
vision. Accordingly, what I expressed in this book cannot be anything but my vision
of the matter, which may not be perceived as neutral—it is necessarily directional.

References
1. Y.I. Abdel-Aziz and H.M. Karara. Direct linear transformation into object space coordi-
nates in close-range photogrammetry. In Proc. Symposium on Close-Range Photogram-
metry, Urbana, IL, Jan., pages 1–18, 1971.
2. Y.S. Abu-Mostafa and D. Psaltis. Recognitive aspects of moment invariants. IEEE-
PAMI, 6:698–706, 1984.
3. E.H. Adelson and J.R. Bergen. Spatiotemporal energy models for the perception of
motion. J. of the Optical Society of America A, 2(2):284–299, 1985.
4. D.G. Albrecht and R.L. de Valois. Striate cortex responses to periodic patterns with and
without the fundamental harmonics. J. Physiol., (London), 319:497–514, 1979.
5. A. Aldroubi and H. Feichtinger. Exact iterative reconstruction algorithm for multivariate
irregularly sampled functions in spline-like spaces: The Lp-theory. Proceedings of the
Amer. Math. Soc., 126(9):2677–2686, 1998.
6. B.W. Andrews and D.A. Pollen. Relationship between spatial frequency selectivity and
receptive ﬁeld proﬁle of simple cells. J. Physiol., (London), 287:163–176, 1979.
7. J. Apostolakis, P. Coddington, and E. Marinari. New SIMD algorithms for cluster label-
ing on parallel computers. Int. J. Mod. Phys., C4:749, 1993.
8. K. ˚Astr¨om, R. Cipolla, and P.J. Giblin. Generalised epipolar constraints. In J. O. Ek-
lundh, editor, Computer Vision—ECCV 1996, LNCS 1065, pages 97–108, Springer, Hei-
delberg, 1996.
9. S. Ayer, P. Schroeter, and J. Bigun. Segmentation of moving objects by robust motion
parameter estimation over multiple frames. In J. O. Eklundh, editor, Computer Vision—
ECCV 1994, LNCS 800, pages 316–327, Springer, Heidelberg, 1994.
10. R. Balian. Un principe d’incertitude fort en th´eorie du signal ou en m´ecanique quantique.
C. R. Acad. Sci. Paris, 292(2):1357–1363, 1981.
11. G. Ball and D. Hall. ISODATA, an iterative method of multivariate analysis and pattern
classiﬁcation. In IFIPS Congress, 1965.
12. B.H. Ballard. Generalizing the Hough transform to detect arbitrary shapes. Pattern
Recognition, 13(2):111–112, 1981.
13. G.C. Baylis, E.T. Rolls, and C.M. Leonard. Functional divisions of the temporal lobe
neocortex. J. Neuroscience, 7:330–342, 1987.
14. P.A. Beardsley, A. Zisserman, and D.W. Murray. Sequential updating of projective and
afﬁne structure from motion. Int. Journal of Computer Vision, 23(3):235–297, 1997.
15. M. Bertero, T. Poggio, and V Torre. Illposed problems in early vision. Proceedings of
the IEEE, 76(8):869–889, 1988.

380
References
16. J.C. Bezdek. Pattern recognition with fuzzy objective function algorithm. Plenum, New
York, 1981.
17. J. Bigun. Pattern recognition by detection of local symmetries. In E.S. Gelsema and
L.N. Kanal, editors, Pattern recognition and artiﬁcial intelligence, pages 75–90. North-
Holland, 1988.
18. J. Bigun. Recognition of local symmetries in gray value images by harmonic functions.
In Ninth International Conference on Pattern Recognition, Rome, Nov. 14–17, pages
345–347. IEEE Computer Society, 1988.
19. J. Bigun. A structure feature for some image processing applications based on spiral
functions. Computer Vision, Graphics, and Image Processing, 51(2):166–194, 1990.
20. J. Bigun. Gabor phase in boundary tracking and region segregation. In Proc. DSP &
CAES Conf. Nicosia, Cyprus, July 14-16, pages 229–237. Univ. of Nicosia, 1993.
21. J. Bigun. Speed, frequency, and orientation tuned 3-D Gabor ﬁlter banks and their de-
sign. In Proc. International Conference on Pattern Recognition, ICPR, Jerusalem, pages
C–184–187. IEEE Computer Society, 1994.
22. J. Bigun. Pattern recognition in images by symmetries and coordinate transformations.
Computer Vision and Image Understanding, 68(3):290–307, 1997.
23. J. Bigun, T. Bigun, and K. Nilsson. Recognition by symmetry derivatives and the gen-
eralized structure tensor. IEEE-PAMI, 26:1590–1605, 2004.
24. J. Bigun, K. Choy, and H. Olsson. Evidence on skill differences of women and men
concerning face recognition. In J. Bigun and F. Smeraldi, editors, Audio and Video Based
Biometric Person Authentication—AVBPA 2001, LNCS 2091, pages 44–51, Springer,
Heidelberg, 2001.
25. J. Bigun and J.M.H. du Buf. N-folded symmetries by complex moments in Gabor space.
IEEE-PAMI, 16(1):80–87, 1994.
26. J. Bigun and J.M.H. du Buf. Symmetry interpretation of complex moments and the
local power spectrum. Visual Communication and Image Representation, 6(2):154–163,
1995.
27. J. Bigun, H. Fronthaler, and K. Kollreider. Assuring liveness in biometric identity au-
thentication by real-time face tracking. In International Conference on Computational
Intelligence for Homeland Security and Personal Safety, CIHSPS, Venice, July 21–22,
pages 104–112. IEEE, 2004.
28. J. Bigun and G.H. Granlund. Optimal orientation detection of linear symmetry. In First
International Conference on Computer Vision, ICCV, London, June 8–11, pages 433–
438. IEEE Computer Society, 1987.
29. C.M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Ox-
ford, 1995.
30. M.J. Black and P. Anandan. A framework for the robust estimation of optical ﬂow. In
ICCV-93, Berlin, May 11-14, pages 231–236. IEEE Computer Society, 1993.
31. G.W. Bluman and S. Kumei. Symmetries and differential equations. Springer, Heidel-
berg, 1989.
32. G. Borgefors.
Distance transformations in arbitrary dimensions.
Computer Vision,
Graphics, and Image Processing, 27(3):321–345, 1984.
33. G. Borgefors. Hierarchical chamfer matching: (A) parametric edge matching algorithm.
IEEE-PAMI, 10(6):849–865, 1988.
34. G. Borgefors. Weighted digital distance transforms in four dimensions. Discrete Applied
Mathematics, 125(1):161–176, 2003.
35. G. Borgefors, I. Nystr¨om, and G. Sanniti di Baja. Computing skeletons in three dimen-
sions. Pattern Recognition, 32(7):1225–1236, 1999.

References
381
36. R.K. Bothwell, J.C. Brigham, and R.S. Malpass. Cross-racial identiﬁcation of faces.
Personality and Social Psychology Bulletin, 15:19–25, 1989.
37. P. Brodatz. Textures. Dover, New York, 1966.
38. A.J. Bruce and K.W. Beard. African Americans, and Caucasian Americans recognition
and likability responses to african american and caucasian american faces. Journal of
General Psychology, 124:143–156, 1997.
39. V. Bruce and A. Young. Understanding face recognition. British Journal of Psychology,
77:305–327, 1986.
40. V. Bruce and A. Young. In the eye of the beholder. Oxford University Press, Oxford,
1998.
41. A.H. Bunt, A.E. Hendrickson, J.S. Lund R.D. Lund, and A.F. Fuchs. Monkey retinal
ganglion cells: Morphometric analysis and tracing of axonal projections, with a consid-
eration of the peroxidase technique. J. Comp. Neurol., 164:265–286, 1975.
42. H. Burkhardt. Transformationen zur lageinvarianten Merkmalgewinnung. Habilitation-
sschrift, Universit¨at Karlsruhe, VDI-Verlag, 1979.
43. P. Burt. Fast ﬁlter transforms for image processing. Computer graphics and image
processing, 16:20–51, 1981.
44. S.R.Y. Cajal. Histologie du systeme nerveux de l’homme et des vertebres. Maloine,
Paris, 1911.
45. F.W. Campbell, G.F. Cooper, and C. Enroth-Cugell. The spatial selectivity of the visual
cells of the cat. J. Physiol. (London), 203:223–235, 1969.
46. F.W. Campbell and J.G. Robson. Application of Fourier analysis to the visibility of
gratings. J. Physiol. (London), 197:551–566, 1968.
47. V.A. Casagrande and J.H. Kaas.
The afferent, intrinsic, and efferent connections of
primary visual cortex in primates.
In A. Peters and K. Rockland, editors, Cerebral
Cortex, Vol. 10, pages 201–259, Plenum, New York, 1994.
48. J. W. Cooley and J. W. Tukey. An algorithm for the machine calculation of complex
Fourier series. Mathematics of Computation, 19:297–301, 1965.
49. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273–297,
1995.
50. J. Crowley and H. Christensen. Hardware for active vision systems. Kluwer, 1995.
51. P.E. Danielsson.
Rotation invariant linear operators with directional response.
In
Fifth International Conference on Pattern Recognition, ICPR-80, Miami Beach, Florida,
Dec., pages 1171–1176. IEEE Computer Society, 1980.
52. P.E. Danielsson, Q. Lin, and Q.Z. Ye. Efﬁcient detection of second-degree variations
in 2D and 3D images. Visual Communication and Image Representation, 12:255–305,
2001.
53. I. Daubechies. The wavelet transform, time-frequency localization and signal analysis.
IEEE Trans. on Inf. Theory, 36(5):961–1005, 1990.
54. E.R. Davies. Finding ellipses using the generalised Hough transform. Pattern Recogni-
tion Letters, 9:87–96, 1989.
55. A. Dick, P. Torr, S. Rufﬂe, and R. Cipolla. Combining single view recognition and
multiple view stereo for architectural scenes. In Eighth International Conference On
Computer Vision ICCV–01, July, 9–12, pages 268–274. IEEE Computer Society, 2001.
56. F. Doyle. The historical development of analytical photogrammetry. Photogrammetric
Engineering, 2:259–265, 1964.
57. J.M.H. du Buf. Responses of simple cells: Events, interferences, and ambiguities. Biol.
Cybernet., 68:321–333, 1993.

382
References
58. B. Duc. Motion estimation using invariance under group transformations. In 12th Inter-
national Conference on Pattern Recognition, ICPR–94, Jerusalem, Oct., pages 159–163.
IEEE Computer Society, 1994.
59. B. Duc. Feature design: Applications to motion analysis and identity veriﬁcation. PhD
thesis, Ecole Polytechnique F´ed´erale de Lausanne, 1997.
60. B. Duc, P. Schroeter, and J. Bigun. Motion estimation and segmentation by fuzzy clus-
tering. In International Conference on Image Processing, ICIP-95, Washington D.C.,
Oct. 23–26, pages III–472–475. IEEE Computer Society, 1995.
61. B. Duc, P. Schroeter, and J. Bigun. Spatio-temporal robust motion estimation and seg-
mentation. In Hlavac and Sara, editors, Computer Analysis of Images and Patterns–CAIP
1995, LNCS 970, pages 238–245, Springer, Heidelberg, 1995.
62. R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation. Wiley, New York, 2000.
63. D.E. Dudgeon and R.M. Mersereau. Multi dimensional digital signal processing. Pren-
tice Hall, Englewood Cliffs, N.J., 1981.
64. R. Eagleson. Measurement of the 2D afﬁne Lie group parameters for visual motion
analysis. Spatial Vision, 6(3):183–198, 1992.
65. E.S. Elliott, E.J. Wills, and A.G. Goldstein. The effects of discrimination training on the
recognition of white and oriental faces. Bulletin of the Psychonomic Soceity, 2:71–73,
1973.
66. H.D. Ellis, D.M. Ellis, and J.A. Hosie. Priming effects in children’s face recognition.
British Journal of Psychology, 84:101–110, 1993.
67. B.S. Everitt. Cluster analysis. Heinemann, London, 1974.
68. M.J. Farah. Is face recognition special? evidence from neuropsychology. Behavioral
Brain Research, 76:181–189, 1996.
69. M.J. Farah. The cognitive neuroscience of vision. Blackwell, Maden, 2000.
70. O. Faugeras. Three-Dimensional Computer Vision: A Geometric Viewpoint. MIT Press,
Cambridge, MA, 1993.
71. M. Felsberg and G. Sommer. Image features based on a new approach to 2D rotation
invariant quadrature ﬁlters. In A. Heyden et. al., editor, Computer Vision–ECCV 2002,
LNCS 2350, pages 369–383, Springer, Heidelberg, 2002.
72. M. Ferraro and T.M. Caelli. Relationship between integral transform invariances and
Lie group theory. J. Opt. Soc. Am. A, 5(5):738–742, 1988.
73. D.J. Fleet and A.D. Jepson. Computation of component image velocity from local phase
information. International Journal of Computer Vision, 5(1):77–104, 1990.
74. W. Forstner and E. Gulch. A fast operator for detection and precise location of distinct
points, corners and centres of circular features. In Proc. Intercommission Conference on
Fast Processing of Photogrammetric Data, Interlaken, pages 281–305, 1987.
75. W.T. Freeman and E.H. Adelson. The design and use of steerable ﬁlters. IEEE-PAMI,
13(9):891–906, 1991.
76. H. Fronthaler, K. Kollreider, and J. Bigun. Local feature extraction in ﬁngerprints by
complex ﬁltering. In S. Z. Li et. al., editor, International Workshop on Biometric Recog-
nition Systems–IWBRS 2005, Beijing, Oct. 22–23, LNCS 3781, pages 77–84, Springer,
Heidelberg, 2005.
77. D. Gabor. Theory of communication. Journal of the IEE, 93:429–457, 1946.
78. A. Gagalowicz. Texture modelling applications. The Visual Computer, 3(4):186–200,
1987.
79. I. Gauthier and M.J. Tarr. Becoming a “greeble” expert: Exploring mechanisms for face
recognition. Vision Research, 37:1673–1682, 1997.
80. K.R. Gegenfurtner.
Cortical mechanisms of colour vision.
Nature Reviews Neuro-
science, 4(7):563–572, 2003.

References
383
81. D. Geman and S. Geman. Stochastic relaxation, Gibbs distribution and bayesian restora-
tion of images. IEEE-PAMI, 6(6):721–741, 1984.
82. P. Gerard and A. Gagalowicz. Three dimensional model-based tracking using texture
learning and matching. Pattern Recognition Letters, 21(13-14):1095–1103, 2000.
83. H. Goldstein. Classical Mechanics. Addison-Wesley, Reading, MA, 1986.
84. G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press,
Baltimore, 1989.
85. R.C. Gonzalez and R.E. Woods. Digital image processing. Addison-Wesley, Reading,
MA, 1992.
86. L. Van Gool, T. Tuytelaars, V. Ferrari, C. Strecha, J. Vanden Wyngaerd, and M. Ver-
gauwen. 3D modeling and registration under wide baseline conditions. In Proceedings
ISPRS Symposium on Photogrammetric Computer Vision, pages 3–14, 2002.
87. A. Gordon. Classiﬁcation. Chapman and Hall, New York, 1981.
88. A. Grace and M. Spann. Edge enhancment and ﬁne feature restoration of segmented
objects using pyramid based adaptive ﬁlters. In British Machine Vision Conf., Guildford,
1993.
89. G.H. Granlund. Fourier preprocessing for hand print character recognition. IEEE Trans.
Computers, 21:195–201, 1972.
90. G.H. Granlund. In search of a general picture processing operator. Computer Graphics
and Image Processing, 8(2):155–173, 1978.
91. W.C. Graustein. Introduction to Higher Geometry. MacMillan, New York, 1930.
92. W. Grosky and R. Jain. Optimal quadtrees for image segments. IEEE-PAMI, 5:77–83,
1983.
93. H. Gruner. Photogrammetry: 1776–1976. Photogrammetric Engineering and Remote
Sensing, 43(5):569–574, 1977.
94. O. Hansen and J. Bigun. Local symmetry modeling in multi-dimensional images. Pat-
tern Recognition Letters, 13:253–262, 1992.
95. R.M. Haralick. Some neighborhood operations. Real Time/Parallel Computing Image
Analysis. Plenum, New York, 1981.
96. R.M. Haralick and I. Dinstein. Textural features for image classiﬁcation. IEEE Trans.
Syst. Man and Cybern., 3:610–621, 1973.
97. C. Harris and M. Stephens. A combined corner and edge detector. In Proceedings of the
fourth Alvey Vision Conference, pages 147–151, 1988.
98. R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge
University Press, Cambridge, 2004.
99. M.E. Hasselmo, E.T. Rolls, G.C. Baylis, and V. Nalwa. Object-centered encoding by
face-selective neurons in the cortex in the superior temporal sulcus of the monkey. Ex-
perimental Brain Research, 75:417–429, 1989.
100. M. Hauta-Kasari, W. Wang, S. Toyooka, J. Parkkinen, and R. Lenz. Unsupervised ﬁlter-
ing of munsell spectra. In R.T. Chin and T.-C. Pong, editors, Computer Vision—ACCV
1998, LNCS 1351, pages 248–255, Springer, Heidelberg, 1998.
101. D. J. Heeger. Optical ﬂow from spatio-temporal ﬁlters. In First International Conference
on Computer Vision, ICCV-1, London, June, pages 181–190. IEEE Computer Society,
1987.
102. J. Heikkil¨a and O. Silven. Calibration procedure for short focal length off-the-shelf ccd
cameras. In International Conf. on Pattern Recognition, ICPR-96, pages 166–170. IEEE
Computer Society, 1996.
103. W. Heisenberg. ¨Uber den anschaulichen Inhalt der quantentheoretischen Kinematik und
Mechanik. Z. f¨ur Phys., 43:172–198, 1927.

384
References
104. S.H. Hendry and R.C. Reid.
The koniocellular pathway in primate vision.
Annual
Review of Neuroscience, 23:127–153, 2000.
105. E. Hering. Outlines of a theory of the light sense. Harvard University press, Cambridge,
MA, 1964. (Translation).
106. A. Heyden, G. Sparr, and K. ˚Astr¨om. Perception and action using multilinear forms.
In G. Sommer and J. Koenderink, editors, Algebraic Frames for the Perception–action
Cycle—AFPAC 1997, LNCS 1315, pages 54–65, Springer, Heidelberg, 1997.
107. W.C. Hoffman. The Lie algebra of visual perception. J. Math. Psychol., 3:65–98, 1966.
108. L. Hong, Y. Wand, and A.K. Jain. Fingerprint image enhancement: Algorithm and per-
formance evaluation. IEEE-PAMI, 20(8):777–789, 1998.
109. S.D. van Hooser and S.B. Nelson. Visual system. Encyclopedia of life sciences (online),
2005. Wiley.
110. B.K.P. Horn and B.G. Schunck.
Determining optical ﬂow.
Artiﬁcial intelligence,
17:185–203, 1981.
111. P.V.C. Hough.
Method and means for recognizing complex patterns.
U.S. patent
3,069,654, 1962.
112. M.K. Hu. Visual pattern recognition by moment invariants. IRE Trans. on Information
Theory, pages 179–187, 1962.
113. D.H. Hubel. Eye, brain and vision. Scientiﬁc American Library, 1988.
114. D.H. Hubel and T.N. Wiesel. Receptive ﬁelds of single neurons in the cat’s striate cortex.
J. physiol. (London), 148, 1959.
115. S.van Huffel and J. Wandewalle. The total least squares problem: Computational aspects
and analysis. Frontiers in applied mathematics, 1991.
116. R.A. Hummel. Feature detection using basis functions. Computer Graphics and Image
Processing, 9:40–55, 1979.
117. A. Hyv¨arinen and E. Oja. Independent component analysis: Algorithms and applica-
tions. Neural Networks, 13(4–5):411–430, 2000.
118. J. Illingworth and J. Kittler. The adaptive hough transform. IEEE-PAMI, 9(5):690–698,
1987.
119. M. Irani, B. Rousso, and S. Peleg. Detection and tracking multiple moving objects using
temporal integration. In G. Sandini, editor, Computer Vision—ECCV 1992, LNCS 588,
pages 282–287, Springer, Heidelberg, 1992.
120. ISO-IEC. International standard 13818-2, Information technology generic coding of
moving pictures and associated audio information: Video. Ref. No.: ISO/IEC:13818-2.
ISO, 1996.
121. Special Issue. Brain. Scientiﬁc American, 241(3), 1979.
122. B. J¨ahne. Spatio-Temporal Image Processing. LNCS 751. Springer, Heidelberg, 1993.
123. B. J¨ahne. Digital Image Processing. Springer, Heidelberg, 1997.
124. A.K. Jain and F. Farrokhnia. Unsupervised texture segmentation using Gabor ﬁlters.
Pattern Recognition, 24(12):1167–1186, 1991.
125. M. James. Classiﬁcation algorithms. Collins, London, 1985.
126. S.C. Jeng and W.H. Tsai. Scale-invariant and orientation-invariant generalized Hough
transform - a new approach. Pattern Recognition, 24(11):1037–1051, 1991.
127. D.J. Jobson, Z. Rahman, and G.A. Woodell.
Properties and performance of a cen-
ter/surround retinex. IEEE Trans. on Image Processing, 6(3):451–462., 1997.
128. P. Johansen, M. Nielsen, and O. F. Olsen. Branch points in one-dimensional Gaussian
scale space. Journal of Mathematical Imaging and Vision, 13(3):193–203, December
2000.
129. B. Johansson. Multiscale curvature detection in computer vision. Lic. thesis no. 877;
LIU-TEK-LIC-2001:14, Link¨oping University, 2001.

References
385
130. M.H. Johnson. Developmental cognitive neuroscience. Blackwell, 1997.
131. B. Julesz. Visual pattern discrimination. IRE Trans. on Information Theory, 8:84–92,
1962.
132. K. Kanatani. Group-theoretical methods in image understanding. Springer Series in
Information Sciences. Springer, Heidelberg, 1990.
133. K. Karhunen. Zur Spectraltheorie stochasticher Prozesse. Ann. Acad. Sci. Fennicae, 37,
1947.
134. S. Karlsson and J. Bigun. Texture analysis of multiscal complex moments of the local
power spectrum. Technical Report IDE0574, Halmstad University, November 2005.
135. L. Kaufman and P.J. Rousseeuw. Finding Groups in Data: An Introduction to Cluster
Analysis. Wiley, New York, 1990.
136. J.P. Keener. Principles of applied mathematics. Addison-Wesley, New York, 1988.
137. H. Knutsson. Filtering and reconstruction in image processing. PhD Thesis no:88,
Link¨oping University, 1982.
138. H. Knutsson. Representing local structure using tensors. In Proceedings 6th Scandina-
vian Conf. on Image Analysis, Oulu, June, pages 244–251, 1989.
139. H. Knutsson and G. H. Granlund. Texture analysis using two-dimensional quadrature
ﬁlters. In IEEE Computer Society Workshop on Computer Architecture for Pattern Anal-
ysis and Image Database Management - CAPAIDM, Pasedena, Oct., pages 206–213,
1983.
140. H. Knutsson, M. Hedlund, and G.H. Granlund. Apparatus for Determining the Degree
of Consistency of a Feature in a Region of an Image that is Divided into Discrete Picture
Elements. U.S. patent, 4.747.152, 1988.
141. J.J. Koenderink and A.J. van Doorn. The structure of images. Biological Cybernetics,
50:363–370, 1984.
142. T. Kohonen, E. Oja, O. Simula, A. Visa, and J. Kangas. Engineering applications of the
self-organizing map. Proceedings of the IEEE, 84(10):1358–84, 1996.
143. S.W. Kufﬂer. Discharge patterns and functional organization of mammalian retina. Jour-
nal of Neurophysiology, 16:37–68, 1953.
144. M. Lades, J.C. Vorbruggen, J. Buhmann, J. Lange, C. von der Malsburg, R. P. Hurtz,
and W. Konen. Distortion invariant object recognition in the dynamic link architectures.
IEEE Trans. on Computers, 42(3):300–311, 1993.
145. E.H. Land. The retinex theory of colour vision. Sci. Am., 237:108–129, 1977.
146. E.H. Land. An alternative technique for the computation of the designator in the retinex
theory of color vision. Proc. Nat. Acad. Sci., 83:3078–3086, 1986.
147. D.C. Lay. Linear Algebra and Its Applications. Addison–Wesley, Reading, MA, 1994.
148. F. Leymarie and M.D. Levine. Tracking deformable objects in the plane using an active
contour model. IEEE-PAMI, 15(6):617–634, 1993.
149. Q. Liang and T. Gustavsson. Isointensity directional smoothing for edge-preserving
noise reduction. In ICIP, pages 867–871, 1998.
150. Q. Liang, I. Wendelhag, J. Wikstrand, and T. Gustavsson. A multiscale dynamic pro-
gramming procedure for boundary detection in ultrasonic artery images. IEEE Trans.
Medical Imaging, 19(2):127–142, February 2000.
151. T. Lindeberg. On automatic selection of temporal scales in time-causal scale-space.
In G. Sommer and J. Koenderink, editors, Algebraic Frames for the Perception-Action
Cycle–AFPAC 1997, LNCS 1315, pages 94–113, Springer, Heidelberg, 1997.
152. T. Lindeberg and B. ter H. Romeny. Linear scale-space: I. Basic theory: II. Early visual
operations. Kluwer Academic, Dordrecht, 1994.
153. D. V. Lindley. Making decisions. Wiley, London, 1990.

386
References
154. C. Liu and H. Wechsler. Evolutionary pursuit and its application to face recognition.
IEEE-PAMI, 22:570–582, 2000.
155. M.S. Livingstone and D.H. Hubel. Anatomy and physiology of a color system in the
primate visual cortex. Journal of Neuroscience, 4:309–356, 1984.
156. H.C. Longuet-Higgins. A computer algorithm for reconstructing a scene from two pro-
jections. Nature, 293(133-135), Sep 1981.
157. B.D. Lucas and T. Kanade. An iterative image registration technique with an applica-
tion to stereo vision. In Proc. of the seventh Int. Joint Conf. on Artiﬁcial Intelligence,
Vancouver, pages 674–679, 1981.
158. T.S. Luce. The role of experience in inter-racial recognition. Personality and Social
Psychology Bulletin, 1:39–41, 1974.
159. L. Maffei and A. Fiorentini. The visual cortex as a spatial frequency analyser. Vision
Res., 13:1255–1267, 1973.
160. D. Marr and E. Hildreth. Theory of edge detection. Proc. Royal Society of London
Bulletin, 204:301–328, 1979.
161. G. Medioni, M.S. Lee, and C.K. Tang. A computational framework for segmentation
and grouping. Elsevier, 2000.
162. K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre. XM2VTSDB: The extended
m2vts database. In Audio and Video based Person Authentication - AVBPA99, pages
72–77. University of Maryland, 1999.
163. S. Michel, B. Karoubi, J. Bigun, and S. Corsini. Orientation radiograms for indexing
and identiﬁcation in image databases. In Eusipco-96, European Conference on Signal
Processing, pages 1693–1696, 1996.
164. F.M. de Monasterio, P. Gouras, and J. Tolhurst. Spatial summation, response pattern
and conduction velocity of ganglion cells of the rhesus monkey retina. Vision Research,
16(6):674–678, 1976.
165. J.A. Movshon, I.D. Thompson, D.J., and Tolhurst. Spatial and temporal contrast sen-
sitivity of neurons in areas 17 and 18 of the cat’s visual cortex. J. Physiol. (London),
283:101–120, 1978.
166. G. Murch. Physiological principles for the effective use of color. IEEE CG&A, 4(11):49–
54, 1984.
167. H.H. Nagel. On the estimation of optical ﬂow: relations between different and some new
results. Artiﬁcial intelligence, 33:299–324, 1987.
168. K. Nassau. Physics and Chemistry of Color. Wiley, New York, 1983.
169. K. Nilsson and J. Bigun. Localization of corresponding points in ﬁngerprints by complex
ﬁltering. Pattern Recognition Letters, 24:2135–2144, 2003.
170. D. Nister. Automatic dense reconstruction from uncalibrated video sequences. PhD
thesis, Royal Inst. of Technology, KTH, 2001.
171. T.E. Ogden. The receptor mosaic of aotus trivirgatus: Distribution of rods and cones. J.
Comp. Neurol., 163:193–202, 1975.
172. E. Oja, H. Ogawa, and J. Wangviwattana. Principal component analysis by homoge-
neous neural networks, part I: The weighted subspace criterion. IEICE Transactions on
Information and Systems, E75-D(3):366–375, 1992.
173. G.A. Orban. Neuronal operations in the visual cortex. Springer, Heidelberg, 1984.
174. G.A. Orban and M. Callens. Inﬂuence of movement parameters on area 18 neurones in
the cat. Exp. Brain Res., 30:125–140, 1977.
175. J. Ortega-Garcia, J. Bigun, D. Reynolds, and J. Gonzalez-Rodriguez. Authentication
gets personal with biometrics. IEEE Signal Processing Magazine, pages 50–62, March
2004.

References
387
176. G. Osterberg. Topography of the layer of rods and cones in the human retina. Acta
Ophtalmologica (Supplementum), 6(1):11–97, 1935.
177. P.L. Palmer, M. Petrou, and J. Kittler. A Hough transform algorithm with a 2D hypoth-
esis testing kernel. Computer Vision, Graphics, and Image Processing. Image Under-
standing, 58(2):221–234, 1993.
178. T.V. Papathomas and B. Julesz. Lie differential operators in animal and machine vision.
In J.C. Simon, editor, From pixels to features, pages 115–126. Elsevier, 1989.
179. F. Pedersen, M. Bergstrom, E. Bengtsson, and B. Langstrom. Principal component anal-
ysis of dynamic positron emission tomography images. European Journal of Nuclear
Medicine, 21(12):1285–1292, 1994.
180. P. Perona. Steerable–scalable kernels for edge detection and junction analysis. In G. San-
dini, editor, Computer Vision—ECCV 1992, LNCS 588, pages 3–18, Springer, Heidel-
berg, 1992.
181. D.I. Perrett, P.A. Smith, D.D. Potter, A. Mistlin, A.S. Head, A.D. Milner, and M.A.
Jeeves. Visual cells in the temporal cortex sensitive to face view and gaze direction.
Proc. of the Royal Society of London, Series B, 223:293–317, 1985.
182. M. Persson and J. Bigun. Detection of spots in 2-d electrophoresis gels by symmetry
features. In S. Singh et al., editor, Pattern Recognition and Data Mining—ICAPR 2005,
LNCS 3686, pages 436–445, Springer, Heidelberg, 2005.
183. J. Piper and E. Granum. Computing distance transformations in convex and non-convex
domains. Pattern Recognition, 20(6):599–615, 1987.
184. I. Pitas and A.N. Venetsanopoulos. Shape decomposition by mathematical morphology.
In First International Conference on Computer Vision, ICCV, London, June 8–11, pages
621–625. IEEE Computer Society, Washington, DC,, 1987.
185. I. Pitas and A.N. Venetsanopoulos. Nonlinear Digital Filters: Principles and Applica-
tions. Kluwer Academic, 1990.
186. H.L. Premaratne and J. Bigun. A segmentation-free approach to recognise printed sin-
hala script using linear symmetry. Pattern Recognition, 37:2081–2089, 2004.
187. R. P. N. Rao, G. J. Zelinsky, M. M. Hayhoe, and D. H. Ballard. Eye movements in visual
cognition: a computational study. Technical Report 97.1, National Resource Laboratory
for the Study of Brain and Behaviour, 1997.
188. S.S. Reddi.
Radial and angular invariants for image identiﬁcation.
IEEE-PAMI,
3(2):240–242, 1981.
189. R.C. Reid and J.M. Alonso. Speciﬁcity of monosynaptic connections from thalamus to
visual cortex. Nature, 378:281–284, 1995.
190. T.H. Reiss.
The revised fundamental theorem of moment invariants.
IEEE-PAMI,
13(8):830–834, 1991.
191. H. Romesburg. Cluster Analysis for Researchers. Lifetime Learning, Belmont, 1984.
192. A. Rosenfeld and A.C. Kak. Digital Picture Processing. Academic Press, Inc., Orlando,
USA, 1982.
193. W. Rudin. Real and complex analysis. Mc Graw-Hill, New York, 1987.
194. G. Sandini and M. Tistarelli. Recovery of depth information: Camera motion as an
integration to stereo. In Workshop on Motion: Representation and Analysis Charleston,
SC, May 7–9, pages 39–43. IEEE, 1986.
195. P. Schroeter and J. Bigun. Hierarchical image segmentation by multi-dimensional clus-
tering and orientation adaptive boundary reﬁnement. Pattern Recognition, 28(5):695–
709, 1995.
196. E.L. Schwartz. Computational anatomy and functional architecture of striate cortex: A
spatial mapping approach to perceptual coding. Visual Research, 20, 1980.

388
References
197. J. Serra. Image Analysis and Mathematical Morphology. Academic, 1982.
198. S.K. Sethi and R. Jain. Finding trajectories of feature points in a monocular image
sequence. IEEE-PAMI, 9(1):56–73, 1987.
199. E.P. Simoncelli. A rotation invariant pattern signature. In International Conference on
Image Processing, ICIP-96, pages III–185–188, 1996.
200. A. Singh. An estimation-theoretic framework for image-ﬂow computation. In Third
International Conference on Computer Vision, ICCV, pages 168–177. IEEE Computer
Society, 1990.
201. P. Sinha and T. Poggio. Role of learning in three-dimensional form perception. Nature,
384(6608):460–463, 5 Dec 1996.
202. L. Sirovich and M. Kirby. Application of the Karhunen-Lo`eve procedure for the char-
acterization of human faces. IEEE-PAMI, 12(1):103–108, 1990.
203. D. Slepian. Prolate spheroidal wave functions, Fourier analysis and uncertainty–I. The
Bell System Technical J., pages 43–63, 1961.
204. D. Slepian. Prolate spheroidal wave functions, Fourier analysis and uncertainty–IV: Ex-
tensions to many dimensions; generalized prolate spheroidal functions. The Bell System
Technical J., 43:3009–3057, 1964.
205. F. Smeraldi and J. Bigun. Retinal vision applied to facial features detection and face
authentication. Pattern Recognition Letters, 23:463–475, 2002.
206. H. Sompolinsky and R. Shapley. New perspectives on mechanisms for orientation se-
lectivity. Current Opinion in Neurobiology, 7:514–522, 1997.
207. M. Spann and R. Wilson. A quadtree approach to image segmentation which combines
statistical and spatial information. Pattern Recognition, 18:257–269, 1989.
208. E. Stein and G. Weiss. Fourier Analysis on Euclidean Spaces. Princeton University
Press, Princeton, NJ, 1971.
209. M.H. Stone. Applications of the theory of boolean rings to general topology. Transac-
tions of the American Mathematical Society, 41(3):375481, 1937.
210. G. Strang. Linear algebra and its applications. Harcourt Brace Jovanovich, San Diego,
1980.
211. C. Strecha and L. Van Gool. Dense matching of multiple wide-baseline views. ICCV,
2:1194–1201, 2003.
212. R. Strichartz. A guide to distribution theory and Fourier transforms. CRC, 1994.
213. K. Suzuki, I. Horiba, and N. Sugie. Fast connected-component labeling based on se-
quential local operations in the course of forward raster scan followed by backward
raster scan. In Proc. Int. Conf. on Patt. Rec. ICPR-15, pages II–434–437. IEEE Com-
puter Society, 2000.
214. P. Tchamitchian.
Biorthogonalit´e et th´eorie des op´erateurs.
Rev. Math. Iberoamer.,
3:163–189, 1987.
215. D. Tell and S. Carlsson. Combining appearance and topology for wide baseline match-
ing. In A. Heyden et. al., editor, Computer Vision–ECCV 2002, LNCS 2350, pages
68–81, Springer, Heidelberg, 2002.
216. M. Tistarelli and G. Sandini. On the advantages of log-polar mapping for direct estima-
tion of time-to-impact from optical ﬂow. IEEE-PAMI, 15(4):401–410, 1993.
217. R. B. H. Tootell, M. S. Silverman, E. Switkes, and R. L. de Valois.
Deoxyglucose
analysis of retinotopic organization in primate striate cortex. Science, 218:902–904,
1982.
218. L. Tran and R. Lenz. PCA based representation of color distributions for color based
image retrieval. In International Conference on Image Processing, pages II: 697–700,
2001.

References
389
219. E. Trucco and A. Verri. Introductory techniques for 3D computer vision. Prentice-Hall,
Upper Saddle River, NJ, 1998.
220. M. Turk and A. Pentland. Eigenfaces for recognition. J. Cognitive Neuroscience (Win-
ter), 3(1):71–86, 1991.
221. M. Unser, A. Aldroubi, and M. Eden. Fast B–Spline transforms of continuous image
representation and interpolation. IEEE-PAMI, 13(3):277–285, 1991.
222. S. Le Vay, D. H. Hubel, and T. N. Wiesel. The pattern of ocular dominance columns in
macaque visual cortex revealed by a reduced silver stain. J. Comp. Neurol., 159:559–
576, 1975.
223. H. Wassle and R.B. Illing. The retinal projection to the superior colligulus in the cat: a
quantitative study with HRP. J. Comp. Neurol., 190:333–356, 1980.
224. J. Weickert. Coherence-enhancing shock ﬁlters. In DAGM-Symposium, pages 1–8, 2003.
225. C.F. Westin. A tensor framework for multidimensional signal processing. PhD thesis,
Linkoping University, 1994.
226. R.L. Wheeden and A. Zygmund. Measure and integral. Marcel Dekker, Basel, 1977.
227. R. Wilson and G. Granlund. The uncertainty principle in image processing. IEEE-PAMI,
6:758–767, 1984.
228. R. Wilson and M. Spann. Finite prolate spheroidal sequences and their applications II:
Image feature description and segmentation. IEEE-PAMI, 10(2):193–203, 1988.
229. R. Wilson and M. Spann. Image Segmentation and Uncertainty. Wiley, New York, 1988.
230. A.P. Witkin.
Scale-space ﬁltering.
In 8th Int. Joint Conf. on Artiﬁcial Intelligence,
Karlsruhe, Aug. 8–12, pages 1019–1022, 1983.
231. A. Wouk. A course of applied functional analysis. Wiley, New York, 1979.
232. S. Yamane, S. Kaji, and K. Kawano. What facial features activate face neurons in the
inferotemporal cortex of the monkey. Experimental Brain Research, 73:209–214, 1988.
233. K. Yogesan, H. Schulerud, F. Albregtsen, and H.E. Danielsen. Ultrastructural texture
analysis as a diagnostic tool in mouse liver carcinogenesis. Ultrastructural Pathology,
22(1):27–37, 1998.
234. C.T. Zahn and R.Z. Roskies. Fourier descriptors for plane closed curves. IEEE-T on
computers, C-21(3):269–281, 1972.
235. S. Zeki. A vision of the brain. Blackwell, London, 1993.

Index
n-folded symmetric images, 316
4-connected neighbors, N4, 367
8-connected neighbors, N8, 349, 364, 367
adaptation, 6
additive color model, 28
afﬁne coordinate transformation, 267, 269
afﬁne motion, 267
afﬁne motion parameters, 269
afﬁne transformations, 115
afﬁne warping, 115
analysis formula, 71
analytic, 75
analytic functions, 230
analytic signal, 99
aperture problem, 258
area as shape descriptor, 367
averaging linear symmetry tensors, 176
azimuth, 5
balanced direction tensor, 172
balanced directions, 172, 177, 252
balanced directions corner, 177
band-limited functions, 75
basis, 35, 48
basis truncation error, 330
BCC equation, 270
Besinc function, 107
Bessel function, 107
binary images, 359
binary shape, 366, 371
black and white motion images, 38
blind spot, 8
blobs, 11
boundary descriptors, 359
boundary estimation, 354
boundary reﬁnement, 351
bounding box as shape descriptor, 367
brightness, 31, 37
brightness axis, 31
brightness constancy constraint (BCC), 256
B-splines, 109
calibration pattern, 292
camera matrix, 287
camera obscura, 277
Cartesian Gabor decomposition, 190
Cartesian grid, 145
Cauchy–Riemann equations, 209
center–surround, 7, 24, 25
central vision, 5
change of basis, 50
characteristic function, 74, 103, 127
chirality, 212
chromatic light, 4
chromaticity diagram, 26
CIE, 26
circular addition, 83
circular convolution, 92
circular topology of DFT, 83
circular topology of FD, 371
circular translation, 83, 92
circularity as shape descriptor, 367
closedness, 36
clustering features, 345
clustering ﬂow chart, 345
color, 21, 24

392
Index
color constancy, 24
color motion images, 38
color space, xy, 27
color space, XY Z, 26
colorimetry, 26
Comb distribution, 96
compactness as shape descriptor, 367
complementary color, 29
complex exponentials, 62
complex moments, 163, 368
complex structure tensor, 169
cones, 5, 22
conjugate harmonic function, 210
conjugate symmetry derivative, 231
connected component labelling, 364
continuity, 319
continuous function, 57
continuous image, 57
continuous operators, 109
contralateral side, 10
contralateral view, 10
convergence, 66
convolution, 90
coordinate frame, 36
coordinate transformation, 209, 213
coordinate transformation, harmonic, 210
coordinates, 64
cornea, 4
corner detection, 176, 235
cortical magniﬁcation, 12
covariance matrix, 333
covariance tensor, 167
curvilinear basis, 213
curvilinear coordinates, 209
degree of belongingness, 347
delta function, 87
DFT of convolution, 93
dilation, 359
Dirac distribution δ(x), 87
direct tensor sampling, 180
direction estimation by PCA, 334, 338
direction in ND, 245
direction of hyperplanes, 245
direction of lines, 249
direction tensor, 164
directional dominance , 171
discrete grid, 70
discrete GST, 221, 233
discrete samples, 73
discrete structure tensor, 182, 187
displaced frame difference, DFD, 273
double-opponent color cells, 25
double-opponent color property, 11
down-sampling, 120
eccentricity, 5
eigenfaces, 336
electrophoresis, 238
elementary shape features, 366
elevation, 5
epipolar line, 302
epipoles, 303, 304
equivalence class, 312
erosion, 359
essential matrix, 303
Euclidean space, 35
explanatory variables, 178, 338
extrinsic matrix, 286
face recognition, 146
face sensitive cells, 18
facial landmarks, 147
feature image, 341, 343
ﬁnite extension functions, 61
ﬁnite extension signals, 61
focal length, 278
focusing on objects, 4
Fourier basis, 62
Fourier coefﬁcients, 63
Fourier coefﬁcients theorem, 63
Fourier descriptors, 371
Fourier transform of δ(x), 90
Fourier transform, convolution, 90
Fourier transform, FT, 71
fovea, 5
FT correspondence table, 101
FT of ND hyperplanes, 247
FT pairs table, 100
function addition, 58
function arguments, 58
function scaling, 58
function symbol, 58
function value, 58
fundamental matrix, 304
fusiform gyrus, 19
fuzzy C-means clustering, 347

Index
393
Gabor ﬁlter, 140
Gabor ﬁlter design, 142
Gabor spectrum, 139
Gaussian, 24, 109, 125
Gaussian pyramid, 134, 136, 234, 236, 343
Gaussians in N dimensions, 130
gender bias, 19
generalized Hough transform (GHT), 224
generalized structure tensor (GST), 215
gray value, 37
grid, 110, 121, 360
group direction, 311, 312, 319
group direction by Gabor ﬁlters, 318
group direction tensor, 315, 317
group direction, discrete, 320
grouping, 341, 354
GST examples, 236
harmonic coordinates, 217
harmonic function, 209
harmonic function pair (HFP), 209, 210
harmonic monomials, 228
Heisenberg uncertainty, 127
Hermitian symmetry, 68, 98
hexa-angle representation, 314
Hilbert space, 35, 57
hit–miss transform, 362
homing, 148
homogeneous, 282
homogeneous coordinates, 280
Hough transform, 202
Hough transform of lines, 199
HSB color space, 31
HSV color space, 31
Hu’s moment invariants, 369
hue, 31
image motion, 255
imaging, 37
indeterminacy principle, 127
indicator function, 103
inertia, 248
inertia tensor, 167, 249
inertia tensor eigenvectors, 249
inferotemporal cortex, 18
inﬁnitesimal generator, 213
inner product space, 35
integral curves, 214
intensity, 37
interpolation function, 74
intrinsic matrix, 282
intrinsic parameters, 278
ipsilateral, 10
isocurve families, 239
isocurves, 154, 214, 239
isosurfaces, 256
Karhunen–Lo´eve (KL) transform, 329
koniocelullar cells, 9
Kronecker delta δ(m), 48
label, 348, 365
label image, 348, 365
lack of linear symmetry , 177
Laplace equation, 209
Laplacian pyramid, 134–136, 234, 236, 343
lateral geniculate nucleus, 24
lateral geniculate nucleus (LGN), 24
lens, 4
Lie group of transformations, 213
Lie operator, 213
light intensity, 4, 22
linear operator, 109
linear symmetry in ND, 246
linear symmetry in HFP, 210
linear symmetry tensor, 170, 171
linearly symmetric image, 153
lines and planes, 246
local image, 7
local spectrum, 51, 138
locally orthogonal, 210
log(z)-pattern family, 211
logarithmic spirals, 212
log–polar frequency plane, 146
log–polar Gabor decomposition, 190
log–polar grid, 146
log–polar mapping, 146
luminance, 37
macula lutea, 5
macular vision, 5
magneto resonance images, 251
magnocellular cells, 25
magnocellular layers, 10
maximum translation error, 180
mean square error, 178
meridian, 5
midget cells, 9

394
Index
minimum resistance direction, 180
minimum translation error, 180
minutia, 186
model variables, 338
modulo arithmetic of DFT, 83
modulo arithmetic of FD, 371
moment invariants, 368
moment-based shape, 368
moments, 163, 368
monochromatic, 4
morphological ﬁltering, 359, 360
morphological operations, 360
motion ﬁeld, 255
motion as direction, 245
motion by correlation, 272
motion by two frames, 270
motion image, 113
motion-direction, 14
multiple directions, 311
multiplication of two images, 114
multispectral images, 38
music, 137
nasal, 5
nasal hemiﬁeld, 5
neighborhood function, 359
night vision, 5
norm properties, 41
normal image velocity, 258
normal optical ﬂow, 258, 259
normalized correlation, 54
Nyquist block, 77, 104
Nyquist period, 77
Nyquist theorem, 77
object label, 365
observation set, 329
optic axis, 5
optic chiasm, 9
optic nerve, 8
optic radiations, 10
optic tract, 9
optical axis, 278
optical character recognition, 238
optical character recognition (OCR), 54, 237
optical ﬂow, 256
order of a tensor, 50
orientation, 14
orientation column, 16
oriented butterﬂy ﬁlters, 351
origin, 35
orthogonal function families, 62
orthogonal vectors, 46
orthogonality, 46
Parseval–Plancherel theorem, 66, 89
partial derivatives, 111
parvocellular cells, 25
parvocellular layers, 10
PCA for rare observations, 335
perceptual grouping, 345
perimeter as shape descriptor, 367
periodic functions, 61
periodic signals, 61
photoreceptors, 3
pinhole camera, 277
pixel, 5, 37
pixel value, 37, 50
pointwise convergence, 66
Poisson summation, 96
positive scaling, 44
primary visual cortex, 11
principal component analysis (PCA), 329
principal components, 333
probability distribution function, 125
projection, 47
projection coefﬁcients, 63, 64, 76
projection operator, 47
projections, 63
projective camera, 278
prosopagnosia, 17
pupil, 4
pyramid building, 344
pyramids, 134, 136, 234, 236, 343
quadratic form, 248
range problem, 168, 319
real moments, 163, 368
receptive ﬁeld, 7
reconstruction, 109, 110
region descriptors, 359
regression coefﬁcients, 338
regression problem, 178
relay cells, 10
response measurement, 178
response variables, 338
retinotopic sensor, 146

Index
395
RGB cube, 31
rods, 5
rotation aliasing, 115
rotation matrix, 115
rotation of an image, 114
rotation-invariance, 373, 374
saccadic search, 146, 147
sampling, 73
saturation, 31
scalar product, 44
scalar product conservation, 88
scalar product of functions, 59
scalar product rules, 45
scale space, 134
scale-invariance, 373, 374
Schwartz inequality, 53, 60
second-order statistics, 318
segmentation, 341
separable function, 94
shape, 311, 366, 371
shape in gray images, 311
shift-invariance, 374
simple cells, 13, 146
simple manifolds, 246
sinc function, 74, 75
single direction, 322
singular value decomposition (SVD), 338
spatial continuity, 348
spatial direction, 13
spatial directions constraint, 263
spatial position, 38
spatial resolution, 6
spatio–temporal coordinates, 38
spatio–temporal signal, 3
spectral decomposition, 252
spectral line, 252
spectral plane, 252
spectral theorem, 252
spectrum sampling, 187
steerability, 230
steerable functions, 230
step edge in 3D, 246
stereo extrinsic matrix, 296
stereo image, 301
stixel, 38
strong convergence, 66
structure element, 359
structure tensor, 164, 167, 168
structure tensor decomposition, 173, 175,
254
structure tensor, decomposition in 3D, 252
subtractive color model, 28
superior colliculus, 9
symmetry derivative, 231, 320
symmetry derivative, conjugate, 231
symmetry derivatives of Gaussians, 231
symmetry, N-folded, 311
synthesis, 109
taps, 83
temporal, 5, 113
temporal hemiﬁeld, 5
tensor values, 51
tensor ﬁelds, 53
tensor product, 52
tensor sampling in ND, 263
tensors, 50
tensors, ﬁrst-order, 53
tensors, second-order, 53
tensors, zero-order, 53
tensor-valued pixels, 50
tent function, 108
test images, 313, 314
texture, 23, 318, 322, 324
texture features, 324
texture grouping, 354
topographic organization, 10
trace of a matrix, 250
translating lines, 258
translating points, 259
translation-invariance, 374
triangle inequality, 41
triangulation, 294
tune-on frequencies, 142, 145, 186
two lines in translation, 261
uncertainty principle, 341
unit impulse, 87
unsupervised region segregation, 341
unsupervised segmentation, 341
updating class centers, 346
updating partitions, 345
up-sampling, 121
vector addition, 36
vector scaling, 36
vector space, 36, 41

396
Index
vectors, 35
velocity of a particle, 270
viewpoint transformation, 50
voting in GST and GHT, 226
voxel, 38
warped image, 273
width of a function, 126
X-ray tomography, 251


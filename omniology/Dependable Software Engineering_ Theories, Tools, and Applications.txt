Kim Guldstrand Larsen 
Oleg Sokolsky
Ji Wang (Eds.)
 123
LNCS 10606
Third International Symposium, SETTA 2017
Changsha, China, October 23–25, 2017
Proceedings
Dependable 
Software Engineering 
Theories, Tools, and Applications

Lecture Notes in Computer Science
10606
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7408

Kim Guldstrand Larsen
• Oleg Sokolsky
Ji Wang (Eds.)
Dependable
Software Engineering
Theories, Tools, and Applications
Third International Symposium, SETTA 2017
Changsha, China, October 23–25, 2017
Proceedings
123

Editors
Kim Guldstrand Larsen
Aalborg University
Aalborg
Denmark
Oleg Sokolsky
University of Pennsylvania
Philadelphia, PA
USA
Ji Wang
National University of Defense Technology
Changsha
China
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-69482-5
ISBN 978-3-319-69483-2
(eBook)
https://doi.org/10.1007/978-3-319-69483-2
Library of Congress Control Number: 2017956763
LNCS Sublibrary: SL2 – Programming and Software Engineering
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This volume contains the papers presented at the third in the SETTA (the Symposium
on Dependable Software Engineering: Theories, Tools and Applications) series of
conferences – held during October 23–25, 2017, in Changsha, China. The purpose of
SETTA is to provide an international forum for researchers and practitioners to share
cutting-edge advancements and strengthen collaborations in the ﬁeld of formal methods
and its interoperability with software engineering for building reliable, safe, secure, and
smart systems. The inaugural and second SETTA symposiums were successfully held
in Nanjing (2015) and Beijing (2016).
SETTA 2017 attracted 31 good submissions coauthored by researchers from
18 countries. Each submission was reviewed by at least three Program Committee
members with help from additional reviewers. The Program Committee discussed the
submissions online and decided to accept 19 papers for presentation at the conference.
The program also included four invited talks given by Prof. Cliff B. Jones from
Newcastle University, Prof. Rupak Majumdar from Max Planck Institute for Software
Systems, Prof. Sanjit A. Seshia from the University of California, Berkeley, and Prof.
Jean-Pierre Talpin from Inria, Centre de recherche de Rennes-Bretagne-Atlantique.
We would like to express our gratitude to the authors for submitting their papers to
SETTA 2017. We are particularly thankful to all members of Program Committee and
the additional reviewers, whose hard and professional work in the review process
helped us prepare the high-quality conference program. Special thanks go to our four
invited speakers for presenting their research at the conference. We would like to thank
the Steering Committee for their advice.
Like the previous editions of SETTA, SETTA 2017 had a young SETTA
Researchers Workshop held on October 22, 2017. The Second National Conference on
Formal Methods and Applications in China was also co-located with SETTA during
October 21–22, 2017.
Finally, we thank the conference chair, Prof. Xiangke Liao, publicity chair, Prof. Fu
Song, local chair, Prof. Wei Dong, and the local Organizing Committee of Ling Chang,
Liqian Chen, Zhenbang Chen, Shanshan Li, Wanwei Liu, Yanjun Wen, and Liangze
Yin. We are grateful to Prof. Shuling Wang for taking care of the conference website.
We also thank the Chinese Computer Federation, the National Natural Science
Foundation of China, and the State Key Laboratory for High Performance Computing
of China for their ﬁnancial support. Furthermore, we would like to especially thank
Springer for sponsoring the Best Paper Award.
August 2017
Kim Guldstrand Larsen
Oleg Sokolsky
Ji Wang

Organization
Program Committee
Erika Abraham
RWTH Aachen University, Germany
Farhad Arbab
CWI and Leiden University, The Netherlands
Sanjoy Baruah
Washington University in St. Louis, USA
Michael Butler
University of Southampton, UK
Yuxin Deng
East China Normal University, China
Xinyu Feng
University of Science and Technology of China, China
Goran Frehse
University of Grenoble Alpes-Laboratoire Verimag,
France
Martin Fränzle
University of Oldenburg, Germany
Lindsay Groves
Victoria University of Wellington, New Zealand
Dimitar Guelev
Bulgarian Academy of Sciences, Bulgaria
Fei He
Tsinghua University, China
Deepak Kapur
University of New Mexico, USA
Kim Guldstrand Larsen
Aalborg University, Denmark
Axel Legay
IRISA/Inria, France
Xuandong Li
Nanjing University, China
Shaoying Liu
Hosei University, Japan
Zhiming Liu
Southwest University, China
Xiaoguang Mao
National University of Defense Technology, China
Markus Müller-Olm
Westfälische Wilhelms-Universität Münster, Germany
Raja Natarajan
Tata Institute of Fundamental Research, India
Jun Pang
University of Luxembourg, Luxembourg
Shengchao Qin
Teesside University, UK
Stefan Ratschan
Institute of Computer Science,
Czech Academy of Sciences, Czech Republic
Sriram Sankaranarayanan
University of Colorado Boulder, USA
Oleg Sokolsky
University of Pennsylvania, USA
Martin Steffen
University of Oslo, Norway
Zhendong Su
University of California, Davis, USA
Cong Tian
Xidian University, China
Tarmo Uustalu
Tallinn University of Technology, Estonia
Chao Wang
University of Southern California, USA
Farn Wang
National Taiwan University, China
Ji Wang
National University of Defense Technology, China
Heike Wehrheim
University of Paderborn, Germany
Michael Whalen
University of Minnesota, USA
Wang Yi
Uppsala University, Sweden
Haibo Zeng
Virginia Tech, USA

Naijun Zhan
Institute of Software, Chinese Academy of Sciences,
China
Lijun Zhang
Institute of Software, Chinese Academy of Sciences,
China
Qirun Zhang
University of California, Davis, USA
Additional Reviewers
Bu, Lei
Dghaym, Dana
Dokter, Kasper
Gerwinn, Sebastian
Ghassabani, Elaheh
Howard, Giles
Jakobs, Marie-Christine
Katis, Andreas
König, Jürgen
Le, Quang Loc
Le, Ton Chanh
Li, Dan
Mizera, Andrzej
Qamar, Nafees
Qu, Hongyang
Singh, Abhishek Kr
Stewart, Danielle
Traonouez, Louis-Marie
Travkin, Oleg
Wang, Shuling
Xu, Lili
Xu, Zhiwu
Xue, Bai
VIII
Organization

Abstracts of Invited Talks

General Lessons from a Rely/Guarantee
Development
Cliff B. Jones1, Andrius Velykis1, and Nisansala Yatapanage1,2
1 School of Computing Science,
Newcastle University, Newcastle upon Tyne, UK
2 School of Computer Science and Informatics,
De Montfort University, Leicester, UK
Abstract. Decomposing the design (or documentation) of large systems is a
practical necessity; this prompts the need for a notion of compositional devel-
opment methods; ﬁnding such methods for concurrent software is technically
challenging because of the interference that characterises concurrency. This
paper outlines the development of a difﬁcult example in order to draw out
lessons about such development methods. Although the “rely/guarantee”
approach is employed in the example, the intuitions are more general.

Towards Veriﬁed Artiﬁcial Intelligence
Sanjit A. Seshia
Department of Electrical Engineering and Computer Sciences,
University of California, Berkeley, USA
Abstract. The deployment of artiﬁcial intelligence (AI), particularly of systems
that learn from data and experience, is rapidly expanding in our society. Veriﬁed
artiﬁcial intelligence (AI) is the goal of designing AI-based systems that have
strong,
ideally
provable,
assurances
of
correctness
with
respect
to
mathematically-speciﬁed requirements. In this talk, I will consider Veriﬁed AI
from a formal methods perspective. I will describe ﬁve challenges for achieving
Veriﬁed AI, and ﬁve corresponding principles for addressing these challenges.
I will illustrate these challenges and principles with examples and sample
results, particularly from the domain of autonomous vehicles.
Artiﬁcial intelligence (AI) is a term used for computational systems that attempt to
mimic aspects of human intelligence (e.g., see [1]). In recent years, the deployment of
AI, particularly of systems with the ability to learn from data and experience, has
expanded rapidly. AI is now being used in applications ranging from analyzing medical
data to driving cars. Several of these applications are safety-critical or mission-critical,
and require strong guarantees of reliable operation. Therefore, the question of veriﬁ-
cation and validation of AI-based systems has begun to demand the attention of the
research community. We deﬁne veriﬁed artiﬁcial intelligence (AI) as the goal of
designing AI-based systems that have strong, ideally provable, assurances of correct-
ness with respect to mathematically-speciﬁed requirements.
In this talk, I consider Veriﬁed AI from the perspective of formal methods — a ﬁeld
of computer science and engineering concerned with the rigorous mathematical
speciﬁcation, design, and veriﬁcation of systems [2, 9]. Speciﬁcally, I describe ﬁve
major challenges for achieving Veriﬁed AI: (i) environment modeling — dealing with
environments that have a plethora of unknowns; (ii) formal speciﬁcation – how to
precisely capture assumptions and requirements on learning-based systems; (iii) system
modeling — how to model systems that evolve continuously with new data;
(iv) computational engines — new scalable techniques for training, testing, and
quantitative veriﬁcation, and (v) designing AI systems “correct-by-construction”. I also
describe ﬁve corresponding principles for addressing these challenges: (i) introspective
environment modeling; (ii) system-level speciﬁcations; (iii) abstractions and explana-
tions of learning components; (iv) new randomized and quantitative formal techniques,
and (v) the paradigm of formal inductive synthesis to design learning systems in a
“correct-by-construction” manner. Taken together, we believe that the principles we
suggest can point a way towards the goal of Veriﬁed AI.

Initial promising results have been obtained for the control and veriﬁcation of
learning-based systems, including deep learning systems, in the domain of autonomous
driving [3, 5–7]. The theory of formal inductive synthesis is described in a recent paper
[4]. A more detailed exposition of the ideas presented in this talk can be found in a
paper ﬁrst published in 2016 [8].
References
1. Committee on Information Technology, Automation, and the U.S. Workforce: Information
technology and the U.S. workforce: where are we and where do we go from here? http://www.
nap.edu/24649
2. Clarke, E.M., Wing, J.M.: Formal methods: state of the art and future directions. ACM
Comput. Surv. 28(4), 626–643 (1996)
3. Dreossi, T., Donzé, A., Seshia, S.A.: Compositional falsiﬁcation of cyber-physical systems
with machine learning components. In: NASA Formal Methods - 9th International Sympo-
sium, NFM 2017, pp. 357–372 (2017)
4. Jha, S., Seshia, S.A.: A theory of formal synthesis via inductive learning. Acta Informatica
(2017)
5. Li, W., Sadigh, D., Sastry, S.S., Seshia, S.A.: Synthesis for human-in-the-loop control sys-
tems. In: Ábrahám, E., Havelund, K. (eds.) TACAS 2014. LNCS, vol. 8413, pp. 470–484.
Springer, Berlin (2014)
6. Sadigh, D.: Safe and interactive autonomy: control, learning, and veriﬁcation. PhD thesis,
EECS Department, University of California, Berkeley (2017)
7. Sadigh, D., Sastry, S.S., Seshia, S.A., Dragan, A.D.: Information gathering actions over
human internal state. In: Proceedings of the IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pp. 66–73 (2016)
8. Seshia, S.A., Sadigh, D., Sastry, S.S.: Towards Veriﬁed Artiﬁcial Intelligence. ArXiv e-prints
(2016)
9. Wing, J.M.: A speciﬁer’s introduction to formal methods. IEEE Comput. 23(9), 8–24 (1990)
Towards Veriﬁed Artiﬁcial Intelligence
XIII

Compositional Methods for CPS Design
(Keynote Abstract)
Jean-Pierre Talpin
Inria, Centre de recherche de Rennes-Bretagne-Atlantique
Abstract. Logic and proof theory are probably have proved to be the most
effective mathematical tools to help modularly engineering correct software, and
this since the introduction of type inference, up to fantastic progresses of
SAT-SMT solvers and provers.
In this talk, I will focus on past experience of my former project-team
ESPRESSO in manipulating such tools in the design, implementation, proof and
test-case of Polychrony: an open-source synchronous modeling environment
hosted on the Polarsys industry working group of the Eclipse foundation.
I will highlight the compositionality and scalability of its concepts by
considering two major case studies (with Polychrony and its commercial
implementation, RT-Builder) in developing functional and real-time simulators,
in avionics and automotive, and the integration of these concepts in the AADL
standard.
Before concluding, I will hint on broader perspectives undertaken by my
new project-team, TEA, toward logic-focused, compositional and scalable,
models of concurrent, timed, cyber and physical systems, and the challenges to
build new ADLs from such concepts.
References
1. Nakajima, S., Talpin, J.-P., Toyoshima, M., Yu, H. (eds.): Cyber-Physical System Design
from an Architecture Analysis Viewpoint. Communications of the NII Shonan Meetings.
Springer (2017)
2. Lunel, S., Boyer, B., Talpin, J.-P.: Compositional proofs in dynamic differential logic. In:
International Conference on Applications of Concurrency to System Design. Springer (2017)
3. Besnard, L., Gautier, T., Le Guernic, P., Guy, C., Talpin, J.-P., Larson, B.R., Borde, E.:
Formal semantics of behavior speciﬁcations in the architecture analysis and design language
standard (extended abstract). High-Level Design, Veriﬁcation and Test. IEEE (2016)
4. Gautier, T., Le Guernic, P., Talpin, J.-P., Besnard, L.: Polychronous automata. In: Theo-
retical Aspects of Software Engineering. IEEE (2015)
Joint work with David Berner, Loic Besnard, Adnan Bouakaz, Christian Brunette, Abdoulaye Gamatie,
Thierry Gautier, Yann Glouche, Clment Guy, Alexandre Honorat, Kai Hu, Christophe Junke, Kenneth
Johnson, Sun Ke, Michael Kerboeuf, Paul Le Guernic, Yue Ma, Hugo Metivier, Van Chan Ngo, Julien
Ouy, Klaus Schneider, Sandeep Shukla, Eric Vecchie, Zhibin Yang, Huafeng Yu.
Supported by Nankai University and by Beihang University, NSFC grant 61672074.

5. Ngo, V.C., Talpin, J.-P., Gautier, T., Besnard, L., Le Guernic, P.: Modular translation
validation of a full-sized synchronous compiler using off-the-shelf veriﬁcation tools (ab-
stract). In: International Workshop on Software and Compilers for Embedded Systems,
invited presentation. ACM (2015)
6. Talpin, J.-P., Brandt, J., Gemnde, M., Schneider, K., Shukla, S.: Constructive polychronous
systems. In: Science of Computer Programming. Elsevier (2014)
7. Yu, H., Joshi, P., Talpin, J.-P., Shukla, S., Shiraishi, S.: Model-based integration for auto-
motive control software. In: Digital Automation Conference, Automotive Special Session.
ACM (2015)
8. Bouakaz, A., Talpin, J.-P.: Buffer minimization in earliest-deadline ﬁrst scheduling of
dataow graphs. In: Conference on Languages, Compilers and Tools for Embedded Systems.
ACM (2013)
9. Yu, H., Ma, Y., Glouche, Y., Talpin, J.-P., Besnard, L., Gautier, T., Le Guernic, P., Toom,
A., Laurent, O.: System-level co-simulation of integrated avionics using polychrony. In:
ACM Symposium on Applied Computing. ACM (2011)
10. Talpin, J.-P., Ouy, J., Gautier,T., Besnard, L., Le Guernic, P.: Compositional design of
isochronous systems. In: Science of Computer Programming, Special Issue on APGES.
Elsevier (2011)
Compositional Methods for CPS Design (Keynote Abstract)
XV

Contents
Invited Talk
General Lessons from a Rely/Guarantee Development . . . . . . . . . . . . . . . . .
3
Cliff B. Jones, Andrius Velykis, and Nisansala Yatapanage
Probabilistic and Statistical Analysis
Polynomial-Time Alternating Probabilistic Bisimulation
for Interval MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
Vahid Hashemi, Andrea Turrini, Ernst Moritz Hahn,
Holger Hermanns, and Khaled Elbassioni
Better Automated Importance Splitting for Transient Rare Events . . . . . . . . .
42
Carlos E. Budde, Pedro R. D’Argenio, and Arnd Hartmanns
On the Criticality of Probabilistic Worst-Case Execution Time Models . . . . .
59
Luca Santinelli and Zhishan Guo
Timed and Hybrid Systems
Nested Timed Automata with Invariants. . . . . . . . . . . . . . . . . . . . . . . . . . .
77
Yuwei Wang, Guoqiang Li, and Shoji Yuen
Multi-core Cyclic Executives for Safety-Critical Systems . . . . . . . . . . . . . . .
94
Calvin Deutschbein, Tom Fleming, Alan Burns, and Sanjoy Baruah
Compositional Hoare-Style Reasoning About Hybrid CSP
in the Duration Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
Dimitar P. Guelev, Shuling Wang, and Naijun Zhan
Program Analysis
Termination of Semi-algebraic Loop Programs . . . . . . . . . . . . . . . . . . . . . .
131
Yi Li
Computing Exact Loop Bounds for Bounded Program Verification . . . . . . . .
147
Tianhai Liu, Shmuel Tyszberowicz, Bernhard Beckert,
and Mana Taghdiri
AndroidLeaker: A Hybrid Checker for Collusive Leak
in Android Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
Zipeng Zhang and Xinyu Feng

Modeling and Verification
Remark on Some p Variants. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
Jianxin Xue, Huan Long, and Yuxi Fu
Reasoning About Periodicity on Infinite Words. . . . . . . . . . . . . . . . . . . . . .
200
Wanwei Liu, Fu Song, and Ge Zhou
On Equivalence Checking of Nondeterministic Finite Automata . . . . . . . . . .
216
Chen Fu, Yuxin Deng, David N. Jansen, and Lijun Zhang
A New Decomposition Method for Attractor Detection in Large
Synchronous Boolean Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
Andrzej Mizera, Jun Pang, Hongyang Qu, and Qixia Yuan
Construction of Abstract State Graphs for Understanding Event-B Models . . .
250
Daichi Morita, Fuyuki Ishikawa, and Shinichi Honiden
A Framework for Modeling and Verifying IoT Communication Protocols . . .
266
Maithily Diwan and Meenakshi D’Souza
Formalization
Formal Analysis of Information Flow in HOL . . . . . . . . . . . . . . . . . . . . . .
283
Ghassen Helali, Sofiène Tahar, Osman Hasan, and Tsvetan Dunchev
Formalizing SPARCv8 Instruction Set Architecture in Coq . . . . . . . . . . . . .
300
Jiawei Wang, Ming Fu, Lei Qiao, and Xinyu Feng
Tools
How to Efficiently Build a Front-End Tool for UPPAAL:
A Model-Driven Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
Stefano Schivo, Buğra M. Yildiz, Enno Ruijters, Christopher Gerking,
Rajesh Kumar, Stefan Dziwok, Arend Rensink, and Mariëlle Stoelinga
PranCS: A Protocol and Discrete Controller Synthesis Tool . . . . . . . . . . . . .
337
Idress Husien, Sven Schewe, and Nicolas Berthier
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
XVIII
Contents

Invited Talk

General Lessons from a Rely/Guarantee
Development
CliﬀB. Jones1(B), Andrius Velykis1, and Nisansala Yatapanage1,2
1 School of Computing Science, Newcastle University,
Newcastle upon Tyne, UK
cliff.jones@ncl.ac.uk
2 School of Computer Science and Informatics,
De Montfort University, Leicester, UK
Abstract. Decomposing the design (or documentation) of large systems
is a practical necessity; this prompts the need for a notion of composi-
tional development methods; ﬁnding such methods for concurrent soft-
ware is technically challenging because of the interference that charac-
terises concurrency. This paper outlines the development of a diﬃcult
example in order to draw out lessons about such development methods.
Although the “rely/guarantee” approach is employed in the example, the
intuitions are more general.
1
Introduction
The aim of this paper is to contribute to the discussion about compositional
development for concurrent programs. Much of the paper is taken up with the
development, from its speciﬁcation, of a concurrent garbage collector but the
important messages are by no means conﬁned to the example and are identiﬁed
as lessons.
1.1
Compositional Methods
To clarify the notion of “compositional” development of concurrent programs,
it is worth beginning with some observations about the speciﬁcation and design
of sequential programs. A developer faced with a speciﬁcation for S might make
the design decision to decompose the task using two components that are to be
executed sequentially (S1; S2); that top-level step can be justiﬁed by discharging
a proof obligation involving only the speciﬁcations of S and S1/S2. Moreover,
the developer of either of the sub-components need only be concerned with
its speciﬁcation—not that of its sibling nor that of its parent S. This not only
facilitates separate development, it also increases the chance that any subsequent
modiﬁcations are isolated within the boundary of one speciﬁed component.
As far as is possible, the advantages of compositional development should be
retained for concurrent programs.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 3–22, 2017.
https://doi.org/10.1007/978-3-319-69483-2_1

4
C.B. Jones et al.
Lesson 1. The notion of “compositionality” is best understood by thinking about
a development process: speciﬁcations of separate components ought genuinely
insulate them from one another (and from their context). The ideal is, faced
with a speciﬁed task (module), propose a decomposition (combinator) and specify
any sub-tasks; then prove the decomposition correct wrt (only) the speciﬁcations.
The same process is then repeated on the sub-tasks.
Because of the interference inherent in concurrency, this is not easy to achieve
and, clearly, (pre/)post conditions will not suﬃce. However, numerous exam-
ples exist to indicate that rely/guarantee conditions (see Sect. 1.2) facilitate the
required separation where a designer chooses a decomposition of S into shared-
variable sub-components that are to be executed in parallel (S1 || S2).
1.2
Rely/Guarantee Thinking
The origin of the rely/guarantee (R/G) work is in [Jon81]. More than
20 theses have developed the original idea including [Stø90,Xu92] that look
at progress arguments, [Din00] that moves in the direction of a reﬁnement
calculus form of R/G, [Pre01] that provides an Isabelle soundness proof of a
slightly restricted form of R/G rules, [Col08] that revisits soundness of gen-
eral R/G rules, [Pie09] that addresses usability and [Vaf07,FFS07] manage to
combine R/G thinking with Separation Logic. Furthermore, a number of sepa-
ration logic (see below) papers also employ R/G reasoning (e.g. [BA10,BA13])
and [DFPV09,DYDG+10] from separation logic researchers build on R/G. Any
reader who is unfamiliar with the R/G approach can ﬁnd a brief introduction
in [Jon96]. (A fuller set of references is contained in [HJC14,JHC15].)
The original way of writing R/G speciﬁcations displayed the predicates of
a speciﬁcation delimited by keywords; some of the subsequent papers (notably
those concerned with showing the soundness of the system) present speciﬁcations
as 5-tuples. The reformulation in [HJC14,JHC15] employs a reﬁnement calculus
format [Mor90,BvW98] in which it is much more natural to investigate alge-
braic properties of speciﬁcations. Since some of the predicates for the garbage
collection example are rather long, the keyword style is adopted in this paper
but algebraic properties such as distribution are used as required.
The literature contains many and diverse examples of R/G developments
including:
– Susan Owicki’s [Owi75] problem of ﬁnding the minimum index to an array
at which an element can be found satisfying a predicate is tackled using R/G
thinking in [HJC14]
– a staple of R/G presentations is a concurrent version of the Sieve of Eratos-
thenes introduced in [Hoa72]—see for example [JHC15]
– parallel “cleanup” operations for the Fisher/Galler Algorithm for the
union/ﬁnd problem are developed in [CJ00]
– a development of Simpson’s 4-slot algorithm is given in [JP11]—an even nicer
speciﬁcation using “possible values” (see Sect. 1.3) is contained in [JH16]

General Lessons from a Rely/Guarantee Development
5
The ﬁrst two represent examples in which the R/G conditions are symmetric
in the sense that the concurrent sub-processes have the same speciﬁcations; the
last two items and the concurrent garbage collector presented below are more
interesting because the concurrent processes need diﬀerent speciﬁcations.
Lesson 2. By using relations to express interference, R/G conditions oﬀer a
plausible balance of expressiveness versus tractability—see Sects. 3.2 and 4.
1.3
Challenges
The extent to which compositionality depends on the expressivity of the speci-
ﬁcation notation is an issue and the “possible values” notation used below pro-
vides an interesting discussion point. Much more telling is the contrast with
methods which need the code of sibling processes to reason about interference.
For example [Owi75,OG76] not only postpones a ﬁnal (Einmischungsfrie) check
until the code of all concurrent processes is to hand—this expensive test has to
be repeated when changes are made to any sub-component.
It is useful to distinguish progressively more challenging cases of interference
and the impact that the diﬃculty has on reasoning about correctness:
1. The term “parallel” is often used for threads that share no variables: threads
are in a sense entirely independent and only interact in the sense that they
overlap in time. Hoare [Hoa72] observes that, in this simple case, the con-
junction of the post conditions of the individual threads is an acceptable post
condition for their combination.
2. Over-simplifying, this is a basis for concurrent separation logic. CSL [O’H07]
and the many related logics are, however, aimed at –and capable of– reasoning
about intricate heap based-programs. See also [Par10].
3. It is argued in [JY15] that careful use of abstraction can serve the purpose of
reasoning about some forms of separation.
4. The interference in Owicki’s example that is referred to in the preceding
section is non-trivial because one thread aﬀects a variable used to control
repetition in the other thread. It would be possible to reason about the
development of this example using “auxiliary” (aka “ghost”) variables. The
approach in [Owi75] actually goes further in that the code of the combined
system is employed in the ﬁnal Einmischungsfrei check. Using the R/G app-
roach in [HJC14], however, the interference is adequately characterised by
relation.
5. There are other examples in which relations alone do not appear to be enough.
This is true of even the early stages of development of the concurrent garbage
collector below. A notation for “possible values” [JP11,HBDJ13,JH16] obvi-
ates the need for auxiliary variables in some cases see Sect. 2.1.
6. The question of whether some examples require ghost variables is open and
the discussion is resumed in Sect. 4. That their use is tempting in order to
simplify reasoning about concurrent processes is attested by the number of
proofs that employ them.

6
C.B. Jones et al.
Lesson 3. The use of “ghost” (aka “auxiliary”) variables presents a subtle dan-
ger to compositional development (cf. Lesson 1). The case against is, however,
clear: in the extreme, ghost variables can be used to record complete detail about
the environment of a process. Few researchers would succumb to such extreme
temptation but minimising the use of ghost variables ought be an objective.
It might surprise readers who have heard the current authors inveigh against
ghost variables that the development in this conference paper does in fact use
such a variable. The acknowledgements report a relevant discussion on this point
and a journal paper is planned to explore other options.
1.4
Plan of the Paper
The bulk of this paper (Sects. 2, 3, 4 and 5) oﬀers a development of the concurrent
garbage collector described in [BA84,vdS87]. At several points, “lessons” are
identiﬁed. It is these lessons that are actually the main point of the paper and
the example is chosen to give credence to these intuitions.
The development uses R/G ideas (often referred to as “R/G thinking”) but
the lessons have far wider applicability.
It is also worth mentioning that it is intended to write an extended jour-
nal version of this paper that will contain a full development from an abstract
speciﬁcation hopefully with proofs checked on Isabelle. That paper will also
review various modelling decisions made in the development. Many of twhich
were revised (in some cases more than once) e.g. the decision how to model
Heap was revised several times!1.
2
Preliminary Development
This section builds up to a speciﬁcation of concurrent garbage collection that is
then used as the basis for development in Sects. 3, 4 and 5. The main focus is
on the Collector but, since this runs concurrently with some form of Mutator,
some assumptions have to be recorded about the latter.
2.1
Abstract Spec
It is useful to pin down the basic idea of inaccessible addresses (aka “garbage”)
before worrying about details of heap storage (see Sect. 2.2) and marking
(Sect. 3).
Lesson 4. It is widely accepted that the use of abstract datatypes can clarify
key concepts before discussion turns to implementation details. Implementations
are then viewed as “reiﬁcations” that achieve the same eﬀect as the abstraction.
Formal proof obligations are given, for example, in [Jon90].
1 One plausible alternative is: Heap = (Addr × Pos)
m
−→Addr.

General Lessons from a Rely/Guarantee Development
7
Lesson 4 is common place for sequential programs but it actually has even
greater force for concurrent program development (where it is perhaps under-
employed by many researchers). For example, it is argued in [JY15] that careful
use of abstraction can serve the purpose of reasoning about separation. Further-
more, in R/G examples such as [JP11], such abstractions also make it possible
to address interference and separation at early stages of design.
The set of addresses (Addr) is assumed to be some arbitrary but ﬁnite set;
it is not to be equated with natural numbers since that would suggest that
addresses could have arithmetic operators applied to them.
The abstract states2 contain two sets of addresses: those that are in use
(busy) and those that have been collected into a free set.
Σ0 :: busy : Addr-set
free
: Addr-set
where
inv-Σ0(mk-Σ0(busy, free))
△
busy ∩free = { }
It is, of course, an essential property that the sets busy/free are always disjoint.
(VDM types are restricted by datatype invariants and the set Σ0 only contains
values that satisfy the invariant.) There can however be elements of Addr that
are in neither set—such addresses are to be considered as “garbage” and the
task of a garbage collector is to add such addresses to free.
Eﬀectively, the GC process is an inﬁnite loop repeatedly executing the
Collector operation whose speciﬁcation is:
Collector
ext wr free
rd busy
pre true
rely (busy′ −busy) ⊆free ∧free′ ⊆free
guar free ⊆free′
post (Addr −busy) ⊆

free
The predicate guar-Collector reassures the designer of Mutator that a chosen
free cell will not disappear. The read/write “frames” in a VDM speciﬁcation
provide a shorthand for access and interference: thus Collector actually has an
implied guarantee condition that it cannot change busy.
The rely condition warns the developer of Collector that the Mutator can
consume free addresses. Given this fact, recording a post condition for Collector
is not quite trivial. In a sequential setting, it would be correct to write:
free′ = (Addr −busy)
2 The use of VDM notation should present the reader with no diﬃculty: it has been
widely used for decades and is the subject of an ISO standard; one useful reference
is [Jon90].

8
C.B. Jones et al.
but the concurrent Mutator might be removing addresses from the free set so the
best that the collector can promise is to place all addresses that are originally
garbage into the free set at some point in time. Here is the ﬁrst use of the “pos-
sible values” notation in this paper. In a sequential formulation, post-Collector
would set the lower bound for garbage collection by requiring that any addresses
not reachable (in the initial hp) from roots would be in the ﬁnal free set. To
cope with the fact that a concurrent Mutator can acquire addresses from free,
the correct statement is that all unreachable addresses should appear in some
value of free. The notation discussed in [JP11,HBDJ13,JH16] for the set of pos-
sible values that can be observed by a component is

free.
Lesson 5. The “possible values” notation is a useful addition to –at least– R/G.
2.2
The Heap
This section introduces a model of the heap. The set of addresses that are busy
is deﬁned to be those that are reachable from a set of roots by tracing all of the
pointers in a heap.
Σ1 :: roots : Addr-set
hp
: Heap
free
: Addr-set
where
inv-Σ1(mk-Σ1(roots, hp, free))
△
dom hp = Addr ∧
free ∩reach(roots, hp) = { } ∧
upper bound for GC
∀a ∈free · hp(a) = {[ ]}
Heap = Addr
m
−→Node
Node =

Addr
∗
To smooth the use of this model of Heap, hp(a, i) is written for hp(a)(i) and
(a, i) ∈dom hp has the obvious meaning. When addresses are deleted from
nodes, their position is set to the nil value.
The second conjunct of the invariant deﬁnes the upper bound of garbage
collection; the ﬁnal conjunct requires that free addresses map to empty nodes.
The roots component of Σ1 is taken to be constant.
The child-rel function extracts the relation over addresses from the heap (i.e.
ignoring pointer positions); it drops any nil values.
child-rel : Heap →(Addr × Addr)-set
child-rel(hp)
△
{(a, b) | a ∈dom hp ∧b ∈(elems hp(a)) ∩Addr}

General Lessons from a Rely/Guarantee Development
9
The reach function computes the relational image (with respect to its ﬁrst
argument) of the transitive closure of the heap:
reach : Addr-set × Heap →Addr-set
reach(s, hp)
△
rng (s ◁child-rel(hp)⋆)
A useful lemma states that, starting from some set s, if there is an element
a reachable from s that is not in s, then there must exist a Node which contains
an address not in s (but notice that hp(b, j) might not be a).
A useful lemma is:
∃a · a ∈reach(s, hp) ∧a /∈s ⇒∃(b, j) ∈dom hp · b ∈s ∧hp(b, j) /∈s
3
Marking
The intuition behind the garbage collection (GC) algorithm in [BA84] is to mark
all addresses reachable (over the relation deﬁned by the Heap) from roots, then
sweep any unmarked addresses into free.
The state underlying the chosen garbage collector has an additional compo-
nent to record the addresses that have been marked (the third conjunct of the
invariant ensures that all addresses in (roots ∪free) are always marked).
Σ2 :: roots
: Addr-set
hp
: Heap
free
: Addr-set
marked : Addr-set
inv-Σ2(mk-Σ2(roots, hp, free, marked))
△
dom hp = Addr ∧
free ∩reach(roots, hp) = { } ∧
upper bound for GC
(roots ∪free) ⊆marked ∧
∀a ∈free · hp(a) = {[ ]}
3.1
Sequential Algorithm
Garbage collection runs concurrently with a Mutator which can acquire free
addresses and give rise to garbage that is no longer accessible from roots. A
fully concurrent garbage collector is covered in Sect. 4. This section introduces
(in Fig. 1) code that can be viewed as sequential in the sense that the Mutator
would have to pause; interestingly this same code satisﬁes speciﬁcations for two
more challenging concurrent situations (see Sects. 3.2 and 4).
As observed above, the full garbage collector repeatedly iterates the code
called here Collector. This can be split into three phases. Providing the invariant
is respected, Mark/Sweep do not depend on how many addresses are marked
initially (Unmark is there to ensure that garbage is collected in at most two

10
C.B. Jones et al.
passes) but, thinking of the Collector being run intermittently, it is reasonable
to start by removing any surplus marks.
Collector △(Unmark; Mark; Sweep)
The main interest is in the marking phase. As shown in Fig. 1, the outer
loop propagates a wave of marking over the hp relation; it iterates until no new
addresses are marked. The inner Propagate iterates over all addresses: for each
address that is itself marked, all of its children are marked. (Speciﬁcations of
Mark-kids are in Sects. 3.4 and 4.3.)
Fig. 1. Code for Mark
In the case when the code is running with no interference, R/G reasoning
is not required and the speciﬁcation of Mark and proof that the code in Fig. 1
satisﬁes that speciﬁcation are straightforward. (In fact, they are simpliﬁed cases
of what follows in Sect. 3.2.) When the same code is considered in the interfering
environments in Sects. 3.2 and 4, (diﬀering) R/Gs and, of course, proofs are
needed. The elaboration of the R/Gs is particularly interesting.
Lesson 6. Considering the sequential case is useful because it is then possible
to note how the rely condition (nothing changes) and the guarantee condition
(true) need to be changed to handle concurrency.
3.2
Concurrent GC with Atomic Interference
The complication in the concurrent case is that the Mutator can interfere with
the marking strategy of the Collector by redirecting pointers. This can be
accommodated providing the Mutator marks appropriately whenever it makes
a change.
The development is tackled in two stages: ﬁrstly, this section assumes a
Mutator that atomically both redirects a pointer in a Node and marks the
new address; Sect. 4 shows that even separating the two steps still allows the
Collector code of Fig. 1 to achieve the lower bound of marking but the argument
is more delicate and indicates an expressive limitation of R/G conditions. The
argument to establish the upper bound for marking (and thus the lower bound
of garbage collection) is given in Sect. 5.
If the Mutator were able to update and mark atomically, speciﬁcations and
proofs are straightforward; although this atomicity assumption is unrealistic, it

General Lessons from a Rely/Guarantee Development
11
is informative to compare this section with Sect. 4. As adumbrated in Sect. 1,
the argument is split into a justiﬁcation of the parallel decomposition (Sect. 3.3)
and the decompositions of the Collector/Mutator sub-components, addressed in
Sects. 3.4 and 3.5 respectively.
3.3
Parallel Decomposition
An R/G speciﬁcation of the Collector is:
Collector
ext wr free, marked
rd roots, hp
pre true
rely free′ ⊆free ∧marked ⊆marked′ ∧
∀(a, i) ∈dom hp ·
hp′(a, i) ̸= hp(a, i) ∧hp′(a, i) ∈Addr ⇒hp′(a, i) ∈marked′
guar free ⊆free′
post (Addr −reach(roots, hp)) ⊆

free
lower bound for GC
Here again, the notation for possible values is used to cover interference.
The ﬁnal conjunct of the rely condition is the key property that (for now)
assumes that the environment (i.e. the Mutator) simultaneously marks any
change it makes to the heap.3
The lower bound of addresses to be collected is one part of the require-
ment; the upper bound is constrained by the second conjunct of inv-Σ2. The
lower bound for garbage collection requires setting an upper bound for marking
addresses; this topic is postponed to Sect. 5.
Lesson 7. Such splitting of what would be an equality in the speciﬁcation of a
sequential component is a common R/G tactic.
The corresponding speciﬁcation of the Mutator is:
Mutator
ext wr hp, free, marked
rd roots
pre true
rely free ⊆free′
guar free′ ⊆free ∧marked ⊆marked′ ∧
∀(a, i) ∈dom hp ·
hp′(a, i) ̸= hp(a, i) ∧hp′(a, i) ∈Addr ⇒hp′(a, i) ∈marked′
post true
3 Strictly, the fact that the Collector (in particular, its Sweep component) does not
have write access to hp means that it cannot clean up the nodes in free as required
by the ﬁnal conjunct of inv-Σ2. Changing the guarantee conditions is routine but
uninformative.

12
C.B. Jones et al.
The R/G proof obligation (PO) for concurrent processes requires that each
one’s guarantee condition implies the rely condition of the other(s); in this case
they match identically so the result is immediate.
3.4
Developing the Collector Code
As outlined in Sect. 1, what remains to be done is to develop code that satis-
ﬁes the speciﬁcation of the Collector (in isolation from that of the Mutator)—
i.e. show that the decomposition of the Collector into three phases given in
Sect. 3.1 satisﬁes the Collector speciﬁcation in Sect. 3.3 and then to develop
code for Mark.
A post condition for a sequential version of Unmark could constrain marked′
to be exactly equal to roots∪free but, again, interference must be considered. The
rely condition indicates that the environment can mark addresses so whatever
Unmark removes from marked could be replaced. The possible values notation
is again deployed so that post-Unmark requires that, for every address which
should not be marked, a possible value of marked exists which does not contain
the address. However, this post condition alone would permit an implementation
of Unmark itself ﬁrst to mark an address and then remove the marking; this
erroneous behaviour is ruled out by guar-Unmark. The rely condition indicates
that the free set can also change but, since it can only reduce, this poses no
problem.
Unmark
ext wr marked
rd roots, free
pre true
rely free′ ⊆free
guar marked′ ⊆marked
post ∀a ∈(Addr −(roots ∪free)) · ∃m ∈

marked · a /∈m
The relaxing of the post condition again uses the idea in Lesson 7.
The post condition for Mark also has to cope with the interference absent
from a sequential speciﬁcation and this requires more thought. In the sequential
case, post-Mark could use a strict equality to require that all reachable nodes
are added to marked but here the equality is split into a lower and upper bound.
The lower bound for marking is crucial to preserve the upper bound of garbage
collection (see the second conjunct of inv-Σ2). This lower bound is recorded
in the post condition. (The use of hp′ is, of course, challenging but the post
condition is stable [CJ07,WDP10] under the rely condition.) The “loss” (from
the equality in the sequential case) of the other containment is compensated for
by setting an upper bound for marking (see no-mog in Sect. 5).

General Lessons from a Rely/Guarantee Development
13
Mark
ext wr marked
rd roots, hp, free
pre true
rely rely-Collector
guar marked ⊆marked′
post reach(marked, hp′) ⊆marked′
The relaxing of the post condition once again uses the idea in Lesson 7. Similar
observations to those for Unmark relate to the speciﬁcation of Sweep which, for
the concurrent case, becomes:
Sweep
ext wr free
rd marked
pre true
rely free′ ⊆free ∧marked ⊆marked′
guar free ⊆free′
post (free′ −free) ∩marked = { } ∧
∀a ∈(Addr −marked) · ∃f ∈

free · a ∈f
The rely and guarantee conditions of Collector are distributed (with appro-
priate weakening/strengthening) over the three sub-components; all of the pre
conditions are true; so the remaining PO is:
post-Unmark(σ, σ′) ∧post-Mark(σ′, σ′′) ∧post-Sweep(σ′′, σ′′′) ⇒
post-Collector(σ, σ′′′)
The proof is straightforward.
Turning to the decomposition of Mark (see Fig. 1), in order to prove
post-Mark, a speciﬁcation is needed for Propagate that copes with interference:
Propagate
ext wr marked
rd hp
pre true
rely rely-Collector
guar marked ⊆marked′
post {elems hp′(a) ∩Addr | a ∈marked} ⊆marked′ ∧
(marked ⊂marked′ ∨reach(roots, hp′) ⊆marked′)
The ﬁrst conjunct of the post condition indicates the progress required of the
wave of marking; the second triggers further iterations if any marking has
occurred.
To prove the lower marking bound (i.e. must mark everything that is reach-
able from roots), we use an argument that composes on the right a relation
that expresses the rest of the computation as in [Jon90]: essentially the to-end

14
C.B. Jones et al.
relation states that the remaining iterations of the loop will mark everything
reachable from what is already marked:
to-end(σ, σ′)
△
reach(marked, hp′) ⊆marked′
The PO is:
post-Propagate(σ, σ′) ∧σ.marked ⊂σ′.marked ∧to-end(σ′, σ′′) ⇒
to-end(σ, σ′′)
whose proof is straightforward.
The termination argument follows from there being a limit to the mark-
able elements: a simple upper bound is dom hp but there is a tighter one
(cf. Section 5).
Then trivially:
σ.marked = σ.roots ∧to-end(σ, σ′) ⇒post-Mark(σ, σ′)
Pursuing the decomposition of Propagate (again, see Fig. 1) needs a speciﬁ-
cation of the inner operation:
Mark-kids (x: Addr)
ext wr marked
rd hp
pre true
rely rely-Collector
guar marked ⊆marked′
post (elems hp′(x) ∩Addr) ⊆marked′
In this case, the proof is more conventional and a relation that expresses how
far the marking has progressed is composed on the left:
so-far(σ, σ′)
△
{elems hp(a) ∩Addr | a ∈(marked ∩consid′)} ⊆marked′
The relevant PO is:
so-far(σ, σ′) ∧
consid ′ ̸= Addr ∧post-Mark-kids(σ′, x, σ′′)∧consid ′′ = consid ′∪{x} ⇒
so-far(σ, σ′′)
whose discharge is obvious.
The ﬁnal obligation is to show:
so-far(σ, σ′) ∧consid′ = Addr ⇒post-Propagate(σ, σ′)
The ﬁrst conjunct of post-Propagate is straightforward; the fact that (unless
the marking process is complete) some marking must occur in this iteration of
Propagate follows from the lemma in Sect. 2.2.

General Lessons from a Rely/Guarantee Development
15
3.5
Checking the Interference from Mutator
The mutator is viewed as an inﬁnite loop non-deterministically selecting one of
Redirect, Malloc, Zap as speciﬁed below. At this stage, these are viewed as atomic
operations so no R/Gs are supplied here:4 their respective post conditions must
be shown to imply rely-Mark):
Redirect (a: Addr, i: N1, b: Addr)
ext wr hp, marked
pre {a, b} ⊆reach(roots, hp) ∧i ∈inds hp(a)
post hp′ = hp † {(a, i) →b} ∧marked′ = marked ∪{b}
It follows trivially that:
post-Redirect(σ, σ′) ⇒guar-Mutator(σ, σ′)
For this atomic case, the code (using multiple assignment) would be:
< hp(a), marked : = hp(a) † {i →b}, marked ∪{b} >
Malloc (a: Addr, i: N1, b: Addr)
ext wr hp, free
pre a ∈reach(roots, hp) ∧i ∈inds hp(a) ∧b ∈free
post hp′ = hp † {(a, i) →b} ∧free′ = free −{b}
Malloc preserves the invariant because inv-Σ2 insists that free addresses are
always marked. It follows trivially that:
post-Malloc(σ, σ′) ⇒guar-Mutator(σ, σ′)
Zap (a: Addr, i: N1)
ext wr hp
pre a ∈reach(roots, hp) ∧i ∈inds hp(a)
post hp′ = hp † {(a, i) →nil}
It again follows trivially that:
post-Zap(σ, σ′) ⇒guar-Mutator(σ, σ′)
4
Relaxing Atomicity: Reasoning Using a Ghost
Variable
The interesting challenge remaining is to consider the impact of acknowledg-
ing that the atomicity assumption in Sect. 3.2 about the mutator is unrealistic.
Splitting the atomic assignment (on the two shared variables hp, marked) in
Sect. 3.5 is delicate. The reader would be excused for thinking that performing
4 The all-important non-atomic case for Redirect is covered in Sect. 4.

16
C.B. Jones et al.
the marking ﬁrst would be safe but there is a counter example in the case where
the Collector executes Unmark between the two steps of such an erroneous
Redirect.
For the general lessons that this example illustrates, the interesting con-
clusion is that there appears to be no way to maintain full compositionality
(i.e. expressing all we need to know about the mutator) with standard rely con-
ditions.
Lesson 8. Ghost variables can undermine compositionality/separation (cf.
Lesson 3). Where they appear to be essential, it would be useful to have a test
for this fact.
The diﬃculty can be understood by considering the following scenario.
Redirect can, at the point that it changes hp(a, i) to point to some address
b, go to sleep before performing the marking on which the Collector of Sect. 3.4
relies. There is in fact no danger since, even if b was not marked, there must
be another path to b (see pre-Redirect in Sect. 3.5) and the Collector should
perform the marking when that path (say hp(c, j)) is encountered. If, however,
that hp(c, j) could be destroyed before the Collector gets to c, an incomplete
marking would result that could cause live addresses to be collected as garbage.
What saves the day is that the Mutator cannot make another change without
waking up and marking b.5
It is precisely this three step argument that pinpoints the limitation of using
two state relations in R/G reasoning.
Here, despite all of the reservations expressed in Sect. 1, a ghost variable is
employed to complete the justiﬁcation of the design; thus the state Σ2 is extended
with a ghost variable tbm:

Addr

that can record an address as “to be marked”.
Lesson 9. Lesson 8 asks for a test that would justify the use of ghost variable;
the need for a “3-state” justiﬁcation is such a test.6
4.1
Parallel Decomposition
The rely condition used in Sect. 3.3 is replaced for the non-atomic interference
from the mutator by:
rely-Collector : Σ2 × Σ2 →B
rely-Collector(σ, σ′)
△
free′ ⊆free ∧marked ⊆marked′ ∧
(∀(a, i) ∈hp ·
hp′(a, i) ̸= hp(a, i) ∧hp′(a, i) ∈Addr ⇒
tbm′ = hp′(a, i) ∨hp′(a, i) ∈marked′) ∧
(tbm ̸= nil ∧tbm′ ̸= tbm ⇒tbm ∈marked′ ∧tbm′ = nil)
5 This line of argument rules out multiple Mutator threads.
6 A planned journal version of this paper will investigate other options. It is also hoped
to compare the approaches with RGITL [STER11].

General Lessons from a Rely/Guarantee Development
17
Here again, the PO of the parallel introduction rule is trivial to discharge
because the guarantee condition of the Mutator is identical with the rely condi-
tion of the Collector.
4.2
Developing Mutator Code
As indicated in Sect. 1, it still remains to establish that the design of each compo-
nent satisﬁes its speciﬁcation. Looking ﬁrst at the non-atomic Mutator argument,
the only real challenge is:7
Redirect (a: Addr, i: N1, b: Addr)
ext wr hp, marked
pre {a, b} ⊆reach(roots, hp) ∧i ∈inds hp(a)
rely hp′ = hp
guar rely-Collector
post hp′ = hp † {(a, i) →b} ∧b ∈marked′
Redirect can satisfy this speciﬁcation by executing the following two atomic
steps (but the atomic brackets only surround one shared variable in each case):
< hp(a), tbm : = hp(a) † {i →b}, b >;
< marked, tbm : = marked ∪{b}, nil >
This not only guarantees rely-Collector, but also preserves the following invari-
ant:
tbm ̸= nil ⇒
∃{(a, i), (b, j)} ⊆dom hp · (a, i) ̸= (b, j) ∧hp(a, i) = hp(b, j) = tbm
4.3
Developing Collector Code
Turning to the development of Collector, code must be developed relying only on
the above interface. The only challenge is the mark phase whose speciﬁcation is:
Mark
ext wr marked
rd roots, hp, free
pre true
rely rely-Collector
guar marked ⊆marked′
post reach(marked, hp′) ⊆marked′
The code for Mark is still that in Fig. 1—under interference, the post con-
dition of Propagate has to be further weakened (from Sect. 3.4) to reﬂect that,
7 When removing a pointer, no tbm is set—see Zap(a, i) in Sect. 3.5; also no tbm is
needed in the Malloc case because inv-Σ2 ensures that any free address is marked.

18
C.B. Jones et al.
if there is an address in tbm, its reach might not yet be marked. Importantly,
if the marking is not yet complete, there must have been some node marked in
the current iteration:
Propagate
ext wr marked
rd hp
pre true
rely rely-Collector
guar marked ⊆marked′
post {elems hp′(a)∩Addr | a ∈marked} ⊆(marked′∪({tbm′}∩Addr))∧
(marked ⊂marked′ ∨reach(roots, hp′) ⊆marked′)
Notice that post-Propagate implies there can be at most one address whose
marking is problematic; this fact must be established using the ﬁnal conjunct of
rely-Collector.
The correctness of this loop is interesting—it follows the structure of that in
Sect. 3.4 using a to-end relation and, in fact, the relation is still:
to-end(σ, σ′)
△
reach(marked, hp′) ⊆marked′
The PO is now:
post-Propagate(σ, σ′) ∧σ.marked ⊂σ′.marked ∧to-end(σ′, σ′′) ⇒
to-end(σ, σ′′)
In comparison with the PO in Sect. 3.4, the diﬃcult case is where tbm′ ̸= nil
(in the converse case the earlier proof would suﬃce). What needs to be shown
is that the stray address in tbm′ will be marked. The lemma in Sect. 4.2 ensures
there is another path to the address in tbm′; this will be marked if there are
further iterations of Propagate and these are ensured by the lemma at the end
of Sect. 2.2 which, combined with the second conjunct of post-Propagate, avoids
premature termination.
The code in Fig. 1 shows how Propagate uses Mark-kids in the inner loop.
Mark-kids (x: Addr)
ext wr marked
rd hp
pre true
rely rely-Collector
guar marked ⊆marked′
post (elems hp′(x) ∩Addr) ⊆(marked′ ∪({tbm′} ∩Addr)))

General Lessons from a Rely/Guarantee Development
19
Again, the POs are as for the atomic case, but with:
so-far(σ, σ′)
△
{elems hp(a) ∩Addr | a ∈(marked ∩consid′)} ⊆
(marked′ ∪({tbm′} ∩Addr))
5
Lower Limit of GC
Sections 3.2 and 4 address (under diﬀerent assumptions) the lower bound for
marking and thus ensure that no active addresses are treated as garbage. Unless
an upper bound for marking is established however, Mark could mark every
address and no garbage would be collected. The R/G technique of splitting, for
example, a set equality into two containments often results in such a residual
PO.
Addresses that were garbage in the initial state (Addr −(reach(roots, hp) ∪
free)) should not be marked (thus any garbage will be collected at the latest
after two passes of Collect). A predicate “no marked old garbage” can be used
for the upper bound of marking:
no-mog : Addr-set × Addr-set × Heap × Addr-set →B
no-mog(r, f , h, m)
△
(Addr −(reach(r, h) ∪f )) ∩m = { }
The intuitive argument is simple: the Collector and Mutator only mark things
reachable from roots and the Mutator can change the reachable graph but only
links to addresses (from free or previously reachable from roots) that were never
“garbage”.
6
Related Work
The nine lessons are the real message of this paper; the (garbage collection)
example illustrates and hopefully clariﬁes the issues for the reader. The current
authors believe that examples are essential to drive research.
Many papers exist on garbage collection algorithms, where the veriﬁcation
is usually performed at the code level, e.g. [GGH07,HL10], which both use the
PVS theorem prover. In [TSBR08], a copying collector with no concurrency is
veriﬁed using separation logic. An Owicki-Gries proof of Ben-Ari’s algorithm is
given in [NE00]; while this examines multiple mutators, the method results in
a very large number of POs. The proof of Ben-Ari’s algorithm in [vdS87], also
using Owicki-Gries, reasons directly at the code level without using abstraction.
Perhaps the closest to our approach is [PPS10], which presents a reﬁnement-
based approach for deriving various garbage collection algorithms from an
abstract speciﬁcation. This approach is very interesting and for future work it is
worth exploring how the approach given here could be used to verify a similar

20
C.B. Jones et al.
family of algorithms. It would appear that the rely-guarantee method produces
a more compositional proof, as the approach in [PPS10] requires more integrated
reasoning about the actions of the Mutator and Collector. Similarly, in [VYB06],
a series of transformations is used to derive various concurrent garbage collection
algorithms from an initial algorithm.
Acknowledgements. We have beneﬁted from productive discussions with researchers
including Jos´e Nuno Oliviera and attendees at the January 2017 Northern Concur-
rency Working Group held at Teesside University. In particular, Simon Doherty pointed
out that GC is a nasty challenge for any compositional approach because the muta-
tor/collector were clearly thought out together; this is true but looking at an example at
the fringe of R/G expressivity has informed the notion of compositional development.
Our colleagues in Newcastle, Leo Freitas and Diego Machado Dias are currently
formalising proofs of the lemmas and POs using Isabelle.
The authors gratefully acknowledge funding for their research from EPSRC grant
Taming Concurrency.
References
[BA84] Ben-Ari, M.: Algorithms for on-the-ﬂy garbage collection. ACM Trans.
Program. Lang. Syst. 6(3), 333–344 (1984)
[BA10] Bornat, R., Amjad, H.: Inter-process buﬀers in separation logic with rely-
guarantee. Formal Aspects Comput. 22(6), 735–772 (2010)
[BA13] Bornat, R., Amjad, H.: Explanation of two non-blocking shared-variable
communication algorithms. Formal Aspects Comput. 25(6), 893–931
(2013)
[BvW98] Back, R.-J.R., von Wright, J.: Reﬁnement Calculus: A Systematic Intro-
duction. Springer, New York (1998)
[CJ00] Collette, P., Jones, C.B.: Enhancing the tractability of rely/guarantee
speciﬁcations in the development of interfering operations. In: Plotkin,
G., Stirling, C., Tofte, M. (eds.) Proof, Language and Interaction, Chap.
10, pp. 277–307. MIT Press (2000)
[CJ07] Coleman, J.W., Jones, C.B.: A structural proof of the soundness of
rely/guarantee rules. J. Logic Comput. 17(4), 807–841 (2007)
[Col08] Coleman, J.W.: Constructing a Tractable Reasoning Framework upon a
Fine-Grained Structural Operational Semantics. Ph.D. thesis, Newcastle
University, January 2008
[DFPV09] Dodds, M., Feng, X., Parkinson, M., Vafeiadis, V.: Deny-guarantee rea-
soning. In: Castagna, G. (ed.) ESOP 2009. LNCS, vol. 5502, pp. 363–377.
Springer, Heidelberg (2009). doi:10.1007/978-3-642-00590-9 26
[Din00] J¨urgen Dingel. Systematic Parallel Programming. Ph.D. thesis, Carnegie
Mellon University (2000). CMU-CS-99-172
[DYDG+10] Dinsdale-Young, T., Dodds, M., Gardner, P., Parkinson, M.J., Vafeiadis,
V.: Concurrent abstract predicates. In: D’Hondt, T. (ed.) ECOOP 2010.
LNCS, vol. 6183, pp. 504–528. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-14107-2 24
[FFS07] Feng, X., Ferreira, R., Shao, Z.: On the relationship between concur-
rent separation logic and assume-guarantee reasoning. In: Nicola, R. (ed.)
ESOP 2007. LNCS, vol. 4421, pp. 173–188. Springer, Heidelberg (2007).
doi:10.1007/978-3-540-71316-6 13

General Lessons from a Rely/Guarantee Development
21
[GGH07] Gao, H., Groote, J.F., Hesselink, W.H.: Lock-free parallel and concurrent
garbage collection by mark & sweep. Sci. Comput. Program. 64(3), 341–
374 (2007)
[HBDJ13] Hayes, I.J., Burns, A., Dongol, B., Jones, C.B.: Comparing degrees of
non-determinism in expression evaluation. Comput. J. 56(6), 741–755
(2013)
[HJC14] Hayes, I.J., Jones, C.B., Colvin, R.J.: Laws and semantics for rely-
guarantee reﬁnement. Technical Report CS-TR-1425, Newcastle Univer-
sity, July 2014
[HL10] Hesselink, W.H., Lali, M.I.: Simple concurrent garbage collection almost
without synchronization. Formal Methods Syst. Des. 36(2), 148–166
(2010)
[Hoa72] Hoare, C.A.R.: Towards a theory of parallel programming. In: Operating
System Techniques, pp. 61–71. Academic Press (1972)
[JH16] Jones, C.B., Hayes, I.J.: Possible values: exploring a concept for concur-
rency. J. Logical Algebraic Methods Program. 85(5, Part 2), 972–984
(2016). Articles dedicated to Prof. J. N. Oliveira on the occasion of his
60th birthday
[JHC15] Jones, C.B., Hayes, I.J., Colvin, R.J.: Balancing expressiveness in for-
mal approaches to concurrency. Formal Aspects Comput. 27(3), 475–497
(2015)
[Jon81] Jones,C.B.: Development Methods for Computer Programs including
a Notion of Interference. Ph.D. thesis, Oxford University, June 1981.
Oxford University Computing Laboratory (now Computer Science) Tech-
nical Monograph PRG-25
[Jon90] Jones, C.B.: Systematic Software Development using VDM, 2nd edn.
Prentice Hall International, Englewood Cliﬀs (1990)
[Jon96] Jones, C.B.: Accommodating interference in the formal design of concur-
rent object-based programs. Formal Methods Syst. Des. 8(2), 105–122
(1996)
[JP11] Jones, C.B., Pierce, K.G.: Elucidating concurrent algorithms via layers
of abstraction and reiﬁcation. Formal Aspects Comput. 23(3), 289–306
(2011)
[JY15] Jones, C.B., Yatapanage, N.: Reasoning about separation using abstrac-
tion and reiﬁcation. In: Calinescu, R., Rumpe, B. (eds.) SEFM 2015.
LNCS, vol. 9276, pp. 3–19. Springer, Cham (2015). doi:10.1007/
978-3-319-22969-0 1
[Mor90] Morgan, C.: Programming from Speciﬁcations. Prentice-Hall, New York
(1990)
[NE00] Nieto, L.P., Esparza, J.: Verifying single and multi-mutator garbage col-
lectors with owicki-gries in Isabelle/HOL. In: Nielsen, M., Rovan, B.
(eds.) MFCS 2000. LNCS, vol. 1893, pp. 619–628. Springer, Heidelberg
(2000). doi:10.1007/3-540-44612-5 57
[OG76] Owicki, S.S., Gries, D.: An axiomatic proof technique for parallel pro-
grams I. Acta Informatica 6(4), 319–340 (1976)
[O’H07] O’Hearn, P.W.: Resources, concurrency and local reasoning. Theoret.
Comput. Sci. 375(1–3), 271–307 (2007)
[Owi75] Owicki, S.: Axiomatic Proof Techniques for Parallel Programs. Ph.D.
thesis, Department of Computer Science, Cornell University (1975)

22
C.B. Jones et al.
[Par10] Parkinson, M.: The next 700 separation logics. In: Leavens, G.T.,
O’Hearn, P., Rajamani, S.K. (eds.) VSTTE 2010. LNCS, vol. 6217, pp.
169–182. Springer, Heidelberg (2010). doi:10.1007/978-3-642-15057-9 12
[Pie09] Pierce, K.: Enhancing the Useability of Rely-Guaranteee Conditions for
Atomicity Reﬁnement. Ph.D. thesis, Newcastle University (2009)
[PPS10] Pavlovic, D., Pepper, P., Smith, D.R.: Formal derivation of concurrent
garbage collectors. In: Bolduc, C., Desharnais, J., Ktari, B. (eds.) MPC
2010. LNCS, vol. 6120, pp. 353–376. Springer, Heidelberg (2010). doi:10.
1007/978-3-642-13321-3 20
[Pre01] Nieto, L.P.: Veriﬁcation of parallel programs with the owicki-gries and
rely-guarantee methods in Isabelle/HOL. Ph.D. thesis, Institut f¨ur Infor-
matic der Technischen Universitaet M¨unchen (2001)
[STER11] Schellhorn, G., Tofan, B., Ernst, G., Reif, W.: Interleaved programs and
rely-guarantee reasoning with ITL. In: TIME, pp. 99–106 (2011)
[Stø90] Stølen, K.: Development of Parallel Programs on Shared Data-Structures.
Ph.D. thesis, Manchester University (1990). UMCS-91-1-1
[TSBR08] Torp-Smith, N., Birkedal, L., Reynolds, J.C.: Local reasoning about a
copying garbage collector. ToPLaS 30, 1–58 (2008)
[Vaf07] Vafeiadis, V.: Modular ﬁne-grained concurrency veriﬁcation. Ph.D. thesis,
University of Cambridge (2007)
[vdS87] van de Jan, L.A.: “Algorithms for on-the-ﬂy garbage collection” revisited.
Inf. Process. Lett. 24(4), 211–216 (1987)
[VYB06] Vechev, M.T., Yahav, E., Bacon, D.F.: Correctness-preserving derivation
of concurrent garbage collection algorithms. In: PLDI, pp. 341–353 (2006)
[WDP10] Wickerson, J., Dodds, M., Parkinson, M.: Explicit stabilisation for
modular rely-guarantee reasoning. In: Gordon, A.D. (ed.) ESOP 2010.
LNCS, vol. 6012, pp. 610–629. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-11957-6 32
[Xu92] Xu, Q.: A Theory of State-based Parallel Programming. Ph.D. thesis,
Oxford University (1992)

Probabilistic and Statistical Analysis

Polynomial-Time Alternating Probabilistic
Bisimulation for Interval MDPs
Vahid Hashemi1(B), Andrea Turrini2, Ernst Moritz Hahn1,2,
Holger Hermanns1, and Khaled Elbassioni3
1 Saarland University, Saarland Informatics Campus, Saarbr¨ucken, Germany
hashemi@depend.uni-saarland.de
2 State Key Laboratory of Computer Science, ISCAS, Beijing, China
3 Masdar Institute of Science and Technology, Abu Dhabi, UAE
Abstract. Interval Markov decision processes (IMDPs) extend classical
MDPs by allowing intervals to be used as transition probabilities. They
provide a powerful modelling tool for probabilistic systems with an addi-
tional variation or uncertainty that relaxes the need of knowing the exact
transition probabilities, which are usually diﬃcult to get from real sys-
tems. In this paper, we discuss a notion of alternating probabilistic bisim-
ulation to reduce the size of the IMDPs while preserving the probabilistic
CTL properties it satisﬁes from both computational complexity and com-
positional reasoning perspectives. Our alternating probabilistic bisimu-
lation stands on the competitive way of resolving the IMDP nondeter-
minism which in turn ﬁnds applications in the settings of the controller
(parameter) synthesis for uncertain (parallel) probabilistic systems. By
using the theory of linear programming, we improve the complexity of
computing the bisimulation from the previously known EXPTIME to
PTIME. Moreover, we show that the bisimulation for IMDPs is a con-
gruence with respect to two facets of parallelism, namely synchronous
product and interleaving. We ﬁnally demonstrate the practical eﬀective-
ness of our proposed approaches by applying them on several case studies
using a prototypical tool.
1
Introduction
Markov Decision Processes (MDPs) are a widely and commonly used mathe-
matical abstraction that permits to study properties of real world systems in a
rigorous way. The actual system is represented by means of a model subsuming
the states the system can be in and the transitions representing how the system
evolves from one state to another; the actual properties are encoded as logical
formulas that are then veriﬁed against the model.
This work is supported by the ERC Advanced Investigators Grant 695614
(POWVER), by the CAS/SAFEA International Partnership Program for Creative
Research Teams, by the National Natural Science Foundation of China (Grants No.
61550110506 and 61650410658), by the Chinese Academy of Sciences Fellowship for
International Young Scientists, and by the CDZ project CAP (GZ 1023).
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 25–41, 2017.
https://doi.org/10.1007/978-3-319-69483-2_2

26
V. Hashemi et al.
MDPs are suitable for modelling two core aspects of the behavior of the real
world systems: nondeterminism and probability. A nondeterministic behavior can
be introduced to model a behavior of the system that is just partially known (like
receiving an asynchronous message, of which it is known it can be received in the
current state but no information is available so to quantify its likelihood) or to
leave implementation details open. A probabilistic behavior occurs whenever the
successor state of the system is not uniquely determined by the current system
and the performed action, but depends on a random choice; such a choice can
be due to the design of the system, as it is required by the implementation
of a distributed consensus protocol with faulty processes [3,14], or by physical
properties that need to be taken into account, like transmission errors.
Finding the exact probability values for the transitions is sometimes a dif-
ﬁcult task: while probabilities introduced by design can be well known, prob-
abilities modelling physical properties are usually estimated by observing the
actual system. This means that the resulting MDP is a more or less appropriate
abstraction of the real system, depending on how close the estimated probability
values are to the actual values; as a consequence, the actual properties of the
real system are more or less reﬂected by the satisfaction of the formulas by the
model.
Interval Markov Decision Processes (IMDPs) extend the classical MDPs by
including uncertainty over the transition probabilities. Instead of a single value
for the probability of reaching a speciﬁc successor by taking a transition, IMDPs
allow ranges of possible probability values given as closed intervals of the reals.
Thereby, IMDPs provide a powerful modelling tool for probabilistic systems
with an additional variation or uncertainty concerning the knowledge of exact
transition probabilities. They are especially useful to represent realistic stochas-
tic systems that, for instance, evolve in unknown environments with bounded
behavior or do not preserve the Markov property.
Since their introduction (under the name of bounded-parameter MDPs) [16],
IMDPs have been receiving a lot of attention in the formal veriﬁcation com-
munity. They are particularly viewed as the appropriate abstraction model for
uncertain systems with large state spaces, including continuous dynamical sys-
tems, for the purpose of analysis, veriﬁcation, and control synthesis. Several
model checking and control synthesis techniques have been developed [37,38,43]
causing a boost in the applications of IMDPs, ranging from veriﬁcation of con-
tinuous stochastic systems (e.g., [30]) to robust strategy synthesis for robotic
systems (e.g., [32–34,43]).
Bisimulation minimisation is a well-known technique that has been success-
fully used to reduce the size of a system while preserving the properties it satis-
ﬁes [5,8,9,23,27]; this helps the task of the property solver, since it has to work
on a smaller system. Compositional minimisation permits to minimise the single
components of the system before combining them, thus making the task of the
minimiser easier and extending its applicability to larger systems. In this paper,
we show that this approach is suitable also for IMDPs. The contributions of the
paper are as follows.

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
27
– We deﬁne alternating probabilistic bisimulations to compress the IMDP
model size with respect to the controller synthesis semantics while preserving
probabilistic CTL property satisfaction. We show that the compressed models
can be computed in polynomial time.
– From the perspective of compositional reasoning, we show that alternating
probabilistic bisimulations for IMDPs are congruences with respect to two
facets of parallelism, namely synchronous product and interleaving.
– We show promising results on a variety of case studies, obtained by proto-
typical implementations of all algorithms.
Related work.
Related work can be grouped into three categories: uncer-
tain Markov model formalisms, bisimulation minimization, and compositional
minimization.
Firstly, from the modelling viewpoint, various probabilistic modelling for-
malisms with uncertain transitions are studied in the literature. Interval Markov
Chains (IMCs) [25,28] or abstract Markov chains [13] extend standard discrete-
time Markov Chains (MCs) with interval uncertainties. They do not feature the
non-deterministic choices of transitions. Uncertain MDPs [38] allow more gen-
eral sets of distributions to be associated with each transition, not only those
described by intervals. They usually are restricted to rectangular uncertainty sets
requiring that the uncertainty is linear and independent for any two transitions
of any two states. Parametric MDPs [17], to the contrary, allow such depen-
dencies as every probability is described as a rational function of a ﬁnite set of
global parameters. IMDPs extend IMCs by inclusion of nondeterminism and are
a subset of uncertain MDPs and parametric MDPs.
Secondly, as regards to the bisimulation minimization for uncertain or para-
metric probabilistic models, works in [18,20,21] explored the computational com-
plexity and approximability of deciding probabilistic bisimulation for IMDPs
with respect to the cooperative resolution of nondeterminism. In this work, we
show that IMDPs can be minimized eﬃciently with respect to the competitive
resolution of nondeterminism.
Lastly, from the viewpoint of compositional minimization, IMCs [25] and
abstract Probabilistic Automata (PA) [10,11] serve as speciﬁcation theories for
MC and PA, featuring satisfaction relation and various reﬁnement relations.
In [22], the authors discuss the key ingredients to build up the operations of
parallel composition for composing IMDP components at run-time. Our paper
follows this spirit for alternating probabilistic bisimulation on IMDPs.
Structure of the paper. We start with necessary preliminaries in Sect. 2. In Sect. 3,
we give the deﬁnitions of alternating probabilistic bisimulation for interval MDP
and discuss their properties. A polynomial time decision algorithm to decide
alternating probabilistic bisimulation for IMDPs and also compositional rea-
soning are discussed in Sects. 4 and 5, respectively. In Sect. 6, we demonstrate
our approach on some case studies and present promising experimental results.
Finally, in Sect. 7 we conclude the paper.

28
V. Hashemi et al.
2
Mathematical Preliminaries
For a set X, denote by Disc(X) the set of discrete probability distributions over
X. Intuitively, a discrete probability distribution ρ is a function ρ: X →R≥0
such that 
x∈X ρ(x) = 1; for X′ ⊆X, we write ρ(X′) for 
x∈X′ ρ(x). Given
ρ ∈Disc(X), we denote by Supp(ρ) the set { x ∈X | ρ(x) > 0 } and by δx,
where x ∈X, the Dirac distribution such that δx(y) = 1 for y = x, 0 otherwise.
For a probability distribution ρ, we also write ρ = { (x, px) | x ∈X } where px
is the probability of x.
The lifting L(R) [31] of a relation R ⊆X × Y is deﬁned as follows: for ρX ∈
Disc(X) and ρY ∈Disc(Y ), ρX L(R) ρY holds if there exists a weighting function
w: X × Y →[0, 1] such that (1) w(x, y) > 0 implies x R y, (2) 
y∈Y w(x, y) =
ρX(x), and (3) 
x∈X w(x, y) = ρY (y). When R is an equivalence relation on X,
ρ1 L(R) ρ2 holds if for each C ∈X/R, ρ1(C) = ρ2(C) where X/R = { [x]R | x ∈
X } and [x]R = { y ∈X | y R x }.
For a vector x ∈Rn we denote by xi, its i-th component, and we call x a
weight vector if x ∈Rn
≥0 and n
i=1 xi = 1. Given two vectors x, y ∈Rn, their
Euclidean inner product x · y is deﬁned as x · y = xT y = n
i=1 xi · yi. We write
x ≤y if xi ≤yi for each 1 ≤i ≤n and we denote by 1 ∈Rn the vector such
that 1i = 1 for each 1 ≤i ≤n. For a set of vectors S = {s1, . . . , sm} ⊆Rn,
we say that s is a convex combination of elements of S, if s = m
i=1 wi · si for
some weight vector w ∈Rm
≥0. For a given set P ⊆Rn, we denote by conv P
the convex hull of P and by Ext(P) the set of extreme points of P. If P is a
polytope in Rn then for each 1 ≤i ≤n, the projection projei P on the i-th
dimension of P is deﬁned as projei P = [mini P, maxi P] where ei ∈Rn is such
that ei
i = 1 and ei
j = 0 for each j ̸= i, mini P = min{ xi | x ∈P }, and
maxi P = max{ xi | x ∈P }.
2.1
Interval Markov Decision Processes
We now deﬁne Interval Markov Decision Processes (IMDPs) as an extension of
MDPs, which allows for the inclusion of transition probability uncertainties as
intervals. IMDPs belong to the family of uncertain MDPs and allow to describe
a set of MDPs with identical (graph) structures that diﬀer in distributions asso-
ciated with transitions.
Deﬁnition 1 (IMDPs). An Interval Markov Decision Process (IMDP) M is
a tuple (S, ¯s, A, AP, L, I ), where S is a ﬁnite set of states, ¯s ∈S is the initial
state, A is a ﬁnite set of actions, AP is a ﬁnite set of atomic propositions,
L: S →2AP is a labelling function, and I : S × A × S →I ∪{[0, 0]} is a total
interval transition probability function with I = { [l, u] ⊆R | 0 < l ≤u ≤1 }.
Given s ∈S and a ∈A, we call ha
s ∈Disc(S) a feasible distribution reachable
from s by a, denoted by s
a
−→ha
s, if, for each state s′ ∈S, we have ha
s(s′) ∈
I (s, a, s′). We denote the set of feasible distributions for state s and action a
by Ha
s, i.e., Ha
s = { ha
s ∈Disc(S) | s
a
−→ha
s } and we denote the set of available

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
29
actions at state s ∈S by A(s), i.e., A(s) = { a ∈A | Ha
s ̸= ∅}. We assume that
A(s) ̸= ∅for all s ∈S.
We deﬁne the size of M, written |M|, as the number of non-zero entries of
I , i.e., |M| = |{ (s, a, s′, ι) ∈S × A × S × I | I (s, a, s′) = ι }| ∈O(|S|2 · |A|).
A path ξ in M is a ﬁnite or inﬁnite sequence of states ξ = s0s1 . . . such that
for each i ≥0 there exists ai ∈A(si) such that I (si, ai, si+1) ∈I. The i-th state
along the path ξ is denoted by ξ[i] and, if the path is ﬁnite, we denote by last(ξ)
its last state. The sets of all ﬁnite and inﬁnite paths in M are denoted by Paths∗
and Paths, respectively.
The nondeterministic choices between available actions and feasible distrib-
utions present in an IMDP are resolved by strategies and natures, respectively.
Deﬁnition 2 (Strategy and Nature in IMDPs).
Given an IMDP M, a
strategy is a function σ: Paths∗→Disc(A) such that for each path ξ ∈Paths∗,
σ(ξ) ∈Disc(A(last(ξ)). A nature is a function π: Paths∗× A →Disc(S) such
that for each path ξ ∈Paths∗and action a ∈A(s), π(ξ, a) ∈Ha
s where s =
last(ξ).
The sets of all strategies and all natures are denoted by Σ and Π, respectively.
Given a ﬁnite path ξ of an IMDP, a strategy σ, and a nature π, the system
evolution proceeds as follows: let s = last(ξ). First, an action a ∈A(s) is chosen
probabilistically by σ. Then, π resolves the uncertainties and chooses one fea-
sible distribution ha
s ∈Ha
s. Finally, the next state s′ is chosen according to the
distribution ha
s, and the path ξ is extended by s′.
A strategy σ and a nature π induce a probability measure over paths as fol-
lows. The basic measurable events are the cylinder sets of ﬁnite paths, where the
cylinder set of a ﬁnite path ξ is the set Cylξ = { ξ′ ∈Paths | ξ is a preﬁx of ξ′ }.
The probability Prσ,π
M of a state s′ is deﬁned to be Prσ,π
M [Cyls′] = δ¯s(s′) and the
probability Prσ,π
M [Cylξs′] of traversing a ﬁnite path ξs′ is deﬁned to be
Prσ,π
M [Cylξs′] = Prσ,π
M [Cylξ] ·

a∈A(last(ξ))
σ(ξ)(a) · π(ξ, a)(s′).
Standard measure theoretical arguments ensure that Prσ,π
M extends uniquely to
the σ-ﬁeld generated by cylinder sets.
Fig. 1. An example of IMDP.
As an example of IMDPs, consider the
one depicted in Fig. 1. The set of states is
S = {s, t, u} with s being the initial one;
the set of actions is A = {a, b} while the
set of atomic propositions assigned to each
state by the labelling function L is repre-
sented by the letters in curly brackets near
each state. Finally, the transition probability
intervals are I (s, a, t) = [ 1
3, 2
3], I (s, a, u) =
[ 1
10, 1], I (s, b, t) = [ 2
5, 3
5], I (s, b, u) = [ 1
4, 2
3],
I (t, a, t) = I (u, b, u) = [1, 1], and I (t, b, t) =
I (u, a, u) = [0, 0].

30
V. Hashemi et al.
2.2
Probabilistic Computation Tree Logic (PCTL)
There are various ways how to describe properties of IMDPs. Here we focus on
probabilistic CTL (PCTL) [19]. The syntax of PCTL state formulas ϕ and PCTL
path formulas ψ is given by:
ϕ := a | ¬ϕ | ϕ1 ∧ϕ2 | P▷◁p(ψ)
ψ := Xϕ | ϕ1 U ϕ2 | ϕ1 U≤k ϕ2
where a ∈AP, p ∈[0, 1] is a rational constant, ▷◁∈{≤, <, ≥, >}, and k ∈N.
The semantics of a PCTL formula with respect to IMDPs is very similar to
the classical PCTL semantics for MDPs: they coincide on all formulas except
for P▷◁p(ψ), where they may diﬀer depending on how the nondeterminism is
resolved. Formally, for the formulas they agree on, given a state s and a state
formula ϕ, the satisfaction relation s |= ϕ is deﬁned as follows:
s |= a
if a ∈L(s);
s |= ¬ϕ
if it is not the case that s |= ϕ, also written s ̸|= ϕ;
s |= ϕ1 ∧ϕ2
if s |= ϕ1 and s |= ϕ2.
Given an inﬁnite path ξ = s1s2 . . . and a path formula ψ, the satisfaction relation
ξ |= ψ is deﬁned as follows:
ξ |= Xϕ
if s2 |= ϕ;
ξ |= ϕ1 U≤k ϕ2
if there exists i ≤k such that si |= ϕ2
and sj |= ϕ1 for every 1 ≤j < i;
ξ |= ϕ1 U ϕ2
if there exists k ∈N such that ξ |= ϕ1 U≤k ϕ2.
Regarding the state formula P▷◁p(ψ), its semantics depends on the way the non-
determinism is resolved for the probabilistic operator P▷◁p(ψ). When quantifying
both types of nondeterminism universally, the corresponding satisfaction relation
s |= P▷◁p(ψ) is deﬁned as follows:
s |= P▷◁p(ψ) if ∀σ ∈Σ : ∀π ∈Π : Prσ,π
s
[Pathsψ] ▷◁p
(∀)
where Pathsψ = { ξ ∈Paths | ξ |= ψ } denotes the set of inﬁnite paths satisfying
ψ. It is easy to show that the set Pathsψ is measurable for any path formula ψ,
hence its probability can be computed and compared with p. When the IMDP
is actually an MDP, i.e., all intervals are single values, then the satisfaction
relation s |= P▷◁p(ψ) in Equation (∀) coincides with the corresponding deﬁnition
for MDPs (cf. [2, Sect. 10.6.2]). We explain later how the semantics diﬀers for a
diﬀerent resolution of nondeterminism for strategy and nature.
3
Alternating Probabilistic Bisimulation for IMDPs
This section revisits required main results on probabilistic bisimulation for
IMDPs, as developed in [20]. In the setting of this paper, we consider alter-
nating probabilistic bisimulation which stems from the competitive resolution

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
31
of nondeterminisms in IMDPs. In the competitive semantics, the strategy and
nature are playing in a game against each other; therefore, they are resolved
competitively. This semantics is very natural in the context of controller syn-
thesis for systems with uncertain probabilities or in the context of parameter
synthesis for parallel systems.
In this paper, in order to resolve the stochastic nondeterminism we focus on
the dynamic approach [24,42], i.e., independently at each computation step as
it is easier to work with algorithmically and can be seen as a relaxation of the
static approach that is often intractable [4,7,12,16].
To this end, we consider the controller synthesis semantics to resolve the two
sources of IMDP nondeterminisms and discuss the resultant alternating prob-
abilistic bisimulation. Note that there is another variant of alternating proba-
bilistic bisimulation based on the parameter synthesis semantics [20]. However,
the alternating bisimulations relations resulting from these two semantics coin-
cide [20, Theorem 4].
In the controller synthesis semantics, we search for a strategy σ such that for
any nature π, a ﬁxed property ϕ is satisﬁed. This corresponds to the satisfaction
relation |=(∃σ∀) in PCTL, obtained from |= by replacing the rule (∀) with
s |=(∃σ∀) P▷◁p(ψ) if ∃σ ∈Σ : ∀π ∈Π : Prσ,π
s
[Pathsψ] ▷◁p.
(∃σ∀)
As regards to bisimulation, the competitive setting is not common. We deﬁne
a bisimulation similar to the alternating bisimulation of [1] applied to non-
stochastic two-player games. For a decision ρ ∈Disc(A) of σ, let s
ρ
−→μ denote
that μ is a possible successor distribution, i.e., there are decisions μa of π for
each a ∈Supp(ρ) such that μ = 
a∈A ρ(a) · μa.
Deﬁnition 3. Given an IMDP M, let R ⊆S×S be an equivalence relation. We
say that R is an alternating probabilistic (∃σ∀)-bisimulation if for any (s, t) ∈
R we have that L(s) = L(t) and for each ρs ∈Disc(A(s)) there exists ρt ∈
Disc(A(t)) such that for each t ρt
−→μt there exists s ρs
−→μs such that μs L(R) μt.
We write s ∼(∃σ∀) t whenever (s, t) ∈R for some alternating probabilistic (∃σ∀)-
bisimulation R.
The exact alternation of quantiﬁers might be counter-intuitive at ﬁrst sight.
Note that it exactly corresponds to the situation in non-stochastic games [1].
The deﬁned bisimulation preserves the PCTL logic with respect to the |=(∃σ∀)
semantics.
Theorem 4. For states s ∼(∃σ∀) t and any PCTL formula ϕ, we have s |=(∃σ∀)
ϕ if and only if t |=(∃σ∀) ϕ.
As a concluding remark, it is worthwhile to note that Deﬁnition 3 can be seen
as the conservative extension of probabilistic bisimulation for (state-labelled)
MDPs. To see that, assume the set of uncertainty for every transition is a sin-
gleton. Since there is only one choice for the nature, the role of nature can be
safely removed from the deﬁnitions.

32
V. Hashemi et al.
4
A PTIME Decision Algorithm for Bisimulation
Minimization
Computation of the alternating probabilistic bisimulation ∼(∃σ∀) for IMDPs fol-
lows the standard partition reﬁnement approach [6,15,26,35]. However, the core
part is ﬁnding out whether two states “violate the deﬁnition of bisimulation”.
This veriﬁcation routine amounts to check that s and t have the same set of
strictly minimal polytopes detailed as follows.
For s ∈S and a ∈A(s), recall that Ha
s denotes the polytope of feasible
successor distributions over states with respect to taking the action a in the
state s. By Ps,a
R , we denote the polytope of feasible successor distributions over
equivalence classes of R with respect to taking the action a in the state s. Given
an interval [l, u], let inf[l, u] = l and sup[l, u] = u. For μ ∈Disc(S/R) we set
μ ∈Ps,a
R
if, for each C ∈S/R, we have μ(C) ∈I (s, a, C) where
I (s, a, C) =

min

1,

s′∈C
inf I (s, a, s′)

, min

1,

s′∈C
sup I (s, a, s′)

.
It is not diﬃcult to see that each Ps,a
R
can be represented as an H-polytope.
To simplify our presentation, we shall ﬁx an order over the equivalence classes
in S/R. By doing so, any distribution ρ ∈Disc(S/R) can be seen as a vector
v ∈Rn
≥0 such that vi = ρ(Ci) for each 1 ≤i ≤n, where n = |S/R| and Ci
is the i-th equivalence class in the order. For the above discussion, ρ ∈Ps,a
R
if
and only if ρ(Ci) ∈[ls,a
i
, us,a
i
] for any 1 ≤i ≤n and ρ ∈Disc(S/R), where ls,a
and us,a are vectors such that ls,a
i
= min{1, 
s′∈Ci inf I (s, a, s′)} and us,a
i
=
min{1, 
s′∈Ci sup I (s, a, s′)} for each 1 ≤i ≤n. Therefore, Ps,a
R
corresponds to
an H-polytope deﬁned by { xs,a ∈Rn | ls,a ≤xs,a ≤us,a, 1 · xs,a = 1 }.
Deﬁnition 5 (Strictly minimal polytopes). Given an IMDP M, a state s,
an equivalence relation R ⊆S × S, and a set { Ps,a
R | a ∈A(s) } where for each
a ∈A(s), for given ls,a, us,a ∈Rn, Ps,a
R
is the convex polytope Ps,a
R
= { xs,a ∈
Rn | ls,a ≤xs,a ≤us,a, 1 · xs,a = 1 }, a polytope Ps,a
R
is called strictly minimal,
if for no ρ ∈Disc(A(s) \ {a}), we have Ps,ρ
R
⊆Ps,a
R
where Ps,ρ
R
is deﬁned as
Ps,ρ
R = { xs,ρ ∈Rn | xs,ρ = 
b∈A(s)\{a} ρ(b) · xs,b ∧xs,b ∈Ps,b
R }.
Checking violation of a given pair of states amounts to check if the states have
the same set of strictly minimal polytopes. Formally,
Lemma 6 (cf. [20]).
Given an IMDP M and s, t ∈S, we have s ∼(∃σ∀) t if
and only if L(s) = L(t) and { Ps,a
∼(∃σ∀) | a ∈A and Ps,a
∼(∃σ∀)is strictly minimal } =
{ Pt,a
∼(∃σ∀) | a ∈A and Pt,a
∼(∃σ∀) is strictly minimal }.
The expensive procedure in the analysis of the worst case time complexity of com-
puting the coarsest alternating probabilistic bisimulation ∼(∃σ∀), as described
in [20], is to check the strict minimality of a polytope Ps,a
R
for a ∈A(s). This
decision problem has been shown to be exponentially veriﬁable via a reduction

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
33
to a system of linear (in)equalities in EXPTIME. In this paper, we give a poly-
nomial time routine to verify the strict minimality of a polytope which in turn
enables a polynomial time decision algorithm to decide ∼(∃σ∀). To this aim, we
use the following equivalent form of the Farkas’ Lemma [39].
Lemma 7. Let A ∈Rm×n, b ∈Rm and c ∈Rn. Then, Ax ≤b implies c·x ≤d
if and only if there exists y ∈Rm
≥0 such that AT y = c and b · y ≤d.
This variant of Farkas’ Lemma leads us to establish the main result of the
paper. Formally,
Theorem 8. Given an IMDP M, a state s ∈S, an equivalence relation R ⊆
S × S and a set { Ps,a
R | a ∈A(s) } deﬁned as in Deﬁnition 5, checking whether
for each a ∈A(s), the polytope Ps,a
R
is strictly minimal, is in P.
Proof. Let A(s) = {a0, a1, . . . , am}, n = |S/R|, and Pi = Ps,ai
R
for 0 ≤i ≤m.
We describe the veriﬁcation routine to check the strict minimality of P0; the same
routine applies to the other polytopes. We consider the converse of the strict min-
imality problem which asks to decide whether there exist λ1, λ2, . . . , λm ∈R≥0
such that m
i=1 λi = 1 and m
i=1 λiPi ⊆P0. We show that the latter problem
can be casted as an LP via Farkas’ Lemma 7. To this aim, we alternatively
reformulate the converse problem as “do there exist λ1, λ2, . . . , λm ∈R≥0 with
m
i=1 λi = 1, such that xi ∈Pi for each 1 ≤i ≤m implies m
i=1 λixi ∈P0?”.
For every ﬁxed λ1, λ2, . . . , λm ∈R≥0 with m
i=1 λi = 1, the implication
“(∀1 ≤i ≤m : xi ∈Pi) =⇒m
i=1 λixi ∈P0” can be written as the conjunction
of 2n conditions:
m

i=1
li ≤xi ≤ui ∧
m

i=1
1 · xi = 1 =⇒
m

i=1
λixi
k ≥l0
k
(1)
m

i=1
li ≤xi ≤ui ∧
m

i=1
1 · xi = 1 =⇒
m

i=1
λixi
k ≤u0
k
(2)
for all 1 ≤k ≤n. (Note that the condition 1 · m
i=1 λixi = 1 is trivially satisﬁed
if 1 · xi = 1 for all 1 ≤i ≤m.) Each of the conditions (1) and (2), by Farkas’
Lemma, is equivalent to the feasibility of a system of inequalities; for instance,
for a given k, (1) is true if and only if there exist vectors μk,i, νk,i ∈Rn
≥0 and
scalars θk,i, ηk,i ∈R≥0 for each 1 ≤i ≤m satisfying:
μk,i −νk,i + θk,i1 −ηk,i1 = −λiek
∀1 ≤i ≤m
(3)
m

i=1
	
ui · μk,i −li · νk,i + θk,i −ηk,i
≤−l0
k
(4)
Similarly, for a given k, (2) is true if and only if there exist vectors μk,i, νk,i ∈
Rn
≥0 and scalars θk,i, ηk,i ∈R≥0 for each 1 ≤i ≤m satisfying:
μk,i −νk,i + θk,i1 −ηk,i1 = λiek
∀1 ≤i ≤m
(5)
m

i=1
(ui · μk,i −li · νk,i + θk,i −ηk,i) ≤u0
k
(6)

34
V. Hashemi et al.
Algorithm 1: Bisimulation(M)
Input: A relation R on S × S
Output: A probabilistic bisimulation R
1 begin
2
R ←{ (s, t) ∈S × S | L(s) = L(t) };
3
repeat
4
R′ ←R;
5
forall s ∈S do
6
D ←∅;
7
forall t ∈[s]R do
8
if Violate(s, t, R) then
9
D ←D ∪{t};
10
split [s]R in R into D and [s]R \ D;
11
until R = R′;
12
return R;
Procedure 2: Violate(s, t, R)
Input: States s, t and relation R
Output: Checks if s ∼R t
1 begin
2
S, T ←∅;
3
forall a ∈A do
4
if Ps,a
R
is strictly minimal then
5
S ←S ∪{Ps,a
R };
6
if Pt,a
R
is strictly minimal then
7
T ←T ∪{Pt,a
R };
8
return S ̸= T;
Fig. 2. Alternating probabilistic bisimulation algorithm for interval MDPs
Thus, the converse problem we are aiming to solve reduces to checking the exis-
tence of vectors μk,i, νk,i, μk,i, νk,i ∈Rn
≥0 and scalars λi, θk,i, ηk,i, θk,i, ηk,i ∈
R≥0 for each 1 ≤i ≤m satisfying (3)–(6) and m
i=1 λi = 1. That amounts to
solve an LP problem, which is known to be in P.
⊓⊔
As stated earlier, in order to compute ∼(∃σ∀) we follow the standard parti-
tion reﬁnement approach formalized by the procedure Bisimulation in Fig. 2.
Namely, we start with R being the complete relation and iteratively remove from
R pairs of states that violate the deﬁnition of bisimulation with respect to R.
Clearly the core part of the algorithm is to check if two states “violate the deﬁn-
ition of bisimulation”. The violation of bisimilarity of s and t with respect to R,
which is addressed by the procedure Violate, is checked by verifying if states s
and t have the same set of strictly minimal polytopes. As a result of Theorem 8,
this veriﬁcation routine can be checked in polynomial time. As regards the com-
putational complexity of Algorithm 1, let |S| = n and |A| = m. The procedure
Violate in Fig. 2 is called at most n3 times. The procedure Violate is then
linear in m and in the complexity of checking strict minimality of Ps,a
R and Pt,a
R ,
which is in O(|M|O(1)). Putting all these together, we get the following result.
Theorem 9. Given an IMDP M, computing ∼(∃σ∀) belongs to O(|M|O(1)).
5
Compositional Reasoning
In order to study the compositional minimization, that is, to split a complex
IMDP as parallel composition of several simpler IMDPs and then to use the
bisimulation as a means to reduce the size of each of these IMDPs before per-
forming the model checking for a given PCTL formula ϕ, we have to extend
the notion of bisimulation from one IMDP to a pair of IMDPs; we do this by
following the usual construction (see, e.g., [6,40]). Given two IMDPs M1 and

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
35
M2, we say that they are alternating probabilistic (∃σ∀)-bisimilar, denoted by
M1 ∼(∃σ∀) M2, if there exists an alternating probabilistic (∃σ∀)-bisimulation on
the disjoint union of M1 and M2 such that ¯s1 ∼(∃σ∀) ¯s2. We can now establish
the ﬁrst property needed for the compositional minimization, that is, transitivity
of ∼(∃σ∀):
Theorem 10. Given three IMDPs M1, M2, and M3, whenever M1 ∼(∃σ∀)
M2 and M2 ∼(∃σ∀) M3, then M1 ∼(∃σ∀) M3.
For the second property needed by the compositional minimization, that is,
that ∼(∃σ∀) is preserved by the parallel composition operator, we ﬁrst have to
introduce such an operator; to this end, we consider a slight adaption of synchro-
nous product of M1 and M2 as introduced in [22]. Such a synchronous product
makes use of a subclass of the Segala’s (simple) probabilistic automata [40,41],
called action agnostic probabilistic automata [22], where each automaton has as
set of actions the same singleton set {f}, that is, all transitions are labelled by
the same external action f: an (action agnostic) probabilistic automaton (PA) is
a tuple P = (S, ¯s, AP, L, D), where S is a set of states, ¯s ∈S is the start state,
AP is a ﬁnite set of atomic propositions, L: S →2AP is a labelling function, and
D ⊆S × Disc(S) is a probabilistic transition relation.
Deﬁnition 11. Given two IMDPs M1 and M2, we deﬁne the synchronous
product of M1 and M2 as M1 ⊗M2 := F(UF(M1) ⊗UF(M2)) where
• the unfolding mapping UF: IMDP →PA is a function that maps a given
IMDP M = (S, ¯s, A, AP, L, I ) to the PA P = (S, ¯s, AP, L, D) where D =
{ (s, μ) | s ∈S, ∃a ∈A(s) : μ ∈Ext(Ha
s)∧Ha
s is a strictly minimal polytope };
• the folding mapping F: PA →IMDP transforms a PA P = (S, ¯s, AP, L, D)
into the IMDP M = (S, ¯s, {f}, AP, L, I ) where, for each s, t ∈S, I (s, f, t) =
projet conv { μ | (s, μ) ∈D };
• the synchronous product of two PAs P1 and P2, denoted by P1 ⊗P2, is the
probabilistic automaton P = (S, ¯s, AP, L, D) where S = S1 × S2, ¯s = (¯s1, ¯s2),
AP = AP1 ∪AP2, for each (s1, s2) ∈S, L(s1, s2) = L1(s1) ∪L2(s2), and
D = { ((s1, s2), μ1 × μ2) | (s1, μ1) ∈D1 and (s2, μ2) ∈D2 }, where μ1 × μ2 is
deﬁned for each (t1, t2) ∈S1 × S2 as (μ1 × μ2)(t1, t2) = μ1(t1) · μ2(t2).
As stated earlier, Deﬁnition 11 is slightly diﬀerent from its counterpart in [22].
As a matter of fact, due to the competitive semantics for resolving the nondeter-
minism, only actions whose uncertainty set is a strictly minimal polytope play
a role in deciding the alternating bisimulation relation ∼(∃σ∀). In particular,
for the compositional reasoning keeping state actions whose uncertainty set is
not strictly minimal induces spurious behaviors and therefore, inﬂuences on the
soundness of the parallel operator deﬁnition. In order to avoid such redundan-
cies, we can either preprocess the IMDPs before composing by removing state
actions whose uncertainty set is not strictly minimal or restricting the unfolding
mapping UF to unfold a given IMDP while ensuring that all extreme transitions
in the resultant probabilistic automaton correspond to extreme points of strictly
minimal polytopes in the original IMDP. For the sake of simplicity, we choose
the latter.

36
V. Hashemi et al.
Theorem 12. Given three IMDPs M1, M2, and M3, if M1 ∼(∃σ∀) M2, then
M1 ⊗M3 ∼(∃σ∀) M2 ⊗M3.
We have considered so far the parallel composition via synchronous produc-
tion, which is working by the deﬁnition of folding collapsing all labels to a single
transition. Here we consider the other extreme of the parallel composition: inter-
leaving only.
Deﬁnition 13. Given two IMDPs Ml and Mr, we deﬁne the interleaved com-
position
of Ml and Mr as the IMDP M = (S, ¯s, A, AP, L, I ) where
• S = Sl × Sr;
• ¯s = (¯sl, ¯sr);
• A = (Al × {l}) ∪(Ar × {r});
• AP = APl ∪APr;
• for each (sl, sr) ∈S, L(sl, sr) = Ll(sl) ∪Lr(sr); and
•
I ((sl, sr), (a, i), (tl, tr)) =
⎧
⎪
⎨
⎪
⎩
Il(sl, a, tl)
if i = l and tr = sr,
Ir(sr, a, tr)
if i = r and tl = sl,
[0, 0]
otherwise.
Theorem 14. Given three IMDPs M1, M2, and M3, if M1 ∼(∃σ∀) M2, then
6
Case Studies
We implemented in a prototypical tool the proposed bisimulation minimization
algorithm and applied it to several case studies. The bisimulation algorithm is
tested on several PRISM [29] benchmarks extended to support also intervals
in the transitions. For the evaluation, we have used a machine with a 3.6 GHz
Intel i7-4790 with 16 GB of RAM of which 12 assigned to the tool; the timeout
has been set to 30 mins. Our tool reads a model speciﬁcation in the PRISM
input language and constructs an explicit-state representation of the state space.
Afterwards, it computes the quotient using the algorithm in Fig. 2.
Table 1 shows the performance of our prototype on a number of case studies
taken from the PRISM website [36], where we have replaced some of the prob-
abilistic choices with intervals. Despite using an explicit representation for the
model, the prototype is able to manage cases studies in the order of millions of
states and transitions (columns “Model”, “|S|”, and “|I |”). The time in seconds
required to compute the bisimulation relation and the size of the correspond-
ing quotient IMDPare shown in columns “t∼”, “|S∼|”, and “|I∼|”. In order to
improve the performance of the tool, we have implemented optimizations, such
as caching equivalent LP problems, which improve the runtime of our prototype.
Because of this, we saved to solve several LP problems in each tool run, thereby
avoiding the potentially costly solution of LP problems from becoming a bottle-
neck. However, the more reﬁnements are needed, the more time is required to

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
37
Table 1. Experimental evaluation of the bisimulation computation
Model
|S|
|I | t∼(s)
|S∼|
|I∼|
Consensus-Shared-Coin-3
5 216
13 380
1
787
1 770
Consensus-Shared-Coin-4
43 136
144 352
3
2 189
5 621
Consensus-Shared-Coin-5
327 936
1 363 120
26
5 025 14 192
Consensus-Shared-Coin-6
2 376 448 11 835 456
238 10 173 30 861
Crowds-5-10
111 294
261 444
1
107
153
Crowds-5-20
2 061 951
7 374 951
20
107
153
Crowds-5-30
12 816 233 61 511 033
149
107
153
Crowds-5-40
–MO–
Mutual-Exclusion-PZ-3
2 368
8 724
4
475
1 632
Mutual-Exclusion-PZ-4
27 600
136 992
70
3 061 13 411
Mutual-Exclusion-PZ-5
308 800
1 930 160
534 12 732 65 661
Mutual-Exclusion-PZ-6
3 377 344 25 470 144
–TO–
Dining-Phils-LR-nofair-3
956
3 048
1
172
509
Dining-Phils-LR-nofair-4
9 440
40 120
14
822
3 285
Dining-Phils-LR-nofair-5
93 068
494 420
622
5 747 29 279
Dining-Phils-LR-nofair-6
917 424
5 848 524
–TO–
3
4
5
6
103
104
105
106
107
Model parameters
Consensus-Shared-Coin model
5
10
15
20
25
30
102
103
104
105
106
107
108
Model parameters
Crowds model
|S|
|I |
|S∼|
|I∼|
Fig. 3. Eﬀectiveness of bisimulation minimization on model reduction

38
V. Hashemi et al.
3
4
5
6
10−2
10−1
Model parameters
Consensus-Shared-Coin model
5
10
15
20
25
30
10−5
10−4
10−3
10−2
Model parameters
Crowds model
|S∼|/|S|
|I∼|/|I |
Fig. 4. State and transition reduction ratio by bisimulation minimization
complete the minimization, since several new LP problems need to be solved.
The plots in Fig. 3 show graphically the number of states and transitions for
the Consensus and Crowds experiments, where for the latter we have considered
more instances than the ones reported in Table 1. As we can see, the bisimula-
tion minimization is able to reduce considerably the size of the IMDP, by several
orders of magnitude. Additionally, this reduction correlates positively with the
number of model parameters as depicted in Fig. 4.
7
Concluding Remarks
In this paper, we have analyzed interval Markov decision processes under con-
troller synthesis semantics in a dynamic setting. In particular, we provided
an eﬃcient compositional bisimulation minimization approach for IMDPs with
respect to the competitive semantics, encompassing both the controller and para-
meter synthesis semantics. In this regard, we proved that alternating probabilis-
tic bisimulation for IMDPs with respect to the competitive semantics can be
decided in polynomial time. From perspective of compositional reasoning, we
showed that alternating probabilistic bisimulations for IMDPs are congruences
with respect to synchronous product and interleaving. Finally, we presented
results obtained with a prototype tool on several case studies to show the eﬀec-
tiveness of the developed algorithm.
The core part of this algorithm relies on verifying strictly minimal polytopes
in polynomial time, which depends on the special structure of the uncertainty
polytopes. For future work, we aim to explore the possibility of preserving this
computational eﬃciency for MDPs with richer formalisms for uncertainties such
as likelihood or ellipsoidal uncertainties.

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
39
References
1. Alur, R., Henzinger, T.A., Kupferman, O., Vardi, M.Y.: Alternating reﬁnement
relations. In: Sangiorgi, D., de Simone, R. (eds.) CONCUR 1998. LNCS, vol. 1466,
pp. 163–178. Springer, Heidelberg (1998). doi:10.1007/BFb0055622
2. Baier, C., Katoen, J.-P.: Principles of Model Checking. The MIT Press, Cambridge
(2008)
3. Ben-Or, M.: Another advantage of free choice: completely asynchronous agreement
protocols (extended abstract). In: PODC, pp. 27–30 (1983)
4. Benedikt, M., Lenhardt, R., Worrell, J.: LTL model checking of interval Markov
chains. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
32–46. Springer, Heidelberg (2013). doi:10.1007/978-3-642-36742-7 3
5. B¨ode, E., Herbstritt, M., Hermanns, H., Johr, S., Peikenkamp, T., Pulungan, R.,
Rakow, J., Wimmer, R., Becker, B.: Compositional dependability evaluation for
STATEMATE. ITSE 35(2), 274–292 (2009)
6. Cattani, S., Segala, R.: Decision algorithms for probabilistic bisimulation. In: Brim,
L., Kˇret´ınsk´y, M., Kuˇcera, A., Janˇcar, P. (eds.) CONCUR 2002. LNCS, vol. 2421,
pp. 371–386. Springer, Heidelberg (2002). doi:10.1007/3-540-45694-5 25
7. Chatterjee, K., Sen, K., Henzinger, T.A.: Model-checking ω-regular properties of
interval Markov chains. In: Amadio, R. (ed.) FoSSaCS 2008. LNCS, vol. 4962, pp.
302–317. Springer, Heidelberg (2008). doi:10.1007/978-3-540-78499-9 22
8. Chehaibar, G., Garavel, H., Mounier, L., Tawbi, N., Zulian, F.: Speciﬁcation, ver-
iﬁcation of the PowerScale R
⃝bus arbitration protocol: An industrial experiment
with LOTOS. In: FORTE, pp. 435–450 (1996)
9. Coste, N., Hermanns, H., Lantreibecq, E., Serwe, W.: Towards performance predic-
tion of compositional models in industrial GALS designs. In: Bouajjani, A., Maler,
O. (eds.) CAV 2009. LNCS, vol. 5643, pp. 204–218. Springer, Heidelberg (2009).
doi:10.1007/978-3-642-02658-4 18
10. Delahaye, B., Katoen, J.-P., Larsen, K.G., Legay, A., Pedersen, M.L., Sher, F.,
W asowski, A.: Abstract probabilistic automata. In: Jhala, R., Schmidt, D. (eds.)
VMCAI 2011. LNCS, vol. 6538, pp. 324–339. Springer, Heidelberg (2011). doi:10.
1007/978-3-642-18275-4 23
11. Delahaye, B., Katoen, J.-P., Larsen, K.G., Legay, A., Pedersen, M.L., Sher, F.,
Wasowski, A.: New results on abstract probabilistic automata. In: ACSD, pp. 118–
127 (2011)
12. Delahaye, B., Larsen, K.G., Legay, A., Pedersen, M.L., W asowski, A.: Decision
problems for interval Markov Chains. In: Dediu, A.-H., Inenaga, S., Mart´ın-Vide,
C. (eds.) LATA 2011. LNCS, vol. 6638, pp. 274–285. Springer, Heidelberg (2011).
doi:10.1007/978-3-642-21254-3 21
13. Fecher, H., Leucker, M., Wolf, V.: Don’t Know in probabilistic systems. In: Valmari,
A. (ed.) SPIN 2006. LNCS, vol. 3925, pp. 71–88. Springer, Heidelberg (2006).
doi:10.1007/11691617 5
14. Fischer, M.J., Lynch, N.A., Paterson, M.S.: Impossibility of distributed consensus
with one faulty process. J. ACM 32(2), 374–382 (1985)
15. Gebler, D., Hashemi, V., Turrini, A.: Computing behavioral relations for proba-
bilistic concurrent systems. In: Remke, A., Stoelinga, M. (eds.) Stochastic Model
Checking. Rigorous Dependability Analysis Using Model Checking Techniques for
Stochastic Systems. LNCS, vol. 8453, pp. 117–155. Springer, Heidelberg (2014).
doi:10.1007/978-3-662-45489-3 5

40
V. Hashemi et al.
16. Givan, R., Leach, S.M., Dean, T.L.: Bounded-parameter Markov decision processes.
Artif. Intell. 122(1–2), 71–109 (2000)
17. Hahn, E.M., Han, T., Zhang, L.: Synthesis for PCTL in parametric markov decision
processes. In: Bobaru, M., Havelund, K., Holzmann, G.J., Joshi, R. (eds.) NFM
2011. LNCS, vol. 6617, pp. 146–161. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-20398-5 12
18. Hahn, E.M., Hashemi, V., Hermanns, H., Turrini, A.: Exploiting robust optimiza-
tion for interval probabilistic bisimulation. In: Agha, G., Van Houdt, B. (eds.)
QEST 2016. LNCS, vol. 9826, pp. 55–71. Springer, Cham (2016). doi:10.1007/
978-3-319-43425-4 4
19. Hansson, H., Jonsson, B.: A logic for reasoning about time and reliability. Formal
Asp. Comput. 6(5), 512–535 (1994)
20. Hashemi, V., Hateﬁ, H., Krˇc´al, J.: Probabilistic bisimulations for PCTL model
checking of interval MDPs. In: SynCoP, pp. 19–33. EPTCS (2014)
21. Hashemi,
V.,
Hermanns,
H.,
Song,
L.,
Subramani,
K.,
Turrini,
A.,
Wojciechowski, P.: Compositional bisimulation minimization for interval Markov
decision processes. In: Dediu, A.-H., Janouˇsek, J., Mart´ın-Vide, C., Truthe, B.
(eds.) LATA 2016. LNCS, vol. 9618, pp. 114–126. Springer, Cham (2016). doi:10.
1007/978-3-319-30000-9 9
22. Hashemi, V., Hermanns, H., Turrini, A.: Compositional reasoning for interval
Markov decision processes. http://arxiv.org/abs/1607.08484
23. Hermanns, H., Katoen, J.-P.: Automated compositional Markov chain generation
for a plain-old telephone system. SCP 36(1), 97–127 (2000)
24. Iyengar, G.N.: Robust dynamic programming. Math. Oper. Res. 30(2), 257–280
(2005)
25. Jonsson, B., Larsen, K.G.: Speciﬁcation and reﬁnement of probabilistic processes.
In: LICS, pp. 266–277 (1991)
26. Kanellakis, P.C., Smolka, S.A.: CCS expressions, ﬁnite state processes, and three
problems of equivalence. I&C, pp. 43–68 (1990)
27. Katoen, J.-P., Kemna, T., Zapreev, I., Jansen, D.N.: Bisimulation minimisation
mostly speeds up probabilistic model checking. In: Grumberg, O., Huth, M. (eds.)
TACAS 2007. LNCS, vol. 4424, pp. 87–101. Springer, Heidelberg (2007). doi:10.
1007/978-3-540-71209-1 9
28. Kozine, I., Utkin, L.V.: Interval-valued ﬁnite Markov chains. Reliable Comput.
8(2), 97–113 (2002)
29. Kwiatkowska, M., Norman, G., Parker, D.: PRISM 4.0: veriﬁcation of probabilistic
real-time systems. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011. LNCS, vol.
6806, pp. 585–591. Springer, Heidelberg (2011). doi:10.1007/978-3-642-22110-1 47
30. Lahijanian, M., Andersson, S.B., Belta, C.: Formal veriﬁcation and synthesis for
discrete-time stochastic systems. IEEE Tr. Autom. Contr. 60(8), 2031–2045 (2015)
31. Larsen, K.G., Skou, A.: Bisimulation through probabilistic testing (preliminary
report). In: POPL, pp. 344–352 (1989)
32. Luna, R., Lahijanian, M., Moll, M., Kavraki, L.E.: Asymptotically optimal sto-
chastic motion planning with temporal goals. In: Akin, H.L., Amato, N.M., Isler,
V., van der Stappen, A.F. (eds.) Algorithmic Foundations of Robotics XI. STAR,
vol. 107, pp. 335–352. Springer, Cham (2015). doi:10.1007/978-3-319-16595-0 20
33. Luna, R., Lahijanian, M., Moll, M., Kavraki, L.E.: Fast stochastic motion planning
with optimality guarantees using local policy reconﬁguration. In: ICRA, pp. 3013–
3019 (2014)
34. Luna, R., Lahijanian, M., Moll, M., Kavraki, L.E.: Optimal and eﬃcient stochastic
motion planning in partially-known environments. In: AAAI, pp. 2549–2555 (2014)

Polynomial-Time Alternating Probabilistic Bisimulation for Interval MDPs
41
35. Paige, R., Tarjan, R.E.: Three partition reﬁnement algorithms. SIAM J. Comput.
16(6), 973–989 (1987)
36. PRISM model checker. http://www.prismmodelchecker.org/
37. Puggelli, A.: Formal Techniques for the Veriﬁcation and Optimal Control of Prob-
abilistic Systems in the Presence of Modeling Uncertainties. Ph.D. thesis, EECS
Department, University of California, Berkeley (2014)
38. Puggelli, A., Li, W., Sangiovanni-Vincentelli, A.L., Seshia, S.A.: Polynomial-time
veriﬁcation of PCTL properties of MDPs with convex uncertainties. In: Sharygina,
N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 527–542. Springer, Heidelberg
(2013). doi:10.1007/978-3-642-39799-8 35
39. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, New York (1998)
40. Segala, R.: Modeling and Veriﬁcation of Randomized Distributed Real-Time Sys-
tems. Ph.D. thesis, MIT (1995)
41. Segala, R.: Probability and nondeterminism in operational models of concurrency.
In: Baier, C., Hermanns, H. (eds.) CONCUR 2006. LNCS, vol. 4137, pp. 64–78.
Springer, Heidelberg (2006). doi:10.1007/11817949 5
42. Sen, K., Viswanathan, M., Agha, G.: Model-checking markov chains in the presence
of uncertainties. In: Hermanns, H., Palsberg, J. (eds.) TACAS 2006. LNCS, vol.
3920, pp. 394–410. Springer, Heidelberg (2006). doi:10.1007/11691372 26
43. Wolﬀ, E.M., Topcu, U., Murray, R.M.: Robust control of uncertain Markov decision
processes with temporal logic speciﬁcations. In: CDC, pp. 3372–3379 (2012)

Better Automated Importance Splitting
for Transient Rare Events
Carlos E. Budde1,2(B), Pedro R. D’Argenio2,3(B), and Arnd Hartmanns1(B)
1 University of Twente, Enschede, The Netherlands
a.hartmanns@utwente.nl
2 Universidad Nacional de Córdoba, Córdoba, Argentina
3 Saarland University, Saarbrücken, Germany
Abstract. Statistical model checking uses simulation to overcome the
state space explosion problem in formal veriﬁcation. Yet its runtime
explodes when faced with rare events, unless a rare event simulation
method like importance splitting is used. The eﬀectiveness of impor-
tance splitting hinges on nontrivial model-speciﬁc inputs: an importance
function with matching splitting thresholds. This prevents its use by
non-experts for general classes of models. In this paper, we propose new
method combinations with the goal of fully automating the selection of
all parameters for importance splitting. We focus on transient (reacha-
bility) properties, which particularly challenged previous techniques, and
present an exhaustive practical evaluation of the new approaches on case
studies from the literature. We ﬁnd that using Restart simulations with
a compositionally constructed importance function and thresholds deter-
mined via a new expected success method most reliably succeeds and per-
forms very well. Our implementation within the Modest Toolset sup-
ports various classes of formal stochastic models and is publicly available.
1
Introduction
Nuclear reactors, smart power grids, automated storm surge barriers, networked
industrial automation systems: We increasingly rely on critical technical systems
and infrastructures whose failure would have drastic consequences. It is impera-
tive to perform a quantitative evaluation in the design phase based on a formal
stochastic model, e.g. on extensions of continuous-time Markov chains (CTMC),
stochastic Petri nets (SPN), or fault trees. Only if the probability of failure can be
shown to be suﬃciently low can the system design be implemented. Calculating
such probabilities—which may be on the order of 10−19 or lower—is challenging:
For ﬁnite-state Markov chains or probabilistic timed automata (PTA [23]), prob-
abilistic model checking can numerically approximate the desired probabilities,
but the state space explosion problem limits it to small models. For other models,
in particular those involving events governed by general continuous probability
distributions, model checking techniques only exist for speciﬁc subclasses with
limited scalability [26] or merely compute probability bounds [14].
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 42–58, 2017.
https://doi.org/10.1007/978-3-319-69483-2_3

Better Automated Importance Splitting for Transient Rare Events
43
Statistical model checking (SMC [17,33]), i.e. using Monte Carlo simulation
with formal models, has become a popular alternative for large models and for-
malisms not amenable to model checking. It trades memory for runtime: memory
usage is constant but the number of simulation runs explodes with the desired
precision. When an event’s true probability is 10−19, for example, we may want
to be conﬁdent that the error of our estimation is at most on the order of 10−20.
Rare event simulation (RES [28]) methods have been developed to attack this
problem. They increase the number of simulation runs that reach the rare event
and adjust the statistical evaluation accordingly. The main RES methods are
importance sampling and importance splitting. The former modiﬁes probabili-
ties in the model to make the event more likely. The challenge lies in ﬁnding a
good such change of measure. Importance splitting instead performs more sim-
ulation runs, which however may start from a non-initial state and end early.
Here, the challenge is to ﬁnd an importance function that assigns to each state
a value indicating how “close” it is to the rare event. More (partial) runs will
be started from states with higher importance. Additionally, depending on the
concrete splitting method used, thresholds (the subset of importance values at
which to start new runs) and splitting factors (how many new runs to generate
at each threshold) need to be chosen. The performance of RES varies drastically
with the choices made for these parameters. The quality of a choice of para-
meters highly depends on the model at hand; making good choices requires an
expert in the system domain, the modelling formalism, and the selected RES
method.
Aligning RES with the spirit of (statistical) model checking as a “push-
button” approach requires methods to automatically select (usually) good para-
meters. These methods must not negate the memory usage advantages of SMC.
Between importance sampling and splitting, the latter appears more amenable to
automatic approaches that work well across modelling formalisms (CTMC, PTA,
etc.). We previously proposed a compositional method to automatically con-
struct an importance function [4]. Its compositionality is the key to low memory
usage. Our Fig tool [3] for RES of input-output stochastic automata (IOSA [8])
implements this method together with the Restart splitting algorithm [30],
thresholds computed via a sequential Monte Carlo (SEQ) approach [3,6], and a
single ﬁxed splitting factor speciﬁed by the user for all thresholds. Experimen-
tal results [3] show that Fig works well for steady-state measures, but less so
for transient properties. In particular, runtime varies signiﬁcantly between tool
invocations due to diﬀerent thresholds being computed by SEQ, and the optimal
splitting factor varies signiﬁcantly between diﬀerent models.
Our contributions. In this paper, we investigate several alternative combinations
of splitting and threshold/factor selection algorithms with the goal of improving
the automation, robustness and performance of importance splitting for RES
in SMC. We keep the compositional method for automatic importance function
construction as implemented in Fig. Aside from Restart, we consider the ﬁxed
eﬀort [9] and ﬁxed success [25,28] splitting methods (Sect. 3). While Restart
was proposed for steady-state measures and only later extended to transient

44
C.E. Budde et al.
properties [31], the latter two were designed for estimating probabilities of tran-
sient events in the ﬁrst place. For threshold selection, we specify a new “expected
success” (EXP) technique as an alternative to SEQ (Sect. 4). EXP selects thresh-
olds and an individual splitting factor for each threshold, removing the need
for the user to manually select a global splitting factor. We implemented all
techniques in the modes simulator of the Modest Toolset [16]. They can be
freely combined, and work for all the formalisms supported by modes—including
CTMC, IOSA, and deterministic PTA. Our ﬁnal and major contribution is an
extensive experimental evaluation (Sect. 5) of the various combinations on six
case studies.
Related work. A thorough theoretical and empirical comparison of variants of
Restart is presented in [9], albeit in a non-automated setting. Approaching
the issue of automation, Jégourel et al. [19,20] use a layered restatement of
the formula specifying the rare event to build an importance function for use
with adaptive multilevel splitting [5], the predecessor of SEQ. Garvels et al. [10]
derive importance functions for ﬁnite Markov chains from knowledge about their
steady-state behaviour. For SPN, Zimmermann and Maciel [34] provide a mono-
lithic method, though limited to a restricted class of models and throughput
measures [35]. Importance sampling has been automated for SPN [27] restricted
to Markovian ﬁring delays and a global parameterisation of the transition inten-
sities [35]. The diﬃculties of automating importance sampling are also illustrated
in [18]: the proposed automatic change of measure guarantees a variance reduc-
tion, yet is proved for stochastic behaviour described by integrable products of
exponentials and uniforms only. We do not aim at provable improvements in
speciﬁc settings, but focus on general models and empirically study which meth-
ods work best in practice. We are not aware of other practical methods for, or
comparisons of, automated splitting approaches on general models.
2
Preliminaries
We write {| . . . |} for multisets, in contrast to sets written as { . . . }. N is the set
of natural numbers { 0, 1, . . . } and N+ = N \ { 0 }. In our algorithms, operation
S.remove() returns and removes an element from the set S. The element may be
picked according to any policy (e.g. uniformly at random, in FIFO order, etc.).
2.1
Simulation Models
We develop RES approaches that can work for any stochastic formalism com-
bining discrete and continuous state. We thus use an abstract notion of models:
Deﬁnition 1. A (simulation) model M is a discrete-time Markov process whose
states consist of a discrete and (optionally) a continuous part. It has a ﬁxed
initial state that can be obtained as M.initial(). Operation M.step(s) samples a
path from state s and returns the path’s next state after one time step.

Better Automated Importance Splitting for Transient Rare Events
45
Example 1. A CTMC M ctmc is a continuous-time stochastic process. We can
cast it as a simulation model Msim by using the number of transitions taken as
the (discrete) time steps of Msim. Thus, given a state s of M ctmc, Msim.step(s)
returns the ﬁrst state s′ of M ctmc encountered after taking a single transition
from s on a sample path. In eﬀect, we follow the embedded discrete-time Markov
chain. Only if the event of interest refers to time do we also need to keep track
of the global elapsed (continuous) time as part of the states of Msim.
We require models to be Markov processes. For formalisms with memory, e.g.
due to general continuous probability distributions, we encode the memory (e.g.
values and expiration times of clocks in the case of IOSA) in the state space. We
compute probabilities for transient properties, or more precisely the probability
to reach a set of target states while avoiding another disjoint set of states:
Deﬁnition 2. A transient property φ ∈S →{ true, false, undecided } for a
model with state space S maps target states to true, states to be avoided to false,
and all other states to undecided. We require that the probability of reaching a
state where φ returns true or false is 1 from a model’s initial state.
To determine whether a sample path satisﬁes φ, evaluate φ sequentially for every
state on the path and return the ﬁrst outcome ̸= undecided. Standard SMC/
Monte Carlo simulation generates a large number n of sample paths to estimate
the probability p of the transient property as ˆp
def
= ntrue
n , where ntrue is the number
of paths that satisﬁed φ, and reports a conﬁdence interval around the estimate for
a speciﬁed conﬁdence level. This corresponds to estimating the value of the until
formula P=?(¬ avoid Utarget) in a logic like PCTL (as used in e.g. Prism [22]) for
state formulas avoid and target. Time-bounded until U≤b is encoded by tracking
the elapsed time tglobal in states and including tglobal > b in avoid.
2.2
Ingredients of Importance Splitting
Importance splitting increases the simulation eﬀort for states “close” to the target
set. Closeness is represented by an importance function fI ∈S →N that maps
each state to its importance in { 0, . . . , max fI }. To simplify our presentation,
we assume that fI(M.initial()) = 0, (φ(starget) = true) ⇒fI(starget) = max fI,
and if s′ := M.next(s), then |fI(s) −fI(s′)| ≤1. These assumptions can easily be
removed. The performance, but not the correctness, of all importance splitting
methods hinges on the quality of the importance function. Traditionally, it is
speciﬁed ad hoc for each model domain by a RES expert [9,28,30]. Methods
to automatically compute one [10,19,20,34] are usually specialised to a speciﬁc
formalism or a particular model structure, potentially providing guaranteed eﬃ-
ciency improvements. We build on the method of [4] that is applicable to any
stochastic compositional model with a partly discrete state space. It does not
provide mathematical guarantees of performance improvements, but is aimed at
generality and providing “usually good” results with minimal user input.
Compositional fI . A compositional model is a parallel composition of compo-
nents M = M1 ∥. . . ∥Mn. Each component can be seen as a model on its own, but

46
C.E. Budde et al.
the components may interact, usually via some synchronisation/handshaking
mechanism. We write the projection of state s of M to the discrete local variables
of component Mi as s|i. The compositional method works as follows:
1. Convert the target set formula target to negation normal form (NNF) and
associate each literal targetj with the component M(targetj) whose local state
variables it refers to. Literals must not refer to multiple components.
2. Explore the discrete part of the state space of each component Mi. For each
targetj with Mi = M(targetj), use reverse breadth-ﬁrst search to compute the
local minimum distance f j
i (s|i) of each state s|i to a state satisfying targetj.
3. In the syntax of the NNF of target, replace every occurrence of targetj by
f j
i (s|i) with i such that Mi = M(targetj), and every Boolean operator ∧or ∨
by +. Use the resulting formula as the importance function fI(s).
Full implementation details can be found in [3]. Other operators can be used in
place of +, e.g. max or multiplication. Aside from the choice of operator, with
+ as default since it works well for most models, the procedure requires no user
input. It takes into account both the structure of the target set formula and
the structure of the state space. Memory usage is determined by the number
of discrete local states (required to be ﬁnite) over all components. Typically,
component state spaces are small even when the composed state space explodes.
Levels, Thresholds and Factors. Given a model and importance function fI,
importance splitting could spawn more simulation runs whenever the current
sample path moves from a state with importance i to one with importance
j > i. Using the compositional approach, the probability of visiting a state
with a higher importance is often close to 1 for many of the i, so splitting on
every increment would lead to excessively many (partial) runs and high runtime.
Importances are hence partitioned into a set of intervals called levels. This results
in a level function fL ∈S →N where, again, the initial state is on level 0 and all
target states are on the highest level max fL. We refer to the boundary between
the highest importance of level l −1 and the lowest importance i of level l as the
threshold Tl, identiﬁed by i. Some splitting methods are further parameterised
by the “amount of splitting” at each threshold or the “eﬀort” at each level; we
use splitting factor and eﬀort functions fS resp. fE in N →N+ for this purpose.
3
Splitting Methods
We now brieﬂy describe, from a practical perspective, the three diﬀerent
approaches to importance splitting that we implemented and evaluated.
3.1
Restart
Originally discovered in 1970 [2] and popularised by J. Villén-Altamirano and M.
Villén-Altamirano [30], the Restart importance splitting method was designed

Better Automated Importance Splitting for Transient Rare Events
47
Input: model M, level function fL, splitting factors fS, transient property φ
1 S := {| ⟨M.initial(), 0⟩|}, ˆp := 0
// start with the initial state from level 0
2 while S ̸= ∅do
// perform main and child runs (Restart loop)
3
⟨s, l⟩:= S.remove(), lcreate := l
// get next split and store creation level
4
while φ(s) = undecided do
// run until property decided (simulation loop)
5
s := M.step(s)
// simulate up to next change in discrete state
6
if fL(s) < lcreate then break
// moved below creation level: kill run
7
else if fL(s) > l then
// moved one level up: split run
8
l := fL(s), S := S ∪{| ⟨s, l⟩, . . .(fS(l) −1 times). . . , ⟨s, l⟩|}
9
if φ(s) then ˆp := ˆp + 1/ l
i=1 fS(l) // update result if we hit the rare event
10 return ˆp
Algorithm 1. The Restart method for importance splitting
Fig. 1. Restart
for
steady-state
measures
and
later
extended to transient properties [31]. It
works by performing one main simulation
run from the initial state. As soon as any
run crosses a threshold from below, new
child runs are started from the ﬁrst state
in the new level l (the run is split). Their
number is given by l’s splitting factor:
fS(l) −1 child runs are started, resulting
in fS(l) runs that continue after splitting.
Each run is tagged with the level on which
it is created. When a run crosses a thresh-
old from above into a level below its creation level, it ends (the run is killed). A
run also ends when it reaches an avoid or target state. We state Restart for-
mally as Algorithm 1. Figure 1 illustrates its behaviour. The horizontal axis is the
model’s time steps while the vertical direction shows the current state’s impor-
tance. target states are marked ✓and avoid states are marked ✗. We have three
levels with thresholds at importances 3 to 4 and 9 to 10. fS is { 1 
→3, 2 
→2 }.
The result of a Restart run—consisting of a main and several child runs—is
the weighted number of runs that reach target. Each run’s weight is 1 divided
by the product of the splitting factors of all levels. The result is thus a positive
rational number. Note that this is in contrast to standard Monte Carlo simula-
tion, where each run is a Bernoulli trial with outcome 0 or 1. This aﬀects the
statistical analysis on which the conﬁdence interval over multiple runs is built.
Restart is carefully designed s.t. the mean of the results of many Restart runs
is an unbiased estimator for the true probability of the transient property [32].
3.2
Fixed Eﬀort
In contrast to Restart, each run of the ﬁxed eﬀort method [9,11] performs a
ﬁxed number fE(l) of partial runs on each level l. Each of these ends when it

48
C.E. Budde et al.
Input: model M, level function fL, eﬀort function fE, transient property φ
1 L := { 0 →[ S := { M.initial() }, n := 0, up := 0 ] }
// set up data for level 0
2 for l from 0 to max fL do
// iterate over all levels from initial to target
3
for i from 1 to fE(l) do
// perform sub-runs on level (ﬁxed eﬀort loop)
4
s :∈L(l).S, L(l).n := L(l).n + 1
// pick from the level’s initial states
5
while φ(s) = undecided do
// run until φ is decided (simulation loop)
6
s := M.step(s)
// simulate up to next change in discrete state
7
if fL(s) > l then
// moved one level up: end sub-run
8
L(l).up := L(l).up + 1
// level-up run for current level
9
L(fL(s)).S := L(fL(s)).S ∪{ s }
// initial state for next level
10
break
11
if φ(s) then L(l).up := L(l).up + 1 // hit rare event (highest level only)
12
if L(l).up = 0 then return 0
// we cannot reach the target any more
13 return max fL
i=0
L(l).up/L(l).n
// multiply conditional level-up prob. estimates
Algorithm 2. The ﬁxed eﬀort method for importance splitting
Fig. 2. Fixed eﬀort
either crosses a threshold from below into
level l + 1, encounters a target state, or
encounters an avoid state. We count the
ﬁrst two cases as nlup. In the ﬁrst case, the
new state is stored in a set of initial states
for level l + 1. When all partial runs for
level l have ended, the algorithm moves
to level l + 1, starting the next round of
partial runs from the previously collected
initial states of the new level. This behav-
iour is illustrated in Fig. 2 (with fE(l) = 5
for all levels) and formally stated as Algo-
rithm 2. The initial state of each partial run can be chosen randomly, or in a
round-robin fashion among the available initial states [9]. When a ﬁxed eﬀort
run ends, the fraction of partial runs started in level l that moved up is an
approximation of the conditional probability of reaching level l + 1 given that
level l was reached. Since target states exist only on the highest level, the overall
result is thus simply the product of the fraction nlup/fE(l) for all levels l, i.e.
a rational number in the interval [0, 1]. The average of the result of many ﬁxed
eﬀort runs is again an unbiased estimator for the probability of the transient
property [11].
The ﬁxed eﬀort method is speciﬁcally designed for transient properties. Its
advantage is predictability: each run involves at most max fL
l=0
fE(l) partial runs.
Like Restart needs splitting levels via function fS, ﬁxed eﬀort needs the eﬀort
function fE that determines the number of partial runs for each level.

Better Automated Importance Splitting for Transient Rare Events
49
3.3
Fixed Success
Fixed eﬀort intuitively controls the simulation eﬀort by adjusting the estimator’s
imprecision. The ﬁxed success method [1,25] turns this around: its parameters
control the imprecision, but the eﬀort then varies. Instead of launching a ﬁxed
number of partial runs per level, ﬁxed success keeps launching such runs until
fE(l) of them have reached the next level (or a target state in case of the highest
level). Illustrated in Fig. 3 (with fE(l) = 4 for all levels), the algorithmic steps
are as in Algorithm 2 except for two changes: First, the for loop in line 3 is
replaced by a while loop with condition L(l).up < fE(l). Second, the ﬁnal
return statement in line 13 uses a diﬀerent estimator: instead of max fL
i=0
L(l).up
L(l).n ,
we have to return max fL
i=0
L(l).up−1
L(l).n−1 . This is due to the underlying negative
binomial distribution; see [1] for details. The method thus requires fE(l) ≥2 for
all levels l.
Fig. 3. Fixed success
From the automation perspective, the
advantage of ﬁxed success is that it self-
adapts to the (a priori unknown) proba-
bility of levelling up: if that probability is
low for some level, more partial runs will
be generated on it, and vice-versa. How-
ever, the desired number of successes still
needs to be speciﬁed. 20 is suggested as
a starting point in [1], but for a speciﬁc
setting already. A disadvantage of ﬁxed
success is that it is not guaranteed to ter-
minate: If the model, importance function
and thresholds are such that, with positive probability, it may happen that all
initial states found for some level lie in a bottom strongly connected component
without target states, then the (modiﬁed) loop of line 3 of the algorithm diverges.
We have not encountered this situation in our experiments, though.
4
Determining Thresholds and Factors
To determine the splitting levels/thresholds, we implement and compare two
approaches: the sequential Monte Carlo (SEQ) method from [3] and a new tech-
nique that tries to ensure a certain expected number of runs that level up.
4.1
Sequential Monte Carlo
Our ﬁrst approach is inspired by the sequential Monte Carlo splitting tech-
nique [6]. It works in two alternating phases: First, n simulation runs determine
the importances that can be reached from the current level , keeping track of the
state of maximum importance for each run. We sort these states by ascending
importance and pick the importance of the one at position n −k, i.e. the (n –
k)-th n-quantile of importances, as the start of the next level. This means that

50
C.E. Budde et al.
Input: model M, importance function fI, transient property φ, n ∈N+
1 fL := fI, fE := { l →n | l ∈{ 0, . . . , max fI } }
2 m := 0, e := 0, pup := { l →0 | l ∈{ 0, . . . , max fI } }
3 while pup(max fI) = 0 do
// roughly estimate the level-up probabilities
4
m := m + 1, L := level data computed in one ﬁxed eﬀort run (Algorithm 2)
5
for l from 0 to max fI do pup(l) := pup(l) + 1
m(L(l).up/L(l).n −pup(l))
6 for l from 0 to max fI do
// turn level-up probabilities into splitting factors
7
split := 1/pup(l) + e, F(l) := ⌊split + 0.5⌋, e := split −F(l)
8 return F
// if F(l) > 1, then l is a threshold and F(l) the splitting factor
Algorithm 3. The expected success method for threshold and factor selection
as parameter k grows, the width of the levels decreases and the probability of
moving from one level to the next increases. In the second phase, the algorithm
randomly selects k new initial states that lie just above the newfound threshold
via more simulation runs. This extra phase is needed to obtain new reachable
states because we cannot generate them directly as in the setting of [6]. We
then proceed to the next round to compute the next threshold from the new
initial states. Detailed pseudocode is shown as Algorithm 5 in [3]. The result is
a sequence of importances characterising a level function.
This SEQ algorithm only determines the splitting levels. It does not decide
on splitting factors, which the user must select if they wish to run Restart. Fig
and modes request a ﬁxed splitting factor g and then run SEQ with k = n/g.
When used with ﬁxed eﬀort and ﬁxed success, we set k = n/2 and use a user-
speciﬁed eﬀort value e for all levels. A value for n must also be speciﬁed; by
default n = 1000. The degree of automation oﬀered by SEQ is clearly not satis-
factory. Furthermore, we found in previous experiments with Fig that the levels
computed by diﬀerent SEQ runs diﬀered signiﬁcantly, leading to large varia-
tions in Restart performance [3]. Combined with mediocre results for transient
properties, this was the main trigger for the work we present in this paper.
SEQ may get stuck in the same way as ﬁxed success. We encountered this
with our wlan case study of Sect. 5. Our tool thus restarts SEQ after a 30 s
timeout; on the wlan model, it then always succeeded with at most two retries.
4.2
Expected Success
To replace SEQ, we propose a new approach based on the rule-of-thumb that one
would like the expected number of runs that move up on each level to be 1. This
rule is called “balanced growth” by Garvels [11]. The resulting procedure, shown
as Algorithm 3, is conceptually much simpler than SEQ: We ﬁrst perform ﬁxed
eﬀort runs, using constant eﬀort n and each importance as a level, until the rare
event is encountered. We extract the approximations of the conditional level-up
probabilities computed inside the ﬁxed eﬀort runs, averaging the values if we
need multiple runs (line 5). After that, we set the factor for each importance to
one divided by the (very rough) estimate of the respective conditional probability

Better Automated Importance Splitting for Transient Rare Events
51
computed in the ﬁrst phase. Since splitting factors are natural numbers, we round
each factor, but carry the rounding error to the next importance. In this way,
even if the exact splitting factors would all be close to 1, we get a rounded
splitting factor of 2 for some of the importances. The result is a mapping from
importances to splitting factors, characterising both the level function fL—every
importance with a factor ̸= 1 starts a new level—and the splitting function fS.
We call this procedure the expected success (ES) method. Aside from the choice
of n (we use a default of n = 256, which has worked well in all experiments),
it provides full automation with Restart. To use it with ﬁxed eﬀort, we need
a user-speciﬁed base eﬀort value e, and then set fE to { l 
→e · fS(l) | l ∈
{ 0, . . . , max fL } } resulting in a weighted ﬁxed eﬀort approach. Note that our
default of n = 256 is much lower than the default of n = 1000 for SEQ. This
is because SEQ performs simple simulation runs where ES performs ﬁxed eﬀort
runs, each of which provides more information about the behaviour of the model.
We also experimented with expected numbers of runs that move up of 2
and 4, but these always lead to dismal performance or timeouts due to too many
splits or partial runs in our experiments, so we do not consider them any further.
5
Experimental Evaluation
The goal of our work was to ﬁnd a RES approach that provides consistently
good performance at a maximal degree of automation. We thus implemented
compositional importance function generation, the splitting methods described
in Sect. 3, and the threshold calculation methods of Sect. 4 in the modes simulator
of the Modest Toolset [14]. This allowed us to study CTMC queueing mod-
els, network protocols modelled as PTA, and a more complex ﬁleserver setting
modelled as stochastic timed automata (STA [14]) using a single tool.
5.1
Case Studies
tandem: Tandem queueing networks are standard benchmarks in probabilistic
model checking and RES [10–12,24,29]. We consider the case from [4] with all
exponentially distributed interarrival times (a CTMC). The arrival rate into the
ﬁrst queue q1 (initially empty) is 3 and its service rate is 2. After that, packets
move into the second queue q2 (initially containing one packet), to be processed
at rate 6. The model has one parameter C, the capacity of each queue. We
estimate the value of the transient property P=?(q2 > 0 U q2 = C), i.e. of the
second queue becoming full without having been empty before.
openclosed: Our second CTMC has two parallel queues [13], both initially
empty: an open queue qo, receiving packets at rate 1 from an external source,
and a closed queue qc that receives internal packets. One server processes pack-
ets from both queues: packets from qo are processed at rate 4 while qc is empty;
otherwise, packets from qc are served at rate 2. The latter packets are put back
into another internal queue, which are independently moved back to qc at rate
1
2. We study the system as in [3] with a single packet in internal circulation,

52
C.E. Budde et al.
i.e. an M/M/1 queue with server breakdowns, and the capacity of qo as parameter.
We estimate P=?(¬ reset U lost): the probability that qo overﬂows before a packet
is processed from qo or qc such that the respective queue becomes empty again.
breakdown: The ﬁnal queueing system that we consider [21] as a CTMC consists
of ten sources of two types, ﬁve of each, that produce packets at rate λ1 = 3 (type
1) or λ2 = 6 (type 2), periodically break down with rate β1 = 2 resp. β2 = 4 and
get repaired with rate α1 = 3 resp. α2 = 1. The produced packets are collected
in a single queue, attended to by a server with service rate μ = 100, breakdown
rate γ = 3 and repair rate δ = 4. Again, and as in [4], we parameterise the model
by the queue’s capacity, here denoted K, and estimate P=?(¬ reset U buf = K):
starting from a single packet in the queue, what is the probability for the queue
to overﬂow before it becomes empty?
brp: We also study two PTA examples from [15]. The ﬁrst is the bounded
retransmission protocol, another classic benchmark in formal veriﬁcation. We
use parameter M to determine the actual parameters N (the number of chunks
to transmit), MAX (the retransmission bound) and TD (the transmission delay)
by way of ⟨N, MAX , TD⟩= ⟨16 · 2M, 4 · M, 4 · 2M⟩. We thus consider the large
instances ⟨32, 4, 8⟩, ⟨64, 8, 16⟩and ⟨128, 12, 32⟩. To avoid nondeterminism, TD is
both lower and upper bound for the delay. We estimate P=?(true U snok ∧i > N
2 ),
i.e. the probability that the sender eventually reports unsuccessful transmission
after more than half of the chunks have been sent successfully.
wlan: Our second PTA model is of IEEE 802.11 wireless LAN with two stations.
In contrast to [15] and the original Prism case study, we use the timing parame-
ters from the standard (leading to a model too large for standard probabilistic
model checkers) and a stochastic semantics of the PTA (scheduling events as soon
as possible and resolving all other nondeterminism uniformly). The parameter is
K, the maximum backoﬀcounter value. We estimate P=?(true U bc1 = bc2 = K),
the probability that both stations’ backoﬀcounters reach K.
ﬁleserver: Our last case study combines exponentially and uniformly distrib-
uted delays. It is an STA model of a ﬁle server where some ﬁles are archived
and require signiﬁcantly more time to retrieve. Introduced in [14], we change
the archive access time from nondeterministic to continuously uniform over the
same interval. Model parameter C is the server’s queue size. We estimate the
time-bounded probability of queue overﬂow: P=?(true U≤1000 queue = C).
We consider several queueing systems since these are frequently used bench-
marks for RES [10–13,21,24,29]. The CTMC could easily be modiﬁed to use
general distributions and our techniques and tools would still work the same.
5.2
Experimental Setup
The experiments for the tandem and wlan models were performed on a four-core
Intel Core i5-6600T (2.7/3.5 GHz) system running 64-bit Windows 10 v1607 x64
using three simulation threads. All other experiments ran on a six-core Intel Xeon

Better Automated Importance Splitting for Transient Rare Events
53
Table 1. Model data and performance results
E5-2620v3 (2.4/3.2 GHz, 12 logical processors) system with Mono 5.2 on 64-bit
Debian v4.9.25 using ﬁve simulation threads each for two separate experiments
running concurrently. We used a timeout of 600 s for the tandem, openclosed and
brp models and 1200 s for the others. Simulations were run until the half-width of
the 95 % normal conﬁdence interval was at most 10 % of the currently estimated
mean1. By this use of a relative width, precision automatically adapted to the
rareness of the event. We also performed SMC/Monte Carlo simulation as a
comparison baseline (labelled “SMC” in results), where modes uses the Agresti-
Coull approximation of the binomial conﬁdence interval. For each case study
and parameterisation, we evaluated the following combinations of methods:
– Restart with thresholds selected via SEQ and a ﬁxed splitting factor g ∈
{ 2, 4, 8, 16 } (labelled “Restart g”), using n = 512 and k = n/g for SEQ;
– Restart with thresholds and splitting factors determined by the ES method
(labelled “Restart ES”) and the default n = 256 for ES;
– ﬁxed eﬀort with SEQ (n = 512, k = n/2) and eﬀort e ∈{ 16, 64, 256 };
– weighted ﬁxed eﬀort with ES (labelled “-weighted”) as described in Sect. 4.2
using base eﬀort e ∈{ 8, 16, 128 } since all weights are ≥2;
– ﬁxed success with SEQ as before (n = 512, k = n/2) and the required number
of successes for each level being either 8, 32 or 128.
We did not consider ES in cases where the splitting factors it computes would
not be used (such as with “unweighted” ﬁxed eﬀort or ﬁxed success). The default
1 We rely on the standard CLT assumption for large enough sample sizes; to this end,
we do not stop before we obtain at least one sample > 0 and at least 50 samples.

54
C.E. Budde et al.
Fig. 4. Selected performance results compared (runtimes in seconds)

Better Automated Importance Splitting for Transient Rare Events
55
of using addition to replace ∧and ∨in the compositional importance function
(cf. Sect. 2.2) worked well except for wlan, where we used max instead. Restart
with SEQ and a user-speciﬁed splitting factor is the one approach used in Fig [3].
5.3
Results
We provide an overview of the performance results for all model instances in
Table 1. We report the averages of three runs of each experiment to account for
ﬂuctuations due to the inherent randomisation in the simulation and especially
in the threshold selection algorithms. Column ˆp lists the average of all (up to 45)
individual estimates for each instance. All estimates were consistent, including
SMC in the few cases where it did not time out. To verify that the compositional
importance function construction does not lead to high memory usage, we list
the total number of states that it needs to store in column nI. These numbers
are consistently low; even on the two PTA cases, they are far below the total
number of states of the composed state spaces. The remaining columns report
the total time, in seconds, that each approach took to compute the importance
function, perform threshold selection, and use the respective splitting method to
estimate the probability of the transient rare event. Dashes mark timeouts.
We show some interesting cases graphically with added details in Fig. 4. ✗
marks timeouts. Each bar’s darker part is the time needed to compute the impor-
tance function and thresholds. The lighter part is the time for the actual RES.
The former, which is almost entirely spent in threshold selection, is much lower
for ES than for SEQ. The error bars show the standard deviation between the
convergence times of the three runs that we performed for each experiment.
Our experiments ﬁrst conﬁrm previous observations made with Fig: The
performance of Restart depends not only on the importance function, but also
very much on the thresholds and splitting factor. Out of g ∈{ 2, 4, 8, 16 }, there
was no single optimal splitting factor that worked well for all models. Restart
with ES usually performed best, being drastically faster than any other method
in many cases. This is a very encouraging result since Restart with ES is
also the one approach that requires no more user-selected parameters. We thus
selected it as the default for modes. The wlan case is the only one where this
default, and in fact none of the Restart-based methods, terminated within our
1200 s time bound. All of the splitting methods speciﬁcally designed for transient
properties, however, worked for wlan, with ﬁxed success performing best. They
also work reasonably well on the other cases, but we see that their performance
depends on the chosen eﬀort parameter. In contrast to the splitting factors for
Restart, though, we can make a clear recommendation for this choice: larger
eﬀort values rather consistently result in better performance.
6
Conclusion
We investigated ways to improve the automation and performance of impor-
tance splitting to perform rare event simulation for general classes of stochastic

56
C.E. Budde et al.
models. For this purpose, we studied and implemented three existing splitting
methods and two threshold selection algorithms, one from a previous tool and
one new. Our implementation in the Modest Toolset is publicly available at
www.modestchecker.net. We performed extensive experiments, resulting in the
only practical comparison of Restart and other methods that we are aware of.
Our results show that we have found a fully automated rare event simulation
approach based on importance splitting that performs very well: automatic com-
positional importance functions together with Restart and the expected success
method. It is also easier to implement than our previous approach with SEQ in
Fig, and ﬁnally pushes automated importance splitting for general models into
the realm of very rare events with probabilities down to the order of 10−23.
As future work, we would like to more deeply investigate models with few
points of randomisation such as the PTA examples that proved to be the most
challenging for our methods, and combine RES with the lightweight scheduler
sampling technique [7] to properly handle models that include nondeterminism.
Acknowledgements. We are grateful to José Villén-Altamirano for very helpful dis-
cussions that led to our eventual design of the expected success method.
This work is supported by the 3TU.BSR project, ERC grant 695614 (POWVER), the
NWO SEQUOIA project, and SeCyT-UNC projects 05/BP12 and 05/B497.
References
1. Amrein, M., Künsch, H.R.: A variant of importance splitting for rare event esti-
mation: Fixed number of successes. ACM Trans. Model. Comput. Simul. 21(2),
13:1–13:20 (2011)
2. Bayes, A.J.: Statistical techniques for simulation models. Aust. Comput. J. 2(4),
180–184 (1970)
3. Budde, C.E.: Automation of Importance Splitting Techniques for Rare Event Simu-
lation. Ph.D. thesis, Universidad Nacional de Córdoba, Córdoba, Argentina (2017)
4. Budde, C.E., D’Argenio, P.R., Monti, R.E.: Compositional construction of impor-
tance functions in fully automated importance splitting. In: VALUETOOLS (2016)
5. Cérou, F., Guyader, A.: Adaptive multilevel splitting for rare event analysis. Stoch.
Anal. Appl. 25(2), 417–443 (2007)
6. Cérou, F., Moral, P.D., Furon, T., Guyader, A.: Sequential Monte Carlo for rare
event estimation. Stat. Comput. 22(3), 795–808 (2012)
7. D’Argenio, P.R., Hartmanns, A., Legay, A., Sedwards, S.: Statistical approximation
of optimal schedulers for probabilistic timed automata. In: Ábrahám, E., Huisman,
M. (eds.) IFM 2016. LNCS, vol. 9681, pp. 99–114. Springer, Cham (2016). doi:10.
1007/978-3-319-33693-0_7
8. D’Argenio, P.R., Lee, M.D., Monti, R.E.: Input/Output stochastic automata. In:
Fränzle, M., Markey, N. (eds.) FORMATS 2016. LNCS, vol. 9884, pp. 53–68.
Springer, Cham (2016). doi:10.1007/978-3-319-44878-7_4
9. Garvels, M.J.J., Kroese, D.P.: A comparison of RESTART implementations. In:
Winter Simulation Conference, WSC, pp. 601–608 (1998)
10. Garvels, M.J.J., van Ommeren, J.C.W., Kroese, D.P.: On the importance function
in splitting simulation. Eur. Trans. Telecommun. 13(4), 363–371 (2002)

Better Automated Importance Splitting for Transient Rare Events
57
11. Garvels, M.J.J.: The splitting method in rare event simulation. Ph.D. thesis, Uni-
versity of Twente, Enschede, The Netherlands (2000)
12. Glasserman, P., Heidelberger, P., Shahabuddin, P., Zajic, T.: A large deviations
perspective on the eﬃciency of multilevel splitting. IEEE Trans. Autom. Control
43(12), 1666–1679 (1998)
13. Glasserman, P., Heidelberger, P., Shahabuddin, P., Zajic, T.: Multilevel splitting
for estimating rare event probabilities. Oper. Res. 47(4), 585–600 (1999)
14. Hahn, E.M., Hartmanns, A., Hermanns, H.: Reachability and reward checking for
stochastic timed automata. In: ECEASST 70 (2014)
15. Hartmanns, A., Hermanns, H.: A Modest approach to checking probabilistic timed
automata. In: QEST, pp. 187–196. IEEE Computer Society (2009)
16. Hartmanns, A., Hermanns, H.: The Modest Toolset: an integrated environment
for quantitative modelling and veriﬁcation. In: Ábrahám, E., Havelund, K. (eds.)
TACAS 2014. LNCS, vol. 8413, pp. 593–598. Springer, Heidelberg (2014). doi:10.
1007/978-3-642-54862-8_51
17. Hérault, T., Lassaigne, R., Magniette, F., Peyronnet, S.: Approximate probabilistic
model checking. In: Steﬀen, B., Levi, G. (eds.) VMCAI 2004. LNCS, vol. 2937, pp.
73–84. Springer, Heidelberg (2004). doi:10.1007/978-3-540-24622-0_8
18. Jegourel, C., Larsen, K.G., Legay, A., Mikučionis, M., Poulsen, D.B., Sedwards,
S.: Importance sampling for stochastic timed automata. In: Fränzle, M., Kapur,
D., Zhan, N. (eds.) SETTA 2016. LNCS, vol. 9984, pp. 163–178. Springer, Cham
(2016). doi:10.1007/978-3-319-47677-3_11
19. Jegourel, C., Legay, A., Sedwards, S.: Importance splitting for statistical model
checking rare properties. In: Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol.
8044, pp. 576–591. Springer, Heidelberg (2013). doi:10.1007/978-3-642-39799-8_38
20. Jegourel, C., Legay, A., Sedwards, S.: An eﬀective heuristic for adaptive importance
splitting in statistical model checking. In: Margaria, T., Steﬀen, B. (eds.) ISoLA
2014. LNCS, vol. 8803, pp. 143–159. Springer, Heidelberg (2014). doi:10.1007/
978-3-662-45231-8_11
21. Kroese, D.P., Nicola, V.F.: Eﬃcient estimation of overﬂow probabilities in queues
with breakdowns. Perform. Eval. 36, 471–484 (1999)
22. Kwiatkowska, M., Norman, G., Parker, D.: PRISM 4.0: veriﬁcation of probabilistic
real-time systems. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011. LNCS, vol.
6806, pp. 585–591. Springer, Heidelberg (2011). doi:10.1007/978-3-642-22110-1_47
23. Kwiatkowska, M.Z., Norman, G., Segala, R., Sproston, J.: Automatic veriﬁcation
of real-time systems with discrete probability distributions. Theor. Comput. Sci.
282(1), 101–150 (2002)
24. L’Ecuyer, P., Demers, V., Tuﬃn, B.: Rare events, splitting, and quasi-Monte Carlo.
ACM Trans. Model. Comput. Simul. 17(2) (2007)
25. LeGland, F., Oudjane, N.: A sequential particle algorithm that keeps the particle
system alive. In: EUSIPCO, pp. 1–4. IEEE (2005)
26. Paolieri, M., Horváth, A., Vicario, E.: Probabilistic model checking of regenerative
concurrent systems. IEEE Trans. Softw. Eng. 42(2), 153–169 (2016)
27. Reijsbergen, D., de Boer, P.-T., Scheinhardt, W., Haverkort, B.: Automated rare
event simulation for stochastic Petri nets. In: Joshi, K., Siegle, M., Stoelinga, M.,
D’Argenio, P.R. (eds.) QEST 2013. LNCS, vol. 8054, pp. 372–388. Springer, Hei-
delberg (2013). doi:10.1007/978-3-642-40196-1_31
28. Rubino, G., Tuﬃn, B. (eds.): Rare Event Simulation Using Monte Carlo Methods.
Wiley (2009)
29. Villén-Altamirano, J.: Rare event RESTART simulation of two-stage networks.
Eur. J. Oper. Res. 179(1), 148–159 (2007)

58
C.E. Budde et al.
30. Villén-Altamirano, M., Villén-Altamirano, J.: RESTART: a method for accelerat-
ing rare event simulations. In: Queueing, Performance and Control in ATM (ITC-
13), pp. 71–76. Elsevier (1991)
31. Villén-Altamirano, M., Villén-Altamirano, J.: RESTART: a straightforward
method for fast simulation of rare events. In: WSC, pp. 282–289. ACM (1994)
32. Villén-Altamirano, M., Villén-Altamirano, J.: Analysis of restart simulation: theo-
retical basis and sensitivity study. Eur. Trans. Telecommun. 13(4), 373–385 (2002)
33. Younes, H.L.S., Simmons, R.G.: Probabilistic veriﬁcation of discrete event systems
using acceptance sampling. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002. LNCS,
vol. 2404, pp. 223–235. Springer, Heidelberg (2002). doi:10.1007/3-540-45657-0_17
34. Zimmermann, A., Maciel, P.: Importance function derivation for RESTART simu-
lations of Petri nets. In: RESIM 2012, pp. 8–15 (2012)
35. Zimmermann, A., Reijsbergen, D., Wichmann, A., Canabal Lavista, A.: Numerical
results for the automated rare event simulation of stochastic Petri nets. In: RESIM,
pp. 1–10 (2016)

On the Criticality of Probabilistic Worst-Case
Execution Time Models
Luca Santinelli1(B) and Zhishan Guo2
1 ONERA, Toulouse, France
luca.santinelli@onera.fr
2 Missouri University of Science and Technology, Rolla, USA
guozh@mst.edu
Abstract. Probabilistic approaches to timing analysis derive probabil-
ity distributions to upper bound task execution time. The main purpose
of probability distributions instead of deterministic bounds, is to have
more ﬂexible and less pessimistic worst-case models. However, in order to
guarantee safe probabilistic worst-case models, every possible execution
condition needs to be taken into account.
In this work, we propose probabilistic representations which is able to
model every task and system execution conditions, included the worst-
cases. Combining probabilities and multiple conditions oﬀers a ﬂexible
and accurate representation that can be applied with mixed-critical task
models and fault eﬀect characterizations on task executions. A case study
with single- and multi-core real-time systems is provided to illustrate the
completeness and versatility of the representation framework we provide.
1
Introduction
Nowadays real-time systems are mostly implemented with multi-core and many-
core commercial-oﬀ-the-shelf platforms. Cache memories, branch predictors,
communication buses/networks and other features present in such implementa-
tions allow increasing average performance; nonetheless, they make predictability
much harder to guarantee.
Task execution times are aﬀected by the large variety of system execution
conditions which can happen at runtime. Also, the numerous interferences within
real-time systems may result into signiﬁcant variations of tasks execution times.
This acerbates with multi- and many-core real-time systems implementations.
In essence, task execution time resembles to a random variable where the value
depends on diﬀerent outcomes.
Timing analysis seeks upper bounds to the task execution time, and the pre-
dictability required by real-time systems can be granted. Classically, the bounds
are deterministic Worst-Case Execution Times (WCET) which are single values
able to upper bound the time needed to ﬁnish execution. In order to be safe,
WCETs have to account for any case/execution condition possible, including
the highly improbable pathological cases such as faults. Deterministic WCETs
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 59–74, 2017.
https://doi.org/10.1007/978-3-319-69483-2_4

60
L. Santinelli and Z. Guo
could be very pessimistic with respect to task actual execution times, and could
lead to resource under-utilization.
Probabilistic worst-case models generalize WCETs with worst-case distribu-
tions, probabilistic WCETs (pWCETs), which upper bounds any possible task
execution behavior. pWCET representations focus on probability of occurrence
of worst-case conditions and abstract them into multiple worst-case values with
their correspondent probability of happening. The challenge with probabilistic
timing analysis is guaranteeing the pWCET safety.
System faults have a non negligible impact on worst-case models; although
as pathological and improbable cases, they have to be considered with timing
analysis. Task models have to embed fault manifestations and fault eﬀects in
order to provide upper bounds to every possible condition the real-time system
can experience at runtime.
With multi- and many-core implementations, it comes the opportunity to
combine diﬀerent applications on the same platform which may have diﬀerent
criticality levels, e.g. safety-critical, mission-critical, non-critical, designating the
level of assurance needed against failure. The very low acceptable failure rates
(e.g. 10−9 failures per hour) for high criticality applications imply the need
for signiﬁcantly more rigorous and costly development as well as veriﬁcation
processes than required by low criticality applications. The gradual transfor-
mation of real-time systems into Mixed Criticality (MC) systems demands for
timing analysis ables to cope with multiple criticality levels for tasks.
State of the Art: First papers on probabilistic timing modeling describe the
worst-case execution time of tasks with random variables, using either discrete
[2,22] or continuous [15] distributions. Since Edgar and Burns [10], several papers
have worked on obtaining safe and reliable probabilistic Worst-Case Execution
Time (pWCET) estimates [7,13,14].
Among probabilistic timing analysis approaches, it is possible to distinguish
between Static Probabilistic Timing Analysis (SPTA) and Measurement-Based
Probabilistic Timing Analysis (MBPTA). SPTA methods analyze the software
and use a model of the hardware behavior to derive an estimate of pWCET;
SPTA is applicable when some part of the system or the environment have been
artiﬁcially time randomized, [3,8]. MBPTA approaches rely on the Extreme
Value Theory (EVT) for computing pWCET estimates out of measured behav-
iors [6,11,16]. Figure 1 depicts key elements for MBPTA which accepts input
measurements of task execution times under speciﬁc execution conditions and
applies to those the EVT for inferring pWCET estimates.
Fault modeling and fault management intertwines with timing analysis as
faults introduce latencies to the task execution behavior which have to be embed-
ded into the task models. As examples, in [19] backups are executed for fault
tolerance and recovering form task errors caused by hardware or software faults;
in [25] it is proposed an algorithm to abort and restart a task in case of conﬂicts
in shared resources accesses. In [21], an attempt of including faults into MBPTA
and apply fault-based task models for schedulability and sensitivity analysis of
real-time systems.

On the Criticality of Probabilistic Worst-Case Execution Time Models
61
Research on MC scheduling focuses upon the Vestal task model which assigns
multiple WCET estimates to each task, [23]. This is motivated by the fact that
diﬀerent tools for WCET estimates may be more or less conservative than one
another, [4,24] as reviews. To the best of our knowledge, [12] is a ﬁrst attempt
of a probabilistic MC task modeling with WCET estimates associated to the
probability/conﬁdence of being WCET bound.
Contribution: This work proposes a probabilistic representation framework
for real-time tasks which composes of multiple probabilistic worst-case models,
each estimating the worst-case of a speciﬁc possible execution condition that
both tasks and the system can encounter.
The probabilistic task model proposed can be applied to the MC problem,
since probabilities and multiple pWCETs can characterize the criticality modes
as diﬀerent task conditions as well as diﬀerent conﬁdences. Besides, the proba-
bilistic representation can be used for characterize the eﬀects that faults have on
the tasks executions, proving to be ﬂexible and safe. A case study is presented for
validating the probabilistic models and illustrating its eﬀectiveness in modeling
diﬀerent conditions/criticalities.
Organization of the paper: Section 2 presents the probabilistic background
applied in this work. Section 3 describes the probabilistic worst-case model pro-
posed and based on multiple execution conditions possible. Section 4 details the
MC task representation derived from the the probabilistic worst-case model.
The impact of faults on task models are also considered. Section 5 shows prob-
abilistic MC task models from three diﬀerent real-time test cases; Sect. 6 is for
conclusions and future works.
2
System Modeling
A real-time task consists of a sequence of recurring jobs, each has to complete
execution by a given deadline.
In a periodic task system, a task is described with the 4-tuple (Oi, Ti, Di, Ci),
where Oi is the oﬀset or starting time that speciﬁes the time instant at which
the ﬁrst job of τi is released. Ti is the period as the temporal separation between
two successive jobs of τi. Di is the deadline which deﬁnes the time interval
[Oi + j · Ti, Oi + j · Ti + Di) in which task execution has to take place (the j-th
job of τi). Ci is the WCET deﬁning the processing requirements for each job.
The scheduling policy decides the task execution ordering, possibly with pre-
emption or migration between cores; schedulability analysis of task models guar-
antees the respect of the timing constraints (system predictability) checking if
there are enough resources for the tasks to ﬁnish executions by their deadlines.
In this work, we consider a set Γ = {τ1, . . . , τn} of n periodic probabilistic
tasks τi as such their worst-case execution time is modeled with pWCETs. The
probabilistic modeling framework proposed applies to either single-core, multi-
core or many-core processors.

62
L. Santinelli and Z. Guo
2.1
pWCETs and WCET Thresholds
The pWCET Ci of a task τi is deﬁned as the worst-case estimate distribution that
upper-bounds any possible execution time the task can exhibit [8]. Ci generalize
deterministic WCET estimates Ci by including multiple values, each with the
probability of being the worst-case of the task execution time1.
Assuming the pWCET Ci as continuous distribution, the probability distribu-
tion function (pdf) pdfCi describes the probability of happening of certain events
from the random variable Ci; it is such that P(C1 ≤Ci ≤C2) =
 C2
C1 pdfCi(C)dC
and
 ∞
0
pdfCi(C)dC = 1.
cdfCi denotes the cumulative distribution function (cdf) representation of Ci,
cdfCi(C) = P(Ci ≤C) =
 C
0 pdfCi(x) while the inverse cumulative distribution
function (icdf) icdfCi(C) outlines the exceedence thresholds. icdfCi(C) = P(Ci ≥
C) is the probability of having execution time greater than C, icdfCi(C) = 1 −
 C
0 pdfCi(x).
In case of discrete distributions pWCETs, it is pdfCi(C) = P(Ci = C),
cdfCi(C) = C
0 pdfCi(x) and icdfCi(C) = 1 −C
0 pdfCi(x).
WCET Thresholds From Ci, it is possible to deﬁne WCET thresholds ⟨Ci,j, pi,j⟩
where the value Ci,j is associated to the probability pi,j of being the WCET
for τi. pi,j
def
= icdfCi(Ci,j) quantiﬁes the conﬁdence on Ci,j of being the task
worst-case execution time and 1 −pi,j is the probability of respecting Ci,j.
Depending on the granularity of the pWCET, it would be possible to deﬁne
WCET thresholds at probability of 10−3, 10−6, 10−9, etc.
Worst-Case Distribution Independence. Assuming Ci to be the probabilis-
tic worst-case distribution estimate of τi, it means that in Ci there have already
been included all the possible interferences (and their eﬀects as latencies) that
τi suﬀers, [5]. This is the deﬁnition of statistical independences between tasks,
i.e. the task execution distribution does not change in presence or not of inter-
ferences – conditional probability.
For example, the pWCET distribution of task τi in presence of τj, equiv-
alently while executing together with τj (the conditional distribution pdfCi|Cj)
does not change from the case where τi runs in isolation (pdfCi) since all the
interferences from τj have been taken into account in the worst-case distribution
bound and in order to guarantee it. It is pdfCi|Cj = pdfCi which corresponds to the
deﬁnition of statistical independence between pWCETs and thus tasks. Previous
independence condition holds with all the tasks in Γ and for all the system exe-
cution conditions possible, i.e. worst-case distributions guarantee independence
between between tasks.
1 In the following, calligraphic letters are used to represent distributions while non-
calligraphic letters are for scalars or deterministic values.

On the Criticality of Probabilistic Worst-Case Execution Time Models
63
3
Probabilistic Worst-Case Representations
Whenever correctly applied, the EVT produces a continuous distribution which
is a safe estimation of the worst-case behavior of the task. The EVT guarantees
that if certain hypotheses are veriﬁed, from the actual measured behavior it is
possible to infer rare events, where the worst-case execution time lie [6]. The
outcome of the MBPTA/EVT is the pWCET estimate Ci.
Figure 1 shows the basics of MBPTA with the EVT applied to measurements
of task execution time – average execution time – for inferring pWCET estima-
tion Ci. The task under observation is τi while the rest of the real-time application
Γ \ τi contributes to the interferences on τi. Besides, the speciﬁc measurement
execution condition applied for the task executions sk = f(I, Env, Map, . . .) and
the measurements themselves deﬁne the task’s actual behavior.
Fig. 1. MBPTA where pWCETs are inferred with the EVT speciﬁc execution condi-
tions sk = f(I, Env, Map, . . .) applied for measurements.
The guarantees that the EVT provides worst-case task models strongly
depend on what has been measured, e.g. the execution conditions for the mea-
surements, the conﬁdence or representativity of the measurements.
A trace of execution time measurements accounts for some of the interfering
conditions and inputs (to the system and tasks) which happen at runtime. The
pWCET estimate Ci from the EVT embeds those system conditions and others
which have not been measured (the so called rare events which are costly to
observe by measurements), i.e. the EVT is able to infer some of the unknowns
from the known measurements. Unfortunately, not all the unknowns can be
estimated with the only use of the EVT.
An execution scenario sj = f(I, Env, Map, . . .) abstracts the execution con-
ditions the system (and the task) subdue to. sj represents instances of the inputs
(for tasks and system) I, of the environment Env, of the task mapping and
scheduling policy Map, etc.; sk is a function of I, Env, Map and more. For a
real-time system, there exist a ﬁnite set S of all the possible execution scenar-
ios, S = {s1, s2, . . . , sn}, since inputs, environment conditions, mapping, etc. are
ﬁnite.
The same reasoning would be applicable to SPTA with diﬀerent conditions
sj = f(I, Env, Map, . . .) possible for tasks.

64
L. Santinelli and Z. Guo
3.1
Worst-Case Bounding
The absolute pWCET Ci is the worst-case distribution that upper bounds every
task execution time obtained under any possible execution scenario sj ∈S. The
absolute pWCET Ci is safe if it upper bounds every task execution time under
any execution scenario.
Given sj ∈S, the pWCET Csj
i
comes from the measurements taken under
sj and the EVT applied to them (Fig. 1). Csj
i
is the pWCET speciﬁc to sj, the
relative pWCET; the relative pWCET Csj
i
is safe if it upper bounds any task
execution time under sj.
Measurement representativity is a fundamental requirement for guaranteeing
both absolute and relative pWCETs. We hereby focus on representativity as
the measurement capability of well characterizing multiple execution conditions
(worst-cases included) like in [1,17]. Diﬀerently than those works, we do not
consider artiﬁcially randomized systems that aim at increasing the chances of
measuring the worst-case. We believe that the input representativity can be built
from an enhanced knowledge of the system and of its scenarios, thus from a study
of the system, its S and the coverage of the execution conditions.
From the partial ordering between pWCETs [9], it is possible deﬁning a
notion of dominance for scenarios. With respect to task τi, given sr and st from
S, sr dominates st if and only if Csr
i
is greater than or equal to Cst
i , Csr
i
⪰Cst
i , ⪰
being the “greater than or equal to” operator which deﬁnes the partial ordering
between distributions [9].
It is also possible deﬁning the notion of equivalence between scenarios. Given
sk and sj from S, with respect to task τi sk is equivalent to sj if and only if
there exist values in the support of Csk
i
and Csj
i
for which Csk
i
⪰Csj
i
and there
exist other values in the support of Csk
i
and Cs
′j
i
for which Csj
i
⪰Csk
i .
For a set of equivalent scenarios Sj = {sj, sk, . . . , st} ⊆S (sk, . . . , st equiva-
lent to sj), it is possible deﬁning the scenario sj∗that dominates all the scenarios
in Sj. It would be such that Csj∗
i
def
= maxsj∈Sj{Csj
i }, while with the icdf rep-
resentation, it would be icdfCsj∗
i
(C)
def
= maxsj∈Sj{icdfCsj
i (C)}. sj∗is not a real
scenario, but it dominates all the sk ∈Sj.
Worst-Case Set. The Worst-Case Set task representation is the collection of
all the pWCET from S; Ci is the Worst-Case Set representation as a set of
pWCET estimates such that:
Ci
def
= (Cs1
i , Cs2
i , . . . , Csn
i ).
(1)
With partial ordering between relative pWCETs it is possible ordering scenarios
and get S = {s1, s2, s3, . . . , sk} such that Csk ⪰Csk−1 ⪰. . . ⪰Cs1; the Worst-
Case Set becomes:
Ci
def
= (Cs1
i , Cs2
i , . . . , Csk
i ),
(2)
with sk the worst-case scenario for τi, sworst ≡sk.

On the Criticality of Probabilistic Worst-Case Execution Time Models
65
Although, we hereby focus on MBPTA, the Worst-Case Set representation
applies to both MBPTA and SPTA with multiple execution conditions possible.
Worst-Case Set and Dominance. Although with actual real-time systems
it is reasonable to assume a ﬁnite number of measurement scenarios, enumer-
ating them all remains a complex problem. With dominance between scenarios,
it would be possible neglecting the dominated scenarios in order to ease the
task representation from Eqs. (1) and (2). Moreover, with equivalence between
scenarios it would be possible to assume the correspondent dominating scenario
s∗,j to represent all the equivalent scenarios Sj.
From Eqs. (1) and (2), fewer dominating scenarios S∗could be considered
to represent the task execution behavior. S∗= {sr, sj, sk} ⊆S is such that sr
dominates some scenarios in S, sj dominates other scenarios as well as sr and
sk dominates all the scenarios. The Worst-Case Set becomes:
Ci
def
= (Csr
i , Csj
i , Csk
i ),
(3)
as a less complex probabilistic representation to the task executions; Eq. (3)
remains a safe representation for the task behavior since the worst-cases sk and
Csk
i
are included.
Fig. 2. Multiple scenarios S = {s1, s2, . . . , sk ≡sworst}, each with a trace of execution
time measurements and pWCET estimate.
Figure 2 depicts S = {s1, s2, . . . , sk ≡sworst}, each scenario sj with a trace
of execution time measurements; the resulting pWCETs Csk
i
are illustrated with
the partial ordering guaranteed by ⪰.
At this stage, S is assumed to be known; future work will investigate how to
obtain the diﬀerent scenarios and how to guarantee the existence of worst-cases
among them.
3.2
Probabilistic Task Models
Combining the orthogonal information of WCET thresholds (with probability
associated) and execution scenarios from the Worst-Case Set, Eqs. (1), (2) or (3),
there are possible the inter-scenario and intra-scenario representations.

66
L. Santinelli and Z. Guo
Inter-scenario Representation. The inter-scenario representation character-
izes task behavior across scenarios. Given an exceeding probability p and the
WCET threshold at that exceeding probability for each scenario sj ∈S (equiv-
alently sj ∈S∗), it is ⟨Ci, p⟩such that:
Ci
def
= (Cs1
i , Cs2
i , . . . , Csk
i )
(4)
is the set of WCET thresholds such that ⟨Cj
i , p⟩∀sj ∈S. As an example, it is
possible picking p = 10−9 with ⟨Cj
i , 10−9⟩∀sj ∈S. Equation (4) is the inter-
scenario representation for the task worst-case execution time.
Intra-scenario Representation. The intra-scenario representation describes
the task behavior focusing on a speciﬁc scenario. For a given scenario sj ∈
S (equivalently sj
∈S∗) and a set of exceeding thresholds probabilities
(p1, p2, . . . pm) it is:
ˆCsj
i
def
= (⟨Csj
1,i, p1⟩, ⟨Csj
2,i, p2⟩, . . . , ⟨Csj
m,i, pm⟩).
(5)
Equation (5) is the intra-scenario representation for the task worst-case execu-
tion time on a speciﬁc scenario with all the meaningful WCET thresholds and
exceeding probabilities.
4
Task Modeling Through Criticalities
Each pWCET estimations composing the Worst-Case Set representation implic-
itly carries conﬁdence (as safety) of being the absolute task pWCET; execution
scenarios may be more or less safe in deﬁning pWCET estimates and WCET
thresholds. For example, s1 from Eq. (2) provides the least conﬁdent absolute
pWCET as Cs1
i : Cs1
i
is the least safe absolute pWCET for τi; s2 provides slightly
more conﬁdence that Cs2
i
is the absolute pWCET for τi: Cs2
i
is relatively more
safe than Cs1
i . Going on with the scenarios within S, the safety increases up
until sk which is the worst-case scenario and Csk
i
is the only 100% safe absolute
pWCET for τi; Csk
i
is the safest among the pWCETs.
The MC task model makes use of multiple WCETs for characterizing the
task behavior; such bounds results from diﬀerent timing analysis tools as well as
diﬀerent criticality requirements that task can respect at runtime. For example,
in the two-criticality-level case, each task is designated as being of either higher
(hi) or lower (lo) criticality, and two WCETs are speciﬁed for each hi-criticality
task: a lo-WCET determined by a less pessimistic tool or a less demanding
safety requirements (e.g. mission-critical or non-critical), and a larger hi-WCET
determined by a more conservative tool or more safety-critical requirements.
For real-time systems, safety and criticality have a strong relationship so that
they can be interchanged whenever applied for timing analysis and schedulability
analysis: a safe pWCET is the worst-case models which can apply with high
critical modes.

On the Criticality of Probabilistic Worst-Case Execution Time Models
67
Least critical Scenario lo-critical. In case of s1 from Eq. (2), the task has
the least execution time, thus the least dominating relative pWCET Cs1
i . Cs1
i
upper bounds any (and only) possible execution time resulting from s1; it is
the last safe absolute pWCET, equivalently the least critical lo-criticality. Cs1
i
is applied to characterize lo-criticality requirements of τ1 and the lo-criticality
functional mode.
Critical Scenarios mi-critical. From S ordered by dominance, Eq. (2), s2
dominates s1 because under s2 the task suﬀers execution times bigger than
under s1. Considering Cs2
i
as absolute pWCET, it would be slightly safer than
Cs1
i , but it is not safe enough to upper bound the other sj ∈S. Cs2
i
is the middle
criticality (mi-criticality) characterization for τi.
With s3, it is Cs3
i
dominating Cs2
i
and Cs1
i
since s3 produces larger execution
times than s1 and s2. Thus, Cs3
i
would be safer than Cs2
i
as absolute pWCET.
Also Cs3
i
is a mi-criticality characterization for τi but more critical than Cs2
i .
We distinguish between mi-2-criticality and mi-3-criticality, respectively for
s2 and s3, and Cs3
i
⪰Cs2
i . Other intermediate criticality levels can be deﬁned
from sj ∈(S \ sk).
Most Critical Scenario, hi-critical. The pWCET Csk
i
from sk is the safest
absolute pWCET. Csk
i
is also the hi-criticality bound to the task behavior. Csk
i
≡
Csworst
i
represents the worst conditions and is the most conservative upper bound
for τi to be applied in the highest critical modes.
From Cs1
i
it would be possible to extract ⟨Ci(lo), 10−9⟩. We name Ci(lo) the
lo-critical WCET threshold as it results from the least safe pWCET model Cs1
i
≡
Clo
i . Ci(lo) is the lo-criticality WCET threshold with a conﬁdence of 10−9.
sj, with 1 < j < k form Eq. (2), is a mi-j-criticality scenario that upper bounds
all the scenarios sr such that r ≤j; Csj
i
≡Cmi−j
i
is the mi-j-criticality pWCET.
Csj
i
⪰Csj−1
i
and ⟨Ci(mi −j), 10−9⟩is such that Ci(mi −j) ≥Ci(mi −j −1);
Ci(mi −j) is the mi-j-criticality WCET threshold with a conﬁdence of 10−9.
sk is the worst-case scenario and Csk
i
is the absolute pWCET for τi. Csk
i
≡Chi
i
is the hi-criticality pWCET and sk is the hi-criticality scenario for the worst
conditions. From Chi
i , it is ⟨Ci(hi), 10−9⟩such that Ci(hi) is the hi-criticality
WCET threshold. Chi
i ⪰Cmi−j
i
and Ci(hi) ≥Ci(mi −j).
From the diﬀerence in safety/criticality between sworst, s3, s2 and s1 exe-
cution conditions, it is Chi
i
⪰Cmi
i
⪰Clo
i . Also, Ci(hi) ≥Ci(mi) ≥Ci(lo) for
the same probability p from respectively Chi
i , Cmi
i
and Clo
i . How much they diﬀer
depends on the relationship between the scenarios and the impact that the sce-
narios have on the execution times of tasks. p = 10−9 is chosen arbitrarily, but
the probabilistic modeling proposed can make use of any probability, depending
on the conﬁdence requirements.
The MC Worst-Case Set representation for τi is:
Ci
def
= (Clo
i , . . . , Cmi−j
i
, . . . , Csk
i ).
(6)

68
L. Santinelli and Z. Guo
For the intra- and inter-scenario perspective, adding criticality levels to Eqs. (4)
and (5) it is Ci
def
=
(C(lo)i, . . . , C(hi)i) for the inter-scenario MC represen-
tation ⟨Ci, p⟩at probability p and ˆC(↕)i
def
=
(⟨C(↕)1,i, p1⟩, ⟨C(↕)2,i, p2⟩, . . . ,
⟨C(↕)m,i, pm⟩) for the intra-scenario MC representation at the criticality level l
and probabilities p1, p2, . . . pm.
MC Probabilistic Task Model. With three criticality levels, the MC task
model based on the Worst-Case Set is:
τi = ([Ci, ⟨Ci, 10−9⟩, ( ˆClo
i , ˆCmi
i , ˆChi
i )], Ti, Di),
(7)
where Ci = (Clo
i , Cmi
i , Chi
i ) and ⟨Ci, p⟩= (Clo
i , Cmi
i , Chi
i ). The intra-scenario repre-
sentation is such that ˆClo
i
= (⟨Clo
1,i, 10−3⟩, ⟨Clo
2,i, 10−6⟩, ⟨Clo
3,i, 10−9⟩) for the lo-
safety, ˆCmi
i
= (⟨Cmi
1,i, 10−3⟩, ⟨Cmi
2,i, 10−6⟩, ⟨Cmi
3,i, 10−9⟩) for the mi-safety scenario
and ˆChi
i = (⟨Chi
1,i, 10−3⟩, ⟨Chi
2,i, 10−6⟩, ⟨Chi
3,i, 10−9⟩) for the hi-safety scenario.
The MC task model is essentially asserting that depending on the conditions
for the timing analysis applied it is possible to have more or less guarantees
on the pWCET and the WCET thresholds estimates. Only by considering most
of the possibilities (necessarily the dominating ones) the MC worst-case models
are safe. The MC task model can be generalized to multiple criticality levels and
diﬀerent probabilities in order to better cope with the requirements.
Worst-Case Sets Independence. It is necessary to investigate the statistical
independence between criticality levels and pWCETs for the MC Worst-Case
Set representation. It has already been showed that there exist independence
between absolute pWCETs, thus between hi-criticality representations Chi
i
and
Ci(hi) from sk.
Supposing to have sj representing a criticality level other than hi-criticality,
what happens to the pWCET estimates of τi and τk? Under sj, the condi-
tional probability pdfCsj
i |Csj
k equals pdfCsj
i
(equivalently pdfCsj
k |Csj
i
equals pdfCsj
k )
because all the eﬀects from sj have been included into the relative pWCETs Csj
i
and Csj
k . This assures tasks independence with the same scenario.
With sr dominating sj for both τi and τk, what happens to Csr
i
and Csj
k ? It
is pdfCsr
i |Csj
k = pdfCsr
i , since all the eﬀects of sj and τk on τi have been already
taken into account by Csr
i . This guarantees the independence between τi and τk
under sr and sj, respectively for τi and τk.
Worst-Case Set representations guarantee tasks independence and indepen-
dence between criticality levels which will ease task combination and schedula-
bility analysis.
4.1
Fault Occurrence and Eﬀect
Faults can be modeled with the probability f(t, T) of a fault occurring; f(t, T)
is the probability of fault in a system component by time t, failure ≤t, given

On the Criticality of Probabilistic Worst-Case Execution Time Models
69
that the component was still functional at the end of the previous interval t−T,
failure > t −T. T is the scrubbing period, i.e. the time interval between two
consecutive fault detection to avoid error accumulation.
Several probability distributions are used to model failure times [18]. One
commonly used is the log-normal failure distribution:
f(t, T) = cdfnorm
ln(t)−μ
σ
−cdfnorm( ln(t−T )−μ
σ
)
1 −cdfnorm( ln(t−T )−μ
σ
)
,
(8)
where cdfnorm the cumulative density function of the normal distribution.
The mean and standard deviation parameters of such distribution can
be computed from the Mean Time To Failure (MTTF) such that μ
=
ln(MTTF 2/
√
varMT T F + MTTF 2) and σ =

ln(1 + varMT T F /MTTF 2). In
Eq. (8), f(t, T) depends on the actual time t and the scrubbing period T.
Fault and Criticalities. Faults (either transient or permanent) translate into
penalties δ (latencies) to the task execution time which depends on the time t
the fault happens, δ(t). With C(t) the expected task execution time at time t, in
presence of fault it would be C(t) + δ(t) the task execution time accounting for
the fault penalty on task computations. With a measurement-based approach,
fault eﬀects on task execution can be measured and directly embedded into traces
of execution time measurements; then, with EVT it is possible infer pWCET
estimates which upper bounds faulty execution conditions.
Diﬀerent scenarios are possible with respect to faults. By considering non-
faulty conditions (fault never happening), it is scenario sNF that describes the
task behavior. Here, the execution times observed exploit only the task functional
behavior due to the absence of faults. For sNF it exists CsNF
i
; sNF is the lo-
criticality scenario with Clo
i
≡CNF
i
, C(lo) and ˆCi(lo) representing it.
It could also exist sF W which assumes that the worst fault condition man-
ifests at runtime; CF W
i
is the pWCET estimate for sF W . sF W is the wort-case
scenario where the task executes always under the most critical conditions;
Chi
i ≡CF W
i
, Ci(hi) and ˆCi(hi) represents it.
In between these two extreme scenarios, it exist a set of possible faulty
scenarios where faults are not as extreme as sF W and sNF , nonetheless they
happen and aﬀect the normal task behavior. For example, it could exist sF 1
which is the mi-1-criticality scenario with Cmi−1
i
≡CsF 1
i
, Ci(mi −1) and
ˆCi(mi −1); Ci(mi −1) ≥Ci(lo) and Cmi−1
i
⪰Clo
i . It could also exist sF 2
as the mi-2-criticality scenario with Cmi−2
i
≡CsF 2
i
, C(mi −2) and ˆCi(mi −2);
Ci(mi −2) ≥Ci(mi −1) and Cmi−2
i
⪰Cmi−1
i
.
Speciﬁc to faults and faulty scenarios, it is S = {sNF
≡slo, sF 1 ≡
smi−1, sF 2 ≡smi−2, . . . , sF W ≡smi} with the task MC Worst-Case Set given
by Ci = (Cslo
i
, Csmi−1
i
, Csmi−2
i
, . . . , Cshi
i ).
What we are hereby proposing is a representation framework that applies
to faults eﬀects. It could abstract diﬀerent faults and fault tolerant mechanisms

70
L. Santinelli and Z. Guo
implemented as recovery functions or task extra-executions resulting into larger
task execution times and worst-case execution times.
5
Case Study
Three case studies are presented for illustrating the ﬂexibility and the eﬀective-
ness of the Worst-Case Set representation in modeling execution conditions, MC
tasks and fault eﬀects.
5.1
Test Case 1
As a ﬁrst test case, it is the ns task from the M¨alardalen WCET benchmark
executed a multi-core platform, [20]; in this conﬁguration, ns executes alone
on one core while other tasks and the OS execute on diﬀerent cores making
interference through shared resources. The ns considered here has four scenar-
ios S = {s1, s2, s3, s4} depending only to the task inputs; s4 ≡sworst dom-
inates all the other scenarios, s3 dominates s2 and s1, and so on. The four
traces of execution time measurements are trace 1, trace 2, trace 3 and trace 4,
respectively for s1, s2, s3 and s4 and they embed all the known task behav-
iors, worst-case included. From S = {s1, s2, s3, s4}, it is possible to deﬁne four
criticality levels (lo, mi −2, mi −3, hi) for the scenarios and their respective
pWCETs, S = {s1 ≡slo, s2 ≡smi−2, s3 ≡smi−3, s4 ≡shi}. Figure 3 presents
the four traces of measurements under slo, smi−1, smi−2 and shi, respectively
trace 1, trace 2, trace 3 and trace 4. The criticality levels depend on the exe-
cution considered and the dominance between them. Figure 6 compares the
pWCETs (Clo, Cmi−2, Cmi−3, Chi). shi is conﬁrmed to be the worst-case scenario
and Chi is the hi-criticality task pWCET.
Fig. 3. ns multi-scenario benchmark with four execution time measurements traces;
the execution time in ordinate are in CPU cycles.

On the Criticality of Probabilistic Worst-Case Execution Time Models
71
5.2
Test Case 2
The second test case is the lms task from the M¨alardalen WCET benchmark. The
task is executed in a multi-core platform concurrently with other interfering task
and RTEMS OS within the same core as well as outside it, [20]. While executing,
lms experience two scenarios S = {s1, s2} from OS and environmental conditions
possible at runtime; two traces of execution time exist, trace2 1 and trace2 2 and
they cover all the conditions lms can experience. trace2 2 dominates trace2 1 in
terms of measured execution times. The two scenarios deﬁne lo-criticality and
hi-criticality conditions and pWCET models.
Fig. 4. lms benchmark with two traces of execution time measurements trace2 1 and
trace2 2.
Figure 4 presents the two traces of measurements and a comparison between
Clo and Chi. In particular, Fig. 4(c) illustrates the partial ordering between the
two criticality level with trace2 2 dominating trace2 2.
5.3
Test Case 3
The last test case composes of an artiﬁcial task τ1, τ1 = (([C1, ⟨C1, 10−9⟩,
( ˆClo
1 , ˆCmi
1 , ˆChi
1 )], 50, 50). Task period and deadline coincides and are equal to
50 CPU cycles.
τ1 can execute under its normal functional behavior (no-fault present) slo,
under fault condition smi or under the worst fault condition shi.
τ1 is an artiﬁcial tasks since its normal execution time are considered to follow
a Normal distribution and not measured from a benchmark. For smi, the penalties
δ are extracted randomly from a uniform distribution with a deﬁned MTTF
applied with Eq. (8). Finally, for shi, the penalties δ are extracted the same
uniform distribution but with a smaller MTTF and exhibiting more frequent
faults (more critical).

72
L. Santinelli and Z. Guo
The MC Worst-Case Set representation combines criticality levels (scenar-
ios) and probabilities such that: ⟨Clo
1,1 = 8, 10−6⟩, ⟨Clo
1,2 = 10, 10−9⟩, ⟨Cmi
1,1 =
12, 10−6⟩and ⟨Cmi
1,2 = 14, 10−9⟩, and ⟨Chi
1,1 = 16, 10−6⟩and ⟨Chi
1,2 = 17, 10−9⟩,
from the Normal and Uniform laws applied. p = 10−6 and p = 10−9 are chosen
arbitrarily, but any exceeding probability applies.
Fig. 5. Measurements of τ1 execution time under S = {slo, smi, shi} execution condi-
tions. Execution times in ordinate are in CPU cycles.
Figure 5 illustrates traces of measurements for τ1 obtained as formerly
described. With shi the faults are more frequent, making it the most-critical sce-
nario, and that reﬂects in the pWCET threshold. Figure 7 details the pWCETs
for S = {slo, smi, shi}; Chi is named task1 hi, Cmi is named task1 mi and Clo is
named task1 lo. The dominance between scenarios and criticality levels is con-
ﬁrmed validating the MC task model for faults.
To note that between pWCETs in Fig. 7 there is not strict dominance, since
curves overlaps for small values. The broader dominance as well as the validity
Fig. 6.
ns
pWCET
comparison;
pWCET represented as icdf and exe-
cution time in abscissa are in CPU
cycles and ordinate has log scale.
Fig. 7. τ1 pWCETs represented as icdf;
execution times values are in CPU cycles
and ordinate has log scale.

On the Criticality of Probabilistic Worst-Case Execution Time Models
73
of the probabilistic representation is guaranteed at larger values and smaller
probabilities, p ≤10−3.
6
Conclusion
The work proposed is a probabilistic representation framework for task execution
behaviors named Worst-Case Set, which relies on multiple pWCETs for charac-
terizing the diverse scenarios sj = f(I, Env, Map, . . .) aﬀecting task executions;
probabilities and coverage of multiple execution conditions make the Worst-Case
Set ﬂexible and accurate for task models. The Worst-Case Set is applied for MC
models (for diﬀerent scenario, each with a criticality level associated) and for
faults and fault eﬀects on task executions (also related to criticality). A case
study is presented to validate the probabilistic representation and illustrate its
ﬂexibility in modeling multiple task behaviors from diverse scenarios and criti-
calities.
Future work will apply the Worst-Case Set representations to probabilistic
schedulability analysis as well as to develop schedulability strategies that will
leverage probabilities and multiple criticality levels. It has been assumed that S
was known; future work will be devoted to investigate scenarios complexity and
scenarios completeness for safety and criticality guarantees to probabilistic task
models.
References
1. Abella, J., Qui˜nones, E., Wartel, F., Vardanega, T., Cazorla, F.J.: Heart of gold:
Making the improbable happen to increase conﬁdence in MBPTA. In: 26th Euromi-
cro Conference on Real-Time System (eCRTS) (2014)
2. Abeni, L., Buttazzo, G.: QoS guarantee using probabilistic deadlines. In: IEEE
Euromicro Conference on Real-Time Systems (ECRTS99) (1999)
3. Altmeyer, S., Cucu-Grosjean, L., Davis, R.I.: Static probabilistic timing analysis
for real-time systems using random replacement caches. Real-Time Syst. 51(1),
77–123 (2015)
4. Burns, A., Davis, R.: Mixed-criticality systems: a review (2015). http://www-users.
cs.york.ac.uk/∼burns/review.pdf
5. Cucu-Grosjean, L.: Independence - a misunderstood property of and for (proba-
bilistic) real-time systems. In: the 60th Anniversary of A. Burns, York (2013)
6. Cucu-Grosjean, L., Santinelli, L., Houston, M., Lo, C., Vardanega, T., Kosmidis,
L., Abella, J., Mezzeti, E., Quinones, E., Cazorla, F.J.: Measurement-based prob-
abilistic timing analysis for multi-path programs. In: 23rd Euromicro Conference
on Real-Time Systems (ECRTS). IEEE (2012)
7. Cucu-Grosjean, L., Santinelli, L., Houston, M., Lo, C., Vardanega, T., Kosmidis,
L., Abella, J., Mezzetti, E., Qui˜nones, E., Cazorla, F.J.: Measurement-based prob-
abilistic timing analysis for multi-path programs. In: (ECRTS) (2012)
8. Davis, R.I., Santinelli, L., Altmeyer, S., Maiza, C., Cucu-Grosjean, L.: Analysis of
probabilistic cache related pre-emption delays. In: Proceedings of the 25th IEEE
Euromicro Conference on Real-Time Systems (ECRTS) (2013)

74
L. Santinelli and Z. Guo
9. D´ıaz, J., Garcia, D., Kim, K., Lee, C., Bello, L., Lopez, J.M., Mirabella, O.: Sto-
chastic analysis of periodic real-time systems. In: 23rd of the IEEE Real-Time
Systems Symposium (RTSS) (2002)
10. Edgar, S., Burns, A.: Statistical analysis of WCET for scheduling. In: 22nd of the
IEEE Real-Time Systems Symposium (2001)
11. Guet, F., Santinelli, L., Morio, J.: On the reliability of the probabilistic worst-case
execution time estimates. In: 8th European Congress on Embedded Real Time
Software and Systems (ERTS) (2016)
12. Guo, Z., Santinelli, L., Yang, K.: Edf schedulability analysis on mixed-criticality
systems with permitted failure probability. In: 21st IEEE International Conference
on Embedded and Real-Time Computing System and Applications (2015)
13. Hansen, J., Hissam, S., Moreno, G.: Statistical- based WCET estimation and
validation. In: the 9th International Workshop on Worst-Case Execution Time
(WCET) (2009)
14. Hardy, D., Puaut, I.: Static probabilistic worst case execution time estimation for
architectures with faulty instruction caches. In: International Conference on Real-
Time Networks and Systems (RTNS) (2013)
15. Lehoczky, J.: Real-time queueing theory. In: 10th of the IEEE Real-Time Systems
Symposium (RTSS 1996) (1996)
16. Lima, G., Dias, D., Barros, E.: Extreme value theory for estimating task execution
time bounds: a careful look. In: 28th Euromicro Conference on Real-Time System
(eCRTS) (2016)
17. Milutinovic, S., Abella, J., Cazorla, F.J.: Modelling probabilistic cache representa-
tiveness in the presence of arbitrary access patterns. In: 19th IEEE International
Symposium on Real-Time Distributed Computing, (ISORC) (2016)
18. Panerati, J., Abdi, S., Beltrame, G.: Balancing system availability and lifetime
with dynamic hidden Markov models. In: NASA/ESA Conference on Adaptive
Hardware and Systems (AHS) (2014)
19. Pathan, R.M.: Fault-tolerant and real-time scheduling for mixed-criticality sys-
tems. Real-Time Syst. 50, 509–547 (2014)
20. Santinelli, L., Guet, F., Morio, J.: Revising measurement-based probabilistic timing
analysis. In: Proceedings of the IEEE Real-Time and Embedded Technology and
Applications Symposium (RTAS) (2017)
21. Santinelli, L., Guo, Z., George, L.: Fault-aware sensitivity analysis for probabilistic
real-time systems. In: 2016 IEEE International Symposium on Defect and Fault
Tolerance in VLSI and Nanotechnology Systems (DFT) (2016)
22. Tia, T., Deng, Z., Shankar, M., Storch, M., Sun, J., Wu, L., Liu, J.: Probabilistic
performance guarantee for real-time tasks with varying computation times. In:
IEEE Real-Time and Embedded Technology and Applications Symposium (1995)
23. Vestal, S.: Preemptive scheduling of multi-criticality systems with varying degrees
of execution time assurance. In: Proceedings of the 28th IEEE International Real-
Time Systems Symposium (RTSS). IEEE Computer Society (2007)
24. Wilhelm, R., Engblom, J., Ermedahl, A., Holsti, N., Thesing, S., Whalley, D.B.,
Bernat, G., Ferdinand, C., Heckmann, R., Mitra, T., Mueller, F., Puaut, I.,
Puschner, P.P., Staschulat, J., Stenstr¨om, P.: The worst-case execution-time prob-
lem - overview of methods and survey of tools. ACM Trans. Embedded Comput.
Syst. 7(3), 36:1–36:53 (2008)
25. Wong, H.C., Burns, A.: Schedulability analysis for the abort-and-restart (AR)
model. In: Proceedings of the 22nd International Conference on Real-Time Net-
works and Systems (RTNS) (2014)

Timed and Hybrid Systems

Nested Timed Automata with Invariants
Yuwei Wang1, Guoqiang Li1(B), and Shoji Yuen2
1 School of Software, Shanghai Jiao Tong University, Shanghai, China
{wangywgg,li.g}@sjtu.edu.cn
2 Graduate School of Information Science, Nagoya University, Nagoya, Japan
yuen@is.nagoya-u.ac.jp
Abstract. Invariants are usually adopted into timed systems to con-
strain the time passage within each control location. It is well-known
that a timed automaton with invariants can be encoded to an equivalent
one without invariants. When recursions are taken into consideration, few
results show whether invariants aﬀect expressiveness. This paper inves-
tigates the eﬀect of invariants to Nested Timed Automata (NeTAs), a
typical real-timed recursive system. In particular, we study the reacha-
bility problem for NeTA-Is, which extend NeTAs with invariants. It is
shown that the reachability problem is undecidable on NeTA-Is with a
single global clock, while it is decidable when no invariants are given. Fur-
thermore, we also show that the reachability is decidable if the NeTA-Is
contains no global clocks by showing that a good stack content still sat-
isﬁes well-formed constraints.
1
Introduction
From the past century, many research studies have been carried out on modeling
and veriﬁcation of real time systems. The pioneer work can be traced to Timed
Automata (TAs) [1,2], which is one of the most successful models among them
due to its simplicity, eﬀectiveness and fruitful results. A TA is a ﬁnite automaton
with a ﬁnite set of clocks that grow uniformly. Besides the constraints assigned on
the transitions of TAs, they can also be assigned to each control location, named
invariants, to constrain time passages of models. Invariants usually play a crucial
role in the application modelling and veriﬁcation [3], since in reality a system is
not allowed to stay in one location for arbitrarily long time. It is well-known that
TAs with and without invariants have the same expressive power [3]. However,
little research has been conducted in investigating the impact of invariants on
the reachability problem of timed systems with recursions.
This paper proposes an extension of Nested Timed Automata (NeTAs) [4,5],
called NeTA-Is. A NeTA is a pushdown system whose stack contains TAs with
global clocks passing information among diﬀerent contexts. TAs in the stack can
either be proceeding, in which clocks proceed as time elapses, or frozen, where
clocks remain unchanged. NeTA-Is naturally extend NeTAs with invariants at
each control location that must be fulﬁlled in all valid runs. Studies in [5] have
shown that in NeTAs, (i) the reachability with a single global clock is decidable,
and (ii) the reachability with multiple global clocks is undecidable. While in this
paper, we show that (i) the reachability problem of a NeTA-I is undecidable even
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 77–93, 2017.
https://doi.org/10.1007/978-3-319-69483-2_5

78
Y. Wang et al.
with a single global clock by encoding Minsky machines to NeTA-Is, and (ii) it
is decidable when the NeTA-I has no global clocks by showing that a good stack
content still satisﬁes well-formed constraints [6].
Related Work. Timed Automata (TAs) [1,2] are the ﬁrst model for real-timed
systems. TAs are essentially ﬁnite automata extended with real-valued variables,
called clocks. The reachability of TAs is shown to be decidable based on con-
struction of regions and zones. It is also shown that invariants do not aﬀect the
decidability and thus only a syntactic sugar. Based on timed automata, lots of
extensions are proposed and investigated especially for a recursive structure.
Dense Timed Pushdown Automata (DTPDAs) [7] combine timed automata
and pushdown automata, where each stack frame containing not only a stack
symbol but also a real-valued clock behaves as a basic unit of push/pop oper-
ations. The reachability of a DTPDA is shown to be decidable by encoding it
to a PDA using the region technique. Another decidability proof is given in [6]
through a general framework, well-structured pushdown systems. We adopt this
framework in this paper to prove the decidability of reachability of Constraint
DTPDAs, which extend DTPDAs with clock constraints on each location.
Recursive Timed Automata (RTAs) [8] contain ﬁnite components, each of
which is a special timed automaton and can recursively invoke other components.
Two mechanisms, pass-by-value and pass-by-reference, can be used to passing
clocks among diﬀerent components. A clock is global if it is always passed by
reference, whereas it is local if it is always passed by value. Although the reach-
ability problem of RTAs is undecidable, it is decidable if all clocks are global or
all clocks are local.
Similarly, the reachability problems of both Timed Recursive State Machines
(TRSMs), which combine recursive state machines (RSMs) and TAs, and
Extended Pushdown Timed Automata (EPTAs), which augment Pushdown
Timed Automata (PTAs) with an additional stack, are undecidable, while they
are decidable in some restricted subclasses [9].
To the best of our knowledge, all these prior formal models focusing on timed
systems with recursive structures lacks discussions of the impact of invariants,
including DTPDAs, RTAs, TRSMs, EPTAs and NeTAs.
Paper Organization. The remainder of this paper is structured as follows: In
Sect. 2 we introduce basic terminologies and notations. Section 3 deﬁnes syntax
and the semantics of NeTA-Is. Section 4 shows that the reachability problem of
NeTA-Is is Turing-complete. Section 5 introduces a model Constraint DTPDAs
and shows its decidability. Section 6 is devoted to proofs of decidability results of
NeTA-Is without global clocks by encoding it to a Constraint DTPDA. Section 7
concludes this paper with summarized results.
2
Preliminaries
For ﬁnite words w = aw′, we denote a = head(w) and w′ = tail(w). The
concatenation of two words w, v is denoted by w.v, and ϵ is the empty word.

Nested Timed Automata with Invariants
79
Let R≥0 and N denote the sets of non-negative real numbers and natural
numbers, respectively. Let ω denote the ﬁrst limit ordinal. Let I denote the set
of intervals. An interval is a set of numbers, written as (a, b′), [a, b], [a, b′) or
(a, b], where a, b ∈N and b′ ∈N ∪{ω}. For a number r ∈R≥0 and an interval
I ∈I, we use r ∈I to denote that r belongs to I.
Let X = {x1, . . . , xn} be a ﬁnite set of clocks. The set of clock constraints,
Φ(X), over X is deﬁned by φ :: = ⊤| x ∈I | φ ∧φ where x ∈X and I ∈I.
An operation of extracting constraint EC(φ, x) is deﬁned by induction over its
argument φ.
EC(⊤, x)
=
[0, ω)
EC(x ∈I, x)
=
I
EC(y ∈I, x)
=
[0, ω) if x ̸= y
EC(φ1 ∧φ2, x)
=
EC(φ1, x)

EC(φ2, x)
A clock valuation ν : X →R≥0, assigns a value to each clock x ∈X. ν0 denotes
the clock valuation assigning each clock in X to 0. For a clock valuation ν and
a clock constraint φ, we write ν |= φ to denote that ν satisﬁes the constraint
φ. Given a clock valuation ν and a time t ∈R≥0, (ν + t)(x) = ν(x) + t, for
x ∈X. A clock assignment function ν[y1 ←b1, · · · , yn ←bn] is deﬁned by
ν[y1 ←b1, · · · , yn ←bn](x) = bi if x = yi for 1 ≤i ≤n, and ν(x) otherwise.
Val(X) is used to denote the set of clock valuation of X.
2.1
Timed Automata
A timed automaton is a ﬁnite automaton augmented with a ﬁnite set of clocks [1,2].
Deﬁnition 1 (Timed Automata). A timed automaton (TA) is a tuple A =
(Q, q0, X, I, Δ) ∈A , where
– Q is a ﬁnite set of control locations, with the initial location q0 ∈Q,
– X is a ﬁnite set of clocks,
– I : Q →Φ(X) is a function assigning each location with a clock constraint on
X, called invariants.
– Δ ⊆Q × O × Q, where O is a set of operations. A transition δ ∈Δ is a
triplet (q1, φ, q2), written as q1
φ−→q2, in which φ is either of
Local ϵ, an empty operation,
Test x ∈I? where x ∈X is a clock and I ∈I is an interval,
Reset x ←0 where x ∈X, and
Value passing x ←x′ where x, x′ ∈X.
Given a TA A ∈A , we use Q(A), q0(A), X(A), I(A) and Δ(A) to represent
its set of control locations, initial location, set of clocks, function of invariants and
set of transitions, respectively. We will use similar notations for other models.
We call the four operations Local, Test, Reset, and Value passing as
internal actions which will be used in Deﬁnition 3.
Deﬁnition 2 (Semantics of TAs). Given a TA (Q, q0, X, I, Δ), a conﬁgura-
tion is a pair (q, ν) of a control location q ∈Q and a clock valuation ν on X.
The transition relation of the TA is represented as follows,

80
Y. Wang et al.
– Progress transition: (q, ν)
t−→A (q, ν + t), where t ∈R≥0, ν |= I(q) and
(ν + t) |= I(q).
– Discrete transition: (q1, ν1)
φ−→A (q2, ν2), if q1
φ−→q2 ∈Δ, ν1 |= I(q1), ν2 |=
I(q2) and one of the following holds,
• Local φ = ϵ, then ν1 = ν2.
• Test φ = x ∈I?, ν1 = ν2 and ν2(x) ∈I holds. The transition can be
performed only if the value of x belongs to I.
• Reset φ = x ←0, ν2 = ν1[x ←0]. This operation resets clock x to 0.
• Value passing φ = x ←x′, then ν2 = ν1[x ←ν1(x′)]. The transition
passes value of clock x′ to clock x.
The initial conﬁguration is (q0, ν0).
Remark 1. The TA deﬁnition in Deﬁnition 1 follows the style in [4] and is slightly
diﬀerent from the original deﬁnition in [1]. In [1], several test and reset operations
could be performed in a single discrete transition. It can be shown that our
deﬁnition of TA can soundly simulate the time traces in the original deﬁnition.
3
Nested Timed Automata with Invariants
A nested timed automaton with invariants (NeTA-I) extended from NeTAs1 [5]
is a pushdown system whose stack alphabet is timed automata. It can either
behave like a TA (internal operations), push or fpush the current working TA to
the stack, pop a TA from the stack or reference global clocks. Global clocks can
be used to constrain the global behavior or passing value of local clocks among
diﬀerent TAs. The invariants can be classiﬁed into global invariants, which are
constraints on global clocks, and local invariants, which are constraints on local
clocks. In the executions of a NeTA-I, all invariants must be satisﬁed at all
reachable conﬁgurations, including global invariants and local invariants. Note
that because the stack contains only information belonging to TAs and does not
contain the global clock valuation, there is no need to check global invariants in
the stack.
Deﬁnition 3 (Nested Timed Automata with Invariants). A nested timed
automaton with invariants (NeTA-I) is a tuple N = (T, A0, X, C, I, Δ), where
– T is a ﬁnite set of TAs {A0, A1, · · · , An}, with the initial TA A0 ∈T. We
assume the sets of control locations of Ai, denoted by Q(Ai), are mutually
disjoint, i.e., Q(Ai) ∩Q(Aj) = ∅for i ̸= j. For simplicity, we assume that
each Ai in T shares the same set of local clocks X.
– C is a ﬁnite set of global clocks, and X is the ﬁnite set of k local clocks.
– I : Q →Φ(C) is a function that assigns to each control location an invari-
ant on global clocks. For clarity, I(q) denotes the global invariant in q, and
I(Ai)(q) denotes the local invariant in q where q ∈Ai.
– Δ ⊆Q × (Q ∪{ε}) × Actions+ × Q × (Q ∪{ε}) describes transition rules
below, where Q = ∪Ai∈T Q(Ai).
1 The NeTAs here are called “NeTA-Fs” in [5].

Nested Timed Automata with Invariants
81
A transition rule is described by a sequence of Actions = {internal, push,
fpush, pop, c ∈I, c ←0, x ←c, c ←x} where c ∈C and x ∈X.
Internal (q, ε, internal, q′, ε), which describes an internal transition in the work-
ing TA with q, q′ ∈Q(Ai).
Push (q, ε, push, q0(Ai′), q), which interrupts the currently working TA Ai at
q ∈Q(Ai) and pushes it to the stack with all local clocks of Ai. The local
clocks in the stack generated by Push operation are proceeding, i.e., still
evolve as time elapses. Then, a TA Ai′ newly starts.
Freeze-Push (F-Push) (q, ε, fpush, q0(Ai′), q), which is similar to Push
except that all local clocks in the stack generated by F-Push are frozen (i.e.
stay the same as time elapses).
Pop (q, q′, pop, q′, ε), which restarts Ai′ in the stack from q′ ∈Q(Ai′) after Ai
has ﬁnished at q ∈Q(Ai) and all local clocks restart with values in the top
stack frame.
Global-test (q, ε, c ∈I?, q′, ε), which tests whether the value of a global clock c
is in I with q, q′ ∈Q(Ai).
Global-reset (q, ε, c ←0, q′, ε) with c ∈C, which resets the global clock c to 0
with q, q′ ∈Q(Ai).
Global-load (q, ε, x ←c, q′, ε), which assigns the value of a global clock c to a
local clock x ∈X in the working TA with q, q′ ∈Q(Ai).
Global-store (q, ε, c ←x, q′, ε), which assigns the value of a local clock x ∈X
of the working TA to a global clock c with q, q′ ∈Q(Ai).
Deﬁnition 4 (Semantics of NeTA-Is). Given a NeTA-I (T, A0, X, C, I, Δ),
let ValX = {ν : X →R≥0} and ValC = {μ : C →R≥0}. A conﬁguration of a
NeTA-I is an element (⟨q, ν, μ⟩, v) with a control location q ∈Q, a local clock
valuation ν ∈ValX, a global clock valuation μ ∈ValC and a stack v ∈(Q ×
{0, 1} × ValX)∗. We say a stack v is good, written as v⇑, if all local invariants
are satisﬁed in v, i.e., for each content ⟨qi, flagi, νi⟩in v with qi ∈Q(Aj), νi |=
I(Aj)(qi) holds. We also denote v + t by setting νi := progress(νi, t, flagi) of
each ⟨qi, flagi, νi⟩in the stack where progress(ν, t, flag) =

ν + t if flag = 1
ν
if flag = 0
– Progress transition: (⟨q, ν, μ⟩, v)
t−→(⟨q, ν + t, μ + t⟩, v + t) for t ∈R≥0, where
q ∈Q(Ai), ν |= I(Ai)(q), μ |= I(q), (ν + t) |= I(Ai)(q), (μ + t) |= I(q), v⇑
and (v + t)⇑.
– Discrete transition: (⟨q, ν, μ⟩, v)
ϕ−→(⟨q′, ν′, μ′⟩, v′), where q ∈Q(Ai), q′ ∈
Q(A′
i), ν |= I(Ai)(q), μ |= I(q), ν′ |= I(A′
i)(q′), μ′ |= I(q′), v⇑, v′⇑, and
one of the following holds.
• Internal (⟨q, ν, μ⟩, v)
ϕ−→(⟨q′, ν′, μ⟩, v), if (q, ε, internal, q′, ε) ∈Δ and
⟨q, ν⟩
ϕ−→⟨q′, ν′⟩is in Deﬁnition 2.
• Push
(⟨q, ν, μ⟩, v)
push
−−−→
(⟨q0(Ai′), ν0, μ⟩, ⟨q, 1, ν⟩.v),
if
(q, ε, push,
q0(Ai′), q) ∈Δ.
• F-Push (⟨q, ν, μ⟩, v)
f-push
−−−−→(⟨q0(Ai′), ν0, μ⟩, ⟨q, 0, ν⟩.v), if (q, ε, fpush,
q0(Ai′), q) ∈Δ.
• Pop (⟨q, ν, μ⟩, ⟨q′, flag, ν′⟩.w)
pop
−−→(⟨q′, ν′, μ⟩, w), if (q, q′, pop, q′, ε) ∈Δ.
• Global-test (⟨q, ν, μ⟩, v)
c∈I?
−−−→(⟨q′, ν, μ⟩, v), if (q, ε, c ∈I?, q′, ε) ∈Δ and
μ(c) ∈I.

82
Y. Wang et al.
• Global-reset (⟨q, ν, μ⟩, v)
c←0
−−−→
(⟨q′, ν, μ[c
←
0]⟩, v), if (q, ε, c
←
0, q′, ε) ∈Δ.
• Global-load (⟨q, ν, μ⟩, v)
x←c
−−−→(⟨q′, ν[x ←μ(c)], μ⟩, v), if (q, ε, x ←
c, q′, ε) ∈Δ.
• Global-store(⟨q, ν, μ⟩, v)
c←x
−−−→(⟨q′, ν, μ[c ←ν(x)]⟩, v), if (q, ε, c ←
x, q′, ε) ∈Δ.
The initial conﬁguration of a NeTA-I is (⟨q0(A0), ν0, μ0⟩, ε), where ν0(x) = 0
for x ∈X and μ0(c) = 0 for c ∈C. We use −→to range over these transitions,
and −→∗is the reﬂexive and transitive closure of −→.
Intuitively, in a stack v = (q1, flag1, ν1) . . . (qn, flagn, νn), qi is the control
location of the pushed/fpushed TA, flagi ∈{0, 1} is a ﬂag for whether the TA
is pushed (flagi = 1) or fpushed (flagi = 0) and νi is a clock valuation for the
local clocks of the pushed/fpushed TA.
4
Undecidability Results of NeTA-Is
In this section, we prove undecidability of NeTA-Is by encoding the halting
problem of Minsky machines [10] to NeTA-Is with a single global clock.
Deﬁnition 5 (Minsky Machine). A Minsky machine M is a tuple (L, C, D)
where:
– L is a ﬁnite set of states, and lf ∈L is the terminal state,
– C = {ct1, ct2} is the set of two counters, and
– D is the ﬁnite set of transition rules of the following types,
• increment counter d = inc(l, cti, lk): start from l, cti := cti + 1, goto lk,
• test-and-decrement counter d = dec(l, cti, lk, lm): start from l, if (cti >
0) then (cti := cti −1, goto lk) else goto lm,
where cti ∈C, d ∈D and l, lk, lm ∈L.
In this encoding, we use three TAs, A0, A1 and A2. Each TA has three local
clocks x0, x1 and x2. A0 is a special TA, as two local clocks of A0, x1 and x2
encode values of two counters as xi = 2−cti for i = 1, 2. Decrementing and
incrementing the counter cti are simulated by doubling and halving of the value
of the local clock xi in A0, respectively. In all TAs, x0 is used to prevent time
progress. In A1 and A2, x1 and x2 are used for temporarily storing value. We
use only one global clock c to pass value among diﬀerent TAs.
There are two types of locations in the encoding, q-locations and e-locations.
All q-locations are assigned with invariants x0 ∈[0, 0]. These invariants ensures
that in all reachable conﬁgurations at q-locations, the value of x0 must be 0. So
time does not elapse at q-locations.
The idea of doubling or halving of xi in A0 is as follows. First the value of xi
is stored to the global clock c. Then the current TA A0 is fpushed to the stack
and through transitions in A1 and A2, the global clock c is doubled or halved.
Later A0 is popped back and the value of c is loaded to xi. Since all locations are
q-locations in A0, time does not elapse in A0. This ensures that while doubling
or halving a local clock, the other one is left unchanged.

Nested Timed Automata with Invariants
83
The encoding is shown formally as follows.
A Minsky machine M = (L, C, D) can be encoded into a NeTA-I N =
(T, A0, X, C′, I, Δ), with T = {A0, A1, A2} where
– Q(A0) = {ql | l ∈L} {qinc,i,lk
1
| inc(cti, l, lk) ∈D}
 {qdec,i,lk
j
| dec(cti, l, lk, lm) ∈D, 1 ≤j ≤2}
Q(A1) = {qinc,i,lk
j
| inc(cti, l, lk) ∈D, 2 ≤j ≤8}
 {einc,i,lk
j
| inc(cti, l, lk) ∈D, j = 1, 2 or 4}
 {qdec,i,lk
j
| dec(cti, l, lk, lm) ∈D, 3 ≤j ≤7}
 {edec,i,lk
2
| dec(cti, l, lk, lm) ∈D}
Q(A2) = {qinc,i,lk
j
| inc(cti, l, lk) ∈D, 9 ≤j ≤11}
 {einc,i,lk
3
| inc(cti, l, lk) ∈D}
 {qdec,i,lk
j
| dec(cti, l, lk, lm) ∈D, 8 ≤j ≤10}
 {edec,i,lk
1
| dec(cti, l, lk, lm) ∈D}
– X = {x0, x1, x2} and C′ = {c}.
– I(Ai)(q ) = x0 ∈[0, 0] and I(Ai)(e ) = ⊤where 0 ≤i ≤2 and
denotes any
valid symbol. Here q denotes the q-location, which is labeled with q, and e
denotes the e-location, which is labeled with e.
– Δ is shown implicitly in the following simulations due to limited space.
• increment counter simulate inc(l, cti, lk). Initially ν(xi) = d with 0 <
d ≤1. In qlk, xi will be halved. The value of xi is stored to the global clock
c and context is changed to A1. Then the value of c is halved. Although the
timed elapsed in state einc,i,lk
2
and einc,i,lk
3
are nondeterministic, to reach
the location qlk, the value of x1 and c must coincide (i.e., they reach 1
together) at state einc,i,lk
4
. The readers can check that timed elapsed in
einc,i,lk
1
must be 1 −d, in einc,i,lk
2
and einc,i,lk
4
must be d/2, and in einc,i,lk
3
must be 1 −d/2
ql
c←xi
−−−→qinc,i,lk
1
fpush
−−−−→einc,i,lk
1
x0←0
−−−−→qinc,i,lk
2
c∈[1,1]?
−−−−−→qinc,i,lk
3
c←0
−−−→
einc,i,lk
2
x0←0
−−−−→qinc,i,lk
4
fpush
−−−−→einc,i,lk
3
x0←0
−−−−→qinc,i,lk
9
c∈[1,1]?
−−−−−→qinc,i,lk
10
c←x1
−−−→qinc,i,lk
11
pop
−−→qinc,i,lk
4
x2←0
−−−−→einc,i,lk
4
x0←0
−−−−→qinc,i,lk
5
c∈[1,1]?
−−−−−→qinc,i,lk
6
x1∈[1,1]?
−−−−−−→qinc,i,lk
7
c←x2
−−−→qinc,i,lk
8
pop
−−→qinc,i,lk
1
xi←c
−−−→qlk
• test-and-decrement counter simulate dec(l, cti, lk, lm). Initially ν(xi) =
d with 0 < d ≤1. At the beginning of the simulation, xi = 1 is tested,
which encodes the zero test of cti. In qlk, xi will be doubled. The readers
can also check that to reach the location qlk, timed elapsed in edec,i,lk
1
must
be 1 −d, and in edec,i,lk
2
must be d.
ql
xi∈[1,1]?
−−−−−−→qlm and
ql
xi∈(0,1)?
−−−−−−→qdec,i,lk
1
c←xi
−−−→qdec,i,lk
2
fpush
−−−−→qdec,i,lk
3
x1←c
−−−→qdec,i,lk
4
fpush
−−−−→
edec,i,lk
1
x0←0
−−−−→qdec,i,lk
8
c∈[1,1]?
−−−−−→qdec,i,lk
9
c←x1
−−−→qdec,i,lk
10
pop
−−→qdec,i,lk
4
ϵ−→
edec,i,lk
2
x0←0
−−−−→
qdec,i,lk
5
c∈[1,1]?
−−−−−→
qdec,i,lk
6
c←x1
−−−→
qdec,i,lk
7
pop
−−→
qdec,i,lk
2
xi←c
−−−→qlk

84
Y. Wang et al.
Theorem 1. The reachability of a NeTA-I with a single global clock is undecidable.
Remark 2. The invariants here are used to prevent time progress, and it can not
be simulated by the traditional approach if pop rules are allowed, i.e., simply
resetting x0 to 0 ﬁrst, and then using a test transition x0 ∈[0, 0]? at the tail.
For example, in the pop rule qinc,i,lk
11
pop
−−→qinc,i,lk
4
, the state qinc,i,lk
11
is the ﬁnal
state of A2, and there is no way using only test transition x0 ∈[0, 0]? to promise
time not elapsing in qinc,i,lk
11
. Because after popping, we can not check values of
the local clocks in the original TA A2, which has been already popped from the
stack. Of course, if we introduce a fresh global clock, say c0, the test transition
c0 ∈[0, 0]? can prevent time progress. Then it is actually an encoding from a
Minsky machine to a NeTA with two global clocks and without invariants, which
is consistent with results in [5].
5
Constraint Dense Timed Pushdown Automata
In this section, we ﬁrst present syntax and semantics of Constraint Dense Timed
Pushdown Automata. Later, we introduce digiwords and operations which are
used for encoding from a Constraint Dense Timed Pushdown Automaton to a
snapshot pushdown system. Finally, the decidability of reachability of a snapshot
pushdown system is shown by observing that it is a growing WSPDS with a well-
formed constraint [6].
Deﬁnition 6 (Constraint Dense Timed Pushdown Automata). A con-
straint dense timed pushdown automaton (Constraint DTPDA) is a tuple D =
⟨S, s0, Γ, X, I, Δ⟩∈D, where
– S is a ﬁnite set of states with the initial state s0 ∈S,
– Γ is a ﬁnite stack alphabet,
– X is a ﬁnite set of clocks (with |X| = k),
– I : S →Φ(X) is a function that assigns to each state an invariant, and
– Δ ⊆S × Action+ × S is a ﬁnite set of transitions.
A (discrete) transition δ ∈Δ is a sequence of actions (s1, o1, s2), · · · , (si, oi, si+1)
written as s1
o1;··· ;oi
−−−−−→si+1, in which oj (for 1 ≤j ≤i) is one of the followings,
– Local ϵ, an empty operation,
– Test φ, where φ ∈Φ(X) is a clock constraint,
– Reset x ←0 where x ∈X,
– Value passing x ←x′ where x, x′ ∈X,
– Push push(γ), where γ ∈Γ is a stack symbol,
– F-Push fpush(γ), where γ ∈Γ is a stack symbol, and
– Pop pop(γ), where γ ∈Γ is a stack symbol.
Deﬁnition 7 (Semantics of Constraint DTPDAs).
For a Constraint
DTPDA ⟨S, s0, Γ, X, I, Δ⟩, a conﬁguration is a triplet (s, w, ν) with a state
s ∈S, a stack w ∈(Γ × (R≥0)k × {0, 1} × Φ(X))∗, and a clock valua-
tion ν on X. Similarly, a stack w good, written as w⇑, if for each con-
tent (γi, ¯ti, flagi, φi) in w, we have ν[x1 ←t1, · · · , xk ←tk] |= φi where

Nested Timed Automata with Invariants
85
¯ti = (t1, · · · , tk). For w = (γ1, ¯t1, flag1, φ1). · · · .(γn, ¯tn, flagn, φn), a t-time
passage on the stack, written as w + t, is (γ1, progress′(¯t1, t, flag1), flag1, φ1).
· · · .(γn, progress′(¯tn, t, flagn), flagn, φn) where
progress′(¯t, t, flag) =

(t1 + t, · · · , tk + t) if flag = 1 and ¯t = (t1, · · · , tk)
¯t
if flag = 0
The transition relation of the Constraint DTPDA is deﬁned as follows:
– Progress transition: (s, w, ν)
t−→D (s, w + t, ν + t), where t ∈R≥0, w⇑, ν |=
I(s), (w + t)⇑and (ν + t) |= I(s).
– Discrete transition: (s1, w1, ν1)
o−→D (s2, w2, ν2), if s1
o−→s2, w⇑
1 ,ν1 |= I(s1),
w⇑
2 ,ν2 |= I(s2) and one of the following holds,
• Local o = ϵ, then w1 = w2, and ν1 = ν2.
• Test o = φ, then w1 = w2, ν1 = ν2 and ν1 |= φ.
• Reset o = x ←0, then w1 = w2, ν2 = ν1[x ←0].
• Value passing o = x ←x′, then w1 = w2, ν2 = ν1[x ←ν1(x′)].
• Push o = push(γ), then ν2 = ν0, w2 = (γ, (ν1(x1), · · · , ν1(xk)), 1,
I(s1)).w1 for X = {x1, · · · , xk}.
• F-Push
o = fpush(γ), then ν2 = ν0, w2 = (γ, (ν1(x1), · · · , ν1(xk)), 0, I(s1)).w1
for X = {x1, · · · , xk}.
• Pop o
=
pop(γ), then ν2
=
ν1[x1
←
t1, · · · , xk
←
tk], w1
=
(γ, (t1, · · · , tk), flag, φ).w2.
The initial conﬁguration κ0 = (s0, ϵ, ν0). We use −→D to range over these
transitions, and −→∗
D is the transitive closure of −→D.
Intuitively, in a stack w = (γ1, ¯t1, flag1, φ1). · · · .(γn, ¯tn, flagn, φn), γi is a
stack symbol, ¯ti is k-tuple of clocks values of x1, · · · , xk respectively, flagi = 1 if
the stack frame is pushed and flagi = 0 if fpushed and φi is a clock constraint.
Example 1. Figure 1 shows transitions between conﬁgurations of a Constraint
DTPDA with S = {s1, s2, s3, · · · }, X = {x1, x2}, Γ = {a, b, d} and I = {I(s1) =
x1 ∈[0, 1) ∧x2 ∈[3, 4), I(s2) = x1 ∈[0, 3), I(s3) = ⊤, · · · }. Values changed from
the last conﬁguration are in bold. For simplicity, we omit some transitions and
start from s1. From s1 to s2, a discrete transition fpush(d) pushes d to the stack
with the values of x1 and x2, frozen. After pushing, value of x1 and x2 will be
reset to zero. Then, at state s2, a progress transition elapses 2.6 time units, and
each value grows older for 2.6 except for frozen clocks in the top. From s2 to s3,
the batched transition ﬁrst pops symbol d from the stack and clock values are
recovered from the poped clocks. Then, the value of x1 is reset to 0. Note that
the invariants are always satisﬁed in these reachable conﬁgurations.
In the following subsections, we denote the set of ﬁnite multisets over D by
MP(D), and the union of two multisets M, M ′ by M ⊎M ′. We regard a ﬁnite set
as a multiset with the multiplicity 1, and a ﬁnite word as a multiset by ignoring
the ordering. Let frac(t) = t −floor(t) for t ∈R≥0.

86
Y. Wang et al.
w
ν
I
(a, (1.9, 4.5), 1, x1 ∈[1, 6))
(b, (6.7, 2.9), 0, ⊤)
(a, (3.1, 5.2), 1, x2 ∈[5, ω))
(d, (4.2, 3.3), 1, ⊤)
x1 ←0.5
x2 ←3.9
x1 ∈[0, 1) ∧x2 ∈[3, 4)
(d, (0.5, 3.9), 0, x1 ∈[0, 1) ∧x2 ∈[3, 4))
(a, (1.9, 4.5), 1, x1 ∈[1, 6))
(b, (6.7, 2.9), 0, ⊤)
(a, (3.1, 5.2), 1, x2 ∈[5, ω))
(d, (4.2, 3.3), 1, ⊤)
x1 ←0
x2 ←0
x1 ∈[0, 3)
(s1, w1, ν1)
fpush(d)
−−−−−−−−−−−−−−−−−→D (s2, w2, ν2)
2.6
−−−−−−−−−−−→D
(d, (0.5, 3.9), 0, x1 ∈[0, 1) ∧x2 ∈[3, 4))
(a, (4.5, 7.1), 1, x1 ∈[1, 6))
(b, (6.7, 2.9), 0, ⊤)
(a, (5.7, 7.8), 1, x2 ∈[5, ω))
(d, (6.8, 5.9, 1, ⊤)
x1 ←2.6
x2 ←2.6
x1 ∈[0, 3)
(a, (4.5, 7.1), 1, x1 ∈[1, 6))
(b, (6.7, 2.9), 0, ⊤)
(a, (5.7, 7.8), 1, x2 ∈[5, ω))
(d, (6.8, 5.9), 1, ⊤)
x1 ←0
x2 ←3.9
⊤
−−−→D (s2, w3, ν3)
pop(d);x1←0
−−−−−−−−−−−−−−−→D (s3, w4, ν4)
Fig. 1. An example of constraint DTPDAs
5.1
Digiword and Its Operations
Let ⟨S, s0, Γ, X, I, Δ⟩be a Constraint DTPDA, and let n be the largest integer
(except for ω) appearing in I and Δ.
Deﬁnition 8 (Two Subsets of Intervals). Let
Intv(n) = {r2i = [i, i] | 0 ≤i ≤n} ∪{r2i+1 = (i, i + 1) | 0 ≤i < n} ∪{r2n+1 = (n, ω)}
Let I(n) denote a subset of intervals I such that all integers appearing in I(n)
are less than or equal to n. For v ∈R≥0, proj(v) = ri if v ∈ri ∈Intv(n).
Example 2. In Example 1, n = 6 and we have 13 intervals in Intv(6),
0 r1 1 r3
2 r5 3 r7 4 r9 5 r11 6
r13
r0
r2
r4
r6
r8
r10
r12
I(6) contains intervals (a, b′), [a, b], [a, b′) and (a, b] where a, b ∈{0, 1, . . . , 6}
and b′ ∈{0, 1, . . . , 6, ω}.
Intv(n) intend to contain digitizations of clocks, e.g., if a clock has value 1.9,
then we say it is in r3. I(n) intend to contain intervals in invariants, e.g., an
invariant x ∈[1, 2] ∧y ∈(3, 4) can be split into two intervals [1, 2] and (3, 4).
Both Intv(n) and I(n) are ﬁnite sets.

Nested Timed Automata with Invariants
87
Deﬁnition 9 (Digitization).
A digitization digi : MP((X ∪Γ) × R≥0 ×
{0, 1} × I(n)) →MP((X ∪Γ) × Intv(n) × {0, 1} × I(n))∗is deﬁned as follows.
For ¯Y ∈MP((X ∪Γ)×R≥0 ×{0, 1}×I(n)), digi( ¯Y) is a word Y0Y1 · · · Ym,
where Y0, Y1, · · · , Ym are multisets that collect (x, proj(t), flag, I)’s having the
same frac(t) for (x, t, flag, I) ∈¯Y. Among them, Y0 (which is possibly empty)
is reserved for the collection of (x, proj(t), flag, I) with frac(t) = 0 and t ≤
n (i.e., proj(t) = r2i for 0 ≤i ≤n). We assume that Yi except for Y0 is
non-empty (i.e., Yi = ∅with i > 0 is omitted), and Yi’s are sorted by the
increasing order of frac(t) (i.e., frac(t) < frac(t′) for (x, proj(t), flag, I) ∈Yi
and (x′, proj(t′), flag′, I′) ∈Yi+1).
For Yi ∈MP((X ∪Γ) × Intv(n) × {0, 1} × I(n)), we deﬁne the projections
by prc(Yi) = {(x, proj(t), 1, I) ∈Yi} and frz(Yi) = {(x, proj(t), 0, I) ∈Yi}.
We overload the projections on ¯Y = Y0Y1 · · · Ym ∈(MP((X ∪Γ) × Intv(n) ×
{0, 1} × I(n)))∗such that frz( ¯Y ) = frz(Y0)frz(Y1) · · · frz(Ym) and prc( ¯Y ) =
prc(Y0)prc(Y1) · · · prc(Ym).
For a stack frame v = (γ, (t1, · · · , tk), flag, φ) of a Constraint DTPDA,
we denote a word (γ, t1, flag, EC(φ, x1)) · · · (γ, tk, flag, EC(φ, xk)) by dist(v).
Given a state s and a clock valuation ν, we deﬁne a word time(s, ν) =
(x1, ν(x1), 1, EC(I(s), x1)) . . . (xk, ν(xk), 1, EC(I(s), xk)) where x1 . . . xk ∈X.
Example 3. For the conﬁguration ϱ1 = (s1, v4 · · · v1, ν1) in Example 1, let
¯Y = dist(v4) ⊎. . . ⊎dist(v1) ⊎time(s1, ν1), and ¯Y = digi( ¯Y), i.e.,
¯Y = {(a, 1.9, 1, [1, 6)), (a, 4.5, 1, [0, ω)), (b, 6.7, 0, [0, ω)), (b, 2.9, 0, [0, ω)),
(a, 3.1, 1, [0, ω)), (a, 5.2, 1, [5, ω)), (d, 4.2, 1, [0, ω)),
(d, 3.3, 1, [0, ω)), (x1, 0.5, 1, [0, 1)), (x2, 3.9, 1, [3, 4))}
¯Y = {(a, r7, 1, [0, ω))}{(a, r11, 1, [5, ω)), (d, r9, 1, [0, ω))}{(d, r7, 1, [0, ω))}
{(x1, r1, 1, [0, 1)), (a, r9, 1, [0, ω))}{(b, r13, 0, [0, ω))}{(x2, r7, 1, [3, 4)),
(a, r3, 1, [1, 6)), (b, r5, 0, [0, ω))}
prc( ¯Y ) = {(a, r7, 1, [0, ω))}{(a, r11, 1, [5, ω)), (d, r9, 1, [0, ω))}{(d, r7, 1, [0, ω))}
{(x1, r1, 1, [0, 1)), (a, r9, 1, [0, ω))}{(x2, r7, 1, [3, 4)), (a, r3, 1, [1, 6))}
frz( ¯Y ) = {(b, r13, 0, [0, ω))}(b, r5, 0, [0, ω))}
Deﬁnition 10 (Digiwords and k-pointers). A word ¯Y ∈(MP((X ∪Γ) ×
Intv(n) × {0, 1} × I(n)))∗is called a digiword. We say a digiword ¯Y is good,
written as ¯Y ⇑, if for all (x, ri, flag, I) in ¯Y , ri ⊆I. We denote ¯Y |Λ for Λ ⊆
Γ ∪X, by removing (x, ri, flag, I) with x ̸∈Λ. A k-pointer ¯ρ of ¯Y is a tuple of k
pointers to mutually diﬀerent k elements in ¯Y |Γ . We refer to the element pointed
by the i-th pointer by ¯ρ[i]. From now on, we assume that a digiword has two pairs
of k-pointers (¯ρ1, ¯ρ2) and (¯τ1, ¯τ2) that point to only proceeding and frozen clocks,
respectively. We call (¯ρ1, ¯ρ2) proceeding k-pointers and (¯τ1, ¯τ2) frozen k-pointers.
We also assume that they do not overlap each other, i.e., there are no i, j, such
that ¯ρ1[i] = ¯ρ2[j] or ¯τ1[i] = ¯τ2[j].
¯ρ1 and ¯ρ2 intend the store of values of the proceeding clocks at the last and
one before the last Push, respectively. ¯τ1 and ¯τ2 intend similar for frozen clocks
at F-Push.

88
Y. Wang et al.
Deﬁnition 11 (Embedding over Digiwords). For digiwords ¯Y = Y1 · · · Ym
and ¯Z = Z1 · · · Zm′ with pairs of k-pointers (¯ρ1, ¯ρ2), (¯τ1, ¯τ2), and (¯ρ′
1, ¯ρ′
2), (¯τ ′
1, ¯τ ′
2),
respectively, we deﬁne an embedding ¯Y ⊑¯Z, if there exists a monotonic injection
f : [1..m] →[1..m′] such that Yi ⊆Zf(i) for each i ∈[1..m], f ◦¯ρi = ¯ρ′
i and
f ◦¯τi = ¯τ ′
i for i = 1, 2.
The embedding ⊑is a well-quasi-ordering which will be exploited in Sect. 5.3.
Deﬁnition 12 (Operations on Digiwords).
Let ¯Y
= Y0 · · · Ym, ¯Y ′ =
Y ′
0 · · · Y ′
m′ ∈(MP((X ∪Γ) × Intv(n) × {0, 1} × I(n)))∗such that ¯Y (resp.
¯Y ′) has two pairs of proceeding and frozen k-pointers (¯ρ1, ¯ρ2) and (¯τ1, ¯τ2) (resp.
(¯ρ′
1, ¯ρ′
2) and (¯τ ′
1, ¯τ ′
2)). We deﬁne digiword operations as follows.
– Decomposition: Let Z ∈MP((X ∪Γ)×Intv(n)×{0, 1}×I(n)). If Z ⊆Yj,
decomp( ¯Y , Z) = (Y0 · · · Yj−1, Yj, Yj+1 · · · Ym).
– Refresh refresh( ¯Y , s) for s ∈S is obtained by updating all elements
(x, ri, 1, I) with (x, ri, 1, EC(I(s), x)) for x ∈X.
– Init init( ¯Y ) is obtained by removing all elements (x, r, 1, I) from ¯Y and
inserting (x, r0, 1, [0, w]) to Y0 for all x ∈X.
– Insertx insertx( ¯Y , x, y) adds (x, ri, 1, I) to Yj for (y, ri, 1, I) ∈Yj, x, y ∈X.
– InsertI: Let Z ∈MP((X∪Γ)×Intv(n)×{0, 1}×I(n)) with (x, ri, flag, I) ∈
Z for x ∈X ∪Γ. insertI( ¯Y , Z) inserts Z to ¯Y such that
⎧
⎨
⎩
either take the union of Z and Yj for j > 0, or put Z at any place after Y0
if i is odd
take the union of Z and Y0
if i is even
– Delete. delete( ¯Y , x) for x ⊆X is obtained from ¯Y by deleting the element
(x, r, 1, I) indexed by x.
– Permutation. Let
¯V
=
prc( ¯Y )
=
V0V1 · · · Vk and
¯U
=
frz( ¯Y )
=
U0U1 · · · Uk′. A one-step permutation ¯Y ⇒¯Y ′ is given by ⇒
=
⇒s ∪⇒c,
deﬁned below. We denote inc(Vj) for Vj in which each ri is updated to ri+1
for i < 2n + 1.
(⇒s) Let

decomp(U0 . inc(V0) . tl( ¯Y ), Vk) = ( ¯Y k
⊣, ˆY k, ¯Y k
⊢)
decomp(insertI(( ˆY k \ Vk) . ¯Y k
⊢, Vk), Vk) = ( ¯Zk
⊣, ˆZk, ¯Zk
⊢).
For j with 0 ≤j < k, we repeat to set

decomp( ¯Y j+1
⊣
. ¯Zj+1
⊣
, Vj) = ( ¯Y j
⊣, ˆY j, ¯Y j
⊢)
decomp(insertI(( ˆY j \ Vj) . ¯Y j
⊢, Vj), Vj) = ( ¯Zj
⊣, ˆZj, ¯Zj
⊢).
Then, ¯Y ⇒s ¯Y ′ = ¯Y 0
⊣¯Z0
⊣ˆZ0 ¯Z1
⊣ˆZ1 · · · ¯Zk
⊣ˆZk ¯Zk
⊢.
(⇒c) Let ¯Y k
⊣= U0 ∪inc(Vk) and ¯Zk
⊣= inc(V0) Y1 · · · (Yi′ \ Vk) · · · Ym.
For j with 0 ≤j < k, we repeat to set

decomp( ¯Y j+1
⊣
. ¯Zj+1
⊣
, Vj) = ( ¯Y j
⊣, ˆY j, ¯Y j
⊢)
decomp(insertI(( ˆY j \ Vj). ¯Y j
⊢, Vj), Vj) = ( ¯Zj
⊣, ˆZj, ¯Zj
⊢).
Then, ¯Y ⇒c ¯Y ′ = ¯Y 0
⊣¯Z0
⊣ˆZ0 ¯Z1
⊣ˆZ1 · · · ¯Zk−1
⊣
ˆZk−1 ¯Zk−1
⊢
.

Nested Timed Automata with Invariants
89
(¯ρ1, ¯ρ2) is updated to correspond to the permutation accordingly, and (¯τ1, ¯τ2)
is kept unchanged.
– Rotate: For proceeding k-pointers (¯ρ1, ¯ρ2) of ¯Y and ¯ρ of ¯Z, let ¯Y |Γ ⇒∗¯Z|Γ
such that the permutation makes ¯ρ1 match with ¯ρ. Then, rotate¯ρ1→¯ρ(¯ρ2) is
the corresponding k-pointer of ¯Z to ¯ρ2.
– Mapflag
→
mapfl
→( ¯Y , γ) for γ ∈Γ is obtained from ¯Y by, for each xi ∈X,
replacing (xi, rj, 1, I) with (γ, rj, fl, I). Accordingly, if fl = 1, ¯ρ1[i] is updated
to point to (γ, rj, 1, I), and ¯ρ2 is set to the original ¯ρ1. If fl = 0, ¯τ1[i] is
updated to point to (γ, rj, 0, I), and ¯τ2 is set to the original ¯τ1.
– Mapflag
←
mapfl
←( ¯Y , ¯Y ′, γ) for γ ∈Γ is obtained,
(if fl = 1) by replacing each ¯ρ1[i] = (γ, rj, 1, I) in ¯Y |Γ with (xi, rj, 1, I) for
xi ∈X. Accordingly, new ¯ρ1 is set to the original ¯ρ2, and new ¯ρ2 is set
to rotate¯ρ′
1→¯ρ2(¯ρ′
2). ¯τ1 and ¯τ2 are kept unchanged.
(if fl = 0) by replacing each ¯τ1[i] = (γ, rj, 0, I) in ¯Y |Γ with (xi, rj, 1, I) for
xi ∈X. Accordingly, new ¯τ1 is set to the original ¯τ2, and new ¯τ2 is set to
¯τ ′
2. ¯ρ1 and ¯ρ2 are kept unchanged.
We will use these operations on digiwords for encoding in the next subsection.
5.2
Snapshot Pushdown System
In this subsection, we show that a Constraint DTPDA is encoded into its dig-
itization, called a snapshot pushdown system (snapshot PDS), which keeps the
digitization of all clocks in the top stack frame, as a digiword. The keys of the
encoding are, (1) when a pop occurs, the time progress recorded at the top stack
symbol is propagated to the next stack symbol after ﬁnding a permutation by
matching between proceeding k-pointers ¯ρ2 and ¯ρ′
1, and (2) only invariants in
the top stack frame need to be checked. Before showing the encoding, we ﬁrst
deﬁne the encoded conﬁguration, called snapshot conﬁguration.
Deﬁnition 13 (Snapshot Conﬁguration).
Let π : ϱ0 = (s0, ϵ, ν0) −→∗
D
ϱ = (s, w, ν) be a transition sequence of a Constraint DTPDA from the initial
conﬁguration. If π is not empty, we refer the last step as λ : ϱ′ −→D ϱ, and the
preceding sequence by π′ : ϱ0 −→∗
D ϱ′. Let w = vm · · · v1. A snapshot is snap(π) =
( ¯Y , flag(vm)), where ¯Y = digi(⊎idist(vi) ⊎time(s, ν)). Let a k-pointer ¯ξ(π) be
¯ξ(π)[i] = (γ, proj(ti), flag(vm), I) for (γ, ti, flag(vm), I) ∈dist(vm). A snapshot
conﬁguration Snap(π) is inductively deﬁned from Snap(π′).
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(s0, snap(ϵ))
if π = ϵ.(¯ρ1, ¯ρ2) and (¯τ1, ¯τ2) are undeﬁned.
(s′, snap(π) tail(Snap(π′)))
if λ is Timeprogress with ¯Y ′ ⇒∗¯Y .
Then, thepermutation ¯Y ′ ⇒∗¯Y updates (¯ρ′
1, ¯ρ′
2) to (¯ρ1, ¯ρ2).
(s′, snap(π) tail(Snap(π′)))
if λ is Local, Test, Reset, Value −passing.
(s, snap(π) Snap(π′))
if λ is Push. Then, (¯ρ1, ¯ρ2) = (¯ξ(π), ¯ρ′
1).
(s, snap(π) Snap(π′))
if λ is F −Push.Then, (¯τ1, ¯τ2) = (¯ξ(π), ¯τ′
1).
(s, snap(π) tail(tail(Snap(π′))))
if λ is Pop.
If flag = 1, (¯ρ1, ¯ρ2) = (¯ρ′
2, rotate¯ρ′′
1 →¯ρ′
2(¯ρ′′
2)); otherwise, (¯τ1, ¯τ2) = (¯τ′
2, ¯τ′′
2 ).
We refer head(Snap(π′)) by ¯Y ′, head(tail(Snap(π′)) by ¯Y ′′. Pairs of pointers
of ¯Y , ¯Y ′, and ¯Y ′′ are denoted by (¯ρ1, ¯ρ2), (¯ρ′
1, ¯ρ′
2), and (¯ρ′′
1, ¯ρ′′
2), respectively. If
not mentioned, pointers are kept as is.

90
Y. Wang et al.
Deﬁnition 14 (Snapshot PDS). For a Constraint DTPDA ⟨S, s0, Γ, X, I, ∇⟩,
a snapshot PDS S is a PDS (with possibly inﬁnite stack alphabet)
⟨S ∪{serr}, s0, (MP((X ∪Γ) × Intv(n) × {0, 1} × I(n)))∗× {0, 1}, Δd⟩.
with the initial conﬁguration ⟨s0, ({(x, r0, 1, EC(I(s0), x)) | x ∈X}, 1)⟩. For
simplicity, we deﬁne s′′ =

s′
if
¯Y ′⇑,
serr
otherwise where serr is a special error
state that is used to indicate invariants are violated. Then Δd consists of:
Progress
⟨s, ( ¯Y , flag)⟩→S ⟨s′′, ( ¯Y ′, flag)⟩for ¯Y ⇒∗¯Y ′, where s′ = s.
Local (s
ϵ−→s′
∈Δ)
⟨s, ( ¯Y , flag)⟩→S
⟨s′′, ( ¯Y ′, flag)⟩, where ¯Y ′
=
refresh( ¯Y , s′).
Test (s
φ−→
s′
∈
Δ)
⟨s, ( ¯Y , flag)⟩
→S
⟨s′′, ( ¯Y ′, flag)⟩, where ¯Y ′
=
refresh( ¯Y , s′), if for every (x, ri, flag, I) ∈¯Y with x ∈X, ri ⊆EC(φ, x)
holds,
Reset (s
x←0
−−−→s′ ∈Δ with λ ⊆X)
⟨s, ( ¯Y , flag)⟩→S ⟨s′′, ( ¯Y ′, flag)⟩,
where ¯Y ′ = refresh(insertI(delete( ¯Y , x), (x, r0, 1, [0, w))), s′).
Value-passing (s
x←y
−−−→
s′
∈
Δ with x, y
∈
X)
⟨s, ( ¯Y , flag)⟩
→S
⟨s′′, ( ¯Y ′, flag)⟩,
where ¯Y ′ = refresh(insertx(delete( ¯Y , x), x, y), s′)⟩.
Push (s
push(γ)
−−−−−→s′ ∈Δ; fl = 1) and F-Push (s
fpush(γ)
−−−−−−→s′ ∈Δ; fl = 0)
⟨s, ( ¯Y , flag)⟩→S ⟨s′′, ( ¯Y ′, fl)( ¯Y , flag)⟩,
where ¯Y ′ = refresh(init(mapfl
→( ¯Y , γ), s′).
Pop (s
pop(γ)
−−−−→s′ ∈Δ)
⟨s, ( ¯Y , flag)( ¯Y ′′, flag′)⟩→S ⟨s′′, ( ¯Y ′, flag′)⟩,
where ¯Y ′ = refresh(mapflag
←
( ¯Y , ¯Y ′′, γ), s′).
By induction on the number of steps of transitions, the encoding relation
between a Constraint DTPDA and a snapshot PDS is observed.
Lemma 1. Let us denote ϱ0 and ϱ (resp. ⟨s0, ˜w0⟩and ⟨s, ˜w⟩) for the initial
conﬁguration and a conﬁguration of a Constraint DTPDA (resp. its encoded
snapshot PDS S).
(Preservation) If π : ϱ0 −→∗
D ϱ, there exists ⟨s, ˜w⟩such that ⟨s0, ˜w0⟩→∗
S
⟨s, ˜w⟩and Snap(π) = ⟨s, ˜w⟩.
(Reﬂection) If ⟨s0, ˜w0⟩→∗
S ⟨s, ˜w⟩,
s = serr is an error state, or
s ̸= serr and there exists π : ϱ0 −→∗
D ϱ with Snap(π) = ⟨s, ˜w⟩.
5.3
Well-Formed Constraint
A snapshot PDS is a growing WSPDS (Deﬁnition 6 in [6]) and ⇓Υ gives a well-
formed constraint (Deﬁnition 8 in [6]). Let us recall the deﬁnitions.
Let P be a set of control locations and let Γ be a stack alphabet. Diﬀerent
from an ordinary deﬁnition of PDSs, we do not assume that P and Γ are ﬁnite,
but associated with well-quasi-orderings (WQOs) ⪯and ≤, respectively. Note
that the embedding ⊑over digiwords is a WQO by Higman’s lemma.

Nested Timed Automata with Invariants
91
For w = α1α2 · · · αn, v = β1β2 · · · βm ∈Γ ∗, let w ≪v if m = n and ∀i ∈
[1..n].αi ≤βi. We extend ≪on conﬁgurations such that (p, w) ≪(q, v) if p ⪯q
and w ≪v for p, q ∈P and w, v ∈Γ ∗. A partial function ψ ∈PFun(X, Y ) is
monotonic if γ ≤γ′ with γ ∈dom(ψ) implies ψ(γ) ≪ψ(γ′) and γ′ ∈dom(ψ).
A a well-structured PDS (WSPDS) is a triplet ⟨(P, ⪯), (Γ, ≤), Δ⟩of a set
(P, ⪯) of WQO states, a WQO stack alphabet (Γ, ≤), and a ﬁnite set Δ ⊆
PFun(P × Γ, P × Γ ≤2) of monotonic partial functions. A WSPDS is growing if,
for each ψ(p, γ) = (q, w) with ψ ∈Δ and (q′, w′) ≫(q, w), there exists (p′, γ′)
with (p′, γ′) ≫(p, γ) such that ψ(p′, γ′) ≫(q′, w′).
A well-formed constraint describes a syntactical feature that is preserved
under transitions. Theorem 5 in [6] ensures the reachability of a growing WSPDS
when it has a well-formed constraint.
Deﬁnition 15 (Well-formed constraint).
Let a conﬁguration (s, ˜w) of a
snapshot PDS S. An element in a stack frame of ˜w has a parent if it has a
corresponding element in the next stack frame. The transitive closure of the
parent relation is an ancestor. An element in ˜w is marked, if its ancestor is
pointed by a pointer in some stack frame. We deﬁne a projection ⇓Υ ( ˜w) by
removing unmarked elements in ˜w. We say that ˜w is well-formed if ⇓Υ ( ˜w) = ˜w.
The idea of ⇓Υ is to remove unnecessary elements (i.e., elements not related
to previous actions) from the stack content. Note that a conﬁguration reachable
from the initial conﬁguration by →∗
S is always well-formed. Since a snapshot
PDS is a growing WSPDS with ⇓Υ , we conclude Theorem 2 from Lemma 1.
Theorem 2. The reachability of a Constraint DTPDA is decidable.
6
Decidability Results of NeTA-Is
In this section, we encode NeTA-I with no global clocks to constraint DTPDAs
and thus show the decidability of the former model.
Given a NeTA-I N = (T, A0, X, C, I, Δ) with no global clocks (C = ∅), we
deﬁne the target Constraint DTPDA E(N) = ⟨S, s0, Γ, X, I′, ∇⟩such that
– S = Γ = 
Ai∈T Q(Ai) is the set of all control locations of TAs in T.
– s0 = q0(A0) is the initial control location of the initial TA A0.
– X = {x1, ..., xk}is the set of k local clocks.
– I′ : S →Φ(X) is a function such that I′(s) = I(Ai)(s) where s ∈Q(Ai).
– ∇is the union 
Ai∈T Δ(Ai)  H(N) where

Δ(Ai) = {Local, Test, Reset, Value-passing},
H(N) consists of rules below.
Push
q
push(q)
−−−−−→q0(Ai′)
if (q, ε, push, q0(Ai′), q) ∈Δ(N)
F −Push q
fpush(q)
−−−−−−→q0(Ai′)
if (q, ε, f-push, q0(Ai′), q) ∈Δ(N)
Pop
q
pop(q′)
−−−−→q′
if (q, q′, pop, q′, ε)) ∈Δ(N)

92
Y. Wang et al.
Deﬁnition 16. Let N be a NeTA-I (T, A0, X, C, I, Δ) with no global clocks and
let E(N) be the encoded constraint DTPDA ⟨S, s0, Γ, X, I′, ∇⟩. For a conﬁgu-
ration κ = (⟨q, ν, μ⟩, v) of N such that v = (q1, flag1, ν1) . . . (qn, flagn, νn),
κ denotes a conﬁguration (q, w(κ), ν) of E(N) where w(κ) = w1 · · · wn with
wi = (qi, νi, flagi, I(qi)).
We can prove that transitions are preserved and reﬂected by the encoding.
Lemma 2. For a NeTA-I N with no global clocks, its encoded Constraint
DTPDA E(N), and conﬁgurations κ, κ′ of N,
(Preservation) if κ −→κ′, then κ −→∗
D κ′, and
(Reﬂection) if κ −→∗
D ϱ, there exists κ′ with ϱ −→∗
D κ′ and κ −→∗κ′.
Theorem 3. The reachability of a NeTA-I with no global clocks is decidable.
7
Conclusion
This paper proposes a model NeTA-Is by extending NeTAs with invariants
assigned to each control location. We have shown that the reachability problem
of a NeTA-I with a single global clock is undecidable, while that of a NeTA-I
without global clocks is decidable. Compared to the diﬀerent result of NeTA [5],
it is revealed that unlike that of timed automata, invariants aﬀect the expressive-
ness of timed recursive systems. Hence, when adopting timed recursive systems
to model and verify complex real-time systems, one should carefully consider the
introduction of invariants.
Acknowledgements. This work is supported by National Natural Science Founda-
tion of China with grant Nos. 61472240, 61672340, 61472238, and the NSFC-JSPS
bilateral joint research project with grant No. 61511140100.
References
1. Alur, R., Dill, D.L.: A theory of timed automata. Theoret. Comput. Sci. 126,
183–235 (1994)
2. Henzinger, T.A., Nicollin, X., Sifakis, J., Yovine, S.: Symbolic model checking for
real-time systems. Inf. Comput. 111, 193–244 (1994)
3. Bengtsson, J., Yi, W.: Timed automata: semantics, algorithms and tools. In: Desel,
J., Reisig, W., Rozenberg, G. (eds.) ACPN 2003. LNCS, vol. 3098, pp. 87–124.
Springer, Heidelberg (2004). doi:10.1007/978-3-540-27755-2 3
4. Li, G., Cai, X., Ogawa, M., Yuen, S.: Nested timed automata. In: Braberman,
V., Fribourg, L. (eds.) FORMATS 2013. LNCS, vol. 8053, pp. 168–182. Springer,
Heidelberg (2013). doi:10.1007/978-3-642-40229-6 12
5. Li, G., Ogawa, M., Yuen, S.: Nested timed automata with frozen clocks. In:
Sankaranarayanan, S., Vicario, E. (eds.) FORMATS 2015. LNCS, vol. 9268, pp.
189–205. Springer, Cham (2015). doi:10.1007/978-3-319-22975-1 13
6. Cai, X., Ogawa, M.: Well-structured pushdown system: case of dense timed push-
down automata. In: Codish, M., Sumii, E. (eds.) FLOPS 2014. LNCS, vol. 8475,
pp. 336–352. Springer, Cham (2014). doi:10.1007/978-3-319-07151-0 21

Nested Timed Automata with Invariants
93
7. Abdulla, P.A., Atig, M.F., Stenman, J.: Dense-timed pushdown automata. In: Pro-
ceedings of the LICS 2012, pp. 35–44. IEEE Computer Society (2012)
8. Trivedi, A., Wojtczak, D.: Recursive timed automata. In: Bouajjani, A., Chin, W.-N.
(eds.) ATVA 2010. LNCS, vol. 6252, pp. 306–324. Springer, Heidelberg (2010).
doi:10.1007/978-3-642-15643-4 23
9. Benerecetti, M., Minopoli, S., Peron, A.: Analysis of timed recursive state
machines. In: Proceedings of the TIME 2010, pp. 61–68. IEEE Computer Soci-
ety (2010)
10. Minsky, M.: Computation: Finite and Inﬁnite Machines. Prentice-Hall, Upper Sad-
dle River (1967)

Multi-core Cyclic Executives
for Safety-Critical Systems
Calvin Deutschbein1, Tom Fleming2, Alan Burns2, and Sanjoy Baruah3(B)
1 The University of North Carolina at Chapel Hill, Chapel Hill, USA
2 The University of York, York, UK
3 Washington University in St. Louis, St. Louis, USA
baruah@wustl.edu
Abstract. In a cyclic executive, a series of pre-determined frames are
executed in sequence; once the series is complete the sequence is repeated.
Within each frame individual units of computation are executed, again in
a pre-speciﬁed sequence. The implementation of cyclic executives upon
multi-core platforms is considered. A Linear Programming (LP) based
formulation is presented of the problem of constructing cyclic execu-
tives upon multiprocessors for a particular kind of recurrent real-time
workload – collections of implicit-deadline periodic tasks. Techniques are
described for solving the LP formulation under diﬀerent kinds of restric-
tions in order to obtain preemptive and non-preemptive cyclic executives.
1
Introduction and Motivation
Real-time scheduling theory has made great advances over the past several
decades. Despite these advances, interactions with industrial collaborators in
highly safety-critical application domains, particularly those that are subject to
stringent certiﬁcation requirements, reveal that the use of the very simple cyclic
executive approach [1] remains surprisingly wide-spread for scheduling safety-
critical systems. A cyclic executive (CE) is a simple deterministic scheme that
consists, for a single processor, of the repeated execution of a series of frames,
each comprising a sequence of jobs that execute in their deﬁning sequence and
must complete by the end of the frame. Although there are a number of draw-
backs to using cyclic executives (some are discussed in Sect. 2), this approach
oﬀers two signiﬁcant advantages, predictability and low run-time overhead, that
are responsible for their continued widespread use in scheduling highly safety-
critical systems.
Highly safety-critical real-time systems have traditionally been implemented
upon custom-built single-core processors that are designed to guarantee pre-
dictable timing behavior during run-time. As safety-critical software has become
more computation-intensive, however, it has proved too expensive to custom-
build hardware powerful enough to accommodate the computational require-
ments of such software; hence, there is an increasing trend towards implementing
safety-critical systems upon commercial oﬀ-the-shelf (COTS) platforms. Most
COTS processors today tend to be multi-core ones; this motivates our research
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 94–109, 2017.
https://doi.org/10.1007/978-3-319-69483-2_6

Multi-core Cyclic Executives for Safety-Critical Systems
95
described here into the construction of CEs that are suitable for execution upon
multi-core processors.
This research. We derive several approaches to constructing cyclic executives
for implicit-deadline periodic task systems upon identical multiprocessors. These
approaches share the commonality that they are all based upon formulating the
schedule construction problem as a linear program (LP). Cyclic executives in
which jobs may be preempted can be derived from solutions to such LPs; since
eﬃcient polynomial-time algorithms are known for solving LPs, this approach
enables us to design algorithms for constructing preemptive CEs that have run-
ning time polynomial in the size of the CE.
In order to construct non-preemptive CEs from a solution to the LP, the
LP must be further constrained to require that some variables may only take
on integer values: this is an integer linear program, or ILP. Solving an ILP is
known to be NP-hard [8], and hence unlikely to be solvable exactly in poly-
nomial time. However, the optimization community has recently been devoting
immense eﬀort to devise extremely eﬃcient implementations of ILP solvers, and
highly optimized libraries with such eﬃcient implementations are widely avail-
able today. Since CEs are constructed prior to run-time, we believe that it is
reasonable to attempt to solve ILPs exactly rather than only approximately, and
seek to obtain ILP formulations that we will seek to solve exactly to construct
non-preemptive multiprocessor CEs for implicit-deadline periodic task systems.
However if this is not practical for particular problem instances, we devise an
approximation algorithm with polynomial running time for constructing non-
preemptive CEs, and evaluate the performance of this approximation algorithm
vis-a-vis the exact one both via the theoretical metric of speedup factor, and via
simulation experiments on synthetically generated workloads. We additionally
show that for a particular kind of workload that is quite common in practice –
systems of harmonic tasks – even better results are obtainable.
2
Cyclic Executives
In this section we provide a brief introduction to the cyclic executive approach
to hard-real-time scheduling. This is by no means comprehensive or complete;
for a textbook description, please consult [11, Chap. 5.2–5.4].
In the cyclic executive approach, a schedule called a major schedule is deter-
mined prior to run-time, which describes the sequence of actions (i.e., computa-
tions) to be performed during some ﬁxed period of time called the major cycle.
The actions of a major schedule are executed cyclically, going back to the begin-
ning at the start of each major cycle.1 The major schedule is further divided into
1 Multiple major schedules may be deﬁned for a single system, specifying the desired
system behavior for diﬀerent modes of system operation; switching between modes
is accomplished by swapping the major schedule used. If a major cycle is of not too
large a duration, then switches between modes may be restricted to only occur at
the end of major cycles.

96
C. Deutschbein et al.
one or more frames (also known as minor schedules or minor cycles). Each frame
is allocated a ﬁxed length of time during which the computations assigned to
that frame must be executed. Timing correctness is monitored at frame bound-
aries via hardware interrupts generated by a timer circuit: if the computations
assigned to a frame are discovered to have not completed by the end of the frame
then a frame overrun error is ﬂagged and control transferred to an error-handling
routine.
The chief beneﬁts of the cyclic executive approach to scheduling are its imple-
mentation simplicity and eﬃciency, and the timing predictability it oﬀers: if we
have a reliable upper bound on the execution duration of each computation
then an application’s schedulability is determined by construction (i.e., if we are
successful in building the CE then we can be assured that all deadlines are met).
The chief challenge lies in constructing the schedules. This problem is ren-
dered particularly challenging by the requirement that for implementation eﬃ-
ciency considerations, timing monitoring is performed only at frame boundaries
— as stated above, a timer is set at the start of a frame to go oﬀat the end of
the frame, at which point in time it is veriﬁed that all actions assigned to that
frame have indeed completed execution (if not, corrective action must be taken
via a call to error-handling routines). CE’s are typically used for periodic work-
loads. Hence the schedule-generation approach proposed in [1] requires that at
least one frame lie within the interval formed by the instants that each action —
“job” — become available for execution, and the instant that it has a deadline.
For eﬃciency considerations, it is usually required that all tasks have a period
that is a multiple of the minor cycle, and a deadline that is no smaller than
the minor cycle duration. Schedule construction is in general highly intractable
for many interesting models of periodic processes [1]; however, heuristics have
been developed that permit system developers to construct such schedules for
reasonably complex systems (as Baker & Shaw have observed [1], “if we do not
insist on optimality, practical cases can be scheduled using heuristics”).
In this paper, we model our periodic workload as a task system of implicit-
deadline periodic tasks. Some of our results additionally require that the tasks
have harmonic periods: for any pair of tasks τi and τj, it is the case that Ti divides
Tj exactly or Tj divides Ti exactly. Although this does constitute a signiﬁcant
restriction on the periodic task model, many safety-critical systems appear to
respect this restriction.
3
Workload Model
Throughout this paper we assume that we are given a task system τ = {τi =
(Ci, Ti)}N
i=1 of N implicit-deadline periodic2 tasks that are to be scheduled upon
an m-processor identical multiprocessor platform. The worst-case execution time
(WCET) of τi is Ci, and its period is Ti. Let P denote the least common multiple
(lcm) of the periods of all the tasks in τ (P is often called the hyper-period of τ),
2 We highlight that these are periodic, not sporadic, tasks: τi generates jobs at time-
instants k × Ti, for all k ∈N.

Multi-core Cyclic Executives for Safety-Critical Systems
97
N and m
Number of tasks and processors
τi = (Ci, Ti)
The i’th task has worst-case execution time Ci and period Ti
P
lcmN
i=1{Ti} – the hyperperiod. Selected as major cycle duration
F
gcdN
i=1{Ti}. Selected as minor cycle (frame) duration
f
The amount of execution that a single processor can accommodate in
one frame. Upon unit-speed processors, f = F
Φk
The k’th frame, for k ∈{1, 2, . . . , P/F}
n
The total number of jobs in one hyperperiod. n = N
i=1(P/Ti)
ji = (ai, ci, di) The i’th job, 1 ≤i ≤n. Its arrival time, WCET, and absolute deadline
J
The collection of these n jobs
xijk
LP variable: the fraction of the i’th job assigned to the j’th processor
during the k’th frame
Fig. 1. Some of the notation used in this paper
and let F denote the greatest common divisor (gcd) of the periods of all the tasks
in τ. P is selected as the duration of the major cycle, and F the duration of the
minor cycle, of the CE’s we will construct.
Some further notation and terminology: Let J = {j1, j2, . . . , jn) denote all
the jobs generated by τ that have their arrival times and deadlines within
the interval [0, P), and let ai, ci and di denote the arrival time, WCET, and
(absolute) deadline respectively of job ji. (We will often represent a job ji by an
ordered 3-tuple of its parameters: ji
def
= (ai, ci, di). We refer to the interval [ai, di)
as the scheduling window of this job ji.) Note that the number of jobs n may
in general take on a value that is exponential in the number of tasks N. Since
we are seeking to explicitly construct a schedule for the n jobs, we believe that
it is reasonable to evaluate the eﬃciency of algorithms for constructing these
schedules in terms of the number of jobs n to be scheduled rather than in terms
of the number of periodic tasks N.
Without loss of generality, we assume that the tasks are indexed according
to non-decreasing periods: Ti ≤Ti+1 for all i, 1 ≤i < N. For harmonic task
systems τ, the tasks have harmonic periods: Ti divides Ti+1 exactly for all i,
1 ≤i < N.
Example 1. Consider a system τ comprising three tasks τ1, τ2, and τ3, with peri-
ods T1 = 4, T2 = 6, and T3 = 12. P = lcm(4, 6, 12) = 12; F = gcd(4, 6, 12) = 2.
(Therefore, minor cycle duration is 2, and major cycle duration is 12.) For
this τ, J comprises the six jobs j1–j6 depicted in Fig. 2. There are (12/2) =
six frames or minor cycles within the major cycle – these are labeled in the
ﬁgure as Φ1, Φ2, . . . , Φ6 with Φk spanning the interval [2(k −1), 2k].
4
Representing Cyclic Executives as Linear Programs
In this section we represent the problem of constructing a cyclic executive as
a linear program. We start out with a brief review of some well-known facts
concerning linear programs that we will use in later sections of the paper.

98
C. Deutschbein et al.
-
-
-
0
2
4
6
8
10
12
Φ1
Φ2
Φ3
Φ4
Φ5
Φ6
τ1
τ2
τ3
j1
j2
j3
j4
j5
j6
Fig. 2. The jobs generated by the task system of Example 1.
4.1
Some Linear Programming Background
In an integer linear program (ILP), one is given a set of v variables, some or all of
which are restricted to take on integer values only, a collection of “constraints”
that are expressed as linear inequalities over these v variables, and an “objective
function,” also expressed as a linear inequality of these variables. The set of all
points in v-dimensional space over which all the constraints hold is called the
feasible region for the integer linear program. The goal is to ﬁnd the extremal
(maximum or minimum, as speciﬁed) value of the objective function over the
feasible region.
A linear program (LP) is like an ILP, without the constraint that some of
the variables are restricted to take on integer values only. That is, in an LP
over a given set of v variables, one is given a collection of constraints that
are expressed as linear inequalities over these v variables, and an objective
function, also expressed as a linear inequality of these variables. The region in
v-dimensional space over which all the constraints hold is again called the fea-
sible region for the linear program, and the goal is to ﬁnd the extremal value
of the objective function over the feasible region. A region is said to be convex
if, for any two points p1 and p2 in the region and any scalar λ, 0 ≤λ ≤1, the
point (λ · p1 + (1 −λ) · p2) is also in the region. A vertex of a convex region is
a point p in the region such that there are no distinct points p1 and p2 in the
region, and a scalar λ, 0 < λ < 1, such that [p ≡λ · p1 + (1 −λ) · p2].
It is known that an LP can be solved in polynomial time by the ellipsoid
algorithm [9] or the interior point algorithm [7].
We now state without proof some basic facts concerning linear programming
optimization problems.
Fact 1. The feasible region for a LP problem is convex, and the objective func-
tion reaches its optimal value at a vertex point of the feasible region.
An optimal solution to an LP problem that is a vertex point of the feasible
region is called a basic solution to the LP problem.

Multi-core Cyclic Executives for Safety-Critical Systems
99
Fact 2. A basic solution to an LP can be found in polynomial time.
Fact 3. Consider a linear program on v variables with each variable subject to
the constraint that it be ≥0 (such constraints are called non-negativity con-
straints). Suppose that in addition to these non-negativity constraints there are
c other linear constraints. If c < v, then at most v of the variables have non-zero
values at each vertex of the feasible region (including at all basic solutions).
4.2
An LP Representation of CEs
Given a periodic task system comprising N tasks for which an m-processor cyclic
executive is to be obtained, we now describe the construction of a linear program
with

N × m × (P/F)

variables, each of which is subject to a non-negativity
constraint (i.e., each may only take on a value ≥0), and

n+(m+N)×(P/F)

additional linear constraints.
4.2.1
Variables
We will have a variable xijk denote the fraction of job ji that is scheduled upon
the j’th processor during the k’th frame. The index i takes on each integer value
in the range [1, n] (recall that n denotes the total number of jobs generated by
all the periodic tasks over the hyper-period). For each i,
– The index j takes on each integer value in the range [1, m].
– Note that job ji may only execute within those frames that are contained
in the scheduling window – the interval [ai, di) – of job ji. The index k,
therefore, only takes on values over the range of frame-numbers of those
frames contained within [ai, di).
The total number of xijk variables is equal to

N × m × (P/F)

, where N
denotes the number of periodic tasks, m denotes the number of processors, and
P/F represents the number of minor cycles.
4.2.2
An Objective Function
Let f denote the amount of computing that can be accomplished by a processor
executing for the duration F of an entire frame; for unit-speed processors, f = F.
We will deﬁne the following objective function for our LP:
minimize f
(1)
The value of f obtained by solving the LP represents the minimum amount
of computation needed to be completed by an individual processor within a
duration F; if the available processors can indeed accommodate this amount of
computation, then the solution is a feasible one.

100
C. Deutschbein et al.
4.2.3
Constraints
Since the xijk variables represent fractions of jobs, they must all be assigned
values that are ≥0; hence, they are all subject to non-negativity constraints. In
addition, these variables are used to construct a linear program representation
of a CE, via the following constraints:
1. We represent the requirement that each job must receive the required amount
of execution by having the constraints

all j,k
xijk = 1 for each i, 1 ≤i ≤n
(2)
There are n such constraints, one per job.
2. We represent the requirement that each processor may be assigned no more
than f units of execution during each minor cycle by having the constraints

all i
xijk · ci ≤f for each j, 1 ≤j ≤m and k, 1 ≤k ≤P/F
(3)
There are m × (P/F) such constraints.
3. We represent the requirement that each job may be assigned no more than f
units of execution during each minor cycle by having the constraints

all j
xijk · ci ≤f for each i, 1 ≤i ≤n and k, 1 ≤k ≤P/F
(4)
There are N × (P/F) such constraints.
The total number of constraints is thus equal to [n + (m + N) × (P/F)].
4.2.4
Solving the LP
With regards to the LP constructed above, observe that
1. Given an assignment of integer values (i.e., either 0 or 1) to each of the xijk
variables that satisfy the constraints of the LP, we may construct a non-
preemptive cyclic executive in the following manner: for each xijk that is
assigned the value 1, schedule the execution of job ji on the j’th processor
during the k’th frame.
2. Given an assignment of non-negative values to the xijk variables that sat-
isfy the constraints of the LP, we may construct a global preemptive cyclic
executive in the following manner. For each xijk that is assigned a non-zero
value, schedule job ji for a duration xijk ×ci on the j’th processor during the
k’th frame. (Of course, care must be taken to ensure that during each frame
no job executes concurrently upon two diﬀerent processors – we will see in
Sect. 5 below how this is ensured.)
That is, an integer solution to the ILP yields a non-preemptive cyclic executive
while a fractional solution yields a global preemptive cyclic executive. We discuss
the problem of obtaining such solutions, and thereby obtaining preemptive and
non-preemptive cyclic executives respectively, in Sects. 5 and 6 respectively.

Multi-core Cyclic Executives for Safety-Critical Systems
101
5
Preemptive Cyclic Executives
In this section we discuss the problem of constructing preemptive cyclic exec-
utives for implicit-deadline periodic task systems by obtaining solutions to the
linear program described above.
Let us suppose that we have solved the linear program, and have thus
obtained an assignment of non-negative values to the xijk variables that satisfy
the constraints of the LP. We now describe the manner in which we construct a
preemptive cyclic executive for the ko’th frame Φko; the entire cyclic executive
is obtained by repeating this procedure for each ko, 1 ≤ko ≤(P/F).
For each job jio observe that
χio
def
=
m

j=1
xiojko
represents the total amount of execution assigned to job jio during frame Φko
in the solution to the LP. By Constraint 4 of the LP, it follows that χio ≤f
for each job jio; i.e. no job is assigned more than f units of execution over the
frame. Additionally, it follows from summing Constraint 3 of the LP over all m
processors (i.e., for all values of the variable j in Constraint 3) that

n

io=1
χio

≤m × f.
We have thus shown that (i) no individual job is scheduled during the frame for
more than the computing capacity of a single processor during one frame, and
(ii) the total amount of execution scheduled over the interval does not exceed
the cumulative computing capacity of the frame (across all m processors). We
may therefore construct a schedule within the frame using McNaughton’s wrap-
around rule [12] in the following manner:
1. We order the jobs that receive any execution within frame Φko arbitrarily.
2. Then we begin placing jobs on the processors in order, ﬁlling the j’th processor
entirely before starting the (j + 1)’th processor. Thus, a job jio may be split
across processors, assigned to the last t time units of the frame on the j’th
processor and the ﬁrst (χio −t) time units of the frame on the (j + 1)’th
processor; since χio ≤f, these assignments will not overlap in time.
It is evident that this can all be accomplished eﬃciently within run-time
polynomial in the representation of the task system.
Implementation. In Sect. 6.2 below, we describe experiments that we have
conducted comparing ILP-based exact and LP-based approximate algorithms
for constructing non-preemptive CEs. These experiments required us to solve
LPs, similar to the kind described here, using the Gurobi Optimization tool [6];
performance of the Gurobi Optimization tool scaled very well with the size of
the task system in these experiments.

102
C. Deutschbein et al.
6
Non-preemptive Cyclic Executives
We now discuss the process of obtaining 0/1 integer solutions to the linear pro-
gram deﬁned in Sect. 4.2; as discussed there, such a solution can be used to
construct non-preemptive cyclic executives for the periodic task system repre-
sented using the linear program.
Let us start out observing that in order for a non-preemptive cyclic executive
to exist, it is necessary that any job ﬁts into an individual frame; i.e., that
N
max
i=1 {Ci} ≤f
(5)
Any task system for which this condition does not hold cannot be scheduled
non-preemptively.
Let us now take a closer look at the LP that was constructed in Sect. 4.2.
Consider any 0/1 integer solution to this LP. Each xijk variable will take on
value either zero or one in such a 0/1 integer solution; hence in the LP, the
Constraints 2 render the Constraints 4 redundant. To see why this should be so,
consider any job (say, jio), and any frame (say, Φko). From Constraints 2 and
the fact that each xijk variable is assigned a value of zero or one, it follows that
in any 0/1 integer solution to the linear program we will have

j
xiojko = 0

or

j
xiojko = 1

,
depending upon whether job jio is scheduled (on any processor) within the Frame
Φko or not. We thus see that at most one of the xiojko’s can equal 1, from which it
follows that Constraint 4 necessarily holds for job jio within Frame Φko. We may
therefore omit the Constraints 4 in the linear program. Hence for non-preemptive
schedules, we have a somewhat simpler ILP that needs to be solved, comprising

N × m × P
F

variables but only

n + m × P
F

constraints.
6.1
An Approximation Algorithm
The problem of ﬁnding a 0/1 solution to a Linear Program is NP-hard in the
strong sense; all algorithms known for obtaining such solutions have running
time that is exponential in the number of variables and constraints. As we had
mentioned earlier, this intractability of Integer Linear Programming does not
necessarily rule out the ILP-based approach to constructing cyclic executives
that we have described above, since excellent solvers have been implemented
that are able to solve very large ILPs in reasonable amounts of time.
However, the fact of the matter is that not all ILPs can be solved eﬃciently.
We now describe an approximation algorithm for constructing Cyclic Executives,
that does not require us to solve ILPs exactly. The algorithm is approximate in
the sense that it may fail to construct Cyclic Executives for some input instances

Multi-core Cyclic Executives for Safety-Critical Systems
103
for which CE’s do exist (and could have been constructed using the exponential-
time ILP-based method discussed above). In Theorem 1 we quantify the non-
optimality of our approximation algorithm.
Our algorithm starts out constructing the linear program as described in
Sect. 4.2, but without the Constraints 4 (as discussed above, the Constraints 2
render these redundant). However, rather than seeking to solve the NP-hard
problem of obtaining a 0/1 integer solution to this problem, we instead replace
the 0/1 integer constraints with the requirement that each xijk variable be a
non-negative real number no larger than one (i.e., that 0 ≤xijk ≤1 for all
variables xijk), and then obtain a basic solution3 to the resulting linear program
(without the constraint that variables take on integer values). As stated in Fact 2
of Sect. 4.1, such a basic solution can be found eﬃciently in polynomial time.
Recall that our LP has

N × m × P
F

variables but only

n + m × P
F

constraints. By Fact 3 of Sect. 4.1, at most

n + m × P
F

of the variables will
take on non-zero values at the basic solution. Some of these non-zero values will
be equal to one – each such value determines the frame and processor upon
which a job is to be scheduled in the cyclic executive. I.e., for each xijk that is
assigned a value equal to one in the basic solution, we assign job ji to the j’th
processor during frame Φk.
It remains to schedule the jobs which were not assigned as above — these
are the jobs for which Constraint 2 was satisﬁed in the LP solution by having
multiple non-zero terms on the LHS. This is done according to the following
procedure; the correctness of this procedure is proved in [10].
1. Consider all the variables X
def
= xijk that have been assigned non-zero values
strictly less than one in the basic solution. That is,
X
def
=

xijk such that 0 < xijk < 1 in the basic solution

2. Construct a bipartite graph with
(a) A vertex for each job jio such that there is some (one or more) xiojk ∈X.
Let V1 denote the set of all such vertices that are added.
(b) A vertex for each ordered pair [jo, ko] such that there is some (one or
more) xijoko ∈X. Let V2 denote the set of all such vertices that are
added.
(c) For each xiojoko ∈X add an edge in this bipartite graph from the vertex in
V1 corresponding to job jio, to the vertex in V2 corresponding to ordered
pair [jo, ko].
3. It has been shown in [10] that there is a matching in this bipartite graph
that includes all the vertices in V1. Such bipartite matchings can be found in
polynomial time using standard network-ﬂow algorithms.
3 Recall from Sect. 4.1 above that a basic solution to an LP is an optimal solution that
is a vertex point of the feasible region deﬁned by the constraints of the LP.

104
C. Deutschbein et al.
4. Once such a bipartite matching is obtained, each job corresponding to a vertex
in V1 is assigned to the processor and frame corresponding to the vertex in V2
to which it has been matched. In this manner, each processor in each frame
is guaranteed to be assigned at most one job during this process of assigning
the jobs that were not already assigned in the basic solution.
6.2
Evaluating the Approximation Algorithm
We now compare the eﬀectiveness of the polynomial-time approximation algo-
rithm of Sect. 6.1 with that of the ILP-based exact algorithm (solving which
takes exponential time in the worst case). We start out with theoretical evalu-
ation: Corollary 1 quantiﬁes the worst-case performance of the approximation
algorithm via te speedup factor metric. We have also conducted some simulation
experiments on randomly-generated workloads, to get a feel for typical (rather
than worst-case) eﬀectiveness – these are discussed in Sect. 6.2 below.
Theorem 1. Let fopt denote the minimum amount of computation that must
be accommodated on an individual processor within each frame in any feasi-
ble m-processor CE for a given implicit-deadline periodic task system τ. Let
Cmax denote the largest WCET of any task in τ: Cmax
def= maxτi∈τ{Ci}. The
polynomial-time approximation algorithm of Sect. 6.1 above will successfully con-
struct a CE for τ upon m processors, with each processor needing to accommodate
no more than (fopt + Cmax) amount of execution during any frame.
Proof: Since (as we had argued in Sect. 4) an integer solution to the ILP rep-
resents an optimal CE, observe that the minimum value of f computed in an
integer solution to an ILP would be equal to fopt. And since the ILP is more
constrained than the Linear Program, the minimum value for f computed in the
(not necessary integral) solution to the LP obtained by the polynomial-time algo-
rithm of Sect. 6.1 is ≤fopt. Let fLP denote this minimum value of f computed
as a solution to the LP; we thus have that fLP ≤fopt.
In constructing the CE above, the polynomial-time algorithm of Sect. 6.1
schedules each job according to one of two rules:
1. If variable xiojoko is assigned a value one in the solution to the LP, then job
jio is scheduled upon the jo’th processor during frame Φko.
2. Any job jio not scheduled as above is scheduled upon the processor-frame
pair to which it gets matched in the bipartite matching.
Clearly, the jobs assigned according to the ﬁrst rule would ﬁt upon the processors
if each had a computing capacity of fLP within each frame. Now, observe that the
matching in the bipartite graph assigns at most one job to each processor during
any given frame; therefore, the additional execution assigned to any processor
during any frame is < Cmax. Hence each processor could accommodate all the
execution assigned it within each frame provided it had a computing capacity of
at least fLP + Cmax, which is < (fopt + Cmax).
⊓⊔

Multi-core Cyclic Executives for Safety-Critical Systems
105
The speedup factor of an algorithm A is deﬁned to be smallest positive real
number x such that any task system that is successfully scheduled upon a par-
ticular platform by an optimal algorithm is successfully scheduled by algorithm
A upon a platform in which the speed or computing capacity of all processors
are scaled up by a factor (1 + x).
Corollary 1: The polynomial-time approximation algorithm of Sect. 6.1 has a
speedup bound no larger than 2.
Proof: By Theorem 1 above, If a CE can be constructed for task system τ by
an optimal algorithm upon m speed-fopt processors, it can be scheduled by the
polynomial-time algorithm of Sect. 6.1 upon m speed-

fopt + Cmax

processors.
The corollary follows from the observation that Cmax is necessarily ≤fopt; hence

fopt + Cmax

/fopt is ≤2fopt/fopt ≤2.
⊓⊔
Experimental Evaluation. We saw above (Corollary 1) that the polynomial-
time approximation algorithm of Sect. 6.1 has a speedup factor no worse than
2. We have conducted some experiments on randomly-generated synthetic work-
loads to further compare the performance of the approximation algorithm with
the exact approach of solving the ILP.
Workload generation. The task system parameters for each experiment were
randomly generated using a variant of the methods used in prior research such
as [3,5], in the following manner:
– Task utilizations (Ui) were generated using the UUniFast algorithm [2].
– Task periods were set to be at one of F ×{1, 2, 3, 4} (the frame size F was set
equal to 25ms in these experiments, in accordance with prior recent work on
cyclic executives such as [3,5]). Periods were assigned randomly and uniformly
over these four values. (Since we are restricting attention in this paper to
implicit-deadline systems, job deadlines were set equal to their periods.)
– Task WCETs were determined as the product of utilization and period.
– All task systems in which one or more tasks had a WCET greater than minor
cycle duration F, were discarded (since such systems are guaranteed to have
no feasible non-preemptive schedules).
(For some of our experiments, we needed task systems in which the largest
WCET of any task (the parameter Cmax of Theorem 1 was bounded at one-
half of three-quarters the frame size. In generating task systems for these
experiments, we discarded all task systems in which some task had WCET
greater than the bound.)
– All the experiments assumed a four-processor platform (m ←4).
Experiments conducted, and observations made. We conducted two sets
of experiments; in each experiment within each set,

106
C. Deutschbein et al.
1. A task system was generated using the procedure detailed above, with a spec-
iﬁed number of tasks, a speciﬁed total utilization, and for some experiments,
a speciﬁed bound on Cmax.
Each task system so generated was scheduled in two diﬀerent ways.
2. First, it was scheduled non-preemptively by generating a linear program as
described in Sect. 4.2, and then solved as an ILP using the Gurobi [6] opti-
mization tool (instrumented to time out after two seconds of execution, earlier
experiments indicating that for systems of 20 tasks on 4 processors, longer
runs never improved upon the value obtained within the ﬁrst two seconds).
3. Second, it was scheduled preemptively by solving the linear program obtained
above as an LP (i.e., without any integrality constraints) using Gurobi, and
then applying the technique described in Sect. 6.1 to obtain a non-preemptive
cyclic executive. The maximum amount of computation assigned to any
processor within an individual frame in this schedule was determined, and
designated as fmax.
4. The speedup factor needed by the polynomial-time approximation algorithm
for this particular task system was then computed as
max

1, fmax
F

(Recall that F denotes the frame size, chosen to equal 25 ms in our experi-
ments.)
We now describe the two sets of experiments separately.
6.2.1
Variation of Speedup Factor with System Utilization
As explained above, the speedup bound of 2 identiﬁed in Corollary 1 above is
a worst-case one. In this set of experiments, we set out to determine how the
speedup factor of a randomly-generated system tends to depend upon the cumu-
lative utilization of the task system. We therefore generated 400 task systems,
each comprising 20 tasks, to have cumulative system utilization equal to U, for
each value of U between 0 and 4 in steps of 0.05. The observed speedup factor
needed by the approximation algorithm to schedule each task system was deter-
mined as described above, and the average and standard deviations computed.
These values, plotted in Fig. 3, show a clear increasing trend: as overall utiliza-
tion increases, so does the speedup factor needed to construct a non-preemptive
schedule using the approximation algorithm.
6.2.2
Variation of Speedup Factor with Cmax
Theorem 1 reveals that the speedup factor depends upon the value of Cmax,
the largest WCET of any individual task. To investigate this relationship, we
generated 100 task systems with overall utilization U for each value of U between
2 and 4 in steps of 0.05, in which the value of Cmax was bounded from above at
half the frame size, three quarters the frame size, and the full frame size. The
observed speedup factor needed by the approximation algorithm to schedule each

Multi-core Cyclic Executives for Safety-Critical Systems
107
Fig. 3. Investigating how speedup factor changes with overall system utilization. The
mean observed speedup factor over 400 task systems at each utilization is depicted, as
is the range within one standard deviation from the mean.
task system was determined as described above, and the average over the 100
individual task systems at each data point computed. These values, plotted in
Fig. 4, show a clear increasing trend within each system utilization: the larger
the bound on Cmax, the greater the observed speedup factor.
Fig. 4. Investigating how observed speedup factor depends upon Cmax, the largest
WCET of any task. The mean observed speedup factor over 100 task systems is plotted,
for Cmax bounded at 1
2, 3
4, and 1 times the frame size.

108
C. Deutschbein et al.
6.3
Special Case: Harmonic Task Systems
Let us now consider systems in which the tasks have harmonic periods: for any
pair of tasks τi and τj, it is the case that Ti divides Tj exactly or Tj divides Ti
exactly. Many highly safety-critical systems are explicitly designed to respect
this restriction; additionally, many systems that are not harmonic are often
representable as the union of a few – two or three – harmonic sub-systems.
For any job ji, let us deﬁne Fi to be the set of frames that lie within ji’s
scheduling window. For the task system of Example 1 (as depicted in Fig. 2),
e.g., we have
F1 = {Φ1, Φ2}, F2 = {Φ3, Φ4}, F3 = {Φ5, Φ6}, F4 = {Φ1, Φ2, Φ3}, F5 = {Φ4, Φ5, Φ6},
and F6 = {Φ1, Φ2, Φ3, Φ4, Φ5, Φ6}.
Lemma 1: For any two jobs ji and jℓin harmonic task systems, it is the case
that

Fi ⊆Fj

or

Fj ⊆Fi

or

Fi
	
Fj is empty

⊓⊔
A polynomial-time approximation scheme (PTAS) was derived in [4] for the
problem of scheduling on restricted identical machines with nested processing set
restrictions; this PTAS can be directly applied to our problem of constructing
non-preemptive cyclic executives for implicit-deadline periodic task systems with
harmonic periods. This allows us to conclude that for the special case of harmonic
task systems, polynomial-time approximation algorithms may be devised for
constructing cyclic schedules that are accurate to any desired degree of accuracy.
7
Conclusions
Cyclic executives (CEs) are widely used in safety-critical systems industries, par-
ticularly in those application domains that are subject to statutory certiﬁcation
requirements. In our experience, current approaches to the construction of CEs
are either ad hoc and based on the expertise and experience of individual system
integrators, or make use of tools that are based on model checking or heuristic
search.
Recent signiﬁcant advances in the state of the art in the development of linear
programming tools, as epitomized in the Gurobi optimizer [6], have motivated us
to consider the use of linear programming for constructing CEs. We have shown
that CEs for workloads that may be modeled as collections of implicit-deadline
periodic tasks are easily and conveniently represented as linear programs (LPs).
These LPs are solved very eﬃciently in polynomial time by LP tools like Gurobi;
such solutions directly lead to preemptive CEs. If a non-preemptive CE is desired
then one must solve an integer LP (ILP), which is a somewhat less tractable
problem than solving LPs. However, our experiments indicate that Gurobi is
able to solve most ILP problems representing non-preemptive CEs for collections

Multi-core Cyclic Executives for Safety-Critical Systems
109
of implicit-deadline periodic tasks quite eﬀectively in a reasonable amount of
time. We have also developed an approximation algorithm for constructing non-
preemptive CEs that runs in polynomial time, and performs quite favorably in
comparison to the exact algorithm in terms of both a worst-case quantitative
metric (speedup factor) and in experiments on randomly-generated synthetic
workloads.
Acknowledgements. This research is supported by NSF grants CNS 1409175 and
CPS 1446631, AFOSR grant FA9550-14-1-0161, and ARO grant W911NF-14-1-0499.
References
1. Baker, T.P., Shaw, A.: The cyclic executive model and Ada. In: Proceedings of the
IEEE Real-Time Systems Symposium, pp. 120–129 (1988)
2. Bini, E., Buttazzo, G.: Measuring the performance of schedulability tests. Real-
Time Syst. 30(1–2), 129–154 (2005)
3. Burns, A., Fleming, T., Baruah, S.: Cyclic executives, multi-core platforms and
mixed criticality applications. In: Proceedings of the 2015 27th EuroMicro Confer-
ence on Real-Time Systems, ECRTS 2015. IEEE Computer Society Press, Lund
(Sweden) (2015)
4. Epstein, L., Levin, A.: Scheduling with processing set restrictions: PTAS results
for several variants. Int. J. Prod. Econ. 133(2), 586–595 (2011)
5. Fleming, T., Burns, A.: Extending mixed criticality scheduling. In: Proceedings of
the International Workshop on Mixed Criticality Systems (WMC), December 2013
6. Gurobi Optimization Inc: Gurobi Optimizer Reference Manual (2016). http://
www.gurobi.com
7. Karmakar, N.: A new polynomial-time algorithm for linear programming. Combi-
natorica 4, 373–395 (1984)
8. Karp, R.: Reducibility among combinatorial problems. In: Miller, R., Thatcher,
J. (eds.) Complexity of Computer Computations, pp. 85–103. Plenum Press,
New York (1972)
9. Khachiyan, L.:
A polynomial
algorithm
in
linear
programming. Dokklady
Akademiia Nauk SSSR 244, 1093–1096 (1979)
10. Lenstra, J.K., Shmoys, D., Tardos, E.: Approximation algorithms for scheduling
unrelated parallel machines. Math. Program. 46, 259–271 (1990)
11. Liu, J.W.S.: Real-Time Systems. Prentice-Hall Inc., Upper Saddle River (2000)
12. McNaughton, R.: Scheduling with deadlines and loss functions. Manag. Sci. 6, 1–12
(1959)

Compositional Hoare-Style Reasoning
About Hybrid CSP in the Duration Calculus
Dimitar P. Guelev1(B), Shuling Wang2, and Naijun Zhan2,3
1 Institute of Mathematics and Informatics,
Bulgarian Academy of Sciences, Soﬁa, Bulgaria
gelevdp@math.bas.bg
2 State Key Laboratory of Computer Science, Institute of Software,
Chinese Academy of Sciences, Beijing, China
3 University of Chinese Academy of Sciences, Beijing, China
Abstract. Deductive methods for the veriﬁcation of hybrid systems
vary on the format of statements in correctness proofs. Building on the
example of Hoare triple-based reasoning, we have investigated several
such methods for systems described in Hybrid CSP, each based on a
diﬀerent assertion language, notation for time, and notation for proofs,
and each having its pros and cons with respect to expressive power, com-
positionality and practical convenience. In this paper we propose a new
approach based on weakly monotonic time as the semantics for inter-
leaving, the Duration Calculus (DC) with inﬁnite intervals and general
ﬁxpoints as the logic language, and a new meaning for Hoare-like triples
which uniﬁes assertions and temporal conditions. We include a proof
system for reasoning about the properties of systems written in the new
form of triples that is complete relative to validity in DC.
1
Introduction
Hybrid systems exhibit combinations of discrete and continuous evolution, the
typical example being a continuous plant with discrete control. A number of
abstract models and requirement speciﬁcation languages have been proposed
for the veriﬁcation of hybrid systems, the commonest model being hybrid
automata [3,20,24]. Hybrid CSP (HCSP) [19,39] is a process algebra which
extends CSP by constructs for continuous evolution described in terms of
ordinary diﬀerential equations, with domain boundary- and communication-
triggered interruptions. The mechanism of synchronization is message passing.
Because of its compositionality, HCSP can be used to handle complex and open
systems. Here follows an example of a simple generic HCSP description of a
continuously evolving plant with discrete control:
(while ⊤do ⟨F( ˙x, x, u) = 0⟩ sensor!x →actuator?u) ∥
(while ⊤do (wait d; sensor?s; actuator!C(s)))
The plant evolves according to some continuous law F that depends on a control
parameter u. The controller samples the state of the plant and updates the
control parameter once every d time units.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 110–127, 2017.
https://doi.org/10.1007/978-3-319-69483-2_7

Compositional Hoare-Style Reasoning About Hybrid CSP
111
In this paper we propose a Hoare-style proof system for reasoning about
hybrid systems which are modelled in HCSP. The features of HCSP which are
handled by the logic include communication, timing constraints, interrupts and
continuous evolution governed by diﬀerential equations. Our proof system is
based on the Duration Calculus (DC, [4,5]), which is a ﬁrst-order real-time tem-
poral logic and therefore enables the veriﬁcation of HCSP systems for temporal
properties. DC is an interval-based temporal logic. The form of the satisfaction
relation in DC is I, σ |= ϕ, where ϕ is a temporal formula, I is an interpretation
of the respective vocabulary over time, and σ is a reference interval of real time,
unlike point-based TLs, where a reference time point is used. The advantages
of intervals stem from the possibility to accommodate a complete execution of
a process and have reference to termination time points of processes as well as
the starting points. Pioneering work on interval-based reasoning includes Allen’s
interval algebra and Halpern and Shoham’s logic [2,15]. ITLs have been studied
in depth with respect to the various models of time by a number of authors, cf.
e.g. [9]. Since an interval can be described as the pair of its endpoints, interval-
based logics are also viewed as two-dimensional modal logics [35,36]. Interval
Temporal Logic (ITL) was ﬁrst proposed and developed by Moszkowski [7,26,27]
for discrete time, as a reasoning tool for digital circuits. DC can be viewed as
a theory in real time ITL. We use the inﬁnite interval variant of DC which was
proposed in [6], which allows intervals whose right end is ∞for modelling non-
terminating behaviour. We include an operator for Kleene star in order to model
iterative behaviour, to facilitate the handling of liveness properties. Axioms and
proof rules about inﬁnite time and Kleene star in DC can be found in [11,13].
Hoare-style
proof
systems
are
about
proving
triples
of
the
form
{P} code {Q}, which stand for the partial correctness property P(x) ∧
code(x, x′) →Q(x′). The meaning of triples generalizes to the setting of reactive
systems in various ways, the common feature of them all being that P and/or
Q are temporal properties. In our system code is a HCSP term, P and Q are
written in DC. The intended meaning is
Given an inﬁnite run which satisﬁes P at some initial subinterval, code
causes it to satisfy also Q at the initial subinterval representing the
execution of code.
(1)
The initial subinterval which is supposed to satisfy P, can as well be a degenerate
(0-length) one. Then P boils down to an assertion on the initial state. This
interval can also be to the entire inﬁnite run in question. In this case P can
describe conditions provided by the environment throughout runs. Q is supposed
to hold at an interval which exactly matches that of the execution of code. In
case code does not terminate, this would be the whole inﬁnite run too. Using
our DC semantics [[.]] for HCSP terms, the validity of {P} code {Q} is deﬁned
as the validity of
P ⌢⊤⇒¬([[ code ]] ∧¬(Q⌢⊤))
at inﬁnite intervals, which is equivalent to (1).

112
D.P. Guelev et al.
We exploit the form of triples to obtain a compositional proof system, with
each rule corresponding to a basic HCSP construct. This forces proofs to follow
the structure of the given HCSP term. Triples in this form facilitate assume-
guarantee reasoning too. For instance,
{A} code 1{B}
{B} code 2{C}
{A} code 1∥code 2{((B⌢⊤) ∧C) ∨(B ∧(C⌢⊤)}
where ∥denotes parallel composition, is an admissible rule in our proof system,
despite not being among the basic rules. A detailed study of assume-guarantee
reasoning about discrete-time reactive systems in terms of triples of a similar
form with point-based temporal logic conditions can be found in [1].
The main result about our proof system in the paper is its completeness
relative to validity in DC.
Structure of the paper. After brief preliminaries on HCSP, we propose a
weakly-monotonic time semantics for it in terms of DC formulas and prove its
equivalence to an appropriate operational semantics. Next we give our proof
system and use the DC-based semantics to demonstrate its relative completeness.
Finally we summarize a generalization of the approach where arbitrary ﬁxpoints
can be used instead of HCSP’s tail recursion and the use of ∥and the respective
rather involved proof rule can be eliminated. That turns out to require both
the general ﬁxpoint operator of DC [29] and the right-neighbourhood modality
(cf. [4]) to handle the meaning of Ps in the presence of properly recursive calls.
We conclude by discussing related work and make some remarks.
2
Preliminaries
Syntax and informal semantics of Hybrid CSP. Process terms have the
syntax
P, Q ::= skip |
do nothing;
x1, . . . , xn := e1, . . . , en |
simultaneous assignment;
wait d | await b |
ﬁxed time delay; wait until b becomes true;
ch?x | ch!e | IO |
input
and
output;
communication-guarded
choice;
⟨F( ˙x, x) = 0 ∧b⟩|
x evolves according to F as long as b holds;
⟨F( ˙x, x) = 0 ∧b⟩ IO |
evolve by F until ¬b or IO becomes ready;
terminate, if ¬b is reached ﬁrst;
otherwise execute IO;
P; Q | P ∥Q |
sequential composition; parallel composition
if b then P else Q | P ⊔Q | conditional; internal non-deterministic choice;
µX.G
recursion.
In the above BNF, IO has the following form:
ch1?x1 →P1[] . . . []chk?xk →Pk[]chk+1!ek+1 →Pk+1[] . . . []chn!en →Pn (2)

Compositional Hoare-Style Reasoning About Hybrid CSP
113
for some arbitrary k, n, x1, . . . , xk, ek+1, . . . , en and some distinct ch1, . . . , chn.
IO engages in one of the indicated communications as soon as a partner process
becomes ready, and then proceeds as the respective Pi. In μX.G, G has the
syntax
G ::= H | −→x := −→e ; P | ⟨F( ˙x, x) = 0 ∧b⟩; P | if b then G else G
| G ⊔G | G; P | μY.G | H∥H
where H stands for arbitrary X-free terms, Y ̸= X and P can be any process
term. This restricts X of μX.G to be guarded in G and rules out occurrences
of X of μX.G in the scope of ∥in G. The communication primitives ch?x and
ch!e are not mentioned in the syntax for G as they are treated as derived in
this paper. They can be assigned the role of guards which −→x := −→e has in (1).
Obviously X is guarded in the P1, . . . , Pn of IO as in (2) too. Below we focus
on the commonest instance of μ
while b do P ⇌μX.if b then (P; X) else skip
(3)
CSP’s Kleene star P ∗⇌μX.(skip ⊔(P; X)), which stands for some unspeciﬁed
number of successive executions of P, is handled similarly. We explain how our
setting ports to general ﬁxpoints, and some technical beneﬁts from that, other
than the obvious gain in expressive power, in a dedicated section.
The Duration Calculus. We use DC with inﬁnite intervals as in [6,13] and
Kleene star. The reader is referred to [4] for a comprehensive introduction. DC
is a classical ﬁrst-order predicate modal logic with one normal binary modality
called chop and written ⌢. The time domain is R∞= R ∪{∞}. Satisfaction
has the form I, σ |= ϕ where I is an interpretation of the non-logical symbols,
σ ∈I(R∞), I(R∞) = {[t1, t2]; t1 ∈R, t2 ∈R∞, t1 ≤t2}. Flexible non-logical
symbols depend on the reference intervals for their meaning. Chop is deﬁned by
the clause
I, σ |= (ϕ⌢ψ) iﬀeither there exists a t ∈σ \ {∞} such that I, [min σ, t] |= ϕ
and I, [t, max σ] |= ψ, or max σ = ∞and I, σ |= ϕ.
Along with the usual ﬁrst-order non-logical symbols, DC features boolean valued
state variables, which form boolean combinations called state expressions. The
value It(S) of a state expression S is supposed to change between 0 and 1 only
ﬁnitely many times in every bounded interval of time. Duration terms

S take a
state expression S as the operand. The value of

S at interval σ is
max σ

min σ
I(S)(t)dt,
which is the combined length of the parts of σ which satisfy S. ℓis used for

(0 ⇒0) and always evaluates to the length of the reference interval. Other
common deﬁned constructs include
⌈S⌉0 ⇌

S = ℓ, ⌈S⌉⇌⌈S⌉0 ∧ℓ̸= 0, 3ϕ ⇌(⊤; ϕ; ⊤), 2ϕ ⇌¬2¬ϕ.
In this paper we additionally use the following abbreviations:
⌈S⌉ﬁn ⇌⌈S⌉∧ℓ< ∞,
⌈S⌉0
ﬁn ⇌⌈S⌉0 ∧ℓ< ∞
i2◦ϕ ⇌¬(¬ϕ; ℓ̸= 0),
s2◦ϕ ⇌¬(⊤; ℓ̸= 0 ∧¬ϕ)

114
D.P. Guelev et al.
In Sect. 6 we use the converse neighbourhood modality 3c
l of Neighbourhood Logic
(and the corresponding system of DC), which is deﬁned by the clause:
I, σ |= 3c
l ϕ iﬀI, [min σ, t] |= ϕ for some t ∈R ∪{∞}, t ≥min σ.
We also use the least ﬁxpoint operator μ. In formulas μX.ϕ, where X is a dedi-
cated type of variable, ϕ can only have positive occurrences of X. The meaning
of μX.ϕ, is deﬁned by means of the operator of type P(I(R∞)) →P(I(R∞)),
which maps A ⊆I(R∞) to {σ ∈I(R∞) : IA
X, σ |= ϕ}. I, σ |= μX.ϕ iﬀσ
appears in the least ﬁxpoint of this operator, which happens to be monotonic
by virtue of the syntactic condition on ϕ. Using μ, Kleene star ϕ∗is deﬁned as
μX.(ℓ= 0 ∨ϕ⌢X).
3
Operational Semantics of HCSP
Ownership of variables. We write Var(P) for the set of the program variables
which occur in P. Expressions of the form ˙x in continuous evolution process
terms are, syntactically, just program variables, and are restricted not to appear
in arithmetical expressions e outside the F( ˙x, x) of continuous evolution terms,
or on the left hand side of :=. As it becomes clear below, the dependency between
x and ˙x as functions of time is spelled out as part of the semantics of continuous
evolution. We write Var:=(P) for all the variables in Var(P) which occur on the
left hand side of :=, in ch? statements, and the x-es or ˙x-es in any of the forms
of continuous evolution within P. Parallel composition P∥Q is well-formed only
if Var:=(P) ∩Var:=(Q) = ∅.
Modelling input and output. We treat ch?x and ch!e as derived constructs
as they can be deﬁned in terms of dedicated shared variables ch?, ch! and ch
after [28]:1
ch!e ⇌ch := e; ch! := ⊤; await ch?; await ¬ch?; ch! := ⊥
ch?x ⇌ch? := ⊤; await ch!; x := ch; ch? := ⊥; await ¬ch!
We assume that ch!, ch ∈Var:=(ch!e) and ch? ∈Var:=(ch?x). Communication-
guarded external choice IO can be deﬁned similarly. We omit the deﬁnition as it
is lengthy and otherwise uninsightful. The other derived constructs are deﬁned
as follows:
⟨F( ˙x, x) = 0 ∧b⟩d Q ⇌t := 0; ⟨F( ˙x, x) = 0 ∧˙t = 1 ∧b ∧t ≤d⟩;
if ¬(¬b) then Q else skip
wait d
⇌⟨0 = 0 ∧⊤⟩d skip
⟨F( ˙x, x) = 0 ∧b⟩ IO ⇌⟨F( ˙x, x) = 0 ∧b ∧
i∈I
¬ch∗
i ⟩; if

i∈I
¬ch∗
i then skip else IO
1 Hoare style proof rules for a system with ch?x and ch!e appearing as primitive con-
structs were proposed by Zhou Chaochen et al. in [12]. That work features a diﬀerent
type of triples and follows the convention that process variables are not observable
across threads thus ruling out a shared-variable emulation of communication.

Compositional Hoare-Style Reasoning About Hybrid CSP
115
Here ch∗
i stands for chi?, resp. chi!, depending on whether the respective action
in IO is input or output. To account of the impossibility to mechanically (and
computationally) tell apart x < c from x ≤c about time-dependent quantities
x, in ¬(¬b) we use a for a condition that deﬁnes the topological closure of
{x : a(x)}. It is assumed that b admits a syntactical deﬁnition. E.g., x < c is
x ≤c for x being a continuous function of time.
Reduction of HCSP process terms. Next we deﬁne a reduction relation
P
A,V
−→Q where V is a set of process variables and
A : σ →(V ′ ∪{r, n} →R∞∪{0, 1}),
(4)
where V ′ is a set of process variables, r and n are boolean variables outside
V ′ and σ ∈I. In R∞∪{0, 1} we emphasize the additional use of 0, 1 ∈R as
truth values. We consider P
A,V
−→Q only for V such that Var:=(P) ⊆V ⊆V ′.
For HCSP terms P in the scope of a Q which on its turn is an operand of a
∥, with no other ∥s between this one and P, the semantics of P must specify
the behaviour of all the variables from Var:=(Q), which are controlled by the
enveloping thread Q of P. V ′ \ V is meant to include the variables which are not
controlled by the enveloping thread of P but still may be accessed in it. In the
sequel we write domA for σ and Var(A) for V ′ from (4).
If V ⊆Var(A), then A|V stands for the restriction of A to the variables
from V . I.e., given A as in (4),
A|V : σ →(V ∪{r, n} →R∞∪{0, 1}).
Given an arithmetic or boolean expression e such that V (e) ⊆V (A), we write
At(e) for the value of e under A at time t ∈domA. Given A and B such that
max domA = min domB, Var(A) = Var(B) and Amax domA(x) = Bmin domB(x)
for all x ∈Var(A) ∪{r, n}, A; B is determined by the conditions domA; B =
domA ∪domB, (A; B)t(x) = At(x) for t ∈domA and (A; B)t(x) = Bt(x) for
t ∈domB for all x ∈Var(A)∪{r, n}. A complete and possibly inﬁnite behaviour
of P can be deﬁned as A1; A2; . . . where Pi−1
Ai,V
−→Pi, and P0 = P.
The auxiliary variables r and n. To handle the causal ordering of compu-
tation steps without having to account of the negligibly small time delays they
contribute, we allow stretches of time in which continuous evolution is ’frozen’,
and which are meant to just keep apart time points with diﬀerent successive
variable values that have been obtained by computation. Intervals of negligi-
ble time are marked by the boolean variable r. P (or any of its descendant
processes) claims exclusive control over the process variables during such inter-
vals, thus achieving atomicity of assignment. Time used for computation steps
by any process which runs in parallel with P or P itself is marked by n. Hence
At(r) ≤At(n), t ∈domA always holds in the mappings (4). As it becomes clear

116
D.P. Guelev et al.
below, each operand Pi of a P1∥P2 has its own r, and no two such variables
evaluate to 1 at the same time, which facilitates encoding the meaning of ∥by
conjunction. In processes with loops and no other recursive calls, the rs can be
enumerated globally. More involved form of recursive calls require the rs to be
quantiﬁed away.
This approach is known as the true synchrony hypothesis. It was introduced
to the setting of DC in [30] and was developed in [10,11] where properties ϕ of
the overall behaviour of a process in terms of the relevant continuously evolving
quantities are written (ϕ/¬N), the projection of ϕ onto state ¬N, which holds
iﬀϕ holds at the interval obtained by gluing the ¬N-parts of the reference one.
The approach is alternative to the use of super-dense chop [16].
The reduction rules. To abbreviate conditions on A in the rules which generate
the valid instances of P
A,V
−→Q below, given an X ⊆Var(A) and a boolean or
arithmetical expression e, we put:
const(X, A)
⇌
x∈X
(∀t ∈domA)(At(x) = Amin domA(x))
const◦(X, A)
⇌
x∈X
(∀t ∈domA \ {max domA})(At(x) = Amin domA(x))
const◦(e, A, a) ⇌(∀t ∈domA \ {max domA})(At(e) = a)
Below we omit the mirror images of rules about commutative connectives
⊔and ∥.
max domA = min domA
skip
A,V
−→✓
const(V \ {x1, . . . , xn}, A)
const◦({x1, . . . , xn}, A)
const◦(r ∧n, A, 1)
Amax domA(xi) = Amin domA(ei), i = 1, . . . , n
max domA < ∞
x1, . . . , xn := e1, . . . , en
A,V
−→✓
P
A,V
−→P ′
P ′ ̸= ✓
P; Q
A,V
−→P ′; Q
P
A,V
−→✓
max domA < ∞
P; Q
A,V
−→Q
P
A,V
−→P ′
max domA = ∞
P; Q
A,V
−→P ′
const◦(F( ˙x, x), A, 0)
const◦(At( ˙x) −d
dtAt(x), A, 0)
const◦(¬r ∧¬n ∧b, A, 1)
const(V \ { ˙x, x}, A)
⟨F( ˙x, x) = 0 ∧b⟩
A,V
−→⟨F( ˙x, x) = 0 ∧b⟩
Amin domA(b) = 0
max domA = min domA
⟨F( ˙x, x) = 0 ∧b⟩
A,V
−→✓
P
A,V
−→R
Amin domA(b) = 1
if b then P else Q
A,V
−→R
Q
A,V
−→R
Amin domA(b) = 0
if b then P else Q
A,V
−→R
P
A,V
−→P ′
P ⊔Q
A,V
−→P ′

Compositional Hoare-Style Reasoning About Hybrid CSP
117
[μX.P/X]P
A,V
−→Q
μX.P
A,V
−→Q
Var(A) ∪Var(B) ⊆Var(C)
C|Var(A)∪{r,n} = A
C|Var(B)∪{r,n} = B
const◦(¬r ∧¬n, C, 1)
V1 ∩V2 = ∅
P
A,V1
−→✓Q
B,V2
−→✓
P∥Q
C,V1∪V2
−→
✓
Var(A) ∪Var(B) ⊆Var(C)
C|Var(A)∪{r,n} = A
C|Var(B)∪{r,n} = B
const◦(¬r ∧¬n, C, 1)
V1 ∩V2 = ∅
P
A,V1
−→P ′ Q
B,V2
−→Q′
P ′ ̸= ✓, Q′ ̸= ✓
P∥Q
C,V1∪V2
−→
P ′∥Q′
Var(A) ∪Var(B) ⊆Var(C)
C|Var(A)∪{r,n} = A
C|Var(B)∪{r,n} = B
const◦(¬r ∧¬n, C, 1)
V1 ∩V2 = ∅
P
A,V1
−→P ′ Q
B,V2
−→✓
P ′ ̸= ✓
P∥Q
C,V1∪V2
−→
P ′
V ⊆V ′
const(V ′ \ V, B)
V ′ ∪Var(A) ⊆Var(B)
B|Var(A)∪{r,n} = A
const◦(r ∧n, A, 1)
P
A,V
−→P ′
P ′ ̸= ✓
P∥Q
B,V ′
−→P ′∥Q
V ⊆V ′
const(V ′ \ V, B)
V ′ ∪Var(A) ⊆Var(B)
B|Var(A)∪{r,n} = A
const◦(r ∧n, A, 1)
P
A,V
−→✓
P∥Q
B,V ′
−→Q
4
A DC Semantics of Hybrid Communicating Sequential
Processes
Given a process P, [[P]], with some subscripts to be speciﬁed below, is a DC
formula which deﬁnes the class of DC interpretations that represent runs of P.
Process variables and their corresponding DC temporal variables.
Real-valued process variables x are modelled by pairs of DC temporal variables
x and x′, which are meant to store the value of x at the beginning and at the
end of the reference interval, respectively. The axiom
2∀z¬(x′ = z⌢x ̸= z).
entails that the values of x and x′ are determined by the beginning and the end
point of the reference interval, respectively. It can be shown that
|=DC 2∀z¬(x′ = z⌢x ̸= z) ⇒
2(x = c ⇒¬(x ̸= c⌢⊤)) ∧2(x′ = c ⇒¬(⊤⌢x′ ̸= c))
This is known as the locality principle in ITL about x. About primed variables
x′, the locality principle holds wrt the endpoints of reference intervals. Boolean
process variables are similarly modelled by propositional temporal letters. For
the sake of brevity we put

118
D.P. Guelev et al.
loc(X) ⇌

x∈X
x is real
2∀z¬(x′ = z⌢x ̸= z) ∧

x∈X
x is boolean
2¬((x′⌢¬x) ∨(¬x′⌢x))
In the sequel, given a DC term e or formula ϕ written using only unprimed
variables, e′ and ϕ′ stand for the result of replacing all these variables by their
respective primed counterparts.
Time derivatives of process variables. As mentioned above, terms of the
form ˙x where x is a process variable are treated as distinct process variables and
are modelled by their respective temporal variables ˙x and ˙x′. The requirement
on ˙x to be interpreted as the time derivative of x is incorporated in the semantics
of continuous evolution statements.
Computation time and the parameters [[.]]. As explained in Sect. 3, we allow
stretches of time that are dedicated to computation steps and are marked by the
auxiliary boolean process variable r. Such stretches of time are conveniently
excluded when calculating the duration of process execution. To this end, in DC
formulas, we use a state variable R which indicates the time taken by computa-
tion steps by the reference process. Similarly, a state variable N indicates time
for computation steps by which any process that runs in parallel with the ref-
erence one, including the reference one. R and N match the auxiliary variables
r and n from the operational semantics and, just like r and n, are supposed to
satisfy the condition R ⇒N. We assume that all continuous evolution becomes
temporarily suspended during intervals in which computation is performed, with
the relevant real quantities and their derivatives remaining frozen. To guaran-
tee the atomicity of assignment, computation intervals of diﬀerent processes are
not allowed to overlap. As it becomes clear in the DC semantics of ∥below,
Pi of P1∥P2 are each given its own variable Ri, i = 1, 2, to mark computation
time, and R1 and R2 are required to satisfy the constraints ¬(R1 ∧R2) and
R1 ∨R2 ⇔R where R is the variable which marks computation times for the
whole of P1∥P2.
The semantics [[P]]R,N,V of a HCSP term P is given in terms of the DC
temporal variables which correspond to the process variables occurring in P, the
state variables R and N, and the set of variables V which are controlled by P’s
immediately enveloping ∥-operand.
Assignment. To express that the process variables from X ⊆V may change at
the end of the reference interval only, and those from V \ X remain unchanged,
we write
const(V, X) ⇌

x∈V \X
x is real
2(x′ = x) ∧

x∈V \X
x is boolean
2(x ⇔x′)∧

x∈X
x is real
i2◦(x′ = x) ∧

x∈X
x is boolean
i2◦(x′ ⇔x).

Compositional Hoare-Style Reasoning About Hybrid CSP
119
The meaning of simultaneous assignment is as follows:
[[x1, . . . , xn := e1, . . . , en]]R,N,V ⇌⌈R⌉ﬁn ∧const(V, {x1, . . . , xn})∧

i=1,...,n
xi is real
x′
i = ei ∧

i=1,...,n
xi is boolean
x′
i ⇔ei.
Parallel composition. Consider processes P1 and P2 and V ⊇Var:=(P1∥P2).
Let 1 = 2, 2 = 1. Let
∃∥(R, R1, R2, V, P1, P2)ϕ ⇌∃R1∃R2
 ⌈(R1 ∨R2 ⇔R) ∧¬(R1 ∧R2)⌉0∧
2
i=1
2(⌈Ri⌉⇒const(V \ Var:=(Pi))) ∧ϕ

.
∃∥(R, R1, R2, V1, V2) means that
– the R-subintervals for the computation steps of P1∥P2 can be divided into R1-
and R2-subintervals to mark the computation steps of some sub-processes P1
and P2 of P which run in parallel;
– the variables which are not controlled by Pi remain unchanged during Pi’s
computation steps, i = 1, 2, and, ﬁnally,
– some property ϕ, which can involve R1 and R2, holds.
The universal dual ∀∥of ∃∥is deﬁned in the usual way. Let Vi abbreviate
Var:=(Pi). Now we can deﬁne [[P1∥P2]]R,N,V as
∃∥(R, R1, R2, V, P1, P2)
2
i=1

[[Pi]]Ri,N,Vi ∧(⌈N ∧¬Ri⌉0
ﬁn
⌢[[Pi]]Ri,N,Vi
⌢⌈¬Ri⌉0)∨
(⌈N ∧¬Ri⌉0
ﬁn
⌢[[Pi]]Ri,N,Vi) ∧([[Pi]]Ri,N,Vi
⌢⌈¬Ri⌉0)

(5)
To understand the four disjunctive members of Φ above, note that P1∥P2 always
starts with some action on behalf of either P1, or P2, or both, in the case of
continuous evolution. Hence (at most) one of Pi, i = 1, 2, needs to allow negligible
time for Pi’s ﬁrst step. This is expressed by a ⌈N ∧¬Ri⌉0
ﬁn before [[Pi]]Ri,N,Vi.
The amount of time allowed is ﬁnite and may be 0 in case both P1 and P2
start with continuous evolution in parallel. This makes it necessary to consider
two cases, depending on which process starts ﬁrst. If Pi terminates before Pi,
then a ⌈N ∧¬Ri⌉0 interval follows [[Pi]]Ri,N,Vi. This generates two more cases to
consider, depending on the value of i.
The deﬁnitions of [[x1, . . . , xn := e1, . . . , en]]R,N,V and [[P1∥P2]]R,N,V already
appear in (3) and (5). Here follow the deﬁnitions for the rest of the basic con-
structs:

120
D.P. Guelev et al.
[[await b]]R,N,V
⇌const(V ) ∧(⌈¬R⌉∨ℓ= 0) ∧
i2◦¬b′ ∧(b′ ∨ℓ= ∞)
[[⟨F( ˙x, x) = 0 ∧b⟩]]R,N,V
⇌
⎛
⎜
⎜
⎜
⎜
⎝
const(V \ { ˙x, x}) ∧⌈¬R⌉∧
2
⎛
⎜
⎜
⎝
⌈N⌉⇒const({ ˙x, x})∧
∀ub2( ˙x ≤ub) ⇒x′ ≤x + ub
	
¬N∧
∀lb2( ˙x ≥lb) ⇒x′ ≥x + lb
	
¬N∧
F( ˙x, x) = 0
⎞
⎟
⎟
⎠∧
s2◦b
⎞
⎟
⎟
⎟
⎟
⎠
⌢(¬b ∧ℓ= 0)
[[P; Q]]R,N,V
⇌([[P]]R,N,V
⌢⌈N ∧¬R⌉0
ﬁn
⌢[[Q]]R,N,V )
[[P ⊔Q]]R,N,V
⇌[[P]]R,N,V ∨[[Q]]R,N,V
[[if b then P else Q]]R,N,V ⇌(b ∧[[P]]R,N,V ) ∨(¬b ∧[[Q]]R,N,V )
[[while b do P]]R,N,V
⇌(b ∧[[P]]R,N,V
⌢⌈¬R⌉0)∗⌢(¬b ∧ℓ= 0)
To understand [[⟨F( ˙x, x) = 0 ∧b⟩]]R,N,V , observe that, assuming I to be the DC
interpretation in question and λt.It( ˙x) to be continuous, the two inequalities in
[[⟨F( ˙x, x) = 0 ∧b⟩]]R,N,V express that
It2(x) −It1(x) =
t2
t1
It( ˙x)(1 −It(N))dt
at all ﬁnite subintervals [t1, t2] of the reference intervals. This means that both
˙x and x are constant in N-subintervals, and It2(x) −It1(x) =
t2
t1
It( ˙x)dt holds at
¬N-subintervals.
Completeness of [[.]]. Given a process term P, every DC interpretation I for
the vocabulary of [[P]]N, N, V represents a valid behaviour of P with N being
true in the subintervals which P uses for computation steps. To realize this,
consider HCSP terms P, Q of the syntax
P, Q ::= skip | A; R | []iAi; Ri | if b then P else Q | P ⊔Q
A
::= x := e | await b | ⟨F( ˙x, x) = 0 ∧b⟩
(6)
where R and Ri stand for a process term with no restrictions on its syntax
(e.g., occurrences of while-terms are allowed). (6) is the guarded normal form
(GNF) for HCSP terms, with the guards being the primitive terms of the form A,
and can be established by induction on the construction of terms, with suitable
equivalences for each combination of guarded operands that ∥may happen to
have. E.g.,
⟨F1( ˙x, x) = 0 ∧b1⟩; P1∥⟨F2( ˙x, x) = 0 ∧b2⟩; P2
(7)
is equivalent to
⟨F1( ˙x, x) = 0 ∧F2( ˙x, x) = 0 ∧b1 ∧b2⟩;
if b1 then ⟨F1( ˙x, x) = 0 ∧b1⟩; P1∥P2
else if b2 P1∥⟨F2( ˙x, x) = 0 ∧b2⟩; P2
else P1∥P2
(8)

Compositional Hoare-Style Reasoning About Hybrid CSP
121
Note that []iAi; Ri is a modest generalization of IO as deﬁned in (2). Some com-
binations of operands of ∥require external choice to be extended this way, with
the intended meaning being that if none of the Ais which have the forms ch?x
and ch!e is ready to execute, then some other option can be pursued immediately.
For example, driving ∥inwards may require using that
(ch1?x →P1[]ch2!e →P2[]ch3?y →P3)∥ch1!f; Q1∥ch2?z; Q2 ≡
((x := f; (P1∥Q1)∥ch2?z; Q2) ⊔(z := e; (P2∥Q2)∥ch1!f; Q1))[]
ch3?y; P3∥ch1!f; Q1∥ch2?z; Q2.
On the RHS of ≡above, one of the assignments and the respective subsequent
process are bound to take place immediately in case the environment is not ready
to communicate over ch3.
The GNF renders the correspondence between the semantics of guards and
the As which appear in the rules for
A,V
−→explicit, thus making obvious that any
ﬁnite preﬁx of a valid behaviour satisﬁes some chop-sequence of guards that can
be generated by repeatedly applying the GNF a corresponding number of times
and then using the distributivity of chop over disjunction, starting with the given
process term, and then proceeding to transform the R-parts of guarded normal
forms that appear in the process. The converse holds too. This entails that the
denotational semantics is equivalent to the operational one.
5
Reasoning About Hybrid Communicating Sequential
Processes with DC Hoare Triples
We propose reasoning in terms of triples of the form
{A}P{G}V
(9)
where A and G are DC formulas, P is a HCSP term, and V is a set of program
variables. V is supposed to denote the variables whose evolution needs to be
speciﬁed in the semantics of P, e.g., an assignment x := e in P is supposed to
leave the values of the variables y ̸= x unchanged. This enables deriving, e.g.,
{y = 0}x := 1{y = 0}{x,y}, which would be untrue for a y that belongs to
a process that runs in parallel with the considered one and is therefore not a
member of V . Triple (9) is valid, if
|= loc(V ) ∧⌈R ⇒N⌉0 ∧(A⌢⊤) ⇒¬([[P]]R,N,V ∧¬G⌢⊤)
(10)
Since R and N typically occur in [[P]]R,N,V , triples (9) can have occurrences of
R and N in A and G too, with their intended meanings.
Next we propose axioms and rules for deriving triples about processes with
each of the HCSP constructs as the main one in them. For P of one of the forms
skip, x1, . . . , xn := e1, . . . , en, and ⟨F( ˙x, x) = 0 ∧b⟩, we introduce the axioms
{⊤}P{[[P]]R,N,V }V .

122
D.P. Guelev et al.
where V can be any set of process variables such that V ⊇Var:=(P). Here
follow the rules for reasoning about processes which are built using each of the
remaining basic constructs:
(seq)
{A}P{G}V
{B}Q{H}V
loc(V ) ∧⌈R ⇒N⌉0 ∧(A⌢⊤) ⇒¬(G⌢¬(⌈N ∧¬R⌉0⌢B⌢⊤))
{A}P; Q{(G⌢⌈N ∧¬R⌉0⌢H)}V
(⊔)
{A}P{G}V
{B}Q{H}V
{A ∧B}P ⊔Q{G ∨H}V
(if)
{A ∧b}P{G}V
{A ∧¬b}Q{G}V
{A}if b then P else Q{G}V
(while)
{A}P{G}V
loc(V ) ∧⌈R ⇒N⌉0 ∧(A⌢⊤) ⇒¬(G ∧ℓ< ∞⌢¬3c
l (⌈¬R⌉0⌢A))
{A}while b do P{((b ∧G⌢⌈¬R⌉0)∗⌢¬b ∧ℓ= 0)}V
Parallel composition. The established pattern suggests the following proof
rule (∥):
{A1}P1{G1}Var:=(P1)
{A2}P2{G2}Var:=(P2)

∀∥(R, R1, R2, V, P1, P2)
 2
i=1
¬(⌈N ∧¬Ri⌉0
ﬁn
⌢¬[Ri/R]Ai
⌢⊤) ∧([Ri/R]Ai
⌢⊤)

P1∥P2
⎧
⎨
⎩∃∥(R, R1, R2, V, P1, P2)
⎛
⎝
2
i=1
Gi ∧(⌈N ∧¬Ri⌉0
ﬁn
⌢Gi
⌢⌈¬Ri⌉0)
∨
(⌈N ∧¬Ri⌉0
ﬁn
⌢Gi) ∧(Gi
⌢⌈¬Ri⌉0)
⎞
⎠
⎫
⎬
⎭
V
This rule can be shown to be complete as it straightforwardly encodes the
semantics of ∥. However, it is not convenient for proof search as it only derives
triples with a special form of the condition on the righthand side and actually
induces the use of [[Pi]]Ri,N,Var:=(Pi) as Gi, which typically give excess detail. We
discuss a way around this inconvenience below, together with the modiﬁcations
of the setting which are needed in order to handle general HCSP ﬁxpoints μX.P.
General rules. Along with the process-speciﬁc rules, we also need the rules
(N)
loc(V ) ∧⌈R ⇒N⌉0 ∧3c
l A ⇒G
{A}P{G}V
Var:=(P) ⊂V
(K)
{A}P{G ⇒H}V
{B}P{G}V
loc(V ) ∧⌈R ⇒N⌉0 ∧3c
l A ⇒3c
l B
{A}P{H}V
These rules are analogous to the modal form N of G¨odel’s generalization rule
(also known as the necessitation rule) and the modal axiom K.
Soundness and Completeness. The soundness of the proof rules is estab-
lished by a straightforward induction on the construction of proofs with the
deﬁnition of [[.]]R,N,V . The system is also complete relative to validity in DC.

Compositional Hoare-Style Reasoning About Hybrid CSP
123
This eﬀectively means that we allow all valid DC formulas as axioms in proofs,
or, equivalently, given some suﬃciently powerful set of proof rules and axioms for
the inference of valid DC formulas in DC with inﬁnite intervals, our proof rules
about triples suﬃce for deriving all the valid triples. Such systems are beyond
the scope of our work. A Hilbert-style proof system for ITL with inﬁnite intervals
was proposed and shown to be complete with respect to an abstractly deﬁned
class of time models (linearly ordered commutative groups) in [13], building on
similar work about ﬁnite intervals from [8].
The deductive completeness of our proof system boils down to the possibility
to infer triples of the form {⊤}P{G}V for any given term P and a certain
strongest corresponding G, which turns out to be the DC formula [[P]]R,N,V that
we call the semantics of P. Then the validity of {⊤}P{[[P]]R,N,V }V is used to
derive any valid triple about P by a simple use of the proof rules K and N.
The completeness now follows from the fact that [[P]]R,N,V deﬁnes the class of
all valid behaviours of P.
Proposition 1. The triple
{⊤}P{[[P]]R,N,V }V
(11)
is derivable for all process terms P and all V such that Var:=(P) ⊇V .
Corollary 1 (relative completeness of the Hoare-style proof system).
Let A, G and P be such that (10) is valid. Then (9) is derivable in the extension
of the given proof system by all DC theorems.
6
General Fixpoints and Bottom-Up Proof Search
in HCSP
To avoid the constraints on the form of the conclusion of rule (∥), we propose
a set of rules which correspond to the various possible forms of the operands of
the designated ∥in the considered HCSP term. These rules enable bottom-up
proof search much like when using the rules for (just) CSP constructs, which is
key to the applicability of classical Hoare-style proof. We propose separate rules
for each combination of main connectives in the operands of ∥, except ∥itself
and the ﬁxpoint construct. For instance, the equivalence between (7) and (8)
suggests the following rule for this particular combination of ∥with the other
connectives:
{P}⟨F1( ˙x, x) = 0 ∧F2( ˙x, x) = 0 ∧b1 ∧b2⟩{R}
{R ∧b1 ∧¬b2}⟨F1( ˙x, x) = 0 ∧b1⟩; P1∥P2{Q}
{R ∧b2 ∧¬b1}P1∥⟨F2( ˙x, x) = 0 ∧b2⟩; P2{Q}
{R ∧¬b1 ∧¬b2}P1∥P2{Q}
{P}⟨F1( ˙x, x) = 0 ∧b1⟩; P1∥⟨F2( ˙x, x) = 0 ∧b2⟩; P2{Q}
Rules like the above one use the possibility to drive ∥inwards by equivalences
like that between (7) and (8), which can be derived for all combinations of the

124
D.P. Guelev et al.
main connectives of ∥’s operands, except for loops, and indeed can be used to
eliminate ∥. For while-loops, the GNF contains a copy of the loop on the RHS
of chop:
while b do P ≡if b then (P; while b do P) else skip.
(12)
Tail-recursive instances of μX.G come handy in completing the elimination of ∥
in such cases by standard means, namely, by treating equivalences such as (12)
as the equations leading to deﬁnitions such as (3).
To handle general recursion in our setting, we need to take care of the
special way in which we treat A from {A}P{G} in (10). In the rule for
{A}while b do P{G} clipping of initial G-subintervals of an A-interval are
supposed to leave us with suﬃx subintervals which satisfy (A⌢⊤), to provide
for successive executions of P. With X allowed on the LHS of chop in the P
of μX.P, special care needs to be taken for this to be guaranteed. To this end,
instead of (A⌢⊤), 3c
l A is used to state that A holds at an interval that starts at
the same time point as the reference one, and is not necessarily its subinterval.
This is needed for reasoning from the viewpoint of intervals which accommodate
nested recursive executions. The meaning of triples (9) becomes
|= loc(V ) ∧⌈R ⇒N⌉0 ∧3c
l A ∧[[P]]R,N,V ⇒G.
(13)
In this setting, μX.P admits the proof rule, where X does not occur in A:
(μ)
loc(V ) ∧⌈R ⇒N⌉0 ∧3c
l A ∧G ⇒[3c
l A ∧X/X]G
{A}P{G}V
{A}μX.P{μX.G}V
This rule subsumes the one for while −do, but only as part of a suitably
revised variant of the whole proof system wrt (13). E.g., the rule for sequential
composition becomes
{A}P{G}V
{B}Q{H}V
loc(V ) ∧⌈R ⇒N⌉0 ∧3c
l A ⇒¬(G ∧ℓ< ∞⌢¬3c
l (⌈N ∧¬R⌉0⌢B))
{A}P; Q{(G⌢⌈N ∧¬R⌉0⌢H)}V
7
Related Work
Our work was inﬂuenced by the studies on DC-based reasoning about process-
algebraic speciﬁcation languages in [14,17,18,38]. In a previous paper we pro-
posed a calculus for HCSP [22], which was based on DC in a limited way and
lacked compositionality. In [12,37] we gave other variants of compositional and
sound calculi for HCSP with diﬀerent assertion and temporal condition formats.
Completeness was not considered. The approach in [12] and in this paper is
largely drawn from [10] where computation time was proposed to be treated as
negligible in order to simplify delay calculation, and the operator of projection
was proposed to facilitate writing requirements with negligible time ignored.

Compositional Hoare-Style Reasoning About Hybrid CSP
125
Hoare-style reasoning about real-time systems was also studied in the literature
with explicit time logical languages [21]. However, our view is that using temporal
logic languages is preferable. Dedicated temporal constructs both lead to more
readable speciﬁcations, and facilitate the identiﬁcation of classes of requirements
that can be subjected to automated analysis. Another approach to the veriﬁca-
tion of hybrid systems is Platzer’s Diﬀerential Dynamic Logic [32]. However, the
hybrid programs considered there have limited functionality. Communication,
parallelism and interrupts are not handled. For logic compositionality, assume-
guarantee reasoning has been studied for communication-based concurrency in
CSP without timing in [25,31].
Both in our work and in alternative approaches such as [25], the treatment
of continuous evolution is somewhat separated from the analysis of the other
basic process-algebraic constructs. Indeed we make a small step forward here
by fully expressing the meaning of diﬀerential law-governed evolution in DC,
which is theoretically suﬃcient to carry out all the relevant reasoning in the
logic. Of course, the feasibility of such an approach is nowhere close to the
state of art in the classical theory of ordinary diﬀerential equations. Indeed it
would typically lead to formalized accounts of classical reasoning. Techniques for
reasoning about the ODE-related requirements are the topic of separate studies,
see, e.g., [23,33,34].
8
Concluding Remarks
We have presented a weakly monotonic time-based semantics and a correspond-
ing Hoare style proof system for HCSP with both the semantics and the temporal
conditions in triples being in ﬁrst-order DC with inﬁnite intervals and extreme
ﬁxpoints. The proof system is compositional but the proof rule for parallel com-
position introduces complications because of the special form of the triples that
it derives. However, we have shown that HCSP equivalences that can serve as
elimination rules for ∥can also be used to derive proof rules for ∥which do
not bring the above diﬃculty and indeed are perfectly compatible with standard
bottom-up proof search. Interestingly, the informal reading of the derived rules
for ∥together with the ones which are inherited from CSP, does not require the
mention of weakly monotonic time technicalities. This means that the use of
this special semantics can be restricted to establishing the soundness of practi-
cally relevant proof systems and awareness of its intricacies is not essential for
applying the system. The meaning of triples we propose subsumes classical pre-
/postcondition Hoare triples and triples linking (hybrid) temporal conditions in
a streamlined way. This is a corollary of the choice to use assumptions which hold
at an arbitrary initial subintervals, which is also compatible with reasoning about
invariants A in terms of statements of the form (A⌢⊤) ⇒¬([[P]] ∧¬(A⌢⊤)).
Acknowledgements. Dimitar Guelev was partially supported through Bulgarian NSF
Contract DN02/15/19.12.2016. Shuling Wang and Naijun Zhan were partially supported
by NSFC under grants 61625206, by “973 Program” under grant No. 2014CB340701,

126
D.P. Guelev et al.
by CDZ project CAP (GZ 1023), and by the CAS/SAFEA International Partnership
Program for Creative Research Teams.
References
1. Abadi, M., Lamport, L.: Composing speciﬁcations. ACM Trans. Program. Lang.
Syst. 15(1), 73–132 (1993)
2. Allen, J.F.: Maintaining knowledge about temporal intervals. Commun. ACM
26(11), 832–843 (1983)
3. Alur, R., Courcoubetis, C., Henzinger, T.A., Ho, P.-H.: Hybrid automata: an algo-
rithmic approach to the speciﬁcation and veriﬁcation of hybrid systems. In: Gross-
man, R.L., Nerode, A., Ravn, A.P., Rischel, H. (eds.) HS 1991-1992. LNCS, vol.
736, pp. 209–229. Springer, Heidelberg (1993). doi:10.1007/3-540-57318-6 30
4. Zhou, C., Hansen, M.R.: Duration Calculus: A Formal Approach to Real-Time
Systems. EATCS. Springer, Heidelberg (2004). doi:10.1007/978-3-662-06784-0
5. Zhou, C., Hoare, C.A.R., Ravn, A.P.: A calculus of durations. Inf. Process. Lett.
40(5), 269–276 (1991)
6. Zhou, C., Dang, V.H., Li, X.: A duration calculus with inﬁnite intervals. In: Reichel,
H. (ed.) FCT 1995. LNCS, vol. 965, pp. 16–41. Springer, Heidelberg (1995). doi:10.
1007/3-540-60249-6 39
7. Cau, A., Moszkowski, B., Zedan, H.: ITL web pages. http://www.antonio-cau.co.
uk/ITL/
8. Dutertre, B.: On First-order Interval Temporal Logic. Report CSD-TR-94-3,
Department of Computer Science, Royal Holloway, University of London (1995)
9. Goranko, V., Montanari, A., Sciavicco, G.: A road map of interval temporal logics
and duration calculi. J. Appl. Non Classical Logics 14(1–2), 9–54 (2004)
10. Guelev, D.P., Hung, D.V.: Preﬁx and projection onto state in duration calculus.
In: Proceedings of TPTS 2002, ENTCS, vol. 65, no. 6. Elsevier Science (2002)
11. Guelev, D.P., Van Hung, D.: A relatively complete axiomatisation of projection
onto state in the duration calculus. J. Appl. Non Class. Logics 14(1–2), 151–182
(2004). Special Issue on Interval Temporal Logics and Duration Calculi
12. Guelev, D.P., Wang, S., Zhan, N., Zhou, C.: Super-dense computation in ver-
iﬁcation of hybrid CSP processes. In: Fiadeiro, J.L., Liu, Z., Xue, J. (eds.)
FACS 2013. LNCS, vol. 8348, pp. 13–22. Springer, Cham (2014). doi:10.1007/
978-3-319-07602-7 3
13. Wang, H., Xu, Q.: Completeness of temporal logics over inﬁnite intervals. Discr.
Appl. Math. 136(1), 87–103 (2004)
14. Zhu, H., He, J.: A DC-based semantics for verilog. Technical report 183,
UNU/IIST, P.O. Box 3058, Macau (2000)
15. Halpern, J.Y., Shoham, Y.: A propositional logic of time intervals. In: Proceedings
of LICS 1986, pp. 279–292. IEEE Computer Society Press (1986)
16. Hansen, M.R., Zhou, C.: Chopping a point. In: BCS-FACS 7th Reﬁnement Work-
shop, Electronic Workshops in Computing. Springer (1996)
17. Haxthausen, A.E., Yong, X.: Linking DC together with TRSL. In: Grieskamp, W.,
Santen, T., Stoddart, B. (eds.) IFM 2000. LNCS, vol. 1945, pp. 25–44. Springer,
Heidelberg (2000). doi:10.1007/3-540-40911-4 3
18. He, J., Xu, Q.: Advanced features of duration calculus and their applications in
sequential hybrid programs. Formal Asp. Comput. 15(1), 84–99 (2003)
19. He, J.: From CSP to hybrid systems. In: Roscoe, A.W. (ed.) A Classical Mind, pp.
171–189. Prentice Hall International (UK) Ltd., Hertfordshire (1994)

Compositional Hoare-Style Reasoning About Hybrid CSP
127
20. Henzinger, T.A.: The theory of hybrid automata. In: Proceedings of LICS 1996,
pp. 278–292. IEEE Computer Society Press (1996)
21. Hooman, J.: Extending Hoare logic to real-time. Formal Asp. Comput. 6(6A),
801–826 (1994)
22. Liu, J., Lv, J., Quan, Z., Zhan, N., Zhao, H., Zhou, C., Zou, L.: A calculus for
hybrid CSP. In: Ueda, K. (ed.) APLAS 2010. LNCS, vol. 6461, pp. 1–15. Springer,
Heidelberg (2010). doi:10.1007/978-3-642-17164-2 1
23. Liu, J., Zhan, N., Zhao, H.: Computing semi-algebraic invariants for polynomial
dynamical systems. In: Proceedings of EMSOFT 2011, pp. 97–106. ACM (2011)
24. Manna, Z., Pnueli, A.: Verifying hybrid systems. In: Grossman, R.L., Nerode, A.,
Ravn, A.P., Rischel, H. (eds.) HS 1991-1992. LNCS, vol. 736, pp. 4–35. Springer,
Heidelberg (1993). doi:10.1007/3-540-57318-6 22
25. Misra, J., Chandy, K.M.: Proofs of networks of processes. IEEE Trans. Software
Eng. 7(4), 417–426 (1981)
26. Moszkowski, B.: Temporal logic for multilevel reasoning about hardware. IEEE
Comput. 18(2), 10–19 (1985)
27. Moszkowski, B.: Executing Temporal Logic Programs. Cambridge University Press,
Cambridge (1986). http://www.cse.dmu.ac.uk/∼cau/papers/tempura-book.pdf
28. Olderog, E.-R., Hoare, C.A.R.: Speciﬁcation-oriented semantics for communicating
processes. In: Diaz, J. (ed.) ICALP 1983. LNCS, vol. 154, pp. 561–572. Springer,
Heidelberg (1983). doi:10.1007/BFb0036937
29. Pandya, P.K.: Some extensions to propositional mean-value calculus: expressive-
ness and decidability. In: Kleine B¨uning, H. (ed.) CSL 1995. LNCS, vol. 1092, pp.
434–451. Springer, Heidelberg (1996). doi:10.1007/3-540-61377-3 52
30. Pandya, P.K., Hung, D.: Duration calculus of weakly monotonic time. In: Ravn,
A.P., Rischel, H. (eds.) FTRTFT 1998. LNCS, vol. 1486, pp. 55–64. Springer,
Heidelberg (1998). doi:10.1007/BFb0055336
31. Pandya, P.K., Joseph, M.: P - a logic - a compositional proof system for distributed
programs. Distrib. Comput. 5, 37–54 (1991)
32. Platzer, A.: Diﬀerential dynamic logic for hybrid systems. J. Autom. Reasoning
41(2), 143–189 (2008)
33. Prajna, S., Jadbabaie, A.: Safety veriﬁcation of hybrid systems using barrier certiﬁ-
cates. In: Alur, R., Pappas, G.J. (eds.) HSCC 2004. LNCS, vol. 2993, pp. 477–492.
Springer, Heidelberg (2004). doi:10.1007/978-3-540-24743-2 32
34. Sankaranarayanan, S., Sipma, H.B., Manna, Z.: Constructing invariants for hybrid
systems. In: Alur, R., Pappas, G.J. (eds.) HSCC 2004. LNCS, vol. 2993, pp. 539–
554. Springer, Heidelberg (2004). doi:10.1007/978-3-540-24743-2 36
35. Venema, Y.: A modal logic for chopping intervals. J. Logic Comput. 1(4), 453–476
(1991)
36. Venema, Y.: Many-dimensional modal logics. Ph.D. thesis, University of Amster-
dam (1991)
37. Wang, S., Zhan, N., Guelev, D.: An assume/guarantee based compositional cal-
culus for hybrid CSP. In: Agrawal, M., Cooper, S.B., Li, A. (eds.) TAMC
2012. LNCS, vol. 7287, pp. 72–83. Springer, Heidelberg (2012). doi:10.1007/
978-3-642-29952-0 13
38. Yong, X., George, C.: An operational semantics for timed RAISE. In: Wing, J.M.,
Woodcock, J., Davies, J. (eds.) FM 1999. LNCS, vol. 1709, pp. 1008–1027. Springer,
Heidelberg (1999). doi:10.1007/3-540-48118-4 4
39. Zhou, C., Wang, J., Ravn, A.P.: A formal description of hybrid systems. In: Alur,
R., Henzinger, T.A., Sontag, E.D. (eds.) HS 1995. LNCS, vol. 1066, pp. 511–530.
Springer, Heidelberg (1996). doi:10.1007/BFb0020972

Program Analysis

Termination of Semi-algebraic Loop Programs
Yi Li(B)
Chongqing Key Laboratory of Automated Reasoning and Cognition,
CIGIT, CAS, Chongqing, China
zm liyi@163.com
Abstract. Program termination is a fundamental research topic in pro-
gram analysis. In this paper, we investigate the termination of a class of
semi-algebraic loop programs. We relate the termination of such a loop
program to a certain semi-algebraic system. Also, we show that under
some conditions, such a loop program does not terminate over the reals
if and only if its corresponding semi-algebraic system has a real solution.
1
Introduction
Termination analysis of loop programs is very important for software correct-
ness. A standard technique to prove the termination of a loop is to ﬁnd a rank-
ing function, which maps a program state into an element of some well-founded
ordered set, such that the value descends whenever the loop completes an iter-
ation. Several methods for synthesizing polynomial ranking functions have been
presented in [1,3–6,9–13,15–17,20]. Additionally, the complexity of the linear
ranking function problem for linear-constraint loops is discussed in [2,4,5].
It is well known that the termination of loop programs is undecidable, even
for the class of linear programs. In [21], Tiwari proved that the termination of
a class of single-path loops with linear guards and assignments is decidable over
the reals. The termination of this kind of linear loop program was reconsidered in
[18,23]. Braverman [8] generalized the work of Tiwari, and showed that termina-
tion of a simple of class linear loops over the integers is decidable. Xia et al. [22]
gave the Non-Zero Minimum condition under which the termination problem of
loops with linear updates and nonlinear polynomial loop conditions is decidable
over the reals. In addition, there are some other methods for determining ter-
mination problem of loop programs. For instance, in [7] Bradley et al. applied
ﬁnite diﬀerence trees to prove termination of multipath loops with polynomial
guards and assignments. In [19], Zhan et al. analyzed the termination problems
for multi-path polynomial programs with equational loop guards and established
suﬃcient conditions for termination and nontermination.
In this paper, we investigate the termination of loop programs of the form
P : while C(x) > 0 do
{F(x, x′) ≥0}
endwhile
(1)
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 131–146, 2017.
https://doi.org/10.1007/978-3-319-69483-2_8

132
Y. Li
where x, x′ ∈Rn, C(x) = (c1(x), ..., cs(x)) ∈(R[x])s and F(x, x′) = (f1(x, x′),
..., fm(x, x′)) ∈(R[x])m. And ci’s and fj’s are all homogeneous polynomials.
Given a polynomial h(x) in x, we say h(x) is a homogeneous polynomial, if nonzero
terms of h(x) all have the same degree. Let E(x, x′) = (C(x)T , F(x, x′)T ). Let
S = {(x, x′) ∈R2n : E(x, x′) ▷0} where ▷= {>, . . . , >



s times
, ≥, . . . , ≥



m times
}.
Diﬀerent from a program whose loop body is an assignment not an inequal-
ity, the program deﬁned as above has nondeterministic update statements. Such
inequalities in loop body of Program P may arise due to abstraction. We take an
example from [20] to illustrate this. Consider the loop while(i−j ≥1)do (i, j) :=
(i −Nat, j + Pos)od, where i, j are integer variables, Nat and Pos stand for
any nonnegative and positive integer number respectively. It is easy to see that
the update statements i := i −Nat, j := j + Pos can be expressed by the
inequalities i′ ≤i, j′ ≥j + 1. Thus, the above loop can be abstracted as
while(i −j ≥1)do {i′ ≤i, j′ ≥j + 1}od. More examples about programs
with nondeterministic update statements can be found in [1,4–6,13].
For convenience, we say that Program P is deﬁned by E(x, x′)▷0. And let P ≜
P(E(x, x′)). Moreover, since E(x, x′) ▷0 is a semi-algebraic system and all alge-
braic expressions of it are homogeneous, Program P is called homogeneous semi-
algebraic loop program. The notion of semi-algebraic programs is not new, which
ﬁrst appears in [13]. Especially, if all the expressions in E(x, x′) ▷0 are linear,
then we call E(x, x′) ▷0 the homogeneous linear semi-algebraic system, denoted
E(x, x′)Lin ▷0, and the program deﬁned by E(x, x′)Lin ▷0 is called homogeneous
linear semi-algebraic program. At the same time, let P be the nonhomogeneous
semi-algebraic program, i.e., P contains at least one nonhomogeneous expression.
For nonhomogeneous linear semi-algebraic program P, its termination has been
widely studied in [1,4–6,11,17] by synthesizing linear polynomial ranking func-
tions. For nonhomogeneous nonlinear semi-algebraic program P, [13] presented
an algorithm to computing non-linear polynomial ranking functions of P by semi-
deﬁnite programming solving. In the above works, the synthesized ranking func-
tions are all polynomial ranking functions. The reason is that polynomials can be
dealt with more eﬃciently and more conveniently in many computer algebra sys-
tems. It is well known that the existence of ranking functions of a program exactly
implies the program must terminate. However, for a given program, even if it has
ranking functions, its ranking functions are not necessarily polynomials. It is easy
to construct an example of a program which terminates but has no polynomial
ranking functions. For example, consider the loop while(x > 0)do {x′ = −x; }od.
It is easy to see that the loop is terminating, but has no polynomial ranking func-
tions. (Suppose that there is a polynomial ranking function ρ(x) = m
i=0 aixi
for the loop. By the ranking conditions of ranking functions in Deﬁnition 3, we
have ∀x, x > 0 ⇒ρ(x) −ρ(x′) = ρ(x) −ρ(−x) = m
i=0 ai(xi −x′i) =
m
i=0 ai(x −x′)(i−1
j=0 xjx′i−1−j) = m
i=0 ai(2x)(i−1
j=0 xj(−x)i−1−j) ≥1. How-
ever, the above formula cannot hold, since as x →0, we have ρ(x)−ρ(x′) →0  1.

Termination of Semi-algebraic Loop Programs
133
That is a contradiction. Therefore, the loop has no polynomial ranking functions.)
Therefore, for the homogeneous semi-algebraic program P, the above methods
based on synthesizing polynomial ranking functions may fail to check if P termi-
nates. In this paper, we mainly consider the termination of P. This is because,
Program P always can be equivalently converted to a homogeneous program such
as P by introducing two additional variables z and z′. This will be further analyzed
in Sect. 4.
The rest of the paper is organized as follows. In Sect. 2, we introduce some
basic notions regarding on semi-algebraic systems, ranking functions. In Sect. 3,
we establish some conditions for Program P such that under such conditions,
checking if Program P does not terminate is equivalent to checking if a certain
semi-algebraic system has a real solution. In other words, if such semi-algebraic
system has a real solution, then Program P is non-terminating over the reals.
Otherwise, a ranking function, which indicates Program P is terminating over
the reals, can be constructed. In Sect. 4, we show that the termination of a
nonhomogeneous semi-algebraic program can always be equivalently reduced to
that of a homogeneous one obtained by introducing additional program variables.
Section 5 concludes the paper.
2
Preliminaries
In the section, some basic deﬁnitions on ranking functions, semi-algebraic sys-
tems will be introduced.
Deﬁnition 1 (Nontermination). Given the homogeneous semi-algebraic pro-
gram P deﬁned as before, we say that Program P is non-terminating over the
reals, if there exists an inﬁnite sequence {xi}+∞
i=0 ⊆Rn such that E(xi, xi+1) ▷0
for any i ≥0. And the ﬁrst element x0 in such inﬁnity sequence is called a
non-terminating point (or non-terminating input). Especially, we say x0 is a
terminating point (or terminating input), if x0 is not a non-terminating point.
Note that the deﬁnition of nontermination of Program P can also be equiva-
lently restated as: Program P is non-terminating over the reals, if there exists an
inﬁnite sequence {(xi, xi+1)}+∞
i=0 ⊆R2n such that E(xi, xi+1) ▷0 for any i ≥0.
If such the inﬁnity sequence {xi}+∞
i=0 (or {(xi, xi+1)}+∞
i=0 ) does not exist, then we
say Program P is terminating over the reals. Moreover, the deﬁnition of non-
termination of P can be easily extended to the nonhomogeneous semi-algebraic
program P.
Let R be the ﬁeld of real numbers. A semi-algebraic system is a set of
equations, inequations and inequalities given by polynomials. And the coeﬃ-
cients of those polynomials are all real numbers. Let v = (v1, ..., vd)T ∈Rd,
x = (x1, ..., xn)T ∈Rn. Next, we give the deﬁnition of semi-algebraic systems
(SASs for short).

134
Y. Li
Deﬁnition 2 (Semi-algebraic systems). A semi-algebraic system is a conjunc-
tive polynomial formula of the form
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
p1(v, x) = 0, ..., pr(v, x) = 0,
g1(v, x) ≥0, ..., gk(v, x) ≥0,
gk+1(v, x) > 0, ..., gt(v, x) > 0,
h1(v, x) ̸= 0, ..., hm(v, x) ̸= 0,
(2)
where r ≥1, t ≥k ≥0, m ≥0 and all pi’s, gi’s and hi’s are polynomials in
R[v, x] \ R. An semi-algebraic system is called parametric if d ̸= 0, otherwise
constant, where d is the dimension of v.
We next recall the deﬁnition of ranking functions.
Deﬁnition 3 (Ranking functions). Given Program P deﬁned as before, we say
ρ(x) is a ranking function for P, if the following formula is true over the reals,
∀x, x′.

E(x, x′) ▷0 ⇒ρ(x) ≥0 ∧ρ(x) −ρ(x′) ≥1

.
(3)
It is well known that the existence of ranking functions for P implies that Pro-
gram P is terminating. For convenience, the function ρ satisfying Formula (3) is
also called a ranking function over the set S = {(x, x′) : E(x, x′) ▷0}.
3
Termination of Homogeneous Semi-algebraic Programs
In the section, we consider the termination of the homogeneous semi-algebraic
program P as deﬁned in (1). We will give some conditions under which Program
P is nonterminating over the reals if and only if a certain semi-algebraic system
has real solutions. Some useful lemmas will be introduced ﬁrst.
Lemma 1 [14]. If H : Rk →Rd is continuous and S ⊆Rk is bounded and
closed, then the image H(S) of S under the continuous mapping H is bounded
and closed too.
Lemma 2 [14]. If H : Rk →Rd is continuous and S ⊆Rk is connected, then
the image H(S) of S under the continuous mapping H is connected too.
Given the homogeneous semi-algebraic Program P ≜P(E(x, x′)) deﬁned
as in (1), let S = {(x, x′) ∈R2n : E(x, x′) ▷0} and ¯S = {(x, x′) ∈R2n :
E(x, x′) ≥0}. Clearly, S ⊆¯S. Since the algebraic expressions in E(x, x′) are all
homogeneous polynomials, the set ¯S is a subset of R2n formed by rays starting
from the origin. Therefore, ¯S is a closed cone.
Let U(x, x′) =
x′
|x′| −
x
|x|. Let U( ¯S \ {(0, 0)}) = {U(x, x′) :
(x, x′) ∈¯S \
{(0, 0)}} = {u ∈Rn : u =
x′
|x′| −
x
|x| ∧(x, x′) ∈¯S \ {(0, 0)}}. Deﬁne
Hλ(x, x′) = x′ −λx, (λ > 0).

Termination of Semi-algebraic Loop Programs
135
Clearly, for any ﬁxed positive number λ∗, Hλ∗(x, x′) is a linear mapping from
R2n to Rn. Let Hλ( ¯S) = {Hλ(x, x′) : (x, x′) ∈¯S} = {u ∈Rn : u = x′ −λx ∧
(x, x′) ∈¯S}. and let T = 
λ>0 Hλ( ¯S). It is not diﬃcult to see that
T = {u ∈Rn : u = x′ −λx ∧λ > 0 ∧(x, x′) ∈¯S}.
Proposition 1. With the above notion. Suppose that the function U(x, x′) is
continuous on ¯S \ {(0, 0)}. We have U( ¯S \ {(0, 0)}) ⊆T .
Proof. Because U(x, x′) is continuous on ¯S\{(0, 0)}, for any (x, x′) ∈¯S\{(0, 0)},
we have |x′| ̸= 0 ∧|x| ̸= 0. Take arbitrarily an element u from U( ¯S \ {(0, 0)}).
By the deﬁnition of U( ¯S \ {(0, 0)}), there must exist (x, x′) ∈¯S \ {(0, 0)} such
that u = U(x, x′) =
x′
|x′| −
x
|x|. And
u = x′
|x′| −x
|x| =
 x′
|x′|

−|x′|
|x| ·
 x
|x′|

.
Set λ∗= |x′|
|x| > 0. It is easy to see that the point ( x
|x′|, x′
|x′|) ∈¯S, since ¯S is a cone
whose apex is the origin and the point (x, x′) ∈¯S ∧x′ ̸= 0 ∧x ̸= 0. Therefore,
by the deﬁnition of Hλ( ¯S), we get u ∈Hλ∗( ¯S) ⊆T , since ( x
|x′|, x′
|x′|) ∈¯S and
λ∗> 0.
□
Remark 1. Notice that T can be obtained by removing the quantiﬁer preﬁx from
the given formula,
∃x∃x′∃λ.(λ > 0 ∧E(x, x′) ≥0 ∧u = x′ −λx).
Now, let us construct the semi-algebraic system as follows.
sysm ≜x′ = λx ∧λ > 0 ∧x′ ̸= 0 ∧x ̸= 0 ∧E(x, x′)▷0.
(4)
Let Tx=0 = {(x, x′) ∈R2n :
x = 0, x′ ̸= 0} and let Tx′=0 = {(x, x′) ∈
R2n : x ̸= 0, x′ = 0}. Let T̸= = {(x, x′) ∈R2n : x ̸= 0, x′ ̸= 0}. Obviously,
R2n = Tx=0∪Tx′=0∪T̸=∪{(0, 0)}. Denote by ∂( ¯S) the set of all boundary points
of ¯S.
Lemma 3. Given Program P deﬁned as in (1), let ⊙= {(x, x′) ∈R2n :
|(x, x′)| = 1} be the unit sphere. If ¯S \ {(0, 0)} is connected, then ⊙∩¯S \ {(0, 0)}
is a connected subset of R2n.
Proof. Given any two points ˆz0 = (ˆx, ˆx′),ˆz1 = (ˆy, ˆy′) ∈¯S \ (0, 0) ∩⊙. Since ¯S is
a cone of R2n and ˆz0,ˆz1 ∈¯S, we have λˆz0, λˆz1 ⊆¯S for any λ > 0. That is, there
must exist two rays ℓ
oˆz0, ℓ
oˆz1 in ¯S, which start from the origin O and pass through
ˆz0,ˆz1, respectively. Obviously, the rays ℓ
oˆz0, ℓ
oˆz1 intersect with the unit sphere ⊙
at ˆz0 and ˆz1, respectively. Take arbitrarily two nonzero points z0, z1 from the rays
ℓ
oˆz0, ℓ
oˆz1, respectively. Since z0, z1 ∈¯S \ {(0, 0)} and ¯S \ {(0, 0)} is connected,
there exists a path L(z0, z1) ⊆¯S \ (0, 0) from z0 to z1. Let z ∈L(z0, z1).

136
Y. Li
Clearly, z ̸= (0, 0) and for any z ∈L(z0, z1), the ray ℓ
oz ⊆¯S. Denote by ˆz
the intersection point of the ray ℓ
oz and ⊙. When z continuously moves from
z0 to z1 along the path L(z0, z1), the ray ℓ
oz also continuously moves from
ℓ
oz0 to ℓ
oz1. Obviously, the set of all the intersection points ˆz of the ray ℓ
oz
and ⊙forms a path ˆL(ˆz0,ˆz1) ⊆⊙∩( ¯S \ {(0, 0)}) connecting ˆz0 and ˆz1. Since
ˆz0,ˆz1 are taken arbitrarily from ( ¯S \ {(0, 0)}) ∩⊙, we have for any two points
ˆz0,ˆz1 ∈⊙∩( ¯S \ {(0, 0)}), there must exist a path ˆL(ˆz0,ˆz1) in ⊙∩( ¯S \ {(0, 0)}),
which connects ˆz0 and ˆz1. Hence, ⊙∩( ¯S \ {(0, 0)}) is connected.
□
Remark 2. This lemma indicates that if the set ¯S \ {(0, 0)} is connected, then
the intersection of ¯S and the unit sphere ⊙is also connected. This result will be
used in the proof of Theorem 1.
Next, we will give our main result about the termination of Program P.
Theorem 1. Let Program P be as in (1), and let the following conditions hold:
(A) ¯S  (Tx=0
 Tx′=0) = ∅, i.e., ¯S ⊆T̸= ∪{(0, 0)},
(B) the formula
x′ = λx ∧λ > 0 ∧x′ ̸= 0 ∧x ̸= 0 ∧(x, x′) ∈∂( ¯S)
(5)
has no real solutions,
(C) ¯S \ {(0, 0)} is a connected set,
(D) there exist a closed convex polyhedral cone C = {u ∈Rn : Au ≥0} and a
hyperplane L = {u : L(u) = aT u = 0}, such that U( ¯S \ {(0, 0)}) ⊆C and
C ∩L = {0}.
Then P is non-terminating over the reals iﬀ(4) has a real solution.
Proof. (I) Suppose that the semi-algebraic system (4) has real solutions. That
is, there exist (ˆx, ˆx′) ∈R2n and λ ∈R, such that
ˆx′ = λˆx ∧λ > 0 ∧ˆx′ ̸= 0 ∧ˆx ̸= 0 ∧E(ˆx, ˆx′) ▷0
Then, we can construct the following inﬁnite sequence,
x0 = ˆx, x1 = λx0 = ˆx′, x2 = λ2x0, ..., xn = λnx0, ....
It is easy to see that
(xn, xn+1) = (λnx0, λn+1x0) = λn(x0, λx0) = λn(ˆx, ˆx′).
It immediately follows that E(xn, xn+1) ▷0 for any nonnegative integer n ≥0,
since E(ˆx, ˆx′) ▷0, λ > 0 and the polynomials in E(x, x′) are all homogeneous
polynomials in x, x′. Therefore, by the deﬁnition of nontermination of Program
P stated in Sect. 1, Program P is non-terminating over the reals and x = ˆx is a
nonterminating point for P.

Termination of Semi-algebraic Loop Programs
137
(II) Let πx : R2n →Rn be a projection mapping. Deﬁne πx( ¯S) = {x ∈Rn :
(x, x′) ∈¯S}. The remaining task is to claim that if the four hypotheses (A),
(B), (C), (D) of the theorem are all satisﬁed, then the semi-algebraic system (4)
has no real solutions implies that Program P is terminating over the reals. Our
approach to prove this is through the construction of a ranking functions over
¯S \ {(0, 0)}.
First, by the hypothesis (A), we get that ¯S \ {(0, 0)} ⊆T̸=. Let ¯S∗= ¯S \
{(0, 0)}. Clearly, ¯S∗consists of all the rays with the exclusion of the origin
(0, 0), lying in the cone ¯S. Therefore, for any (x, x′) ∈¯S∗and any λ > 0, we
have λ(x, x′) ∈¯S∗. Furthermore, by the hypothesis (B), we know that (5) has
no real solutions is equivalent to the quantiﬁed formula below holds,
∀x, x′, λ.(λ > 0 ∧x′ ̸= 0 ∧x ̸= 0 ∧(x, x′) ∈∂( ¯S) ⇒x′ ̸= λx).
This further indicates that the formula
∀x, x′, λ.(λ > 0 ∧(x, x′) ∈∂( ¯S) \ {(0, 0)} ⇒x′ ̸= λx)
(6)
is true, since ¯S = {(0, 0)} ∪¯S∗and ¯S∗⊆T̸= by the hypothesis (A). At the same
time, as the semi-algebraic system (4) has no real solutions, we have the formula
below
∀x, x′, λ.(λ > 0 ∧(x, x′) ∈S ⇒x′ ̸= λx),
(7)
is true, where S = {(x, x′) ∈R2n : E(x, x′) ▷0}. Since ¯S∗∪{(0, 0)} = ¯S =
S ∪∂¯S = S ∪(∂( ¯S)\{(0, 0)})∪{(0, 0)}, it follows that S ∪(∂( ¯S)\{(0, 0)}) = ¯S∗.
Therefore, by (6) and (7), we get that the following quantiﬁed formula
∀x, x′, λ.(λ > 0 ∧(x, x′) ∈¯S∗⇒x′ ̸= λx)
(8)
is true. Next, we will construct a ranking function ρ(x) over ¯S∗such that ρ(x)
satisﬁes the following formulas
∀x, x′.

(x, x′) ∈¯S∗⇒ρ(x) ≥0

,
∀x, x′.

(x, x′) ∈¯S∗⇒ρ(x) −ρ(x′) ≥1

.
(9)
Let U(x, x′) =
x′
|x′| −
x
|x|. Since ¯S∗⊆T̸=, U(x, x′) : R2n →Rn is a continuous
mapping over ¯S∗. Let U( ¯S∗) =

u ∈Rn :
u =
x′
|x′| −
x
|x| ∧(x, x′) ∈¯S∗
.
By Formula (8), we get 0 ̸∈U( ¯S∗).

This is because, if 0 ∈U( ¯S∗), then by the
deﬁnition of U( ¯S∗), there exists (x, x′) ∈¯S∗such that 0 =
x′
|x′| −x
|x|. This clearly
implies that there exists a positive number λ = |x′|
|x| , such that x′ = λx, which
contradicts with Formula (8)

. Hence, for any (x, x′) ∈¯S∗,
0 < |U(x, x′)| ≤

x′
|x′|
 +

x
|x|
 ≤2.
(10)
Therefore, the image U( ¯S∗) of ¯S∗under the mapping U(x, x′) is bounded. Addi-
tionally, for any (x, x′) ∈T̸= and any λ > 0, we have u =
x′
|x′| −x
|x| =
λx′
|λx′| −λx
|λx|.

138
Y. Li
This indicates that the images of any two points (x, x′), (λx, λx′) on each ray
of ¯S∗under the mapping U(x, x′) are the same. In other words, for any two
points (x, x′), (y, y′) ∈¯S∗, if (y, y′) = λ(x, x′) for a certain λ > 0, then
U(x, x′) = U(y, y′). This enables us to take (y, y′) =
1
|(x,x′)|(x, x′) for any
point (x, x′) ∈¯S∗, since U(y, y′) ≡U(x, x′). It is easy to see that |(y, y′)| = 1,
i.e., (y, y′) ∈⊙. With the above arguments, for any point (x, x′) ∈¯S∗, taking
λ =
1
|(x,x′)|, we have
U( ¯S∗) = {u ∈Rn : u = x′
|x′| −x
|x| ∧(x, x′) ∈¯S∗}
= {u ∈Rn : u = λx′
|λx′| −λx
|λx| ∧(λx, λx′) ∈¯S∗∧|λ(x, x′)| = 1}
= {u ∈Rn : u = y′
|y′| −y
|y| ∧(y, y′) ∈¯S∗∧|(y, y′)| = 1}
= {u ∈Rn : u = y′
|y′| −y
|y| ∧(y, y′) ∈¯S ∧|(y, y′)| = 1}
= U( ¯S∗∩⊙) = U( ¯S ∩⊙).
(11)
The last equality is guaranteed by ¯S ∩⊙= ( ¯S∗∪{(0, 0)}) ∩⊙= ¯S∗∩⊙. Since
both ¯S and ⊙are closed sets, ¯S ∩⊙is a closed set. This implies ¯S∗∩⊙is
also a closed set. By the hypothesis (C), Lemma 3 and the above statements,
we get that ¯S∗∩⊙is a bounded, closed and connected set. Since U(x, x′) is
continuous over ¯S∗, according to Lemmas 1 and 2, U( ¯S∗∩⊙) is also a bounded,
closed and connected set, since ¯S∗∩⊙a bounded, closed and connected set.
This clearly indicates that U( ¯S∗) is also a bounded, closed and connected set,
since U( ¯S∗) = U( ¯S∗∩⊙) by Formula (11). Because 0 ̸∈U( ¯S∗), we get that
0 ̸∈U( ¯S∗∩⊙).
By Formula (10), since 0 < |U(x, x′)| ≤2 for any (x, x′) ∈¯S∗, it follows
that 0 < |U(x, x′)| ≤2 for any (x, x′) ∈¯S∗∩⊙. Since U( ¯S∗∩⊙) is bounded,
closed and connected and | ◦| : Rn →R is a continuous mapping, by properties
of continuous functions, we get that there exists a positive number c, such that
0 < c ≤|U(x, x′)| ≤2,
(12)
for any (x, x′) ∈¯S∗∩⊙.
By the hypothesis (D), we know that there exists a closed polyhedral cone C
and a hyperplane L such that U( ¯S∗) ⊆C and C ∩L = {0}. Hence, U( ¯S∗∩⊙) ⊆
C. Since 0 ̸∈U( ¯S∗∩⊙), U( ¯S∗∩⊙) ⊆C and C ∩L = {0}, we get that {u ∈Rn :
L(u) = 0} ∩U( ¯S∗∩⊙) = ∅. That is, for any u ∈U( ¯S∗∩⊙), L(u) = aT u ̸= 0.
Also, by Formula (11), since U( ¯S∗) = U( ¯S∗∩⊙), we have L(u) = aT u ̸= 0, for
any u ∈U( ¯S∗). Furthermore, since U( ¯S∗) is a bounded, closed and connected
set and L(u) = aT u ̸= 0, for any u ∈U( ¯S∗), there will be two cases to consider.
Case 1. For any u ∈U( ¯S∗), L(u) = aT u < 0.
Case 2. For any u ∈U( ¯S∗), L(u) = aT u > 0.
(i) Consider Case 1. Since U( ¯S∗) is a bounded, closed and connected set
and the continuous function L(u) = aT u < 0 for any u ∈U( ¯S∗), by properties

Termination of Semi-algebraic Loop Programs
139
of continuous functions, we have that there exists a positive number c1 > 0, such
that L(u) = aT u ≤−c1 < 0 for any u ∈U( ¯S∗). Therefore, by the deﬁnition of
U( ¯S∗), we obtain that for any (x, x′) ∈¯S∗,
aT ( x′
|x′| −x
|x|) ≤−c1 < 0,
(13)
that is,
aT
c1
( x
|x| −x′
|x′|) ≥1 > 0,
(14)
for all (x, x′) ∈¯S∗. Let μ(x) =
x
|x| and μ( ¯S∗) = {u ∈Rn :
u = μ(x) ∧
(x, x′) ∈¯S∗}. Obviously, μ(x) is a continuous function over πx( ¯S∗) (or ¯S∗) by
the hypothesis (A). And for any (x, x′) ∈¯S∗, we have |μ(x)| ≡|μ(πx(x, x′))| = 1.
This implies that for any u ∈μ( ¯S∗), |u| = 1. Hence, the set μ( ¯S∗) is bounded.
Similar to Formula (11), for any point (x, x′) ∈¯S∗, taking λ =
1
|(x,x′)| and
setting (y, y′) = λ(x, x′), we get
μ( ¯S∗) = {u ∈Rn : u = x
|x| ∧(x, x′) ∈¯S∗}
= {u ∈Rn : u = λx
|λx| ∧(λx, λx′) ∈¯S∗∧|λ(x, x′)| = 1}
= {u ∈Rn : u = y
|y| ∧(y, y′) ∈¯S∗∧|(y, y′)| = 1}
= {u ∈Rn : u = y
|y| ∧(y, y′) ∈¯S ∧|(y, y′)| = 1}
= μ( ¯S∗∩⊙) = μ( ¯S ∩⊙).
(15)
Since ¯S∗∩⊙is a bounded and closed set and μ(x) is continuous on ¯S∗, μ( ¯S∗)
is bounded and closed. Therefore, the linear function Z(u) = aT
c1 u has the min-
imum value over μ( ¯S∗), i.e., there exists c2 ∈R, such that Z(u) = aT
c1 u ≥c2 for
all u ∈μ( ¯S∗). By the deﬁnition of μ( ¯S∗), we obtain that for all (x, x′) ∈¯S∗,
aT
c1
· x
|x| −c2 ≥0.
(16)
Let ρ(x) = aT
c1 · x
|x| −c2. By the deﬁnition of ranking function in Deﬁnition 3, and
Formula (14)and (16), ρ(x) is a ranking function over ¯S∗. And it immediately
follows that ρ(x) is also a ranking function over S, since S ⊊¯S∗. This implies
that Program P is terminating.
(ii) Consider Case 2: for any u ∈U( ¯S∗), L(u) = aT u > 0. Since U( ¯S∗) is a
bounded, closed and connected set and the continuous function L(u) = aT u > 0
for any u ∈U( ¯S∗), by properties of continuous functions, we have that there
exists a positive number d1 > 0, such that L(u) = aT u ≥d1 > 0 for any
u ∈U( ¯S∗). By the deﬁnition of U( ¯S∗), we obtain that for any (x, x′) ∈¯S∗,
aT ( x′
|x′| −x
|x|) ≥d1 > 0,
(17)

140
Y. Li
that is,
−aT
d1
( x
|x| −x′
|x′|) ≥1 > 0.
(18)
Similar to the analysis in Case 1, we deﬁne μ(x) =
x
|x| and μ( ¯S∗) = {u ∈Rn :
u = μ(x) ∧(x, x′) ∈¯S∗}. Let Z(u) =
−aT
d1 u. Since ¯S∗is a bounded, closed
and connected set and Z(u) is a continuous function over ¯S∗, there must exist
d2 ∈R, such that Z(x) = −aT
d1 u ≥d2 for all u ∈μ( ¯S∗). By the deﬁnition of
μ( ¯S∗), we have that for all (x, x′) ∈¯S∗,
−aT
d1
· x
|x| −d2 ≥0.
(19)
Let ρ(x) = −aT
d1
·
x
|x| −d2. By the deﬁnition of ranking functions, (18) and (19)
ρ(x) is a ranking function over ¯S∗. Also, ρ(x) is a ranking function over S, since
S ⊊¯S∗. This clearly implies that Program P is terminating.
By the arguments presented in (I) and (II), we get that if the four hypothe-
ses (A), (B), (C) and (D) are all satisﬁed, then the semi-algebraic system (4)
has no real solutions implies that Program P is terminating over the reals,
since a ranking function ρ(x) over S can always be constructed under such
hypotheses.
□
Especially, if E(x, x′) ▷0 is a linear semi-algebraic system, i.e.,
E(x, x′) ▷0 = ELin(x, x′) ▷0 = (C(x)T > 0, F(x, x′)T ≥0)T ,
C(x) ≜ACx,
F(x, x′) ≜AF x + A′
F x′,
(20)
where AC ∈Rs×n, AF , A′
F ∈Rm×n, then the conditions in Theorem 1 can be
further relaxed, as follows.
Theorem 2. Given Program P deﬁned by ELin(x, x′), if the following condi-
tions
(A) ¯S  (Tx=0
 Tx′=0) = ∅,
(B) the semi-algebraic system {x′ = λx∧λ > 0∧x′ ̸= 0∧x ̸= 0∧(x, x′) ∈∂( ¯S)}
has no real solutions,
(C) there exist a closed convex polyhedral cone C = {u ∈Rn : AuT ≥0} and
a hyperplane L = {u :
L(u) = 0}, such that U( ¯S \ {(0, 0)}) ⊆C and
C ∩L = {0},
are satisﬁed, then, Program P is non-terminating over the reals if and only if
the semi-algebraic system (4) has real solutions.
Proof. Let B =
AC
AF

and B′ =
 0
A′
F

. Then, ELin(x, x′)▷0 ≜Bx+B′x′▷0.
The proof of the theorem is completely similar to that of Theorem 1. We just need
to note that when E(x, x′) = ELin(x, x′), ¯S = {(x, x′) ∈R2n : ELin(x, x′)▷0} =

Termination of Semi-algebraic Loop Programs
141
{(x, x′) ∈R2n : Bx + B′x′ ▷0} is a convex polyhedral cone with vertex at the
origin. It is very easy to see that ¯S\{(0, 0)} is still a connected set, since ¯S\{(0, 0)}
is a convex set. Therefore, the hypothesis (C) in Theorem 1 is naturally satisﬁed,
when E(x, x′) = ELin(x, x′).
□
We now take the following extremely simple example to illustrate our method.
Example 1. Consider the homogeneous semi-algebraic loop below
while x1 > 0 ∧x1 −x2 > 0 do
{−x1 + 4x′
2 ≥0, x′
1 + 4x2 + 7x1 −x′
2 ≥0, x′
1 + x′
2 ≥0, −x′
1 ≥0}
(21)
Let E(x1, x2, x′
1, x′
2) = (x1, x1 −x2, −x1 +4x′
2, x′1 +4x2 +7x1 −x′
2, x′
1 +x′
2, −x′
1)
and S = {(x1, x2, x′
1, x′
2) ∈R4 : E(x1, x2, x′
1, x′
2) ▷0}, where ▷= (>, >, ≥, ≥,
≥, ≥). Since E(x1, x2, x′
1, x′
2) ▷0 is linear semi-algebraic system, we next apply
Theorem 2 to determine if the program terminates. Let ¯S = {(x1, x2, x′
1, x′
2) ∈
R4 : E(x1, x2, x′
1, x′
2) ≥0}.
Step 1. To check if the hypothesis (A) holds. To check if ¯S (Tx=0

Tx′=0) = ∅is equivalent to check if ¯S  Tx=0 = ∅and ¯S  Tx′=0 = ∅.
This is equivalent to check if the two semi-algebraic systems ¯Sx=0 and ¯Sx′=0
have only zero solution on x′ and x, respectively. Where ¯Sx=0(resp. ¯Sx′=0)
is obtained by substituting x = 0 (resp. x′ = 0) into ¯S. In the example,
¯Sx=0 = {(x′
1, x′
2) ∈R2 :
4x′
2 ≥0, x′
1 −x′
2 ≥0, x′
1 + x′
2 ≥0, −x′
1 ≥0},
¯Sx′=0 = {(x1, x2) ∈R2 :
x1 ≥0, x1 −x2 ≥0, −x1 ≥0, 4x2 + 7x1 ≥0}.
By veriﬁcation, we ﬁnd that ¯Sx=0 = {0} and ¯Sx′=0 = {0}. Thus, the hypothesis
(A) is satisﬁed.
Step 2. To check if the hypothesis (B) holds, we may check if the following
formula
{x′ = λx ∧λ > 0 ∧x′ ̸= 0 ∧x ̸= 0 ∧(x, x′) ∈¯S}
(22)
has no real solutions, since ∂( ¯S) ⊆¯S. By the tool RegularChains, we get that
Formula (22) indeed has no real solutions. Hence, the hypothesis (B) is met.
Step 3. Check if the hypothesis (C) holds is to checking if there exist a
close convex polyhedral cone C and a hyperplane L, such that U( ¯S∗) ⊆C and
C ∩L = {0}. To do this, we ﬁrst predeﬁne the polyhedral C ≜Au ≥0 and
the hyperplane L ≜aT u = 0. Where A = (aij)n×n is a parametric matrix and
aT = (a1, ..., an) is a parametric vector. Therefore, in the example, to check if the
hypothesis (C) holds is equivalent to check if there exist a11, a12, a21, a22, a1, a2,
such that the following quantiﬁed formula,
∀x∀x′∀u.(u = x′
|x′| −x
|x| ∧(x, x′) ∈¯S \ {(0, 0)} =⇒Au ≥0),
(23)
holds and the following system
{Au ≥0 ∧aT u = 0},
(24)
has only zero solution. By the hypothesis (A), since ¯S∗= ¯S \ {(0, 0)} ⊆T̸=, the
function U(x, x′) is a continuous function on ¯S∗. Hence, by Proposition 1, we

142
Y. Li
have U( ¯S∗) ⊆T . Since U( ¯S∗) = {u ∈Rn : u =
x′
|x′| −x
|x| ∧(x, x′) ∈¯S \{(0, 0)}}
and T = {u ∈Rn : u = x′ −λx ∧λ > 0 ∧(x, x′) ∈¯S}, to check if Formula (23)
is true, we may check if the following quantiﬁed formula holds,
∀x∀x′∀u.(u = x′ −λx ∧λ > 0 ∧(x, x′) ∈¯S =⇒Au ≥0).
(25)
This is because, if Formula (25) is true, then Formula (23) is also true.
In addition, to decide if Formula (24) has only zero solution is equivalent to
decide if the following quantiﬁed formula is true,
∀x∀x′∀u.(a11u1 + a12u2 ≥0 ∧a21u1 + a22u2 ≥0 ∧(u1 ̸= 0 ∨u2 ̸= 0)
=⇒a1u1 + a2u2 ̸= 0).
(26)
Eliminating x, x′, u from Formula (25) and (26), we get a semi-algebraic system
only on aij’s, denoted by Φ(a11, ..., a22, aT ). Solving Φ(a11, ..., a22, aT ), we obtain
that a11 = −2, a12 = 1, a21 = −1, a22 = 1, a1 = −3
2, a2 = 1. Therefore, there
indeed exist a11 = −2, a12 = 1, a21 = −1, a22 = 1, a1 = −3
2, a2 = 1, such
that both Formula (25) and Formula (26) hold. This immediately implies that
when a11 = −2, a12 = 1, a21 = −1, a22 = 1, a1 = −3
2, a2 = 1, Formula (23)
holds and the system(24) has only zero solution. Therefore, the hypothesis (C)
is satisﬁed. By Theorem 2, the program is non-terminating over the reals if and
only if the semi-algebraic system (4) has real solutions. By computing, we ﬁnd
the semi-algebraic system (4) has no real solutions. This clearly indicates that
the program is terminating.
4
The Non-homogeneous Case
Theorem 1 indicates that when some conditions are satisﬁed, the termination
of the homogeneous semi-algebraic program P can be equivalently reduced to
semi-algebraic systems solving. In the section, we will further show that the
termination of a non-homogeneous semi-algebraic program P is equivalent to
that of a homogeneous program such as P. Consider the non-homogeneous semi-
algebraic program P,
P : while C(x) > 0 do
{ F(x, x′) ≥0}
endwhile
(27)
where x, x′ ∈Rn, C(x) = (c1(x), ..., cs(x)) ∈(R[x])s and F(x, x′) = ( f1(x, x′),
..., fm(x, x′)) ∈(R[x])m. Let E(x, x′) = ( C(x)T , F(x, x′)T ). For P, we say that
P is non-terminating over the reals, if there exists an inﬁnite sequence {xi}+∞
i=0 ⊆
Rn such that E(xi, xi+1) ▷0 for any i ≥0, where ▷is deﬁned as before. More
importantly, one always can homogenize Program P by introducing two new
program variables z, z′ and adding three additional constraints on the z and z′
to obtain the following homogeneous semi-algebraic program P H such as P

Termination of Semi-algebraic Loop Programs
143
P H : while C(x, z) > 0 ∧z > 0 do
{F(x, x′, z) ≥0, z′ ≥z, −z′ ≥−z}
endwhile,
(28)
where C(x, z) = (c1(x, z), ..., cs(x, z))), F(x, x′, z) = (f1(x, x′, z), ..., fm(x, x′,
z)), ci(x, z) and fj(x, x′, z) are homogeneous polynomials, for i = 1, ..., s, j =
1, ..., m.
Example 2. consider the nonhomogeneous semi-algebraic program Q: while
(x1 −x2
2 −1 > 0∧x2
1 +2x2 +1 > 0)do{x′
1 +x′
2 −x1 +1 ≥0, x3
1 +x4
2 −x′
2 −3 ≥0}.
In Q, all the algebraic expressions x1 −x2
2 −1, x2
1 +2x2 +1, x′
1 +x′
2 −x1 +1, x3
1 +
x4
2 −x′
2 −3 are nonhomogeneous polynomials. By introducing additional vari-
ables z, z′, Q can be converted to a homogeneous semi-algebraic program QH:
while(x1·z−x2
2−1·z2 > 0∧x2
1+2x2·z+1·z2 > 0∧z > 0)do{x′
1+x′
2−x1+1·z ≥
0, x3
1 · z + x4
2 −x′
2 · z3 −3 · z4 ≥0, z′ ≥z, −z′ ≥−z}. According to the following
Theorem 3, we have the termination of Q is equivalent to that of QH.
Note that the update statements on z, {z′ ≥z, −z′ ≥−z} in (28), is equiv-
alent to z′ = z. For Program P H, denote by dci and dfj the degrees of ci(x, z)
and fj(x, x′, z). Let y = (x, z) and y′ = (x′, z′). Let ▷H = (>, . . . , >



s+1 times
, ≥, . . . , ≥



m+2 times
)
and ▷cf = ▷= (>, . . . , >



s times
, ≥, . . . , ≥



m times
).
Deﬁne
EH(y, y′) ▷H 0 ≜(
s+1



C(x, z)T > 0, z > 0,
m+2



F(x, x′, z)T ≥0, z′ −z ≥0, −z′ + z ≥0)T ,
EH
cf(y, y′) ▷cf 0 ≜(C(x, z)T > 0, F(x, x′, z)T ≥0)T ,
SH = {(y, y′) ∈R2n+2 : EH(y, y′) ▷H 0}.
(29)
Obviously, for any (y, y′) ∈R2n+2,
EH(y, y′) ▷H 0 =⇒EH
cf(y, y′) ▷cf 0.
(30)
The following theorem indicates that the termination of P is equivalent to
that of P H.
Theorem 3. The non-homogeneous Program P is non-terminating over the
reals if and only if the homogeneous Program P H is non-terminating over the
reals.
Proof. If the non-homogenous Program P does not terminate, say on input
x = x∗∈Rn, then the homogenous Program P H does not terminate on input
y = (x, z) = (x∗, 1).
For the converse, suppose that the homogenous Program P H does not ter-
minate on input y = y∗= (x∗, z∗). That is, there exists a inﬁnite sequence

144
Y. Li
{y∗
i }+∞
i=0 = {y∗
0 = (x∗
0, z∗
0) = y∗= (x∗, z∗), y∗
1, y∗
2, ..., y∗
i = (x∗
i , z∗
i ), ...} ⊆SH,
which is produced by the input y∗, such that
EH(y∗
i , y∗
i+1) ▷H 0
(31)
for i = 0, 1, .... Clearly, for all i = 0, 1, ...., we have πz(y∗
i ) = z∗= z∗
0, since
{z′ ≥z, −z′ ≥−z} in (28) is equivalent to z′ = z, where πz is a projection
mapping from Rn+1 to R. Therefore, Formula (31) is equivalent to
C(x∗
i , z∗
i ) > 0 ∧z∗
i > 0 ∧F(x∗
i , x∗
i+1, z∗
i ) ≥0 ∧z∗
i+1 = z∗
i = z∗,
(32)
for all i = 0, 1, .... Set y∗∗= ( x∗
z∗, 1). We next show that P H does not terminate
on y = y∗∗. Since ci(x, z)’s and fj(x, x′, z)’s are all homogeneous polynomials
in x, x′, z, we have
⎛
⎜
⎜
⎝
(z∗)dc1 · c1( x∗
i
z∗, 1) > 0
...
(z∗)dcs · cs( x∗
i
z∗, 1) > 0
⎞
⎟
⎟
⎠∧z∗
i
z∗= 1 > 0 ∧
⎛
⎜
⎜
⎝
(z∗)df1 · F1( x∗
i
z∗,
x∗
i+1
z∗, 1) ≥0
...
(z∗)dfm · Fm( x∗
i
z∗,
x∗
i+1
z∗, 1) ≥0
⎞
⎟
⎟
⎠
(33)
for all i = 0, 1, .... Since z∗> 0, we further get that
C
x∗
i
z∗, 1

> 0 ∧z∗
i
z∗= 1 > 0 ∧F
x∗
i
z∗, x∗
i+1
z∗, 1

≥0
(34)
for all i = 0, 1, .... Setting ˆyi = (ˆxi, ˆzi) = ( x∗
i
z∗, 1), we obtain that
EH(ˆyi, ˆyi+1) ▷H 0,
for all i = 0, 1, 2, .... Hence, Program P H does not terminate on y∗∗= ˆy0 =
( x∗
0
z∗, 1) = ( x∗
z∗, 1). In other words, we can construct a inﬁnite sequence
{ˆy0 = y∗∗, ˆy1, ˆy2, ..., ˆyi = (x∗
i
z∗, 1), ..., }
such that
EH(ˆyi, ˆyi+1) = EH
x∗
i
z∗, 1, x∗
i+1
z∗, 1

▷H 0
for any i ≥0. Thus, for any i ≥0, by Formula (30) we get
EH
cf(ˆyi, ˆyi+1) = EH
cf(x∗
i
z∗, 1, x∗
i+1
z∗, 1) ▷cf 0.
This immediately implies that
E
x∗
i
z∗, x∗
i+1
z∗

▷0
for any i ≥0, since E(x, x′) ≡EH
cf(x, 1, x′, 1) and ▷= ▷cf. Therefore, Pro-
gram P does not terminate on x =
x∗
0
z∗=
x∗
z∗, according to the deﬁnition of
nontermination of P.
□

Termination of Semi-algebraic Loop Programs
145
5
Conclusion
We have analyzed the termination of a class of semi-algebraic loop programs.
Some conditions are given such that under such conditions the termination prob-
lem of this kind of loop programs over the reals can be equivalently reduced to
the satisﬁability of a certain semi-algebraic system, i.e., when such conditions
are satisﬁed, a semi-algebraic loop program is not terminating over the reals if
and only if the constructed semi-algebraic system has a real solution. Especially,
for the nonhomogeneous semi-algebraic program P, one always can convert P
to its homogeneous version P H by introducing two auxiliary variables z, z′ and
adding three additional constraints {z > 0, z′ ≥z, −z′ ≥−z} to the body of
loop. The above constraints can also be rewritten as {z > 0, z′ = z}. This par-
ticular constraints cause that the only λ satisfying the semi-algebraic system
sysmP H, which is associated with P H and is similar to Formula (4), is 1,
sysmP H ≜y′ = λy ∧λ > 0 ∧y′ ̸= 0 ∧y ̸= 0 ∧EH(y, y′)▷H0,
(35)
where y = (x, z), y′ = (x′, z′). Therefore, since λ = 1, for each solution (y, y′)
of Formula (35), we have y = y′.
Acknowledgments. The author would like to thank the anonymous reviewers for
their helpful suggestions. This research is partially supported by the National Natural
Science Foundation of China NNSFC (61572024, 61103110).
References
1. Bagnara, R., Mesnard, F.: Eventual linear ranking functions. In: Proceedings of the
15th Symposium on Principles and Practice of Declarative Programming. Madrid,
Spain, pp. 229–238. ACM (2013)
2. Bagnara, R., Mesnard, F., Pescetti, A., Zaﬀanella, E.: A new look at the automatic
synthesis of linear ranking functions. Inf. Comput. 215, 47–67 (2012)
3. Ben-Amram, A.M., Genaim, S.: On multiphase-linear ranking functions. In:
Majumdar, R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 601–620.
Springer, Cham (2017). doi:10.1007/978-3-319-63390-9 32
4. Ben-Amram, A., Genaim, S.: On the linear ranking problem for integer linear-
constraint loops. In: Proceedings of the 40th Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL 2013), Rome, Italy,
pp. 51–62. ACM (2013)
5. Ben-Amram, A., Genaim, S.: Ranking functions for linear-constraint loops. J. ACM
61(4), 1–55 (2014)
6. Bradley, A.R., Manna, Z., Sipma, H.B.: Linear ranking with reachability. In:
Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576, pp. 491–504.
Springer, Heidelberg (2005). doi:10.1007/11513988 48
7. Bradley, A.R., Manna, Z., Sipma, H.B.: Termination of polynomial programs. In:
Cousot, R. (ed.) VMCAI 2005. LNCS, vol. 3385, pp. 113–129. Springer, Heidelberg
(2005). doi:10.1007/978-3-540-30579-8 8
8. Braverman, M.: Termination of integer linear programs. In: Ball, T., Jones, R.B.
(eds.) CAV 2006. LNCS, vol. 4144, pp. 372–385. Springer, Heidelberg (2006).
doi:10.1007/11817963 34

146
Y. Li
9. Chen, H.Y., Flur, S., Mukhopadhyay, S.: Termination proofs for linear simple loops.
In: Min´e, A., Schmidt, D. (eds.) SAS 2012. LNCS, vol. 7460, pp. 422–438. Springer,
Heidelberg (2012). doi:10.1007/978-3-642-33125-1 28
10. Chen, Y., Xia, B., Yang, L., Zhan, N., Zhou, C.: Discovering non-linear ranking
functions by solving semi-algebraic systems. In: Jones, C.B., Liu, Z., Woodcock,
J. (eds.) ICTAC 2007. LNCS, vol. 4711, pp. 34–49. Springer, Heidelberg (2007).
doi:10.1007/978-3-540-75292-9 3
11. Col´oon, M.A., Sipma, H.B.: Synthesis of linear ranking functions. In: Margaria,
T., Yi, W. (eds.) TACAS 2001. LNCS, vol. 2031, pp. 67–81. Springer, Heidelberg
(2001). doi:10.1007/3-540-45319-9 6
12. Cook, B., See, A., Zuleger, F.: Ramsey vs. lexicographic termination proving.
In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp. 47–61.
Springer, Heidelberg (2013). doi:10.1007/978-3-642-36742-7 4
13. Cousot, P.: Proving program invariance and termination by parametric abstrac-
tion, lagrangian relaxation and semideﬁnite programming. In: Cousot, R. (ed.)
VMCAI 2005. LNCS, vol. 3385, pp. 1–24. Springer, Heidelberg (2005). doi:10.
1007/978-3-540-30579-8 1
14. Duistermaat, J., Kolk, J.: Multidimensional Real Analysis. Cambridge University
Press, Cambridge (2004)
15. Heizmann, M., Hoenicke, J., Leike, J., Podelski, A.: Linear ranking for linear lasso
programs. In: Hung, D., Ogawa, M. (eds.) ATVA 2013. LNCS, vol. 8172, pp. 365–
380. Springer, Cham (2013). doi:10.1007/978-3-319-02444-8 26
16. Leike, J., Heizmann, M.: Ranking templates for linear loops. In: ´Abrah´am, E.,
Havelund, K. (eds.) TACAS 2014. LNCS, vol. 8413, pp. 172–186. Springer,
Heidelberg (2014). doi:10.1007/978-3-642-54862-8 12
17. Li, Y., Zhu, G., Feng, Y.: The L-depth eventual linear ranking functions for single-
path linear constraints loops. In: 10th International Symposium on Theoretical
Aspects of Software Engineering (TASE 2016), pp. 30–37. IEEE (2016)
18. Li, Y.: Witness to non-termination of linear programs. Theor. Comput. Sci. 681,
75–100 (2017)
19. Liu, J., Xu, M., Zhan, N.J., Zhao, H.J.: Discovering non-terminating inputs for
multi-path polynomial programs. J. Syst. Sci. Complex. 27, 1284–1304 (2014)
20. Podelski, A., Rybalchenko, A.: A complete method for the synthesis of linear rank-
ing functions. In: Steﬀen, B., Levi, G. (eds.) VMCAI 2004. LNCS, vol. 2937, pp.
239–251. Springer, Heidelberg (2004). doi:10.1007/978-3-540-24622-0 20
21. Tiwari, A.: Termination of linear programs. In: Alur, R., Peled, D.A. (eds.)
CAV 2004. LNCS, vol. 3114, pp. 70–82. Springer, Heidelberg (2004). doi:10.1007/
978-3-540-27813-9 6
22. Xia, B., Zhang, Z.: Termination of linear programs with nonlinear constraints. J.
Symb. Comput. 45(11), 1234–1249 (2010)
23. Xia, B., Yang, L., Zhan, N., Zhang, Z.: Symbolic decision procedure for termination
of linear programs. Formal Aspects Comput. 23(2), 171–190 (2011)

Computing Exact Loop Bounds for Bounded
Program Veriﬁcation
Tianhai Liu1(B), Shmuel Tyszberowicz2,3, Bernhard Beckert1,
and Mana Taghdiri4
1 Karlsruhe Institute of Technology, Karlsruhe, Germany
liu@ira.uka.de
2 RISE, Southwest University, Chongqing, China
3 The Academic College Tel Aviv Yaﬀo, Tel Aviv, Israel
4 Horus Software GmbH, Ettlingen, Germany
Abstract. Bounded program veriﬁcation techniques verify functional
properties of programs by analyzing the program for user-provided
bounds on the number of objects and loop iterations. Whereas those two
kinds of bounds are related, existing bounded program veriﬁcation tools
treat them as independent parameters and require the user to provide
them. We present a new approach for automatically calculating exact
loop bounds, i.e., the greatest lower bound and the least upper bound,
based on the number of objects. This ensures that the veriﬁcation is
complete with respect to all the conﬁgurations of objects on the heap
and thus enhances the conﬁdence in the correctness of the analyzed pro-
gram. We compute the loop bounds by encoding the program and its
speciﬁcation as a logical formula, and solve it using an SMT solver. We
performed experiments to evaluate the precision of our approach in loop
bounds computation.
1
Introduction
Bounded program veriﬁcation techniques (e.g. [7,15,24]) verify functional prop-
erties of object-oriented programs, where loops are unrolled and the number of
objects for each class is bounded. These techniques typically encode the program
and the property of interest into a logical formula and check the satisﬁability
of the formula by invoking an SMT solver. They provide an attractive trade-oﬀ
between automation and completeness. They automatically exhaustively ana-
lyze a program based on the user-provided bounds1 and thus guarantee to ﬁnd
any bug (with respect to the analyzed property) within the bounds, but defects
outside bounds may be missed. As a result, bounded program veriﬁcation has
becomes an increasingly attractive choice for gaining conﬁdence in the correct-
ness of software.
Existing bounded program veriﬁcation techniques typically require the user
to provide two kinds of bounds as separate parameters: (1) the loop bounds that
1 They analyze a program based on both bounds—objects and loop iterations. Thus,
not all object space within bounds is necessarily explored (as explain in what follows).
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 147–163, 2017.
https://doi.org/10.1007/978-3-319-69483-2_9

148
T. Liu et al.
limit the number of iterations for each loop, and (2) the class bounds that limit
the number of objects for each class. (The class bound for a primitive type,
e.g., the Java int, is the size of integer bit-width.) These two kinds of bounds,
however, are not independent and have to be chosen carefully; the class bounds
can aﬀect the number of loop iterations, and the loop bounds can inﬂuence the
size of object space to be explored. To clarify, consider the following loop that
traverses an acyclic singly-linked list in Java, starting from the header entry
l.head: e=l.head; while(e!=null){ e=e.next;}.
Supposing the loop bound is 2, we unroll the loop twice and then add an assume
clause which fails if it iterates more than 2 times. The code will be: e=l.head;
if(e!=null){ e=e.next; if(e!=null){ e=e.next;assume(e==null);}}.
When the list has at most one element (implied, e.g., by the provided class
bound), the second if-condition always evaluates to false and thus the following
code is unreachable. On the other hand, if the list contains at least 5 elements
(implied, e.g., by the speciﬁcation or by the rest of the code), the encoding
formula of the code evaluates to false, since the assume statement will never
evaluate to true. Furthermore, when the list contains up to 5 elements, only the
lists with at most 2 elements are analyzed and the other elements in the list are
not treated.
When a loop is unrolled too many times (i.e., more than the least upper
bound on the number of loop iterations), the unrolled program has many
unreachable paths which may impede the performance of the underlying solver.
The veriﬁcation process may fail due to the solver being overloaded. When a
loop is unrolled too few times (i.e., fewer than the greatest lower bound on the
number of loop iterations), none of the program executions that reach the loop
will be valid in the unrolled program, thus any property concerning the loop will
vacuously hold in all runs. This is also the case for inﬁnite loops; we consider a
loop inﬁnite when it does not terminate for any input. Selecting a loop bound
that lies strictly between the greatest lower- and the least upper-bound causes
the analysis to be incomplete (i.e., it explores only a part of object space).
Several approaches have been developed to compute loop upper bounds
(e.g. [4,10,18,22]), and many of them (e.g. [4,10,18]) do not require a speciﬁc
bound for objects; they compute loop upper bounds as functions, based on the
input sizes. However, none of those approaches can handle arbitrary conﬁgura-
tion of objects on the heap. They either focus only on primitive types [4,18,22]
or support only particular conﬁguration of objects [10]. Furthermore, none of
those approaches considers speciﬁcations in computing loop bounds, and many
of them compute a valid upper bound, which is not necessarily the least upper
bound.
Incremental bounded model checkers, e.g. NBIS [11], also can be used to
compute loop upper bounds. Starting from an initial number, they unroll a loop
and check whether the loop condition still holds after the last unrolled iteration.
If so, a new upper bound candidate is found and checked again iteratively. This
approach, however, is imprecise in the presence of class bounds and speciﬁca-
tions. It may compute upper bounds that are higher than the least upper bound,

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
149
thus many unreachable paths arise in the unrolled program, and the veriﬁcation
may fail. To overcome this potential failure, the user may restart veriﬁcation
with smaller class bounds. Thus the conﬁdence in the correctness of the code is
reduced, as the number of relevant objects is smaller. Moreover, this approach
does not compute bounds when the loops are non-terminating. A loop is con-
sidered non-terminating when it does not terminate for at least one input while
it may terminate for other inputs. This is inconsistent with bounded program
veriﬁcation tool, which do analyze the terminating executions of a method and
ignore non-terminating runs.
We present an approach that is meant to be used as a pre-processing phase
in bounded program veriﬁcation. It focuses on data-structure-rich programs and
can handle arbitrary conﬁgurations for the objects in the heap. Given both a
program method m selected for analysis and class bounds b, we compute both the
greatest lower bound and the least upper bound for each loop that is reachable
from m. Our approach, therefore, can provide the user with an insight on what
loop bounds to consider in bounded program veriﬁcation, and to enhance the
conﬁdence for program correctness with respect to the class bounds. When a
method speciﬁcation exists, we consider it as well. In addition to numerical
bounds, we also output a pre-state (and an execution trace) that witnesses each
computed bound, which guarantees that the computed bounds are feasible. We
produce loop bounds even for a non-terminating loop, provided that the loop
has at least one terminating execution; recall our deﬁnition of non-terminating.
This is consistent with bounded program veriﬁcation approaches. Besides, we
can detect unreachable loops by analyzing the results of bound computation.
We compute loop lower- and upper-bounds for Java programs annotated in
a subset of JML (Java Modeling Language) [13]. We translate the code, its pre-
condition, and its sub-routines’ speciﬁcations (if they exist) into a ﬁrst-order
formula, encoding the loops’ eﬀects as recursive uninterpreted functions. The
resulting formula is solved for the exact greatest lower and least upper bounds
for each loop. This is achieved by calling an SMT (satisﬁability modulo theo-
ries) solver that is able to solve optimization problems. Given a formula f and
an optimization objective o, the SMT solver ﬁnds a model for f to achieve the
goal o. Several oﬀ-the-shelf SMT solvers have been extended to solve optimiza-
tion problems, e.g., Z3 [19] with νZ [3] and SYMBA [14], MathSAT5 [5] with
OptiMathSAT [21]. Besides, some SMT-based algorithms for solving optimiza-
tion problems have been developed, e.g., the authors of [17] integrated an SMT
solver with a classical incremental solving algorithm to solve generic optimiza-
tion problems, and an algorithm in [20] aims to solve linear arithmetic problems.
Our approach takes advantage of these recent advances in SMT solvers. Our tar-
get logic is undecidable, i.e., it is possible for the underlying solver to output
‘unknown’. However, for the small class bounds generally used in bounded pro-
gram veriﬁcation, the solver returns a deﬁnite answer.
We have implemented our approach and NBIS’ approach in prototype tools
BoundJ and IncUnroll, respectively. We compared the computed bounds using
these tools. Our experiments reveal that in all cases BoundJ has computed

150
T. Liu et al.
precise loop bounds, while IncUnroll does not. On the other hand, IncUnroll
can produces loop bounds with the increased class bounds, while BoundJ returns
‘unknown’ for large class bounds.
2
Logical Formalism
We focus on analyzing object-oriented programs, and currently support a basic
subset of Java—excluding ﬂoating-point numbers, strings, generics, and concur-
rency. We support class hierarchy without interfaces and abstract classes. Any
method which is called by the analyzed method and has no speciﬁcation is inlined
into its call sites; otherwise it is replaced by its speciﬁcation. Speciﬁcations are
written in JML [13], and should not include exceptional behaviors and model
ﬁelds. The constructs requires and ensures deﬁne, respectively, a method’s
pre- and postcondition. We support arbitrarily nested universal and existential
quantiﬁers, and allow using the JML reachability construct.
We translate the given code and its speciﬁcations into a ﬁrst-order SMT logic
that consists of quantiﬁed bit-vectors, unbounded integers, and uninterpreted
functions. We now describe our target logic. For this we use the SMT-LIB 2.0
syntax [2], in which expressions are given in a preﬁx notation. The command
(declare-fun f (A1 .. An−1) An) declares a function f : A1 × .. × An−1 →
An. Constants are functions that take no arguments. The command (assert F)
asserts a formula F in the current logical context; multiple assert statements are
assumed to be implicitly conjoined. The (push) command pushes an empty asser-
tion set onto the assertion-set stack. and (pop) pops the top assertion set from the
stack. The command (check-sat) triggers solving a conjunction of formulas. The
operator ite denotes a ternary if-then-else expression. Basic formulas are com-
bined using the boolean operators and, or, not, and => (implies). Universal and
existential quantiﬁers are denoted by the keywords forall and exists. The Z3
solver contains two extensions of SMT-LIB to express optimization objectives.
The command (maximize t) instructs the solver to produce a model that max-
imizes the value of the integer term t and to return the assignment for t in the
solution, if one exists. The (minimize t) command ﬁnds the smallest value of t.
Our translation uses the ﬁxed-size bit-vectors theory, in which sorts are of
the form ( BitVec m), where m is a non-negative integer denoting the size of the
bit-vector. Bit-vectors of diﬀerent sizes represent diﬀerent sorts in SMT. This
theory models the precise semantics of unsigned and of signed two-complements
arithmetic, supporting a large number of logical and arithmetic operations on bit
vectors. The translation also uses the unbounded integers theory, which contains
only one sort Int, corresponding to integer numbers. It supports arithmetic
operations to be applied to numerical constants or variables.
3
Motivating Example
We use two examples to illustrate that we compute precise loop bounds. Figure 1
shows a Java program for a check-in process in a youth hostel. The check-in

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
151
1
int guest = 0;
2
//@requires (\forall int i; 0<=i&&i<ages.length; 0<ages[i]&&ages[i]<=18);
3
void checkin(int[] ages) {
4
for (int i = 0; i < ages.length; i++) {
5
if (ages[i] <= 27) guest++;}
6
openRoomFor();}
7
//@ ensures 3<=\old(guest) && \old(guest)<=10;
8
native void openRoomFor();
Fig. 1. A simple program for youth hostel checkin. The method openRoomFor’s post-
condition at line 7 constrains the range of values of guest.
process requires that the guests will be young (the precondition of the method
checkin), and the group size is neither greater than 10 nor less than 3 (the
postcondition2 of the method openRoomFor). Carefully inspecting the code, we
notice that the branch condition at line 5 is implied true by the precondition of
the method checkin. Thus the number of loop (line 4) iterations equals to the
number of young guests (the ﬁeld guest when the loop terminates). Suppose
the Java int bit-width is 6 (i.e., integer numbers ranging from −32 to 31). We
evaluate the loop lower- and upper bounds to 3 and 10, respectively. However,
using an approach that computes loop bounds by incrementally unrolling loops
(Sect. 1), the loop upper bound is 31, since it does not consider the postcondition
of the method openRoomFor when evaluating the loop condition. The incremental
loop unrolling approach provides no lower bounds computation.
Figure 2 illustrates a Java implementation of a copy method for a singly-
linked list of Data entries. Given an instance d of Data, the copy method deeply
copies the receiver list, starting from the ﬁrst occurrence (exclusive) of d. If
d does not exist, nothing is copied. Bounded program veriﬁcation techniques
analyze programs with respect to speciﬁc bounds on the number of objects.
Assume that the maximum number of objects of type List, Entry, and Data is
2, 26, and 1, respectively. The bounds for each loop are computed separately.
For instance, when the upper bound of the second loop (Loop2 at line 9) is
computed, no speciﬁc bound for the preceding loop (Loop1 at line 4) is assumed.
Our technique computes the loop upper bounds considering all the cases in
which the loops terminate. Loop1 may not terminate for some inputs, e.g., when
the receiver list is a cyclic one. For all inputs for which Loop1 terminates our
technique outputs 26 as the upper bound and generates a witness in which an
acyclic list contains 26 entries, where the last one is followed by null. For all
inputs for which Loop2 terminates, our technique outputs 12 as Loop2’s upper
bound and generates a witness where an acyclic list has 13 entries, where the
ﬁrst one has Data d. That makes sense, because the copy method deep copies an
acyclic linked list, and fresh entries one-to-one correspond (excluding the entry
containing d) to the entries in the receiver list. Thus Loop2 can allocate at most
2 We intentionally refer to the postcondition rather than to a precondition that limits
the number of guests in order to demonstrate our approach.

152
T. Liu et al.
1
class List { Entry head;
2
List copy(Data d) {
3
Entry curr = head;
4
while (curr != null && curr.data != d) curr = curr.next; // Loop 1
5
List result = new List();
6
if (curr != null) {
7
curr = curr.next;
8
Entry last = null;
9
while (curr != null) {
// Loop 2
10
Entry e = new Entry(curr.data);
11
if (last == null) { result.head = e; }
12
else { last.next = e; }
13
last = e;
14
curr = curr.next;
15
}
16
}
17
return result;}}
18
class Entry {/*@ nullable */ Entry next; /*@ nullable */ Data data;
19
Entry(Data d) { data = d; next = null;}}
20
class Data {}
Fig. 2. A Java program to perform a deep copy on a linked list.
12 fresh objects, and a total of 25 (=12 ∗2 + 1) Entry objects are used. If Loop2
iterates 13 times, e.g., a total 27 objects are needed, which is larger than the
bound (26) on Entry.
As shown, the number of loop iterations heavily depends on both the spec-
iﬁcations and the number of objects in the analyzed domain. Furthermore, the
number of iterations of diﬀerent loops is inter-dependent. Detecting those depen-
dencies manually and computing the precise loop bounds can be prohibitively dif-
ﬁcult, thus an automatic approach for computing exact loop bounds (in the pres-
ence of speciﬁcations and class bounds) can signiﬁcantly enhance the bounded
program veriﬁcation engineers’ conﬁdence in the correctness of the analyzed
programs.
4
Our Approach
Given a method p selected for analysis from a piece of code and a set of class
bounds b, our technique computes for each loop l two numbers, GLBl and LUBl.
They respectively denote the greatest lower bound and the least upper bound on
the number of iterations of l, since we ensure that no valid execution of p can
iterate the loop l less times than GLBl or more times than LUBl. In order
to analyze only valid executions, we consider the whole code when computing
the bounds for a loop l. For each computed bound the output also contains a
witnessing pre-state and an execution trace.
We translates the given method p and its constraints (speciﬁcations) c (con-
sisting of p’s precondition and the annotations of the methods reachable from p)

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
153
into an SMT formula, based on a set of user-provided bounds b on the number of
instances of the classes. Let T denote this translation; then T[p, c, b] produces a
tuple (s, f, Nl), where s is the pre-state of p, f is an SMT formula that encodes
the control ﬂow and dataﬂow of p and the additional constraints b and c, and Nl
represents the number of times the loop condition has been checked for loop l.
We distinguish various loops inside p using loop ids.
The LUBl is computed by delegating a formula of the form f ∧exit(l, Nl) ∧
maximize(Nl) to an SMT solver that provides the functions to solve optimiza-
tion problems. The function exit means that the loop l exits after checking the
loop condition Nl times. The maximize command instructs the solver to ﬁnd
a model where Nl is the biggest compared to the values in other models. A
satisfying solution to this formula represents a terminating execution of p in
which the loop l is reachable when running p and the number of iterations of l
is Nl −1 (Nl > 0), and unreachable in case that Nl = 0. When the formula is
unsatisﬁable, it means that either the user-provided class bounds are too small
or the methods are over-speciﬁed (e.g., the precondition of the analyzed method
is false). The GLBl is computed similarly to least upper bound computation,
using the command minimize instead of maximize.
Unbounded integers are used in our translation to encode loop iterations.
Since (1) a loop may not terminate, and (2) even for terminating loops, the num-
ber of iterations is not known and thus cannot be bounded apriori. Hence, our
target logic is undecidable, and it is possible for the solver to output ‘unknown’.
In such a case, our analysis terminates with no conclusive outcome.
4.1
Encoding Control Flow
We encode the control ﬂow of the analyzed method using a computation
graph [24]. Each node in this graph represents a control point in the program,
and each edge represents either a program statement or a branch condition.
There are exactly one node to entry the graph and one node to exit from the
graph. If a loop in the analyzed method is triggered multiple times, due to either
method invocations or that it is an inner loop, then multiple occurrences of the
loop exist in the computation graph. We compute the loop bounds for each loop
occurrence. This is consistent with bounded program veriﬁcation.
Bounded program veriﬁcation tools such as Jalloy [24] and InspectJ [15] also
use computation graphs to encode control ﬂow. However, they unroll loops and
thus assume that the graph is acyclic. In that case, control ﬂow can be encoded
by simple boolean variables. Our approach, on the other hand, preserves loops
as cycles in the graph and encodes their (cyclic) control ﬂow via uninterpreted
functions in the SMT logic. More precisely, similarly to previous approaches,
we encode an edge that does not belong to any loop from node m to node n,
using a boolean variable Emn, whose truth value denotes whether the edge is
traversed or not. When an edge belongs to a loop, the encoding must clarify in
which loop iterations the edge is traversed. Therefore, when an edge from m to
n belongs to loop l, we encode it using a boolean-valued, uninterpreted function
Emn : Int>0 →Bool (Int>0 denotes positive integers). The expression Emn(i)

154
T. Liu et al.
evaluates to true if the edge is traversed in the ith iteration of l. The exit edge of
l is traversed only once the loop condition is not fulﬁlled for the (Nl)th iteration
of the loop. We encode the exit edge of a loop l as the expression Emn(Nl),
where Nl > 0 and Nl = 1+K, where K is the number of iterations of the loop l.
We use the term entry edge (exit edge) to denote an edge that leads to the
entry node (exits from the exit node) of a loop but does not belong to that loop.
We use the term head edge (tail edge) to denote the ﬁrst (the last) edge of a loop.
The control ﬂow for the computation of the loop bounds is encoded using the
following four general rules. (1) The ﬁrst edge of the computation graph must be
traversed. (2) If an edge Emn is traversed, at least one of the outgoing edges of
node n must be traversed (dataﬂow constraints prevent more than one outgoing
edge from being traversed). If node n belongs to a loop l, the iteration index
must be considered. In particular, if n is the loop’s head node, then (3) if a head
edge is traversed, either the ﬁrst iteration starts or the loop exits before the ﬁrst
iteration, and (4) if a tail edge at the ith iteration of the loop is traversed, then
either the (i + 1)th iteration starts or the loop exits before this iteration.
Figure 3 provides an example. Figure 3(a) shows a Java method that sets
the ﬁeld data of all list elements to the input value, provided that this value
is not null. Figure 3(b) gives the corresponding computation graph. The edge
labels denote the statements and branch conditions in a special SSA-like format
as described in Sect. 4.2. Figure 3(c) presents our encoding of the control ﬂow.
In this example, E 0 1, E 1 2, E 2 3, E 2 8, E 3 4, E 4 5, and E 8 9 are boolean
variables, while E 5 6, E 6 7, E 7 5, and E 5 8 are boolean-valued functions. The
edge E 4 5 is an entry edge, E 5 6 is a head edge, E 7 5 is a tail edge, and E 5 8
is an exit edge. The numbers preceding the constraints correspond to the four
encoding rules presented above. For each loop l in the computation graph, we
introduce an integer variable Nl to represent the number of times that the loop
condition has been checked; e.g., NL0 and NL1 in Fig. 3(b) and N L 1 (that
encodes NL1) in Fig. 3(c).
4.2
Encoding Dataﬂow
We now provide an overview of our encoding of Java statements, which is based
on the InspectJ approach [15]. We focus on how loops aﬀect the encoding. The
types that are accessed in the analyzed code are encoded using bit-vectors in the
SMT logic. That is, if a Java type T is bounded by the user-provided number
n, we encode T as a bit-vector of size ⌈log(n + 1)⌉(including the null value).
In the following description, we use BV [T] to represent the bit-vector of a Java
type T.
In an acyclic computation graph, all variables and ﬁelds of the program are
renamed so that they are assigned at most once along each path of the graph.
Since our computation graphs can be cyclic, renaming cannot be achieved by
enumerating all paths. We rename variables and ﬁelds of the program assuming
that each loop constructs a separate naming context (similar to a called method,
e.g.). This separates the naming of variables (ﬁelds) in one loop from the others,

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
155
Fig. 3. (a) sample Java code, (b) its computation graph, (c) our control ﬂow encoding.
The character ‘L’ in (b) denotes loop id. The variable NL0 represents the number of
times that the loop condition has been checked. After exiting the loop it is renamed to
NL1, and is encoded in (c) as the SMT variable N L 1.
which makes it easier to support complex loop structures. More precisely, renam-
ing variables (ﬁelds) involves the following steps. (1) Starting from the innermost
loop l, we give any variable (ﬁeld) that may be updated by l an initial name, and
then perform renaming within the body of l as for an acyclic computation graph.
(2) We collapse the cycle (loop) l of the computation graph into a single node,
denoting the initial and the ﬁnal names of the variables updated in l. (3) We
repeat step 1, considering the collapsed loops. Hence, any time a collapsing node
is visited, adequate conditions are produced to ensure that the variables (ﬁelds)
of the current context hold the same values as the initial/ﬁnal variables (ﬁelds)
of the collapsed loop. In the example in Fig. 3(b), d0, NL0, NL1, e0, e1, r0, r1,
and data0 belong to the outer context, whereas Le0, Le1, Ldata0, and Ldata1
belong to the loop context. Since the loop does not update the next0 ﬁeld and
the constants, e.g., this, p0 and ret, both contexts share them. Data accesses

156
T. Liu et al.
Fig. 4. Dataﬂow Encoding SMT formulas: (a) dataﬂow (b) frame conditions. Each
class has a distinct null value, e.g., the null Data and the null Entry.
outside loops are encoded as follows: A variable v of type T is encoded as an
SMT variable v : BV [T], and a ﬁeld f of type T2 declared in a class T1 is encoded
as a function f : BV [T1] →BV [T2]. However, if a variable or ﬁeld is updated
within a loop, one needs to know the updates performed in each loop iteration.
A variable vl of type T that may be modiﬁed within a loop l is encoded as a
function vl : Int>0 →BV [T], where vl(i) denotes the value of v in the ith itera-
tion of l. Similarly, a ﬁeld f of type T2 declared in a class of type T1, that may be
modiﬁed within a loop l, is encoded as a function fl : Int>0 ×BV [T1] →BV [T2],
fl(i, o) denotes the value of o.f in the ith iteration of the loop. Figure 4(a) shows
the dataﬂow formulas for Fig. 3(b). The ﬁrst 8 formulas correspond to the edges
outside the loop. The last 3 encode the dataﬂow in each loop iteration.
Frame Conditions. Frame conditions are used to avoid underspeciﬁcation
of nodes with multiple incoming edges, and to ensure the correctness of the
dataﬂow. The highlighted expressions in Fig. 3(b) are the frame conditions for
the corresponding dataﬂow expressions. Lets take the merge node 8 for example.
Since the variables r, e, and NL are updated (and thus renamed) only in the
path 2 →3 →4 →5 →8, frame conditions for these variables are required when
the path 2 →8 is taken; the last formula in Fig. 4(b) is the relevant frame condi-
tion. Special frame conditions are required when the merge node is a loop’s head
node, e.g., node 5 in Fig. 3(b). Before the ﬁrst iteration, Le0(1) equals e0 and
Ldata0(1) equals data0 (encoded as the ﬁrst formula in Fig. 4(b)). After the last

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
157
iteration, e1 equals Le0(Nl) (encoded as the second formula in Fig. 4(b)). Fur-
thermore, in each new iteration i + 1, Le0(i + 1) equals Le1(i) and Ldata0(i + 1)
equals Ldata1(i) (encoded as the third formula in Fig. 4(b)). It should be noted
that the frame condition for Nl ensures that the variable N L 1 equals to the
number of times that the loop condition has been checked. Without the frame
condition we may get wrong loop bounds, due to traversing spurious paths.
Nested Loops. If a loop l2 is nested in a loop l1, the iterations of l2 depend
on the iterations of l1. Therefore, we encode those variables (ﬁelds) that are
updated in the inner loop l2 by adding an additional iteration column to the SMT
functions that represent those variables (ﬁelds). That is, if a variable vl2 of type T
is modiﬁed within l2, we declare an SMT function vl2 : Int>0 × Int>0 →BV [T],
where vl2(i1, i2) denotes the value of vl2 in the (i2)th iteration of l2 while in the
(i1)th iteration of l1. Updated ﬁelds are encoded in a similar way by adding an
additional column. Moreover, the edge variables encoding the control ﬂow of l2
will also get an additional column corresponding to the iteration number of the
outer loop. It works the same way for any depth of nesting.
Encoding Speciﬁcations. Computing loop bounds does not require any user-
provided speciﬁcations or annotations; the user only provides bounds on the
number of elements of each type. However, if the precondition of the analyzed
method or the method contracts of the called methods are provided, our analysis
will take them into account. That is, if the user provides method contracts for
an invoked method, they will be used to substitute any call to the method.
Otherwise, the method body will be inlined in its call sites. More details can be
found in our previous work [15].
4.3
Computing Loop Bounds
In order to compute the bounds of a loop l, we constrain l to terminate, that
is, its exit edge to be traversed when its loop condition has been checked Nl
times, if l is reachable from the analyzed method. We also trigger the solver to
ﬁnd the model where Nl has the maximal assignment. We solve the conjunction
of all the control ﬂow, dataﬂow, frame conditions, and speciﬁcation formulas. If
this formula is satisﬁable, Nl is assigned a value in the satisfying solution, where
Nl > 0 denotes the loop l is reachable and its bound is Nl, Nl = 0 denotes l
is not reachable. If the formula is unsatisﬁable, then either the user-provided
class bounds are not large enough or the user-provided speciﬁcations are not
consistent by themselves. The following formulas give the SMT commands that
computes the least upper bound for the example of Fig. 3.
(push) (assert (=> (> N_L_1 0) (E_5_8 N_L_1)))
(maximize N_L_1) (check-sat) (get-model) (pop)
To compute a loop lower bound, we just replace the maximize command by
minimize. It is possible for the solver to output ‘unknown’ because our logic is
undecidable, and then our analysis terminates with no conclusive outcome.

158
T. Liu et al.
5
Experiments
Our prototype tool (BoundJ) uses: the Jimple 3-address intermediate repre-
sentation provided by the Soot optimization framework [23] to preprocess Java
program code; the Common JML Tools package (ISU) [13] to preprocess JML
speciﬁcations; and Z3 version 4.4.2 [19] as the underlying SMT solver. We also
have considered the approach used in NBIS [11]to evaluate the precision of our
approach. Since NBIS targets C and C++ code, and does not consider spec-
iﬁcations or class bounds, we implemented that approach in a prototype tool
(IncUnroll) that targets Java and accepts the same inputs that BoundJ does.
We report on a collection of benchmarks, selected from InspectJ [15] (a bounded
program veriﬁcation system), KeY [1] (a program veriﬁcation system), JDK
(Java Development Kit), and TPDB (Termination Programs Data Base) [25].
All the experiments3 have been performed on an Intel Core 2.50 GHz with 4 GB
of RAM using Linux 64bit.
Table 1. Results of computing loop bounds using BoundJ and IncUnroll.
The evaluation results are shown in Table 1. The Method column shows the
names of the entry methods of the analyzed programs. There are in total 6 pro-
3 The complete benchmarks can be found at http://asa.iti.kit.edu/478.php.

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
159
grams have been analyzed. To increase the complexity of the speciﬁcations, we
also added special method contracts for the sub-routines (if exist). The method
deleteMin of the class BinaryHeap (that calls another 3 methods, 109 LOC)
eﬀectively extracts the minimum element in a min heap4 and restores the prop-
erties of min heap. The removeDups method of the class KeYList (3 methods,
33 LOC) removes the duplicate elements from a queue. The add method (2
methods, 39 LOC) is classical implementation in JDK 1.7. All those methods
have complex preconditions, i.e., quantiﬁers have been involved. The methods
deleteMin and add also use JML reachability expressions to constrain the heap
conﬁgurations. The copy method is the method presented in Sect. 3. In addition
to these data-structure-based benchmarks, we have also used benchmarks that
involve only primitive types. Such benchmarks are typical for the loop bound
computation and the non-termination detection communities. They (the meth-
ods fibnacci and gause) do not have any speciﬁcation and there are around 10
LOC in average in each method, however, we have selected these benchmarks for
the following two reasons: (i) The number of iterations for many of these loops
is non-linearly distributed. Therefore, computing their loop bounds is particu-
larly challenging in many existing approaches. (ii) To validate that our approach
indeed computes loop bounds for the methods that contain at least one terminat-
ing path. For each analyzed program, we have exploited each tool to compute ∼4
loop upper bounds for diﬀerent class bounds, and in total 25 loop upper bounds
computations have been done using each tool. Besides, we also used BoundJ to
compute the loop lower bounds. Thus in total we have done 75 computations.
The Scope Size column shows the bounds on the number of objects of each
class and on the size of the integer bit-width. For a scope size n, the analyses of
deleteMin, removeDups, and copy methods have to explore data spaces of size
(n + 1)11 ∗29n, (n + 1)9 ∗2n, and (n + 1)5 ∗2n, respectively. (The numbers are
calculated based on the number of accessed classes, ﬁelds, and parameters. Com-
putations are skipped for space reasons.) The columns GLB and LUB represent
the computed lower and upper bounds, respectively. When a method contains
more than one loop, the bounds are shown as a sequence of numbers separated by
commas. The symbol X denotes that the loop is not reachable from the method.
Question marks (?) mean that no deﬁnite answer is achieved after the timeout
limit of 20 min.
In Table 1 we observe that: (i) BoundJ computed exact loop lower-/upper-
bounds for all data structure-rich methods. A careful inspection of the code
reveals that all computed loop lower- and upper bounds are greatest and least,
respectively. (ii) IncUnroll does not always compute precise loop bounds as
BoundJ does. Since IncUnroll does not consider the whole code in loop bounds
computation, on average its computed loop upper bounds are 4.2 times (median
4, maximum 13) greater than the ones BoundJ computed. In addition, IncUn-
roll failed to compute the loop upper bounds for the non-terminating methods
copy and fibonacci because of timeout, while BoundJ still produced the loop
4 A min heap is a binary heap where the values that are stored in the children nodes
are greater than the value stored in the parent node.

160
T. Liu et al.
upper bounds for the methods since it considers all program executions that
terminate. (iii) Nevertheless, IncUnroll can compute loop upper bounds with
increased class bounds, while BoundJ timeouts for 2 (of 25) cases. According
to the small scope hypothesis [12], bounded program veriﬁcation systems aim to
analyze the program with respect to a small scope. Furthermore, BoundJ always
produces the exact loop bounds based on the user-provided class bounds, thus
it guarantees any bounded program veriﬁcation is complete for the given class
bounds and enhances the conﬁdence in the correctness of the analyzed program.
6
Related Work
Various techniques have been developed to compute loop bounds in real-time sys-
tems. To estimate loop bounds, they either require annotations [8] or perform a
numerical analysis to achieve a numerical interval of loop upper bounds [6].
To achieve better analysis performance and more precise loop bounds, the
technique described in [16] employs a combination of abstract interpretation,
inter-procedural program slicing, and inter-procedural dataﬂow analysis. Unlike
our technique that requires explicit user-provided class bounds, the techniques
described in [4,9,22] generate symbolic bounds (functions) in terms of loop
inputs. All these approaches, however, focus on numerical loops; some of them
(e.g. [4,6]) even require well-structured loops with no branches inside them. Our
approach can work on arbitrary loops and target data structure-rich programs.
To compute bounds for complex loops in C++ code, SPEED [10] gener-
ates computational complexity bound functions that contain well-implemented
abstract data structures. For loops that access data structures, it generates sym-
bolic bound expressions in terms of numerical properties of the data structures
and user-deﬁned quantitative functions. Generating symbolic bounds depends
on the generation of loop invariants.
SPEED, however, requires user-provided speciﬁcations, does not support
complex heap conﬁgurations (e.g., transitive reachability) in the precondition
of the analyzed method, and in some cases outputs only an approximate loop
bound functions.
The incremental bounded model checker NBIS [11] can be used to compute
loop upper bounds. It instruments every loop in a given C/C++ code with an
unrolling assertion that checks whether the loop can iterate beyond the cur-
rent loop bound. The encoding of the code along with the unrolling assertions
is checked for satisﬁability. A satisﬁable instance denotes an execution trace in
which a loop iterates more times than its current bound. That loop is then
unrolled according to the newly-found bound and the process starts over. How-
ever, such an approach has the following three drawbacks. (1) It does not ter-
minate in case of non-terminating loops. (2) It does not always return the least
upper bound, because the trace corresponding to a satisfying instance stops the
execution when exiting the loop with a new bound, hence the code following

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
161
the loop is ignored.5 Preventing the trace from stopping is not possible, since it
requires an encoding that does not depend on the loops being unrolled (since the
needed number of unrolling is unknown prior to invoking the satisﬁability pro-
cedure),6 (3) This approach reduces the necessary conﬁdence in the correctness
of the analyzed code. The over-approximated loop upper bounds result in many
unreachable paths after loop unrolling. Hence, the formulas that are translated
from the unreachable paths may overload the underlying solver and cause the
veriﬁcation process to fail. It might be due to the computed upper bound of
the loop is too high or the user-provided class bounds are too big. When the
veriﬁcation engineer uses smaller class bounds than those in the previous run
to re-compute the loop upper bounds, the new loop upper bounds still may be
too high, since the computation ignores the code and speciﬁcations following the
loop under consideration. If the engineer arbitrarily selects smaller loop upper
bounds, not all inputs concerning the class bounds are completely analyzed, thus
the correctness of the code is not guaranteed for the class bounds. Consequently,
although this iterative approach is successful for terminating loops in the absence
of class bounds and speciﬁcations, it is not applicable in the context of bounded
program veriﬁcation since the class bounds are necessary and the conﬁdence in
the correctness of the code is not guaranteed for the class bounds.
7
Conclusion
We have presented an approach for computing exact loop bounds of a given
loop for bounded inputs. Such an analysis is particularly useful for bounded
program veriﬁcation in which the user has to provide bounds on both the size
of objects and the number of loop unrollings. Our approach provides the user
with insight on what loop bounds to consider and enhances the conﬁdence in the
correctness of the analyzed programs. We focus on data-structure-rich programs
and support arbitrary conﬁgurations of the objects on a heap. We translate
the Java code and its JML speciﬁcations (excluding the postconditions of the
entry method) into an SMT formula and solve it using an SMT solver that
can solve optimization problems. We compared our approach with another one
that incrementally unrolls a loop and checks whether the loop condition still
holds after the last unrolled iteration. Experiments show that our method indeed
produces the exact bounds, whereas the other method computes lower and upper
5 For instance, when each iteration of the loop allocates one instance of class A and the
code after the loop allocates 2 objects of type A. If bound(A) = 5, no valid execution
of the code (with respect to class bounds) can iterate the loop more than 3 times,
whereas the computed upper bound will be 5 when ignoring the code after the loop.
6 An alternative checking whether the trace is valid with respect to the class bounds
by executing (symbolically or dynamically) the whole code. Invalidity of the current
instance, however, does not necessarily mean that the newly-found loop bound is
impossible; it may still be that another satisfying instance can be valid and gives a
higher loop bound. Thus, in the worst case, such a validity check requires enumer-
ating all possible satisfying instances, which makes the approach impractical.

162
T. Liu et al.
bounds, which not necessarily are the exact ones. Our approach can assist the
bounded program veriﬁcation engineers to obtain complete conﬁdence in the
veriﬁcation process. Although our analysis is not guaranteed to produce a deﬁnite
outcome (due to the undecidability of the target logic), our experiments show
that in practice the unknown outcome occurs for higher input bounds and not
for the small bounds that are typically used in bounded program veriﬁcation.
Bounded veriﬁcation techniques unroll not only loops but also recursive meth-
ods. Currently we handle loops only; computing bounds for recursion is left for
future work. To improve scalability, we will optimize the encoding formulas using
quantify elimination techniques, e.g., symmetry breaking and pattern matching.
We will also study the application of our approach to other areas, e.g., worst-case
execution time and dynamic heap consumption program analysis.
References
1. Ahrendt, W., Beckert, B., Bubel, R., H¨ahnle, R., Schmitt, P.H., Ulbrich, M. (eds.):
Deductive Software Veriﬁcation – The KeY Book: From Theory to Practice. LNCS,
vol. 10001. Springer, Heidelberg (2016). doi:10.1007/978-3-319-49812-6
2. Barrett, C., Fontaine, P., Tinelli, C.: The SMT-LIB standard: Version 2.5. Tech-
nical report, The University of Iowa (2015)
3. Bjørner, N., Phan, A.-D., Fleckenstein, L.: vZ - an optimizing SMT solver. In:
Baier, C., Tinelli, C. (eds.) TACAS 2015. LNCS, vol. 9035, pp. 194–199. Springer,
Heidelberg (2015). doi:10.1007/978-3-662-46681-0 14
4. Blanc, R., Henzinger, T.A., Hottelier, T., Kov´acs, L.: ABC: algebraic bound com-
putation for loops. In: Clarke, E.M., Voronkov, A. (eds.) LPAR 2010. LNCS, vol.
6355, pp. 103–118. Springer, Heidelberg (2010). doi:10.1007/978-3-642-17511-4 7
5. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
93–107. Springer, Heidelberg (2013). doi:10.1007/978-3-642-36742-7 7
6. Cullmann, C., Martin, F.: Data-ﬂow based detection of loop bounds. In: WCET.
OASICS, vol. 6. Schloss Dagstuhl (2007)
7. Dennis, G.D.: A Relational Framework for Bounded Program Veriﬁcation. Ph.D.
thesis, MIT (2009)
8. Gulavani, B.S., Gulwani, S.: A numerical abstract domain based on expression
abstraction and max operator with application in timing analysis. In: Gupta, A.,
Malik, S. (eds.) CAV 2008. LNCS, vol. 5123, pp. 370–384. Springer, Heidelberg
(2008). doi:10.1007/978-3-540-70545-1 35
9. Gulwani, S., Jain, S., Koskinen, E.: Control-ﬂow reﬁnement and progress invariants
for bound analysis. In: PLDI, pp. 375–385. ACM (2009)
10. Gulwani, S., Mehra, K.K., Chilimbi, T.M.: SPEED: precise and eﬃcient static
estimation of program computational complexity. In: POPL, pp. 127–139. ACM
(2009)
11. G¨unther, H., Weissenbacher, G.: Incremental bounded software model checking.
In: SPIN, pp. 40–47. ACM (2014)
12. Jackson, D.: Software Abstractions: Logic, Language and Analysis. MIT, Cam-
bridge (2016)
13. Leavens, G.T., Baker, A.L., Ruby, C.: Preliminary design of JML: a behavioral
interface speciﬁcation language for java. ACM SIGSOFT Softw. Eng. Notes 31(3),
1–38 (2006)

Computing Exact Loop Bounds for Bounded Program Veriﬁcation
163
14. Li, Y., Albarghouthi, A., Kincaid, Z., Gurﬁnkel, A., Chechik, M.: Symbolic opti-
mization with SMT solvers. In: POPL, pp. 607–618. ACM (2014)
15. Liu, T., Nagel, M., Taghdiri, M.: Bounded program veriﬁcation using an SMT
solver: a case study. In: ICST, pp. 101–110. IEEE (2012)
16. Lokuciejewski, P., Cordes, D., Falk, H., Marwedel, P.: A fast and precise static loop
analysis based on abstract interpretation, program slicing and polytope models. In:
CGO, pp. 136–146. IEEE (2009)
17. Ma, F., Yan, J., Zhang, J.: Solving generalized optimization problems subject to
SMT constraints. In: Snoeyink, J., Lu, P., Su, K., Wang, L. (eds.) AAIM/FAW
-2012. LNCS, vol. 7285, pp. 247–258. Springer, Heidelberg (2012). doi:10.1007/
978-3-642-29700-7 23
18. Michiel, M.D., Bonenfant, A., Cass´e, H., Sainrat, P.: Static loop bound analysis
of C programs based on ﬂow analysis and abstract interpretation. In: RTCSA, pp.
161–166. IEEE (2008)
19. Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R., Rehof,
J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg (2008).
doi:10.1007/978-3-540-78800-3 24
20. Sebastiani, R., Tomasi, S.: Optimization in SMT with LA (Q) cost functions. In:
Gramlich, B., Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS, vol. 7364, pp.
484–498. Springer, Heidelberg (2012). doi:10.1007/978-3-642-31365-3 38
21. Sebastiani, R., Trentin, P.: OptiMathSAT: a tool for optimization modulo theories.
In: Kroening, D., P˘as˘areanu, C.S. (eds.) CAV 2015. LNCS, vol. 9206, pp. 447–454.
Springer, Cham (2015). doi:10.1007/978-3-319-21690-4 27
22. Shkaravska, O., Kersten, R., van Eekelen, M.: Test-based inference of polynomial
loop-bound functions. In: PPPJ, pp. 99–108. ACM (2010)
23. Vall´ee-Rai, R., Co, P., Gagnon, E., Hendren, L.J., Lam, P., Sundaresan, V.: Soot
- a Java bytecode optimization framework. In: CASCON, p. 13. IBM (1999)
24. Vaziri, M.: Finding Bugs in Software with a Constraint Solver. Ph.D. thesis, MIT
(2004)
25. Termination problems data base (TPDB). http://termination-portal.org/wiki/
TPDB. Accessed June 2017

AndroidLeaker: A Hybrid Checker for Collusive
Leak in Android Applications
Zipeng Zhang1,2 and Xinyu Feng1,2(B)
1 School of Computer Science and Technology,
University of Science and Technology of China, Hefei, China
zhangzpp@mail.ustc.edu.cn, xyfeng@ustc.edu.cn
2 Suzhou Institute for Advanced Study,
University of Science and Technology of China, Suzhou, China
Abstract. Android phones often carry sensitive personal information
such as contact books or physical locations. Such private data can be
easily leaked by buggy applications by accident or by malicious applica-
tions intentionally. Much work has been proposed for privacy protection
in Android systems, but there still lacks eﬀective approaches to prevent
information leak caused by Inter-Component Communication (ICC).
We present AndroidLeaker, a new hybrid analysis tool of privacy pro-
tection based on taint analysis for Android applications to prevent the
privacy leak caused by multiple application cooperation. Our approach
combines static analysis and dynamic checking. Static analysis is used to
check the information leak in the individual applications and dynamic
checking at runtime is responsible for preventing the information leak
caused by cooperation of multiple applications. Such a combination may
eﬀectively reduce the runtime overhead of pure dynamic checking, and
reduce false alarms in pure static analysis.
Keywords: Taint analysis · Security · Information ﬂow control ·
Android
1
Introduction
With the growth of smart phone applications, more and more private infor-
mation, such as contact books, physical locations etc., are stored in the phone.
Users are now faced with the high risk of these applications stealing their privacy.
There is already a lot of work to detect the privacy leak in Android applications,
but there are two problems that have not been solved very well:
1. The deﬁnition of privacy leak is over-restricted. Current tools [1,4,5,13,14,16,
20,21] generally regard all behaviors that send privacy outside the device as
the information leak, such as sending contact books to the Internet. However,
This work is supported in part by grants from National Natural Science Foundation
of China (NSFC) under Grant Nos. 61632005 and 61379039.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 164–180, 2017.
https://doi.org/10.1007/978-3-319-69483-2_10

AndroidLeaker
165
only the privacy sending that a user disallows should be recognized as privacy
leak. For example, a taxi booking application like Uber may send your location
to its server to help send the nearest vehicle for you. Such behavior should
not be viewed as the leakage.
In addition, those tools [1,9,21] that analyze each application separately may
regard all program points that send the privacy to another application as the
leak points, even though the privacy may not leave the system. It prevents
benign cooperation among applications.
2. The lack of eﬀective approaches to prevent collusive leak. Generally, there are
two kinds of mechanisms to prevent collusive leak:
(a) Analyzing the application interactions statically. For example, Epicc [11]
infers the possible interactions among applications by analyzing the con-
ﬁguration ﬁles AndroidManifest.xml of Android applications. The com-
ponents that can receive messages from another application or be invoked
by another application must be declared in the conﬁguration ﬁles explic-
itly. Analysis of the application code may ﬁnd the program points that
perform ICC. By combining these two kinds of information, an interac-
tion graph among applications is established. But this kind of graph may
include lots of interactions that may never happen, thus introduce lots of
false alarms.
(b) Tracking the contents of messages sent among applications dynamically.
TaintDroid [4] and TaintART [14] add the support for tracking informa-
tion ﬂow in Android operating system. Therefore the privacy accessed
by an application or sent from each other can be detected at runtime.
But they do not track the implicit channel, such as the information ﬂow
dependent on the control ﬂow. Moreover, since the operating system needs
to perform the information ﬂow tracking for each instruction, especially
when even the data contains no privacy, considerable time and memory
overheads are introduced.
To address the ﬁrst problem, we extend the existing Android permission
model by introducing a new kind of permission, send permission. Send permission
is a collection of private information that an application is allowed to send outside
of the device. For example, if an application declares its send permission as
{Location}, then sending the location outside is not considered as information
leak. Conversely, sending the contact book to the Internet is recognized as leak
because of the lack of such send permission.
In contrast, the existing Android permission model is based on access control
and is more coarse-grained. If an application needs to access some private infor-
mation, then it needs to declare the request of the corresponding permission in
the conﬁguration ﬁle. The permissions are granted at the time of installation, if
the user agrees. However, if the user does not grant it the permissions, for the
concern that it may disclose the privacy, then the corresponding functionalities
cannot be used. For example, an input method may need to access the contact
book to provide hints when the user types his friend’s name. If the input method
is not authorized to read the contact book, then the user is not able to use this

166
Z. Zhang and X. Feng
1
// action is "com.test.action.MY_ACTION"
2
public
class
MyReceiver
extends
BroadcastReceiver {
3
public
void
onReceive(Context
context , Intent
intent ){
4
String ip = intent. getStringExtras ("ip");
5
String
number = getContact("Alice");
6
try{
7
Socket
socket = new
Socket(ip , "8080");
8
OutputStream
out = socket.getOutputStream ();
9
DataOutputStream
wrt = DataOutputStream (out);
10
wrt.writeUTF(number );
11
} catch(IOException e){...}
12
}
13
14
private
String
getContact(String
name ){
15
// get the phone
number of a given
name
16
}
17
}
(a)
1
public
class
MyActivity
extends
Activity{
2
public
void
onCreate(Bundle
savedInstanceState ){
3
...
4
Intent
intent = new
Intent ();
5
intent.setAction("com.test.action.MY_ACTION");
6
intent.putExtra("ip", "192.168.1.1");
7
sendBroadcast(intent );
8
}
9
}
(b)
Fig. 1. An example showing collusive leak
convenient feature. We can use the send permission to address this dilemma. An
input method may declare its send permission as {}, i.e., it does not send any
private information outside. Then the user can safely grant the access permission
of contact book to it, without worrying that their privacy information is leaked.
Before the discussion of the solution of the second problem, we ﬁrst show
an example in Fig. 1, where an untrusted application sends the privacy out with
the help of another trusted application. Application (a) is a trusted application,
and it is authorized to access the contact book. It receives an IP address from
another application and then sends the contact book to that address. Here we
assume that it only sends out the phone number of Alice. Application (b) may
be a malicious application, and it is not allowed to access any conﬁdential data.
It sends a non-conﬁdential message to application (a), which contains an IP
address and a port number. With the help of application (a), application (b)
sends the contact book to the speciﬁc address successfully, although it does not
access any privacy directly. The static analysis tools before [1,3,11] do not detect
this kind of leakage, because application (b) itself never access any conﬁdential

AndroidLeaker
167
data, let alone leak the privacy. The dynamic analysis tools [4,14] cannot ﬁnd
this leak either, because they just inform the user that application (a) sends the
contact book to the network. Since application (a) is trusted, so the user will
not prevent this privacy sending.
To address this problem, we propose a combination of static and dynamic
checking, which can reduce the runtime overhead of pure dynamic checking and
reduce the false alarms in pure static analysis. In the static analysis, we check
the information leak generated by individual applications, and collect some infor-
mation for dynamic analysis. In the dynamic analysis, communications among
applications are checked and those which may cause privacy leak are prevented.
Figure 2 shows the overall architecture of our method. During the static
phase, our tool takes the Android application package (APK) as the input, and
read the permissions from its conﬁguration ﬁle. Then it analyzes the possible
privacy leaks in the application and outputs the leak points if they are found. If
no leak is found, it generates the security labels of the outgoing messages and a
dependent send permission, which are taken as the input by the instrumentation
phase. The dynamic checking is performed by instrumentation. The communica-
tions among applications ICC is inspected at runtime, and the interaction that
may result in the privacy leak is prevented. The leak behavior is also recorded.
Fig. 2. The overall architecture
Our work makes the following contributions:
1. We introduce a novel permission, send permission, to describe the collection
of private information that an application is allowed to send out. On the
one hand, the user can allow the application to access the conﬁdential data
without worrying about privacy leak. On the other hand, a secure application
that must access conﬁdential data can declare the send permission as low as
possible to show it is indeed trustworthy.
2. We develop a new hybrid analysis tool AndroidLeaker to prevent the informa-
tion leak caused by the cooperation of multiple applications. We can reduce
false-alarms in pure static analysis and reduce the runtime overhead of pure
dynamic checking.
The rest of this paper is organized as follows. Section 2 presents the new per-
missions and gives the key ideas of the static analysis and dynamic analysis.

168
Z. Zhang and X. Feng
Section 3 discusses the implementation of AndroidLeaker and shows some impor-
tant details about the taint analysis. Section 4 shows the evaluate results for
AndroidLeaker. Section 5 compares our method with related work and concludes.
2
Our Approach
We ﬁrst give the extension of the Android permission model, and then introduce
the static checking rules and dynamic checking based on the extension.
2.1
The Policy
Each application is associated with a security policy θ, which represents the
permissions to access and spread the privacy. As shown in Fig. 3, a security
policy is a triple (gap, gsp, dsp). gap is the permission to access the privacy,
similar with the access permission in Android. Unlike in Android, gap not only
limits the access to privacy directly through system calls, but also restricts the
application from getting privacy by inter-component communication. gsp is the
permission to send the privacy out. Only the private information described in gsp
are allowed to be sent outside. gsp is usually the subset of gap, because a privacy
can be sent out only when it can be accessed ﬁrst. gap and gsp are provided by
the programmer, and are declared in the Android conﬁguration ﬁle. The third
part dsp is the dependent send permission. It is generated in the static analysis
phase and used to help check the communications among applications. gap, gsp
and dsp are all labels. A label is a set of variables, each representing a kind of
private information. All labels form a partial lattice (L, ⊑), with the top element
⊤containing all the private information. L1 ⊑L2 means that L1 is lower than
L2, i.e., L1 is a subset of L2. L1 ⊔L2 is the least upper bound of L1 and L2.
Dependent send permission dsp speciﬁes the private information that an
application may send out when requested by other applications. It is a reﬁne-
ment of the send permission gsp. There is a dependent send behavior in Appli-
cation (a) in Fig. 1, because whether it sends the contact book to the network
depends on the messages from other applications. So the dependent send per-
mission of Application (a) is {ContactBook}. On the contrary, there is no risk
to allow an untrusted application to communicate with a privileged application
with empty dependent send permission. Generally, the dependent send permis-
sion dsp is lower than the send permission gsp, (i.e., dsp ⊑gsp) — An application
disallowed to send out the private information cannot send out that privacy for
others.
(Labels) L, π, gap, gsp ∈P(Vars)
(Policy)
θ
::= (gap, gsp, dsp)
L1 ⊑L2 iﬀL1 ⊆L2 ∨L2 = ⊤
L1 ⊔L2 =

⊤
if L1 =⊤or L2 =⊤
L1 ∪L2
otherwise
Fig. 3. The label and policy

AndroidLeaker
169
As shown in Fig. 1, the security policy of Application (a) is declared as
({ContactBook}, {ContactBook}, {ContactBook}), which means it may read
the contact book, send it out, and also send it out for other applications. The
security policy of Application (b) is ({}, {}, {}). Because it cannot access any
privacy, its send permission and dependent send permission must be {} too.
In the following parts, we present the key checking rules for static analysis
and dynamic analysis.
2.2
Static Analysis
Representative taint analysis rules are shown in Fig. 4. The judgement is in
the form of π, θ ⊢{Γ} c {Γ ′}. The pc label π represents the security label of
the program counter, which is used to check the implicit information channel
dependent on control ﬂows. θ is the security policy. Γ is the type environment
which is a partial mapping from variables to labels, i.e., Γ ∈Vars ⇀Labels. It
stores the private information that may be contained in diﬀerent variables. Γ and
Γ ′ represent the type environments of variables before and after the execution
of the instruction c respectively. Here variables are divided into two classes:
the local variable p and shared variable s. The former is variables declared in an
application, and the latter represents the private information maintained globally
by the Android system and shared across applications. The judgement Γ ⊢e : L
means that the label of expression e is L under the given type environment Γ.
The judgement Γ ⊢b : L for the boolean expression b is similar. Note that the
dependent send permission dsp in the policy θ is not used in the static checking.
It is inferred in the static checking phase and used in the dynamic checking.
To simplify the discussion, we use some primitives to model the actual oper-
ations in Android (shown in Fig. 5). Primitives recv and send are used to
model the inter-component communication operations. An application can send
a broadcast to another application, or start an activity or a service in another
application. All these can be viewed as a simple message-passing style inter-
process communication. The message is called Intent in Android. p := recv()
π, (gap, gsp, ) ⊢{Γ} p := recv() {Γ[p 
→gap]}
(Recv)
π, θ ⊢{Γ} send(e1, e2) {Γ} (Send)
Γ ⊢e : Le
π ⊔Le ⊑gsp
π, (gap, gsp, ) ⊢{Γ} out(e) {Γ} (Out)
s ∈gap
π, (gap, gsp, ) ⊢{Γ} p := get(s) {Γ[p 
→π⊔{s}]} (Get)
Γ ⊢b : Lb
π ⊔Lb, θ ⊢{Γ} {ci} {Γ ′
i}
i = 1, 2
π, θ ⊢{Γ} if b then c1 else c2 {Γ1 ⊔Γ2}
(If)
Fig. 4. Selected rules for taint analysis

170
Z. Zhang and X. Feng
Primitives
Operations
p := recv()
onReceive(), getIntent(), onActivityResult(), . . .
send(e1, e2)
sendBroadcast(), startActivity(), startActivityForResult(), . . .
out(e)
sendTextMessage(), openConnection(), connect(), . . .
p := get(s)
managedQuery(), getDeviceId(), getLatitude(), . . .
Fig. 5. The primitives
gets the intent from another application and stores it in p. send(e1, e2) sends
an intent, represented by expression e2 here, to an application identiﬁed by
expression e1. Primitive out(e) is used to model the system calls to send the
information e outside the device through Internet or text messages, etc. It is
the only way that the information can leave the system. Primitive p := get(s)
is used to model the operations to read the private information (modeled as
global variables, as explained before), such as the contact book, location, short
messages etc. It reads the private information s into the local variable p.
As shown in Fig. 4, the Recv rule assigns the label gap to the variable p
at the end of the receive command. This is a conservative estimation since the
actual label of the incoming message is unknown statically. It is safe because
the dynamic checking for send ensures that the label of the outgoing message
should be lower than the receiver’s gap. The checking for send primitive is done
at runtime (explained below), so the static checking does nothing (see the Send
rule). The Out rule says that, to output the private information, an application
needs to have the corresponding send permission. The Get rule says that the
access of the private information represented as the shared variable s requires
the corresponding access permission, i.e., s ∈gap. It assigns the label π ⊔{s} to
the local variable p at the end to record the implicit information ﬂow through
the control ﬂow. The If rule shows how the implicit information ﬂow is tracked.
The label of the condition b is joined with the pc label π. The resulting type
environment is the union of the resulting type environments of two branches. The
union of two type environments Γ1 ⊔Γ2 is deﬁned as {p ; (Γ1(p) ⊔Γ2(p)) | p ∈
dom(Γ1)} (note dom(Γ1) = dom(Γ2)). The checking rules for other statements
are standard and are omitted here.
Dependent Send Permission. Other than the taint analysis, we also infer the
dependent send permission in the static phase. Each application declares the
methods which can be invoked by other applications in its conﬁguration ﬁle.
The output of private information in these exposed methods should be declared
in the dependent send permission. As an example, the onReceive function of
Application (a) in Fig. 1 is one such method. To infer the dependent send per-
mission, we traverse these exposed methods only and apply the following rule
for each out command:
Γ ⊢e : Le
π, Γ ⊢{dsp} out(e) {dsp ⊔Le ⊔π}

AndroidLeaker
171
We start from an empty label {} as the initial dsp. The type environment Γ and
the pc label π is computed in the taint analysis previously.
2.3
The Instrumentation and the Dynamic Analysis
The send primitive is the only program point which needs to be dynamically
checked. The dynamic checking is implemented by code instrumentation, which
inserts permission checking code before each corresponding system call. The
system call is executed only if the checking is passed. The following rule shows
the checking for each send primitive.
Γ ⊢e2 : L2
L = L2 ⊔π
Θ(cid) = (gapa, gspa, dspa)
Θ(e1) = (gapb, gspb, dspb)
L ⊑gapb
L ̸⊑gspa ⇒L ̸⊑gspb
dspb ⊑dspa
π, Γ, Θ ⊢send(e1, e2)
π and Γ are the pc label and the type environment derived from the static
taint analysis. Θ is a mapping from application identiﬁers to the policy set, i.e.,
Θ ::= {t1 ; θ1, . . . , tn ; θn}. It stores policies of all applications and can be
globally accessed. (gapa, gspa, dspa) and (gapb, gspb, dspb) are the policies of the
sender and the receiver respectively, which are acquired from Θ. The ﬁrst line in
the premise computes the label L of the outgoing message. It is the union of the
label L2 of message e2 and the pc label π. The second line shows the constraints
for send primitive.
To show the validity of the checking, we ﬁrst analyze the possible ways that
one application may leak privacy with the help of another application.
Figure 6 shows four cases in which the communication between two appli-
cations may result in the privacy leak. The three sets above the applications
are their policies. The policies of application A and application B are referred as
(gapa, gspa, dspa) and (gapb, gspb, dspb) respectively in the following explanation,
and label L represents the label of the message. In case (a), A has the permis-
sion to access and send out the contact book, while B has neither permissions. A
Fig. 6. Cases of forbidden send

172
Z. Zhang and X. Feng
should not send contact book to B, or B will access the privacy it is not allowed
to access. So all messages an application receives should not contain any privacy
which are not declared in its access permission. The ﬁrst restriction is
L ⊑gapb.
In case (b), A has the permission to access the contact book, but is not
allowed to send the contact book out, while B can send the contact book out.
If A sends contact book to B, then it is possible that B may send out contact
book for A. This case should be forbidden. The second restriction is
L ̸⊑gspa ⇒L ̸⊑gspb.
It means that if the message cannot be sent out by A, then it is disallowed to
be sent to an application which may send it out.
In case (c), A is not authorized to send out the contact book, and B may
send out the contact book depending on the message it receives. Application (a)
in Fig. 1 shows such a case. Any messages should be forbidden to be sent from A
to B, when even the message does not contain any privacy. Therefore, the third
restriction is
dspb ⊑gspa.
This restriction forbids A from leaking conﬁdential data by invoking B. Because
A can only call B to send the conﬁdential data which is described in dspb, so A
must have the permission to send the data which dspb describes.
The above three constraints, however, are not suﬃcient to prevent leakage.
Consider the following example:
A sends a non-conﬁdential data m to B, and then B just passes m to C
directly. The communications between A and B, and that between B and C, do
not violate these three restrictions above, but A sends a message to C indirectly,
which violates the restriction shown in case (c).
This example explains why sending message from A to B in case (d) in Fig. 6
should be forbidden. We require that if A sends messages to B which may send
conﬁdential data dependently, then A must declare the corresponding dependent
send permission. This is our fourth constraint:
dspb ⊑dspa
From the fourth condition and the property that dspa ⊑gspa, we can derive
the third condition.

AndroidLeaker
173
In summary, the condition to perform the send operation successfully is
L ⊑gapb
and
L ̸⊑gspa ⇒L ̸⊑gspb
and
dspb ⊑dspa
This is what we need to check at runtime.
3
The Implementation
AndroidLeaker is based on the Soot framework [15], which converts Android
Dalvik bytecode into the call graph and provides usefull basic data ﬂow analysis
framework. We use Heros [2], a generic implementation of inter-procedural, ﬁnite,
distributive subset(IFDS) problems [12] solver, to perform the inter-procedural
data ﬂow analysis. In the following parts, we introduce the concrete implementa-
tion of static and dynamic analysis, and then give some important details about
the actual taint analysis and the instrumentation.
3.1
The Overall Architecture
The overall architecture of our static analysis is shown in Fig. 7. Before the taint
analysis, we need to collect the following information:
1. Sources and sinks: Android APIs, including those serving as sources and sinks,
change frequently with the upgrade of Android. To adapt to the change,
we allow users to customize the source and sink functions. The component
SourceSinkParser parses the user-deﬁned sources and sinks.
2. Callback functions: Callback functions are not invoked directly in the pro-
gram. To analyze the call back functions, we treat them as entries of the
program. The procedure CallbackAnalysis is used to ﬁnd all the callbacks
the program registered. There are two kinds of callbacks: those registered in
the program and those declared in the conﬁguration ﬁles.
Fig. 7. The concrete architecture of static analysis

174
Z. Zhang and X. Feng
3. Alias: Alias information is needed in order to handle information ﬂow through
object ﬁelds. Alias Analysis traverses the program and generates the alias
information.
4. Entries and permissions: The entry points and required permissions of the
application are declared in the conﬁguration ﬁle AndroidManifest.xml, which
are extracted by the component ManifestParser. The callbacks return by
Call Analysis are also considered as entries. The permissions include the
access permissions and the send permissions.
The information collected above are fed into TaintAnalysis. To improve
the precision, our analysis is ﬂow-sensitive, context-sensitive, ﬁeld-sensitive and
object-sensitive. The TaintAnalysis generates the security labels of the outgo-
ing messages and the dependent send permission, which are used by the phase
Instrumentation to instrument the dynamic checking code into the application.
The TaintAnalysis also records the dependent send permission in the conﬁgu-
ration ﬁle AndroidManifest.xml. Finally, the instrumented application is sent to
the android application signer to obtain a signed version, which can be installed
in the phone.
Fig. 8. The architecture of dynamic analysis
The dynamic checking model is shown in Fig. 8. To check the permissions
at the inter-component communication program points, both permissions of the
sender and the receiver should be known (recall the dynamic checking for send
primitive in Sect. 2.3). Therefore, the policy of each application should be avail-
able at runtime. PermissionManager is responsible for providing the policies of
speciﬁc applications. It collects the permissions of all installed applications by
inquiring the operating system, since the system stores the permissions declared
in the application conﬁguration ﬁle at installation time. Before sending a mes-
sage to another application, the application ﬁrst queries the policy of the target
application from PermissionManager, then checks the permissions. If the check
fails, the corresponding inter-component communication is abandoned.
3.2
Implicit Flow Caused by Exception Handling
Similar to the condition statement and the loop statement, the exceptions may
inﬂuence the control ﬂow of the program as well, thus generating implicit chan-
nels. Figure 9 shows such an example. In the main function, it reads a private
data into variable x ﬁrst, and then calls function f with the parameter x. The

AndroidLeaker
175
Fig. 9. An example for implicit ﬂow due to exception
function f decides whether to throw an exception or return normally depending
on the value of x. After the execution of function f, if f throws an exception, then
y is set to 1, otherwise y remains 0. Although x is not assigned to y directly, we
can know whether x is higher than 0 by testing the value of y.
Part (b) in Fig. 9 shows the control ﬂow graph of program (a). The control
ﬂow graph of function f is expanded and embedded into that of the function
main. In the expanded control ﬂow graph, it is clear to see that the value of y
depends on the value of x.
To prevent this kind of implicit channels, when a branch node is analyzed,
we compute its immediate post-dominator [7], and push a new pc label on the
stack, along with the node’s immediate post-dominator. When the immediate
post-dominator is reached, the pc label is popped. Since the inter-procedural
analysis is too time-consuming, we use on-the-ﬂy intra-procedural static analysis
of immediate post-dominator. If the immediate post-dominator of a branch node
inside its function is null, then there are two cases: (a) if this function does
not throw an exception, then its immediate post-dominator should be the exit
point of the function; (b) otherwise, its immediate post-dominator should be
the immediate post-dominator of the calling point. For example, the immediate
post-dominator of node 1⃝in Fig. 9 is node 2⃝, because function f, to which
node 1⃝belongs, throws an exception and the immediate post-dominator of the
calling point of f, i.e., line 5 in program (a), is node 2⃝.

176
Z. Zhang and X. Feng
4
Experimental Evaluation
Precision for individual applications. We test 82 examples from DroidBench, a
test suite released with FlowDroid [1] to assess the taint analyses. It does not
contain any test cases with privacy leak caused by communication between appli-
cations. Instead, it regards all communication points as sinks. Correspondingly,
to test the examples in AndroidLeaker, we set the send permission of appli-
cations to be {} by default. Since Android lifecycle and Java threading have
not been considered in our implementation yet, and the support of Android
callbacks is limited, we just test the other 10 categories, as shown in Table 1.
Compared with FlowDroid (version 1.5), AndroidLeaker gets a slightly lower
precision (82%), and a slightly higher recall (69%). Most of the missed leaks are
due to the lack of support for some Java and Android features, such as static
initialization, unreachable code and application modeling etc.
Inter-Component Communication. We do not know any standard test suite to
test the information leak caused by application cooperation, so we develop four
sample applications to test the communication between applications, and com-
pare AndroidLeaker with FlowDroid [1]. Table 2 shows the results for the privacy
leak test caused by inter-component communication.
Each example contains two applications: one sends a broadcast (by call-
ing sendBroadcast, a representative inter-component communication method
in Android), and the other receives it. In the ﬁrst two examples, the sender A
sends the device ID d (private data) to the receiver B, and B sends anything it
Table 1. DroidBench test results
App category
AndroidLeaker FlowDroid
⊗
∗
◦
⊗
∗◦
EmulatorDetection
3
0
0
2
0 1
InterComponentCommunication 10 0
7
14 1 3
GeneralJava
13 1
9
12 1 8
Reﬂection
3
0
1
1
0 3
Aliasing
0
1
0
0
1 0
ArraysAndLists
3
4
0
2
4 1
ImplicitFlows
3
0
1
0
0 4
AndroidSpeciﬁc
7
0
3
7
0 3
FieldAndroidObjectSensitivity
2
4
0
2
0 0
InterAppCommunication
2
0
1
3
0 0
Total
46 10 21
43 7 23
Precision=⊗/(⊗+ ∗)
82%
86%
Recall= ⊗/(⊗+ ◦)
69%
65%
⊗= correct warning ∗= false warnings ◦= missed leaks.

AndroidLeaker
177
Table 2. The privacy leak test for inter-component communication
Example
Sender’s policy
Receiver’s policy
AndroidLeaker
FlowDroid
Expect
broadcastTest1
({d},{d}, {})
({d},{d},{})
No leak
Leak
No leak
broadcastTest2
({d},{d}, {})
({},{}, {})
Leak
Leak
Leak
broadcastTest3
({},{},{})
({d},{d}, {d})
Leak
No leak
Leak
broadcastTest4
({},{},{})
({},{}, {})
No leak
No leak
No leak
receives to the Internet. Whether there is information leak or not depends on the
policies of these two applications. If B has the permission to access and send out
the device ID, then there is no information leak, as in the broadcastTest1 exam-
ple. Otherwise the send operation of B should be regarded as information leak, as
in the broadcastTest2 example. Since FlowDroid considers all inter-component
communication as sinks, it reports information leak in both examples. Androi-
dLeaker can distinguish the two situations and reports the right results.
The third example is similar with the program in Fig. 1. The sender A sends
an IP address and a port number to the receiver B. B reads the deviceID, and
then sends it to the IP address and the port number it receives. Since B may
send the device ID out for A but A itself has no permission to send, this com-
munication may result in privacy leak. The forth example is a variation of the
third example and the only diﬀerence is that the receiver B does nothing. So it
is safe to allow A to send messages to B. Since the FlowDroid detects nothing
private sent from A to B, it reports no leak for the last two examples. Androi-
dLeaker can distinguish these two cases and report the right results. Compared
with FlowDroid, AndroidLeaker gets lower false positive alarms for leak caused
by inter-component communication.
Runtime Overhead. We test the runtime overhead for inter-component com-
munication on an Android 4.4 phone with the cpu of Qualcomm MSM 8226
(4 cores, 1.2 GHz, 1 G Memory). Since the instrumented checking for all com-
munication APIs are the same, we take the system call sendBroadcast as an
example to test the overhead, because it is asynchronous and directly returns
without user interaction. It takes 0.4 s to run the sendBroadcast for 1000 times,
and 6 s to run the instrumented version. That is AndroidLeaker adds 5.6 ms for
each inter-component communication system call. The overall overhead depends
on the number of the communication system calls when an application runs.
That is, the total overhead is T +N×5.6 ms
T
−1, where T is the execution time
of the original application, and N is the number of the communication sys-
tem calls. By contrast, TaintDroid introduces 14% CPU overhead on average
for each application [4]. If communication happens rarely, AndroidLeaker gets
better performance.

178
Z. Zhang and X. Feng
5
Related Work and Conclusion
Various static analysis techniques have been proposed to detect privacy leak
in Android [1,5,6,8,9,11,17,18,20,21]. CHEX [8] is a tool to locate one kind
of vulnerable applications (hijacking vulnerable applications). An application is
hijacking vulnerable if it exposes an interface to perform some privileged action
such that another application without the permission may access the conﬁdential
data by calling it. In our method, this kind of vulnerable applications can be
also detected, because one application is forbidden to access or send conﬁdential
data on behalf of another unprivileged application.
Leakminer [19] and FlowDroid [1] are both based on the soot framework and
implement the Android lifecycle, but they do not consider the cooperation among
applications and focus on the analysis of one application. Epicc [11] performs a
study of ICC (inter-component communication) vulnerabilities, including send-
ing an Intent that may be intercepted by a malicious component, or exposing
components to be launched by a malicious Intent. This is similar with our concern
with ICC. It analyzes the sensitive data transmission in Android by analyzing
the conﬁguration ﬁle, i.e., AndroidManifest.xml, to infer the communication.
Since Epicc performs a static analysis and adopts a conservative strategy while
we perform dynamic checking when the communication occurs, we may achieve
less false positive.
ComDroid [3] detects the implicit Intents possibly intercepted by malicious
applications and the components exposed to external applications which may
be utilized by the malicious applications to acquire conﬁdential data or perform
some unauthorized actions. It does not concern whether the private information
is contained in Intents or returned by the components.
There are also some works [4,10,14] based on dynamic checking or dynamic
taint analysis. TaintDroid [4] tracks the ﬂow of privacy sensitive data through
third party applications and automatically taints data from privacy-sensitive
sources and tracks the labels when the sensitive data propagates. Aquifer [10]
prevents accidental information inside one application from being stolen by
another application. Both of them do not track the implicit channel depend-
ing on the control ﬂow.
Conclusion. In this paper, we present a new hybrid analysis for Android appli-
cations to prevent the information leak caused by application cooperations. The
combination of the static checking and the dynamic checking can reduce the
overhead at runtime eﬀectively.

AndroidLeaker
179
References
1. Arzt, S., Rasthofer, S., Fritz, C., Bodden, E., Bartel, A., Klein, J., Le Traon, Y.,
Octeau, D., McDaniel, P.: FlowDroid: precise context, ﬂow, ﬁeld, object-sensitive
and lifecycle-aware taint analysis for android apps. In: PLDI 2014, pp. 259–269
(2014)
2. Bodden, E.: Inter-procedural data-ﬂow analysis with IFDS/IDE and soot. In:
SOAP 2012, pp. 3–8 (2012)
3. Chin, E., Felt, A.P., Greenwood, K., Wagner, D.: Analyzing inter-application com-
munication in android. In: Proceedings of the 9th International Conference on
Mobile Systems, Applications, and Services, pp. 239–252 (2011)
4. Enck, W., Gilbert, P., Chun, B.G., Cox, L.P., Jung, J., McDaniel, P., Sheth, A.N.:
TaintDroid: an information-ﬂow tracking system for realtime privacy monitoring
on smartphones. In: OSDI 2010, pp. 1–6 (2010)
5. Gibler, C., Crussell, J., Erickson, J., Chen, H.: AndroidLeaks: automatically detect-
ing potential privacy leaks in android applications on a large scale. In: Katzen-
beisser, S., Weippl, E., Camp, L.J., Volkamer, M., Reiter, M., Zhang, X. (eds.)
Trust 2012. LNCS, vol. 7344, pp. 291–307. Springer, Heidelberg (2012). doi:10.
1007/978-3-642-30921-2 17
6. Kim, J., Yoon, Y., Yi, K., Shin, J.: SCANDAL: static analyzer for detecting privacy
leaks in android applications. In: MoST 2012 (2012)
7. Lengauer, T., Tarjan, R.E.: A fast algorithm for ﬁnding dominators in a ﬂowgraph.
ACM Trans. Program. Lang. Syst. 1(1), 121–141 (1979)
8. Lu, L., Li, Z., Wu, Z., Lee, W., Jiang, G.: CHEX: statically vetting android apps
for component hijacking vulnerabilities. In: CCS 2012, pp. 229–240 (2012)
9. Mann, C., Starostin, A.: A framework for static detection of privacy leaks in
android applications. In: Proceedings of the 27th Annual ACM Symposium on
Applied Computing, SAC 2012, pp. 1457–1462 (2012)
10. Nadkarni, A., Enck, W.: Preventing accidental data disclosure in modern operating
systems. In: CCS 2013, pp. 1029–1042 (2013)
11. Octeau, D., McDaniel, P., Jha, S., Bartel, A., Bodden, E., Klein, J., Le Traon,
Y.: Eﬀective inter-component communication mapping in android with Epicc: an
essential step towards holistic security analysis. In: SEC 2013, pp. 543–558 (2013)
12. Reps, T., Horwitz, S., Sagiv, M.: Precise interprocedural dataﬂow analysis via
graph reachability. In: POPL 1995, pp. 49–61 (1995)
13. Sakamoto, S., Okuda, K., Nakatsuka, R., Yamauchi, T.: DroidTrack: tracking infor-
mation diﬀusion and preventing information leakage on android. In: Park, J.J.J.H.,
Ng, J.K.-Y., Jeong, H.Y., Waluyo, B. (eds.) Multimedia and Ubiquitous Engi-
neering. LNEE, vol. 240, pp. 243–251. Springer, Dordrecht (2013). doi:10.1007/
978-94-007-6738-6 31
14. Sun, M., Wei, T., Lui, J.C.: TaintART: a practical multi-level information-ﬂow
tracking system for android runtime. In: CCS 2016, pp. 331–342 (2016)
15. Vall´ee-Rai, R., Co, P., Gagnon, E., Hendren, L., Lam, P., Sundaresan, V.: Soot: a
java bytecode optimization framework. In: CASCON 2010, pp. 214–224 (2010)
16. Xia, M., Gong, L., Lyu, Y., Qi, Z., Liu, X.: Eﬀective real-time android application
auditing. In: S&P 2015, pp. 899–914 (2015)
17. Xiao, X., Tillmann, N., Fahndrich, M., de Halleux, J., Moskal, M.: User-aware
privacy control via extended static-information-ﬂow analysis. In: ASE 2012, pp.
80–89 (2012)

180
Z. Zhang and X. Feng
18. Xu, R., Sa¨ıdi, H., Anderson, R.: Aurasium: Practical policy enforcement for android
applications. In: Security 2012, pp. 27–27 (2012)
19. Yang, Z., Yang, M.: LeakMiner: detect information leakage on android with static
taint analysis. In: WCSE 2012, pp. 101–104 (2012)
20. Yang, Z., Yang, M., Zhang, Y., Gu, G., Ning, P., Wang, X.S.: AppIntent: analyzing
sensitive data transmission in android for privacy leakage detection. In: CCS 2013,
pp. 1043–1054 (2013)
21. Zhao, Z., Osorio, F.C.C.: TrustDroid: preventing the use of smartphones for infor-
mation leaking in corporate networks through the used of static analysis taint
tracking. In: MALWARE 2012, pp. 135–143 (2012)

Modeling and Verification

Remark on Some π Variants
Jianxin Xue1,2(B), Huan Long2, and Yuxi Fu2
1 College of Computer and Information Engineering,
Shanghai Polytechnic University, Shanghai, China
jxxue@sspu.edu.cn
2 BASICS, MOE-MS Key Laboratory for Intelligent Computing and Intelligent
Systems, Department of Computer Science, Shanghai Jiao Tong University,
Shanghai, China
longhuan@sjtu.edu.cn, fu-yx@cs.sjtu.edu.cn
Abstract. Two π variants that restrict the use of received names are
studied. For either variant the external characterization of the absolute
equality is given using a family of bisimulations; the expressive com-
pleteness of the calculus is established; and a complete equational proof
system is constructed. The relative expressiveness between the two vari-
ants and their relationship to π are revealed in terms of subbisimilarity.
1
Introduction
The π-calculus of Milner, Parrow and Walker [MPW92] has proved to be a
versatile and robust programming language. Being a prime model of interac-
tion, it accommodates the λ-calculus [Mil92,CF11] and is capable of explain-
ing a wide range of phenomena where dynamic reconﬁguration is a fundamen-
tal property [Wal95]. The name-passing communication mechanism of π is so
strong that most of its variations are able to simulate each other both oper-
ationally and observationally to a considerable degree [San93,Tho95,San96a].
It has been an interesting topic to investigate diﬀerent π-variants from diﬀer-
ent viewpoints [Pal03,Gor08,FL10]. The results obtained so far are important
in that they help improve our understanding of the interaction models at both
technical and conceptual levels [Fu16].
There are a number of ways to restrain the power of the π-calculus. Diﬀerent
variants are obtained by considering diﬀerent forms of the choice operator and
the recursion operator. Based on the results on CCS [BGZ03,BGZ04,GSV04],
the relative expressive power of these variants have been examined [Pal03,
Gor08], the most recent results being reported in [FL10]. Less close relatives
are obtained by restricting the usage of the output preﬁx operator. In the pri-
vate π-calculus [San96a], denoted by πP , the exported names are always local
names. Apart from its ability to code up higher order processes, the expres-
sive power of πP is almost unknown. The diﬃculty is partly attributed to the
fact that it has a diﬀerent set of action labels than the other π-variants. In
the asynchronous π-calculus studied in [HT91a,HT91b,Bou92,ACS96] an out-
put preﬁx is detached from any continuation. The reason why this simple syn-
tactic manipulation provides a semantic modeling of asynchrony is explained
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 183–199, 2017.
https://doi.org/10.1007/978-3-319-69483-2_11

184
J. Xue et al.
in [Fu10] using the theory by process. It is our personal view that the theory of
the asynchronous π-calculus is best seen as an example of the theories deﬁned
by the processes [Fu10]. More distant cousins of the π-calculus are the process-
passing calculi [Tho95,San93]. It is shown in [Fu16] however that these models
are too weak from the viewpoint of computation, despite of the result proved
in [LPSS08]. To achieve completeness we need to turn the higher order model
into abstraction passing calculi [San93,XYL15,Fu17].
Instead of restraining the power of the operators, the calculi that impose
conditions on the usage of the received names are the truly proper vari-
ants of the name-passing calculus [FZ15]. In the local π-calculus introduced
in [Mer00,MS04] a received name can never be used as an input channel. A piece
of program that deﬁnes a local subroutine with name say f can be rest assured
that no other programs or subroutines share the name f. This is certainly a
useful safety property and the πL-calculus is deﬁned in this way. Symmetrically
the πR-calculus imposes the condition that a received name cannot appear as an
output channel. A host that intends to send a piece of information to another
site may send a local channel name to the site and upload the information using
the private channel. In this way the host makes sure that it is not at the receiv-
ing end of anything from any aliens. In another variant, called πS-calculus, a
process is not allowed to pass a received name to a third party. This appears as
a more fundamental restriction on the name-passing mechanism. In this model
the dynamic reconﬁguration of the local communication topology is regional. One
can never know a secret about you from a third party. Despite of their practical
signiﬁcance these three important variants have not been systematically studied.
Which aspects of πL, πR, πS should we look into? We cannot claim to under-
stand the calculi if we do not know the relative expressiveness between them and
the π-calculus. This brings up the issue of model independence since the expres-
siveness relationship must be deﬁned irrespectively of any particular model. As
it turned out, the majority results in process theory are about particular mod-
els [Hoa78,Mil89,SW01]. The lack of the emphasis on model independence has
been a blocking factor for the development of the process theory. Theory of Inter-
action proposed in [Fu16] is an attempt to provide a theory for all interaction
models. The fundamental part of Theory of Interaction, the theory of equality,
the theory of expressiveness, and the theory of completeness have been outlined,
and a number of foundational results have been revealed. The applications of
this general approach to the value-passing calculus and the name-passing cal-
culus are reported in [Fu13,FZ15]. We will apply the general methodology of
Theory of Interaction to πL, πR and πS. The observational theory developed in
this manner will help to construct an equational proof system for each of the
variants. As a fallout, we will be able to say more about the theory of πP .
Section 2 deﬁnes the semantics of the π-calculus and the three variants.
Section 3 characterizes for πL and πR the absolute equality in terms of
external bisimulation. Section 4 discusses the relative expressiveness of πL, πR
and π-calculus. Section 5 conﬁrms that all the three variants are legitimate mod-

Remark on Some π Variants
185
els of interaction. Section 6 describes the equational proof systems for πL and
πR. Section 7 takes a look at the private π-calculus. Section 8 concludes.
2
Pi and the Variants
Our deﬁnition of the π-calculus follows the presentation given in [FZ15].
Throughout the paper, we adopt the following notational conventions.
– The small letters a, b, c, d, e, f, g, h from the beginning of the alphabet range
over the inﬁnite set N of the names.
– The lowercase letters u, v, w, x, y, z towards the end of the alphabet range
over the inﬁnite set Nv of the name variables.
– The letters l, m, n, o, p, q in the middle of the alphabet range over N ∪Nv.
We often write c for a name sequence c1, . . . , cn and similarly x for x1, . . . , xn.
In the π-calculus a name received in a communication can be used as either an
output channel, or an input channel, or the content of a further communication.
The π-terms are deﬁned by the following grammar:
T := 0 |

i∈I
n(x).Ti |

i∈I
nmi.Ti | T | T ′ | (c)T | [p=q]T | [p̸=q]T | !n(x).T | !nm.T.
In the πL-calculus a received name cannot be used as an input channel. The
πL-terms are inductively generated by following grammar:
T := 0 |

i∈I
a(x).Ti |

i∈I
nmi.Ti | T | T ′ | (c)T | [p=q]T | [p̸=q]T | !a(x).T | !nm.T.
In the πR-calculus a received name cannot appear as an output channel. Its
terms are produced by the following grammar:
T := 0 |

i∈I
n(x).Ti |

i∈I
ami.Ti | T | T ′ | (c)T | [p=q]T | [p̸=q]T | !n(x).T | !am.T.
Finally in the πS-calculus a received name is not allowed to be transmitted to
another process. The grammar is
T := 0 |

i∈I
n(x).Ti |

i∈I
nci.Ti | T | T ′ | (c)T | [p=q]T | [p̸=q]T | !n(x).T | !nc.T.

i∈I n(x).Ti is an input choice term and 
i∈I nmi.Ti an output choice term.
The binder n(x) is an input preﬁx and nmi an output preﬁx. The components
n(x).Ti and nmi.Ti are called summands. In n(x).Ti the name variable x is
bound. A name variable is free if it is not bound. We use guarded replications
!n(x).T and !nm.T. The conditional operator [p=q] is a match and [p̸=q] a mis-
match. The restriction term (c)T is in localization form, where the name c is
local. A name is global if it is not local. We will write gn( ) for the function that

186
J. Xue et al.
returns the set of the global names. The derived preﬁx operator a(c).T abbrevi-
ates (c)ac.T. We assume α-convention, meaning that no misuse of names/name
variables ever occurs. The application of a substitution σ = {n1/x1, . . . , ni/xi}
to a term is denoted by Tσ. A term is open if it contains free name variables; it
is closed otherwise. A process is a closed term. For each π-variant π′, the set of
the π′-processes is denoted by Pπ′ and is ranged over by L, M, N, O, P, Q.
The semantics of π, πL, πR, πS is deﬁned by the same labeled transition sys-
tem. The observable action set is L = {ab, ab, a(c) | a, b, c ∈N}. The action set
L ∪{τ} is ranged over by λ. The semantic rules are given below.
Action

i∈I a(x).Ti
ac
−→Ti{c/x}

i∈I aci.Ti
aci
−→Ti
Composition
T
λ
−→T ′
S | T
λ
−→S | T ′
S
ac
−→S′
T
ac
−→T ′
S | T
τ
−→S′ | T ′
S
ac
−→S′
T
a(c)
−→T ′
S | T
τ
−→(c)(S′ | T ′)
Localization
T
ac
−→T ′
(c)T
a(c)
−→T ′
T
λ
−→T ′
(c)T
λ
−→(c)T ′
c ̸∈gn(λ)
Condition
T
λ
−→T ′
[a=a]T
λ
−→T ′
T
λ
−→T ′
[a̸=b]T
λ
−→T ′
Replication
!ac.T
ac
−→T | !ac.T
!a(x).T
ac
−→T{c/x} | !a(x).T
The notation =⇒denotes the reﬂexive and transitive closure of
τ
−→.
3
Observational Theory
The ﬁrst fundamental relationship in process theory is the equality relationship.
It is argued in [Fu16] that from the point of view of both computation and
interaction, there is only one equality that satisﬁes the following conditions:
– it is model independent;
– it is an equality for self-evolving and interactive objects.
Self-evolution is the feature of computation and interaction is what a process
is supposed to do. We refer the reader to [Fu16] for a detailed argument and
convincing technical support for the above remarks and the principles behind
the argument. In this short paper we simply repeat the relevant deﬁnitions.

Remark on Some π Variants
187
Deﬁnition 1. A symmetric relation R on processes is a bisimulation if it vali-
dates the following bisimulation property:
– If QRP
τ
−→P ′ then one of the following statements is valid:
(i) Q =⇒Q′ for some Q′ such that Q′RP and Q′RP ′.
(ii) Q =⇒Q′′RP for some Q′′ such that ∃Q′.Q′′
τ
−→Q′RP ′.
It is codivergent if the following codivergence property is satisﬁed:
– If QRP
τ
−→P1
τ
−→. . .
τ
−→Pi . . . is an inﬁnite computation, then ∃Q′.∃i ⩾
1.Q
τ
=⇒Q′RPi.
It is extensional if the following extensionality property holds:
1. if MRN and PRQ then (M | P)R(N | Q);
2. if PRQ then (a)PR(a)Q for every a ∈N.
It is equipollent if P ⇓⇔Q ⇓whenever PRQ, where P ⇓, meaning that P is
observable, if and only if P =⇒
λ
−→P ′ for some P ′ and some λ ̸= τ.
The bisimulation of the above deﬁnition is what van Glabbeek and Weijland
called branching bisimulation [vGW89,Bae96]. Codivergence is Priese’s even-
tually progressing property [Pri78]. Equipollence is the most abstract form of
Milner and Sangiorgi’s barbness condition [MS92]. All the properties introduced
in Deﬁnition 1 are model independent. Their combination imposes a minimal
requirement from the point of computation as well as interaction and a maximal
condition from the point of model independence.
Deﬁnition 2. The absolute equality = is the largest relation on processes vali-
dating the following statements:
1. The relation is reﬂexive;
2. The relation is equipollent, extensional, codivergent and bisimilar.
The approach to extend the absolute equality from the processes to the terms is
standard. The model independence necessarily means that the absolute equality
is diﬃcult to work with. If there is a single technical lemma that helps reason
about =, it must be the Bisimulation Lemma stated next.
Lemma 1. If P =⇒P ′ = Q and Q =⇒Q′ = P, then P = Q.
The property stated in Lemma 1 is called X-property by De Nicola, Montanari
and Vaandrager [DNMV90].
Once we have deﬁned the absolute equality, we can distinguish two kinds
of internal actions. We will write T
ι
−→T ′ if T
τ
−→T ′ ̸= T, and T →T ′ if
T
τ
−→T ′ = T.
The observational theory discusses model speciﬁc characterizations of the
absolute equality. A model dependent counterpart of = is often far more
tractable. An external bisimilarity is a Milner-Park style bisimulation [Mil89,
Par81] in which every action is explicitly bisimulated. The external characteri-
zation of = for the π-calculus is given in [Fu16]. For the πL-calculus we can give

188
J. Xue et al.
an external counterpart in terms of a family of relations in the style of Sangiorgi’s
open bisimulations [San96b]. In the following deﬁnition ⊆f stands for the ﬁnite
subset relationship.
Deﬁnition 3. A πL-bisimulation is a family {RF}F⊆f N of codivergent symmet-
ric bisimulations on PπL if the following statements are valid whenever PRFQ:
1. If P
ab
−→P ′ then Q =⇒Q′′
ab
−→Q′RFP ′ and PRFQ′′ for some Q′, Q′′.
2. If P
ab
−→P ′ and a /∈F then Q =⇒Q′′
ab
−→Q′RFP ′ and PRFQ′′ for some
Q′, Q′′.
3. If P
a(c)
−→P ′ and a /∈F then Q =⇒Q′′ a(c)
−→Q′RF∪{c}P ′ and PRFQ′′ for
some Q′, Q′′.
We write

≃πL
F

F⊆f N for the largest πL-bisimulation, each ≃πL
F
is called the
F-πL-bisimilarity. The ≃πL-bisimilarity ≃πL is the ∅-πL-bisimilarity.
The idea of Deﬁnition 3 is that an indexing set F records all the local names
that have been opened up as it were by bound output actions. If P ≃πL
F Q and
a ∈F then the action ab say need not be bisimulated for the reason that no
environments that have received the local name a will ever do any input actions
at a. A similar idea motivates the following deﬁnition.
Deﬁnition 4. A πR-bisimulation is a family {RF}F⊆f N of codivergent sym-
metric bisimulations on PπR if the followings are valid whenever PRFQ:
1. If P
ab
−→P ′ and a /∈F then Q =⇒Q′′
ab
−→Q′RFP ′ and PRFQ′′ for some
Q′, Q′′.
2. If P
ab
−→P ′ then Q =⇒Q′′
ab
−→Q′RFP ′ and PRFQ′′ for some Q′, Q′′.
3. If P
a(c)
−→P ′ then Q =⇒Q′′ a(c)
−→Q′RF∪{c}P ′ and PRFQ′′ for some Q′, Q′′.
We write

≃πR
F

F⊆f N for the largest πR-bisimulation, each ≃πR
F
is called the
F-πR-bisimilarity. The ≃πR-bisimilarity ≃πR is the ∅-πR-bisimilarity.
The proof of the next lemma is routine.
Lemma 2. Both ≃πL and ≃πR are equivalence and congruence relations.
The next is another useful technical lemma.
Lemma 3. The following statements are valid:
1. If P ≃πL
F∪{c} Q and c is not a global output channel in P | Q then P ≃πL
F Q;
2. If P ≃πR
F∪{c} Q and c is not a global input channel in P | Q then P ≃πR
F
Q;
3. If P ≃π′
F∪{c} Q for π′ ∈{πL, πR} then (c)(P | A) ≃π′
F (c)(Q | A) for each A.
Without further ado, we come to the main result of this section.

Remark on Some π Variants
189
Theorem 1. The following are valid:
1. The πL-bisimilarity ≃πL coincides with =πL.
2. The πR-bisimilarity ≃πR coincides with =πR.
Proof. Lemma 2 implies both ≃πL⊆=πL and ≃πR⊆=πR. The proof of the reverse
inclusions is a modiﬁcation of a proof in [Fu05]. For the present proof we
only have to mention the part that diﬀers from the previous proofs. (2) Let
{RF}F⊆f N be deﬁned in the following manner.
R{c1,...,cn}
def
=
⎧
⎨
⎩(P, Q)






{a1, . . . , an} ∩gn(P | Q) = ∅,
(c1, . . . , cn)(a1c1 | . . . | ancn | P) =πR
(c1, . . . , cn)(a1c1 | . . . | ancn | Q)
⎫
⎬
⎭.
We prove that {RF}F⊆f N is a πR-bisimulation. Suppose A = B, where
A
def
= (c1, . . . , cn)(a1c1 | . . . | ancn | P),
B
def
= (c1, . . . , cn)(a1c1 | . . . | ancn | Q),
such that {a1, . . . , an} ∩gn(P | Q) = ∅. Consider P
a(c)
−→P ′ for some c ̸∈
{c1, . . . , cn}. Let d, f, an+1 be fresh and let D be deﬁned by
D
def
= a(x).(an+1x | [x ̸∈gn(P | Q)]f) + a(x).d.
Now
A | D
τ
−→(c)(A′ | an+1c | [c ̸∈gn(P | Q)]f)
must be bisimulated by
B | D =⇒B′′ | D
τ
−→(c)(B′ | an+1c | [c ̸∈gn(P | Q)]f).
Since B′′ | D
τ
−→(c)(B′ | an+1c | [c ̸∈gn(P | Q)]f)) must be a change-of-state,
it must be the case that A | D =πR B′′ | D. It follows easily from Bisimulation
Lemma that B =⇒B′′ a(c)
−→B′ =πR A′. Clearly,
A′ ≡(c1, . . . , cn)(a1c1 | . . . | ancn | P ′),
B′ ≡(c1, . . . , cn)(a1c1 | . . . | ancn | Q′),
B′′ ≡(c1, . . . , cn)(a1c1 | . . . | ancn | Q′′)
for some P ′, Q′, Q′′. It follows from B =⇒B′′ a(c)
−→B′ that Q =⇒Q′′ a(c)
−→Q′.
Moreover PR{c1,...,cn}Q′′ and P ′R{c1,...,cn,c}Q′ by deﬁnition. We can symmetri-
cally deal with (1).
⊓⊔

190
J. Xue et al.
4
Relative Expressiveness
The second fundamental relationship in process theory is the expressiveness rela-
tionship between process calculi. For this relationship to make sense at all, model
independence has to be a born property. A theory of expressiveness is developed
in [Fu16]. The philosophy of the theory is that the expressiveness relationship is
the generalization of the absolute equality from one model to two models. Again
we will simply repeat the deﬁnition here.
Deﬁnition 5. Suppose M, N are two π-variants. A binary relation R ⊆PM×PN
is a subbisimilarity if it validates the following statements.
1. R is reﬂexive in the following sense:
(a) R is total, meaning that ∀M ∈PM.∃N ∈PN.MRN.
(b) R is sound, meaning that N1R−1M1 =M M2RN2 implies N1 =N N2.
2. R is equipollent, extensional, codivergent and bisimilar.
We say that M is subbisimilar to N, notated by M ⊑N, if there is a subbisimi-
larity from M to N. We write M ⊏N if M ⊑N and N ̸⊑M. Intuitively M ⊑N
means that N is at least as expressive as M.
Theorem 2. Suppose M, N ∈{π, πL, πR}. If M, N are distinct then M ̸⊑N.
Proof. Suppose F is a subbisimilarity from M to N. The proof of Theorem 4.23
in [Fu16] essentially shows that, for all P, Q such that PFQ, the following Global
Bisimulation property holds:
– If P
ac
−→P ′ then Q =⇒Q′′
ac
−→Q′F−1P ′ and PFQ′′ for some Q′′, Q′.
– If Q
ac
−→Q′ then P =⇒P ′′
ac
−→P ′F−1Q′ and P ′′FQ for some Q′′, Q′.
– If P
ac
−→P ′ then Q =⇒Q′′
ac
−→Q′F−1P ′ and PFQ′′ for some Q′′, Q′.
– If Q
ac
−→Q′ then P =⇒P ′′
ac
−→P ′F−1Q′ and P ′′FQ for some Q′′, Q′.
Using the above property the following crucial fact is established in [Fu16]:
Self Interpretation: AFA whenever A contains no replication operator.
The Global Bisimulation property, the extensionality and Theorem 1 are neces-
sary to guarantee the Self Interpretation property.
Now we can argue as follows:
– It should be clear that a(x).x F a(x).x implies π ̸⊑πL, a(x).x F a(x).x
implies π ̸⊑πR.
– It follows from the Self Interpretation property that
(d)(ad | d.e) F (d)(ad | d.e),
(1)
(d)(ad | d) F (d)(ad | d).
(2)
The equality (d)(ad | d.e) = (d)(ad | d) holds in πL. It holds in none of π, πR.
Therefore (1) and (2) imply πL ̸⊑π and πL ̸⊑πR.

Remark on Some π Variants
191
– Similarly the Self Interpretation property implies
(d)(ad | d.e) F (d)(ad | d.e),
(d)(ad | d) F (d)(ad | d),
from which πR ̸⊑π and πR ̸⊑πL follow.
We are done.
⊓⊔
5
Expressive Completeness
The variants πL, πR, πS would not be interesting if they are not complete. For
interaction models completeness means that the computable functions can be
encoded as interactive processes. There are many notions of Turing complete-
ness. In this paper we adopt the deﬁnition introduced in [Fu16]. The idea is to
formalize the following interactive version of the Church-Turing Thesis:
Axiom of Completeness. C ⊑M for all models of interaction M.
Here C is the Computability Model, which is basically the interaction model of
computable function. The reader is referred to [Fu16] for the deﬁnition of C and
what it means for M to satisfy C ⊑M. It is suﬃcient to say that a proof of
completeness boils down to showing how the natural numbers are deﬁned and
how the recursive functions are translated into processes that can input natural
numbers and output the computing results. To avoid confusion we will write
0, 1, 2, . . . , i, . . . for the natural numbers. We will use the following notations for
the recursive functions deﬁned in [Rog87]:
– s(x) is the successor function.
– i(x1, . . . , xn) is the n-ary constant function with value i.
– pi
n(x1, .., xn) is the n-ary projection function at the i-th parameter.
– f(f1(x), .., fi(x)) is the function composed of f(x1, . . . , xi), f1(x), .., fi(x).
– rec z.[f(x, x′, z), g(x)] is the recursion function deﬁned by f(x, x′, z), g(x).
– μz.f(x, z) is the minimization function over f(x, z).
Theorem 3. C ⊑πL, C ⊑πR and C ⊑πS.
The encoding of the natural numbers in πL is as [[0]]πL
c
def
=
c(z).z and
[[n+1]]πL
c
def
= (d)(c(z).zd | [[n]]πL
d ). Every number is accessible at a global name.
The encoding makes use of a special name ⊥. This is harmless because ⊥never
appears as a channel name in our encoding. Since an input number might be
used several time when computing a recursive function, a persistent form of
the above encoding is necessary. This is given by [[!0]]πL
c
def
= !c(z).z
and
[[!n+1]]πL
c
def
= (d)(!c(z).zd | [[!n]]πL
d ). In sequel we shall use Milner’s encoding of

192
J. Xue et al.
the polyadic π-preﬁxes in terms of the monadic preﬁxes [Mil93]. This is given
by
a(x1, · · · , xk).T
def
= a(z).z(c).c(x1). · · · .c(xk).T,
a⟨n1, · · · , nk⟩.T
def
= a(d).d(y).yn1. · · · .ynk.T.
It is not a very faithful translation [Fu16]. But this point should not concern us
since the above encoding will only be used locally. Now for every πL-term T, we
would like to introduce a πL-term Rp(n, a).T that is capable of converting an
input number to its persistent form. More speciﬁcally, it enables the following
change-of-state internal action: [[n]]πL
c
| Rp(c, d).T
ι
−→= [[!n]]πL
d
| T. The process
Rp(n, a).T is deﬁned by
(f)(n(c).c(y).([y=⊥](!a(z).z⊥| T) | [y̸=⊥]y(c).c(z).(d)f⟨d, z⟩.!d(z1).z1⊥)
| !f(u, v).([v=⊥](!a(z).zu | T) | [v̸=⊥]v(c).c(z).(d)f⟨d, z⟩.!d(z2).z2u)).
Once we have the process [[n+1]]πL
c
and [[!n+1]]πL
c , we might want to make a copy
of them when necessary. This is achieved by Cp(n, a).T deﬁned by
(f)(n(c).c(y).([y=⊥](a(z).z⊥| T) | [y̸=⊥]y(c).c(z).(d)f⟨d, z⟩.d(z1).z1⊥)
| !f(u, v).([v=⊥](a(z).zu | T) | [v̸=⊥]v(c).c(z).(d)f⟨d, z⟩.d(z2).z2u)),
which is very much similar to the previous process. Clearly the following inter-
actions are admissible.
[[n]]πL
c
| Cp(c, d).T
ι
−→= [[n]]πL
d
| T,
[[!n]]πL
c
| Cp(c, d).T
ι
−→= [[!n]]πL
c
| [[n]]πL
d
| T.
An n-ary function f(x1, · · · , xn) is translated to a process that picks up n
inputs consecutively before outputting the result. The input and output actions
must be carried out in particular channels. We write [[F b
a1···an(f(x1, · · · , xn))]]πL
for the translation of f(x1, · · · , xn) in πL at the input channels a1, . . . , an and
the output channel b. The structural deﬁnition goes as follows:
– The successor, constant, projection and composition functions are deﬁned as
follows:
[[F b
a1(s(x))]]πL def
= (d1)Rp(a1, d1).(c)Cp(d1, c).b(x).x(c),
[[F b
a1···an(in(x1, · · · , xn))]]πL def
= (d1)Rp(a1, d1). · · · .(dn)Rp(an, dn).[[i]]πL
b ,
[[F b
a1···an(pi
n(x1, · · · , xn))]]πL def
= (d1)Rp(a1, d1). · · · .(dn)Rp(an, dn).Cp(di, b),
[[F b
a1···an(f(f1(x), · · · , fi(x)))]]πL def
= (d1)Rp(a1, d1). · · · .(dn)Rp(an, dn).(c1 · · · ci)
([[F b
c1···ci(f(x))]]πL | [[F c1
d1···dn(f1(x))]]πL
| · · · | [[F ci
d1···dn(fi(x))]]πL)

Remark on Some π Variants
193
– [[F b
a1···an+1(recz.[f(x, x′, z), g(x)])]]πL is the following process
(d1)Rp(a1, d1). · · · .(dn+1)Rp(an+1, dn+1).dn+1(c).c(y).
([y=⊥][[F b
d1,··· ,dn(g(x))]]πL
| [y̸=⊥](f)(Rec | (g)f⟨g, y⟩.g(w).[[F b
d1,··· ,dn,w,yf(x, x′, z)]]πL))
where Rec stands for
!f(u, v).u(d).([v=⊥][[F d
d1,··· ,dn(g(x))]]πL
| [v̸=⊥]v(c).c(y).(g)f⟨g, y⟩.(g(w).[[F d
d1,··· ,dn,w,v(f(x, x′, x′′))]]πL)).
– [[F b
a1···an(μz.[f(x, z)])]]πL is the following process
(d1)Rp(a1, d1). · · · .(dn)Rp(an, dn).(f)(Mu | f(c).[[0]]πL
c )
where Mu stands for
!f(v).(g)Rp(v, g).(d)([[F d
d1,··· ,dn,g(f(x, z))]]πL
| d(e).e(z).([z=⊥]Cp(g, b). | [z̸=⊥]f(c).(c′)Cp(g, c′).c(y).yc′)).
This completes the deﬁnition of [[ ]]πL. The reader can work out the encoding
[[ ]]πR symmetrically. The proof of the completeness of πS is subsumed by that
for πP . See Sect. 7 for more details. We only have to remark that the parametric
deﬁnitions can be implemented in πS using the replication operator.
6
Proof System
Based on Theorem 1 one may talk about complete equational proof systems for
the absolute equality of the π-variants. In view of Theorem 3 no decidable proof
system is possible for all processes. However the ﬁnite fragment consisting of 0,
the choice operator, the match/mismatch operator and the localization operator
is decidable. Equational systems for various congruence relations on the ﬁnite
π-processes are well-known. The paper by Parrow and Sangiorgi [PS95] deserves
particular attention. A complete equational system AS for the absolute equality
on the ﬁnite π-terms is studied in [FZ15]. In this section we shall brieﬂy explain
how to construct complete systems for πL and πR by extending AS.
L
n(c).C[
i∈I cmi.Ti] = n(c).C[0]
R
n(c).C[
i∈I c(x).Ti] = n(c).C[0]
Fig. 1. Axioms for the variants.

194
J. Xue et al.
In Fig. 1 two axioms are proposed. The law L is valid for ≃πL and the law R is
valid for ≃πR. Let ASL be AS ∪{L} and ASR be AS ∪{R}. The ﬁrst indication
of the power of ASL, as well as ASR, is a normalization lemma stating that
all ﬁnite terms can be converted to some normal forms. Due to the presence of
mobility the normal forms for the πL-terms are a bit involved. But the main
deﬁnition is the following.
Deﬁnition 6. Suppose F, G ⊆f N ∪Nv. The πL-term T is a normal form on
F and G if it is of the form

i∈I
λi.Ti
such that for each i ∈I one of the followings holds.
1. If λi = τ then Ti is a normal form on F and G.
2. If λi = ¯nm and n ̸∈G then Ti is a normal form on F and G.
3. If λi = ¯n(c) and n ̸∈G then Ti ≡(
n∈F c̸=n)T c
i for some normal form T c
i
on F ∪{c} and G ∪{c}.
4. If λi = c(x) then Ti is of the form
(

n∈F
x̸=n)T ̸=
i +

m∈F
[x=m]T m
i
such that T ̸=
i
is a normal form on F ∪{x} and G, and, for each m ∈F,
x ̸∈fv(T m
i ) and T m
i
is a normal form on F and G.
The reader can easily work out the deﬁnition of the normal forms for the πR-
processes from Deﬁnitions 3, 4 and 6. The rest of the arguments and proofs are
almost an reiteration of corresponding arguments and proofs in [FZ15]. Without
further ado, let’s state the main result.
Theorem 4. The following statements are valid:
1. S ≃πL T if and only if ASL ⊢τ.S = τ.T for all ﬁnite πL-terms S, T.
2. S ≃πR T if and only if ASR ⊢τ.S = τ.T for all ﬁnite πR-terms S, T.
The above theorem is concerned with πL-terms, from which we can easily
derive that P ≃πL Q if and only if ASL ⊢P = Q for all ﬁnite πL-processes P, Q
and that P ≃πR Q if and only if ASR ⊢P = Q for all ﬁnite πR-processes P, Q.
7
Private Pi
The private π-calculus of Sangiorgi [San96a], denoted by πP , is interesting in
that it is a nontrivial model that stays between CCS and the π-calculus. All
mobility admitted in πP is internal. It is shown in [FL10] that πP fails to be
complete if recursion is provided by the replication operator. So normally πP
comes with the parametric deﬁnition. Here is its grammar:
T := 0 |

i∈I
n(x).Ti |

i∈I
n(c).Ti | T | T ′ | (c)T | D(p1, . . . , pn).

Remark on Some π Variants
195
A parametric deﬁnition is given by
D(x1, . . . , xn) = T,
(3)
where {x1, . . . , xn} is the set of all the name variables in T. An instantiation of
the parametric deﬁnition at p1, . . . , pn, denoted by D(p1, . . . , pn), is the term
T{p1/x1, . . . , pn/xn}. The match and mismatch operators are absent in πP
because they are useless in this particular model. The operational semantics
of πP can be read oﬀfrom the semantics of the π-calculus. We will not repeat
it here.
The Turing completeness of πP [BGZ03] does not imply the completeness of
πP since the latter is a strictly stronger property. We still need be assured that
πP is a legitimate model according to the Axiom of Completeness. Unlike in πL
and πR the natural numbers in πP must be deﬁned in preﬁx form.
[[0]]πP
p
def
= p(b0, b1).b0,
(4)
[[n+1]]πP
p
def
= p(b0, b1).[[n]]πP
b1 .
(5)
To get a feeling of how (4,5) work, let’s see how the successor function Sc
and the predecessor function Sb are deﬁned.
Sc(u, v).T
def
= u(x0, x1).(v(e0, e1).e1(f0, f1).Com(x0, x1, f0, f1) | T),
Sb(u, v).T
def
= u(x0, x1).(x0.v(e0, e1).e0
| x1(y0, y1).v(e0, e1).Com(y0, y1, e0, e1) | T),
where Com is introduced by the following parametric deﬁnition:
Com(x0, x1, y0, y1) = x0.y0 | x1(z0, z1).y1(c0, c1).Com(z0, z1, c0, c1).
The interesting thing about Com is that it works in a lazy fashion. It is only
when a produced number is being used can the copy mechanism of Com be
invoked. This feature is prominent in all the following encodings. For example
the persistent form of the natural number must make use of Com:
[[!0]]πP
p
= [[!0]]πP
p
| [[0]]πP
p ,
[[!n+1]]πP
p
def
= (c)([[!n]]πP
c
| !PSc(c, p)),
where
PSc(u, v).T
def
= v(e0, e1).(u(x0, x1).e1(f0, f1).Com(x0, x1, f0, f1) | T).
The copy term can be simply deﬁned in terms of Com:
Cp(u, v).T
def
= u(x0, x1).(v(e1, e2).Com(x0, x1, e1, e2) | T).

196
J. Xue et al.
It is not diﬃcult to see that the following actions are admissible:
[[n]]πP
a
| Cp(a, b).T
ι
−→= [[n]]πP
b
| T,
[[!n]]πP
a
| Cp(a, b).T
ι
−→= [[!n]]πP
a
| [[n]]πP
b
| T.
The deﬁnition of the replication term is slightly more involved:
Rp(u, v).T
def
= u(x0, x1).(x0.[[!0]]πP
v
| x1(y0, y1).(c)(!PSc(c, v) | R(y0, y1, c)) | T),
R(y0, y1, w) = y0.[[!0]]πP
w | y1(z0, z1).(c)(!PSc(c, w) | R(z0, z1, c)).
The reader is advised to verify that the following action is admissible:
[[n]]πP
a
| Rp(a, b).T
ι
−→= [[!n]]πP
b
| T.
Another process useful to the encoding is the following:
Eq(u, v).(T0, T1)
def
= u(x0, x1).v(y0, y1).
(d0d1)(E(x0, x1, y0, y1, d0, d1) | d0.T0 | d1.T1),
E(x0, x1, y0, y1, u0, u1)
def
= x0.(y0.u0 | y1(z0, z1).u1)
| x1(z0, z1).(y0.u1 | y1(w0, w1).E(z0, z1, w0, w1, u0, u1)).
The encoding [[ ]]πP of the computable functions in πP can now be given. It
should be enough to explain how the recursion functions and the minimization
functions are interpreted.
– [[F b
a1···an+1(recz.[f(x, x′, z), g(x)])]]πP is deﬁned by the following process
(d1)Rp(a1, d1). · · · .(dn)Rp(an, dn).(dn+1)Rp(an+1, dn+1).
(d0)([[!d0(0)]]πP | Eq(d0, dn+1).([[F b
˜
d(g(˜x))]]πP ,
(e′eg)Sb(dn+1, e′).Rp(e′, e).([[F b
˜
dge(f(˜x, x′, x′′))]]πP | Rec(g, e, d0, d)))),
where
Rec(u, v, z0, z) = Eq(z0, v).([[F u
˜z (g(˜x))]]πP ,
(e′eg)Sb(v, e′).Rp(e′, e).([[F u
˜zge(f(˜x, x′, x′′))]]πP | Rec(g, e, z0, z))).
– [[F b
a1···an(μz.[f(x, z)])]]πP is the following process
(d1)Rp(a1, d1). · · · .(dn)Rp(an, dn).
(d0d′)([[!d0(0)]]πP | [[F d′
˜d,d0(f(˜x, z))]]πP | Eq(d0, d′).(Cp(d0, b),
(e′ef)Sc(d0, e′).Rp(e′, e).([[F f
˜d,e(f(˜x, z))]]πP | Mu(f, e, d0, d, b)))),
where
Mu(u, v, z0, z, w) = Eq(z0, u).(Cp(v, w),
(e′ef)Sc(v, e′).Rp(e′, e).([[F f
˜
d,e(f(˜x, z))]]πP | Mu(f, e, z0, z, w))).
We have coded up all the computable functions. Hence the next result.
Theorem 5. πP , and consequently πS as well, are complete.

Remark on Some π Variants
197
8
Remark
We have provided the observational theories for πL and πR. Our attempt to do
the same thing for πS, πP has not been successful. What has stopped us from
getting a similar picture for the latter is the absence of an external characteri-
zation of the absolute equality for either variant. It is easy to conceive a family
of explicit bisimilarities for πS. But we probably need a new technique to show
that it gives rise to an alternative way of deﬁning the absolute equality in πS.
The external characterization of = for πP appears more elusive. For one thing
the match and the mismatch operators are not of any use in any proof since they
are redundant in πP . So πP poses a bigger challenge.
The completeness of all the four calculi raises the following question: What
kind of universal processes does each of them have? It has been shown in [Fu]
that the π-calculus proper has very powerful universal processes. The situations
in πL, πR, πS, πP remain to be seen.
Acknowledgments. This work has been supported by National Natural Science
Foundation of China (61502296, 61472239, 61261130589) and Natural Science Founda-
tion of Shanghai (15ZR1417000).
References
[ACS96] Amadio, R.M., Castellani, I., Sangiorgi, D.: On bisimulations for the asyn-
chronous π-calculus. In: Montanari, U., Sassone, V. (eds.) CONCUR 1996.
LNCS, vol. 1119, pp. 147–162. Springer, Heidelberg (1996). doi:10.1007/
3-540-61604-7 53
[Bae96] Baeten, J.: Branching bisimilarity is an equivalence indeed. Inform.
Process. Lett. 58, 141–147 (1996)
[BGZ03] Busi, N., Gabbrielli, M., Zavattaro, G.: Replication vs. recursive deﬁni-
tions in channel based calculi. In: Baeten, J.C.M., Lenstra, J.K., Parrow,
J., Woeginger, G.J. (eds.) ICALP 2003. LNCS, vol. 2719, pp. 133–144.
Springer, Heidelberg (2003). doi:10.1007/3-540-45061-0 12
[BGZ04] Busi, N., Gabbrielli, M., Zavattaro, G.: Comparing recursion, replication,
and iteration in process calculi. In: D´ıaz, J., Karhum¨aki, J., Lepist¨o, A.,
Sannella, D. (eds.) ICALP 2004. LNCS, vol. 3142, pp. 307–319. Springer,
Heidelberg (2004). doi:10.1007/978-3-540-27836-8 28
[Bou92] Boudol, G.: Asynchrony and the π-calculus. Technical report RR-1702,
INRIA Sophia-Antipolis (1992)
[CF11] Cai, X., Fu, Y.: The λ-calculus in the π-calculus. Math. Struct. Comput.
Sci. 21, 943–996 (2011)
[DNMV90] Nicola, R., Montanari, U., Vaandrager, F.: Back and forth bisimulations.
In: Baeten, J.C.M., Klop, J.W. (eds.) CONCUR 1990. LNCS, vol. 458, pp.
152–165. Springer, Heidelberg (1990). doi:10.1007/BFb0039058
[FL10] Fu, Y., Lu, H.: On the expressiveness of interaction. Theoret. Comput. Sci.
411, 1387–1451 (2010)
[Fu] Fu, Y.: The universal process. In: Logical Methods in Computer Science
(to appear)

198
J. Xue et al.
[Fu05] Fu, Y.: On quasi open bisimulation. Theoret. Comput. Sci. 338, 96–126
(2005)
[Fu10] Fu, Y.: Theory by process. In: Gastin, P., Laroussinie, F. (eds.) CONCUR
2010. LNCS, vol. 6269, pp. 403–416. Springer, Heidelberg (2010). doi:10.
1007/978-3-642-15375-4 28
[Fu13] Fu, Y.: The Value-Passing Calculus. In: Liu, Z., Woodcock, J., Zhu, H.
(eds.) Theories of Programming and Formal Methods. LNCS, vol. 8051, pp.
166–195. Springer, Heidelberg (2013). doi:10.1007/978-3-642-39698-4 11
[Fu16] Fu, Y.: Theory of interaction. Theoret. Comput. Sci. 611, 1–49 (2016)
[Fu17] Fu, Y.: On the expressive power of name-passing communication. In: CON-
CUR 2017 (2017)
[FZ15] Fu, Y., Zhu, H.: The name-passing calculus. arXiv:1508.00093 (2015)
[Gor08] Gorla, D.: Comparing communication primitives via their relative expres-
sive power. Inf. Comput. 206, 931–952 (2008)
[GSV04] Giambiagi, P., Schneider, G., Valencia, F.D.: On the expressiveness of inﬁ-
nite behavior and name scoping in process calculi. In: Walukiewicz, I. (ed.)
FoSSaCS 2004. LNCS, vol. 2987, pp. 226–240. Springer, Heidelberg (2004).
doi:10.1007/978-3-540-24727-2 17
[Hoa78] Hoare, C.: Communicating sequential processes. Commun. ACM 21, 666–
677 (1978)
[HT91a] Honda, K., Tokoro, M.: An object calculus for asynchronous communica-
tion. In: America, P. (ed.) ECOOP 1991. LNCS, vol. 512, pp. 133–147.
Springer, Heidelberg (1991). doi:10.1007/BFb0057019
[HT91b] Honda, K., Tokoro, M.: On asynchronous communication semantics. In:
Tokoro, M., Nierstrasz, O., Wegner, P. (eds.) ECOOP 1991. LNCS, vol.
612, pp. 21–51. Springer, Heidelberg (1992). doi:10.1007/3-540-55613-3 2
[LPSS08] Lanese, I., Perez, J., Sangiorgi, D., Schmitt, A.: On the expressiveness and
decidability of higher-order process calculi. In: Proceedings of LICS 2008,
pp. 145–155 (2008)
[Mer00] Merro, M.: Locality in the π-calculus and applications to object-oriented
languages. PhD thesis, Ecole des Mines de Paris (2000)
[Mil89] Milner, R.: Communication and Concurrency. Prentice Hall, Upper Saddle
River (1989)
[Mil92] Milner, R.: Functions as processes. Math. Struct. Comput. Sci. 2, 119–146
(1992)
[Mil93] Milner, R.: The polyadic π-calculus: a tutorial. In: Bauer, F.L., Brauer,
W., Schwichtenberg, H. (eds.) Logic and Algebra of Speciﬁcation. NATO
ASI Series (Series F: Computer & Systems Sciences), vol. 94, pp. 203–246.
Springer, Heidelberg (1993). doi:10.1007/978-3-642-58041-3 6
[MPW92] Milner, R., Parrow, J., Walker, D.: A calculus of mobile processes. Inform.
Comput. 100, 1–40 (Part I), 41–77 (Part II) (1992)
[MS92] Milner, R., Sangiorgi, D.: Barbed bisimulation. In: Kuich, W. (ed.) ICALP
1992. LNCS, vol. 623, pp. 685–695. Springer, Heidelberg (1992). doi:10.
1007/3-540-55719-9 114
[MS04] Merro, M., Sangiorgi, D.: On asynchrony in name-passing calculi. Math.
Struct. Comput. Sci. 14, 715–767 (2004)
[Pal03] Palamidessi, C.: Comparing the expressive power of the synchronous and
the asynchronous π-calculus. Math. Struct. Comput. Sci. 13, 685–719
(2003)

Remark on Some π Variants
199
[Par81] Park, D.: Concurrency and automata on inﬁnite sequences. In: Deussen,
P. (ed.) GI-TCS 1981. LNCS, vol. 104, pp. 167–183. Springer, Heidelberg
(1981). doi:10.1007/BFb0017309
[Pri78] Priese, L.: On the concept of simulation in asynchronous, concurrent sys-
tems. Progress Cybern. Syst. Res. 7, 85–92 (1978)
[PS95] Parrow, J., Sangiorgi, D.: Algebraic theories for name-passing calculi. Inf.
Comput. 120, 174–197 (1995)
[Rog87] Rogers, H.: Theory of Recursive Functions and Eﬀective Computability.
MIT Press, Cambridge (1987)
[San93] Sangiorgi, D.: From π-calculus to higher-order π-calculus – and back. In:
Gaudel, M.-C., Jouannaud, J.-P. (eds.) CAAP 1993. LNCS, vol. 668, pp.
151–166. Springer, Heidelberg (1993). doi:10.1007/3-540-56610-4 62
[San96a] Sangiorgi, D.: π-calculus, internal mobility and agent-passing calculi. The-
oret. Comput. Sci. 167, 235–274 (1996)
[San96b] Sangiorgi, D.: A theory of bisimulation for π-calculus. Acta Informatica 3,
69–97 (1996)
[SW01] Sangiorgi, D., Walker, D.: The π Calculus: A Theory of Mobile Processes.
Cambridge University Press, Cambridge (2001)
[Tho95] Thomsen, B.: A theory of higher order communicating systems. Inf. Com-
put. 116, 38–57 (1995)
[vGW89] van Glabbeek, R., Weijland, W.: Branching time and abstraction in bisim-
ulation semantics. In: Information Processing 1989, North-Holland, pp.
613–618 (1989)
[Wal95] Walker, D.: Objects in the π-calculus. Inf. Comput. 116, 253–271 (1995)
[XYL15] Xu, X., Yin, Q., Long, H.: On the computation power of name parameter-
ization in higher-order processes. In: ICE 2015 (2015)

Reasoning About Periodicity on Inﬁnite Words
Wanwei Liu1(B), Fu Song2, and Ge Zhou1
1 College of Computer Science, National University of Defense Technology,
Changsha, China
wwliu@nudt.edu.cn
2 School of Information Science and Technology,
ShanghaiTech University, Pudong, China
Abstract. Characterization of temporal properties is the original pur-
pose of inventing of temporal logics. In this paper, we show that the
property like “some event holds periodically” is not omega-regular. Such
property is called “periodicity”, which plays an important role in task
scheduling and system design. To give a characterization of periodicity,
we present the logic QPLTL, which is an extension of LTL via adding
quantiﬁed step variables. Based on the decomposition theorem, we show
that the satisfiability problem of QPLTL is PSPACE-complete.
1
Introduction
In some sense, concurrency and interleaving are the main course of complex-
ity in distributed and parallel programs. For such a program, we are in general
concerned about its interactive behaviors, rather than the ﬁnal output. Hence,
various temporal logics are designed to characterize such issues of parallel pro-
grams. Characterization of temporal properties is always one of the central topics
in the research on temporal logics.
Linear-time temporal logic (LTL), is one of the most frequently used logics,
which is obtained from propositional logic via merely adding two temporal con-
nectives — X (next) and U (until). Simple as such logic is, LTL could express a
majority of properties which are usually concerned about — for example, respon-
sibility — such as the formula G(req →F ack), which depicts the assertion that
“each request will be eventually acknowledged”.
In [Wol83], Wolper pointed out some properties, like “p holds at least in every
even moment” (we refer to it as G2p in this paper), cannot be expressed by any
LTL formula. As a consequence, numerous extensions are presented to enhance
the expressive power. To mention a few: In [VW94], ω-automata are employed
as extended temporal connectives. In [BB87], Banieqbal and Barringer yield
the linear-time version of modal μ-calculus. In [LS07], Leucker and S´anchez
propose to use regular-expressions as preﬁxes of formulae. All of these logics
are shown to be as expressive as the full ω-regular languages, or the whole
set of (nondeterministic) B¨uchi automata [B¨uc62]. As a result, properties like
G2p can be described by these logics. In the view of formal languages, LTL
formulae precisely correspond to star-free ω-regular languages [Tho79], in which
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 200–215, 2017.
https://doi.org/10.1007/978-3-319-69483-2_12

Reasoning About Periodicity on Inﬁnite Words
201
the Kleene-star (∗) and the omega-power (ω) operators can only be applied to Σ
(the alphabet). Meanwhile, in the automata-theoretic perspective, LTL formulae
are inter-convertable with B¨uchi automata of special forms, e.g., counter-free
automata or aperiodic automata (cf. [DG08] for a comprehensive survey).
We say that an event f happens periodically, if there exists some number
n > 0 (called the period, which is not pre-given) such that f holds at least in
every moment which is a multiple of n. Such kind of property is called period-
icity, which plays an important role in task scheduling and system design. For
example, when designing a synchronous circuit, we need to guarantee the clock
interrupt generates inﬁnitely and equidistantly. An aerospace control system has
a diagnosis module which periodically checks whether the system works properly.
If it was not, then the system immediately enters the recovery state.
Enlightened by the fact that “G2p cannot be expressed by any LTL formula”,
we conjecture that “the periodicity property ∃k.Gkp is not expressible by any
ω-regular language”. We give an aﬃrmative proof of this conjecture in this paper,
and hence deduce that ω-regular properties are not closed under inﬁnite unions
and/or intersections. To express periodicity within linear-time framework, we
tentatively suggest to add quantiﬁed step variables into logics. For example, if
we use μTL [BB87] as the base logic, the aforementioned property could be
described by the formula ∃k.νZ.(p ∧XkZ). However, such mechanism gives rise
to an expressive power beyond expectations — we show that, if we allow nesting
of step variables, all formulae of Peano arithmetic can be encoded — provided
that the base logic involves the X-operator and two distinct propositions. Hence,
it leads to undecidability for the satisfiability problem of the logic.
Thus, to obtain a decidable extension, we have to impose strong syntactic
restriction to the logic. In this paper, using LTL as the base logic, we introduce
the logic Quantiﬁed Periodic LTL (QPLTL, for short). In such logic, formulae
are categorized into four groups, the ﬁrst three are contained within ω-regular
languages, and the last group is specially tailored for deﬁning periodicity: Each
of such formulae uses at most one step variable, and the occurrence of this
variable must be of special form. We show that, for QPLTL, the satisfiability
problem is also PSPACE-complete. Our proof approach is mainly based on the
decomposition theorem [AS85], namely, each property could be decomposed into
an intersection of a liveness property and a safety property, with which, we give a
normal form of star-free liveness properties. Instead of deciding the satisﬁability
of the formulae, we give a decision procedure to decide their validity.
The rest part of this paper is organized as follows: Sect. 2 introduces some
basic notations and deﬁnitions. In Sect. 3, we show that periodicity properties
are not ω-regular. Thus, we suggest to add quantiﬁed step variables into linear-
time logics to gain such an enhancement. However, we show that an unrestricted
use of such extension will result in undecidability of satisfiability, provided
that the base logic involves the X-operator and at least two propositions. We
then present the logic QPLTL in Sect. 4 via imposing strict syntactic constraints
to the use of step variables, by revealing a normal form of star-free liveness
properties, we show that the satisfiability problem of the proposed logic is
PSPACE-complete. We summarize this paper and discuss future work in Sect. 5.

202
W. Liu et al.
2
Preliminaries
2.1
Finite and Inﬁnite Words
Fix an alphabet Σ, whose elements are called letters. We call w an ω-word (or
inﬁnite-word) if w ∈Σω, and we call w a ﬁnite-word if w ∈Σ∗.
We use |w| to denote the length of w, and deﬁnitely |w| = ∞if w is an
ω-word. For each i < |w|, let w(i) be the ith letter of w. Remind the ﬁrst letter
should be w(0).
Given w1 ∈Σ∗and w2 ∈Σ∗∪Σω, we denote by w1 · w2 the concatenation
of w1 and w2. Namely, we have: (w1 · w2)(i) = w1(i) whenever i < |w1|; and
(w1 · w2)(i) = w2(i −|w1|) whenever i ≥|w1|. In this case, we say that w1 and
w2 are respectively a preﬁx and a postﬁx (or suﬃx) of w1 · w2.
In the rest of this paper, we ﬁx a (potentially inﬁnite) set P of propositions,
the elements range over p, p1, p2, etc., and when mentioning about “words”,
without explicit declaration, we let Σ = 2P.
2.2
Linear-Time Temporal Logic
Syntax. Formulae of linear-time temporal logic (LTL, [Pnu77]), ranging over
f, g, f1, f2, . . ., can be described by the following abstract grammar:
f ::= ⊥| p | f →f | Xf | f U f.
We usually use the following derived operators as syntactic sugars:
¬f
def
= f →⊥
⊤
def
= ¬⊥
f1 ∨f2
def
= ¬f1 →f2
f1 ∧f2
def
= ¬(¬f1 ∨¬f2)
Ff
def
= ⊤U f f1 ↔f2
def
= (f1 →f2) ∧(f2 →f1)
f1Rf2
def
= ¬(¬f1 U ¬f2)
Gf
def
= ¬F¬f
f1 W f2
def
= (f1 U f2) ∨Gf2
Semantics. The satisfaction relation (|=) can be deﬁned w.r.t. an ω-word w ∈
(2P)ω and a position i ∈N. Inductively:
– w, i ̸|= ⊥for each w and i.
– w, i |= p iﬀp ∈w(i).
– w, i |= f1 →f2 iﬀeither w, i ̸|= f1 or w, i |= f2.
– w, i |= Xf iﬀw, i + 1 |= f.
– w, i |= f1 U f2 iﬀw, j |= f2 for some j ≥i and w, k |= f1 for each k ∈[i, j).
We may abbreviate w, 0 |= f as w |= f. The language of f, denoted by L (f),
consists of all ω-words initially satisfying f, i.e., L (f) = {w ∈(2P)ω | w |= f}.
X, U and W are respectively the “next”, “until” and “weak until” operators.
According to the deﬁnition, Gf holds if f holds at each position, and Ff is true
if f eventually holds at some position. In addition, f1 W f2 holds if at every
position, either f1 holds, or at some previous position f2 holds.

Reasoning About Periodicity on Inﬁnite Words
203
2.3
Automata on Finite/Inﬁnite-Words
An automaton is a tuple A = (Q, Σ, δ, Q0, F) where: Q is a ﬁnite set of states;
Σ is a ﬁnite alphabet; δ : Q × Σ →2Q, is the transition function; Q0 ⊆Q is a
set of initial states; and F ⊆Q is a set of accepting states.
The automaton A is deterministic, if #Q0 = 1, and #δ(q, a) = 1 for each
q ∈Q and a ∈Σ.
In this paper, we are both concerned about automata on ﬁnite and inﬁnite
words. For an automaton A on inﬁnite (resp. ﬁnite) words, a run of A over a word
w ∈Σω (resp. w ∈Σ∗) is a sequence q0, q1, . . . ∈Qω (resp. q0, q1, . . . , q|w| ∈Q∗),
where q0 ∈Q0 and qi+1 ∈δ(qi, w(i)).
If A is an automaton on ﬁnite words, the run q0, q1, . . . , qm is accepting if
qm ∈F. Otherwise, if A is on inﬁnite word, the run q0, q1, . . . is accepting if
there are inﬁnitely many i’s having qi ∈F — such kind of acceptance is called
B¨uchi
[B¨uc62] acceptance condition. For this reason, in this paper, automata
on inﬁnite words are also called B¨uchi automata.
For convenience, we in what follows use a three-letter-acronym to designate
the type of an automaton: The ﬁrst letter can be either “N” (non-deterministic)
or “D” (deterministic). The second letter can either be “B” or “F”, referring
to B¨uchi and ﬁnite acceptance condition, respectively. The last letter is always
“A”, the acronym of automaton. For example, NBA stands for nondeterministic
B¨uchi automaton, and DFA means deterministic automaton on ﬁnite words.
A word w is accepted by A, if A has an accepting run on it. We denote by
L (A) the set of words accepted by A, call it the language of A. In the case of
L = L (A), we say that L is recognized by A.
It is well known that NFAs and DFAs are of equivalent expressive power,
because each NFA can be transformed into a DFA via the power-set construction.
However, it is not the case for B¨uchi automata. NBAs are strictly more expressive
than DBAs. As an example, let Σ = {a, b}, then the language consisting of “ω-
words involving ﬁnitely many a’s” can only be recognized by NBAs. For B¨uchi
automata, determinization requires more complex acceptance conditions, such
as Rabin or parity (cf. [Rab69,McN66]).
Languages recognized by NBAs are called ω-regular ones. Meanwhile, trans-
formation from LTL formulae to B¨uchi automata has been well studied ever
since the logic was presented (cf. [TH02,ST03] etc.).
Theorem 1. Given an LTL formula f, there is an NBA Af such that L (Af) =
L (f).
Theorem 2. Given an automaton A = (Q, Σ, δ, Q0, F), then there exists an
automaton A′ = (Q′, Σ, δ′, Q′
0, F ′) such that L (A′) = L (A) and #Q′
0 = 1.
Proof. Let q be some new state not belonging to Q, then: let Q′ = Q ∪{q}; let
the function δ′ be the extension of δ by deﬁning δ′(q, a) = 
q0∈Q0 δ(q0, a) for
each a ∈Σ; let Q′
0 = {q}; and, if A is an NFA/DFA and Q0 ∩F ̸= ∅then we
let F ′ = F ∪{q}, otherwise, let F ′ = F.
⊓⊔

204
W. Liu et al.
2.4
Safety and Liveness
Generally speaking, each set L ⊆(2P)ω deﬁnes a property, and a formula f
corresponds to L if L (f) = L. Remind that in this deﬁnition, f is not necessary
to be an LTL formula. Actually, it could be a formula of more (or less) powerful
logic within linear-time framework.
There are two kinds of fundamental properties relating to temporal logics,
called safety and liveness. Informally, safety and liveness are described as “the
bad thing never happens” and “the good thing eventually happens”, respectively.
Below we give a formal characterization, which is introduced in [AS85].
Safety: A formula f corresponds to a safety property if: For every ω-word w,
w ̸|= f implies there is some ﬁnite preﬁx (called “bad-preﬁx”) w′ of w, such
that w′ · w′′ ̸|= f for each ω-word w′′.
Liveness: A formula f corresponds to a liveness property if: For every ﬁnite-
word w, there is some ω-word w′ having w · w′ |= f.
Theorem 3 ([AS85,CS01]). For safety and liveness properties, we have:
1. Safety properties are closed under ﬁnite unions and arbitrary intersections.
2. Liveness properties are closed under arbitrary unions, but not under
intersections.
3. ⊤is the only property which is both a safety and a liveness property.
4. For any property f, there exists a liveness property g and a safety property h
such that f = g ∧h.
The last proposition is the so-called decomposition theorem. As an example,
the LTL formula f U g can be decomposed as (fWg) ∧Fg, where f W g corre-
sponds to a safety property, whereas Fg corresponds to a liveness property.
Lemma 1. If f = 
i fi corresponds to a liveness property, then so does each fi.
Proof. Otherwise, according to the decomposition theorem, fi can be written as
the conjunction of some gi and hi, which respectively correspond to a liveness
property and a safety property. If hi ̸↔⊤, there is some ω-word w violating hi.
Just let w′ ∈(2P)∗be the bad-preﬁx of w w.r.t. hi, we thus have w′ · w′′ ̸|= f
for every w′′ ∈(2P)ω, contradiction!
⊓⊔
3
Periodicity and Step Variables
3.1
Periodicity: Beyond Omega-Regular
In [Wol83], Wolper pointed out that: The property G2p, namely “p holds at every
even moment”, is not expressible in LTL. Indeed, L (G2p) = 
k∈N L (X2kp),
where Xn is the shorthand of n successive X-operators. This implies that lan-
guages captured by LTL formulae are not closed under inﬁnite intersections.
This naturally enlightens us to make one step ahead — we would like to know:
“Are ω-regular languages closed under inﬁnite intersections/unions?” Now, let us
consider the language 
k>0 L (Gkp), which consists of all ω-words along which
p holds periodically. Remind that w |= Gkp if w, i × k |= p for each i ∈N.

Reasoning About Periodicity on Inﬁnite Words
205
Theorem 4. The language 
k>0 L (Gkp) is not ω-regular.
Proof. Assume by contradiction that this language is ω-regular, then there is
an NBA A precisely recognizing it. Therefore, each ω-word being of the form
({p} · ∅k)ω is accepted by A.
Suppose that A has n states, and let us ﬁx some k > n. Suppose that the
corresponding accepting run of A on w0 = ({p} · ∅k)ω is σ0 = s0, s1, s2, . . ., and
we denote si by σ0(i). Since we have totally n states, for each t ∈N, there exists
a pair (it, jt), s.t. 0 < it < jt ≤k and σ0(t × (k + 1) + it) = σ0(t × (k + 1) + jt).
Since σ0(i0) = σ0(j0) in the case of t = 0, for any ℓ0 > 0, from Pumping
lemma, A has the run
σ1 = σ0(0), σ0(1), . . . , σ0(i0), [σ0(i0 + 1), . . . , σ0(j0)]ℓ0, σ0(j0 + 1), σ0(j0 + 2) . . .
on the word
w1 = ({p} · ∅i0−1 · ∅(j0−i0)×ℓ0 · (∅)k−j0+1) · ({p} · ∅k)ω.
σ1 is deﬁnitely an accepting run because states occurring inﬁnitely often in σ0
are the same as that in σ1. Let L0 = k + (j0 −i0) × (ℓ0 −1), then we have
w1 = ({p} · ∅L0) · ({p} · ∅k)ω. The above process is depicted by Fig. 1.
Also note that σ0(ℓ) = σ1(ℓ+L0−k) for each ℓ> k, and hence σ0(k+1+i1) =
σ0(k + 1 + j1) implies σ1(L0 + 1 + i1) = σ1(L0 + 1 + j1). Then, for any ℓ1 > 0,
using Pumping lemma again, A also has an accepting run on the word
w2 = ({p} · ∅L0) · ({p} · ∅i1−1 · ∅(j1−i1)×ℓ1 · (∅)k−j1+1) · ({p} · ∅k)ω.
Now, let L1 = k+(j1 −i1)×(ℓ1 −1), then w2 = ({p}·∅L0)·({p}·∅L1)·({p}·∅k)ω.
Likewise and stepwise, we may obtain a sequence of ω-words accepted by A:
– w0 = ({p} · ∅k)ω.
– w1 = ({p} · ∅L0) · ({p} · ∅k)ω.
– w2 = ({p} · ∅L0) · ({p} · ∅L1) · ({p} · ∅k)ω.
– · · ·
Since that wi+1 is constructed based on wi, we can always choose a proper ℓi+1
to guarantee that Li+1 > Li for each i. Then, consider the limit
w∞= ({p} · ∅L0) · ({p} · ∅L1) · ({p} · ∅L2) · . . .
of all such wis: On one hand, we have w∞∈L (A) because the corresponding
run σ∞is accepting — observe that each occurrence of an accepting state in σ0
will be “postponed” to at most ﬁnitely many steps in σ∞. On the other hand,
the “distance” between two adjacent “occurrences” of p monotonically increases,
and it would be eventually larger than any ﬁxed number — this implies that p
cannot have a period w.r.t. w∞. We thus get a contradiction because A accepts
some word not belonging to 
k>0 L (Gkp).
⊓⊔

206
W. Liu et al.
Fig. 1. The construction of w1 from w0.
Note that for each ﬁxed number k > 0, the language L (Gkp) is ω-regular,
and it is not the case for 
k>0 L (Gkp), and we thus have the following corollary.
Corollary 1. ω-regular languages are not closed under inﬁnite unions and/or
intersections.
Remark 1. We say that ω-regular languages are not closed under inﬁnite inter-
sections because such languages are closed under complement.
⊓⊔
3.2
Logics with Step Variables
To express periodicity, we tentatively propose to add (quantiﬁed) step variables
into logics. As an example, if we choose μTL as the base logic, then the afore-
mentioned property can be described as1 ∃k.νZ.(p ∧XkXZ) and here k is a
step variable. As another example, one can see that ∀k.XkXkp is precisely the
description of G2p.
From now on, we ﬁx a set K of step variables, whose elements range over k, k1,
k2, etc., and each step variable is interpreted as a natural number. In addition, we
require that the base logic must involve the X-operator to designate “distance
between events”. Therefore, at ﬁrst thought, each such extension involves the
following logic (called core logic)
f ::= ⊥| p | f →f | Xf | Xkf | ∃k.f
as fragment2. For convenience, we deﬁne ∀k.f as the shorthand of ¬∃k.¬f (recall
that ¬g stands for g →⊥).
Since we have step variables, when giving semantics of a formula, besides an
ω-word and a position, an evaluation v : K →N is also required. For the core
logic, we deﬁne the semantics as follows.
1 Note that we here have an extra X in the formula, because k can be assigned to 0.
2 Remind that step variables cannot be instantiated as concrete numbers in such logic,
hence we have both Xf and Xkf in the grammar.

Reasoning About Periodicity on Inﬁnite Words
207
– w, i ̸|=v ⊥for every w ∈(2P)ω, i ∈N.
– w, i |=v p iﬀp ∈w(i).
– w, i |=v f1 →f2 iﬀeither w, i ̸|=v f1 or w, i |=v f2.
– w, i |=v Xf iﬀw, i + 1 |=v f.
– w, i |=v Xkf iﬀw, i + v(k) |=v f.
– w, i |=v ∃k.f iﬀthere is some n ∈N such that w, i |=v[k←n] f.
Here, v[k ←n] is also an evaluation which is almost identical to v, except that
it assigns n to k. To simplify notations, when f is a closed formula3, we often
omit v from the subscript; meanwhile, we can also omit i whenever i = 0.
Though such kind of extensions seems to be natural and succinct, however,
we will show that the satisfiability problem, even if for the core logic, is not
decidable. But before that, we ﬁrst introduce the following notations:
– We abbreviate X . . . X
  
n times
f and Xk . . . Xk



n times
f as Xnf and Xn×kf, respectively, where
n ∈N and k ∈K.
– We sometimes directly write Xt1Xt2f as Xt1+t2f.
Note that in this setting, both the addition (+) and the multiplication (×)
are communicative and associative. Meanwhile, “×” is distributive w.r.t. “+”,
namely, t1 × t2 + t1 × t3 can be rewritten as t1 × (t2 + t3).
Theorem 5. The satisfiability problem of the core logic is undecidable.
Proof. Our goal is to show that “each formula of Peano arithmeticcan be encoded
with the core logic”. To this end, we need to build the following predicates:
1. Fix a proposition p ∈P, let fp be ∀k1.∀k2.∃k3.(Xk1+k3p ̸↔Xk1+k2+k3+1p).
Actually, fp just depicts the non-shifting property of p. i.e., w |= fp only if
for each i, j ∈N with i < j, there is some t having: either (w, i + t |= p and
w, j + t ̸|= p) or (w, i + t ̸|= p and w, j + t |= p) — indeed, one can just view
k1 as i and view k1 + k2 + 1 as j. Note that fp is satisﬁed by any ω-word
along which p occurs inﬁnitely often and the distance between two adjacent
occurrences of p monotonically increases.
2. Let P=(k1, k2)
def
= fp ∧∀k.(Xk1+kp ↔Xk2+kp). Hence, w, i |=v P=(k1, k2)
iﬀv(k1) = v(k2). Because, if v(k1) ̸= v(k2), according to the deﬁnition of
fp, there must exist some n ∈N, such that p diﬀers from w(v(k1) + n) and
w(v(k2) + n).
3. Let P<(k1, k2)
def
= ∃k.P=(k1 +k +1, k2). Then, w, i |=v P<(k1, k2) iﬀv(k1) <
v(k2).
4. Subsequently, we use P+(k1, k2, k3) to denote P=(k1 + k2, k3). According to
the deﬁnition, w, i |=v P+(k1, k2, k3) iﬀv(k3) = v(k1 + k2) = v(k1) + v(k2).
3 That is, f involves no free variable.

208
W. Liu et al.
5. Now, let us ﬁx another proposition q ∈P and deﬁne
fq
def
= q ∧Xq ∧∀k1.∃k2.Xk1+k2q ∧
∀k1.∀k2.∀k3.(Xk1q ∧Xk2q ∧Xk3q∧
P<(k1, k2) ∧P<(k2, k3)∧
∀k4.((P<(k1, k4) ∧P<(k4, k2)) ∨(P<(k2, k4) ∧P<(k4, k3)) →¬Xk4q)
→∃k5.∃k6.(P+(k5, k1, k2) ∧P+(k6, k2, k3) ∧P+(2, k5, k6)))
We may assert that w, i |= fq iﬀi is a complete square number (i.e., i = j2 for
some j). Let us explain: The ﬁrst line indicates that q holds inﬁnitely often,
and it holds at the positions of 0 and 1. For every three adjacent positions
k1, k2, k3 at which q holds (hence, q does not hold between k1 and k2, nor
between k2 and k3), we have (k3 −k2) = 2 + (k2 −k1). Inductively, we can
show that q becomes true precisely at positions 0, 1, 4, . . . , (n −1)2, n2,
(n + 1)2, . . . 4.
6. We let P2(k1, k2) be
fq ∧Xk2q ∧Xk2+2×k1+1q ∧¬∃k3.(P<(k2, k3) ∧P<(k3, 2 × k1 + k2 + 1) ∧Xk3q)
then w |=v P2(k1, k2) iﬀv(k2) = (v(k1))2.
7. Lastly, we deﬁne
P×(k1, k2, k3)
def
= ∃k4.∃k5.∃k6.(P2(k1, k4) ∧P2(k2, k5)
∧P2(k1 + k2, k6) ∧P=(2 × k3 + k4 + k5, k6))
Then, once w |=v P×(k1, k2, k3) holds, we can infer that there is some evalua-
tion v′ which agrees with v on k1, k2 and k3 (i.e., v′(ki) = v(ki) for i = 1, 2, 3)
having
⎧
⎪
⎪
⎨
⎪
⎪
⎩
v′(k4) = (v′(k1))2
v′(k5) = (v′(k2))2
v′(k6) = (v′(k1) + v′(k2))2
v′(k6) = v′(k4) + v′(k5) + 2 × v′(k3)
and we thus subsequently have v(k3) = v(k1) × v(k2).
Therefore, “addition”, “multiplication”, and the “less than” relation over natural
numbers can be encoded in terms of the core logic. Since quantiﬁers are also
involved here, the satisfiability problem of Peano arithmetic, which is known
to be undecidable (cf. [G¨od31,Chu36]), is now reduced to that of the core logic. ⊓⊔
4
The Logic QPLTL
Theorem 5 indicates that the unrestricted use of step variables leads to undecid-
ability for the satisfiability problem. To obtain a decidable extension, we need
to impose strong syntactic constraints to the logic. In this paper, we investigate
the logic Quantiﬁed Periodic LTL (QPLTL for short), based on LTL.
4 The encoding of fq is enlightened by [Sch10].

Reasoning About Periodicity on Inﬁnite Words
209
4.1
Syntax and Semantics
A QPLTL formula can be one of the following forms:
(I) An LTL formula.
(II) A formula like f1 Un f2 or f1 Wn f2, where f1 and f2 are LTL formulae, n
is a positive natural number.
(III) Boolean or temporal combinations of (I) and (II).
(IV) A formula being of the form
Æ
k.(f1 Uk f2) or
Æ
k.(f1 Wk f2), where
Æ
∈
{
E
,
A
}, k ∈K, f1 and f2 are two LTL formulae.
Given an ω-word w ∈(2P)ω, and a position i ∈N, we deﬁne the satisfaction
relation as follows.
– Satisfaction of an LTL formula is deﬁned in the way same as before.
– w, i |= f1 Un f2 iﬀthere is some t ∈N such that w, i + t × n |= f2 and
w, i + j × n |= f1 for every j < t.
– w, i |= f1 Wn f2 iﬀeither w, i |= f1 Un f2 or w, i+t×n |= f1 for each t ∈N.
– Boolean and temporal combinations are deﬁned accordingly with the corre-
sponding operators.
– w, i |= ∃k.(f1 Uk f2) (resp. w, i |= ∃k.(f1 Wk f2)) iﬀthere is some n > 0 such
that w, i |= f1 Un f2 (resp. w, i |= f1 Wn f2).
– w, i |= ∀k.(f1 Uk f2) (resp. w, i |= ∀k.(f1 Wk f2)) iﬀfor each n > 0 we have
w, i |= f1 Un f2 (resp. w, i |= f1 Wn f2).
As usual, we directly write w, 0 |= f as w |= f, and we also deﬁne the following
derived notations5.
Fnf
def
= ⊤Un f
Gnf
def
= f Wn ⊥
Æ
k.Fkf
def
=
Æ
k.⊤Uk f
Æ
k.Gkf
def
=
Æ
k.f Wn ⊥
Indeed, from the proof of Theorem 5, one can see that nested use of quantiﬁers
leads to undecidability of satisfaction decision. Thus, in QPLTL, we use at most
one step variable in a formula.
Remark 2. f1 U1 f2 and f1 W1 f2 can be just written as f1 U f2 and f1 W f2,
which coincide with the deﬁnitions given in LTL.
Also remind that a step variable must be interpreted as a positive number —
beware that this is diﬀerent from that in Sect. 3.2. This is just for the following
consideration: If we allow assigning 0 to a step variable, the formula ∃k.Gkp is no
longer the description of “p holds periodically”, because this formula is weaker
than G0p (which is equivalent to p). Note that in such setting, the “core logic”
remains undecidable — the proof can be obtained via doing a simple adaptation
from that of Theorem 5.
5 We do not consider the “releases” (R) operator here, which is the duality of U.
Indeed, f1Rf2 is equivalent to f2 W(f1 ∧f2).

210
W. Liu et al.
To make things compatible, for the formulae like f1 Un f2 or f1 Wn f2, we
don’t allow n = 0. Actually, according to the deﬁnition, we can see that f1 U0f2
and f1 W0 f2 are just merely f2 and f1 ∨f2, and such operators are redundant.
Note that Xn is still the abbreviation of n successive Xs. We don’t explic-
itly add the formulae like ∃k.Xkf and ∀k.Xkf into the logic, because they are
essentially XFf and XGf, respectively.
⊓⊔
4.2
The Decision Problem
In this section, we will show that the satisfiability problem of QPLTL is
decidable. Indeed, this proof also reveals the close connection among liveness,
safety and periodicity.
We can equivalently transform each LTL formula f to make it involve only
literals (i.e., ⊤, ⊥, formulae like p or ¬p), ∨, ∧, X, U and W. We in what follows
call it the positive normal form (PNF, for short) of f. An LTL formula is U-free
if its PNF involves no U-operator.
Lemma 2 ([CS01]). Each U-free LTL formula corresponds to a safety property.
Below we give a characterization of LTL formulae corresponding to liveness
properties. In some sense, it could be considered as a normal form of such kind
of star-free properties.
Theorem 6. If the LTL formula f corresponds to a liveness property, then it
can be equivalently written as 
i(f ′
i ∨Ff ′′
i ), where each f ′
i corresponds to a safety
property.
Proof. Suppose that f is already in its PNF, then we conduct a series of trans-
formations on f.
First of all, we use the pattern f1 U f2 ↔(f1 W f2) ∧Ff2 to replace each
occurrence of U operator with that of W and F.
Then, use the following rules
X(f1 ∧f2) ↔Xf1 ∧Xf2
X(f1 ∨f2) ↔Xf1 ∨Xf2
X(f1 W f2) ↔(Xf1)W(Xf2)
XFf ′ ↔FXf ′
to push X inward, until X occurrs only before a literal or another X.
Subsequently, repeatedly use the following schemas6
(f1 ∧f2)W f3 ↔(f1 W f3) ∧(f2 W f3)
f1 W(f2 ∨f3) ↔(f1 W f2) ∨(f1 W f3)
(f1 ∨Ff2)W f3 ↔F(f2 ∧X(f1 W f3)) ∨(f1 W f3) ∨F(f3 ∧Ff2) ∨FGFf2
f1 W(f2 ∧Ff3) ↔(f1 W f2) ∧(F(f2 ∧Ff3) ∨Gf1)
(Ff1)W f2 ↔f2 ∨F(f1 ∧Xf2) ∨F(f2 ∧Ff1) ∨FGFf1
f1 W(Ff2) ↔Gf1 ∨Ff2
6 For example, when dealing with (p1 ∨p2 ∧Fp3)W p4, we need ﬁrst transform the ﬁrst
operand into disjunctive normal form — i.e., rewrite it as ((p1∨Fp3)∧(p2∨Fp3))W p4,
and then conduct the transformation with the ﬁrst rule and the third rule. Similarly,
for the second operand, we need ﬁrst transform it into conjunctive normal form.

Reasoning About Periodicity on Inﬁnite Words
211
until the following holds: For each subformula f ′ = f1 W f2, if f1 or f2 involves
F, then f ′ must be contained in the scope of some other F. Remind that we
equivalently write GFfi as FGFfi in the second rule and the ﬁfth rule to fulﬁll
such requirement7. Note that when the third or the ﬁfth schema is applied, one
need further push Xs inward using the previous group of rules.
Lastly, we write the resulting formula into the conjunctive normal form 
i fi
where each fi = 
j fi,j. We now show that it must be of the desired form.
– First of all, since f corresponds to a liveness property, so does each fi (cf.
Lemma 1).
– Second, for each fi = 
j fi,j, if there is no such fi,j whose outermost operator
is F, then fi is U-free8. From Lemma 2, fi also corresponds to a safety property.
Hence, such fi is essentially equivalent to ⊤(cf. Theorem 3), and we can
simply remove this conjunct.
– Then, for the other fis: By applying the scheme Fg1 ∨. . .∨Fgn ↔F(g1 ∨. . .∨
gn), we can just preserve one disjunct having F as the outermost operator,
denote it by Ff ′′
i . Since other disjuncts are U-free, the disjunction of them,
denoted by f ′
i, corresponds to a safety property (cf. Theorem 3).
Then, the above discussion concludes the proof.
⊓⊔
Lemma 3 ([AS87]). Given an LTL formula f, the question “whether f corre-
sponds to a liveness property” is decidable. In addition, given a safety LTL for-
mula, then there is an automaton on ﬁnite words recognizing all its bad-preﬁxes.
Theorem 7. Given two LTL formulae f1 and f2, where f2 corresponds to a
safety property, then the question “whether exists some ω-word w such that w |=
Gf1 and w |= ∃k.Gk¬f2” is decidable.
Proof. Let A1 = (Q1, 2P, δ1, {q1}, F1) be the NBA recognizing L (Gf1), and
A2 = (Q2, 2P, δ2, {q2}, F2) be the NFA9 recognizing the bad-preﬁxes of f2.
For each q
∈
Q1, we can deﬁne a product
Aq
1 ⊗A2
def
=
(Q1 ×
Q2, 2P, δ′, {(q, q2)}), where
δ′((q′
1, q′
2), a) =

{(q′′
1 , q′′
2 ) | q′′
1 ∈δ1(q′
1, a), q′′
2 ∈δ2(q′
2, a)}
q′
2 ̸∈F
{(q′′
1 , q′′
2 ) | q′′
1 ∈δ1(q′
1, a), q′′
2 ∈Q2}
q′
2 ∈F .
The product is almost an automaton, but we are not concerned about its run
over words, hence the accepting state set is not given here. Instead, we will
deﬁne the notion of accepting loops: An accepting loop is a ﬁnite sequence
(q1,0, q2,0), (q1,1, q2,1), . . . , (q1,m, q2,m) ∈(Q1 × Q2)∗such that:
7 Note that the operator G is derived from W.
8 Because, the previous transformations could guarantee that: If the outermost oper-
ator of fi,j is not F, then no F occurs in fi,j.
9 Remind that each automaton can be equivalently transformed into another one hav-
ing a unique initial state, and the transformation is linear (cf. Theorem 2). Indeed,
to obtain a ﬁnite alphabet, here we may temporarily take P as the set constituted
with propositions occurring in f1 or f2.

212
W. Liu et al.
1. q1,0 = q1,m and q2,0 = q2,m.
2. For each 0 ≤i < m, there is some ai having (q1,i+1, q2,i+1) ∈δ′((q1,i, q2,i), ai).
3. There exists some 0 ≤i < m such that q1,i ∈F1.
4. There also exists some 0 ≤j < m such that q2,j ∈F2.
We will then show the following claim:
There is some ω-word w ∈(2P)ω making w |= Gf1 and w |= ∃k.Gk¬f2
iﬀthere is some q ∈Q1 such that Aq
1 ⊗A2 involves an accepting loop
starting from (q, q2).
=⇒: Suppose that w |= Gf1 and w |= ∃k.Gk¬f2 with the period n, namely
w |= Gn¬f2 — which implies w, i × n ̸|= f2 for every i ∈N.
Let σ = σ(0), σ(1), σ(2), . . . ∈Qω
1 be an accepting run of A1 on w. Then, there
exists some qf ∈F1 such that there are inﬁnitely many i’s having σ(i) = qf.
Because the run is inﬁnite, there must exist some q ∈Q1 fulﬁlling: there are
inﬁnitely many i’s having σ(i × n) = q. W.l.o.g., suppose that σ(i0 × n) = q.
Because w, i0 × n ̸|= f2, there exists some bad-preﬁx w′ of f2, which starts
from w(i0 × n). Let i1 be the number fulﬁlling that: σ(i1 × n) = q, and (i1 −
i0) × n > |w′|, and there exists some ℓ∈[i0 × n, i1 × n) having σ(ℓ) = qf ∈F1.
Since w′ is a bad-preﬁx of f2, there is a ﬁnite accepting run σ′ = σ′(0), σ′(1),
. . . , σ′(|w′|) of A2 on w′, where σ′(0) = q2 and σ′(|w′|) ∈F2.
Let t = (i1 −i0) × n, since t > |w|′, according to the construction, we can
prolong σ′ by deﬁning σ′(j) = σ′(|w′|) for each j ∈(|w′|, t) and σ′(t) = q2.
For each j ∈[0, t], we let σ′′(j) = σ(i0 × n + j). Thus, we get the accepting
loop (σ′′(0), σ′(0)), . . . , (σ′′(t), σ′(t)) in the product. Indeed, according to the
construction, we can see that both (σ′′(0), σ′(0)) and (σ′′(t), σ′(t)) are (q, q2),
and the loop is accepting.
⇐=: Conversely, suppose (q1,0, q2,0), . . . , (q1,n, q2,n) to be an accepting loop of
Aq
1 ⊗A2 where q1,0 = q1,n = q and q2,0 = q2,n = q2. We can, of course, assume
that q is reachable from q1 in A1 — if not so, such state can be safely removed.
Suppose that (q1,i+1, q2,i+1) ∈δ′((q1,i, q2,i), ai), we let w = (a0·a1·. . .·an−1)ω.
Also let m ∈[0, n) be the minimal index having q2,m ∈F2, then a0 ·a1 ·. . .·am−1
is a bad-preﬁx of f2, hence w violates f2 with the period n, and thus w |=
∃k.Gk¬f2.
What left is to ensure that w |= Gf1 also holds. Suppose that q1 reaches q
via reading the ﬁnite word w0, then w0 · w ∈L (A1), and thus w0 · w |= Gf1.
Consequently, we have w |= Gf1.
⊓⊔
Theorem 8. Given an LTL formula f, the question “whether ∃k.Gk¬f is sat-
isﬁable” is decidable.
Proof. First of all, we may decompose f as g∧h where g corresponds to a liveness
property and h corresponds to a safety property.
If h ̸↔⊤, then there is a ﬁnite-word w0 acting as the bad-preﬁx of h, and
hence a bad-preﬁx of f. Therefore, the ω-word wω
0 = w0 · w0 · w0 · . . . violates f
periodically, and thus ∃k.Gk¬f must be satisﬁable.

Reasoning About Periodicity on Inﬁnite Words
213
In what follows, we just consider the case that h ↔⊤, and hence f corre-
sponds to a liveness property. From Theorem 6, let us further assume the normal
form of f is m
i=1 fi, where fi = f ′
i ∨Ff ′′
i , and f ′
i corresponds to a safety property.
Thus, f is periodically violated, if and only if: there is some ω-word w, a set
of indices J ⊆{1, 2, . . . , m}, and a positive number n, such that for each i ∈N
we have w, i × n ̸|= fj for some j ∈J.
Then, the obligation is to detect the existence of such word w, index set J
and period n. First, we may choose the set J, and the number of such choices
are ﬁnite. We can further assume that w |= G¬f ′′
j for each j ∈J, because:
– If there is some j ∈J having w |= GFf ′′
j , then the disjunct fj is never
violated by w. In this case, we may choose some J′ ⊆J \ {j} to be the index
set.
– If there are ﬁnitely many i’s having w, i |= f ′′
j , then we may choose some
postﬁx of w to be the word.
Now, given J, the problem becomes: Find some ω-word w, which fulﬁlls w |=

j∈J G¬f ′′
j and w |= ∃k.Gk¬ 
j∈J f ′
j. Since that 
j∈J G¬f ′′
j is equivalent to
G 
j∈J ¬f ′′
j , and 
j∈J f ′
j corresponds to a safety property, from Theorem 7, we
know that this problem is decidable.
⊓⊔
Theorem 9. The satisfiability problem of QPLTL is decidable.
Proof. Since formulae of Type (I)–(III) can be expressed by some logics equal
to ω-regular languages, such as μTL or ETL, we here just consider formulae of
Type (IV).
First, we lift the negation operator (¬) to that group of formulae by deﬁning
¬
Æ
k.(f1 Uk f2)
def
=
Æ
k.(¬f2 Wk(¬f1 ∧¬f2))
¬
Æ
k.(f1 Wk f2)
def
=
Æ
k.(¬f2 Uk(¬f1 ∧¬f2))
where
Æ
is
A
(resp.
E
) if
Æ
is
E
(resp.
A
). Indeed, for each such formula f, we
can examine that w |= f iﬀw ̸|= ¬f for every ω-word w. Hence, such lifting
is admissible. This also implies that formulae of Type (IV) are closed under
negation.
Then, for such a formula, instead of deciding its satisﬁability, we would rather
decide its validity, because f is satisﬁable iﬀ¬f is not valid. Below gives the
decision approach.
(1) ∃k.(f1 Wk f2) is valid iﬀf1 ∨f2 is valid.
(2) ∀k.(f1 Wk f2) is valid iﬀf1 ∨f2 is valid.
(3) ∃k.(f1 Uk f2) is valid iﬀf2 ∨(f1 ∧Ff2) is valid.
(4) ∀k.(f1 Uk f2) is valid iﬀf1 ∨f2 is valid and ∃k.Gk¬f2 is not satisﬁable.
For (1) and (2): If
Æ
k.(f1 Wk f2) is valid, then w ̸|= f2 implies w |= f1 for
each ω-word w, hence f1 ∨f2 must be valid. Conversely, if f1 ∨f2 is valid, then
w |= f1 Wn f2 holds for any positive natural number n.

214
W. Liu et al.
For (3): The “only if” direction is also trivial, we just show the “if” direction.
Indeed, if f2 ∨(f1 ∧Ff2) is valid, then for each ω-word w, according to the
deﬁnition, w |= f2 implies w |= ∃k.(f1 Uk f2) holds; otherwise, if w |= f1 ∧Ff2,
w.l.o.g., assume that w, 0 |= f1 and w, n |= f2, this also guarantees w |=
∃k.(f1 Uk f2) because w |= f1 Un f2.
As for the “only if” direction of (4), in the same way as before, we can infer
that f1 ∨f2 should be valid if ∀k.(f1 Uk f2) is. Meanwhile, since for every ω-word
w and every n > 0 there exists some i ∈N having w, i×n |= f2, hence ∃k.Gk¬f2
should not be satisﬁable. Conversely, ∃k.Gk¬f2 is not satisﬁable implies that
∀k.Fkf2 is valid. Then, for each ω-word w and each n > 0, there exists a minimal
number i making w, i×n |= f2, and since f1 ∨f2 is valid, we have w, j ×n |= f1
for every j < i, hence w |= f1 Un f2.
⊓⊔
From the decision procedure, one can examine that each step could be accom-
plished with polynomial (in the size of the formula) space. Since PSPACE is
closed under relativization, namely, PSPACEPSPACE = PSPACE, we can
thus conclude that satisfiability of QPLTL is in PSPACE. On the other
hand, the satisfiability problem of LTL is also PSPACE-hard. Thus, we
have the following conclusion.
Corollary 2. The satisfiability problem of QPLTL is PSPACE-complete.
5
Discussion and Future Work
In this paper, we suggest to use quantiﬁed step variables to describe periodicity.
As an attempt, using LTL as the base logic, we can obtain one of its decidable
periodic extension — QPLTL.
Indeed, formulae of Type (I)–(III) constitutes a proper super logic of LTL,
whereas a subset of whole ω-regular properties. It is interesting to study the
relation between this set and ω-regular languages.
For formulae of Type (IV), actually, we may make a bit relaxation on that
part. For example, consider the formula f = ∃k.FGkp, which gives the assertion
“p eventually holds periodically”, and call such property soft periodicity. We can
see that f is satisﬁable if and only if ∃k.Gkp is satisﬁable.
To make the logic more ﬂexible in syntax and more expressive, as a future
work, we need carefully study some extensions of QPLTL. For example, Boolean
combinations of formulae of Type (IV), or combinations of Type (III) and (IV).
Further, we are also wonder about the decidability of the extension built up
from more expressive logics, such as linear-time μTL. The key issue is also to
establish the corresponding normal form of general liveness properties.
Acknowledgement. The ﬁrst author would thank Normann Decker, Daniel Thoma
and Martin Leucker for the fruitful discussion on this problem. We would also thank the
anonymous reviewers for their valuable comments on an earlier version of this paper.
Wanwei Liu is supported by Natural Science Foundation of China (No. 61103012, No.
61379054, and No. 61532007). Fu Song is supported by Natural Science Foundation of
China (No. 61402179 and No. 61532019).

Reasoning About Periodicity on Inﬁnite Words
215
References
[AS85] Alpern, B., Schneider, F.B.: Deﬁning liveness. Inf. Process. Lett. 21, 181–185
(1985)
[AS87] Alpern, B., Schneider, F.B.: Recognizing safety and liveness. Distrib. Com-
put. 2(3), 117–126 (1987)
[BB87] Banieqbal, B., Barringer, H.: Temporal logic with ﬁxed points. In: Banieqbal,
B., Barringer, H., Pnueli, A. (eds.) Temporal Logic in Speciﬁcation.
LNCS, vol. 398, pp. 62–74. Springer, Heidelberg (1989). doi:10.1007/
3-540-51803-7 22
[B¨uc62] B¨uchi, J.R.: On a decision method in restricted second order arithmetic. In:
Proceedings of the International Congress on Logic, Method and Philosophy
of Science 1960, pp. 1–12, Palo Alto. Stanford University Press (1962)
[Chu36] Church, A.: A note on the Entscheidungsproblem. J. Symb. Log. 1, 101–102
(1936)
[CS01] Clarke, E.M., Schlingloﬀ, B.: Model checking. In: Robinson, A., Voronkov, A.
(eds.) Handbook of Automated Reasoning. vol. 2, Chapt. 24, pp. 1369–1522.
MIT and Elsevier Science Publishers (2001)
[DG08] Diekert, V., Gastin, P.: First-order deﬁnable languages. In: Flum, J., Gr¨adel,
E., Wilke, T. (eds.) Logic and Automata: History and Perspectives [in Honor
of Wolfgang Thomas], vol. 2, pp. 261–306. Amsterdam University Press
(2008)
[G¨od31] G¨odel, K.: ¨Uber formal unentscheidbare s¨atze der principia mathematica und
verwandter system I. Monatshefte f¨ur Methematik und Physik 38, 173–198
(1931)
[LS07] Leucker, M., S´anchez, C.: Regular linear temporal logic. In: Jones, C.B., Liu,
Z., Woodcock, J. (eds.) ICTAC 2007. LNCS, vol. 4711, pp. 291–305. Springer,
Heidelberg (2007). doi:10.1007/978-3-540-75292-9 20
[McN66] McNaughton, R.: Testing and generating inﬁnite sequences by a ﬁnite
automaton. Inf. Comput. 9, 521–530 (1966)
[Pnu77] Pnueli, A.: The temporal logic of programs. In: Proceedings of 18th IEEE
Symposium on Foundation of Computer Science (FOCS 1977), pp. 46–57.
IEEE Computer Society (1977)
[Rab69] Rabin, M.O.: Decidability of second order theories and automata on inﬁnite
trees. Trans. AMS 141, 1–35 (1969)
[Sch10] Schweikardt, N.: On the expressive power of monadic least ﬁxed point logic.
Theor. Comput. Sci. 350(2–3), 1123–1135 (2010)
[ST03] Sebastiani, R., Tonetta, S.: “More deterministic” vs. “smaller” B¨uchi
automata for eﬃcient LTL model checking. In: Geist, D., Tronci, E. (eds.)
CHARME 2003. LNCS, vol. 2860, pp. 126–140. Springer, Heidelberg (2003).
doi:10.1007/978-3-540-39724-3 12
[TH02] Taurainen, H., Heljanko, K.: Testing LTL formula translation into B¨uchi
automata. STTT 4, 57–70 (2002)
[Tho79] Thomas, W.: Star-free regular sets of omega-sequences. Inf. Control 42, 148–
156 (1979)
[VW94] Vardi, M.Y., Wolper, P.: Reasoning about inﬁnite computations. Inf. Comput.
115(1), 1–37 (1994)
[Wol83] Wolper, P.: Temporal logic can be more expressive. Inf. Control 56(1–2),
72–99 (1983)

On Equivalence Checking of Nondeterministic
Finite Automata
Chen Fu1,3(B), Yuxin Deng2, David N. Jansen1, and Lijun Zhang1,3
1 State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences, Beijing, China
fchen@ios.ac.cn
2 Shanghai Key Laboratory of Trustworthy Computing,
East China Normal University, Shanghai, China
3 University of Chinese Academy of Sciences, Beijing, China
Abstract. We provide a comparative study of some typical algorithms
for language equivalence in nondeterministic ﬁnite automata and vari-
ous combinations of optimization techniques. We ﬁnd that their practi-
cal eﬃciency mostly depends on the density and the alphabet size of the
automaton under consideration. Based on our experiments, we suggest
to use HKC (Hopcroft and Karp’s algorithm up to congruence) [4] if the
density is large and the alphabet is small; otherwise, we recommend the
antichain algorithm (Wulf, Doyen, Henzinger, Raskin) [6]. Bisimulation
equivalence and memoisation both pay oﬀin general. When comparing
highly structured automata over a large alphabet, one should use sym-
bolic algorithms.
1
Introduction
Checking whether two nondeterministic ﬁnite automata (NFA) accept the same
language is important in many application domains such as compiler construc-
tion and model checking. Unfortunately, solving this problem is costly: it is
PSPACE-complete [16].
However, the problem is much easier when restricted to deterministic ﬁnite
automata (DFA) where nondeterminism is ruled out. Checking language equiv-
alence for DFA can be done using either minimisation [10,12,18] or Hopcroft
and Karp’s algorithm (HK algorithm) [9]. The former searches for equivalent
states in the whole state space of a ﬁnite automaton or the disjoint union of two
automata. This works well in practice because for DFA, bisimulation equivalence,
simulation equivalence and language equivalence coincide, and both bisimulation
and simulation can be computed in polynomial time. The HK algorithm is more
appropriate in the case where one only wants to know if two particular states are
language equivalent because it is an “on-the-ﬂy” algorithm that explores merely
the part of state space that is really needed. It should be mentioned that the
HK algorithm exploits a technique nowadays called coinduction [15].
A straightforward idea for checking the language equivalence of two NFA is
to convert them into DFA through a standard powerset construction, and then
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 216–231, 2017.
https://doi.org/10.1007/978-3-319-69483-2_13

On Equivalence Checking of Nondeterministic Finite Automata
217
execute an equivalence checking algorithm for DFA. Since there are exponen-
tially many state sets in the powerset, one would like to avoid constructing them
as much as possible. In particular, if one only needs to decide if two speciﬁc
sets of states in a nondeterministic ﬁnite automaton are equivalent, one can con-
struct the state sets on-the-ﬂy, and simultaneously try to build a bisimulation
that relates these sets. With this approach, the number of constructed sets is
usually much smaller than the exponential worst-case bound. In terms of imple-
mentation, it is easy to adapt a naive version of the HK algorithm from DFA
to NFA: The algorithm maintains two sets todo and R. The set todo contains
the pairs (X, Y ) to be checked, where X and Y are two sets of NFA states. The
set R contains the checked (equivalent) pairs. It is a bisimulation relation when
the algorithm terminates successfully. There are several optimizations for this
algorithm:
1. Reduce the set R by constructing a relation that is not a bisimuation but is a
bisimulation up to equivalence, e.g. the HK algorithm [9], or up to congruence,
e.g. the HKC algorithm [4].
2. Often todo is implemented as a list that may contain repeated elements. Avoid
these repetitions by some memoisation techniques [13].
3. Represent the automata symbolically rather than explicitly by using binary
decision diagrams (BDD) [13,19].
4. Saturate the given automata with respect to bisimulation equivalence or sim-
ulation preorder.
An alternative approach to checking NFA equivalence is to use antichain
algorithms [2,6]. The basic idea is to check language inclusion in both directions.
This approach also exploits the coinduction technique: in order to check whether
the language of a set of NFA states X is a subset of the language of a set of
NFA states Y , it simultaneously tries to build a simulation relation, relating
each state x ∈X (as a state in the NFA) to Y (as a state in the DFA). This
algorithm can also be optimized by reducing the list todo by memoisation or by
reducing the list R with antichains. The antichain algorithm can be enhanced
by exploiting any preorder contained in language inclusion [2]. For example, the
simulation preorder can be used for this purpose.
In this paper we investigate the mentioned algorithms and their combinations
with optimizations to achieve the best time eﬃciency. We ﬁnd that in most cases
the antichain algorithm is stable and often outperforms other algorithms. In con-
trast, the performance of HK and HKC algorithms may vary a lot, depending
on the size of the alphabet and the density of transitions in the automata under
consideration. When the size of the alphabet is small (e.g. 2) and the density is
large (e.g. 1.25 or 1.5), HKC is the best choice. Otherwise, computing congru-
ence closures is very costly and renders HKC impractical to use. One should use
memoisation because it mostly accelerates the algorithms. Further, if the consid-
ered automata are highly structured and over a large alphabet, one should try
the symbolic algorithms because the BDDs are usually small in such situations.
Finally we suggest to minimize the automata by bisimilarity instead of saturat-
ing the automata by similarity before performing the algorithms. Although the

218
C. Fu et al.
latter is more powerful, the time eﬃciency of computing the bisimilarity makes
the total time shorter.
The rest of this paper is organized as follows. In Sect. 2 we recall some basic
concepts. In Sect. 3 we introduce the HK, HKC and antichain algorithms and
relevant optimizations. In Sect. 4 we assess those techniques introduced previ-
ously by comparing their running times experimentally. We discuss related work
in Sect. 5 and summarize our recommendations in the concluding Sect. 6.
2
Preliminaries
Finite Automata. A Nondeterministic Finite Automaton (NFA) A is a tuple
(Σ, Q, I, F, δ) where: Σ is an alphabet, Q is a ﬁnite set of states, I ⊆Q is a non-
empty set of initial states, F ⊆Q is a set of accepting states, and δ ⊆Q×Σ ×Q
is the transition relation. A word u = u1u2 . . . un is accepted by q ∈Q if there
exists a sequence q0u1q1u2 . . . unqn such that q0 = q, qj ∈δ(qj−1, uj) for all
0 < j ≤n and qn ∈F. Deﬁne L(q) = {u | u is accepted by q} as the language
of q and L(A) = 
q∈I L(q) as the language of A. Two NFA A, B are said to be
language equivalent iﬀL(A) = L(B).
An NFA is called deterministic if |I| = 1 and |δ(q, a)| ≤1 for all q ∈Q and
a ∈Σ. For each NFA A = (Σ, Q, I, F, δ), we can use the standard powerset
construction [3, Sect. 4.1] to transform it to a DFA A♯= (Σ, Q♯, I♯, F ♯, δ♯) with
the same language.
The NFA equivalence checking problem is to decide whether two given NFA
accept the same language.
Simulation and Bisimulation
Deﬁnition 1. Let R, R′ ⊆Q×Q be two binary relations on states, we say that
R s-progresses to R′, denoted R ↣s R′, if x R y implies:
– if x is accepting, then y is accepting;
– for any a ∈Σ and x′ ∈δ(x, a), there exists some y′ ∈δ(y, a) such that
x′ R′ y′.
A simulation is a relation R such that R ↣s R and a bisimulation is a relation
R such that R ↣s R and R−1 ↣s R−1, where R−1 is the inverse relation of R.
The largest simulation and bisimulation are called similarity and bisimilarity,
denoted by ≾and ∼, respectively. For any NFA, if a bisimulation between two
states can be found, then they are language equivalent. Similarly, for any NFA,
if a simulation between two states can be found, for example x ≾y, then L(x) ⊆
L(y). The reverse direction of these two implications holds in general only for
DFA. Computing similarity needs O(|δ| · |Q|) time [1,7,8,14], while computing
bisimilarity is faster, as it is in O(|δ| · log |Q|) [18]. Bisimulation is a sound proof
technique for checking language equivalence of NFA and it is also complete for
DFA. Simulation is a sound proof technique for checking language inclusion of
NFA and it is also complete for DFA.

On Equivalence Checking of Nondeterministic Finite Automata
219
Binary Decision Diagrams. A standard technique [13,19] for working with
automata over a large alphabet consists in using BDDs to represent the
automata. A Binary Decision Diagram (BDD) over a set of variables Xn =
{x1, x2, . . . , xn} is a directed, acyclic graph having leaf nodes and internal nodes.
There is exactly one root node in a BDD; each internal node is labelled with a
variable and has two outgoing edges whose ends are other nodes. The leaf nodes
are labelled with 0 or 1. After ﬁxing the order of variables, any BDD can be
transformed into a reduced one which has the fewest nodes [5]. In the sequel, we
only work with reduced ordered BDDs, which we simply call BDDs.
BDDs can be used to represent functions of type 2Xn →{0, 1}. Here, we
use BDDs to represent NFA. The advantage is that one often does not need
many variables. For example, if there are 2k letters, one only needs k variables
to encode (the characteristic function of) a set of letters.
3
Algorithms and Optimizations
3.1
A Naive Algorithm for Language Equivalence Checking
A naive adaptation of Hopcroft and Karp’s algorithm from DFA to NFA is
shown in Algorithm 1. Starting with the two sets of initial states, we do the
powerset construction on-the-ﬂy for both NFA and simultaneously try to build
a bisimulation relating these two sets. The sets of states of the NFA become the
states of the DFA constructed by powerset construction. We use two sets: todo
and R. We call a pair (X, Y ) a bad pair if one of X and Y is accepting but
the other is not. Whenever we pick a pair from the set todo, we check if it is a
bad pair; if it isn’t, we generate their successors and insert these successor pairs
into todo. The set R is used to store the processed pairs: if a pair is in R, the
Algorithm 1. The Naive Eq algorithm for checking NFA equivalence
Input: two NFA A = (Σ, QA, IA, FA, δA) and B = (Σ, QB, IB, FB, δB)
Output: “Yes” if L(A) = L(B), otherwise “No”
1: R := ∅, todo := {(IA, IB)}
2: while todo ̸= ∅do
3:
Pick (X, Y ) ∈todo and remove it
4:
if (X, Y ) ̸∈R then
5:
if (X, Y ) is a bad pair then
6:
return “No, L(A) ̸= L(B).”
7:
end if
8:
for all a ∈Σ do
9:
todo := todo ∪{(δ♯
A(X, a), δ♯
B(Y, a))}
10:
end for
11:
R := R ∪{(X, Y )}
12:
end if
13: end while
14: return “Yes, L(A) = L(B).”

220
C. Fu et al.
states are language equivalent if the pairs in todo are language equivalent. In a
formula, R ↣s R ∪todo and R−1 ↣s R−1 ∪todo−1. If the algorithm terminates
with the return value “yes”, R is a bisimulation between A♯and B♯.
When checking (X, Y ), the algorithm eventually determinizes both parts cor-
responding to X and Y , that is, it compares X (regarded as state of A♯) with
Y (regarded as state of B♯). Based on the naive algorithm, one can imagine
several optimizations: One idea is to try to reduce the number of pairs in R; the
algorithm proposed in [9] by Hopcroft and Karp (called the HK algorithm) does
so. In [4] Bonchi and Pous extend the HK algorithm by exploiting the technique
of bisimulation up to congruence and obtain the HKC algorithm, in which R
contains even fewer pairs. Another idea is to reduce the number of pairs in todo
by so-called memoisation. The observation is very simple: one does not need
to insert the same pair into todo more than once. (In practice, the set todo is
often implemented as a list, so it actually can “contain” an element multiple
times.) Besides these, one can do some preprocessing: One can use bisimilarity
or similarity to saturate the NFA before running the algorithms, in the hope
to accelerate the main algorithm. In addition, we also test whether it is a good
idea to use BDDs to represent transition functions and then perform symbolic
algorithms.
3.2
Reducing R
We only need to know whether there exists a bisimulation relating two sets,
and it is unnecessary to build the whole bisimulation. So we can reduce R to a
relation that is contained in – and suﬃcient to infer – a bisimulation.
HK Algorithm. Hopcroft and Karp [9] propose that if an encountered pair is
not in R but in its reﬂexive, symmetric and transitive closure, we can also skip
this pair. Ignoring the concrete data structure to store equivalence classes, the
HK algorithm consists in simply replacing Line 4 in Algorithm 1 with
4:
if (X, Y ) ̸∈rst(R) then
where rst is the function mapping each relation R ⊆P(Q) × P(Q) into its
reﬂexive, symmetric, and transitive closure. With this optimization, the num-
ber of pairs in R will be reduced. When the algorithm returns “yes”, R is no
longer a bisimulation, but is contained in a bisimulation [4] and one can infer a
bisimulation from R.
HKC Algorithm. Based on the simple observation that if L(X1) = L(Y1) and
L(X2) = L(Y2) then L(X1 ∪X2) = L(Y1 ∪Y2), Bonchi and Pous [4] improve
the HK algorithm with congruence closure. One gets the HKC algorithm just by
replacing Line 4 in Algorithm 1 with
4:
if (X, Y ) ̸∈rstu(R ∪todo) then
where rstu is the reﬂexive, symmetric, and transitive closure extended with the
following union of relations: u(R) is the smallest relation containing R satisfying:

On Equivalence Checking of Nondeterministic Finite Automata
221
if X1 R Y1 and X2 R Y2, then (X1 ∪X2) u(R) (Y1 ∪Y2). Note that Bonchi and
Pous use R ∪todo rather than R because this helps to skip more pairs, and this
is safe since all pairs in todo will eventually be processed [4].
When the HKC algorithm returns “yes”, R is also contained in a bisimula-
tion [4] and suﬃcient to infer one.
In order to check whether a pair is in the equivalence closure of a relation,
Hopcroft and Karp use disjoint sets forests to represent equivalence classes, which
allow to check (X, Y ) ̸∈rst(R) in almost constant amortised time. Unfortunately,
this data structure cannot help one to do the checking for congruence closure
rstu. Bonchi and Pous use a set rewriting approach to do this. However, this
requires to scan the pairs in R one by one, which makes it slow. As we shall
show in the experiments, this has a great impact on the performance of HKC.
3.3
Reducing todo
The pairs in todo are those to be processed. However, if this set is implemented as
a list or similar data structure, there are often redundancies. We can remember
that some element has already been inserted into todo earlier; this is called
memoisation. In Line 9, we check whether we have inserted the same pair into
todo earlier and only insert the pair if it never has been in todo. (Note that this
also skips pairs that have in the meantime moved from todo to R.) We can use
hash sets to check this condition in constant time.
3.4
BDD Representation
Pous [13] proposed a symbolic algorithm for checking language equivalence of
ﬁnite automata over large input alphabets. By processing internal nodes, the
symbolic algorithm may insert fewer pairs into todo, which makes us want to
know whether it can save time if we perform the symbolic algorithm instead of
the explicit one.
The symbolic version of the HK and HKC algorithms and of memoisation
can be easily constructed from the explicit ones. The only diﬀerence is that the
pairs of sets of states become pairs of BDD nodes, including leaf nodes and
internal nodes. If a pair of internal nodes is skipped, then all its successors are
also skipped. This is why the symbolic algorithm may have fewer pairs in todo.
3.5
Preprocessing Operations
Bonchi and Pous [4] extend the HKC algorithm to exploit the similarity preorder.
It suﬃces to notice that for any similarity pair x ≾y (in the NFA), we have
{x, y} ∼{y} (in the DFA). So to check whether (X, Y ) ∈rstu(R ∪todo), it
suﬃces to compute the congruence closure of X and Y w.r.t. the pairs from
R ∪todo ∪{({x, y}, {y})|x ≾y}. This may allow to skip more pairs. However,
the time required to compute similarity may be expensive.

222
C. Fu et al.
Since bisimilarity can be computed in less time than similarity, it may be
advantageous to replace similarity with bisimilarity. So we can replace the simi-
larity with bisimilarity to get another algorithm, i.e. computing the congruence
closure of X and Y w.r.t. the pairs from R ∪todo ∪{({x}, {y})|x ∼y}.
As a matter of fact, we can use this technique as a preprocessing operation.
For similarity, we saturate the original NFA w.r.t. the similarity preorder before
running the algorithms. For bisimilarity, we choose another approach, that is
taking a quotient according to bisimilarity. This amounts to saturating the NFA
w.r.t. bisimilarity, but it is more eﬃcient. Note that we do not take a quotient
according to simulation equivalence, because it is less powerful than saturating
the NFA w.r.t. similarity. Although taking a quotient according to bisimilarity
may be less powerful than saturating by similarity, using bisimilarity makes the
total time shorter in many cases, which is shown in Sect. 4.
3.6
Algorithms for Language Inclusion Checking
Instead of directly checking language equivalence for NFA, it is possible to
check two underlying language inclusions: for any pair (X, Y ), we check whether
L(X) ⊆L(Y ) and L(Y ) ⊆L(X). If both of them hold, we have L(X) = L(Y ).
So it is enough to solve the problem of checking L(X) ⊆L(Y ). The naive algo-
rithm is quite similar to the one for checking equivalence. Here, we call a pair
(x, Y ) a bad pair if x is accepting but Y not. The idea of the algorithm is still:
Whenever we pick a pair from todo, we check whether it is a bad pair; if not, we
insert the successor pairs into todo.
When checking (X, Y ), the algorithm eventually determinizes the part corre-
sponding to Y and remains nondeterministic for X, that is, it compares x ∈X
from A and Y from B♯. The sets todo and R will therefore be subsets of
QA ×P(QB). Again, the naive algorithm allows for several optimizations. Mem-
oisation can be used without modiﬁcations. The antichain algorithm proposed
in [6] aims to reduce the number of pairs in R, and it can be enhanced by
exploiting similarity [2].
Given a partial order (X, ⊑), an antichain is a subset Y ⊆X containing only
incomparable elements. The antichain algorithm exploits antichains over the set
QA × P(QB), equipped with the partial order (x1, Y1) ⊑(x2, Y2) iﬀx1 = x2 and
Y1 ⊆Y2.
In order to check L(X) ⊆L(Y ) for two sets of states X, Y , the antichain
algorithm ensures that R is an antichain of pairs (x′, Y ′). If one of these pairs
p is larger than a previously encountered pair p′ ∈R (i.e. p′ ⊑p) then the
language inclusion corresponding to p is subsumed by p′ so that p can be skipped.
Conversely, if there are some pairs p1, . . . , pn ∈R which are all larger than p
(i.e. p ⊑pi for all 1 ≤i ≤n), one can safely remove them: they are subsumed
by p and, by doing so, the set R remains an antichain. We denote the antichain
algorithm as “AC”.
Abdulla et al. [2] propose to accelerate the antichain algorithm by exploiting
similarity. The idea is that when processing a pair (x, Y ), if there is a previously

On Equivalence Checking of Nondeterministic Finite Automata
223
encountered pair (x′, Y ′) such that x ≾x′ and Y ′ ≾Y (which means ∀y′ ∈
Y ′, ∃y ∈Y : y′ ≾y), then (x, Y ) can be skipped because it is subsumed by
(x′, Y ′). For the same reason as in Sect. 3.5, we can also use bisimilarity instead
of similarity.
Here, we can still take a quotient according to bisimilarity before performing
the algorithms. However, we can not saturate the NFA like in Sect. 3.5 because
the algorithms need to maintain an antichain and if we saturate the NFA, there
would be lots of pairs which can be actually skipped. So we can only exploit
similarity while running the algorithms, which sometimes slows them down.
4
Experiments
In this section, we describe some experiments to compare the performance of
the algorithms mentioned above. We implemented all these algorithms in Java.
For the symbolic one, we use the JavaBDD library [20]. We compute bisimilarity
according to the algorithm in [18] and similarity according to [8], which, as far as
we know, are the two fastest algorithms to compute bisimilarity and similarity,
respectively. All the implementation details are available at https://github.com/
fuchen1991/EBEC.
We conducted the experiments on random automata and automata obtained
from model-checking problems. All the experiments were performed on a machine
with an Intel i7-2600 3.40 GHz CPU and 8 GB RAM.
Random automata: We generate diﬀerent random NFA by changing three
parameters: the number of states (|Q|), the number of letters (|Σ|), and the
density (d), which is the average out-degree of each state with respect to each
letter. Although Tabakov and Vardi [17] empirically showed that one statisti-
cally gets the most challenging NFA with d = 1.25, we ﬁnd that the algorithms
are quite sensitive to changes in density and the densities of many NFA from
model checking are much smaller than this value, so we test more values for
this parameter. For each setting, we generated 100 NFA. To make sure that
all the algorithms meet their worst cases, there are no accepting states in the
NFA (So we have to skip the operation of removing non-coaccessible states,
otherwise this reduces each NFA to the empty one). The two initial state sets
are two distinct singleton sets.
Automata from model checking: Bonchi and Pous [4] use the same auto-
mata as Abdulla, Chen, Hol´ık et al. [2], which come from the model check-
ing of various programs (the bakery algorithm, bubble sort, and a producer-
consumer system) and are available from L. Hol´ık’s website. We also use these
automata. The diﬀerence between our work and Bonchi and Pous’s is that
they only show the performance of HKC and AC after preprocessing with
similarity, while we compare more algorithms.
We record the running time of each algorithm on the above NFAs, mea-
sured as the average over four executions. We depict all the data by boxplots –
a method for graphically depicting groups of numerical data through their

224
C. Fu et al.
quartiles. In a boxplot, the box denotes the values between the lower quartile
and the upper quartile, and the horizontal line in the box denotes the median
value. Numbers which are outside 1.5 times the interquartile range above the
upper quartile or below the lower quartile are regarded as outliers and shown as
individual points. Finally, the two end points of the vertical line outside the box
denote the minimal and maximal values that are not considered to be outliers.
4.1
Memoisation Accelerates the Algorithms
In most situations, memoisation saves time because it reduces the number of
pairs in todo. But the eﬀects on the three algorithms are diﬀerent, that is, it
saves more time to optimize HKC with memoisation than HK and AC. For
HKC, repeated pairs are skipped because they are in the congruence closure of
R (Algorithm 1, Line 4). This check costs much more time than the corresponding
checks of HK and AC. Besides, memoisation needs less time than all the three
methods to remove repeated pairs. So as we can see in Fig. 1, the huge diﬀerence
of the number of pairs in todo leads to the huge diﬀerence of time for HKC, but
leads to small diﬀerence for HK and AC.
●●●●
●
●
●
●
●●●●●●●
●●●
●
●
●
●
●●●●●●
1e+01
1e+03
AC
HK
HKC
Time (ms)
(left) Plain
(right) Memoisation
●
●●●●
●
●●●●
●
●
●
●
●
●●
1e+03
1e+04
AC
HK
HKC
Number of pairs in todo
(left) Plain
(right) Memoisation
Fig. 1. Comparison of algorithms without and with memoisation (|Q| = 500, |Σ| =
50, d = 0.1). The y-axis is logarithmic.
When memoisation only reduces a few pairs in todo, it just costs a little more
time because memoisation requires only constant time. So it always pays oﬀto
use memoisation. In the following, we always use memoisation for HK, HKC,
and AC.
4.2
BDDs Are only Suitable for Highly Structured NFA
As discussed in Sect. 3.4, storing NFA with BDDs and performing symbolic algo-
rithms can reduce the pairs in todo. However, we ﬁnd that this variant slows down

On Equivalence Checking of Nondeterministic Finite Automata
225
the algorithms on random NFA for most settings, as shown in Fig. 2(a). This is
because it is hard to generate highly structured random NFA.
The size of the BDD highly depends on the structure of the automaton. If an
automaton has many symmetries, there are fewer nodes in the BDD; this makes
the symbolic algorithms run faster. Also, the performance of explicit algorithms
on automata over large alphabet is bad, while BDDs can represent large alpha-
bets with few variables, so symbolic algorithms are preferable for this kind of
automata.
In order to show this, we let |Q| = 4, |Σ| = 217 = 131072, and d = 4.0, which
may be impractical, but a clear example to exhibit the advantage of symbolic
algorithms. In this NFA, each state has a transition to all states on all input
symbols, so the algorithms only need to process two pairs, namely the pair of
initial state sets and the pair of sets containing all states. An explicit algorithm
needs to scan every symbol in Σ, while there is only one node in the BDD, which
makes symbolic algorithm faster. The result is shown in Fig. 2(b).
●
●
●
●
●
●
●
●
1e+01
1e+02
1e+03
HK
HKC
Time (ms)
(left) Explicit
(right) Symbolic
(a) |Q| = 500, |Σ| = 50, d = 0.1
●●
●
●●
●
●●●●
●
●●●●
●
●
●●●●●●
●
●
●●●●●●●
1e+01
1e+03
HK
HKC
Time (ms)
(left) Explicit
(right) Symbolic
(b) |Q| = 4, |Σ| = 131072, d = 4.0
Fig. 2. Comparison of explicit and symbolic algorithms. The time-axis is logarithmic.
4.3
Comparison of HK, HKC and AC
In addition to |Q|, |Σ| and d, there is another parameter that should be con-
sidered: the number of transitions, |δ| = |Q| · |Σ| · d. Here, we compare the
performance of HK, HKC and AC under diﬀerent settings of these parameters.
First, let us ﬁx |Σ| = 2 and d = 1.25 and vary the state space size |Q|.
The result is shown in Fig. 3(a). When increasing |Q| (and also |δ|), the time
required by all the three algorithms increases, but HK’s time increases much
stronger than the others’. HKC performs best, and AC has some bad outliers,
which can be more than 100 times slower than HKC.

226
C. Fu et al.
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1e−02
1e+00
1e+02
1e+04
50
150
450
1350
Number of states
Time (ms)
HK
HKC
AC
(a) |Σ| = 2, d = 1.25
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
1e−02
1e+00
1e+02
1e+04
2
40
800
Number of letters
Time (ms)
HK
HKC
AC
(b) |Q| = 150, d = 1.25
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
1e−02
1e+00
1e+02
1e+04
0.1
0.4
0.7
1
1.25
1.5
Density
Time (ms)
HK
HKC
AC
(c) |Q| = 300, |Σ| = 10
●
●
●
●
●
●
●●
●●
●
●●●●
●
●●
●
●
●
●●
●
●
●
●●●●●●
●
●●
●●
●●●
●
●●●●●●
●
●●●●●●●●●●
●
●
●●●●●●●
●
●●●●●●●
●
●●
●
●
●
●●●
1e−02
1e+00
1e+02
1e+04
(2,1.25)(3,0.833) (5,0.5) (10,0.25)
(Number of letters, density)
Time (ms)
HKC
AC
(d) |Q| = 1000, |δ| = 2500
●
●
●●
●●
●
●
●
●
●●
●
●
●●
●●●
●
●●
●
●●
●
●
●●
●
●
●●
●
●
●
●●●●
●
1e−02
1e+00
1e+02
1e+04
(1000,1.25)(1250,1.0)(2500,0.5)(5000,0.25)
(Number of states, density)
Time (ms)
HKC
AC
(e) |Σ| = 2, |δ| = 2500
●
●
●
●
●
●●●
●
●●
●
●●●●●●●●●●●●●●●
1e+00
1e+02
1e+04
(400,1.25) (500,1.0) (1000,0.5)(2000,0.25)
(Number of states, density)
Time (ms)
HKC
AC
(f) |Σ| = 50, |δ| = 25000
Fig. 3. Comparison of HK, HKC and AC. The time-axis is logarithmic.

On Equivalence Checking of Nondeterministic Finite Automata
227
Then, we ﬁx |Q| = 150 and d = 1.25 and vary the alphabet size |Σ|. The
result is shown in Fig. 3(b): Upon increasing |Σ| (and also |δ|), the time required
by all the three algorithms increases, and again HK’s time increases the most.
The average performance of AC is best overall; however, if |Σ| = 2, there are
some very slow outliers, so HKC may be preferable, as its performance is not
much worse than the average of AC.
Next, we ﬁx |Q| = 300 and |Σ| = 10 and vary the density d. The measure-
ments are shown in Fig. 3(c). Basically, the time ﬁrst increases then decreases
as the density grows. But the peak values appear at diﬀerent densities for the
three algorithms. The peak value of HKC appears to be near d = 1, but for
HK and AC at a much larger density. We do not expect very large densities in
practice; if the density is maximal (d = |Q|), then it can be found very quickly
that all states are language equivalent. Still, HK always performs worst. When
the density is between 0.1 and 1.25, AC performs 10 to 100 times faster than
HKC, but when the density is 1.5, HKC can be several times faster than AC.
Until now, we have seen that HK always performs worse than at least one
of the other two algorithms, and no one of HKC and AC always performs best.
In the following, we only compare the performance of HKC and AC to ﬁnd out
in which situation HKC performs better and in which situation AC performs
better.
Let us ﬁx |Q| = 500 and |δ| = 2500 and let |Σ| and d vary. The result is shown
in Fig. 3(d). The running time remains approximately the same as we ﬁx |Q| and
|δ|, but HKC performs better (does not have slow outliers) at |Σ| = 2, d = 1.25
and worse in other settings. Then we ﬁx |Σ| = 2 and |δ| = 2500. The result
is shown in Fig. 3(e). We can see that HKC is always worse except d = 1.25.
Finally, we ﬁx |Σ| = 50 and |δ| = 25000. The result is shown in Fig. 3(f). We can
see that HKC is always slower than AC when |Σ| = 50. The smaller the density
is, the larger diﬀerence between the performance of HKC and AC is.
In conclusion, HKC performs better when d is large and |Σ| is small. In this
setting, the maximal value of AC is always very large, which can be 10 to 100
times slower than HKC. Moreover, there are always some bad outliers for AC.
So HKC is a better choice in this setting. But If d is small or |Σ| is large, AC
can even be more than 100 times faster than HKC.
4.4
Automata from Model Checking
Now we compare the performance of HK, HKC, AC and all these three algo-
rithms with preprocessing with similarity and bisimilarity on the automata from
model checking. Bonchi and Pous state that HKC can mimic AC even for lan-
guage inclusion problem. Here, we also use these algorithms to check language
inclusion for the automata.
We perform all the three algorithms without any preprocessing (“Plain”),
with saturating w.r.t. similarity (“Sim”), and minimizing w.r.t. bisimilarity
(“Bisim”), respectively. We separate the results into those for which the inclu-
sion holds and those for which the inclusion does not hold. Figure 4 shows the
total running time of each algorithm. First, we ﬁnd that the densities of these

228
C. Fu et al.
automata are all between 0.05 and 0.49, and over half of them are smaller than
0.19. Their alphabet sizes are between 7 and 36, and over half of them are smaller
than 20. We also observe that the performance diﬀerence of the algorithms is
similar to Fig. 3(c) with d = 0.1. This is approximately in agreement with our
conclusion for random automata. Second, as we can see the total running time
of “Bisim” is much shorter than “Sim” because computing bisimilarity is often
much faster than similarity. Preprocessing with similarity may even be slower
than no preprocessing at all, but this does not happen for bisimilarity.
●●
●●
●●
●
●
●●
●
1e−01
1e+02
1e+05
Plain
Sim
Bisim
Time (ms)
HK
HKC
AC
(a) Inclusion holds
●
●●
●
●
●●
1e−01
1e+02
1e+05
Plain
Sim
Bisim
Time (ms)
HK
HKC
AC
(b) Inclusion does not hold
Fig. 4. Comparison of diﬀerent preprocessing operations. The time-axis is logarithmic.
4.5
Tools: EBEC, hknt, and VATA
We also conducted experiments with other tools. Abdulla et al. [2] implemented
their algorithm in their tool “VATA” [11] in C++, and Bonchi and Pous [4]
implemented the HKC algorithm in their tool “hknt” in OCaml.
We again run experiments on the same automata sets used in Sect. 4.4. The
result is shown in Fig. 5. In this ﬁgure, “EBEC” denotes our tool in which
we choose the antichain algorithm with memoisation and preprocessing with
bisimilarity, since it is the optimal combination according our previous experi-
ments. “hknt” denotes Bonchi and Pous’s tool running their HKC algorithm and
“VATA, VATA sim” the tool of Abdulla et al. running the basic antichain algo-
rithm and antichain algorithm with preprocessing with similarity respectively.
We choose 10 min as the timeout and ﬁnd that for a few tests, hknt and
VATA sim does not terminate in this time, while EBEC and VATA both ter-
minate. We also see that our tool performs about 10 times faster than hknt in
both situations. When the inclusion holds, EBEC is 2–3 times faster than VATA
and VATA sim. When the inclusion does not hold, our tool EBEC has the same

On Equivalence Checking of Nondeterministic Finite Automata
229
●
●
●
●●
●●●
●
●
1e+00
1e+02
1e+04
1e+06
EBEC
hknt
VATA
VATA_sim
Time (ms)
(a) Inclusion holds
1e+00
1e+02
1e+04
1e+06
EBEC
hknt
VATA
VATA_sim
Time (ms)
(b) Inclusion does not hold
Fig. 5. Comparison of the tools. The time-axis is logarithmic.
performance as VATA on most automata and is up to 2.5 times slower on some
automata. However, one should note that if the inclusion does not hold, the
performance highly depends on the order of visiting the states.
5
Related Work
To our knowledge, VATA implemented by Abdulla et al. [2] is the most eﬃcient
implementation currently available for the antichain algorithms. They compare
their algorithm with the naive algorithm and the basic antichain algorithm [6],
but not with other algorithms.
Bonchi and Pous propose that HKC can be optimized by similarity [4], but
their implementation of the algorithm to compute similarity is slow, because it
is based on the algorithm proposed by Henzinger et al. [7], which is no longer
the fastest known one. Bonchi and Pous compare HKC with HK and AC only on
random automata, and they also compare HKC and AC after preprocessing with
similarity. However, they do not show the total time of these two algorithms.
In our work, we ﬁnd that preprocessing with similarity is hardly beneﬁcial, it
even makes the total time longer sometimes. So we propose preprocessing with
bisimilarity, and the experiments show that it is preferable.
Pous [13] proposes a symbolic algorithm for checking language equivalence of
ﬁnite automata over large alphabets and applies it to Kleene algebra with tests.
We have implemented the symbolic version of HK and HKC, and show that the
symbolic algorithm is suitable for highly structured automata, especially those
over large alphabets.

230
C. Fu et al.
6
Conclusion
We have reviewed various algorithms and optimization techniques for checking
language equivalence of NFA, and compared their performance by experiments.
We ﬁnd that their practical eﬃciencies depend very much on the automata
under consideration. But according to the automata to be checked, one can
choose the appropriate algorithm: If the density is large and the alphabet size
is small, then one should choose HKC (Hopcroft and Karp’s algorithm up to
congruence) [4], otherwise the antichain algorithm (Wulf, Doyen, Henzinger,
Raskin) [6]. Moreover, one should always use memoisation and minimize the
automata by bisimilarity before performing the algorithm. One may choose to
use a symbolic algorithm when working with highly structured automata over a
large alphabet. Finally, we compared the performance of our tool “EBEC” with
“VATA” and “hknt” and showed that EBEC performs better on most automata
we tested.
Acknowledgements. This work has been supported by the National Natural Sci-
ence Foundation of China (Grants 61532019, 61472473, 61672229), the CAS/SAFEA
International Partnership Program for Creative Research Teams, the Sino-German
CDZ project CAP (GZ 1023) and Shanghai Municipal Natural Science Foundation
(16ZR1409100).
References
1. Abdulla, P.A., Bouajjani, A., Hol´ık, L., Kaati, L., Vojnar, T.: Computing Sim-
ulations over Tree Automata. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS
2008. LNCS, vol. 4963, pp. 93–108. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-78800-3 8
2. Abdulla, P.A., Chen, Y.-F., Hol´ık, L., Mayr, R., Vojnar, T.: When simulation meets
antichains. In: Esparza, J., Majumdar, R. (eds.) TACAS 2010. LNCS, vol. 6015,
pp. 158–174. Springer, Heidelberg (2010). doi:10.1007/978-3-642-12002-2 14
3. Baier, C., Katoen, J.-P.: Principles of Model Checking. MIT Press, Cambridge
(2008)
4. Bonchi, F., Pous, D.: Checking NFA equivalence with bisimulations up to congru-
ence. In: Principles of Programming Languages (POPL 2013), pp. 457–468. ACM,
New York (2013)
5. Bryant, R.E.: Graph-based algorithms for boolean function manipulation. IEEE
Trans. Comput. 35(8), 677–691 (1986). https://doi.org/10.1109/TC.1986.1676819
6. De Wulf, M., Doyen, L., Henzinger, T.A., Raskin, J.-F.: Antichains: a new algo-
rithm for checking universality of ﬁnite automata. In: Ball, T., Jones, R.B. (eds.)
CAV 2006. LNCS, vol. 4144, pp. 17–30. Springer, Heidelberg (2006). doi:10.1007/
11817963 5
7. Henzinger, M.R., Henzinger, T.A., Kopke, P.W.: Computing simulations on ﬁnite
and inﬁnite graphs. In: Symposium on Foundations of Computer Science (FOCS),
Milwaukee, pp. 453–462. IEEE Computer Society (1995)
8. Hol´ık, L., ˇSim´aˇcek, J.: Optimizing an LTS-simulation algorithm. Comput. Inform.
29(6+), 1337–1348 (2010). http://www.cai.sk/ojs/index.php/cai/article/view/147

On Equivalence Checking of Nondeterministic Finite Automata
231
9. Hopcroft, J.E., Karp, R.M.: A linear algorithm for testing equivalence of ﬁnite
automata. Technical report, 71–114, Cornell University, Ithaca (1971). http://hdl.
handle.net/1813/5958
10. Hopcroft, J.: An n log n algorithm for minimizing states in a ﬁnite automaton. In:
Kohavi, Z., Paz, A. (eds.) Theory of Machines and Computations, pp. 189–196.
Academic Press, New York (1971)
11. Leng´al, O., ˇSim´aˇcek, J., Vojnar, T.: VATA: a library for eﬃcient manipulation
of non-deterministic tree automata. In: Flanagan, C., K¨onig, B. (eds.) TACAS
2012. LNCS, vol. 7214, pp. 79–94. Springer, Heidelberg (2012). doi:10.1007/
978-3-642-28756-5 7
12. Paige, R., Tarjan, R.E.: Three partition reﬁnement algorithms. SIAM J. Comput.
16(6), 973–989 (1987)
13. Pous, D.: Symbolic algorithms for language equivalence and Kleene algebra with
tests. In: Principles of Programming Languages (POPL 2015), pp. 357–368. ACM,
New York (2015)
14. Ranzato, F., Tapparo, F.: A new eﬃcient simulation equivalence algorithm. In:
Symposium on Logic in Computer Science (LICS), pp. 171–180. IEEE Computer
Society (2007)
15. Rutten, J.J.M.M.: Automata and coinduction (an exercise in coalgebra). In: San-
giorgi, D., de Simone, R. (eds.) CONCUR 1998. LNCS, vol. 1466, pp. 194–218.
Springer, Heidelberg (1998). doi:10.1007/BFb0055624
16. Stockmeyer, L.J., Meyer, A.R.: Word problems requiring exponential time. In:
Proceedings of the 5th Annual ACM Symposium on Theory of Computing (STOC),
pp. 1–9 (1973)
17. Tabakov, D., Vardi, M.Y.: Experimental evaluation of classical automata construc-
tions. In: Sutcliﬀe, G., Voronkov, A. (eds.) LPAR 2005. LNCS, vol. 3835, pp. 396–
411. Springer, Heidelberg (2005). doi:10.1007/11591191 28
18. Valmari, A.: Bisimilarity minimization in O(m log n) time. In: Franceschinis,
G., Wolf, K. (eds.) PETRI NETS 2009. LNCS, vol. 5606, pp. 123–142. Springer,
Heidelberg (2009). doi:10.1007/978-3-642-02424-5 9
19. Veanes, M.: Applications of symbolic ﬁnite automata. In: Konstantinidis, S. (ed.)
CIAA 2013. LNCS, vol. 7982, pp. 16–23. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-39274-0 3
20. Whaley, J.: JavaBDD. http://javabdd.sourceforge.net/. Accessed 13 June 2017

A New Decomposition Method for Attractor
Detection in Large Synchronous Boolean
Networks
Andrzej Mizera1, Jun Pang1,2, Hongyang Qu3, and Qixia Yuan1(B)
1 Faculty of Science, Technology and Communication, University of Luxembourg,
Luxembourg, Luxembourg
{andrzej.mizera,jun.pang,qixia.yuan}@uni.lu
2 Interdisciplinary Centre for Security, Reliability and Trust,
University of Luxembourg, Luxembourg, Luxembourg
3 Department of Automatic Control and Systems Engineering,
University of Sheﬃeld, Sheﬃeld, UK
h.qu@sheffield.ac.uk
Abstract. Boolean networks is a well-established formalism for mod-
elling biological systems. An important challenge for analysing a Boolean
network is to identify all its attractors. This becomes challenging for large
Boolean networks due to the well-known state-space explosion problem.
In this paper, we propose a new SCC-based decomposition method for
attractor detection in large synchronous Boolean networks. Experimen-
tal results show that our proposed method is signiﬁcantly better in terms
of performance when compared to existing methods in the literature.
1
Introduction
Boolean networks (BN) is a well-established framework widely used for modelling
biological systems, such as gene regulatory networks (GRNs). Although it is
simple by mainly focusing on the wiring of a system, BN can still capture the
important dynamic property of the modelled system, e.g., the attractors. In the
literature, attractors are hypothesised to characterise cellular phenotypes [1] or
to correspond to functional cellular states such as proliferation, apoptosis, or
diﬀerentiation [2]. Hence, attractor identiﬁcation is of vital importance to the
study of biological systems modelled as BNs.
Attractors are deﬁned based on the BN’s state space (often represented as
a transition system or graph), the size of which is exponential in the number of
nodes in the network. Therefore, attractor detection becomes non-trivial when
it comes to a large network. In the BN framework, algorithms for detecting
attractors have been extensively studied in the literature. The ﬁrst study dates
back to the early 2000s when Somogyi and Greller proposed an enumeration and
simulation method [3]. The idea is to enumerate all the possible states and to
run simulation from each of them until an attractor is found. This method is
Q. Yuan—Supported by the National Research Fund, Luxembourg (grant 7814267).
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 232–249, 2017.
https://doi.org/10.1007/978-3-319-69483-2_14

A New Decomposition Method for Attractor Detection
233
largely limited by the network size as its running time grows exponentially with
the number of nodes in the BN. Later on, the performance of attractor detection
has been greatly improved with the use of two techniques. The ﬁrst technique is
binary decision diagrams (BDDs). This type of methods [4,5] encodes Boolean
functions of BNs with BDDs and represents the network’s corresponding transi-
tion system with BDD structures. Using the BDD operations, the forward and
backward reachable states can be often eﬃciently computed. Detecting attractors
is then reduced to ﬁnding ﬁx point sets of states in the corresponding transition
system. The other technique makes use of satisﬁability (SAT) solvers. It trans-
forms attractor detection in BNs into a SAT problem [6]. An unfolding of the
transition relation of the BN for a bounded number of steps is represented as
a propositional formula. The formula is then solved by a SAT solver to identify
a valid path in the state transition system of the BN. The process is repeated
iteratively for larger and larger bounded numbers of steps until all attractors
are identiﬁed. The eﬃciency of the algorithm largely relies on the number of
unfolding steps required and the number of nodes in the BN. Recently, a few
decomposition methods [7–9] were proposed to deal with large BNs. The main
idea is to decompose a large BN into small components based on its structure,
to detect attractors of the small components, and then recover the attractors of
the original BN.
In this paper, we propose a new decomposition method for attractor detection
in BNs, in particular, in large synchronous BNs, where all the nodes are updated
synchronously at each time point. Considering the fact that a few decomposition
methods have already been introduced, we explain our new method by showing
its main diﬀerences from the existing ones. First, our method carefully considers
the semantics of synchronous BNs and thus it does not encounter a problem
that the method proposed in [7] does. We explain this in more details in Sect. 3.
Second, our new method considers the dependency relation among diﬀerent sub-
networks when detecting attractors of them while our previous method [8] does
not require this. We show with experimental results that this consideration can
signiﬁcantly improve the performance of attractor detection in large BNs. Fur-
ther, the decomposition method in [9] is designed for asynchronous networks
while here we extend it for synchronous networks. As a consequence, the key
operation realisation for the synchronous BNs is completely re-designed with
respect to the one for asynchronous BNs in [9].
2
Preliminaries
2.1
Boolean Networks
A Boolean network (BN) is composed of two elements: binary-valued nodes,
which represents elements of a biological system, and Boolean functions, which
represent interactions between the elements. The concept of BNs was ﬁrst
introduced in 1969 by S. Kauﬀman for analysing the dynamical properties of
GRNs [10], where each gene was assumed to be in only one of two possible
states: ON/OFF.

234
A. Mizera et al.
Deﬁnition 1 (Boolean network). A Boolean network G(V, f) consists of
a set of nodes V = {v1, v2, . . . , vn}, also referred to as genes, and a vector of
Boolean functions f = (f1, f2, . . . , fn), where fi is a predictor function associ-
ated with node vi (i = 1, 2, . . . , n). A state of the network is given by a vector
x = (x1, x2, . . . , xn) ∈{0, 1}n, where xi ∈{0, 1} is a value assigned to node vi.
Since the nodes are binary, the state space of a BN is exponential in
the number of nodes. Each node vi ∈V has an associated subset of nodes
{vi1, vi2, . . . , vik(i)}, referred to as the set of parent nodes of vi, where k(i) is
the number of parent nodes and 1 ≤i1 < i2 < · · · < ik(i) ≤n. Starting from
an initial state, the BN evolves in time by transiting from one state to another.
The state of the network at a discrete time point t (t = 0, 1, 2, . . .) is given by
a vector x(t) = (x1(t), x2(t), . . . , xn(t)), where xi(t) is a binary-valued variable
that determines the value of node vi at time point t. The value of node vi at
time point t + 1 is given by the predictor function fi applied to the values of the
parent nodes of vi at time t, i.e., xi(t + 1) = fi(xi1(t), xi2(t), . . . , xik(i)(t)). For
simplicity, with slight abuse of notation, we use fi(xi1, xi2, . . . , xik(i)) to denote
the value of node vi at the next time step. For any j ∈[1, k(i)], node vij is called
a parent node of vi and vi is called a child node of vij.
In general, the Boolean predictor functions can be formed by combinations
of any logical operators, e.g., logical and ∧, or ∨, and negation ¬, applied
to variables associated with the respective parent nodes. The BNs are divided
into two types based on the time evolution of their states, i.e., synchronous
and asynchronous. In synchronous BNs, values of all the variables are updated
simultaneously; while in asynchronous BNs, one variable at a time is randomly
selected for update.
In this paper, we focus on synchronous BNs. The transition relation of a syn-
chronous BN is given by
T (x(t), x(t + 1))
=
n

i=1

xi(t + 1) ↔fi(xi1(t), xi2(t), · · · , xiki(t))

.
(1)
It states that in every step, all the nodes are updated synchronously according
to their Boolean functions.
In many cases, a BN G(V, f) is studied as a state transition system. Formally,
the deﬁnition of state transition system is given as follows.
Deﬁnition 2 (State transition system). A state transition system T is a 3-
tuple ⟨S, S0, T⟩where S is a ﬁnite set of states, S0 ⊆S is the initial set of states
and T ⊆S × S is the transition relation. When S = S0, we write ⟨S, T⟩.
A BN can be easily modelled as a state transition system: the set S is just
the state space of the BN, so there are 2n states for a BN with n nodes; the
initial set of states S0 is the same as S; ﬁnally, the transition relation T is given
by Eq. 1.

A New Decomposition Method for Attractor Detection
235
Example 1. A BN with 3 nodes is shown in Fig. 1a. Its Boolean functions are
given as: f1 = ¬(x1 ∧x2), f2 = x1 ∧¬x2, and f3 = ¬x2. In Fig. 1a, the three
circles v1, v2 and v3 represent the three nodes of the BN. The edges between
nodes represent the interactions between nodes. Applying the transition relation
to each of the states, we can get the corresponding state transition system. For
better understanding, we demonstrate the state transition system as a state
transition graph in this paper. The corresponding state transition graph of this
example is shown in Fig. 1b.
v3
v2
v1
(a) A BN with 3 nodes.
000
101
001
011
110
111
100
010
(b) Transition graph of the BN in Example 1.
Fig. 1. The Boolean network in Example 1 and its state transition graph.
In the transition graph of Fig. 1b, the three states (000), (1 ∗1)1 can be
reached from each other but no other state can be reached from any of them.
This forms an attractor of the BN. The formal deﬁnition of an attractor is given
as follows.
Deﬁnition 3 (Attractor of a BN). An attractor of a BN is a set of states
satisfying that any state in this set can be reached from any other state in this
set and no state in this set can reach any other state that is not in this set.
When analysing an attractor, we often need to identify transition relations
between the attractor states. We call an attractor together with its state transi-
tion relation an attractor system (AS). The states constituting an attractor are
called attractor states. The attractors of a BN characterise its long-run behav-
iour [11] and are of particular interest due to their biological interpretation.
For synchronous BNs, each state of the network can only have at most one
outgoing transition. Therefore, the transition graph of an attractor in a synchro-
nous BN is simply a loop. By detecting all the loops in a synchronous BN, one
can identify all its attractors.
Example 2. The BN given in Example 1 has one attractor, i.e., {(000), (1 ∗1)}.
1 We use ∗to denote that the bit can have value either 1 or 0, so (1 ∗1) actually
denotes two states: 101 and 111.

236
A. Mizera et al.
2.2
Encoding BNs in BDDs
Binary decision diagrams (BDDs) were introduced by Lee in [12] and Akers
in [13] to represent Boolean functions [12,13]. BDDs have the advantage of
memory eﬃciency and have been applied in many model checking algorithms
to alleviate the state space explosion problem. A BN G(V, f) can be modelled
as a state transition system, which can then be encoded in a BDD.
Each variable in V can be represented by a binary BDD variable. By slight
abuse of notation, we also use V to denote the set of BDD variables. In order to
encode the transition relation, another set V ′ of BDD variables, which is a copy
of V , is introduced: V encodes the possible current states, i.e., x(t), and V ′
encodes the possible next states, i.e., x(t+1). Hence, the transition relation can
be viewed as a Boolean function T : 2|V |+|V ′| →{0, 1}, where values 1 and 0
indicate a valid and an invalid transition, respectively. Our attractor detection
algorithm, which will be discussed in the next section, also utilizes two basis
functions: Image(X, T) = {s′ ∈S | ∃s ∈X such that (s, s′) ∈T}, which returns
the set of target states that can be reached from any state in X ⊆S with a sin-
gle transition in T; Preimage(X, T) = {s′ ∈S | ∃s ∈X such that (s′, s) ∈T},
which returns the set of predecessor states that can reach a state in X with a
single transition. To simplify the presentation, we deﬁne Preimagei(X, T) =
Preimage(...(Preimage(X, T)))



i
with Preimage0(X, T) = X. In this way, the
set of all states that can reach a state in X via transitions in T is deﬁned
as an iterative procedure Predecessors(X, T) =
n	
i=0
Preimagen(X, T) such that
Preimagen(X, T) = Preimagen+1(X, T). Given a set of states X ⊆S, the pro-
jection T|X of T on X is deﬁned as T|X = {(s, s′) ∈T | s ∈X ∧s′ ∈X}.
3
The New Method
In this section, we describe in details the new SCC-based decomposition method
for detecting attractors of large synchronous BNs and we prove its correctness.
The method consists of three steps. First, we divide a BN into sub-networks
called blocks and this step is performed on the network structure, instead of the
state transition system of the network. Second, we detect attractors in blocks.
Last, we recover attractors of the original BN based on attractors of the blocks.
3.1
Decompose a BN
We start by giving the formal deﬁnition of a block.
Deﬁnition 4 (Block). Given a BN G(V, f) with V = {v1, v2, . . . , vn} and f =
{f1, f2, . . . , fn}, a block B(V B, f B) is a subset of the network, where V B ⊆V .
For any node vi ∈V B, if B contains all the parent nodes of vi, its Boolean
function in B remains the same as in G, i.e., fi; otherwise, the Boolean function
is undetermined, meaning that additional information is required to determine

A New Decomposition Method for Attractor Detection
237
the value of vi in B. We call the nodes with undetermined Boolean functions as
undetermined nodes. We refer to a block as an elementary block if it contains
no undetermined nodes.
We consider synchronous networks in this paper and therefore a block is also
under the synchronous updating scheme, i.e., all the nodes in the block will be
updated at each given time point no matter this node is undetermined or not.
We now introduce a method to construct blocks, using SCC-based decom-
position. Formally, the standard graph-theoretical deﬁnition of an SCC is as
follows.
Deﬁnition 5 (SCC). Let G be a directed graph and V be its vertices. A strongly
connected component (SCC) of G is a maximal set of vertices C ⊆V such that
for every pair of vertices u and v, there is a directed path from u to v and vice
versa.
We ﬁrst decompose a given BN into SCCs. Figure 2a shows the decomposition
of a BN into four SCCs: Σ1, Σ2, Σ3, and Σ4. A node outside an SCC that is
a parent to a node in the SCC is referred to as a control node of this SCC. In
Fig. 2a, node v1 is a control node of Σ2 and Σ4; node v2 is a control node of Σ3;
and node v6 is a control node of Σ4. The SCC Σ1 does not have any control node.
An SCC together with its control nodes forms a block. For example, in Fig. 2a,
Σ2 and its control node v1 form one block B2. Σ1 itself is a block, denoted
as B1, since the SCC it contains does not have any control node. If a control
node in a block Bi is a determined node in another block Bj, block Bj is called
a parent of block Bi and Bi is a child of Bj. By adding directed edges from all
parent blocks to all their child blocks, we form a directed acyclic graph (DAG)
of the blocks as the blocks are formed from SCCs. As long as the block graph
is guaranteed to be a DAG, other strategies to form blocks can be used. Two
blocks can be merged into one larger block. For example, blocks B1 and B2 can
be merged together to form a larger block B1,2.
A state of a block is a binary vector of length equal to the size of the block
which determines the values of all the nodes in the block. In this paper, we use
a number of operations on the states of a BN and its blocks. Their deﬁnitions
are given below.
v1
v2
v3
v4
v5
v6
v7
v8
Σ1
Σ3
Σ2
Σ4
(a) SCC decomposition.
00
01
11
10
(b) Transition graph of block B1.
Fig. 2. SCC decomposition and the transition graph of block B1.

238
A. Mizera et al.
Deﬁnition 6 (Projection map, Compressed state, Mirror states). For
a BN G and its block B, where the set of nodes in B is V B = {v1, v2, . . . , vm}
and the set of nodes in G is V = {v1, v2, . . . , vm, vm+1, . . . , vn}, the projection
map πB : X →XB is given by x = (x1, x2, . . . , xm, xm+1, . . . , xn) →πB(x) =
(x1, x2, . . . , xm). For any set of states S ⊆X, we deﬁne πB(S) = {πB(x) : x ∈
S}. The projected state πB(x) is called a compressed state of x. For any state
xB ∈XB, we deﬁne its set of mirror states in G as MG(xB) = {x | πB(x) =
xB}. For any set of states SB ⊆XB, its set of mirror states is MG(SB) =
{x | πB(x) ∈SB}.
The concept of projection map can be naturally extended to blocks. Given a
block with nodes V B = {v1, v2, . . . , vm}, let V B′ = {v1, v2, . . . , vj} ⊆V B. We
can deﬁne πB′ : XB →XB′ as x = (x1, x2, . . . , xm) →πB′(x) = (x1, x2, . . . , xj),
and for a set of states SB ⊆XB, we deﬁne πB′(SB) = {πB′(x) : x ∈SB}.
3.2
Detect Attractors in a Block
An elementary block does not depend on any other block while a non-elementary
block does. Therefore, they can be treated separately. We ﬁrst consider the case of
elementary blocks. An elementary block is in fact a BN; therefore, the deﬁnition
of attractors in a BN can be directly taken to the concept of an elementary
block.
Deﬁnition 7 (Preservation of attractors). Given a BN G and an elemen-
tary block B in G, let A = {A1, A2, . . . , Am} be the set of attractors of G and
AB = {AB
1 , AB
2 , . . . , AB
m′} be the set of attractors of B. We say that B preserves
the attractors of G if for any k ∈[1, m], there is an attractor AB
k′ ∈AB such
that πB(Ak) ⊆AB
k′.
Example 3. Consider the BN G1 in Example 1. Its set of attractors is A =
{{(000), (1 ∗1)}}. Nodes v1 and v2 form an elementary block B1. Since B1
is an elementary block, it can be viewed as a BN. The transition graph of B1
is shown in Fig. 3a. Its set of attractors is AB1 = {{(00), (1∗)}} (nodes are
arranged as v1, v2). We have πB1({(000), (1 ∗1)}) = {(00), (1∗)} ∈AB1, i.e.,
block B1 preserves the attractors of G1.
With Deﬁnition 7, we have the following lemma and theorem.
01
10
00
11
(a) Transition graph of Block B1 in G1.
00
01
10
11
(b) Transition graph of the “realisation”.
Fig. 3. Two transition graphs used in Examples 3 and 4.

A New Decomposition Method for Attractor Detection
239
Lemma 1. Given a BN G and an elementary block B in G, let Φ be the set of
attractor states of G and ΦB be the set of attractor states of B. If B preserves
the attractors of G, then Φ ⊆MG(ΦB).
Theorem 1. Given a BN G, let B be an elementary block in G. B preserves
the attractors of G.
For an elementary block B, the mirror states of its attractor states cover
all G’s attractor states according to Lemma 1 and Theorem 1. Therefore, by
searching from the mirror states only instead of the whole state space, we can
detect all the attractor states.
We now consider the case of non-elementary blocks.
Deﬁnition 8 (Parent SCC, Ancestor SCC). An SCC Σi is called a parent
SCC (or parent for short) of another SCC Σj if Σi contains at least one control
node of Σj. Denote P(Σi) the set of parent SCCs of Σi. An SCC Σk is called
an ancestor SCC (or ancestor for short) of an SCC Σj if and only if either
(1) Σk is a parent of Σj or (2) Σk is a parent of Σk′, where Σk′ is an ancestor
of Σj. Denote Ω(Σj) the set of ancestor SCCs of Σi.
For an SCC Σj, if it has no parent SCC, then this SCC can form an elemen-
tary block; if it has at least one parent, then it must have an ancestor that has
no parent, and all its ancestors Ω(Σj) together can form an elementary block,
which is also a BN. The SCC-based decomposition will usually result in one or
more non-elementary blocks.
Deﬁnition 9 (Crossability, Cross operations).
Let G be a BN and
let Bi be a non-elementary block in G with the set of nodes V Bi
=
{vp1, vp2, . . . , vps, vq1, vq2, . . . , vqt}, where qk (k ∈[1, t]) are the indices of the
control nodes also contained by Bi’s parent block Bj and pk (k ∈[1, s]) are
the indices of the remaining nodes. We denote the set of nodes in Bj as
V Bj = {vq1, vq2, . . . , vqt, vr1, vr2, . . . , vru}, where rk (k ∈[1, u]) are the indices
of the non-control nodes in Bj. Let further xBi = (x1, x2, . . . , xs, yi
1, yi
2, . . . , yi
t)
be a state of Bi and xBj = (yj
1, yj
2, . . . , yj
t , z1, z2, . . . , zu) be a state of Bj. States
xBi and xBj are said to be crossable, denoted as xBi C xBj, if the values of
their common nodes are the same, i.e., yi
k = yj
k for all k ∈[1, t]. The cross
operation of two crossable states xBi and xBj is deﬁned as Π(xBi, xBj) =
(x1, x2, . . . , xs, yi
1, yi
2, . . . , yi
t, z1, z2, . . . , zu). The notion of crossability naturally
extends to two blocks without common nodes: any two states of these blocks are
crossable.
We say SBi ⊆XBi and SBj ⊆XBj are crossable, denoted as SBi C SBj,
if at least one of the sets is empty or the following two conditions hold: (1) for
any state xBi ∈SBi, there exists a state xBj ∈SBj such that xBi and xBj
are crossable; (2) vice versa. The cross operation on two crossable non-empty
sets of states SBi and SBj is deﬁned as Π(SBi, SBj) = {Π(xBi, xBj) | xBi ∈
SBi, xBj ∈SBj and xBi C xBj}. When one of the two sets is empty, the cross
operation simply returns the other set, i.e., Π(SBi, SBj) = SBi if SBj = ∅and
Π(SBi, SBj) = SBj if SBi = ∅.

240
A. Mizera et al.
Let SBi = {SBi | SBi ⊆XBi} be a family of sets of states in Bi and
SBj = {SBj | SBj ⊆XBj} be a family of sets of states in Bj. We say SBi
and SBj are crossable, denoted as SBi C SBj if (1) for any set SBi ∈SBi, there
exists a set SBj ∈SBj such that SBi and SBj are crossable; (2) vice versa.
The cross operation on two crossable families of sets SBi and SBj is deﬁned as
Π(SBi, SBj) = {Π(Si, Sj) | Si ∈SBi, Sj ∈SBj and Si C Sj}.
Proposition 1. Let V C be the set of control nodes shared by two blocks Bi and
Bj, i.e., V C = V Bi ∩V Bj and let SBi ⊆XBi and SBj ⊆XBj. Then SBi C SBj
is equivalent to πC(SBi) = πC(SBj).
After decomposing a BN into SCCs, there is at least one SCC with no con-
trol nodes. Hence, there is at least one elementary block in every BN. Moreover,
for each non-elementary block we can construct, by merging all its predecessor
blocks, a single parent elementary block. We detect the attractors of the elemen-
tary blocks and use the detected attractors to guide the values of the control
nodes of their child blocks. The guidance is achieved by considering realisations
of the dynamics of a non-elementary block with respect to the attractors of its
parent elementary block, shortly referred to as realisations of a non-elementary
block. In some cases, a realisation of a non-elementary block is simply obtained
by assigning new Boolean functions to the control nodes of the block. How-
ever, in many cases, it is not this simple and a realisation of a non-elementary
block is obtained by explicitly constructing a transition system of this block
corresponding to the considered attractor of the elementary parent block. Since
the parent block of a non-elementary block may have more than one attractor,
a non-elementary block may have more than one realisation.
Deﬁnition 10 (Realisation of a non-elementary block). Let Bi be a non-
elementary block formed by merging an SCC with its control nodes. Let nodes
u1, u2, . . . , ur be all the control nodes of Bi which are also contained in its
elementary parent block Bj (we can merge Bi’s ancestor blocks to form Bj if
Bi has more than one parent block or has a non-elementary parent block). Let
ABj
1 , ABj
2 , . . . , ABj
t
be the attractor systems of Bj. For any k ∈[1, t], a realisation
of block Bi with respect to ABj
k
is a state transition system such that
1. the transitions are as follows: for any transition xBj →˜xBj in the attractor
system of ABj
k , there is a transition xBi,j →˜xBi,j in the realisation such that
xBi,j C xBj and ˜xBi,j C ˜xBj; each transition in the realisation is caused by
the update of all nodes synchronously: the update of non-control nodes of Bi is
regulated by the Boolean functions of the nodes and the update of nodes in its
parent block Bj is regulated by the transitions of the attractor system of ABj
k ;
In the realisation of a non-elementary block all the nodes of its single elemen-
tary parent block are considered and not only the control nodes of the parent
block. This allows to distinguish the potentially diﬀerent states in which the val-
ues of control nodes are the same. Without this, a state in the state transition
graph of the realisation may have more than one out-going transition, which

A New Decomposition Method for Attractor Detection
241
is contrary to the fact that the out-going transition for a state in a synchro-
nous network is always determined. Although the deﬁnition of attractors can
still be applied to such a transition graph, the attractor detection algorithms
for synchronous networks, e.g., SAT-based algorithms, may not work any more.
Moreover, the meaning of attractors in such a graph are not consistent with
the synchronous semantics and therefore the detected “attractors” may not be
attractors of the synchronous BN. Note that the decomposition method men-
tioned in [7] did not take care of this and therefore produces incorrect results in
certain cases. We now give an example to illustrate one of such cases.
Example 4. Consider the BN in Example 1, which can be divided into two blocks:
block B1 with nodes v1, v2 and block B2 with nodes v2, v3. The transition graph
of B1 is shown in Fig. 3a and its attractor is (00) →(10) →(11). If we do not
include the node v1 when forming the realisation of B2, we will get a transi-
tion graph as shown in Fig. 3b, which contains two states with two out-going
transitions. This is contrary to the synchronous semantics. Moreover, recovering
attractors with the attractors in this graph will lead to a non-attractor state of
the original BN, i.e., (001).
For asynchronous networks, however, such a distinction is not necessary since the
situation of multiple out-going transitions is in consistent with the asynchronous
updating semantics. Deﬁnition 10 forms the basis for a key diﬀerence between
this decomposition method for synchronous BNS and the one for asynchronous
BNs proposed in [9].
Constructing realisations for a non-elementary block is a key process for
obtaining its attractors. For each realisation, the construction process requires
the knowledge of all the transitions in the corresponding attractor of its elemen-
tary parent block. In Sect. 4, we explain in details how to implement it with
BDDs. We now give some examples.
Example 5. Consider the BN in Fig. 2a. Its Boolean functions are given as
follows:

f1 = x1 ∧x2,
f2 = x1 ∨¬x2, f3 = ¬x4 ∧x3,
f4 = x1 ∨x3,
f5 = x2 ∧x6,
f6 = x5 ∧x6,
f7 = (x1 ∨x6) ∧x8, f8 = x7 ∨x8.
(2)
The network contains four SCCs Σ1, Σ2, Σ3 and Σ4. For any Σi (i ∈[1, 4]), we
form a block Bi by merging Σi with its control nodes. Block B1 is an elementary
block and its transition graph is shown in Fig. 2b. Block B1 has two attractors,
i.e., {(0∗)} and {(11)}. Regarding the ﬁrst attractor, block B3 has a realisation
by setting the nodes v1 and v2 (nodes from its parent block B1) to contain
transitions {(00) →(01), (01) →(00)}. The transition graph of this realisation
is shown in Fig. 4a. Regarding the second attractor, block B3 has a realisation
by setting nodes v1 and v2 to contain only the transition {(11) →(11)}. Its
transition graph is shown in Fig. 4b.

242
A. Mizera et al.
0000
0100
0110
0001
0010
0111
0101
0011
(a) Realisation 1 of Example 5.
1100
1110
1111
1101
(b) Realisation 2 of Example 5.
Fig. 4. Transition graphs of two realisations in Example 5.
A realisation of a non-elementary block takes care of the dynamics of the
undetermined nodes, providing a transition system of the block. Therefore, we
can extend the attractor deﬁnition to realisations of non-elementary blocks as
follows.
Deﬁnition 11 (Attractors of a non-elementary block).
An attractor of
a realisation of a non-elementary block is a set of states satisfying that any state
in this set can be reached from any other state in this set and no state in this
set can reach any other state that is not in this set. The attractors of a non-
elementary block is the set of the attractors of all realisations of the block.
With the above deﬁnition, we can extend Deﬁnition 10 by allowing Bj to
be a non-elementary block as well. As long as Bi’s parent block Bj contains
all the control nodes of block Bi, the attractors of Bj can be used to form the
realisations of Bi, no matter Bj is elementary or not. Observe that using a non-
elementary block as a parent block does not change the fact that the attractor
states of the parent block contain the values of all the nodes in the current block
and all its ancestor blocks.
Computing attractors for non-elementary blocks requires the knowledge of
the attractors of its parent blocks. Therefore, we need to order the blocks so that
for any block Bi, the attractors of its parent blocks are always detected before
it. The blocks are ordered topologically. For easier explanation of the order, we
introduce the concept of topological credit as follows. For simpliﬁcation, we refer
topological credit as credit in the remaining part of the paper.
Deﬁnition 12 (Topological credit). Given a BN G, the an elementary block
Bi of G has a topological credit of 0, denoted as P(Bi) = 0. Let Bj be a non-
elementary block and Bj1, . . . , Bjp(j) be all its parent blocks. The topological credit
of Bj is deﬁned as P(Bj) = maxp(j)
k=1(P(Bjk)) + 1, where p(j) is the number of
parent blocks of Bj.
3.3
Recovering Attractors for the Original BN
After computing attractors for all the blocks, we need to recover attractors for
the original BN, with the help of the following theorem.

A New Decomposition Method for Attractor Detection
243
Theorem 2. Let G be a BN and let Bi be one of its blocks. Denote Ω(Bi) as the
block formed by all Bi’s ancestor blocks and denote X(Bi) as the block formed
by merging Bi with Ω(Bi). X(Bi) is in fact an elementary block, which is also
a BN. The attractors of block Bi are in fact the attractors of X(Bi).
Theorem 3. Given a BN G, where Bi and Bj are its two blocks, let ABi and
ABj be the set of attractors for Bi and Bj, respectively. Let Bi,j be the block
got by merging the nodes in Bi and Bj. Denote the set of all attractor states
of Bi,j as SBi,j. If Bi and Bj are both elementary blocks, ABi C ABj and
∪A∈Π(ABi,ABj )A = SBi,j.
The above developed theoretical background with Theorems 2 and 3 being its
core result, allows us to design a new decomposition-based approach towards
detection of attractors in large synchronous BNs. The idea is as follows. We
divide a BN into blocks according to the detected SCCs. We order the blocks in
the ascending order based on their credits and detect attractors of the ordered
blocks one by one in an iterative way. We start from detecting attractors of
elementary blocks (credit 0), and continue to detect blocks with higher credits
after constructing their realisations. According to Theorem 2, by detecting the
attractors of a block, we in fact obtain the attractors of the block formed by
the current block and all its ancestor blocks. Hence, after the attractors of all
the blocks have been detected, either we have obtained the attractors of the
original BN or we have obtained the attractors of several elementary blocks of
this BN. According to Theorem 3, we can perform a cross operation for any two
elementary blocks (credits 0) to recover the attractor states of the two merged
blocks. The resulting merged block will form a new elementary block, i.e., one
with credit 0. The attractors can be easily identiﬁed from the set of attractor
states. By iteratively performing the cross operation until a single elementary
block containing all the nodes of the BN is obtained, we can recover the attractors
of the original BN. The details of this new algorithm are discussed in the next
section. In addition, we have the following corollary that extends Theorem 3 by
allowing Bi and Bj to be non-elementary blocks. This corollary will be used in
the next section.
Corollary 1. Given a BN G, where Bi and Bj are its two blocks, let ABi and
ABj be the set of attractors for Bi and Bj, respectively. Let Bi,j be the block got
by merging the nodes in Bi and Bj. Denote the set of attractor states of Bi,j as
SBi,j. It holds that ABi C ABj and ∪S∈Π(ABi,ABj )S = SBi,j.
4
A BDD-Based Implementation
We describe the SCC-based attractor detection method in Algorithm 1. As men-
tioned in Sect. 2.2, we encode BNs in BDDs; hence most operations in this algo-
rithm is performed with BDDs. Algorithm 1 takes a BN G and its corresponding
transition system T as inputs, and outputs the set of attractors of G. In this

244
A. Mizera et al.
Algorithm 1. SCC-based decomposition algorithm
1: procedure SCC Detect(G, T )
2:
B := Form Block(G); A := ∅; Ba := ∅; k := size of B;
3:
initialise dictionary Aℓ;
//Aℓis a dictionary storing the set of attractors for each block
4:
for i := 1; i <= k; i + + do
5:
if Bi is an elementary block then
6:
T Bi := transition system converted from Bi;
//see Sect. 2.2 for more details
7:
Ai := Detect(T Bi); Aℓ.add((Bi, Ai));
8:
else Ai := ∅;
9:
if Bp
i is the only parent block of Bi then
10:
Ap
i := Aℓ.getAtt(Bp
i );
//obtain attractors of Bp
i
11:
else Bp := {Bp
1 , Bp
2 , . . . , Bp
m} be the parent blocks of Bi (ascending ordered);
12:
Bc := Bp
1 ;
//Bp is ordered based on credits
13:
for j := 2; j <= m; j + + do
14:
Bc,j := a new block containing nodes in Bc and Bp
j ;
15:
if (Ap
i := Aℓ.getAtt(Bc,j)) == ∅then
16:
A := Π(Aℓ.getAtt(Bc), Aℓ.getAtt(Bj)); Ap
i := D(A);
17:
//D(A) returns all the attractors from attractor states sets A
18:
Aℓ.add(Bc,j, Ap
i );
19:
end if
20:
Bc := Bc,j;
21:
end for
22:
end if
23:
for A ∈Ap
i do
24:
T Bi(A) := ⟨SBi(A), T Bi(A)⟩;
//obtain the realisation of Bi with A
25:
Ai := Ai ∪Detect(T Bi(A));
26:
end for
27:
Aℓ.add((Bi, Ai));
//the add operation will not add duplicated elements
28:
Aℓ.add((Bi,ancestors, Ai));
//Bi,ancestors means Bi and all its ancestor blocks
29:
for any Bp ∈{Bp
1 , Bp
2 , . . . , Bp
m} do
//Bp
1 , Bp
2 , . . . , Bp
m are parent blocks of Bi
30:
Aℓ.add((Bi,p, Ai));
31:
end for
32:
end if
33:
end for
34:
for Bi ∈B and Bi has no child block do
35:
A = D(Π(Aℓ.get(Bi), A));
36:
end for
37:
return A.
38: end procedure
39: procedure Form Block(G)
40:
decompose G into SCCs and form blocks with SCCs and their control nodes;
41:
order the blocks in an ascending order according to their credits; B := (B1, . . . , Bk);
42:
return B.
//B is the list of blocks after ordering
43: end procedure
algorithm, we denote by Detect(T ) a basic function for detecting attractors of
a given transition system T . Lines 23–26 of this algorithm describe the process
for detecting attractors of a non-elementary block. The algorithm detects the
attractors of all the realisations of the non-elementary block and performs the
union operation of the detected attractors. For this, if the non-elementary block
has only one parent block, its attractors are already computed as the blocks
are considered in the ascending order with respect to their credits by the main

A New Decomposition Method for Attractor Detection
245
for loop in line 4. Otherwise, all the parent blocks are considered in the for
loop in lines 13–21. By iteratively applying the cross operation in line 16 to the
attractor sets of the ancestor blocks in the ascending order, the attractor states
of a new block formed by merging all the parent blocks are computed as assured
by Corollary 1. The attractors are then identiﬁed from the attractor states with
one more operation. The correctness of the algorithm is stated as Theorem 4.
Theorem 4. Algorithm 1 correctly identiﬁes the set of attractors of a given BN G.
We continue to illustrate in Example 6 how Algorithm 1 detects attractors.
Example 6. Consider the BN shown in Example 5 and its four blocks. Block B1
is an elementary block and it has two attractors, i.e., A1 = {{(0∗)}, {(11)}}. To
detect the attractors of block B2, we ﬁrst form realisations of B2 with the attrac-
tors of its parent block B1. B1 has two attractors so there are two realisations
for B2. The transition graphs of the two realisations are shown in Figs. 5a and b.
We get two attractors for block B2, i.e., A2 = {{(0 ∗00)}, {(1101)}}. Those
two attractors are also attractors for the merged block B1,2, i.e., A1,2 = A2. In
0100
0001
0111
0010
0000
0101
0011
0110
(a) Realisation 1 of B2.
1100
1110
1101
1111
(b) Realisation 2 of B2.
Fig. 5. Two realisations used in Example 6.
010000
010010
010001
010011
000000
000010
000001
000011
(a) The ﬁrst realisation of B4
110000
110011
110010
110001
(b) The second realisation of B4
111100
111111
111110
111101
(c) The third realisation of B4
Fig. 6. Transition graphs of the three realisations for block B4.

246
A. Mizera et al.
Algorithm 2. Leaf-based optimisation
1: procedure Leaf Detect(G)
2:
form an elementary block B by removing all the leaves of G;
3:
AB := SCC Detect (B); ΦB := ∪AB∈ABAB;
//detect attractors of B
4:
T := transition system of G with state space restricted to MG(ΦB);
5:
A := Detect (T );
6:
return A.
7: end procedure
Example 5, we have shown the two realisations of B3 regarding the two attractors
of B1. Clearly, B3 has two attractors, i.e., A3 = {{(0∗00)}, {(1100)}, {(1111)}}.
B4 has two parent blocks. Therefore, we need to merge the two parent blocks
to form a single parent block. Since the attractors of the merged block B1,3 are
the same as B3, we directly obtain the attractors of B1,3, i.e., A1,3 = A3 =
{{(0 ∗00)}, {(1100), (1111)}}. There are three attractors so there will be three
realisations for block B4. The transition graph of the three realisations are shown
in Fig. 6. From the transition graphs, we easily get the attractors of B4, i.e., A4 =
{{(0 ∗0000)}, {(0 ∗0001)}, {(110000)}, {(110011)}, {(111100)}, {(111111)}}.
Now the attractors for all the blocks have been detected. We can now obtain
all the attractors of the BN by several cross operations. We start from
the block with the largest credit, i.e., block B4. The attractors of B4 in
fact cover blocks B1, B3 and B4. The remaining block is B2. We perform
a cross operation on A2 and A4 and based on the obtained result we detect
the attractors of the BN, i.e., A = D(Π(A2, A4) = {{(0 ∗000000)}, {(0 ∗
000001)}, {(11010011)}, {(11010000)}, {(11011111)}, {(11011100)}}.
4.1
An Optimisation
It often happens that a BN contains many leaf nodes that do not have any child
node. Each of the leaf nodes will be treated as an SCC in our algorithm and it
is not worth the eﬀort to process an SCC with only one leaf node. Therefore, we
treat leaf nodes in a special way. Formally, leaf nodes are recursively deﬁned as
follows.
Deﬁnition 13. A node in a BN is a leaf node (or leaf for short) if and only if
it is not the only node in the BN and either (1) it has no child nodes except for
itself or (2) it has no other children after iteratively removing all its child nodes
which are leaf nodes.
Algorithm 2 outlines the leaf-based decomposition approach for attractor
detection. We now show that Algorithm 2 can identify all attractor states of
a given BN.
Theorem 5. Algorithm 2 correctly identiﬁes all the attractor states of a given
BN G.

A New Decomposition Method for Attractor Detection
247
5
Experimental Results
We have implemented the decomposition algorithm presented in Sect. 4 in the
model checker MCMAS [14]. In this section, we demonstrate the eﬃciency of
our method by comparing our method with the state-of-the-art decomposition
method mentioned in [8] which is also based on BDD implementation. We gener-
ate 33 random BN models with diﬀerent number of nodes using the tool ASSA-
PBN [15,16] and compare the performance of the two methods on these 33
models. All the experiments are conducted on a computer with an Intel Xeon
W3520@2.67 GHz CPU and 12 GB memory.
We name our proposed decomposition method M1 and the one in [8] M2.
There are two possible implementations of the Detect function used in Algo-
rithm 1 as mentioned in [8]: monolithic and enumerative. We use the monolithic
one which is shown to be more suitable for small networks as the decomposed
sub-networks are relatively small. Since the method in [8] uses similar leaf reduc-
tion technique, we make comparisons on both the original models and the models
whose leaves are removed in order to eliminate the inﬂuence of leaf nodes. We
set the expiration time to 3 h. Before removing leaf nodes, there are 11 cases
that both methods fail to process. Among the other 22 cases, our method is
faster than M2 in 16 cases, which is approximately 73%. After removing leaf
nodes, there are 5 cases that both methods fail to process. Among the other 28
cases, our method is faster than M2 in 25 cases, which is approximately 89%. We
demonstrate the results for 7 models in Table 1 and the remaining result can be
found in [17]. Since our method considers the dependency relation between dif-
ferent blocks, the attractors of all the blocks need to be computed; while method
M2 can ignore the blocks with only leaf nodes. Therefore, the performance of
our method is more aﬀected by the leaf nodes. This is why the percentage that
our method is faster than M2 is increased from 73% to 89% when leaf nodes
are removed. Notably, after eliminating the inﬂuence of leaf nodes, our method
is signiﬁcantly faster than M2. The “–” in Table 1 means the method fails to
process the model within 3 h. The speedup is therefore not applicable (N/A) for
Table 1. Selected results for the performance comparison of methods M1 and M2.
Model
ID
# nodes # non-
leaves
#
attractors
Original models
Models with leaves removed
tM2[s] tM1[s] Speedup tM2[s]
tM1[s] Speedup
1
100
7
32
4.56
0.86
5.3
0.58
0.02
29.0
2
120
9
1
18.13
0.95
19.1
1.10
0.04
27.5
3
150
19
2
201.22 1.66
121.2
0.74
0.02
37.0
4
200
6
16
268.69 7.04
38.2
0.97
0.02
48.5
5
250
25
12
533.57 11.16
47.8
0.90
0.04
22.5
6
300
88
1
–
–
N/A
238.96
65.33
3.7
7
450
43
8
–
60.82
N/A
3704.33 0.17
21790.2

248
A. Mizera et al.
this result. The speedup is computed as tM2/tM1, where tM1 is the time cost for
M1 and tM2 is the time cost for M2. All the time shown in Table 1 is in seconds.
In general, we obtain a larger speedup when the number of attractors is rela-
tively small. This is due to that our method takes the attractors of the parent
block into account when forming a realisation of a non-elementary block and
the number of realisations increases with the number of attractors. Summaris-
ing, our new method shows a signiﬁcant improvement on the state-of-the-art
decomposition method.
6
Conclusion and Future Work
We have introduced a new SCC-based decomposition method for attractor detec-
tion of large synchronous BNs. Although our decomposition method shares sim-
ilar ideas on how to decompose a large network with existing decomposition
methods, our method diﬀers from them in the key process and has signiﬁcant
advantages.
First, our method is designed for synchronous BNs, as a consequence the key
process for constructing realisations in our method is totally diﬀerent from the
one in [9], which is designed for asynchronous networks. Secondly, our method
considers the dependency relation among the sub-networks. The method in [8]
does not rely on this relation and only takes the detected attractors in sub-
networks to restrict the initial states when recovering the attractors for the
original network. In this way, the decomposition method in [8] potentially cannot
scale up very well for large networks, as it still requires a BDD encoding of the
transition relation of the whole network. This is our main motivation to extend
our previous work [9] towards synchronous BNs. Experimental results show that
our method is signiﬁcantly faster than the one in [8]. Lastly, we have shown that
the method proposed in [7] cannot compute correct results in certain cases.
Our current implementation is based on BDDs. One future work is to use
SAT-solvers to implement the Detect function as SAT-based methods are nor-
mally more eﬃcient in terms of attractor detection for synchronous BNs [6].
References
1. Kauﬀman, S.: Homeostasis and diﬀerentiation in random genetic control networks.
Nature 224, 177–178 (1969)
2. Huang, S.: Genomics, complexity and drug discovery: insights from Boolean net-
work models of cellular regulation. Pharmacogenomics 2(3), 203–222 (2001)
3. Somogyi, R., Greller, L.D.: The dynamics of molecular networks: applications to
therapeutic discovery. Drug Discov. Today 6(24), 1267–1277 (2001)
4. Garg, A., Xenarios, I., Mendoza, L., De Micheli, G.: An eﬃcient method for
dynamic analysis of gene regulatory networks and in silico gene perturbation
experiments. In: Speed, T., Huang, H. (eds.) RECOMB 2007. LNCS, vol. 4453,
pp. 62–76. Springer, Heidelberg (2007). doi:10.1007/978-3-540-71681-5 5

A New Decomposition Method for Attractor Detection
249
5. Garg, A., Di Cara, A., Xenarios, I., Mendoza, L., De Micheli, G.: Synchronous
versus asynchronous modeling of gene regulatory networks. Bioinformatics 24(17),
1917–1925 (2008)
6. Dubrova, E., Teslenko, M.: A SAT-based algorithm for ﬁnding attractors in syn-
chronous Boolean networks. IEEE/ACM Trans. Comput. Biol. Bioinform. 8(5),
1393–1399 (2011)
7. Guo, W., Yang, G., Wu, W., He, L., Sun, M.: A parallel attractor ﬁnding algorithm
based on Boolean satisﬁability for genetic regulatory networks. PLOS ONE 9(4),
e94258 (2014)
8. Yuan, Q., Qu, H., Pang, J., Mizera, A.: Improving BDD-based attractor detection
for synchronous Boolean networks. Sci. China Inf. Sci. 59(8), 080101 (2016)
9. Mizera, A., Pang, J., Qu, H., Yuan, Q.: Taming asynchrony for attractor detection
in large Boolean networks (Technical report) (2017). http://arxiv.org/abs/1704.
06530
10. Kauﬀman, S.A.: Metabolic stability and epigenesis in randomly constructed genetic
nets. J. Theor. Biol. 22(3), 437–467 (1969)
11. Shmulevich, I., Dougherty, E.R.: Probabilistic Boolean Networks: The Modeling
and Control of Gene Regulatory Networks. SIAM Press (2010)
12. Lee, C.Y.: Representation of switching circuits by binary-decision programs. Bell
Syst. Tech. J. 38(4), 985–999 (1959)
13. Akers, S.B.: Binary decision diagrams. IEEE Trans. Comput. 100(6), 509–516
(1978)
14. Lomuscio, A., Qu, H., Raimondi, F.: MCMAS: An open-source model checker for
the veriﬁcation of multi-agent systems. Int. J. Softw. Tools Technol. Transf. (2015)
15. Mizera, A., Pang, J., Yuan, Q.: ASSA-PBN: an approximate steady-state analyser
of probabilistic Boolean networks. In: Finkbeiner, B., Pu, G., Zhang, L. (eds.)
ATVA 2015. LNCS, vol. 9364, pp. 214–220. Springer, Cham (2015). doi:10.1007/
978-3-319-24953-7 16
16. Mizera, A., Pang, J., Yuan, Q.: ASSA-PBN 2.0: a software tool for probabilistic
Boolean networks. In: Bartocci, E., Lio, P., Paoletti, N. (eds.) CMSB 2016. LNCS,
vol. 9859, pp. 309–315. Springer, Cham (2016). doi:10.1007/978-3-319-45177-0 19
17. Mizera, A., Pang, J., Qu, H., Yuan, Q.: Benchmark Boolean networks. http://
satoss.uni.lu/software/ASSA-PBN/benchmark/attractor syn.xlsx

Construction of Abstract State Graphs
for Understanding Event-B Models
Daichi Morita1(B), Fuyuki Ishikawa2, and Shinichi Honiden1,2
1 The University of Tokyo, Tokyo, Japan
2 National Institute of Informatics, Tokyo, Japan
{d-morita,f-ishikawa,honiden}@nii.ac.jp
Abstract. Event-B is a formal method that supports correctness by
construction in system modeling using stepwise reﬁnement. However, it
is diﬃcult to understand the rigorous behaviors of models from Event-B
speciﬁcations, such as the reachable state space or the possible sequences
of events. This is because the Event-B model is described in a style that
lists events that have concurrently been enabled depending on their guard
conditions. This paper proposes a method that helps in understanding
the rigorous behaviors of an Event-B model by creating an abstract state
graph. The core of our method involves dividing the concrete state space
by using the guard conditions of individual events to extract states that
are essential to enable possible transitions to be understood. Moreover,
we further divided the state space by using the guard conditions of events
in the models before reﬁnement to support understanding of changes in
behaviors between the models before and after reﬁnement. Our unique
approach facilitated ﬁnding of invariants that were not speciﬁed but held,
which were useful for validation.
1
Introduction
Event-B [1] is a formal speciﬁcation language based on ﬁrst-order predicate logic
and set theory. It adopts a reﬁnement mechanism to allow developers to grad-
ually build a model while ensuring its correctness by using these mathematical
methods. Developers in Event-B modeling start from the most abstract machine
to build the model and then reﬁne it by building a more concrete machine
that introduces new aspects so that it is closer to the comprehensive machine
to be obtained. This reﬁnement process is continued until the comprehensive
machine is obtained that includes all the target aspects. This reﬁnement mech-
anism reduces the diﬃculty in rigorously modeling and verifying a complicated
model by enabling focus on individual small steps.
Event-B is a state-based formal method and the behavior of an Event-B
model is expressed by states and transitions. Thus, it is important for developers
to comprehend the reachable state space or the possible sequences of events
in terms of validation. However, this is diﬃcult because inﬁnite sets, such as
This work is partially supported by JSPS KAKENHI Grant Number 17H01727.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 250–265, 2017.
https://doi.org/10.1007/978-3-319-69483-2_15

Construction of Abstract State Graphs for Understanding Event-B Models
251
integers, can be used as types of variables and thus the size of the state space
can be inﬁnite or too large to comprehend.
ProB [12] is one of the standard tools to check Event-B models. Although
ProB provides several methods of visualizing the state space [9,11], there have
been problems with these methods. They have required developers to specify that
the range of inﬁnite sets be ﬁnite to generate a graph because they constructed
the state space by exhaustive simulation. Thus, the state space was restricted and
developers could miss unexpected behaviors outside the space. Moreover, there
are no methods of graph visualization that takes reﬁnement into consideration,
even though it would be useful to know how a concrete machine can reﬁne an
abstract one.
This paper proposes two methods of graph visualization of an Event-B model
from the speciﬁcations without simulation to enable behaviors to be rigorously
understood. The ﬁrst method involves constructing an abstract state graph using
predicate abstraction [8], which is useful to enable developers to explore the full
state space and not to overlook the diﬀerences in behaviors, according to the
range of inﬁnite sets. Our key idea was to use guard conditions of events for
predicate abstraction, which allowed us to extract essential insights into possi-
ble transitions (event occurrences) in each state. The second method was graph
visualization that took reﬁnement into account. It is useful for developers to
validate behaviors by checking the correspondence between the states and tran-
sitions of abstract and concrete machines. Moreover, these graphs are useful
for developers to ﬁnd stronger invariants than those described in the speciﬁ-
cations and help them to validate the state space. The unique feature in our
approach is that we ﬁrst constructed apparently reachable states from the speci-
ﬁed predicates, such as invariants, and we then examined actual (un)reachability.
This approach could expose unexpectedly unreachable states, which represented
implicit expectations or faults.
We have organized the rest of this paper as follows. Section 2 provides the nec-
essary background on Event-B and Sect. 3 describes our methods of generating
graphs. Section 4 explains how we evaluated our methods by providing various
applications. Section 5 relates our study to other studies and Sect. 6 concludes
the paper.
2
Preliminaries
2.1
Event-B Models
An Event-B model consists of two modules, which are called machine and con-
text. A context contains a set of constants and a set of axioms. Axioms are
predicates that denote the constraints that the constants must satisfy. A machine
contains a set of variables, a set of invariants and a set of events. Invariants are
predicates that denote the safety properties that developers require variables
and constants to satisfy. That they are actually invariants is veriﬁed by proving
some statements, which are called “proof obligations”. An event mainly consists
of some guards and actions. Guards are predicates to denote the conditions under

252
D. Morita et al.
which an event is to be enabled. Actions are called before-after predicates that
denote the relationships between the values of variables just before and after
an event. The values of the constants cannot be changed by the events. Thus,
the states of the model consist of the dynamic values of the variables and the
static values of the constants. The transitions between them are triggered by the
occurrence of the events.
For example, let us take Abrial’s model of “controlling cars on a bridge”
(from [1, Chap. 2]) into account. There is a mainland, an island, a bridge between
them, and traﬃc lights that control cars going to and coming from them in the
model. There was only the mainland and island in the initial model. It consisted
of a context Ctx0 and a machine Mac0 shown in Fig. 1. The constant d deﬁned
in Ctx0 denotes the maximum number of cars allowed to be on the island. The
variable n deﬁned in Mac0 represents the number of cars on the island. The
invariant inv0 2 means that constant d is actually the maximum number of cars
on the island. The states of the model consist of two values of variable n and
constant d, such as (n, d) = (0, 1). The state space is inﬁnite because d can
be an arbitrary natural number that is more than zero. The event init is the
initialization event, ML out is the event corresponding to the transition of a car
from the mainland to the island, and ML in is its inverse event. The guard grd1
of the event ML out is n < d and the action act1 is the before-after predicate
n′ = n + 1. The value of the variable just after an event has occurred makes its
before-after predicate true. A primed variable, such as n′ appearing in act1 in a
before-after predicate, denotes the value of the variable just after an event has
occurred. Thus, the before-after predicate act1 means that the value of variable
n just after the event is equal to the value of variable n just before it has occurred
plus one.
context
Ctx0
constants d
axioms
axm0 1
d ∈IN
axm0 2
d > 0
sees
Ctx0
variables
n
invariants
inv0 1
n ∈IN
inv0 2
n ≤d
Events
initialization
begin
init:
n = 0
end
ML out
ML in
when
when
grd1:
n < d
grd1: 0 < n
then
then
act1:
n = n + 1
act1: n = n −1
end
end
Fig. 1. Event-B speciﬁcations of Mac0

Construction of Abstract State Graphs for Understanding Event-B Models
253
2.2
Event-B Reﬁnement
The reﬁnement mechanism in Event-B is a way of gradually building a model.
An abstract machine is reﬁned by a new machine and new features and details
are introduced into the abstract machine. A new machine is called a concrete
machine. A sequence of machines linked by a reﬁnement relationship is called
a “reﬁnement chain”. An increasingly more complicated but accurate model is
built through stepwise reﬁnements.
For example, Mac1 (Fig. 2) reﬁnes Mac0. A one-way bridge is introduced
into the abstract machine. The variable a is the number of cars on the bridge
going to the island, b is the number on the island and c is the number on the
bridge coming to the mainland. The variable n deﬁned in Mac0 is replaced by
these three variables and the invariant inv1 2 denotes the relationship between
n and a, b, c. The states consist of four values of the variables a, b, c and the
constant d. The invariant inv1 3 denotes that the bridge is one-way. The two
events ML out and ML in in Mac0 are reﬁned as they are events on these
reﬁnes Mac0
sees
Ctx0
variables
a, b, c
invariants
inv1 1
a ∈IN ∧b ∈IN ∧c ∈IN
inv1 2
a + b + c = n
inv1 3
a = 0 ∨c = 0
Events
initialization
begin
init:
a = 0 ∧b = 0 ∧c = 0
end
ML out
ML in
reﬁnes
ML out
reﬁnes ML in
when
when
grd1:
a + b < d
grd1: 0 < c
grd2:
c = 0
then
then
act1:
a = a + 1
act1:
c = c −1
end
end
IL in
IL out
when
when
grd1:
0 < a
grd1: 0 < b
grd2: a = 0
then
then
act1:
a = a −1
act1:
b = b −1
act2:
b = b + 1
act2:
c = c + 1
end
end
Fig. 2. Event-B speciﬁcations of Mac1

254
D. Morita et al.
three variables. ML out in Mac1 corresponds to the transition of a car from the
mainland to the bridge and ML in is its inverse event. These reﬁned events must
simulate the original ones. The before-after predicate of ML out is a′ = a+1 but
it implicitly contains b′ = b and c′ = c, which means that the values of missing
variables in the predicate are equal to the values just before the event. Thus, its
precise before-after predicate is a′ = a + 1 ∧b′ = b ∧c′ = c, and it simulates that
of the original because the invariant a + b + c = n holds. Guards of the reﬁned
event must not contradict those of the original. The guard of the event ML out
in Mac1 is a + b < d and does not contradict the guard n < d of the abstract
one because the invariant a + b + c = n holds.
New events can be introduced into abstract models. IL in and IL out in
Mac1 are new events. IL in corresponds to the transition of a car from the
bridge to the island and IL out is its inverse event. They do not need to modify
any abstract variables so that the abstract model is not contradicted, i.e., they
need to reﬁne null event skip in which guard is true and action has no meaning.
3
Method
3.1
Construction of Abstract State Graph (CASG)
The goal discussed in this subsection was to construct a state graph from the
Event-B speciﬁcations, which is useful for understanding the rigorous behavior
of an Event-B model. The state space that may be inﬁnite is abstracted by
predicate abstraction. We called the graph an “abstract state graph” as in Graf
and Sa¨ıdi [8] and called our method of constructing the abstract state graph
CASG.
Let us assume that we have a machine M. We use symbols InvM to denote
the conjunction of all invariants and axioms that appear in the reﬁnement chain
that precedes M, and BAevt for the conjunction of all before-after predicates
of the event evt and the event that reﬁnes evt in the reﬁnement chain. For
example, InvMac1 = (d > 0 ∧n ≤d ∧a + b + c = n ∧(a = 0 ∨c = 0)) and
BAML out = (n′ = n + 1 ∧a′ = a + 1 ∧b′ = b ∧c′ = c). We also use symbols Gevt
to denote the conjunction of all the guards of event evt and EvtM to denote the
set of the events of model M.
An abstract state graph of the Event-B machine consists of (S, I, L, δ), where
S is a set of abstract states, I is a set of initial abstract states, L is a labeling
function of the set S and δ is a transition function with guard conditions. An
abstract state that constitutes S is deﬁned by a predicate and a set of states
that satisfy the predicate. For example, 0 ≤n ≤2 represents the set of states
{(n, d) | n, d ∈IN, 0 ≤n ≤2}. After this we will use a predicate to denote an
abstract state, i.e., we will refer to abstract states and predicates as exchangeable
words. We will use “concrete states” to refer to states of the model to distinguish
them from abstract states.

Construction of Abstract State Graphs for Understanding Event-B Models
255
First, let us deﬁne set S of abstract states. We use sat(p) to denote that the
predicate p is satisﬁable. Set S is constructed as:
S =

s | E ⊆EvtM, s = InvM ∧

evt∈E
Gevt ∧

evt∈EvtM\E
¬Gevt , sat(s)

.
This deﬁnition constructs abstract states by making equivalence classes of con-
crete states that satisfy invariants in terms of enabled events in each state. The
possibly inﬁnite state space is reduced into ﬁnite state space. However, note
that the state space is approximated by the invariants and it may include some
unreachable concrete states. This is discussed in Subsect. 3.5. This approximation
is reasonable since invariants are properties that developers require the model to
satisfy and they are veriﬁed by discharging proof obligations. Although the idea
that states that have the same enabled events are regarded as being the same
is similar to the method described in Leuschel and Turner [13], our method
does not require developers to specify the range of inﬁnite sets. It also provides
predicates that explain the conditions of individual states.
The set I of initial abstract states is the set of abstract states that the before-
after predicate of the initialization event satisﬁes.
The labeling function L for each s ∈S, to specify the events enabled in an
abstract state, is deﬁned as:
L(s) = {evt ∈EvtM | sat(Gevt ∧s)}.
The transition function δ is then constructed. We use after(s) for each
predicate s to denote a predicate where all variable symbols are replaced with
primed variable symbols, which means that they are values just after events
have occurred. Note that after only replaces variable symbols, and not constant
symbols. The δ for each s ∈S and evt ∈L(s) is deﬁned as:
δ(s, evt) = {(s′, g) | s′ ∈S, g = s ∧BAevt ∧after(s′), sat(g)}.
The predicate BAevt ∧after(s′) is like the weakest precondition if the state will
be s′ just after evt has occurred. Thus, g is a guard condition of the transition.
Note that even if there is an edge, the corresponding transition cannot always
occur in M because of our approximation.
Let us take the model Mac0 (Fig. 1) as an example. The graph constructed by
using CASG is outlined in Fig. 3. The two lines in each ellipse, such as {ML out}
and n = 0&d > 0 in the top ellipse, denote the enabled events and abstract
states. Type invariants have been omitted. The two lines beside the arrow denote
the name of the event and the guard condition.
A graph constructed by using CASG is an abstraction of the actual state
graph of an Event-B model, in which state space may be inﬁnite. Solving satis-
ﬁability problems enables us to explore the full state space when checking the
existence of transitions. An important aspect of this abstraction is that transi-
tions that actually occur in the model are in the graph and transitions that are
not in the graph do not occur in the model.

256
D. Morita et al.
{ML_out}
n=0&d>0
{ML_out,ML_in}
0<n<d
ML_out
0<n<d-1
ML_in
1<n<d
ML_in
n=1&d>1
{ML_in}
n=d&d>0
ML_out
0<n=d-1
ML_out
n=0&d>1
ML_out
n=0&d=1
ML_in
1<n=d
ML_in
n=1&d=1
Fig. 3. Abstract state graph of Mac0
3.2
Construction of Reﬁnement Abstract State Graph (CRASG)
This subsection explains how we constructed a graph that took reﬁnement into
consideration. We assumed that we had machines MA and MC, such that MC
reﬁned MA. Each state in MA was reﬁned by some states in MC through the
reﬁnement. We wanted to reﬂect such a relation between the graphs of MA and
MC. In other words, each abstract state of the graph of MC constructed with
the method described in this subsection corresponds to one abstract state of the
graph of MA constructed with CASG. We called the method of constructing the
abstract state graph that took reﬁnement into account CRASG.
Let us deﬁne a binary relation RV to clarify the relation between the abstract
state graphs for MA and MC. The RV is a binary relation between the abstract
states of the graphs. Let SA be the abstract states set of the graphs for MA that
is constructed by using CASG, and let SC be the abstract states set of the graph
for MC that is constructed by using CRASG. The RV is deﬁned as:
RV = {(s, s′) ∈SC × SA | sat(s ∧s′)}.
Here, (s, s′) ∈RV means that there is a concrete state in s that corresponds
to a state in s′. Our main objective in this subsection is to explain how we
constructed the graph of MC, such that RV is a function, which means each
abstract state in SC corresponds to one abstract state in SA.
Let us now construct the abstract states set S. Let Evt = EvtMC ∪EvtMA.
The S is constructed as:
S =

s | E ⊆Evt, s = InvMC ∧

evt∈E
Gevt ∧

evt∈Evt\E
¬Gevt, sat(s)

.

Construction of Abstract State Graphs for Understanding Event-B Models
257
The construction of S splits the state space of MC by the equivalence relation,
where the enabled events of MC are the same and those of MA are the same if
the space is projected onto the space of MA. Therefore, the abstract states in the
graph of MA that are constructed by using CASG are divided even more into S,
and RV becomes a function. Note that unlike CASG, there can be states where
the same events of MC are enabled. Then, the remainder of the construction of
the graph is similar to that with CASG.
For example, the graph of Mac1 that is constructed by using CRASG is
given in Fig. 4. The squares denote the abstract states of the Mac0 graph. The
dashed arrows mean that they correspond to transitions in the Mac0 graph.
The predicates in the graph have been omitted to the extent that they can be
understood. The guard labels have been completely omitted.
{ML_in}
{ML_out,ML_in}
{ML_out}
{ML_out}
a=b=c=0
{IL_in}
a>0&0â ¤b&a+b=d&c=0
ML_out
{ML_out,IL_in}
a>0&bâ ¥0&a+b<d&c=0
ML_out
IL_in
{IL_out}
a=c=0&b=d
IL_in
{ML_in,IL_out}
a=0&b>0&c>0&b+c=d
IL_out
{ML_in}
a=b=0&c=d
IL_out
{ML_out,IL_out}
a=c=0&0<b<d
ML_in
{ML_in,IL_out}
a=0&b>0&c>0&b+c<d
ML_in
IL_out
IL_out
ML_in
{ML_in}
a=b=0&0<c<d
ML_in
ML_out
ML_outIL_in
IL_in
ML_out
ML_out
IL_out
IL_out
ML_in
ML_inIL_out
IL_out
ML_in
ML_in
Fig. 4. Abstract state graph of Mac1 by taking reﬁnement into consideration
The graph of MC that is constructed by using CRASG is the reﬁnement
of the graph of MA that is constructed with CASG. The abstract states of the
graph of the concrete model can be grouped by which abstract state of the graph
for MA they satisfy because the binary relation RV is a function. Moreover, the
transitions of the graph for MC can be grouped by which transition of the graph
for MA they simulate. These facts indicate that the graph provides a visualization
of how MC simulates MA in terms of states and transitions.
3.3
Implementation
These two methods need to solve numerous satisﬁability problems. Event-B is
based on ﬁrst-order predicate logic, and thus predicates used in Event-B models

258
D. Morita et al.
are ﬁrst-order predicates within a practical range. A satisﬁability modulo the-
ories (SMT) solver [3] is one of the tools used to solve them automatically.
Although the range of problems that it can solve is limited, it can solve many of
the predicates in practical Event-B models. We used Z3 [5] as an SMT solver for
our implementation. We succeeded in automatically creating a graph for Mac0,
Mac1 and Mac2 (as a result of the second reﬁnement of “controlling cars on a
bridge”) by using CASG and the graph for Mac1 and Mac2 by using CRASG.
3.4
Checking for Existence of Transitions
A transition corresponding to an edge in the graph of the model constructed
with our methods could not always actually occur in the model because of our
approximation. However, developers could check whether or not each transition
(s, evt, s′) could actually occur by using linear temporal logic (LTL [7]) model
checking. The properties to check for occurrences could be directly represented
by using LTL[e] [15] that introduced the operator [evt], which meant the next
executed event was evt. The condition that a transition (s, evt, s′) could occur
was formulated by LTL[e] in the form ¬G¬(s ∧[evt] ∧X(s′)).
The LTL model checker could be used in ProB [14], which also supports
LTL[e]. Note that it adopts lower approximation and model checking is done
under some values of constants. Thus, developers need to appropriately set the
range.
3.5
Checking Validity and Strengthening Invariants
This subsection introduces another unique use of the graphs that were con-
structed with our methods due to our construction. It promotes the enhance-
ment of invariants to strictly represent their expectations on acceptable and
unacceptable states.
Suﬃcient invariants should ideally be given to strictly distinguish expecta-
tions on reachable and unreachable state spaces. However, this does not always
hold, even at the level of the model appearing in [1], which is one of the most
well-known references on Event-B. One reason for this is that developers com-
pletely understand the reachable concrete state space and decide not to add
invariants to the model because it is redundant and these would not matter. In
other words, reachable spaces are indirectly constrained by other means, such
as guard conditions in events. This implicit approach may cause problems when
other developers try to understand and revise the model in this case. Another
reason is that they do not understand the reachable state space and have missed
the invariants held in the model. There probably will be unexpected behaviors
in the model in this case. Therefore, it is worth suggesting invariants that hold
to conﬁrm validity.
Our approach approximates the concrete state space with the invariants of
a model. Thus, our graph can provide the diﬀerence between the actual reach-
able state space and the invariants by examining reachability. Other visualiza-
tion methods [9,11] could not provide this because their construction was based

Construction of Abstract State Graphs for Understanding Event-B Models
259
on the simulation of the model and did not take invariants into consideration.
Developers then checked each of the suggested invariants; they could be invari-
ants that should have been added or they represented unexpectedly attained
unreachable states caused by faults in the model (e.g., overly strong guard con-
ditions). There are three methods that suggest invariants that can be achieved
by using our graph.
First, let us assume there is an unreachable abstract state s from the initial
abstract state. Then, its negation ¬s is an invariant. This is because if there are
no transitions into the abstract state in the model, then all the events preserve
the negation.
Second, let us assume that there is a transition s
evt,g
−−−→s′ in the graph
that actually does not occur in the model (recall Subsect. 3.4). There are some
unreachable concrete states in the source abstract state s of the transition in that
case. We can ﬁnd such a state by using an SMT-solver and ﬁnding an assignment
of the predicate s ∧g ∧BAevt ∧s′. Developers can then ﬁnd some invariants by
investigating why this state was unreachable.
Third, let us assume that there is abstract state s that contains a concrete
state unreachable from other abstract states. The condition is formulated as:
after(s) ⇒

(s′,evt):(s,g)∈δ(s′,evt)
s′ ∧BAevt.
The predicate s′ ∧BAevt means the possible reachable states from s′ just after
event evt has occurred. Thus, the formula means that abstract state s is actually
included in the possible reachable states from other states. There are unreachable
concrete states in s if the condition does not hold, and some invariants can be
added.
4
Evaluation
4.1
Setting
We evaluated our methods by using three applications of the graphs.
Subsection 4.2 explains how we investigated the graph constructed by using
CASG. This is useful for understanding the overall behaviors of the model and
the validation of the state space by using predicates. Subsection 4.3 describes how
we investigated the graph constructed with CRASG. This would help devel-
opers understand the details on changes in behaviors caused by reﬁnement.
Subsection 3.5 explains how many we found the stronger invariants than those
described in the speciﬁcations. We used the Mac2 model (from [1, Chap. 2]) that
reﬁnes Mac1 to evaluate our methods in addition to Mac0 and Mac1.
4.2
Abstract State Space Exploration
Our main objective was to help developers understand the behaviors of an Event-
B model. Our methods provide state graphs that represent behaviors. Abstrac-
tion of the state space reduces the complexity of the original state graph and

260
D. Morita et al.
makes it easy for developers to comprehend behaviors. In addition, abstract
states help them validate the model since these states are represented by predi-
cates on the variables and constants in the model and the predicates can promote
developers’ understanding of the states.
Here, we will provide an example of exploring the state space by using our
graph. Our graph provides the conditions for the variables and constants that
represent the set of all concrete states that enable the same events. The example
of Mac0 (Fig. 1) indicates that, if ML out and ML in are concurrently enabled,
then developers can see that 0 < n < d holds in the state on the left in Fig. 3. The
predicate is easy for them to compare with their intentions because they describe
various predicates in Event-B modeling and they can perceive the situation with
the model from the predicates. If they write the guard of ML out incorrectly
as n < d −1, the predicate of the state with label {ML out, ML in} will be
0 < n < d −1. They can then ﬁnd that the guard is incorrect because their
intention is for the number of cars allowed on the island to be d, but this is not
achieved. It is also important for d to be symbolic. Due to this, they can validate
that this condition holds no matter what the value of constant d is.
The graph of an Event-B model constructed by using CASG helps developers
validate the state space by providing predicates on the variables and constants.
However, this method does not completely solve the problem of complexity in the
graph if the model is very complicated. One possible solution is for developers
to focus on the change caused by reﬁnement, which will be discussed in next
section. This cannot completely solve the problem, but it is eﬀective for limited
ranges of investigation.
It also helps developers to comprehend overall sequences of the executed
events by searching paths in the graph, even though not all sequences of events
that correspond to the paths can be executed. The guard conditions of the tran-
sitions are useful when validating sequences of events because they can express
conditions where the sequences can be executed.
4.3
Reﬁnement Abstract State Graph Exploration
A graph constructed by using CRASG helps developers understand changes in
behaviors caused by reﬁnement. Changes are not trivial from the speciﬁcations
because reﬁnement is across the invariants, guards, and actions. The graph is
very useful for understanding some aspects of the eﬀect of reﬁnement.
An aspect is how a concrete model simulates its abstract model. In other
words, the correspondences between abstract states and transitions in the model
before reﬁnement to those in the model after reﬁnement are drawn in the graphs.
Let us take model Mac1 (Fig. 2) as an example. There is a square in Fig. 4 that is
labeled {ML in}, which represents the abstract state of the graph for Mac0 that
satisﬁes the predicate n = d∧d > 0. The predicate n = d means that the number
of cars on the island has reached the limit. Thus, the states and transitions in the
square describe how the cars are moving between the island and mainland along
the bridge to solve traﬃc jams in the model Mac1. There are also two transitions
labeled ML out between the squares labeled {ML out, ML in}, and {ML in}

Construction of Abstract State Graphs for Understanding Event-B Models
261
in Fig. 4. Such transitions correspond to the transition in the graph in Fig. 3 that
are labeled ML out between the abstract states labeled {ML out, ML in} and
{ML in}, which means that the number of cars on the island has just reached
the limit.
Another aspect is how a concrete model does not simulate its abstract model.
The abstract model can be reﬁned so that some transitions of the abstract
model cannot occur in the concrete model by strengthening guards regardless
of whether they have been intended or not. This graph helps developers at such
times to discover what transition has occurred and what has not occurred in the
concrete model. If developers write the guard of ML out of Mac1 incorrectly as
a + b < d −1, the transitions that correspond to the transition labeled ML out
between abstract states labeled {ML out, ML in} and {ML in} will actually
disappear from the graph. Developers can then ﬁnd the degree of degradation
and check whether it is intended.
The graph constructed by using CRASG provides correspondences in the
state graph for concrete and abstract models. It is useful for developers to fully
understand reﬁnement by comparing it with the models. It also helps developers
explore the model by focusing on the change if the model is complicated.
4.4
Checking Validity and Strengthening Invariants
We applied the method described in Subsect. 3.5 to Mac2 and found missing
invariants that were needed to express precise reachable states. The details are
described in Appendix A. As a result, we discovered seven invariants and dis-
charged their proof obligations on the Rodin platform [2], which is equipped
with theorem provers. Moreover, we checked that the state space expressed by
the invariants was the actual reachable state space of the Mac2 model.
However, methods such as these three present several problems. One problem
is that the methods are not automatic except for the ﬁrst one. The methods
provide a hint but require some suggestions by developers. Even though they
are required to do so, it is diﬃcult for other graph visualization methods to
ﬁnd stronger invariants. Moreover, such suggestions also help them validate the
model and the discovered invariants support the building of a more accurate
model.
5
Related Work
Our method is classiﬁed in terms of abstraction as the predicate abstraction
described in Graf and Saidi [8]. Given a set of predicates, it splits the state
space of variables appearing in a program or model based on the Boolean value
of the predicates. It can solve a state space explosion problem in model checking
by providing appropriate predicates. We used it for graph visualization of an
Event-B model. We chose the set of guards for the event and the invariants
of the model as input for predicate abstraction. This very eﬀectively expressed
behaviors of the model because the state space could be approximated by the

262
D. Morita et al.
invariants and the guards determined the sequences of the executed events. The
unique feature of our approach is that we ﬁrst construct apparently reachable
states from speciﬁed predicates, such as invariants, and we then examine actual
(un)reachability.
Graph visualization is one of the primary methods of enabling the behaviors
of a ﬁnite state machine to be understood. As described in Dulac et al. [6], the
readability of formal speciﬁcations is a factor that has not widely been used
in industry and visualization often helps people understand speciﬁcations. Our
methods were aimed at visualizing the behaviors of a possibly inﬁnite state
machine of Event-B by reducing it into a ﬁnite state machine using predicate
abstraction.
There are several other methods of visualizing the state space of an Event-B
model. ProB [12] can generate a state graph of the model. The number of states
and transitions in the graph are sizably large because it tries to generate the
original state space. Thus, it is hard to understand behaviors. Moreover, ProB
requires the range of inﬁnite sets to be speciﬁed. Thus, the generated state space
may be restricted and developers may miss unexpected behaviors. However, the
state space of our graph is approximated by invariants so that all the behaviors
of the model can be expressed in the graph.
Other methods of visualizing the state space are described in Leuschel and
Turner [13]. There are two methods called the deterministic ﬁnite automaton
(DFA)-abstraction algorithm and the signature merge method. These methods
have aimed at reducing the complexity of the graph generated by ProB. The
method of DFA abstraction is based on the classical minimization algorithm for
DFA. This method produces a graph in which the sequences of transitions are
equivalent to those in the original state space, but it cannot eﬀectively reduce
the state space and its graph still makes it diﬃcult for developers to under-
stand behaviors. The signature merge method is similar to ours in terms of the
way abstraction focuses on enabled events. However, there are no predicates to
represent the states and developers thus ﬁnd our graph is more understandable.
Another similar approach described in Ladenberger and Leuschel [11] is cre-
ating projection diagrams. A projection diagram is an abstraction of the origi-
nal graph that is obtained by using some projection function. The method can
reduce complexity more eﬀectively than the two approaches explained above and
our methods by focusing on certain variables or some expressions in the model.
However, it may lose too much information to enable the overall behavior to be
understood, unlike that in ours.
In contrast to visualizing the state space of the model, uniﬁed modeling
language-B (UML-B) [16] is a method of building an Event-B model by drawing
a diagram. It is similar to UML and easy to use by developers who are familiar
with it. It mitigates the burden in Event-B modeling but is not suitable for
building a complex model.
Another method of understanding the behavior of models is model check-
ing [4]. In particular, Hoang et al. [10] stated that proof obligations in Event-
B ensure safety properties; on the other hand, LTL model checking ensured

Construction of Abstract State Graphs for Understanding Event-B Models
263
temporal liveness properties. However, it is diﬃcult to check overall behavior
unlike that in our methods, such as the reachable state space or the sequences
of executed events.
6
Conclusion
We proposed two methods of constructing the graph of an Event-B model from
the speciﬁcations. Our methods are useful for graphically understanding the rig-
orous behavior of the model. They also allow developers to investigate reachable
or unreachable states and transitions that cannot be searched by other graph
visualizations [11,13]. The second method is useful for checking the correspon-
dence between the graphs of the abstract and concrete machines and understand-
ing changes in behaviors caused by reﬁnement. Additionally, our methods enable
developers to enhance invariants to strictly represent their expectations. We con-
cluded that our methods could help developers to understand the behaviors of
the model and validate it from various viewpoints. One possible direction in
future work is to develop a more eﬀective way of visualization for large systems.
A
Appendix
This appendix explains how we investigated the advanced and unique use
described in Subsects. 3.5 and 4.4 to discover invariants that were stronger than
the invariants described in the speciﬁcations by using a graph constructed with
CASG. We used the Mac2 model and the speciﬁcations are in Abrial [1, Chap. 2].
We applied the ﬁrst method and discovered three unreachable states from
the graph. One of them is represented by
a = 0 ∧0 < b < d ∧c = 0 ∧ml tl = il tl = red ∧ml pass = il pass = true.
We then tried to add the predicate
¬(a = 0 ∧0 < b < d ∧c = 0 ∧ml tl = il tl = red ∧ml pass = il pass = true)
as an invariant to the Mac2 model on the Rodin platform [2]. As proof obligation
is automatically discharged by them, the predicate is actually an invariant of the
model. This invariant is equivalent to:
(a = c = 0 ∧ml tl = il tl = red ∧ml pass = il pass = true) ⇒(b = 0 ∨b = d),
which means that if all the traﬃc lights are red, the ﬂags are true and there
are no cars on the bridge, then the number of cars on the island is zero or has
reached its capacity. Developers can check if the situation is valid in the model.
We investigated the number of transitions in the Mac2 graph constructed
by using CASG that could occur in the second method. We speciﬁed the range
of the constant d from one to 10 because it seemed to be suﬃcient from our
investigation of the model. We checked all 58 edges in the graph and discovered

264
D. Morita et al.
16 edges that did not actually occur. One of them was the transition labeled
IL in from the abstract state represented by:
a > 0∧b ≥0∧a+b < d−1∧c = 0∧ml tl = green∧il tl = red∧il pass = true
to another represented by:
a = c = 0 ∧b < d −1 ∧ml tl = green ∧il tl = red ∧il pass = true
∧(b = 0 ∨(b > 0 ∧ml pass = false)).
(1)
A concrete state where the transition can occur is:
(a, b, c, d, ml tl, il tl, ml pass, il pass) = (1, 1, 0, 4, green, red, false, true).
However, it is actually unreachable because the condition ml pass = false
requires the event ML tl green to occur and ML out 1 and ML out 2 must
not subsequently occur. There was some suggestion that the model always sat-
isﬁes a > 0 ⇒ml pass = true because a > 0 means ML out 1 or ML out 2
has occurred at least once just after ML tl green has taken place. Then, we
added it as an invariant to the Rodin platform, but its proof obligation was not
automatically discharged. Due to an analysis of the failure of the proof, which
is often used in Event-B, we added (ml tl = red ∧a + b ̸= d) ⇒a = 0 as an
invariant and all proof obligations were automatically discharged.
Finally, let us take Mac2 as an example of the third method. The abstract
state represented by the predicate (1) does not satisfy the condition. All the
transitions into it are labeled ML tl green. Since ML tl green makes ml pass
false, all concrete states where ml pass is true in the abstract state are unreach-
able. There was some suggestion that the predicate a = b = 0∧ml tl = green ⇒
ml pass = false was an invariant. Therefore, we added it and proof obligation
was discharged.
References
1. Abrial, J.R.: Modeling in Event-B: System and Software Engineering. Cambridge
University Press, New York (2010)
2. Abrial, J.R., Butler, M., Hallerstede, S., Hoang, T.S., Mehta, F., Voisin, L.: Rodin:
an open toolset for modelling and reasoning in Event-B. Int. J. Softw. Tools Tech-
nol. Transf. (STTT) 12(6), 447–466 (2010)
3. Barrett, C., Sebastiani, R., Seshia, S., Tinelli, C.: Satisﬁability modulo theories.
In: Handbook of Satisﬁability, Frontiers in Artiﬁcial Intelligence and Applications,
vol. 185, Chap. 26, pp. 825–885. IOS Press (2009)
4. Clarke, E.M., Grumberg, O., Peled, D.A.: Model Checking. MIT Press, Cambridge
(1999)
5. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). doi:10.1007/978-3-540-78800-3 24
6. Dulac, N., Viguier, T., Leveson, N.G., Storey, M.D.: On the use of visualization in
formal requirements speciﬁcation. In: Proceedings of the IEEE Joint International
Conference on Requirements Engineering, pp. 71–80 (2002)

Construction of Abstract State Graphs for Understanding Event-B Models
265
7. Gabbay, D., Pnueli, A., Shelah, S., Stavi, J.: On the temporal analysis of fairness.
In: Proceedings of the 7th ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages (POPL 1980), pp. 163–173. ACM, New York (1980)
8. Graf, S., Saidi, H.: Construction of abstract state graphs with PVS. In: Grumberg,
O. (ed.) CAV 1997. LNCS, vol. 1254, pp. 72–83. Springer, Heidelberg (1997). doi:10.
1007/3-540-63166-6 10
9. Ham, F.V., van de Wetering, H., Van Wijk, J.J.: Visualization of state transition
graphs. In: Proceedings of the IEEE Symposium on Information Visualization 2001
(INFOVIS 2001), p. 59. IEEE Computer Society, Washington, DC (2001)
10. Hoang, T.S., Schneider, S., Treharne, H., Williams, D.M.: Foundations for using
linear temporal logic in Event-B reﬁnement. Form. Aspects Comput. 28(6), 909–
935 (2016)
11. Ladenberger, L., Leuschel, M.: Mastering the visualization of larger state spaces
with projection diagrams. In: Butler, M., Conchon, S., Za¨ıdi, F. (eds.) ICFEM
2015. LNCS, vol. 9407, pp. 153–169. Springer, Cham (2015). doi:10.1007/
978-3-319-25423-4 10
12. Leuschel, M., Butler, M.: ProB: an automated analysis toolset for the B method.
Int. J. Softw. Tools Technol. Transf. 10(2), 185–203 (2008)
13. Leuschel, M., Turner, E.: Visualising larger state spaces in PROB. In: Treharne,
H., King, S., Henson, M., Schneider, S. (eds.) ZB 2005. LNCS, vol. 3455, pp. 6–23.
Springer, Heidelberg (2005). doi:10.1007/11415787 2
14. Lichtenstein, O., Pnueli, A.: Checking that ﬁnite state concurrent programs satisfy
their linear speciﬁcation. In: Proceedings of the 12th ACM SIGACT-SIGPLAN
Symposium on Principles of Programming Languages (POPL 1985), pp. 97–107.
ACM, New York (1985)
15. Plagge, D., Leuschel, M.: Seven at one stroke: LTL model checking for high-level
speciﬁcations in B, Z, CSP, and more. Int. J. Softw. Tools Technol. Transf. 12(1),
9–21 (2010)
16. Snook, C., Butler, M.: UML-B and Event-B: an integration of languages and tools.
In: Proceedings of the IASTED International Conference on Software Engineering
(SE 2008), pp. 336–341. ACTA Press, Anaheim (2008)

A Framework for Modeling and Verifying IoT
Communication Protocols
Maithily Diwan and Meenakshi D’Souza(B)
International Institute of Information Technology, Bangalore, India
maithily.diwan@iiitb.org, meenakshi@iiitb.ac.in
Abstract. Communication protocols are integral part of the ubiqui-
tous IoT. There are numerous light-weight protocols with small foot-
print available in the Industry. However, they have no formal semantics
and are not formally veriﬁed. Since these protocols have many com-
mon features, we propose a uniﬁed approach to verify these protocols
through a framework in Event-B. We begin with an abstract model of
an IoT communication protocol which encompasses common features
of various protocols. The abstract model is then reﬁned into concrete
models for individual IoT protocols using reﬁnement and decomposi-
tion techniques of Event-B. Using the above framework, we present
models of MQTT, MQTT-SN and CoAP protocols, and verify com-
munication properties like connection-establishment, persistent-sessions,
caching, proxying, message ordering, QoS, etc. Our protocol models can
be integrated-with or extended-to other formal models of IoT systems
using machine-decomposition within Event-B, thus paving way for for-
mal modeling and veriﬁcation of IoT systems.
Keywords: IoT protocols · MQTT · MQTT-SN · CoAP · Formal mod-
eling and veriﬁcation · Event-B
1
Introduction
IoT is prevalent in various industries like health care, automotive, manufactur-
ing, power grid and domotics to name a few. IoT not only connects diﬀerent
computing devices but sensors, actuators, people and virtually any object. With
the prediction that there will be over 20 billion devices by 2020 [7], IoT will
be an integral part of our lives. The end nodes in the IoT are usually sensors
or small devices which have limited processing capability and low memory. In
such cases, the devices send unprocessed data to cloud which is then shared
with other devices/systems subscribing to this large amount of data (either raw
data or processed by server), making communication between these devices an
important aspect of IoT.
Various protocols are used for communication in an IoTsystem. TCP/IP is a
popular protocol used in lower layers. Several protocols are adapted for the appli-
cation layer in an IoT system - Message Queue Telemetry Transport Protocol
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 266–280, 2017.
https://doi.org/10.1007/978-3-319-69483-2_16

A Framework for Modeling and Verifying IoT Communication Protocols
267
(MQTT) [9], Message Queue Telemetry Transport Protocol Sensors (MQTT-
SN) [10], The Constrained Application Protocol (CoAP) [11], eXtensible Mes-
saging and Presence Protocol (XMPP) [12], Advanced Message Queuing Proto-
col (AMQP) [13] to name a few. Most of them are being used for IoT systems
as they are bandwidth eﬃcient, light-weight and have small code foot-print [8].
Features like publish-subscribe, messaging layer, QoS(Quality of Service) levels,
resource discovery, re-transmission, etc. are prevalent in these protocols.
Our framework for an IoT protocol modeling and veriﬁcation is realized
through an abstract model of the protocol. The abstract model consists of com-
monalities among various application layer protocols like communication modes,
connection establishment procedure, message layer, time tracking and attacker
modules. We then decompose these various modules and reﬁne them into more
concrete models for individual protocols. Properties that hold true for these
protocols are veriﬁed in these models. We use Event-B to model the commu-
nication channel and the client and server side communication entities, all of
which together implement the protocol. By verifying the accuracy of the model
through simulations, invariant checking and LTL properties satisﬁability, we are
able to conclude that our models of various protocols are correct.
Messages/streams are used as basic entities of communication between mul-
tiple clients and servers. Structure of a message apart from payload, usually
consists of many ﬁelds of various types. Event-B provides record datatypes [3]
through which complicated message structure with multiple attributes and sub-
attributes can be expressed succinctly. All the properties of the protocol to
be proved are expressed as invariants which are essentially predicates that are
always true. The automatic and interactive proof discharge in Event-B using
the Rodin tool [4] veriﬁes if these invariants (properties) are satisﬁed for all the
events in the model.
The paper is organized as follows. IoT protocols and their properties are
described in Sect. 2. Section 3 highlights the features of Event-B that we use for
modeling. Our Event-B model and their reﬁnements for diﬀerent protocols are
detailed in Sect. 4. Veriﬁed properties and their results are presented in Sect. 5.
Section 6 discusses related work and Sect. 7 presents the conclusion and on-going
work.
2
IoT Communication Protocols: MQTT, MQTT-SN
and CoAP
Most of the applications in IoT need a reliable network and use existing Internet
to communicate with the cloud/servers and with other nodes. Hence it is common
to use the existing TCP/IP stack with underlying physical, DLL, network and
transport layer. However TCP/IP is heavy weight as compared to a lighter UDP,
in which case reliability has to be built in the application layer protocol. Other
IoT communication requirements are: low bandwidth, low memory consumption,
small code foot-print, self recovery, resource discovery, light-weight, low message
overhead, low power consumption, authentication, security, appropriate QoS.

268
M. Diwan and M. D’Souza
We brieﬂy describe some of the application protocols highlighting the features
and properties which we verify in this paper.
2.1
MQTT
MQTT [9] is a publish-subscribe protocol designed for constrained devices con-
nected over unreliable, low bandwidth networks. It gives ﬂexibility to connect
multiple servers to multiple clients. The protocol has low message overhead which
makes it bandwidth eﬃcient and can be easily implemented on a low powered
device. Signiﬁcant features oﬀered by MQTT are explained below:
1. 3 levels of QoS: “At most once”- no acknowledgement is expected for a publish
message, “at least once” - every message receives an acknowledgement and
“exact once” - guaranteed message delivery without duplicates.
2. Subscribe: Clients can subscribe/unsubscribe to a topic with desired QoS.
3. Keep-alive: In absence of application messages within keep-alive time, client
sends a ping request to keep the network active.
4. Persistent Session: Persistent session is achieved by storing the session state
of channel and can be restored upon re-connection. It includes previous con-
ﬁgurations, subscriptions, unacknowledged messages, etc.
5. Retain Message: When a new subscriber or an oﬄine subscriber re-connects,
the retained message is immediately published with conﬁgured QoS.
6. Will message: Pre-conﬁgured “will” message is sent by the server when a
publishing client goes oﬄine and wants to inform the subscribing clients.
7. Authentication: A user-password feature is used for authentication. TLS
(Transport Layer Security) is optional for data encryption [9].
2.2
MQTT-SN
MQTT-SN is another data centric protocol and is based on MQTT with adapta-
tions to suit the wireless communication environment. Unlike MQTT, MQTT-SN
does not require an underlying network like TCP/IP making it a low complexity,
light weight protocol. Signiﬁcant diﬀerences between MQTT and MQTT-SN are
listed below:
1. Gateway Advertisement and Discovery: A MQTT-SN client conntects to
MQTT server via a gateway, implementing translation between the two pro-
tocols. A discovery procedure is used by the clients to discover the actual
network address of an operating server/gateway.
2. Topic Registration: To reduce bandwidth, a client can use pre-deﬁned short
topic names/IDs or register a long topic name with the server and use a
corresponding topic ID for further communication.
3. QoS −1: In addition to QoS 0, 1 and 2, MQTT-SN oﬀers QoS −1 where the
client communicates with the server without a formal connection establish-
ment and topic registration procedures.
4. Support of Sleeping Clients: Power saving clients can to go to sleep mode
and wake up periodically using keep-alive message. A server/gateway buﬀers
messages destined to the client and send them to client when they wake up.

A Framework for Modeling and Verifying IoT Communication Protocols
269
2.3
CoAP
CoAP is a specialized web transfer protocol based on REST architecture, ful-
ﬁlling Machine to Machine (M2M) requirements in constrained environments.
CoAP has low header overhead, parsing complexity, and has uri based address-
ing. It is stateless HTTP mapping, allowing proxies to be built providing access
to CoAP resources via HTTP. Following are signiﬁcant features of CoAP:
1. Layered Architecture: CoAP implements a request-response model with asyn-
chronous message exchanges at lower layer. The messaging layer deals with
UDP and asynchronous nature of interactions, and the request-response inter-
actions use method and response codes. Requests and responses are carried
in conﬁrmable and non-conﬁrmable messages. A response can be piggybacked
in acknowledgement or separate message.
2. Unicast/multicast requests: For discovering resources and services in the net-
work, CoAP uses multicast request. After a connection is established with a
server, unicast mode is used.
3. Reliability: CoAP uses a layer of messages that supports optional reliability
of “at least once” with an exponential back-oﬀmechanism.
4. Proxying and Caching: A cache could be located in an endpoint or an interme-
diary called proxy. Caching is enabled using freshness and validity information
carried with CoAP responses. A max-age option in a response indicates its
not fresh after its age is greater than the speciﬁed time. A proxy can however
validate the stored response with server even after max-age expiry.
5. Resource Discovery: Like MQTT-SN, CoAP uses multicast requests to dis-
cover services and resources in the network.
6. Observe feature: CoAP can be used in publish-subscribe mode by using
observe and notiﬁcation options.
7. Security: Optional security using Datagram Transport Layer Security
(DTLS).
3
Event-B
Event-B [1] is based on B-Method which provides a formal methodology for
system-level modeling and analysis. Event-B uses set theory as a modeling nota-
tion and ﬁrst order predicate calculus for writing axioms and invariants. It uses
step by step reﬁnement to represent systems at diﬀerent abstraction levels and
provides proofs to verify consistency of reﬁnements. Initially the model is con-
structed on basis of known requirements. As and when required, one can reﬁne
and add the new properties while satisfying the requirements in the underlying
model.
An Event-B model has two types of components: contexts and machines. Con-
texts contain all the data structures required for the system which are expressed
as sets, constants and relations over the sets. A machine “sees” a context to use
the data structures or types. A machine has several events and can also deﬁne

270
M. Diwan and M. D’Souza
Table 1. Comparison of IoT communication protocols
Sl.no Protocol feature
MQTT
MQTT-SN
CoAP
1
Architecture
Asynchronous
Message exchange
Asynchronous
Message exchange
REST architecture
Layered Approach
2
Transport Layer
TCP
Any
UDP
3
Communication type UniCast
UniCast/Multicast
UniCast/Multicast
4
Addressing
ClientID
Server address
ClientID
Server address
Uri Based
5
Messaging pattern
Publish Subscribe
Publish Subscribe
Request-Response
Publish-Subscribe
6
QoS Levels
AtmostOnce,
AtleastOnce,
ExactOnce
AtmostOnce,
AtleastOnce,
ExactOnce
AtmostOnce,
AtleastOnce
7
Persistent Session
Yes
Yes
Yes
8
Retained Message
/Oﬄine/Caching
Yes
Yes
Yes
9
Proxying/Caching
No
Yes
Yes
10
Resource Discovery
No
Yes
Yes
11
Sleep Mode
No
Yes
Yes
12
Security
Optional TLS
Optional TLS
Optional DTLS
variables and its types. A machine can reﬁne another machine to introduce new
events, reﬁne events, split events or merge events. An event consists of guards
which need to be satisﬁed before the actions in events are executed. When an
event is enabled and executed, the variables are updated as per the actions in
the event.
An invariant is a condition on the state variables that must hold permanently.
In order to achieve this, it is required to prove that, under the invariant in
question and under the guards of each event, the invariant still holds after being
modiﬁed according to the transition associated with that event [5].
Rodin and ProB
Rodin [4] implements Event-B and is based on Eclipse platform. It provides
an environment for modeling reﬁnements and discharges proofs. It has sophisti-
cated automatic provers like PP, ML and SMT, which automatically discharge
proofs for reﬁnements, feasibility, invariants and well-deﬁnedness of expressions
within guards, actions and invariants. Event-B also provides interactive prov-
ing mechanism for manual proofs which can be used when the automatic proof
discharge fails. Rodin oﬀers various plug-ins for development including diﬀerent
text editors, decomposition/modularization tools, simulator ProB, etc.
ProB [6] provides a simulation environment through animation for Event-B
model. A given machine can be simulated with all its events. In the animation
environment, one can select and run the given events by selecting parameters
or execute with random solution. During simulation, the state of the system
before and after every event execution can be observed. The state gives values
of all the variables in the machine, evaluates invariants, axioms and guards for

A Framework for Modeling and Verifying IoT Communication Protocols
271
all the events. Additionally any expression can be monitored in the animator.
The model can also be checked for deadlocks, invariant violations and errors in
the model which will help to construct an accurate model.
4
Protocol Modeling and Decomposition Using Event-B
A communication channel is a network connection which is established between
a client and a server or between two clients or between two servers. In an IoT
system there could be multiple channels connecting several clients and servers.
Our Event-B model consists of communication channels of the IoT system which
implement a communication protocol. As shown in the Fig. 1, the model has
Event-B contexts and machines. The contexts have all the data structures and
axioms required to setup a machine. The machine includes communication part
of client and server implemented as events, and the properties required to be
veriﬁed are written as invariants.
Communication Protocol
Event−B Model
 Context
Sets in the Context
Channels, Servers, Clients, Messages
Constants and Axioms in the Contexts
Message Attributes and properties
 Machine 
Events in the Machine
Guards of the Event
Attacker, Time Tracking
Transmit/Receive Messages, Error Event
Conditions for message transmission/
consumption,ErrorDetection ,Timer
Variables in the Machine
Buffers, Channel Properties, Timers
Protocol Properties for IoT
Invariants in the Machine
Fig. 1. Mapping between communication protocol and Event-B model
The protocol modeling is done in two major steps:
1. Building a common abstract model encompassing the common features of
various protocols.
2. Reﬁning this common abstract model into a concrete model of a particular
IoT protocol.
Our modeling is done using the techniques of machine decomposition [14], reﬁne-
ment [2] and atomicity decomposition in Event-B [15].

272
M. Diwan and M. D’Souza
4.1
Common Abstract Model
The common abstract model implements the commonalities among various pro-
tocols as mentioned in Table 1. Figure 2 is a diagrammatic representation of the
abstract model.
Context: A basic communication entity is modelled as a message. Set named
MSG and all its attributes are deﬁned as relations over the set MSG and the
sets deﬁned for the attributes. A projection function is used to extract the value
of an attribute for a given message [3].
Machine Reﬁnements: The atomicity of event Communication Channel is
broken into two events representing modes of communication: Unicast and
Broadcast/Multicast. Similarly a further reﬁnement of the model breaks down
the atomicity of these events into Service and Resource Discovery. A UniCast
event is broken into ChannelEstablishment and ChannelConversation events.
Since these events are not yet atomic, they can be further split as shown in
Fig. 3 where ChannelConversation of previous reﬁnement is further broken into
many more events. Figures 2 and 3 together show the three reﬁnement steps
done in the common abstract model. It is to be noted that our common abstract
model does not breakdown to the lowest atomic level of events. This is achieved
in the next step of building concrete model for a particular protocol.
Communication
Channel
Establishment
UniCast
MultiCast/
BroadCast
Channel
Channel
Conversation
Resource Discovery
Service Discovery
1st Refinement
2nd Refinement
Fig. 2. Atomicity decomposition of common abstract model
NonConfirmable
MessageSend
Confirmable
MessageSend
Timer
Increment
Timer
Intruder
Message
Acknowlege
Timeout
Detection
Conversation
Channel
Send
Message
3rd Refinement
4th Refinement
Fig. 3. Atomicity decomposition of ChannelConversation module

A Framework for Modeling and Verifying IoT Communication Protocols
273
Machine Decomposition: The leaves of the atomicity decomposition diagram
give us the events of the ﬁnal reﬁnement of the common model. Further on when
we build models of particular protocols, these events further explode into more
atomic events blowing up the size of the model. It has been observed that many of
these events have very few interfaces among them and they can be independently
be reﬁned. This allows us to use the technique of machine decomposition in
Event-B. Figure 4 gives such a decomposition of our abstract model. In Sect. 4.2
we give an example of how these modules of decomposed machines are further
reﬁned to give more concrete model of MQTT.
Fig. 4. Machine decomposition of common abstract model
Events in Decomposed Modules
1. Multicast/Broadcast: It is used when a node has to communicate to more
than one peer node. The Multicast/Broadcast event is broken down into
atomic events Service Discovery and Resource Discovery which are used to
ﬁnd the nodes that can publish the required information on the network. Once
the nodes with required resources/services are discovered, the information is
shared with ChannelEstablishment module.
2. ChannelEstablishment: The List of Resources/Services is used to establish
connection with the desired node. Events ConnectRequest and ConnectAc-
knowledgement are used for connection establishment. After the communica-
tion is over the connection can be disconnected to release the limited resources
through Disconnect event. Disconnect event is made convergent to avoid live
lock in the model. Error handling events detect errors and appropriately ter-
minate connections as per the session conﬁgurations. In our model, error
detection events are related to connection time-out and reconnecting an exist-
ing channel. Timeout error information is communicated through Timeout
interface with Timer module and the channelEstablished interface is shared
with ChannelConversation modules.

274
M. Diwan and M. D’Souza
3. ChannelConversation: This is a pseudo module which contains the Message-
Exchange, Timer and Intruder modules.
4. MessageExchange: This module includes all the application message transfer
events i.e., all the transmit/receive events for message send and acknowledge-
ment. These events update the message buﬀers and track time for message
transmission and reception.
5. Timers: There is a global time ticking through an event called “Timer” and
there are local timers maintained by client and server. These timers are incre-
mented when either there is a send event happening or to just delay time in
case of channel inactivity. Every transmission and reception event will store
the time at which each message was sent or received. Time tracking is used for
keep-alive mechanism, time-out handling and for verifying time related prop-
erties. In further reﬁnements of concrete protocols, timers can also be used
for strategies like exponential back oﬀin case of failed acknowledgement.
6. Intruder: This module is introduced to emulate disturbance in channel which
leads to loss of messages. A malicious Intruder event can consume any message
in the channel that is not yet received by the intended client or server. Intruder
can simulate attackers, connection drops, or any other disturbances in the
network that can lead to loss of the application message. This is a convergent
event and does not run forever.
4.2
Concrete Protocol Models
From the common abstract model, the decomposed machines are reﬁned further
to add details speciﬁc to a protocol. Some of the features which are not used
in the protocol need not be used or reﬁned. For example there is no broadcast
or multicast support in MQTT protocol. Hence this module does not need any
reﬁnement in MQTT model. The contexts from the abstract model are extended
to add detailed attributes. Channel variables and internal buﬀers are introduced
to track the dynamic behaviour of the channel that include messages in channel,
topics subscribed, payload counters, send and receive buﬀers, timers, conﬁgura-
tion settings, etc. Following is a detailed description of MQTT protocol model
created from the abstract model. We then brieﬂy describe the other two protocol
models (MQTT-SN and CoAP) which follow similar procedures.
MQTT Protocol Model: MQTT protocol is modeled by abstracting commu-
nication network in an IoT system consisting of two channels. For illustrative
purpose, we have modeled the channels with two servers and two clients.
ChannelEstablishment Module - From the abstract module containing events
ConnectRequest, ConnectAcknowledgement and Disconnect, MQTT speciﬁc
reﬁnement is done to include conﬁguration details and disconnection due to
errors. When a channel is established, the conﬁguration settings of the channel
communicated between the client and the server are stored in channel variables.
MessageExchange Module - First reﬁnement of the module introduces publish
and subscribe message with their acknowledgement events. These events are

A Framework for Modeling and Verifying IoT Communication Protocols
275
further reﬁned to send original message, duplicate message and reception of the
message at both client and server sides. Figure 5 gives the reﬁnement steps and
atomic decomposition for transmit messages in this module. Similar model is
built for acknowledgement messages.
Publish
QoS2
Publish
QoS1
Message
Release QoS2
Publish
Publish Qos2
Release Rcv
1st Refinement
2nd Refinement
Subscribe
Send
Subscribe
Send
Subscribe
Receive
.    .   .   .
Publish
QoS2 Orig
Publish
QoS2 Rcv
Publish
QoS2 Dup
Publish
QoS2 Orig
Client
Publish
QoS2 Rcv
Publish
QoS2 Rcv
Server
.    .   .   .
.    .   .   .
.    .   .
.    .   .
3rd Refinement
Fig. 5. Atomicity decomposition of conﬁrmable message transmission - QoS1 and QoS2
To track if the correct message is delivered with required QoS and time, the
“Payload” is implemented as a counter with a range of 0 to 9 which allows us
to uniquely identify every message transmitted. The range of the counter can be
extended to any number without aﬀecting our model. By keeping a track of how
many times the message with a given payload value is received, we can verify
interesting properties related to QoS, message ordering, retained message and
persistent sessions. Figure 6 is an example of the QoS0 Publish event transmitted
by an MQTT client. The guards ensure that a message of type publish with QoS0
is transmitted on the channel which is already established. In the actions, the
channel is populated with a new message carrying unique payload, ClientTimer
is initialized, direction of the message is set, PayloadCounter is incremented and
Timer-increment event is triggered.
Timer and Intruder modules - Timer Module is reﬁned to include ClientSide
and ServerSide Timer, and corresponding Timeout events. Intruder Module does
not have any particular reﬁnement for MQTT.
MQTT-SN Protocol Reﬁnement: MQTT-SN model reuses MessageEx-
change, ChannelEstablishment, Timer and Intruder Modules from MQTT. The
Multicast/Broadcast Module is reﬁned from common abstract model to add
events related to gateway discovery in the network using search gateway mes-
sages. New topic registration procedure is added to the ChannelConversation
Module.
CoAP Protocol Reﬁnement: ChannelConversation module from abstract
model is reﬁned to include request-response layer by adding events that are

276
M. Diwan and M. D’Souza
Fig. 6. Event for publishing message with QoS0
enabled to send a request and receive a corresponding response either piggy-
backed or separate. Each of these events then trigger the message layer events
to transmit conﬁrmable or non-conﬁrmable messages and receive correspond-
ing acknowledgements. Token ID matching and message ID matching is carried
out to ensure every request receives its response. ChannelEstablishment module
is reﬁned to add multi-hop connection consisting of multiple channels. Multi-
cast/Broadcast module is reﬁned to discover resources and services in the net-
work. Timer and Intruder modules are directly used from the common abstract
model.
4.3
Model Validation
ProB is used for validating our model through simulation of events and check-
ing LTL properties for common abstract model. Accuracy of the model can be
obtained by executing diﬀerent runs and observing the sequence of events and
variable values in each of these events. ProB also reports any invariant violation
or error in events which is then corrected in the model. Model validation is also
done by writing and verifying invariants.
5
Veriﬁcation of IoT Properties Using Event-B
Following are some of the signiﬁcant properties that are veriﬁed through the
model by writing them as invariants that have to be satisﬁed for all the events
in protocol speciﬁc models. The property invariant contains two parts well-
deﬁnedness expressions and the actual property to be proved. We omit the
well-deﬁnedness conditions and state only the actual property to be proved.
Properties 1 to 7 are veriﬁed in MQTT and MQTT-SN models and 8 to 11 are
veriﬁed in CoAP model.

A Framework for Modeling and Verifying IoT Communication Protocols
277
1. Message Ordering: If both client and server make sure that no more than one
message is “in-ﬂight” at any one time, then no QoS1 message will be received
after any later one. For example a subscriber might receive them in the order
1, 2, 3, 3, 4 but not 1, 2, 3, 2, 3, 4.
Refer to Sect. 4.6 in [9].
∀ch·∀pc1 · ∀pc2 · ((pc1 ∈0 · ·9 ∧pc2 ∈0 · ·9 ∧ch ∈establishChannel
∧(pc1 ∈Client MsgSentQoS2(ch) ∨pc1 ∈Client MsgSentQoS1(ch))
∧(pc2 ∈Client MsgSentQoS2(ch) ∨pc2 ∈Client MsgSentQoS1(ch))
∧(time > SendTRange(pc2) + Response Timeout)
∧pc1 ̸= pc2 ∧(SendTRange(pc1) < SendTRange(pc2))
⇒(RcvTRange(pc1) ≤RcvTRange(pc2))
(1)
2. Persistent Session: When a client reconnects with “CleanSession” set to 0,
both the client and server must re-send any unacknowledged publish pack-
ets (where QoS > 0) and publish release packets using their original packet
Identiﬁers. Refer to Normative Statement number MQTT-4.4.0-1 in [9]. The
variable RcvTRange is updated with current time only after the message is
received. Hence it should be greater than the SendTRange time.
∀ch · ∀pc · ((pc ∈0 · ·9 ∧ch ∈establishChannel
∧Channel CleanSess(ch) = FALSE
∧((pc ∈Client MsgSentQoS1(ch)) ∨(pc ∈Client MsgSentQoS2(ch))
∧(time > (SendTRange(pc) + Response Timeout))
⇒(RcvTRange(pc) > SendTRange(pc)))
(2)
3. QoS of a message from Client1 to Client2: The eﬀective QoS of any message
received by the subscriber is minimum of QoS with which the publishing client
transmits this message and the QoS set by the subscriber while subscribing
for the given topic. E.g. Publishing client sends with QoS1 oand subscribing
client has subscribed with QoS2, with eﬀective QoS being 1. Refer to Sect.
4.3 in [9].
∀ch · ∀pc · ∀chnl · ∀msg · ((pc ∈0 · ·9 ∧ch ∈establishChannel
∧msg ∈MSG ∧chnl ∈establishChannel
∧(pc ∈Client MsgSentQoS1(ch)
∧(msg 	→((PUBLISH 	→AtleastOnce) 	→pc)) ∈Msg Type QoS
∧((Msg Topic(msg) 	→ExactOnce) ∈Channel TopicQoS(chnl))
∧((time −SendTRange(pc)) ⩾Response Timeout)))
⇒(∃QC · ((QC ≥1) ∧Client MsgReceived 2(chnl) = QC)))
(3)
4. Exponential Backoﬀ: The sender retransmits the Conﬁrmable message at
exponentially increasing intervals, until it receives an acknowledgement or
runs out of attempts. Refer to Sect. 4.2 in [11].

278
M. Diwan and M. D’Souza
∀ch · ∀pc · ((pc ∈0 · ·11 ∧ch ∈establishChannel ∧pc ∈MsgSent(ch)
∧RetransmissionCounter(pc) ⩾Max Retransmit(ch)
⇒((SendTRange(pc) −SendTPrev(pc)) ≤Ack Timeout(pc)
∧(SendTRange(pc) −SendTPrev(pc)) > 0))
(4)
5.1
Proof Obligations Results
Our validated models of MQTT, MQTT-SN and CoAP have together discharged
1840 proof obligations, of which 88% proof obligations were automatically dis-
charged through AtlierB, SMT, PP and ML provers. The proof obligations
include well-deﬁnedness of predicates and expressions in invariants, guards,
actions, variant and witnesses of all the events, feasibility checks, variable re-
use check, guard strengthening and witness feasibility in reﬁnements, variant
checks for natural number and decreasing variants for convergent and antici-
pated events, theorems in axioms and invariant preservation for reﬁnements and
invariants used for veriﬁcation of required properties. About 30% of proofs dis-
charged in the models are for veriﬁcation of properties written as invariants.
Table 2 gives a summary of the properties veriﬁed.
Table 2. Proof obligation statistics for veriﬁed properties of IoT protocols
Sl.no Protocol property
Proof obligations Result
1
Duplicate Channel
10
Passed
2
Message Ordering
34
Passed
3
Persistent Session
34
Passed
4
QoS1 in single channel
26
Passed
5
QoS2 in single channel
26
Passed
6
Retained QoS1 message
24
Passed
7
Retained QoS2 message
24
Passed
8
Eﬀective QoS0 in Multi channel(3 cases)
66
Passed
9
Eﬀective QoS1 in Multi channel(3 cases)
66
Passed
10
Eﬀective QoS2 in Multi channel(3 cases)
72
Passed
11
Request-Response Matching and Timeout
39
Passed
12
Conﬁrmable Message ID Matching and Timeout 39
Passed
13
Exponential Backoﬀ
39
Passed

A Framework for Modeling and Verifying IoT Communication Protocols
279
6
Related Work
Communication protocols for IoT have been used for over a decade now, but
there has been no attempt to provide formal semantics for these protocols. A
recent paper shows that there are scenarios where MQTT has failed to adhere
to the QoS requirement [16]. However the paper is limited to partial model of
MQTT protocol for QoS properties. In another work, a protocol used for IoT -
Zigbee is veriﬁed for properties related to connection establishment proper-
ties [17] using Event-B. In [19] and [20], the authors give methods to evaluate
performance of MQTT protocol with regards to diﬀerent QoS levels used and
compare with other IoT protocol CoAP. In [18] the author again tests connection
properties using passive testing for XMPP protocol in IoT.
We diﬀer from the above mentioned approaches by proposing a framework
comprising of a common model for IoT protocols which can be used to build
models of diﬀerent IoT protocols. These models verify properties required for
IoT like connection establishment, persistent sessions, retained-message trans-
mission, will messages, message ordering, proxying, caching and QoS and pro-
vide proof obligations for these properties through automatic proof discharge
and interactive proof discharge methods.
7
Conclusion and Future Work
In this paper we have proposed a framework using Event-B to model IoT proto-
cols. We then have used this framework and went on to model some of the widely
used IoT protocols viz., MQTT, MQTT-SN and CoAP. Through simulation and
proof obligation discharge in Rodin, we have formally veriﬁed that the properties
related to QoS, persistent session, will, retain messages, resource discovery, two
layered request-response architecture, caching, proxying and message deduplica-
tion. We show that the protocols work as intended in an uninterrupted network
as well as with an intruder which consumes messages in the network. The three
protocols modeled in this paper implement simple mechanisms to provide reliable
message transfer over a lossy network. They are also able to reduce overhead by
providing features like persistent connections, retain messages, caching and prox-
ying which are essential for IoT systems. Our work is a stepping stone towards
providing formal semantics of IoT protocols and systems.
Future research would focus on modeling the other aspects of protocols like
security, user authentication, encryption and diﬀerent attacker modules. We
would also like to move veriﬁcation of more properties from the concrete pro-
tocol models to the common abstract model. We would like to further compare
other protocols for IoT like AMQP and XMPP by modeling them using our
framework. It would also be interesting to integrate the protocol model into an
existing model of IoT system and verify the properties required at the system
level.

280
M. Diwan and M. D’Souza
References
1. Event-B. http://www.Event-B.org/
2. Abrial, J.R.: Modeling in Event-B: System and Software Engineering. Cambridge
University Press, Cambridge (2010)
3. Evans, N., Butler, M.: A proposal for records in Event-B. In: Misra, J., Nipkow, T.,
Sekerinski, E. (eds.) FM 2006. LNCS, vol. 4085, pp. 221–235. Springer, Heidelberg
(2006). doi:10.1007/11813040 16
4. Rodin Tool. http://wiki.Event-B.org/index.php/Rodin Platform
5. Rodin Hand Book. https://www3.hhu.de/stups/handbook/rodin/current/pdf/
rodin-doc.pdf
6. ProB tool. https://www3.hhu.de/stups/prob/index.php/Main Page
7. Gartner newsroom. http://www.gartner.com/newsroom/id/3165317
8. Karagiannis, V., Chatzimisios, P., Vazquez-Gallego, F., Alonso-Zarate, J.: A survey
on application layer protocols for the internet of things. Trans. IoT Cloud Comput.
3(1), 11–7 (2015)
9. MQTT Ver. 3.1.1. http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.
1-os.html
10. MQTT-SN Ver. 1.2. http://mqtt.org/new/wp-content/uploads/2009/06/MQTT-
SN spec v1.2.pdf
11. The Constrained Application Protocol (CoAP) RFC7252. https://tools.ietf.org/
html/rfc7252
12. Extensible Messaging and Presence Protocol (XMPP) Core RFC6120. http://
xmpp.org/rfcs/rfc6120.html
13. Advanced Message Queuing Protocol ver. 1.0. http://docs.oasis-open.org/amqp/
core/v1.0/amqp-core-complete-v1.0.pdf
14. Pascal, C., Renato, S.: Event-B model decomposition, DEPLOY Plenary Technical
Workshop (2009)
15. Salehi Fathabadi, A., Butler, M., Rezazadeh, A.: A systematic approach to atom-
icity decomposition in Event-B. In: Eleftherakis, G., Hinchey, M., Holcombe, M.
(eds.) SEFM 2012. LNCS, vol. 7504, pp. 78–93. Springer, Heidelberg (2012). doi:10.
1007/978-3-642-33826-7 6
16. Aziz, B.: A formal model and analysis of the MQ telemetry transport protocol. In:
Ninth International Conference, Availability, Reliability and Security (ARES), pp.
59–68. Fribourg (2014)
17. Gawanmeh, A.: Embedding and veriﬁcation of ZigBee protocol stack in Event-B.
In: Procedia Computer Science, vol. 5, pp. 736–741. ISSN 1877–0509 (2011)
18. Che, X., Maag, S.: A passive testing approach for protocols in Internet of Things.
In: Green Computing and Communications (GreenCom), IEEE and Internet of
Things (iThings/CPSCom), IEEE International Conference on and IEEE Cyber,
Physical and Social Computing, pp. 678–684. IEEE Press (2013)
19. Lee, S., Kim, H., Hong, D.K., Ju, H.: Correlation analysis of MQTT loss and
delay according to QoS level. In: The International Conference on Information
Networking(ICOIN), pp. 714–717. IEEE (2013)
20. Thangavel, D., Ma, X., Valera, A., Tan, H.X., Tan, C.K.: Performance evaluation
of MQTT and CoAP via a common middleware. In: IEEE Ninth International Con-
ference, Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP),
pp. 1–6. IEEE Press (2014)

Formalization

Formal Analysis of Information Flow in HOL
Ghassen Helali1(B), Soﬁ`ene Tahar1, Osman Hasan1, and Tsvetan Dunchev2
1 Electrical and Computer Engineering, Concordia University, Montreal, Canada
{helali,o hasan}@encs.concordia.ca,
tahar@ece.concordia.ca
2 Computer Science and Engineering, University of Bologna, Bologna, Italy
tsvetan.dunchev@unibo.it
Abstract. Protecting information has become very important due to
the safety-critical nature of many computer-based applications. Informa-
tion ﬂow analysis plays a very important role in quantifying information-
related properties under external attacks. Traditionally, information ﬂow
analysis is performed using paper-and-pencil based proofs or computer
simulations but due to their inherent nature, these methods are prone
to errors and thus cannot guarantee accurate analysis. As an accurate
alternative, we propose to conduct the information ﬂow analysis within
the sound core of a higher-order-logic theorem prover. For this purpose,
some of the most commonly used information ﬂow measures, including
Shanon entropy, mutual information, min-entropy, belief min-entropy,
have been formalized. In this paper, we use the Shannon entropy and
mutual information formalizations to formally verify the Data Process-
ing and Jensen’s inequalities. Moreover, we extend the security model for
the case of the partial guess scenario to formalize the gain min-entropy.
These formalizations allow us to reason about the information ﬂow of a
wide range of systems within a theorem prover. For illustration purposes,
we perform a formal comparison between the min-entropy leakage and
the gain leakage.
Keywords: Information ﬂow · Entropy · Gain function · g-Leakage ·
Theorem proving · Higher-order logic
1
Introduction
Information ﬂow analysis mainly consists of using information measures to evalu-
ate the amount of information an attacker could get by observing the low output
of a system or a protocol. Examples of this analysis include the evaluation of
anonymity protocols [27] and security networks [31]. Protecting the conﬁdential-
ity of sensitive information and guaranteeing a perfect level of anonymity are
increasingly being required in numerous ﬁelds such as electronic payments [17],
auctioning [32] and voting [7].
Various techniques for analyzing the information ﬂow have been used. The
possibilistic approach [3] consists of using non-deterministic behaviors to model
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 283–299, 2017.
https://doi.org/10.1007/978-3-319-69483-2_17

284
G. Helali et al.
the given system. Information ﬂow analysis based on epistemic logic [11] and
process calculi [28] fall into the category of possibilistic analysis. This approach
is limited in terms of distinguishing between systems of varying degrees of pro-
tection [10]. As a solution for this limitation, probabilistic approaches, based on
information and statistics, are considered as a more reliable alternative for com-
puting information ﬂow. In a threat model where the secret should be guessed
in one try, the main objective of the attacker is to maximize the probability
of guessing the right value of the high input (secret), in one try, by betting on
the most probable element. To cater for this particular threat model, Renyi’s
entropy metrics [26], i.e., min-entropy and belief min-entropy are employed [30].
These measures are commonly used to eﬀectively reason about deterministic and
probabilistic systems.
Due to the diﬃculty of preventing the information leakage completely,
“small” leaks are usually tolerated [19,29] by the above-mentioned information
ﬂow measures. With respect to the partial guess, g-leakage [2] is introduced as
a generalization of the min-entropy model. The main idea of this notion is to
extend the vulnerability in order to take into consideration the so called gain
function g. The gain function models the proﬁt that an attacker gets by using a
certain guess z over the secret x. The gain value ranges from 0, when the guess
has no corresponding secret value, to 1, in the case of an ideal guess. Hence the
vulnerability (g-vulnerability) is redeﬁned as the maximum expected gain over
all possible guesses [2].
Traditionally, the quantitative analysis of information ﬂow has been con-
ducted using paper-and-pencil and computer simulation. The paper-and-pencil
technique cannot cope with complex systems due to the high chances of human
error while dealing with large models. On the other hand, the computer simula-
tion approach cannot be considered accurate due to the use of numeric approx-
imations. In order to overcome those shortcomings, formal methods [12] have
been proposed as a sound technique to enhance accuracy of safety-critical sys-
tems. For instance, in [19], the probabilistic mode checker PRISM has been used
to reason about several information systems, e.g., the Dining Cryptographers
protocol. However, the state-space explosion problem of model checking limits
the scope of its usage in information ﬂow analysis. In contrast, higher-order-logic
theorem proving can be used for the analysis of information ﬂow to overcome
these limitations.
In [8], Coble has formalized the conditional mutual information in the
higher-order-logic theorem prover HOL4 [1] based on the Lebesgue integra-
tion. These fundamentals have been later used to formally analyze the privacy
and the anonymity guarantees and proposed the Dining Cryptographers. How-
ever, Coble’s formalization of Lebesgue integrals can only consider ﬁnite-valued
measures, functions and integrals. Considering this fact, Mhamdi et al. [22] gen-
eralized the formalizations of the probability and information theories by intro-
ducing the notions of extended real numbers and formalizing Borel sigma algebra
that covers larger classes of functions in terms of integrability and conver-
gence. The authors further used these fundamentals to formalize the measures

Formal Analysis of Information Flow in HOL
285
of entropy, relative entropy and mutual information [9]. In the same context,
information and conditional information leakage degree have been formalized
[23] in HOL4 to assess security and anonymity protocols. Similarly, H¨olzl [15,16]
formalized a generic version of the measure, probability and information theories
in Isabelle/HOL. This deﬁnition is very similar to Coble’s work. H¨olzl used the
measure and the probability theories to deﬁne the Kullback-Leibler divergence,
entropy, conditional entropy, mutual information and conditional mutual infor-
mation and verify the properties related to the quantiﬁcation of the information
represented by a random variable [24].
Most of our work is based on the probability and information theories, for-
malized in Mhamdi’s work [21], due to their completeness and availability in
HOL4. We previously used these fundamentals to develop formal reasoning sup-
port for information ﬂow using min-entropy and belief min-entropy [14], which
we are extending to the gain min-entropy (g-leakage), which considers the model
where the secret is totally guessed based on a partial gain about the secret
using a certain guess. We will also use the formalized information measures in
[23] to conduct the formal veriﬁcation of the Data Processing [4] and Jensen’s
[18] inequalities which are major properties in information ﬂow analysis. In the
information ﬂow context, the Data Processing Inequality (DPI) states that any
post-processing of data does not increase the information leakage while Jensen’s
Inequality shows a relation between data averaging and data processing.
To the best of our knowledge, these measures have not been formalized before.
We apply them to conduct an information leakage analysis of a threat scenario
to compare min-entropy leakage and g-leakage (based on gain min-entropy) and
since a small/partial leak can be tolerated, we show that the min-entropy leakage
can be arbitrarily greater than the g-leakage.
2
Preliminaries
This section describes the HOL4 environment as well as the formalization of
probability and information theories, which we would be building upon to for-
malize the DPI and Jensen’s Inequality as well as the gain Min-Entropy notions.
2.1
HOL Theorem Prover
The HOL system is an environment for interactive theorem proving in higher
order logic. Higher-order logic is a system of deduction with a precise semantics
and is expressive enough to be used for the speciﬁcation of almost all classical
mathematics theories. In order to ensure secure theorem proving, the logic in
the HOL system is represented in the strongly-typed functional programming
language ML. An ML abstract data type is used to represent higher-order-logic
theorems and the only way to interact with the theorem prover is by executing
ML procedures that operate on values of these data types.

286
G. Helali et al.
Soundness is assured as every new theorem must be veriﬁed by applying
the basic axioms and primitive inference rules or any other previously veriﬁed
theorems/inference rules. The HOL system has been used to formalize pure
mathematics and verify industrial software and hardware systems.
2.2
Probability and Information Theory
Probability and information theories provide mathematical models to evaluate
the uncertainty of random phenomena. These concepts are commonly used in
diﬀerent ﬁelds of engineering and computer sciences, such as signal process-
ing, data compression and data communication, to quantify the information.
Recently, the probability and information theories have been widely used for
cryptographic and information ﬂow analysis [29]. Some foundational notions of
these formalizations are described below.
Let X and Y denote discrete random variables, with x and y and X and Y
denoting their speciﬁc values and set of all possible values, respectively. Similarly,
the probabilities of X and Y being equal to x and y is denoted by p(x) and p(y),
respectively.
– Probability Space: a measure space such that the measure of the state space
is 1.
– Independent Events: Two events X and Y are independent iﬀp(X ∩Y ) =
p(X)p(Y ).
– Random Variable: X : Ω →R is a random variable iﬀX is ( F, B(R))
measurable, where Ω is the state space, F denotes the set of events and B is
the Borel sigma algebra of real valued functions.
– Joint Probability: A probabilistic measure where the likelihood of two events
occurring together and at the same point in time is calculated. Joint probability
is the probability of event Y occurring at the same time event X occurs. It is
mathematically expressed as p(X ∩Y ) or p(X, Y ).
– Conditional Probability: A probabilistic measure where an event X will
occur, given that one or more other events Y have occurred. Mathematically
p(X|Y ) or p(X∩Y )
p(Y ) .
– Expected Value: E[X] of a random variable X is its Lebesgue integral with
respect to the probability measure. The following properties of the expected
value have been veriﬁed in HOL4 [22]:
1. E[X + Y ] = E[X] + E[Y ]
2. E[aX] = aE[X]
3. E[a] = a
4. X ≤Y then E[X] ≤E[Y ]
5. X and Y are independent then E[XY ] = E[X]E[Y ]
– Variance and Covariance: Variance and covariance have been formalized in
HOL4 using the formalization of expectation. The following properties have
been veriﬁed [22]:
1. V ar(X) = E[X2] −E[X]2
2. Cov(X, Y ) = E[XY ] −E[X]E[Y ]

Formal Analysis of Information Flow in HOL
287
3. V ar(X) ≥0
4. ∀a ∈R, V ar(aX) = a2V ar(X)
5. V ar(X + Y ) = V ar(X) + V ar(Y ) + 2Cov(X, Y )
The above-mentioned deﬁnitions and properties have been utilized to formalize
the foundations of information theory in HOL4 [22]. The widely used information
theoretic measures can be deﬁned as:
– The Shannon Entropy: It measures the uncertainty of a random variable
H(X) = −

x∈X
p(x)log p(x)
– The Conditional Entropy: It measures the amount of uncertainty of X when
Y is known
H(X|Y ) = −

y∈Y
p(y)

x∈X
p(x|y)log p(x|y)
– The Mutual Information: It represents the amount of information that has
been leaked
I(X; Y ) = I(Y ; X) = H(X) −H(X|Y )
– The Relative Entropy or Kullback Leiber Distance: It measures the inaccuracy
or information divergence of assuming that the distribution is q when the true
distribution is p
D(p∥q) =

x∈X
p(x)log p(x)
q(x)
– The Guessing Entropy: It measures the expected number of tries required to
guess the value of X optimally
G(X) =

1≤i≤n
ip(xi)
– The R´enyi Entropy: It is related to the diﬃculty of guessing the value of X
Hα(X) =
1
1 −α log (

x∈X
P[X = x]α)
Among the measures listed above, Mhamdi [21] and Coble [8] formalized the
Entropy, Conditional Entropy, Relative Entropy and Mutual Information in
HOL4 and H¨olzl [15] formalized similar concepts in Isabelle/HOL.
3
Shannon Based Information Flow
In this section, we will use the most common measures to quantify information
ﬂow, such as Shannon entropy, related entropy and mutual information formal-
ized in [23] to formally verify the Data Processing Inequality as well as Jensen’s
Inequality properties.

288
G. Helali et al.
3.1
Data Processing Inequality
According to the Data Processing Inequality (DPI), post-processing cannot
increase information. Quantitatively, considering three random variables X, Y
and Z satisfying the Markov property [5], the DPI states that Z cannot have
more information about X than Y has about X; which is
I(X, Z) ≤I(X, Y )
Our formalization is based on the Discrete Time Markov Chain formalization
(DTMC) [20], formalized information measures and probability theory [21].
The motivation behind this deﬁnition relies on the fact that the three random
variables X, Y and Z satisfy the Markov property and thus
p(x, y, z) = p(x).p(y, z|x) = p(x).p(y|x).p(z|x, y)
Similarly, we can also deduce that
p(z|x, y) = p(z|y)
In order to formally verify the DPI, we ﬁrst formalized the conditional mutual
information
Deﬁnition 1. (Conditional Mutual Information)
For discrete random variables X, Y , and Z, conditional mutual information is
deﬁned as
I(X; Y |Z) =

z∈Z

y∈Y

x∈X
PX,Y,Z(x, y, z) log Pz(Z).PX,Y,Z(x, y, z)
PX,Z(x, z).PY,Z(y, z)
= H(X, Z) + H(Y, Z) −H(X, Y, Z) −H(Z)
= H(X|Z) −H(X|Y, Z)
Then, using the commutativity of the distribution function which says that
PY,Z((y, z))
=
PZ,Y ((z, y)), we get the following equality: I(X; Y, Z)
=
I(X; Z, Y ) Therefore, the following result can be deduced:
I(X; Y |Z) + I(X; Z) = I(X; Y, Z) = I(X; Z, Y ) = I(X; Z|Y ) + I(X; Y )
Theorem 1. (Symmetry of Mutual Information Property)
⊢∀b p X Y Z.
(POW (p_space p) = events p) ∧prob_space p ∧
random_variable X p s1 ∧random_variable Y p s2 ∧
random_variable Z p s3 ∧random_variable (λ x.(Z x, Y x)) p s32 ∧
FINITE (p_space p) ∧
(mutual_information b p s1 s2 X Y ̸= −∞∧
mutual_information b p s1 s2 X Y ̸= +∞∧
mutual_information b p s1 s3 X Z ̸= −∞∧

Formal Analysis of Information Flow in HOL
289
mutual_information b p s1 s3 X Z ̸= +∞) ⇒
conditional_mutual_information b p s1 s3 s2 X Z Y +
mutual_information b p s1 s2 X Y =
conditional_mutual_information b p s1 s2 s3 X Y Z +
mutual_information b p s1 s3 X Z
where POW and FINITE refer to the power set operator and ﬁniteness tester in
HOL4 respectively.
The proof of the property above relies on the associativity of the joint dis-
tribution, namely P(X, (Y, Z)) = P(X, (Z, Y )) as well as the symmetry of the
additivity. Now we formally verify our main goal, DPI, as follows
Theorem 2. (Data Processing Inequality: DPI)
For all random variables X, Y and Z satisfying the Markov property, the DPI
states that I(X; Z) ≤I(X; Y )
which is formalized in HOL as follows:
⊢∀b p X Y Z.(POW (p_space p) = events p) ∧prob_space p ∧
random_variable X p s1 ∧random_variable Y p s2 ∧
random_variable Z p s3 ∧
random_variable (λ x.(Y x, Z x)) p s23 ∧
random_variable (λ x.(Z x, Y x)) p s32 ∧
FINITE (p_space p) ∧
mc p X Y Z∧
(mutual_information b p s1 s2 X Y ̸= −∞∧
mutual_information b p s1 s2 X Y ̸= +∞∧
mutual_information b p s1 s3 X Z ̸= −∞∧
mutual_information b p s1 s3 X Z ̸= +∞) ⇒
mutual_information b p s1 s2 X Y ≥
mutual_information b p s1 s3 X Z
where mc p X Y Z denotes the Markov property and the assertions related to
the mutual information are constraints to avoid the inﬁnite bounds of the infor-
mation leakage.
For proving this theorem, we ﬁrst need to prove the following two properties:
– ∀X, Y random variables X and Y , the mutual information between X and Y
is non-negative, I(X; Y ) ≥0
– if X, Y and Z form a Markov chain, then I(X; Z|Y ) = 0
Applying the above properties to the equality:
I(X; Y |Z) + I(X; Z) = I(X; Z|Y ) + I(X; Y )
as well as the previously veriﬁed property which states
I(X; Y, Z) = I(X; Z, Y )
our result can be proved.

290
G. Helali et al.
The above result states that any transformation of the output channel Y will
not give more information about the input X than itself. This concept also states
that the information content of a signal cannot be increased via a local physi-
cal operation: post-processing cannot increase information. The main challenges
of proving this result in HOL is to use the formalized notions of probability
and information theories and reason about one of the major applications of the
information theory. By proving the DPI, we show the usefulness of the theoretic
information framework formalized in HOL.
3.2
Jensen’s Inequality
Jensen’s inequality has applications in many ﬁelds of applied mathematics and
speciﬁcally information theory. For example, it plays a key role in the proof
of the information inequality, 0 ≤D(p||q). In the following, we prove Jensen’s
inequality in its measure theoretic form as an application for information theory
formalized in HOL. We ﬁrst formalize in HOL4 the notion of convex functions:
Deﬁnition 2. (Convex function)
⊢conv_func = ∀x y z. (x<y ∧y<z) ⇒
((f(y)-f(x))/(y-x) ≤(f(z)-f(y))/(z-y))
Now, let Ω be a probability space, μ is a measure funcion on Ω, and g and f be
arbitrary convex functions on the real numbers, respectively. Then according to
Jensen’s inequality:

Ω
f(g(x)) dμ ≥f(

Ω
g(x) dμ).
The most challenging part of the proof of Jensen’s inequality is to prove the
existence of subderivatives a and b of f, such that for all x, a.x + b ≤f(x), where
for x0 =

Ω
g(x) dμ we reach the equality a.x0 + b = f(x0). This follows from the
following two facts:
– According to the Mean value theorem, there exists ν such that if x < ν < ξ,
then: f(x)−f(ξ)
x−ξ
= f ′(ν)
– Since f is convex, then its derivative increases, i.e. f ′(ν) ≤f ′(ξ)
Having a and b, the proof of Jensen’s inequality is straightforward:

Ω
f(g(x)) dμ ≥

Ω
(a.g(x) + b) dμ
≥a.

Ω
g(x) dμ + b.

Ω
1 dμ
≥a.x0 + b
≥f(x0)
= f(

Ω
g(x) dμ)
Since μ is a measure, it holds that μ(Ω) = 1. Therefore

Ω
1 dμ = 1.
Using the monotonicity of sub-derivatives and the existence of a convex func-
tion properties, we formalize Jensen’s inequality for the continuous case:

Formal Analysis of Information Flow in HOL
291
Theorem 3. (Jensen’s Inequality)
⊢∀f g m. measure_space m ∧integrable m g ∧(b = integral m λy.b)∧
(a *(integral m λx.g(x)) + b = f((integral m λx. g(x)))) ∧
(∀x.a * x + b ≤f x) ⇒
f (integral m λx.g(x)) ≤integral m λx.(f(g(x)))
The result veriﬁed above is a relation between the integral of a convex function
and the value of a convex function of an integral. In the information theoretic
context, Jensen’s inequality relates the averaging of data to the transformation
of data. This result is then formally veriﬁed in HOL4.
4
Partial Guess, Gain Function and g-Leakage
In this section, we analyze the threat scenarios where the secret is totally guessed
in one try by using min-entropy measures [14]. This model is extended with the
presence of the attacker’s belief leading to the concept of the belief min-entropy
[14]. Since the guess of the sensitive information can be partial, we formalize the
gain function and the gain min-entropy. We ﬁrst start by the formalization of
the gain function and the related leakage properties. Compared to min-entropy
and belief min-entropy measures, where the secret is assumed to be guessed in
one try, the new model assumes that the secret can be partially guessed. We
then introduce the notion of gain functions, which range from 0 to 1 and operate
over a guess z and a secret x. Then g(z, x) models the gain that an attacker gets
about the secret x using the guess z.
Deﬁnition 3. (Gain Function)
Given a set X of possible secrets and a ﬁnite and non-empty set of guesses Z, a
gain function is deﬁned as: g : Z × X →[0, 1]
For the rest of the paper, Hg, Vg and ILg will respectively denote gain min
entropy (also called as g-min-entropy), (prior/posterior)gain vulnerabilty (also
called g-vulnerability) and gain information leakage (also called g-leakage).
Based on the gain function, we deﬁne the prior vulnerability:
Deﬁnition 4. (Prior g-Vulnerability)
Given a gain function g, a random variable X modeling the a-priori behavior,
the prior g-vulnerability is
Vg(X) = max
z∈Z

x∈X
p(X = x).g(z, x)
which is formalized in HOL4 as follows
⊢∀p X g Z. prior_g_vulnerability = extreal_max_set
(IMAGE (λz.

x∈X
distribution p X {x}.g(z,x))
(IMAGE X (p_space p)) Z)

292
G. Helali et al.
where IMAGE f s in HOL denotes the image of the set s by the function f
which in our case is X(Ω) and extreal max set (IMAGE f s) refers to the
max of the set IMAGE f s which in our case is the maximum probability over
the distributions set.
Compared to the previous deﬁnition of vulnerability, the above deﬁnition
shows that the gain is weighted by the probability of the secret itself, which
means that the adversary A tries to make a guess maximizing the gain about
every x from X.
Deﬁnition 5. (Posterior g-Vulnerability)
Given a gain function g, a high input behavior modeled by the random variable
X and a low output modeled by Y , the posterior g-vulnerability is
Vg(X|Y ) =

y∈Y
max
z∈Z

x∈X
p(X = x).p(Y = y|X = x).g(z, x)
=

y∈Y
max
z∈Z

x∈X
p(X = x, Y = y).g(z, x)
This deﬁnition can be formalized in HOL4 as
⊢∀p X Y g Z. posterior_g_vulnerability =

y∈Y
extreal_max_set(IMAGE (λz.

x∈X
distribution p Y {y}.
conditional_distribution p X Y ({x},{y}).g(z,x))
(IMAGE X (p_space p)))
Now we deﬁne the uncertainty measures; g-min-entropy (initial uncertainty),
and conditional g-min-entropy, (remaining uncertainty) which will be used to
deﬁne the g-leakage.
Deﬁnition 6. (g-Min-Entropy, g-Conditional-Min-Entropy and g-Leakage)
⊢g_min_entropy p X g Z = -log(prior_g_vulnerability p X g Z)
⊢g_conditional_min_entropy p X Y g Z =
-log(posterior_g_vulnerability p X Y g Z)
⊢g_information_leakage p X Y g Z =
g_min_entropy p X g Z - g_conditional_min_entropy p X Y g Z
We next consider a model where the attacker can get partial knowledge about
the secret using a certain guess. The gain function models the beneﬁt that the
attacker gets about the secret. We then verify that the prior g-vulnerability
cannot exceed the posterior g-vulnerability. Thus the g-leakage is positive:
Theorem 4. (Positive g-Leakage)
⊢∀p X Y g Z. prob_space p ∧FINITE (p_spac p) ∧FINITE Z ∧
Z ̸= ∅∧p_space p ̸= ∅∧
∀x. x ∈p_space p ⇒
{x} ∈events p ∧events p = POW(p_space p) ∧
∀x z. 0 ≤g(z, x) ∧g(z, x) ≤1⇒
0≤g_information_leakage p X Y g Z

Formal Analysis of Information Flow in HOL
293
Proof. First, note that the gain information leakage (g-leakage) is ILg
=
Hg(X) −Hg(X|Y ) = log(Vg(X|Y )) −log(Vg(X)). After simpliﬁcation, our goal
will be reduced to Vg(X) ≤Vg(X|Y ). Then
Vg(X) = max
z

x

y
P(X = x, Y = y).g(z, x)
≤

y
max
z

x
P(X = x, Y = y).g(z, x)
≤Vg(X|Y )
Next, we will study the case when the g-leakage is equal to zero. We will eval-
uate the condition under which this result occurs. Before stating this property
formally we need ﬁrst to deﬁne the notion of the expected gain of a guess z.
With respect to the same conﬁguration, the prior and posterior expected gains
are deﬁned as:
Deﬁnition 7. (Prior Expected Gain)
Eg(z) =

x
P(X = x).g(z, x)
which is formalized in HOL4 as follows
⊢∀p X g z. prior_expected_gain p X g Z =

x∈X(Ω)
distribution p X {x}.g(z,x)
Deﬁnition 8. (Posterior Expected Gain)
Given an output y the expected gain of a guess z is
Eg(z, y) =

x
P(X = x)P(Y
= y | X = x).g(z, x)
The HOL4 formalization of this deﬁnition is
⊢∀p X Y y g z. posterior_expected_gain p X Y y g z =

x∈X(Ω)
distribution p X {x}.
conditional_distribution p Y X ({y},{x}).g(z,x)
In the context of vulnerabilities and information ﬂow, these deﬁnitions satisfy
the following properties:
Theorem 5. (Expected Gain and Vulnerabilities)
– Vg(X) = max
z
Eg(z)
– Vg(X | Y) =

y
max
z
Eg(z,y)
– Eg(z) =

y
Eg(z, y)

294
G. Helali et al.
We prove these results in the HOL4 theorem prover as follows
⊢∀p X g Z. prior g vulnerability p X g Z =
extreal max set (IMAGE (λz. prior expected gain p X g z) Z)
⊢∀p X Y g Z. FINITE Ω ∧prob space p ∧
(∀x. x ∈Ω ⇒{x} ∈events p) ⇒
posterior g vulnerability p X Y g Z =

y∈Y (Ω)
extreal max set
(IMAGE (λz. posterior expected gain p X Y y g z) Z)
⊢∀p X Y y g z. prob space p ∧FINITE Ω ∧Ω ̸= ∅∧
(∀x. x∈Ω ⇒{x} ∈events p) ∧
(∀x. x∈(X(Ω)) ⇒0≤g(z,x) ∧g(z,x)≤1) ⇒
prior expected gain p X g z =

y∈Y (Ω)
posterior expected gain p X Y y g z
We will later use these properties in order to verify the zero valued g-leakage
result. We prove the fact that the g-leakage of 0 is related to the expected gain
of all outputs, i.e., this statement occurs if there exists a guess z′ maximizing
the expected gain for all outputs y.
Theorem 6. (Zero Gain Information Leakage)
Given a random variable X modeling the initial uncertainty, a random variable
Y modeling the remaining uncertainty and a gain function g, the g-leakage is 0
if there exists a guess z′ ∈Z such that: ∀z y. Eg(z′, y) ≥Eg(z, y)
In the HOL4 environment, this property is formalized as follows:
⊢∀p X Y y g Z. (prob space p ∧FINITE (p space p) ∧
((p space p) ̸= ∅) ∧
(∀x. x ∈p space p ⇒{x} ∈events p) ∧(FINITE Z) ∧
(events p = POW (p space p)) ∧(Z ̸= ∅) ∧
(∀x z. (0 ≤(g (z,x))) ∧((g (z, x)) ≤1)) ∧
(0 < prior g vulnerability p X g Z)) ⇒
((∃z’. (z’∈Z) ∧(∀z y. (posterior expected gain p X Y y g z) ≤
(posterior expected gain p X Y y g z’))) ⇒
(g information leakage p X Y g Z = 0))
Proof. If such a guess exists, then we ﬁrst prove that it corresponds to the
maximum prior expected gain ∀z. Eg(z′) ≥Eg(z). Then, using the previous
results, it follows that the posterior g-vulnerability is equal to the prior expected
gain of the best guess
Vg(X, Y ) =

y
Eg(z, y) =

y
max
z
Eg(z, y) =

y
Eg(z′, y) = Eg(z′)

Formal Analysis of Information Flow in HOL
295
However, since Eg(z′) is the prior g-vulnerability Vg(X), from Theorem 5, so it
follows from the deﬁnition of the g-leakage that this measure is 0.
Based on the soundness of theorem proving, the above-mentioned formally
veriﬁed theorems are guaranteed to be accurate and contain all the required
assumptions. Moreover, these results can be built upon to reason about infor-
mation ﬂow analysis of various applications within the sound core of a theorem
prover.
5
Min-Entropy Leakage and g-Leakage
In this section, we illustrate the practical usefulness of the theoretical foundations
developed in this paper so far. We will present a threat scenario in which we
conduct a comparison between the min-entropy leakage and the g-leakage and
show that the g-leakage can be smaller than min-entropy leakage. Consider the
channel (Matrix of transitional probabilities), described in Table 2, where xi are
the high inputs and yi are the outputs modelled, respectively, with the random
variables X and Y . We assume for this example that inputs and outputs are
uniformly distributed.
Table 1. Transition channel
y1 y2
x1
1
2
1
2
x2 1
0
x3 0
1
Table 2. Gain function
gd x1 x2
x3
z1 1
0
0
z2 0
1
0.98
z3 0
0.98 1
For our particular example, we consider the gain function called the distance
gain function between the secrets, assuming that X = Z, gd(z, x) = 1 −d(z, x)
where d(z, x) is the normalized distance between z and x. Using this conﬁgura-
tion, we prove that the min-entropy leakage is equal to log 2 = 1 and g-leakage
is equal to log
2
1.98. The formalization of this theorem in HOL4 is
Theorem 7. (Comparing Min-Entropy Leakage and g-Leakage)
⊢∀p X Y Z g. (prob space p) ∧(FINITE (p space p)) ∧
(∀x. (x ∈p space p) ⇒{x} ∈events p) ∧
((IMAGE X (p space p)) = {0;1;2}) ∧
((IMAGE Y (p space p)) = {0;1}) ∧(Z = {0;1;2}) ∧
(∀x. x ∈(IMAGE X (p space p)) ⇒distribution p X {x} =
(1/|IMAGE X (p space p)|) ∧
(∀y. y ∈(IMAGE Y (p space p)) ⇒
distribution p Y {y} = (1/|IMAGE Y (p space p)|) ∧
(conditional distribution p Y X ({0},{0}) = (1/2)) ∧
(conditional distribution p Y X ({0},{1}) = 1) ∧
(conditional distribution p Y X ({0},{2}) = 0) ∧
(conditional distribution p Y X ({1},{0}) = (1/2)) ∧

296
G. Helali et al.
(conditional distribution p Y X ({1},{1}) = 0) ∧
(conditional distribution p Y X ({1},{2}) = 1) ∧
(g(0,0) = 1) ∧(g(0,1) = 0) ∧(g(0,2) = 0) ∧(g(1,0) = 0) ∧
(g(1,1) = 1) ∧(g(1,2) = Normal (0.98)) ∧(g(2,0) = 0) ∧
(g(2,1) = Normal (0.98)) ∧(g(2,2) = 1)) ⇒
((information leakage p X Y = 1) ∧
(g information leakage p X Y g Z = log (2/(Normal(1.98)))))
where g refers to the gain function, conditional distribution denotes the
transition distribution with respect to Tables 1 and 2 and the term Normal is
used for the extended real numbers theory.
The proof of this result is conducted using the vulnerability properties, prob-
ability reasoning and real analysis. The ﬁrst part of the theorem is proved using
Theorem 4.2. The second part of the goal is veriﬁed by computing the values of
the initial and remaining vulnerabilities. We ﬁnd that
Vgd(X) = 1
3max{1 + 0 + 0, 0 + 1 + 0.98, 0 + 0.98 + 1} = 0.66
Now for the posterior vulnerability, we calculate the posterior distribution
P(X = x|Y = y1) = ( 1
3, 2
3, 0).
Vgd(X = x|Y = y1) = max
⎧
⎨
⎩
1
3.1 + 2
3.0 + 0.0,
1
3.0 + 2
3.1 + 0.0.98,
1
3.0 + 2
3.0.98 + 0.1
⎫
⎬
⎭= 2
3
Similarly, we prove that Vgd(X = x|Y = y2) = 2
3 and then by rewriting these val-
ues on their corresponding quantities, we get ILgd(X, Y ) = log
2
3
0.66 ≈log
2
1.98.
Here the min-entropy leakage is greater than the g-leakage. Perceptively, this
example diﬀerentiates between x2 and x3. Due to the relations (z2, x3) and
(z3, x2), under the distance gain function, it follows that x2 and x3 are so close
(a gain of 0.98). Thus the g-vulnerability hardly increases. The proof of this
result required 550 lines of HOL4 code [13] and around 60 man-hours in terms
of reasoning eﬀort. These results are considered to be accurate and the analysis
covers any type of systems (in terms of state space size).
6
Conclusion
This paper presents a formalization of some of the most commonly used proper-
ties of information ﬂow in higher-order logic. These properties, depending on the
threat model, are based on Shannon entropy and gain min-entropy. This formal-
ization provides a more reliable and richer information ﬂow analysis framework
compared to the traditional deﬁnitions of quantitative information ﬂow analysis
as formalized measures cover a wide variety of threat scenarios. We used the
formalized notions of Shannon entropy to verify the Data Processing Inequality,
which states that leakage cannot be increased by post processing of the informa-
tion, and Jensen’s Inequality, which is a relation between the averaging and the

Formal Analysis of Information Flow in HOL
297
processing of information ﬂow. The g-leakage is introduced as a generalization of
the min-entropy leakage and belief min-entropy leakage to assess the case where
the secret is guessed partially using a gain function, which models the beneﬁt
that an attacker gets about the secret. Gain functions engender the possibility to
cover a variety of operational scenarios. The proposed formalization can be built
upon to conduct the information ﬂow analysis within the sound core of a theo-
rem prover and thus the analysis is guaranteed to be free of approximation and
precision errors. For illustration purposes, we performed a comparison analysis
between the min-entropy leakage and the g-leakage using the HOL4 theorem
prover and the analysis results were found to be generic and accurate.
This work is conducted as a formal framework that can be used to formally
verify many information ﬂow aspects depending on the threat model. It pro-
vides a reasonable foundation for information ﬂow in HOL. Many applications
can be analyzed using our formalization, such as the Crowds protocol [25] and
Freenets [6]. We are aiming to extend this work for the formal analysis of chan-
nel capacity (min-capacity and gain-capacity) and compare them with Shanon
capacity. Starting from a speciﬁc leakage bound, our work can be used to eval-
uate the input set based on the output set. This formalization can in turn be
used to formally ensure a speciﬁc level of security of critical information.
References
1. HOL4, hol.sourceforge.net (2017)
2. Alvim, M.S., Chatzikokolakis, K., Palamidessi, C., Smith, G.: Measuring informa-
tion leakage using generalized gain functions. In: IEEE Symposium on Computer
Security Foundations, pp. 265–279 (2012)
3. Andrea, S.: Possibilistic information theory: a coding theoretic approach. Fuzzy
Sets Syst. 132(1), 11–32 (2002)
4. Beaudry, N.J., Renner, R.: An intuitive proof of the data processing inequality.
Quantum Inform. Comput. 12(5–6), 432–441 (2012)
5. Chung, K.L.: Markov Chains with Stationary Transition Probabilities (1967)
6. Clarke, I., Sandberg, O., Wiley, B., Hong, T.W.: Freenet: a distributed anonymous
information storage and retrieval system. In: Federrath, H. (ed.) Designing Privacy
Enhancing Technologies. LNCS, vol. 2009, pp. 46–66. Springer, Heidelberg (2001).
doi:10.1007/3-540-44702-4 4
7. Clarkson, M.R., Chong, S., Myers, A.C.: Civitas: toward a secure voting system. In:
IEEE Symposium on Security and Privacy, pp. 354–368. IEEE Computer Society
(2008)
8. Coble, A.R.: Anonymity, Information, and Machine-Assisted Proof. Ph.D. thesis,
King’s College, University of Cambridge, UK (2010)
9. Cover, T.M., Thomas, J.: Entropy, relative entropy and mutual information. In:
Elements of Information Theory. Wiley-Interscience (1991)
10. Dubois, D., Nguyen, H.T., Prade, H.: Possibility theory, probability and fuzzy sets:
misunderstandings, bridges and gaps. In: Dubois, D., Prade, H. (eds.) Fundamen-
tals of Fuzzy Sets. The Handbooks of Fuzzy Sets Series, pp. 343–438. Kluwer,
Boston (2000)
11. Halpern, J., O’Neill, K.: Anonymity and information hiding in multiagent systems.
J. Comput. Secur. 13(3), 483–514 (2005)

298
G. Helali et al.
12. Hasan, O., Tahar, S.: Formal veriﬁcation methods. In: Encyclopedia of Information
Science and Technology, pp. 7162–7170. IGI Global Pub. (2015)
13. Helali, G., Dunchev, C., Hasan, O., Tahar, S.: Towards The Quantitative Analy-
sis of Information Flow in HOL, HOL4 code (2017). http://hvg.ece.concordia.ca/
projects/prob-it/gainMinEntropy.php
14. Helali, G., Hasan, O., Tahar, S.: Formal analysis of information ﬂow using
min-entropy and belief min-entropy. In: Iyoda, J., de Moura, J. (eds.) SBMF
2013. LNCS, vol. 8195, pp. 131–146. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-41071-0 10
15. H¨olzl, J.: Construction and Stochastic Applications of Measure Spaces in
Higher-Order Logic. Ph.D. thesis, Institut f¨ur Informatik, Technische Universit¨at
M¨unchen, Germany (2012)
16. H¨olzl, J., Heller, A.: Three chapters of measure theory in Isabelle/HOL. In: van
Eekelen, M., Geuvers, H., Schmaltz, J., Wiedijk, F. (eds.) ITP 2011. LNCS, vol.
6898, pp. 135–151. Springer, Heidelberg (2011). doi:10.1007/978-3-642-22863-6 12
17. Hua, J., Jing, Y.: On-line payment and security of e-commerce. In: Interna-
tional Conference on Computer Engineering and Applications, pp. 545–550. CEA,
WSEAS (2007)
18. Jebara, T., Pentland, A.: On Reversing Jensen’s Inequality. In: Advances in Neural
Information Processing Systems 13. MIT Press (2000)
19. Chatzikokolakis, K., Palamidessi, C., Panangaden, P.: Anonymity protocols as
noisy channels. Inf. Comput. 206(2–4), 378–401 (2008)
20. Liu, L.: Formalization of Discrete-time Markov Chains in HOL. Ph.D. thesis, Dept.
of Electrical and Computer Engineering, Concordia University, Canada (2013)
21. Mhamdi, T.: Information-Theoretic Analysis using Theorem Proving. Ph.D. the-
sis, Dept. of Electrical and Computer Engineering, Concordia University, Canada
(2012)
22. Mhamdi, T., Hasan, O., Tahar, S.: Formalization of entropy measures in HOL. In:
Eekelen, M., Geuvers, H., Schmaltz, J., Wiedijk, F. (eds.) ITP 2011. LNCS, vol.
6898, pp. 233–248. Springer, Heidelberg (2011). doi:10.1007/978-3-642-22863-6 18
23. Mhamdi, T., Hasan, O., Tahar, S.: Quantitative analysis of information ﬂow using
theorem proving. In: Aoki, T., Taguchi, K. (eds.) ICFEM 2012. LNCS, vol. 7635,
pp. 119–134. Springer, Heidelberg (2012). doi:10.1007/978-3-642-34281-3 11
24. Mhamdi, T., Hasan, O., Tahar, S.: Formalization of measure theory and lebesgue
integration for probabilistic analysis in HOL. ACM Trans. Embedded Comput.
Syst. 12(1) (2013)
25. Reiter, M.K., Rubin, A.D.: Crowds: anonymity for web transactions. ACM Trans.
Inform. Syst. Secur. 1(1), 66–92 (1998)
26. R´enyi, A.: On measures of entropy and information. In: Berkeley Symposium on
Mathematics, Statistics and Probability, pp. 547–561 (1961)
27. Sassone, V., ElSalamouny, E., Hamadou, S.: Trust in crowds: probabilistic behav-
iour in anonymity protocols. In: Wirsing, M., Hofmann, M., Rauschmayer, A. (eds.)
TGC 2010. LNCS, vol. 6084, pp. 88–102. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-15640-3 7
28. Schneider, S., Sidiropoulos, A.: CSP and anonymity. In: Bertino, E., Kurth, H.,
Martella, G., Montolivo, E. (eds.) ESORICS 1996. LNCS, vol. 1146, pp. 198–218.
Springer, Heidelberg (1996). doi:10.1007/3-540-61770-1 38
29. Smith, G.: On the foundations of quantitative information ﬂow. In: Alfaro, L. (ed.)
FoSSaCS 2009. LNCS, vol. 5504, pp. 288–302. Springer, Heidelberg (2009). doi:10.
1007/978-3-642-00596-1 21

Formal Analysis of Information Flow in HOL
299
30. Smith, G.: Quantifying information ﬂow using min-entropy. In: IEEE International
Conference on Quantitative Evaluation of Systems, pp. 159–167 (2011)
31. Syverson, P., Goldschlag, D., Reed, M.: Anonymous connections and onion routing.
In: IEEE Symposium on Security and Privacy, Oackland, California, pp. 44–54
(1997)
32. Trevathan, J.: Privacy and Security in Online Auctions. Ph.D. thesis, School
of Mathematics, Physics and Information Technology, James Cook University,
Australia (2007)

Formalizing SPARCv8 Instruction Set
Architecture in Coq
Jiawei Wang1, Ming Fu1, Lei Qiao2, and Xinyu Feng1(B)
1 University of Science and Technology of China, Hefei, China
xyfeng@ustc.edu.cn
2 Beijing Institute of Control Engineering, Beijing, China
Abstract. The SPARCv8 instruction set architecture (ISA) has been
widely used in various processors for workstations, embedded systems,
and space missions. In order to formally verify the correctness of embed-
ded operating systems running on SPARCv8 processors, one has to for-
malize the semantics of SPARCv8 ISA. In this paper, we present our
formalization of SPARCv8 ISA, which is faithful to the realistic design of
SPARCv8. We also prove the determinacy and isolation properties with
respect to the operational semantics of our formal model. In addition,
we have veriﬁed that a trap handler function handling window overﬂows
satisﬁes the user’s expectations based on our formal model. All of the
formalization and proofs have been mechanized in Coq.
Keywords: SPARCv8 · Coq · Veriﬁcation · Operational semantics
1
Introduction
Computer systems have been widely used in national defense, ﬁnance and other
ﬁelds. Building high-conﬁdence systems plays a signiﬁcant role in the develop-
ment of computer systems. Operating system kernel is the most foundational
software of computer systems, and its reliability is the key in building high-
conﬁdence computer system.
In aerospace and other security areas, the underlying operating system is
usually implemented in C and assembly languages. In existing OS veriﬁcation
projects, e.g., CertiμC/OS-II [20] and seL4 [17], the assembly code is usually not
modeled in order to simplify the formalization of the target machine. They use
abstract speciﬁcations to describe the behavior of the assembly code to avoid
exposing the details of underlying machines, e.g., register and stack. Therefore,
the assembly code in OS kernels is not actually veriﬁed. To verify whether the
assembly code satisﬁes its abstract speciﬁcations, it is inevitable to formalize the
semantics of the assembly instructions.
This work is supported in part by grants from National Natural Science Foundation
of China (NSFC) under Grant Nos. 61632005, 61379039 and 61502031.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 300–316, 2017.
https://doi.org/10.1007/978-3-319-69483-2_18

Formalizing SPARCv8 Instruction Set Architecture in Coq
301
As a highly eﬃcient and reliable microprocessor, the SPARCv8 [6] instruc-
tion set architecture has been widely used in various processors for worksta-
tions, embedded systems, and space missions. For instance, SpaceOS [19] run-
ning on SPARCv8 processors is an embedded operating system developed by
Beijing Institute of Control Engineering (BICE) and deployed in the central
computer of Chang’e-3 lunar exploration mission. On the one hand, to formally
verify SpaceOS, we need to formalize the SPARCv8 instruction set and build the
mathematical semantic model of the assembly instructions. On the other hand,
to ensure the consistency between the behavior of the target assembly code and
the C source code, we hope to use the certiﬁed compiler CompCert [18] to com-
pile SpaceOS. However, CompCert only supports translating Clight, which is
an important subset of C, into ARM [1], x86 [10], PowerPC [5] instruction set
currently. It does not support SPARCv8 at the backend. Extending CompCert
to support SPARCv8 requires us to formalize the SPARCv8 instruction set. In
this paper, we make the following contributions:
– We formalize the SPARCv8 ISA. Our formal model is faithful to the behaviors
of the instructions described in the SPARCv8 manual [7], including most of
the features in SPARCv8, e.g., windowed registers, delayed control transfer,
interrupts and traps.
– We prove that the operational semantics of our formal model satisfy the
determinacy property, and the execution in the user mode or the supervisor
mode satisﬁes the isolation property.
– We take the trap handler for window overﬂows as an example, and give its pre-
condition and post-condition to specify the expected behaviors. Like proving
programs with Hoare triples, we prove that the trap handler satisﬁes the given
pre-/post-conditions and does not throw any exceptions.
– All of the formalization and proofs have been mechanized in Coq [2]. They
contain around 11000 lines of coq scripts in total. The source code can be
accessed via the link [3].
Related Work. Fox and Myreen gave the ARMv7 ISA model [14], they used
monadic speciﬁcation and formalized the instruction decoding and operational
semantics. Narges Khakpour et al. proved some security properties of ARMv7 in
the proof assistant tool HOL4, including the kernel security property, user mode
isolation property, and so on. Andrew Kennedy et al. formalized the subset of
x86 in Coq [16], and they used type classes, notations and the mathematics
library Ssreﬂect [8]. The CompCert compiler also has the formal modeling of
ARM and x86. There are lots of modeling work related to the x86 and ARM,
but due to the speciﬁc features of SPARCv8, these x86 and ARM ISA models
can not be used directly for the SPARCv8 ISA.
Zhe Hou et al. modeled the SPARCv8 ISA in the proof assistant tool
Isabelle [15], which is close to our work. But their work is focused on the
SPARCv8 processor itself, instead of the assembly code running on it. To ver-
ify the assembly code, we need a better deﬁnition on the syntax and operation
semantics. And the deﬁnition of machine state needs to be hierarchical and

302
J. Wang et al.
easy to use when we verify the code running on it. Additionally, they did not
model the interrupt feature in SPARCv8, hence their model could not describe
the uncertainty of the operational semantics caused by interrupt. Besides, our
formalization of the SPARCv8 ISA is implemented in Coq, while CompCert is
implemented in Coq too. We can use our Coq implementation to extend the
CompCert at the backend to support SPARCv8 in the future.
There are some other veriﬁcation work at assembly level [11–13], which give
the formal models of diﬀerent subset of x86 the instruction set and the behavior
of the x86 interrupt management. They mainly study the veriﬁcation technology
of x86 assembly code, the instruction set is relatively small. In the meanwhile,
the model is simple. We formalized the SPARCV8 ISA by considering all the
features of SPARCv8. In the next section, we will give a brief overview of these
features.
2
Overview of SPARCv8 ISA
The Scalable Processor Architecture (SPARC) is a reduced instruction set com-
puting (RISC) instruction set architecture (ISA) originally developed by Sun
Microsystems [9]. It is widely used in the electronic systems of space devices for
its high performance, high reliability and low power consumption. Compared to
other architectures, SPARCv8 has the following unique mechanisms:
– A variety of control-transfer instructions (CTIs) and annulled delay instruc-
tions for more ﬂexible function jumps.
– The register window and window rotation mechanism for swapping context
more eﬃciently.
– Two modes, user mode and supervisor mode, for separating the application
code and operating system code at the physical level.
– A variety of traps for swapping modes through a special trap table that
contains the ﬁrst 4 instructions of each trap handler.
– Delayed-write mechanism for delaying the execution of register write opera-
tion for several cycles.
These characteristics pose quite a few challenges for formal modeling. We use
the example below to demonstrate the subtle control ﬂow in SPARCv8.
Example. The following function CALLER calls the function SUM3 to add three
variables together.
CALLER:
...
1
mov 1, %o0
2
mov 2, %o1
3
call SUM3
4
mov 3, %o2
5
mov %o0, %l7
...
SUM3:
6
save %sp, -64, %sp
7
add %i0, %i1, %l7
8
add %l7, %i2, %l7
9
ret
10
restore %l7, 0, %o0

Formalizing SPARCv8 Instruction Set Architecture in Coq
303
The function SUM3 requires three input parameters. When the CALLER calls
SUM3, it places the ﬁrst two arguments, then calls SUM3 (Line 3) before placing the
third argument (Line 4). In other words, the call instruction will be executed
before the mov instruction which places the last argument. The reason is that
when we call an another function by using instructions such as call, it will
record the address that is going to jump to in the current execution cycle. The
real transfer procedure is executed in the next instruction cycle. This feature is
called “delayed transfer”, which also happens at lines 9 and 10.
In SUM3, we use save and restore instructions (Lines 6 and 10) to save and
restore the caller’s context. When this program is running, both CALLER and
SUM3 have register windows as their contexts, and their windows are overlapping.
When the CALLER needs to save the context and pass the parameters to SUM3,
it will put the parameters in the overlapping section and rotates the window so
that the SUM3’s register window is exposed. At this point, the non-overlapping
portion of the CALLER’s window is hidden. These steps are implemented by the
save instruction. When SUM3 needs to pass the return value to the CALLER, it
will put the return value in the overlap section and rotate the window to destroy
its own space. These steps are implemented by the restore instruction.
The semantics of the delayed transfer and the window rotation mechanism
are quite tricky in SPARCv8. In addition, other special mechanisms of SPARCv8
mentioned above are complicated and their behaviors are non-trivial. Therefore,
it is necessary to give a formal model of the SPARCv8 ISA, which is the basis
of verifying the SPARCv8 code.
3
Modeling SPARCv8 ISA
The SPARCv8 instruction set provides programmers with a hardware-oriented
assembly programming language. To formalize it, ﬁrst we need to provide the
abstract syntax of the given language. Then we deﬁne the machine state. Finally,
we give the operational semantics for the instructions.
3.1
Syntax
Figure 1 shows the syntax of the SPARCv8 assembly language. Here we only give
some typical instructions i that show the key features introduced in Sect. 2. bicca
makes a delayed control transfer if the condition η holds, otherwise it annuls the
next instruction and executes the following code (unless η is al, as explained in
Sect. 3.3). The conditional expression η can be always (al), equal (eq), not equal
(ne), etc.. save (or restore) saves (or restores) the caller’s context by rotating
the register window. ticc triggers a software trap, and rett returns from traps.
wr writes some speciﬁc registers, which are deﬁned as Symbol. Symbol contains
the processor state register (psr), window invalid mask register (wim), trap base
register (tbr), multiply/divide register (y) and ancillary state registers (asr). asr
are used to store the processor’s ancillary state. The write by wr may be delayed
for several cycles, as explained in Sects. 3.2 and 3.3.

304
J. Wang et al.
Fig. 1. The syntax of the SPARCv8 assembly language
The address expressions, operand expressions and trap expressions in these
instructions are deﬁned as OpExp, AddrExp and TrapExp. w stands for 32-bit
integer constants (Word). r stands for general registers (GenReg).
Note that the call, mov, and ret instructions in the example in Sect. 2 are
not given in the syntax, since they are all synthetic instructions, which can be
deﬁned from the basic instructions [7].
3.2
Machine States
Register File. Here we give the deﬁnition of the register ﬁles (RegFile).
(RegName) q :: = r | ς | pc | npc | κ | τ
(RegFile) R ∈RegName →Word
We use q to represent the register name (RegName), including GenReg and
Symbol, which were explained in Sect. 3.1. It also includes the program counter
pc, the next program counter npc, the trap ﬂag τ and annulling ﬂag κ. A register
ﬁle R is modeled as a total function mapping register names to 32-bit integers.
Program Counters. SPARCv8 uses two program counters, viz., pc and npc to
control the execution. pc contains the address of the instruction currently being
executed, while npc holds the address of the next instruction (assuming a trap
does not occur). The function next below deﬁnes the change of program counters
when no transfer occurs. It updates pc with npc and increases npc by 4.
next(R)
def
=== R{pc ⇝R(npc)}{npc ⇝R(npc) + 4}
If transfer occurs during the instruction execution, for example, if the evaluation
of conditional expression returns true when we execute the instruction bicca, the
function djmp will be executed:
djmp(w, R)
def
=== R{pc ⇝R(npc)}{npc ⇝w}
djmp updates pc with npc and sets npc to the target address. As mentioned in
the example in Sect. 2, when we call a function, the target address w is stored
in npc in the current execution cycle. Because the next instruction is fetched
from pc, the transfer is not made immediately and is delayed to the next cycle
instead. The delayed transfer is applied for all transfer instructions in SPARCv8.

Formalizing SPARCv8 Instruction Set Architecture in Coq
305
Window Registers. We use the frame and the frame list to describe the win-
dow registers and window rotating. The deﬁnitions are given as follows:
(Frame) f
:: = [w0, . . . , w7]
(RState) Q :: = (R, F)
(FrameList) F
:: = nil | f::F
A frame is an array that contains 8 words, and a frame list is a list of frames
and its length is 2N-3 (N is the number of windows).
We divide the general registers (r0 . . . r31) in the register ﬁle R into four
groups, global out, local and in, as shown in Fig. 2(1). They represent the current
view of the accessible general registers. There are also registers unaccessible,
which are grouped into frames and stored on the frame list. We pair the register
ﬁle and the frame list together as the register state Q.
Fig. 2. Left rotation of the window
The view of currently accessible registers can be changed by rotating the
window, which exchanges the data between the register ﬁle and the frame list.
This is done to save and restore execution contexts, as what the caller and
sum3 do in the example in Sect. 2. Below we demonstrate the left rotation of
the window in Fig. 2. The formal deﬁnition is given as left win(Q) in Fig. 3. The
rotation takes the following steps:
– We convert three groups of the general registers (out, local and in) into a frame
list consisting of 3 frames, as shown in Fig. 2(1). The conversion is formalized
as fetch(R) in Fig. 3.
– As shown in Fig. 2(2) and (4), we can insert these 3 frames at the end of the
frame list, then left rotate the frame list.

306
J. Wang et al.
Fig. 3. Deﬁnition of the window rotation
– Finally, as shown in Fig. 2(4) and (3), we remove 3 frames from the tail of the
frame list, and insert them to the corresponding positions in the 32 general
registers. The last two steps are modeled as (F ′, l) := left(F, fetch(R)) and
R′ := replace(l, R) in Fig. 3.
Since the left rotation of the window increases the label of current window by 1,
we also need to update the current window pointer (cwp, a segment of psr) to
the new value, namely post cwp.
Delayed Writes. When we execute the wr instruction to write the symbol
register, the execution will be delayed for X cycles (0 ≤X ≤3). The value of X
is implementation-dependent. The delay list D consists of a sequence of delayed
writes d. Each d is a triple consisting of the remaining cycles to be delayed, the
target register and value to be written.
(InitDC) X ∈[0..3]
(DelayCycle)
c
∈[0..X]
(DelayItem)
d
:: = (c, ς, w)
(DelayList) D :: = nil | d::D
There are 2 operations deﬁned on the delay list, as shown bellow.
– When we execute the wr instruction, we will insert a delayed write into the
delay list using function set delay:
set delay(ς, w, D)
def
=== (X, ς, w)::D
– At the beginning of each instruction cycle, we scan the delay list, remove the
delayed writes whose delay cycles are 0 and execute them, and then decrement
the delay cycles of the remaining delayed writes, as shown below:

Formalizing SPARCv8 Instruction Set Architecture in Coq
307
exe delay(Q, D)
def
===
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
(write symbol(ς, w, Q), D′)
if D = (0, ς, w)::D′,
let (Q′, D′′) := exe delay(Q, D′)
in (Q′, (n −1, ς, w)::D′′)
if D = (n, ς, w)::D′,
n ̸= 0
(Q, D)
otherwise
where Q = (R, F)
write symbol writes the value w into the register ς. The details can be found
in the technical report [4].
Machine States and Code Heap. We use M to represent the memory
(Memory), which maps the addresses (Address) to words. The full memory is
split into two parts for the user mode and the supervisor mode respectively. We
formalize the memory as a pair that consists of the user memory Mu and the
supervisor memory Ms. The machine state S contains the memory pair Φ, the
register state Q and the delay list D.
(Address)
a
∈Word
(Memory) M ∈Address ⇀Word
(MemPair) Φ :: = (Mu, Ms)
(State) S :: = (Φ, Q, D)
Besides the machine state, we also deﬁne the code heap C, the pair of code
heap Δ and the event e, shown as below.
(Label)
l
∈
Word
(CodeHeap) C
∈
Label ⇀SparcIns
(CodePair) Δ :: = (Cu, Cs)
(World) W
:: = (Δ, S)
(Event)
e
:: = w | ⊥
(EventList)
E
:: = nil | e::E
C represents the code heap, which maps the labels to the instructions. The
code heap of user mode and supervisor mode together form the pair of code
heap Δ. The whole world W consists of the code heap Δ of two modes and the
machine state S. e stands for events. If a trap occurs, the corresponding trap
label w is recorded as an event, otherwise it is ⊥. An event list E is introduced
for producing events of the multi-step execution.
3.3
Operational Semantics
We deﬁne the operational semantics with multiple layers as shown in Fig. 4,
where the main features of SPARCv8 are introduced at diﬀerent layers. This
layered operational semantics is good for our veriﬁcation work, for example,
when we verify some instructions such as bicca, ticc, etc., we will only consider
the register ﬁle and memory. If we put the exposed window register and the
hidden window register on the same layer as [7] or [15] does, all the registers will
always show up in the veriﬁcation process.
In Fig. 4, from the top to the bottom, we ﬁrst deﬁne the operational semantics
of some simple instructions which only access the register ﬁle and memory using
the transition (M, R)
i
−−−→(M ′, R′).

308
J. Wang et al.
Fig. 4. The structure of operational semantics
Secondly, we lift the ﬁrst layer and give the operational semantic of speciﬁc
instructions about the window register and delayed write features using the
transition (M, Q, D) ◦
i
−−−→(M ′, Q′, D′).
Thirdly, we use the transition C ⊢(M, Q, D) •−−−→(M ′, Q′, D′) to deﬁne
the operational semantics of delay execution and annulling ﬂag handling.
Finally, we give the operational semantic rules of interrupt, trap execution
and mode switch as the transition Δ ⊢S
e
====⇒S′, which deﬁnes the whole
behavior of the entire program. Next, we will introduce some rules of the oper-
ational semantics of each layer. The omitted rules can be found in the technical
report [4] and our Coq implementations [3].
Simple Instructions. The bicca η β instruction evaluates the address expres-
sion β to get the value w, and requires the address w to be word-aligned. It
decides whether to transfer and whether to annul the next instruction by the
conditional expression.
– If the value of the conditional expression is false, it makes no transfer and
sets the annulling ﬂag only.
[[ β ]]R = w
word aligned(w)
[[ η ]]R = false
(M, R)
bicca η β
−−−−−−−→(M, set annul(next(R)))
(bicca-false)
– If the type of the conditional expression is not al and the value is true, it
executes the delayed transfer but does not annul the next intruction.
[[ β ]]R = w
word aligned(w)
η ̸= al
[[ η ]]R = true
(M, R)
bicca η β
−−−−−−−→(M, djmp(w, R))
(bicca-true)
– If the type of the conditional expression is al, we will execute the delayed
transfer and set the annulling ﬂag. The rule is omitted here (see TR [4]).
Recall the deﬁnition of next and djmp in Sect. 3.2. Other functions not deﬁned
here can be found in the TR [4].

Formalizing SPARCv8 Instruction Set Architecture in Coq
309
ticc η γ evaluates the trap expression γ. If the condition η is true, it sets
the trap ﬂag (set user trap). We use w<6:0> to represent the lowest 7 bits of w.
ticc η γ does nothing if η is false (the corresponding rule omitted).
[[ γ ]]R = w
[[ η ]]R = true
(M, R)
ticc η γ
−−−−−→(M, set user trap(w<6:0>, R))
(ticc-true)
Rules for other simple instructions are given in the TR [4].
Window Registers and Delayed Writes. Here we give semantics for instruc-
tions that manipulate the frame list and delay list. First, we use the lift1 rule
to lift the transition (M, R)
i
−−−→(M ′, R′) to (M, Q, D) ◦
i
−−−→(M ′, Q′, D′).
(M, R)
i
−−−→(M ′, R′)
(M, (R, F), D) ◦
i
−−−→(M ′, (R′, F), D)
(lift1)
When the wr rd α ς instruction is executed in user mode (usr mode), since it
does not have permissions for access the register wim, tbr and psr, so ς must
be y or asri. Then it executes the XOR operation of α and rd to get the value
w. Next we insert the triple (X, ς, w) into the delay list D using the function
set delay (see Sect. 3.2). Finally, it resets pc and npc with the function next.
Fig. 5. Auxiliary deﬁnitions

310
J. Wang et al.
usr mode(R)
ς =y or asri
[[ rd ]]R xor [[ α ]]R =w
D′ = set delay(ς, w, D)
(M, (R, F), D) ◦
wr rd α ς
−−−−−−−→(M, (next(R), F), D′)
(wrusr)
When the wr rd α ς instruction is executed in supervisor mode, it has fully
access to all symbol registers. The rule of it is given in the TR [4].
For the rules save and restore, we ﬁrst decrease or increase the label of
the window using the function dec win or inc win. Then we evaluate the operand
expression α to get the value a. Next we assign the value of [[ rs ]]R + a to rd.
The deﬁnition of the inc win can be found in Fig. 5. The increasing operation is
allowed if the post-window (post cwp) is not masked (¬win masked). The func-
tion dec win is similar to the inc win and therefore it is not given here.
dec win(R, F) = (R′, F ′)
[[ α ]]R = a
R′′ = R′{rd ⇝[[ rs ]]R + a}
(M, (R, F), D) ◦
save rs α rd
−−−−−−−−→(M, (next(R′′), F ′), D)
(save)
inc win(R, F) = (R′, F ′)
[[ α ]]R = a
R′′ = R′{rd ⇝[[ rs ]]R + a}
(M, (R, F), D) ◦
restore rs α rd
−−−−−−−−−−→(M, (next(R′′), F ′), D)
(restore)
For the rule rett, we ﬁrst require that the trap is not enabled (¬trap enabled)
and the system is in supervisor mode (sup mode). Then we evaluate the address
expression β to get the value w and require the address w to be word-aligned.
Then we increase the label of the window (inc win), enable the trap (enable trap)
and restore the previous mode (restore mode) by using function rett f, which is
deﬁned in Fig. 5. Finally, the system transfers to the address w by using djmp.
¬trap enabled(R)
sup mode(R)
[[ β ]]R = w
word aligned(w)
rett f(R, F) = (R′, F ′)
(M, (R, F), D) ◦
rett β
−−−−→(M, (djmp(w, R′), F ′), D)
(rett)
Exceptions. If some of the conditions (e.g., word-aligned) in the above rules are
not satisﬁed, the system will throw exceptions. Exceptions include traps and
abortions. Traps such as divided by zero, memory not aligned, window overﬂow,
and so on, will put the trap type label into the trap type register (a segment of
tbr), then the system will execute this trap in the next cycle (see the explanation
below). The abortions make the system to get stuck.
Executing Delay and Handling Annulling Flag. Here we check the delay
list and handle the annulling ﬂag.
– The exe delay function executes the delayed writes (as described in Sect. 3.2).
If the annulled ﬂag has not been setted (¬annulled), it will pick up an instruc-
tion from the code heap and execute it.
exe delay(Q, D) = (Q′, D′)
¬annulled(Q′)
C(Q′.pc) = i
(M, Q′, D′) ◦
i
−−−→(M ′, Q′′, D′′)
C ⊢(M, Q, D) •−−−→(M ′, Q′′, D′′)

Formalizing SPARCv8 Instruction Set Architecture in Coq
311
– Otherwise, If the annulled ﬂag has been setted (annulled), it will skip one
instruction and unset the annulling ﬂag (clear annul).
exe delay(Q, D) = (Q′, D′)
annulled(Q′)
next(clear annul(Q′)) = Q′′
C ⊢(M, Q, D) •−−−→(M, Q′′, D′)
Interrupts, Traps and Mode Switch. In each instruction cycle, we deal with
interrupts and traps ﬁrst.
– If there is an interrupt request with level w and it is allowed (interrupt), the
system triggers a trap after this external interrupt happens. It will record the
trap type (get tt) and execute this trap (exe trap), then it will dispatch an
instruction. The deﬁnition of interrupt and get tt can be found in the technical
report [4].
interrupt(w, Q) = Q′
get tt(Q′) = w′
exe trap(Q′) = Q′′
Cs ⊢(Ms, Q′′, D) •−−−→(M ′
s, Q′′′, D′)
(Cu, Cs) ⊢((Mu, Ms), Q, D)
w′
=====⇒((Mu, M ′
s), Q′′′, D′)
When we execute the trap (exe trap), ﬁrst we need to make sure that the
system allows traps to occur (trap enabled). Then we rotate the window to
the right (right win), forbid traps to occur (disable trap), save the current
mode (save mode) and enter the supervisor mode (to sup). Finally we save
pc and npc to register r17 and r18 by using function save pc npc, unset the
trap ﬂag (clear trap) and jump to the address of the trap handler (tbr jmp).
Function exe trap, tbr jmp and save pc npc are deﬁned in Fig. 5. The function
right win is similar to the left win and therefore it is not given here.
– If the system has a trap, it will record and execute this trap, and then dispatch
an instruction.
has trap(Q)
get tt(Q) = w
exe trap(Q) = Q′
Cs ⊢(Ms, Q′, D) •−−−→(M ′
s, Q′′, D′)
(Cu, Cs) ⊢((Mu, Ms), Q, D)
w
=====⇒((Mu, M ′
s), Q′′, D′)
– If the system does not have traps, it will select the code heap and the memory
according to the mode (usr mode or sup mode) and dispatch an instruction.
¬has trap(Q)
usr mode(Q)
Cu ⊢(Mu, Q, D) •−−−→(M ′
u, Q′, D′)
(Cu, Cs) ⊢((Mu, Ms), Q, D) ====⇒((M ′
u, Ms), Q′, D′)
¬has trap(Q)
sup mode(Q)
Cs ⊢(Ms, Q, D) •−−−→(M ′
s, Q′, D′)
(Cu, Cs) ⊢((Mu, Ms), Q, D) ====⇒((Mu, M ′
s), Q′, D′)

312
J. Wang et al.
Multi-step Execution. In a single step, the system changes from the state S to
the state S′ and produces an event e. The event e is used to record whether the
system has a trap in an instruction cycle. If the trap occurs, it will record the
trap type into the event list. Otherwise e is ⊥for steps where no trap occurs.
The transition of zero-or-multiple steps is deﬁned as below.
Δ ⊢S |
nil
======⇒0 S
Δ ⊢S
e
====⇒S′′
Δ ⊢S′′ |
E
=====⇒n S′
Δ ⊢S |
e::E
======⇒n+1 S′
4
Determinacy and Isolation Properties
In this section, we will prove that our formal model satisﬁes the determinacy
and isolation properties. The determinacy property explains that the execution
of the machine is deterministic with the given sequence of external interrupts.
The isolation property characterizes separation of the memory space of the user
mode and the supervisor mode, which guarantees the space security of the entire
system.
We use Δ ⊢S |
E
==⇒∗S1 to represent zero-or-multiple steps of the execution
under the given sequence of external interrupts E. Theorem 4.1 says that, if two
executions start from the same initial states and both of them produce the same
sequence of external interrupts, then they should arrive at the same ﬁnal states.
Theorem 4.1 (Determinacy). If Δ ⊢S |
E
==⇒∗S1, Δ ⊢S |
E
==⇒∗S2, then
S1 = S2. where Δ ⊢S |
E
==⇒∗S′ is deﬁned as ∃n, Δ ⊢S |
E
==⇒n S′.
In SPARCv8 ISA, triggering a trap is the only way of switching to the super-
visor mode. We will prove this property ﬁrst. That is, if a system is running in
the user mode at the beginning, it will run in the user mode forever if there is
no trap. First, we give the conditions of running n steps in user mode as below:
Δ ⊢S •==⇒n S′
def
=== usr mode(S) ∧empty DL(S) ∧Δ ⊢S |
E
==⇒n S′
∧no trap event(E)
where
empty DL(S)
def
=== D = nil
where S = ((Mu, Ms), Q, D)
no trap event(E)
def
=== ∀e ∈E, e = ⊥
We ﬁrst require the system to be in the user mode initially (usr mode). Sec-
ond, because of the delayed write feature, we need to require the delay list to
be empty (empty DL), otherwise the system may enter the supervisor mode if
there is a delayed write item in the delay list that will modify the S segment
of PSR. Finally, we require there is no trap in the system after several steps
(no trap event).
After giving these conditions, we need to prove that the system is always
running in the user mode under these conditions, as shown in Theorem 4.2:

Formalizing SPARCv8 Instruction Set Architecture in Coq
313
Theorem 4.2 (In User Mode). If Δ ⊢S •==⇒n S′, then usr mode(S′).
It
says
that,
if
the
system
satisﬁes
all
the
conditions
deﬁned
in
Δ ⊢S •==⇒n S′, it will be in the user mode after n steps. Since this theorem
is true for all n, the system should be in the user mode after arbitrary steps.
So we can call Δ ⊢S •==⇒n S′ as “the system is running in the user mode for
n-steps”. This property will be used in proving the isolation property later.
Based on Theorem 4.2, we apply it to prove if a system is running in user
mode, it does not have the permission to read and write the resource that belongs
to the supervisor mode. The isolation property is shown bellow:
Theorem 4.3 (Write Isolation). If Δ ⊢S •==⇒n S′, then sup part eq(S, S′)
where
sup part eq(S, S′)
def
=== Ms = M ′
s
where S = ((Mu, Ms), Q, D) , S′ = ((M ′
u, M ′
s), Q′, D′)
Theorem 4.4 (Read Isolation). If usr code eq(Δ1, Δ2), usr state eq(S1, S2),
and Δ1 ⊢S1 •==⇒n S′
1, Δ2 ⊢S2 •==⇒n S′
2, then usr state eq(S′
1, S′
2)
where
usr state eq(S, S′)
def
=== Q = Q′ ∧Mu = M ′
u
where S = ((Mu, Ms), Q, D) , S′ = ((M ′
u, M ′
s), Q′, D′)
usr code eq(Δ, Δ′)
def
=== Cu = C′
u
where Δ = (Cu, Cs) , Δ′ = (C′
u, C′
s)
Theorem 4.3 shows that if the system is running in the user mode, it does not
modify the resource that belongs to the supervisor mode. Theorem 4.4 shows
that if a particular part of two systems are the same at the beginning, they will
always be the same when the system is running in the user mode for several
steps. The above two theorems show the isolation property of SPARCv8.
5
Verifying a Window Overﬂow Trap Handler
In this section, we verify a trap handler, which is used to handle exception of
the window overﬂow. The number of windows provided by SPARCv8 is ﬁnite. If
we execute the save instruction to save the context when all the windows have
already been used, it will cause a window overﬂow trap. The window overﬂow
trap handler will be executed to handle the trap. We give the code of the trap
handler as below.
First, the handler takes the next window as the masked window, which is
implemented by loop shift operation (Lines 1–5 and 7–10). Then the pointer
(named cwp, a segment of psr) that always points to the current window points to
the next window, and we store the value of the current window into the memory
(Lines 6 and 11–26). Finally, the handler restores cwp and returns (Lines 27–30).
The window overﬂow trap handler saves the oldest element of the window into
the memory and makes the window available for the upcoming save operation.

314
J. Wang et al.
WINDOW OVERFLOW:
1
mov %wim,%l3
2
mov %g1,%l7
3
srl %l3,1,%g1
4
sll %l3,NWINDOWS-1,%l4
5
or %l4,%g1,%g1
6
save
7
mov %g1,%wim
8
nop
9
nop
10
nop
11
st %l0,[%sp+0]
12
st %l1,[%sp+4]
13
st %l2,[%sp+8]
14
st %l3,[%sp+12]
15
st %l4,[%sp+16]
16
st %l5,[%sp+20]
17
st %l6,[%sp+24]
18
st %l7,[%sp+28]
19
st %i0,[%sp+32]
20
st %i1,[%sp+36]
21
st %i2,[%sp+40]
22
st %i3,[%sp+44]
23
st %i4,[%sp+48]
24
st %i5,[%sp+52]
25
st %i6,[%sp+56]
26
st %i7,[%sp+60]
27
restore
28
mov %l7,%g1
29
jmp %l1
30
rett %l2
To verify this window overﬂow trap handler, we ﬁrst need to give its speciﬁ-
cations, namely, the precondition and the postcondition shown as below:
overﬂow pre cond(W)
def
===
single mask(R(cwp), R(wim)) ∧handler context(R)
∧normal cursor(R) ∧align context(Q) ∧
set function(R(pc), windowoverﬂow, Cs) ∧
D = nil ∧length(F) = 2N −3
where W = (Δ, (Φ, Q, D)), Δ = (Cu, Cs), Q = (R, F)
overﬂow post cond(W)
def
===
single mask(pre cwp(2, R), R(wim))
where W = (Δ, (Φ, Q, D)), Q = (R, F)
In the pre-condition, single mask(w, R(wim)) indicates that the system sim-
ply masks the window w, and the rest of the window is all available. pre cwp(n, R)
gives the window in front of the current window and the distance of them is
n. handler context contains the unique state of the system after the exe trap
function is executed. For example, the system must be in the supervisor mode,
the trap must be disabled, and so on. normal cursor and handler context illus-
trate the requirements for pc and npc before entering the overﬂow trap handler.
align context requires the address to be word-aligned. The rest gives the require-
ments for the delay list and the frame list.
In the post-condition, when we ﬁnish running the trap handler and return to
the original function where the trap occurs, cwp will point to the window used by
the original function. At this point, the next window is no longer masked, which
means the next window is available. In SPARCv8, when we execute the save
instruction and enter the next window, the label of the window is decreased. So
we use pre cwp(2, R) to represent the label of the window that has been masked,
which also means that we have an available window now.

Formalizing SPARCv8 Instruction Set Architecture in Coq
315
Then we verify the correctness of the handler by showing that the handler
can be safely executed under the given pre-condition. As shown in Theorem 5.1,
it says, if the initial state satisﬁes the precondition, then we can safely execute
to a resulting state satisfying the postcondition within 30 steps, and no trap
occurs during the execution. More details about the speciﬁcation and proofs can
be found in the technical report [4] and our Coq implementation [3].
Theorem 5.1 (Correctness of the Window Overﬂow Trap Handler).
If overﬂow pre cond(Δ, S), then forall S’ and E, if Δ ⊢S |
E
==⇒30 S′, then
overﬂow post cond(Δ, S′) and no trap event(E).
6
Conclusion and Future Work
In this paper, we have formalized the SPARCv8 instruction set in Coq, which
provides the formal model for verifying SpaceOS at the assembly level. Also the
formalization can help us to add SPARCv8 into the backend of CompCert in the
future. Since the correctness and availability are also critical in formal modeling
of SPARCv8, we prove the determinacy and isolation properties to validate the
model, and we also verify the window overﬂow handler to show the availability
of our formalization.
For the future work, we will give the syntax and operational semantics of
the remaining instructions, including integer arithmetic instructions, ﬂoating
point instructions, and coprocessor instructions. To facilitate the code veriﬁca-
tion process, we will develop a program logic for reasoning about the assembly
code, instead of doing veriﬁcation in terms of the operational semantics directly.
We hope to extend CompCert backend to support the SPARCv8 assembly
language.
References
1. Arm architecture. https://en.wikipedia.org/wiki/ARM architecture
2. The coq proof assistant. https://coq.inria.fr
3. Formalizing sparcv8 instruction set architecture in coq (project code). https://
github.com/wangjwchn/sparcv8-coq
4. Formalizing sparcv8 instruction set architecture in coq (technical report). https://
wangjwchn.github.io/pdf/sparc-coq-tr.pdf
5. Powerpc. https://en.wikipedia.org/wiki/PowerPC
6. Sparc. https://en.wikipedia.org/wiki/SPARC
7. The sparc architecture manual v8. http://gaisler.com/doc/sparcv8.pdf
8. Ssreﬂect. http://ssr.msr-inria.inria.fr
9. Sun microsystems. https://en.wikipedia.org/wiki/Sun Microsystems
10. x86. https://en.wikipedia.org/wiki/X86
11. Feng, X., Shao, Z.: Modular veriﬁcation of concurrent assembly code with dynamic
thread creation and termination. In: International Conference on Functional Pro-
gramming (ICFP), pp. 254–267. ACM (2005)

316
J. Wang et al.
12. Feng, X., Shao, Z., Dong, Y., Guo, Y.: Certifying low-level programs with hard-
ware interrupts and preemptive threads. In: Conference on Programming Language
Design and Implementation (PLDI), pp. 170–182. ACM (2008)
13. Feng, X., Shao, Z., Vaynberg, A., Xiang, S., Ni, Z.: Modular veriﬁcation of assem-
bly code with stack-based control abstractions. In: Conference on Programming
Language Design and Implementation (PLDI), pp. 401–414. ACM (2006)
14. Fox, A., Myreen, M.O.: A trustworthy monadic formalization of the ARMv7
instruction set architecture. In: Kaufmann, M., Paulson, L.C. (eds.) ITP
2010. LNCS, vol. 6172, pp. 243–258. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-14052-5 18
15. Hou, Z., Sanan, D., Tiu, A., Liu, Y., Hoa, K.C.: An executable formalisation of
the SPARCv8 instruction set architecture: a case study for the LEON3 processor.
In: Fitzgerald, J., Heitmeyer, C., Gnesi, S., Philippou, A. (eds.) FM 2016. LNCS,
vol. 9995, pp. 388–405. Springer, Cham (2016). doi:10.1007/978-3-319-48989-6 24
16. Kennedy, A., Benton, N., Jensen, J.B., Dagand, P.E.: Coq: the world’s best macro
assembler?. In: Proceedings of the 15th Symposium on Principles and Practice of
Declarative Programming (PPDP), pp. 13–24. ACM (2013)
17. Klein, G., Elphinstone, K., Heiser, G., Andronick, J., Cock, D., Derrin, P., Elka-
duwe, D., Engelhardt, K., Kolanski, R., Norrish, M., et al.: sel4: formal veriﬁcation
of an os kernel. In: Proceedings of the ACM SIGOPS 22nd Symposium on Oper-
ating Systems Principles (SOSP), pp. 207–220. ACM (2009)
18. Leroy, X.: Formal certiﬁcation of a compiler back-end, or: programming a compiler
with a proof assistant. In: 33rd Symposium Principles of Programming Languages
(POPL), pp. 42–54. ACM (2006)
19. Qiao, L., Yang, M., Gu, B., Yang, H., Liu, B.: An embedded operating system
design for the lunar exploration rover. In: Proceedings of the 2011 Fifth Inter-
national Conference on Secure Software Integration and Reliability Improvement-
Companion (SSIRI-C), pp. 160–165. IEEE Computer Society (2011)
20. Xu, F., Fu, M., Feng, X., Zhang, X., Zhang, H., Li, Z.: A practical veriﬁca-
tion framework for preemptive OS kernels. In: Chaudhuri, S., Farzan, A. (eds.)
CAV 2016. LNCS, vol. 9780, pp. 59–79. Springer, Cham (2016). doi:10.1007/
978-3-319-41540-6 4

Tools

How to Eﬃciently Build a Front-End Tool
for UPPAAL: A Model-Driven Approach
Stefano Schivo1, Bu˘gra M. Yildiz1, Enno Ruijters1(B), Christopher Gerking2,
Rajesh Kumar1, Stefan Dziwok3, Arend Rensink1, and Mari¨elle Stoelinga1
1 Formal Methods and Tools, University of Twente, Enschede, The Netherlands
{s.schivo,b.m.yildiz,e.j.j.ruijters,r.kumar,
a.rensink,m.i.a.stoelinga}@utwente.nl
2 Software Engineering, Heinz Nixdorf Institute,
Paderborn University, Paderborn, Germany
christopher.gerking@upb.de
3 Software Engineering, Fraunhofer IEM, Paderborn, Germany
stefan.dziwok@iem.fraunhofer.de
Abstract. We propose a model-driven engineering approach that facil-
itates the production of tool chains that use the popular model checker
Uppaal as a back-end analysis tool. In this approach, we introduce a
metamodel for Uppaal’s input model, containing both timed-automata
concepts and syntax-related elements for C-like expressions. We also
introduce a metamodel for Uppaal’s query language to specify temporal
properties; as well as a metamodel for traces to interpret Uppaal’s coun-
terexamples and witnesses. The approach provides a systematic way to
build software bridging tools (i.e., tools that translate from a domain-
speciﬁc language to Uppaal’s input language) such that these tools
become easier to debug, extend, reuse and maintain. We demonstrate our
approach on ﬁve diﬀerent domains: cyber-physical systems, hardware-
software co-design, cyber-security, reliability engineering and software
timing analysis.
1
Introduction
Uppaal [3] is a leading model checker for real-time systems, allowing one to
verify automatically whether a system meets its timing requirements. Uppaal
and its extensions have been applied to a large number of domains, ranging
from communication protocols [28], over planning [4] to systems biology [31]. As
such, Uppaal is a popular back-end for various other real-time analysis tools,
such as ANIMO [31], sdf2ta [13] and STATE [19]. Typically such tools take their
inputs in a domain-speciﬁc language (DSL) and translate these inputs into timed
automata, which are then fed into Uppaal to perform the analysis. In this way,
domain experts can write their models in a DSL that they are familiar with,
while still using Uppaal’s powerful analysis algorithms behind the scenes.
A disadvantage of this approach is, however, that the tools that translate
from a DSL to Uppaal’s input language, i.e., software bridging tools, are often
implemented ad hoc, and hence diﬃcult to debug, reuse, extend and maintain.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 319–336, 2017.
https://doi.org/10.1007/978-3-319-69483-2_19

320
S. Schivo et al.
To overcome this problem, we advocate to develop these tools with model-
driven engineering (MDE) techniques, which studies [26] have demonstrated can
lead to faster software development, with higher levels of interoperability and
lower cost. MDE is an approach that uses models as ﬁrst-class citizens, rather
than as by-product of intermediate steps. In MDE, a metamodel captures core
concepts and behavior of a certain domain. Then, domain-speciﬁc models are
instances of this metamodel and can be transformed to other models, formats
or formalisms via model transformations.
In this paper, we propose an MDE approach for tools that use Uppaal
as a back-end. In the context of our approach, we introduce metamodels for
Uppaal timed automata, Uppaal’s query language and its diagnostic traces,
in order to transform the domain-speciﬁc models to Uppaal, analyze them and
transform the results back to a domain-speciﬁc representation, respectively. Our
metamodels also support Uppaal’s extensions with cost [4] and probability [7].
We show our approach on ﬁve diverse application domains: cyber-physical
systems, namely, coordination protocols of MechatronicUML; hardware-soft-
ware co-design, namely, scheduling of synchronous dataﬂow graphs; cyber-se-
curity, namely, analysis of attack trees; reliability engineering, namely, analysis
of fault trees; and software timing analysis, namely, timing analysis of Java
applications.
Our contributions. To summarize, our main contribution is an MDE approach
for building software bridging tools around the Uppaal model checker. Con-
cretely, we introduce (1) metamodels1 for Uppaal’s timed automata, queries
and traces, providing all the ingredients needed to construct Uppaal models,
verify relevant properties and interpret the results; (2) model transformations
from several domain-speciﬁc models to the Uppaal models and back; and (3)
ﬁve case studies demonstrating how the approach is applied in practice and
supports a wide range of application domains.
Overview of our MDE approach. The proposed approach can be seen in Fig. 1.
Taking into consideration the analysis of a (generic) domain-speciﬁc model, the
most important steps involving a bridging software tool that implements our
approach are the following:
– In Step 1, a domain-speciﬁc model is generated/created by the domain expert.
This model is an instance of the metamodel of a particular domain of inter-
est. Such a metamodel deﬁnes the concepts and their relationships in that
domain. For some domains, it may be more convenient to deﬁne multiple
related metamodels targeting distinct concerns.
– In Step 2, the domain-speciﬁc model is transformed to a timed-automata
model, conforming to the Uppaal Timed Automata metamodel (uta) we
propose as part of the contribution of this paper. A snippet of such a trans-
formation can be found in Fig. 7.
1 The metamodels are available at https://github.com/uppaal-emf/uppaal.

How to Eﬃciently Build a Front-End Tool for UPPAAL
321
– In Step 3, the property against which the domain-speciﬁc model is to be
checked is speciﬁed in a query language speciﬁc to the domain.
– In Step 4, the query speciﬁed in the domain-speciﬁc query language is trans-
formed to a corresponding Uppaal query, in turn conforming to the Uppaal
Query metamodel (uqu) we propose as part of the contribution of this paper.
– In Step 5, Uppaal checks if the timed-automata model (a uta model) satisﬁes
the property speciﬁed by the generated query (a uqu model). The result of
this operation is usually a diagnostic trace. As part of this step, the uta and
uqu models are transformed into the native Uppaal input formats; moreover,
the diagnostic trace natively produced by Uppaal is transformed into yet
another model, conforming to the Uppaal Trace metamodel (utr) that we
also propose as part of the contribution of this paper.
– In Step 6, the utr model is transformed back to a domain-speciﬁc represen-
tation. This representation can conform to a metamodel that is designed to
express the analysis results in an understandable way by the domain experts.
UPPAAL
Domain
conforms to
Domain-Speciﬁc
Metamodel
Domain-Speciﬁc 
Model
Timed Automata 
Model
 UPPAAL Timed 
Automata 
Metamodel (UTA)
conforms to
conforms to
Domain-Speciﬁc 
Query Language
Domain-Speciﬁc 
Query
UPPAAL Query
UPPAAL Query 
Metamodel (UQU)
conforms to
Trace Model
UPPAAL Trace 
Metamodel (UTR)
conforms to
Model 
TransformaƟon
Model 
TransformaƟon
Model Checking
Model 
TransformaƟon
1
2
3
4
5
6
conforms to
Domain-Speciﬁc 
RepresentaƟon 
Metamodel
Domain-Speciﬁc 
RepresentaƟon
Fig. 1. The generic model-driven engineering approach for building front-end tools that
use Uppaal as a back-end analysis engine.

322
S. Schivo et al.
Organization of the paper. Section 2 provides some background information
about MDE and the timed-automata formalism. Section 3 introduces the three
metamodels and their transformations. Section 4 discusses the case studies.
Section 5 discusses the related work and Sect. 6 concludes the paper.
2
Background
In this section, we provide some background information about model-driven
engineering (cf. Sect. 2.1) and the timed-automata formalism (cf. Sect. 2.2).
2.1
Model-Driven Engineering
Models are powerful tools to express structure, behavior and other properties
in domains such as engineering, physics, architecture and other ﬁelds. Model-
Driven Engineering (MDE) is a software engineering approach that considers
models not only as documentation, but also adopts them as basic abstractions
to be used directly in development processes [33].
To deﬁne models of a particular domain, we need to specify their language.
In MDE, such a language (often referred to as a domain-speciﬁc language, DSL)
is also speciﬁed as a model at a more abstract level, called a metamodel. A meta-
model captures core concepts and behavior of a certain domain, and deﬁnes the
permitted structure and behavior, to which its instances (models) must adhere.
Another way of saying this is that metamodels describe the syntax of models
[34]. Following the common terminology, we will write that a model conforms to
or is an instance of its metamodel.
MDE provides interoperability between domains (and tools in these domains)
via model transformations. The concept of model transformation is shown in
Fig. 2. Model transformations are usually deﬁned in a language designed specif-
ically to this aim and map the elements of a source metamodel to the elements
of a target metamodel. The transformation engine executes the transformation
deﬁnition on the input model and generates an output model.
Model
conforms to
Target Metamodel
Model
conforms to
Source Metamodel
TransformaƟon 
DeﬁniƟon
TransformaƟon
Engine
executes
maps from
maps to
input
output
Fig. 2. The concept of model transformation.

How to Eﬃciently Build a Front-End Tool for UPPAAL
323
Beneﬁts of MDE. MDE provides a range of important beneﬁts [36], some of
which we brieﬂy discuss below:
– Interoperability: As we have mentioned before, there can be multiple domains
in a project where various tools are used, each with its own I/O formats. MDE
provides interoperability between these domains (and tools in these domains)
via model transformations.
– Higher level of reusability: The metamodels, models and tools from a domain
can be reused by many projects targeting the same domains. Such reuse also
increases the quality of the ﬁnal product since the reused units are revised
and improved continuously.
– Faster tool development: Domain experts only focus on the concepts of the
domain while creating models. Transformations on these models are imple-
mented using languages designed speciﬁcally for model transformations rather
than using general-purpose languages. Because of these advantages of MDE,
the development time of tools decreases.
Tool Choice. There are a number of tools for realizing MDE. The case studies
presented in this paper are implemented using the Eclipse Modeling Framework
(EMF) [35], a state-of-art tool for implementing MDE techniques. EMF provides
the Ecore format for deﬁning metamodels and many plug-ins to support various
functionalities, such as querying, validation and transformation of models.
2.2
Timed Automata and UPPAAL
Timed automata are ﬁnite-state automata with the addition of real-valued clocks
and synchronization channels. In Fig. 3, we show an example timed-automata
model (from [5]), with clocks x and y. Locations are indicated by circles (double
circle for the initial location), and transitions are represented by edges. Condi-
tions on clocks can enable transitions (e.g., x > 10 in Fig. 3b, from dim to oﬀ)
or allow residence in locations (y < 5 in Fig. 3a). Synchronizations can occur
when two automata perform complementary actions on the same channel: in the
example, outputs press! synchronize with inputs press?. When taking a transi-
tion, clocks can be reset (x:=0, y:=0).
Timed-automata models are veriﬁed with Uppaal [3] through queries
expressed in a subset of CTL [12]. In Fig. 3c, we show the trace resulting from
the veriﬁcation of the reachability query E<>lamp.bright, which asks whether
a state where the lamp automaton is in the bright location is reachable. The
veriﬁcation returns a positive outcome, together with a witness trace, listing the
sequence of states and transitions leading to the desired target.
In addition to the standard version of Uppaal, some of the models presented
in this paper are intended for analysis by Uppaal CORA [4], which allows to
compute cost-optimal traces (see Sect. 4.2), and Uppaal-SMC [7], which allows
to perform statistical model checking (see Sect. 4.4).

324
S. Schivo et al.
relax
study
y < 5
idle
y>10 
press!
press!
y:=0
press!
press!
press!
y:=0
(a) Student
bright
dim
oﬀ
x > 10
press?
press?
x<=10
press?
press?
x:=0
(b) Lamp
Verifying formula 1: E<> lamp.bright
-- Formula is satisﬁed.
Showing example trace.
State:
( student.idle lamp.oﬀ)
student.y=0 lamp.x=0
Transitions:
student.idle–>student. id5 { 1, press!, y := 0 }
lamp.oﬀ–>lamp.dim { 1, press?, x := 0 }
State:
( student. id5 lamp.dim )
student.y=0 lamp.x=0
Transitions:
student. id5–>student.study { 1, press!, 1 }
lamp.dim–>lamp.bright { x <= 10, press?, 1 }
State:
( student.study lamp.bright )
student.y=0 lamp.x=0
(c) Trace
Fig. 3. An example of a timed-automata model (a, b) and the textual output (c) of
verifying the reachability query E<>lamp.bright as provided by Uppaal’s command-
line tool.
3
Metamodels for the Approach
We use metamodeling to represent the domain of timed automata and enable the
back-end analysis of domain-speciﬁc models. Our approach extends the work by
Greenyer and Rieke [17] towards full-ﬂedged metamodels, covering all language
features accepted by the Uppaal model checker. Thereby, we make sure that
model transformations may freely use any of Uppaal’s concepts when translat-
ing domain-speciﬁc models into timed-automata models.
In Sect. 3.1, we present the metamodel for Uppaal timed automata (uta).
Section 3.2 describes a metamodel extension for Uppaal’s query language (uqu).
A metamodel for traces obtained from Uppaal (utr) is given in Sect. 3.3.
3.1
The UPPAAL Timed Automata Metamodel
Figure 4a shows an excerpt from our Uppaal Timed Automata metamodel
(uta), extending the metamodeling approach proposed in [17]. This metamodel
reﬂects the basic structure of timed automata accepted by Uppaal.
At the core of uta is a network of timed automata (NTA). An NTA includes a
set of global Declarations, containing instances of the abstract base class Declara-
tion. A declaration is used to introduce elements such as clocks or synchronization
channels. Primarily, an NTA includes a non-empty set of templates where each
Template represents a type of timed automaton. Moreover, an NTA contains a
separate set of system declarations. These are speciﬁc TemplateInstances (omitted
from the ﬁgure), which constitute the set of concrete timed automata that make
up the system to be model-checked.

How to Eﬃciently Build a Front-End Tool for UPPAAL
325
NTA
name : String
Template
name : String
LocaƟon
name : String
Edge
DeclaraƟons
DeclaraƟon
templates
global
system
local
declaraƟons
edges
locaƟons
incoming
outgoing
1..*
1..*
0..*
0..*
0..*
0..*
iniƟal
Expression
invariant
updates
0..*
1
1
(a) uta (Uppaal Timed Automata)
Property
UnaryProperty
quanƟﬁer : PathQuanƟﬁer
operator : TemporalOperator
Expression
LeadsToProperty
right
leŌ
expression
PropertyRepository
properƟes
1
1
1
0..*
(b) uqu (Uppaal Queries)
Fig. 4. Partial views from the uta and uqu.
Templates include locations and edges, and every Template refers to one par-
ticular initial location. Templates may also include local declarations (e.g., for
clocks that should not be reset from outside the automaton). Every Location
refers to its incoming and outgoing edges. In addition, a Location speciﬁes an invari-
ant which is a boolean expression as an instance of the abstract base class Expres-
sion. An Edge may contain expressions as well to specify updates of variables (e.g.,
clock resets). The metamodel also contains syntax-related elements for the C-like
expressions supported by Uppaal.
uta models are not the native input format of Uppaal and, therefore, are
not directly processable. We have implemented a model-to-text transformation,
which takes a uta model as input and transforms it into Uppaal native XML.
3.2
The UPPAAL Query Metamodel
Figure 4b depicts an excerpt from our Uppaal Query metamodel (uqu). Queries
are temporal logic properties to be veriﬁed using model checking. Multiple
queries are bundled by a PropertyRepository, which is the root class of the meta-
model. A repository contains a set of properties, where every Property represents
one query. Every property is either a UnaryProperty or a LeadsToProperty.
A UnaryProperty is a temporal formula that conforms to the computation tree
logic (CTL, [12]). First, such a property includes a quantiﬁer (one of universal
or existential quantiﬁcation) to describe whether the property must hold on all
execution paths, or at least one path. Second, it consists of a modal operator (one
of globally or ﬁnally) to describe if the property needs to hold in all states of a
certain execution path, or needs to hold eventually in some state. Third, unary
properties include an expression to be evaluated in the context of the quantiﬁer
and the operator. For example, this expression could represent an active location
inside an automaton, or a clock value. To this end, uqu extends uta and reuses
the Expression class introduced in Sect. 3.1.

326
S. Schivo et al.
A LeadsToProperty represents a binary property connecting two expressions by
means of the leads-to operator supported by Uppaal. Please note that, according
to the restrictions imposed by Uppaal on the set of CTL formulas supported, our
metamodel does not allow nested properties. However, we introduce dedicated
classes for logical connections of expressions (omitted from Fig. 4b), precisely
reﬂecting the range of functions actually supported by Uppaal.
Like uta models, also uqu queries have to be transformed to Uppaal’s
native format before they can be actually processed. For this purpose, we provide
another model-to-text transformation.
3.3
The UPPAAL Trace Metamodel
The outcome of evaluating a query in Uppaal can be twofold: either a simple
“yes” (for a universally quantiﬁed query claiming that a given property holds for
all paths) or “no” (for an existentially quantiﬁed query asking whether a path
with a given property exists), and possibly a trace through the state-space of
the timed-automata model along which the query fails to hold (for a universal
query) or that is a witness (for an existential query). Queries are very often
formulated in such a way that it is known a priori whether they hold or not,
the interesting part of the outcome is then that diagnostic trace.
Uppaal outputs its traces in a native textual format that is not too well doc-
umented. From [6], we have taken a metamodel (utr) to capture the information
in a tractable way and a parser that produces utr models from Uppaal’s out-
put. Like uqu, also utr depends on uta itself, so that the traces can refer back
to their constituent components. Figure 5 gives a high-level overview of utr.
Trace
TransiƟon
EdgeTransiƟon
DelayTransiƟon
delay : Float
transiƟons
states
target
transiƟonTo
transiƟonFrom
1
1..*
LocaƟon
Edge
edges
1..*
locaƟons
State
name : String
valuaƟons : ValuaƟon [0..*]
clocks : ClockBoundary [0..*]
Ɵme : Float
1 source
0..*
Fig. 5. A partial view from utr (Uppaal Trace metamodel).
A Trace consists of States and Transitions; every State except the ﬁnal one has
a single outgoing Transition. A State refers to a set of Locations (one for every
TemplateInstance in the system, though that cannot be seen from the provided
metamodel fragment), together with Valuations, i.e., bindings for all the variables
to concrete values, as well as boundaries for all the clocks in the system (the Val-
uation and ClockBoundary classiﬁers are omitted from the ﬁgure). Finally, a State
stores the absolute time at which the system arrived in that state. A Transition
can either be a DelayTransition, in which only time passes, or an EdgeTransition,
in which a number of Edges (one for every TemplateInstance involved) ﬁre in syn-
chrony. Location and Edge are imported from uta.

How to Eﬃciently Build a Front-End Tool for UPPAAL
327
4
Case Studies
The general MDE approach we propose for bridging software tools has been
introduced in Sect. 1. In this section, we present ﬁve case studies that have put
this approach into practice.
In Table 1, an overview of these case studies is given. After the section num-
ber, the second column shows to which domain the approach is applied. The
third column contains the list of the metamodels that are used to describe that
domain. The fourth column gives the motivation why model checking is used for
the particular case study. The ﬁfth column shows which steps from the approach
(given in Fig. 1) are implemented in the particular case study. The following sub-
sections describe these case studies in more detail.
The transformations for the cyber-physical systems case study are speciﬁed
in the QVTo [27] language, for the other cases in the Epsilon Transformation
Language [21]. Translation of the timed-automata models to the XML input
ﬁles for Uppaal is performed via the Xtend [11] language, using its template
expressions for model-to-text transformations.
4.1
Coordination Protocols of CPSs
Future cyber-physical systems (CPS; e.g., cars, railway systems, smart facto-
ries) will heavily interact with each other to contribute to aspects like safety,
eﬃciency, comfort and human health. They may achieve this by coordinating
their actions via asynchronous message exchange. However, such a coordination
must be safe and has to obey hard real-time constraints because any (timing)
error may lead to severe damage and even loss of human life. Consequently, the
development of so-called coordination protocols that specify the allowed message
exchange sequences requires formal veriﬁcation like model checking to guarantee
the functional correctness of the coordination.
Model checkers like Uppaal are appropriate for verifying such coordina-
tion protocols but their language has no built-in support for domain-speciﬁc
aspects like asynchronous communication including message buﬀers and quality-
of-service (QoS) assumptions (e.g., message delay and reliability). Consequently,
the domain expert has to encode these aspects manually, which is a complex and
error-prone task. Therefore, the model-driven method MechatronicUML [10]
deﬁnes a DSL for specifying coordination protocols of CPS at a more abstract
level. Among others, this DSL enables to specify hierarchical state machines,
real-time constraints, message buﬀers and the QoS assumptions of the proto-
col. Furthermore, MechatronicUML deﬁnes a domain query language to ease
the speciﬁcation of formal veriﬁcation properties that a coordination protocol
of MechatronicUML shall fulﬁll. For example, the requirement “At least one
instance per message type of the coordination protocol can be in transit” may be
speciﬁed as follows: forall(m : MessageTypes) EF messageInTransit(m).
In [9,15], we have achieved to fully hide the model checker Uppaal from the
domain expert by specifying domain-speciﬁc model checking for coordination
protocols of MechatronicUML using Uppaal. Our approach requires all six

328
S. Schivo et al.
Table 1. An overview of the case studies applying the proposed approach.
Sect. Domain
Domain Metamodels
Motivation for using
a model checker
Steps
of
the
approach
4.1
Cyber-
Physical
Systems
Protocol, Query
To verify whether a
coordination
proto-
col fulﬁlls all stated
properties
1, 2, 3, 4, 5, 6
4.2
Hardware-
Software
Co-Design
Synchronous Data Flow
Graph, Hardware Plat-
form, Allocation
To obtain a sched-
ule
for
the
execu-
tion of the tasks con-
sidering optimization
objectives of resource
and energy
1, 2, 5, 6
4.3
Cyber-
Security
Attack-Fault tree
To obtain a schedule
of attack steps opti-
mizing objectives like
time and cost, or sto-
chastic values, e.g.,
probability of attack
within mission time
1, 2, 5, 6
4.4
Reliability
Engineering
Attack-Fault tree
To obtain the prob-
ability
of
failure
within mission time
1, 2, 5
4.5
Software
Timing
Analysis
Java Bytecode, Timing
Analysis Extension
To
validate
Java
applications
to
ensure
that
they
fulﬁll
their
timing
speciﬁcations
1, 2, 5
steps that we introduce in Sect. 1. In particular, we assume that the coordination
protocol and its domain queries are speciﬁed in Steps 1 and 3. Then, in Step 2,
we transform a coordination protocol of MechatronicUML into a set of timed
automata that conform to uta. Moreover, in Step 4, we transform our domain
query language into properties that conform to uqu. We automate Uppaal
in Step 5 and parse the textual trace into a model that conforms to the utr
metamodel. Finally, in Step 6, we apply a model transformation to translate
the trace back to the level of MechatronicUML in order to show the trace
to the domain expert. We have implemented our concepts successfully into the
MechatronicUML Tool Suite.
4.2
Synchronous Dataﬂow Graphs
Hardware-software (HW-SW) co-design is an engineering approach to simulta-
neously design the hardware and software components of a system to meet opti-
mization objectives. Synchronous dataﬂow (SDF) graphs [25] are a frequently

How to Eﬃciently Build a Front-End Tool for UPPAAL
329
used formalism in the HW-SW co-design domain to represent streaming and
dataﬂow applications in terms of their computation tasks and the data rela-
tionships among them. Tasks are represented as nodes, and data input-output
relationships between these tasks are represented as edges. SDF graphs can be
used to calculate an (energy- or time-) optimal schedule of an application allo-
cated on a particular hardware platform.
In [1], we have applied the generic approach presented in this paper for
scheduling analysis of SDF graphs with an energy-optimization objective. Three
metamodels are introduced as domain metamodels: The SDF metamodel rep-
resenting SDF graphs, the hardware platform metamodel representing multi-
processor hardware platforms on which SDF graphs can be mapped, and the
allocation metamodel representing such mappings. The domain-speciﬁc model,
which consists of one instance of each metamodel, is transformed to a timed-
automata model and is analyzed with Uppaal CORA [4]. The trace resulting
from this analysis, which is an instance of the trace metamodel given in Sect. 3.3,
represents an energy-optimal schedule. In order to make the result available to
the domain experts, we have implemented a model transformation from trace
models to schedule models. Schedule models conform to the Schedule metamodel
(see Fig. 6) that we have developed and described below.
Schedule is the root of the metamodel. It consists of Executors, Executables and
Tasks. An Executor represents a processing unit (which is usually a processor or
a core) that executes a task. An Executable is a computation unit that can be
executed while a Task is one execution instance of an Executable. A Task has a
start time and an optional end time, which are both Time references. The end
time is optional since a Schedule may contain Tasks that have not ﬁnished.
4.3
Attack Tree Analysis
Modern day infrastructures are frequently faced with cyber attacks. A key chal-
lenge is to identify the most dangerous security vulnerabilities, estimate their
likelihood and prioritize investments to protect the system from the most riskful
scenarios. Security experts often model threat scenarios and perform quantitative
risk assessment using attack trees (ATs). These describe how atomic attack steps
(the tree leaves) combine into complex attacks (intermediate nodes, also called
gates), leading to the security breach represented by the root of the tree. Over the
years, numerous formalisms inspired by ATs have been proposed [22]. As they
Time
Schedule
Task
name : String
Executor
name : String
Executable
name : String
tasks
executors
executable
executor
1
1
startTime
endTime
1
value : Float
executables
0..*
0..*
0..*
Fig. 6. The Schedule metamodel.

330
S. Schivo et al.
all share the same basic structure, we have developed a metamodel [20] to sup-
port interoperability between the diﬀerent tools made to analyze attack trees.
Furthermore, as attack trees resemble fault trees, we enriched the AT meta-
model with fault tree constructs, resulting in the attack-fault tree (AFT) meta-
model [29].
A piece of the transformation from attack trees to Uppaal can be seen
in Fig. 7. This section produces the overall structure (i.e., system declaration
in Uppaal) from the class called AttackTree in the metamodel AFT. The
.equivalent() function transforms each node into an Uppaal template and
declaration, automatically selecting the transformation rule for that node.
rule Base transform at : AFT!AttackTree to out : Uppaal!NTA {
out.systemDeclarations = new Uppaal!SystemDeclarations();
out.systemDeclarations.system = new Uppaal!System();
var iList = new Uppaal!InstantiationList();
out.systemDeclarations.system.instantiationList.add(iList);
for (node : AFT!Node in at.Nodes) {
var converted = node.equivalent();
if (converted <> null) {
out.template.add(converted.get(0));
out.systemDeclarations.declaration.add(converted.get(1));
iList.template.add(converted.get(1).declaredTemplate);
}
}
out.addTopLevel(at.Root);
}
rule andGate transform node : AFT!Node to ret : List {
guard : node.nodeType.isKindOf(AFT!AND)
...
Fig. 7. Snippet of the translation from the Attack Tree metamodel to uta.
Traditional ATs are static, and their leaves are decorated with single
attributes like cost or time. In order to account for multiple attributes and
temporal dependencies we deﬁned transformations from AFT models to uta
models. The security properties that can be checked require either optimization,
like “What is the cost-optimal path taken by an attacker? [24]”, or the use of
stochastic values, like “What is the probability of an attack within m months?
[23]”. Similar to what we did for Synchronous Dataﬂow models, the results of
optimization queries are computed using Uppaal CORA. The outcome of such
analysis is a trace which is automatically parsed, obtaining a utr model. A trace
obtained from this analysis can additionally be transformed into a schedule, rep-
resented by an instance of the Schedule metamodel described in Sect. 4.2. The
adoption of MDE allows us to reuse the Schedule metamodel to describe results
from the attack tree domain, as they are semantically close to the SDF results.
The stochastic values are computed using Uppaal-SMC. Plotting these results
over time yields graphs similar to the one in Fig. 8.

How to Eﬃciently Build a Front-End Tool for UPPAAL
331
Currently, the optimization and stochastic security properties are expressed
as queries speciﬁc for Uppaal CORA and Uppaal-SMC, making them incom-
patible with the current query metamodel.
4.4
Fault Tree Analysis
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0
 2
 4
 6
 8
 10
Unreliability
Years
Fig. 8. Example plot of reliability
over time as produced by automatic
analysis of a fault tree using the
Uppaal-SMC metamodel.
As society becomes ever more dependent on
complex technological systems, the failure
of these systems can have disastrous conse-
quences. The ﬁeld of reliability engineering
uses various methods to analyze such sys-
tems, to ensure that they meet the required
high standards of dependability.
A popular formalism to perform such an
analysis is fault tree analysis. Faults trees
(FTs) are similar to attack trees (described
in Sect. 4.3), however rather than model-
ing deliberate steps in executing an attack,
they model component failures (called basic
events) that may combine to cause system failures or other undesired events.
Standard FTs were developed in the 1960 s and describe only boolean combi-
nations of faults. Since then, a large number of variations and extensions of fault
trees have been developed [30], covering aspects such as timing dependencies,
uncertainty, and maintenance. Most of these extensions were developed indepen-
dently and traditional tools do not support combinations. MDE simpliﬁes the
combination of models of diﬀerent kinds and the analysis of those aspects that
are shared between the diﬀerent formalisms.
Fault trees are described in a uniﬁed attack-fault tree (AFT) metamodel
also used for attack trees. The main diﬀerence from ATs is in the attributes
of the basic events. Where attack steps are controlled by an external attacker
who makes deliberate decisions based on factors such as cost, faults are inher-
ently stochastic in nature: The failure time is not externally decided, but rather
governed by a probability distribution attached to the fault.
The AFT metamodel supports basic events governed by hypoexponential
distributions, and gates from standard fault trees, dynamic fault trees [8] and
fault maintenance trees [29], as well as gates from attack trees.
As one of the analysis back-ends of the AFT metamodel, we provide a model
transformation to a uta model. Unlike most applications described in this paper,
the analysis of this model does not result in a trace or a schedule, nor can
its queries be expressed in the current query metamodel. Queries are usually
probabilistic in nature, asking questions such as “What is the probability of
the system failing within 5 y”. Results are then numeric values answering such
queries. While it is possible to extract a trace from an FT, its value is limited
due to the stochastic nature of the fault tree.

332
S. Schivo et al.
Instead, the typical use of the fault tree metamodel is to produce one
Uppaal-SMC model and automatically query the failure probability at dif-
ferent times. The results of these queries can than be used to produce a plot of
the system reliability over time, such as the one shown in Fig. 8.
4.5
Analysis of Java Programs
Model-based veriﬁcation techniques for software applications require the exis-
tence of expressive models. Typically, these models are derived manually, which
is a labor-intensive and error-prone task. Also, models need to be maintained
and kept consistent with the software application, lest they become outdated.
The framework we have introduced in [38] adopts the generic approach pre-
sented in this paper for automatically deriving timed-automata models to vali-
date Java applications, timing requirements in particular, using model checking.
In this framework, the bytecode metamodel [37] and its timing analysis exten-
sion are introduced as the domain metamodels. The instance of the bytecode
metamodel (bytecode model) is generated from the target Java application auto-
matically using the JBCPP plug-in. Following this, the bytecode metamodel is
enriched through a number of model transformations with additional informa-
tion necessary for analysis; this includes recursion handling, loop detection, loop
iteration bounding, timing information, etc. The additional information is repre-
sented as an instance of the extension metamodel. The enriched bytecode model
is then transformed to a uta model to be analyzed with Uppaal.
Queries are currently manually written and results of the model checking
process are not translated back to a domain-speciﬁc representation such as to
a source-code view. However, the implementation of these points using MDE is
suggested in the generic approach is a future direction of the study in [38].
5
Related Work
There are many studies that use Uppaal to verify systems. We limit this section
to the studies that automatically transform domain-speciﬁc models to timed
automata, or map the results of model checking back to the domain of interest.
The tool ANIMO (Analysis of Networks with Interactive MOdeling) [32] has
been introduced to analyze complex biological processes in living cells. ANIMO
transforms the domain-speciﬁc models deﬁned by biologists to Uppaal models;
then the results of the model checking process are presented back in a domain-
speciﬁc fashion. The transformations in ANIMO are implemented in a general-
purpose language, i.e., Java, whereas the case studies reported in this paper use
languages speciﬁcally designed for model management tasks.
Frost et al. [14] have introduced a tool for static analysis of timing properties
of Java programs. The tool transforms the domain-speciﬁc model, which consists
of the program, the virtual machine, and the hardware models, to an Uppaal
timed-automata model. The paper does not report any use of MDE techniques.

How to Eﬃciently Build a Front-End Tool for UPPAAL
333
A toolset to support design-space exploration of embedded systems was intro-
duced by Basten et al. [2]. It aims for the reuse of models between various
domains, by providing Java libraries to read design models written in its own
speciﬁcation language and then transform them for use with other tools includ-
ing Uppaal for design-space exploration. If the toolset needs to support a new
tool, one has to implement new transformations using these libraries. Using a
language not speciﬁcally designed for such transformations leads to challenges in
maintaining the toolset, which are in fact stated as a future direction of research.
In the study by Fakih et al. [13], a tool named sdf2ta has been introduced for
analyzing timing bounds of SDF graphs. The tool takes an SDF graph deﬁned
using the tool SDF3 and a hardware model deﬁned separately, and automatically
generates an Uppaal timed-automata model. Similar to our tooling choice, they
have used EMF for the implementation of sdf2ta, however, it is not reported how
the generation of the timed-automata model is achieved.
Herber and Glesner [19] proposed a framework to verify hardware-software
co-designs using timed automata. It translates the co-design implemented in
SystemC to Uppaal’s timed automata format. This translation is automatically
achieved by the SystemC Timed Automata Transformation Engine (STATE)
that is speciﬁcally designed for SystemC-to-Uppaal transformations. STATE is
implemented directly in Java, which limits interoperability with other tools.
In the work by Hartmanns and Hermanns [18], a toolset has been intro-
duced to facilitate the reuse of various model checkers targeting the stochastic
hybrid automata formalism. The toolset uses a high-level compositional model-
ing language that serves as an interoperability point among existing languages
and tools. Conceptually, this language is similar to a metamodel and the trans-
formations from/to this language are implemented using traditional compiler
techniques.
The study by Glatz et al. [16] uses model checking to test distributed control
systems. The authors mathematically deﬁne a mapping from concepts in the
control systems domain to the timed-automata domain. In their approach, they
suggest implementing this mapping as a translation between the XML formats
of these domains, which can be seen as a textual model-based transformation.
6
Conclusions
We have demonstrated the use of MDE in the development of software bridging
tools that use Uppaal as a back-end analysis tool. Our approach uses metamod-
els as the foundation to translate domain-speciﬁc concepts into timed-automata
models and queries; the results delivered by Uppaal are similarly translated
back to the original domain, providing experts with access to formal analysis
techniques without requiring additional training. We have presented ﬁve case
studies in diﬀerent domains to demonstrate how our approach has been applied
in practice with the aim of a higher level of interoperability, faster software
development and easier maintainability.

334
S. Schivo et al.
The principles we have presented here can be applied to formalisms and
analysis tools diﬀerent from timed automata and Uppaal by replacing the cen-
tral metamodels uta, uqu and utr with suitable counterparts. Thus we expect
our approach to be generally applicable in the development of more software
bridging tools which act between DSLs and formal methods.
Acknowledgements. This research was partially funded by STW and ProRail under
the project ArRangeer (grant 12238), STW project SEQUOIA (15474), NWO projects
BEAT (612001303) and SamSam (628.005.015), and EU project SUCCESS.
References
1. Ahmad, W., Yildiz, B.M., Rensink, A., Stoelinga, M.: A model-driven framework
for hardware-software co-design of dataﬂow applications. In: Berger, C., Mousavi,
M.R., Wisniewski, R. (eds.) CyPhy 2016. LNCS, vol. 10107, pp. 1–16. Springer,
Cham (2017). doi:10.1007/978-3-319-51738-4 1
2. Basten, T., Hamberg, R., Reckers, F., Verriet, J.: Model-Based Design of Adaptive
Embedded Systems. Springer Publishing Company, New York (2013). doi:10.1007/
978-1-4614-4821-1
3. Behrmann, G., David, A., Larsen, K.G., H˚akansson, J., Petterson, P., Yi, W.,
Hendrink, M.: Uppaal 4.0. In: Proceedings of 3rd International Conference on
Quantitative Evaluation of Systems (QEST), pp. 125–126 (2006). https://doi.org/
10.1109/QEST.2006.59
4. Behrmann, G., Larsen, K.G., Rasmussen, J.I.: Optimal scheduling using priced
timed automata. SIGMETRICS Perform. Eval. Rev. 32(4), 34–40 (2005)
5. Bengtsson, J., Yi, W.: Timed automata: semantics, algorithms and tools. In: Desel,
J., Reisig, W., Rozenberg, G. (eds.) ACPN 2003. LNCS, vol. 3098, pp. 87–124.
Springer, Heidelberg (2004). doi:10.1007/978-3-540-27755-2 3
6. Brandt, J.: Understanding attacks: modeling the outcome of attack tree analysis.
In: 25th Twente Student Conference on IT, vol. 25. University of Twente (2016),
BSc. Thesis; see. http://referaat.cs.utwente.nl/conference/25/paper
7. Bulychev, P., David, A., Larsen, K.G., Miku˘cionis, M., Poulsen, D.B., Legay, A.,
Wang, Z.: Uppaal-SMC: statistical model checking for priced timed automata. In:
Proceedings of 10th Wks. Quantitative Aspects of Programming Languages (2012).
https://doi.org/10.4204/EPTCS.85.1
8. Dugan, J.B., Bavuso, S.J., Boyd, M.A.: Fault trees and sequence dependencies. In:
Proceedings of Annual Reliability and Maintainability Symposium, pp. 286–293,
January 1990
9. Dziwok, S., Gerking, C., Heinzemann, C.: Domain-speciﬁc Model Checking of
MechatronicUML Models Using Uppaal. Technical report tr-ri-15-346, Paderborn
University, Jul 2015. https://www.hni.uni-paderborn.de/pub/9121
10. Dziwok, S., Pohlmann, U., Piskachev, G., Schubert, D., Thiele, S., Gerking, C.: The
mechatronicUML design method: process and language for platform-independent
modeling. Technical report tr-ri-16-352, Software Engineering Department, Fraun-
hofer IEM / Software Engineering Group, Heinz Nixdorf Institute , version 1.0,
December 2016
11. Eclipse foundation Inc: XTend - modernized Java. https://www.eclipse.org/xtend/
index.html

How to Eﬃciently Build a Front-End Tool for UPPAAL
335
12. Emerson, E.A., Clarke, E.M.: Using branching time temporal logic to synthesize
synchronization skeletons. Sci. Comput. Program. 2(3), 241–266 (1982)
13. Fakih, M., Gr¨uttner, K., Fr¨anzle, M., Rettberg, A.: State-based Real-time analysis
of SDF applications on MPSoCs with shared communication resources. J. Syst.
Archit. 61(9), 486–509 (2015)
14. Frost, C., Jensen, C., Luckow, K.S., Thomsen, B.: WCET analysis of java bytecode
featuring common execution environments. In: Proceedings of 9th International
Wks. Java Technologies for Real-Time and Embedded Systems, pp. 30–39. ACM
(2011)
15. Gerking, C., Sch¨afer, W., Dziwok, S., Heinzemann, C.: Domain-speciﬁc model
checking for cyber-physical systems. In: Proceedings of 12th Wks. Model-Driven
Engineering, Veriﬁcation and Validation (MoDeVVa 2015). Ottawa, September
2015
16. Glatz, B., Cleary, F., Horauer, M., Schuster, H., Balog, P.: Complementing test-
ing of IEC61499 function blocks with model-checking. In: Proceedings of 12th
IEEE/ASME International Conference on Mechatronic, Embedded Systems and
Applications (MESA) (2016)
17. Greenyer, J., Rieke, J.: Applying advanced TGG concepts for a complex transfor-
mation of sequence diagram speciﬁcations to timed game automata. In: Sch¨urr, A.,
Varr´o, D., Varr´o, G. (eds.) AGTIVE 2011. LNCS, vol. 7233, pp. 222–237. Springer,
Heidelberg (2012). doi:10.1007/978-3-642-34176-2 19
18. Hartmanns, A., Hermanns, H.: The modest toolset: an integrated environment
for quantitative modelling and veriﬁcation. In: ´Abrah´am, E., Havelund, K. (eds.)
TACAS 2014. LNCS, vol. 8413, pp. 593–598. Springer, Heidelberg (2014). doi:10.
1007/978-3-642-54862-8 51
19. Herber, P., Glesner, S.: A HW/SW co-veriﬁcation framework for systemC. ACM
TECS 12(1s), 61:1–61:23 (2013)
20. Huistra, D.: A unifying model for attack trees. Research Project. University of
Twente (2015). http://essay.utwente.nl/69399/
21. Kolovos, D.S., Paige, R.F., Polack, F.A.C.: The epsilon transformation language.
In: Vallecillo, A., Gray, J., Pierantonio, A. (eds.) ICMT 2008. LNCS, vol. 5063,
pp. 46–60. Springer, Heidelberg (2008). doi:10.1007/978-3-540-69927-9 4
22. Kordy, B., Pi`etre-Cambac´ed`es, L., Schweitzer, P.: DAG-based attack and defense
modeling: don’t miss the forest for the attack trees. Comput. Sci. Rev. 13–14,
1–38 (2014)
23. Kumar, R., Stoelinga, M.: Quantitative security and safety analysis with attack-
fault trees. In: Proceedings of IEEE 18th International Symposium High Assurance
Systems Engineering (HASE), pp. 25–32, January 2017
24. Kumar, R., Ruijters, E., Stoelinga, M.: Quantitative attack tree analysis via
priced timed automata. In: Sankaranarayanan, S., Vicario, E. (eds.) FOR-
MATS 2015. LNCS, vol. 9268, pp. 156–171. Springer, Cham (2015). doi:10.1007/
978-3-319-22975-1 11
25. Lee, E.A., Messerschmitt, D.G.: Synchronous data ﬂow. Proc. IEEE 75(9), 1235–
1245 (1987)
26. Mohagheghi, P., Dehlen, V.: Where Is the proof? - a review of experiences from
applying MDE in industry. In: Schieferdecker, I., Hartman, A. (eds.) ECMDA-
FA 2008. LNCS, vol. 5095, pp. 432–443. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-69100-6 31
27. Object
Management
Group
(OMG):
Meta
Object
Facility
(MOF)
2.0
Query/View/Transformation Speciﬁcation, Version 1.2. OMG Document Number
formal/01 Feb 2015. http://www.omg.org/spec/QVT/1.2

336
S. Schivo et al.
28. Ravn, A.P., Srba, J., Vighio, S.: A formal analysis of the web services atomic
transaction protocol with UPPAAL. In: Margaria, T., Steﬀen, B. (eds.) ISoLA
2010. LNCS, vol. 6415, pp. 579–593. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-16558-0 47
29. Ruijters, E., Guck, D., Drolenga, P., Stoelinga, M.: Fault maintenance trees: reli-
ability contered maintenance via statistical model checking. In: Proceedings IEEE
62nd Annual Reliability and Maintainability Symposium (RAMS). IEEE, January
2016
30. Ruijters, E., Stoelinga, M.: Fault tree analysis: a survey of the state-of-the-art in
modeling, analysis and tools. Comput. Sci. Rev. 15–16, 29–62 (2015)
31. Schivo, S., Scholma, J., Wanders, B., Camacho, R.A.U., van der Vet, P.E., Karperien,
M., Langerak, R., van de Pol, J., Post, J.N.: Modeling Biological Pathway Dynamics
With Timed Automata. IEEE J. Biomed. Health Inform. 18(3), 832–839 (2014)
32. Schivo, S., Scholma, J., van der Vet, P.E., Karperien, M., Post, J.N., van de Pol,
J., Langerak, R.: Modelling with ANIMO: between fuzzy logic and diﬀerential
equations. BMC Syst. Biol. 10(1), 56 (2016)
33. da Silva, A.R.: Model-driven engineering: A survey supported by the uniﬁed con-
ceptual model. Comput. Languages, Systems & Structures 43, 139–155 (2015)
34. Sprinkle, J., Rumpe, B., Vangheluwe, H., Karsai, G.: Metamodelling. In: Model-
Based Engineering of Embedded Real-Time Systems, pp. 57–76. Springer (2010)
35. Steinberg, D., Budinsky, F., Paternostro, M., Merks, E.: EMF: Eclipse modeling
framework 2.0. Addison-Wesley Professional, 2nd edn. (2009)
36. V¨olter, M., Stahl, T., Bettin, J., Haase, A., Helsen, S.: Model-driven software
development: technology, engineering, management. John Wiley & Sons (2006)
37. Yildiz, B.M., Bochisch, C.M., Rensink, A., Aksit, A.: An MDE approach for mod-
ular program analyses. In: Proc. Modularity in Modelling Workshop (2017)
38. Yildiz, B.M., Rensink, A., Bockisch, C., Aksit, M.: A Model-Derivation Framework
for Software Analysis. In: Proc. 2nd Wks. Models for Formal Analysis of Real
Systems (MARS) (2017)

PranCS: A Protocol and Discrete Controller
Synthesis Tool
Idress Husien, Sven Schewe, and Nicolas Berthier(B)
Department of Computer Science, University of Liverpool, Liverpool, UK
nicolas.berthier@liverpool.ac.uk
Abstract. PranCS is a tool for synthesizing protocol adapters and dis-
crete controllers. It exploits general search techniques such as simulated
annealing and genetic programming for homing in on correct solutions,
and evaluates the ﬁtness of candidates by using model-checking results.
Our Proctocol and Controller Synthesis (PranCS) tool uses NuSMV as
a back-end for the individual model-checking tasks and a simple candi-
date mutator to drive the search.
PranCS is also designed to explore the parameter space of the search
techniques it implements. In this paper, we use PranCS to study the
inﬂuence of turning various parameters in the synthesis process.
1
Introduction
Discrete Controller Synthesis (DCS) and Program Synthesis have similar goals:
they are automated techniques to infer a control strategy and an implementation,
respectively, that is correct by construction.
There are mild diﬀerences between these two classes of problems. DCS typ-
ically operates on the model of a plant. It seeks the automated construction of
a strategy to control the plant, such that its runs satisfy a set of given objec-
tives [2,22]. Similarly, program synthesis seeks to infer an implementation, often
of a reactive system, such that the runs of this system satisfy a given speciﬁ-
cation [21]. Program synthesis is particularly attractive for the construction of
protocols that govern the intricate interplay between diﬀerent threads; we use
mutual exclusion and leader election as examples.
Apart from their numerous applications to manufacturing systems [19,22,
24], DCS algorithms have been used to enforce fault-tolerance [11], deadlock
avoidance in multi-threaded programs [23], and correct resource management in
embedded systems [1,3].
Foundations of DCS and program synthesis are similar to principles of model-
checking [5,8]. Model-checking refers to automated techniques that determines
whether or not a system satisﬁes a number of speciﬁcations. Traditional DCS
algorithms are inspired by this approach. Given a model of the plant, they ﬁrst
exhaustively compute an unsafe portion of the state-space to avoid for the desired
This work was supported by the Ministry of Higher Education in Iraq through the
University of Kirkuk and by the EPSRC through grant EP/M027287/1.
c
⃝Springer International Publishing AG 2017
K.G. Larsen et al. (Eds.): SETTA 2017, LNCS 10606, pp. 337–349, 2017.
https://doi.org/10.1007/978-3-319-69483-2_20

338
I. Husien et al.
objectives to be satisﬁed, and then derive a strategy that avoids entering the
unsafe region. Finally, a controller is built that restricts the behaviour of the
plant according to this strategy, so that it is guaranteed to always comply with
its speciﬁcation. Just as for model-checking, symbolic approaches for solving
DCS problems have been successfully investigated [2,4,10,20].
Techniques based on genetic programming [7,12–17], as well as on simu-
lated annealing [13,14], have been tried for program synthesis. Instead of per-
forming an exhaustive search, these techniques proceed by using a measure of
the ﬁtness—reﬂecting the question “How close am I to satisfying the speciﬁ-
cation?”—to ﬁnd a short path towards a solution. Among the generic search
techniques that look promising for this approach, we focus on genetic program-
ming [18] and simulated annealing [7,12]. When applied to program synthesis,
both search techniques work by successively mutating candidate programs that
are deemed “good” by using some measure of their ﬁtness. We obtain their ﬁt-
ness for meeting the desired objectives by using a model-checker to measure the
share of objectives that are satisﬁed by the candidate program, cf. [13,14,16,17].
Simulated annealing keeps one candidate solution, and a “cooling schedule”
describes the evolution of a “temperature”. In a sequence of iterations, the algo-
rithm mutates the current candidate and compares the ﬁtness of the old and
new candidate. If the ﬁtness increases, the new candidate is always maintained.
If it decreases, a random process decides if the new candidate replaces the old
one in the next iteration. The chances of the new candidate to replace the old
one then decrease with the gap in the ﬁtness and increase with the temperature;
thus, a lower temperature makes the system “stiﬀer”.
Genetic programming maintains a population of candidate programs over a
number of iterations. In each iteration, new candidate programs are generated
by mutation or by mixing randomly selected candidates (“crossover”). At the
end of each iteration, the number of candidates under consideration is shrunken
back to the original number. A higher ﬁtness makes it more likely for a candidate
to survive this step.
In Sect. 2, we describe the tool PranCS, which implements the simulated
annealing based approach proposed in [13,14] as well as approaches based on
similar genetic programming from [16,17]. PranCS uses quantitative measures
for partial compliance with a speciﬁcation, which serve as a measure for the
ﬁtness (or: quality) of a candidate solution. Furthering on the comparison of
simulated annealing with genetic programming [13,14], we extend the quest for
the best general search technique in Sect. 3 by:
1. looking for good cooling schedules for simulated annealing; and
2. investigating the impact of the population size and crossover ratio for genetic
programming.
2
Overview of PranCS
PranCS implements several generic search algorithms that can be used for solving
DCS problems as well as for synthesising programs.

PranCS: A Protocol and Discrete Controller Synthesis Tool
339
2.1
Representing Candidates
The representation of candidates depends on the kind of problems to solve.
Candidate programs are represented as abstract syntax trees according to the
grammar of the sought implementation. They feature conditional and iteration
statements, assignments to one variable taken among a given set, and expressions
involving such variables. Candidates for DCS only involve a series of assignments
to a given subset of Boolean variables involved in the system (called “control-
lables”).
2.2
Structure of PranCS
The structure of PranCS is shown in Fig. 1. Via the user interface, the user can
select a search technique, and enter the problem to solve along with values for
relevant parameters of the selected algorithm. For program synthesis, the user
enters the number, size, and type of variables that candidate implementations
may use, and whether thay may involve complex conditional statements (“if”
and “while” statements). DCS problems are manually entered as a series of
assignments to state variables involving expressions expressed on state and input
variables; the user also lists the subset of input variables that are “controllable”.
In both cases, the user also provides the speciﬁcation as a list of objectives.
User
Interface
Generator
Translator
Fitness
Measure
NuSMV
Search
Tech-
nique
parameters
properties
Candidate
Model
Output
properties
Fitness
Output
candidate
update
Fig. 1. Overview of PranCS.
Generator. The Generator uses the
parameters provided to either gen-
erate new candidates or to update
them
when
required
during
the
search.
Translator
&
NuSMV.
We
use
NuSMV
[6]
as
a
model-checker.
Every candidate is translated into
the modelling language of NuSMV
using a method suggested by Clark
and Jacob [7]. (We detail this trans-
lation for programs and plants in [14]
and [13] respectively, and give an example program translation in Appendix A.)
The resulting model is then model-checked against the desired properties. The
result forms the basis of a ﬁtness function for the selected search technique.
Fitness Measure. To design a ﬁtness measure for candidates, we make the
hypothesis that the share of objectives that are satisﬁed so far by a candidate is a
good indication of its suitability w.r.t. the desired speciﬁcation. We additionally
observe that weaker properties that can be mechanically derived are useful to
identify good candidates worth selecting for the generation of further potential
solutions. For example, if a property shall hold on all paths, it is better if it
holds on some path, and even better if it holds almost surely.

340
I. Husien et al.
Search Technique. The ﬁtness measure obtained for a candidate is used as a
ﬁtness function for the selected search technique. If a candidate is evaluated as
correct, we return (and display) it to the user. Otherwise, depending on the
search technique selected and the old and new ﬁtness measure/s, the current
candidate or population is updated, and one or more candidates are sent for
change to the Generator. The process is re-started if no solution has been found
in a predeﬁned number of steps (genetic programming) or when the cooling
schedule expires (simulated annealing).
2.3
Selecting and Tuning Search Techniques
In terms of search techniques, PranCS implements the following methods: genetic
programming, and simulated annealing. Katz and Peled [17] extend genetic pro-
gramming by considering the ﬁtness as a pair of “safety-ﬁtness” and “liveness-
ﬁtness”, where the latter is only used for equal values of “safety-ﬁtness”. Building
upon this idea, we deﬁne two ﬂavours for both simulated annealing and genetic
programming: rigid (where the classic ﬁtness function is used) and safety-ﬁrst,
which uses the two-step ﬁtness approach as above. Further, genetic programming
can be used with or without crossovers between candidates [13,14].
Fig. 2. Graphical User Interface. PranCS allows the user
to ﬁne-tune each search technique by means of dedicated
parameters.
Depending on the
selected search tech-
nique, the tool allows
the user to input para-
meters
that
control
the dynamics of the
synthesis process. These
parameters determine
the likelihood of ﬁnd-
ing a correct program
in each iteration and
the expected running
time for each iteration,
and thus heavily inﬂu-
ence the overall search
speed. For the genetic
programming approach, the parameters include the population size, the number
of selected candidates, the number of iterations, and the crossover ratio. For
simulated annealing, the user chooses the initial temperature and the cooling
schedule. Figure 2 shows the graphical user interface of PranCS.
Parameters for Simulated Annealing. In simulated annealing (SA), the intuition
is that, at the beginning of the search phase, the temperature is high, and it
cools down as time goes by. The higher the temperature, the higher is the like-
lihood that a new candidate solution with inferior ﬁtness replaces the previous
solution. While this allows for escaping local minima, it can also happen that
the candidates develop into an undesirable direction. For this reason, simulated

PranCS: A Protocol and Discrete Controller Synthesis Tool
341
annealing does not continue for ever, but is re-started at the end of the cooling
schedule. Consequently, there is a sweet-spot in just how long a cooling sched-
ule should be and when it becomes preferable to re-start, but this sweet-spot is
diﬃcult to ﬁnd. We report our experiments with PranCS for tuning the cooling
schedule in Sect. 3.1.
Parameters for Genetic Programming. For Genetic Programming (GP), the
parameters are the initial population size, the crossover vs mutation ratio, and
the ﬁtness measure used to select the individuals. The population size aﬀects the
algorithm in two ways: a larger population size could provide better diversity
and reduce the number of iterations required or, for a ﬁxed number of iterations,
increase the likelihood of ﬁnding a solution. However, it also increases the time
spent for each individual iteration. The crossover ratio describes the amount of
new candidates that are generated by mating. Crossovers allow for the appear-
ance of solutions that synthesise the best traits of good candidates, and a high
crossover ratio promises to make this more likely. This requires, however, a high
degree of diversity in the population, where these traits need to draw from dif-
ferent parts of the program tree, and it comes to the cost of creating diversity
through a reduction of the number of mutations applied in each iteration.
We investigate how the population size and crossover ratio aﬀect the perfor-
mance of these algorithms in Sects. 3.2 and 3.3.
3
Exploration of the Parameter Space
Besides serving as a synthesis tool, PranCS provides the user with the ability
to compare various search techniques. In [13,14], we have carried out experi-
ments by applying our algorithms to generate correct solutions on benchmarks
comprising mutual exclusion, leader election, and DCS problems of growing size
and complexity. With parameter values borrowed from [16,17], we could already
accelerate synthesis signiﬁcantly using simulated annealing compared to genetic
programming (by 1.5 to 2 orders of magnitude).
In this paper, our aim is to further explore the performance impact of the
parameters for each search technique. We thus reuse the same scalable bench-
marks as in [13,14]: program synthesis problems consist of mutual exclusion (“2
or 3 shared bits”) and leader election (“3 or 4 nodes”); DCS problems com-
pute controllers enforcing mutual exclusions and progress between 1 to 6 tasks
modelled as automata (“1 through 6-Tasks”).
In all Tables, execution times are in seconds; t is the mean execution time of
single executions (succeeding or failing), and columns T extrapolate t based on
the success rate obtained in 100 single executions (columns “%”).
3.1
Exploring Cooling Schedules for Simulated Annealing
In order to test if the hypothesis from [9] that simulated annealing does most of
its work during the middle stages—while being in a good temperature range—

342
I. Husien et al.
holds for our application, we have developed the tool to allow for “cooling sched-
ules” that do not cool at all, but use a constant temperature. In order to be com-
parable to the default strategy, we use up to 25,001 iterations in each attempt.
We have run 100 attempts to create a correct candidate using various con-
stant temperatures, and inferred expected overall running times T based on the
success rates and average execution time of single executions t. We ﬁrst report the
results for program synthesis and DCS problems in Tables 1 and 2 respectively.
Table 1. Impact of search temperature (θ) for program synthesis with safety-ﬁrst
simulated annealing
θ
3 nodes
4 nodes
2 shared bits
3 shared bits
t
%
T
t
%
T
t
%
T
t
%
T
0.7
316
0 ∞
521
0 ∞
147
0 ∞
155
0 ∞
400
285
0 ∞
493
0 ∞
143
0 ∞
148
0 ∞
4,000
196 11 1,781 368 10 3,680 129
3 4,300 121
4 3,025
7,000
97 14 692
314 13 2,415
77 12 641
81 11 252
10,000
73 21 347
138 18 766
15 22 68
17 24 70
13,000
78 22 354
146 19 768
16 23 69
18 24 75
16,000
83 20 415
150 17 882
17 21 80
19 22 86
20,000
87 19 457
153 15 1,020
21 20 105
23 22 104
25,000
94 17 494
167 13 1,284
23 19 121
25 21 191
30,000
108 15 720
184 11 1,672
28 18 155
30 19 157
40,000
117 15 780
193 11 1,754
31 16 193
34 17 200
50,000
129 13 992
201 10 2,010
37 15 246
41 16 256
100,000 193 12 1,608 287
9 3,188
52 11 472
58 13 446
The ﬁndings support the hypothesis that some temperatures are much better
suited than others: low temperatures provide a very small chance of succeeding,
and the chances also go down at the high temperature end.
While the values for low temperatures are broadly what we had expected,
the high end performed better than we had thought. This might be because
some small guidance is maintained even for inﬁnite temperature, as a change
that is decreasing the ﬁtness is taken with an (almost) 50% chance in this case,
while increases are always selected. However, the ﬁgures for high temperatures
are much worse than the ﬁgures for the good temperature range of 10,000 to
16,000.
In the majority of cases, the best results have been obtained at a temperature
of 10,000. Notably, these results are better than the running time for the cooling
schedule that uses a linear decline in the temperature as used and reported in [13,
14]. They indicate that it seems likely that the last third of the improvement
cycles in this cooling schedule had little avail, especially for smaller problems.

PranCS: A Protocol and Discrete Controller Synthesis Tool
343
Table 2. Impact of search temperature (θ) for DCS with Safety-ﬁrst simulated
annealing
1-Task
2-Tasks
3-Tasks
4-Tasks
5-Tasks
6-Tasks
θ
t
% T
t
% T
t
% T
t
% T
t
% T
t
% T
0.7
163
0 ∞
177
0 ∞
192
0 ∞
332
0 ∞
298
0 ∞
613 0
∞
400
93
0 ∞
99
0 ∞
163
0 ∞
167
0 ∞
153
0 ∞
598 0
∞
4,000
54
7 771
58
6 966
88
6 1,466
98
3 3,266
98
4 2,450 278 3
9,266
7,000
39 12 325
47
9 522
45
9 500
65
6 1,083
79
6 1,316 125 5
2,500
10,000
18 19 94
29 14 207
26 11 236
39
9 433
61
9 677
99 8
1,237
13,000
22 20 110
33 15 220
31 11 281
43 11 390
67 10 670
115 9
1,277
16,000
29 19 152
39 13 300
37 10 370
58
9 644
73
8 912
127 9
1,411
20,000
37 17 217
47 11 427
42 10 420
67
9 744
81
6 1,350 134 7
1,914
25,000
43 15 286
56 10 560
47
9 522
81
7 1,157
89
6 1,483 152 6
2,533
30,000
49 15 326
67 10 670
56
8 700
89
6 1,483 102
4 2,550 159 6
2,650
40,000
53 13 407
75
9 833
63
9 700
95
6 1,583 116
3 3,866 168 6
2,800
50,000
59 12 491
82
7 1,171
79
7 1,128 103
5 2,060 128
4 3,200 192 5
3,840
100,000
72 11 654
94
7 1,342
98
7 1,400 118
4 2,950 178
3 5,933 253 4
6,325
A robust temperature sweet-spot clearly exists for our scalable benchmarks,
suggesting that the quest for robust and generic good cooling schedules is worth
pursuing.
3.2
Impact of Population Size for Genetic Programming
One of the important parameters of genetic programming is the initial population
size; another parameter worth tuning is the number of candidates η selected for
mating at each iteration of the algorithm. In order to investigate their eﬀects on
our synthesis approach and evaluate the actual cost of large population sizes,
we deﬁned several setups with various values for the population size |P| and
amount of mating candidates η. We then performed 100 executions of our GP-
based algorithms with each of these setups for the 2 shared bits mutual exclusion
and 2-Tasks problems.
We show the results in Tables 3 and 4. As expected, increasing the size of the
initial population also dramatically increases the cost of ﬁnding a good solution.
Broadly speaking, increasing the population size reduces the number of iterations
and increases the success rate, but it also increases the computation time required
at each individual iteration. Smaller population sizes appear to beneﬁt individual
running times more than they harm success rates.
The impact of η on performance appears very limited on the range we have
investigated.

344
I. Husien et al.
Table 3. Impact of population size (|P|) for Program Synthesis (2 shared bits mutual
exclusion only)
Rigid GP
Safety-ﬁrst GP
w/o crossover
with crossover
w/o crossover with crossover
|P|
η t
%
T
t
%
T
t
%
T
t
%
T
150 5 583
7
8,328 589
9
6,544 113 31 364
115 33 348
7 583
7
8,328 589
9
6,544 113 31 364
115 33 348
9 584
7
8,342 588
9
6,533 113 31 364
114 33 345
250 5 1,024 12 8,533 1,057 15 7,046 230 46 500
245 49 500
7 1,024 12 8,533 1,057 15 7,046 230 46 500
245 49 500
9 1,024 12 8,533 1,057 15 7,046 231 46 502
245 49 500
350 5 1,435 15 9,566 1,451 18 8,061 325 63 515
367 67 547
7 1,435 15 9,566 1,451 18 8,061 325 63 515
366 67 546
9 1,435 15 9,566 1,451 19 7,636 325 64 507
367 67 547
Table 4. Impact of population size (|P|) for DCS (2-Tasks only)
Rigid GP
Safety-ﬁrst GP
w/o crossover
with crossover
w/o crossover
with crossover
|P|
η t
% T
t
%
T
t
%
T
t
%
T
150 5 463
3
15,433 484
4
12,100 132 13 1,015 138 15 920
7 463
3
15,433 485
4
12,125 132 13 1,015 139 15 926
9 464
3
15,466 485
4
12,125 131 13 1,007 139 14 992
250 5 943
5
18,860 969
7
13,842 241 18 1,338 218 19 1,147
7 943
5
18,860 969
7
13,842 241 18 1,338 218 19 1,147
9 943
5
18,860 969
7
13,842 242 18 1,344 218 19 1,147
350 5 1,517 9
16,855 1,557 10 15,570 403 24 1,679 340 24 1,416
7 1,517 9
16,855 1,557 10 15,570 403 24 1,679 340 24 1,416
9 1,518 9
16,866 1,557 10 15,570 403 24 1,679 340 24 1,416
3.3
Impact of Crossover Ratio for Genetic Programming
Finally, we have also studied the eﬀect of changing the share between crossover
and mutation in genetic programming.
We report our results in Tables 5 and 6. Interestingly, the running time per
instance increased with the share of crossovers, which might point to a produc-
tion of more complex candidate solutions. Regarding expected running times,
the results also indicate the existence of a sweet-spot for the crossover ratio at
around 20% for both Rigid and Safety-ﬁrst variants of the algorithm.

PranCS: A Protocol and Discrete Controller Synthesis Tool
345
Table 5. Impact of crossover ratio (ρ, in percent) for Program Synthesis with Rigid
and Safety-ﬁrst GP
Rigid GP
Safety-ﬁrst GP
ρ
t
%
T
t
%
T
2 shared bits
0
583
7
8,328
113
31
364
20
589
9
6,544
115
33
348
40
602
9
6,688
123
33
372
60
614
8
7,657
134
33
406
80
613
8
7,662
142
21
676
100
652
2
32,600
151
5
3,020
3 shared bits
0
615
7
8,785
171
17
1,005
20
620
9
6,888
175
19
921
40
637
9
7,077
187
19
984
60
658
8
8,225
196
19
1,031
80
669
4
16,725
207
11
1,881
100
682
2
34,100
223
3
7,433
3 nodes
0
1,120
3
37,333
418
15
2,786
20
1,123
6
18,716
421
16
2,631
40
1,137
5
22,740
427
16
2,668
60
1,149
5
22,980
453
13
3,484
80
1,154
3
38,466
469
9
5,211
100
1,167
2
58,350
487
4
12,175
4 nodes
0
1,311
3
43,700
536
11
4,872
20
1,314
5
26,280
541
14
3,864
40
1,325
4
33,125
557
13
4,284
60
1,336
3
44,533
569
13
4,376
80
1,345
3
44,833
581
9
6,455
100
1,353
2
67,650
593
3
17,966
Table 6. Impact of crossover ratio (ρ, in percent) for DCS with Rigid and Safety-
ﬁrst GP
Rigid GP
Safety-ﬁrst GP
ρ
t
%
T
t
%
T
1-Task
0
378
4
9,450
89
17
523
20
385
5
7,700
94
20
470
40
403
5
8,060
101
19
531
60
418
4
10,450
109
19
573
80
425
3
14,166
116
12
966
100
438
1
43,800
124
5
2,480
(continued)

346
I. Husien et al.
Table 6. (Continued)
Rigid GP
Safety-ﬁrst GP
ρ
t
% T
t
%
T
2-Tasks
0
475
3
15,833
127
13
976
20
484
4
12,100
138
15
920
40
491
4
12,275
146
15
973
60
501
3
16,700
158
13
1,215
80
509
2
25,450
169
11
1,536
100
521
1
52,100
181
4
4,525
3-Tasks
0
571
3
19,033
189
9
2,100
20
589
4
14,725
201
11
1,827
40
597
3
19,900
209
11
1,900
60
606
3
20,200
217
8
2,712
80
613
1
61,300
225
7
3,214
100
627
1
62,700
239
3
7,966
4-Tasks
0
658
3
21,933
288
9
3,200
20
664
4
16,600
296
12
2,466
40
679
4
16,975
303
11
2,754
60
687
3
22,900
313
10
3,130
80
693
2
34,650
321
8
4,012
100
711
1
71,100
333
4
8,325
5-Tasks
0
776
1
77,600
438
7
6,257
20
787
3
26,233
445
11
4,045
40
792
3
26,400
451
8
5,637
60
799
2
39,950
459
7
6,557
80
804
2
40,200
467
5
9,340
100
815
1
81,500
479
2
23,950
6-Tasks
0
961
2
48,050
659
6
10,983
20
972
3
32,400
673
10
6,730
40
981
2
49,050
679
10
6,790
60
989
2
49,450
695
7
9,928
80
997
2
49,850
703
4
17,575
100
1,011
1
101,100
718
2
35,900
4
Conclusion
Together with our extensive exploration of the parameter space, the evaluation
of PranCS indicates that simulated annealing is faster than genetic programming
(we report some synthesis times with the best parameters observed using simu-
lated annealing in Table 7), and that some temperature ranges are more useful
than others. Additional information about the tool can be found at: https://cgi.
csc.liv.ac.uk/∼idresshu/index2.html.

PranCS: A Protocol and Discrete Controller Synthesis Tool
347
Table 7. Synthesis times with the best parameters observed for Simulated Annealing
with linearly decreasing cooling schedule applied to our DCS benchmarks; results for
row “2-Tasks” should be compared with best results reported in Table 4 for solving the
same DCS benchmark problem using GP-based algorithms.
Rigid SA
Safety-ﬁrst SA
t
%
T
t
%
T
1-Task
20
13 153
19
16 118
2-Tasks 25
10 250
24
13 184
3-Tasks 33
9
366
29
10 290
4-Tasks 47
9
522
43
9
477
5-Tasks 76
8
950
70
9
777
6-Tasks 119 7
1,700 106 7
1,514
In order to integrate this result into the cooling schedule we plan to use an
adaptive cooling schedule, in which the decrements of the temperature depends
on the improvement of the ﬁtness.
Appendix A Pseud-Code to NuSMV Translation Example
To evaluate the ﬁtness of the produced program, it is ﬁrst translated into the
language of the model checker NuSMV [6]. We have used the translation method
suggested by Clark and Jacob [7].
In this translation, the program is converted into very simple statements,
similar to assembly language. To simplify the translation, the program lines
Fig. 3. Translation example – source pseudo-code (left) and target NuSMV (right)

348
I. Husien et al.
are ﬁrst labeled, and this label is then used as a pointer that represents the
program counter (PC). From this intermediate language, the NuSMV model is
built by creating (case) and (next) statements that use the PC. Figure 3 shows
the translation of a mutual exclusion algorithm.
References
1. Altisen, K., Clodic, A., Maraninchi, F., Rutten, E.: Using controller-synthesis tech-
niques to build property-enforcing layers. In: Degano, P. (ed.) ESOP 2003. LNCS,
vol. 2618, pp. 174–188. Springer, Heidelberg (2003). doi:10.1007/3-540-36575-3 13
2. Asarin, E., Maler, O., Pnueli, A.: Symbolic controller synthesis for discrete
and timed systems. In: Antsaklis, P., Kohn, W., Nerode, A., Sastry, S. (eds.)
HS 1994. LNCS, vol. 999, pp. 1–20. Springer, Heidelberg (1995). doi:10.1007/
3-540-60472-3 1
3. Berthier, N., Maraninchi, F., Mounier, L.: Synchronous Programming of Device
Drivers for Global Resource Control in Embedded Operating Systems. ACM Trans.
Embed. Comput. Syst. 12(1s), 39: 1–39: 26., March 2013
4. Berthier, N., Marchand, H.: Discrete controller synthesis for inﬁnite state systems
with ReaX. In: 12th Internation Workshop on Discrete Event Systems. WODES
20114, IFAC, pp. 46–53, May 2014
5. Burch, J.R., Clarke, E.M., McMillan, K.L., Dill, D.L., Hwang, L.J.: Symbolic model
checking: 1020 states and beyond. Inf. Comput. 98(2), 142–170 (1992)
6. Cimatti, A., Clarke, E., Giunchiglia, E., Giunchiglia, F., Pistore, M., Roveri, M.,
Sebastiani, R., Tacchella, A.: NuSMV 2: an opensource tool for symbolic model
checking. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002. LNCS, vol. 2404, pp.
359–364. Springer, Heidelberg (2002). doi:10.1007/3-540-45657-0 29
7. Clark, J.A., Jacob, J.L.: Protocols are programs too: the meta-heuristic search for
security protocols. Inf. Softw. Technol. 43, 891–904 (2001)
8. Clarke, E.M., Grumberg, O., Peled, D.A.: Model Checking. MIT Press, Cambridge
(1999)
9. Connolly, D.: An improved annealing scheme for the qap. Eur. J. Oper. Res. 46,
93–100 (1990)
10. Cury, J.E., Krogh, B.H., Niinomi, T.: Synthesis of supervisory controllers for hybrid
systems based on approximating automata. IEEE Trans. Autom. Control 43(4),
564–568 (1998)
11. Girault, A., Rutten, ´E.: Automating the addition of fault tolerance with discrete
controller synthesis. Formal Methods Syst. Des. 35(2), 190 (2009)
12. Henderson, D., Jacobson, S.H., Johnson, A.W.: The theory and practice of sim-
ulated annealing. In: Glover, F., Kochenberger, G.A. (eds.) Handbook of Meta-
heuristics, International Series in Operations Research & Management Science,
vol. 57, pp. 287–319. Springer, Boston (2003). doi:10.1007/0-306-48056-5 10
13. Husien, I., Berthier, N., Schewe, S.: A hot method for synthesising cool controllers.
In: Proceedings of the 24th ACM SIGSOFT International SPIN Symposium on
Model Checking of Software. SPIN 2017, pp. 122–131. ACM, New York (2017)
14. Husien, I., Schewe, S.: Program generation using simulated annealing and model
checking. In: De Nicola, R., K¨uhn, E. (eds.) SEFM 2016. LNCS, vol. 9763, pp.
155–171. Springer, Cham (2016). doi:10.1007/978-3-319-41591-8 11

PranCS: A Protocol and Discrete Controller Synthesis Tool
349
15. Johnson, C.G.: Genetic programming with ﬁtness based on model checking. In:
Ebner, M., O’Neill, M., Ek´art, A., Vanneschi, L., Esparcia-Alc´azar, A.I. (eds.)
EuroGP 2007. LNCS, vol. 4445, pp. 114–124. Springer, Heidelberg (2007). doi:10.
1007/978-3-540-71605-1 11
16. Katz, G., Peled, D.: Model checking-based genetic programming with an appli-
cation to mutual exclusion. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS
2008. LNCS, vol. 4963, pp. 141–156. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-78800-3 11
17. Katz, G., Peled, D.: Model checking driven heuristic search for correct programs.
In: Peled, D.A., Wooldridge, M.J. (eds.) MoChArt 2008. LNCS (LNAI), vol. 5348,
pp. 122–131. Springer, Heidelberg (2009). doi:10.1007/978-3-642-00431-5 8
18. Koza, J.R.: Genetic Programming: On the Programming of Computers by Means
of Natural Selection. MIT Press, Cambridge (1992)
19. Krogh, B.H., Holloway, L.E.: Synthesis of feedback control logic for discrete man-
ufacturing systems. Automatica 27(4), 641–651 (1991)
20. Marchand, H., Bournai, P., Le Borgne, M., Le Guernic, P.: Synthesis of discrete-
event controllers based on the signal environment. Discrete Event Dynamic Syst.
Theory Appl. 10(4), 325–346 (2000)
21. Pnueli, A., Rosner, R.: On the synthesis of a reactive module. In: Proceedings
of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages. POPL 1989. pp. 179–190. ACM, New York (1989)
22. Ramadge, P., Wonham, W.: The control of discrete event systems. Proc. IEEE
Spec. Issue Dyn. Discr. Event Syst. 77(1), 81–98 (1989)
23. Wang, Y., Lafortune, S., Kelly, T., Kudlur, M., Mahlke, S.: The theory of dead-
lock avoidance via discrete control. In: Proceedings of the 36th Annual ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pp.
252–263. POPL 2009. ACM, New York (2009)
24. Zhou, M., DiCesare, F.: Petri Net Synthesis for Discrete Event Control of Manu-
facturing Systems, vol. 204. Springer Science & Business Media, Heidelberg (2012).
doi:10.1007/978-1-4615-3126-5

Author Index
Baruah, Sanjoy
94
Beckert, Bernhard
147
Berthier, Nicolas
337
Budde, Carlos E.
42
Burns, Alan
94
D’Argenio, Pedro R.
42
D’Souza, Meenakshi
266
Deng, Yuxin
216
Deutschbein, Calvin
94
Diwan, Maithily
266
Dunchev, Tsvetan
283
Dziwok, Stefan
319
Elbassioni, Khaled
25
Feng, Xinyu
164, 300
Fleming, Tom
94
Fu, Chen
216
Fu, Ming
300
Fu, Yuxi
183
Gerking, Christopher
319
Guelev, Dimitar P.
110
Guo, Zhishan
59
Hahn, Ernst Moritz
25
Hartmanns, Arnd
42
Hasan, Osman
283
Hashemi, Vahid
25
Helali, Ghassen
283
Hermanns, Holger
25
Honiden, Shinichi
250
Husien, Idress
337
Ishikawa, Fuyuki
250
Jansen, David N.
216
Jones, Cliff B.
3
Kumar, Rajesh
319
Li, Guoqiang
77
Li, Yi
131
Liu, Tianhai
147
Liu, Wanwei
200
Long, Huan
183
Mizera, Andrzej
232
Morita, Daichi
250
Pang, Jun
232
Qiao, Lei
300
Qu, Hongyang
232
Rensink, Arend
319
Ruijters, Enno
319
Santinelli, Luca
59
Schewe, Sven
337
Schivo, Stefano
319
Song, Fu
200
Stoelinga, Mariëlle
319
Taghdiri, Mana
147
Tahar, Soﬁène
283
Turrini, Andrea
25
Tyszberowicz, Shmuel
147
Velykis, Andrius
3
Wang, Jiawei
300
Wang, Shuling
110
Wang, Yuwei
77
Xue, Jianxin
183
Yatapanage, Nisansala
3
Yildiz, Buğra M.
319
Yuan, Qixia
232
Yuen, Shoji
77
Zhan, Naijun
110
Zhang, Lijun
216
Zhang, Zipeng
164
Zhou, Ge
200


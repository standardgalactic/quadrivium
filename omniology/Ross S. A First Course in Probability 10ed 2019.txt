1 of 848
Tenth Edition

Tenth Edition
Sheldon Ross
University of Southern California
Director, Portfolio Management: Deirdre Lynch
Courseware Portfolio Manager: Suzanna Bainbridge
Courseware Portfolio Management Assistant: Morgan Danna
Content Producer: Tara Corpuz
Managing Producer: Scott Disanno
Producer: Jon Wooding
Product Marketing Manager: Yvonne Vannatta
Product Marketing Assistant: Jon Bryant
Field Marketing Manager: Evan St. Cyr
Senior Author Support/Technology Specialist: Joe Vetere
Manager, Rights and Permissions: Gina Cheselka
Cover Design: Studio Montage
Production Coordination, Composition, and Illustrations: Integra Software Services
Pvt. Ltd.
Manufacturing Buyer: Carol Melville, LSC Communications
Cover Image: Adrienne Bresnahan/Moment/Getty Images
Copyright © 2019, 2014, 2010 by Pearson Education, Inc. All Rights Reserved.
Printed in the United States of America. This publication is protected by copyright,
and permission should be obtained from the publisher prior to any prohibited
reproduction, storage in a retrieval system, or transmission in any form or by any
means, electronic, mechanical, photocopying, recording, or otherwise. For
information regarding permissions, request forms and the appropriate contacts within
2 of 848

the Pearson Education Global Rights & Permissions department, please visit
www.pearsoned.com/permissions/.
Credits and acknowledgments borrowed from other sources and reproduced, with
permission, in this textbook appear on page 506 within text.
PEARSON, ALWAYS LEARNING, and MYLAB are exclusive trademarks owned by
Pearson Education, Inc. or its affiliates in the U.S. and/or other countries.
Unless otherwise indicated herein, any third-party trademarks that may appear in this
work are the property of their respective owners and any references to third-party
trademarks, logos or other trade dress are for demonstrative or descriptive purposes
only. Such references are not intended to imply any sponsorship, endorsement,
authorization, or promotion of Pearson’s products by the owners of such marks, or
any relationship between the owner and Pearson Education, Inc. or its affiliates,
authors, licensees or distributors.
Library of Congress Cataloging-in-Publication Data
Names: Ross, Sheldon M., author.
Title: A first course in probability / Sheldon Ross (University of Southern California).
Other titles: Probability
Description: Tenth edition. | Boston: Pearson, 2018. | Includes index.
Identifiers: LCCN 2018006823 | ISBN 9780134753119 | ISBN 0134753119
Subjects: LCSH: Probabilities—Textbooks.
Classification: LCC QA273 .R83 2018 | DDC 519.2—dc23
LC record available at https://lccn.loc.gov/2018006823
1 18
ISBN-10:  0-13-475311-9
ISBN-13: 978-0-13-475311-9
3 of 848

For Rebecca
Preface x
1 Combinatorial Analysis 1
1.1 Introduction 1
1.2 The Basic Principle of Counting 2
1.3 Permutations 3
1.4 Combinations 5
1.5 Multinomial Coefficients 9
1.6 The Number of Integer Solutions of Equations 12
Summary 15
Problems 15
Theoretical Exercises 18
Self-Test Problems and Exercises 20
2 Axioms of Probability 22
2.1 Introduction 22
2.2 Sample Space and Events 22
2.3 Axioms of Probability 26
2.4 Some Simple Propositions 29
2.5 Sample Spaces Having Equally Likely Outcomes 33
2.6 Probability as a Continuous Set Function 44
2.7 Probability as a Measure of Belief 48
Summary 49
Problems 50
Theoretical Exercises 55
Self-Test Problems and Exercises 56
3 Conditional Probability and Independence 58
3.1 Introduction 58
4 of 848

3.2 Conditional Probabilities 58
3.3 Bayes’s Formula 64
3.4 Independent Events 78
3.5 P(ꞏ|F) Is a Probability 95
Summary 102
Problems 103
Theoretical Exercises 113
Self-Test Problems and Exercises 116
4 Random Variables 119
4.1 Random Variables 119
4.2 Discrete Random Variables 123
4.3 Expected Value 126
4.4 Expectation of a Function of a Random Variable 128
4.5 Variance 132
4.6 The Bernoulli and Binomial Random Variables 137
4.6.1 Properties of Binomial Random Variables 142
4.6.2 Computing the Binomial Distribution Function 145
4.7 The Poisson Random Variable 146
4.7.1 Computing the Poisson Distribution Function 158
4.8 Other Discrete Probability Distributions 158
4.8.1 The Geometric Random Variable 158
4.8.2 The Negative Binomial Random Variable 160
4.8.3 The Hypergeometric Random Variable 163
4.8.4 The Zeta (or Zipf) Distribution 167
4.9 Expected Value of Sums of Random Variables 167
4.10 Properties of the Cumulative Distribution Function 172
Summary 174
Problems 175
Theoretical Exercises 182
Self-Test Problems and Exercises 186
5 of 848

5 Continuous Random Variables 189
5.1 Introduction 189
5.2 Expectation and Variance of Continuous Random Variables 193
5.3 The Uniform Random Variable 197
5.4 Normal Random Variables 200
5.4.1 The Normal Approximation to the Binomial Distribution 207
5.5 Exponential Random Variables 211
5.5.1 Hazard Rate Functions 215
5.6 Other Continuous Distributions 218
5.6.1 The Gamma Distribution 218
5.6.2 The Weibull Distribution 219
5.6.3 The Cauchy Distribution 220
5.6.4 The Beta Distribution 221
5.6.5 The Pareto Distribution 223
5.7 The Distribution of a Function of a Random Variable 224
Summary 227
Problems 228
Theoretical Exercises 231
Self-Test Problems and Exercises 233
6 Jointly Distributed Random Variables 237
6.1 Joint Distribution Functions 237
6.2 Independent Random Variables 247
6.3 Sums of Independent Random Variables 258
6.3.1 Identically Distributed Uniform Random Variables 258
6.3.2 Gamma Random Variables 260
6.3.3 Normal Random Variables 262
6.3.4 Poisson and Binomial Random Variables 266
6.4 Conditional Distributions: Discrete Case 267
6.5 Conditional Distributions: Continuous Case 270
6.6 Order Statistics 276
6 of 848

6.7 Joint Probability Distribution of Functions of Random Variables 280
6.8 Exchangeable Random Variables 287
Summary 290
Problems 291
Theoretical Exercises 296
Self-Test Problems and Exercises 299
7 Properties of Expectation 303
7.1 Introduction 303
7.2 Expectation of Sums of Random Variables 304
7.2.1 Obtaining Bounds from Expectations via the Probabilistic Method
317
7.2.2 The Maximum–Minimums Identity 319
7.3 Moments of the Number of Events that Occur 321
7.4 Covariance, Variance of Sums, and Correlations 328
7.5 Conditional Expectation 337
7.5.1 Definitions 337
7.5.2 Computing Expectations by Conditioning 339
7.5.3 Computing Probabilities by Conditioning 349
7.5.4 Conditional Variance 354
7.6 Conditional Expectation and Prediction 356
7.7 Moment Generating Functions 360
7.7.1 Joint Moment Generating Functions 369
7.8 Additional Properties of Normal Random Variables 371
7.8.1 The Multivariate Normal Distribution 371
7.8.2 The Joint Distribution of the Sample Mean and Sample Variance
373
7.9 General Definition of Expectation 375
Summary 377
Problems 378
Theoretical Exercises 385
7 of 848

Self-Test Problems and Exercises 390
8 Limit Theorems 394
8.1 Introduction 394
8.2 Chebyshev’s Inequality and the Weak Law of Large Numbers 394
8.3 The Central Limit Theorem 397
8.4 The Strong Law of Large Numbers 406
8.5 Other Inequalities and a Poisson Limit Result 409
8.6 Bounding the Error Probability When Approximating a Sum of
Independent Bernoulli Random Variables by a Poisson Random Variable
418
8.7 The Lorenz Curve 420
Summary 424
Problems 424
Theoretical Exercises 426
Self-Test Problems and Exercises 428
9 Additional Topics in Probability 430
9.1 The Poisson Process 430
9.2 Markov Chains 432
9.3 Surprise, Uncertainty, and Entropy 437
9.4 Coding Theory and Entropy 441
Summary 447
Problems and Theoretical Exercises 447
Self-Test Problems and Exercises 448
10 Simulation 450
10.1 Introduction 450
10.2 General Techniques for Simulating Continuous Random Variables
453
10.2.1 The Inverse Transformation Method 453
10.2.2 The Rejection Method 454
10.3 Simulating from Discrete Distributions 459
8 of 848

10.4 Variance Reduction Techniques 462
10.4.1 Use of Antithetic Variables 463
10.4.2 Variance Reduction by Conditioning 463
10.4.3 Control Variates 465
Summary 465
Problems 466
Self-Test Problems and Exercises 467
Answers to Selected Problems 468
Solutions to Self-Test Problems and Exercises 470
Index 502
Common Discrete Distributions
inside front cover
Common Continuous Distributions
inside back cover
“We see that the theory of probability is at bottom only common sense reduced to
calculation; it makes us appreciate with exactitude what reasonable minds feel by a
sort of instinct, often without being able to account for it... It is remarkable that this
science, which originated in the consideration of games of chance, should have
become the most important object of human knowledge.... The most important
questions of life are, for the most part, really only problems of probability.” So said
the famous French mathematician and astronomer (the “Newton of France”) Pierre-
Simon, Marquis de Laplace. Although many people believe that the famous marquis,
who was also one of the great contributors to the development of probability, might
have exaggerated somewhat, it is nevertheless true that probability theory has
become a tool of fundamental importance to nearly all scientists, engineers, medical
practitioners, jurists, and industrialists. In fact, the enlightened individual had learned
to ask not “Is it so?” but rather “What is the probability that it is so?”
This book is intended as an elementary introduction to the theory of probability for
students in mathematics, statistics, engineering, and the sciences (including
9 of 848

computer science, biology, the social sciences, and management science) who
possess the prerequisite knowledge of elementary calculus. It attempts to present
not only the mathematics of probability theory, but also, through numerous examples,
the many diverse possible applications of this subject.
Chapter 1
 presents the basic principles of combinatorial analysis, which are most
useful in computing probabilities.
Chapter 2
 handles the axioms of probability theory and shows how they can be
applied to compute various probabilities of interest.
Chapter 3
 deals with the extremely important subjects of conditional probability
and independence of events. By a series of examples, we illustrate how conditional
probabilities come into play not only when some partial information is available, but
also as a tool to enable us to compute probabilities more easily, even when no partial
information is present. This extremely important technique of obtaining probabilities
by “conditioning” reappears in Chapter 7
, where we use it to obtain expectations.
The concept of random variables is introduced in Chapters 4
, 5
, and 6
.
Discrete random variables are dealt with in Chapter 4
, continuous random
variables in Chapter 5
, and jointly distributed random variables in Chapter 6
.
The important concepts of the expected value and the variance of a random variable
are introduced in Chapters 4
 and 5
, and these quantities are then determined
for many of the common types of random variables.
Additional properties of the expected value are considered in Chapter 7
. Many
examples illustrating the usefulness of the result that the expected value of a sum of
random variables is equal to the sum of their expected values are presented.
Sections on conditional expectation, including its use in prediction, and on moment-
generating functions are contained in this chapter. In addition, the final section
introduces the multivariate normal distribution and presents a simple proof
concerning the joint distribution of the sample mean and sample variance of a
sample from a normal distribution.
Chapter 8
 presents the major theoretical results of probability theory. In particular,
we prove the strong law of large numbers and the central limit theorem. Our proof of
the strong law is a relatively simple one that assumes that the random variables have
a finite fourth moment, and our proof of the central limit theorem assumes Levy’s
continuity theorem. This chapter also presents such probability inequalities as
Markov’s inequality, Chebyshev’s inequality, and Chernoff bounds. The final section
10 of 848

of Chapter 8
 gives a bound on the error involved when a probability concerning a
sum of independent Bernoulli random variables is approximated by the
corresponding probability of a Poisson random variable having the same expected
value.
Chapter 9
 presents some additional topics, such as Markov chains, the Poisson
process, and an introduction to information and coding theory, and Chapter 10
considers simulation.
As in the previous edition, three sets of exercises are given at the end of each
chapter. They are designated as Problems, Theoretical Exercises, and Self-Test
Problems and Exercises. This last set of exercises, for which complete solutions
appear in Solutions to Self-Test Problems and Exercises, is designed to help
students test their comprehension and study for exams.
The tenth edition continues the evolution and fine tuning of the text. Aside from a
multitude of small changes made to increase the clarity of the text, the new edition
includes many new and updated problems, exercises, and text material chosen both
for inherent interest and for their use in building student intuition about probability.
Illustrative of these goals are Examples 4n of Chapter 3
, which deals with
computing NCAA basketball tournament win probabilities, and Example 5b of
Chapter 4
, which introduces the friendship paradox. There is also new material
on the Pareto distribution (introduced in Section 5.6.5
), on Poisson limit results
(in Section 8.5
), and on the Lorenz curve (in Section 8.7
).
I would like to thank the following people who have graciously taken the time to
contact me with comments for improving the text: Amir Ardestani, Polytechnic
University of Teheran; Joe Blitzstein, Harvard University; Peter Nuesch, University of
Lausaunne; Joseph Mitchell, SUNY, Stony Brook; Alan Chambless, actuary; Robert
Kriner; Israel David, Ben-Gurion University; T. Lim, George Mason University; Wei
Chen, Rutgers; D. Monrad, University of Illinois; W. Rosenberger, George Mason
University; E. Ionides, University of Michigan; J. Corvino, Lafayette College; T.
Seppalainen, University of Wisconsin; Jack Goldberg; University of Michigan; Sunil
Dhar, New Jersey Institute of Technology; Vladislav Kargin, Stanford University;
Marlene Miller; Ahmad Parsian; and Fritz Scholz, University of Washington.
I would also like to especially thank the reviewers of the ninth and tenth editions:
11 of 848

Richard Laugesen, University of Illinois; Stacey Hancock, Clark University; Stefan
Heinz, University of Wyoming; and Brian Thelen, University of Michigan; Mark Ward,
Purdue University. I would like to thank the accuracy checker, Stacey Hancock
(Montana State University), for her careful review.
Finally, I would like to thank the following reviewers for their many helpful comments.
Reviewers of the tenth edition are marked with an asterisk.
K. B. Athreya, Iowa State University
Richard Bass, University of Connecticut
Robert Bauer, University of Illinois at Urbana-Champaign
Phillip Beckwith, Michigan Tech
Arthur Benjamin, Harvey Mudd College
Geoffrey Berresford, Long Island University
Baidurya Bhattacharya, University of Delaware
Howard Bird, St. Cloud State University
Shahar Boneh, Metropolitan State College of Denver
Jean Cadet, State University of New York at Stony Brook
Steven Chiappari, Santa Clara University
Nicolas Christou, University of California, Los Angeles
James Clay, University of Arizona at Tucson
Francis Conlan, University of Santa Clara
Justin Corvino, Lafayette College
Jay DeVore, California Polytechnic University, San Luis Obispo
Scott Emerson, University of Washington
Thomas R. Fischer, Texas A & M University
Anant Godbole, Michigan Technical University
Zakkula Govindarajulu, University of Kentucky
Richard Groeneveld, Iowa State University
*Stacey Hancock, Clark University
Mike Hardy, Massachusetts Institute of Technology
12 of 848

Bernard Harris, University of Wisconsin
Larry Harris, University of Kentucky
David Heath, Cornell University
*Stefan Heinz, University of Wyoming
Stephen Herschkorn, Rutgers University
Julia L. Higle, University of Arizona
Mark Huber, Duke University
Edward Ionides, University of Michigan
Anastasia Ivanova, University of North Carolina
Hamid Jafarkhani, University of California, Irvine
Chuanshu Ji, University of North Carolina, Chapel Hill
Robert Keener, University of Michigan
*Richard Laugesen, University of Illinois
Fred Leysieffer, Florida State University
Thomas Liggett, University of California, Los Angeles
Helmut Mayer, University of Georgia
Bill McCormick, University of Georgia
Ian McKeague, Florida State University
R. Miller, Stanford University
Ditlev Monrad, University of Illinois
Robb J. Muirhead, University of Michigan
Joe Naus, Rutgers University
Nhu Nguyen, New Mexico State University
Ellen O’Brien, George Mason University
N. U. Prabhu, Cornell University
Kathryn Prewitt, Arizona State University
Jim Propp, University of Wisconsin
William F. Rosenberger, George Mason University
13 of 848

Myra Samuels, Purdue University
I. R. Savage, Yale University
Art Schwartz, University of Michigan at Ann Arbor
Therese Shelton, Southwestern University
Malcolm Sherman, State University of New York at Albany
Murad Taqqu, Boston University
*Brian Thelen, University of Michigan
Eli Upfal, Brown University
Ed Wheeler, University of Tennessee
Allen Webster, Bradley University
S. R.
smross@usc.edu
1.1 Introduction
1.2 The Basic Principle of Counting
1.3 Permutations
1.4 Combinations
1.5 Multinomial Coefficients
1.6 The Number of Integer Solutions of Equations
Here is a typical problem of interest involving probability: A communication system is
to consist of  seemingly identical antennas that are to be lined up in a linear order.
𝑛
14 of 848

The resulting system will then be able to receive all incoming signals and will be
called functional as long as no two consecutive antennas are defective. If it turns out
that exactly 
 of the  antennas are defective, what is the probability that the
resulting system will be functional? For instance, in the special case where 
 and
 there are 6 possible system configurations, namely,
where 1 means that the antenna is working and 0 that it is defective. Because the
resulting system will be functional in the first 3 arrangements and not functional in the
remaining 3, it seems reasonable to take 
 as the desired probability. In the case
of general  and 
 we could compute the probability that the system is functional in
a similar fashion. That is, we could count the number of configurations that result in
the system’s being functional and then divide by the total number of all possible
configurations.
From the preceding discussion, we see that it would be useful to have an effective
method for counting the number of ways that things can occur. In fact, many
problems in probability theory can be solved simply by counting the number of
different ways that a certain event can occur. The mathematical theory of counting is
formally known as combinatorial analysis.
The basic principle of counting will be fundamental to all our work. Loosely put, it
states that if one experiment can result in any of 
 possible outcomes and if another
experiment can result in any of  possible outcomes, then there are mn possible
outcomes of the two experiments.
The basic principle of counting
Suppose that two experiments are to be performed. Then if experiment 1 can
result in any one of 
 possible outcomes and if, for each outcome of
experiment 1, there are  possible outcomes of experiment 2, then together
there are mn possible outcomes of the two experiments.
Proof of the Basic Principle: The basic principle may be proven by enumerating all
the possible outcomes of the two experiments; that is,
𝑚
𝑛
𝑛ൌ4
𝑚ൌ2,
0
1 1
0
0
1 0
1
1
0 1
0
0
0 1
1
1
0 0
1
1
1 0
0
3
6 ൌ1
2
𝑛
𝑚,
𝑚
𝑛
𝑚
𝑛
15 of 848

where we say that the outcome is (
) if experiment 1 results in its th possible
outcome and experiment 2 then results in its th possible outcome. Hence, the set of
possible outcomes consists of 
 rows, each containing  elements. This proves the
result.
Example 2a
A small community consists of 10 women, each of whom has 3 children. If one
woman and one of her children are to be chosen as mother and child of the year,
how many different choices are possible?
Solution
By regarding the choice of the woman as the outcome of the first experiment and
the subsequent choice of one of her children as the outcome of the second
experiment, we see from the basic principle that there are 
 possible
choices.
When there are more than two experiments to be performed, the basic principle can
be generalized.
The generalized basic principle of counting
If  experiments that are to be performed are such that the first one may result
in any of 
 possible outcomes; and if, for each of these 
 possible outcomes,
there are 
 possible outcomes of the second experiment; and if, for each of
the possible outcomes of the first two experiments, there are 
 possible
outcomes of the third experiment; and if, then there is a total of 
possible outcomes of the  experiments.
Example 2b
A college planning committee consists of 3 freshmen, 4 sophomores, 5 juniors,
and 2 seniors. A subcommittee of 4, consisting of 1 person from each class, is to
be chosen. How many different subcommittees are possible?
Solution
We may regard the choice of a subcommittee as the combined outcome of the
four separate experiments of choosing a single representative from each of the
classes. It then follows from the generalized version of the basic principle that
ሺ1, 1ሻ,
ሺ1, 2ሻ,
. . .,
ሺ1, 𝑛ሻ
ሺ2, 1ሻ,
ሺ2, 2ሻ,
. . .,
ሺ2, 𝑛ሻ
⋮
ሺ𝑚, 1ሻ,
ሺ𝑚, 2ሻ,
. . .,
ሺ𝑚, 𝑛ሻ
𝑖, 𝑗
𝑖
𝑗
𝑚
𝑛
10 ൈ3 ൌ30
𝑟
𝑛ଵ
𝑛ଵ
𝑛ଶ
𝑛ଷ
𝑛ଵ⋅𝑛ଶ⋯𝑛௥
𝑟
16 of 848

there are 
 possible subcommittees.
Example 2c
How many different 7-place license plates are possible if the first 3 places are to
be occupied by letters and the final 4 by numbers?
Solution
By the generalized version of the basic principle, the answer is
Example 2d
How many functions defined on  points are possible if each functional value is
either 0 or 1?
Solution
Let the points be 
 Since 
 must be either 0 or 1 for each 
it follows that there are 
 possible functions.
Example 2e
In Example 2c
, how many license plates would be possible if repetition
among letters or numbers were prohibited?
Solution
In this case, there would be 
 possible
license plates.
How many different ordered arrangements of the letters 
 and  are possible? By
direct enumeration we see that there are 6, namely, abc, acb, bac, bca, cab, and
cba. Each arrangement is known as a permutation. Thus, there are 6 possible
permutations of a set of 3 objects. This result could also have been obtained from
the basic principle, since the first object in the permutation can be any of the 3, the
second object in the permutation can then be chosen from any of the remaining 2,
and the third object in the permutation is then the remaining 1. Thus, there are
 possible permutations.
Suppose now that we have  objects. Reasoning similar to that we have just
used for the 3 letters then shows that there are
3 ൈ4 ൈ5 ൈ2 ൌ120
26 ⋅26 ⋅26 ⋅10 ⋅10 ⋅10 ⋅10 ൌ175,760,000.
𝑛
1,2, . . . ,𝑛.
𝑓ሺ𝑖ሻ
𝑖ൌ1,2, . . . ,𝑛,
2௡
26 ⋅25 ⋅24 ⋅10 ⋅9 ⋅8 ⋅7 ൌ78,624,000
𝑎, 𝑏,
𝑐
3 ⋅2 ⋅1 ൌ6
𝑛
17 of 848

different permutations of the  objects.
Whereas 
 (read as “n factorial”) is defined to equal 
 when  is a positive
integer, it is convenient to define 
 to equal 
Example 3a
How many different batting orders are possible for a baseball team consisting of
9 players?
Solution
There are 
 possible batting orders.
Example 3b
A class in probability theory consists of 6 men and 4 women. An examination is
given, and the students are ranked according to their performance. Assume that
no two students obtain the same score.
a. How many different rankings are possible?
b. If the men are ranked just among themselves and the women just among
themselves, how many different rankings are possible?
Solution
a. (a) Because each ranking corresponds to a particular ordered
arrangement of the 10 people, the answer to this part is 
b. (b) Since there are 6! possible rankings of the men among themselves
and 4! possible rankings of the women among themselves, it follows from
the basic principle that there are 
 possible
rankings in this case.
Example 3c
Ms. Jones has 10 books that she is going to put on her bookshelf. Of these, 4 are
mathematics books, 3 are chemistry books, 2 are history books, and 1 is a
language book. Ms. Jones wants to arrange her books so that all the books
dealing with the same subject are together on the shelf. How many different
arrangements are possible?
Solution
There are 4! 3! 2! 1! arrangements such that the mathematics books are first in
𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ⋯3 ⋅2 ⋅1 ൌ𝑛!
𝑛
𝑛!
1 ⋅2⋯𝑛
𝑛
0!
1.
9! ൌ362,880
10! ൌ3,628,800.
ሺ6!ሻሺ4!ሻൌሺ720ሻሺ24ሻൌ17,280
18 of 848

line, then the chemistry books, then the history books, and then the language
book. Similarly, for each possible ordering of the subjects, there are 4! 3! 2! 1!
possible arrangements. Hence, as there are 4! possible orderings of the subjects,
the desired answer is 
We shall now determine the number of permutations of a set of  objects when
certain of the objects are indistinguishable from one another. To set this situation
straight in our minds, consider the following example.
Example 3d
How many different letter arrangements can be formed from the letters 
Solution
We first note that there are 6! permutations of the letters 
 when the
 and the 
 are distinguished from one another. However, consider any one
of these permutations for instance, 
 If we now permute the ’s
among themselves and the ’s among themselves, then the resultant
arrangement would still be of the form 
 That is, all 3! 2! permutations
are of the form 
 Hence, there are 
 letter
arrangements of the letters 
In general, the same reasoning as that used in Example 3d
 shows that
there are
different permutations of  objects, of which 
 are alike, 
 are alike, 
are alike.
Example 3e
A chess tournament has 10 competitors, of which 4 are Russian, 3 are from the
United States, 2 are from Great Britain, and 1 is from Brazil. If the tournament
result lists just the nationalities of the players in the order in which they placed,
how many outcomes are possible?
4! 4! 3! 2! 1! ൌ6912.
𝑛
𝑃𝐸𝑃𝑃𝐸𝑅?
𝑃ଵ𝐸ଵ𝑃ଶ𝑃ଷ𝐸ଶ𝑅
3𝑃's
2𝐸's
𝑃ଵ𝑃ଶ𝐸ଵ𝑃ଷ𝐸ଶ𝑅.
𝑃
𝐸
𝑃𝑃𝐸𝑃𝐸𝑅.
𝑃ଵ𝑃ଶ𝐸ଵ𝑃ଷ𝐸ଶ𝑅
𝑃ଵ𝑃ଶ𝐸ଶ𝑃ଷ𝐸ଵ𝑅
𝑃ଵ𝑃ଷ𝐸ଵ𝑃ଶ𝐸ଶ𝑅
𝑃ଵ𝑃ଷ𝐸ଶ𝑃ଶ𝐸ଵ𝑅
𝑃ଶ𝑃ଵ𝐸ଵ𝑃ଷ𝐸ଶ𝑅
𝑃ଶ𝑃ଵ𝐸ଶ𝑃ଷ𝐸ଵ𝑅
𝑃ଶ𝑃ଷ𝐸ଵ𝑃ଵ𝐸ଶ𝑅
𝑃ଶ𝑃ଷ𝐸ଶ𝑃ଵ𝐸ଵ𝑅
𝑃ଷ𝑃ଵ𝐸ଵ𝑃ଶ𝐸ଶ𝑅
𝑃ଷ𝑃ଵ𝐸ଶ𝑃ଶ𝐸ଵ𝑅
𝑃ଷ𝑃ଶ𝐸ଵ𝑃ଵ𝐸ଶ𝑅
𝑃ଷ𝑃ଶ𝐸ଶ𝑃ଵ𝐸ଵ𝑅
𝑃𝑃𝐸𝑃𝐸𝑅.
6!/ሺ3! 2!ሻൌ60 possible
𝑃𝐸𝑃𝑃𝐸𝑅.
𝑛!
𝑛ଵ! 𝑛ଶ! ⋯ 𝑛௥!
𝑛
𝑛ଵ
𝑛ଶ
. . . ,𝑛௥
19 of 848

Solution
There are
possible outcomes.
Example 3f
How many different signals, each consisting of 9 flags hung in a line, can be
made from a set of 4 white flags, 3 red flags, and 2 blue flags if all flags of the
same color are identical?
Solution
There are
different signals.
We are often interested in determining the number of different groups of  objects
that could be formed from a total of  objects. For instance, how many different
groups of 3 could be selected from the 5 items 
 and 
 To answer this
question, reason as follows: Since there are 5 ways to select the initial item, 4 ways
to then select the next item, and 3 ways to select the final item, there are thus 
ways of selecting the group of 3 when the order in which the items are selected is
relevant. However, since every group of 3–say, the group consisting of items 
and  will be counted 6 times (that is, all of the permutations ABC, ACB, BAC, BCA,
CAB, and CBA will be counted when the order of selection is relevant), it follows that
the total number of groups that can be formed is
In general, as 
 represents the number of different ways that a
group of  items could be selected from  items when the order of selection is
relevant, and as each group of  items will be counted ! times in this count, it follows
that the number of different groups of  items that could be formed from a set of 
10!
4! 3! 2! 1! ൌ12,600
9!
4! 3! 2! ൌ1260
𝑟
𝑛
𝐴, 𝐵, 𝐶, 𝐷,
𝐸?
5 ⋅4 ⋅3
𝐴, 𝐵,
𝐶
5 ⋅4 ⋅3
3 ⋅2 ⋅1 ൌ10
𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑟൅1ሻ
𝑟
𝑛
𝑟
𝑟
𝑟
𝑛
20 of 848

items is
Notation and terminology
We define 
 for 
 by
and say that 
 (read as “  choose “) represents the number of possible
combinations of  objects taken  at a time.
Thus, 
 represents the number of different groups of size  that could be selected
from a set of  objects when the order of selection is not considered relevant.
Equivalently, 
 is the number of subsets of size  that can be chosen from a set of
size  Using that 
 note that 
 which is consistent with
the preceding interpretation because in a set of size  there is exactly  subset of
size  (namely, the entire set), and exactly one subset of size  (namely the empty
set). A useful convention is to define 
 equal to  when either 
 or 
Example 4a
A committee of 3 is to be formed from a group of 20 people. How many different
committees are possible?
Solution
There are 
 possible committees.
Example 4b
From a group of 5 women and 7 men, how many different committees consisting
of 2 women and 3 men can be formed? What if 2 of the men are feuding and
refuse to serve on the committee together?
Solution
𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑟൅1ሻ
𝑟!
ൌ
𝑛!
ሺ𝑛െ𝑟ሻ! 𝑟!
ቆ𝑛
𝑟ቇ,
𝑟൑𝑛,
ቆ𝑛
𝑟ቇൌ
𝑛!
ሺ𝑛െ𝑟ሻ! 𝑟!
ቆ𝑛
𝑟ቇ
𝑛
𝑟
𝑛
𝑟
ቆ𝑛
𝑟ቇ
𝑟
𝑛
ቆ𝑛
𝑟ቇ
𝑟
𝑛.
0! ൌ1,
ቆ𝑛
𝑛ቇൌቆ𝑛
0ቇൌ𝑛!
0!𝑛! ൌ1,
𝑛
1
𝑛
0
ቆ𝑛
𝑟ቇ
0
𝑟൐𝑛
𝑟൏0.
ቆ20
3 ቇൌ20 ⋅19 ⋅18
3 ⋅2 ⋅1
ൌ1140
21 of 848

As there are 
 possible groups of 2 women, and 
 possible groups of 3
men, it follows from the basic principle that there are 
 possible committees consisting of 2 women and 3 men.
Now suppose that  of the men refuse to serve together. Because a total of
 out of the 
 possible groups of 3 men contain both of the
feuding men, it follows that there are 
 groups that do not contain both
of the feuding men. Because there are still 
 ways to choose the 2
women, there are 
 possible committees in this case.
Example 4c
Consider a set of  antennas of which 
 are defective and 
 are functional
and assume that all of the defectives and all of the functionals are considered
indistinguishable. How many linear orderings are there in which no two
defectives are consecutive?
Solution
Imagine that the 
 functional antennas are lined up among themselves. Now,
if no two defectives are to be consecutive, then the spaces between the
functional antennas must each contain at most one defective antenna. That is, in
the 
 possible positions–represented in Figure 1.1
 by carets–
between the 
 functional antennas, we must select 
 of these in which to
put the defective antennas. Hence, there are 
 possible orderings in
which there is at least one functional antenna between any two defective ones.
Figure 1.1 No consecutive defectives.
The figure shows No consecutive defectives
A useful combinatorial identity, known as Pascal’s identity, is
Equation (4.1)
 may be proved analytically or by the following combinatorial
argument: Consider a group of  objects, and fix attention on some particular one of
ቆ5
2ቇ
ቆ7
3ቇ
ቆ5
2ቇቆ7
3ቇൌ5 ⋅4
2 ⋅1 .
7 ⋅6 ⋅5
3 ⋅2 ⋅1 ൌ350
2
ቆ2
2ቇቆ5
1ቇൌ5
ቆ7
3ቇൌ35
35 െ5 ൌ30
ቆ5
2ቇൌ10
30 ⋅10 ൌ300
𝑛
𝑚
𝑛െ𝑚
𝑛െ𝑚
𝑛െ𝑚൅1
𝑛െ𝑚
𝑚
ቆ𝑛െ𝑚൅1
𝑚
ቇ
ቆ𝑛
𝑟ቇൌቆ𝑛െ1
𝑟െ1ቇ൅ቆ𝑛െ1
𝑟
ቇ 1 ൑𝑟൑𝑛
(4.1)
𝑛
22 of 848

these objects–call it object 1. Now, there are 
 groups of size  that contain
object 1 (since each such group is formed by selecting 
 from the remaining
 objects). Also, there are 
 groups of size  that do not contain object 1.
As there is a total of 
 groups of size 
Equation (4.1)
 follows.
The values 
 are often referred to as binomial coefficients because of their
prominence in the binomial theorem.
The binomial theorem
We shall present two proofs of the binomial theorem. The first is a proof by
mathematical induction, and the second is a proof based on combinatorial
considerations.
Proof of the Binomial Theorem by Induction: When 
Equation (4.2)
reduces to
Assume Equation (4.2)
 for 
 Now,
Letting 
 in the first sum and 
 in the second sum, we find that
ቆ𝑛െ1
𝑟െ1ቇ
𝑟
𝑟െ1
𝑛െ1
ቆ𝑛െ1
𝑟
ቇ
𝑟
ቆ𝑛
𝑟ቇ
𝑟,
ቆ𝑛
𝑟ቇ
ሺ𝑥൅𝑦ሻ௡ൌ
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ 𝑥௞𝑦௡െ௞
(4.2)
𝑛ൌ1,
𝑥൅𝑦ൌቆ1
0ቇ 𝑥଴𝑦ଵ൅ቆ1
1ቇ 𝑥ଵ𝑦଴ൌ𝑦൅𝑥
𝑛െ1.
ሺ𝑥൅𝑦ሻ௡ൌ
ሺ𝑥൅𝑦ሻሺ𝑥൅𝑦ሻ௡െଵ
ൌ
ሺ𝑥൅𝑦ሻ෍
௞ൌ଴
௡ൌଵ
ቆ𝑛െ1
𝑘
ቇ 𝑥௞𝑦௡െଵെ௞
ൌ
෍
௞ൌ଴
௡െଵ
ቆ𝑛െ1
𝑘
ቇ𝑥௞൅ଵ𝑦௡െଵെ௞൅
෍
௞ൌ଴
௡െଵ
ቆ𝑛െ1
𝑘
ቇ 𝑥௞𝑦௡െ௞
𝑖ൌ𝑘൅1
𝑖ൌ𝑘
23 of 848

where the next-to-last equality follows by Equation (4.1)
. By induction, the
theorem is now proved.
Combinatorial Proof of the Binomial Theorem: Consider the product
Its expansion consists of the sum of 
 terms, each term being the product of 
factors. Furthermore, each of the 
 terms in the sum will contain as a factor either 
or 
 for each 
 For example,
Now, how many of the 
 terms in the sum will have  of the 
’s and 
 of the
’s as factors? As each term consisting of  of the 
’s and 
 of the 
’s
corresponds to a choice of a group of  from the  values 
 there are 
such terms. Thus, letting 
 we see that
Example 4d
Expand 
Solution
ሺ𝑥൅𝑦ሻ௡
ൌ
෍
௜ൌଵ
௡
ቆ𝑛െ1
𝑖െ1ቇ 𝑥௜𝑦௡െ௜൅
෍
௜ൌ଴
௡െଵ
ቆ𝑛െ1
𝑖
ቇ 𝑥௜𝑦௡െ௜
ൌ
෍
௜ൌଵ
௡െଵ
ቆ𝑛െ1
𝑖െ1ቇ 𝑥௜𝑦௡െ௜൅𝑥௡൅𝑦௡൅
෍
௜ൌଵ
௡െଵ
ቆ𝑛െ1
𝑖
ቇ 𝑥௜𝑦௡െ௜
ൌ𝑥௡൅
෍
௜ൌଵ
௡െଵ
ቈቆ𝑛െ1
𝑖െ1ቇ൅ቆ𝑛െ1
𝑖
ቇ቉ 𝑥௜𝑦௡െ௜൅𝑦௡
ൌ𝑥௡൅
෍
௜ൌଵ
௡െଵ
ቆ𝑛
𝑖ቇ 𝑥௜𝑦௡െ௜൅𝑦௡
ൌ
෍
௜ൌ଴
௡
ቆ𝑛
𝑖ቇ 𝑥௜𝑦௡െ௜
ሺ𝑥ଵ൅𝑦ଵሻሺ𝑥ଶ൅𝑦ଶሻ⋯ሺ𝑥௡൅𝑦௡ሻ
2௡
𝑛
2௡
𝑥௜
𝑦௜
𝑖ൌ1, 2, . . . ,𝑛.
ሺ𝑥ଵ൅𝑦ଵሻሺ𝑥ଶ൅𝑦ଶሻൌ𝑥ଵ𝑥ଶ൅𝑥ଵ𝑦ଶ൅𝑦ଵ𝑥ଶ൅𝑦ଵ𝑦ଶ
2௡
𝑘
𝑥௜
ሺ𝑛െ𝑘ሻ
𝑦௜
𝑘
𝑥௜
ሺ𝑛െ𝑘ሻ
𝑦௜
𝑘
𝑛
𝑥ଵ,  𝑥ଶ…,  𝑥௡,
ቆ𝑛
𝑘ቇ
𝑥௜ൌ𝑥, 𝑦௜ൌ𝑦, 𝑖ൌ1, . . . ,𝑛,
ሺ𝑥൅𝑦ሻ௡ൌ
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ 𝑥௞𝑦௡െ௞
ሺ𝑥൅𝑦ሻଷ.
24 of 848

Example 4e
How many subsets are there of a set consisting of  elements?
Solution
Since there are 
 subsets of size  the desired answer is
This result could also have been obtained by assigning either the number 0 or
the number 1 to each element in the set. To each assignment of numbers, there
corresponds, in a one-to-one fashion, a subset, namely, that subset consisting of
all elements that were assigned the value 1. As there are 
 possible
assignments, the result follows.
Note that we have included the set consisting of 0 elements (that is, the null set)
as a subset of the original set. Hence, the number of subsets that contain at least
1 element is 
In this section, we consider the following problem: A set of  distinct items is to be
divided into  distinct groups of respective sizes 
 where 
How many different divisions are possible? To answer this question, we note that
there are 
 possible choices for the first group; for each choice of the first group,
there are 
 possible choices for the second group; for each choice of the first
two groups, there are 
 possible choices for the third group; and so on.
It then follows from the generalized version of the basic counting principle that there
are
ሺ𝑥൅𝑦ሻଷ
ൌ
ቆ3
0ቇ 𝑥଴𝑦ଷ൅ቆ3
1ቇ 𝑥ଵ𝑦ଶ൅ቆ3
2ቇ 𝑥ଶ𝑦ଵ൅ቆ3
3ቇ 𝑥ଷ𝑦଴
ൌ
𝑦ଷ൅3𝑥𝑦ଶ൅3𝑥ଶ𝑦൅𝑥ଷ
𝑛
ቆ𝑛
𝑘ቇ
𝑘,
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇൌሺ1 ൅1ሻ௡ൌ2௡
2௡
2௡െ1.
𝑛
𝑟
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥,
෍
௜ൌଵ
௥
𝑛௜ൌ𝑛.
ቆ
𝑛
𝑛ଵ
ቇ
൭
𝑛െ𝑛ଵ
𝑛ଶ
൱
൭
𝑛െ𝑛ଵെ𝑛ଶ
𝑛ଷ
൱
25 of 848

possible divisions.
Another way to see this result is to consider the  values 
 where  appears 
 times, for 
 Every permutation of these values
corresponds to a division of the  items into the  groups in the following manner: Let
the permutation 
 correspond to assigning item  to group 
 item  to
group 
 and so on. For instance, if 
 and if 
 then the
permutation 
 corresponds to assigning items 
 to the first
group, items 
 to the second group, and item  to the third group. Because every
permutation yields a division of the items and every possible division results from
some permutation, it follows that the number of divisions of  items into  distinct
groups of sizes 
 is the same as the number of permutations of  items of
which 
 are alike, and 
 are alike, 
 and 
 are alike, which was shown in
Section 1.3
 to equal 
Notation
If 
 we define 
 by
Thus, 
 represents the number of possible divisions of  distinct
objects into  distinct groups of respective sizes 
Example 5a
A police department in a small city consists of 10 officers. If the department
policy is to have 5 of the officers patrolling the streets, 2 of the officers working
full time at the station, and 3 of the officers on reserve at the station, how many
different divisions of the 10 officers into the 3 groups are possible?
Solution
ቆ
𝑛
𝑛ଵ
ቇ൭
𝑛െ𝑛ଵ
𝑛ଶ
൱⋯൭
𝑛െ𝑛ଵെ𝑛ଶെ⋯െ𝑛௥െଵ
𝑛௥
൱
ൌ
𝑛!
ሺ𝑛െ𝑛ଵሻ! 𝑛ଵ!
ሺ𝑛െ𝑛ଵሻ!
ሺ𝑛െ𝑛ଵെ𝑛ଶሻ! 𝑛ଶ!⋯ሺ𝑛െ𝑛ଵെ𝑛ଶെ⋯െ𝑛௥െଵሻ!
0! 𝑛௥!
ൌ
𝑛!
𝑛ଵ! 𝑛ଶ!⋯𝑛௥!
𝑛
 1,1, . . . ,1,2, . . . ,2, . . . ,
𝑟, . . . ,𝑟,
𝑖
𝑛௜
𝑖ൌ1, . . . ,𝑟.
𝑛
𝑟
𝑖ଵ,𝑖ଶ, . . . ,𝑖௡
1
𝑖ଵ,
2
𝑖ଶ,
𝑛ൌ8
𝑛ଵൌ4,𝑛ଶൌ3, and 𝑛ଷൌ1,
1, 1, 2, 3, 2, 1, 2, 1
1, 2, 6, 8
3, 5, 7
4
𝑛
𝑟
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥
𝑛
𝑛ଵ
𝑛ଶ
. . . ,
𝑛௥
 
𝑛!
𝑛ଵ!𝑛ଶ!⋯𝑛௥! .
𝑛ଵ൅𝑛ଶ൅⋯൅𝑛௥ൌ𝑛,
ቆ
𝑛
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥
ቇ
ቆ
𝑛
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥
ቇൌ
𝑛!
𝑛ଵ! 𝑛ଶ!⋯𝑛௥!
ቆ
𝑛
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥
ቇ
𝑛
𝑟
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥.
26 of 848

There are 
 possible divisions.
Example 5b
Ten children are to be divided into an  team and a  team of 5 each. The 
team will play in one league and the  team in another. How many different
divisions are possible?
Solution
There are 
 possible divisions.
Example 5c
In order to play a game of basketball, 10 children at a playground divide
themselves into two teams of 5 each. How many different divisions are possible?
Solution
Note that this example is different from Example 5b
 because now the order of
the two teams is irrelevant. That is, there is no  or  team, but just a division
consisting of 2 groups of 5 each. Hence, the desired answer is
The proof of the following theorem, which generalizes the binomial theorem, is left as
an exercise.
The multinomial theorem
That is, the sum is over all nonnegative integer-valued vectors 
such that 
The numbers 
 are known as multinomial coefficients.
Example 5d
In the first round of a knockout tournament involving 
 players, the 
10!
5! 2! 3! ൌ2520
𝐴
𝐵
𝐴
𝐵
10!
5! 5! ൌ252
𝐴
𝐵
10!/ሺ5! 5!ሻ
2!
ൌ126
               ሺ𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ሻ௡ൌ
෍
   ሺ௡భ, .   .   . , ௡ೝሻ:
௡భ൅⋯൅௡ೝൌ௡
  ቆ
𝑛
𝑛ଵ, 𝑛ଶ, .   .   . , 𝑛௥
ቇ 𝑥ଵ
௡భ𝑥ଶ
௡మ⋯𝑥௥ 
௡ೝ
ሺ𝑛ଵ,𝑛ଶ, . . . ,𝑛௥ሻ
𝑛ଵ൅𝑛ଶ൅⋯൅𝑛௥ൌ𝑛.
ቆ
𝑛
𝑛ଵ,𝑛ଶ, . . . ,𝑛௥
ቇ
𝑛ൌ2௠
𝑛
27 of 848

players are divided into 
 pairs, with each of these pairs then playing a game.
The losers of the games are eliminated while the winners go on to the next
round, where the process is repeated until only a single player remains. Suppose
we have a knockout tournament of  players.
a. How many possible outcomes are there for the initial round? (For
instance, one outcome is that 1 beats 2, 3 beats 4, 5 beats 6, and 7 beats
8.)
b. How many outcomes of the tournament are possible, where an outcome
gives complete information for all rounds?
Solution
One way to determine the number of possible outcomes for the initial round is to
first determine the number of possible pairings for that round. To do so, note that
the number of ways to divide the  players into a first pair, a second pair, a third
pair, and a fourth pair is 
 Thus, the number of possible pairings
when there is no ordering of the  pairs is 
 For each such pairing, there are
 possible choices from each pair as to the winner of that game, showing that
there are 
 possible results of round  [Another way to see this is to
note that there are 
 possible choices of the  winners and, for each such
choice, there are 
 ways to pair the  winners with the  losers, showing that
there are 
 possible results for the first round.]
Similarly, for each result of round  there are 
 possible outcomes of round 
and for each of the outcomes of the first two rounds, there are 
 possible
outcomes of round  Consequently, by the generalized basic principle of
counting, there are 
 possible outcomes of the tournament. Indeed,
the same argument can be used to show that a knockout tournament of 
players has 
 possible outcomes.
Knowing the preceding result, it is not difficult to come up with a more direct
argument by showing that there is a one-to-one correspondence between the set
of possible tournament results and the set of permutations of 
 To obtain
such a correspondence, rank the players as follows for any tournament result:
Give the tournament winner rank  and give the final-round loser rank  For the
𝑛/2
8
8
ቆ
8
2,  2,  2,  2ቇൌ8!
2ସ.
4
8!
2ସ4! .
2
8!2ସ
2ସ4! ൌ8!
4!
1.
ቆ8
4ቇ
4
4!
4
4
4!ቆ8
4ቇൌ8!
4!
1,
4!
2!
2,
2!
1!
3.
  8!
4!
4!
2!
2!
1! ൌ8!
𝑛ൌ2௠
𝑛!
1, . . . ,𝑛.
1,
2.
28 of 848

two players who lost in the next-to-last round, give rank  to the one who lost to
the player ranked  and give rank  to the one who lost to the player ranked 
For the four players who lost in the second-to-last round, give rank  to the one
who lost to player ranked  rank  to the one who lost to the player ranked 
rank  to the one who lost to the player ranked  and rank  to the one who lost
to the player ranked  Continuing on in this manner gives a rank to each player.
(A more succinct description is to give the winner of the tournament rank  and
let the rank of a player who lost in a round having 
 matches be 
 plus the rank
of the player who beat him, for 
) In this manner, the result of the
tournament can be represented by a permutation 
 where  is the
player who was given rank  Because different tournament results give rise to
different permutations, and because there is a tournament result for each
permutation, it follows that there are the same number of possible tournament
results as there are permutations of 
Example 5e
* Asterisks denote material that is optional.
An individual has gone fishing at Lake Ticonderoga, which contains four types of fish:
lake trout, catfish, bass, and bluefish. If we take the result of the fishing trip to be the
numbers of each type of fish caught, let us determine the number of possible
outcomes when a total of 
 fish are caught. To do so, note that we can denote the
outcome of the fishing trip by the vector 
 where 
 is the number of
trout that are caught, 
 is the number of catfish, 
 is the number of bass, and 
 is
the number of bluefish. Thus, the number of possible outcomes when a total of 
fish are caught is the number of nonnegative integer vectors 
 that sum
to 
3
1
4
2.
5
1,
6
2,
7
3,
8
4.
1
2௞
2௞
𝑘ൌ0,  . . . , 𝑚െ1.
𝑖ଵ,𝑖ଶ, . . . ,𝑖௡,
𝑖௝
𝑗.
1, . . . ,𝑛.
ሺ𝑥ଵ൅𝑥ଶ൅𝑥ଷሻଶ
ൌ
ቆ
2
2,  0,  0ቇ 𝑥ଵ
ଶ𝑥ଶ
଴𝑥ଷ
଴൅ቆ
2
0,  2,  0ቇ 𝑥ଵ
଴𝑥ଶ
ଶ𝑥ଷ
଴
൅ቆ
2
0,  0,  2ቇ 𝑥ଵ
଴𝑥ଶ
଴𝑥ଷ
ଶ൅ቆ
2
1,  1,  0ቇ 𝑥ଵ
ଵ𝑥ଶ
ଵ𝑥ଷ
଴
൅ቆ
2
1,  0,  1ቇ 𝑥ଵ
ଵ𝑥ଶ
଴𝑥ଷ
ଵ൅ቆ
2
0,  1,  1ቇ 𝑥ଵ
଴𝑥ଶ
ଵ𝑥ଷ
ଵ
ൌ
𝑥ଵ
ଶ൅𝑥ଶ
ଶ൅𝑥ଷ
ଶ൅2𝑥ଵ𝑥ଶ൅2𝑥ଵ𝑥ଷ൅2𝑥ଶ𝑥ଷ
10
ሺ𝑥ଵ, 𝑥ଶ,  𝑥ଷ,  𝑥ସሻ
𝑥ଵ
𝑥ଶ
𝑥ଷ
𝑥ସ
10
ሺ𝑥ଵ, 𝑥ଶ,  𝑥ଷ,  𝑥ସሻ
10.
29 of 848

More generally, if we supposed there were  types of fish and that a total of  were
caught, then the number of possible outcomes would be the number of nonnegative
integer-valued vectors 
 such that
To compute this number, let us start by considering the number of positive integer-
valued vectors 
 that satisfy the preceding. To determine this number,
suppose that we have  consecutive zeroes lined up in a row:
Note that any selection of 
 of the 
 spaces between adjacent zeroes (see
Figure 1.2
) corresponds to a positive solution of 6.1
 by letting 
 be the
number of zeroes before the first chosen space, 
 be the number of zeroes between
the first and second chosen space, 
 and 
 being the number of zeroes following
the last chosen space.
Figure 1.2 Number of positive solutions.
For instance, if we have 
 and 
 then (with the choices represented by dots)
the choice
corresponds to the solution 
 As positive solutions of (6.1)
correspond, in a one-to-one fashion, to choices of 
 of the adjacent spaces, it
follows that the number of differerent positive solutions is equal to the number of
different selections of 
 of the 
 adjacent spaces. Consequently, we have the
following proposition.
Proposition 6.1
𝑟
𝑛
𝑥ଵ, . . . ,𝑥௥
𝑥ଵ൅𝑥ଶ൅. . . ൅𝑥௥ൌ𝑛
(6.1)
𝑥ଵ, . . . ,𝑥௥
𝑛
0 0 0  . . .  0 0
𝑟െ1
𝑛െ1
𝑥ଵ
𝑥ଶ
. . . ,
𝑥௡
𝑛ൌ8
𝑟ൌ3,
0.0 0 0 0.0 0 0
𝑥ଵൌ1, 𝑥ଶൌ4, 𝑥ଷൌ3.
𝑟െ1
𝑟െ1
𝑛െ1
30 of 848

There are 
 distinct positive integer-valued vectors 
satisfying the equation
To obtain the number of nonnegative (as opposed to positive) solutions, note that the
number of nonnegative solutions of 
 is the same as the number
of positive solutions of 
 (seen by letting 
).
Hence, from Proposition 6.1
, we obtain the following proposition.
Proposition 6.2
There are 
 distinct nonnegative integer-valued vectors 
satisfying the equation
Thus, using Proposition 6.2
, we see that there are 
 possible
outcomes when a total of 
 Lake Ticonderoga fish are caught.
Example 6a
How many distinct nonnegative integer-valued solutions of 
 are
possible?
Solution
There are 
 such solutions: (0, 3), (1, 2), (2, 1), (3, 0).
Example 6b
An investor has $20,000 to invest among 4 possible investments. Each
investment must be in units of $1000. If the total $20,000 is to be invested, how
many different investment strategies are possible? What if not all the money
needs to be invested?
Solution
If we let 
 2, 3, 4, denote the number of thousands invested in investment
 then, when all is to be invested, 
 are integers satisfying the
equation
ቆ𝑛െ1
𝑟െ1ቇ
ሺ𝑥ଵ, 𝑥ଶ, . . . ,𝑥௥ሻ
𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ൌ𝑛, 𝑥௜൐0, 𝑖ൌ1, . . . ,𝑟
𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ൌ𝑛
𝑦ଵ൅⋯൅𝑦௥ൌ𝑛൅𝑟
𝑦௜ൌ𝑥௜൅1, 𝑖ൌ1, . . . ,𝑟
ቆ𝑛൅𝑟െ1
𝑟െ1
ቇ
ሺ𝑥ଵ,𝑥ଶ, . . . ,𝑥௥ሻ
𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ൌ𝑛
ቆ13
3 ቇൌ286
10
𝑥ଵ൅𝑥ଶൌ3
ቆ3 ൅2 െ1
2 െ1
ቇൌ4
𝑥௜, 𝑖ൌ1,
𝑖,
𝑥ଵ,  𝑥ଶ,  𝑥ଷ,  𝑥ସ
31 of 848

Hence, by Proposition 6.2
, there are 
 possible investment
strategies. If not all of the money needs to be invested, then if we let 
 denote
the amount kept in reserve, a strategy is a nonnegative integer-valued vector
 satisfying the equation
Hence, by Proposition 6.2
, there are now 
 possible strategies.
Example 6c
How many terms are there in the multinomial expansion of 
Solution
where the sum is over all nonnegative integer-valued 
 such that
 Hence, by Proposition 6.2
, there are 
 such
terms.
Example 6d
Let us consider again Example 4c
, in which we have a set of  items, of
which 
 are (indistinguishable and) defective and the remaining 
 are (also
indistinguishable and) functional. Our objective is to determine the number of
linear orderings in which no two defectives are next to each other. To determine
this number, let us imagine that the defective items are lined up among
themselves and the functional ones are now to be put in position. Let us denote
 as the number of functional items to the left of the first defective, 
 as the
number of functional items between the first two defectives, and so on. That is,
schematically, we have
Now, there will be at least one functional item between any pair of defectives as
long as 
 Hence, the number of outcomes satisfying the
condition is the number of vectors 
 that satisfy the equation
𝑥ଵ൅𝑥ଶ൅𝑥ଷ൅𝑥ସൌ20 𝑥௜൒0
ቆ23
3 ቇൌ1771
𝑥ହ
ሺ𝑥ଵ,  𝑥ଶ,  𝑥ଷ,  𝑥ସ,  𝑥ହሻ
𝑥ଵ൅𝑥ଶ൅𝑥ଷ൅𝑥ସ൅𝑥ହൌ20
ቆ24
4 ቇൌ10,626
ሺ𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ሻ௡?
ሺ𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ሻ௡ൌ෍ቆ
𝑛
𝑛ଵ, . . . ,𝑛௥
ቇ 𝑥ଵ
௡భ⋯𝑥௥
௡ೝ
ሺ𝑛ଵ, . . . ,𝑛௥ሻ
𝑛ଵ൅⋯൅𝑛௥ൌ𝑛.
ቆ𝑛൅𝑟െ1
𝑟െ1
ቇ
𝑛
𝑚
𝑛െ𝑚
𝑥ଵ
𝑥ଶ
𝑥ଵ 0 𝑥ଶ 0⋯𝑥௠ 0 𝑥௠൅ଵ
𝑥௜൐0, 𝑖ൌ2, . . . ,𝑚.
𝑥ଵ, . . . ,𝑥௠൅ଵ
32 of 848

But, on letting 
 we see that this
number is equal to the number of positive vectors 
 that satisfy the
equation
Hence, by Proposition 6.1
, there are 
 such outcomes, in
agreement with the results of Example 4c
.
Suppose now that we are interested in the number of outcomes in which each
pair of defective items is separated by at least 2 functional items. By the same
reasoning as that applied previously, this would equal the number of vectors
satisfying the equation
Upon letting 
 we see that
this is the same as the number of positive solutions of the equation
Hence, from Proposition 6.1
, there are 
 such outcomes.
The basic principle of counting states that if an experiment consisting of two phases
is such that there are  possible outcomes of phase 1 and, for each of these 
outcomes, there are 
 possible outcomes of phase 2, then there are nm possible
outcomes of the experiment.
There are 
 possible linear orderings of  items. The quantity 0!
is defined to equal 1.
Let
𝑥ଵ൅⋯൅𝑥௠൅ଵൌ𝑛െ𝑚, 𝑥ଵ൒0, 𝑥௠൅ଵ൒0, 𝑥௜൐0, 𝑖ൌ2, . . . , 𝑚
𝑦ଵൌ𝑥ଵ൅1, 𝑦௜ൌ𝑥௜, 𝑖ൌ2, . . . ,𝑚, 𝑦௠൅ଵൌ𝑥௠൅ଵ൅1,
ሺ𝑦ଵ, . . . ,𝑦௠൅ଵሻ
𝑦ଵ൅𝑦ଶ൅⋯൅𝑦௠൅ଵൌ𝑛െ𝑚൅2
ቆ𝑛െ𝑚൅1
𝑚
ቇ
𝑥ଵ൅⋯൅𝑥௠൅ଵൌ𝑛െ𝑚, 𝑥ଵ൒0, 𝑥௠൅ଵ൒0, 𝑥௜൒2, 𝑖ൌ2, . . . , 𝑚
𝑦ଵൌ𝑥ଵ൅1, 𝑦௜ൌ𝑥௜െ1, 𝑖ൌ2, . . . ,𝑚, 𝑦௠൅ଵൌ𝑥௠൅ଵ൅1,
𝑦ଵ൅⋯൅𝑦௠൅ଵൌ𝑛െ2𝑚൅3
ቆ𝑛െ2𝑚൅2
𝑚
ቇ
𝑛
𝑛
𝑚
𝑛! ൌ𝑛ሺ𝑛െ1ሻ⋯3 ⋅2 ⋅1
𝑛
ቆ𝑛
𝑖ቇൌ
𝑛!
ሺ𝑛െ𝑖ሻ! 𝑖!
33 of 848

when 
 and let it equal 0 otherwise. This quantity represents the number of
different subgroups of size  that can be chosen from a set of size  It is often called
a binomial coefficient because of its prominence in the binomial theorem, which
states that
For nonnegative integers 
 summing to 
is the number of divisions of  items into  distinct nonoverlapping subgroups of
sizes 
 These quantities are called multinomial coefficients.
0 ൑𝑖൑𝑛,
𝑖
𝑛.
ሺ𝑥൅𝑦ሻ௡ൌ෍
௜ൌ଴
௡
ቆ𝑛
𝑖ቇ 𝑥௜𝑦௡െ௜
𝑛ଵ, . . . ,𝑛௥
𝑛,
ቆ
𝑛
𝑛ଵ, 𝑛ଶ, .   .   . , 𝑛௥
ቇൌ
𝑛!
𝑛ଵ!𝑛ଶ!⋯𝑛௥!
𝑛
𝑟
𝑛ଵ,𝑛ଶ. . . ,𝑛௥.
1.
a. How many different 7-place license plates are possible if the first
2 places are for letters and the other 5 for numbers?
b. Repeat part (a) under the assumption that no letter or number
can be repeated in a single license plate.
2. How many outcome sequences are possible when a die is rolled four
times, where we say, for instance, that the outcome is 3, 4, 3, 1 if the
first roll landed on 3, the second on 4, the third on 3, and the fourth on
1?
3. Twenty workers are to be assigned to 20 different jobs, one to each
job. How many different assignments are possible?
4. John, Jim, Jay, and Jack have formed a band consisting of 4
instruments. If each of the boys can play all 4 instruments, how many
different arrangements are possible? What if John and Jim can play all
4 instruments, but Jay and Jack can each play only piano and drums?
5. For years, telephone area codes in the United States and Canada
consisted of a sequence of three digits. The first digit was an integer
between 2 and 9, the second digit was either 0 or 1, and the third digit
was any integer from 1 to 9. How many area codes were possible?
How many area codes starting with a 4 were possible?
6. A well-known nursery rhyme starts as follows:
34 of 848

“As I was going to St. Ives
I met a man with 7 wives.
Each wife had 7 sacks.
Each sack had 7 cats.
Each cat had 7 kittens
”
How many kittens did the traveler meet?
7.
a. In how many ways can 3 boys and 3 girls sit in a row?
b. In how many ways can 3 boys and 3 girls sit in a row if the boys
and the girls are each to sit together?
c. In how many ways if only the boys must sit together?
d. In how many ways if no two people of the same sex are allowed
to sit together?
8. When all letters are used, how many different letter arrangements
can be made from the letters
a. Fluke?
b. Propose?
c. Mississippi?
d. Arrange?
9. A child has 12 blocks, of which 6 are black, 4 are red, 1 is white, and
1 is blue. If the child puts the blocks in a line, how many arrangements
are possible?
10. In how many ways can 8 people be seated in a row if
a. there are no restrictions on the seating arrangement?
b. persons  and  must sit next to each other?
c. there are 4 men and 4 women and no 2 men or 2 women can sit
next to each other?
d. there are 5 men and they must sit next to one another?
e. there are 4 married couples and each couple must sit together?
11. In how many ways can 3 novels, 2 mathematics books, and 1
chemistry book be arranged on a bookshelf if
a. the books can be arranged in any order?
b. the mathematics books must be together and the novels must
be together?
c. the novels must be together, but the other books can be
arranged in any order?
. . .
𝐴
𝐵
35 of 848

12. How many  digit numbers 
 with 
 all ranging from  to 
have at least  of their digits equal. How many have exactly  equal
digits.
13. How many different letter permutations, of any length, can be made
using the letters M O T T O. (For instance, there are  possible
permutations of length )
14. Five separate awards (best scholarship, best leadership qualities,
and so on) are to be presented to selected students from a class of 30.
How many different outcomes are possible if
a. a student can receive any number of awards?
b. each student can receive at most 1 award?
15. Consider a group of 20 people. If everyone shakes hands with
everyone else, how many handshakes take place?
16. How many 5-card poker hands are there?
17. A dance class consists of 22 students, of which 10 are women and
12 are men. If 5 men and 5 women are to be chosen and then paired
off, how many results are possible?
18. A student has to sell 2 books from a collection of 6 math, 7 science,
and 4 economics books. How many choices are possible if
a. both books are to be on the same subject?
b. the books are to be on different subjects?
19. Seven different gifts are to be distributed among 10 children. How
many distinct results are possible if no child is to receive more than one
gift?
20. A committee of 7, consisting of 2 Republicans, 2 Democrats, and 3
Independents, is to be chosen from a group of 5 Republicans, 6
Democrats, and 4 Independents. How many committees are possible?
21. From a group of 8 women and 6 men, a committee consisting of 3
men and 3 women is to be formed. How many different committees are
possible if
a. 2 of the men refuse to serve together?
b. 2 of the women refuse to serve together?
c. 1 man and 1 woman refuse to serve together?
22. A person has 8 friends, of whom 5 will be invited to a party.
a. How many choices are there if 2 of the friends are feuding and
will not attend together?
b. How many choices if 2 of the friends will only attend together?
3
𝑥𝑦𝑧,
𝑥,  𝑦,  𝑧
0
9
2
2
3
1.
36 of 848

23. Consider the grid of points shown at the top of the next column.
Suppose that, starting at the point labeled 
 you can go one step up or
one step to the right at each move. This procedure is continued until
the point labeled  is reached. How many different paths from  to 
are possible?
Hint: Note that to reach  from 
 you must take 4 steps to the right
and 3 steps upward.
24. In Problem 23
, how many different paths are there from  to 
that go through the point circled in the following lattice?
25. A psychology laboratory conducting dream research contains 3
rooms, with 2 beds in each room. If 3 sets of identical twins are to be
assigned to these 6 beds so that each set of twins sleeps in different
beds in the same room, how many assignments are possible?
26.
a. Show 
b. Simplify 
𝐴,
𝐵
𝐴
𝐵
𝐵
𝐴,
𝐴
𝐵
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ 2௞ൌ3௡
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ 𝑥௞
37 of 848

27. Expand 
28. The game of bridge is played by 4 players, each of whom is dealt
13 cards. How many bridge deals are possible?
29. Expand 
30. If 12 people are to be divided into 3 committees of respective sizes
3, 4, and 5, how many divisions are possible?
31. If 8 new teachers are to be divided among 4 schools, how many
divisions are possible? What if each school must receive 2 teachers?
32. Ten weight lifters are competing in a team weight-lifting contest. Of
the lifters, 3 are from the United States, 4 are from Russia, 2 are from
China, and 1 is from Canada. If the scoring takes account of the
countries that the lifters represent, but not their individual identities,
how many different outcomes are possible from the point of view of
scores? How many different outcomes correspond to results in which
the United States has 1 competitor in the top three and 2 in the bottom
three?
33. Delegates from 10 countries, including Russia, France, England,
and the United States, are to be seated in a row. How many different
seating arrangements are possible if the French and English delegates
are to be seated next to each other and the Russian and U.S.
delegates are not to be next to each other?
* 34. If 8 identical blackboards are to be divided among 4 schools, how
many divisions are possible? How many if each school must receive at
least 1 blackboard?
* 35. An elevator starts at the basement with 8 people (not including the
elevator operator) and discharges them all by the time it reaches the
top floor, number 6. In how many ways could the operator have
perceived the people leaving the elevator if all people look alike to him?
What if the 8 people consisted of 5 men and 3 women and the operator
could tell a man from a woman?
* 36. We have $20,000 that must be invested among 4 possible
opportunities. Each investment must be integral in units of $1000, and
there are minimal investments that need to be made if one is to invest
in these opportunities. The minimal investments are $2000, $2000,
$3000, and $4000. How many different investment strategies are
available if
a. an investment must be made in each opportunity?
b. investments must be made in at least 3 of the 4 opportunities?
* 37. Suppose that 
 fish are caught at a lake that contains  distinct
types of fish.
ሺ3𝑥ଶ൅𝑦ሻ
ହ.
ሺ𝑥ଵ൅2𝑥ଶ൅3𝑥ଷሻସ.
10
5
38 of 848

a. How many different outcomes are possible, where an outcome
specifies the numbers of caught fish of each of the  types?
b. How many outcomes are possible when  of the 
 fish caught
are trout?
c. How many when at least  of the 
 are trout?
5
3
10
2
10
1. Prove the generalized version of the basic counting principle.
2. Two experiments are to be performed. The first can result in any one of 
possible outcomes. If the first experiment results in outcome  then the
second experiment can result in any of 
 possible outcomes, 
What is the number of possible outcomes of the two experiments?
3. In how many ways can  objects be selected from a set of  objects if the
order of selection is considered relevant?
4. There are 
 different linear arrangements of  balls of which  are black
and 
 are white. Give a combinatorial explanation of this fact.
5. Determine the number of vectors 
 such that each 
 is either 0 or
1 and
6. How many vectors 
 are there for which each 
 is a positive integer
such that 
 and 
7. Give an analytic proof of Equation (4.1)
.
8. Prove that
Hint: Consider a group of  men and 
 women. How many groups of size 
are possible?
9. Use Theoretical Exercise 8
 to prove that
10. From a group of  people, suppose that we want to choose a committee of
𝑚
𝑖,
𝑛௜
𝑖ൌ1, 2, . . . ,𝑚.
𝑟
𝑛
ቆ𝑛
𝑟ቇ
𝑛
𝑟
𝑛െ𝑟
ሺ𝑥ଵ, . . . ,𝑥௡ሻ,
𝑥௜
෍
௜ൌଵ
௡
𝑥௜൒𝑘
𝑥ଵ, . . . ,𝑥௞
𝑥௜
1 ൑𝑥௜൑𝑛
𝑥ଵ൏𝑥ଶ൏⋯൏𝑥௞?
ቆ𝑛൅𝑚
𝑟
ቇ
ൌ
ቆ𝑛
0ቇቆ𝑚
𝑟ቇ൅ቆ𝑛
1ቇቆ𝑚
𝑟െ1ቇ
൅⋯൅ቆ𝑛
𝑟ቇቆ𝑚
0ቇ
𝑛
𝑚
𝑟
ቆ2𝑛
𝑛ቇൌ
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ
ଶ
𝑛
39 of 848

 one of whom is to be designated as chairperson.
a. By focusing first on the choice of the committee and then on the choice
of the chair, argue that there are 
 possible choices.
b. By focusing first on the choice of the nonchair committee members and
then on the choice of the chair, argue that there are 
possible choices.
c. By focusing first on the choice of the chair and then on the choice of
the other committee members, argue that there are 
 possible
choices.
d. Conclude from parts (a), (b), and (c) that
e. Use the factorial definition of 
 to verify the identity in part (d).
11. The following identity is known as Fermat’s combinatorial identity:
Give a combinatorial argument (no computations are needed) to establish this
identity.
Hint: Consider the set of numbers 1 through  How many subsets of size 
have  as their highest numbered member?
12. Consider the following combinatorial identity:
a. Present a combinatorial argument for this identity by considering a set
of  people and determining, in two ways, the number of possible
selections of a committee of any size and a chairperson for the
committee.
Hint:
i. How many possible selections are there of a committee of size 
and its chairperson?
ii. How many possible selections are there of a chairperson and
the other committee members?
𝑘, 𝑘൑𝑛,
ቆ𝑛
𝑘ቇ 𝑘
ቆ
𝑛
𝑘െ1ቇ ሺ𝑛െ𝑘൅1ሻ
𝑛 ቆ𝑛െ1
𝑘െ1ቇ
𝑘 ቆ𝑛
𝑘ቇൌሺ𝑛െ𝑘൅1ሻቆ
𝑛
𝑘െ1ቇൌ𝑛 ቆ𝑛െ1
𝑘െ1ቇ
ቆ𝑚
𝑟ቇ
ቆ𝑛
𝑘ቇൌ
෍
௜ൌ௞
௡
ቆ𝑖െ1
𝑘െ1ቇ 𝑛൒𝑘
𝑛.
𝑘
𝑖
෍
௞ൌଵ
௡
𝑘 ቆ𝑛
𝑘ቇൌ𝑛⋅2௡െଵ
𝑛
𝑘
40 of 848

b. Verify the following identity for 
For a combinatorial proof of the preceding, consider a set of  people
and argue that both sides of the identity represent the number of
different selections of a committee, its chairperson, and its secretary
(possibly the same as the chairperson).
Hint:
i. How many different selections result in the committee containing
exactly  people?
ii. How many different selections are there in which the
chairperson and the secretary are the same? (ANSWER: 
)
iii. How many different selections result in the chairperson and the
secretary being different?
c. Now argue that
13. Show that, for 
Hint: Use the binomial theorem.
14. From a set of  people, a committee of size  is to be chosen, and from
this committee, a subcommittee of size 
 is also to be chosen.
a. Derive a combinatorial identity by computing, in two ways, the number
of possible choices of the committee and subcommittee–first by
supposing that the committee is chosen first and then the
subcommittee is chosen, and second by supposing that the
subcommittee is chosen first and then the remaining members of the
committee are chosen.
b. Use part (a) to prove the following combinatorial identity:
c. Use part (a) and Theoretical Exercise 13
 to show that
𝑛ൌ1,  2,  3,  4,  5:
෍
௞ൌଵ
௡
ቆ𝑛
𝑘ቇ 𝑘ଶൌ2௡െଶ𝑛ሺ𝑛൅1ሻ
𝑛
𝑘
𝑛2௡െଵ.
෍
௞ൌଵ
௡
ቆ𝑛
𝑘ቇ 𝑘ଷൌ2௡െଷ𝑛ଶሺ𝑛൅3ሻ
𝑛൐0,
෍
௜ൌ଴
௡
ሺെ1ሻ௜ቆ𝑛
𝑖ቇൌ0
𝑛
𝑗
𝑖,𝑖൑𝑗,
෍
௝ൌ௜
௡
ቆ𝑛
𝑗ቇቆ𝑗
𝑖ቇൌቆ𝑛
𝑖ቇ 2௡െ௜ 𝑖൑𝑛
41 of 848

15. Let 
 be the number of vectors 
 for which each 
 is a
positive integer satisfying 
 and 
a. Without any computations, argue that
Hint: How many vectors are there in which 
b. Use the preceding recursion to compute 
Hint: First compute 
 for 
 2, 3, 4, 5.
16. Consider a tournament of  contestants in which the outcome is an
ordering of these contestants, with ties allowed. That is, the outcome
partitions the players into groups, with the first group consisting of the players
who tied for first place, the next group being those who tied for the next-best
position, and so on. Let 
 denote the number of different possible
outcomes. For instance, 
 since, in a tournament with 2 contestants,
player 1 could be uniquely first, player 2 could be uniquely first, or they could
tie for first
a. List all the possible outcomes when 
b. With (0) defined to equal 1, argue, without any computations, that
Hint: How many outcomes are there in which  players tie for last
place?
c. Show that the formula of part (b) is equivalent to the following:
d. Use the recursion to find (3) and (4).
17. Present a combinatorial explanation of why 
18. Argue that
෍
௝ൌ௜
௡
ቆ𝑛
𝑗ቇቆ𝑗
𝑖ቇ ሺെ1ሻ௡െ௝ൌ0 𝑖൏𝑛
𝐻௞ሺ𝑛ሻ
𝑥ଵ, . . . ,𝑥௞
𝑥௜
1 ൑𝑥௜൑𝑛
𝑥ଵ൑𝑥ଶ൑⋯൑𝑥௞.
𝐻ଵሺ𝑛ሻൌ
𝑛
𝐻௞ሺ𝑛ሻൌ
෍
௝ൌଵ
௡
𝐻௞െଵሺ𝑗ሻ 𝑘൐1
𝑥௞ൌ𝑗?
𝐻ଷሺ5ሻ.
𝐻ଶሺ𝑛ሻ
𝑛ൌ1,
𝑛
𝑁ሺ𝑛ሻ
𝑁ሺ2ሻൌ3,
𝑛ൌ3.
𝑁
𝑁ሺ𝑛ሻൌ෍
௜ൌଵ
௡
ቆ𝑛
𝑖ቇ 𝑁ሺ𝑛െ𝑖ሻ
𝑖
𝑁ሺ𝑛ሻൌ
෍
௜ൌ଴
௡െଵ
ቆ𝑛
𝑖ቇ 𝑁ሺ𝑖ሻ
𝑁
𝑁
ቆ𝑛
𝑟ቇൌቆ
𝑛
𝑟,  𝑛െ𝑟ቇ
42 of 848

Hint: Use an argument similar to the one used to establish Equation (4.1)
.
19. Prove the multinomial theorem.
* 20. In how many ways can  identical balls be distributed into  urns so that
the th urn contains at least 
 balls, for each 
 Assume that
* 21. Argue that there are exactly 
 solutions of
for which exactly  of the 
 are equal to 0.
* 22. Consider a function 
 of  variables. How many different
partial derivatives of order  does  possess?
* 23. Determine the nu mber of vectors 
 such that each 
 is a
nonnegative integer and
ቆ
𝑛
𝑛ଵ, 𝑛ଶ, .   .   . , 𝑛௥
ቇൌ
ቆ
𝑛െ1
𝑛ଵെ1, 𝑛ଶ, .   .   . , 𝑛௥
ቇ
൅ቆ
𝑛െ1
𝑛ଵ, 𝑛ଶെ1, .   .   . , 𝑛௥
ቇ൅⋯
൅ቆ
𝑛െ1
𝑛ଵ, 𝑛ଶ, .   .   . , 𝑛௥െ1ቇ
𝑛
𝑟
𝑖
𝑚௜
𝑖ൌ1, . . . ,𝑟?
𝑛൒෍
௜ൌଵ
௥
𝑚௜.
ቆ𝑟
𝑘ቇቆ
𝑛െ1
𝑛െ𝑟൅𝑘ቇ
𝑥ଵ൅𝑥ଶ൅⋯൅𝑥௥ൌ𝑛
𝑘
𝑥௜
𝑓ሺ𝑥ଵ, . . . ,𝑥௡ሻ
𝑛
𝑟
𝑓
ሺ𝑥ଵ, . . . ,𝑥௡ሻ
𝑥௜
෍
௜ൌଵ
௡
𝑥௜൑𝑘
1. How many different linear arrangements are there of the letters A, B, C, D,
E, F for which
a. A and B are next to each other?
b. A is before B?
c. A is before B and B is before C?
d. A is before B and C is before D?
e. A and B are next to each other and C and D are also next to each
other?
f. E is not last in line?
2. If 4 Americans, 3 French people, and 3 British people are to be seated in a
row, how many seating arrangements are possible when people of the same
nationality must sit next to each other?
43 of 848

3. A president, treasurer, and secretary, all different, are to be chosen from a
club consisting of 10 people. How many different choices of officers are
possible if
a. there are no restrictions?
b.  and  will not serve together?
c.  and  will serve together or not at all?
d.  must be an officer?
e.  will serve only if he is president?
4. A student is to answer 7 out of 10 questions in an examination. How many
choices has she? How many if she must answer at least 3 of the first 5
questions?
5. In how many ways can a man divide 7 gifts among his 3 children if the
eldest is to receive 3 gifts and the others 2 each?
6. How many different 7-place license plates are possible when 3 of the
entries are letters and 4 are digits? Assume that repetition of letters and
numbers is allowed and that there is no restriction on where the letters or
numbers can be placed.
7. Give a combinatorial explanation of the identity
8. Consider -digit numbers where each digit is one of the 10 integers
 How many such numbers are there for which
a. no two consecutive digits are equal?
b. 0 appears as a digit a total of  times, 
9. Consider three classes, each consisting of  students. From this group of 
 students, a group of 3 students is to be chosen.
a. How many choices are possible?
b. How many choices are there in which all 3 students are in the same
class?
c. How many choices are there in which 2 of the 3 students are in the
same class and the other student is in a different class?
d. How many choices are there in which all 3 students are in different
classes?
e. Using the results of parts (a) through (d), write a combinatorial identity.
10. How many 5-digit numbers can be formed from the integers 
 if no
digit can appear more than twice? (For instance, 41434 is not allowed.)
11. From 
 married couples, we want to select a group of  people that is not
𝐴
𝐵
𝐶
𝐷
𝐸
𝐹
ቆ𝑛
𝑟ቇൌቆ
𝑛
𝑛െ𝑟ቇ
𝑛
0,1, . . . ,9.
𝑖
𝑖ൌ0, . . . ,𝑛?
𝑛
3
𝑛
1,2, . . . ,9
10
6
44 of 848

allowed to contain a married couple.
a. How many choices are there?
b. How many choices are there if the group must also consist of  men
and  women?
12. A committee of 6 people is to be chosen from a group consisting of 7 men
and 8 women. If the committee must consist of at least 3 women and at least
2 men, how many different committees are possible?
* 13. An art collection on auction consisted of 4 Dalis, 5 van Goghs, and 6
Picassos. At the auction were 5 art collectors. If a reporter noted only the
number of Dalis, van Goghs, and Picassos acquired by each collector, how
many different results could have been recorded if all of the works were sold?
* 14. Determine the number of vectors 
 such that each 
 is a
positive integer and
where 
15. A total of  students are enrolled in a review course for the actuarial
examination in probability. The posted results of the examination will list the
names of those who passed, in decreasing order of their scores. For instance,
the posted result will be “Brown, Cho” if Brown and Cho are the only ones to
pass, with Brown receiving the higher score. Assuming that all scores are
distinct (no ties), how many posted results are possible?
16. How many subsets of size  of the set 
 contain at least
one of the elements 
17. Give an analytic verification of
Now, give a combinatorial argument for this identity.
18. In a certain community, there are 3 families consisting of a single parent
and 1 child, 3 families consisting of a single parent and 2 children, 5 families
consisting of 2 parents and a single child, 7 families consisting of 2 parents
and 2 children, and 6 families consisting of 2 parents and 3 children. If a
parent and child from the same family are to be chosen, how many possible
choices are there?
19. If there are no restrictions on where the digits and letters are placed, how
many 8-place license plates consisting of 5 letters and 3 digits are possible if
no repetitions of letters or digits are allowed? What if the 3 digits must be
consecutive?
3
3
ሺ𝑥ଵ, . . . ,𝑥௡ሻ
𝑥௜
෍
௜ൌଵ
௡
𝑥௜൑𝑘
𝑘൒𝑛.
𝑛
4
𝑆ൌሼ1, 2, . . . ,20ሽ
1, 2, 3, 4, 5?
ቆ𝑛
2ቇൌቆ𝑘
2ቇ൅𝑘ሺ𝑛െ𝑘ሻ൅ቆ𝑛െ𝑘
2
ቇ,    1 ൑𝑘൑𝑛
45 of 848

2.1 Introduction
2.2 Sample Space and Events
2.3 Axioms of Probability
2.4 Some Simple Propositions
2.5 Sample Spaces Having Equally Likely Outcomes
2.6 Probability as a Continuous Set Function
2.7 Probability as a Measure of Belief
In this chapter, we introduce the concept of the probability of an event and then show
how probabilities can be computed in certain situations. As a preliminary, however,
we need to discuss the concept of the sample space and the events of an
experiment.
20. Verify the identity
a. by a combinatorial argument that first notes that 
 is the number of
different  letter sequences that can be formed from an alphabet
consisting of  letters, and then determines how many of these letter
sequences have letter 1 a total of 
 times and letter 2 a total of 
times and ... and letter  a total of 
 times;
b. by using the multinomial theorem.
21. Simplify 
෍
௫భ൅. . . ൅௫ೝൌ௡, ௫೔൒଴
𝑛!
𝑥ଵ!𝑥ଶ!⋯𝑥௥! ൌ𝑟௡
𝑟௡
𝑛
𝑟
𝑥ଵ
𝑥ଶ
𝑟
𝑥௥
𝑛െቆ𝑛
2ቇ൅ቆ𝑛
3ቇെ… ൅ሺെ1ሻ௡൅ଵቆ𝑛
𝑛ቇ
46 of 848

Consider an experiment whose outcome is not predictable with certainty. However,
although the outcome of the experiment will not be known in advance, let us suppose
that the set of all possible outcomes is known. This set of all possible outcomes of an
experiment is known as the sample space of the experiment and is denoted by S.
Following are some examples:
1. If the outcome of an experiment consists of the determination of the sex of a
newborn child, then
where the outcome  means that the child is a girl and  that it is a boy.
2. If the outcome of an experiment is the order of finish in a race among the 7
horses having post positions 1, 2, 3, 4, 5, 6, and 7, then
The outcome (2, 3, 1, 6, 5, 4, 7) means, for instance, that the number 2 horse
comes in first, then the number 3 horse, then the number 1 horse, and so on.
3. If the experiment consists of flipping two coins, then the sample space
consists of the following four points:
The outcome will be (
) if both coins are heads, (
) if the first coin is
heads and the second tails, (
) if the first is tails and the second heads, and
(
) if both coins are tails.
4. If the experiment consists of tossing two dice, then the sample space consists
of the 36 points
where the outcome 
 is said to occur if  appears on the leftmost die and 
on the other die.
5. If the experiment consists of measuring (in hours) the lifetime of a transistor,
then the sample space consists of all nonnegative real numbers; that is,
Any subset  of the sample space is known as an event. In other words, an event is
a set consisting of possible outcomes of the experiment. If the outcome of the
experiment is contained in 
 then we say that  has occurred. Following are some
examples of events.
𝑆ൌሼ𝑔, 𝑏ሽ
𝑔
𝑏
𝑆ൌሼall 7! permutations of ሺ1, 2, 3, 4, 5, 6, 7ሻሽ
𝑆ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻ, ሺ𝑡, 𝑡ሻሽ
ℎ, ℎ
ℎ, 𝑡
𝑡, ℎ
𝑡, 𝑡
𝑆ൌሼሺ𝑖, 𝑗ሻ: 𝑖, 𝑗ൌ1, 2, 3, 4, 5, 6ሽ
ሺ𝑖, 𝑗ሻ
𝑖
𝑗
𝑆ൌሼ𝑥: 0 ൑𝑥൏∞ሽ
𝐸
𝐸,
𝐸
47 of 848

In the preceding Example 1, if 
 then  is the event that the child is a girl.
Similarly, if 
 then  is the event that the child is a boy.
In Example 2, if
then  is the event that horse 3 wins the race.
In Example 3, if 
 then  is the event that a head appears on the
first coin.
In Example 4, if 
 then  is the event that
the sum of the dice equals 7.
In Example 5, if 
 then  is the event that the transistor does not
last longer than 5 hours.
For any two events  and  of a sample space  we define the new event 
 to
consist of all outcomes that are either in  or in  or in both  and 
 That is, the
event 
 will occur if either E or  occurs. For instance, in Example 1, if 
 is
the event that the child is a girl and 
 is the event that the child is a boy, then
is the whole sample space  In Example 3, if 
 is the event that the
first coin lands heads, and 
 is the event that the second coin lands
heads, then
is the event that at least one of the coins lands heads and thus will occur provided
that both coins do not land tails.
The event 
 is called the union of the event  and the event 
Similarly, for any two events  and 
 we may also define the new event EF, called
the intersection of  and 
 to consist of all outcomes that are both in  and in 
 That
is, the event EF (sometimes written 
) will occur only if both  and  occur. For
instance, in Example 3, if 
 is the event that at least 1 head
occurs and 
 is the event that at least 1 tail occurs, then
𝐸ൌሼ𝑔ሽ,
𝐸
𝐹ൌሼ𝑏ሽ,
𝐹
𝐸ൌሼall outcomes in 𝑆 starting with a 3ሽ
𝐸
𝐸ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻሽ,
𝐸
𝐸ൌሼሺ1, 6ሻ, ሺ2, 5ሻ, ሺ3, 4ሻ, ሺ4, 3ሻ, ሺ5, 2ሻ, ሺ6, 1ሻሽ,
𝐸
𝐸ൌሼ𝑥: 0 ൑𝑥൑5ሽ,
𝐸
𝐸
𝐹
𝑆,
𝐸∪𝐹
𝐸
𝐹
𝐸
𝐹.
𝐸∪𝐹
𝐹
𝐸ൌሼ𝑔ሽ
𝐹ൌሼ𝑏ሽ
𝐸∪𝐹ൌሼ𝑔, 𝑏ሽ
𝑆.
𝐸ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻሽ
𝐹ൌሼሺ𝑡, ℎሻ, ሺℎ, ℎሻሽ
𝐸∪𝐹ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻሽ
𝐸∪𝐹
𝐸
𝐹.
𝐸
𝐹,
𝐸
𝐹,
𝐸
𝐹.
𝐸∩𝐹
𝐸
𝐹
𝐸ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻሽ
𝐹ൌሼሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻ, ሺ𝑡, 𝑡ሻሽ
𝐸𝐹ൌሼሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻሽ
48 of 848

is the event that exactly 1 head and 1 tail occur. In Example 4, if
 is the event that the sum of the dice is 7
and 
 is the event that the sum is 6, then the
event EF does not contain any outcomes and hence could not occur. To give such an
event a name, we shall refer to it as the null event and denote it by 
 (That is, 
refers to the event consisting of no outcomes.) If 
 then  and  are said to be
mutually exclusive.
We define unions and intersections of more than two events in a similar manner. If
 are events, then the union of these events, denoted by 
 is defined
to be that event that consists of all outcomes that are in 
 for at least one value of
 Similarly, the intersection of the events 
 denoted by 
 is
defined to be the event consisting of those outcomes that are in all of the events
Finally, for any event 
 we define the new event 
 referred to as the complement of
 to consist of all outcomes in the sample space  that are not in 
 That is, 
 will
occur if and only if  does not occur. In Example 4, if event
 then 
 will occur when the sum of the
dice does not equal 7. Note that because the experiment must result in some
outcome, it follows that 
For any two events  and 
 if all of the outcomes in  are also in 
 then we say that
 is contained in 
 or  is a subset of 
 and write 
 (or equivalently, 
which we sometimes say as  is a superset of ). Thus, if 
 then the occurrence
of  implies the occurrence of 
 If 
 and 
 we say that  and  are equal
and write 
A graphical representation that is useful for illustrating logical relations among events
is the Venn diagram. The sample space  is represented as consisting of all the
outcomes in a large rectangle, and the events 
 are represented as
consisting of all the outcomes in given circles within the rectangle. Events of interest
can then be indicated by shading appropriate regions of the diagram. For instance, in
the three Venn diagrams shown in Figure 2.1
, the shaded areas represent,
respectively, the events 
EF, and 
 The Venn diagram in Figure 2.2
indicates that 
Figure 2.1 Venn diagrams.
𝐸ൌሼሺ1, 6ሻ, ሺ2, 5ሻ, ሺ3, 4ሻ, ሺ4, 3ሻ, ሺ5, 2ሻ, ሺ6, 1ሻሽ
𝐹ൌሼሺ1, 5ሻ, ሺ2, 4ሻ, ሺ3, 3ሻ, ሺ4, 2ሻ, ሺ5, 1ሻሽ
Ø.
Ø
𝐸𝐹ൌØ,
𝐸
𝐹
𝐸ଵ, 𝐸ଶ, ...
∪
௡ൌଵ
ஶ
𝐸௡,
𝐸௡
𝑛ൌ1, 2, ....
𝐸௡,
∩
௡ൌଵ
ஶ
𝐸௡,
𝐸௡, 𝑛ൌ1, 2, ... .
𝐸,
𝐸௖,
𝐸,
𝑆
𝐸.
𝐸௖
𝐸
𝐸ൌሼሺ1, 6ሻ, ሺ2, 5ሻ, ሺ3, 4ሻ, ሺ4, 3ሻ, ሺ5, 2ሻ, ሺ6, 1ሻሽ,
𝐸௖
𝑆௖ൌØ.
𝐸
𝐹,
𝐸
𝐹,
𝐸
𝐹,
𝐸
𝐹,
𝐸⊂𝐹
𝐹⊃𝐸,
𝐹
𝐸
𝐸⊂𝐹,
𝐸
𝐹.
𝐸⊂𝐹
𝐹⊂𝐸,
𝐸
𝐹
𝐸ൌ𝐹.
𝑆
𝐸, 𝐹, 𝐺, ...
𝐸∪𝐹,
𝐸௖.
𝐸⊂𝐹.
49 of 848

(a) Shaded region: 
(b) Shaded region: FF
(b) Shaded region: E
Figure 2.2
𝐸∪𝐹.
c
𝐸⊂𝐹.
50 of 848

The operations of forming unions, intersections, and complements of events obey
certain rules similar to the rules of algebra. We list a few of these rules:
These relations are verified by showing that any outcome that is contained in the
event on the left side of the equality sign is also contained in the event on the right
side, and vice versa. One way of showing this is by means of Venn diagrams. For
instance, the distributive law may be verified by the sequence of diagrams in Figure
2.3
.
Figure 2.3
(a) Shaded region: EG.
Commutative laws
𝐸∪𝐹
ൌ𝐹∪𝐸
𝐸𝐹
ൌ𝐹𝐸
Associative laws
ሺE ∪Fሻ∪G
ൌ𝐸∪ሺ𝐹∪𝐺ሻ
ሺ𝐸𝐹ሻ𝐺
ൌ𝐸ሺ𝐹𝐺ሻ
Distributive laws
ሺ𝐸∪𝐹ሻ𝐺
ൌ𝐸𝐺∪𝐹𝐺
𝐸𝐹∪𝐺
ൌሺ𝐸∪𝐺ሻሺ𝐹∪𝐺ሻ
ሺ𝐸∪𝐹ሻ𝐺ൌ𝐸𝐺∪𝐹𝐺.
51 of 848

(b) Shaded region: FG.
(c) Shaded region: 
The following useful relationships among the three basic operations of forming
unions, intersections, and complements are known as DeMorgan’s laws:
For instance, for two events E and F, DeMorgan’s laws state that
ሺ𝐸∪𝐹ሻ𝐺.
ቌ
௜
ଵ
௡
𝐸௜ቍ
௖
ൌ
௜
ଵ
௡
𝐸௜
௖
ቌ
௜
ଵ
௡
𝐸௜ቍ
௖
ൌ
௜
ଵ
௡
𝐸௜
௖
ሺ𝐸∪𝐹ሻ௖ൌ𝐸௖𝐹௖ and ሺ𝐸𝐹ሻ௖ൌ𝐸௖∪𝐹௖
52 of 848

which can be easily proven by using Venn diagrams (see Theoretical Exercise
7
).
To prove DeMorgan’s laws for general  suppose first that  is an outcome of
 Then  is not contained in 
 which means that x is not
contained in any of the events 
 implying that  is contained in 
 for
all 
 and thus is contained in 
 To go the other way, suppose
that x is an outcome of 
Then  is contained in 
 for all 
which means that  is not contained in 
 for any 
 implying that x is not
contained in 
in turn implying that  is contained in 
 This proves the first
of DeMorgan’s laws.
To prove the second of DeMorgan’s laws, we use the first law to obtain
which, since 
 is equivalent to
Taking complements of both sides of the preceding equation yields the result we
seek, namely,
One way of defining the probability of an event is in terms of its long run relative
frequency. Such a definition usually goes as follows: We suppose that an
experiment, whose sample space is  is repeatedly performed under exactly the
same conditions. For each event  of the sample space  we define 
 to be the
number of times in the first  repetitions of the experiment that the event  occurs.
Then 
 the probability of the event E, is defined as
𝑛,
𝑥
ቆ
∪
௜ൌଵ
௡
𝐸௜ቇ
௖
.
𝑥
∪
௜ൌଵ
௡
𝐸௜,
𝐸௜, 𝑖ൌ1, 2, ... , 𝑛,
𝑥
𝐸௜
௖
𝑖ൌ1,  2,   .   .   .  ,
∩
௜ൌଵ
௡
𝐸௜
௖.
∩
௜ൌଵ
௡
𝐸௜
௖.
𝑥
𝐸௜
௖
𝑖ൌ1, 2, . . . , 𝑛,
𝑥
𝐸௜
𝑖ൌ1, 2, . . . , 𝑛,
∪
௜
௡
𝐸௜,
𝑥
൬∪
ଵ
௡
𝐸௜൰
௖
.
ቆ
௜ൌଵ
௡
𝐸௜
௖ቇ
௖
ൌ
∩
௜ൌଵ
௡
ሺ𝐸௜
௖ሻ
௖
ሺ𝐸௖ሻ
௖ൌ𝐸,
൬∪
ଵ
௡
𝐸௜
௖൰
௖
ൌ∩
ଵ
௡
𝐸௜
ଵ
௡
𝐸௜
௖ൌቆ
ଵ
௡
𝐸௜ቇ
௖
𝑆,
𝐸
𝑆,
𝑛ሺ𝐸ሻ
𝑛
𝐸
𝑃ሺ𝐸ሻ,
53 of 848

That is, 
 is defined as the (limiting) proportion of time that  occurs. It is thus the
limiting relative frequency of 
Although the preceding definition is certainly intuitively pleasing and should always
be kept in mind by the reader, it possesses a serious drawback: How do we know
that 
 will converge to some constant limiting value that will be the same for
each possible sequence of repetitions of the experiment? For example, suppose that
the experiment to be repeatedly performed consists of flipping a coin. How do we
know that the proportion of heads obtained in the first  flips will converge to some
value as  gets large? Also, even if it does converge to some value, how do we know
that, if the experiment is repeatedly performed a second time, we shall obtain the
same limiting proportion of heads?
Proponents of the relative frequency definition of probability usually answer this
objection by stating that the convergence of 
 to a constant limiting value is an
assumption, or an axiom, of the system. However, to assume that 
 will
necessarily converge to some constant value seems to be an extraordinarily
complicated assumption. For, although we might indeed hope that such a constant
limiting frequency exists, it does not at all seem to be a priori evident that this need
be the case. In fact, would it not be more reasonable to assume a set of simpler and
more self-evident axioms about probability and then attempt to prove that such a
constant limiting frequency does in some sense exist? The latter approach is the
modern axiomatic approach to probability theory that we shall adopt in this text. In
particular, we shall assume that, for each event  in the sample space  there exists
a value 
 referred to as the probability of 
 We shall then assume that all these
probabilities satisfy a certain set of axioms, which, we hope the reader will agree, is
in accordance with our intuitive notion of probability.
Consider an experiment whose sample space is  For each event  of the sample
space  we assume that a number 
 is defined and satisfies the following three
axioms:
The three axioms of probability
Axiom 1
Axiom 2
𝑃ሺ𝐸ሻൌ
௡
ஶ
𝑛ሺ𝐸ሻ
𝑛
𝑃ሺ𝐸ሻ
𝐸
𝐸.
𝑛ሺ𝐸ሻ/𝑛
𝑛
𝑛
𝑛ሺ𝐸ሻ/𝑛
𝑛ሺ𝐸ሻ/𝑛
𝐸
𝑆,
𝑃ሺ𝐸ሻ,
𝐸.
𝑆.
𝐸
𝑆,
𝑃ሺ𝐸ሻ
0 ൑𝑃ሺ𝐸ሻ൑1
𝑃ሺ𝑆ሻൌ1
54 of 848

Axiom 3
For any sequence of mutually exclusive events 
 (that is, events for
which 
 when 
),
We refer to 
 as the probability of the event 
Thus, Axiom 1
 states that the probability that the outcome of the experiment is an
outcome in  is some number between 0 and 1. Axiom 2
 states that, with
probability 1, the outcome will be a point in the sample space 
Axiom 3
 states
that, for any sequence of mutually exclusive events, the probability of at least one of
these events occurring is just the sum of their respective probabilities.
If we consider a sequence of events 
, where 
and 
for
 then, because the events are mutually exclusive and because 
we have, from Axiom 3
,
implying that
That is, the null event has probability 0 of occurring.
Note that it follows that, for any finite sequence of mutually exclusive events 
This equation follows from Axiom 3
 by defining 
 as the null event for all values
of  greater than 
Axiom 3
 is equivalent to Equation (3.1)
 when the sample
space is finite. (Why?) However, the added generality of Axiom 3
 is necessary
when the sample space consists of an infinite number of points.
Example 3a
If our experiment consists of tossing a coin and if we assume that a head is as
𝐸ଵ, 𝐸ଶ, ...
𝐸௜𝐸௝ൌØ
𝑖്𝑗
𝑃ቆ
௜ൌଵ
ஶ
𝐸௜ቇൌ
௜ൌଵ
ஶ
𝑃ሺ𝐸௜ሻ
𝑃ሺ𝐸ሻ
𝐸.
𝐸
𝑆.
𝐸ଵ,  𝐸ଶ,   .   .   .  ,
𝐸ଵൌ𝑆
𝐸ଵൌ∅
𝑖൐1,
𝑆ൌ
∪
௜ൌଵ
ஶ
𝐸௜,
𝑃ሺ𝑆ሻൌ෍
௜ൌଵ
ஶ
𝑃ሺ𝐸௜ሻൌ𝑃ሺ𝑆ሻ൅෍
௜ൌଶ
ஶ
𝑃ሺØሻ
𝑃ሺØሻൌ0
𝐸ଵ,
𝐸ଶ, . . . , 𝐸௡,
𝑃൬∪
ଵ
௡
𝐸௜൰ൌ
Σ
௜ൌଵ
௡
𝑃ሺ𝐸௜ሻ
(3.1)
𝐸௜
𝑖
𝑛.
55 of 848

likely to appear as a tail, then we would have
On the other hand, if the coin were biased and we believed that a head were
twice as likely to appear as a tail, then we would have
Example 3b
If a die is rolled and we suppose that all six sides are equally likely to appear,
then we would have 
From Axiom 3
, it would thus follow that the probability of rolling an even
number would equal
The assumption of the existence of a set function 
 defined on the events of a
sample space  and satisfying Axioms 1
, 2
, and 3
, constitutes the modern
mathematical approach to probability theory. It is hoped that the reader will agree
that the axioms are natural and in accordance with our intuitive concept of probability
as related to chance and randomness. Furthermore, using these axioms, we shall be
able to prove that if an experiment is repeated over and over again, then, with
probability 1, the proportion of time during which any specific event E occurs will
equal 
 This result, known as the strong law of large numbers, is presented in
Chapter 8
. In addition, we present another possible interpretation of probability–
as being a measure of belief–in Section 2.7
.
Technical Remark. We have supposed that 
 is defined for all the events  of
the sample space. Actually, when the sample space is an uncountably infinite set,
 is defined only for a class of events called measurable. However, this restriction
need not concern us, as all events of any practical interest are measurable.
In this section, we prove some simple propositions regarding probabilities. We first
note that since  and 
 are always mutually exclusive and since 
 we
have, by Axioms 2
 and 3
,
𝑃ሺሼ𝐻ሽሻൌ𝑃ሺሼ𝑇ሽሻൌ1
2
𝑃ሺሼ𝐻ሽሻൌ2
3  𝑃ሺሼ𝑇ሽሻൌ1
3
𝑃ሺሼ1ሽሻൌ𝑃ሺሼ2ሽሻൌ𝑃ሺሼ3ሽሻൌ𝑃ሺሼ4ሽሻൌ𝑃ሺሼ5ሽሻൌ𝑃ሺሼ6ሽሻൌ1
6.
𝑃ሺሼ2,4,6ሽሻൌ𝑃ሺሼ2ሽሻ൅𝑃ሺሼ4ሽሻ൅𝑃ሺሼ6ሽሻൌ1
2
𝑃,
𝑆
𝑃ሺ𝐸ሻ.
𝑃ሺ𝐸ሻ
𝐸
𝑃ሺ𝐸ሻ
𝐸
𝐸௖
𝐸∪𝐸௖ൌ𝑆,
56 of 848

Or, equivalently, we have Proposition 4.1
.
Proposition 4.1
In words, Proposition 4.1
 states that the probability that an event does not
occur is 1 minus the probability that it does occur. For instance, if the probability
of obtaining a head on the toss of a coin is 
 then the probability of obtaining a
tail must be 
Our second proposition states that if the event  is contained in the event 
 then
the probability of  is no greater than the probability of 
Proposition 4.2
If 
 then 
Proof. Since 
 it follows that we can express  as
Hence, because  and 
 are mutually exclusive, we obtain, from Axiom 3
,
which proves the result, since 
Proposition 4.2
 tells us, for instance, that the probability of rolling a 1 with a die is
less than or equal to the probability of rolling an odd value with the die.
The next proposition gives the relationship between the probability of the union of
two events, expressed in terms of the individual probabilities, and the probability of
the intersection of the events.
Proposition 4.3
Proof To derive a formula for 
 we first note that 
 can be written as
the union of the two disjoint events  and 
 Thus, from Axiom 3
, we obtain
1 ൌ𝑃ሺ𝑆ሻൌ𝑃ሺ𝐸∪𝐸௖ሻൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐸௖ሻ
𝑃ሺ𝐸௖ሻൌ1 െ𝑃ሺ𝐸ሻ
3
8 ,
5
8 .
𝐸
𝐹,
𝐸
𝐹.
𝐸⊂𝐹,
𝑃ሺ𝐸ሻ൑𝑃ሺ𝐹ሻ.
𝐸⊂𝐹,
𝐹
𝐹ൌ𝐸∪𝐸௖𝐹
𝐸
𝐸௖𝐹
𝑃ሺ𝐹ሻൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐸௖𝐹ሻ
𝑃ሺ𝐸௖𝐹ሻ൒0.
𝑃ሺ𝐸∪𝐹ሻൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻെ𝑃ሺ𝐸𝐹ሻ
𝑃ሺ𝐸∪𝐹ሻ,
𝐸∪𝐹
𝐸
𝐸௖𝐹.
57 of 848

Furthermore, since 
 we again obtain from Axiom 3
or, equivalently,
thereby completing the proof.
Proposition 4.3
 could also have been proved by making use of the Venn diagram
in Figure 2.4
.
Figure 2.4 Venn diagram.
Let us divide 
 into three mutually exclusive sections, as shown in Figure 2.5
.
In words, section I represents all the points in  that are not in  (that is, 
),
section II represents all points both in  and in  (that is, EF), and section III
represents all points in  that are not in  (that is, 
).
Figure 2.5 Venn diagram in sections.
From Figure 2.5
, we see that
𝑃ሺ𝐸∪𝐹ሻ
ൌ𝑃ሺ𝐸∪𝐸௖𝐹ሻ
ൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐸௖𝐹ሻ
𝐹ൌ𝐸𝐹∪𝐸௖𝐹,
𝑃ሺ𝐹ሻൌ𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐸௖𝐹ሻ
𝑃ሺ𝐸௖𝐹ሻൌ𝑃ሺ𝐹ሻെ𝑃ሺ𝐸𝐹ሻ
𝐸∪𝐹
𝐸
𝐹
𝐸𝐹௖
𝐸
𝐹
𝐹
𝐸
𝐸௖𝐹
58 of 848

As I, II, and III are mutually exclusive, it follows from Axiom 3
 that
which shows that
and Proposition 4.3
 is proved, since II = EF.
Example 4a
J is taking two books along on her holiday vacation. With probability .5, she will
like the first book; with probability .4, she will like the second book; and with
probability .3, she will like both books. What is the probability that she likes
neither book?
Solution
Let 
 denote the event that J likes book 
 Then the probability that she
likes at least one of the books is
Because the event that J likes neither book is the complement of the event that
she likes at least one of them, we obtain the result
We may also calculate the probability that any one of the three events 
 and 
occurs, namely,
which, by Proposition 4.3
, equals
Now, it follows from the distributive law that the events 
 and 
 are
𝐸∪𝐹
ൌI ∪II ∪III
𝐸
ൌI ∪II
𝐹
ൌII ∪III
𝑃ሺ𝐸∪𝐹ሻ
ൌ𝑃ሺIሻ൅𝑃ሺIIሻ൅𝑃ሺIIIሻ
𝑃ሺ𝐸ሻ
ൌ𝑃ሺIሻ൅𝑃ሺIIሻ
𝑃ሺ𝐹ሻ
ൌ𝑃ሺIIሻ൅𝑃ሺIIIሻ
𝑃ሺ𝐸∪𝐹ሻൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻെ𝑃ሺIIሻ
𝐵௜
𝑖, 𝑖ൌ1, 2.
𝑃ሺ𝐵ଵ∪𝐵ଶሻൌ𝑃ሺ𝐵ଵሻ൅𝑃ሺ𝐵ଶሻെ𝑃ሺ𝐵ଵ𝐵ଶሻൌ.5 ൅.4 െ.3 ൌ.6
𝑃ሺ𝐵ଵ
௖𝐵ଶ
௖ሻൌ𝑃൫ሺ𝐵ଵ∪𝐵ଶሻ௖൯ൌ1 െ𝑃ሺ𝐵ଵ∪𝐵ଶሻൌ.4
𝐸, 𝐹,
𝐺
𝑃ሺ𝐸∪𝐹∪𝐺ሻൌ𝑃ሾሺ𝐸∪𝐹ሻ∪𝐺ሿ
𝑃ሺ𝐸∪𝐹ሻ൅𝑃ሺ𝐺ሻെ𝑃ሾሺ𝐸∪𝐹ሻ𝐺ሿ
ሺ𝐸∪𝐹ሻ𝐺
𝐸𝐺∪𝐹𝐺
59 of 848

equivalent; hence, from the preceding equations, we obtain
In fact, the following proposition, known as the inclusion–exclusion identity, can be
proved by mathematical induction:
Proposition 4.4
The summation 
 is taken over all of the 
 possible
subsets of size  of the set 
In words, Proposition 4.4
 states that the probability of the union of  events
equals the sum of the probabilities of these events taken one at a time, minus the
sum of the probabilities of these events taken two at a time, plus the sum of the
probabilities of these events taken three at a time, and so on.
Remarks 1. For a noninductive argument for Proposition 4.4
, note first that if
an outcome of the sample space is not a member of any of the sets 
 then its
probability does not contribute anything to either side of the equality. Now,
suppose that an outcome is in exactly 
 of the events 
 where 
 Then,
since it is in 
 its probability is counted once in 
 also, as this
outcome is contained in 
 subsets of the type 
 its probability is
counted
times on the right of the equality sign in Proposition 4.4
. Thus, for 
 we
𝑃ሺ𝐸∪𝐹∪𝐺ሻ
       ൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻെ𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐺ሻെ𝑃ሺ𝐸𝐺∪𝐹𝐺ሻ
       ൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻെ𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐺ሻെ𝑃ሺ𝐸𝐺ሻെ𝑃ሺ𝐹𝐺ሻ൅𝑃ሺ𝐸𝐺𝐹𝐺ሻ
       ൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻ൅𝑃ሺ𝐺ሻ൅𝑃ሺ𝐸𝐹ሻെ𝑃ሺ𝐸𝐺ሻെ𝑃ሺ𝐹𝐺ሻ൅𝑃ሺ𝐸𝐹𝐺ሻ
𝑃ሺ𝐸ଵ∪𝐸ଶ∪⋯∪𝐸௡ሻൌ
෍
௜ൌଵ
௡
𝑃ሺ𝐸௜ሻെ
෍
௜భழ௜మ
𝑃ሺ𝐸௜భ𝐸௜మሻ൅⋯
൅ሺെ1ሻ௥൅ଵ
෍
௜భழ௜మழ⋯ழ௜ೝ
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜ೝሻ
൅⋯൅ሺെ1ሻ௡൅ଵ𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡ሻ
෍
௜భழ௜మழ⋯ழ௜ೝ
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜ೝሻ
ቆ𝑛
𝑟ቇ
𝑟
ሼ1,  2 … , 𝑛ሽ.
𝑛
𝐸௜,
𝑚
𝐸௜,
𝑚൐0.
∪
௜𝐸௜,
𝑃൬∪
௜𝐸௜൰ ;
ቆ𝑚
𝑘ቇ
𝐸௜భ𝐸௜మ⋯𝐸௜ೖ,
ቆ𝑚
1ቇെቆ𝑚
2ቇ൅ቆ𝑚
3ቇെ⋯േቆ𝑚
𝑚ቇ
𝑚൐0,
60 of 848

must show that
However, since 
 the preceding equation is equivalent to
and the latter equation follows from the binomial theorem, since
2. The following is a succinct way of writing the inclusion–exclusion identity:
3. In the inclusion–exclusion identity, going out one term results in an upper
bound on the probability of the union, going out two terms results in a
lower bound on the probability, going out three terms results in an upper
bound on the probability, going out four terms results in a lower bound,
and so on. That is, for events 
 we have
and so on. To prove the validity of these bounds, note the identity
1 ൌቆ𝑚
1 ቇെቆ𝑚
2 ቇ൅ቆ𝑚
3 ቇെ ⋯  േቆ𝑚
𝑚ቇ
1 ൌቆ𝑚
0ቇ,
෍
௜ൌ଴
௠
ቆ𝑚
𝑖ቇሺെ1ሻ௜ൌ0
0 ൌሺെ1 ൅1ሻ௠ൌ෍
௜ൌ଴
௠
ቆ𝑚
𝑖ቇሺെ1ሻ௜ሺ1ሻ௠െ௜
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻൌ
∑
௥ൌଵ
௡
ሺെ1ሻ௥൅ଵ
∑
௜భழ⋯ழ௜ೝ
𝑃ሺ𝐸௜భ⋯𝐸௜ೝሻ
𝐸ଵ,...,𝐸௡,
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻ൑
∑
௜ൌଵ
௡
𝑃ሺ𝐸௜ሻ
(4.1)
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻ൒
∑
௜ൌଵ
௡
𝑃ሺ𝐸௜ሻെ
∑
௝ழ௜
𝑃ሺ𝐸௜𝐸௝ሻ
(4.2)
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻ൑
∑
௜ൌଵ
௡
𝑃ሺ𝐸௜ሻെ
∑
௝ழ௜
𝑃ሺ𝐸௜𝐸௝ሻ൅
∑
௞ழ௝ழ௜
𝑃ሺ𝐸௜𝐸௝𝐸௞ሻ
(4.3)
∪௜ൌଵ
௡
𝐸௜ൌ𝐸ଵ∪𝐸ଵ
௖𝐸ଶ∪𝐸ଵ
௖𝐸ଶ
௖𝐸ଷ∪⋯∪𝐸ଵ
௖⋯𝐸௡െଵ
௖
𝐸௡
61 of 848

That is, at least one of the events 
 occurs if 
 occurs, or if 
 does not occur
but 
 does, or if 
 and 
 do not occur but 
 does, and so on. Because the
right-hand side is the union of disjoint events, we obtain
Now, let 
be the event that none of the first 
events occurs. Applying the identity
shows that
or, equivalently,
Substituting this equation into (4.4)
 yields
Because probabilities are always nonnegative, Inequality (4.1) follows directly
from Equation (4.5)
. Now, fixing  and applying Inequality (1) to 
yields
which, by Equation (4.5)
, gives Inequality (4.2). Similarly, fixing i and applying
Inequality (4.2) to 
 yields
𝐸௜
𝐸ଵ
𝐸ଵ
𝐸ଶ
𝐸ଵ
𝐸ଶ
𝐸ଷ
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻ
ൌ𝑃ሺ𝐸ଵሻ൅𝑃ሺ𝐸ଵ
௖𝐸ଶሻ൅𝑃ሺ𝐸ଵ
௖𝐸ଶ
௖𝐸ଷሻ൅... ൅𝑃ሺ𝐸ଵ
௖⋯𝐸௡െଵ
௖
𝐸௡ሻ
ൌ𝑃ሺ𝐸௜ሻ൅
௜ൌଶ
௡
𝑃ሺ𝐸ଵ
௖⋯𝐸௜െଵ
௖
𝐸௜ሻ
(4.4)
𝐵௜ൌ𝐸ଵ
௖⋯𝐸௜െଵ
௖
ൌሺ∪௝ழ௜𝐸௝ሻ௖
𝑖െ1
𝑃ሺ𝐸௜ሻൌ𝑃ሺ𝐵௜𝐸௜ሻ൅𝑃ሺ𝐵௜
௖𝐸௜ሻ
𝑃ሺ𝐸௜ሻൌ𝑃ሺ𝐸ଵ
௖…𝐸௜െଵ
௖
𝐸௜ሻൌ𝑃ሺ𝐸௜ሻ൅𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ
𝑃ሺ𝐸ଵ
௖⋯𝐸௜െଵ
௖
𝐸௜ሻൌ𝑃ሺ𝐸௜ሻെ𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻൌ෍
௜
𝑃ሺ𝐸௜ሻെ෍
௜
𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ
(4.5)
𝑖
𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ
𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ൑෍
௝ழ௜
𝑃ሺ𝐸௜𝐸௝ሻ
𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ
𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ
൒෍
௝ழ௜
𝑃ሺ𝐸௜𝐸௝ሻെ
෍
௞ழ௝ழ௜
𝑃ሺ𝐸௜𝐸௝𝐸௜𝐸௞ሻ
ൌ෍
௝ழ௜
𝑃ሺ𝐸௜𝐸௝ሻെ
෍
௞ழ௝ழ௜
𝑃ሺ𝐸௜𝐸௝𝐸௞ሻ
62 of 848

which, by Equation (4.5)
, gives Inequality (4.3). The next inclusion–exclusion
inequality is now obtained by fixing  and applying Inequality (4.3) to
 and so on.
The first inclusion-exclusion inequality, namely that
is known as Boole’s inequality.
In many experiments, it is natural to assume that all outcomes in the sample space
are equally likely to occur. That is, consider an experiment whose sample space  is
a finite set, say, 
 Then, it is often natural to assume that
which implies, from Axioms 2
 and 3
 (why?), that
From this equation, it follows from Axiom 3
 that, for any event 
In words, if we assume that all outcomes of an experiment are equally likely to occur,
then the probability of any event  equals the proportion of outcomes in the sample
space that are contained in 
Example 5a
If two dice are rolled, what is the probability that the sum of the upturned faces
will equal 7?
Solution
We shall solve this problem under the assumption that all of the 36 possible
𝑖
 𝑃ሺ∪௝ழ௜𝐸௜𝐸௝ሻ,
𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻ൑෍
௜ൌଵ
௡
𝑃ሺ𝐸௜ሻ
𝑆
𝑆ൌሼ1, 2, ... , 𝑁ሽ.
𝑃ሺሼ1ሽሻൌ𝑃ሺሼ2ሽሻൌ⋯ൌ𝑃ሺሼ𝑁ሽሻ
𝑃ሺሼ𝑖ሽሻൌ1
𝑁 𝑖ൌ1, 2, ... , 𝑁
𝐸,
𝑃ሺ𝐸ሻൌnumber of outcomes in 𝐸
number of outcomes in 𝑆
𝐸
𝐸.
63 of 848

outcomes are equally likely. Since there are 6 possible outcomes–namely, (1, 6),
(2, 5), (3, 4), (4, 3), (5, 2), and (6, 1)–that result in the sum of the dice being
equal to 7, the desired probability is 
Example 5b
If 3 balls are “randomly drawn” from a bowl containing 6 white and 5 black balls,
what is the probability that one of the balls is white and the other two black?
Solution
If we regard the balls as being distinguishable and the order in which they are
selected as being relevant, then the sample space consists of 
outcomes. Furthermore, there are 
 outcomes in which the first ball
selected is white and the other two are black; 
 outcomes in which
the first is black, the second is white, and the third is black; and 
 in
which the first two are black and the third is white. Hence, assuming that
“randomly drawn” means that each outcome in the sample space is equally likely
to occur, we see that the desired probability is
This problem could also have been solved by regarding the outcome of the
experiment as the unordered set of drawn balls. From this point of view, there are
 outcomes in the sample space. Now, each set of 3 balls corresponds
to 3! outcomes when the order of selection is noted. As a result, if all outcomes
are assumed equally likely when the order of selection is noted, then it follows
that they remain equally likely when the outcome is taken to be the unordered set
of selected balls. Hence, using the latter representation of the experiment, we
see that the desired probability is
which, of course, agrees with the answer obtained previously.
When the experiment consists of a random selection of  items from a set of  items,
we have the flexibility of either letting the outcome of the experiment be the ordered
selection of the  items or letting it be the unordered set of items selected. In the
6
36 ൌ1
6.
11 ⋅10 ⋅9 ൌ990
6 ⋅5 ⋅4 ൌ120
5 ⋅6 ⋅4 ൌ120
5 ⋅4 ⋅6 ൌ120
120 ൅120 ൅120
990
ൌ4
11
ቆ11
3 ቇൌ165
ቆ6
1ቇ ቆ5
2ቇ
ቆ11
3 ቇ
ൌ4
11
𝑘
𝑛
𝑘
64 of 848

former case, we would assume that each new selection is equally likely to be any of
the so far unselected items of the set, and in the latter case, we would assume that
all 
 possible subsets of  items are equally likely to be the set selected. For
instance, suppose  people are to be randomly selected from a group of 20
individuals consisting of 
 married couples, and we want to determine 
 the
probability that the 5 chosen are all unrelated. (That is, no two are married to each
other.) If we regard the sample space as the set of  people chosen, then there are
 equally likely outcomes. An outcome that does not contain a married couple
can be thought of as being the result of a six-stage experiment: In the first stage,  of
the 
 couples to have a member in the group are chosen; in the next  stages, 1 of
the 2 members of each of these couples is selected. Thus, there are 
 possible
outcomes in which the  members selected are unrelated, yielding the desired
probability of
In contrast, we could let the outcome of the experiment be the ordered selection of
the  individuals. In this setting, there are 
 equally likely
outcomes, of which 
 outcomes result in a group of  unrelated
individuals, yielding the result
We leave it for the reader to verify that the two answers are identical.
Example 5c
A committee of 5 is to be selected from a group of 6 men and 9 women. If the
selection is made randomly, what is the probability that the committee consists of
3 men and 2 women?
Solution
Because each of the 
possible committees is equally likely to be selected,
the desired probability is
ቆ𝑛
𝑘ቇ
𝑘
5
10
𝑃ሺ𝑁ሻ,
5
ቆ20
5 ቇ
5
10
5
ቆ10
5 ቇ2ହ
5
𝑃ሺ𝑁ሻൌ
ቆ10
5 ቇ2ହ
ቆ20
5 ቇ
5
20 ⋅19 ⋅18 ⋅17 ⋅16
20 ⋅18 ⋅16 ⋅14 ⋅12
5
𝑃ሺ𝑁ሻൌ20 ⋅18 ⋅16 ⋅14 ⋅12
20 ⋅19 ⋅18 ⋅17 ⋅16
ቆ15
5 ቇ
65 of 848

Example 5d
An urn contains  balls, one of which is special. If  of these balls are withdrawn
one at a time, with each selection being equally likely to be any of the balls that
remain at the time, what is the probability that the special ball is chosen?
Solution
Since all of the balls are treated in an identical manner, it follows that the set of 
balls selected is equally likely to be any of the 
 sets of  balls. Therefore,
We could also have obtained this result by letting 
 denote the event that the
special ball is the th ball to be chosen, 
 Then, since each one of the 
balls is equally likely to be the th ball chosen, it follows that 
 Hence,
because these events are clearly mutually exclusive, we have
We could also have argued that 
by noting that there are
! equally likely outcomes of the experiment, of
which 
!
result in the special ball being the th one chosen. From this reasoning, it follows
that
Example 5e
Suppose that 
 balls, of which  are red and 
 are blue, are arranged in a
linear order in such a way that all 
 possible orderings are equally likely. If
we record the result of this experiment by listing only the colors of the successive
ቆ6
3ቇቆ9
2ቇ
ቆ15
5 ቇ
ൌ240
1001
𝑛
𝑘
𝑘
ቆ𝑛
𝑘ቇ
𝑘
𝑃ሼspecial ball is selectedሽൌ
ቆ1
1ቇቆ𝑛െ1
𝑘െ1ቇ
ቆ𝑛
𝑘ቇ
ൌ𝑘
𝑛
𝐴௜
𝑖
𝑖ൌ1, ... , 𝑘.
𝑛
𝑖
𝑃ሺ𝐴௜ሻൌ1/𝑛.
𝑃ሼspecial ball is selectedሽൌ𝑃ቆ
∪
௜ൌଵ
௞
𝐴௜ቇൌ෍
௜ൌଵ
௞
𝑃ሺ𝐴௜ሻൌ𝑘
𝑛
𝑃ሺ𝐴௜ሻൌ1/𝑛,
𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑘൅1ሻൌ𝑛!/ሺ𝑛െ𝑘ሻ
ሺ𝑛െ1ሻሺ𝑛െ2ሻ⋯ሺ𝑛െ𝑖൅1ሻሺ1ሻሺ𝑛െ𝑖ሻ⋯ሺ𝑛െ𝑘൅1ሻൌሺ𝑛െ1ሻ!/ሺ𝑛െ𝑘ሻ
𝑖
𝑃ሺ𝐴௜ሻൌሺ𝑛െ1ሻ!
𝑛!
ൌ1
𝑛
𝑛൅𝑚
𝑛
𝑚
ሺ𝑛൅𝑚ሻ!
66 of 848

balls, show that all the possible results remain equally likely.
Solution
Consider any one of the 
 possible orderings, and note that any
permutation of the red balls among themselves and of the blue balls among
themselves does not change the sequence of colors. As a result, every ordering
of colorings corresponds to ! 
! different orderings of the 
 balls, so every
ordering of the colors has probability 
of occurring.
For example, suppose that there are 2 red balls, numbered 
 and 2 blue
balls, numbered 
 Then, of the 4! possible orderings, there will be 2! 2!
orderings that result in any specified color combination. For instance, the
following orderings result in the successive balls alternating in color, with a red
ball first:
Therefore, each of the possible orderings of the colors has probability 
 of
occurring.
Example 5f
A poker hand consists of 5 cards. If the cards have distinct consecutive values
and are not all of the same suit, we say that the hand is a straight. For instance,
a hand consisting of the five of spades, six of spades, seven of spades, eight of
spades, and nine of hearts is a straight. What is the probability that one is dealt a
straight?
Solution
We start by assuming that all 
 possible poker hands are equally likely. To
determine the number of outcomes that are straights, let us first determine the
number of possible outcomes for which the poker hand consists of an ace, two,
three, four, and five (the suits being irrelevant). Since the ace can be any 1 of the
4 possible aces, and similarly for the two, three, four, and five, it follows that there
are 
 outcomes leading to exactly one ace, two, three, four, and five. Hence,
since in 4 of these outcomes all the cards will be of the same suit (such a hand is
called a straight flush), it follows that there are 
 hands that make up a
straight of the form ace, two, three, four, and five. Similarly, there are 
hands that make up a straight of the form ten, jack, queen, king, and ace. Thus,
there are 
 hands that are straights, and it follows that the desired
ሺ𝑛൅𝑚ሻ!
𝑛𝑚
𝑛൅𝑚
𝑛!𝑚!
ሺ𝑛൅𝑚ሻ!
𝑟ଵ,𝑟ଶ,
𝑏ଵ,𝑏ଶ.
𝑟ଵ,  𝑏ଵ,  𝑟ଶ,  𝑏ଶ
𝑟ଵ,  𝑏ଶ,  𝑟ଶ,  𝑏ଵ
𝑟ଶ,  𝑏ଵ,  𝑟ଵ,  𝑏ଶ
𝑟ଶ,  𝑏ଶ,  𝑟ଵ,  𝑏ଵ
4
24 ൌ1
6
ቆ52
5 ቇ
4ହ
4ହെ4
4ହെ4
10ሺ4ହെ4ሻ
67 of 848

probability is
Example 5g
A 5-card poker hand is said to be a full house if it consists of 3 cards of the same
denomination and 2 other cards of the same denomination (of course, different
from the first denomination). Thus, a full house is three of a kind plus a pair. What
is the probability that one is dealt a full house?
Solution
Again, we assume that all 
 possible hands are equally likely. To determine
the number of possible full houses, we first note that there are 
 different
combinations of, say, 2 tens and 3 jacks. Because there are 13 different choices
for the kind of pair and, after a pair has been chosen, there are 12 other choices
for the denomination of the remaining 3 cards, it follows that the probability of a
full house is
Example 5h
In the game of bridge, the entire deck of 52 cards is dealt out to 4 players. What
is the probability that
a. one of the players receives all 13 spades;
b. each player receives 1 ace?
Solution
a. Letting 
 be the event that hand  has all 
 spades, then
Because the events 
 are mutually exclusive, the probability
10ሺ4ହെ4ሻ
ቆ52
5 ቇ
ൎ.0039
ቆ52
5 ቇ
ቆ4
2ቇቆ4
3ቇ
13 ⋅12 ⋅ቆ4
2ቇቆ4
3ቇ
ቆ52
5 ቇ
ൎ.0014
𝐸௜
𝑖
13
𝑃ሺ𝐸௜ሻൌ
1
ቆ52
13ቇ
,  𝑖ൌ1, 2, 3, 4
𝐸௜, 𝑖ൌ1, 2, 3, 4,
68 of 848

that one of the hands is dealt all 13 spades is
b. Let the outcome of the experiment be the sets of 13 cards of each of the
players 1, 2, 3, 4. To determine the number of outcomes in which each of
the distinct players receives exactly 1 ace, put aside the aces and note
that there are 
 possible divisions of the other 48 cards
when each player is to receive 12. Because there are 4! ways of dividing
the 4 aces so that each player receives 1, we see that the number of
possible outcomes in which each player receives exactly 1 ace is
As there are 
possible hands, the desired probability is thus
Some results in probability are quite surprising when initially encountered. Our next
two examples illustrate this phenomenon.
Example 5i
If  people are present in a room, what is the probability that no two of them
celebrate their birthday on the same day of the year? How large need  be so
that this probability is less than 
Solution
As each person can celebrate his or her birthday on any one of 365 days, there
are a total of 
 possible outcomes. (We are ignoring the possibility of
someone having been born on February 29.) Assuming that each outcome is
equally likely, we see that the desired probability is
 It is a rather surprising fact that when
 this probability is less than 
 That is, if there are 23 or more people in a
room, then the probability that at least two of them have the same birthday
𝑃ሺ∪௜ൌଵ
ସ
𝐸௜ሻൌ
∑
௜ൌଵ
ସ
𝑃ሺ𝐸௜ሻൌ4/ቆ52
13ቇൎ6.3 ൈ10െଵଶ
ቆ
48
12, 12, 12, 12ቇ
4!ቆ
48
12, 12, 12, 12ቇ.
ቆ
52
13,  13, 13, 13ቇ
4!ቆ
48
12,12,12,12ቇ
ቆ
52
13,13,13,13ቇ
ൎ.1055
𝑛
𝑛
1
2 ?
ሺ365ሻ௡
ሺ365ሻሺ364ሻሺ363ሻ... ሺ365 െ𝑛൅1ሻ/ሺ365ሻ௡.
𝑛൒23,
1
2 .
69 of 848

exceeds 
 Many people are initially surprised by this result, since 23 seems so
small in relation to 365, the number of days of the year. However, every pair of
individuals has probability 
of having the same birthday, and in a
group of 23 people, there are 
 different pairs of individuals. Looked at
this way, the result no longer seems so surprising.
When there are 50 people in the room, the probability that at least two share the
same birthday is approximately .970, and with 100 persons in the room, the odds
are better than 3,000,000:1. (That is, the probability is greater than 
that at least two people have the same birthday.)
Example 5j
A deck of 52 playing cards is shuffled, and the cards are turned up one at a time
until the first ace appears. Is the next card–that is, the card following the first
ace–more likely to be the ace of spades or the two of clubs?
Solution
To determine the probability that the card following the first ace is the ace of
spades, we need to calculate how many of the (52)! possible orderings of the
cards have the ace of spades immediately following the first ace. To begin, note
that each ordering of the 52 cards can be obtained by first ordering the 51 cards
different from the ace of spades and then inserting the ace of spades into that
ordering. Furthermore, for each of the (51)! orderings of the other cards, there is
only one place where the ace of spades can be placed so that it follows the first
ace. For instance, if the ordering of the other 51 cards is
then the only insertion of the ace of spades into this ordering that results in its
following the first ace is
Therefore, there are (51)! orderings that result in the ace of spades following the
first ace, so
1
2 .
365
ሺ365ሻଶൌ
1
365
ቆ23
2 ቇൌ253
3 ൈ10଺
3 ൈ10଺൅1
4𝑐, 6ℎ, 𝐽𝑑, 5𝑠, 𝐴𝑐, 7𝑑, ... , 𝐾ℎ
4𝑐, 6ℎ, 𝐽𝑑, 5𝑠, 𝐴𝑐, 𝐴𝑠, 7𝑑, ... , 𝐾ℎ
𝑃ሼthe ace of spades follows the ϐirst aceሽൌሺ51ሻ!
ሺ52ሻ! ൌ1
52
70 of 848

In fact, by exactly the same argument, it follows that the probability that the two
of clubs (or any other specified card) follows the first ace is also 
 In other
words, each of the 52 cards of the deck is equally likely to be the one that follows
the first ace!
Many people find this result rather surprising. Indeed, a common reaction is to
suppose initially that it is more likely that the two of clubs (rather than the ace of
spades) follows the first ace, since that first ace might itself be the ace of spades.
This reaction is often followed by the realization that the two of clubs might itself
appear before the first ace, thus negating its chance of immediately following the
first ace. However, as there is one chance in four that the ace of spades will be
the first ace (because all 4 aces are equally likely to be first) and only one chance
in five that the two of clubs will appear before the first ace (because each of the
set of 5 cards consisting of the two of clubs and the 4 aces is equally likely to be
the first of this set to appear), it again appears that the two of clubs is more likely.
However, this is not the case, and our more complete analysis shows that they
are equally likely.
Example 5k
A football team consists of 20 offensive and 20 defensive players. The players
are to be paired in groups of 2 for the purpose of determining roommates. If the
pairing is done at random, what is the probability that there are no offensive–
defensive roommate pairs? What is the probability that there are 2  offensive–
defensive roommate pairs, 
Solution
There are
ways of dividing the 40 players into 20 ordered pairs of two each. (That is, there
are 
ways of dividing the players into a first pair, a second pair, and so
on.) Hence, there are 
 ways of dividing the players into
(unordered) pairs of 2 each. Furthermore, since a division will result in no
offensive–defensive pairs if the offensive (and defensive) players are paired
among themselves, it follows that there are 
 such divisions.
Hence, the probability of no offensive–defensive roommate pairs, call it 
 is
given by
1
52 .
𝑖
𝑖ൌ1, 2, ... , 10?
ቆ
40
2, 2, ... , 2ቇൌሺ40ሻ!
ሺ2!ሻଶ଴
ሺ40ሻ!/2ଶ଴
ሺ40ሻ!/2ଶ଴ሺ20ሻ!
ሾሺ20ሻ!/2ଵ଴ሺ10ሻ!ሿ
ଶ
𝑃଴,
71 of 848

To determine 
 the probability that there are 2  offensive–defensive pairs, we
first note that there are 
 ways of selecting the 2  offensive players and the
2  defensive players who are to be in the offensive–defensive pairs. These 4
players can then be paired up into (2 )! possible offensive–defensive pairs. (This
is so because the first offensive player can be paired with any of the 2  defensive
players, the second offensive player with any of the remaining 
 defensive
players, and so on.) As the remaining 
 offensive (and defensive) players
must be paired among themselves, it follows that there are
divisions that lead to 2  offensive–defensive pairs. Hence,
The 
 can now be computed, or they can be approximated by
making use of a result of Stirling, which shows that ! can be approximated by
 For instance, we obtain
Our next three examples illustrate the usefulness of the inclusion–exclusion identity
(Proposition 4.4)
. In Example 51
, the introduction of probability enables us to
obtain a quick solution to a counting problem.
Example 5l
A total of 36 members of a club play tennis, 28 play squash, and 18 play
badminton. Furthermore, 22 of the members play both tennis and squash, 12
play both tennis and badminton, 9 play both squash and badminton, and 4 play
all three sports. How many members of this club play at least one of three
𝑃଴ൌ
ቆሺ20ሻ!
2ଵ଴ሺ10ሻ!ቇ
ଶ
ሺ40ሻ!
2ଶ଴ሺ20ሻ!
ൌ
ሾሺ20ሻ!ሿଷ
ሾሺ10ሻ!ሿଶሺ40ሻ!
𝑃ଶ௜,
𝑖
ቆ20
2𝑖ቇ
ଶ
𝑖
𝑖
𝑖
𝑖
𝑖
2𝑖െ1
20 െ2𝑖
ቆ20
2𝑖ቇ
ଶ
ሺ2𝑖ሻ!ቈ
ሺ20 െ2𝑖ሻ!
2ଵ଴െ௜ሺ10 െ𝑖ሻ!
቉
ଶ
𝑖
𝑃ଶ௜ൌ
ቆ20
2𝑖ቇ
ଶ
ሺ2𝑖ሻ!ቈ
ሺ20 െ2𝑖ሻ!
2ଵ଴െ௜ሺ10 െ𝑖ሻ!
቉
ଶ
ሺ40ሻ!
2ଶ଴ሺ20ሻ!
  𝑖ൌ0, 1, ... , 10
𝑃ଶ௜, 𝑖ൌ0, 1, ..., 10,
𝑛
𝑛௡൅ଵ/ଶ𝑒െ௡2𝜋
√
.
𝑃଴ൎ1.3403 ൈ10െ଺
𝑃ଵ଴ൎ.345861
𝑃ଶ଴ൎ7.6068 ൈ10െ଺
72 of 848

sports?
Solution
Let  denote the number of members of the club, and introduce probability by
assuming that a member of the club is randomly selected. If, for any subset  of
members of the club, we let 
 denote the probability that the selected member
is contained in 
 then
Now, with  being the set of members that plays tennis,  being the set that plays
squash, and  being the set that plays badminton, we have, from Proposition
4.4
,
Hence, we can conclude that 43 members play at least one of the sports.
The next example in this section not only possesses the virtue of giving rise to a
somewhat surprising answer, but is also of theoretical interest.
Example 5m The Matching Problem
Suppose that each of  men at a party throws his hat into the center of the room.
The hats are first mixed up, and then each man randomly selects a hat. What is
the probability that none of the men selects his own hat?
Solution
We first calculate the complementary probability of at least one man selecting his
own hat. Let us denote by 
 the event that the th man selects his
own hat. Now, by the inclusion-exclusion identity 
 the probability that
at least one of the men selects his own hat, is given by
𝑁
𝐶
𝑃ሺ𝐶ሻ
𝐶,
𝑃ሺ𝐶ሻൌnumber of members in 𝐶
𝑁
𝑇
𝑆
𝐵
𝑃ሺ𝑇∪𝑆∪𝐵ሻ
ൌ𝑃ሺ𝑇ሻ൅𝑃ሺ𝑆ሻ൅𝑃ሺ𝐵ሻെ𝑃ሺ𝑇𝑆ሻെ𝑃ሺ𝑇𝐵ሻെ𝑃ሺ𝑆𝐵ሻ൅𝑃ሺ𝑇𝑆𝐵ሻ
ൌ36 ൅28 ൅18 െ22 െ12 െ9 ൅4
𝑁
ൌ43
𝑁
𝑁
𝐸௜, 𝑖ൌ1, 2, ... , 𝑁
𝑖
𝑃ቆ
∪
௜ൌଵ
ே
𝐸௜ቇ,
73 of 848

If we regard the outcome of this experiment as a vector of N numbers, where the
th element is the number of the hat drawn by the th man, then there are !
possible outcomes. [The outcome 
 means, for example, that each
man selects his own hat.] Furthermore, 
 the event that each of the n
men 
 selects his own hat, can occur in any of
! possible ways; for, of the remaining 
men, the first can select any of 
 hats, the second can then select any of
 hats, and so on. Hence, assuming that all ! possible outcomes are
equally likely, we see that
Also, as there are 
 terms in 
 it follows that
Thus,
Hence, the probability that none of the men selects his own hat is
Upon letting 
 in the identity 
the preceding probability
when  is large is seen to be approximately equal to 
 In other words,
for  large, the probability that none of the men selects his own hat is
𝑃൭
௜ൌଵ
ே
𝐸௜൱ൌ
෍
௜ൌଵ
ே
𝑃ሺ𝐸௜ሻെ
෍
௜భழ௜మ
𝑃ሺ𝐸௜భ𝐸௜మሻ൅⋯
൅ሺെ1ሻ௡൅ଵ 
෍
௜భழ௜మ⋯ழ௜೙
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻ
൅⋯൅ሺെ1ሻே൅ଵ𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸ேሻ
𝑖
𝑖
𝑁
ሺ1, 2, 3, ... , 𝑁ሻ
𝐸௜భ𝐸௜మ...𝐸௜೙,
𝑖ଵ, 𝑖ଶ, ..., 𝑖௡
ሺ𝑁െ𝑛ሻሺ𝑁െ𝑛െ1ሻ⋯3 ⋅2 ⋅1 ൌሺ𝑁െ𝑛ሻ
𝑁െ𝑛
𝑁െ𝑛
𝑁െ𝑛െ1
𝑁
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻൌሺ𝑁െ𝑛ሻ!
𝑁!
ቆ𝑁
𝑛ቇ
෍
௜భழ௜మ⋯ழ௜೙
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻ,
෍
௜భழ௜మ⋯ழ௜೙
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻൌ
𝑁!
ሺ𝑁െ𝑛ሻ!𝑛!
ሺ𝑁െ𝑛ሻ!
𝑁!
ൌ1
𝑛!
𝑃ቌ
௜
ଵ
ே
𝐸௜ቍൌ1 െ1
2! ൅1
3! െ⋯൅ሺെ1ሻே൅ଵ1
𝑁!
1 െ1 ൅1
2! െ1
3! ൅... ൅ሺെ1ሻே
𝑁!
ൌ෍
௜ൌ଴
ே
ሺെ1ሻ௜/𝑖!
𝑥ൌെ1
𝑒௫ൌ෍
௜ൌ଴
ஶ
𝑥௜/𝑖!,
𝑁
𝑒െଵൎ.3679.
𝑁
74 of 848

approximately .37. (How many readers would have incorrectly thought that this
probability would go to 1 as 
)
For another illustration of the usefulness of the inclusion-exclusion identity, consider
the following example.
Example 5n
Compute the probability that if 10 married couples are seated at random at a
round table, then no wife sits next to her husband.
Solution
If we let 
 denote the event that the th couple sit next to each
other, it follows that the desired probability is 
 Now, from the
inclusion-exclusion identity,
To compute 
 we first note that there are 19! ways of arranging 20
people around a round table. (Why?) The number of arrangements that result in
a specified set of  men sitting next to their wives can most easily be obtained by
first thinking of each of the  married couples as being single entities. If this were
the case, then we would need to arrange 
 entities around a round table,
and there are clearly 
 such arrangements. Finally, since each of the
 married couples can be arranged next to each other in one of two possible
ways, it follows that there are 
 arrangements that result in a
specified set of  men each sitting next to their wives. Therefore,
Thus, from Proposition 4.4
, we obtain that the probability that at least one
married couple sits together is
and the desired probability is approximately .3395.
𝑁→∞?
𝐸௜, 𝑖ൌ1, 2, ... , 10
𝑖
1 െ𝑃ቆ
∪
௜ൌଵ
ଵ଴
𝐸௜ቇ.
𝑃൭
ଵ
ଵ଴
𝐸௜൱ൌ
෍
ଵ
ଵ଴
𝑃ሺ𝐸௜ሻെ⋯൅ሺെ1ሻ௡൅ଵ
෍
௜భழ௜మழ⋯ழ௜೙
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻ
൅⋯െ𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸ଵ଴ሻ
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻ,
𝑛
𝑛
20 െ𝑛
ሺ20 െ𝑛െ1ሻ!
𝑛
2௡ሺ20 െ𝑛െ1ሻ!
𝑛
𝑃ሺ𝐸௜భ𝐸௜మ⋯𝐸௜೙ሻൌ2௡ሺ19 െ𝑛ሻ!
ሺ19ሻ!
ቆ10
1 ቇ2ଵሺ18ሻ!
ሺ19ሻ! െቆ10
2 ቇ2ଶሺ17ሻ!
ሺ19ሻ! ൅ቆ10
3 ቇ2ଷሺ16ሻ!
ሺ19ሻ! െ⋯െቆ10
10ቇ2ଵ଴
9!
ሺ19ሻ! ൎ.6605
75 of 848

*Example 5o Runs
Consider an athletic team that had just finished its season with a final record of 
wins and 
 losses. By examining the sequence of wins and losses, we are
hoping to determine whether the team had stretches of games in which it was
more likely to win than at other times. One way to gain some insight into this
question is to count the number of runs of wins and then see how likely that
result would be when all 
 orderings of the  wins and 
 losses
are assumed equally likely. By a run of wins, we mean a consecutive sequence
of wins. For instance, if 
 and the sequence of outcomes was
WWLLWWWLWLLLWWWW, then there would be 4 runs of wins–the first run
being of size 2, the second of size 3, the third of size 1, and the fourth of size 4.
Suppose now that a team has  wins and 
 losses. Assuming that all 
 orderings are equally likely, let us determine the probability
that there will be exactly  runs of wins. To do so, consider first any vector of
positive integers 
with 
 and let us see how many
outcomes result in r runs of wins in which the th run is of size 
 For
any such outcome, if we let 
 denote the number of losses before the first run of
wins, 
 the number of losses between the first 2 runs of wins, 
 the
number of losses after the last run of wins, then the 
 satisfy
and the outcome can be represented schematically as
Hence, the number of outcomes that result in  runs of wins the th of size
is equal to the number of integers 
 that satisfy the
foregoing, or, equivalently, to the number of positive integers
that satisfy
By Proposition 6.1
 in Chapter 1
, there are 
such outcomes.
Hence, the total number of outcomes that result in  runs of wins is 
𝑛
𝑚
ሺ𝑛൅𝑚ሻ!/ሺ𝑛! 𝑚!ሻ
𝑛
𝑚
𝑛ൌ10, 𝑚ൌ6,
𝑛
𝑚
ሺ𝑛൅𝑚ሻ!/
ሺ𝑛! 𝑚!ሻൌቆ𝑛൅𝑚
𝑛
ቇ
𝑟
𝑥ଵ, 𝑥ଶ, …, 𝑥௥
𝑥ଵ൅⋯൅𝑥௥ൌ𝑛,
𝑖
𝑥௜, 𝑖ൌ1, ... , 𝑟.
𝑦ଵ
𝑦ଶ
... , 𝑦௥൅ଵ
𝑦௜
𝑦ଵ൅𝑦ଶ൅⋯൅𝑦௥൅ଵൌ𝑚  𝑦ଵ൒0,𝑦௥൅ଵ൒0,𝑦௜൐0,𝑖ൌ2, ... , 𝑟
𝐿𝐿... 𝐿
௬భ
 𝑊𝑊... 𝑊
௫భ
 𝐿... 𝐿
௬మ
 𝑊𝑊... 𝑊
௫మ
 ⋯ 𝑊𝑊
௫ೝ
 𝐿... 𝐿
௬ೝ൅భ
𝑟
𝑖
𝑥௜, 𝑖ൌ1,  … 𝑟െ
𝑦ଵ , ... , 𝑦௥൅ଵ
𝑦̅ ̅ଵൌ𝑦ଵ൅1
𝑦̅ ̅ଵൌ𝑦௜,  𝑖ൌ2, …, 𝑟,  𝑦̅ ̅௥൅ଵൌ𝑦௥൅ଵ൅1
𝑦̅ ̅ଵ൅𝑦̅ ̅ଶ൅⋯൅𝑦̅ ̅௥൅ଵൌ𝑚൅2
ቆ𝑚൅1
𝑟
ቇ
𝑟
ቆ𝑚൅1
𝑟
ቇ
76 of 848

multiplied by the number of positive integral solutions of 
 Thus,
again from Proposition 6.1
, there are 
 outcomes resulting in
 runs of wins. As there are 
 equally likely outcomes, it follows that
For example, if 
 and 
 then the probability of 7 runs is
if all
outcomes are equally likely. Hence, if the outcome was WLWLWLWLWWLWLW,
then we might suspect that the team’s probability of winning was changing over
time. (In particular, the probability that the team wins seems to be quite high
when it lost its last game and quite low when it won its last game.) On the other
extreme, if the outcome were WWWWWWWWLLLLLL, then there would have
been only 1 run, and as 
it would thus
again seem unlikely that the team’s probability of winning remained unchanged
over its 14 games.
A sequence of events 
 is said to be an increasing sequence if
whereas it is said to be a decreasing sequence if
If 
 is an increasing sequence of events, then we define a new event,
denoted by 
by
𝑥ଵ൅ ⋯  ൅ 𝑥௥ൌ𝑛.
ቆ𝑚൅1
𝑟
ቇቆ𝑛െ1
𝑟െ1ቇ
𝑟
ቆ𝑛൅𝑚
𝑛
ቇ
𝑃ሺሼ𝑟 runs of winsሽሻൌ
ቆ𝑚൅1
𝑟
ቇቆ𝑛െ1
𝑟െ1ቇ
ቆ𝑚൅𝑛
𝑛
ቇ
 𝑟൒1
𝑛ൌ8
𝑚ൌ6,
ቆ7
7ቇ ቆ7
6ቇቆ14
8 ቇൌ1/429
ቆ14
8 ቇ
𝑃ሺሼ1 runሽሻൌ ቆ7
1ቇቆ7
0ቇቆ14
8 ቇൌ1/429,
*
ሼ𝐸௡,𝑛൒1ሽ
𝐸ଵ⊂𝐸ଶ⊂⋯⊂𝐸௡⊂𝐸௡൅ଵ⊂⋯
𝐸ଵ⊃𝐸ଶ⊃⋯⊃𝐸௡⊃𝐸௡൅ଵ⊃⋯
ሼ𝐸௡,𝑛൒1ሽ
lim
௡→ஶ  𝐸௡,
77 of 848

Similarly, if 
 is a decreasing sequence of events, we define 
 by
We now prove the following Proposition 6.1
:
Proposition 6.1
If 
 is either an increasing or a decreasing sequence of events, then
Proof Suppose, first, that 
 is an increasing sequence, and define the
events 
by
where we have used the fact that 
 since the events are
increasing. In words, 
 consists of those outcomes in 
 that are not in any of
the earlier 
 It is easy to verify that the 
 are mutually exclusive events
such that
Thus,
lim
௡→ஶ  𝐸௡ൌ
௜
ଵ
ஶ
𝐸௜
ሼ𝐸௡,𝑛൒1ሽ
lim
௡→ஶ𝐸௡,
lim
௡→ஶ  𝐸௡ൌ
௜
ଵ
ஶ
𝐸௜
ሼ𝐸௡,𝑛൒1ሽ
lim
௡→ஶ𝑃ሺ𝐸௡ሻൌ𝑃ሺlim
௡→ஶ𝐸௡ሻ
ሼ𝐸௡,𝑛൒1ሽ
𝐹௡,  𝑛൒1,
𝐹ଵൌ𝐸ଵ
𝐹௡ൌ𝐸௡൭
ଵ
௡െଵ
𝐸௜൱
௖
ൌ𝐸௡𝐸௡െଵ
௖
 𝑛൐1
∪
ଵ
௡െଵ
𝐸௜ൌ𝐸௡െଵ,
𝐹௡
𝐸௡
𝐸௜,𝑖൏𝑛.
𝐹௡
௜
ଵ
ஶ
𝐹௜ൌ
௜
ଵ
ஶ
𝐸௜ and 
௜
ଵ
௡
𝐹௜ൌ
௜
ଵ
௡
𝐸௜ for all  𝑛൒1
78 of 848

which proves the result when 
 is increasing.
If 
is a decreasing sequence, then 
 is an increasing
sequence; hence, from the preceding equations,
However, because 
 it follows that
or, equivalently,
or
which proves the result.
Example 6a Probability and a “Paradox”
𝑃ቆ
ଵ
ஶ𝐸௜ቇ
ൌ𝑃ቆ
ଵ
ஶ𝐹௜ቇ
ൌ
ଵ
ஶ
𝑃ሺ𝐹௜ሻ ሺby Axiom 3ሻ
ൌ
lim
௡  → ஶଵ
௡
𝑃ሺ𝐹௜ሻ
ൌ
lim
௡  → ஶ𝑃൭
ଵ
௡
𝐹௜൱
ൌ
lim
௡  → ஶ𝑃൭
ଵ
௡
𝐸௜൱
ൌ
lim
௡  → ஶ𝑃ሺ𝐸௡ሻ
ሼ𝐸௡, 𝑛൒1ሽ
ሼ𝐸௡, 𝑛൒1ሽ
ሼ𝐸௡
௖,𝑛൒1ሽ
𝑃൬
ଵ
ஶ𝐸௜
௖൰ൌ
lim
௡→ஶ𝑃ሺ𝐸௡
௖ሻ
ଵ
ஶ𝐸௜
௖ൌቆ
ଵ
ஶ𝐸௜ቇ
௖
,
𝑃൭ቆ
ଵ
ஶ𝐸௜ቇ
௖
൱ൌ
lim
௡→ஶ𝑝ሺ𝐸௡
௖ሻ
1 െ𝑃ቆ
ଵ
ஶ𝐸௜ቇൌ
௡→ஶሾ1 െ𝑃ሺ𝐸௡ሻሿൌ1 െ
௡→ஶ𝑃ሺ𝐸௡ሻ
𝑃ቆ
ଵ
ஶ𝐸௜ቇൌ
௡→ஶ𝑃ሺ𝐸௡ሻ
79 of 848

Suppose that we possess an infinitely large urn and an infinite collection of balls
labeled ball number 1, number 2, number 3, and so on. Consider an experiment
performed as follows: At 1 minute to 12 ඘.ඕ., balls numbered 1 through 10 are
placed in the urn and ball number 10 is withdrawn. (Assume that the withdrawal
takes no time.) At  minute to 12 ඘.ඕ., balls numbered 11 through 20 are placed
in the urn and ball number 20 is withdrawn. At  minute to 12 ඘.ඕ., balls
numbered 21 through 30 are placed in the urn and ball number 30 is withdrawn.
At  minute to 12 ඘.ඕ., and so on. The question of interest is, How many balls are
in the urn at 12 ඘.ඕ.?
The answer to this question is clearly that there is an infinite number of balls in
the urn at 12 ඘.ඕ., since any ball whose number is not of the form 10
 will
have been placed in the urn and will not have been withdrawn before 12 ඘.ඕ.
Hence, the problem is solved when the experiment is performed as described.
However, let us now change the experiment and suppose that at 1 minute to 12
඘.ඕ., balls numbered 1 through 10 are placed in the urn and ball number 1 is
withdrawn; at  minute to 12 ඘.ඕ., balls numbered 11 through 20 are placed in the
urn and ball number 2 is withdrawn; at  minute to 12 P.M, balls numbered 21
through 30 are placed in the urn and ball number 3 is withdrawn; at  minute to
12 ඘.ඕ., balls numbered 31 through 40 are placed in the urn and ball number 4 is
withdrawn, and so on. For this new experiment, how many balls are in the urn at
12 ඘.ඕ.?
Surprisingly enough, the answer now is that the urn is empty at 12 ඘.ඕ. For,
consider any ball–say, ball number  At some time prior to 12 ඘.ඕ. [in particular,
at 
 minutes to 12 ඘.ඕ.], this ball would have been withdrawn from the urn.
Hence, for each  ball number  is not in the urn at 12 ඘.ඕ.; therefore, the urn
must be empty at that time.
Because for all  the number of balls in the urn after the nth interchange is the
same in both variations of the experiment, most people are surprised that the two
scenarios produce such different results in the limit. It is important to recognize
that the reason the results are different is not because there is an actual paradox,
or mathematical contradiction, but rather because of the logic of the situation,
and also that the surprise results because one’s initial intuition when dealing with
infinity is not always correct. (This latter statement is not surprising, for when the
theory of the infinite was first developed by the mathematician Georg Cantor in
the second half of the nineteenth century, many of the other leading
1
2
1
4
1
8
𝑛, 𝑛൒1,
ଵ
ଶ
1
4
1
8
𝑛.
ቀ1
2ቁ
௡െଵ
𝑛,
𝑛
𝑛,
80 of 848

mathematicians of the day called it nonsensical and ridiculed Cantor for making
such claims as that the set of all integers and the set of all even integers have
the same number of elements.)
We see from the preceding discussion that the manner in which the balls are
withdrawn makes a difference. For, in the first case, only balls numbered
 are ever withdrawn, whereas in the second case all of the balls are
eventually withdrawn. Let us now suppose that whenever a ball is to be
withdrawn, that ball is randomly selected from among those present. That is,
suppose that at 1 minute to 12 ඘.ඕ. balls numbered 1 through 10 are placed in
the urn and a ball is randomly selected and withdrawn, and so on. In this case,
how many balls are in the urn at 12 ඘.ඕ.?
Solution
We shall show that, with probability 1, the urn is empty at 12 ඘.ඕ. Let us first
consider ball number 1. Define 
 to be the event that ball number 1 is still in the
urn after the first  withdrawals have been made. Clearly,
[To understand this equation, just note that if ball number 1 is still to be in the urn
after the first  withdrawals, the first ball withdrawn can be any one of 9, the
second any one of 18 (there are 19 balls in the urn at the time of the second
withdrawal, one of which must be ball number 1), and so on. The denominator is
similarly obtained.]
Now, the event that ball number 1 is in the urn at 12 ඘.ඕ. is just the event
Because the events 
 are decreasing events, it follows from
Proposition 6.1
 that
We now show that
10𝑛,𝑛൒1,
𝐸௡
𝑛
𝑃ሺ𝐸௡ሻൌ
9 ⋅18 ⋅27 ⋯ሺ9𝑛ሻ
10 ⋅19 ⋅28 ⋯ሺ9𝑛൅1ሻ
𝑛
∩
௡ൌଵ
ஶ
𝐸௡.
𝐸௡,𝑛൒1,
𝑃ሼball number 1 is in the urn at 12P.M.ሽ
     ൌ𝑃൬
∩
௡ൌଵ
ஶ
𝐸௡൰
     ൌ
lim
௡→ஶ𝑃ሺ𝐸௡ሻ
     ൌ
Π
௡ൌଵ
ஶ
ቀ
ଽ௡
ଽ௡൅ଵቁ
Π
௡ൌଵ
ஶ
9𝑛
9𝑛൅1 ൌ0
81 of 848

Since
this is equivalent to showing that
Now, for all 
Hence, letting 
 and using the fact that 
 yields
Thus, letting 
 denote the event that ball number  is in the urn at 12 ඘.ඕ., we
have shown that 
 Similarly, we can show that 
 for all 
(For instance, the same reasoning shows that 
 for
) Therefore, the probability that the urn is not empty at 12 ඘.ඕ.,
 satisfies
by Boole’s inequality.
Π
௡ൌଵ
ஶ
ቆ
9𝑛
9𝑛൅1ቇൌቈ
Π
௡ൌଵ
ஶ
ቆ9𝑛൅1
9𝑛
ቇ቉
െଵ
Π
௡ൌଵ
ஶ
ቆ1 ൅1
9𝑛ቇൌ∞
𝑚൒1,
௡
ଵ
ஶ
ቆ1 ൅1
9𝑛ቇ൒
௡
ଵ
௠
ቆ1 ൅1
9𝑛ቇ
ൌቆ1 ൅1
9ቇቆ1 ൅1
18ቇቆ1 ൅1
27ቇ⋯ቆ1 ൅1
9𝑚ቇ
൐1
9 ൅1
18 ൅1
27 ൅⋯൅1
9𝑚
ൌ1
9 ෍
௜ൌଵ
௠
1
𝑖
𝑚→∞
෍
௜ൌଵ
ஶ
1/𝑖ൌ∞
ෑ
௡ൌଵ
ஶ
ቆ1 ൅1
9𝑛ቇൌ∞
𝐹௜
𝑖
𝑃ሺ𝐹ଵሻൌ0.
𝑃ሺ𝐹௜ሻൌ0
𝑖.
𝑃ሺ𝐹௜ሻൌ
ෑ
௡ൌଶ
ஶ
ሾ9𝑛/ሺ9𝑛൅1ሻሿ
𝑖ൌ11,12,...,20.
𝑃൬∪
ଵ
ஶ𝐹௜൰,
𝑃ቆ
ଵ
ஶ𝐹௜ቇ൑෍
ଵ
ஶ
𝑃ሺ𝐹௜ሻൌ0
82 of 848

Thus, with probability 1, the urn will be empty at 12 ඘.ඕ.
Thus far we have interpreted the probability of an event of a given experiment as
being a measure of how frequently the event will occur when the experiment is
continually repeated. However, there are also other uses of the term probability. For
instance, we have all heard such statements as “It is 90 percent probable that
Shakespeare actually wrote Hamlet” or “The probability that Oswald acted alone in
assassinating Kennedy is .8.” How are we to interpret these statements?
The most simple and natural interpretation is that the probabilities referred to are
measures of the individual’s degree of belief in the statements that he or she is
making. In other words, the individual making the foregoing statements is quite
certain that Oswald acted alone and is even more certain that Shakespeare wrote
Hamlet. This interpretation of probability as being a measure of the degree of one’s
belief is often referred to as the personal or subjective view of probability.
It seems logical to suppose that a “measure of the degree of one’s belief” should
satisfy all of the axioms of probability. For example, if we are 70 percent certain that
Shakespeare wrote Julius Caesar and 10 percent certain that it was actually
Marlowe, then it is logical to suppose that we are 80 percent certain that it was either
Shakespeare or Marlowe. Hence, whether we interpret probability as a measure of
belief or as a long-run frequency of occurrence, its mathematical properties remain
unchanged.
Example 7a
Suppose that in a 7-horse race, you believe that each of the first 2 horses has a
20 percent chance of winning, horses 3 and 4 each have a 15 percent chance,
and the remaining 3 horses have a 10 percent chance each. Would it be better
for you to wager at even money that the winner will be one of the first three
horses or to wager, again at even money, that the winner will be one of the
horses 1, 5, 6, and 7?
Solution
On the basis of your personal probabilities concerning the outcome of the race,
your probability of winning the first bet is 
 whereas it is
 for the second bet. Hence, the first wager is more attractive.
Note that in supposing that a person’s subjective probabilities are always consistent
.2 ൅.2 ൅.15 ൌ.55,
.2 ൅.1 ൅.1 ൅.1 ൌ.5
83 of 848

with the axioms of probability, we are dealing with an idealized rather than an actual
person. For instance, if we were to ask someone what he thought the chances were
of
a. rain today,
b. rain tomorrow,
c. rain both today and tomorrow,
d. rain either today or tomorrow,
it is quite possible that, after some deliberation, he might give 30 percent, 40 percent,
20 percent, and 60 percent as answers. Unfortunately, such answers (or such
subjective probabilities) are not consistent with the axioms of probability. (Why not?)
We would of course hope that after this was pointed out to the respondent, he would
change his answers. (One possibility we could accept is 30 percent, 40 percent, 10
percent, and 60 percent.)
Let  denote the set of all possible outcomes of an experiment.  is called the sample
space of the experiment. An event is a subset of  If 
 are events, then
 called the union of these events, consists of all outcomes that are in at
least one of the events 
 Similarly, 
 sometimes written as 
is called the intersection of the events 
 and consists of all outcomes that are in all
of the events 
For any event 
 we define 
 to consist of all outcomes in the sample space that are
not in 
 We call 
 the complement of the event A. The event 
 which is empty of
outcomes, is designated by Ø and is called the null set. If 
 then we say that 
and  are mutually exclusive.
For each event  of the sample space  we suppose that a number 
 called the
probability of 
 is defined and is such that
i. 
ii. 
iii. For mutually exclusive events 
𝑆
𝑆
𝑆.
𝐴௜,𝑖ൌ1,...,𝑛,
∪
௜ൌଵ
௡
𝐴௜,
𝐴௜,𝑖ൌ1,...,𝑛.
∩
௜ൌଵ
௡
𝐴௜,
𝐴ଵ⋯𝐴௡,
𝐴௜
𝐴௜,𝑖ൌ1,...,𝑛.
𝐴,
𝐴௖
𝐴.
𝐴௖
𝑆௖,
𝐴𝐵ൌØ,
𝐴
𝐵
𝐴
𝑆,
𝑃ሺ𝐴ሻ,
𝐴,
0 ൑𝑃ሺ𝐴ሻ൑1
𝑃ሺ𝑆ሻൌ1
𝐴௜,𝑖൒1,
𝑃൬
∪
௜ൌଵ
ஶ
𝐴௜൰ൌ෍
௜ൌଵ
ஶ
𝑃ሺ𝐴௜ሻ
84 of 848

 represents the probability that the outcome of the experiment is in 
It can be shown that
A useful result is that
which can be generalized to give
This result is known as the inclusion–exclusion identity.
If  is finite and each one point set is assumed to have equal probability, then
where 
 denotes the number of outcomes in the event 
 can be interpreted either as a long-run relative frequency or as a measure of
one’s degree of belief.
𝑃ሺ𝐴ሻ
𝐴.
𝑃ሺ𝐴௖ሻൌ1 െ𝑃ሺ𝐴ሻ
𝑃ሺ𝐴∪𝐵ሻൌ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐵ሻെ𝑃ሺ𝐴𝐵ሻ
𝑃ቆ
௜ൌଵ
௡
𝐴௜ቇൌ
௜
ଵ
௡
𝑃ሺ𝐴௜ሻെ
௜ழ௝𝑃ሺ𝐴௜𝐴௝ሻ
൅
∑∑∑
௜ழ௝ழ௞
𝑃ሺ𝐴௜𝐴௝𝐴௞ሻ
൅⋯൅ሺെ1ሻ௡൅ଵ𝑃ሺ𝐴ଵ⋯𝐴௡ሻ
𝑆
𝑃ሺ𝐴ሻൌ|𝐴|
|𝑆|
|𝐸|
𝐸.
𝑃ሺ𝐴ሻ
1. A box contains 3 marbles: 1 red, 1 green, and 1 blue. Consider an
experiment that consists of taking 1 marble from the box and then replacing it
in the box and drawing a second marble from the box. Describe the sample
space. Repeat when the second marble is drawn without replacing the first
marble.
2. In an experiment, die is rolled continually until a 6 appears, at which point
the experiment stops. What is the sample space of this experiment? Let 
denote the event that  rolls are necessary to complete the experiment. What
points of the sample space are contained in 
 What is 
𝐸௡
𝑛
𝐸௡?
ቆ
ଵ
ஶ𝐸௡ቇ
௖
?
85 of 848

3. Two dice are thrown. Let  be the event that the sum of the dice is odd, let
 be the event that at least one of the dice lands on 1, and let  be the event
that the sum is 5. Describe the events 
 and EFG.
4. A, B, and C take turns flipping a coin. The first one to get a head wins. The
sample space of this experiment can be defined by
a. Interpret the sample space.
b. Define the following events in terms of  10pt
i. A wins = A.
ii. B wins = B.
iii. 
Assume that A flips first, then B, then C, then A, and so on.
5. A system is composed of 5 components, each of which is either working or
failed. Consider an experiment that consists of observing the status of each
component, and let the outcome of the experiment be given by the vector
 where 
 is equal to 1 if component  is working and is
equal to 0 if component  is failed.
a. How many outcomes are in the sample space of this experiment?
b. Suppose that the system will work if components 1 and 2 are both
working, or if components 3 and 4 are both working, or if components
1, 3, and 5 are all working. Let 
 be the event that the system will
work. Specify all the outcomes in 
c. Let  be the event that components 4 and 5 are both failed. How many
outcomes are contained in the event 
d. Write out all the outcomes in the event AW.
6. A hospital administrator codes incoming patients suffering gunshot wounds
according to whether they have insurance (coding 1 if they do and 0 if they do
not) and according to their condition, which is rated as good (g), fair (f), or
serious (s). Consider an experiment that consists of the coding of such a
patient.
a. Give the sample space of this experiment.
b. Let  be the event that the patient is in serious condition. Specify the
outcomes in 
c. Let  be the event that the patient is uninsured. Specify the outcomes
in 
d. Give all the outcomes in the event 
𝐸
𝐹
𝐺
𝐸𝐹,𝐸∪𝐹,𝐹𝐺,𝐸𝐹௖,
𝑆ൌቊ1,01,001,0001,...,
0000⋯
𝑆:
ሺ𝐴∪𝐵ሻ௖.
ሺ𝑥ଵ, 𝑥ଶ, 𝑥ଷ, 𝑥ସ, 𝑥ହሻ,
𝑥௜
𝑖
𝑖
𝑊
𝑊.
𝐴
𝐴?
𝐴
𝐴.
𝐵
𝐵.
𝐵௖∪𝐴.
86 of 848

7. Consider an experiment that consists of determining the type of job–either
blue collar or white collar–and the political affiliation Republican, Democratic,
or Independent–of the 15 members of an adult soccer team. How many
outcomes are
a. in the sample space?
b. in the event that at least one of the team members is a blue-collar
worker?
c. in the event that none of the team members considers himself or
herself an Independent?
8. Suppose that  and  are mutually exclusive events for which 
and 
 What is the probability that
a. either  or  occurs?
b.  occurs but  does not?
c. both  and  occur?
9. A retail establishment accepts either the American Express or the VISA
credit card. A total of 24 percent of its customers carry an American Express
card, 61 percent carry a VISA card, and 11 percent carry both cards. What
percentage of its customers carry a credit card that the establishment will
accept?
10. Sixty percent of the students at a certain school wear neither a ring nor a
necklace. Twenty percent wear a ring and 30 percent wear a necklace. If one
of the students is chosen randomly, what is the probability that this student is
wearing
a. a ring or a necklace?
b. a ring and a necklace?
11. A total of 28 percent of American males smoke cigarettes, 7 percent
smoke cigars, and 5 percent smoke both cigars and cigarettes.
a. What percentage of males smokes neither cigars nor cigarettes?
b. What percentage smokes cigars but not cigarettes?
12. An elementary school is offering 3 language classes: one in Spanish, one
in French, and one in German. The classes are open to any of the 100
students in the school. There are 28 students in the Spanish class, 26 in the
French class, and 16 in the German class. There are 12 students who are in
both Spanish and French, 4 who are in both Spanish and German, and 6 who
are in both French and German. In addition, there are 2 students taking all 3
classes.
a. If a student is chosen randomly, what is the probability that he or she is
𝐴
𝐵
𝑃ሺ𝐴ሻൌ.3
𝑃ሺ𝐵ሻൌ.5.
𝐴
𝐵
𝐴
𝐵
𝐴
𝐵
87 of 848

not in any of the language classes?
b. If a student is chosen randomly, what is the probability that he or she is
taking exactly one language class?
c. If 2 students are chosen randomly, what is the probability that at least 1
is taking a language class?
13. A certain town with a population of 100,000 has 3 newspapers: I, II, and
III. The proportions of townspeople who read these papers are as follows:
I: 10 percent I and II: 8 percent I and II and III: 1 percent
II: 30 percent I and III: 2 percent
III: 5 percent II and III: 4 percent
(The list tells us, for instance, that 8000 people read newspapers I and II.)
a. Find the number of people who read only one newspaper.
b. How many people read at least two newspapers?
c. If I and III are morning papers and II is an evening paper, how many
people read at least one morning paper plus an evening paper?
d. How many people do not read any newspapers?
e. How many people read only one morning paper and one evening
paper?
14. The following data were given in a study of a group of 1000 subscribers to
a certain magazine: In reference to job, marital status, and education, there
were 312 professionals, 470 married persons, 525 college graduates, 42
professional college graduates, 147 married college graduates, 86 married
professionals, and 25 married professional college graduates. Show that the
numbers reported in the study must be incorrect.
Hint: Let 
 and  denote, respectively, the set of professionals, married
persons, and college graduates. Assume that one of the 1000 persons is
chosen at random, and use Proposition 4.4
 to show that if the given
numbers are correct, then 
15. If it is assumed that all 
 poker hands are equally likely, what is the
probability of being dealt
a. a flush? (A hand is said to be a flush if all 5 cards are of the same suit.)
b. one pair? (This occurs when the cards have denominations 
where 
 and  are all distinct.)
c. two pairs? (This occurs when the cards have denominations 
where 
 and  are all distinct.)
d. three of a kind? (This occurs when the cards have denominations 
 where 
 and  are all distinct.)
e. four of a kind? (This occurs when the cards have denominations 
𝑀, 𝑊,
𝐺
𝑃ሺ𝑀∪𝑊∪𝐺ሻ൐1.
ቆ52
5 ቇ
𝑎, 𝑎, 𝑏, 𝑐, 𝑑,
𝑎, 𝑏, 𝑐,
𝑑
𝑎, 𝑎, 𝑏, 𝑏, 𝑐,
𝑎, 𝑏,
𝑐
𝑎, 𝑎,
𝑎, 𝑏, 𝑐,
𝑎, 𝑏,
𝑐
𝑎, 𝑎, 𝑎,
88 of 848

)
16. Poker dice is played by simultaneously rolling 5 dice. Show that
a. 
b. 
c. 
d. 
e. 
f. 
g. 
17. Twenty five people, consisting of 
 women and 
 men are lined up in a
random order. Find the probability that the ninth woman to appear is in
position 17. That is, find the probability there are  women in positions  thru
 and a woman in position 
18. Two cards are randomly selected from an ordinary playing deck. What is
the probability that they form a blackjack? That is, what is the probability that
one of the cards is an ace and the other one is either a ten, a jack, a queen, or
a king?
19. Two symmetric dice have had two of their sides painted red, two painted
black, one painted yellow, and the other painted white. When this pair of dice
is rolled, what is the probability that both dice land with the same color face
up?
20. Suppose that you are playing blackjack against a dealer. In a freshly
shuffled deck, what is the probability that neither you nor the dealer is dealt a
blackjack?
21. A small community organization consists of 20 families, of which 4 have
one child, 8 have two children, 5 have three children, 2 have four children, and
1 has five children.
a. If one of these families is chosen at random, what is the probability it
has  children, 
b. If one of the children is randomly chosen, what is the probability that
child comes from a family having  children, 
22. Consider the following technique for shuffling a deck of  cards: For any
initial ordering of the cards, go through the deck one card at a time and at
each card, flip a fair coin. If the coin comes up heads, then leave the card
where it is; if the coin comes up tails, then move that card to the end of the
deck. After the coin has been flipped  times, say that one round has been
completed. For instance, if 
 and the initial ordering is 1, 2, 3, 4, then if
the successive flips result in the outcome 
 then the ordering at the end
of the round is 1, 4, 2, 3. Assuming that all possible outcomes of the sequence
𝑎, 𝑏.
𝑃ሼno two alikeሽൌ.0926;
𝑃ሼone pairሽൌ.4630;
𝑃ሼtwo pairሽൌ.2315;
𝑃ሼthree alikeሽൌ.1543;
𝑃ሼfull houseሽൌ.0386;
𝑃ሼfour alikeሽൌ.0193;
𝑃ሼϐive alikeሽൌ.0008.
15
10
8
1
16
17.
𝑖
𝑖ൌ1, 2, 3, 4, 5?
𝑖
𝑖ൌ1, 2, 3, 4, 5?
𝑛
𝑛
𝑛ൌ4
ℎ, 𝑡, 𝑡, ℎ,
89 of 848

of  coin flips are equally likely, what is the probability that the ordering after
one round is the same as the initial ordering?
23. A pair of fair dice is rolled. What is the probability that the second die
lands on a higher value than does the first?
24. If two dice are rolled, what is the probability that the sum of the upturned
faces equals  Find it for 
25. A pair of dice is rolled until a sum of either 5 or 7 appears. Find the
probability that a 5 occurs first.
Hint: Let 
 denote the event that a 5 occurs on the th roll and no 5 or 7
occurs on the first 
 rolls. Compute 
 and argue that 
 is
the desired probability.
26. The game of craps is played as follows: A player rolls two dice. If the sum
of the dice is either a 2, 3, or 12, the player loses; if the sum is either a 7 or an
11, the player wins. If the outcome is anything else, the player continues to roll
the dice until she rolls either the initial outcome or a 7. If the 7 comes first, the
player loses, whereas if the initial outcome reoccurs before the 7 appears, the
player wins. Compute the probability of a player winning at craps.
Hint: Let 
 denote the event that the initial outcome is  and the player wins.
The desired probability is 
 To compute 
 define the events
 to be the event that the initial sum is i and the player wins on the th roll.
Argue that 
27. An urn contains 3 red and 7 black balls. Players  and  withdraw balls
from the urn consecutively until a red ball is selected. Find the probability that
 selects the red ball. (  draws the first ball, then 
 and so on. There is no
replacement of the balls drawn.)
28. An urn contains 5 red, 6 blue, and 8 green balls. If a set of 3 balls is
randomly selected, what is the probability that each of the balls will be (a) of
the same color? (b) of different colors? Repeat under the assumption that
whenever a ball is selected, its color is noted and it is then replaced in the urn
before the next selection. This is known as sampling with replacement.
29. An urn contains  white and 
 black balls, where  and 
 are positive
numbers.
a. If two balls are randomly withdrawn, what is the probability that they are
the same color?
b. If a ball is randomly withdrawn and then replaced before the second
one is drawn, what is the probability that the withdrawn balls are the
same color?
𝑛
𝑖?
𝑖ൌ2, 3,   ...   , 11, 12.
𝐸௡
𝑛
𝑛െ1
𝑃ሺ𝐸௡ሻ
෍
௡ൌଵ
ஶ
𝑃ሺ𝐸௡ሻ
𝐸௜
𝑖
෍
௜ൌଶ
ଵଶ
𝑃ሺ𝐸௜ሻ.
𝑃ሺ𝐸௜ሻ,
𝐸௜,௡
𝑛
𝑃ሺ𝐸௜ሻൌ
෍
௡ൌଵ
ஶ
𝑃ሺ𝐸௜,௡ሻ.
𝐴
𝐵
𝐴
𝐴
𝐵,
𝑛
𝑚
𝑛
𝑚
90 of 848

c. Show that the probability in part (b) is always larger than the one in part
(a).
30. The chess clubs of two schools consist of, respectively, 8 and 9 players.
Four members from each club are randomly chosen to participate in a contest
between the two schools. The chosen players from one team are then
randomly paired with those from the other team, and each pairing plays a
game of chess. Suppose that Rebecca and her sister Elise are on the chess
clubs at different schools. What is the probability that
a. Rebecca and Elise will be paired?
b. Rebecca and Elise will be chosen to represent their schools but will not
play each other?
c. either Rebecca or Elise will be chosen to represent her school?
31. A 3-person basketball team consists of a guard, a forward, and a center.
a. If a person is chosen at random from each of three different such
teams, what is the probability of selecting a complete team?
b. What is the probability that all 3 players selected play the same
position?
32. A group of individuals containing  boys and  girls is lined up in random
order; that is, each of the 
 permutations is assumed to be equally
likely. What is the probability that the person in the th position, 
is a girl?
33. A forest contains 20 elk, of which 5 are captured, tagged, and then
released. A certain time later, 4 of the 20 elk are captured. What is the
probability that 2 of these 4 have been tagged? What assumptions are you
making?
34. The second Earl of Yarborough is reported to have bet at odds of 1000 to
1 that a bridge hand of 13 cards would contain at least one card that is ten or
higher. (By ten or higher we mean that a card is either a ten, a jack, a queen,
a king, or an ace.) Nowadays, we call a hand that has no cards higher than 9
Yarborough. What is the probability that a randomly selected bridge hand is
a Yarborough?
35. Seven balls are randomly withdrawn from an urn that contains 
 red, 16
blue, and 18 green balls. Find the probability that
a.  red,  blue, and  green balls are withdrawn;
b. at least  red balls are withdrawn;
c. all withdrawn balls are the same color;
d. either exactly  red balls or exactly 3 blue balls are withdrawn.
𝑏
𝑔
ሺ𝑏൅𝑔ሻ!
𝑖
1 ൑𝑖൑𝑏൅𝑔,
𝑎
12
3
2
2
2
3
91 of 848

36. Two cards are chosen at random from a deck of 52 playing cards. What is
the probability that they
a. are both aces?
b. have the same value?
37. An instructor gives her class a set of 10 problems with the information that
the final exam will consist of a random selection of 5 of them. If a student has
figured out how to do 7 of the problems, what is the probability that he or she
will answer correctly
a. all 5 problems?
b. at least 4 of the problems?
38. There are n socks, 3 of which are red, in a drawer. What is the value of 
if, when 2 of the socks are chosen randomly, the probability that they are both
red is 
39. There are 5 hotels in a certain town. If 3 people check into hotels in a day,
what is the probability that they each check into a different hotel? What
assumptions are you making?
40. If  balls are randomly chosen from an urn containing  red,  white, 
blue, and  green balls, find the probability that
a. at least one of the  balls chosen is green;
b. one ball of each color is chosen.
41. If a die is rolled 4 times, what is the probability that 6 comes up at least
once?
42. Two dice are thrown n times in succession. Compute the probability that
double 6 appears at least once. How large need  be to make this probability
at least 
43.
a. If  people, including  and 
 are randomly arranged in a line, what is
the probability that  and  are next to each other?
b. What would the probability be if the people were randomly arranged in
a circle?
44. Five people, designated as 
 are arranged in linear order.
Assuming that each possible order is equally likely, what is the probability that
a. there is exactly one person between  and 
b. there are exactly two people between  and 
c. there are three people between  and 
𝑛
1
2 ?
4
4
5
6
7
4
𝑛
1
2 ?
𝑁
𝐴
𝐵,
𝐴
𝐵
𝐴, 𝐵, 𝐶, 𝐷, 𝐸,
𝐴
𝐵?
𝐴
𝐵?
𝐴
𝐵?
92 of 848

45. A woman has  keys, of which one will open her door.
a. If she tries the keys at random, discarding those that do not work, what
is the probability that she will open the door on her th try?
b. What if she does not discard previously tried keys?
46. How many people have to be in a room in order that the probability that at
least two of them celebrate their birthday in the same month is at least 
Assume that all possible monthly outcomes are equally likely.
47. Suppose that  of the numbers 
 are chosen. Find the probability
that  is the third smallest value chosen.
48. Given 20 people, what is the probability that among the 12 months in the
year, there are 4 months containing exactly 2 birthdays and 4 containing
exactly 3 birthdays?
49. A group of 6 men and 6 women is randomly divided into 2 groups of size 6
each. What is the probability that both groups will have the same number of
men?
50. In a hand of bridge, find the probability that you have 5 spades and your
partner has the remaining 8.
51. Suppose that  balls are randomly distributed into  compartments. Find
the probability that 
 balls will fall into the first compartment. Assume that all
 arrangements are equally likely.
52. A closet contains 10 pairs of shoes. If 8 shoes are randomly selected,
what is the probability that there will be
a. no complete pair?
b. exactly 1 complete pair?
53. If  people, consisting of  couples, are randomly arranged in a row, find
the probability that no person is next to their partner.
54. Compute the probability that a bridge hand is void in at least one suit.
Note that the answer is not
(Why not?)
Hint: Use Proposition 4.4
.
55. Compute the probability that a hand of 13 cards contains
a. the ace and king of at least one suit;
b. all 4 of at least 1 of the 13 denominations.
𝑛
𝑘
1
2 ?
5
1, 2, ... , 14
9
𝑛
𝑁
𝑚
𝑁௡
8
4
ቆ4
1ቇቆ39
13ቇ
ቆ52
13ቇ
93 of 848

Prove the following relations:
56. Two players play the following game: Player  chooses one of the three
spinners pictured in Figure 2.6
, and then player  chooses one of the
remaining two spinners. Both players then spin their spinner, and the one that
lands on the higher number is declared the winner. Assuming that each
spinner is equally likely to land in any of its 3 regions, would you rather be
player A or player 
 Explain your answer!
Figure 2.6 Spinners
𝐴
𝐵
𝐵?
1. 
2. If 
 then 
3. 
 and 
4. 
 and
𝐸𝐹⊂𝐸⊂𝐸∪𝐹.
𝐸⊂𝐹,
𝐹௖⊂𝐸௖.
𝐹ൌ𝐹𝐸∪𝐹𝐸௖
𝐸∪𝐹ൌ𝐸∪𝐸௖𝐹.
൬∪
ଵ
ஶ𝐸௜൰𝐹ൌ∪
ଵ
ஶ𝐸௜𝐹
൬∩
ଵ
ஶ𝐸௜൰∪𝐹ൌ
ଵ
ஶ
ሺ𝐸௜∪𝐹ሻ.
94 of 848

5. For any sequence of events 
 define a new sequence 
 of
disjoint events (that is, events such that 
 whenever 
) such that
for all 
6. Let 
 and  be three events. Find expressions for the events so that, of
 and 
a. only  occurs;
b. both  and 
 but not 
 occur;
c. at least one of the events occurs;
d. at least two of the events occur;
e. all three events occur;
f. none of the events occurs;
g. at most one of the events occurs;
h. at most two of the events occur;
i. exactly two of the events occur;
j. at most three of the events occur.
7. Use Venn diagrams
a. to simplify the expression 
b. to prove DeMorgan’s laws for events  and 
 [That is, prove
 and 
8. Let  be a given set. If, for some 
 are mutually exclusive
nonempty subsets of  such that 
, then we call the set
 a partition of  Let 
 denote the number of different partitions of
 Thus, 
 (the only partition being 
) and 
 (the two
partitions being 
a. Show, by computing all partitions, that 
b. Show that
and use this equation to compute 
Hint: One way of choosing a partition of 
 items is to call one of the
items special. Then we obtain different partitions by first choosing
 then a subset of size 
 of the nonspecial items, and
then any of the 
 partitions of the remaining  nonspecial items. By
adding the special item to the subset of size 
 we obtain a partition
𝐸ଵ,𝐸ଶ,...,
𝐹ଵ,𝐹ଶ,...
𝐹௜𝐹௝ൌØ
𝑖്𝑗
𝑛൒1,
ଵ
௡
𝐹௜ൌ
ଵ
௡
𝐸௜
𝐸, 𝐹,
𝐺
𝐸, 𝐹,
𝐺,
𝐸
𝐸
𝐺,
𝐹,
ሺ𝐸∪𝐹ሻሺ𝐸∪𝐹௖ሻ;
𝐸
𝐹.
 ሺ𝐸∪𝐹ሻ௖ൌ𝐸௖𝐹௖,
 ሺ𝐸𝐹ሻ௖ൌ𝐸௖∪𝐹௖.ሿ
𝑆
𝑘൐0, 𝑆ଵ,𝑆ଶ, ... , 𝑆௞
𝑆
∪
௜ൌଵ
௞
𝑆௜ൌ𝑆,
ሼ𝑆ଵ,𝑆ଶ,...,𝑆௞ሽ
𝑆.
𝑇௡
ሼ1,2,...,𝑛ሽ.
𝑇ଵൌ1
𝑆ଵൌሼ1ሽ
𝑇ଶൌ2
ሼሼ1,2,ሽሽ, ሼሼ1ሽ, ሼ2ሽሻ.
𝑇ଷൌ5,𝑇ସൌ15.
𝑇௡൅ଵൌ1 ൅
෍
௞ൌଵ
௡
ቆ𝑛
𝑘ቇ𝑇௞
𝑇ଵ଴.
𝑛൅1
𝑘,𝑘ൌ0, 1, ... , 𝑛,
𝑛െ𝑘
𝑇௞
𝑘
𝑛െ𝑘,
95 of 848

of all 
 items.
9. Suppose that an experiment is performed  times. For any event  of the
sample space, let 
 denote the number of times that event  occurs and
define 
 Show that 
 satisfies Axioms 1, 2, and 3.
10. Prove that
11. If 
 and 
 show that 
 In general, prove
Bonferroni’s inequality, namely,
12. Show that the probability that exactly one of the events  or  occurs
equals 
13. Prove that
14. Prove Proposition 4.4
 by mathematical induction.
15. An urn contains 
 white and  black balls. If a random sample of size  is
chosen, what is the probability that it contains exactly  white balls?
16. Use induction to generalize Bonferroni’s inequality to  events. That is,
show that
17. Consider the matching problem, Example 5m
, and define 
 to be the
number of ways in which the  men can select their hats so that no man
selects his own. Argue that
This formula, along with the boundary conditions 
 can then be
solved for 
 and the desired probability of no matches would be 
Hint: After the first man selects a hat that is not his own, there remain 
men to select among a set of 
 hats that does not contain the hat of one
of these men. Thus, there is one extra man and one extra hat. Argue that we
can get no matches either with the extra man selecting the extra hat or with
the extra man not selecting the extra hat.
18. Let 
 denote the number of ways of tossing a coin  times such that
successive heads never appear. Argue that
Hint: How many outcomes are there that start with a head, and how many
start with a tail? If 
 denotes the probability that successive heads never
appear when a coin is tossed  times, find 
 (in terms of 
) when all
possible outcomes of the  tosses are assumed equally likely. Compute 
𝑛൅1
𝑛
𝐸
𝑛ሺ𝐸ሻ
𝐸
𝑓ሺ𝐸ሻൌ𝑛ሺ𝐸ሻ/𝑛.
𝑓ሺ⋅ሻ
𝑃ሺ𝐸∪𝐹∪𝐺ሻൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻ൅𝑃ሺ𝐺ሻെ𝑃ሺ𝐸௖𝐹𝐺ሻെ𝑃ሺ𝐸𝐹௖𝐺ሻെ𝑃ሺ𝐸𝐹𝐺௖ሻെ𝑃ሺ𝐸𝐹𝐺௖ሻെ2𝑃ሺ𝐸𝐹𝐺
𝑃ሺ𝐸ሻൌ.9
𝑃ሺ𝐹ሻൌ.8,
𝑃ሺ𝐸𝐹ሻ൒.7.
𝑃ሺ𝐸𝐹ሻ൒𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻെ1
𝐸
𝐹
𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻെ2𝑃ሺ𝐸𝐹ሻ.
𝑃ሺ𝐸𝐹௖ሻൌ𝑃ሺ𝐸ሻെ𝑃ሺ𝐸𝐹ሻ.
𝑀
𝑁
𝑟
𝑘
𝑛
𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡ሻ൒𝑃ሺ𝐸ଵሻ൅⋯൅𝑃ሺ𝐸௡ሻെሺ𝑛െ1ሻ
𝐴ே
𝑁
𝐴ேൌሺ𝑁െ1ሻሺ𝐴ேെଵ൅𝐴ேെଶሻ
𝐴ଵൌ0, 𝐴ଶൌ1,
𝐴ே,
𝐴ே/𝑁!.
𝑁െ1
𝑁െ1
𝑓௡
𝑛
𝑓௡ൌ𝑓௡െଵ൅𝑓௡െଶ  𝑛൒2, where 𝑓଴≡1, 𝑓ଵ≡2
𝑃௡
𝑛
𝑃௡
𝑓௡
𝑛
𝑃ଵ଴.
96 of 848

19. An urn contains  red and 
 blue balls. They are withdrawn one at a time
until a total of 
 red balls have been withdrawn. Find the probability that
a total of  balls are withdrawn.
Hint: A total of  balls will be withdrawn if there are 
 red balls in the first
 withdrawals and the th withdrawal is a red ball.
20. Consider an experiment whose sample space consists of a countably
infinite number of points. Show that not all points can be equally likely. Can all
points have a positive probability of occurring?
*21. Consider Example 5o
, which is concerned with the number of runs of
wins obtained when  wins and 
 losses are randomly permuted. Now
consider the total number of runs–that is, win runs plus loss runs–and show
that
𝑛
𝑚
𝑟,𝑟൑𝑛,
𝑘
𝑘
𝑟െ1
𝑘െ1
𝑘
𝑛
𝑚
𝑃ሼ2𝑘 runsሽൌ2
ቆ𝑚െ1
𝑘െ1ቇቆ𝑛െ1
𝑘െ1ቇ
ቆ𝑚൅𝑛
𝑛
ቇ
𝑃ሼ2𝑘൅1 𝑟𝑢𝑛𝑠ሽ
ൌ
ቆ𝑚െ1
𝑘െ1ቇቆ𝑛െ1
𝑘
ቇ൅ቆ𝑚െ1
𝑘
ቇቆ𝑛െ1
𝑘െ1ቇ
ቆ𝑚൅𝑛
𝑛
ቇ
1. A cafeteria offers a three-course meal consisting of an entree, a starch, and
a dessert. The possible choices are given in the following table:
Course
Choices
Entree
Chicken or roast beef
Starch
Pasta or rice or potatoes
Dessert
Ice cream or Jello or apple pie or a peach
A person is to choose one course from each category.
a. How many outcomes are in the sample space?
b. Let  be the event that ice cream is chosen. How many outcomes are
in 
c. Let  be the event that chicken is chosen. How many outcomes are in
𝐴
𝐴?
𝐵
97 of 848

d. List all the outcomes in the event AB.
e. Let  be the event that rice is chosen. How many outcomes are in 
f. List all the outcomes in the event ABC.
2. A customer visiting the suit department of a certain store will purchase a
suit with probability .22, a shirt with probability .30, and a tie with probability
.28. The customer will purchase both a suit and a shirt with probability .11,
both a suit and a tie with probability .14, and both a shirt and a tie with
probability .10. A customer will purchase all 3 items with probability .06. What
is the probability that a customer purchases
a. none of these items?
b. exactly 1 of these items?
3. A deck of cards is dealt out. What is the probability that the 14th card dealt
is an ace? What is the probability that the first ace occurs on the 14th card?
4. Let  denote the event that the midtown temperature in Los Angeles is 
 and let  denote the event that the midtown temperature in New York is 
. Also, let  denote the event that the maximum of the midtown temperatures
in New York and in Los Angeles is 
 If 
 and 
find the probability that the minimum of the two midtown temperatures is 
5. An ordinary deck of 52 cards is shuffled. What is the probability that the top
four cards have
a. different denominations?
b. different suits?
6. Urn  contains 3 red and 3 black balls, whereas urn  contains 4 red and 6
black balls. If a ball is randomly selected from each urn, what is the probability
that the balls will be the same color?
7. In a state lottery, a player must choose 8 of the numbers from 1 to 40. The
lottery commission then performs an experiment that selects 8 of these 40
numbers. Assuming that the choice of the lottery commission is equally likely
to be any of the 
combinations, what is the probability that a player has
a. all 8 of the numbers selected by the lottery commission?
b. 7 of the numbers selected by the lottery commission?
c. at least 6 of the numbers selected by the lottery commission?
8. From a group of 3 first-year students, 4 sophomores, 4 juniors, and 3
seniors, a committee of size 4 is randomly selected. Find the probability that
the committee will consist of
a. 1 from each class;
𝐵?
𝐶
𝐶?
𝐴
70∘F
,
𝐵
70∘F
𝐶
70∘F.
𝑃ሺ𝐴ሻൌ.3,𝑃ሺ𝐵ሻൌ.4,
𝑃ሺ𝐶ሻൌ.2,
70∘F.
𝐴
𝐵
ቆ40
8 ቇ
98 of 848

b. 2 sophomores and 2 juniors;
c. only sophomores or juniors.
9. For a finite set 
 let 
 denote the number of elements in 
a. Show that
b. More generally, show that
10. Consider an experiment that consists of 6 horses, numbered 1 through 6,
running a race, and suppose that the sample space consists of the 6! possible
orders in which the horses finish. Let  be the event that the number-1 horse
is among the top three finishers, and let  be the event that the number-2
horse comes in second. How many outcomes are in the event 
11. A 5-card hand is dealt from a well-shuffled deck of 52 playing cards. What
is the probability that the hand contains at least one card from each of the four
suits?
12. A basketball team consists of 6 frontcourt and 4 backcourt players. If
players are divided into roommates at random, what is the probability that
there will be exactly two roommate pairs made up of a backcourt and a
frontcourt player?
13. Suppose that a person chooses a letter at random from R E S E R V E
and then chooses one at random from V E R T I C A L. What is the probability
that the same letter is chosen?
14. Prove Boole’s inequality:
15. Show that if 
 for all 
 then 
16. Let 
 denote the number of partitions of the set 
 into 
nonempty subsets, where 
 (See Theoretical Exercise 8
for the
definition of a partition.) Argue that
Hint: In how many partitions is 
 a subset, and in how many is 1 an element
𝐴,
𝑁ሺ𝐴ሻ
𝐴.
𝑁ሺ𝐴∪𝐵ሻൌ𝑁ሺ𝐴ሻ൅𝑁ሺ𝐵ሻെ𝑁ሺ𝐴𝐵ሻ
𝑁ቆ
௜ൌଵ
௡
𝐴௜ቇൌ
෍
௜
𝑁ሺ𝐴௜ሻെ෍෍
௜ழ௝
𝑁ሺ𝐴௜𝐴௝ሻ
൅⋯൅ሺെ1ሻ௡൅ଵ𝑁ሺ𝐴ଵ⋯𝐴௡ሻ
𝐴
𝐵
𝐴∪𝐵?
𝑃ቆ
௜
ଵ
ஶ
𝐴௜ቇ൑෍
௜ൌଵ
ஶ
𝑃ሺ𝐴௜ሻ
𝑃ሺ𝐴௜ሻൌ1
𝑖൒1,
𝑃ቆ
௜
ଵ
ஶ
𝐴௜ቇൌ1.
𝑇௞ሺ𝑛ሻ
ሼ1,...,𝑛ሽ
𝑘
1 ൑𝑘൑𝑛.
𝑇௞ሺ𝑛ሻൌ𝑘𝑇௞ሺ𝑛െ1ሻ൅𝑇௞െଵሺ𝑛െ1ሻ
ሼ1ሽ
99 of 848

3.1 Introduction
3.2 Conditional Probabilities
3.3 Bayes’s Formula
3.4 Independent Events
3.5 P(ꞏ|F) Is a Probability
of a subset that contains other elements?
17. Five balls are randomly chosen, without replacement, from an urn that
contains 5 red, 6 white, and 7 blue balls. Find the probability that at least one
ball of each color is chosen.
18. Four red, 8 blue, and 5 green balls are randomly arranged in a line.
a. What is the probability that the first  balls are blue?
b. What is the probability that none of the first  balls is blue?
c. What is the probability that the final  balls are of different colors?
d. What is the probability that all the red balls are together?
19. Ten cards are randomly chosen from a deck of 
 cards that consists of
 cards of each of  different suits. Each of the selected cards is put in one of
 piles, depending on the suit of the card.
a. What is the probability that the largest pile has  cards, the next largest
has  the next largest has  and the smallest has  card?
b. What is the probability that two of the piles have 3 cards, one has 4
cards, and one has no cards?
20. Balls are randomly removed from an urn initially containing 
 red and 10
blue balls. What is the probability that all of the red balls are removed before
all of the blue ones have been removed?
5
5
3
52
13
4
4
4
3,
2,
1
20
100 of 848

In this chapter, we introduce one of the most important concepts in probability theory,
that of conditional probability. The importance of this concept is twofold. In the first
place, we are often interested in calculating probabilities when some partial
information concerning the result of an experiment is available; in such a situation,
the desired probabilities are conditional. Second, even when no partial information is
available, conditional probabilities can often be used to compute the desired
probabilities more easily.
Suppose that we toss 2 dice, and suppose that each of the 36 possible outcomes is
equally likely to occur and hence has probability 
 Suppose further that we
observe that the first die is a 3. Then, given this information, what is the probability
that the sum of the 2 dice equals 8? To calculate this probability, we reason as
follows: Given that the initial die is a 3, there can be at most 6 possible outcomes of
our experiment, namely, (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and (3, 6). Since each of
these outcomes originally had the same probability of occurring, the outcomes
should still have equal probabilities. That is, given that the first die is a 3, the
(conditional) probability of each of the outcomes (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), and
(3, 6) is 
 whereas the (conditional) probability of the other 30 points in the sample
space is 0. Hence, the desired probability will be 
If we let  and  denote, respectively, the event that the sum of the dice is 8 and the
event that the first die is a 3, then the probability just obtained is called the
conditional probability that E occurs given that F has occurred and is denoted by
A general formula for 
 that is valid for all events  and  is derived in the same
manner: If the event  occurs, then, in order for  to occur, it is necessary that the
actual occurrence be a point both in  and in ; that is, it must be in 
 Now, since
we know that  has occurred, it follows that  becomes our new, or reduced, sample
space; hence, the probability that the event 
 occurs will equal the probability of EF
relative to the probability of . That is, we have the following definition.
Definition
1
36 .
1
6 ,
1
6 .
𝐸
𝐹
𝑃ሺ𝐸|𝐹ሻ
𝑃ሺ𝐸|𝐹ሻ
𝐸
𝐹
𝐹
𝐸
𝐸
𝐹
𝐸𝐹.
𝐹
𝐹
𝐸𝐹
𝐹
101 of 848

If 
 then
Example 2a
Joe is 
 percent certain that his missing key is in one of the two pockets of his
hanging jacket, being 
 percent certain it is in the left-hand pocket and 
percent certain it is in the right-hand pocket. If a search of the left-hand pocket
does not find the key, what is the conditional probability that it is in the other
pocket?
Solution
If we let  be the event that the key is in the left-hand pocket of the jacket, and 
be the event that it is in the right-hand pocket, then the desired probability
 can be obtained as follows:
If each outcome of a finite sample space  is equally likely, then, conditional on the
event that the outcome lies in a subset 
 all outcomes in  become equally
likely. In such cases, it is often convenient to compute conditional probabilities of the
form 
 by using  as the sample space. Indeed, working with this reduced
sample space often results in an easier and better understood solution. Our next two
examples illustrate this point.
Example 2b
A coin is flipped twice. Assuming that all four points in the sample space
 are equally likely, what is the conditional probability
that both flips land on heads, given that (a) the first flip lands on heads? (b) at
least one flip lands on heads?
Solution
Let 
 be the event that both flips land on heads; let 
be the event that the first flip lands on heads; and let 
 be
𝑃ሺ𝐹ሻ൐0,
𝑃ሺ𝐸|𝐹ሻൌ𝑃ሺ𝐸𝐹ሻ
𝑃ሺ𝐹ሻ
(2.1)
80
40
40
𝐿
𝑅
𝑃ሺ𝑅||𝐿௖ሻ
𝑃ሺ𝑅||𝐿௖ሻ
ൌ
𝑃ሺ𝑅𝐿௖ሻ
𝑃ሺ𝐿௖ሻ
ൌ
𝑃ሺ𝑅ሻ
1 െ𝑃ሺ𝐿ሻ
ൌ
2/3
𝑆
𝐹⊂𝑆,
𝐹
𝑃ሺ𝐸||𝐹ሻ
𝐹
𝑆ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻ, ሺ𝑡, 𝑡ሻሽ
𝐵ൌሼሺℎ, ℎሻሽ
𝐹ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻሽ
𝐴ൌሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻሽ
102 of 848

the event that at least one flip lands on heads. The probability for (a) can be
obtained from
For (b), we have
Thus, the conditional probabilityhat both flips land on heads given that the first
one does is 
 whereas the conditional probability that both flips land on heads
given that at least one does is only 
 Many students initially find this latter
result surprising. They reason that given that at least one flip lands on heads,
there are two possible results: Either they both land on heads or only one does.
Their mistake, however, is in assuming that these two possibilities are equally
likely. Initially there are  equally likely outcomes. Because the information that at
least one flip lands on heads is equivalent to the information that the outcome is
not 
 we are left with the  equally likely outcomes 
 only one
of which results in both flips landing on heads.
Example 2c
In the card game bridge, the 52 cards are dealt out equally to 4 players–called
East, West, North, and South. If North and South have a total of 8 spades among
them, what is the probability that East has 3 of the remaining 5 spades?
Solution
Probably the easiest way to compute the desired probability is to work with the
reduced sample space. That is, given that North–South have a total of 8 spades
among their 26 cards, there remains a total of 26 cards, exactly 5 of them being
spades, to be distributed among the East–West hands. Since each distribution is
equally likely, it follows that the conditional probability that East will have exactly
𝑃ሺ𝐵||𝐹ሻ
ൌ
𝑃ሺ𝐵𝐹ሻ
𝑃ሺ𝐹ሻ
ൌ
𝑃ሺሼሺℎ, ℎሻሽሻ
𝑃ሺሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻሽሻ
ൌ
1/4
2/4 ൌ1/2
𝑃ሺ𝐵||𝐴ሻ
ൌ
𝑃ሺ𝐵𝐴ሻ
𝑃ሺ𝐴ሻ
ൌ
𝑃ሺሼሺℎ, ℎሻሽሻ
𝑃ሺሼሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻሽሻ
ൌ
1/4
3/4 ൌ1/3
1/2,
1/3.
4
ሺ𝑡, 𝑡ሻ,
3
ሺℎ, ℎሻ, ሺℎ, 𝑡ሻ, ሺ𝑡, ℎሻ,
103 of 848

3 spades among his or her 13 cards is
Multiplying both sides of Equation (2.1)
 by 
 we obtain
In words, Equation (2.2)
 states that the probability that both  and  occur is
equal to the probability that  occurs multiplied by the conditional probability of 
given that  occurred. Equation (2.2)
 is often quite useful in computing the
probability of the intersection of events.
Example 2d
Celine is undecided as to whether to take a French course or a chemistry course.
She estimates that her probability of receiving an A grade would be  in a French
course and  in a chemistry course. If Celine decides to base her decision on the
flip of a fair coin, what is the probability that she gets an A in chemistry?
Solution
Let C be the event that Celine takes chemistry and  denote the event that she
receives an A in whatever course she takes, then the desired probability is 
(CA), which is calculated by using Equation (2.2)
 as follows:
Example 2e
Suppose that an urn contains 8 red balls and 4 white balls. We draw 2 balls from
the urn without replacement. (a) If we assume that at each draw, each ball in the
urn is equally likely to be chosen, what is the probability that both balls drawn are
red? (b) Now suppose that the balls have different weights, with each red ball
having weight  and each white ball having weight . Suppose that the
probability that a given ball in the urn is the next one selected is its weight divided
ቆ5
3ቇቆ21
10ቇ
ቆ26
13ቇ
ൎ.339
𝑃ሺ𝐹ሻ,
𝑃ሺ𝐸𝐹ሻൌ𝑃ሺ𝐹ሻ𝑃ሺ𝐸||𝐹ሻ
(2.2)
𝐸
𝐹
𝐹
𝐸
𝐹
1
2
2
3
𝐴
𝑃
𝑃ሺ𝐶𝐴ሻ
ൌ
𝑃ሺ𝐶ሻ𝑃ሺ𝐴||𝐶ሻ
ൌ
ቆ1
2ቇቆ2
3ቇൌ1
3
𝑟
𝑤
104 of 848

by the sum of the weights of all balls currently in the urn. Now what is the
probability that both balls are red?
Solution
Let 
 and 
 denote, respectively, the events that the first and second balls
drawn are red. Now, given that the first ball selected is red, there are 7 remaining
red balls and 4 white balls, so 
 As 
 is clearly 
 the desired
probability is
Of course, this probability could have been computed by 
For part (b), we again let 
 be the event that the th ball chosen is red and use
Now, number the red balls, and let 
 be the event that the first ball
drawn is red ball number . Then
Moreover, given that the first ball is red, the urn then contains 7 red and 4 white
balls. Thus, by an argument similar to the preceding one,
Hence, the probability that both balls are red is
A generalization of Equation (2.2)
, which provides an expression for the
probability of the intersection of an arbitrary number of events, is sometimes referred
to as the multiplication rule.
The multiplication rule
𝑅ଵ
𝑅ଶ
𝑃ሺ𝑅ଶ|𝑅ଵሻൌ7
11 .
𝑃ሺ𝑅ଵሻ
8
12 ,
𝑃ሺ𝑅ଵ𝑅ଶሻൌ
𝑃ሺ𝑅ଵሻ𝑃ሺ𝑅ଶ||𝑅ଵሻ
ൌ
ቆ2
3ቇቆ7
11ቇൌ14
33
𝑃ቆ𝑅ଵ𝑅ଶቇൌቆ8
2ቇ/ቆ12
2 ቇ.
𝑅௜
𝑖
𝑃ሺ𝑅ଵ𝑅ଶሻൌ𝑃ሺ𝑅ଵሻ𝑃ሺ𝑅ଶ|𝑅ଵሻ
𝐵௜, 𝑖ൌ1, …, 8
𝑖
𝑃ሺ𝑅ଵሻൌ𝑃൬∪೔ൌభ
଼
𝐵௜൰ൌ
∑
௜ൌଵ
଼
𝑃ሺ𝐵௜ሻൌ8 
𝑟
8𝑟൅4𝑤
𝑃ሺ𝑅ଶ|𝑅ଵሻൌ
7𝑟
7𝑟൅4𝑤
𝑃ቆ𝑅ଵ𝑅ଶቇൌ
8𝑟
8𝑟൅4𝑤 
7𝑟
7𝑟൅4𝑤
105 of 848

In words, the multiplication rule states that 
 the probability that all of the
events 
 occur, is equal to 
 the probability that 
 occurs, multiplied
by 
 the conditional probability that 
 occurs given that 
 has occurred,
multiplied by 
 the conditional probability that 
 occurs given that both 
and 
 have occurred, and so on.
To prove the multiplication rule, just apply the definition of conditional probability to its
right-hand side, giving
Example 2f
In the match problem stated in Example 5m
 of Chapter 2
, it was shown
that 
 the probability that there are no matches when  people randomly select
from among their own  hats, is given by
What is the probability that exactly  of the  people have matches?
Solution
Let us fix our attention on a particular set of  people and determine the
probability that these  individuals have matches and no one else does. Letting 
denote the event that everyone in this set has a match, and letting  be the event
that none of the other 
 people have a match, we have
Now, let 
 be the event that the th member of the set has a match.
Then
𝑃ሺ𝐸ଵ𝐸ଶ𝐸ଷ⋯𝐸௡ሻൌ𝑃ሺ𝐸ଵሻ𝑃ሺ𝐸ଶ||𝐸ଵሻ𝑃ሺ𝐸ଷ||𝐸ଵ𝐸ଶሻ⋯𝑃ሺ𝐸௡||𝐸ଵ⋯𝐸௡െଵሻ
𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡ሻ,
𝐸ଵ, 𝐸ଶ, …, 𝐸௡
𝑃ሺ𝐸ଵሻ,
𝐸ଵ
𝑃ሺ𝐸ଶ||𝐸ଵሻ,
𝐸ଶ
𝐸ଵ
𝑃ሺ𝐸ଷ||𝐸ଵ𝐸ଶሻ,
𝐸ଷ
𝐸ଵ
𝐸ଶ
𝑃ቆ𝐸ଵቇ𝑃ሺ𝐸ଵ𝐸ଶሻ
𝑃ሺ𝐸ଵሻ
𝑃ሺ𝐸ଵ𝐸ଶ𝐸ଷሻ
𝑃ሺ𝐸ଵ𝐸ଶሻ⋯𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡ሻ
𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡െଵሻൌ𝑃ቆ𝐸ଵ𝐸ଶ⋯𝐸௡ቇ
𝑃ே,
𝑁
𝑁
𝑃ேൌ෍
௜ൌ଴
ே
ሺെ1ሻ௜/𝑖!
𝑘
𝑁
𝑘
𝑘
𝐸
𝐺
𝑁െ𝑘
𝑃ሺ𝐸𝐺ሻൌ𝑃ሺ𝐸ሻ𝑃ሺ𝐺|𝐸ሻ
𝐹௜, 𝑖ൌ1, …, 𝑘,
𝑖
𝑃ሺ𝐸ሻൌ
𝑃ሺ𝐹ଵ𝐹ଶ⋯𝐹௞ሻ
ൌ
𝑃ሺ𝐹ଵሻ𝑃ሺ𝐹ଶ||𝐹ଵሻ𝑃ሺ𝐹ଷ||𝐹ଵ𝐹ଶሻ⋯𝑃ሺ𝐹௞||𝐹ଵ⋯𝐹௞െଵሻ
ൌ
1
𝑁 
1
𝑁െ1  
1
𝑁െ2⋯
1
𝑁െ𝑘൅1
ൌ
ሺ𝑁െ𝑘ሻ!
𝑁!
106 of 848

Given that everyone in the set of  has a match, the other 
 people will be
randomly choosing among their own 
 hats, so the probability that none of
them has a match is equal to the probability of no matches in a problem having
 people choosing among their own 
 hats. Therefore,
showing that the probability that a specified set of  people have matches and no
one else does is
Because there will be exactly  matches if the preceding is true for any of the
 sets of  individuals, the desired probability is
Example 2g
An ordinary deck of 52 playing cards is randomly divided into 4 piles of 13 cards
each. Compute the probability that each pile has exactly 1 ace.
Solution
Define events 
 as follows:
The desired probability is 
 and by the multiplication rule,
Now,
𝑘
𝑁െ𝑘
𝑁െ𝑘
𝑁െ𝑘
𝑁െ𝑘
𝑃ሺ𝐺|𝐸ሻൌ𝑃ேെ௞ൌ
∑
௜ൌ଴
ேെ௞
ሺെ1ሻ௜/𝑖!
𝑘
𝑃ቆ𝐸𝐺ቇൌሺ𝑁െ𝑘ሻ!
𝑁!
 𝑃ேെ௞
𝑘
ቆ𝑁
𝑘ቇ
𝑘
𝑃ሺexactly 𝑘 matchesሻൌ
ቆ𝑁
𝑘ቇ𝑃ቆ𝐸𝐺ቇ
ൌ
𝑃ேെ௄/𝐾!
ൎ
𝑒െଵ/𝑘!
when 𝑁 is large
𝐸௜, 𝑖ൌ1, 2, 3, 4,
𝐸ଵ
ൌሼthe ace of spades is in any one of the pilesሽ
𝐸ଶ
ൌሼthe ace of spades and the ace of hearts are in different pilesሽ
𝐸ଷ
ൌሼthe aces of spades, hearts, and diamonds are all in different pilesሽ
𝐸ସ
ൌሼall 4 aces are in different pilesሽ
𝑃ሺ𝐸ଵ𝐸ଶ𝐸ଷ𝐸ସሻ,
𝑃ሺ𝐸ଵ𝐸ଶ𝐸ଷ𝐸ସሻൌ𝑃ሺ𝐸ଵሻ𝑃ሺ𝐸ଶ||𝐸ଵሻ𝑃ሺ𝐸ଷ||𝐸ଵ𝐸ଶሻ𝑃ሺ𝐸ସ||𝐸ଵ𝐸ଶ𝐸ଷሻ
107 of 848

since 
 is the sample space . To determine 
 consider the pile that
contains the ace of spades. Because its remaining 
 cards are equally likely to
be any 
 of the remaining 
 cards, the probability that the ace of hearts is
among them is 
 giving that
Also, given that the ace of spades and ace of hearts are in different piles, it
follows that the set of the remaining 
 cards of these two piles is equally likely to
be any set of 
 of the remaining 
 cards. As the probability that the ace of
diamonds is one of these 
 is 
 we see that
Because the same logic as used in the preceding yields that
the probability that each pile has exactly  ace is
That is, there is approximately a 10.5 percent chance that each pile will contain
an ace. (Problem 13
 gives another way of using the multiplication rule to
solve this problem.)
Example 2h
Four of the eight teams in the quarterfinal round of the 2016 European
Champions League Football (soccer) tournament were the acknowledged strong
teams Barcelona, Bayern Munich, Real Madrid, and Paris St-Germain. The
pairings in this round are supposed to be totally random, in the sense that all
possible pairings are equally likely. Assuming this is so, find the probability that
none of the strong teams play each other in this round. (Surprisingly, it seems to
be a common occurrence in this tournament that, even though the pairings are
supposedly random, the very strong teams are rarely matched against each
other in this round.)
𝑃ሺ𝐸ଵሻൌ1
𝐸ଵ
𝑆
𝑃ሺ𝐸ଶ||𝐸ଵሻ,
12
12
51
12/51,
𝑃ሺ𝐸ଶ|𝐸ଵሻൌ1 െ12
51 ൌ39
51
24
24
50
24
24/50,
𝑃ሺ𝐸ଷ|𝐸ଵ𝐸ଶሻൌ1 െ24
50 ൌ26
50
𝑃ሺ𝐸ସ|𝐸ଵ𝐸ଶ𝐸ଷሻൌ1 െ36
49 ൌ13
49
1
𝑃ቆ𝐸ଵ𝐸ଶ𝐸ଷ𝐸ସቇൌ39 ⋅26 ⋅13
51 ⋅50 ⋅49 ൎ. 105
108 of 848

Solution
If we number the four strong teams  through  and then let 
 be
the event that team  plays one of the four weak teams, then the desired
probability is 
 By the multiplication rule
The preceding follows by first noting that because team  is equally likely to be
matched with any of the other  teams, we have that 
 Now, given
that 
 occurs, team  is equally likely to be matched with any of five teams:
namely, teams 
 or any of the three weak teams not matched with team . As
three of these five teams are weak, we see that 
 Similarly,
given that events 
 and 
 have occurred, team  is equally likely to be
matched with any from a set of three teams, consisting of team  and the
remaining two weaker teams not matched with  or . Hence,
 Finally, given that 
 and 
 all occur, team  will be
matched with the remaining weak team not matched with any of 
 giving
that 
Remarks Our definition of 
 is consistent with the interpretation of probability
as being a long-run relative frequency. To see this, suppose that  repetitions of the
experiment are to be performed, where  is large. We claim that if we consider only
those experiments in which  occurs, then 
 will equal the long-run proportion
of them in which  also occurs. To verify this statement, note that since 
 is the
long-run proportion of experiments in which  occurs, it follows that in the 
repetitions of the experiment,  will occur approximately 
 times. Similarly, in
approximately 
 of these experiments, both  and  will occur. Hence, out of
the approximately 
 experiments in which  occurs, the proportion of them in
which  also occurs is approximately equal to
Because this approximation becomes exact as  becomes larger and larger, we have
the appropriate definition of 
1
4,
𝑊௜, 𝑖ൌ1, 2, 3, 4,
𝑖
𝑃ሺ𝑊ଵ𝑊ଶ𝑊ଷ𝑊ସሻ.
𝑃ሺ𝑊ଵ𝑊ଶ𝑊ଷ𝑊ସሻൌ
𝑃ሺ𝑊ଵሻ𝑃ሺ𝑊ଶ||𝑊ଵሻ𝑃ሺ𝑊ଷ||𝑊ଵ𝑊ଶሻ𝑃ሺ𝑊ସ||𝑊ଵ𝑊ଶ𝑊ଷሻ
ൌሺ4/7ሻሺ3/5ሻሺ2/3ሻሺ1ሻ
ൌ
8/35
1
7
 𝑃ሺ𝑊ଵሻൌ4/7 .
𝑊ଵ
2
3, 4,
1
𝑃ሺ𝑊ଶ||𝑊ଵሻൌ3/5 .
𝑊ଵ
𝑊ଶ
3
4
1
2
𝑃ሺ𝑊ଷ||𝑊ଵ𝑊ଶሻൌ2/3.
𝑊ଵ, 𝑊ଶ,
𝑊ଷ
4
1, 2, 3,
𝑃ሺ𝑊ସ||𝑊ଵ𝑊ଶ𝑊ଷሻൌ1.
𝑃ሺ𝐸||𝐹ሻ
𝑛
𝑛
𝐹
𝑃ሺ𝐸||𝐹ሻ
𝐸
𝑃ሺ𝐹ሻ
𝐹
𝑛
𝐹
𝑛𝑃ሺ𝐹ሻ
𝑛𝑃ሺ𝐸𝐹ሻ
𝐸
𝐹
𝑛𝑃ሺ𝐹ሻ
𝐹
𝐸
𝑛𝑃ሺ𝐸𝐹ሻ
𝑛𝑃ሺ𝐹ሻൌ𝑃ሺ𝐸𝐹ሻ
𝑃ሺ𝐹ሻ
𝑛
𝑃ሺ𝐸||𝐹ሻ.
109 of 848

Let  and  be events. We may express  as
for, in order for an outcome to be in 
 it must either be in both  and  or be in  but
not in . (See Figure 3.1
.) As EF and 
 are clearly mutually exclusive, we
have, by Axiom 3,
Figure 3.1 
; 
Equation (3.1)
 states that the probability of the event  is a weighted average of
the conditional probability of  given that  has occurred and the conditional
probability of  given that  has not occurred—each conditional probability being
given as much weight as the event on which it is conditioned has of occurring. This is
an extremely useful formula, because its use often enables us to determine the
probability of an event by first “conditioning” upon whether or not some second event
has occurred. That is, there are many instances in which it is difficult to compute the
probability of an event directly, but it is straightforward to compute it once we know
whether or not some second event has occurred. We illustrate this idea with some
examples.
Example 3a
(Part 1)
An insurance company believes that people can be divided into two classes:
those who are accident prone and those who are not. The company’s statistics
show that an accident-prone person will have an accident at some time within a
fixed 1-year period with probability .4, whereas this probability decreases to .2 for
a person who is not accident prone. If we assume that 30 percent of the
𝐸
𝐹
𝐸
𝐸ൌ𝐸𝐹∪𝐸𝐹௖
𝐸,
𝐸
𝐹
𝐸
𝐹
𝐸𝐹௖
𝑃ሺ𝐸ሻ
ൌ
𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐸𝐹௖ሻ
ൌ
𝑃ሺ𝐸|𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸||𝐹௖ሻ𝑃ሺ𝐹௖ሻ
ൌ
𝑃ሺ𝐸|𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸||𝐹௖ሻሾ1 െ𝑃ሺ𝐹ሻሿ
(3.1)
𝐸ൌ𝐸𝐹∪𝐸𝐹௖. 𝐸𝐹ൌShaded Area 𝐸𝐹௖ൌStriped Area .
𝐸
𝐸
𝐹
𝐸
𝐹
110 of 848

population is accident prone, what is the probability that a new policyholder will
have an accident within a year of purchasing a policy?
Solution
We shall obtain the desired probability by first conditioning upon whether or not
the policyholder is accident prone. Let 
 denote the event that the policyholder
will have an accident within a year of purchasing the policy, and let  denote the
event that the policyholder is accident prone. Hence, the desired probability is
given by
Example 3a
(Part 2)
Suppose that a new policyholder has an accident within a year of purchasing a
policy. What is the probability that he or she is accident prone?
Solution
The desired probability is
Example 3b
Consider the following game played with an ordinary deck of 52 playing cards:
The cards are shuffled and then turned over one at a time. At any time, the
player can guess that the next card to be turned over will be the ace of spades; if
it is, then the player wins. In addition, the player is said to win if the ace of
spades has not yet appeared when only one card remains and no guess has yet
been made. What is a good strategy? What is a bad strategy?
Solution
Every strategy has probability 1/52 of winning! To show this, we will use induction
to prove the stronger result that for an  card deck, one of whose cards is the ace
𝐴ଵ
𝐴
𝑃ሺ𝐴ଵሻൌ
𝑃ሺ𝐴ଵ||𝐴ሻ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐴ଵ||𝐴௖ሻ𝑃ሺ𝐴௖ሻ
ൌሺ. 4ሻሺ. 3ሻ൅ሺ. 2ሻሺ. 7ሻൌ. 26
𝑃ሺ𝐴||𝐴ଵሻ
ൌ
𝑃ሺ𝐴𝐴ଵሻ
𝑃ሺ𝐴ଵሻ
ൌ
𝑃ሺ𝐴ሻ𝑃ሺ𝐴ଵ||𝐴ሻ
𝑃ሺ𝐴ଵሻ
ൌ
ሺ. 3ሻሺ. 4ሻ
. 26
ൌ6
13
𝑛
111 of 848

of spades, the probability of winning is 1/  no matter what strategy is employed.
Since this is clearly true for 
 assume it to be true for an 
 card deck,
and now consider an  card deck. Fix any strategy, and let  denote the
probability that the strategy guesses that the first card is the ace of spades.
Given that it does, the player’s probability of winning is 1/ . If, however, the
strategy does not guess that the first card is the ace of spades, then the
probability that the player wins is the probability that the first card is not the ace
of spades, namely, 
 multiplied by the conditional probability of winning
given that the first card is not the ace of spades. But this latter conditional
probability is equal to the probability of winning when using an 
 card deck
containing a single ace of spades; it is thus, by the induction hypothesis,
 Hence, given that the strategy does not guess the first card, the
probability of winning is
Thus, letting  be the event that the first card is guessed, we obtain
Example 3c
In answering a question on a multiple-choice test, a student either knows the
answer or guesses. Let  be the probability that the student knows the answer
and 
 be the probability that the student guesses. Assume that a student who
guesses at the answer will be correct with probability 
 where 
 is the
number of multiple-choice alternatives. What is the conditional probability that a
student knew the answer to a question given that he or she answered it
correctly?
Solution
Let  and  denote, respectively, the events that the student answers the
question correctly and the event that he or she actually knows the answer. Now,
𝑛,
𝑛ൌ1,
𝑛െ1
𝑛
𝑝
𝑛
ሺ𝑛െ1ሻ/𝑛,
𝑛െ1
1/ሺ𝑛െ1ሻ.
𝑛െ1
𝑛
1
𝑛െ1 ൌ1
𝑛
𝐺
𝑃ሼwinሽ
ൌ
𝑃ሼwin|𝐺ሽ𝑃ሺ𝐺ሻ൅𝑃ሼwin||𝐺௖ሽሺ1 െ𝑃ሺ𝐺ሻሻൌ1
𝑛𝑝൅1
𝑛ሺ1 െ𝑝ሻ
ൌ
1
𝑛
𝑝
1 െ𝑝
1/𝑚,
𝑚
𝐶
𝐾
112 of 848

For example, if 
 then the probability that the student knew the
answer to a question he or she answered correctly is 
Example 3d
A laboratory blood test is 95 percent effective in detecting a certain disease when
it is, in fact, present. However, the test also yields a “false positive” result for 1
percent of the healthy persons tested. (That is, if a healthy person is tested, then,
with probability .01, the test result will imply that he or she has the disease.) If .5
percent of the population actually has the disease, what is the probability that a
person has the disease given that the test result is positive?
Solution
Let  be the event that the person tested has the disease and  the event that
the test result is positive. Then the desired probability is
Thus, only 32 percent of those persons whose test results are positive actually
have the disease. Many students are often surprised at this result (they expect
the percentage to be much higher, since the blood test seems to be a good one),
so it is probably worthwhile to present a second argument that, although less
rigorous than the preceding one, is probably more revealing. We now do so.
Since .5 percent of the population actually has the disease, it follows that, on the
𝑃ሺ𝐾|𝐶ሻൌ
𝑃ሺ𝐾𝐶ሻ
𝑃ሺ𝐶ሻ
ൌ
𝑃ሺ𝐶||𝐾ሻ𝑃ሺ𝐾ሻ
𝑃ሺ𝐶||𝐾ሻ𝑃ሺ𝐾ሻ൅𝑃ሺ𝐶||𝐾௖ሻ𝑃ሺ𝐾௖ሻ
ൌ
𝑝
𝑝൅ሺ1/𝑚ሻሺ1 െ𝑝ሻ
ൌ
𝑚𝑝
1 ൅ሺ𝑚െ1ሻ𝑝
𝑚ൌ5, 𝑝ൌ1
2 ,
5
6 .
𝐷
𝐸
𝑃ሺ𝐷|𝐸ሻ
ൌ
𝑃ሺ𝐷𝐸ሻ
𝑃ሺ𝐸ሻ
ൌ
𝑃ሺ𝐸|𝐷ሻ𝑃ሺ𝐷ሻ
𝑃ሺ𝐸|𝐷ሻ𝑃ሺ𝐷ሻ൅𝑃ሺ𝐸||𝐷௖ሻ𝑃ሺ𝐷௖ሻ
ൌ
ሺ.95ሻሺ.005ሻ
ሺ.95ሻሺ.005ሻ൅ሺ.01ሻሺ.995ሻ
ൌ
95
294 ൎ.323
113 of 848

average, 1 person out of every 200 tested will have it. The test will correctly
confirm that this person has the disease with probability .95. Thus, on the
average, out of every 200 persons tested, the test will correctly confirm that .95
person has the disease. On the other hand, out of the (on the average) 199
healthy people, the test will incorrectly state that (199)(.01) of these people have
the disease. Hence, for every .95 diseased persons that the test correctly states
is ill, there are (on the average) (199)(.01) healthy persons who the test
incorrectly states are ill. Thus, the proportion of time that the test result is correct
when it states that a person is ill is
Equation (3.1)
 is also useful when one has to reassess one’s personal
probabilities in the light of additional information. For instance, consider the
examples that follow.
Example 3e
Consider a medical practitioner pondering the following dilemma: “If I’m at least
80 percent certain that my patient has this disease, then I always recommend
surgery, whereas if I’m not quite as certain, then I recommend additional tests
that are expensive and sometimes painful. Now, initially I was only 60 percent
certain that Jones had the disease, so I ordered the series A test, which always
gives a positive result when the patient has the disease and almost never does
when he is healthy. The test result was positive, and I was all set to recommend
surgery when Jones informed me, for the first time, that he was diabetic. This
information complicates matters because, although it doesn’t change my original
60 percent estimate of his chances of having the disease in question, it does
affect the interpretation of the results of the A test. This is so because the A test,
while never yielding a positive result when the patient is healthy, does
unfortunately yield a positive result 30 percent of the time in the case of diabetic
patients who are not suffering from the disease. Now what do I do? More tests or
immediate surgery?”
Solution
In order to decide whether or not to recommend surgery, the doctor should first
compute her updated probability that Jones has the disease given that the A test
result was positive. Let  denote the event that Jones has the disease and  the
event that the A test result is positive. The desired conditional probability is then
. 95
. 95 ൅ሺ199ሻሺ. 01ሻൌ95
294 ൎ. 323
𝐷
𝐸
114 of 848

Note that we have computed the probability of a positive test result by
conditioning on whether or not Jones has the disease and then using the fact that
because Jones is a diabetic, his conditional probability of a positive result given
that he does not have the disease, 
 equals .3. Hence, as the doctor
should now be more than 80 percent certain that Jones has the disease, she
should recommend surgery.
Example 3f
At a certain stage of a criminal investigation, the inspector in charge is 60 percent
convinced of the guilt of a certain suspect. Suppose, however, that a new piece
of evidence which shows that the criminal has a certain characteristic (such as
left-handedness, baldness, or brown hair) is uncovered. If 20 percent of the
population possesses this characteristic, how certain of the guilt of the suspect
should the inspector now be if it turns out that the suspect has the characteristic?
Solution
Letting  denote the event that the suspect is guilty and  the event that he
possesses the characteristic of the criminal, we have
where we have supposed that the probability of the suspect having the
characteristic if he is, in fact, innocent is equal to .2, the proportion of the
population possessing the characteristic.
Example 3g
𝑃ሺ𝐷||𝐸ሻൌ
𝑃ሺ𝐷𝐸ሻ
𝑃ሺ𝐸ሻ
ൌ
𝑃ሺ𝐷ሻ𝑃ሺ𝐸||𝐷ሻ
𝑃ሺ𝐸||𝐷ሻ𝑃ሺ𝐷ሻ൅𝑃ሺ𝐸||𝐷௖ሻ𝑃ሺ𝐷௖ሻ
ൌ
ሺ. 6ሻ1
1ሺ. 6ሻ൅ሺ. 3ሻሺ. 4ሻ
ൌ
. 833
𝑃ሺ𝐸||𝐷௖ሻ,
𝐺
𝐶
𝑃ሺ𝐺||𝐶ሻ
ൌ
𝑃ሺ𝐺𝐶ሻ
𝑃ሺ𝐶ሻ
ൌ
𝑃ሺ𝐶||𝐺ሻ𝑃ሺ𝐺ሻ
𝑃ሺ𝐶||𝐺ሻ𝑃ሺ𝐺ሻ൅𝑃ሺ𝐶||𝐺௖ሻ𝑃ሺ𝐺௖ሻ
ൌ
1ሺ. 6ሻ
1ሺ. 6ሻ൅ሺ. 2ሻሺ. 4ሻ
ൎ
. 882
115 of 848

In the world bridge championships held in Buenos Aires in May 1965, the famous
British bridge partnership of Terrence Reese and Boris Schapiro was accused of
cheating by using a system of finger signals that could indicate the number of
hearts held by the players. Reese and Schapiro denied the accusation, and
eventually a hearing was held by the British bridge league. The hearing was in
the form of a legal proceeding with prosecution and defense teams, both having
the power to call and cross-examine witnesses. During the course of the
proceeding, the prosecutor examined specific hands played by Reese and
Schapiro and claimed that their playing these hands was consistent with the
hypothesis that they were guilty of having illicit knowledge of the heart suit. At
this point, the defense attorney pointed out that their play of these hands was
also perfectly consistent with their standard line of play. However, the prosecution
then argued that as long as their play was consistent with the hypothesis of guilt,
it must be counted as evidence toward that hypothesis. What do you think of the
reasoning of the prosecution?
Solution
The problem is basically one of determining how the introduction of new
evidence (in this example, the playing of the hands) affects the probability of a
particular hypothesis. If we let  denote a particular hypothesis (such as the
hypothesis that Reese and Schapiro are guilty) and  the new evidence, then
where P(H) is our evaluation of the likelihood of the hypothesis before the
introduction of the new evidence. The new evidence will be in support of the
hypothesis whenever it makes the hypothesis more likely—that is, whenever
 From Equation (3.2)
, this will be the case whenever
or, equivalently, whenever
In other words, any new evidence can be considered to be in support of a
particular hypothesis only if its occurrence is more likely when the hypothesis is
𝐻
𝐸
𝑃ሺ𝐻||𝐸ሻ
ൌ
𝑃ሺ𝐻𝐸ሻ
𝑃ሺ𝐸ሻ
ൌ
𝑃ሺ𝐸||𝐻ሻ𝑃ሺ𝐻ሻ
𝑃ሺ𝐸||𝐻ሻ𝑃ሺ𝐻ሻ൅𝑃ሺ𝐸||𝐻௖ሻሾ1 െ𝑃ሺ𝐻ሻሿ
(3.2)
𝑃ሺ𝐻||𝐸ሻ൒𝑃ሺ𝐻ሻ.
𝑃ሺ𝐸||𝐻ሻ൒𝑃ሺ𝐸||𝐻ሻ𝑃ሺ𝐻ሻ൅𝑃ሺ𝐸||𝐻௖ሻሾ1 െ𝑃ሺ𝐻ሻሿ
𝑃ሺ𝐸||𝐻ሻ൒𝑃ሺ𝐸||𝐻௖ሻ
116 of 848

true than when it is false. In fact, the new probability of the hypothesis depends
on its initial probability and the ratio of these conditional probabilities, since, from
Equation (3.2)
,
Hence, in the problem under consideration, the play of the cards can be
considered to support the hypothesis of guilt only if such play would have been
more likely if the partnership were cheating than if it were not. As the prosecutor
never made this claim, his assertion that the evidence is in support of the guilt
hypothesis is invalid.
Example 3h
Twins can be either identical or fraternal. Identical, also called monozygotic, twins
form when a single fertilized egg splits into two genetically identical parts.
Consequently, identical twins always have the same set of genes. Fraternal, also
called dizygotic, twins develop when two eggs are fertilized and implant in the
uterus. The genetic connection of fraternal twins is no more or less the same as
siblings born at separate times. A Los Angeles County, California, scientist
wishing to know the current fraction of twin pairs born in the county that are
identical twins has assigned a county statistician to study this issue. The
statistician initially requested each hospital in the county to record all twin births,
indicating whether or not the resulting twins were identical. The hospitals,
however, told her that to determine whether newborn twins were identical was
not a simple task, as it involved the permission of the twins’ parents to perform
complicated and expensive DNA studies that the hospitals could not afford. After
some deliberation, the statistician just asked the hospitals for data listing all twin
births along with an indication as to whether the twins were of the same sex.
When such data indicated that approximately 
 percent of twin births were
same-sexed, the statistician declared that approximately 
 percent of all twins
were identical. How did she come to this conclusion?
Solution
The statistician reasoned that identical twins are always of the same sex,
whereas fraternal twins, having the same relationship to each other as any pair of
siblings, will have probability 
 of being of the same sex. Letting  be the event
that a pair of twins is identical, and 
 be the event that a pair of twins is of the
same sex, she computed the probability 
 by conditioning on whether the
twin pair was identical. This gave
𝑃ሺ𝐻|𝐸ሻൌ
𝑃ሺ𝐻ሻ
𝑃ሺ𝐻ሻ൅ሾ1 െ𝑃ሺ𝐻ሻሿ𝑃ሺ𝐸|𝐻௖ሻ
𝑃ሺ𝐸|𝐻ሻ
64
28
1/2
𝐼
𝑆𝑆
𝑃ሺ𝑆𝑆ሻ
117 of 848

or
which, using that 
 yielded the result
The change in the probability of a hypothesis when new evidence is introduced can
be expressed compactly in terms of the change in the odds of that hypothesis, where
the concept of odds is defined as follows.
Definition
The odds of an event  are defined by
That is, the odds of an event  tell how much more likely it is that the event 
occurs than it is that it does not occur. For instance, if 
 then
 so the odds are 2. If the odds are equal to 
 then it is common
to say that the odds are “  to 1” in favor of the hypothesis.
Consider now a hypothesis  that is true with probability 
 and suppose that new
evidence  is introduced. Then, the conditional probabilities, given the evidence 
that  is true and that  is not true are respectively given by
Therefore, the new odds after the evidence  has been introduced are
That is, the new value of the odds of  is the old value multiplied by the ratio of the
conditional probability of the new evidence given that  is true to the conditional
probability given that  is not true. Thus, Equation (3.3)
 verifies the result of
Example 3f
, since the odds, and thus the probability of 
 increase whenever the
𝑃ሺ𝑆𝑆ሻൌ𝑃ሺ𝑆𝑆||𝐼ሻ𝑃ሺ𝐼ሻ൅𝑃ሺ𝑆𝑆||𝐼௖ሻ𝑃ሺ𝐼௖ሻ
𝑃ሺ𝑆𝑆ሻൌ1 ൈ𝑃ሺ𝐼ሻ൅1
2 ൈሾ1 െ𝑃ሺ𝐼ሻሿൌ1
2 ൅1
2  𝑃ሺ𝐼ሻ
𝑃ሺ𝑆𝑆ሻൎ. 64
𝑃ሺ𝐼ሻൎ. 28
𝐴
𝑃ሺ𝐴ሻ
𝑃ሺ𝐴௖ሻൌ
𝑃ሺ𝐴ሻ
1 െ𝑃ሺ𝐴ሻ
𝐴
𝐴
𝑃ቆ𝐴ቇൌ2
3 ,
𝑃ሺ𝐴ሻൌ2𝑃ሺ𝐴௖ሻ,
𝛼,
𝛼
𝐻
𝑃ሺ𝐻ሻ,
𝐸
𝐸,
𝐻
𝐻
𝑃ሺ𝐻|𝐸ሻൌ𝑃ሺ𝐸||𝐻ሻ𝑃ሺ𝐻ሻ
𝑃ሺ𝐸ሻ
𝑃ሺ𝐻௖|𝐸ሻൌ𝑃ሺ𝐸||𝐻௖ሻ𝑃ሺ𝐻௖ሻ
𝑃ሺ𝐸ሻ
𝐸
𝑃ሺ𝐻||𝐸ሻ
𝑃ሺ𝐻௖||𝐸ሻൌ𝑃ሺ𝐻ሻ
𝑃ሺ𝐻௖ሻ
𝑃ሺ𝐸||𝐻ሻ
𝑃ሺ𝐸||𝐻௖ሻ
(3.3)
𝐻
𝐻
𝐻
𝐻,
118 of 848

new evidence is more likely when  is true than when it is false. Similarly, the odds
decrease whenever the new evidence is more likely when  is false than when it is
true.
Example 3i
An urn contains two type  coins and one type  coin. When a type  coin is
flipped, it comes up heads with probability 
 whereas when a type  coin is
flipped, it comes up heads with probability 
 A coin is randomly chosen from
the urn and flipped. Given that the flip landed on heads, what is the probability
that it was a type  coin?
Solution
Let  be the event that a type  coin was flipped, and let 
 be the event that
a type  coin was flipped. We want 
 where heads is the event that
the flip landed on heads. From Equation (3.3)
, we see that
Hence, the odds are 
 or, equivalently, the probability is 
 that a type 
coin was flipped.
Equation (3.1)
 may be generalized as follows: Suppose that 
 are
mutually exclusive events such that
In other words, exactly one of the events 
 must occur. By writing
and using the fact that the events 
 are mutually exclusive, we obtain
𝐻
𝐻
𝐴
𝐵
𝐴
1/4,
𝐵
3/4.
𝐴
𝐴
𝐴
𝐵ൌ𝐴௖
𝐵
𝑃ሺ𝐴|| headsሻ,
𝑃ሺ𝐴|headsሻ
𝑃ሺ𝐴௖||headsሻ
ൌ
𝑃ሺ𝐴ሻ
𝑃ሺ𝐵ሻ
𝑃ሺ heads|𝐴ሻ
𝑃ሺ heads|𝐵ሻ
ൌ
2/3
1/3   1/4
3/4
ൌ
2/3
2/3:1,
2/5
𝐴
𝐹ଵ, 𝐹ଶ, …, 𝐹௡
∪
௜ൌଵ
௡
𝐹௜ൌ𝑆
𝐹ଵ, 𝐹ଶ, …, 𝐹௡
𝐸ൌ
∪
௜ൌଵ
௡
𝐸𝐹௜
𝐸𝐹௜, 𝑖ൌ1, …, 𝑛
119 of 848

Thus, Equation (3.4)
, often referred to as the law of total probability, shows how,
for given events 
 of which one and only one must occur, we can compute
 by first conditioning on which one of the 
 occurs. That is, Equation (3.4)
states that 
 is equal to a weighted average of 
 each term being
weighted by the probability of the event on which it is conditioned.
Example 3j
In Example 5j
 of Chapter 2
, we considered the probability that, for a
randomly shuffled deck, the card following the first ace is some specified card,
and we gave a combinatorial argument to show that this probability is 
 Here
is a probabilistic argument based on conditioning: Let  be the event that the
card following the first ace is some specified card, say, card . To compute 
we ignore card  and condition on the relative ordering of the other 51 cards in
the deck. Letting  be the ordering gives
Now, given 
 there are 52 possible orderings of the cards, corresponding to
having card  being the th card in the deck, 
 But because all 
possible orderings were initially equally likely, it follows that, conditional on 
each of the 52 remaining possible orderings is equally likely. Because card  will
follow the first ace for only one of these orderings, we have 
implying that 
Again, let 
 be a set of mutually exclusive and exhaustive events (meaning
that exactly one of these events must occur).
Suppose now that  has occurred and we are interested in determining which one of
the 
 also occurred. Then, by Equation (3.4)
, we have the following proposition.
Proposition 3.1
𝑃ሺ𝐸ሻൌ
෍
௜ൌଵ
௡
𝑃ሺ𝐸𝐹௜ሻ
ൌ
෍
௜ൌଵ
௡
𝑃ሺ𝐸|𝐹௜ሻ𝑃ሺ𝐹௜ሻ
(3.4)
𝐹ଵ, 𝐹ଶ, …, 𝐹௡,
𝑃ሺ𝐸ሻ
𝐹௜
𝑃ሺ𝐸ሻ
𝑃ሺ𝐸||𝐹௜ሻ,
1
52 .
𝐸
𝑥
𝑃ሺ𝐸ሻ,
𝑥
𝐎
𝑃ቌ𝐸ቍൌ෍
𝐎
𝑃ሺ𝐸|𝐎ሻ𝑃ሺ𝐎ሻ
𝐎,
𝑥
𝑖
𝑖ൌ1, …,52.
52!
𝐎,
𝑥
 𝑃ሺ𝐸|𝐎ሻൌ1/52,
 𝑃ሺ𝐸ሻൌ1/52.
𝐹ଵ, …,𝐹௡
𝐸
𝐹௝
120 of 848

Equation (3.5)
 is known as Bayes’s formula, after the English philosopher
Thomas Bayes. If we think of the events 
 as being possible “hypotheses” about
some subject matter, then Bayes’s formula may be interpreted as showing us how
opinions about these hypotheses held before the experiment was carried out [that is,
the 
] should be modified by the evidence produced by the experiment.
Example 3k
A plane is missing, and it is presumed that it was equally likely to have gone
down in any of 3 possible regions. Let 
 denote the probability
that the plane will be found upon a search of the th region when the plane is, in
fact, in that region. (The constants 
 are called overlook probabilities, because
they represent the probability of overlooking the plane; they are generally
attributable to the geographical and environmental conditions of the regions.)
What is the conditional probability that the plane is in the th region given that a
search of region 1 is unsuccessful?
Solution
Let 
 be the event that the plane is in region  and let  be the event
that a search of region 1 is unsuccessful. From Bayes’s formula, we obtain
For 
𝑃൫𝐹௝ห𝐸൯
ൌ
𝑃൫𝐸𝐹௝൯
𝑃ሺ𝐸ሻ
ൌ
𝑃൫𝐸ห𝐹௝൯𝑃൫𝐹௝൯
෍
௜ൌଵ
௡
𝑃ሺ𝐸|𝐹௜ሻ𝑃ሺ𝐹௜ሻ
(3.5)
𝐹௝
𝑃൫𝐹௝൯
1 െ𝛽௜, 𝑖ൌ1, 2, 3,
𝑖
𝛽௜
𝑖
𝑅௜, 𝑖ൌ1, 2, 3,
𝑖,
𝐸
𝑃ሺ𝑅ଵ|𝐸ሻ
ൌ
𝑃ሺ𝐸𝑅ଵሻ
𝑃ሺ𝐸ሻ
ൌ
𝑃ሺ𝐸|𝑅ଵሻ𝑃ሺ𝑅ଵሻ
∑௜ൌଵ
ଷ
𝑃ሺ𝐸|𝑅௜ሻ𝑃ሺ𝑅௜ሻ
ൌ
൫𝛽ଵ൯1
3
൫𝛽ଵ൯1
3 ൅ሺ1ሻ1
3 ൅ሺ1ሻ1
3
ൌ
𝛽ଵ
𝛽ଵ൅2
𝑗ൌ2, 3,
121 of 848

Note that the updated (that is, the conditional) probability that the plane is in
region  given the information that a search of region 1 did not find it, is greater
than the initial probability that it was in region  when 
 and is less than the
initial probability when 
 This statement is certainly intuitive, since not finding
the plane in region 1 would seem to decrease its chance of being in that region
and increase its chance of being elsewhere. Further, the conditional probability
that the plane is in region 1 given an unsuccessful search of that region is an
increasing function of the overlook probability 
 This statement is also intuitive,
since the larger 
 is, the more it is reasonable to attribute the unsuccessful
search to “bad luck” as opposed to the plane’s not being there. Similarly,
 is a decreasing function of 
The next example has often been used by unscrupulous probability students to win
money from their less enlightened friends.
Example 3l
Suppose that we have 3 cards that are identical in form, except that both sides of
the first card are colored red, both sides of the second card are colored black,
and one side of the third card is colored red and the other side black. The 3 cards
are mixed up in a hat, and 1 card is randomly selected and put down on the
ground. If the upper side of the chosen card is colored red, what is the probability
that the other side is colored black?
Solution
Let RR, BB, and RB denote, respectively, the events that the chosen card is all
red, all black, or the red–black card. Also, let  be the event that the upturned
side of the chosen card is red. Then, the desired probability is obtained by
𝑃൫𝑅௝ห𝐸൯
ൌ
𝑃൫𝐸ห𝑅௝൯𝑃൫𝑅௝൯
𝑃ሺ𝐸ሻ
ൌ
ቆ1ቇ1
3
ቆ𝛽ଵቇ1
3 ൅1
3 ൅1
3
ൌ
1
𝛽ଵ൅2  𝑗ൌ2, 3
𝑗,
𝑗
𝑗്1
𝑗ൌ1.
𝛽ଵ.
𝛽ଵ
𝑃൫𝑅௝ห𝐸൯, 𝑗്1,
𝛽ଵ.
𝑅
122 of 848

Hence, the answer is 
 Some students guess  as the answer by incorrectly
reasoning that given that a red side appears, there are two equally likely
possibilities: that the card is the all-red card or the red–black card. Their mistake,
however, is in assuming that these two possibilities are equally likely. For, if we
think of each card as consisting of two distinct sides, then we see that there are 6
equally likely outcomes of the experiment—namely, 
—where
the outcome is 
 if the first side of the all-red card is turned face up, 
 if the
second side of the all-red card is turned face up, 
 if the red side of the red–
black card is turned face up, and so on. Since the other side of the upturned red
side will be black only if the outcome is 
 we see that the desired probability is
the conditional probability of 
 given that either 
 or 
 or 
 occurred, which
obviously equals 
Example 3m
A new couple, known to have two children, has just moved into town. Suppose
that the mother is encountered walking with one of her children. If this child is a
girl, what is the probability that both children are girls?
Solution
Let us start by defining the following events:
G : The first (that is, the oldest) child is a girl.
G : The second child is a girl.
G: The child seen with the mother is a girl.
Also, let 
 and  denote similar events, except that “girl” is replaced by
“boy.” Now, the desired probability is 
 which can be expressed as
follows:
𝑃ሺ𝑅𝐵||𝑅ሻ
ൌ
𝑃ሺ𝑅𝐵∩𝑅ሻ
𝑃ሺ𝑅ሻ
ൌ
𝑃ሺ𝑅||𝑅𝐵ሻ𝑃ሺ𝑅𝐵ሻ
𝑃ሺ𝑅||𝑅𝑅ሻ𝑃ሺ𝑅𝑅ሻ൅𝑃ሺ𝑅||𝑅𝐵ሻ𝑃ሺ𝑅𝐵ሻ൅𝑃ሺ𝑅||𝐵𝐵ሻ𝑃ሺ𝐵𝐵ሻ
ൌ
ቆ1
2ቇቆ1
3ቇ
ቆ1ቇቆ1
3ቇ൅ቆ1
2ቇቆ1
3ቇ൅0ቆ1
3ቇ
ൌ1
3
1
3 .
1
2
𝑅ଵ, 𝑅ଶ, 𝐵ଵ, 𝐵ଶ, 𝑅ଷ, 𝐵ଷ
𝑅ଵ
𝑅ଶ
𝑅ଷ
𝑅ଷ,
𝑅ଷ
𝑅ଵ
𝑅ଶ
𝑅ଷ
1
3 .
1
2
𝐵ଵ, 𝐵ଶ,
𝐵
𝑃ሺ𝐺ଵ𝐺ଶ||𝐺ሻ,
123 of 848

Also,
where the final equation used the results 
 and 
 If
we now make the usual assumption that all 4 gender possibilities are equally
likely, then we see that
Thus, the answer depends on whatever assumptions we want to make about the
conditional probabilities that the child seen with the mother is a girl given the
event 
 and that the child seen with the mother is a girl given the event 
For instance, if we want to assume, on the one hand, that, independently of the
genders of the children, the child walking with the mother is the elder child with
some probability  then it would follow that
implying under this scenario that
If, on the other hand, we were to assume that if the children are of different
genders, then the mother would choose to walk with the girl with probability 
independently of the birth order of the children, then we would have
implying that
𝑃ሺ𝐺ଵ𝐺ଶ||𝐺ሻൌ
𝑃ሺ𝐺ଵ𝐺ଶ𝐺ሻ
𝑃ሺ𝐺ሻ
ൌ
𝑃ሺ𝐺ଵ𝐺ଶሻ
𝑃ሺ𝐺ሻ
𝑃ሺ𝐺ሻ
ൌ𝑃ሺ𝐺||𝐺ଵ𝐺ଶሻ𝑃ሺ𝐺ଵ𝐺ଶሻ൅𝑃ሺ𝐺||𝐺ଵ𝐵ଶሻ𝑃ሺ𝐺ଵ𝐵ଶሻ൅𝑃ሺ𝐺||𝐵ଵ𝐺ଶሻ𝑃ሺ𝐵ଵ𝐺ଶሻ൅𝑃ሺ𝐺||𝐵ଵ𝐵ଶሻ𝑃ሺ𝐵
ൌ𝑃ሺ𝐺ଵ𝐺ଶሻ൅𝑃ሺ𝐺||𝐺ଵ𝐵ଶሻ𝑃ሺ𝐺ଵ𝐵ଶሻ൅𝑃ሺ𝐺||𝐵ଵ𝐺ଶሻ𝑃ሺ𝐵ଵ𝐺ଶሻ
𝑃ሺ𝐺||𝐺ଵ𝐺ଶሻൌ1
𝑃ሺ𝐺||𝐵ଵ𝐵ଶሻൌ0.
𝑃ሺ𝐺ଵ𝐺ଶ|𝐺ሻ
ൌ
1
4
1
4 ൅𝑃ሺ𝐺|𝐺ଵ𝐵ଶሻ/4 ൅𝑃ሺ𝐺|𝐵ଵ𝐺ଶሻ/4
ൌ
1
1 ൅𝑃ሺ𝐺|𝐺ଵ𝐵ଶሻ൅𝑃ሺ𝐺|𝐵ଵ𝐺ଶሻ
𝐺ଵ𝐵ଶ
𝐺ଶ𝐵ଵ.
𝑝,
𝑃ሺ𝐺||𝐺ଵ𝐵ଶሻൌ𝑝ൌ1 െ𝑃ሺ𝐺||𝐵ଵ𝐺ଶሻ
𝑃ሺ𝐺ଵ𝐺ଶ|𝐺ሻൌ1
2
𝑞,
𝑃ሺ𝐺||𝐺ଵ𝐵ଶሻൌ𝑃ሺ𝐺||𝐵ଵ𝐺ଶሻൌ𝑞
124 of 848

For instance, if we took 
 meaning that the mother would always choose to
walk with a daughter, then the conditional probability that she has two daughters
would be 
 which is in accord with Example 2b
 because seeing the mother
with a daughter is now equivalent to the event that she has at least one daughter.
Hence, as stated, the problem is incapable of solution. Indeed, even when the
usual assumption about equally likely gender probabilities is made, we still need
to make additional assumptions before a solution can be given. This is because
the sample space of the experiment consists of vectors of the form 
 where
 is the gender of the older child, 
 is the gender of the younger child, and 
identifies the birth order of the child seen with the mother. As a result, to specify
the probabilities of the events of the sample space, it is not enough to make
assumptions only about the genders of the children; it is also necessary to
assume something about the conditional probabilities as to which child is with the
mother given the genders of the children.
Example 3n
A bin contains 3 types of disposable flashlights. The probability that a type 1
flashlight will give more than 100 hours of use is .7, with the corresponding
probabilities for type 2 and type 3 flashlights being .4 and .3, respectively.
Suppose that 20 percent of the flashlights in the bin are type 1, 30 percent are
type 2, and 50 percent are type 3.
a. What is the probability that a randomly chosen flashlight will give more
than 100 hours of use?
b. Given that a flashlight lasted more than 100 hours, what is the conditional
probability that it was a type  flashlight, 
Solution
a. Let  denote the event that the flashlight chosen will give more than 100
hours of use, and let 
 be the event that a type  flashlight is chosen,
 To compute 
 we condition on the type of the flashlight, to
obtain
There is a 41 percent chance that the flashlight will last for more than 100
hours.
𝑃ሺ𝐺ଵ𝐺ଶ|𝐺ሻൌ
1
1 ൅2𝑞
𝑞ൌ1,
1
3 ,
𝑠ଵ, 𝑠ଶ, 𝑖,
𝑠ଵ
𝑠ଶ
𝑖
𝑗
𝑗ൌ1, 2, 3
𝐴
𝐹௝
𝑗
𝑗ൌ1, 2, 3.
𝑃ሺ𝐴ሻ,
𝑃ሺ𝐴ሻൌ
𝑃ሺ𝐴||𝐹ଵሻ𝑃ሺ𝐹ଵሻ൅𝑃ሺ𝐴||𝐹ଶሻ𝑃ሺ𝐹ଶሻ൅𝑃ሺ𝐴||𝐹ଷሻ𝑃ሺ𝐹ଷሻ
ൌሺ. 7ሻሺ. 2ሻ൅ሺ. 4ሻሺ. 3ሻ൅ሺ. 3ሻሺ. 5ሻൌ. 41
125 of 848

b. The probability is obtained by using Bayes’s formula:
Thus,
For instance, whereas the initial probability that a type 1 flashlight is
chosen is only .2, the information that the flashlight has lasted more than
100 hours raises the probability of this event to 
Example 3o
A crime has been committed by a solitary individual, who left some DNA at the
scene of the crime. Forensic scientists who studied the recovered DNA noted
that only five strands could be identified and that each innocent person,
independently, would have a probability of 
 of having his or her DNA match
on all five strands. The district attorney supposes that the perpetrator of the crime
could be any of the 1 million residents of the town. Ten thousand of these
residents have been released from prison within the past 10 years; consequently,
a sample of their DNA is on file. Before any checking of the DNA file, the district
attorney thinks that each of the 10,000 ex-criminals has probability  of being
guilty of the new crime, whereas each of the remaining 990,000 residents has
probability 
 where 
 (That is, the district attorney supposes that each
recently released convict is  times as likely to be the crime’s perpetrator as is
each town member who is not a recently released convict.) When the DNA that is
analyzed is compared against the database of the 10,000 ex-convicts, it turns out
that A. J. Jones is the only one whose DNA matches the profile. Assuming that
the district attorney’s estimate of the relationship between  and  is accurate,
what is the probability that A. J. is guilty?
Solution
To begin, note that because probabilities must sum to 1, we have
Thus,
𝑃൫𝐹௝ห𝐴൯
ൌ
𝑃൫𝐴𝐹௝൯
𝑃ሺ𝐴ሻ
ൌ
𝑃൫𝐴ห𝐹௝൯𝑃൫𝐹௝൯
. 41
𝑃ሺ𝐹ଵ||𝐴ሻൌሺ. 7ሻሺ. 2ሻ/ . 41 ൌ14/41
𝑃ሺ𝐹ଶ||𝐴ሻൌሺ. 4ሻሺ. 3ሻ/ . 41 ൌ12/41
𝑃ሺ𝐹ଷ||𝐴ሻൌሺ. 3ሻሺ. 5ሻ/ . 41 ൌ15/41
14/41 ൎ. 341.
10െହ
𝛼
𝛽,
𝛼ൌ𝑐𝛽.
𝑐
𝛼
𝛽
1 ൌ10, 000𝛼൅990, 000𝛽ൌሺ10, 000𝑐൅990, 000ሻ𝛽
126 of 848

Now, let  be the event that A. J. is guilty, and let 
 denote the event that A. J. is
the only one of the 10,000 on file to have a match. Then,
On the one hand, if A. J. is guilty, then he will be the only one to have a DNA
match if none of the others on file have a match. Therefore,
On the other hand, if A. J. is innocent, then in order for him to be the only match,
his DNA must match (which will occur with probability 
), all others in the
database must be innocent, and none of these others can have a match. Now,
given that A. J. is innocent, the conditional probability that all the others in the
database are also innocent is
Also, the conditional probability, given their innocence, that none of the others in
the database will have a match is 
 Therefore,
Because 
 the preceding formula gives
Thus, if the district attorney’s initial thoughts were that an arbitrary ex-convict was
100 times more likely to have committed the crime than was a nonconvict (that is,
), then 
 and
𝛽ൌ
1
10, 000𝑐൅990, 000,  𝛼ൌ
𝑐
10, 000𝑐൅990, 000
𝐺
𝑀
𝑃ሺ𝐺||𝑀ሻ
ൌ
𝑃ሺ𝐺𝑀ሻ
𝑃ሺ𝑀ሻ
ൌ
𝑃ሺ𝐺ሻ𝑃ሺ𝑀||𝐺ሻ
𝑃ሺ𝑀||𝐺ሻ𝑃ሺ𝐺ሻ൅𝑃ሺ𝑀||𝐺௖ሻ𝑃ሺ𝐺௖ሻ
𝑃൫𝑀ห𝐺൯ൌ൫1 െ10െହ൯
ଽଽଽଽ
10െହ
𝑃ሺ all others innocent|𝐴𝐽 innocentሻ
ൌ
𝑃ሺ all in database innocentሻ
𝑃ሺ𝐴𝐽 innocentሻ
ൌ
1 െ10, 000𝛼
1 െ𝛼
൫1 െ10െହ൯
ଽଽଽଽ.
𝑃ሺ𝑀||𝐺௖ሻൌ
10െହቆ1 െ10, 000𝛼
1 െ𝛼
ቇቆ1 െ10െହቇ
ଽଽଽଽ
𝑃ሺ𝐺ሻൌ𝛼,
𝑃ሺ𝐺||𝑀ሻൌ
𝛼
𝛼൅10െହ൫1 െ10, 000𝛼൯ൌ
1
. 9 ൅10െହ
𝛼
𝑐ൌ100
𝛼ൌ
1
19, 900
127 of 848

If the district attorney initially thought that the appropriate ratio was 
 then
 and
If the district attorney initially thought that the criminal was equally likely to be any
of the members of the town
 then 
 and
Thus, the probability ranges from approximately 9 percent when the district
attorney’s initial assumption is that all the members of the population have the
same chance of being the perpetrator to approximately 91 percent when she
assumes that each ex-convict is 100 times more likely to be the criminal than is a
specified townsperson who is not an ex-convict.
The previous examples in this chapter show that 
 the conditional probability
of  given 
 is not generally equal to 
 the unconditional probability of . In other
words, knowing that  has occurred generally changes the chances of ’s
occurrence. In the special cases where 
 does in fact equal 
 we say that
 is independent of . That is,  is independent of  if knowledge that  has occurred
does not change the probability that  occurs.
Since 
 it follows that  is independent of  if
The fact that Equation (4.1)
 is symmetric in  and  shows that whenever  is
independent of 
 is also independent of . We thus have the following definition.
Definition
Two events  and  are said to be independent if Equation (4.1)
 holds.
Two events  and  that are not independent are said to be dependent.
𝑃ሺ𝐺|𝑀ሻൌ
1
1.099 ൎ0.9099
𝑐ൌ10,
𝛼ൌ
1
109, 000
𝑃ሺ𝐺|𝑀ሻൌ
1
1.99 ൎ0.5025
ሺ𝑐ൌ1ሻ,
𝛼ൌ10െ଺
𝑃ሺ𝐺|𝑀ሻൌ
1
10.9 ൎ0.0917
𝑃ሺ𝐸||𝐹ሻ,
𝐸
𝐹,
𝑃ሺ𝐸ሻ,
𝐸
𝐹
𝐸
𝑃ሺ𝐸||𝐹ሻ
𝑃ሺ𝐸ሻ,
𝐸
𝐹
𝐸
𝐹
𝐹
𝐸
𝑃ሺ𝐸||𝐹ሻൌ𝑃ሺ𝐸𝐹ሻ/𝑃ሺ𝐹ሻ,
𝐸
𝐹
𝑃ሺ𝐸𝐹ሻൌ𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ
(4.1)
𝐸
𝐹
𝐸
𝐹, 𝐹
𝐸
𝐸
𝐹
𝐸
𝐹
128 of 848

Example 4a
A card is selected at random from an ordinary deck of 52 playing cards. If  is the
event that the selected card is an ace and  is the event that it is a spade, then 
and  are independent. This follows because 
 whereas 
and 
Example 4b
Two coins are flipped, and all 4 outcomes are assumed to be equally likely. If  is
the event that the first coin lands on heads and  the event that the second lands
on tails, then  and  are independent, since 
 whereas
 and 
Example 4c
Suppose that we toss 2 fair dice. Let 
 denote the event that the sum of the dice
is 6 and  denote the event that the first die equals 4. Then
whereas
Hence, 
 and  are not independent. Intuitively, the reason for this is clear
because if we are interested in the possibility of throwing a 6 (with 2 dice), we
shall be quite happy if the first die lands on 4 (or, indeed, on any of the numbers
1, 2, 3, 4, and 5), for then we shall still have a possibility of getting a total of 6. If,
however, the first die landed on 6, we would be unhappy because we would no
longer have a chance of getting a total of 6. In other words, our chance of getting
a total of 6 depends on the outcome of the first die; thus, 
 and  cannot be
independent.
Now, suppose that we let 
 be the event that the sum of the dice equals 7. Is 
independent of ? The answer is yes, since
𝐸
𝐹
𝐸
𝐹
𝑃ቆ𝐸𝐹ቇൌ1
52 ,
𝑃ቆ𝐸ቇൌ4
52
𝑃ቆ𝐹ቇൌ13
52 .
𝐸
𝐹
𝐸
𝐹
𝑃ሺ𝐸𝐹ሻൌ𝑃ሺሼሺ𝐻, 𝑇ሻሽሻൌ1
4 ,
𝑃ሺ𝐸ሻൌ𝑃ሺሼሺ𝐻, 𝐻ሻ, ሺ𝐻, 𝑇ሻሽሻൌ1
2
𝑃ሺ𝐹ሻൌ𝑃ሺሼሺ𝐻, 𝑇ሻ, ሺ𝑇, 𝑇ሻሽሻൌ1
2 .
𝐸ଵ
𝐹
𝑃ሺ𝐸ଵ𝐹ሻൌ𝑃ሺሼሺ4, 2ሻሽሻൌ1
36
𝑃ሺ𝐸ଵሻ𝑃ሺ𝐹ሻൌቆ5
36ቇቆ1
6ቇൌ
5
216
𝐸ଵ
𝐹
𝐸ଵ
𝐹
𝐸ଶ
𝐸ଶ
𝐹
𝑃ሺ𝐸ଶ𝐹ሻൌ𝑃ሺሼሺ4, 3ሻሽሻൌ1
36
129 of 848

whereas
We leave it for the reader to present the intuitive argument why the event that the
sum of the dice equals 7 is independent of the outcome on the first die.
Example 4d
If we let  denote the event that the next president is a Republican and  the
event that there will be a major earthquake within the next year, then most people
would probably be willing to assume that  and  are independent. However,
there would probably be some controversy over whether it is reasonable to
assume that  is independent of 
 where  is the event that there will be a
recession within two years after the election.
We now show that if  is independent of 
 then  is also independent of 
Proposition 4.1
If  and  are independent, then so are  and 
Proof. Assume that  and  are independent. Since 
 and EF and
 are obviously mutually exclusive, we have
or, equivalently,
and the result is proved.
Thus, if  is independent of 
 then the probability of ’s occurrence is unchanged by
information as to whether or not  has occurred.
Suppose now that  is independent of  and is also independent of . Is  then
necessarily independent of FG? The answer, somewhat surprisingly, is no, as the
following example demonstrates.
Example 4e
𝑃ሺ𝐸ଶሻ𝑃ሺ𝐹ሻൌቆ1
6ቇቆ1
6ቇൌ1
36
𝐸
𝐹
𝐸
𝐹
𝐸
𝐺,
𝐺
𝐸
𝐹,
𝐸
𝐹௖.
𝐸
𝐹
𝐸
𝐹௖.
𝐸
𝐹
𝐸ൌ𝐸𝐹∪𝐸𝐹௖
𝐸𝐹௖
𝑃ሺ𝐸ሻൌ
𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐸𝐹௖ሻ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸𝐹௖ሻ
𝑃ሺ𝐸𝐹௖ሻൌ
𝑃ሺ𝐸ሻሾ1 െ𝑃ሺ𝐹ሻሿ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹௖ሻ
𝐸
𝐹,
𝐸
𝐹
𝐸
𝐹
𝐺
𝐸
130 of 848

Two fair dice are thrown. Let  denote the event that the sum of the dice is 7. Let
 denote the event that the first die equals 4 and  denote the event that the
second die equals 3. From Example 4c
, we know that  is independent of 
and the same reasoning as applied there shows that  is also independent of ;
but clearly,  is not independent of FG [since 
It would appear to follow from Example 4e
 that an appropriate definition of the
independence of three events 
 and  would have to go further than merely
assuming that all of the 
 pairs of events are independent. We are thus led to the
following definition.
Definition
Three events 
 and  are said to be independent if
Note that if 
 and  are independent, then  will be independent of any event
formed from  and . For instance,  is independent of 
 since
Of course, we may also extend the definition of independence to more than three
events. The events 
 are said to be independent if for every subset
 of these events,
Finally, we define an infinite set of events to be independent if every finite subset of
those events is independent.
Sometimes, a probability experiment under consideration consists of performing a
sequence of subexperiments. For instance, if the experiment consists of continually
tossing a coin, we may think of each toss as being a subexperiment. In many cases,
𝐸
𝐹
𝐺
𝐸
𝐹,
𝐸
𝐺
𝐸
𝑃ሺ𝐸||𝐹𝐺ሻൌ1ሿ.
𝐸, 𝐹,
𝐺
ቆ3
2ቇ
𝐸, 𝐹,
𝐺
𝑃ሺ𝐸𝐹𝐺ሻൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ𝑃ሺ𝐺ሻ
𝑃ሺ𝐸𝐹ሻൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ
𝑃ሺ𝐸𝐺ሻൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐺ሻ
𝑃ሺ𝐹𝐺ሻൌ
𝑃ሺ𝐹ሻ𝑃ሺ𝐺ሻ
𝐸, 𝐹,
𝐺
𝐸
𝐹
𝐺
𝐸
𝐹∪𝐺,
𝑃ሾ𝐸ሺ𝐹∪𝐺ሻሿൌ
𝑃ሺ𝐸𝐹∪𝐸𝐺ሻ
ൌ
𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐸𝐺ሻെ𝑃ሺ𝐸𝐹𝐺ሻ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸ሻ𝑃ሺ𝐺ሻെ𝑃ሺ𝐸ሻ𝑃ሺ𝐹𝐺ሻ
ൌ
𝑃ሺ𝐸ሻሾ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐺ሻെ𝑃ሺ𝐹𝐺ሻሿ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹∪𝐺ሻ
𝐸ଵ, 𝐸ଶ, …, 𝐸௡
𝐸ଵᇱ, 𝐸ଶᇱ, …, 𝐸௥ᇱ, 𝑟൑𝑛
𝑃ሺ𝐸ଵᇱ𝐸ଶᇱ⋯𝐸௥ᇱሻൌ𝑃ሺ𝐸ଵᇱሻ𝑃ሺ𝐸ଶᇱሻ⋯𝑃ሺ𝐸௥ᇱሻ
131 of 848

it is reasonable to assume that the outcomes of any group of the subexperiments
have no effect on the probabilities of the outcomes of the other subexperiments. If
such is the case, we say that the subexperiments are independent. More formally,
we say that the subexperiments are independent if 
 is necessarily an
independent sequence of events whenever 
 is an event whose occurrence is
completely determined by the outcome of the th subexperiment.
If each subexperiment has the same set of possible outcomes, then the
subexperiments are often called trials.
Example 4f
An infinite sequence of independent trials is to be performed. Each trial results in
a success with probability  and a failure with probability 
 What is the
probability that
a. at least 1 success occurs in the first  trials;
b. exactly  successes occur in the first  trials;
c. all trials result in successes?
Solution
In order to determine the probability of at least 1 success in the first  trials, it is
easiest to compute first the probability of the complementary event: that of no
successes in the first  trials. If we let 
 denote the event of a failure on the th
trial, then the probability of no successes is, by independence,
Hence, the answer to part (a) is 
To compute the answer to part (b), consider any particular sequence of the first 
outcomes containing  successes and 
 failures. Each one of these
sequences will, by the assumed independence of trials, occur with probability
 Since there are 
 such sequences [there are 
permutations of  successes and 
 failures], the desired probability in part (b)
is
To answer part (c), we note that, by part (a), the probability of the first  trials all
resulting in success is given by
𝐸ଵ, 𝐸ଶ, …, 𝐸௡, …
𝐸௜
𝑖
𝑝
1 െ𝑝.
𝑛
𝑘
𝑛
𝑛
𝑛
𝐸௜
𝑖
𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡ሻൌ𝑃ሺ𝐸ଵሻ𝑃ሺ𝐸ଶሻ⋯𝑃ሺ𝐸௡ሻൌሺ1 െ𝑝ሻ௡
1 െሺ1 െ𝑝ሻ௡.
𝑛
𝑘
𝑛െ𝑘
𝑝௞൫1 െ𝑝൯
௡െ௞.
ቆ𝑛
𝑘ቇ
𝑛!/𝑘!ሺ𝑛െ𝑘ሻ!
𝑘
𝑛െ𝑘
𝑃ሼexactly 𝑘 successesሽൌቆ𝑛
𝑘ቇ𝑝௞ሺ1 െ𝑝ሻ௡െ௞
𝑛
132 of 848

Thus, using the continuity property of probabilities (Section 2.6
), we see that
the desired probability is given by
Example 4g
A system composed of  separate components is said to be a parallel system if it
functions when at least one of the components functions. (See Figure 3.2
.)
For such a system, if component  which is independent of the other
components, functions with probability 
 what is the probability that
the system functions?
Figure 3.2 Parallel system: functions if current flows from A to B.
Solution
Let 
 denote the event that component  functions. Then,
𝑃ሺ𝐸ଵ
௖𝐸ଶ
௖⋯𝐸௡
௖ሻൌ𝑝௡
𝑃൬
∩
௜ൌଵ
ஶ
𝐸௜
௖൰
ൌ
𝑃൬lim
௡→ஶ
∩
௜ൌଵ
ஶ
𝐸௜
௖൰
ൌ
lim
௡→ஶ𝑃൬
∩
௜ൌଵ
ஶ
𝐸௜
௖൰
ൌ
lim
௡𝑝௡ൌቊ0
if 𝑝൏1
1
if 𝑝ൌ1
𝑛
𝑖,
𝑝௜, 𝑖ൌ1, …, 𝑛,
𝐴௜
𝑖
𝑃ሼsystem functionsሽ
ൌ
1 െ𝑃ሼsystem does not functionሽ
ൌ
1 െ𝑃ሼall components do not functionሽ
ൌ
1 െ𝑃൬∩
௜𝐴௜
௖൰
ൌ
1 െෑ
௜ൌଵ
௜
൫1 െ𝑝௜൯ by independence
133 of 848

Example 4h
Independent trials consisting of rolling a pair of fair dice are performed. What is
the probability that an outcome of 5 appears before an outcome of 7 when the
outcome of a roll is the sum of the dice?
Solution
If we let 
 denote the event that no 5 or 7 appears on the first 
 trials and a
5 appears on the th trial, then the desired probability is
Now, since 
 and 
 we obtain, by the
independence of trials,
Thus,
This result could also have been obtained by the use of conditional probabilities.
If we let  be the event that a 5 occurs before a 7, then we can obtain the desired
probability, 
 by conditioning on the outcome of the first trial, as follows: Let 
be the event that the first trial results in a 5, let  be the event that it results in a
7, and let  be the event that the first trial results in neither a 5 nor a 7. Then,
conditioning on which one of these events occurs gives
However,
𝐸௡
𝑛െ1
𝑛
𝑃൬
∪
௡ൌଵ
ஶ
𝐸௡൰ൌ
෍
௡ൌଵ
ஶ
𝑃൭𝐸௡൱
𝑃ሼ5 on any trialሽൌ4
36
𝑃ሼ7 on any trialሽൌ6
36 ,
𝑃൭𝐸௡൱ൌቆ1 െ10
36ቇ
௡െଵ4
36
𝑃൬
∪
௡ൌଵ
ஶ
𝐸௡൰
ൌ
1
9∑௡ൌଵ
ஶ
ቆ13
18ቇ
௡െଵ
ൌ
1
9
1
1 െ13
18
ൌ
2
5
𝐸
𝑃ሺ𝐸ሻ,
𝐹
𝐺
𝐻
𝑃ሺ𝐸ሻൌ𝑃ሺ𝐸||𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸||𝐺ሻ𝑃ሺ𝐺ሻ൅𝑃ሺ𝐸||𝐻ሻ𝑃ሺ𝐻ሻ
134 of 848

The first two equalities are obvious. The third follows because if the first outcome
results in neither a 5 nor a 7, then at that point the situation is exactly as it was
when the problem first started—namely, the experimenter will continually roll a
pair of fair dice until either a 5 or 7 appears. Furthermore, the trials are
independent; therefore, the outcome of the first trial will have no effect on
subsequent rolls of the dice. Since 
 and 
 it
follows that
or
The reader should note that the answer is quite intuitive. That is, because a 5
occurs on any roll with probability 
 and a 7 with probability 
 it seems
intuitive that the odds that a 5 appears before a 7 should be 6 to 4 against. The
probability should then be 
 as indeed it is.
The same argument shows that if  and  are mutually exclusive events of an
experiment, then, when independent trials of the experiment are performed, the
event  will occur before the event  with probability
Example 4i
Suppose there are  types of coupons and that each new coupon collected is,
independent of previous selections, a type  coupon with probability 
 Suppose  coupons are to be collected. If 
 is the event that there
is at least one type  coupon among those collected, then, for 
 find
a. 
𝑃ሺ𝐸||𝐹ሻ
ൌ
1
𝑃ሺ𝐸||𝐺ሻ
ൌ
0
𝑃ሺ𝐸||𝐻ሻ
ൌ
𝑃ሺ𝐸ሻ
𝑃ቆ𝐹ቇൌ4
36, 𝑃ሺ𝐺ሻൌ6
36 ,
𝑃ቆ𝐻ቇൌ26
36 ,
𝑃ሺ𝐸ሻൌ1
9 ൅𝑃ሺ𝐸ሻ13
18
𝑃ቆ𝐸ቇൌ2
5
4
36
6
36 ,
4
10 ,
𝐸
𝐹
𝐸
𝐹
𝑃ሺ𝐸ሻ
𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻ
𝑛
𝑖
𝑝௜,
෍
௜ൌଵ
௡
𝑝௜ൌ1.
𝑘
𝐴௜
𝑖
𝑖്𝑗,
𝑃ሺ𝐴௜ሻ
135 of 848

b. 
c. 
Solution
where the preceding used that each coupon is, independently, not of type  with
probability 
 Similarly,
where the preceding used that each coupon is, independently, neither of type 
nor type  with probability 
To determine 
 we will use the identity
which, in conjunction with parts (a) and (b), yields
Consequently,
The next example presents a problem that occupies an honored place in the history
of probability theory. This is the famous problem of the points. In general terms, the
problem is this: Two players put up stakes and play some game, with the stakes to
go to the winner of the game. An interruption requires them to stop before either has
won and when each has some sort of a “partial score.” How should the stakes be
𝑃൫𝐴௜∪𝐴௝൯
𝑃൫𝐴௜ห𝐴௝൯
𝑃ሺ𝐴௜ሻ
ൌ1 െ𝑃ሺ𝐴௜
௖ሻ
ൌ1 െ𝑃ሼno coupon is type 𝑖ሽ
ൌ1 െ൫1 െ𝑝௜൯
௞
𝑖
1 െ𝑝௜.
𝑃൫𝐴௜∪𝐴௝൯
ൌ
1 െ𝑃ቀ൫𝐴௜∪𝐴௝൯
௖ቁ
ൌ
1 െ𝑃ሼno coupon is either type 𝑖 or type 𝑗ሽ
ൌ
1 െቀ1 െ𝑝௜െ𝑝௝ቁ
௞
𝑖
𝑗
1 െ𝑝௜െ𝑝௝.
𝑃൫𝐴௜ห𝐴௝൯,
𝑃൫𝐴௜∪𝐴௝൯ൌ𝑃൫𝐴௜൯൅𝑃൫𝐴௝൯െ𝑃൫𝐴௜𝐴௝൯
𝑃൫𝐴௜𝐴௝൯
ൌ
1 െቀ1 െ𝑝௜ቁ
௞
൅1 െቀ1 െ𝑝௝ቁ
௞
െቂ1 െቀ1 െ𝑝௜െ𝑝௝ቁ
௞
ቃ
ൌ
1 െቀ1 െ𝑝௜ቁ
௞
െቀ1 െ𝑝௝ቁ
௞
൅ቀ1 െ𝑝௜െ𝑝௝ቁ
௞
𝑃൫𝐴௜ห𝐴௝൯ൌ𝑃൫𝐴௜𝐴௝൯
𝑃൫𝐴௝൯
ൌ
1 െ൫1 െ𝑝௜൯
௞െቀ1 െ𝑝௝ቁ
௞
൅ቀ1 െ𝑝௜െ𝑝௝ቁ
௞
1 െቀ1 െ𝑝௝ቁ
௞
136 of 848

divided?
This problem was posed to the French mathematician Blaise Pascal in 1654 by the
Chevalier de Méré, who was a professional gambler at that time. In attacking the
problem, Pascal introduced the important idea that the proportion of the prize
deserved by the competitors should depend on their respective probabilities of
winning if the game were to be continued at that point. Pascal worked out some
special cases and, more importantly, initiated a correspondence with the famous
Frenchman Pierre de Fermat, who had a reputation as a great mathematician. The
resulting exchange of letters not only led to a complete solution to the problem of the
points, but also laid the framework for the solution to many other problems
connected with games of chance. This celebrated correspondence, considered by
some as the birth date of probability theory, was also important in stimulating interest
in probability among the mathematicians in Europe, for Pascal and Fermat were both
recognized as being among the foremost mathematicians of the time. For instance,
within a short time of their correspondence, the young Dutch mathematician
Christiaan Huygens came to Paris to discuss these Problems and solutions, and
interest and activity in this new field grew rapidly.
Example 4j The problem of the points
Independent trials resulting in a success with probability  and a failure with
probability 
 are performed. What is the probability that  successes occur
before 
 failures? If we think of  and  as playing a game such that  gains 1
point when a success occurs and  gains 1 point when a failure occurs, then the
desired probability is the probability that  would win if the game were to be
continued in a position where  needed  and  needed 
 more points to win.
Solution
We shall present two solutions. The first comes from Pascal and the second from
Fermat.
Let us denote by 
 the probability that  successes occur before 
 failures.
By conditioning on the outcome of the first trial, we obtain
(Why? Reason it out.) Using the obvious boundary conditions 
we can solve these equations for 
 Rather than go through the tedious
details, let us instead consider Fermat’s solution.
Fermat argued that in order for  successes to occur before 
 failures, it is
necessary and sufficient that there be at least  successes in the first 
𝑝
1 െ𝑝
𝑛
𝑚
𝐴
𝐵
𝐴
𝐵
𝐴
𝐴
𝑛
𝐵
𝑚
𝑃௡,௠
𝑛
𝑚
𝑃௡,௠ൌ𝑝𝑃௡െଵ,௠൅൫1 െ𝑝൯𝑃௡,௠െଵ𝑛൒1, 𝑚൒1
𝑃௡,଴ൌ0, 𝑃଴,௠ൌ1,
𝑃௡,௠.
𝑛
𝑚
𝑛
𝑚൅𝑛െ1
137 of 848

trials. (Even if the game were to end before a total of 
 trials were
completed, we could still imagine that the necessary additional trials were
performed.) This is true, for if there are at least  successes in the first 
trials, there could be at most 
 failures in those 
 trials; thus, 
successes would occur before 
 failures. If, however, there were fewer than 
successes in the first 
 trials, there would have to be at least 
 failures
in that same number of trials; thus,  successes would not occur before 
failures.
Hence, since, as shown in Example 4f
, the probability of exactly  successes
in 
 trials is 
 it follows that the desired
probability of  successes before 
 failures is
The following example gives another instance where determining the probability that
a player wins a match is made easier by assuming that the play continues even after
the match winner has been determined.
Example 4k Service protocol in a serve and rally game
Consider a serve and rally match (such as volleyball, badminton, or squash)
between two players,  and . The match consists of a sequence of rallies, with
each rally beginning with a serve by one of the players and continuing until one
of the players has won the rally. The winner of the rally receives a point, and the
match ends when one of the players has won a total of  points, with that player
being declared the winner of the match. Suppose whenever a rally begins with 
as the server, that  wins that rally with probability 
 and that  wins it with
probability 
 and that a rally that begins with  as the server is won by
 with probability 
 and by  with probability 
 Player  is to be the
initial server. There are two possible server protocols that are under
consideration: “winner serves,” which means that the winner of a rally is the
server for the next rally, or “alternating serve,” which means that the server
alternates from rally to rally, so that no two consecutive rallies have the same
server. Thus, for instance, if 
 then the successive servers under the “winner
serves” protocol would be 
 if  wins the first point, then  the next,
then  wins the next two. On the other hand, the sequence of servers under the
“alternating serve” protocol will always be 
 until the match winner
is decided. If you were player 
 which protocol would you prefer?
𝑚൅𝑛െ1
𝑛
𝑚൅𝑛െ1
𝑚െ1
𝑚൅𝑛െ1
𝑛
𝑚
𝑛
𝑚൅𝑛െ1
𝑚
𝑛
𝑚
𝑘
𝑚൅𝑛െ1
ቆ𝑚൅𝑛െ1
𝑘
ቇ𝑝௞ቆ1 െ𝑝ቇ
௠൅௡െଵെ௞
,
𝑛
𝑚
𝑃௡,௠ൌ
෍
௞ൌ௡
௠൅௡െଵ
ቆ𝑚൅𝑛െ1
𝑘
ቇ𝑝௞ቌ1 െ𝑝ቍ
௠൅௡െଵെ௞
𝐴
𝐵
𝑛
𝐴
𝐴
𝑝஺
𝐵
𝑞஺ൌ1 െ𝑝஺,
𝐵
𝐴
𝑝஻
𝐵
𝑞஻ൌ1 െ𝑝஻.
𝐴
𝑛ൌ3,
𝐴, 𝐴, 𝐵, 𝐴, 𝐴
𝐴
𝐵
𝐴
𝐴, 𝐵, 𝐴, 𝐵, 𝐴, . . .
𝐴,
138 of 848

Solution
Surprisingly, it turns out that it makes no difference, in that the probability that 
is the match winner is the same under either protocol. To show that this is the
case, it is advantageous to suppose that the players continue to play until a total
of 
 rallies have been completed. The first player to win  rallies would then
be the one who has won at least  of the 
 rallies. To begin, note that if the
alternating serve protocol is being used, then player  will serve exactly  times
and player  will serve exactly 
 times in the 
 rallies.
Now consider the winner serve protocol, again assuming that the players
continue to play until 
 rallies have been completed. Because it makes no
difference who serves the “extra rallies” after the match winner has been
decided, suppose that at the point at which the match has been decided
(because one of the players has won  points), the remainder (if there are any) of
the 
 rallies are all served by the player who lost the match. Note that this
modified service protocol does not change the fact that the winner of the match
will still be the player who wins at least  of the 
 rallies. We claim that
under this modified service protocol,  will always serve  times and  will
always serve 
 times. Two cases show this.
Case 1: A wins the match.
Because  serves first, it follows that ’s second serve will immediately follow ’s
first point; ’s third serve will immediately follow ’s second point; and, in
particular, ’s th serve will immediately follow ’s 
 point. But this will be
the last serve of  before the match result is decided. This is so because either 
will win the point on that serve and so have  points, or  will lose the point and
so the serve will switch to, and remain with,  until  wins point number . Thus,
provided that  wins the match, it follows that  would have served a total of 
times at the moment the match is decided. Because, by the modified service
protocol,  will never again serve, it follows in this case that  serves exactly 
times.
Case 2: B wins the match.
Because  serves first, ’s first serve will come immediately after ’s first point; 
’s second serve will come immediately after ’s second point; and, in particular, 
’s 
 serve will come immediately after ’s 
 point. But that will be the
last serve of  before the match is decided because either  will win the point on
that serve and so have  points, or  will lose the point and so the serve will
switch to, and remain with,  until  wins point number . Thus, provided that 
wins the match, we see that  would have served a total of 
 times at the
𝐴
2𝑛െ1
𝑛
𝑛
2𝑛െ1
𝐴
𝑛
𝐵
𝑛െ1
2𝑛െ1
2𝑛െ1
𝑛
2𝑛െ1
𝑛
2𝑛െ1
𝐴
𝑛
𝐵
𝑛െ1
𝐴
𝐴
𝐴
𝐴
𝐴
𝐴
𝑛
𝐴
ሺ𝑛െ1ሻ
𝐴
𝐴
𝑛
𝐴
𝐵
𝐴
𝑛
𝐴
𝐴
𝑛
𝐴
𝐴
𝑛
𝐴
𝐵
𝐵
𝐵
𝐵
𝐵
ሺ𝑛െ1ሻ
𝐵
ሺ𝑛െ1ሻ
𝐵
𝐵
𝑛
𝐵
𝐴
𝐵
𝑛
𝐵
𝐵
𝑛െ1
139 of 848

moment the match is decided. Because, by the modified service protocol,  will
never again serve, it follows in this case that  serves exactly 
 times, and,
as there are a total of 
 rallies, that  serves exactly  times.
Thus, we see that under either protocol,  will always serve  times and  will
serve 
 times and the winner of the match will be the one who wins at least 
points. But since  wins each rally that he serves with probability 
 and wins
each rally that  serves with probability 
 it follows that the probability that  is
the match winner is, under either protocol, equal to the probability that there are
at least  successes in 
 independent trials, when  of these trials result in
a success with probability 
 and the other 
 trials result in a success with
probability 
 Consequently, the win probabilities for both protocols are the
same.
Our next two examples deal with gambling problems, with the first having a
surprisingly elegant analysis.*
*The remainder of this section should be considered optional.
Example 4l
Suppose that initially there are  players, with player  having 
 units,
 At each stage, two of the players are chosen to play a game,
with the winner of the game receiving 1 unit from the loser. Any player whose
fortune drops to 0 is eliminated, and this continues until a single player has all
 units, with that player designated as the victor. Assuming that the
results of successive games are independent and that each game is equally
likely to be won by either of its two players, find 
 the probability that player  is
the victor.
Solution
To begin, suppose that there are  players, with each player initially having 1 unit.
Consider player . Each stage she plays will be equally likely to result in her
either winning or losing 1 unit, with the results from each stage being
independent.
In addition, she will continue to play stages until her fortune becomes either 0 or
. Because this is the same for all  players, it follows that each player has the
same chance of being the victor, implying that each player has probability 
 of
being the victor. Now, suppose these  players are divided into  teams, with
team  containing 
 players, 
 Then, the probability that the victor is a
member of team  is 
 But because
𝐵
𝐵
𝑛െ1
2𝑛െ1
𝐴
𝑛
𝐴
𝑛
𝐵
𝑛െ1
𝑛
𝐴
𝑝஺
𝐵
𝑝஻
𝐴
𝑛
2𝑛െ1
𝑛
𝑝஺
𝑛െ1
𝑝஻.
𝑟
𝑖
𝑛௜
𝑛௜൐0, 𝑖ൌ1, …, 𝑟.
𝑛≡෍
௜ൌଵ
௥
𝑛௜
𝑃௜,
𝑖
𝑛
𝑖
𝑛
𝑛
1/𝑛
𝑛
𝑟
𝑖
𝑛௜
𝑖ൌ1, …, 𝑟.
𝑖
𝑛௜/𝑛.
140 of 848

a. team  initially has a total fortune of 
 units, 
 and
b. each game played by members of different teams is equally likely to be
won by either player and results in the fortune of members of the winning
team increasing by 1 and the fortune of the members of the losing team
decreasing by 1,
it is easy to see that the probability that the victor is from team  is exactly the
probability we desire. Thus, 
 Interestingly, our argument shows that
this result does not depend on how the players in each stage are chosen.
In the gambler’s ruin problem, there are only 2 gamblers, but they are not assumed
to be of equal skill.
Example 4m The gambler’s ruin problem
Two gamblers,  and 
 bet on the outcomes of successive flips of a coin. On
each flip, if the coin comes up heads,  collects 1 unit from 
 whereas if it comes
up tails,  pays 1 unit to . They continue to do this until one of them runs out of
money. If it is assumed that the successive flips of the coin are independent and
each flip results in a head with probability  what is the probability that  ends up
with all the money if he starts with  units and  starts with 
 units?
Solution
Let  denote the event that  ends up with all the money when he starts with 
and  starts with 
 and to make clear the dependence on the initial fortune of
 let 
 We shall obtain an expression for P(E) by conditioning on the
outcome of the first flip as follows: Let  denote the event that the first flip lands
on heads; then
Now, given that the first flip lands on heads, the situation after the first bet is that
 has 
 units and  has 
 Since the successive flips are assumed
to be independent with a common probability  of heads, it follows that from that
point on, ’s probability of winning all the money is exactly the same as if the
game were just starting with  having an initial fortune of 
 and  having an
initial fortune of 
 Therefore,
and similarly,
𝑖
𝑛௜
𝑖ൌ1, …, 𝑟,
𝑖
𝑃௜ൌ𝑛௜/𝑛.
𝐴
𝐵,
𝐴
𝐵,
𝐴
𝐵
𝑝,
𝐴
𝑖
𝐵
𝑁െ𝑖
𝐸
𝐴
𝑖
𝐵
𝑁െ𝑖,
𝐴,
𝑃௜ൌ𝑃ሺ𝐸ሻ.
𝐻
𝑃௜ൌ𝑃ሺ𝐸ሻ
ൌ
𝑃ሺ𝐸||𝐻ሻ𝑃ሺ𝐻ሻ൅𝑃ሺ𝐸||𝐻௖ሻ𝑃ሺ𝐻௖ሻ
ൌ
𝑝𝑃ሺ𝐸||𝐻ሻ൅ሺ1 െ𝑝ሻ𝑃ሺ𝐸||𝐻௖ሻ
𝐴
𝑖൅1
𝐵
𝑁െሺ𝑖൅1ሻ.
𝑝
𝐴
𝐴
𝑖൅1
𝐵
𝑁െሺ𝑖൅1ሻ.
𝑃ሺ𝐸||𝐻ሻൌ𝑃௜൅ଵ
141 of 848

Hence, letting 
 we obtain
By making use of the obvious boundary conditions 
 and 
 we shall
now solve Equation (4.2)
. Since 
 these equations are equivalent to
or
Hence, since 
 we obtain, from Equation (4.3)
,
Adding the first 
 equations of (4.4
) yields
𝑃ሺ𝐸||𝐻௖ሻൌ𝑃௜െଵ
𝑞ൌ1 െ𝑝,
𝑃௜ൌ𝑝𝑃௜൅ଵ൅𝑞𝑃௜െଵ𝑖ൌ1, 2, …, 𝑁െ1
(4.2)
𝑃଴ൌ0
𝑃ேൌ1,
𝑝൅𝑞ൌ1,
𝑝𝑃௜൅𝑞𝑃௜ൌ𝑝𝑃௜൅ଵ൅𝑞𝑃௜െଵ
𝑃௜൅ଵെ𝑃௜ൌ𝑞
𝑝ቆ𝑃௜െ𝑃௜െଵቇ𝑖ൌ1, 2, …, 𝑁െ1
(4.3)
𝑃଴ൌ0,
𝑃ଶെ𝑃ଵ
ൌ
𝑞
𝑝ቆ𝑃ଵെ𝑃଴ቇൌ𝑞
𝑝𝑃ଵ
𝑃ଷെ𝑃ଶ
ൌ
𝑞
𝑝൭𝑃ଶെ𝑃ଵ൱ൌቆ𝑞
𝑝ቇ
ଶ
𝑃ଵ
.
.
.
𝑃௜െ𝑃௜െଵ
ൌ
𝑞
𝑝൭𝑃௜െଵെ𝑃௜െଶ൱ൌቆ𝑞
𝑝ቇ
௜െଵ
𝑃ଵ
.
.
.
𝑃ேെ𝑃ேെଵ
ൌ
𝑞
𝑝൭𝑃ேെଵെ𝑃ேെଶ൱ൌቆ𝑞
𝑝ቇ
ேെଵ
𝑃ଵ
(4.4)
𝑖െ1
𝑃௜െ𝑃ଵൌ𝑃ଵ൥ቆ𝑞
𝑝ቇ൅ቆ𝑞
𝑝ቇ
ଶ
൅⋯൅ቆ𝑞
𝑝ቇ
௜െଵ
൩
142 of 848

or
Using the fact that 
 we obtain
Hence,
Let 
 denote the probability that  winds up with all the money when  starts
with  and  starts with 
 Then, by symmetry to the situation described, and
on replacing  by  and  by 
 it follows that
Moreover, since 
 is equivalent to 
 we have, when 
𝑃௜ൌ
⎧
⎨
⎩
⎪
⎪
1 െሺ𝑞/𝑝ሻ௜
1 െሺ𝑞/𝑝ሻ𝑃ଵ
if  𝑞
𝑝്1
𝑖𝑃ଵ
if  𝑞
𝑝ൌ1
𝑃ேൌ1,
𝑃ଵൌ
⎧
⎨
⎩
⎪
⎪
1 െሺ𝑞/𝑝ሻ
1 െሺ𝑞/𝑝ሻே
if 𝑝്1
2
1
𝑁 
if 𝑝ൌ1
2
𝑃௜ൌ
⎧
⎨
⎩
⎪
⎪
1 െሺ𝑞/𝑝ሻ௜
1 െሺ𝑞/𝑝ሻே
if 𝑝്1
2
𝑖
𝑁
if 𝑝ൌ1
2
(4.5)
𝑄௜
𝐵
𝐴
𝑖
𝐵
𝑁െ𝑖.
𝑝
𝑞
𝑖
𝑁െ𝑖,
𝑄௜ൌ
⎧
⎨
⎩
⎪
⎪
1 െሺ𝑝/𝑞ሻேെ௜
1 െሺ𝑝/𝑞ሻே
if 𝑞്1
2
𝑁െ𝑖
𝑁
if 𝑞ൌ1
2
𝑞ൌ1
2
𝑝ൌ1
2 ,
𝑞്1
2 ,
𝑃௜൅𝑄௜
ൌ
1 െሺ𝑞/𝑝ሻ௜
1 െሺ𝑞/𝑝ሻே൅1 െሺ𝑝/𝑞ሻேെ௜
1 െሺ𝑝/𝑞ሻே
ൌ
𝑝ேെ𝑝ேሺ𝑞/𝑝ሻ
௜
𝑝ேെ𝑞ே
൅𝑞ேെ𝑞ேሺ𝑝/𝑞ሻ
ேെ௜
𝑞ேെ𝑝ே
ൌ
𝑝ேെ𝑝ேെ௜𝑞௜െ𝑞ே൅𝑞௜𝑝ேെ௜
𝑝ேെ𝑞ே
ൌ
1
143 of 848

This result also holds when 
 so
In words, this equation states that with probability 1, either  or  will wind up
with all of the money; in other words, the probability that the game continues
indefinitely with ’s fortune always being between 1 and 
 is zero. (The
reader must be careful because, a priori, there are three possible outcomes of
this gambling game, not two: Either  wins, or  wins, or the game goes on
forever with nobody winning. We have just shown that this last event has
probability 0.)
As a numerical illustration of the preceding result, if  were to start with 5 units
and  with 10, then the probability of ’s winning would be  if  were 
whereas it would jump to
if  were .6.
A special case of the gambler’s ruin problem, which is also known as the
problem of duration of play, was proposed to Huygens by Fermat in 1657. The
version Huygens proposed, which he himself solved, was that  and  have 12
coins each. They play for these coins in a game with 3 dice as follows: Whenever
11 is thrown (by either—it makes no difference who rolls the dice),  gives a coin
to . Whenever 14 is thrown,  gives a coin to . The person who first wins all
the coins wins the game. Since 
 and 
 we see
from Example 4h
 that, for 
 this is just the gambler’s ruin problem with
 and 
 The general form of the gambler’s ruin problem was
solved by the mathematician James Bernoulli and published 8 years after his
death in 1713.
For an Application of the gambler’s ruin problem to drug testing, suppose that
two new drugs have been developed for treating a certain disease. Drug  has a
cure rate 
 in the sense that each patient treated with drug  will be
cured with probability 
 These cure rates are, however, not known, and we are
interested in finding a method for deciding whether 
 or 
 To decide
𝑝ൌ𝑞ൌ1
2 ,
𝑃௜൅𝑄௜ൌ1
𝐴
𝐵
𝐴
𝑁െ1
𝐴
𝐵
𝐴
𝐵
𝐴
1
3
𝑝
1
2 ,
1 െቆ2
3ቇ
ହ
1 െቆ2
3ቇ
ଵହൎ. 87
𝑝
𝐴
𝐵
𝐴
𝐵
𝐵
𝐴
𝑃ሼroll 11ሽൌ27
216
𝑃ሼroll 14ሽൌ15
216 ,
𝐴,
𝑝ൌ15
42, 𝑖ൌ12,
𝑁ൌ24.
𝑖
𝑝௜, 𝑖ൌ1, 2,
𝑖
𝑝௜.
𝑝ଵ൐𝑝ଶ
𝑝ଶ൐𝑝ଵ.
144 of 848

on one of these alternatives, consider the following test: Pairs of patients are to
be treated sequentially, with one member of the pair receiving drug 1 and the
other drug 2. The results for each pair are determined, and the testing stops
when the cumulative number of cures from one of the drugs exceeds the
cumulative number of cures from the other by some fixed, predetermined
number. More formally, let
For a predetermined positive integer 
 the test stops after pair 
 where  is the
first value of  such that either
or
In the former case, we assert that 
 and in the latter that 
In order to help ascertain whether the foregoing is a good test, one thing we
would like to know is the probability that it leads to an incorrect decision. That is,
for given 
 and 
 where 
 what is the probability that the test will
incorrectly assert that 
? To determine this probability, note that after each
pair is checked, the cumulative difference of cures using drug 1 versus drug 2 will
go up by 1 with probability 
—since this is the probability that drug 1
leads to a cure and drug 2 does not—or go down by 1 with probability 
or remain the same with probability 
 Hence, if we
consider only those pairs in which the cumulative difference changes, then the
difference will go up by 1 with probability
and down by 1 with probability
𝑋௝ൌቊ1
if the patient in the 𝑗th pair that receives drug 1 is cured
0
otherwise
𝑌௝ൌቊ1
if the patient in the 𝑗th pair that receives drug 2 is cured
0
otherwise
𝑀,
𝑁,
𝑁
𝑛
𝑋ଵ൅⋯൅𝑋௡െሺ𝑌ଵ൅⋯൅𝑌௡ሻൌ𝑀
𝑋ଵ൅⋯൅𝑋௡െሺ𝑌ଵ൅⋯൅𝑌௡ሻൌെ𝑀
𝑝ଵ൐𝑝ଶ
𝑝ଶ൐𝑝ଵ.
𝑝ଵ
𝑝ଶ,
𝑝ଵ൐𝑝ଶ,
𝑝ଶ൐𝑝ଵ
𝑝ଵ൫1 െ𝑝ଶ൯
൫1 െ𝑝ଵ൯𝑝ଶ,
𝑝ଵ𝑝ଶ൅൫1 െ𝑝ଵ൯൫1 െ𝑝ଶ൯.
𝑝
ൌ
𝑃ሼup 1 || up 1 or down 1ሽ
ൌ
𝑝ଵ൫1 െ𝑝ଶ൯
𝑝ଵ൫1 െ𝑝ଶ൯൅൫1 െ𝑝ଵ൯𝑝ଶ
1 െ𝑝ൌ
𝑝ଶ൫1 െ𝑝ଵ൯
𝑝ଵ൫1 െ𝑝ଶ൯൅൫1 െ𝑝ଵ൯𝑝ଶ
145 of 848

Thus, the probability that the test will assert that 
 is equal to the
probability that a gambler who wins each (one-unit) bet with probability  will go
down 
 before going up 
. But Equation (4.5)
, with 
 shows
that this probability is given by
where
For instance, if 
 and 
 then the probability of an incorrect decision
is .017 when 
 and reduces to .0003 when 
Example 4n
A total of 
 teams are selected to play in the end of season NCAA college
basketball tournament. These 
 are divided into four groups, called brackets, of
size 
 each, with the teams in each bracket being given seedings ranging from
 (the top rated team in the bracket) to 
 (the lowest rated team in the bracket).
The teams in each bracket play each other in a knockout style tournament,
meaning a loss knocks a team out of the tournament. Naming a team by its
seeding, the schedule of games to be played by the teams in a bracket is as
given by the following graph:
Figure 3.3 Ncaa tournament bracket format
𝑝ଶ൐𝑝ଵ
𝑝
𝑀
𝑀
𝑖ൌ𝑀, 𝑁ൌ2𝑀,
𝑝ൌ
𝑃൛test asserts that 𝑝ଶ൐𝑝ଵൟ
ൌ
1 െ
1 െቆ1 െ𝑝
𝑝
ቇ
ெ
1 െቆ1 െ𝑝
𝑝
ቇ
ଶெ
ൌ
1 െ
1
1 ൅ቆ1 െ𝑝
𝑝
ቇ
ெ
ൌ
1
1 ൅𝛾ெ
𝛾ൌ
𝑝
1 െ𝑝ൌ
𝑝ଵ൫1 െ𝑝ଶ൯
𝑝ଶ൫1 െ𝑝ଵ൯
𝑝ଵൌ. 6
𝑝ଶൌ. 4,
𝑀ൌ5
𝑀ൌ10.
64
64
16
1
16
146 of 848

Thus, for instance, teams 1 and 16 play a game in round one, as do teams  and
 with the winners of these games then playing each other in round two. Let
 denote the round in which  and  would play if both teams
win up to that point. That is, 
 if  and  would play in round  if each won
8
9,
𝑟ሺ𝑖, 𝑗ሻൌ𝑟ሺ𝑗, 𝑖ሻ, 𝑖്𝑗
𝑖
𝑗
𝑟ሺ𝑖, 𝑗ሻൌ𝑘
𝑖
𝑗
𝑘
147 of 848

its first 
 games. For instance, 
Let us focus on a single one of the brackets, and let us suppose that, no matter
what has previously occurred, if  and  ever play each other then  will win with
probability 
 Let 
 be the probability that team  is the winner of the
bracket, 
 Because 
 is the probability that  wins  games, we will
compute the values 
 by determining the quantities 
where 
 is defined to be the probability that  wins its first  games. The
probabilities 
 will be determined recursively, first for 
 then for 
then for 
 and finally for 
 which will yield 
Let 
 be the set of possible opponents of  in round . To
compute 
 we will condition on which of the teams in 
 reaches round .
Because a team will reach round  if that team wins its first 
 games, this
gives
Now, because any team that plays a team in 
 in any of rounds 
 is
a possible opponent of team  in round  it follows that all games in rounds
 involving a team in 
 will be against another team in that set, and
thus the results of these games do not affect which teams  would play in its first
 games. Consequently, whether team  reaches round  is independent of
which team in 
 reaches round . Hence, for 
where the next to last equality follows because whether  wins its first 
games is independent of the event that  wins its first 
 games. Hence, from
(4.6
) and the preceding equation, we have that
𝑘െ1
𝑟ሺ1, 16ሻൌ1, 𝑟ሺ1, 8ሻൌ2, 𝑟ሺ1, 5ሻൌ3,
𝑟ሺ1, 6ሻൌ4.
𝑖
𝑗
𝑖
𝑝௜,௝ൌ1 െ𝑝௝,௜.
𝑃௜
𝑖
𝑖ൌ1, …, 16.
𝑃௜
𝑖
4
𝑃ଵ, …, 𝑃ଵ଺
𝑃௜ሺ𝑘ሻ, 𝑖ൌ1, …, 16,
𝑃௜ሺ𝑘ሻ
𝑖
𝑘
𝑃௜ሺ𝑘ሻ
𝑘ൌ1,
𝑘ൌ2,
𝑘ൌ3,
𝑘ൌ4
𝑃௜ൌ𝑃௜ሺ4ሻ.
𝑂௜ሺ𝑘ሻൌሼ𝑗:ሺ𝑟ሺ𝑖, 𝑗ሻൌ𝑘ሽ
𝑖
𝑘
𝑃௜ሺ𝑘ሻ,
𝑂௜ሺ𝑘ሻ
𝑘
𝑘
𝑘െ1
𝑃௜൮𝑘൲ൌ
෍
௝∈ை೔ሺ௞ሻ
𝑃൮𝑖 win its ϐirst 𝑘 games|𝑗reaches round 𝑘൲ 𝑃௝൮𝑘െ1൲
(4.6)
𝑂௜ሺ𝑘ሻ
1, …, 𝑘െ1
𝑖
𝑘,
1, …, 𝑘െ1
𝑂௜ሺ𝑘ሻ
𝑖
𝑘െ1
𝑖
𝑘
𝑂௜ሺ𝑘ሻ
𝑘
𝑗∈𝑂௜ሺ𝑘ሻ
𝑃ሺ𝑖 win its ϐirst 𝑘 games|𝑗 reaches round 𝑘ሻ
ൌ
𝑃ሺ𝑖 win its ϐirst 𝑘െ1 games, 𝑖 beats 𝑗|𝑗 reaches round 𝑘ሻ
ൌ
𝑃ሺ𝑖 win its ϐirst 𝑘െ1 gamesሻ𝑃ሺ𝑖 beats 𝑗|𝑖 and 𝑗 reach round 𝑘ሻ
ൌ
𝑃௜ቀ𝑘െ1ቁ 𝑝௜,௝
𝑖
𝑘െ1
𝑗
𝑘െ1
148 of 848

Starting with 
 the preceding enables us to determine 
 for all 
which then enables us to determine 
 for all  and so on, up to 
To indicate how to apply the recursive equations (4.7)
, suppose that
 Thus, for instance, the probability that team  (the second seed)
beats team  (the seventh seed) is 
 To compute, 
 the
probability that  wins the bracket, start with the quantities 
equal to the probabilities that each team wins its first game.
The quantities 
 are then obtained by using the preceding along with the
recursion (4.7
). For instance, because the set of possible opponents of team 
in round  is 
 we have that
The other quantities 
 are obtained similarly, and are used to obtain
the quantities 
 which are then used to obtain 
𝑃௜ሺ𝑘ሻൌ
෍
௝∈ை೔ሺ௞ሻ
𝑃௜൮𝑘െ1൲ 𝑝௜,௝ 𝑃௝൮𝑘െ1൲
ൌ
𝑃௜൮𝑘െ1൲
෍
௝∈ை೔ሺ௞ሻ
𝑃௝൮𝑘െ1൲ 𝑝௜,௝
(4.7)
𝑃௜ሺ0ሻൌ1,
𝑃௜ሺ1ሻ
𝑖,
𝑃௜ሺ2ሻ
𝑖,
𝑃௜ൌ𝑃௜ሺ4ሻ.
𝑝௜,௝ൌ
𝑗
𝑖൅𝑗.
2
7
𝑝ଶ,଻ൌ7/9.
𝑃௜ൌ𝑃௜ሺ4ሻ,
𝑖
𝑃௜ሺ1ሻ, 𝑖ൌ1, …, 16,
𝑃ଵሺ1ሻൌ
𝑝ଵ,ଵ଺ൌ16/17 ൌ1 െ𝑃ଵ଺ቀ1ቁ
𝑃ଶሺ1ሻൌ
𝑝ଶ,ଵହൌ15/17 ൌ1 െ𝑃ଵହቀ1ቁ
𝑃ଷሺ1ሻൌ
𝑝ଷ,ଵସൌ14/17 ൌ1 െ𝑃ଵସቀ1ቁ
𝑃ସሺ1ሻൌ
𝑝ସ,ଵଷൌ13/17 ൌ1 െ𝑃ଵଷቀ1ቁ
𝑃ହሺ1ሻൌ
𝑝ହ,ଵଶൌ12/17 ൌ1 െ𝑃ଵଶቀ1ቁ
𝑃଺ሺ1ሻൌ
𝑝଺,ଵଵൌ11/17 ൌ1 െ𝑃ଵଵቀ1ቁ
𝑃଻ሺ1ሻൌ
𝑝଻,ଵ଴ൌ10/17 ൌ1 െ𝑃ଵ଴ቀ1ቁ
𝑃଼ሺ1ሻൌ
𝑝଼,ଽൌ9/17  ൌ1 െ𝑃ଽቀ1ቁ
𝑃௜ሺ2ሻ
1
2
𝑂ଵሺ2ሻൌሼ8, 9ሽ,
𝑃ଵቆ2ቇൌ𝑃ଵቆ1ቇቀ𝑃଼ቀ1ቁ𝑝ଵ,଼൅𝑃ଽቀ1ቁ𝑝ଵ,ଽቁൌ16
17 ቆ9
17
8
9 ൅8
17
9
10ቇൎ. 8415
𝑃௜ሺ2ሻ, …, 𝑃ଵ଺ሺ2ሻ
𝑃௜ሺ3ሻ, 𝑖ൌ1, …, 16,
𝑃௜ൌ𝑃௜ሺ4ሻ,
𝑖ൌ1, …, 16 .
149 of 848

Suppose that we are presented with a set of elements and we want to determine
whether at least one member of the set has a certain property. We can attack this
question probabilistically by randomly choosing an element of the set in such a way
that each element has a positive probability of being selected. Then the original
question can be answered by a consideration of the probability that the randomly
selected element does not have the property of interest. If this probability is equal to
 then none of the elements of the set has the property; if it is less than  then at
least one element of the set has the property.
The final example of this section illustrates this technique.
Example 4o
The complete graph having  vertices is defined to be a set of  points (called
vertices) in the plane and the 
 lines (called edges) connecting each pair of
vertices. The complete graph having 3 vertices is shown in Figure 3.4
.
Suppose now that each edge in a complete graph having  vertices is to be
colored either red or blue. For a fixed integer  a question of interest is, Is there
a way of coloring the edges so that no set of  vertices has all of its 
connecting edges the same color? It can be shown by a probabilistic argument
that if  is not too large, then the answer is yes.
Figure 3.4:
The argument runs as follows: Suppose that each edge is, independently, equally
likely to be colored either red or blue. That is, each edge is red with probability 
Number the 
 sets of  vertices and define the events 
 as
follows:
1,
1,
𝑛
𝑛
ቆ𝑛
2ቇ
𝑛
𝑘,
𝑘
ቆ𝑘
2ቇ
𝑛
1
2 .
ቆ𝑛
𝑘ቇ
𝑘
𝐸௜, 𝑖ൌ1, …, ቆ𝑛
𝑘ቇ
𝐸௜ൌሼall of the connecting edges of the 𝑖th set of 𝑘 vertices are the same colorሽ
150 of 848

Now, since each of the 
 connecting edges of a set of  vertices is equally
likely to be either red or blue, it follows that the probability that they are all the
same color is
Therefore, because
we find that 
 the probability that there is a set of  vertices all of whose
connecting edges are similarly colored, satisfies
Hence, if
or, equivalently, if
then the probability that at least one of the 
 sets of  vertices has all of its
connecting edges the same color is less than 1. Consequently, under the
preceding condition on  and  it follows that there is a positive probability that
no set of  vertices has all of its connecting edges the same color. But this
conclusion implies that there is at least one way of coloring the edges for which
no set of  vertices has all of its connecting edges the same color.
Remarks
a. Whereas the preceding argument established a condition on  and  that
guarantees the existence of a coloring scheme satisfying the desired property,
ቆ𝑘
2ቇ
𝑘
𝑃ቌ𝐸௜ቍൌ2ቆ1
2ቇ
௞൫௞െଵ൯/ଶ
𝑃൬∪
௜𝐸௜൰൑෍
௜
𝑃ሺ𝐸௜ሻ ቌ Boole′s inequalityቍ
𝑃൬∪
௜𝐸௜൰,
𝑘
𝑃൬∪
௜𝐸௜൰൑ቆ𝑛
𝑘ቇቆ1
2ቇ
௞൫௞െଵ൯/ଶെଵ
ቆ𝑛
𝑘ቇቆ1
2ቇ
௞൫௞െଵ൯/ଶെଵ
൏1
ቆ𝑛
𝑘ቇ൏2௞൫௞െଵ൯/ଶെଵ
ቆ𝑛
𝑘ቇ
𝑘
𝑛
𝑘,
𝑘
𝑘
𝑛
𝑘
151 of 848

it gives no information about how to obtain such a scheme (although one
possibility would be simply to choose the colors at random, check to see if the
resulting coloring satisfies the property, and repeat the procedure until it
does).
b. The method of introducing probability into a problem whose statement is
purely deterministic has been called the probabilistic method.† Other
examples of this method are given in Theoretical Exercise 24
 and
Examples 2t and 2u of Chapter 7
.
†See N. Alon, J. Spencer, and P. Erdos, The Probabilistic Method (New York: John
Wiley & Sons, Inc., 1992).
Conditional probabilities satisfy all of the properties of ordinary probabilities, as is
proved by Proposition 5.1
, which shows that 
 satisfies the three axioms of
a probability.
Proposition 5.1
a. 
b. 
c. If 
are mutually exclusive events, then
Proof. To prove part (a), we must show that 
 The left-side
inequality is obvious, whereas the right side follows because 
 which
implies that 
 Part (b) follows because
Part (c) follows from
𝑃ሺ𝐸||𝐹ሻ
0 ൑𝑃ሺ𝐸||𝐹ሻ൑1.
𝑃ሺ𝑆||𝐹ሻൌ1.
𝐸௜, 𝑖ൌ1, 2, …,
𝑃൬
∪
௜ൌଵ
ஶ
𝐸௜ฬ𝐹൰ൌ෍
௜ൌଵ
ஶ
𝑃ቌ𝐸௜ቮ𝐹ቍ
0 ൑𝑃ሺ𝐸𝐹ሻ/𝑃ሺ𝐹ሻ൑1.
𝐸𝐹⊂𝐹,
𝑃ሺ𝐸𝐹ሻ൑𝑃ሺ𝐹ሻ.
𝑃ሺ𝑆|𝐹ሻൌ𝑃ሺ𝑆𝐹ሻ
𝑃ሺ𝐹ሻൌ𝑃ሺ𝐹ሻ
𝑃ሺ𝐹ሻൌ1
152 of 848

where the next-to-last equality follows because 
 implies that
If we define 
 then, from Proposition 5.1
, Q(E) may be regarded
as a probability function on the events of . Hence, all of the propositions previously
proved for probabilities apply to 
 For instance, we have
or, equivalently,
Also, if we define the conditional probability 
 by
 then, from Equation (3.1)
, we have
Since
𝑃൬
∪
௜ൌଵ
ஶ
𝐸௜|𝐹൰
ൌ
𝑃൬൬
௜ൌଵ
ஶ
𝐸௜൰𝐹൰
𝑃ሺ𝐹ሻ
ൌ
𝑃൬
ଵ
ஶ𝐸௜𝐹൰
𝑃ሺ𝐹ሻ
 since ൬∪
ଵ
ஶ𝐸௜൰𝐹ൌ∪
ଵ
ஶ𝐸௜𝐹
ൌ
෍
ଵ
ஶ
𝑃ሺ𝐸௜𝐹ሻ
𝑃ሺ𝐹ሻ
ൌ
෍
ଵ
ஶ
𝑃ሺ𝐸௜|𝐹ሻ
𝐸௜𝐸௝ൌ Ø 
𝐸௜𝐹𝐸௝𝐹ൌ Ø .
𝑄ሺ𝐸ሻൌ𝑃ሺ𝐸||𝐹ሻ,
𝑆
𝑄ሺ𝐸ሻ.
𝑄ሺ𝐸ଵ∪𝐸ଶሻൌ𝑄ሺ𝐸ଵሻ൅𝑄ሺ𝐸ଶሻെ𝑄ሺ𝐸ଵ𝐸ଶሻ
𝑃ሺ𝐸ଵ∪𝐸ଶ||𝐹ሻൌ𝑃ሺ𝐸ଵ||𝐹ሻ൅𝑃ሺ𝐸ଶ||𝐹ሻെ𝑃ሺ𝐸ଵ𝐸ଶ||𝐹ሻ
𝑄ሺ𝐸ଵ||𝐸ଶሻ
𝑄ሺ𝐸ଵ||𝐸ଶሻൌ𝑄ሺ𝐸ଵ𝐸ଶሻ/𝑄ሺ𝐸ଶሻ,
𝑄ሺ𝐸ଵሻൌ𝑄ሺ𝐸ଵ||𝐸ଶሻ𝑄ሺ𝐸ଶሻ൅𝑄ሺ𝐸ଵ||𝐸ଶ
௖ሻ𝑄ሺ𝐸ଶ
௖ሻ
(5.1)
153 of 848

Equation (5.1)
 is equivalent to
Example 5a
Consider Example 3a
, which is concerned with an insurance company that
believes that people can be divided into two distinct classes: those who are
accident prone and those who are not. During any given year, an accident-prone
person will have an accident with probability .4, whereas the corresponding figure
for a person who is not prone to accidents is .2. What is the conditional
probability that a new policyholder will have an accident in his or her second year
of policy ownership, given that the policyholder has had an accident in the first
year?
Solution
If we let  be the event that the policyholder is accident prone and we let
 be the event that he or she has had an accident in the th year, then
the desired probability 
 may be obtained by conditioning on whether or
not the policyholder is accident prone, as follows:
Now,
However, 
 is assumed to equal 
 and it was shown in Example 3a
 that
 Hence,
𝑄ሺ𝐸ଵ|𝐸ଶሻ
ൌ
𝑄ሺ𝐸ଵ𝐸ଶሻ
𝑄ሺ𝐸ଶሻ
ൌ
𝑃ሺ𝐸ଵ𝐸ଶ|𝐹ሻ
𝑃ሺ𝐸ଶ|𝐹ሻ
ൌ
𝑃ሺ𝐸ଵ𝐸ଶ𝐹ሻ
𝑃ሺ𝐹ሻ
𝑃ሺ𝐸ଶ𝐹ሻ
𝑃ሺ𝐹ሻ
ൌ
𝑃ሺ𝐸ଵ|𝐸ଶ𝐹ሻ
𝑃ሺ𝐸ଵ||𝐹ሻൌ𝑃ሺ𝐸ଵ||𝐸ଶ𝐹ሻ𝑃ሺ𝐸ଶ||𝐹ሻ൅𝑃ሺ𝐸ଵ||𝐸ଶ
௖𝐹ሻ𝑃ሺ𝐸ଶ
௖||𝐹ሻ
𝐴
𝐴௜, 𝑖ൌ1, 2,
𝑖
𝑃ሺ𝐴ଶ||𝐴ଵሻ
𝑃ሺ𝐴ଶ||𝐴ଵሻൌ𝑃ሺ𝐴ଶ||𝐴𝐴ଵሻ𝑃ሺ𝐴||𝐴ଵሻ൅𝑃ሺ𝐴ଶ||𝐴௖𝐴ଵሻ𝑃ሺ𝐴௖||𝐴ଵሻ
𝑃ሺ𝐴|𝐴ଵሻൌ𝑃ሺ𝐴ଵ𝐴ሻ
𝑃ሺ𝐴ଵሻൌ𝑃ሺ𝐴ଵ|𝐴ሻ𝑃ሺ𝐴ሻ
𝑃ሺ𝐴ଵሻ
𝑃ሺ𝐴ሻ
3
10 ,
𝑃ሺ𝐴ଵሻൌ. 26.
154 of 848

Thus,
Since 
 and 
 it follows
that
Example 5b
A female chimp has given birth. It is not certain, however, which of two male
chimps is the father. Before any genetic analysis has been performed, it is
believed that the probability that male number  is the father is  and the
probability that male number  is the father is 
 DNA obtained from the
mother, male number  and male number  indicates that on one specific
location of the genome, the mother has the gene pair 
 male number  has
the gene pair 
 and male number  has the gene pair 
 If a DNA test
shows that the baby chimp has the gene pair 
 what is the probability that
male number  is the father?
Solution
Let all probabilities be conditional on the event that the mother has the gene pair
 male number  has the gene pair 
 and male number  has the gene
pair 
 Now, let 
 be the event that male number 
 is the father, and
let 
 be the event that the baby chimp has the gene pair 
 Then,
 is obtained as follows:
𝑃ሺ𝐴|𝐴ଵሻൌሺ.4ሻሺ.3ሻ
.26
ൌ6
13
𝑃ሺ𝐴௖|𝐴ଵሻൌ1 െ𝑃ሺ𝐴|𝐴ଵሻൌ7
13
𝑃ሺ𝐴ଶ||𝐴𝐴ଵሻൌ𝑃ሺ𝐴ଶ||𝐴ሻൌ. 4
𝑃ሺ𝐴ଶ||𝐴௖𝐴ଵሻൌ𝑃ሺ𝐴ଶ||𝐴௖ሻൌ. 2,
𝑃ሺ𝐴ଶ|𝐴ଵሻൌሺ.4ሻ6
13 ൅ሺ.2ሻ7
13 ൎ.29
1
𝑝
2
1 െ𝑝.
1,
2
ሺ𝐴, 𝐴ሻ,
1
ሺ𝑎, 𝑎ሻ,
2
ሺ𝐴, 𝑎ሻ.
ሺ𝐴, 𝑎ሻ,
1
ሺ𝐴, 𝐴ሻ,
1
ሺ𝑎, 𝑎ሻ,
2
ሺ𝐴, 𝑎ሻ.
𝑀௜
𝑖, 𝑖ൌ1, 2,
𝐵஺,௔
ሺ𝐴, 𝑎ሻ.
𝑃൫𝑀ଵห𝐵஺,௔൯
𝑃൫𝑀ଵห𝐵஺,௔൯ൌ
𝑃൫𝑀ଵ𝐵஺,௔൯
𝑃൫𝐵஺,௔൯
ൌ
𝑃൫𝐵஺,௔ห𝑀ଵ൯𝑃൫𝑀ଵ൯
𝑃൫𝐵஺,௔ห𝑀ଵ൯𝑃൫𝑀ଵ൯൅𝑃൫𝐵஺,௔ห𝑀ଶ൯𝑃൫𝑀ଶ൯
ൌ
1 ⋅𝑝
1 ⋅𝑝൅ሺ1/2ሻሺ1 െ𝑝ሻ
ൌ
2𝑝
1 ൅𝑝
155 of 848

Because 
 when 
 the information that the baby’s gene pair is 
increases the probability that male number  is the father. This result is intuitive
because it is more likely that the baby would have gene pair 
 if 
 is true
than if 
 is true (the respective conditional probabilities being  and 
).
The next example deals with a problem in the theory of runs.
Example 5c
Independent trials, each resulting in a success with probability  or a failure with
probability 
 are performed. We are interested in computing the
probability that a run of  consecutive successes occurs before a run of 
consecutive failures.
Solution
Let  be the event that a run of  consecutive successes occurs before a run of
 consecutive failures. To obtain 
 we start by conditioning on the outcome
of the first trial. That is, letting  denote the event that the first trial results in a
success, we obtain
Now, given that the first trial was successful, one way we can get a run of 
successes before a run of 
 failures would be to have the next 
 trials all
result in successes. So, let us condition on whether or not that occurs. That is,
letting  be the event that trials 2 through  all are successes, we obtain
On the one hand, clearly, 
; on the other hand, if the event 
occurs, then the first trial would result in a success, but there would be a failure
some time during the next 
 trials. However, when this failure occurs, it would
wipe out all of the previous successes, and the situation would be exactly as if
we started out with a failure. Hence,
Because the independence of trials implies that  and  are independent, and
because 
 it follows from Equation (5.3)
 that
2𝑝
1 ൅𝑝൐𝑝
𝑝൏1,
ሺ𝐴, 𝑎ሻ
1
ሺ𝐴, 𝑎ሻ
𝑀ଵ
𝑀ଶ
1
1/2
𝑝
𝑞ൌ1 െ𝑝,
𝑛
𝑚
𝐸
𝑛
𝑚
𝑃ሺ𝐸ሻ,
𝐻
𝑃ሺ𝐸ሻൌ𝑝𝑃ሺ𝐸||𝐻ሻ൅𝑞𝑃ሺ𝐸||𝐻௖ሻ
(5.2)
𝑛
𝑚
𝑛െ1
𝐹
𝑛
𝑃ሺ𝐸||𝐻ሻൌ𝑃ሺ𝐸||𝐹𝐻ሻ𝑃ሺ𝐹||𝐻ሻ൅𝑃ሺ𝐸||𝐹௖𝐻ሻ𝑃ሺ𝐹௖||𝐻ሻ
(5.3)
𝑃ሺ𝐸||𝐹𝐻ሻൌ1
𝐹௖𝐻
𝑛െ1
𝑃ሺ𝐸||𝐹௖𝐻ሻൌ𝑃ሺ𝐸||𝐻௖ሻ
𝐹
𝐻
𝑃ሺ𝐹ሻൌ𝑝௡െଵ,
156 of 848

We now obtain an expression for 
 in a similar manner. That is, we let 
denote the event that trials 2 through 
 are all failures. Then,
Now, 
 is the event that the first 
 trials all result in failures, so 
Also, if 
 occurs, then the first trial is a failure, but there is at least one
success in the next 
 trials. Hence, since this success wipes out all previous
failures, we see that
Thus, because 
 we obtain, from (5.5
),
Solving Equations (5.4)
 and (5.6
) yields
and
Thus,
It is interesting to note that by the symmetry of the problem, the probability of
obtaining a run of 
 failures before a run of  successes would be given by
𝑃ሺ𝐸||𝐻ሻൌ𝑝௡െଵ൅ሺ1 െ𝑝௡െଵሻ𝑃ሺ𝐸||𝐻௖ሻ
(5.4)
𝑃ሺ𝐸||𝐻௖ሻ
𝐺
𝑚
𝑃ሺ𝐸||𝐻௖ሻൌ𝑃ሺ𝐸||𝐺𝐻௖ሻ𝑃ሺ𝐺||𝐻௖ሻ൅𝑃ሺ𝐸||𝐺௖𝐻௖ሻ𝑃ሺ𝐺௖||𝐻௖ሻ
(5.5)
𝐺𝐻௖
𝑚
𝑃ሺ𝐸||𝐺𝐻௖ሻൌ0.
𝐺௖𝐻௖
𝑚െ1
𝑃ሺ𝐸||𝐺௖𝐻௖ሻൌ𝑃ሺ𝐸||𝐻ሻ
𝑃ሺ𝐺௖||𝐻௖ሻൌ𝑃ሺ𝐺௖ሻൌ1 െ𝑞௠െଵ,
𝑃ሺ𝐸||𝐻௖ሻൌሺ1 െ𝑞௠െଵሻ𝑃ሺ𝐸||𝐻ሻ
(5.6)
𝑃ሺ𝐸|𝐻ሻൌ
𝑝௡െଵ
𝑝௡െଵ൅𝑞௠െଵെ𝑝௡െଵ𝑞௠െଵ
𝑃ሺ𝐸||𝐻௖ሻൌ
ሺ1 െ𝑞௠െଵሻ𝑝௡െଵ
𝑝௡െଵ൅𝑞௠െଵെ𝑝௡െଵ𝑞௠െଵ
𝑃ሺ𝐸ሻൌ
𝑝𝑃ሺ𝐸||𝐻ሻ൅𝑞𝑃ሺ𝐸||𝐻௖ሻ
ൌ
𝑝௡൅𝑞𝑝௡െଵሺ1 െ𝑞௠െଵሻ
𝑝௡െଵ൅𝑞௠െଵെ𝑝௡െଵ𝑞௠െଵ
ൌ
𝑝௡െଵሺ1 െ𝑞௠ሻ
𝑝௡െଵ൅𝑞௠െଵെ𝑝௡െଵ𝑞௠െଵ
(5.7)
𝑚
𝑛
157 of 848

Equation (5.7)
 with  and  interchanged and  and 
 interchanged. Hence,
this probability would equal
Since Equations (5.7)
 and (5.8
) sum to 1, it follows that, with probability 1,
either a run of  successes or a run of 
 failures will eventually occur.
As an example of Equation (5.7)
, we note that, in tossing a fair coin, the
probability that a run of 2 heads will precede a run of 3 tails is 
 For 2
consecutive heads before 4 consecutive tails, the probability rises to 
In our next example, we return to the matching problem and obtain a solution by
using conditional probabilities.
Example 5d
At a party,  people take off their hats. The hats are then mixed up, and each
person randomly selects one. We say that a match occurs if a person selects his
or her own hat. What is the probability of
a. no matches?
b. exactly  matches?
Solution
a. Let  denote the event that no matches occur, and to make explicit the
dependence on  write 
 We start by conditioning on whether or
not the first person selects his or her own hat—call these events 
 and
 respectively. Then,
Clearly, 
 so
Now, 
 is the probability of no matches when 
 people select
from a set of 
 hats, when one person, called the “extra” person, does
𝑝
𝑞
𝑛
𝑚
𝑃ሼrun of 𝑚 failures before a run of 𝑛 successesሽ
ൌ
𝑞௠െଵሺ1 െ𝑝௡ሻ
𝑞௠െଵ൅𝑝௡െଵെ𝑞௠െଵ𝑝௡െଵ
(5.8)
𝑛
𝑚
7
10 .
5
6 .
𝑛
𝑘
𝐸
𝑛,
𝑃௡ൌ𝑃ሺ𝐸ሻ.
𝑀
𝑀௖,
𝑃௡ൌ𝑃ሺ𝐸ሻൌ𝑃ሺ𝐸||𝑀ሻ𝑃ሺ𝑀ሻ൅𝑃ሺ𝐸||𝑀௖ሻ𝑃ሺ𝑀௖ሻ
𝑃ሺ𝐸||𝑀ሻൌ0,
𝑃௡ൌ𝑃ሺ𝐸||𝑀௖ሻ𝑛െ1
𝑛
(5.9)
𝑃ሺ𝐸||𝑀௖ሻ
𝑛െ1
𝑛െ1
158 of 848

not have their hat in the collection, and one hat, called the “extra” hat,
does not belong to any of the people. This can happen in either of two
mutually exclusive ways: Either there are no matches and the extra
person does not select the extra hat (this being the hat of the person who
chose first) or there are no matches and the extra person does select the
extra hat. The probability of the first of these events is just 
 which is
seen by regarding the extra hat as “belonging” to the extra person.
Because the second event has probability 
 we have
Thus, from Equation (5.9)
,
or, equivalently,
However, since 
 is the probability of no matches when  people select
among their own hats, we have
So, from Equation (5.10)
,
and, in general,
b. To obtain the probability of exactly  matches, we consider any fixed group
of  people. The probability that they, and only they, select their own hats
is
where 
 is the conditional probability that the other 
 people,
𝑃௡െଵ,
ሾ1/ሺ𝑛െ1ሻሿ𝑃௡െଶ,
𝑃ሺ𝐸||𝑀௖ሻൌ𝑃௡െଵ൅
1
𝑛െ1 𝑃௡െଶ
𝑃௡ൌ𝑛െ1
𝑛
𝑃௡െଵ൅1
𝑛𝑃௡െଶ
𝑃௡െ𝑃௡െଵൌെ1
𝑛ቆ𝑃௡െଵെ𝑃௡െଶቇ
(5.10)
𝑃௡
𝑛
𝑃ଵൌ0 𝑃ଶൌ1
2
𝑃ଷെ𝑃ଶ
ൌ
െሺ𝑃ଶെ𝑃ଵሻ
3
ൌെ1
3!   or  𝑃ଷൌ1
2! െ1
3!
𝑃ସെ𝑃ଷ
ൌ
െሺ𝑃ଷെ𝑃ଶሻ
4
ൌ1
4!   or  𝑃ସൌ1
2! െ1
3! ൅1
4!
𝑃௡ൌ1
2! െ1
3! ൅1
4! െ⋯൅ሺെ1ሻ௡
𝑛!
𝑘
𝑘
1
𝑛
1
𝑛െ1⋯
1
𝑛െሺ𝑘െ1ሻ𝑃௡െ௞ൌሺ𝑛െ𝑘ሻ!
𝑛!
𝑃௡െ௞
𝑃௡െ௞
𝑛െ𝑘
159 of 848

selecting among their own hats, have no matches. Since there are 
choices of a set of  people, the desired probability of exactly  matches is
An important concept in probability theory is that of the conditional independence
of events. We say that the events 
 and 
 are conditionally independent given
 if given that  occurs, the conditional probability that 
 occurs is unchanged
by information as to whether or not 
 occurs. More formally, 
 and 
 are said
to be conditionally independent given  if
or, equivalently,
The notion of conditional independence can easily be extended to more than two
events, and this extension is left as an exercise.
The reader should note that the concept of conditional independence was
implicitly employed in Example 5a
, where it was assumed that the events that
a policyholder had an accident in his or her th year, 
 were conditionally
independent given whether or not the person was accident prone. The following
example, sometimes referred to as Laplace’s rule of succession, further
illustrates the concept of conditional independence.
Example 5e Laplace’s rule of succession
There are 
 coins in a box. When flipped, the th coin will turn up heads with
probability 
 A coin is randomly selected from the box and is then
repeatedly flipped. If the first  flips all result in heads, what is the conditional
probability that the 
 flip will do likewise?
Solution
Letting 
 denote the event that the first  flips all land heads, the desired
probability is
ቆ𝑛
𝑘ቇ
𝑘
𝑘
𝑃௡െ௞
𝑘!
ൌ
1
2! െ1
3! ൅⋯൅ሺെ1ሻ௡െ௞
ሺ𝑛െ𝑘ሻ!
𝑘!
𝐸ଵ
𝐸ଶ
𝐹
𝐹
𝐸ଵ
𝐸ଶ
𝐸ଵ
𝐸ଶ
𝐹
𝑃ሺ𝐸ଵ||𝐸ଶ𝐹ሻൌ𝑃ሺ𝐸ଵ||𝐹ሻ
(5.11)
𝑃ሺ𝐸ଵ𝐸ଶ||𝐹ሻൌ𝑃ሺ𝐸ଵ||𝐹ሻ𝑃ሺ𝐸ଶ||𝐹ሻ
(5.12)
𝑖
𝑖ൌ1, 2, …,
𝑘൅1
𝑖
𝑖/𝑘, 𝑖ൌ0, 1, …, 𝑘.
𝑛
ሺ𝑛൅1ሻ
𝐻௡
𝑛
160 of 848

To compute 
 we condition on which coin is chosen. That is, letting 
denote the event that coin  is selected, we have that
Now, given that coin  is selected, it is reasonable to assume that the outcomes
will be conditionally independent, with each one resulting in a head with
probability 
 Hence,
As 
 this yields that
Thus,
If  is large, we can use the integral approximations
So, for  large,
Example 5f Updating information sequentially
𝑃ሺ𝐻௡൅ଵ|𝐻௡ሻൌ𝑃ሺ𝐻௡൅ଵ𝐻௡ሻ
𝑃ሺ𝐻௡ሻ
ൌ𝑃ሺ𝐻௡൅ଵሻ
𝑃ሺ𝐻௡ሻ
𝑃ሺ𝐻௡ሻ,
𝐶௜
𝑖
𝑃ሺ𝐻௡ሻൌ෍
௜ൌ଴
௞
𝑃ሺ𝐻௡|𝐶௜ሻ𝑃ሺ𝐶௜ሻ
𝑖
𝑖/𝑘.
𝑃ሺ𝐻௡|𝐶௜ሻൌሺ𝑖/𝑘ሻ௡
𝑃ቆ𝐶௜ቇൌ
1
𝑘൅1,
𝑃ቌ𝐻௡ቍൌ
1
𝑘൅1 ෍
௜ൌ଴
௞
ቌ𝑖/𝑘ቍ
௡
𝑃ሺ𝐻௡൅ଵ|𝐻௡ሻൌ
∑௜ൌ଴
௞
ሺ𝑖/𝑘ሻ௡൅ଵ
∑௜ൌ଴
௞
ሺ𝑖/𝑘ሻ௡
𝑘
1
𝑘෍
௜ൌ଴
௞
൬𝑖
𝑘൰
௡൅ଵ
ൎ
඲
଴
ଵ
𝑥௡൅ଵ𝑑𝑥ൌ
1
𝑛൅2
1
𝑘෍
௝ൌ଴
௞
൬𝑗
𝑘൰
௡
ൎ
඲
଴
ଵ
𝑥௡𝑑𝑥ൌ
1
𝑛൅1
𝑘
𝑃ሺ𝐻௡൅ଵ|𝐻௡ሻൎ𝑛൅1
𝑛൅2
161 of 848

Suppose there are  mutually exclusive and exhaustive possible hypotheses,
with initial (sometimes referred to as prior) probabilities 
Now, if information that the event  has occurred is received, then the conditional
probability that 
 is the true hypothesis (sometimes referred to as the updated or
posterior probability of 
) is
Suppose now that we learn first that 
 has occurred and then that 
 has
occurred. Then, given only the first piece of information, the conditional
probability that 
 is the true hypothesis is
whereas given both pieces of information, the conditional probability that 
 is the
true hypothesis is 
 which can be computed by
One might wonder, however, when one can compute 
 by using the
right side of Equation (5.13)
 with 
 and with 
 replaced by
 That is, when is it legitimate to regard 
 as
the prior probabilities and then use (5.13
) to compute the posterior
probabilities?
Solution
The answer is that the preceding is legitimate, provided that for each 
the events 
 and 
 are conditionally independent, given 
 For if this is the
case, then
Therefore,
𝑛
𝑃ሺ𝐻௜ሻ, ∑௜ൌଵ
௡
𝑃൫𝐻௜൯ൌ1.
𝐸
𝐻௜
𝐻௜
𝑃ሺ𝐻௜|𝐸ሻൌ
௉൫ாหு೔൯௉൫ு೔൯
෍
ೕ௉ቀாቚுೕቁ௉ቀுೕቁ
(5.13)
𝐸ଵ
𝐸ଶ
𝐻௜
𝑃ሺ𝐻௜|𝐸ଵሻൌ𝑃ሺ𝐸ଵ|𝐻௜ሻ𝑃ሺ𝐻௜ሻ
𝑃ሺ𝐸ଵሻ
ൌ
௉൫ாభหு೔൯௉൫ு೔൯
෍
ೕ௉ቀாభቚுೕቁ௉ቀுೕቁ
𝐻௜
𝑃ሺ𝐻௜||𝐸ଵ𝐸ଶሻ,
𝑃ሺ𝐻௜|𝐸ଵ𝐸ଶሻൌ
௉൫ாభாమหு೔൯௉൫ு೔൯
෍
ೕ௉ቀாభாమቚுೕቁ௉ቀுೕቁ
𝑃ሺ𝐻௜||𝐸ଵ𝐸ଶሻ
𝐸ൌ𝐸ଶ
𝑃൫𝐻௝൯
𝑃൫𝐻௝ห𝐸ଵ൯, 𝑗ൌ1, …, 𝑛.
𝑃൫𝐻௝ห𝐸ଵ൯, 𝑗൒1,
𝑗ൌ1, …, 𝑛,
𝐸ଵ
𝐸ଶ
𝐻௝.
𝑃൫𝐸ଵ𝐸ଶห𝐻௝൯ൌ𝑃൫𝐸ଶห𝐻௝൯𝑃൫𝐸ଵห𝐻௝൯,  𝑗ൌ1, …, 𝑛
162 of 848

where 
 Since the preceding equation is valid for all  we
obtain, upon summing,
showing that
and yielding the result
For instance, suppose that one of two coins is chosen to be flipped. Let 
 be the
event that coin 
 is chosen, and suppose that when coin  is flipped, it
lands on heads with probability 
 Then, the preceding equations show
that to sequentially update the probability that coin  is the one being flipped,
given the results of the previous flips, all that must be saved after each new flip is
the conditional probability that coin  is the coin being used. That is, it is not
necessary to keep track of all earlier results.
For events  and 
 the conditional probability of  given that  has occurred is
denoted by 
 and is defined by
𝑃ሺ𝐻௜||𝐸ଵ𝐸ଶሻൌ
𝑃ሺ𝐸ଶ||𝐻௜ሻ𝑃ሺ𝐸ଵ||𝐻௜ሻ𝑃ሺ𝐻௜ሻ
𝑃ሺ𝐸ଵ𝐸ଶሻ
ൌ
𝑃ሺ𝐸ଶ||𝐻௜ሻ𝑃ሺ𝐸ଵ𝐻௜ሻ
𝑃ሺ𝐸ଵ𝐸ଶሻ
ൌ
𝑃ሺ𝐸ଶ||𝐻௜ሻ𝑃ሺ𝐻௜||𝐸ଵሻ𝑃ሺ𝐸ଵሻ
𝑃ሺ𝐸ଵ𝐸ଶሻ
ൌ
𝑃ሺ𝐸ଶ||𝐻௜ሻ𝑃ሺ𝐻௜||𝐸ଵሻ
𝑄ሺ1, 2ሻ
𝑄ቆ1, 2ቇൌ𝑃ሺ𝐸ଵ𝐸ଶሻ
𝑃ሺ𝐸ଵሻ.
𝑖,
1 ൌ
෍
௜ൌଵ
௡
𝑃ሺ𝐻௜|𝐸ଵ𝐸ଶሻൌ
෍
௜ൌଵ
௡
𝑃ሺ𝐸ଶ|𝐻௜ሻ𝑃ሺ𝐻௜|𝐸ଵሻ
𝑄ሺ1, 2ሻ
𝑄ሺ1, 2ሻൌ
෍
௜ൌଵ
௡
𝑃ሺ𝐸ଶ|𝐻௜ሻ𝑃ሺ𝐻௜|𝐸ଵሻ
𝑃ሺ𝐻௜|𝐸ଵ𝐸ଶሻൌ
𝑃ሺ𝐸ଶ|𝐻௜ሻ𝑃ሺ𝐻௜|𝐸ଵሻ
∑௜ൌଵ
௡
𝑃ሺ𝐸ଶ|𝐻௜ሻ𝑃ሺ𝐻௜|𝐸ଵሻ
𝐻௜
𝑖, 𝑖ൌ1, 2,
𝑖
𝑝௜, 𝑖ൌ1, 2.
1
1
𝐸
𝐹,
𝐸
𝐹
𝑃ሺ𝐸||𝐹ሻ
163 of 848

The identity
is known as the multiplication rule of probability.
A valuable identity is
which can be used to compute 
 by “conditioning” on whether  occurs.
 is called the odds of the event . The identity
shows that when new evidence  is obtained, the value of the odds of  becomes its
old value multiplied by the ratio of the conditional probability of the new evidence
when  is true to the conditional probability when  is not true.
Let 
 be mutually exclusive events whose union is the entire sample
space. The identity
is known as Bayes’s formula. If the events 
 are competing hypotheses,
then Bayes’s formula shows how to compute the conditional probabilities of these
hypotheses when additional evidence  becomes available.
The denominator of Bayes’s formula uses that
which is called the law of total probability.
If 
 then we say that the events  and  are independent. This
condition is equivalent to 
 and to 
 Thus, the events 
𝑃ሺ𝐸|𝐹ሻൌ𝑃ሺ𝐸𝐹ሻ
𝑃ሺ𝐹ሻ
𝑃ሺ𝐸ଵ𝐸ଶ⋯𝐸௡ሻൌ𝑃ሺ𝐸ଵሻ𝑃ሺ𝐸ଶ||𝐸ଵሻ⋯𝑃ሺ𝐸௡||𝐸ଵ⋯𝐸௡െଵሻ
𝑃ሺ𝐸ሻൌ𝑃ሺ𝐸||𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸||𝐹௖ሻ𝑃ሺ𝐹௖ሻ
𝑃ሺ𝐸ሻ
𝐹
𝑃ሺ𝐻ሻ/𝑃ሺ𝐻௖ሻ
𝐻
𝑃ሺ𝐻||𝐸ሻ
𝑃ሺ𝐻௖||𝐸ሻൌ𝑃ሺ𝐻ሻ 𝑃ሺ𝐸||𝐻ሻ
𝑃ሺ𝐻௖ሻ𝑃ሺ𝐸||𝐻௖ሻ
𝐸
𝐻
𝐻
𝐻
𝐹௜, 𝑖ൌ1, …, 𝑛,
𝑃൫𝐹௝|𝐸൯ൌ
𝑃൫𝐸ห𝐹௝൯𝑃൫𝐹௝൯
෍
௜ൌଵ
௡
𝑃ሺ𝐸|𝐹௜ሻ𝑃ሺ𝐹௜ሻ
𝐹௜, 𝑖ൌ1, …, 𝑛,
𝐸
𝑃ሺ𝐸ሻൌ
∑
௜ൌଵ
௡
𝑃ሺ𝐸|𝐹௜ሻ𝑃ሺ𝐹௜ሻ
𝑃ሺ𝐸𝐹ሻൌ𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ,
𝐸
𝐹
𝑃ሺ𝐸||𝐹ሻൌ𝑃ሺ𝐸ሻ
𝑃ሺ𝐹||𝐸ሻൌ𝑃ሺ𝐹ሻ.
𝐸
164 of 848

and  are independent if knowledge of the occurrence of one of them does not affect
the probability of the other.
The events 
 are said to be independent if, for any subset 
 of them,
For a fixed event 
 can be considered to be a probability function on the
events  of the sample space.
𝐹
𝐸ଵ, …, 𝐸௡
𝐸௜భ, …, 𝐸௜ೝ
𝑃൫𝐸௜భ⋯𝐸௜ೝ൯ൌ𝑃൫𝐸௜భ൯⋯𝑃൫𝐸௜ೝ൯
𝐹, 𝑃ሺ𝐸||𝐹ሻ
𝐸
3.1. Two fair dice are rolled. What is the conditional probability that at least
one lands on 6 given that the dice land on different numbers?
3.2. If two fair dice are rolled, what is the conditional probability that the first
one lands on 6 given that the sum of the dice is ? Compute for all values of 
between 2 and 12.
3.3. Use Equation (2.1)
 to compute in a hand of bridge the conditional
probability that East has 3 spades given that North and South have a
combined total of 8 spades.
3.4. What is the probability that at least one of a pair of fair dice lands on 6,
given that the sum of the dice is 
3.5. An urn contains 6 white and 9 black balls. If 4 balls are to be randomly
selected without replacement, what is the probability that the first 2 selected
are white and the last 2 black?
3.6. Consider an urn containing 12 balls, of which 8 are white. A sample of
size 4 is to be drawn with replacement (without replacement). What is the
conditional probability (in each case) that the first and third balls drawn will be
white given that the sample drawn contains exactly 3 white balls?
3.7. The king comes from a family of 2 children. What is the probability that
the other child is his sister?
3.8. A couple has 2 children. What is the probability that both are girls if the
older of the two is a girl?
3.9. Consider 3 urns. Urn  contains 2 white and 4 red balls, urn  contains 8
white and 4 red balls, and urn  contains 1 white and 3 red balls. If 1 ball is
selected from each urn, what is the probability that the ball chosen from urn 
was white given that exactly 2 white balls were selected?
3.10. Three cards are randomly selected, without replacement, from an
ordinary deck of 52 playing cards. Compute the conditional probability that the
first card selected is a spade given that the second and third cards are
spades.
𝑖
𝑖
𝑖, 𝑖ൌ2, 3, …, 12?
𝐴
𝐵
𝐶
𝐴
165 of 848

3.11. Two cards are randomly chosen without replacement from an ordinary
deck of 
 cards. Let  be the event that both cards are aces, let 
 be the
event that the ace of spades is chosen, and let  be the event that at least
one ace is chosen. Find
a. 
b. 
3.12. Suppose distinct values are written on each of  cards, which are then
randomly given the designations 
 and . Given that card A’s value is less
than card B’s value, find the probability it is also less than card C’s value.
3.13. A recent college graduate is planning to take the first three actuarial
examinations in the coming summer. She will take the first actuarial exam in
June. If she passes that exam, then she will take the second exam in July,
and if she also passes that one, then she will take the third exam in
September. If she fails an exam, then she is not allowed to take any others.
The probability that she passes the first exam is .9. If she passes the first
exam, then the conditional probability that she passes the second one is .8,
and if she passes both the first and the second exams, then the conditional
probability that she passes the third exam is .7.
a. What is the probability that she passes all three exams?
b. Given that she did not pass all three exams, what is the conditional
probability that she failed the second exam?
3.14. Suppose that an ordinary deck of 52 cards (which contains 4 aces) is
randomly divided into 4 hands of 13 cards each. We are interested in
determining  the probability that each hand has an ace. Let 
 be the event
that the th hand has exactly one ace. Determine 
 by using
the multiplication rule.
3.15. An urn initially contains 5 white and 7 black balls. Each time a ball is
selected, its color is noted and it is replaced in the urn along with 2 other balls
of the same color. Compute the probability that
a. the first 2 balls selected are black and the next 2 are white;
b. of the first 4 balls selected, exactly 2 are black.
3.16. An ectopic pregnancy is twice as likely to develop when the pregnant
woman is a smoker as it is when she is a nonsmoker. If 32 percent of women
of childbearing age are smokers, what percentage of women having ectopic
pregnancies are smokers?
3.17. Ninety-eight percent of all babies survive delivery. However, 15 percent
of all births involve Cesarean (C) sections, and when a C section is
performed, the baby survives 96 percent of the time. If a randomly chosen
52
𝐵
𝐴௦
𝐴
𝑃ሺ𝐵||𝐴௦ሻ
𝑃ሺ𝐵||𝐴ሻ
3
𝐴, 𝐵,
𝐶
𝑝,
𝐸௜
𝑖
𝑝ൌ𝑃ሺ𝐸ଵ𝐸ଶ𝐸ଷ𝐸ସሻ
166 of 848

pregnant woman does not have a C section, what is the probability that her
baby survives?
3.18. In a certain community, 36 percent of the families own a dog and 22
percent of the families that own a dog also own a cat. In addition, 30 percent
of the families own a cat. What is
a. the probability that a randomly selected family owns both a dog and a
cat?
b. the conditional probability that a randomly selected family owns a dog
given that it owns a cat?
3.19. A total of 46 percent of the voters in a certain city classify themselves as
Independents, whereas 30 percent classify themselves as Liberals and 24
percent say that they are Conservatives. In a recent local election, 35 percent
of the Independents, 62 percent of the Liberals, and 58 percent of the
Conservatives voted. A voter is chosen at random. Given that this person
voted in the local election, what is the probability that he or she is
a. an Independent?
b. a Liberal?
c. a Conservative?
d. What percent of voters participated in the local election?
3.20. A total of 48 percent of the women and 37 percent of the men who took
a certain “quit smoking” class remained nonsmokers for at least one year after
completing the class. These people then attended a success party at the end
of a year. If 62 percent of the original class was male,
a. what percentage of those attending the party were women?
b. what percentage of the original class attended the party?
3.21. Fifty-two percent of the students at a certain college are females. Five
percent of the students in this college are majoring in computer science. Two
percent of the students are women majoring in computer science. If a student
is selected at random, find the conditional probability that
a. the student is female given that the student is majoring in computer
science;
b. this student is majoring in computer science given that the student is
female.
3.22. A total of 500 married working couples were polled about their annual
salaries, with the following information resulting:
Wife
Husband
167 of 848

Less than $125,000
More than $125,000
Less than $125,000
212
198
More than $125,000
36
54
For instance, in 36 of the couples, the wife earned more and the husband
earned less than $125,000. If one of the couples is randomly chosen, what is
a. the probability that the husband earns less than $125,000?
b. the conditional probability that the wife earns more than $125,000 given
that the husband earns more than this amount?
c. the conditional probability that the wife earns more than $125,000 given
that the husband earns less than this amount?
3.23. A red die, a blue die, and a yellow die (all six sided) are rolled. We are
interested in the probability that the number appearing on the blue die is less
than that appearing on the yellow die, which is less than that appearing on the
red die. That is, with 
 and  denoting, respectively, the number appearing
on the blue, yellow, and red die, we are interested in 
a. What is the probability that no two of the dice land on the same
number?
b. Given that no two of the dice land on the same number, what is the
conditional probability that 
c. What is 
3.24. Urn I contains 2 white and 4 red balls, whereas urn II contains 1 white
and 1 red ball. A ball is randomly chosen from urn I and put into urn II, and a
ball is then randomly selected from urn II. What is
a. the probability that the ball selected from urn II is white?
b. the conditional probability that the transferred ball was white given that
a white ball is selected from urn II?
3.25. Twenty percent of B’s phone calls are with her daughter. Sixty five
percent of the time that B speaks with her daughter she hangs up the phone
with a smile on her face. Given that B has just hung up the phone with a smile
on her face, we are interested in the conditional probability that the phone call
was with her daughter. Do we have enough information to determine this
probability. If yes, what is it; if no, what additional information is needed.
3.26. Each of 2 balls is painted either black or gold and then placed in an urn.
Suppose that each ball is colored black with probability  and that these
𝐵, 𝑌,
𝑅
𝑃ሺ𝐵൏𝑌൏𝑅ሻ.
𝐵൏𝑌൏𝑅?
𝑃ሺ𝐵൏𝑌൏𝑅ሻ
1
2
168 of 848

events are independent.
a. Suppose that you obtain information that the gold paint has been used
(and thus at least one of the balls is painted gold). Compute the
conditional probability that both balls are painted gold.
b. Suppose now that the urn tips over and 1 ball falls out. It is painted
gold. What is the probability that both balls are gold in this case?
Explain.
3.27. The following method was proposed to estimate the number of people
over the age of 50 who reside in a town of known population 100,000: “As you
walk along the streets, keep a running count of the percentage of people you
encounter who are over 50. Do this for a few days; then multiply the
percentage you obtain by 100,000 to obtain the estimate.” Comment on this
method.
Hint: Let  denote the proportion of people in the town who are over 50.
Furthermore, let 
 denote the proportion of time that a person under the age
of 50 spends in the streets, and let 
 be the corresponding value for those
over 50. What quantity does the method suggested estimate? When is the
estimate approximately equal to 
3.28. Suppose that 5 percent of men and 0.25 percent of women are color
blind. A color-blind person is chosen at random. What is the probability of this
person being male? Assume that there are an equal number of males and
females. What if the population consisted of twice as many males as females?
3.29. All the workers at a certain company drive to work and park in the
company’s lot. The company is interested in estimating the average number of
workers in a car. Which of the following methods will enable the company to
estimate this quantity? Explain your answer.
A. Randomly choose  workers, find out how many were in the cars in
which they were driven, and take the average of the  values.
B. Randomly choose  cars in the lot, find out how many were driven in
those cars, and take the average of the  values.
3.30 Suppose that an ordinary deck of 52 cards is shuffled and the cards are
then turned over one at a time until the first ace appears. Given that the first
ace is the 20th card to appear, what is the conditional probability that the card
following it is the
a. ace of spades?
b. two of clubs?
3.31. There are 15 tennis balls in a box, of which 9 have not previously been
used. Three of the balls are randomly chosen, played with, and then returned
𝑝
𝛼ଵ
𝛼ଶ
𝑝?
𝑛
𝑛
𝑛
𝑛
169 of 848

to the box. Later, another 3 balls are randomly chosen from the box. Find the
probability that none of these balls has ever been used.
3.32. Consider two boxes, one containing 1 black and 1 white marble, the
other 2 black and 1 white marble. A box is selected at random, and a marble
is drawn from it at random. What is the probability that the marble is black?
What is the probability that the first box was the one selected given that the
marble is white?
3.33. Ms. Aquina has just had a biopsy on a possibly cancerous tumor. Not
wanting to spoil a weekend family event, she does not want to hear any bad
news in the next few days. But if she tells the doctor to call only if the news is
good, then if the doctor does not call, Ms. Aquina can conclude that the news
is bad. So, being a student of probability, Ms. Aquina instructs the doctor to
flip a coin. If it comes up heads, the doctor is to call if the news is good and
not call if the news is bad. If the coin comes up tails, the doctor is not to call.
In this way, even if the doctor doesn’t call, the news is not necessarily bad. Let
 be the probability that the tumor is cancerous; let  be the conditional
probability that the tumor is cancerous given that the doctor does not call.
a. Which should be larger,  or 
b. Find  in terms of 
 and prove your answer in part (a).
3.34. A family has  children with probability 
 where
 A child from this family is randomly
chosen. Given that this child is the eldest child in the family, find the
conditional probability that the family has
a. only  child;
b.  children.
Redo (a) and (b) when the randomly selected child is the youngest child of the
family.
3.35. On rainy days, Joe is late to work with probability .3; on nonrainy days,
he is late with probability .1. With probability .7, it will rain tomorrow.
a. Find the probability that Joe is early tomorrow.
b. Given that Joe was early, what is the conditional probability that it
rained?
3.36. In Example 3f
, suppose that the new evidence is subject to different
possible interpretations and in fact shows only that it is 90 percent likely that
the criminal possesses the characteristic in question. In this case, how likely
would it be that the suspect is guilty (assuming, as before, that he has the
characteristic)?
3.37. With probability .6, the present was hidden by mom; with probability .4, it
𝛼
𝛽
𝛼
𝛽
𝛽
𝛼,
𝑗
𝑝௝,
𝑝ଵൌ. 1, 𝑝ଶൌ. 25, 𝑝ଷൌ. 35, 𝑝ସൌ. 3.
1
4
170 of 848

was hidden by dad. When mom hides the present, she hides it upstairs 70
percent of the time and downstairs 30 percent of the time. Dad is equally likely
to hide it upstairs or downstairs.
a. What is the probability that the present is upstairs?
b. Given that it is downstairs, what is the probability it was hidden by dad?
3.38. Stores 
 and  have 50, 75, and 100 employees, respectively, and
50, 60, and 70 percent of them respectively are women. Resignations are
equally likely among all employees, regardless of sex. One woman employee
resigns. What is the probability that she works in store 
3.39.
a. A gambler has a fair coin and a two-headed coin in his pocket. He
selects one of the coins at random; when he flips it, it shows heads.
What is the probability that it is the fair coin?
b. Suppose that he flips the same coin a second time and, again, it shows
heads. Now what is the probability that it is the fair coin?
c. Suppose that he flips the same coin a third time and it shows tails. Now
what is the probability that it is the fair coin?
3.40. Urn  has 5 white and 7 black balls. Urn  has 3 white and 12 black
balls. We flip a fair coin. If the outcome is heads, then a ball from urn  is
selected, whereas if the outcome is tails, then a ball from urn  is selected.
Suppose that a white ball is selected. What is the probability that the coin
landed tails?
3.41. In Example 3a
, what is the probability that someone has an accident
in the second year given that he or she had no accidents in the first year?
3.42. Consider a sample of size 3 drawn in the following manner: We start
with an urn containing 5 white and 7 red balls. At each stage, a ball is drawn
and its color is noted. The ball is then returned to the urn, along with an
additional ball of the same color. Find the probability that the sample will
contain exactly
a. 0 white balls;
b. 1 white ball;
c. 3 white balls;
d. 2 white balls.
3.43. A deck of cards is shuffled and then divided into two halves of 26 cards
each. A card is drawn from one of the halves; it turns out to be an ace. The
ace is then placed in the second half-deck. The half is then shuffled, and a
card is drawn from it. Compute the probability that this drawn card is an ace.
Hint: Condition on whether or not the interchanged card is selected.
𝐴, 𝐵,
𝐶
𝐶?
𝐴
𝐵
𝐴
𝐵
171 of 848

3.44. Twelve percent of all U.S. households are in California. A total of 1.3
percent of all U.S. households earn more than $250,000 per year, while a total
of 3.3 percent of all California households earn more than $250,000 per year.
a. What proportion of all non-California households earn more than
$250,000 per year?
b. Given that a randomly chosen U.S. household earns more than
$250,000 per year, what is the probability it is a California household?
3.45. There are 3 coins in a box. One is a two-headed coin, another is a fair
coin, and the third is a biased coin that comes up heads 75 percent of the
time. When one of the 3 coins is selected at random and flipped, it shows
heads. What is the probability that it was the two-headed coin?
3.46. Three prisoners are informed by their jailer that one of them has been
chosen at random to be executed and the other two are to be freed. Prisoner
 asks the jailer to tell him privately which of his fellow prisoners will be set
free, claiming that there would be no harm in divulging this information
because he already knows that at least one of the two will go free. The jailer
refuses to answer the question, pointing out that if  knew which of his fellow
prisoners were to be set free, then his own probability of being executed
would rise from  to  because he would then be one of two prisoners. What
do you think of the jailer’s reasoning?
3.47. There is a 
 percent chance that  can fix her busted computer. If 
cannot, then there is a 
 percent chance that her friend B can fix it.
a. Find the probability it will be fixed by either A or B.
b. If it is fixed, what is the probability it will be fixed by B.
3.48. In any given year, a male automobile policyholder will make a claim with
probability 
 and a female policyholder will make a claim with probability 
where 
 The fraction of the policyholders that are male is 
A policyholder is randomly chosen. If 
 denotes the event that this
policyholder will make a claim in year  show that
Give an intuitive explanation of why the preceding inequality is true.
3.49. An urn contains 5 white and 10 black balls. A fair die is rolled and that
number of balls is randomly chosen from the urn. What is the probability that
all of the balls selected are white? What is the conditional probability that the
die landed on 3 if all the balls selected are white?
3.50. Each of 2 cabinets identical in appearance has 2 drawers. Cabinet 
contains a silver coin in each drawer, and cabinet  contains a silver coin in
one of its drawers and a gold coin in the other. A cabinet is randomly selected,
𝐴
𝐴
1
3
1
2
30
𝐴
𝐴
40
𝑝௠
𝑝௙,
𝑝௙്𝑝௠.
𝛼, 0 ൏𝛼൏1.
𝐴௜
𝑖,
𝑃ሺ𝐴ଶ|𝐴ଵሻ𝑃ሺ𝐴ଵሻ
𝐴
𝐵
172 of 848

one of its drawers is opened, and a silver coin is found. What is the probability
that there is a silver coin in the other drawer?
3.51. Prostate cancer is the most common type of cancer found in males. As
an indicator of whether a male has prostate cancer, doctors often perform a
test that measures the level of the prostate-specific antigen (PSA) that is
produced only by the prostate gland. Although PSA levels are indicative of
cancer, the test is notoriously unreliable. Indeed, the probability that a
noncancerous man will have an elevated PSA level is approximately 
increasing to approximately 
 if the man does have cancer. If, on the basis
of other factors, a physician is 
 percent certain that a male has prostate
cancer, what is the conditional probability that he has the cancer given that
a. the test indicated an elevated PSA level?
b. the test did not indicate an elevated PSA level?
Repeat the preceding calculation, this time assuming that the physician
initially believes that there is a 
 percent chance that the man has prostate
cancer.
3.52. Suppose that an insurance company classifies people into one of three
classes: good risks, average risks, and bad risks. The company’s records
indicate that the probabilities that good-, average-, and bad-risk persons will
be involved in an accident over a 1-year span are, respectively, .05, .15, and
.30. If 20 percent of the population is a good risk, 50 percent an average risk,
and 30 percent a bad risk, what proportion of people have accidents in a fixed
year? If policyholder  had no accidents in 2012, what is the probability that
he or she is a good risk? is an average risk?
3.53. A worker has asked her supervisor for a letter of recommendation for a
new job. She estimates that there is an 80 percent chance that she will get the
job if she receives a strong recommendation, a 40 percent chance if she
receives a moderately good recommendation, and a 10 percent chance if she
receives a weak recommendation. She further estimates that the probabilities
that the recommendation will be strong, moderate, and weak are .7, .2, and .1,
respectively.
a. How certain is she that she will receive the new job offer?
b. Given that she does receive the offer, how likely should she feel that
she received a strong recommendation? a moderate recommendation?
a weak recommendation?
c. Given that she does not receive the job offer, how likely should she feel
that she received a strong recommendation? a moderate
recommendation? a weak recommendation?
3.54. Players 
 are randomly lined up. The first two players in line
. 135,
. 268
70
30
𝐴
𝐴, 𝐵, 𝐶, 𝐷
173 of 848

then play a game; the winner of that game then plays a game with the person
who is third in line; the winner of that game then plays a game with the person
who is fourth in line. The winner of that last game is considered the winner of
the tournament. If A wins each game it plays with probability  determine the
probability that A is the winner of the tournament.
3.55. Players 
 are playing a tournament. Two of these three players are
randomly chosen to play a game in round one, with the winner then playing
the remaining player in round two. The winner of round two is the tournament
victor. Assume that all games are independent and that  wins when playing
against  with probability 
a. Find the probability that  is the tournament victor.
b. If  is the tournament victor, find the conditional probability that  did
not play in round one.
3.56. Suppose there are two coins, with coin  landing heads when flipped
with probability 
 and coin  with probability 
 Suppose also that we
randomly select one of these coins and then continually flip it. Let 
 denote
the event that flip 
 lands heads. Also, let 
 be the event that coin 
was chosen, 
a. Find 
b. Find 
c. Find 
d. Find 
3.57. In a  game series played with two teams, the first team to win a total of
 games is the winner. Suppose that each game played is independently won
by team A with probability .
a. Given that one team leads  to  what is the probability that it is team
 that leads.
b. Given that one team leads  to  what is the probability that team wins
the series.
3.58. A parallel system functions whenever at least one of its components
works. Consider a parallel system of  components, and suppose that each
component works independently with probability 
 Find the conditional
probability that component 1 works given that the system is functioning.
3.59. If you had to construct a mathematical model for events  and 
 as
described in parts (a) through (e), would you assume that they were
independent events? Explain your reasoning.
a.  is the event that a businesswoman has blue eyes, and  is the event
𝑝,
1, 2, 3
𝑖
𝑗
𝑖
𝑖൅𝑗.
1
1
1
1
. 3
2
. 5.
𝐻௝
𝑗, 𝑗൒1,
𝐶௜
𝑖
𝑖ൌ1, 2.
𝑃ሺ𝐻ଵሻ.
𝑃ሺ𝐻ଶ||𝐻ଵሻ.
𝑃ሺ𝐶ଵ||𝐻ଵሻ.
𝑃ሺ𝐻ଶ𝐻ଷ𝐻ସ||𝐻ଵሻ.
7
4
𝑝
3
0,
𝐴
3
0,
𝑛
1
2 .
𝐸
𝐹,
𝐸
𝐹
174 of 848

that her secretary has blue eyes.
b.  is the event that a professor owns a car, and  is the event that he is
listed in the telephone book.
c.  is the event that a man is under 6 feet tall, and  is the event that he
weighs more than 200 pounds.
d.  is the event that a woman lives in the United States, and  is the
event that she lives in the Western Hemisphere.
e.  is the event that it will rain tomorrow, and  is the event that it will rain
the day after tomorrow.
3.60. In a class, there are 4 first-year boys, 6 first-year girls, and 6 sophomore
boys. How many sophomore girls must be present if sex and class are to be
independent when a student is selected at random?
3.61. Suppose that you continually collect coupons and that there are 
different types. Suppose also that each time a new coupon is obtained, it is a
type  coupon with probability 
 Suppose that you have just
collected your th coupon. What is the probability that it is a new type?
Hint: Condition on the type of this coupon.
3.62. A simplified model for the movement of the price of a stock supposes
that on each day the stock’s price either moves up 1 unit with probability  or
moves down 1 unit with probability 
 The changes on different days are
assumed to be independent.
a. What is the probability that after 2 days the stock will be at its original
price?
b. What is the probability that after 3 days the stock’s price will have
increased by 1 unit?
c. Given that after 3 days the stock’s price has increased by 1 unit, what
is the probability that it went up on the first day?
3.63. Suppose that we want to generate the outcome of the flip of a fair coin,
but that all we have at our disposal is a biased coin that lands on heads with
some unknown probability  that need not be equal to 
 Consider the
following procedure for accomplishing our task:
1. Flip the coin.
2. Flip the coin again.
3. If both flips land on heads or both land on tails, return to step 1.
4. Let the result of the last flip be the result of the experiment.
a. Show that the result is equally likely to be either heads or tails.
b. Could we use a simpler procedure that continues to flip the coin until
the last two flips are different and then lets the result be the outcome of
𝐸
𝐹
𝐸
𝐹
𝐸
𝐹
𝐸
𝐹
𝑚
𝑖
𝑝௜, 𝑖ൌ1, …, 𝑚.
𝑛
𝑝
1 െ𝑝.
𝑝
1
2 .
175 of 848

the final flip?
3.64. Independent flips of a coin that lands on heads with probability  are
made. What is the probability that the first four outcomes are
a. 
b. 
c. What is the probability that the pattern 
 occurs before the
pattern 
Hint for part (c): How can the pattern 
 occur first?
3.65. The color of a person’s eyes is determined by a single pair of genes. If
they are both blue-eyed genes, then the person will have blue eyes; if they are
both brown-eyed genes, then the person will have brown eyes; and if one of
them is a blue-eyed gene and the other a brown-eyed gene, then the person
will have brown eyes. (Because of the latter fact, we say that the brown-eyed
gene is dominant over the blue-eyed one.) A newborn child independently
receives one eye gene from each of its parents, and the gene it receives from
a parent is equally likely to be either of the two eye genes of that parent.
Suppose that Smith and both of his parents have brown eyes, but Smith’s
sister has blue eyes.
a. What is the probability that Smith possesses a blueeyed gene?
b. Suppose that Smith’s wife has blue eyes. What is the probability that
their first child will have blue eyes?
c. If their first child has brown eyes, what is the probability that their next
child will also have brown eyes?
3.66. Genes relating to albinism are denoted by  and . Only those people
who receive the  gene from both parents will be albino. Persons having the
gene pair 
 are normal in appearance and, because they can pass on the
trait to their offspring, are called carriers. Suppose that a normal couple has
two children, exactly one of whom is an albino. Suppose that the nonalbino
child mates with a person who is known to be a carrier for albinism.
a. What is the probability that their first offspring is an albino?
b. What is the conditional probability that their second offspring is an
albino given that their firstborn is not?
3.67. Barbara and Dianne go target shooting. Suppose that each of Barbara’s
shots hits a wooden duck target with probability 
 while each shot of
Dianne’s hits it with probability 
 Suppose that they shoot simultaneously at
the same target. If the wooden duck is knocked over (indicating that it was
hit), what is the probability that
a. both shots hit the duck?
𝑝
𝐻, 𝐻, 𝐻, 𝐻?
𝑇, 𝐻, 𝐻, 𝐻?
𝐻, 𝐻, 𝐻, 𝐻?
𝑇, 𝐻, 𝐻, 𝐻?
𝐻, 𝐻, 𝐻, 𝐻?
𝐴
𝑎
𝑎
𝐴, 𝑎
𝑝ଵ,
𝑝ଶ.
176 of 848

b. Barbara’s shot hit the duck?
What independence assumptions have you made?
3.68.
 and  are involved in a duel. The rules of the duel are that they are to
pick up their guns and shoot at each other simultaneously. If one or both are
hit, then the duel is over. If both shots miss, then they repeat the process.
Suppose that the results of the shots are independent and that each shot of 
will hit  with probability 
 and each shot of  will hit  with probability 
What is
a. the probability that  is not hit?
b. the probability that both duelists are hit?
c. the probability that the duel ends after the th round of shots?
d. the conditional probability that the duel ends after the th round of
shots given that  is not hit?
e. the conditional probability that the duel ends after the th round of
shots given that both duelists are hit?
3.69. Assume, as inExample 3h
, that 64 percent of twins are of the same
sex. Given that a newborn set of twins is of the same sex, what is the
conditional probability that the twins are identical?
3.70. The probability of the closing of the th relay in the circuits shown in
Figure 3.5
 is given by 
 2, 3, 4, 5. If all relays function
independently, what is the probability that a current flows between  and  for
the respective circuits?
Figure 3.5 Circuits for problem 3.70
𝐴
𝐵
𝐴
𝐵
𝑝஺,
𝐵
𝐴
𝑝஻.
𝐴
𝑛
𝑛
𝐴
𝑛
𝑖
𝑝௜, 𝑖ൌ1,
𝐴
𝐵
177 of 848

Hint for (b): Condition on whether relay 3 closes.
3.71. An engineering system consisting of  components is said to be a -out-
of-  system 
 if the system functions if and only if at least  of the 
components function. Suppose that all components function independently of
one another.
a. If the th component functions with probability 
 2, 3, 4, compute
the probability that a 2-out-of-4 system functions.
b. Repeat part (a) for a 3-out-of-5 system.
c. Repeat for a -out-of-  system when all the 
 equal  (that is,
).
3.72. In Problem 3.70a
, find the conditional probability that relays 1 and 2
are both closed given that a current flows from  to .
3.73. A certain organism possesses a pair of each of 5 different genes (which
we will designate by the first 5 letters of the English alphabet). Each gene
appears in 2 forms (which we designate by lowercase and capital letters). The
capital letter will be assumed to be the dominant gene, in the sense that if an
organism possesses the gene pair xX, then it will outwardly have the
appearance of the  gene. For instance, if  stands for brown eyes and  for
blue eyes, then an individual having either gene pair XX or xX will have brown
eyes, whereas one having gene pair xx will have blue eyes. The characteristic
appearance of an organism is called its phenotype, whereas its genetic
constitution is called its genotype. (Thus, 2 organisms with respective
genotypes aA, bB, cc, dD, ee and AA, BB, cc, DD, ee would have different
genotypes but the same phenotype.) In a mating between 2 organisms, each
one contributes, at random, one of its gene pairs of each type. The 5
contributions of an organism (one of each of the 5 types) are assumed to be
independent and are also independent of the contributions of the organism’s
mate. In a mating between organisms having genotypes aA, bB, cC, dD, eE
and aa, bB, cc, Dd, ee what is the probability that the progeny will (i)
phenotypically and (ii) genotypically resemble
𝑛
𝑘
𝑛
ሺ𝑘൑𝑛ሻ
𝑘
𝑛
𝑖
𝑃௜, 𝑖ൌ1,
𝑘
𝑛
𝑃௜
𝑝
𝑃௜ൌ𝑝, 𝑖ൌ1, 2, …, 𝑛
𝐴
𝐵
𝑋
𝑋
𝑥
178 of 848

a. the first parent?
b. the second parent?
c. either parent?
d. neither parent?
3.74. There is a 50–50 chance that the queen carries the gene for hemophilia.
If she is a carrier, then each prince has a 50–50 chance of having hemophilia.
If the queen has had three princes without the disease, what is the probability
that the queen is a carrier? If there is a fourth prince, what is the probability
that he will have hemophilia?
3.75. A town council of 7 members contains a steering committee of size 3.
New ideas for legislation go first to the steering committee and then on to the
council as a whole if at least 2 of the 3 committee members approve the
legislation. Once at the full council, the legislation requires a majority vote (of
at least 4) to pass. Consider a new piece of legislation, and suppose that each
town council member will approve it, independently, with probability . What is
the probability that a given steering committee member’s vote is decisive in
the sense that if that person’s vote were reversed, then the final fate of the
legislation would be reversed? What is the corresponding probability for a
given council member not on the steering committee?
3.76.Suppose that each child born to a couple is equally likely to be a boy or a
girl, independently of the sex distribution of the other children in the family. For
a couple having 5 children, compute the probabilities of the following events:
a. All children are of the same sex.
b. The 3 eldest are boys and the others girls.
c. Exactly 3 are boys.
d. The 2 oldest are girls.
e. There is at least 1 girl.
3.77.
 and  alternate rolling a pair of dice, stopping either when  rolls the
sum 9 or when  rolls the sum 6. Assuming that  rolls first, find the
probability that the final roll is made by .
3.78. In a certain village, it is traditional for the eldest son (or the older son in a
two-son family) and his wife to be responsible for taking care of his parents as
they age. In recent years, however, the women of this village, not wanting that
responsibility, have not looked favorably upon marrying an eldest son.
a. If every family in the village has two children, what proportion of all
sons are older sons?
b. If every family in the village has three children, what proportion of all
sons are eldest sons?
Assume that each child is, independently, equally likely to be either a
𝑝
𝐴
𝐵
𝐴
𝐵
𝐴
𝐴
179 of 848

boy or a girl.
3.79. Suppose that  and  are mutually exclusive events of an experiment.
Show that if independent trials of this experiment are performed, then  will
occur before  with probability 
3.80. Consider an unending sequence of independent trials, where each trial
is equally likely to result in any of the outcomes 
 or . Given that outcome
 is the last of the three outcomes to occur, find the conditional probability that
a. the first trial results in outcome ;
b. the first two trials both result in outcome .
3.81.
 and  play a series of games. Each game is independently won by 
with probability  and by  with probability 
 They stop when the total
number of wins of one of the players is two greater than that of the other
player. The player with the greater number of total wins is declared the winner
of the series.
a. Find the probability that a total of 4 games are played.
b. Find the probability that A is the winner of the series.
3.82. In successive rolls of a pair of fair dice, what is the probability of getting
2 sevens before 6 even numbers?
3.83. In a certain contest, the players are of equal skill and the probability is 
that a specified one of the two contestants will be the victor. In a group of 
players, the players are paired off against each other at random. The 
winners are again paired off randomly, and so on, until a single winner
remains. Consider two specified contestants,  and 
 and define the events
 by
a. Find 
b. Find 
c. Let 
 Show that
and use this formula to check the answer you obtained in part (b).
Hint: Find 
 by conditioning on which of the events 
occur. In simplifying your answer, use the algebraic identity
𝐸
𝐹
𝐸
𝐹
𝑃ሺ𝐸ሻ/ሾ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻሿ.
1, 2,
3
3
1
1
𝐴
𝐵
𝐴
𝑝
𝐵
1 െ𝑝.
1
2
2௡
2௡െଵ
𝐴
𝐵,
𝐴௜, 𝑖൑𝑛, 𝐸
𝐴௜:
𝐴 plays in exactly 𝑖 contests
𝐸:
𝐴 and 𝐵 never play each other
𝑃ሺ𝐴௜ሻ, 𝑖ൌ1, …, 𝑛.
𝑃ሺ𝐸ሻ.
𝑃௡ൌ𝑃ሺ𝐸ሻ.
𝑃௡ൌ
1
2௡െ1 ൅2௡െ2
2௡െ1 ቆ1
2ቇ
ଶ
𝑃௡െଵ
𝑃ሺ𝐸ሻ
𝐴௜, 𝑖ൌ1, …, 𝑛
180 of 848

For another approach to solving this problem, note that there are a total
of 
 games played.
d. Explain why 
 games are played.
Number these games, and let 
 denote the event that  and  play
each other in game 
e. What is 
f. Use part (e) to find 
3.84. An investor owns shares in a stock whose present value is 25. She has
decided that she must sell her stock if it goes either down to 10 or up to 40. If
each change of price is either up 1 point with probability .55 or down 1 point
with probability .45, and the successive changes are independent, what is the
probability that the investor retires a winner?
3.85.
 and  flip coins.  starts and continues flipping until a tail occurs, at
which point  starts flipping and continues until there is a tail. Then  takes
over, and so on. Let 
 be the probability of the coin landing on heads when 
flips and 
 when  flips. The winner of the game is the first one to get
a. 2 heads in a row;
b. a total of 2 heads;
c. 3 heads in a row;
d. a total of 3 heads.
In each case, find the probability that  wins.
3.86. Die  has 4 red and 2 white faces, whereas die  has 2 red and 4 white
faces. A fair coin is flipped once. If it lands on heads, the game continues with
die ; if it lands on tails, then die  is to be used.
a. Show that the probability of red at any throw is 
b. If the first two throws result in red, what is the probability of red at the
third throw?
c. If red turns up at the first two throws, what is the probability that it is die
 that is being used?
3.87. An urn contains 12 balls, of which 4 are white. Three players—
 and
 successively draw from the urn,  first, then 
 then 
 then 
 and so on.
The winner is the first one to draw a white ball. Find the probability of winning
for each player if
a. each ball is replaced after it is drawn;
෍
௜ൌଵ
௡െଵ
𝑖𝑥௜െଵൌ1 െ𝑛𝑥௡െଵ൅ሺ𝑛െ1ሻ𝑥௡
ሺ1 െ𝑥ሻଶ
2௡െ1
2௡െ1
𝐵௜
𝐴
𝐵
𝑖, 𝑖ൌ1, …, 2௡െ1.
𝑃ሺ𝐵௜ሻ
𝑃ሺ𝐸ሻ.
𝐴
𝐵
𝐴
𝐵
𝐴
𝑃ଵ
𝐴
𝑃ଶ
𝐵
𝐴
𝐴
𝐵
𝐴
𝐵
1
2 .
𝐴
𝐴, 𝐵,
𝐶
𝐴
𝐵,
𝐶,
𝐴,
181 of 848

b. the balls that are withdrawn are not replaced.
3.88. Repeat Problem 3.87
 when each of the 3 players selects from his
own urn. That is, suppose that there are 3 different urns of 12 balls with 4
white balls in each urn.
3.89. Let 
 and suppose that  and  are, independently, equally
likely to be any of the 
 subsets (including the null set and  itself) of .
a. Show that
Hint: Let 
 denote the number of elements in . Use
Show that 
3.90. Consider an eight team tournament with the format given in Figure
3.6
. If the probability that team  beats team  if they play is 
 find the
probability that team  wins the tournament.
Figure 3.6
𝑆ൌሼ1, 2, …, 𝑛ሽ
𝐴
𝐵
2௡
𝑆
𝑆
𝑃ሼ𝐴⊂𝐵ሽൌቆ3
4ቇ
௡
𝑁ሺ𝐵ሻ
𝐵
𝑃ሼ𝐴⊂𝐵ሽൌ
∑
௜ൌ଴
௡
𝑃ሼ𝐴⊂𝐵||𝑁ሺ𝐵ሻൌ𝑖ሽ𝑃ሼ𝑁ሺ𝐵ሻൌ𝑖ሽ
𝑃൝𝐴𝐵ൌ Øൡൌቆ3
4ቇ
௡
.
𝑖
𝑗
𝑗
𝑖൅𝑗,
1
182 of 848

3.91. ConsiderExample 2a
, but now suppose that when the key is in a
certain pocket, there is a 
 percent chance that a search of that pocket will
not find the key. Let  and  be, respectively, the events that the key is in the
right-hand pocket of the jacket and that it is in the left-hand pocket. Also, let 
be the event that a search of the right-hand jacket pocket will be successful in
10
𝑅
𝐿
𝑆ோ
183 of 848

finding the key, and let 
 be the event that a search of the left-hand jacket
pocket will be unsuccessful and, thus, not find the key. Find 
 the
conditional probability that a search of the right-hand pocket will find the key
given that a search of the left-hand pocket did not, by
a. using the identity
determining 
 by conditioning on whether or not the key is in the
right-hand pocket, and determining 
 by conditioning on whether
or not the key is in the left-hand pocket;
b. using the identity
3.92. InExample 5e
, what is the conditional probability that the th coin was
selected given that the first  trials all result in heads?
3.93. In Laplace’s rule of succession (Example 5e
), are the outcomes of
the successive flips independent? Explain.
3.94. A person tried by a 3-judge panel is declared guilty if at least 2 judges
cast votes of guilty. Suppose that when the defendant is in fact guilty, each
judge will independently vote guilty with probability .7, whereas when the
defendant is in fact innocent, this probability drops to .2. If 70 percent of
defendants are guilty, compute the conditional probability that judge number 3
votes guilty given that
a. judges 1 and 2 vote guilty;
b. judges 1 and 2 cast 1 guilty and 1 not guilty vote;
c. judges 1 and 2 both cast not guilty votes.
Let 
 denote the event that judge  casts a guilty vote. Are these
events independent? Are they conditionally independent? Explain.
3.95. Each of  workers is independently qualified to do an incoming job with
probability . If none of them is qualified then the job is rejected; otherwise the
job is assigned to a randomly chosen one of the qualified workers. Find the
probability that worker  is assigned to the first incoming job. Hint: Condition
on whether or not at least one worker is qualified.
3.96. Suppose in the preceding problem that 
 and that worker  is
qualified with probability 
a. Find the probability that worker  is assigned to the first incoming job.
b. Given that worker  is assigned to the first job, find the conditional
probability that worker  was qualified for that job.
𝑈௅
𝑃ሺ𝑆ோ||𝑈௅ሻ,
𝑃ሺ𝑆ோ|𝑈௅ሻൌ𝑃ሺ𝑆ோ𝑈௅ሻ
𝑃ሺ𝑈௅ሻ
𝑃ሺ𝑆ோ𝑈௅ሻ
𝑃ሺ𝑈௅ሻ
𝑃ሺ𝑆ோ|𝑈௅ሻൌ𝑃ሺ𝑆ோ|𝑅𝑈௅ሻ𝑃ሺ𝑅|𝑈௅ሻ൅ 𝑃ሺ𝑆ோ||𝑅௖𝑈௅ሻ𝑃ሺ𝑅௖|𝑈௅ሻ
𝑖
𝑛
𝐸௜, 𝑖ൌ1, 2, 3
𝑖
𝑛
𝑝
1
𝑛ൌ2
𝑖
𝑝௜, 𝑖ൌ1, 2.
1
1
2
184 of 848

3.97. Each member of a population of size  is, independently of other
members, female with probability  or male with probability 
 Two
individuals of the same sex will, independently of other pairs, be friends with
probability ; whereas two individuals of opposite sex will be friends with
probability 
 Let 
 be the event that persons  and  are friends.
a. Find 
b. Are 
 and 
 independent.
c. Are 
 and 
 conditionally independent given the sex of person 1.
d. Find 
𝑛
𝑝
1 െ𝑝.
𝛼
𝛽.
𝐴௞,௥
𝑘
𝑟
𝑃൫𝐴ଵ,ଶ൯.
𝐴ଵ,ଶ
𝐴ଵ,ଷ
𝐴ଵ,ଶ
𝐴ଵ,ଷ
𝑃൫𝐴ଵ,ଶ𝐴ଵ,ଷ൯.
3.1. Show that if 
 then
3.2. Let 
 Express the following probabilities as simply as possible:
3.3. Consider a school community of 
 families, with 
 of them having 
children, 
 Consider the following two methods for
choosing a child:
A. Choose one of the 
 families at random and then randomly choose a
child from that family.
B. Choose one of the 
 children at random.
Show that method 1 is more likely than method 2 to result in the choice of a
firstborn child.
Hint: In solving this problem, you will need to show that
To do so, multiply the sums and show that for all pairs 
 the coefficient of
the term 
 is greater in the expression on the left than in the one on the
right.
3.4. A ball is in any one of  boxes and is in the th box with probability 
 If
the ball is in box  a search of that box will uncover it with probability 
 Show
that the conditional probability that the ball is in box  given that a search of
box  did not uncover it, is
𝑃ሺ𝐴ሻ൐0,
𝑃ሺ𝐴𝐵|𝐴ሻ൒𝑃ሺ𝐴𝐵|𝐴
𝐵ሻ
𝐴⊂𝐵.
𝑃ሺ𝐴|𝐵ሻ,  𝑃ሺ𝐴||𝐵௖ሻ,  𝑃ሺ𝐵|𝐴ሻ,  𝑃ሺ𝐵||𝐴௖ሻ
𝑚
𝑛௜
𝑖
𝑖ൌ1, …, 𝑘, ෍
௜ൌଵ
௞
𝑛௜ൌ𝑚.
𝑚
∑௜ൌଵ
௞
𝑖𝑛௜
෍
௜ൌଵ
௞
𝑖𝑛௜෍
௝ൌଵ
௞
𝑛௝
𝑗൒
෍
௜ൌଵ
௞
𝑛௜෍
௝ൌଵ
௞
𝑛௝
𝑖, 𝑗,
𝑛௜𝑛௝
𝑛
𝑖
𝑝௜.
𝑖,
𝛼௜.
𝑗,
𝑖
185 of 848

3.5.
a. Prove that if  and  are mutually exclusive, then
b. Prove that if 
 are mutually exclusive, then
3.6. Prove that if 
 are independent events, then
3.7.
a. An urn contains  white and 
 black balls. The balls are withdrawn one
at a time until only those of the same color are left. Show that with
probability 
 they are all white.
Hint: Imagine that the experiment continues until all the balls are
removed, and consider the last ball withdrawn.
b. A pond contains 3 distinct species of fish, which we will call the Red,
Blue, and Green fish. There are  Red,  Blue, and  Green fish.
Suppose that the fish are removed from the pond in a random order.
(That is, each selection is equally likely to be any of the remaining fish.)
What is the probability that the Red fish are the first species to become
extinct in the pond?
Hint: Write 
 and compute the probabilities on
the right by first conditioning on the last species to be removed.
3.8. Let 
 and  be events relating to the experiment of rolling a pair of
dice.
a. If
either prove that 
 or give a counterexample by defining
events 
 and  for which that relationship is not true.
b. If
𝑝௝
1 െ𝛼௜𝑝௜
if 𝑗്𝑖
൫1 െ𝛼௜൯𝑝௜
1 െ𝛼௜𝑝௜
if 𝑗ൌ𝑖
𝐸
𝐹
𝑃ሺ𝐸|𝐸∪𝐹ሻൌ
𝑃ሺ𝐸ሻ
𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻ
𝐸௜, 𝑖൒1
𝑃ሺ𝐸𝑗| ∪௜ൌଵ
ஶ
𝐸௜ሻൌ
𝑃൫𝐸௝൯
∑௜ൌଵ
ஶ
𝑃ሺ𝐸௜ሻ
𝐸ଵ, 𝐸ଶ, …, 𝐸௡
𝑃ሺ𝐸ଵ∪𝐸ଶ∪⋯∪𝐸௡ሻൌ1 െ
∏
௜ൌଵ
௡
ሾ1 െ𝑃ሺ𝐸௜ሻሿ
𝑛
𝑚
𝑛/ሺ𝑛൅𝑚ሻ,
𝑟
𝑏
𝑔
𝑃ሼ𝑅ሽൌ𝑃ሼ𝑅𝐵𝐺ሽ൅𝑃ሼ𝑅𝐺𝐵ሽ,
𝐴, 𝐵,
𝐶
𝑃ሺ𝐴||𝐶ሻ൐𝑃ሺ𝐵||𝐶ሻ  and  𝑃ሺ𝐴||𝐶௖ሻ൐𝑃ሺ𝐵||𝐶௖ሻ
𝑃ሺ𝐴ሻ൐𝑃ሺ𝐵ሻ
𝐴, 𝐵,
𝐶
186 of 848

either prove that 
 or give a counterexample by
defining events 
 and  for which that relationship is not true.
Hint: Let  be the event that the sum of a pair of dice is 
; let  be the event
that the first die lands on ; let  be the event that the second die lands on .
3.9. Consider two independent tosses of a fair coin. Let  be the event that
the first toss results in heads, let  be the event that the second toss results in
heads, and let  be the event that in both tosses the coin lands on the same
side. Show that the events 
 and  are pairwise independent—that is, 
and  are independent,  and  are independent, and  and  are
independent—but not independent.
3.10. Two percent of women age 45 who participate in routine screening have
breast cancer. Ninety percent of those with breast cancer have positive
mammographies. Eight percent of the women who do not have breast cancer
will also have positive mammographies. Given that a woman has a positive
mammography, what is the probability she has breast cancer?
3.11. In each of  independent tosses of a coin, the coin lands on heads with
probability . How large need  be so that the probability of obtaining at least
one head is at least 
3.12. Show that 
 then
Hint: Suppose that an infinite number of coins are to be flipped. Let 
 be the
probability that the th coin lands on heads, and consider when the first head
occurs.
3.13. The probability of getting a head on a single toss of a coin is . Suppose
that  starts and continues to flip the coin until a tail shows up, at which point
 starts flipping. Then  continues to flip until a tail comes up, at which point 
takes over, and so on. Let 
 denote the probability that  accumulates a
total of  heads before  accumulates 
. Show that
* 3.14. Suppose that you are gambling against an infinitely rich adversary and
at each stage you either win or lose 1 unit with respective probabilities  and
 Show that the probability that you eventually go broke is
𝑃ሺ𝐴||𝐶ሻ൐𝑃ሺ𝐴||𝐶௖ሻ and 𝑃ሺ𝐵||𝐶ሻ൐𝑃ሺ𝐵||𝐶௖ሻ
𝑃ሺ𝐴𝐵||𝐶ሻ൐𝑃ሺ𝐴𝐵||𝐶௖ሻ
𝐴, 𝐵,
𝐶
𝐶
10
𝐴
6
𝐵
6
𝐴
𝐵
𝐶
𝐴, 𝐵,
𝐶
𝐴
𝐵
𝐴
𝐶
𝐵
𝐶
𝑛
𝑝
𝑛
1
2 ?
0 ൑𝑎௜൑1, 𝑖ൌ1, 2, …,
∑௜ൌଵ
ஶ
ቂ𝑎௜∏௝ൌଵ
௜െଵቀ1 െ𝑎௝ቁቃ൅∏௜ൌଵ
ஶ
ቀ1 െ𝑎௜ቁൌ1
𝑎௜
𝑖
𝑝
𝐴
𝐵
𝐵
𝐴
𝑃௡,௠
𝐴
𝑛
𝐵
𝑚
𝑃௡,௠ൌ𝑝𝑃௡െଵ,௠൅൫1 െ𝑝൯൫1 െ𝑃௠,௡൯
𝑝
1 െ𝑝.
1
 if 𝑝൑1
2
3𝑝𝑡ሺ𝑞/𝑝ሻ௜
 if 𝑝൐1
2
187 of 848

where 
 and where  is your initial fortune.
3.15. Independent trials that result in a success with probability  are
successively performed until a total of  successes is obtained. Show that the
probability that exactly  trials are required is
Use this result to solve the problem of the points (Example 4j
.
Hint: In order for it to take  trials to obtain  successes, how many successes
must occur in the first 
 trials?
3.16. Independent trials that result in a success with probability  and a failure
with probability 
 are called Bernoulli trials. Let 
 denote the probability
that  Bernoulli trials result in an even number of successes (0 being
considered an even number). Show that
and use this formula to prove (by induction) that
3.17. Suppose that  independent trials are performed, with trial  being a
success with probability 
 Let 
 denote the probability that the total
number of successes that result is an odd number.
a. Find 
 for 
b. Conjecture a general formula for 
c. Derive a formula for 
 in terms of 
d. Verify that your conjecture in part (b) satisfies the recursive formula in
part (c). Because the recursive formula has a unique solution, this then
proves that your conjecture is correct.
3.18. Let 
 denote the probability that no run of 3 consecutive heads appears
in  tosses of a fair coin. Show that
Find 
Hint: Condition on the first tail.
3.19. Consider the gambler’s ruin problem, with the exception that  and 
agree to play no more than  games. Let 
 denote the probability that 
winds up with all the money when  starts with  and  starts with 
Derive an equation for 
 in terms of 
 and 
 and compute
𝑞ൌ1 െ𝑝
𝑖
𝑝
𝑟
𝑛
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ቆ1 െ𝑝ቇ
௡െ௥
𝑛
𝑟
𝑛െ1
𝑝
1 െ𝑝
𝑃௡
𝑛
𝑃௡ൌ𝑝ሺ1 െ𝑃௡െଵሻ൅ሺ1 െ𝑝ሻ𝑃௡െଵ 𝑛൒1
𝑃௡ൌ1 ൅ሺ1 െ2𝑝ሻ௡
2
𝑛
𝑖
1/ሺ2𝑖൅1ሻ.
𝑃௡
𝑃௡
𝑛ൌ1, 2, 3, 4, 5.
𝑃௡.
𝑃௡
𝑃௡െଵ.
𝑄௡
𝑛
𝑄௡
ൌ
1
2 𝑄௡െଵ൅1
4 𝑄௡െଶ൅1
8 𝑄௡െଷ
𝑄଴
ൌ
𝑄ଵൌ𝑄ଶൌ1
𝑄଼.
𝐴
𝐵
𝑛
𝑃௡,௜
𝐴
𝐴
𝑖
𝐵
𝑁െ𝑖.
𝑃௡,௜
𝑃௡െଵ, ௜൅ଵ
𝑃௡െଵ, ௜െଵ,
188 of 848

3.20. Consider two urns, each containing both white and black balls. The
probabilities of drawing white balls from the first and second urns are,
respectively,  and 
 Balls are sequentially selected with replacement as
follows: With probability 
 a ball is initially chosen from the first urn, and with
probability 
 it is chosen from the second urn. The subsequent selections
are then made according to the rule that whenever a white ball is drawn (and
replaced), the next ball is drawn from the same urn, but when a black ball is
drawn, the next ball is taken from the other urn. Let 
 denote the probability
that the th ball is chosen from the first urn. Show that
and use this formula to prove that
Let 
 denote the probability that the th ball selected is white. Find 
 Also,
compute 
 and 
3.21. The Ballot Problem. In an election, candidate  receives  votes and
candidate  receives 
 votes, where 
 Assuming that all of the
 orderings of the votes are equally likely, let 
 denote the
probability that  is always ahead in the counting of the votes.
a. Compute 
b. Find 
c. On the basis of your results in parts (a) and (b), conjecture the value of
d. Derive a recursion for 
 in terms of 
 and 
 by
conditioning on who receives the last vote.
e. Use part (d) to verify your conjecture in part (c) by an induction proof on
3.22. As a simplified model for weather forecasting, suppose that the weather
(either wet or dry) tomorrow will be the same as the weather today with
probability . Show that the weather is dry on January 1, then 
 the
probability that it will be dry  days later, satisfies
Prove that
𝑃଻,ଷ, 𝑁ൌ5.
𝑝
𝑝′.
𝛼,
1 െ𝛼,
𝛼௡
𝑛
𝛼௡൅ଵൌ𝛼௡ሺ𝑝൅𝑝′ െ1ሻ൅1 െ𝑝′ 𝑛൒1
𝛼௡ൌ
1 െ𝑝′
2 െ𝑝െ𝑝′ ൅ቆ𝛼െ
1 െ𝑝′
2 െ𝑝െ𝑝′ ቇൈ ሺ𝑝൅𝑝′ െ1ሻ௡െଵ
𝑃௡
𝑛
𝑃௡.
lim௡→ஶ𝛼௡
lim௡→ஶ𝑃௡.
𝐴
𝑛
𝐵
𝑚
𝑛൐𝑚.
ሺ𝑛൅𝑚ሻ!/𝑛! 𝑚!
𝑃௡,௠
𝐴
𝑃ଶ,ଵ, 𝑃ଷ,ଵ, 𝑃ଷ,ଶ, 𝑃ସ,ଵ, 𝑃ସ,ଶ, 𝑃ସ,ଷ.
𝑃௡,ଵ, 𝑃௡,ଶ.
𝑃௡,௠.
𝑃௡,௠
𝑃௡െଵ,௠
𝑃௡,௠െଵ
𝑛൅𝑚.
𝑝
𝑃௡,
𝑛
𝑃௡
ൌሺ2𝑝െ1ሻ𝑃௡െଵ൅ሺ1 െ𝑝ሻ 𝑛൒1
𝑃଴
ൌ
1
𝑃௡ൌ1
2 ൅1
2ቆ2𝑝െ1ቇ
௡
 𝑛൒0
189 of 848

3.23. A bag contains  white and  black balls. Balls are chosen from the bag
according to the following method:
A. A ball is chosen at random and is discarded.
B. A second ball is then chosen. If its color is different from that of the
preceding ball, it is replaced in the bag and the process is repeated
from the beginning. If its color is the same, it is discarded and we start
from step 2.
In other words, balls are sampled and discarded until a change in color
occurs, at which point the last ball is returned to the urn and the process starts
anew. Let 
 denote the probability that the last ball in the bag is white.
Prove that
Hint: Use induction on 
3.24. A round-robin tournament of  contestants is a tournament in which
each of the 
 pairs of contestants play each other exactly once, with the
outcome of any play being that one of the contestants wins and the other
loses. For a fixed integer 
 a question of interest is whether it is
possible that the tournament outcome is such that for every set of  players,
there is a player who beat each member of that set. Show that if
then such an outcome is possible.
Hint: Suppose that the results of the games are independent and that each
game is equally likely to be won by either contestant. Number the 
 sets of
 contestants, and let 
 denote the event that no contestant beat all of the 
players in the th set. Then use Boole’s inequality to bound 
3.25. Prove directly that
3.26. Prove the equivalence of Equations (5.11)
 and (5.12
).
3.27. Extend the definition of conditional independence to more than 2 events.
3.28. Prove or give a counterexample. If 
 and 
 are independent, then they
are conditionally independent given .
3.29. In Laplace’s rule of succession (Example 5e
), show that if the first 
flips all result in heads, then the conditional probability that the next 
 flips
𝑎
𝑏
𝑃௔,௕
𝑃௔,௕ൌ1
2
𝑘≡𝑎൅𝑏.
𝑛
ቆ𝑛
2ቇ
𝑘, 𝑘൏𝑛,
𝑘
ቆ𝑛
𝑘ቇ൥1 െቆ1
2ቇ
௞
൩
௡െ௞
൏1
ቆ𝑛
𝑘ቇ
𝑘
𝐵௜
𝑘
𝑖
𝑃൬∪
௜𝐵௜൰.
𝑃ሺ𝐸||𝐹ሻൌ𝑃ሺ𝐸||𝐹𝐺ሻ𝑃ሺ𝐺||𝐹ሻ൅𝑃ሺ𝐸||𝐹𝐺௖ሻ𝑃ሺ𝐺௖||𝐹ሻ
𝐸ଵ
𝐸ଶ
𝐹
𝑛
𝑚
190 of 848

also result in all heads is approximately 
 when  is large.
3.30. In Laplace’s rule of succession (Example 5e
), suppose that the first 
flips resulted in  heads and 
 tails. Show that the probability that the
 flip turns up heads is 
 To do so, you will have to prove
and use the identity
Hint: To prove the identity, let 
 Integrating by
parts yields
Starting with 
 prove the identity by induction on 
.
3.31. Suppose that a nonmathematical, but philosophically minded, friend of
yours claims that Laplace’s rule of succession must be incorrect because it
can lead to ridiculous conclusions. “For instance,” says he, “the rule states
that if a boy is 10 years old, having lived 10 years, the boy has probability 
of living another year. On the other hand, if the boy has an 80-year-old
grandfather, then, by Laplace’s rule, the grandfather has probability 
 of
surviving another year. However, this is ridiculous. Clearly, the boy is more
likely to survive an additional year than the grandfather is.” How would you
answer your friend?
ሺ𝑛൅1ሻ/ሺ𝑛൅𝑚െ1ሻ
𝑘
𝑛
𝑟
𝑛െ𝑟
ሺ𝑛൅1ሻ
ሺ𝑟൅1ሻ/ሺ𝑛൅2ሻ.
඲
଴
ଵ
𝑦௡ቆ1 െ𝑦ቇ
௠
𝑑𝑦ൌ
𝑛!𝑚!
ሺ𝑛൅𝑚൅1ሻ!
𝐶ቆ𝑛, 𝑚ቇൌ඲
଴
ଵ
𝑦௡ቆ1 െ𝑦ቇ
௠
𝑑𝑦.
𝐶൬𝑛, 𝑚൰ൌ
𝑚
𝑛൅1 𝐶൬𝑛൅1, 𝑚െ1൰
𝐶ሺ𝑛, 0ሻൌ1/ሺ𝑛൅1ሻ,
𝑚
11
12
81
82
3.1. In a game of bridge, West has no aces. What is the probability of
his partner’s having (a) no aces? (b) 2 or more aces? (c) What would
the probabilities be if West had exactly 1 ace?
3.2. The probability that a new car battery functions for more than
10,000 miles is .8, the probability that it functions for more than
20,000 miles is .4, and the probability that it functions for more than
30,000 miles is .1. If a new car battery is still working after 10,000
miles, what is the probability that
a. its total life will exceed 20,000 miles?
b. its additional life will exceed 20,000 miles?
191 of 848

3.3. How can 20 balls, 10 white and 10 black, be put into two urns so
as to maximize the probability of drawing a white ball if an urn is
selected at random and a ball is drawn at random from it?
3.4. Urn  contains 2 white balls and 1 black ball, whereas urn 
contains 1 white ball and 5 black balls. A ball is drawn at random
from urn  and placed in urn . A ball is then drawn from urn . It
happens to be white. What is the probability that the ball transferred
was white?
3.5. An urn has  red and  white balls that are randomly removed
one at a time. Let 
 be the event that the th ball removed is red.
Find
a. 
b. 
c. 
3.6. An urn contains  black balls and  red balls. One of the balls is
drawn at random, but when it is put back in the urn,  additional balls
of the same color are put in with it. Now, suppose that we draw
another ball. Show that the probability that the first ball was black,
given that the second ball drawn was red, is 
3.7. A friend randomly chooses two cards, without replacement, from
an ordinary deck of 52 playing cards. In each of the following
situations, determine the conditional probability that both cards are
aces.
a. You ask your friend if one of the cards is the ace of spades,
and your friend answers in the affirmative.
b. You ask your friend if the first card selected is an ace, and
your friend answers in the affirmative.
c. You ask your friend if the second card selected is an ace, and
your friend answers in the affirmative.
d. You ask your friend if either of the cards selected is an ace,
and your friend answers in the affirmative.
3.8. Show that
Suppose that, before new evidence is observed, the hypothesis  is
three times as likely to be true as is the hypothesis . If the new
evidence is twice as likely when  is true than it is when  is true,
which hypothesis is more likely after the evidence has been
observed?
𝐴
𝐵
𝐴
𝐵
𝐵
𝑟
𝑤
𝑅௜
𝑖
𝑃ሺ𝑅௜ሻ
𝑃ሺ𝑅ହ||𝑅ଷሻ
𝑃ሺ𝑅ଷ||𝑅ହሻ
𝑏
𝑟
𝑐
𝑏/ሺ𝑏൅𝑟൅𝑐ሻ.
𝑃ሺ𝐻||𝐸ሻ
𝑃ሺ𝐺||𝐸ሻൌ𝑃ሺ𝐻ሻ
𝑃ሺ𝐺ሻ
𝑃ሺ𝐸||𝐻ሻ
𝑃ሺ𝐸||𝐺ሻ
𝐻
𝐺
𝐺
𝐻
192 of 848

3.9. You ask your neighbor to water a sickly plant while you are on
vacation. Without water, it will die with probability .8; with water, it will
die with probability .15. You are 90 percent certain that your neighbor
will remember to water the plant.
a. What is the probability that the plant will be alive when you
return?
b. If the plant is dead upon your return, what is the probability
that your neighbor forgot to water it?
3.10. Six balls are to be randomly chosen from an urn containing 8
red, 10 green, and 12 blue balls.
a. What is the probability at least one red ball is chosen?
b. Given that no red balls are chosen, what is the conditional
probability that there are exactly 2 green balls among the 6
chosen?
3.11. A type C battery is in working condition with probability 
whereas a type D battery is in working condition with probability 
 A
battery is randomly chosen from a bin consisting of 8 type C and 6
type D batteries.
a. What is the probability that the battery works?
b. Given that the battery does not work, what is the conditional
probability that it was a type C battery?
3.12. Maria will take two books with her on a trip. Suppose that the
probability that she will like book 1 is 
 the probability that she will
like book 2 is 
 and the probability that she will like both books is
 Find the conditional probability that she will like book  given that
she did not like book 1.
3.13. Balls are randomly removed from an urn that initially contains
20 red and 10 blue balls.
a. What is the probability that all of the red balls are removed
before all of the blue ones have been removed? Now suppose
that the urn initially contains 20 red, 10 blue, and 8 green
balls.
b. Now what is the probability that all of the red balls are
removed before all of the blue ones have been removed?
c. What is the probability that the colors are depleted in the order
blue, red, green?
d. What is the probability that the group of blue balls is the first of
the three groups to be removed?
. 7,
. 4.
. 6,
. 5,
. 4.
2
193 of 848

3.14. A coin having probability 
 of landing on heads is flipped. 
observes the result—either heads or tails—and rushes off to tell .
However, with probability 
 will have forgotten the result by the
time he reaches . If  has forgotten, then, rather than admitting this
to 
 he is equally likely to tell  that the coin landed on heads or that
it landed tails. (If he does remember, then he tells  the correct
result.)
a. What is the probability that  is told that the coin landed on
heads?
b. What is the probability that  is told the correct result?
c. Given that  is told that the coin landed on heads, what is the
probability that it did in fact land on heads?
3.15. In a certain species of rats, black dominates over brown.
Suppose that a black rat with two black parents has a brown sibling.
a. What is the probability that this rat is a pure black rat (as
opposed to being a hybrid with one black and one brown
gene)?
b. Suppose that when the black rat is mated with a brown rat, all
5 of their offspring are black. Now what is the probability that
the rat is a pure black rat?
3.16.
a. In Problem 3.70b
, find the probability that a current flows
from  to 
 by conditioning on whether relay 1 closes.
b. Find the conditional probability that relay 3 is closed given that
a current flows from  to .
3.17. For the -out-of-  system described in Problem 3.71
,
assume that each component independently works with probability
 Find the conditional probability that component 1 is working, given
that the system works, when
a. 
;
b. 
3.18. Mr. Jones has devised a gambling system for winning at
roulette. When he bets, he bets on red and places a bet only when
the 10 previous spins of the roulette have landed on a black number.
He reasons that his chance of winning is quite large because the
probability of 11 consecutive spins resulting in black is quite small.
What do you think of this system?
. 8
𝐴
𝐵
. 4, 𝐴
𝐵
𝐴
𝐵,
𝐵
𝐵
𝐵
𝐵
𝐵
𝐴
𝐵,
𝐴
𝐵
𝑘
𝑛
1
2 .
𝑘ൌ1, 𝑛ൌ2
𝑘ൌ2, 𝑛ൌ3.
194 of 848

3.19. Three players simultaneously toss coins. The coin tossed by
 turns up heads with probability 
 If one person
gets an outcome different from those of the other two, then he is the
odd man out. If there is no odd man out, the players flip again and
continue to do so until they get an odd man out. What is the
probability that  will be the odd man?
3.20. Suppose that there are  possible outcomes of a trial, with
outcome  resulting with probability 
 If two
independent trials are observed, what is the probability that the result
of the second trial is larger than that of the first?
3.21. If  flips 
 and  flips  fair coins, show that the probability
that  gets more heads than  is 
Hint: Condition on which player has more heads after each has
flipped  coins. (There are three possibilities.)
3.22. Prove or give counterexamples to the following statements:
a. If  is independent of  and  is independent of 
 then  is
independent of 
b. If  is independent of 
 and  is independent of 
 and
 then  is independent of 
c. If  is independent of 
 and  is independent of 
 and  is
independent of FG, then  is independent of EF.
3.23. Let  and  be events having positive probability. State
whether each of the following statements is (i) necessarily true, (ii)
necessarily false, or (iii) possibly true.
a. If  and  are mutually exclusive, then they are independent.
b. If  and  are independent, then they are mutually exclusive.
c. 
 and  and  are mutually exclusive.
d. 
 and  and  are independent.
3.24. Rank the following from most likely to least likely to occur:
A. A fair coin lands on heads.
B. Three independent trials, each of which is a success with
probability .8, all result in successes.
C. Seven independent trials, each of which is a success with
probability .9, all result in successes.
3.25. Two local factories,  and 
 produce radios. Each radio
produced at factory  is defective with probability .05, whereas each
𝐴ሺ𝐵ሻሾ𝐶ሿ
𝑃ଵሺ𝑃ଶሻሾ𝑃ଷሿ.
𝐴
𝑛
𝑖
𝑝௜, 𝑖ൌ1, …, 𝑛, ෍
௜ൌଵ
௡
𝑝௜ൌ1.
𝐴
𝑛൅1
𝐵
𝑛
𝐴
𝐵
1
2
𝑛
𝐸
𝐹
𝐸
𝐺,
𝐸
𝐹∪𝐺.
𝐸
𝐹,
𝐸
𝐺,
𝐹𝐺ൌ Ø ,
𝐸
𝐹∪𝐺.
𝐸
𝐹,
𝐹
𝐺,
𝐸
𝐺
𝐴
𝐵
𝐴
𝐵
𝐴
𝐵
𝑃ሺ𝐴ሻൌ𝑃ሺ𝐵ሻൌ. 6,
𝐴
𝐵
𝑃ሺ𝐴ሻൌ𝑃ሺ𝐵ሻൌ. 6,
𝐴
𝐵
𝐴
𝐵,
𝐴
195 of 848

one produced at factory  is defective with probability .01. Suppose
you purchase two radios that were produced at the same factory,
which is equally likely to have been either factory  or factory . If
the first radio that you check is defective, what is the conditional
probability that the other one is also defective?
3.26. Show that if 
 then 
3.27. An urn initially contains 1 red and 1 blue ball. At each stage, a
ball is randomly withdrawn and replaced by two other balls of the
same color. (For instance, if the red ball is initially chosen, then there
would be 2 red and 1 blue balls in the urn when the next selection
occurs.) Show by mathematical induction that the probability that
there are exactly  red balls in the urn after  stages have been
completed is 
3.28. A total of 2  cards, of which 2 are aces, are to be randomly
divided among two players, with each player receiving  cards. Each
player is then to declare, in sequence, whether he or she has
received any aces. What is the conditional probability that the second
player has no aces, given that the first player declares in the
affirmative, when (a) 
? (b) 
? (c) 
? To what does
the probability converge as  goes to infinity? Why?
3.29. There are  distinct types of coupons, and each coupon
obtained is, independently of prior types collected, of type  with
probability 
a. If  coupons are collected, what is the probability that one of
each type is obtained?
b. Now suppose that 
 Let 
 be the
event that there are no type  coupons among the  collected.
Apply the inclusion–exclusion identity for the probability of the
union of events to 
 to prove the identity
3.30. Show that for any events  and 
Hint: Compute 
 by conditioning on whether  occurs.
3.31.
a. If the odds of  is 
 what is the probability that  occurs.
b. If the odds of  is  what is the probability that  occurs.
𝐵
𝐴
𝐵
𝑃ሺ𝐴||𝐵ሻൌ1,
𝑃ሺ𝐵௖||𝐴௖ሻൌ1.
𝑖
𝑛
1
𝑛൅1, 1 ൑𝑖൑𝑛൅1.
𝑛
𝑛
𝑛ൌ2
𝑛ൌ10
𝑛ൌ100
𝑛
𝑛
𝑖
𝑝௜, ∑௜ൌଵ
௡
𝑝௜ൌ1.
𝑛
𝑝ଵൌ𝑝ଶൌ… ൌ𝑝௡ൌ1/𝑛.
𝐸௜
𝑖
𝑛
𝑃ሺ∪௜𝐸௜ሻ
𝑛! ൌ
෍
௞ൌ଴
௡
ሺെ1ሻ௞ቆ𝑛
𝑘ቇሺ𝑛െ𝑘ሻ௡
𝐸
𝐹,
𝑃ሺ𝐸||𝐸∪𝐹ሻ൒𝑃ሺ𝐸||𝐹ሻ
𝑃ሺ𝐸||𝐸∪𝐹ሻ
𝐹
𝐴
2/3,
𝐴
𝐴
5,
𝐴
196 of 848

3.32. A fair coin is flipped  times. Let  be the event that all flips
land heads.
a. What is the odds of the event .
b. What is the conditional odds of the event  given that at least
one of the coins landed heads.
3.33. If the events 
 are independent, show that
3.34. Players 
 are in a contest. Two of them are randomly
chosen to play a game in round one, with the winner then playing the
remaining player in round two. The winner of the round two game is
the winner of the contest. Assuming that all games are independent
and that  wins when playing against  with probability 
 find the
probability that  is the winner of the contest. Given that  is the
winner, what is the conditional probability that  did not play in the
first round.
3.35. If  balls are randomly chosen from an urn containing  red, 
white,  blue, and  green balls, find the conditional probability they
are all white given that all balls are of the same color.
3.36. In a 4 player tournament, player 1 plays player 2, player 3 plays
player 4, with the winners then playing for the championship.
Suppose that a game between player  and player  is won by player
 with probability 
 Find the probability that player  wins the
championship.
3.37. In a tournament involving players 
 players  and  play a
game, with the loser departing and the winner then playing against
player  with the loser of that game departing and the winner then
playing player 
 and so on. The winner of the game against player 
is the tournament winner. Suppose that a game between players 
and  is won by player  with probability 
a. Find the probability that player  is the tournament winner.
b. If 
 find the probability that player  is the tournament
winner.
3
𝐸
𝐸
𝐸
𝐸, 𝐹, 𝐺
 𝑃ሺ𝐸||𝐹𝐺௖ሻൌ𝑃ሺ𝐸ሻ.
1, 2, 3,
𝑖
𝑗
𝑖
𝑖൅𝑗,
1
1
1
4
4
5
6
7
𝑖
𝑗
𝑖
𝑖
𝑖൅𝑗.
1
1, …, 𝑛,
1
2
3,
4,
𝑛
𝑖
𝑗
𝑖
𝑖
𝑖൅𝑗.
3
𝑛ൌ4,
4
197 of 848

4.1 Random Variables
4.2 Discrete Random Variables
4.3 Expected Value
4.4 Expectation of a Function of a Random Variable
4.5 Variance
4.6 The Bernoulli and Binomial Random Variables
4.7 The Poisson Random Variable
4.8 Other Discrete Probability Distributions
4.9 Expected Value of Sums of Random Variables
4.10 Properties of the Cumulative Distribution function
When an experiment is performed, we are frequently interested mainly in some
function of the outcome as opposed to the actual outcome itself. For instance, in
tossing dice, we are often interested in the sum of the two dice and are not really
concerned about the separate values of each die. That is, we may be interested in
knowing that the sum is 7 and may not be concerned over whether the actual
outcome was (1, 6), (2, 5), (3, 4), (4, 3), (5, 2), or (6, 1). Also, in flipping a coin, we
may be interested in the total number of heads that occur and not care at all about
the actual head–tail sequence that results. These quantities of interest, or, more
formally, these real-valued functions defined on the sample space, are known as
random variables.
Because the value of a random variable is determined by the outcome of the
experiment, we may assign probabilities to the possible values of the random
variable.
Example 1a
Suppose that our experiment consists of tossing 3 fair coins. If we let  denote
the number of heads that appear, then  is a random variable taking on one of
the values 0, 1, 2, and 3 with respective probabilities
𝑌
𝑌
198 of 848

Since  must take on one of the values 0 through 3, we must have
which, of course, is in accord with the preceding probabilities.
Example 1b
A life insurance agent has  elderly clients, each of whom has a life insurance
policy that pays $100,000 upon death. Let  be the event that the younger one
dies in the following year, and let  be the event that the older one dies in the
following year. Assume that  and  are independent, with respective
probabilities 
 and 
 If  denotes the total amount of money
(in units of 
) that will be paid out this year to any of these clients’
beneficiaries, then  is a random variable that takes on one of the possible
values 
 with respective probabilities
Example 1c
Four balls are to be randomly selected, without replacement, from an urn that
contains 
 balls numbered 1 through 20. If  is the largest numbered ball
selected, then  is a random variable that takes on one of the values 
Because each of the 
 possible selections of  of the 
 balls is equally likely,
the probability that  takes on each of its possible values is
𝑃ሼ𝑌ൌ0ሽ
ൌ𝑃ሼሺ𝑡, 𝑡, 𝑡ሻሽൌ1
8
𝑃ሼ𝑌ൌ1ሽ
ൌ𝑃ሼሺ𝑡, 𝑡, ℎሻ, ሺ𝑡, ℎ, 𝑡ሻ, ሺℎ, 𝑡, 𝑡ሻሽൌ3
8
𝑃ሼ𝑌ൌ2ሽ
ൌ𝑃ሼሺ𝑡, ℎ, ℎሻ, ሺℎ, 𝑡, ℎሻ, ሺℎ, ℎ, 𝑡ሻሽൌ3
8
𝑃ሼ𝑌ൌ3ሽ
ൌ𝑃ሼሺℎ, ℎ, ℎሻሽൌ1
8
𝑌
1 ൌ𝑃൭
௒ൌ௜
ଷ
ሼ𝑌ൌ𝑖ሽ൱ൌ෍
௜ൌ଴
ଷ
𝑃ሼ𝑌ൌ𝑖ሽ
2
𝑌
𝑂
𝑌
𝑂
𝑃ሺ𝑌ሻൌ. 05
𝑃ሺ𝑂ሻൌ. 10 .
𝑋
$100, 000
𝑋
0, 1, 2
𝑃ሼ𝑋ൌ0ሽ
ൌ
𝑃ሺ𝑌௖𝑂௖ሻൌ𝑃ሺ𝑌௖ሻ𝑃ሺ𝑂௖ሻൌሺ.95ሻሺ.9ሻൌ.855
𝑃ሼ𝑋ൌ1ሽ
ൌ
𝑃ሺ𝑌𝑂௖ሻ൅𝑃ሺ𝑌௖𝑂ሻൌሺ.05ሻሺ.9ሻ൅ሺ.95ሻሺ.1ሻൌ.140
𝑃ሼ𝑋ൌ2ሽ
ൌ
𝑃ሺ𝑌𝑂ሻൌሺ.05ሻሺ.1ሻൌ.005
20
𝑋
𝑋
4, 5, …, 20 .
ቆ20
4 ቇ
4
20
𝑋
199 of 848

This is so because the number of selections that result in 
 is the number of
selections that result in ball numbered  and three of the balls numbered 
through 
 being selected. As there are 
 such selections, the
preceding equation follows.
Suppose now that we want to determine 
. One way, of course, is to just
use the preceding to obtain
However, a more direct approach for determining 
 would be to use
where the preceding results because  will be less than or equal to 
 when the
 balls chosen are among balls numbered  through 
.
Example 1d
Independent trials consisting of the flipping of a coin having probability  of
coming up heads are continually performed until either a head occurs or a total of
 flips is made. If we let  denote the number of times the coin is flipped, then 
is a random variable taking on one of the values 
 with respective
probabilities
𝑃ሼ𝑋ൌ𝑖ሽൌ
ቆ𝑖െ1
3 ቇ
ቆ20
4 ቇ
,
𝑖ൌ4, …, 20
𝑋ൌ𝑖
𝑖
1
𝑖െ1
ቆ1
1ቇቆ𝑖െ1
3 ቇ
𝑃ሼ𝑋൐10ሽ
𝑃ሼ𝑋൐10ሽൌ
෍
௜ൌଵଵ
ଶ଴
𝑃ሼ𝑋ൌ𝑖ሽൌ
෍
௜ൌଵଵ
ଶ଴
ቆ𝑖െ1
3 ቇ
ቆ20
4 ቇ
𝑃ሺ𝑋൐10ሻ
𝑃ሼ𝑋൐10ሽൌ1 െ𝑃ሼ𝑋൑10ሽൌ1 െ
ቆ10
4 ቇ
ቆ20
4 ቇ
𝑋
10
4
1
10
𝑝
𝑛
𝑋
𝑋
1, 2, 3, …, 𝑛
200 of 848

As a check, note that
Example 1e
Suppose that there are  distinct types of coupons and that each time one
obtains a coupon, it is, independently of previous selections, equally likely to be
any one of the  types. One random variable of interest is , the number of
coupons that need to be collected until one obtains a complete set of at least one
of each type. Rather than derive 
 directly, let us start by considering the
probability that  is greater than . To do so, fix  and define the events
 as follows: 
 is the event that no type  coupon is contained among
the first  coupons collected, 
. Hence, by the inclusion-exclusion
identity
𝑃ሼ𝑋ൌ1ሽ
ൌ
𝑃ሼℎሽൌ𝑝
𝑃ሼ𝑋ൌ2ሽ
ൌ
𝑃ሼሺ𝑡, ℎሻሽൌሺ1 െ𝑝ሻ𝑝
𝑃ሼ𝑋ൌ3ሽ
ൌ
𝑃ሼሺ𝑡, 𝑡, ℎሻሽൌሺ1 െ𝑝ሻଶ𝑝
⋮
𝑃ሼ𝑋ൌ𝑛െ1ሽ
ൌ
𝑃൝ሺ𝑡, 𝑡, …, 𝑡,
௡െଶ
ℎሻൡൌሺ1 െ𝑝ሻ௡െଶ𝑝
𝑃ሼ𝑋ൌ𝑛ሽ
ൌ
𝑃൝ሺ𝑡, 𝑡, …, 𝑡,
௡െଵ
𝑡ሻ, ሺ𝑡, 𝑡, …, 𝑡,
௡െଵ
ℎሻൡൌሺ1 െ𝑝ሻ௡െଵ
𝑃ቆ
௑ൌ௜
௡
ሼ𝑋ൌ𝑖ሽቇ
ൌ
෍
௜ൌଵ
௡
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ
෍
௜ൌଵ
௡െଵ
𝑝ሺ1 െ𝑝ሻ௜െଵ൅ሺ1 െ𝑝ሻ௡െଵ
ൌ𝑝൥1 െሺ1 െ𝑝ሻ௡െଵ
1 െሺ1 െ𝑝ሻ
൩൅ሺ1 െ𝑝ሻ௡െଵ
ൌ1 െሺ1 െ𝑝ሻ௡െଵ൅ሺ1 െ𝑝ሻ௡െଵ
ൌ1
𝑟
𝑟
𝑇
𝑃ሼ𝑇ൌ𝑛ሽ
𝑇
𝑛
𝑛
𝐴ଵ, 𝐴ଶ, …, 𝐴௥
𝐴௝
𝑗
𝑛
𝑗ൌ1, …, 𝑟
201 of 848

Now, 
 will occur if each of the  coupons collected is not of type . Since each
of the coupons will not be of type  with probability 
, we have, by the
assumed independence of the types of successive coupons,
Also, the event 
 will occur if none of the first  coupons collected is of
either type 
 or type 
. Thus, again using independence, we see that
The same reasoning gives
and we see that for 
,
The probability that  equals  can now be obtained from the preceding formula
by the use of
𝑃ሼ𝑇൐𝑛ሽ
ൌ
𝑃ቆ
௝ൌଵ
௥
𝐴௝ቇ
ൌ
∑
௝
𝑃ሺ𝐴௝ሻെ
∑
௝భழ௝మ
∑𝑃ሺ𝐴௝భ𝐴௝మሻ൅⋯
൅ሺെ1ሻ௞൅ଵ
∑
௝భழ௝మழ⋯  ழ௝ೖ
∑∑𝑃ሺ𝐴௝భ𝐴௝మ⋯𝐴௝ೖሻ⋯
൅ሺെ1ሻ௥൅ଵ𝑃ሺ𝐴ଵ𝐴ଶ⋯𝐴௥ሻ
𝐴௝
𝑛
𝑗
𝑗
ሺ𝑟െ1ሻ/𝑟
𝑃ሺ𝐴௝ሻൌቆ𝑟െ1
𝑟
ቇ
௡
𝐴௝భ𝐴௝మ
𝑛
𝑗ଵ
𝑗ଶ
𝑃ሺ𝐴௝భ𝐴௝మሻൌቆ𝑟െ2
𝑟
ቇ
௡
𝑃ሺ𝐴௝భ𝐴௝మ⋯𝐴௝ೖሻൌቆ𝑟െ𝑘
𝑟
ቇ
௡
𝑛൐0
𝑃ሼ𝑇൐𝑛ሽ
ൌ
𝑟ቆ𝑟െ1
𝑟
ቇ
௡
െቆ𝑟
2ቇቆ𝑟െ2
𝑟
ቇ
௡
൅ቆ𝑟
3ቇቆ𝑟െ3
𝑟
ቇെ⋯
൅ሺെ1ሻ௥ቆ
𝑟
𝑟െ1ቇቆ1
𝑟ቇ
௡
ൌ
෍
௜ൌଵ
௥െଵ
ቆ𝑟
𝑖ቇ൬𝑟െ𝑖
𝑟
൰
௡
ሺെ1ሻ௜൅ଵ
(1.1)
𝑇
𝑛
𝑃ሼ𝑇൐𝑛െ1ሽൌ𝑃ሼ𝑇ൌ𝑛ሽ൅𝑃ሼ𝑇൐𝑛ሽ
202 of 848

or, equivalently,
Another random variable of interest is the number of distinct types of coupons
that are contained in the first  selections—call this random variable 
. To
compute 
, let us start by fixing attention on a particular set of  distinct
types, and let us then determine the probability that this set constitutes the set of
distinct types obtained in the first  selections. Now, in order for this to be the
situation, it is necessary and sufficient that of the first  coupons obtained,
A: each is one of these k types
B: each of these k types is represented
Now, each coupon selected will be one of the  types with probability k/r, so the
probability that  will be valid is 
. Also, given that a coupon is of one of the
 types under consideration, it is easy to see that it is equally likely to be of any
one of these  types. Hence, the conditional probability of  given that  occurs
is the same as the probability that a set of  coupons, each equally likely to be
any of  possible types, contains a complete set of all  types. But this is just the
probability that the number needed to amass a complete set, when choosing
among  types, is less than or equal to  and is thus obtainable from Equation
(1.1)
 with  replacing . Thus, we have
Finally, as there are 
 possible choices for the set of  types, we arrive at
Remark We can obtain a useful bound on
by using Boole’s inequality along with the inequality 
.
𝑃ሼ𝑇ൌ𝑛ሽൌ𝑃ሼ𝑇൐𝑛െ1ሽെ𝑃ሼ𝑇൐𝑛ሽ
𝑛
𝐷௡
𝑃ሼ𝐷௡ൌ𝑘ሽ
𝑘
𝑛
𝑛
𝑘
𝐴
ሺ𝑘/𝑟ሻ௡
𝑘
𝑘
𝐵
𝐴
𝑛
𝑘
𝑘
𝑘
𝑛
𝑘
𝑟
𝑃ሺ𝐴ሻ
ൌ
ቆ𝑘
𝑟ቇ
௡
𝑃ሺ𝐵|𝐴ሻ
ൌ
1 െ
෍
௜ൌଵ
௞െଵ
ቆ𝑘
𝑖ቇቆ𝑘െ𝑖
𝑘
ቇ
௡
ሺെ1ሻ௜൅ଵ
ቆ𝑟
𝑘ቇ
𝑘
𝑃ሼ𝐷௡ൌ𝑘ሽൌ
ቆ𝑟
𝑘ቇ𝑃ቆ𝐴𝐵ቇ
ൌ
ቆ𝑟
𝑘ቇቆ𝑘
𝑟ቇ
௡
቎1 െ
෍
௜ൌଵ
௞െଵ
ቆ𝑘
𝑖ቇቆ𝑘െ𝑖
𝑘
ቇ
௡
ሺെ1ሻ௜൅ଵ቏
𝑃ሺ𝑇൐𝑛ሻൌ𝑃ቀ
௝ൌଵ
௥
𝐴௝ቁ
𝑒െ௫൒1 െ𝑥
203 of 848

The first inequality is Boole’s inequality, which says that the probability of the
union of events is always less than or equal to the sum of the probabilities of
these events, and the last inequality uses that 
.
For a random variable , the function  defined by
is called the cumulative distribution function or, more simply, the distribution function
of . Thus, the distribution function specifies, for all real values , the probability that
the random variable is less than or equal to .
Now, suppose that 
. Then, because the event 
 is contained in the event
, it follows that 
, the probability of the former, is less than or equal to 
,
the probability of the latter. In other words, 
 is a nondecreasing function of .
Other general properties of the distribution function are given in Section 4.10
.
A random variable that can take on at most a countable number of possible values is
said to be discrete. For a discrete random variable , we define the probability mass
function
 of  by
The probability mass function 
 is positive for at most a countable number of
values of . That is, if  must assume one of the values 
, then
Since  must take on one of the values 
, we have
𝑃ሺ𝑇൐𝑛ሻൌ
𝑃൫
௝ൌଵ
௥
𝐴௝൯
൑
෍
௝ൌଵ
௥
𝑃ሺ𝐴௝ሻ
ൌ
𝑟ቆ1 െ1
𝑟ቇ
௡
൑
𝑟𝑒െ௡/௥
𝑒െଵ/௥൒1 െ1/𝑟
𝑋
𝐹
𝐹ሺ𝑥ሻൌ𝑃ሼ𝑋൑𝑥ሽ   െ∞൏𝑥൏∞
𝑋
𝑥
𝑥
𝑎൑𝑏
ሼ𝑋൑𝑎ሽ
ሼ𝑋൑𝑏ሽ
𝐹ሺ𝑎ሻ
𝐹ሺ𝑏ሻ
𝐹ሺ𝑥ሻ
𝑥
𝑋
𝑝ሺ𝑎ሻ
𝑋
𝑝ሺ𝑎ሻൌ𝑃ሼ𝑋ൌ𝑎ሽ
𝑝ሺ𝑎ሻ
𝑎
𝑋
𝑥ଵ, 𝑥ଶ, …
𝑝ሺ𝑥௜ሻ൒0
for 𝑖ൌ1, 2, …
𝑝ሺ𝑥ሻൌ0
for all other values of 𝑥
𝑋
𝑥௜
204 of 848

It is often instructive to present the probability mass function in a graphical format by
plotting 
 on the -axis against 
 on the -axis. For instance, if the probability
mass function of  is
we can represent this function graphically as shown in Figure 4.1
. Similarly, a
graph of the probability mass function of the random variable representing the sum
when two dice are rolled looks like Figure 4.2
.
Figure 4.1
Figure 4.2
෍
௜ൌଵ
ஶ
𝑝ሺ𝑥௜ሻൌ1
𝑝ሺ𝑥௜ሻ
𝑦
𝑥௜
𝑥
𝑋
𝑝ሺ0ሻൌ1
4
𝑝ሺ1ሻൌ1
2
𝑝ሺ2ሻൌ1
4
205 of 848

Example 2a
The probability mass function of a random variable  is given by
, where  is some positive value. Find (a) 
 and
(b) 
.
Solution
Since 
 we have
which, because 
, implies that
Hence,
a. 
b. 
𝑋
𝑝ሺ𝑖ሻൌ𝑐𝜆௜/𝑖!, 𝑖ൌ0, 1, 2, …
𝜆
𝑃ሼ𝑋ൌ0ሽ
𝑃ሼ𝑋൐2ሽ
෍
௜ൌ଴
ஶ
𝑝ሺ𝑖ሻൌ1,
𝑐෍
௜ൌ଴
ஶ
𝜆௜
𝑖! ൌ1
𝑒 ௫ൌ෍
௜ൌ଴
ஶ
𝑥௜/𝑖!
𝑐𝑒ఒൌ1 or 𝑐ൌ𝑒െఒ
𝑃൛𝑋ൌ0ൟൌ𝑒െఒ𝜆଴/0! ൌ𝑒െఒ
𝑃ሼ𝑋൐2ሽ
ൌ1 െ𝑃ሼ𝑋൑2ሽൌ1 െ𝑃ሼ𝑋ൌ0ሽെ𝑃ሼ𝑋ൌ1ሽ
െ𝑃ሼ𝑋ൌ2ሽ
ൌ1 െ𝑒െఒെ𝜆𝑒െఒെ𝜆ଶ𝑒െఒ
2
206 of 848

The cumulative distribution function  can be expressed in terms of 
 by
If  is a discrete random variable whose possible values are 
, where
, then the distribution function  of  is a step function. That is, the
value of  is constant in the intervals 
 and then takes a step (or jump) of
size 
 at 
. For instance, if  has a probability mass function given by
then its cumulative distribution function is
This function is depicted graphically in Figure 4.3
.
Figure 4.3
𝐹
𝑝ሺ𝑎ሻ
𝐹ሺ𝑎ሻൌ
෍
all  ௫  ൑ ௔
𝑝ሺ𝑥ሻ
𝑋
𝑥ଵ, 𝑥ଶ, 𝑥ଷ, …
𝑥ଵ൏𝑥ଶ൏𝑥ଷ൏⋯
𝐹
𝑋
𝐹
ሺ𝑥௜െଵ, 𝑥௜ሻ
𝑝ሺ𝑥௜ሻ
𝑥௜
𝑋
𝑝ሺ1ሻൌ1
4
𝑝ሺ2ሻൌ1
2
𝑝ሺ3ሻൌ1
8
𝑝ሺ4ሻൌ1
8
𝐹ሺ𝑎ሻൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
0
𝑎൏
1
1
4
1
൑
𝑎൏
2
3
4
2
൑
𝑎൏
3
7
8
3
൑
𝑎൏
4
1
4
൑
𝑎
207 of 848

Note that the size of the step at any of the values 1, 2, 3, and 4 is equal to the
probability that  assumes that particular value.
One of the most important concepts in probability theory is that of the expectation of
a random variable. If  is a discrete random variable having a probability mass
function 
, then the expectation, or the expected value, of , denoted by 
, is
defined by
In words, the expected value of  is a weighted average of the possible values that 
can take on, each value being weighted by the probability that  assumes it. For
instance, on the one hand, if the probability mass function of  is given by
then
is just the ordinary average of the two possible values, 0 and 1, that  can assume.
On the other hand, if
then
is a weighted average of the two possible values 0 and 1, where the value 1 is given
twice as much weight as the value 0, since 
.
Another motivation of the definition of expectation is provided by the frequency
interpretation of probabilities. This interpretation (partially justified by the strong law
of large numbers, to be presented in Chapter 8
) assumes that if an infinite
𝑋
𝑋
𝑝ሺ𝑥ሻ
𝑋
𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿൌ
෍
௫: ௣ሺ௫ሻவ଴
𝑥𝑝ሺ𝑥ሻ
𝑋
𝑋
𝑋
𝑋
𝑝ሺ0ሻൌ1
2 ൌ𝑝ሺ1ሻ
𝐸ሾ𝑋ሿൌ0ቆ1
2ቇ൅1ቆ1
2ቇൌ1
2
𝑋
𝑝ሺ0ሻൌ1
3   𝑝ሺ1ሻൌ2
3
𝐸ሾ𝑋ሿൌ0ቆ1
3ቇ൅1ቆ2
3ቇൌ2
3
𝑝ሺ1ሻൌ2𝑝ሺ0ሻ
208 of 848

sequence of independent replications of an experiment is performed, then, for any
event , the proportion of time that  occurs will be 
. Now, consider a random
variable  that must take on one of the values 
 with respective probabilities
, and think of  as representing our winnings in a single game of
chance. That is, with probability 
, we shall win 
 units 
. By the
frequency interpretation, if we play this game continually, then the proportion of time
that we win 
 will be 
. Since this is true for all 
, it follows that our
average winnings per game will be
Example 3a
Find 
, where  is the outcome when we roll a fair die.
Solution
Since 
, we obtain
Example 3b
We say that  is an indicator variable for the event  if
Find 
.
Solution
Since 
, we have
That is, the expected value of the indicator variable for the event  is equal to the
probability that  occurs.
Example 3c
A contestant on a quiz show is presented with two questions, questions 1 and 2,
𝐸
𝐸
𝑃ሺ𝐸ሻ
𝑋
𝑥ଵ, 𝑥ଶ, …𝑥௡
𝑝ሺ𝑥ଵሻ, 𝑝ሺ𝑥ଶሻ, …, 𝑝ሺ𝑥௡ሻ
𝑋
𝑝ሺ𝑥௜ሻ
𝑥௜
𝑖ൌ1, 2, …, 𝑛
𝑥௜
𝑝ሺ𝑥௜ሻ
𝑖, 𝑖ൌ1, 2, …, 𝑛
෍
௜ൌଵ
௡
𝑥௜𝑝ሺ𝑥௜ሻൌ𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿ
𝑋
𝑝ሺ1ሻൌ𝑝ሺ2ሻൌ𝑝ሺ3ሻൌ𝑝ሺ4ሻൌ𝑝ሺ5ሻൌ𝑝ሺ6ሻൌ1
6
𝐸ሾ𝑋ሿൌ1ቆ1
6ቇ൅2ቆ1
6ቇ൅3ቆ1
6ቇ൅4ቆ1
6ቇ൅5ቆ1
6ቇ൅6ቆ1
6ቇൌ7
2
𝐼
𝐴
𝐼ൌቊ
1
 if 𝐴 occurs 
0
 if 𝐴௖ occurs 
𝐸ሾ𝐼ሿ
𝑝ሺ1ሻൌ𝑃ሺ𝐴ሻ, 𝑝ሺ0ሻൌ1 െ𝑃ሺ𝐴ሻ
𝐸ሾ𝐼ሿൌ𝑃ሺ𝐴ሻ
𝐴
𝐴
209 of 848

which he is to attempt to answer in some order he chooses. If he decides to try
question  first, then he will be allowed to go on to question , 
, only if his
answer to question  is correct. If his initial answer is incorrect, he is not allowed
to answer the other question. The contestant is to receive 
 dollars if he
answers question  correctly, 
. For instance, he will receive 
 dollars
if he answers both questions correctly. If the probability that he knows the answer
to question  is 
, which question should he attempt to answer first so as
to maximize his expected winnings? Assume that the events 
, that he
knows the answer to question  are independent events.
Solution
On the one hand, if he attempts to answer question 1 first, then he will win
Hence, his expected winnings in this case will be
On the other hand, if he attempts to answer question 2 first, his expected
winnings will be
Therefore, it is better to try question 1 first if
or, equivalently, if
For example, if he is 60 percent certain of answering question 1, worth $200,
correctly and he is 80 percent certain of answering question 2, worth $100,
correctly, then he should attempt to answer question 2 first because
Example 3d
𝑖
𝑗𝑗്𝑖
𝑖
𝑉௜
𝑖
𝑖ൌ1, 2
𝑉ଵ൅𝑉ଶ
𝑖
𝑃௜, 𝑖ൌ1, 2
𝐸௜, 𝑖ൌ1, 2
𝑖
0
 with probability 1   െ𝑃ଵ
𝑉ଵ
 with probability  𝑃ଵሺ1 െ𝑃ଶሻ
𝑉ଵ൅𝑉ଶ
 with probability  𝑃ଵ𝑃ଶ
𝑉ଵ𝑃ଵሺ1 െ𝑃ଶሻ൅ሺ𝑉ଵ൅𝑉ଶሻ𝑃ଵ𝑃ଶ
𝑉ଶ𝑃ଶሺ1 െ𝑃ଵሻ൅ሺ𝑉ଵ൅𝑉ଶሻ𝑃ଵ𝑃ଶ
𝑉ଵ𝑃ଵሺ1 െ𝑃ଶሻ൒𝑉ଶ𝑃ଶሺ1 െ𝑃ଵሻ
𝑉ଵ𝑃ଵ
1 െ𝑃ଵ
൒𝑉ଶ𝑃ଶ
1 െ𝑃ଶ
400 ൌሺ100ሻሺ.8ሻ
.2
൐ሺ200ሻሺ.6ሻ
.4
ൌ300
210 of 848

A school class of 120 students is driven in 3 buses to a symphonic performance.
There are 36 students in one of the buses, 40 in another, and 44 in the third bus.
When the buses arrive, one of the 120 students is randomly chosen. Let 
denote the number of students on the bus of that randomly chosen student, and
find 
.
Solution
Since the randomly chosen student is equally likely to be any of the 120
students, it follows that
Hence,
However, the average number of students on a bus is 
, showing that
the expected number of students on the bus of a randomly chosen student is
larger than the average number of students on a bus. This is a general
phenomenon, and it occurs because the more students there are on a bus, the
more likely it is that a randomly chosen student would have been on that bus. As
a result, buses with many students are given more weight than those with fewer
students. (See Self-Test Problem 4.4
)
Remark The probability concept of expectation is analogous to the physical concept
of the center of gravity of a distribution of mass. Consider a discrete random variable
 having probability mass function 
. If we now imagine a weightless rod in
which weights with mass 
, are located at the points 
 (see Figure
4.4
), then the point at which the rod would be in balance is known as the center of
gravity. For those readers acquainted with elementary statics, it is now a simple
matter to show that this point is at 
.†
Figure 4.4
𝑋
𝐸ሾ𝑋ሿ
𝑃ሼ𝑋ൌ36ሽൌ36
120
𝑃ሼ𝑋ൌ40ሽൌ40
120
𝑃ሼ𝑋ൌ44ሽൌ44
120
𝐸ሾ𝑋ሿൌ36ቆ3
10ቇ൅40ቆ1
3ቇ൅44ቆ11
30ቇൌ1208
30
ൌ40 . 2667
120/3 ൌ40
𝑋
𝑝ሺ𝑥௜ሻ, 𝑖൒1
𝑝ሺ𝑥௜ሻ, 𝑖൒1
𝑥௜, 𝑖൒1
𝐸ሾ𝑋ሿ
211 of 848

To prove this, we must show that the sum of the torques tending to
turn the point around 
 is equal to 0. That is, we must show that
, which is immediate.
Suppose that we are given a discrete random variable along with its probability mass
function and that we want to compute the expected value of some function of , say,
. How can we accomplish this? One way is as follows: Since 
 is itself a
discrete random variable, it has a probability mass function, which can be
determined from the probability mass function of . Once we have determined the
probability mass function of 
, we can compute 
 by using the definition of
expected value.
Example 4a
Let  denote a random variable that takes on any of the values 
, 0, and 1 with
respective probabilities
Compute 
.
Solution
Let 
. Then the probability mass function of  is given by
Hence,
Note that
†
𝐸ሾ𝑋ሿ
0 ൌ෍
௜
ሺ𝑥௜െ𝐸ሾ𝑋ሿሻ𝑝ሺ𝑥௜ሻ
𝑋
𝑔ሺ𝑋ሻ
𝑔ሺ𝑋ሻ
𝑋
𝑔ሺ𝑋ሻ
𝐸ሾ𝑔ሺ𝑋ሻሿ
𝑋
െ1
𝑃ሼ𝑋ൌെ1ሽൌ. 2 𝑃ሼ𝑋ൌ0ሽൌ. 5 𝑃ሼ𝑋ൌ1ሽൌ. 3
𝐸ൣ𝑋ଶ൧
𝑌ൌ𝑋ଶ
𝑌
𝑃ሼ𝑌ൌ1ሽ
ൌ
𝑃ሼ𝑋ൌെ1ሽ൅𝑃ሼ𝑋ൌ1ሽൌ.5
𝑃ሼ𝑌ൌ0ሽ
ൌ
𝑃ሼ𝑋ൌ0ሽൌ.5
𝐸ሾ𝑋ଶሿൌ𝐸ሾ𝑌ሿൌ1ሺ. 5ሻ൅0ሺ. 5ሻൌ. 5
. 5 ൌ𝐸ሾ𝑋ଶሿ്ሺ𝐸ሾ𝑋ሿሻଶൌ. 01
212 of 848

Although the preceding procedure will always enable us to compute the expected
value of any function of  from a knowledge of the probability mass function of ,
there is another way of thinking about 
: Since 
 will equal 
 whenever
 is equal to , it seems reasonable that 
 should just be a weighted average
of the values 
, with 
 being weighted by the probability that  is equal to .
That is, the following result is quite intuitive.
Proposition 4.1
If  is a discrete random variable that takes on one of the values 
, with
respective probabilities 
, then, for any real-valued function ,
Before proving this proposition, let us check that it is in accord with the results of
Example 4a
. Applying it to that example yields
which is in agreement with the result given in Example 4a
.
Proof of Proposition 4.1 The proof of Proposition 4.1
 proceeds, as in the
preceding verification, by grouping together all the terms in 
 having the
same value of 
. Specifically, suppose that 
, represent the different
values of 
. Then, grouping all the 
 having the same value gives
Example 4b
𝑋
𝑋
𝐸ሾ𝑔ሺ𝑋ሻሿ
𝑔ሺ𝑋ሻ
𝑔ሺ𝑥ሻ
𝑋
𝑥
𝐸ሾ𝑔ሺ𝑋ሻሿ
𝑔ሺ𝑥ሻ
𝑔ሺ𝑥ሻ
𝑋
𝑥
𝑋
𝑥௜, 𝑖൒1
𝑝ሺ𝑥௜ሻ
𝑔
𝐸ሾ𝑔ሺ𝑋ሻሿൌ෍
௜
𝑔ሺ𝑥௜ሻ𝑝ሺ𝑥௜ሻ
𝐸൛𝑋ଶൟ
ൌ
ሺെ1ሻଶሺ.2ሻ൅0ଶሺ.5ሻ൅1ଶሺ.3ሻ
ൌ
1ሺ.2 ൅.3ሻ൅0ሺ.5ሻ
ൌ
.5
෍
௜
𝑔ሺ𝑥௜ሻ𝑝ሺ𝑥௜ሻ
𝑔ሺ𝑥௜ሻ
𝑦௝, 𝑗൒1
𝑔ሺ𝑥௜ሻ, 𝑖൒1
𝑔ሺ𝑥௜ሻ
෍
௜
𝑔ሺ𝑥௜ሻ𝑝ሺ𝑥௜ሻ
ൌ
෍
௝
෍
௜: ௚ሺ௫೔ሻൌ௬ೕ
𝑔ሺ𝑥௜ሻ𝑝ሺ𝑥௜ሻ
ൌ
෍
௝
෍
௜: ௚ሺ௫೔ሻൌ௬ೕ
𝑦௝𝑝ሺ𝑥௜ሻ
ൌ
෍
௝
𝑦௝
෍
௜: ௚ሺ௫೔ሻൌ௬ೕ
𝑝ሺ𝑥௜ሻ
ൌ
෍
௝
𝑦௝𝑃൛𝑔ሺ𝑋ሻൌ𝑦௜ൟ
ൌ𝐸ሾ𝑔ሺ𝑋ሻሿ
213 of 848

A product that is sold seasonally yields a net profit of  dollars for each unit sold
and a net loss of  dollars for each unit left unsold when the season ends. The
number of units of the product that are ordered at a specific department store
during any season is a random variable having probability mass function
. If the store must stock this product in advance, determine the number
of units the store should stock so as to maximize its expected profit.
Solution
Let  denote the number of units ordered. If  units are stocked, then the profit—
call it 
—can be expressed as
Hence, the expected profit equals
To determine the optimum value of , let us investigate what happens to the profit
when we increase  by 1 unit. By substitution, we see that the expected profit in
this case is given by
Therefore,
𝑏
ℓ
𝑝ሺ𝑖ሻ, 𝑖൒0
𝑋
𝑠
𝑃ሺ𝑠ሻ
𝑃ሺ𝑠ሻ
ൌ𝑏𝑋െሺ𝑠െ𝑋ሻℓ
if 𝑋൑𝑠
ൌ𝑠𝑏
if 𝑋൐𝑠
𝐸ሾ𝑃ሺ𝑠ሻሿ
ൌ
෍
௜ൌ଴
௦
ሾ𝑏𝑖െሺ𝑠െ𝑖ሻℓሿ𝑝ሺ𝑖ሻ൅
෍
௜ൌ௦൅ଵ
ஶ
𝑠𝑏𝑝ሺ𝑖ሻ
ൌ
ሺ𝑏൅ℓሻ෍
௜ൌ଴
௦
𝑖𝑝ሺ𝑖ሻെ𝑠ℓ෍
௜ൌ଴
௦
𝑝ሺ𝑖ሻ൅𝑠𝑏቎1 െ෍
௜ൌ଴
௦
𝑝ሺ𝑖ሻ቏
ൌ
ሺ𝑏൅ℓሻ෍
௜ൌ଴
௦
𝑖𝑝ሺ𝑖ሻെሺ𝑏൅ℓሻ𝑠෍
௜ൌ଴
௦
𝑝ሺ𝑖ሻ൅𝑠𝑏
ൌ
𝑠𝑏൅ሺ𝑏൅ℓሻ෍
௜ൌ଴
௦
ሺ𝑖െ𝑠ሻ𝑝ሺ𝑖ሻ
𝑠
𝑠
𝐸ሾ𝑃ሺ𝑠൅1ሻሿ
ൌ
𝑏ሺ𝑠൅1ሻ൅ሺ𝑏൅ℓሻ෍
௜ൌ଴
௦൅ଵ
ሺ𝑖െ𝑠െ1ሻ𝑝ሺ𝑖ሻ
ൌ
𝑏ሺ𝑠൅1ሻ൅ሺ𝑏൅ℓሻ෍
௜ൌ଴
௦
ሺ𝑖െ𝑠െ1ሻ𝑝ሺ𝑖ሻ
214 of 848

Thus, stocking 
 units will be better than stocking  units whenever
Because the left-hand side of Equation (4.1)
 is increasing in  while the right-
hand side is constant, the inequality will be satisfied for all values of 
,
where s  is the largest value of  satisfying Equation (4.1)
. Since
it follows that stocking 
 items will lead to a maximum expected profit.
Example 4c Utility
Suppose that you must choose one of two possible actions, each of which can
result in any of  consequences, denoted as 
. Suppose that if the first
action is chosen, then consequence 
 will result with probability 
,
whereas if the second action is chosen, then consequence 
 will result with
probability 
, where 
. The following approach
can be used to determine which action to choose: Start by assigning numerical
values to the different consequences. First, identify the least and the most
desirable consequences—call them  and , respectively; give consequence 
the value 0 and give  the value 1. Now consider any of the other 
consequences, say, 
. To value this consequence, imagine that you are given
the choice between either receiving 
 or taking part in a random experiment that
either earns you consequence  with probability  or consequence  with
probability 
. Clearly, your choice will depend on the value of . On the one
hand, if 
, then the experiment is certain to result in consequence , and
since  is the most desirable consequence, you will prefer participating in the
experiment to receiving 
. On the other hand, if 
, then the experiment will
result in the least desirable consequence—namely, —so in this case you will
prefer the consequence 
 to participating in the experiment. Now, as 
decreases from 1 to 0, it seems reasonable that your choice will at some point
switch from participating in the experiment to the certain return of 
, and at that
𝐸ሾ𝑃ሺ𝑠൅1ሻሿെ𝐸ሾ𝑃ሺ𝑠ሻሿൌ𝑏െሺ𝑏൅ℓሻ෍
௜ൌ଴
௦
𝑝ሺ𝑖ሻ
𝑠൅1
𝑠
෍
௜ൌ଴
௦
𝑝ሺ𝑖ሻ൏
𝑏
𝑏൅ℓ
(4.1)
𝑠
𝑠൑𝑠*
*
𝑠
𝐸ሾ𝑃ሺ0ሻሿ൏⋯൏𝐸ሾ𝑃ሺ𝑠*ሻሿ൏𝐸ሾ𝑃ሺ𝑠* ൅1ሻሿ൐𝐸ሾ𝑃ሺ𝑠* ൅2ሻሿ൐⋯
𝑠* ൅1
𝑛
𝐶ଵ, …, 𝐶௡
𝐶௜
𝑝௜, 𝑖ൌ1, …, 𝑛
𝐶௜
𝑞௜, 𝑖ൌ1, …, 𝑛
෍
௜ൌଵ
௡
𝑝௜ൌ෍
௜ൌଵ
௡
𝑞௜ൌ1
𝑐
𝐶
𝑐
𝐶
𝑛െ2
𝐶௜
𝐶௜
𝐶
𝑢
𝑐
1 െ𝑢
𝑢
𝑢ൌ1
𝐶
𝐶
𝐶௜
𝑢ൌ0
𝑐
𝐶௜
𝑢
𝐶௜
215 of 848

critical switch point you will be indifferent between the two alternatives. Take that
indifference probability  as the value of the consequence 
. In other words, the
value of 
 is that probability  such that you are indifferent between either
receiving the consequence 
 or taking part in an experiment that returns
consequence  with probability  or consequence  with probability 
. We
call this indifference probability the utility of the consequence 
, and we
designate it as 
.
To determine which action is superior, we need to evaluate each one. Consider
the first action, which results in consequence 
 with probability 
. We
can think of the result of this action as being determined by a two-stage
experiment. In the first stage, one of the values 
 is chosen according to the
probabilities 
; if value  is chosen, you receive consequence 
. However,
since 
 is equivalent to obtaining consequence  with probability 
 or
consequence  with probability 
, it follows that the result of the two-
stage experiment is equivalent to an experiment in which either consequence 
or consequence  is obtained, with  being obtained with probability
Similarly, the result of choosing the second action is equivalent to taking part in
an experiment in which either consequence  or consequence  is obtained, with
 being obtained with probability
Since  is preferable to , it follows that the first action is preferable to the second
action if
In other words, the worth of an action can be measured by the expected value of
the utility of its consequence, and the action with the largest expected utility is the
most preferable.
A simple logical consequence of Proposition 4.1
 is Corollary 4.1
.
Corollary 4.1
𝑢
𝐶௜
𝐶௜
𝑢
𝐶௜
𝐶
𝑢
𝑐
1 െ𝑢
𝐶௜
𝑢ሺ𝐶௜ሻ
𝐶௜
𝑝௜, 𝑖ൌ1, …, 𝑛
1, …, 𝑛
𝑝ଵ, …, 𝑝௡
𝑖
𝐶௜
𝐶௜
𝐶
𝑢ሺ𝐶௜ሻ
𝑐
1 െ𝑢ሺ𝐶௜ሻ
𝐶
𝑐
𝐶
෍
௜ൌଵ
௡
𝑝௜𝑢ሺ𝐶௜ሻ
𝐶
𝑐
𝐶
෍
௜ൌଵ
௡
𝑞௜𝑢ሺ𝐶௜ሻ
𝐶
𝑐
෍
௜ൌଵ
௡
𝑝௜𝑢ሺ𝐶௜ሻ൐෍
௜ൌଵ
௡
𝑞௜𝑢ሺ𝐶௜ሻ
216 of 848

If  and  are constants, then
Proof
The expected value of a random variable , 
, is also referred to as the mean or
the first moment of X. The quantity 
, is called the nth moment of X. By
Proposition 4.1
, we note that
Given a random variable  along with its distribution function , it would be extremely
useful if we were able to summarize the essential properties of  by certain suitably
defined measures. One such measure would be 
, the expected value of .
However, although 
 yields the weighted average of the possible values of , it
does not tell us anything about the variation, or spread, of these values. For
instance, although random variables 
, , and  having probability mass functions
determined by
all have the same expectation—namely, 0—there is a much greater spread in the
𝑎
𝑏
𝐸ሾ𝑎𝑋൅𝑏ሿൌ𝑎𝐸ሾ𝑋ሿ൅𝑏
𝐸ሾ𝑎𝑋൅𝑏ሿൌ
෍
௫: ௣ሺ௫ሻவ଴
ሺ𝑎𝑥൅𝑏ሻ𝑝ሺ𝑥ሻ
ൌ
𝑎
෍
௫: ௣ሺ௫ሻவ଴
𝑥𝑝ሺ𝑥ሻ൅𝑏
෍
௫: ௣ሺ௫ሻவ଴
𝑝ሺ𝑥ሻ
ൌ
𝑎𝐸ሾ𝑋ሿ൅𝑏
𝑋𝐸ሾ𝑋ሿ
𝐸ሾ𝑋௡ሿ, 𝑛൒1
𝐸ሾ𝑋௡ሿൌ
෍
௫: ௣ሺ௫ሻவ଴
𝑥௡𝑝ሺ𝑥ሻ
𝑋
𝐹
𝐹
𝐸ሾ𝑋ሿ
𝑋
𝐸ሾ𝑋ሿ
𝑋
𝑊𝑌
𝑍
𝑊 
ൌ0 with probability 1
𝑌
ൌ
⎧
⎨
⎩
⎪
⎪
െ1
with probability   1
2
൅1
with probability  1
2
𝑍
ൌ
⎧
⎨
⎩
⎪
⎪
െ100
with probability 1
2
൅100
with probability 1
2
217 of 848

possible values of  than in those of 
 (which is a constant) and in the possible
values of  than in those of .
Because we expect  to take on values around its mean 
, it would appear that a
reasonable way of measuring the possible variation of  would be to look at how far
apart  would be from its mean, on the average. One possible way to measure this
variation would be to consider the quantity 
, where 
. However, it
turns out to be mathematically inconvenient to deal with this quantity, so a more
tractable quantity is usually considered—namely, the expectation of the square of the
difference between  and its mean. We thus have the following definition.
Definition
If  is a random variable with mean , then the variance of , denoted by Var
, is defined by
An alternative formula for Var
 is derived as follows:
That is,
In words, the variance of  is equal to the expected value of 
 minus the square of
its expected value. In practice, this formula frequently offers the easiest way to
compute Var
.
Example 5a
Calculate Var
 if  represents the outcome when a fair die is rolled.
𝑌
𝑊
𝑍
𝑌
𝑋
𝐸ሾ𝑋ሿ
𝑋
𝑋
𝐸ሾ||𝑋െ𝜇||ሿ
𝜇ൌ𝐸ሾ𝑋ሿ
𝑋
𝑋
𝜇
𝑋
ሺ𝑋ሻ
Varሺ𝑋ሻൌ𝐸ሾሺ𝑋െ𝜇ሻଶሿ
ሺ𝑋ሻ
Varሺ𝑋ሻൌ
𝐸ሾሺ𝑋െ𝜇ሻଶ൧
ൌ
෍
௫
ሺ𝑥െ𝜇ሻଶ𝑝ሺ𝑥ሻ
ൌ
෍
௫
ሺ𝑥ଶെ2𝜇𝑥൅𝜇ଶሻ𝑝ሺ𝑥ሻ
ൌ
෍
௫
𝑥ଶ𝑝ሺ𝑥ሻെ2𝜇෍
௫
𝑥𝑝ሺ𝑥ሻ൅𝜇ଶ෍
௫
𝑝ሺ𝑥ሻ
ൌ
𝐸ൣ𝑋ଶ൧െ2𝜇ଶ൅𝜇ଶ
ൌ
𝐸ൣ𝑋ଶ൧െ𝜇ଶ
Varሺ𝑋ሻൌ𝐸ሾ𝑋ଶሿെሺ𝐸ሾ𝑋ሿሻଶ
𝑋
𝑋ଶ
ሺ𝑋ሻ
ሺ𝑋ሻ
𝑋
218 of 848

Solution
It was shown in Example 3a
 that 
. Also,
Hence,
Because 
 is the sum of nonnegative
terms, it follows that 
 or equivalently, that
That is, the expected value of the square of a random variable is at least as large as
the square of its expected value.
Example 5b
The friendship paradox is often expressed as saying that on average your friends
have more friends than you do. More formally, suppose that there are  people in
a certain population, labeled 
, and that certain pairs of these individuals
are friends. This friendship network can be graphically represented by having a
circle for each person and then having a line between circles to indicate that
those people are friends. For instance, Figure 4.5
 indicates that there are 
people in the community and that persons  and  are friends, persons  and 
are friends, persons  and  are friends, and persons  and  are friends.
Figure 4.5 A friendship graph
𝐸ሾ𝑋ሿൌ7
2
𝐸ൣ𝑋ଶ൧ 
ൌ
1ଶቆ1
6ቇ൅2ଶቆ1
6ቇ൅3ଶቆ1
6ቇ൅4ଶቆ1
6ቇ൅5ଶቆ1
6ቇ൅6ଶቆ1
6ቇ
ൌ
ቆ1
6ቇቆ91ቇ
Varሺ𝑋ሻൌ91
6 െቆ7
2ቇ
ଶ
ൌ35
12
Var ሺ𝑋ሻൌ𝐸ሾሺ𝑋െ𝜇ሻଶሿൌ෍
௫
ሺ𝑥െ𝜇ሻଶ𝑃ሺ𝑋ൌ𝑥ሻ
Var ሺ𝑋ሻ൒0
𝐸ൣ𝑋ଶ൧൒ሺ𝐸ሾ𝑋ሿሻଶ
𝑛
1, 2, …, 𝑛
4
1
2
1
3
1
4
2
4
219 of 848

Let 
 denote the number of friends of person  and let 
. (Thus,
for the network of Figure 4.5
, 
, 
, 
, 
 and
.) Now, let  be a randomly chosen individual, equally likely to be any of
. That is,
Letting 
 in Proposition 4.1
, it follows that 
, the expected
number of friends of , is
Also, letting 
, it follows from Proposition 4.1
 that 
, the
expected value of the square of the number of friends of , is
𝑓ሺ𝑖ሻ
𝑖
𝑓ൌ෍
௜ൌଵ
௡
𝑓ሺ𝑖ሻ
𝑓ሺ1ሻൌ3 𝑓ሺ2ሻൌ2 𝑓ሺ3ሻൌ1 𝑓ሺ4ሻൌ2
𝑓ൌ8
𝑋
1, 2, …, 𝑛
𝑃ሺ𝑋ൌ𝑖ሻൌ1/𝑛,  𝑖ൌ1, …, 𝑛.
𝑔ሺ𝑖ሻൌ𝑓ሺ𝑖ሻ
𝐸ሾ𝑓ሺ𝑋ሻሿ
𝑋
𝐸ሾ𝑓ሺ𝑋ሻሿൌ෍
௜ൌଵ
௡
𝑓ሺ𝑖ሻ𝑃ሺ𝑋ൌ𝑖ሻൌ෍
௜ൌଵ
௡
𝑓ሺ𝑖ሻ/𝑛ൌ𝑓/𝑛
𝑔ሺ𝑖ሻൌ𝑓ଶሺ𝑖ሻ
𝐸ሾ𝑓ଶሺ𝑋ሻሿ
𝑋
𝐸ሾ𝑓ଶሺ𝑋ሻሿൌ෍
௜ൌଵ
௡
𝑓ଶሺ𝑖ሻ𝑃ሺ𝑋ൌ𝑖ሻൌ෍
௜ൌଵ
௡
𝑓ଶሺ𝑖ሻ/𝑛
220 of 848

Consequently, we see that
Now suppose that each of the  individuals writes the names of all their friends,
with each name written on a separate sheet of paper. Thus, an individual with 
friends will use  separate sheets. Because person  has 
 friends, there will
be 
 separate sheets of paper, with each sheet containing one of
the  names. Now choose one of these sheets at random and let  denote the
name on that sheet. Let us compute 
, the expected number of friends of
the person whose name is on the chosen sheet. Now, because person  has 
friends, it follows that  is the name on 
 of the sheets, and thus  is the name
on the chosen sheet with probability 
. That is,
Consequently,
Thus, from (5.1
), we see that
where the inequality follows because the expected value of the square of any
random variable is always at least as large as the square of its expectation.
Thus, 
, which says that the average number of friends that a
randomly chosen individual has is less than (or equal to if all the individuals have
the same number of friends) the average number of friends of a randomly chosen
friend.
Remark The intuitive reason for the friendship paradox is that  is equally likely
to be any of the  individuals. On the other hand  is chosen with a probability
proportional to its number of friends; that is, the more friends an individual has
𝐸ൣ𝑓ଶሺ𝑋ሻ൧
𝐸ሾ𝑓ሺ𝑋ሻሿൌ
෍
௜ൌଵ
௡
𝑓ଶሺ𝑖ሻ
𝑓
(5.1)
𝑛
𝑘
𝑘
𝑖
𝑓ሺ𝑖ሻ
𝑓ൌ෍
௜ൌଵ
௡
𝑓ሺ𝑖ሻ
𝑛
𝑌
𝐸ሾ𝑓ሺ𝑌ሻሿ
𝑖
𝑓ሺ𝑖ሻ
𝑖
𝑓ሺ𝑖ሻ
𝑖
𝑓ሺ𝑖ሻ
𝑓
𝑃ሺ𝑌ൌ𝑖ሻൌ𝑓ሺ𝑖ሻ
𝑓,  𝑖ൌ1, …, 𝑛.
𝐸ሾ𝑓ሺ𝑌ሻሿൌ෍
௜ൌଵ
௡
𝑓ሺ𝑖ሻ𝑃ሺ𝑌ൌ𝑖ሻൌ෍
௜ൌଵ
௡
𝑓ଶሺ𝑖ሻ/𝑓
(5.2)
𝐸ሾ𝑓ሺ𝑌ሻሿൌ𝐸ሾ𝑓ଶሺ𝑋ሻሿ
𝐸ሾ𝑓ሺ𝑋ሻሿ൒𝐸ሾ𝑓ሺ𝑋ሻሿ
𝐸ሾ𝑓ሺ𝑋ሻሿ൑𝐸ሾ𝑓ሺ𝑌ሻሿ
𝑋
𝑛
𝑌
221 of 848

the more likely that individual will be . Thus,  is biased towards individuals with
a large number of friends and so it is not surprising that the average number of
friends that  has is larger than the average number of friends that  has.
The following is a further example illustrating the usefulness of the inequality that the
expected value of a square is at least as large as the square of the expected value.
Example 5c
Suppose there are 
 days in a year, and that each person is independently born
on day  with probability 
, 
, 
. Let 
 be the event that
persons  and  are born on the same day.
a. Find 
b. Find 
c. Show 
Solution
a. Because the event that  and  have the same birthday is the union of the
 mutually exclusive events that they were both born on day 
,
we have that
b. Using the definition of conditional probability we obtain that
where the preceding used that 
 is the union of the 
 mutually
exclusive events that 
 were all born on day 
.
c. It follows from parts (a) and (b) that 
 is equivalent to
. To prove this inequality, let  be a random variable
that is equal to 
 with probability 
. That is, 
, 
.
Then
𝑌
𝑌
𝑌
𝑋
𝑚
𝑟
𝑝௥𝑟ൌ1, …, 𝑚෍
௥ൌଵ
௠
𝑝௥ൌ1
𝐴௜,௝
𝑖
𝑗
𝑃ሺ𝐴ଵ,ଷሻ
𝑃ሺ𝐴ଵ,ଷห𝐴ଵ,ଶሻ
𝑃ሺ𝐴ଵ,ଷห𝐴ଵ,ଶሻ൒𝑃ሺ𝐴ଵ,ଷሻ
1
3
𝑚
𝑟, 𝑟ൌ1, …, 𝑚
𝑃ሺ𝐴ଵ,ଷሻൌ෍
௥
𝑝ଶ
௥.
𝑃ሺ𝐴ଵ,ଷห𝐴ଵ,ଶሻ
ൌ
𝑃ሺ𝐴ଵ,ଶ𝐴ଵ,ଷሻ
𝑃ሺ𝐴ଵ,ଶሻ
ൌ
෍
௥
𝑝ଷ
௥
෍
௥
𝑝ଶ௥
𝐴ଵ,ଶ𝐴ଵ,ଷ
𝑚
1, 2, 3
𝑟, 𝑟ൌ1, …, 𝑚
𝑃ሺ𝐴ଵ,ଷห𝐴ଵ,ଶሻ൒𝑃ሺ𝐴ଵ,ଷሻ
෍
௥
𝑝ଷ
௥൒൬෍
௥
𝑝ଶ
௥൰
ଶ
𝑋
𝑝௥
𝑝௥
𝑃ሺ𝑋ൌ𝑝௥ሻൌ𝑝௥𝑟ൌ1, …, 𝑚
𝐸ሾ𝑋ሿൌ෍
௥
𝑝௥𝑃ሺ𝑋ൌ𝑝௥ሻൌ෍
௥
𝑝ଶ
௥ ,
𝐸ൣ𝑋ଶ൧ൌ෍
௥
𝑝ଶ
௥𝑃ሺ𝑋ൌ𝑝௥ሻൌ෍
௥
𝑝ଷ
௥
222 of 848

and the result follows because 
Remark The intuitive reason for why part (c) is true is that if the “popular days”
are the ones whose probabilities are relatively large, then knowing that  and 
share the same birthday makes it more likely (than when we have no information)
that the birthday of  is a popular day and that makes it more likely that  will
have the same birthday as does .
A useful identity is that for any constants  and ,
To prove this equality, let 
 and note from Corollary 4.1
 that
. Therefore,
Remarks
a. Analogous to the means being the center of gravity of a distribution of mass,
the variance represents, in the terminology of mechanics, the moment of
inertia.
b. The square root of the Var
 is called the standard deviation of , and we
denote it by SD
. That is,
Discrete random variables are often classified according to their probability mass
functions. In the next few sections, we consider some of the more common types.
Suppose that a trial, or an experiment, whose outcome can be classified as either a
success or a failure is performed. If we let 
 when the outcome is a success and
 when it is a failure, then the probability mass function of  is given by
𝐸ൣ𝑋ଶ൧൒ሺ𝐸ሾ𝑋ሿሻଶ.
1
2
1
3
1
𝑎
𝑏
Varሺ𝑎𝑋൅𝑏ሻൌ𝑎ଶVarሺ𝑋ሻ
𝜇ൌ𝐸ሾ𝑋ሿ
𝐸ሾ𝑎𝑋൅𝑏ሿൌ𝑎𝜇൅𝑏
Varሺ𝑎𝑋൅𝑏ሻൌ
𝐸ൣሺ𝑎𝑋൅𝑏െ𝑎𝜇െ𝑏ሻଶ൧
ൌ
𝐸ൣ𝑎ଶሺ𝑋െ𝜇ሻଶ൧
ൌ
𝑎ଶ𝐸ൣሺ𝑋െ𝜇ሻଶ൧
ൌ
𝑎ଶVarሺ𝑋ሻ
ሺ𝑋ሻ
𝑋
ሺ𝑋ሻ
𝑆𝐷ሺ𝑋ሻൌ
Varሺ𝑋ሻ
ඥ
𝑋ൌ1
𝑋ൌ0
𝑋
223 of 848

where , 
, is the probability that the trial is a success.
A random variable  is said to be a Bernoulli random variable (after the Swiss
mathematician James Bernoulli) if its probability mass function is given by
Equations (6.1)
 for some 
.
Suppose now that  independent trials, each of which results in a success with
probability  or in a failure with probability 
, are to be performed. If  represents
the number of successes that occur in the  trials, then  is said to be a binomial
random variable with parameters ( , ). Thus, a Bernoulli random variable is just a
binomial random variable with parameters (1, ).
The probability mass function of a binomial random variable having parameters (
)
is given by
The validity of Equation (6.2)
 may be verified by first noting that the probability of
any particular sequence of  outcomes containing  successes and 
 failures is,
by the assumed independence of trials, 
. Equation (6.2)
 then follows,
since there are 
 different sequences of the  outcomes leading to  successes
and 
 failures. This perhaps can most easily be seen by noting that there are 
different choices of the  trials that result in successes. For instance, if 
,
then there are 
 ways in which the four trials can result in two successes,
namely, any of the outcomes ( , , , ), ( , , , ), ( , , , ), ( , , , ), ( , , , ),
and ( , , , ), where the outcome ( , , , ) means, for instance, that the first two
trials are successes and the last two failures. Since each of these outcomes has
probability 
 of occurring, the desired probability of two successes in the
four trials is 
.
Note that, by the binomial theorem, the probabilities sum to 1; that is,
𝑝ሺ0ሻൌ
𝑃ሼ𝑋ൌ0ሽൌ1 െ𝑝
𝑝ሺ1ሻൌ
𝑃ሼ𝑋ൌ1ሽൌ𝑝
(6.1)
𝑝0 ൑𝑝൑1
𝑋
𝑝∈ሺ0, 1ሻ
𝑛
𝑝
1 െ𝑝
𝑋
𝑛
𝑋
𝑛𝑝
𝑝
𝑛, 𝑝
𝑝ሺ𝑖ሻൌቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜
𝑖ൌ0, 1, …, 𝑛
(6.2)
𝑛
𝑖
𝑛െ𝑖
𝑝௜ሺ1 െ𝑝ሻ௡െ௜
ቆ𝑛
𝑖ቇ
𝑛
𝑖
𝑛െ𝑖
ቆ𝑛
𝑖ቇ
𝑖
𝑛ൌ4, 𝑖ൌ2
ቆ4
2ቇൌ6
𝑠𝑠𝑓𝑓
𝑠𝑓𝑠𝑓
𝑠𝑓𝑓𝑠
𝑓𝑠𝑠𝑓
𝑓𝑠𝑓𝑠
𝑓𝑓𝑠𝑠
𝑠𝑠𝑓𝑓
𝑝ଶሺ1 െ𝑝ሻଶ
ቆ4
2ቇ𝑝ଶሺ1 െ𝑝ሻଶ
224 of 848

Example 6a
Five fair coins are flipped. If the outcomes are assumed independent, find the
probability mass function of the number of heads obtained.
Solution
If we let  equal the number of heads (successes) that appear, then  is a
binomial random variable with parameters 
. Hence, by Equation
(6.2)
,
Example 6b
It is known that screws produced by a certain company will be defective with
probability .01, independently of one another. The company sells the screws in
packages of 10 and offers a money-back guarantee that at most 1 of the 10
screws is defective. What proportion of packages sold must the company
replace?
Solution
If  is the number of defective screws in a package, then  is a binomial random
variable with parameters (10, .01). Hence, the probability that a package will
have to be replaced is
෍
௜ൌ଴
ஶ
𝑝ሺ𝑖ሻൌ෍
௜ൌ଴
௡
ቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜ൌሾ𝑝൅ሺ1 െ𝑝ሻሿ௡ൌ1
𝑋
𝑋
ቆ𝑛ൌ5,  𝑝ൌ1
2ቇ
𝑃ሼ𝑋ൌ0ሽ
ൌቆ5
0ቇቆ1
2ቇ
଴
ቆ1
2ቇ
ହ
ൌ
1
32
𝑃ሼ𝑋ൌ1ሽ
ൌቆ5
1ቇቆ1
2ቇ
ଵ
ቆ1
2ቇ
ସ
ൌ
5
32
𝑃ሼ𝑋ൌ2ሽ
ൌቆ5
2ቇቆ1
2ቇ
ଶ
ቆ1
2ቇ
ଷ
ൌ
10
32
𝑃ሼ𝑋ൌ3ሽ
ൌቆ5
3ቇቆ1
2ቇ
ଷ
ቆ1
2ቇ
ଶ
ൌ
10
32
𝑃ሼ𝑋ൌ4ሽ
ൌቆ5
4ቇቆ1
2ቇ
ସ
ቆ1
2ቇ
ଵ
ൌ
5
32
𝑃ሼ𝑋ൌ5ሽ
ൌቆ5
5ቇቆ1
2ቇ
ହ
ቆ1
2ቇ
଴
ൌ
1
32
𝑋
𝑋
225 of 848

Thus, only .4 percent of the packages will have to be replaced.
Example 6c
The following gambling game, known as the wheel of fortune (or chuck-a-luck), is
quite popular at many carnivals and gambling casinos: A player bets on one of
the numbers 1 through 6. Three dice are then rolled, and if the number bet by the
player appears  times, 
, then the player wins  units; if the number bet
by the player does not appear on any of the dice, then the player loses 1 unit. Is
this game fair to the player? (Actually, the game is played by spinning a wheel
that comes to rest on a slot labeled by three of the numbers 1 through 6, but this
variant is mathematically equivalent to the dice version.)
Solution
If we assume that the dice are fair and act independently of one another, then the
number of times that the number bet appears is a binomial random variable with
parameters 
. Hence, letting  denote the player’s winnings in the game, we
have
In order to determine whether or not this is a fair game for the player, let us
calculate 
. From the preceding probabilities, we obtain
Hence, in the long run, the player will lose 17 units per every 216 games he
1 െ𝑃ሼ𝑋ൌ0ሽെ𝑃ሼ𝑋ൌ1ሽ
ൌ
1 െቆ10
0 ቇሺ.01ሻ଴ሺ.99ሻଵ଴െቆ10
1 ቇሺ.01ሻଵሺ.99ሻଽ
ൎ
.004
𝑖
𝑖ൌ1, 2, 3
𝑖
ቆ3, 1
6ቇ
𝑋
𝑃ሼ𝑋ൌെ1ሽ
ൌቆ3
0ቇቆ1
6ቇ
଴
ቆ5
6ቇ
ଷ
ൌ
125
216
𝑃ሼ𝑋ൌ1ሽ
ൌቆ3
1ቇቆ1
6ቇ
ଵ
ቆ5
6ቇ
ଶ
ൌ
75
216
𝑃ሼ𝑋ൌ2ሽ
ൌቆ3
2ቇቆ1
6ቇ
ଶ
ቆ5
6ቇ
ଵ
ൌ
15
216
𝑃ሼ𝑋ൌ3ሽ
ൌቆ3
3ቇቆ1
6ቇ
ଷ
ቆ5
6ቇ
଴
ൌ
1
216
𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿ
ൌ
െ125 ൅75 ൅30 ൅3
216
ൌ
െ17
216
226 of 848

plays.
In the next example, we consider the simplest form of the theory of inheritance as
developed by Gregor Mendel (1822—1884).
Example 6d
Suppose that a particular trait (such as eye color or left-handedness) of a person
is classified on the basis of one pair of genes, and suppose also that 
represents a dominant gene and  a recessive gene. Thus, a person with dd
genes is purely dominant, one with rr is purely recessive, and one with rd is
hybrid. The purely dominant and the hybrid individuals are alike in appearance.
Children receive 1 gene from each parent. If, with respect to a particular trait, 2
hybrid parents have a total of 4 children, what is the probability that 3 of the 4
children have the outward appearance of the dominant gene?
The preceding Figure 4.6a
 and b
 shows what can happen when hybrid
yellow (dominant) and green (recessive) seeds are crossed.
Figure 4.6 (a) Crossing pure yellow seeds with pure green seeds; (b)
Crossing hybrid first-generation seeds.
𝑑
𝑟
227 of 848

Solution
If we assume that each child is equally likely to inherit either of 2 genes from
each parent, the probabilities that the child of 2 hybrid parents will have dd, rr,
and rd pairs of genes are, respectively, 
 and . Hence, since an offspring will
have the outward appearance of the dominant gene if its gene pair is either dd or
rd, it follows that the number of such children is binomially distributed with
parameters 
. Thus, the desired probability is
Example 6e
Consider a jury trial in which it takes 8 of the 12 jurors to convict the defendant;
that is, in order for the defendant to be convicted, at least 8 of the jurors must
vote him guilty. If we assume that jurors act independently and that whether or
not the defendant is guilty, each makes the right decision with probability , what
is the probability that the jury renders a correct decision?
Solution
The problem, as stated, is incapable of solution, for there is not yet enough
information. For instance, if the defendant is innocent, the probability of the jury
rendering a correct decision is
1
4, 1
4,
1
2
ቆ4, 3
4ቇ
ቆ4
3ቇቆ3
4ቇ
ଷ
ቆ1
4ቇ
ଵ
ൌ27
64
𝜃
228 of 848

whereas, if he is guilty, the probability of a correct decision is
Therefore, if  represents the probability that the defendant is guilty, then, by
conditioning on whether or not he is guilty, we obtain the probability that the jury
renders a correct decision:
Example 6f
A communication system consists of  components, each of which will,
independently, function with probability . The total system will be able to operate
effectively if at least one-half of its components function.
a. For what values of  is a 5-component system more likely to operate
effectively than a 3-component system?
b. In general, when is a 
-component system better than a 
-component system?
Solution
a. Because the number of functioning components is a binomial random
variable with parameters (
), it follows that the probability that a
5-component system will be effective is
whereas the corresponding probability for a 3-component system is
Hence, the 5-component system is better if
෍
௜ൌହ
ଵଶ
ቆ12
𝑖ቇ𝜃௜ሺ1 െ𝜃ሻଵଶെ௜
෍
௜ൌ଼
ଵଶ
ቆ12
𝑖ቇ𝜃௜ሺ1 െ𝜃ሻଵଶെ௜
𝛼
𝛼෍
௜ൌ଼
ଵଶ
ቆ12
𝑖ቇ𝜃௜ሺ1 െ𝜃ሻଵଶെ௜൅ሺ1 െ𝛼ሻ෍
௜ൌହ
ଵଶ
ቆ12
𝑖ቇ𝜃௜ሺ1 െ𝜃ሻଵଶെ௜
𝑛
𝑝
𝑝
ሺ2𝑘൅1ሻ
ሺ2𝑘െ1ሻ
𝑛, 𝑝
ቆ5
3ቇ𝑝ଷሺ1 െ𝑝ሻଶ൅ቆ5
4ቇ𝑝ସሺ1 െ𝑝ሻ൅𝑝ହ
ቆ3
2ቇ𝑝ଶሺ1 െ𝑝ሻ൅𝑝ଷ
10𝑝ଷሺ1 െ𝑝ሻଶ൅5𝑝ସሺ1 െ𝑝ሻ൅𝑝ହ൐3𝑝ଶሺ1 െ𝑝ሻ൅𝑝ଷ
229 of 848

which reduces to
or
b. In general, a system with 
 components will be better than one with
 components if (and only if) 
. To prove this, consider a system
of 
 components and let  denote the number of the first 
 that
function. Then
which follows because the 
-component system will be effective if
either
i. 
;
ii. 
 and at least one of the remaining 2 components function; or
iii. 
 and both of the next 2 components function.
Since
we obtain
We will now examine the properties of a binomial random variable with parameters 
and . To begin, let us compute its expected value and variance. To begin, note that
3ሺ𝑝െ1ሻଶሺ2𝑝െ1ሻ൐0
𝑝൐1
2
2𝑘൅1
2𝑘െ1
𝑝൐1
2
2𝑘൅1
𝑋
2𝑘െ1
𝑃ଶ௞൅ଵሺeffectiveሻൌ
𝑃ሼ𝑋൒𝑘൅1ሽ൅𝑃ሼ𝑋ൌ𝑘ሽሺ1 െሺ1 െ𝑝ሻଶሻ
൅𝑃ሼ𝑋ൌ𝑘െ1ሽ𝑝ଶ
ሺ2𝑘൅1ሻ
𝑋൒𝑘൅1
𝑋ൌ𝑘
𝑋ൌ𝑘െ1
𝑃ଶ௞െଵሺeffectiveሻൌ
𝑃ሼ𝑋൒𝑘ሽ
ൌ
𝑃ሼ𝑋ൌ𝑘ሽ൅𝑃ሼ𝑋൒𝑘൅1ሽ
𝑃ଶ௞൅ଵሺeffectiveሻെ𝑃ଶ௞െଵሺeffectiveሻ
   ൌ𝑃ሼ𝑋ൌ𝑘െ1ሽ𝑝ଶെሺ1 െ𝑝ሻଶ𝑃ሼ𝑋ൌ𝑘ሽ
   ൌቆ2𝑘െ1
𝑘െ1 ቇ𝑝௞െଵሺ1 െ𝑝ሻ௞𝑝ଶെሺ1 െ𝑝ሻଶቆ2𝑘െ1
𝑘
ቇ𝑝௞ሺ1 െ𝑝ሻ௞െଵ
   ൌቆ2𝑘െ1
𝑘
ቇ𝑝௞ሺ1 െ𝑝ሻ௞ሾ 𝑝െሺ1 െ𝑝ሻሿ sinceቆ2𝑘െ1
𝑘െ1 ቇൌቆ2𝑘െ1
𝑘
ቇ
   ൐0 ⇔𝑝൐
ଵ
ଶ
𝑛
𝑝
230 of 848

Using the identity
gives
where  is a binomial random variable with parameters 
, . Setting 
 in the
preceding equation yields
That is, the expected number of successes that occur in  independent trials when
each is a success with probability  is equal to np. Setting 
 in the preceding
equation and using the preceding formula for the expected value of a binomial
random variable yields
Since 
, we obtain
Summing up, we have shown the following:
𝐸ൣ𝑋௞൧
ൌ
෍
௜ൌ଴
௡
𝑖௞ቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜
ൌ
෍
௜ൌଵ
௡
𝑖௞ቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜
𝑖ቆ𝑛
𝑖ቇൌ𝑛ቆ𝑛െ1
𝑖െ1ቇ
𝐸ൣ𝑋௞൧
ൌ
𝑛𝑝෍
௜ൌଵ
௡
𝑖௞െଵቆ𝑛െ1
𝑖െ1ቇ𝑝௜െଵሺ1 െ𝑝ሻ௡െ௜
ൌ
𝑛𝑝෍
௝ൌ଴
௡െଵ
ሺ𝑗൅1ሻ௞െଵቆ𝑛െ1
𝑗
ቇ𝑝௝ሺ1 െ𝑝ሻ௡െଵെ௝
by letting
𝑗ൌ𝑖െ1 
ൌ
𝑛𝑝𝐸ൣሺ𝑌൅1ሻ௞െଵ൧
𝑌
𝑛െ1 𝑝
𝑘ൌ1
𝐸ሾ𝑋ሿൌ𝑛𝑝
𝑛
𝑝
𝑘ൌ2
𝐸ൣ𝑋ଶ൧
ൌ
𝑛𝑝𝐸ሾ𝑌൅1ሿ
ൌ
𝑛𝑝ሾሺ𝑛െ1ሻ𝑝൅1ሿ
𝐸ሾ𝑋ሿൌ𝑛𝑝
Varሺ𝑋ሻൌ
𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
ൌ
𝑛𝑝ሾሺ𝑛െ1ሻ𝑝൅1ሿെሺ𝑛𝑝ሻଶ
ൌ
𝑛𝑝ሺ1 െ𝑝ሻ
231 of 848

If  is a binomial random variable with parameters  and , then
The following proposition details how the binomial probability mass function first
increases and then decreases.
Proposition 6.1
If  is a binomial random variable with parameters (
), where 
, then
as  goes from 0 to 
 first increases monotonically and then decreases
monotonically, reaching its largest value when  is the largest integer less than or
equal to 
.
Proof We prove the proposition by considering 
 and
determining for what values of  it is greater or less than 1. Now,
Hence, 
 if and only if
or, equivalently, if and only if
and the proposition is proved.
As an illustration of Proposition 6.1
, consider Figure 4.7
, the graph of the
probability mass function of a binomial random variable with parameters 
.
Figure 4.7 Graph of 
𝑋
𝑛
𝑝
𝐸ሾ𝑋ሿ
ൌ
𝑛𝑝
Varሺ𝑋ሻൌ
𝑛𝑝ሺ1 െ𝑝ሻ
𝑋
𝑛, 𝑝
0 ൏𝑝൏1
𝑘
𝑛, 𝑃ሼ𝑋ൌ𝑘ሽ
𝑘
ሺ𝑛൅1ሻ𝑝
𝑃ሼ𝑋ൌ𝑘ሽ/𝑃ሼ𝑋ൌ𝑘െ1ሽ
𝑘
𝑃ሼ𝑋ൌ𝑘ሽ
𝑃ሼ𝑋ൌ𝑘െ1ሽ
ൌ
𝑛!
ሺ𝑛െ𝑘ሻ!𝑘! 𝑝௞ሺ1 െ𝑝ሻ௡െ௞
𝑛!
ሺ𝑛െ𝑘൅1ሻ!ሺ𝑘െ1ሻ! 𝑝௞െଵሺ1 െ𝑝ሻ௡െ௞൅ଵ
ൌ
ሺ𝑛െ𝑘൅1ሻ𝑝
𝑘ሺ1 െ𝑝ሻ
𝑃ሼ𝑋ൌ𝑘ሽ൒𝑃ሼ𝑋ൌ𝑘െ1ሽ
ሺ𝑛െ𝑘൅1ሻ𝑝൒𝑘ሺ1 െ𝑝ሻ
𝑘൑ሺ𝑛൅1ሻ𝑝
ቆ10, 1
2ቇ
𝑝ሺ𝑘ሻൌቆ10
𝑘ቇቆ1
2ቇ
ଵ଴
.
232 of 848

Example 6g
In a U.S. presidential election, the candidate who gains the maximum number of
votes in a state is awarded the total number of electoral college votes allocated
to that state. The number of electoral college votes of a given state is roughly
proportional to the population of that state—that is, a state with population  has
roughly nc electoral votes. (Actually, it is closer to 
, as a state is given an
electoral vote for each member it has in the House of Representatives, with the
number of such representatives being roughly proportional to the population of
the state, and one electoral college vote for each of its two senators.) Let us
determine the average power of a citizen in a state of size  in a close
presidential election, where, by average power in a close election, we mean that
a voter in a state of size 
 will be decisive if the other 
 voters split
their votes evenly between the two candidates. (We are assuming here that  is
odd, but the case where  is even is quite similar.)
Because the election is close, we shall suppose that each of the other 
voters acts independently and is equally likely to vote for either candidate.
Hence, the probability that a voter in a state of size 
 will make a
difference to the outcome is the same as the probability that 
 tosses of a fair
coin land heads and tails an equal number of times. That is,
𝑛
𝑛𝑐൅2
𝑛
𝑛ൌ2𝑘൅1
𝑛െ1
𝑛
𝑛
𝑛െ1 ൌ2𝑘
𝑛ൌ2𝑘൅1
2𝑘
233 of 848

To approximate the preceding equality, we make use of Stirling’s approximation,
which says that for  large,
where we say that 
 when the ratio 
 approaches 1 as  approaches
. Hence, it follows that
Because such a voter (if he or she makes a difference) will affect nc electoral
votes, the expected number of electoral votes a voter in a state of size  will
affect—or the voter’s average power—is given by
Thus, the average power of a voter in a state of size  is proportional to the
square root of , showing that in presidential elections, voters in large states
have more power than do those in smaller states.
Suppose that  is binomial with parameters ( , ). The key to computing its
distribution function
is to utilize the following relationship between 
 and 
, which was
𝑃ሼ voter in state of size  2𝑘൅ 1 makes a difference ሽ
   ൌቆ2𝑘
𝑘ቇቆ1
2ቇ
௞
ቆ1
2ቇ
௞
   ൌ
ሺ2𝑘ሻ!
𝑘!𝑘!2ଶ௞
𝑘
𝑘! ∼𝑘௞൅ଵ/ଶ𝑒െ௞2𝜋
√
𝑎௞∼𝑏௞
𝑎௞/𝑏௞
𝑘
∞
𝑃ሼvoter in state of size  2𝑘൅ 1 makes a differenceሽ
   ∼ሺ2𝑘ሻଶ௞൅ଵ/ଶ𝑒െଶ௞2𝜋
√
𝑘ଶ௞൅ଵ𝑒െଶ௞ሺ2𝜋ሻ2ଶ௞ൌ
1
𝑘𝜋
√
𝑛
 average power 
ൌ
𝑛𝑐𝑃ሼmakes a differenceሽ
∼
𝑛𝑐
𝑛𝜋/2
ඥ
ൌ
𝑐2𝑛/𝜋
ඥ
𝑛
𝑛
𝑋
𝑛𝑝
𝑃ሼ𝑋൑𝑖ሽൌ
෍
௞ൌ଴
௜
ቆ𝑛
𝑘ቇ𝑝௞ሺ1 െ𝑝ሻ௡െ௞
𝑖ൌ0, 1, …, 𝑛
𝑃ሼ𝑋ൌ𝑘൅1ሽ
𝑃ሼ𝑋ൌ𝑘ሽ
234 of 848

established in the proof of Proposition 6.1
:
Example 6h
Let  be a binomial random variable with parameters 
. Then,
starting with 
 and recursively employing Equation (6.3)
, we
obtain
A computer program that utilizes the recursion (6.3
) to compute the binomial
distribution function is easily written. To compute 
, the program should first
compute 
 and then use the recursion to successively compute
, and so on.
Historical note
Independent trials having a common probability of success  were first studied
by the Swiss mathematician Jacques Bernoulli (1654–1705). In his book Ars
Conjectandi (The Art of Conjecturing), published by his nephew Nicholas eight
years after his death in 1713, Bernoulli showed that if the number of such trials
were large, then the proportion of them that were successes would be close to
 with a probability near 1.
Jacques Bernoulli was from the first generation of the most famous
mathematical family of all time. Altogether, there were between 8 and 12
Bernoullis, spread over three generations, who made fundamental contributions
to probability, statistics, and mathematics. One difficulty in knowing their exact
𝑃ሼ𝑋ൌ𝑘൅1ሽൌ
𝑝
1 െ𝑝
𝑛െ𝑘
𝑘൅1 𝑃ሼ𝑋ൌ𝑘ሽ
(6.3)
𝑋
𝑛ൌ6,  𝑝ൌ. 4
𝑃ሼ𝑋ൌ0ሽൌሺ. 6ሻ଺
𝑃ሼ𝑋ൌ0ሽ
ൌ
ሺ.6ሻ଺ൎ.0467
𝑃ሼ𝑋ൌ1ሽ
ൌ
4
6
6
1 𝑃ሼ𝑋ൌ0ሽ
ൎ
.1866
𝑃ሼ𝑋ൌ2ሽ
ൌ
4
6
5
2 𝑃ሼ𝑋ൌ1ሽ
ൎ
.3110
𝑃ሼ𝑋ൌ3ሽ
ൌ
4
6
4
3 𝑃ሼ𝑋ൌ2ሽ
ൎ
.2765
𝑃ሼ𝑋ൌ4ሽ
ൌ
4
6
3
4 𝑃ሼ𝑋ൌ3ሽ
ൎ
.1382
𝑃ሼ𝑋ൌ5ሽ
ൌ
4
6
2
5 𝑃ሼ𝑋ൌ4ሽ
ൎ
.0369
𝑃ሼ𝑋ൌ6ሽ
ൌ
4
6
1
6 𝑃ሼ𝑋ൌ5ሽ
ൎ
.0041
𝑃ሼ𝑋൑𝑖ሽ
𝑃ሼ𝑋ൌ𝑖ሽ
𝑃ሼ𝑋ൌ𝑖െ1ሽ, 𝑃ሼ𝑋ൌ𝑖െ2ሽ
𝑝
𝑝
235 of 848

number is the fact that several had the same name. (For example, two of the
sons of Jacques’s brother Jean were named Jacques and Jean.) Another
difficulty is that several of the Bernoullis were known by different names in
different places. Our Jacques (sometimes written Jaques) was, for instance,
also known as Jakob (sometimes written Jacob) and as James Bernoulli. But
whatever their number, their influence and output were prodigious. Like the
Bachs of music, the Bernoullis of mathematics were a family for the ages!
Example 6i
If  is a binomial random variable with parameters 
 and 
, find
 and 
.
Solution
A binomial calculator can be used to obtain the following solutions:
Figure 4.8
A random variable  that takes on one of the values 
 is said to be a Poisson
random variable with parameter  if, for some 
,
Equation (7.1)
 defines a probability mass function, since
𝑋
𝑛ൌ100
𝑝ൌ. 75
𝑃ሼ𝑋ൌ70ሽ
𝑃ሼ𝑋൑70ሽ
𝑋
0, 1, 2, …
𝜆
𝜆൐0
𝑝ሺ𝑖ሻൌ𝑃ሼ𝑋ൌ𝑖ሽൌ𝑒െఒ𝜆௜
𝑖!  𝑖ൌ0, 1, 2, …
(7.1)
236 of 848

The Poisson probability distribution was introduced by Siméon Denis Poisson in a
book he wrote regarding the application of probability theory to lawsuits, criminal
trials, and the like. This book, published in 1837, was entitled Recherches sur la
probabilité des jugements en matière criminelle et en matière civile (Investigations
into the Probability of Verdicts in Criminal and Civil Matters).
The Poisson random variable has a tremendous range of applications in diverse
areas because it may be used as an approximation for a binomial random variable
with parameters (
) when  is large and  is small enough so that np is of
moderate size. To see this, suppose that  is a binomial random variable with
parameters (
), and let 
. Then
Now, for  large and  moderate,
Hence, for  large and  moderate,
In other words, if  independent trials, each of which results in a success with
probability , are performed, then when  is large and  is small enough to make np
moderate, the number of successes occurring is approximately a Poisson random
variable with parameter 
. This value  (which will later be shown to equal the
expected number of successes) will usually be determined empirically.
Some examples of random variables that generally obey the Poisson probability law
[that is, they obey Equation (7.1)
] are as follows:
1. The number of misprints on a page (or a group of pages) of a book
෍
௜ൌ଴
ஶ
𝑝ሺ𝑖ሻൌ𝑒െఒ෍
௜ൌ଴
ஶ
𝜆௜
𝑖! ൌ𝑒െఒ𝑒ఒൌ1
𝑛, 𝑝
𝑛
𝑝
𝑋
𝑛, 𝑝
𝜆ൌ𝑛𝑝
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ
𝑛!
ሺ𝑛െ𝑖ሻ!𝑖! 𝑝௜ሺ1 െ𝑝ሻ௡െ௜
ൌ
𝑛!
ሺ𝑛െ𝑖ሻ!𝑖! ሺ𝜆
𝑛ሻ
௜
ሺ1 െ𝜆
𝑛ሻ
௡െ௜
ൌ
𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑖൅1ሻ
𝑛௜
𝜆௜
𝑖!
ሺ1 െ𝜆/𝑛ሻ௡
ሺ1 െ𝜆/𝑛ሻ௜
𝑛
𝜆
ቆ1 െ𝜆
𝑛ቇ
௡
ൎ𝑒െఒ  𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑖൅1ሻ
𝑛௜
ൎ1 ቆ1 െ𝜆
𝑛ቇ
௜
ൎ1
𝑛
𝜆
𝑃ሼ𝑋ൌ𝑖ሽൎ𝑒െఒ𝜆௜
𝑖!
𝑛
𝑝
𝑛
𝑝
𝜆ൌ𝑛𝑝
𝜆
237 of 848

2. The number of people in a community who survive to age 100
3. The number of wrong telephone numbers that are dialed in a day
4. The number of packages of dog biscuits sold in a particular store each day
5. The number of customers entering a post office on a given day
6. The number of vacancies occurring during a year in the federal judicial system
7. The number of -particles discharged in a fixed period of time from some
radioactive material
Each of the preceding and numerous other random variables are approximately
Poisson for the same reason—namely, because of the Poisson approximation to the
binomial. For instance, we can suppose that there is a small probability  that each
letter typed on a page will be misprinted. Hence, the number of misprints on a page
will be approximately Poisson with 
, where  is the number of letters on a
page. Similarly, we can suppose that each person in a community has some small
probability of reaching age 100. Also, each person entering a store may be thought
of as having some small probability of buying a package of dog biscuits, and so on.
Example 7a
Suppose that the number of typographical errors on a single page of this book
has a Poisson distribution with parameter 
. Calculate the probability that
there is at least one error on this page.
Solution
Letting  denote the number of errors on this page, we have
Example 7b
Suppose that the probability that an item produced by a certain machine will be
defective is .1. Find the probability that a sample of 10 items will contain at most
1 defective item.
Solution
The desired probability is 
, whereas
the Poisson approximation yields the value 
.
Example 7c
Consider an experiment that consists of counting the number of  particles given
𝛼
𝑝
𝜆ൌ𝑛𝑝
𝑛
𝜆ൌ1
2
𝑋
𝑃൛𝑋൒1ൟൌ1 െ𝑃൛𝑋ൌ0ൟൌ1 െ𝑒െଵ/ଶൎ. 393
ቆ10
0 ቇሺ. 1ሻ଴ሺ. 9ሻଵ଴൅ቆ10
1 ቇሺ. 1ሻଵሺ. 9ሻଽൌ. 7361
𝑒െଵ൅𝑒െଵൎ. 7358
𝛼
238 of 848

off in a 1-second interval by 1 gram of radioactive material. If we know from past
experience that on the average, 3.2 such  particles are given off, what is a good
approximation to the probability that no more than 2  particles will appear?
Solution
If we think of the gram of radioactive material as consisting of a large number 
of atoms, each of which has probability of 
 of disintegrating and sending off
an  particle during the second considered, then we see that to a very close
approximation, the number of  particles given off will be a Poisson random
variable with parameter 
. Hence, the desired probability is
Before computing the expected value and variance of the Poisson random variable
with parameter , recall that this random variable approximates a binomial random
variable with parameters  and  when  is large,  is small, and 
. Since such
a binomial random variable has expected value 
 and variance
 (since  is small), it would seem that both the expected
value and the variance of a Poisson random variable would equal its parameter .
We now verify this result:
Thus, the expected value of a Poisson random variable  is indeed equal to its
parameter . To determine its variance, we first compute 
:
𝛼
𝛼
𝑛
3.2/𝑛
𝛼
𝛼
𝜆ൌ3 . 2
𝑃ሼ𝑋൑2ሽ
ൌ
𝑒െଷ.ଶ൅3.2𝑒െଷ.ଶ൅ሺ3.2ሻଶ
2
𝑒െଷ.ଶ
ൎ
.3799
𝜆
𝑛
𝑝
𝑛
𝑝
𝜆ൌ𝑛𝑝
𝑛𝑝ൌ𝜆
𝑛𝑝ሺ1 െ𝑝ሻൌ𝜆ሺ1 െ𝑝ሻൎ𝜆
𝑝
𝜆
𝐸ሾ𝑋ሿ
ൌ
෍
௜ൌ଴
ஶ
𝑖𝑒െఒ𝜆௜
𝑖!
ൌ
𝜆෍
௜ൌଵ
ஶ
𝑒െఒ𝜆௜െଵ
ሺ𝑖െ1ሻ!
ൌ
𝜆𝑒െఒ෍
௝ൌ଴
ஶ
𝜆௝
𝑗!  by letting 
𝑗ൌ𝑖െ1
ൌ
𝜆 since ෍
௝ൌ଴
ஶ
𝜆௝
𝑗! ൌ𝑒ఒ
𝑋
𝜆
𝐸ൣ𝑋ଶ൧
239 of 848

where the final equality follows because the first sum is the expected value of a
Poisson random variable with parameter  and the second is the sum of the
probabilities of this random variable. Therefore, since we have shown that 
,
we obtain
Hence, the expected value and variance of a Poisson random variable are both
equal to its parameter .
We have shown that the Poisson distribution with parameter np is a very good
approximation to the distribution of the number of successes in  independent trials
when each trial has probability  of being a success, provided that  is large and 
small. In fact, it remains a good approximation even when the trials are not
independent, provided that their dependence is weak. For instance, recall the
matching problem (Example 5m
 of Chapter 2
) in which  men randomly
select hats from a set consisting of one hat from each person. From the point of view
of the number of men who select their own hat, we may regard the random selection
as the result of  trials where we say that trial  is a success if person  selects his
own hat, 
. Defining the events 
, by
it is easy to see that
Thus, we see that although the events 
 are not independent, their
dependence, for large , appears to be weak. Because of this, it seems reasonable
𝐸ሾ𝑋ଶሿ
ൌ
෍
௜ൌ଴
ஶ
𝑖𝑒െఒ𝜆௜
𝑖!
ൌ
𝜆෍
௜ൌଵ
ஶ
𝑒െఒ𝜆௜െଵ
ሺ𝑖െ1ሻ!
ൌ
𝜆෍
௝ൌ଴
ஶ
ሺ𝑗൅1ሻ𝑒െఒ𝜆௝
𝑗!
  by letting 
𝑗ൌ𝑖െ1
ൌ
𝜆 ቎෍
௝ൌ଴
ஶ
𝑗𝑒െఒ𝜆௝
𝑗!
൅෍
௝ൌ଴
ஶ
𝑒െఒ𝜆௝
𝑗!
቏
𝜆
𝐸ሾ𝑋ሿൌ𝜆
Varሺ𝑋ሻൌ
𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
ൌ
𝜆
𝜆
𝑛
𝑝
𝑛
𝑝
𝑛
𝑛
𝑖
𝑖
𝑖ൌ1, …, 𝑛
𝐸௜, 𝑖ൌ1, …, 𝑛
𝐸௜ൌሼ trial 𝑖 is a success ሽ
𝑃ሼ𝐸௜ሽൌ1
𝑛and 𝑃ሼ𝐸௜|𝐸௝ሽൌ
1
𝑛െ1, 𝑗്𝑖
𝐸௜, 𝑖ൌ1, …, 𝑛
𝑛
240 of 848

to expect that the number of successes will approximately have a Poisson
distribution with parameter 
 and indeed this is verified in Example 5m
of Chapter 2
.
For a second illustration of the strength of the Poisson approximation when the trials
are weakly dependent, let us consider again the birthday problem presented in
Example 5i
 of Chapter 2
. In this example, we suppose that each of  people
is equally likely to have any of the 365 days of the year as his or her birthday, and
the problem is to determine the probability that a set of  independent people all
have different birthdays. A combinatorial argument was used to determine this
probability, which was shown to be less than  when 
.
We can approximate the preceding probability by using the Poisson approximation
as follows: Imagine that we have a trial for each of the 
 pairs of individuals  and
, and say that trial ,  is a success if persons  and  have the same birthday. If
we let 
 denote the event that trial ,  is a success, then, whereas the 
 events
, are not independent (see Theoretical Exercise 4.21
), their
dependence appears to be rather weak. (Indeed, these events are even pairwise
independent, in that any 2 of the events 
 and 
 are independent—again, see
Theoretical Exercise 4.21
). Since 
, it is reasonable to suppose
that the number of successes should approximately have a Poisson distribution with
mean 
. Therefore,
To determine the smallest integer  for which this probability is less than , note that
is equivalent to
Taking logarithms of both sides, we obtain
𝑛ൈ1/𝑛ൌ1
𝑛
𝑛
1
2
𝑛ൌ23
ቆ𝑛
2ቇ
𝑖
𝑗, 𝑖്𝑗
𝑖𝑗
𝑖
𝑗
𝐸௜௝
𝑖𝑗
ቆ𝑛
2ቇ
𝐸௜௝, 1 ൑𝑖൏𝑗൑𝑛
𝐸௜௝
𝐸௞௟
𝑃ሺ𝐸௜௝ሻൌ1/365
ቆ𝑛
2ቇ/365 ൌ𝑛ሺ𝑛െ1ሻ/730
𝑃ሼ no 2 people have the same birthday ሽ
ൌ𝑃ሼ0  successesሽ
ൎ
expቊെ𝑛ሺ𝑛െ1ሻ
730
ቋ
𝑛
1
2
expቊെ𝑛ሺ𝑛െ1ሻ
730
ቋ൑1
2
expቊ𝑛ሺ𝑛െ1ሻ
730
ቋ൒2
241 of 848

which yields the solution 
, in agreement with the result of Example 5i
 of
Chapter 2
.
Suppose now that we wanted the probability that among the  people, no 3 of them
have their birthday on the same day. Whereas this now becomes a difficult
combinatorial problem, it is a simple matter to obtain a good approximation. To begin,
imagine that we have a trial for each of the 
 triplets , , , where
, and call the , ,  trial a success if persons , , and  all have
their birthday on the same day. As before, we can then conclude that the number of
successes is approximately a Poisson random variable with parameter
Hence,
This probability will be less than  when  is such that
which is equivalent to 
. Thus, the approximate probability that at least 3
people in a group of size 84 or larger will have the same birthday exceeds .
For the number of events to occur to approximately have a Poisson distribution, it is
not essential that all the events have the same probability of occurrence, but only
that all of these probabilities be small. The following is referred to as the Poisson
paradigm.
Poisson paradigm
Consider  events, with 
 equal to the probability that event  occurs,
 If all the 
 are “small” and the trials are either independent or at
most “weakly dependent,” then the number of these events that occur
𝑛ሺ𝑛െ1ሻ൒
730 log 2
ൎ
505.997
𝑛ൌ23
𝑛
ቆ𝑛
3ቇ
𝑖𝑗𝑘
1 ൑𝑖൏𝑗൏𝑘൑𝑛
𝑖𝑗𝑘
𝑖𝑗
𝑘
ቆ𝑛
3ቇ𝑃ሼ𝑖, 𝑗, 𝑘  have the same birthday ሽ
ൌ
ቆ𝑛
3ቇቆ1
365ቇ
ଶ
ൌ
𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ
6 ൈሺ365ሻଶ
𝑃ሼ no 3 have the same birthday ሽൎexpቊെ𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ
799350
ቋ
1
2
𝑛
𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ൒799350 log 2 ൎ554067 . 1
𝑛൒84
1
2
𝑛
𝑝௜
𝑖
𝑖ൌ1, …, 𝑛.
𝑝௜
242 of 848

approximately has a Poisson distribution with mean 
Our next example not only makes use of the Poisson paradigm, but also illustrates a
variety of the techniques we have studied so far.
Example 7d Length of the longest run
A coin is flipped  times. Assuming that the flips are independent, with each one
coming up heads with probability , what is the probability that there is a string of
 consecutive heads?
Solution
We will first use the Poisson paradigm to approximate this probability. Now, if for
, we let 
 denote the event that flips 
 all land
on heads, then the desired probability is that at least one of the events 
 occur.
Because 
 is the event that starting with flip , the next  flips all land on heads,
it follows that 
. Thus, when 
 is small, we might think that the number
of the 
 that occur should have an approximate Poisson distribution. However,
such is not the case, because, although the events all have small probabilities,
some of their dependencies are too great for the Poisson distribution to be a
good approximation. For instance, because the conditional probability that flips
 are all heads given that flips 
 are all heads is equal to the
probability that flip 
 is a head, it follows that
which is far greater than the unconditional probability of 
.
The trick that enables us to use a Poisson approximation is to note that there will
be a string of  consecutive heads either if there is such a string that is
immediately followed by a tail or if the final  flips all land on heads.
Consequently, for 
, let 
 be the event that flips 
 are all
heads and flip 
 is a tail; also, let 
 be the event that flips 
are all heads. Note that
Thus, when 
 is small, each of the events 
 has a small probability of
occurring. Moreover, for 
, if the events 
 and 
 refer to nonoverlapping
sequences of flips, then 
; if they refer to overlapping sequences,
then 
. Hence, in both cases, the conditional probabilities are close to
the unconditional ones, indicating that , the number of the events 
 that occur,
෍
௜ൌଵ
௡
𝑝௜.
𝑛
𝑝
𝑘
𝑖ൌ1, …, 𝑛െ𝑘൅1
𝐻௜
𝑖, 𝑖൅1, …, 𝑖൅𝑘െ1
𝐻௜
𝐻௜
𝑖
𝑘
𝑃ሺ𝐻௜ሻൌ𝑝௞
𝑝௞
𝐻௜
2, …, 𝑘൅1
1, …, 𝑘
𝑘൅1
𝑃ሺ𝐻ଶ|𝐻ଵሻൌ𝑝
𝐻ଶ
𝑘
𝑘
𝑖ൌ1, …, 𝑛െ𝑘
𝐸௜
𝑖, …, 𝑖൅𝑘െ1
𝑖൅𝑘
𝐸௡െ௞൅ଵ
𝑛െ𝑘൅1, …, 𝑛
𝑃ሺ𝐸௜ሻൌ
𝑝௞ሺ1 െ𝑝ሻ,  𝑖൑𝑛െ𝑘
𝑃ሺ𝐸௡െ௞൅ଵሻൌ
𝑝௞
𝑝௞
𝐸௜
𝑖്𝑗
𝐸௜
𝐸௝
𝑃ሺ𝐸௜|𝐸௝ሻൌ𝑃ሺ𝐸௜ሻ
𝑃ሺ𝐸௜|𝐸௝ሻൌ0
𝑁
𝐸௜
243 of 848

should have an approximate Poisson distribution with mean
Because there will not be a run of  heads if (and only if) 
, the preceding
gives
If we let 
 denote the largest number of consecutive heads in the  flips, then,
because 
 will be less than  if (and only if) there are no head strings of length 
, the preceding equation can be written as
Now, let us suppose that the coin being flipped is fair; that is, suppose that
. Then the preceding gives
where the final approximation supposes that 
 (that is, that 
).
Let 
, and assume that  is an integer. For 
,
Consequently,
which implies that
For instance,
𝐸ሾ𝑁ሿൌ
෍
௜ൌଵ
௡െ௞൅ଵ
𝑃ሺ𝐸௜ሻൌሺ𝑛െ𝑘ሻ𝑝௞ሺ1 െ𝑝ሻ൅𝑝௞
𝑘
𝑁ൌ0
𝑃ሺ no head strings of length 𝑘 ሻൌ𝑃ሺ𝑁ൌ0ሻൎexp൛െሺ𝑛െ𝑘ሻ𝑝௞ሺ1 െ𝑝ሻെ𝑝௞ൟ
𝐿௡
𝑛
𝐿௡
𝑘
𝑘
𝑃൛𝐿௡൏𝑘ൟൎexp൛െ ሺ𝑛െ𝑘ሻ𝑝௞ሺ1 െ𝑝ሻെ𝑝௞ൟ
𝑝ൌ1/2
𝑃ሼ𝐿௡൏𝑘ሽൎexpቊെ  𝑛െ𝑘൅2
2௞൅ଵ
ቋൎexp൜െ
𝑛
2௞൅ଵൠ
𝑒
ೖെమ
మೖ൅భൎ1
𝑘െ2
2௞൅ଵൎ0
𝑗ൌlogଶ𝑛
𝑗
𝑘ൌ𝑗൅𝑖
𝑛
2௞൅ଵൌ
𝑛
2௝2௜൅ଵൌ
1
2௜൅ଵ
𝑃ሼ𝐿௡൏𝑗൅𝑖ሽൎexpሼെሺ1/2ሻ௜൅ଵሽ
𝑃ሼ𝐿௡ൌ𝑗൅𝑖ሽ
ൌ
𝑃ሼ𝐿௡൏𝑗൅𝑖൅1ሽെ𝑃ሼ𝐿௡൏𝑗൅𝑖ሽ
ൎ
exp൛െሺ1/2ሻ௜൅ଶൟെexp൛െሺ1/2ሻ௜൅ଵൟ
244 of 848

Thus, we observe the rather interesting fact that no matter how large  is, the
length of the longest run of heads in a sequence of  flips of a fair coin will be
within  of 
 with a probability approximately equal to 
.
We now derive an exact expression for the probability that there is a string of 
consecutive heads when a coin that lands on heads with probability  is flipped 
times. With the events 
, as defined earlier, and with 
denoting, as before, the length of the longest run of heads,
The inclusion–exclusion identity for the probability of a union can be written as
Let 
 denote the set of flip numbers to which the event 
 refers. (So, for
instance, 
.) Now, consider one of the -way intersection
probabilities that does not include the event 
. That is, consider
 where 
. On the one hand, if there is any
overlap in the sets 
 then this probability is . On the other hand, if there
is no overlap, then the events 
 are independent. Therefore,
We must now determine the number of different choices of
 for which there is no overlap in the sets 
. To do
so, note first that each of the 
, 
, refer to 
 flips, so, without any
𝑃ሼ𝐿௡൏𝑗െ3ሽ
ൎ𝑒െସൎ.0183
𝑃ሼ𝐿௡ൌ𝑗െ3ሽ
ൎ𝑒െଶെ𝑒െସൎ.1170
𝑃ሼ𝐿௡ൌ𝑗െ2ሽ
ൎ𝑒െଵെ𝑒െଶൎ.2325
𝑃ሼ𝐿௡ൌ𝑗െ1ሽ
ൎ𝑒െଵ/ଶെ𝑒െଵൎ.2387
𝑃ሼ𝐿௡ൌ𝑗ሽ
ൎ𝑒െଵ/ସെ𝑒െଵ/ଶൎ.1723
𝑃ሼ𝐿௡ൌ𝑗൅1ሽ
ൎ𝑒െଵ/଼െ𝑒െଵ/ସൎ.1037
𝑃ሼ𝐿௡ൌ𝑗൅2ሽ
ൎ𝑒െଵ/ଵ଺െ𝑒െଵ/଼ൎ.0569
𝑃ሼ𝐿௡ൌ𝑗൅3ሽ
ൎ𝑒െଵ/ଷଶെ𝑒െଵ/ଵ଺ൎ.0298
𝑃ሼ𝐿௡൒𝑗൅4ሽ
ൎ1 െ𝑒െଵ/ଷଶൎ.0308
𝑛
𝑛
2
logଶሺ𝑛ሻെ1
. 86
𝑘
𝑝
𝑛
𝐸௜, 𝑖ൌ1, …, 𝑛െ𝑘൅1
𝐿௡
𝑃ሺ𝐿௡൒𝑘ሻൌ𝑃ሺ there is a string of 𝑘 consecutive heads ሻൌ𝑃ቀ
௜ൌଵ
௡െ௞൅ଵ𝐸௜ቁ
𝑃ሺ∪௜ൌଵ
௡െ௞൅ଵ𝐸௜ሻൌ
෍
௥ൌଵ
௡െ௞൅ଵ
ሺെ1ሻ௥൅ଵ
෍
௜భழ⋯ழ௜ೝ
𝑃ሺ𝐸௜భ⋯𝐸௜ೝሻ
𝑆௜
𝐸௜
𝑆ଵൌሼ1, …, 𝑘൅1ሽ
𝑟
𝐸௡െ௞൅ଵ
𝑃ሺ𝐸௜భ⋯𝐸௜ೝሻ
𝑖ଵ൏⋯൏𝑖௥൏𝑛െ𝑘൅1
𝑆௜భ, …, 𝑆௜ೝ
0
𝐸௜భ, …, 𝐸௜ೝ
𝑃ሺ𝐸௜భ⋯𝐸௜ೝሻൌ൝
0,
  if there is any overlap in 𝑆௜భ, …, 𝑆௜ೝ 
𝑝௥௞ሺ1 െ𝑝ሻ௥,
  if there is no overlap 
𝑖ଵ൏⋯൏𝑖௥൏𝑛െ𝑘൅1
𝑆௜భ, …, 𝑆௜ೝ
𝑆௜ೕ𝑗ൌ1, …, 𝑟
𝑘൅1
245 of 848

overlap, they together refer to 
 flips. Now consider any permutation of 
identical letters  and of 
 identical letters . Interpret the number of 
‘s before the first  as the number of flips before 
, the number of ‘s between
the first and second  as the number of flips between 
 and 
, and so on, with
the number of ‘s after the final  representing the number of flips after 
.
Because there are 
 permutations of  letters  and of 
 letters
, with every such permutation corresponding (in a one-to-one fashion) to a
different nonoverlapping choice, it follows that
We must now consider -way intersection probabilities of the form
where 
. Now, this probability will equal  if there is any
overlap in 
 if there is no overlap, then the events of the
intersection will be independent, so
By a similar argument as before, the number of nonoverlapping sets 
,
 will equal the number of permutations of 
 letters  (one for each of the
sets 
 and of 
 letters  (one
for each of the trials that are not part of any of 
. Since there
are 
 permutations of 
 letters  and of 
 letters , we
have
Putting it all together yields the exact expression, namely,
where we utilize the convention that 
 if 
.
𝑟ሺ𝑘൅1ሻ
𝑟
𝑎
𝑛െ𝑟ሺ𝑘൅1ሻ
𝑏
𝑏
𝑎
𝑆௜భ
𝑏
𝑎
𝑆௜భ
𝑆௜మ
𝑏
𝑎
𝑆௜ೝ
ቆ𝑛െ𝑟𝑘
𝑟
ቇ
𝑟
𝑎
𝑛െ𝑟ሺ𝑘൅1ሻ
𝑏
෍
௜భழ… ழ௜ೝழ௡െ௞൅ଵ
𝑃ሺ𝐸௜భ⋯𝐸௜ೝሻൌቆ𝑛െ𝑟𝑘
𝑟
ቇ𝑝௥௞ሺ1 െ𝑝ሻ௥
𝑟
𝑃ሺ𝐸௜భ⋯𝐸௜ೝെభ𝐸௡െ௞൅ଵሻ,
𝑖ଵ൏… ൏𝑖௥െଵ൏𝑛െ𝑘൅1
0
𝑆௜భ, …, 𝑆௜ೝെభ, 𝑆௡െ௞;
𝑃ሺ𝐸௜భ⋯𝐸௜ೝെభ𝐸௡െ௞൅ଵሻൌൣ𝑝௞ሺ1 െ𝑝ሻ൧௥െଵ𝑝௞ൌ𝑝௞௥ሺ1 െ𝑝ሻ௥െଵ
𝑆௜భ, …, 𝑆௜ೝെభ
𝑆௡െ௞
𝑟െ1
𝑎
𝑆௜భ, …, 𝑆௜ೝെభሻ
𝑛െሺ𝑟െ1ሻሺ𝑘൅1ሻെ𝑘ൌ𝑛െ𝑟𝑘െሺ𝑟െ1ሻ
𝑏
𝑆௜భ, …, 𝑆௜ೝെభ, 𝑆௡െ௞൅ଵሻ
ቆ𝑛െ𝑟𝑘
𝑟െ1 ቇ
𝑟െ1
𝑎
𝑛െ𝑟𝑘െሺ𝑟െ1ሻ
𝑏
෍
௜భழ… ழ௜ೝെభழ௡െ௞൅ଵ
𝑃ሺ𝐸௜భ⋯𝐸௜ೝെభ𝐸௡െ௞൅ଵሻൌቆ𝑛െ𝑟𝑘
𝑟െ1 ቇ𝑝௞௥ሺ1 െ𝑝ሻ௥െଵ
𝑃ሺ𝐿௡൒𝑘ሻൌ
෍
௥ൌଵ
௡െ௞൅ଵ
ሺെ1ሻ௥൅ଵ቎ቆ𝑛െ𝑟𝑘
𝑟
ቇ൅1
𝑝ቆ𝑛െ𝑟𝑘
𝑟െ1 ቇ቏𝑝௞௥ሺ1 െ𝑝ሻ௥
ቆ𝑚ൌ0
𝑗
ቇ
𝑚൏𝑗
246 of 848

From a computational point of view, a more efficient method for computing the
desired probability than the use of the preceding identity is to derive a set of
recursive equations. To do so, let 
 be the event that there is a string of 
consecutive heads in a sequence of  flips, and let 
 We will derive a
set of recursive equations for 
 by conditioning on when the first tail appears.
For 
, let 
 be the event that the first tail appears on flip , and let  be
the event that the first  flips are all heads. Because the events 
 are
mutually exclusive and exhaustive (that is, exactly one of these events must
occur), we have
Now, given that the first tail appears on flip , where 
, it follows that those 
flips are wasted as far as obtaining a string of  heads in a row; thus, the
conditional probability of this event is the probability that such a string will occur
among the remaining 
 flips. Therefore,
Because 
, the preceding equation gives
Starting with 
, 
, and 
, we can use the latter formula to
recursively compute 
, and so on, up to 
. For instance, suppose we
want the probability that there is a run of  consecutive heads when a fair coin is
flipped  times. Then, with 
, we have 
, 
. Because, when
, the recursion becomes
we obtain
𝐴௡
𝑘
𝑛
𝑃௡ൌ𝑃ሺ𝐴௡ሻ.
𝑃௡
𝑗ൌ1, …, 𝑘
𝐹௝
𝑗
𝐻
𝑘
𝐹ଵ, …, 𝐹௞, 𝐻
𝑃ሺ𝐴௡ሻൌ
෍
௝ൌଵ
௞
𝑃ሺ𝐴௡|𝐹௝ሻ𝑃ሺ𝐹௝ሻ൅𝑃ሺ𝐴௡|𝐻ሻ𝑃ሺ𝐻ሻ
𝑗
𝑗൏𝑘
𝑗
𝑘
𝑛െ𝑗
𝑃ሺ𝐴௡|𝐹௝ሻൌ𝑃௡െ௝
𝑃ሺ𝐴௡|𝐻ሻൌ1
𝑃௡
ൌ
𝑃ሺ𝐴௡ሻ
ൌ
෍
௝ൌଵ
௞
𝑃௡െ௝ 𝑃ሺ𝐹௝ሻ൅𝑃ሺ𝐻ሻ
ൌ
෍
௝ൌଵ
௞
𝑃௡െ௝ 𝑝 ௝െଵሺ1 െ𝑝ሻ൅𝑝௞
𝑃௝ൌ0 𝑗൏𝑘
𝑃௞ൌ𝑝௞
𝑃௞൅ଵ, 𝑃௞൅ଶ
𝑃௡
2
4
𝑘ൌ2
𝑃ଵൌ0 𝑃ଶൌሺ1/2ሻଶ
𝑝ൌ1/2
𝑃௡ൌ
෍
௝ൌଵ
௞
𝑃௡െ௝ ሺ1/2ሻ௝൅ሺ1/2ሻ௞
𝑃ଷൌ𝑃ଶሺ1/2ሻ൅𝑃ଵሺ1/2ሻଶ൅ሺ1/2ሻଶൌ3/8
247 of 848

and
which is clearly true because there are  outcomes that result in a string of 
consecutive heads: hhhh, hhht, hhth, hthh, thhh, hhtt, thht, and tthh. Each of
these outcomes occurs with probability 
.
Another use of the Poisson probability distribution arises in situations where “events”
occur at certain points in time. One example is to designate the occurrence of an
earthquake as an event; another possibility would be for events to correspond to
people entering a particular establishment (bank, post office, gas station, and so on);
and a third possibility is for an event to occur whenever a war starts. Let us suppose
that events are indeed occurring at certain (random) points of time, and let us
assume that for some positive constant , the following assumptions hold true:
1. The probability that exactly 1 event occurs in a given interval of length  is
equal to 
, where 
 stands for any function 
 for which
. [For instance, 
 is 
, whereas 
 is not.]
2. The probability that 2 or more events occur in an interval of length  is equal
to 
.
3. For any integers 
 and any set of  nonoverlapping intervals, if
we define 
 to be the event that exactly  of the events under consideration
occur in the th of these intervals, then events 
 are independent.
Loosely put, assumptions 1 and 2 state that for small values of , the probability that
exactly 1 event occurs in an interval of size  equals 
 plus something that is small
compared with , whereas the probability that 2 or more events occur is small
compared with . Assumption 3 states that whatever occurs in one interval has no
(probability) effect on what will occur in other, nonoverlapping intervals.
We now show that under assumptions 1, 2, and 3, the number of events occurring in
any interval of length  is a Poisson random variable with parameter 
. To be
precise, let us call the interval [0, ] and denote the number of events occurring in
that interval by 
. To obtain an expression for 
, we start by breaking
the interval [0, ] into  nonoverlapping subintervals, each of length t/n (Figure
4.9
).
Figure 4.9
𝑃ସൌ𝑃ଷሺ1/2ሻ൅𝑃ଶሺ1/2ሻଶ൅ሺ1/2ሻଶൌ1/2
8
2
1/16
𝜆
ℎ
𝜆ℎ൅𝑜ሺℎሻ
𝑜ሺℎሻ
𝑓ሺℎሻ
lim
௛→଴𝑓ሺℎሻ/ℎൌ0
𝑓ሺℎሻൌℎଶ
𝑜ሺℎሻ
𝑓ሺℎሻൌℎ
ℎ
𝑜ሺℎሻ
𝑛,  𝑗ଵ,  𝑗ଶ, …,  𝑗௡
𝑛
𝐸௜
𝑗௜
𝑖
𝐸ଵ, 𝐸ଶ, …, 𝐸௡
ℎ
ℎ
𝜆ℎ
ℎ
ℎ
𝑡
𝜆𝑡
𝑡
𝑁ሺ𝑡ሻ
𝑃ሼ𝑁ሺ𝑡ሻൌ𝑘ሽ
𝑡
𝑛
248 of 848

Now,
The preceding equation holds because the event on the left side of Equation
(7.2)
, that is, 
, is clearly equal to the union of the two mutually exclusive
events on the right side of the equation. Letting  and  denote the two mutually
exclusive events on the right side of Equation (7.2)
, we have
Now, for any 
 as 
, so 
 as 
, by the definition
of 
. Hence,
Moreover, since assumptions 1 and 2 imply that†
The sum of two functions, both of which are o(h), is also o(h). This is
so because if 
, then
.
𝑃ሼ𝑁ሺ𝑡ሻൌ𝑘ሽ
ൌ
𝑃ሼ 𝑘 of the 𝑛 subintervals contain exactly 1 event and the other 𝑛െ𝑘 contain 0 eve
൅𝑃ሼ𝑁ሺ𝑡ሻൌ𝑘  and at least 1 subinterval contains 2 or more events ሽ
(7.2)
ሼ𝑁ሺ𝑡ሻൌ𝑘ሽ
𝐴
𝐵
𝑃ሺ𝐵ሻ൑
𝑃ሼ at least one subinterval contains 2 or more events ሽ
ൌ
𝑃ቆ
௜ൌଵ
௡
ሼ𝑖th subinterval contains 2 or more events ሽቇ
൑
෍
௜ൌଵ
௡
𝑃ሼ𝑖th subinterval contains 2 or more events ሽ  by Boole′s 
inequality
ൌ
෍
௜ൌଵ
௡
𝑜൬𝑡
𝑛൰  by assumption 2 
ൌ
ൌ𝑛𝑜൬𝑡
𝑛൰
ൌ
𝑡ቈ𝑜ሺ𝑡/𝑛ሻ
𝑡/𝑛቉
𝑡, 𝑡/𝑛→0
𝑛→∞
𝑜ሺ𝑡/𝑛ሻ/ሺ𝑡/𝑛ሻ→0
𝑛→∞
𝑜ሺℎሻ
𝑃ሺ𝐵ሻ→0 as 𝑛→∞
(7.3)
𝑃ሼ 0 events occur in an interval of length  hሽ
   ൌ1 െሾ𝜆ℎ൅𝑜ሺℎሻ൅𝑜ሺℎሻሿൌ1 െ𝜆ℎെ𝑜ሺℎሻ
†
lim௛→଴𝑓ሺℎሻ/ℎൌlim௛→଴𝑔ሺℎሻ/ℎൌ0
lim௛→଴ሾ𝑓ሺℎሻ൅𝑔ሺℎሻሿ/ℎൌ0
249 of 848

we see from the independence assumption (number 3) that
However, since
it follows, by the same argument that verified the Poisson approximation to the
binomial, that
Thus, from Equations (7.2)
, (7.3)
, and (7.4)
, by letting 
, we obtain
Hence, if assumptions 1, 2, and 3 are satisfied, then the number of events occurring
in any fixed interval of length  is a Poisson random variable with mean 
, and we
say that the events occur in accordance with a Poisson process having rate . The
value , which can be shown to equal the rate per unit time at which events occur, is
a constant that must be empirically determined.
The preceding discussion explains why a Poisson random variable is usually a good
approximation for such diverse phenomena as the following:
1. The number of earthquakes occurring during some fixed time span
2. The number of wars per year
3. The number of electrons emitted from a heated cathode during a fixed time
period
4. The number of deaths, in a given period of time, of the policyholders of a life
insurance company
Example 7e
𝑃ሺ𝐴ሻ
ൌ𝑃ሼ𝑘  of the subintervals contain exactly 1 event and the other 
  𝑛െ𝑘  contain 0 events ሽ
ൌቆ𝑛
𝑘ቇቈ𝜆𝑡
𝑛൅𝑜൬𝑡
𝑛൰቉
௞
ቈ1 െቆ𝜆𝑡
𝑛ቇെ𝑜൬𝑡
𝑛൰቉
௡െ௞
𝑛ቈ𝜆𝑡
𝑛൅𝑜൬𝑡
𝑛൰቉ൌ𝜆𝑡൅𝑡ቈ𝑜ሺ𝑡/𝑛ሻ
𝑡/𝑛቉→𝜆𝑡as 𝑛→∞
𝑃ሺ𝐴ሻ→𝑒െఒ௧ሺ𝜆𝑡ሻ௞
𝑘!
as 𝑛→∞
(7.4)
𝑛→∞
𝑃ሼ𝑁ሺ𝑡ሻൌ𝑘ሽൌ𝑒െఒ௧ሺ𝜆𝑡ሻ௞
𝑘!
𝑘ൌ0, 1, …
(7.5)
𝑡
𝜆𝑡
𝜆
𝜆
250 of 848

Suppose that earthquakes occur in the western portion of the United States in
accordance with assumptions 1, 2, and 3, with 
 and with 1 week as the unit
of time. (That is, earthquakes occur in accordance with the three assumptions at
a rate of 2 per week.)
a. Find the probability that at least 3 earthquakes occur during the next 2
weeks.
b. Find the probability distribution of the time, starting from now, until the next
earthquake.
Solution
a. From Equation (7.5)
, we have
b. Let  denote the amount of time (in weeks) until the next earthquake.
Because  will be greater than  if and only if no events occur within the
next  units of time, we have, from Equation (7.5)
,
so the probability distribution function  of the random variable  is given
by
If  is Poisson with parameter , then
Starting with 
, we can use (7.6
) to compute successively
𝜆ൌ2
𝑃ሼ𝑁ሺ2ሻ൒3ሽ
ൌ
1 െ𝑃ሼ𝑁ሺ2ሻൌ0ሽെ𝑃ሼ𝑁ሺ2ሻൌ1ሽെ𝑃ሼ𝑁ሺ2ሻൌ2ሽ
ൌ
1 െ𝑒െସെ4𝑒െସെ4ଶ
2 𝑒െସ
ൌ
1 െ13𝑒െସ
𝑋
𝑋
𝑡
𝑡
𝑃൛𝑋൐𝑡ൟൌ𝑃൛𝑁ሺ𝑡ሻൌ0ൟൌ𝑒െఒ௧
𝐹
𝑋
𝐹ሺ𝑡ሻൌ𝑃ሼ𝑋൑𝑡ሽൌ1 െ𝑃ሼ𝑋൐𝑡ሽ
ൌ
1 െ𝑒െఒ௧
ൌ
1 െ𝑒െଶ௧
𝑋
𝜆
𝑃ሼ𝑋ൌ𝑖൅1ሽ
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ𝑒െఒ𝜆௜൅ଵ/ሺ𝑖൅1ሻ!
𝑒െఒ𝜆௜/𝑖!
ൌ
𝜆
𝑖൅1
(7.6)
𝑃൛𝑋ൌ0ൟൌ𝑒െఒ
251 of 848

We can use a module to compute the Poisson probabilities for Equation (7.6)
.
Example 7f
a. Determine 
 when  is Poisson with mean 100.
b. Determine 
 when  is Poisson with mean 1000.
Solution
Using the Poisson calculator of StatCrunch yields the solutions:
a. 
b. 
Suppose that independent trials, each having a probability 
, of being a
success, are performed until a success occurs. If we let  equal the number of trials
required, then
Equation (8.1)
 follows because, in order for  to equal , it is necessary and
sufficient that the first 
 trials are failures and the th trial is a success. Equation
(8.1)
 then follows, since the outcomes of the successive trials are assumed to be
independent.
Since
𝑃ሼ𝑋ൌ1ሽ
ൌ
𝜆𝑃ሼ𝑋ൌ0ሽ
𝑃ሼ𝑋ൌ2ሽ
ൌ
𝜆
2 𝑃ሼ𝑋ൌ1ሽ
⋮
𝑃ሼ𝑋ൌ𝑖൅1ሽ
ൌ
𝜆
𝑖൅1 𝑃ሼ𝑋ൌ𝑖ሽ
𝑃ሼ𝑋൑90ሽ
𝑋
𝑃ሼ𝑌൑1075ሽ
𝑌
𝑃ሼ𝑋൑90ሽൌ. 17138
𝑃ሼ𝑌൑1075ሽൌ. 99095
𝑝, 0 ൏𝑝൏1
𝑋
𝑃ሼ𝑋ൌ𝑛ሽൌሺ1 െ𝑝ሻ௡െଵ𝑝 𝑛ൌ1, 2, …
(8.1)
𝑋
𝑛
𝑛െ1
𝑛
෍
௡ൌଵ
ஶ
𝑃ሼ𝑋ൌ𝑛ሽൌ𝑝෍
௡ൌଵ
ஶ
ሺ1 െ𝑝ሻ௡െଵൌ
𝑝
1 െሺ1 െ𝑝ሻൌ1
252 of 848

it follows that with probability 1, a success will eventually occur. Any random variable
 whose probability mass function is given by Equation (8.1)
 is said to be a
geometric random variable with parameter .
Example 8a
An urn contains  white and 
 black balls. Balls are randomly selected, one at a
time, until a black one is obtained. If we assume that each ball selected is
replaced before the next one is drawn, what is the probability that
a. exactly  draws are needed?
b. at least  draws are needed?
Solution
If we let  denote the number of draws needed to select a black ball, then 
satisfies Equation (8.1)
 with 
. Hence,
a. 
b. 
Of course, part (b) could have been obtained directly, since the probability that at
least  trials are necessary to obtain a success is equal to the probability that the
first 
 trials are all failures. That is, for a geometric random variable,
Example 8b
Find the expected value of a geometric random variable.
Solution
With 
, we have
𝑋
𝑝
𝑁
𝑀
𝑛
𝑘
𝑋
𝑋
𝑝ൌ𝑀/ሺ𝑀൅𝑁ሻ
𝑃ሼ𝑋ൌ𝑛ሽൌቆ
𝑁
𝑀൅𝑁ቇ
௡െଵ
𝑀
𝑀൅𝑁ൌ𝑀𝑁௡െଵ
ሺ𝑀൅𝑁ሻ௡
𝑃ሼ𝑋൒𝑘ሽ
ൌ
𝑀
𝑀൅𝑁
෍
௡ൌ௞
ஶ
ቆ
𝑁
𝑀൅𝑁ቇ
௡െଵ
ൌ
ቆ
𝑀
𝑀൅𝑁ቇቆ
𝑁
𝑀൅𝑁ቇ
௞െଵ
⎡
⎣
1 െ
𝑁
𝑀൅𝑁
⎤
⎦
ൌ
ቆ
𝑁
𝑀൅𝑁ቇ
௞െଵ
𝑘
𝑘െ1
𝑃ሼ𝑋൒𝑘ሽൌሺ1 െ𝑝ሻ௞െଵ
𝑞ൌ1 െ𝑝
253 of 848

Hence,
yielding the result
In other words, if independent trials having a common probability  of being
successful are performed until the first success occurs, then the expected
number of required trials equals 1/ . For instance, the expected number of rolls
of a fair die that it takes to obtain the value 1 is 6.
Example 8c
Find the variance of a geometric random variable.
Solution
To determine Var
, let us first compute 
. With 
, we have
𝐸ሾ𝑋ሿ
ൌ
෍
௜ൌଵ
ஶ
𝑖𝑞௜െଵ𝑝
ൌ
෍
௜ൌଵ
ஶ
ሺ𝑖െ1 ൅1ሻ𝑞௜െଵ𝑝
ൌ
෍
௜ൌଵ
ஶ
ሺ𝑖െ1ሻ𝑞௜െଵ𝑝൅෍
௜ൌଵ
ஶ
𝑞௜െଵ𝑝
ൌ
෍
௜ൌଵ
ஶ
𝑗𝑞௜𝑝൅1
ൌ
𝑞෍
௝ൌଵ
ஶ
𝑗𝑞௝െଵ𝑝൅1
ൌ
𝑞𝐸ሾ𝑋ሿ൅1
𝑝𝐸ሾ𝑋ሿൌ1
𝐸ሾ𝑋ሿൌ1
𝑝
𝑝
𝑝
ሺ𝑋ሻ
𝐸ൣ𝑋ଶ൧
𝑞ൌ1 െ𝑝
254 of 848

Using 
, the equation for 
 yields
Hence,
giving the result
Suppose that independent trials, each having probability 
, of being a
success are performed until a total of  successes is accumulated. If we let  equal
the number of trials required, then
Equation (8.2)
 follows because, in order for the th success to occur at the th
trial, there must be 
 successes in the first 
 trials and the th trial must be a
success. The probability of the first event is
𝐸ሾ𝑋ଶሿ
ൌ
෍
௜ൌଵ
ஶ
𝑖ଶ𝑞௜െଵ𝑝
ൌ
෍
௜ൌଵ
ஶ
ሺ𝑖െ1 ൅1ሻଶ𝑞௜െଵ𝑝
ൌ
෍
௜ൌଵ
ஶ
ሺ𝑖െ1ሻଶ𝑞௜െଵ𝑝൅෍
௜ൌଵ
ஶ
2ሺ𝑖െ1ሻ𝑞௜െଵ𝑝൅෍
௜ൌଵ
ஶ
𝑞௜െଵ𝑝
ൌ
෍
௜ൌଵ
ஶ
𝑗ଶ𝑞௝𝑝൅2 ෍
௝ൌଵ
ஶ
𝑗𝑞௝𝑝൅1
ൌ
𝑞𝐸ሾ𝑋ଶሿ൅2𝑞𝐸ሾ𝑋ሿ൅1
𝐸ሾ𝑋ሿൌ1/𝑝
𝐸ൣ𝑋ଶ൧
𝑝𝐸ሾ𝑋ଶሿൌ2𝑞
𝑝൅1
𝐸ሾ𝑋ଶሿൌ2𝑞൅𝑝
𝑝ଶ
ൌ𝑞൅1
𝑝ଶ
Varሺ𝑋ሻൌ𝑞൅1
𝑝ଶ
െ1
𝑝ଶൌ𝑞
𝑝ଶൌ1 െ𝑝
𝑝ଶ
𝑝, 0 ൏𝑝൏1
𝑟
𝑋
𝑃ሼ𝑋ൌ𝑛ሽൌቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥
𝑛ൌ𝑟, 𝑟൅1, …
(8.2)
𝑟
𝑛
𝑟െ1
𝑛െ1
𝑛
255 of 848

and the probability of the second is ; thus, by independence, Equation (8.2)
 is
established. To verify that a total of  successes must eventually be accumulated,
either we can prove analytically that
or we can give a probabilistic argument as follows: The number of trials required to
obtain  successes can be expressed as 
, where 
 equals the
number of trials required for the first success, 
 the number of additional trials after
the first success until the second success occurs, 
 the number of additional trials
until the third success, and so on. Because the trials are independent and all have
the same probability of success, it follows that 
 are all geometric random
variables. Hence, each is finite with probability 1, so 
 must also be finite,
establishing Equation (8.3)
.
Any random variable  whose probability mass function is given by Equation
(8.2)
 is said to be a negative binomial random variable with parameters ( , ).
Note that a geometric random variable is just a negative binomial with parameter (1,
).
In the next example, we use the negative binomial to obtain another solution of the
problem of the points.
Example 8d
If independent trials, each resulting in a success with probability , are
performed, what is the probability of  successes occurring before  failures?
Solution
The solution will be arrived at by noting that  successes will occur before 
failures if and only if the th success occurs no later than the (
) trial. This
follows because if the th success occurs before or at the (
) trial, then it
must have occurred before the th failure, and conversely. Hence, from Equation
(8.2)
, the desired probability is
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥െଵሺ1 െ𝑝ሻ௡െ௥
𝑝
𝑟
෍
௡ൌ௥
ஶ
𝑃ሼ𝑋ൌ𝑛ሽൌ
෍
௡ൌ௥
ஶ
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥ൌ1
(8.3)
𝑟
𝑌ଵ൅𝑌ଶ൅⋯൅𝑌௥
𝑌ଵ
𝑌ଶ
𝑌ଷ
𝑌ଵ, 𝑌ଶ, …, 𝑌௥
෍
௜ൌଵ
௥
𝑌௜
𝑋
𝑟𝑝
𝑝
𝑝
𝑟
𝑠
𝑟
𝑠
𝑟
𝑟൅𝑠െ1
𝑟
𝑟൅𝑠െ1
𝑠
256 of 848

Example 8e The Banach match problem
At all times, a pipe-smoking mathematician carries 2 matchboxes—1 in his left-
hand pocket and 1 in his right-hand pocket. Each time he needs a match, he is
equally likely to take it from either pocket. Consider the moment when the
mathematician first discovers that one of his matchboxes is empty. If it is
assumed that both matchboxes initially contained  matches, what is the
probability that there are exactly  matches, 
, in the other box?
Solution
Let  denote the event that the mathematician first discovers that the right-hand
matchbox is empty and that there are  matches in the left-hand box at the time.
Now, this event will occur if and only if the 
 choice of the right-hand
matchbox is made at the (
) trial. Hence, from Equation (8.2)
(with 
), we see that
Since there is an equal probability that it is the left-hand box that is first
discovered to be empty and there are  matches in the right-hand box at that
time, the desired result is
Example 8f
Compute the expected value and the variance of a negative binomial random
variable with parameters  and .
Solution
We have
෍
௡ൌ௥
௥൅௦െଵ
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥
𝑁
𝑘
𝑘ൌ0, 1, …, 𝑁
𝐸
𝑘
ሺ𝑁൅1ሻ
𝑁൅1 ൅𝑁െ𝑘
𝑝ൌ1
2, 𝑟ൌ𝑁൅1, and 𝑛ൌ2𝑁െ𝑘൅1
𝑃ሺ𝐸ሻൌቆ2𝑁െ𝑘
𝑁
ቇቆ1
2ቇ
ଶேെ௞൅ଵ
𝑘
2𝑃ሺ𝐸ሻൌቆ2𝑁െ𝑘
𝑁
ቇቆ1
2ቇ
ଶேെ௞
𝑟
𝑝
257 of 848

where  is a negative binomial random variable with parameters 
. Setting
 in the preceding equation yields
Setting 
 in the equation for 
 and using the formula for the expected
value of a negative binomial random variable gives
Therefore,
Thus, from Example 8f
, if independent trials, each of which is a success with
probability , are performed, then the expected value and variance of the number of
trials that it takes to amass  successes is r/p and 
, respectively.
Since a geometric random variable is just a negative binomial with parameter 
,
it follows from the preceding example that the variance of a geometric random
variable with parameter  is equal to 
, which checks with the result of
Example 8c
.
Example 8g
Find the expected value and the variance of the number of times one must throw
𝐸ൣ𝑋௞൧
ൌ
෍
௡ൌ௥
ஶ
𝑛௞ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥
ൌ
𝑟
𝑝
෍
௡ൌ௥
ஶ
𝑛௞െଵቆ𝑛
𝑟ቇ𝑝௥൅ଵሺ1 െ𝑝ሻ௡െ௥ since 𝑛ቆ𝑛െ1
𝑟െ1ቇൌ𝑟ቆ𝑛
𝑟ቇ
ൌ
𝑟
𝑝
෍
௠ൌ௥൅ଵ
ஶ
ሺ𝑚െ1ሻ௞െଵቆ𝑚െ1
𝑟
ቇ𝑝௥൅ଵሺ1 െ𝑝ሻ௠െሺ௥൅ଵሻ by setting 
𝑚ൌ𝑛൅1
ൌ
𝑟
𝑝𝐸ൣሺ𝑌െ1ሻ௞െଵ൧
𝑌
𝑟൅1, 𝑝
𝑘ൌ1
𝐸ሾ𝑋ሿൌ𝑟
𝑝
𝑘ൌ2
𝐸ሾ𝑋௞ሿ
𝐸ൣ𝑋ଶ൧
ൌ
𝑟
𝑝𝐸ሾ𝑌െ1ሿ
ൌ
𝑟
𝑝ቆ𝑟൅1
𝑝
െ1ቇ
Varሺ𝑋ሻ
ൌ
𝑟
𝑝ቆ𝑟൅1
𝑝
െ1ቇെቆ𝑟
𝑝ቇ
ଶ
ൌ
𝑟ሺ1 െ𝑝ሻ
𝑝ଶ
𝑝
𝑟
𝑟ሺ1 െ𝑝ሻ/𝑝ଶ
𝑟ൌ1
𝑝
ሺ1 െ𝑝ሻ/𝑝ଶ
258 of 848

a die until the outcome 1 has occurred 4 times.
Solution
Since the random variable of interest is a negative binomial with parameters
 and 
, it follows that
Now, let us suppose that the independent trials are not ended when there have been
a total of  successes, but that they continue on. Aside from , the number of trials
until there have been  successes, some other random variables of interest are, for
,
Y : the number of trials until there have been  failures;
V : the number of trials until there have been either  successes or  failures;
Z : the number of trials until there have been both at least  successes and at least 
failures.
Because each trial is independently a failure with probability 
, it follows that  is
a negative binomial random variable with probability mass function
To determine the probability mass function of 
, note that the possible
values of  are all less than 
. Suppose 
. If either the 
 success or the
 failure occurs at time  then, because 
, the other event would not yet
have occurred. Consequently,  will equal  if either  or  is equal to . Because we
cannot have both that 
 and that 
, this yields
To determine the probability mass function of 
, note that 
. For
, if either the 
 success or the 
 failure occurs at time  then the other
𝑟ൌ4
𝑝ൌ1
6
𝐸ሾ𝑋ሿൌ
24
Varሺ𝑋ሻൌ
4ቆ5
6ቇ
ቆ1
6ቇ
ଶൌ120
𝑟
𝑋
𝑟
𝑠൐0
𝑠
𝑟
𝑠
𝑟
𝑠
1 െ𝑝
𝑌
𝑃ሺ𝑌ൌ𝑛ሻൌቆ𝑛െ1
𝑠െ1ቇሺ1 െ𝑝ሻ௦𝑝௡െ௦ ,
𝑛൒𝑠
𝑉ൌminሺ𝑋, 𝑌ሻ
𝑉
𝑟൅𝑠
𝑛൏𝑟൅𝑠
𝑟௧௛
𝑠௧௛
𝑛
𝑛൏𝑟൅𝑠
𝑉
𝑛
𝑋
𝑌
𝑛
𝑋ൌ𝑛
𝑌ൌ𝑛
𝑃ሺ𝑉ൌ𝑛ሻൌ
𝑃ሺ𝑋ൌ𝑛ሻ൅𝑃ሺ𝑌ൌ𝑛ሻ
ൌ
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥൅ቆ𝑛െ1
𝑠െ1ቇሺ1 െ𝑝ሻ௦𝑝௡െ௦ ,
𝑛൏𝑟൅𝑠
𝑍ൌmaxሺ𝑋, 𝑌ሻ
𝑍൒𝑟൅𝑠
𝑛൒𝑟൅𝑠
𝑟௧௛
𝑠௧௛
𝑛
259 of 848

event must have already occurred by time . Consequently, for 
,  will equal
 if either  or  is equal to . This gives
Suppose that a sample of size  is to be chosen randomly (without replacement)
from an urn containing  balls, of which 
 are white and 
 are black. If we let 
denote the number of white balls selected, then
A random variable  whose probability mass function is given by Equation (8.4)
for some values of , , 
 is said to be a hypergeometric random variable.
Remark Although we have written the hypergeometric probability mass function with
 going from 0 to 
 will actually be 0, unless  satisfies the inequalities
. However, Equation (8.4)
 is always valid because of
our convention that 
 is equal to 0 when either 
 or 
.
Example 8h
An unknown number, say, , of animals inhabit a certain region. To obtain some
information about the size of the population, ecologists often perform the
following experiment: They first catch a number, say, 
, of these animals, mark
them in some manner, and release them. After allowing the marked animals time
to disperse throughout the region, a new catch of size, say, , is made. Let 
denote the number of marked animals in this second capture. If we assume that
the population of animals in the region remained fixed between the time of the
two catches and that each time an animal was caught it was equally likely to be
any of the remaining uncaught animals, it follows that  is a hypergeometric
random variable such that
𝑛
𝑛൒𝑟൅𝑠𝑍
𝑛
𝑋
𝑌
𝑛
𝑃ሺ𝑍ൌ𝑛ሻൌ
𝑃ሺ𝑋ൌ𝑛ሻ൅𝑃ሺ𝑌ൌ𝑛ሻ
ൌ
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥൅ቆ𝑛െ1
𝑠െ1ቇሺ1 െ𝑝ሻ௦𝑝௡െ௦ ,
𝑛൏𝑟൅𝑠
𝑛
𝑁
𝑚
𝑁െ𝑚
𝑋
𝑃ሼ𝑋ൌ𝑖ሽൌ
ቆ𝑚
𝑖ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇ
ቆ𝑁
𝑛ቇ
𝑖ൌ0, 1, …, 𝑛
(8.4)
𝑋
𝑛𝑁𝑚
𝑖
𝑛, 𝑃ሼ𝑋ൌ𝑖ሽ
𝑖
𝑛െሺ𝑁െ𝑚ሻ൑𝑖൑minሺ𝑛, 𝑚ሻ
ቆ𝑟
𝑘ቇ
𝑘൏0
𝑟൏𝑘
𝑁
𝑚
𝑛
𝑋
𝑋
260 of 848

Suppose now that  is observed to equal . Then, since 
 represents the
probability of the observed event when there are actually  animals present in
the region, it would appear that a reasonable estimate of  would be the value of
 that maximizes 
. Such an estimate is called a maximum likelihood
estimate. (See Theoretical Exercises 13
 and 18
 for other examples of this
type of estimation procedure.)
The maximization of 
 can be done most simply by first noting that
Now, the preceding ratio is greater than 1 if and only if
or, equivalently, if and only if
Thus, 
 is first increasing and then decreasing and reaches its maximum
value at the largest integral value not exceeding mn/i. This value is the maximum
likelihood estimate of . For example, suppose that the initial catch consisted of
 animals, which are marked and then released. If a subsequent catch
consists of 
 animals of which 
 are marked, then we would estimate
that there are some 500 animals in the region. (Note that the preceding estimate
could also have been obtained by assuming that the proportion of marked
animals in the region, m/N, is approximately equal to the proportion of marked
animals in our second catch, i/n.)
Example 8i
A purchaser of electrical components buys them in lots of size 10. It is his policy
to inspect 3 components randomly from a lot and to accept the lot only if all 3 are
nondefective. If 30 percent of the lots have 4 defective components and 70
percent have only 1, what proportion of lots does the purchaser reject?
Solution
𝑃ሼ𝑋ൌ𝑖ሽൌ
ቆ𝑚
𝑖ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇ
ቆ𝑁
𝑛ቇ
≡𝑃௜ሺ𝑁ሻ
𝑋
𝑖
𝑃௜ሺ𝑁ሻ
𝑁
𝑁
𝑁
𝑃௜ሺ𝑁ሻ
𝑃௜ሺ𝑁ሻ
𝑃௜ሺ𝑁ሻ
𝑃௜ሺ𝑁െ1ሻൌሺ𝑁െ𝑚ሻሺ𝑁െ𝑛ሻ
𝑁ሺ𝑁െ𝑚െ𝑛൅𝑖ሻ
ሺ𝑁െ𝑚ሻሺ𝑁െ𝑛ሻ൒𝑁ሺ𝑁െ𝑚െ𝑛൅𝑖ሻ
𝑁൑𝑚𝑛
𝑖
𝑃௜ሺ𝑁ሻ
𝑁
𝑚ൌ50
𝑛ൌ40
𝑖ൌ4
261 of 848

Let  denote the event that the purchaser accepts a lot. Now,
Hence, 46 percent of the lots are rejected.
If  balls are randomly chosen without replacement from a set of  balls of which the
fraction 
 is white, then the number of white balls selected is hypergeometric.
Now, it would seem that when 
 and  are large in relation to , it shouldn’t make
much difference whether the selection is being done with or without replacement,
because, no matter which balls have previously been selected, when 
 and  are
large, each additional selection will be white with a probability approximately equal to
. In other words, it seems intuitive that when 
 and  are large in relation to , the
probability mass function of  should approximately be that of a binomial random
variable with parameters  and . To verify this intuition, note that if  is
hypergeometric, then, for 
,
Example 8j
Determine the expected value and the variance of , a hypergeometric random
variable with parameters , , and 
.
𝐴
𝑃ሺ𝐴ሻൌ
𝑃ሺ𝐴| lot has 4 defectives ሻ3
10 ൅𝑃ሺ𝐴| lot has 1 defective ሻ7
10
ൌ
ቆ4
0ቇቆ6
3ቇ
ቆ10
3 ቇ
ቆ3
10ቇ൅
ቆ1
0ቇቆ9
3ቇ
ቆ10
3 ቇ
ቆ7
10ቇ
ൌ
54
100
𝑛
𝑁
𝑝ൌ𝑚/𝑁
𝑚
𝑁
𝑛
𝑚
𝑁
𝑝
𝑚
𝑁
𝑛
𝑋
𝑛
𝑝
𝑋
𝑖൑𝑛
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ
ቆ𝑚
𝑖ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇ
ቆ𝑁
𝑛ቇ
ൌ
𝑚!
ሺ𝑚െ𝑖ሻ! 𝑖!
ሺ𝑁െ𝑚ሻ!
ሺ𝑁െ𝑚െ𝑛൅𝑖ሻ! ሺ𝑛െ𝑖ሻ!
ሺ𝑁െ𝑛ሻ! 𝑛!
𝑁!
ൌ
ቆ𝑛
𝑖ቇ𝑚
𝑁
𝑚െ1
𝑁െ1⋯𝑚െ𝑖൅1
𝑁െ𝑖൅1
𝑁െ𝑚
𝑁െ𝑖
𝑁െ𝑚െ1
𝑁െ𝑖െ1
⋯𝑁െ𝑚െሺ𝑛െ𝑖െ1ሻ
𝑁െ𝑖െሺ𝑛െ𝑖െ1ሻ
ൎ
ቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜ when 𝑝ൌ𝑚/𝑁 and 𝑚 and 𝑁 are
large in relation to 𝑛 and 𝑖 
𝑋
𝑛𝑁
𝑚
262 of 848

Solution
Using the identities
we obtain
where  is a hypergeometric random variable with parameters 
, 
, and
. Hence, upon setting 
, we have
In words, if  balls are randomly selected from a set of  balls, of which 
 are
white, then the expected number of white balls selected is nm/N.
Upon setting 
 in the equation for 
, we obtain
where the final equality uses our preceding result to compute the expected value
of the hypergeometric random variable .
Because 
, we can conclude that
𝐸ൣ𝑋௞൧
ൌ
෍
௜ൌ଴
௡
𝑖௞𝑃ሼ𝑋ൌ𝑖ሽ
ൌ
෍
௜ൌଵ
௡
𝑖௞ቆ𝑚
𝑖ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇቆ𝑁
𝑛ቇ
𝑖ቆ𝑚
𝑖ቇൌ𝑚ቆ𝑚െ1
𝑖െ1 ቇand 𝑛ቆ𝑁
𝑛ቇൌ𝑁ቆ𝑁െ1
𝑛െ1ቇ
𝐸ൣ𝑋௞൧
ൌ
𝑛𝑚
𝑁
෍
௜ൌଵ
௡
𝑖௞െଵቆ𝑚െ1
𝑖െ1 ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇቆ𝑁െ1
𝑛െ1ቇ
ൌ
𝑛𝑚
𝑁
෍
௝ൌ଴
௡െଵ
ሺ𝑗൅1ሻ௞െଵቆ𝑚െ1
𝑗
ቇቆ𝑁െ𝑚
𝑛െ1 െ𝑗ቇቆ𝑁െ1
𝑛െ1ቇ
ൌ
𝑛𝑚
𝑁𝐸ൣሺ𝑌൅1ሻ௞െଵ൧
𝑌
𝑛െ1 𝑁െ1
𝑚െ1
𝑘ൌ1
𝐸ሾ𝑋ሿൌ𝑛𝑚
𝑁
𝑛
𝑁
𝑚
𝑘ൌ2
𝐸ൣ𝑋௞൧
𝐸ൣ𝑋ଶ൧
ൌ
𝑛𝑚
𝑁𝐸ሾ𝑌൅1ሿ
ൌ
𝑛𝑚
𝑁ቈሺ𝑛െ1ሻሺ𝑚െ1ሻ
𝑁െ1
൅1቉
𝑌
𝐸ሾ𝑋ሿൌ𝑛𝑚/𝑁
263 of 848

Letting 
 and using the identity
shows that
Remark We have shown in Example 8j
 that if  balls are randomly selected
without replacement from a set of  balls, of which the fraction  are white, then the
expected number of white balls chosen is np. In addition, if  is large in relation to 
[so that 
 is approximately equal to 1], then
In other words, 
 is the same as when the selection of the balls is done with
replacement (so that the number of white balls is binomial with parameters  and ),
and if the total collection of balls is large, then Var
 is approximately equal to what
it would be if the selection were done with replacement. This is, of course, exactly
what we would have guessed, given our earlier result that when the number of balls
in the urn is large, the number of white balls chosen approximately has the mass
function of a binomial random variable.
A random variable is said to have a zeta (sometimes called the Zipf) distribution if its
probability mass function is given by
for some value of 
. Since the sum of the foregoing probabilities must equal 1, it
follows that
Varሺ𝑋ሻൌ𝑛𝑚
𝑁ቈሺ𝑛െ1ሻሺ𝑚െ1ሻ
𝑁െ1
൅1 െ𝑛𝑚
𝑁቉
𝑝ൌ𝑚/𝑁
𝑚െ1
𝑁െ1 ൌ𝑁𝑝െ1
𝑁െ1 ൌ𝑝െ1 െ𝑝
𝑁െ1
Varሺ𝑋ሻൌ
𝑛𝑝⎡
⎣
ሺ𝑛െ1ሻ𝑝െሺ𝑛െ1ሻ1 െ𝑝
𝑁െ1 ൅1 െ𝑛𝑝⎤
⎦
ൌ
𝑛𝑝ሺ1 െ𝑝ሻቆ1 െ𝑛െ1
𝑁െ1ቇ
𝑛
𝑁
𝑝
𝑁
𝑛
ሺ𝑁െ𝑛ሻ/ሺ𝑁െ1ሻ
Varሺ𝑋ሻൎ𝑛𝑝ሺ1 െ𝑝ሻ
𝐸ሾ𝑋ሿ
𝑛
𝑝
ሺ𝑋ሻ
𝑃ሼ𝑋ൌ𝑘ሽൌ
𝐶
𝑘ఈ൅ଵ 𝑘ൌ1, 2, …
𝛼൐0
264 of 848

The zeta distribution owes its name to the fact that the function
is known in mathematical disciplines as the Riemann zeta function (after the German
mathematician G. F. B. Riemann).
The zeta distribution was used by the Italian economist V. Pareto to describe the
distribution of family incomes in a given country. However, it was G. K. Zipf who
applied zeta distribution to a wide variety of problems in different areas and, in doing
so, popularized its use.
A very important property of expectations is that the expected value of a sum of
random variables is equal to the sum of their expectations. In this section, we will
prove this result under the assumption that the set of possible values of the
probability experiment—that is, the sample space —is either finite or countably
infinite. Although the result is true without this assumption (and a proof is outlined in
the theoretical exercises), not only will the assumption simplify the argument, but it
will also result in an enlightening proof that will add to our intuition about
expectations. So, for the remainder of this section, suppose that the sample space 
is either a finite or a countably infinite set.
For a random variable , let 
 denote the value of  when 
 is the outcome of
the experiment. Now, if  and  are both random variables, then so is their sum. That
is, 
 is also a random variable. Moreover, 
Example 9a
Suppose that the experiment consists of flipping a coin  times, with the outcome
being the resulting sequence of heads and tails. Suppose  is the number of
heads in the first  flips and  is the number of heads in the final  flips. Let
 Then, for instance, for the outcome 
,
𝐶ൌ቎෍
௞ൌଵ
ஶ
ቆ1
𝑘ቇ
ఈ൅ଵ
቏െଵ
𝜁ሺ𝑠ሻൌ1 ൅ቆ1
2ቇ
௦
൅ቆ1
3ቇ
௦
൅⋯൅ቆ1
𝑘ቇ
௦
൅⋯
𝑆
𝑆
𝑋
𝑋ሺ𝑠ሻ
𝑋
𝑠∈𝑆
𝑋
𝑌
𝑍ൌX ൅Y 
𝑍ሺ𝑠ሻൌ𝑋ሺ𝑠ሻ൅𝑌ሺ𝑠ሻ.
5
𝑋
3
𝑌
2
𝑍ൌ𝑋൅𝑌.
𝑠ൌሺℎ, 𝑡, ℎ, 𝑡, ℎሻ
265 of 848

meaning that the outcome 
 results in  heads in the first three flips, 
head in the final two flips, and a total of  heads in the five flips.
Let 
 be the probability that  is the outcome of the experiment. Because
we can write any event  as the finite or countably infinite union of the mutually
exclusive events 
, it follows by the axioms of probability that
When 
, the preceding equation gives
Now, let  be a random variable, and consider 
. Because 
 is the value of 
when  is the outcome of the experiment, it seems intuitive that 
—the weighted
average of the possible values of , with each value weighted by the probability that
 assumes that value—should equal a weighted average of the values 
with 
 weighted by the probability that  is the outcome of the experiment. We
now prove this intuition.
Proposition 9.1
Proof Suppose that the distinct values of  are 
 For each , let 
 be the
event that  is equal to 
. That is, 
. Then,
𝑋ሺ𝑠ሻൌ
2
𝑌ሺ𝑠ሻൌ
1
𝑍ሺ𝑠ሻൌ
𝑋ሺ𝑠ሻ൅𝑌ሺ𝑠ሻൌ3
ሺℎ, 𝑡, ℎ, 𝑡, ℎሻ
2
1
3
𝑝ሺ𝑠ሻൌ𝑃ሺሼ𝑠ሽሻ
𝑠
𝐴
ሼ𝑠ሽ, 𝑠∈𝐴
𝑃ሺ𝐴ሻൌ
෍
௦∈஺
𝑝ሺ𝑠ሻ
𝐴ൌ𝑆
1 ൌ෍
௦∈ௌ
𝑝ሺ𝑠ሻ
𝑋
𝐸ሾ𝑋ሿ
𝑋ሺ𝑠ሻ
𝑋
𝑠
𝐸ሾ𝑋ሿ
𝑋
𝑋
𝑋ሺ𝑠ሻ, 𝑠  ∈ 𝑆,
𝑋ሺ𝑠ሻ
𝑠
𝐸ሾ𝑋ሿൌ෍
௦∈ௌ
𝑋ሺ𝑠ሻ 𝑝ሺ𝑠ሻ
𝑋
𝑥௜, 𝑖൒1 .
𝑖
𝑆௜
𝑋
𝑥௜
𝑆௜ൌሼ𝑠:𝑋ሺ𝑠ሻൌ𝑥௜ሽ
266 of 848

where the final equality follows because 
 are mutually exclusive events
whose union is .
Example 9b
Suppose that two independent flips of a coin that comes up heads with
probability  are made, and let  denote the number of heads obtained. Because
it follows from the definition of expected value that
which agrees with
We now prove the important and useful result that the expected value of a sum of
random variables is equal to the sum of their expectations.
Corollary 9.2
For random variables 
,
𝐸ሾ𝑋ሿ
ൌ
෍
௜
𝑥௜𝑃ሼ𝑋ൌ𝑥௜ሽ
ൌ
෍
௜
𝑥௜𝑃ሼ𝑆௜ሽ
ൌ
෍
௜
𝑥௜෍
௦∈ௌ೔
𝑝ሺ𝑠ሻ
ൌ
෍
௜
෍
௦∈ௌ೔
𝑥௜𝑝ሺ𝑠ሻ
ൌ
෍
௜
෍
௦∈ௌ೔
𝑋ሺ𝑠ሻ𝑝ሺ𝑠ሻ
ൌ
෍
௦∈ௌ
𝑋ሺ𝑠ሻ𝑝ሺ𝑠ሻ
𝑆ଵ, 𝑆ଶ, …
𝑆
𝑝
𝑋
𝑃ሺ𝑋ൌ0ሻൌ
𝑃ሺ𝑡, 𝑡ሻൌሺ1 െ𝑝ሻଶ,
𝑃ሺ𝑋ൌ1ሻൌ
𝑃ሺℎ, 𝑡ሻ൅𝑃ሺ𝑡, ℎሻൌ2𝑝ሺ1 െ𝑝ሻ
𝑃ሺ𝑋ൌ2ሻൌ
𝑃ሺℎ, ℎሻൌ𝑝ଶ
𝐸ሾ𝑋ሿൌ0 ⋅ሺ1 െ𝑝ሻଶ൅1 ⋅2𝑝ሺ1 െ𝑝ሻ൅2 ⋅𝑝ଶൌ2𝑝
𝐸ሾ𝑋ሿ
ൌ
𝑋ሺℎ, ℎሻ𝑝ଶ൅𝑋ሺℎ, 𝑡ሻ𝑝ሺ1 െ𝑝ሻ൅𝑋ሺ𝑡, ℎሻሺ1 െ𝑝ሻ𝑝൅𝑋ሺ𝑡, 𝑡ሻሺ1 െ𝑝ሻଶ
ൌ
 2𝑝ଶ൅𝑝ሺ1 െ𝑝ሻ൅ሺ1 െ𝑝ሻ𝑝
ൌ
 2𝑝
𝑋ଵ, 𝑋ଶ, …, 𝑋௡
267 of 848

Proof Let 
. Then, by Proposition 9.1
,
Example 9c
Find the expected value of the sum obtained when  fair dice are rolled.
solution
Let  be the sum. We will compute 
 by using the representation
where 
 is the upturned value on die . Because 
 is equally likely to be any of
the values from  to , it follows that
which yields the result
Example 9d
Find the expected total number of successes that result from  trials when trial 
is a success with probability 
𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿ
𝑍ൌ෍
௜ൌଵ
௡
𝑋௜
𝐸ሾ𝑍ሿ
ൌ
෍
௦∈ௌ
𝑍ሺ𝑠ሻ𝑝ሺ𝑠ሻ
ൌ
෍
௦∈ௌ
ሺ𝑋ଵሺ𝑠ሻ൅𝑋ଶሺ𝑠ሻ൅… ൅𝑋௡ሺ𝑠ሻሻ𝑝ሺ𝑠ሻ
ൌ
෍
௦∈ௌ
𝑋ଵሺ𝑠ሻ𝑝ሺ𝑠ሻ൅෍
௦∈ௌ
𝑋ଶሺ𝑠ሻ𝑝ሺ𝑠ሻ൅… ൅෍
௦∈ௌ
𝑋௡ሺ𝑠ሻ𝑝ሺ𝑠ሻ
ൌ
𝐸ሾ𝑋ଵሿ൅𝐸ሾ𝑋ଶሿ൅… ൅𝐸ሾ𝑋௡ሿ
𝑛
𝑋
𝐸ሾ𝑋ሿ
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
𝑋௜
𝑖
𝑋௜
1
6
𝐸ሾ𝑋௜ሿൌ෍
௜ൌଵ
଺
𝑖ሺ1/6ሻൌ21/6 ൌ7/2
𝐸ሾ𝑋ሿൌ𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿൌ3 . 5 𝑛
𝑛
𝑖
𝑝௜,  𝑖ൌ1, …, 𝑛.
268 of 848

solution
Letting
we have the representation
Consequently,
Note that this result does not require that the trials be independent. It includes as
a special case the expected value of a binomial random variable, which assumes
independent trials and all 
 and thus has mean 
. It also gives the
expected value of a hypergeometric random variable representing the number of
white balls selected when  balls are randomly selected, without replacement,
from an urn of  balls of which 
 are white. We can interpret the hypergeometric
as representing the number of successes in  trials, where trial  is said to be a
success if the th ball selected is white. Because the th ball selected is equally
likely to be any of the  balls and thus has probability 
 of being white, it
follows that the hypergeometric is the number of successes in  trials in which
each trial is a success with probability 
. Hence, even though these
hypergeometric trials are dependent, it follows from the result of Example 9d
that the expected value of the hypergeometric is 
.
Example 9e
Derive an expression for the variance of the number of successful trials in
Example 9d
, and apply it to obtain the variance of a binomial random variable
with parameters  and , and of a hypergeometric random variable equal to the
number of white balls chosen when  balls are randomly chosen from an urn
containing  balls of which 
 are white.
Solution
Letting  be the number of successful trials, and using the same representation
𝑋௜ൌቊ1,
 if trial 𝑖 is a success 
0,
 if trial 𝑖 is a failure 
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿൌ෍
௜ൌଵ
௡
𝑝௜
𝑝௜ൌ𝑝,
𝑛𝑝
𝑛
𝑁
𝑚
𝑛
𝑖
𝑖
𝑖
𝑁
𝑚/𝑁
𝑛
𝑝ൌ𝑚/𝑁
𝑛𝑝ൌ𝑛𝑚/𝑁
𝑛
𝑝
𝑛
𝑁
𝑚
𝑋
269 of 848

for —namely, 
—as in the previous example, we have
where the final equation used that 
 However, because the possible
values of both 
 and 
 are  or , it follows that
Hence,
Thus, with 
, the preceding and the result of Example
9d
 yield that
If  is binomial with parameters , , then 
 and, by the independence of
trials, 
, 
. Consequently, Equation (9.1)
 yields that
On the other hand, if  is hypergeometric, then as each of the  balls is equally
𝑋
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
𝐸ൣ𝑋ଶ൧
ൌ𝐸቎ቌ෍
௜ൌଵ
௡
𝑋௜ቍቌ෍
௝ൌଵ
௡
𝑋௝ቍ቏
ൌ𝐸቎෍
௜ൌଵ
௡
𝑋௜ቌ𝑋௜൅෍
௝ஷଵ
𝑋௝ቍ቏
ൌ𝐸቎෍
௜ൌଵ
௡
𝑋೔
ଶ൅෍
௜ൌଵ
௡
෍
௝ஷଵ
𝑋௜𝑋௝቏
ൌ
෍
௜ൌଵ
௡
𝐸ൣ𝑋೔
ଶ൧൅෍
௜ൌଵ
௡
෍
௝ஷଵ
𝐸ൣ𝑋௜𝑋௝൧
ൌ
෍
௜
𝑝௜൅෍
௜ൌଵ
௡
෍
௝ஷଵ
𝐸ൣ𝑋௜𝑋௝൧
𝑋ଶ
௜ൌ𝑋௜.
𝑋௜
𝑋௝
0
1
𝑋௜𝑋௝ൌቊ
1,
if 𝑋௜ൌ1, 𝑋௝ൌ1
0,
otherwise
𝐸ൣ𝑋௜𝑋௝൧ൌ𝑃൛𝑋௜ൌ1, 𝑋௝ൌ1ൟൌ𝑃ሺtrials 𝑖 and 𝑗  are successes ሻ
𝑝௜,௝ൌ𝑃ሺ𝑋௜ൌ1, 𝑋௝ൌ1ሻ
 Var ሺ𝑋ሻൌ෍
௜ൌଵ
௡
𝑝௜൅෍
௜ൌଵ
௡
෍
௝ஷ௜
𝑝௜,௝െሺ෍
௜ൌଵ
௡
𝑝௜ሻଶ
(9.1)
𝑋
𝑛𝑝
𝑝௜ൌ𝑝
𝑝௜,௝ൌ𝑝ଶ𝑖്𝑗
 Var ሺ𝑋ሻൌ𝑛𝑝൅𝑛ሺ𝑛െ1ሻ𝑝ଶെ𝑛ଶ𝑝ଶൌ𝑛𝑝ሺ1 െ𝑝ሻ
𝑋
𝑁
270 of 848

likely to be the 
 ball chosen, it follows that 
. Also, for 
which follows because given that the 
 ball selected is white, each of the other
 balls, of which 
 are white, is equally likely to be the 
 ball selected.
Consequently, (9.1
) yields that
which, as shown in Example 8j
, can be simplified to yield
where 
Recall that for the distribution function  of , 
 denotes the probability that the
random variable  takes on a value that is less than or equal to . The following are
some properties of the cumulative distribution function (c.d.f.) :
1.  is a nondecreasing function; that is, if 
, then 
.
2. 
.
3. 
.
4.  is right continuous. That is, for any  and any decreasing sequence
, that converges to 
.
Property 1 follows, as was noted in Section 4.1
, because, for 
, the event
 is contained in the event 
 and so cannot have a larger probability.
Properties 2, 3, and 4 all follow from the continuity property of probabilities (Section
2.6
). For instance, to prove property 2, we note that if 
 increases to 
, then the
events 
, are increasing events whose union is the event 
.
Hence, by the continuity property of probabilities,
𝑖௧௛
𝑝௜ൌ𝑚/𝑁
𝑖്𝑗
𝑝௜,௝ൌ𝑃ሺ𝑋௜ൌ1, 𝑋௝ൌ1ሻൌ𝑃ሺ𝑋௜ൌ1ሻ𝑃ሺ𝑋௝ൌ1|𝑋௜ൌ1ሻൌ𝑚
𝑁  𝑚െ1
𝑁െ1
𝑖௧௛
𝑁െ1
𝑚െ1
𝑗௧௛
Varሺ𝑋ሻൌ𝑛𝑚
𝑁൅𝑛ሺ𝑛െ1ሻ  𝑚
𝑁  𝑚െ1
𝑁െ1 െ൬𝑛𝑚
𝑁൰
ଶ
Varሺ𝑋ሻൌ𝑛𝑝ሺ1 െ𝑝ሻቆ1 െ𝑛െ1
𝑁െ1ቇ
𝑝ൌ𝑚/𝑁.
𝐹
𝑋𝐹ሺ𝑏ሻ
𝑋
𝑏
𝐹
𝐹
𝑎൏𝑏
𝐹ሺ𝑎ሻ൑𝐹ሺ𝑏ሻ
lim
௕→ஶ𝐹ሺ𝑏ሻൌ1
lim
௕→െஶ𝐹ሺ𝑏ሻൌ0
𝐹
𝑏
𝑏௡, 𝑛൒1
𝑏,
lim
௡→ஶ𝐹ሺ𝑏௡ሻൌ𝐹ሺ𝑏ሻ
𝑎൏𝑏
ሼ𝑋൑𝑎ሽ
ሼ𝑋൑𝑏ሽ
𝑏௡
∞
ሼ𝑋൑𝑏௡ሽ, 𝑛൒1
ሼ𝑋൏∞ሽ
lim
௡→ஶ𝑃ሼ𝑋൑𝑏௡ሽൌ𝑃ሼ𝑋൏∞ሽൌ1
271 of 848

which proves property 2.
The proof of property 3 is similar and is left as an exercise. To prove property 4, we
note that if 
 decreases to , then 
, are decreasing events whose
intersection is 
. The continuity property then yields
which verifies property 4.
All probability questions about  can be answered in terms of the c.d.f., . For
example,
This equation can best be seen to hold if we write the event 
 as the union of
the mutually exclusive events 
 and 
. That is,
so
which establishes Equation (10.1)
.
If we want to compute the probability that  is strictly less than , we can again apply
the continuity property to obtain
Note that 
 does not necessarily equal 
, since 
 also includes the
probability that  equals .
Example 10a
The distribution function of the random variable  is given by
𝑏௡
𝑏
ሼ𝑋൑𝑏௡ሽ, 𝑛൒1
ሼ𝑋൑𝑏ሽ
lim
௡→ஶ𝑃ሼ𝑋൑𝑏௡ሽൌ𝑃ሼ𝑋൑𝑏ሽ
𝑋
𝐹
𝑃ሼ𝑎൏𝑋൑𝑏ሽൌ𝐹ሺ𝑏ሻെ𝐹ሺ𝑎ሻ for all  𝑎൏𝑏
(10.1)
ሼ𝑋൑𝑏ሽ
ሼ𝑋൑𝑎ሽ
ሼ𝑎൏𝑋൑𝑏ሽ
ሼ𝑋൑𝑏ሽൌሼ𝑋൑𝑎ሽ∪ሼ𝑎൏𝑋൑𝑏ሽ
𝑃ሼ𝑋൑𝑏ሽൌ𝑃ሼ𝑋൑𝑎ሽ൅𝑃ሼ𝑎൏𝑋൑𝑏ሽ
𝑋
𝑏
𝑃ሼ𝑋൏𝑏ሽ
ൌ
𝑃ቆlim
௡→ஶቊ𝑋൑𝑏െ1
𝑛ቋቇ
ൌ
lim
௡→ஶ𝑃ቆ𝑋൑𝑏െ1
𝑛ቇ
ൌ
lim
௡→ஶ𝐹ቆ𝑏െ1
𝑛ቇ
𝑃ሼ𝑋൏𝑏ሽ
𝐹ሺ𝑏ሻ
𝐹ሺ𝑏ሻ
𝑋
𝑏
𝑋
272 of 848

A graph of 
 is presented in Figure 4.10
. Compute (a) 
, (b)
, (c) 
, and (d) 
.
Figure 4.10 Graph of 
.
Solution
a. 
b. 
c. 
d. 
𝐹ሺ𝑥ሻൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
0
𝑥൏0
𝑥
2
0 ൑𝑥൏1
2
3
1 ൑𝑥൏2
11
12
2 ൑𝑥൏3
1
3 ൑𝑥
𝐹ሺ𝑥ሻ
𝑃ሼ𝑋൏3ሽ
𝑃ሼ𝑋ൌ1ሽ
𝑃ሼ𝑋൐1
2ሽ
𝑃ሼ2 ൏𝑋൑4ሽ
𝑓ሺ𝑥ሻ
𝑃ሼ𝑋൏3ሽൌlim
௡𝑃ቊ𝑋൑3 െ1
𝑛ቋൌlim
௡𝐹ቆ3 െ1
𝑛ቇൌ11
12
𝑃ሼ𝑋ൌ1ሽൌ
𝑃ሼ𝑋൑1ሽെ𝑃ሼ𝑋൏1ሽ
ൌ
𝐹ሺ1ሻെlim
௡ 𝐹ቆ1 െ1
𝑛ቇൌ2
3 െ1
2 ൌ1
6
𝑃ቊ𝑋൐1
2ቋ
ൌ
1 െ𝑃ቊ𝑋൑1
2ቋ
ൌ
1 െ𝐹ቆ1
2ቇൌ3
4
𝑃ሼ2 ൏𝑋൑4ሽ
ൌ𝐹ሺ4ሻെ𝐹ሺ2ሻ
ൌ
1
12
273 of 848

A real-valued function defined on the outcome of a probability experiment is called a
random variable.
If  is a random variable, then the function 
 defined by
is called the distribution function of . All probabilities concerning  can be stated in
terms of .
A random variable whose set of possible values is either finite or countably infinite is
called discrete. If  is a discrete random variable, then the function
is called the probability mass function of . Also, the quantity 
 defined by
is called the expected value of . 
 is also commonly called the mean or the
expectation of .
A useful identity states that for a function ,
The variance of a random variable , denoted by Var
, is defined by
The variance, which is equal to the expected square of the difference between  and
its expected value, is a measure of the spread of the possible values of . A useful
identity is
The quantity 
 is called the standard deviation of .
We now note some common types of discrete random variables. The random
𝑋
𝐹ሺ𝑥ሻ
𝐹ሺ𝑥ሻൌ𝑃ሼ𝑋൑𝑥ሽ
𝑋
𝑋
𝐹
𝑋
𝑝ሺ𝑥ሻൌ𝑃ሼ𝑋ൌ𝑥ሽ
𝑋
𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿൌ
෍
௫: ௣ሺ௫ሻவ଴
𝑥𝑝ሺ𝑥ሻ
𝑋𝐸ሾ𝑋ሿ
𝑋
𝑔
𝐸ሾ𝑔ሺ𝑋ሻሿൌ
෍
௫: ௣ሺ௫ሻவ଴
𝑔ሺ𝑥ሻ𝑝ሺ𝑥ሻ
𝑋
ሺ𝑋ሻ
Varሺ𝑋ሻൌ𝐸ሾሺ𝑋െ𝐸ሾ𝑋ሿሻଶሿ
𝑋
𝑋
Varሺ𝑋ሻൌ𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
Varሺ𝑋ሻ
ඥ
𝑋
274 of 848

variable  whose probability mass function is given by
is said to be a binomial random variable with parameters  and . Such a random
variable can be interpreted as being the number of successes that occur when 
independent trials, each of which results in a success with probability , are
performed. Its mean and variance are given by
The random variable  whose probability mass function is given by
is said to be a Poisson random variable with parameter . If a large number of
(approximately) independent trials are performed, each having a small probability of
being successful, then the number of successful trials that result will have a
distribution that is approximately that of a Poisson random variable. The mean and
variance of a Poisson random variable are both equal to its parameter . That is,
The random variable  whose probability mass function is given by
is said to be a geometric random variable with parameter . Such a random variable
represents the trial number of the first success when each trial is independently a
success with probability . Its mean and variance are given by
The random variable  whose probability mass function is given by
is said to be a negative binomial random variable with parameters  and . Such a
random variable represents the trial number of the th success when each trial is
independently a success with probability . Its mean and variance are given by
𝑋
𝑝ሺ𝑖ሻൌቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜
𝑖ൌ0, …, 𝑛
𝑛
𝑝
𝑛
𝑝
𝐸ሾ𝑋ሿൌ𝑛𝑝
Varሺ𝑋ሻൌ𝑛𝑝ሺ1 െ𝑝ሻ
𝑋
𝑝ሺ𝑖ሻൌ𝑒െఒ𝜆௜
𝑖!
𝑖൒0
𝜆
𝜆
𝐸ሾ𝑋ሿൌVarሺ𝑋ሻൌ𝜆
𝑋
𝑝ሺ𝑖ሻൌ𝑝ሺ1 െ𝑝ሻ௜െଵ
𝑖ൌ1, 2, …
𝑝
𝑝
𝐸ሾ𝑋ሿൌ1
𝑝
Varሺ𝑋ሻൌ1 െ𝑝
𝑝ଶ
𝑋
𝑝ሺ𝑖ሻൌቆ𝑖െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௜െ௥
𝑖൒𝑟
𝑟
𝑝
𝑟
𝑝
275 of 848

A hypergeometric random variable  with parameters , , and 
 represents the
number of white balls selected when  balls are randomly chosen from an urn that
contains  balls of which 
 are white. The probability mass function of this random
variable is given by
With 
, its mean and variance are
An important property of the expected value is that the expected value of a sum of
random variables is equal to the sum of their expected values. That is,
𝐸ሾ𝑋ሿൌ𝑟
𝑝
Varሺ𝑋ሻൌ𝑟ሺ1 െ𝑝ሻ
𝑝ଶ
𝑋
𝑛𝑁
𝑚
𝑛
𝑁
𝑚
𝑝ሺ𝑖ሻൌ
ቆ𝑚
𝑖ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇ
ቆ𝑁
𝑛ቇ
𝑖ൌ0, …, 𝑚
𝑝ൌ𝑚/𝑁
𝐸ሾ𝑋ሿൌ𝑛𝑝
Varሺ𝑋ሻൌ𝑁െ𝑛
𝑁െ1 𝑛𝑝ሺ1 െ𝑝ሻ
𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿ
4.1. Two balls are chosen randomly from an urn containing 8 white, 4
black, and 2 orange balls. Suppose that we win $2 for each black ball
selected and we lose $1 for each white ball selected. Let  denote
our winnings. What are the possible values of , and what are the
probabilities associated with each value?
4.2. Two fair dice are rolled. Let  equal the product of the 2 dice.
Compute 
 for 
.
4.3. Three dice are rolled. By assuming that each of the 
possible outcomes is equally likely, find the probabilities attached to
the possible values that  can take on, where  is the sum of the 3
dice.
4.4. Five men and 5 women are ranked according to their scores on
an examination. Assume that no two scores are alike and all 10!
possible rankings are equally likely. Let  denote the highest ranking
achieved by a woman. (For instance, 
 if the top-ranked person
𝑋
𝑋
𝑋
𝑃ሼ𝑋ൌ𝑖ሽ
𝑖ൌ1, …, 36
6ଷൌ216
𝑋
𝑋
𝑋
𝑋ൌ1
276 of 848

is female.) Find 
.
4.5. Let  represent the difference between the number of heads and
the number of tails obtained when a coin is tossed  times. What are
the possible values of ?
4.6. In Problem 4.5
, for 
, if the coin is assumed fair, what
are the probabilities associated with the values that  can take on?
4.7. Suppose that a die is rolled twice. What are the possible values
that the following random variables can take on:
a. the maximum value to appear in the two rolls;
b. the minimum value to appear in the two rolls;
c. the sum of the two rolls;
d. the value of the first roll minus the value of the second roll?
4.8. If the die in Problem 4.7
 is assumed fair, calculate the
probabilities associated with the random variables in parts (a)
through (d).
4.9. Repeat Example 1c
 when the balls are selected with
replacement.
4.10. Let  be the winnings of a gambler. Let 
 and
suppose that
Compute the conditional probability that the gambler wins
 given that he wins a positive amount.
4.11. The random variable  is said to follow the distribution of
Benford’s Law if
It has been shown to be a good fit for the distribution of the first digit
of many real life data values.
a. Verify that the preceding is a probability mass function by
showing that 
.
b. Find 
.
4.12. In the game of Two-Finger Morra, 2 players show 1 or 2 fingers
and simultaneously guess the number of fingers their opponent will
show. If only one of the players guesses correctly, he wins an
amount (in dollars) equal to the sum of the fingers shown by him and
his opponent. If both players guess correctly or if neither guesses
𝑃ሼ𝑋  ൌ𝑖ሽ,  𝑖ൌ1,  2,  3, …,  8,  9,  10
𝑋
𝑛
𝑋
𝑛ൌ3
𝑋
𝑋
𝑝ሺ𝑖ሻൌ𝑃ሺ𝑋ൌ𝑖ሻ
𝑝ሺ0ሻൌ
1/3; 𝑝ሺ1ሻൌ𝑝ሺെ1ሻൌ13/55;
𝑝ሺ2ሻൌ
𝑝ሺെ2ሻൌ1/11; 𝑝ሺ3ሻൌ𝑝ሺെ3ሻൌ1/165
𝑖,  𝑖ൌ1, 2, 3,
𝑋
𝑃ሺ𝑋ൌ𝑖ሻൌlogଵ଴ቆ𝑖൅1
𝑖
ቇ,
𝑖ൌ1, 2, 3, …, 9
෍
௜ൌଵ
ଽ
𝑃ሺ𝑋ൌ𝑖ሻൌ1
𝑃ሺ𝑋൑𝑗ሻ
277 of 848

correctly, then no money is exchanged. Consider a specified player,
and denote by  the amount of money he wins in a single game of
Two-Finger Morra.
a. If each player acts independently of the other, and if each
player makes his choice of the number of fingers he will hold
up and the number he will guess that his opponent will hold up
in such a way that each of the 4 possibilities is equally likely,
what are the possible values of  and what are their
associated probabilities?
b. Suppose that each player acts independently of the other. If
each player decides to hold up the same number of fingers
that he guesses his opponent will hold up, and if each player
is equally likely to hold up 1 or 2 fingers, what are the possible
values of  and their associated probabilities?
4.13. A salesman has scheduled two appointments to sell vacuum
cleaners. His first appointment will lead to a sale with probability .3,
and his second will lead independently to a sale with probability .6.
Any sale made is equally likely to be either for the deluxe model,
which costs $1000, or the standard model, which costs $500.
Determine the probability mass function of , the total dollar value of
all sales.
4.14. Five distinct numbers are randomly distributed to players
numbered 1 through 5. Whenever two players compare their
numbers, the one with the higher one is declared the winner. Initially,
players 1 and 2 compare their numbers; the winner then compares
her number with that of player 3, and so on. Let  denote the number
of times player 1 is a winner. Find 
.
4.15. The National Basketball Association (NBA) draft lottery involves
the 11 teams that had the worst won–lost records during the year. A
total of 66 balls are placed in an urn. Each of these balls is inscribed
with the name of a team: Eleven have the name of the team with the
worst record, 10 have the name of the team with the second-worst
record, 9 have the name of the team with the third-worst record, and
so on (with 1 ball having the name of the team with the 11th-worst
record). A ball is then chosen at random, and the team whose name
is on the ball is given the first pick in the draft of players about to
enter the league. Another ball is then chosen, and if it “belongs” to a
team different from the one that received the first draft pick, then the
team to which it belongs receives the second draft pick. (If the ball
belongs to the team receiving the first pick, then it is discarded and
𝑋
𝑋
𝑋
𝑋
𝑋
𝑃ሼ𝑋ൌ𝑖ሽ, 𝑖ൌ0, 1, 2, 3, 4
278 of 848

another one is chosen; this continues until the ball of another team is
chosen.) Finally, another ball is chosen, and the team named on the
ball (provided that it is different from the previous two teams)
receives the third draft pick. The remaining draft picks 4 through 11
are then awarded to the 8 teams that did not “win the lottery,” in
inverse order of their won–lost records. For instance, if the team with
the worst record did not receive any of the 3 lottery picks, then that
team would receive the fourth draft pick. Let  denote the draft pick
of the team with the worst record. Find the probability mass function
of .
4.16. A deck of  cards numbered  through  are to be turned over
one a time. Before each card is shown you are to guess which card it
will be. After making your guess, you are told whether or not your
guess is correct but not which card was turned over. It turns out that
the strategy that maximizes the expected number of correct guesses
fixes a permutation of the  cards, say 
, and then continually
guesses  until it is correct, then continually guesses  until either it
is correct or all cards have been turned over, and then continually
guesses , and so on. Let  denote the number of correct guesses
yielded by this strategy. Determine 
.
Hint: In order for  to be at least  what must be the order of cards
.
4.17. Suppose that the distribution function of  is given by
a. Find 
.
b. Find 
.
4.18. Four independent flips of a fair coin are made. Let  denote the
number of heads obtained. Plot the probability mass function of the
random variable 
.
4.19. If the distribution function of  is given by
𝑋
𝑋
𝑛
1
𝑛
𝑛
1, 2, …, 𝑛
1
2
3
𝐺
𝑃ሺ𝐺ൌ𝑘ሻ
𝐺
𝑘
1, …, 𝑘
𝑋
𝐹ሺ𝑏ሻൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
0
𝑏൏0
𝑏
4
0 ൑𝑏൏1
1
2 ൅𝑏െ1
4
1 ൑𝑏൏2
11
12
2 ൑𝑏൏3
1
3 ൑𝑏
𝑃ሼ𝑋ൌ𝑖ሽ, 𝑖ൌ1, 2, 3
𝑃ሼ1
2 ൏𝑋൏3
2ሽ
𝑋
𝑋െ2
𝑋
279 of 848

calculate the probability mass function of .
4.20. A gambling book recommends the following “winning strategy”
for the game of roulette: Bet $1 on red. If red appears (which has
probability 
), then take the $1 profit and quit. If red does not
appear and you lose this bet (which has probability 
 of occurring),
make additional $1 bets on red on each of the next two spins of the
roulette wheel and then quit. Let  denote your winnings when you
quit.
a. Find 
.
b. Are you convinced that the strategy is indeed a “winning”
strategy? Explain your answer!
c. Find 
.
4.21. Four buses carrying 148 students from the same school arrive
at a football stadium. The buses carry, respectively, 40, 33, 25, and
50 students. One of the students is randomly selected. Let  denote
the number of students who were on the bus carrying the randomly
selected student. One of the 4 bus drivers is also randomly selected.
Let  denote the number of students on her bus.
a. Which of 
 or 
 do you think is larger? Why?
b. Compute 
 and 
.
4.22. Suppose that two teams play a series of games that ends when
one of them has won  games. Suppose that each game played is,
independently, won by team  with probability . Find the expected
number of games that are played when (a) 
 and (b) 
. Also,
show in both cases that this number is maximized when 
.
4.23. You have $1000, and a certain commodity presently sells for $2
per ounce. Suppose that after one week the commodity will sell for
𝐹ሺ𝑏ሻൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
0
𝑏൏0
1
2
0 ൑𝑏൏1
3
5
1 ൑𝑏൏2
4
5
2 ൑𝑏൏3
9
10
3 ൑𝑏൏3.5
1
𝑏൒3.5
𝑋
18
38
20
38
𝑋
𝑃ሼ𝑋൐0ሽ
𝐸ሾ𝑋ሿ
𝑋
𝑌
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ
𝑖
𝐴
𝑝
𝑖ൌ2
𝑖ൌ3
𝑝ൌ1
2
280 of 848

either $1 or $4 an ounce, with these two possibilities being equally
likely.
a. If your objective is to maximize the expected amount of money
that you possess at the end of the week, what strategy should
you employ?
b. If your objective is to maximize the expected amount of the
commodity that you possess at the end of the week, what
strategy should you employ?
4.24.
 and  play the following game:  writes down either number
1 or number 2, and  must guess which one. If the number that 
has written down is  and  has guessed correctly,  receives  units
from . If  makes a wrong guess,  pays  unit to . If 
randomizes his decision by guessing 1 with probability  and 2 with
probability 
, determine his expected gain if (a)  has written
down number 1 and (b)  has written down number 2.
What value of  maximizes the minimum possible value of ‘s
expected gain, and what is this maximin value? (Note that ‘s
expected gain depends not only on , but also on what  does.)
Consider now player . Suppose that she also randomizes her
decision, writing down number 1 with probability . What is ‘s
expected loss if (c)  chooses number 1 and (d)  chooses number
2?
What value of  minimizes ‘s maximum expected loss? Show that
the minimum of ‘s maximum expected loss is equal to the maximum
of ‘s minimum expected gain. This result, known as the minimax
theorem, was first established in generality by the mathematician
John von Neumann and is the fundamental result in the
mathematical discipline known as the theory of games. The common
value is called the value of the game to player .
4.25. Two coins are to be flipped. The first coin will land on heads
with probability .6, the second with probability .7. Assume that the
results of the flips are independent, and let  equal the total number
of heads that result.
a. Find 
.
b. Determine 
.
4.26. One of the numbers 1 through 10 is randomly chosen. You are
to try to guess the number chosen by asking questions with “yes–no”
answers. Compute the expected number of questions you will need
to ask in each of the following two cases:
𝐴
𝐵
𝐴
𝐵
𝐴
𝑖
𝐵
𝐵
𝑖
𝐴
𝐵
𝐵
3
4
𝐴
𝐵
𝑝
1 െ𝑝
𝐴
𝐴
𝑝
𝐵
𝐵
𝑝
𝐴
𝐴
𝑞
𝐴
𝐵
𝐵
𝑞
𝐴
𝐴
𝐵
𝐵
𝑋
𝑃ሼ𝑋ൌ1ሽ
𝐸ሾ𝑋ሿ
281 of 848

a. Your th question is to be “Is it ?” 
.
b. With each question you try to eliminate one-half of the
remaining numbers, as nearly as possible.
4.27. An insurance company writes a policy to the effect that an
amount of money  must be paid if some event  occurs within a
year. If the company estimates that  will occur within a year with
probability , what should it charge the customer in order that its
expected profit will be 10 percent of ?
4.28. A sample of 3 items is selected at random from a box
containing 20 items of which 4 are defective. Find the expected
number of defective items in the sample.
4.29. There are two possible causes for a breakdown of a machine.
To check the first possibility would cost 
 dollars, and, if that were
the cause of the breakdown, the trouble could be repaired at a cost
of 
 dollars. Similarly, there are costs 
 and 
 associated with the
second possibility. Let  and 
 denote, respectively, the
probabilities that the breakdown is caused by the first and second
possibilities. Under what conditions on 
, should we
check the first possible cause of breakdown and then the second, as
opposed to reversing the checking order, so as to minimize the
expected cost involved in returning the machine to working order?
Note: If the first check is negative, we must still check the other
possibility.
4.30. A person tosses a fair coin until a tail appears for the first time.
If the tail appears on the th flip, the person wins 
 dollars. Let 
denote the player’s winnings. Show that 
. This problem
is known as the St. Petersburg paradox.
a. Would you be willing to pay $1 million to play this game once?
b. Would you be willing to pay $1 million for each game if you
could play for as long as you liked and only had to settle up
when you stopped playing?
4.31. Each night different meteorologists give us the probability that it
will rain the next day. To judge how well these people predict, we will
score each of them as follows: If a meteorologist says that it will rain
with probability , then he or she will receive a score of
We will then keep track of scores over a certain time span and
𝑖
𝑖
𝑖ൌ1,  2,  3,  4, 5,  6,  7,  8,  9,  10
𝐴
𝐸
𝐸
𝑝
𝐴
𝐶ଵ
𝑅ଵ
𝐶ଶ
𝑅ଶ
𝑝
1 െ𝑝
𝑝, 𝐶௜, 𝑅௜, 𝑖ൌ1, 2
𝑛
2௡
𝑋
𝐸ሾ𝑋ሿൌ൅∞
𝑝
1 െሺ1 െ𝑝ሻଶ  if it does rain 
1 െ𝑝ଶ
  if it does not rain 
282 of 848

conclude that the meteorologist with the highest average score is the
best predictor of weather. Suppose now that a given meteorologist is
aware of our scoring mechanism and wants to maximize his or her
expected score. If this person truly believes that it will rain tomorrow
with probability 
, what value of  should he or she assert so as to
maximize the expected score?
4.32. To determine whether they have a certain disease, 100 people
are to have their blood tested. However, rather than testing each
individual separately, it has been decided first to place the people
into groups of 10. The blood samples of the 10 people in each group
will be pooled and analyzed together. If the test is negative, one test
will suffice for the 10 people, whereas if the test is positive, each of
the 10 people will also be individually tested and, in all, 11 tests will
be made on this group. Assume that the probability that a person has
the disease is .1 for all people, independently of one another, and
compute the expected number of tests necessary for each group.
(Note that we are assuming that the pooled test will be positive if at
least one person in the pool has the disease.)
4.33. A newsboy purchases papers at 10 cents and sells them at 15
cents. However, he is not allowed to return unsold papers. If his daily
demand is a binomial random variable with 
,
approximately how many papers should he purchase so as to
maximize his expected profit?
4.34. In Example 4b
, suppose that the department store incurs an
additional cost of  for each unit of unmet demand. (This type of cost
is often referred to as a goodwill cost because the store loses the
goodwill of those customers whose demands it cannot meet.)
Compute the expected profit when the store stocks  units, and
determine the value of  that maximizes the expected profit.
4.35. A box contains 5 red and 5 blue marbles. Two marbles are
withdrawn randomly. If they are the same color, then you win $1.10; if
they are different colors, then you win 
. (That is, you lose
$1.00.) Calculate
a. the expected value of the amount you win;
b. the variance of the amount you win.
4.36. Consider the friendship network described by Figure 4.5
.
Let  be a randomly chosen person and let  be a randomly chosen
friend of . With 
 equal to the number of friends of person , show
that 
.
4.37. Consider Problem 4.22
 with 
. Find the variance of the
𝑝*
𝑝
𝑛ൌ10, 𝑝ൌ1
3
𝑐
𝑠
𝑠
െ$1 . 00
𝑋
𝑍
𝑋
𝑓ሺ𝑖ሻ
𝑖
𝐸ሾ𝑓ሺ𝑍ሻሿ൒𝐸ሾ𝑓ሺ𝑋ሻሿ
𝑖ൌ2
283 of 848

number of games played, and show that this number is maximized
when 
.
4.38. Find Var
 and Var
 for  and  as given in Problem
4.21
.
4.39. If 
 and 
, find
a. 
;
b. 
.
4.40. A ball is drawn from an urn containing 3 white and 3 black
balls. After the ball is drawn, it is replaced and another ball is drawn.
This process goes on indefinitely. What is the probability that of the
first 4 balls drawn, exactly 2 are white?
4.41. On a multiple-choice exam with 3 possible answers for each of
the 5 questions, what is the probability that a student will get 4 or
more correct answers just by guessing?
4.42. A man claims to have extrasensory perception. As a test, a fair
coin is flipped 10 times and the man is asked to predict the outcome
in advance. He gets 7 out of 10 correct. What is the probability that
he would have done at least this well if he did not have ESP?
4.43. A and B will take the same 
-question examination. Each
question will be answered correctly by A with probability 
,
independently of her results on other questions. Each question will
be answered correctly by B with probability 
, independently both of
her results on the other questions and on the performance of A.
a. Find the expected number of questions that are answered
correctly by both A and B.
b. Find the variance of the number of questions that are
answered correctly by either A or B.
4.44. A communications channel transmits the digits 0 and 1.
However, due to static, the digit transmitted is incorrectly received
with probability .2. Suppose that we want to transmit an important
message consisting of one binary digit. To reduce the chance of
error, we transmit 00000 instead of 0 and 11111 instead of 1. If the
receiver of the message uses “majority” decoding, what is the
probability that the message will be wrong when decoded? What
independence assumptions are you making?
4.45. A satellite system consists of  components and functions on
any given day if at least  of the  components function on that day.
On a rainy day, each of the components independently functions with
probability 
, whereas on a dry day, each independently functions
𝑝ൌ1
2
ሺ𝑋ሻ
ሺ𝑌ሻ
𝑋
𝑌
𝐸ሾ𝑋ሿൌ1
Varሺ𝑋ሻൌ5
𝐸ሾሺ2 ൅𝑋ሻଶሿ
Varሺ4 ൅3𝑋ሻ
10
. 7
. 4
𝑛
𝑘
𝑛
𝑝ଵ
284 of 848

with probability 
. If the probability of rain tomorrow is , what is the
probability that the satellite system will function?
4.46. A student is getting ready to take an important oral examination
and is concerned about the possibility of having an “on” day or an
“off” day. He figures that if he has an on day, then each of his
examiners will pass him, independently of one another, with
probability .8, whereas if he has an off day, this probability will be
reduced to .4. Suppose that the student will pass the examination if a
majority of the examiners pass him. If the student believes that he is
twice as likely to have an off day as he is to have an on day, should
he request an examination with 3 examiners or with 5 examiners?
4.47. Suppose that it takes at least 9 votes from a 12-member jury to
convict a defendant. Suppose also that the probability that a juror
votes a guilty person innocent is .2, whereas the probability that the
juror votes an innocent person guilty is .1. If each juror acts
independently and if 65 percent of the defendants are guilty, find the
probability that the jury renders a correct decision. What percentage
of defendants is convicted?
4.48. In some military courts, 9 judges are appointed. However, both
the prosecution and the defense attorneys are entitled to a
peremptory challenge of any judge, in which case that judge is
removed from the case and is not replaced. A defendant is declared
guilty if the majority of judges cast votes of guilty, and he or she is
declared innocent otherwise. Suppose that when the defendant is, in
fact, guilty, each judge will (independently) vote guilty with probability
.7, whereas when the defendant is, in fact, innocent, this probability
drops to .3.
a. What is the probability that a guilty defendant is declared
guilty when there are (i) 9, (ii) 8, and (iii) 7 judges?
b. Repeat part (a) for an innocent defendant.
c. If the prosecuting attorney does not exercise the right to a
peremptory challenge of a judge, and if the defense is limited
to at most two such challenges, how many challenges should
the defense attorney make if he or she is 60 percent certain
that the client is guilty?
4.49. It is known that diskettes produced by a certain company will
be defective with probability .01, independently of one another. The
company sells the diskettes in packages of size 10 and offers a
money-back guarantee that at most 1 of the 10 diskettes in the
package will be defective. The guarantee is that the customer can
𝑝ଶ
𝛼
285 of 848

return the entire package of diskettes if he or she finds more than 1
defective diskette in it. If someone buys 3 packages, what is the
probability that he or she will return exactly 1 of them?
4.50. When coin  is flipped, it lands on heads with probability 
;
when coin  is flipped, it lands on heads with probability 
. One of
these coins is randomly chosen and flipped 
 times.
a. What is the probability that the coin lands on heads on exactly
 of the 
 flips?
b. Given that the first of these 10 flips lands heads, what is the
conditional probability that exactly  of the 
 flips land on
heads?
4.51. Each member of a population of size  is, independently,
female with probability  or male with probability 
. Let  be the
number of the other 
 members of the population that are the
same sex as is person . (So 
 if all  people are of the
same sex.)
a. Find 
.
Now suppose that two people of the same sex will, independently of
other pairs, be friends with probability ; whereas two persons of
opposite sexes will be friends with probability . Find the probability
mass function of the number of friends of person .
4.52. In a tournament involving players 
 players  and  play
a game, with the loser departing and the winner then playing against
player , with the loser of that game departing and the winner then
playing player The winner of the game involving player  is the
tournament winner. Suppose that a game between players  and  is
won by player  with probability 
.
a. Find the expected number of games played by player .
b. Find the expected number of games played by player .
4.53. Suppose that a biased coin that lands on heads with probability
 is flipped 10 times. Given that a total of 6 heads results, find the
conditional probability that the first 3 outcomes are
a. , ,  (meaning that the first flip results in heads, the second
in tails, and the third in tails);
b. , , .
4.54. The expected number of typographical errors on a page of a
certain magazine is .2. What is the probability that the next page you
1
. 4
2
. 7
10
7
10
7
10
𝑛
𝑝
1 െ𝑝
𝑋
𝑛െ1
1
𝑋ൌ𝑛െ1
𝑛
𝑃ሺ𝑋ൌ𝑖ሻ,  𝑖ൌ0, …, 𝑛െ1
𝛼
𝛽
1
1, 2, 3, 4,
1
2
3
4
4
𝑖
𝑗
𝑖
𝑖
𝑖൅𝑗
1
3
𝑝
ℎ𝑡𝑡
𝑡ℎ𝑡
286 of 848

read contains (a) 0 and (b) 2 or more typographical errors? Explain
your reasoning!
4.55. The monthly worldwide average number of airplane crashes of
commercial airlines is 3.5. What is the probability that there will be
a. at least 2 such accidents in the next month;
b. at most 1 accident in the next month? Explain your reasoning!
4.56. Approximately 80,000 marriages took place in the state of New
York last year. Estimate the probability that for at least one of these
couples,
a. both partners were born on April 30;
b. both partners celebrated their birthday on the same day of the
year.
State your assumptions.
4.57. State your assumptions. Suppose that the average number of
cars abandoned weekly on a certain highway is 2.2. Approximate the
probability that there will be
a. no abandoned cars in the next week;
b. at least 2 abandoned cars in the next week.
4.58. A certain typing agency employs 2 typists. The average
number of errors per article is 3 when typed by the first typist and 4.2
when typed by the second. If your article is equally likely to be typed
by either typist, approximate the probability that it will have no errors.
4.59. How many people are needed so that the probability that at
least one of them has the same birthday as you is greater than ?
4.60. Suppose that the number of accidents occurring on a highway
each day is a Poisson random variable with parameter 
.
a. Find the probability that 3 or more accidents occur today.
b. Repeat part (a) under the assumption that at least 1 accident
occurs today.
4.61. Compare the Poisson approximation with the correct binomial
probability for the following cases:
a. 
 when 
, 
;
b. 
 when 
, 
;
c. 
 when 
, 
;
d. 
 when 
, 
.
4.62. If you buy a lottery ticket in 50 lotteries, in each of which your
1
2
𝜆ൌ3
𝑃ሼ𝑋ൌ2ሽ
𝑛ൌ8 𝑝ൌ. 1
𝑃ሼ𝑋ൌ9ሽ
𝑛ൌ10 𝑝ൌ. 95
𝑃ሼ𝑋ൌ0ሽ
𝑛ൌ10 𝑝ൌ. 1
𝑃ሼ𝑋ൌ4ሽ
𝑛ൌ9 𝑝ൌ. 2
287 of 848

chance of winning a prize is 
, what is the (approximate)
probability that you will win a prize
a. at least once?
b. exactly once?
c. at least twice?
4.63. The number of times that a person contracts a cold in a given
year is a Poisson random variable with parameter 
. Suppose
that a new wonder drug (based on large quantities of vitamin C) has
just been marketed that reduces the Poisson parameter to 
 for
75 percent of the population. For the other 25 percent of the
population, the drug has no appreciable effect on colds. If an
individual tries the drug for a year and has 2 colds in that time, how
likely is it that the drug is beneficial for him or her?
4.64. The probability of being dealt a full house in a hand of poker is
approximately .0014. Find an approximation for the probability that in
1000 hands of poker, you will be dealt at least 2 full houses.
4.65. Consider  independent trials, each of which results in one of
the outcomes 
 with respective probabilities
 Show that if all the 
 are small, then the
probability that no trial outcome occurs more than once is
approximately equal to 
4.66. People enter a gambling casino at a rate of 1 every 2 minutes.
a. What is the probability that no one enters between 12:00 and
12:05?
b. What is the probability that at least 4 people enter the casino
during that time?
4.67. The suicide rate in a certain state is 1 suicide per 100,000
inhabitants per month.
a. Find the probability that in a city of 400,000 inhabitants within
this state, there will be 8 or more suicides in a given month.
b. What is the probability that there will be at least 2 months
during the year that will have 8 or more suicides?
c. Counting the present month as month number 1, what is the
probability that the first month to have 8 or more suicides will
be month number 
? What assumptions are you
making?
1
100
𝜆ൌ5
𝜆ൌ3
𝑛
1, …, 𝑘
𝑝ଵ, …, 𝑝௞,  ෍
௜ൌଵ
௞
𝑝௜ൌ1 .
𝑝௜
expሺെ𝑛ሺ𝑛െ1ሻ෍
௜
𝑝ଶ
௜/2ሻ.
𝑖, 𝑖൒1
288 of 848

4.68. Each of 500 soldiers in an army company independently has a
certain disease with probability 
. This disease will show up in a
blood test, and to facilitate matters, blood samples from all 500
soldiers are pooled and tested.
a. What is the (approximate) probability that the blood test will be
positive (that is, at least one person has the disease)?
Suppose now that the blood test yields a positive result.
b. What is the probability, under this circumstance, that more
than one person has the disease?
Now, suppose one of the 500 people is Jones, who knows that
he has the disease.
c. What does Jones think is the probability that more than one
person has the disease?
Because the pooled test was positive, the authorities have
decided to test each individual separately. The first 
 of
these tests were negative, and the th one–which was on
Jones–was positive.
d. Given the preceding scenario, what is the probability, as a
function of , that any of the remaining people have the
disease?
4.69. A total of 
 people, consisting of  married couples, are
randomly seated (all possible orderings being equally likely) at a
round table. Let 
 denote the event that the members of couple  are
seated next to each other, 
a. Find 
.
b. For 
 find 
.
c. Approximate the probability, for  large, that there are no
married couples who are seated next to each other.
4.70. Repeat the preceding problem when the seating is random but
subject to the constraint that the men and women alternate.
4.71. In response to an attack of 10 missiles, 500 antiballistic
missiles are launched. The missile targets of the antiballistic missiles
are independent, and each antiballstic missile is equally likely to go
towards any of the target missiles. If each antiballistic missile
independently hits its target with probability 
, use the Poisson
paradigm to approximate the probability that all missiles are hit.
4.72. A fair coin is flipped 10 times. Find the probability that there is a
string of  consecutive heads by
a. using the formula derived in the text;
1/10ଷ
𝑖െ1
𝑖
𝑖
2𝑛
𝑛
𝐶௜
𝑖
𝑖ൌ1, …, 𝑛.
𝑃ሺ𝐶௜ሻ
𝑗്𝑖,
𝑃ሺ𝐶௝|𝐶௜ሻ
𝑛
. 1
4
289 of 848

b. using the recursive equations derived in the text.
c. Compare your answer with that given by the Poisson
approximation.
4.73. At time , a coin that comes up heads with probability  is
flipped and falls to the ground. Suppose it lands on heads. At times
chosen according to a Poisson process with rate 
 the coin is picked
up and flipped. (Between these times, the coin remains on the
ground.) What is the probability that the coin is on its head side at
time ?
Hint: What would be the conditional probability if there were no
additional flips by time , and what would it be if there were additional
flips by time ?
4.74. Consider a roulette wheel consisting of 38 numbers 1 through
36, 0, and double 0. If Smith always bets that the outcome will be
one of the numbers 1 through 12, what is the probability that
a. Smith will lose his first 5 bets;
b. his first win will occur on his fourth bet?
4.75. Two athletic teams play a series of games; the first team to win
4 games is declared the overall winner. Suppose that one of the
teams is stronger than the other and wins each game with probability
.6, independently of the outcomes of the other games. Find the
probability, for 
, that the stronger team wins the series in
exactly  games. Compare the probability that the stronger team wins
with the probability that it would win a 2-out-of-3 series.
4.76. Suppose in Problem 4.75
 that the two teams are evenly
matched and each has probability  of winning each game. Find the
expected number of games played.
4.77. An interviewer is given a list of people she can interview. If the
interviewer needs to interview 5 people, and if each person
(independently) agrees to be interviewed with probability , what is
the probability that her list of people will enable her to obtain her
necessary number of interviews if the list consists of (a) 5 people and
(b) 8 people? For part (b), what is the probability that the interviewer
will speak to exactly (c) 6 people and (d) 7 people on the list?
4.78. A fair coin is continually flipped until heads appears for the 10th
time. Let  denote the number of tails that occur. Compute the
probability mass function of .
4.79. Solve the Banach match problem (Example 8e
) when the
0
𝑝
𝜆,
𝑡
𝑡
𝑡
𝑖ൌ4,  5,  6,  7
𝑖
1
2
2
3
𝑋
𝑋
290 of 848

left-hand matchbox originally contained 
 matches and the right-
hand box contained 
 matches.
4.80. In the Banach matchbox problem, find the probability that at the
moment when the first box is emptied (as opposed to being found
empty), the other box contains exactly  matches.
4.81. An urn contains 4 white and 4 black balls. We randomly choose
4 balls. If 2 of them are white and 2 are black, we stop. If not, we
replace the balls in the urn and again randomly select 4 balls. This
continues until exactly 2 of the 4 chosen are white. What is the
probability that we shall make exactly  selections?
4.82. Suppose that a batch of 100 items contains 6 that are defective
and 94 that are not defective. If  is the number of defective items in
a randomly drawn sample of 10 items from the batch, find (a)
 and (b) 
.
4.83. A game popular in Nevada gambling casinos is Keno, which is
played as follows: Twenty numbers are selected at random by the
casino from the set of numbers 1 through 80. A player can select
from 1 to 15 numbers; a win occurs if some fraction of the player’s
chosen subset matches any of the 20 numbers drawn by the house.
The payoff is a function of the number of elements in the player’s
selection and the number of matches. For instance, if the player
selects only 1 number, then he or she wins if this number is among
the set of 20, and the payoff is $2.20 won for every dollar bet. (As the
player’s probability of winning in this case is , it is clear that the
“fair” payoff should be $3 won for every $1 bet.) When the player
selects 2 numbers, a payoff (of odds) of $12 won for every $1 bet is
made when both numbers are among the 20.
a. What would be the fair payoff in this case?
Let 
 denote the probability that exactly  of the  numbers
chosen by the player are among the 20 selected by the house.
b. Compute 
c. The most typical wager at Keno consists of selecting 10
numbers. For such a bet, the casino pays off as shown in the
following table. Compute the expected payoff:
Keno Payoffs in 10 Number Bets
Number of matches
Dollars won for each $1 bet
0—4
–1
𝑁ଵ
𝑁ଶ
𝑘
𝑛
𝑋
𝑃ሼ𝑋ൌ0ሽ
𝑃ሼ𝑋൐2ሽ
1
4
𝑃௡, ௞
𝑘
𝑛
𝑃௡, ௞
291 of 848

Keno Payoffs in 10 Number Bets
Number of matches
Dollars won for each $1 bet
5
1
6
17
7
179
8
1,299
9
2,599
10
24,999
4.84. In Example 8i
, what percentage of  defective lots does the
purchaser reject? Find it for 
. Given that a lot is rejected, what
is the conditional probability that it contained 4 defective
components?
4.85. A purchaser of transistors buys them in lots of 20. It is his
policy to randomly inspect 4 components from a lot and to accept the
lot only if all 4 are nondefective. If each component in a lot is,
independently, defective with probability .1, what proportion of lots is
rejected?
4.86. There are three highways in the county. The number of daily
accidents that occur on these highways are Poisson random
variables with respective parameters 
, and 
 Find the
expected number of accidents that will happen on any of these
highways today.
4.87. Suppose that 
 balls are put into  boxes, with each ball
independently being put in box  with probability 
a. Find the expected number of boxes that do not have any balls.
b. Find the expected number of boxes that have exactly  ball.
4.88. There are  types of coupons. Independently of the types of
previously collected coupons, each new coupon collected is of type 
with probability 
 If  coupons are collected, find the
expected number of distinct types that appear in this set. (That is,
𝑖
𝑖ൌ1, 4
. 3, . 5
. 7 .
10
5
𝑖
𝑝௜,  ෍
௜ൌଵ
ହ
𝑝௜ൌ1 .  
1
𝑘
𝑖
𝑝௜,  ෍
௜ൌଵ
௞
𝑝௜ൌ1 .
𝑛
292 of 848

find the expected number of types of coupons that appear at least
once in the set of  coupons.)
4.89. An urn contains 
 red,  black, and  green balls. One of the
colors is chosen at random (meaning that the chosen color is equally
likely to be any of the  colors), and then  balls are randomly
chosen from the urn. Let  be the number of these balls that are of
the chosen color.
a. Find 
.
b. Let 
 equal  if the 
 ball selected is of the chosen color,
and let it equal  otherwise. Find 
, 
.
c. Find 
.
Hint: Express  in terms of 
, 
, 
, 
.
𝑛
10
8
7
3
4
𝑋
𝑃ሺ𝑋ൌ0ሻ
𝑋௜
1
𝑖௧௛
0
𝑃ሺ𝑋௜ൌ1ሻ𝑖ൌ1, 2, 3, 4
𝐸ሾ𝑋ሿ
𝑋
𝑋ଵ𝑋ଶ𝑋ଷ𝑋ସ
4.1. There are  distinct types of coupons, and each time one is obtained it
will, independently of past choices, be of type  with probability 
.
Let  denote the number one need select to obtain at least one of each type.
Compute 
.
Hint: Use an argument similar to the one used in Example 1e
.
4.2. If  has distribution function , what is the distribution function of 
?
4.3. If  has distribution function , what is the distribution function of the
random variable 
, where  and  are constants, 
?
4.4. The random variable  is said to have the Yule-Simons distribution if
a. Show that the preceding is actually a probability mass function. That is,
show that 
b. Show that 
c. Show that 
Hint: For (a), first use that 
 then
use that 
.
4.5. Let  be a nonnegative integer-valued random variable. For nonnegative
values 
, show that
𝑁
𝑖
𝑃௜, 𝑖ൌ1, …, 𝑁
𝑇
𝑃ሼ𝑇ൌ𝑛ሽ
𝑋
𝐹
𝑒௑
𝑋
𝐹
𝛼𝑋൅𝛽
𝛼
𝛽
𝛼്0
𝑋
𝑃ሼ𝑋ൌ𝑛ሽൌ
4
𝑛ሺ𝑛൅1ሻሺ𝑛൅2ሻ ,  𝑛൒1
෍
௡ൌଵ
ஶ
𝑃ሼ𝑋ൌ𝑛ሽൌ1 .
𝐸ሾ𝑋ሿൌ2 .
𝐸ൣ𝑋ଶ൧ൌ∞.
1
𝑛ሺ𝑛  ൅ 1ሻሺ𝑛  ൅ 2ሻൌ
1
𝑛ሺ𝑛  ൅ 1ሻെ
1
𝑛ሺ𝑛  ൅ 2ሻ,
𝑘
𝑛ሺ𝑛  ൅ 𝑘ሻൌ1
𝑛െ
1
𝑛  ൅ 𝑘
𝑁
𝑎௝, 𝑗൒1
293 of 848

Then show that
and
4.6. Let  be such that
Find 
 such that 
.
4.7. Let  be a random variable having expected value  and variance 
.
Find the expected value and variance of
4.8. Find 
 if
4.9. Show how the derivation of the binomial probabilities
leads to a proof of the binomial theorem
when  and  are nonnegative.
Hint: Let 
4.10. Let  be a binomial random variable with parameters  and . Show that
4.11. Let  be the number of successes that result from 
 independent trials,
when each trial is a success with probability . Show that 
 is a
෍
௝ൌଵ
ஶ
ሺ𝑎ଵ൅… ൅𝑎௝ሻ𝑃ሼ𝑁ൌ𝑗ሽൌ෍
௜ൌଵ
ஶ
𝑎௜𝑃ሼ𝑁൒𝑖ሽ
𝐸ሾ𝑁ሿൌ෍
௜ൌଵ
ஶ
𝑃ሼ𝑁൒𝑖ሽ
𝐸ሾ𝑁ሺ𝑁൅1ሻሿൌ2 ෍
௜ൌଵ
ஶ
𝑖𝑃ሼ𝑁൒𝑖ሽ
𝑋
𝑃ሼ𝑋ൌ1ሽൌ𝑝ൌ1 െ𝑃ሼ𝑋ൌെ1ሽ
𝑐്1
𝐸ሾ𝑐௑ሿൌ1
𝑋
𝜇
𝜎ଶ
𝑌ൌ𝑋െ𝜇
𝜎
Varሺ𝑋ሻ
𝑃ሺ𝑋ൌ𝑎ሻൌ𝑝ൌ1 െ𝑃ሺ𝑋ൌ𝑏ሻ
𝑃ሼ𝑋ൌ𝑖ሽൌቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜,
𝑖ൌ0, …, 𝑛
ሺ𝑥൅𝑦ሻ௡ൌ෍
௜ൌ଴
௡
𝑛𝑖
𝑥௜𝑦௡െ௜
𝑥
𝑦
𝑝ൌ
𝑥
𝑥൅𝑦.
𝑋
𝑛
𝑝
𝐸൥
1
𝑋൅1൩ൌ1 െሺ1 െ𝑝ሻ௡൅ଵ
ሺ𝑛൅1ሻ𝑝
𝑋
2𝑛
𝑝
𝑃ሺ𝑋ൌ𝑛ሻ
294 of 848

decreasing function of .
4.12. Consider  independent sequential trials, each of which is successful
with probability . If there is a total of  successes, show that each of the
 possible arrangements of the  successes and 
 failures is
equally likely.
4.13. There are  components lined up in a linear arrangement. Suppose that
each component independently functions with probability . What is the
probability that no 2 neighboring components are both nonfunctional?
Hint: Condition on the number of defective components and use the results of
Example 4c
 of Chapter 1
.
4.14. Let  be a binomial random variable with parameters ( , ). What value
of  maximizes 
? This is an example of a statistical
method used to estimate  when a binomial ( , ) random variable is
observed to equal . If we assume that  is known, then we estimate  by
choosing that value of  that maximizes 
. This is known as the
method of maximum likelihood estimation.
4.15. A family has  children with probability 
, where 
.
a. What proportion of families has no children?
b. If each child is equally likely to be a boy or a girl (independently of each
other), what proportion of families consists of  boys (and any number
of girls)?
4.16. Suppose that  independent tosses of a coin having probability  of
coming up heads are made. Show that the probability that an even number of
heads results is 
, where 
. Do this by proving and then
utilizing the identity
where [ /2] is the largest integer less than or equal to /2. Compare this
exercise with Theoretical Exercise 3.5
 of Chapter 3
.
4.17. Let  be a Poisson random variable with parameter . Show that
 increases monotonically and then decreases monotonically as 
increases, reaching its maximum when  is the largest integer not exceeding 
.
Hint: Consider 
.
4.18. Let  be a Poisson random variable with parameter .
a. Show that
𝑛
𝑛
𝑝
𝑘
𝑛!/ሾ𝑘!ሺ𝑛െ𝑘ሻ!ሿ
𝑘
𝑛െ𝑘
𝑛
𝑝
𝑋
𝑛𝑝
𝑝
𝑃ሼ𝑋ൌ𝑘ሽ, 𝑘ൌ0, 1, …, 𝑛
𝑝
𝑛𝑝
𝑘
𝑛
𝑝
𝑝
𝑃ሼ𝑋ൌ𝑘ሽ
𝑛
𝛼𝑝௡, 𝑛൒1
𝛼൑ሺ1 െ𝑝ሻ/𝑝
𝑘
𝑛
𝑝
1
2
⎡
⎣
1 ൅ሺ𝑞െ𝑝ሻ௡⎤
⎦
𝑞ൌ1 െ𝑝
෍
௜ൌ଴
ሾ௡/ଶሿ
ቆ𝑛
2𝑖ቇ𝑝ଶ௜𝑞௡െଶ௜ൌ1
2ሾሺ𝑝൅𝑞ሻ௡൅ሺ𝑞െ𝑝ሻ௡ሿ
𝑛
𝑛
𝑋
𝜆
𝑃ሼ𝑋ൌ𝑖ሽ
𝑖
𝑖
𝜆
𝑃ሼ𝑋ൌ𝑖ሽ/𝑃ሼ𝑋ൌ𝑖െ1ሽ
𝑋
𝜆
𝑃ሼ𝑋 is evenሽൌ1
2
⎡
⎣
1 ൅𝑒െଶఒ⎤
⎦
295 of 848

by using the result of Theoretical Exercise 4.15
 and the
relationship between Poisson and binomial random variables.
b. Verify the formula in part (a) directly by making use of the expansion of
.
4.19. Let  be a Poisson random variable with parameter . What value of 
maximizes 
?
4.20. Show that  is a Poisson random variable with parameter , then
Now use this result to compute 
.
4.21. Consider  coins, each of which independently comes up heads with
probability . Suppose that  is large and  is small, and let 
. Suppose
that all  coins are tossed; if at least one comes up heads, the experiment
ends; if not, we again toss all  coins, and so on. That is, we stop the first time
that at least one of the  coins come up heads. Let  denote the total number
of heads that appear. Which of the following reasonings concerned with
approximating 
 is correct (in all cases,  is a Poisson random variable
with parameter )?
a. Because the total number of heads that occur when all  coins are
rolled is approximately a Poisson random variable with parameter ,
b. Because the total number of heads that occur when all  coins are
rolled is approximately a Poisson random variable with parameter ,
and because we stop only when this number is positive,
c. Because at least one coin comes up heads,  will equal 1 if none of the
other 
 coins come up heads. Because the number of heads
resulting from these 
 coins is approximately Poisson with mean
,
4.22. From a set of  randomly chosen people, let 
 denote the event that
persons  and  have the same birthday. Assume that each person is equally
likely to have any of the 365 days of the year as his or her birthday. Find
a. 
;
b. 
;
c. 
. What can you conclude from your answers to parts
𝑒െఒ൅𝑒ఒ
𝑋
𝜆
𝜆
𝑃ሼ𝑋ൌ𝑘ሽ, 𝑘൒0
𝑋
𝜆
𝐸ሾ𝑋௡ሿൌ𝜆𝐸ሾሺ𝑋൅1ሻ௡െଵሿ
𝐸ൣ𝑋ଷ൧
𝑛
𝑝
𝑛
𝑝
𝜆ൌ𝑛𝑝
𝑛
𝑛
𝑛
𝑋
𝑃ሼ𝑋ൌ1ሽ
𝑌
𝜆
𝑛
𝜆
𝑃ሼ𝑋ൌ1ሽൎ𝑃ሼ𝑌ൌ1ሽൌ𝜆𝑒െఒ
𝑛
𝜆
𝑃ሼ𝑋ൌ1ሽൎ𝑃ሼ𝑌ൌ1|𝑌൐0ሽൌ
𝜆𝑒െఒ
1  െ 𝑒െఒ
𝑋
𝑛െ1
𝑛െ1
ሺ𝑛െ1ሻ𝑝ൎ𝜆
𝑃൛𝑋ൌ1ൟൎ𝑃൛𝑌ൌ0ൟൌ𝑒െఒ
𝑛
𝐸௜௝
𝑖
𝑗
𝑃ሺ𝐸ଷ,ସห𝐸ଵ,ଶሻ
𝑃ሺ𝐸ଵ,ଷห𝐸ଵ,ଶሻ
𝑃ሺ𝐸ଶ,ଷห𝐸ଵ,ଶ∩𝐸ଵ,ଷሻ
296 of 848

(a)—(c) about the independence of the 
 events 
?
4.23. An urn contains 2  balls, of which 2 are numbered 1, 2 are numbered
, and 2 are numbered . Balls are successively withdrawn 2 at a time
without replacement. Let  denote the first selection in which the balls
withdrawn have the same number (and let it equal infinity if none of the pairs
withdrawn has the same number). We want to show that, for 
,
To verify the preceding formula, let 
 denote the number of pairs withdrawn
in the first  selections, 
.
Argue that when  is large, 
 can be regarded as the number of successes
in  (approximately) independent trials.
a. Approximate 
 when  is large.
b. Write the event 
 in terms of the value of one of the variables
.
c. Verify the limiting probability given for 
.
4.24. Consider a random collection of  individuals. In approximating the
probability that no  of these individuals share the same birthday, a better
Poisson approximation than that obtained in the text (at least for values of 
between 
 and 
) is obtained by letting 
 be the event that there are at
least  birthdays on day , 
.
a. Find 
.
b. Give an approximation for the probability that no 3 individuals share the
same birthday.
c. Evaluate the preceding when 
. (The exact probability is derived
in Example 1g
 of Chapter 6
.)
4.25. Here is another way to obtain a set of recursive equations for
determining 
, the probability that there is a string of  consecutive heads in
a sequence of  flips of a fair coin that comes up heads with probability :
a. Argue that for 
, there will be a string of  consecutive heads if
either
1. there is a string of  consecutive heads within the first 
flips, or
2. there is no string of  consecutive heads within the first 
flips, flip 
 is a tail, and flips 
 are all heads.
b. Using the preceding, relate 
 to 
. Starting with 
, the
ቆ𝑛
2ቇ
𝐸௜௝
𝑛
2, …
𝑛
𝑇
0 ൏𝛼൏1
lim
௡𝑃ሼ𝑇൐𝛼𝑛ሽൌ𝑒െఈ/ଶ
𝑀௞
𝑘
𝑘ൌ1, …, 𝑛
𝑛
𝑀௞
𝑘
𝑃ሼ𝑀௞ൌ0ሽ
𝑛
ሼ𝑇൐𝛼𝑛ሽ
𝑀௞
𝑃ሼ𝑇൐𝛼𝑛ሽ
𝑛
3
𝑛
80
90
𝐸௜
3
𝑖𝑖ൌ1, …, 365
𝑃ሺ𝐸௜ሻ
𝑛ൌ88
𝑃௡
𝑘
𝑛
𝑝
𝑘൏𝑛
𝑘
𝑘
𝑛െ1
𝑘
𝑛െ𝑘െ1
𝑛െ𝑘
𝑛െ𝑘൅1, …, 𝑛
𝑃௡
𝑃௡െଵ
𝑃௞ൌ𝑝௞
297 of 848

recursion can be used to obtain 
, then 
, and so on, up to 
.
4.26. Suppose that the number of events that occur in a specifiedime is a
Poisson random variable with parameter . If each event is counted with
probability , independently of every other event, show that the number of
events that are counted is a Poisson random variable with parameter 
. Also,
give an intuitive argument as to why this should be so. As an application of the
preceding result, suppose that the number of distinct uranium deposits in a
given area is a Poisson random variable with parameter 
. If, in a fixed
period of time, each deposit is discovered independently with probability 
,
find the probability that (a) exactly 1, (b) at least 1, and (c) at most 1 deposit is
discovered during that time.
4.27. Prove
Hint: Use integration by parts.
4.28. If  is a geometric random variable, show analytically that
Using the interpretation of a geometric random variable, give a verbal
argument as to why the preceding equation is true.
4.29. Let  be a negative binomial random variable with parameters  and ,
and let  be a binomial random variable with parameters  and . Show that
Hint: Either one could attempt an analytical proof of the preceding equation,
which is equivalent to proving the identity
or one could attempt a proof that uses the probabilistic interpretation of these
random variables. That is, in the latter case, start by considering a sequence
of independent trials having a common probability  of success. Then try to
express the events 
 and 
 in terms of the outcomes of this
sequence.
4.30. For a hypergeometric random variable, determine
𝑃௞൅ଵ
𝑃௞൅ଶ
𝑃௡
𝜆
𝑝
𝜆𝑝
𝜆ൌ10
1
50
෍
௜ൌ଴
௡
𝑒െఒ𝜆௜
𝑖! ൌ1
𝑛!඲
ఒ
ஶ
𝑒െ௫𝑥௡𝑑𝑥
𝑋
𝑃ሼ𝑋ൌ𝑛൅𝑘||𝑋൐𝑛ሽൌ𝑃ሼ𝑋ൌ𝑘ሽ
𝑋
𝑟
𝑝
𝑌
𝑛
𝑝
𝑃ሼ𝑋൐𝑛ሽൌ𝑃ሼ𝑌൏𝑟ሽ
෍
௜ൌ௡൅ଵ
ஶ
ቆ𝑖െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௜െ௥
ൌ
෍
௜ൌ଴
௥െଵ
ቆ𝑛
𝑖ቇ
ൈ  𝑝௜ሺ1 െ𝑝ሻ௡െ௜
𝑝
ሼ𝑋൐𝑛ሽ
ሼ𝑌൏𝑟ሽ
𝑃ሼ𝑋ൌ𝑘൅1ሽ/𝑃ሼ𝑋ൌ𝑘ሽ
298 of 848

4.31. Balls numbered 1 through  are in an urn. Suppose that 
, of
them are randomly selected without replacement. Let  denote the largest
number selected.
a. Find the probability mass function of .
b. Derive an expression for [ ] and then use Fermat’s combinatorial
identity (see Theoretical Exercise 11
 of Chapter 1
) to simplify
the expression.
4.32. A jar contains 
 chips, numbered 
. A set of size  is
drawn. If we let  denote the number of chips drawn having numbers that
exceed each of the numbers of those remaining, compute the probability
mass function of .
4.33. A jar contains  chips. Suppose that a boy successively draws a chip
from the jar, each time replacing the one drawn before drawing another. The
process continues until the boy draws a chip that he has previously drawn. Let
 denote the number of draws, and compute its probability mass function.
4.34. Repeat Theoretical Exercise 4.33
, this time assuming that
withdrawn chips are not replaced before the next selection.
4.35. From a set of  elements, a nonempty subset is chosen at random in the
sense that all of the nonempty subsets are equally likely to be selected. Let 
denote the number of elements in the chosen subset. Using the identities
given in Theoretical Exercise 12
 of Chapter 1
, show that
Show also that for  large,
in the sense that the ratio Var( ) to /4 approaches 1 as  approaches 
.
Compare this formula with the limiting form of Var
 when
.
4.36. An urn initially contains one red and one blue ball. At each stage, a ball
is randomly chosen and then replaced along with another of the same color.
Let  denote the selection number of the first chosen ball that is blue. For
instance, if the first selection is red and the second blue, then  is equal to 2.
a. Find 
.
b. Show that with probability 1, a blue ball is eventually chosen. (That is,
𝑁
𝑛, 𝑛൑𝑁
𝑌
𝑌
𝐸𝑌
𝑚൅𝑛
1, 2, …, 𝑛൅𝑚
𝑛
𝑋
𝑋
𝑛
𝑋
𝑛
𝑋
𝐸ሾ𝑋ሿ
ൌ
𝑛
2 െቆ1
2ቇ
௡െଵ
Varሺ𝑋ሻ
ൌ
𝑛⋅2ଶ௡െଶെ𝑛ሺ𝑛൅1ሻ2௡െଶ
ሺ2௡െ1ሻ
ଶ
𝑛
Varሺ𝑋ሻ∼𝑛
4
𝑋
𝑛
𝑛
∞
ሺ𝑌ሻ
𝑃ሼ𝑌ൌ𝑖ሽൌ1/𝑛, 𝑖ൌ1, …, 𝑛
𝑋
𝑋
𝑃ሼ𝑋൐𝑖ሽ, 𝑖൒1
299 of 848

show that 
.)
c. Find 
.
4.37. Suppose the possible values of  are 
, the possible values of  are
, and the possible values of 
 are 
 Let 
 denote the set of all
pairs of indices 
 such that 
; that is, 
a. Argue that
b. Show that
c. Using the formula from part (b), argue that
d. Show that
e. Prove that
𝑃ሼ𝑋൏∞ሽൌ1
𝐸ሾ𝑋ሿ
𝑋
ሼ𝑥௜ሽ
𝑌
ሼ𝑦௝ሽ
𝑋൅𝑌
ሼ𝑧௞ሽ.
𝐴௞
ሺ𝑖, 𝑗ሻ
𝑥௜൅𝑦௝ൌ𝑧௞
𝐴௞ൌሼሺ𝑖, 𝑗ሻ:𝑥௜൅𝑦௝ൌ𝑧௞ሽ.
𝑃ሼ𝑋൅𝑌ൌ𝑧௞ሽൌ
෍
ሺ௜, ௝ሻ∈஺ೖ
𝑃ሼ𝑋ൌ𝑥௜, 𝑌ൌ𝑦௝ሽ
𝐸ሾ𝑋൅𝑌ሿ
ൌ
෍
௞
෍
ሺ௜, ௝ሻ∈஺ೖ
ሺ𝑥௜൅𝑦௝ሻ𝑃ቄ𝑋ൌ𝑥௜, 𝑌ൌ𝑦௝ቅ
𝐸ሾ𝑋൅𝑌ሿ
ൌ
෍
௜
෍
௝
ሺ𝑥௜൅𝑦௝ሻ𝑃ቄ𝑋ൌ𝑥௜, 𝑌ൌ𝑦௝ቅ
𝑃ሺ𝑋ൌ𝑥௜ሻൌ
෍
௝
𝑃ሺ𝑋ൌ𝑥௜, 𝑌ൌ𝑦௝ሻ,
𝑃ሺ𝑌ൌ𝑦௜ሻൌ
෍
௝
𝑃ሺ𝑋ൌ𝑥௜, 𝑌ൌ𝑦௝ሻ,
𝐸ሾ𝑋൅𝑌ሿൌ𝐸ሾ𝑋ሿ൅𝐸ሾ𝑌ሿ
4.1. Suppose that the random variable  is equal to the number of hits
obtained by a certain baseball player in his next 3 at bats. If
, and 
, find 
.
4.2. Suppose that  takes on one of the values 0, 1, and 2. If for some
constant 
, find 
.
4.3. A coin that when flipped comes up heads with probability  is
flipped until either heads or tails has occurred twice. Find the expected
number of flips.
4.4. A certain community is composed of 
 families, 
 of which have 
𝑋
𝑃ሼ𝑋ൌ1ሽൌ. 3, 𝑃ሼ𝑋ൌ2ሽൌ. 2
𝑃ሼ𝑋ൌ0ሽൌ3𝑃ሼ𝑋ൌ3ሽ
𝐸ሾ𝑋ሿ
𝑋
𝑐, 𝑃ሼ𝑋ൌ𝑖ሽൌ𝑐𝑃ሼ𝑋ൌ𝑖െ1ሽ, 𝑖ൌ1, 2
𝐸ሾ𝑋ሿ
𝑝
𝑚
𝑛௜
𝑖
300 of 848

children, 
 If one of the families is randomly chosen, let 
denote the number of children in that family. If one of the 
children is randomly chosen, let  denote the total number of children
in the family of that child. Show that 
.
4.5. Suppose that 
. If 
, find
.
4.6. There are 2 coins in a bin. When one of them is flipped, it lands on
heads with probability .6, and when the other is flipped, it lands on
heads with probability .3. One of these coins is to be randomly chosen
and then flipped. Without knowing which coin is chosen, you can bet
any amount up to $10, and you then either win that amount if the coin
comes up heads or lose it if it comes up tails. Suppose, however, that
an insider is willing to sell you, for an amount , the information as to
which coin was selected. What is your expected payoff if you buy this
information? Note that if you buy it and then bet , you will end up
either winning 
 or 
 (that is, losing 
 in the latter case).
Also, for what values of  does it pay to purchase the information?
4.7. A philanthropist writes a positive number  on a piece of red paper,
shows the paper to an impartial observer, and then turns it face down
on the table. The observer then flips a fair coin. If it shows heads, she
writes the value 
 and, if tails, the value 
, on a piece of blue paper,
which she then turns face down on the table. Without knowing either
the value  or the result of the coin flip, you have the option of turning
over either the red or the blue piece of paper. After doing so and
observing the number written on that paper, you may elect to receive
as a reward either that amount or the (unknown) amount written on the
other piece of paper. For instance, if you elect to turn over the blue
paper and observe the value 100, then you can elect either to accept
100 as your reward or to take the amount (either 200 or 50) on the red
paper. Suppose that you would like your expected reward to be large.
a. Argue that there is no reason to turn over the red paper first,
because if you do so, then no matter what value you observe, it
is always better to switch to the blue paper.
b. Let  be a fixed nonnegative value, and consider the following
strategy: Turn over the blue paper, and if its value is at least ,
then accept that amount. If it is less than , then switch to the
red paper. Let 
 denote the reward obtained if the
philanthropist writes the amount  and you employ this strategy.
෍
௜ൌଵ
௥
𝑛௜ൌ𝑚.
𝑋
෍
௜ൌଵ
௥
𝑖𝑛௜
𝑌
𝐸ሾ𝑌ሿ൒𝐸ሾ𝑋ሿ
𝑃ሼ𝑋ൌ0ሽൌ1 െ𝑃ሼ𝑋ൌ1ሽ
𝐸ሾ𝑋ሿൌ3Varሺ𝑋ሻ
𝑃ሼ𝑋ൌ0ሽ
𝐶
𝑥
𝑥െ𝐶
െ𝑥െ𝐶
𝑥൅𝐶
𝐶
𝑥
2𝑥
𝑥/2
𝑥
𝑦
𝑦
𝑦
𝑅௬ሺ𝑥ሻ
𝑥
301 of 848

Find 
. Note that 
 is the expected reward if the
philanthropist writes the amount  when you employ the strategy
of always choosing the blue paper.
4.8. Let 
, represent a binomial random variable with parameters
 and . Argue that
Hint: The number of successes less than or equal to  is equivalent to
what statement about the number of failures?
4.9. If  is a binomial random variable with expected value 6 and
variance 2.4, find 
.
4.10. An urn contains  balls numbered 1 through . If you withdraw 
balls randomly in sequence, each time replacing the ball selected
previously, find 
, where  is the maximum of the 
chosen numbers.
Hint: First find 
.
4.11. Teams  and  play a series of games, with the first team to win 3
games being declared the winner of the series. Suppose that team 
independently wins each game with probability . Find the conditional
probability that team  wins
a. the series given that it wins the first game;
b. the first game given that it wins the series.
4.12. A local soccer team has 5 more games left to play. If it wins its
game this weekend, then it will play its final 4 games in the upper
bracket of its league, and if it loses, then it will play its final games in
the lower bracket. If it plays in the upper bracket, then it will
independently win each of its games in this bracket with probability .4,
and if it plays in the lower bracket, then it will independently win each of
its games with probability .7. If the probability that the team wins its
game this weekend is .5, what is the probability that it wins at least 3 of
its final 4 games?
4.13. Each of the members of a 7-judge panel independently makes a
correct decision with probability . If the panel’s decision is made by
majority rule, what is the probability that the panel makes the correct
decision? Given that  of the judges agreed, what is the probability that
the panel made the correct decision?
4.14. On average, 5.2 hurricanes hit a certain region in a year. What is
the probability that there will be 3 or fewer hurricanes hitting this year?
4.15. The number of eggs laid on a tree leaf by an insect of a certain
𝐸ൣ𝑅௬ሺ𝑥ሻ൧
𝐸ሾ𝑅଴ሺ𝑥ሻሿ
𝑥
𝐵ሺ𝑛, 𝑝ሻ
𝑛
𝑝
𝑃ሼ𝐵ሺ𝑛, 𝑝ሻ൑𝑖ሽൌ1 െ𝑃ሼ𝐵ሺ𝑛, 1 െ𝑝ሻ൑𝑛െ𝑖െ1ሽ
𝑖
𝑋
𝑃ሼ𝑋ൌ5ሽ
𝑛
𝑛
𝑚
𝑃ሼ𝑋ൌ𝑘ሽ, 𝑘ൌ1, …, 𝑚
𝑋
𝑚
𝑃ሼ𝑋൑𝑘ሽ
𝐴
𝐵
𝐴
𝑝
𝐴
.7
4
302 of 848

type is a Poisson random variable with parameter . However, such a
random variable can be observed only if it is positive, since if it is 0,
then we cannot know that such an insect was on the leaf. If we let 
denote the observed number of eggs, then
where  is Poisson with parameter . Find 
.
4.16. Each of  boys and  girls, independently and randomly, chooses
a member of the other sex. If a boy and girl choose each other, they
become a couple. Number the girls, and let 
 be the event that girl
number  is part of a couple. Let 
 be the
probability that no couples are formed.
a. What is 
?
b. What is 
c. When  is large, approximate 
.
d. When  is large, approximate 
, the probability that exactly 
couples are formed.
e. Use the inclusion—exclusion identity to evaluate 
4.17. A total of 
 people, consisting of  married couples, are
randomly divided into  pairs. Arbitrarily number the women, and let 
denote the event that woman  is paired with her husband.
a. Find 
.
b. For 
, find 
c. When  is large, approximate the probability that no wife is
paired with her husband.
d. If each pairing must consist of a man and a woman, what does
the problem reduce to?
4.18. A casino patron will continue to make $5 bets on red in roulette
until she has won 4 of these bets.
a. What is the probability that she places a total of 9 bets?
b. What are her expected winnings when she stops?
Remark: On each bet, she will either win $5 with probability 
or lose $5 with probability 
.
4.19. When three friends go for coffee, they decide who will pay the
check by each flipping a coin and then letting the “odd person” pay. If
all three flips produce the same result (so that there is no odd person),
then they make a second round of flips, and they continue to do so until
𝜆
𝑌
𝑃ሼ𝑌ൌ𝑖ሽൌ𝑃ሼ𝑋ൌ𝑖|𝑋൐0ሽ
𝑋
𝜆
𝐸ሾ𝑌ሿ
𝑛
𝑛
𝐺௜
𝑖
𝑃଴ൌ1 െ𝑃൫
௜ൌଵ
௡
𝐺௜൯
𝑃ሺ𝐺௜ሻ
𝑃ሺ𝐺௜|𝐺௝ሻ?
𝑛
𝑃଴
𝑛
𝑃௞
𝑘
𝑃଴.
2𝑛
𝑛
𝑛
𝑊௜
𝑖
𝑃ሺ𝑊௜ሻ
𝑖്𝑗
𝑃ሺ𝑊௜|𝑊௝ሻ.
𝑛
18
38
20
38
303 of 848

there is an odd person. What is the probability that
a. exactly 3 rounds of flips are made?
b. more than 4 rounds are needed?
4.20. Show that if  is a geometric random variable with parameter ,
then
Hint: You will need to evaluate an expression of the form 
. To
do so, write 
, and then interchange the sum and the
integral.
4.21. Suppose that
a. Show that 
 is a Bernoulli random variable.
b. Find Var
.
4.22. Each game you play is a win with probability . You plan to play 5
games, but if you win the fifth game, then you will keep on playing until
you lose.
a. Find the expected number of games that you play.
b. Find the expected number of games that you lose.
4.23. Balls are randomly withdrawn, one at a time without replacement,
from an urn that initially has  white and 
 black balls. Find the
probability that  white balls are drawn before 
 black balls,
.
4.24. Ten balls are to be distributed among 5 urns, with each ball going
into urn  with probability 
 Let 
 denote the number of
balls that go into urn . Assume that events corresponding to the
locations of different balls are independent.
a. What type of random variable is 
 Be as specific as possible.
b. For 
, what type of random variable is 
c. Find 
.
4.25. For the match problem (Example 5m
 in Chapter 2
), find
𝑋
𝑝
𝐸ሾ1/𝑋ሿൌെ𝑝logሺ𝑝ሻ
1 െ𝑝
෍
௜ൌଵ
ஶ
𝑎௜/𝑖
𝑎௜/𝑖ൌ඲
଴
௔
𝑥௜െଵ𝑑𝑥
𝑃ሼ𝑋ൌ𝑎ሽൌ𝑝,  𝑃ሼ𝑋ൌ𝑏ሽൌ1 െ𝑝
𝑋െ𝑏
𝑎െ𝑏
ሺ𝑋ሻ
𝑝
𝑁
𝑀
𝑛
𝑚
𝑛൑𝑁, 𝑚൑𝑀
𝑖
𝑝௜,  ෍
௜ൌଵ
ହ
𝑝௜ൌ1 .
𝑋௜
𝑖
𝑋௜?
𝑖്𝑗
𝑋௜൅𝑋௝?
𝑃ሼ𝑋ଵ൅𝑋ଶ൅𝑋ଷൌ7ሽ
304 of 848

a. the expected number of matches.
b. the variance of the number of matches.
4.26. Let  be the probability that a geometric random variable  with
parameter  is an even number.
a. Find  by using the identity 
.
b. Find  by conditioning on whether 
 or 
4.27. Two teams will play a series of games, with the winner being the
first team to win a total of  games. Suppose that, independently of
earlier results, team  wins each game it plays with probability ,
. Let  denote the number of games that are played.
a. Show that 
 with equality only when 
.
b. Give an intuitive explanation for why equality results when
.
Hint: Consider what needs to be true in order for the number of
games to be either  or .
c. If 
, find the probability that the team that wins the first
game wins the series.
4.28. An urn has  white and 
 black balls. Balls are randomly
withdrawn, without replacement, until a total of 
 white balls have
been withdrawn. The random variable  equal to the total number of
balls that are withdrawn is said to be a negative hypergeometric
random variable.
a. Explain how such a random variable differs from a negative
binomial random variable.
b. Find 
Hint for (b): In order for 
 to happen, what must be the results of
the first 
 withdrawals?
4.29. There are  coins which when flipped come up heads,
respectively, with probabilities 
, 
, 
. One of these coins is
randomly chosen and continually flipped.
a. Find the probability that there are a total of  heads in the first 
flips.
b. Find the probability that the first head occurs on flip .
4.30. If  is a binomial random variable with parameters  and , what
type of random variable is 
.
𝛼
𝑋
𝑝
𝛼
𝛼ൌ෍
௜ൌଵ
ஶ
 𝑃ሼ𝑋ൌ2𝑖ሽ 
𝛼
𝑋ൌ1
𝑋൐1 .
4
1
𝑝
0 ൏𝑝൏1
𝑁
𝑃ሺ𝑁ൌ6ሻ൒𝑃ሺ𝑁ൌ7ሻ
𝑝ൌ1/2
𝑝ൌ1/2
6
7
𝑝ൌ1/2
𝑛
𝑚
𝑘, 𝑘൑𝑛
𝑋
𝑃ሼ𝑋ൌ𝑟ሽ.
𝑋ൌ𝑟
𝑟െ1
3
1/3 1/2 3/4
5
8
5
𝑋
𝑛
𝑝
𝑛െ𝑋
305 of 848

5.1 Introduction
5.2 Expectation and Variance of Continuous Random Variables
5.3 The Uniform Random Variable
5.4 Normal Random Variables
5.5 Exponential Random Variables
5.6 Other Continuous Distributions
5.7 The Distribution of a Function of a Random Variable
In Chapter 4
, we considered discrete random variable—that is, random variables
whose set of possible values is either finite or countably infinite. However, there also
exist random variables whose set of possible values is uncountable. Two examples
4.31. Let  be the 
 smallest number in a random sample of  of the
numbers 
. Find the probability mass function of .
4.32. Balls are randomly removed from an urn consisting of  red and
 blue balls. Let  denote the number of balls that have to be removed
until a total of  red balls have been removed.  is said to be a negative
hypergeometric random variable.
a. Find the probability mass function of .
b. Find the probability mass function of , equal to the number of
balls that have to be removed until either  red balls or  blue
balls have been removed.
c. Find the probability mass function of , equal to the number of
balls that have to be removed until both at least  red balls and
at least  blue balls have been removed.
d. Find the probability that  red balls are removed before  blue
balls have been removed.
𝑋
𝑖௧௛
𝑛
1, …, 𝑛൅𝑚
𝑋
𝑛
𝑚
𝑋
𝑟
𝑋
𝑋
𝑉
𝑟
𝑠
𝑍
𝑟
𝑠
𝑟
𝑠
306 of 848

are the time that a train arrives at a specified stop and the lifetime of a transistor. Let
 be such a random variable. We say that  is a continuous † random variable if
there exists a nonnegative function 
 defined for all real 
 having the
property that for any set  of real numbers, ‡
† Sometimes called absolutely continuous.
 Actually, for technical reasons, Equation (1.1)
 is true only for the
measurable sets 
 which, fortunately, include all sets of practical
interest.
The function  is called the probability density function of the random variable 
(See Figure 5.1
.)
Figure 5.1 Probability density function 
In words, Equation (1.1)
 states that the probability that  will be in  may be
obtained by integrating the probability density function over the set 
 Since  must
assume some value,  must satisfy
𝑋
𝑋
𝑓,
𝑥∈ሺെ∞,∞ሻ,
𝐵
𝑃ሼ𝑋∈𝐵ሽൌ඲
஻
𝑓ሺ𝑥ሻ𝑑𝑥
(1.1)
‡
𝐵,
𝑓
𝑋.
𝑓.
𝑋
𝐵
𝐵.
𝑋
𝑓
307 of 848

All probability statements about  can be answered in terms of 
 For instance, from
Equation (1.1)
, letting 
 we obtain
If we let 
 in Equation (1.2)
, we get
In words, this equation states that the probability that a continuous random variable
will assume any fixed value is zero. Hence, for a continuous random variable,
Example 1a
Suppose that  is a continuous random variable whose probability density
function is given by
a. What is the value of 
b. Find 
Solution
a. Since  is a probability density function, we must have 
implying that
1 ൌ𝑃ሼ𝑋∈ሺെ∞,∞ሻሽൌ඲
െஶ
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥
𝑋
𝑓.
𝐵ൌሾ𝑎,𝑏ሿ,
𝑃ሼ𝑎൑𝑋൑𝑏ሽൌ඲
௔
௕
𝑓ሺ𝑥ሻ𝑑𝑥
(1.2)
𝑎ൌ𝑏
𝑃ሼ𝑋ൌ𝑎ሽൌ඲
௔
௔
𝑓ሺ𝑥ሻ𝑑𝑥ൌ0
𝑃ሼ𝑋൏𝑎ሽൌ𝑃ሼ𝑋൑𝑎ሽൌ𝐹ሺ𝑎ሻൌ඲
െஶ
௔
𝑓ሺ𝑥ሻ𝑑𝑥
𝑋
𝑓ሺ𝑥ሻൌ൝𝐶ሺ4𝑥െ2𝑥ଶሻ0 ൏𝑥൏2
0
otherwise
𝐶?
𝑃ሼ𝑋൐1ሽ.
𝑓
඲
െஶ
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥ൌ1,
𝐶඲
଴
ଶ
ሺ4𝑥െ2𝑥ଶሻ𝑑𝑥ൌ1
308 of 848

or
or
Hence,
b. 
Example 1b
The amount of time in hours that a computer functions before breaking down is a
continuous random variable with probability density function given by
What is the probability that
a. a computer will function between 50 and 150 hours before breaking
down?
b. it will function for fewer than 100 hours?
Solution
a. Since
we obtain
Hence, the probability that a computer will function between 50 and 150
hours before breaking down is given by
𝐶ቈ2𝑥ଶെ2𝑥ଷ
3 ቉ቤ
௫ൌ଴
௫ൌଶ
ൌ1
𝐶ൌ3
8
𝑃ሼ𝑋൐1ሽൌ඲
ଵ
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥ൌ3
8඲
ଵ
ଶ
ሺ4𝑥െ2𝑥ଶሻ𝑑𝑥ൌ1
2
𝑓ሺ𝑥ሻൌቊ𝜆𝑒െ௫/ଵ଴଴𝑥൒0
0
𝑥൏0
1 ൌ඲
െஶ
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥ൌ𝜆඲
଴
ஶ
𝑒െ௫/ଵ଴଴𝑑𝑥
1 ൌെ𝜆ሺ100ሻ𝑒െ௫/ଵ଴଴|
||
|଴
ஶ
ൌ100𝜆 o𝑟 𝜆ൌ
1
100
𝑃ሼ50 ൏𝑋൏150ሽ
ൌ
඲
ହ଴
ଵହ଴
ଵ
ଵ଴଴𝑒െ௫/ଵ଴଴𝑑𝑥ൌെ𝑒െ௫/ଵ଴଴ቤ
ହ଴
ଵହ଴
ൌ
𝑒െଵ/ଶെ𝑒െଷ/ଶൎ.383
309 of 848

b. Similarly,
In other words, approximately 63.2 percent of the time, a computer will fail before
registering 100 hours of use.
Example 1c
The lifetime in hours of a certain kind of radio tube is a random variable having a
probability density function given by
What is the probability that exactly 2 of 5 such tubes in a radio set will have to be
replaced within the first 150 hours of operation? Assume that the events
 that the ith such tube will have to be replaced within this time are
independent.
Solution
From the statement of the problem, we have
Hence, from the independence of the events 
 it follows that the desired
probability is
The relationship between the cumulative distribution  and the probability density  is
expressed by
𝑃ሼ𝑋൏100ሽൌ඲
଴
ଵ଴଴
1
100 𝑒െ௫/ଵ଴଴𝑑𝑥ൌെ𝑒െ௫/ଵ଴଴ቤ
଴
ଵ଴଴
ൌ1 െ𝑒െଵൎ.632
𝑓ሺ𝑥ሻൌቐ
0
𝑥൑100
ଵ଴଴
௫మ
𝑥൐100
𝐸௜, 𝑖ൌ1,2,3,4,5,
𝑃ሺ𝐸௜ሻ
ൌ
඲
଴
ଵହ଴
𝑓ሺ𝑥ሻ𝑑𝑥
ൌ
100඲
ଵ଴଴
ଵହ଴
𝑥െଶ𝑑𝑥
ൌ
1
3
𝐸௜,
൭5
2൱ቆ1
3ቇ
ଶ
ቆ2
3ቇ
ଷ
ൌ80
243
𝐹
𝑓
310 of 848

Differentiating both sides of the preceding equation yields
That is, the density is the derivative of the cumulative distribution function. A
somewhat more intuitive interpretation of the density function may be obtained from
Equation (1.2)
 as follows:
when  is small and when 
 is continuous at 
 In other words, the probability
that  will be contained in an interval of length  around the point  is approximately
 From this result, we see that 
 is a measure of how likely it is that the
random variable will be near 
Example 1d
If  is continuous with distribution function 
 and density function 
 find the
density function of 
Solution
We will determine 
 in two ways. The first way is to derive, and then
differentiate, the distribution function of 
Differentiation gives
Another way to determine 
 is to note that
𝐹ሺ𝑎ሻൌ𝑃ሼ𝑋∈ሺെ∞,𝑎ሿሽൌ඲
െஶ
௔
𝑓ሺ𝑥ሻ𝑑𝑥
𝑑
𝑑𝑎𝐹ሺ𝑎ሻൌ𝑓ሺ𝑎ሻ
𝑃൜𝑎െ𝜀
2 ൑𝑋൑𝑎൅𝜀
2ൠൌ඲
௔െఌ/ଶ
௔൅ఌ/ଶ
𝑓ሺ𝑥ሻ𝑑𝑥ൎ𝜀𝑓ሺ𝑎ሻ
𝜀
𝑓ሺ⋅ሻ
𝑥ൌ𝑎.
𝑋
𝜀
𝑎
𝜀𝑓ሺ𝑎ሻ.
𝑓ሺ𝑎ሻ
𝑎.
𝑋
𝐹௑
𝑓௑,
𝑌ൌ2𝑋.
𝑓௒
𝑌:
𝐹௒ሺ𝑎ሻ
ൌ
𝑃ሼ𝑌൑𝑎ሽ
ൌ
𝑃ሼ2𝑋൑𝑎ሽ
ൌ
𝑃ሼ𝑋൑𝑎/2ሽ
ൌ
𝐹௑ሺ𝑎/2ሻ
𝑓௒ሺ𝑎ሻൌ1
2 𝑓௑ሺ𝑎/2ሻ
𝑓௒
311 of 848

Dividing through by  gives the same result as before.
In Chapter 4
, we defined the expected value of a discrete random variable  by
If  is a continuous random variable having probability density function 
 then,
because
it is easy to see that the analogous definition is to define the expected value of  by
Example 2a
Find 
 when the density function of  is
Solution
𝜀𝑓௒ሺ𝑎ሻൎ
𝑃ቄ𝑎െ
ఌ
ଶ൑𝑌൑𝑎൅
ఌ
ଶቅ
ൌ
𝑃ቄ𝑎െ
ఌ
ଶ൑2𝑋൑𝑎൅
ఌ
ଶቅ
ൌ
𝑃ቄ
௔
ଶെ
ఌ
ସ൑𝑋൑
௔
ଶ൅
ఌ
ସቅ
ൎ
ఌ
ଶ𝑓௑ሺ𝑎/2ሻ
𝜀
𝑋
𝐸ሾ𝑋ሿൌ෍
௫
𝑥𝑃ሼ𝑋ൌ𝑥ሽ
𝑋
𝑓ሺ𝑥ሻ,
𝑓ሺ𝑥ሻ𝑑𝑥ൎ𝑃ሼ𝑥൑𝑋൑𝑥൅𝑑𝑥ሽ for 𝑑𝑥 small
𝑋
𝐸ሾ𝑋ሿൌ඲
െஶ
ஶ
𝑥𝑓ሺ𝑥ሻ𝑑𝑥
𝐸ሾ𝑋ሿ
𝑋
𝑓ሺ𝑥ሻൌቊ2𝑥
if 0 ൑𝑥൑1
0
otherwise
312 of 848

Example 2b
The density function of  is given by
Find 
Solution
Let 
 We start by determining 
 the cumulative distribution function of 
Now, for 
By differentiating 
 we can conclude that the probability density function of 
is given by
Hence,
𝐸ሾ𝑋ሿ
ൌ඲𝑥𝑓ሺ𝑥ሻ 𝑑𝑥
ൌ඲
଴
ଵ
2𝑥ଶ 𝑑𝑥
ൌ
ଶ
ଷ
𝑋
𝑓ሺ𝑥ሻൌቊ1
if 0 ൑𝑥൑1
0
otherwise
𝐸ሾ𝑒௑ሿ.
𝑌ൌ𝑒௑.
𝐹௒,
𝑌.
1 ൑𝑥൑𝑒,
𝐹௒ሺ𝑥ሻൌ
𝑃ሼ𝑌൑𝑥ሽ
ൌ
𝑃ሼ𝑒௑൑𝑥ሽ
ൌ
𝑃ሼ𝑋൑logሺ𝑥ሻሽ
ൌ
඲
଴
୪୭୥ሺ௫ሻ
𝑓ሺ𝑦ሻ𝑑𝑦
ൌ
logሺ𝑥ሻ
𝐹௒ሺ𝑥ሻ,
𝑌
𝑓௒ሺ𝑥ሻൌ1
𝑥  1 ൑𝑥൑𝑒
313 of 848

Although the method employed in Example 2b
 to compute the expected value of
a function of  is always applicable, there is, as in the discrete case, an alternative
way of proceeding. The following is a direct analog of Proposition 4.1
 of Chapter
4
.
Proposition 2.1
If  is a continuous random variable with probability density function 
 then,
for any real-valued function 
An application of Proposition 2.1
 to Example 2b
 yields
which is in accord with the result obtained in that example.
The proof of Proposition 2.1
 is more involved than that of its discrete random
variable analog. We will present such a proof under the provision that the random
variable 
 is nonnegative. (The general proof, which follows the argument in
the case we present, is indicated in Theoretical Exercises 5.2
 and 5.3
.)
We will need the following lemma, which is of independent interest.
Lemma 2.1
For a nonnegative random variable 
𝐸ሾ𝑒௑ሿൌ𝐸ሾ𝑌ሿ
ൌ
െஶ
ஶ
𝑥𝑓௒ሺ𝑥ሻ𝑑𝑥
ൌ
ଵ
௘
𝑑𝑥
ൌ𝑒െ1
𝑋
𝑋
𝑓ሺ𝑥ሻ,
𝑔,
𝐸ሾ𝑔ሺ𝑋ሻሿൌ඲
െஶ
ஶ
𝑔ሺ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥
𝐸ሾ𝑒௑ሿ
ൌ
඲
଴
ଵ
𝑒௫𝑑𝑥  since 𝑓ሺ𝑥ሻൌ1, 0 ൏𝑥൏1
ൌ
𝑒െ1
𝑔ሺ𝑋ሻ
𝑌,
314 of 848

Proof We present a proof when  is a continuous random variable with
probability density function 
 We have
where we have used the fact that 
 Interchanging the
order of integration in the preceding equation yields
Proof of Proposition 2.1 From Lemma 2.1
, for any function  for which 
which completes the proof.
Example 2c
A stick of length 1 is split at a point  having density function 
𝐸ሾ𝑌ሿൌ඲
଴
ஶ
𝑃ሼ𝑌൐𝑦ሽ𝑑𝑦
𝑌
𝑓௒.
඲
଴
ஶ
𝑃ሼ𝑌൐𝑦ሽ𝑑𝑦ൌ඲
଴
ஶ
඲
௬
ஶ
𝑓௒ሺ𝑥ሻ𝑑𝑥𝑑𝑦
𝑃ሼ𝑌൐𝑦ሽൌ඲
௬
ஶ
𝑓௒ሺ𝑥ሻ𝑑𝑥.
඲
଴
ஶ
𝑃ሼ𝑌൐𝑦ሽ 𝑑𝑦
ൌ඲
଴
ஶ
ቌ඲
଴
௫
𝑑𝑦ቍ𝑓௒ሺ𝑥ሻ𝑑𝑥
ൌ඲
଴
ஶ
𝑥𝑓௒ሺ𝑥ሻ𝑑𝑥
ൌ𝐸ሾ𝑌ሿ
𝑔
𝑔ሺ𝑥ሻ൒0
,
𝐸ሾ𝑔ሺ𝑋ሻሿ
ൌ
඲
଴
ஶ
𝑃ሼ𝑔ሺ𝑋ሻ൐𝑦ሽ 𝑑𝑦
ൌ
඲
଴
ஶ
඲
௫:௚ሺ௫ሻவ௬
𝑓ሺ𝑥ሻ𝑑𝑥 𝑑𝑦
ൌ
඲
௫:௚ሺ௫ሻவ଴
඲
଴
௚ሺ௫ሻ
𝑑𝑦 𝑓ሺ𝑥ሻ𝑑𝑥
ൌ
඲
௫:௚ሺ௫ሻவ଴
𝑔ሺ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥
𝑈
𝑓ሺ𝑢ሻൌ1, 0 ൏𝑢൏1.
315 of 848

Determine the expected length of the piece that contains the point 
Solution
Let 
 denote the length of the substick that contains the point 
 and note
that
(See Figure 5.2
.) Hence, from Proposition 2.1
,
Figure 5.2 Substick containing point 
 (a) 
 (b) 
Since 
 is maximized when 
 it is interesting to note that the
expected length of the substick containing the point  is maximized when  is the
midpoint of the original stick.
Example 2d
Suppose that if you are  minutes early for an appointment, then you incur the
cost cs, and if you are  minutes late, then you incur the cost ks. Suppose also
that the travel time from where you presently are to the location of your
appointment is a continuous random variable having probability density function
 Determine the time at which you should depart if you want to minimize your
expected cost.
𝑝, 0 ൑𝑝൑1.
𝐿௣ሺ𝑈ሻ
𝑝,
𝐿௣ሺ𝑈ሻൌቊ1 െ𝑈
𝑈൏𝑝
𝑈
𝑈൐𝑝
𝐸ሾ𝐿௣ሺ𝑈ሻሿ
ൌ
඲
଴
ଵ
𝐿௣ሺ𝑢ሻ𝑑𝑢
ൌ
඲
଴
௣
ሺ1 െ𝑢ሻ𝑑𝑢൅඲
௣
ଵ
𝑢𝑑𝑢
ൌ
1
2 െሺ1 െ𝑝ሻଶ
2
൅1
2 െ𝑝ଶ
2
ൌ
1
2 ൅𝑝ሺ1 െ𝑝ሻ
𝑝:
𝑈൏𝑝;
𝑈൐𝑝.
𝑝ሺ1 െ𝑝ሻ
𝑝ൌ1
2,
𝑝
𝑝
𝑠
𝑠
𝑓.
316 of 848

Solution
Let  denote the travel time. If you leave  minutes before your appointment, then
your cost—call it 
—is given by
Therefore,
The value of  that minimizes 
 can now be obtained by calculus.
Differentiation yields
Equating the rightmost side to zero shows that the minimal expected cost is
obtained when you leave  minutes before your appointment, where  satisfies
As in Chapter 4
, we can use Proposition 2.1
 to show the following.
Corollary 2.1
If  and  are constants, then
The proof of Corollary 2.1
 for a continuous random variable  is the same as
the one given for a discrete random variable. The only modification is that the
sum is replaced by an integral and the probability mass function by a probability
density function.
𝑋
𝑡
𝐶௧ሺ𝑋ሻ
𝐶௧ሺ𝑋ሻൌ൝
𝑐ሺ𝑡െ𝑋ሻ
if 𝑋൑𝑡
𝑘ሺ𝑋െ𝑡ሻif 𝑋൒𝑡
𝐸ሾ𝐶௧ሺ𝑋ሻሿ
ൌ
඲
଴
ஶ
𝐶௧ሺ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥
ൌ
඲
଴
௧
𝑐ሺ𝑡െ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥൅඲
௧
ஶ
𝑘ሺ𝑥െ𝑡ሻ𝑓ሺ𝑥ሻ𝑑𝑥
ൌ
𝑐𝑡඲
଴
௧
𝑓ሺ𝑥ሻ𝑑𝑥െ𝑐඲
଴
௧
𝑥𝑓ሺ𝑥ሻ𝑑𝑥൅𝑘඲
௧
ஶ
𝑥𝑓ሺ𝑥ሻ𝑑𝑥െ𝑘𝑡඲
௧
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥
𝑡
𝐸ሾ𝐶௧ሺ𝑋ሻሿ
ௗ
ௗ௧𝐸ሾ𝐶௧ሺ𝑋ሻሿ
ൌ
𝑐𝑡 𝑓ሺ𝑡ሻ൅𝑐𝐹ሺ𝑡ሻെ𝑐𝑡 𝑓ሺ𝑡ሻെ𝑘𝑡 𝑓ሺ𝑡ሻ൅𝑘𝑡 𝑓ሺ𝑡ሻെ𝑘ሾ1 െ𝐹ሺ𝑡ሻሿ
ൌ
ሺ𝑘൅𝑐ሻ𝐹ሺ𝑡ሻെ𝑘
𝑡*
𝑡*
𝐹ሺ𝑡*ሻൌ
𝑘
𝑘൅𝑐
𝑎
𝑏
𝐸ሾ𝑎𝑋൅𝑏ሿൌ𝑎𝐸ሾ𝑋ሿ൅𝑏
𝑋
317 of 848

The variance of a continuous random variable is defined exactly as it is for a
discrete random variable, namely, if  is a random variable with expected value 
 then the variance of  is defined (for any type of random variable) by
The alternative formula,
is established in a manner similar to its counterpart in the discrete case.
Example 2e
Find Var
 for  as given in Example 2a
.
Solution
We first compute 
Hence, since 
 we obtain
It can be shown that, for constants  and 
The proof mimics the one given for discrete random variables.
There are several important classes of continuous random variables that appear
frequently in applications of probability; the next few sections are devoted to a study
of some of them.
𝑋
𝜇
,
𝑋
Varሺ𝑋ሻൌ𝐸ሾሺ𝑋െ𝜇ሻଶሿ
Varሺ𝑋ሻൌ𝐸ሾ𝑋ଶሿെሺ𝐸ሾ𝑋ሿሻଶ
ሺ𝑋ሻ
𝑋
𝐸ሾ𝑋ଶሿ.
𝐸ሾ𝑋ଶሿ
ൌ
඲
െஶ
ஶ
𝑥ଶ𝑓ሺ𝑥ሻ𝑑𝑥
ൌ
඲
଴
ଵ
2𝑥ଷ𝑑𝑥
ൌ
1
2
𝐸ሾ𝑋ሿൌ2
3,
Varሺ𝑋ሻൌ1
2 െቆ2
3ቇ
ଶ
ൌ1
18
𝑎
𝑏,
Varሺ𝑎𝑋൅𝑏ሻൌ𝑎ଶVarሺ𝑋ሻ
318 of 848

A random variable is said to be uniformly distributed over the interval (0, 1) if its
probability density function is given by
Note that Equation (3.1)
 is a density function, since 
 and
 Because 
 only when 
 it follows that 
must assume a value in interval (0, 1). Also, since 
 is constant for 
 is
just as likely to be near any value in (0, 1) as it is to be near any other value. To
verify this statement, note that for any 
In other words, the probability that  is in any particular subinterval of (0, 1) equals
the length of that subinterval.
In general, we say that  is a uniform random variable on the interval 
 if the
probability density function of  is given by
Since 
 it follows from Equation (3.2)
 that the distribution
function of a uniform random variable on the interval 
 is given by
Figure 5.3
 presents a graph of 
 and 
𝑓ሺ𝑥ሻൌቊ1
0 ൏𝑥൏1
0
otherwise
(3.1)
𝑓ሺ𝑥ሻ൒0
඲
െஶ
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥ൌ඲
଴
ଵ
𝑑𝑥ൌ1.
𝑓ሺ𝑥ሻ൐0
𝑥∈ሺ0,1ሻ,
𝑋
𝑓ሺ𝑥ሻ
𝑥∈ሺ0,1ሻ, 𝑋
0 ൏𝑎൏𝑏൏1,
𝑃ሼ𝑎൑𝑋൑𝑏ሽൌ඲
௔
௕
𝑓ሺ𝑥ሻ𝑑𝑥ൌ𝑏െ𝑎
𝑋
𝑋
ሺ𝛼, 𝛽ሻ
𝑋
𝑓ሺ𝑥ሻൌ൝
ଵ
ఉെఈ
if 𝛼൏𝑥൏𝛽
0
otherwise
(3.2)
𝐹ሺ𝑎ሻൌ඲
െஶ
௔
𝑓ሺ𝑥ሻ𝑑𝑥,
ሺ𝛼, 𝛽ሻ
𝐹ሺ𝑎ሻൌ
⎧
⎨
⎩
⎪
⎪
0
𝑎൑𝛼
௔െఈ
ఉെఈ
𝛼൏𝑎൏𝛽
1
𝑎൒𝛽
𝑓ሺ𝑎ሻ
𝐹ሺ𝑎ሻ.
319 of 848

Figure 5.3 Graph of (a) 
 and (b) 
 for a uniform 
 random variable.
Example 3a
Let  be uniformly distributed over 
 Find (a) [ ] and (b) Var
Solution
a. 
𝑓ሺ𝑎ሻ
𝐹ሺ𝑎ሻ
ሺ𝛼, 𝛽ሻ
𝑋
ሺ𝛼, 𝛽ሻ.
𝐸𝑋
ሺ𝑋ሻ.
𝐸ሾ𝑋ሿ
ൌ
඲
െஶ
ஶ
𝑥𝑓ሺ𝑥ሻ𝑑𝑥
ൌ
඲
ఈ
ఉ
𝑥
𝛽െ𝛼𝑑𝑥
ൌ
𝛽ଶെ𝛼ଶ
2ሺ𝛽െ𝛼ሻ
ൌ
𝛽൅𝛼
2
320 of 848

In words, the expected value of a random variable that is uniformly
distributed over some interval is equal to the midpoint of that interval.
b. To find Var
 we first calculate 
Hence,
Therefore, the variance of a random variable that is uniformly distributed
over some interval is the square of the length of that interval divided by 12.
Example 3b
If  is uniformly distributed over (0, 10), calculate the probability that (a) 
(b) 
 and (c) 
Solution
a. 
b. 
c. 
Example 3c
Buses arrive at a specified stop at 15-minute intervals starting at 7 ඉ.ඕ. That is,
they arrive at 7, 7:15, 7:30, 7:45, and so on. If a passenger arrives at the stop at
a time that is uniformly distributed between 7 and 7:30, find the probability that he
waits
a. less than 5 minutes for a bus;
ሺ𝑋ሻ,
𝐸ሾ𝑋ଶሿ.
𝐸ሾ𝑋ଶሿ
ൌ
඲
ఈ
ఉ
1
𝛽െ𝛼𝑥ଶ𝑑𝑥
ൌ
𝛽ଷെ𝛼ଷ
3ሺ𝛽െ𝛼ሻ
ൌ
𝛽ଶ൅𝛼𝛽൅𝛼ଶ
3
Varሺ𝑋ሻ
ൌ
𝛽ଶ൅𝛼𝛽൅𝛼ଶ
3
െሺ𝛼൅𝛽ሻଶ
4
ൌ
ሺ𝛽െ𝛼ሻଶ
12
𝑋
𝑋൏3,
𝑋൐6,
3 ൏𝑋൏8.
𝑃ሼ𝑋൏3ሽൌ඲
଴
ଷ
1
10 𝑑𝑥ൌ3
10
𝑃ሼ𝑋൐6ሽൌ඲
଺
ଵ଴
1
10 𝑑𝑥ൌ4
10
𝑃ሼ3 ൏𝑋൏8ሽൌ඲
ଷ
଼
1
10 𝑑𝑥ൌ1
2
321 of 848

b. more than 10 minutes for a bus.
Solution
Let  denote the number of minutes past 7 that the passenger arrives at the stop.
Since  is a uniform random variable over the interval (0, 30), it follows that the
passenger will have to wait less than 5 minutes if (and only if) he arrives between
7:10 and 7:15 or between 7:25 and 7:30. Hence, the desired probability for part
(a) is
Similarly, he would have to wait more than 10 minutes if he arrives between 7
and 7:05 or between 7:15 and 7:20, so the probability for part (b) is
The next example was first considered by the French mathematician Joseph L. F.
Bertrand in 1889 and is often referred to as Bertrand’s paradox. It represents our
initial introduction to a subject commonly referred to as geometrical probability.
Example 3d
Consider a random chord of a circle. What is the probability that the length of the
chord will be greater than the side of the equilateral triangle inscribed in that
circle?
Solution
As stated, the problem is incapable of solution because it is not clear what is
meant by a random chord. To give meaning to this phrase, we shall reformulate
the problem in two distinct ways.
The first formulation is as follows: The position of the chord can be determined by
its distance from the center of the circle. This distance can vary between 0 and 
the radius of the circle. Now, the length of the chord will be greater than the side
of the equilateral triangle inscribed in the circle if the distance from the chord to
the center of the circle is less than /2. Hence, by assuming that a random chord
is a chord whose distance  from the center of the circle is uniformly distributed
between 0 and 
 we see that the probability that the length of the chord is
greater than the side of an inscribed equilateral triangle is
𝑋
𝑋
𝑃ሼ10 ൏𝑋൏15ሽ൅𝑃ሼ25 ൏𝑋൏30ሽൌ඲
ଵ଴
ଵହ
1
30 𝑑𝑥൅඲
ଶହ
ଷ଴
1
30 𝑑𝑥ൌ1
3
𝑃ሼ0 ൏𝑋൏5ሽ൅𝑃ሼ15 ൏𝑋൏20ሽൌ1
3
𝑟,
𝑟
𝐷
𝑟,
322 of 848

For our second formulation of the problem, consider an arbitrary chord of the
circle; through one end of the chord, draw a tangent. The angle  between the
chord and the tangent, which can vary from 
 to 
 determines the position of
the chord. (See Figure 5.4
.) Furthermore, the length of the chord will be
greater than the side of the inscribed equilateral triangle if the angle  is between
 and 
 Hence, assuming that a random chord is a chord whose angle  is
uniformly distributed between 
 and 
 we see that the desired answer in this
formulation is
Figure 5.4
Note that random experiments could be performed in such a way that  or 
would be the correct probability. For instance, if a circular disk of radius  is
thrown on a table ruled with parallel lines a distance 
 apart, then one and only
one of these lines would cross the disk and form a chord. All distances from this
chord to the center of the disk would be equally likely, so that the desired
probability that the chord’s length will be greater than the side of an inscribed
equilateral triangle is 
 In contrast, if the experiment consisted of rotating a
needle freely about a point  on the edge (see Figure 5.4
) of the circle, the
desired answer would be 
𝑃൜𝐷൏𝑟
2ൠൌ𝑟/2
𝑟
ൌ1
2
𝜃
0∘
180∘,
𝜃
60∘
120∘.
𝜃
0∘
180∘,
𝑃ሼ60 ൏𝜃൏120ሽൌ120 െ60
180
ൌ1
3
1
2
1
3
𝑟
2𝑟
1
2 .
𝐴
1
3 .
323 of 848

We say that  is a normal random variable, or simply that  is normally distributed,
with parameters  and 
 if the density of  is given by
This density function is a bell-shaped curve that is symmetric about 
 (See Figure
5.5
.)
Figure 5.5 Normal density function: (a) 
 (b) arbitrary 
The normal distribution was introduced by the French mathematician Abraham
DeMoivre in 1733, who used it to approximate probabilities associated with binomial
random variables when the binomial parameter  is large. This result was later
extended by Laplace and others and is now encompassed in a probability theorem
known as the central limit theorem, which is discussed in Chapter 8
. The central
limit theorem, one of the two most important results in probability theory,† gives a
theoretical base to the often noted empirical observation that, in practice, many
random phenomena obey, at least approximately, a normal probability distribution.
Some examples of random phenomena obeying this behavior are the height of a
man or woman, the velocity in any direction of a molecule in gas, and the error made
in measuring a physical quantity.
𝑋
𝑋
𝜇
𝜎ଶ
𝑋
𝑓ሺ𝑥ሻൌ
1
2𝜋
√
𝜎𝑒െሺ௫െఓሻమ/ଶఙమ   െ∞൏𝑥൏∞
𝜇.
𝜇ൌ0, 𝜎ൌ1;
𝜇, 𝜎ଶ.
𝑛
324 of 848

† The other is the strong law of large numbers.
To prove that 
 is indeed a probability density function, we need to show that
Making the substitution 
 we see that
Hence, we must show that
Toward this end, let 
 Then
We now evaluate the double integral by means of a change of variables to polar
coordinates. (That is, let 
 and 
) Thus,
Hence, 
 and the result is proved.
𝑓ሺ𝑥ሻ
1
2𝜋
√
𝜎඲
െஶ
ஶ
𝑒െሺ௫െఓሻమ/ଶఙమ𝑑𝑥ൌ1
𝑦ൌሺ𝑥െ𝜇ሻ/𝜎,
1
2𝜋
√
𝜎඲
െஶ
ஶ
𝑒െሺ௫െఓሻమ/ଶఙమ𝑑𝑥ൌ
1
2𝜋
√
඲
െஶ
ஶ
𝑒െ௬మ/ଶ𝑑𝑦
඲
െஶ
ஶ
𝑒െ௬మ/ଶ𝑑𝑦ൌ
2𝜋
√
𝐼ൌ඲
െஶ
ஶ
𝑒െ௬మ/ଶ𝑑𝑦.
𝐼ଶ
ൌ
඲
െஶ
ஶ
𝑒െ௬మ/ଶ𝑑𝑦඲
െஶ
ஶ
𝑒െ௫మ/ଶ𝑑𝑥
ൌ
඲
െஶ
ஶ
඲
െஶ
ஶ
𝑒െሺ௬మ൅௫మሻ/ଶ𝑑𝑦 𝑑𝑥
𝑥ൌ𝑟cos 𝜃, 𝑦ൌ𝑟sin 𝜃,
𝑑𝑦𝑑𝑥ൌ𝑟𝑑𝜃𝑑𝑟.
𝐼ଶ
ൌ
඲
଴
ஶ
඲
଴
ଶగ
𝑒െ௥మ/ଶ𝑟 𝑑𝜃 𝑑𝑟
ൌ
2𝜋඲
଴
ஶ
𝑟𝑒െ௥మ/ଶ 𝑑𝑟
ൌ
െ2𝜋𝑒െ௥మ/ଶ||଴
ஶ
ൌ
2𝜋
𝐼ൌ
2𝜋
√
,
325 of 848

An important fact about normal random variables is that if  is normally distributed
with parameters  and 
 then 
 is normally distributed with parameters
 and 
 To prove this statement, suppose that 
 (The proof when 
is similar.) Let 
 denote the cumulative distribution function of 
 Then
where 
 is the cumulative distribution function of 
 By differentiation, the density
function of  is then
which shows that  is normal with parameters 
 and 
An important implication of the preceding result is that if  is normally distributed with
parameters  and 
 then 
 is normally distributed with parameters 
and 
 Such a random variable is said to be a standard, or a unit, normal random
variable.
We now show that the parameters  and 
 of a normal random variable represent,
respectively, its expected value and variance.
Example 4a
Find 
 and 
 when  is a normal random variable with parameters  and
Solution
Let us start by finding the mean and variance of the standard normal random
variable 
 We have
𝑋
𝜇
𝜎ଶ,
𝑌ൌ𝑎𝑋൅𝑏
𝑎𝜇൅𝑏
𝑎ଶ𝜎ଶ.
𝑎൐0.
𝑎൏0
𝐹௒
𝑌.
𝐹௒ሺ𝑥ሻ
ൌ𝑃ሼ𝑌൑𝑥ሽ
ൌ𝑃ሼ𝑎𝑋൅𝑏൑𝑥ሽ
ൌ𝑃ቄ𝑋൑
௫െ௕
௔ቅ
ൌ𝐹௑ቀ
௫െ௕
௔ቁ
𝐹௑
𝑋.
𝑌
𝑓௒ሺ𝑥ሻൌ
1
𝑎𝑓௑ቆ𝑥െ𝑏
𝑎
ቇ
ൌ
1
2𝜋
√
𝑎𝜎exp൝െቆ𝑥െ𝑏
𝑎
െ𝜇ቇ
ଶ
/2𝜎ଶൡ
ൌ
1
2𝜋
√
𝑎𝜎expሼെሺ𝑥െ𝑏െ𝑎𝜇ሻଶ/2ሺ𝑎𝜎ሻଶሽ
𝑌
𝑎𝜇൅𝑏
𝑎ଶ𝜎ଶ.
𝑋
𝜇
𝜎ଶ,
𝑍ൌሺ𝑋െ𝜇ሻ/𝜎
0
1 .
𝜇
𝜎ଶ
𝐸ሾ𝑋ሿ
Varሺ𝑋ሻ
𝑋
𝜇
𝜎ଶ.
𝑍ൌሺ𝑋െ𝜇ሻ/𝜎.
326 of 848

Thus,
Integration by parts (with 
 and 
 now gives
Because 
 the preceding yields the results
and
It is customary to denote the cumulative distribution function of a standard normal
random variable by 
 That is,
The values of 
 for nonnegative  are given in Table 5.1
. For negative values
of 
 can be obtained from the relationship
𝐸ሾ𝑍ሿ
ൌ
඲
െஶ
ஶ
𝑥𝑓௓ሺ𝑥ሻ𝑑𝑥
ൌ
ଵ
ଶగ
√
඲
െஶ
ஶ
𝑥𝑒െ௫మ/ଶ𝑑𝑥
ൌ
െ
ଵ
ଶగ
√
𝑒െ௫మ/ଶቚ
െஶ
ஶ
ൌ0
Varሺ𝑍ሻ
ൌ
𝐸ሾ𝑍ଶሿ
ൌ
ଵ
ଶగ
√
඲
െஶ
ஶ
𝑥ଶ𝑒െ௫మ/ଶ𝑑𝑥
𝑢ൌ𝑥
𝑑𝑣ൌ𝑥𝑒െ௫మ/ଶሻ
Varሺ𝑍ሻൌ
ଵ
ଶగ
√
ቌെ𝑥𝑒െ௫మ/ଶቤ
െஶ
ஶ
൅඲
െஶ
ஶ
𝑒െ௫మ/ଶ𝑑𝑥ቍ
ൌ
ଵ
ଶగ
√
඲
െஶ
ஶ
𝑒െ௫మ/ଶ𝑑𝑥
ൌ
1
𝑋ൌ𝜇൅𝜎𝑍,
𝐸ሾ𝑋ሿൌ𝜇൅𝜎𝐸ሾ𝑍ሿൌ𝜇
Varሺ𝑋ሻൌ𝜎ଶVarሺ𝑍ሻൌ𝜎ଶ
Φ ሺ𝑥ሻ.
Φ ሺ𝑥ሻൌ
1
2𝜋
√
඲
െஶ
௫
𝑒െ௬మ/ଶ𝑑𝑦
Φ ሺ𝑥ሻ
𝑥
𝑥, Φ ሺ𝑥ሻ
327 of 848

Table 5.1 Area 
 Under the Standard Normal Curve to the Left of 
Φ ሺെ𝑥ሻൌ1 െΦ ሺ𝑥ሻ   െ∞൏𝑥൏∞
(4.1)
Φ ሺ𝑥ሻ
𝑋.
.00
.01
.02
.03
.04
.05
.06
.07
.08
.09
.0
.5000
.5040
.5080
.5120
.5160
.5199
.5239
.5279
.5319
.5359
.1
.5398
.5438
.5478
.5517
.5557
.5596
.5636
.5675
.5714
.5753
.2
.5793
.5832
.5871
.5910
.5948
.5987
.6026
.6064
.6103
.6141
.3
.6179
.6217
.6255
.6293
.6331
.6368
.6406
.6443
.6480
.6517
.4
.6554
.6591
.6628
.6664
.6700
.6736
.6772
.6808
.6844
.6879
.5
.6915
.6950
.6985
.7019
.7054
.7088
.7123
.7157
.7190
.7224
.6
.7257
.7291
.7324
.7357
.7389
.7422
.7454
.7486
.7517
.7549
.7
.7580
.7611
.7642
.7673
.7704
.7734
.7764
.7794
.7823
.7852
.8
.7881
.7910
.7939
.7967
.7995
.8023
.8051
.8078
.8106
.8133
.9
.8159
.8186
.8212
.8238
.8264
.8289
.8315
.8340
.8365
.8389
1.0
.8413
.8438
.8461
.8485
.8508
.8531
.8554
.8577
.8599
.8621
1.1
.8643
.8665
.8686
.8708
.8729
.8749
.8770
.8790
.8810
.8830
1.2
.8849
.8869
.8888
.8907
.8925
.8944
.8962
.8980
.8997
.9015
1.3
.9032
.9049
.9066
.9082
.9099
.9115
.9131
.9147
.9162
.9177
1.4
.9192
.9207
.9222
.9236
.9251
.9265
.9279
.9292
.9306
.9319
1.5
.9332
.9345
.9357
.9370
.9382
.9394
.9406
.9418
.9429
.9441
1.6
.9452
.9463
.9474
.9484
.9495
.9505
.9515
.9525
.9535
.9545
1.7
.9554
.9564
.9573
.9582
.9591
.9599
.9608
.9616
.9625
.9633
𝑋
328 of 848

The proof of Equation (4.1)
, which follows from the symmetry of the standard
normal density, is left as an exercise. This equation states that if  is a standard
normal random variable, then
Since 
 is a standard normal random variable whenever  is normally
distributed with parameters  and 
 it follows that the distribution function of  can
be expressed as
Example 4b
If  is a normal random variable with parameters 
 and 
 find (a)
 (b) 
 (c) 
Solution
.00
.01
.02
.03
.04
.05
.06
.07
.08
.09
1.8
.9641
.9649
.9656
.9664
.9671
.9678
.9686
.9693
.9699
.9706
1.9
.9713
.9719
.9726
.9732
.9738
.9744
.9750
.9756
.9761
.9767
2.0
.9772
.9778
.9783
.9788
.9793
.9798
.9803
.9808
.9812
.9817
2.1
.9821
.9826
.9830
.9834
.9838
.9842
.9846
.9850
.9854
.9857
2.2
.9861
.9864
.9868
.9871
.9875
.9878
.9881
.9884
.9887
.9890
2.3
.9893
.9896
.9898
.9901
.9904
.9906
.9909
.9911
.9913
.9916
2.4
.9918
.9920
.9922
.9925
.9927
.9929
.9931
.9932
.9934
.9936
2.5
.9938
.9940
.9941
.9943
.9945
.9946
.9948
.9949
.9951
.9952
2.6
.9953
.9955
.9956
.9957
.9959
.9960
.9961
.9962
.9963
.9964
𝑋
𝑍
𝑃ሼ𝑍൑െ𝑥ሽൌ𝑃ሼ𝑍൐𝑥ሽ   െ∞൏𝑥൏∞
𝑍ൌሺ𝑋െ𝜇ሻ/𝜎
𝑋
𝜇
𝜎ଶ,
𝑋
𝐹௑ሺ𝑎ሻൌ𝑃ሼ𝑋൑𝑎ሽൌ𝑃ቆ𝑋െ𝜇
𝜎
൑𝑎െ𝜇
𝜎
ቇൌΦ ൬𝑎െ𝜇
𝜎
൰
𝑋
𝜇ൌ3
𝜎ଶൌ9,
𝑃ሼ2 ൏𝑋൏5ሽ;
𝑃ሼ𝑋൐0ሽ;
𝑃ሼ||𝑋െ3|| ൐6ሽ.
329 of 848

a. 
b. 
c. 
Example 4c
An examination is frequently regarded as being good (in the sense of
determining a valid grade spread for those taking it) if the test scores of those
taking the examination can be approximated by a normal density function. (In
other words, a graph of the frequency of grade scores should have approximately
the bell-shaped form of the normal density.) The instructor often uses the test
scores to estimate the normal parameters  and 
 and then assigns the letter
grade A to those whose test score is greater than 
 B to those whose score
is between  and 
 C to those whose score is between 
 and 
 D to
those whose score is between 
 and 
 and F to those getting a score
below 
 (This strategy is sometimes referred to as grading “on the curve.”)
Since
𝑃ሼ2 ൏𝑋൏5ሽ
ൌ𝑃ቊ2 െ3
3
൏𝑋െ3
3
൏5 െ3
3
ቋ
ൌ𝑃ቊെ1
3 ൏𝑍൏2
3ቋ
ൌ
Φ ቆ2
3ቇെΦ ቆെ1
3ቇ
ൌ
Φ ቆ2
3ቇെቈ1 െΦ ቆ1
3ቇ቉
ൎ
.3779
𝑃ሼ𝑋൐0ሽ
ൌ
𝑃ቊ𝑋െ3
3
൐0 െ3
3
ቋൌ𝑃ሼ𝑍൐െ1ሽ
ൌ
1 െΦ ሺെ1ሻ
ൌ
Φ ሺ1ሻ
ൎ
.8413
𝑃ሼ||𝑋െ3|| ൐6ሽ
ൌ
𝑃ሼ𝑋൐9ሽ൅𝑃ሼ𝑋൏െ3ሽ
ൌ
𝑃ቊ𝑋െ3
3
൐9 െ3
3
ቋ൅𝑃ቊ𝑋െ3
3
൏െ3 െ3
3
ቋ
ൌ
𝑃ሼ𝑍൐2ሽ൅𝑃ሼ𝑍൏െ2ሽ
ൌ
1 െΦ ሺ2ሻ൅Φ ሺെ2ሻ
ൌ
2ሾ1 െΦ ሺ2ሻሿ
ൎ
.0456
𝜇
𝜎ଶ
𝜇൅𝜎,
𝜇
𝜇൅𝜎,
𝜇െ𝜎
𝜇,
𝜇െ2𝜎
𝜇െ𝜎,
𝜇െ2𝜎.
330 of 848

it follows that approximately 16 percent of the class will receive an A grade on
the examination, 34 percent a B grade, 34 percent a C grade, and 14 percent a D
grade; 2 percent will fail.
Example 4d
An expert witness in a paternity suit testifies that the length (in days) of human
gestation is approximately normally distributed with parameters 
 and
 The defendant in the suit is able to prove that he was out of the country
during a period that began 290 days before the birth of the child and ended 240
days before the birth. If the defendant was, in fact, the father of the child, what is
the probability that the mother could have had the very long or very short
gestation indicated by the testimony?
Solution
Let  denote the length of the gestation, and assume that the defendant is the
father. Then the probability that the birth could occur within the indicated period is
Example 4e
Suppose that a binary message either 0 or 1 must be transmitted by wire from
location  to location 
 However, the data sent over the wire are subject to a
channel noise disturbance, so, to reduce the possibility of error, the value 2 is
sent over the wire when the message is 1 and the value 
 is sent when the
message is 0. If 
 is the value sent at location 
 then 
 the value
𝑃ሼ𝑋൐𝜇൅𝜎ሽ
ൌ𝑃ቄ
௑െఓ
ఙ
൐1ቅൌ1 െΦ ሺ1ሻൎ .1587
𝑃ሼ𝜇൏𝑋൏𝜇൅𝜎ሽ
ൌ𝑃ቄ0 ൏
௑െఓ
ఙ
൏1ቅൌΦ ሺ1ሻെΦ ሺ0ሻൎ .3413
𝑃ሼ𝜇െ𝜎൏𝑋൏𝜇ሽ
ൌ𝑃ቄെ1 ൏
௑െఓ
ఙ
൏0ቅ
ൌΦ ሺ0ሻെΦ ሺെ1ሻൎ .3413
𝑃ሼ𝜇െ2𝜎൏𝑋൏𝜇െ𝜎ሽ
ൌ𝑃ቄെ2 ൏
௑െఓ
ఙ
൏െ1ቅ
ൌΦ ሺ2ሻെΦ ሺ1ሻൎ .1359
𝑃ሼ𝑋൏𝜇െ2𝜎ሽ
ൌ𝑃ቄ
௑െఓ
ఙ
൏െ2ቅൌΦ ሺെ2ሻൎ .0228
𝜇ൌ270
𝜎ଶൌ100.
𝑋
𝑃ሼ𝑋൐290 or 𝑋൏240ሽ
ൌ
𝑃ሼ𝑋൐290ሽ൅𝑃ሼ𝑋൏240ሽ
ൌ
𝑃ቄ
௑െଶ଻଴
ଵ଴
൐2ቅ൅𝑃ቄ
௑െଶ଻଴
ଵ଴
൏െ3ቅ
ൌ
1 െΦ ሺ2ሻ൅1 െΦ ሺ3ሻ
ൎ
.0241
𝐴
𝐵.
െ2
𝑥,𝑥ൌേ2,
𝐴,
𝑅,
331 of 848

received at location 
 is given by 
 where  is the channel noise
disturbance. When the message is received at location 
 the receiver decodes it
according to the following rule:
If 
 then 1 is concluded.
If 
 then 0 is concluded.
Because the channel noise is often normally distributed, we will determine the
error probabilities when  is a standard normal random variable.
Two types of errors can occur: One is that the message 1 can be incorrectly
determined to be 0, and the other is that 0 can be incorrectly determined to be 1.
The first type of error will occur if the message is 1 and 
 whereas the
second will occur if the message is 0 and 
 Hence,
and
Example 4f
Value at Risk (VAR) has become a key concept in financial calculations. The VAR
of an investment is defined as that value  such that there is only a  percent
chance that the loss from the investment will be greater than 
 If 
 the gain
from an investment, is a normal random variable with mean  and variance 
then because the loss is equal to the negative of the gain, the VAR of such an
investment is that value  such that
Using that 
 is normal with mean 
 and variance 
 we see that
Because, as indicated by Table 5.1
, 
 we see that
𝐵,
𝑅ൌ𝑥൅𝑁,
𝑁
𝐵,
𝑅൒.5,
𝑅൏.5,
𝑁
2 ൅𝑁൏.5,
െ2 ൅𝑁൒.5.
𝑃ሼerror|message is 1ሽ
ൌ𝑃ሼ𝑁൏െ1.5ሽ
ൌ1 െΦ ሺ1.5ሻ
ൎ
.0668
𝑃ሼerror|message is 0ሽ
ൌ
𝑃ሼ𝑁൒2.5ሽ
ൌ
1 െΦ ሺ2.5ሻൎ
.0062
𝑣
1
𝑣.
𝑋,
𝜇
𝜎ଶ,
𝑣
.01 ൌ𝑃ሼെ𝑋൐𝜈ሽ
െ𝑋
െ𝜇
𝜎ଶ,
.01
ൌ
𝑃ቄ
െ௑൅ఓ
ఙ
൐
ఔ൅ఓ
ఙቅ
ൌ
1 െΦ ቀ
௩൅ఓ
ఙቁ
Φ ሺ2.33ሻൌ.99,
332 of 848

That is,
Consequently, among a set of investments all of whose gains are normally
distributed, the investment having the smallest VAR is the one having the largest
value of 
An important result in probability theory known as the DeMoivre–Laplace limit
theorem states that when  is large, a binomial random variable with parameters 
and  will have approximately the same distribution as a normal random variable with
the same mean and variance as the binomial. This result was proved originally for
the special case of 
 by DeMoivre in 1733 and was then extended to general 
by Laplace in 1812. It formally states that if we “standardize” the binomial by first
subtracting its mean np and then dividing the result by its standard deviation
 then the distribution function of this standardized random variable
(which has mean 0 and variance 1) will converge to the standard normal distribution
function as 
The DeMoivre–Laplace Limit Theorem
If 
 denotes the number of successes that occur when  independent trials,
each resulting in a success with probability 
 are performed, then, for any
as 
Because the preceding theorem is only a special case of the central limit theorem,
which is presented in Chapter 8
, we shall not present a proof.
Note that we now have two possible approximations to binomial probabilities: the
Poisson approximation, which is good when  is large and  is small, and the normal
approximation, which can be shown to be quite good when 
 is large. (See
Figure 5.6
.) [The normal approximation will, in general, be quite good for values
𝜈൅𝜇
𝜎
ൌ2.33
𝜈ൌ𝑉𝐴𝑅ൌ2.33𝜎െ𝜇
𝜇െ2.33𝜎.
𝑛
𝑛
𝑝
𝑝ൌ1
2
𝑝
𝑛𝑝ሺ1 െ𝑝ሻ
ඥ
,
𝑛→∞.
𝑆௡
𝑛
𝑝,
𝑎൏𝑏,
𝑃൝𝑎൑
𝑆௡െ𝑛𝑝
𝑛𝑝ሺ1 െ𝑝ሻ
ඥ
൑𝑏ൡ→Φ ሺ𝑏ሻെΦ ሺ𝑎ሻ
𝑛→∞.
𝑛
𝑝
𝑛𝑝ሺ1 െ𝑝ሻ
333 of 848

of  satisfying 
]
Figure 5.6 The probability mass function of a binomial (n, p) random variable
becomes more and more “normal” as n becomes larger and larger.
Example 4g
Let  be the number of times that a fair coin that is flipped 40 times lands on
heads. Find the probability that 
 Use the normal approximation and then
compare it with the exact solution.
Solution
To employ the normal approximation, note that because the binomial is a discrete
integer-valued random variable, whereas the normal is a continuous random
variable, it is best to write 
 as 
 before applying
the normal approximation (this is called the continuity correction). Doing so gives
𝑛
𝑛𝑝ሺ1 െ𝑝ሻ൒10.
𝑋
𝑋ൌ20.
𝑃ሼ𝑋ൌ𝑖ሽ
𝑃ሼ𝑖െ1/2 ൏𝑋൏𝑖൅1/2ሽ
𝑃ሼ𝑋ൌ
20ሽൌ𝑃ሼ19.5 ൏𝑋൏20.5ሽ
ൌ
𝑃ቊ19.5 െ20
10
√
൏𝑋െ20
10
√
൏20.5 െ20
10
√
ቋ
ൎ
𝑃ቊെ.16 ൏𝑋െ20
10
√
൏.16ቋ
ൎ
Φ ሺ.16ሻെΦ ሺെ.16ሻൎ.1272
334 of 848

The exact result is
Example 4h
The ideal size of a first-year class at a particular college is 150 students. The
college, knowing from past experience that, on the average, only 30 percent of
those accepted for admission will actually attend, uses a policy of approving the
applications of 450 students. Compute the probability that more than 150 first-
year students attend this college.
Solution
If  denotes the number of students who attend, then  is a binomial random
variable with parameters 
 and 
 Using the continuity correction, we
see that the normal approximation yields
Hence, less than 6 percent of the time do more than 150 of the first 450 accepted
actually attend. (What independence assumptions have we made?)
Example 4i
To determine the effectiveness of a certain diet in reducing the amount of
cholesterol in the bloodstream, 100 people are put on the diet. After they have
been on the diet for a sufficient length of time, their cholesterol count will be
taken. The nutritionist running this experiment has decided to endorse the diet if
at least 65 percent of the people have a lower cholesterol count after going on
the diet. What is the probability that the nutritionist endorses the new diet if, in
fact, it has no effect on the cholesterol level?
Solution
Let us assume that if the diet has no effect on the cholesterol count, then, strictly
by chance, each person’s count will be lower than it was before the diet with
probability 
 Hence, if  is the number of people whose count is lowered, then
the probability that the nutritionist will endorse the diet when it actually has no
effect on the cholesterol count is
𝑃ሼ𝑋ൌ20ሽൌቆ40
20ቇቆ1
2ቇ
ସ଴
ൎ.1254
𝑋
𝑋
𝑛ൌ450
𝑝ൌ.3.
𝑃ሼ𝑋൒150.5ሽ
ൌ
𝑃൝𝑋െሺ450ሻሺ.3ሻ
450ሺ.3ሻሺ.7ሻ
ඥ
൒150.5 െሺ450ሻሺ.3ሻ
450ሺ.3ሻሺ.7ሻ
ඥ
ൡ
ൎ
1 െΦ ሺ1.59ሻ
ൎ
.0559
1
2 .
𝑋
335 of 848

Example 4j
Fifty-two percent of the residents of New York City are in favor of outlawing
cigarette smoking on university campuses. Approximate the probability that more
than 50 percent of a random sample of  people from New York are in favor of
this prohibition when
a. 
b. 
c. 
How large would  have to be to make this probability exceed .95?
Solution
Let  denote the number of residents of New York City. To answer the preceding
question, we must first understand that a random sample of size  is a sample
such that the  people were chosen in such a manner that each of the 
subsets of  people had the same chance of being the chosen subset.
Consequently, 
 the number of people in the sample who are in favor of the
smoking prohibition, is a hypergeometric random variable. That is, 
 has the
same distribution as the number of white balls obtained when  balls are chosen
from an urn of  balls, of which .52  are white. But because  and .52  are both
large in comparison with the sample size 
 it follows from the binomial
approximation to the hypergeometric (see Section 4.8.3
) that the distribution
of 
 is closely approximated by a binomial distribution with parameters  and
 The normal approximation to the binomial distribution then shows that
෍
௜ൌ଺ହ
ଵ଴଴
ቆ100
𝑖ቇቆ1
2ቇ
ଵ଴଴
ൌ𝑃ሼ𝑋||  ൒64.5ሽ
ൌ𝑃
⎧
⎨
⎩
⎪
⎪
⎪
⎪
𝑋െሺ100ሻቆ1
2ቇ
100ቆ1
2ቇቆ1
2ቇ
ඨ
൒2.9
⎫
⎬
⎭
⎪
⎪
⎪
⎪
ൎ1 െΦ ሺ2.9ሻ
ൎ.0019
𝑛
𝑛ൌ11
𝑛ൌ101
𝑛ൌ1001
𝑛
𝑁
𝑛
𝑛
ቆ𝑁
𝑛ቇ
𝑛
𝑆௡,
𝑆௡
𝑛
𝑁
𝑁
𝑁
𝑁
𝑛,
𝑆௡
𝑛
𝑝ൌ.52.
336 of 848

Thus,
In order for this probability to be at least .95, we would need 
Because 
 is an increasing function and 
 this means that
or
That is, the sample size would have to be at least 1692.
Historical notes concerning the normal distribution
The normal distribution was introduced by the French mathematician Abraham
DeMoivre in 1733. DeMoivre, who used this distribution to approximate
probabilities connected with coin tossing, called it the exponential bell-shaped
curve. Its usefulness, however, became truly apparent only in 1809, when the
famous German mathematician Karl Friedrich Gauss used it as an integral part
of his approach to predicting the location of astronomical entities. As a result, it
became common after this time to call it the Gaussian distribution.
During the mid- to late 19th century, however, most statisticians started to
believe that the majority of data sets would have histograms conforming to the
Gaussian bell-shaped form. Indeed, it came to be accepted that it was “normal”
for any well-behaved data set to follow this curve. As a result, following the lead
of the British statistician Karl Pearson, people began referring to the Gaussian
curve by calling it simply the normal curve. (A partial explanation as to why so
many data sets conform to the normal curve is provided by the central limit
theorem, which is presented in Chapter 8
.)
Abraham DeMoivre (1667–1754)
𝑃ሼ𝑆௡൐.5𝑛ሽ
ൌ𝑃൝𝑆௡െ.52𝑛
𝑛ሺ.52ሻሺ.48ሻ
ඥ
൐
.5𝑛െ.52𝑛
𝑛ሺ.52ሻሺ.48ሻ
ඥ
ൡ
ൌ𝑃൝𝑆௡െ.52𝑛
𝑛ሺ.52ሻሺ.48ሻ
ඥ
൐.04 𝑛
√ൡ
ൎΦ ሺ.04 𝑛
√ሻ
𝑃ሼ𝑆௡൐.5𝑛ሽൎ
⎧
⎨
⎩
⎪
⎪
Φ ሺ.1328ሻൌ.5528,
𝑖𝑓 𝑛ൌ11
Φ ሺ.4020ሻൌ.6562,
𝑖𝑓 𝑛ൌ101
Φ ሺ1.2665ሻൌ.8973, 𝑖𝑓 𝑛ൌ1001
Φ ሺ.04 𝑛
√ሻ൐.95.
Φ ሺ𝑥ሻ
Φ ሺ1.645ሻൌ.95,
.04 𝑛
√
൐1.645
𝑛൒1691.266
337 of 848

Today there is no shortage of statistical consultants, many of whom ply their
trade in the most elegant of settings. However, the first of their breed worked, in
the early years of the 18th century, out of a dark, grubby betting shop in Long
Acres, London, known as Slaughter’s Coffee House. He was Abraham
DeMoivre, a Protestant refugee from Catholic France, and, for a price, he would
compute the probability of gambling bets in all types of games of chance.
Although DeMoivre, the discoverer of the normal curve, made his living at the
coffee shop, he was a mathematician of recognized abilities. Indeed, he was a
member of the Royal Society and was reported to be an intimate of Isaac
Newton.
Listen to Karl Pearson imagining DeMoivre at work at Slaughter’s Coffee
House: “I picture DeMoivre working at a dirty table in the coffee house with a
broken-down gambler beside him and Isaac Newton walking through the crowd
to his corner to fetch out his friend. It would make a great picture for an inspired
artist.”
Karl Friedrich Gauss
Karl Friedrich Gauss (1777–1855), one of the earliest users of the normal
curve, was one of the greatest mathematicians of all time. Listen to the words
of the well-known mathematical historian E. T. Bell, as expressed in his 1954
book Men of Mathematics: In a chapter entitled “The Prince of Mathematicians,”
he writes, “Archimedes, Newton, and Gauss; these three are in a class by
themselves among the great mathematicians, and it is not for ordinary mortals
to attempt to rank them in order of merit. All three started tidal waves in both
pure and applied mathematics. Archimedes esteemed his pure mathematics
more highly than its applications;
Newton appears to have found the chief justification for his mathematical
inventions in the scientific uses to which he put them; while Gauss declared it
was all one to him whether he worked on the pure or on the applied side.”
A continuous random variable whose probability density function is given, for some
 by
𝜆൐0,
𝑓ሺ𝑥ሻൌቊ𝜆𝑒െఒ௫
𝑖𝑓 𝑥൒0
0
𝑖𝑓 𝑥൏0
338 of 848

is said to be an exponential random variable (or, more simply, is said to be
exponentially distributed) with parameter 
 The cumulative distribution function 
of an exponential random variable is given by
Note that 
 as, of course, it must. The parameter  will now
be shown to equal the reciprocal of the expected value.
Example 5a
Let  be an exponential random variable with parameter 
 Calculate (a) [ ]
and (b) Var
Solution
(a) Since the density function is given by
we obtain, for 
Integrating by parts (with 
 and 
) yields
Letting 
 and then 
 gives
𝜆.
𝐹ሺ𝑎ሻ
𝐹ሺ𝑎ሻ
ൌ𝑃ሼ𝑋൑𝑎ሽ
ൌ׬଴
௔𝜆𝑒െఒ௫𝑑𝑥
ൌെ𝑒െఒ௫ห଴
௔
ൌ1 െ𝑒െఒ௔ 𝑎൒0
𝐹ሺ∞ሻൌ඲
଴
ஶ
𝜆𝑒െఒ௫𝑑𝑥ൌ1,
𝜆
𝑋
𝜆.
𝐸𝑋
ሺ𝑋ሻ.
𝑓ሺ𝑥ሻൌቊ𝜆𝑒െఒ௫𝑥൒0
0
𝑥൏0
𝑛൐0,
𝐸ሾ𝑋௡ሿൌ඲
଴
ஶ
𝑥௡𝜆𝑒െఒ௫𝑑𝑥
𝜆𝑒െఒ௫ൌ𝑑𝑣
𝑢ൌ𝑥௡
𝐸ሾ𝑋௡ሿ
ൌെ𝑥௡𝑒െఒ௫ห଴
ஶ൅׬଴
ஶ𝑒െఒ௫𝑛𝑥௡െଵ𝑑𝑥
ൌ0 ൅
௡
ఒ׬଴
ஶ𝜆𝑒െఒ௫𝑥௡െଵ𝑑𝑥
ൌ
௡
ఒ𝐸ሾ𝑋௡െଵሿ
𝑛ൌ1
𝑛ൌ2
𝐸ሾ𝑋ሿൌ
ଵ
ఒ
𝐸ሾ𝑋ଶሿൌ
ଶ
ఒ𝐸ሾ𝑋ሿൌ
ଶ
ఒమ
339 of 848

(b) Hence,
Thus, the mean of the exponential is the reciprocal of its parameter 
 and the
variance is the mean squared.
In practice, the exponential distribution often arises as the distribution of the amount
of time until some specific event occurs. For instance, the amount of time (starting
from now) until an earthquake occurs, or until a new war breaks out, or until a
telephone call you receive turns out to be a wrong number are all random variables
that tend in practice to have exponential distributions. (For a theoretical explanation
of this phenomenon, see Section 4.7
.)
Example 5b
Suppose that the length of a phone call in minutes is an exponential random
variable with parameter 
 If someone arrives immediately ahead of you at
a public telephone booth, find the probability that you will have to wait
a. more than 10 minutes;
b. between 10 and 20 minutes.
Solution
Let  denote the length of the call made by the person in the booth. Then the
desired probabilities are
a. 
b. 
We say that a nonnegative random variable  is memoryless if
If we think of  as being the lifetime of some instrument, Equation (5.1)
 states
that the probability that the instrument survives for at least 
 hours, given that it
has survived  hours, is the same as the initial probability that it survives for at least 
Varሺ𝑋ሻൌ2
𝜆2 െቆ1
𝜆ቇ
ଶ
ൌ1
𝜆ଶ
𝜆,
𝜆ൌ1
10 .
𝑋
𝑃ሼ𝑋൐10ሽ
ൌ1 െ𝐹ሺ10ሻ
ൌ𝑒െଵൎ.368
𝑃ሼ10 ൏𝑋൏20ሽ
ൌ𝐹ሺ20ሻെ𝐹ሺ10ሻ
ൌ𝑒െଵെ𝑒െଶൎ.233
𝑋
𝑃ሼ𝑋൐𝑠൅𝑡 || 𝑋൐𝑡ሽൌ𝑃ሼ𝑋൐𝑠ሽ for all 𝑠, 𝑡  ൒ 0
(5.1)
𝑋
𝑠൅𝑡
𝑡
𝑠
340 of 848

hours. In other words, if the instrument is alive at age  the distribution of the
remaining amount of time that it survives is the same as the original lifetime
distribution. (That is, it is as if the instrument does not “remember” that it has already
been in use for a time 
)
Equation (5.1)
 is equivalent to
or
Since Equation (5.2)
 is satisfied when  is exponentially distributed (for
), it follows that exponentially distributed random variables are
memoryless.
Example 5c
Consider a post office that is staffed by two clerks. Suppose that when Mr. Smith
enters the system, he discovers that Ms. Jones is being served by one of the
clerks and Mr. Brown by the other. Suppose also that Mr. Smith is told that his
service will begin as soon as either Ms. Jones or Mr. Brown leaves. If the amount
of time that a clerk spends with a customer is exponentially distributed with
parameter 
 what is the probability that of the three customers, Mr. Smith is the
last to leave the post office?
Solution
The answer is obtained by reasoning as follows: Consider the time at which Mr.
Smith first finds a free clerk. At this point, either Ms. Jones or Mr. Brown would
have just left, and the other one would still be in service. However, because the
exponential is memoryless, it follows that the additional amount of time that this
other person (either Ms. Jones or Mr. Brown) would still have to spend in the post
office is exponentially distributed with parameter 
 That is, it is the same as if
service for that person were just starting at this point. Hence, by symmetry, the
probability that the remaining person finishes before Smith leaves must equal 
It turns out that not only is the exponential distribution memoryless, but it is also the
unique distribution possessing this property. To see this, suppose that  is
memoryless and let 
 Then, by Equation (5.2)
,
𝑡,
𝑡.
𝑃ሼ𝑋൐𝑠൅𝑡,𝑋൐𝑡ሽ
𝑃ሼ𝑋൐𝑡ሽ
ൌ𝑃ሼ𝑋൐𝑠ሽ
𝑃ሼ𝑋൐𝑠൅𝑡ሽൌ𝑃ሼ𝑋൐𝑠ሽ𝑃ሼ𝑋൐𝑡ሽ
(5.2)
𝑋
𝑒െఒሺ௦൅௧ሻൌ𝑒െఒ௦𝑒െఒ௧
𝜆,
𝜆.
1
2 .
𝑋
𝐹̅ ̅̅ ̅ሺ𝑥ሻൌ𝑃ሼ𝑋൐𝑥ሽ.
341 of 848

That is, 
 satisfies the functional equation
However, it turns out that the only right continuous solution of this functional equation
is
†One can prove Equation (5.3)
 as follows: If 
then
and repeating this yields 
 Also,
Hence, 
 which, since  is right continuous,
implies that 
 Because 
 we obtain
 where 
and, since a distribution function is always right continuous, we must have
which shows that  is exponentially distributed.
Example 5d
Suppose that the number of miles that a car can run before its battery wears out
is exponentially distributed with an average value of 10,000 miles. If a person
desires to take a 5000-mile trip, what is the probability that he or she will be able
to complete the trip without having to replace the car battery? What can be said
𝐹̅ ̅̅ ̅ሺ𝑠൅𝑡ሻൌ𝐹̅ ̅̅ ̅ሺ𝑠ሻ𝐹̅ ̅̅ ̅ሺ𝑡ሻ
𝐹̅ ̅̅ ̅ሺ⋅ሻ
𝑔ሺ𝑠൅𝑡ሻൌ𝑔ሺ𝑠ሻ𝑔ሺ𝑡ሻ
†
𝑔ሺ𝑠൅𝑡ሻൌ𝑔ሺ𝑠ሻ𝑔ሺ𝑡ሻ,
𝑔ቆ2
𝑛ቇൌ𝑔ቆ1
𝑛൅1
𝑛ቇൌ𝑔ଶቆ1
𝑛ቇ
𝑔ሺ𝑚/𝑛ሻൌ𝑔௠ሺ1/𝑛ሻ.
𝑔ሺ1ሻൌ𝑔ቆ1
𝑛൅1
𝑛൅⋯൅1
𝑛ቇൌ𝑔௡ቆ1
𝑛ቇ 𝑜𝑟 𝑔ቆ1
𝑛ቇൌሺ𝑔ሺ1ሻሻଵ/௡
𝑔ሺ𝑚/𝑛ሻൌሺ𝑔ሺ1ሻሻ௠/௡,
𝑔
𝑔ሺ𝑥ሻൌሺ𝑔ሺ1ሻሻ௫.
𝑔ሺ1ሻൌቆ𝑔ቆ1
2ቇቇ
ଶ
൒0,
𝑔ሺ𝑥ሻൌ𝑒െఒ௫,
𝜆ൌെlogሺ𝑔ሺ1ሻሻ.
𝑔ሺ𝑥ሻൌ𝑒െఒ௫
(5.3)
𝐹̅ ̅̅ ̅ሺ𝑥ሻൌ𝑒െఒ௫ or 𝐹ሺ𝑥ሻൌ𝑃ሼ𝑋൑𝑥ሽൌ1 െ𝑒െఒ௫
𝑋
342 of 848

when the distribution is not exponential?
Solution
It follows by the memoryless property of the exponential distribution that the
remaining lifetime (in thousands of miles) of the battery is exponential with
parameter 
 Hence, the desired probability is
However, if the lifetime distribution F is not exponential, then the relevant
probability is
where  is the number of miles that the battery had been in use prior to the start
of the trip. Therefore, if the distribution is not exponential, additional information
is needed (namely, the value of ) before the desired probability can be
calculated.
A variation of the exponential distribution is the distribution of a random variable that
is equally likely to be either positive or negative and whose absolute value is
exponentially distributed with parameter 
 Such a random variable is said to
have a Laplace distribution,  and its density is given by
‡ It also is sometimes called the double exponential random variable.
Its distribution function is given by
𝜆ൌ1
10 .
𝑃ሼremaining lifetime  ൐ 5ሽൌ1 െ𝐹ሺ5ሻൌ𝑒െହఒൌ𝑒െଵ/ଶൎ.607
𝑃ሼlifetime ൐𝑡൅5|lifetime ൐𝑡ሽൌ1 െ𝐹ሺ𝑡൅5ሻ
1 െ𝐹ሺ𝑡ሻ
𝑡
𝑡
𝜆,𝜆˙ ൒0.
‡
𝑓ሺ𝑥ሻൌ1
2 𝜆𝑒െఒห௫ห  െ∞൏𝑥൏∞
343 of 848

Example 5e
Consider again Example 4e
, which supposes that a binary message is to be
transmitted from  to 
 with the value 2 being sent when the message is 1 and
 when it is 0. However, suppose now that rather than being a standard normal
random variable, the channel noise  is a Laplacian random variable with
parameter 
 Suppose again that if  is the value received at location 
 then
the message is decoded as follows:
If 
 then 1 is concluded.
If 
 then 0 is concluded.
In this case, where the noise is Laplacian with parameter 
 the two types of
errors will have probabilities given by
On comparing this with the results of Example 4e
, we see that the error
probabilities are higher when the noise is Laplacian with 
 than when it is a
standard normal variable.
𝐹ሺ𝑥ሻ
ൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
ଵ
ଶ
െஶ
௫
𝜆𝑒ఒ௬𝑑𝑦
𝑥൏0
ଵ
ଶ
െஶ
଴
𝜆𝑒ఒ௬𝑑𝑦൅
ଵ
ଶ
଴
௫
𝜆𝑒െఒ௬𝑑𝑦𝑥൐0
ൌ൞
ଵ
ଶ𝑒ఒ௫
𝑥൏0
1 െ
ଵ
ଶ𝑒െఒ௫𝑥൐0
𝐴
𝐵,
െ2
𝑁
𝜆ൌ1.
𝑅
𝐵,
𝑅൒.5,
𝑅൏.5,
𝜆ൌ1,
𝑃ሼerror||message 1 is sentሽ
ൌ𝑃ሼ𝑁൏െ1.5ሽ
ൌ
ଵ
ଶ𝑒െଵ.ହ
ൎ.1116
𝑃ሼerror||message 0 is sentሽ
ൌ𝑃ሼ𝑁൒2.5ሽ
ൌ
ଵ
ଶ𝑒െଶ.ହ
ൎ.041
𝜆ൌ1
344 of 848

Consider a positive continuous random variable  that we interpret as being the
lifetime of some item. Let  have distribution function  and density 
 The hazard
rate (sometimes called the failure rate) function 
 of  is defined by
To interpret 
 suppose that the item has survived for a time  and we desire the
probability that it will not survive for an additional time dt. That is, consider
 Now,
Thus, 
 represents the conditional probability intensity that a -unit-old item will
fail.
Suppose now that the lifetime distribution is exponential. Then, by the memoryless
property, it follows that the distribution of remaining life for a -year-old item is the
same as that for a new item. Hence, 
 should be constant. In fact, this checks out,
since
Thus, the failure rate function for the exponential distribution is constant. The
parameter  is often referred to as the rate of the distribution.
It turns out that the failure rate function 
 uniquely determines the
distribution function 
 To prove this, we integrate 
 from  to  to obtain
𝑋
𝑋
𝐹
𝑓.
𝜆ሺ𝑡ሻ
𝐹
𝜆ሺ𝑡ሻൌ𝑓ሺ𝑡ሻ
𝐹̅ ̅̅ ̅ሺ𝑡ሻ, where 𝐹̅ ̅̅ ̅  ൌ 1  െ 𝐹
𝜆ሺ𝑡ሻ,
𝑡
𝑃ሼ𝑋∈ሺ𝑡,𝑡൅𝑑𝑡ሻ||𝑋൐𝑡ሽ.
𝑃ሼ𝑋∈ሺ𝑡, 𝑡൅𝑑𝑡ሻ|𝑋൐𝑡ሽ
ൌ
௉൛௑∈ሺ௧,௧൅ௗ௧ሻ,௑வ௧ൟ
௉൛௑வ௧ൟ
ൌ
௉൛௑∈ሺ௧,௧൅ௗ௧ሻൟ
௉൛௑வ௧ൟ
ൎ
௙ሺ௧ሻ
ி̅ ̅̅ ̅ሺ௧ሻ𝑑𝑡
𝜆ሺ𝑡ሻ
𝑡
𝑡
𝜆ሺ𝑡ሻ
𝜆ሺ𝑡ሻ
ൌ
௙ሺ௧ሻ
ி̅ ̅̅ ̅ሺ௧ሻ
ൌ
ఒ௘െഊ೟
௘െഊ೟
ൌ𝜆
𝜆
𝜆ሺ𝑠ሻ, 𝑠൒0,
𝐹.
𝜆ሺ𝑠ሻ
0
𝑡
345 of 848

where the second equality used that 
 and the final equality used that
 Solving the preceding equation for 
 gives
Hence, a distribution function of a positive continuous random variable can be
specified by giving its hazard rate function. For instance, if a random variable has a
linear hazard rate function—that is, if
then its distribution function is given by
and differentiation yields its density, namely,
When 
 the preceding equation is known as the Rayleigh density function.
Example 5f
One often hears that the death rate of a person who smokes is, at each age,
twice that of a nonsmoker. What does this mean? Does it mean that a
nonsmoker has twice the probability of surviving a given number of years as
does a smoker of the same age?
Solution
If 
 denotes the hazard rate of a smoker of age  and 
 that of a
nonsmoker of age  then the statement at issue is equivalent to the statement
଴
௧
𝜆ሺ𝑠ሻ𝑑𝑠
ൌ
଴
௧
௙ሺ௦ሻ
ଵെிሺ௦ሻ𝑑𝑠
ൌെlogሺ1 െ𝐹ሺ𝑠ሻሻ||଴
௧
ൌെlogሺ1 െ𝐹ሺ𝑡ሻሻ൅logሺ1 െ𝐹ሺ0ሻሻ
ൌെlogሺ1 െ𝐹ሺ𝑡ሻሻ
𝑓ሺ𝑠ሻൌ
ௗ
ௗ௦𝐹ሺ𝑠ሻ
𝐹ሺ0ሻൌ0.
𝐹ሺ𝑡ሻ
𝐹ሺ𝑡ሻൌ1 െexpቐെ඲
଴
௧
𝜆ሺ𝑠ሻ 𝑑𝑠ቑ
(5.4)
𝜆ሺ𝑡ሻൌ𝑎൅𝑏𝑡
𝐹ሺ𝑡ሻൌ1 െ𝑒െ௔௧െ௕௧మ/ଶ
𝑓ሺ𝑡ሻൌሺ𝑎൅𝑏𝑡ሻ𝑒െሺ௔௧൅௕௧మ/ଶሻ 𝑡൒0
𝑎ൌ0,
𝜆௦ሺ𝑡ሻ
𝑡
𝜆௡ሺ𝑡ሻ
𝑡,
346 of 848

that
The probability that an -year-old nonsmoker will survive until age 
 is
whereas the corresponding probability for a smoker is, by the same reasoning,
In other words, for two people of the same age, one of whom is a smoker and the
other a nonsmoker, the probability that the smoker survives to any given age is
the square (not one-half) of the corresponding probability for a nonsmoker. For
instance, if 
 then the probability that a 50-year-old
nonsmoker reaches age 60 is 
 whereas the corresponding
probability for a smoker is 
Equation (5.4)
 can be used to show that only exponential random variables are
memoryless. For if a random variable has a memoryless distribution then the
remaining life of a  year old must be the same for all 
 That is, if  is memoryless,
then 
 But, by Equation (5.4)
, this implies that the distribution function of
 is 
 showing that  is exponential with rate 
𝜆௦ሺ𝑡ሻൌ2𝜆௡ሺ𝑡ሻ
𝐴
𝐵, 𝐴൏𝐵,
𝑃ሼ𝐴-year-old nonsmoker reaches age 𝐵ሽ
  ൌ𝑃ሼnonsmoker's lifetime  ൐ 𝐵||nonsmoker's  lifetime  ൐ 𝐴ሽ
  ൌ1 െ𝐹nonሺ𝐵ሻ
1 െ𝐹nonሺ𝐴ሻ
  ൌ
exp𝑦ቄെ׬଴
஻𝜆௡ሺ𝑡ሻ 𝑑𝑡ቅ
expቄെ׬଴
஺𝜆௡ሺ𝑡ሻ 𝑑𝑡ቅ
  ൌexpቄെ׬஺
஻𝜆௡ሺ𝑡ሻ 𝑑𝑡ቅ
from (5.4)
𝑃ሼ𝐴-year-old smoker reaches age 𝐵ሽ
ൌexp
⎧
⎨
⎩
⎪
⎪
െ
஺
஻
𝜆௦ሺ𝑡ሻ 𝑑𝑡
⎫
⎬
⎭
⎪
⎪
ൌexpቄെ2׬஺
஻𝜆௡ሺ𝑡ሻ 𝑑𝑡ቅ
ൌ
⎡
⎣
⎢
⎢
⎢
⎢
exp
⎧
⎨
⎩
⎪
⎪
െ
஺
஻
𝜆௡ሺ𝑡ሻ 𝑑𝑡
⎫
⎬
⎭
⎪
⎪
⎤
⎦
⎥
⎥
⎥
⎥
ଶ
𝜆௡ሺ𝑡ሻൌ1
30 , 50 ൑𝑡൑60,
𝑒െଵ/ଷൎ.7165,
𝑒െଶ/ଷൎ.5134.
𝑠
𝑠.
𝑋
𝜆ሺ𝑠ሻൌ𝑐.
𝑋
𝐹ሺ𝑡ሻൌ1 െ𝑒െ௖௧,
𝑋
𝑐.
347 of 848

A random variable is said to have a gamma distribution with parameters 
 if its density function is given by
where 
 called the gamma function, is defined as
Integration of 
 by parts yields
For integral values of 
 say, 
 we obtain, by applying Equation (6.1)
repeatedly,
Since 
 it follows that, for integral values of 
ሺ𝛼,𝜆ሻ, 𝜆൐0,
𝛼൐0,
𝑓ሺ𝑥ሻൌቐ
ఒ௘െഊೣሺఒ௫ሻഀെభ
୻ሺఈሻ
𝑥൒0
0
𝑥൏0
𝛤ሺ𝛼ሻ,
Γ ሺ𝛼ሻൌ඲
଴
ஶ
𝑒െ௬𝑦ఈെଵ𝑑𝑦
Γ ሺ𝛼ሻ
Γ ሺ𝛼ሻ
ൌെ𝑒െ௬𝑦ఈെଵอ
଴
ஶ
൅
଴
ஶ
𝑒െ௬ሺ𝛼െ1ሻ𝑦ఈെଶ𝑑𝑦
ൌሺ𝛼െ1ሻ
଴
ஶ
𝑒െ௬𝑦ఈെଶ𝑑𝑦
ൌሺ𝛼െ1ሻΓ ሺ𝛼െ1ሻ
(6.1)
𝛼,
𝛼ൌ𝑛,
Γ ሺ𝑛ሻ
ൌሺ𝑛െ1ሻΓ ሺ𝑛െ1ሻ
ൌሺ𝑛െ1ሻሺ𝑛െ2ሻΓ ሺ𝑛െ2ሻ
ൌ⋯
ൌሺ𝑛െ1ሻሺ𝑛െ2ሻ⋯3 ⋅2 Γ ሺ1ሻ
Γ ሺ1ሻൌ඲
଴
ஶ
𝑒െ௫𝑑𝑥ൌ1,
𝑛,
Γ ሺ𝑛ሻൌሺ𝑛െ1ሻ!
348 of 848

When  is a positive integer, say, 
 the gamma distribution with parameters
 often arises, in practice as the distribution of the amount of time one has to
wait until a total of  events has occurred. More specifically, if events are occurring
randomly and in accordance with the three axioms of Section 4.7
, then it turns
out that the amount of time one has to wait until a total of  events has occurred will
be a gamma random variable with parameters 
 To prove this, let 
 denote the
time at which the th event occurs, and note that 
 is less than or equal to  if and
only if the number of events that have occurred by time  is at least 
 That is, with
 equal to the number of events in [0, ],
where the final identity follows because the number of events in [0, ] has a Poisson
distribution with parameter 
 Differentiation of the preceding now yields the density
function of 
Hence, 
 has the gamma distribution with parameters 
 (This distribution is
often referred to in the literature as the n-Erlang distribution.) Note that when 
this distribution reduces to the exponential distribution.
The gamma distribution with 
 and 
 a positive integer, is called the 
(read “chi-squared”) distribution with  degrees of freedom. The chi-squared
distribution often arises in practice as the distribution of the error involved in
attempting to hit a target in -dimensional space when each coordinate error is
normally distributed. This distribution will be studied in Chapter 6
, where its
relation to the normal distribution is detailed.
Example 6a
𝛼
𝛼ൌ𝑛,
ሺ𝛼, 𝜆ሻ
𝑛
𝑛
ሺ𝑛, 𝜆ሻ.
𝑇௡
𝑛
𝑇௡
𝑡
𝑡
𝑛.
𝑁ሺ𝑡ሻ
𝑡
𝑃ሼ𝑇௡൑𝑡ሽ
ൌ𝑃ሼ𝑁ሺ𝑡ሻ൒𝑛ሽ
ൌ
෍
௝ൌ௡
ஶ
𝑃ሼ𝑁ሺ𝑡ሻൌ𝑗ሽ
ൌ
෍
௝ൌ௡
ஶ
𝑒െఒ௧ሺ𝜆𝑡ሻ௝
𝑗!
𝑡
𝜆𝑡.
𝑇௡:
𝑓ሺ𝑡ሻ
ൌ
෍
௝ൌ௡
ஶ
𝑒െఒ௧𝑗ሺ𝜆𝑡ሻ௝െଵ𝜆
𝑗!
െ
෍
௝ൌ௡
ஶ
𝜆𝑒െఒ௧ሺ𝜆𝑡ሻ௝
𝑗!
ൌ
෍
௝ൌ௡
ஶ
𝜆𝑒െఒ௧ሺ𝜆𝑡ሻ௝െଵ
ሺ𝑗െ1ሻ!
െ
෍
௝ൌ௡
ஶ
𝜆𝑒െఒ௧ሺ𝜆𝑡ሻ௝
𝑗!
ൌ
ఒ௘െഊ೟ሺఒ௧ሻ೙െభ
ሺ௡െଵሻ!
𝑇௡
ሺ𝑛,𝜆ሻ.
𝑛ൌ1,
𝜆ൌ1
2
𝛼ൌ𝑛/2, 𝑛
𝜒௡
ଶ
𝑛
𝑛
349 of 848

Let  be a gamma random variable with parameters  and 
 Calculate (a) [ ]
and (b) Var
Solution
a. 
b. By first calculating 
 we can show that
The details are left as an exercise.
The Weibull distribution is widely used in engineering practice due to its versatility. It
was originally proposed for the interpretation of fatigue data, but now its use has
been extended to many other engineering problems. In particular, it is widely used in
the field of life phenomena as the distribution of the lifetime of some object,
especially when the “weakest link” model is appropriate for the object. That is,
consider an object consisting of many parts, and suppose that the object
experiences death (failure) when any of its parts fails. It has been shown (both
theoretically and empirically) that under these conditions, a Weibull distribution
provides a close approximation to the distribution of the lifetime of the item.
The Weibull distribution function has the form
A random variable whose cumulative distribution function is given by Equation
(6.2)
 is said to be a Weibull random variable with parameters 
 and 
Differentiation yields the density:
𝑋
𝛼
𝜆.
𝐸𝑋
ሺ𝑋ሻ.
𝐸ሾ𝑋ሿ
ൌ
ଵ
୻ሺఈሻ׬଴
ஶ𝜆𝑥𝑒െఒ௫ሺ𝜆𝑥ሻఈെଵ𝑑𝑥
ൌ
ଵ
ఒ୻ሺఈሻ׬଴
ஶ𝜆𝑒െఒ௫ሺ𝜆𝑥ሻఈ𝑑𝑥
ൌ
୻ሺఈ൅ଵሻ
ఒ୻ሺఈሻ
ൌ
ఈ
ఒ by Equation ሺ6.1ሻ
𝐸ሾ𝑋ଶሿ,
Varሺ𝑋ሻൌ𝛼
𝜆ଶ
𝐹ሺ𝑥ሻൌቐ
0
𝑥൑𝜈
1 െexp൜െቀ
௫െఔ
ఈቁ
ఉ
ൠ
𝑥൐𝜈
(6.2)
𝜈, 𝛼,
𝛽.
350 of 848

A random variable is said to have a Cauchy distribution with parameter 
 if its density is given by
Example 6b
Suppose that a narrow-beam flashlight is spun around its center, which is located
a unit distance from the -axis. (See figure 5.7
.) Consider the point  at which
the beam intersects the -axis when the flashlight has stopped spinning. (If the
beam is not pointing toward the -axis, repeat the experiment.)
Figure 5.7
As indicated in Figure 5.7
, the point  is determined by the angle  between
the flashlight and the -axis, which, from the physical situation, appears to be
uniformly distributed between 
 and 
 The distribution function of  is
thus given by
where the last equality follows since 
 being uniform over 
 has
distribution
𝑓ሺ𝑥ሻൌቐ
0
𝑥൑𝜈
ఉ
ఈቀ
௫െఔ
ఈቁ
ఉെଵ
exp൜െቀ
௫െఔ
ఈቁ
ఉ
ൠ
𝑥൐𝜈
𝜃,
െ∞൏𝜃൏∞,
𝑓ሺ𝑥ሻൌ1
𝜋
1
1 ൅ሺ𝑥െ𝜃ሻଶ  െ∞൏𝑥൏∞
𝑥
𝑋
𝑥
𝑥
𝑋
𝜃
𝑦
െ𝜋/2
𝜋/2.
𝑋
𝐹ሺ𝑥ሻ
ൌ𝑃ሼ𝑋൑𝑥ሽ
ൌ𝑃ሼtan 𝜃൑𝑥ሽ
ൌ𝑃ሼ𝜃൑tanെଵ𝑥ሽ
ൌ
ଵ
ଶ൅
ଵ
గtanെଵ𝑥
𝜃,
ሺെ𝜋/2, 𝜋/2ሻ,
351 of 848

Hence, the density function of  is given by
and we see that  has the Cauchy distribution.
‡ That 
 can be seen as follows: If
 then 
 so
or
A random variable is said to have a beta distribution if its density is given by
where
The beta distribution can be used to model a random phenomenon whose set of
possible values is some finite interval [
]—which, by letting  denote the origin and
taking 
 as a unit measurement, can be transformed into the interval [0, 1].
When 
 the beta density is symmetric about 
 giving more and more weight to
𝑃ሼ𝜃൑𝑎ሽൌ𝑎െሺെ𝜋/2ሻ
𝜋
ൌ1
2 ൅𝑎
𝜋   െ𝜋
2 ൏𝑎൏𝜋
2
𝑋
𝑓ሺ𝑥ሻൌ𝑑
𝑑𝑥𝐹ሺ𝑥ሻൌ
1
𝜋ሺ1 ൅𝑥ଶሻ   െ∞൏𝑥൏∞
𝑋
‡
ௗ
ௗ௫ሺ
െଵ𝑥ሻൌ1/ሺ1 ൅𝑥ଶሻ
𝑦ൌ
െଵ𝑥,
tan 𝑦ൌ𝑥,
1 ൌ𝑑
𝑑𝑥ሺtan 𝑦ሻൌ𝑑
𝑑𝑦ሺtan 𝑦ሻ𝑑𝑦
𝑑𝑥ൌ𝑑
𝑑𝑦ቆsin 𝑦
cos 𝑦ቇ𝑑𝑦
𝑑𝑥ൌ൭
ଶ𝑦൅
ଶ𝑦
ଶ𝑦
൱𝑑𝑦
𝑑𝑥
𝑑𝑦
𝑑𝑥ൌ
ଶ𝑦
ଶ𝑦൅
ଶ𝑦
ൌ
1
ଶ𝑦൅1
ൌ
1
𝑥ଶ൅1
𝑓ሺ𝑥ሻൌቐ
ଵ
஻ሺ௔,௕ሻ𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ
0 ൏𝑥൏1
0
otherwise
𝐵ሺ𝑎,𝑏ሻൌ඲
଴
ଵ
𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ𝑑𝑥
𝑐, 𝑑
𝑐
𝑑െ𝑐
𝑎ൌ𝑏,
1
2,
352 of 848

regions about  as the common value  increases. When 
 the beta
distribution reduces to the uniform 
 distribution. (See Figure 5.8
.) When
 the density is skewed to the left (in the sense that smaller values become
more likely), and it is skewed to the right when 
 (See Figure 5.9
.)
Figure 5.8 Beta densities with parameters (
) when 
Figure 5.9 Beta densities with parameters (
) when 
1
2
𝑎
𝑎ൌ𝑏ൌ1,
ሺ0,1ሻ
𝑏൐𝑎,
𝑎൐𝑏.
𝑎, 𝑏
𝑎ൌ𝑏.
𝑎, 𝑏
𝑎/ሺ𝑎൅𝑏ሻൌ1/20.
353 of 848

The relationship
can be shown to exist between
and the gamma function.
Using Equation (6.3)
 along with the identity 
 which was given
in Equation (6.1)
 it follows that
The preceding enables us to easily derive the mean and variance of a beta random
variable with parameters  and 
 For if  is such a random variable, then
𝐵ሺ𝑎,𝑏ሻൌΓ ሺ𝑎ሻΓ ሺ𝑏ሻ
Γ ሺ𝑎൅𝑏ሻ
(6.3)
𝐵ሺ𝑎,𝑏ሻൌ඲
଴
ଵ
𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ𝑑𝑥
Γ ሺ𝑥൅1ሻൌ𝑥Γ ሺ𝑥ሻ,
𝐵ሺ𝑎൅1, 𝑏ሻ
𝐵ሺ𝑎, 𝑏ሻ
ൌΓ ሺ𝑎൅1ሻΓ ሺ𝑏ሻ
Γ ሺ𝑎൅𝑏൅1ሻ 
Γ ሺ𝑎൅𝑏ሻ
Γ ሺ𝑎ሻΓ ሺ𝑏ሻൌ
𝑎
𝑎൅𝑏
𝑎
𝑏.
𝑋
354 of 848

Similarly, it follows that
The identity 
 now yields
Remark A verification of Equation (6.3)
 appears in Example 7c
 of Chapter
6
.
If  is an exponential random variable with rate  and 
 then
is said to be a Pareto random variable with parameters  and 
 The parameter
 is called the index parameter, and  is called the minimum parameter (because
). The distribution function of  is derived as follows: For 
𝐸ሾ𝑋ሿ
ൌ
ଵ
஻ሺ௔, ௕ሻ
଴
ଵ
𝑥௔ሺ1 െ𝑥ሻ௕െଵ𝑑𝑥
ൌ
஻ሺ௔൅ଵ, ௕ሻ
஻ሺ௔, ௕ሻ
ൌ
௔
௔൅௕
𝐸ሾ𝑋ଶሿ
ൌ
ଵ
஻ሺ௔,௕ሻ׬଴
ଵ𝑥௔൅ଵሺ1 െ𝑥ሻ௕െଵ𝑑𝑥
ൌ
஻ሺ௔൅ଶ,௕ሻ
஻ሺ௔,௕ሻ
ൌ
஻ሺ௔൅ଶ,௕ሻ
஻ሺ௔൅ଵ,௕ሻ
஻ሺ௔൅ଵ,௕ሻ
஻ሺ௔,௕ሻ
ൌ
ሺ௔൅ଵሻ௔
ሺ௔൅௕൅ଵሻሺ௔൅௕ሻ
Varሺ𝑋ሻൌ𝐸ሾ𝑋ଶሿെሺ𝐸ሾ𝑋ሿሻଶ
Varሺ𝑋ሻ
ൌ
௔ሺ௔൅ଵሻ
ሺ௔൅௕ሻሺ௔൅௕൅ଵሻെሺ
௔
௔൅௕ሻ
ଶ
ൌ
௔௕
ሺ௔൅௕ሻమሺ௔൅௕൅ଵሻ
𝑋
𝜆
𝑎൐0,
𝑌ൌ𝑎𝑒௑
𝑎
𝜆.
𝜆൐0
𝑎
𝑃ሼ𝑌൐𝑎ሽൌ1
𝑌
𝑦൒𝑎,
355 of 848

Hence, the distribution function of  is
Differentiating the distribution function yields the density function of 
When 
 it is easily checked that 
 When 
 will be finite only when 
 In this case,
Hence, when 
Remarks (a) We could also have derived the moments of  by using the
representation 
 where  is exponential with rate 
 This yields, for 
𝑃ሺ𝑌൐𝑦ሻ
ൌ𝑃ሺ𝑎𝑒௑൐𝑦ሻ
ൌ𝑃ሺ𝑒௑൐𝑦/𝑎ሻ
ൌ𝑃ሺ𝑋൐logሺ𝑦/𝑎ሻሻ
ൌ𝑒െ୪୭୥ሺ௬/௔ሻ
ൌ𝑒െ୪୭୥ሺሺ௬/௔ሻഊሻ
ൌሺ𝑎/𝑦ሻఒ
𝑌
𝐹௒ሺ𝑦ሻൌ1 െ𝑃ሺ𝑌൐𝑦ሻൌ1 െ𝑎ఒ𝑦െఒ,  𝑦൒𝑎
𝑌:
𝑓௒ሺ𝑦ሻൌ𝜆𝑎ఒ𝑦െሺఒ൅ଵሻ,  𝑦൒𝑎
𝜆൑1
𝐸ሾ𝑌ሿൌ∞.
𝜆൐1,
𝐸ሾ𝑌ሿ
ൌ׬௔
ஶ𝜆𝑎ఒ𝑦െఒ𝑑𝑦
ൌ𝜆𝑎ఒ௬భെഊ
ଵെఒฬ
௔
ஶ
ൌ𝜆𝑎ఒ௔భെഊ
ఒെଵ
ൌ
ఒ௔
ఒെଵ
𝐸ሾ𝑌ଶሿ
𝜆൐2.
𝐸ሾ𝑌ଶሿ
ൌ׬௔
ஶ𝜆𝑎ఒ𝑦ଵെఒ𝑑𝑦
ൌ𝜆𝑎ఒ௬మെഊ
ଶെఒฬ
௔
ஶ
ൌ
ఒ௔మ
ఒെଶ
𝜆൐2
Varሺ𝑌ሻൌ𝜆𝑎ଶ
𝜆െ2 െ
2ఒ𝑎ଶ
ሺ𝜆െ1ሻଶൌ
𝜆𝑎ଶ
ሺ𝜆െ2ሻሺ𝜆െ1ሻଶ
𝑌
𝑌ൌ𝑎𝑒௑,
𝑋
𝜆  .
𝜆൐𝑛,
356 of 848

(b) Where the density function 
 of the Pareto is positive (that is, when 
) it is
a constant times a power of 
 and for this reason it is called a power law density.
(c) The Pareto distribution has been found to be useful in applications relating to
such things as
i. the income or wealth of members of a population;
ii. the file size of internet traffic (under the TCP protocol);
iii. the time to compete a job assigned to a supercomputer;
iv. the size of a meteorite;
v. the yearly maximum one day rainfalls in different regions.
Further properties of the Pareto distribution will be developed in later chapters.
Often, we know the probability distribution of a random variable and are interested in
determining the distribution of some function of it. For instance, suppose that we
know the distribution of  and want to find the distribution of 
 To do so, it is
necessary to express the event that 
 in terms of  being in some set. We
illustrate with the following examples.
Example 7a
Let  be uniformly distributed over (0, 1). We obtain the distribution of the
random variable 
 defined by 
 as follows: For 
For instance, the density function of  is given by
𝐸ሾ𝑌௡ሿൌ𝑎௡𝐸ሾ𝑒௡௑ሿൌ𝑎௡඲
଴
ஶ
𝑒௡௫𝑒െ௫𝑑𝑥ൌ𝑎௡඲
଴
ஶ
𝑒െሺെ௡ሻ௫𝑑𝑥ൌ𝑎௡
െ𝑛
𝑓ሺ𝑦ሻ
𝑦൐𝑎
𝑦,
𝑋
𝑔ሺ𝑋ሻ.
𝑔ሺ𝑋ሻ൑𝑦
𝑋
𝑋
𝑌,
𝑌ൌ𝑋௡,
0 ൑𝑦൑1,
𝐹௒ሺ𝑦ሻ
ൌ𝑃ሼ𝑌൑𝑦ሽ
ൌ𝑃ሼ𝑋௡൑𝑦ሽ
ൌ𝑃൛𝑋൑𝑦ଵ/௡ൟ
ൌ𝐹௑ሺ𝑦ଵ/௡ሻ
ൌ𝑦ଵ/௡
𝑌
357 of 848

Example 7b
If  is a continuous random variable with probability density 
 then the
distribution of 
 is obtained as follows: For 
Differentiation yields
Example 7c
If  has a probability density 
 then 
 has a density function that is
obtained as follows: For 
Hence, on differentiation, we obtain
The method employed in Examples 7a through 7c can be used to prove
Theorem 7.1
.
Theorem 7.1
Let  be a continuous random variable having probability density function 
Suppose that 
 is a strictly monotonic (increasing or decreasing),
differentiable (and thus continuous) function of 
 Then the random variable 
defined by 
 has a probability density function given by
𝑓௒ሺ𝑦ሻൌ൝
ଵ
௡𝑦ଵ/௡െଵ0 ൑𝑦൑1
0
otherwise
𝑋
𝑓௑,
𝑌ൌ𝑋ଶ
𝑦൒0,
𝐹௒ሺ𝑦ሻ
ൌ𝑃ሼ𝑌൑𝑦ሽ
ൌ𝑃൛𝑋ଶ൑𝑦ൟ
ൌ𝑃൛െ
𝑦
ඥ
൑𝑋൑
𝑦
ඥൟ
ൌ𝐹௑ሺ𝑦
ඥሻെ𝐹௑ሺെ
𝑦
ඥሻ
𝑓௒ሺ𝑦ሻൌ
1
2 𝑦
ඥ
ሾ𝑓௑ሺ𝑦
ඥሻ൅𝑓௑ሺെ
𝑦
ඥሻሿ
𝑋
𝑓௑,
𝑌ൌ|𝑋|
𝑦൒0,
𝐹௒ሺ𝑦ሻ
ൌ𝑃ሼ𝑌൑𝑦ሽ
ൌ𝑃ሼ|𝑋| ൑𝑦ሽ
ൌ𝑃ሼെ𝑦൑𝑋൑𝑦ሽ
ൌ𝐹௑ሺ𝑦ሻെ𝐹௑ሺെ𝑦ሻ
𝑓௒ሺ𝑦ሻൌ𝑓௑ሺ𝑦ሻ൅𝑓௑ሺെ𝑦ሻ  𝑦൒0
𝑋
𝑓௑.
𝑔ሺ𝑥ሻ
𝑥.
𝑌
𝑌ൌ𝑔ሺ𝑋ሻ
358 of 848

where 
 is defined to equal that value of  such that 
We shall prove Theorem 7.1
 when 
 is an increasing function.
Proof Suppose that 
 for some 
 Then, with 
Differentiation gives
which agrees with Theorem 7.1
, since 
 is nondecreasing, so its derivative
is nonnegative.
When 
 for any 
 then 
 is either 0 or 1, and in either case 
Example 7d
Let  be a continuous nonnegative random variable with density function 
 and
let 
 Find 
 the probability density function of 
Solution
If 
 then
and
Hence, from Theorem 7.1
, we obtain, for y  0,
For 
 this gives
𝑓௒ሺ𝑦ሻൌ൞
𝑓௑ሾ𝑔െଵሺ𝑦ሻሿฬ
ௗ
ௗ௬𝑔െଵሺ𝑦ሻฬ
𝑖𝑓 𝑦  ൌ 𝑔ሺ𝑥ሻ for some 𝑥
0
𝑖𝑓 𝑦  ് 𝑔ሺ𝑥ሻ for all 𝑥
𝑔െଵሺ𝑦ሻ
𝑥
𝑔ሺ𝑥ሻൌ𝑦.
𝑔ሺ𝑥ሻ
𝑦ൌ𝑔ሺ𝑥ሻ
𝑥.
𝑌ൌ𝑔ሺ𝑋ሻ,
𝐹௒ሺ𝑦ሻ
ൌ𝑃ሼ𝑔ሺ𝑋ሻ൑𝑦ሽ
ൌ𝑃ሼ𝑋൑𝑔െଵሺ𝑦ሻሽ
ൌ𝐹௑ሺ𝑔െଵሺ𝑦ሻሻ
𝑓௒ሺ𝑦ሻൌ𝑓௑ሺ𝑔െଵሺ𝑦ሻሻ𝑑
𝑑𝑦𝑔െଵሺ𝑦ሻ
𝑔െଵሺ𝑦ሻ
𝑦്𝑔ሺ𝑥ሻ
𝑥,
𝐹௒ሺ𝑦ሻ
𝑓௒ሺ𝑦ሻൌ0.
𝑋
𝑓,
𝑌ൌ𝑋௡.
𝑓௒,
𝑌.
𝑔ሺ𝑥ሻൌ𝑥௡,
𝑔െଵሺ𝑦ሻൌ𝑦ଵ/௡
𝑑
𝑑𝑦ሼ𝑔െଵሺ𝑦ሻሽൌ1
𝑛𝑦ଵ/௡െଵ
൒
𝑓௒ሺ𝑦ሻൌ1
𝑛𝑦ଵ/௡െଵ𝑓ሺ𝑦ଵ/௡ሻ
𝑛ൌ2,
359 of 848

which (since 
) is in agreement with the result of Example 7b
.
Example 7e The Lognormal Distribution
If  is a normal random variable with mean  and variance 
 then the random
variable
is said to be a lognormal random variable with parameters  and 
 Thus, a
random variable  is lognormal if 
 is a normal random variable. The
lognormal is often used as the distribution of the ratio of the price of a security at
the end of one day to its price at the end of the prior day. That is, if 
 is the price
of some security at the end of day 
 then it is often supposed that 
 is a
lognormal random variable, implying that 
 is normal. Thus, to
assume that 
 is lognormal is to assume that
where  is normal.
Let us now use Theorem 7.1
 to derive the density of a lognormal random
variable  with parameters  and 
 Because 
 where  is normal with
mean  and variance 
 we need to determine the inverse of the function
 Because
we obtain upon taking logarithms that
Using that 
Theorem 7.1
 yields the density:
𝑓௒ሺ𝑦ሻൌ
1
2 𝑦
ඥ
𝑓ሺ𝑦
ඥሻ
𝑋൒0
𝑋
𝜇
𝜎ଶ,
𝑌ൌ𝑒௑
𝜇
𝜎ଶ.
𝑌
logሺ𝑌ሻ
𝑆௡
𝑛,
ௌ೙
ௌ೙െభ
𝑋≡log൬
ௌ೙
ௌ೙െభ൰
ௌ೙
ௌ೙െభ
𝑆௡ൌ𝑆௡െଵ𝑒௑
𝑋
𝑌
𝜇
𝜎ଶ.
𝑌ൌ𝑒௑,
𝑋
𝜇
𝜎ଶ,
𝑔ሺ𝑥ሻൌ𝑒௫.
𝑦ൌ𝑔ሺ𝑔െଵሺ𝑦ሻሻൌ𝑒௚െభሺ௬ሻ
𝑔െଵሺ𝑦ሻൌlogሺ𝑦ሻ
 
ௗ
ௗ௬𝑔െଵሺ𝑦ሻൌ1/𝑦,
𝑓௒ሺ𝑦ሻൌ
1
2𝜋
√
 𝜎𝑦expሼെሺlogሺ𝑦ሻെ𝜇ሻଶ/2𝜎ଶሽ, 𝑦൐0
360 of 848

A random variable  is continuous if there is a nonnegative function 
 called the
probability density function of 
 such that, for any set 
If  is continuous, then its distribution function  will be differentiable and
The expected value of a continuous random variable  is defined by
A useful identity is that for any function 
As in the case of a discrete random variable, the variance of  is defined by
A random variable  is said to be uniform over the interval (
) if its probability
density function is given by
Its expected value and variance are
A random variable  is said to be normal with parameters  and 
 if its probability
density function is given by
𝑋
𝑓,
𝑋,
𝐵,
𝑃ሼ𝑋∈𝐵ሽൌ඲
஻
𝑓ሺ𝑥ሻ𝑑𝑥
𝑋
𝐹
𝑑
𝑑𝑥𝐹ሺ𝑥ሻൌ𝑓ሺ𝑥ሻ
𝑋
𝐸ሾ𝑋ሿൌ඲
െஶ
ஶ
𝑥𝑓ሺ𝑥ሻ𝑑𝑥
𝑔,
𝐸ሾ𝑔ሺ𝑋ሻሿൌ඲
െஶ
ஶ
𝑔ሺ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥
𝑋
Varሺ𝑋ሻൌ𝐸ሾሺ𝑋െ𝐸ሾ𝑋ሿሻଶሿ
𝑋
𝑎, 𝑏
𝑓ሺ𝑥ሻൌ൝
ଵ
௕െ௔
𝑎൑𝑥൑𝑏
0
otherwise
𝐸ሾ𝑋ሿൌ𝑎൅𝑏
2
 Varሺ𝑋ሻൌሺ𝑏െ𝑎ሻଶ
12
𝑋
𝜇
𝜎ଶ
𝑓ሺ𝑥ሻൌ
1
2𝜋
√
𝜎𝑒െሺ௫െఓሻమ/ଶఙమ  െ∞൏𝑥൏∞
361 of 848

It can be shown that
If  is normal with mean  and variance 
 then 
 defined by
is normal with mean 0 and variance 1. Such a random variable is said to be a
standard normal random variable. Probabilities about  can be expressed in terms of
probabilities about the standard normal variable 
 whose probability distribution
function can be obtained either from Table 5.1
, the normal calculator on
StatCrunch, or a website.
When  is large, the probability distribution function of a binomial random variable
with parameters  and  can be approximated by that of a normal random variable
having mean np and variance 
A random variable whose probability density function is of the form
is said to be an exponential random variable with parameter 
 Its expected value
and variance are, respectively,
A key property possessed only by exponential random variables is that they are
memoryless, in the sense that, for positive  and 
If  represents the life of an item, then the memoryless property states that for any 
the remaining life of a -year-old item has the same probability distribution as the life
of a new item. Thus, one need not remember the age of an item to know its
distribution of remaining life.
Let  be a nonnegative continuous random variable with distribution function  and
density function 
 The function
𝜇ൌ𝐸ሾ𝑋ሿ 𝜎ଶൌVarሺ𝑋ሻ
𝑋
𝜇
𝜎ଶ,
𝑍,
𝑍ൌ𝑋െ𝜇
𝜎
𝑋
𝑍,
𝑛
𝑛
𝑝
𝑛𝑝ሺ1 െ𝑝ሻ.
𝑓ሺ𝑥ሻൌቊ𝜆𝑒െఒ௫𝑥൒0
 0
otherwise
𝜆.
𝐸ሾ𝑋ሿൌ1
𝜆 Varሺ𝑋ሻൌ1
𝜆ଶ
𝑠
𝑡,
𝑃ሼ𝑋൐𝑠൅𝑡||𝑋൐𝑡ሽൌ𝑃ሼ𝑋൐𝑠ሽ
𝑋
𝑡,
𝑡
𝑋
𝐹
𝑓.
𝜆ሺ𝑡ሻൌ
𝑓ሺ𝑡ሻ
1 െ𝐹ሺ𝑡ሻ 𝑡൒0
362 of 848

is called the hazard rate, or failure rate, function of 
 If we interpret  as being the
life of an item, then for small values of 
 is approximately the probability that
a -unit-old item will fail within an additional time dt. If  is the exponential distribution
with parameter 
 then
In addition, the exponential is the unique distribution having a constant failure rate.
A random variable is said to have a gamma distribution with parameters  and  if its
probability density function is equal to
and is 0 otherwise. The quantity 
 is called the gamma function and is defined
by
The expected value and variance of a gamma random variable are, respectively,
A random variable is said to have a beta distribution with parameters (
) if its
probability density function is equal to
and is equal to 0 otherwise. The constant (
) is given by
The mean and variance of such a random variable are, respectively,
𝐹.
𝑋
𝑑𝑡, 𝜆ሺ𝑡ሻ 𝑑𝑡
𝑡
𝐹
𝜆,
𝜆ሺ𝑡ሻൌ𝜆 𝑡൒0
𝛼
𝜆
𝑓ሺ𝑥ሻൌ𝜆𝑒െఒ௫ሺ𝜆𝑥ሻఈെଵ
Γ ሺ𝛼ሻ
 𝑥൒0
Γ ሺ𝛼ሻ
Γ ሺ𝛼ሻൌ඲
଴
ஶ
𝑒െ௫𝑥ఈെଵ𝑑𝑥
𝐸ሾ𝑋ሿൌ𝛼
𝜆 Varሺ𝑋ሻൌ𝛼
𝜆ଶ
𝑎, 𝑏
𝑓ሺ𝑥ሻൌ
1
𝐵ሺ𝑎,𝑏ሻ𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ 0 ൑𝑥൑1
𝐵𝑎, 𝑏
𝐵ሺ𝑎,𝑏ሻൌ඲
଴
ଵ
𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ𝑑𝑥
𝐸ሾ𝑋ሿൌ
𝑎
𝑎൅𝑏 Varሺ𝑋ሻൌ
𝑎𝑏
ሺ𝑎൅𝑏ሻଶሺ𝑎൅𝑏൅1ሻ
363 of 848

5.1. Let  be a random variable with probability density function
a. What is the value of 
b. What is the cumulative distribution function of 
5.2. A system consisting of one original unit plus a spare can function for a
random amount of time 
 If the density of  is given (in units of months) by
what is the probability that the system functions for at least 5 months?
5.3. Consider the function
Could  be a probability density function? If so, determine C. Repeat if 
were given by
5.4. The probability density function of 
 the lifetime of a certain type of
electronic device (measured in hours), is given by
a. Find 
b. What is the cumulative distribution function of 
c. What is the probability that of 6 such types of devices, at least 3 will
function for at least 15 hours? What assumptions are you making?
5.5. A filling station is supplied with gasoline once a week. If its weekly volume
of sales in thousands of gallons is a random variable with probability density
function
𝑋
𝑓ሺ𝑥ሻൌ൝𝑐ሺ1 െ𝑥ଶሻെ1 ൏𝑥൏1
0
otherwise
𝑐?
𝑋?
𝑋.
𝑋
𝑓ሺ𝑥ሻൌ൝𝐶𝑥𝑒െ௫/ଶ
𝑥൐0
0
𝑥൑0
𝑓ሺ𝑥ሻൌቐ𝐶ሺ2𝑥െ𝑥ଷሻ0 ൏𝑥൏5
2
0
otherwise
𝑓
𝑓ሺ𝑥ሻ
𝑓ሺ𝑥ሻൌቐ𝐶ሺ2𝑥െ𝑥ଶሻ0 ൏𝑥൏5
2
0
otherwise
𝑋,
𝑓ሺ𝑥ሻൌቐ
ଵ଴
௫మ
𝑥൐10
0
𝑥൑10
𝑃ሼ𝑋൐20ሽ.
𝑋?
364 of 848

what must the capacity of the tank be so that the probability of the supply
being exhausted in a given week is .01?
5.6. Compute [ ] if  has a density function given by
a. 
b. 
c. 
5.7. The density function of  is given by
If 
 find  and 
5.8. The lifetime in hours of an electronic tube is a random variable having a
probability density function given by
Compute the expected lifetime of such a tube.
5.9. Consider Example 4b
 of Chapter 4
, but now suppose that the
seasonal demand is a continuous random variable having probability density
function 
 Show that the optimal amount to stock is the value s* that satisfies
where  is net profit per unit sale,  is the net loss per unit unsold, and  is the
cumulative distribution function of the seasonal demand.
5.10. Trains headed for destination  arrive at the train station at 15-minute
intervals starting at 7 ඉ.ඕ., whereas trains headed for destination  arrive at
15-minute intervals starting at 7:05 ඉ.ඕ.
a. If a certain passenger arrives at the station at a time uniformly
distributed between 7 and 8 ඉ.ඕ. and then gets on the first train that
arrives, what proportion of time does he or she go to destination 
b. What if the passenger arrives at a time uniformly distributed between
7:10 and 8:10 ඉ.ඕ.?
𝑓ሺ𝑥ሻൌ൝5ሺ1 െ𝑥ሻସ
0 ൏𝑥൏1
0
otherwise
𝐸𝑋
𝑋
𝑓ሺ𝑥ሻൌ൝
ଵ
ସ𝑥𝑒െ௫/ଶ
𝑥൐0
0
otherwise
;
𝑓ሺ𝑥ሻൌ൝𝑐ሺ1 െ𝑥ଶሻെ1 ൏𝑥൏1
0
otherwise
;
𝑓ሺ𝑥ሻൌቐ
ହ
௫మ
𝑥൐5
0
𝑥൑5
.
𝑋
𝑓ሺ𝑥ሻൌቊ𝑎൅𝑏𝑥ଶ0 ൑𝑥൑1
0
otherwise
𝐸ሾ𝑋ሿൌ3
5,
𝑎
𝑏.
𝑓ሺ𝑥ሻൌ𝑥𝑒െ௫  𝑥൒0
𝑓.
𝐹ሺ𝑠*ሻൌ
𝑏
𝑏൅ℓ
𝑏
ℓ
𝐹
𝐴
𝐵
𝐴?
365 of 848

5.11. A point is chosen at random on a line segment of length 
 Interpret this
statement, and find the probability that the ratio of the shorter to the longer
segment is less than 
5.12. A bus travels between the two cities  and 
 which are 100 miles apart.
If the bus has a breakdown, the distance from the breakdown to city  has a
uniform distribution over (0, 100). There is a bus service station in city 
 in 
and in the center of the route between  and 
 It is suggested that it would
be more efficient to have the three stations located 25, 50, and 75 miles,
respectively, from 
 Do you agree? Why?
5.13. You arrive at a bus stop at 10 ඉ.ඕ., knowing that the bus will arrive at
some time uniformly distributed between 10 and 10:30.
a. What is the probability that you will have to wait longer than 10
minutes?
b. If, at 10:15, the bus has not yet arrived, what is the probability that you
will have to wait at least an additional 10 minutes?
5.14. Let  be a uniform (0, 1) random variable. Compute 
 by using
Proposition 2.1
, and then check the result by using the definition of
expectation.
5.15. If  is a normal random variable with parameters 
 and 
compute
a. 
b. 
c. 
d. 
e. 
5.16. The annual rainfall (in inches) in a certain region is normally distributed
with 
 and 
 What is the probability that starting with this year, it will
take more than 10 years before a year occurs having a rainfall of more than
50 inches? What assumptions are you making?
5.17. The salaries of physicians in a certain speciality are approximately
normally distributed. If 
 percent of these physicians earn less than $180,000
and 
 percent earn more than $320,000, approximately what fraction earn
a. less than $200,000?
b. between $280,000 and $320,000?
5.18. Suppose that  is a normal random variable with mean 5. If
 approximately what is Var
5.19. Let  be a normal random variable with mean 12 and variance 4. Find
the value of  such that 
𝐿.
1
4 .
𝐴
𝐵,
𝐴
𝐴,
𝐵,
𝐴
𝐵.
𝐴.
𝑋
𝐸ሾ𝑋௡ሿ
𝑋
𝜇ൌ10
𝜎ଶൌ36,
𝑃ሼ𝑋൐5ሽ;
𝑃ሼ4 ൏𝑋൏16ሽ;
𝑃ሼ𝑋൏8ሽ;
𝑃ሼ𝑋൏20ሽ;
𝑃ሼ𝑋൐16ሽ.
𝜇ൌ40
𝜎ൌ4.
25
25
𝑋
𝑃ሼ𝑋൐9ሽൌ.2,
ሺ𝑋ሻ?
𝑋
𝑐
𝑃ሼ𝑋൐𝑐ሽൌ.10.
366 of 848

5.20. If 65 percent of the population of a large community is in favor of a
proposed rise in school taxes, approximate the probability that a random
sample of 100 people will contain
a. at least 50 who are in favor of the proposition;
b. between 60 and 70 inclusive who are in favor;
c. fewer than 75 in favor.
5.21. Suppose that the height, in inches, of a 25-year-old man is a normal
random variable with parameters 
 and 
 What percentage of
25-year-old men are taller than 6 feet, 2 inches? What percentage of men in
the 6-footer club are taller than 6 feet, 5 inches?
5.22. Every day Jo practices her tennis serve by continually serving until she
has had a total of 50 successful serves. If each of her serves is, independently
of previous ones, successful with probability 
 approximately what is the
probability that she will need more than 
 serves to accomplish her goal?
Hint: Imagine even if Jo is successful that she continues to serve until she
has served exactly 
 times. What must be true about her first 
 serves if
she is to reach her goal?
5.23. One thousand independent rolls of a fair die will be made. Compute an
approximation to the probability that the number 6 will appear between 150
and 200 times inclusively. If the number 6 appears exactly 200 times, find the
probability that the number 5 will appear less than 150 times.
5.24. The lifetimes of interactive computer chips produced by a certain
semiconductor manufacturer are normally distributed with parameters
 hours and 
 hours. What is the approximate
probability that a batch of 100 chips will contain at least 20 whose lifetimes are
less than 
5.25. Each item produced by a certain manufacturer is, independently, of
acceptable quality with probability .95. Approximate the probability that at
most 10 of the next 150 items produced are unacceptable.
5.26. Two types of coins are produced at a factory: a fair coin and a biased
one that comes up heads 55 percent of the time. We have one of these coins
but do not know whether it is a fair coin or a biased one. In order to ascertain
which type of coin we have, we shall perform the following statistical test: We
shall toss the coin 1000 times. If the coin lands on heads 525 or more times,
then we shall conclude that it is a biased coin, whereas if it lands on heads
fewer than 525 times, then we shall conclude that it is a fair coin. If the coin is
actually fair, what is the probability that we shall reach a false conclusion?
What would it be if the coin were biased?
5.27. In 10,000 independent tosses of a coin, the coin landed on heads 5800
times. Is it reasonable to assume that the coin is not fair? Explain.
𝜇ൌ71
𝜎ଶൌ6.25.
.4,
100
100
100
𝜇ൌ1.4 ൈ10଺
𝜎ൌ3 ൈ10ହ
1.8 ൈ10଺?
367 of 848

5.28. Twelve percent of the population is left handed. Approximate the
probability that there are at least 20 left-handers in a school of 200 students.
State your assumptions.
5.29. A model for the movement of a stock supposes that if the present price
of the stock is 
 then after one period, it will be either us with probability  or
ds with probability 
 Assuming that successive movements are
independent, approximate the probability that the stock’s price will be up at
least 30 percent after the next 1000 periods if 
 and
5.30. An image is partitioned into two regions, one white and the other black.
A reading taken from a randomly chosen point in the white section will be
normally distributed with 
 and 
 whereas one taken from a
randomly chosen point in the black region will have a normally distributed
reading with parameters (6, 9). A point is randomly chosen on the image and
has a reading of 5. If the fraction of the image that is black is 
 for what value
of  would the probability of making an error be the same, regardless of
whether one concluded that the point was in the black region or in the white
region?
5.31.
a. A fire station is to be located along a road of length 
 If fires
occur at points uniformly chosen on (0, ), where should the station be
located so as to minimize the expected distance from the fire? That is,
choose  so as to
when  is uniformly distributed over (0, ).
b. Now suppose that the road is of infinite length—stretching from point 0
outward to 
 If the distance of a fire from point 0 is exponentially
distributed with rate 
 where should the fire station now be located?
That is, we want to minimize 
 where  is now exponential
with rate 
5.32. The time (in hours) required to repair a machine is an exponentially
distributed random variable with parameter 
 What is
a. What is the probability that a repair time exceeds 2 hours?
b. the conditional probability that a repair takes at least 10 hours, given
that its duration exceeds 9 hours?
5.33. If  is uniformly distributed on 
 find the distribution of
5.34. Jones figures that the total number of thousands of miles that a racing
𝑠,
𝑝
1 െ𝑝.
𝑢ൌ1.012, 𝑑ൌ0.990,
𝑝ൌ.52.
𝜇ൌ4
𝜎ଶൌ4,
𝛼,
𝛼
𝐴,𝐴൏∞.
𝐴
𝑎
minimize 𝐸ሾ|𝑋െ𝑎|ሿ
𝑋
𝐴
∞.
𝜆,
𝐸ሾ||𝑋െ𝑎||ሿ,
𝑋
𝜆.
𝜆ൌ1
2 .
𝑈
ሺ0,  1ሻ,
𝑌ൌെlogሺ𝑈ሻ.
368 of 848

auto can be driven before it would need to be junked is an exponential
random variable with parameter 
 Smith has a used car that he claims has
been driven only 10,000 miles. If Jones purchases the car, what is the
probability that she would get at least 20,000 additional miles out of it? Repeat
under the assumption that the lifetime mileage of the car is not exponentially
distributed, but rather is (in thousands of miles) uniformly distributed over (0,
40).
5.35. If  is an exponential random variable with parameter 
 and 
 find
the density function of 
 What kind of random variable is 
5.36. The lung cancer hazard rate 
 of a -year-old male smoker is such
that
Assuming that a 40-year-old male smoker survives all other hazards, what is
the probability that he survives to (a) age 50 and (b) age 60 without
contracting lung cancer?
5.37. Suppose that the life distribution of an item has the hazard rate function
 What is the probability that
a. the item survives to age 2?
b. the item’s lifetime is between .4 and 1.4?
c. a 1-year-old item will survive to age 2?
5.38. If  is uniformly distributed over 
 find
(a) 
(b) the density function of the random variable 
5.39. If  is uniformly distributed over (0, 5), what is the probability that the
roots of the equation 
 are both real?
5.40. If  is an exponential random variable with parameter 
 compute
the probability density function of the random variable  defined by 
5.41. If  is uniformly distributed over 
 find  and  if 
5.42. If  is uniformly distributed over (0, 1), find the density function of 
5.43. Find the distribution of 
 where  is a fixed constant and  is
uniformly distributed on 
 Such a random variable R arises in the
theory of ballistics. If a projectile is fired from the origin at an angle  from the
earth with a speed 
 then the point  at which it returns to the earth can be
expressed as 
 where  is the gravitational constant, equal to
980 centimeters per second squared.
5.44. Let  be a lognormal random variable (see Example 7e
 for its
definition) and let 
 be a constant. Answer true or false to the following,
1
20 .
𝑋
𝜆,
𝑐൐0,
𝑐𝑋.
𝑐𝑋.
𝜆ሺ𝑡ሻ
𝑡
𝜆ሺ𝑡ሻൌ.027 ൅.00025ሺ𝑡െ40ሻଶ  𝑡൒40
𝜆ሺ𝑡ሻൌ𝑡ଷ, 𝑡൐0.
𝑋
ሺെ1,1ሻ,
𝑃ሼ|𝑋| ൐1
2ሽ;
|𝑋|.
𝑌
4𝑥ଶ൅4𝑥𝑌൅𝑌൅2 ൌ0
𝑋
𝜆ൌ1,
𝑌
𝑌ൌlog 𝑋.
𝑋
ሺ𝑎, 𝑏ሻ,
𝑎
𝑏
𝐸ሾ𝑋ሿൌ10,
Varሺ𝑋ሻൌ48.
𝑋
𝑌ൌ𝑒௑.
𝑅ൌ𝐴sin 𝜃,
𝐴
𝜃
ሺെ𝜋/2,𝜋/2ሻ.
𝛼
𝜈,
𝑅
𝑅ൌሺ𝑣ଶ/𝑔ሻ sin2𝛼,
𝑔
𝑌
𝑐൐0
369 of 848

and then give an explanation for your answer.
a. 
 is lognormal;
b. 
 is lognormal.
𝑐𝑌
𝑐൅𝑌
5.1. The speed of a molecule in a uniform gas at equilibrium is a random
variable whose probability density function is given by
where 
 and 
 and 
 denote, respectively, Boltzmann’s
constant, the absolute temperature of the gas, and the mass of the molecule.
Evaluate  in terms of 
5.2. Show that
Hint: Show that
5.3. Show that if  has density function 
 then
Hint: Using Theoretical Exercise 5.2
, start with
and then proceed as in the proof given in the text when 
5.4. Prove Corollary 2.1
.
5.5. Use the result that for a nonnegative random variable 
𝑓ሺ𝑥ሻൌ൝𝑎𝑥ଶ𝑒െ௕௫మ𝑥൒0
0
𝑥൏0
𝑏ൌ𝑚/2𝑘𝑇
𝑘, 𝑇,
𝑚
𝑎
𝑏.
𝐸ሾ𝑌ሿൌ඲
଴
ஶ
𝑃ሼ𝑌൐𝑦ሽ 𝑑𝑦െ඲
଴
ஶ
𝑃ሼ𝑌൏െ𝑦ሽ 𝑑𝑦
඲
଴
ஶ
𝑃൜𝑌൏െ𝑦ൠ 𝑑𝑦
ൌെ඲
െஶ
଴
𝑥𝑓௒ሺ𝑥ሻ𝑑𝑥
඲
଴
ஶ
𝑃ቊ𝑌൐𝑦ቋ 𝑑𝑦
ൌ඲
଴
ஶ
𝑥𝑓௒ሺ𝑥ሻ𝑑𝑥
𝑋
𝑓,
𝐸ሾ𝑔ሺ𝑋ሻሿൌ඲
െஶ
ஶ
𝑔ሺ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥
𝐸ሾ𝑔ሺ𝑋ሻሿൌ඲
଴
ஶ
𝑃ሼ𝑔ሺ𝑋ሻ൐𝑦ሽ𝑑𝑦െ඲
଴
ஶ
𝑃ሼ𝑔ሺ𝑋ሻ൏െ𝑦ሽ𝑑𝑦
𝑔ሺ𝑋ሻ൒0.
𝑌,
370 of 848

to show that for a nonnegative random variable 
Hint: Start with
and make the change of variables 
5.6. Define a collection of events 
 having the property that
 for all  but 
Hint: Let  be uniform over (0, 1) and define each 
 in terms of 
5.7. The standard deviation of 
 denoted SD(X), is given by
Find 
 if  has variance 
5.8. Let  be a random variable that takes on values between 0 and 
 That
is, 
 Show that
Hint: One approach is to first argue that
and then use this inequality to show that
5.9. Show that  is a standard normal random variable; then, for 
a. 
b. 
c. 
5.10. Let 
 denote the probability density function of a normal random
variable with mean  and variance 
 Show that 
 and 
 are points of
inflection of this function. That is, show that 
 when 
 or
𝐸ሾ𝑌ሿൌ඲
଴
ஶ
𝑃ሼ𝑌൐𝑡ሽ𝑑𝑡
𝑋,
𝐸ሾ𝑋௡ሿൌ඲
଴
ஶ
𝑛𝑥௡െଵ𝑃ሼ𝑋൐𝑥ሽ𝑑𝑥
𝐸ሾ𝑋௡ሿൌ඲
଴
ஶ
𝑃ሼ𝑋௡൐𝑡ሽ𝑑𝑡
𝑡ൌ𝑥௡.
𝐸௔, 0 ൏𝑎൏1,
𝑃ሺ𝐸௔ሻൌ1
𝑎
𝑃ቀ∩
௔𝐸௔ቁൌ0.
𝑋
𝐸௔
𝑋.
𝑋,
𝑆𝐷ሺ𝑋ሻൌ
Varሺ𝑋ሻ
ඥ
𝑆𝐷ሺ𝑎𝑋൅𝑏ሻ
𝑋
𝜎ଶ.
𝑋
𝑐.
𝑃ሼ0 ൑𝑋൑𝑐ሽൌ1.
Varሺ𝑋ሻ൑𝑐ଶ
4
𝐸ሾ𝑋ଶሿ൑𝑐𝐸ሾ𝑋ሿ
Varሺ𝑋ሻ൑𝑐ଶሾ𝛼ሺ1 െ𝛼ሻሿ where 𝛼ൌ𝐸ሾ𝑋ሿ
𝑐
𝑍
𝑥൐0,
𝑃ሼ𝑍൐𝑥ሽൌ𝑃ሼ𝑍൏െ𝑥ሽ;
𝑃ሼ|𝑍| ൐𝑥ሽൌ2𝑃ሼ𝑍൐𝑥ሽ;
𝑃ሼ|𝑍|  ൏ 𝑥ሽൌ2𝑃ሼ𝑍൏𝑥ሽെ1 .
𝑓ሺ𝑥ሻ
𝜇
𝜎ଶ.
𝜇െ𝜎
𝜇൅𝜎
𝑓ᇱᇱሺ𝑥ሻൌ0
𝑥ൌ𝜇െ𝜎
𝑥ൌ𝜇൅𝜎.
371 of 848

5.11. Let  be a standard normal random variable 
 and let  be a
differentiable function with derivative 
a. Show that 
b. Show that 
c. Find 
5.12. Use the identity of Theoretical Exercises 5.5
 to derive 
 when 
is an exponential random variable with parameter 
5.13. The median of a continuous random variable having distribution function
 is that value 
 such that 
 That is, a random variable is just as
likely to be larger than its median as it is to be smaller. Find the median of  if
 is
a. uniformly distributed over (
);
b. normal with parameters 
c. exponential with rate 
5.14. The mode of a continuous random variable having density  is the value
of  for which 
 attains its maximum. Compute the mode of  in cases (a),
(b), and (c) of Theoretical Exercises 5.13
.
5.15. If  is an exponential random variable with parameter 
 and 
show that cX is exponential with parameter 
5.16. Compute the hazard rate function of  when  is uniformly distributed
over (0, ).
5.17. If  has hazard rate function 
 compute the hazard rate function of
aX where  is a positive constant.
5.18. Verify that the gamma density function integrates to 1.
5.19. If  is an exponential random variable with mean 
 show that
Hint: Make use of the gamma density function to evaluate 
5.20. Verify that
when  is a gamma random variable with parameters  and 
5.21. Show that 
Hint:
 Make the change of variables 
 and
then relate the resulting expression to the normal distribution.
𝑍
𝑍,
𝑔
𝑔'.
 𝐸ሾ𝑔'ሺ𝑍ሻሿൌ𝐸ሾ𝑍𝑔ሺ𝑍ሻሿ;
 𝐸ሾ𝑍௡൅ଵሿൌ𝑛𝐸ሾ𝑍௡െଵሿ.
𝐸ሾ𝑍ସሿ.
𝐸ሾ𝑋ଶሿ
𝑋
𝜆.
𝐹
𝑚
𝐹ሺ𝑚ሻൌ1
2 .
𝑋
𝑋
𝑎, 𝑏
𝜇, 𝜎ଶ;
𝜆.
𝑓
𝑥
𝑓ሺ𝑥ሻ
𝑋
𝑋
𝜆,
𝑐൐0,
𝜆/𝑐.
𝑋
𝑋
𝑎
𝑋
𝜆௑ሺ𝑡ሻ,
𝑎
𝑋
1/𝜆,
𝐸ሾ𝑋௞ሿൌ𝑘!
𝜆௞  𝑘ൌ1, 2, ...
𝐸ሾ𝑋௞ሿ.
Varሺ𝑋ሻൌ𝛼
𝜆ଶ
𝑋
𝛼
𝜆.
Γ ቆ1
2ቇൌ
𝜋
√.
Γ ቆ1
2ቇൌ඲
଴
ஶ
𝑒െ௫𝑥െଵ/ଶ𝑑𝑥.
𝑦ൌ
2𝑥
√
372 of 848

5.22. Compute the hazard rate function of a gamma random variable with
parameters 
 and show it is increasing when 
 and decreasing when
5.23. Compute the hazard rate function of a Weibull random variable and
show it is increasing when 
 and decreasing when 
5.24. Show that a plot of 
 against log  will be a straight
line with slope  when 
 is a Weibull distribution function. Show also that
approximately 63.2 percent of all observations from such a distribution will be
less than 
 Assume that 
5.25. Let
Show that if  is a Weibull random variable with parameters 
 and 
 then
 is an exponential random variable with parameter 
 and vice versa.
5.26. Let  be a continuous distribution function. If  is uniformly distributed
on 
 find the distribution function of 
 where 
 is the inverse
function of 
 (That is, 
 if 
)
5.27. If  is uniformly distributed over (
), what random variable, having a
linear relation with 
 is uniformly distributed over (0, 1)?
5.28. Consider the beta distribution with parameters (
). Show that
a. when 
 and 
 the density is unimodal (that is, it has a unique
mode) with mode equal to 
b. when 
 and 
 the density is either unimodal with
mode at 0 or 1 or U-shaped with modes at both 0 and 1;
c. when 
 all points in [0, 1] are modes.
5.29. Let  be a continuous random variable having cumulative distribution
function F. Define the random variable  by 
 Show that  is uniformly
distributed over (0, 1).
5.30. Let  have probability density 
 Find the probability density function of
the random variable  defined by 
5.31. Find the probability density function of 
 when  is normally
distributed with parameters  and 
 The random variable  is said to have a
lognormal distribution (since log  has a normal distribution) with parameters
 and 
5.32. Let  and  be independent random variables that are both equally likely
to be either 
 where  is very large. Let  denote the greatest
common divisor of  and 
 and let 
a. Give a heuristic argument that 
Hint: Note that in order for  to equal 
 must divide both  and 
ሺ𝛼,𝜆ሻ
𝛼൒1
𝛼൑1.
𝛽൒1
𝛽൑1.
logሺlogሺ1 െ𝐹ሺ𝑥ሻሻെଵሻ
𝑥
𝛽
𝐹ሺ⋅ሻ
𝛼.
𝑣ൌ0.
𝑌ൌቆ𝑋െ𝜈
𝛼
ቇ
ఉ
𝑋
𝜈, 𝛼,
𝛽,
𝑌
𝜆ൌ1
𝐹
𝑈
ሺ0, 1ሻ,
𝑌ൌ𝐹െଵሺ𝑈ሻ,
𝐹െଵ
𝐹.
𝑦ൌ𝐹െଵሺ𝑥ሻ
𝐹ሺ𝑦ሻൌ𝑥.
𝑋
𝑎, 𝑏
𝑋,
𝑎, 𝑏
𝑎൐1
𝑏൐1,
ሺ𝑎െ1ሻ/ሺ𝑎൅𝑏െ2ሻ;
𝑎൑1, 𝑏൑1,
𝑎൅𝑏൏2,
𝑎ൌ1 ൌ𝑏,
𝑋
𝑌
𝑌ൌ𝐹ሺ𝑋ሻ.
𝑌
𝑋
𝑓௑.
𝑌
𝑌ൌ𝑎𝑋൅𝑏.
𝑌ൌ𝑒௑
𝑋
𝜇
𝜎ଶ.
𝑌
𝑌
𝜇
𝜎ଶ.
𝑋
𝑌
1, 2, ... , ሺ10ሻே,
𝑁
𝐷
𝑋
𝑌,
𝑄௞ൌ𝑃ሼ𝐷ൌ𝑘ሽ.
𝑄௞ൌ
ଵ
௞మ𝑄ଵ.
𝐷
𝑘, 𝑘
𝑋
𝑌
373 of 848

and also 
 and 
 must be relatively prime. (That is, 
 and
 must have a greatest common divisor equal to 1.)
b. Use part (a) to show that
It is a well-known identity that 
 so 
 (In
number theory, this is known as the Legendre theorem.)
c. Now argue that
where 
 is the ith-smallest prime greater than 1.
Hint: X and Y will be relatively prime if they have no common prime
factors. Hence, from part (b), we see that
5.33. Prove Theorem 7.1
 when 
 is a decreasing function.
𝑋/𝑘,
𝑌/𝑘
𝑋/𝑘,
𝑌/𝑘
𝑄ଵ
ൌ𝑃ሼ𝑋 and 𝑌 are relatively primeሽ
ൌ
ଵ
෍
ೖൌభ
ಮ
ଵ/௞మ
෍
ଵ
ஶ
1/𝑘ଶൌ𝜋ଶ/6,
𝑄ଵൌ6/𝜋ଶ.
𝑄ଵൌෑ
௜ൌଵ
ஶ
ቆ𝑃௜
ଶെ1
𝑃௜
ଶ
ቇ
𝑃௜
ෑ
௜ൌଵ
ஶ
ቆ𝑃௜
ଶെ1
𝑃௜
ଶ
ቇൌ6
𝜋ଶ
𝑔ሺ𝑥ሻ
5.1. The number of minutes of playing time of a certain high school basketball
player in a randomly chosen game is a random variable whose probability
density function is given in the following figure:
Find the probability that the player plays
a. more than 15 minutes;
b. between 20 and 35 minutes;
c. less than 30 minutes;
d. more than 36 minutes.
374 of 848

5.2. For some constant 
 the random variable  has the probability density
function
Find (a)  and (b) 
5.3. For some constant 
 the random variable  has the probability density
function
Find (a) [ ] and (b) Var
5.4. The random variable  has the probability density function
If 
 find (a) 
 and (b) Var
5.5. The random variable  is said to be a discrete uniform random variable
on the integers 
 if
For any nonnegative real number 
 let Int
 (sometimes written as [ ]) be
the largest integer that is less than or equal to 
 Show that if  is a uniform
random variable on (0, 1), then 
 is a discrete uniform random
variable on 
5.6. Your company must make a sealed bid for a construction project. If you
succeed in winning the contract (by having the lowest bid), then you plan to
pay another firm $100,000 to do the work. If you believe that the minimum bid
(in thousands of dollars) of the other participating companies can be modeled
as the value of a random variable that is uniformly distributed on (70, 140),
how much should you bid to maximize your expected profit?
5.7. To be a winner in a certain game, you must be successful in three
successive rounds. The game depends on the value of 
 a uniform random
variable on (0, 1). If 
 then you are successful in round 1; if 
 then
you are successful in round 2; and if 
 then you are successful in round
3.
a. Find the probability that you are successful in round 1.
b. Find the conditional probability that you are successful in round 2 given
that you were successful in round 1.
c. Find the conditional probability that you are successful in round 3 given
𝑐,
𝑋
𝑓ሺ𝑥ሻൌቊ𝑐𝑥௡0 ൏𝑥൏1
0
otherwise
𝑐
𝑃ሼ𝑋൐𝑥ሽ, 0 ൏𝑥൏1.
𝑐,
𝑋
𝑓ሺ𝑥ሻൌቊ𝑐𝑥ସ0 ൏𝑥൏2
0
otherwise
𝐸𝑋
ሺ𝑋ሻ.
𝑋
𝑓ሺ𝑥ሻൌቊ𝑎𝑥൅𝑏𝑥ଶ
0 ൏𝑥൏1
0
otherwise
𝐸ሾ𝑋ሿൌ.6,
𝑃ሼ𝑋൏1
2ሽ
ሺ𝑋ሻ.
𝑋
1,2, ..., 𝑛
𝑃ሼ𝑋ൌ𝑖ሽൌ1
𝑛 𝑖ൌ1,2,..., 𝑛
𝑥,
ሺ𝑥ሻ
𝑥
𝑥.
𝑈
𝑋ൌInt ሺ𝑛𝑈ሻ൅1
1, ..., 𝑛.
𝑈,
𝑈൐.1,
𝑈൐.2,
𝑈൐.3,
375 of 848

that you were successful in rounds 1 and 2.
d. Find the probability that you are a winner.
5.8. A randomly chosen IQ test taker obtains a score that is approximately a
normal random variable with mean 100 and standard deviation 15. What is the
probability that the score of such a person is (a) more than 125; (b) between
90 and 110?
5.9. Suppose that the travel time from your home to your office is normally
distributed with mean 40 minutes and standard deviation 7 minutes. If you
want to be 95 percent certain that you will not be late for an office appointment
at 1 ඘.ඕ., what is the latest time that you should leave home?
5.10. The life of a certain type of automobile tire is normally distributed with
mean 34,000 miles and standard deviation 4000 miles.
a. What is the probability that such a tire lasts more than 40,000 miles?
b. What is the probability that it lasts between 30,000 and 35,000 miles?
c. Given that it has survived 30,000 miles, what is the conditional
probability that the tire survives another 10,000 miles?
5.11. The annual rainfall in Cleveland, Ohio, is approximately a normal
random variable with mean 40.2 inches and standard deviation 8.4 inches.
What is the probability that
a. next year’s rainfall will exceed 44 inches?
b. the yearly rainfalls in exactly 3 of the next 7 years will exceed 44
inches?
Assume that if 
 is the event that the rainfall exceeds 44 inches in year 
(from now), then the events 
 are independent.
5.12. The following table uses 1992 data concerning the percentages of male
and female full-time workers whose annual salaries fall into different ranges:
Earnings range
Percentage of female
Percentage of males
9999
8.6
4.4
10,000–19,999
38.0
21.1
20,000–24,999
19.4
15.8
25,000–49,999
29.2
41.5
50,000
4.8
17.2
𝐴௜
𝑖
𝐴௜, 𝑖൒1,
൑
൒
376 of 848

Suppose that random samples of 200 male and 200 female full-time workers
are chosen. Approximate the probability that
a. at least 70 of the women earn $25,000 or more;
b. at most 60 percent of the men earn $25,000 or more;
c. at least three-fourths of the men and at least half the women earn
$20,000 or more.
5.13. At a certain bank, the amount of time that a customer spends being
served by a teller is an exponential random variable with mean 5 minutes. If
there is a customer in service when you enter the bank, what is the probability
that he or she will still be with the teller after an additional 4 minutes?
5.14. Suppose that the cumulative distribution function of the random variable
 is given by
Evaluate (a) 
 (b) 
 (c) the hazard rate function of 
 (d) 
[ ]; (e) Var
Hint: For parts (d) and (e), you might want to make use of the results of
Theoretical Exercise 5.5.
5.15. The number of years that a washing machine functions is a random
variable whose hazard rate function is given by
a. What is the probability that the machine will still be working 6 years
after being purchased?
b. If it is still working 6 years after being purchased, what is the
conditional probability that it will fail within the next 2 years?
5.16. A standard Cauchy random variable has density function
Show that if  is a standard Cauchy random variable, then 1/  is also a
standard Cauchy random variable.
5.17. A roulette wheel has 38 slots, numbered 0, 00, and 1 through 36. If you
bet 1 on a specified number, then you either win 35 if the roulette ball lands on
that number or lose 1 if it does not. If you continually make such bets,
approximate the probability that
a. you are winning after 34 bets;
𝑋
𝐹ሺ𝑥ሻൌ1 െ𝑒െ௫మ 𝑥൐0
𝑃ሼ𝑋൐2ሽ;
𝑃ሼ1 ൏𝑋൏3ሽ;
𝐹;
𝐸
𝑋
ሺ𝑋ሻ.
𝜆ሺ𝑡ሻൌ൞
.2
0 ൏𝑡൏2
.2 ൅.3ሺ𝑡െ2ሻ2 ൑𝑡൏5
1.1
𝑡൐5
𝑓ሺ𝑥ሻൌ
1
𝜋ሺ1 ൅𝑥ଶሻ  െ∞൏𝑥൏∞
𝑋
𝑋
377 of 848

b. you are winning after 1000 bets;
c. you are winning after 100,000 bets.
Assume that each roll of the roulette ball is equally likely to land on any of the
38 numbers.
5.18. There are two types of batteries in a bin. When in use, type  batteries
last (in hours) an exponentially distributed time with rate 
 A battery
that is randomly chosen from the bin will be a type  battery with probability
 If a randomly chosen battery is still operating after  hours of
use, what is the probability that it will still be operating after an additional 
hours?
5.19. Evidence concerning the guilt or innocence of a defendant in a criminal
investigation can be summarized by the value of an exponential random
variable  whose mean  depends on whether the defendant is guilty. If
innocent, 
 if guilty, 
 The deciding judge will rule the defendant
guilty if 
 for some suitably chosen value of 
a. If the judge wants to be 
 percent certain that an innocent man will not
be convicted, what should be the value of c?
b. Using the value of c found in part (a), what is the probability that a
guilty defendant will be convicted?
5.20. For any real number 
 define 
 by
Let  be a constant.
a. Show that
when  is a standard normal random variable.
b. Find 
 when  is normal with mean  and variance 
5.21. With 
 being the probability that a normal random variable with
mean  and variance  is less than 
 which of the following are true:
a. 
b. 
c. 
5.22. Let  be a uniform 
 random variable, and let 
 be constants.
a. Show that if 
 then 
 is uniformly distributed on 
 and if
𝑖
𝜆௜,  𝑖ൌ1,  2. .
𝑖
𝑝௜, ෍
௜ൌଵ
ଶ
𝑝௜ൌ1.
𝑡
𝑠
𝑋
𝜇
𝜇ൌ1;
𝜇ൌ2.
𝑋൐𝑐
𝑐.
95
𝑦,
𝑦൅
𝑦൅ൌ𝑦,
if 𝑦൒0
0,
if 𝑦൏0
𝑐
𝐸ሾሺ𝑍െ𝑐ሻ൅ሿൌ
1
2𝜋
√
𝑒െ௖మ/ଶെ𝑐ሺ1 െΦ ሺ𝑐ሻሻ
𝑍
𝐸ሾሺ𝑋െ𝑐ሻ൅ሿ
𝑋
𝜇
𝜎ଶ.
Φ ሺ𝑥ሻ
0
1
𝑥,
Φ ሺെ𝑥ሻൌΦ ሺ𝑥ሻ
Φ ሺ𝑥ሻ൅Φ ሺെ𝑥ሻൌ1
Φ ሺെ𝑥ሻൌ1/ Φ ሺ𝑥ሻ
𝑈
ሺ0, 1ሻ
𝑎൏𝑏
𝑏൐0,
𝑏𝑈
ሺ0, 𝑏ሻ,
378 of 848

6.1 Joint Distribution Functions
 then 
 is uniformly distributed on 
b. Show that 
 is uniformly distributed on 
c. What function of  is uniformly distributed on 
d. Show that 
 is a uniform 
 random variable.
e. Show that 
 is a uniform 
 random variable.
5.23. Let
a. Show that  is a probability density function. (That is, show that
 and 
)
b. If  has density function 
 find 
5.24. Let
where 
a. Show that 
 is a density function. That is, show that 
 and
that 
b. Find 
c. Find 
𝑏൏0,
𝑏𝑈
ሺ𝑏, 0ሻ.
𝑎൅𝑈
ሺ𝑎, 1 ൅𝑎ሻ.
𝑈
ሺ𝑎, 𝑏ሻ?
minሺ𝑈, 1 െ𝑈ሻ
ሺ0, 1/2ሻ
maxሺ𝑈, 1 െ𝑈ሻ
ሺ1/2, 1ሻ
𝑓ሺ𝑥ሻൌ
⎧
⎨
⎩
⎪
⎪
ଵ
ଷ𝑒௫,
if  𝑥൏0
ଵ
ଷ
if  0 ൑𝑥൏1
ଵ
ଷ𝑒െሺ௫െଵሻ
if  𝑥൒1
𝑓
𝑓ሺ𝑥ሻ൒0,
඲
െஶ
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥ൌ1.
𝑋
𝑓,
𝐸ሾ𝑋ሿ.
𝑓ሺ𝑥ሻൌ
𝜃ଶ
1 ൅𝜃ሺ1 ൅𝑥ሻ𝑒െఏ௫, 𝑥൐0
𝜃൐0.
𝑓ሺ𝑥ሻ
𝑓ሺ𝑥ሻ൒0,
඲
଴
ஶ
𝑓ሺ𝑥ሻ𝑑𝑥ൌ1.
𝐸ሾ𝑋ሿ
Varሺ𝑋ሻ.
379 of 848

6.2 Independent Random Variables
6.3 Sums of Independent Random Variables
6.4 Conditional Distributions: Discrete Case
6.5 Conditional Distributions: Continuous Case
6.6 Order Statistics
6.7 Joint Probability Distribution of Functions of Random Variables
6.8 Exchangeable Random Variables
Thus far, we have concerned ourselves only with probability distributions for single
random variables. However, we are often interested in probability statements
concerning two or more random variables. In order to deal with such probabilities, we
define, for any two random variables  and , the joint cumulative probability
distribution function of  and  by
All joint probability statements about  and  can, in theory, be answered in terms of
their joint distribution function. For instance,
whenever 
. To verify Equation (1.1)
, note that for 
,
giving that
Also, because for 
,
we have that when 
, 
𝑋
𝑌
𝑋
𝑌
𝐹ሺ𝑎, 𝑏ሻൌ𝑃ሼ𝑋൑𝑎, 𝑌൑𝑏ሽ  െ∞൏𝑎, 𝑏൏∞
𝑋
𝑌
𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑏ଵ൏𝑌൑𝑏ଶሻൌ𝐹ሺ𝑎ଶ, 𝑏ଶሻ൅𝐹ሺ𝑎ଵ, 𝑏ଵሻെ𝐹ሺ𝑎ଵ, 𝑏ଶሻെ𝐹ሺ𝑎ଶ, 𝑏ଵሻ
(1.1)
𝑎ଵ൏𝑎ଶ,  𝑏ଵ൏𝑏ଶ
𝑎ଵ൏𝑎ଶ
𝑃ሺ𝑋൑𝑎ଶ,  𝑌൑𝑏ሻൌ𝑃ሺ𝑋൑𝑎ଵ,  𝑌൑𝑏ሻ൅𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑌൑𝑏ሻ
𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑌൑𝑏ሻൌ𝐹ሺ𝑎ଶ, 𝑏ሻെ𝐹ሺ𝑎ଵ, 𝑏ሻ
(1.2)
𝑏ଵ൏𝑏ଶ
𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑌൑𝑏ଶሻൌ𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑌൑𝑏ଵሻ൅𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑏ଵ൏𝑌൑𝑏ଶሻ
𝑎ଵ൏𝑎ଶ𝑏ଵ൏𝑏ଶ
380 of 848

where the final equality used Equation (1.2)
.
When  and  are discrete random variables, with  taking on one of the values 
,
, and  one of the values 
 it is convenient to define the joint probability
mass function of  and  by
Using that the event 
 is the union of the mutually exclusive events
, 
, it follows that the probability mass function of  can be
obtained from the joint probability mass function by
Similarly, the probability mass function of  is obtained from
Example 1a
Suppose that 3 balls are randomly selected from an urn containing 3 red, 4 white,
and 5 blue balls. If we let  and  denote, respectively, the number of red and
white balls chosen, then the joint probability mass function of  and
 is obtained by noting that 
 if, of the  balls
selected,  are red,  are white, and 
 are blue. Because all subsets of
size  are equally likely to be chosen, it follows that
𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑏ଵ൏𝑌൑𝑏ଶሻ
ൌ𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑌൑𝑏ଶሻെ𝑃ሺ𝑎ଵ൏𝑋൑𝑎ଶ,  𝑌൑𝑏ଵሻ
ൌ𝐹ሺ𝑎ଶ, 𝑏ଶሻെ𝐹ሺ𝑎ଵ, 𝑏ଶሻെ𝐹ሺ𝑎ଶ, 𝑏ଵሻ൅𝐹ሺ𝑎ଵ, 𝑏ଵሻ
𝑋
𝑌
𝑋
𝑥௜
𝑖൒1
𝑌
𝑦௝,  𝑗൒1,
𝑋
𝑌
𝑝ሺ𝑥, 𝑦ሻൌ𝑃ሺ𝑋ൌ𝑥, 𝑌ൌ𝑦ሻ
ሼ𝑋ൌ𝑥ሽ
ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦௝ቅ𝑗൒1
𝑋
𝑝௑ሺ𝑥ሻ
ൌ𝑃ሺ𝑋ൌ𝑥ሻ
ൌ𝑃ቀ
௝ቄ𝑋ൌ𝑥, 𝑌ൌ𝑦௝ቅቁ
ൌ෍
௝
𝑃ሺ𝑋ൌ𝑥, 𝑌ൌ𝑦௝ሻ
ൌ෍
௝
𝑝ሺ𝑥, 𝑦௝ሻ
𝑌
𝑝௒ሺ𝑦ሻൌ෍
௜
𝑝ሺ𝑥௜, 𝑦ሻ
𝑋
𝑌
𝑋
𝑌, 𝑝ሺ𝑖, 𝑗ሻൌ𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ,
𝑋ൌ𝑖, 𝑌ൌ𝑗
3
𝑖
𝑗
3 െ𝑖െ𝑗
3
𝑝ሺ𝑖, 𝑗ሻൌ
ቆ3
𝑖ቇቆ4
𝑗ቇቆ
5
3 െ𝑖െ𝑗ቇ
ቆ12
3 ቇ
381 of 848

Consequently,
These probabilities can most easily be expressed in tabular form, as in Table
6.1
. The reader should note that the probability mass function of  is obtained
by computing the row sums, whereas the probability mass function of  is
obtained by computing the column sums. Because the individual probability mass
functions of  and  thus appear in the margin of such a table, they are often
referred to as the marginal probability mass functions of  and 
 respectively.
Table 6.1 
𝑝ሺ0, 0ሻൌቆ5
3ቇ/ቆ12
3 ቇൌ10
220
𝑝ሺ0, 1ሻൌቆ4
1ቇቆ5
2ቇ/ቆ12
3 ቇൌ40
220
𝑝ሺ0, 2ሻൌቆ4
2ቇቆ5
1ቇ/ቆ12
3 ቇൌ30
220
𝑝ሺ0, 3ሻൌቆ4
3ቇ/ቆ12
3 ቇൌ
4
220
𝑝ሺ1, 0ሻൌቆ3
1ቇቆ5
2ቇ/ቆ12
3 ቇൌ30
220
𝑝ሺ1, 1ሻൌቆ3
1ቇቆ4
1ቇቆ5
1ቇ/ቆ12
3 ቇൌ60
220
𝑝ሺ1, 2ሻൌቆ3
1ቇቆ4
2ቇ/ቆ12
3 ቇൌ18
220
𝑝ሺ2, 0ሻൌቆ3
2ቇቆ5
1ቇ/ቆ12
3 ቇൌ15
220
𝑝ሺ2, 1ሻൌቆ3
2ቇቆ4
1ቇ/ቆ12
3 ቇൌ12
220
𝑝ሺ3, 0ሻൌቆ3
3ቇ/ቆ12
3 ቇൌ
1
220
𝑋
𝑌
𝑋
𝑌
𝑋
𝑌,
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ.
382 of 848

Example 1b
Suppose that 15 percent of the families in a certain community have no children,
20 percent have 1 child, 35 percent have 2 children, and 30 percent have 3.
Suppose further that in each family each child is equally likely (independently) to
be a boy or a girl. If a family is chosen at random from this community, then 
the number of boys, and 
 the number of girls, in this family will have the joint
probability mass function shown in Table 6.2
.
Table 6.2 
The probabilities shown in Table 6.2
 are obtained as follows:
𝐵,
𝐺,
𝑃ሼ𝐵ൌ𝑖, 𝐺ൌ𝑗ሽ.
383 of 848

We leave the verification of the remaining probabilities in the table to the reader.
Example 1c
Consider independent trials where each trial is a success with probability 
 Let
 denote the number of trials until there have been  successes, and let 
denote the number of trials until there have been  failures. Suppose we want to
derive their joint probability mass function 
 To do so, first
consider the case 
 In this case, write
Now, if there have been  successes after trial  then there have been 
failures by that point. Hence, the conditional distribution of 
 given that 
is the distribution of  plus the number of additional trials after trial  until there
have been an additional 
 failures. Hence,
Because 
 is a negative binomial random variable with parameters 
 and
 is a negative binomial random variable with parameters 
the preceding yields
We leave it as an exercise to determine the analogous expression when 
We say that  and  are jointly continuous if there exists a function 
 defined
for all real  and 
 having the property that for every set  of pairs of real numbers
(that is,  is a set in the two-dimensional plane),
𝑃ሼ𝐵ൌ0, 𝐺ൌ0ሽ
ൌ𝑃ሼ no children ሽൌ.15
𝑃ሼ𝐵ൌ0, 𝐺ൌ1ሽ
ൌ𝑃ሼ 1 girl and total of 1 child ሽ
ൌ𝑃ሼ 1 child ሽ𝑃ቊ 1 girl | 1 child ቋൌሺ.20ሻቆ1
2ቇ
𝑃ሼ𝐵ൌ0, 𝐺ൌ2ሽ
ൌ𝑃ሼ 2 girls and total of 2 children ሽ
ൌ𝑃ሼ 2 children ሽ𝑃ሼ 2 girls | 2 children ሽൌሺ.35ሻቆ1
2ቇ
ଶ
𝑝.
𝑋௥
𝑟
𝑌௦
𝑠
𝑃ሺ𝑋௥ൌ𝑖, 𝑌௦ൌ𝑗ሻ.
𝑖൏𝑗.
𝑃ሺ𝑋௥ൌ𝑖, 𝑌௦ൌ𝑗ሻൌ𝑃ሺ𝑋௥ൌ𝑖ሻ𝑃ሺ𝑌௦ൌ𝑗||𝑋௥ൌ𝑖ሻ
𝑟
𝑖
𝑖െ𝑟
𝑌௦,
𝑋௥ൌ𝑖,
𝑖
𝑖
𝑠െ𝑖൅𝑟
𝑃ሺ𝑋௥ൌ𝑖, 𝑌௦ൌ𝑗ሻൌ𝑃ሺ𝑋௥ൌ𝑖ሻ𝑃ሺ𝑌௦െ௜൅௥ൌ𝑗െ𝑖ሻ ,  𝑖൏𝑗
𝑋௥
ሺ𝑟, 𝑝ሻ
𝑌௦െ௜൅௥
ሺ𝑠െ𝑖൅𝑟, 1 െ𝑝ሻ,
𝑃ሺ𝑋௥ൌ𝑖, 𝑌௦ൌ𝑗ሻൌቆ𝑖െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௜െ௥ ቆ
𝑗െ𝑖െ1
𝑠െ𝑖൅𝑟െ1ቇሺ1 െ𝑝ሻ௦െ௜൅௥𝑝௝െ௦െ௥ ,  𝑖൏𝑗
𝑗൏𝑖.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻ,
𝑥
𝑦,
𝐶
𝐶
384 of 848

The function 
 is called the joint probability density function of  and 
 If  and
 are any sets of real numbers, then by defining 
 we see
from Equation (1.3)
 that
Because
it follows, upon differentiation, that
wherever the partial derivatives are defined. Another interpretation of the joint density
function, obtained from Equation (1.4)
, is
when 
 and 
 are small and 
 is continuous at 
 Hence, 
 is a
measure of how likely it is that the random vector (
) will be near (
).
If  and  are jointly continuous, they are individually continuous, and their
probability density functions can be obtained as follows:
where
𝑃ሼሺ𝑋, 𝑌ሻ∈𝐶ሽൌ
׬׬
ሺ௫, ௬ሻ∈஼ 𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
(1.3)
𝑓ሺ𝑥, 𝑦ሻ
𝑋
𝑌.
𝐴
𝐵
𝐶ൌሼሺ𝑥, 𝑦ሻ:𝑥∈𝐴, 𝑦∈𝐵ሽ,
𝑃ሼ𝑋∈𝐴, 𝑌∈𝐵ሽൌ඲
஻
඲
஺
𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
(1.4)
𝐹ሺ𝑎, 𝑏ሻ
ൌ𝑃ሼ𝑋∈ሺെ∞,  𝑎ሿ, 𝑌∈ሺെ∞, 𝑏ሿሽ
ൌ׬െஶ
௕
׬െஶ
௔
𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
𝑓ሺ𝑎, 𝑏ሻൌ
∂ଶ
∂𝑎∂𝑏𝐹ሺ𝑎, 𝑏ሻ
𝑃ሼ𝑎൏𝑋൏𝑎൅𝑑𝑎, 𝑏൏𝑌൏𝑏൅𝑑𝑏ሽ
ൌ׬௕
ௗ൅ௗ௕׬௔
௔൅ௗ௔𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
ൎ𝑓ሺ𝑎, 𝑏ሻ 𝑑𝑎 𝑑𝑏
𝑑𝑎
𝑑𝑏
𝑓ሺ𝑥, 𝑦ሻ
𝑎, 𝑏.
𝑓ሺ𝑎, 𝑏ሻ
𝑋, 𝑌
𝑎, 𝑏
𝑋
𝑌
𝑃ሼ𝑋∈𝐴ሽ
ൌ𝑃ሼ𝑋∈𝐴, 𝑌∈ሺെ∞, ∞ሻሽ
ൌ׬஺׬െஶ
ஶ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑦 𝑑𝑥
ൌ׬஺𝑓௑ሺ𝑥ሻ 𝑑𝑥
385 of 848

is thus the probability density function of 
 Similarly, the probability density function
of  is given by
Example 1d
The joint density function of  and  is given by
Compute (a) 
 (b) 
 and (c) 
Solution
a. 
Now,
giving that
𝑓௑ሺ𝑥ሻൌ඲
െஶ
ஶ
𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑦
𝑋.
𝑌
𝑓௒ሺ𝑦ሻൌ඲
െಮ
ஶ
𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌቊ2𝑒െ௫𝑒െଶ௬
 0 ൏𝑥൏∞,   0 ൏𝑦൏∞
0
 otherwise
𝑃ሼ𝑋൐1, 𝑌൏1ሽ,
𝑃ሼ𝑋൏𝑌ሽ,
𝑃ሼ𝑋൏𝑎ሽ.
𝑃ሺ𝑋൐1,  𝑌൏1ሻൌ඲
଴
ଵ
඲
ଵ
ஶ
2𝑒െ௫𝑒െଶ௬ 𝑑𝑥 𝑑𝑦
඲
ଵ
ஶ
𝑒െ௫𝑑𝑥ൌെ𝑒െ௫|ଵ
ஶൌ𝑒െଵ
𝑃ሺ𝑋൐1,  𝑌൏1ሻൌ𝑒െଵ඲
଴
ଵ
2𝑒െଶ௬𝑑𝑦ൌ𝑒െଵሺ1 െ𝑒െଶሻ
386 of 848

b. 
c. 
Example 1e
Consider a circle of radius 
 and suppose that a point within the circle is
randomly chosen in such a manner that all regions within the circle of equal area
are equally likely to contain the point. (In other words, the point is uniformly
distributed within the circle.) If we let the center of the circle denote the origin and
define  and  to be the coordinates of the point chosen (Figure 6.1
), then,
since (
) is equally likely to be near each point in the circle, it follows that the
joint density function of  and  is given by
Figure 6.1 Joint probability distribution.
𝑃ሼ𝑋൏𝑌ሽ
ൌ   
׬׬
ሺ௫, ௬ሻ: ௫ழ௬
2𝑒െ௫𝑒െଶ௬ 𝑑𝑥 𝑑𝑦
ൌ׬଴
ஶ׬଴
௬2𝑒െ௫𝑒െଶ௬ 𝑑𝑥 𝑑𝑦
ൌ׬଴
ஶ2𝑒െଶ௬ሺ1 െ𝑒െ௬ሻ𝑑𝑦
ൌ׬଴
ஶ2𝑒െଶ௬𝑑𝑦െ׬଴
ஶ2𝑒െଷ௬𝑑𝑦
ൌ1 െ2
3
ൌ1
3
𝑃ሼ𝑋൏𝑎ሽ
ൌ׬଴
௔׬଴
ஶ2𝑒െଶ௬𝑒െ௫ 𝑑𝑦 𝑑𝑥
ൌ׬଴
௔𝑒െ௫𝑑𝑥
ൌ1 െ𝑒െ௔
𝑅,
𝑋
𝑌
𝑋, 𝑌
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ൝𝑐
if  𝑥ଶ൅𝑦ଶ൑𝑅ଶ
0
if  𝑥ଶ൅𝑦ଶ൐𝑅ଶ
387 of 848

for some value of 
a. Determine 
b. Find the marginal density functions of  and 
c. Compute the probability that 
 the distance from the origin of the point
selected, is less than or equal to 
d. Find  [ ].
Solution
a. Because
it follows that
We can evaluate 
either by using polar coordinates or,
more simply, by noting that it represents the area of the circle and is thus
𝑐.
𝑐.
𝑋
𝑌.
𝐷,
𝑎.
𝐸𝐷
඲
െஶ
ஶ
඲
െஶ
ஶ
𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑦 𝑑𝑥ൌ1
𝑐    
඲඲
௫మ൅௬మ൑ோమ
 𝑑𝑦 𝑑𝑥ൌ1
׬׬௫మ൅௬మ൑ோమ 𝑑𝑦 𝑑𝑥
388 of 848

equal to 
 Hence,
b. 
and it equals 0 when 
 By symmetry, the marginal density of  is
given by
c. The distribution function of 
 the distance from the origin, is
obtained as follows: For 
where we have used the fact that 
is the area of a circle
of radius a and thus is equal to πα .
d. From part (c), the density function of  is
Hence,
𝜋𝑅ଶ.
𝑐ൌ
1
𝜋𝑅ଶ
𝑓௑ሺ𝑥ሻ
ൌ׬െஶ
ஶ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑦
ൌ
1
𝜋𝑅ଶ׬௫మ൅௬మ൑ோమ 𝑑𝑦
ൌ
1
𝜋𝑅ଶ׬
െೌ
௔ 𝑑𝑦,  where 𝑎ൌ
Rଶെxଶ
ඥ
ൌ
2
𝜋𝑅ଶ
𝑅ଶെ𝑥ଶ
ඥ
,  𝑥ଶ൑𝑅ଶ
𝑥ଶ൐𝑅ଶ.
𝑌
𝑓௒ሺ𝑦ሻ
ൌ
2
𝜋𝑅ଶ
𝑅ଶെ𝑦ଶ
ට
,
𝑦ଶ൑𝑅ଶ
ൌ0
𝑦ଶ൐𝑅ଶ
𝐷ൌ
𝑋ଶ൅𝑌ଶ
ඥ
,
0 ൑𝑎൑𝑅,
𝐹஽ሺ𝑎ሻ
ൌ𝑃ቄ𝑋ଶ൅𝑌ଶ
ඥ
൑𝑎ቅ
ൌ𝑃൛𝑋ଶ൅𝑌ଶ൑𝑎ଶൟ
ൌ 
׬׬
௫మ൅௬మ  ൑  ௔మ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑦 𝑑𝑥
ൌ
1
𝜋𝑅ଶ    
׬׬
௫మ൅௬మ  ൑  ௔మ 𝑑𝑦 𝑑𝑥
ൌ𝜋𝑎ଶ
𝜋𝑅ଶ
ൌ𝑎ଶ
𝑅ଶ
׬׬௫మ൅௬మ ൑ ௔మ 𝑑𝑦 𝑑𝑥
2
𝐷
𝑓஽ሺ𝑎ሻൌ2𝑎
𝑅ଶ 0 ൑𝑎൑𝑅
𝐸ሾ𝐷ሿൌ2
𝑅ଶ඲
଴
ோ
𝑎ଶ𝑑𝑎ൌ2𝑅
3
389 of 848

Example 1f
The joint density of  and  is given by
Find the density function of the random variable 
Solution
We start by computing the distribution function of 
 For 
Differentiation shows that the density function of 
 is given by
We can also define joint probability distributions for  random variables in exactly the
same manner as we did for 
 For instance, the joint cumulative probability
distribution function 
 of the  random variables 
 is defined
by
Further, the  random variables are said to be jointly continuous if there exists a
function 
 called the joint probability density function, such that, for any
set  in -space,
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌቊ𝑒െሺ௫൅௬ሻ0 ൏𝑥൏∞,   0 ൏𝑦൏∞
0
otherwise
𝑋/𝑌.
𝑋/𝑌.
𝑎൐0,
𝐹௑/௒ሺ𝑎ሻ
ൌ𝑃ቊ𝑋
𝑌൑𝑎ቋ
ൌ 
׬׬
௫/௬  ൑ ௔
 𝑒െሺ௫൅௬ሻ 𝑑𝑥 𝑑𝑦
ൌ׬଴
ஶ׬଴
௔௬𝑒െሺ௫൅௬ሻ 𝑑𝑥 𝑑𝑦
ൌ׬଴
ஶሺ1 െ𝑒െ௔௬ሻ𝑒െ௬𝑑𝑦
ൌቊെ𝑒െ௬൅𝑒െሺ௔൅ଵሻ௬
𝑎൅1 ቋቤ
଴
ஶ
ൌ1 െ
1
𝑎൅1
𝑋/𝑌
𝑓௑/௒ሺ𝑎ሻൌ1/ሺ𝑎൅1ሻଶ, 0 ൏𝑎൏∞.
𝑛
𝑛ൌ2 .
𝐹ሺ𝑎ଵ, 𝑎ଶ, … , 𝑎௡ሻ
𝑛
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝐹ሺ𝑎ଵ, 𝑎ଶ, … , 𝑎௡ሻൌ𝑃ሼ𝑋ଵ൑𝑎ଵ, 𝑋ଶ൑𝑎ଶ, … , 𝑋௡൑𝑎௡ሽ
𝑛
𝑓ሺ𝑥ଵ, 𝑥ଶ, … , 𝑥௡ሻ,
𝐶
𝑛
𝑃ሼሺ𝑋ଵ, 𝑋ଶ, … , 𝑋௡ሻ∈𝐶ሽൌ  
඲඲⋯඲
ሺ௫భ, …  , ௫೙ሻ∈஼
𝑓ሺ𝑥ଵ, … , 𝑥௡ሻ𝑑𝑥ଵ𝑑𝑥ଶ⋯ 𝑑𝑥௡
390 of 848

In particular, for any n sets of real numbers 
Example 1g The Multinomial Distribution
One of the most important joint distributions is the multinomial distribution, which
arises when a sequence of  independent and identical experiments is
performed. Suppose that each experiment can result in any one of  possible
outcomes, with respective probabilities 
 If we let 
denote the number of the  experiments that result in outcome number  then
whenever 
Equation (1.5)
 is verified by noting that any sequence of outcomes for the 
experiments that leads to outcome  occurring 
 times for 
 will, by
the assumed independence of experiments, have probability 
 of
occurring. Because there are 
 such sequences of outcomes
(there are 
 different permutations of  things of which 
 are alike, 
are alike, 
 are alike), Equation (1.5)
 is established. The joint distribution
whose joint probability mass function is specified by Equation (1.5)
 is called
the multinomial distribution. Note that when 
 the multinomial reduces to the
binomial distribution.
Note also that any sum of a fixed set of the 
will have a binomial distribution.
That is, if 
 then 
 will be a binomial random variable with
parameters  and 
This follows because 
 represents the
number of the  experiments whose outcome is in 
 and each experiment will
independently have such an outcome with probability 
As an application of the multinomial distribution, suppose that a fair die is rolled 9
times. The probability that 1 appears three times, 2 and 3 twice each, 4 and 5
once each, and 6 not at all is
𝐴ଵ, 𝐴ଶ, … , 𝐴௡,
𝑃ሼ𝑋ଵ∈𝐴ଵ, 𝑋ଶ∈𝐴ଶ, … , 𝑋௡∈𝐴௡ሽ
     ൌ׬஺೙׬஺೙െభ⋯׬஺భ𝑓ሺ𝑥ଵ, … , 𝑥௡ሻ 𝑑𝑥ଵ𝑑𝑥ଶ⋯𝑑𝑥௡
𝑛
𝑟
𝑝ଵ, 𝑝ଶ, … , 𝑝௥, ෍
௜ൌଵ
௥
𝑝௜ൌ1 .
𝑋௜
𝑛
𝑖,
𝑃ሼ𝑋ଵൌ𝑛ଵ, 𝑋ଶൌ𝑛ଶ, … , 𝑋௥ൌ𝑛௥ሽൌ
𝑛!
𝑛ଵ!𝑛ଶ!⋯𝑛௥! 𝑝ଵ
௡భ𝑝ଶ
௡మ⋯𝑝௥
௡ೝ
(1.5)
෍
௜ൌଵ
௥
𝑛௜ൌ𝑛.
𝑛
𝑖
𝑛௜
𝑖ൌ1, 2, … , 𝑟
𝑝ଵ
೙భ𝑝ଶ
೙మ… 𝑝௥
௡௥
𝑛!/ሺ𝑛ଵ!𝑛ଶ!… 𝑛௥!ሻ
𝑛!/𝑛ଵ! … 𝑛௥!
𝑛
𝑛ଵ
𝑛ଶ
… , 𝑛௥
𝑟ൌ2,
𝑋௜
'𝑠
𝑁⊂ሼ1, 2, … , 𝑟ሽ,
෍௜∈ே𝑋௜
𝑛
𝑝ൌ෍௜∈ே 𝑝௜.
෍௜∈ே 𝑋௜
𝑛
𝑁,
෍
௜∈ே
𝑝௜.
391 of 848

We can also use the multinomial distribution to analyze a variation of the
classical birthday problem which asks for the probability that no  people in a
group of size  have the same birthday when the birthdays of the  people are
independent and each birthday is equally likely to be any of the 
 days of the
year. Because this probability is  when 
 (why is this), we will suppose
that 
 To find the desired probability, note that there will be no set of 
people having the same birthday if each of the 
 days of the year is the
birthday of at most  persons. Now, this will be the case if for some 
 the
event 
 occurs, where 
 is the event that the 
 days of the year can be
partitioned into three groups of respective sizes 
 and 
 such
that every day in the first group is the birthday of exactly  of the  individuals,
every day in the second group is the birthday of exactly  of the  individuals,
and every day in the third group is the birthday of none of the  individuals. Now,
because each day of the year is equally likely to be the birthday of an individual,
it follows, for a given partition of the 
 days into three groups of respective
sizes 
 and 
 that the probability each day in the first group is
the birthday of exactly  of the  individuals, each day in the second group is the
birthday of exactly  of the  individuals, and each day in the third group is the
birthday of none of the  individuals is equal to the multinomial probability
As the number of partitions of the 
 days of the year into  groups of
respective sizes 
 is 
 it follows that
As the events 
 are mutually exclusive we have that
When 
 the preceding gives
9!
3!2!2!1!1!0! ቆ1
6ቇ
ଷ
ቆ1
6ቇ
ଶ
ቆ1
6ቇ
ଶ
ቆ1
6ቇ
ଵ
ቆ1
6ቇ
ଵ
ቆ1
6ቇ
଴
ൌ
9!
3!2!2! ቆ1
6ቇ
ଽ
3
𝑛
𝑛
365
0
𝑛൐730
𝑛൑730 .
3
365
2
𝑖൑𝑛/2
𝐴௜
𝐴௜
365
𝑖, 𝑛െ2𝑖,
365 െ𝑛൅𝑖
2
𝑛
1
𝑛
𝑛
365
𝑖, 𝑛െ2𝑖,
365 െ𝑛൅𝑖,
2
𝑛
1
𝑛
𝑛
𝑛!
ሺ2!ሻ௜ሺ1!ሻ௡െଶ௜ሺ0!ሻଷ଺ହെ௡൅௜ሺ1
365ሻ௡.
365
3
𝑖, 𝑛െ2𝑖, 365 െ𝑛൅𝑖
365!
𝑖!ሺ𝑛െ2𝑖ሻ!ሺ365 െ𝑛൅𝑖ሻ!,
𝑃ሺ𝐴௜ሻൌ
365!
𝑖!ሺ𝑛െ2𝑖ሻ!ሺ365 െ𝑛൅𝑖ሻ!   𝑛!
2௜ሺ1
365ሻ௡ ,  𝑖൑𝑛/2
𝐴௜, 𝑖൑𝑛/2,
𝑃ሼno set of three with same birthdayሽൌ
෍
௜ൌ଴
ሾ௡/ଶሿ
365!
𝑖!ሺ𝑛െ2𝑖ሻ!ሺ365 െ𝑛൅𝑖ሻ!   𝑛!
2௜ሺ1
365ሻ௡
𝑛ൌ88,
392 of 848

The random variables  and  are said to be independent if, for any two sets of real
numbers  and 
In other words,  and  are independent if, for all  and 
 the events 
and 
 are independent.
It can be shown by using the three axioms of probability that Equation (2.1)
 will
follow if and only if, for all 
Hence, in terms of the joint distribution function  of  and 
 and  are
independent if
When  and  are discrete random variables, the condition of independence (2.1) is
equivalent to
The equivalence follows because, if Equation (2.1)
 is satisfied, then we obtain
Equation (2.2)
 by letting  and  be, respectively, the one-point sets 
 and
 Furthermore, if Equation (2.2)
 is valid, then for any sets 
𝑃ሼno set of three with same birthdayሽൌ෍
௜ൌ଴
ସସ
365!
𝑖!ሺ88 െ2𝑖ሻ!ሺ277 ൅𝑖ሻ!
88!
2௜ሺ1
365ሻ଼଼ൎ. 504
𝑋
𝑌
𝐴
𝐵,
𝑃ሼ𝑋∈𝐴, 𝑌∈𝐵ሽൌ𝑃ሼ𝑋∈𝐴ሽ𝑃ሼ𝑌∈𝐵ሽ
(2.1)
𝑋
𝑌
𝐴
𝐵,
𝐸஺ൌሼ𝑋∈𝐴ሽ
𝐹஻ൌሼ𝑌∈𝐵ሽ
𝑎, 𝑏
𝑃ሼ𝑋൑𝑎, 𝑌൑𝑏ሽൌ𝑃ሼ𝑋൑𝑎ሽ𝑃ሼ𝑌൑𝑏ሽ
𝐹
𝑋
𝑌, 𝑋
𝑌
𝐹ሺ𝑎, 𝑏ሻൌ𝐹௑ሺ𝑎ሻ𝐹௒ሺ𝑏ሻ  for all  𝑎, 𝑏
𝑋
𝑌
𝑝ሺ𝑥, 𝑦ሻൌ𝑝௑ሺ𝑥ሻ𝑝௒ሺ𝑦ሻ  for all  𝑥, 𝑦
(2.2)
𝐴
𝐵
𝐴ൌሼ𝑥ሽ
𝐵ൌሼ𝑦ሽ.
𝐴, 𝐵,
393 of 848

and Equation (2.1)
 is established.
In the jointly continuous case, the condition of independence is equivalent to
Thus, loosely speaking,  and  are independent if knowing the value of one does
not change the distribution of the other. Random variables that are not independent
are said to be dependent.
Example 2a
Suppose that 
 independent trials having a common probability of success 
are performed. If  is the number of successes in the first  trials, and  is the
number of successes in the final 
 trials, then  and  are independent, since
knowing the number of successes in the first  trials does not affect the
distribution of the number of successes in the final 
 trials (by the assumption of
independent trials). In fact, for integral  and 
In contrast,  and  will be dependent, where  is the total number of successes
in the 
 trials. (Why?)
Example 2b
Suppose that the number of people who enter a post office on a given day is a
Poisson random variable with parameter 
 Show that if each person who enters
the post office is a male with probability  and a female with probability 
then the number of males and females entering the post office are independent
Poisson random variables with respective parameters 
 and 
𝑃ሼ𝑋∈𝐴, 𝑌∈𝐵ሽ
ൌ
෍
௬∈஻
෍
௫∈஺
𝑝ሺ𝑥, 𝑦ሻ
ൌ
෍
௬∈஻
෍
௫∈஺
𝑝௑ሺ𝑥ሻ𝑝௒ሺ𝑦ሻ
ൌ
෍
௬∈஻
𝑝௒ሺ𝑦ሻ෍
௫∈஺
𝑝௑ሺ𝑥ሻ
ൌ𝑃ሼ𝑌∈𝐵 ሽ𝑃ሼ𝑋∈𝐴ሽ
𝑓ሺ𝑥, 𝑦ሻൌ𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ  for all  𝑥, 𝑦
𝑋
𝑌
𝑛൅𝑚
𝑝
𝑋
𝑛
𝑌
𝑚
𝑋
𝑌
𝑛
𝑚
𝑥
𝑦,
𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽ
ൌቆ𝑛
𝑥ቇ 𝑝௫ሺ1 െ𝑝ሻ௡െ௫ቆ𝑚
𝑦ቇ 𝑝௬ሺ1 െ𝑝ሻ௠െ௬  0 ൑𝑥൑𝑛,
0 ൑𝑦൑𝑚
ൌ𝑃ሼ𝑋ൌ𝑥ሽ𝑃ሼ𝑌ൌ𝑦ሽ
𝑋
𝑍
𝑍
𝑛൅𝑚
𝜆.
𝑝
1 െ𝑝,
𝜆𝑝
𝜆ሺ1 െ𝑝ሻ.
394 of 848

Solution
Let  and  denote, respectively, the number of males and females that enter the
post office. We shall show the independence of  and  by establishing
Equation (2.2)
. To obtain an expression for 
 we condition on
whether or not 
 This gives:
[Note that this equation is merely a special case of the formula
]
Since 
 is clearly 0, we obtain
Now, because 
 is the total number of people who enter the post office, it
follows, by assumption, that
Furthermore, given that 
 people do enter the post office, since each person
entering will be male with probability 
 it follows that the probability that exactly 
of them will be male (and thus  of them female) is just the binomial probability
That is,
Substituting Equations (2.4)
 and (2.5
) into Equation (2.3)
 yields
𝑋
𝑌
𝑋
𝑌
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ,
𝑋൅𝑌ൌ𝑖൅𝑗.
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ
ൌ𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗||𝑋൅𝑌ൌ𝑖൅𝑗ሽ𝑃ሼ𝑋൅𝑌ൌ𝑖൅𝑗ሽ
  ൅𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗||𝑋൅𝑌്𝑖൅𝑗ሽ𝑃ሼ𝑋൅𝑌്𝑖൅𝑗ሽ
𝑃ሺ𝐸ሻൌ𝑃ሺ𝐸||𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐸||𝐹௖ሻ𝑃ሺ𝐹௖ሻ.
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗||𝑋൅𝑌്𝑖൅𝑗ሽ
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽൌ𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗||𝑋൅𝑌ൌ𝑖൅𝑗ሽ𝑃ሼ𝑋൅𝑌ൌ𝑖൅𝑗ሽ
(2.3)
𝑋൅𝑌
𝑃ሼ𝑋൅𝑌ൌ𝑖൅𝑗ሽൌ𝑒െఒ
𝜆௜൅௝
ሺ𝑖൅𝑗ሻ!
(2.4)
𝑖൅𝑗
𝑝,
𝑖
𝑗
ቆ𝑖൅𝑗
𝑖
ቇ𝑝௜ሺ1 െ𝑝ሻ௝.
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗|𝑋൅𝑌ൌ𝑖൅𝑗ሽൌቆ𝑖൅𝑗
𝑖
ቇ𝑝௜ሺ1 െ𝑝ሻ௝
(2.5)
395 of 848

Hence,
and similarly,
Equations (2.6)
, (2.7)
, and (2.8)
 establish the desired result.
Example 2c
A man and a woman decide to meet at a certain location. If each of them
independently arrives at a time uniformly distributed between 12 noon and 1 ඘.ඕ.,
find the probability that the first to arrive has to wait longer than 10 minutes.
Solution
If we let  and  denote, respectively, the time past 12 that the man and the
woman arrive, then  and  are independent random variables, each of which is
uniformly distributed over (0, 60). The desired probability,
 which, by symmetry, equals 
 is
obtained as follows:
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ
ൌቆ𝑖൅𝑗
𝑖
ቇ𝑝௜ሺ1 െ𝑝ሻ௝𝑒െఒ
𝜆௜൅௝
ሺ𝑖൅𝑗ሻ!
ൌ𝑒െఒሺ𝜆𝑝ሻ௜
𝑖!𝑗! ሾ𝜆ሺ1 െ𝑝ሻሿ௝
ൌ𝑒െఒሺ𝜆𝑝ሻ௜
𝑖!
𝑒െఒሺ1 െ𝑝ሻሾ𝜆ሺ1 െ𝑝ሻሿ௝
𝑗!
(2.6)
𝑃ሼ𝑋ൌ𝑖ሽൌ𝑒െఒ௣ሺ𝜆𝑝ሻ௜
𝑖!
෍
௝
𝑒െఒሺଵെ௣ሻൣ𝜆ሺ1 െ𝑝൯൧௝
𝑗!
ൌ𝑒െఒ௣ሺ𝜆𝑝ሻ௜
𝑖!
(2.7)
𝑃ሼ𝑌ൌ𝑗ሽൌ𝑒െఒሺଵെ௣ሻൣ𝜆ሺ1 െ𝑝൯൧௝
𝑗!
(2.8)
𝑋
𝑌
𝑋
𝑌
𝑃ሼ𝑋൅10 ൏𝑌ሽ൅𝑃ሼ𝑌൅10 ൏𝑋ሽ,
2𝑃ሼ𝑋൅10 ൏𝑌ሽ,
396 of 848

Our next example presents the oldest problem dealing with geometrical probabilities.
It was first considered and solved by Buffon, a French naturalist of the eighteenth
century, and is usually referred to as Buffon’s needle problem.
Example 2d Buffon’s Needle Problem
A table is ruled with equidistant parallel lines a distance  apart. A needle of
length 
 where 
 is randomly thrown on the table. What is the probability
that the needle will intersect one of the lines (the other possibility being that the
needle will be completely contained in the strip between two lines)?
Solution
Let us determine the position of the needle by specifying (1) the distance  from
the middle point of the needle to the nearest parallel line and (2) the angle 
between the needle and the projected line of length 
 (See Figure 6.2
.) The
needle will intersect a line if the hypotenuse of the right triangle in Figure 6.2
is less than 
—that is, if
Figure 6.2
As  varies between 0 and 
 and  between 0 and 
 it is reasonable to
2𝑃ሼ𝑋൅10 ൏𝑌ሽ
ൌ2 
׬׬
௫൅ଵ଴
ழ௬
 𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
ൌ2  
׬׬
௫൅ଵ଴
ழ௬
 𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑥 𝑑𝑦
ൌ2 ׬ଵ଴
଺଴׬଴
௬െଵ଴ቆ1
60ቇ
ଶ
 𝑑𝑥 𝑑𝑦
ൌ
2
ሺ60ሻଶ׬ଵ଴
଺଴ሺ𝑦െ10ሻ 𝑑𝑦
ൌ25
36
𝐷
𝐿,
𝐿൑𝐷,
𝑋
𝜃
𝑋.
𝐿/2
𝑋
cos𝜃൏𝐿
2  or 𝑋൏𝐿
2 cos𝜃
𝑋
𝐷/2
𝜃
𝜋/2,
397 of 848

assume that they are independent, uniformly distributed random variables over
these respective ranges. Hence,
Example 2e Characterization of the Normal Distribution
Let  and  denote the horizontal and vertical miss distances when a bullet is
fired at a target, and assume that
1.  and  are independent continuous random variables having
differentiable density functions.
2. The joint density 
 of  and  depends on (
) only
through 
Loosely put, assumption 2 states that the probability of the bullet landing on any
point of the –  plane depends only on the distance of the point from the target
and not on its angle of orientation. An equivalent way of phrasing this assumption
is to say that the joint density function is rotation invariant.
It is a rather interesting fact that assumptions 1 and 2 imply that  and  are
normally distributed random variables. To prove this, note first that the
assumptions yield the relation
for some function 
 Differentiating Equation (2.9)
 with respect to  yields
Dividing Equation (2.10)
 by Equation (2.9)
 gives
𝑃ቊ𝑋൏𝐿
2 cos𝜃ቋ
ൌ     
׬׬
௫ழ௅/ଶ cos  ௬
𝑓௑ሺ𝑥ሻ𝑓ఏሺ𝑦ሻ 𝑑𝑥 𝑑𝑦
ൌ4
𝜋𝐷׬଴
గ/ଶ׬଴
௅/ଶ cos ௬ 𝑑𝑥 𝑑𝑦
ൌ4
𝜋𝐷׬଴
గ/ଶ𝐿
2 cos 𝑦 𝑑𝑦
ൌ2𝐿
𝜋𝐷
∗
𝑋
𝑌
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ
𝑋
𝑌
𝑥, 𝑦
𝑥ଶ൅𝑦ଶ.
𝑥𝑦
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻൌ𝑔ሺ𝑥ଶ൅𝑦ଶሻ
(2.9)
𝑔.
𝑥
𝑓′௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻൌ2𝑥 𝑔′ሺ𝑥ଶ൅𝑦ଶሻ
(2.10)
𝑓′௑ሺ𝑥ሻ
𝑓௑ሺ𝑥ሻൌ2𝑥 𝑔′ሺ𝑥ଶ൅𝑦ଶሻ
𝑔ሺ𝑥ଶ൅𝑦ଶሻ
398 of 848

or
Because the value of the left-hand side of Equation (2.11)
 depends only on 
whereas the value of the right-hand side depends on 
 it follows that the
left-hand side must be the same for all 
 To see this, consider any 
 and let
 be such that 
 Then, from Equation (2.11)
, we
obtain
Hence,
which implies, upon integration of both sides, that
Since 
 it follows that  is necessarily negative, and we may
write 
 Thus,
That is,  is a normal random variable with parameters 
 and 
 A similar
argument can be applied to 
 to show that
Furthermore, it follows from assumption 2 that 
 and that  and  are thus
independent, identically distributed normal random variables with parameters
 and 
A necessary and sufficient condition for the random variables  and  to be
independent is for their joint probability density function (or joint probability mass
function in the discrete case) (
) to factor into two terms, one depending only on
𝑓′௑ሺ𝑥ሻ
2𝑥 𝑓௑ሺ𝑥ሻൌ𝑔′ሺ𝑥ଶ൅𝑦ଶሻ
𝑔ሺ𝑥ଶ൅𝑦ଶሻ
(2.11)
𝑥,
𝑥ଶ൅𝑦ଶ,
𝑥.
𝑥ଵ, 𝑥ଶ
𝑦ଵ, 𝑦ଶ
𝑥ଶ
ଵ൅𝑦ଶ
ଵൌ𝑥ଶ
ଶ൅𝑦ଶ
ଶ.
𝑓′௑ሺ𝑥ଵሻ
2𝑥ଵ 𝑓௑ሺ𝑥ଵሻൌ𝑔′ሺ𝑥ଶ
ଵ൅𝑦ଶ
ଵሻ
𝑔ሺ𝑥ଶଵ൅𝑦ଶଵሻൌ𝑔′ሺ𝑥ଶ
ଶ൅𝑦ଶ
ଶሻ
𝑔ሺ𝑥ଶଶ൅𝑦ଶଶሻൌ
𝑓′௑ሺ𝑥ଶሻ
2𝑥ଶ 𝑓௑ሺ𝑥ଶሻ
𝑓′௑ሺ𝑥ሻ
𝑥 𝑓௑ሺ𝑥ሻൌ𝑐 or  𝑑
𝑑𝑥ሺlog𝑓௑ሺ𝑥ሻሻൌ𝑐𝑥
log𝑓௑ሺ𝑥ሻൌ𝑎൅𝑐𝑥ଶ
2  or 𝑓௑ሺ𝑥ሻൌ𝑘𝑒௖௫మ/ଶ
׬െஶ
ஶ
𝑓௑ሺ𝑥ሻ 𝑑𝑥ൌ1,
𝑐
𝑐ൌെ1/𝜎ଶ.
𝑓௑ሺ𝑥ሻൌ𝑘𝑒െ௫మ/ଶఙమ
𝑋
𝜇ൌ0
𝜎ଶ.
𝑓௒ሺ𝑦ሻ
𝑓௒ሺ𝑦ሻൌ
1
2𝜋
√
𝜎̅ ̅ 𝑒െ௬మ/ଶఙ̅ ̅̅ ̅మ
𝜎ଶൌ𝜎̅ ̅ଶ
𝑋
𝑌
𝜇ൌ0
𝜎ଶ.
𝑋
𝑌
𝑓𝑥, 𝑦
399 of 848

 and the other depending only on 
Proposition 2.1
The continuous (discrete) random variables  and  are independent if and only
if their joint probability density (mass) function can be expressed as
Proof Let us give the proof in the continuous case. First, note that independence
implies that the joint density is the product of the marginal densities of  and 
so the preceding factorization will hold when the random variables are
independent. Now, suppose that
Then
where 
and 
 Also,
Since 
 it follows that
and the proof is complete.
Example 2f
If the joint density function of  and  is
and is equal to 0 outside this region, are the random variables independent?
What if the joint density function is
𝑥
𝑦.
𝑋
𝑌
𝑓௑,௒ሺ𝑥, 𝑦ሻൌℎሺ𝑥ሻ 𝑔ሺ𝑦ሻ    െ∞൏𝑥൏∞, െ∞൏𝑦൏∞
𝑋
𝑌,
𝑓௑,௒ሺ𝑥, 𝑦ሻൌℎቀ𝑥ሻ 𝑔ቀ𝑦ሻ
1
ൌ׬െஶ
ஶ׬െஶ
ஶ𝑓௑,௒ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
ൌ׬െஶ
ஶℎሺ𝑥ሻ 𝑑𝑥׬െஶ
ஶ𝑔ሺ𝑦ሻ 𝑑𝑦
ൌ𝐶ଵ𝐶ଶ
𝐶ଵൌ׬െஶ
ஶℎሺ𝑥ሻ 𝑑𝑥
𝐶ଶൌ׬െஶ
ஶ𝑔ሺ𝑦ሻ 𝑑𝑦.
𝑓௑ሺ𝑥ሻ
ൌ׬െஶ
ஶ𝑓௑,௒ሺ𝑥, 𝑦ሻ 𝑑𝑦ൌ𝐶ଶ ℎሺ𝑥ሻ
𝑓௒ሺ𝑦ሻ
ൌ׬െஶ
ஶ𝑓௑,௒ሺ𝑥, 𝑦ሻ 𝑑𝑥ൌ𝐶ଵ 𝑔ሺ𝑦ሻ
𝐶ଵ𝐶ଶൌ1,
𝑓௑,௒ሺ𝑥, 𝑦ሻൌ𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ6𝑒െଶ௫𝑒െଷ௬  0 ൏𝑥൏∞,   0 ൏𝑦൏∞
𝑓ሺ𝑥, 𝑦ሻൌ24𝑥𝑦  0 ൏𝑥൏1,    0 ൏𝑦൏1,    0 ൏𝑥൅𝑦൏1
400 of 848

and is equal to 0 otherwise?
Solution
In the first instance, the joint density function factors, and thus the random
variables, are independent (with one being exponential with rate 2 and the other
exponential with rate 3). In the second instance, because the region in which the
joint density is nonzero cannot be expressed in the form 
 the joint
density does not factor, so the random variables are not independent. This can
be seen clearly by letting
and writing
which clearly does not factor into a part depending only on  and another
depending only on 
The concept of independence may, of course, be defined for more than two random
variables. In general, the  random variables 
 are said to be independent
if, for all sets of real numbers 
As before, it can be shown that this condition is equivalent to
Finally, we say that an infinite collection of random variables is independent if every
finite subcollection of them is independent.
Example 2g How can a computer choose a random subset?
Most computers are able to generate the value of, or simulate, a uniform (0, 1)
random variable by means of a built-in subroutine that (to a high degree of
approximation) produces such “random numbers.” As a result, it is quite easy for
a computer to simulate an indicator (that is, a Bernoulli) random variable.
𝑥∈𝐴, 𝑦∈𝐵,
𝐼ሺ𝑥, 𝑦ሻൌቊ1
  if  0 ൏𝑥൏1,   0 ൏𝑦൏1,   0 ൏𝑥൅𝑦൏1
0
 otherwise
𝑓ሺ𝑥, 𝑦ሻൌ24𝑥𝑦 𝐼ሺ𝑥, 𝑦ሻ
𝑥
𝑦.
𝑛
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝐴ଵ, 𝐴ଶ, … , 𝐴௡,
𝑃ሼ𝑋ଵ∈𝐴ଵ, 𝑋ଶ∈𝐴ଶ, … , 𝑋௡∈𝐴௡ሽൌෑ
௜ൌଵ
௡
𝑃ሼ𝑋௜∈𝐴௜ሽ
𝑃ሼ𝑋ଵ൑𝑎ଵ, 𝑋ଶ൑𝑎ଶ, … , 𝑋௡൑𝑎௡ሽ
               ൌ
௜ൌଵ
௡
𝑃ሼ𝑋௜൑𝑎௜ሽ   for all  𝑎ଵ, 𝑎ଶ, … , 𝑎௡
401 of 848

Suppose  is an indicator variable such that
The computer can simulate  by choosing a uniform (0, 1) random number  and
then letting
Suppose that we are interested in having the computer select 
 of the
numbers 
 in such a way that each of the 
 subsets of size  is equally
likely to be chosen. We now present a method that will enable the computer to
solve this task. To generate such a subset, we will first simulate, in sequence, 
indicator variables 
 of which exactly  will equal 1. Those  for which
 will then constitute the desired subset.
To generate the random variables 
 start by simulating  independent
uniform (0, 1) random variables 
 Now define
and then, once 
 are determined, recursively set
In words, at the (
)th stage, we set 
 equal to 1 (and thus put 
 into the
desired subset) with a probability equal to the remaining number of places in the
subset 
 divided by the remaining number of possibilities
(namely, 
). Hence, the joint distribution of 
 is determined from
𝐼
𝑃ሼ𝐼ൌ1ሽൌ𝑝ൌ1 െ𝑃ሼ𝐼ൌ0ሽ
𝐼
𝑈
𝐼ൌ1
i𝑓  𝑈൏𝑝
0
i𝑓  𝑈൒𝑝
𝑘, 𝑘൑𝑛,
1, 2, … , 𝑛
ቆ𝑛
𝑘ቇ
𝑘
𝑛
𝐼ଵ, 𝐼ଶ, … , 𝐼௡,
𝑘
𝑖
𝐼௜ൌ1
𝐼ଵ, … , 𝐼௡,
𝑛
𝑈ଵ, 𝑈ଶ, … , 𝑈௡.
𝐼ଵൌቐ1
 if  𝑈ଵ൏𝑘
𝑛
0
 otherwise
𝐼ଵ, … , 𝐼௜
𝐼௜൅ଵൌ൞1
if 𝑈௜൅ଵ൏𝑘െሺ𝐼ଵ൅⋯൅𝐼௜ሻ
𝑛െ𝑖
0
otherwise
𝑖൅1
𝐼௜൅ଵ
𝑖൅1
ቌnamely, 𝑘െ෍
௝ൌଵ
௜
𝐼௝ቍ,
𝑛െ𝑖
𝐼ଵ, 𝐼ଶ, … , 𝐼௡
𝑃ሼ𝐼ଵൌ1ሽ
ൌ𝑘
𝑛
𝑃ሼ𝐼௜൅ଵൌ1||𝐼ଵ, … , 𝐼௜ሽ
ൌ
𝑘െ෍
௝ൌଵ
௜
𝐼௝
𝑛െ𝑖
 1 ൏𝑖൏𝑛
402 of 848

The proof that the preceding formula results in all subsets of size  being equally
likely to be chosen is by induction on 
 It is immediate when 
 (that
is, when 
), so assume it to be true whenever 
 Now,
suppose that 
 and consider any subset of size —say,
—and consider the following two cases.
Case 1: 
Now given that 
 the remaining elements of the subset are chosen as if a
subset of size 
 were to be chosen from the 
 elements 
Hence, by the induction hypothesis, the conditional probability that this will result
in a given subset of size 
 being selected is 
 Hence,
Case 2:
where the induction hypothesis was used to evaluate the preceding conditional
probability.
Thus, in all cases, the probability that a given subset of size  will be the subset
chosen is 
Remark The foregoing method for generating a random subset has a very low
memory requirement. A faster algorithm that requires somewhat more memory is
presented in Section 10.1
. (The latter algorithm uses the last  elements of a
random permutation of 
)
𝑘
𝑘൅𝑛.
𝑘൅𝑛ൌ2
𝑘ൌ1, 𝑛ൌ1
𝑘൅𝑛൑𝑙.
𝑘൅𝑛ൌ𝑙൅1,
𝑘
𝑖ଵ൑𝑖ଶ൑⋯൑𝑖௞
𝑖ଵൌ1
𝑃ሼ𝐼ଵ
ൌ𝐼௜మൌ⋯ൌ𝐼௜ೖൌ1, 𝐼௝ൌ0  otherwiseሽ
ൌ𝑃ሼ𝐼ଵൌ1ሽ𝑃൛𝐼௜మൌ⋯ൌ𝐼௜ೖൌ1, 𝐼௝ൌ0  otherwise||𝐼ଵൌ1ൟ
𝐼ଵൌ1,
𝑘െ1
𝑛െ1
2, 3, … , 𝑛.
𝑘െ1
1/ቆ𝑛െ1
𝑘െ1ቇ.
𝑃ሼ𝐼ଵ
ൌ𝐼௜మൌ⋯ൌ𝐼௜ೖൌ1, 𝐼௝ൌ0  otherwiseൟ
ൌ𝑘
𝑛
1
ቆ𝑛െ1
𝑘െ1ቇ
ൌ
1
ቆ𝑛
𝑘ቇ
𝑖ଵ്1
𝑃ሼ𝐼௜ଵ
ൌ𝐼௜మൌ⋯ൌ𝐼௜ೖൌ1, 𝐼௝ൌ0  otherwiseሽ
ൌ𝑃൛𝐼௜భൌ⋯ൌ𝐼௜ೖൌ1, 𝐼௝ൌ0  otherwise||𝐼ଵൌ0ൟ𝑃ሼ𝐼ଵൌ0ሽ
ൌ
1
ቆ𝑛െ1
𝑘
ቇ
ቆ1 െ𝑘
𝑛ቇൌ
1
ቆ𝑛
𝑘ቇ
𝑘
1/ቆ𝑛
𝑘ቇ.
𝑘
1, 2, … , 𝑛.
403 of 848

Example 2h
Let 
 be independent and uniformly distributed over (0, 1). Compute
Solution
Since
we have
Example 2i Probabilistic Interpretation of Half-Life
Let 
 denote the number of nuclei contained in a radioactive mass of material
at time 
 The concept of half-life is often defined in a deterministic fashion by
stating this it is an empirical fact that, for some value 
 called the half-life,
[Note that 
] Since the preceding implies that, for any nonnegative
 and 
it follows that no matter how much time  has already elapsed, in an additional
time  the number of existing nuclei will decrease by the factor 
Because the deterministic relationship just given results from observations of
radioactive masses containing huge numbers of nuclei, it would seem that it
might be consistent with a probabilistic interpretation. The clue to deriving the
appropriate probability model for half-life resides in the empirical observation that
the proportion of decay in any time interval depends neither on the total number
𝑋, 𝑌, 𝑍
𝑃ሼ𝑋൒𝑌𝑍ሽ.
𝑓௑,௒,௓ሺ𝑥, 𝑦, 𝑧ሻ
ൌ𝑓௑ሺ𝑥ሻ 𝑓௒ሺ𝑦ሻ 𝑓௓ሺ𝑧ሻ
ൌ1,  0 ൑𝑥൑1,  0 ൑𝑦൑1,  0 ൑𝑧൑1
𝑃ሼ𝑋൒𝑌𝑍ሽ
ൌ
׬׬׬
௫൒௬௭
𝑓௑, ௒, ௓ሺ𝑥, 𝑦, 𝑧ሻ 𝑑𝑥 𝑑𝑦 𝑑𝑧
ൌ׬଴
ଵ׬଴
ଵ׬௬௭
ଵ 𝑑𝑥 𝑑𝑦 𝑑𝑧
ൌ׬଴
ଵ׬଴
ଵሺ1 െ𝑦𝑧ሻ 𝑑𝑦 𝑑𝑧
ൌ׬଴
ଵ൬1 െ𝑧
2൰ 𝑑𝑧
ൌ3
4
𝑁ሺ𝑡ሻ
𝑡.
ℎ,
𝑁ሺ𝑡ሻൌ2െ௧/௛𝑁൫0ሻ 𝑡൐0
𝑁ሺℎሻൌ𝑁ሺ0ሻ/2 .
𝑠
𝑡,
𝑁ሺ𝑡൅𝑠ሻൌ2െሺ௦൅௧ሻ/௛𝑁൫0ሻൌ2െ௧/௛𝑁൫𝑠ሻ
𝑠
𝑡,
2െ௧/௛.
404 of 848

of nuclei at the beginning of the interval nor on the location of this interval [since
 depends neither on 
 nor on ]. Thus, it appears that the
individual nuclei act independently and with a memoryless life distribution.
Consequently, since the unique life distribution that is memoryless is the
exponential distribution, and since exactly one-half of a given amount of mass
decays every  time units, we propose the following probabilistic model for
radioactive decay.
Probabilistic interpretation of the half-life h: The lifetimes of the individual
nuclei are independent random variables having a life distribution that is
exponential with median equal to 
 That is, if  represents the lifetime of a given
nucleus, then
(Because 
 and the preceding can be written as
it can be seen that  indeed has an exponential distribution with median 
)
Note that under the probabilistic interpretation of half-life just given, if one starts
with (0) nuclei at time 0, then 
 the number of nuclei that remain at time 
will have a binomial distribution with parameters 
 and 
Results of Chapter 8
 will show that this interpretation of half-life is consistent
with the deterministic model when considering the proportion of a large number
of nuclei that decay over a given time frame. However, the difference between
the deterministic and probabilistic interpretation becomes apparent when one
considers the actual number of decayed nuclei. We will now indicate this with
regard to the question of whether protons decay.
There is some controversy over whether or not protons decay. Indeed, one
theory predicts that protons should decay with a half-life of about 
 years.
To check this prediction empirically, it has been suggested that one follow a large
number of protons for, say, one or two years and determine whether any of them
decay within that period. (Clearly, it would not be feasible to follow a mass of
protons for 
 years to see whether one-half of it decays.) Let us suppose that
we are able to keep track of 
 protons for  years. The number of
decays predicted by the deterministic model would then be given by
𝑁ሺ𝑡൅𝑠ሻ/𝑁ሺ𝑠ሻ
𝑁ሺ𝑠ሻ
𝑠
ℎ
ℎ.
𝐿
𝑃൛𝐿൏𝑡ൟൌ1 െ2െ௧/௛
𝑃ሼ𝐿൏ℎሽൌ1
2
𝑃ሼ𝐿൏𝑡ሽൌ1 െexpቊെ𝑡log 2
ℎቋ
𝐿
ℎ.
𝑁
𝑁ሺ𝑡ሻ,
𝑡
𝑛ൌ𝑁ሺ0ሻ
𝑝ൌ2െ௧/௛.
ℎൌ10ଷ଴
10ଷ଴
𝑁ሺ0ሻൌ10ଷ଴
𝑐
405 of 848

For instance, the deterministic model predicts that in 2 years there should be
1.3863 decays, and it would thus appear to be a serious blow to the hypothesis
that protons decay with a half-life of 
 years if no decays are observed over
those 2 years.
Let us now contrast the conclusions just drawn with those obtained from the
probabilistic model. Again, let us consider the hypothesis that the half-life of
protons is 
 years, and suppose that we follow  protons for  years.
Since there is a huge number of independent protons, each of which will have a
very small probability of decaying within this time period, it follows that the
number of protons that decay will have (to a very strong approximation) a
Poisson distribution with parameter equal to 
 Thus,
and, in general,
Thus, we see that even though the average number of decays over 2 years is (as
predicted by the deterministic model) 1.3863, there is 1 chance in 4 that there will
not be any decays, thereby indicating that such a result in no way invalidates the
original hypothesis of proton decay.
Remarks Independence is a symmetric relation. The random variables  and  are
independent if their joint density function (or mass function in the discrete case) is
the product of their individual density (or mass) functions. Therefore, to say that  is
independent of  is equivalent to saying that  is independent of  — or just that 
and  are independent. As a result, in considering whether  is independent of  in
situations where it is not at all intuitive that knowing the value of  will not change the
probabilities concerning 
 it can be beneficial to interchange the roles of  and 
𝑁ሺ0ሻെ𝑁ሺ𝑐ሻ
ൌℎሺ1 െ2െ௖/௛ሻ
ൌ1 െ2െ௖/௛
1/ℎ
ൎ
lim
௫→଴  1 െ2െ௖௫
𝑥
   since  1
ℎൌ10െଷ଴ൎ0
ൌ
lim
௫→଴ ሺ𝑐2െ௖௫ log 2ሻ by L’Hôpital’s rule
ൌ𝑐 log 2 ൎ.6931𝑐
10ଷ଴
ℎൌ10ଷ଴
ℎ
𝑐
ℎሺ1 െ2െ௖/௛ሻൎ𝑐 log 2 .
𝑃ሼ0  decaysሽ
ൌ𝑒െ௖ log ଶ
ൌ𝑒െ log ሺଶ೎ሻൌ1
2௖
𝑃ሼ𝑛  decaysሽൌ2െ௖ሾ𝑐 log 2ሿ௡
𝑛!
  𝑛൒0
𝑋
𝑌
𝑋
𝑌
𝑌
𝑋
𝑋
𝑌
𝑋
𝑌
𝑌
𝑋,
𝑋
𝑌
406 of 848

and ask instead whether  is independent of 
 The next example illustrates this
point.
Example 2j
If the initial throw of the dice in the game of craps results in the sum of the dice
equaling 4, then the player will continue to throw the dice until the sum is either 4
or 7. If this sum is 4, then the player wins, and if it is 7, then the player loses. Let
 denote the number of throws needed until either 4 or 7 appears, and let 
denote the value (either 4 or 7) of the final throw. Is  independent of 
 That is,
does knowing which of 4 or 7 occurs first affect the distribution of the number of
throws needed until that number appears? Most people do not find the answer to
this question to be intuitively obvious. However, suppose that we turn it around
and ask whether  is independent of 
 That is, does knowing how many throws
it takes to obtain a sum of either 4 or 7 affect the probability that that sum is
equal to 4? For instance, suppose we know that it takes  throws of the dice to
obtain a sum of either 4 or 7. Does this affect the probability distribution of the
final sum? Clearly not, since all that is important is that its value is either 4 or 7,
and the fact that none of the first 
 throws were either 4 or 7 does not change
the probabilities for the th throw. Thus, we can conclude that  is independent
of 
 or equivalently, that  is independent of 
As another example, let 
 be a sequence of independent and identically
distributed continuous random variables, and suppose that we observe these
random variables in sequence. If 
 for each 
 then we say
that 
 is a record value. That is, each random variable that is larger than all
those preceding it is called a record value. Let 
 denote the event that 
 is a
record value. Is 
 independent of 
 That is, does knowing that the th
random variable is the largest of the first  change the probability that the 
random variable is the largest of the first 
 While it is true that 
 is
independent of 
 this may not be intuitively obvious. However, if we turn the
question around and ask whether 
 is independent of 
 then the result is
more easily understood. For knowing that the 
 value is larger than
 clearly gives us no information about the relative size of 
 among the
first  random variables. Indeed, by symmetry, it is clear that each of these 
random variables is equally likely to be the largest of this set, so
 Hence, we can conclude that 
 and 
 are
independent events.
Remark It follows from the identity
𝑌
𝑋.
𝑁
𝑋
𝑁
𝑋?
𝑋
𝑁.
𝑛
𝑛െ1
𝑛
𝑋
𝑁,
𝑁
𝑋.
𝑋ଵ, 𝑋ଶ, … 
𝑋௡൐𝑋௜
𝑖ൌ1, … , 𝑛െ1,
𝑋௡
𝐴௡
𝑋௡
𝐴௡൅ଵ
𝐴௡?
𝑛
𝑛
ሺ𝑛൅1ሻ
𝑛൅1?
𝐴௡൅ଵ
𝐴௡,
𝐴௡
𝐴௡൅ଵ,
ሺ𝑛൅1ሻ
𝑋ଵ, … , 𝑋௡
𝑋௡
𝑛
𝑛
𝑃ሺ𝐴௡||𝐴௡൅ଵሻൌ𝑃ሺ𝐴௡ሻൌ1/𝑛.
𝐴௡
𝐴௡൅ଵ
𝑃ሼ𝑋ଵ൑𝑎ଵ, … , 𝑋௡൑𝑎௡ሽ
    ൌ𝑃ሼ𝑋ଵ൑𝑎ଵሽ𝑃ሼ𝑋ଶ൑𝑎ଶ||𝑋ଵ൑𝑎ଵሽ⋯𝑃ሼ𝑋௡൑𝑎௡||𝑋ଵ൑𝑎ଵ, … , 𝑋௡െଵ൑𝑎௡െଵሽ
407 of 848

that the independence of 
 can be established sequentially. That is, we
can show that these random variables are independent by showing that
It is often important to be able to calculate the distribution of 
 from the
distributions of  and  when  and  are independent. Suppose that  and  are
independent, continuous random variables having probability density functions 
and 
 The cumulative distribution function of 
 is obtained as follows:
The cumulative distribution function 
 is called the convolution of the distributions
 and 
 (the cumulative distribution functions of  and 
 respectively).
By differentiating Equation (3.1)
, we find that the probability density function 
of 
 is given by
𝑋ଵ, … , 𝑋௡
𝑋ଶ
 is independent of 𝑋ଵ 
𝑋ଷ
 is independent of 𝑋ଵ, 𝑋ଶ 
𝑋ସ
 is independent of 𝑋ଵ, 𝑋ଶ, 𝑋ଷ
⋮
𝑋௡
is independent of 𝑋ଵ, … , 𝑋௡െଵ 
𝑋൅𝑌
𝑋
𝑌
𝑋
𝑌
𝑋
𝑌
𝑓௑
𝑓௒.
𝑋൅𝑌
𝐹௑൅௒ሺ𝑎ሻ
ൌ𝑃ሼ𝑋൅𝑌൑𝑎ሽ
ൌ  
׬׬
௫൅௬൑௔
𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑥 𝑑𝑦
ൌ׬െஶ
ஶ׬െஶ
௔െ௬𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑥 𝑑𝑦
ൌ׬െஶ
ஶ׬െஶ
௔െ௬𝑓௑ሺ𝑥ሻ𝑑𝑥𝑓௒ሺ𝑦ሻ 𝑑𝑦
ൌ׬െஶ
ஶ𝑓௑ሺ𝑎െ𝑦ሻ 𝑓௒ሺ𝑦ሻ 𝑑𝑦
(3.1)
𝐹௑൅௒
𝐹௑
𝐹௒
𝑋
𝑌,
𝑓௑൅௒
𝑋൅𝑌
𝑓௑൅௒ሺ𝑎ሻ
ൌ𝑑
𝑑𝑎׬െஶ
ஶ𝐹௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ׬െஶ
ஶ
𝑑
𝑑𝑎𝐹௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑦
ൌ׬െஶ
ஶ𝑓௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑦
(3.2)
408 of 848

It is not difficult to determine the density function of the sum of two independent
uniform 
 random variables.
Example 3a Sum of two independent uniform random variables
If  and  are independent random variables, both uniformly distributed on (0, 1),
calculate the probability density of 
Solution
From Equation (3.2)
, since
we obtain
For 
 this yields
For 
 we get
Hence,
Because of the shape of its density function (see Figure 6.3
), the random
variable 
 is said to have a triangular distribution.
Figure 6.3 Triangular density function.
ሺ0, 1ሻ
𝑋
𝑌
𝑋൅𝑌.
𝑓௑ሺ𝑎ሻൌ𝑓௒ሺ𝑎ሻൌቊ1
0 ൏𝑎൏1
0
otherwise
𝑓௑൅௒ሺ𝑎ሻൌ඲
଴
ଵ
𝑓௑ሺ𝑎െ𝑦ሻ 𝑑𝑦
0 ൑𝑎൑1,
𝑓௑൅௒ሺ𝑎ሻൌ඲
଴
௔
𝑑𝑦ൌ𝑎
1 ൏𝑎൏2,
𝑓௑൅௒ሺ𝑎ሻൌ඲
௔െଵ
ଵ
𝑑𝑦ൌ2 െ𝑎
𝑓௑൅௒ሺ𝑎ሻൌ൞
𝑎
 0 ൑𝑎൑1
2 െ𝑎 1 ൏𝑎൏2
0
 otherwise
𝑋൅𝑌
409 of 848

Now, suppose that 
 are independent uniform 
 random variables,
and let
Whereas a general formula for 
 is messy, it has a particularly nice form when
 Indeed, we now use mathematical induction to prove that
Because the proceeding equation is true for 
 assume that
Now, writing
and using the fact that the 
 are all nonnegative, we see from Equation (3.1)
that, for 
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
ሺ0, 1ሻ
𝐹௡ሺ𝑥ሻൌ𝑃ሼ𝑋ଵ൅…  ൅𝑋௡൑𝑥ሽ
𝐹௡ሺ𝑥ሻ
𝑥൑1 .
𝐹௡ሺ𝑥ሻൌ𝑥௡/𝑛! ,  0 ൑𝑥൑1
𝑛ൌ1,
𝐹௡െଵሺ𝑥ሻൌ𝑥௡െଵ/ሺ𝑛െ1ሻ! ,  0 ൑𝑥൑1
෍
௜ൌଵ
௡
𝑋௜ൌ
෍
௜ൌଵ
௡െଵ
𝑋௜൅𝑋௡
𝑋௜
0 ൑𝑥൑1,
𝐹௡ሺ𝑥ሻ
ൌ׬଴
ଵ𝐹௡െଵሺ𝑥െ𝑦ሻ𝑓௑೙ሺ𝑦ሻ𝑑𝑦
ൌ
1
ሺ𝑛െ1ሻ!׬଴
௫ሺ𝑥െ𝑦ሻ௡െଵ 𝑑𝑦 by the induction hypothesis
ൌ
1
ሺ𝑛െ1ሻ!׬଴
௫𝑤௡െଵ𝑑𝑤 ሺby 𝑤ൌ𝑥െ𝑦ሻ
ൌ𝑥௡/𝑛!
410 of 848

which completes the proof.
For an interesting application of the preceding formula, let us use it to determine the
expected number of independent uniform 
 random variables that need to be
summed to exceed 
 That is, with 
 being independent uniform 
random variables, we want to determine 
 where
Noting that  is greater than 
 if and only if 
 we see that
Because
we see that, for 
Therefore,
That is, the mean number of independent uniform 
 random variables that must
be summed for the sum to exceed  is equal to 
Recall that a gamma random variable has a density of the form
An important property of this family of distributions is that for a fixed value of 
 it is
closed under convolutions.
ሺ0, 1ሻ
1 .
𝑋ଵ, 𝑋ଶ, … 
ሺ0, 1ሻ
𝐸ሾ𝑁ሿ,
𝑁ൌminሼ𝑛:𝑋ଵ൅…  ൅𝑋௡൐1ሽ
𝑁
𝑛൐0
𝑋ଵ൅… ൅𝑋௡൑1,
𝑃ሼ𝑁൐𝑛ሽൌ𝐹௡ሺ1ሻൌ1/𝑛! ,  𝑛൐0
𝑃ሼ𝑁൐0ሽൌ1 ൌ1/0!
𝑛൐0,
𝑃ሼ𝑁ൌ𝑛ሽൌ𝑃ሼ𝑁൐𝑛െ1ሽെ𝑃ሼ𝑁൐𝑛ሽൌ
1
ሺ𝑛െ1ሻ! െ1
𝑛! ൌ𝑛െ1
𝑛!
𝐸ሾ𝑁ሿ
ൌ
෍
௡ൌଵ
ஶ
𝑛ሺ𝑛െ1ሻ
𝑛!
ൌ
෍
௡ൌଶ
ஶ
1
ሺ𝑛െ2ሻ!
ൌ𝑒
ሺ0, 1ሻ
1
𝑒.
𝑓ሺ𝑦ሻൌ𝜆𝑒െఒ௬ሺ𝜆𝑦ሻ௧െଵ
Γሺ𝑡ሻ
 0 ൏𝑦൏∞
𝜆,
411 of 848

Proposition 3.1
If  and  are independent gamma random variables with respective parameters
 and 
 then 
 is a gamma random variable with parameters
Proof Using Equation (3.2)
, we obtain
where  is a constant that does not depend on 
 But, as the preceding is a
density function and thus must integrate to 1, the value of  is determined, and
we have
Hence, the result is proved.
It is now a simple matter to establish, by using Proposition 3.1
 and induction,
that if 
 are independent gamma random variables with respective
parameters 
 then 
 is gamma with parameters 
We leave the proof of this statement as an exercise.
Example 3b
Let 
 be  independent exponential random variables, each having
parameter 
 Then, since an exponential random variable with parameter  is the
same as a gamma random variable with parameters 
 it follows from
Proposition 3.1
 that 
 is a gamma random variable with
parameters 
If 
 are independent standard normal random variables, then 
 is
said to have the chi-squared (sometimes seen as 
) distribution with  degrees of
freedom. Let us compute the density function of 
 When 
 and from
𝑋
𝑌
ሺ𝑠, 𝜆ሻ
ሺ𝑡, 𝜆ሻ,
𝑋൅𝑌
ሺ𝑠൅𝑡, 𝜆ሻ.
𝑓௑൅௒ሺ𝑎ሻ
ൌ
1
Γሺ𝑠ሻΓሺ𝑡ሻ׬଴
௔𝜆𝑒െఒሺ௔െ௬ሻሾ𝜆ሺ𝑎െ𝑦ሻሿ௦െଵ𝜆𝑒െఒ௬ሺ𝜆𝑦ሻ௧െଵ 𝑑𝑦
ൌ𝐾𝑒െఒ௔׬଴
௔ሺ𝑎െ𝑦ሻ௦െଵ𝑦௧െଵ𝑑𝑦
ൌ𝐾𝑒െఒ௔𝑎௦൅௧െଵ׬଴
ଵሺ1 െ𝑥ሻ௦െଵ𝑥௧െଵ 𝑑𝑥 by letting  𝑥ൌ𝑦
𝑎
ൌ𝐶𝑒െఒ௔𝑎௦൅௧െଵ
𝐶
𝑎.
𝐶
𝑓௑൅௒ሺ𝑎ሻൌ𝜆𝑒െఒ௔ሺ𝜆𝑎ሻ௦൅௧െଵ
Γሺ𝑠൅𝑡ሻ
𝑋௜, 𝑖ൌ1, … , 𝑛
ሺ𝑡௜, 𝜆ሻ, 𝑖ൌ1, … , 𝑛,
෍
௜ൌଵ
௡
𝑋௜
ቌ෍
௜ൌଵ
௡
𝑡௜, 𝜆ቍ.
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑛
𝜆.
𝜆
ሺ1, 𝜆ሻ,
𝑋ଵ൅𝑋ଶ൅⋯൅𝑋௡
ሺ𝑛, 𝜆ሻ.
𝑍ଵ, 𝑍ଶ, … , 𝑍௡
෍
௜ൌଵ
௡
𝑍௜
ଶ
𝜒ଶ
𝑛
𝑌.
𝑛ൌ1, 𝑌ൌ𝑍ଶ
ଵ,
412 of 848

Example 7b
 of Chapter 5
, we see that its probability density function is given
by
But we recognize the preceding as the gamma distribution with parameters 
[A by-product of this analysis is that 
] But since each 
 is gamma
 it follows from Proposition 3.1
 that the chi-squared distribution with 
degrees of freedom is just the gamma distribution with parameters 
 and
hence has a probability density function given by
When  is an even integer, 
 whereas when  is odd, 
can be obtained from iterating the relationship 
 and then using
the previously obtained result that 
 [For instance,
]
In practice, the chi-squared distribution often arises as the distribution of the square
of the error involved when one attempts to hit a target in -dimensional space when
the coordinate errors are taken to be independent standard normal random
variables. It is also important in statistical analysis.
𝑓௓మሺ𝑦ሻ
ൌ
1
2 𝑦
ඥ
ൣ𝑓௓ሺ𝑦
ඥሻ൅𝑓௓ሺെ
𝑦
ඥሻ൧
ൌ
1
2 𝑦
ඥ
2
2𝜋
√
𝑒െ௬/ଶ
ൌ
1
2 𝑒െ௬/ଶሺ𝑦/2ሻଵ/ଶെଵ
𝜋
√
ቆ1
2, 1
2ቇ.
Γቆ1
2ቇൌ
𝜋
√.
𝑍௜
ଶ
ቆ1
2, 1
2ቇ,
𝑛
ቆ𝑛/2, 1
2ቇ
𝑓௒ ሺ𝑦ሻ
ൌ
1
2 𝑒െ௬/ଶ൬𝑦
2൰
௡/ଶെଵ
Γ൬𝑛
2൰
 𝑦൐0
ൌ𝑒െ௬/ଶ𝑦௡/ଶെଵ
2௡/ଶΓ൬𝑛
2൰
  𝑦൐0
𝑛
Γሺ𝑛/2ሻൌሾሺ𝑛/2ሻെ1ሿ!,
𝑛
Γሺ𝑛/2ሻ
Γሺ𝑡ሻൌሺ𝑡െ1ሻΓሺ𝑡െ1ሻ
Γቆ1
2ቇൌ
𝜋
√.
Γቆ5
2ቇൌ3
2 Γቆ3
2ቇൌ3
2
1
2 Γቆ1
2ቇൌ3
4
𝜋
√.
𝑛
413 of 848

We can also use Equation (3.2)
 to prove the following important result about
normal random variables.
Proposition 3.2
If 
 are independent random variables that are normally distributed
with respective parameters 
 then 
 is normally distributed
with parameters 
 and 
Proof of Proposition 3.2: To begin, let  and  be independent normal random
variables with  having mean 0 and variance 
 and  having mean 0 and
variance 1. We will determine the density function of 
 by utilizing Equation
(3.2)
. Now, with
we have
where the preceding follows because 
 Now,
Hence,
𝑋௜, 𝑖ൌ1, … , 𝑛,
𝜇௜, 𝜎ଶ
௜, 𝑖ൌ1, … , 𝑛,
෍
௜ൌଵ
௡
𝑋௜
෍
௜ൌଵ
௡
𝜇௜
෍
௜ൌଵ
௡
𝜎௜
ଶ.
𝑋
𝑌
𝑋
𝜎ଶ
𝑌
𝑋൅𝑌
𝑐ൌ
1
2𝜎ଶ൅1
2 ൌ1 ൅𝜎ଶ
2𝜎ଶ
𝑓௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ
ൌ
1
2𝜋
√
𝜎 exp ൝െሺ𝑎െ𝑦ሻଶ
2𝜎ଶ
ൡ
1
2𝜋
√
 exp ቊെ𝑦ଶ
2 ቋ
ൌ
1
2𝜋𝜎 exp ቊെ𝑎ଶ
2𝜎ଶቋ exp ൜െ𝑐𝑦ଶ൅𝑎𝑦
𝜎ଶൠ
ൌ
1
2𝜋𝜎 exp ቊെ𝑎ଶ
2𝜎ଶെ𝑐ቆ𝑦ଶെ
2𝑎𝑦
1 ൅𝜎ଶቇቋ
2𝑐
1 ൅𝜎ଶൌ1
𝜎ଶ.
𝑎ଶ
2𝜎ଶ൅𝑐ቆ𝑦ଶെ
2𝑦𝑎
1 ൅𝜎ଶቇ
ൌ𝑎ଶ
2𝜎ଶ൅𝑐൬𝑦െ
𝑎
1 ൅𝜎ଶ൰
ଶ
െ𝑐
𝑎ଶ
ሺ1 ൅𝜎ଶሻଶ
ൌ𝑎ଶ
2𝜎ଶ൅𝑐൬𝑦െ
𝑎
1 ൅𝜎ଶ൰
ଶ
െ
𝑎ଶ
2𝜎ଶሺ1 ൅𝜎ଶሻ
ൌ𝑎ଶ
2𝜎ଶቆ1 െ
1
1 ൅𝜎ଶቇ൅𝑐൬𝑦െ
𝑎
1 ൅𝜎ଶ൰
ଶ
ൌ
𝑎ଶ
2ሺ1 ൅𝜎ଶሻ൅𝑐൬𝑦െ
𝑎
1 ൅𝜎ଶ൰
ଶ
414 of 848

From Equation (3.2)
, we obtain that
where  does not depend on 
 But this implies that 
 is normal with mean 0
and variance 
Now, suppose that 
 and 
 are independent normal random variables with 
having mean 
 and variance 
Then
But since 
 is normal with mean 0 and variance 
and
 is normal with mean 0 and variance 1, it follows from our previous
result that 
 is normal with mean 0 and variance
implying that 
 is normal with mean 
 and variance
Thus, Proposition 3.2
 is established when 
 The general case now
follows by induction. That is, assume that Proposition 3.2
 is true when there
are 
 random variables. Now consider the case of 
 and write
By the induction hypothesis, 
 is normal with mean 
 and variance
Therefore, by the result for 
 is normal with mean 
𝑓௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻൌ
1
2𝜋𝜎expቊെ
𝑎ଶ
2ሺ1 ൅𝜎ଶሻቋ expቊെ𝑐ሺ𝑦െ
𝑎
1 ൅𝜎ଶሻ
ଶ
ቋ
𝑓௑൅௒ሺ𝑎ሻ
ൌ
1
2𝜋𝜎expቊെ
𝑎ଶ
2ሺ1 ൅𝜎ଶሻቋ඲
െஶ
ஶ
expቊെ𝑐 ሺ𝑦െ
𝑎
1 ൅𝜎ଶሻ
ଶ
ቋ 𝑑𝑦
ൌ
1
2𝜋𝜎expቊെ
𝑎ଶ
2ሺ1 ൅𝜎ଶሻቋ඲
െஶ
ஶ
expሼെ𝑐𝑥ଶሽ 𝑑𝑥
ൌ𝐶 expቊെ
𝑎ଶ
2ሺ1 ൅𝜎ଶሻቋ
𝐶
𝑎.
𝑋൅𝑌
1 ൅𝜎ଶ.
𝑋ଵ
𝑋ଶ
𝑋௜
𝜇௜
𝜎௜
ଶ,  𝑖ൌ1, 2.
𝑋ଵ൅𝑋ଶൌ𝜎ଶቆ𝑋ଵെ𝜇ଵ
𝜎ଶ
൅𝑋ଶെ𝜇ଶ
𝜎ଶ
ቇ൅𝜇ଵ൅𝜇ଶ
ሺ𝑋ଵെ𝜇ଵሻ/𝜎ଶ
𝜎ଵ
ଶ/𝜎ଶ
ଶ,
ሺ𝑋ଶെ𝜇ଶሻ/𝜎ଶ
ሺ𝑋ଵെ𝜇ଵሻ/𝜎ଶ൅ሺ𝑋ଶെ𝜇ଶሻ/𝜎ଶ
1 ൅𝜎ଵ
ଶ/𝜎ଶ
ଶ,
𝑋ଵ൅𝑋ଶ
𝜇ଵ൅𝜇ଶ
𝜎ଶ
ଶሺ1 ൅𝜎ଵ
ଶ/𝜎ଶ
ଶሻൌ𝜎ଵ
ଶ൅𝜎ଶ
ଶ.
𝑛ൌ2 .
𝑛െ1
𝑛,
෍
௜ൌଵ
௡
 𝑋௜ൌ
෍
௜ൌଵ
௡െଵ
𝑋௜൅𝑋௡
෍
௜ൌଵ
௡െଵ
𝑋௜
෍
௜ൌଵ
௡െଵ
𝜇௜
෍
௜െଵ
௡െଵ
𝜎௜
ଶ.
𝑛ൌ2,
෍
௜ൌଵ
௡
𝑋௜
෍
௜ൌଵ
௡
𝜇௜
415 of 848

and variance 
Example 3c
A basketball team will play a 44-game season. Twenty-six of these games are
against class A teams and 18 are against class B teams. Suppose that the team
will win each game against a class A team with probability .4 and will win each
game against a class B team with probability .7. Suppose also that the results of
the different games are independent. Approximate the probability that
a. the team wins 25 games or more;
b. the team wins more games against class A teams than it does against
class B teams.
Solution
a. Let 
 and 
 respectively denote the number of games the team wins
against class A and against class B teams. Note that 
 and 
 are
independent binomial random variables and
By the normal approximation to the binomial, 
 and 
 will have
approximately the same distribution as would independent normal random
variables with the preceding expected values and variances. Hence, by
Proposition 3.2
, 
 will have approximately a normal distribution
with mean 23 and variance 10.02. Therefore, letting  denote a standard
normal random variable, we have
b. We note that 
 will have approximately a normal distribution with
mean 2.2 and variance 10.02. Hence,
෍
௜െଵ
௡
𝜎௜
ଶ.
𝑋஺
𝑋஻
𝑋஺
𝑋஻
𝐸ሾ𝑋஺ሿൌ26ሺ.4ሻൌ10.4
Varሺ𝑋஺ሻൌ26ሺ.4ሻሺ.6ሻൌ6.24
𝐸ሾ𝑋஻ሿൌ18ሺ.7ሻൌ12.6
Varሺ𝑋஻ሻൌ18ሺ.7ሻሺ.3ሻൌ3.78
𝑋஺
𝑋஻
𝑋஺൅𝑋஻
𝑍
𝑃ሼ𝑋஺൅𝑋஻൒25ሽ
ൌ𝑃ሼ𝑋஺൅𝑋஻൒24.5ሽ
ൌ𝑃ቊ𝑋஺൅𝑋஻െ23
10.02
√
൒24.5 െ23
10.02
√
ቋ
ൎ𝑃ቊ𝑍൒
1.5
10.02
√
ቋ
ൎ1 െ𝑃ሼ𝑍൏.4739ሽ
ൎ.3178
𝑋஺െ𝑋஻
െ
416 of 848

Therefore, there is approximately a 31.78 percent chance that the team will win
at least 25 games and approximately a 19.68 percent chance that it will win more
games against class A teams than against class B teams.
The random variable  is said to be a lognormal random variable with parameters 
and  if log 
 is a normal random variable with mean  and variance 
 That is, 
is lognormal if it can be expressed as
where  is a normal random variable.
Example 3d
Starting at some fixed time, let 
 denote the price of a certain security at the
end of  additional weeks, 
 A popular model for the evolution of these
prices assumes that the price ratios 
 are independent and
identically distributed lognormal random variables. Assuming this model, with
parameters 
 what is the probability that
a. the price of the security increases over each of the next two weeks?
b. the price at the end of two weeks is higher than it is today?
Solution
Let  be a standard normal random variable. To solve part (a), we use the fact
that log
 increases in  to conclude that 
 if and only if
 As a result, we have
𝑃ሼ𝑋஺െ𝑋஻൒1ሽ
ൌ𝑃ሼ𝑋஺െ𝑋஻൒.5ሽ
ൌ𝑃ቊ𝑋஺െ𝑋஻൅2.2
10.02
√
൒.5 ൅2.2
10.02
√
ቋ
ൎ𝑃ቊ𝑍൒
2.7
10.02
√
ቋ
ൎ1 െ𝑃ሼ𝑍൏.8530ሽ
ൎ.1968
𝑌
𝜇
𝜎
ሺ𝑌ሻ
𝜇
𝜎ଶ.
𝑌
𝑌ൌ𝑒௑
𝑋
𝑆ሺ𝑛ሻ
𝑛
𝑛൒1 .
𝑆ሺ𝑛ሻ/𝑆ሺ𝑛െ1ሻ, 𝑛൒1,
𝜇ൌ. 0165, 𝜎ൌ. 0730,
𝑍
ሺ𝑥ሻ
𝑥
𝑥൐1
 log ሺ𝑥ሻ൐ log ሺ1ሻൌ0 .
𝑃ቊ𝑆ሺ1ሻ
𝑆ሺ0ሻ൐1ቋ
ൌ𝑃ቊlogቆ𝑆ሺ1ሻ
𝑆ሺ0ሻቇ൐0ቋ
ൌ𝑃ቊ𝑍൐െ.0165
.0730 ቋ
ൌ𝑃ሼ𝑍൏.2260ሽ
ൌ.5894
417 of 848

In other words, the probability that the price is up after one week is .5894. Since
the successive price ratios are independent, the probability that the price
increases over each of the next two weeks is 
To solve part (b), we reason as follows:
However, 
 being the sum of two independent normal
random variables with a common mean .0165 and a common standard deviation
.0730, is a normal random variable with mean .0330 and variance 
Consequently,
Rather than attempt to derive a general expression for the distribution of 
 in the
discrete case, we shall consider some examples.
Example 3e Sums of independent poisson random variables
If  and  are independent Poisson random variables with respective parameters
 and 
 compute the distribution of 
Solution
Because the event 
 may be written as the union of the disjoint events
 we have
ሺ. 5894ሻଶൌ. 3474 .
𝑃ቊ𝑆ሺ2ሻ
𝑆ሺ0ሻ൐1ቋ
ൌ𝑃ቊ𝑆ሺ2ሻ
𝑆ሺ1ሻ
𝑆ሺ1ሻ
𝑆ሺ0ሻ൐1ቋ
ൌ𝑃ቊlogቆ𝑆ሺ2ሻ
𝑆ሺ1ሻቇ൅logቆ𝑆ሺ1ሻ
𝑆ሺ0ሻቇ൐0ቋ
log ቆ𝑆ሺ2ሻ
𝑆ሺ1ሻቇ൅log ቆ𝑆ሺ1ሻ
𝑆ሺ0ሻቇ,
2ሺ. 0730ሻଶ.
𝑃ቊ𝑆ሺ2ሻ
𝑆ሺ0ሻ൐1ቋ
ൌ𝑃ቊ𝑍൐െ.0330
.0730 2
√ቋ
ൌ𝑃ሼ𝑍൏.31965ሽ
ൌ.6254
𝑋൅𝑌
𝑋
𝑌
𝜆ଵ
𝜆ଶ,
𝑋൅𝑌.
ሼ𝑋൅𝑌ൌ𝑛ሽ
ሼ𝑋ൌ𝑘, 𝑌ൌ𝑛െ𝑘ሽ, 0 ൑𝑘൑𝑛,
418 of 848

Thus, 
 has a Poisson distribution with parameter 
Example 3f Sums of independent binomial random variables
Let  and  be independent binomial random variables with respective
parameters (
) and (
). Calculate the distribution of 
Solution
Recalling the interpretation of a binomial random variable, and without any
computation at all, we can immediately conclude that 
 is binomial with
parameters 
 This follows because  represents the number of
successes in  independent trials, each of which results in a success with
probability ; similarly,  represents the number of successes in 
 independent
trials, each of which results in a success with probability 
 Hence, given that 
and  are assumed independent, it follows that 
 represents the number of
successes in 
 independent trials when each trial has a probability  of
resulting in a success. Therefore, 
 is a binomial random variable with
parameters 
 To check this conclusion analytically, note that
𝑃ሼ𝑋൅𝑌ൌ𝑛ሽ
ൌ
෍
௞ൌ଴
௡
𝑃ሼ𝑋ൌ𝑘, 𝑌ൌ𝑛െ𝑘ሽ
ൌ
෍
௞ൌ଴
௡
𝑃ሼ𝑋ൌ𝑘ሽ𝑃ሼ𝑌ൌ𝑛െ𝑘ሽ
ൌ
෍
௞ൌ଴
௡
𝑒െఒభ𝜆௞
ଵ
𝑘! 𝑒െఒమ𝜆௡െ௞
ଶ
ሺ𝑛െ𝑘ሻ!
ൌ𝑒െሺఒభ൅ఒమሻ
෍
௞ൌ଴
௡
𝜆௞
ଵ𝜆௡െ௞
ଶ
𝑘!ሺ𝑛െ𝑘ሻ!
ൌ𝑒െሺఒభ൅ఒమሻ
𝑛!
෍
௞ൌ଴
௡
𝑛!
𝑘!ሺ𝑛െ𝑘ሻ! 𝜆௞
ଵ𝜆௡െ௞
ଶ
ൌ𝑒െሺఒభ൅ఒమሻ
𝑛!
ሺ𝜆ଵ൅𝜆ଶሻ௡
𝑋൅𝑋
𝜆ଵ൅𝜆ଶ.
𝑋
𝑌
𝑛, 𝑝
𝑚, 𝑝
𝑋൅𝑌.
𝑋൅𝑌
ሺ𝑛൅𝑚, 𝑝ሻ.
𝑋
𝑛
𝑝
𝑌
𝑚
𝑝.
𝑋
𝑌
𝑋൅𝑌
𝑛൅𝑚
𝑝
𝑋൅𝑌
ሺ𝑛൅𝑚, 𝑝ሻ.
419 of 848

where 
 and where 
 when 
 Thus,
and the conclusion follows upon application of the combinatorial identity
Recall that for any two events  and 
 the conditional probability of  given  is
defined, provided that 
 by
Hence, if  and  are discrete random variables, it is natural to define the conditional
probability mass function of  given that 
 by
for all values of  such that 
 Similarly, the conditional probability
distribution function of  given that 
 is defined, for all  such that 
 by
𝑃ሼ𝑋൅𝑌ൌ𝑘ሽ
ൌ෍
௜ൌ଴
௡
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑘െ𝑖ሽ
ൌ෍
௜ൌ଴
௡
𝑃ሼ𝑋ൌ𝑖ሽ𝑃ሼ𝑌ൌ𝑘െ𝑖ሽ
ൌ෍
௜ൌ଴
௡
ቆ𝑛
𝑖ቇ𝑝௜𝑞௡െ௜ቆ𝑚
𝑘െ𝑖ቇ𝑝௞െ௜𝑞௠െ௞൅௜
𝑞ൌ1 െ𝑝
ቆ𝑟
𝑗ቇൌ0
𝑗൏0 .
𝑃ሼ𝑋൅𝑌ൌ𝑘ሽൌ𝑝௞𝑞௡൅௠െ௞෍
௜ൌ଴
௡
ቆ𝑛
𝑖ቇቆ𝑚
𝑘െ𝑖ቇ
ቆ𝑛൅𝑚
𝑘
ቇൌ෍
௜ൌ଴
௡
ቆ𝑛
𝑖ቇቆ𝑚
𝑘െ𝑖ቇ
𝐸
𝐹,
𝐸
𝐹
𝑃ሺ𝐹ሻ൐0,
𝑃ሺ𝐸|𝐹ሻൌ𝑃ሺ𝐸𝐹ሻ
𝑃ሺ𝐹ሻ
𝑋
𝑌
𝑋
𝑌ൌ𝑦,
𝑝௑ห௒ሺ𝑥ቚ𝑦ሻ
ൌ𝑃ሼ𝑋ൌ𝑥||𝑌ൌ𝑦ሽ
ൌ𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽ
𝑃ሼ𝑌ൌ𝑦ሽ
ൌ𝑝ሺ𝑥, 𝑦ሻ
𝑝௒ሺ𝑦ሻ
𝑦
𝑝௒ሺ𝑦ሻ൐0 .
𝑋
𝑌ൌ𝑦
𝑦
𝑝௒ሺ𝑦ሻ൐0,
420 of 848

In other words, the definitions are exactly the same as in the unconditional case,
except that everything is now conditional on the event that 
 If  is independent
of 
 then the conditional mass function and the distribution function are the same as
the respective unconditional ones. This follows because if  is independent of 
then
Example 4a
Suppose that 
 the joint probability mass function of  and 
 is given by
Calculate the conditional probability mass function of  given that 
Solution
We first note that
Hence,
and
𝐹௑ห௒ሺ𝑥ቚ𝑦ሻ
ൌ𝑃ሼ𝑋൑𝑥||𝑌ൌ𝑦ሽ
ൌ
෍
௔൑௫
𝑝௑ห௒ሺ𝑎ቚ𝑦ሻ
𝑌ൌ𝑦.
𝑋
𝑌,
𝑋
𝑌,
𝑝௑ห௒ሺ𝑥ቚ𝑦ሻ
ൌ𝑃ሼ𝑋ൌ𝑥||𝑌ൌ𝑦ሽ
ൌ𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽ
𝑃ሼ𝑌ൌ𝑦ሽ
ൌ𝑃ሼ𝑋ൌ𝑥ሽ𝑃ሼ𝑌ൌ𝑦ሽ
𝑃ሼ𝑌ൌ𝑦ሽ
ൌ𝑃ሼ𝑋ൌ𝑥ሽ
𝑝ሺ𝑥, 𝑦ሻ,
𝑋
𝑌,
𝑝ሺ0, 0ሻൌ. 4 𝑝ሺ0, 1ሻൌ. 2 𝑝ሺ1, 0ሻൌ. 1 𝑝ሺ1, 1ሻൌ. 3
𝑋
𝑌ൌ1 .
𝑝௒ሺ1ሻൌ෍
௫
𝑝ሺ𝑥, 1ሻൌ𝑝ሺ0, 1ሻ൅𝑝ሺ1, 1ሻൌ. 5
𝑝௑ห௒ሺ0|1ሻൌ𝑝ሺ0, 1ሻ
𝑝௒ሺ1ሻൌ2
5
𝑝௑ห௒ሺ1|1ሻൌ𝑝ሺ1, 1ሻ
𝑝௒ሺ1ሻൌ3
5
421 of 848

Example 4b
If  and  are independent Poisson random variables with respective parameters
 and 
 calculate the conditional distribution of  given that 
Solution
We calculate the conditional probability mass function of  given that 
as follows:
where the last equality follows from the assumed independence of  and .
Recalling (Example 3e
) that 
 has a Poisson distribution with parameter
 we see that the preceding equals
In other words, the conditional distribution of  given that 
 is the
binomial distribution with parameters  and 
We can also talk about joint conditional distributions, as is indicated in the next two
examples.
Example 4c
Consider the multinomial distribution with joint probability mass function
Such a mass function results when  independent trials are performed, with each
𝑋
𝑌
𝜆ଵ
𝜆ଶ,
𝑋
𝑋൅𝑌ൌ𝑛.
𝑋
𝑋൅𝑌ൌ𝑛
𝑃ሼ𝑋ൌ𝑘||𝑋൅𝑌ൌ𝑛ሽ
ൌ𝑃ሼ𝑋ൌ𝑘, 𝑋൅𝑌ൌ𝑛ሽ
𝑃ሼ𝑋൅𝑌ൌ𝑛ሽ
ൌ𝑃ሼ𝑋ൌ𝑘, 𝑌ൌ𝑛െ𝑘ሽ
𝑃ሼ𝑋൅𝑌ൌ𝑛ሽ
ൌ𝑃ሼ𝑋ൌ𝑘ሽ𝑃ሼ𝑌ൌ𝑛െ𝑘ሽ
𝑃ሼ𝑋൅𝑌ൌ𝑛ሽ
𝑋
𝑌
𝑋൅𝑌
𝜆ଵ൅𝜆ଶ,
𝑃ሼ𝑋ൌ𝑘||𝑋൅𝑌ൌ𝑛
ൌ
௘െഊభఒభೖ
௞!
௘െഊమఒమ೙െೖ
ሺ௡െ௞ሻ!
൤
௘െሺഊభ൅ഊమሻሺఒభ൅ఒమሻ೙
௡!
൨
െଵ
ൌ
௡!
ሺ௡െ௞ሻ!
ఒభೖఒమ೙െೖ
ሺఒభ൅ఒమሻ೙
ൌቆ𝑛
𝑘ቇ൬
ఒభ
ఒభ൅ఒమ൰
௞
൬
ఒమ
ఒభ൅ఒమ൰
௡െ௞
𝑋
𝑋൅𝑌ൌ𝑛
𝑛
𝜆ଵ/ሺ𝜆ଵ൅𝜆ଶሻ.
𝑃ሼ𝑋௜ൌ𝑛௜, 𝑖ൌ1, … , 𝑘ሽൌ
𝑛!
𝑛ଵ!⋯𝑛௞! 𝑝ଵ
௡భ⋯𝑝௞
௡ೖ,  𝑛௜൒0,   ෍
௜ൌଵ
௞
𝑛௜ൌ𝑛
𝑛
422 of 848

trial resulting in outcome  with probability 
The random variables
 represent, respectively, the number of trials that result in outcome
 Suppose we are given that 
 of the trials resulted in outcome 
 for
 where
Then, because each of the other 
 trials must have resulted in one of the
outcomes 
 it would seem that the conditional distribution of 
 is the
multinomial distribution on 
 trials with respective trial outcome probabilities
where 
 is the probability that a trial results in one of the outcomes
Solution
To verify this intuition, let 
 be such that 
 Then
where the probability in the denominator was obtained by regarding outcomes
 as a single outcome having probability 
 thus showing that the
probability is a multinomial probability on  trials with outcome probabilities
 Because 
 the preceding can be written as
and our intuition is upheld.
Example 4d
Consider  independent trials, with each trial being a success with probability 
𝑖
𝑝௜,
෍
௜ൌଵ
௞
𝑝௜ൌ1.
𝑋௜, 𝑖ൌ1, … , 𝑘,
𝑖, 𝑖ൌ1, … , 𝑘.
𝑛௝
𝑗,
𝑗ൌ𝑟൅1, … , 𝑘,
Σ௝ൌ௥൅ଵ
௞
 𝑛௝ൌ𝑚൑𝑛.
𝑛െ𝑚
1, … , 𝑟,
𝑋ଵ, … , 𝑋௥
𝑛െ𝑚
𝑃ሼoutcome 𝑖|outcome is not any of 𝑟൅1, … ,kሽൌ
𝑝௜
𝐹௥
,  𝑖ൌ1, … , 𝑟
𝐹௥ൌΣ௜ൌଵ
௞
 𝑝௜
1, … , 𝑟.
𝑛ଵ, … , 𝑛௥,
෍
௜ൌଵ
௥
𝑛௜ൌ𝑛െ𝑚.
𝑃ሼ𝑋ଵൌ𝑛ଵ, … , 𝑋௥ൌ𝑛௥||𝑋௥൅ଵൌ𝑛௥൅ଵ, … 𝑋௞
ൌ𝑛௞ሽ
ൌ
𝑃ሼ𝑋ଵൌ𝑛ଵ,  … ,  𝑋௞ൌ𝑛௞ሽ
𝑃ሼ𝑋௥൅ଵൌ𝑛௥൅ଵ,  … ,  𝑋௞ൌ𝑛௞ሽ
ൌ
𝑛!
𝑛!⋯𝑛௞! 𝑝ଵ
௡భ⋯𝑝௥
௡ೝ𝑝௥൅ଵ
௡ೝ൅భ ⋯𝑝௞
௡ೖ
𝑛!
ሺ𝑛െ𝑚ሻ!𝑛௥൅ଵ!⋯𝑛௞! 𝐹௥
௡െ௠𝑝௥൅ଵ
௡ೝ൅భ⋯𝑝௞
௡ೖ
1, … , 𝑟
𝐹௥,
𝑛
𝐹௥, 𝑝௥൅ଵ, … , 𝑝௞.
෍
௜ൌଵ
௥
𝑛௜ൌ𝑛െ𝑚,
𝑃ሼ𝑋ଵൌ𝑛ଵ, … , 𝑋௥ൌ𝑛௥||𝑋௥൅ଵൌ𝑛௥൅ଵ, … 𝑋௞ൌ𝑛௞ሽ
ൌሺ𝑛െ𝑚ሻ!
𝑛ଵ!⋯𝑛௥! ቆ
𝑝ଵ
𝐹௥
ቇ
௡భ
⋯ቆ
𝑝௥
𝐹௥
ቇ
௡ೝ
𝑛
𝑝.
423 of 848

Given a total of  successes, show that all possible orderings of the  successes
and 
 failures are equally likely.
Solution
We want to show that given a total of  successes, each of the 
possible
orderings of  successes and 
 failures is equally likely. Let  denote the
number of successes, and consider any ordering of  successes and 
failures, say, 
 Then
If  and  have a joint probability density function 
 then the conditional
probability density function of  given that 
 is defined, for all values of  such
that 
 by
To motivate this definition, multiply the left-hand side by 
 and the right-hand side
by 
 to obtain
𝑘
𝑘
𝑛െ𝑘
𝑘
ቆ𝑛
𝑘ቇ
𝑘
𝑛െ𝑘
𝑋
𝑘
𝑛െ𝑘
𝐨ൌሺ𝑠, … , 𝑠, 𝑓, … , 𝑓ሻ.
𝑃ሺ𝐨||𝑋ൌ𝑘ሻ
ൌ𝑃ሺ𝐨, 𝑋ൌ𝑘ሻ
𝑃ሺ𝑋ൌ𝑘ሻ
ൌ
𝑃ሺ𝐨ሻ
𝑃ሺ𝑋ൌ𝑘ሻ
ൌ
𝑃௞ሺ1 െ𝑝ሻ௡െ௞
ቆ𝑛
𝑘ቇ𝑝௞ሺ1 െ𝑝ሻ௡െ௞
ൌ
1
ቆ𝑛
𝑘ቇ
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻ,
𝑋
𝑌ൌ𝑦
𝑦
𝑓௒ሺ𝑦ሻ൐0,
𝑓௑ห௒ሺ𝑥|𝑦ሻൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻ
𝑑𝑥
ሺ𝑑𝑥 𝑑𝑦ሻ/𝑑𝑦
𝑓௑ห௒ሺ𝑥|𝑦ሻ 𝑑𝑥
ൌ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
𝑓௒ሺ𝑦ሻ 𝑑𝑦
ൎ𝑃ሼ𝑥൑𝑋൑𝑥൅𝑑𝑥, 𝑦൑𝑌൑𝑦൅𝑑𝑦ሽ
𝑃ሼ𝑦൑𝑌൑𝑦൅𝑑𝑦ሽ
ൌ𝑃ሼ𝑥൑𝑋൑𝑥൅𝑑𝑥|𝑦൑𝑌൑𝑦൅𝑑𝑦ሽ
424 of 848

In other words, for small values of 
 and 
 represents the conditional
probability that  is between  and 
 given that  is between  and 
The use of conditional densities allows us to define conditional probabilities of events
associated with one random variable when we are given the value of a second
random variable. That is, if  and  are jointly continuous, then, for any set 
In particular, by letting 
 we can define the conditional cumulative
distribution function of X given that Y = y by
The reader should note that by using the ideas presented in the preceding
discussion, we have been able to give workable expressions for conditional
probabilities, even though the event on which we are conditioning (namely, the event
) has probability 0.
If  and  are independent continuous random variables, the conditional density of 
given that 
 is just the unconditional density of 
 This is so because, in the
independent case,
Example 5a
The joint density of  and  is given by
Compute the conditional density of  given that 
 where 
Solution
For 
 we have
𝑑𝑥
𝑑𝑦, 𝑓௑|௒ሺ𝑥|𝑦ሻ𝑑𝑥
𝑋
𝑥
𝑥൅𝑑𝑥
𝑌
𝑦
𝑦൅𝑑𝑦.
𝑋
𝑌
𝐴,
𝑃ሼ𝑋∈𝐴|𝑌ൌ𝑦ሽൌ඲
஺
𝑓௑ห௒ሺ𝑥|𝑦ሻ 𝑑𝑥
𝐴ൌሺെ∞, 𝑎ሻ
𝐹௑ห௒ሺ𝑎|𝑦ሻ≡𝑃ሼ𝑋൑𝑎|𝑌ൌ𝑦ሽൌ඲
െஶ
௔
𝑓௑|௒ሺ𝑥|𝑦ሻ 𝑑𝑥
ሼ𝑌ൌ𝑦ሽ
𝑋
𝑌
𝑋
𝑌ൌ𝑦
𝑋.
𝑓௑ห௒ሺ𝑥|𝑦ሻൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻൌ
𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ
𝑓௒ሺ𝑦ሻ
ൌ𝑓௑ሺ𝑥ሻ
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌቐ
12
5 𝑥ሺ2 െ𝑥െ𝑦ሻ 0 ൏𝑥൏1, 0 ൏𝑦൏1
0
 otherwise
𝑋
𝑌ൌ𝑦,
0 ൏𝑦൏1 .
0 ൏𝑥൏1, 0 ൏𝑦൏1,
425 of 848

Example 5b
Suppose that the joint density of  and  is given by
Find 
Solution
We first obtain the conditional density of  given that 
Hence,
Example 5c The t-distribution
𝑓௑ห௒ሺ𝑥|𝑦ሻ
ൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻ
ൌ
𝑓ሺ𝑥, 𝑦ሻ
׬െஶ
ஶ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥
ൌ
𝑥ሺ2 െ𝑥െ𝑦ሻ
׬଴
ଵ𝑥ሺ2 െ𝑥െ𝑦ሻ 𝑑𝑥
ൌ𝑥ሺ2 െ𝑥െ𝑦ሻ
2
3 െ𝑦/2
ൌ6𝑥ሺ2 െ𝑥െ𝑦ሻ
4 െ3𝑦
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ൞
𝑒െ௫/௬𝑒െ௬
𝑦
 0 ൏𝑥൏∞, 0 ൏𝑦൏∞
0
 otherwise
𝑃ሼ𝑋൐1||𝑌ൌ𝑦ሽ.
𝑋
𝑌ൌ𝑦.
𝑓௑ห௒ሺ𝑥ቚ𝑦ሻ
ൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻ
ൌ
𝑒െ௫/௬𝑒െ௬/𝑦
𝑒െ௬׬଴
ஶሺ1/𝑦ሻ𝑒െ௫/௬ 𝑑𝑥
ൌ1
𝑦𝑒െ௫/௬
𝑃ሼ𝑋൐1||𝑌ൌ𝑦ሽ
ൌ׬ଵ
ஶ1
𝑦𝑒െ௫/௬ 𝑑𝑥
ൌെ𝑒െ௫/௬หଵ
ஶ
ൌ𝑒െଵ/௬
426 of 848

If  and  are independent, with  having a standard normal distribution and 
having a chi-squared distribution with  degrees of freedom, then the random
variable  defined by
is said to have a t-distribution with  degrees of freedom. As will be seen in
Section 7.8
, the t-distribution has important applications in statistical
inference. At present, we will content ourselves with computing its density
function. This will be accomplished by using the conditional density of  given 
to obtain the joint density function of  and 
 from which we will then obtain the
marginal density of 
 To begin, note that because of the independence of  and
 it follows that the conditional distribution of  given that 
 is the
distribution of 
 which is normal with mean  and variance 
 Hence,
the conditional density of  given that 
 is
Using the preceding, along with the following formula for the chi-squared density
given in Example 3b
 of this chapter,
we obtain that the joint density of 
 is
Letting 
 and integrating the preceding over all 
 gives
𝑍
𝑌
𝑍
𝑌
𝑛
𝑇
𝑇ൌ
𝑍
𝑌/𝑛
ඥ
ൌ
𝑛
√  𝑍
𝑌
√
𝑛
𝑇
𝑌
𝑇
𝑌,
𝑇.
𝑍
𝑌,
𝑇
𝑌ൌ𝑦
𝑛/𝑦
ඥ
 𝑍,
0
𝑛/𝑦.
𝑇
𝑌ൌ𝑦
𝑓்|௒ሺ𝑡|𝑦ሻൌ
1
2𝜋𝑛/𝑦
ඥ
 𝑒െ௧మ௬/ଶ௡ ,   െ∞൏𝑡൏∞
𝑓௒ሺ𝑦ሻൌ𝑒െ௬/ଶ𝑦௡/ଶെଵ
2௡/ଶ Γሺ𝑛/2ሻ
 ,  𝑦൐0
𝑇, 𝑌
𝑓்,௒ሺ𝑡, 𝑦ሻ
ൌ
1
2𝜋𝑛
√
 2௡/ଶ Γሺ𝑛/2ሻ
 𝑒െ௧మ௬/ଶ௡𝑒െ௬/ଶ𝑦ሺ௡െଵሻ/ଶ
ൌ
1
𝜋𝑛
√
 2ሺ௡൅ଵሻ/ଶ Γሺ𝑛/2ሻ
𝑒െ𝑡ଶ൅𝑛
2𝑛
 𝑦  𝑦ሺ௡െଵሻ/ଶ ,  𝑦൐0,   െ∞൏𝑡൏∞
 𝑐ൌ𝑡ଶ൅𝑛
2𝑛
,
𝑦,
427 of 848

Example 5d The bivariate normal distribution
One of the most important joint distributions is the bivariate normal distribution.
We say that the random variables 
 have a bivariate normal distribution if, for
constants 
 their joint density function is given,
for all 
 by
We now determine the conditional density of  given that 
 In doing so, we
will continually collect all factors that do not depend on  and represent them by
the constants 
 The final constant will then be found by using that
 We have
𝑓்ሺ𝑡ሻ
ൌ׬଴
ஶ𝑓்,௒ሺ𝑡, 𝑦ሻ 𝑑𝑦
ൌ
1
𝜋𝑛
√
 2ሺ௡൅ଵሻ/ଶ Γሺ𝑛/2ሻ
׬଴
ஶ𝑒െ௖௬ 𝑦ሺ௡െଵሻ/ଶ 𝑑𝑦
ൌ
𝑐െሺ௡൅ଵሻ/ଶ
𝜋𝑛
√
  2ሺ௡൅ଵሻ/ଶ Γሺ𝑛/2ሻ
׬଴
ஶ𝑒െ௫𝑥ሺ௡െଵሻ/ଶ 𝑑𝑥  ሺby letting  𝑥ൌ𝑐𝑦ሻ
ൌ
𝑛ሺ௡൅ଵሻ/ଶ Γቆ𝑛൅1
2
ቇ
𝜋𝑛
√
 ሺ𝑡ଶ൅𝑛ሻ
ሺ௡൅ଵሻ/ଶ Γ൬𝑛
2൰
 ቆ because   1
𝑐ൌ
2𝑛
𝑡ଶ൅𝑛ቇ
ൌ
Γቆ𝑛൅1
2
ቇ
𝜋𝑛
√
  Γ൬𝑛
2൰
 ቆ1 ൅𝑡ଶ
𝑛ቇ
െሺ௡൅ଵሻ/ଶ
 ,   െ∞൏𝑡൏∞
𝑋, 𝑌
𝜇௫, 𝜇௬, 𝜎௫൐0, 𝜎௬൐0, െ1 ൏𝜌൏1,
െ∞൏𝑥, 𝑦൏∞,
𝑓ሺ𝑥, 𝑦ሻൌ
1
2𝜋𝜎௫𝜎௬
1 െ𝜌ଶ
ඥ
 exp൝െ
1
2ሺ1 െ𝜌ଶሻ൥ቆ
𝑥െ𝜇௫
𝜎௫
ቇ
ଶ
  ൅ ቆ
𝑦െ𝜇௬
𝜎௬
ቇ
ଶ
െ2𝜌
ሺ𝑥െ𝜇௫ሻሺ𝑦െ𝜇௬ሻ
𝜎௫𝜎௬
൩ൡ
𝑋
𝑌ൌ𝑦.
𝑥
𝐶௜.
඲
െஶ
ஶ
𝑓௑ห௒ሺ𝑥|𝑦ሻ 𝑑𝑥ൌ1 .
𝑓௑ห௒ሺ𝑥|𝑦ሻ
ൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻ
ൌ𝐶ଵ𝑓ሺ𝑥, 𝑦ሻ
ൌ𝐶ଶexp൝െ
1
2ሺ1 െ𝜌ଶሻ൥ቆ
𝑥െ𝜇௫
𝜎௫
ቇ
ଶ
െ2𝜌
𝑥ሺ𝑦െ𝜇௬ሻ
𝜎௫𝜎௬
൩ൡ
ൌ𝐶ଷ expቊെ
1
2𝜎௫
ଶሺ1 െ𝜌ଶሻቈ𝑥ଶെ2𝑥ቆ𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑦െ𝜇௬ሻቇ቉ቋ
ൌ𝐶ସ exp൝െ
1
2𝜎௫
ଶሺ1 െ𝜌ଶሻቈ𝑥െቆ𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑦െ𝜇௬ሻቇ቉
ଶ
ൡ
428 of 848

Recognizing the preceding equation as a normal density, we can conclude that
given 
 the random variable  is normally distributed with mean
 and variance 
 Also, because the joint density of
 is exactly the same as that of 
 except that 
 are interchanged with
 it similarly follows that the conditional distribution of  given 
 is the
normal distribution with mean 
 and variance 
 It
follows from these results that the necessary and sufficient condition for the
bivariate normal random variables  and  to be independent is that 
 (a
result that also follows directly from their joint density, because it is only when
 that the joint density factors into two terms, one depending only on  and
the other only on ).
With 
 the marginal density of  can be obtained from
Now, with 
Hence, making the change of variable 
 yields that
where  does not depend on 
 But this shows that  is normal with mean 
and variance 
Similarly, we can show that Y is normal with mean 
 and
𝑌ൌ𝑦,
𝑋
𝜇௫൅𝜌
ఙ௫
ఙ௬ሺ𝑦െ𝜇௬ሻ
𝜎ଶ
௫ሺ1 െ𝜌ଶሻ.
𝑌, 𝑋
𝑋, 𝑌,
𝜇௫, 𝜎௫
𝜇௬, 𝜎௬,
𝑌
𝑋ൌ𝑥
𝜇௬൅𝜌
ఙ௬
ఙ௫ሺ𝑥െ𝜇௫ሻ
𝜎ଶ
௬ሺ1 െ𝜌ଶሻ.
𝑋
𝑌
𝜌ൌ0
𝜌ൌ0
𝑥
𝑦
𝐶ൌ
1
2𝜋𝜎௫𝜎௬
1 െ𝜌ଶ
ඥ
,
𝑋
𝑓௑ሺ𝑥ሻ
ൌ׬െஶ
ஶ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑦
ൌ𝐶׬െஶ
ஶexp൝െ
1
2ሺ1 െ𝜌ଶሻ൥ቆ
𝑥െ𝜇௫
𝜎௫
ቇ
ଶ
൅ቆ
𝑦െ𝜇௬
𝜎௬
ቇ
ଶ
െ2𝜌
ሺ𝑥െ𝜇௫ሻሺ𝑦െ𝜇௬ሻ
𝜎௫𝜎௬
൩ൡ 𝑑𝑦
𝑤ൌ
𝑦െ𝜇௬
𝜎௬
,
ሺ
𝑥െ𝜇௫
𝜎௫
ሻ
ଶ
൅ሺ
𝑦െ𝜇௬
𝜎௬
ሻ
ଶ
െ
2𝜌ሺ𝑥െ𝜇௫ሻሺ𝑦െ𝜇௬ሻ
𝜎௫𝜎௬
ൌሺ
𝑥െ𝜇௫
𝜎௫
ሻ
ଶ
൅𝑤ଶെ
2𝜌ሺ𝑥െ𝜇௫ሻ𝑤
𝜎௫
ൌሺ𝑤െ
𝜌ሺ𝑥െ𝜇௫ሻ
𝜎௫
ሻ
ଶ
൅ሺ1 െ𝜌ଶሻሺ
𝑥െ𝜇௫
𝜎௫
ሻ
ଶ
𝑤ൌ
𝑦െ𝜇௬
௬
𝑓௑ሺ𝑥ሻ
ൌ𝐶𝜎௬𝑒െሺ௫െఓೣሻమ/ଶఙೣమ׬െஶ
ஶexp൝െ
1
2ሺ1 െ𝜌ଶሻሺ𝑤െ
𝜌ሺ𝑥െ𝜇௫ሻ
𝜎௫
ሻ
ଶ
ൡ𝑑𝑤
ൌ𝐶𝜎௬𝑒െሺ௫െఓೣሻమ/ଶೣమ׬െஶ
ஶexpቊെ
𝑣ଶ
2ሺ1 െ𝜌ଶሻቋ𝑑𝑣 by letting 𝑣ൌ𝑤െ
𝜌ሺ𝑥െ𝜇௫ሻ
𝜎௫
ൌ𝐾𝑒െሺ௫െఓೣሻమ/ଶఙೣమ
𝐾
𝑥.
𝑋
𝜇௫
𝜎௫
ଶ.
𝜇௬
429 of 848

variance 
We can also talk about conditional distributions when the random variables are
neither jointly continuous nor jointly discrete. For example, suppose that  is a
continuous random variable having probability density function  and  is a discrete
random variable, and consider the conditional distribution of  given that 
Then
and letting 
 approach 0 gives
thus showing that the conditional density of  given that 
 is given by
Example 5e
Consider 
 trials having a common probability of success. Suppose,
however, that this success probability is not fixed in advance but is chosen from a
uniform (0, 1) population. What is the conditional distribution of the success
probability given that the 
 trials result in  successes?
Solution
If we let  denote the probability that a given trial is a success, then  is a
uniform (0, 1) random variable. Also, given that 
 the 
 trials are
independent with common probability of success 
 so 
 the number of
successes, is a binomial random variable with parameters 
 Hence, the
conditional density of  given that 
 is
𝜎௬
ଶ.
𝑋
𝑓
𝑁
𝑋
𝑁ൌ𝑛.
𝑃ሼ𝑥൏𝑋〈𝑥൅𝑑𝑥|𝑁ൌ𝑛ሽ
𝑑𝑥
  ൌ𝑃ሼ𝑁ൌ𝑛||𝑥൏𝑋൏𝑥൅𝑑𝑥ሽ
𝑃ሼ𝑁ൌ𝑛ሽ
𝑃ሼ𝑥൏𝑋൏𝑥൅𝑑𝑥ሽ
𝑑𝑥
𝑑𝑥
lim
ௗ௫→଴
𝑃ሼ𝑥൏𝑋൏𝑥൅𝑑𝑥||𝑁ൌ𝑛ሽ
𝑑𝑥
ൌ𝑃ሼ𝑁ൌ𝑛||𝑋ൌ𝑥ሽ
𝑃ሼ𝑁ൌ𝑛ሽ
𝑓ሺ𝑥ሻ
𝑋
𝑁ൌ𝑛
𝑓௑หேሺ𝑥|𝑛ሻൌ𝑃ሼ𝑁ൌ𝑛||𝑋ൌ𝑥ሽ
𝑃ሼ𝑁ൌ𝑛ሽ
𝑓ሺ𝑥ሻ
𝑛൅𝑚
𝑛൅𝑚
𝑛
𝑋
𝑋
𝑋ൌ𝑥,
𝑛൅𝑚
𝑥,
𝑁,
ሺ𝑛൅𝑚, 𝑥ሻ.
𝑋
𝑁ൌ𝑛
𝑓௑|ேሺ𝑥|𝑛ሻ
ൌ
𝑃൛𝑁ൌ𝑛ห𝑋ൌ𝑥ൟ𝑓௑ሺ𝑥ሻ
𝑃ሼ𝑁ൌ𝑛ሽ
ൌ
ቆ𝑛൅𝑚
𝑛
ቇ𝑥௡ሺ1 െ𝑥ሻ௠
𝑃ሼ𝑁ൌ𝑛ሽ
0 ൏𝑥൏1
ൌ𝑐𝑥௡ሺ1 െ𝑥ሻ௠
430 of 848

where  does not depend on 
 Thus, the conditional density is that of a beta
random variable with parameters 
The preceding result is quite interesting, for it states that if the original or prior (to
the collection of data) distribution of a trial success probability is uniformly
distributed over (0, 1) [or, equivalently, is beta with parameters (1, 1)], then the
posterior (or conditional) distribution given a total of  successes in 
 trials is
beta with parameters 
 This is valuable, for it enhances our
intuition as to what it means to assume that a random variable has a beta
distribution.
We are often interested in the conditional distribution of a random variable  given
that  lies in some set 
 When  is discrete, the conditional probability mass
function is given by
Similarly, when  is continuous with density function 
 the conditional density
function of  given that 
 is
Example 5f
A Pareto random variable with positive parameters 
 has distribution function
and density function
An important feature of Pareto distributions is that for 
 the conditional
distribution of a Pareto random variable  with parameters  and 
 given that it
exceeds 
 is the Pareto distribution with parameters 
 and 
 This follows
because
thus verifying that the conditional distribution is Pareto with parameters 
 and 
𝑐
𝑥.
𝑛൅1, 𝑚൅1 .
𝑛
𝑛൅𝑚
ሺ1 ൅𝑛, 1 ൅𝑚ሻ.
𝑋
𝑋
𝐴.
𝑋
𝑃ሺ𝑋ൌ𝑥|𝑋∈𝐴ሻൌ𝑃ሺ𝑋ൌ𝑥, 𝑋∈𝐴ሻ
𝑃ሺ𝑋∈𝐴ሻ
ൌ൞
𝑃ሺ𝑋ൌ𝑥ሻ
𝑃ሺ𝑋∈𝐴ሻ,
 if   𝑥∈𝐴
0,
 if   𝑥∉𝐴
𝑋
𝑓,
𝑋
𝑋∈𝐴
𝑓௑ห௑∈஺ሺ𝑥ሻൌ
𝑓ሺ𝑥ሻ
𝑃ሺ𝑋∈𝐴ሻൌ
𝑓ሺ𝑥ሻ
׬஺𝑓൫𝑦൯𝑑𝑦 ,  𝑥∈𝐴
𝑎, 𝜆
𝐹ሺ𝑥ሻൌ1 െ𝑎ఒ𝑥െఒ,  𝑥൐𝑎
𝑓ሺ𝑥ሻൌ𝜆𝑎ఒ𝑥െఒെଵ,  𝑥൐𝑎
𝑥଴൐𝑎
𝑋
𝑎
λ ,
𝑥଴,
𝑥଴
𝜆.
𝑓௑ห௑வ௫బሺ𝑥ሻൌ
𝑓ሺ𝑥ሻ
𝑃ሼ𝑋൐𝑥଴ሽൌ𝜆𝑎ఒ𝑥ఒെଵ
𝑎ఒ𝑥଴
െఒ
ൌ 𝜆𝑥଴
ఒ 𝑥െఒെଵ,  𝑥൐𝑥଴
𝑥଴
𝜆.
431 of 848

Let 
 be  independent and identically distributed continuous random
variables having a common density  and distribution function 
 Define
The ordered values 
 are known as the order statistics
corresponding to the random variables 
 In other words, 
 are
the ordered values of 
The joint density function of the order statistics is obtained by noting that the order
statistics 
 will take on the values 
 if and only if, for some
permutation 
 of 
Since, for any permutation 
 of 
it follows that, for 
Dividing by 
 and letting 
 yields
*
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑛
𝑓
𝐹.
𝑋ሺଵሻ
ൌ smallest of 𝑋ଵ,  𝑋ଶ,  … ,  𝑋௡
𝑋ሺଶሻ
ൌ second smallest of 𝑋ଵ,  𝑋ଶ,  … ,  𝑋௡
⋮
𝑋ሺ௝ሻ
ൌ𝑗th smallest of 𝑋ଵ,  𝑋ଶ,  … ,  𝑋௡
⋮
𝑋ሺ௡ሻ
ൌ largest of 𝑋ଵ,  𝑋ଶ,  … ,  𝑋௡
𝑋ሺଵሻ൑𝑋ሺଶሻ൑⋯൑𝑋ሺ௡ሻ
𝑋ଵ, 𝑋ଶ, … , 𝑋௡.
𝑋ሺଵሻ, … , 𝑋ሺ௡ሻ
𝑋ଵ, … , 𝑋௡.
𝑋ሺଵሻ, … , 𝑋ሺ௡ሻ
𝑥ଵ൑𝑥ଶ൑⋯൑𝑥௡
ሺ𝑖ଵ, 𝑖ଶ, … , 𝑖௡ሻ
ሺ1, 2, … , 𝑛ሻ,
𝑋ଵൌ𝑥௜భ, 𝑋ଶൌ𝑥௜మ, … , 𝑋௡ൌ𝑥௜೙
ሺ𝑖ଵ, … , 𝑖௡ሻ
ሺ1, 2, … , 𝑛ሻ,
𝑃൜𝑥௜భെ𝜀
2 ൏𝑋ଵ൏𝑥௜భ൅𝜀
2, … , 𝑥௜೙െ𝜀
2 ൏𝑋௡൏𝑥௜೙൅𝜀
2ൠ
   ൎ𝜀௡𝑓௑భ,⋯,௑೙ሺ𝑥௜భ, … , 𝑥௜೙ሻ
   ൌ𝜀௡𝑓ሺ𝑥௜భሻ⋯𝑓ሺ𝑥௜೙ሻ
   ൌ𝜀௡𝑓ሺ𝑥ଵሻ⋯𝑓ሺ𝑥௡ሻ
𝑥ଵ൏𝑥ଶ൏⋯൏𝑥௡,
𝑃൜𝑥ଵെ𝜀
2 ൏𝑋ሺଵሻ൏𝑥ଵ൅𝜀
2, … , 𝑥௡െ𝜀
2 ൏𝑋ሺ௡ሻ൏𝑥௡൅𝜀
2ൠ
   ൎ𝑛! 𝜀௡𝑓ሺ𝑥ଵሻ⋯𝑓ሺ𝑥௡ሻ
𝜀௡
𝜀→0
𝑓௑ሺభሻ,… ,௑ሺ೙ሻሺ𝑥ଵ, 𝑥ଶ, … , 𝑥௡ሻൌ𝑛!𝑓ሺ𝑥ଵሻ⋯𝑓ሺ𝑥௡ሻ 𝑥ଵ൏𝑥ଶ൏⋯൏𝑥௡
(6.1)
432 of 848

Equation (6.1)
 is most simply explained by arguing that, in order for the vector
 to equal 
 it is necessary and sufficient for 
to equal one of the ! permutations of 
 Since the probability (density)
that 
 equals any given permutation of 
 is just 
Equation (6.1)
 follows.
Example 6a
Along a road 1 mile long are 3 people “distributed at random.” Find the probability
that no 2 people are less than a distance of  miles apart when 
Solution
Let us assume that “distributed at random” means that the positions of the 3
people are independent and uniformly distributed over the road. If 
 denotes the
position of the th person, then the desired probability is
 Because
it follows that
where we have made the change of variables 
 Continuing the
string of equalities yields
Hence, the desired probability that no 2 people are within a distance  of each
other when 3 people are uniformly and independently distributed over an interval
of size 1 is 
 when 
 In fact, the same method can be used to
prove that when  people are distributed at random over the unit interval, the
desired probability is
〈𝑋ሺଵሻ, … , 𝑋ሺ௡ሻ〉
〈𝑥ଵ, … , 𝑥௡〉,
〈𝑋ଵ, … , 𝑋௡〉
𝑛
〈𝑥ଵ, … , 𝑥௡〉.
〈𝑋ଵ, … , 𝑋௡〉
〈𝑥ଵ, … , 𝑥௡〉
𝑓ሺ𝑥ଵሻ⋯𝑓ሺ𝑥௡ሻ,
𝑑
𝑑൑1
2 .
𝑋௜
𝑖
𝑃൛𝑋ሺ௜ሻ൐𝑋ሺ௜െଵሻ൅𝑑, 𝑖ൌ2, 3ൟ.
𝑓௑ሺభሻ,௑ሺమሻ,௑ሺయሻሺ𝑥ଵ, 𝑥ଶ, 𝑥ଷሻൌ3! 0 ൏𝑥ଵ൏𝑥ଶ൏𝑥ଷ൏1
𝑃൛𝑋ሺ௜ሻ൐𝑋ሺ௜െଵሻ൅𝑑, 𝑖ൌ2, 3ൟ
ൌ  ׬׬׬௫೔வ௫ೕെభ൅ௗ 𝑓௑ሺభሻ,௑ሺమሻ,௑ሺయሻሺ𝑥ଵ, 𝑥ଶ, 𝑥ଷሻ 𝑑𝑥ଵ 𝑑𝑥ଶ 𝑑𝑥ଷ
ൌ3!׬଴
ଵെଶௗ׬௫భ൅ௗ
ଵെௗ׬௫మ൅ௗ
ଵ
 𝑑𝑥ଷ 𝑑𝑥ଶ 𝑑𝑥ଵ
ൌ6׬଴
ଵെଶௗ׬௫భ൅ௗ
ଵെௗሺ1 െ𝑑െ𝑥ଶሻ 𝑑𝑥ଶ 𝑑𝑥ଵ
ൌ6׬଴
ଵെଶௗ׬଴
ଵെଶௗെ௫భ𝑦ଶ 𝑑𝑦ଶ 𝑑𝑥ଵ
𝑦ଶൌ1 െ𝑑െ𝑥ଶ.
ൌ3׬଴
ଵെଶௗሺ1 െ2𝑑െ𝑥ଵሻଶ 𝑑𝑥ଵ
ൌ3׬଴
ଵെଶௗ𝑦ଵ
ଶ 𝑑𝑦ଵ
ൌሺ1 െ2𝑑ሻଷ
𝑑
ሺ1 െ2𝑑ሻଷ
𝑑൑1
2 .
𝑛
433 of 848

The proof is left as an exercise.
The density function of the th-order statistic 
 can be obtained either by
integrating the joint density function (6.1
) or by direct reasoning as follows: In
order for 
 to equal 
 it is necessary for 
 of the  values 
 to be less
than 
 of them to be greater than 
 and 1 of them to equal 
 Now, the
probability density that any given set of 
 of the 
‘s are less than 
 another
given set of 
 are all greater than 
 and the remaining value is equal to  equals
Hence, since there are
different partitions of the  random variables 
 into the preceding three
groups, it follows that the density function of 
 is given by
Example 6b
When a sample of 
 random variables (that is, when 
 independent
and identically distributed random variables) is observed, the 
 smallest is
called the sample median. If a sample of size 3 from a uniform distribution over
(0, 1) is observed, find the probability that the sample median is between  and
Solution
From Equation (6.2)
, the density of 
 is given by
Hence,
⎡
⎣
1 െሺ𝑛െ1ቇ𝑑⎤
⎦
௡  when 𝑑൑
1
𝑛െ1
𝑗
𝑋ሺ௝ሻ
𝑋ሺ௝ሻ
𝑥,
𝑗െ1
𝑛
𝑋ଵ, … , 𝑋௡
𝑥, 𝑛െ𝑗
𝑥,
𝑥.
𝑗െ1
𝑋௜
𝑥,
𝑛െ𝑗
𝑥,
𝑥
ൣ𝐹൫𝑥൯൧௝െଵൣ1 െ𝐹൫𝑥൯൧௡െ௝𝑓൫𝑥ሻ
ቆ
𝑛
𝑗െ1, 𝑛െ𝑗, 1ቇൌ
𝑛!
ሺ𝑛െ𝑗ሻ!ሺ𝑗െ1ሻ!
𝑛
𝑋ଵ, … , 𝑋௡
𝑋ሺ௝ሻ
𝑓௑ሺೕሻሺ𝑥ሻൌ
𝑛!
ሺ𝑛െ𝑗ሻ!ሺ𝑗െ1ሻ!ሾ𝐹ሺ𝑥ሻሿ௝െଵሾ1 െ𝐹ሺ𝑥ሻሿ௡െ௝𝑓ሺ𝑥ሻ
(6.2)
2𝑛൅1
2𝑛൅1
ሺ𝑛൅1ሻ
1
4
3
4 .
𝑋ሺଶሻ
𝑓௑ሺమሻሺ𝑥ሻൌ3!
1!1! 𝑥ሺ1 െ𝑥ሻ  0 ൏𝑥൏1
434 of 848

The cumulative distribution function of 
 can be found by integrating Equation
(6.2)
. That is,
However, 
 could also have been derived directly by noting that the th order
statistic is less than or equal to  if and only if there are  or more of the 
‘s that are
less than or equal to 
 Thus, because the number of 
‘s that are less than or equal
to  is a binomial random variable with parameters 
 it follows that
If, in Equations (6.3)
 and (6.4)
, we take  to be the uniform (0, 1) distribution
[that is, 
], then we obtain the interesting analytical identity
We can employ the same type of argument that we used in establishing Equation
(6.2)
 to find the joint density of 
 and 
 the 
 and 
 smallest of the values
 For suppose that 
 and 
 Then the event that 
is equivalent to the event that the  data values can be divided into  groups of
respective sizes 
 that satisfy the condition that all 
members of the first group have values less than 
 the one member of the second
group has value 
 all 
 members of the third group have values between 
and 
 the one member of the fourth group has value 
 and all 
 members of
the last group have values greater than 
 Now, for any specified division of the 
𝑃ቊ1
4 ൏𝑋ሺଶሻ൏3
4ቋ
ൌ6׬ଵ/ସ
ଷ/ସ𝑥ሺ1 െ𝑥ሻ 𝑑𝑥
ൌ6ቊ𝑥ଶ
2 െ𝑥ଷ
3 ቋቤ
௫ൌଵ/ସ
௫ൌଷ/ସ
ൌ11
16
𝑋ሺ௝ሻ
𝐹௑ሺೕሻሺ𝑦ሻൌ
𝑛!
ሺ𝑛െ𝑗ሻ!ሺ𝑗െ1ሻ!඲
െஶ
௬
ሾ𝐹ሺ𝑥ሻሿ௝െଵሾ1 െ𝐹ሺ𝑥ሻሿ௡െ௝𝑓ሺ𝑥ሻ 𝑑𝑥
(6.3)
𝐹௑ሺೕሻሺ𝑦ሻ
𝑗
𝑦
𝑗
𝑋௜
𝑦.
𝑋௜
𝑦
𝑛, 𝑝ൌ𝐹ሺ𝑦ሻ,
𝐹௑ሺ௝ሻሺ𝑦ሻൌ𝑃൛𝑋ሺ௝ሻ൑𝑦ൟ
ൌ𝑃൛𝑗 or more of the 𝑋௝'s are  ൑𝑦ൟ
ൌ
෍
௞ൌ௝
௡
ቆ𝑛
𝑘ቇሾ𝐹ሺ𝑦ሻሿ௞ሾ1 െ𝐹ሺ𝑦ሻሿ௡െ௞
(6.4)
𝐹
𝑓ሺ𝑥ሻൌ1, 0 ൏𝑥൏1
෍
௞ൌ௝
௡
ቆ𝑛
𝑘ቇ𝑦௞ሺ1 െ𝑦ሻ௡െ௞ൌ
𝑛!
ሺ𝑛െ𝑗ሻ!ሺ𝑗െ1ሻ!඲
଴
௬
𝑥௝െଵሺ1 െ𝑥ሻ௡െ௝ 𝑑𝑥 0 ൑𝑦൑1
(6.5)
𝑋ሺ௜ሻ
𝑋ሺ௝ሻ,
𝑖௧௛
𝑗௧௛
𝑋ଵ, … , 𝑋௡.
𝑖൏𝑗
𝑥௜൏𝑥௝.
𝑋ሺ௜ሻൌ𝑥௜, 𝑋ሺ௝ሻൌ𝑥௝
𝑛
5
𝑖െ1, 1, 𝑗െ𝑖െ1, 1, 𝑛െ𝑗,
𝑖െ1
𝑥௜,
𝑥௜,
𝑗െ𝑖െ1
𝑥௜
𝑥௝,
𝑥௝,
𝑛െ𝑗
𝑥௝.
𝑛
435 of 848

values into  such groups, the preceding condition will hold with probability (density)
As there are 
 such divisions of the  values, and as the
condition cannot hold for more than one of these divisions, it follows, for
 that
Example 6c Distribution of the range of a random sample
Suppose that  independent and identically distributed random variables
 are observed. The random variable  defined by 
 is
called the range of the observed random variables. If the random variables 
have distribution function  and density function 
 then the distribution of  can
be obtained from Equation (6.6)
 as follows: For 
Making the change of variable 
 yields
Thus,
Equation (6.7)
 can be evaluated explicitly only in a few special cases. One
5
𝐹௜െଵሺ𝑥௜ሻ𝑓ሺ𝑥௜ሻ൫𝐹ሺ𝑥௝ሻെ𝐹ሺ𝑥௜ሻ൯
௝െ௜െଵ 𝑓ሺ𝑥௝ሻ ൫1 െ𝐹ሺ𝑥௝ሻ൯
௡െ௝
𝑛!
ሺ𝑖െ1ሻ!1!ሺ𝑗െ𝑖െ1ሻ!1!ሺ𝑛െ𝑗ሻ!
𝑛
𝑖൏𝑗,  𝑥௜൏𝑥௝,
𝑓௑ሺ೔ሻ,௑ሺೕሻሺ𝑥௜, 𝑥௝ሻൌ
𝑛!
ሺ𝑖െ1ሻ!ሺ𝑗െ𝑖െ1ሻ!ሺ𝑛െ𝑗ሻ! 𝐹௜െଵሺ𝑥௜ሻ𝑓ሺ𝑥௜ሻൣ𝐹ሺ𝑥௝ሻെ𝐹ሺ𝑥௜ሻ൧
௝െ௜െଵ𝑓ሺ𝑥௝ሻൣ1 െ𝐹ሺ𝑥௝ሻ൧
௡െ௝
(6.6)
𝑛
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑅
𝑅ൌ𝑋ሺ௡ሻെ𝑋ሺଵሻ
𝑋௜
𝐹
𝑓,
𝑅
𝑎൒0,
𝑃ሼ𝑅൑𝑎ሽ
ൌ𝑃൛𝑋ሺ௡ሻെ𝑋ሺଵሻ൑𝑎ൟ
ൌ   
׬׬
௫೙െ௫భ൑௔
𝑓௑ሺభሻ,௑ሺ೙ሻሺ𝑥ଵ, 𝑥௡ሻ 𝑑𝑥ଵ 𝑑𝑥௡
ൌ׬െஶ
ஶ׬െ௫భ
௫భ൅௔
𝑛!
ሺ𝑛െ2ሻ! ሾ𝐹ሺ𝑥௡ሻെ𝐹ሺ𝑥ଵሻሿ௡െଶ𝑓ሺ𝑥ଵሻ𝑓ሺ𝑥௡ሻ 𝑑𝑥௡ 𝑑𝑥ଵ
𝑦ൌ𝐹ሺ𝑥௡ሻെ𝐹ሺ𝑥ଵሻ, 𝑑𝑦ൌ𝑓ሺ𝑥௡ሻ 𝑑𝑥௡
׬௫భ
௫భ൅௔ሾ𝐹ሺ𝑥௡ሻെ𝐹ሺ𝑥ଵሻሿ௡െଶ𝑓ሺ𝑥௡ሻ 𝑑𝑥௡
ൌ׬଴
ிሺ௫భ൅௔ሻെிሺ௫భሻ𝑦௡െଶ𝑑𝑦
ൌ
1
𝑛െ1 ሾ𝐹ሺ𝑥ଵ൅𝑎ሻെ𝐹ሺ𝑥ଵሻሿ௡െଵ
𝑃ሼ𝑅൑𝑎ሽൌ𝑛඲
െஶ
ஶ
ሾ𝐹ሺ𝑥ଵ൅𝑎ሻെ𝐹ሺ𝑥ଵሻሿ௡െଵ𝑓ሺ𝑥ଵሻ 𝑑𝑥ଵ
(6.7)
436 of 848

such case is when the 
’s are all uniformly distributed on (0, 1). In this case, we
obtain, from Equation (6.7)
, that for 
Differentiation yields the density function of the range: given in this case by
That is, the range of  independent uniform (0, 1) random variables is a beta
random variable with parameters 
Let 
 and 
 be jointly continuous random variables with joint probability density
function 
 It is sometimes necessary to obtain the joint distribution of the
random variables 
 and 
 which arise as functions of 
 and 
 Specifically,
suppose that 
 and 
 for some functions 
 and 
Assume that the functions 
 and 
 satisfy the following conditions:
1. The equations 
 and 
 can be uniquely solved for
 and 
 in terms of 
 and 
 with solutions given by, say,
2. The functions 
 and 
 have continuous partial derivatives at all points
 and are such that the 
 determinant
at all points 
Under these two conditions, it can be shown that the random variables 
 and 
 are
jointly continuous with joint density function given by
𝑋௜
0 ൏𝑎൏1,
𝑃ሼ𝑅൏𝑎ሽ
ൌ𝑛׬଴
ଵሾ𝐹ሺ𝑥ଵ൅𝑎ሻെ𝐹ሺ𝑥ଵሻሿ௡െଵ𝑓ሺ𝑥ଵሻ 𝑑𝑥ଵ
ൌ𝑛׬଴
ଵെ௔𝑎௡െଵ 𝑑𝑥ଵ൅𝑛׬ଵെ௔
ଵ
ሺ1 െ𝑥ଵሻ௡െଵ 𝑑𝑥ଵ
ൌ𝑛ሺ1 െ𝑎ሻ𝑎௡െଵ൅𝑎௡
𝑓ோሺ𝑎ሻൌ൝𝑛ሺ𝑛െ1ሻ𝑎௡െଶሺ1 െ𝑎ሻ
 0 ൑𝑎൑1
0
 otherwise
𝑛
𝑛െ1, 2 .
𝑋ଵ
𝑋ଶ
𝑓௑భ,௑మ.
𝑌ଵ
𝑌ଶ,
𝑋ଵ
𝑋ଶ.
𝑌ଵൌ𝑔ଵሺ𝑋ଵ, 𝑋ଶሻ
𝑌ଶൌ𝑔ଶሺ𝑋ଵ, 𝑋ଶሻ
𝑔ଵ
𝑔ଶ.
𝑔ଵ
𝑔ଶ
𝑦ଵൌ𝑔ଵሺ𝑥ଵ, 𝑥ଶሻ
𝑦ଶൌ𝑔ଶሺ𝑥ଵ, 𝑥ଶሻ
𝑥ଵ
𝑥ଶ
𝑦ଵ
𝑦ଶ,
𝑥ଵൌℎଵሺ𝑦ଵ, 𝑦ଶሻ, 𝑥ଶൌℎଶሺ𝑦ଵ,  𝑦ଶሻ.
𝑔ଵ
𝑔ଶ
ሺ𝑥ଵ, 𝑥ଶሻ
2 ൈ2
𝐽ሺ𝑥ଵ, 𝑥ଶሻൌተ
ተ
∂𝑔ଵ
∂𝑥ଵ
 
∂𝑔ଵ
∂𝑥ଶ
∂𝑔ଶ
∂𝑥ଵ
 
∂𝑔ଶ
∂𝑥ଶ
ተ
ተ
≡
∂𝑔ଵ
∂𝑥ଵ
∂𝑔ଶ
∂𝑥ଶ
െ
∂𝑔ଵ
∂𝑥ଶ
∂𝑔ଶ
∂𝑥ଵ
്0
ሺ𝑥ଵ, 𝑥ଶሻ.
𝑌ଵ
𝑌ଶ
437 of 848

where 
A proof of Equation (7.1)
 would proceed along the following lines:
The joint density function can now be obtained by differentiating Equation (7.2)
with respect to 
 and 
 That the result of this differentiation will be equal to the
right-hand side of Equation (7.1)
 is an exercise in advanced calculus whose proof
will not be presented in this book.
Example 7a
Let 
 and 
 be jointly continuous random variables with probability density
function 
 Let 
 Find the joint density function of
 and 
 in terms of 
Solution
Let 
 and 
 Then
Also, since the equations 
 and 
 have
 as their solution, it follows from Equation
(7.1)
 that the desired density is
For instance, if 
 and 
 are independent uniform (0, 1) random variables, then
𝑓௒భ௒మሺ𝑦ଵ, 𝑦ଶሻൌ𝑓௑భ,௑మሺ𝑥ଵ, 𝑥ଶሻቚ𝐽ሺ𝑥ଵ, 𝑥ଶሻቚ
െଵ
(7.1)
𝑥ଵൌℎଵሺ𝑦ଵ,  𝑦ଶሻ,  𝑥ଶൌℎଶሺ𝑦ଵ, 𝑦ଶሻ.
𝑃ሼ𝑌ଵ൑𝑦ଵ, 𝑌ଶ൑𝑦ଶሽൌ
඲඲
ሺ௫భ, ௫మሻ:
௚భሺ௫భ, ௫మሻ  ൑ ௬భ
௚మሺ௫భ, ௫మሻ  ൑ ௬మ
𝑓௑భ,௑మ ሺ𝑥ଵ, 𝑥ଶሻ 𝑑𝑥ଵ 𝑑𝑥ଶ
(7.2)
𝑦ଵ
𝑦ଶ.
𝑋ଵ
𝑋ଶ
𝑓௑భ,௑మ.
𝑌ଵൌ𝑋ଵ൅𝑋ଶ, 𝑌ଶൌ𝑋ଵെ𝑋ଶ.
𝑌ଵ
𝑌ଶ
𝑓௑భ,௑మ.
𝑔ଵሺ𝑥ଵ, 𝑥ଶሻൌ𝑥ଵ൅𝑥ଶ
𝑔ଶሺ𝑥ଵ, 𝑥ଶሻൌ𝑥ଵെ𝑥ଶ.
𝐽ሺ𝑥ଵ, 𝑥ଶሻൌቤ1
1
1
െ1ቤൌെ2
𝑦ଵൌ𝑥ଵ൅𝑥ଶ
𝑦ଶൌ𝑥ଵെ𝑥ଶ
𝑥ଵൌሺ𝑦ଵ൅𝑦ଶሻ/2,  𝑥ଶൌሺ𝑦ଵെ𝑦ଶሻ/2
𝑓௒భ,௒మሺ𝑦ଵ, 𝑦ଶሻൌ1
2 𝑓௑భ, ௑మ൬
𝑦ଵ൅𝑦ଶ
2
,
𝑦ଵെ𝑦ଶ
2
൰
𝑋ଵ
𝑋ଶ
438 of 848

or if 
 and 
 are independent exponential random variables with respective
parameters 
 and 
 then
Finally, if 
 and 
 are independent standard normal random variables, then
Thus, not only do we obtain (in agreement with Proposition 3.2
) that both
 and 
 are normal with mean 0 and variance 2, but we also
conclude that these two random variables are independent. (In fact, it can be
shown that if 
 and 
 are independent random variables having a common
distribution function 
 then 
 will be independent of 
 if and only if
 is a normal distribution function.)
Example 7b
Let (
) denote a random point in the plane, and assume that the rectangular
coordinates  and  are independent standard normal random variables. We are
interested in the joint distribution of 
 the polar coordinate representation of (
). (See Figure 6.4
.)
Figure 6.4
𝑓௒భ,௒మሺ𝑦ଵ, 𝑦ଶሻൌቐ
1
2
 0 ൑𝑦ଵ൅𝑦ଶ൑2,  0 ൑𝑦ଵെ𝑦ଶ൑2
0
 otherwise
𝑋ଵ
𝑋ଶ
𝜆ଵ
𝜆ଶ,
𝑓௒భ,௒మሺ𝑦ଵ, 𝑦ଶሻ
     ൌ൞
𝜆ଵ𝜆ଶ
2
exp൜െ𝜆ଵ൬
𝑦ଵ൅𝑦ଶ
2
൰െ𝜆ଶ൬
𝑦ଵെ𝑦ଶ
2
൰ൠ
  𝑦ଵ൅𝑦ଶ൒0,  𝑦ଵെ𝑦ଶ൒0
  otherwise
𝑋ଵ
𝑋ଶ
𝑓௒భ,௒మሺ𝑦ଵ, 𝑦ଶሻ
ൌ1
4𝜋𝑒െൣሺ௬భ൅௬మሻమ/଼൅ሺ௬భെ௬మሻమ/଼൧
ൌ1
4𝜋𝑒െሺ௬మభ൅௬మమሻ/ସ
ൌ
1
4𝜋
√
𝑒െ௬మభ/ସ
1
4𝜋
√
𝑒െ௬మమ/ସ
𝑋ଵ൅𝑋ଶ
𝑋ଵെ𝑋ଶ
𝑋ଵ
𝑋ଶ
𝐹,
𝑋ଵ൅𝑋ଶ
𝑋ଵെ𝑋ଶ
𝐹
𝑋, 𝑌
𝑋
𝑌
𝑅, 𝛩,
𝑥, 𝑦
• ൌ Random point . ሺ𝑋, 𝑌ሻൌሺ𝑅,  𝛩ሻ.
439 of 848

Suppose first that  and  are both positive. For  and  positive, letting
 and 
 we see that
Hence,
Because the conditional joint density function of 
 given that they are both
positive is
we see that the conditional joint density function of 
 and
 given that  and  are both positive, is
𝑋
𝑌
𝑥
𝑦
𝑟ൌ𝑔ଵሺ𝑥, 𝑦ሻൌ
𝑥ଶ൅𝑦ଶ
ඥ
𝜃ൌ𝑔ଶሺ𝑥, 𝑦ሻൌtanെଵ𝑦/𝑥,
∂𝑔ଵ
∂𝑥
ൌ
𝑥
𝑥ଶ൅𝑦ଶ
ඥ
∂𝑔ଵ
∂𝑦
ൌ
𝑦
𝑥ଶ൅𝑦ଶ
ඥ
∂𝑔ଶ
∂𝑥
ൌ
1
1 ൅ሺ𝑦/𝑥ሻଶ൬െ𝑦
𝑥ଶ൰ൌ
െ𝑦
𝑥ଶ൅𝑦ଶ
∂𝑔ଶ
∂𝑦
ൌ
1
𝑥ൣ1 ൅ሺ𝑦/𝑥ሻଶ൧
ൌ
𝑥
𝑥ଶ൅𝑦ଶ
𝐽ሺ𝑥, 𝑦ሻൌ
𝑥ଶ
ሺ𝑥ଶ൅𝑦ଶሻଷ/ଶ൅
𝑦ଶ
ሺ𝑥ଶ൅𝑦ଶሻଷ/ଶൌ
1
𝑥ଶ൅𝑦ଶ
ඥ
ൌ1
𝑟
𝑋, 𝑌
𝑓ሺ𝑥, 𝑦|𝑋൐0, 𝑌൐0ሻൌ
𝑓ሺ𝑥, 𝑦ሻ
𝑃ሺ𝑋൐0, 𝑌൐0ሻൌ2
𝜋𝑒െሺ௫మ൅௬మሻ/ଶ,  𝑥൐0, 𝑦൐0
𝑅ൌ
𝑋ଶ൅𝑌ଶ
ඥ
𝛩ൌtanെଵሺ𝑌/𝑋ሻ,
𝑋
𝑌
440 of 848

Similarly, we can show that
As the joint density is an equally weighted average of these four conditional joint
densities, we obtain that the joint density of 
 is given by
Now, this joint density factors into the marginal densities for  and 
 so  and 
are independent random variables, with  being uniformly distributed over 
and  having the Rayleigh distribution with density
(For instance, when one is aiming at a target in the plane, if the horizontal and
vertical miss distances are independent standard normals, then the absolute
value of the error has the preceding Rayleigh distribution.)
This result is quite interesting, for it certainly is not evident a priori that a random
vector whose coordinates are independent standard normal random variables will
have an angle of orientation that not only is uniformly distributed, but also is
independent of the vector’s distance from the origin.
If we wanted the joint distribution of 
 and 
 then, since the transformation
 and 
 has the Jacobian
it follows that
𝑓ሺ𝑟, 𝜃|
||
|
𝑋൐0, 𝑌൐0ሻൌ2
𝜋𝑟𝑒െ௥మ/మ,
0 ൏𝜃൏𝜋/2,
0 ൏𝑟൏∞
𝑓ሺ𝑟, 𝜃|
||
|
𝑋൏0, 𝑌൐0ሻൌ2
𝜋𝑟𝑒െ௥మ/మ,
𝜋/2 ൏𝜃൏𝜋,
0 ൏𝑟൏∞
𝑓ሺ𝑟, 𝜃|
||
|
𝑋൏0, 𝑌൏0ሻൌ2
𝜋𝑟𝑒െ௥మ/మ,
𝜋൏𝜃൏3𝜋/2,
0 ൏𝑟൏∞
𝑓ሺ𝑟, 𝜃|
||
|
𝑋൐0, 𝑌൏0ሻൌ2
𝜋𝑟𝑒െ௥మ/మ,
3𝜋/2 ൏𝜃൏2𝜋,
0 ൏𝑟൏∞
𝑅, 𝛩
𝑓ሺ𝑟, 𝜃ሻൌ1
2𝜋𝑟𝑒െ௥మ/ଶ  0 ൏𝜃൏2𝜋,   0 ൏𝑟൏∞
𝑅
𝛩,
𝑅
𝛩
𝛩
ሺ0, 2𝜋ሻ
𝑅
𝑓ሺ𝑟ሻൌ𝑟𝑒െ௥మ/ଶ  0 ൏𝑟൏∞
𝑅ଶ
𝛩,
𝑑ൌ𝑔ଵሺ𝑥, 𝑦ሻൌ𝑥ଶ൅𝑦ଶ
𝜃ൌ𝑔ଶሺ𝑥, 𝑦ሻൌtanെଵ𝑦/𝑥
𝐽ൌቮ
2𝑥
െ𝑦
𝑥ଶ൅𝑦ଶ
2𝑦
𝑥
𝑥ଶ൅𝑦ଶ
ቮൌ2
𝑓ሺ𝑑, 𝜃ሻൌ1
2 𝑒െௗ/ଶ1
2𝜋  0 ൏𝑑൏∞,   0 ൏𝜃൏2𝜋
441 of 848

Therefore, 
 and  are independent, with 
 having an exponential distribution
with parameter 
 But because 
 it follows by definition that 
 has
a chi-squared distribution with 2 degrees of freedom. Hence, we have a
verification of the result that the exponential distribution with parameter  is the
same as the chi-squared distribution with 2 degrees of freedom.
The preceding result can be used to simulate (or generate) normal random
variables by making a suitable transformation on uniform random variables. Let
 and 
 be independent random variables, each uniformly distributed over (0,
1). We will transform 
 into two independent standard normal random
variables 
 and 
 by first considering the polar coordinate representation 
of the random vector 
 From the preceding, 
 and  will be
independent, and, in addition, 
 will have an exponential
distribution with parameter 
 But 
 has such a distribution, since,
for 
Also, because 
 is a uniform 
 random variable, we can use it to
generate 
 That is, if we let
then 
 can be taken to be the square of the distance from the origin and  can
be taken to be the angle of orientation of 
 Now, since
it follows that
are independent standard normal random variables.
Example 7c
If  and  are independent gamma random variables with parameters 
 and
 respectively, compute the joint density of 
 and 
𝑅ଶ
𝛩
𝑅ଶ
1
2 .
𝑅ଶൌ𝑋ଶ൅𝑌ଶ,
𝑅ଶ
1
2
𝑈ଵ
𝑈ଶ
𝑈ଵ, 𝑈ଶ
𝑋ଵ
𝑋ଶ
ሺ𝑅, 𝛩ሻ
ሺ𝑋ଵ, 𝑋ଶሻ.
𝑅ଶ
𝛩
𝑅ଶൌ𝑋ଶ
ଵ൅𝑋ଶ
ଶ
𝜆ൌ1
2 .
െ2 log 𝑈ଵ
𝑥൐0,
𝑃ሼെ2 log 𝑈ଵ൏𝑥ሽ
ൌ𝑃൜log 𝑈ଵ൐െ𝑥
2ൠ
ൌ𝑃൛𝑈ଵ൐𝑒െ௫/ଶൟ
ൌ1 െ𝑒െ௫/ଶ
2𝜋 𝑈ଶ
ሺ0, 2𝜋ሻ
𝛩.
𝑅ଶൌെ2 log 𝑈ଵ
𝛩ൌ2𝜋𝑈ଶ
𝑅ଶ
𝜃
ሺ𝑋ଵ, 𝑋ଶሻ.
𝑋ଵൌ𝑅 cos 𝛩, 𝑋ଶൌ𝑅 sin 𝛩,
𝑋ଵൌ
െ2log𝑈ଵ
ඥ
 cos ሺ2𝜋𝑈ଶሻ
𝑋ଶൌ
െ2log𝑈ଵ
ඥ
 sin ሺ2𝜋𝑈ଶሻ
𝑋
𝑌
ሺ𝛼, 𝜆ሻ
ሺ𝛽, 𝜆ሻ,
𝑈ൌ𝑋൅𝑌
𝑉ൌ𝑋/ሺ𝑋൅𝑌ሻ.
442 of 848

Solution
The joint density of  and  is given by
Now, if 
 then
so
Finally, as the equations 
 have as their solutions
 we see that
Hence, 
 and 
 are independent, with 
 having a gamma
distribution with parameters 
 and 
 having a beta distribution
with parameters 
 The preceding reasoning also shows that 
 the
normalizing factor in the beta density, is such that
This entire result is quite interesting. For suppose there are 
 jobs to be
performed, each (independently) taking an exponential amount of time with rate 
to be completed and suppose that we have two workers to perform these jobs.
Worker I will do jobs 
 and worker II will do the remaining 
 jobs. If we let
 and  denote the total working times of workers I and II, respectively, then
(either from the foregoing result or from Example 3b
)  and  will be
𝑋
𝑌
𝑓௑,௒ሺ𝑥, 𝑦ሻ
ൌ𝜆𝑒െఒ௫ሺ𝜆𝑥ሻఈെଵ
Γሺ𝛼ሻ
𝜆𝑒െఒ௬ሺ𝜆𝑦ሻఉെଵ
Γሺ𝛽ሻ
ൌ
𝜆ఈ൅ఉ
Γሺ𝛼ሻΓሺ𝛽ሻ𝑒െఒሺ௫൅௬ሻ𝑥ఈെଵ𝑦ఉെଵ
𝑔ଵሺ𝑥, 𝑦ሻൌ𝑥൅𝑦, 𝑔ଶሺ𝑥, 𝑦ሻൌ𝑥/ሺ𝑥൅𝑦ሻ,
∂𝑔ଵ
∂𝑥ൌ
∂𝑔ଵ
∂𝑦ൌ1 
∂𝑔ଶ
∂𝑥ൌ
𝑦
ሺ𝑥൅𝑦ሻଶ 
∂𝑔ଶ
∂𝑦ൌെ
𝑥
ሺ𝑥൅𝑦ሻଶ
𝐽ሺ𝑥, 𝑦ሻൌተ
1
𝑦
ሺ𝑥൅𝑦ሻଶ
1
െ𝑥
ሺ𝑥൅𝑦ሻଶ
ተൌെ
1
𝑥൅𝑦
𝑢ൌ𝑥൅𝑦, 𝑣ൌ𝑥/ሺ𝑥൅𝑦ሻ
𝑥ൌ𝑢𝑣, 𝑦ൌ𝑢ሺ1 െ𝑣ሻ,
𝑓௎,௏ሺ𝑢, 𝑣ሻ
ൌ𝑓௑,௒ሾ𝑢𝑣, 𝑢ሺ1 െ𝑣ሻሿ𝑢
ൌ𝜆𝑒െఒ௨ሺ𝜆𝑢ሻఈ൅ఉെଵ
Γሺ𝛼൅𝛽ሻ
𝑣ఈെଵሺ1 െ𝑣ሻఉെଵΓሺ𝛼൅𝛽ሻ
Γሺ𝛼ሻΓሺ𝛽ሻ
𝑋൅𝑌
𝑋/ሺ𝑋൅𝑌ሻ
𝑋൅𝑌
ሺ𝛼൅𝛽, 𝜆ሻ
𝑋/ሺ𝑋൅𝑌ሻ
ሺ𝛼, 𝛽ሻ.
𝐵ሺ𝛼, 𝛽ሻ,
𝐵ሺ𝛼, 𝛽ሻ
≡׬଴
ଵ𝑣ఈെଵሺ1 െ𝑣ሻఉെଵ𝑑𝑣
ൌΓሺ𝛼ሻΓሺ𝛽ሻ
Γሺ𝛼൅𝛽ሻ
𝑛൅𝑚
𝜆
1, 2, … , 𝑛,
𝑚
𝑋
𝑌
𝑋
𝑌
443 of 848

independent gamma random variables having parameters 
 and 
respectively. It then follows that independently of the working time needed to
complete all 
 jobs (that is, of 
), the proportion of this work that will be
performed by worker I has a beta distribution with parameters (
).
When the joint density function of the  random variables 
 is given
and we want to compute the joint density function of 
 where
the approach is the same—namely, we assume that the functions 
 have
continuous partial derivatives and that the Jacobian determinant
at all points 
 Furthermore, we suppose that the equations
 have a unique
solution, say, 
 Under these
assumptions, the joint density function of the random variables 
 is given by
where 
Example 7d
Let 
 and 
 be independent standard normal random variables. If
 compute the joint density
function of 
Solution
Letting 
 the Jacobian of these
transformations is given by
ሺ𝑛, 𝜆ሻ
ሺ𝑚, 𝜆ሻ,
𝑛൅𝑚
𝑋൅𝑌
𝑛, 𝑚
𝑛
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑌ଵ, 𝑌ଶ, … , 𝑌௡,
𝑌ଵൌ𝑔ଵሺ𝑋ଵ, … , 𝑋௡ሻ 𝑌ଶൌ𝑔ଶሺ𝑋ଵ, … , 𝑋௡ሻ, …  𝑌௡ൌ𝑔௡ሺ𝑋ଵ, … , 𝑋௡ሻ
𝑔௜
𝐽ሺ𝑥ଵ, … , 𝑥௡ሻൌ
ተ
ተተ
ተ
∂𝑔ଵ
∂𝑥ଵ
∂𝑔ଵ
∂𝑥ଶ
⋯
∂𝑔ଵ
∂𝑥௡
∂𝑔ଶ
∂𝑥ଵ
∂𝑔ଶ
∂𝑥ଶ
⋯
∂𝑔ଶ
∂𝑥௡
∂𝑔௡
∂𝑥ଵ
∂𝑔௡
∂𝑥ଶ
⋯
∂𝑔௡
∂𝑥௡
ተ
ተተ
ተ
്0
ሺ𝑥ଵ, … , 𝑥௡ሻ.
𝑦ଵൌ𝑔ଵሺ𝑥ଵ, … , 𝑥௡ሻ, 𝑦ଶൌ𝑔ଶሺ𝑥ଵ, … , 𝑥௡ሻ, … , 𝑦௡ൌ𝑔௡ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑥ଵൌℎଵሺ𝑦ଵ, … , 𝑦௡ሻ, … , 𝑥௡ൌℎ௡ሺ𝑦ଵ, … , 𝑦௡ሻ.
𝑌௜
𝑓௒భ,… ,௒೙ሺ𝑦ଵ, … , 𝑦௡ሻൌ𝑓௑భ,… ,௑೙ሺ𝑥ଵ, … , 𝑥௡ሻቚ𝐽ሺ𝑥ଵ, … , 𝑥௡ሻቚ
െଵ
(7.3)
𝑥௜ൌℎ௜ሺ𝑦ଵ, … , 𝑦௡ሻ, 𝑖ൌ1, 2, … , 𝑛.
𝑋ଵ, 𝑋ଶ,
𝑋ଷ
𝑌ଵൌ𝑋ଵ൅𝑋ଶ൅𝑋ଷ, 𝑌ଶൌ𝑋ଵെ𝑋ଶ,   and  𝑌ଷൌ𝑋ଵെ𝑋ଷ,
𝑌ଵ, 𝑌ଶ, 𝑌ଷ.
𝑌ଵൌ𝑋ଵ൅𝑋ଶ൅𝑋ଷ, 𝑌ଶൌ𝑋ଵെ𝑋ଶ, 𝑌ଷൌ𝑋ଵെ𝑋ଷ,
444 of 848

As the preceding transformations yield that
we see from Equation (7.3)
 that
Hence, as
we see that
where
Example 7e
Let 
 be independent and identically distributed exponential random
variables with rate 
 Let
a. Find the joint density function of 
b. Use the result of part (a) to find the density of 
c. Find the conditional density of 
 given that 
𝐽ൌተ
1
1
1
1
െ1
0
1
0
െ1
ተൌ3
𝑋ଵൌ𝑌ଵ൅𝑌ଶ൅𝑌ଷ
3
 𝑋ଶൌ𝑌ଵെ2𝑌ଶ൅𝑌ଷ
3
 𝑋ଷൌ𝑌ଵ൅𝑌ଶെ2𝑌ଷ
3
𝑓௒భ, ௒మ, ௒యሺ𝑦ଵ,  𝑦ଶ,  𝑦ଷሻ
ൌ1
3 𝑓௑భ, ௑మ, ௑యቆ
𝑦ଵ൅𝑦ଶ൅𝑦ଷ
3
,
𝑦ଵെ2𝑦ଶ൅𝑦ଷ
3
,
𝑦ଵ൅𝑦ଶെ2𝑦ଷ
3
ቇ
𝑓௑భ, ௑మ, ௑యሺ𝑥ଵ,  𝑥ଶ,  𝑥ଷሻൌ
1
ሺ2𝜋ሻଷ/ଶ𝑒
െ෍
೔ൌభ
య
௫మ೔/ଶ
𝑓௒భ, ௒మ, ௒యሺ𝑦ଵ,  𝑦ଶ,  𝑦ଷሻൌ
1
3ሺ2𝜋ሻଷ/ଶ𝑒െொሺ௬భ, ௬మ, ௬యሻ/ଶ
𝑄ሺ𝑦ଵ, 𝑦ଶ, 𝑦ଷሻ
ൌቆ
𝑦ଵ൅𝑦ଶ൅𝑦ଷ
3
ቇ
ଶ
൅ቆ
𝑦ଵെ2𝑦ଶ൅𝑦ଷ
3
ቇ
ଶ
൅ቆ
𝑦ଵ൅𝑦ଶെ2𝑦ଷ
3
ቇ
ଶ
ൌ𝑦ଶ
ଵ
3 ൅2
3 𝑦ଶ
ଶ൅2
3 𝑦ଶ
ଷെ2
3 𝑦ଶ𝑦ଷ
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝜆.
𝑌௜ൌ𝑋ଵ൅⋯൅𝑋௜ 𝑖ൌ1, … , 𝑛
𝑌ଵ, … , 𝑌௡.
𝑌௡.
𝑌ଵ, … , 𝑌௡െଵ
𝑌௡ൌ𝑡.
445 of 848

Solution
a. The Jacobian of the transformations 
 is
Since only the first term of the determinant will be nonzero, we have
 Now, the joint density function of 
 is given by
Hence, because the preceding transformations yield
it follows from Equation (7.3)
 that the joint density function of 
is
b. To obtain the marginal density of 
 let us integrate out the other
variables one at a time. Doing this gives
Continuing, we obtain
The next integration yields
𝑌ଵൌ𝑋ଵ, 𝑌ଶൌ𝑋ଵ൅𝑋ଶ, … ,
𝑌௡ൌ𝑋ଵ൅⋯൅𝑋௡
𝐽ൌ
ተ
ተ
ተ
1
0
0
0
⋯
0
1
1
0
0
⋯
0
1
1
1
0
⋯
0
⋯
⋯
⋯
⋯
1
1
1
1
⋯
1
ተ
ተ
ተ
𝐽ൌ1 .
𝑋ଵ, … , 𝑋௡
𝑓௑భ,… ,௑೙ሺ𝑥ଵ, … , 𝑥௡ሻൌෑ
௜ൌଵ
௡
𝜆𝑒െఒ௫೔ 0 ൏𝑥௜൏∞,   𝑖ൌ1, … , 𝑛
𝑋ଵൌ𝑌ଵ, 𝑋ଶൌ𝑌ଶെ𝑌ଵ, … , 𝑋௜ൌ𝑌௜െ𝑌௜െଵ, … , 𝑋௡ൌ𝑌௡െ𝑌௡െଵ
𝑌ଵ, … , 𝑌௡
𝑓௒భ,… ,௒೙ሺ𝑦ଵ, … , 𝑦௡ሻൌ𝑓௑భ,… ,௑೙ሺ𝑦ଵ, 𝑦ଶെ𝑦ଵ, … , 𝑦௡െ𝑦௡െଵሻ
ൌ𝜆௡exp ቐെ𝜆 ቎𝑦ଵ൅෍
௜ൌଶ
௡
ሺ𝑦௜െ𝑦௜െଵሻ቏ቑ
ൌ𝜆௡𝑒െఒ௬೙ 0 ൏𝑦ଵ, 0 ൏𝑦௜െ𝑦௜െଵ, 𝑖ൌ2, … , 𝑛
ൌ𝜆௡𝑒െఒ௬೙ 0 ൏𝑦ଵ൏𝑦ଶ൏⋯൏𝑦௡
𝑌௡,
𝑓௒మ,… ,௒೙ሺ𝑦ଶ, … , 𝑦௡ሻ
ൌ׬଴
௬మ𝜆௡𝑒െఒ௬೙𝑑𝑦ଵ
ൌ𝜆௡𝑦ଶ𝑒െఒ௬೙ 0 ൏𝑦ଶ൏𝑦ଷ൏⋯൏𝑦௡
𝑓௒య,… ,௒೙ሺ𝑦ଷ, … , 𝑦௡ሻ
ൌ׬଴
௬య𝜆௡ 𝑦2𝑒െఒ௬೙𝑑𝑦ଶ
ൌ𝜆௡𝑦ଷ
ଶ
2 𝑒െఒ௬೙ 0 ൏𝑦ଷ൏𝑦ସ൏⋯൏𝑦௡
446 of 848

Continuing in this fashion gives
which, in agreement with the result obtained in Example 3b
, shows
that 
 is a gamma random variable with parameters  and 
c. The conditional density of 
 given that 
 is, for
Because 
 is the density of a uniform random variable on
 it follows that conditional on 
 are distributed as the order
statistics of 
 independent uniform 
 random variables.
The random variables 
 are said to be exchangeable if, for every
permutation 
 of the integers 
for all 
 That is, the  random variables are exchangeable if their joint
distribution is the same no matter in which order the variables are observed.
Discrete random variables will be exchangeable if
for all permutations 
 and all values 
 This is equivalent to stating that
 is a symmetric function of the vector
 which means that its value does not change when the values of the
vector are permuted.
𝑓௒ర,… ,௒೙ሺ𝑦ସ, … , 𝑦௡ሻൌ𝜆௡𝑦ସ
ଷ
3! 𝑒െఒ௬೙ 0 ൏𝑦ସ൏⋯൏𝑦௡
𝑓௒೙ሺ𝑦௡ሻൌ𝜆௡
𝑦௡
௡െଵ
ሺ𝑛െ1ሻ! 𝑒െఒ௬೙ 0 ൏𝑦௡
𝑋ଵ൅⋯൅𝑋௡
𝑛
𝜆.
𝑌ଵ, … , 𝑌௡െଵ
𝑌௡ൌ𝑡
0 ൏𝑦ଵ൏…  ൏𝑦௡െଵ൏𝑡,
𝑓௒భ,… ,௒೙െభห௒೙ሺ𝑦ଵ, … , 𝑦௡െଵ|𝑡ሻ
ൌ
𝑓௒భ,… ,௒೙െభ,௒೙ሺ𝑦ଵ, … , 𝑦௡െଵ, 𝑡ሻ
𝑓௒೙ሺ𝑡ሻ
ൌ
𝜆௡𝑒െఒ௧
𝜆𝑒െఒ௧ሺ𝜆𝑡ሻ௡െଵ/ሺ𝑛െ1ሻ!
ൌሺ𝑛െ1ሻ!
𝑡௡െଵ
𝑓ሺ𝑦ሻൌ1/𝑡, 0 ൏𝑦൏𝑡,
ሺ0, 𝑡ሻ,
𝑌௡ൌ𝑡, 𝑌ଵ, … , 𝑌௡െଵ
𝑛െ1
ሺ0, 𝑡ሻ
*
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑖ଵ, … , 𝑖௡
1, … , 𝑛,
𝑃൛𝑋௜భ൑𝑥ଵ, 𝑋௜మ൑𝑥ଶ, … , 𝑋௜೙൑𝑥௡ൟൌ𝑃൛𝑋ଵ൑𝑥ଵ, 𝑋ଶ൑𝑥ଶ, … , 𝑋௡൑𝑥௡ൟ
𝑥ଵ, … , 𝑥௡.
𝑛
𝑃൛𝑋௜భൌ𝑥ଵ, 𝑋௜మൌ𝑥ଶ, … , 𝑋௜೙ൌ𝑥௡ൟൌ𝑃൛𝑋ଵൌ𝑥ଵ, 𝑋ଶൌ𝑥ଶ, … , 𝑋௡ൌ𝑥௡ൟ
𝑖ଵ, … , 𝑖௡,
𝑥ଵ, … , 𝑥௡.
𝑝ሺ𝑥ଵ, 𝑥ଶ, … , 𝑥௡ሻൌ𝑃ሼ𝑋ଵൌ𝑥ଵ, … , 𝑋௡ൌ𝑥௡ሽ
ሺ𝑥ଵ, … , 𝑥௡ሻ,
447 of 848

Example 8a
Suppose that balls are withdrawn one at a time and without replacement from an
urn that initially contains  balls, of which  are considered special, in such a
manner that each withdrawal is equally likely to be any of the balls that remains
in the urn at the time. Let 
 if the th ball withdrawn is special and let 
otherwise. We will show that the random variables 
 are exchangeable.
To do so, let 
 be a vector consisting of  ones and 
 zeros.
However, before considering the joint mass function evaluated at 
 let
us try to gain some insight by considering a fixed such vector for instance,
consider the vector 
 which is assumed to have  ones and
 zeros. Then
which follows because the probability that the first ball is special is 
 the
conditional probability that the next one is special is 
 the
conditional probability that the next one is not special is 
 and so
on. By the same argument, it follows that 
 can be expressed as the
product of  fractions. The successive denominator terms of these fractions will
go from  down to 1. The numerator term at the location where the vector
 is 1 for the th time is 
 and where it is 0 for the th time it is
 Hence, since the vector 
 consists of  ones and 
zeros, we obtain
Since this is a symmetric function of 
 it follows that the random
variables are exchangeable.
Remark Another way to obtain the preceding formula for the joint probability mass
function is to regard all the  balls as distinguishable from one another. Then, since
the outcome of the experiment is an ordering of these balls, it follows that there are 
! equally likely outcomes. Finally, because the number of outcomes having special
and nonspecial balls in specified places is equal to the number of ways of permuting
the special and the nonspecial balls among themselves, namely 
 we
obtain the preceding mass function.
It is easily seen that if 
 are exchangeable, then each 
 has the same
probability distribution. For instance, if  and  are exchangeable discrete random
𝑛
𝑘
𝑋௜ൌ1
𝑖
𝑋௜ൌ0
𝑋ଵ, … , 𝑋௡
ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑘
𝑛െ𝑘
ሺ𝑥ଵ, … , 𝑥௡ሻ,
ሺ1, 1, 0, 1, 0, … , 0, 1ሻ,
𝑘
𝑛െ𝑘
𝑝ሺ1, 1, 0, 1, 0, … , 0, 1ሻൌ𝑘
𝑛
𝑘െ1
𝑛െ1
𝑛െ𝑘
𝑛െ2
𝑘െ2
𝑛െ3
𝑛െ𝑘െ1
𝑛െ4
⋯1
2
1
1
𝑘/𝑛,
ሺ𝑘െ1ሻ/ሺ𝑛െ1ሻ,
ሺ𝑛െ𝑘ሻ/ሺ𝑛െ2ሻ,
𝑝ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑛
𝑛
ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑖
𝑘െሺ𝑖െ1ሻ,
𝑖
𝑛െ𝑘െሺ𝑖െ1ሻ.
ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑘
𝑛െ𝑘
𝑝ሺ𝑥ଵ, … , 𝑥௡ሻൌ𝑘!ሺ𝑛െ𝑘ሻ!
𝑛!
 𝑥௜ൌ0, 1, ෍
௜ൌଵ
௡
𝑥௜ൌ𝑘
ሺ𝑥ଵ, … , 𝑥௡ሻ,
𝑛
𝑛
𝑘!ሺ𝑛െ𝑘ሻ!,
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑋௜
𝑋
𝑌
448 of 848

variables, then
For example, it follows from Example 8a
 that the th ball withdrawn will be special
with probability 
 which is intuitively clear, since each of the  balls is equally
likely to be the th one selected.
Example 8b
In Example 8a
, let 
 denote the selection number of the first special ball
withdrawn, let 
 denote the additional number of balls that are then withdrawn
until the second special ball appears, and, in general, let 
 denote the additional
number of balls withdrawn after the 
 special ball is selected until the th is
selected, 
For instance, if 
 and
 then 
 Now,
otherwise; thus, from the joint mass function of the 
 we obtain
Hence, the random variables 
 are exchangeable. Note that it follows
from this result that the number of cards one must select from a well-shuffled
deck until an ace appears has the same distribution as the number of additional
cards one must select after the first ace appears until the next one does, and so
on.
Example 8c
The following is known as Polya’s urn model: Suppose that an urn initially
contains  red and 
 blue balls. At each stage, a ball is randomly chosen, its
color is noted, and it is then replaced along with another ball of the same color.
Let 
 if the th ball selected is red and let it equal 0 if the th ball is blue,
 To obtain a feeling for the joint probabilities of these 
 note the following
special cases:
𝑃ሼ𝑋ൌ𝑥ሽൌ෍
௬
𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽൌ෍
௬
𝑃ሼ𝑋ൌ𝑦, 𝑌ൌ𝑥ሽൌ𝑃ሼ𝑌ൌ𝑥ሽ
𝑖
𝑘/𝑛,
𝑛
𝑖
𝑌ଵ
𝑌ଶ
𝑌௜
ሺ𝑖െ1ሻ
𝑖
𝑖ൌ1, … , 𝑘.
𝑛ൌ4, 𝑘ൌ2
𝑋ଵൌ1, 𝑋ଶൌ0, 𝑋ଷൌ0, 𝑋ସൌ1,
𝑌ଵൌ1, 𝑌ଶൌ3 .
𝑌ଵൌ𝑖ଵ, 𝑌ଶൌ𝑖ଶ, … , 𝑌௞ൌ𝑖௞⇔𝑋௜భൌ𝑋௜భ൅௜మൌ⋯ൌ𝑋௜భ൅⋯൅௜ೖൌ1, 𝑋௝ൌ0,
𝑋௜,
𝑃ሼ𝑌ଵൌ𝑖ଵ, 𝑌ଶൌ𝑖ଶ, … , 𝑌௞ൌ𝑖௞ሽൌ𝑘!ሺ𝑛െ𝑘ሻ!
𝑛!
 𝑖ଵ൅⋯൅𝑖௞൑𝑛
𝑌ଵ, … , 𝑌௞
𝑛
𝑚
𝑋௜ൌ1
𝑖
𝑖
𝑖൒1 .
𝑋௜,
𝑃ሼ𝑋ଵ
ൌ1, 𝑋ଶൌ1, 𝑋ଷൌ0, 𝑋ସൌ1, 𝑋ହൌ0ሽ
ൌ
𝑛
𝑛൅𝑚 
𝑛൅1
𝑛൅𝑚൅1  
𝑚
𝑛൅𝑚൅2  
𝑛൅2
𝑛൅𝑚൅3  
𝑚൅1
𝑛൅𝑚൅4
ൌ
𝑛ሺ𝑛൅1ሻሺ𝑛൅2ሻ𝑚ሺ𝑚൅1ሻ
ሺ𝑛൅𝑚ሻሺ𝑛൅𝑚൅1ሻሺ𝑛൅𝑚൅2ሻሺ𝑛൅𝑚൅3ሻሺ𝑛൅𝑚൅4ሻ
449 of 848

and
By the same reasoning, for any sequence 
 that contains  ones and
 zeros, we have
Therefore, for any value of 
 the random variables 
 are exchangeable.
An interesting corollary of the exchangeability in this model is that the probability
that the th ball selected is red is the same as the probability that the first ball
selected is red, namely, 
 (For an intuitive argument for this initially
nonintuitive result, imagine that all the 
 balls initially in the urn are of
different types. That is, one is a red ball of type 
 one is a red ball of type 
one is a red ball type of 
 one is a blue ball of type 
 and so on, down to the
blue ball of type 
 Suppose that when a ball is selected it is replaced along with
another of its type. Then, by symmetry, the th ball selected is equally likely to be
of any of the 
 distinct types. Because  of these 
 types are red, the
probability is 
)
Our final example deals with continuous random variables that are exchangeable.
Example 8d
Let 
 be independent uniform (0, 1) random variables, and denote
their order statistics by 
 That is, 
 is the th smallest of
 Also, let
Show that 
 are exchangeable.
Solution
𝑃ሼ𝑋ଵ
ൌ0, 𝑋ଶൌ1, 𝑋ଷൌ0, 𝑋ସൌ1, 𝑋ହൌ1ሽ
ൌ
𝑚
𝑛൅𝑚 
𝑛
𝑛൅𝑚൅1  
𝑚൅1
𝑛൅𝑚൅2  
𝑛൅1
𝑛൅𝑚൅3  
𝑛൅2
𝑛൅𝑚൅4
ൌ
𝑛ሺ𝑛൅1ሻሺ𝑛൅2ሻ𝑚ሺ𝑚൅1ሻ
ሺ𝑛൅𝑚ሻሺ𝑛൅𝑚൅1ሻሺ𝑛൅𝑚൅2ሻሺ𝑛൅𝑚൅3ሻሺ𝑛൅𝑚൅4ሻ
𝑥ଵ, … , 𝑥௞
𝑟
𝑘െ𝑟
𝑃ሼ𝑋ଵ
ൌ𝑥ଵ, … , 𝑋௞ൌ𝑥௞ሽ
ൌ𝑛ሺ𝑛൅1ሻ⋯ሺ𝑛൅𝑟െ1ሻ𝑚ሺ𝑚൅1ሻ⋯ሺ𝑚൅𝑘െ𝑟െ1ሻ
ሺ𝑛൅𝑚ሻ⋯ሺ𝑛൅𝑚൅𝑘െ1ሻ
𝑘,
𝑋ଵ, … , 𝑋௞
𝑖
𝑛
𝑛൅𝑚.
𝑛൅𝑚
1,
2, … ,
𝑛,
1,
𝑚.
𝑖
𝑛൅𝑚
𝑛
𝑛൅𝑚
𝑛
𝑛൅𝑚.
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑋ሺଵሻ, … , 𝑋ሺ௡ሻ.
𝑋ሺ௝ሻ
𝑗
𝑋ଵ, 𝑋ଶ, … , 𝑋௡.
𝑌ଵൌ𝑋ሺଵሻ,
𝑌௜ൌ𝑋ሺ௜ሻെ𝑋ሺ௜െଵሻ,  𝑖ൌ2, … 𝑛
𝑌ଵ, … , 𝑌௡
450 of 848

The transformations
yield
As it is easy to see that the Jacobian of the preceding transformations is equal to
1, so, from Equation (7.3)
, we obtain
where  is the joint density function of the order statistics. Hence, from Equation
(6.1)
, we obtain that
or, equivalently,
Because the preceding joint density is a symmetric function of 
 we see
that the random variables 
 are exchangeable.
The joint cumulative probability distribution function of the pair of random variables 
and  is defined by
All probabilities regarding the pair can be obtained from 
 To find the individual
probability distribution functions of  and 
 use
If  and  are both discrete random variables, then their joint probability mass
function is defined by
𝑦ଵൌ𝑥ଵ,  𝑦௜ൌ𝑥௜െ𝑥௜െଵ,  𝑖ൌ2, … , 𝑛
𝑥௜ൌ𝑦ଵ൅⋯൅𝑦௜ 𝑖ൌ1, … , 𝑛
𝑓௒భ,… ,௒೙ሺ𝑦ଵ, 𝑦ଶ, … , 𝑦௡ሻൌ𝑓ሺ𝑦ଵ, 𝑦ଵ൅𝑦ଶ, … , 𝑦ଵ൅⋯൅𝑦௡ሻ
𝑓
𝑓௒భ,… ,௒೙ሺ𝑦ଵ, 𝑦ଶ, … , 𝑦௡ሻൌ𝑛! 0 ൏𝑦ଵ൏𝑦ଵ൅𝑦ଶ൏⋯൏𝑦ଵ൅⋯൅𝑦௡൏1
𝑓௒భ,… ,௒೙ሺ𝑦ଵ, 𝑦ଶ, … , 𝑦௡ሻൌ𝑛! 0 ൏𝑦௜൏1,  𝑖ൌ1, … , 𝑛,  𝑦ଵ൅⋯൅𝑦௡൏1
𝑦ଵ, … , 𝑦௡,
𝑌ଵ, … , 𝑌௡
𝑋
𝑌
𝐹ሺ𝑥, 𝑦ሻൌ𝑃ሼ𝑋൑𝑥, 𝑌൑𝑦ሽ   െ∞൏𝑥, 𝑦൏∞
𝐹.
𝑋
𝑌,
𝐹௑ሺ𝑥ሻൌ
lim
௬→ஶ𝐹ሺ𝑥, 𝑦ሻ 𝐹௒ሺ𝑦ሻൌ
lim
௫→ஶ𝐹ሺ𝑥, 𝑦ሻ
𝑋
𝑌
𝑝ሺ𝑖, 𝑗ሻൌ𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ
451 of 848

The individual mass functions are
The random variables  and  are said to be jointly continuous if there is a function 
(
), called the joint probability density function, such that for any two-dimensional
set 
It follows from the preceding formula that
If  and  are jointly continuous, then they are individually continuous with density
functions
The random variables  and  are independent if, for all sets  and 
If the joint distribution function (or the joint probability mass function in the discrete
case, or the joint density function in the continuous case) factors into a part
depending only on  and a part depending only on 
 then  and  are independent.
In general, the random variables 
 are independent if, for all sets of real
numbers 
If  and  are independent continuous random variables, then the distribution
function of their sum can be obtained from the identity
𝑃ሼ𝑋ൌ𝑖ሽൌ෍
௝
𝑝ሺ𝑖, 𝑗ሻ  𝑃ሼ𝑌ൌ𝑗ሽൌ෍
௜
𝑝ሺ𝑖, 𝑗ሻ
𝑋
𝑌
𝑓
𝑥, 𝑦
𝐶,
𝑃ሼሺ𝑋, 𝑌ሻ∈𝐶ሽൌ඲඲
஼
𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
𝑃ሼ𝑥൏𝑋൏𝑥൅𝑑𝑥,  𝑦൏𝑌൏𝑦൅𝑑𝑦ሽൎ𝑓ሺ𝑥, 𝑦ሻ 𝑑𝑥 𝑑𝑦
𝑋
𝑌
𝑓௑ሺ𝑥ሻൌ඲
െஶ
ஶ
𝑓ቆ𝑥, 𝑦ቇ𝑑𝑦  𝑓௒ሺ𝑦ሻൌ඲
െஶ
ஶ
𝑓ቆ𝑥, 𝑦ሻ 𝑑𝑥
𝑋
𝑌
𝐴
𝐵,
𝑃ሼ𝑋∈𝐴, 𝑌∈𝐵ሽൌ𝑃ሼ𝑋∈𝐴ሽ𝑃ሼ𝑌∈𝐵ሽ
𝑥
𝑦,
𝑋
𝑌
𝑋ଵ, … , 𝑋௡
𝐴ଵ, … , 𝐴௡,
𝑃ሼ𝑋ଵ∈𝐴ଵ, … , 𝑋௡∈𝐴௡ሽൌ𝑃ሼ𝑋ଵ∈𝐴ଵሽ⋯𝑃ሼ𝑋௡∈𝐴௡ሽ
𝑋
𝑌
𝐹௑൅௒ሺ𝑎ሻൌ඲
െஶ
ஶ
𝐹௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
452 of 848

If 
 are independent normal random variables with respective
parameters 
 and 
 then 
 is normal with parameters 
and 
If 
 are independent Poisson random variables with respective
parameters 
 then 
 is Poisson with parameter 
If  and  are discrete random variables, then the conditional probability mass
function of  given that 
 is defined by
where  is their joint probability mass function. Also, if  and  are jointly continuous
with joint density function 
 then the conditional probability density function of 
given that 
 is given by
The ordered values 
 of a set of independent and identically
distributed random variables are called the order statistics of that set. If the random
variables are continuous and have density function 
 then the joint density function
of the order statistics is
The random variables 
 are called exchangeable if the joint distribution of
 is the same for every permutation 
 of 
𝑋௜, 𝑖ൌ1, … , 𝑛,
𝜇௜
𝜎ଶ
௜, 𝑖ൌ1, … , 𝑛,
෍
௜ൌଵ
௡
𝑋௜
෍
௜ൌଵ
௡
𝜇௜
෍
௜ൌଵ
௡
𝜎ଶ
௜.
𝑋௜, 𝑖ൌ1, … , 𝑛,
𝜆௜, 𝑖ൌ1, … , 𝑛,
෍
௜ൌଵ
௡
𝑋௜
෍
௜ൌଵ
௡
𝜆௜.
𝑋
𝑌
𝑋
𝑌ൌ𝑦
𝑃ሼ𝑋ൌ𝑥|𝑌ൌ𝑦ሽൌ𝑝ሺ𝑥, 𝑦ሻ
𝑝௒ሺ𝑦ሻ
𝑝
𝑋
𝑌
𝑓,
𝑋
𝑌ൌ𝑦
𝑓௑|௒ሺ𝑥|𝑦ሻൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻ
𝑋ሺଵሻ൑𝑋ሺଶሻ൑⋯൑𝑋ሺ௡ሻ
𝑓,
𝑓ሺ𝑥ଵ, … , 𝑥௡ሻൌ𝑛!𝑓ሺ𝑥ଵሻ ⋯ 𝑓ሺ𝑥௡ሻ  𝑥ଵ൑𝑥ଶ൑⋯൑𝑥௡
𝑋ଵ, … , 𝑋௡
𝑋௜భ, … , 𝑋௜೙
𝑖ଵ, … , 𝑖௡
1, … , 𝑛.
6.1. Two fair dice are rolled. Find the joint probability mass function
of  and  when
a.  is the largest value obtained on any die and  is the sum of
the values;
b.  is the value on the first die and  is the larger of the two
𝑋
𝑌
𝑋
𝑌
𝑋
𝑌
453 of 848

values;
c.  is the smallest and  is the largest value obtained on the
dice.
6.2. Suppose that 3 balls are chosen without replacement from an
urn consisting of 5 white and 8 red balls. Let 
 equal 1 if the th ball
selected is white, and let it equal 0 otherwise. Give the joint
probability mass function of
a. 
;
b. 
6.3. In Problem 8
, suppose that the white balls are numbered,
and let 
 equal 1 if the th white ball is selected and 0 otherwise.
Find the joint probability mass function of
a. 
;
b. 
6.4. Repeat Problem 6.2
 when the ball selected is replaced in the
urn before the next selection.
6.5. Repeat Problem 6.3a
 when the ball selected is replaced in
the urn before the next selection.
6.6. The severity of a certain cancer is designated by one of the
grades 
 with  being the least severe and  the most severe.
If  is the score of an initially diagnosed patient and  the score of
that patient after three months of treatment, hospital data indicates
that 
 is given by
a. Find the probability mass functions of  and of ;
b. Find 
 and 
c. Find 
 and 
6.7. Consider a sequence of independent Bernoulli trials, each of
which is a success with probability 
 Let 
 be the number of
failures preceding the first success, and let 
 be the number of
failures between the first two successes. Find the joint mass function
of 
 and 
6.8. The joint probability density function of  and  is given by
𝑋
𝑌
𝑋௜
𝑖
𝑋ଵ, 𝑋ଶ
𝑋ଵ, 𝑋ଶ, 𝑋ଷ.
𝑌௜
𝑖
𝑌ଵ, 𝑌ଶ
𝑌ଵ, 𝑌ଶ, 𝑌ଷ.
1, 2, 3, 4
1
4
𝑋
𝑌
𝑝ሺ𝑖, 𝑗ሻൌ𝑃ሺ𝑋ൌ𝑖, 𝑌ൌ𝑗ሻ
𝑝ሺ1, 1ሻൌ.08,
𝑝ሺ1,  2ሻൌ.06,
𝑝ሺ1, 3ሻൌ.04,
𝑝ሺ1, 4ሻൌ.02
𝑝ሺ2, 1ሻൌ.06,
𝑝ሺ2, 2ሻൌ.12,
𝑝ሺ2, 3ሻൌ.08,
𝑝ሺ2, 4ሻൌ.04
𝑝ሺ3, 1ሻൌ.03,
𝑝ሺ3, 2ሻൌ.09,
𝑝ሺ3, 3ሻൌ.12,
𝑝ሺ3, 4ሻൌ.06
𝑝ሺ4, 1ሻൌ.01,
𝑝ሺ4, 2ሻൌ.03,
𝑝ሺ4, 3ሻൌ.07,
𝑝ሺ4, 4ሻൌ.09
𝑋
𝑌
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ.
Var ሺ𝑋ሻ
Var ሺ𝑌ሻ.
𝑝.
𝑋ଵ
𝑋ଶ
𝑋ଵ
𝑋ଶ.
𝑋
𝑌
454 of 848

a. Find 
b. Find the marginal densities of  and 
c. Find [ ].
6.9. The joint probability density function of  and  is given by
a. Verify that this is indeed a joint density function.
b. Compute the density function of 
c. Find 
d. Find 
e. Find [ ].
f. Find [ ].
6.10. The joint probability density function of  and  is given by
Find (a) 
 and (b) 
6.11. In Example 1d
, verify that 
 is indeed a joint density. function. That is, check that
 and that 
6.12. The number of people who enter a drugstore in a given hour is
a Poisson random variable with parameter 
 Compute the
conditional probability that at most 3 men entered the drugstore,
given that 10 women entered in that hour. What assumptions have
you made?
6.13. A man and a woman agree to meet at a certain location about
12:30 ඘.ඕ. If the man arrives at a time uniformly distributed between
12:15 and 12:45, and if the woman independently arrives at a time
uniformly distributed between 12:00 and 1 ඘.ඕ., find the probability
that the first to arrive waits no longer than 5 minutes. What is the
probability that the man arrives first?
6.14. An ambulance travels back and forth at a constant speed along
a road of length 
 At a certain moment of time, an accident occurs
at a point uniformly distributed on the road. [That is, the distance of
the point from one of the fixed ends of the road is uniformly
distributed over (0, ).] Assuming that the ambulance’s location at the
moment of the accident is also uniformly distributed, and assuming
𝑓ሺ𝑥, 𝑦ሻൌ𝑐ሺ𝑦ଶെ𝑥ଶሻ𝑒െ௬  െ𝑦൑𝑥൑𝑦,  0 ൏𝑦൏∞
𝑐.
𝑋
𝑌.
𝐸𝑋
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ6
7 ൬𝑥ଶ൅𝑥𝑦
2 ൰ 0 ൏𝑥൏1,  0 ൏𝑦൏2
𝑋.
𝑃ሼ𝑋൐𝑌ሽ.
𝑃ቊ𝑌൐1
2
|
||
|
𝑋൏1
2ቋ.
𝐸𝑋
𝐸𝑌
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑒െሺ௫൅௬ሻ  0 ൑𝑥൏∞, 0 ൑𝑦൏∞
𝑃ሼ𝑋൏𝑌ሽ
𝑃ሼ𝑋൏𝑎ሽ.
 𝑓ሺ𝑥, 𝑦ሻൌ2𝑒െ௫𝑒െଶ௬,  0 ൏𝑥൏∞,
0 ൏𝑦൏∞,
𝑓ሺ𝑥, 𝑦ሻ൒0,
׬െஶ
ஶ
׬െஶ
ஶ
𝑓ሺ𝑥, 𝑦ሻ𝑑𝑥 𝑑𝑦ൌ1 .
𝜆ൌ10 .
𝐿.
𝐿
455 of 848

independence of the variables, compute the distribution of the
distance of the ambulance from the accident.
6.15. The random vector (
) is said to be uniformly distributed
over a region  in the plane if, for some constant 
 its joint density is
a. Show that 
Suppose that (
) is uniformly distributed over the square
centered at (0, 0) and with sides of length 2.
b. Show that  and  are independent, with each being
distributed uniformly over 
c. What is the probability that (
) lies in the circle of radius 1
centered at the origin? That is, find 
6.16. Suppose that  points are independently chosen at random on
the circumference of a circle, and we want the probability that they all
lie in some semicircle. That is, we want the probability that there is a
line passing through the center of the circle such that all the points
are on one side of that line, as shown in the following diagram:
Let 
 denote the  points. Let  denote the event that all the
points are contained in some semicircle, and let 
 be the event that
all the points lie in the semicircle beginning at the point 
 and going
clockwise for 
a. Express  in terms of the 
b. Are the 
 mutually exclusive?
c. Find 
𝑋, 𝑌
𝑅
𝑐,
𝑓ሺ𝑥, 𝑦ሻൌ൝𝑐
 ifሺ𝑥,  𝑦ሻ∈𝑅
0
 otherwise
1/𝑐ൌ area of region  𝑅.
𝑋, 𝑌
𝑋
𝑌
ሺെ1, 1ሻ.
𝑋, 𝑌
𝑃൛𝑋ଶ൅𝑌ଶ൑1ൟ.
𝑛
𝑃ଵ, … , 𝑃௡
𝑛
𝐴
𝐴௜
𝑃௜
180∘, 𝑖ൌ1, … , 𝑛.
𝐴
𝐴௜.
𝐴௜
𝑃ሺ𝐴ሻ.
456 of 848

6.17. Three points 
 are selected at random on a line 
What is the probability that 
 lies between 
 and 
6.18. Let 
 and 
 be independent binomial random variables with
 having parameters 
 Find
a. 
b. 
c. 
6.19. Show that 
, 
 is a joint density
function. Assuming that  is the joint density function of 
 find
a. the marginal density of ;
b. the marginal density of ;
c. 
d. 
6.20. The joint density of  and  is given by
Are  and  independent? If, instead, ( , ) were given by
would  and  be independent?
6.21. Let
and let it equal 0 otherwise.
a. Show that (
) is a joint probability density function.
b. Find [ ].
c. Find [ ].
6.22. The joint density function of  and  is
a. Are  and  independent?
b. Find the density function of 
c. Find 
6.23. The random variables  and  have joint density function
𝑋ଵ, 𝑋ଶ, 𝑋ଷ
𝐿.
𝑋ଶ
𝑋ଵ
𝑋ଷ?
𝑋ଵ
𝑋ଶ
𝑋௜
ሺ𝑛௜, 𝑝௜ሻ, 𝑖ൌ1, 2 .
𝑃ሺ𝑋ଵ𝑋ଶൌ0ሻ;
𝑃ሺ𝑋ଵ൅𝑋ଶൌ1ሻ;
𝑃ሺ𝑋ଵ൅𝑋ଶൌ2 ሻ.
𝑓ሺ𝑥, 𝑦ሻൌ1/𝑥0 ൏𝑦൏𝑥൏1,
𝑓
𝑋, 𝑌
𝑌
𝑋
𝐸ሾ𝑋ሿ;
𝐸ሾ𝑌ሿ.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌቊ𝑥𝑒െሺ௫൅௬ሻ 𝑥൐0,  𝑦൐0
0
 otherwise
𝑋
𝑌
𝑓𝑥𝑦
𝑓ሺ𝑥, 𝑦ሻൌቊ2
 0 ൏𝑥൏𝑦,  0 ൏𝑦൏1
0
 otherwise
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ24𝑥𝑦 0 ൑𝑥൑1,  0 ൑𝑦൑1,  0 ൑𝑥൅𝑦൑1
𝑓𝑥𝑦
𝐸𝑋
𝐸𝑌
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ൝
𝑥൅𝑦
 0 ൏𝑥൏1,  0 ൏𝑦൏1
0
 otherwise
𝑋
𝑌
𝑋.
𝑃ሼ𝑋൅𝑌൏1 ሽ.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ12𝑥𝑦ሺ1 െ𝑥ሻ 0 ൏𝑥൏1,  0 ൏𝑦൏1
457 of 848

and equal to 0 otherwise.
a. Are  and  independent?
b. Find [ ].
c. Find [ ].
d. Find Var
e. Find Var
6.24. Consider independent trials, each of which results in outcome
 with probability 
 Let  denote the
number of trials needed to obtain an outcome that is not equal to 0,
and let  be that outcome.
a. Find 
b. Find 
c. Show that 
d. Is it intuitive to you that  is independent of 
e. Is it intuitive to you that  is independent of 
6.25. Suppose that 
 people arrive at a service station at times that
are independent random variables, each of which is uniformly
distributed over 
 Let  denote the number that arrive in the
first hour. Find an approximation for 
6.26. Suppose that 
 are independent random variables, each
being uniformly distributed over (0, 1).
a. What is the joint cumulative distribution function of 
b. What is the probability that all of the roots of the equation
 are real?
6.27. If 
 and 
 are independent exponential random variables
with respective parameters 
 and 
 find the distribution of
 Also compute 
6.28. The time that it takes to service a car is an exponential random
variable with rate 
a. If A. J. brings his car in at time  and M. J. brings her car in at
time  what is the probability that M. J.’s car is ready before
A. J.’s car? (Assume that service times are independent and
service begins upon arrival of the car.)
b. If both cars are brought in at time 
 with work starting on M.
J.’s car only when A. J.’s car has been completely serviced,
what is the probability that M. J.’s car is ready before time 
𝑋
𝑌
𝐸𝑋
𝐸𝑌
ሺ𝑋ሻ.
ሺ𝑌ሻ.
𝑖, 𝑖ൌ0, 1, … , 𝑘
𝑝௜, ෍
௜ൌ଴
௞
𝑝௜ൌ1 .
𝑁
𝑋
𝑃ሼ𝑁ൌ𝑛ሽ, 𝑛൒1 .
𝑃ሼ𝑋ൌ𝑗ሽ, 𝑗ൌ1, … , 𝑘.
𝑃ሼ𝑁ൌ𝑛, 𝑋ൌ𝑗ሽൌ𝑃ሼ𝑁ൌ𝑛ሽ𝑃ሼ𝑋ൌ𝑗ሽ.
𝑁
𝑋?
𝑋
𝑁?
10଺
ሺ0, 10଺ሻ.
𝑁
𝑃ሼ𝑁ൌ𝑖ሽ.
𝐴, 𝐵, 𝐶,
𝐴, 𝐵, 𝐶?
𝐴𝑥ଶ൅𝐵𝑥൅𝐶ൌ0
𝑋ଵ
𝑋ଶ
𝜆ଵ
𝜆ଶ,
𝑍ൌ𝑋ଵ/𝑋ଶ.
𝑃ሼ𝑋ଵ൏𝑋ଶሽ.
1 .
0
𝑡,
0,
2?
458 of 848

6.29. The gross daily sales at a certain restaurant are a normal
random variable with mean $2200 and standard deviation $230.
What is the probability that
a. the total gross sales over the next 2 days exceeds $5000;
b. daily sales exceed $2000 in at least 2 of the next 3 days?
What independence assumptions have you made?
6.30. Jill’s bowling scores are approximately normally distributed with
mean 170 and standard deviation 20, while Jack’s scores are
approximately normally distributed with mean 160 and standard
deviation 15. If Jack and Jill each bowl one game, then assuming
that their scores are independent random variables, approximate the
probability that
a. Jack’s score is higher;
b. the total of their scores is above 350.
6.31. According to the U.S. National Center for Health Statistics, 25.2
percent of males and 23.6 percent of females never eat breakfast.
Suppose that random samples of 200 men and 200 women are
chosen. Approximate the probability that
a. at least 110 of these 400 people never eat breakfast;
b. the number of the women who never eat breakfast is at least
as large as the number of the men who never eat breakfast.
6.32. Monthly sales are independent normal random variables with
mean 
 and standard deviation 
a. Find the probability that exactly  of the next  months have
sales greater than 100.
b. Find the probability that the total of the sales in the next 
months is greater than 
6.33. Let 
 and 
 be independent normal random variables, each
having mean 
 and variance 
 Which probability is larger
a. 
 or 
;
b. 
 or 
6.34. Suppose  and  are independent normal random variables
with mean 
 and variance 
 Find  such that
6.35. Teams 
 are all scheduled to play each of the other
teams 
 times. Whenever team  plays team 
 team  is the winner
100
5 .
3
6
4
420 .
𝑋ଵ
𝑋ଶ
10
𝜎ଶ.
𝑃ሺ𝑋ଵ൐15ሻ 
 𝑃ሺ𝑋ଵ൅𝑋ଶ൐25ሻ
𝑃ሺ𝑋ଵ൐15ሻ 
 𝑃ሺ𝑋ଵ൅𝑋ଶ൐30ሻ.
𝑋
𝑌
10
4 .
𝑥
𝑃ሺ𝑋൅𝑌൐𝑥ሻൌ𝑃ሺ𝑋൐15ሻ.
1, 2, 3, 4
10
𝑖
𝑗,
𝑖
459 of 848

with probability 
 where
a. Approximate the probability that team 1 wins at least 
games.
Suppose we want to approximate the probability that team 
wins at least as many games as does team 
 To do so, let 
be the number of games that team  wins against team 
 let
 be the total number of games that team  wins against
teams  and 
 and let  be the total number of games that
team  wins against teams  and 
b. Are 
 independent.
c. Express the event that team 2 wins at least as many games
as does team  in terms of the random variables 
d. Approximate the probability that team  wins at least as many
games as team 
Hint: Approximate the distribution of any binomial random variable
by a normal with the same mean and variance.
6.36. Let 
 be independent with the same continuous
distribution function 
 and let 
 be the median of that distribution.
That is, 
a. If  is the number of the values 
 that are less than
 what type of random variable is 
b. Let 
 be the values 
 arranged
in increasing order. That is, 
 is, for 
 the 
smallest of 
 Find 
6.37. The expected number of typographical errors on a page of a
certain magazine is .2. What is the probability that an article of 10
pages contains (a) 0 and (b) 2 or more typographical errors? Explain
your reasoning!
6.38. The monthly worldwide average number of airplane crashes of
commercial airlines is 2.2. What is the probability that there will be
a. more than 2 such accidents in the next month?
b. more than 4 such accidents in the next 2 months?
c. more than 5 such accidents in the next 3 months?
Explain your reasoning!
6.39. In Problem 6.4
, calculate the conditional probability mass
𝑃௜,௝,
𝑃ଵ,ଶൌ.6,  𝑃ଵ,ଷൌ.7,  𝑃ଵ,ସൌ.75
𝑃ଶ,ଵൌ.4,  𝑃ଶ,ଷൌ.6,  𝑃ଶ,ସൌ.70
20
2
1 .
𝑋
2
1,
𝑌
2
3
4,
𝑍
1
3
4 .
𝑋, 𝑌, 𝑍
1
𝑋, 𝑌, 𝑍.
2
1 .
𝑋ଵ, … , 𝑋ଵ଴
𝐹,
𝑚
𝐹ሺ𝑚ሻൌ.5.
𝑁
𝑋ଵ, … , 𝑋ଵ଴
𝑚,
𝑁.
𝑋ሺଵሻ൏𝑋ሺଶሻ൏⋯൏𝑋ሺଵ଴ሻ
𝑋ଵ, … , 𝑋ଵ଴
𝑋ሺ௜ሻ
𝑖ൌ1,  …  ,  10,
𝑖௧௛
𝑋ଵ, … , 𝑋ଵ଴.
𝑃ሺ𝑋ሺଶሻ൏𝑚൏𝑋ሺ଼ሻሻ.
460 of 848

function of 
 given that
a. 
;
b. 
6.40. In Problem 6.3
, calculate the conditional probability mass
function of 
 given that
a. 
;
b. 
6.41. The discrete integer valued random variables 
 are
independent if for all 
Show that if 
 are independent then  and  are independent.
That is, show that the preceding implies that
6.42. Choose a number  at random from the set of numbers
 Now choose a number at random from the subset no
larger than 
 that is, from 
 Call this second number 
a. Find the joint mass function of  and 
b. Find the conditional mass function of X given that 
 Do it
for 
c. Are  and  independent? Why?
6.43. Two dice are rolled. Let  and  denote, respectively, the
largest and smallest values obtained. Compute the conditional mass
function of  given 
 for 
 Are  and  independent?
Why?
6.44. The joint probability mass function of X and  is given by
a. Compute the conditional mass function of  given
b. Are  and  independent?
c. Compute 
6.45. The joint density function of  and  is given by
𝑋ଵ
𝑋ଶൌ1
𝑋ଶൌ0 .
𝑌ଵ
𝑌ଶൌ1
𝑌ଶൌ0 .
𝑋, 𝑌, 𝑍
𝑖, 𝑗, 𝑘
𝑃ሺ𝑋ൌ𝑖, 𝑌ൌ𝑗, 𝑍ൌ𝑘ሻൌ𝑃ሺ𝑋ൌ𝑖ሻ𝑃ሺ𝑌ൌ𝑗ሻ𝑃ሺ𝑍ൌ𝑘ሻ
𝑋, 𝑌, 𝑍
𝑋
𝑌
𝑃ሺ𝑋ൌ𝑖, 𝑌ൌ𝑗ሻൌ𝑃ሺ𝑋ൌ𝑖ሻ𝑃ሺ𝑌ൌ𝑗ሻ
𝑋
ሼ1,2,3,4,5ሽ.
𝑋,
ሼ1, … , 𝑋ሽ.
𝑌.
𝑋
𝑌.
𝑌ൌ𝑖.
𝑖ൌ1, 2, 3, 4, 5 .
𝑋
𝑌
𝑋
𝑌
𝑌
𝑋ൌ𝑖,
𝑖ൌ1, 2, … , 6 .
𝑋
𝑌
𝑌
𝑝ሺ1, 1ሻൌ1
8   𝑝ሺ1, 2ሻൌ1
4
𝑝ሺ2, 1ሻൌ1
8   𝑝ሺ2, 2ሻൌ1
2
𝑋
𝑌ൌ𝑖, 𝑖ൌ1, 2 .
𝑋
𝑌
𝑃ሼ𝑋𝑌൑3ሽ, 𝑃ሼ𝑋൅𝑌൐2ሽ, 𝑃ሼ𝑋/𝑌൐1ሽ.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑥𝑒െ௫൫௬൅ଵሻ 𝑥൐0,  𝑦൐0
461 of 848

a. Find the conditional density of 
 given 
 and that of 
given 
b. Find the density function of 
6.46. The joint density of  and  is
Find the conditional distribution of 
 given 
6.47. An insurance company supposes that each person has an
accident parameter and that the yearly number of accidents of
someone whose accident parameter is  is Poisson distributed with
mean 
 They also suppose that the parameter value of a newly
insured person can be assumed to be the value of a gamma random
variable with parameters  and 
 If a newly insured person has 
accidents in her first year, find the conditional density of her accident
parameter. Also, determine the expected number of accidents that
she will have in the following year.
6.48. If 
 are independent random variables that are
uniformly distributed over (0, 1), compute the probability that the
largest of the three is greater than the sum of the other two.
6.49. A complex machine is able to operate effectively as long as at
least 3 of its 5 motors are functioning. If each motor independently
functions for a random amount of time with density function
 compute the density function of the length of time
that the machine functions.
6.50. If 3 trucks break down at points randomly distributed on a road
of length 
 find the probability that no 2 of the trucks are within a
distance  of each other when 
6.51. Consider a sample of size 5 from a uniform distribution over (0,
1). Compute the probability that the median is in the interval 
6.52. If 
 are independent and identically distributed
exponential random variables with the parameter 
 compute
a. 
b. 
6.53. Let 
 be the order statistics of a set of 
independent uniform 
 random variables. Find the conditional
distribution of 
 given that 
6.54. Let 
 and 
 be independent standard normal random
variables. Show that 
 has a bivariate normal distribution when
𝑋,
𝑌ൌ𝑦,
𝑌,
𝑋ൌ𝑥.
𝑍ൌ𝑋𝑌.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑐ሺ𝑥ଶെ𝑦ଶሻ𝑒െ௫ 0 ൑𝑥൏∞,   െ𝑥൑𝑦൑𝑥
𝑌,
𝑋ൌ𝑥.
𝜆
𝜆.
𝑠
𝛼.
𝑛
𝑋ଵ, 𝑋ଶ, 𝑋ଷ
𝑓ሺ𝑥ሻൌ𝑥𝑒െ௫, 𝑥൐0,
𝐿,
𝑑
𝑑൑𝐿/2 .
ቆ1
4, 3
4ቇ.
𝑋ଵ, 𝑋ଶ, 𝑋ଷ, 𝑋ସ, 𝑋ହ
𝜆,
𝑃ሼminሺ𝑋ଵ, … , 𝑋ହሻ൑𝑎ሽ;
𝑃ሼmaxሺ𝑋ଵ, … , 𝑋ହሻ൑𝑎ሽ.
 𝑋ሺଵ൯, 𝑋ሺଶሻ,…, 𝑋ሺ௡ሻ
𝑛
ሺ0, 1ሻ
𝑋ሺ௡ሻ
 𝑋ሺଵሻൌ𝑠ଵ, 𝑋ሺଶሻൌ𝑠ଶ, … , 𝑋ሺ௡െଵሻൌ𝑠௡െଵ.
𝑍ଵ
𝑍ଶ
𝑋, 𝑌
𝑋ൌ𝑍ଵ,  𝑌ൌ𝑍ଵ൅𝑍ଶ.
462 of 848

6.55. Derive the distribution of the range of a sample of size 2 from a
distribution having density function 
6.56. Let  and  denote the coordinates of a point uniformly chosen
in the circle of radius 1 centered at the origin. That is, their joint
density is
Find the joint density function of the polar coordinates
 and 
6.57. If  and  are independent random variables both uniformly
distributed over (0, 1), find the joint density function of
6.58. If  is uniform on 
 and 
 independent of 
 is
exponential with rate 1, show directly (without using the results of
Example 7b
) that  and  defined by
are independent standard normal random variables.
6.59.  and  have joint density function
a. Compute the joint density function of 
b. What are the marginal densities?
6.60. If  and  are independent and identically distributed uniform
random variables on (0, 1), compute the joint density of
a. 
;
b. 
;
c. 
6.61. Repeat Problem 6.60
 when  and  are independent
exponential random variables, each with parameter 
6.62. If 
 and 
 are independent exponential random variables,
each having parameter 
 find the joint density function of
 and 
6.63. If 
 and  are independent random variables having
identical density functions 
 derive the joint
distribution of 
𝑓ሺ𝑥ሻൌ2𝑥,  0 ൏𝑥൏1 .
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ1
𝜋 𝑥ଶ൅𝑦ଶ൑1
𝑅ൌሺ𝑋ଶ൅𝑌ଶሻଵ/ଶ
Θ ൌtanെଵ𝑌/𝑋.
𝑋
𝑌
𝑅ൌ
𝑋ଶ൅𝑌ଶ
ඥ
, 𝛩ൌtanെଵ𝑌/𝑋.
𝑈
ሺ0,  2𝜋ሻ
𝑍,
𝑈,
𝑋
𝑌
𝑋ൌ
2𝑍
√
 cos  𝑈
𝑌ൌ
2𝑍
√
 sin 𝑈
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ
1
𝑥ଶ𝑦ଶ 𝑥൒1,  𝑦൒1
𝑈ൌ𝑋𝑌, 𝑉ൌ𝑋/𝑌.
𝑋
𝑌
𝑈ൌ𝑋൅𝑌, 𝑉ൌ𝑋/𝑌
𝑈ൌ𝑋, 𝑉ൌ𝑋/𝑌
𝑈ൌ𝑋൅𝑌, 𝑉ൌ𝑋/ሺ𝑋൅𝑌ሻ.
𝑋
𝑌
𝜆ൌ1 .
𝑋ଵ
𝑋ଶ
𝜆,
𝑌ଵൌ𝑋ଵ൅𝑋ଶ
𝑌ଶൌ𝑒௑భ.
𝑋, 𝑌,
𝑍
𝑓ሺ𝑥ሻൌ𝑒െ௫, 0 ൏𝑥൏∞,
𝑈ൌ𝑋൅𝑌,  𝑉ൌ𝑋൅𝑍, 𝑊ൌ𝑌൅𝑍.
463 of 848

6.64. In Example 8b, let 
 Show that
 are exchangeable. Note that 
 is the number of
balls one must observe to obtain a special ball if one considers the
balls in their reverse order of withdrawal.
6.65. Consider an urn containing  balls numbered 
 and
suppose that  of them are randomly withdrawn. Let 
 equal 1 if ball
number  is removed and let 
 be 0 otherwise. Show that 
are exchangeable.
𝑌௞൅ଵൌ𝑛൅1 െ෍
௜ൌଵ
௞
𝑌௜.
𝑌ଵ, … , 𝑌௞, 𝑌௞൅ଵ
𝑌௞൅ଵ
𝑛
1, … , 𝑛,
𝑘
𝑋௜
𝑖
𝑋௜
𝑋ଵ, … , 𝑋௡
6.1. Suppose 
 have a joint distribution function 
 Show
how to obtain the distribution functions 
 and
6.2. Suppose that  and  are integer valued random variables and
have a joint distribution function 
a. Give an expression, in terms of the joint distribution function,
for 
b. Give an expression, in terms of the joint distribution function,
for 
6.3. Suggest a procedure for using Buffon’s needle problem to
estimate 
 Surprisingly enough, this was once a common method of
evaluating 
6.4. Solve Buffon’s needle problem when 
ANSWER: 
 where 
6.5. If  and  are independent continuous positive random
variables, express the density function of (a) 
 and (b) 
in terms of the density functions of  and 
 Evaluate the density
functions in the special case where  and  are both exponential
random variables.
6.6. If  and  are jointly continuous with joint density function
 show that 
 is continuous with density function
6.7.
𝑋, 𝑌
𝐹ሺ𝑥, 𝑦ሻ.
𝐹௑ሺ𝑥ሻൌ𝑃ሼ𝑋൑𝑥ሽ
𝐹௒ሺ𝑦ሻൌ𝑃ሼ𝑌൑𝑦ሽ.
𝑋
𝑌
𝐹ሺ𝑖, 𝑗ሻൌ𝑃ሺ𝑋൑𝑖, 𝑌൑𝑗ሻ.
𝑃ሺ𝑋ൌ𝑖, 𝑌൑𝑗ሻ.
𝑃ሺ𝑋ൌ𝑖, 𝑌ൌ𝑗ሻ.
𝜋.
𝜋.
𝐿൐𝐷.
2𝐿
𝜋𝐷ሺ1 െsin 𝜃ሻ൅2𝜃/𝜋,
cos 𝜃ൌ𝐷/𝐿.
𝑋
𝑌
𝑍ൌ𝑋/𝑌
𝑍ൌ𝑋𝑌
𝑋
𝑌.
𝑋
𝑌
𝑋
𝑌
𝑓௑,௒ሺ𝑥, 𝑦ሻ,
𝑋൅𝑌
𝑓௑൅௒ሺ𝑡ሻൌ඲
െஶ
ஶ
𝑓௑,௒ሺ𝑥, 𝑡െ𝑥ሻ 𝑑𝑥
464 of 848

a. If  has a gamma distribution with parameters 
 what is
the distribution of 
b. Show that
has a gamma distribution with parameters 
 when  is a
positive integer and 
 is a chi-squared random variable
with 2  degrees of freedom.
6.8. Let  and  be independent continuous random variables with
respective hazard rate functions 
 and 
 and set
a. Determine the distribution function of 
 in terms of those of 
and 
b. Show that 
 the hazard rate function of 
 is given by
6.9. Let 
 be independent exponential random variables
having a common parameter 
 Determine the distribution of
6.10. The lifetimes of batteries are independent exponential random
variables, each having parameter 
 A flashlight needs 2 batteries to
work. If one has a flashlight and a stockpile of  batteries, what is the
distribution of time that the flashlight can operate?
6.11. Let 
 be independent continuous random
variables having a common distribution function  and density
function 
 and set
a. Show that  does not depend on 
Hint: Write  as a five-dimensional integral and make the
change of variables 
b. Evaluate 
c. Give an intuitive explanation for your answer to (b).
6.12. Show that the jointly continuous (discrete) random variables
 are independent if and only if their joint probability density
(mass) function 
 can be written as
𝑋
ሺ𝑡, 𝜆ሻ,
𝑐𝑋, 𝑐൐0?
1
2𝜆𝜒ଶ௡
ଶ
𝑛, 𝜆
𝑛
𝜒ଶ
ଶ௡
𝑛
𝑋
𝑌
𝜆௑ሺ𝑡ሻ
𝜆௒ሺ𝑡ሻ,
𝑊ൌminሺ𝑋, 𝑌ሻ.
𝑊
𝑋
𝑌.
𝜆ௐሺ𝑡ሻ,
𝑊,
𝜆ௐሺ𝑡ሻൌ𝜆௑ሺ𝑡ሻ൅𝜆௒ሺ𝑡ሻ
𝑋ଵ, … , 𝑋௡
𝜆.
minሺ𝑋ଵ, … , 𝑋௡ሻ.
𝜆.
𝑛
𝑋ଵ,  𝑋ଶ,  𝑋ଷ,  𝑋ସ,  𝑋ହ
𝐹
𝑓,
𝐼ൌ𝑃ሼ𝑋ଵ൏𝑋ଶ൏𝑋ଷ൏𝑋ସ൏𝑋ହሽ
𝐼
𝐹.
𝐼
𝑢௜ൌ𝐹ሺ𝑥௜ሻ, 𝑖ൌ1, … , 5 .
𝐼.
𝑋ଵ, … , 𝑋௡
𝑓ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑓ሺ𝑥ଵ, … , 𝑥௡ሻൌෑ
௜ൌଵ
௡
𝑔௜ሺ𝑥௜ሻ
465 of 848

for nonnegative functions 
6.13. In Example 5e
, we computed the conditional density of a
success probability for a sequence of trials when the first 
 trials
resulted in  successes. Would the conditional density change if we
specified which  of these trials resulted in successes?
6.14. Suppose that  and  are independent geometric random
variables with the same parameter 
a. Without any computations, what do you think is the value of
Hint: Imagine that you continually flip a coin having probability
 of coming up heads. If the second head occurs on the th
flip, what is the probability mass function of the time of the first
head?
b. Verify your conjecture in part (a).
6.15. Consider a sequence of independent trials, with each trial being
a success with probability 
 Given that the th success occurs on
trial 
 show that all possible outcomes of the first 
 trials that
consist of 
 successes and 
 failures are equally likely.
6.16. If  and  are independent binomial random variables with
identical parameters  and 
 show analytically that the conditional
distribution of  given that 
 is the hypergeometric
distribution. Also, give a second argument that yields the same result
without any computations.
Hint: Suppose that 2  coins are flipped. Let  denote the number of
heads in the first  flips and  the number in the second  flips.
Argue that given a total of 
 heads, the number of heads in the first
 flips has the same distribution as the number of white balls
selected when a sample of size 
 is chosen from  white and 
black balls.
6.17. Suppose that 
 are independent Poisson random
variables with respective means 
 Let 
 and
 The random vector 
 is said to have a bivariate
Poisson distribution. Find its joint probability mass function. That is,
find 
6.18. Suppose  and  are both integer-valued random variables.
Let
and
𝑔௜ሺ𝑥൯, 𝑖ൌ1, … , 𝑛.
𝑛൅𝑚
𝑛
𝑛
𝑋
𝑌
𝑝.
𝑃ሼ𝑋ൌ𝑖||𝑋൅𝑌ൌ𝑛ሽ?
𝑝
𝑛
𝑝.
𝑘
𝑛,
𝑛െ1
𝑘െ1
𝑛െ𝑘
𝑋
𝑌
𝑛
𝑝,
𝑋
𝑋൅𝑌ൌ𝑚
𝑛
𝑋
𝑛
𝑌
𝑛
𝑚
𝑛
𝑚
𝑛
𝑛
𝑋௜, 𝑖ൌ1, 2, 3
𝜆௜, 𝑖ൌ1, 2, 3 .
𝑋ൌ𝑋ଵ൅𝑋ଶ
𝑌ൌ𝑋ଶ൅𝑋ଷ.
𝑋, 𝑌
𝑃ሼ𝑋ൌ𝑛,  𝑌ൌ𝑚ሽ.
𝑋
𝑌
𝑝ሺ𝑖|| 𝑗ሻൌ𝑃ሺ𝑋ൌ𝑖||𝑌ൌ𝑗ሻ
466 of 848

Show that
6.19. Let 
 be independent and identically distributed
continuous random variables. Compute
a. 
b. 
c. 
d. 
6.20. Let  denote a random variable uniformly distributed over (0,
1). Compute the conditional distribution of  given that
a. 
b. 
where 
6.21. Suppose that 
 the amount of moisture in the air on a given
day, is a gamma random variable with parameters 
 That is, its
density is 
 Suppose also that
given that 
 the number of accidents during that day—call it 
—has a Poisson distribution with mean 
 Show that the conditional
distribution of 
 given that 
 is the gamma distribution with
parameters 
6.22. Let 
 be a gamma random variable with parameters 
 and
suppose that conditional on 
 are independent
exponential random variables with rate 
 Show that the conditional
distribution of 
 given that 
 is gamma
with parameters 
6.23. A rectangular array of mn numbers arranged in  rows, each
consisting of 
 columns, is said to contain a saddlepoint if there is a
number that is both the minimum of its row and the maximum of its
column. For instance, in the array
the number 1 in the first row, first column is a saddlepoint. The
𝑞ሺ𝑗||𝑖ሻൌ𝑃ሺ𝑌ൌ𝑗||𝑋ൌ𝑖ሻ
𝑃ሺ𝑋ൌ𝑖, 𝑌ൌ𝑗ሻൌ
𝑝ሺ𝑖|| 𝑗ሻ
∑௜
𝑝ሺ𝑖|| 𝑗ሻ
𝑞ሺ𝑗||𝑖ሻ
𝑋ଵ, 𝑋ଶ, 𝑋ଷ
𝑃ሼ𝑋ଵ൐𝑋ଶ||𝑋ଵ൐𝑋ଷሽ;
𝑃ሼ𝑋ଵ൐𝑋ଶ||𝑋ଵ൏𝑋ଷሽ;
𝑃ሼ𝑋ଵ൐𝑋ଶ||𝑋ଶ൐𝑋ଷሽ;
𝑃ሼ𝑋ଵ൐𝑋ଶ||𝑋ଶ൏𝑋ଷሽ.
𝑈
𝑈
𝑈൐𝑎;
𝑈൏𝑎;
0 ൏𝑎൏1 .
𝑊,
ሺ𝑡, 𝛽ሻ.
𝑓ሺ𝑤ሻൌ𝛽𝑒െఉ௪ሺ𝛽𝑤ሻ௧െଵ/Γሺ𝑡ሻ, 𝑤൐0 .
𝑊ൌ𝑤,
𝑁
𝑤.
𝑊
𝑁ൌ𝑛
ሺ𝑡൅𝑛, 𝛽൅1ሻ.
𝑊
ሺ𝑡, 𝛽ሻ,
𝑊ൌ𝑤, 𝑋ଵ, 𝑋ଶ, … , 𝑋௡
𝑤.
𝑊
𝑋ଵൌ𝑥ଵ, 𝑋ଶൌ𝑥ଶ, … , 𝑋௡ൌ𝑥௡
ቌ𝑡൅𝑛, 𝛽൅෍
௜ൌଵ
௡
𝑥௜ቍ.
𝑛
𝑚
1
3
2
0
െ2
6
.5
12
3
467 of 848

existence of a saddlepoint is of significance in the theory of games.
Consider a rectangular array of numbers as described previously and
suppose that there are two individuals—  and —who are playing
the following game:  is to choose one of the numbers 
 and
 one of the numbers 
 These choices are announced
simultaneously, and if  chose  and  chose 
 then  wins from 
the amount specified by the number in the th row, th column of the
array. Now suppose that the array contains a saddlepoint—say the
number in the row  and column  call this number 
 Now if player
 chooses row 
 then that player can guarantee herself a win of at
least 
 (since 
 is the minimum number in the row ). On the
other hand, if player  chooses column 
 then he can guarantee
that he will lose no more than 
 (since 
 is the maximum number
in the column ). Hence, as  has a way of playing that guarantees
her a win of 
 and as  has a way of playing that guarantees he
will lose no more than 
 it seems reasonable to take these two
strategies as being optimal and declare that the value of the game to
player  is 
If the nm numbers in the rectangular array described are
independently chosen from an arbitrary continuous distribution, what
is the probability that the resulting array will contain a saddlepoint?
6.24. If  is exponential with rate 
 find 
where 
 is defined as the largest integer less than or equal to 
Can you conclude that 
 and 
 are independent?
6.25. Suppose that 
 is a cumulative distribution function. Show
that (a) 
 and (b) 
 are also cumulative distribution
functions when  is a positive integer.
Hint: Let 
 be independent random variables having the
common distribution function 
 Define random variables  and  in
terms of the 
 so that 
 and
6.26. Show that if  people are distributed at random along a road 
miles long, then the probability that no 2 people are less than a
distance  miles apart is when 
What if 
6.27. Suppose that 
 are independent exponential random
variables with rate λ. Find
a. 
 the conditional density of 
 given that
b. 
𝐴
𝐵
𝐴
1, 2, … , 𝑛
𝐵
1, 2, … , 𝑚.
𝐴
𝑖
𝐵
𝑗,
𝐴
𝐵
𝑖
𝑗
𝑟
𝑘
𝑥௥௞.
𝐴
𝑟,
𝑥௥௞
𝑥௥௞
𝑟
𝐵
𝑘,
𝑥௥௞
𝑥௥௞
𝑘
𝐴
𝑥௥௞
𝐵
𝑥௥௞,
𝐴
𝑥௥௞.
𝑋
𝜆,
𝑃ሼሾ𝑋ሿൌ𝑛, 𝑋െሾ𝑋ሿ൑𝑥ሽ,
ሾ𝑥ሿ
𝑥.
ሾ𝑋ሿ
𝑋െሾ𝑋ሿ
𝐹ሺ𝑥ሻ
𝐹௡ሺ𝑥ሻ
1 െሾ1 െ𝐹ሺ𝑥ሻሿ௡
𝑛
𝑋ଵ, … , 𝑋௡
𝐹.
𝑌
𝑍
𝑋௜
𝑃ሼ𝑌൑𝑥ሽൌ𝐹௡ሺ𝑥ሻ
𝑃ሼ𝑍൑𝑥ሽൌ1 െሾ1 െ𝐹ሺ𝑥ሻሿ௡.
𝑛
𝐿
𝐷
𝐷൑𝐿/ሺ𝑛െ1ሻ, ሾ1 െሺ𝑛െ1ሻ𝐷/𝐿ሿ௡.
𝐷൐𝐿/ሺ𝑛െ1ሻ?
𝑋ଵ, … , 𝑋௡
𝑓௑భห௑భ൅… ൅௑೙ሺ𝑥ቚ𝑡ሻ,
𝑋ଵ
𝑋ଵ൅…  ൅𝑋௡ൌ𝑡;
𝑃ሺ𝑋ଵ൏𝑥||𝑋ଵ൅…  ൅𝑋௡ൌ𝑡ሻ.
468 of 848

6.28. Establish Equation (6.2)
 by differentiating Equation
(6.4)
.
6.29. Show that the median of a sample of size 
 from a
uniform distribution on (0, 1) has a beta distribution with parameters
6.30. Suppose that 
 are independent and identically
distributed continuous random variables. For
 find 
 That is, find the
probability that the function 
 is a unimodal
function with maximal value 
Hint: Write
6.31. Compute the density of the range of a sample of size  from a
continuous distribution having density function 
6.32. Let 
 be the ordered values of 
independent uniform (0, 1) random variables. Prove that for
where 
 and 
6.33. Let 
 be a set of independent and identically distributed
continuous random variables having distribution function 
 and let
 denote their ordered values. If 
 independent of the
 also has distribution 
 determine
a. 
b. 
c. 
6.34. Let 
 be independent and identically distributed random
variables having distribution function  and density 
 The quantity
 defined to be the average of the smallest and
largest values in 
 is called the midrange of the sequence.
Show that its distribution function is
6.35. Let 
 be independent uniform (0, 1) random variables.
Let 
 denote the range and 
 the
midrange of 
 Compute the joint density function of  and 
2𝑛൅1
ሺ𝑛൅1,  𝑛൅1ሻ.
𝑋ଵ, … , 𝑋௡
𝐴ൌ൛𝑋ଵ൏⋯൏𝑋௝൐𝑋௝൅ଵ൐⋯൐𝑋௡ൟ,
𝑃ሺ𝐴ሻ.
𝑋ሺ𝑖ሻൌ𝑋௜,  𝑖ൌ1, … , 𝑛,  
𝑋ሺ𝑗ሻ.
𝐴ൌ൛maxሺ𝑋ଵ, … , 𝑋௝ሻൌmaxሺ𝑋ଵ, … , 𝑋௡ሻ,  
𝑋ଵ൏⋯൏𝑋௝,   𝑋௝൅ଵ൐⋯൐𝑋௡ൟ
𝑛
𝑓.
𝑋ሺଵሻ൑𝑋ሺଶሻ൑⋯൑𝑋ሺ௡ሻ
𝑛
1 ൑𝑘൑𝑛൅1,
𝑃൛𝑋ሺ௞ሻെ𝑋ሺ௞െଵሻ൐𝑡ൟൌሺ1 െ𝑡ሻ௡
𝑋ሺ଴ሻ≡0, 𝑋ሺ௡൅ଵሻ≡1,
0 ൏𝑡൏1 .
𝑋ଵ, … , 𝑋௡
𝐹,
𝑋ሺ௜ሻ, 𝑖ൌ1, … , 𝑛
𝑋,
𝑋௜, 𝑖ൌ1, … , 𝑛,
𝐹,
𝑃൛𝑋൐𝑋ሺ௡ሻൟ;
𝑃൛𝑋൐𝑋ሺଵሻൟ;
𝑃൛𝑋ሺ௜ሻ൏𝑋൏𝑋ሺ௝ሻൟ, 1 ൑𝑖൏𝑗൑𝑛.
𝑋ଵ, … , 𝑋௡
𝐹
𝑓.
𝑀≡ൣ𝑋ሺଵሻ൅𝑋ሺ௡ሻ൧/2,
𝑋ଵ, … , 𝑋௡,
𝐹ெሺ𝑚ሻൌ𝑛඲
െஶ
௠
ሾ𝐹ሺ2𝑚െ𝑥ሻെ𝐹ሺ𝑥ሻሿ௡െଵ𝑓ሺ𝑥ሻ 𝑑𝑥
𝑋ଵ, … , 𝑋௡
𝑅ൌ𝑋ሺ௡ሻെ𝑋ሺଵሻ
𝑀ൌൣ𝑋ሺ௡ሻ൅𝑋ሺଵሻ൧/2
𝑋ଵ, … , 𝑋௡.
𝑅
𝑀
469 of 848

6.36. If  and  are independent standard normal random variables,
determine the joint density function of
Then use your result to show that 
 has a Cauchy distribution.
6.37. Suppose that 
 has a bivariate normal distribution with
parameters 
a. Show that 
 has a bivariate normal distribution
with parameters 
b. What is the joint distribution of 
6.38. Suppose that  has a beta distribution with parameters 
and that the conditional distribution of  given that 
 is binomial
with parameters 
 Show that the conditional density of 
given that 
 is the density of a beta random variable with
parameters 
 is said to be a beta binomial random
variable.
6.39. Consider an experiment with  possible outcomes, having
respective probabilities 
 and suppose we
want to assume a probability distribution on the probability vector
 Because 
 we cannot define a density on
 but what we can do is to define one on 
 and
then take 
 The Dirichlet distribution takes
 to be uniformly distributed over the set
 That is, the
Dirichlet density is
a. Determine 
Hint: Use results from Section 6.3.1
.
Let 
 be independent uniform 
 random variables.
b. Show that the Dirichlet density is the conditional density of
 given that 
c. Show that 
 has a Dirichlet
.
𝑋
𝑌
𝑈ൌ𝑋 𝑉ൌ𝑋
𝑌
𝑋/𝑌
ሺ𝑋, 𝑌ሻ
𝜇௫, 𝜇௬, 𝜎௫, 𝜎௬, 𝜌.
ሺ
𝑋െ𝜇௫
𝜎𝑥
,
𝑌െ𝜇௬
𝜎𝑦
ሻ
0, 1, 0, 1, 𝜌.
ሺ𝑎𝑋൅𝑏, 𝑐𝑌൅𝑑ሻ.
𝑋
ሺ𝑎, 𝑏ሻ,
𝑁
𝑋ൌ𝑥
ሺ𝑛൅𝑚, 𝑥ሻ.
𝑋
𝑁ൌ𝑛
ሺ𝑛൅𝑎, 𝑚൅𝑏ሻ. 𝑁
𝑛
𝑃ଵ, … , 𝑃௡, ෍
௜ൌଵ
௡
𝑃௜ൌ1,
ሺ𝑃ଵ, … , 𝑃௡ሻ.
෍
௜ൌଵ
௡
𝑃௜ൌ1,
𝑃ଵ, … , 𝑃௡,
𝑃ଵ, … , 𝑃௡െଵ
𝑃௡ൌ1 െ෍
௜ൌଵ
௡െଵ
𝑃௜.
ሺ𝑃ଵ, … , 𝑃௡െଵሻ
𝑆ൌሼሺ𝑝ଵ, … , 𝑝௡െଵሻ: ෍
௜ൌଵ
௡െଵ
𝑝௜൏1, 𝑝௜൐0, 𝑖ൌ1, … , 𝑛െ1ሽ.
𝑓௉భ,… ,௉೙െభሺ𝑝ଵ, … , 𝑝௡െଵሻൌ𝐶,
𝑝௜൐0, 𝑖ൌ1, … , 𝑛െ1,
௜ൌଵ
௡െଵ
𝑝௜൏1
𝐶.
𝑈ଵ, … , 𝑈௡
ሺ0, 1ሻ
𝑈ଵ, … , 𝑈௡െଵ
෍
௜ൌଵ
௡െଵ
𝑈௜൏1 .
*
𝑈ሺଵሻ, 𝑈ሺଶሻെ𝑈ሺଵሻ, … , 𝑈ሺ௡ሻെ𝑈ሺ௡െଵሻ
470 of 848

distribution, where 
 are the order statistics of
6.40. Let 
 and 
 be, respectively,
the joint distribution function and the joint density function of
Show that
6.41. For given constants 
 let 
 and let
 and 
 be, respectively the joint
distribution function and the joint density function of 
a. Express 
 in terms of the joint distribution
function of 
b. Express 
 in terms of the joint density
function of 
c. Use Equation (7.3)
 to verify your answer to part (b).
𝑈ሺଵሻ, … , 𝑈ሺ௡ሻ
𝑈ଵ, … , 𝑈௡.
𝐹௑భ,… ,௑೙ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑓௑భ,… ,௑೙ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑋ଵ, … , 𝑋௡.
∂௡
∂𝑥ଵ⋯∂𝑥௡
𝐹௑భ,… ,௑೙ሺ𝑥ଵ, … , 𝑥௡ሻൌ𝑓௑భ,… ,௑೙ሺ𝑥ଵ, … , 𝑥௡ሻ.
𝑐௜൐0,
𝑌௜ൌ𝑐௜𝑋௜,  𝑖ൌ1, … , 𝑛,
𝐹௒భ,… ,௒೙ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑓௒భ,… ,௒೙ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑌ଵ, … , 𝑌௡
𝐹௒భ,… ,௒೙ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑋ଵ, … , 𝑋௡
𝑓௒భ,… ,௒೙ሺ𝑥ଵ, … , 𝑥௡ሻ
𝑋ଵ, … , 𝑋௡.
6.1. Each throw of an unfair die lands on each of the odd numbers 1, 3,
5 with probability  and on each of the even numbers with probability 2
a. Find 
b. Suppose that the die is tossed. Let  equal 1 if the result is an
even number, and let it be 0 otherwise. Also, let  equal 1 if the
result is a number greater than three and let it be 0 otherwise.
Find the joint probability mass function of  and 
 Suppose
now that 12 independent tosses of the die are made.
c. Find the probability that each of the six outcomes occurs exactly
twice.
d. Find the probability that 4 of the outcomes are either one or two,
4 are either three or four, and 4 are either five or six.
e. Find the probability that at least 8 of the tosses land on even
numbers.
6.2. The joint probability mass function of the random variables 
is
𝐶
𝐶.
𝐶.
𝑋
𝑌
𝑋
𝑌.
𝑋, 𝑌, 𝑍
𝑝ሺ1, 2, 3ሻൌ𝑝ሺ2, 1, 1ሻൌ𝑝ሺ2, 2, 1ሻൌ𝑝ሺ2, 3, 2ሻൌ1
4
471 of 848

Find (a) 
 and (b) 
6.3. The joint density of  and  is given by
a. Find 
b. Find the density function of 
c. Find the density function of 
d. Find [ ].
e. Find [ ].
6.4. Let 
 where all  are positive integers. Argue that if
 has a multinomial distribution, then so does 
 where,
with 
That is, 
 is the sum of the first 
 of the 
 is the sum of the next
 and so on.
6.5. Suppose that 
 and  are independent random variables that
are each equally likely to be either 1 or 2. Find the probability mass
function of (a) 
 (b) 
 and (c) 
6.6. Let  and  be continuous random variables with joint density
function
where  is a constant.
a. What is the value of 
b. Are  and  independent?
c. Find 
6.7. The joint density function of  and  is
a. Are  and  independent?
b. Find the density function of 
c. Find the density function of 
d. Find the joint distribution function.
e. Find [ ].
𝐸ሾ𝑋𝑌𝑍ሿ,
𝐸ሾ𝑋𝑌൅𝑋𝑍൅𝑌𝑍ሿ.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝐶ሺ𝑦െ𝑥ሻ𝑒െ௬  െ𝑦൏𝑥൏𝑦,  0 ൏𝑦൏∞
𝐶.
𝑋.
𝑌.
𝐸𝑋
𝐸𝑌
𝑟ൌ𝑟ଵ൅…  ൅𝑟௞,
𝑟௜
𝑋ଵ, … , 𝑋௥
𝑌ଵ, … , 𝑌௞
𝑟଴ൌ0,
𝑌௜ൌ
෍
௝ൌ௥೔െభ൅ଵ
௥೔െభ൅௥೔
𝑋௝ ,  𝑖൑𝑘
𝑌ଵ
𝑟ଵ
𝑋′𝑠,  𝑌ଶ
𝑟ଶ,
𝑋, 𝑌,
𝑍
𝑋𝑌𝑍,
𝑋𝑌൅𝑋𝑍൅𝑌𝑍,
𝑋ଶ൅𝑌𝑍.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌቐ
𝑥
5 ൅𝑐𝑦
 0 ൏𝑥൏1,   1 ൏𝑦൏5
0
 otherwise
𝑐
𝑐?
𝑋
𝑌
𝑃ሼ𝑋൅𝑌൐3ሽ.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌቊ𝑥𝑦
 0 ൏𝑥൏1,   0 ൏𝑦൏2
0
 otherwise
𝑋
𝑌
𝑋.
𝑌.
𝐸𝑌
472 of 848

f. Find 
6.8. Consider two components and three types of shocks. A type 1
shock causes component 1 to fail, a type 2 shock causes component 2
to fail, and a type 3 shock causes both components 1 and 2 to fail. The
times until shocks 1, 2, and 3 occur are independent exponential
random variables with respective rates 
 and 
 Let 
 denote
the time at which component  fails, 
 The random variables
 are said to have a joint bivariate exponential distribution. Find
6.9. Consider a directory of classified advertisements that consists of 
pages, where 
 is very large. Suppose that the number of
advertisements per page varies and that your only method of finding
out how many advertisements there are on a specified page is to count
them. In addition, suppose that there are too many pages for it to be
feasible to make a complete count of the total number of
advertisements and that your objective is to choose a directory
advertisement in such a way that each of them has an equal chance of
being selected.
a. If you randomly choose a page and then randomly choose an
advertisement from that page, would that satisfy your objective?
Why or why not?
Let 
 denote the number of advertisements on page
 and suppose that whereas these quantities are
unknown, we can assume that they are all less than or equal to
some specified value 
 Consider the following algorithm for
choosing an advertisement.
Call each pass of the algorithm through step 1 an iteration. For
instance, if the first randomly chosen page is rejected and the
second accepted, then we would have needed 2 iterations of the
algorithm to obtain an advertisement.
b. What is the probability that a single iteration of the algorithm
results in the acceptance of an advertisement on page 
𝑃ሼ𝑋൅𝑌൏1ሽ.
𝜆ଵ, 𝜆ଶ,
𝜆ଷ.
𝑋௜
𝑖
𝑖ൌ1, 2 .
𝑋ଵ, 𝑋ଶ
𝑃ሼ𝑋ଵ൐𝑠, 𝑋ଶ൐𝑡ሽ.
𝑚
𝑚
𝑛ሺ𝑖ሻ
𝑖, 𝑖ൌ1, … , 𝑚,
𝑛.
Choose a page at random. Suppose it is page 
Determine 
 by counting the number of
advertisements on page 
“Accept” page  with probability 
/
 If page  is
accepted, go to step 3. Otherwise, return to step 1.
Randomly choose one of the advertisements on
page 
Step 1.
𝑋.
𝑛ሺ𝑋ሻ
𝑋.
Step 2.
𝑋
𝑛ሺ𝑋ሻ𝑛.
𝑋
Step 3.
𝑋.
𝑖?
473 of 848

c. What is the probability that a single iteration of the algorithm
results in the acceptance of an advertisement?
d. What is the probability that the algorithm goes through 
iterations, accepting the th advertisement on page  on the final
iteration?
e. What is the probability that the th advertisement on page  is
the advertisement obtained from the algorithm?
f. What is the expected number of iterations taken by the
algorithm?
6.10. The “random” parts of the algorithm in Self-Test Problem 6.9
can be written in terms of the generated values of a sequence of
independent uniform (0, 1) random variables, known as random
numbers. With [ ] defined as the largest integer less than or equal to 
the first step can be written as follows:
a. Explain why the above is equivalent to step 1 of Problem 6.8
.
Hint: What is the probability mass function of 
b. Write the remaining steps of the algorithm in a similar style.
6.11. Let 
 be a sequence of independent uniform (0, 1) random
variables. For a fixed constant 
 define the random variable  by
Is  independent of 
 That is, does knowing the value of the first
random variable that is greater than  affect the probability distribution
of when this random variable occurs? Give an intuitive explanation for
your answer.
6.12. The accompanying dartboard is a square whose sides are of
length 6:
𝑘
𝑗
𝑖
𝑗
𝑖
𝑥
𝑥,
Generate a uniform (0, 1) random variable 
 Let
 and determine the value of 
Step 1.
𝑈.
𝑋ൌሾ𝑚𝑈ሿ൅1,
𝑛ሺ𝑋ሻ.
𝑋?
𝑋ଵ, 𝑋ଶ, … 
𝑐,
𝑁
𝑁ൌminሼ𝑛:𝑋௡൐𝑐ሽ
𝑁
𝑋ே?
𝑐
474 of 848

The three circles are all centered at the center of the board and are of
radii 1, 2, and 3, respectively. Darts landing within the circle of radius 1
score 30 points, those landing outside this circle, but within the circle of
radius 2, are worth 20 points, and those landing outside the circle of
radius 2, but within the circle of radius 3, are worth 10 points. Darts that
do not land within the circle of radius 3 do not score any points.
Assuming that each dart that you throw will, independently of what
occurred on your previous throws, land on a point uniformly distributed
in the square, find the probabilities of the accompanying events:
a. You score 20 on a throw of the dart.
b. You score at least 20 on a throw of the dart.
c. You score 0 on a throw of the dart.
d. The expected value of your score on a throw of the dart.
e. Both of your first two throws score at least 10.
f. Your total score after two throws is 30.
6.13. A model proposed for NBA basketball supposes that when two
teams with roughly the same record play each other, the number of
points scored in a quarter by the home team minus the number scored
by the visiting team is approximately a normal random variable with
mean 1.5 and variance 6. In addition, the model supposes that the
point differentials for the four quarters are independent. Assume that
this model is correct.
a. What is the probability that the home team wins?
b. What is the conditional probability that the home team wins,
given that it is behind by 5 points at halftime?
c. What is the conditional probability that the home team wins,
given that it is ahead by 5 points at the end of the first quarter?
475 of 848

6.14. Let  be a geometric random variable with parameter 
Suppose that the conditional distribution of  given that 
 is the
gamma distribution with parameters  and 
 Find the conditional
probability mass function of  given that 
6.15. Let  and  be independent uniform (0, 1) random variables.
a. Find the joint density of 
b. Use the result obtained in part (a) to compute the density
function of 
6.16. You and three other people are to place bids for an object, with
the high bid winning. If you win, you plan to sell the object immediately
for $10,000. How much should you bid to maximize your expected
profit if you believe that the bids of the others can be regarded as being
independent and uniformly distributed between $7,000 and $10,000
thousand dollars?
6.17. Find the probability that 
 is a permutation of 
when 
 are independent and
a. each is equally likely to be any of the values 
b. each has the probability mass function
6.18. Let 
 and 
 be independent random vectors, with
each vector being a random ordering of  ones and 
 zeros. That
is, their joint probability mass functions are
Let
denote the number of coordinates at which the two vectors have
different values. Also, let 
 denote the number of values of  for which
a. Relate  to 
b. What is the distribution of 
c. Find [ ].
d. Find Var
𝑁
𝑝.
𝑋
𝑁ൌ𝑛
𝑛
𝜆.
𝑁
𝑋ൌ𝑥.
𝑋
𝑌
𝑈ൌ𝑋, 𝑉ൌ𝑋൅𝑌.
𝑉.
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
1, 2, … , 𝑛,
𝑋ଵ, 𝑋ଶ, … , 𝑋௡
1, … , 𝑛;
𝑃ሼ𝑋௜ൌ𝑗ሽൌ𝑝௝, 𝑗ൌ1, … , 𝑛.
𝑋ଵ, … , 𝑋௡
𝑌ଵ, … , 𝑌௡
𝑘
𝑛െ𝑘
𝑃ሼ𝑋ଵൌ𝑖ଵ, … , 𝑋௡ൌ𝑖௡ሽ
ൌ𝑃ሼ𝑌ଵൌ𝑖ଵ, … , 𝑌௡ൌ𝑖௡ሽ
ൌ
1
ቆ𝑛
𝑘ቇ
 ,  𝑖௝ൌ0, 1,   ෍
௝ൌଵ
௡
𝑖௝ൌ𝑘
𝑁ൌ෍
௜ൌଵ
௡
|𝑋௜െ𝑌௜|
𝑀
𝑖
𝑋௜ൌ1, 𝑌௜ൌ0 .
𝑁
𝑀.
𝑀?
𝐸𝑁
ሺ𝑁ሻ.
476 of 848

*6.19. Let 
 be independent standard normal random
variables, and let
a. What is the conditional distribution of 
 given that 
 Find
it for 
b. Show that, for 
 the conditional distribution of 
 given
that 
 is normal with mean 
 and variance 
6.20. Let 
 be a sequence of independent and identically
distributed continuous random variables. Find
a. 
b. 
6.21. Prove the identity
Hint: Derive an expression for 
 by taking the probability
of the complementary event.
6.22. In Example 1c
, find 
 when 
6.23. A Pareto random variable  with parameters 
 has
distribution function 
 For 
 verify
that the conditional distribution of  given that 
 is that of a Pareto
random variable with parameters 
 by evaluating
6.24. Verify the identity 
6.25. In a contest originating with  players, each player independently
advances to the next round, with player  advancing with probability 
If no players advance to the next round, then the contest ends and all
the players in the just concluded round are declared co-winners. If only
one player advances, then that player is declared the winner and the
contest ends. If two or more players advance, then those players play
another round. Let 
 denote the number of rounds that  plays.
a. Find 
Hint: Note that 
 will occur if  advances
at least  times and at least one of the other players advances at
least 
 times.
b. Find P(i is either the sole winner or one of the co-winners).
Hint: It might help to imagine that a player always continues to
𝑍ଵ, 𝑍ଶ, … , 𝑍௡
𝑆௝ൌ෍
௜ൌଵ
௝
𝑍௜
𝑆௡
𝑆௞ൌ𝑦.
𝑘ൌ1, … , 𝑛െ1 .
1 ൑𝑘൑𝑛,
𝑆௞
𝑆௡ൌ𝑥
𝑥𝑘/𝑛
𝑘ሺ𝑛െ𝑘ሻ/𝑛.
𝑋ଵ, 𝑋ଶ, … 
𝑃ሼ𝑋଺൐𝑋ଵ||𝑋ଵൌmaxሺ𝑋ଵ, … , 𝑋ହሻሽ
𝑃ሼ𝑋଺൐𝑋ଶ||𝑋ଵൌmaxሺ𝑋ଵ, … , 𝑋ହሻሽ
𝑃ሼ𝑋൑𝑠, 𝑌൑𝑡ሽൌ𝑃ሼ𝑋൑𝑠ሽ൅𝑃ሼ𝑌൑𝑡ሽ൅𝑃ሼ𝑋൐𝑠, 𝑌൐𝑡ሽെ1
𝑃ሼ𝑋൐𝑠, 𝑌൐𝑡ሽ
𝑃ሺ𝑋௥ൌ𝑖, 𝑌௦ൌ𝑗ሻ
𝑗൏𝑖.
𝑋
𝑎൐0, ൐0
𝐹ሺ𝑥ሻൌ1 െ𝑎ఒ𝑥െఒ, 𝑥൐𝑎. 𝑥൐𝑎.
𝑥଴൐𝑎,
𝑋
𝑋൐𝑥଴
ሺ𝑥଴, λ ሻ
𝑃ሺ𝑋൐𝑥||𝑋൐𝑥଴ሻ.
𝑓௑ሺ𝑥ሻൌ඲
െஶ
ஶ
𝑓௑ห௒ሺ𝑥|𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦.
𝑛
𝑖
𝑝௜.
𝑋௜
𝑖
𝑃ሺ𝑋௜൒𝑘ሻ.
ሼ𝑋௜൒𝑘ሽ
𝑖
𝑘
𝑘െ1
477 of 848

7.1 Introduction
7.2 Expectation of Sums of Random Variables
7.3 Moments of the Number of Events that Occur
7.4 Covariance, Variance of Sums, and Correlations
7.5 Conditional Expectation
7.6 Conditional Expectation and Prediction
7.7 Moment Generating Functions
7.8 Additional Properties of Normal Random Variables
7.9 General Definition of Expectation
play rounds until he or she fails to advance. (That is, if there is a
sole winner then imagine that that player continues on until a
failure occurs.)
c. Find P(i is the sole winner)
6.26. Let 
 be independent nonnegative integer valued random
variables, and let 
 With 
 we
want to determine 
 Let 
 if 
 is even and let it
equal 
 if 
 is odd.
In parts (a) and (b) fill in the missing word at the end of the sentence.
a.  is even if and only if the number of 
 that are odd is
b.  is even if and only if 
 is
c. Find 
d. Find 
Hint: Use parts (b) and (c).
𝑋ଵ, … , 𝑋௡
𝛼௜ൌ𝑃ሺ𝑋௜ is even ሻ, 𝑖ൌ1, … , 𝑛.
𝑆ൌ෍
௜ൌଵ
௡
𝑋௜
𝑝ൌ𝑃ሺ𝑆  is even ሻ.
𝑌௜ൌ1
𝑋௜
െ1
𝑋௜
𝑆
𝑋ଵ, … , 𝑋௡
𝑆
ෑ
௜ൌଵ
௡
𝑌௜
𝐸⎡
⎣
ෑ
௜ൌଵ
௡
𝑌௜⎤
⎦
.
𝑃ሺ𝑆 is even ሻ.
478 of 848

In this chapter, we develop and exploit additional properties of expected values. To
begin, recall that the expected value of the random variable  is defined by
when  is a discrete random variable with probability mass function p(x), and by
when  is a continuous random variable with probability density function f(x).
Since E[X] is a weighted average of the possible values of , it follows that if  must
lie between  and , then so must its expected value. That is, if
then
To verify the preceding statement, suppose that  is a discrete random variable for
which 
. Since this implies that 
 for all  outside of the
interval [ , ], it follows that
In the same manner, it can be shown that 
, so the result follows for discrete
random variables. As the proof in the continuous case is similar, the result follows.
𝑋
𝐸ሾ𝑋ሿൌ෍
௫
𝑥𝑝ሺ𝑥ሻ
𝑋
𝐸ሾ𝑋ሿൌ඲
െஶ
ஶ
𝑥𝑓ሺ𝑋ሻ𝑑𝑥
𝑋
𝑋
𝑋
𝑎
𝑏
𝑃ሼ𝑎൑𝑋൑𝑏ሽൌ1
𝑎൑𝐸ሾ𝑋ሿ൑𝑏
𝑋
𝑃ሼ𝑎൑𝑋൑𝑏ሽൌ1
𝑝ሺ𝑥ሻൌ0
𝑥
𝑎𝑏
𝐸ሾ𝑋ሿ
ൌ
෍
௫: ௣ሺ௑ሻவ଴
𝑥𝑝ሺ𝑋ሻ
൒
෍
௫: ௣ሺ௑ሻவ଴
𝑎𝑝ሺ𝑋ሻ
ൌ
𝑎
෍
௫: ௣ሺ௑ሻவ଴
𝑝ሺ𝑋ሻ
ൌ
𝑎
𝐸ሾ𝑋ሿ൑𝑏
479 of 848

For a two-dimensional analog of Propositions 4.1
 of Chapter 4
 and 2
.1 of
Chapter 5
, which give the computational formulas for the expected value of a
function of a random variable, suppose that  and  are random variables and  is a
function of two variables. Then we have the following result.
Proposition 2.1
If  and  have a joint probability mass function p(x,y), then
If  and  have a joint probability density function f(x,y), then
Let us give a proof of Proposition 2.1
 when the random variables  and  are
jointly continuous with joint density function 
 and when 
 is a
nonnegative random variable. Because 
, we have, by Lemma 2.1 of
Chapter 5
, that
Writing
shows that
Interchanging the order of integration gives
𝑋
𝑌
𝑔
𝑋
𝑌
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿൌ෍
௬
෍
௫
𝑔ሺ𝑥, 𝑦ሻ𝑝ሺ𝑥, 𝑦ሻ
𝑋
𝑌
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿൌ඲
െஶ
ஶ
඲
െஶ
ஶ
𝑔ሺ𝑥, 𝑦ሻ𝑓ሺ𝑥, 𝑦ሻ𝑑𝑥𝑑𝑦
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻ
𝑔ሺ𝑋, 𝑌ሻ
𝑔ሺ𝑋, 𝑌ሻ൒0
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿൌ඲
଴
ஶ
𝑃ሼ𝑔ሺ𝑋, 𝑌ሻ൐𝑡ሽ𝑑𝑡
𝑃ሼ𝑔ሺ𝑋, 𝑌ሻ൐𝑡ሽൌ඲඲
ሺ௫,௬ሻ:௚ሺ௫,௬ሻவ௧
𝑓ሺ𝑥, 𝑦ሻ𝑑𝑦𝑑𝑥
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿൌ඲
଴
ஶ
඲඲
ሺ௫,௬ሻ:௚ሺ௫,௬ሻவ௧
𝑓ሺ𝑥, 𝑦ሻ𝑑𝑦𝑑𝑥𝑑𝑡
480 of 848

Thus, the result is proven when 
 is a nonnegative random variable. The
general case then follows as in the one-dimensional case. (See Theoretical
Exercises 2 and 3 of Chapter 5
.)
Example 2a
An accident occurs at a point  that is uniformly distributed on a road of length .
At the time of the accident, an ambulance is at a location  that is also uniformly
distributed on the road. Assuming that  and  are independent, find the
expected distance between the ambulance and the point of the accident.
Solution
We need to compute 
. Since the joint density function of  and  is
it follows from Proposition 2.1
 that
Now,
Therefore,
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿ
ൌ
඲
௫
඲
௬
඲
௧ൌ଴
௚ሺ௫,௬ሻ
𝑓ሺ𝑥, 𝑦ሻ𝑑𝑡𝑑𝑦𝑑𝑥
ൌ
඲
௫
඲
௬
𝑔ሺ𝑥, 𝑦ሻ𝑓ሺ𝑥, 𝑦ሻ𝑑𝑦𝑑𝑥
𝑔ሺ𝑋, 𝑌ሻ
𝑋
𝐿
𝑌
𝑋
𝑌
𝐸ሾ||𝑋െ𝑌||ሿ
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ1
𝐿ଶ,  0 ൏𝑥൏𝐿,  0 ൏𝑦൏𝐿
𝐸ሾ|𝑋െ𝑌|ሿൌ1
𝐿ଶ඲
଴
௅
඲
଴
௅
|𝑥െ𝑦| 𝑑𝑦𝑑𝑥
඲
଴
௅
|𝑥െ𝑦|𝑑𝑦
ൌ
඲
଴
௫
ሺ𝑥െ𝑦ሻ𝑑𝑦൅඲
௫
௅
ሺ𝑦െ𝑥ሻ𝑑𝑦
ൌ
𝑥ଶ
2 ൅𝐿ଶ
2 െ𝑥ଶ
2 െ𝑥ሺ𝐿െ𝑥ሻ
ൌ
𝐿ଶ
2 ൅𝑥ଶെ𝑥𝐿
481 of 848

For an important application of Proposition 2.1
, suppose that 
 and 
 are
both finite and let 
. Then, in the continuous case,
The same result holds in general; thus, whenever 
 and 
] are finite,
Example 2b
Suppose that for random variables  and ,
That is, for any outcome of the probability experiment, the value of the random
variable  is greater than or equal to the value of the random variable . Since
 is equivalent to the inequality 
, it follows that 
, or,
equivalently,
Using Equation (2.1)
, we may show by a simple induction proof that if 
 is
finite for all 
, then
𝐸ሾ||𝑋െ𝑌||ሿ
ൌ
1
𝐿ଶ඲
଴
௅
ቆ𝐿ଶ
2 ൅𝑥ଶെ𝑥𝐿ቇ𝑑𝑥
ൌ
𝐿
3
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ
𝑔ሺ𝑋, 𝑌ሻൌ𝑋൅𝑌
𝐸ሾ𝑋൅𝑌ሿ
ൌ
඲
െஶ
ஶ
඲
െஶ
ஶ
ሺ𝑥൅𝑦ሻ𝑓ሺ𝑥, 𝑦ሻ𝑑𝑥𝑑𝑦
ൌ
඲
െஶ
ஶ
඲
െஶ
ஶ
𝑥𝑓ሺ𝑥, 𝑦ሻ𝑑𝑦𝑑𝑥൅඲
െஶ
ஶ
඲
െஶ
ஶ
𝑦𝑓ሺ𝑥, 𝑦ሻ𝑑𝑥𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑥𝑓௑ሺ𝑥ሻ𝑑𝑥൅඲
െஶ
ஶ
𝑦𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
𝐸ሾ𝑋ሿ൅𝐸ሾ𝑌ሿ
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌
𝐸ሾ𝑋൅𝑌ሿൌ𝐸ሾ𝑋ሿ൅𝐸ሾ𝑌ሿ
(2.1)
𝑋
𝑌
𝑋൒𝑌
𝑋
𝑌
𝑋൒𝑌
𝑋െ𝑌൒0
𝐸ሾ𝑋െ𝑌ሿ൒0
𝐸ሾ𝑋ሿ൒𝐸ሾ𝑌ሿ
𝐸ሾ𝑋௜ሿ
𝑖ൌ1, …, 𝑛
𝐸ሾ𝑋ଵ൅⋯൅𝑋௡ሿൌ𝐸ሾ𝑋ଵሿ൅⋯൅𝐸ሾ𝑋௡ሿ
(2.2)
482 of 848

Equation (2.2)
 is an extremely useful formula whose utility will now be illustrated
by a series of examples.
Example 2c The sample mean
Let 
 be independent and identically distributed random variables having
distribution function  and expected value . Such a sequence of random
variables is said to constitute a sample from the distribution . The quantity
is called the sample mean. Compute 
.
Solution
That is, the expected value of the sample mean is , the mean of the distribution.
When the distribution mean  is unknown, the sample mean is often used in
statistics to estimate it.
Example 2d Boole’s inequality
Let 
 denote events, and define the indicator variables 
, by
Let
𝑋ଵ, …, 𝑋௡
𝐹
𝜇
𝐹
𝑋̅ ̅̅ ̅ൌ෍
௜ൌଵ
௡
𝑋௜
𝑛
𝐸ሾ𝑋̅ ̅̅ ̅ሿ
𝐸ሾ𝑋̅ ̅̅ ̅ሿ
ൌ𝐸቎෍
௜ൌଵ
௡
𝑋௜
𝑛቏
ൌ
1
𝑛𝐸቎෍
௜ൌଵ
௡
𝑋௜቏
ൌ
1
𝑛෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿ
ൌ𝜇  since  𝐸ሾ𝑋௜ሿ≡𝜇
𝜇
𝜇
𝐴ଵ, …, 𝐴௡
𝑋௜, 𝑖ൌ1, …, 𝑛
𝑋௜ൌቊ
1
  if 𝐴௜ occurs
0
 otherwise
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
483 of 848

so  denotes the number of the events 
 that occur. Finally, let
so  is equal to 1 if at least one of the 
 occurs and is 0 otherwise. Now, it is
immediate that
so
But since
and
we obtain Boole’s inequality, namely,
The next three examples show how Equation (2.2)
 can be used to calculate the
expected value of binomial, negative binomial, and hypergeometric random
variables. These derivations should be compared with those presented in Chapter
4
.
Example 2e Expectation of a binomial random variable
Let  be a binomial random variable with parameters  and . Recalling that
such a random variable represents the number of successes in  independent
trials when each trial has probability  of being a success, we have that
where
𝑋
𝐴௜
𝑌ൌቊ1
 if  𝑋൒1
0
   otherwise
𝑌
𝐴௜
𝑋൒𝑌
𝐸ሾ𝑋ሿ൒𝐸ሾ𝑌ሿ
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻ
𝐸ሾ𝑌ሿൌ𝑃ሼat least one of the  𝐴௜ occurሽൌ𝑃 ൭
௜
ଵ
௡
𝐴௜൱
𝑃 ൭
௜
ଵ
௡
𝐴௜൱൑෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻ
𝑋
𝑛
𝑝
𝑛
𝑝
𝑋ൌ𝑋ଵ൅𝑋ଶ൅⋯൅𝑋௡
484 of 848

Hence, 
 is a Bernoulli random variable having expectation
 Thus,
Example 2f Mean of a negative binomial random variable
If independent trials having a constant probability p of being successes are
performed, determine the expected number of trials required to amass a total of 
successes.
Solution
If  denotes the number of trials needed to amass a total of  successes, then 
is a negative binomial random variable that can be represented by
where 
 is the number of trials required to obtain the first success, 
 the
number of additional trials until the second success is obtained, 
 the number of
additional trials until the third success is obtained, and so on. That is, 
represents the number of additional trials required after the (
) success until a
total of  successes is amassed. A little thought reveals that each of the random
variables 
 is a geometric random variable with parameter . Hence, from the
results of Example 8b
 of Chapter 4
, 
; thus,
Example 2g Mean of a hypergeometric random variable
If  balls are randomly selected from an urn containing  balls of which 
 are
white, find the expected number of white balls selected.
Solution
Let  denote the number of white balls selected, and represent  as
where
𝑋௜ൌቊ1
  if the  𝑖th trial is a success
0
  if the  𝑖th trial is a failure
𝑋௜
𝐸ሾ𝑋௜ሿൌ1ሺ𝑝ሻ൅0ሺ1 െ𝑝ሻ.
𝐸ሾ𝑋ሿൌ𝐸ሾ𝑋ଵሿ൅𝐸ሾ𝑋ଶሿ൅⋯൅𝐸ሾ𝑋௡ሿൌ𝑛𝑝
𝑟
𝑋
𝑟
𝑋
𝑋ൌ𝑋ଵ൅𝑋ଶ൅⋯൅𝑋௥
𝑋ଵ
𝑋ଶ
𝑋ଷ
𝑋௜
𝑖െ1
𝑖
𝑋௜
𝑝
𝐸ሾ𝑋௜ሿൌ1/𝑝, 𝑖ൌ1, 2, …, 𝑟
𝐸ሾ𝑋ሿൌ𝐸ሾ𝑋ଵሿ൅⋯൅𝐸ሾ𝑋௥ሿൌ𝑟
𝑝
𝑛
𝑁
𝑚
𝑋
𝑋
𝑋ൌ𝑋ଵ൅⋯൅𝑋௠
485 of 848

Now
Hence,
We could also have obtained the preceding result by using the alternative
representation
where
Since the  th ball selected is equally likely to be any of the  balls, it follows that
so
Example 2h Expected number of matches
Suppose that  people throw their hats into the center of a room. The hats are
mixed up, and each person randomly selects one. Find the expected number of
people who select their own hat.
𝑋௜ൌቊ1
if the  𝑖th white ball is selected
0
otherwise
𝐸ሾ𝑋௜ሿ
ൌ
𝑃ሼ𝑋௜ൌ1ሽ
ൌ
𝑃ሼ𝑖th white ball is selectedሽ
ൌ
ቆ1
1ቇ ቆ𝑁െ1
𝑛െ1 ቇ
𝑁
𝑛
ൌ
𝑛
𝑁
𝐸ሾ𝑋ሿൌ𝐸ሾ𝑋ଵሿ൅⋯൅𝐸ሾ𝑋௠ሿൌ𝑚𝑛
𝑁
𝑋ൌ𝑌ଵ൅⋯൅𝑌௡
𝑌௜ൌቊ1
if the  𝑖th ball selected is white
0
otherwise
𝑖
𝑁
𝐸ሾ𝑌௜ሿൌ𝑚
𝑁
𝐸ሾ𝑋ሿൌ𝐸ሾ𝑌ଵሿ൅⋯൅𝐸ሾ𝑌௡ሿൌ𝑛𝑚
𝑁
𝑁
486 of 848

Solution
Letting  denote the number of matches, we can compute 
 most easily by
writing
where
Since, for each , the  th person is equally likely to select any of the  hats,
Thus,
Hence, on the average, exactly one person selects his own hat.
Example 2i Coupon-collecting problems
Suppose that there are  types of coupons, and each time one obtains a coupon,
it is equally likely to be any one of the  types. Find the expected number of
coupons one needs to amass before obtaining a complete set of at least one of
each type.
Solution
Let  denote the number of coupons collected before a complete set is attained.
We compute 
 by using the same technique we used in computing the mean
of a negative binomial random variable (Example 2f
). That is, we define
 to be the number of additional coupons that need be obtained
after  distinct types have been collected in order to obtain another distinct type,
and we note that
When  distinct types of coupons have already been collected, a new coupon
obtained will be of a distinct type with probability 
. Therefore,
𝑋
𝐸ሾ𝑋ሿ
𝑋ൌ𝑋ଵ൅𝑋ଶ൅⋯൅𝑋ே
𝑋௜ൌቊ1
if the  𝑖th person selects his own hat
0
otherwise
𝑖
𝑖
𝑁
𝐸ሾ𝑋௜ሿൌ𝑃ሼ𝑋௜ൌ1ሽൌ1
𝑁
𝐸ሾ𝑋ሿൌ𝐸ሾ𝑋ଵሿ൅⋯൅𝐸ሾ𝑋ேሿൌቆ1
𝑁ቇ𝑁ൌ1
𝑁
𝑁
𝑋
𝐸ሾ𝑋ሿ
𝑋௜, 𝑖ൌ0, 1, …, 𝑁െ1
𝑖
𝑋ൌ𝑋଴൅𝑋ଵ൅⋯൅𝑋ேെଵ
𝑖
ሺ𝑁െ𝑖ሻ/𝑁
487 of 848

or, in other words, 
 is a geometric random variable with parameter 
.
Hence,
implying that
Example 2j
Ten hunters are waiting for ducks to fly by. When a flock of ducks flies overhead,
the hunters fire at the same time, but each chooses his target at random,
independently of the others. If each hunter independently hits his target with
probability , compute the expected number of ducks that escape unhurt when a
flock of size 10 flies overhead.
Solution
Let 
 equal 1 if the th duck escapes unhurt and 0 otherwise, for 
.
The expected number of ducks to escape can be expressed as
To compute 
, we note that each of the hunters will,
independently, hit the th duck with probability 
 so
Hence,
Example 2k Expected number of runs
𝑃ሼ𝑋௜ൌ𝑘ሽൌ𝑁െ𝑖
𝑁
൬𝑖
𝑁൰
௞െଵ
 𝑘൒1
𝑋௜
ሺ𝑁െ𝑖ሻ/𝑁
𝐸ሾ𝑋௜ሿൌ
𝑁
𝑁െ𝑖
𝐸ሾ𝑋ሿൌ1 ൅
𝑁
𝑁െ1 ൅
𝑁
𝑁െ2 ൅⋯൅𝑁
1
ൌ𝑁⎡
⎣
1 ൅⋯൅
1
𝑁െ1 ൅1
𝑁
⎤
⎦
𝑝
𝑋௜
𝑖
𝑖ൌ1, 2, …, 10
𝐸ሾ𝑋ଵ൅⋯൅𝑋ଵ଴ሿൌ𝐸ሾ𝑋ଵሿ൅⋯൅𝐸ሾ𝑋ଵ଴ሿ
𝐸ሾ𝑋௜ሿൌ𝑃ሼ𝑋௜ൌ1ሽ
𝑖
𝑝/10,
𝑃ሼ𝑋௜ൌ1ሽൌ൬1 െ𝑝
10൰
ଵ଴
𝐸ሾ𝑋ሿൌ10൬1 െ𝑝
10൰
ଵ଴
488 of 848

Suppose that a sequence of 
1’s and 
0’s is randomly permuted so that each
of the 
 possible arrangements is equally likely. Any consecutive
string of 1’s is said to constitute a run of 1’s for instance, if 
, and the
ordering is 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, then there are 3 runs of 1’s and we are
interested in computing the mean number of such runs. To compute this quantity,
let
Therefore, (1), the number of runs of 1, can be expressed as
and it follows that
Now,
and for 
,
Hence,
Similarly, 
 the expected number of runs of 0’s, is
and the expected number of runs of either type is
𝑛
𝑚
ሺ𝑛൅𝑚ሻ!/ሺ𝑛!𝑚!ሻ
𝑛ൌ6, 𝑚ൌ4
𝐼௜ൌቊ1
if a run of 1's starts at the  𝑖th position
0
otherwise
𝑅
𝑅ሺ1ሻൌ
෍
௜ൌଵ
௡൅௠
𝐼௜
𝐸ሾ𝑅ሺ1ሻሿൌ
෍
௜ൌଵ
௡൅௠
𝐸ሾ𝐼௜ሿ
𝐸ሾ𝐼ଵሿ
ൌ
𝑃ሼ“1” in position 1ሽ
ൌ
𝑛
𝑛൅𝑚
1 ൏𝑖൑𝑛൅𝑚
𝐸ሾ𝐼௜ሿൌ
𝑃ሼ“0” in position  𝑖െ1, “1” in position  𝑖ሽ
ൌ
𝑚
𝑛൅𝑚
𝑛
𝑛൅𝑚െ1
𝐸ሾ𝑅ሺ1ሻሿൌ
𝑛
𝑛൅𝑚൅ሺ𝑛൅𝑚െ1ሻ
𝑛𝑚
ሺ𝑛൅𝑚ሻሺ𝑛൅𝑚െ1ሻ
𝐸ሾ𝑅ሺ0ሻሿ,
𝐸ሾ𝑅ሺ0ሻሿൌ
𝑚
𝑛൅𝑚൅
𝑛𝑚
𝑛൅𝑚
489 of 848

Example 2l A random walk in the plane
Consider a particle initially located at a given point in the plane, and suppose that
it undergoes a sequence of steps of fixed length, but in a completely random
direction. Specifically, suppose that the new position after each step is one unit of
distance from the previous position and at an angle of orientation from the
previous position that is uniformly distributed over (0, 2 ). (See Figure 7.1
.)
Compute the expected square of the distance from the origin after  steps.
Figure 7.1
Solution
Letting (
) denote the change in position at the th step, 
, in
rectangular coordinates, we have
where 
, are, by assumption, independent uniform (0, 2 ) random
𝐸ሾ𝑅ሺ1ሻ൅𝑅ሺ0ሻሿൌ1 ൅2𝑛𝑚
𝑛൅𝑚
𝜋
𝑛
𝑋௜, 𝑌௜
𝑖
𝑖ൌ1, …, 𝑛
𝑋௜
ൌ
cos 𝜃௜
𝑌௜
ൌ
sin 𝜃௜
𝜃௜, 𝑖ൌ1, …, 𝑛
𝜋
490 of 848

variables. Because the position after  steps has rectangular coordinates
, it follows that 
, the square of the distance from the origin,
is given by
where 
. Taking expectations and using the independence of
 and 
 when 
 and the fact that
we arrive at
Example 2m Analyzing the quick-sort algorithm
Suppose that we are presented with a set of  distinct values 
 and
that we desire to put them in increasing order, or as it is commonly stated, to sort
them. An efficient procedure for accomplishing this task is the quick-sort
algorithm, which is defined as follows: When 
, the algorithm compares the
two values and then puts them in the appropriate order. When 
, one of the
elements is randomly chosen say it is 
 and then all of the other values are
compared with 
. Those smaller than 
 are put in a bracket to the left of 
 and
those larger than 
 are put in a bracket to the right of 
. The algorithm then
repeats itself on these brackets and continues until all values have been sorted.
For instance, suppose that we desire to sort the following 10 distinct values:
𝑛
ቌ෍
௜ൌଵ
௡
𝑋௜, ෍
௜ൌଵ
௡
𝑌௜ቍ
𝐷ଶ
𝐷ଶ
ൌ
ቌ෍
௜ൌଵ
௡
𝑋௜ቍ
ଶ
൅ቌ෍
௜ൌଵ
௡
𝑌௜ቍ
ଶ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋ଶ
௜൅𝑌ଶ
௜ሻ൅෍෍
௜ஷ௝
ሺ𝑋௜𝑋௝൅𝑌௜𝑌௝ሻ
ൌ
𝑛൅෍෍
௜ஷ௝
ሺcos 𝜃௜ cos 𝜃௝൅sin 𝜃௜ sin 𝜃௝ሻ
cosଶ 𝜃௜൅sinଶ 𝜃௜ൌ1
𝜃௜
𝜃௝
𝑖്𝑗
2𝜋𝐸ሾcos 𝜃௜ሿ
ൌ඲
଴
ଶగ
cos 𝑢𝑑𝑢ൌsin 2𝜋െsin 0 ൌ0
2𝜋𝐸ሾsin 𝜃௜ሿ
ൌ඲
଴
ଶగ
sin 𝑢𝑑𝑢ൌcos 0 െcos 2𝜋ൌ0
𝐸ൣ𝐷ଶ൧ൌ𝑛
𝑛
𝑥ଵ, 𝑥ଶ, …, 𝑥௡
𝑛ൌ2
𝑛൐2
𝑥௜
𝑥௜
𝑥௜
𝑥௜
𝑥௜
𝑥௜
5, 9, 3, 10, 11, 14, 8, 4, 17, 6
491 of 848

We start by choosing one of them at random (that is, each value has probability
 of being chosen). Suppose, for instance, that the value 10 is chosen. We then
compare each of the others to this value, putting in a bracket to the left of 10 all
those values smaller than 10 and to the right all those larger. This gives
We now focus on a bracketed set that contains more than a single value say–the
one on the left of the preceding–and randomly choose one of its values–say that
6 is chosen. Comparing each of the values in the bracket with 6 and putting the
smaller ones in a new bracket to the left of 6 and the larger ones in a bracket to
the right of 6 gives
If we now consider the leftmost bracket, and randomly choose the value 4 for
comparison, then the next iteration yields
This continues until there is no bracketed set that contains more than a single
value.
If we let  denote the number of comparisons that it takes the quick-sort
algorithm to sort  distinct numbers, then 
 is a measure of the effectiveness
of this algorithm. To compute 
, we will first express  as a sum of other
random variables as follows. To begin, give the following names to the values
that are to be sorted: Let 1 stand for the smallest, let 2 stand for the next
smallest, and so on. Then, for 
, let 
 equal 1 if  and  are ever
directly compared, and let it equal 0 otherwise. With this definition, it follows that
implying that
1
10
ሼ5,9,3,8,4,6ሽ,10,ሼ11,14,17ሽ
ሼ5,3,4ሽ,6,ሼ9,8ሽ,10,ሼ11,14,17ሽ
ሼ3ሽ,4,ሼ5ሽ,6,ሼ9,8ሽ,10,ሼ11,14,17ሽ
𝑋
𝑛
𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿ
𝑋
1 ൑𝑖൏𝑗൑𝑛
𝐼ሺ𝑖, 𝑗ሻ
𝑖
𝑗
𝑋ൌ
෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
𝐼ሺ𝑖, 𝑗ሻ
492 of 848

To determine the probability that  and  are ever compared, note that the values
 will initially be in the same bracket (since all values are initially
in the same bracket) and will remain in the same bracket if the number chosen
for the first comparison is not between  and . For instance, if the comparison
number is larger than , then all the values 
 will go in a bracket to
the left of the comparison number, and if it is smaller than , then they will all go
in a bracket to the right. Thus all the values 
 will remain in the
same bracket until the first time that one of them is chosen as a comparison
value. At that point all the other values between  and  will be compared with this
comparison value. Now, if this comparison value is neither  nor , then upon
comparison with it,  will go into a left bracket and  into a right bracket, and thus 
and  will be in different brackets and so will never be compared. On the other
hand, if the comparison value of the set 
 is either  or , then
there will be a direct comparison between  and . Now, given that the
comparison value is one of the values between  and , it follows that it is equally
likely to be any of these 
 values, and thus the probability that it is either 
or  is 
 Therefore, we can conclude that
and
To obtain a rough approximation of the magnitude of 
 when  is large, we
can approximate the sums by integrals. Now
𝐸ሾ𝑋ሿ
ൌ
𝐸቎෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
𝐼ሺ𝑖, 𝑗ሻ቏
ൌ
෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
𝐸ሾ𝐼ሺ𝑖, 𝑗ሻሿ
ൌ
෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
𝑃ሼ𝑖 and  𝑗 are ever comparedሽ
𝑖
𝑗
𝑖, 𝑖൅1, …, 𝑗െ1, 𝑗
𝑖
𝑗
𝑗
𝑖, 𝑖൅1, …, 𝑗െ1, 𝑗
𝑖
𝑖, 𝑖൅1, …, 𝑗െ1, 𝑗
𝑖
𝑗
𝑖
𝑗
𝑖
𝑗
𝑖
𝑗
𝑖, 𝑖൅1, …, 𝑗െ1, 𝑗
𝑖
𝑗
𝑖
𝑗
𝑖
𝑗
𝑗െ𝑖൅1
𝑖
𝑗
2/ሺ𝑗െ𝑖൅1ሻ.
𝑃ሼ𝑖 and  𝑗 are ever comparedሽൌ
2
𝑗െ𝑖൅1
𝐸ሾ𝑋ሿൌ
෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
2
𝑗െ𝑖൅1
𝐸ሾ𝑋ሿ
𝑛
493 of 848

Thus
Thus we see that when  is large, the quick-sort algorithm requires, on average,
approximately 
 comparisons to sort  distinct values.
Example 2n The probability of a union of events
Let 
 denote events, and define the indicator variables 
, by
Now, note that
Hence,
෍
௝ൌ௜൅ଵ
௡
2
𝑗െ𝑖൅1 ൎ
඲
௜൅ଵ
௡
2
𝑥െ𝑖൅1 𝑑𝑥
ൌ
2 logሺ𝑥െ𝑖൅1ሻ||
௡
௜൅ଵ
ൌ
2 logሺ𝑛െ𝑖൅1ሻെ2 logሺ2ሻ
ൎ
2 logሺ𝑛െ𝑖൅1ሻ
𝐸ሾ𝑋ሿ
ൎ
෍
௜ൌଵ
௡െଵ
2 logሺ𝑛െ𝑖൅1ሻ
ൎ
2඲
ଵ
௡െଵ
logሺ𝑛െ𝑥൅1ሻ𝑑𝑥
ൌ
2඲
ଶ
௡
logሺ𝑦ሻ𝑑𝑦
ൌ
2ሺ𝑦 logሺ𝑦ሻെ𝑦ሻ||ଶ
௡
ൎ
2𝑛 logሺ𝑛ሻ
𝑛
2𝑛 logሺ𝑛ሻ
𝑛
𝐴ଵ, …𝐴௡
𝑋௜, 𝑖ൌ1, …, 𝑛
𝑋௜ൌቊ
1
if  𝐴௜ occurs
0
otherwise
1 െෑ
௜ൌଵ
௡
ሺ1 െ𝑋௜ሻൌቊ
1
if  ∪𝐴௜ occurs
0
otherwise
𝐸቎1 െෑ
௜ൌଵ
௡
ሺ1 െ𝑋௜ሻ቏ൌ𝑃൬
∪
௜ൌଵ
௡
𝐴௜൰
494 of 848

Expanding the left side of the preceding formula yields
However,
so
Thus, Equation (2.3)
 is just a statement of the well-known inclusion-exclusion
formula for the union of events:
When one is dealing with an infinite collection of random variables 
, each
having a finite expectation, it is not necessarily true that
To determine when (2.4
) is valid, we note that 
 Thus,
𝑃൬
∪
௜ൌଵ
௡
𝐴௜൰ൌ
𝐸቎෍
௜ൌଵ
௡
𝑋௜െ෍෍
௜ழ௝
 𝑋௜𝑋௝൅෍෍෍
௜ழ௝ழ௞
𝑋௜𝑋௝𝑋௞
െ⋯൅ሺെ1ሻ௡൅ଵ𝑋ଵ⋯𝑋௡൧
(2.3)
𝑋௜భ𝑋௜మ⋯𝑋௜ೖൌ൝
1
if  𝐴௜భ𝐴௜మ⋯𝐴௜ೖ occurs
0
otherwise
𝐸ൣ𝑋௜భ⋯𝑋௜ೖ൧ൌ𝑃ሺ𝐴௜భ⋯𝐴௜ೖ൯
𝑃ሺ∪𝐴௜ሻൌ∑௜𝑃ሺ𝐴௜ሻെ∑∑
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻ൅
∑∑∑
௜ழ௝ழ௞
𝑃ሺ𝐴௜𝐴௝𝐴௞ሻ 
                         െ⋯൅ሺെ1ሻ௡൅ଵ𝑃ሺ𝐴ଵ⋯𝐴௡ሻ
𝑋௜, 𝑖൒1
𝐸቎෍
௜ൌଵ
ஶ
𝑋௜቏ൌ෍
௜ൌଵ
ஶ
𝐸ሾ𝑋௜ሿ
(2.4)
෍
௜ൌଵ
ஶ
𝑋௜ൌ
lim
௡→ஶ
෍
௜ൌଵ
௡
𝑋௜.
495 of 848

Hence, Equation (2.4)
 is valid whenever we are justified in interchanging the
expectation and limit operations in Equation (2.5)
. Although, in general, this
interchange is not justified, it can be shown to be valid in two important special
cases:
1. The 
 are all nonnegative random variables. (That is, 
 for all )
2. 
Example 2o
Consider any nonnegative, integer-valued random variable . If, for each 
,
we define
then
Hence, since the 
 are all nonnegative, we obtain
𝐸 ቎෍
௜ൌଵ
ஶ
𝑋௜቏ൌ
𝐸 ቎lim
௡→ஶ෍
௜ൌଵ
௡
𝑋௜቏
ൌ
?
lim
௡→ஶ𝐸 ቎෍
௜ൌଵ
௡
𝑋௜቏
ൌ
lim
௡→ஶ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿ
ൌ
෍
௜ൌଵ
ஶ
𝐸ሾ𝑋௜ሿ
(2.5)
𝑋௜
𝑃ሼ𝑋௜൒0ሽൌ1
𝑖.
෍
௜ൌଵ
ஶ
𝐸ሾ|𝑋௜|ሿ൏∞.
𝑋
𝑖൒1
𝑋௜ൌቊ1
  if  𝑋൒𝑖
0
  if  𝑋൏𝑖
෍
௜ൌଵ
ஶ
𝑋௜ൌ
෍
௜ൌଵ
௑
𝑋௜൅
෍
௜ൌ௑൅ଵ
ஶ
𝑋௜
ൌ
෍
௜ൌଵ
௑
1 ൅
෍
௜ൌ௑൅ଵ
ஶ
0
ൌ
𝑋
𝑋௜
496 of 848

a useful identity.
Example 2p
Suppose that  elements—call them 1, 2, , —must be stored in a computer in
the form of an ordered list. Each unit of time, a request will be made for one of
these elements 
 being requested, independently of the past, with probability
, 
, 
 Assuming that these probabilities are known, what
ordering minimizes the average position in the line of the element requested?
Solution
Suppose that the elements are numbered so that 
. To
show that 1, 2, ,  is the optimal ordering, let  denote the position of the
requested element. Now, under any ordering say, 
,
Summing over  and using Equation (2.6)
 yields
thus showing that ordering the elements in decreasing order of the probability
that they are requested minimizes the expected position of the element
requested.
𝐸ሾ𝑋ሿ
ൌ
෍
௜ൌଵ
ஶ
𝐸ሺ𝑋௜ሻ
ൌ
෍
௜ൌଵ
ஶ
𝑃ሼ𝑋൒𝑖ሽ
(2.6)
𝑛
… 𝑛
—𝑖
𝑃ሺ𝑖ሻ𝑖൒1 ෍
௜
𝑃ሺ𝑖ሻൌ1.
𝑃ሺ1ሻ൒𝑃ሺ2ሻ൒⋯൒𝑃ሺ𝑛ሻ
… 𝑛
𝑋
𝑂ൌ𝑖ଵ, 𝑖ଶ, …, 𝑖௡
𝑃ைሼ𝑋൒𝑘ሽ
ൌ
෍
௝ൌ௞
௡
𝑃ሺ𝑖௝ሻ
൒
෍
௝ൌ௞
௡
𝑃ሺ𝑗ሻ
ൌ
𝑃ଵ,ଶ, . . . , ௡ ሼ𝑋൒𝑘ሽ
𝑘
𝐸௢ሾ𝑋ሿ൒𝐸ଵ,ଶ, . . . , ௡ሾ𝑋ሿ
497 of 848

The probabilistic method is a technique for analyzing the properties of the elements
of a set by introducing probabilities on the set and then studying an element chosen
according to those probabilities. The technique was previously seen in Example 4l of
Chapter 3
, where it was used to show that a set contained an element that
satisfied a certain property. In this subsection, we show how it can sometimes be
used to bound complicated functions.
Let  be a function on the elements of a finite set , and suppose that we are
interested in
A useful lower bound for 
 can often be obtained by letting  be a random element
of  for which the expected value of 
 is computable and then noting that
 implies that
with strict inequality if 
 is not a constant random variable. That is, 
 is a
lower bound on the maximum value.
Example 2q The maximum number of hamiltonian paths in a tournament
A round-robin tournament of 
 contestants is a tournament in which each of
the 
 pairs of contestants play each other exactly once. Suppose that the
players are numbered 
. The permutation 
 is said to be a
Hamiltonian path if  beats 
 beats 
, and 
 beats 
. A problem of
some interest is to determine the largest possible number of Hamiltonian paths.
As an illustration, suppose that there are 3 players. On the one hand, one of
them wins twice, then there is a single Hamiltonian path. (For instance, if 1 wins
twice and 2 beats 3, then the only Hamiltonian path is 1, 2, 3.) On the other hand,
if each of the players wins once, then there are 3 Hamiltonian paths. (For
instance, if 1 beats 2, 2 beats 3, and 3 beats 1, then 1, 2, 3; 2, 3, 1; and 3, 1, 2,
are all Hamiltonians.) Hence, when 
, there is a maximum of 3 Hamiltonian
paths.
We now show that there is an outcome of the tournament that results in more
than 
 Hamiltonian paths. To begin, let the outcome of the tournament
specify the result of each of the 
 games played, and let  denote the set of all
𝑓
𝐴
𝑚ൌmax 
௦∈஺𝑓൬𝑠൰
𝑚
𝑆
𝐴
𝑓ሺ𝑆ሻ
𝑚൒𝑓ሺ𝑆ሻ
𝑚൒𝐸ሾ𝑓ሺ𝑆ሻሿ
𝑓ሺ𝑆ሻ
𝐸ሾ𝑓ሺ𝑆ሻሿ
𝑛൐2
ቆ𝑛
2ቇ
1, 2, 3, …, 𝑛
𝑖ଵ, 𝑖ଶ, …𝑖௡
𝑖ଵ
𝑖ଶ, 𝑖ଶ
𝑖ଷ, …
𝑖௡െଵ
𝑖௡
𝑛ൌ3
𝑛!/2௡െଵ
ቆ𝑛
2ቇ
𝐴
498 of 848

 possible tournament outcomes. Then, with 
 defined as the number of
Hamiltonian paths that result when the outcome is 
, we are asked to show
that
To show this, consider the randomly chosen outcome  that is obtained when the
results of the 
 games are independent, with each contestant being equally
likely to win each encounter. To determine 
, the expected number of
Hamiltonian paths that result from the outcome , number the
permutations, and, for 
 let
Since
it follows that
Because, by the assumed independence of the outcomes of the games, the
probability that any specified permutation is a Hamiltonian is 
, it follows
that
Therefore,
Since 
 is not a constant random variable, the preceding equation implies that
there is an outcome of the tournament having more than 
 Hamiltonian
paths.
Example 2r
2
ቆ௡
ଶቇ
𝑓ሺ𝑠ሻ
𝑠∈𝐴
max 
௦
𝑓ቆ𝑠ቇ൒
𝑛!
2௡െଵ
𝑆
ቆ𝑛
2ቇ
𝐸ሾ𝑓ሺ𝑆ሻሿ
𝑆
𝑛!
𝑖ൌ1, …, 𝑛!,
𝑋௜ൌቊ1
if permutation  𝑖 is  a Hamiltonian
0
otherwise
𝑓ሺ𝑆ሻൌ෍
௜
𝑋௜
𝐸ሾ𝑓ሺ𝑆ሻሿൌ෍
௜
𝐸ሾ𝑋௜ሿ
ሺ1/2ሻ௡െଵ
𝐸ሾ𝑋௜ሿൌ𝑃ሼ𝑋௜ൌ1ሽൌሺ1/2ሻ௡െଵ
𝐸ሾ𝑓ሺ𝑆ሻሿൌ𝑛!ሺ1/2ሻ௡െଵ
𝑓ሺ𝑆ሻ
𝑛!/2௡െଵ
499 of 848

A grove of 52 trees is arranged in a circular fashion. If 15 chipmunks live in these
trees, show that there is a group of 7 consecutive trees that together house at
least 3 chipmunks.
Solution
Let the neighborhood of a tree consist of that tree along with the next six trees
visited by moving in the clockwise direction. We want to show that for any choice
of living accommodations of the 15 chipmunks, there is a tree that has at least 3
chipmunks living in its neighborhood. To show this, choose a tree at random and
let  denote the number of chipmunks that live in its neighborhood. To determine
 arbitrarily number the 15 chipmunks and for 
, let
Because
we obtain that
However, because 
 will equal 1 if the randomly chosen tree is any of the 7
trees consisting of the tree in which chipmunk  lives along with its 6 neighboring
trees when moving in the counterclockwise direction,
Consequently,
showing that there exists a tree with more than 2 chipmunks living in its
neighborhood.
𝑋
𝐸ሾ𝑋ሿ,
𝑖ൌ1, …, 15
𝑋௜ൌቊ1,
 if chipmunk  𝑖 lives in the neighborhood of the randomly chosen tree
0,
otherwise
𝑋ൌ෍
௜ൌଵ
ଵହ
𝑋௜
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
ଵହ
𝐸ሾ𝑋௜ሿ
𝑋௜
𝑖
𝐸ሾ𝑋௜ሿൌ𝑃ሼ𝑋௜ൌ1ሽൌ7
52
𝐸ሾ𝑋ሿൌ105
52 ൐2
*
500 of 848

We start with an identity relating the maximum of a set of numbers to the minimums
of the subsets of these numbers.
Proposition 2.2
For arbitrary numbers 
,
Proof We will give a probabilistic proof of the proposition. To begin, assume that
all the 
 are in the interval [0, 1]. Let  be a uniform (0, 1) random variable, and
define the events 
, by 
. That is, 
 is the event that the
uniform random variable is less than 
. Because at least one of these events 
will occur if  is less than at least one of the values 
, we have that
Therefore,
Also,
In addition, because all of the events 
 will occur if  is less than all the
values 
, we see that the intersection of these events is
implying that
Thus, the proposition follows from the inclusion–exclusion formula for the
probability of the union of events:
𝑥௜, 𝑖ൌ1, …, 𝑛
max
௜
 𝑥௜ൌ෍
௜
𝑥௜െ෍
௜ழ௝
minሺ𝑥௜, 𝑥௝ሻ൅
෍
௜ழ௝ழ௞
minሺ𝑥௜, 𝑥௝, 𝑥௞ሻ
  ൅… ൅ሺെ1ሻ௡൅ଵminሺ𝑥ଵ, …, 𝑥௡ሻ
𝑥௜
𝑈
𝐴௜, 𝑖ൌ1, …, 𝑛
𝐴௜ൌሼ𝑈൏𝑥௜ሽ
𝐴௜
𝑥௜
𝐴௜
𝑈
𝑥௜
∪௜𝐴௜ൌ൜𝑈൏max
௜
 𝑥௜ൠ
𝑃ሺ∪௜𝐴௜ሻൌ𝑃൜𝑈൏max
௜
 𝑥௜ൠൌmax
௜
 𝑥௜
𝑃ሺ𝐴௜ሻൌ𝑃ሼ𝑈൏𝑥௜ሽൌ𝑥௜
𝐴௜భ, …, 𝐴௜ೝ
𝑈
𝑥௜భ, …, 𝑥௜ೝ
𝐴௜భ…𝐴௜ೝൌቊ𝑈൏
min
௝ൌଵ, … ௥𝑥௜ೕቋ
𝑃ሺ𝐴௜భ…𝐴௜ೝሻൌ𝑃ቊ𝑈൏
min
௝ൌଵ, … ௥𝑥௜ೕቋൌ
min
௝ൌଵ, … ௥𝑥௜ೕ
501 of 848

When the 
 are nonnegative, but not restricted to the unit interval, let  be such
that all the 
 are less than . Then the identity holds for the values 
, and
the desired result follows by multiplying through by . When the 
 can be
negative, let  be such that 
 for all . Therefore, by the preceding,
Letting
we can rewrite the foregoing identity as
But
The preceding two equations show that
and the proposition is proven.
It follows from Proposition 2.2
 that for any random variables 
,
Taking expectations of both sides of this equality yields the following relationship
between the expected value of the maximum and those of the partial minimums:
𝑃ሺ∪௜𝐴௜ሻ
ൌ
෍
௜
𝑃ሺ𝐴௜ሻെ෍
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻ൅
෍
௜ழ௝ழ௞
𝑃ሺ𝐴௜𝐴௝𝐴௞ሻ
൅… ൅ሺെ1ሻ௡൅ଵ𝑃ሺ𝐴ଵ…𝐴௡ሻ
𝑥௜
𝑐
𝑥௜
𝑐
𝑦௜ൌ𝑥௜/𝑐
𝑐
𝑥௜
𝑏
𝑥௜൅𝑏൐0
𝑖
max
௜
ሺ𝑥௜൅𝑏ሻൌ
෍
௜
ሺ𝑥௜൅𝑏ሻെ෍
௜ழ௝
minሺ𝑥௜൅𝑏, 𝑥௝൅𝑏ሻ
൅⋯൅ሺെ1ሻ௡൅ଵminሺ𝑥ଵ൅𝑏, …, 𝑥௡൅𝑏ሻ
𝑀ൌ෍
௜
𝑥௜െ෍
௜ழ௝
minሺ𝑥௜, 𝑥௝ሻ൅⋯൅ሺെ1ሻ௡൅ଵminሺ𝑥ଵ, …, 𝑥௡ሻ
max
௜
 𝑥௜൅𝑏ൌ𝑀൅𝑏ቆ𝑛െቆ𝑛
2ቇ൅⋯൅ሺെ1ሻ௡൅ଵቆ𝑛
𝑛ቇቇ
0 ൌሺ1 െ1ሻ௡ൌ1 െ𝑛൅ቆ𝑛
2ቇ൅⋯൅ሺെ1ሻ௡ቆ𝑛
𝑛ቇ
max
௜
  𝑥௜ൌ𝑀
𝑋ଵ, …, 𝑋௡
max
௜
𝑋௜ൌ෍
௜
𝑋௜െ෍
௜ழ௝
minሺ𝑋௜, 𝑋௝ሻ൅⋯൅ሺെ1ሻ௡൅ଵminሺ𝑋ଵ, …, 𝑋௡ሻ
502 of 848

Example 2s Coupon collecting with unequal probabilities
Suppose there are  types of coupons and that each time one collects a coupon,
it is, independently of previous coupons collected, a type  coupon with
probability 
, 
 Find the expected number of coupons one needs to
collect to obtain a complete set of at least one of each type.
Solution
If we let 
 denote the number of coupons one needs to collect to obtain a type ,
then we can express  as
Because each new coupon obtained is a type  with probability 
, 
 is a
geometric random variable with parameter 
. Also, because the minimum of 
and 
 is the number of coupons needed to obtain either a type  or a type , it
follows that for 
, min (
) is a geometric random variable with parameter
. Similarly, min (
), the number needed to obtain any of types , ,
or , is a geometric random variable with parameter 
, and so on.
Therefore, the identity (2.7
) yields
Noting that
and using the identity
𝐸ሾmax
௜
𝑋௜ሿൌ෍
௜
𝐸ሾ𝑋௜ሿെ෍
௜ழ௝
𝐸ሾminሺ𝑋௜, 𝑋௝ሻሿ
  ൅⋯൅ሺെ1ሻ௡൅ଵ𝐸ሾminሺ𝑋ଵ, …, 𝑋௡ሻሿ
(2.7)
𝑛
𝑖
𝑝௜
෍
௜ൌଵ
௡
𝑝௜ൌ1.
𝑋௜
𝑖
𝑋
𝑋ൌ
max
௜ൌଵ, … , ௡𝑋௜
𝑖
𝑝௜𝑋௜
𝑝௜
𝑋௜
𝑋௝
𝑖
𝑗
𝑖്𝑗
𝑋௜, 𝑋௝
𝑝௜൅𝑝௝
𝑋௜, 𝑋௝, 𝑋௞
𝑖𝑗
𝑘
𝑝௜൅𝑝௝൅𝑝௞
𝐸ሾ𝑋ሿൌ෍
௜
1
𝑝௜
െ෍
௜ழ௝
1
𝑝௜൅𝑝௝
൅
෍
௜ழ௝ழ௞
1
𝑝௜൅𝑝௝൅𝑝௞
൅⋯൅ሺെ1ሻ௡൅ଵ
1
𝑝ଵ൅⋯൅𝑝௡
඲
଴
ஶ
𝑒െ௣௫𝑑𝑥ൌ1
𝑝
503 of 848

shows, upon integrating the identity, that
which is a more useful computational form.
Many of the examples solved in the previous section were of the following form: For
given events 
 find 
, where  is the number of these events that occur.
The solution then involved defining an indicator variable  for event 
 such that
Because
we obtained the result
Now suppose we are interested in the number of pairs of events that occur. Because
 will equal  if both 
 and 
 occur and will equal  otherwise, it follows that the
number of pairs is equal to 
 But because  is the number of events that
occur, it also follows that the number of pairs of events that occur is 
.
Consequently,
1 െෑ
௜ൌଵ
௡
ሺ1 െ𝑒െ௣೔௫ሻൌ෍
௜
𝑒െ௣೔௫െ෍
௜ழ௝
𝑒െሺ௣೔൅௣ೕሻ௫൅⋯൅ሺെ1ሻ௡൅ଵ𝑒െሺ௣భ൅⋅⋅⋅൅௣೙൯௫
𝐸ሾ𝑋ሿൌ඲
଴
ஶ
ቌ1 െෑ
௜ൌଵ
௡
ሺ1 െ𝑒െ௣೔௫ሻቍ𝑑𝑥
𝐴ଵ, …, 𝐴௡,
𝐸ሾ𝑋ሿ
𝑋
𝐼௜
𝐴௜
𝐼௜ൌቊ
1,
 if 𝐴௜ occurs
0,
 otherwise
𝑋ൌ෍
௜ൌଵ
௡
𝐼௜
𝐸ሾ𝑋ሿൌ𝐸቎෍
௜ൌଵ
௡
𝐼௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝐼௜ሿൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻ
(3.1)
𝐼௜𝐼௝
1
𝐴௜
𝐴௝
0
෍
௜ழ௝
𝐼௜𝐼௝.
𝑋
ቆ𝑋
2ቇ
504 of 848

where there are 
 terms in the summation. Taking expectations yields
or
giving that
which yields E[X ], and thus 
Moreover, by considering the number of distinct subsets of  events that all occur, we
see that
Taking expectations gives the identity
Example 3a Moments of binomial random variables
Consider n independent trials, with each trial being a success with probability p .
Let A be the event that trial i is a success. When 
Consequently, Equation (3.2
) yields
ቆ𝑋
2ቇൌ෍
௜ழ௝
𝐼௜𝐼௝
ቆ𝑛
2ቇ
𝐸ቈቆ𝑋
2ቇ቉ൌ෍
௜ழ௝
 𝐸ൣ𝐼௜𝐼௝൧ൌ෍
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻ
(3.2)
𝐸቎𝑋ሺ𝑋െ1ሻ
2
቏ൌ෍
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻ
𝐸ሾ𝑋ଶሿെ𝐸ሾ𝑋ሿൌ2 ෍
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻ
(3.3)
2
Var ሺ𝑋൯ൌ𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿ൯
ଶ.
𝑘
ቆ𝑋
𝑘ቇൌ
෍
௜భழ௜మழ… ழ௜ೖ
𝐼௜భ𝐼௜మ⋯𝐼௜ೖ
𝐸቎ቆ𝑋
𝑘ቇ቏ൌ
෍
௜భழ௜మழ… ழ௜ೖ
𝐸ሾ𝐼௜భ𝐼௜మ⋯𝐼௜ೖሿൌ
෍
௜భழ௜మழ… ழ௜ೖ
𝑃ሺ𝐴௜భ𝐴௜మ⋯𝐴௜ೖሻ
(3.4)
i
𝑖ൌ𝑗,  𝑃൫𝐴௜𝐴௝൯ൌ𝑝ଶ.
505 of 848

or
or
Now, 
, so, from the preceding equation
which is in agreement with the result obtained in Section 4.6.1
.
In general, because 
, we obtain from Equation (3.4)
 that
or, equivalently,
The successive values 
, 
, can be recursively obtained from this
identity. For instance, with 
, it yields
or
or
Example 3b Moments of hypergeometric random variables
𝐸቎ቆ𝑋
2ቇ቏ൌ෍
௜ழ௝
𝑝ଶൌቆ𝑛
2ቇ𝑝ଶ
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ𝑛ሺ𝑛െ1ሻ𝑝ଶ
𝐸ൣ𝑋ଶ൧െ𝐸ሾ𝑋ሿൌ𝑛൫𝑛െ1൯𝑝ଶ
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻൌ𝑛𝑝
Var ሺ𝑋൯ൌ𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿ൯
ଶൌ𝑛൫𝑛െ1ሻ𝑝ଶ൅𝑛𝑝െሺ𝑛𝑝൯
ଶൌ𝑛𝑝൫1 െ𝑝ሻ
𝑃ሺ𝐴௜భ𝐴௜మ⋯𝐴௜ೖ൯ൌ𝑝௞
𝐸቎ቆ𝑋
𝑘ቇ቏ൌ
෍
௜భழ௜మழ… ழ௜ೖ
𝑝௞ൌቆ𝑛
𝑘ቇ𝑝௞
𝐸ൣ𝑋൫𝑋െ1൯⋯ሺ𝑋െ𝑘൅1൯൧ൌ𝑛൫𝑛െ1൯⋯ሺ𝑛െ𝑘൅1൯𝑝௞
𝐸ൣ𝑋௞൧𝑘൒3
𝑘ൌ3
𝐸ሾ𝑋ሺ𝑋െ1ሻሺ𝑋െ2ሻሿൌ𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ𝑝ଷ
𝐸ൣ𝑋ଷെ3𝑋ଶ൅2𝑋൧ൌ𝑛൫𝑛െ1൯൫𝑛െ2൯𝑝ଷ
𝐸ൣ𝑋ଷ൧
ൌ
3𝐸ൣ𝑋ଶ൧െ2𝐸ሾ𝑋ሿ൅𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ𝑝ଷ
ൌ
3𝑛ሺ𝑛െ1ሻ𝑝ଶ൅𝑛𝑝൅𝑛ሺ𝑛െ1ሻሺ𝑛െ2ሻ𝑝ଷ
506 of 848

Suppose  balls are randomly selected from an urn containing  balls, of which
 are white. Let 
 be the event that the th ball selected is white. Then 
 the
number of white balls selected, is equal to the number of the events 
 that
occur. Because the th ball selected is equally likely to be any of the  balls, of
which 
 are white, 
 Consequently, Equation (3.1)
 gives that
. Also, since
we obtain, from Equation (3.2)
, that
or
showing that
This formula yields the variance of the hypergeometric, namely,
which agrees with the result obtained in Example 8j
 of Chapter 4
.
Higher moments of  are obtained by using Equation (3.4)
. Because
Equation (3.4)
 yields
𝑛
𝑁
𝑚
𝐴௜
𝑖
𝑋,
𝐴ଵ, …, 𝐴௡
𝑖
𝑁
𝑚
𝑃ሺ𝐴௜ሻൌ𝑚/𝑁.
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻൌ𝑛𝑚/𝑁
𝑃ሺ𝐴௜𝐴௝ሻൌ𝑃ሺ𝐴௜ሻ𝑃ሺ𝐴௝|𝐴௜ሻൌ𝑚
𝑁
𝑚െ1
𝑁െ1
𝐸቎ቆ𝑋
2ቇ቏ൌ෍
௜ழ௝
𝑚ሺ𝑚െ1ሻ
𝑁ሺ𝑁െ1ሻൌቆ𝑛
2ቇ𝑚ሺ𝑚െ1ሻ
𝑁ሺ𝑁െ1ሻ
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ𝑛ሺ𝑛െ1ሻ𝑚ሺ𝑚െ1ሻ
𝑁ሺ𝑁െ1ሻ
𝐸ሾ𝑋ଶሿൌ𝑛ቆ𝑛െ1ቇ𝑚ሺ𝑚െ1ሻ
𝑁ሺ𝑁െ1ሻ൅𝐸ሾ𝑋ሿ
Var ሺ𝑋ሻ
ൌ
𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
ൌ
𝑛ሺ𝑛െ1ሻ𝑚ሺ𝑚െ1ሻ
𝑁ሺ𝑁െ1ሻ൅𝑛𝑚
𝑁െ𝑛ଶ𝑚ଶ
𝑁ଶ
ൌ
𝑚𝑛
𝑁ቈሺ𝑛െ1ሻሺ𝑚െ1ሻ
𝑁െ1
൅1 െ𝑚𝑛
𝑁቉
𝑋
𝑃ሺ𝐴௜భ𝐴௜మ⋯𝐴௜ೖቇൌ𝑚ሺ𝑚െ1ሻ⋯ሺ𝑚െ𝑘൅1ሻ
𝑁ሺ𝑁െ1ሻ⋯ሺ𝑁െ𝑘൅1ሻ
507 of 848

or
Example 3c Moments in the match problem
For 
, let 
 be the event that person  selects his or her own hat in the
match problem. Then
which follows because, conditional on person  selecting her own hat, the hat
selected by person  is equally likely to be any of the other 
 hats, of which
one is his own. Consequently, with  equal to the number of people who select
their own hat, it follows from Equation (3.2)
 that
thus showing that
Therefore, 
 Because 
 we obtain that
Hence, both the mean and variance of the number of matches is . For higher
moments, we use Equation (3.4)
, along with the fact that
 to obtain
or
𝐸ቈቆ𝑋
𝑘ቇ቉ൌቆ𝑛
𝑘ቇ𝑚ሺ𝑚െ1ሻ⋯ሺ𝑚െ𝑘൅1ሻ
𝑁ሺ𝑁െ1ሻ⋯ሺ𝑁െ𝑘൅1ሻ
𝐸ሾ𝑋ሺ𝑋െ1ሻ⋯ሺ𝑋െ𝑘൅1ሻሿ
                     ൌ𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑘൅1ሻ𝑚ሺ𝑚െ1ሻ⋯ሺ𝑚െ𝑘൅1ሻ
𝑁ሺ𝑁െ1ሻ⋯ሺ𝑁െ𝑘൅1ሻ
𝑖ൌ1, …, 𝑁
𝐴௜
𝑖
𝑃ሺ𝐴௜𝐴௝ሻൌ𝑃ሺ𝐴௜ሻ𝑃ሺ𝐴௝|𝐴௜ሻൌ1
𝑁
1
𝑁െ1
𝑖
𝑗
𝑁െ1
𝑋
𝐸቎ቆ𝑋
2ቇ቏ൌ෍
௜ழ௝
1
𝑁ሺ𝑁െ1ሻൌቆ𝑁
2ቇ
1
𝑁ሺ𝑁െ1ሻ
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ1
𝐸ൣ𝑋ଶ൧ൌ1 ൅𝐸ሾ𝑋ሿ.
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
ே
𝑃ሺ𝐴௜ሻ ൌ 1,
Var ሺ𝑋൯ൌ𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶൌ1 .
1
𝑃ሺ𝐴௜భ𝐴௜మ⋯𝐴௜ೖሻൌ
1
𝑁ሺ𝑁െ1ሻ⋯ሺ𝑁െ𝑘൅1ሻ,
𝐸ቈቆ𝑋
 𝑘ቇ቉ൌቆ𝑁
 𝑘ቇ
1
𝑁ሺ𝑁െ1ሻ⋯ሺ𝑁െ𝑘൅1ሻ
508 of 848

Example 3d Another coupon-collecting problem
Suppose that there are  distinct types of coupons and that, independently of
past types collected, each new one obtained is type  with probability 
,
. Find the expected value and variance of the number of different
types of coupons that appear among the first  collected.
Solution
We will find it more convenient to work with the number of uncollected types. So,
let  equal the number of types of coupons collected, and let 
denote
the number of uncollected types. With 
 defined as the event that there are no
type  coupons in the collection,  is equal to the number of the events 
that occur. Because the types of the successive coupons collected are
independent, and, with probability 
 each new coupon is not type , we have
Hence, 
, from which it follows that
Similarly, because each of the  coupons collected is neither a type  nor a type 
coupon, with probability 
, we have
Thus,
or
Hence, we obtain
𝐸ሾ𝑋ሺ𝑋െ1ሻ⋯ሺ𝑋െ𝑘൅1ሻሿൌ1
𝑁
𝑗
𝑝௝
෍
௝ൌଵ
ே
𝑝௝ൌ1
𝑛
𝑌
𝑋ൌ𝑁െ𝑌
𝐴௜
𝑖
𝑋
𝐴ଵ, …, 𝐴ே
1 െ𝑝௜
𝑖
𝑃ሺ𝐴௜ሻൌሺ1 െ𝑝௜ሻ௡
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
ே
ሺ1 െ𝑝௜ሻ௡
𝐸ሾ𝑌ሿൌ𝑁െ𝐸ሾ𝑋ሿൌ𝑁െ෍
௜ൌଵ
ே
ሺ1 െ𝑝௜ሻ௡
𝑛
𝑖
𝑗
1 െ𝑝௜െ𝑝௝
𝑃ሺ𝐴௜𝐴௝ሻൌሺ1 െ𝑝௜െ𝑝௝ሻ௡,  𝑖്𝑗
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ2 ෍
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻൌ2 ෍
௜ழ௝
ሺ1 െ𝑝௜െ𝑝௝ሻ௡
𝐸ሾ𝑋ଶሿൌ2 ෍
௜ழ௝
ሺ1 െ𝑝௜െ𝑝௝ሻ௡൅𝐸ሾ𝑋ሿ
509 of 848

In the special case where 
, 
, the preceding formulas give
and
Example 3e The negative hypergeometric random variables
Suppose an urn contains 
 balls, of which  are special and 
 are ordinary.
These items are removed one at a time, with each new removal being equally
likely to be any of the balls that remain in the urn. The random variable , equal
to the number of balls that need be withdrawn until a total of  special balls have
been removed, is said to have a negative hypergeometric distribution. The
negative hypergeometric distribution bears the same relationship to the
hypergeometric distribution as the negative binomial does to the binomial. That
is, in both cases, rather than considering a random variable equal to the number
of successes in a fixed number of trials (as are the binomial and hypergeometric
variables), they refer to the number of trials needed to obtain a fixed number of
successes.
To obtain the probability mass function of a negative hypergeometric random
variable , note that  will equal  if both
1. the first 
 withdrawals consist of 
 special and 
 ordinary balls
and
2. the th ball withdrawn is special.
Consequently,
Var ሺ𝑌ሻ
ൌVar ሺ𝑋ሻ
ൌ𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
ൌ2 ෍
௜ழ௝
ሺ1 െ𝑝௜െ𝑝௝ሻ௡൅෍
௜ൌଵ
ே
ሺ1 െ𝑝௜ሻ௡െቌ෍
௜ൌଵ
ே
ሺ1 െ𝑝௜ሻ௡ቍ
ଶ
𝑝௜ൌ1/𝑁𝑖ൌ1, …, 𝑁
𝐸ሾ𝑌ሿൌ𝑁൥1 െቆ1 െ1
𝑁ቇ
௡
൩
Var ሺ𝑌ሻൌ𝑁ሺ𝑁െ1ሻቆ1 െ2
𝑁ቇ
௡
൅𝑁ቆ1 െ1
𝑁ቇ
௡
െ𝑁ଶቆ1 െ1
𝑁ቇ
ଶ௡
𝑛൅𝑚
𝑛
𝑚
𝑌
𝑟
𝑌
𝑌
𝑘
𝑘െ1
𝑟െ1
𝑘െ𝑟
𝑘
510 of 848

We will not, however, utilize the preceding probability mass function to obtain the
mean and variance of . Rather, let us number the 
 ordinary balls as 
,
and then, for each 
 let 
 be the event that 
 is withdrawn before 
special balls have been removed. Then, if  is the number of the events 
that occur, it follows that  is the number of ordinary balls that are withdrawn
before a total of  special balls have been removed. Consequently,
showing that
To determine 
 consider the 
 balls consisting of 
 along with the 
special balls. Of these 
 balls, 
 is equally likely to be the first one
withdrawn, or the second one withdrawn, 
 or the final one withdrawn. Hence,
the probability that it is among the first  of these to be selected (and so is
removed before a total or  special balls have been withdrawn) is 
.
Consequently,
and
Thus, for instance, the expected number of cards of a well-shuffled deck that
would need to be turned over until a spade appears is 
 and the
expected number of cards that would need to be turned over until an ace
appears is 
.
To determine 
, we use the identity
𝑃ሼ𝑌ൌ𝑘ሽൌ
ቆ   𝑛
𝑟െ1ቇቆ   𝑚
𝑘െ𝑟ቇ
ቆ𝑛൅𝑚
𝑘െ1 ቇ
𝑛െ𝑟൅1
𝑛൅𝑚െ𝑘൅1
𝑌
𝑚
𝑜ଵ, …, 𝑜௠
𝑖ൌ1, …, 𝑛,
𝐴௜
𝑜௜
𝑟
𝑋
𝐴ଵ, …, 𝐴௠
𝑋
𝑟
𝑌ൌ𝑟൅𝑋
𝐸ሾ𝑌ሿൌ𝑟൅𝐸ሾ𝑋ሿൌ𝑟൅෍
௜ൌଵ
௠
𝑃ሺ𝐴௜ሻ
𝑃ሺ𝐴௜ሻ,
𝑛൅1
𝑜௜
𝑛
𝑛൅1
𝑜௜
…,
𝑟
𝑟
𝑟
𝑛൅1
𝑃ሺ𝐴௜ሻൌ
𝑟
𝑛൅1
𝐸ሾ𝑌ሿൌ𝑟൅𝑚
𝑟
𝑛൅1 ൌ𝑟ሺ𝑛൅𝑚൅1ሻ
𝑛൅1
1 ൅39
14 ൌ3 . 786,
1 ൅48
5 ൌ10 . 6
Varሺ𝑌ሻൌVarሺ𝑋ሻ
511 of 848

Now, 
 is the probability that both 
 and 
 are removed before there have
been a total of  special balls removed. So consider the 
 balls consisting of
, and the  special balls. Because all withdrawal orderings of these balls are
equally likely, the probability that 
 and 
 are both among the first 
 of them
to be removed (and so are both removed before  special balls have been
withdrawn) is
Consequently,
so
Because 
, this yields
A little algebra now shows that
Example 3f Singletons in the coupon collector’s problem
Suppose that there are  distinct types of coupons and that, independently of
past types collected, each new one obtained is equally likely to be any of the 
types. Suppose also that one continues to collect coupons until a complete set of
at least one of each type has been obtained. Find the expected value and
variance of the number of types for which exactly one coupon of that type is
collected.
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ2 ෍
௜ழ௝
𝑃ሺ𝐴௜𝐴௝ሻ
𝑃ሺ𝐴௜𝐴௝൯
𝑜௜
𝑜௝
𝑟
𝑛൅2
𝑜௜, 𝑜௝
𝑛
𝑜௜
𝑜௝
𝑟൅1
𝑟
𝑃൫𝐴௜𝐴௝൯ൌ
ቆ2
2ቇቆ   𝑛
𝑟െ1ቇ
ቆ𝑛൅2
𝑟൅1ቇ
ൌ
𝑟ሺ𝑟൅1ሻ
ሺ𝑛൅1ሻሺ𝑛൅2ሻ
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ2ቆ𝑚
2ቇ
𝑟ሺ𝑟൅1ሻ
ሺ𝑛൅1ሻሺ𝑛൅2ሻ
𝐸ሾ𝑋ଶሿൌ𝑚ሺ𝑚െ1ሻ
𝑟ሺ𝑟൅1ሻ
ሺ𝑛൅1ሻሺ𝑛൅2ሻ൅𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿൌ𝑚
𝑟
𝑛൅1
VarሺYሻൌVarሺXሻൌ𝑚ሺ𝑚െ1ሻ
𝑟ሺ𝑟൅1ሻ
ሺ𝑛൅1ሻሺ𝑛൅2ሻ൅𝑚
𝑟
𝑛൅1 െሺ𝑚
𝑟
𝑛൅1ሻ
ଶ
VarሺYሻൌ𝑚𝑟ሺ𝑛൅1 െ𝑟ሻሺ𝑛൅𝑚൅1ሻ
ሺ𝑛൅1ሻଶሺ𝑛൅2ሻ
𝑛
𝑛
512 of 848

Solution
Let  equal the number of types for which exactly one of that type is collected.
Also, let 
 denote the th type of coupon to be collected, and let 
 be the event
that there is only a single type 
 coupon in the complete set. Because  is equal
to the number of the events 
 that occur, we have
Now, at the moment when the first type 
 coupon is collected, there remain 
types that need to be collected to have a complete set. Because, starting at this
moment, each of these 
 types (the n – i not yet collected and type T) is
equally likely to be the last of these types to be collected, it follows that the type
T will be the last of these types (and so will be a singleton) with probability
 Consequently, 
 yielding
To determine the variance of the number of singletons, let 
, for 
, be the
event that the first type 
 coupon to be collected is still the only one of its type to
have been collected at the moment that the first type 
 coupon has been
collected. Then
Now, 
 is the probability that when a type 
 has just been collected, of the
 types consisting of type 
 and the 
 as yet uncollected types, a type
 is not among the first 
 of these types to be collected. Because type 
 is
equally likely to be the first, or second, or 
 of these types to be
collected, we have
Now, conditional on the event 
, both 
 and 
 will occur if, at the time the first
type 
 coupon is collected, of the 
 types consisting of types 
, and
the 
 as yet uncollected types, 
 and 
 are both collected after the other
. But this implies that
𝑋
𝑇௜
𝑖
𝐴௜
𝑇௜
𝑋
𝐴ଵ, …, 𝐴௡
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻ
𝑇௜
𝑛െ𝑖
𝑛െ𝑖൅1
i
i
1
𝑛െ𝑖൅1 .
𝑃ሺ𝐴௜ሻൌ
1
𝑛െ𝑖൅1,
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
1
𝑛െ𝑖൅1 ൌ෍
௜ൌଵ
௡
1
𝑖
𝑆௜, ௝
𝑖൏𝑗
𝑇௜
𝑇௝
𝑃ሺ𝐴௜𝐴௝൯ൌ𝑃ሺ𝐴௜𝐴௝|𝑆௜,௝൯𝑃ሺ𝑆௜,௝൯
𝑃ሺ𝑆௜,௝൯
𝑇௜
𝑛െ𝑖൅1
𝑇௜
𝑛െ𝑖
𝑇௜
𝑗െ𝑖
𝑇௜
…, 𝑛െ𝑖൅1
𝑃ሺ𝑆௜,௝ሻൌ1 െ
𝑗െ𝑖
𝑛െ𝑖൅1 ൌ𝑛൅1 െ𝑗
𝑛൅1 െ𝑖
𝑆௜,௝
𝐴௜
𝐴௝
𝑇௝
𝑛െ𝑗൅2
𝑇௜, 𝑇௝
𝑛െ𝑗
𝑇௜
𝑇௝
𝑛െ𝑗
513 of 848

Therefore,
yielding
Consequently, using the previous result for 
, we obtain
The following proposition shows that the expectation of a product of independent
random variables is equal to the product of their expectations.
Proposition 4.1
If  and  are independent, then, for any functions  and ,
Proof Suppose that  and  are jointly continuous with joint density 
. Then
𝑃ሺ𝐴௜𝐴௝|𝑆௜,௝ሻൌ2
1
𝑛െ𝑗൅2
1
𝑛െ𝑗൅1
𝑃ሺ𝐴௜𝐴௝ሻൌ
2
ሺ𝑛൅1 െ𝑖ሻሺ𝑛൅2 െ𝑗ሻ,  𝑖൏𝑗
𝐸ሾ𝑋ሺ𝑋െ1ሻሿൌ4 ෍
௜ழ௝
1
ሺ𝑛൅1 െ𝑖ሻሺ𝑛൅2 െ𝑗ሻ
𝐸ሾ𝑋ሿ
Var ሺ𝑋ሻൌ4 ෍
௜ழ௝
1
ሺ𝑛൅1 െ𝑖ሻሺ𝑛൅2 െ𝑗ሻ൅෍
௜ൌଵ
௡
1
𝑖െቌ෍
௜ൌଵ
௡
1
𝑖ቍ
ଶ
𝑋
𝑌
ℎ
𝑔
𝐸ሾ𝑔ሺ𝑋ሻℎሺ𝑌ሻሿൌ𝐸ሾ𝑔ሺ𝑋ሻሿ𝐸ሾℎሺ𝑌ሻሿ
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻ
514 of 848

The proof in the discrete case is similar.
Just as the expected value and the variance of a single random variable give us
information about that random variable, so does the covariance between two random
variables give us information about the relationship between the random variables.
Definition
The covariance between  and , denoted by Cov (
), is defined by
Upon expanding the right side of the preceding definition, we see that
Note that if  and  are independent, then, by Proposition 4.1
, Cov 
.
However, the converse is not true. A simple example of two dependent random
variables  and  having zero covariance is obtained by letting  be a random
variable such that
and defining
Now,
, so 
. Also,
. Thus,
𝐸ሾ𝑔ሺ𝑥ሻℎሺ𝑌ሻሿ
ൌ
඲
െஶ
ஶ
඲
െஶ
ஶ
𝑔ሺ𝑥ሻℎሺ𝑦ሻ𝑓ሺ𝑥, 𝑦ሻ𝑑𝑥𝑑𝑦
ൌ
඲
െஶ
ஶ
඲
െஶ
ஶ
𝑔ሺ𝑥ሻℎሺ𝑦ሻ𝑓௑ሺ𝑥ሻ𝑓௒ሺ𝑦ሻ𝑑𝑥𝑑𝑦
ൌ
඲
െஶ
ஶ
ℎሺ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦඲
െஶ
ஶ
𝑔ሺ𝑥ሻ𝑓௑ሺ𝑥ሻ𝑑𝑥
ൌ
𝐸ሾℎሺ𝑌ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ
𝑋
𝑌
𝑋, 𝑌
Covሺ𝑋, 𝑌ሻൌ𝐸ሾሺ𝑋െ𝐸ሾ𝑋ሿሻሺ𝑌െ𝐸ሾ𝑌ሿሻሿ
Cov ሺ𝑋, 𝑌ሻൌ
𝐸ሾ𝑋𝑌െ𝐸ሾ𝑋ሿ𝑌െ𝑋𝐸ሾ𝑌ሿ൅𝐸ሾ𝑌ሿ𝐸ሾ𝑋ሿሿ
ൌ
𝐸ሾ𝑋𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿ൅𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿ
ൌ
𝐸ሾ𝑋𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿ
𝑋
𝑌
ሺ𝑋, 𝑌ሻൌ0
𝑋
𝑌
𝑋
𝑃ሼ𝑋ൌ0ሽൌ𝑃ሼ𝑋ൌ1ሽൌ𝑃ሼ𝑋ൌെ1ሽൌ1
3
𝑌ൌቊ0
  if  𝑋്0
1
  if  𝑋ൌ0
𝑋𝑌ൌ0
𝐸ሾ𝑋𝑌ሿൌ0
𝐸ሾ𝑋ሿൌ0
Cov ሺ𝑋, 𝑌ሻൌ𝐸ሾ𝑋𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿൌ0
515 of 848

However,  and  are clearly not independent.
The following proposition lists some of the properties of covariance.
Proposition 4.2
i. 
ii. 
iii. 
iv. 
Proof of Proposition 4.2 Parts (i) and (ii) follow immediately from the definition
of covariance, and part (iii) is left as an exercise for the reader. To prove part (iv),
which states that the covariance operation is additive (as is the operation of
taking expectations), let 
 and 
. Then
and
where the last equality follows because the expected value of a sum of random
variables is equal to the sum of the expected values.
It follows from parts (ii) and (iv) of Proposition 4.2
, upon taking
, that
𝑋
𝑌
Cov ሺ𝑋, 𝑌ሻൌCov ሺ𝑌, 𝑋ሻ
Cov ሺ𝑋, 𝑋ሻൌCovሺ𝑋ሻ
Cov ሺ𝑎𝑋, 𝑌ሻൌ𝑎 Cov ሺ𝑋, 𝑌ሻ
Cov  ቌ෍
௜ൌଵ
௡
𝑋௜,   ෍
௝ൌଵ
௠
𝑌௝ቍൌ෍
௜ൌଵ
௡
෍
௝ൌଵ
௠
 Cov ሺ𝑋௜, 𝑌௝ሻ
𝜇௜ൌ𝐸ൣ𝑋௜൧
𝑣௝ൌ𝐸ൣ𝑌௝൧
𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝜇௜,  𝐸቎෍
௝ൌଵ
௠
𝑌௝቏ൌ
෍
௝ൌଵ
௠
𝑣௝
Cov ቌ෍
௜ൌଵ
௡
 𝑋௜,   ෍
௝ൌଵ
௠
 𝑌௝ቍൌ
𝐸 ቎ቌ෍
௜ൌଵ
௡
 𝑋௜െ෍
௜ൌଵ
௡
 𝜇௜ቍቌ෍
௝ൌଵ
௠
 𝑌௝െ෍
௝ൌଵ
௠
 𝑣௝ቍ቏
ൌ
𝐸 ቎෍
௜ൌଵ
௡
 ൫𝑋௜െ𝜇௜൯െ෍
௝ൌଵ
௠
 ሺ𝑌௝െ𝑣௝ሻ቏
ൌ
𝐸 ቎෍
௜ൌଵ
௡
෍
௝ൌଵ
௠
൫𝑋௜െ𝜇௜൯൫𝑌௝െ𝑣௝൯቏
ൌ
෍
௜ൌଵ
௡
෍
௝ൌଵ
௠
𝐸 ൣሺ𝑋௜െ𝜇௜൯ሺ𝑌௝െ𝑣௝ሻ൧
𝑌௝ൌ𝑋௝, 𝑗ൌ1, …, 𝑛
516 of 848

Since each pair of indices 
, appears twice in the double summation, the
preceding formula is equivalent to
If 
 are pairwise independent, in that 
 and 
 are independent for 
,
then Equation (4.1)
 reduces to
The following examples illustrate the use of Equation (4.1)
.
Example 4a
Let 
 be independent and identically distributed random variables having
expected value  and variance 
, and as in Example 2c
, let 
be the sample mean. The quantities 
 are called deviations, as
they equal the differences between the individual data and the sample mean.
The random variable
Var ቌ෍
௜ൌଵ
௡
𝑋௜ቍൌ
Cov ቌ෍
௜ൌଵ
௡
𝑋௜,   ෍
௝ൌଵ
௡
𝑋௝ቍ
ൌ
෍
௜ൌଵ
௡
෍
௝ൌଵ
௡
 Cov ሺ𝑋௜, 𝑋௝ሻ
ൌ
෍
௜ൌଵ
௡
ቌCov ሺ𝑋௜, 𝑋௜ሻ൅
෍
௝: ௝ஷ௜
Cov ሺ𝑋௜, 𝑋௝ሻቍ
ൌ
෍
௜ൌଵ
௡
Var ሺ𝑋௜ሻ൅෍෍
௜ஷ௝
 Cov ሺ𝑋௜, 𝑋௝ሻ
𝑖, 𝑗, 𝑖്𝑗
Var ቌ෍
௜ൌଵ
௡
𝑋௜ቍൌ෍
௜ൌଵ
௡
Var ሺ𝑋௜ሻ൅2෍෍
௜ழ௝
Covሺ𝑋௜, 𝑋௝ሻ
(4.1)
𝑋ଵ, …, 𝑋௡
𝑋௜
𝑋௝
𝑖്𝑗
Var ቌ෍
௜ൌଵ
௡
𝑋௜ቍൌ෍
௜ൌଵ
௡
Var ሺ𝑋௜ሻ
𝑋ଵ, …, 𝑋௡
𝜇
𝜎ଶ
𝑋̅ ̅̅ ̅ൌ෍
௜ൌଵ
௡
𝑋௜/𝑛
𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛,
𝑆ଶൌ෍
௜ൌଵ
௡
ሺ𝑋௜െ𝑋̅ ̅̅ ̅ሻ
ଶ
𝑛െ1
517 of 848

is called the sample variance. Find (a) Var( ) and (b)
.
Solution
a. 
b. We start with the following algebraic identity:
Taking expectations of the preceding yields
where the final equality made use of part (a) of this example and the one
preceding it made use of the result of Example 2c
, namely, that 
.
Dividing through by 
 shows that the expected value of the sample variance
is the distribution variance 
.
Our next example presents another method for obtaining the variance of a binomial
random variable.
𝑋̅ ̅̅ ̅
𝐸ൣ𝑆ଶ൧
Var ሺ𝑋̅ ̅̅ ̅ሻ
ൌ
ቆ1
𝑛ቇ
ଶ
 Var ቌ෍
௜ൌଵ
௡
𝑋௜ቍ
ൌ
ቆ1
𝑛ቇ
ଶ
෍
௜ൌଵ
௡
 Varሺ𝑋௜ሻ    by independence
ൌ
𝜎ଶ
𝑛
ሺ𝑛െ1ሻ𝑆ଶ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇൅𝜇െ𝑋̅ ̅̅ ̅ሻ
ଶ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇ሻଶ൅෍
௜ൌଵ
௡
ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ
ଶെ2ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇ሻ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇ሻଶ൅𝑛ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ
ଶെ2ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ𝑛ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇ሻଶെ𝑛ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ
ଶ
ሺ𝑛െ1ሻ𝐸ሾ𝑆ଶሿൌ
෍
௜ൌଵ
௡
𝐸ሾሺ𝑋௜െ𝜇ሻଶሿെ𝑛𝐸ሾሺ𝑋̅ ̅̅ ̅െ𝜇ሻଶሿ
ൌ
𝑛𝜎ଶെ𝑛Var ሺ𝑋̅ ̅̅ ̅ሻ
ൌ
ሺ𝑛െ1ሻ𝜎ଶ
𝐸ሾ𝑋̅ ̅̅ ̅ሿൌ𝜇
𝑛െ1
𝜎ଶ
518 of 848

Example 4b Variance of a binomial random variable
Compute the variance of a binomial random variable  with parameters  and .
Solution
Since such a random variable represents the number of successes in 
independent trials when each trial has the common probability  of being a
success, we may write
where the 
 are independent Bernoulli random variables such that
Hence, from Equation (4.1)
, we obtain
But
Thus,
Example 4c Sampling from a finite population
Consider a set of  people, each of whom has an opinion about a certain subject
that is measured by a real number  that represents the person’s “strength of
feeling” about the subject. Let 
 represent the strength of feeling of person
.
Suppose that the quantities 
, are unknown and, to gather
information, a group of  of the  people is “randomly chosen” in the sense that
all of the 
 subsets of size  are equally likely to be chosen. These  people
are then questioned and their feelings determined. If  denotes the sum of the 
sampled values, determine its mean and variance.
𝑋
𝑛
𝑝
𝑛
𝑝
𝑋ൌ𝑋ଵ൅⋯൅𝑋௡
𝑋௜
𝑋௜ൌቊ1
if the  𝑖th trial is a success
0
otherwise
Var ሺ𝑋ሻൌVar ሺ𝑋ଵሻ൅⋯൅Var ሺ𝑋௡ሻ
Var ሺ𝑋௜ሻൌ
𝐸ൣ𝑋ଶ
௜൧െ൫𝐸ൣ𝑋௜൧൯
ଶ
ൌ
𝐸ൣ𝑋௜൧െ൫𝐸ൣ𝑋௜൧൯
ଶ  since  𝑋ଶ
௜ൌ𝑋௜
ൌ
𝑝െ𝑝ଶ
Var ሺ𝑋ሻൌ𝑛𝑝ሺ1 െ𝑝ሻ
𝑁
𝑣
𝑣௜
𝑖, 𝑖ൌ1, …𝑁
𝑣௜, 𝑖ൌ1, …, 𝑁
𝑛
𝑁
ቆ𝑁
2 ቇ
𝑛
𝑛
𝑆
𝑛
519 of 848

An important application of the preceding problem is to a forthcoming election in
which each person in the population is either for or against a certain candidate or
proposition. If we take 
 to equal 1 if person  is in favor and 0 if he or she is
against, then 
 represents the proportion of the population that is in
favor. To estimate , a random sample of  people is chosen, and these people
are polled. The proportion of those polled who are in favor – that is, S/n – is often
used as an estimate of .
Solution
For each person 
, define an indicator variable  to indicate whether or
not that person is included in the sample. That is,
Now,  can be expressed by
so
Because
it follows that
𝑣௜
𝑖
𝑣̅ ̅ ൌ෍
௜ൌଵ
ே
𝑣௜/𝑁
𝑣̅ ̅
𝑛
𝑣̅ ̅
𝑖, 𝑖ൌ1, …, 𝑁
𝐼௜
𝐼௜ൌቊ1
 if person  𝑖is in the random sample
0
otherwise
𝑆
𝑆ൌ෍
௜ൌଵ
ே
𝑣௜𝐼௜
𝐸ሾ𝑆ሿൌ෍
௜ൌଵ
ே
𝑣௜𝐸ሾ𝐼௜ሿ
Var ሺ𝑆ሻൌ
෍
௜ൌଵ
ே
Var ሺ𝑣௜𝐼௜ሻ൅2 ∑∑
௜ழ௝
 Cov ሺ𝑣௜𝐼௜, 𝑣௝𝐼௝ሻ
ൌ
෍
௜ൌଵ
ே
𝑣ଶ
௜ Var ሺ𝐼௜ሻ൅2 ∑∑
௜ழ௝
𝑣௜𝑣௝ Cov ሺ𝐼௜, 𝐼௝ሻ
𝐸ሾ𝐼௜ሿ
ൌ
𝑛
𝑁
𝐸ൣ𝐼௜𝐼௝൧
ൌ
𝑛
𝑁
𝑛െ1
𝑁െ1
520 of 848

Hence,
The expression for Var 
 can be simplified somewhat by using the identity
 After some simplification, we obtain
Consider now the special case in which Np of the ’s are equal to 1 and the
remainder equal to 0. Then, in this case,  is a hypergeometric random variable
and has mean and variance given, respectively, by
and
The quantity S/n, equal to the proportion of those sampled that have values
equal to 1, is such that
Var ሺ𝐼௜ሻൌ
𝑛
𝑁൬1 െ𝑛
𝑁൰
Cov ሺ𝐼௜, 𝐼௝൯
ൌ
𝑛ሺ𝑛െ1ሻ
𝑁ሺ𝑁െ1ሻെ൬𝑛
𝑁൰
ଶ
ൌ
െ𝑛ሺ𝑁െ𝑛ሻ
𝑁ଶሺ𝑁െ1൯
𝐸ሾ𝑆ሿ
ൌ
𝑛෍
௜ൌଵ
ே
𝑣௜
𝑁ൌ𝑛𝑣̅ ̅
Var ሺ𝑆ሻ
ൌ
𝑛
𝑁ቆ𝑁െ𝑛
𝑁
ቇ෍
௜ൌଵ
ே
𝑣ଶ
௜െ2𝑛ሺ𝑁െ𝑛ሻ
𝑁ଶሺ𝑁െ1ሻ  ෍෍
௜ழ௝
 𝑣௜𝑣௝
ሺ𝑆ሻ
ሺ𝑣ଵ൅⋯൅𝑣ேሻଶൌ෍
௜ൌଵ
ே
𝑣௜
ଶ൅2 ∑∑
௜ழ௝
 𝑣௜𝑣௝.
Var ሺ𝑆ሻൌ𝑛ሺ𝑁െ𝑛ሻ
𝑁െ1
 
⎛
⎝
⎜
⎜⎜
⎜
෍
௜ൌଵ
ே
𝑣௜
ଶ
𝑁
െ𝑣̅ ̅ଶ
⎞
⎠
⎟
⎟⎟
⎟
𝑣
𝑆
𝐸ሾ𝑆ሿൌ𝑛𝑣̅ ̅ ൌ𝑛𝑝   since  𝑣̅ ̅ ൌ𝑁𝑝
𝑁ൌ𝑝
Var ሺ𝑆ሻ
ൌ
𝑛ሺ𝑁െ𝑛ሻ
𝑁െ1
ቆ𝑁𝑝
𝑁െ𝑝ଶቇ
ൌ
𝑛ሺ𝑁െ𝑛ሻ
𝑁െ1
𝑝ሺ1 െ𝑝ሻ
521 of 848

The correlation of two random variables  and , denoted by 
, is defined,
as long as Var 
Var 
 is positive, by
It can be shown that
To prove Equation (4.2)
, suppose that and  have variances given by 
 and
, respectively. Then, on the one hand,
implying that
On the other hand,
implying that
which completes the proof of Equation (4.2)
.
𝐸⎡
⎣
𝑆
𝑛
⎤
⎦
ൌ
𝑝
Var ቆ𝑆
𝑛ቇ
ൌ
𝑁െ𝑛
𝑛ሺ𝑁െ1ሻ𝑝ሺ1 െ𝑝ሻ
𝑋
𝑌
𝜌ሺ𝑋, 𝑌ሻ
ሺ𝑋ሻ
ሺ𝑌ሻ
𝜌ሺ𝑋, 𝑌ሻൌ
Cov ሺ𝑋, 𝑌ሻ
Var ሺ𝑋ሻVar ሺ𝑌ሻ
ඥ
െ1 ൑𝜌ሺ𝑋, 𝑌ሻ൑1
(4.2)
𝑌
𝜎ଶ
௫
𝜎ଶ
௬
0
൑
Var ቆ𝑋
𝜎௫
൅𝑌
𝜎௬
ቇ
ൌ
Var ሺ𝑋ሻ
𝜎ଶ௫
൅Var ሺ𝑌ሻ
𝜎ଶ௬
൅2 Cov ሺ𝑋, 𝑌ሻ
𝜎௫𝜎௬
ൌ
2ሾ1 ൅𝜌ሺ𝑋, 𝑌ሻሿ
െ1 ൑𝜌ሺ𝑋, 𝑌ሻ
0
൑
Var ቆ𝑋
𝜎௫
െ𝑌
𝜎௬
ቇ
ൌ
Var ሺ𝑋ሻ
𝜎ଶ௫
൅Var ሺ𝑌ሻ
ሺെ𝜎௬ሻଶെ2 Cov ሺ𝑋, 𝑌ሻ
𝜎௫𝜎௬
ൌ
2ሾ1 ൅𝜌ሺ𝑋, 𝑌ሻሿ
𝜌ሺ𝑋, 𝑌ሻ൑1
522 of 848

In fact, since 
 implies that  is constant with probability 1 (this intuitive
relationship will be rigorously proven in Chapter 8
), it follows from the proof of
Equation (4.2)
 that 
 implies that 
, where 
and 
 implies that 
, where 
. We leave it
as an exercise for the reader to show that the reverse is also true: that if
, then 
 is either 
 or 
, depending on the sign of .
The correlation coefficient is a measure of the degree of linearity between  and . A
value of 
 near 
 or 
 indicates a high degree of linearity between  and ,
whereas a value near 0 indicates that such linearity is absent. A positive value of
 indicates that  tends to increase when  does, whereas a negative value
indicates that  tends to decrease when  increases. If 
, then  and  are
said to be uncorrelated.
Example 4d
Let 
 and 
 be indicator variables for the events  and . That is,
Then
so
Thus, we obtain the quite intuitive result that the indicator variables for  and 
are either positively correlated, uncorrelated, or negatively correlated, depending
on whether 
 is, respectively, greater than, equal to, or less than 
.
Our next example shows that the sample mean and a deviation from the sample
mean are uncorrelated.
Example 4e
Let 
 be independent and identically distributed random variables having
Var ሺ𝑍ሻൌ0
𝑍
𝜌ሺ𝑋, 𝑌ሻൌ1
𝑌ൌ𝑎൅𝑏𝑋
𝑏ൌ𝜎௬/𝜎௫൐0
𝜌ሺ𝑋, 𝑌ሻൌെ1
𝑌ൌ𝑎൅𝑏𝑋
𝑏ൌെ𝜎௬/𝜎௫൏0
𝑌ൌ𝑎൅𝑏𝑋
𝜌ሺ𝑋, 𝑌ሻ
൅1
െ1
𝑏
𝑋
𝑌
𝜌ሺ𝑋, 𝑌ሻ
൅1
െ1
𝑋
𝑌
𝜌ሺ𝑋, 𝑌ሻ
𝑌
𝑋
𝑌
𝑋
𝜌ሺ𝑋, 𝑌ሻൌ0
𝑋
𝑌
𝐼஺
𝐼஻
𝐴
𝐵
𝐼஺ൌቊ1
  if  𝐴 occurs
0
 otherwise
𝐼஻ൌቊ1
  if  𝐵 occurs
0
 otherwise
𝐸ሾ𝐼஺ሿൌ𝑃ሺ𝐴ሻ
𝐸ሾ𝐼஻ሿൌ𝑃ሺ𝐵ሻ
𝐸ሾ𝐼஺𝐼஻ሿൌ𝑃ሺ𝐴𝐵ሻ
Cov ሺ𝐼஺, 𝐼஻ሻൌ
𝑃ሺ𝐴𝐵ሻെ𝑃ሺ𝐴ሻ𝑃ሺ𝐵ሻ
ൌ
𝑃ሺ𝐵ሻሾ𝑃ሺ𝐴||𝐵ሻെ𝑃ሺ𝐴ሻሿ
𝐴
𝐵
𝑃ሺ𝐴||𝐵ሻ
𝑃ሺ𝐴ሻ
𝑋ଵ, …, 𝑋௡
523 of 848

variance 
. Show that
Solution
We have
where the next-to-last equality uses the result of Example 4a
 and the final
equality follows because
Although  and the deviation 
 are uncorrelated, they are not, in general,
independent. However, in the special case where the 
 are normal random
variables, it turns out that not only is  independent of a single deviation, but it is
independent of the entire sequence of deviations 
 This result
will be established in Section 7.8
, where we will also show that, in this case,
the sample mean  and the sample variance 
 are independent, with
 having a chi-squared distribution with 
 degrees of freedom.
(See Example 4a
 for the definition of 
.)
Example 4f
Consider 
 independent trials, each of which results in any of  possible
outcomes with probabilities 
. If we let 
, denote
the number of the 
 trials that result in outcome , then 
 have the
multinomial distribution
𝜎ଶ
Cov ሺ𝑋௜െ𝑋̅ ̅̅ ̅, 𝑋̅ ̅̅ ̅ሻൌ0
Cov ሺ𝑋௜െ𝑋̅ ̅̅ ̅, 𝑋̅ ̅̅ ̅ሻൌ
Cov ሺ𝑋௜, 𝑋̅ ̅̅ ̅ሻെ Cov ሺ𝑋̅ ̅̅ ̅, 𝑋̅ ̅̅ ̅ሻ
ൌ
Cov ቌ𝑋௜, 1
𝑛෍
௝ൌଵ
௡
𝑋௝ቍെ Var ሺ𝑋̅ ̅̅ ̅ሻ
ൌ
1
𝑛෍
௝ൌଵ
௡
Cov ሺ𝑋௜, 𝑋௝ሻെ𝜎ଶ
𝑛
ൌ
𝜎ଶ
𝑛െ𝜎ଶ
𝑛ൌ0
Cov ሺ𝑋௜, 𝑋௝ሻൌ൝
0
if 𝑗്𝑖 by independence
𝜎ଶif  𝑗ൌ𝑖 since Var ሺ𝑋௜ሻൌ𝜎ଶ
𝑋̅ ̅̅ ̅
𝑋௜െ𝑋̅ ̅̅ ̅
𝑋௜
𝑋̅ ̅̅ ̅
𝑋௝െ𝑋̅ ̅̅ ̅, 𝑗ൌ1, …, 𝑛.
𝑋̅ ̅̅ ̅
𝑆ଶ
ሺ𝑛െ1൯𝑆ଶ/𝜎ଶ
𝑛െ1
𝑆ଶ
𝑚
𝑟
𝑝ଵ, …, 𝑝௥, ෍
௜ൌଵ
௥
𝑝௜ൌ1
𝑁௜, 𝑖ൌ1, …, 𝑟
𝑚
𝑖
𝑁ଵ, 𝑁ଶ, …, 𝑁௥
𝑃ሼ𝑁ଵൌ𝑛ଵ, …, 𝑁௥ൌ𝑛௥ሽൌ
𝑚!
𝑛ଵ!…𝑛௥! 𝑝௡భଵ⋯𝑝௡ೝ௥,   ෍
௜ൌଵ
௥
𝑛௜ൌ𝑚
524 of 848

For 
, it seems likely that when 
 is large,
 would tend to be small; hence,
it is intuitive that they should be negatively correlated. Let us compute their
covariance by using Proposition 4.2
(iv) and the representation
where
From Proposition 4.2
(iv), we have
Now, on the one hand, when 
,
since the outcome of trial  is independent of the outcome of trial . On the other
hand,
where the equation uses the fact that 
 since trial  cannot result
in both outcome  and outcome . Hence, we obtain
which is in accord with our intuition that 
 and 
 are negatively correlated.
𝑖്𝑗
𝑁௜
𝑁௝
𝑁௜ൌ
෍
௞ൌଵ
௠
𝐼௜ሺ𝑘ሻ   and   𝑁௝ൌ
෍
௞ൌଵ
௠
𝐼௝ሺ𝑘ሻ
𝐼௜ሺ𝑘ሻ
ൌ
ቊ1
if trial  𝑘 results in outcome  𝑖
0
otherwise
𝐼௝ሺ𝑘ሻ
ൌ
ቊ1
if trial  𝑘 results in outcome  𝑗
0
otherwise
Cov ሺ𝑁௜, 𝑁௝ሻൌ
෍
ℓൌଵ
௠
෍
௞ൌଵ
௠
Cov ሺ𝐼௜ሺ𝑘ሻ, 𝐼௝ሺℓሻሻ
𝑘്ℓ
Cov ൫𝐼௜൫𝑘൯, 𝐼௝൫ℓ൯൯ൌ0
𝑘
ℓ
Cov ൫𝐼௜ሺℓ൯, 𝐼௝൫ℓ൯൯
ൌ
𝐸ൣ𝐼௜൫ℓ൯𝐼௝൫ℓ൯൧െ𝐸ൣ𝐼௜൫ℓ൯൧𝐸ൣ𝐼௝൫ℓ൯൧
ൌ
0 െ𝑝௜𝑝௝ൌെ𝑝௜𝑝௝
𝐼௜൫ℓ൯𝐼௝൫ℓ൯ൌ0,
ℓ
𝑖
𝑗
Cov ሺ𝑁௜, 𝑁௝ሻൌെ𝑚𝑝௜𝑝௝
𝑁௜
𝑁௝
525 of 848

Recall that if  and  are jointly discrete random variables, then the conditional
probability mass function of 
 given that 
 is defined for all  such that
 by
It is therefore natural to define, in this case, the conditional expectation of  given
that 
 for all values of  such that 
 by
Example 5a
If  and  are independent binomial random variables with identical parameters 
and  calculate the conditional expected value of  given that 
.
Solution
Let us first calculate the conditional probability mass function of  given that
. For 
where we have used the fact (see Example 3f
 of Chapter 6
) that 
 is
a binomial random variable with parameters 2  and . Hence, the conditional
distribution of 
given that 
 is the hypergeometric distribution, and
𝑋
𝑌
𝑋,
𝑌ൌ𝑦,
𝑦
𝑃ሼ𝑌ൌ𝑦ሽ൐0,
𝑝௑ห௒ሺ𝑥|𝑦ሻൌ𝑃ሼ𝑋ൌ𝑥|𝑌ൌ𝑦ሽൌ𝑝ሺ𝑥, 𝑦ሻ
𝑝௒ሺ𝑦൯
𝑋
𝑌ൌ𝑦,
𝑦
𝑝௒ሺ𝑦൯൐0,
𝐸ሾ𝑋|𝑌ൌ𝑦ሿ
ൌ
෍
௫
 𝑥𝑃ሼ𝑋ൌ𝑥|𝑌ൌ𝑦ሽ
ൌ
෍
௫
 𝑥𝑝𝑋|𝑌ሺ𝑥|𝑦ሻ
𝑋
𝑌
𝑛
𝑝,
𝑋
𝑋൅𝑌ൌ𝑚
𝑋
𝑋൅𝑌ൌ𝑚
𝑘൑minሺ𝑛, 𝑚ሻ,
𝑃ሼ𝑋ൌ𝑘||𝑋൅𝑌ൌ𝑚ሽ
ൌ
𝑃ሼ𝑋ൌ𝑘, 𝑋൅𝑌ൌ𝑚ሽ
𝑃ሼ𝑋൅𝑌ൌ𝑚ሽ
ൌ
𝑃ሼ𝑋ൌ𝑘, 𝑌ൌ𝑚െ𝑘ሽ
𝑃ሼ𝑋൅𝑌ൌ𝑚ሽ
ൌ
𝑃ሼ𝑋ൌ𝑘ሽ𝑃ሼ𝑌ൌ𝑚െ𝑘ሽ
𝑃ሼ𝑋൅𝑌ൌ𝑚ሽ
ൌ
ቆ𝑛
𝑘ቇ 𝑝௞ሺ1 െ𝑝ሻ௡െ௞ ቆ     𝑛
𝑚െ𝑘ቇ 𝑝௠െ௞ሺ1 െ𝑝ሻ௡െ௠൅௞
ቆ2𝑛
𝑚ቇ 𝑝௠ሺ1 െ𝑝ሻଶ௡െ௠
ൌ
ቆ𝑛
𝑘ቇ  ቆ     𝑛
𝑚െ𝑘ቇ
ቆ2𝑛
𝑚ቇ
𝑋൅𝑌
𝑛
𝑝
𝑋,
𝑋൅𝑌ൌ𝑚,
526 of 848

from Example 2g
, we obtain
Similarly, let us recall that if  and  are jointly continuous with a joint probability
density function 
 then the conditional probability density of 
 given that 
is defined for all values of  such that 
 by
It is natural, in this case, to define the conditional expectation of 
 given that 
by
provided that 
.
Example 5b
Suppose that the joint density of  and  is given by
Compute 
.
Solution
We start by computing the conditional density
𝐸ሾ𝑋|𝑋൅𝑌ൌ𝑚ሿൌ𝑚
2
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻ,
𝑋,
𝑌ൌ𝑦,
𝑦
𝑓௒ሺ𝑦൯൐0
𝑓௑ห௒ሺ𝑥|𝑦ሻൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓𝑌ሺ𝑦ሻ
𝑋,
𝑌ൌ𝑦,
𝐸ሾ𝑋|𝑌ൌ𝑦ሿൌ඲
െஶ
ஶ
𝑥𝑓௑|௒ሺ𝑥|𝑦ሻ𝑑𝑥
𝑓௒ሺ𝑦൯൐0
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑒െ௫/௬𝑒െ௬
𝑦
 0 ൏𝑥൏∞, 0 ൏𝑦൏∞
𝐸ሾ𝑋||𝑌ൌ𝑦ሿ
527 of 848

Hence, the conditional distribution of 
 given that 
 is just the exponential
distribution with mean . Thus,
Remark Just as conditional probabilities satisfy all of the properties of ordinary
probabilities, so do conditional expectations satisfy the properties of ordinary
expectations. For instance, such formulas as
and
remain valid. As a matter of fact, conditional expectation given that 
 can be
thought of as being an ordinary expectation on a reduced sample space consisting
only of outcomes for which 
.
𝑓௑|௒ሺ𝑥|𝑦ሻ
ൌ
𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦൯
ൌ
𝑓ሺ𝑥, 𝑦ሻ
඲
െஶ
ஶ
𝑓ቆ𝑥, 𝑦ቇ𝑑𝑥
ൌ
ሺ1/𝑦൯𝑒െ௫/௬𝑒െ௬
඲
଴
ஶ
ሺ1/𝑦ቇ𝑒െ௫/௬𝑒െ௬𝑑𝑥
ൌ
ሺ1/𝑦൯𝑒െ௫/௬
඲
଴
ஶ
ሺ1/𝑦ቇ𝑒െ௫/௬𝑑𝑥
ൌ
1
𝑦𝑒െ௫/௬
𝑋,
𝑌ൌ𝑦,
𝑦
𝐸ሾ𝑋|𝑌ൌ𝑦ሿൌ඲
଴
ஶ
𝑥
𝑦𝑒െ௫/௬𝑑𝑥ൌ𝑦
𝐸ሾ𝑔ሺ𝑋ሻ|𝑌ൌ𝑦ሿൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
෍
௫
𝑔ሺ𝑋ሻ𝑝௑|௒ሺ𝑥|𝑦ሻ
in the discrete case
඲
െஶ
ஶ
𝑔ሺ𝑋ሻ𝑓௑ห௒ሺ𝑥|𝑦ሻ𝑑𝑥
in the continuous case
𝐸቎෍
௜ൌଵ
௡
𝑋௜|𝑌ൌ𝑦቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜|𝑌ൌ𝑦ሿ
𝑌ൌ𝑦
𝑌ൌ𝑦
528 of 848

Let us denote by 
 that function of the random variable  whose value at 
is 
. Note that 
 is itself a random variable. An extremely important
property of conditional expectations is given by the following proposition.
Proposition 5.1 The conditional expectation formula
If  is a discrete random variable, then Equation (5.1)
 states that
whereas if  is continuous with density 
 then Equation (5.1)
 states
We now give a proof of Equation (5.1)
 in the case where  and  are both
discrete random variables.
Proof of Equation (5.1)
 When X and Y Are Discrete: We must show that
Now, the right-hand side of Equation (5.2)
 can be written as
𝐸ሾ𝑋||𝑌ሿ
𝑌
𝑌ൌ𝑦
𝐸ሾ𝑋||𝑌ൌ𝑦ሿ
𝐸ሾ𝑋||𝑌ሿ
 𝐸ሾ𝑋ሿൌ𝐸ሾ𝐸ሾ𝑋||𝑌ሿሿ
(5.1)
𝑌
𝐸ሾ𝑋ሿൌ෍
௬
𝐸ሾ𝑋|𝑌ൌ𝑦ሿ𝑃ሼ𝑌ൌ𝑦ሽ
(5.1a)
𝑌
𝑓௒ሺ𝑦൯,
𝐸ሾ𝑋ሿൌ඲
െஶ
ஶ
𝐸ሾ𝑋|𝑌ൌ𝑦ሿ𝑓௒ሺ𝑦ሻ𝑑𝑦
(5.1b)
𝑋
𝑌
𝐸ሾ𝑋ሿൌ෍
௬
𝐸ሾ𝑋|𝑌ൌ𝑦ሿ𝑃ሼ𝑌ൌ𝑦ሽ
(5.2)
529 of 848

and the result is proved.
One way to understand Equation (5.2)
 is to interpret it as follows: To calculate
 we may take a weighted average of the conditional expected value of  given
that 
 each of the terms 
 being weighted by the probability of the
event on which it is conditioned. (Of what does this remind you?) This is an
extremely useful result that often enables us to compute expectations easily by first
conditioning on some appropriate random variable. The following examples illustrate
its use.
Example 5c
A miner is trapped in a mine containing 3 doors. The first door leads to a tunnel
that will take him to safety after 3 hours of travel. The second door leads to a
tunnel that will return him to the mine after 5 hours of travel. The third door leads
to a tunnel that will return him to the mine after 7 hours. If we assume that the
miner is at all times equally likely to choose any one of the doors, what is the
expected length of time until he reaches safety?
Solution
Let  denote the amount of time (in hours) until the miner reaches safety, and let
 denote the door he initially chooses. Now,
However,
෍
௬
𝐸ሾ𝑋||𝑌ൌ𝑦ሿ𝑃ሼ𝑌ൌ𝑦ሽ
ൌ
 ෍
௬
෍
௫
 𝑥𝑃ሼ𝑋ൌ𝑥||𝑌ൌ𝑦ሽ𝑃ሼ𝑌ൌ𝑦ሽ
ൌ
෍
௬
෍
௫
 𝑥𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽ
𝑃ሼ𝑌ൌ𝑦ሽ
𝑃ሼ𝑌ൌ𝑦ሽ 
ൌ
෍
௬
෍
௫
 𝑥𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽ 
ൌ
෍
௫
𝑥෍
௬
𝑃ሼ𝑋ൌ𝑥, 𝑌ൌ𝑦ሽ
ൌ
෍
௫
𝑥𝑃ሼ𝑋ൌ𝑥ሽ
ൌ
𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿ,
𝑋
𝑌ൌ𝑦,
𝐸ሾ𝑋||𝑌ൌ𝑦ሿ
𝑋
𝑌
𝐸ሾ𝑋ሿ
ൌ
𝐸ሾ𝑋||𝑌ൌ1ሿ𝑃ሼ𝑌ൌ1ሽ൅𝐸ሾ𝑋||𝑌ൌ2ሿ𝑃ሼ𝑌ൌ2ሽ
൅𝐸ሾ𝑋||𝑌ൌ3ሿ𝑃ሼ𝑌ൌ3ሽ
ൌ
1
3ሺ𝐸ሾ𝑋||𝑌ൌ1ሿ൅𝐸ሾ𝑋|𝑌ൌ2ሿ൅𝐸ሾ𝑋|𝑌ൌ3ሿሻ
530 of 848

To understand why Equation (5.3)
 is correct, consider, for instance,
 and reason as follows: If the miner chooses the second door, he
spends 5 hours in the tunnel and then returns to his cell. But once he returns to
his cell, the problem is as before; thus, his expected additional time until safety is
just [ ]. Hence,
. The argument behind the other
equalities in Equation (5.3)
 is similar. Hence,
or
Example 5d Expectation of a sum of a random number of random variables
Suppose that the number of people entering a department store on a given day is
a random variable with mean 50. Suppose further that the amounts of money
spent by these customers are independent random variables having a common
mean of $8. Finally, suppose also that the amount of money spent by a customer
is also independent of the total number of customers who enter the store. What is
the expected amount of money spent in the store on a given day?
Solution
If we let  denote the number of customers who enter the store and 
 the
amount spent by the th such customer, then the total amount of money spent
can be expressed as 
 Now,
But
𝐸ሾ𝑋||𝑌
ൌ
1ሿൌ3
𝐸ሾ𝑋||𝑌
ൌ
2ሿൌ5 ൅𝐸ሾ𝑋ሿ
𝐸ሾ𝑋||𝑌
ൌ
3ሿൌ7 ൅𝐸ሾ𝑋ሿ
(5.3)
𝐸ሾ𝑋||𝑌ൌ2ሿ
𝐸𝑋
𝐸ሾ𝑋||𝑌ൌ2ሿൌ5 ൅𝐸ሾ𝑋ሿ
𝐸ሾ𝑋ሿൌ1
3ሺ3 ൅5 ൅𝐸ሾ𝑋ሿ൅7 ൅𝐸ሾ𝑋ሿሻ
𝐸ሾ𝑋ሿൌ15
𝑁
𝑋௜
𝑖
෍
௜ൌଵ
ே
𝑋௜.
𝐸቎෍
ଵ
ே
𝑋௜቏ൌ𝐸቎𝐸቎෍
ଵ
ே
𝑋௜|𝑁቏቏
531 of 848

which implies that
Thus,
Hence, in our example, the expected amount of money spent in the store is
 or $400.
Example 5e
The game of craps is begun by rolling an ordinary pair of dice. If the sum of the
dice is 2, 3, or 12, the player loses. If it is 7 or 11, the player wins. If it is any other
number  the player continues to roll the dice until the sum is either 7 or . If it is
7, the player loses; if it is  the player wins. Let  denote the number of rolls of
the dice in a game of craps. Find -5pt
a. 
;
b. 
c. 
Solution
If we let 
 denote the probability that the sum of the dice is  then
To compute 
 we condition on  the initial sum, giving
𝐸቎෍
ଵ
ே
𝑋௜|𝑁ൌ𝑛቏
ൌ
𝐸቎෍
ଵ
௡
𝑋௜|𝑁ൌ𝑛቏
ൌ
𝐸቎෍
ଵ
௡
𝑋௜቏  by the independence of the  𝑋௜ and  𝑁
ൌ
𝑛𝐸ሾ𝑋ሿ  where  𝐸ሾ𝑋ሿൌ𝐸ሾ𝑋௜ሿ
𝐸቎෍
ଵ
ே
𝑋௜|𝑁቏ൌ𝑁𝐸ሾ𝑋ሿ
𝐸቎෍
௜ൌଵ
ே
𝑋௜቏ൌ𝐸ሾ𝑁𝐸ሾ𝑋ሿሿൌ𝐸ሾ𝑁ሿ𝐸ሾ𝑋ሿ
50 ൈ$8,
𝑖,
𝑖
𝑖,
𝑅
𝐸ሾ𝑅ሿ
𝐸ሾ𝑅|| player winsሿ;
𝐸ሾ𝑅|| player losesሿ.
𝑃௜
𝑖,
𝑃௜ൌ𝑃ଵସെ௜ൌ𝑖െ1
36 ,  𝑖ൌ2, …, 7
𝐸ሾ𝑅ሿ,
𝑆,
532 of 848

However,
The preceding equation follows because if the sum is a value  that does not end
the game, then the dice will continue to be rolled until the sum is either  or 7, and
the number of rolls until this occurs is a geometric random variable with
parameter 
. Therefore,
To determine 
 let us start by determining  the probability that the
player wins. Conditioning on  yields
where the preceding uses the fact that the probability of obtaining a sum of 
before one of 7 is 
. Now, let us determine the conditional probability
mass function of  given that the player wins. Letting 
 we
have
and, for 
𝐸ሾ𝑅ሿൌ෍
௜ൌଶ
ଵଶ
𝐸ሾ𝑅|𝑆ൌ𝑖ሿ𝑃௜
𝐸ሾ𝑅|𝑆ൌ𝑖ሿൌ൞
1,
if  𝑖ൌ2, 3, 7, 11, 12
1 ൅
1
𝑃௜൅𝑃଻
,
otherwise
𝑖
𝑖
𝑃௜൅𝑃଻
𝐸ሾ𝑅ሿ
ൌ
1 ൅෍
௜ൌସ
଺
𝑃௜
𝑃௜൅𝑃଻
൅෍
௜ൌ଼
ଵ଴
𝑃௜
𝑃௜൅𝑃଻
ൌ
1 ൅2ሺ3/9 ൅4/10 ൅5/11ሻൌ3.376
𝐸ሾ𝑅|| win ሿ,
𝑝,
𝑆
𝑝
ൌ
෍
௜ൌଶ
ଵଶ
𝑃ሼwin ||𝑆ൌ𝑖ሽ𝑃௜
ൌ
𝑃଻൅𝑃ଵଵ൅෍
௜ൌସ
଺
𝑃௜
𝑃௜൅𝑃଻
𝑃௜൅෍
௜ൌ଼
ଵ଴
𝑃௜
𝑃௜൅𝑃଻
𝑃௜
ൌ
0.493
𝑖
𝑃௜/ሺ𝑃௜൅𝑃଻ሻ
𝑆,
𝑄௜ൌ𝑃൛𝑆ൌ𝑖หwinൟ,
𝑄ଶൌ𝑄ଷൌ𝑄ଵଶൌ0,  𝑄଻ൌ𝑃଻/𝑝,  𝑄ଵଵൌ𝑃ଵଵ/𝑝
𝑖ൌ4, 5, 6, 8, 9, 10,
533 of 848

Now, conditioning on the initial sum gives
However, as was noted in Example 2j
 of Chapter 6
, given that the initial
sum is  the number of additional rolls needed and the outcome (whether a win
or a loss) are independent. (This is easily seen by first noting that conditional on
an initial sum of  the outcome is independent of the number of additional dice
rolls needed and then using the symmetry property of independence, which
states that if event  is independent of event 
 then event  is independent of
event .) Therefore,
Although we could determine 
 exactly as we did
 it is easier to use
implying that
Example 5f
As defined in Example 5d
 of Chapter 6
, the bivariate normal joint density
function of the random variables  and  is
𝑄௜
ൌ
𝑃ሼ𝑆ൌ𝑖, winሽ
𝑃ሼwinሽ
ൌ
𝑃௜𝑃ሼwin ||𝑆ൌ𝑖ሽ
𝑝
ൌ
𝑃ଶ
௜
𝑝ሺ𝑃௜൅𝑃଻ሻ
𝐸ሾ𝑅| win ሿൌ෍
௜
𝐸ሾ𝑅| win , 𝑆ൌ𝑖ሿ𝑄௜
𝑖,
𝑖,
𝐴
𝐵,
𝐵
𝐴
𝐸ሾ𝑅|winሿ
ൌ
෍
௜
𝐸ሾ𝑅|𝑆ൌ𝑖ሿ𝑄௜
ൌ
1 ൅෍
௜ൌସ
଺
𝑄௜
𝑃௜൅𝑃଻
൅෍
௜ൌ଼
ଵ଴
 
𝑄௜
𝑃௜൅𝑃଻
ൌ
2.938
𝐸ሾ𝑅|| player losesሿ
𝐸ሾ𝑅|| player winsሿ,
𝐸ሾ𝑅ሿൌ𝐸ሾ𝑅|| win ሿ𝑝൅𝐸ሾ𝑅|| lose ሿሺ1 െ𝑝ሻ
𝐸ሾ𝑅| lose ሿൌ𝐸ሾ𝑅ሿെ𝐸ሾ𝑅|| win ሿ𝑝
1 െ𝑝
ൌ3 . 801
𝑋
𝑌
534 of 848

We will now show that  is the correlation between  and . As shown in
Example 5c
, 
 and 
Consequently,
To determine 
 we condition on . That is, we use the identity
Recalling from Example 5d
 that the conditional distribution of  given that
 is normal with mean 
 we see that
Consequently,
implying that
𝑓ሺ𝑥, 𝑦ሻൌ 
1
2𝜋𝜎௫𝜎௬
1 െ𝜌ଶ
ඥ
  exp ൝െ
1
2ሺ1 െ𝜌ଶሻ൥ቆ
𝑥െ𝜇௫
𝜎௫
ቇ
ଶ
൅ቆ
𝑦െ𝜇௬
𝜎௬
ቇ
ଶ
                      െ2𝜌
ሺ𝑥െ𝜇௫ሻቀ𝑦െ𝜇௬ቁ
𝜎௫𝜎௬
቏ቑ
𝜌
𝑋
𝑌
𝜇௫ൌ𝐸ൣ𝑋൧,𝜎௫
ଶൌ Var ሺ𝑋ሻ,
𝜇௬ൌ𝐸ሾ𝑌ሿ,𝜎௬
ଶൌ Var ሺ𝑌ሻ.
Corr ሺ𝑋, 𝑌ሻൌ
Cov ሺ𝑋, 𝑌ሻ
𝜎௫𝜎௬
ൌ
𝐸ሾ𝑋𝑌ሿെ𝜇௫𝜇௬
𝜎௫𝜎௬
𝐸ሾ𝑋𝑌ሿ,
𝑌
𝐸ሾ𝑋𝑌ሿൌ𝐸ሾ𝐸ሾ𝑋𝑌||𝑌ሿሿ
𝑋
𝑌ൌ𝑦
𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑦െ𝜇௬ሻ,
𝐸ሾ𝑋𝑌||𝑌
ൌ
𝑦ሿൌ𝐸ሾ𝑋𝑦||𝑌ൌ𝑦ሿ
ൌ
𝑦𝐸ሾ𝑋||𝑌ൌ𝑦ሿ
ൌ
𝑦ቈ𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑦െ𝜇௬ሻ቉
ൌ
𝑦𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑦ଶെ𝜇௬𝑦ሻ
𝐸ሾ𝑋𝑌|𝑌ሿൌ𝑌𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑌ଶെ𝜇௬𝑌ሻ
535 of 848

Therefore,
Sometimes 
 is easy to compute, and we use the conditioning identity to compute
a conditional expected value. This approach is illustrated by our next example.
Example 5g
Consider  independent trials, each of which results in one of the outcomes
 with respective probabilities 
. Let 
 denote the
number of trials that result in outcome 
. For 
 find
Solution
To solve (a), let
Then
or, equivalently,
Now, the unconditional distribution of 
 is binomial with parameters 
. Also,
given that 
 each of the 
 trials that does not result in outcome  will,
𝐸ሾ𝑋𝑌ሿ
ൌ
𝐸ቈ𝑌𝜇௫൅𝜌𝜎௫
𝜎௬
ሺ𝑌ଶെ𝜇௬𝑌ሻ቉
ൌ
𝜇௫𝐸ሾ𝑌ሿ൅𝜌𝜎௫
𝜎௬
𝐸ሾ𝑌ଶെ𝜇௬𝑌ሿ
ൌ
𝜇௫𝜇௬൅𝜌𝜎௫
𝜎௬
൫𝐸ൣ𝑌ଶ൧െ𝜇ଶ
௬൯
ൌ
𝜇௫𝜇௬൅𝜌𝜎௫
𝜎௬
Var ሺ𝑌ሻ
ൌ
𝜇௫𝜇௬൅𝜌𝜎௫𝜎௬
Corr ሺ𝑋, 𝑌ሻൌ𝜌𝜎௫𝜎௬
𝜎௫𝜎௬
ൌ𝜌
𝐸ሾ𝑋ሿ
𝑛
1, …, 𝑘,
𝑝ଵ, …, 𝑝௞,෍
௜ൌଵ
௞
𝑝௜ൌ1
𝑁௜
𝑖,𝑖ൌ1, …, 𝑘
𝑖്𝑗,
ሺa ሻ 𝐸ൣ𝑁௝ห𝑁௜൐0൧   and   ሺb ሻ 𝐸ൣ𝑁௝ห𝑁௜൐1൧
𝐼ൌ൝
0,
 if Ni ൌ0
1,
 if Ni൐0
𝐸ൣ𝑁௝൧ൌ𝐸ൣ𝑁௝ห𝐼ൌ0൧𝑃൛𝐼ൌ0ൟ൅𝐸ൣ𝑁௝ห𝐼ൌ1൧𝑃൛𝐼ൌ1ൟ
𝐸ൣ𝑁௝൧ൌ𝐸ൣ𝑁௝ห𝑁௜ൌ0൧𝑃൛𝑁௜ൌ0ൟ൅𝐸ൣ𝑁௝ห𝑁௜൐0൧𝑃൛𝑁௜൐0ൟ
𝑁௝
𝑛, 𝑝௝
𝑁௜ൌ𝑟,
𝑛െ𝑟
𝑖
536 of 848

independently, result in outcome  with probability 
.
Consequently, the conditional distribution of 
 given that 
 is binomial with
parameters 
. (For a more detailed argument for this conclusion, see
Example 4c
 of Chapter 6
.) Because 
 the preceding
equation yields
giving the result
We can solve part (b) in a similar manner. Let
Then
or, equivalently,
This equation yields
giving the result
𝑗
𝑃ሺ𝑗| not  𝑖ሻൌ
𝑝௝
1 െ𝑝௜
𝑁௝,
𝑁௜ൌ𝑟,
𝑛െ𝑟,
𝑝௝
1 െ𝑝௜
𝑃൛𝑁௜ൌ0ൟൌሺ1 െ𝑝௜൯
௡,
𝑛𝑝௝ൌ𝑛
𝑝௝
1 െ𝑝௜
ሺ1 െ𝑝௜ሻ௡൅𝐸ሾ𝑁௝|𝑁௜൐0ሿሺ1 െሺ1 െ𝑝௜ሻ௡ሻ
𝐸ሾ𝑁௝|𝑁௜൐0ሿൌ𝑛𝑝௝ 
1 െሺ1 െ𝑝௜ሻ௡െଵ
1 െሺ1 െ𝑝௜ሻ௡
𝐽ൌ൞
0,
 if N௜ൌ0
1,
 if N௜ൌ1
2,
 if N௜൐1
𝐸ൣ𝑁௝൧ൌ𝐸ൣ𝑁௝ห𝐽ൌ0൧𝑃൛𝐽ൌ0ൟ൅𝐸ൣ𝑁௝ห𝐽ൌ1൧𝑃൛𝐽ൌ1ൟ
  ൅𝐸ൣ𝑁௝ห𝐽ൌ2൧𝑃൛𝐽ൌ2ൟ
𝐸ൣ𝑁௝൧ൌ𝐸ൣ𝑁௝ห𝑁௜ൌ0൧𝑃൛𝑁௜ൌ0ൟ൅𝐸ൣ𝑁௝ห𝑁௜ൌ1൧𝑃൛𝑁௜ൌ1ൟ
  ൅𝐸ൣ𝑁௝ห𝑁௜൐1൧𝑃൛𝑁௜൐1ൟ
𝑛𝑝௝ൌ𝑛
𝑝௝
1 െ𝑝௜
ሺ1 െ𝑝௜ሻ௡൅ሺ𝑛െ1ሻ
𝑝௝
1 െ𝑝௜
𝑛𝑝௜ሺ1 െ𝑝௜ሻ௡െଵ
  ൅𝐸ൣ𝑁௝ห𝑁௜൐1൧൫1 െሺ1 െ𝑝௜൯
௡െ𝑛𝑝௜ሺ1 െ𝑝௜൯
௡െଵ൯
𝐸ሾ𝑁௝|𝑁௜൐1ሿൌ
𝑛𝑝௝ቂ1 െሺ1 െ𝑝௜ሻ௡െଵെሺ𝑛െ1ሻ𝑝௜ሺ1 െ𝑝௜ሻ௡െଶቃ
1 െሺ1 െ𝑝௜ሻ௡െ𝑛𝑝௜ሺ1 െ𝑝௜ሻ௡െଵ
537 of 848

It is also possible to obtain the variance of a random variable by conditioning. We
illustrate this approach by the following example.
Example 5h Variance of the geometric distribution
Independent trials, each resulting in a success with probability  are
successively performed. Let  be the time of the first success. Find Var 
.
Solution
Let 
 if the first trial results in a success and 
 otherwise. Now,
To calculate 
 we condition on  as follows:
However,
These two equations follow because, on the one hand, if the first trial results in a
success, then, clearly,
; thus,
. On the other hand, if the first trial
results in a failure, then the total number of trials necessary for the first success
will have the same distribution as 1 (the first trial that results in failure) plus the
necessary number of additional trials. Since the latter quantity has the same
distribution as 
 we obtain 
Hence,
However, as was shown in Example 8b
 of Chapter 4
,
;
therefore,
or
𝑝,
𝑁
ሺ𝑁ሻ
𝑌ൌ1
𝑌ൌ0
Var ሺ𝑁൯ൌ𝐸ൣ𝑁ଶ൧െሺ𝐸ൣ𝑁൧൯
ଶ
𝐸ൣ𝑁ଶ൧,
𝑌
𝐸ൣ𝑁ଶ൧ൌ𝐸ൣ𝐸ൣ𝑁ଶห𝑌൧൧
𝐸ൣ𝑁ଶห𝑌
ൌ
1ሿൌ1
𝐸ൣ𝑁ଶห𝑌
ൌ
0ሿൌ𝐸ሾሺ1 ൅𝑁ሻଶሿ
𝑁ൌ1
𝑁ଶൌ1
𝑁,
𝐸ൣ𝑁ଶห𝑌ൌ0൧ൌ𝐸ൣ൫1 ൅𝑁൯
ଶ൧
𝐸ൣ𝑁ଶ൧
ൌ
𝐸ൣ𝑁ଶห𝑌ൌ1൧𝑃൛𝑌ൌ1ൟ൅𝐸ൣ𝑁ଶห𝑌ൌ0൧𝑃൛𝑌ൌ0ൟ
ൌ
𝑝൅ሺ1 െ𝑝ሻ𝐸ሾሺ1 ൅𝑁ሻଶሿ
ൌ
1 ൅ሺ1 െ𝑝൯𝐸ൣ2𝑁൅𝑁ଶ൧
𝐸ሾ𝑁ሿൌ1/𝑝
𝐸ሾ𝑁ଶሿൌ1 ൅2ሺ1 െ𝑝ሻ
𝑝
൅ሺ1 െ𝑝ሻ𝐸ሾ𝑁ଶሿ
538 of 848

Consequently,
Example 5i
Consider a gambling situation in which there are  players, with player  initially
having 
 units, 
 At each stage, two of the players are chosen
to play a game, with the winner of the game receiving 1 unit from the loser. Any
player whose fortune drops to 0 is eliminated, and this continues until a single
player has all 
 units, with that player designated as the victor.
Assuming that the results of successive games are independent and that each
game is equally likely to be won by either of its two players, find the average
number of stages until one of the players has all  units.
Solution
To find the expected number of stages played, suppose first that there are only 2
players, with players 1 and 2 initially having  and 
 units, respectively. Let 
denote the number of stages that will be played, and let 
Then, for
...
where 
 is the additional number of stages needed beyond the first stage.
Taking expectations gives
Conditioning on the result of the first stage then yields
Now, if player 1 wins at the first stage, then the situation from that point on is
exactly the same as in a problem that supposes that player 1 starts with 
and player 2 with 
 units. Consequently,
𝐸ሾ𝑁ଶሿൌ2 െ𝑝
𝑝ଶ
Var ሺ𝑁ሻ
ൌ
𝐸ൣ𝑁ଶ൧െሺ𝐸ൣ𝑁൧ሻଶ
ൌ
2 െ𝑝
𝑝ଶ
െቆ1
𝑝ቇ
ଶ
ൌ
1 െ𝑝
𝑝ଶ
𝑟
𝑖
𝑛௜
𝑛௜൐0,  𝑖ൌ1,  … ,  𝑟.
n ≡෍
௜ൌଵ
௥
𝑛௜
𝑛
𝑗
𝑛െ𝑗
𝑋௝
𝑚௝ൌ𝐸ൣ𝑋௝൧.
𝑗ൌ1,
, 𝑛െ1,
𝑋௝ൌ1 ൅𝐴௝
𝐴௝
𝑚௝ൌ1 ൅𝐸ൣ𝐴௝൧
𝑚௝ൌ1 ൅𝐸ൣ𝐴௝ห1  wins ϐirst stage ൧1/2 ൅𝐸ൣ𝐴௝ห2  wins ϐirst stage ൧1/2
𝑗൅1
𝑛െሺ𝑗൅1ሻ
539 of 848

and, analogously,
Thus,
or, equivalently,
Using that 
 the preceding equation yields
suggesting that
To prove the preceding equality, we use mathematical induction. Since we’ve
already shown the equation to be true for 
 we take as the induction
hypothesis that it is true whenever 
. Now we must prove that it is true for
. Using Equation (5.4)
 yields
which completes the induction proof of (5.5)
. Letting 
 in (5.5)
, and
using that 
 now yields that
𝐸ൣ𝐴௝ห1  wins ϐirst stage ൧ൌ𝑚௝൅ଵ
𝐸ൣ𝐴௝ห2  wins ϐirst stage ൧ൌ𝑚௝െଵ
𝑚௝ൌ1 ൅1
2 𝑚௝൅ଵ൅1
2 𝑚௝െଵ
𝑚௝൅ଵൌ2𝑚௝െ𝑚௝െଵെ2,  𝑗ൌ1, …, 𝑛െ1
(5.4)
𝑚଴ൌ0,
𝑚ଶ
ൌ
2𝑚ଵെ2
𝑚ଷ
ൌ
2𝑚ଶെ𝑚ଵെ2 ൌ3𝑚ଵെ6 ൌ3ሺ𝑚ଵെ2ሻ
𝑚ସ
ൌ
2𝑚ଷെ𝑚ଶെ2 ൌ4𝑚ଵെ12 ൌ4ሺ𝑚ଵെ3ሻ
𝑚௜ൌ𝑖ሺ𝑚ଵെ𝑖൅1ሻ,   𝑖ൌ1, …, 𝑛
(5.5)
𝑖ൌ1, 2,
𝑖൑𝑗൏𝑛
𝑗൅1
𝑚௝൅ଵ
ൌ
2𝑚௝െ𝑚௝െଵെ2
ൌ
2𝑗ሺ𝑚ଵെ𝑗൅1ሻെሺ𝑗െ1ሻሺ𝑚ଵെ𝑗൅2ሻെ2 ሺby the induction hypothesis ሻ
ൌ
൫𝑗൅1൯𝑚ଵെ2𝑗ଶ൅2𝑗൅𝑗ଶെ3𝑗൅2 െ2
ൌ
൫𝑗൅1൯𝑚ଵെ𝑗ଶെ𝑗
ൌሺ𝑗൅1ሻሺ𝑚ଵെ𝑗ሻ
𝑖ൌ𝑛
𝑚௡ൌ0,
𝑚ଵൌ𝑛െ1
540 of 848

which, again using (5.5)
, gives the result
Thus, the mean number of games played when there are only 2 players with
initial amounts  and 
 is the product of their initial amounts. Because both
players play all stages, this is also the mean number of stages involving player 1.
Now let us return to the problem involving  players with initial amounts 
...
 Let  denote the number of stages needed to obtain a victor, and
let 
 denote the number of stages involving player . Now, from the point of view
of player  starting with 
 he will continue to play stages, independently being
equally likely to win or lose each one, until his fortune is either  or 0. Thus, the
number of stages he plays is exactly the same as when he has a single opponent
with an initial fortune of 
. Consequently, by the preceding result, it follows
that
so
But because each stage involves two players,
Taking expectations now yields
It is interesting to note that while our argument shows that the mean number of
stages does not depend on the manner in which the teams are selected at each
stage, the same is not true for the distribution of the number of stages. To see
this, suppose 
 and 
If players 1 and 2 are chosen in
the first stage, then it will take at least three stages to determine a winner,
whereas if player 3 is in the first stage, then it is possible for there to be only two
𝑚௜ൌ𝑖ሺ𝑛െ𝑖ሻ
𝑖
𝑛െ𝑖
𝑟
𝑛௜, 𝑖ൌ1,
෍
௜ൌଵ
௥
 𝑛௜ൌ𝑛.
𝑋
𝑋௜
𝑖
𝑖,
𝑛௜,
𝑛
𝑛െ𝑛௜
𝐸ሾ𝑋௜ሿൌ𝑛௜ሺ𝑛െ𝑛௜ሻ
𝐸቎෍
௜ൌଵ
௥
𝑋௜቏ൌ෍
௜ൌଵ
௥
𝑛௜ሺ𝑛െ𝑛௜ሻൌ𝑛ଶെ෍
௜ൌଵ
௥
𝑛ଶ
௜
𝑋ൌ1
2 ෍
௜ൌଵ
௥
𝑋௜
𝐸ሾ𝑋ሿൌ1
2 ቌ𝑛ଶെ෍
௜ൌଵ
௥
𝑛ଶ
௜ቍ
𝑟ൌ3, 𝑛ଵൌ𝑛ଶൌ1,
𝑛ଷൌ2 .
541 of 848

stages.
In our next example, we use conditioning to verify a result previously noted in
Section 6.3.1
: that the expected number of uniform 
 random variables that
need to be added for their sum to exceed 1 is equal to .
Example 5j
Let 
 be a sequence of independent uniform (0, 1) random variables. Find
 when
Solution
We will find E[N] by obtaining a more general result. For 
 let
and set
That is,
 is the number of uniform (0, 1) random variables we must add until
their sum exceeds  and 
 is its expected value. We will now derive an
equation for 
 by conditioning on 
. This gives, from Equation (5.1b)
,
Now,
The preceding formula is obviously true when 
. It is also true when 
since, if the first uniform value is  then, at that point, the remaining number of
ሺ0, 1ሻ
𝑒
𝑈ଵ, 𝑈ଶ, …
𝐸ሾ𝑁ሿ
𝑁ൌmin ቐ𝑛:  ෍
௜ൌଵ
௡
𝑈௜൐1ቑ
𝑥∈ሾ0, 1ሿ,
𝑁ሺ𝑥ሻൌmin ቐ𝑛:  ෍
௜ൌଵ
௡
𝑈௜൐𝑥ቑ
𝑚ሺ𝑥ሻൌ𝐸ሾ𝑁ሺ𝑋ሻሿ
𝑁ሺ𝑥ሻ
𝑥,
𝑚ሺ𝑥ሻ
𝑚ሺ𝑥ሻ
𝑈ଵ
𝑚ሺ𝑥ሻൌ඲
଴
ଵ
𝐸ሾ𝑁ሺ𝑋ሻ|𝑈ଵൌ𝑦ሿ𝑑𝑦
(5.6)
𝐸ሾ𝑁ሺ𝑋ሻ|𝑈ଵൌ𝑦ሿൌ൝
1
if  𝑦൐𝑥
1 ൅𝑚ሺ𝑥െ𝑦ሻ
if  𝑦൑𝑥
(5.7)
𝑦൐𝑥
𝑦൑𝑥,
𝑦,
542 of 848

uniform random variables needed is the same as if we were just starting and
were going to add uniform random variables until their sum exceeded 
.
Substituting Equation (5.7)
 into Equation (5.6)
 gives
Differentiating the preceding equation yields
or, equivalently,
Integrating this equation gives
or
Since 
 it follows that 
 so we obtain
Therefore,
(1), the expected number of uniform (0, 1) random variables that
need to be added until their sum exceeds 1, is equal to .
Not only can we obtain expectations by first conditioning on an appropriate random
variable, but we can also use this approach to compute probabilities. To see this, let
 denote an arbitrary event, and define the indicator random variable  by
𝑥െ𝑦
𝑚ሺ𝑥ሻൌ
1 ൅඲
଴
௫
𝑚ሺ𝑥െ𝑦ሻ𝑑𝑦
ൌ
1 ൅඲
଴
௫
𝑚ሺ𝑢ሻ𝑑𝑢  by letting
𝑢ൌ𝑥െ𝑦
𝑚′ሺ𝑥ሻൌ𝑚ሺ𝑋ሻ
𝑚′ሺ𝑥ሻ
𝑚ሺ𝑥ሻൌ1
logሾ𝑚ሺ𝑋ሻሿൌ𝑥൅𝑐
𝑚ሺ𝑥ሻൌ𝑘𝑒௫
𝑚ሺ0ሻൌ1,
𝑘ൌ1,
𝑚ሺ𝑥ሻൌ𝑒௫
𝑚
𝑒
𝐴
𝑋
𝑋ൌቊ1
if  𝐴 occurs
0
if  𝐴 does not occur
543 of 848

It follows from the definition of  that
Therefore, from Equations (5.1a
) and (5.1b)
, we obtain
Note that if  is a discrete random variable taking on one of the values 
 then
by defining the events 
 by 
Equation (5.8)
 reduces to
the familiar equation
where 
 are mutually exclusive events whose union is the sample space.
Example 5k The best-prize problem
Suppose that we are to be presented with  distinct prizes, in sequence. After
being presented with a prize, we must immediately decide whether to accept it or
to reject it and consider the next prize. The only information we are given when
deciding whether to accept a prize is the relative rank of that prize compared to
ones already seen. That is, for instance, when the fifth prize is presented, we
learn how it compares with the four prizes we’ve already seen. Suppose that
once a prize is rejected, it is lost, and that our objective is to maximize the
probability of obtaining the best prize. Assuming that all ! orderings of the prizes
are equally likely, how well can we do?
Solution
Rather surprisingly, we can do quite well. To see this, fix a value 
 and
consider the strategy that rejects the first  prizes and then accepts the first one
that is better than all of those first . Let 
 denote the probability that the
best prize is selected when this strategy is employed. To compute this probability,
condition on 
 the position of the best prize. This gives
𝑋
𝐸ሾ𝑋ሿ
ൌ
𝑃ሺ𝐴ሻ
𝐸ሾ𝑋||𝑌
ൌ
𝑦ሿൌ𝑃ሺ𝐴||𝑌ൌ𝑦ሻ  for any random variable  𝑌
𝑃ሺ𝐴ሻ
ൌ෍
௬
𝑃ሺ𝐴|𝑌ൌ𝑦ሻ𝑃ሺ𝑌ൌ𝑦ሻ  if  𝑌 is discrete
ൌ඲
െஶ
ஶ
𝑃ሺ𝐴ห𝑌ൌ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦  if  𝑌 is continuous
(5.8)
𝑌
𝑦ଵ, …, 𝑦௡,
𝐵௜, 𝑖ൌ1, …, 𝑛,
𝐵௜ൌ൛𝑌ൌ𝑦௜ൟ,
𝑃ሺ𝐴ሻൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴||𝐵௜ሻ𝑃ሺ𝐵௜ሻ
𝐵ଵ, …, 𝐵௡
𝑛
𝑛
𝑘,0 ൑𝑘൏𝑛,
𝑘
𝑘
𝑃௞ሺ𝑏𝑒𝑠𝑡ሻ
𝑋,
544 of 848

Now, on the one hand, if the overall best prize is among the first  then no prize
is ever selected under the strategy considered. That is,
On the other hand, if the best prize is in position  where 
 then the best
prize will be selected if the best of the first 
 prizes is among the first (for
then none of the prizes in positions 
 would be selected). But,
conditional on the best prize being in position  it is easy to verify that all possible
orderings of the other prizes remain equally likely, which implies that each of the
first 
 prizes is equally likely to be the best of that batch. Hence, we have
From the preceding, we obtain
Now, if we consider the function
then
𝑃௞ሺbest ሻൌ෍
௜ൌଵ
௡
𝑃௞ሺbest |𝑋ൌ𝑖ሻ𝑃ሺ𝑋ൌ𝑖ሻ
ൌ1
𝑛෍
௜ൌଵ
௡
𝑃௞ሺbest |𝑋ൌ𝑖ሻ
𝑘,
𝑃௞ሺbest ||𝑋ൌ𝑖ሻൌ0  if  𝑖൑𝑘
𝑖,
𝑖൐𝑘,
𝑖െ1
𝑘
𝑘൅1, 𝑘൅2, …, 𝑖െ1
𝑖,
𝑖െ1
𝑃௞ሺbest ||𝑋ൌ
𝑖ሻൌ𝑃ሼbest of ϐirst  𝑖െ1  is among the ϐirst  𝑘||𝑋ൌ𝑖ሽ
ൌ
𝑘
𝑖െ1   if   𝑖൐𝑘
𝑃௞ሺbest ሻ
ൌ
𝑘
𝑛
෍
௜ൌ௞൅ଵ
௡
1
𝑖െ1
ൎ
𝑘
𝑛඲
௞൅ଵ
௡
1
𝑥െ1 𝑑𝑥
ൌ
𝑘
𝑛log ቆ𝑛െ1
𝑘
ቇ
ൎ
𝑘
𝑛log ൬𝑛
𝑘൰
𝑔ሺ𝑥ሻൌ𝑥
𝑛log ൬𝑛
𝑥൰
545 of 848

so
Thus, since 
 we see that the best strategy of the type
considered is to let the first n/e prizes go by and then accept the first one to
appear that is better than all of those. In addition, since 
 the
probability that this strategy selects the best prize is approximately 
.
Remark Most people are quite surprised by the size of the probability of
obtaining the best prize, thinking that this probability would be close to 0 when 
is large. However, even without going through the calculations, a little thought
reveals that the probability of obtaining the best prize can be made reasonably
large. Consider the strategy of letting half of the prizes go by and then selecting
the first one to appear that is better than all of those. The probability that a prize
is actually selected is the probability that the overall best is among the second
half, and this is . In addition, given that a prize is selected, at the time of
selection that prize would have been the best of more than /2 prizes to have
appeared and would thus have probability of at least  of being the overall best.
Hence, the strategy of letting the first half of all prizes go by and then accepting
the first one that is better than all of those prizes has a probability greater than 
of obtaining the best prize.
Example 5l
Let  be a uniform random variable on (0, 1), and suppose that the conditional
distribution of 
 given that 
 is binomial with parameters  and . Find the
probability mass function of .
Solution
Conditioning on the value of  gives
𝑔′ሺ𝑥ሻൌ1
𝑛log ൬𝑛
𝑥൰െ1
𝑛
𝑔′ሺ𝑥൰ൌ0 ⇒log ൬𝑛
𝑥൰ൌ1 ⇒𝑥ൌ𝑛
𝑒
𝑃௞ሺ𝑏𝑒𝑠𝑡ሻൎ𝑔ሺ𝑘ሻ,
𝑔ሺ𝑛/𝑒ሻൌ1/𝑒,
1/𝑒ൎ. 36788
𝑛
1
2
𝑛
1
2
1
4
𝑈
𝑋,
𝑈ൌ𝑝,
𝑛
𝑝
𝑋
𝑈
546 of 848

Now, it can be shown (a probabilistic proof is given in Section 6.6)
 that
Hence, we obtain
That is, we obtain the surprising result that if a coin whose probability of coming
up heads is uniformly distributed over (0, 1) is flipped  times, then the number of
heads occurring is equally likely to be any of the values 
.
Because the preceding conditional distribution has such a nice form, it is worth
trying to find another argument to enhance our intuition as to why such a result is
true. To do so, let 
 be 
 independent uniform (0, 1) random
variables, and let  denote the number of the random variables 
 that are
smaller than .Since all the random variables 
 have the same
distribution, it follows that  is equally likely to be the smallest, or second
smallest, or largest of them; so  is equally likely to be any of the values 
.
However, given that 
 the number of the 
 that are less than  is a binomial
random variable with parameters  and  thus establishing our previous result.
Example 5m
A random sample of  balls is chosen from an urn that contains  red and 
 blue
balls. If  is equally likely to be any of the values 
 find the probability that
all the balls in the sample are red.
Solution
Conditioning on  yields
𝑃ሼ𝑋
ൌ
𝑖ሽൌ඲
଴
ଵ
𝑃ሼ𝑋ൌ𝑖|𝑈ൌ𝑝ሽ𝑓௎ሺ𝑝ሻ𝑑𝑝
ൌ
඲
଴
ଵ
𝑃ሼ𝑋ൌ𝑖|𝑈ൌ𝑝ሽ𝑑𝑝
ൌ
𝑛!
𝑖!ሺ𝑛െ𝑖ሻ!඲
଴
ଵ
𝑝௜ሺ1 െ𝑝ሻ௡െ௜𝑑𝑝
඲
଴
ଵ
𝑝௜ሺ1 െ𝑝ሻ௡െ௜𝑑𝑝ൌ𝑖!ሺ𝑛െ𝑖ሻ!
ሺ𝑛൅1ሻ!
𝑃ሼ𝑋ൌ𝑖ሽൌ
1
𝑛൅1  𝑖ൌ0, …, 𝑛
𝑛
0, …, 𝑛
𝑈, 𝑈ଵ, …, 𝑈௡
𝑛൅1
𝑋
𝑈ଵ, …, 𝑈௡
𝑈
𝑈, 𝑈ଵ, …, 𝑈௡
𝑈
𝑋
0, 1, …, 𝑛
𝑈ൌ𝑝,
𝑈௜
𝑈
𝑛
𝑝,
𝑋
𝑛
𝑚
𝑋
1, …, 𝑛,
𝑋
547 of 848

Now, given that the sample is of size  each of the 
 subsets of size  is
equally likely to be the chosen set of balls. As 
 of these subsets have all red
balls, it follows that 
 and thus that
However, though not obvious, it turns out that the preceding can be simplified,
and indeed yields the surprising result that
To prove the preceding formula, we will not make use of our earlier result, but
rather we will use induction on . When 
 the urn contains  red and 
 blue
balls and so a random sample of size  will be red with probability 
. So,
assume the result is true whenever the urn contains 
 red and 
 blue balls
and a random sample whose size is equally likely to be any of 
 is to be
chosen. Now consider the case of  red and 
 blue balls. Start by conditioning
not on the value of  but only on whether or not 
. This yields
Now, if 
 then in order for all balls in the sample to be red, the first one
chosen must be red, which occurs with probability 
 and then all of the
 remaining balls in the sample must be red. But given that the first ball
chosen is red, the remaining 
 balls will be randomly selected from an urn
containing 
 red and 
 blue balls. As 
 given that 
 is equally likely
to be any of the values 
 it follows by the induction hypothesis that
𝑃ሺall balls are red ሻൌ෍
௜ൌଵ
௡
𝑃ሺall balls are red |𝑋ൌ𝑖ሻ𝑃ሺ𝑋ൌ𝑖ሻ
𝑖,
ቆ𝑛൅𝑚
𝑖
ቇ
𝑖
ቆ𝑛
𝑖ቇ
𝑃ሼall balls are red|𝑋ൌ𝑖ሽൌ
ቆ௡
௜ቇ
ቆ௡൅௠
௜
ቇ
𝑃ሺall balls are redሻൌ1
𝑛෍
௜ൌଵ
௡
ቆ𝑛
𝑖ቇ
ቆ𝑛൅𝑚
𝑖
ቇ
𝑃ሺall balls are red ሻൌ
1
𝑚൅1,   for all  𝑛, 𝑚
𝑛
𝑛ൌ1,
1
𝑚
1
1
𝑚൅1
𝑛െ1
𝑚
1, …, 𝑛െ1
𝑛
𝑚
𝑋
𝑋ൌ1
𝑃ሺall balls are red ሻ
ൌ
𝑃ሺall red ||𝑋ൌ1ሻ𝑃ሺ𝑋ൌ1ሻ൅𝑃ሺall red ||𝑋൐1ሻ𝑃ሺ𝑋൐1ሻ
ൌ
𝑛
𝑛൅𝑚
1
𝑛൅𝑃ሺall red |𝑋൐1ቇ𝑛െ1
𝑛
𝑋൐1
𝑛
𝑛൅𝑚,
𝑋െ1
𝑋െ1
𝑛െ1
𝑚
𝑋െ1,
𝑋൐1,
1, …, 𝑛െ1,
548 of 848

Thus,
Example 5n
Suppose that  and  are independent continuous random variables having
densities 
 and 
 respectively. Compute 
.
Solution
Conditioning on the value of  yields
where
Example 5o
Suppose that  and  are independent continuous random variables. Find the
distribution function and the density function of 
.
Solution
𝑃ሺall balls are red |𝑋൐1ሻൌ
𝑛
𝑛൅𝑚
1
𝑚൅1
𝑃ሺall balls are red ሻ
ൌ
1
𝑛൅𝑚൅
𝑛
𝑛൅𝑚
1
𝑚൅1
𝑛െ1
𝑛
ൌ
1
𝑛൅𝑚ሺ1 ൅𝑛െ1
𝑚൅1ሻ
ൌ
1
𝑚൅1
𝑋
𝑌
𝑓௑
𝑓௒,
𝑃ሼ𝑋൏𝑌ሽ
𝑌
𝑃ሼ𝑋൏𝑌ሽ
ൌ
඲
െஶ
ஶ
𝑃ሼ𝑋൏𝑌|𝑌ൌ𝑦ሽ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑃ሼ𝑋൏𝑦|𝑌ൌ𝑦ሽ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑃ሼ𝑋൏𝑦ሽ𝑓௒ሺ𝑦ሽ𝑑𝑦  by independence
ൌ
඲
െஶ
ஶ
𝐹௑ሺ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
𝐹௑ሺ𝑦ሻൌ඲
െஶ
௬
𝑓௑ሺ𝑥ሻ𝑑𝑥
𝑋
𝑌
𝑋൅𝑌
549 of 848

By conditioning on the value of 
 we obtain
Differentiation yields the density function of 
:
Just as we have defined the conditional expectation of  given the value of 
 we can
also define the conditional variance of  given that 
:
That is,
 is equal to the (conditional) expected square of the difference
between  and its (conditional) mean when the value of  is given. In other words,
 is exactly analogous to the usual definition of variance, but now all
expectations are conditional on the fact that  is known.
There is a very useful relationship between Var 
 the unconditional variance of 
and 
 the conditional variance of  given 
 that can often be applied to
compute Var 
. To obtain this relationship, note first that by the same reasoning
that yields 
 we have
𝑌,
𝑃ሼ𝑋൅𝑌൏𝑎ሽ
ൌ
඲
െஶ
ஶ
𝑃ሼ𝑋൅𝑌൏𝑎|𝑌ൌ𝑦ሽ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑃ሼ𝑋൅𝑦൏𝑎|𝑌ൌ𝑦ሽ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑃ሼ𝑋൏𝑎െ𝑦ሽ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝐹௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
𝑋൅𝑌
𝑓௑൅௒ሺ𝑎൯
ൌ
𝑑
𝑑𝑎඲
െஶ
ஶ
𝐹௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑑
𝑑𝑎𝐹௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑓௑ሺ𝑎െ𝑦ሻ𝑓௒ሺ𝑦ሻ𝑑𝑦
𝑋
𝑌,
𝑋
𝑌ൌ𝑦
Var ሺ𝑋||𝑌ሻ≡𝐸ሾሺ𝑋െ𝐸ሾ𝑋||𝑌ሿሻଶ||𝑌ሿ
Var ሺ𝑋||𝑌ሻ
𝑋
𝑌
Var ሺ𝑋||𝑌ሻ
𝑌
ሺ𝑋ሻ,
𝑋,
Var ሺ𝑋||𝑌ሻ,
𝑋
𝑌,
ሺ𝑋ሻ
Var ሺ𝑋ሻൌ𝐸ൣ𝑋ଶ൧െሺ𝐸ൣ𝑋൧stretchyൌ'false'ሻଶ,
550 of 848

so
Also, since 
 we have
Hence, by adding Equations (5.9)
 and (5.10)
, we arrive at the following
proposition.
Proposition 5.2 The Conditional Variance Formula
Example 5p
Suppose that by any time  the number of people who have arrived at a train
depot is a Poisson random variable with mean 
. If the initial train arrives at the
depot at a time (independent of when the passengers arrive) that is uniformly
distributed over (0, ), what are the mean and variance of the number of
passengers who enter the train?
Solution
For each 
 let 
 denote the number of arrivals by  and let  denote the
time at which the train arrives. The random variable of interest is then 
.
Conditioning on  gives
Hence,
so taking expectations gives
Var ሺ𝑋ห𝑌ሻൌ𝐸ൣ𝑋ଶห𝑌൧െሺ𝐸ൣ𝑋ห𝑌൧൯
ଶ
𝐸ሾVar ሺ𝑋||𝑌ሻሿ
ൌ
𝐸ൣ𝐸ൣ𝑋ଶห𝑌൧൧െ𝐸ൣ൫𝐸ൣ𝑋ห𝑌൧൯
ଶ൧
ൌ
𝐸ൣ𝑋ଶ൧െ𝐸ൣ൫𝐸ൣ𝑋ห𝑌൧൯
ଶ൧
(5.9)
𝐸ሾ𝐸ሾ𝑋||𝑌ሿሿൌ𝐸ሾ𝑋ሿ,
Var ሺ𝐸ሾ𝑋||𝑌ሿሻൌ𝐸ሾሺ𝐸ሾ𝑋||𝑌ሿሻଶሿെሺ𝐸ሾ𝑋ሿሻଶ
(5.10)
Var ሺ𝑋ሻൌ𝐸ሾVar ሺ𝑋||𝑌ሻሿ൅Var ሺ𝐸ሾ𝑋||𝑌ሿሻ
𝑡
𝜆𝑡
𝑇
𝑡൒0,
𝑁ሺ𝑡ሻ
𝑡,
𝑌
𝑁ሺ𝑌ሻ
𝑌
𝐸ሾ𝑁ሺ𝑌ሻ||𝑌ൌ𝑡ሿ
ൌ
𝐸ሾ𝑁ሺ𝑡ሻ||𝑌ൌ𝑡ሿ
ൌ
𝐸ሾ𝑁ሺ𝑡ሻሿ  by the independence of Y and Nሺ𝑡ሻ
ൌ
𝜆𝑡                  since 𝑁ሺ𝑡ሻ is Poisson with mean 𝜆𝑡
𝐸ሾ𝑁ሺ𝑌ሻ||𝑌ሿൌ𝜆𝑌
551 of 848

To obtain Var(
), we use the conditional variance formula:
Thus,
Hence, from the conditional variance formula,
where we have used the fact that 
.
Example 5q Variance of a sum of a random number of random variables
Let 
 be a sequence of independent and identically distributed random
variables, and let  be a nonnegative integer-valued random variable that is
independent of the sequence 
. To compute 
 we condition
on :
The preceding result follows because, given 
 is just the sum of a fixed
number of independent random variables, so its expectation and variance are
just the sums of the individual means and variances, respectively. Hence, from
the conditional variance formula,
𝐸ሾ𝑁ሺ𝑌ሻ቉ൌ𝜆𝐸ሾ𝑌ሿൌ𝜆𝑇
2
𝑁ሺ𝑌ሻ
Varሺ𝑁ሺ𝑌ሻ||𝑌ൌ𝑡ሻൌ
Varሺ𝑁ሺ𝑡ሻ||𝑌ൌ𝑡ሻ
ൌ
Varሾ𝑁ሺ𝑡ሻሿ  by independence
ൌ
𝜆𝑡
Var ሺ𝑁ሺ𝑌ሻ||𝑌ሻൌ
𝜆𝑌
𝐸ሾ𝑁ሺ𝑌ሻ||𝑌ሿ
ൌ
𝜆𝑌
Var ሺ𝑁ሺ𝑌ሻሻൌ
𝐸ሾ𝜆𝑌ሿ൅Var ሺ𝜆𝑌ሻ
ൌ
𝜆𝑇
2 ൅𝜆ଶ𝑇ଶ
12
Var ൫𝑌൯ൌ𝑇ଶ/12
𝑋ଵ, 𝑋ଶ, …
𝑁
𝑋௜, 𝑖൒1
Var ቌ෍
௜ൌଵ
ே
𝑋௜ቍ,
𝑁
𝐸቎෍
௜ൌଵ
ே
𝑋௜|𝑁቏
ൌ𝑁𝐸ሾ𝑋ሿ
Var ቌ෍
௜ൌଵ
ே
𝑋௜|𝑁ቍ
ൌ𝑁 Var ሺ𝑋ሻ
𝑁, ෍
௜ൌଵ
ே
𝑋௜
552 of 848

Sometimes a situation arises in which the value of a random variable  is observed
and then, on the basis of the observed value, an attempt is made to predict the value
of a second random variable . Let 
 denote the predictor; that is, if  is observed
to equal  then 
 is our prediction for the value of . Clearly, we would like to
choose  so that 
 tends to be close to . One possible criterion for closeness is
to choose  so as to minimize 
. We now show that, under this
criterion, the best possible predictor of  is 
.
Proposition 6.1
Proof
However, given 
 being a function of 
 can be treated as a
constant. Thus,
Hence, from Equations (6.1)
 and (6.2
), we obtain
Var ቌ෍
௜ൌଵ
ே
𝑋௜ሻൌ𝐸ሾ𝑁ሿVar ሺ𝑋ቍ൅ሺ𝐸ሾ𝑋ሿሻଶVar ሺ𝑁ሻ
𝑋
𝑌
𝑔ሺ𝑋ሻ
𝑋
𝑥,
𝑔ሺ𝑥ሻ
𝑌
𝑔
𝑔ሺ𝑋ሻ
𝑌
𝑔
𝐸ሾሺ𝑌െ𝑔ሺ𝑋ሻሻଶሿ
𝑌
𝑔ሺ𝑋ሻൌ𝐸ሾ𝑌||𝑋ሿ
𝐸ሾሺ𝑌െ𝑔ሺ𝑋ሻሻଶሿ൒𝐸ሾሺ𝑌െ𝐸ሾ𝑌||𝑋ሿሻଶሿ
𝐸ሾሺ𝑌െ𝑔ሺ𝑋ሻሻଶห𝑋ሿ
ൌ
𝐸ሾሺ𝑌െ𝐸ሾ𝑌||𝑋ሿ൅𝐸ሾ𝑌||𝑋ሿെ𝑔ሺ𝑋ሻሻ
ଶ||𝑋ሿ
ൌ
𝐸ሾሺ𝑌െ𝐸ሾ𝑌||𝑋ሿሻ
ଶ||𝑋ሿ
൅𝐸ሾሺ𝐸ሾ𝑌||𝑋ሿെ𝑔ሺ𝑋ሻሻ
ଶ||𝑋ሿ
൅2𝐸ሾሺ𝑌െ𝐸ሾ𝑌||𝑋ሿሻሺ𝐸ሾ𝑌||𝑋ሿെ𝑔ሺ𝑋ሻሻ||𝑋ሿ
(6.1)
𝑋, 𝐸ሾ𝑌||𝑋ሿെ𝑔ሺ𝑋ሻ,
𝑋,
𝐸ሾሺ𝑌െ𝐸ሾ𝑌|𝑋ሿሻሺ𝐸ሾ𝑌|𝑋ሿെ𝑔ሺ𝑋ሻሻ|𝑋ሿ
 
ൌ
ሺ𝐸ሾ𝑌||𝑋ሿെ𝑔ሺ𝑋ሻሻ𝐸ሾ𝑌െ𝐸ሾ𝑌||𝑋ሿ||𝑋ሿ
 
ൌ
ሺ𝐸ሾ𝑌||𝑋ሿെ𝑔ሺ𝑋ሻሻሺ𝐸ሾ𝑌||𝑋ሿെ𝐸ሾ𝑌||𝑋ሿሻ
 
ൌ0
(6.2)
𝐸ሾሺ𝑌െ𝑔ሺ𝑋ሻሻଶ||𝑋ሿ൒𝐸ሾሺ𝑌െ𝐸ሾ𝑌||𝑋ሿሻଶ||𝑋ሿ
553 of 848

and the desired result follows by taking expectations of both sides of the
preceding expression.
Remark A second, more intuitive, although less rigorous, argument verifying
Proposition 6.1
 is as follows: It is straightforward to verify that 
 is
minimized at 
. (See Theoretical Exercise 1.) Thus, if we want to predict
the value of  when there are no data available to use, the best possible
prediction, in the sense of minimizing the mean square error, is to predict that 
will equal its mean. However, if the value of the random variable  is observed to
be  then the prediction problem remains exactly as in the previous (no-data)
case, with the exception that all probabilities and expectations are now
conditional on the event that 
. Hence, the best prediction in this situation is
to predict that  will equal its conditional expected value given that 
 thus
establishing Proposition 6.1
.
Example 6a
Suppose that the son of a man of height (in inches) attains a height that is
normally distributed with mean 
 and variance 4. What is the best prediction
of the height at full growth of the son of a man who is 6 feet tall?
Solution
Formally, this model can be written as
where  is a normal random variable, independent of 
 having mean 0 and
variance 4. The  and 
 of course, represent the heights of the man and his son,
respectively. The best prediction 
 is thus equal to
Example 6b
Suppose that if a signal value  is sent from location 
 then the signal value
received at location  is normally distributed with parameters (  1). If  the value
of the signal sent at 
 is normally distributed with parameters (
), what is the
best estimate of the signal sent if 
 the value received at 
 is equal to ?
Solution
𝐸ሾሺ𝑌െ𝑐ሻଶሿ
𝑐ൌ𝐸ሾ𝑌ሿ
𝑌
𝑌
𝑋
𝑥,
𝑋ൌ𝑥
𝑌
𝑋ൌ𝑥,
𝑥
𝑥൅1
𝑌ൌ𝑋൅1 ൅𝑒
𝑒
𝑋,
𝑋
𝑌,
𝐸ሾ𝑌||𝑋ൌ72ሿ
𝐸ሾ𝑌||𝑋
ൌ
72ሿൌ𝐸ሾ𝑋൅1 ൅𝑒||𝑋ൌ72ሿ
ൌ
73 ൅𝐸ሾ𝑒||𝑋ൌ72ሿ
ൌ
73 ൅𝐸ሺ𝑒ሻ  by independence
ൌ
73
𝑠
𝐴,
𝐵
𝑠,
𝑆,
𝐴,
𝜇, 𝜎ଶ
𝑅,
𝐵,
𝑟
554 of 848

Let us start by computing the conditional density of  given . We have
where  does not depend on . Now,
where 
 and 
 do not depend on . Hence,
where  does not depend on . Thus, we may conclude that the conditional
distribution of  the signal sent, given that  is received, is normal with mean and
variance now given by
Consequently, from Proposition 6.1
, given that the value received is  the
best estimate, in the sense of minimizing the mean square error, for the signal
sent is
Writing the conditional mean as we did previously is informative, for it shows that
it equals a weighted average of  the a priori expected value of the signal, and 
𝑆
𝑅
𝑓ௌหோሺ𝑠|𝑟ሻൌ
𝑓ௌ, ோሺ𝑠, 𝑟ሻ
𝑓ோሺ𝑟ሻ
ൌ
𝑓ௌሺ𝑠ሻ𝑓ோหௌሺ𝑟|𝑠ሻ
𝑓ோሺ𝑟ሻ
ൌ
𝐾𝑒െሺ௦െఓሻమ/ଶఙమ𝑒െሺ௥െ௦ሻమ/ଶ
𝐾
𝑠
ሺ𝑠െ𝜇ሻଶ
2𝜎ଶ
൅ሺ𝑟െ𝑠ሻଶ
2
ൌ𝑠ଶቆ1
2𝜎ଶ൅1
2ቇെ൬𝜇
𝜎ଶ൅𝑟൰𝑠൅𝐶ଵ
ൌ
1 ൅𝜎ଶ
2𝜎ଶቈ𝑠ଶെ2ቆ𝜇൅𝑟𝜎ଶ
1 ൅𝜎ଶቇ𝑠቉൅𝐶ଵ
ൌ
1 ൅𝜎ଶ
2𝜎ଶ
ቆ𝑠െሺ𝜇൅𝑟𝜎ଶሻ
1 ൅𝜎ଶቇ
ଶ
൅𝐶ଶ
𝐶ଵ
𝐶ଶ
𝑠
𝑓ௌ|ோሺ𝑠|𝑟ሻൌ𝐶 exp
⎧
⎨
⎩
⎪
⎪
െቈ𝑠െሺ𝜇൅𝑟𝜎ଶሻ
1 ൅𝜎ଶ቉ଶ
2ቆ
𝜎ଶ
1 ൅𝜎ଶቇ
⎫
⎬
⎭
⎪
⎪
𝐶
𝑠
𝑆,
𝑟
𝐸ሾ𝑆||𝑅ൌ
𝑟ሿൌ𝜇൅𝑟𝜎ଶ
1 ൅𝜎ଶ
Var ሺ𝑆||𝑅ൌ
𝑟ሻൌ
𝜎ଶ
1 ൅𝜎ଶ
𝑟,
𝐸ሾ𝑆|𝑅ൌ𝑟ሿൌ
1
1 ൅𝜎ଶ𝜇൅
𝜎ଶ
1 ൅𝜎ଶ𝑟
𝜇,
𝑟,
555 of 848

the value received. The relative weights given to  and  are in the same
proportion to each other as 1 (the conditional variance of the received signal
when  is sent) is to 
(the variance of the signal to be sent).
Example 6c
In digital signal processing, raw continuous analog data  must be quantized, or
discretized, in order to obtain a digital representation. In order to quantize the raw
data 
 an increasing set of numbers 
 such that
 and 
 is fixed, and the raw data are then
quantized according to the interval 
 in which  lies. Let us denote by 
the discretized value when 
 and let  denote the observed
discretized value—that is,
The distribution of  is given by
Suppose now that we want to choose the values 
 so as to
minimize 
 the expected mean square difference between the raw
data and their quantized version.
a. Find the optimal values 
.
For the optimal quantizer 
 show that
b. 
 so the mean square error quantizer preserves the input
mean;
c. 
Solution
(a) For any quantizer 
 upon conditioning on the value of 
 we obtain
Now, if we let
then
𝜇
𝑟
𝑠
𝜎ଶ
𝑋
𝑋,
𝑎௜, 𝑖ൌ0, േ1, േ2, …,
lim
௜→൅ஶ𝑎௜ൌ∞
lim
௜→െஶ𝑎௜ൌെ∞
ሺ𝑎௜, 𝑎௜൅ଵሿ
𝑋
𝑦௜
𝑋∈ሺ𝑎௜, 𝑎௜൅ଵሿ,
𝑌
𝑌ൌ𝑦௜  𝑖𝑓  𝑎௜൏𝑋൑𝑎௜൅ଵ
𝑌
𝑃ሼ𝑌ൌ𝑦௜ሽൌ𝐹௑ሺ𝑎௜൅ଵሻെ𝐹௑ሺ𝑎௜ሻ
𝑦௜, 𝑖ൌ0, േ1, േ2, …
𝐸ሾሺ𝑋െ𝑌ሻଶሿ,
𝑦௜, 𝑖ൌ0, േ1, …
𝑌,
𝐸ሾ𝑌ሿൌ𝐸ሾ𝑋ሿ,
Var ሺ𝑌ሻൌVar ሺ𝑋ሻെ𝐸ሾሺ𝑋െ𝑌ሻଶሿ.
𝑌,
𝑌,
𝐸ሾሺ𝑋െ𝑌ሻଶሾൌ෍
௜
𝐸ሾሺ𝑋െ𝑦௜ሻଶ|𝑎௜൏𝑋൑𝑎௜൅ଵሿ𝑃ሼ𝑎௜൏𝑋൑𝑎௜൅ଵሽ
𝐼ൌ𝑖  𝑖𝑓  𝑎௜൏𝑋൑𝑎௜൅ଵ
556 of 848

and by Proposition 6.1
, this quantity is minimized when
Now, since the optimal quantizer is given by 
 it follows that
(b) 
(c) 
It sometimes happens that the joint probability distribution of  and  is not
completely known; or if it is known, it is such that the calculation of 
 is
mathematically intractable. If, however, the means and variances of  and  and the
correlation of  and  are known, then we can at least determine the best linear
predictor of  with respect to .
To obtain the best linear predictor of  with respect to 
 we need to choose  and 
so as to minimize 
 Now,
Taking partial derivatives, we obtain
Setting Equations (6.3)
 to 0 and solving for  and  yields the solutions
𝐸ൣ൫𝑋െ𝑦௜൯
ଶห𝑎௜൏𝑋൑𝑎௜൅ଵ൧ൌ𝐸ൣ൫𝑋െ𝑦௜൯
ଶห𝐼ൌ𝑖൧
𝑦௜
ൌ
𝐸ሾ𝑋||𝐼ൌ𝑖ሿ
ൌ
𝐸ሾ𝑋||𝑎௜൏𝑋൑𝑎௜൅ଵሿ
ൌ
඲
௔೔
௔೔൅భ
𝑥𝑓௑ሺ𝑥൯𝑑𝑥
𝐹௑ሺ𝑎௜൅ଵሻെ𝐹௑ሺ𝑎௜ሻ
𝑌ൌ𝐸ሾ𝑋||𝐼ሿ,
𝐸ሾ𝑌ሿൌ𝐸ሾ𝑋ሿ
Var ሺ𝑋ሻ
ൌ
𝐸ሾVar ሺ𝑋||𝐼ሻሿ  ൅ Var ሺ𝐸ሾ𝑋||𝐼ሿሻ
ൌ
𝐸ሾ𝐸ሾሺ𝑋െ𝑌ሻଶห𝐼൧൧  ൅ Var ሺ𝑌ሻ
ൌ
𝐸ൣሺ𝑋െ𝑌ሻଶ൧  ൅ Var ሺ𝑌ሻ
𝑋
𝑌
𝐸ሾ𝑌||𝑋ൌ𝑥ሿ
𝑋
𝑌
𝑋
𝑌
𝑌
𝑋
𝑌
𝑋,
𝑎
𝑏
𝐸ሾሺ𝑌െሺ𝑎൅𝑏𝑋ሻሻଶሿ.
𝐸ቂሺ𝑌െሺ𝑎൅𝑏𝑋ሻሻଶቃ
ൌ
𝐸ൣ𝑌ଶെ2𝑎𝑌െ2𝑏𝑋𝑌൅𝑎ଶ൅2𝑎𝑏𝑋൅𝑏ଶ𝑋ଶ൧
ൌ
𝐸ൣ𝑌ଶ൧െ2𝑎𝐸ሾ𝑌ሿെ2𝑏𝐸ሾ𝑋𝑌ሿ൅𝑎ଶ
൅2𝑎𝑏𝐸ሾ𝑋ሿ൅𝑏ଶ𝐸ൣ𝑋ଶ൧
∂
∂𝑎𝐸ቂሺ𝑌െ𝑎െ𝑏𝑋ሻଶቃ
ൌ
െ2𝐸ሾ𝑌ሿ൅2𝑎൅2𝑏𝐸ሾ𝑋ሿ
∂
∂𝑏𝐸ቂሺ𝑌െ𝑎െ𝑏𝑋ሻଶቃ
ൌ
െ2𝐸ሾ𝑋𝑌ሿ൅2𝑎𝐸ሾ𝑋ሿ൅2𝑏𝐸ൣ𝑋ଶ൧
(6.3)
𝑎
𝑏
557 of 848

where 
 and 
. It is easy to verify that
the values of  and  from Equation (6.4)
 minimize 
 thus, the
best (in the sense of mean square error) linear predictor  with respect to  is
where 
 and 
.
The mean square error of this predictor is given by
We note from Equation (6.5)
 that if  is near 
 or 
 then the mean square
error of the best linear predictor is near zero.
Example 6d
An example in which the conditional expectation of  given  is linear in 
 and
hence in which the best linear predictor of  with respect to  is the best overall
predictor, is when  and  have a bivariate normal distribution. For, as shown in
Example 5d
 of Chapter 6
, in that case,
𝑏
ൌ
𝐸ሾ𝑋𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿ
𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻ
ଶൌCov ሺ𝑋, 𝑌ሻ
𝜎௫
ଶ
ൌ𝜌𝜎௬
𝜎௫
𝑎
ൌ
𝐸ሾ𝑌ሿെ𝑏𝐸ሾ𝑋ሿൌ𝐸ሾ𝑌ሿെ𝜌𝜎௬𝐸ሾ𝑋ሿ
𝜎௫
(6.4)
𝜌ൌ Correlation ሺ𝑋, 𝑌ሻ,  𝜎௬
ଶൌ Var ሺ𝑌ሻ,
𝜎௫
ଶൌ Var ሺ𝑋ሻ
𝑎
𝑏
𝐸ሾሺ𝑌െ𝑎െ𝑏𝑋ሻଶሿ;
𝑌
𝑋
𝜇௬൅𝜌𝜎௬
𝜎௫
ሺ𝑋െ𝜇௫ሻ
𝜇௬ൌ𝐸ቂ𝑌ቃ
𝜇௫ൌ𝐸ൣ𝑋൧
𝐸൥ቆ𝑌െ𝜇௬െ𝜌𝜎௬
𝜎௫
൫𝑋െ𝜇௫൯ቇ
ଶ
൩
   ൌ𝐸൤ቀ𝑌െ𝜇௬ቁ
ଶ
൨൅𝜌ଶ𝜎௬
ଶ
𝜎௫
ଶ𝐸ቂ൫𝑋െ𝜇௫൯
ଶቃെ2𝜌𝜎௬
𝜎௫
𝐸ቂቀ𝑌െ𝜇௬ቁ൫𝑋െ𝜇௫൯ቃ
   ൌ𝜎௬
ଶ൅𝜌ଶ𝜎௬
ଶെ2𝜌ଶ𝜎௬
ଶ
   ൌ𝜎௬
ଶሺ1 െ𝜌ଶሻ
(6.5)
𝜌
൅1
െ1,
𝑌
𝑋
𝑋,
𝑌
𝑋
𝑋
𝑌
𝐸ሾ𝑌|𝑋ൌ𝑥ሿൌ𝜇௬൅𝜌𝜎௬
𝜎௫
ሺ𝑥െ𝜇௫ሻ
558 of 848

The moment generating function 
 of the random variable  is defined for all real
values of  by
We call 
 the moment generating function because all of the moments of  can
be obtained by successively differentiating 
 and then evaluating the result at
. For example,
where we have assumed that the interchange of the differentiation and expectation
operators is legitimate. That is, we have assumed that
in the discrete case and
in the continuous case. This assumption can almost always be justified and, indeed,
is valid for all of the distributions considered in this book. Hence, from Equation
(7.1)
, evaluated at 
 we obtain
Similarly,
𝑀ሺ𝑡ሻ
𝑋
𝑡
𝑀ሺ𝑡ሻ
ൌ𝐸ሾ𝑒௧௑ሿ
ൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
෍
௫
𝑒௧௫𝑝ሺ𝑥ሻ
if 𝑋 is discrete with mass function 𝑝ሺ𝑥ሻ
඲
െஶ
ஶ
𝑒௧௫𝑓ሺ𝑥ሻ𝑑𝑥
if 𝑋 is continuous with density 𝑓ሺ𝑥ሻ
𝑀ሺ𝑡ሻ
𝑋
𝑀ሺ𝑡ሻ
𝑡ൌ0
𝑀′ሺ𝑡ሻൌ
𝑑
𝑑𝑡𝐸ቈ𝑒௧௑቉
ൌ
𝐸ቈ𝑑
𝑑𝑡ሺ𝑒௧௑ሻ቉
ൌ
𝐸ሾ𝑋𝑒௧௑ሿ
(7.1)
𝑑
𝑑𝑡൥෍
௫
𝑒௧௫𝑝ሺ𝑋ሻ൩ൌ෍
௫
𝑑
𝑑𝑡ሾ𝑒௧௫𝑝ሺ𝑋ሻሿ
𝑑
𝑑𝑡ቈ඲𝑒௧௫𝑓ሺ𝑋ሻ𝑑𝑥቉ൌ඲𝑑
𝑑𝑡ቈ𝑒௧௫𝑓ሺ𝑋ሻ቉𝑑𝑥
𝑡ൌ0,
𝑀′ሺ0ሻൌ𝐸ሾ𝑋ሿ
559 of 848

Thus,
In general, the  th derivative of 
 is given by
implying that
We now compute 
 for some common distributions.
Example 7a Binomial distribution with parameters n and p
If  is a binomial random variable with parameters  and  then
where the last equality follows from the binomial theorem. Differentiation yields
Thus,
𝑀″ ሺ𝑡ሻ
ൌ
𝑑
𝑑𝑡𝑀ᇱሺ𝑡ሻ
ൌ
𝑑
𝑑𝑡𝐸ሾ𝑋𝑒௧௑ሿ
ൌ
𝐸ቈ𝑑
𝑑𝑡ሺ𝑋𝑒௧௑ሻ቉
ൌ
𝐸ሾ𝑋ଶ𝑒௧௑ሿ
𝑀″ ሺ0൯ൌ𝐸ൣ𝑋ଶ൧
𝑛
𝑀ሺ𝑡ሻ
𝑀௡ሺ𝑡ሻൌ𝐸ሾ𝑋௡𝑒௧௑ሿ 𝑛൒1
𝑀௡ሺ0ሻൌ𝐸ሾ𝑋௡ሿ 𝑛൒1
𝑀ሺ𝑡ሻ
𝑋
𝑛
𝑝,
𝑀ሺ𝑡ሻ
ൌ
𝐸ሾ𝑒௧௑ሿ
ൌ
෍
௞ൌ଴
௡
𝑒௧௞ቆ𝑛
𝑘ቇ 𝑝௞ሺ1 െ𝑝ሻ௡െ௞
ൌ
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ ሺ𝑝𝑒௧ሻ௞ሺ1 െ𝑝ሻ௡െ௞
ൌሺ𝑝𝑒௧൅1 െ𝑝ሻ௡
𝑀′ሺ𝑡ሻൌ𝑛ሺ𝑝𝑒௧൅1 െ𝑝ሻ௡െଵ𝑝𝑒௧
𝐸ሾ𝑋ሿൌ𝑀′ሺ0ሻൌ𝑛𝑝
560 of 848

Differentiating a second time yields
so
The variance of  is given by
verifying the result obtained previously.
Example 7b Poisson distribution with mean λ
If  is a Poisson random variable with parameter  then
Differentiation yields
Thus,
𝑀ᇳሺ𝑡ሻൌ𝑛ሺ𝑛െ1ሻሺ𝑝𝑒௧൅1 െ𝑝ሻ௡െଶሺ𝑝𝑒௧ሻଶ൅𝑛ሺ𝑝𝑒௧൅1 െ𝑝ሻ௡െଵ𝑝𝑒௧
𝐸ൣ𝑋ଶ൧ൌ𝑀ᇳሺ0ሻൌ𝑛ሺ𝑛െ1ሻ𝑝ଶ൅𝑛𝑝
𝑋
Var ሺ𝑋ሻ
ൌ
𝐸ൣ𝑋ଶ൧െ൫𝐸ൣ𝑋൧൯
ଶ
ൌ
𝑛ሺ𝑛െ1ሻ𝑝ଶ൅𝑛𝑝െ𝑛ଶ𝑝ଶ
ൌ
𝑛𝑝ሺ1 െ𝑝ሻ
𝑋
𝜆,
𝑀ሺ𝑡ሻ
ൌ
𝐸ሾ𝑒௧௑ሿ
ൌ
෍
௡ൌ଴
ஶ
𝑒௧௡𝑒െఒ𝜆௡
𝑛!
ൌ
𝑒െఒ෍
௡ൌ଴
ஶ
ሺ𝜆𝑒௧ሻ௡
𝑛!
ൌ
𝑒െఒ𝑒ఒ௘೟
ൌ
expሼ𝜆ሺ𝑒௧െ1ሻሽ
𝑀′ሺ𝑡ሻ
ൌ
𝜆𝑒௧ expሼ𝜆ሺ𝑒௧െ1ሻሽ
𝑀ᇳሺ𝑡ሻ
ൌሺ𝜆𝑒௧ሻଶ expሼ𝜆ሺ𝑒௧െ1ሻሽ൅𝜆𝑒௧ expሼ𝜆ሺ𝑒௧െ1ሻሽ
𝐸ሾ𝑋ሿ
ൌ
𝑀ᇱሺ0ሻൌ𝜆
𝐸ൣ𝑋ଶ൧
ൌ
𝑀ᇳሺ0ሻൌ𝜆ଶ൅𝜆
Var ሺ𝑋ሻൌ
𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
ൌ
𝜆
561 of 848

Hence, both the mean and the variance of the Poisson random variable equal 
Example 7c Exponential distribution with parameter λ
We note from this derivation that for the exponential distribution, 
 is defined
only for values of  less than  Differentiation of 
 yields
Hence,
The variance of  is given by
Example 7d Normal distribution
We first compute the moment generating function of a standard normal random
variable with parameters 0 and 1. Letting  be such a random variable, we have
𝜆.
𝑀ሺ𝑡ሻൌ
𝐸ሾ𝑒௧௑ሿ
ൌ
඲
଴
ஶ
𝑒௧௫𝜆𝑒െఒ௫ 𝑑𝑥
ൌ
𝜆඲
଴
ஶ
𝑒െ൫ఒെ௧൯௫ 𝑑𝑥
ൌ
𝜆
𝜆െ𝑡 for 𝑡൏𝜆
𝑀ሺ𝑡ሻ
𝑡
𝜆.
𝑀ሺ𝑡ሻ
𝑀ᇱሺ𝑡ሻൌ
𝜆
ሺ𝜆െ𝑡ሻଶ,  𝑀ᇳሺ𝑡ሻൌ
2𝜆
ሺ𝜆െ𝑡ሻଷ
𝐸ሾ𝑋ሿൌ𝑀ᇱሺ0ሻൌ1
𝜆,  𝐸ൣ𝑋ଶ൧ൌ𝑀ᇳሺ0ሻൌ2
𝜆ଶ
𝑋
Var ሺ𝑋ሻൌ
𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶ
ൌ
1
𝜆ଶ
𝑍
562 of 848

Hence, the moment generating function of the standard normal random variable
 is given by 
 To obtain the moment generating function of an
arbitrary normal random variable, we recall (see Section 5.4
) that 
will have a normal distribution with parameters  and 
 whenever  is a
standard normal random variable. Hence, the moment generating function of
such a random variable is given by
By differentiating, we obtain
Thus,
𝑀௓ሺ𝑡ሻ
ൌ𝐸ሾ𝑒௧௓ሿ
ൌ
1
2𝜋
√
඲
െஶ
ஶ
𝑒௧௫𝑒െ௫మ/ଶ𝑑𝑥
ൌ
1
2𝜋
√
඲
െஶ
ஶ
expቊെሺ𝑥ଶെ2𝑡𝑥ሻ
2
ቋ𝑑𝑥
ൌ
1
2𝜋
√
඲
െஶ
ஶ
exp൝െሺ𝑥െ𝑡ሻଶ
2
൅𝑡ଶ
2 ൡ𝑑𝑥
ൌ𝑒௧మ/ଶ
1
2𝜋
√
඲
െஶ
ஶ
𝑒െ൫௫െ௧൯మ/ଶ𝑑𝑥
ൌ𝑒௧మ/ଶ
𝑍
𝑀௓ቀ𝑡ቁൌ𝑒௧మ/ଶ.
𝑋ൌ𝜇൅𝜎𝑍
𝜇
𝜎ଶ
𝑍
𝑀௑ሺ𝑡ሻ
ൌ𝐸ሾ𝑒௧௑ሿ
ൌ𝐸ቂ𝑒௧൫ఓ൅ఙ௓൯ቃ
ൌ𝐸ሾ𝑒௧ఓ𝑒௧ఙ௓ሿ
ൌ𝑒௧ఓ𝐸ሾ𝑒௧ఙ௓ሿ
ൌ𝑒௧ఓ𝑀௓ሺ𝑡𝜎ሻ
ൌ𝑒௧ఓ𝑒൫௧ఙ൯మ/ଶ
ൌexpቊ𝜎ଶ𝑡ଶ
2
൅𝜇𝑡ቋ
𝑀ᇱ
௑ሺ𝑡ሻൌሺ𝜇൅𝑡𝜎ଶሻ expቊ𝜎ଶ𝑡ଶ
2
൅𝜇𝑡ቋ
𝑀ᇳ
௑ሺ𝑡ሻൌሺ𝜇൅𝑡𝜎ଶሻ
ଶ expቊ𝜎ଶ𝑡ଶ
2
൅𝜇𝑡ቋ൅𝜎ଶ expቊ𝜎ଶ𝑡ଶ
2
൅𝜇𝑡ቋ
563 of 848

implying that
Tables 7.1
 and 7.2
 (on page 364) give the moment generating functions for
some common discrete and continuous distributions.
Table 7.1 Discrete probability distribution.
Table 7.2 Continuous probability distribution.
𝐸ሾ𝑋ሿ
ൌ
𝑀ᇱሺ0ሻൌ𝜇
𝐸ൣ𝑋ଶ൧
ൌ
𝑀ᇳሺ0ሻൌ𝜇ଶ൅𝜎ଶ
Varሺ𝑋ሻ
ൌ
𝐸ൣ𝑋ଶ൧െ𝐸൫ൣ𝑋൧൯
ଶ
ൌ
𝜎ଶ
Probability mass
function, 
Moment
generating
function, 
Mean
Variance
Binomial
with
parameters
Poisson
with
parameter
Geometric
with
parameter
Negative
binomial
with
parameters
𝑝ሺ𝑥ሻ
𝑀ሺ𝑡ሻ
𝐧, 𝐩;
𝟎൑𝐩൑𝟏
ቆ𝑛
𝑥ቇ𝑝௫ሺ1 െ𝑝ሻ௡െ௫
ሺ𝑝𝑒௧൅1 െ𝑝ሻ௡
𝑛𝑝
𝑛𝑝ሺ1 െ𝑝ሻ
𝑥ൌ0, 1, …, 𝑛
𝐥൐𝟎
𝑒െఒ𝜆௫
𝑥!
𝑥ൌ0, 1, 2, …
expሼ𝜆ሺ𝑒௧െ1ሻሽ
𝜆
𝜆
𝟎൑𝐩൑𝟏
𝑝ሺ1 െ𝑝ሻ௫െଵ
𝑥ൌ1, 2, …
𝑝𝑒௧
1 െሺ1 െ𝑝ሻ𝑒௧
1
𝑝
1 െ𝑝
𝑝ଶ
𝐫, 𝐩;
ቆ𝑛െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௡െ௥
𝑛ൌ𝑟, 𝑟൅1, …
ቈ
𝑝𝑒௧
1 െሺ1 െ𝑝ሻ𝑒௧቉
௥
𝑟
𝑝
𝑟ሺ1 െ𝑝ሻ
𝑝ଶ
Probability density function, 
Moment
Mean
𝑓ሺ𝑥ሻ
564 of 848

An important property of moment generating functions is that the moment generating
function of the sum of independent random variables equals the product of the
individual moment generating functions. To prove this, suppose that  and  are
independent and have moment generating functions 
 and 
 respectively.
Then 
 the moment generating function of 
 is given by
where the next-to-last equality follows from Proposition 4.1
, since  and  are
independent.
Another important result is that the moment generating function uniquely determines
the distribution. That is, if 
 exists and is finite in some region about 
 then
the distribution of  is uniquely determined. For instance, if
then it follows from Table 7.1
 that  is a binomial random variable with
generating
function, 
Uniform
over 
Exponential
with
parameter
Gamma
with
parameters
Normal
with
𝑀ሺ𝑡ሻ
ሺ𝐚,  𝐛ሻ
𝑓ሺ𝑥ሻൌቐ
1
𝑏െ𝑎
𝑎൏𝑥൏𝑏
0
otherwise
𝑒௧௕െ𝑒௧௔
𝑡ሺ𝑏െ𝑎ሻ
𝑎൅𝑏
2
𝐥൐0
𝑓ሺ𝑥ሻൌቊ𝜆𝑒െఒ௫
𝑥൒0
0
𝑥൏0
𝜆
𝜆െ𝑡
1
𝜆
ሺ𝐬, 𝐥ሻ, 𝐥൐0
𝑓ሺ𝑥ሻൌ൞
𝜆𝑒െఒ௫ሺ𝜆𝑥ሻ௦െଵ
𝛤ሺ𝑠ሻ
𝑥൒0
0
𝑥൏0
ቆ
𝜆
𝜆െ𝑡ቇ
௦
𝑠
𝜆
𝑓ቆ𝑥ቇൌ
1
2𝜋
√
𝜎𝑒െ൫௫െఓ൯మ/ଶఙమ  െ∞൏𝑥൏∞
expቊ𝜇𝑡൅𝜎ଶ𝑡ଶ
2 ቋ
𝜇
𝑋
𝑌
𝑀௑ሺ𝑡ሻ
𝑀௒ሺ𝑡ሻ,
𝑀௑൅௒ሺ𝑡ሻ,
𝑋൅𝑌,
𝑀௑൅௒ሺ𝑡ሻ
ൌ𝐸ቂ𝑒௧൫௑൅௒൯ቃ
ൌ𝐸ሾ𝑒௧௑𝑒௧௒ሿ
ൌ𝐸ሾ𝑒௧௑ሿ𝐸ሾ𝑒௧௒ሿ
ൌ𝑀௑ሺ𝑡ሻ𝑀௒ሺ𝑡ሻ
𝑋
𝑌
𝑀௑ሺ𝑡ሻ
𝑡ൌ0,
𝑋
𝑀௑ሺ𝑡ሻൌቆ1
2ቇ
ଵ଴
ሺ𝑒௧൅1ሻଵ଴,
𝑋
565 of 848

parameters 10 and 
Example 7e
Suppose that the moment generating function of a random variable  is given by
 What is 
Solution
We see from Table 7.1
 that 
 is the moment generating
function of a Poisson random variable with mean 3. Hence, by the one-to-one
correspondence between moment generating functions and distribution
functions, it follows that  must be a Poisson random variable with mean 3. Thus,
Example 7f Sums of independent binomial random variables
If  and  are independent binomial random variables with parameters 
 and
 respectively, what is the distribution of 
Solution
The moment generating function of 
 is given by
However, 
 is the moment generating function of a binomial
random variable having parameters 
 and  Thus, this must be the
distribution of 
Example 7g Sums of independent poisson random variables
Calculate the distribution of 
 when  and  are independent Poisson
random variables with means respectively 
 and 
Solution
Hence, 
 is Poisson distributed with mean 
 verifying the result given
in Example 3e
 of Chapter 6
.
1
2 .
𝑋
𝑀ቀ𝑡ቁൌ𝑒ଷ൫௘೟െଵ൯.
𝑃ሼ𝑋ൌ0ሽ?
𝑀ቀ𝑡ቁൌ𝑒ଷ൫௘೟െଵ൯
𝑋
𝑃ሼ𝑋ൌ0ሽൌ𝑒െଷ.
𝑋
𝑌
ሺ𝑛, 𝑝ሻ
ሺ𝑚, 𝑝ሻ,
𝑋൅𝑌?
𝑋൅𝑌
𝑀௑൅௒ሺ𝑡ሻൌ𝑀௑ሺ𝑡ሻ𝑀௒ሺ𝑡ሻൌሺ𝑝𝑒௧൅1 െ𝑝ሻ௡ሺ𝑝𝑒௧൅1 െ𝑝ሻ௠
ൌሺ𝑝𝑒௧൅1 െ𝑝ሻ௠൅௡
ሺ𝑝𝑒௧൅1 െ𝑝ሻ௠൅௡
𝑚൅𝑛
𝑝.
𝑋൅𝑌.
𝑋൅𝑌
𝑋
𝑌
𝜆ଵ
𝜆ଶ.
𝑀௑൅௒ሺ𝑡ሻൌ
𝑀௑ሺ𝑡ሻ𝑀௒ሺ𝑡ሻ
ൌ
expሼ𝜆ଵሺ𝑒௧െ1ሻሽ expሼ𝜆ଶሺ𝑒௧െ1ሻሽ
ൌ
expሼሺ𝜆ଵ൅𝜆ଶሻሺ𝑒௧െ1ሻሽ
𝑋൅𝑌
𝜆ଵ൅𝜆ଶ,
566 of 848

Example 7h Sums of independent normal random variables
Show that if  and  are independent normal random variables with respective
parameters 
 and 
 then 
 is normal with mean 
 and
variance 
Solution
which is the moment generating function of a normal random variable with mean
 and variance 
 The desired result then follows because the
moment generating function uniquely determines the distribution.
Example 7i
Compute the moment generating function of a chi-squared random variable with
 degrees of freedom.
Solution
We can represent such a random variable as
where 
 are independent standard normal random variables. Let 
 be
its moment generating function. Then, by the preceding,
where  is a standard normal random variable. Now,
𝑋
𝑌
൫𝜇ଵ, 𝜎ଵ
ଶ൯
൫𝜇ଶ, 𝜎ଶ
ଶ൯,
𝑋൅𝑌
𝜇ଵ൅𝜇ଶ
𝜎ଵ
ଶ൅𝜎ଶ
ଶ.
𝑀௑൅௒ሺ𝑡ሻ
ൌ
𝑀௑ሺ𝑡ሻ𝑀௒ሺ𝑡ሻ
ൌ
expቊ𝜎ଵ
ଶ𝑡ଶ
2
൅𝜇ଵ𝑡ቋexpቊ𝜎ଶ
ଶ𝑡ଶ
2
൅𝜇ଶ𝑡ቋ
ൌ
expቊ൫𝜎ଵ
ଶ൅𝜎ଶ
ଶ൯𝑡ଶ
2
൅൫𝜇ଵ൅𝜇ଶ൯𝑡ቋ
𝜇ଵ൅𝜇ଶ
𝜎ଵ
ଶ൅𝜎ଶ
ଶ.
𝑛
𝑍ଵ
ଶ൅⋯൅𝑍௡
ଶ
𝑍ଵ, …, 𝑍௡
𝑀ሺ𝑡ሻ
𝑀ቀ𝑡ቁൌቀ𝐸ቂ𝑒௧௓మቃቁ
௡
𝑍
𝐸ቂ𝑒௧௓మቃ
ൌ
1
2𝜋
√
඲
െஶ
ஶ
𝑒௧௫మ𝑒െ௫మ/ଶ𝑑𝑥
ൌ
1
2𝜋
√
඲
െஶ
ஶ
𝑒െ௫మ/ଶఙమ𝑑𝑥 where 𝜎ଶൌቆ1 െ2𝑡ቇ
െଵ
ൌ
𝜎
ൌሺ1 െ2𝑡ሻ
െଵ/ଶ
567 of 848

where the next-to-last equality uses the fact that the normal density with mean 0
and variance 
 integrates to 1. Therefore,
Example 7j Moment generating function of the sum of a random number of
random variables
Let 
 be a sequence of independent and identically distributed random
variables, and let  be a nonnegative, integer-valued random variable that is
independent of the sequence 
 We want to compute the moment
generating function of
(In Example 5d
,  was interpreted as the amount of money spent in a store
on a given day when both the amount spent by a customer and the number of
customers are random variables.)
To compute the moment generating function of 
 we first condition on  as
follows:
where
Hence,
Thus,
𝜎ଶ
𝑀ሺ𝑡ሻൌሺ1 െ2𝑡ሻ
െ௡/ଶ
𝑋ଵ, 𝑋ଶ, …
𝑁
𝑋, 𝑖൒1.
𝑌ൌ෍
௜ൌଵ
ே
𝑋௜
𝑌
𝑌,
𝑁
𝐸቎exp ቐ𝑡෍
ଵ
ே
𝑋௜ቑቮ𝑁ൌ𝑛቏
ൌ
𝐸 ቎exp ቐ𝑡෍
ଵ
ே
𝑋௜ቑቮ𝑁ൌ𝑛቏
ൌ
𝐸 ቎exp ቐ𝑡෍
ଵ
ே
𝑋௜ቑ቏
ൌሾ𝑀௑ሺ𝑡ሻሿ௡
𝑀௑ሺ𝑡ሻൌ𝐸ሾ𝑒௧௑೔ሿ
𝐸ሾ𝑒௧௒|𝑁ሿൌሺ𝑀௑ሺ𝑡ሻሻே
𝑀௒ሺ𝑡ሻൌ𝐸ቂሺ𝑀௑ሺ𝑡ሻሻேቃ
568 of 848

The moments of  can now be obtained upon differentiation, as follows:
So
verifying the result of Example 5d
. (In this last set of equalities, we have used
the fact that 
Also,
so
Hence, from Equations (7.2)
 and (7.3
), we have
Example 7k
Let  denote a uniform random variable on (0, 1), and suppose that conditional
on 
 the random variable  has a binomial distribution with parameters 
and  In Example 5k
, we showed that  is equally likely to take on any of the
𝑌
𝑀௒
ᇱሺ𝑡ሻൌ𝐸ሾ𝑁ሺ𝑀௑ሺ𝑡ሻሻேെଵ𝑀௑
ᇱሺ𝑡ሻሿ
𝐸ሾ𝑌ሿ
ൌ𝑀ᇱ
௒ሺ0ሻ
ൌ𝐸ቂ𝑁ሺ𝑀௑ሺ0ሻሻேെଵ𝑀ᇱ
௑ሺ0ሻቃ
ൌ𝐸ሾ𝑁𝐸ሾ𝑋ሿሿ
ൌ𝐸ሾ𝑁ሿ𝐸ሾ𝑋ሿ
(7.2)
𝑀௑ሺ0ሻൌ𝐸ሾ𝑒଴௑ሿൌ1.ሻ
𝑀ᇳ
௒ሺ𝑡ሻൌ𝐸ቂ𝑁ሺ𝑁െ1ሻሺ𝑀௑ሺ𝑡ሻሻேെଶሺ𝑀ᇱ
௑ሻ
ଶ൅𝑁ሺ𝑀௑ሺ𝑡ሻሻேെଵ𝑀ᇳ
௑ሺ𝑡ሻቃ
𝐸ൣ𝑌ଶ൧
ൌ
𝑀ᇳ
௒ሺ0ሻ
ൌ
𝐸ሾ𝑁ሺ𝑁െ1ሻሺ𝐸ሾ𝑋ሿሻଶ൅𝑁𝐸ൣ𝑋ଶ൧
ൌሺ𝐸ሾ𝑋ሿሻଶ൫𝐸ൣ𝑁ଶ൧െ𝐸ሾ𝑁ሿ൯൅𝐸ሾ𝑁ሿ𝐸ൣ𝑋ଶ൧
ൌ
𝐸ሾ𝑁ሿቀ𝐸ൣ𝑋ଶ൧െሺ𝐸ሾ𝑋ሿሻଶቁ൅ሺ𝐸ሾ𝑋ሿሻଶ𝐸ൣ𝑁ଶ൧
ൌ
𝐸ሾ𝑁ሿVarሺ𝑋ሻ൅ሺ𝐸ሾ𝑋ሿሻଶ𝐸ൣ𝑁ଶ൧
(7.3)
Varሺ𝑌ሻ
ൌ
𝐸ሾ𝑁ሿVarሺ𝑋ሻ൅ሺ𝐸ሾ𝑋ሿሻଶቀ𝐸ൣ𝑁ଶ൧െሺ𝐸ሾ𝑁ሿሻଶቁ
ൌ
𝐸ሾ𝑁ሿVarሺ𝑋ሻ൅ሺ𝐸ሾ𝑋ሿሻଶVarሺ𝑁ሻ
𝑌
𝑌ൌ𝑝,
𝑋
𝑛
𝑝.
𝑋
569 of 848

values 
 Establish this result by using moment generating functions.
Solution
To compute the moment generating function of 
 start by conditioning on the
value of 
 Using the formula for the binomial moment generating function gives
Now,  is uniform on (0, 1), so, upon taking expectations, we obtain
Because the preceding is the moment generating function of a random variable
that is equally likely to be any of the values 
 the desired result follows
from the fact that the moment generating function of a random variable uniquely
determines its distribution.
It is also possible to define the joint moment generating function of two or more
random variables. This is done as follows: For any  random variables 
 the
joint moment generating function, 
 is defined, for all real values of
 by
The individual moment generating functions can be obtained from 
 by
letting all but one of the ‘s be 0. That is,
where the  is in the th place.
0, 1, …, 𝑛.
𝑋,
𝑌.
𝐸ሾ𝑒௧௑||𝑌ൌ𝑝ሿൌሺ𝑝𝑒௧൅1 െ𝑝ሻ
௡
𝑌
𝐸ሾ𝑒௧௑ሿ
ൌ
඲
଴
ଵ
ሺ𝑝𝑒௧൅1 െ𝑝ሻ௡ 𝑑𝑝
ൌ
1
𝑒௧െ1 ඲
ଵ
௘೟
𝑦௡𝑑𝑦 ሺby the substitution 𝑦ൌ𝑝𝑒௧൅1 െ𝑝ሻ
ൌ
1
𝑛൅1
𝑒௧൫௡൅ଵ൯െ1
𝑒௧െ1
ൌ
1
𝑛൅1 ሺ1 ൅𝑒௧൅𝑒ଶ௧൅⋯൅𝑒௡௧ሻ
0, 1, …, 𝑛,
𝑛
𝑋ଵ, …, 𝑋௡,
𝑀ሺ𝑡ଵ, …, 𝑡௡ሻ,
𝑡ଵ, …, 𝑡௡,
𝑀൫𝑡ଵ, …, 𝑡௡൯ൌ𝐸ൣ𝑒௧భ௑భ൅⋅⋅⋅൅௧೙௑೙൧
𝑀ሺ𝑡ଵ, …, 𝑡௡ሻ
𝑡௝
𝑀௑೔൫𝑡൯ൌ𝐸ൣ𝑒௧௑೔൧ൌ𝑀൫0, …, 0, 𝑡, 0, …, 0൯
𝑡
𝑖
570 of 848

It can be proven (although the proof is too advanced for this text) that the joint
moment generating function 
 uniquely determines the joint distribution of
 This result can then be used to prove that the  random variables 
are independent if and only if
For the proof in one direction, if the  random variables are independent, then
For the proof in the other direction, if Equation (7.4
) is satisfied, then the joint
moment generating function 
 is the same as the joint moment generating
function of  independent random variables, the th of which has the same
distribution as 
 As the joint moment generating function uniquely determines the
joint distribution, this must be the joint distribution; hence, the random variables are
independent.
Example 7l
Let  and  be independent normal random variables, each with mean  and
variance 
 In Example 7a
 of Chapter 6
, we showed that 
 and 
are independent. Let us now establish that 
 and 
 are independent by
computing their joint moment generating function:
But we recognize the preceding as the joint moment generating function of the
sum of a normal random variable with mean 
 and variance 
 and an
independent normal random variable with mean 0 and variance 
 Because the
joint moment generating function uniquely determines the joint distribution, it
follows that 
 and 
 are independent normal random variables.
𝑀ሺ𝑡ଵ, …, 𝑡௡ሻ
𝑋ଵ, …, 𝑋௡.
𝑛
𝑋ଵ, …, 𝑋௡
𝑀ሺ𝑡ଵ, …, 𝑡௡ሻൌ𝑀௑భሺ𝑡ଵሻ⋯𝑀௑೙ሺ𝑡௡ሻ
(7.4)
𝑛
𝑀ሺ𝑡ଵ, …, 𝑡௡ሻ
ൌ
𝐸ቂ𝑒൫௧భ௑భ൅⋅⋅⋅൅௧೙௑೙൯ቃ
ൌ
𝐸ሾ𝑒௧భ௑భ⋯𝑒௧೙௑೙ሿ
ൌ
𝐸ሾ𝑒௧భ௑భሿ⋯𝐸ሾ𝑒௧೙௑೙ሿ by independence
ൌ
𝑀௑భ൫𝑡ଵ൯⋯𝑀௑೙൫𝑡௡൯
𝑀ሺ𝑡ଵ, …, 𝑡௡ሻ
𝑛
𝑖
𝑋௜.
𝑋
𝑌
𝜇
𝜎ଶ.
𝑋൅𝑌
𝑋െ𝑌
𝑋൅𝑌
𝑋െ𝑌
𝐸ቂ𝑒௧൫௑൅௒൯൅௦൫௑െ௒൯ቃ
ൌ
𝐸ቂ𝑒൫௧൅௦൯௑൅൫௧െ௦൯௒ቃ
ൌ
𝐸ቂ𝑒൫௧൅௦൯௑ቃ𝐸ቂ𝑒൫௧െ௦൯௒ቃ
ൌ
𝑒ఓ൫௧൅௦൯൅ఙమ൫௧൅௦൯మ/ଶ𝑒ఓ൫௧െ௦൯൅ఙమ൫௧െ௦൯మ/ଶ
ൌ
𝑒ଶఓ௧൅ఙమ௧మ𝑒ఙమ௦మ
2𝜇
2𝜎ଶ
2𝜎ଶ.
𝑋൅𝑌
𝑋െ𝑌
571 of 848

In the next example, we use the joint moment generating function to verify a result
that was established in Example 2b
 of Chapter 6
.
Example 7m
Suppose that the number of events that occur is a Poisson random variable with
mean  and that each event is independently counted with probability  Show
that the number of counted events and the number of uncounted events are
independent Poisson random variables with respective means 
 and 
Solution
Let  denote the total number of events, and let 
 denote the number of them
that are counted. To compute the joint moment generating function of 
 the
number of events that are counted, and 
 the number that are uncounted,
start by conditioning on  to obtain
which follows because, conditional on 
 is a binomial random variable
with parameters  and  Hence,
Taking expectations of both sides of this equation yields
Now, since  is Poisson with mean  it follows that 
 Therefore,
for any positive value  we see (by letting 
 that 
 Thus,
As the preceding is the joint moment generating function of independent Poisson
random variables with respective means 
 and 
 the result is proven.
𝜆
𝑝.
𝜆𝑝
𝜆ሺ1 െ𝑝ሻ.
𝑋
𝑋௖
𝑋௖,
𝑋െ𝑋௖,
𝑋
𝐸ቂ𝑒௦௑೎൅௧൫௑െ௑೎൯|𝑋ൌ𝑛ቃ
ൌ𝑒௧௡𝐸ቂ𝑒൫௦െ௧൯௑೎|𝑋ൌ𝑛ቃ
ൌ𝑒௧௡ሺ𝑝𝑒௦െ௧൅1 െ𝑝ሻ௡
ൌሺ𝑝𝑒௦൅ሺ1 െ𝑝ሻ𝑒௧ሻ௡
𝑋ൌ𝑛, 𝑋௖
𝑛
𝑝.
𝐸ቂ𝑒௦௑೎൅௧൫௑െ௑೎൯|𝑋ቃൌሺ𝑝𝑒௦൅ሺ1 െ𝑝ሻ𝑒௧ሻ௑
𝐸ቂ𝑒௦௑೎൅௧൫௑െ௑೎൯ቃൌ𝐸ቂቀ𝑝𝑒௦൅ቀ1 െ𝑝ቁ𝑒௧ቁ
௑
ቃ
𝑋
𝜆,
𝐸ቂ𝑒௧௑ቃൌ𝑒ఒ൫௘೟െଵ൯.
𝑎
𝑎ൌ𝑒௧ሻ
𝐸ቂ𝑎௑ቃൌ𝑒ఒ൫௔െଵ൯.
𝐸ቂ𝑒௦௑೎൅௧൫௑െ௑೎൯ቃ
ൌ𝑒ఒ൫௣௘ೞ൅൫ଵെ௣൯௘೟െଵ൯
ൌ𝑒ఒ௣൫௘ೞെభ൯𝑒ఒ൫ଵെ௣൯൫௘೟െଵ൯
𝜆𝑝
𝜆ሺ1 െ𝑝ሻ,
572 of 848

Let 
 be a set of  independent standard normal random variables. If, for
some constants 
 and 
then the random variables 
 are said to have a multivariate normal
distribution.
From the fact that the sum of independent normal random variables is itself a normal
random variable, it follows that each 
 is a normal random variable with mean and
variance given, respectively, by
Let us now consider
the joint moment generating function of 
 The first thing to note is that since
 is itself a linear combination of the independent normal random variables
 it is also normally distributed. Its mean and variance are
𝑍ଵ, …, 𝑍௡
𝑛
𝑎௜௝, 1 ൑𝑖൑𝑚, 1 ൑𝑗൑𝑛,
𝜇௜, 1 ൑𝑖൑𝑚,
𝑋ଵ
ൌ
𝑎ଵଵ𝑍ଵ൅⋯൅𝑎ଵ௡𝑍௡൅𝜇ଵ
𝑋ଶ
ൌ
𝑎ଶଵ𝑍ଵ൅⋯൅𝑎ଶ௡𝑍௡൅𝜇ଶ
⋮
𝑋௜
ൌ
𝑎௜ଵ𝑍ଵ൅⋯൅𝑎௜௡𝑍௡൅𝜇௜
⋮
𝑋௠
ൌ
𝑎௠ଵ𝑍ଵ൅⋯൅𝑎௠௡𝑍௡൅𝜇௠
𝑋ଵ, …, 𝑋௠
𝑋௜
𝐸ሾ𝑋௜ሿ
ൌ
𝜇௜
Varሺ𝑋௜ሻ
ൌ
෍
௝ൌଵ
௡
𝑎௜௝
ଶ
𝑀ሺ𝑡ଵ, …, 𝑡௠ሻൌ𝐸ሾexp ሼ𝑡ଵ𝑋ଵ൅⋯൅𝑡௠𝑋௠ሽሿ
𝑋ଵ, …, 𝑋௠.
෍
௜ൌଵ
௠
𝑡௜𝑋௜
𝑍ଵ, …, 𝑍௡,
𝐸቎෍
௜ൌଵ
௠
𝑡௜𝑋௜቏ൌ෍
௜ൌଵ
௠
𝑡௜𝜇௜
573 of 848

and
Now, if  is a normal random variable with mean  and variance 
 then
Thus,
which shows that the joint distribution of 
 is completely determined from a
knowledge of the values of 
 and 
It can be shown that when 
 the multivariate normal distribution reduces to the
bivariate normal.
Example 8a
Find 
 for bivariate normal random variables  and  having parameters
Solution
Because 
 is normal with mean
and variance
we obtain
Varቌ෍
௜ൌଵ
௠
𝑡௜𝑋௜ቍൌ
Covቌ෍
௜ൌଵ
௠
𝑡௜𝑋௜, ෍
௝ൌଵ
௠
𝑡௝𝑋௝ቍ
ൌ
෍
௜ൌଵ
௠
෍
௝ൌଵ
௠
𝑡௜𝑡௝Cov൫𝑋௜, 𝑋௝൯
𝑌
𝜇
𝜎ଶ,
𝐸ሾ𝑒௒ሿൌ𝑀௒ሺ𝑡ሻ|௧ൌଵൌ𝑒ఓ൅ఙమ/ଶ
𝑀ሺ𝑡ଵ, …, 𝑡௠ሻൌexpቐ෍
௜ൌଵ
௠
𝑡௜𝜇௜൅1
2 ෍
௜ൌଵ
௠
෍
௜ൌଵ
௠
𝑡௜𝑡௝Cov൫𝑋௜, 𝑋௝൯ቑ
𝑋ଵ, …, 𝑋௠
𝐸ሾ𝑋௜ሿ
Cov൫𝑋௜, 𝑋௝൯, 𝑖, 𝑗ൌ1, …, 𝑚.
𝑚ൌ2,
𝑃ሺ𝑋൏𝑌ሻ
𝑋
𝑌
𝜇௫ൌ𝐸ሾ𝑋ሿ, 𝜇௬ൌ𝐸ሾ𝑌ሿ, 𝜎௫
ଶൌVarሺ𝑋ሻ, 𝜎௬
ଶൌVarሺ𝑌ሻ, 𝜌ൌCorrሺ𝑋, 𝑌ሻ
𝑋െ𝑌
𝐸ቂ𝑋െ𝑌ቃൌ𝜇௫െ𝜇௬
Varሺ𝑋െ𝑌ሻൌ
Varሺ𝑋ሻ൅Varሺെ𝑌ሻ൅2Covሺ𝑋, െ𝑌ሻ
ൌ
𝜎௫
ଶ൅𝜎௬
ଶെ2𝜌𝜎௫𝜎௬
574 of 848

Example 8b
Suppose that the conditional distribution of 
 given that 
 is normal with
mean  and variance  Moreover, suppose that  itself is a normal random
variable with mean  and variance 
 Find the conditional distribution of  given
that 
Solution
Rather than using and then simplifying Bayes’s formula, we will solve this
problem by first showing that 
 has a bivariate normal distribution. To do so,
note that the joint density function of 
 can be written as
where 
 is a normal density with mean  and variance 
However, if we
let  be a standard normal random variable that is independent of 
 then the
conditional distribution of 
 given that 
 is also normal with mean  and
variance 
Consequently, the joint density of 
 is the same as that of 
Because the former joint density is clearly bivariate normal (since 
 and 
are both linear combinations of the independent normal random variables  and
, it follows that 
 has a bivariate normal distribution. Now,
and
𝑃ሼ𝑋൏𝑌ሽൌ
𝑃ሼ𝑋െ𝑌൏0ሽ
ൌ
𝑃
⎧
⎨
⎩
⎪
⎪
𝑋െ𝑌െቀ𝜇௫െ𝜇௬ቁ
𝜎௫
ଶ൅𝜎௬
ଶെ2𝜌𝜎௫𝜎௬
ට
൏
െቀ𝜇௫െ𝜇௬ቁ
𝜎௫
ଶ൅𝜎௬
ଶെ2𝜌𝜎௫𝜎௬
ට
⎫
⎬
⎭
⎪
⎪
ൌ
Φ⎛
⎝
⎜⎜
𝜇௬െ𝜇௫
𝜎௫
ଶ൅𝜎௬
ଶെ2𝜌𝜎௫𝜎௬
ට
⎞
⎠
⎟⎟
𝑋,
Θ ൌ𝜃,
𝜃
1.
Θ
𝜇
𝜎ଶ.
Θ
𝑋ൌ𝑥.
𝑋, Θ
𝑋, Θ
𝑓௑,Θሺ𝑥, 𝜃ሻൌ𝑓௑หΘሺ𝑥|𝜃ሻ𝑓Θሺ𝜃ሻ
𝑓௑หΘሺ𝑥|𝜃ሻ
𝜃
1 .
𝑍
Θ,
𝑍൅Θ,
Θ ൌ𝜃,
𝜃
1 .
𝑍൅Θ, Θ
𝑋, Θ .
𝑍൅Θ
Θ
𝑍
Θሻ
𝑋, Θ
𝐸ሾ𝑋ሿൌ
𝐸ሾ𝑍൅Θሿൌ𝜇
Varሺ𝑋ሻൌ
Varሺ𝑍൅Θሻൌ1 ൅𝜎ଶ
575 of 848

Because 
 has a bivariate normal distribution, the conditional distribution of 
given that 
 is normal with mean
and variance
Let 
 be independent normal random variables, each with mean  and
variance 
 Let 
 denote their sample mean. Since the sum of
independent normal random variables is also a normal random variable, it follows
that  is a normal random variable with (from Examples 2c
 and 4a
) expected
value  and variance 
Now, recall from Example 4e
 that
Also, note that since 
 are all linear combinations of the
independent standard normals 
 it follows that
 has a joint distribution that is multivariate normal. If we let  be a
normal random variable, with mean  and variance 
 that is independent of the
𝜌
ൌ
Corrሺ𝑋, Θሻ
ൌ
Corrሺ𝑍൅Θ, Θሻ
ൌ
Covሺ𝑍൅Θ, Θሻ
Varሺ𝑍൅ΘሻVarሺΘሻ
ඥ
ൌ
𝜎
1 ൅𝜎ଶ
√
𝑋, Θ
Θ,
𝑋ൌ𝑥,
𝐸ሾΘ|𝑋ൌ𝑥ሿ
ൌ
𝐸ሾΘሿ൅𝜌
VarሺΘሻ
Varሺ𝑋ሻ
ඨ
ሺ𝑥െ𝐸ሾ𝑋ሿሻ
ൌ
𝜇൅
𝜎ଶ
1 ൅𝜎ଶሺ𝑥െ𝜇ሻ
VarሺΘ|𝑋ൌ𝑥ሻൌ
VarሺΘሻሺ1 െ𝜌ଶሻ
ൌ
𝜎ଶ
1 ൅𝜎ଶ
𝑋ଵ, …, 𝑋௡
𝜇
𝜎ଶ.
𝑋̅ ̅̅ ̅ൌ෍
௜ൌଵ
௡
𝑋௜/𝑛
𝑋̅ ̅̅ ̅
𝜇
𝜎ଶ/𝑛.
Covሺ𝑋̅ ̅̅ ̅, 𝑋௜െ𝑋̅ ̅̅ ̅ሻൌ0,  𝑖ൌ1, …, 𝑛
(8.1)
𝑋̅ ̅̅ ̅, 𝑋ଵെ𝑋̅ ̅̅ ̅, 𝑋ଶെ𝑋̅ ̅̅ ̅, …, 𝑋௡െ𝑋̅ ̅̅ ̅
ሺ𝑋௜െ𝜇ሻ/𝜎, 𝑖ൌ1, …, 𝑛,
𝑋̅ ̅̅ ̅, 𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛
𝑌
𝜇
𝜎ଶ/𝑛,
576 of 848

 then 
 also has a multivariate normal distribution and,
indeed, because of Equation (8.1)
, has the same expected values and
covariances as the random variables 
 But since a multivariate
normal distribution is determined completely by its expected values and covariances,
it follows that 
 and 
 have the same joint
distribution, thus showing that  is independent of the sequence of deviations
Since  is independent of the sequence of deviations 
 it is also
independent of the sample variance
Since we already know that  is normal with mean  and variance 
 it remains
only to determine the distribution of 
 To accomplish this, recall, from Example
4a
, the algebraic identity
Upon dividing the preceding equation by 
 we obtain
Now,
is the sum of the squares of  independent standard normal random variables and so
is a chi-squared random variable with  degrees of freedom. Hence, from Example
7i
, its moment generating function is 
 Also, because
𝑋௜, 𝑖ൌ1, …, 𝑛,
𝑌, 𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛
𝑋̅ ̅̅ ̅, 𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛.
𝑌, 𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛
𝑋̅ ̅̅ ̅, 𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛
𝑋̅ ̅̅ ̅
𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛.
𝑋̅ ̅̅ ̅
𝑋௜െ𝑋̅ ̅̅ ̅, 𝑖ൌ1, …, 𝑛,
𝑆ଶ≡෍
௜ൌଵ
௡
ሺ𝑋௜െ𝑋̅ ̅̅ ̅ሻ
ଶ/ሺ𝑛െ1ሻ.
𝑋̅ ̅̅ ̅
𝜇
𝜎ଶ/𝑛,
𝑆ଶ.
ሺ𝑛െ1ሻ𝑆ଶ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝑋̅ ̅̅ ̅ሻ
ଶ
ൌ
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇ሻଶെ𝑛ሺ𝑋̅ ̅̅ ̅െ𝜇ሻ
ଶ
𝜎ଶ,
ሺ𝑛െ1ሻ𝑆ଶ
𝜎ଶ
൅ቆ𝑋̅ ̅̅ ̅െ𝜇
𝜎/ 𝑛
√ቇ
ଶ
ൌ෍
௜ൌଵ
௡
ቆ𝑋௜െ𝜇
𝜎
ቇ
ଶ
(8.2)
෍
௜ൌଵ
௡
ቆ𝑋௜െ𝜇
𝜎
ቇ
ଶ
𝑛
𝑛
ሺ1 െ2𝑡ሻ
െ௡/ଶ.
ቆ𝑋̅ ̅̅ ̅െ𝜇
𝜎/ 𝑛
√ቇ
ଶ
577 of 848

is the square of a standard normal variable, it is a chi-squared random variable with
1 degree of freedom, and so has moment generating function 
 Now, we
have seen previously that the two random variables on the left side of Equation
(8.2)
are independent. Hence, as the moment generating function of the sum of
independent random variables is equal to the product of their individual moment
generating functions, we have
or
But as 
 is the moment generating function of a chi-squared random
variable with 
 degrees of freedom, we can conclude, since the moment
generating function uniquely determines the distribution of the random variable, that
that is the distribution of 
Summing up, we have shown the following.
Proposition 8.1
If 
 are independent and identically distributed normal random variables
with mean  and variance 
 then the sample mean  and the sample variance
 are independent.  is a normal random variable with mean  and variance
 is a chi-squared random variable with 
 degrees of
freedom.
Up to this point, we have defined expectations only for discrete and continuous
random variables. However, there also exist random variables that are neither
discrete nor continuous, and they, too, may possess an expectation. As an example
of such a random variable, let  be a Bernoulli random variable with parameter
 and let  be a uniformly distributed random variable over the interval [0, 1].
Furthermore, suppose that  and  are independent, and define the new random
variable 
 by
ሺ1 െ2𝑡ሻ
െଵ/ଶ.
𝐸ቂ𝑒௧൫௡െଵ൯ௌమ/ఙమቃቀ1 െ2𝑡ቁ
െଵ/ଶ
ൌቀ1 െ2𝑡ቁ
െ௡/ଶ
𝐸ቂ𝑒௧൫௡െଵ൯ௌమ/ఙమቃൌቀ1 െ2𝑡ቁ
െ൫௡െଵ൯/ଶ
ሺ1 െ2𝑡ሻ
െ൫௡െଵ൯/ଶ
𝑛െ1
൫𝑛െ1൯𝑆ଶ/𝜎ଶ.
𝑋ଵ, …, 𝑋௡
𝜇
𝜎ଶ,
𝑋̅ ̅̅ ̅
𝑆ଶ
𝑋̅ ̅̅ ̅
𝜇
𝜎ଶ/𝑛; ൫𝑛െ1൯𝑆ଶ/𝜎ଶ
𝑛െ1
𝑋
𝑝ൌ1
2 ,
𝑌
𝑋
𝑌
𝑊
𝑊ൌቊ𝑋if 𝑋ൌ1
𝑌
if 𝑋്1
578 of 848

Clearly, 
 is neither a discrete (since its set of possible values, [0, 1], is
uncountable) nor a continuous (since 
 random variable.
In order to define the expectation of an arbitrary random variable, we require the
notion of a Stieltjes integral. Before defining this integral, let us recall that for any
function 
 is defined by
where the limit is taken over all 
 as 
 and where
For any distribution function 
 we define the Stieltjes integral of the nonnegative
function  over the interval 
 by
where, as before, the limit is taken over all 
 as 
 and
where 
 Further, we define the Stieltjes integral over the
whole real line by
Finally, if  is not a nonnegative function, we define 
 and 
 by
Because 
 and 
 and 
 are both nonnegative functions, it is
natural to define
𝑊
𝑃ሼ𝑊ൌ1ሽൌ1
2ሻ
𝑔, ׬௔
௕𝑔ሺ𝑋ሻ𝑑𝑥
඲
௔
௕
𝑔ሺ𝑋ሻ𝑑𝑥ൌlim ෍
௜ൌଵ
௡
𝑔ሺ𝑥௜ሻሺ𝑥௜െ𝑥௜െଵሻ
𝑎ൌ𝑥଴൏𝑥ଵ൏𝑥ଶ⋯൏𝑥௡ൌ𝑏
𝑛→∞
max
௜ൌଵ, … , ௡ሺ𝑥௜െ𝑥௜െଵሻ→0.
𝐹,
𝑔
ሾ𝑎,  𝑏ሿ
඲
௔
௕
𝑔ሺ𝑋ሻ 𝑑𝐹ሺ𝑋ሻൌlim ෍
௜ൌଵ
௡
𝑔ሺ𝑥௜ሻሾ𝐹ሺ𝑥௜ሻെ𝐹ሺ𝑥௜െଵሻሿ
𝑎ൌ𝑥଴൏𝑥ଵ൏⋯൏𝑥௡ൌ𝑏
𝑛→∞
max
௜ൌଵ, … , ௡ቆ𝑥௜െ𝑥௜െଵቇ→0.
඲
െஶ
ஶ
𝑔ሺ𝑥ሻ 𝑑𝐹ሺ𝑥ሻൌ
lim
௔→െஶ
௕→൅ஶ
඲
௔
௕
𝑔ሺ𝑥ሻ 𝑑𝐹ሺ𝑥ሻ
𝑔
𝑔൅
𝑔െ
𝑔൅ሺ𝑥ሻ
ൌ൝
𝑔ሺ𝑥ሻ
if 𝑔ሺ𝑥ሻ൒0
        0
if 𝑔ሺ𝑥ሻ൏0
𝑔െሺ𝑥ሻ
ൌ൝
        0
if 𝑔ሺ𝑥ሻ൒0
െ𝑔ሺ𝑋ሻ
if 𝑔ሺ𝑥ሻ൏0
𝑔൫𝑥൯ൌ𝑔൅൫𝑥൯െ𝑔െ൫𝑥൯
𝑔൅
𝑔െ
579 of 848

and we say that 
 exists as long as 
 and
 are not both equal to 
If  is an arbitrary random variable having cumulative distribution 
 we define the
expected value of  by
It can be shown that if  is a discrete random variable with mass function 
 then
whereas if  is a continuous random variable with density function 
 then
The reader should note that Equation (9.1)
 yields an intuitive definition of 
consider the approximating sum
of 
 Because 
 is just the probability that  will be in the interval
 the approximating sum multiplies the approximate value of  when it is in
the interval 
 by the probability that it will be in that interval and then sums
over all the intervals. Clearly, as these intervals get smaller and smaller in length, we
obtain the “expected value” of 
Stieltjes integrals are mainly of theoretical interest because they yield a compact way
of defining and dealing with the properties of expectation. For instance, the use of
Stieltjes integrals avoids the necessity of having to give separate statements and
proofs of theorems for the continuous and the discrete cases. However, their
඲
െஶ
ஶ
𝑔ሺ𝑥ሻ 𝑑𝐹ሺ𝑥ሻൌ඲
െஶ
ஶ
𝑔൅ሺ𝑥ሻ 𝑑𝐹ሺ𝑥ሻെ඲
െஶ
ஶ
𝑔െሺ𝑥ሻ 𝑑𝐹ሺ𝑥ሻ
׬െஶ
ஶ𝑔ሺ𝑋ሻ 𝑑𝐹ሺ𝑋ሻ
׬െஶ
ஶ𝑔൅ሺ𝑥ሻ 𝑑𝐹ሺ𝑥ሻ
׬െஶ
ஶ
𝑔െቀ𝑥ቁ 𝑑𝐹ቀ𝑥ቁ
൅∞.
𝑋
𝐹,
𝑋
𝐸ሾ𝑋ሿൌ඲
െஶ
ஶ
𝑥 𝑑𝐹ሺ𝑥ሻ
(9.1)
𝑋
𝑝ሺ𝑥ሻ,
඲
െஶ
ஶ
𝑥𝑑𝐹ሺ𝑥ሻൌ
෍
௫: ௣ሺ௫ሻவ଴
𝑥𝑝ሺ𝑥ሻ
𝑋
𝑓ሺ𝑥ሻ,
඲
െஶ
ஶ
𝑥𝑑𝐹ሺ𝑥ሻൌ඲
െஶ
ஶ
𝑥𝑓ሺ𝑥ሻ𝑑𝑥
𝐸ሾ𝑋ሿ;
෍
௜ൌଵ
௡
𝑥௜ሾ𝐹ሺ𝑥௜ሻെ𝐹ሺ𝑥௜െଵሻሿ
𝐸ሾ𝑋ሿ.
𝐹ሺ𝑥௜ሻെ𝐹ሺ𝑥௜െଵሻ
𝑋
ሺ𝑥௜െଵ, 𝑥௜ሿ,
𝑋
ሺ𝑥௜െଵ, 𝑥௜ሿ
𝑋.
580 of 848

properties are very much the same as those of ordinary integrals, and all of the
proofs presented in this chapter can easily be translated into proofs in the general
case.
If  and  have a joint probability mass function 
, then
whereas if they have a joint density function 
, then
A consequence of the preceding equations is that
which generalizes to
The covariance between random variables  and  is given by
A useful identity is
When 
 and 
 the preceding formula gives
𝑋
𝑌
𝑝ሺ𝑥, 𝑦ሻ
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿൌ෍
௬
෍
௫
𝑔ሺ𝑥, 𝑦ሻ𝑝ሺ𝑥, 𝑦ሻ
𝑓ሺ𝑥, 𝑦ሻ
𝐸ሾ𝑔ሺ𝑋, 𝑌ሻሿൌ඲
െஶ
ஶ
඲
െஶ
ஶ
𝑔ሺ𝑥, 𝑦ሻ𝑓ሺ𝑥, 𝑦ሻ𝑑𝑥𝑑𝑦
𝐸ሾ𝑋൅𝑌ሿൌ𝐸ሾ𝑋ሿ൅𝐸ሾ𝑌ሿ
𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿ
𝑋
𝑌
Covሺ𝑋, 𝑌ሻ
ൌ𝐸ሾሺ𝑋െ𝐸ሾ𝑋ሿሻሺ𝑌െ𝐸ሾ𝑌ሿሻሿ
ൌ𝐸ሾ𝑋𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿ
Cov ቌ෍
௜ൌଵ
௡
𝑋௜, ෍
௝ൌଵ
௠
𝑌௝ቍൌ෍
௜ൌଵ
௡
෍
௝ൌଵ
௠
Covሺ𝑋௜, 𝑌௝ሻ
𝑛ൌ𝑚
𝑌௜ൌ𝑋௜, 𝑖ൌ1, …, 𝑛,
Var ቌ෍
௜ൌଵ
௡
𝑋௜ቍൌ෍
௜ൌଵ
௡
Varሺ𝑋௜ሻ൅2෍෍
௜ழ௝
Covሺ𝑋௜, 𝑌௝ሻ
581 of 848

The correlation between  and 
 denoted by 
 is defined by
If  and  are jointly discrete random variables, then the conditional expected value
of 
 given that 
 is defined by
If  and  are jointly continuous random variables, then
where
is the conditional probability density of  given that 
 Conditional expectations,
which are similar to ordinary expectations except that all probabilities are now
computed conditional on the event that 
 satisfy all the properties of ordinary
expectations.
Let 
 denote that function of  whose value at 
 is 
 A very
useful identity is
In the case of discrete random variables, this equation reduces to the identity
and, in the continuous case, to
The preceding equations can often be applied to obtain 
 by first “conditioning” on
𝑋
𝑌,
𝜌ሺ𝑋, 𝑌ሻ,
𝜌ሺ𝑋, 𝑌ሻൌ
Cov ሺ𝑋, 𝑌ሻ
Varሺ𝑋ሻVarሺ𝑌ሻ
ඥ
𝑋
𝑌
𝑋,
𝑌ൌ𝑦,
𝐸ሾ𝑋|𝑌ൌ𝑦ሿൌ෍
௫
𝑥𝑃ሼ𝑋ൌ𝑥|𝑌ൌ𝑦ሿ
𝑋
𝑌
𝐸ሾ𝑋|𝑌ൌ𝑦ሿൌ඲
െஶ
ஶ
𝑥𝑓௑ห௒ሺ𝑥|𝑦ሻ
𝑓௑ห௒ሺ𝑥|𝑦ሻൌ𝑓ሺ𝑥, 𝑦ሻ
𝑓௒൫𝑦൯
𝑋
𝑌ൌ𝑦.
𝑌ൌ𝑦,
𝐸ሾ𝑋||𝑌ሿ
𝑌
𝑌ൌ𝑦
𝐸ሾ𝑋||𝑌ൌ𝑦ሿ.
𝐸ሾ𝑋ሿൌ𝐸ሾ𝐸ሾ𝑋||𝑌ሿሿ
𝐸ሾ𝑋ሿൌ෍
௬
𝐸ሾ𝑋|𝑌ൌ𝑦ሿ𝑃ሼ𝑌ൌ𝑦ሽ
𝐸ሾ𝑋ሿൌ඲
െஶ
ஶ
𝐸ሾ𝑋|𝑌ൌ𝑦ሿ𝑓௒ሺ𝑦ሻ𝑑𝑦
𝐸ሾ𝑋ሿ
582 of 848

the value of some other random variable 
 In addition, since, for any event 
 where 
 is 1 if  occurs and is 0 otherwise, we can use the same
equations to compute probabilities.
The conditional variance of 
 given that 
 is defined by
Let 
 be that function of  whose value at 
 is 
 The
following is known as the conditional variance formula:
Suppose that the random variable  is to be observed and, on the basis of its value,
one must then predict the value of the random variable 
 In such a situation, it turns
out that among all predictors, 
 has the smallest expectation of the square of
the difference between it and 
The moment generating function of the random variable  is defined by
The moments of  can be obtained by successively differentiating 
 and then
evaluating the resulting quantity at 
 Specifically, we have
Two useful results concerning moment generating functions are, first, that the
moment generating function uniquely determines the distribution function of the
random variable and, second, that the moment generating function of the sum of
independent random variables is equal to the product of their moment generating
functions. These results lead to simple proofs that the sum of independent normal
(Poisson, gamma) random variables remains a normal (Poisson, gamma) random
variable.
If 
 are all linear combinations of a finite set of independent standard normal
random variables, then they are said to have a multivariate normal distribution. Their
joint distribution is specified by the values of 
If 
 are independent and identically distributed normal random variables, then
their sample mean
𝑌.
𝐴,
𝑃ሺ𝐴ሻൌ𝐸ሾ𝐼஺ሿ,
𝐼஺
𝐴
𝑋,
𝑌ൌ𝑦,
Varሺ𝑋||𝑌ൌ𝑦ሻൌ𝐸ሾሺ𝑋െ𝐸ሾ𝑋||𝑌ൌ𝑦ሿሻଶ||𝑌ൌ𝑦ሿ
Varሺ𝑋||𝑌ሻ
𝑌
𝑌ൌ𝑦
Varሺ𝑋||𝑌ൌ𝑦ሻ.
Varሺ𝑋ሻൌ𝐸ሾVarሺ𝑋||𝑌ሻሿ൅Varሺ𝐸ሾ𝑋||𝑌ሿሻ
𝑋
𝑌.
𝐸ሾ𝑌||𝑋ሿ
𝑌.
𝑋
𝑀ሺ𝑡ሻൌ𝐸ሾ𝑒௧௑ሿ
𝑋
𝑀ሺ𝑡ሻ
𝑡ൌ0.
𝐸ሾ𝑋௡ሿൌ𝑑௡
𝑑𝑡௡𝑀ሺ𝑡ሻቤ௧ൌ଴ 𝑛ൌ1, 2, …
𝑋ଵ, …, 𝑋௠
𝐸ൣ𝑋௜൧, Cov൫𝑋௜, 𝑋௝൯, 𝑖, 𝑗ൌ1, …, 𝑚.
𝑋ଵ, …, 𝑋௡
583 of 848

and their sample variance
are independent. The sample mean  is a normal random variable with mean  and
variance 
 the random variable 
 is a chi-squared random variable
with 
 degrees of freedom.
𝑋̅ ̅̅ ̅ൌ෍
௜ൌଵ
௡
𝑋௜
𝑛
𝑆ଶൌ෍
௜ൌଵ
௡
ሺ𝑋௜െ𝑋̅ ̅̅ ̅ሻ
ଶ
𝑛െ1
𝑋̅ ̅̅ ̅
𝜇
𝜎ଶ/𝑛;
൫𝑛െ1൯𝑆ଶ/𝜎ଶ
𝑛െ1
7.1 A player throws a fair die and simultaneously flips a fair coin. If
the coin lands heads, then she wins twice, and if tails, then she wins
one-half of the value that appears on the die. Determine her
expected winnings.
7.2 The game of Clue involves 6 suspects, 6 weapons, and 9 rooms.
One of each is randomly chosen and the object of the game is to
guess the chosen three.
a. How many solutions are possible?
In one version of the game, the selection is made and then
each of the players is randomly given three of the remaining
cards. Let 
 and  be, respectively, the numbers of
suspects, weapons, and rooms in the set of three cards given
to a specified player. Also, let  denote the number of
solutions that are possible after that player observes his or her
three cards.
b. Express  in terms of 
 and 
c. Find 
7.3 Gambles are independent, and each one results in the player
being equally likely to win or lose 1 unit. Let 
 denote the net
winnings of a gambler whose strategy is to stop gambling
immediately after his first win. Find
a. 
b. 
c. 
𝑆, 𝑊,
𝑅
𝑋
𝑋
𝑆, 𝑊,
𝑅.
𝐸ሾ𝑋ሿ.
𝑊
𝑃ሼ𝑊൐0ሽ
𝑃ሼ𝑊൏0ሽ
𝐸ሾ𝑊ሿ
584 of 848

7.4. If  and  have joint density function
find
a. 
b. 
c. 
7.5 The county hospital is located at the center of a square whose
sides are 3 miles wide. If an accident occurs within this square, then
the hospital sends out an ambulance. The road network is
rectangular, so the travel distance from the hospital, whose
coordinates are (0, 0), to the point 
 is 
 If an accident
occurs at a point that is uniformly distributed in the square, find the
expected travel distance of the ambulance.
7.6 A fair die is rolled 10 times. Calculate the expected sum of the 10
rolls.
7.7 Suppose that  and  each randomly and independently choose
3 of 10 objects. Find the expected number of objects
a. chosen by both  and 
b. not chosen by either  or 
c. chosen by exactly one of  and 
7.8.
 people arrive separately to a professional dinner. Upon arrival,
each person looks to see if he or she has any friends among those
present. That person then sits either at the table of a friend or at an
unoccupied table if none of those present is a friend. Assuming that
each of the 
 pairs of people is, independently, a pair of friends
with probability  find the expected number of occupied tables.
Hint: Let 
 equal 1 or 0, depending on whether the th arrival sits at
a previously unoccupied table.
7.9. A total of  balls, numbered 1 through  are put into  urns, also
numbered 1 through  in such a way that ball  is equally likely to go
into any of the urns 
 Find
a. the expected number of urns that are empty;
b. the probability that none of the urns is empty.
7.10 Consider 3 trials, each having the same probability of success.
Let  denote the total number of successes in these trials. If
𝑋
𝑌
𝑓௑,௒ሺ𝑥, 𝑦ሻൌ൝1/𝑦,
if 0 ൏𝑦൏1,  0 ൏𝑥൏𝑦
0,
otherwise
𝐸ሾ𝑋𝑌ሿ
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ
ሺ𝑥,  𝑦ሻ
|𝑥| ൅|𝑦|.
𝐴
𝐵
𝐴
𝐵;
𝐴
𝐵;
𝐴
𝐵.
𝑁
ቆ𝑁
2ቇ
𝑝,
𝑋௜
𝑖
𝑛
𝑛,
𝑛
𝑛
𝑖
1, 2, …, 𝑖.
𝑋
585 of 848

 what is
a. the largest possible value of 
b. the smallest possible value of 
In both cases, construct a probability scenario that results in
 having the stated value.
Hint: For part (b), you might start by letting  be a uniform random
variable on (0, 1) and then defining the trials in terms of the value of
7.11 Consider  independent flips of a coin having probability  of
landing on heads. Say that a changeover occurs whenever an
outcome differs from the one preceding it. For instance, if 
 and
the outcome is 
 then there are 3 changeovers. Find the
expected number of changeovers.
Hint: Express the number of changeovers as the sum of 
Bernoulli random variables.
7.12. A group of  men and  women is lined up at random.
a. Find the expected number of men who have a woman next to
them.
b. Repeat part (a), but now assuming that the group is randomly
seated at a round table.
7.13. A set of 1000 cards numbered 1 through 1000 is randomly
distributed among 1000 people with each receiving one card.
Compute the expected number of cards that are given to people
whose age matches the number on the card.
7.14. An urn has 
 black balls. At each stage, a black ball is
removed and a new ball that is black with probability  and white with
probability 
 is put in its place. Find the expected number of
stages needed until there are no more black balls in the urn.
NOTE: The preceding has possible applications to understanding the
AIDS disease. Part of the body’s immune system consists of a
certain class of cells known as T-cells. There are 2 types of T-cells,
called CD4 and CD8. Now, while the total number of T-cells in AIDS
sufferers is (at least in the early stages of the disease) the same as
that in healthy individuals, it has recently been discovered that the
mix of CD4 and CD8 T-cells is different. Roughly 60 percent of the
T-cells of a healthy person are of the CD4 type, whereas the
percentage of the T-cells that are of CD4 type appears to decrease
continually in AIDS sufferers. A recent model proposes that the HIV
virus (the virus that causes AIDS) attacks CD4 cells and that the
body’s mechanism for replacing killed T-cells does not differentiate
𝐸ሾ𝑋ሿൌ1 . 8,
𝑃ሼ𝑋ൌ3ሽ?
𝑃ሼ𝑋ൌ3ሽ?
𝑃ሼ𝑋ൌ3ሽ
𝑈
𝑈.
𝑛
𝑝
𝑛ൌ5
𝐻𝐻𝑇𝐻𝑇,
𝑛െ1
𝑛
𝑛
𝑚
𝑝
1 െ𝑝
586 of 848

between whether the killed T-cell was CD4 or CD8. Instead, it just
produces a new T-cell that is CD4 with probability .6 and CD8 with
probability .4. However, although this would seem to be a very
efficient way of replacing killed T-cells when each one killed is
equally likely to be any of the body’s T-cells (and thus has probability
.6 of being CD4), it has dangerous consequences when facing a
virus that targets only the CD4 T-cells.
7.15. In Example 2h
, say that  and 
 form a matched pair if
 chooses the hat belonging to  and  chooses the hat belonging to 
Find the expected number of matched pairs.
7.16. Let  be a standard normal random variable, and, for a fixed 
set
Show that 
7.17. A deck of  cards numbered 1 through  is thoroughly shuffled
so that all possible ! orderings can be assumed to be equally likely.
Suppose you are to make  guesses sequentially, where the th one
is a guess of the card in position  Let  denote the number of
correct guesses.
a. If you are not given any information about your earlier
guesses, show that for any strategy, 
b. Suppose that after each guess you are shown the card that
was in the position in question. What do you think is the best
strategy? Show that under this strategy,
c. Suppose that you are told after each guess whether you are
right or wrong. In this case, it can be shown that the strategy
that maximizes 
 is one that keeps on guessing the same
card until you are told you are correct and then changes to a
new card. For this strategy, show that
Hint: For all parts, express  as the sum of indicator (that is,
𝑖
𝑗, 𝑖്𝑗,
𝑖
𝑗
𝑗
𝑖.
𝑍
𝑥,
𝑋ൌቊ𝑍
if 𝑍൐𝑥
0
otherwise
𝐸ሾ𝑋ሿൌ
1
2𝜋
√
𝑒െ௫మ/ଶ.
𝑛
𝑛
𝑛
𝑛
𝑖
𝑖.
𝑁
𝐸ሾ𝑁ሿൌ1.
𝐸ሾ𝑁ሿൌ
1
𝑛൅
1
𝑛െ1 ൅⋯൅1
ൎ
඲
ଵ
௡
1
𝑥𝑑𝑥ൌlog 𝑛
𝐸ሾ𝑁ሿ
𝐸ሾ𝑁ሿ
ൌ
1 ൅1
2! ൅1
3! ൅⋯൅1
𝑛!
ൎ
𝑒െ1
𝑁
587 of 848

Bernoulli) random variables.
7.18. Cards from an ordinary deck of 52 playing cards are turned
face up one at a time. If the 1st card is an ace, or the 2nd a deuce, or
the 3rd a three, or 
 or the 13th a king, or the 14 an ace, and so on,
we say that a match occurs. Note that we do not require that the (
 card be any particular ace for a match to occur but only that
it be an ace. Compute the expected number of matches that occur.
7.19. A certain region is inhabited by  distinct types of a certain
species of insect. Each insect caught will, independently of the types
of the previous catches, be of type  with probability
a. Compute the mean number of insects that are caught before
the first type 1 catch.
b. Compute the mean number of types of insects that are caught
before the first type 1 catch.
7.20 In an urn containing  balls, the th ball has weight
 The balls are removed without replacement, one at a
time, according to the following rule: At each selection, the probability
that a given ball in the urn is chosen is equal to its weight divided by
the sum of the weights remaining in the urn. For instance, if at some
time 
 is the set of balls remaining in the urn, then the next
selection will be  with probability 
Compute the expected number of balls that are withdrawn before ball
number 1 is removed.
7.21. For a group of 100 people, compute
a. the expected number of days of the year that are birthdays of
exactly 3 people;
b. the expected number of distinct birthdays.
7.22. How many times would you expect to roll a fair die before all 6
sides appeared at least once?
7.23. Urn 1 contains 5 white and 6 black balls, while urn 2 contains 8
white and 10 black balls. Two balls are randomly selected from urn 1
and are put into urn 2. If 3 balls are then randomly selected from urn
2, compute the expected number of white balls in the trio.
Hint: Let 
 if the th white ball initially in urn 1 is one of the three
…,
13𝑛൅1ሻ
𝑟
𝑖
𝑃௜, 𝑖ൌ1, …, 𝑟 ෍
ଵ
௥
𝑃௜ൌ1
𝑛
𝑖
𝑊ሺ𝑖ሻ, 𝑖ൌ1, …, 𝑛.
𝑖ଵ, …, 𝑖௥
𝑖௝
𝑊ሺ𝑖௝ሻ
෍
௞ൌଵ
௥
𝑊ሺ𝑖௞ሻ, 𝑗ൌ1, …, 𝑟.
𝑋௜ൌ1
𝑖
588 of 848

selected, and let 
 otherwise. Similarly, let 
 if the th white
ball from urn 2 is one of the three selected, and let 
 otherwise.
The number of white balls in the trio can now be written as
7.24. A bottle initially contains 
 large pills and  small pills. Each
day, a patient randomly chooses one of the pills. If a small pill is
chosen, then that pill is eaten. If a large pill is chosen, then the pill is
broken in two; one part is returned to the bottle (and is now
considered a small pill) and the other part is then eaten.
a. Let  denote the number of small pills in the bottle after the
last large pill has been chosen and its smaller half returned.
Find 
Hint: Define 
 indicator variables, one for each of the
small pills initially present and one for each of the 
 small pills
created when a large one is split in two. Now use the
argument of Example 2m
.
b. Let  denote the day on which the last large pill is chosen.
Find 
Hint: What is the relationship between  and 
7.25. Let 
 be a sequence of independent and identically
distributed continuous random variables. Let 
 be such that
That is,  is the point at which the sequence stops decreasing. Show
that 
Hint: First find 
7.26. If 
 are independent and identically distributed
random variables having uniform distributions over (0, 1), find
a. 
b. 
* 7.27. If 101 items are distributed among 10 boxes, then at least one
of the boxes must contain more than 10 items. Use the probabilistic
method to prove this result.
* 7.28. The -of- -out-of-  circular reliability system, 
consists of  components that are arranged in a circular fashion.
Each component is either functional or failed, and the system
functions if there is no block of  consecutive components of which at
least  are failed. Show that there is no way to arrange 47
𝑋௜ൌ0
𝑌௜ൌ1
𝑖
𝑌௜ൌ0
෍
ଵ
ହ
𝑋௜൅෍
ଵ
଼
𝑌௜.
𝑚
𝑛
𝑋
𝐸ሾ𝑋ሿ.
𝑛൅𝑚
𝑚
𝑌
𝐸ሾ𝑌ሿ.
𝑋
𝑌?
𝑋ଵ, 𝑋ଶ, …
𝑁൒2
𝑋ଵ൒𝑋ଶ൒⋯൒𝑋ேെଵ൏𝑋ே
𝑁
𝐸ሾ𝑁ሿൌ𝑒.
𝑃ሼ𝑁൒𝑛ሽ.
𝑋ଵ, 𝑋ଶ, …, 𝑋௡
𝐸ሾmaxሺ𝑋ଵ, …, 𝑋௡ሻሿ;
𝐸ሾminሺ𝑋ଵ, …, 𝑋௡ሻሿ.
𝑘
𝑟
𝑛
𝑘൑𝑟൑𝑛,
𝑛
𝑟
𝑘
589 of 848

components, 8 of which are failed, to make a functional 3-of-12-out-
of-47 circular system.
* 7.29. There are 4 different types of coupons, the first 2 of which
comprise one group and the second 2 another group. Each new
coupon obtained is type  with probability 
 where
 Find the expected number of coupons
that one must obtain to have at least one of
a. all 4 types;
b. all the types of the first group;
c. all the types of the second group;
d. all the types of either group.
* 7.30. If  and  are independent and identically distributed with
mean  and variance 
 find
7.31. In Problem 7.6
, calculate the variance of the sum of the
rolls.
7.32. In Problem 7.9
, compute the variance of the number of
empty urns.
7.33. If 
 and 
 find
a. 
b. 
7.34 If 10 married couples are randomly seated at a round table,
compute (a) the expected number and (b) the variance of the number
of wives who are seated next to their husbands.
7.35 Cards from an ordinary deck are turned face up one at a time.
Compute the expected number of cards that need to be turned face
up in order to obtain
a. 2 aces;
b. 5 spades;
c. all 13 hearts.
7.36. Let  be the number of 1’s and  the number of 2’s that occur
in  rolls of a fair die. Compute 
7.37. A die is rolled twice. Let  equal the sum of the outcomes, and
let  equal the first outcome minus the second. Compute 
7.38. Suppose  and  have the following joint probability mass
function.
𝑖
𝑝௜,
𝑝ଵൌ𝑝ଶൌ1/8, 𝑝ଷൌ𝑝ସൌ3/8.
𝑋
𝑌
𝜇
𝜎ଶ,
𝐸ሾሺ𝑋െ𝑌ሻଶሿ
𝐸ሾ𝑋ሿൌ1
Varሺ𝑋ሻൌ5,
𝐸ሾሺ2 ൅𝑋ሻଶሿ;
Varሺ4 ൅3𝑋ሻ.
𝑋
𝑌
𝑛
Covሺ𝑋, 𝑌ሻ.
𝑋
𝑌
Covሺ𝑋, 𝑌ሻ.
𝑋
𝑌
590 of 848

a. Find 
 and 
b. Find 
 and 
c. Find 
d. Find the correlation between  and 
7.39 Suppose that  balls are randomly removed from an urn
containing  red and 
 blue balls. Let 
 if the 
 ball removed is
red, and let it be  otherwise, 
a. Do you think that 
 is negative, zero, or positive.
b. Validate your answer to part (a).
Suppose the red balls are numbered, and let 
 equal  if red
ball number  is removed, and let it be  if that ball is not
removed.
c. Do you think that 
 is negative, zero, or positive.
d. Validate your answer to part (c).
7.40. The random variables  and  have a joint density function
given by
Compute 
7.41. Let 
 be independent with common mean  and common
variance 
 and set 
 For 
 find
7.42. The joint density function of  and  is given by
Find 
 and show that 
7.43. A pond contains 100 fish, of which 30 are carp. If 20 fish are
caught, what are the mean and variance of the number of carp
among the 20? What assumptions are you making?
7.44. A group of 20 people consisting of 10 men and 10 women is
randomly arranged into 10 pairs of 2 each. Compute the expectation
and variance of the number of pairs that consist of a man and a
woman. Now suppose the 20 people consist of 10 married couples.
𝑝ሺ1, 1ሻൌ
. 10, 𝑝ሺ1, 2ሻൌ. 12, 𝑝ሺ1, 3ሻൌ. 16
𝑝ሺ2, 1ሻൌ
. 08, 𝑝ሺ2, 2ሻൌ. 12, 𝑝ሺ2, 3ሻൌ. 10
𝑝ሺ3, 1ሻൌ
. 06, 𝑝ሺ3, 2ሻൌ. 06, 𝑝ሺ3, 3ሻൌ. 20
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ.
Varሺ𝑋ሻ
Varሺ𝑌ሻ.
Covሺ𝑋, 𝑌ሻ.
𝑋
𝑌.
2
𝑛
𝑚
𝑋௜ൌ1
𝑖௧௛
0
𝑖ൌ1, 2 .
Covሺ𝑋ଵ, 𝑋ଶሻ
𝑌௜
1
𝑖
0
Covሺ𝑌ଵ, 𝑌ଶሻ
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ൝2𝑒െଶ௫/𝑥
0 ൑𝑥൏∞, 0 ൑𝑦൑𝑥
0
otherwise
Covሺ𝑋, 𝑌ሻ.
𝑋ଵ, …
𝜇
𝜎ଶ,
𝑌௡ൌ𝑋௡൅𝑋௡൅ଵ൅𝑋௡൅ଶ.
𝑗൒0,
Cov൫𝑌௡, 𝑌௡൅௝൯.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ1
𝑦𝑒െ൫௬൅௫/௬൯,  𝑥൐0, 𝑦൐0
𝐸ሾ𝑋ሿ,  𝐸ሾ𝑌ሿ,
Covሺ𝑋, 𝑌ሻൌ1.
591 of 848

Compute the mean and variance of the number of married couples
that are paired together.
7.45. Let 
 be independent random variables having an
unknown continuous distribution function 
 and let 
 be
independent random variables having an unknown continuous
distribution function 
 Now order those 
 variables, and let
The random variable 
 is the sum of the ranks of the 
sample and is the basis of a standard statistical procedure (called
theWilcoxon sum-of-ranks test) for testing whether  and  are
identical distributions. This test accepts the hypothesis that 
when  is neither too large nor too small. Assuming that the
hypothesis of equality is in fact correct, compute the mean and
variance of 
Hint: Use the results of Example 3e
.
7.46. Between two distinct methods for manufacturing certain goods,
the quality of goods produced by method  is a continuous random
variable having distribution 
 Suppose that  goods are
produced by method 1 and 
 by method 2. Rank the 
 goods
according to quality, and let
For the vector 
 which consists of  1’s and 
 2’s, let 
denote the number of runs of 1. For instance, if 
 and
 then 
 If 
(that is, if the two methods
produce identically distributed goods), what are the mean and
variance of 
7.47. If 
 and 
 are (pairwise) uncorrelated random
variables, each having mean 0 and variance 1, compute the
correlations of
A. 
 and 
B. 
 and 
7.48. Consider the following dice game, as played at a certain
𝑋ଵ, 𝑋ଶ, …, 𝑋௡
𝐹,
𝑌ଵ, 𝑌ଶ, …, 𝑌௠
𝐺.
𝑛൅𝑚
𝐼௜ൌ൞
1
if the 𝑖th smallest of the 𝑛൅𝑚
variables is from the 𝑋 sample
0
otherwise
𝑅ൌ
෍
௜ൌଵ
௡൅௠
𝑖𝐼௜
𝑋
𝐹
𝐺
𝐹ൌ𝐺
𝑅
𝑅.
𝑖
𝐹௜, 𝑖ൌ1, 2.
𝑛
𝑚
𝑛൅𝑚
𝑋௝ൌ൞
1
if the 𝑖th best was produced from
method 1
2
otherwise
𝑋ଵ, 𝑋ଶ, …, 𝑋௡൅௠,
𝑛
𝑚
𝑅
𝑛ൌ5, 𝑚ൌ2,
𝑋ൌ1, 2, 1, 1, 1, 1, 2,
𝑅ൌ2.
𝐹ଵൌ𝐹ଶ
𝑅?
𝑋ଵ, 𝑋ଶ, 𝑋ଷ,
𝑋ସ
𝑋ଵ൅𝑋ଶ
𝑋ଶ൅𝑋ଷ;
𝑋ଵ൅𝑋ଶ
𝑋ଷ൅𝑋ସ.
592 of 848

gambling casino: Players 1 and 2 roll a pair of dice in turn. The bank
then rolls the dice to determine the outcome according to the
following rule: Player 
 wins if his roll is strictly greater than
the bank’s. For 
 let
and show that 
 and 
 are positively correlated. Explain why this
result was to be expected.
7.49 Consider a graph having  vertices labeled 
 and
suppose that, between each of the 
 pairs of distinct vertices, an
edge is independently present with probability  The degree of
vertex  designated as 
 is the number of edges that have vertex 
as one of their vertices.
a. What is the distribution of 
b. Find 
 the correlation between 
 and 
7.50. A fair die is successively rolled. Let  and  denote,
respectively, the number of rolls necessary to obtain a 6 and a 5.
Find
a. 
b. 
c. 
7.51. There are two misshapen coins in a box; their probabilities for
landing on heads when they are flipped are, respectively,  and 
One of the coins is to be randomly chosen and flipped 
 times.
Given that two of the first three flips landed on heads, what is the
conditional expected number of heads in the 
 flips?
7.52. The joint density of  and  is given by
Compute 
7.53. The joint density of  and  is given by
Compute 
7.54. A population is made up of  disjoint subgroups. Let 
 denote
the proportion of the population that is in subgroup 
 If the
𝑖, 𝑖ൌ1, 2,
𝑖ൌ1, 2,
𝐼௜ൌቊ1
if 𝑖 wins
0
otherwise
𝐼ଵ
𝐼ଶ
𝑛
1, 2, …, 𝑛,
ቆ𝑛
2ቇ
𝑝.
𝑖,
𝐷௜,
𝑖
𝐷௜?
𝜌൫𝐷௜, 𝐷௝൯,
𝐷௜
𝐷௝.
𝑋
𝑌
𝐸ሾ𝑋ሿ;
𝐸ሾ𝑋||𝑌ൌ1ሿ;
𝐸ሾ𝑋||𝑌ൌ5ሿ.
.4
.7.
10
10
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑒െ௫/௬𝑒െ௬
𝑦
,  0 ൏𝑥൏∞,  0 ൏𝑦൏∞
𝐸ൣ𝑋ଶห𝑌ൌ𝑦൧.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ𝑒െ௬
𝑦,  0 ൏𝑥൏𝑦,  0 ൏𝑦൏∞
𝐸ൣ𝑋ଷห𝑌ൌ𝑦൧.
𝑟
𝑝௜
𝑖, 𝑖ൌ1, …, 𝑟.
593 of 848

average weight of the members of subgroup  is 
 what is
the average weight of the members of the population?
7.55. A prisoner is trapped in a cell containing 3 doors. The first door
leads to a tunnel that returns him to his cell after 2 days’ travel. The
second leads to a tunnel that returns him to his cell after 4 days’
travel. The third door leads to freedom after 1 day of travel. If it is
assumed that the prisoner will always select doors 1, 2, and 3 with
respective probabilities .5, .3, and .2, what is the expected number of
days until the prisoner reaches freedom?
7.56. Consider the following dice game: A pair of dice is rolled. If the
sum is 7, then the game ends and you win 0. If the sum is not 7, then
you have the option of either stopping the game and receiving an
amount equal to that sum or starting over again. For each value of
 find your expected return if you employ the strategy of
stopping the first time that a value at least as large as  appears.
What value of  leads to the largest expected return?
Hint: Let 
 denote the return when you use the critical value  To
compute 
 condition on the initial sum.
7.57. Ten hunters are waiting for ducks to fly by. When a flock of
ducks flies overhead, the hunters fire at the same time, but each
chooses his target at random, independently of the others. If each
hunter independently hits his target with probability .6, compute the
expected number of ducks that are hit. Assume that the number of
ducks in a flock is a Poisson random variable with mean 6.
7.58. The number of people who enter an elevator on the ground
floor is a Poisson random variable with mean 10. If there are  floors
above the ground floor, and if each person is equally likely to get off
at any one of the  floors, independently of where the others get off,
compute the expected number of stops that the elevator will make
before discharging all of its passengers.
7.59. Suppose that the expected number of accidents per week at an
industrial plant is 5. Suppose also that the numbers of workers
injured in each accident are independent random variables with a
common mean of 2.5. If the number of workers injured in each
accident is independent of the number of accidents that occur,
compute the expected number of workers injured in a week.
7.60. A coin having probability  of coming up heads is continually
flipped until both heads and tails have appeared. Find
a. the expected number of flips;
b. the probability that the last flip lands on heads.
𝑖
𝑤௜, 𝑖ൌ1, …, 𝑟,
𝑖, 𝑖ൌ2, …, 12,
𝑖
𝑖
𝑋௜
𝑖.
𝐸ሾ𝑋௜ሿ,
𝑁
𝑁
𝑝
594 of 848

7.61. A coin that comes up heads with probability  is continually
flipped. Let  be the number of flips until there have been both at
least  heads and at least 
 tails. Derive an expression for 
 by
conditioning on the number of heads in the first 
 flips.
7.62. There are 
 participants in a game. Each person
independently is a winner with probability  The winners share a
total prize of 1 unit. (For instance, if 4 people win, then each of them
receives 
 whereas if there are no winners, then none of the
participants receives anything.) Let  denote a specified one of the
players, and let  denote the amount that is received by 
a. Compute the expected total prize shared by the players.
b. Argue that 
c. Compute 
 by conditioning on whether  is a winner, and
conclude that
when  is a binomial random variable with parameters  and 
7.63. Each of 
 players pays 1 unit to a kitty in order to play the
following game: A fair coin is to be flipped successively  times,
where  is an odd number, and the successive outcomes are noted.
Before the  flips, each player writes down a prediction of the
outcomes. For instance, if 
 then a player might write down
 which means that he or she predicts that the first flip will
land on heads, the second on heads, and the third on tails. After the
coins are flipped, the players count their total number of correct
predictions. Thus, if the actual outcomes are all heads, then the
player who wrote 
 would have 2 correct predictions. The
total kitty of 
 is then evenly split up among those players having
the largest number of correct predictions.
Since each of the coin flips is equally likely to land on either heads or
tails, 
 of the players have decided to make their predictions in a
totally random fashion. Specifically, they will each flip one of their
own fair coins  times and then use the result as their prediction.
However, the final 2 of the players have formed a syndicate and will
use the following strategy: One of them will make predictions in the
same random fashion as the other 
 players, but the other one will
then predict exactly the opposite of the first. That is, when the
randomizing member of the syndicate predicts an 
 the other
𝑝
𝑁
𝑛
𝑚
𝐸ሾ𝑁ሿ
𝑛൅𝑚
𝑛൅1
𝑝.
1
4 ,
𝐴
𝑋
𝐴.
𝐸ሾ𝑋ሿൌ1 െሺ1 െ𝑝ሻ௡൅ଵ
𝑛൅1
.
𝐸ሾ𝑋ሿ
𝐴
𝐸ሾሺ1 ൅𝐵ሻെଵሿൌ1 െሺ1 െ𝑝ሻ௡൅ଵ
ሺ𝑛൅1ሻ𝑝
𝐵
𝑛
𝑝.
𝑚൅2
𝑛
𝑛
𝑛
𝑛ൌ3,
ሺ𝐻, 𝐻, 𝑇ሻ,
ሺ𝐻, 𝐻, 𝑇ሻ,
𝑚൅2
𝑚
𝑛
𝑚
𝐻,
595 of 848

member predicts a 
 For instance, if the randomizing member of the
syndicate predicts 
 then the other one predicts 
a. Argue that exactly one of the syndicate members will have
more than 
 correct predictions. (Remember,  is odd.)
b. Let  denote the number of the 
 nonsyndicate players who
have more than 
 correct predictions. What is the
distribution of 
c. With  as defined in part (b), argue that
d. Use part (c) of Problem 7.62
 to conclude that
and explicitly compute this number when 
 and 3. Because it
can be shown that
it follows that the syndicate’s strategy always gives it a positive
expected profit.
7.64. The number of goals that J scores in soccer games that her
team wins is Poisson distributed with mean  while the number she
scores in games that her team loses is Poisson distributed with mean
 Assume that, independent of earlier results, J’s team wins each
new game it plays with probability 
a. Find the expected number of goals that J scores in her team’s
next game.
b. Find the probability that  scores  goals in her next  games.
Hint: Would it be useful to know how many of those games were
won by J’s team.
Suppose J’s team has just entered a tournament in which it will
continue to play games until it loses. Let  denote the total number of
goals scored by J in the tournament. Also, let  be the number of
games her team plays in the tournament.
a. Find 
𝑇.
ሺ𝐻, 𝐻, 𝑇ሻ,
ሺ𝑇, 𝑇, 𝐻ሻ.
𝑛/2
𝑛
𝑋
𝑚
𝑛/2
𝑋?
𝑋
𝐸ሾpay off to the syndicateሿ
ൌሺ𝑚൅2ሻ
ൈ𝐸⎡
⎣
1
𝑋൅1
⎤
⎦
𝐸ሾpayoff to the syndicateሿ
ൌ
2ሺ𝑚൅2ሻ
𝑚൅1
ൈ൥1 െቆ1
2ቇ
௠൅ଵ
൩
𝑚ൌ1, 2,
2ሺ𝑚൅2ሻ
𝑚൅1 ൥1 െቆ1
2ቇ
௠൅ଵ
൩൐2
2,
1.
𝑝.
𝐽
6
4
𝑋
𝑁
𝐸ሾ𝑋ሿ.
596 of 848

b. Find 
c. Find 
7.65. If the level of infection of a tree is 
 then each treatment will
independently be successful with probability 
 Consider a tree
whose infection level is assumed to be the value of a uniform 
random variable.
a. Find the probability that a single treatment will result in a cure.
b. Find the probability that the first two treatments are
unsuccessful.
c. Find the probability it will take  treatments for the tree to be
cured.
7.66. Let 
 be independent random variables with the common
distribution function 
 and suppose they are independent of 
 a
geometric random variable with parameter  Let 
a. Find 
 by conditioning on 
b. Find 
c. Find 
d. Use (b) and (c) to rederive the probability you found in (a).
7.67. Let 
 be a sequence of independent uniform (0, 1)
random variables. In Example 5i
, we showed that for
 where
This problem gives another approach to establishing that result.
a. Show by induction on  that for 
 and all 
Hint: First condition on 
 and then use the induction hypothesis.
Use part (a) to conclude that
7.68. An urn contains 30 balls, of which 10 are red and 8 are blue.
From this urn, 12 balls are randomly withdrawn. Let  denote the
number of red and  the number of blue balls that are withdrawn.
Find 
𝑃ሺ𝑋ൌ0ሻ.
𝑃ሺ𝑁ൌ3||𝑋ൌ5ሻ.
𝑥,
1 െ𝑥.
ሺ0, 1ሻ
𝑛
𝑋ଵ, …
𝐹,
𝑁,
𝑝.
𝑀ൌmaxሺ𝑋ଵ, …, 𝑋ேሻ.
𝑃ሼ𝑀൑𝑥ሽ
𝑁.
𝑃ሼ𝑀൑𝑥||𝑁ൌ1ሽ.
𝑃ሼ𝑀൑𝑥||𝑁൐1ሽ.
𝑈ଵ, 𝑈ଶ, …
0 ൑𝑥൑1, 𝐸ሾ𝑁ሺ𝑥ሻሿൌ𝑒௫,
𝑁ሺ𝑥ሻൌminቐ𝑛: ෍
௜ൌଵ
௡
𝑈௜൐𝑥ቑ
𝑛
0 ൏𝑥൑1
𝑛൒0,
𝑃ሼ𝑁ሺ𝑥ሻ൒𝑛൅1ሽൌ𝑥௡
𝑛!
𝑈ଵ
𝐸ሾ𝑁ሺ𝑥ሻሿൌ𝑒௫
𝑋
𝑌
Covሺ𝑋, 𝑌ሻ
597 of 848

a. by defining appropriate indicator (that is, Bernoulli) random
variables
b. by conditioning (on either  or 
 to determine 
7.69. Type  light bulbs function for a random amount of time having
mean 
 and standard deviation 
 A light bulb randomly
chosen from a bin of bulbs is a type 1 bulb with probability  and a
type 2 bulb with probability 
 Let  denote the lifetime of this
bulb. Find
a. 
b. 
7.70. The number of winter storms in a good year is a Poisson
random variable with mean 3, whereas the number in a bad year is a
Poisson random variable with mean 5. If next year will be a good
year with probability .4 or a bad year with probability .6, find the
expected value and variance of the number of storms that will occur.
7.71. In Example 5c
, compute the variance of the length of time
until the miner reaches safety.
7.72. Consider a gambler who, at each gamble, either wins or loses
her bet with respective probabilities  and 
 A popular gambling
system known as the Kelley strategy is to always bet the fraction
 of your current fortune when 
 Compute the expected
fortune after  gambles of a gambler who starts with  units and
employs the Kelley strategy.
7.73. The number of accidents that a person has in a given year is a
Poisson random variable with mean  However, suppose that the
value of  changes from person to person, being equal to 2 for 60
percent of the population and 3 for the other 40 percent. If a person
is chosen at random, what is the probability that he will have (a) 0
accidents and (b) exactly 3 accidents in a certain year? What is the
conditional probability that he will have 3 accidents in a given year,
given that he had no accidents the preceding year?
7.74. Repeat Problem 7.73
 when the proportion of the population
having a value of  less than  is equal to 
7.75 Consider an urn containing a large number of coins, and
suppose that each of the coins has some probability  of turning up
𝑋௜, 𝑌௝such that 𝑋ൌ෍
௜ൌଵ
ଵ଴
𝑋௜, 𝑌ൌ
෍
௝ൌଵ
଼
𝑌௝
𝑋
𝑌ሻ
𝐸ሾ𝑋𝑌ሿ.
𝑖
𝜇௜
𝜎௜, 𝑖ൌ1, 2.
𝑝
1 െ𝑝.
𝑋
𝐸ሾ𝑋ሿ;
Varሺ𝑋ሻ.
𝑝
1 െ𝑝.
2𝑝െ1
𝑝൐1
2 .
𝑛
𝑥
𝜆.
𝜆
𝜆
𝑥
1 െ𝑒െ௫.
𝑝
598 of 848

heads when it is flipped. However, this value of  varies from coin to
coin. Suppose that the composition of the urn is such that if a coin is
selected at random from it, then the -value of the coin can be
regarded as being the value of a random variable that is uniformly
distributed over [0, 1]. If a coin is selected at random from the urn
and flipped twice, compute the probability that
a. the first flip results in a head;
b. both flips result in heads.
7.76. In Problem 7.75
, suppose that the coin is tossed  times.
Let  denote the number of heads that occur. Show that
Hint: Make use of the fact that
when  and  are positive integers.
7.77. Suppose that in Problem 7.75
, we continue to flip the coin
until a head appears. Let  denote the number of flips needed. Find
a. 
b. 
c. 
7.78. In Example 6b
, let  denote the signal sent and  the signal
received.
a. Compute 
b. Compute 
c. Is  normally distributed?
d. Compute 
7.79. In Example 6c
, suppose that  is uniformly distributed over
(0, 1). If the discretized regions are determined by 
and 
 calculate the optimal quantizer  and compute
7.80 The moment generating function of  is given by
 and that of  by 
 If  and
 are independent, what are
a. 
𝑝
𝑝
𝑛
𝑋
𝑃ሼ𝑋ൌ𝑖ሽൌ
1
𝑛൅1  𝑖ൌ0, 1, …, 𝑛
඲
଴
ଵ
𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ𝑑𝑥ൌሺ𝑎െ1ሻ!ሺ𝑏െ1ሻ!
ሺ𝑎൅𝑏െ1ሻ!
𝑎
𝑏
𝑁
𝑃ሼ𝑁൒𝑖ሽ, 𝑖൒1;
𝑃ሼ𝑁ൌ𝑖ሽ;
𝐸ሾ𝑁ሿ.
𝑆
𝑅
𝐸ሾ𝑅ሿ.
Varሺ𝑅ሻ.
𝑅
Covሺ𝑅, 𝑆ሻ.
𝑋
𝑎଴ൌ0, 𝑎ଵൌ1
2 ,
𝑎ଶൌ1,
𝑌
𝐸ሾሺ𝑋െ𝑌ሻଶሿ.
𝑋
𝑀௑ሺ𝑡ሻൌexpሼ2𝑒௧െ2ሽ
𝑌
𝑀௒ሺ𝑡ሻൌቆ3
4 𝑒௧൅1
4ቇ
ଵ଴
.
𝑋
𝑌
𝑃ሼ𝑋൅𝑌ൌ2ሽ?
599 of 848

b. 
c. 
7.81 . Let  be the value of the first die and  the sum of the values
when two dice are rolled. Compute the joint moment generating
function of  and 
7.82 . The joint density of  and  is given by
a. Compute the joint moment generating function of  and 
b. Compute the individual moment generating functions.
7.83. Two envelopes, each containing a check, are placed in front of
you. You are to choose one of the envelopes, open it, and see the
amount of the check. At this point, either you can accept that amount
or you can exchange it for the check in the unopened envelope.
What should you do? Is it possible to devise a strategy that does
better than just accepting the first envelope?
Let  and 
 denote the (unknown) amounts of the checks,
and note that the strategy that randomly selects an envelope and
always accepts its check has an expected return of 
Consider the following strategy: Let 
 be any strictly increasing
(that is, continuous) distribution function. Choose an envelope
randomly and open it. If the discovered check has the value  then
accept it with probability 
 and exchange it with probability
a. Show that if you employ the latter strategy, then your expected
return is greater than 
b. Hint: Condition on whether the first envelope has the value 
or 
c. Now consider the strategy that fixes a value  and then
accepts the first check if its value is greater than  and
exchanges it otherwise.
a. Show that for any  the expected return under the 
-strategy is always at least 
 and that it is
strictly larger than 
 if  lies between  and 
b. Let  be a continuous random variable on the whole
line, and consider the following strategy: Generate the
value of 
 and if 
 then employ the -strategy of
part (b). Show that the expected return under this
𝑃ሼ𝑋𝑌ൌ0ሽ?
𝐸ሾ𝑋𝑌ሿ?
𝑋
𝑌
𝑋
𝑌.
𝑋
𝑌
𝑓ሺ𝑥, 𝑦ሻൌ
1
2𝜋
√
𝑒െ௬𝑒െሺ௫െ௬ሻమ/ଶ   0 ൏𝑦൏∞,
െ∞൏𝑥൏∞
𝑋
𝑌.
𝐴
𝐵, 𝐴൏𝐵,
ሺ𝐴൅𝐵ሻ/2.
𝐹ሺ⋅ሻ
𝑥,
𝐹ሺ𝑥ሻ
1 െ𝐹ሺ𝑥ሻ.
ሺ𝐴൅𝐵ሻ/2
𝐴
𝐵.
𝑥
𝑥
𝑥,
𝑥
ሺ𝐴൅𝐵ሻ/2
ሺ𝐴൅𝐵ሻ/2
𝑥
𝐴
𝐵.
𝑋
𝑋,
𝑋ൌ𝑥,
𝑥
600 of 848

strategy is greater than 
7.84. Successive weekly sales, in units of 
 have a bivariate
normal distribution with common mean 
 common standard
deviation  and correlation 
a. Find the probability that the total of the next  weeks’ sales
exceeds 
b. If the correlation were  rather than 
 do you think that this
would increase or decrease the answer to (a)? Explain your
reasoning.
c. Repeat (a) when the correlation is 
ሺ𝐴൅𝐵ሻ/2.
$1, 000,
40,
6,
.6.
2
90.
.2
.6,
.2.
7.1. Show that 
 is minimized at 
7.2. Suppose that  is a continuous random variable with density function 
Show that 
 is minimized when  is equal to the median of 
Hint: Write
Now break up the integral into the regions where 
 and where 
 and
differentiate.
7.3 Prove Proposition 2.1
 when
a. 
and  have a joint probability mass function;
b.  and  have a joint probability density function and 
 for all
7.4 Let  be a random variable having finite expectation  and variance 
and let 
 be a twice differentiable function. Show that
Hint: Expand 
 in a Taylor series about  Use the first three terms and
ignore the remainder.
7.5 If 
 and  is a differentiable function such that 
 show that
Hint: Define random variables 
 so that
𝐸ሾሺ𝑋െ𝑎ሻଶሿ
𝑎ൌ𝐸ሾ𝑋ሿ.
𝑋
𝑓.
𝐸ሾ||𝑋െ𝑎||ሿ
𝑎
𝐹.
𝐸ሾ|𝑋െ𝑎|ሿൌ඲|𝑥െ𝑎|𝑓ሺ𝑥ሻ𝑑𝑥
𝑥൏𝑎
𝑥൐𝑎,
𝑋
𝑌
𝑋
𝑌
𝑔ሺ𝑥, 𝑦ሻ൒0
𝑥, 𝑦.
𝑋
𝜇
𝜎ଶ,
𝑔ሺ⋅ሻ
𝐸ሾ𝑔ሺ𝑋ሻሿൎ𝑔ሺ𝜇ሻ൅𝑔ᇳሺ𝜇ሻ
2
𝜎ଶ
𝑔ሺ⋅ሻ
𝜇.
𝑋൒0
𝑔
𝑔ሺ0ሻൌ0,
𝐸ሾ𝑔ሺ𝑋ሻሿൌ඲
଴
ஶ
𝑃ሺ𝑋൐𝑡ሻ𝑔ᇱሺ𝑡ሻ 𝑑𝑡
𝐼ሺ𝑡ሻ, 𝑡൒0
601 of 848

7.6. Let 
 be arbitrary events, and define
 Show that
Hint: Let  denote the number of the 
 that occur. Show that both sides of
the preceding equation are equal to 
7.7. In the text, we noted that
when the 
 are all nonnegative random variables. Since an integral is a limit
of sums, one might expect that
whenever 
 are all nonnegative random variables; this result is
indeed true. Use it to give another proof of the result that for a nonnegative
random variable 
Hint: Define, for each nonnegative  the random variable 
 by
Now relate 
 to 
7.8. We say that  is stochastically larger than 
 written 
 if, for all 
Show that if 
 then 
 when
a.  and  are nonnegative random variables;
b.  and  are arbitrary random variables.
Hint: Write  as
𝑔ሺ𝑋ሻൌ඲
଴
௑
𝑔ᇱሺ𝑡ሻ 𝑑𝑡ൌ඲
଴
ஶ
𝐼ሺ𝑡ሻ𝑔ᇱሺ𝑡ሻ 𝑑𝑡
𝐴ଵ, 𝐴ଶ, …, 𝐴௡
𝐶௞ൌሼat least 𝑘𝑜𝑓 the 𝐴௜ occurሽ.
෍
௞ൌଵ
௡
𝑃ሺ𝐶௞ሻൌ
෍
௞ൌଵ
௡
𝑃ሺ𝐴௞ሻ
𝑋
𝐴௜
𝐸ሾ𝑋ሿ.
𝐸቎෍
௜ൌଵ
ஶ
𝑋௜቏ൌ෍
௜ൌଵ
ஶ
𝐸ሾ𝑋௜ሿ
𝑋௜
𝐸ቈ඲
଴
ஶ
𝑋ሺ𝑡ሻ𝑑𝑡቉ൌ඲
଴
ஶ
𝐸ሾ𝑋ሺ𝑡ሻሿ𝑑𝑡
𝑋ሺ𝑡ሻ, 0 ൑𝑡൏∞,
𝑋,
𝐸ሾ𝑋ሻൌ඲
଴
ஶ
𝑃ሼ𝑋൐𝑡ሽ𝑑𝑡
𝑡,
𝑋ሺ𝑡ሻ
𝑋ሺ𝑡ሻൌቊ1
if 𝑡൏𝑋
0
if 𝑡൒𝑋
׬଴
ஶ𝑋ሺ𝑡ሻ𝑑𝑡
𝑋.
𝑋
𝑌,
𝑋൒௦௧𝑌,
𝑡,
𝑃ሼ𝑋൐𝑡ሽ൒𝑃ሼ𝑌൐𝑡ሽ
𝑋൒௦௧𝑌,
𝐸ሾ𝑋ሿ൒𝐸ሾ𝑌ሿ
𝑋
𝑌
𝑋
𝑌
𝑋
602 of 848

where
Similarly, represent  as 
 Then make use of part (a).
7.9. Show that  is stochastically larger than  if and only if
for all increasing functions 
Hint: Show that 
 then 
 by showing that
 and then using Theoretical Exercise 7.8
. To show that if
 for all increasing functions  then 
define an appropriate increasing function 
7.10. A coin having probability  of landing on heads is flipped  times.
Compute the expected number of runs of heads of size 1, of size 2, and of
size 
7.11. Let 
 be independent and identically distributed positive
random variables. For 
 find
7.12. Consider  independent trials, each resulting in any one of  possible
outcomes with probabilities 
 Let  denote the number of outcomes
that never occur in any of the trials. Find 
 and show that among all
probability vectors 
 is minimized when 
7.13. Let 
 be a sequence of independent random variables having the
probability mass function
The random variable 
 is said to have the Cantor distribution.
Find 
 and 
7.14. Let 
 be independent and identically distributed continuous
random variables. We say that a record value occurs at time 
 if 
for all 
 Show that
a. 
𝑋ൌ𝑋൅െ𝑋െ
𝑋൅ൌቊ𝑋if 𝑋൒0
0
if 𝑋൏0,  𝑋െൌቊ
0
if 𝑋൒0
െ𝑋
if 𝑋൐0
𝑌
𝑌൅െ𝑌െ.
𝑋
𝑌
𝐸ሾ𝑓ሺ𝑋ሻሿ൒𝐸ሾ𝑓ሺ𝑌ሻሿ
𝑓.
𝑋൒௦௧𝑌,
𝐸ሾ𝑓ሺ𝑋ሻሿ൒𝐸ሾ𝑓ሺ𝑌ሻሿ
𝑓ሺ𝑋ሻ൒௦௧𝑓ሺ𝑌ሻ
𝐸ሾ𝑓ሺ𝑋ሻሿ൒𝐸ሾ𝑓ሺ𝑌ሻሿ
𝑓,
𝑃ሼ𝑋൐𝑡ሽ൒𝑃ሼ𝑌൐𝑡ሽ,
𝑓.
𝑝
𝑛
𝑘, 1 ൑𝑘൑𝑛.
𝑋ଵ, 𝑋ଶ, …, 𝑋௡
𝑘൑𝑛,
𝐸
⎡
⎣
⎢
⎢
⎢⎢
⎢
⎢
෍
௜ൌଵ
௞
𝑋௜
෍
௜ൌଵ
௡
𝑋௜
⎤
⎦
⎥
⎥
⎥⎥
⎥
⎥
𝑛
𝑟
𝑃ଵ, 𝑃ଶ, …, 𝑃௥.
𝑋
𝐸ሾ𝑋ሿ
𝑃ଵ, …, 𝑃௥, 𝐸ሾ𝑋ሿ
𝑃௜ൌ1/𝑟, 𝑖ൌ1, …, 𝑟.
𝑋ଵ, 𝑋ଶ, …
𝑃ሼ𝑋௡ൌ0ሽൌ𝑃ሼ𝑋௡ൌ2ሽൌ1/2 ,  𝑛൒1
𝑋ൌ∑௡ൌଵ
ஶ
𝑋௡/3௡
𝐸ሾ𝑋ሿ
Varሺ𝑋ሻ.
𝑋ଵ, …, 𝑋௡
𝑗, 𝑗൑𝑛,
𝑋௝൒𝑋௜
1 ൑𝑖൑𝑗.
𝐸ሾnumber of record valuesሿൌ
෍
௝ൌଵ
௡
1/𝑗;
603 of 848

b. 
7.15 For Example 2i
, show that the variance of the number of coupons
needed to amass a full set is equal to
When  is large, this can be shown to be approximately equal (in the sense
that their ratio approaches 1 as 
 to 
7.16 Consider  independent trials, the th of which results in a success with
probability 
a. Compute the expected number of successes in the  trials—call it 
b. For a fixed value of  what choice of 
 maximizes the variance
of the number of successes?
c. What choice minimizes the variance?
* 7.17. Suppose that each of the elements of 
 is to be colored
either red or blue. Show that if 
 are subsets of  there is a way of
doing the coloring so that at most 
 of these subsets have all
their elements the same color (where 
 denotes the number of elements in
the set 
.
7.18. Suppose that 
 and 
 are independent random variables having a
common mean  Suppose also that 
 and 
 The value
of  is unknown, and it is proposed that  be estimated by a weighted average
of 
 and 
 That is, 
 will be used as an estimate of  for
some appropriate value of  Which value of  yields the estimate having the
lowest possible variance? Explain why it is desirable to use this value of 
7.19. In Example 4f
, we showed that the covariance of the multinomial
random variables 
 and 
 is equal to 
 by expressing 
 and 
 as
the sum of indicator variables. We could also have obtained that result by
using the formula
a. What is the distribution of 
b. Use the preceding identity to show that 
7.20. Show that  and  are identically distributed and not necessarily
Varሺnumber of record valuesሻൌ
෍
௝ൌଵ
௡
ሺ𝑗െ1ሻ/𝑗ଶ.
෍
௜ൌଵ
ேെଵ
𝑖𝑁
ሺ𝑁െ𝑖ሻଶ
𝑁
𝑁→∞ሻ
𝑁ଶ𝜋ଶ/6.
𝑛
𝑖
𝑃௜
𝑛
𝜇.
𝜇,
𝑃ଵ, …, 𝑃௡
𝑆ൌሼ1, 2, …, 𝑛ሽ
𝐴ଵ, …, 𝐴௥
𝑆,
෍
௜ൌଵ
௥
ሺ1/2ሻห஺೔หെଵ
|𝐴|
𝐴ሻ
𝑋ଵ
𝑋ଶ
𝜇.
Varሺ𝑋ଵሻൌ𝜎ଵ
ଶ
Varሺ𝑋ଶሻൌ𝜎ଶ
ଶ
𝜇
𝜇
𝑋ଵ
𝑋ଶ.
𝜆𝑋ଵ൅ሺ1 െ𝜆ሻ𝑋ଶ
𝜇
𝜆.
𝜆
𝜆.
𝑁௜
𝑁௝
െ𝑚𝑃௜𝑃௝
𝑁௜
𝑁௝
Var൫𝑁௜൅𝑁௝൯ൌ Varሺ𝑁௜ሻ൅ Var൫𝑁௝൯൅2 Cov൫𝑁௜, 𝑁௝൯
𝑁௜൅𝑁௝?
Cov൫𝑁௜, 𝑁௝൯ൌെ𝑚𝑃௜𝑃௝.
𝑋
𝑌
604 of 848

independent, then
7.21. The Conditional Covariance Formula. The conditional covariance of 
and 
 given 
 is defined by
a. Show that
b. Prove the conditional covariance formula
c. Set 
 in part (b) and obtain the conditional variance formula.
7.22. Let 
 denote the order statistics from a set of  uniform (0,
1) random variables, and note that the density function of 
 is given by
a. Compute 
b. Which value of  minimizes, and which value maximizes, 
7.23. Show that 
 then
7.24. Show that  is a standard normal random variable and if  is defined by
 then
7.25. Prove the Cauchy–Schwarz inequality, namely,
Hint: Unless 
 for some constant, in which case the inequality holds
with equality, it follows that for all 
Covሺ𝑋൅𝑌, 𝑋െ𝑌ሻൌ0
𝑋
𝑌,
𝑍,
Covሺ𝑋, 𝑌||𝑍ሻ≡𝐸ሾሺ𝑋െ𝐸ሾ𝑋||𝑍ሿሻሺ𝑌െ𝐸ሾ𝑌||𝑍ሿሻ||𝑍ሿ
Covሺ𝑋,  𝑌||𝑍ሻൌ𝐸ሾ𝑋𝑌|𝑍ሿെ𝐸ሾ𝑋|𝑍ሿ𝐸ሾ𝑌|𝑍ሿ
Covሺ𝑋, 𝑌ሻ
ൌ
𝐸ሾCovሺ𝑋, 𝑌||𝑍ሻሿ
൅Covሺ𝐸ሾ𝑋||𝑍ሿ, 𝐸ሾ𝑌||𝑍ሿሻ
𝑋ൌ𝑌
𝑋൫௜൯, 𝑖ൌ1, …, 𝑛,
𝑛
𝑋൫௜൯
𝑓ሺ𝑥ሻൌ
𝑛!
ሺ𝑖െ1ሻ!ሺ𝑛െ𝑖ሻ! 𝑥௜െଵሺ1 െ𝑥ሻ௡െ௜ 0 ൏𝑥൏1
Varቀ𝑋൫௜൯ቁ, 𝑖ൌ1, …, 𝑛.
𝑖
Varቀ𝑋൫௜൯ቁ?
𝑌ൌ𝑎൅𝑏𝑋,
𝜌ሺ𝑋, 𝑌ሻൌቊ൅1
if 𝑏൐0
െ1
if 𝑏൏0
𝑍
𝑌
𝑌ൌ𝑎൅𝑏𝑍൅𝑐𝑍ଶ,
𝜌ሺ𝑌, 𝑍ሻൌ
𝑏
𝑏ଶ൅2𝑐ଶ
ඥ
൫𝐸ൣ𝑋𝑌൧൯
ଶ൑𝐸ൣ𝑋ଶ൧𝐸ൣ𝑌ଶ൧
𝑌ൌെ𝑡𝑋
𝑡,
0 ൏𝐸ൣ൫𝑡𝑋൅𝑌൯
ଶ൧ൌ𝐸ൣ𝑋ଶ൧𝑡ଶ൅2𝐸ൣ𝑋𝑌൧𝑡൅𝐸ൣ𝑌ଶ൧
605 of 848

Hence, the roots of the quadratic equation
must be imaginary, which implies that the discriminant of this quadratic
equation must be negative.
7.26. Show that if  and  are independent, then
a. in the discrete case;
b. in the continuous case.
7.27. Prove that 
7.28. Prove that if 
 for all  then  and  are uncorrelated;
give a counterexample to show that the converse is not true.
Hint: Prove and use the fact that 
7.29. Show that 
7.30. Let 
 be independent and identically distributed random
variables. Find
7.31. Consider Example 4f
, which is concerned with the multinomial
distribution. Use conditional expectation to compute 
 and then use
this to verify the formula for 
 given in Example 4f
.
7.32. An urn initially contains  black and  white balls. At each stage, we add
 black balls and then withdraw, at random,  balls from the 
 balls in
the urn. Show that
7.33. For an event 
 let 
 equal 1 if  occurs and let it equal 0 if  does not
occur. For a random variable 
 show that
7.34. A coin that lands on heads with probability  is continually flipped.
Compute the expected number of flips that are made until a string of  heads
in a row is obtained.
Hint: Condition on the time of the first occurrence of tails to obtain the
equation
𝐸ൣ𝑋ଶ൧𝑡ଶ൅2𝐸ൣ𝑋𝑌൧𝑡൅𝐸ൣ𝑌ଶ൧ൌ0
𝑋
𝑌
𝐸ሾ𝑋||𝑌ൌ𝑦ሿൌ𝐸ሾ𝑋ሿ for all 𝑦
𝐸ሾ𝑔ሺ𝑋ሻ𝑌||𝑋ሿൌ𝑔ሺ𝑋ሻ𝐸ሾ𝑌||𝑋ሿ.
𝐸ሾ𝑌||𝑋ൌ𝑥ሿൌ𝐸ሾ𝑌ሿ
𝑥,
𝑋
𝑌
𝐸ሾ𝑋𝑌ሿൌ𝐸ሾ𝑋𝐸ሾ𝑌||𝑋ሿሿ.
Covሺ𝑋, 𝐸ሾ𝑌||𝑋ሿሻൌCovሺ𝑋, 𝑌ሻ.
𝑋ଵ, …, 𝑋௡
𝐸ሾ𝑋ଵ||𝑋ଵ൅⋯൅𝑋௡ൌ𝑥ሿ
𝐸ൣ𝑁௜𝑁௝൧,
Cov൫𝑁௜, 𝑁௝൯
𝑏
𝑤
𝑟
𝑟
𝑏൅𝑤൅𝑟
𝐸ሾnumber of white balls after stage 𝑡ሿ
  ൌቆ
𝑏൅𝑤
𝑏൅𝑤൅𝑟ቇ
௧
𝑤
𝐴,
𝐼஺
𝐴
𝐴
𝑋,
𝐸ሾ𝑋|𝐴ሿൌ𝐸ሾ𝑋𝐼஺ሿ
𝑃ሺ𝐴ሻ
𝑝
𝑟
606 of 848

Simplify and solve for 
7.35. For another approach to Theoretical Exercise 7.34
, let 
 denote
the number of flips required to obtain a run of  consecutive heads.
a. Determine 
b. Determine 
 in terms of 
c. What is 
d. What is 
7.36. The probability generating function of the discrete nonnegative integer
valued random variable  having probability mass function 
 is defined
by
Let  be a geometric random variable with parameter 
 where
 Suppose that  is independent of 
 and show that
7.37. One ball at a time is randomly selected from an urn containing  white
and  black balls until all of the remaining balls are of the same color. Let 
denote the expected number of balls left in the urn when the experiment ends.
Compute a recursive formula for 
 and solve when 
 and 
7.38. An urn contains  white and  black balls. After a ball is drawn, it is
returned to the urn if it is white; but if it is black, it is replaced by a white ball
from another urn. Let 
 denote the expected number of white balls in the urn
after the foregoing operation has been repeated  times.
a. Derive the recursive equation
b. Use part (a) to prove that
𝐸ሾ𝑋ሿ
ൌሺ1 െ𝑝ሻ෍
௜ൌଵ
௥
𝑝௜െଵሺ𝑖൅𝐸ሾ𝑋ሿሻ
൅ሺ1 െ𝑝ሻ
෍
௜ൌ௥൅ଵ
ஶ
𝑝௜െଵ𝑟
𝐸ሾ𝑋ሿ.
𝑇௥
𝑟
𝐸ሾ𝑇௥||𝑇௥െଵሿ.
𝐸ሾ𝑇௥ሿ
𝐸ሾ𝑇௥െଵሿ.
𝐸ሾ𝑇ଵሿ?
𝐸ሾ𝑇௥ሿ?
𝑋
𝑝௝, 𝑗൒0,
𝜙ሺ𝑠ሻൌ𝐸ሾ𝑠௑ሿൌ
෍
௝ൌ଴
ஶ
𝑝௝𝑠௝
𝑌
𝑝ൌ1 െ𝑠,
0 ൏𝑠൏1.
𝑌
𝑋,
𝜙ሺ𝑠ሻൌ𝑃ሼ𝑋൏𝑌ሽ
𝑎
𝑏
𝑀௔,௕
𝑀௔,௕
𝑎ൌ3
𝑏ൌ5.
𝑎
𝑏
𝑀௡
𝑛
𝑀௡൅ଵൌቆ1 െ
1
𝑎൅𝑏ቇ𝑀௡൅1
𝑀௡ൌ𝑎൅𝑏െ𝑏ቆ1 െ
1
𝑎൅𝑏ቇ
௡
607 of 848

c. What is the probability that the 
 ball drawn is white?
7.39. The best linear predictor of  with respect to 
 and 
 is equal to
 where 
 and  are chosen to minimize
Determine 
 and 
7.40. The best quadratic predictor of  with respect to  is 
where 
 and  are chosen to minimize 
 Determine
 and 
7.41. Use the conditional variance formula to determine the variance of a
geometric random variable  having parameter 
7.42. Let  be a normal random variable with parameters 
 and 
and let  independent of 
 be such that 
 Now define
 by
In words,  is equally likely to equal either  or 
a. Are  and  independent?
b. Are  and  independent?
c. Show that  is normal with mean 0 and variance 1.
d. Show that 
7.43. It follows from Proposition 6.1
 and the fact that the best linear
predictor of  with respect to  is 
 that if
then
(Why?) Verify this directly.
7.44. Show that for random variables  and 
where
7.45. Consider a population consisting of individuals able to produce offspring
ሺ𝑛൅1ሻ
𝑌
𝑋ଵ
𝑋ଶ
𝑎൅𝑏𝑋ଵ൅𝑐𝑋ଶ,
𝑎, 𝑏,
𝑐
𝐸ሾሺ𝑌െሺ𝑎൅𝑏𝑋ଵ൅𝑐𝑋ଶሻሻଶሿ
𝑎, 𝑏,
𝑐.
𝑌
𝑋
𝑎൅𝑏𝑋൅𝑐𝑋ଶ,
𝑎, 𝑏,
𝑐
𝐸ൣ൫𝑌െ൫𝑎൅𝑏𝑋൅𝑐𝑋ଶ൯൯
ଶ൧.
𝑎, 𝑏,
𝑐.
𝑋
𝑝.
𝑋
𝜇ൌ0
𝜎ଶൌ1,
𝐼,
𝑋,
𝑃ሼ𝐼ൌ1ሽൌ1
2 ൌ𝑃ሼ𝐼ൌ0ሽ.
𝑌
𝑌ൌቊ
𝑋
if 𝐼ൌ1
െ𝑋
if 𝐼ൌ0
𝑌
𝑋
െ𝑋.
𝑋
𝑌
𝐼
𝑌
𝑌
Covሺ𝑋, 𝑌ሻൌ0.
𝑌
𝑋
𝜇௬൅𝜌𝜎௬
𝜎௫
ሺ𝑋െ𝜇௫ሻ
𝐸ሾ𝑌||𝑋ሿൌ𝑎൅𝑏𝑋
𝑎ൌ𝜇௬െ𝜌𝜎௬
𝜎௫
𝜇௫ 𝑏ൌ𝜌𝜎௬
𝜎௫
𝑋
𝑍,
𝐸ൣ൫𝑋െ𝑌൯
ଶ൧ൌ𝐸ൣ𝑋ଶ൧െ𝐸ൣ𝑌ଶ൧
𝑌ൌ𝐸ሾ𝑋||𝑍ሿ
608 of 848

of the same kind. Suppose that by the end of its lifetime, each individual will
have produced  new offspring with probability 
 independently of the
number produced by any other individual. The number of individuals initially
present, denoted by 
 is called the size of the zeroth generation. All
offspring of the zeroth generation constitute the first generation, and their
number is denoted by 
 In general, let 
 denote the size of the th
generation. Let 
 and 
 denote, respectively,
the mean and the variance of the number of offspring produced by a single
individual. Suppose that 
 that is, initially there is a single individual in
the population.
a. Show that
b. Use part (a) to conclude that
c. Show that
d. Use part (c) to conclude that
e. The model just described is known as a branching process, and an
important question for a population that evolves along such lines is the
probability that the population will eventually die out. Let  denote this
probability when the population starts with a single individual. That is,
f. Argue that  satisfies
Hint: Condition on the number of offspring of the initial member of the
population.
7.46. Verify the formula for the moment generating function of a uniform
random variable that is given in Table 7.2
. Also, differentiate to verify the
formulas for the mean and variance.
𝑗
𝑃௝, 𝑗൒0,
𝑋଴,
𝑋ଵ.
𝑋௡
𝑛
𝜇ൌ
෍
௝ൌ଴
ஶ
𝑗𝑃௝
𝜎ଶൌ
෍
௝ൌ଴
ஶ
ሺ𝑗െ𝜇ሻଶ𝑃௝
𝑋଴ൌ1
𝐸ሾ𝑋௡ሿൌ𝜇𝐸ሾ𝑋௡െଵሿ
𝐸ሾ𝑋௡ሿൌ𝜇௡
Varሺ𝑋௡ሻൌ𝜎ଶ𝜇௡െଵ൅𝜇ଶVarሺ𝑋௡െଵሻ
Varሺ𝑋௡ሻൌ൞
𝜎ଶ𝜇௡െଵቆ𝜇௡െ1
𝜇െ1 ቇ
if 𝜇്1
𝑛𝜎ଶ
if 𝜇ൌ1
𝜋
𝜋ൌ𝑃ሼpopulation eventually dies out||𝑋଴ൌ1ሻ
𝜋
𝜋ൌ
෍
௝ൌ଴
ஶ
𝑃௝𝜋௝
609 of 848

7.47. For a standard normal random variable 
 let 
 Show that
Hint: Start by expanding the moment generating function of  into a Taylor
series about 0 to obtain
7.48. Let  be a normal random variable with mean  and variance 
 Use
the results of Theoretical Exercise 7.47
 to show that
In the preceding equation, 
 is the largest integer less than or equal to
 Check your answer by letting 
 and 
7.49. If 
 where  and  are constants, express the moment
generating function of  in terms of the moment generating function of 
7.50. The positive random variable  is said to be a lognormal random
variable with parameters  and 
 if 
 is a normal random variable with
mean  and variance 
 Use the normal moment generating function to find
the mean and variance of a lognormal random variable.
7.51. Let  have moment generating function 
 and define 
Show that
7.52. Use Table 7.2
 to determine the distribution of 
 when 
are independent and identically distributed exponential random variables,
each having mean 
7.53. Show how to compute 
 from the joint moment generating
function of  and 
7.54. Suppose that 
 have a multivariate normal distribution. Show that
 are independent random variables if and only if
𝑍,
𝜇௡ൌ𝐸ൣ𝑍௡൧.
𝜇௡ൌ൞
0
when 𝑛 is odd
ሺ2𝑗ሻ!
2௝𝑗!
when 𝑛ൌ2𝑗
𝑍
𝐸ሾ𝑒௧௓ሿ
ൌ𝑒௧మ/ଶ
ൌ
෍
௝ൌ଴
ஶ
ሺ𝑡ଶ/2ሻ
௝
𝑗!
𝑋
𝜇
𝜎ଶ.
𝐸ሾ𝑋௡ሿൌ
෍
௝ൌଵ
ሾ௡/ଶሿቆ𝑛
2𝑗ቇ𝜇௡െଶ௝𝜎ଶ௝ሺ2𝑗ሻ!
2௝𝑗!
ሾ𝑛/2ሿ
𝑛/2.
𝑛ൌ1
𝑛ൌ2.
𝑌ൌ𝑎𝑋൅𝑏,
𝑎
𝑏
𝑌
𝑋.
𝑋
𝜇
𝜎ଶ
logሺ𝑋ሻ
𝜇
𝜎ଶ.
𝑋
𝑀ሺ𝑡ሻ,
𝛹ሺ𝑡ሻൌlog𝑀ሺ𝑡ሻ.
𝛹ᇳሺ𝑡ሻ||௧ൌ଴ൌ Varሺ𝑋ሻ
෍
௜ൌଵ
௡
𝑋௜
𝑋ଵ, …, 𝑋௡
1/𝜆.
Covሺ𝑋, 𝑌ሻ
𝑋
𝑌.
𝑋ଵ, …, 𝑋௡
𝑋ଵ, …, 𝑋௡
Cov൫𝑋௜, 𝑋௝൯ൌ0 when 𝑖്j
610 of 848

7.55. If  is a standard normal random variable, what is 
7.56. Suppose that  is a normal random variable with mean  and variance
 and suppose also that the conditional distribution of 
 given that 
 is
normal with mean  and variance 
a. Argue that the joint distribution of 
 is the same as that of 
when  is a standard normal random variable that is independent of 
b. Use the result of part (a) to argue that 
 has a bivariate normal
distribution.
c. Find 
 and 
d. Find 
e. What is the conditional distribution of  given that 
𝑍
Covሺ𝑍, 𝑍ଶሻ?
𝑌
𝜇
𝜎ଶ,
𝑋,
𝑌ൌ𝑦,
𝑦
1.
𝑋, 𝑌
𝑌൅𝑍, 𝑌
𝑍
𝑌.
𝑋, 𝑌
𝐸ሾ𝑋ሿ, Varሺ𝑋ሻ,
Corr ሺ𝑋, 𝑌ሻ.
𝐸ሾ𝑌||𝑋ൌ𝑥ሿ.
𝑌
𝑋ൌ𝑥?
7.1. Consider a list of 
 names, where the same name may appear more than
once on the list. Let 
 denote the number of times that the
name in position  appears on the list, and let  denote the number of distinct
names on the list.
a. Express  in terms of the variables 
 Let  be a
uniform (0, 1) random variable, and let 
b. What is the probability mass function of 
c. Argue that 
7.2 An urn has  white and 
 black balls that are removed one at a time in a
randomly chosen order. Find the expected number of instances in which a
white ball is immediately followed by a black one.
7.3 Twenty individuals consisting of 10 married couples are to be seated at 5
different tables, with 4 people at each table.
a. If the seating is done “at random,” what is the expected number of
married couples that are seated at the same table?
b. If 2 men and 2 women are randomly chosen to be seated at each table,
what is the expected number of married couples that are seated at the
same table?
7.4 If a die is to be rolled until all sides have appeared at least once, find the
expected number of times that outcome 1 appears.
7.5. A deck of 2  cards consists of  red and  black cards. The cards are
shuffled and then turned over one at a time. Suppose that each time a red
card is turned over, we win 1 unit if more red cards than black cards have
been turned over by that time. (For instance, if 
 and the result is 
𝑚
𝑛ሺ𝑖ሻ, 𝑖ൌ1, …, 𝑚,
𝑖
𝑑
𝑑
𝑚, 𝑛ሺ𝑖ሻ, 𝑖ൌ1, …, 𝑚.
𝑈
𝑋ൌሾ𝑚𝑈ሿ൅1.
𝑋?
𝐸ሾ𝑚/𝑛ሺ𝑋ሻሿൌ𝑑.
𝑛
𝑚
𝑛
𝑛
𝑛
𝑛ൌ2
𝑟 𝑏 𝑟 𝑏,
611 of 848

then we would win a total of 2 units.) Find the expected amount that we win.
7.6. Let 
 be events, and let  denote the number of them that
occur. Also, let 
 if all of these events occur, and let it be 0 otherwise.
Prove Bonferroni’s inequality, namely,
Hint: Argue first that 
7.7. Let  be the smallest value obtained when  numbers are randomly
chosen from the set 
 Find 
 by interpreting  as a negative
hypergeometric random variable.
7.8. An arriving plane carries  families. A total of 
 of these families have
checked in a total of  pieces of luggage, 
 Suppose that when the
plane lands, the 
 pieces of luggage come out of the plane in a
random order. As soon as a family collects all of its luggage, it immediately
departs the airport. If the Sanchez family checked in  pieces of luggage, find
the expected number of families that depart after they do.
* 7.9. Nineteen items on the rim of a circle of radius 1 are to be chosen. Show
that for any choice of these points, there will be an arc of (arc) length 1 that
contains at least 4 of them.
7.10. Let  be a Poisson random variable with mean  Show that if  is not
too small, then
Hint: Use the result of Theoretical Exercise 7.4
 to approximate 
7.11. Suppose in Self-Test Problem 7.3
 that the 20 people are to be
seated at seven tables, three of which have 4 seats and four of which have 2
seats. If the people are randomly seated, find the expected value of the
number of married couples that are seated at the same table.
7.12. Individuals 1 through 
 are to be recruited into a firm in the
following manner: Individual 1 starts the firm and recruits individual 2.
Individuals 1 and 2 will then compete to recruit individual 3. Once individual 3
is recruited, individuals 1, 2, and 3 will compete to recruit individual 4, and so
on. Suppose that when individuals 
 compete to recruit individual 
each of them is equally likely to be the successful recruiter.
a. Find the expected number of the individuals 
 who did not recruit
anyone else.
b. Derive an expression for the variance of the number of individuals who
did not recruit anyone else, and evaluate it for 
𝐴ଵ, 𝐴ଶ, …, 𝐴௡
𝑁
𝐼ൌ1
𝑃ሺ𝐴ଵ⋯𝐴௡ሻ൒෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻെሺ𝑛െ1ሻ
𝑁൑𝑛െ1 ൅𝐼.
𝑋
𝑘
1, …, 𝑛.
𝐸ሾ𝑋ሿ
𝑋
𝑟
𝑛௝
𝑗
෍
௝
𝑛௝ൌ𝑟.
𝑁ൌ෍
௝
𝑗𝑛௝
𝑗
𝑋
𝜆.
𝜆
Var൫𝑋
√൯ൎ. 25
𝐸ൣ𝑋
√൧.
𝑛, 𝑛൐1,
1, 2, …, 𝑖
𝑖൅1,
1, …, 𝑛
𝑛ൌ5.
612 of 848

7.13 The nine players on a basketball team consist of 2 centers, 3 forwards,
and 4 backcourt players. If the players are paired up at random into three
groups of size 3 each, find (a) the expected value and (b) the variance of the
number of triplets consisting of one of each type of player.
7.14 A deck of 52 cards is shuffled and a bridge hand of 13 cards is dealt out.
Let  and  denote, respectively, the number of aces and the number of
spades in the hand.
a. Show that  and  are uncorrelated.
b. Are they independent?
7.15 Each coin in a bin has a value attached to it. Each time that a coin with
value  is flipped, it lands on heads with probability  When a coin is
randomly chosen from the bin, its value is uniformly distributed on (0, 1).
Suppose that after the coin is chosen but before it is flipped, you must predict
whether it will land on heads or on tails. You will win 1 if you are correct and
will lose 1 otherwise.
a. What is your expected gain if you are not told the value of the coin?
b. Suppose now that you are allowed to inspect the coin before it is
flipped, with the result of your inspection being that you learn the value
of the coin. As a function of  the value of the coin, what prediction
should you make?
c. Under the conditions of part (b), what is your expected gain?
7.16. In Self-Test Problem 7.1
, we showed how to use the value of a
uniform (0, 1) random variable (commonly called a random number) to obtain
the value of a random variable whose mean is equal to the expected number
of distinct names on a list. However, its use required that one choose a
random position and then determine the number of times that the name in that
position appears on the list. Another approach, which can be more efficient
when there is a large amount of replication of names, is as follows: As before,
start by choosing the random variable  as in Problem 7.1
. Now identify
the name in position 
 and then go through the list, starting at the beginning,
until that name appears. Let  equal 0 if you encounter that name before
getting to position 
 and let  equal 1 if your first encounter with the name is
at position 
 Show that 
Hint: Compute 
 by using conditional expectation.
7.17. A total of 
 items are to be sequentially distributed among  cells, with
each item independently being put in cell  with probability 
 Find
the expected number of collisions that occur, where a collision occurs
whenever an item is put into a nonempty cell.
7.18. Let  be the length of the initial run in a random ordering of  ones and
𝑋
𝑌
𝑋
𝑌
𝑝
𝑝.
𝑝,
𝑋
𝑋,
𝐼
𝑋,
𝐼
𝑋.
𝐸ሾ𝑚𝐼ሿൌ𝑑.
𝐸ሾ𝐼ሿ
𝑚
𝑛
𝑗
𝑝௝, 𝑗ൌ1, …, 𝑛.
𝑋
𝑛
613 of 848

 zeros. That is, if the first  values are the same (either all ones or all zeros),
then 
 Find 
7.19. There are  items in a box labeled  and 
 in a box labeled 
 A coin
that comes up heads with probability  and tails with probability 
 is
flipped. Each time it comes up heads, an item is removed from the  box, and
each time it comes up tails, an item is removed from the  box. (If a box is
empty and its outcome occurs, then no items are removed.) Find the expected
number of coin flips needed for both boxes to become empty.
Hint: Condition on the number of heads in the first 
 flips.
7.20. Let  be a nonnegative random variable having distribution function 
Show that if 
 then
Hint: Start with the identity
where
* 7.21. Let 
 not all equal to  be such that 
 Show that
there is a permutation 
 such that 
Hint: Use the probabilistic method. (It is interesting that there need not be a
permutation whose sum of products of successive pairs is positive. For
instance, if 
 and 
 there is no such permutation.)
7.22. Suppose that 
 are independent Poisson random variables
with respective means 
 Let 
 and 
 The
random vector 
 is said to have a bivariate Poisson distribution.
a. Find 
 and 
b. Find 
c. Find the joint probability mass function 
7.23. Let 
 be a sequence of independent and identically
distributed random vectors. That is, 
 is independent of, and has the
same distribution as, 
 and so on. Although 
 and 
 can be dependent,
𝑚
𝑘
𝑋൒𝑘.
𝐸ሾ𝑋ሿ.
𝑛
𝐻
𝑚
𝑇.
𝑝
1 െ𝑝
𝐻
𝑇
𝑛൅𝑚
𝑋
𝐹.
𝐹̅ ̅̅ ̅ሺ𝑥ሻൌ1 െ𝐹ሺ𝑥ሻ,
𝐸ሾ𝑋௡ሿൌ඲
଴
ஶ
𝑥௡െଵ𝐹̅ ̅̅ ̅ሺ𝑥ሻ𝑑𝑥
𝑋௡ൌ
𝑛඲
଴
௑
𝑥௡െଵ𝑑𝑥
ൌ
𝑛඲
଴
ஶ
𝑥௡െଵ𝐼௑ሺ𝑥ሻ𝑑𝑥
𝐼௫ሺ𝑥ሻൌቊ1,
if 𝑥൏𝑋
0,
otherwise
𝑎ଵ, …, 𝑎௡,
0,
∑௜ൌଵ
௡
𝑎௜ൌ0.
𝑖ଵ, …, 𝑖௡
∑௝ൌଵ
௡
𝑎௜ೕ𝑎௜ೕ൅భ൏0.
𝑛ൌ3, 𝑎ଵൌ𝑎ଶൌെ1,
𝑎ଷൌ2,
𝑋௜, 𝑖ൌ1, 2, 3,
𝜆௜, 𝑖ൌ1, 2, 3.
𝑋ൌ𝑋ଵ൅𝑋ଶ
𝑌ൌ𝑋ଶ൅𝑋ଷ.
𝑋, 𝑌
𝐸ሾ𝑋ሿ
𝐸ሾ𝑌ሿ.
Covሺ𝑋, 𝑌ሻ.
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ.
ሺ𝑋௜, 𝑌௜ሻ, 𝑖ൌ1, …,
𝑋ଵ, 𝑌ଵ
𝑋ଶ, 𝑌ଶ,
𝑋௜
𝑌௜
614 of 848

 and 
 are independent when 
 Let
Find 
7.24. Three cards are randomly chosen without replacement from an ordinary
deck of 
 cards. Let  denote the number of aces chosen.
a. Find 
b. Find 
7.25. Let  be the standard normal distribution function, and let  be a normal
random variable with mean  and variance  We want to find 
 To do
so, let  be a standard normal random variable that is independent of 
 and
let
a. Show that 
b. Show that 
c. Show that 
Hint: What is the distribution of 
The preceding comes up in statistics. Suppose you are about to observe the
value of a random variable  that is normally distributed with an unknown
mean  and variance  and suppose that you want to test the hypothesis that
the mean  is greater than or equal to  Clearly you would want to reject this
hypothesis if  is sufficiently small. If it results that 
 then the -value of
the hypothesis that the mean is greater than or equal to  is defined to be the
probability that  would be as small as  if  were equal to  (its smallest
possible value if the hypothesis were true). (A small -value is taken as an
indication that the hypothesis is probably false.) Because  has a standard
normal distribution when 
 the -value that results when 
 is 
Therefore, the preceding shows that the expected -value that results when
the true mean is  is 
7.26. A coin that comes up heads with probability  is flipped until either a
total of  heads or of 
 tails is amassed. Find the expected number of flips.
Hint: Imagine that one continues to flip even after the goal is attained. Let 
denote the number of flips needed to obtain  heads, and let  denote the
number of flips needed to obtain 
 tails. Note that
𝑋௜
𝑌௝
𝑖്𝑗.
𝜇௫ൌ𝐸ሾ𝑋௜ሿ,  𝜇௬ൌ𝐸ሾ𝑌௜ሿ,  𝜎௫
ଶൌ Varሺ𝑋௜ሻ,
𝜎௬
ଶൌ Varሺ𝑌௜ሻ,  𝜌ൌCorr ሺ𝑋௜, 𝑌௜ሻ
Corr ሺ∑௜ൌଵ
௡
𝑋௜, ∑௝ൌଵ
௡
𝑌௝ሻ.
52
𝑋
𝐸ሾ𝑋|the ace of spades is chosenሿ.
𝐸ሾ𝑋|at least one ace is chosenሿ.
𝛷
𝑋
𝜇
1.
𝐸ሾ𝛷ሺ𝑋ሻሿ.
𝑍
𝑋,
𝐼ൌቊ1,
if  Z൏X
0,
if Z൒X
𝐸ሾ𝐼||𝑋ൌ𝑥ሿൌ𝛷ሺ𝑥ሻ.
𝐸ሾ𝛷ሺ𝑋ሻሿൌ𝑃ሼ𝑍൏𝑋ሽ.
𝐸ሾ𝛷ሺ𝑋ሻሿൌ𝛷൬𝜇
2
√൰.
𝑋െ𝑍?
𝑋
𝜇
1,
𝜇
0.
𝑋
𝑋ൌ𝑥,
𝑝
0
𝑋
𝑥
𝜇
0
𝑝
𝑋
𝜇ൌ0,
𝑝
𝑋ൌ𝑥
𝛷ሺ𝑥ሻ.
𝑝
𝜇
𝛷൬𝜇
2
√൰.
𝑝
𝑛
𝑚
𝑋
𝑛
𝑌
𝑚
615 of 848

Compute 
 by conditioning on the
number of heads in the first 
 flips.
7.27. A deck of  cards numbered 1 through  initially in any arbitrary order, is
shuffled in the following manner: At each stage, we randomly choose one of
the cards and move it to the front of the deck, leaving the relative positions of
the other cards unchanged. This procedure is continued until all but one of the
cards has been chosen. At this point, it follows by symmetry that all 
possible orderings are equally likely. Find the expected number of stages that
are required.
7.28. Suppose that a sequence of independent trials in which each trial is a
success with probability  is performed until either a success occurs or a total
of  trials has been reached. Find the mean number of trials that are
performed.
Hint: The computations are simplified if you use the identity that for a
nonnegative integer valued random variable 
7.29. Suppose that  and  are both Bernoulli random variables. Show that 
and  are independent if and only if 
7.30. In the generalized match problem, there are  individuals of whom 
wear hat size 
 There are also  hats, of which 
 are of size
If each individual randomly chooses a hat (without
replacement), find the expected number who choose a hat that is their size.
7.31. For random variables  and 
 show that
That is, show that the standard deviation of a sum is always less than or equal
to the sum of the standard deviations.
7.32. Let 
 be a random permutation of 
 (That is,
 is equally likely to be any of the 
 permutations of
 For a given 
 let  be the the 
 smallest of the values
 Show that 
Hint: Note that if we let 
 equal  if 
 and let it equal  otherwise,
that
7.33. Suppose that  is uniformly distributed over 
 and that the
 maxሺ𝑋, 𝑌ሻ൅minሺ𝑋, 𝑌ሻൌ𝑋൅𝑌. 
𝐸ሾmaxሺ𝑋, 𝑌ሻሿ
𝑛൅𝑚െ1
𝑛
𝑛,
𝑛!
𝑝
𝑛
𝑋,
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
ஶ
𝑃ሼ𝑋൒𝑖ሽ
𝑋
𝑌
𝑋
𝑌
Covሺ𝑋, 𝑌ሻൌ0 .
𝑛
𝑛௜
𝑖, ∑௜ൌଵ
௥
𝑛௜ൌ𝑛.
𝑛
ℎ௜
𝑖, ∑௜ൌଵ
௥
ℎ௜ൌ𝑛.
𝑋
𝑌,
Varሺ𝑋൅𝑌ሻ
ට
൑
Varሺ𝑋ሻ
ට
൅
Varሺ𝑌ሻ
ට
𝑅ଵ, …, 𝑅௡൅௠
1, …, 𝑛൅𝑚.
𝑅ଵ, …, 𝑅௡൅௠
ሺ𝑛൅𝑚ሻ!
1, …, 𝑛൅𝑚.ሻ
𝑖൑𝑛,
𝑋
𝑖௧௛
𝑅ଵ, …, 𝑅௡.
𝐸ሾ𝑋ሿൌ𝑖൅𝑚
𝑖
𝑛൅1 .
𝐼௡൅௞
1
𝑅௡൅௞൏𝑋
0
𝑋ൌ𝑖൅
෍
௞ൌଵ
௠
𝐼௡൅௞
𝑌
ሺ0, 1ሻ,
616 of 848

8.1 Introduction
8.2 Chebyshev’s Inequality and the Weak Law of Large Numbers
8.3 The Central Limit Theorem
8.4 The Strong Law of Large Numbers
8.5 Other Inequalities and a Poisson Limit Result
8.6 Bounding the Error Probability When Approximating a Sum of Independent
Bernoulli Random Variables by a Poisson Random Variable
8.7 The Lorenz Curve
The most important theoretical results in probability theory are limit theorems. Of
these, the most important are those classified either under the heading laws of large
numbers or under the heading central limit theorems. Usually, theorems are
considered to be laws of large numbers if they are concerned with stating conditions
under which the average of a sequence of random variables converges (in some
sense) to the expected average. By contrast, central limit theorems are concerned
with determining conditions under which the sum of a large number of random
variables has a probability distribution that is approximately normal.
conditional distribution of 
 given that 
 is uniform over 
a. Find 
b. Find 
c. Find 
d. Find 
e. Find the probability density function of 
𝑋,
𝑌ൌ𝑦,
ሺ0, 𝑦ሻ.
𝐸ሾ𝑋ሿ.
Covሺ𝑋, 𝑌ሻ.
Varሺ𝑋ሻ.
𝑃ሼ𝑋൑𝑥ሽ.
𝑋.
617 of 848

We start this section by proving a result known as Markov’s inequality.
Proposition 2.1 Markov’s inequality
If  is a random variable that takes only nonnegative values, then for any value
Proof For 
 let
and note that, since 
Taking expectations of the preceding inequality yields
which, because 
 proves the result.
As a corollary, we obtain Proposition 2.2.
Proposition 2.2 Chebyshev’s inequality
If  is a random variable with finite mean  and variance 
 then for any value
Proof Since 
 is a nonnegative random variable, we can apply Markov’s
inequality (with 
) to obtain
𝑋
𝑎൐0,
𝑃ሼ𝑋൒𝑎ሽ൑𝐸ሾ𝑋ሿ
𝑎
𝑎൐0,
𝐼ൌቊ1
if 𝑋൒𝑎
0
otherwise
𝑋൒0,
𝐼൑𝑋
𝑎
𝐸ሾ𝐼ሿ൑𝐸ሾ𝑋ሿ
𝑎
𝐸ሾ𝐼ሿൌ𝑃ሼ𝑋൒𝑎ሽ,
𝑋
𝜇
𝜎ଶ,
𝑘൐0,
𝑃ሼ|𝑋െ𝜇| ൒𝑘ሽ൑𝜎ଶ
𝑘ଶ
ሺ𝑋െ𝜇ሻଶ
𝑎ൌ𝑘ଶ
𝑃ቄሺ𝑋െ𝜇ሻଶ൒𝑘ଶቅ൑𝐸ሾሺ𝑋െ𝜇ሻଶሿ
𝑘ଶ
(2.1)
618 of 848

But since 
 if and only if 
Equation (2.1)
 is equivalent
to
and the proof is complete.
The importance of Markov’s and Chebyshev’s inequalities is that they enable us to
derive bounds on probabilities when only the mean or both the mean and the
variance of the probability distribution are known. Of course, if the actual distribution
were known, then the desired probabilities could be computed exactly and we would
not need to resort to bounds.
Example 2a
Suppose that it is known that the number of items produced in a factory during a
week is a random variable with mean 50.
a. What can be said about the probability that this week’s production will
exceed 75?
b. If the variance of a week’s production is known to equal 25, then what can
be said about the probability that this week’s production will be between
40 and 60?
Solution
Let  be the number of items that will be produced in a week.
a. By Markov’s inequality,
b. By Chebyshev’s inequality,
Hence,
so the probability that this week’s production will be between 40 and 60 is at least
.75.
ሺ𝑋െ𝜇ሻଶ൒𝑘ଶ
|𝑋െ𝜇| ൒𝑘,
𝑃ሼ|𝑋െ𝜇| ൒𝑘ሽ൑𝐸ሾሺ𝑋െ𝜇ሻଶሿ
𝑘ଶ
ൌ𝜎ଶ
𝑘ଶ
𝑋
𝑃ሼ𝑋൐75ሽ൑𝐸ሾ𝑋ሿ
75 ൌ50
75 ൌ2
3
𝑃ሼ|𝑋െ50| ൒10ሽ൑𝜎ଶ
10ଶൌ1
4
𝑃ሼ|𝑋െ50| ൏10ሽ൒1 െ1
4 ൌ3
4
619 of 848

As Chebyshev’s inequality is valid for all distributions of the random variable 
 we
cannot expect the bound on the probability to be very close to the actual probability
in most cases. For instance, consider Example 2b
.
Example 2b
If  is uniformly distributed over the interval (0, 10), then, since 
 and
 it follows from Chebyshev’s inequality that
whereas the exact result is
Thus, although Chebyshev’s inequality is correct, the upper bound that it
provides is not particularly close to the actual probability.
Similarly, if  is a normal random variable with mean  and variance 
Chebyshev’s inequality states that
whereas the actual probability is given by
Chebyshev’s inequality is often used as a theoretical tool in proving results. This use
is illustrated first by Proposition 2.3
 and then, most importantly, by the weak law
of large numbers.
Proposition 2.3
If 
 then
In other words, the only random variables having variances equal to 0 are those
that are constant with probability 1.
Proof By Chebyshev’s inequality, we have, for any 
𝑋,
𝑋
𝐸ሾ𝑋ሿൌ5
Var ሺ𝑋ሻൌ25
3 ,
𝑃ሼ|𝑋െ5| ൐4ሽ൑
25
3ሺ16ሻൎ.52
𝑃ሼ||𝑋െ5|| ൐4ሽൌ.20
𝑋
𝜇
𝜎ଶ,
𝑃ሼ|𝑋െ𝜇| ൐2𝜎ሽ൑1
4
𝑃ሼ|𝑋െ𝜇| ൐2𝜎ሽൌ𝑃ቊ|
||
|
𝑋െ𝜇
𝜎
|
||
|
൐2ቋൌ2ሾ1 െΦሺ2ሻሿൎ.0456
Varሺ𝑋ሻൌ0,
𝑃ሼ𝑋ൌ𝐸ሾ𝑋ሿሽൌ1
𝑛൒1,
620 of 848

Letting 
 and using the continuity property of probability yields
and the result is established.
Theorem 2.1 The weak law of large numbers
Let 
 be a sequence of independent and identically distributed random
variables, each having finite mean 
 Then, for any 
Proof We shall prove the theorem only under the additional assumption that the
random variables have a finite variance 
 Now, since
it follows from Chebyshev’s inequality that
and the result is proven.
The weak law of large numbers was originally proven by James Bernoulli for the
special case where the 
 are 0, 1 (that is, Bernoulli) random variables. His
statement and proof of this theorem were presented in his book Ars Conjectandi,
which was published in 1713, eight years after his death, by his nephew Nicholas
Bernoulli. Note that because Chebyshev’s inequality was not known in Bernoulli’s
time, Bernoulli had to resort to a quite ingenious proof to establish the result. The
general form of the weak law of large numbers presented in Theorem 2.1
 was
proved by the Russian mathematician Khintchine.
𝑃ቊ|𝑋െ𝜇| ൐1
𝑛ቋൌ0
𝑛→∞
0 ൌ
lim 
௡→ஶ 𝑃ቊ|𝑋െ𝜇| ൐1
𝑛ቋ
ൌ
𝑃ቊlim
௡→ஶቊ|𝑋െ𝜇| ൐1
𝑛ቋቋ
ൌ
𝑃ሼ𝑋്𝜇ሽ
𝑋ଵ,𝑋ଶ, . . .
𝐸ሾ𝑋௜ሿൌ𝜇.
𝜀൐0,
𝑃ቄቚ
௑భ൅⋯൅௑೙
௡
െ𝜇ቚ൒𝜀ቅ→0  as  𝑛→∞
𝜎ଶ.
𝐸⎡
⎣
𝑋ଵ൅ ⋯  ൅𝑋௡
𝑛
⎤
⎦
ൌ𝜇 and     Var ቆ𝑋ଵ൅ ⋯  ൅𝑋௡
𝑛
ቇൌ𝜎ଶ
𝑛
𝑃ቊ|
||
|
𝑋ଵ൅⋯൅𝑋௡
𝑛
െ𝜇|
||
|
൒𝜀ቋ൑𝜎ଶ
𝑛𝜀ଶ
𝑋௜
621 of 848

The central limit theorem is one of the most remarkable results in probability theory.
Loosely put, it states that the sum of a large number of independent random
variables has a distribution that is approximately normal. Hence, it not only provides
a simple method for computing approximate probabilities for sums of independent
random variables, but also helps explain the remarkable fact that the empirical
frequencies of so many natural populations exhibit bell-shaped (that is, normal)
curves.
In its simplest form, the central limit theorem is as follows.
Theorem 3.1 The central limit theorem
Let 
 be a sequence of independent and identically distributed random
variables, each having mean  and variance 
 Then the distribution of
tends to the standard normal as 
 That is, for 
The key to the proof of the central limit theorem is the following lemma, which we
state without proof.
Lemma 3.1
Let 
 be a sequence of random variables having distribution functions
 and moment generating functions 
 and let  be a random variable
having distribution function 
 and moment generating function 
 If
 for all  then 
 for all  at which 
 is
continuous.
If we let  be a standard normal random variable, then, since 
 it
follows from Lemma 3.1
 that if 
 as 
 then 
as 
We are now ready to prove the central limit theorem.
Proof of the Central Limit Theorem: Let us assume at first that 
 and
 We shall prove the theorem under the assumption that the moment
generating function of the 
 exists and is finite. Now, the moment
generating function of 
 is given by
𝑋ଵ,𝑋ଶ, . . .
𝜇
𝜎ଶ.
𝑋ଵ൅⋯൅𝑋௡െ𝑛𝜇
𝜎𝑛
√
𝑛→∞.
െ∞൏𝑎൏∞,
𝑃ቊ𝑋ଵ൅⋯൅𝑋௡െ𝑛𝜇
𝜎𝑛
√
൑𝑎ቋ→
1
2𝜋
√
඲
െஶ
௔
𝑒െ௫మ/ଶ𝑑𝑥 as 𝑛→∞
𝑍ଵ,𝑍ଶ, . . .
𝐹௓೙
𝑀௓೙, 𝑛൒1,
𝑍
𝐹௓
𝑀௓.
𝑀௓೙ሺ𝑡ሻ→𝑀௓ሺ𝑡ሻ
𝑡,
𝐹௓೙ሺ𝑡ሻ→𝐹௓ሺ𝑡ሻ
𝑡
𝐹௓ሺ𝑡ሻ
𝑍
𝑀௓ሺ𝑡ሻൌ𝑒௧మ/ଶ,
𝑀௓೙ሺ𝑡ሻ→𝑒௧మ/ଶ
𝑛→∞,
𝐹௓೙ሺ𝑡ሻ→Φ ሺ𝑡ሻ
𝑛→∞.
𝜇ൌ0
𝜎ଶൌ1.
𝑋௜, 𝑀ሺ𝑡ሻ,
𝑋௜/ 𝑛
√
622 of 848

Thus, the moment generating function of 
 is given by 
 Let
and note that
Now, to prove the theorem, we must show that 
 as 
 or,
equivalently, that 
 as 
 To show this, note that
Thus, the central limit theorem is proven when 
 and 
 The result now
follows in the general case by considering the standardized random variables
 and applying the preceding result, since 
Remark Although Theorem 3.1
 states only that, for each 
𝐸ቈexpቊ𝑡𝑋௜
𝑛
√ቋ቉ൌ𝑀൬𝑡
𝑛
√൰
෍
௜ൌଵ
௡
𝑋௜/ 𝑛
√
൤𝑀൬𝑡
𝑛
√൰൨
௡
.
𝐿ሺ𝑡ሻൌlog 𝑀ሺ𝑡ሻ
𝐿ሺ0ሻൌ
0
𝐿ᇱሺ0ሻൌ
𝑀ᇱሺ0ሻ
𝑀ሺ0ሻ
ൌ
𝜇
ൌ
0
𝐿ᇳሺ0ሻൌ
𝑀ሺ0ሻ𝑀ᇳሺ0ሻെሾ𝑀ᇱሺ0ሻሿ
ଶ
ሾ𝑀ሺ0ሻሿଶ
ൌ
𝐸ሾ𝑋ଶሿ
ൌ
1
ሾ𝑀ሺ𝑡/ 𝑛
√ሻሿ
௡→𝑒௧మ/ଶ
𝑛→∞,
𝑛𝐿ሺ𝑡/ 𝑛
√ሻ→𝑡ଶ/2
𝑛→∞.
lim
௡→ஶ
𝐿൫𝑡/ 𝑛
√൯
𝑛െଵ
ൌ
lim
௡→ஶ
െ𝐿ᇱ൫𝑡/ 𝑛
√൯𝑛െଷ/ଶ೟
െ2𝑛െଶ
  by L’Hôpital’s rule
ൌ
lim
௡→ஶቈ𝐿ᇱ൫𝑡/ 𝑛
√൯𝑡
2𝑛െଵ/ଶ቉
ൌ
lim
௡→ஶቈെ𝐿ᇳ൫𝑡/ 𝑛
√൯𝑛െଷ/ଶ𝑡ଶ
െ2𝑛െଷ/ଶ
቉  again by L’Hôpital’s rule
ൌ
lim
௡→ஶቈ𝐿ᇳ൬𝑡
𝑛
√൰𝑡ଶ
2 ቉
ൌ
𝑡ଶ
2
𝜇ൌ0
𝜎ଶൌ1.
𝑋௜* ൌሺ𝑋௜െ𝜇ሻ/𝜎
𝐸ሾ𝑋௜*ሿൌ0,  Varሺ𝑋௜*ሻൌ1.
𝑎,
623 of 848

it can, in fact, be shown that the convergence is uniform in  [We say that
 uniformly in  if, for each 
 there exists an  such that
 for all  whenever 
]
The first version of the central limit theorem was proven by DeMoivre around 1733
for the special case where the 
 are Bernoulli random variables with 
 The
theorem was subsequently extended by Laplace to the case of arbitrary  (Since a
binomial random variable may be regarded as the sum of  independent and
identically distributed Bernoulli random variables, this justifies the normal
approximation to the binomial that was presented in Section 5.4.1.
) Laplace also
discovered the more general form of the central limit theorem given in Theorem
3.1
. His proof, however, was not completely rigorous and, in fact, cannot easily be
made rigorous. A truly rigorous proof of the central limit theorem was first presented
by the Russian mathematician Liapounoff in the period 1901–1902.
Figure 8.1
 illustrates the central limit theorem by plotting the probability mass
functions of  independent random variables having a specified mass function when
(a) 
 (b) 
 (c) 
 and (d) 
Figure 8.1(a)
𝑃ቊ𝑋ଵ൅⋯൅𝑋௡െ𝑛𝜇
𝜎𝑛
√
൑𝑎ቋ→Φሺ𝑎ሻ
𝑎.
𝑓௡ሺ𝑎ሻ→𝑓ሺ𝑎ሻ
𝑎
𝜀൐0,
𝑁
ห𝑓௡ሺ𝑎ሻെ𝑓ሺ𝑎ሻห൏𝜀
𝑎
𝑛൒𝑁.
𝑋௜
𝑝ൌ1
2 .
𝑝.
𝑛
𝑛
𝑛ൌ5,
𝑛ൌ10,
𝑛ൌ25,
𝑛ൌ100.
624 of 848

Figure 8.1(b)
625 of 848

Figure 8.1(c)
626 of 848

Figure 8.1(d)
627 of 848

Example 3a
An astronomer is interested in measuring the distance, in light-years, from his
observatory to a distant star. Although the astronomer has a measuring
technique, he knows that because of changing atmospheric conditions and
normal error, each time a measurement is made, it will not yield the exact
distance, but merely an estimate. As a result, the astronomer plans to make a
series of measurements and then use the average value of these measurements
as his estimated value of the actual distance. If the astronomer believes that the
values of the measurements are independent and identically distributed random
variables having a common mean  (the actual distance) and a common
variance of 4 (light-years), how many measurements need he make to be
reasonably sure that his estimated distance is accurate to within 
 light–year?
Solution
Suppose that the astronomer decides to make  observations. If 
are the  measurements, then, from the central limit theorem, it follows that
𝑑
േ.5
𝑛
𝑋ଵ, 𝑋ଶ, . . . ,𝑋௡
𝑛
628 of 848

has approximately a standard normal distribution. Hence,
Therefore, if the astronomer wants, for instance, to be 95 percent certain that his
estimated value is accurate to within .5 light-year, he should make 
measurements, where 
 is such that
Thus, from Table 5.1
 of Chapter 5
,
As 
 is not integral valued, he should make 62 observations.
Note, however, that the preceding analysis has been done under the assumption
that the normal approximation will be a good approximation when 
Although this will usually be the case, in general the question of how large 
need be before the approximation is “good” depends on the distribution of the 
If the astronomer is concerned about this point and wants to take no chances, he
can still solve his problem by using Chebyshev’s inequality. Since
Chebyshev’s inequality yields
𝑍௡ൌ
෍
௜ൌଵ
௡
𝑋௜െ𝑛𝑑
2 𝑛
√
𝑃
⎧
⎨
⎩
⎪
⎪
⎪
⎪
െ.5 ൑
෍
௜ൌଵ
௡
𝑋௜
𝑛
െ𝑑൑.5
⎫
⎬
⎭
⎪
⎪
⎪
⎪
ൌ𝑃ቊെ.5
𝑛
√
2 ൑𝑍௡൑.5
𝑛
√
2 ቋ
ൎ
Φቆ𝑛
√
4 ቇെϕ ቆെ𝑛
√
4 ቇൌ2Φቆ𝑛
√
4 ቇെ1
𝑛*
𝑛*
2Φቆ𝑛*
√
4 ቇെ1 ൌ.95
or
Φቆ𝑛*
√
4 ቇൌ.975
𝑛*
√
4
ൌ1.96
or 𝑛* ൌሺ7.84ሻଶൎ61.47
𝑛*
𝑛ൌ62.
𝑛
𝑋௜.
𝐸቎෍
௜ൌଵ
௡
𝑋௜
𝑛቏ൌ𝑑  Varቌ෍
௜ൌଵ
௡
𝑋௜
𝑛ቍൌ4
𝑛
629 of 848

Hence, if he makes 
 observations, he can be 95 percent certain
that his estimate will be accurate to within .5 light-year.
Example 3b
The number of students who enroll in a psychology course is a Poisson random
variable with mean 100. The professor in charge of the course has decided that if
the number enrolling is 120 or more, he will teach the course in two separate
sections, whereas if fewer than 120 students enroll, he will teach all of the
students together in a single section. What is the probability that the professor
will have to teach two sections?
Solution
The exact solution
does not readily yield a numerical answer. However, by recalling that a Poisson
random variable with mean 100 is the sum of 100 independent Poisson random
variables, each with mean 1, we can make use of the central limit theorem to
obtain an approximate solution. If  denotes the number of students who enroll in
the course, we have
where we have used the fact that the variance of a Poisson random variable is
equal to its mean.
Example 3c
If 10 fair dice are rolled, find the approximate probability that the sum obtained is
between 30 and 40, inclusive.
Solution
𝑃ቐቮ෍
௜ൌଵ
௡
𝑋௜
𝑛െ𝑑ቮ൐.5ቑ൑
4
𝑛ሺ.5ሻଶൌ16
𝑛
𝑛ൌ16/.05 ൌ320
𝑒െଵ଴଴
෍
௜ൌଵଶ଴
ஶ
ሺ100ሻ௜
𝑖!
𝑋
𝑃ሼ𝑋൒120ሽ
ൌ𝑃ሼ𝑋൒119.5ሽ  ሺthe continuity correctionሻ
ൌ𝑃ቊ𝑋െ100
100
√
൒119.5 െ100
100
√
ቋ
ൎ
1 െΦሺ1.95ሻ
ൎ
.0256
630 of 848

Let 
 denote the value of the th die, 
 Since
the central limit theorem yields
Example 3d
Let 
 be independent random variables, each uniformly distributed
over (0, 1). Calculate an approximation to 
Solution
Since 
 and 
 we have, by the central limit theorem,
Hence, 
 will be greater than 6 only 14 percent of the time.
Example 3e
An instructor has 50 exams that will be graded in sequence. The times required
to grade the 50 exams are independent, with a common distribution that has
mean 20 minutes and standard deviation 4 minutes. Approximate the probability
that the instructor will grade at least 25 of the exams in the first 450 minutes of
𝑋௜
𝑖
𝑖ൌ1,2, . . . ,10.
𝐸ሺ𝑋௜ሻൌ7
2,  Varሺ𝑋௜ሻൌ𝐸ሾ𝑋௜
ଶሿെሺ𝐸ሾ𝑋௜ሿሻଶൌ35
12,
𝑃ሼ29.5 ൑𝑋൑40.5ሽൌ
𝑃
⎧
⎨
⎩
⎪
⎪
⎪
⎪
29.5 െ35
350
12
ඨ
൑𝑋െ35
350
12
ඨ
൑40.5 െ35
350
12
ඨ
⎫
⎬
⎭
⎪
⎪
⎪
⎪
ൎ
2Φሺ1.0184ሻെ1
ൎ
.692
𝑋௜,𝑖ൌ1, . . . ,10,
𝑃ቐ෍
௜ൌଵ
ଵ଴
𝑋௜൐6ቑ.
𝐸ሾ𝑋௜ሿൌ1
2
Varሺ𝑋௜ሻൌ1
12 ,
𝑃ቐ෍
ଵ
ଵ଴
𝑋௜൐6ቑൌ
𝑃
⎧
⎨
⎩
⎪
⎪
⎪
⎪
෍
ଵ
ଵ଴
𝑋௜െ5
10ቆ1
12ቇ
ඨ
൐
6 െ5
10ቆ1
12ቇ
ඨ
⎫
⎬
⎭
⎪
⎪
⎪
⎪
ൎ
1 െΦ൫1.2
√
൯
ൎ
.1367
෍
௜ൌଵ
ଵ଴
𝑋௜
631 of 848

work.
Solution
If we let 
 be the time that it takes to grade exam  then
is the time it takes to grade the first 25 exams. Because the instructor will grade
at least 25 exams in the first 450 minutes of work if the time it takes to grade the
first 25 exams is less than or equal to 450, we see that the desired probability is
 To approximate this probability, we use the central limit theorem.
Now,
and
Consequently, with  being a standard normal random variable, we have
Central limit theorems also exist when the 
 are independent, but not
necessarily identically distributed random variables. One version, by no means
the most general, is as follows.
Theorem 3.2 Central limit theorem for independent random variables
Let 
 be a sequence of independent random variables having respective
means and variances 
 If (a) the 
 are uniformly
bounded—that is, if for some 
 for all  and (b) 
𝑋௜
𝑖,
𝑋ൌ෍
௜ൌଵ
ଶହ
𝑋௜
 𝑃ሼ𝑋൑450ሽ.
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
ଶହ
𝐸ሾ𝑋௜ሿൌ25ሺ20ሻൌ500
Varሺ𝑋ሻൌ෍
௜ൌଵ
ଶହ
 Varሺ𝑋௜ሻൌ25ሺ16ሻൌ400
𝑍
𝑃ሼ𝑋൑450ሽ
ൌ𝑃ቊ𝑋െ500
400
√
൑450 െ500
400
√
ቋ
ൎ
𝑃ሼ𝑍൑െ2.5ሽ
ൌ𝑃ሼ𝑍൒2.5ሽ
ൌ1 െΦሺ2.5ሻൎ.006
𝑋௜
𝑋ଵ,𝑋ଶ, . . .
𝜇௜ൌ𝐸ሾ𝑋௜ሿ,𝜎௜
ଶൌVarሺ𝑋௜ሻ.
𝑋௜
𝑀,   𝑃ሼ|𝑋௜| ൏𝑀ሽൌ1
𝑖,
෍
௜ൌଵ
ஶ
𝜎௜
ଶൌ∞
632 of 848

—then
Historical note: Pierre-Simon, Marquis de Laplace (1749–1827)
The central limit theorem was originally stated and proven by the French
mathematician Pierre-Simon, Marquis de Laplace, who came to the theorem
from his observations that errors of measurement (which can usually be
regarded as being the sum of a large number of tiny forces) tend to be normally
distributed. Laplace, who was also a famous astronomer (and indeed was
called “the Newton of France”), was one of the great early contributors to both
probability and statistics. Laplace was also a popularizer of the uses of
probability in everyday life. He strongly believed in its importance, as is
indicated by the following quotations taken from his published book Analytical
Theory of Probability: “We see that the theory of probability is at bottom only
common sense reduced to calculation; it makes us appreciate with exactitude
what reasonable minds feel by a sort of instinct, often without being able to
account for it.
 It is remarkable that this science, which originated in the
consideration of games of chance, should become the most important object of
human knowledge.
 The most important questions of life are, for the most
part, really only problems of probability.”
The application of the central limit theorem to show that measurement errors
are approximately normally distributed is regarded as an important contribution
to science. Indeed, in the seventeenth and eighteenth centuries, the central
limit theorem was often called the law of frequency of errors. Listen to the
words of Francis Galton (taken from his book Natural Inheritance, published in
1889): “I know of scarcely anything so apt to impress the imagination as the
wonderful form of cosmic order expressed by the ‘Law of Frequency of Error.’
The Law would have been personified by the Greeks and deified, if they had
known of it. It reigns with serenity and in complete self-effacement amidst the
wildest confusion. The huger the mob and the greater the apparent anarchy, the
more perfect is its sway. It is the supreme law of unreason.”
𝑃
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇௜ሻ
෍
௜ൌଵ
௡
𝜎௜
ଶ
ඩ
൑𝑎
⎫
⎬
⎭
⎪
⎪
⎪
⎪
⎪
⎪
→Φሺ𝑎ሻ as 𝑛→∞
. . .
. . .
633 of 848

The strong law of large numbers is probably the best-known result in probability
theory. It states that the average of a sequence of independent random variables
having a common distribution will, with probability 1, converge to the mean of that
distribution.
Theorem 4.1 The strong law of large numbers
Let 
 be a sequence of independent and identically distributed random
variables, each having a finite mean 
 Then, with probability 1,
That is, the strong law of large numbers states that
As an application of the strong law of large numbers, suppose that a sequence of
independent trials of some experiment is performed. Let  be a fixed event of the
experiment, and denote by 
 the probability that  occurs on any particular
trial. Letting
we have, by the strong law of large numbers, that with probability 1,
Since 
 represents the number of times that the event  occurs in the
first  trials, we may interpret Equation (4.1)
 as stating that with probability 1,
the limiting proportion of time that the event  occurs is just 
Although the theorem can be proven without this assumption, our proof of the
strong law of large numbers will assume that the random variables 
 have a
finite fourth moment. That is, we will suppose that 
Proof of the Strong Law of Large Numbers: To begin, assume that  the
𝑋ଵ,𝑋ଶ, . . .
𝜇ൌ𝐸ሾ𝑋௜ሿ.
𝑋ଵ൅𝑋ଶ൅⋯൅𝑋௡
𝑛
→𝜇as
𝑛→∞
†
†
𝑝ቄlim
௡→ஶሺ𝑋ଵ൅... ൅𝑋௡ሻ/𝑛ൌ𝜇ቅൌ1
𝐸
𝑃ሺ𝐸ሻ
𝐸
𝑋௜ൌቊ1
if 𝐸 occurs on the 𝑖th trial
0
if 𝐸 does not occur on the 𝑖th trial
𝑋ଵ൅⋯൅𝑋௡
𝑛
→𝐸ሾ𝑋ሿൌ𝑃ሺ𝐸ሻ
(4.1)
𝑋ଵ൅⋯൅𝑋௡
𝐸
𝑛
𝐸
𝑃ሺ𝐸ሻ.
𝑋௜
𝐸ሾ𝑋௜
ସሿൌ𝐾൏∞.
𝜇,
634 of 848

mean of the 
 is equal to 0. Let 
 and consider
Expanding the right side of the preceding equation results in terms of the form
where 
 and  are all different. Because all the 
 have mean 0, it follows by
independence that
Now, for a given pair  and  there will be 
 terms in the expansion that will
equal 
 Hence, upon expanding the preceding product and taking
expectations term by term, it follows that
where we have once again made use of the independence assumption. Now,
since
we have
Therefore, from the preceding, we obtain
which implies that
𝑋௜,
𝑆௡ൌ෍
௜ൌଵ
௡
𝑋௜
𝐸ሾ𝑆௡
ସሿ
ൌ
𝐸ሾሺ𝑋ଵ൅⋯൅𝑋௡ሻሺ𝑋ଵ൅⋯൅𝑋௡ሻ
ൈሺ𝑋ଵ൅⋯൅𝑋௡ሻሺ𝑋ଵ൅⋯൅𝑋௡ሻሿ
𝑋௜
ସ,  𝑋௜
ଷ𝑋௝,  𝑋௜
ଶ𝑋௝
ଶ,  𝑋௜
ଶ𝑋௝𝑋௞,
and
𝑋௜𝑋௝𝑋௞𝑋௟
𝑖, 𝑗, 𝑘,
𝑙
𝑋௜
𝐸ሾ𝑋௜
ଷ𝑋௝ሿ
ൌ
𝐸ሾ𝑋௜
ଷሿ𝐸ሾ𝑋௝ሿൌ0
𝐸ሾ𝑋௜
ଶ𝑋௝𝑋௞ሿ
ൌ
𝐸ሾ𝑋௜
ଶሿ𝐸ሾ𝑋௝ሿ𝐸ሾ𝑋௞ሿൌ0
𝐸ሾ𝑋௜𝑋௝𝑋௞𝑋௟ሿ
ൌ
0
𝑖
𝑗,
ቆ4
2ቇൌ6
𝑋௜
ଶ𝑋௝
ଶ.
𝐸ሾ𝑆௡
ସሿ
ൌ
𝑛𝐸ሾ𝑋௜
ସሿ൅6ቆ𝑛
2ቇ𝐸ሾ𝑋௜
ଶ𝑋௝
ଶሿ
ൌ
𝑛𝐾൅3𝑛ሺ𝑛െ1ሻ𝐸ሾ𝑋௜
ଶሿ𝐸ሾ𝑋௝
ଶሿ
0 ൑Varሺ𝑋௜
ଶሻൌ𝐸ሾ𝑋௜
ସሿെሺ𝐸ሾ𝑋௜
ଶሿሻ
ଶ
ሺ𝐸ሾ𝑋௜
ଶሿሻ
ଶ൑𝐸ሾ𝑋௜
ସሿൌ𝐾
𝐸ሾ𝑆௡
ସሿ൑𝑛𝐾൅3𝑛ሺ𝑛െ1ሻ𝐾
635 of 848

Therefore,
But the preceding implies that with probability 1, 
 (For if there is
a positive probability that the sum is infinite, then its expected value is infinite.)
But the convergence of a series implies that its th term goes to 0; so we can
conclude that with probability 1,
But if 
 goes to 0, then so must 
 hence, we have proven
that with probability 1,
When  the mean of the 
 is not equal to 0, we can apply the preceding
argument to the random variables 
 to obtain that with probability 1,
or, equivalently,
which proves the result.
Figure 8.2
 illustrates the strong law by giving the results of a simulation of 
independent random variables having a specified probability mass function. The
averages of the  variables are given when (a) 
 (b) 
 and (c)
Figure 8.2(a)
𝐸ቈ𝑆௡
ସ
𝑛ସ቉൑𝐾
𝑛ଷ൅3𝐾
𝑛ଶ
𝐸⎡
⎣
⎢⎢
෍
௡ൌଵ
ஶ
𝑆௡
ସ
𝑛ସ
⎤
⎦
⎥⎥
ൌ
෍
௡ൌଵ
ஶ
𝐸ቈ𝑆௡
ସ
𝑛ସ቉൏∞
෍
௡ൌଵ
ஶ
𝑆௡
ସ/𝑛ସ൏∞.
𝑛
lim
௡→ஶ
𝑆௡
ସ
𝑛ସൌ0
𝑆௡
ସ/𝑛ସൌሺ𝑆௡/𝑛ሻସ
𝑆௡/𝑛;
𝑆௡
𝑛→0
as
𝑛→∞
𝜇,
𝑋௜,
𝑋௜െ𝜇
lim
௡→ஶ෍
௜ൌଵ
௡
ሺ𝑋௜െ𝜇ሻ
𝑛
ൌ0
lim
௡→ஶ෍
௜ൌଵ
௡
𝑋௜
𝑛ൌ𝜇
𝑛
𝑛
𝑛ൌ100,
𝑛ൌ1000,
𝑛ൌ10,000.
636 of 848

Figure 8.2(b)
637 of 848

Figure 8.2(c)
638 of 848

Many students are initially confused about the difference between the weak and the
strong laws of large numbers. The weak law of large numbers states that for any
specified large value 
 is likely to be near  However, it does
not say that 
 is bound to stay near  for all values of  larger than
 Thus, it leaves open the possibility that large values of 
can occur infinitely often (though at infrequent intervals). The strong law shows that
this cannot occur. In particular, it implies that, with probability 1, for any positive value
will be greater than  only a finite number of times.
The strong law of large numbers was originally proven, in the special case of
Bernoulli random variables, by the French mathematician Borel. The general form of
𝑛* , ሺ𝑋ଵ൅⋯൅𝑋௡*ሻ/𝑛*
𝜇.
ሺ𝑋ଵ൅⋯൅𝑋௡ሻ/𝑛
𝜇
𝑛
𝑛* .
||ሺ𝑋ଵ൅⋯൅𝑋௡ሻ/𝑛െ𝜇||
𝜀,
ቮ෍
ଵ
௡𝑋௜
𝑛െ𝜇ቮ
𝜀
639 of 848

the strong law presented in Theorem 4.1
 was proven by the Russian
mathematician A. N. Kolmogorov.
We are sometimes confronted with situations in which we are interested in obtaining
an upper bound for a probability of the form 
 where  is some positive
value and when only the mean 
 and variance 
 of the distribution
of  are known. Of course, since 
 implies that 
 it follows
from Chebyshev’s inequality that
However, as the following proposition shows, it turns out that we can do better.
Proposition 5.1 One-sided Chebyshev inequality
If  is a random variable with mean 0 and finite variance 
 then, for any 
Proof Let 
 and note that
Hence,
where the inequality is obtained by noting that since 
implies that 
 Upon applying Markov’s inequality, the
preceding yields that
Letting 
 [which is easily seen to be the value of  that minimizes
] gives the desired result.
𝑃ሼ𝑋െ𝜇൒𝑎ሽ,
𝑎
𝜇ൌ𝐸ሾ𝑋ሿ
𝜎ଶൌVarሺ𝑋ሻ
𝑋
𝑋െ𝜇൒𝑎൐0
|𝑋െ𝜇| ൒𝑎,
𝑃ሼ𝑋െ𝜇൒𝑎ሽ൑𝑃ሼ|𝑋െ𝜇| ൒𝑎ሽ൑𝜎ଶ
𝑎ଶ
when
𝑎൐0
𝑋
𝜎ଶ,
𝑎൐0,
𝑃ሼ𝑋൒𝑎ሽ൑
𝜎ଶ
𝜎ଶ൅𝑎ଶ
𝑏൐0
𝑋൒𝑎is equivalent to
𝑋൅𝑏൒𝑎൅𝑏
𝑃ሼ𝑋൒𝑎ሽ
ൌ
𝑃ሼ𝑋൅𝑏൒𝑎൅𝑏ሽ
൑
𝑃൛ሺ𝑋൅𝑏ሻଶ൒ሺ𝑎൅𝑏ሻଶൟ
𝑎൅𝑏൐0, 𝑋൅𝑏൒𝑎൅𝑏
ሺ𝑋൅𝑏ሻଶ൒ሺ𝑎൅𝑏ሻଶ.
𝑃ሼ𝑋൒𝑎ሽ൑𝐸ሾሺ𝑋൅𝑏ሻଶሿ
ሺ𝑎൅𝑏ሻଶ
ൌ𝜎ଶ൅𝑏ଶ
ሺ𝑎൅𝑏ሻଶ
𝑏ൌ𝜎ଶ/𝑎
𝑏
ሺ𝜎ଶ൅𝑏ଶሻ/ሺ𝑎൅𝑏ሻଶ
640 of 848

Example 5a
If the number of items produced in a factory during a week is a random variable
with mean 100 and variance 400, compute an upper bound on the probability that
this week’s production will be at least 120.
Solution
It follows from the one-sided Chebyshev inequality that
Hence, the probability that this week’s production will be 120 or more is at most
If we attempted to obtain a bound by applying Markov’s inequality, then we would
have obtained
which is a far weaker bound than the preceding one.
Suppose now that  has mean  and variance 
 Since both 
 and 
 have
mean 0 and variance 
 it follows from the one-sided Chebyshev inequality that, for
and
Thus, we have the following corollary.
Corollary 5.1
If 
 and 
 then, for 
𝑃ሼ𝑋൒120ሽൌ𝑃ሼ𝑋െ100 ൒20ሽ൑
400
400 ൅ሺ20ሻଶൌ1
2
1
2 .
𝑃ሼ𝑋൒120ሽ൑𝐸ሺ𝑋ሻ
120 ൌ5
6
𝑋
𝜇
𝜎ଶ.
𝑋െ𝜇
𝜇െ𝑋
𝜎ଶ,
𝑎൐0,
𝑃ሼ𝑋െ𝜇൒𝑎ሽ൑
𝜎ଶ
𝜎ଶ൅𝑎ଶ
𝑃ሼ𝜇െ𝑋൒𝑎ሽ൑
𝜎ଶ
𝜎ଶ൅𝑎ଶ
𝐸ሾ𝑋ሿൌ𝜇
Varሺ𝑋ሻൌ𝜎ଶ,
𝑎൐0,
𝑃ሼ𝑋൒𝜇൅𝑎ሽ
൑
𝜎ଶ
𝜎ଶ൅𝑎ଶ
𝑃ሼ𝑋൑𝜇െ𝑎ሽ
൑
𝜎ଶ
𝜎ଶ൅𝑎ଶ
641 of 848

Example 5b
A set of 200 people consisting of 100 men and 100 women is randomly divided
into 100 pairs of 2 each. Give an upper bound to the probability that at most 30 of
these pairs will consist of a man and a woman.
Solution
Number the men arbitrarily from 1 to 100, and for 
 let
Then 
 the number of man–woman pairs, can be expressed as
Because man  is equally likely to be paired with any of the other 199 people, of
which 100 are women, we have
Similarly, for 
where 
 since, given that man  is paired with a
woman, man  is equally likely to be paired with any of the remaining 197 people,
of which 99 are women. Hence, we obtain
𝑖ൌ1,2, . . . 100,
𝑋௜ൌቊ1
if man 𝑖 is paired with a woman
0
otherwise
𝑋,
𝑋ൌ෍
௜ൌଵ
ଵ଴଴
𝑋௜
𝑖
𝐸ሾ𝑋௜ሿൌ𝑃ሼ𝑋௜ൌ1ሽൌ100
199
𝑖്𝑗,
𝐸ሾ𝑋௜𝑋௝ሿ
ൌ
𝑃൛𝑋௜ൌ1, 𝑋௝ൌ1ൟ
ൌ
𝑃ሼ𝑋௜ൌ1ሽ𝑃൛𝑋௝ൌ1ห𝑋௜ൌ1ൟൌ100
199
99
197
𝑃൛𝑋௝ൌ1ห𝑋௜ൌ1ൟൌ99/197,
𝑖
𝑗
642 of 848

The Chebyshev inequality then yields
Thus, there are fewer than 6 chances in 100 that fewer than 30 men will be
paired with women. However, we can improve on this bound by using the one-
sided Chebyshev inequality, which yields
When the moment generating function of the random variable  is known, we can
obtain even more effective bounds on 
 Let
be the moment generating function of the random variable 
 Then, for 
Similarly, for 
𝐸ሾ𝑋ሿ
ൌ
෍
௜ൌଵ
ଵ଴଴
𝐸ሾ𝑋௜ሿ
ൌሺ100ሻ100
199
ൎ
50.25
Varሺ𝑋ሻ
ൌ
෍
௜ൌଵ
ଵ଴଴
Varሺ𝑋௜ሻ൅2 ෍
௜ழ௝
෍covሺ𝑋௜, 𝑋௝ሻ
ൌ100 100
199
99
199 ൅2ቆ100
2 ቇ൥100
199
99
197 െቆ100
199ቇ
ଶ
൩
ൎ
25.126
𝑃ሼ𝑋൑30ሽ൑𝑃ሼ|𝑋െ50.25| ൒20.25ሽ൑25.126
ሺ20.25ሻଶൎ.061
𝑃ሼ𝑋൑30ሽ
ൌ
𝑃ሼ𝑋൑50.25 െ20.25ሽ
൑
25.126
25.126 ൅ሺ20.25ሻଶ
ൎ
.058
𝑋
𝑃ሼ𝑋൒𝑎ሽ.
𝑀ሺ𝑡ሻൌ𝐸ሾ𝑒௧௑ሿ
𝑋.
𝑡൐0,
𝑃ሼ𝑋൒𝑎ሽ
ൌ
𝑃ሼ𝑒௧௑൒𝑒௧௔ሽ
൑
𝐸ሾ𝑒௧௑ሿ𝑒െ௧௔
by Markov's inequality
𝑡൏0,
𝑃ሼ𝑋൑𝑎ሽൌ
𝑃ሼ𝑒௧௑൒𝑒௧௔ሽ
൑
𝐸ሾ𝑒௧௑ሿ𝑒െ௧௔
643 of 848

Thus, we have the following inequalities, known as Chernoff bounds.
Proposition 5.2 Chernoff bounds
Since the Chernoff bounds hold for all  in either the positive or negative
quadrant, we obtain the best bound on 
 by using the  that minimizes
Example 5c Chernoff bounds for the standard normal random variable
If  is a standard normal random variable, then its moment generating function is
 so the Chernoff bound on 
 is given by
Now the value of 
 that minimizes 
 is the value that minimizes
 which is 
 Thus, for 
 we have
Similarly, we can show that, for 
Example 5d Chernoff bounds for the Poisson random variable
If  is a Poisson random variable with parameter  then its moment generating
function is 
 Hence, the Chernoff bound on 
 is
Minimizing the right side of the preceding inequality is equivalent to minimizing
 and calculus shows that the minimal value occurs when 
Provided that 
 this minimizing value of  will be positive. Therefore,
assuming that 
 and letting 
 in the Chernoff bound yields
or, equivalently,
𝑃ሼ𝑋൒𝑎ሽ൑𝑒െ௧௔𝑀ሺ𝑡ሻfor all
𝑡൐0
𝑃ሼ𝑋൑𝑎ሽ൑𝑒െ௧௔𝑀ሺ𝑡ሻfor all
𝑡൏0
𝑡
𝑃ሼ𝑋൒𝑎ሽ
𝑡
𝑒െ௧௔𝑀ሺ𝑡ሻ.
𝑍
𝑀ሺ𝑡ሻൌ𝑒௧మ/ଶ,
𝑃ሼ𝑍൒𝑎ሽ
𝑃ሼ𝑍൒𝑎ሽ൑𝑒െ௧௔𝑒௧మ/ଶ
for all
𝑡൐0
𝑡,𝑡൐0,
𝑒௧మ/ଶെ௧௔
𝑡ଶ/2 െ𝑡𝑎,
𝑡ൌ𝑎.
𝑎൐0,
𝑃ቄ𝑍൒𝑎ቅ൑𝑒െ௔మ/ଶ
𝑎൏0,
𝑃ቄ𝑍൑𝑎ቅ൑𝑒െ௔మ/ଶ
𝑋
𝜆,
𝑀ሺ𝑡ሻൌ𝑒ఒሺ௘೟െଵሻ.
𝑃ሼ𝑋൒𝑖ሽ
𝑃ቄ𝑋൒𝑖ቅ൑𝑒ఒሺ௘೟െଵሻ𝑒െ௜௧ 𝑡൐0
𝜆ሺ𝑒௧െ1ሻെ𝑖𝑡,
𝑒௧ൌ𝑖/𝜆.
𝑖/𝜆൐1,
𝑡
𝑖൐𝜆
𝑒௧ൌ𝑖/𝜆
𝑃ሼ𝑋൒𝑖ሽ൑𝑒ఒሺ௜/ఒെଵሻቆ𝜆
𝑖ቇ
௜
644 of 848

Example 5e
Consider a gambler who is equally likely to either win or lose 1 unit on every play,
independently of his past results. That is, if 
 is the gambler’s winnings on the 
th play, then the 
 are independent and
Let 
 denote the gambler’s winnings after  plays. We will use the
Chernoff bound on 
 To start, note that the moment generating function
of 
 is
Now, using the McLaurin expansions of 
 and 
 we see that
Therefore,
Since the moment generating function of the sum of independent random
variables is the product of their moment generating functions, we have
𝑃ሼ𝑋൒𝑖ሽ൑𝑒െఒሺ𝑒𝜆ሻ௜
𝑖௜
𝑋௜
𝑖
𝑋௜
𝑃ቄ𝑋௜ൌ1ቅൌ𝑃ቄ𝑋௜ൌെ1ቅൌ1
2
𝑆௡ൌ෍
௜ൌଵ
௡
𝑋௜
𝑛
𝑃ሼ𝑆௡൒𝑎ሽ.
𝑋௜
𝐸ሾ𝑒௧௑ሿൌ𝑒௧൅𝑒െ௧
2
𝑒௧
𝑒െ௧,
𝑒௧൅𝑒െ௧
ൌ
1 ൅𝑡൅𝑡ଶ
2! ൅𝑡ଷ
3! ൅⋯൅ቆ1 െ𝑡൅𝑡ଶ
2! െ𝑡ଷ
3! ൅⋯ቇ
ൌ
2ቊ1 ൅𝑡ଶ
2! ൅𝑡ସ
4! ൅⋯ቋ
ൌ
2 ෍
௡ൌ଴
ஶ
𝑡ଶ௡
ሺ2𝑛ሻ!
൑
2 ෍
௡ൌ଴
ஶ
ሺ𝑡ଶ/2ሻ
௡
𝑛!
 since ሺ2𝑛ሻ! ൒𝑛!2௡
ൌ
2𝑒௧మ/ଶ
𝐸ሾ𝑒௧௑ሿ൒𝑒௧మ/ଶ
𝐸ሾ𝑒௧ௌ೙ሿൌ
ሺ𝐸ሾ𝑒௧௑ሿሻ
௡
൑
𝑒௡௧మ/ଶ
645 of 848

Using the preceding result along with the Chernoff bound gives
The value of  that minimizes the right side of the preceding is the value that
minimizes 
 and this value is 
 Supposing that 
 (so that the
minimizing  is positive) and letting 
 in the preceding inequality yields
This latter inequality yields, for example,
whereas the exact probability is
The next inequality is one having to do with expectations rather than probabilities.
Before stating it, we need the following definition.
Definition
A twice-differentiable real-valued function 
 is said to be convex if 
for all 
 similarly, it is said to be concave if 
Some examples of convex functions are 
 and 
 for
 If 
 is convex, then 
 is concave, and vice versa.
Proposition 5.3 Jensen’s inequality
If 
 is a convex function, then
provided that the expectations exist and are finite.
Proof Expanding 
 in a Taylor’s series expansion about 
 yields
𝑃ቄ𝑆௡൒𝑎ቅ൑𝑒െ௧௔𝑒௡௧మ/ଶ 𝑡൐0
𝑡
𝑛𝑡ଶ/2 െ𝑡𝑎,
𝑡ൌ𝑎/𝑛.
𝑎൐0
𝑡
𝑡ൌ𝑎/𝑛
𝑃ቄ𝑆௡൒𝑎ቅ൑𝑒െ௔మ/ଶ௡ 𝑎൐0
𝑃൛𝑆ଵ଴൒6ൟ൑𝑒െଷ଺/ଶ଴ൎ.1653
𝑃ሼ𝑆ଵ଴൒6ሽ
ൌ𝑃ሼgambler wins at least 8 of the ϐirst 10 gamesሽ
ൌ
ቆ10
8 ቇ൅ቆ10
9 ቇ൅ቆ10
10ቇ
2ଵ଴
ൌ
56
1024 ൎ.0547
𝑓ሺ𝑥ሻ
𝑓ᇳሺ𝑥ሻ൒0
𝑥;
𝑓ᇳሺ𝑥ሻ൑0.
𝑓ሺ𝑥ሻൌ𝑥ଶ, 𝑓ሺ𝑥ሻൌ𝑒௔௫,
𝑓ሺ𝑥ሻൌെ𝑥ଵ/௡
𝑥൒0.
𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻൌെ𝑓ሺ𝑥ሻ
𝑓ሺ𝑥ሻ
𝐸ሾ𝑓ሺ𝑋ሻሿ൒𝑓ሺ𝐸ሾ𝑋ሿሻ
𝑓ሺ𝑥ሻ
𝜇ൌ𝐸ሾ𝑋ሿ
𝑓ሺ𝑥ሻൌ𝑓ሺ𝜇ሻ൅𝑓ᇱሺ𝜇ሻሺ𝑥െ𝜇ሻ൅𝑓ᇳሺ𝜉ሻሺ𝑥െ𝜇ሻଶ
2
646 of 848

where  is some value between  and  Since 
 we obtain
Hence,
Taking expectations yields
and the inequality is established.
Example 5f
An investor is faced with the following choices: Either she can invest all of her
money in a risky proposition that would lead to a random return  that has mean
 or she can put the money into a risk-free venture that will lead to a return of 
with probability 1. Suppose that her decision will be made on the basis of
maximizing the expected value of 
 where  is her return and  is her utility
function. By Jensen’s inequality, it follows that if  is a concave function, then
 so the risk-free alternative is preferable, whereas if  is convex,
then 
 so the risky investment alternative would be preferred.
The following proposition, which implies that the covariance of two increasing
functions of a random variable is nonnegative, is quite useful.
Proposition 5.4
If  and  are increasing functions then
Proof To prove the preceding inequality suppose that  and  are independent
with the same distribution and that  and  are both increasing functions. Then,
because  and  are increasing, 
 and 
 will both be
positive when 
and will both be negative when 
 Consequently, their
product is positive. That is,
Taking expectations gives
𝜉
𝑥
𝜇.
𝑓ᇳሺ𝜉ሻ൒0,
𝑓ሺ𝑥ሻ൒𝑓ሺ𝜇ሻ൅𝑓ᇱሺ𝜇ሻሺ𝑥െ𝜇ሻ
𝑓ሺ𝑋ሻ൒𝑓ሺ𝜇ሻ൅𝑓ᇱሺ𝜇ሻሺ𝑋െ𝜇ሻ
𝐸ሾ𝑓ሺ𝑋ሻሿ൒𝑓ሺ𝜇ሻ൅𝑓ᇱሺ𝜇ሻ𝐸ሾ𝑋െ𝜇ሿൌ𝑓ሺ𝜇ሻ
𝑋
𝑚,
𝑚
𝑢ሺ𝑅ሻ,
𝑅
𝑢
𝑢
𝐸ሾ𝑢ሺ𝑋ሻሿ൑𝑢ሺ𝑚ሻ,
𝑢
𝐸ሾ𝑢ሺ𝑋ሻሿ൒𝑢ሺ𝑚ሻ,
𝑓
𝑔
𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿ൒𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ
𝑋
𝑌
𝑓
𝑔
𝑓
𝑔
𝑓ሺ𝑋ሻെ𝑓ሺ𝑌ሻ
𝑔ሺ𝑋ሻെ𝑔ሺ𝑌ሻ
𝑋൐𝑌
𝑋൏𝑌.
ሺ𝑓ሺ𝑋ሻെ𝑓ሺ𝑌ሻሻሺ𝑔ሺ𝑋ሻെ𝑔ሺ𝑌ሻሻ൒0
𝐸ሾሺ𝑓ሺ𝑋ሻെ𝑓ሺ𝑌ሻሻሺ𝑔ሺ𝑋ሻെ𝑔ሺ𝑌ሻሻሿ൒0
647 of 848

Multiplying through and taking expectations term by term yields
Now,
Similarly, 
 and
 Hence, from Equation (5.1)
 we obtain that
which proves the result.
Example 5g
Suppose there are 
 days in a year, and that each person is independently born
on day  with probability 
 Let 
 be the event that
persons  and  are born on the same day. In Example 5c
 of Chapter 4
,
we showed that the information that persons  and  have the same birthday
makes it more likely that persons  and  have the same birthday. After proving
this result, we argued that it was intuitive because if “popular days” are the ones
whose probabilities are relatively large, then knowing that  and  share the
same birthday makes it more likely (than when we have no information) that the
birthday of person  is a popular day and that makes it more likely that person 
will have the same birthday as does  To give credence to this intuition, suppose
that the days are renumbered so that 
 is an increasing function of  That is,
renumber the days so that day  is the day with lowest birthday probability, day 
is the day with second lowest birthday probability, and so on. Letting  be the
birthday of person  then because the higher numbered days are the most
popular our intuitive explanation would lead us to believe that the expected value
of  should increase upon the information that  and  have the same birthday.
That is, it should be that 
 To verify this, let  be the birthday of
person  and note that
𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿെ𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑌ሻሿെ𝐸ሾ𝑓ሺ𝑌ሻ𝑔ሺ𝑋ሻሿ൅𝐸ሾ𝑓ሺ𝑌ሻ𝑔ሺ𝑌ሻሿ൒0
(5.1)
𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑌ሻሿ
ൌ𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑌ሻሿ
by independence of 𝑋 and 𝑌
ൌ𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ
because 𝑋 and 𝑌 have the same distribution
𝐸ሾ𝑓ሺ𝑌ሻ𝑔ሺ𝑋ሻሿൌ𝐸ሾ𝑓ሺ𝑌ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿൌ𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ,
𝐸ሾ𝑓ሺ𝑌ሻ𝑔ሺ𝑌ሻሿൌ𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿ.
2𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿെ2𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ൒0
𝑚
𝑟
𝑝௥,𝑟ൌ1, . . . ,𝑚, ෍
௥ൌଵ
௠
𝑝௥ൌ1.
𝐴௜,௝
𝑖
𝑗
1
2
1
3
1
2
1
3
1.
𝑝௥
𝑟.
1
2
𝑋
1,
𝑋
1
2
𝐸ሾ𝑋ห𝐴ଵ,ଶሿ൒𝐸ሾ𝑋ሿ.
𝑌
2,
648 of 848

Hence,
Because 
 we need show that
But
and thus we must show that
which follows from Proposition 5.4
 because both 
 and 
are increasing functions of 
When 
 is an increasing and 
 is a decreasing function, then it is a simple
consequence of Proposition 5.4
 that
We leave the verification of the preceding as an exercise.
Our final example of this section deals with a Poisson limit result.
Example 5h A Poisson Limit Result
Consider a sequence of independent trials, with each trial being a success with
probability  If we let  be the number of trials until there have been a total of 
successes, then  is a negative binomial random variable with
𝑃൫𝑋ൌ𝑟|𝐴ଵ,ଶ൯
ൌ
𝑃൫𝑋ൌ𝑟, 𝐴ଵ,ଶ൯
𝑃൫𝐴ଵ,ଶ൯
ൌ
𝑃ሺ𝑋ൌ𝑟, 𝑌ൌ𝑟ሻ
∑௥𝑃ሺ𝑋ൌ𝑟, 𝑌ൌ𝑟ሻ
ൌ
𝑝௥
ଶ
∑௥𝑝௥
ଶ
𝐸ൣ𝑋|𝐴ଵ,ଶ൧ൌ∑
௥
𝑟𝑃൫𝑋ൌ𝑟|𝐴ଵ,ଶ൯ൌ
∑௥𝑟𝑝௥
ଶ
∑௥𝑝௥
ଶ
𝐸ሾ𝑋ሿൌ∑௥𝑟𝑃ሺ𝑋ൌ𝑟ሻൌ∑௥𝑟𝑝௥,
∑
௥
𝑟𝑝௥
ଶ൒൬∑
௥
𝑟𝑝௥൰൬∑
௥
𝑝௥
ଶ൰
𝐸ሾ𝑋𝑝௑ሿൌ෍
௥
𝑟𝑝௥𝑃ሺ𝑋ൌ𝑟ሻൌ෍
௥
𝑟𝑝௥
ଶ, 𝐸ሾ𝑝௑ሿൌ෍
௥
𝑝௥
ଶ, 𝐸ሾ𝑋ሿൌ෍𝑟𝑝௥
𝐸ሾ𝑋𝑝௑ሿ൒𝐸ሾ𝑝௑ሿ𝐸ሾ𝑋ሿ
𝑓ሺ𝑋ሻൌ𝑋
𝑔ሺ𝑋ሻൌ𝑝௑
𝑋.
𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻ
𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿ൑𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ
𝑝.
𝑌
𝑟
𝑌
649 of 848

Thus, when 
 we have that
Now, when  is large, 
 Thus, as  becomes larger, the mean of 
grows proportionately with  while the variance converges to  Hence, we might
expect that when  is large  will be close to its mean value of 
 Now, if we
let  be the number of failures that result in those  trials - that is,  is the
number of failures before there have been a total of  successes - then when  is
large, because  is approximately 
 it would seem that  would
approximately have the distribution of the number of failures in 
 independent
trials when each trial is a failure with probability 
 But by the
Poisson limit of the binomial, such a random variable should approximately be
Poisson with mean 
 That is, as 
 we might expect that the
distribution of  converges to that of a Poisson random variable with mean  We
now show that this is indeed true.
Because 
 if the 
 success occurs on trial 
 we see that
When 
Also,
𝐸ሾ𝑌ሿൌ𝑟
𝑝,
Varሺ𝑌ሻൌ𝑟ሺ1 െ𝑝ሻ
𝑝ଶ
𝑝ൌ
𝑟
𝑟൅𝜆,
𝐸ሾ𝑌ሿൌ𝑟൅𝜆,
Varሺ𝑌ሻൌ
ఒሺ௥൅ఒሻ
௥
𝑟
Varሺ𝑌ሻൎ𝜆.
𝑟
𝑌
𝑟,
𝜆.
𝑟
𝑌
𝑟൅𝜆.
𝑋
𝑌
𝑋
𝑟
𝑟
𝑌
𝑟൅𝜆,
𝑋
𝑟൅𝜆
1 െ𝑝ൌ
𝜆
𝜆൅𝑟.
ሺ𝑟൅𝜆ሻ
𝜆
𝜆൅𝑟ൌ𝜆.
𝑟→∞,
𝑋
λ.
𝑋ൌ𝑘
𝑟௧௛
𝑟൅𝑘,
𝑃ሺ𝑋ൌ𝑘ሻൌ
𝑃ሺ𝑌ൌ𝑟൅𝑘ሻ
ൌ
ቆ𝑟൅𝑘െ1
𝑟െ1
ቇ𝑝௥ሺ1 െ𝑝ሻ௞
𝑝ൌ
𝑟
𝑟൅𝜆,
ቆ𝑟൅𝑘െ1
𝑟െ1
ቇሺ1 െ𝑝ሻ௞ൌ
ቆ𝑟൅𝑘െ1
𝑘
ቇሺ
𝜆
𝑟൅𝜆ሻ
௞
ൌ
ሺ𝑟൅𝑘െ1ሻሺ𝑟൅𝑘െ2ሻ⋯𝑟
𝑘!
 
𝜆௞
ሺ𝑟൅𝜆ሻ௞
ൌ
𝜆௞
𝑘!
𝑟൅𝑘െ1
𝑟൅𝜆
  𝑟൅𝑘െ2
𝑟൅𝜆
⋯
𝑟
𝑟൅𝜆
→
𝜆௞
𝑘!   as  𝑟→∞
650 of 848

Thus, we see that
In this section, we establish bounds on how closely a sum of independent Bernoulli
random variables is approximated by a Poisson random variable with the same
mean. Suppose that we want to approximate the sum of independent Bernoulli
random variables with respective means 
 Starting with a sequence
 of independent Poisson random variables, with 
 having mean 
 we will
construct a sequence of independent Bernoulli random variables 
 with
parameters 
 such that
Letting 
 and 
 we will use the preceding inequality to conclude
that
Finally, we will show that the preceding inequality implies that for any set of real
numbers 
Since  is the sum of independent Bernoulli random variables and  is a Poisson
1
𝑝௥ൌሺ𝑟൅𝜆
𝑟
ሻ
௥
ൌሺ1 ൅𝜆
𝑟ሻ
௥
→𝑒ఒ  as  𝑟→∞
𝑃ሺ𝑋ൌ𝑘ሻ→𝑒െఒ𝜆௞
𝑘!   as  𝑟→∞
𝑝ଵ,𝑝ଶ, . . . ,𝑝௡.
𝑌ଵ, . . . ,𝑌௡
𝑌௜
𝑝௜,
𝑋ଵ, . . . ,𝑋௡
𝑝ଵ, . . . ,𝑝௡
𝑃ሼ𝑋௜്𝑌௜ሽ൑𝑝௜
ଶ  for each 𝑖
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
𝑌ൌ෍
௜ൌଵ
௡
𝑌௜,
𝑃ሼ𝑋്𝑌ሽ൑෍
௜ൌଵ
௡
 𝑝௜
ଶ
𝐴,
||𝑃ሼ𝑋∈𝐴ሽെ𝑃ሼ𝑌∈𝐴ሽ|| ൑෍
௜ൌଵ
௡
 𝑝௜
ଶ
𝑋
𝑌
651 of 848

random variable, the latter inequality will yield the desired bound.
To show how the task is accomplished, let 
 be independent Poisson
random variables with respective means 
 Now let 
 be independent
random variables that are also independent of the 
’s and are such that
This definition implicitly makes use of the inequality
in assuming that 
Next, define the random variables 
 by
Note that
Now, if 
 is equal to 0, then so must 
 equal 0 (by the definition of 
). Therefore,
Now let 
 and 
 and note that  is the sum of independent
Bernoulli random variables and  is Poisson with the expected value
 Note also that the inequality 
 implies that 
 for
some  so
𝑌௜,𝑖ൌ1, . . . ,𝑛
𝑝௜.
𝑈ଵ, . . . ,𝑈௡
𝑌௜
𝑈௜ൌቐ
0
with probability ሺ1 െ𝑝௜ሻ𝑒௣೔
1
with probability 1 െሺ1 െ𝑝௜ሻ𝑒௣೔
𝑒െ௣൒1 െ𝑝
ሺ1 െ𝑝௜ሻ𝑒௣೔൑1.
𝑋௜,𝑖ൌ1, . . . ,𝑛,
𝑋௜ൌቊ
0
if 𝑌௜ൌ𝑈௜ൌ0
1
otherwise
𝑃ሼ𝑋௜ൌ0ሽൌ𝑃ሼ𝑌௜ൌ0ሽ𝑃ሼ𝑈௜ൌ0ሽൌ𝑒െ௣೔ሺ1 െ𝑝௜ሻ𝑒௣೔ൌ1 െ𝑝௜
𝑃ሼ𝑋௜ൌ1ሽൌ1 െ𝑃ሼ𝑋௜ൌ0ሽൌ𝑝௜
𝑋௜
𝑌௜
𝑋௜
𝑃ሼ𝑋௜്𝑌௜ሽ
ൌ
𝑃ሼ𝑋௜ൌ1, 𝑌௜്1ሽ
ൌ
𝑃ሼ𝑌௜ൌ0, 𝑋௜ൌ1ሽ൅𝑃ሼ𝑌௜൐1ሽ
ൌ
𝑃ሼ𝑌௜ൌ0, 𝑈௜ൌ1ሽ൅𝑃ሼ𝑌௜൐1ሽ
ൌ
𝑒െ௣೔ሾ1 െሺ1 െ𝑝௜ሻ𝑒௣೔ሿ൅1 െ𝑒െ௣೔െ𝑝௜𝑒െ௣೔
ൌ
𝑝௜െ𝑝௜𝑒െ௣೔
൑
𝑝௜
ଶ  ሺsince 1 െ𝑒െ௣൑𝑝ሻ
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
𝑌ൌ෍
௜ൌଵ
௡
𝑌௜,
𝑋
𝑌
𝐸ሾ𝑌ሿൌ𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௡
𝑝௜.
𝑋്𝑌
𝑋௜്𝑌௜
𝑖,
652 of 848

For any event 
 let 
 the indicator variable for the event 
 be defined by
Note that for any set of real numbers 
The preceding inequality follows from the fact that since an indicator variable is either
0 or 1, the left-hand side equals 1 only when 
 and 
 But this
would imply that 
 and 
 which means that 
 so the right side would
also equal 1. Upon taking expectations of the preceding inequality, we obtain
By reversing  and 
 we obtain, in the same manner,
Thus, we can conclude that
Therefore, we have proven that with 
Remark When all the 
 are equal to 
 is a binomial random variable. Hence, the
preceding inequality shows that, for any set of nonnegative integers 
𝑃ሼ𝑋്𝑌ሽ
൑
𝑃ሼ𝑋௜്𝑌௜ for some 𝑖ሽ
൑
෍
௜ൌଵ
௡
𝑃ሼ𝑋௜്𝑌௜ሽ  ሺBoole's inequalityሻ
൑
෍
௜ൌଵ
௡
𝑝௜
ଶ
𝐵,
𝐼஻,
𝐵,
𝐼஻ൌቊ1
if 𝐵 occurs
0
otherwise
𝐴,
𝐼൛௑∈஺ൟെ𝐼൛௒∈஺ൟ൑𝐼൛௑ஷ௒ൟ
𝐼൛௑∈஺ൟൌ1
𝐼൛௒∈஺ൟൌ0.
𝑋∈𝐴
𝑌∈𝐴,
𝑋്𝑌,
𝑃ሼ𝑋∈𝐴ሽെ𝑃ሼ𝑌∈𝐴ሽ൑𝑃ሼ𝑋്𝑌ሽ
𝑋
𝑌,
𝑃ሼ𝑌∈𝐴ሽെ𝑃ሼ𝑋∈𝐴ሽ൑𝑃ሼ𝑋്𝑌ሽ
||𝑃ሼ𝑋∈𝐴ሽെ𝑃ሼ𝑌∈𝐴ሽ|| ൑𝑃ሼ𝑋്𝑌ሽ
𝜆ൌ
Σ
௜ൌଵ
௡
𝑝௜,
ቮ𝑃ቐ෍
௜ൌଵ
௡
𝑋௜∈𝐴ቑെ෍
௜∈஺
𝑒െఒ𝜆௜
𝑖!
ቮ൑෍
௜ൌଵ
௡
𝑝௜
ଶ
𝑝௜
𝑝, 𝑋
𝐴,
653 of 848

The Lorenz curve 
 is a plot of the fraction of the total income of a
population that is earned by the 
 percent of the population having the lowest
incomes. For instance, 
 is the fraction of total income earned by the lower half of
income earners. Suppose that the earnings of the members of a population can be
represented by the quantities 
 where the 
 are independent and identically
distributed positive continuous random variables with distribution function 
 Now, let
 be a random variable with distribution 
 and define 
 to be that value such that
The quantity 
 is called the 
 percentile of the distribution 
 With 
 defined by
it follows that 
 is the fraction of the first  members of the
population that have incomes less than 
 Upon letting 
 and applying the
strong law of large numbers to the independent and identically distributed random
variables 
 the preceding yields that, with probability 
That is, with probability 
 is the fraction of the population whose income is less
than 
 The fraction of the total income earned by those earning less than 
 can be
obtained by noting that the fraction of the total income of the first  members of the
population that is from those earning less that 
 is 
 Letting
 yields that
ቮ෍
௜∈஺
ቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜െ෍
௜∈஺
𝑒െ௡௣ሺ𝑛𝑝ሻ௜
𝑖!
ቮ൑𝑛𝑝ଶ
𝐿ሺ𝑝ሻ, 0 ൏𝑝൏1
100𝑝
𝐿ሺ.5ሻ
𝑋ଵ,𝑋ଶ, . . .
𝑋௜
𝐹.
𝑋
𝐹,
𝜉௣
𝑃ቄ𝑋൑𝜉௣ቅൌ𝐹ሺ𝜉௣ሻൌ𝑝
𝜉௣
100𝑝
𝐹.
𝐼ሺ𝑥ሻ
𝐼ሺ𝑥ሻൌ൝
1,
  if  𝑥൏𝜉௣
0,
  if  𝑥൒𝜉௣
𝐼ሺ𝑋ଵሻ൅. . . ൅𝐼ሺ𝑋௡ሻ
𝑛
𝑛
𝜉௣.
𝑛→∞,
𝐼ሺ𝑋௞ሻ, 𝑘൒1,
1,
lim
௡→ஶ
𝐼ሺ𝑋ଵሻ൅. . . ൅𝐼ሺ𝑋௡ሻ
𝑛
ൌ𝐸ሾ𝐼ሺ𝑋ሻሿൌ𝐹ሺ𝜉௣ሻൌ𝑝
1, 𝑝
𝜉௣.
𝜉௣
𝑛
𝜉௣
𝑋ଵ𝐼ሺ𝑋ଵሻ൅. . . ൅𝑋௡𝐼ሺ𝑋௡ሻ
𝑋ଵ൅. . . ൅𝑋௡
.
𝑛→∞,
𝐿ሺ𝑝ሻൌ
lim
௡→ஶ
𝑋ଵ𝐼ሺ𝑋ଵሻ൅. . . ൅𝑋௡𝐼ሺ𝑋௡ሻ
𝑛
𝑋ଵ൅. . . ൅𝑋௡
𝑛
ൌ𝐸ሾ𝑋𝐼ሺ𝑋ሻሿ
𝐸ሾ𝑋ሿ
,
654 of 848

where the final equality was obtained by applying the strong law of large numbers to
both the numerator and denominator of the preceding fraction. Letting 
 and
noting that
shows that
Example 7a
If  is the distribution function of a uniform random variable on 
 where
 then
Because 
 we see that 
 Because the mean
of a uniform 
 random variable is 
 we obtain from Equation
(7.1)
 that
When 
 the preceding gives that 
 Also, letting  converge to 
gives that
which can be interpreted as saying that 
 when all members of the
population earn the same amount.
𝜇ൌ𝐸ሾ𝑋ሿ,
𝐸ሾ𝑋 𝐼ሺ𝑋ሻሿൌ඲
଴
ஶ
𝑥𝐼ሺ𝑥ሻ𝑓ሺ𝑥ሻ𝑑𝑥ൌ඲
଴
క೛
𝑥 𝑓ሺ𝑥ሻ𝑑𝑥
𝐿ሺ𝑝ሻൌ𝐸ሾ𝑋𝐼ሺ𝑋ሻሿ
𝐸ሾ𝑋ሿ
ൌ
1
𝐸ሾ𝑋ሿ඲
଴
క೛
𝑥𝑓ሺ𝑥ሻ𝑑𝑥
(7.1)
𝐹
ሺ𝑎,𝑏ሻ,
0 ൑𝑎൏𝑏,
𝐹ሺ𝑥ሻൌ඲
௔
௫
1
𝑏െ𝑎𝑑𝑥ൌ𝑥െ𝑎
𝑏െ𝑎, 𝑎൏𝑥൏𝑏
𝑝ൌ𝐹ሺ𝜉௣ሻൌ
𝜉௣െ𝑎
𝑏െ𝑎,
𝜉௣ൌ𝑎൅ሺ𝑏െ𝑎ሻ𝑝.
ሺ𝑎, 𝑏ሻ
ሺ𝑎൅𝑏ሻ/2,
𝐿ሺ𝑝ሻ
ൌ
2
𝑎൅𝑏඲
௔
௔൅ሺ௕െ௔ሻ௣
𝑥
𝑏െ𝑎𝑑𝑥
ൌ
ሺ𝑎൅ሺ𝑏െ𝑎ሻ𝑝ሻଶെ𝑎ଶ
ሺ𝑎൅𝑏ሻሺ𝑏െ𝑎ሻ
ൌ
2𝑝𝑎൅ሺ𝑏െ𝑎ሻ𝑝ଶ
𝑎൅𝑏
𝑎ൌ0,
𝐿ሺ𝑝ሻൌ𝑝ଶ.
𝑎
𝑏
lim
௔→௕𝐿ሺ𝑝ሻൌ𝑝
𝐿ሺ𝑝ሻൌ𝑝
655 of 848

A useful formula for 
 can be obtained by letting
and then noting that
Conditioning on 
 gives that
which shows that
Example 7b
If  is the distribution function of an exponential random variable with mean 
then 
 and so 
 Because the lack of
memory property of the exponential implies that
 we obtain from Equation (7.2)
 that the
fraction of all income that is earned by those earning at least 
 is
giving that
Example 7c
If  is the distribution function of a Pareto random variable with parameters 
 then 
 Consequently, 
 giving that
𝐿ሺ𝑝ሻ
𝐽ሺ𝑥ሻൌ1 െ𝐼ሺ𝑋ሻൌ൝
0,   if  𝑥൏𝜉௣
1,   if  𝑥൒𝜉௣
1 െ𝐿ሺ𝑝ሻൌ𝐸ሾ𝑋ሿെ𝐸ሾ𝑋𝐼ሺ𝑋ሻሿ
𝐸ሾ𝑋ሿ
ൌ𝐸ሾ𝑋𝐽ሺ𝑋ሻሿ
𝐸ሾ𝑋ሿ
𝐽ሺ𝑋ሻ
𝐸ሾ𝑋𝐽ሺ𝑋ሻሿ
ൌ
𝐸ሾ𝑋𝐽ሺ𝑋ሻ||𝐽ሺ𝑋ሻൌ1ሿ𝑃ሺ𝐽ሺ𝑋ሻൌ1ሻ൅𝐸ሾ𝑋𝐽ሺ𝑋ሻ||𝐽ሺ𝑋ሻൌ0ሿ𝑃ሺ𝐽ሺ𝑋ሻൌ0ሻ
ൌ
𝐸ሾ𝑋ቚ𝑋൒𝜉௣ሿሺ1 െ𝑝ሻ
1 െ𝐿ሺ𝑝ሻൌ
𝐸ቂ𝑋|𝑋൒𝜉௣ቃሺ1 െ𝑝ሻ
𝐸ሾ𝑋ሿ
(7.2)
𝐹
1,
𝑝ൌ𝐹ሺ𝜉௣ሻൌ1 െ𝑒െక೛,
𝜉௣ൌെlogሺ1 െ𝑝ሻ.
𝐸ቂ𝑋|𝑋൐𝜉௣ቃൌ𝜉௣൅𝐸ሾ𝑋ሿൌ𝜉௣൅1,
𝜉௣
1 െ𝐿ሺ𝑝ሻ
ൌ
ሺ𝜉௣൅1ሻሺ1 െ𝑝ሻ
ൌ
ሺ1 െlogሺ1 െ𝑝ሻሻሺ1 െ𝑝ሻ
ൌ
1 െ𝑝െሺ1 െ𝑝ሻlogሺ1 െ𝑝ሻ
𝐿ሺ𝑝ሻൌ𝑝൅ሺ1 െ𝑝ሻlogሺ1 െ𝑝ሻ
𝐹
𝜆൐0,
𝑎൐0,
𝐹ሺ𝑥ሻൌ1 െ𝑎λ
𝑥λ , 𝑥൒𝑎.
𝑝ൌ𝐹ሺ𝜉௣ሻൌ1 െ𝑎λ
𝜉௣
λ ,
656 of 848

When 
 it was shown in Section 5.6.5
 that 
 In addition, it
was shown in Example 5f
 of Chapter 6
 that if  is Pareto with parameters
 then the conditional distribution of  given that it exceeds 
 is Pareto
with parameters 
 Consequently, when 
 and thus
Equation (7.2)
 yields that
or
We now prove some properties of the function 
Proposition 7.1
 is an increasing, convex function of  such that 
Proof That 
 increases in  follows from its definition. To prove convexity we
must show that 
 increases in  for 
 or equivalently, that
the proportion of the total income earned by those with incomes between 
 and
 increases in  But this follows because, for all  the same proportion of the
population – namely, 
 percent - earns between 
 and 
 and 
increases in  (For instance, 
 percent of the population have incomes in the
 to 
 percentile and 
 percent of the population have incomes in the 
 to 
percentile, and as the incomes earned by the  percent of the population in the
 to 
 percentile are all less than those earned by the  percent of the
population in the 
 to 
 percentile, it follows that the proportion of the
population income of those in the 
 to 
 percentile is less than the proportion
of those in the 
 to 
 percentile.) To establish that 
 we see from
Equation (7.1)
 that we need to show that 
 But this follows
because 
 equal to  if 
 and to  if 
 is a decreasing and 
is an increasing function of  which from Proposition 5.4
 implies that
Because 
 with 
 when all members of the population have the same
income, the area of the “hump”, equal to the region between the straight line and the
𝜉௣
λ ൌ
𝑎ఒ
1 െ𝑝 or 𝜉௣ൌ𝑎ሺ1 െ𝑝ሻെଵ/λ
𝜆൐1,
𝐸ሾ𝑋ሿൌ
𝜆𝑎
𝜆െ1 .
𝑋
λ, 𝑎
𝑋
𝑥଴, 𝑥଴൐𝑎,
𝜆, 𝑥଴.
𝜆൐1, 𝐸ቂ𝑋|𝑋൐𝜉௣ቃൌ
𝜆𝜉௣
𝜆െ1 ,
1 െ𝐿ሺ𝑝ሻൌ
𝐸ቂ𝑋|𝑋൐𝜉௣ቃሺ1 െ𝑝ሻ
𝐸ሾ𝑋ሿ
ൌ
𝜉௣ሺ1 െ𝑝ሻ
𝑎
ൌሺ1 െ𝑝ሻଵെଵ/ఒ
𝐿ሺ𝑝ሻൌ1 െሺ1 െ𝑝ሻ
ఒെଵ
ఒ
𝐿ሺ𝑝ሻ.
𝐿ሺ𝑝ሻ
𝑝,
 𝐿ሺ𝑝ሻ൑𝑝.
𝐿ሺ𝑝ሻ
𝑝
𝐿ሺ𝑝൅𝑎ሻെ𝐿ሺ𝑝ሻ
𝑝
𝑝൑1 െ𝑎;
𝜉௣
𝜉௣൅௔
𝑝.
𝑝,
100𝑎
𝜉௣
𝜉௣൅௔,
𝜉௣
𝑝.
10
40
50
10
45
55
5
40
45
5
50
55
40
50
45
55
𝐿ሺ𝑝ሻ൑𝑝,
𝐸ሾ𝑋𝐼ሺ𝑋ሻሿ൑𝐸ሾ𝑋ሿ𝑝.
𝐼ሺ𝑥ሻ,
1
𝑥൏𝜉௣
0
𝑥൒𝜉௣,
ℎሺ𝑥ሻൌ𝑥
𝑥,
𝐸ሾ𝑋𝐼ሺ𝑋ሻሿ൑𝐸ሾ𝑋ሿ𝐸ሾ𝐼ሺ𝑋ሻሿൌ𝐸ሾ𝑋ሿ𝑝.
𝐿ሺ𝑝ሻ൑𝑝
𝐿ሺ𝑝ሻൌ𝑝
657 of 848

Lorenz curve (the shaded region in Figure 8.3
), is an indication of the inequality
of incomes.
Figure 8.3 The Hump of the Lorenz Curve
A measure of the inequality of the incomes is given by the Gini index, which is the
ratio of the area of the hump divided by the area under the straight line 
Because the area of a triangle is one half its base times its height, it follows that the
Gini index, call it 
 is given by
Example 7d
Find the Gini index when 
 the distribution of earnings for an individual in the
population, is uniform on (0, 1), and when  is exponential with rate 
Solution
When  is the uniform 
 distribution, then as shown in Example 7a
,
 giving that 
 When  is exponential, then from
Example 7b
𝐿ሺ𝑝ሻൌ𝑝.
𝐺,
𝐺ൌ
1/2 െ׬଴
ଵ 𝐿ሺ𝑝ሻ𝑑𝑝
1/2
ൌ1 െ2඲
଴
ଵ
𝐿ሺ𝑝ሻ𝑑𝑝
𝐹,
𝐹
𝜆.
𝐹
ሺ0,𝑏ሻ
𝐿ሺ𝑝ሻൌ𝑝ଶ,
𝐺ൌ1 െ2/3 ൌ1/3.
𝐹
658 of 848

Integrating by parts with 
 shows that
where L’hopital’s rule was used to obtain that 
 Hence,
 giving that 
 Because larger values of  indicate more
inequality, we see that the inequality is larger when the distribution is exponential
than when it is uniform.
Two useful probability bounds are provided by the Markov and Chebyshev
inequalities. The Markov inequality is concerned with nonnegative random variables
and says that for  of that type,
for every positive value  The Chebyshev inequality, which is a simple consequence
of the Markov inequality, states that if  has mean  and variance 
 then, for every
positive 
The two most important theoretical results in probability are the central limit theorem
and the strong law of large numbers. Both are concerned with a sequence of
independent and identically distributed random variables. The central limit theorem
says that if the random variables have a finite mean  and a finite variance 
 then
the distribution of the sum of the first  of them is, for large  approximately that of a
normal random variable with mean 
 and variance 
 That is, if 
 is the
sequence, then the central limit theorem states that for every real number 
඲
଴
ଵ
𝐿ሺ𝑝ሻ𝑑𝑝
ൌ
඲
଴
ଵ
ሺ𝑝൅ሺ1 െ𝑝ሻlogሺ1 െ𝑝ሻሻ𝑑𝑝
ൌ
1
2 ൅඲
଴
ଵ
𝑥logሺ𝑥ሻ𝑑𝑥
𝑢ൌlog 𝑥, 𝑑𝑣ൌ𝑥𝑑𝑥
඲
଴
ଵ
𝑥logሺ𝑥ሻ𝑑𝑥ൌെ඲
଴
ଵ
𝑥
2 𝑑𝑥ൌെ1/4
lim௫→଴𝑥ଶlogሺ𝑥ሻൌ0.
඲
଴
ଵ
𝐿ሺ𝑝ሻ𝑑𝑝ൌ1/4,
𝐺ൌ1/2.
𝐺
𝑋
𝑃ሼ𝑋൒𝑎ሽ൑𝐸ሾ𝑋ሿ
𝑎
𝑎.
𝑋
𝜇
𝜎ଶ,
𝑘,
𝑃ሼ|𝑋െ𝜇| ൒𝑘𝜎ሽ൑1
𝑘ଶ
𝜇
𝜎ଶ,
𝑛
𝑛,
𝑛𝜇
𝑛𝜎ଶ.
𝑋௜, 𝑖൒1,
𝑎,
659 of 848

The strong law of large numbers requires only that the random variables in the
sequence have a finite mean  It states that with probability 1, the average of the
first  of them will converge to  as  goes to infinity. This implies that if  is any
specified event of an experiment for which independent replications are performed,
then the limiting proportion of experiments whose outcomes are in  will, with
probability 1, equal 
 Therefore, if we accept the interpretation that “with
probability 1ʹ’ means “with certainty,” we obtain the theoretical justification for the
long-run relative frequency interpretation of probabilities.
lim
௡→ஶ𝑃ቊ𝑋ଵ൅⋯൅𝑋௡െ𝑛𝜇
𝜎𝑛
√
൑𝑎ቋൌ
1
2𝜋
√
඲
െஶ
௔
𝑒െ௫మ/ଶ𝑑𝑥
𝜇.
𝑛
𝜇
𝑛
𝐴
𝐴
𝑃ሺ𝐴ሻ.
8.1. Suppose that  is a random variable with mean and variance
both equal to 20. What can be said about 
8.2. From past experience, a professor knows that the test score of a
student taking her final examination is a random variable with mean
75.
a. Give an upper bound for the probability that a student’s test
score will exceed 85.
b. Suppose, in addition, that the professor knows that the
variance of a student’s test score is equal to 25. What can be
said about the probability that a student will score between 65
and 85?
c. How many students would have to take the examination to
ensure with probability at least .9 that the class average would
be within 5 of 75? Do not use the central limit theorem.
8.3. Use the central limit theorem to solve part (c) of Problem 8.2.
8.4. Let 
 be independent Poisson random variables with
mean 1.
a. Use the Markov inequality to obtain a bound on
b. Use the central limit theorem to approximate
𝑋
𝑃ሼ0 ൏𝑋൏40ሽ?
𝑋ଵ, . . . ,𝑋ଶ଴
𝑃ቐ෍
ଵ
ଶ଴
𝑋௜൐15ቑ
660 of 848

8.5. Fifty numbers are rounded off to the nearest integer and then
summed. If the individual round-off errors are uniformly distributed
over 
 approximate the probability that the resultant sum
differs from the exact sum by more than 3.
8.6. A die is continually rolled until the total sum of all rolls exceeds
300. Approximate the probability that at least 80 rolls are necessary.
8.7. A person has 100 light bulbs whose lifetimes are independent
exponentials with mean 5 hours. If the bulbs are used one at a time,
with a failed bulb being replaced immediately by a new one,
approximate the probability that there is still a working bulb after 525
hours.
8.8. In Problem 8.7
, suppose that it takes a random time,
uniformly distributed over (0, .5), to replace a failed bulb.
Approximate the probability that all bulbs have failed by time 550.
8.9. If  is a gamma random variable with parameters (  1),
approximately how large must  be so that
8.10. Civil engineers believe that 
 the amount of weight (in units of
1000 pounds) that a certain span of a bridge can withstand without
structural damage resulting, is normally distributed with mean 400
and standard deviation 40. Suppose that the weight (again, in units
of 1000 pounds) of a car is a random variable with mean 3 and
standard deviation .3. Approximately how many cars would have to
be on the bridge span for the probability of structural damage to
exceed .1?
8.11. Many people believe that the daily change of price of a
company’s stock on the stock market is a random variable with mean
0 and variance 
 That is, if 
 represents the price of the stock on
the th day, then
where 
 are independent and identically distributed random
variables with mean 0 and variance 
 Suppose that the stock’s
price today is 100. If 
 what can you say about the probability
that the stock’s price will exceed 105 after 10 days?
8.12. We have 100 components that we will put in use in a sequential
𝑃ቐ෍
ଵ
ଶ଴
𝑋௜൐15ቑ.
ሺെ.5,.5ሻ,
𝑋
𝑛,
𝑛
𝑃ቊ|
||
|
𝑋
𝑛െ1|
||
|
൐.01ቋ൏.01?
𝑊,
𝜎ଶ.
𝑌௡
𝑛
𝑌௡ൌ𝑌௡െଵ൅𝑋௡ 𝑛൒1
𝑋ଵ,𝑋ଶ, . . .
𝜎ଶ.
𝜎ଶൌ1,
661 of 848

fashion. That is, component 1 is initially put in use, and upon failure,
it is replaced by component 2, which is itself replaced upon failure by
component 3, and so on. If the lifetime of component  is
exponentially distributed with mean 
 estimate
the probability that the total life of all components will exceed 1200.
Now repeat when the life distribution of component  is uniformly
distributed over 
8.13. Student scores on exams given by a certain instructor have
mean 74 and standard deviation 14. This instructor is about to give
two exams, one to a class of size 25 and the other to a class of size
64.
a. Approximate the probability that the average test score in the
class of size 25 exceeds 80.
b. Repeat part (a) for the class of size 64.
c. Approximate the probability that the average test score in the
larger class exceeds that of the other class by more than 2.2
points.
d. Approximate the probability that the average test score in the
smaller class exceeds that of the other class by more than 2.2
points.
8.14. A certain component is critical to the operation of an electrical
system and must be replaced immediately upon failure. If the mean
lifetime of this type of component is 100 hours and its standard
deviation is 30 hours, how many of these components must be in
stock so that the probability that the system is in continual operation
for the next 2000 hours is at least .95?
8.15. An insurance company has 10,000 automobile policyholders.
The expected yearly claim per policyholder is $240, with a standard
deviation of $800. Approximate the probability that the total yearly
claim exceeds $2.7 million.
8.16. A.J. has 20 jobs that she must do in sequence, with the times
required to do each of these jobs being independent random
variables with mean 50 minutes and standard deviation 10 minutes.
M.J. has 20 jobs that he must do in sequence, with the times
required to do each of these jobs being independent random
variables with mean 52 minutes and standard deviation 15 minutes.
a. Find the probability that A.J. finishes in less than 900 minutes.
b. Find the probability that M.J. finishes in less than 900 minutes.
c. Find the probability that A.J. finishes before M.J.
𝑖
10 ൅𝑖/10,𝑖ൌ1, . . . ,100,
𝑖
ሺ0,20 ൅𝑖/5ሻ,𝑖ൌ1, . . . ,100.
662 of 848

8.17. Redo Example 5b
 under the assumption that the number of
man–woman pairs is (approximately) normally distributed. Does this
seem like a reasonable supposition?
8.18. Repeat part (a) of Problem 8.2
 when it is known that the
variance of a student’s test score is equal to 25.
8.19. A lake contains 4 distinct types of fish. Suppose that each fish
caught is equally likely to be any one of these types. Let  denote the
number of fish that need be caught to obtain at least one of each
type.
a. Give an interval (
) such that 
b. Using the one-sided Chebyshev inequality, how many fish
need we plan on catching so as to be at least 90 percent
certain of obtaining at least one of each type?
8.20. If  is a nonnegative random variable with mean 25, what can
be said about
a. 
b. 
c. [log ]?
d. 
8.21. Let  be a nonnegative random variable. Prove that
8.22. Would the results of Example 5f
 change if the investor were
allowed to divide her money and invest the fraction 
 in
the risky proposition and invest the remainder in the risk-free
venture? Her return for such a split investment would be
8.23. Let  be a Poisson random variable with mean 20.
a. Use the Markov inequality to obtain an upper bound on
b. Use the one-sided Chebyshev inequality to obtain an upper
bound on 
c. Use the Chernoff bound to obtain an upper bound on 
d. Approximate  by making use of the central limit theorem.
e. Determine  by running an appropriate program.
8.24. If  is a Poisson random variable with mean 100, then
 is approximately
a. .02,
𝑌
𝑎, 𝑏
𝑃ሼ𝑎൑𝑌൑𝑏ሽ൒.90.
𝑋
𝐸ሾ𝑋ଷሿ?
𝐸ሾ𝑋
√ሿ?
𝐸
𝑋
𝐸ሾ𝑒െ௑ሿ?
𝑋
𝐸ሾ𝑋ሿ൑ሺ𝐸ሾ𝑋ଶሿሻ
ଵ/ଶ൑ሺ𝐸ሾ𝑋ଷሿሻ
ଵ/ଷ൑⋯
𝛼,0 ൏𝛼൏1,
𝑅ൌ𝛼𝑋൅ሺ1 െ𝛼ሻ𝑚.
𝑋
𝑝ൌ𝑃ሼ𝑋൒26ሽ
𝑝.
𝑝.
𝑝
𝑝
𝑋
𝑃ሼ𝑋൐120ሽ
663 of 848

b. .5 or
c. .3?
8.25. Suppose that the distribution of earnings of members of a
population is Pareto with parameters 
 where
a. Show that the top 
 percent of earners earn 
 percent of the
total earnings.
b. Show that the top 
 percent of the top 
 percent of earners
earn 
 percent of the earnings of the top 
 percent of
earners. (That is, show that the top  percent of all earners
earn 
 percent of the total earnings of the top 
 percent of
all earners.)
8.26. If 
 is an increasing and 
 is a decreasing function, show
that 
8.27. If 
 is the Lorenz curve associated with the random variable
 show that 
8.28. Suppose that 
 is the Lorenz curve associated with the
random variable  and that 
a. Find the Lorenz curve associated with the random variable 
b. Show that 
 the Lorenz curve associated with the random
variable 
 is
c. Verify that the answer to part (b) is in accordance with the
formulas given in Example 7a
 in the case that  is uniform
over the interval 
 and 
λ, 𝑎൐0,
𝜆ൌlogሺ5ሻ
logሺ4ሻൎ1.161.
20
80
20
20
80
20
4
80
20
𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻ
𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿ൑𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ.
𝐿ሺ𝑝ሻ
𝑋,
𝐿ሺ𝑝ሻൌ
𝐸ቂ𝑋|𝑋൏𝜉௣ቃ𝑝
𝐸ሾ𝑋ሿ
.
𝐿ሺ𝑝ሻ
𝑋
𝑐൐0.
𝑐𝑋.
𝐿௖ሺ𝑝ሻ,
𝑋൅𝑐,
𝐿௖ሺ𝑝ሻൌ𝐿ሺ𝑝ሻ𝐸ሾ𝑋ሿ൅𝑝𝑐
𝐸ሾ𝑋ሿ൅𝑐
𝑋
ሺ0, 𝑏െ𝑎ሻ
𝑐ൌ𝑎.
8.1. If  has variance 
 then  the positive square root of the variance, is
called the standard deviation. If  has mean  and standard deviation  show
that
𝑋
𝜎ଶ,
𝜎,
𝑋
𝜇
𝜎,
𝑃ሼ|𝑋െ𝜇| ൒𝑘𝜎ሽ൑1
𝑘ଶ
664 of 848

8.2. If  has mean  and standard deviation  the ratio 
 is called the
measurement signal-to-noise ratio of 
 The idea is that  can be expressed
as 
 with  representing the signal and 
 the noise. If we
define 
 as the relative deviation of  from its signal (or mean)
 show that for 
8.3. Compute the measurement signal-to-noise ratio–that is, 
 where
 and 
–of the following random variables:
a. Poisson with mean 
b. binomial with parameters  and 
c. geometric with mean 1/
d. uniform over 
e. exponential with mean 
f. normal with parameters 
8.4. Let 
 be a sequence of random variables and  a constant such
that for each 
 as 
 Show that for any bounded
continuous function 
8.5. Let 
 be a continuous function defined for 
 Consider the
functions
(called Bernstein polynomials) and prove that
Hint: Let 
 be independent Bernoulli random variables with mean 
Show that
and then use Theoretical Exercise 8.4
.
Since it can be shown that the convergence of 
 to 
 is uniform in 
the preceding reasoning provides a probabilistic proof of the famous
Weierstrass theorem of analysis, which states that any continuous function on
a closed interval can be approximated arbitrarily closely by a polynomial.
8.6.
𝑋
𝜇
𝜎,
𝑟≡|𝜇|/𝜎
𝑋.
𝑋
𝑋ൌ𝜇൅ሺ𝑋െ𝜇ሻ,
𝜇
𝑋െ𝜇
||ሺ𝑋െ𝜇ሻ/𝜇|| ≡𝐷
𝑋
𝜇,
𝛼൐0,
𝑃ሼ𝐷൑𝛼ሽ൒1 െ
1
𝑟ଶ𝛼ଶ
|𝜇| /𝜎,
𝜇ൌ𝐸ሾ𝑋ሿ
𝜎ଶൌVarሺ𝑋ሻ
𝜆;
𝑛
𝑝;
𝑝;
ሺ𝑎, 𝑏ሻ;
1/𝜆;
𝜇, 𝜎ଶ.
𝑍௡, 𝑛൒1,
𝑐
𝜀൐0, 𝑃ሼ||𝑍௡െ𝑐|| ൐𝜀ሽ→0
𝑛→∞.
𝑔,
𝐸ሾ𝑔ሺ𝑍௡ሻሿ→𝑔ሺ𝑐ሻ 𝑎𝑠 𝑛→∞
𝑓ሺ𝑥ሻ
0 ൑𝑥൑1.
𝐵௡ሺ𝑥ሻൌ
෍
௞ൌ଴
௡
𝑓ቆ𝑘
𝑛ቇቆ𝑛
𝑘ቇ𝑥௞ሺ1 െ𝑥ሻ௡െ௞
lim
௡→ஶ𝐵௡ሺ𝑥ሻൌ𝑓ሺ𝑥ሻ
𝑋ଵ,𝑋ଶ, . . .
𝑥.
𝐵௡ሺ𝑥ሻൌ𝐸ቈ𝑓ቆ𝑋ଵ൅⋯൅𝑋௡
𝑛
ቇ቉
𝐵௡ሺ𝑥ሻ
𝑓ሺ𝑥ሻ
𝑥,
665 of 848

a. Let  be a discrete random variable whose possible values are 
If 
 is nonincreasing in 
 prove that
b. Let  be a nonnegative continuous random variable having a
nonincreasing density function. Show that
8.7. Suppose that a fair die is rolled 100 times. Let 
 be the value obtained
on the th roll. Compute an approximation for
8.8. Explain why a gamma random variable with parameters 
 has an
approximately normal distribution when  is large.
8.9. Suppose a fair coin is tossed 1000 times. If the first 100 tosses all result
in heads, what proportion of heads would you expect on the final 900 tosses?
Comment on the statement “The strong law of large numbers swamps but
does not compensate.”
8.10. If  is a Poisson random variable with mean  show that for 
8.11.Let  be a binomial random variable with parameters  and  Show that,
for 
a. 
 occurs when  is such that 
 where
b. 
8.12.The Chernoff bound on a standard normal random variable  gives
 Show, by considering the density of  that the right
side of the inequality can be reduced by the factor 2. That is, show that
8.13 Show that if 
 and 
 is such that 
 then 
8.14 Let 
 be a sequence of independent and identically distributed
random variables with distribution 
 having a finite mean and variance.
𝑋
1,2, . . . .
𝑃ሼ𝑋ൌ𝑘ሽ
𝑘ൌ1,2, . . . ,
𝑃ሼ𝑋ൌ𝑘ሽ൑2 𝐸ሾ𝑋ሿ
𝑘ଶ
𝑋
𝑓ሺ𝑥ሻ൑2𝐸ሾ𝑋ሿ
𝑥ଶ
 for all 𝑥൐0
𝑋௜
𝑖
𝑃ቐෑ
ଵ
ଵ଴଴
𝑋௜൑𝑎ଵ଴଴ቑ 1 ൏𝑎൏6
ሺ𝑡,𝜆ሻ
𝑡
𝑋
𝜆,
𝑖൏𝜆,
𝑃ሼ𝑋൑𝑖ሽ൑𝑒െఒሺ𝑒𝜆ሻ௜
𝑖௜
𝑋
𝑛
𝑝.
𝑖൐𝑛𝑝,
minimum  𝑒െ௧௜𝐸ሾ𝑒௧௑ሿ
𝑡
𝑒௧ൌ
𝑖𝑞
ሺ𝑛െ𝑖ሻ𝑝,
𝑞ൌ1 െ𝑝.
𝑃ሼ𝑋൒𝑖ሽ൑
𝑛௡
𝑖௜ሺ𝑛െ𝑖ሻ௡െ௜𝑝௜ሺ1 െ𝑝ሻ௡െ௜.
𝑍
𝑃ቄ𝑍൐𝑎ቅ൑𝑒െ௔మ/ଶ, 𝑎൐0.
𝑍,
𝑃ሼ𝑍൐𝑎ሽ൑1
2 𝑒െ௔మ/ଶ 𝑎൐0
𝐸ሾ𝑋ሿ൏0
𝜃്0
𝐸ሾ𝑒ఏ௑ሿൌ1,
𝜃൐0.
𝑋ଵ,𝑋ଶ, . . .
𝐹,
666 of 848

Whereas the central limit theorem states that the distribution of 
approaches a normal distribution as  goes to infinity, it gives us no
information about how large  need be before the normal becomes a good
approximation. Whereas in most applications, the approximation yields good
results whenever 
 and oftentimes for much smaller values of  how
large a value of  is needed depends on the distribution of 
 Give an
example of a distribution  such that the distribution of 
 is not close to
a normal distribution.
Hint: Think Poisson.
8.15. If  and  are density functions that are positive over the same region,
then the Kullback-Leiber divergence from density  to density  is defined by
where the notation 
 is used to indicate that  has density function 
a. Show that 
b. Use Jensen’s inequality and the identity 
 to
show that 
8.16. Let 
 be the Lorenz curve associated with the distribution function 
with density function  and mean 
a. Show that
Hint: Starting with 
 make the change of variable
b. Use part (a) to show that 
 is convex.
c. Show that
d. Verify the preceding formula by using it to compute the Gini index of a
uniform 
 and an exponential random variable, comparing your
answers with those given in Example 7d
.
෍
௜ൌଵ
௡
𝑋௜
𝑛
𝑛
𝑛൒20,
𝑛,
𝑛
𝑋௜.
𝐹
෍
௜ൌଵ
ଵ଴଴
𝑋௜
𝑓
𝑔
𝑓
𝑔
𝐾𝐿ሺ𝑓,𝑔ሻൌ𝐸௙ሾlogቆ𝑓ሺ𝑋ሻ
𝑔ሺ𝑋ሻቇሿൌ඲logቆ𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻቇ𝑓ሺ𝑥ሻ𝑑𝑥
𝐸௙ሾℎሺ𝑋ሻሿ
𝑋
𝑓.
𝐾𝐿ሺ𝑓,𝑓ሻൌ0
logሺ𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻሻൌെlogሺ𝑔ሺ𝑥ሻ
𝑓ሺ𝑥ሻሻ,
𝐾𝐿ሺ𝑓,𝑔ሻ൒0
𝐿ሺ𝑝ሻ
𝐹,
𝑓
𝜇.
𝐿ሺ𝑝ሻൌ1
𝜇඲
଴
௣
𝐹െଵሺ𝑦ሻ𝑑𝑦
𝐿ሺ𝑝ሻൌ1
𝜇඲
଴
క೛
𝑥𝑓ሺ𝑥ሻ𝑑𝑥,
𝑦ൌ𝐹ሺ𝑥ሻ.
𝐿ሺ𝑝ሻ
඲
଴
ଵ
𝐿ሺ𝑝ሻ𝑑𝑝ൌ1
𝜇඲
଴
ஶ
ሺ1 െ𝐹ሺ𝑥ሻሻ𝑥𝑓ሺ𝑥ሻ𝑑𝑥
ሺ0,1ሻ
667 of 848

8.1. The number of automobiles sold weekly at a certain dealership
is a random variable with expected value 16. Give an upper bound to
the probability that
a. next week’s sales exceed 18;
b. next week’s sales exceed 25.
8.2. Suppose in Problem 8.14
 that the variance of the number of
automobiles sold weekly is 9.
a. Give a lower bound to the probability that next week’s sales
are between 10 and 22, inclusively.
b. Give an upper bound to the probability that next week’s sales
exceed 18.
8.3. If
give an upper bound to
a. 
b. 
c. 
8.4. Suppose that the number of units produced daily at factory  is a
random variable with mean 20 and standard deviation 3 and the
number produced at factory  is a random variable with mean 18 and
standard deviation 6. Assuming independence, derive an upper
bound for the probability that more units are produced today at
factory  than at factory 
8.5. The amount of time that a certain type of component functions
before failing is a random variable with probability density function
Once the component fails, it is immediately replaced by another one
of the same type. If we let 
 denote the lifetime of the th component
to be put in use, then 
 represents the time of the th
failure. The long-term rate at which failures occur, call it  is defined
by
𝐸ሾ𝑋ሿ
ൌ75
𝐸ሾ𝑌ሿൌ75 Varሺ𝑋ሻൌ10
Varሺ𝑌ሻ
ൌ12
Covሺ𝑋, 𝑌ሻൌെ3
𝑃ሼ||𝑋െ𝑌|| ൐15ሽ;
𝑃ሼ𝑋൐𝑌൅15ሽ;
𝑃ሼ𝑌൐𝑋൅15ሽ.
𝐴
𝐵
𝐵
𝐴.
𝑓ሺ𝑥ሻൌ2𝑥 0 ൏𝑥൏1
𝑋௜
𝑖
𝑆௡ൌ෍
௜ൌଵ
௡
𝑋௜
𝑛
𝑟,
668 of 848

Assuming that the random variables 
 are independent,
determine 
8.6. In Self-Test Problem 8.5
, how many components would one
need to have on hand to be approximately 90 percent certain that the
stock would last at least 35 days?
8.7. The servicing of a machine requires two separate steps, with the
time needed for the first step being an exponential random variable
with mean .2 hour and the time for the second step being an
independent exponential random variable with mean .3 hour. If a
repair person has 20 machines to service, approximate the
probability that all the work can be completed in 8 hours.
8.8. On each bet, a gambler loses 1 with probability .7, loses 2 with
probability .2, or wins 10 with probability .1. Approximate the
probability that the gambler will be losing after his first 100 bets.
8.9. Determine  so that the probability that the repair person in Self-
Test Problem 8.7
 finishes the 20 jobs within time  is
approximately equal to .95.
8.10. A tobacco company claims that the amount of nicotine in one of
its cigarettes is a random variable with mean 2.2 mg and standard
deviation .3 mg. However, the average nicotine content of 100
randomly chosen cigarettes was 3.1 mg. Approximate the probability
that the average would have been as high as or higher than 3.1 if the
company’s claims were true.
8.11. Each of the batteries in a collection of 40 batteries is equally
likely to be either a type A or a type B battery. Type A batteries last
for an amount of time that has mean 50 and standard deviation 15;
type B batteries last for an amount of time that has mean 30 and
standard deviation 6.
a. Approximate the probability that the total life of all 40 batteries
exceeds 1700.
b. Suppose it is known that 20 of the batteries are type A and 20
are type B. Now approximate the probability that the total life
of all 40 batteries exceeds 1700.
8.12. A clinic is equally likely to have 2, 3, or 4 doctors volunteer for
service on a given day. No matter how many volunteer doctors there
are on a given day, the numbers of patients seen by these doctors
are independent Poisson random variables with mean 30. Let 
denote the number of patients seen in the clinic on a given day.
𝑟ൌ
lim
௡→ஶ
𝑛
𝑆௡
𝑋௜, 𝑖൒1,
𝑟.
𝑡
𝑡
𝑋
669 of 848

9.1 The Poisson Process
9.2 Markov Chains
9.3 Surprise, Uncertainty, and Entropy
a. Find [ ].
b. Find Var
c. Use a table of the standard normal probability distribution to
approximate 
8.13. The strong law of large numbers states that with probability 
the successive arithmetic averages of a sequence of independent
and identically distributed random variables converge to their
common mean  What do the successive geometric averages
converge to? That is, what is
8.14. Each new book donated to a library must be processed.
Suppose that the time it takes to process a book has mean 10
minutes and standard deviation 3 minutes. If a librarian has 40 books
to process,
a. approximate the probability that it will take more than 420
minutes to process all these books;
b. approximate the probability that at least 25 books will be
processed in the first 240 minutes.
What assumptions have you made?
8.15. Prove Chebyshev’s sum inequality, which says that if
 and 
 then
𝐸𝑋
ሺ𝑋ሻ.
𝑃ሼ𝑋൐65ሽ.
1,
𝜇.
lim
௡→ஶቌෑ
௜ൌଵ
௡
𝑋௜ቍ
ଵ/௡
𝑎ଵ൒𝑎ଶ൒⋯൒𝑎௡
𝑏ଵ൒𝑏ଶ൒⋯൒𝑏௡,
𝑛෍
௜ൌଵ
௡
𝑎௜𝑏௜൒൬෍
௜ൌଵ
௡
𝑎௜൰൬෍
௜ൌଵ
௡
𝑏௜൰.
670 of 848

9.4 Coding Theory and Entropy
Before we define a Poisson process, let us recall that a function  is said to be 
 if
That is, 
is 
if, for small values of 
is small even in relation to  Suppose
now that “events” are occurring at random points at time, and let 
denote the
number of events that occur in the time interval 
 The collection of random
variables 
is said to be a Poisson process having rate
 if
i. 
ii. The numbers of events that occur in disjoint time intervals are independent.
iii. The distribution of the number of events that occur in a given interval depends
only on the length of that interval and not on its location.
iv. 
v. 
Thus, condition (i) states that the process begins at time 0. Condition (ii), the
independent increment assumption, states, for instance, that the number of events
that occur by time  [that is, 
] is independent of the number of events that occur
between  and 
 [that is, 
]. Condition (iii), the stationary increment
assumption, states that the probability distribution of 
 is the same for
all values of 
In Chapter 4
, we presented an argument, based on the Poisson distribution being
a limiting version of the binomial distribution, that the foregoing conditions imply that
 has a Poisson distribution with mean 
 We will now obtain this result by a
different method.
Lemma 1.1
For a Poisson process with rate 
Proof Let 
 We derive a differential equation for 
 in the
following manner:
𝑓
𝑜ሺℎሻ
lim
௛→଴
𝑓ሺℎሻ
ℎ
ൌ0.
𝑓
𝑜ሺℎሻ
ℎ, 𝑓ሺℎሻ
ℎ.
𝑁ሺ𝑡ሻ
ሾ0, 𝑡ሿ.
ሼ𝑁ሺ𝑡ሻ, 𝑡൒0ሽ
λ, λ ൐0,
𝑁ሺ0ሻൌ0.
𝑃ሼ𝑁ሺℎሻൌ1ሽൌλℎ൅𝑜ሺℎሻ.
𝑃ሼ𝑁ሺℎሻ൒2ሽൌ𝑜ሺℎሻ.
𝑡
𝑁ሺ𝑡ሻ
𝑡
𝑡൅𝑠
𝑁ሺ𝑡൅𝑠ሻെ𝑁ሺ𝑡ሻ
𝑁ሺ𝑡൅𝑠ሻെ𝑁ሺ𝑡ሻ
𝑡.
𝑁ሺ𝑡ሻ
λ𝑡.
𝜆,
𝑃൛𝑁ሺ𝑡ሻൌ0ൟൌ𝑒െఒ௧
𝑃଴ሺ𝑡ሻൌ𝑃ሼ𝑁ሺ𝑡ሻൌ0ሽ.
𝑃଴ሺ𝑡ሻ
671 of 848

where the final two equations follow from condition (ii) plus the fact that
conditions (iv) and (v) imply that 
 Hence,
Now, letting 
 we obtain
or, equivalently,
which implies, by integration, that
or
Since 
 we arrive at
For a Poisson process, let 
 denote the time the first event occurs. Further, for
 let 
 denote the time elapsed between the 
 and the th event. The
sequence 
 is called the sequence of interarrival times. For instance, if
 and 
 then the first event of the Poisson process would have occurred
at time 5 and the second at time 15.
We shall now determine the distribution of the 
 To do so, we first note that the
event 
 takes place if and only if no events of the Poisson process occur in
the interval 
 thus,
𝑃଴ሺ𝑡൅ℎሻ
ൌ𝑃ሼ𝑁ሺ𝑡൅ℎሻൌ0ሽ
ൌ𝑃ሼ𝑁ሺ𝑡ሻൌ0, 𝑁ሺ𝑡൅ℎሻെ𝑁ሺ𝑡ሻൌ0ሽ
ൌ𝑃ሼ𝑁ሺ𝑡ሻൌ0ሽ𝑃ሼ𝑁ሺ𝑡൅ℎሻെ𝑁ሺ𝑡ሻൌ0ሽ
ൌ𝑃଴ሺ𝑡ሻሾ1 െλℎ൅𝑜ሺℎሻሿ
𝑃ሼ𝑁ሺℎሻൌ0ሽൌ1 െλℎ൅𝑜ሺℎሻ.
𝑃଴ሺ𝑡൅ℎሻെ𝑃଴ሺ𝑡ሻ
ℎ
ൌെ𝜆𝑃଴ሺ𝑡ሻ൅𝑜ሺℎሻ
ℎ
ℎ→0,
𝑃ᇱ
଴ሺ𝑡ሻൌെλ𝑃଴ሺ𝑡ሻ
𝑃ᇱ
଴ሺ𝑡ሻ
𝑃଴ሺ𝑡ሻൌെλ
log 𝑃଴ሺ𝑡ሻൌെ𝜆𝑡൅𝑐
𝑃଴ሺ𝑡ሻൌ𝐾𝑒െλ௧
𝑃଴ሺ0ሻൌ𝑃ሼ𝑁ሺ0ሻൌ0ሽൌ1,
𝑃଴ሺ𝑡ሻൌ𝑒െλ௧
𝑇ଵ
𝑛൐1,
𝑇௡
ሺ𝑛െ1ሻ
𝑛
ሼ𝑇௡, 𝑛ൌ1, 2, ...ሽ
𝑇ଵൌ5
𝑇ଶൌ10,
𝑇௡.
ሼ𝑇ଵ൐𝑡ሽ
ሾ0, 𝑡ሿ;
𝑃൛𝑇ଵ൐𝑡ൟൌ𝑃൛𝑁ሺ𝑡ሻൌ0ൟൌ𝑒െλ௧
672 of 848

Hence, 
 has an exponential distribution with mean 
 Now,
However,
where the last two equations followed from the assumptions about independent and
stationary increments. From the preceding, we conclude that 
 is also an
exponential random variable with mean 
 and, furthermore, that 
 is independent
of 
 Repeating the same argument yields Proposition 1.1
.
Proposition 1.1
 are independent exponential random variables, each with mean 
Another quantity of interest is 
 the arrival time of the th event, also called the
waiting time until the th event. It is easily seen that
hence, from Proposition 1.1
 and the results of Section 5.6.1
, it follows
that 
 has a gamma distribution with parameters  and  That is, the probability
density of 
 is given by
We are now ready to prove that 
 is a Poisson random variable with mean 
Theorem 1.1
For a Poisson process with rate 
Proof Note that the th event of the Poisson process will occur before or at time
 if and only if the number of events that occur by  is at least  That is,
𝑇ଵ
1/λ .
𝑃ሼ𝑇ଶ൐𝑡ሽൌ𝐸ሾ𝑃ሼ𝑇ଶ൐𝑡||𝑇ଵሽሿ
𝑃ሼ𝑇ଶ൐𝑡||𝑇ଵൌ𝑠ሽ
ൌ
𝑃ሼ0 events in ሺ𝑠, 𝑠൅𝑡ሿ||𝑇ଵൌ𝑠ሽ
ൌ
𝑃ሼ0 events in ሺ𝑠, 𝑠൅𝑡ሿሽ
ൌ
𝑒െλ௧
𝑇ଶ
1/λ
𝑇ଶ
𝑇ଵ.
𝑇ଵ, 𝑇ଶ, ...
1/λ .
𝑆௡,
𝑛
𝑛
𝑆௡ൌ෍
௜ൌଵ
௡
𝑇௜ 𝑛൒1
𝑆௡
𝑛
λ.
𝑆௡
𝑓ௌ೙ሺ𝑥ሻൌλ𝑒െλ௫ሺλ𝑥ሻ௡െଵ
ሺ𝑛െ1ሻ!  𝑥൒0
𝑁ሺ𝑡ሻ
λ𝑡.
λ,
𝑃ሼ𝑁ሺ𝑡ሻൌ𝑛ሽൌ𝑒െλ௧ሺλ𝑡ሻ௡
𝑛!
𝑛
𝑡
𝑡
𝑛.
673 of 848

so
But the integration-by-parts formula 
 with 
 and
 yields
which completes the proof.
Consider a sequence of random variables 
 and suppose that the set of
possible values of these random variables is 
 It will be helpful to interpret
 as being the state of some system at time  and, in accordance with this
interpretation, we say that the system is in state  at time  if 
 The sequence
of random variables is said to form a Markov chain if, each time the system is in state
 there is some fixed probability—call it 
—that the system will next be in state 
That is, for all 
The values 
 are called the transition probabilities of the
Markov chain, and they satisfy
(Why?) It is convenient to arrange the transition probabilities 
 in a square array as
𝑁ሺ𝑡ሻ൒𝑛⇔𝑆௡൑𝑡
𝑃ሼ𝑁ሺ𝑡ሻൌ𝑛ሽ
ൌ
𝑃ሼ𝑁ሺ𝑡ሻ൒𝑛ሽെ𝑃ሼ𝑁ሺ𝑡ሻ൒𝑛൅1ሽ
ൌ
𝑃ሼ𝑆௡൑𝑡ሽെ𝑃ሼ𝑆௡൅ଵ൑𝑡ሽ
ൌ
଴
௧
λ𝑒െλ௫ሺλ௫ሻ೙െభ
ሺ௡െଵሻ! 𝑑𝑥െ
଴
௧
λ𝑒െλ௫ሺλ௫ሻ೙
௡! 𝑑𝑥
඲𝑢𝑑𝑣ൌ𝑢𝑣െ඲𝑣𝑑𝑢
𝑢ൌ𝑒െλ௫
𝑑𝑣ൌλሾሺλ𝑥ሻ௡െଵ/ሺ𝑛െ1ሻ!ሿ 𝑑𝑥
඲
଴
௧
λ𝑒െλ௫ሺλ𝑥ሻ௡െଵ
ሺ𝑛െ1ሻ! 𝑑𝑥ൌ𝑒െλ௧ሺλ𝑡ሻ௡
𝑛!
൅඲
଴
௧
λ𝑒െλ௫ሺλ𝑥ሻ௡
𝑛!
𝑑𝑥
𝑋଴, 𝑋ଵ, ... ,
ሼ0, 1, ... , 𝑀ሽ.
𝑋௡
𝑛,
𝑖
𝑛
𝑋௡ൌ𝑖.
𝑖,
𝑃௜௝
𝑗.
𝑖଴, ... , 𝑖௡െଵ, 𝑖, 𝑗,
𝑃൛𝑋௡൅ଵൌ𝑗ห𝑋௡ൌ𝑖, 𝑋௡െଵൌ𝑖௡െଵ, ... , 𝑋ଵൌ𝑖ଵ, 𝑋଴ൌ𝑖଴ൟൌ𝑃௜௝
𝑃௜௝, 0 ൑𝑖൑𝑀, 0 ൑𝑗൑𝑁,
𝑃௜௝൒0
෍
௝ൌ଴
ெ
𝑃௜௝ൌ1 𝑖ൌ0, 1, ... , 𝑀
𝑃௜௝
674 of 848

follows:
Such an array is called a matrix.
Knowledge of the transition probability matrix and of the distribution of 
 enables us,
in theory, to compute all probabilities of interest. For instance, the joint probability
mass function of 
 is given by
and continual repetition of this argument demonstrates that the preceding is equal to
Example 2a
Suppose that whether it rains tomorrow depends on previous weather conditions
only through whether it is raining today. Suppose further that if it is raining today,
then it will rain tomorrow with probability 
 and if it is not raining today, then it will
rain tomorrow with probability 
If we say that the system is in state 0 when it rains and state 1 when it does not,
then the preceding system is a two-state Markov chain having transition
probability matrix
That is, 
Example 2b
Consider a gambler who either wins 1 unit with probability  or loses 1 unit with
probability 
 at each play of the game. If we suppose that the gambler will
quit playing when his fortune hits either 0 or 
 then the gambler’s sequence of
fortunes is a Markov chain having transition probabilities
ቱ
ቱ
𝑃଴଴
𝑃଴ଵ
⋯
𝑃଴ெ
𝑃ଵ଴
𝑃ଵଵ
⋯
𝑃ଵெ
⋮
𝑃ெ଴
𝑃ெଵ
⋯
𝑃ெெ
ቱ
ቱ
𝑋଴
𝑋଴, ... , 𝑋௡
𝑃ሼ𝑋௡
ൌ𝑖௡, 𝑋௡െଵൌ𝑖௡െଵ, ..., 𝑋ଵൌ𝑖ଵ, 𝑋଴ൌ𝑖଴ሽ
ൌ𝑃ሼ𝑋௡ൌ𝑖௡||𝑋௡െଵൌ𝑖௡െଵ, ..., 𝑋଴ൌ𝑖଴ሽ𝑃ሼ𝑋௡െଵൌ𝑖௡െଵ, ..., 𝑋଴ൌ𝑖଴ሽ
ൌ𝑃௜೙െభ, ௜೙𝑃ሼ𝑋௡െଵൌ𝑖௡െଵ, ..., 𝑋଴ൌ𝑖଴ሽ
𝑃௜೙െభ, ௜೙𝑃௜೙െమ, ௜೙െభ⋯𝑃௜భ, ௜మ𝑃௜బ, ௜భ𝑃൛𝑋଴ൌ𝑖଴ൟ
𝛼,
𝛽.
ብ𝛼
1 െ𝛼
𝛽
1 െ𝛽ብ
𝑃଴଴ൌ𝛼ൌ1 െ𝑃଴ଵ, 𝑃ଵ଴ൌ𝛽ൌ1 െ𝑃ଵଵ.
𝑝
1 െ𝑝
𝑀,
675 of 848

Example 2c
The husband-and-wife physicists Paul and Tatyana Ehrenfest considered a
conceptual model for the movement of molecules in which 
 molecules are
distributed among 2 urns. At each time point, one of the molecules is chosen at
random and is removed from its urn and placed in the other one. If we let 
denote the number of molecules in the first urn immediately after the th
exchange, then 
 is a Markov chain with transition probabilities
Thus, for a Markov chain, 
 represents the probability that a system in state  will
enter state  at the next transition. We can also define the two-stage transition
probability 
 that a system presently in state  will be in state  after two additional
transitions. That is,
The 
can be computed from the 
 as follows:
In general, we define the -stage transition probabilities, denoted as 
 by
Proposition 2.1
, known as the Chapman–Kolmogorov equations, shows how the
𝑃௜, ௜൅ଵ
ൌ
𝑝ൌ1 െ𝑃௜, ௜െଵ
𝑖ൌ1, ..., 𝑀െ1
𝑃଴଴
ൌ
𝑃ெெൌ1
𝑀
𝑋௡
𝑛
ሼ𝑋଴, 𝑋ଵ, ...ሽ
𝑃௜, ௜൅ଵ
ൌ𝑀െ𝑖
𝑀
 0 ൑𝑖൑𝑀
𝑃௜, ௜െଵ
ൌ𝑖
𝑀 0 ൑𝑖൑𝑀
𝑃௜௝
ൌ0 if 𝑗ൌ𝑖 or | 𝑗െ𝑖| ൐1
𝑃௜௝
𝑖
𝑗
𝑃௜௝
ሺଶሻ
𝑖
𝑗
𝑃௜௝
ሺଶሻൌ𝑃ቄ𝑋௠൅ଶൌ𝑗||𝑋௠ൌ𝑖ቅ
𝑃௜௝
ሺଶሻ
𝑃௜௝
𝑃௜௝
ሺଶሻ
ൌ𝑃ሼ𝑋ଶൌ𝑗||𝑋଴ൌ𝑖ሽ
ൌ
෍
௞ൌ଴
ெ
𝑃ሼ𝑋ଶൌ𝑗, 𝑋ଵൌ𝑘||𝑋଴ൌ𝑖ሽ
ൌ
෍
௞ൌ଴
ெ
𝑃ሼ𝑋ଶൌ𝑗||𝑋ଵൌ𝑘, 𝑋଴ൌ𝑖ሽ𝑃ሼ𝑋ଵൌ𝑘||𝑋଴ൌ𝑖ሽ
ൌ
෍
௞ൌ଴
ெ
𝑃௞௝𝑃௜௞
𝑛
𝑃௜௝
ሺ௡ሻ,
𝑃௜௝
ሺ௡ሻൌ𝑃ሼ𝑋௡൅௠ൌ𝑗||𝑋௠ൌ𝑖ሽ
676 of 848

 can be computed.
Proposition 2.1 The Chapman–Kolmogorov Equations
Proof
Example 2d A random walk
An example of a Markov chain having a countably infinite state space is the
random walk, which tracks a particle as it moves along a one-dimensional axis.
Suppose that at each point in time, the particle will move either one step to the
right or one step to the left with respective probabilities  and 
 That is,
suppose the particle’s path follows a Markov chain with transition probabilities
If the particle is at state  then the probability that it will be at state  after 
transitions is the probability that 
 of these steps are to the right and
 are to the left. Since each step will be to the
right, independently of the other steps, with probability  it follows that the
preceding is just the binomial probability
where 
 is taken to equal 0 when  is not a nonnegative integer less than or
equal to  The preceding formula can be rewritten as
𝑃௜௝
ሺ௡ሻ
𝑃௜௝
ሺ௡ሻൌ
෍
௞ൌ଴
ெ
𝑃௜௞
ሺ௥ሻ𝑃௞௝
ሺ௡െ௥ሻfor all 0 ൏𝑟൏𝑛
𝑃௜௝
ሺ௡ሻ
ൌ𝑃ሼ𝑋௡ൌ𝑗||𝑋଴ൌ𝑖ሽ
ൌ෍
௞
𝑃ሼ𝑋௡ൌ𝑗, 𝑋௥ൌ𝑘||𝑋଴ൌ𝑖ሽ
ൌ෍
௞
𝑃ሼ𝑋௡ൌ𝑗||𝑋௥ൌ𝑘, 𝑋଴ൌ𝑖ሽ𝑃ሼ𝑋௥ൌ𝑘||𝑋଴ൌ𝑖ሽ
ൌ෍
௞
𝑃௞௝
ሺ௡െ௥ሻ𝑃௜௞
ሺ௥ሻ
𝑝
1 െ𝑝.
𝑃௜, ௜൅ଵൌ𝑝ൌ1 െ𝑃௜, ௜െଵ 𝑖ൌ0, േ1, ...
𝑖,
𝑗
𝑛
ሺ𝑛െ𝑖൅𝑗ሻ/2
𝑛െሾሺ𝑛െ𝑖൅𝑗ሻ/2ሿൌሺ𝑛൅𝑖െ𝑗ሻ/2
𝑝,
𝑃௜௝
௡ൌ൭
𝑛
ሺ𝑛െ𝑖൅𝑗ሻ/2൱𝑝ሺ௡െ௜൅௝ሻ/ଶሺ1 െ𝑝ሻሺ௡൅௜െ௝ሻ/ଶ
ቆ𝑛
𝑥ቇ
𝑥
𝑛.
677 of 848

Although the 
 denote conditional probabilities, we can use them to derive
expressions for unconditional probabilities by conditioning on the initial state. For
instance,
For a large number of Markov chains, it turns out that 
 converges, as 
 to a
value 
 that depends only on  That is, for large values of  the probability of being
in state  after  transitions is approximately equal to 
 no matter what the initial
state was. It can be shown that a sufficient condition for a Markov chain to possess
this property is that for some 
Markov chains that satisfy Equation (2.1)
 are said to be ergodic. Since
Proposition 2.1
 yields
it follows, by letting 
 that for ergodic chains,
Furthermore, since 
 we also obtain, by letting 
𝑃௜, ௜൅ଶ௞
ଶ௡
ൌቆ2𝑛
𝑛൅𝑘ቇ𝑝௡൅௞ሺ1 െ𝑝ሻ௡െ௞ 𝑘ൌ0, േ1, ... , േ𝑛
𝑃௜, ௜൅ଶ௞൅ଵ
ଶ௡൅ଵ
ൌቆ2𝑛൅1
𝑛൅𝑘൅1ቇ𝑝௡൅௞൅ଵሺ1 െ𝑝ሻ௡െ௞
𝑘ൌ0, േ1, ... , േ𝑛, െሺ𝑛൅1ሻ   
𝑃௜௝
ሺ௡ሻ
𝑃ሼ𝑋௡ൌ𝑗ሽ
ൌ෍
௜
𝑃ሼ𝑋௡ൌ𝑗||𝑋଴ൌ𝑖ሽ𝑃ሼ𝑋଴ൌ𝑖ሽ
ൌ෍
௜
𝑃௜௝
ሺ௡ሻ𝑃ሼ𝑋଴ൌ𝑖ሽ
𝑃௜௝
ሺ௡ሻ
𝑛→∞,
𝜋௝
𝑗.
𝑛,
𝑗
𝑛
𝜋௝,
𝑛൐0,
𝑃௜௝
ሺ௡ሻ൐0 for all 𝑖, 𝑗ൌ0, 1, ... , 𝑀
(2.1)
𝑃௜௝
ሺ௡൅ଵሻൌ
෍
௞ൌ଴
ெ
𝑃௜௞
ሺ௡ሻ𝑃௞௝
𝑛→∞,
𝜋௝ൌ
෍
௞ൌ଴
ெ
𝜋௞𝑃௞௝
(2.2)
1 ൌ
෍
௝ൌ଴
ெ
𝑃௜௝
ሺ௡ሻ,
𝑛→∞,
678 of 848

In fact, it can be shown that the 
 are the unique nonnegative solutions
of Equations (2.2)
 and (2.3
). All this is summed up in Theorem 2.1
, which
we state without proof.
Theorem 2.1
For an ergodic Markov chain,
exists, and the 
 are the unique nonnegative solutions of
Example 2e
Consider Example 2a
, in which we assume that if it rains today, then it will
rain tomorrow with probability 
 and if it does not rain today, then it will rain
tomorrow with probability 
 From Theorem 2.1
, it follows that the limiting
probabilities 
 and 
 of rain and of no rain, respectively, are given by
which yields
For instance, if 
 and 
 then the limiting probability of rain on the th
day is 
The quantity 
 is also equal to the long-run proportion of time that the Markov chain
෍
௝ൌ଴
ெ
𝜋௝ൌ1
(2.3)
𝜋௝, 0 ൑𝑗൑𝑀,
𝜋௝ൌ
lim
௡→ஶ𝑃௜௝
ሺ௡ሻ
𝜋௝, 0 ൑𝑗൑𝑀,
𝜋௝
ൌ
෍
௞ൌ଴
ெ
𝜋௞𝑃௞௝
෍
௝ൌ଴
ெ
𝜋௝
ൌ1
𝛼,
𝛽.
𝜋଴
𝜋ଵ
𝜋଴
ൌ𝛼𝜋଴൅𝛽𝜋ଵ
𝜋ଵ
ൌሺ1 െ𝛼ሻ𝜋଴൅ሺ1 െ𝛽ሻ𝜋ଵ
𝜋଴൅𝜋ଵ
ൌ1
𝜋଴ൌ
𝛽
1 ൅𝛽െ𝛼 𝜋ଵൌ
1 െ𝛼
1 ൅𝛽െ𝛼
𝛼ൌ.6
𝛽ൌ.3,
𝑛
𝜋଴ൌ3
7 .
𝜋௝
679 of 848

is in state 
 To see intuitively why this might be so, let 
 denote the
long-run proportion of time the chain is in state  (It can be proven using the strong
law of large numbers that for an ergodic chain, such long-run proportions exist and
are constants.) Now, since the proportion of time the chain is in state  is 
 and
since, when in state  the chain goes to state  with probability 
 it follows that the
proportion of time the Markov chain is entering state  from state  is equal to 
Summing over all  shows that 
 the proportion of time the Markov chain is
entering state  satisfies
Since clearly it is also true that
it thus follows, since by Theorem 2.1
 the 
 are the unique solution of
the preceding, that 
 The long-run proportion interpretation of 
is generally valid even when the chain is not ergodic.
Example 2f
Suppose in Example 2c
 that we are interested in the proportion of time that
there are  molecules in urn 1, 
 By Theorem 2.1
, these quantities
will be the unique solution of
However, as it is easily checked that
satisfy the preceding equations, it follows that these are the long-run proportions
𝑗, 𝑗ൌ0, ... , 𝑀.
𝑃௝
𝑗.
𝑘
𝑃௞,
𝑘,
𝑗
𝑃௞௝,
𝑗
𝑘
𝑃௞𝑃௞௝.
𝑘
𝑃௝,
𝑗,
𝑃௝ൌ෍
௞
𝑃௞𝑃௞௝
෍
௝
𝑃௝ൌ1
𝜋௝, 𝑗ൌ0, ... , 𝑀
𝑃௝ൌ𝜋௝, 𝑗ൌ0, ... , 𝑀.
𝜋௝
𝑗
𝑗ൌ0, ... , 𝑀.
𝜋଴
ൌ𝜋ଵൈ1
𝑀
𝜋௝
ൌ𝜋௝െଵൈ𝑀െ𝑗൅1
𝑀
൅𝜋௝൅ଵൈ𝑗൅1
𝑀
 𝑗ൌ1, ... , 𝑀
𝜋ெ
ൌ𝜋ெെଵൈ1
𝑀
෍
௝ൌ଴
ெ
𝜋௝
ൌ1
𝜋௝ൌቆ𝑀
𝑗ቇቆ1
2ቇ
ெ
 𝑗ൌ0, ... , 𝑀
680 of 848

of time that the Markov chain is in each of the states. (See Problem 9.11
 for
an explanation of how one might have guessed at the foregoing solution.)
Consider an event E that can occur when an experiment is performed. How
surprised would we be to hear that  does, in fact, occur? It seems reasonable to
suppose that the amount of surprise engendered by the information that  has
occurred should depend on the probability of 
 For instance, if the experiment
consists of rolling a pair of dice, then we would not be too surprised to hear that 
has occurred when  represents the event that the sum of the dice is even (and thus
has probability ), whereas we would certainly be more surprised to hear that  has
occurred when  is the event that the sum of the dice is 12 (and thus has probability
).
In this section, we attempt to quantify the concept of surprise. To begin, let us agree
to suppose that the surprise one feels upon learning that an event  has occurred
depends only on the probability of 
 and let us denote by 
 the surprise evoked
by the occurrence of an event having probability  We determine the functional form
of 
 by first agreeing on a set of reasonable conditions that 
 should satisfy
and then proving that these axioms require that 
 have a specified form. We
assume throughout that 
 is defined for all 
 but is not defined for events
having 
Our first condition is just a statement of the intuitive fact that there is no surprise in
hearing that an event that is sure to occur has indeed occurred.
Axiom 1
Our second condition states that the more unlikely an event is to occur, the greater is
the surprise evoked by its occurrence.
Axiom 2
 is a strictly decreasing function of 
 that is, if 
 then 
The third condition is a mathematical statement of the fact that we would intuitively
expect a small change in  to correspond to a small change in 
Axiom 3
𝐸
𝐸
𝐸.
𝐸
𝐸
1
2
𝐸
𝐸
1
36
𝐸
𝐸,
𝑆ሺ𝑝ሻ
𝑝.
𝑆ሺ𝑝ሻ
𝑆ሺ𝑝ሻ
𝑆ሺ𝑝ሻ
𝑆ሺ𝑝ሻ
0 ൏𝑝൑1
𝑝ൌ0 .
𝑆ሺ1ሻൌ0
𝑆ሺ𝑝ሻ
𝑝;
𝑝൏𝑞,
𝑆ሺ𝑝ሻ൐𝑆ሺ𝑞ሻ.
𝑝
𝑆ሺ𝑝ሻ.
681 of 848

 is a continuous function of 
To motivate the final condition, consider two independent events  and  having
respective probabilities 
 and 
 Since 
 the surprise
evoked by the information that both  and  have occurred is (pq). Now, suppose
that we are told first that  has occurred and then, afterward, that  has also
occurred. Since 
 is the surprise evoked by the occurrence of 
 it follows that
 represents the additional surprise evoked when we are informed that 
has also occurred. However, because  is independent of 
 the knowledge that 
occurred does not change the probability of 
 hence, the additional surprise should
just be 
 This reasoning suggests the final condition.
Axiom 4
We are now ready for Theorem 3.1
, which yields the structure of 
Theorem 3.1
If 
 satisfies Axioms 1
 through 4
, then
where  is an arbitrary positive integer.
Proof It follows from Axiom 4
 that
and by induction that
Also, since, for any integral 
 it follows that
Thus, from Equations (3.1)
 and (3.2)
, we obtain
𝑆ሺ𝑝ሻ
𝑝.
𝐸
𝐹
𝑃ሺ𝐸ሻൌ𝑝
𝑃ሺ𝐹ሻൌ𝑞.
𝑃ሺ𝐸𝐹ሻൌ𝑝𝑞,
𝐸
𝐹
𝑆
𝐸
𝐹
𝑆ሺ𝑝ሻ
𝐸,
𝑆ሺ𝑝𝑞ሻെ𝑆ሺ𝑝ሻ
𝐹
𝐹
𝐸,
𝐸
𝐹;
𝑆ሺ𝑞ሻ.
𝑆ሺ𝑝𝑞ሻൌ𝑆ሺ𝑝ሻ൅𝑆ሺ𝑞ሻ  0 ൏𝑝൑1, 0 ൏𝑞൑1
𝑆ሺ𝑝ሻ.
𝑆ሺ⋅ሻ
𝑆ሺ𝑝ሻൌെ𝐶logଶ𝑝
𝐶
𝑆ሺ𝑝ଶሻൌ𝑆ሺ𝑝ሻ൅𝑆ሺ𝑝ሻൌ2𝑆ሺ𝑝ሻ
𝑆ሺ𝑝௠ሻൌ𝑚𝑆ሺ𝑝ሻ
(3.1)
𝑛, 𝑆ሺ𝑝ሻൌ𝑆ሺ𝑝ଵ/௡⋯𝑝ଵ/௡ሻൌ𝑛 𝑆ሺ𝑝ଵ/௡ሻ,
𝑆ሺ𝑝ଵ/௡ሻൌ1
𝑛𝑆ሺ𝑝ሻ
(3.2)
682 of 848

which is equivalent to
whenever  is a positive rational number. But by the continuity of (Axiom 3
),
it follows that Equation (3.3)
 is valid for all nonnegative  (Reason this out.)
Now, for any 
 let 
 Then 
 and from Equation
(3.3)
,
where 
 by Axioms 2
 and 1
.
It is usual to let  equal 1, in which case the surprise is said to be expressed in units
of bits (short for binary digits).
Next, consider a random variable  that must take on one of the values 
 with
respective probabilities 
 Since 
 represents the surprise evoked if 
takes on the value 
 it follows that the expected amount of surprise we shall
receive upon learning the value of  is given by
For the remainder of this chapter, we write log  for 
 Also, we
use ln  for 
The quantity 
 is known in information theory as the entropy of the random
variable 
 (In case one of the 
 we take 0 log 0 to equal 0.) It can be shown
(and we leave it as an exercise) that 
 is maximized when all of the 
 are equal.
(Is this intuitive?)
𝑆ሺ𝑝௠/௡ሻ
ൌ𝑚𝑆ሺ𝑝ଵ/௡ሻ
ൌ𝑚
𝑛𝑆ሺ𝑝ሻ
𝑆ሺ𝑝௫ሻൌ𝑥𝑆ሺ𝑝ሻ
(3.3)
𝑥
𝑆
𝑥.
𝑝, 0 ൏𝑝൑1,
𝑥ൌെlogଶ𝑝.
𝑝ൌቀ1
2ቁ
௫
,
𝑆ሺ𝑝ሻൌ𝑆൬ቀ1
2ቁ
௫
൰ൌ𝑥𝑆ቀ1
2ቁൌെ𝐶logଶ𝑝
𝐶ൌ𝑆ቀ1
2ቁ൐𝑆ሺ1ሻൌ0
𝐶
𝑋
𝑥ଵ, ... , 𝑥௡
𝑝ଵ, ... , 𝑝௡.
െlog 𝑝௜
𝑋
𝑥௜, †
𝑋
𝐻ሺ𝑋ሻൌെ෍
௜ൌଵ
௡
𝑝௜log 𝑝௜
†
𝑥
logଶ𝑥.
𝑥
log௘𝑥.
𝐻ሺ𝑋ሻ
𝑋.
𝑝௜ൌ0,
𝐻ሺ𝑋ሻ
𝑝௜
683 of 848

Since 
 represents the average amount of surprise one receives upon learning
the value of 
 it can also be interpreted as representing the amount of uncertainty
that exists as to the value of 
 In fact, in information theory, 
 is interpreted as
the average amount of information received when the value of  is observed. Thus,
the average surprise evoked by 
 the uncertainty of 
 or the average amount of
information yielded by  all represent the same concept viewed from three slightly
different points of view.
Now consider two random variables  and  that take on the respective values
 and 
 with joint mass function
It follows that the uncertainty as to the value of the random vector (X, Y), denoted by
 is given by
Suppose now that  is observed to equal 
 In this situation, the amount of
uncertainty remaining in  is given by
where
Hence, the average amount of uncertainty that will remain in  after  is observed is
given by
where
Proposition 3.1
 relates 
 to 
 and 
 It states that the uncertainty
as to the value of  and  is equal to the uncertainty of  plus the average
uncertainty remaining in  when  is to be observed.
𝐻ሺ𝑋ሻ
𝑋,
𝑋.
𝐻ሺ𝑋ሻ
𝑋
𝑋,
𝑋,
𝑋
𝑋
𝑌
𝑥ଵ, ... , 𝑥௡
𝑦ଵ, ... , 𝑦௠
𝑝ሺ𝑥௜, 𝑦௝ሻൌ𝑃ቄ𝑋ൌ𝑥௜, 𝑌ൌ𝑦௝ቅ
𝐻ሺ𝑋, 𝑌ሻ,
𝐻ሺ𝑋, 𝑌ሻൌെ෍
௜
෍
௝
𝑝ሺ𝑥௜, 𝑦௝ሻ log 𝑝ሺ𝑥௜, 𝑦௝ሻ
𝑌
𝑦௝.
𝑋
𝐻௒ൌ௬ೕሺ𝑋ሻൌെ෍
௜
𝑝ሺ𝑥௜|𝑦௝ሻ log 𝑝ሺ𝑥௜|𝑦௝ሻ
𝑝ሺ𝑥௜|𝑦௝ሻൌ𝑃ሼ𝑋ൌ𝑥௜|𝑌ൌ𝑦௝ሽ
𝑋
𝑌
𝐻௒ሺ𝑋ሻൌ෍
௝
𝐻௒ൌ௬ೕሺ𝑋ሻ𝑝௒ሺ𝑦௝ሻ
𝑝௒ሺ𝑦௝ሻൌ𝑃ቄ𝑌ൌ𝑦௝ቅ
𝐻ሺ𝑋, 𝑌ሻ
𝐻ሺ𝑌ሻ
𝐻௒ሺ𝑋ሻ.
𝑋
𝑌
𝑌
𝑋
𝑌
684 of 848

Proposition 3.1
Proof Using the identity 
yields
It is a fundamental result in information theory that the amount of uncertainty in a
random variable  will, on the average, decrease when a second random variable 
is observed. Before proving this statement, we need the following lemma, whose
proof is left as an exercise.
Lemma 3.1
with equality only at 
Theorem 3.2
with equality if and only if  and  are independent.
Proof
𝐻ሺ𝑋, 𝑌ሻൌ𝐻ሺ𝑌ሻ൅𝐻௒ሺ𝑋ሻ
𝑝ሺ𝑥௜, 𝑦௝ሻൌ𝑝௒ሺ𝑦௝ሻ𝑝ሺ𝑥௜|𝑦௝ሻ
𝐻ሺ𝑋, 𝑌ሻ
ൌെ෍
௜
෍
௝
𝑝ሺ𝑥௜, 𝑦௝ሻlog 𝑝ሺ𝑥௜, 𝑦௝ሻ
ൌെ෍
௜
෍
௝
𝑝௒ሺ𝑦௝ሻ𝑝ሺ𝑥௜|𝑦௝ሻሾlog 𝑝௒ሺ𝑦௝ሻ൅log 𝑝ሺ𝑥௜|𝑦௝ሻሿ
ൌെ෍
௝
𝑝௒ሺ𝑦௝ሻlog 𝑝௒ሺ𝑦௝ሻ෍
௜
𝑝ሺ𝑥௜|𝑦௝ሻ
      െ෍
௝
𝑝௒ሺ𝑦௝ሻ෍
௜
𝑝ሺ𝑥௜|𝑦௝ሻ log 𝑝ሺ𝑥୧|𝑦୨ሻ
ൌ 𝐻ሺ𝑌ሻ൅𝐻௒ሺ𝑋ሻ
𝑋
𝑌
ln 𝑥൑𝑥െ1 𝑥൐0
𝑥ൌ1 .
𝐻௒ሺ𝑋ሻ൑𝐻ሺ𝑋ሻ
𝑋
𝑌
685 of 848

Suppose that the value of a discrete random vector  is to be observed at location 
and then transmitted to location  via a communication network that consists of two
signals, 0 and 1. In order to do this, it is first necessary to encode each possible
value of  in terms of a sequence of 0’s and 1’s. To avoid any ambiguity, it is usually
required that no encoded sequence can be obtained from a shorter encoded
sequence by adding more terms to the shorter.
For instance, if  can take on four possible values 
 and 
 then one
possible coding would be
That is, if 
 then the message 00 is sent to location 
 whereas if 
 then
01 is sent to 
 and so on. A second possible coding is
𝐻௒ሺ𝑋ሻെ𝐻ሺ𝑋ሻ
ൌ෍
௜
෍
௝
𝑝ሺ𝑥௜|𝑦௝ሻ logሾ𝑝ሺ𝑥௜|𝑦௝ሻሿ𝑝ሺ𝑦௜ሻ൅෍
௜
෍
௝
𝑝ሺ𝑥௜,  𝑦௝ሻ log 𝑝ሺ𝑥௜ሻ
ൌ෍
௜
෍
௝
𝑝ሺ𝑥௜|𝑦௝ሻ logቈ𝑝ሺ𝑥௜ሻ
𝑝ሺ𝑥௜|𝑦௜ሻ቉
൑log 𝑒෍
௜
෍
௝
𝑝ሺ𝑥௜, 𝑦௝ሻ൤
௣ሺ௫೔ሻ
௣ሺ௫೔|௬೔ሻെ1൨  by Lemma 3.1
ൌlog 𝑒 ቎෍
௜
෍
௝
𝑝ሺ𝑥௜ሻ𝑝ሺ𝑦௝ሻെ෍
௜
෍
௝
𝑝ሺ𝑥௜, 𝑦௝ሻ቏
ൌlog 𝑒ሾ1 െ1ሿ
ൌ0
𝑋
𝐴
𝐵
𝑋
𝑋
𝑥ଵ, 𝑥ଶ, 𝑥ଷ,
𝑥ସ,
𝑥ଵ
↔00
𝑥ଶ
↔01
𝑥ଷ
↔10
𝑥ସ
↔11
(4.1)
𝑋ൌ𝑥ଵ,
𝐵,
𝑋ൌ𝑥ଶ,
𝐵,
𝑥ଵ
↔0
𝑥ଶ
↔10
𝑥ଷ
↔110
𝑥ସ
↔111
(4.2)
686 of 848

However, a coding such as
is not allowed because the coded sequences for 
 and 
 are both extensions of the
one for 
One of the objectives in devising a code is to minimize the expected number of bits
(that is, binary digits) that need to be sent from location  to location 
 For example,
if
then the code given by Equation (4.2)
 would expect to send
 bits, whereas the code given by Equation (4.1)
would expect to send 2 bits. Hence, for the preceding set of probabilities, the
encoding in Equation (4.2)
 is more efficient than that in Equation (4.1)
.
The preceding discussion raises the following question: For a given random vector 
what is the maximum efficiency achievable by an encoding scheme? The answer is
that for any coding, the average number of bits that will be sent is at least as large as
the entropy of 
 To prove this result, known in information theory as the noiseless
coding theorem, we shall need Lemma 4.1
.
Lemma 4.1
Let  take on the possible values 
 Then, in order to be able to encode
the values of  in binary sequences (none of which is an extension of another) of
respective lengths 
 it is necessary and sufficient that
𝑥ଵ
↔0
𝑥ଶ
↔1
𝑥ଷ
↔00
𝑥ସ
↔01
𝑥ଷ
𝑥ସ
𝑥ଵ.
𝐴
𝐵.
𝑃ሼ𝑋ൌ𝑥ଵሽൌ1
2
𝑃ሼ𝑋ൌ𝑥ଶሽൌ1
4
𝑃ሼ𝑋ൌ𝑥ଷሽൌ1
8
𝑃ሼ𝑋ൌ𝑥ସሽൌ1
8
1
2ሺ1ሻ൅1
4ሺ2ሻ൅1
8ሺ3ሻ൅1
8ሺ3ሻൌ1.75
𝑋,
𝑋.
𝑋
𝑥ଵ, ... , 𝑥ே.
𝑋
𝑛ଵ, ... , 𝑛ே,
෍
௜ൌଵ
ே
ቀ1
2ቁ
௡೔൑1
687 of 848

Proof For a fixed set of  positive integers 
 let 
 denote the number of
the 
 that are equal to 
 For there to be a coding that assigns 
 bits to
the value 
 it is clearly necessary that 
 Furthermore,
because no binary sequence is allowed to be an extension of any other, we must
have 
 (This follows because 
 is the number of binary
sequences of length 2, whereas 
 is the number of sequences that are
extensions of the 
 binary sequence of length 1.) In general, the same
reasoning shows that we must have
for 
 In fact, a little thought should convince the reader that these
conditions are not only necessary, but also sufficient for a code to exist that
assigns 
 bits to 
Rewriting inequality (4.3) as
and dividing by 
 yields the necessary and sufficient conditions, namely,
However, because 
 is increasing in  it follows that Equation
(4.4)
 will be true if and only if
The result is now established, since, by the definition of 
 as the number of 
that equal j, it follows that
We are now ready to prove Theorem 4.1
.
𝑁
𝑛ଵ, ... , 𝑛ே,
𝑤௝
𝑛௜
𝑗, 𝑗ൌ1, ... .
𝑛௜
𝑥௜, 𝑖ൌ1, ... , 𝑁,
𝑤ଵ൑2 .
𝑤ଶ൑2ଶെ2𝑤ଵ.
2ଶ
2𝑤ଵ
𝑤ଵ
𝑤௡൑2௡െ𝑤ଵ2௡െଵെ𝑤ଶ2௡െଶെ⋯െ𝑤௡െଵ2
(4.3)
𝑛ൌ1, ... .
𝑛௜
𝑥௜, 𝑖ൌ1, ... , 𝑁.
𝑤௡൅𝑤௡െଵ2 ൅𝑤௡െଶ2ଶ൅⋯൅𝑤ଵ2௡െଵ൑2௡ 𝑛ൌ1, ...
2௡
෍
௝ൌଵ
௡
𝑤௝ቀ1
2ቁ
௝
൑1  for all 𝑛
(4.4)
෍
௝ൌଵ
௡
𝑤௝ቀ1
2ቁ
௝
𝑛,
෍
௝ൌଵ
ஶ
𝑤௝ቀ1
2ቁ
௝
൑1
𝑤௝
𝑛௜
෍
௝ൌଵ
ஶ
𝑤௝ቀ1
2ቁ
௝
ൌ෍
௜ൌଵ
ே
ቀ1
2ቁ
௡೔
688 of 848

Theorem 4.1 The noiseless coding theorem
Let  take on the values 
 with respective probabilities 
Then, for any coding of  that assigns 
 bits to 
Proof Let 
 Then
Hence,
Example 4a
Consider a random variable  with probability mass function
Since
𝑋
𝑥ଵ, ... , 𝑥ே
𝑝ሺ𝑥ଵሻ, ... , 𝑝ሺ𝑥ேሻ.
𝑋
𝑛௜
𝑥௜,
෍
௜ൌଵ
ே
𝑛௜𝑝ሺ𝑥௜ሻ൒𝐻ሺ𝑋ሻൌെ෍
௜ൌଵ
ே
𝑝ሺ𝑥௜ሻ log  𝑝ሺ𝑥௜ሻ
𝑃௜ൌ𝑝ሺ𝑥௜ሻ, 𝑞௜ൌ2െ௡೔
෍
௝ൌଵ
ே
2െ௡ೕ, 𝑖ൌ1, ... , 𝑁.
െ෍
௜ൌଵ
ே
𝑃௜logቆ𝑃௜
𝑞௜
ቇ
ൌെlog 𝑒  ෍
௜ൌଵ
ே
𝑃௜lnቆ𝑃௜
𝑞௜
ቇ
ൌlog 𝑒  ෍
௜ൌଵ
ே
𝑃௜lnቆ
𝑞௜
𝑃௜
ቇ
൑log 𝑒෍
௜ൌଵ
ே
𝑃௜ቆ
𝑞௜
𝑃௜
െ1ቇ by Lemma 3.1
ൌ0 since  ෍
௜ൌଵ
ே
𝑃௜ൌ෍
௜ൌଵ
ே
𝑞௜ൌ1
െ෍
௜ൌଵ
ே
𝑃௜log 𝑃௜
൑െ  ෍
௜ൌଵ
ே
𝑃௜log 𝑞௜
ൌ  ෍
௜ൌଵ
ே
𝑛௜𝑃௜൅ logቌ෍
௝ൌଵ
ே
2െ௡ೕቍ
൑෍
௜ൌଵ
ே
𝑛௜𝑃௜ by Lemma 4.1
𝑋
𝑝ሺ𝑥ଵሻൌ1
2  𝑝ሺ𝑥ଶሻൌ1
4  𝑝ሺ𝑥ଷሻൌ𝑝ሺ𝑥ସሻൌ1
8
689 of 848

it follows from Theorem 4.1
 that there is no more efficient coding scheme
than
For most random vectors, there does not exist a coding for which the average
number of bits sent attains the lower bound 
 However, it is always possible to
devise a code such that the average number of bits is within 1 of 
 To prove
this, define 
 to be the integer satisfying
Now,
so, by Lemma 4.1
, we can associate sequences of bits having lengths 
 with the
 The average length of such a sequence,
satisfies
or
𝐻ሺ𝑋ሻ
ൌെ⎡
⎣
1
2 log 1
2 ൅1
4 log 1
4 ൅1
4 log 1
8
⎤
⎦
ൌ1
2 ൅2
4 ൅3
4
ൌ1.75
𝑥ଵ↔0
𝑥ଶ↔10
𝑥ଷ↔110
𝑥ସ↔111
𝐻ሺ𝑋ሻ.
𝐻ሺ𝑋ሻ.
𝑛௜
െlog 𝑝ሺ𝑥௜ሻ൑𝑛௜൏െlog 𝑝ሺ𝑥௜ሻ൅1
෍
௜ൌଵ
ே
2െ௡೔൑෍
௜ൌଵ
ே
2୪୭୥௣ሺ௫೔ሻൌ෍
௜ൌଵ
ே
𝑝ሺ𝑥௜ሻൌ1
𝑛௜
𝑥௜, 𝑖ൌ1, ... , 𝑁.
𝐿ൌ෍
௜ൌଵ
ே
𝑛௜ 𝑝ሺ𝑥௜ሻ
െ෍
௜ൌଵ
ே
𝑝ሺ𝑥௜ሻ log 𝑝ሺ𝑥௜ሻ൑𝐿൏െ෍
௜ൌଵ
ே
𝑝ሺ𝑥௜ሻ log 𝑝ሺ𝑥௜ሻ൅1
𝐻ሺ𝑋ሻ൑𝐿൏𝐻ሺ𝑋ሻ൅1
690 of 848

Example 4b
Suppose that 10 independent tosses of a coin having probability  of coming up
heads are made at location  and the result is to be transmitted to location 
The outcome of this experiment is a random vector 
 where 
 is
1 or 0 according to whether or not the outcome of the  th toss is heads. By the
results of this section, it follows that  the average number of bits transmitted by
any code, satisfies
with
for at least one code. Now, since the 
 are independent, it follows from
Proposition 3.1
 and Theorem 3.2
 that
If 
 then 
 and it follows that we can do no better than just
encoding  by its actual value. For example, if the first 5 tosses come up heads
and the last 5 tails, then the message 1111100000 is transmitted to location 
However, if 
 we can often do better by using a different coding scheme. For
instance, if 
 then
Thus, there is an encoding for which the average length of the encoded message
is no greater than 9.11.
One simple coding that is more efficient in this case than the identity code is to
break up 
 into 5 pairs of 2 random variables each and then, for
 code each of the pairs as follows:
𝑝
𝐴
𝐵.
𝑋ൌሺ𝑋ଵ, ... , 𝑋ଵ଴ሻ,
𝑋௜
𝑖
𝐿,
𝐻ሺ𝑋ሻ൑𝐿
𝐿൏𝐻ሺ𝑋ሻ൅1
𝑋௜
𝐻ሺ𝑋ሻൌ𝐻ሺ𝑋ଵ, ... , 𝑋ଵ଴ሻ
ൌ෍
௜ൌଵ
ଵ଴
𝐻ሺ𝑋௜ሻ
ൌെ10ሾ𝑝log 𝑝൅ሺ1 െ𝑝ሻ logሺ1 െ𝑝ሻሿ
𝑝ൌ1
2,
𝐻ሺ𝑋ሻൌ10,
𝑋
𝐵.
𝑝്1
2,
𝑝ൌ1
4,
𝐻ሺ𝑋ሻൌെ10ቀ1
4 log 1
4 ൅3
4 log 3
4ቁൌ8.11
ሺ𝑋ଵ, ... , 𝑋ଵ଴ሻ
𝑖ൌ1, 3, 5, 7, 9,
𝑋௜ൌ0, 𝑋௜൅ଵൌ0 ↔0
𝑋௜ൌ0, 𝑋௜൅ଵൌ1 ↔10
𝑋௜ൌ1, 𝑋௜൅ଵൌ0 ↔110
𝑋௜ൌ1, 𝑋௜൅ଵൌ1 ↔111
691 of 848

The total message transmitted is the successive encodings of the preceding
pairs.
For instance, if the outcome TTTHHTTTTH is observed, then the message
 is sent. The average number of bits needed to transmit the message
with this code is
Up to this point, we have assumed that the message sent at location  is received
without error at location 
 However, there are always certain errors that can occur
because of random disturbances along the communications channel. Such random
disturbances might lead, for example, to the message 00101101, sent at 
 being
received at  in the form 01101101.
Let us suppose that a bit transmitted at location  will be correctly received at
location  with probability  independently from bit to bit. Such a communications
system is called a binary symmetric channel. Suppose further that 
 and we
want to transmit a message consisting of a large number of bits from  to 
 Thus,
direct transmission of the message will result in an error probability of .20 for each
bit, which is quite high. One way to reduce this probability of bit error would be to
transmit each bit 3 times and then decode by majority rule. That is, we could use the
following scheme:
Encode
Decode
Encode
Decode
Note that if no more than one error occurs in transmission, then the bit will be
correctly decoded. Hence, the probability of bit error is reduced to
a considerable improvement. In fact, it is clear that we can make the probability of bit
error as small as we want by repeating the bit many times and then decoding by
majority rule. For instance, the scheme
010110010
5൥1ቆ3
4ቇ
ଶ
൅2ቆ1
4ቇቆ3
4ቇ൅3ቆ1
4ቇቆ3
4ቇ൅3ቆ1
4ቇ
ଶ
൩
ൌ135
16
ൎ8.44
𝐴
𝐵.
𝐴,
𝐵
𝐴
𝐵
𝑝,
𝑝ൌ.8
𝐴
𝐵.
0 →000
000
001
010
100
⎫
⎬
⎭
⎪
⎪
→0
1 →111
111
110
101
011
⎫
⎬
⎭
⎪
⎪
→1
ሺ.2ሻଷ൅3ሺ.2ሻଶሺ.8ሻൌ.104
692 of 848

Encode
Decode
 of 17 0’s
By majority rule
 of 17 1’s
will reduce the probability of bit error to below .01.
The problem with this type of encoding scheme is that although it decreases the
probability of bit error, it does so at the cost of also decreasing the effective rate of
bits sent per signal. (See Table 9.1.
)
Table 9.1 Repetition of Bits Encoding Scheme.
Probability of error (per bit)
Rate (bits transmitted per signal)
.20
1
.10
.01
In fact, at this point it may appear inevitable to the reader that decreasing the
probability of bit error to 0 always results in also decreasing the effective rate at
which bits are transmitted per signal to 0. However, a remarkable result of
information theory known as the noisy coding theorem and due to Claude Shannon
demonstrates that this is not the case. We now state this result as Theorem 4.2
.
Theorem 4.2 The noisy coding theorem
There is a number  such that for any value  that is less than 
 and for any
 there exists a coding–decoding scheme that transmits at an average rate
of  bits sent per signal and with an error (per bit) probability of less than  The
largest such value of — call it C  –is called the channel capacity, and for the
binary symmetric channel,
†For an entropy interpretation of C*, see Theoretical Exercise 9.18.
0 →string
1 →string
.33 ቆൌ1
3ቇ
.06 ቆൌ1
17ቇ
𝐶
𝑅
𝐶,
𝜀൐0,
𝑅
𝜀.
𝐶
*†
𝐶* ൌ1 ൅𝑝log 𝑝൅ሺ1 െ𝑝ሻ logሺ1 െ𝑝ሻ
693 of 848

The Poisson process having rate  is a collection of random variables 
that relate to an underlying process of randomly occurring events. For instance, 
represents the number of events that occur between times 0 and  The defining
features of the Poisson process are as follows:
i. The number of events that occur in disjoint time intervals are independent.
ii. The distribution of the number of events that occur in an interval depends only
on the length of the interval.
iii. Events occur one at a time.
iv. Events occur at rate 
It can be shown that 
 is a Poisson random variable with mean 
 In addition, if
 are the times between the successive events, then they are independent
exponential random variables with rate 
A sequence of random variables 
 each of which takes on one of the values
 is said to be a Markov chain with transition probabilities 
 if, for all
If we interpret 
 as the state of some process at time  then a Markov chain is a
sequence of successive states of a process that has the property that whenever it
enters state  then, independently of all past states, the next state is  with
probability 
 for all states  and  For many Markov chains, the probability of
being in state  at time  converges to a limiting value that does not depend on the
initial state. If we let 
 denote these limiting probabilities, then they are
the unique solution of the equations
Moreover, 
 is equal to the long-run proportion of time that the chain is in state 
Let  be a random variable that takes on one of  possible values according to the
set of probabilities 
 The quantity
λ
ሼ𝑁ሺ𝑡ሻ, 𝑡൒0ሽ
𝑁ሺ𝑡ሻ
𝑡.
λ.
𝑁ሺ𝑡ሻ
λ𝑡.
𝑇௜, 𝑖൒1,
λ.
𝑋௡, 𝑛൒0,
0, ... , 𝑀,
𝑃௜, ௝
𝑛, 𝑖଴, ... , 𝑖௡, 𝑖, 𝑗,
𝑃൛𝑋௡൅ଵൌ𝑗ห𝑋௡ൌ𝑖, 𝑋௡െଵൌ𝑖௡െଵ, ... , 𝑋଴ൌ𝑖଴ൟൌ𝑃௜, ௝
𝑋௡
𝑛,
𝑖,
𝑗
𝑃௜, ௝,
𝑖
𝑗.
𝑗
𝑛
𝜋௝, 𝑗ൌ0, ... , 𝑀,
𝜋௝
ൌ෍
௜ൌ଴
ெ
𝜋௜𝑃௜, ௝ 𝑗ൌ0, ... , 𝑀
෍
௝ൌଵ
ெ
𝜋௝
ൌ1
𝜋௝
𝑗.
𝑋
𝑛
൛𝑝ଵ, ... , 𝑝௡ൟ.
694 of 848

is called the entropy of 
 It can be interpreted as representing either the average
amount of uncertainty that exists regarding the value of  or the average information
received when  is observed. Entropy has important implications for binary codings
of 
𝐻ሺ𝑋ሻൌെ෍
௜ൌଵ
௡
𝑝௜logଶሺ𝑝௜ሻ
𝑋.
𝑋
𝑋
𝑋.
9.1. Customers arrive at a bank at a Poisson rate  Suppose that two
customers arrived during the first hour. What is the probability that
a. both arrived during the first 20 minutes?
b. at least one arrived during the first 20 minutes?
9.2. Cars cross a certain point in the highway in accordance with a
Poisson process with rate 
 per minute. If Al runs blindly across the
highway, what is the probability that he will be uninjured if the amount
of time that it takes him to cross the road is  seconds? (Assume that if
he is on the highway when a car passes by, then he will be injured.) Do
this exercise for 
9.3. Suppose that in Problem 9.2
, Al is agile enough to escape
from a single car, but if he encounters two or more cars while
attempting to cross the road, then he is injured. What is the probability
that he will be unhurt if it takes him  seconds to cross? Do this
exercise for 
9.4. Suppose that 3 white and 3 black balls are distributed in two urns
in such a way that each urn contains 3 balls. We say that the system is
in state  if the first urn contains  white balls, 
 At each stage,
1 ball is drawn from each urn and the ball drawn from the first urn is
placed in the second, and conversely with the ball from the second urn.
Let 
 denote the state of the system after the th stage, and compute
the transition probabilities of the Markov chain 
9.5. Consider Example 2a
. If there is a 50–50 chance of rain today,
compute the probability that it will rain 3 days from now if 
 and
9.6. Compute the limiting probabilities for the model of Problem 9.4
.
9.7. A transition probability matrix is said to be doubly stochastic if
λ.
λ ൌ3
𝑠
𝑠ൌ2, 5, 10, 20 .
𝑠
𝑠ൌ5, 10, 20, 30 .
𝑖
𝑖
𝑖ൌ0,1,2,3 .
𝑋௡
𝑛
ሼ𝑋௡, 𝑛൒0ሽ.
𝛼ൌ.7
𝛽ൌ.3 .
695 of 848

for all states 
 Show that such a Markov chain is ergodic,
then 
9.8. On any given day, Buffy is either cheerful (c), so-so (s), or gloomy
(g). If she is cheerful today, then she will be c, s, or g tomorrow with
respective probabilities .7, .2, and .1. If she is so-so today, then she will
be c, s, or g tomorrow with respective probabilities .4, .3, and .3. If she
is gloomy today, then Buffy will be c, s, or g tomorrow with probabilities
.2, .4, and .4. What proportion of time is Buffy cheerful?
9.9. Suppose that whether it rains tomorrow depends on past weather
conditions only through the past 2 days. Specifically, suppose that if it
has rained yesterday and today, then it will rain tomorrow with
probability .8; if it rained yesterday but not today, then it will rain
tomorrow with probability .3; if it rained today but not yesterday, then it
will rain tomorrow with probability .4; and if it has not rained either
yesterday or today, then it will rain tomorrow with probability .2. What
proportion of days does it rain?
9.10. A certain person goes for a run each morning. When he leaves
his house for his run, he is equally likely to go out either the front or the
back door, and similarly, when he returns, he is equally likely to go to
either the front or the back door. The runner owns 5 pairs of running
shoes, which he takes off after the run at whichever door he happens
to be. If there are no shoes at the door from which he leaves to go
running, he runs barefooted. We are interested in determining the
proportion of time that he runs barefooted.
a. Set this problem up as a Markov chain. Give the states and the
transition probabilities.
b. Determine the proportion of days that he runs barefooted.
9.11. This problem refers to Example 2f
.
a. Verify that the proposed value of 
 satisfies the necessary
equations.
b. For any given molecule, what do you think is the (limiting)
probability that it is in urn 1?
c. Do you think that the events that molecule 
 is in urn 1 at
a very large time would be (in the limit) independent?
d. Explain why the limiting probabilities are as given.
෍
௜ൌ଴
ெ
𝑃௜௝ൌ1
𝑗ൌ0, 1, ... , 𝑀.
ෑ
௝
ൌ1/ሺ𝑀൅1ሻ, 𝑗ൌ0, 1, ... , 𝑀.
ෑ
௝
𝑗, 𝑗൒1,
696 of 848

9.12. Determine the entropy of the sum that is obtained when a pair of
fair dice is rolled.
9.13. Prove that if  can take on any of  possible values with
respective probabilities 
 then 
 is maximized when
 What is 
 equal to in this case?
9.14. A pair of fair dice is rolled. Let
and let  equal the value of the first die. Compute (a) 
 (b) 
and (c) H(X, Y).
9.15. A coin having probability 
 of coming up heads is flipped 6
times. Compute the entropy of the outcome of this experiment.
9.16. A random variable can take on any of  possible values 
with respective probabilities 
 We shall attempt to
determine the value of  by asking a series of questions, each of which
can be answered “yes” or “no.” For instance, we may ask “Is 
”
or “Is  equal to either 
 or 
 or 
” and so on. What can you say
about the average number of such questions that you will need to ask
to determine the value of 
9.17. Show that for any discrete random variable  and function 
9.18. In transmitting a bit from location  to location 
 if we let 
denote the value of the bit sent at location  and  denote the value
received at location 
 then 
 is called the rate of
transmission of information from A to B. The maximal rate of
transmission, as a function of 
 is called the
channel capacity. Show that for a binary symmetric channel with
 the channel capacity is attained
by the rate of transmission of information when 
 and its
value is 
𝑋
𝑛
𝑃ଵ, ... , 𝑃௡,
𝐻ሺ𝑋ሻ
𝑃௜ൌ1/𝑛, 𝑖ൌ1, ... , 𝑛.
𝐻ሺ𝑋ሻ
𝑋ൌቊ1 if the sum of the dice is 6
0 otherwise
𝑌
𝐻ሺ𝑌ሻ,
𝐻௒ሺ𝑋ሻ,
𝑝ൌ2
3
𝑛
𝑥ଵ, ... , 𝑥௡
𝑝ሺ𝑥௜ሻ, 𝑖ൌ1, ... , 𝑛.
𝑋
𝑋ൌ𝑥ଵ?
𝑋
𝑥ଵ
𝑥ଶ
𝑥ଷ?
𝑋?
𝑋
𝑓,
𝐻ሺ𝑓ሺ𝑋ሻሻ൑𝐻ሺ𝑋ሻ
𝐴
𝐵,
𝑋
𝐴
𝑌
𝐵,
𝐻ሺ𝑋ሻെ𝐻௒ሺ𝑋ሻ
𝑃ሼ𝑋ൌ1ሽൌ1 െ𝑃ሼ𝑋ൌ0ሽ,
𝑃ሼ𝑌ൌ1||𝑋ൌ1ሽൌ𝑃ሼ𝑌ൌ0||𝑋ൌ0ሽൌ𝑝,
𝑃ቄ𝑋ൌ1ቅൌ1
2
1 ൅𝑝 log 𝑝൅ሺ1 െ𝑝ሻ logሺ1 െ𝑝ሻ.
9.1. Events occur according to a Poisson process with rate 
 per
hour.
a. What is the probability that no events occur between times 8
and 10 in the morning?
b. What is the expected value of the number of events that occur
λ ൌ3
697 of 848

between times 8 and 10 in the morning?
c. What is the expected time of occurrence of the fifth event after
2 P.M.?
9.2. Customers arrive at a certain retail establishment according to a
Poisson process with rate  per hour. Suppose that two customers
arrive during the first hour. Find the probability that
a. both arrived in the first 20 minutes;
b. at least one arrived in the first 30 minutes.
9.3. Four out of every five trucks on the road are followed by a car,
while one out of every six cars is followed by a truck. What proportion
of vehicles on the road are trucks?
9.4. A certain town’s weather is classified each day as being rainy,
sunny, or overcast, but dry. If it is rainy one day, then it is equally
likely to be either sunny or overcast the following day. If it is not rainy,
then there is one chance in three that the weather will persist in
whatever state it is in for another day, and if it does change, then it is
equally likely to become either of the other two states. In the long
run, what proportion of days are sunny? What proportion are rainy?
9.5. Let  be a random variable that takes on 5 possible values with
respective probabilities .35, .2, .2, .2, and .05. Also, let  be a
random variable that takes on 5 possible values with respective
probabilities .05, .35, .1, .15, and .35.
a. Show that 
b. Using the result of Problem 9.13
, give an intuitive
explanation for the preceding inequality.
λ
𝑋
𝑌
𝐻ሺ𝑋ሻ൐𝐻ሺ𝑌ሻ.
Kඍඕඍඖඡ, J., L. Sඖඍඔඔ, and A. Kඖඉ඘඘. Denumerable Markov Chains. New
York: D. Van Nostrand Company, 1966.
Pඉකජඍඖ, E. Stochastic Processes. San Francisco: Holden-Day, Inc., 1962.
R඗ඛඛ, S. M. Introduction to Probability Models, 11th ed. San Diego: Academic
Press, Inc., 2014.
[1]
[2]
[3]
698 of 848

10.1 Introduction
10.2 General Techniques for Simulating Continuous Random Variables
10.3 Simulating from Discrete Distributions
10.4 Variance Reduction Techniques
How can we determine the probability of our winning a game of solitaire? (By
solitaire, we mean any one of the standard solitaire games played with an ordinary
deck of 52 playing cards and with some fixed playing strategy.) One possible
approach is to start with the reasonable hypothesis that all (52)! possible
arrangements of the deck of cards are equally likely to occur and then attempt to
determine how many of these lead to a win. Unfortunately, there does not appear to
be any systematic method for determining the number of arrangements that lead to a
win, and as (52)! is a rather large number and the only way to determine whether a
particular arrangement leads to a win seems to be by playing the game out, it can be
seen that this approach will not work.
In fact, it might appear that the determination of the probability of winning at solitaire
is mathematically intractable. However, all is not lost, for probability falls not only
within the realm of mathematics, but also within the realm of applied science; and, as
in all applied sciences, experimentation is a valuable technique. For our solitaire
example, experimentation takes the form of playing a large number of such games
or, better yet, programming a computer to do so. After playing, say,  games, if we let
R඗ඛඛ, S. M. Stochastic Processes, 2d ed. New York: John Wiley & Sons, Inc.,
1996.
𝑛
𝑋௜ൌቊ1
if the 𝑖th game results in a win
0
otherwise
699 of 848

then 
 will be independent Bernoulli random variables for which
Hence, by the strong law of large numbers, we know that
will, with probability 1, converge to 
 That is, by playing a large
number of games, we can use the proportion of games won as an estimate of the
probability of winning. This method of empirically determining probabilities by means
of experimentation is known as simulation.
In order to use a computer to initiate a simulation study, we must be able to generate
the value of a uniform (0, 1) random variable; such variates are called random
numbers. To generate them, most computers have a built-in subroutine, called a
random-number generator, whose output is a sequence of pseudorandom
numbers—a sequence of numbers that is, for all practical purposes, indistinguishable
from a sample from the uniform (0, 1) distribution. Most random-number generators
start with an initial value 
 called the seed, and then recursively compute values by
specifying positive integers 
 and 
 and then letting
where the foregoing means that 
 is divided by 
 and the remainder is taken
as the value of 
 Thus, each 
 is either 
 and the quantity 
 is
taken as an approximation to a uniform (0, 1) random variable. It can be shown that
subject to suitable choices for 
 and 
Equation (1.1)
 gives rise to a
sequence of numbers that look as if they were generated from independent uniform
(0, 1) random variables.
As our starting point in simulation, we shall suppose that we can simulate from the
uniform (0, 1) distribution, and we shall use the term random numbers to mean
independent random variables from this distribution.
In the solitaire example, we would need to program a computer to play out the game
starting with a given ordering of the cards. However, since the initial ordering is
supposed to be equally likely to be any of the (52)! possible permutations, it is also
necessary to be able to generate a random permutation. Using only random
numbers, the following algorithm shows how this can be accomplished. The
𝑋௜, 𝑖ൌ1, ... , 𝑛
𝐸ሾ𝑋௜ሿൌ𝑃ሼwin at solitaireሽ
෍
௜ൌଵ
௡
𝑋௜
𝑛ൌ
number of games won
number of games played
𝑃ሼwin at solitaireሽ
𝑋଴,
𝑎, 𝑐,
𝑚,
𝑋௡൅ଵൌሺ𝑎𝑋௡൅𝑐ሻ modulo 𝑚 𝑛൒0
(1.1)
𝑎𝑋௡൅𝑐
𝑚
𝑋௡൅ଵ.
𝑋௡
0, 1, ... , 𝑚െ1,
𝑋௡/𝑚
𝑎, 𝑐,
𝑚,
700 of 848

algorithm begins by randomly choosing one of the elements and then putting it in
position 
 it then randomly chooses among the remaining elements and puts the
choice in position 
 and so on. The algorithm efficiently makes a random choice
among the remaining elements by keeping these elements in an ordered list and
then randomly choosing a position on that list.
Example 1a Generating a random permutation
Suppose we are interested in generating a permutation of the integers 
such that all ! possible orderings are equally likely. Then, starting with any initial
permutation, we will accomplish this after 
 steps, where we interchange the
positions of two of the numbers of the permutation at each step. Throughout, we
will keep track of the permutation by letting 
 denote the number
currently in position  The algorithm operates as follows:
1. Consider any arbitrary permutation, and let 
 denote the element in
position 
 [For instance, we could take 
]
2. Generate a random variable 
 that is equally likely to equal any of the
values 
3. Interchange the values of 
 and 
 The value of 
 will now
remain fixed. [For instance, suppose that 
 and initially
 If 
 then the new permutation is
 and element 3 will remain in
position 4 throughout.]
4. Generate a random variable 
 that is equally likely to be either 
5. Interchange the values of 
 and 
 [If 
 then the new
permutation is 
]
6. Generate 
 which is equally likely to be either 
7. Interchange the values of 
 and 
 [If 
 then the new
permutation is 
 and this is the final
permutation.]
8. Generate 
 and so on. The algorithm continues until 
 is generated,
and after the next interchange the resulting permutation is the final one.
To implement this algorithm, it is necessary to be able to generate a random
variable that is equally likely to be any of the values 
 To accomplish
this, let  denote a random number—that is,  is uniformly distributed on (0, 1)—
and note that 
 is uniform on 
 Hence,
𝑛;
𝑛െ1,
1, 2, ... , 𝑛
𝑛
𝑛െ1
𝑋ሺ𝑖ሻ,𝑖ൌ1, ... , 𝑛
𝑖.
𝑋ሺ𝑖ሻ
𝑖, 𝑖ൌ1... ,𝑛.
𝑋ሺ𝑖ሻൌ𝑖, 𝑖ൌ1, ... , 𝑛.
𝑁௡
1, 2, ... , 𝑛.
𝑋ሺ𝑁௡ሻ
𝑋ሺ𝑛ሻ.
𝑋ሺ𝑛ሻ
𝑛ൌ4
𝑋ሺ𝑖ሻൌ𝑖, 𝑖ൌ1, 2, 3, 4.
𝑁ସൌ3,
𝑋ሺ1ሻൌ1, 𝑋ሺ2ሻൌ2, 𝑋ሺ3ሻൌ4, 𝑋ሺ4ሻൌ3,
𝑁௡െଵ
1, 2, ...,
𝑛െ1.
𝑋ሺ𝑁௡െଵሻ
𝑋ሺ𝑛െ1ሻ.
𝑁ଷൌ1,
𝑋ሺ1ሻൌ4, 𝑋ሺ2ሻൌ2, 𝑋ሺ3ሻൌ1, 𝑋ሺ4ሻൌ3.
𝑁௡െଶ,
1, 2,  ... ,   𝑛െ2.
𝑋ሺ𝑁௡െଶሻ
𝑋ሺ𝑛െ2ሻ.
𝑁ଶൌ1,
𝑋ሺ1ሻൌ2, 𝑋ሺ2ሻൌ4, 𝑋ሺ3ሻൌ1, 𝑋ሺ4ሻൌ3,
𝑁௡െଷ,
𝑁ଶ
1, 2,  ... ,   𝑘.
𝑈
𝑈
𝑘𝑈
ሺ0, 𝑘ሻ.
𝑃ቄ𝑖െ1 ൏𝑘𝑈൏𝑖ቅൌ
ଵ
௞
𝑖ൌ1,   ... ,    𝑘
701 of 848

so if we take 
 where 
 is the integer part of  (that is, the largest
integer less than or equal to ), then 
 will have the desired distribution.
The algorithm can now be succinctly written as follows:
The foregoing algorithm for generating a random permutation is extremely useful.
For instance, suppose that a statistician is developing an experiment to compare
the effects of 
 different treatments on a set of  subjects. He decides to split the
subjects into 
 different groups of respective sizes 
 where
with the members of the th group to receive treatment  To
eliminate any bias in the assignment of subjects to treatments (for instance, it
would cloud the meaning of the experimental results if it turned out that all the
“best’’ subjects had been put in the same group), it is imperative that the
assignment of a subject to a given group be done “at random.” How is this to be
accomplished?
Another technique for randomly dividing the subjects when 
was presented in Example 2g of Chapter 6
. The preceding
procedure is faster, but requires more space than the one of
Example 2g.
A simple and efficient procedure is to arbitrarily number the subjects 1 through 
and then generate a random permutation 
 of 
 Now assign
subjects 
 to be in group 1; 
 to be in
group 2; and, in general, group  is to consist of subjects numbered
𝑁௞ൌሾ𝑘𝑈ሿ൅1,
ሾ𝑥ሿ
𝑥
𝑥
𝑁௞
Let 
 be any permutation of 
 [For instance, we
can set 
]
Let 
Generate a random number  and set 
Interchange the values of 
 and 
Reduce the value of  by 1, and if 
 go to step 3.
 is the desired random generated permutation.
Step 1.
𝑋ሺ1ሻ,   ... ,   𝑋ሺ𝑛ሻ
1, 2,  ... ,   𝑛.
𝑋ሺ𝑖ሻൌ𝑖,  𝑖ൌ1,  ... ,   𝑛.
Step 2.
𝐼ൌ𝑛.
Step 3.
𝑈
𝑁ൌሾ𝐼𝑈ሿ൅1.
Step 4.
𝑋ሺ𝑁ሻ
𝑋ሺ𝐼ሻ.
Step 5.
𝐼
𝐼൐1,
Step 6.
𝑋ሺ1ሻ,   ... ,   𝑋ሺ𝑛ሻ
𝑚
𝑛
𝑚
𝑛ଵ,  𝑛ଶ,   ... ,   𝑛௠,
𝛴௜ൌଵ
௠ 𝑛௜ൌ𝑛,
𝑖
𝑖.
†
†
𝑚ൌ2
𝑛
𝑋ሺ1ሻ,   ... ,   𝑋ሺ𝑛ሻ
1, 2,  ... ,   𝑛.
𝑋ሺ1ሻ,  𝑋ሺ2ሻ,   ... ,   𝑋ሺ𝑛ଵሻ
𝑋ሺ𝑛ଵ൅1ሻ,   ... ,   𝑋ሺ𝑛ଵ൅𝑛ଶሻ
𝑗
𝑋ሺ𝑛ଵ൅𝑛ଶ൅⋯൅𝑛௝െଵ൅𝑘ሻ,  𝑘ൌ1,  ... ,   𝑛௝.
702 of 848

In this section, we present two general methods for using random numbers to
simulate continuous random variables.
A general method for simulating a random variable having a continuous distribution
—called the inverse transformation method—is based on the following proposition.
Proposition 2.1
Let  be a uniform (0, 1) random variable. For any continuous distribution
function 
 if we define the random variable  by
then the random variable  has distribution function 
 [
 is defined to equal
that value  for which 
]
Proof
Now, since 
 is a monotone function, it follows that 
 if and only if
 Hence, from Equation (2.1)
, we have
It follows from Proposition 2.1
 that we can simulate a random variable  having
a continuous distribution function  by generating a random number  and then
setting 
Example 2a Simulating an exponential random variable
If 
 then 
 is that value of  such that
or
Hence, if  is a uniform (0, 1) variable, then
𝑈
𝐹,
𝑌
𝑌ൌ𝐹െଵሺ𝑈ሻ
𝑌
𝐹. 𝐹െଵሺ𝑥ሻ
𝑦
𝐹ሺ𝑦ሻൌ𝑥.
𝐹௒ሺ𝑎ሻ
ൌ𝑃ሼ𝑌൑𝑎ሽ
ൌ𝑃൛𝐹െଵሺ𝑈ሻ൑𝑎ൟ
(2.1)
𝐹ሺ𝑥ሻ
𝐹െଵሺ𝑈ሻ൑𝑎
𝑈൑𝐹ሺ𝑎ሻ.
𝐹௒ሺ𝑎ሻ
ൌ𝑃ሼ𝑈൑𝐹ሺ𝑎ሻሽ
ൌ𝐹ሺ𝑎ሻ
𝑋
𝐹
𝑈
𝑋ൌ𝐹െଵሺ𝑈ሻ.
𝐹ሺ𝑥ሻൌ1 െ𝑒െ௫,
𝐹െଵሺ𝑢ሻ
𝑥
1 െ𝑒െ௫ൌ𝑢
𝑥ൌെlogሺ1 െ𝑢ሻ
𝑈
703 of 848

is exponentially distributed with mean 1. Since 
 is also uniformly distributed
on (0, 1), it follows that 
 is exponential with mean 1. Since 
 is
exponential with mean  when  is exponential with mean 1, it follows that
 is exponential with mean 
The results of Example 2a
 can also be utilized to stimulate a gamma random
variable.
Example 2b Simulating a gamma
random variable
To simulate from a gamma distribution with parameters 
 when  is an
integer, we use the fact that the sum of  independent exponential random
variables, each having rate  has this distribution. Hence, if 
 are
independent uniform (0, 1) random variables, then
has the desired distribution.
Suppose that we have a method for simulating a random variable having density
function 
 We can use this method as the basis for simulating from the
continuous distribution having density 
 by simulating  from  and then
accepting the simulated value with a probability proportional to 
Specifically, let  be a constant such that
We then have the following technique for simulating a random variable having
density 
𝐹െଵሺ𝑈ሻൌെlogሺ1 െ𝑈ሻ
1 െ𝑈
െlog 𝑈
𝑐𝑋
𝑐
𝑋
െ𝑐log 𝑈
𝑐.
ሺ𝑛, 𝜆ሻ
ሺ𝑛,  λሻ
𝑛
𝑛
𝜆,
𝑈ଵ,   ... ,   𝑈௡
𝑋ൌെ෍
௜ൌଵ
௡
1
𝜆log 𝑈௜ൌെ1
𝜆logቌෑ
௜ൌଵ
௡
𝑈௜ቍ
𝑔ሺ𝑥ሻ.
𝑓ሺ𝑥ሻ
𝑌
𝑔
𝑓ሺ𝑌ሻ/𝑔ሺ𝑌ሻ.
𝑐
𝑓ሺ𝑦ሻ
𝑔ሺ𝑦ሻ൑𝑐 for all 𝑦
𝑓.
Simulate  having density  and simulate a random number 
If 
 set 
 Otherwise return to step 1.
Step 1.
𝑌
𝑔
𝑈.
Step 2. 𝑈൑𝑓ሺ𝑌ሻ/𝑐𝑔ሺ𝑌ሻ,
𝑋ൌ𝑌.
704 of 848

The rejection method is expressed pictorially in Figure 10.1
. We now prove that it
works.
Figure 10.1 Rejection method for simulating a random variable  having
density function 
Proposition 2.2
The random variable  generated by the rejection method has density function 
Proof Let  be the value obtained and let  denote the number of necessary
iterations. Then
where 
 Now, by independence, the joint density function
of  and  is
so, using the foregoing, we have
Letting  approach 
 and using the fact that  is a density gives
𝑋
𝑓.
𝑋
𝑓.
𝑋
𝑁
𝑃ሼ𝑋൑𝑥ሽ
ൌ𝑃ሼ𝑌ே൑𝑥ሽ
ൌ𝑃ቊ𝑌൑𝑥|𝑈൑𝑓ሺ𝑌ሻ
𝑐𝑔ሺ𝑌ሻቋ
ൌ
𝑃ቊ𝑌൑𝑥,   𝑈൑𝑓ሺ𝑌ሻ
𝑐𝑔ሺ𝑌ሻቋ
𝐾
𝐾ൌ𝑃ሼ𝑈൑𝑓ሺ𝑌ሻ/𝑐𝑔ሺ𝑌ሻሽ.
𝑌
𝑈
𝑓ሺ𝑦,  𝑢ሻൌ𝑔ሺ𝑦ሻ 0 ൏𝑢൏1
𝑃ሼ𝑋൑𝑥ሽ
ൌ
ଵ
௄          
∬
                   ௬   ൑  ௫
଴   ൑  ௨   ൑  ௙ሺ௬ሻ/௖௚ሺ௬ሻ
𝑔ሺ𝑦ሻ𝑑𝑢 𝑑𝑦
ൌ
ଵ
௄඲
െஶ
௫
඲
଴
௙ሺ௬ሻ/௖௚ሺ௬ሻ
𝑑𝑢 𝑔ሺ𝑦ሻ 𝑑𝑦
ൌ
ଵ
௖௄඲
െஶ
௫
𝑓ሺ𝑦ሻ 𝑑𝑦
(2.2)
𝑋
∞
𝑓
705 of 848

Hence, from Equation (2.2)
, we obtain
which completes the proof.
Remarks
a. Note that the way in which we “accept the value  with probability 
”
is by generating a random number  and then accepting  if 
b. Since each iteration will independently result in an accepted value with
probability 
 it follows that the number of
iterations has a geometric distribution with mean 
Example 2c Simulating a normal random variable
To simulate a unit normal random variable  (that is, one with mean 0 and
variance 1), note first that the absolute value of  has probability density function
We will start by simulating from the preceding density function by using the
rejection method, with  being the exponential density function with mean 1—that
is,
Now, note that
1 ൌ1
𝑐𝐾඲
െஶ
ஶ
𝑓ሺ𝑦ሻ𝑑𝑦ൌ1
𝑐𝐾
𝑃ሼ𝑋൑𝑥ሽൌ඲
െஶ
௫
𝑓ሺ𝑦ሻ𝑑𝑦
𝑌
𝑓ሺ𝑌ሻ/𝑐𝑔ሺ𝑌ሻ
𝑈
𝑌
𝑈൑𝑓ሺ𝑌ሻ/𝑐𝑔ሺ𝑌ሻ.
𝑃ሼ𝑈൑𝑓ሺ𝑌ሻ/𝑐𝑔ሺ𝑌ሻሽൌ𝐾ൌ1/𝑐,
𝑐.
𝑍
𝑍
𝑓ሺ𝑥ሻൌ
2
2𝜋
√
𝑒െ௫మ/ଶ 0 ൏𝑥൏∞
(2.3)
𝑔
𝑔ሺ𝑥ሻൌ𝑒െ௫ 0 ൏𝑥൏∞
706 of 848

Hence, we can take 
 so, from Equation (2.4)
,
Therefore, using the rejection method, we can simulate the absolute value of a
unit normal random variable as follows:
Once we have simulated a random variable  having Equation (2.3)
 as its
density function, we can then generate a unit normal random variable  by letting
 be equally likely to be either  or 
In step (b), the value  is accepted if 
 which is equivalent
to 
 However, in Example 2a
, it was shown that 
 is
exponential with rate 1, so steps (a) and (b) are equivalent to
Suppose now that the foregoing results in 
 being accepted—so we know that
 is larger than 
 By how much does the one exceed the other? To
answer this question, recall that 
 is exponential with rate 1; hence, given that it
exceeds some value, the amount by which 
 exceeds 
 [that is, its
“additional life” beyond the time 
] is (by the memoryless property)
also exponentially distributed with rate 1. That is, when we accept step 
 not
only do we obtain  (the absolute value of a unit normal), but, by computing
𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻ
ൌ
2
𝜋
ඨexpቊെሺ𝑥ଶെ2𝑥ሻ
2
ቋ
ൌ
2
𝜋
ඨexpቊെሺ𝑥ଶെ2𝑥൅1ሻ
2
൅1
2ቋ
ൌ
2𝑒
𝜋
ඨ
exp൝െሺ𝑥െ1ሻଶ
2
ൡ
൑
2𝑒
𝜋
ඨ
(2.4)
𝑐ൌ
2𝑒/𝜋
ඥ
;
𝑓ሺ𝑥ሻ
𝑐𝑔ሺ𝑥ሻൌexp൝െሺ𝑥െ1ሻଶ
2
ൡ
Generate independent random variables  and 
 being exponential with
rate 1 and  being uniform on (0, 1).
If 
 set 
 Otherwise, return to (a).
(a)
𝑌
𝑈, 𝑌
𝑈
(b)
𝑈൑exp൛െሺ𝑌െ1ሻଶ/2ൟ,
𝑋ൌ𝑌.
𝑋
𝑍
𝑍
𝑋
െ𝑋.
𝑌
𝑈൑exp൛െሺ𝑌െ1ሻଶ/2ൟ,
െlog 𝑈൒ሺ𝑌െ1ሻଶ/2.
െlog 𝑈
(a ) Generate independent exponentials 
 and 
 each with rate 1.
(b ) If 
 set 
 Otherwise, return to (a ).
'
𝑌ଵ
𝑌ଶ,
'
𝑌ଶ൒ሺ𝑌ଵെ1ሻଶ/2,
𝑋ൌ𝑌ଵ.
'
𝑌ଵ
𝑌ଶ
ሺ𝑌ଵെ1ሻଶ/2.
𝑌ଶ
𝑌ଶ
ሺ𝑌ଵെ1ሻଶ/2
ሺ𝑌ଵെ1ሻଶ/2
ሺ𝑏'ሻ,
𝑋
707 of 848

 we also can generate an exponential random variable (that is
independent of ) having rate 1.
Summing up, then, we have the following algorithm that generates an
exponential with rate 1 and an independent unit normal random variable:
The random variables  and  generated by the foregoing algorithm are
independent, with  being normal with mean 0 and variance 1 and  being
exponential with rate 1. (If we want the normal random variable to have mean 
and variance 
 we just take 
)
Remarks
a. Since 
 the algorithm requires a geometrically distributed
number of iterations of step 2 with mean 1.32.
b. If we want to generate a sequence of unit normal random variables, then
we can use the exponential random variable  obtained in step 3 as the
initial exponential needed in step 1 for the next normal to be generated.
Hence, on the average, we can simulate a unit normal by generating
 exponentials and computing 1.32 squares.
Example 2d Simulating normal random variables: the polar method
It was shown in Example 7b of Chapter 6
 that if  and  are independent unit
normal random variables, then their polar coordinates
 are independent, with 
 being exponentially
distributed with mean 2 and  being uniformly distributed on 
 Hence, if 
and 
 are random numbers, then, using the result of Example 2a
, we can
set
𝑌ଶെሺ𝑌ଵെ1ሻଶ/2,
𝑋
Generate 
 an exponential random variable with rate 1.
Generate 
 an exponential random variable with rate 1.
If 
 set 
 and go to step 4.
Otherwise, go to step 1.
Generate a random number 
 and set
Step 1.
𝑌ଵ,
Step 2.
𝑌ଶ,
Step 3. 𝑌ଶെሺ𝑌ଵെ1ሻଶ/2 ൐0,
𝑌ൌ𝑌ଶെሺ𝑌ଵെ1ሻଶ/2
Step 4.
𝑈,
𝑍ൌ
⎧
⎨
⎩
⎪
⎪
𝑌ଵ
if 𝑈൑1
2
െ𝑌ଵ
if 𝑈൐1
2
𝑍
𝑌
𝑍
𝑌
𝜇
𝜎ଶ,
𝜇൅𝜎𝑍.
𝑐ൌ
2𝑒/𝜋
ඥ
ൎ1.32,
𝑌
1.64ሺൌ2 ൈ1.32 െ1ሻ
𝑋
𝑌
𝑅ൌ
𝑋ଶ൅𝑌ଶ
ඥ
,   Θ ൌ
െଵሺ𝑌/𝑋ሻ
𝑅ଶ
Θ
ሺ0,2𝜋ሻ.
𝑈ଵ
𝑈ଶ
𝑅ൌሺെ2 log 𝑈ଵሻଵ/ଶ
Θ ൌ2𝜋𝑈ଶ
708 of 848

from which it follows that
are independent unit normals.
The preceding approach to generating unit normal random variables is called the
Box–Muller approach. Its efficiency suffers somewhat from its need to compute the
sine and cosine values. There is, however, a way to get around this potentially time-
consuming difficulty. To begin, note that if  is uniform on (0, 1), then 2  is uniform
on (0, 2), so 
 is uniform on 
 Thus, if we generate random numbers 
and 
 and set
then 
 is uniformly distributed in the square of area 4 centered at (0, 0). (See
Figure 10.2
)
Figure 10.2
Suppose now that we continually generate such pairs 
 until we obtain one
that is contained in the disk of radius 1 centered at (0, 0)—that is, until 
 It
then follows that such a pair 
 is uniformly distributed in the disk. Now, let 
denote the polar coordinates of this pair. Then it is easy to verify that  and  are
independent, with 
 being uniformly distributed on (0, 1) and  being uniformly
𝑋ൌ𝑅cos Θ ൌሺെ2 log 𝑈ଵሻଵ/ଶcosሺ2𝜋𝑈ଶሻ
𝑌ൌ𝑅sin Θ ൌሺെ2 log 𝑈ଵሻଵ/ଶsinሺ2𝜋𝑈ଶሻ
(2.5)
𝑈
𝑈
2𝑈െ1
ሺെ1,1ሻ.
𝑈ଵ
𝑈ଶ
𝑉ଵൌ2𝑈ଵെ1
𝑉ଶൌ2𝑈ଶെ1
ሺ𝑉ଵ,  𝑉ଶሻ
ሺ𝑉ଵ,  𝑉ଶሻ
𝑉ଵ
ଶ൅𝑉ଶ
ଶ൑1.
ሺ𝑉ଵ,  𝑉ଶሻ
𝑅̅ ̅̅,Θ̅ ̅
𝑅̅ ̅ ̅
Θ̅ ̅
𝑅̅ ̅̅ଶ
Θ̅ ̅
709 of 848

distributed on 
 (See Problem 10.13
.)
Since
it follows from Equation (2.5)
 that we can generate independent unit normals 
and  by generating another random number  and setting
In fact, because (conditional on 
) 
 is uniform on (0, 1) and is
independent of  we can use it instead of generating a new random number 
 thus
showing that
are independent unit normals, where
Summing up, we have the following approach to generating a pair of independent
unit normals:
The preceding algorithm is called the polar method. Since the probability that a
ሺ0,2𝜋ሻ.
sin Θ̅ ̅ ൌ𝑉ଶ
𝑅̅ ̅ ൌ
𝑉ଶ
𝑉ଵ
ଶ൅𝑉ଶ
ଶ
ට
cos Θ̅ ̅ ൌ𝑉ଵ
𝑅̅ ̅ ̅ ൌ
𝑉ଵ
𝑉ଵ
ଶ൅𝑉ଶ
ଶ
ට
𝑋
𝑌
𝑈
𝑋ൌሺെ2log 𝑈ሻଵ/ଶ𝑉ଵ/𝑅̅ ̅
𝑌ൌሺെ2log 𝑈ሻଵ/ଶ𝑉ଶ/𝑅̅ ̅ ̅
𝑉ଵ
ଶ൅𝑉ଶ
ଶ൑1 𝑅̅ ̅ଶ
𝜃̅ ̅,
𝑈,
𝑋ൌሺെ2log 𝑅̅ ̅ଶሻ
ଵ/ଶ𝑉ଵ
𝑅̅ ̅̅ ൌ
െ2log 𝑆
𝑆
ඨ
𝑉ଵ
𝑌ൌሺെ2log 𝑅̅ ̅ ̅ଶሻ
ଵ/ଶ𝑉ଶ
𝑅̅ ̅ ̅ ൌ
െ2log 𝑆
𝑆
ඨ
𝑉ଶ
𝑆ൌ𝑅̅ ̅ଶൌ𝑉ଵ
ଶ൅𝑉ଶ
ଶ
Generate random numbers 
 and 
Set 
If 
 return to step 1.
Return the independent unit normals
Step 1.
𝑈ଵ
𝑈ଶ.
Step 2.
𝑉ଵൌ2𝑈ଵെ1, 𝑉ଶൌ2𝑈ଶെ1, 𝑆ൌ𝑉ଵ
ଶ൅𝑉ଶ
ଶ.
Step 3. 𝑆൐1,
Step 4.
𝑋ൌ
െ2 log 𝑆
𝑆
ඨ
𝑉ଵ,  𝑌ൌ
െ2 log 𝑆
𝑆
ඨ
𝑉ଶ
710 of 848

random point in the square will fall within the circle is equal to 
 (the area of the
circle divided by the area of the square), it follows that, on average, the polar method
will require 
 iterations of step 1. Hence, it will, on average, require 2.546
random numbers, 1 logarithm, 1 square root, 1 division, and 4.546 multiplications to
generate 2 independent unit normals.
Example 2e Simulating a chi-squared random variable
The chi-squared distribution with  degrees of freedom is the distribution of
 where 
 are independent unit normals. Now, it
was shown in Section 6.3
 of Chapter 6
 that 
 has an exponential
distribution with rate 
 Hence, when  is even (say, 
 has a gamma
distribution with parameters 
 Thus, 
 has a chi-squared
distribution with 2  degrees of freedom. Accordingly, we can simulate a chi-
squared random variable with 
 degrees of freedom by first simulating a unit
normal random variable  and then adding 
 to the foregoing. That is,
where 
 are independent,  is a unit normal, and 
 are
uniform (0, 1) random variables.
All of the general methods for simulating random variables from continuous
distributions have analogs in the discrete case. For instance, if we want to simulate a
random variable  having probability mass function
we can use the following discrete time analog of the inverse transform technique:
To simulate  for which 
 let  be uniformly distributed over (0, 1) and
set
𝜋/4
4/𝜋ൎ1.273
𝑛
𝜒௡
ଶൌ𝑍ଵ
ଶ൅⋯൅𝑍௡
ଶ,
𝑍௜,  𝑖ൌ1,  ... ,   𝑛
𝑍ଵ
ଶ൅𝑍ଶ
ଶ
1
2 .
𝑛
𝑛ൌ2𝑘ሻ,  𝜒ଶ௞
ଶ
ቀ𝑘,   1
2 ሻ.
െ2logሺ∏௜ൌଵ
௞
𝑈௜ሻ
𝑘
2𝑘൅1
𝑍
𝑍ଶ
𝜒ଶ௞൅ଵ
ଶ
ൌ𝑍ଶെ2 logቌෑ
௜ൌଵ
௞
𝑈௜ቍ
𝑍,  𝑈ଵ,   ... ,   𝑈௡
𝑍
𝑈ଵ,   ... ,   𝑈௡
𝑍
𝑃ሼ𝑋ൌ𝑥௝ሽൌ𝑃௝,   𝑗ൌ0, 1,  ... ,    ෍
௝
𝑃௝ൌ1
𝑋
𝑃൛𝑋ൌ𝑥௝ൟൌ𝑃௝,
𝑈
711 of 848

Since
it follows that  has the desired distribution.
Example 3a The geometric distribution
Suppose that independent trials, each of which results in a “success” with
probability 
 are continually performed until a success occurs. Letting
 denote the necessary number of trials; then
which is seen by noting that 
 if the first 
 trials are all failures and the th
trial is a success. The random variable  is said to be a geometric random
variable with parameter
 Since
we can simulate such a random variable by generating a random number  and
then setting  equal to that value  for which
or, equivalently, for which
𝑋ൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
⎪
⎪
𝑥ଵ
if 𝑈൑𝑃ଵ
𝑥ଶ
if 𝑃ଵ൏𝑈൑𝑃ଵ൅𝑃ଶ
⋮
𝑥௝
if  ෍
ଵ
௝െଵ
𝑃௜൏𝑈൑෍
௜
௝
𝑃௜
⋮
𝑃ሼ𝑋ൌ𝑥௝ሽൌ𝑃ቐ෍
ଵ
௝െଵ
𝑃௜൏𝑈൑෍
ଵ
௝
𝑃௜ቑൌ𝑃௝
𝑋
𝑝,0 ൏𝑝൏1,
𝑋
𝑃ሼ𝑋ൌ𝑖ሽൌሺ1 െ𝑝ሻ௜െଵ𝑝 𝑖൒1
𝑋ൌ𝑖
𝑖െ1
𝑖
𝑋
𝑝.
෍
௜െଵ
௝െଵ
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ1 െ𝑃ሼ𝑋൐𝑗െ1ሽ
ൌ1 െ𝑃ሼϐirst 𝑗െ1 are all failuresሽ
ൌ1 െሺ1 െ𝑝ሻ௝െଵ 𝑗൒1
𝑈
𝑋
𝑗
1 െሺ1 െ𝑝ሻ௝െଵ൏𝑈൑1 െሺ1 െ𝑝ሻ௝
ሺ1 െ𝑝ሻ௝൑1 െ𝑈൏ሺ1 െ𝑝ሻ௝െଵ
712 of 848

Since 
 has the same distribution as 
 we can define  by
where the inequality has changed sign because 
 is negative [since
]. Using the notation [ ] for the integer part of  (that is, [ ]
is the largest integer less than or equal to ), we can write
As in the continuous case, special simulating techniques have been developed for
the more common discrete distributions. We now present two of these.
Example 3b Simulating a Binomial Random Variable
A binomial (
) random variable can easily be simulated by recalling that it can
be expressed as the sum of  independent Bernoulli random variables. That is, if
 are independent uniform (0, 1) variables, then letting
it follows that 
 is a binomial random variable with parameters  and 
Example 3c Simulating a Poisson Random Variable
To simulate a Poisson random variable with mean  generate independent
uniform (0, 1) random variables 
 stopping at
The random variable 
 has the desired distribution. That is, if we
continue generating random numbers until their product falls below 
 then the
number required, minus 1, is Poisson with mean 
That 
 is indeed a Poisson random variable having mean  can perhaps
1 െ𝑈
𝑈,
𝑋
𝑋
ൌmin ൛𝑗:ሺ1 െ𝑝ሻ௝൑𝑈ൟ
ൌmin ሼ𝑗: 𝑗logሺ1 െ𝑝ሻ൑log 𝑈ሽ
ൌmin ቊ𝑗: 𝑗൒
log 𝑈
logሺ1 െ𝑝ሻቋ
logሺ1 െ𝑝ሻ
logሺ1 െ𝑝ሻ൐ log1 ൌ0
𝑥
𝑥
𝑥
𝑥
𝑋ൌ1 ൅ቈ
log 𝑈
logሺ1 െ𝑝ሻ቉
𝑛, 𝑝
𝑛
𝑈ଵ,   ... ,   𝑈௡
𝑋௜ൌቊ
1
if 𝑈௜൏𝑝
0
othewise
𝑋≡෍
௜ൌଵ
௡
𝑋௜
𝑛
𝑝.
𝜆,
𝑈ଵ,  𝑈ଶ,  ...
𝑁ൌmin ቐ𝑛: ෑ
௜ൌଵ
௡
𝑈௜൏𝑒െఒቑ
𝑋≡𝑁െ1
𝑒െఒ,
𝜆.
𝑋≡𝑁െ1
𝜆
713 of 848

be most easily seen by noting that
is equivalent to
or, taking logarithms, to
or
However, 
 is exponential with rate 1, so  can be thought of as being the
maximum number of exponentials having rate 1 that can be summed and still be
less than  But by recalling that the times between successive events of a
Poisson process having rate 1 are independent exponentials with rate 1, it
follows that  is equal to the number of events by time  of a Poisson process
having rate 1; thus,  has a Poisson distribution with mean 
Let 
 have a given joint distribution, and suppose that we are interested in
computing
where  is some specified function. It sometimes turns out that it is extremely difficult
to analytically compute  and when such is the case, we can attempt to use
simulation to estimate  This is done as follows: Generate 
 having the
same joint distribution as 
 and set
𝑋൅1 ൌmin ቐ𝑛: ෑ
௜ൌଵ
௡
𝑈௜൏𝑒െఒቑ
𝑋ൌmax ቐ𝑛: ෑ
௜ൌଵ
௡
𝑈௜൒𝑒െఒቑ where  ෑ
௜ൌଵ
଴
𝑈௜≡1
𝑋ൌmax ቐ𝑛: ෍
௜ൌଵ
௡
log 𝑈௜൒െ𝜆ቑ
𝑋ൌmax ቐ𝑛: ෍
௜ൌଵ
௡
െlog 𝑈௜൑𝜆ቑ
െlog 𝑈௜
𝑋
λ.
𝑋
λ
𝑋
λ.
𝑋ଵ,   ... ,   𝑋௡
𝜃≡𝐸ሾ𝑔ሺ𝑋ଵ,   ... ,   𝑋௡ሻሿ
𝑔
𝜃,
𝜃.
𝑋ଵ
ሺଵሻ,   ... ,   𝑋௡
ሺଵሻ
𝑋ଵ,   ... ,   𝑋௡
714 of 848

Now let 
 simulate a second set of random variables (independent of the
first set) having the distribution of 
 and set
Continue this until you have generated  (some predetermined number) sets and so
have also computed 
 Now, 
 are independent and identically
distributed random variables, each having the same distribution as 
Thus, if we let  denote the average of these  random variables—that is, if
then
Hence, we can use  as an estimate of  Since the expected square of the
difference between  and  is equal to the variance of 
 we would like this quantity
to be as small as possible. [In the preceding situation, 
 which is
usually not known in advance, but must be estimated from the generated values
] We now present three general techniques for reducing the variance of
our estimator.
In the foregoing situation, suppose that we have generated 
 and 
 which are
identically distributed random variables having mean  Now,
Hence, it would be advantageous (in the sense that the variance would be reduced)
if 
 and 
 were negatively correlated rather than being independent. To see how
we could arrange this, let us suppose that the random variables 
 are
independent and, in addition, that each is simulated via the inverse transform
𝑌ଵൌ𝑔ሺ𝑋ଵ
ሺଵሻ,   ... ,   𝑋௡
ሺଵሻሻ
𝑋ଵ
ሺଶሻ,   ... ,   𝑋௡
ሺଶሻ
𝑋ଵ,   ... ,   𝑋௡
𝑌ଶൌ𝑔ሺ𝑋ଵ
ሺଶሻ,   ... ,   𝑋௡
ሺଶሻሻ
𝑘
𝑌ଵ,  𝑌ଶ,   ... ,   𝑌௞.
𝑌ଵ,   ... ,   𝑌௞
𝑔ሺ𝑋ଵ,   ... ,   𝑋௡ሻ.
𝑌̅ ̅̅ ̅
𝑘
𝑌̅ ̅̅ ̅ൌ෍
௜ൌଵ
௞
𝑌௜
𝑘
𝐸ሾ𝑌̅ ̅ ̅ሿ
ൌ𝜃
𝐸ሾሺ𝑌̅ ̅̅ ̅െ𝜃ሻ
ଶሿ
ൌvarሺ𝑌̅ ̅̅ ̅ሻ
𝑌̅ ̅̅
𝜃.
𝑌̅ ̅̅ ̅
𝜃
𝑌̅ ̅̅ ̅,
varሺ𝑌̅ ̅̅ ̅ሻൌvarሺ𝑌௜ሻ/𝑘,
𝑌ଵ,   ... ,   𝑌௡.
𝑌ଵ
𝑌ଶ,
𝜃.
varቆ𝑌ଵ൅𝑌ଶ
2
ቇ
ൌ1
4ሾvarሺ𝑌ଵሻ൅varሺ𝑌ଶሻ൅2Covሺ𝑌ଵ,   𝑌ଶሻሿ
ൌvarሺ𝑌ଵሻ
2
൅covሺ𝑌ଵ,   𝑌ଶሻ
2
𝑌ଵ
𝑌ଶ
𝑋ଵ,   ... ,   𝑋௡
715 of 848

technique. That is, 
 is simulated from 
 where 
 is a random number and
 is the distribution of 
 Thus, 
 can be expressed as
Now, since 
 is also uniform over (0, 1) whenever  is a random number (and is
negatively correlated with ), it follows that 
 defined by
will have the same distribution as 
 Hence, if 
 and 
 were negatively correlated,
then generating 
 by this means would lead to a smaller variance than if it were
generated by a new set of random numbers. (In addition, there is a computational
savings because, rather than having to generate  additional random numbers, we
need only subtract each of the previous  numbers from 1.) Although we cannot, in
general, be certain that 
 and 
 will be negatively correlated, this often turns out to
be the case, and indeed it can be proven that it will be so whenever  is a monotonic
function.
Let us start by recalling the conditional variance formula (see Section 7.5.4)
Now, suppose that we are interested in estimating 
 by simulating
 and then computing 
 If, for some random variable  we
can compute 
 then, since 
 it follows from the preceding
conditional variance formula that
Thus, since 
 it follows that 
 is a better estimator of 
 than
is 
Example 4a Estimation of 
Let 
 and 
 be random numbers and set 
 As noted in
Example 2d
, 
 will be uniformly distributed in the square of area 4
centered at (0, 0). The probability that this point will fall within the inscribed circle
of radius 1 centered at (0, 0) (see Figure 10.2
) is equal to 
 (the ratio of the
area of the circle to that of the square). Hence, upon simulating a large number 
of such pairs and setting
𝑋௜
𝐹௜
െଵሺ𝑈௜ሻ,
𝑈௜
𝐹௜
𝑋௜.
𝑌ଵ
𝑌ଵൌ𝑔ሺ𝐹ଵ
െଵሺ𝑈ଵሻ,   ... ,   𝐹௡
െଵሺ𝑈௡ሻሻ
1 െ𝑈
𝑈
𝑈
𝑌ଶ
𝑌ଶൌ𝑔ሺ𝐹ଵ
െଵሺ1 െ𝑈ଵሻ,   ... ,   𝐹௡
െଵሺ1 െ𝑈௡ሻሻ
𝑌ଵ.
𝑌ଵ
𝑌ଶ
𝑌ଶ
𝑛
𝑛
𝑌ଵ
𝑌ଶ
𝑔
Varሺ𝑌ሻൌ𝐸ሾVarሺ𝑌||𝑍ሻሿ൅Varሺ𝐸ሾ𝑌||𝑍ሿሻ
𝐸ሾ𝑔ሺ𝑋ଵ,   ... ,   𝑋௡ሻሿ
𝐗ൌሺ𝑋ଵ,   ... ,   𝑋௡ሻ
𝑌ൌ𝑔ሺ𝐗ሻ.
𝑍
𝐸ሾ𝑌||𝑍ሿ,
varሺ𝑌||𝑍ሻ൒0,
Varሺ𝐸ሾ𝑌|𝑍ሿሻ൑Varሺ𝑌ሻ
𝐸ሾ𝐸ሾ𝑌||𝑍ሿሿൌ𝐸ሾ𝑌ሿ,
𝐸ሾ𝑌||𝑍ሿ
𝐸ሾ𝑌ሿ
𝑌.
𝜋
𝑈ଵ
𝑈ଶ
𝑉௜ൌ2𝑈௜െ1, 𝑖ൌ1,2.
ሺ𝑉ଵ,  𝑉ଶሻ
𝜋/4
𝑛
716 of 848

it follows that 
 will be independent and identically distributed
random variables having 
 Thus, by the strong law of large numbers,
Therefore, by simulating a large number of pairs 
 and multiplying the
proportion of them that fall within the circle by 4, we can accurately approximate
The preceding estimator can, however, be improved upon by using conditional
expectation. If we let  be the indicator variable for the pair 
 then, rather
than using the observed value of  it is better to condition on 
 and so utilize
Now,
so
Thus, an improvement on using the average value of  to estimate 
 is to use
the average value of 
 Indeed, since
where  is uniform over (0, 1), we can generate  random numbers  and use
the average value of 
 as our estimate of 
 (Problem 10.14
 shows
that this estimator has the same variance as the average of the  values,
)
𝐼௝ൌቊ1
if the 𝑗th pair falls within the circle
0
otherwise
𝐼௝,  𝑗ൌ1,  ... ,   𝑛,
𝐸ሾ𝐼௝ሿൌ𝜋/4.
𝐼ଵ൅⋯൅𝐼௡
𝑛
→𝜋
4  as 𝑛→∞
ሺ𝑉ଵ,  𝑉ଶሻ
𝜋.
𝐼
ሺ𝑉ଵ,  𝑉ଶሻ,
𝐼,
𝑉ଵ
𝐸ሾ𝐼||𝑉ଵሿ
ൌ𝑃൛𝑉ଵ
ଶ൅𝑉ଶ
ଶ൑1ห𝑉ଵൟ
ൌ𝑃൛𝑉ଶ
ଶ൑1 െ𝑉ଵ
ଶห𝑉ଵൟ
𝑃൛𝑉ଶ
ଶ൑1 െ𝑉ଵ
ଶห𝑉ଵൌ𝜈ൟ
ൌ𝑃൛𝑉ଶ
ଶ൑1 െ𝜈ଶൟ
ൌ𝑃൛െ1 െ𝜈ଶ
√
൑𝑉ଶ൑
1 െ𝜈ଶ
√
ൟ
ൌ
1 െ𝜈ଶ
√
𝐸ሾ𝐼|𝑉ଵሿൌ
1 െ𝑉ଵ
ଶ
ට
𝐼
𝜋/4
1 െ𝑉ଵ
ଶ
ට
.
𝐸൥1 െ𝑉ଵ
ଶ
ට
൩ൌ඲
െଵ
ଵ
1
2
1 െ𝜈ଶ
ඥ
𝑑𝜈ൌ඲
଴
ଵ
1 െ𝑢ଶ
ඥ
𝑑𝑢ൌ𝐸ቂ1 െ𝑈ଶ
ඥ
ቃ
𝑈
𝑛
𝑈
1 െ𝑈ଶ
ඥ
𝜋/4.
𝑛
1 െ𝑉ଶ
ඥ
.
717 of 848

The preceding estimator of  can be improved even further by noting that the
function 
 is a monotonically decreasing function of 
and so the method of antithetic variables will reduce the variance of the estimator
of 
 That is, rather than generating  random numbers and using the
average value of 
 as an estimator of 
 we would obtain an improved
estimator by generating only /2 random numbers  and then using one-half the
average of 
 as the estimator of 
The following table gives the estimates of  resulting from simulations, using
 based on the three estimators.
Method
Estimate of 
Proportion of the random points that fall in the circle
3.1612
Average value of 
3.128448
Average value of 
3.139578
A further simulation using the final approach and 
 yielded the estimate
3.143288.
Again, suppose that we want to use simulation to estimate [ (X)], where 
 But suppose now that for some function  the expected value of (X) is
known—say, it is 
 Then, for any constant  we can also use
as an estimator of [ (X)]. Now,
Simple calculus shows that the foregoing is minimized when
𝜋
𝑔ሺ𝑢ሻൌ
1 െ𝑢ଶ
√
,0 ൑𝑢൑1,
𝑢,
𝐸ሾ1 െ𝑈ଶ
ඥ
ሿ.
𝑛
1 െ𝑈ଶ
ඥ
𝜋/4,
𝑛
𝑈
1 െ𝑈ଶ
ඥ
൅
1 െሺ1 െ𝑈ሻଶ
ට
𝜋/4.
𝜋
𝑛ൌ10,000,
𝜋
1 െ𝑈ଶ
ඥ
1 െ𝑈ଶ
ඥ
൅
1 െሺ1 െ𝑈ሻଶ
ට
𝑛ൌ64,000
𝐸𝑔
Xൌ
ሺ𝑋ଵ,   ... ,   𝑋௡ሻ.
𝑓,
𝑓
𝐸ሾ𝑓ሺ𝐗ሻሿൌ𝜇.
𝑎,
𝑊ൌ𝑔ሺ𝐗ሻ൅𝑎ሾ𝑓ሺ𝐗ሻെ𝜇ሿ
𝐸𝑔
varሺ𝑊ሻൌvarሾ𝑔ሺ𝐗ሻሿ൅𝑎ଶvarሾ𝑓ሺ𝐗ሻሿ൅2𝑎 covሾ𝑔ሺ𝐗ሻ,  𝑓ሺ𝐗ሻሿ
(4.1)
𝑎ൌെcovሾ𝑓ሺ𝐗ሻ,  𝑔ሺ𝐗ሻሿ
varሾ𝑓ሺ𝐗ሻሿ
(4.2)
718 of 848

and for this value of 
Unfortunately, neither Var[ (X)] nor Cov[ (X)], (X)] is usually known, so we cannot
in general obtain the foregoing reduction in variance. One approach in practice is to
use the simulated data to estimate these quantities. This approach usually yields
almost all of the theoretically possible reduction in variance.
Let  be a continuous distribution function and  a uniform (0, 1) random variable.
Then the random variable 
 has distribution function 
 where 
 is that
value  such that 
 Applying this result, we can use the values of uniform (0,
1) random variables, called random numbers, to generate the values of other random
variables. This technique is called the inverse transform method.
Another technique for generating random variables is based on the rejection method.
Suppose that we have an efficient procedure for generating a random variable from
the density function  and that we desire to generate a random variable having
density function  The rejection method for accomplishing this starts by determining
a constant  such that
It then proceeds as follows:
1. Generate  having density 
2. Generate a random number 
3. If 
 set 
 and stop.
4. Return to step 1.
The number of passes through step 1 is a geometric random variable with mean 
Standard normal random variables can be efficiently simulated by the rejection
method (with  being exponential with mean 1) or by the technique known as the
polar algorithm.
To estimate a quantity  one often generates the values of a partial sequence of
𝑎,
varሺ𝑊ሻൌvarሾ𝑔ሺ𝐗ሻሿെcovሾ𝑓ሺ𝐗ሻ,  𝑔ሺ𝐗ሻሿଶ
varሾ𝑓ሺ𝐗ሻሿ
(4.3)
𝑓
𝑓
𝑔
𝐹
𝑈
𝐹െଵሺ𝑈ሻ
𝐹,
𝐹െଵሺ𝑢ሻ
𝑥
𝐹ሺ𝑥ሻൌ𝑢.
𝑔
𝑓.
𝑐
max 𝑓ሺ𝑥ሻ
𝑔ሺ𝑥ሻ൑𝑐
𝑌
𝑔.
𝑈.
𝑈൑𝑓ሺ𝑌ሻ/𝑐𝑔ሺ𝑌ሻ,
𝑋ൌ𝑌
𝑐.
𝑔
𝜃,
719 of 848

random variables whose expected value is  The efficiency of this approach is
increased when these random variables have a small variance. Three techniques
that can often be used to specify random variables with mean  and relatively small
variances are
1. the use of antithetic variables,
2. the use of conditional expectations, and
3. the use of control variates.
𝜃.
𝜃
10.1. The following algorithm will generate a random permutation of the
elements 
 It is somewhat faster than the one presented in
Example 1a
 but is such that no position is fixed until the algorithm
ends. In this algorithm, 
 can be interpreted as the element in
position 
Go to step 3.
a. Explain in words what the algorithm is doing.
b. Show that at iteration —that is, when the value of 
 is
initially set—
 is a random permutation of
Hint: Use induction and argue that
10.2. Develop a technique for simulating a random variable having
density function
1, 2,  ... ,   𝑛.
𝑃ሺ𝑖ሻ
𝑖.
Set 
Set 
If 
 stop. Otherwise, let 
Generate a random number  and let
Step 1.
𝑘ൌ1.
Step 2.
𝑃ሺ1ሻൌ1.
Step 3.
𝑘ൌ𝑛,
𝑘ൌ𝑘൅1.
Step 4.
𝑈
𝑃ሺ𝑘ሻ
ൌ𝑃ሺሾ𝑘𝑈ሿ൅1ሻ
𝑃ሺሾ𝑘𝑈ሿ൅1ሻ
ൌ𝑘
𝑘
𝑃ሺ𝑘ሻ
𝑃ሺ1ሻ,  𝑃ሺ2ሻ,   ... ,   𝑃ሺ𝑘ሻ
1, 2,  ... ,   𝑘.
𝑃௞൛𝑖ଵ,  𝑖ଶ,  ... ,  𝑖௝െଵ,  𝑘,  𝑖௝,  ... ,  𝑖௞െଶ,  𝑖ൟ
  ൌ𝑃௞െଵ൛𝑖ଵ,  𝑖ଶ,  ... ,  𝑖௝െଵ,  𝑖,  𝑖௝,  ... ,  𝑖௞െଶൟ1
𝑘
  ൌ1
𝑘!  by the induction hypothesis
𝑓ሺ𝑥ሻൌ൝𝑒ଶ௫
െ∞൏𝑥൏0
𝑒െଶ௫0 ൏𝑥൏∞
720 of 848

10.3. Give a technique for simulating a random variable having the
probability density function
10.4. Present a method for simulating a random variable having
distribution function
10.5. Use the inverse transformation method to present an approach
for generating a random variable from the Weibull distribution
10.6. Give a method for simulating a random variable having failure
rate function
a. 
b. 
c. 
d. 
10.7. Let  be the distribution function
a. Give a method for simulating a random variable having
distribution  that uses only a single random number.
b. Let 
 be independent random numbers. Show that
c. Use part (b) to give a second method of simulating a random
variable having distribution 
10.8. Suppose it is relatively easy to simulate from 
 for each
 How can we simulate from
𝑓ሺ𝑥ሻൌ
⎧
⎨
⎩
⎪
⎪
1
2ሺ𝑥െ2ሻ
2 ൑𝑥൑3
1
2 ൬2 െ𝑥
3൰
3 ൏𝑥൑6
0
otherwise
𝐹ሺ𝑥ሻൌ
⎧
⎨
⎩
⎪
⎪
⎪
⎪
0
𝑥൑െ3
1
2 ൅𝑥
6
െ3 ൏𝑥൏0
1
2 ൅𝑥ଶ
32
0 ൏𝑥൑4
1
𝑥൐4
𝐹ሺ𝑡ሻൌ1 െ𝑒െ௔௧ഁ 𝑡൒0
𝜆ሺ𝑡ሻൌ𝑐;
λሺ𝑡ሻൌ𝑐𝑡;
λሺ𝑡ሻൌ𝑐𝑡ଶ;
λሺ𝑡ሻൌ𝑐𝑡ଷ.
𝐹
𝐹ሺ𝑥ሻൌ𝑥௡ 0 ൏𝑥൏1
𝐹
𝑈ଵ,   ... ,   𝑈௡
𝑃ሼmaxሺ𝑈ଵ,   ... ,   𝑈௡ሻ൑𝑥ሽൌ𝑥௡
𝐹.
𝐹௜
𝑖ൌ1,  ... ,   𝑛.
721 of 848

a. 
b. 
10.9. Suppose we have a method for simulating random variables from
the distributions 
 and 
 Explain how to simulate from the
distribution
Give a method for simulating from
10.10. In Example 2c
 we simulated the absolute value of a unit
normal by using the rejection procedure on exponential random
variables with rate 1. This raises the question of whether we could
obtain a more efficient algorithm by using a different exponential
density—that is, we could use the density 
 Show that the
mean number of iterations needed in the rejection scheme is minimized
when 
10.11. Use the rejection method with 
 to determine
an algorithm for simulating a random variable having density function
10.12. Explain how you could use random numbers to approximate
 where 
 is an arbitrary function.
Hint: If  is uniform on (0, 1), what is 
10.13. Let 
 be uniformly distributed in the circle of radius 1
centered at the origin. Its joint density is thus
Let 
 and 
 denote the polar
coordinates of (
). Show that  and  are independent, with 
 being
uniform on (0, 1) and  being uniform on 
10.14. In Example 4a, we showed that
𝐹ሺ𝑥ሻൌෑ
௜ൌଵ
௡
𝐹௜ሺ𝑥ሻ?
𝐹ሺ𝑥ሻൌ1 െෑ
௜ൌଵ
௡
ሾ1 െ𝐹௜ሺ𝑥ሻሿ?
𝐹ଵ
𝐹ଶ.
𝐹ሺ𝑥ሻൌ𝑝𝐹ଵሺ𝑥ሻ൅ሺ1 െ𝑝ሻ𝐹ଶሺ𝑥ሻ 0 ൏𝑝൏1
𝐹ሺ𝑥ሻൌ
⎧
⎨
⎩
⎪
⎪
1
3ሺ1 െ𝑒െଷ௫ሻ൅2
3 𝑥
0 ൏𝑥൑1
4 1
3ሺ1 െ𝑒െଷ௫ሻ൅2
3
𝑥൐1
𝑔ሺ𝑥ሻൌ𝜆𝑒െλ௫.
𝜆ൌ1.
𝑔ሺ𝑥ሻൌ1,0 ൏𝑥൏1,
𝑓ሺ𝑥ሻൌ൝60𝑥ଷሺ1 െ𝑥ሻଶ0 ൏𝑥൏1
0
otherwise
׬଴
ଵ𝑘ሺ𝑥ሻ𝑑𝑥,
𝑘ሺ𝑥ሻ
𝑈
𝐸ሾ𝑘ሺ𝑈ሻሿ?
ሺ𝑋,  𝑌ሻ
𝑓ሺ𝑥, 𝑦ሻൌ1
𝜋
0 ൑𝑥ଶ൅𝑦ଶ൑1
𝑅ൌሺ𝑋ଶ൅𝑌ଶሻଵ/ଶ
𝜃ൌ
െଵሺ𝑌/𝑋ሻ
𝑋, 𝑌
𝑅
𝜃
𝑅ଶ
𝜃
ሺ0,2𝜋ሻ.
722 of 848

when  is uniform 
 and  is uniform (0, 1). Now show that
and find their common value.
10.15.
a. Verify that the minimum of (4.1
) occurs when  is as given by
(4.2
).
b. Verify that the minimum of (4.1
) is given by (4.3
).
10.16. Let  be a random variable on (0, 1) whose density is 
Show that we can estimate 
 by simulating  and then taking
 as our estimate. This method, called importance sampling,
tries to choose  similar in shape to 
 so that 
 has a small
variance.
𝐸ሾሺ1 െ𝑉ଶሻଵ/ଶሿൌ𝐸ሾሺ1 െ𝑈ଶሻଵ/ଶሿൌ𝜋
4
𝑉
ሺെ1,1ሻ
𝑈
varሾሺ1 െ𝑉ଶሻଵ/ଶሿൌvarሾሺ1 െ𝑈ଶሻଵ/ଶሿ
𝑎
𝑋
𝑓ሺ𝑥ሻ.
׬଴
ଵ𝑔ሺ𝑥ሻ𝑑𝑥
𝑋
𝑔ሺ𝑋ሻ/𝑓ሺ𝑋ሻ
𝑓
𝑔,
𝑔ሺ𝑋ሻ/𝑓ሺ𝑋ሻ
10.1. The random variable  has probability density function
a. Find the value of the constant 
b. Give a method for simulating such a random variable.
10.2. Give an approach for simulating a random variable having probability
density function
10.3. Give an efficient algorithm to simulate the value of a random variable
with probability mass function
10.4. If  is a normal random variable with mean  and variance 
 define a
random variable  that has the same distribution as  and is negatively
correlated with it.
10.5. Let  and  be independent exponential random variables with mean 1.
a. Explain how we could use simulation to estimate 
b. Show how to improve the estimation approach in part (a) by using a
control variate.
𝑋
𝑓ሺ𝑥ሻൌ𝐶𝑒௫ 0 ൏𝑥൏1
𝐶.
𝑓ሺ𝑥ሻൌ30ሺ𝑥ଶെ2𝑥ଷ൅𝑥ସሻ 0 ൏𝑥൏1
𝑝ଵൌ.15 𝑝ଶൌ.2 𝑝ଷൌ.35 𝑝ସൌ.30
𝑋
𝜇
𝜎ଶ,
𝑌
𝑋
𝑋
𝑌
𝐸ሾ𝑒௑௒ሿ.
723 of 848

[1] Ross, S. M. Simulation. 5th ed. San Diego: Academic Press, Inc., 2012.
1. 67,600,000; 19,656,000
2. 1296
4. 24; 4
5. 144; 18
6. 2401
7. 720; 72; 144; 72
8. 120; 1260; 34,650
9. 27,720
10. 40,320; 10,080; 1152; 2880; 384
11. 720; 72; 144
12. 280, 270
13. 89
14. 24,300,000; 17,100,720
15. 190
16. 2,598,960
18. 42; 94
19. 604,800
20. 600
21. 896; 1000; 910
22. 36; 26
23. 35
24. 18
25. 48
28. 
30. 27,720
31. 65,536; 2520
32. 12,600; 945
33. 564,480
34. 165; 35
35. 1287; 14,112
52!/ሺ13!ሻସ
724 of 848

36. 220; 572
9. 74
10. .4; .1
11. 70; 2
12. .5; .32; 149/198
13. 20,000; 12,000; 11,000; 68,000; 10,000
14. 1.057
15. .0020; .4226; .0475; .0211; .00024
17. .1102
18. .048
19. 5/18
20. .9052
22. 
23. 5/12
25. .4
26. .492929
28. .0888; .2477; .1244; .2099
30. 1/18; 1/6; 1/2
31. 2/9; 1/9
33. 70/323
34. .000547
36. .0045; .0588
37. .0833; .5
38. 4
39. .48
40. .8134; .1148
41. .5177
44. .3; .2; .1
46. 5
47. .1399
48. .00106
49. .4329
50. 
52. .2133841
53. 12/35
54. .0511
55. .2198; .0342
ሺ𝑛൅1ሻ/2௡
2.6084 ൈ10െ଺
725 of 848

1. 1/3
2. 1/6; 1/5; 1/4; 1/3; 1/2; 1
3. .339
5. 6/91
6. 1/2
7. 2/3
8. 1/2
9. 7/11
10. .22
11. 1/17; 1/33
12. 2/3
13. .504; .3629
15. 35/768; 35/128
16. .4848
17. .9835
18. .0792; .264
19. .331; .383; .286; .4862
20. 44.29; 41.18
21. .4; 1/26
22. .496; 3/14; 9/62
23. 5/9; 1/6; 5/54
24. 4/9; 1/2
26. 1/3; 1/2
28. 20/21; 40/41
30. 3/128; 29/1536
31. .0893
32. 7/12; 3/5
35. .76, 49/76
36. 27/31
37. .62, 10/19
38. 1/2
39. 1/3; 1/5; 1
40. 12/37
41. 46/185
42. 3/13; 5/13; 5/52; 15/52
43. 43/459
44. 1.03 percent; .3046
45. 4/9
47. .58; 28/58
726 of 848

50. 2/3
52. .175; 38/165; 17/33
53. .65; 56/65; 8/65; 1/65; 14/35; 12/35; 9/35
54. 
55. 3/20; 17/27
56. .40; 17/40; 3/8; 0.08825
57. 
58. 
60. 9
62. (c) 2/3
65. 2/3; 1/3; 3/4
66. 1/6; 3/20
69. .4375
73. (i) 9/128, 9/128, 18/128, 110/128 (ii) 1/32, 1/32, 1/16, 15/16
74. 1/9; 1/18
76. 1/16; 1/32; 5/16; 1/4; 31/32
77. 9/19
78. 3/4, 7/12
81. 
82. .5550
86. .5; .6; .8
87. 9/19; 6/19; 4/19; 7/15; 53/165; 7/33
91. 9/16
94. 97/142; 15/26; 33/102
95. 
96. 
1
4ሺ2𝑝ଷ൅𝑝ଶ൅𝑝ሻ
𝑝ଷ/ሾ𝑝ଷ൅ሺ1 െ𝑝ሻଷሿ; ሾ𝑝ଷሺ1 െሺ1 െ𝑝ሻସሻ൅ሺ1 െ𝑝ሻଷ
ሺ1 െ𝑝ସሻሿ/ሾ𝑝ଷ൅ሺ1 െ𝑝ሻଷሿ
ሺ1/2ሻ/ሺ1 െሺ1/2ሻ௡െଵሻ
2𝑝ଷሺ1 െ𝑝ሻ൅2𝑝ሺ1 െ𝑝ሻଷ; 𝑝ଶ/ሺ1 െ2𝑝൅2𝑝ଶሻ
1
𝑛ሺ1 െሺ1 െ𝑝ሻ௡ሻ
𝑝1ሺ1 െ𝑝2ሻെ𝑝ଵ𝑝ଶ/2; 𝑝ଶ/ሺ2 െ𝑝ଶሻ
1. 
4. (a) 1/2; 5/18; 5/36; 5/84; 5/252; 1/252; 0; 0; 0; 0
5. 
6. 
11b. 
12. 
13. 
𝑝ሺ4ሻൌ6/91; 𝑝ሺ2ሻൌ8/91; 𝑝ሺ1ሻൌ32/91; 𝑝ሺ0ሻൌ1/91; 𝑝ሺെ1ሻൌ16/91;
𝑝ሺെ2ሻൌ28/91
𝑛െ2𝑖; 𝑖ൌ0,...,𝑛
𝑝ሺ3ሻൌ𝑝ሺെ3ሻൌ1/8; 𝑝ሺ1ሻൌ𝑝ሺെ1ሻൌ3/8
ଵ଴ሺ𝑗൅1ሻ
𝑝ሺ4ሻൌ1/16; 𝑝ሺ3ሻൌ1/8; 𝑝ሺ2ሻൌ1/16; 𝑝ሺ0ሻൌ1/2; 𝑝ሺെ𝑖ሻൌ𝑝ሺ𝑖ሻ;
𝑝ሺ0ሻൌ1
𝑝ሺ0ሻൌ.28; 𝑝ሺ500ሻൌ.27, 𝑝ሺ1000ሻൌ.315; 𝑝ሺ1500ሻൌ.09;
727 of 848

14. 
16. 
17. 1/4; 1/6; 1/12; 1/2
19. 1/2; 1/10; 1/5; 1/10; 1/10
20. .5918; no; 
21. 39.28; 37
24. 
25. .46, 1.3
27. 
28. 3/5
31. 
32. 
33. 3
35. 
38. 82.2; 84.5
40. 3/8
41. 11/243
43. 2.8; 1.476
46. 3
52. 17/12; 99/60
53. 1/10; 1/10
54. 
56. 
57. 
58. .03239
59. 110;
63. .8886
64. .4082
66. .0821; .2424
68. .3935; .2293; .3935
69. 
70. 
71. 
73. 
74. .1500; .1012
76. 5.8125
77. 32/243; 4864/6561; 160/729; 160/729
84. 3/10; 5/6; 75/138
85. .3439
86. 1.5
𝑝ሺ2000ሻൌ.045
𝑝ሺ0ሻൌ1/2; 𝑝ሺ1ሻൌ1/6; 𝑝ሺ2ሻൌ1/12; 𝑝ሺ3ሻൌ1/20; 𝑝ሺ4ሻൌ1/5
𝑘/ሺ𝑘൅1ሻ!, 1 ൑𝑘൏𝑛, 1/𝑛!, 𝑘ൌ𝑛
െ.108
𝑝ൌ11/18; maximum ൌ23/72
𝐴ሺ𝑝൅1/10ሻ
𝑝*
11 െ10ሺ.9ሻଵ଴
െ.067; 1.089
𝑒െ.ଶ; 1 െ1.2𝑒െ.ଶ
1 െ𝑒െ.଺; 1 െ𝑒െଶଵଽ.ଵ଼
𝑒െଶଶ;  1 െ3.2𝑒െଶଶ
2/ሺ2𝑛െ1ሻ; 2/ሺ2𝑛െ2ሻ; 𝑒െଵ
2/𝑛; ሺ2𝑛െ3ሻ/ሺ𝑛െ1ሻଶ; 𝑒െଶ
𝑒െଵ଴௘െఱ
𝑝൅ሺ1 െ𝑝ሻ𝑒െఒ௧
728 of 848

89. .1793; 1/3; 4/3
2. 
3. no; no
4. 1/2, .8999
5. 
6. 
7. 3/5; 6/5
8. 2
10. 2/3; 2/3
11. 2/5
13. 2/3; 1/3
15. .7977; .6827; .3695; .9522; .1587
16. 
17. .315; .136
18. 22.66
19. 14.56
20. .9994; .75; .977
22. .974
23. .9253; .1762
26. .0606; .0525
28. .8363
29. .9993
32. 
33. exponential with parameter 1
34. 
35. exponential with parameter 
39. 3/5
41. 
42. 1/
3.5𝑒െହ/ଶ
1 െሺ.01ሻଵ/ହ
4,0, ∞
ሺ.9938ሻଵ଴
𝑒െଵ; 𝑒െଵ/ଶ
𝑒െଵ; 1/3
𝜆/𝑐
𝑎ൌെ2, 𝑏ൌ22
𝑦
2. (a) 14/39; 10/39; 10/39; 5/39 (b) 84; 70; 70; 70; 40; 40; 40; 15 all divided
by 429
3. 15/26; 5/26; 5/26; 1/26
4. (a) 64/169; 40/169; 40/169; 25/169
6. .20, .30, .30, .20; .18, .30, .31, .21; 2.5; 2.55; 1.05; 1.0275
7. 𝑝ሺ𝑖,𝑗ሻൌ𝑝ଶሺ1 െ𝑝ሻ௜൅௝
729 of 848

8. 
9. 
10. 
12. 
13. 1/6; 1/2
15. 
16. 
17. 1/3
19. –
21. 2/5; 2/5
22. no; 1/3
23. 1/2; 2/3; 1/20; 1/18
25. 
28. 
 1
29. .0326
30. .3772; .2061
31. .0829; .3766
32. 5/16; .0228
33. 
34. 
35. (a) .6572; (b) yes; (d) .2402
36. .9346
37. 
39. 5/13; 8/13
40. 1/6; 5/6; 1/4; 3/4
45. 
46. 
50. 
51. .79297
52. 
56. 
57. 
60. (a) 
𝑐ൌ1/8; 𝐸ሾ𝑋ሿൌ0
ሺ12𝑥ଶ൅6𝑥ሻ/7; 15/56; .8625; 5/7; 8/7
1/2; 1 െ𝑒െ௔
39.3𝑒െହ
𝜋/4
𝑛ሺ1/2ሻ௡െଵ
Inሺ𝑦ሻ,  0 ൏𝑦൏1; 1,  0 ൏𝑥൏1;1/2; 1/4
𝑒െଵ/𝑖!
1
2 𝑒െ௧;
െ3𝑒െଶ
𝑃ሺ𝑋ଵ൅𝑋ଶ൐25ሻ; 𝑃ሺ𝑋ଵ൐15ሻ
20 ൅5 2
√
𝑒െଶ; 1 െ3𝑒െଶ
ሺ𝑦൅1ሻଶ𝑥𝑒െ௫ሺ௬൅ଵሻ; 𝑥𝑒െ௫௬; 𝑒െ௔
1/2 ൅3𝑦/ሺ4𝑥ሻെ𝑦ଷ/ሺ4𝑥ଷሻ
ሺሺ𝐿െ2𝑑ሻ/𝐿ሻଷ
1 െ𝑒െହఒ௔; ሺ1 െ𝑒െఒ௔ሻ
ହ
𝑟/𝜋
𝑟
𝑢/ሺ𝜈൅1ሻଶ
1. 52.5/12
2. 324; 198.8
3. 1/2; 1/4; 0
4. 1/6; 1/4; 1/2
730 of 848

5. 3/2
6. 35
7. .9; 4.9; 4.2
8. 
10. .6; 0
11. 
12. 
14. 
15. 1/2
18. 4
21. .9301; 87.5755
22. 14.7
23. 147/110
26. 
29. 
 12; 4; 
31. 175/6
33. 14, 45
34. 20/19; 360/361
35. 21.2; 18.929; 49.214
36. 
37. 0
38. 1.94, 2.22; .6964, .6516; .0932; .1384
40. 1/8
43. 6; 112/33
44. 100/19; 16,200/6137; 10/19; 3240/6137
47. 1/2; 0
49. 
50. 6; 7; 5.8192
51. 6.07
52. 
53. 
55. 12
56. 8
58. 
59. 12.5
64.
65. 
68. 
ሺ1 െሺ1 െ𝑝ሻேሻ/𝑝
2ሺ𝑛െ1ሻ𝑝ሺ1 െ𝑝ሻ
ሺ3𝑛ଶെ𝑛ሻ/ሺ4𝑛െ2ሻ, 3𝑛ଶ/ሺ4𝑛െ2ሻ
𝑚/ሺ1 െ𝑝ሻ
𝑛/ሺ𝑛൅1ሻ; 1/ሺ𝑛൅1ሻ
437
35 ;
123
35
െ𝑛/36
1/ሺ𝑛െ1ሻ
2𝑦ଶ
𝑦ଷ/4
𝑁ሺ1 െ𝑒െଵ଴/ேሻ
𝑝൅1,  ෍
௜ൌ଴
ସ
ቆ4
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻସെ௜𝑒െሺସ൅௜ሻሺ4 ൅𝑖ሻ଺/6!; ሺ1 ൅𝑝ሻ/ሺ1 െ𝑝ሻ; ሺሺ1 െ𝑝ሻ𝑒ሻ/ሺ𝑒ଶെ𝑝ሻ
1/2;  1/3;  1/ሺ𝑛ሺ𝑛൅1ሻሻ
െ96/145
731 of 848

70. 4.2; 5.16
71. 218
72. 
74. 1/2; 1/16; 2/81
75. 1/2, 1/3
77. 
78. 
 yes; 
84. .151; .141
𝑥ሾ1 ൅ሺ2𝑝െ1ሻଶሿ
௡
1/𝑖; ሾ𝑖ሺ𝑖൅1ሻሿെଵ; ∞
𝜇; 1 ൅𝜎ଶ;
𝜎ଶ
1. 
2. 
3. 
4. 
5. .1416
6. .9431
7. .3085
8. .6932
9. 
10. 117
11. 
13. .0162; .0003; .2514; .2514
14. 
16. .013; .018; .691
18. 
23. .769; .357; .4267; .1093; .112184
24. answer is (a)
൒19/20
15/17;   ൒3/4;   ൒10
൒3
൑4/3;  .8428
ሺ258ሻଶ
൒.057
𝑛൒23
൑.2
1. 1/9; 5/9
3. .9953; .9735; .9098; .7358
10. (b)1/6
14. 2.585; .5417; 3.1267
15. 5.5098
732 of 848

1.1.
a. There are 4! different orderings of the letters C, D, E, F. For each of
these orderings, we can obtain an ordering with A and B next to each
other by inserting A and B, either in the order A, B or in the order B, A,
in any of 5 places, namely, either before the first letter of the
permutation of C, D, E, F, or between the first and second, and so on.
Hence, there are 
 arrangements. Another way of solving
this problem is to imagine that B is glued to the back of A. Then there
are 5! orderings in which A is immediately before B. Since there are
also 5! orderings in which B is immediately before A, we again obtain a
total of 
 different arrangements.
b. There are 
 possible arrangements, and since there are as
many with A before B as with B before A, there are 360 arrangements.
c. Of the 720 possible arrangements, there are as many that have A
before B before C as have any of the 3! possible orderings of A, B, and
C. Hence, there are 
 possible orderings.
d. Of the 360 arrangements that have A before B, half will have C before
D and half D before C. Hence, there are 180 arrangements having A
before B and C before D.
e. Gluing B to the back of A and D to the back of C yields 
 different
orderings in which B immediately follows A and D immediately follows
C. Since the order of A and B and of C and D can be reversed, there
are 
 different arrangements.
f. There are 5! orderings in which E is last. Hence, there are
 orderings in which E is not last.
1.2.
 3! 4! 3! 3!, since there are 3! possible orderings of countries and then
the countrymen must be ordered.
1.3.
a. 
b. 
 The result of part (b) follows because there
are 
 choices not including A or B and there are 
 choices
in which a specified one of A and B, but not the other, serves. The latter
follows because the serving member of the pair can be assigned to any
of the 3 offices, the next position can then be filled by any of the other 8
people, and the final position by any of the remaining 7.
c. 
2 ⋅5 ⋅4! ൌ240
2 ⋅5! ൌ240
6! ൌ720
720/6 ൌ120
4! ൌ24
4 ⋅24 ൌ96
6! െ5! ൌ600
10 ⋅9 ⋅8 ൌ720
8 ⋅7 ⋅6 ൅2 ⋅3 ⋅8 ⋅7 ൌ672.
8 ⋅7 ⋅6
3 ⋅8 ⋅7
8 ⋅7 ⋅6 ൅3 ⋅2 ⋅8 ൌ384.
733 of 848

d. 
e. 
1.4.
a. 
b. 
1.5.
1.6.
 There are 
 choices of the three places for the letters. For
each choice, there are 
 different license plates. Hence, altogether
there are 
 different plates.
1.7.
 Any choice of  of the  items is equivalent to a choice of 
namely, those items not selected.
1.8.
a. 
b. 
 since there are 
 choices of the  places to put the zeroes
and then each of the other 
 positions can be any of the digits
1.9.
a. 
b. 
c. 
d. 
e. 
1.10.
 There are 
 numbers in which no digit is repeated. There
are 
 numbers in which only one specified digit appears twice, so
there are 
 numbers in which only a single digit appears twice.
There are 
 numbers in which two specified digits appear twice, so there
3 ⋅9 ⋅8 ൌ216.
9 ⋅8 ⋅7 ൅9 ⋅8 ൌ576.
ቆ10
7 ቇ
ቆ5
3ቇቆ5
4ቇ൅ቆ5
4ቇቆ5
3ቇ൅ቆ5
5ቇቆ5
2ቇ
ቆ7
3,2,2ቇൌ210
ቆ7
3ቇൌ35
ሺ26ሻଷሺ10ሻସ
35 ⋅ሺ26ሻଷ⋅ሺ10ሻସ
𝑟
𝑛
𝑛െ𝑟,
10 ⋅9 ⋅9⋯9 ൌ10 ⋅9௡െଵ
ቆ𝑛
𝑖ቇ9௡െ௜,
ቆ𝑛
𝑖ቇ
𝑖
𝑛െ𝑖
1, . . . , 9.
ቆ3𝑛
3 ቇ
3ቆ𝑛
3ቇ
ቆ3
1ቇቆ2
1ቇቆ𝑛
2ቇቆ𝑛
1ቇൌ3𝑛ଶሺ𝑛െ1ሻ
𝑛ଷ
ቆ3𝑛
3 ቇൌ3ቆ𝑛
3ቇ൅3𝑛ଶሺ𝑛െ1ሻ൅𝑛ଷ
9 ⋅8 ⋅7 ⋅6 ⋅5
ቆ5
2ቇ⋅8 ⋅7 ⋅6
9ቆ5
2ቇ⋅8 ⋅7 ⋅6
7 ⋅5!
2!2!
734 of 848

are 
 numbers in which two digits appear twice. Thus, the answer is
1.11.
a. We can regard this as a seven-stage experiment. First choose the 
married couples that have a representative in the group, and then
select one of the members of each of these couples. By the
generalized basic principle of counting, there are 
 different
choices.
b. First select the  married couples that have a representative in the
group, and then select the  of those couples that are to contribute a
man. Hence, there are 
 different choices. Another
way to solve this is to first select  men and then select  women not
related to the selected men. This shows that there are
 different choices.
1.12.
 The first term gives the number of
committees that have 3 women and 3 men; the second gives the number that
have 4 women and 2 men.
1.13.
 (number of solutions of 
 (number of solutions of
 (number of solutions of 
 = 
1.14.
 ince there are 
 positive vectors whose sum is  there must be
 such vectors. But 
 is the number of subsets of size 
from the set of numbers 
 in which  is the largest element in the
subset. Consequently, 
 is just the total number of subsets of size
 from a set of size  showing that the preceding answer is equal to 
1.15.
 Let us first determine the number of different results in which 
people pass. Because there are 
 different groups of size  and ! possible
ቆ9
2ቇ 7 ⋅5!
2!2!
9 ⋅8 ⋅7 ⋅6 ⋅5 ൅9 ቆ5
2ቇ⋅8 ⋅7 ⋅6 ൅ቆ9
2ቇ 7 ⋅5!
2!2!
6
 ቆ10
6 ቇ2଺
6
3
ቆ10
6 ቇቆ6
3ቇൌ
10!
4!3!3!
3
3
ቆ10
3 ቇቆ7
3ቇൌ
10!
3!3!4!
ቆ8
3ቇቆ7
3ቇ൅ቆ8
4ቇቆ7
2ቇൌ3430.
𝑥ଵ൅⋯൅𝑥ହൌ4ሻ
𝑥ଵ൅⋯൅𝑥ହൌ5ሻ
𝑥ଵ൅⋯൅𝑥ହൌ6ሻ
ቆ8
4ቇቆ9
4ቇቆ10
4 ቇ.
ቆ𝑗െ1
𝑛െ1ቇ
𝑗,
෍
௝ൌ௡
௞
ቆ𝑗െ1
𝑛െ1ቇ
ቆ𝑗െ1
𝑛െ1ቇ
𝑛
ሼ1, . . . ,𝑘ሽ
𝑗
෍
௝ൌ௡
௞
ቆ𝑗െ1
𝑛െ1ቇ
𝑛
𝑘,
ቆ𝑘
𝑛ቇ.
𝑘
ቆ𝑛
𝑘ቇ
𝑘
𝑘
735 of 848

orderings of their scores, it follows that there are 
 possible results in
which  people pass. Consequently, there are 
 possible results.
1.16.
 The number of subsets of size  is 
 Because the
number of these that contain none of the first five elements is 
the number that contain at least one is 3480. Another way to solve this
problem is to note that there are 
 that contain exactly  of the first
five elements and sum this for 
1.17.
 Multiplying both sides by  we must show that
This follows because the right side is equal to
For a combinatorial argument, consider a group of  items and a subgroup of
 of the  items. Then 
 is the number of subsets of size  that contain 
items from the subgroup of size 
 is the number that contain  item
from the subgroup, and 
 is the number that contain  items from the
subgroup. Adding these terms gives the total number of subgroups of size 
namely, 
1.18.
 There are  choices that can be made from families consisting of a
single parent and 1 child; there are 
 choices that can be made from
families consisting of a single parent and 2 children; there are 
choices that can be made from families consisting of 2 parents and a single
child; there are 
 choices that can be made from families
consisting of 2 parents and 2 children; there are 
 choices that can
be made from families consisting of 2 parents and 3 children. Hence, there
are 83 possible choices.
1.19.
 First choose the  positions for the digits, and then put in the letters
and digits. Thus, there are 
 different plates.
If the digits must be consecutive, then there are  possible positions for the
digits, showing that there are now 
 different
plates.
ቆ𝑛
𝑘ቇ𝑘!
𝑘
෍
௞ൌ଴
௡
ቆ𝑛
𝑘ቇ𝑘!
4
ቆ20
4 ቇൌ4845.
ቆ15
4 ቇൌ1365,
ቆ5
𝑖ቇቆ14
4 െ𝑖ቇ
𝑖
𝑖ൌ1, 2, 3, 4.
2,
𝑛ሺ𝑛െ1ሻൌ𝑘ሺ𝑘െ1ሻ൅2𝑘ሺ𝑛െ𝑘ሻ൅ሺ𝑛െ𝑘ሻሺ𝑛െ𝑘െ1ሻ
𝑘ଶሺ1 െ2 ൅1ሻ൅𝑘ሺെ1 ൅2𝑛െ𝑛െ𝑛൅1ሻ൅𝑛ሺ𝑛െ1ሻ
𝑛
𝑘
𝑛
ቆ𝑘
2ቇ
2
2
𝑘, 𝑘ሺ𝑛െ𝑘ሻ
1
ቆ𝑛െ𝑘
2
ቇ
0
2,
ቆ𝑛
2ቇ.
3
3 ⋅1 ⋅2 ൌ6
5 ⋅2 ⋅1 ൌ10
7 ⋅2 ⋅2 ൌ28
6 ⋅2 ⋅3 ൌ36
3
 ቆ8
3ቇ⋅26 ⋅25 ⋅24 ⋅23 ⋅22 ⋅10 ⋅9 ⋅8 
6
 6 ⋅26 ⋅25 ⋅24 ⋅23 ⋅22 ⋅10 ⋅9 ⋅8 
736 of 848

1.20.
a. Follows since 
 is the number of  letter permutations of the
values 
 in which  appears 
 times, 
b. 
1.21.
 giving that
𝑛!
𝑥ଵ!⋯𝑥௥!
𝑛
1, . . . ,𝑟
𝑖
𝑥௜
෍
௜ൌଵ
௥
 𝑥௜ൌ𝑛.
෍
௫భ൅. . . ൅௫ೝൌ௡
𝑛!
𝑥ଵ!⋯𝑥௥! ൌሺ1 ൅. . . ൅1ሻ௡ൌ𝑟௡.
ሺ1 െ1ሻ௡ൌ1 െቆ𝑛
1ቇ൅ቆ𝑛
2ቇ൅. . . ൅ሺെ1ሻ௡ቆ𝑛
𝑛ቇ,
ቆ𝑛
1ቇെቆ𝑛
2ቇ൅. . . ൅ሺെ1ሻ௡൅ଵቆ𝑛
𝑛ቇൌ1.
2.1.
a. 
b. 
c. 
d. 
e. 8
f. 
2.2.
 Let  be the event that a suit is purchased,  be the event that
a shirt is purchased, and  be the event that a tie is purchased. Then
a. 
b. The probability that two or more items are purchased is
Hence, the probability that exactly 1 item is purchased is
2.3.
 By symmetry, the 14th card is equally likely to be any of the 52
cards; thus, the probability is 4/52. A more formal argument is to count
the number of the 52! outcomes for which the 14th card is an ace. This
yields
Letting  be the event that the first ace occurs on the 14th card, we
have
2 ⋅3 ⋅4 ൌ24
2 ⋅3 ൌ6
3 ⋅4 ൌ12
𝐴𝐵ൌሼሺ𝑐,  pasta,  𝑖ሻ, ሺ𝑐,  rice,  𝑖ሻ, ሺ𝑐,  potatoes,  𝑖ሻሽ
𝐴𝐵𝐶ൌሼሺ𝑐,  rice,  𝑖ሻሽ
𝐴
𝐵
𝐶
𝑃ሺ𝐴∪𝐵∪𝐶ሻൌ.22 ൅.30 ൅.28 െ.11 െ.14 െ.10 ൅.06 ൌ.51
1 െ.51 ൌ.49
𝑃ሺ𝐴𝐵∪𝐴𝐶∪𝐵𝐶ሻൌ
.11 ൅.14 ൅.10 െ.06 െ.06
െ .06 ൅.06 ൌ.23
.51 െ.23 ൌ.28.
𝑝ൌ4  ⋅ 51  ⋅50 ⋯ 2  ⋅1
ሺ52ሻ!
ൌ4
52
𝐴
737 of 848

2.4.
 Let  denote the event that the minimum temperature is 70
degrees. Then
Since 
 and 
 subtracting one of the preceding
equations from the other yields
or 
2.5.
a. 
b. 
2.6.
 Let  be the event that both balls are red, and let  be the event
that both are black. Then
2.7.
a. 
b. 
c. 
2.8.
a. 
𝑃ሺ𝐴ሻൌ48 ⋅47 ⋯ 36 ⋅4
52 ⋅51 ⋯ 40 ⋅39 ൌ.0312
𝐷
𝑃ሺ𝐴∪𝐵ሻൌ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐵ሻെ𝑃ሺ𝐴𝐵ሻൌ.7 െ𝑃ሺ𝐴𝐵ሻ
𝑃ሺ𝐶∪𝐷ሻൌ𝑃ሺ𝐶ሻ൅𝑃ሺ𝐷ሻെ𝑃ሺ𝐶𝐷ሻൌ.2 ൅𝑃ሺ𝐷ሻെ𝑃ሺ𝐷𝐶ሻ
𝐴∪𝐵ൌ𝐶∪𝐷
𝐴𝐵ൌ𝐶𝐷,
0 ൌ.5 െ𝑃ሺ𝐷ሻ
𝑃ሺ𝐷ሻൌ.5.
52 ⋅48 ⋅44 ⋅40
52 ⋅51 ⋅50 ⋅49 ൌ.6761
52 ⋅39 ⋅26 ⋅13
52 ⋅51 ⋅50 ⋅49 ൌ.1055
𝑅
𝐵
𝑃ሺ𝑅∪𝐵ሻൌ𝑃ሺ𝑅ሻ൅𝑃ሺ𝐵ሻൌ3 ⋅4
6 ⋅10 ൅3 ⋅6
6 ⋅10 ൌ1/2
1
ቆ40
8 ቇ
ൌ1.3 ൈ10െ଼
ቆ8
7ቇቆ32
1 ቇ
ቆ40
8 ቇ
ൌ3.3 ൈ10െ଺
ቆ8
6ቇቆ32
2 ቇ
ቆ40
8 ቇ
൅1.3 ൈ10െ଼൅3.3 ൈ10െ଺ൌ1.8 ൈ10െସ
3  ⋅ 4 ⋅ 4  ⋅3
ቆ14
4 ቇ
ൌ.1439
738 of 848

b. 
c. 
2.9.
 Let 
 and consider the experiment of randomly
choosing an element of  Then 
 and the results
follow from Propositions 4.3 and 4.4.
2.10.
 Since there are 
 outcomes in which the position of
horse number 1 is specified, it follows that 
 Similarly,
 and 
 Hence, from Self-Test Problem
2.9
, we obtain 
2.11.
 One way to solve this problem is to start with the
complementary probability that at least one suit does not appear. Let
 be the event that no cards from suit  appear. Then
The desired probability is then 1 minus the preceding. Another way to
solve is to let A be the event that all 4 suits are represented, and then
use
where  (
), for instance, is the probability that the first card is
from a new suit, the second is from a new suit, the third is from a new
suit, the fourth is from an old suit (that is, one which has already
ቆ4
2ቇቆ4
2ቇ
ቆ14
4 ቇ
ൌ.0360
ቆ8
4ቇ
ቆ14
4 ቇ
ൌ.0699
𝑆ൌ
∪
௜ൌଵ
௡
𝐴௜,
𝑆.
𝑃ሺ𝐴ሻൌ𝑁ሺ𝐴ሻ/𝑁ሺ𝑆ሻ,
5! ൌ120
𝑁ሺ𝐴ሻൌ360.
𝑁ሺ𝐵ሻൌ120,
𝑁ሺ𝐴𝐵ሻൌ2 ⋅4! ൌ48.
𝑁ሺ𝐴∪𝐵ሻൌ432.
𝐴௜, 𝑖ൌ1, 2, 3, 4,
𝑖
𝑃ቆ
∪
௜ൌଵ
ସ
𝐴௜ቇൌ
෍
௜
𝑃ሺ𝐴௜ሻെ෍
௝
෍
௜: ௜ழ௝
𝑃൫𝐴௜𝐴௝൯
൅⋯െ𝑃ሺ𝐴ଵ𝐴ଶ𝐴ଷ𝐴ସሻ
ൌ4
ቆ39
5 ቇ
ቆ52
5 ቇ
െቆ4
2ቇ
ቆ26
5 ቇ
ቆ52
5 ቇ
൅ቆ4
3ቇ
ቆ13
5 ቇ
ቆ52
5 ቇ
ൌ4
ቆ39
5 ቇ
ቆ52
5 ቇ
െ6
ቆ26
5 ቇ
ቆ52
5 ቇ
൅4
ቆ13
5 ቇ
ቆ52
5 ቇ
𝑃ሺ𝐴ሻ
ൌ
𝑃ሺ𝑛,  𝑛,  𝑛,  𝑛,  𝑜ሻ൅𝑃ሺ𝑛,  𝑛,  𝑛,  𝑜,  𝑛ሻ൅𝑃ሺ𝑛,  𝑛,  𝑜,  𝑛,  𝑛ሻ
൅𝑃ሺ𝑛,  𝑜,  𝑛,  𝑛, 𝑛ሻ
𝑃𝑛, 𝑛, 𝑛, 𝑜, 𝑛
739 of 848

appeared) and the fifth is from a new suit. This gives
2.12.
 There are 
 different divisions of the 10 players into a
first roommate pair, a second roommate pair, and so on. Hence, there
are 
 divisions into 5 roommate pairs. There are 
ways of choosing the frontcourt and backcourt players to be in the
mixed roommate pairs and then 2 ways of pairing them up. As there is
then 1 way to pair up the remaining two backcourt players and
 ways of making two roommate pairs from the remaining
four frontcourt players, the desired probability is
2.13.
 Let  denote the event that letter  is repeated; similarly,
define the events  and 
 Then
2.14.
 Let 
 Then
where the final equality uses the fact that the 
 are mutually exclusive.
The inequality then follows, since 
2.15.
𝑃ሺ𝐴ሻൌ
52  ⋅ 39  ⋅ 26  ⋅ 13  ⋅ 48  ൅ 52  ⋅ 39  ⋅ 26  ⋅ 36  ⋅ 13
52  ⋅ 51  ⋅ 50  ⋅ 49  ⋅ 48
൅52  ⋅ 39  ⋅ 24  ⋅ 26  ⋅ 13  ൅ 52  ⋅ 12  ⋅ 39  ⋅ 26  ⋅ 13
52  ⋅ 51  ⋅ 50  ⋅ 49  ⋅ 48
ൌ
52  ⋅ 39  ⋅ 26  ⋅ 13ሺ48  ൅ 36  ൅ 24  ൅ 12ሻ
52  ⋅ 51  ⋅ 50  ⋅ 49  ⋅ 48
ൌ
.2637
ሺ10ሻ!/2ହ
ሺ10ሻ!/ሺ5!2ହሻ
ቆ6
2ቇቆ4
2ቇ
4!/ሺ2!2ଶሻൌ3
𝑃ሼ2 mixed pairsሽൌ
ቆ6
2ቇ ቆ4
2ቇ ሺ2ሻሺ3ሻ
ሺ10ሻ!/ሺ5!2ହሻൌ.5714
𝑅
𝑅
𝐸
𝑉.
𝑃ሼsame letterሽ
ൌ
𝑃ሺ𝑅ሻ  ൅ 𝑃ሺ𝐸ሻ  ൅ 𝑃ሺ𝑉ሻ
ൌ
2
7
1
8   ൅  3
7
1
8   ൅  1
7
1
8 ൌ3
28
𝐵ଵൌ𝐴ଵ, 𝐵௜ൌ𝐴௜ቆ
∪
௝ൌଵ
௜െଵ
𝐴௝ቇ
௖
,𝑖൐1.
𝑃൬
∪
௜ൌଵ
ஶ
𝐴௜൰
ൌ
𝑃൬
∪
௜ൌଵ
ஶ
𝐵௜൰
ൌ
෍
௜ൌଵ
ஶ
𝑃ሺ𝐵௜ሻ
൑
෍
௜ൌଵ
ஶ
𝑃ሺ𝐴௜ሻ
𝐵௜
𝐵௜⊂𝐴௜.
740 of 848

2.16.
 The number of partitions for which 
 is a subset is equal to
the number of partitions of the remaining 
 elements into 
nonempty subsets, namely, 
 Because there are 
partitions of 
 into  nonempty subsets and then a choice of
 of them in which to place element 1, it follows that there are
 partitions for which 
 is not a subset. Hence, the result
follows.
2.17.
 Let 
 denote, respectively, the events that there are no
red, no white, and no blue balls chosen. Then
Thus, the probability that all colors appear in the chosen subset is
approximately 
2.18.
a. 
b. Because there are  nonblue balls, the probability is
c. Because there are 
 possible orderings of the different colors
and all possibilities for the final  balls are equally likely, the
probability is 
𝑃൬
∩
௜ൌଵ
ஶ
𝐴௜൰ൌ
1 െ𝑃൬൬
∩
௜ൌଵ
ஶ
𝐴௜൰
௖
൰
ൌ
1 െ𝑃൬
∪
௜ൌଵ
ஶ
𝐴௜
௖൰
൒
1 െ෍
௜ൌଵ
ஶ
𝑃ሺ𝐴௜
௖ሻ
ൌ
1
ሼ1ሽ
𝑛െ1
𝑘െ1
𝑇௞െଵሺ𝑛െ1ሻ.
𝑇௞ሺ𝑛െ1ሻ
ሼ2, . . . , 𝑛െ1ሽ
𝑘
𝑘
𝑘𝑇௞ሺ𝑛െ1ሻ
ሼ1ሽ
𝑅, 𝑊, 𝐵
𝑃ሺ𝑅∪𝑊∪𝐵ሻൌ
𝑃ሺ𝑅ሻ൅𝑃ሺ𝑊ሻ൅𝑃ሺ𝐵ሻെ𝑃ሺ𝑅𝑊ሻ
െ𝑃ሺ𝑅𝐵ሻെ𝑃ሺ𝑊𝐵ሻ൅𝑃ሺ𝑅𝑊𝐵ሻ
ൌ
ቆ13
5 ቇ
ቆ18
5 ቇ
  ൅ 
ቆ12
5 ቇ
ቆ18
5 ቇ
  ൅ 
ቆ11
5 ቇ
ቆ18
5 ቇ
  െ 
ቆ7
5ቇ
ቆ18
5 ቇ
െ 
ቆ6
5ቇ
ቆ18
5 ቇ
  െ 
ቆ5
5ቇ
ቆ18
5 ቇ
ൎ
0.2933
1 െ0.2933 ൌ0.7067.
8 ⋅7 ⋅6 ⋅5 ⋅4
17 ⋅16 ⋅15 ⋅14 ⋅13 ൌ
2
221
9
 
9 ⋅8 ⋅7 ⋅6 ⋅5
17 ⋅16 ⋅15 ⋅14 ⋅13 ൌ
9
442 .
3!
3
  3! ⋅4 ⋅8 ⋅5
17 ⋅16 ⋅15 ൌ4
17 .
741 of 848

d. The probability that the red balls are in a specified 4 spots is
 Because there are 
 possible locations of the
red balls where they are all together, the probability is
2.19.
a. The probability that the 10 cards consist of  spades,  hearts, 
diamonds, and  club is 
 Because there
are 
 possible choices of the suits to have 
 and  cards,
respectively, it follows that the probability is
b. Because there are 
 choices of the two suits that are to
have  cards and then  choices for the suit to have  cards, the
probability is 
2.20.
 All the red balls are removed before all the blue ones if and
only if the very last ball removed is blue. Because all 
 balls are
equally likely to be the last ball removed, the probability is 
 
4 ⋅3 ⋅2 ⋅1
17 ⋅16 ⋅15 ⋅14 . 
14
  14 ⋅4 ⋅3 ⋅2 ⋅1
17 ⋅16 ⋅15 ⋅14 ൌ
1
170 . 
4
3
2
1
 
ቆ13
4 ቇቆ13
3 ቇቆ13
2 ቇቆ13
1 ቇ
ቆ52
10ቇ
. 
4!
4,3,2,
1
 
24ቆ13
4 ቇቆ13
3 ቇቆ13
2 ቇቆ13
1 ቇ
ቆ52
10ቇ
. 
ቆ4
2ቇൌ6
3
2
4
 
12ቆ13
3 ቇቆ13
3 ቇቆ13
4 ቇ
ቆ52
10ቇ
. 
30
10/30.
3.1.
a. 
b. 
𝑃ሺno acesሻൌቆ35
13ቇቆ39
13ቇ
1 െ𝑃ሺno acesሻെ
4 ቆ35
12ቇ
ቆ39
13ቇ
742 of 848

c. 
3.2.
 Let 
 denote the event that the life of the battery is greater than
a. 
b. 
3.3.
 Put 1 white and 0 black balls in urn one, and the remaining 9 white and
10 black balls in urn two.
3.4.
 Let  be the event that the transferred ball is white, and let 
 be the
event that a white ball is drawn from urn . Then
3.5.
a. 
 since 
 and
 because  and  are mutually exclusive.
b. 
3.6.
 Let 
 denote the event that ball  is black, and let 
 Then
3.7.
 Let  denote the event that both cards are aces.
𝑃ሺ𝑖 acesሻൌ
ቆ3
𝑖ቇቆ36
13 െ𝑖ቇ
ቆ39
13ቇ
𝐿௜
10,000 ൈ𝑖 miles.
𝑃ሺ𝐿ଶ|𝐿ଵሻൌ𝑃ሺ𝐿ଵ𝐿ଶሻ/𝑃ሺ𝐿ଵሻൌ𝑃ሺ𝐿ଶሻ/𝑃ሺ𝐿ଵሻൌ1/2
𝑃ሺ𝐿ଷ|𝐿ଵሻൌ𝑃ሺ𝐿ଵ𝐿ଷሻ/𝑃ሺ𝐿ଵሻൌ𝑃ሺ𝐿ଷሻ/𝑃ሺ𝐿ଵሻൌ1/8
𝑇
𝑊
𝐵
𝑃ሺ𝑇||𝑊ሻ
ൌ
𝑃ሺ𝑊||𝑇ሻ𝑃ሺ𝑇ሻ
𝑃ሺ𝑊||𝑇ሻ𝑃ሺ𝑇ሻ൅𝑃ሺ𝑊||𝑇௖ሻ𝑃ሺ𝑇௖ሻ
ൌ
ሺ2/7ሻሺ2/3ሻ
ሺ2/7ሻሺ2/3ሻ൅ሺ1/7ሻሺ1/3ሻൌ4/5
𝑃ሺ𝐸|𝐸∪𝐹ሻൌ𝑃ሺ𝐸ሺ𝐸∪𝐹ሻሻ
𝑃ሺ𝐸∪𝐹ሻ
ൌ
𝑃ሺ𝐸ሻ
𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻ
𝐸ሺ𝐸∪𝐹ሻൌ𝐸
𝑃ሺ𝐸∪𝐹ሻൌ𝑃ሺ𝐸ሻ൅𝑃ሺ𝐹ሻ
𝐸
𝐹
𝑃൫𝐸௝ห∪௜ൌଵ
ஶ
𝐸௜൯ൌ𝑃൫𝐸௝ሺ∪௜ൌଵ
ஶ
𝐸௜ሻ൯
𝑃ሺ∪௜ൌଵ
ஶ
𝐸௜ሻ
ൌ
𝑃൫𝐸௝൯
෍
௜ൌଵ
ஶ
𝑃ሺ𝐸௜ሻ
𝐵௜
𝑖
𝑅௜ൌ𝐵௜
௖.
𝑃ሺ𝐵ଵ|𝑅ଶሻൌ
𝑃ሺ𝑅ଶ||𝐵ଵሻ𝑃ሺ𝐵ଵሻ
𝑃ሺ𝑅ଶ||𝐵ଵሻ𝑃ሺ𝐵ଵሻ൅𝑃ሺ𝑅ଶ||𝑅ଵሻ𝑃ሺ𝑅ଵሻ
ൌ
ሾ𝑟/ሾሺ𝑏൅𝑟൅𝑐ሻሿሾ𝑏/ሺ𝑏൅𝑟ሻሿ
ሾ𝑟/ሺ𝑏൅𝑟൅𝑐ሻሿሾ𝑏/ሺ𝑏൅𝑟ሻሿ൅ሾሺ𝑟൅𝑐ሻ/ሺ𝑏൅𝑟൅𝑐ሻሿሾ𝑟/ሺ𝑏൅𝑟ሻሿ
ൌ
𝑏
𝑏൅𝑟൅𝑐
𝐵
743 of 848

a. 
b. Since the second card is equally likely to be any of the remaining 51, of
which 3 are aces, we see that the answer in this situation is also 3/51.
c. Because we can always interchange which card is considered first and
which is considered second, the result should be the same as in part
(b). A more formal argument is as follows:
d. 
3.8.
Hypothesis  is 1.5 times as likely.
3.9.
 Let  denote the event that the plant is alive and let 
 be the event
that it was watered.
a. 
b. 
3.10.
a. Let  be the event that at least one red ball is chosen. Then
𝑃ሼ𝐵|yes to ace of spadesሽ
ൌ
𝑃ሼ𝐵, yes to ace of spadesሽ
𝑃ሼyes to ace of spadesሽ
ൌ
ቆ1
1ቇቆ3
1ቇ
ቆ52
2ቇ
/
ቆ1
1ቇቆ51
1ቇ
ቆ52
2ቇ
ൌ3/51
𝑃ሼ𝐵|second is aceሽ
ൌ
𝑃ሼ𝐵, second is aceሽ
𝑃ሼsecond is aceሽ
ൌ
𝑃ሺ𝐵ሻ
𝑃ሺ𝐵ሻ൅𝑃ሼϐirst is not ace, second is aceሽ
ൌ
ሺ4/52ሻሺ3/51ሻ
ሺ4/52ሻሺ3/51ሻ൅ሺ48/52ሻሺ4/51ሻ
ൌ
3/51
𝑃ሼ𝐵|at least oneሽ
ൌ
𝑃ሺ𝐵ሻ
𝑃ሼat least oneሽ
ൌ
ሺ4/52ሻሺ3/51ሻ
1 െሺ48/52ሻሺ47/51ሻ
ൌ
1/33
𝑃ሺ𝐻||𝐸ሻ
𝑃ሺ𝐺||𝐸ሻൌ𝑃ሺ𝐻𝐸ሻ
𝑃ሺ𝐺𝐸ሻൌ𝑃ሺ𝐻ሻ𝑃ሺ𝐸||𝐻ሻ
𝑃ሺ𝐺ሻ𝑃ሺ𝐸||𝐺ሻ
𝐻
𝐴
𝑊
𝑃ሺ𝐴ሻൌ
𝑃ሺ𝐴||𝑊ሻ𝑃ሺ𝑊ሻ൅𝑃ሺ𝐴||𝑊௖ሻ𝑃ሺ𝑊௖ሻ
ൌ
ሺ.85ሻ ሺ.9ሻ൅ሺ.2ሻ ሺ.1ሻൌ.785
𝑃ሺ𝑊௖||𝐴௖ሻൌ
𝑃ሺ𝐴௖||𝑊௖ሻ𝑃ሺ𝑊௖ሻ
𝑃ሺ𝐴௖ሻ
ൌ
ሺ.8ሻሺ.1ሻ
.215
ൌ16
43
𝑅
744 of 848

b. Let 
 be the event there are exactly  green balls chosen. Working
with the reduced sample space yields
3.11.
 Let 
 be the event that the battery works, and let  and  denote the
events that the battery is a type  and that it is a type  battery, respectively.
a. 
b. 
3.12.
 Let 
 be the event that Maria likes book 
 Then
Using that 
 is the union of the mutually exclusive events 
 and 
 we
see that
Thus,
3.13.
a. This is the probability that the last ball removed is blue. Because each
of the 
 balls is equally likely to be the last one removed, the
probability is 
b. This is the probability that the last red or blue ball to be removed is a
blue ball. Because it is equally likely to be any of the 
 red or blue
balls, the probability that it is blue is 
c. Let 
 denote, respectively, the events that the first color
removed is blue, the second is red, and the third is green. Then
where 
 is just the probability that the very last ball is green and
 is computed by noting that given that the last ball is green,
𝑃ሺ𝑅ሻൌ1 െ𝑃ሺ𝑅௖ሻൌ1 െ
ቆ22
6 ቇ
ቆ30
6 ቇ
𝐺ଶ
2
𝑃ሺ𝐺ଶ|𝑅௖ሻൌ
ቆ10
2 ቇቆ12
4 ቇ
ቆ22
6 ቇ
𝑊
𝐶
𝐷
𝐶
𝐷
 𝑃ሺ𝑊ሻൌ𝑃ሺ𝑊||𝐶ሻ𝑃ሺ𝐶ሻ൅𝑃ሺ𝑊||𝐷ሻ𝑃ሺ𝐷ሻൌ.7ሺ8/14ሻ൅.4ሺ6/14ሻൌ4/7
𝑃ሺ𝐶|𝑊௖ሻൌ𝑃ሺ𝐶𝑊௖ሻ
𝑃ሺ𝑊௖ሻൌ𝑃ሺ𝑊௖||𝐶ሻ𝑃ሺ𝐶ሻ
3/7
ൌ.3ሺ8/14ሻ
3/7
ൌ.4
𝐿௜
𝑖,𝑖ൌ1,2.
𝑃ሺ𝐿ଶ|𝐿ଵ
௖ሻൌ𝑃ሺ𝐿ଵ
௖𝐿ଶሻ
𝑃ሺ𝐿ଵ
௖ሻ
ൌ𝑃ሺ𝐿ଵ
௖𝐿ଶሻ
.4
𝐿ଶ
𝐿ଵ𝐿ଶ
𝐿ଵ
௖𝐿ଶ,
.5 ൌ𝑃ሺ𝐿ଶሻൌ𝑃ሺ𝐿ଵ𝐿ଶሻ൅𝑃ሺ𝐿ଵ
௖𝐿ଶሻൌ.4 ൅𝑃ሺ𝐿ଵ
௖𝐿ଶሻ
𝑃ሺ𝐿ଶ||𝐿ଵ
௖ሻൌ.1
.4 ൌ.25
30
1/3.
30
1/3.
𝐵ଵ,  𝑅ଶ,  𝐺ଷ
𝑃ሺ𝐵ଵ𝑅ଶ𝐺ଷሻൌ𝑃ሺ𝐺ଷሻ𝑃ሺ𝑅ଶ|𝐺ଷሻ𝑃ሺ𝐵ଵ|𝑅ଶ𝐺ଷሻൌ8
38
20
30 ൌ8
57
𝑃ሺ𝐺ଷሻ
𝑃ሺ𝑅ଶ||𝐺ଷሻ
745 of 848

each of the 
 red and 
 blue balls is equally likely to be the last of
that group to be removed, so the probability that it is one of the red
balls is 
 (Of course, 
d. 
3.14.
 Let  be the event that the coin lands heads, let 
 be the event that
 is told that the coin landed heads, let  be the event that  forgets the result
of the toss, and let  be the event that  is told the correct result. Then
a. 
b. 
c. 
Now,
giving the result 
3.15.
 Since the black rat has a brown sibling, we can conclude that both of
its parents have one black and one brown gene.
a. 
b. Let  be the event that all 5 offspring are black, let 
 be the event that
the black rat has 2 black genes, and let 
 be the event that it has 1
black and 1 brown gene. Then
3.16.
 Let  be the event that a current flows from  to 
 and let 
 be the
event that relay  closes. Then
Now,
20
10
20/30.
𝑃ሺ𝐵ଵ|𝑅ଶ𝐺ଷሻൌ1.ሻ
 𝑃ሺ𝐵ଵሻൌ𝑃ሺ𝐵ଵ𝐺ଶ𝑅ଷሻ൅𝑃ሺ𝐵ଵ𝑅ଶ𝐺ଷሻൌ20
38
8
18 ൅8
57 ൌ64
171
𝐻
𝑇௛
𝐵
𝐹
𝐴
𝐶
𝐵
𝑃ሺ𝑇௛ሻൌ
𝑃ሺ𝑇௛|𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝑇௛|𝐹௖ሻ𝑃ሺ𝐹௖ሻ
ൌ
ሺ.5ሻሺ.4ሻ൅𝑃ሺ𝐻ሻሺ.6ሻ
ൌ
.68
𝑃ሺ𝐶ሻൌ
𝑃ሺ𝐶|𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐶|𝐹௖ሻ𝑃ሺ𝐹௖ሻ
ൌ
ሺ.5ሻ ሺ.4ሻ൅1 ሺ.6ሻൌ.80
𝑃ሺ𝐻|𝑇௛ሻൌ𝑃ሺ𝐻𝑇௛ሻ
𝑃ሺ𝑇௛ሻ
𝑃ሺ𝐻𝑇௛ሻ
ൌ𝑃ሺ𝐻𝑇௛|𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐻𝑇௛|𝐹௖ሻ𝑃ሺ𝐹௖ሻ
ൌ𝑃ሺ𝐻|𝐹ሻ𝑃ሺ𝑇௛|𝐻𝐹ሻ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐻ሻ𝑃ሺ𝐹௖ሻ
ൌሺ.8ሻ ሺ.5ሻ ሺ.4ሻ  ൅ ሺ.8ሻ ሺ.6ሻൌ.64
 𝑃ሺ𝐻||𝑇௛ሻൌ.64/.68 ൌ16/17.
𝑃ሺ2 black | at least oneሻൌ
𝑃ሺ2ሻ
𝑃ሺat least oneሻൌ1/4
3/4 ൌ1
3
𝐹
𝐵ଶ
𝐵ଵ
𝑃ሺ𝐵ଶ|𝐹ሻൌ
𝑃ሺ𝐹|𝐵ଶሻ𝑃ሺ𝐵ଶሻ
𝑃ሺ𝐹|𝐵ଶሻ𝑃ሺ𝐵ଶሻ൅𝑃ሺ𝐹|𝐵ଵሻ𝑃ሺ𝐵ଵሻ
ൌ
ሺ1ሻሺ1/3ሻ
ሺ1ሻሺ1/3ሻ൅ሺ1/2ሻହሺ2/3ሻ
ൌ16
17
𝐹
𝐴
𝐵,
𝐶௜
𝑖
𝑃ሺ𝐹ሻൌ𝑃ሺ𝐹|𝐶ଵሻ𝑝ଵ൅𝑃ሺ𝐹|𝐶ଵ
௖ሻ൫1 െ𝑝ଵ൯
746 of 848

Also,
Hence, for part (a), we obtain
For part (b), let 
 Then
3.17.
 Let  be the event that component 1 is working, and let  be the
event that the system functions.
a. 
where 
 was computed by noting that it is equal to 1 minus the
probability that components 1 and 2 are both failed.
b. 
where 
 was computed by noting that it is equal to the probability
that all 3 components work plus the three probabilities relating to
exactly 2 of the components working.
3.18.
 If we assume that the outcomes of the successive spins are
independent, then the conditional probability of the next outcome is
unchanged by the result that the previous 10 spins landed on black.
3.19.
 Condition on the outcome of the initial tosses:
so,
𝑃ሺ𝐹|𝐶ଵሻ
ൌ𝑃ሺ𝐶ସ∪𝐶ଶ𝐶ହ∪𝐶ଷ𝐶ହሻ
ൌ𝑝ସ൅𝑝ଶ𝑝ହ൅𝑝ଷ𝑝ହെ𝑝ସ𝑝ଶ𝑝ହ
െ𝑝ସ𝑝ଷ𝑝ହെ𝑝ଶ𝑝ଷ𝑝ହ൅𝑝ସ𝑝ଶ𝑝ହ𝑝ଷ
𝑃ሺ𝐹|𝐶ଵ
௖ሻൌ
𝑃ሺ𝐶ଶ𝐶ହ∪𝐶ଶ𝐶ଷ𝐶ସሻ
ൌ
𝑝ଶ𝑝ହ൅𝑝ଶ𝑝ଷ𝑝ସെ𝑝ଶ𝑝ଷ𝑝ସ𝑝ହ
𝑃ሺ𝐹ሻ
ൌ
𝑝ଵሺ𝑝ସ൅𝑝ଶ𝑝ହ൅𝑝ଷ𝑝ହെ𝑝ସ𝑝ଶ𝑝ହ
െ𝑝ସ𝑝ଷ𝑝ହെ𝑝ଶ𝑝ଷ𝑝ହ൅𝑝ସ𝑝ଶ𝑝ହ𝑝ଷሻ
൅ሺ1 െ𝑝ଵሻ𝑝ଶሺ𝑝ହ൅𝑝ଷ𝑝ସെ𝑝ଷ𝑝ସ𝑝ହሻ
𝑞௜ൌ1 െ𝑝௜.
𝑃ሺ𝐶ଷ|𝐹ሻൌ
𝑃ሺ𝐹|𝐶ଷሻ𝑃ሺ𝐶ଷሻ/𝑃ሺ𝐹ሻ
ൌ
𝑝ଷሾ1 െ𝑃ሺ𝐶ଵ
௖𝐶ଶ
௖∪𝐶ସ
௖𝐶ହ
௖ሻሿ/𝑃ሺ𝐹ሻ
ൌ
𝑝ଷ൫1 െ𝑞ଵ𝑞ଶെ𝑞ସ𝑞ହ൅𝑞ଵ𝑞ଶ𝑞ସ𝑞ହ൯/𝑃ሺ𝐹ሻ
𝐴
𝐹
𝑃ሺ𝐴|𝐹ሻൌ𝑃ሺ𝐴𝐹ሻ
𝑃ሺ𝐹ሻൌ𝑃ሺ𝐴ሻ
𝑃ሺ𝐹ሻൌ
1/2
1 െሺ1/2ሻଶൌ2
3
𝑃ሺ𝐹ሻ
𝑃ሺ𝐴|𝐹ሻൌ𝑃ሺ𝐴𝐹ሻ
𝑃ሺ𝐹ሻൌ𝑃ሺ𝐹|𝐴ሻ𝑃ሺ𝐴ሻ
𝑃ሺ𝐹ሻ
ൌ
ሺ3/4ሻሺ1/2ሻ
ሺ1/2ሻଷ൅3ሺ1/2ሻଷൌ3
4
𝑃ሺ𝐹ሻ
𝑃ሺ𝐴 oddሻൌ
𝑃ଵሺ1 െ𝑃ଶሻ ሺ1 െ𝑃ଷሻ൅ሺ1 െ𝑃ଵሻ𝑃ଶ𝑃ଷ
൅ 𝑃ଵ𝑃ଶ𝑃ଷ𝑃ሺ𝐴 oddሻ
൅ ሺ1 െ𝑃ଵሻ ሺ1 െ𝑃ଶሻ ሺ1 െ𝑃ଷሻ𝑃ሺ𝐴 oddሻ
747 of 848

3.20.
 Let  and  be the events that the first trial is larger and that the
second is larger, respectively. Also, let  be the event that the results of the
trials are equal. Then
But, by symmetry, 
 thus,
Another way of solving the problem is to note that
To see that the two expressions derived for 
 are equal, observe that
3.21.
 Let 
 then
Now, by symmetry,
𝑃ሺ𝐴 oddሻൌ𝑃ଵሺ1 െ𝑃ଶሻ ሺ1 െ𝑃ଷሻ൅ሺ1 െ𝑃ଵሻ𝑃ଶ𝑃ଷ
𝑃ଵ൅𝑃ଶ൅𝑃ଷെ𝑃ଵ𝑃ଶെ𝑃ଵ𝑃ଷെ𝑃ଶ𝑃ଷ
𝐴
𝐵
𝐸
1 ൌ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐵ሻ൅𝑃ሺ𝐸ሻ
𝑃ሺ𝐴ሻൌ𝑃ሺ𝐵ሻ:
𝑃ሺ𝐵ሻൌ1 െ𝑃ሺ𝐸ሻ
2
ൌ
1 െ෍
௜ൌଵ
௡
𝑝௜
ଶ
2
𝑃ሺ𝐵ሻൌ
෍
௜
෍
௝வ௜
𝑃ሼϐirst trial results in 𝑖,  second trial results in 𝑗ሽ
ൌ
෍
௜
෍
௝வ௜
𝑝௜𝑝௝
𝑃ሺ𝐵ሻ
1
ൌ
෍
௜ൌଵ
௡
𝑝௜෍
௝ൌଵ
௡
𝑝௝
ൌ
෍
௜
෍
௝
𝑝௜𝑝௝
ൌ
෍
௜
𝑝௜
ଶ൅෍
௜
෍
௝ஷ௜
𝑝௜𝑝௝
ൌ
෍
௜
𝑝௜
ଶ൅2෍
௜
෍
௝வ௜
𝑝௜𝑝௝
𝐸ൌሼ𝐴 gets more heads than 𝐵ሽ;
𝑃ሺ𝐸ሻ
ൌ𝑃ሺ𝐸|𝐴 leads after both ϐlip 𝑛ሻ𝑃ሺ𝐴 leads after both ϐlip 𝑛ሻ
൅ 𝑃ሺ𝐸|even after both ϐlip 𝑛ሻ𝑃ሺeven after both ϐlip 𝑛ሻ
൅ 𝑃ሺ𝐸|𝐵 leads after both ϐlip 𝑛ሻ𝑃ሺ𝐵 leads after both ϐlip 𝑛ሻ
ൌ𝑃ሺ𝐴 leadsሻ൅1
2 𝑃ሺevenሻ
748 of 848

Hence,
3.22.
a. Not true: In rolling 2 dice, let 
 and 
2nd die does not land on 3
Then
b. 
c. 
3.23.
a. necessarily false; if they were mutually exclusive, then we would have
b. necessarily false; if they were independent, then we would have
c. necessarily false; if they were mutually exclusive, then we would have
d. possibly true
3.24.
 The probabilities in parts (a), (b), and (c) are 
 and
 respectively.
𝑃ሺ𝐴 leadsሻ
ൌ
𝑃ሺ𝐵 leadsሻ
ൌ
1 െ𝑃ሺevenሻ
2
𝑃ሺ𝐸ሻൌ1
2
𝐸ൌሼsum is 7ሽ,
𝐹ൌሼ1st die does not land on 4ሽ,
𝐺ൌሼ
ሽ.
𝑃ሺ𝐸|𝐹∪𝐺ሻൌ𝑃ሼ7, not  ሺ4, 3ሻሽ
𝑃ሼnot  ሺ4, 3ሻሽ
ൌ5/36
35/36 ൌ5/35 ്𝑃ሺ𝐸ሻ
𝑃ሺ𝐸ሺ𝐹∪𝐺ሻሻൌ
𝑃ሺ𝐸𝐹∪𝐸𝐺ሻ
ൌ
𝑃ሺ𝐸𝐹ሻ൅𝑃ሺ𝐸𝐺ሻ
since 𝐸𝐹𝐺ൌ0
ൌ
𝑃ሺ𝐸ሻሾ𝑃ሺ𝐹ሻ൅𝑃ሺ𝐺ሻሿ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹∪𝐺ሻ
since 𝐹𝐺ൌ0
𝑃ሺ𝐺|𝐸𝐹ሻൌ
𝑃ሺ𝐸𝐹𝐺ሻ
𝑃ሺ𝐸𝐹ሻ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹𝐺ሻ
𝑃ሺ𝐸𝐹ሻ
            since 𝐸 is independent of 𝐹𝐺
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ𝑃ሺ𝐺ሻ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ
    by independence
ൌ
𝑃ሺ𝐺ሻ.
0 ൌ𝑃ሺ𝐴𝐵ሻ്𝑃ሺ𝐴ሻ𝑃ሺ𝐵ሻ
𝑃ሺ𝐴𝐵ሻൌ𝑃ሺ𝐴ሻ𝑃ሺ𝐵ሻ൐0
𝑃ሺ𝐴∪𝐵ሻൌ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐵ሻൌ1.2
.5,ሺ.8ሻଷൌ.512,
ሺ.9ሻ଻ൎ.4783,
749 of 848

3.25.
 Let 
 denote the event that radio  is defective. Also, let 
and  be the events that the radios were produced at factory  and at factory
 respectively. Then
3.26.
 We are given that 
 and must show that this implies that
 One way is as follows:
3.27.
 The result is true for 
 With 
 denoting the event that there are 
red balls in the urn after stage  assume that
Now let 
 denote the event that there are  red balls in the
urn after stage 
 Then
Because there are 
 balls in the urn after stage  it follows that
 is the probability that a red ball is chosen when 
 of the 
balls in the urn are red and 
 is the probability that a red ball is not
chosen when  of the 
 balls in the urn are red. Consequently,
𝐷௜,𝑖ൌ1,2,
𝑖
𝐴
𝐵
𝐴
𝐵,
𝑃ሺ𝐷ଶ|𝐷ଵሻൌ
𝑃ሺ𝐷ଵ𝐷ଶሻ
𝑃ሺ𝐷ଵሻ
ൌ
𝑃ሺ𝐷ଵ𝐷ଶ|𝐴ሻ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐷ଵ𝐷ଶ|𝐵ሻ𝑃ሺ𝐵ሻ
𝑃ሺ𝐷ଵ|𝐴ሻ𝑃ሺ𝐴ሻ൅𝑃ሺ𝐷ଵ|𝐵ሻ𝑃ሺ𝐵ሻ
ൌ
ሺ.05ሻଶሺ1/2ሻ൅ሺ.01ሻଶሺ1/2ሻ
ሺ.05ሻሺ1/2ሻ൅ሺ.01ሻሺ1/2ሻ
ൌ
13/300
𝑃ሺ𝐴𝐵ሻൌ𝑃ሺ𝐵ሻ
𝑃ሺ𝐵௖𝐴௖ሻൌ𝑃ሺ𝐴௖ሻ.
𝑃ሺ𝐵௖𝐴௖ሻൌ
𝑃൫ሺ𝐴∪𝐵ሻ௖൯
ൌ
1 െ𝑃ሺ𝐴∪𝐵ሻ
ൌ
1 െ𝑃ሺ𝐴ሻെ𝑃ሺ𝐵ሻ൅𝑃ሺ𝐴𝐵ሻ
ൌ
1 െ𝑃ሺ𝐴ሻ
ൌ
𝑃ሺ𝐴௖ሻ
𝑛ൌ0.
𝐴௜
𝑖
𝑛,
𝑃ሺ𝐴௜ሻൌ
1
𝑛൅1 ,  𝑖ൌ1, . . . , 𝑛൅1
𝐵௝, 𝑗ൌ1, . . . , 𝑛൅2,
𝑗
𝑛൅1.
𝑃ሺ𝐵௝ሻൌ
෍
௜ൌଵ
௡൅ଵ
𝑃൫𝐵௝ห𝐴௜൯𝑃ሺ𝐴௜ሻ
ൌ
1
𝑛൅1
෍
௜ൌଵ
௡൅ଵ
𝑃൫𝐵௝ห𝐴௜൯
ൌ
1
𝑛൅1 ൣ𝑃൫𝐵௝ห𝐴௝െଵ൯൅𝑃൫𝐵௝ห𝐴௜൯൧
𝑛൅2
𝑛,
𝑃൫𝐵௝ห𝐴௝െଵ൯
𝑗െ1
𝑛൅2
𝑃ሺ𝐵௝ห𝐴௝ሻ
𝑗
𝑛൅2
𝑃൫𝐵௝ห𝐴௝െଵ൯ൌ𝑗െ1
𝑛൅2,
𝑃൫𝐵௝ห𝐴௝൯ൌ𝑛൅2 െ𝑗
𝑛൅2
750 of 848

Substituting these results into the equation for 
 gives
This completes the induction proof.
3.28.
 If 
 is the event that player  receives an ace, then
By arbitrarily numbering the aces and noting that the player who does not
receive ace number one will receive  of the remaining 
 cards, we see
that
Therefore,
We may regard the card division outcome as the result of two trials, where
trial 
 is said to be a success if ace number  goes to the first player.
Because the locations of the two aces become independent as  goes to
infinity, with each one being equally likely to be given to either player, it follows
that the trials become independent, each being a success with probability 1/2.
Hence, in the limiting case where 
 the problem becomes one of
determining the conditional probability that two heads result, given that at
least one does, when two fair coins are flipped. Because 
 converges to
1/3, the answer agrees with that of Example 2b.
3.29.
a. For any permutation 
 of 
 the probability that the
successive types collected is 
 is 
Consequently, the desired probability is 
b. For 
 all distinct,
which follows because there are no coupons of types 
 when
each of the  independent selections is one of the other 
 types. It
𝑃ሺ𝐵௝ሻ
𝑃ሺ𝐵௝ሻൌ
1
𝑛൅1
⎡
⎣
𝑗െ1
𝑛൅2 ൅𝑛൅2 െ𝑗
𝑛൅2
⎤
⎦
ൌ
1
𝑛൅2
𝐴௜
𝑖
𝑃ሺ𝐴௜ሻൌ1 െ
ቆ2𝑛െ2
𝑛
ቇ
ቆ2𝑛
𝑛ቇ
ൌ1 െ1
2
𝑛െ1
2𝑛െ1 ൌ3𝑛െ1
4𝑛െ2
𝑛
2𝑛െ1
𝑃ሺ𝐴ଵ𝐴ଶሻൌ
𝑛
2𝑛െ1
𝑃ሺ𝐴ଶ
௖||𝐴ଵሻൌ1 െ𝑃ሺ𝐴ଶ|𝐴ଵሻൌ1 െ𝑃ሺ𝐴ଵ𝐴ଶሻ
𝑃ሺ𝐴ଵሻ
ൌ𝑛െ1
3𝑛െ1
𝑖, 𝑖ൌ1, 2,
𝑖
𝑛
𝑛→∞,
𝑛െ1
3𝑛െ1
𝑖ଵ, . . . , 𝑖௡
1, 2, . . . ,𝑛,
𝑖ଵ, . . . , 𝑖௡
𝑝௜భ⋯𝑝௜೙ൌෑ
௜ൌଵ
௡
𝑝௜.
𝑛!ෑ
௜ൌଵ
௡
𝑝௜.
𝑖ଵ, . . . ,𝑖௞
𝑃ሺ𝐸௜భ⋯𝐸௜ೖሻൌቆ𝑛െ𝑘
𝑛
ቇ
௡
𝑖ଵ, . . . , 𝑖௞
𝑛
𝑛െ𝑘
751 of 848

now follows by the inclusion–exclusion identity that
Because 
 is the probability that one of each type is
obtained, by part (a) it is equal to 
 Substituting this into the
preceding equation gives
or
or
3.30.
Using
gives
3.31.
a. 2/5
b. 5/6
3.32.
a. 1/7
b. 1/6
𝑃൫∪௜ൌଵ
௡
𝐸௜൯ൌ
෍
௞ൌଵ
௡
ሺെ1ሻ௞൅ଵቆ𝑛
𝑘ቇቆ𝑛െ𝑘
𝑛
ቇ
௡
1 െ𝑃ሺ∪௜ൌଵ
௡
𝐸௜ሻ
𝑛!
𝑛௡.
1 െ𝑛!
𝑛௡ൌ
෍
௞ൌଵ
௡
ሺെ1ሻ௞൅ଵቆ𝑛
𝑘ቇቆ𝑛െ𝑘
𝑛
ቇ
௡
𝑛! ൌ𝑛௡െ
෍
௞ൌଵ
௡
ሺെ1ሻ௞൅ଵቆ𝑛
𝑘ቇሺ𝑛െ𝑘ሻ௡
𝑛! ൌ
෍
௞ൌ଴
௡
ሺെ1ሻ௞ቆ𝑛
𝑘ቇሺ𝑛െ𝑘ሻ௡
𝑃ሺ𝐸|𝐸∪𝐹ሻൌ
𝑃ሺ𝐸|𝐹ሺ𝐸∪𝐹ሻሻ𝑃ሺ𝐹|𝐸∪𝐹ሻ
൅𝑃ሺ𝐸|𝐹௖ሺ𝐸∪𝐹ሻሻ𝑃ሺ𝐹௖||𝐸∪𝐹ሻ
𝐹ሺ𝐸∪𝐹ሻൌ𝐹 and 𝐹௖ሺ𝐸∪𝐹ሻൌ𝐹௖𝐸
𝑃ሺ𝐸|𝐸∪𝐹ሻൌ
𝑃ሺ𝐸|𝐹ሻ𝑃ሺ𝐹|𝐸∪𝐹ሻ൅𝑃ሺ𝐸|𝐸𝐹௖ሻ𝑃ሺ𝐹௖||𝐸∪𝐹ሻ
ൌ
𝑃ሺ𝐸|𝐹ሻ𝑃ሺ𝐹|𝐸∪𝐹ሻ൅𝑃ሺ𝐹௖||𝐸∪𝐹ሻ
൒
𝑃ሺ𝐸|𝐹ሻ𝑃ሺ𝐹|𝐸∪𝐹ሻ൅𝑃ሺ𝐸|𝐹ሻ𝑃ሺ𝐹௖||𝐸∪𝐹ሻ
ൌ
𝑃ሺ𝐸|𝐹ሻ
752 of 848

3.33.
The second equality in the preceding used that 
3.34.
 Let 
 be the event that player 1 wins the contest. Letting  be the
event that player  does not play in round 1, we obtain by conditioning on
whether or not  occurs, that
where the preceding used that if 
 occurs then 1 would have to beat both 2
and 3 to win the tournament. To compute 
 condition on which of 2 or
3 wins the first game. Letting 
 be the event that  wins the first game
Hence, 
 Also,
3.35.
Now,
giving that
𝑃ሺ𝐸|𝐹𝐺௖ሻ
ൌ
𝑃ሺ𝐸𝐹𝐺௖ሻ
𝑃ሺ𝐹𝐺௖ሻ
ൌ
𝑃ሺ𝐸𝐹ሻെ𝑃ሺ𝐸𝐹𝐺ሻ
𝑃ሺ𝐹ሻെ𝑃ሺ𝐹𝐺ሻ
ൌ
𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻെ𝑃ሺ𝐸ሻ𝑃ሺ𝐹ሻ𝑃ሺ𝐺ሻ
𝑃ሺ𝐹ሻെ𝑃ሺ𝐹ሻ𝑃ሺ𝐺ሻ
ൌ
𝑃ሺ𝐸ሻ
𝐸𝐹ൌ𝐸𝐹𝐺∪𝐸𝐹𝐺௖.
𝑊ଵ
𝑂
1
𝑂
𝑃ሺ𝑊ଵሻൌ
𝑃ሺ𝑊ଵ|𝑂ሻ𝑃ሺ𝑂ሻ൅𝑃ሺ𝑊ଵ||𝑂௖ሻ𝑃ሺ𝑂௖ሻ
ൌ
𝑃ሺ𝑊ଵ|𝑂ሻ1
3   ൅  1
3
1
4
2
3
𝑂௖
𝑃ሺ𝑊ଵ||𝑂ሻ,
𝐵௜
𝑖
𝑃ሺ𝑊ଵ|𝑂ሻൌ
𝑃ሺ𝑊ଵ|0,  𝐵ଶሻ𝑃ሺ𝐵ଶ|𝑂ሻ൅𝑃ሺ𝑊ଵ|0,  𝐵ଷሻ𝑃ሺ𝐵ଷ|𝑂ሻ
ൌ
1
3
2
5 ൅1
4
3
5 ൌ17/60
𝑃ሺ𝑊ଵሻൌ3/20.
𝑃ሺ𝑂|𝑊ଵሻൌ𝑃ሺ𝑊ଵ|𝑂ሻ𝑃ሺ𝑂ሻ
𝑃ሺ𝑊ଵሻ
ൌሺ17/60ሻሺ1/3ሻ
3/20
ൌ17/27
𝑃ሺall white || sameሻൌ𝑃ሺall whiteሻ
𝑃ሺsameሻ
𝑃ሺall whiteሻൌ
ቆ5
4ቇ
ቆ22
4 ቇ
,   𝑃ሺsameሻൌ
ቆ4
4ቇ൅ቆ5
4ቇ൅ቆ6
4ቇ൅ቆ7
4ቇ
ቆ22
4 ቇ
𝑃ሺall white || sameሻൌ
ቆ5
4ቇ
ቆ4
4ቇ൅ቆ5
4ቇ൅ቆ6
4ቇ൅ቆ7
4ቇ
ൌ5
56
753 of 848

3.36.
 Let 
 be the probability that 3 beats 4. Because 1 beats 2 with
probability 1/3,
3.37.
a. Condition on who wins the first game to obtain:
b. Condition on the opponent of player 4. If 
 is the event that  is the
opponent, 
 then
Hence,
𝐵ଷ
𝑃ሺ1ሻ
ൌ
𝑃ሺ1|𝐵ଷሻ𝑃ሺ𝐵ଷሻ൅𝑃ሺ1|𝐵ଷ
௖ሻ𝑃ሺ𝐵ଷ
௖ሻൌሺ1/3ሻሺ1/4ሻሺ3/7ሻ
൅ ሺ1/3ሻሺ1/5ሻሺ4/7ሻൌ31/420
𝑃ሺ𝑊ଷሻൌ
𝑃ሺ𝑊ଷ|1 winsሻሺ1/3ሻ  ൅ 𝑃ሺ𝑊ଷ|2 winsሻ ሺ2/3ሻ
ൌሺ1/3ሻሺ3/4ሻෑ
௜ൌସ
௡
3
𝑖൅3   ൅ ሺ2/3ሻሺ3/5ሻෑ
௜ൌସ
௡
3
𝑖൅3
ൌ
13
20 ෑ
௜ൌସ
௡
3
𝑖൅3
𝑂௜
𝑖
𝑖ൌ1, 2, 3,
𝑃ሺ𝑂ଵሻൌ
1
3
1
4 ൌ1
12
𝑃ሺ𝑂ଶሻൌ
2
3
2
5 ൌ4
15
𝑃ሺ𝑂ଷሻൌ
1 െ1
12 െ4
15 ൌ13
20
𝑃ሺ𝑊ସሻൌ෍
௜ൌଵ
ଷ
𝑃ሺ𝑊ସ|𝑂௜ሻ𝑃ሺ𝑂௜ሻൌ4
5
1
12 ൅4
6
4
15 ൅4
7
13
20 ൌ194
315
4.1.
 Since the probabilities sum to 1, we must have
 implying that
 Hence,
4.2.
 The relationship implies that 
 where
 Because these probabilities sum to 1, it follows that
Hence,
4𝑃ሼ𝑋ൌ3ሽ൅.5 ൌ1,
𝑃ሼ𝑋ൌ0ሽൌ.375,𝑃ሼ𝑋ൌ3ሽൌ.125.
𝐸ሾ𝑋ሿൌ1ሺ.3ሻ൅2ሺ.2ሻ൅3ሺ.125ሻൌ1.075.
𝑝௜ൌ𝑐௜𝑝଴, 𝑖ൌ1, 2,
𝑝௜ൌ𝑃൛𝑋ൌ𝑖ൟ.
𝑝଴ሺ1 ൅𝑐൅𝑐ଶሻൌ1 ⇒𝑝଴ൌ
1
1 ൅𝑐൅𝑐ଶ
𝐸ሾ𝑋ሿൌ𝑝ଵ൅2𝑝ଶൌ
𝑐൅2𝑐ଶ
1 ൅𝑐൅𝑐ଶ
754 of 848

4.3.
 Let  be the number of flips. Then the probability mass function of
 is
Hence,
4.4.
 The probability that a randomly chosen family will have  children
is 
 Thus,
Also, since there are 
 children in families having  children, it follows
that the probability that a randomly chosen child is from a family with 
children is 
 Therefore,
Thus, we must show that
or, equivalently, that
or, equivalently, that
But, for a fixed pair 
 the coefficient of 
 in the left-side summation of
the preceding inequality is 
 whereas its coefficient in the right-hand
𝑋
𝑋
𝑝ଶൌ𝑝ଶ൅ሺ1 െ𝑝ሻଶ, 𝑝ଷൌ1 െ𝑝ଶൌ2𝑝ሺ1 െ𝑝ሻ
𝐸ሾ𝑋ሿൌ2𝑝ଶ൅3𝑝ଷൌ2𝑝ଶ൅3ሺ1 െ𝑝ଶሻൌ3 െ𝑝ଶെሺ1 െ𝑝ሻଶ
𝑖
𝑛௜/𝑚.
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௥
𝑖𝑛௜/𝑚
𝑖𝑛௜
𝑖
𝑖
𝑖𝑛௜/ ෍
௜ൌଵ
௥
𝑖𝑛௜.
𝐸ሾ𝑌ሿൌ
෍
௜ൌଵ
௥
𝑖ଶ𝑛௜
෍
௜ൌଵ
௥
𝑖𝑛௜
෍
௜ൌଵ
௥
𝑖ଶ𝑛௜
෍
௜ൌଵ
௥
𝑖𝑛௜
൒
෍
௜ൌଵ
௥
𝑖𝑛௜
෍
௜ൌଵ
௥
𝑛௜
෍
௝ൌଵ
௥
𝑛௝෍
௜ൌଵ
௥
𝑖ଶ𝑛௜൒෍
௜ൌଵ
௥
𝑖𝑛௜෍
௝ൌଵ
௥
𝑗𝑛௝
෍
௜ൌଵ
௥
෍
௝ൌଵ
௥
𝑖ଶ𝑛௜𝑛௝൒෍
௜ൌଵ
௥
෍
௝ൌଵ
௥
𝑖𝑗𝑛௜𝑛௝
𝑖, 𝑗,
𝑛௜𝑛௝
𝑖ଶ൅𝑗ଶ,
755 of 848

summation is 
 Hence, it suffices to show that
which follows because 
4.5.
 Let 
 Then 
 and 
 so
implying that 
 Hence, 
4.6.
 If you wager  on a bet that wins the amount wagered with
probability  and loses that amount with probability 
 then your
expected winnings are
which is positive (and increasing in ) if and only if 
 Thus, if
 one maximizes one’s expected return by wagering 0, and if
 one maximizes one’s expected return by wagering the maximal
possible bet. Therefore, if the information is that the .6 coin was chosen,
then you should bet 10; if the information is that the .3 coin was chosen,
then you should bet 0. Hence, your expected payoff is
Since your expected payoff is 0 without the information (because in this
case the probability of winning is 
), it follows that if the
information costs less than 1, then it pays to purchase it.
4.7.
a. If you turn over the red paper and observe the value  then your
expected return if you switch to the blue paper is
Thus, it would always be better to switch.
b. Suppose the philanthropist writes the amount  on the red paper.
Then the amount on the blue paper is either 
 or 
 Note that if
 then the amount on the blue paper will be at least  and
will thus be accepted. Hence, in this case, the reward is equally
likely to be either 
 or 
 so
If 
 then the blue paper will be accepted if its value is
 and rejected if it is 
 Therefore,
2𝑖𝑗.
𝑖ଶ൅𝑗ଶ൒2𝑖𝑗
ሺ𝑖െ𝑗ሻଶ൒0.
𝑝ൌ𝑃ሼ𝑋ൌ1ሽ.
𝐸ሾ𝑋ሿൌ𝑝
Varሺ𝑋ሻൌ𝑝ሺ1 െ𝑝ሻ,
𝑝ൌ3𝑝ሺ1 െ𝑝ሻ
𝑝ൌ2/3.
𝑃ሼ𝑋ൌ0ሽൌ1/3.
𝑥
𝑝
1 െ𝑝,
𝑥𝑝െ𝑥ሺ1 െ𝑝ሻൌሺ2𝑝െ1ሻ𝑥
𝑥
𝑝൐1/2.
𝑝൑1/2,
𝑝൐1/2,
1
2ሺ1.2 െ1ሻ10 ൅1
2 0 െ𝐶ൌ1 െ𝐶
1
2ሺ.6ሻ൅1
2ሺ.3ሻ൏1/2
𝑥,
2𝑥ሺ1/2ሻ൅𝑥/2ሺ1/2ሻൌ5𝑥/4 ൐𝑥
𝑥
2𝑥
𝑥/2.
𝑥/2 ൒𝑦,
𝑦
2𝑥
𝑥/2,
𝐸ሾ𝑅௬ሺ𝑥ሻሿൌ5𝑥/4, if 𝑥/2 ൒𝑦
𝑥/2 ൏𝑦൑2𝑥,
2𝑥
𝑥/2.
𝐸ሾ𝑅௬ሺ𝑥ሻሿൌ2𝑥ሺ1/2ሻ൅𝑥ሺ1/2ሻൌ3𝑥/2, if 𝑥/2 ൏𝑦൑2𝑥
756 of 848

Finally, if 
 then the blue paper will be rejected. Hence, in
this case, the reward is  so
That is, we have shown that when the amount  is written on the
red paper, the expected return under the -policy is
4.8.
 Suppose that  independent trials, each of which results in a
success with probability  are performed. Then the number of successes
will be less than or equal to  if and only if the number of failures is
greater than or equal to 
 But since each trial is a failure with
probability 
 it follows that the number of failures is a binomial
random variable with parameters  and 
 Hence,
The final equality follows from the fact that the probability that the number
of failures is greater than or equal to 
 is 1 minus the probability that it
is less than 
4.9.
 Since 
 we are given that
 Thus, 
 or 
 Hence,
4.10.
 Let 
 denote the number on the th ball drawn.
Then
Therefore,
4.11.
a. Given that  wins the first game, it will win the series if, from then
2𝑥൏𝑦,
𝑥,
𝑅௬ሺ𝑥ሻൌ𝑥, if 2𝑥൏𝑦
𝑥
𝑦
𝐸ሾ𝑅௬ሺ𝑥ሻሿൌ
⎧
⎨
⎩
⎪
⎪
𝑥
if 𝑥൏𝑦/2
3𝑥/2
if 𝑦/2 ൑𝑥൏2𝑦
5𝑥/4
if 𝑥൒2𝑦
𝑛
𝑝,
𝑖
𝑛െ𝑖.
1 െ𝑝,
𝑛
1 െ𝑝.
𝑃ሼBin ሺ𝑛, 𝑝ሻ൑𝑖ሽ
ൌ
𝑃ሼBin ሺ𝑛, 1 െ𝑝ሻ൒𝑛െ𝑖ሽ
ൌ
1 െ𝑃ሼBin ሺ𝑛, 1 െ𝑝ሻ൑𝑛െ𝑖െ1ሽ
𝑛െ𝑖
𝑛െ𝑖.
𝐸ሾ𝑋ሿൌ𝑛𝑝, Varሺ𝑋ሻൌ𝑛𝑝ሺ1 െ𝑝ሻ,
𝑛𝑝ൌ6, 𝑛𝑝ሺ1 െ𝑝ሻൌ2.4.
1 െ𝑝ൌ.4,
𝑝ൌ.6, 𝑛ൌ10.
𝑃ሼ𝑋ൌ5ሽൌቆ10
5 ቇሺ.6ሻହሺ.4ሻହ
𝑋௜, 𝑖ൌ1, . . . ,𝑚,
𝑖
𝑃ሼ𝑋൑𝑘ሽ
ൌ
𝑃ሼ𝑋ଵ൑𝑘, 𝑋ଶ൑𝑘, .   .   . , 𝑋௠൑𝑘ሽ
ൌ
𝑃ሼ𝑋ଵ൑𝑘ሽ𝑃ሼ𝑋ଶ൑𝑘ሽ ⋯ 𝑃ሼ𝑋௠൑𝑘ሽ
ൌ
ቆ𝑘
𝑛ቇ
௠
𝑃ሼ𝑋ൌ𝑘ሽൌ𝑃ሼ𝑋൑𝑘ሽെ𝑃ሼ𝑋൑𝑘െ1ሽൌቆ𝑘
𝑛ቇ
௠
െቆ𝑘െ1
𝑛
ቇ
௠
𝐴
757 of 848

on, it wins 2 games before team  wins 3 games. Thus,
b. 
4.12.
 To obtain the solution, condition on whether the team wins this
weekend:
4.13.
 Let  be the event that the jury makes the correct decision, and
let  be the event that four of the judges agreed. Then
Also,
4.14.
 Assuming that the number of hurricanes can be approximated by
a Poisson random variable, we obtain the solution
𝐵
𝑃ሼ𝐴 wins |𝐴 wins ϐirstሽൌ෍
௜ൌଶ
ସ
ቆ4
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻସെ௜
𝑃ሼ𝐴 wins ϐirst | 𝐴 winsሽ
ൌ
𝑃ሼ𝐴 wins | 𝐴 wins ϐirstሽ𝑃ሼ𝐴 wins ϐirstሽ
𝑃ሼ𝐴 winsሽ
ൌ
෍
௜ൌଶ
ସ
ቆ4
𝑖ቇ 𝑝௜൅ଵሺ1 െ𝑝ሻସെ௜
෍
௜ൌଷ
ହ
ቆ5
𝑖ቇ 𝑝௜ሺ1 െ𝑝ሻହെ௜
.5 ෍
௜ൌଷ
ସ
ቆ4
𝑖ቇሺ.4ሻ௜ሺ.6ሻସെ௜൅.5 ෍
௜ൌଷ
ସ
ቆ4
𝑖ቇሺ.7ሻ௜ሺ.3ሻସെ௜
𝐶
𝐹
𝑃ሺ𝐶ሻൌ෍
௜ൌସ
଻
ቆ7
𝑖ቇሺ.7ሻ௜ሺ.3ሻ଻െ௜
𝑃ሺ𝐶|𝐹ሻൌ
௉ሺ஼ிሻ
௉ሺிሻ
ൌ
ቆ଻
ସቇሺ.଻ሻరሺ.ଷሻయ
ቆ଻
ସቇሺ.଻ሻరሺ.ଷሻయ൅ቆ଻
ଷቇሺ.଻ሻయሺ.ଷሻర
ൌ
.7
෍
௜ൌ଴
ଷ
𝑒െହ.ଶሺ5.2ሻ௜/𝑖!
758 of 848

4.15.
4.16.
𝐸ሾ𝑌ሿ
ൌ
෍
௜ൌଵ
ஶ
𝑖𝑃ሼ𝑋ൌ𝑖ሽ/𝑃ሼ𝑋൐0ሽ
ൌ
𝐸ሾ𝑋ሿ/𝑃ሼ𝑋൐0ሽ
ൌ
𝜆
1 െ𝑒െఒ
Let  be the event that girl  and girl  choose different boys. Then
Therefore,
Because, when  is large, 
 is small and nearly equal to
 it follows from the Poisson paradigm that the number of
couples is approximately Poisson distributed with mean
 Hence, 
 and 
To determine the probability that a given set of  girls all are
coupled, condition on whether or not  occurs, where  is the
event that they all choose different boys. This gives
Therefore,
and the inclusion–exclusion identity yields
a.
1/𝑛
b.
𝐷
𝑖
𝑗
𝑃ሺ𝐺௜𝐺௝ሻൌ
𝑃൫𝐺௜𝐺௝ห𝐷൯𝑃ሺ𝐷ሻ൅𝑃൫𝐺௜𝐺௝ห𝐷௖൯𝑃ሺ𝐷௖ሻ
ൌ
ሺ1/𝑛ሻଶሺ1 െ1/𝑛ሻ
ൌ
𝑛െ1
𝑛ଷ
𝑃൫𝐺௜|𝐺௝൯ൌ𝑛െ1
𝑛ଶ
c. d.
𝑛
𝑃ሺ𝐺௜ห𝐺௝ሻ
𝑃ሺ𝐺௜ሻ,
෍
௜ൌଵ
௡
𝑃ሺ𝐺௜ሻൌ1.
𝑃଴ൎ𝑒െଵ
𝑃௞ൎ𝑒െଵ/𝑘!
e.
𝑘
𝐷
𝐷
𝑃ሺ𝐺௜భ⋯𝐺௜ೖሻൌ
𝑃൫𝐺௜భ⋯𝐺௜ೖ||𝐷൯𝑃ሺ𝐷ሻ
൅𝑃൫𝐺௜భ⋯𝐺௜ೖ||𝐷௖൯𝑃ሺ𝐷௖ሻ
ൌ
𝑃൫𝐺௜భ⋯𝐺௜ೖ||𝐷൯𝑃ሺ𝐷ሻ
ൌ
ሺ1/𝑛ሻ௞𝑛ሺ𝑛െ1ሻ⋯ሺ𝑛െ𝑘൅1ሻ
𝑛௞
ൌ
𝑛!
ሺ𝑛െ𝑘ሻ!𝑛ଶ௞
෍
௜భழ
.   .   . ழ௜ೖ
𝑃ሺ𝐺௜భ⋯𝐺௜ೖሻൌቆ𝑛
𝑘ቇ𝑃ሺ𝐺௜భ⋯𝐺௜ೖሻ
ൌ
௡!௡!
ሺ௡െ௞ሻ!ሺ௡െ௞ሻ!௞!௡మೖ
759 of 848

4.17.
a. Because woman  is equally likely to be paired with any of the
remaining 
 people, 
b. Because, conditional on 
 woman  is equally likely to be paired
with any of 
 people, 
c. When  is large, the number of wives paired with their husbands
will approximately be Poisson with mean
 Therefore, the probability that there
is no such pairing is approximately 
d. It reduces to the match problem.
4.18.
a. 
b. If 
 is her final winnings and  is the number of bets she makes,
then, since she would have won 4 bets and lost 
 bets, it
follows that
Hence,
4.19.
 The probability that a round does not result in an “odd person” is
equal to 1/4, the probability that all three coins land on the same side.
a. 
b. 
4.20.
 Let 
 Then
1 െ𝑃଴ൌ𝑃ሺ∪௜ൌଵ
௡
𝐺௜ሻൌ
෍
௞ൌଵ
௡
ሺെ1ሻ௞൅ଵ
𝑛!𝑛!
ሺ𝑛െ𝑘ሻ!ሺ𝑛െ𝑘ሻ!𝑘!𝑛ଶ௞
𝑖
2𝑛െ1
𝑃ሺ𝑊௜ሻൌ
1
2𝑛െ1
𝑊௝,
𝑖
2𝑛െ3
𝑃൫𝑊௜|𝑊௝൯ൌ
1
2𝑛െ3
𝑛
෍
௜ൌଵ
௡
𝑃ሺ𝑊௜ሻൌ
𝑛
2𝑛െ1 ൎ1/2.
𝑒െଵ/ଶ.
ቆ8
3ቇ ሺ9/19ሻଷሺ10/19ሻହሺ9/19ሻൌቆ8
3ቇ ሺ9/19ሻସሺ10/19ሻହ
𝑊
𝑋
𝑋െ4
𝑊ൌ20 െ5ሺ𝑋െ4ሻൌ40 െ5𝑋
𝐸ሾ𝑊ሿൌ40 െ5𝐸ሾ𝑋ሿൌ40 െ5ሾ4/ሺ9/19ሻሿൌെ20/9
ሺ1/4ሻଶሺ3/4ሻൌ3/64
ሺ1/4ሻସൌ1/256
𝑞ൌ1 െ𝑝.
760 of 848

4.21.
 Since 
 will equal 1 with probability  or 0 with probability
 it follows that it is a Bernoulli random variable with parameter 
Because the variance of such a Bernoulli random variable is 
 we
have
Hence,
4.22.
 Let  denote the number of games that you play and  the
number of games that you lose.
a. After your fourth game, you will continue to play until you lose.
Therefore, 
 is a geometric random variable with parameter
 so
b. If we let  denote the number of losses you have in the first 4
𝐸ሾ1/𝑋ሿൌ
෍
௜ൌଵ
ஶ
1
𝑖𝑞௜െଵ𝑝
ൌ
𝑝
𝑞෍
௜ൌଵ
ஶ
𝑞௜/𝑖
ൌ
𝑝
𝑞෍
௜ൌଵ
ஶ
඲
଴
௤
𝑥௜െଵ𝑑𝑥
ൌ
𝑝
𝑞඲
଴
௤
෍
௜ൌଵ
ஶ
𝑥௜െଵ𝑑𝑥
ൌ
𝑝
𝑞඲
଴
௤
1
1 െ𝑥𝑑𝑥
ൌ
𝑝
𝑞඲
௣
ଵ
1
𝑦𝑑𝑦
ൌ
െ𝑝
𝑞logሺ𝑝ሻ
𝑋െ𝑏
𝑎െ𝑏
𝑝
1 െ𝑝,
𝑝.
𝑝ሺ1 െ𝑝ሻ,
𝑝ሺ1 െ𝑝ሻൌ
Var ቆ𝑋െ𝑏
𝑎െ𝑏ቇൌ
1
ሺ𝑎െ𝑏ሻଶVarሺ𝑋െ𝑏ሻ
ൌ
1
ሺ𝑎െ𝑏ሻଶVarሺ𝑋ሻ
Varሺ𝑋ሻൌሺ𝑎െ𝑏ሻଶ𝑝ሺ1 െ𝑝ሻ
𝑋
𝑌
𝑋െ4
1 െ𝑝,
𝐸ሾ𝑋ሿൌ𝐸ሾ4 ൅ሺ𝑋െ4ሻሿൌ4 ൅𝐸ሾ𝑋െ4ሿൌ4 ൅
1
1 െ𝑝
𝑍
761 of 848

games, then  is a binomial random variable with parameters 4
and 
 Because 
 we have
4.23.
 A total of  white balls will be withdrawn before a total of 
 black
balls if and only if there are at least  white balls in the first 
withdrawals. (Compare with the problem of the points, Example 4j of
Chapter 3
.) With  equal to the number of white balls among the first
 balls withdrawn,  is a hypergeometric random variable, and it
follows that
4.24.
 Because each ball independently goes into urn  with the same
probability 
 it follows that 
 is a binomial random variable with
parameters 
First note that 
 is the number of balls that go into either urn  or urn
 Then, because each of the 
 balls independently goes into one of
these urns with probability 
 it follows that 
 is a binomial
random variable with parameters 
 and 
By the same logic, 
 is a binomial random variable with
parameters 
 and 
 Therefore,
4.25.
 Let 
 equal  if person  has a match, and let it equal 
otherwise. Then
is the number of matches. Taking expectations gives
where the final equality follows because person  is equally likely to end
𝑍
1 െ𝑝.
𝑌ൌ𝑍൅1,
𝐸ሾ𝑌ሿൌ𝐸ሾ𝑍൅1ሿൌ𝐸ሾ𝑍ሿ൅1 ൌ4ሺ1 െ𝑝ሻ൅1
𝑛
𝑚
𝑛
𝑛൅𝑚െ1
𝑋
𝑛൅𝑚െ1
𝑋
𝑃ሼ𝑋൒𝑛ሽ
ൌ
෍
௜ൌ௡
௡൅௠െଵ
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ
෍
௜ൌ௡
௡൅௠െଵቆ𝑁
𝑖ቇቆ
𝑀
𝑛൅𝑚െ1 െ𝑖ቇ
ቆ𝑁൅𝑀
𝑛൅𝑚െ1ቇ
𝑖
𝑝௜,
𝑋௜
𝑛ൌ10, 𝑝ൌ𝑝௜.
𝑋௜൅𝑋௝
𝑖
𝑗.
10
𝑝௜൅𝑝௝,
𝑋௜൅𝑋௝
10
𝑝௜൅𝑝௝.
𝑋ଵ൅𝑋ଶ൅𝑋ଷ
10
𝑝ଵ൅𝑝ଶ൅𝑝ଷ.
𝑃ሼ𝑋ଵ൅𝑋ଶ൅𝑋ଷൌ7ሽൌቆ10
7 ቇ ሺ𝑝ଵ൅𝑝ଶ൅𝑝ଷሻ଻ሺ𝑝ସ൅𝑝ହሻଷ
𝑋௜
1
𝑖
0
𝑋ൌ෍
௜ൌଵ
௡
𝑋௜
𝐸ሾ𝑋ሿൌ𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿൌ෍
௜ൌଵ
௡
𝑃ሼ𝑋௜ൌ1ሽൌ෍
௜ൌଵ
௡
1/𝑛ൌ1
𝑖
762 of 848

up with any of the  hats.
To compute 
 we use Equation (9.1), which states that
Now, for 
Hence,
which yields
4.26.
 With 
 we have, on the one hand,
On the other hand,
However, given that the first trial is not a success, the number of trials
needed for a success is  plus the geometrically distributed number of
additional trials required. Therefore,
𝑛
Varሺ𝑋ሻ,
𝐸ሾ𝑋ଶሿൌ෍
௜ൌଵ
௡
𝐸ሾ𝑋௜ሿ൅෍
௜ൌଵ
௡
෍
௝ஷ௜
𝐸ሾ𝑋௜𝑋௝ሿ
𝑖്𝑗,
𝐸ሾ𝑋௜𝑋௝ሿ
ൌ𝑃൛𝑋௜ൌ1, 𝑋௝ൌ1ൟൌ𝑃ሼ𝑋௜ൌ1ሽ𝑃൛𝑋௝ൌ1ห𝑋௜ൌ1ൟ
ൌ
1
𝑛
1
𝑛െ1
𝐸ሾ𝑋ଶሿ
ൌ
1 ൅෍
௜ൌଵ
௡
෍
௝ஷ௜
1
𝑛ሺ𝑛െ1ሻ
ൌ
1 ൅𝑛ሺ𝑛െ1ሻ
1
𝑛ሺ𝑛െ1ሻൌ2
Varሺ𝑋ሻൌ2 െ1ଶൌ1
𝑞ൌ1 െ𝑝,
𝑃ሺ𝐸ሻൌ
෍
௜ൌଵ
ஶ
𝑃ሼ𝑋ൌ2𝑖ሽ
ൌ
෍
௜ൌଵ
ஶ
𝑝𝑞ଶ௜െଵ
ൌ
𝑝𝑞෍
௜ൌଵ
ஶ
ሺ𝑞ଶሻ
௜െଵ
ൌ
𝑝𝑞
1
1 െ𝑞ଶ
ൌ
𝑝𝑞
ሺ1 െ𝑞ሻሺ1 ൅𝑞ሻൌ
𝑞
1 ൅𝑞
𝑃ሺ𝐸ሻൌ𝑃ሺ𝐸||𝑋ൌ1ሻ𝑝൅𝑃ሺ𝐸||𝑋൐1ሻ𝑞ൌ𝑞𝑃ሺ𝐸||𝑋൐1ሻ
1
𝑃ሺ𝐸|𝑋൐1ሻൌ𝑃ሺ𝑋൅1 is evenሻൌ𝑃ሺ𝐸௖ሻൌ1 െ𝑃ሺ𝐸ሻ
763 of 848

which yields 
4.27.
 In order for 
 one of the teams must be up 3 games to 2
after the first 5 games and then must win game 6. This gives
On the other hand, 
 if each team wins 3 of the first 6 games. Hence,
Hence,
Calculus shows that 
 is minimized when 
 with the
minimizing value equal to 
(b) In order for 
 one of the teams must be up 3 games to 2 after the
first 5 games, and because when 
 each team is equally likely to
win game 6, it is just as likely that  will equal 6 as that it will equal 7.
(c) Imagine that the teams continue to play even after one of them has
won the series. The team that wins the first game must win at least 3 of
the next 6 games played to win the series. Hence, the desired answer is
4.28.
a. The negative binomial represents the number of balls withdrawn in
a similar experiment but with the exception that the withdrawn ball
would be replaced before the next drawing.
b. Using the hint, we note that 
 if the first 
 balls withdrawn
contain exactly 
 white balls and the next withdrawn ball is
white. Hence,
4.29.
a. 
 𝑃ሺ𝐸ሻൌ𝑞/ሺ1 ൅𝑞ሻ.
𝑁ൌ6
𝑃ሺ𝑁ൌ6ሻ
ൌ
ቆ5
3ቇ 𝑝ଷሺ1 െ𝑝ሻଶ𝑝൅ቆ5
3ቇ ሺ1 െ𝑝ሻଷ𝑝ଶ ሺ1 െ𝑝ሻ
ൌ
10ሺ𝑝ସሺ1 െ𝑝ሻଶ൅ሺ1 െ𝑝ሻସ𝑝ଶሻ
𝑁ൌ7
𝑃ሺ𝑁ൌ7ሻൌቆ6
3ቇ 𝑝ଷሺ1 െ𝑝ሻଷൌ20𝑝ଷሺ1 െ𝑝ሻଷ
𝑃ሺ𝑁ൌ6ሻെ𝑃ሺ𝑁ൌ7ሻൌ𝑝ଶሺ1 െ𝑝ሻଶ
൫10𝑝ଶ൅10 ሺ1 െ𝑝ሻଶെ20𝑝 ሺ1 െ𝑝ሻ൯
ൌ𝑝ଶሺ1 െ𝑝ሻଶሺ40𝑝ଶെ40𝑝൅10ሻ
40𝑝ଶെ40𝑝൅10
𝑝ൌ1/2
0.
𝑁൒6
𝑝ൌ1/2
𝑁
෍
௜ൌଷ
଺
ቆ6
𝑖ቇሺ1/2ሻ଺ൌ42/64.
𝑋ൌ𝑟
𝑟െ1
𝑘െ1
𝑃ሺ𝑋ൌ𝑟ሻൌ
ቆ௡
௞െଵቇቆ௠
௥െ௞ቇ
ቆ௡൅௠
௥െଵቇ
௡െ௞൅ଵ
௡൅௠െ௥൅ଵ,
𝑘൑𝑟൑𝑚൅𝑘
1
3 ቆ8
5ቇ൫ሺ1/3ሻହሺ2/3ሻଷ൅ሺ1/2ሻ଼൅ሺ3/4ሻହሺ1/4ሻଷ൯
764 of 848

b. 
4.30.
 Binomial with parameters  and 
4.31.
4.32.
 if the first 
 balls consist of 
 red and 
 blue
balls, and the next ball is red. Hence,
Let  be the number of balls that have to be removed until a total of  blue
balls have been removed. Then, 
 and for 
Now, 
 Because 
 and 
 either if 
or if 
 we have, for 
 that
1
3 ሾሺ2/3ሻସሺ1/3ሻ൅ሺ1/2ሻହ൅ሺ1/4ሻସሺ3/4ሻሿ
𝑛
1 െ𝑝.
𝑃ሺ𝑋ൌ𝑘ሻൌ
ቆ𝑘െ1
𝑖െ1ቇቆ𝑛൅𝑚െ𝑘
𝑛െ𝑖
ቇ
ቆ𝑛൅𝑚
𝑛
ቇ
𝑋ൌ𝑖
𝑖െ1
𝑟െ1
𝑖െ𝑟
𝑃ሺ𝑋ൌ𝑖ሻൌ
ቆ
𝑛
𝑟െ1ቇቆ𝑚
𝑖െ𝑟ቇ
ቆ𝑛൅𝑚
𝑖െ1 ቇ
𝑛െ𝑟൅1
𝑛൅𝑚െ𝑖൅1 .
𝑌
𝑠
𝑉ൌminሺ𝑋, 𝑌ሻ
𝑖൏𝑟൅𝑠,
𝑃ሺ𝑉ൌ𝑖ሻൌ
𝑃ሺ𝑋ൌ𝑖ሻ൅𝑃ሺ𝑌ൌ𝑖ሻ
ൌ
ቆ
𝑛
𝑟െ1ቇቆ𝑚
𝑖െ𝑟ቇ
ቆ𝑛൅𝑚
𝑖െ1 ቇ
𝑛െ𝑟൅1
𝑛൅𝑚െ𝑖൅1
൅
ቆ𝑚
𝑠െ1ቇቆ𝑛
𝑖െ𝑠ቇ
ቆ𝑛൅𝑚
𝑖െ1 ቇ
𝑚െ𝑠൅1
𝑛൅𝑚െ𝑖൅1
𝑍ൌmaxሺ𝑋, 𝑌ሻ.
𝑍൒𝑟൅𝑠,
𝑍ൌ𝑖൒𝑟൅𝑠
𝑋ൌ𝑖
𝑌ൌ𝑖,
𝑖൒𝑟൅𝑠,
𝑃ሺ𝑍ൌ𝑖ሻ
ൌ
𝑃ሺ𝑋ൌ𝑖ሻ൅𝑃ሺ𝑌ൌ𝑖ሻ
ൌ
ቆ
𝑛
𝑟െ1ቇቆ𝑚
𝑖െ𝑟ቇ
ቆ𝑛൅𝑚
𝑖െ1 ቇ
𝑛െ𝑟൅1
𝑛൅𝑚െ𝑖൅1
൅
ቆ𝑚
𝑠െ1ቇቆ𝑛
𝑖െ𝑠ቇ
ቆ𝑛൅𝑚
𝑖െ1 ቇ
𝑚െ𝑠൅1
𝑛൅𝑚െ𝑖൅1
765 of 848

 if the 
 red ball is removed before a total of 
 balls have been
removed. That is,
𝑋൏𝑌
𝑟௧௛
𝑟൅𝑠
𝑃ሺ𝑋൏𝑌ሻൌ𝑃ሺ𝑋൏𝑟൅𝑠ሻ
ൌ
෍
௜ൌ௥
௥൅௦െଵቆ
𝑛
𝑟െ1ቇቆ𝑚
𝑖െ𝑟ቇ
ቆ𝑛൅𝑚
𝑖െ1 ቇ
𝑛െ𝑟൅1
𝑛൅𝑚െ𝑖൅1 .
5.1.
 Let  be the number of minutes played.
a. 
b. 
c. 
d. 
5.2.
a. 
b. 
5.3.
 First, let us find  by using
a. 
b. 
5.4.
 Since
we obtain 
 Hence,
a. 
b. 
𝑋
𝑃ሼ𝑋൐15ሽൌ1 െ𝑃ሼ𝑋൑15ሽൌ1 െ5ሺ.025ሻൌ.875
𝑃ሼ20 ൏𝑋൏35ሽൌ10ሺ.05ሻ൅5ሺ.025ሻൌ.625
𝑃ሼ𝑋൏30ሽൌ10ሺ.025ሻ൅10ሺ.05ሻൌ.75
𝑃ሼ𝑋൐36ሽൌ4ሺ.025ሻൌ.1
1 ൌ׬଴
ଵ𝑐𝑥௡𝑑𝑥ൌ𝑐/ሺ𝑛൅1ሻ⇒𝑐ൌ𝑛൅1
𝑃ሼ𝑋൐𝑥ሽൌሺ𝑛൅1ሻ׬௫
ଵ𝑥௡𝑑𝑥ൌ𝑥௡൅ଵቚ
௫
ଵ
ൌ1 െ𝑥௡൅ଵ
𝑐
1 ൌ඲
଴
ଶ
𝑐𝑥ସ𝑑𝑥ൌ32𝑐/5 ⇒𝑐ൌ5/32
𝐸ሾ𝑋ሿൌ5
32׬଴
ଶ𝑥ହ𝑑𝑥ൌ5
32
64
6 ൌ5/3
𝐸ሾ𝑋ଶሿൌ5
32׬଴
ଶ𝑥଺𝑑𝑥ൌ5
32
128
7
ൌ20/7 ⇒Varሺ𝑋ሻൌ20/7 െሺ5/3ሻଶൌ5/63
1
ൌ
඲
଴
ଵ
ሺ𝑎𝑥൅𝑏𝑥ଶሻ𝑑𝑥ൌ𝑎/2 ൅𝑏/3
.6
ൌ
඲
଴
ଵ
ሺ𝑎𝑥ଶ൅𝑏𝑥ଷሻ𝑑𝑥ൌ𝑎/3 ൅𝑏/4
𝑎ൌ3.6,𝑏ൌെ2.4.
𝑃ሼ𝑋൏1/2ሽൌ׬଴
ଵ/ଶሺ3.6𝑥െ2.4𝑥ଶሻ𝑑𝑥ൌሺ1.8𝑥ଶെ.8𝑥ଷሻ||଴
ଵ/ଶൌ.35
𝐸ሾ𝑋ଶሿൌ׬଴
ଵሺ3.6𝑥ଷെ2.4𝑥ସሻ 𝑑𝑥ൌ.42 ⇒Varሺ𝑋ሻൌ.06
766 of 848

5.5.
 For 
5.6.
 If you bid 
 then you will either win the bid and make a
profit of 
 with probability 
 or lose the bid and make a
profit of 0 otherwise. Therefore, your expected profit if you bid  is
Differentiating and setting the preceding equal to 0 gives
Therefore, you should bid 
 Your expected profit will be 40/7
thousand dollars.
5.7.
a. 
b. 
c. 
d. 
The answer to part (d) could also have been obtained by multiplying
the probabilities in parts (a), (b), and (c).
5.8.
 Let  be the test score, and let 
 Note that  is a
standard normal random variable.
a. 
b. 
5.9.
 Let  be the travel time. We want to find  such that
which is equivalent to
𝑖ൌ1, . . . ,𝑛,
𝑃ሼ𝑋ൌ𝑖ሽ
ൌ
𝑃ሼInt ሺ𝑛𝑈ሻൌ𝑖െ1ሽ
ൌ
𝑃ሼ𝑖െ1 ൑𝑛𝑈൏𝑖ሽ
ൌ
𝑃ቊ𝑖െ1
𝑛
൑𝑈൏𝑖
𝑛ቋ
ൌ
1/𝑛
𝑥,70 ൑𝑥൑140,
𝑥െ100
ሺ140 െ𝑥ሻ/70
𝑥
1
70ሺ𝑥െ100ሻሺ140 െ𝑥ሻൌ1
70ሺ240𝑥െ𝑥ଶെ14000ሻ
240 െ2𝑥ൌ0
$120,000.
𝑃ሼ𝑈൐.1ሽൌ9/10
𝑃ሼ𝑈൐.2||𝑈൐.1ሽൌ𝑃ሼ𝑈൐.2ሽ/𝑃ሼ𝑈൐.1ሽൌ8/9
𝑃ሼ𝑈൐.3||𝑈൐.2,𝑈൐.1ሽൌ𝑃ሼ𝑈൐.3ሽ/𝑃ሼ𝑈൐.2ሽൌ7/8
𝑃ሼ𝑈൐.3ሽൌ7/10
𝑋
𝑍ൌሺ𝑋െ100ሻ/15.
𝑍
𝑃ሼ𝑋൐125ሽൌ𝑃ሼ𝑍൐25/15ሽൎ.0478
𝑃ሼ90 ൏𝑋൏110ሽ
ൌ
𝑃ሼെ10/15 ൏𝑍൏10/15ሽ
ൌ
𝑃ሼ𝑍൏2/3ሽെ𝑃ሼ𝑍൏െ2/3ሽ
ൌ
𝑃ሼ𝑍൏2/3ሽെሾ1 െ𝑃ሼ𝑍൏2/3ሽሿ
ൎ
.4950
𝑋
𝑥
𝑃ሼ𝑋൐𝑥ሽൌ.05
𝑃ቊ𝑋െ40
7
൐𝑥െ40
7
ቋൌ.05
767 of 848

That is, we need to find  such that
where  is a standard normal random variable. But
Thus,
Therefore, you should leave no later than 8.485 minutes after 12 P.M.
5.10.
 Let  be the tire life in units of one thousand, and let 
Note that  is a standard normal random variable.
a. 
b. 
c. 
5.11.
 Let  be next year’s rainfall and let 
a. 
b. 
5.12.
 Let 
 and 
 denote, respectively, the numbers of men and women
in the samples that earn, in units of 
 at least  per year. Also, let  be a
standard normal random variable.
a. 
b. 
𝑥
𝑃ቊ𝑍൐𝑥െ40
7
ቋൌ.05
𝑍
𝑃ሼ𝑍൐1.645ሽൌ.05
𝑥െ40
7
ൌ1.645 or 𝑥ൌ51.515
𝑋
𝑍ൌሺ𝑋െ34ሻ/4.
𝑍
𝑃ሼ𝑋൐40ሽൌ𝑃ሼ𝑍൐1.5ሽൎ.0668
𝑃ሼ30 ൏𝑋൏35ሽൌ𝑃ሼെ1 ൏𝑍൏.25ሽൌ𝑃ሼ𝑍൏.25ሽെ𝑃ሼ𝑍൐1ሽൎ.44
𝑃ሼ𝑋൐40|𝑋൐30ሽ
ൌ𝑃ሼ𝑋൐40ሽ/𝑃ሼ𝑋൐30ሽ
ൌ𝑃ሼ𝑍൐1.5ሽ/𝑃ሼ𝑍൐െ1ሽൎ.079
𝑋
𝑍ൌሺ𝑋െ40.2ሻ/8.4.
𝑃ሼ𝑋൐44ሽൌ𝑃ሼ𝑍൐3.8/8.4ሽൎ𝑃ሼ𝑍൐.4524ሽൎ.3255
ቆ7
3ቇ ሺ.3255ሻଷሺ.6745ሻସ
𝑀௜
𝑊௜
$1,000,
𝑖
𝑍
𝑃ሼ𝑊ଶହ൒70ሽ
ൌ
𝑃ሼ𝑊ଶହ൒69.5ሽ
ൌ
𝑃൝𝑊ଶହെ200ሺ.34ሻ
200ሺ.34ሻሺ.66ሻ
ඥ
൒69.5 െ200ሺ.34ሻ
200ሺ.34ሻሺ.66ሻ
ඥ
ൡ
ൎ
𝑃ሼ𝑍൒.2239ሽ
ൎ
.4114
𝑃ሼ𝑀ଶହ൒120ሽ
ൌ
𝑃ሼ𝑀ଶହ൒120.5ሽ
ൌ
𝑃൝𝑀ଶହെሺ200ሻሺ.587ሻ
ሺ200ሻሺ.587ሻሺ.413ሻ
ඥ
൒120.5 െሺ200ሻሺ.587ሻ
ሺ200ሻሺ.587ሻሺ.413ሻ
ඥ
ൡ
ൎ
𝑃ሼ𝑍൒.4452ሽ
ൎ
.6719
768 of 848

c. 
Hence,
5.13.
 The lack of memory property of the exponential gives the result
5.14.
a. 
b. 
c. 
d. Let  be a standard normal random variable. Use the identity
 to obtain
e. Use the result of Theoretical Exercise 5.5
 to obtain
Hence, 
𝑃ሼ𝑀ଶ଴൒150ሽ 
  ൌ𝑃ሼ𝑀ଶ଴൒149.5ሽ
  ൌ𝑃൝𝑀ଶ଴െሺ200ሻሺ.745ሻ
ሺ200ሻሺ.745ሻሺ.255ሻ
ඥ
൒149.5 െሺ200ሻሺ.745ሻ
ሺ200ሻሺ.745ሻሺ.255ሻ
ඥ
ൡ
  ൎ𝑃ሼ𝑍൒.0811ሽ
  ൎ.4677
𝑃ሼ𝑊ଶ଴൒100ሽ
  ൌ𝑃ሼ𝑊ଶ଴൒99.5ሽ
  ൌ𝑃൝𝑊ଶ଴െሺ200ሻሺ.534ሻ
ሺ200ሻሺ.534ሻሺ.466ሻ
ඥ
൒99.5 െሺ200ሻሺ.534ሻ
ሺ200ሻሺ.534ሻሺ.466ሻ
ඥ
ൡ
  ൎ𝑃ሼ𝑍൒െ1.0348ሽ
  ൎ.8496
𝑃ሼ𝑀ଶ଴൒150ሽ𝑃ሼ𝑊ଶ଴൒100ሽൎ.3974
𝑒െସ/ହ.
𝑒െଶమൌ𝑒െସ
𝐹ሺ3ሻെ𝐹ሺ1ሻൌ𝑒െଵെ𝑒െଽ
𝜆ሺ𝑡ሻൌ2𝑡𝑒െ௧మ/𝑒െ௧మൌ2𝑡
𝑍
𝐸ሾ𝑋ሿൌ׬଴
ஶ𝑃ሼ𝑋൐𝑥ሽ𝑑𝑥
𝐸ሾ𝑋ሿൌ
඲
଴
ஶ
𝑒െ௫మ𝑑𝑥
ൌ
2െଵ/ଶ඲
଴
ஶ
𝑒െ௬మ/ଶ𝑑𝑦
ൌ
2െଵ/ଶ2𝜋
√
𝑃ሼ𝑍൐0ሽ
ൌ
𝜋
√/2
𝐸ሾ𝑋ଶሿൌ඲
଴
ஶ
2𝑥𝑒െ௫మ𝑑𝑥ൌെ𝑒െ௫మቮ
଴
ஶ
ൌ1
Varሺ𝑋ሻൌ1 െ𝜋/4.
769 of 848

5.15.
a. 
b. 
5.16.
 For 
Differentiation yields
The proof when 
 is similar.
5.17.
 If  denotes the number of the first  bets that you win, then the
amount that you will be winning after  bets is
Thus, we want to determine
when  is a binomial random variable with parameters  and 
a. When 
(Because you will be ahead after 34 bets if you win at least 1 bet, the
𝑃ሼ𝑋൐6ሽൌexpቄെ׬଴
଺𝜆ሺ𝑡ሻ𝑑𝑡ቅൌ𝑒െଷ.ସହ
𝑃ሼ𝑋൏8|𝑋൐6ሽ
ൌ
1 െ𝑃ሼ𝑋൐8|𝑋൐6ሽ
ൌ
1 െ𝑃ሼ𝑋൐8ሽ/𝑃ሼ𝑋൐6ሽ
ൌ
1 െ𝑒െହ.଺ହ/𝑒െଷ.ସହ
ൎ
.8892
𝑥൒0,
𝐹ଵ/௑ሺ𝑥ሻൌ
𝑃ሼ1/𝑋൑𝑥ሽ
ൌ
𝑃ሼ𝑋൑0ሽ  ൅ 𝑃ሼ𝑋൒1/𝑥ሽ
ൌ
1/2 ൅1 െ𝐹௑ሺ1/𝑥ሻ
𝑓ଵ/௑ሺ𝑥ሻ
ൌ
𝑥െଶ𝑓௑ሺ1/𝑥ሻ
ൌ
1
𝑥ଶ𝜋ሺ1 ൅ሺ1/𝑥ሻଶሻ
ൌ
𝑓௑ሺ𝑥ሻ
𝑥൏0
𝑋
𝑛
𝑛
35𝑋െሺ𝑛െ𝑋ሻൌ36𝑋െ𝑛
𝑎ൌ𝑃ሼ36𝑋െ𝑛൐0ሽൌ𝑃ሼ𝑋൐𝑛/36ሽ
𝑋
𝑛
𝑝ൌ1/38.
𝑛ൌ34,
𝑎
ൌ
𝑃ሼ𝑋൒1ሽ
ൌ
𝑃ሼ𝑋൐.5ሽ        ሺthe continuity correctionሻ
ൌ
𝑃൝
𝑋െ34/38
34ሺ1/38ሻሺ37/38ሻ
ඥ
൐
.5 െ34/38
34ሺ1/38ሻሺ37/38ሻ
ඥ
ൡ
   ൌ𝑃൝
𝑋െ34/38
34ሺ1/38ሻሺ37/38ሻ
ඥ
൐െ.4229ൡ
   ൎΦ ሺ.4229ሻ
   ൎ.6638
770 of 848

exact probability in this case is 
)
b. When 
The exact probability—namely, the probability that a binomial 
 random variable is greater than 27—is .3961.
c. When 
The exact probability in this case is .0021.
5.18.
 If  denotes the lifetime of the battery, then the desired probability,
 can be determined as follows:
Another approach is to directly condition on the type of battery and then use
the lack-of-memory property of exponential random variables. That is, we
could do the following:
1 െሺ37/38ሻଷସൌ.5961.
𝑛ൌ1000,
𝑎ൌ
𝑃ሼ𝑋൐27.5ሽ
ൌ
𝑃൝
𝑋െ1000/38
1000ሺ1/38ሻሺ37/38ሻ
ඥ
൐
27.5 െ1000/38
1000ሺ1/38ሻሺ37/38ሻ
ඥ
ൡ
ൎ
1 െΦ ሺ.2339ሻ
ൎ
.4075
𝑛ൌ1000,
𝑝ൌ1/38
𝑛ൌ100,000,
𝑎
ൌ
𝑃ሼ𝑋൐2777.5ሽ
ൌ
𝑃൝
𝑋െ100000/38
100000ሺ1/38ሻሺ37/38ሻ
ඥ
൐
2777.5 െ100000/38
100000ሺ1/38ሻሺ37/38ሻ
ඥ
ൡ
ൎ
1 െΦ ሺ2.883ሻ
ൎ
.0020
𝑋
𝑃ሼ𝑋൐𝑠൅𝑡||𝑋൐𝑡ሽ,
𝑃ሼ𝑋൐𝑠൅𝑡|𝑋൐𝑡ሽ
ൌ
𝑃ሼ𝑋൐𝑠൅𝑡, 𝑋൐𝑡ሽ
𝑃ሼ𝑋൐𝑡ሽ
ൌ
𝑃ሼ𝑋൐𝑠൅𝑡ሽ
𝑃ሼ𝑋൐𝑡ሽ
ൌ
𝑃ሼ𝑋൐𝑠൅𝑡|battery is type 1ሽ𝑝ଵ
     ൅𝑃ሼ𝑋൐𝑠൅𝑡|battery is type 2ሽ𝑝ଶ
𝑃ሼ𝑋൐𝑡|battery is type 1ሽ𝑝ଵ
     ൅𝑃ሼ𝑋൐𝑡|battery is type 2ሽ𝑝ଶ
ൌ
𝑒െఒభሺ௦൅௧ሻ𝑝ଵ൅𝑒െఒమሺ௦൅௧ሻ𝑝ଶ
𝑒െఒభ௧𝑝ଵ൅𝑒െఒమ௧𝑝ଶ
771 of 848

Now for 
 use
5.19.
 Let 
 be an exponential random variable with mean 
a. The value  should be such that 
 Therefore,
or 
b. 
5.20.
a. 
b. Using the fact that  has the same distribution as 
 where  is a
standard normal random variable, yields
𝑃ሼ𝑋൐𝑠൅𝑡|𝑋൐𝑡ሽ
  ൌ𝑃ሼ𝑋൐𝑠൅𝑡|𝑋൐𝑡,  type 1ሽ𝑃ሼtype 1|𝑋൐𝑡ሽ
  ൅𝑃ሼ𝑋൐𝑠൅𝑡|𝑋൐𝑡, type 2ሽ𝑃ሼtype 2|𝑋൐𝑡ሽ
  ൌ𝑒െఒభ௦𝑃ሼtype 1|𝑋൐𝑡ሽ൅𝑒െఒమ௦𝑃ሼtype 2|𝑋൐𝑡ሽ
𝑖ൌ1,2,
𝑃ሼtype 𝑖|𝑋൐𝑡ሽ
ൌ
𝑃ሼtype 𝑖, 𝑋൐𝑡ሽ
𝑃ሼ𝑋൐𝑡ሽ
ൌ
𝑃ሼ𝑋൐𝑡|type 𝑖ሽ𝑝௜
𝑃ሼ𝑋൐𝑡|type 1ሽ𝑝ଵ൅𝑃ሼ𝑋൐𝑡|type 2ሽ𝑝ଶ
ൌ
𝑒െఒ೔௧𝑝௜
𝑒െఒభ௧𝑝ଵ൅𝑒െఒమ௧𝑝ଶ
𝑋௜
𝑖, 𝑖ൌ1, 2.
𝑐
𝑃ሼ𝑋ଵ൐𝑐ሽൌ.05.
𝑒െ௖ൌ.05 ൌ1/20
𝑐ൌlogሺ20ሻൌ2.996.
𝑃ሼ𝑋ଶ൐𝑐ሽൌ𝑒െ௖/ଶൌ
1
20
√
ൌ.2236
𝐸ሾሺ𝑍െ𝑐ሻ൅ሿ
ൌ
1
2𝜋
√
඲
െஶ
ஶ
ሺ𝑥െ𝑐ሻ൅𝑒െ௫మ/ଶ𝑑𝑥
ൌ
1
2𝜋
√
඲
௖
ஶ
ሺ𝑥െ𝑐ሻ𝑒െ௫మ/ଶ𝑑𝑥
ൌ
1
2𝜋
√
඲
௖
ஶ
𝑥𝑒െ௫మ/ଶ𝑑𝑥െ
1
2𝜋
√
඲
௖
ஶ
𝑐𝑒െ௫మ/ଶ𝑑𝑥
ൌ
െ1
2𝜋
√
𝑒െ௫మ/ଶ|
||
|௖
ஶ
െ𝑐ሺ1 െΦ ሺ𝑐ሻሻ
ൌ
1
2𝜋
√
𝑒െ௖మ/ଶെ𝑐ሺ1 െΦ ሺ𝑐ሻሻ
𝑋
𝜇൅𝜎𝑍,
𝑍
772 of 848

where 
5.21.
 Only (b) is true.
5.22.
a. If 
 then for 
Hence,
The argument when 
 is similar.
b. For 
Differentiation yields
c. 
d. For 
Differentiating gives
e. Using that 
 the result follows from
(a), (b), and (d). A direct argument is that, for 
𝐸ሾሺ𝑋െ𝑐ሻ൅ሿ
ൌ
𝐸ሾሺ𝜇൅𝜎𝑍െ𝑐ሻ൅ሿ
ൌ
𝐸ቈ൬𝜎൬𝑍െ𝑐െ𝜇
𝜎
൰൰
൅
቉
ൌ
𝐸ቈ𝜎൬𝑍െ𝑐െ𝜇
𝜎
൰
൅
቉
ൌ
𝜎𝐸ቈ൬𝑍െ𝑐െ𝜇
𝜎
൰
൅
቉
ൌ
𝜎⎡
⎣
1
2𝜋
√
𝑒െ௔మ/ଶെ𝑎ሺ1 െΦ ሺ𝑎ሻሻ⎤
⎦
 𝑎ൌ𝑐െ𝜇
𝜎
.
𝑏൐0,
0 ൏𝑥൏𝑏,
𝑃ሺ𝑏𝑈൏𝑥ሻൌ𝑃ሼ𝑈൏𝑥/𝑏ሽൌ𝑥/𝑏.
𝑓௕௎ሺ𝑥ሻൌ1/𝑏,0 ൏𝑥൏𝑏
𝑏൏0
𝑎൏𝑥൏1 ൅𝑎,
𝑃ሼ𝑎൅𝑈൏𝑥ሽൌ𝑃ሼ𝑈൏𝑥െ𝑎ሽൌ𝑥െ𝑎
𝑓௔൅௎ሺ𝑥ሻൌ1,𝑎൏𝑥൏1 ൅𝑎
𝑎൅ሺ𝑏െ𝑎ሻ𝑈
0 ൏𝑥൏1/2,
𝑃ሼminሺ𝑈, 1 െ𝑈ሻ൏𝑥ሽ
ൌ
𝑃ሺሼ𝑈൏𝑥ሽ∪ሼ𝑈൐1 െ𝑥ሽሻ
ൌ
𝑃ሼ𝑈൏𝑥ሽ൅𝑃ሼ𝑈൐1 െ𝑥ሽൌ2𝑥
𝑓୫୧୬ ሺ௎, ଵെ௎ሻሺ𝑥ሻൌ2, 0 ൏𝑥൏1/2
max ሺ𝑈,1 െ𝑈ሻൌ1 െmin ሺ𝑈,1 െ𝑈ሻ,
1/2 ൏𝑥൏1,
773 of 848

Hence,
5.23.
a. 
b. 
5.24.
a. 
b. With  being exponential with rate 
c. 
 Hence,
𝑃ሼmax ሺ𝑈, 1 െ𝑈ሻ൏𝑥ሽ
ൌ
1 െ𝑃ሼmax ሺ𝑈, 1 െ𝑈ሻ൐𝑥ሽ
ൌ
1 െ𝑃ሺሼ𝑈൐𝑥ሽ∪ሼ𝑈൏1 െ𝑥ሽሻ
ൌ
1 െሺ1 െ𝑥ሻെሺ1 െ𝑥ሻൌ2𝑥െ1
𝑓୫ୟ୶ ሺ௎,ଵെ௎ሻሺ𝑥ሻൌ2, 1/2 ൏𝑥൏1
׬െஶ
଴
𝑒௫𝑑𝑥൅1 ൅׬ଵ
ஶ𝑒െሺ௫െଵሻ𝑑𝑥ൌ1 ൅1 ൅1 ൌ3.
𝐸ሾ𝑋ሿൌ1/2
𝜃
1 ൅𝜃׬଴
ஶሺ1 ൅𝑥ሻ𝜃𝑒െఏ௫𝑑𝑥ൌ
𝜃
1 ൅𝜃ሺ1 ൅1
𝜃ሻൌ1.
𝑌
𝜃,
𝐸ሾ𝑋ሿൌ
𝜃
1 ൅𝜃ሺ𝐸ሾ𝑌ሿ൅𝐸ሾ𝑌ଶሿሻൌ
2 ൅𝜃
𝜃ሺ1 ൅𝜃ሻ.
𝐸ሾ𝑋ଶሿൌ
𝜃
1 ൅𝜃ሺ𝐸ሾ𝑌ଶሿ൅𝐸ሾ𝑌ଷሿሻൌ
𝜃
1 ൅𝜃ሺ2
𝜃ଶ൅6
𝜃ଷሻ.
Varሺ𝑋ሻൌ
𝜃
1 ൅𝜃ሺ2
𝜃ଶ൅6
𝜃ଷሻെሺ2 ൅𝜃
𝜃ሺ1 ൅𝜃ሻሻ
ଶ
6.1.
a. 
b. Let 
 Then
c. 
d. 
e. 
6.2.
a. With 
 we have
Hence,
3𝐶൅6𝐶ൌ1 ⇒𝐶ൌ1/9
𝑝ሺ𝑖,𝑗ሻൌ𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ.
𝑝ሺ1, 1ሻൌ4/9,  𝑝ሺ1, 0ሻൌ2/9,  𝑃ሺ0, 1ሻൌ1/9,  𝑝ሺ0, 0ሻൌ2/9
ሺ12ሻ!
2଺
ሺ1/9ሻ଺ሺ2/9ሻ଺
ሺ12ሻ!
ሺ4!ሻଷሺ1/3ሻଵଶ
෍
௜ൌ଼
ଵଶ
ቆ12
𝑖ቇሺ2/3ሻ௜ሺ1/3ሻଵଶെ௜
𝑝௝ൌ𝑃ሼ𝑋𝑌𝑍ൌ𝑗ሽ,
𝑝଺ൌ𝑝ଶൌ𝑝ସൌ𝑝ଵଶൌ1/4
774 of 848

b. With 
 we have
Hence,
6.3.
 In this solution, we will make use of the identity
which follows because 
 is the density function of a
gamma random variable with parameters 
 and  and must thus
integrate to 1.
a. 
Hence, 
b. Since the joint density is nonzero only when 
 and 
 we
have, for 
For 
c. 
𝐸ሾ𝑋𝑌𝑍ሿൌሺ6 ൅2 ൅4 ൅12ሻ/4 ൌ6
𝑞௝ൌ𝑃ሼ𝑋𝑌൅𝑋𝑍൅𝑌𝑍ൌ𝑗ሽ,
𝑞ଵଵൌ𝑞ହൌ𝑞଼ൌ𝑞ଵ଺ൌ1/4
𝐸ሾ𝑋𝑌൅𝑋𝑍൅𝑌𝑍ሿൌሺ11 ൅5 ൅8 ൅16ሻ/4 ൌ10
඲
଴
ஶ
𝑒െ௫𝑥௡𝑑𝑥ൌ𝑛!
𝑒െ௫𝑥௡/𝑛!, 𝑥൐0,
𝑛൅1
𝜆
1
ൌ
𝐶඲
଴
ஶ
𝑒െ௬඲
െ௬
௬
ሺ𝑦െ𝑥ሻ 𝑑𝑥 𝑑𝑦
ൌ
𝐶඲
଴
ஶ
𝑒െ௬2𝑦ଶ 𝑑𝑦ൌ4𝐶
𝐶ൌ1/4.
𝑦൐𝑥
𝑦൐െ𝑥,
𝑥൐0,
𝑓௑ሺ𝑥ሻൌ
1
4 ඲
௫
ஶ
ሺ𝑦െ𝑥ሻ 𝑒െ௬𝑑𝑦
ൌ
1
4 ඲
଴
ஶ
𝑢𝑒െሺ௫൅௨ሻ𝑑𝑢
ൌ
1
4 𝑒െ௫
𝑥൏0,
𝑓௑ሺ𝑥ሻ
ൌ
1
4 ඲
െ௫
ஶ
ሺ𝑦െ𝑥ሻ 𝑒െ௬𝑑𝑦
ൌ
1
4 ሾെ𝑦𝑒െ௬െ𝑒െ௬൅𝑥𝑒െ௬ሿെ௫
ஶ
ൌሺെ2𝑥𝑒௫൅𝑒௫ሻ/4
𝑓௒ሺ𝑦ሻൌ1
4 𝑒െ௬׬െ௬
௬ሺ𝑦െ𝑥ሻ 𝑑𝑥ൌ1
2 𝑦ଶ𝑒െ௬
775 of 848

d. 
e. 
6.4.
 The multinomial random variables 
 represent the
numbers of each of the types of outcomes 
 that occur in n
independent trials when each trial results in one of the outcomes
with respective probabilities 
 Now, say that a trial results in a
category 1 outcome if that trial resulted in any of the outcome types
 say that a trial results in a category 2 outcome if that trial resulted
in any of the outcome types 
 and so on. With these
definitions, 
 represent the numbers of category 1 outcomes,
category 2 outcomes, up to category k outcomes when n independent
trials that each result in one of the categories 
 with respective
probabilities 
 are performed. But by definition,
such a vector has a multinomial distribution.
6.5.
a. Letting 
 we have
b. Letting 
 we have
c. Letting 
 we have
6.6.
a. 
𝐸ሾ𝑋ሿ
ൌ
1
4 ቎඲
଴
ஶ
𝑥𝑒െ௫𝑑𝑥൅඲
െஶ
଴
ሺെ2𝑥ଶ𝑒௫൅𝑥𝑒௫ሻ 𝑑𝑥቏
ൌ
1
4 ቎1 െ඲
଴
ஶ
ሺ2𝑦ଶ𝑒െ௬൅𝑦𝑒െ௬ሻ 𝑑𝑦቏
ൌ
1
4ሾ1 െ4 െ1ሿൌെ1
𝐸ሾ𝑌ሿൌ1
2 ׬଴
ஶ𝑦ଷ𝑒െ௬ 𝑑𝑦ൌ3
𝑋௜,  𝑖ൌ1, …, 𝑟,
1, …, 𝑟
1, …, 𝑟
𝑝ଵ, …, 𝑝௥.
1, …, 𝑟ଵ;
𝑟ଵ൅1, …, 𝑟ଵ൅𝑟ଶ;
𝑌ଵ, …, 𝑌௞
1, …, 𝑘
෍
௝ൌ௥೔െభ൅ଵ
௥೔െభ൅௥೔
𝑝௜, 𝑖ൌ 1, …, 𝑘,
𝑝௝ൌ𝑃ሼ𝑋𝑌𝑍ൌ𝑗ሽ,
𝑝ଵൌ1/8, 𝑝ଶൌ3/8, 𝑝ସൌ3/8, 𝑝଼ൌ1/8
𝑝௝ൌ𝑃ሼ𝑋𝑌൅𝑋𝑍൅𝑌𝑍ൌ𝑗ሽ,
𝑝ଷൌ1/8, 𝑝ହൌ3/8, 𝑝଼ൌ3/8, 𝑝ଵଶൌ1/8
𝑝௝ൌ𝑃൛𝑋ଶ൅𝑌𝑍ൌ𝑗ൟ,
𝑝ଶൌ1/8, 𝑝ଷൌ1/4, 𝑝ହൌ1/4, 𝑝଺ൌ1/4, 𝑝଼ൌ1/8
1
ൌ
඲
଴
ଵ
඲
ଵ
ହ
ሺ𝑥/5 ൅𝑐𝑦ሻ 𝑑𝑦 𝑑𝑥
ൌ
඲
଴
ଵ
ሺ4𝑥/5 ൅12𝑐ሻ 𝑑𝑥
ൌ
12𝑐൅2/5
776 of 848

Hence, 
b. No, the density does not factor.
c. 
6.7.
a. Yes, the joint density function factors.
b. 
c. 
d. 
e. 
f. 
6.8.
 Let 
 denote the time at which a shock type  of 
 occurs.
For 
6.9.
a. No, advertisements on pages having many ads are less likely to be
chosen than are ones on pages with few ads.
b. 
c. 
d. 
𝑐ൌ1/20.
𝑃ሼ𝑋൅𝑌൐3ሽ
ൌ඲
଴
ଵ
඲
ଷെ௫
ହ
ሺ𝑥/5 ൅𝑦/20ሻ 𝑑𝑦 𝑑𝑥
ൌ඲
଴
ଵ
ሾሺ2 ൅𝑥ሻ𝑥/5 ൅25/40 െሺ3 െ𝑥ሻଶ/40ሿ 𝑑𝑥
ൌ1/5 ൅1/15 ൅5/8 െ19/120 ൌ11/15
𝑓௑ሺ𝑥ሻൌ𝑥׬଴
ଶ𝑦𝑑𝑦ൌ2𝑥,  0 ൏𝑥൏1
𝑓௒ሺ𝑦ሻൌ𝑦׬଴
ଵ𝑥𝑑𝑥ൌ𝑦/2, 0 ൏𝑦൏2
𝑃ሼ𝑋൏𝑥, 𝑌൏𝑦ሽ
ൌ
𝑃ሼ𝑋൏𝑥ሽ𝑃ሼ𝑌൏𝑦ሽ
ൌ
minሺ1, 𝑥ଶሻminሺ1,  𝑦ଶ/4ሻ,  𝑥  ൐ 0, 𝑦  ൐ 0
𝐸ሾ𝑌ሿൌ׬଴
ଶ𝑦ଶ/2𝑑𝑦ൌ4/3
𝑃ሼ𝑋൅𝑌൏1ሽ
ൌ඲
଴
ଵ
𝑥඲
଴
ଵെ௫
𝑦 𝑑𝑦 𝑑𝑥
ൌ
1
2 ඲
଴
ଵ
𝑥ሺ1 െ𝑥ሻଶ𝑑𝑥ൌ1/24
𝑇௜
𝑖,
𝑖ൌ1,2,3,
𝑠൐0,𝑡൐0,
𝑃ሼ𝑋ଵ൐𝑠, 𝑋ଶ൐𝑡ሽ
ൌ
𝑃ሼ𝑇ଵ൐𝑠, 𝑇ଶ൐𝑡, 𝑇ଷ൐maxሺ𝑠, 𝑡ሻሽ
ൌ
𝑃ሼ𝑇ଵ൐𝑠ሽ𝑃ሼ𝑇ଶ൐𝑡ሽ𝑃ሼ𝑇ଷ൐maxሺ𝑠, 𝑡ሻሽ
ൌ
expሼെ𝜆ଵ𝑠ሽexpሼെ𝜆ଶ𝑡ሽexpሼെ𝜆ଷmaxሺ𝑠, 𝑡ሻሽ
ൌ
expሼെሺ𝜆ଵ𝑠൅𝜆ଶ𝑡൅𝜆ଷmaxሺ𝑠, 𝑡ሻሻሽ
1
𝑚
𝑛ሺ𝑖ሻ
𝑛
෍
௜ൌଵ
௠
𝑛ሺ𝑖ሻ
𝑛𝑚
ൌ𝑛̅ ̅/𝑛,  where 𝑛̅ ̅ ൌ෍
௜ൌଵ
௠
𝑛ሺ𝑖ሻ/𝑚
ሺ1 െ𝑛̅ ̅/𝑛ሻ௞െଵ1
𝑚
𝑛ሺ𝑖ሻ
𝑛
1
𝑛ሺ𝑖ሻൌሺ1 െ𝑛̅ ̅/𝑛ሻ௞െଵ/ሺ𝑛𝑚ሻ
777 of 848

e. 
f. The number of iterations is geometric with mean 
6.10.
a. 
b. Step 2. Generate a uniform (0, 1) random variable 
 If
 go to step 3. Otherwise return to step 1.
Step 3. Generate a uniform (0, 1) random variable 
 and select
the element on page  in position 
6.11.
 Yes, they are independent. This can be easily seen by
considering the equivalent question of whether 
 is independent of 
But this is indeed so, since knowing when the first random variable
greater than  occurs does not affect the probability distribution of its
value, which is the uniform distribution on (  1).
6.12.
 Let 
 denote the probability of obtaining  points on a single
throw of the dart. Then
a. 
b. 
c. 
d. 
e. 
f. 
6.13.
 Let  be a standard normal random variable.
a. 
෍
௞ൌଵ
ஶ
1
𝑛𝑚ሺ1 െ𝑛̅ ̅/𝑛ሻ௞െଵൌ1
𝑛̅ ̅𝑚.
𝑛𝑛
√
𝑃ሼ𝑋ൌ𝑖ሽൌ1/𝑚,  𝑖ൌ1, . . . ,𝑚.
𝑈.
𝑈൏𝑛ሺ𝑋ሻ/𝑛,
𝑈,
𝑋
ሾ𝑛ሺ𝑋ሻ𝑈ሿ൅1
𝑋ே
𝑁.
𝑐
𝑐,
𝑝௜
𝑖
𝑝ଷ଴
ൌ
𝜋/36
𝑝ଶ଴
ൌ
4𝜋/36 െ𝑝ଷ଴ൌ𝜋/12
𝑝ଵ଴
ൌ
9𝜋/36 െ𝑝ଶ଴െ𝑝ଷ଴ൌ5𝜋/36
𝑝଴
ൌ
1 െ𝑝ଵ଴െ𝑝ଶ଴െ𝑝ଷ଴ൌ1 െ𝜋/4
𝜋/12
𝜋/9
1 െ𝜋/4
𝜋ሺ30/36 ൅20/12 ൅50/36ሻൌ35𝜋/9
ሺ𝜋/4ሻଶ
2ሺ𝜋/36ሻሺ1 െ𝜋/4ሻ൅2ሺ𝜋/12ሻሺ5𝜋/36ሻ
𝑍
𝑃ቐ෍
௜ൌଵ
ସ
𝑋௜൐0ቑ
ൌ
𝑃
⎧
⎨
⎩
⎪
⎪
⎪
⎪
෍
௜ൌଵ
ସ
𝑋௜െ6
24
√
൐െ6
24
√
⎫
⎬
⎭
⎪
⎪
⎪
⎪
ൎ
𝑃ሼ𝑍൐െ1.2247ሽൎ.8897
778 of 848

b. 
c. 
6.14.
 In the following,  does not depend on 
which shows that, conditional on 
 is a Poisson random
variable with mean 
 That is,
6.15.
a. The Jacobian of the transformation is
As the equations 
 imply that 
 we
obtain
or, equivalently,
b. For 
𝑃ቐ෍
௜ൌଵ
ସ
𝑋௜൐0| ෍
௜ൌଵ
ଶ
𝑋௜ൌെ5ቑൌ
𝑃ሼ𝑋ଷ൅𝑋ସ൐5ሽ
ൌ
𝑃ቊ𝑋ଷ൅𝑋ସെ3
12
√
൐2/ 12
√
ቋ
ൎ
𝑃ሼ𝑍൐.5774ሽൎ.2818
𝑃ቐ෍
௜ൌଵ
ସ
𝑋௜൐0 |𝑋ଵൌ5ቑ
ൌ
𝑃ሼ𝑋ଶ൅𝑋ଷ൅𝑋ସ൐െ5ሽ
ൌ
𝑃ቊ𝑋ଶ൅𝑋ଷ൅𝑋ସെ4.5
18
√
൐െ9.5/ 18
√
ቋ
ൎ
𝑃ሼ𝑍൐െ2.239ሽൎ.9874
𝐶
𝑛.
𝑃ሼ𝑁ൌ𝑛|𝑋ൌ𝑥ሽൌ
𝑓௑หேሺ𝑥|𝑛ሻ𝑃ሼ𝑁ൌ𝑛ሽ/𝑓௑ሺ𝑥ሻ
ൌ
𝐶
1
ሺ𝑛െ1ሻ! ሺ𝜆𝑥ሻ௡െଵሺ1 െ𝑝ሻ௡െଵ
ൌ
𝐶ሺ𝜆ሺ1 െ𝑝ሻ𝑥ሻ௡െଵ/ሺ𝑛െ1ሻ!
𝑋ൌ𝑥, 𝑁െ1
𝜆ሺ1 െ𝑝ሻ𝑥.
𝑃ሼ𝑁ൌ𝑛 |𝑋ൌ𝑥ሽ
ൌ
𝑃ሼ𝑁െ1 ൌ𝑛െ1 |𝑋ൌ𝑥ሽ
ൌ
𝑒െఒሺଵെ௣ሻ௫ሺ𝜆ሺ1 െ𝑝ሻ𝑥ሻ௡െଵ/ሺ𝑛െ1ሻ!,  𝑛  ൒ 1.
𝐽ൌቤ1
0
1
1ቤൌ1
𝑢ൌ𝑥, 𝑣ൌ𝑥൅𝑦
𝑥ൌ𝑢, 𝑦ൌ𝑣െ𝑢,
𝑓௎,௏ሺ𝑢,𝑣ሻൌ𝑓௑,௒ሺ𝑢,𝑣െ𝑢ሻൌ1, 0 ൏𝑢൏1, 0 ൏𝑣െ𝑢൏1
𝑓௎,௏ሺ𝑢,𝑣ሻൌ1, maxሺ𝑣െ1,0ሻ൏𝑢൏minሺ𝑣,1ሻ
0 ൏𝑣൏1,
𝑓௏ሺ𝑣ሻൌ඲
଴
௩
𝑑𝑢ൌ𝑣
779 of 848

For 
6.16.
 Let  be a uniform random variable on (7, 11). If you bid
 you will be the high bidder with probability
Hence, your expected gain—call it 
—if you bid  is
Calculus shows this is maximized when 
6.17.
 Let 
 be a permutation of 
 Then
Therefore, the desired probability is ! 
 which reduces to 
when all 
6.18.
a. Because 
 it follows that 
b. Consider the 
 coordinates whose -values are equal to 0, and
call them the red coordinates. Because the  coordinates whose 
-values are equal to 1 are equally likely to be any of the 
 sets
of  coordinates, it follows that the number of red coordinates
among these  coordinates has the same distribution as the
number of red balls chosen when one randomly chooses  of a set
of  balls of which 
 are red. Therefore, 
 is a hypergeometric
random variable.
c. 
d. Using the formula for the variance of a hypergeometric given in
Example 8j of Chapter 4
, we obtain
1 ൑𝑣൑2,
𝑓௏ሺ𝑣ሻൌ඲
௩െଵ
ଵ
𝑑𝑢ൌ2 െ𝑣
𝑈
𝑥 ,7 ൑𝑥൑10,
ሺ𝑃ሼ𝑈൏𝑥ሽሻ
ଷൌቆ𝑃ቊ𝑈െ7
4
൏𝑥െ7
4
ቋቇ
ଷ
ൌቆ𝑥െ7
4
ቇ
ଷ
𝐸ሾ𝐺ሺ𝑥ሻሿ
𝑥
𝐸ሾ𝐺ሺ𝑥ሻሿൌ1
64 ሺ𝑥െ7ሻଷሺ10 െ𝑥ሻ
𝑥ൌ37/4.
𝑖ଵ,𝑖ଶ, . . . ,𝑖௡,
1,2, . . . ,𝑛.
𝑃ሼ𝑋ଵ
ൌ
𝑖ଵ,  𝑋ଶൌ𝑖ଶ, .   .   . ,  𝑋௡ൌ𝑖௡ሽ
ൌ
𝑃ሼ𝑋ଵൌ𝑖ଵሽ𝑃ሼ𝑋ଶൌ𝑖ଶሽ ⋯𝑃ሼ𝑋௡ൌ𝑖௡ሽ
ൌ
𝑝௜భ𝑝௜మ⋯𝑝௜೙
ൌ
𝑝ଵ𝑝ଶ⋯𝑝௡
𝑛𝑝ଵ𝑝ଶ⋯𝑝௡,
𝑛!
𝑛௡
𝑝௜ൌ1/𝑛.
෍
௜ൌଵ
௡
𝑋௜ൌ෍
௜ൌଵ
௡
𝑌௜,
𝑁ൌ2𝑀.
𝑛െ𝑘
𝑌
𝑘
𝑋
ቆ𝑛
𝑘ቇ
𝑘
𝑘
𝑘
𝑛
𝑛െ𝑘
𝑀
𝐸ሾ𝑁ሿൌ𝐸ሾ2𝑀ሿൌ2𝐸ሾ𝑀ሿൌ2𝑘ሺ𝑛െ𝑘ሻ
𝑛
Varሺ𝑁ሻൌ4 Varሺ𝑀ሻൌ4 𝑛െ𝑘
𝑛െ1 𝑘ሺ1 െ𝑘/𝑛ሻሺ𝑘/𝑛ሻ
780 of 848

6.19.
a. First note that 
 is a normal random variable
with mean 0 and variance 
 that is independent of 
Consequently, given that 
 is a normal random variable
with mean  and variance 
b. Because the conditional density function of 
 given that 
 is
a density function whose argument is  anything that does not
depend on  can be regarded as a constant. (For instance,  is
regarded as a fixed constant.) In the following, the quantities
 are all constants that do not depend on 
But we recognize the preceding as the density function of a normal
random variable with mean 
 and variance 
6.20.
𝑆௡െ𝑆௞ൌ
෍
௜ൌ௞൅ଵ
௡
𝑍௜
𝑛െ𝑘
𝑆௞.
𝑆௞ൌ𝑦, 𝑆௡
𝑦
𝑛െ𝑘.
𝑆௞
𝑆௡ൌ𝑥
𝑦,
𝑦
𝑥
𝐶௜, 𝑖ൌ1, 2, 3, 4
𝑦:
𝑓ௌೖหௌ೙ሺ𝑦|𝑥ሻ
ൌ
𝑓ௌೖ,ௌ೙ሺ𝑦, 𝑥ሻ
𝑓ௌ೙ሺ𝑥ሻ
ൌ𝐶ଵ𝑓ௌ೙หௌೖሺ𝑥|𝑦ሻ𝑓ௌೖሺ𝑦ሻ ൭where 𝐶ଵൌ
1
𝑓ௌ೙ሺ𝑥ሻ൱
ൌ𝐶ଵ
1
2𝜋
√
𝑛െ𝑘
√
𝑒െሺ௫െ௬ሻమ/ଶሺ௡െ௞ሻ
1
2𝜋
√
𝑘
√𝑒െ௬మ/ଶ௞
ൌ𝐶ଶexp൝െሺ𝑥െ𝑦ሻଶ
2ሺ𝑛െ𝑘ሻെ𝑦ଶ
2𝑘ൡ
ൌ𝐶ଷexpቊ
2𝑥𝑦
2ሺ𝑛െ𝑘ሻെ
𝑦ଶ
2ሺ𝑛െ𝑘ሻെ𝑦ଶ
2𝑘ቋ
ൌ𝐶ଷexpቊെ
𝑛
2𝑘ሺ𝑛െ𝑘ሻቆ𝑦ଶെ2 𝑘
𝑛𝑥𝑦ቇቋ
ൌ𝐶ଷexp൝െ
𝑛
2𝑘ሺ𝑛െ𝑘ሻ൥ቆ𝑦െ𝑘
𝑛𝑥ቇ
ଶ
െቆ𝑘
𝑛𝑥ቇ
ଶ
൩ൡ
ൌ𝐶ସexp൝െ
𝑛
2𝑘ሺ𝑛െ𝑘ሻቆ𝑦െ𝑘
𝑛𝑥ቇ
ଶ
ൡ
𝑘
𝑛𝑥
𝑘ሺ𝑛െ𝑘ሻ
𝑛
.
781 of 848

a. 
Thus, the probability that 
 is the largest value is independent of
which is the largest of the other five values. (Of course, this would
not be true if the 
 had different distributions.)
b. One way to solve this problem is to condition on whether 
Now,
Also, by symmetry,
From part (a),
Thus, conditioning on whether 
 yields the result
6.21.
6.22.
 Suppose 
 and consider 
 If there have been
 failures after trial  then there have been 
 successes by that point.
Hence, the conditional distribution of 
 given that 
 is the
distribution of  plus the number of additional trials after trial  until there
have been an additional 
 successes. Hence, for 
𝑃ሼ𝑋଺൐𝑋ଵ|𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻሽ
ൌ𝑃ሼ𝑋଺൐𝑋ଵ,  𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻሽ
𝑃ሼ𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻሽ
ൌ𝑃ሼ𝑋଺ൌmaxሺ𝑋ଵ, .   .   . , 𝑋଺ሻ,  𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻሽ
1/5
ൌ5 1
6
1
5 ൌ1
6
𝑋଺
𝑋௜
𝑋଺൐𝑋ଵ.
𝑃ሼ𝑋଺൐𝑋ଶ||𝑋ଵൌmaxሺ𝑋ଵ, . . . ,𝑋ହሻ,𝑋଺൐𝑋ଵሽൌ1
𝑃ሼ𝑋଺൐𝑋ଶ||𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻ, 𝑋଺൏𝑋ଵሽൌ1
2
𝑃ሼ𝑋଺൐𝑋ଵ||𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻሽൌ1
6
𝑋଺൐𝑋ଵ
𝑃ሼ𝑋଺൐𝑋ଶ||𝑋ଵൌmaxሺ𝑋ଵ, .   .   . , 𝑋ହሻሽൌ1
6 ൅1
2
5
6 ൌ7
12
𝑃ሼ𝑋൐𝑠, 𝑌൐𝑡ሽ
ൌ
1 െ𝑃ሺሼ𝑋൑𝑠ሽ∪ሼ𝑌൑𝑡ሽሻ
ൌ
1 െ𝑃ሼ𝑋൑𝑠ሽെ𝑃ሼ𝑌൑𝑡ሽ൅𝑃ሼ𝑋൑𝑠, 𝑌൑𝑡ሽ
𝑗൏𝑖,
𝑃ሺ𝑋௥ൌ𝑖, 𝑌௦ൌ𝑗ሻ.
𝑠
𝑗
𝑗െ𝑠
𝑋௥,
𝑌௦ൌ𝑗,
𝑗
𝑗
𝑟െ𝑗൅𝑠
𝑗൏𝑖
𝑃ሺ𝑋௥ൌ𝑖,  𝑌௦ൌ𝑗ሻൌ𝑃ሺ𝑌௦ൌ𝑗ሻ𝑃ሺ𝑋௥ൌ𝑖|𝑌௦ൌ𝑗ሻ
ൌ𝑃ሺ𝑌௦ൌ𝑗ሻ𝑃ሺ𝑋௦൅௥െ௝ൌ𝑖െ𝑗ሻ
ൌቆ𝑗െ1
𝑠െ1ቇ ሺ1 െ𝑝ሻ௦𝑝௝െ௦
ቆ
𝑖െ𝑗െ1
𝑠൅𝑟െ𝑗െ1ቇ 𝑝௦൅௥െ௝ሺ1 െ𝑝ሻ௜െ௦െ௥,  𝑗൏𝑖
782 of 848

6.23.
 For 
6.24.
6.25.
a. 
b. Condition on the number of times  would advance if  played
forever, to obtain 
c. 
6.26.
a. even;
b. 1;
c. 
d. 
giving that
For 
𝑥൐𝑥଴, 𝑃ሺ𝑋൐𝑥|𝑋൐𝑥଴ሻൌ𝑃ሺ𝑋൐𝑥ሻ
𝑃ሺ𝑋൐𝑥଴ሻൌ𝑎ఒ𝑥െఒ
𝑎ఒ𝑥଴
െఒൌ𝑥଴
ఒ𝑥െఒ
඲
െஶ
ஶ
𝑓௑ห௒ሺ𝑥|𝑦ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑓ሺ𝑥, 𝑦ሻ
𝑓௒ሺ𝑦ሻ𝑓௒ሺ𝑦ሻ 𝑑𝑦
ൌ
඲
െஶ
ஶ
𝑓ሺ𝑥,  𝑦ሻ 𝑑𝑦
ൌ
𝑓௑ሺ𝑥ሻ
𝑝௜
௞ቆ1 െෑ
௝ஷ௜
ሺ1 െ𝑝௝
௞ሻቇ
𝑖
𝑖
෍
௞ൌ଴
ஶ
𝑝௜
௞ሺ1 െ𝑝௜ሻෑ
௝ஷ௜
ሺ1 െ𝑝௝
௞൅ଵሻ.
෍
௞ൌ଴
ஶ
𝑝௜
௞ሺ1 െ𝑝௜ሻෑ
௝ஷ௜
ሺ1 െ𝑝௝
௞ሻ.
ෑ
௜ൌଵ
௡
ሺ2𝛼௜െ1ሻ;
ෑ
௜ൌଵ
௡
ሺ2𝛼௜െ1ሻൌ
𝐸ሾෑ
௜ൌଵ
௡
𝑌௜ሿ
ൌ
𝑃ሺෑ
௜ൌଵ
௡
𝑌௜ൌ1ሻെ𝑃ሺෑ
௜ൌଵ
௡
𝑌௜ൌെ1ሻ
ൌ
2𝑃ሺෑ
௜ൌଵ
௡
𝑌௜ൌ1ሻെ1
𝑃ሺ𝑆 is evenሻൌ𝑃ሺෑ
௜ൌଵ
௡
𝑌௜ൌ1ሻൌ
1 ൅ෑ
௜ൌଵ
௡
ሺ2𝛼௜െ1ሻ
2
0 ൏𝑥൏1
783 of 848

where 
 does not depend on  Hence, we can
conclude that the conditional density of  given that 
 is beta with
parameters 
 As a byproduct, we also see that
 or equivalently that
𝑓௑หேሺ𝑥|𝑛ሻൌ
𝑃ሺ𝑁ൌ𝑛|𝑋ൌ𝑥ሻ𝑓௑ሺ𝑥ሻ
𝑃ሺ𝑁ൌ𝑛ሻ
ൌ
ቆ𝑛൅𝑚
𝑛
ቇ𝑥௡ሺ1 െ𝑥ሻ௠𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ
𝐵ሺ𝑎, 𝑏ሻ𝑃ሺ𝑁ൌ𝑛ሻ
ൌ
𝐾𝑥௡൅௔െଵሺ1 െ𝑥ሻ௠൅௕െଵ
𝐾ൌ
ቆ𝑛൅𝑚
𝑛
ቇ
𝐵ሺ𝑎, 𝑏ሻ𝑃ሺ𝑁ൌ𝑛ሻ
𝑥.
𝑋
𝑁ൌ𝑛
𝑛൅𝑎, 𝑚൅𝑏.
ቆ𝑛൅𝑚
𝑛
ቇ
𝐵ሺ𝑎, 𝑏ሻ𝑃ሺ𝑁ൌ𝑛ሻൌ
1
𝐵ሺ𝑎൅𝑛, 𝑏൅𝑚ሻ,
𝑃ሺ𝑁ൌ𝑛ሻൌ
ቆ𝑛൅𝑚
𝑛
ቇ𝐵ሺ𝑎൅𝑛,  𝑏൅𝑚ሻ
𝐵ሺ𝑎, 𝑏ሻ
7.1.
a. 
b. 
c. 
7.2.
 Let  equal 1 if the th ball withdrawn is white and the 
 is
black, and let  equal 0 otherwise. If  is the number of instances in
which a white ball is immediately followed by a black one, then we may
express  as
Thus,
𝑑ൌ෍
௜ൌଵ
௠
1/𝑛ሺ𝑖ሻ
𝑃ሼ𝑋ൌ𝑖ሽൌ𝑃ሼሾ𝑚𝑈ሿൌ𝑖െ1ሽൌ𝑃ሼ𝑖െ1 ൑𝑚𝑈൏𝑖ሽൌ1/𝑚, 𝑖ൌ1, . . . ,𝑚
𝐸ቈ𝑚
𝑛ሺ𝑋ሻ቉ൌ෍
௜ൌଵ
௠
𝑚
𝑛ሺ𝑖ሻ𝑃ሼ𝑋ൌ𝑖ሽൌ෍
௜ൌଵ
௠
𝑚
𝑛ሺ𝑖ሻ
1
𝑚ൌ𝑑
𝐼௝
𝑗
ሺ𝑗൅1ሻ
𝐼௝
𝑋
𝑋
𝑋ൌ
෍
௝ൌଵ
௡൅௠െଵ
𝐼௝
784 of 848

The preceding used the fact that each of the 
 balls is equally
likely to be the th one selected and, given that that selection is a white
ball, each of the other 
 balls is equally likely to be the next
ball chosen.
7.3.
 Arbitrarily number the couples, and then let  equal 1 if married
couple number 
 is seated at the same table. Then, if 
represents the number of married couples that are seated at the same
table, we have
so
a. To compute 
 in this case, consider wife number  Since
each of the 
 groups of size 3 not including her is equally
likely to be the remaining members of her table, it follows that
the probability that her husband is at her table is
Hence, 
 and so
𝐸ሾ𝑋ሿൌ
෍
௝ൌଵ
௡൅௠െଵ
𝐸ሾ𝐼௝ሿ
ൌ
෍
௝ൌଵ
௡൅௠െଵ
𝑃൛𝑗௧௛ selection is white,  ሺ𝑗൅1ሻ is blackൟ
ൌ
෍
௝ൌଵ
௡൅௠െଵ
𝑃൛𝑗௧௛ selection is whiteൟ𝑃൛ሺ𝑗൅1ሻ is black ||𝑗௧௛ is whiteൟ
ൌ
෍
௝ൌଵ
௡൅௠െଵ
𝑛
𝑛൅𝑚
𝑚
𝑛൅𝑚െ1
ൌ
𝑛𝑚
𝑛൅𝑚
𝑛൅𝑚
𝑗
𝑛൅𝑚െ1
𝐼௝
𝑗, 𝑗ൌ1, . . . ,10,
𝑋
𝑋ൌ
෍
௝ൌଵ
ଵ଴
𝐼௝
𝐸ሾ𝑋ሿൌ
෍
௝ൌଵ
ଵ଴
𝐸ሾ𝐼௝ሿ
𝐸ሾ𝐼௝ሿ
𝑗.
ቆ19
3 ቇ
ቆ1
1ቇቆ18
2 ቇ
ቆ19
3 ቇ
ൌ3
19
𝐸ሾ𝐼௝ሿൌ3/19
785 of 848

b. In this case, since the 2 men at the table of wife  are equally
likely to be any of the 10 men, it follows that the probability that
one of them is her husband is 2/10, so
7.4.
 From Example 2i
, we know that the expected number of
times that the die need be rolled until all sides have appeared at least
once is 
 Now, if we let 
denote the total number of times that side  appears, then, since
 is equal to the total number of rolls, we have
But, by symmetry, 
 will be the same for all  and thus it follows
from the preceding that 
7.5.
 Let  equal 1 if we win 1 when the th red card to show is
turned over, and let  equal 0 otherwise. (For instance, 
 will equal 1 if
the first card turned over is red.) Hence, if  is our total winnings, then
Now,  will equal 1 if  red cards appear before  black cards. By
symmetry, the probability of this event is equal to 1/2; therefore,
 and 
7.6.
 To see that 
 note that if all events occur, then both
sides of the preceding inequality are equal to  whereas if they do not
all occur, then the inequality reduces to 
 which is clearly true
in this case. Taking expectations yields
However, if we let  equal 1 if 
 occurs and 0 otherwise, then
Since 
 the result follows.
7.7.
 Imagine that the values 
 are lined up in their numerical
𝐸ሾ𝑋ሿൌ30/19
𝑗
𝐸ሾ𝐼௝ሿൌ2/10 and 𝐸ሾ𝑋ሿൌ2
6ሺ1 ൅1/2 ൅1/3 ൅1/4 ൅1/5 ൅1/6ሻൌ14.7.
𝑋௜
𝑖
෍
௜ൌଵ
଺
 𝑋௜
14.7 ൌ𝐸 ቎෍
௜ൌଵ
଺
𝑋௜቏ൌ෍
௜ൌଵ
଺
𝐸ሾ𝑋௜ሿ
𝐸ሾ𝑋௜ሿ
𝑖,
𝐸ሾ𝑋ଵሿൌ14.7/6 ൌ2.45.
𝐼௝
𝑗
𝐼௝
𝐼ଵ
𝑋
𝐸ሾ𝑋ሿൌ𝐸቎෍
௝ൌଵ
௡
𝐼௝቏ൌ
෍
௝ൌଵ
௡
𝐸ሾ𝐼௝ሿ
𝐼௝
𝑗
𝑗
𝐸ሾ𝐼௝ሿൌ1/2
𝐸ሾ𝑋ሿൌ𝑛/2.
𝑁൑𝑛െ1 ൅𝐼,
𝑛,
𝑁൑𝑛െ1,
𝐸ሾ𝑁ሿ൑𝑛െ1 ൅𝐸ሾ𝐼ሿ
𝐼௜
𝐴௜
𝐸ሾ𝑁ሿൌ𝐸቎෍
௜ൌଵ
௡
𝐼௜቏ൌ෍
௜ൌଵ
௡
𝐸ሾ𝐼௜ሿൌ෍
௜ൌଵ
௡
𝑃ሺ𝐴௜ሻ
𝐸ሾ𝐼ሿൌ𝑃ሺ𝐴ଵ⋯𝐴௡ሻ,
1,2, . . . ,𝑛
786 of 848

order and that the  values selected are considered special. From
Example 3e
, the position of the first special value, equal to the
smallest value chosen, has mean 
For a more formal argument, note that 
 if none of the 
smallest values are chosen. Hence,
which shows that  has the same distribution as the random variable of
Example 3e
 (with the notational change that the total number of
balls is now  and the number of special balls is ).
7.8.
 Let  denote the number of families that depart after the
Sanchez family leaves. Arbitrarily number all the 
 non-Sanchez
families, and let 
 equal 1 if family  departs after the
Sanchez family does. Then
Taking expectations gives
Now consider any non-Sanchez family that checked in  pieces of
luggage. Because each of the 
 pieces of luggage checked in either
by this family or by the Sanchez family is equally likely to be the last of
these 
 to appear, the probability that this family departs after the
Sanchez family is 
 Because the number of non-Sanchez families
who checked in  pieces of luggage is 
 when 
 or 
 when
 we obtain
7.9.
 Let the neighborhood of any point on the rim be the arc starting
at that point and extending for a length 1. Consider a uniformly chosen
point on the rim of the circle—that is, the probability that this point lies
on a specified arc of length  is 
—and let  denote the number of
𝑘
1 ൅𝑛െ𝑘
𝑘൅1 ൌ𝑛൅1
𝑘൅1 .
𝑋൒𝑗
𝑗െ1
𝑃ሼ𝑋൒𝑗ሽൌ
ቆ𝑛െ𝑗൅1
𝑘
ቇ
ቆ𝑛
𝑘ቇ
ൌ
ቆ𝑛െ𝑘
𝑗െ1ቇ
ቆ
𝑛
𝑗െ1ቇ
𝑋
𝑛
𝑘
𝑋
𝑁െ1
𝐼௥, 1 ൑𝑟൑𝑁െ1,
𝑟
𝑋ൌ
෍
௥ൌଵ
ேെଵ
𝐼௥
𝐸ሾ𝑋ሿൌ
෍
௥ൌଵ
ேെଵ
𝑃ሼfamily 𝑟 departs after the Sanchez familyሽ
𝑘
𝑘൅𝑗
𝑘൅𝑗
𝑘
𝑘൅𝑗.
𝑘
𝑛௞
𝑘്𝑗,
𝑛௝െ1
𝑘ൌ𝑗,
𝐸ሾ𝑋ሿൌ෍
௞
𝑘𝑛௞
𝑘൅𝑗െ1
2
𝑥
𝑥
2𝜋
𝑋
787 of 848

points that lie in its neighborhood. With  defined to equal 1 if item
number  is in the neighborhood of the random point and to equal 0
otherwise, we have
Taking expectations gives
But because item  will lie in its neighborhood if the random point is
located on the arc of length 1 going from item  in the counterclockwise
direction, it follows that
Hence,
Because 
 at least one of the possible values of  must exceed
3, proving the result.
7.10.
 If 
 then
so the Taylor series expansion of 
 about  gives
Taking expectations yields
Hence,
𝐼௝
𝑗
𝑋ൌ
෍
௝ൌଵ
ଵଽ
𝐼௝
𝐸ሾ𝑋ሿൌ
෍
௝ൌଵ
ଵଽ
𝑃ሼitem 𝑗 lies in the neighborhood of the
      random pointሽ
𝑗
𝑗
𝑃ሼitem 𝑗 lies in the neighborhood of the random pointሽൌ1
2𝜋
𝐸ሾ𝑋ሿൌ19
2𝜋൐3
𝐸ሾ𝑋ሿ൐3,
𝑋
𝑔ሺ𝑥ሻൌ𝑥ଵ/ଶ,
𝑔ᇱሺ𝑥ሻൌ1
2 𝑥െଵ/ଶ,  𝑔ᇳሺ𝑥ሻൌെ1
4 𝑥െଷ/ଶ
𝑥
√
𝜆
𝑋
√
ൎ
𝜆
√൅1
2 𝜆െଵ/ଶሺ𝑋െ𝜆ሻെ1
8 𝜆െଷ/ଶሺ𝑋െ𝜆ሻଶ
𝐸ሾ𝑋
√ሿ
ൎ
𝜆
√൅1
2 𝜆െଵ/ଶ𝐸ሾ𝑋െ𝜆ሿെ1
8 𝜆െଷ/ଶ𝐸ሾሺ𝑋െ𝜆ሻଶሿ
ൌ
𝜆
√െ1
8 𝜆െଷ/ଶ𝜆
ൌ
𝜆
√െ1
8 𝜆െଵ/ଶ
788 of 848

7.11.
 Number the tables so that tables 1, 2, and 3 are the ones with
four seats and tables 4, 5, 6, and 7 are the ones with two seats. Also,
number the women, and let 
 equal 1 if woman  is seated with her
husband at table  Note that
and
Now,  denotes the number of married couples that are seated at the
same table, we have
7.12.
 Let 
 equal 1 if individual  does not recruit anyone, and let 
equal 0 otherwise. Then
Hence,
Varሺ𝑋
√ሻൌ
𝐸ሾ𝑋ሿെሺ𝐸ሾ𝑋
√ሿሻ
ଶ
≃
𝜆െቆ𝜆
√െ1
8 𝜆െଵ/ଶቇ
ଶ
ൌ
1/4 െ
1
64𝜆
ൌ
1/4
𝑋௜,௝
𝑖
𝑗.
𝐸ሾ𝑋௜,௝ሿൌ
ቆ2
2ቇቆ18
2 ቇ
ቆ20
4 ቇ
ൌ3
95 , 𝑗ൌ1, 2, 3
𝐸ሾ𝑋௜,௝ሿൌ
1
ቆ20
2 ቇ
ൌ
1
190 , 𝑗ൌ4, 5, 6, 7
𝑋
𝐸ሾ𝑋ሿ
ൌ
𝐸቎෍
௜ൌଵ
ଵ଴
෍
௝ൌଵ
଻
𝑋௜,௝቏
ൌ
෍
௜ൌଵ
ଵ଴
෍
௝ൌଵ
ଷ
𝐸ሾ𝑋௜,௝ሿ൅෍
௜ൌଵ
ଵ଴
෍
௝ൌସ
଻
𝐸ሾ𝑋௜,௝ሿ
𝑋௜
𝑖
𝑋௜
𝐸ሾ𝑋௜ሿ
ൌ
𝑃ሼ𝑖 does not recruit any of 𝑖൅1, 𝑖൅2, .   .   . , 𝑛ሽ
ൌ
𝑖െ1
𝑖
𝑖
𝑖൅1⋯𝑛െ2
𝑛െ1
ൌ
𝑖െ1
𝑛െ1
𝐸቎෍
௜ൌଵ
௡
𝑋௜቏ൌ෍
௜ൌଵ
௡
𝑖െ1
𝑛െ1 ൌ𝑛
2
789 of 848

From the preceding, we also obtain
Now, for 
Thus,
Therefore,
7.13.
 Let 
 equal 1 if the th triple consists of one of each type of
player. Then
Hence, for part (a), we obtain
Varሺ𝑋௜ሻൌ𝑖െ1
𝑛െ1 ቆ1 െ𝑖െ1
𝑛െ1ቇൌሺ𝑖െ1ሻሺ𝑛െ𝑖ሻ
ሺ𝑛െ1ሻଶ
𝑖൏𝑗,
𝐸ሾ𝑋௜𝑋௝ሿ
ൌ
𝑖െ1
𝑖
⋯𝑗െ2
𝑗െ1
𝑗െ2
𝑗
𝑗െ1
𝑗൅1⋯𝑛െ3
𝑛െ1
ൌ
ሺ𝑖െ1ሻሺ𝑗െ2ሻ
ሺ𝑛െ2ሻሺ𝑛െ1ሻ
Covሺ𝑋௜, 𝑋௝ሻൌ
ሺ𝑖െ1ሻሺ𝑗െ2ሻ
ሺ𝑛െ2ሻሺ𝑛െ1ሻെ𝑖െ1
𝑛െ1
𝑗െ1
𝑛െ1
ൌ
ሺ𝑖െ1ሻሺ𝑗െ𝑛ሻ
ሺ𝑛െ2ሻሺ𝑛െ1ሻଶ
Varቌ෍
௜ൌଵ
௡
𝑋௜ቍൌ෍
௜ൌଵ
௡
Varሺ𝑋௜ሻ൅2 ෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
Covሺ𝑋௜, 𝑋௝ሻ
ൌ෍
௜ൌଵ
௡
ሺ𝑖െ1ሻሺ𝑛െ𝑖ሻ
ሺ𝑛െ1ሻଶ
൅2 ෍
௜ൌଵ
௡െଵ
෍
௝ൌ௜൅ଵ
௡
ሺ𝑖െ1ሻሺ𝑗െ𝑛ሻ
ሺ𝑛െ2ሻሺ𝑛െ1ሻଶ
ൌ
1
ሺ𝑛െ1ሻଶ෍
௜ൌଵ
௡
ሺ𝑖െ1ሻሺ𝑛െ𝑖ሻ
  െ
1
ሺ𝑛െ2ሻሺ𝑛െ1ሻଶ
෍
௜ൌଵ
௡െଵ
ሺ𝑖െ1ሻሺ𝑛െ𝑖ሻሺ𝑛െ𝑖െ1ሻ
𝑋௜
𝑖
𝐸ሾ𝑋௜ሿൌ
ቆ2
1ቇቆ3
1ቇቆ4
1ቇ
ቆ9
3ቇ
ൌ2
7
𝐸቎෍
௜ൌଵ
ଷ
𝑋௜቏ൌ6/7
790 of 848

It follows from the preceding that
Also, for 
Hence, for part (b), we obtain
7.14.
 Let 
 equal 1 if the th card is an ace and let 
be 0 otherwise. Let 
 equal 1 if the th card is a spade and let 
otherwise. Now,
However, 
 is clearly independent of 
 because knowing the suit of a
particular card gives no information about whether it is an ace and thus
cannot affect the probability that another specified card is an ace. More
formally, let 
 be the events, respectively, that card  is a
spade, a heart, a diamond, and a club. Then
But, by symmetry, we have
Varሺ𝑋௜ሻൌሺ2/7ሻሺ1 െ2/7ሻൌ10/49
𝑖്𝑗,
𝐸ሾ𝑋௜𝑋௝ሿ
ൌ
𝑃൛𝑋௜ൌ1, 𝑋௝ൌ1ൟ
ൌ
𝑃ሼ𝑋௜ൌ1ሽ𝑃൛𝑋௝ൌ1 ห𝑋௜ൌ1ൟ
ൌ
ቆ2
1ቇቆ3
1ቇቆ4
1ቇ
ቆ9
3ቇ
ቆ1
1ቇቆ2
1ቇቆ3
1ቇ
ቆ6
3ቇ
ൌ
6/70
Varቌ෍
௜ൌଵ
ଷ
𝑋௜ቍൌ
෍
௜ൌଵ
ଷ
Varሺ𝑋௜ሻ൅2෍෍
௝வଵ
Covሺ𝑋௜, 𝑋௝ሻ
ൌ30/49 ൅2ቆ3
2ቇቆ6
70 െ4
49ቇ
ൌ
312
490
𝑋௜, 𝑖ൌ1, . . . ,13,
𝑖
𝑋௜
𝑌௝
𝑗
𝑌௝ൌ0
Covሺ𝑋, 𝑌ሻ
ൌCovቌ෍
௜ൌଵ
ଵଷ
𝑋௜, ෍
௝ൌଵ
ଵଷ
𝑌௝ቍ
ൌ
෍
௜ൌଵ
ଵଷ
෍
௝ൌଵ
ଵଷ
Covሺ𝑋௜, 𝑌௝ሻ
𝑋௜
𝑌௝
𝐴௜,௦, 𝐴௜,௛, 𝐴௜,ௗ, 𝐴௜,௖
𝑖
𝑃൛𝑌௝ൌ1ൟ
ൌ
1
4 ൫𝑃൛𝑌௝ൌ1ห𝐴௜,௦ൟ൅𝑃൛𝑌௝ൌ1ห𝐴௜,௛ൟ
൅𝑃൛𝑌௝ൌ1ห𝐴௜,ௗൟ൅𝑃൛𝑌௝ൌ1ห𝐴௜,௖ൟ൯
791 of 848

Therefore,
As the preceding implies that
we see that 
 and 
 are independent. Hence, 
 and
thus 
The random variables  and 
 although uncorrelated, are not
independent. This follows, for instance, from the fact that
7.15.
a. Your expected gain without any information is 0.
b. You should predict heads if 
 and tails otherwise.
c. Conditioning on 
 the value of the coin, gives
7.16.
 Given that the name chosen appears in 
 different
positions on the list, since each of these positions is equally likely to be
the one chosen, it follows that
Hence,
Thus, 
7.17.
 Letting 
 equal 1 if a collision occurs when the th item is
placed, and letting it equal 0 otherwise, we can express the total
number of collisions  as
𝑃൛𝑌௝
ൌ
1ห𝐴௜,௦ൟൌ𝑃൛𝑌௝ൌ1ห𝐴௜,௛ൟൌ𝑃൛𝑌௝ൌ1ห𝐴௜,ௗൟ
ൌ
𝑃൛𝑌௝ൌ1ห𝐴௜,௖ൟ
𝑃൛𝑌௝ൌ1ൟൌ𝑃൛𝑌௝ൌ1ห𝐴௜,௦ൟ
𝑃൛𝑌௝ൌ1ൟൌ𝑃൛𝑌௝ൌ1ห𝐴௜,௦
௖ൟ
𝑌௝
𝑋௜
Covሺ𝑋௜,  𝑌௝ሻൌ0,
Covሺ𝑋,  𝑌ሻൌ0.
𝑋
𝑌,
𝑃ሼ𝑌ൌ13||𝑋ൌ4ሽൌ0 ്𝑃ሼ𝑌ൌ13ሽ
𝑝൐1/2
𝑉,
𝐸ሾGainሿൌ
඲
଴
ଵ
𝐸ሾGain |𝑉ൌ𝑝ሿ𝑑𝑝
ൌ
඲
଴
ଵ/ଶ
ሾ1ሺ1 െ𝑝ሻെ1ሺ𝑝ሻሿ 𝑑𝑝൅඲
ଵ/ଶ
ଵ
ሾ1ሺ𝑝ሻെ1ሺ1 െ𝑝ሻሿ 𝑑𝑝
ൌ
1/2
𝑛ሺ𝑋ሻ
𝐸ሾ𝐼||𝑛ሺ𝑋ሻሿൌ𝑃ሼ𝐼ൌ1||𝑛ሺ𝑋ሻሽൌ1/𝑛ሺ𝑋ሻ
𝐸ሾ𝐼ሿൌ𝐸ሾ1/𝑛ሺ𝑋ሻሿ
𝐸ሾ𝑚𝐼ሿൌ𝐸ሾ𝑚/𝑛ሺ𝑋ሻሿൌ𝑑.
𝑋௜
𝑖
𝑋
792 of 848

Therefore,
To determine 
 condition on the cell in which it is placed.
The next to last equality used the fact that, conditional on item  being
placed in cell  item  will cause a collision if any of the preceding 
items were put in cell  Thus,
Interchanging the order of the summations gives
Looking at the result shows that we could have derived it more easily
by taking expectations of both sides of the identity
The expected number of nonempty cells is then found by defining an
indicator variable for each cell, equal to 1 if that cell is nonempty and to
0 otherwise, and then taking the expectation of the sum of these
indicator variables.
7.18.
 Let  denote the length of the initial run. Conditioning on the
first value gives
𝑋ൌ෍
௜ൌଵ
௠
𝑋௜
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௠
𝐸ሾ𝑋௜ሿ
𝐸ሾ𝑋௜ሿ,
𝐸ሾ𝑋௜ሿ
ൌ
෍
௝
𝐸ሾ𝑋௜| placed in cell 𝑗ሿ𝑝௝
ൌ
෍
௝
𝑃ሼ𝑖 causes collision |placed in cell 𝑗ሽ𝑝௝
ൌ
෍
௝
ሾ1 െሺ1 െ𝑝௝ሻ௜െଵሿ𝑝௝
ൌ
1 െ෍
௝
ሺ1 െ𝑝௝ሻ௜െଵ𝑝௝
𝑖
𝑗,
𝑖
𝑖െ1
𝑗.
𝐸ሾ𝑋ሿൌ𝑚െ෍
௜ൌଵ
௠
෍
௝ൌଵ
௡
ሺ1 െ𝑝௝ሻ௜െଵ𝑝௝
𝐸ሾ𝑋ሿൌ𝑚െ𝑛൅෍
௝ൌଵ
௡
ሺ1 െ𝑝௝ሻ௠
number of nonempty cells ൌ𝑚െ𝑋
𝐿
793 of 848

Now, if the first value is one, then the length of the run will be the
position of the first zero when considering the remaining 
values, of which 
 are ones and 
 are zeroes. (For instance, if the
initial value of the remaining 
 is zero, then 
) As a similar
result is true given that the first value is a zero, we obtain from the
preceding, upon using the result from Example 3e
, that
7.19.
 Let  be the number of flips needed for both boxes to become
empty, and let  denote the number of heads in the first 
 flips.
Then
Now, if the number of heads in the first 
 flips is 
 then the
number of additional flips is the number of flips needed to obtain an
additional 
 heads. Similarly, if the number of heads in the first
 flips is 
 then, because there would have been a total of
 tails, the number of additional flips is the number needed
to obtain an additional 
 heads. Since the number of flips needed
for  outcomes of a particular type is a negative binomial random
variable whose mean is  divided by the probability of that outcome, we
obtain
7.20.
 Taking expectations of both sides of the identity given in the
𝐸ሾ𝐿ሿൌ
𝐸ሾ𝐿 |ϐirst value is oneሿ
𝑛
𝑛൅𝑚
൅𝐸ሾ𝐿 |ϐirst value is zeroሿ
𝑚
𝑛൅𝑚
𝑛൅𝑚െ1
𝑛െ1
𝑚
𝑛൅𝑚െ1
𝐿ൌ1.
𝐸ሾ𝐿ሿ
ൌ
𝑛൅𝑚
𝑚൅1
𝑛
𝑛൅𝑚൅𝑛൅𝑚
𝑛൅1
𝑚
𝑛൅𝑚
ൌ
𝑛
𝑚൅1 ൅
𝑚
𝑛൅1
𝑋
𝑌
𝑛൅𝑚
𝐸ሾ𝑋ሿ
ൌ
෍
௜ൌ଴
௡൅௠
𝐸ሾ𝑋|𝑌ൌ𝑖ሿ𝑃ሼ𝑌ൌ𝑖ሽ
ൌ
෍
௜ൌ଴
௡൅௠
𝐸ሾ𝑋|𝑌ൌ𝑖ሿቆ𝑛൅𝑚
𝑖
ቇ 𝑝௜ሺ1 െ𝑝ሻ௡൅௠െ௜
𝑛൅𝑚
𝑖, 𝑖൑𝑛,
𝑛െ𝑖
𝑛൅𝑚
𝑖, 𝑖൐𝑛,
𝑛൅𝑚െ𝑖൏𝑚
𝑖െ𝑛
𝑗
𝑗
𝐸ሾ𝑋ሿ
ൌ
෍
௜ൌ଴
௡
𝑛െ𝑖
𝑝
ቆ𝑛൅𝑚
𝑖
ቇ𝑝௜ሺ1 െ𝑝ሻ௡൅௠െ௜
൅
෍
௜ൌ௡൅ଵ
௡൅௠
𝑖െ𝑛
1 െ𝑝ቆ𝑛൅𝑚
𝑖
ቇ𝑝௜ሺ1 െ𝑝ሻ௡൅௠െ௜
794 of 848

hint yields
Taking the expectation inside the integral sign is justified because all
the random variables 
 are nonnegative.
7.21.
 Consider a random permutation 
 that is equally likely
to be any of the 
 permutations. Then
where the final equality followed from the assumption that 
Since the preceding shows that
it follows that there must be some permutation 
 for which
𝐸ሾ𝑋௡ሿ
ൌ
𝐸቎𝑛඲
଴
ஶ
𝑥௡െଵ𝐼௑ሺ𝑥ሻ 𝑑𝑥቏
ൌ
𝑛඲
଴
ஶ
𝐸ሾ𝑥௡െଵ𝐼௑ሺ𝑥ሻሿ 𝑑𝑥
ൌ
𝑛඲
଴
ஶ
𝑥௡െଵ𝐸ሾ𝐼௑ሺ𝑥ሻሿ 𝑑𝑥
ൌ
𝑛඲
଴
ஶ
𝑥௡െଵ𝐹̅ ̅̅ ̅ሺ𝑥ሻ 𝑑𝑥
𝐼௑ሺ𝑥ሻ,  0 ൏𝑥൏∞,
𝐼ଵ, . . . , 𝐼௡
𝑛!
𝐸ሾ𝑎ூೕ𝑎ூೕ൅భሿ
ൌ
෍
௞
𝐸ቂ𝑎ூೕ𝑎ூೕ൅భቚ𝐼௝ൌ𝑘ቃ൛𝐼௝ൌ𝑘ൟ
ൌ
1
𝑛෍
௞
𝑎௞𝐸ቂ𝑎ூೕ൅భቚ𝐼௝ൌ𝑘ቃ
ൌ
1
𝑛෍
௞
𝑎௞෍
௜
𝑎௜𝑃൛𝐼௝൅ଵൌ𝑖ห𝐼௝ൌ𝑘ൟ
ൌ
1
𝑛ሺ𝑛െ1ሻ෍
௞
𝑎௞෍
௜ஷ௞
𝑎௜
ൌ
1
𝑛ሺ𝑛െ1ሻ෍
௞
𝑎௞ሺെ𝑎௞ሻ
൏
0
෍
௜ൌଵ
௡
𝑎௜ൌ0.
𝐸቎෍
௝ൌଵ
௡െଵ
𝑎ூೕ𝑎ூೕ൅భ቏൏0
𝑖ଵ, . . . , 𝑖௡
෍
௝ൌଵ
௡െଵ
𝑎௜ೕ𝑎௜ೕ൅భ൏0
795 of 848

7.22.
a. 
b. 
c. Conditioning on 
 gives
7.23.
where the next to last equality used the fact that 
7.24.
 Let 
 equal  if the th card chosen is an ace, and let it equal
 otherwise. Because
𝐸ሾ𝑋ሿൌ𝜆ଵ൅𝜆ଶ, 𝐸ሾ𝑌ሿൌ𝜆ଶ൅𝜆ଷ
Covሺ𝑋, 𝑌ሻ
ൌCovሺ𝑋ଵ൅𝑋ଶ,  𝑋ଶ൅𝑋ଷሻ
ൌCovሺ𝑋ଵ, 𝑋ଶ൅𝑋ଷሻ൅Covሺ𝑋ଶ,  𝑋ଶ൅𝑋ଷሻ
ൌCovሺ𝑋ଶ,  𝑋ଶሻ
ൌVarሺ𝑋ଶሻ
ൌλଶ
𝑋ଶ
 𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗ሽ
ൌ෍
௞
𝑃ሼ𝑋ൌ𝑖, 𝑌ൌ𝑗 |𝑋ଶൌ𝑘ሽ𝑃ሼ𝑋ଶൌ𝑘ሽ
ൌ෍
௞
𝑃ሼ𝑋ଵൌ𝑖െ𝑘, 𝑋ଷൌ𝑗െ𝑘|𝑋ଶൌ𝑘ሽ𝑒െఒమ𝜆ଶ
௞/𝑘!
ൌ෍
௞
𝑃ሼ𝑋ଵൌ𝑖െ𝑘, 𝑋ଷൌ𝑗െ𝑘ሽ𝑒െఒమ𝜆ଶ
௞/𝑘!
ൌ෍
௞
𝑃ሼ𝑋ଵൌ𝑖െ𝑘ሽ𝑃ሼ𝑋ଷൌ𝑗െ𝑘ሽ𝑒െఒమ𝜆ଶ
௞/𝑘!
ൌ
෍
௞ൌ଴
୫୧୬ሺ௜. ௝ሻ
𝑒െఒభ
𝜆ଵ
௜െ௞
ሺ𝑖െ𝑘ሻ! 𝑒െఒయ
𝜆ଷ
௝െ௞
ሺ𝑗െ𝑘ሻ! 𝑒െఒమ𝜆ଶ
௞
𝑘!
Corrቌ෍
௜
𝑋௜, ෍
௝
𝑌௝ቍൌ
Covሺ෍
௜
𝑋௜, ෍
௝
𝑌௝ሻ
Varሺ෍
௜
𝑋௜ሻVarሺ෍
௝
𝑌௝ሻ
ඨ
ൌ
෍
௜
෍
௝
Covሺ𝑋௜, 𝑌௝ሻ
𝑛𝜎௫
ଶ𝑛𝜎௬
ଶ
ට
ൌ
෍
௜
Covሺ𝑋௜, 𝑌௜ሻ൅෍
௜
෍
௝ஷ௜
Covሺ𝑋௜, 𝑌௝ሻ
𝑛𝜎௫𝜎௬
ൌ𝑛𝜌𝜎௫𝜎௬
𝑛𝜎௫𝜎௬
ൌ𝜌
 Covሺ𝑋௜, 𝑌௜ሻൌ𝜌𝜎௫𝜎௬
𝑋௜
1
𝑖
0
796 of 848

and 
 it follows that 
 But, with 
being the event that the ace of spades is chosen, we have
Using that 
 gives the result
Similarly, letting  be the event that at least one ace is chosen, we have
Thus,
Another way to solve this problem is to number the four aces, with the
ace of spades having number  and then let 
 equal  if ace number 
is chosen and  otherwise. Then
𝑋ൌ෍
௜ൌଵ
ଷ
𝑋௜
𝐸ሾ𝑋௜ሿൌ𝑃ሼ𝑋௜ൌ1ሽൌ1/13,
𝐸ሾ𝑋ሿൌ3/13.
𝐴
𝐸ሾ𝑋ሿ
ൌ
𝐸ሾ𝑋|𝐴ሿ𝑃ሺ𝐴ሻ൅𝐸ሾ𝑋|𝐴௖ሿ𝑃ሺ𝐴௖ሻ
ൌ
𝐸ሾ𝑋|𝐴ሿ3
52 ൅𝐸ሾ𝑋|𝐴௖ሿ49
52
ൌ
𝐸ሾ𝑋|𝐴ሿ3
52 ൅49
52 𝐸቎෍
௜ൌଵ
ଷ
𝑋௜|𝐴௖቏
ൌ
𝐸ሾ𝑋|𝐴ሿ3
52 ൅49
52 ෍
௜ൌଵ
ଷ
𝐸ሾ𝑋௜|𝐴௖ሿ
ൌ
𝐸ሾ𝑋|𝐴ሿ3
52 ൅49
52 3 3
51
𝐸ሾ𝑋ሿൌ3/13
𝐸ሾ𝑋|𝐴ሿൌ52
3 ቆ3
13 െ49
52
3
17ቇൌ19
17 ൌ1.1176
𝐿
𝐸ሾ𝑋ሿ
ൌ
𝐸ሾ𝑋|𝐿ሿ𝑃ሺ𝐿ሻ൅𝐸ሾ𝑋|𝐿௖ሿ𝑃ሺ𝐿௖ሻ
ൌ
𝐸ሾ𝑋|𝐿ሿ𝑃ሺ𝐿ሻ
ൌ
𝐸ሾ𝑋|𝐿ሿቆ1 െ48 ⋅47 ⋅46
52 ⋅51 ⋅50ቇ
𝐸ሾ𝑋|𝐿ሿൌ
3/13
1 െ48 ⋅47 ⋅46
52 ⋅51 ⋅50
ൎ1.0616
1,
𝑌௜
1
𝑖
0
797 of 848

where we used that the fact given that the ace of spades is chosen the
other two cards are equally likely to be any pair of the remaining 
cards; so the conditional probability that any specified card (not equal
to the ace of spades) is chosen is 
 Also,
Because
we obtain the same answer as before.
7.25.
a. 
b. It follows from part (a) that 
 Therefore,
The result now follows because 
c. Since 
 is normal with mean  and variance  we have
7.26.
 Let  be the number of heads in the first 
 flips. Let
 be the number of flips needed to amass at least  heads
and at least 
 tails. Conditioning on  gives
𝐸ሾ𝑋|𝐴ሿ
ൌ
𝐸቎෍
௜ൌଵ
ସ
𝑌௜|𝑌ଵൌ1቏
ൌ
1 ൅෍
௜ൌଶ
ସ
𝐸ሾ𝑌௜|𝑌ଵሿ
ൌ
1 ൅3 ⋅2
51 ൌ19/17
51
2/51.
𝐸ሾ𝑋|𝐿ሿൌ𝐸቎෍
௜ൌଵ
ସ
𝑌௜|𝐿቏ൌ෍
௜ൌଵ
ସ
𝐸ሾ𝑌௜|𝐿ሿൌ4𝑃ሼ𝑌ଵൌ1|𝐿ሽ
𝑃ሼ𝑌ଵൌ1|𝐿ሽൌ𝑃ሺ𝐴|𝐿ሻൌ𝑃ሺ𝐴𝐿ሻ
𝑃ሺ𝐿ሻൌ𝑃ሺ𝐴ሻ
𝑃ሺ𝐿ሻൌ
3/52
1 െ48 ⋅47 ⋅46
52 ⋅51 ⋅50
𝐸ሾ𝐼||𝑋ൌ𝑥ሿൌ𝑃ሼ𝑍൏𝑋||𝑋ൌ𝑥ሽൌ𝑃ሼ𝑍൏𝑥||𝑋ൌ𝑥ሽൌ𝑃ሼ𝑍൏𝑥ሽൌΦ ሺ𝑥ሻ
𝐸ሾ𝐼||𝑋ሿൌΦ ሺ𝑋ሻ.
𝐸ሾ𝐼ሿൌ𝐸ሾ𝐸ሾ𝐼||𝑋ሿሿൌ𝐸ሾΦ ሺ𝑋ሻሿ
𝐸ሾ𝐼ሿൌ𝑃ሼ𝐼ൌ1ሽൌ𝑃ሼ𝑍൏𝑋ሽ.
𝑋െ𝑍
𝜇
2,
𝑃ሼ𝑋൐𝑍ሽൌ
𝑃ሼ𝑋െ𝑍൐0ሽ
ൌ
𝑃ቊ𝑋െ𝑍െ𝜇
2
√
൐െ𝜇
2
√ቋ
ൌ
1 െΦ ൬െ𝜇
2
√൰
ൌ
Φ ൬𝜇
2
√൰
𝑁
𝑛൅𝑚െ1
𝑀ൌmaxሺ𝑋, 𝑌ሻ
𝑛
𝑚
𝑁
798 of 848

Now, suppose we are given that there are a total of  heads in the first
 trials. If 
 then we have already obtained at least 
 tails,
so the additional number of flips needed is equal to the number needed
for an additional 
 heads; similarly, if 
 then we have already
obtained at least  heads, so the additional number of flips needed is
equal to the number needed for an additional 
 tails.
Consequently, we have
The expected number of flips to obtain either  heads or 
 tails,
 is now given by
7.27.
 This is just the expected time to collect 
 of the  types of
coupons in Example 2i
. By the results of that example the solution
is
7.28.
 With 
𝐸ሾ𝑀ሿൌ෍
௜
𝐸ሾ𝑀|𝑁ൌ𝑖ሿ𝑃ሼ𝑁ൌ𝑖ሽ
ൌ
෍
௜ൌ଴
௡െଵ
𝐸ሾ𝑀|𝑁ൌ𝑖ሿ𝑃ሼ𝑁ൌ𝑖ሽ൅
෍
௜ൌ௡
௡൅௠െଵ
𝐸ሾ𝑀|𝑁ൌ𝑖ሿ𝑃ሼ𝑁ൌ𝑖ሽ
𝑖
𝑛൅𝑚െ1
𝑖൏𝑛,
𝑚
𝑛െ𝑖
𝑖൒𝑛,
𝑛
𝑚െሺ𝑛൅𝑚െ1 െ𝑖ሻ
𝐸ሾ𝑀ሿൌ
෍
௜ൌ଴
௡െଵ
ቆ𝑛൅𝑚െ1 ൅𝑛െ𝑖
𝑝
ቇ𝑃ሼ𝑁ൌ𝑖ሽ
  ൅
෍
௜ൌ௡
௡൅௠െଵ
ቆ𝑛൅𝑚െ1 ൅𝑖൅1 െ𝑛
1 െ𝑝
ቇ𝑃ሼ𝑁ൌ𝑖ሽ
ൌ𝑛൅𝑚െ1 ൅
෍
௜ൌ଴
௡െଵ𝑛െ𝑖
𝑝
ቆ𝑛൅𝑚െ1
𝑖
ቇ 𝑝௜ሺ1 െ𝑝ሻ௡൅௠െଵെ௜
  ൅
෍
௜ൌ௡
௡൅௠െଵ𝑖൅1 െ𝑛
1 െ𝑝
ቆ𝑛൅𝑚െ1
𝑖
ቇ 𝑝௜ሺ1 െ𝑝ሻ௡൅௠െଵെ௜
𝑛
𝑚
𝐸ሾminሺ𝑋, 𝑌ሻሿ,
𝐸ሾminሺ𝑋,𝑌ሻሿൌ𝐸ሾ𝑋൅𝑌െ𝑀ሿൌ𝑛
𝑝൅
𝑚
1 െ𝑝െ𝐸ሾ𝑀ሿ
𝑛െ1
𝑛
1 ൅
𝑛
𝑛െ1 ൅
𝑛
𝑛െ2 ൅. . . ൅𝑛
2
𝑞ൌ1 െ𝑝,
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
ஶ
𝑃ሼ𝑋൒𝑖ሽൌ෍
௜ൌଵ
௡
𝑃ሼ𝑋൒𝑖ሽൌ෍
௜ൌଵ
௡
𝑞௜െଵൌ1 െ𝑞௡
𝑝
799 of 848

7.29.
Hence,
Because
the preceding shows that all of the following are equivalent when  and
 are Bernoulli:
A. 
B. 
C. 
D. 
E. 
7.30.
 Number the individuals, and let 
 equal  if the th individual
who has hat size  chooses a hat of that size, and let 
 equal 
otherwise. Then the number of individuals who choose a hat of their
size is
Hence,
7.31.
 Letting 
 and 
 be, respectively, the variances of  and of 
we obtain, upon squaring both sides, the equivalent inequality
Using that 
 the preceding inequality
becomes
which has already been established.
7.32.
 Noting that  is equal to  plus the number of the values
 that are smaller than 
 it follows that if we let 
equal 1 if 
 and let it equal  otherwise, that
Covሺ𝑋, 𝑌ሻ
ൌ𝐸ሾ𝑋𝑌ሿെ𝐸ሾ𝑋ሿ𝐸ሾ𝑌ሿ
ൌ𝑃ሺ𝑋ൌ1, 𝑌ൌ1ሻെ𝑃ሺ𝑋ൌ1ሻ𝑃ሺ𝑌ൌ1ሻ
Covሺ𝑋,𝑌ሻൌ0  ⇔ 𝑃ሺ𝑋ൌ1, 𝑌ൌ1ሻൌ𝑃ሺ𝑋ൌ1ሻ𝑃ሺ𝑌ൌ1ሻ
Covሺ𝑋, 𝑌ሻൌ
Covሺ1 െ𝑋, 1 െ𝑌ሻൌെCovሺ1 െ𝑋, 𝑌ሻ
ൌ
െCovሺ𝑋, 1 െ𝑌ሻ
𝑋
𝑌
Covሺ𝑋, 𝑌ሻൌ0
𝑃ሺ𝑋ൌ1, 𝑌ൌ1ሻൌ𝑃ሺ𝑋ൌ1ሻ𝑃ሺ𝑌ൌ1ሻ
𝑃ሺ1 െ𝑋ൌ1, 1 െ𝑌ൌ1ሻൌ𝑃ሺ1 െ𝑋ൌ1ሻ𝑃ሺ1 െ𝑌ൌ1ሻ
𝑃ሺ1 െ𝑋ൌ1, 𝑌ൌ1ሻൌ𝑃ሺ1 െ𝑋ൌ1ሻ𝑃ሺ𝑌ൌ1ሻ
𝑃ሺ𝑋ൌ1, 1 െ𝑌ൌ1ሻൌ𝑃ሺ𝑋ൌ1ሻ𝑃ሺ1 െ𝑌ൌ1ሻ
𝑋௜,௝
1
𝑗
𝑖
𝑋௜,௝
0
𝑋ൌ෍
௜ൌଵ
௥
෍
௝ൌଵ
௡೔
𝑋௜,௝
𝐸ሾ𝑋ሿൌ෍
௜ൌଵ
௥
෍
௝ൌଵ
௡೔
𝐸ሾ𝑋௜,௝ሿൌ෍
௜ൌଵ
௥
෍
௝ൌଵ
௡೔
ℎ௜
𝑛ൌ1
𝑛෍
௜ൌଵ
௥
ℎ௜𝑛௜
𝜎௫
ଶ
𝜎௬
ଶ
𝑋
𝑌,
Varሺ𝑋൅𝑌ሻ൑𝜎௫
ଶ൅𝜎௬
ଶ൅2𝜎௫𝜎௬
Varሺ𝑋൅𝑌ሻൌ𝜎௫
ଶ൅𝜎௬
ଶ൅2Covሺ𝑋, 𝑌ሻ,
Corrሺ𝑋, 𝑌ሻൌCovሺ𝑋, 𝑌ሻ
𝜎௫𝜎௬
൑1
𝑋
𝑖
𝑅௡൅ଵ, . . . ,𝑅௡൅௠
𝑋,
𝐼௡൅௞
𝑅௡൅௞൏𝑋
0
800 of 848

Taking expectations gives that
Now,
where the final equality used that 
 is equally likely to be either the
smallest, the second smallest, 
 or the 
 smallest of the
values 
 Hence,
7.33.
a. 
b. 
 which gives that
c. 
 giving that
d. 
e. Differentiate part (d) to obtain the density 
𝑋ൌ𝑖൅
෍
௞ൌଵ
௠
𝐼௡൅௞
𝐸ሾ𝑋ሿൌ𝑖൅
෍
௞ൌଵ
௠
𝐸ሾ𝐼௡൅௞ሿ
𝐸ሾ𝐼௡൅௞ሿ
ൌ
𝑃ሺ𝑅௡൅௞൏𝑋ሻ
ൌ
𝑃ሺ𝑅௡൅௞൏𝑖௧௛ smallest of 𝑅ଵ, .   .   . , 𝑅௡ሻ
ൌ
𝑃ሺ𝑅௡൅௞ is one of the 𝑖 smallest of the values
𝑅ଵ, .   .   . , 𝑅௡,  𝑅௡൅௞ሻ
ൌ
𝑖
𝑛൅1
𝑅௡൅௞
. . . ,
ሺ𝑛൅1ሻ௦௧
𝑅ଵ, . . . ,𝑅௡, 𝑅௡൅௞.
𝐸ሾ𝑋ሿൌ𝑖൅𝑚
𝑖
𝑛൅1
𝐸ሾ𝑋ሿൌ׬଴
ଵ𝐸ሾ𝑋|𝑌ൌ𝑦ሿ 𝑑𝑦ൌ׬଴
ଵ𝑦
2  𝑑𝑦ൌ1/4
𝐸ሾ𝑋𝑌ሿൌ׬଴
ଵ𝐸ሾ𝑋𝑌|𝑌ൌ𝑦ሿ 𝑑𝑦ൌ׬଴
ଵ𝑦ଶ
2 𝑑𝑦ൌ1/6,
Covሺ𝑋, 𝑌ሻൌ1/6 െ1/8 ൌ1/24
𝐸ሾ𝑋ଶሿൌ׬଴
ଵ𝐸ൣ𝑋ଶห𝑌ൌ𝑦൧ 𝑑𝑦ൌ׬଴
ଵ𝑦ଶ
3 𝑑𝑦ൌ1/9,
Varሺ𝑋ሻൌ1
9 െ1
16 ൌ
7
144
𝑃ሺ𝑋൑𝑥ሻൌ
඲
଴
ଵ
𝑃ሺ𝑋൑𝑥|𝑌ൌ𝑦ሻ 𝑑𝑦
ൌ
඲
଴
௫
𝑃ሺ𝑋൑𝑥|𝑌ൌ𝑦ሻ 𝑑𝑦൅඲
௫
ଵ
𝑃ሺ𝑋൑𝑥|𝑌ൌ𝑦ሻ 𝑑𝑦
ൌ
඲
଴
௫
𝑑𝑦൅඲
௫
ଵ
𝑥
𝑦 𝑑𝑦
ൌ
𝑥െ𝑥logሺ𝑥ሻ
𝑓ሺ𝑥ሻൌെlogሺ𝑥ሻ,
801 of 848

0 ൏𝑥൏1.
8.1.
 Let  denote the number of sales made next week, and note that  is
integral. From Markov’s inequality, we obtain the following:
a. 
b. 
8.2.
a. 
b. 
In part (a), we used Chebyshev’s inequality; in part (b), we used its
one-sided version. (See Proposition 5.1.
)
8.3.
 First note that 
 and
Using Chebyshev’s inequality in part (a) and the one-sided version in parts (b)
and (c) gives the following results:
a. 
b. 
c. 
8.4.
 If  is the number produced at factory  and  the number produced at
factory 
 then
8.5.
 Note first that
𝑋
𝑋
𝑃ሼ𝑋൐18ሽൌ𝑃ሼ𝑋൒19ሽ൑𝐸ሾ𝑋ሿ
19 ൌ16/19
𝑃ሼ𝑋൐25ሽൌ𝑃ሼ𝑋൒26ሽ൑𝐸ሾ𝑋ሿ
26 ൌ16/26
𝑃ሼ10 ൑𝑋൑22ሽ
ൌ
𝑃ሼ|𝑋െ16| ൑6ሽ
ൌ
𝑃ሼ|𝑋െ𝜇| ൑6ሽ
ൌ
1 െ𝑃ሼ|𝑋െ𝜇| ൐6ሽ
൒
1 െ9/36 ൌ3/4
𝑃ሼ𝑋൒19ሽൌ𝑃ሼ𝑋െ16 ൒3ሽ൑
9
9 ൅9 ൌ1/2
𝐸ሾ𝑋െ𝑌ሿൌ0
Varሺ𝑋െ𝑌ሻൌVarሺ𝑋ሻ൅Varሺ𝑌ሻെ2Covሺ𝑋, 𝑌ሻൌ28
𝑃ሼ||𝑋െ𝑌|| ൐15ሽ൑28/225
𝑃ሼ𝑋െ𝑌൐15ሽ൑
28
28 ൅225 ൌ28/253
𝑃ሼ𝑌െ𝑋൐15ሽ൑
28
28 ൅225 ൌ28/253
𝑋
𝐴
𝑌
𝐵,
𝐸ሾ𝑌െ𝑋ሿ
ൌ
െ2,  Varሺ𝑌െ𝑋ሻൌ36 ൅9 ൌ45
𝑃ሼ𝑌െ𝑋൐0ሽ
ൌ
𝑃ሼ𝑌െ𝑋൒1ሽ
ൌ
𝑃ሼ𝑌െ𝑋൅2 ൒3ሽ൑
45
45 ൅9 ൌ45/54
𝐸ሾ𝑋௜ሿൌ඲
଴
ଵ
2𝑥ଶ𝑑𝑥ൌ2/3
802 of 848

Now use the strong law of large numbers to obtain
8.6.
 Because 
 and
we have 
 Thus, if there are  components on
hand, then
where  is a standard normal random variable. Since
we see that  should be chosen so that
A numerical computation gives the result 
8.7.
 If  is the time required to service a machine, then
Also, since the variance of an exponential random variable is equal to the
square of its mean, we have
Therefore, with 
 being the time required to service job 
 and 
being a standard normal random variable, it follows that
𝑟
ൌ
lim
௡→ஶ
𝑛
𝑆௡
ൌ
lim
௡→ஶ
1
𝑆௡/𝑛
ൌ
1
lim
௡→ஶ𝑆௡/𝑛
ൌ
1/ሺ2/3ሻൌ3/2
𝐸ሾ𝑋௜ሿൌ2/3
𝐸ሾ𝑋௜
ଶሿൌ඲
଴
ଵ
2𝑥ଷ𝑑𝑥ൌ1/2
Varሺ𝑋௜ሻൌ1/2 െሺ2/3ሻଶൌ1/18.
𝑛
𝑃ሼ𝑆௡൒35ሽ
ൌ
𝑃ሼ𝑆௡൒34.5ሽ  ሺthe continuity correctionሻ
ൌ
𝑃൝𝑆௡െ2𝑛/3
𝑛/18
ඥ
൑34.5 െ2𝑛/3
𝑛/18
ඥ
ൡ
ൎ
𝑃൝𝑍൒34.5 െ2𝑛/3
𝑛/18
ඥ
ൡ
𝑍
𝑃ሼ𝑍൐െ1.284ሽൌ𝑃ሼ𝑍൏1.284ሽൎ.90
𝑛
ሺ34.5 െ2𝑛/3ሻൎെ1.284 𝑛/18
ඥ
𝑛ൌ55.
𝑋
𝐸ሾ𝑋ሿൌ.2 ൅.3 ൌ.5
Varሺ𝑋ሻൌሺ.2ሻଶ൅ሺ.3ሻଶൌ.13
𝑋௜
𝑖, 𝑖ൌ1, . . . , 20,
𝑍
803 of 848

8.8.
 Note first that if  is the gambler’s winnings on a single bet, then
Therefore, with  having a standard normal distribution,
8.9.
 Using the notation of Problem 8.7
, we have
Now, 
 so  should be such that
which yields 
8.10.
 If the claim were true, then, by the central limit theorem, the average
nicotine content (call it ) would approximately have a normal distribution with
mean 2.2 and standard deviation .03. Thus, the probability that it would be as
high as 3.1 is
where  is a standard normal random variable.
8.11.
a. If we arbitrarily number the batteries and let 
 denote the life of battery
 then the 
 are independent and identically distributed
random variables. To compute the mean and variance of the life of, say,
battery  we condition on its type. Letting  equal  if battery  is type 
𝑃ሼ𝑋ଵ൅⋯൅𝑋ଶ଴൏8ሽ
ൌ
𝑃ቊ𝑋ଵ൅⋯൅𝑋ଶ଴െ10
2.6
√
൏8 െ10
2.6
√
ቋ
ൎ
𝑃ሼ𝑍൏െ1.24035ሽ
ൎ
.1074
𝑋
𝐸ሾ𝑋ሿൌെ.7 െ.4 ൅1 ൌെ.1, 𝐸ሾ𝑋ଶሿൌ.7 ൅.8 ൅10 ൌ11.5
  →Varሺ𝑋ሻൌ11.49
𝑍
𝑃ሼ𝑋ଵ൅⋯൅𝑋ଵ଴଴൑െ.5ሽ
ൌ
𝑃ቊ𝑋ଵ൅⋯൅𝑋ଵ଴଴൅10
1149
√
൑െ.5 ൅10
1149
√
ቋ
ൎ
𝑃ሼ𝑍൑.2803ሽ
ൎ
.6104
𝑃ሼ𝑋ଵ൅⋯൅𝑋ଶ଴൏𝑡ሽ
ൌ
𝑃ቊ𝑋ଵ൅⋯൅𝑋ଶ଴െ10
2.6
√
൏𝑡െ10
2.6
√
ቋ
ൎ
𝑃ቊ𝑍൏𝑡െ10
2.6
√
ቋ
𝑃ሼ𝑍൏1.645ሽൎ.95,
𝑡
𝑡െ10
2.6
√
ൎ1.645
𝑡ൎ12.65.
𝑋
𝑃ሼ𝑋൐3.1ሽ
ൌ
𝑃ቊ𝑋െ2.2
.03
൐3.1 െ2.2
.03
ቋ
ൎ
𝑃ሼ𝑍൐30ሽ
ൎ
0
𝑍
𝑋௜
𝑖, 𝑖ൌ1, . . . ,40,
𝑋௜
1,
𝐼
1
1
𝐴
804 of 848

and letting it equal  if it is type 
 we have
yielding
In addition, using the fact that 
 we have
yielding
Thus, 
 are independent and identically distributed random
variables having mean 
 and variance 
 Hence,
with 
 we have
and the central limit theorem yields
b. For this part, let 
 be the total life of all the type  batteries and let 
be the total life of all the type  batteries. Then, by the central limit
theorem, 
 has approximately a normal distribution with mean
 and variance 
 and 
 has
approximately a normal distribution with mean 
 and
variance 
 Because the sum of independent normal
random variables is also a normal random variable, it follows that
 is approximately normal with mean 
 and variance 
Consequently, with 
8.12.
 Let  denote the number of doctors who volunteer. Conditional on the
0
𝐵,
𝐸ሾ𝑋ଵ||𝐼ൌ1ሿൌ50,  𝐸ሾ𝑋ଵ||𝐼ൌ0ሿൌ30
𝐸ሾ𝑋ଵሿൌ50𝑃ሼ𝐼ൌ1ሽ൅30𝑃ሼ𝐼ൌ0ሽൌ50ሺ1/2ሻ൅30ሺ1/2ሻൌ40
𝐸ሾ𝑊ଶሿൌሺ𝐸ሾ𝑊ሿሻଶ൅Varሺ𝑊ሻ,
𝐸ሾ𝑋ଵ
ଶห𝐼ൌ1ሿ
ൌ
ሺ50ሻଶ൅ሺ15ሻଶൌ2725,
𝐸ሾ𝑋ଵ
ଶห𝐼ൌ0ሿ
ൌ
ሺ30ሻଶ൅6ଶൌ936
𝐸ሾ𝑋ଵ
ଶሿൌሺ2725ሻሺ1/2ሻ൅ሺ936ሻሺ1/2ሻൌ1830.5
𝑋ଵ, . . . ,𝑋ସ଴
40
1830.5 െ1600 ൌ230.5.
𝑆ൌ෍
௜ൌଵ
ସ଴
𝑋௜,
𝐸ሾ𝑆ሿൌ40ሺ40ሻൌ1600,  Varሺ𝑆ሻൌ40ሺ230.5ሻൌ9220
𝑃ሼ𝑆൐1700ሽ
ൌ
𝑃ቊ𝑆െ1600
9220
√
൐1700 െ1600
9220
√
ቋ
ൎ
𝑃ሼ𝑍൐1.041ሽ
ൌ
1 െΦ ሺ1.041ሻൌ.149
𝑆஺
𝐴
𝑆஻
𝐵
𝑆஺
20ሺ50ሻൌ1000
20ሺ225ሻൌ4500,
𝑆஻
20ሺ30ሻൌ600
20ሺ36ሻൌ720.
𝑆஺൅𝑆஻
1600
5220.
𝑆ൌ𝑆஺൅𝑆஻,
𝑃ሼ𝑆൐1700ሽ
ൌ
𝑃ቊ𝑆െ1600
5220
√
൐1700 െ1600
5220
√
ቋ
ൎ
𝑃ሼ𝑍൐1.384ሽ
ൌ
1 െΦ ሺ1.384ሻൌ.084
𝑁
805 of 848

event 
 the number of patients seen is distributed as the sum of 
independent Poisson random variables with common mean 30. Because the
sum of independent Poisson random variables is also a Poisson random
variable, it follows that the conditional distribution of  given that 
 is
Poisson with mean 30  Therefore,
As a result,
Also, by the conditional variance formula,
Because
we obtain 
To approximate 
 we would not be justified in assuming that the
distribution of  is approximately that of a normal random variable with mean
90 and variance 690. What we do know, however, is that
where 
 is the probability that a Poisson random variable with mean 30
is greater than 65. That is,
Because a Poisson random variable with mean 30  has the same distribution
as does the sum of 30  independent Poisson random variables with mean 1, it
follows from the central limit theorem that its distribution is approximately
normal with mean and variance equal to 30  Consequently, with 
 being a
Poisson random variable with mean 30  and  being a standard normal
random variable, we can approximate 
 as follows:
𝑁ൌ𝑖,
𝑖
𝑋
𝑁ൌ𝑖
𝑖.
𝐸ሾ𝑋||𝑁ሿൌ30𝑁  Varሺ𝑋||𝑁ሻൌ30𝑁
𝐸ሾ𝑋ሿൌ𝐸ሾ𝐸ሾ𝑋||𝑁ሿሿൌ30𝐸ሾ𝑁ሿൌ90
Varሺ𝑋ሻൌ
𝐸ሾVarሺ𝑋||𝑁ሻሿ൅Varሺ𝐸ሾ𝑋||𝑁ሿሻ
ൌ
30𝐸ሾ𝑁ሿ൅ሺ30ሻଶVarሺ𝑁ሻ
Varሺ𝑁ሻൌ1
3 ሺ2ଶ൅3ଶ൅4ଶሻെ9 ൌ2/3
Varሺ𝑋ሻൌ690.
𝑃ሼ𝑋൐65ሽ,
𝑋
𝑃ሼ𝑋൐65ሽൌ෍
௜ൌଶ
ସ
𝑃ሼ𝑋൐65|𝑁ൌ𝑖ሽ𝑃ሼ𝑁ൌ𝑖ሽൌ1
3 ෍
௜ൌଶ
ସ
𝑃̅ ̅௜ሺ65ሻ
𝑃̅ ̅௜ሺ65ሻ
𝑖
𝑃̅ ̅௜ሺ65ሻൌ1 െ෍
௝ൌ଴
଺ହ
𝑒െଷ଴௜ሺ30𝑖ሻ௝/𝑗!
𝑖
𝑖
𝑖.
𝑋௜
𝑖
𝑍
𝑃̅ ̅௜ሺ65ሻ
806 of 848

Therefore,
leading to the result
If we would have mistakenly assumed that  was approximately normal, we
would have obtained the approximate answer .8244. (The exact probability is
.7440.)
8.13.
 Take logarithms and then apply the strong law of large numbers to
obtain
Therefore,
8.14.
 Let 
 be the time it takes to process book  and let 
a. With  being a standard normal
𝑃̅ ̅௜ሺ65ሻൌ
𝑃ሼ𝑋൐65ሽ
ൌ
𝑃ሼ𝑋൒65.5ሽ
ൌ
𝑃ቊ𝑋െ30𝑖
30𝑖
√
൒65.5 െ30𝑖
30𝑖
√
ቋ
ൎ
𝑃ቊ𝑍൒65.5 െ30𝑖
30𝑖
√
ቋ
𝑃̅ ̅ଶሺ65ሻൎ
𝑃ሼ𝑍൒.7100ሽൎ.2389
𝑃̅ ̅ଷሺ65ሻൎ
𝑃ሼ𝑍൒െ2.583ሽൎ.9951
𝑃̅ ̅ସሺ65ሻൎ
𝑃ሼ𝑍൒െ4.975ሽൎ1
𝑃ሼ𝑋൐65ሽൎ.7447
𝑋
log൦ቌෑ
௜ൌଵ
௡
𝑋௜ቍ
ଵ/௡
൪ൌ1
𝑛෍
௜ൌଵ
௡
logሺ𝑋௜ሻ→𝐸ሾlogሺ𝑋௜ሻሿ
ቌෑ
௜ൌଵ
௡
𝑋௜ቍ
ଵ/௡
→𝑒ாሾ୪୭୥ሺ௑೔ሻሿ
𝑋௜
𝑖,
𝑆௡ൌ෍
௜ൌଵ
௡
𝑋௜.
𝑍
𝑃ሼ𝑆ସ଴൐420ሽ
ൌ
𝑃ቊ𝑆ସ଴െ400
40 ⋅9
√
൐420 െ400
40 ⋅9
√
ቋ
ൎ
𝑃ቊ𝑍൐
20
360
√
ቋൎ.146
807 of 848

b. 
We have assumed that the successive book processing times are
independent.
8.15.
 Let 
 Also, let 
 and 
Then  and  are both increasing functions and so
 which is equivalent to
𝑃ሼ𝑆ଶହ൑240ሽ
ൌ
𝑃ቊ𝑆ଶହെ250
25 ⋅9
√
൑240 െ250
25 ⋅9
√
ቋ
ൎ
𝑃ቊ𝑍൑െ10
15ቋൎ.2525
𝑃ሺ𝑋ൌ𝑖ሻൌ1/𝑛, 𝑖ൌ1, . . . , 𝑛.
𝑓ሺ𝑥ሻൌ𝑎௫
𝑔ሺ𝑥ሻൌ𝑏௫.
𝑓
𝑔
𝐸ሾ𝑓ሺ𝑋ሻ𝑔ሺ𝑋ሻሿ൒𝐸ሾ𝑓ሺ𝑋ሻሿ𝐸ሾ𝑔ሺ𝑋ሻሿ,
1
𝑛෍
௜ൌଵ
௡
𝑎௜𝑏௜൒ሺ1
𝑛෍
௜ൌଵ
௡
𝑎௜ሻሺ1
𝑛෍
௜ൌଵ
௡
𝑏௜ሻ
9.1.
 From axiom (iii), it follows that the number of events that
occur between times 8 and 10 has the same distribution as the
number of events that occur by time 2 and thus is a Poisson random
variable with mean 6. Hence, we obtain the following solutions for
parts (a) and (b):
a. 
b. 
c. It follows from axioms (ii) and (iii) that from any point in time
onward, the process of events occurring is a Poisson process
with rate  Hence, the expected time of the fifth event after 2
P.M. is 
 That is, the expected time of this
event is 3:40 P.M.
9.2.
a. 
𝑃ሼ𝑁ሺ10ሻെ𝑁ሺ8ሻൌ0ሽൌ𝑒െ଺
𝐸ሾ𝑁ሺ10ሻെ𝑁ሺ8ሻሿൌ6
𝜆.
2 ൅𝐸ሾ𝑆ହሿൌ2 ൅5/3.
808 of 848

b. 
9.3.
 Fix a point on the road and let 
 equal 0 if the th vehicle to
pass is a car and let it equal 1 if it is a truck, 
 We now suppose
that the sequence 
 is a Markov chain with transition
probabilities
Then the long-run proportion of times is the solution of
Solving the preceding equations gives
   𝑃ሼ𝑁ሺ1/3ሻൌ2||𝑁ሺ1ሻൌ2ሽ
ൌ𝑃ሼ𝑁ሺ1/3ሻൌ2, 𝑁ሺ1ሻൌ2ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
ൌ𝑃ሼ𝑁ሺ1/3ሻൌ2, 𝑁ሺ1ሻെ𝑁ሺ1/3ሻൌ0ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
ൌ𝑃ሼ𝑁ሺ1/3ሻൌ2ሽ𝑃ሼ𝑁ሺ1ሻെ𝑁ሺ1/3ሻൌ0ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
 ሺby axiom ሺ𝑖𝑖ሻሻ
ൌ𝑃ሼ𝑁ሺ1/3ሻൌ2ሽ𝑃ሼ𝑁ሺ2/3ሻൌ0ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
 ሺby axiom ሺ𝑖𝑖𝑖ሻሻ
ൌ𝑒െఒ/ଷሺ𝜆/3ሻଶ/2!𝑒െଶఒ/ଷ
𝑒െఒ𝜆ଶ/2!
ൌ1/9
     𝑃ሼ𝑁ሺ1/2ሻ൒1||𝑁ሺ1ሻൌ2ሽൌ1 െ𝑃ሼ𝑁ሺ1/2ሻൌ0||𝑁ሺ1ሻൌ2ሽ
ൌ1 െ𝑃ሼ𝑁ሺ1/2ሻൌ0, 𝑁ሺ1ሻൌ2ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
ൌ1 െ𝑃ሼ𝑁ሺ1/2ሻൌ0, 𝑁ሺ1ሻെ𝑁ሺ1/2ሻൌ2ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
ൌ1 െ𝑃ሼ𝑁ሺ1/2ሻൌ0ሽ𝑃ሼ𝑁ሺ1ሻെ𝑁ሺ1/2ሻൌ2ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
ൌ1 െ𝑃ሼ𝑁ሺ1/2ሻൌ0ሽ𝑃ሼ𝑁ሺ1/2ሻൌ2ሽ
𝑃ሼ𝑁ሺ1ሻൌ2ሽ
ൌ1 െ𝑒െఒ/ଶ𝑒െఒ/ଶሺ𝜆/2ሻଶ/2!
𝑒െఒ𝜆ଶ/2!
ൌ1 െ1/4 ൌ3/4
𝑋௡
𝑛
𝑛൒1.
𝑋௡, 𝑛൒1,
𝑃଴,଴ൌ5/6, 𝑃଴,ଵൌ1/6, 𝑃ଵ,଴ൌ4/5, 𝑃ଵ,ଵൌ1/5
𝜋଴
ൌ
𝜋଴ሺ5/6ሻ൅𝜋ଵሺ4/5ሻ
𝜋ଵ
ൌ
𝜋଴ሺ1/6ሻ൅𝜋ଵሺ1/5ሻ
𝜋଴൅𝜋ଵ
ൌ
1
𝜋଴ൌ24/29  𝜋ଵൌ5/29
809 of 848

Thus, 
 percent of the vehicles on the road are cars.
9.4.
 The successive weather classifications constitute a Markov
chain. If the states are 0 for rainy, 1 for sunny, and 2 for overcast,
then the transition probability matrix is as follows:
The long-run proportions satisfy
The solution of the preceding system of equations is
Hence, three-eighths of the days are sunny and one-fourth are rainy.
9.5.
a. A direct computation yields
b. Both random variables take on two of their values with the
same probabilities .35 and .05. The difference is that if they do
not take on either of those values, then 
 but not 
 is equally
likely to take on any of its three remaining possible values.
Hence, from Theoretical Exercise 9.13
, we would expect
the result of part (a).
2400/29 ൎ83
𝐏ൌ
0
1/2
1/2
1/3
1/3
1/3
1/3
1/3
1/3
𝜋଴
ൌ𝜋ଵሺ1/3ሻ൅𝜋ଶሺ1/3ሻ
𝜋ଵ
ൌ𝜋଴ሺ1/2ሻ൅𝜋ଵሺ1/3ሻ൅𝜋ଶሺ1/3ሻ
𝜋ଶ
ൌ𝜋଴ሺ1/2ሻ൅𝜋ଵሺ1/3ሻ൅𝜋ଶሺ1/3ሻ
1
ൌ𝜋଴൅𝜋ଵ൅𝜋ଶ
𝜋଴ൌ1/4, 𝜋ଵൌ3/8, 𝜋ଶൌ3/8
𝐻ሺ𝑋ሻ/𝐻ሺ𝑌ሻൎ1.06
𝑋,
𝑌,
10.1.
a. 
b. 
Hence, if we let 
 then
or
1 ൌ𝐶׬଴
ଵ𝑒௫𝑑𝑥⇒𝐶ൌ1/ሺ𝑒െ1ሻ
𝐹ሺ𝑥ሻൌ𝐶׬଴
௫𝑒௬𝑑𝑦ൌ𝑒௫െ1
𝑒െ1 , 0 ൑𝑥൑1
𝑋ൌ𝐹െଵሺ𝑈ሻ,
𝑈ൌ𝑒௑െ1
𝑒െ1
𝑋ൌlogሺ𝑈ሺ𝑒െ1ሻ൅1ሻ
810 of 848

Thus, we can simulate the random variable  by generating a random
number  and then setting 
10.2.
 Use the acceptance–rejection method with 
Calculus shows that the maximum value of 
/
 occurs at a value of
 such that
or, equivalently, when
The maximum thus occurs when 
 and it follows that
Hence, the algorithm is as follows:
10.3.
 It is most efficient to check the higher probability values first, as in the
following algorithm:
10.4.
10.5.
a. Generate 2  independent exponential random variables with mean
 and then use the estimator 
b. We can use 
 as a control variate to obtain an estimator of the type
Another possibility would be to use 
 as the control variate
and so obtain an estimator of the type
𝑋
𝑈
𝑋ൌlogሺ𝑈ሺ𝑒െ1ሻ൅1ሻ.
𝑔ሺ𝑥ሻൌ1, 0 ൏𝑥൏1.
𝑓ሺ𝑥ሻ𝑔ሺ𝑥ሻ
𝑥, 0 ൏𝑥൏1,
2𝑥െ6𝑥ଶ൅4𝑥ଷൌ0
4𝑥ଶെ6𝑥൅2 ൌሺ4𝑥െ2ሻሺ𝑥െ1ሻൌ0
𝑥ൌ1/2,
𝐶ൌmax 𝑓ሺ𝑥ሻ/𝑔ሺ𝑥ሻൌ30ሺ1/4 െ2/8 ൅1/16ሻൌ15/8
Generate a random number 
Generate a random number 
f 
 set 
 else return to Step 1.
Step 1.
𝑈ଵ.
Step 2.
𝑈ଶ.
Step 3. 𝑈ଶ൑16ሺ𝑈ଵ
ଶെ2𝑈ଵ
ଷ൅𝑈ଵ
ସሻ,
𝑋ൌ𝑈ଵ;
Generate a random number 
If 
 set 
 and stop.
If 
 set 
 and stop.
If 
 set 
 and stop.
Step 1.
𝑈.
Step 2. 𝑈൑.35,
𝑋ൌ3
Step 3. 𝑈൑.65,
𝑋ൌ4
Step 4. 𝑈൑.85,
𝑋ൌ2
Step 5.𝑋ൌ1.
2𝜇െ𝑋
𝑛
1, 𝑋௜, 𝑌௜, 𝑖ൌ1, . . . , 𝑛,
෍
௜ൌଵ
௡
𝑒௑೔௒೔/𝑛.
𝑋𝑌
෍
௜ൌଵ
௡
ሺ𝑒௑೔௒೔൅𝑐𝑋௜𝑌௜ሻ/𝑛
𝑋𝑌൅𝑋ଶ𝑌ଶ/2
෍
௜ൌଵ
௡
ሺ𝑒௑೔௒೔൅𝑐ሾ𝑋௜𝑌௜൅𝑋௜
ଶ𝑌௜
ଶ/2 െ1/2ሿሻ/𝑛
811 of 848

Analytic Theory of Probability, (Laplace), 405
antithetic variables, 463
Archimedes, 211
Ars Conjectandi, 145, 397
associative law for events, 24
axioms of probability, 26–28
axioms of surprise, 438
ballot problem, 115
Banach match problem, 161–162
basic principle of counting, 2
generalized, 2
Bayes’ formula, 72, 103
Benford’s law distribution, 175–176
Bernouulli, Jacques, 145–146,
Bernoulli, James, 146, 397
Bernoulli, Nicholas, 145, 397
Bernoulli random variable, 137, 143
Bernoulli trials, 114
Bernstein polynomials, 427
Bertrand’s paradox, 199–200
best prize problem, 350–351
beta binomial random variable, 299
beta distribution, 221, 233, 298
The motivation behind the preceding formula is based on the fact that
the first three terms of the MacLaurin series expansion of 
 are
𝑒௫௬
1 ൅𝑥𝑦൅ሺ𝑥ଶ𝑦ଶሻ/2.
812 of 848

binary symmetric channel, 445
binomial coefficients, 7
binomial random variable, 137, 174
normal approximation, 207–210
approximation to hypergeometric, 165
computing its mass function, 145
moments of, 322–323
simulation of, 461
sums of independent, 266–267, 366
with randomly chosen success probability, 351–352
binomial theorem, 7, 8
birthday problem, 38, 246–247
bivariate exponential distribution, 300
bivariate normal distribution, 272–274, 343–344, 372, 373
Bonferroni’s inequality, 55, 391
Boole’s inequality, 33, 57, 306–307
Borel, 409
Box-Muller simulation technique, 457
branching process, 389
bridge, 37–38, 60
Buffon’s needle problem, 249–250, 296
Cantor distribution, 387
Cauchy distribution, 220–221
Cauchy-Schwarz inequality, 387
center of gravity, 128
central limit theorem, 200, 397–399, 405
channel capacity, 446
813 of 848

Chapman-Kolmogorov equations, 434–435
Chebychev’s inequality, 395
one-sided, 410, 411
and weak law of large numbers, 397
Chebychev’s sum inequality, 429
Chernoff bound, 412–413
for binomial, 413–414
for standard normal, 413
for Poisson, 413
chi-squared distribution, 261
density function, 261
relation to gamma distribution, 261
simulation of, 459
coding theory, 441
and entropy, 443
combinations, 5, 6
combinatorial analysis, 1
combinatorial identities, 18, 19, 20, 21
commutative law for events, 24
complement of an event, 24
complete graph, 93–94
computing expectations by conditioning, 339–340
computing probabilities by conditioning, 64–71, 72–78, 102, 349
concave function, 415
conditional covariance formula, 387
conditional cumulative distribution function, 270
conditional distribution, 267, 274
continuous case, 270
814 of 848

discrete case, 267
conditional expectation, 337, 338
use in prediction, 356
use in simulation, 463
conditional independence, 100
conditional probability, 58, 59
as a long run relative frequency, 64
as a probability function, 95
satisfying axioms of probability, 95–96
conditional probability density function, 270
conditional probability distribution function, 267
conditional probability mass function, 267
conditional variance, 354
conditional variance formula, 354
and simulation, 463
continuity correction, 208
continuity property of probabilities, 44–46
continuous random variables, 189
control variate, 465
convex function, 415
convolution, 258
correlation, 334
coupon collecting problems, 121–123, 309–310, 320–321, 324–325, 327–328
covariance, 329
inequality, 415–417
craps, 341–343
cumulative distribution function, 123, 174
properties of, 172–174
815 of 848

de Mere, Chevalier, 84
DeMoivre, A., 200, 210, 211, 399
DeMoivre-Laplace limit theorem, 207
DeMorgan’s laws, 26
dependent events, 78
dependent random variables, 248
deviations, 331
discrete random variables, 123, 174
distribution function, see cumulative distribution function,
distribution of a function of a random variable, 224–226
distribution of a sum of a independent random variables, 353–354
distributive law for events, 24
DNA match, 76–78
dominant genes, 108
double exponential distribution, see Lapace distribution
doubly stochastic matrix, 447–448
Ehrenfest urn model, 434, 437
entropy, 439
ergodic Markov chain, 436
Erlang distribution, 219
evaluating evidence, 70–72
events, 23
decreasing sequence of, 44
increasing sequence of, 44
independent, 78
mutually exclusive, 24
exchangeable random variables, 287–288
816 of 848

expectation, 126, 174, 303, 375–376
of a beta random variable, 222
of a binomial random variable, 142, 170, 307
as a center of gravity, 128
of a continuous random variable, 193
of an exponential random variable, 212
of a function of a random variable, 128, 194
of a gamma random variable, 219
of a geometric random variable, 159–160
of a hypergeometric random variable, 166, 170, 308
of a negative binomial random variable, 162, 307–308
of a nonnegative random variable, 162
of a normal random variable, 202–203
of number of matches, 309
of number of runs, 310–311
of a Pareto random variable, 223–224
of a Poisson random variable, 148
of sums of a random number of random variables, 340–341
of sums of random variables, 167–170, 305
of the number of successes, 170
of a uniform random variable, 198
tables of, 364, 365
expected value, see expectation
exponential random variables, 211
moments of, 232
rate of, 216
relation to half life, 255–257
simulation of, 453
817 of 848

sums of, 261
failure rate function, see hazard rate function,
Fermat, P., 84, 89
Fermat’s combinatorial identity, 18
first digit distribution, 175–176
first moment, see mean
frequency interpretation of probability, 27, 126
friendship paradox, 134–135, 178
Galton, F., 405–406
gambler’s ruin problem, 87–89, 346–348
application to drug testing 89–91
multiple player, 86–87
game theory, 177
gamma random variables, 218, 297
relation to chi–squared distribution, 219, 261–262, 296
relation to exponential random variables, 261
relation to Poisson process, 218–219
simulation of, 454
sums of independent, 261
gamma function, 218, 261
relation to beta function, 284
Gauss, KF, 210, 211
Gaussian distribution, see normal distribution
genetics, 108, 109–110
geometric random variable, 158, 174–175
simulation of, 460–461
geometrical probability, 199
818 of 848

Gini index, 423–424
Hamiltonian path, 317–318
hazard rate function, 215–216, 296
relation to distribution function, 216
Huygens, C., 84, 89
hypergeometric random variable, 163–164, 175
relation to binomial, 165
moments of, 323–324
importance sampling, 467
inclusion-exclusion identity, 31–32, 314–315
inclusion-exclusion bounds, 32–33
independent events, 78, 80, 103
conditional, 100
independent increments, 430
independent random variables, 247–248, 252, 253, 257, 258
indicator random variables, 127,
information, 439
interarrival times of a Poisson process, 431–432
integer solution of equations, 12–13
intersection of events, 23, 24
inverse transform method, 453
discrete, 459–460
Jensen’s inequality, 415
joint cumulative probability distribution function, 237, 245
joint moment generating function, 370–371
joint probability density function, 241, 245
of functions of random variables, 280, 285
819 of 848

joint probability mass function, 238
jointly continuous random variables, 241, 245
k-of-n system, 109
keno, 182
Khintchine, 397
knockout tournament, 11–12
Kolmogorov, A., 409
Kullback-Leiber divergence, 427
Laplace, P., 200, 399, 405–406
Laplace distribution, 214–215
Laplace’s rule of succession, 100–101, 115, 116
law of frequency of errors, 406
law of total probability, 72
laws of large numbers, 394
Legendre’s theorem, 233
Liapounoff, 399
limit of events, 44
lineqr prediction, 359
lognormal distribution, 226–227, 265
Lorenz curve, 420, 426, 428
of an exponentially distributed population, 422
of a Pareto distributed population, 422
of a uniformly distributed population, 421
properties of, 422–423
marginal probability mass function, 245
Markov chain, 432–433
Markov’s inequality, 394–395
820 of 848

matching problem, 41–42, 55–56, 62–63, 99–100, 109, 324
maximum likelihood estimates, 183
maximums-minimums identity, 319–320
mean of a random variable, 132
measurable events, 29
median of a random variable, 232, 385
memoryless random variable, 213, 214
Mendel, G., 139
midrange, 298
minimax theorem, 177
mode of a random variable, 232
moment generating function, 360
of a binomialrandom variable, 361
of a chi-squared random variable, 367
of an exponential random variable, 363
of a normal random variable, 363–364
of a Poisson random variable, 362
of a sum of independent random variables, 364
of a sum of a random number of random variable, 367–368
tables for, 364, 365
moments of a random variable, 132
of the number of events that occur, 322
multinomial coefficients, 11
multinomial distribution, 246, 268, 336
multinomial theorem, 11
multiplication rule of probability, 61–62, 102
multivariate normal distribution, 371
joint moment generating function, 372
821 of 848

mutually exclusive events, 24
NCAA tournament, 91–93
negative binomial random variable, 160, 161, 175
relation to binomial, 185
relation to geometric, 161
negative hypergeometric random variable, 188, 325–327
Newton, I., 211
noiseless coding theorem, 442–443
noisy coding theorem, 446
normal random variables, 200, 281–282
approximations to binomial, 207–208
characterization of, 250–251
joint distribution of sample mean and sample variance, 373–375
moments of, 390
simulation, 283–284
simulation by polar method, 457–459
simulation by rejection method, 455–457
sums of independent, 262–263, 366–367
null event, 24
null set, 24
odds of an event, 70–71, 102
order statistics, 276
density of 
, 278
joint density of, 276, 279
parallel system, 81
Pareto, 167
Pareto random variable, 223, 275
𝑗௧௛
822 of 848

partition, 55, 57
Pascal, B., 84
Pascal random variables, see negative binomial random variables,
Pascal’s identity, 7
Pearson, K., 210
permutations, 3
personal view of probability, 48
Poisson, S., 146
Poisson paradigm, 151
Poisson process, 155–157, 430–432
Poisson random variables, 146, 174, 248–249, 268
as an approximation to binomial, 147
as an approximation to the number of events that occur, 149–151
bounds on the error of a Poisson approximation, 418–420
bounds on its probabilities, 413, 427
computing its probabilities, 158
simulation of, 461–462
sums of independent, 266, 366
poker, 36, 37
poker dice, 51
polar algorithm, 457–459
Polya’s urn model, 289
posterior probability, 101
power law density, 224
prior probability, 101
probabilistic method, 94, 317
probability of an event, 27
as a continuous set function, 44–46
823 of 848

as a limiting proportion, 26–27
as a measure of belief, 48–49
probability density function, 189
of a function of a random variable, 225
relation to cumulative distribution function, 192
probability mass function, 123, 174
relation to cumulative distribution function, 125
problem of the points, 84–85, 161
quicksort algorithm, 312–314
random number, 391, 451
pseudo, 451
random number generator, 451
random permutation, 393, 451–452
random subset, 253–254
random variables, 119, 174
random walk, 435
range of a random sample, 279–280
rate of the exponential, 216
Rayleigh density function, 216, 283
record value, 387
reduced sample space, 60
rejection method of simulation, 454–455
relative frequency definition of probability, 27
Riemann zeta function, 167
round robin tournament, 115
runs, 43–44, 97–98, 151–155
longest, 151–155
824 of 848

sample mean, 306
sample median, 278
sample space, 22
sample variance, 331
sampling from a finite population, 209–210
sampling with replacement, 52
sequential updating of information, 101–102
serve and rally games, 85–86
Shannon, C., 446
signal to noise ratio, 426
simulation, 451
St. Petersburg paradox, 178
standard deviation, 137, 174
inequality, 393
standard normal distribution function, 203–204
bounds, 427
table of, 204
standard normal random variable, 202
moments, 389
stationary increments, 430
Stieltjes integral, 375–376
Stirling’s approximation, 144
stochastically larger, 386
strong law of large numbers, 406–408
subjective probability, see personal probability
subset, 24
superset, 24
surprise, 438–439
825 of 848

t distribution, 271–272
transition probabilities, 433
n–stage, 434
trials, 80
triangular distribution, 259
twin problem, 70
uncertainty, 439–441
uncorrelated random variables, 335
uniform random variables, 197
sums of independent, 258–260
union of events, 23, 24
probability formula for, 29–32
unit normal random variable, see standard normal random variable,
utility, 131–132,
value at risk, 206–207
variance, 132, 133, 136, 174
of a beta random variable, 223
of a binomial random variable, 142–143, 171, 332
of a continuous random variable, 196
of an exponential random variable, 212
of a gamma random variable, 219
of a geometric random variable, 160, 345–346
of a hypergeometric random variable, 166–167, 171–172
as a moment of inertia, 137
of a negative binomial random variable, 162
of a normal random variable, 202–203
of a Pareto random variable, 224
826 of 848

of a Poisson random variable, 149
of a sum of a random number of random variables, 355
of sums of random variables, 330
of a uniform random variable, 199
of the number of successes, 171–172
tables for, 364, 365
Venn diagrams, 24, 25
von Neumann, J., 177
waiting times of a Poisson process, 432
weak law of large numbers, 397
Weibull distribution, 219–220
relation to exponential, 233
Weierstrass theorem, 427
Yule-Simon distribution, 183
zeta distribution, 167
Zipf distribution, see zeta distribution
E.T. Bell, Men of Mathematics, Simon and Schuster, 1986; P.S. Laplace, Analytical
Theory of Probability, Book II Chapter II, §4. pp. 194–203; Pierre-Simon, Marquis de
Laplace (1749–1827); Francis Galton, Natural Inheritance, published in 1889.
Bernoulli
 indicates whether a trial that results in a success with probability
 is a success or not.
ሺ𝑝ሻ 𝑋
𝑝
𝑃ሼ𝑋ൌ1ሽൌ𝑝
𝑃ሼ𝑋ൌ0ሽൌ1 െ𝑝
827 of 848

Binomial
 represents the number of successes in  independent trials
when each trial is a success with probability .
Note. Binomial
Bernoulli
Geometric
 is the number of trials needed to obtain a success when each
trial is independently a success with probability .
Negative Binomial
 is the number of trials needed to obtain a total of 
successes when each trial is independently a success with probability .
Note.
1. Negative Binomial
 = Geometric
.
2. Sum of  independent Geometric
 random variables is Negative
Binomial
.
Poisson
 is used to model the number of events that occur over a set
interval when these events are either independent or weakly dependent and each
has a small probability of occurrence.
Note.
1. A Poisson random variable  with parameter 
 provides a good
approximation to a Binomial
 random variable when  is large and  is
small.
2. If events are occurring one at a time in a random manner for which (a) the
number of events that occur in disjoint time intervals is independent and
(b) the probability of an event occurring in any small time interval is
approximately  times the length of the interval, then the number of events
in an interval of length  will be a Poisson
 random variable.
Hypergeometric
 is the number of white balls in a random sample
𝐸ሾ𝑋ሿൌ𝑝, Varሺ𝑋ሻൌ𝑝ሺ1 െ𝑝ሻ.
ሺ𝑛,𝑝ሻ 𝑋
𝑛
𝑝
𝑃ሼ𝑋ൌ𝑖ሽൌቆ𝑛
𝑖ቇ𝑝௜ሺ1 െ𝑝ሻ௡െ௜,
𝑖ൌ0,  1,  … ,  𝑛
𝐸ሾ𝑋ሿൌ𝑛𝑝, Varሺ𝑋ሻൌ𝑛𝑝ሺ1 െ𝑝ሻ.
ሺ1,𝑝ሻൌ 
ሺ𝑝ሻ.
ሺ𝑝ሻ 𝑋
𝑝
𝑃ሺ𝑋ൌ𝑖ሻൌ𝑝ሺ1 െ𝑝ሻ௜െଵ, 𝑖ൌ1,2,...,
𝐸ሾ𝑋ሿൌ
ଵ
௣, Varሺ𝑋ሻൌ
ଵെ௣
௣మ.
ሺ𝑟,𝑝ሻ  𝑋
𝑟
𝑝
𝑃ሺ𝑋ൌ𝑖ሻൌቆ𝑖െ1
𝑟െ1ቇ𝑝௥ሺ1 െ𝑝ሻ௜െ௥, 𝑖ൌ𝑟,𝑟൅1,𝑟൅2,...
𝐸ሾ𝑋ሿൌ
௥
௣, Varሺ𝑋ሻൌ𝑟
ଵെ௣
௣మ.
ሺ1,𝑝ሻ
ሺ𝑝ሻ
𝑟
ሺ𝑝ሻ
ሺ𝑟,𝑝ሻ
ሺ𝜆ሻ  𝑋
𝑃ሼ𝑋ൌ𝑖ሽൌ𝑒െఒ𝜆௜/𝑖!,  𝑖ൌ0, 1, 2, ...
𝐸ሾ𝑋ሿൌλ, Varሺ𝑋ሻൌλ.
𝑋
𝜆ൌ𝑛𝑝
ሺ𝑛,𝑝ሻ
𝑛
𝑝
λ
𝑡
ሺλ𝑡ሻ
ሺ𝑚, 𝑁െ𝑚, 𝑛ሻ 𝑋
828 of 848

of  balls chosen without replacement from an urn of  balls of which 
 are
white.
The preceding uses the convention that 
 if either 
 or 
With 
.
Note. If each ball were replaced before the next selection, then  would be a
Binomial
 random variable.
Negative Hypergeometric 
 is the number of balls that need be removed from
an urn that contains 
 balls, of which  are white, until a total of  white balls
has been removed, where 
Uniform 
 is equally likely to be near each value in the interval 
. Its
density function is
Normal
 is a random fluctuation arising from many causes. Its density
function is
1. When 
 is called a standard normal.
Notes.
2. If  is Normal
 then 
 is standard normal.
3. Sum of independent normal random variables is also normal.
𝑛
𝑁
𝑚
𝑃ሼ𝑋ൌ𝑖ሽൌ
ቆ𝑚
𝑖ቇቆ𝑁െ𝑚
𝑛െ𝑖ቇ
ቆ𝑁
𝑛ቇ
, 𝑖ൌ0, 1, 2, ...
ቆ𝑟
𝑗ቇൌ0
𝑗൏0
𝑗൐𝑟.
𝑝ൌ𝑚/𝑁,  𝐸ሾ𝑋ሿൌ𝑛𝑝, Varሺ𝑋ሻൌ
ேെ௡
ேെଵ𝑛𝑝ሺ1 െ𝑝ሻ
𝑋
ሺ𝑛,𝑝ሻ
 𝑋
𝑛൅𝑚
𝑛
𝑟
𝑟൑𝑛.
𝑃ሼ𝑋ൌ𝑘ሽൌ
ቆ
𝑛
𝑟െ1ቇቆ𝑚
𝑘െ𝑟ቇ
ቆ𝑛൅𝑚
𝑘െ1 ቇ
𝑛െ𝑟൅1
𝑛൅𝑚െ𝑘൅1 , 𝑘൒𝑟
𝐸ሾ𝑋ሿൌ𝑟
௡൅௠൅ଵ
௡൅ଵ
, Varሺ𝑋ሻൌ
௠௥ሺ௡൅ଵെ௥ሻሺ௡൅௠൅ଵሻ
ሺ௡൅ଵሻమሺ௡൅ଶሻ
ሺ𝑎,𝑏ሻ  𝑋
ሺ𝑎,𝑏ሻ
𝑓ሺ𝑥ሻൌ
1
𝑏െ𝑎, 𝑎൏𝑥൏𝑏
𝐸ሾ𝑋ሿൌ
௔൅௕
ଶ, Varሺ𝑋ሻൌ
ሺ௕െ௔ሻమ
ଵଶ
.
ሺ𝜇,𝜎ଶሻ  𝑋
𝑓ሺ𝑥ሻൌ
1
2𝜋
√
𝜎𝑒െሺ௫െఓሻమ/ଶఙమ,  െ∞൏𝑥൏∞
𝐸ሾ𝑋ሿൌ𝜇, Varሺ𝑋ሻൌ𝜎ଶ.
𝜇ൌ0, 𝜎ൌ1, 𝑋
𝑋
ሺ𝜇, 𝜎ଶሻ,
𝑍ൌ
௑െఓ
ఙ
829 of 848

4. An important result is the central limit theorem, which states that the
distribution of the sum of the first  of a sequence of independent and
identically distributed random variables becomes normal as  goes to
infinity, for any distribution of these random variables that has a finite mean
and variance.
Exponential
 is the waiting time until an event occurs when events are
occurring at a rate 
 Its density is
Note.  is memoryless, in that the remaining life of an item whose life distribution
is Exponential
 is also Exponential
 no matter what the current age of the
item is.
Gamma
 When 
,  is the waiting time until  events occur when events
are occurring at a rate 
 Its density is
where 
 is called the gamma function.
Note.
1. Gamma
= Exponential
2. If the random variables are independent, then the sum of a Gamma
and a Gamma
 is a Gamma
3. The sum of  independent and identically distributed exponentials with
parameter  is a Gamma
 random variable.
Beta
 is the distribution of a random variable taking on values in the
interval 
 Its density is
where 
 is called the beta function.
Note.
1. Beta
= Uniform
.
2. The 
 smallest of  independent Uniform 
 random variables is a
𝑛
𝑛
ሺλሻ  𝑋
λ ൐0.
𝑓ሺ𝑥ሻൌλ𝑒െλ௫, 𝑥൐0
𝐸ሾ𝑋ሿൌ
ଵ
λ , Varሺ𝑋ሻൌ
ଵ
λమ, 𝑃ሺ𝑋൐𝑥ሻൌ𝑒െλ௫, 𝑥൐0.
𝑋
ሺλሻ
ሺλሻ, 
ሺ𝛼,λሻ 
𝛼ൌ𝑛𝑋
𝑛
λ ൐0.
𝑓ሺ𝑡ሻൌλ𝑒െλ௧ሺλ𝑡ሻఈെଵ
Γ ሺ𝛼ሻ
,  𝑡൐0
  Γ ሺ𝛼ሻൌ׬଴
ஶ𝑒െ௫𝑥ఈെଵ𝑑𝑥
𝐸ሾ𝑋ሿൌ
ఈ
λ , Varሺ𝑋ሻൌ
ఈ
λమ.
ሺ1,λሻ 
ሺλሻ.
ሺ𝛼ଵ, λሻ 
ሺ𝛼ଶ, λሻ 
ሺ𝛼ଵ൅𝛼ଶ, λሻ.
𝑛
𝜆
ሺ𝑛,𝜆ሻ
ሺ𝑎, 𝑏ሻ  𝑋
ሺ0, 1ሻ.
𝑓ሺ𝑥ሻൌ
1
𝐵ሺ𝑎,𝑏ሻ𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ, 0 ൏𝑥൏1
 𝐵ሺ𝑎,𝑏ሻൌ඲
଴
ଵ
𝑥௔െଵሺ1 െ𝑥ሻ௕െଵ𝑑𝑥
𝐸ሾ𝑋ሿൌ
௔
௔൅௕, Varሺ𝑋ሻൌ
௔௕
ሺ௔൅௕ሻమሺ௔൅௕൅ଵሻ.
ሺ1, 1ሻ
ሺ0,1ሻ
𝑗௧௛
𝑛
ሺ0, 1ሻ
830 of 848

Beta
 random variable.
Chi-Squared
 is the sum of the squares of  independent standard
normal random variables. Its density is
Add 
 and 
Notes.
1. Chi-Squared
= Gamma
.
2. The sample variance of  independent and identically distributed Normal
 random variables multiplied by 
 is a Chi-Squared
random variable, and it is independent of the sample mean.
Cauchy 
 is the tangent of a uniformly distributed random angle between 
and 
. Its density is
 is undefined.
Pareto
 If  is exponential with rate  and 
 then 
 is said to
Pareto with parameters  and . Its density is
When 
, 
, and when 
, 
.
Note.
The conditional distribution of  given that it exceeds 
 is Pareto (
).
1. A First Course in Probability
2. A First Course in Probability
3. Contents
4. Preface
A. General Approach and Mathematical Level
B. Content and Course Planning
C. Changes for the Tenth Edition
5. Chapter 1 Combinatorial Analysis
A. Contents
ሺ𝑗, 𝑛െ𝑗൅1ሻ
ሺ𝑛ሻ𝑋
𝑛
𝑓ሺ𝑥ሻൌ𝑒െ௫/ଶ𝑥
೙
మെଵ
2௡/ଶΓ ሺ𝑛/2ሻ
, 𝑥൐0
𝐸ሺ𝑋ሻ
𝑉𝑎𝑟ሺ𝑋ሻ
ሺ𝑛ሻ
ሺ𝑛/2, 1/2ሻ
𝑛
ሺ𝜇, 𝜎ଶሻ 
௡െଵ
ఙమ
ሺ𝑛െ1ሻ
 𝑋
െ𝜋/2
𝜋/2
𝑓ሺ𝑥ሻൌ
1
𝜋ሺ1 ൅𝑥ଶሻ,  െ∞൏𝑥൏∞
𝐸ሾ𝑋ሿ
ሺλ, 𝑎ሻ
𝑌
λ
𝑎൐0,
𝑋ൌ𝑎𝑒௒
λ
𝑎
𝑓ሺ𝑥ሻൌλ𝑎λ𝑥െሺλ൅ଵሻ,  𝑥൐𝑎
λ ൐1 𝐸ሾ𝑋ሿൌ
λ௔
λെଵ
λ ൐2 Varሺ𝑋ሻൌ
λ௔మ
ሺλെଶሻሺλെଵሻమ
𝑋
𝑥଴൐𝑎
λ,𝑥଴
831 of 848

B. 1.1 Introduction
C. 1.2 The Basic Principle of Counting
D. 1.3 Permutations
E. 1.4 Combinations
F. 1.5 Multinomial Coefficients
G. * 1.6 The Number of Integer Solutions of Equations
H. Summary
I. Problems
J. Theoretical Exercises
K. Self-Test Problems and Exercises
6. Chapter 2 Axioms of Probability
A. Contents
B. 2.1 Introduction
C. 2.2 Sample Space and Events
D. 2.3 Axioms of Probability
E. 2.4 Some Simple Propositions
F. 2.5 Sample Spaces Having Equally Likely Outcomes
G. 2.6 Probability as a Continuous Set Function
H. 2.7 Probability as a Measure of Belief
I. Summary
J. Problems
K. Theoretical Exercises
L. Self-Test Problems and Exercises
7. Chapter 3 Conditional Probability and Independence
A. Contents
B. 3.1 Introduction
C. 3.2 Conditional Probabilities
D. 3.3 Bayes’s Formula
E. 3.4 Independent Events
F. 3.5 P( |F) Is a Probability
G. Summary
H. Problems
I. Theoretical Exercises
J. Self-Test Problems and Exercises
8. Chapter 4 Random Variables
A. Contents
B. 4.1 Random Variables
C. 4.2 Discrete Random Variables
D. 4.3 Expected Value
*
⋅
832 of 848

E. 4.4 Expectation of a Function of a Random Variable
F. 4.5 Variance
G. 4.6 The Bernoulli and Binomial Random Variables
H. 4.7 The Poisson Random Variable
I. 4.8 Other Discrete Probability Distributions
J. 4.9 Expected Value of Sums of Random Variables
K. 4.10 Properties of the Cumulative Distribution function
L. Summary
M. Problems
N. Theoretical Exercises
O. Self-Test Problems and Exercises
9. Chapter 5 Continuous Random Variables
A. Contents
B. 5.1 Introduction
C. 5.2 Expectation and Variance of Continuous Random Variables
D. 5.3 The Uniform Random Variable
E. 5.4 Normal Random Variables
F. 5.5 Exponential Random Variables
G. 5.6 Other Continuous Distributions
H. 5.7 The Distribution of a Function of a Random Variable
I. Summary
J. Problems
K. Theoretical Exercises
L. Self-Test Problems and Exercises
10. Chapter 6 Jointly Distributed Random Variables
A. Contents
B. 6.1 Joint Distribution Functions
C. 6.2 Independent Random Variables
D. 6.3 Sums of Independent Random Variables
E. 6.4 Conditional Distributions: Discrete Case
F. 6.5 Conditional Distributions: Continuous Case
G.  6.6 Order statistics
H. 6.7 Joint Probability Distribution of Functions of Random Variables
I.  6.8 Exchangeable Random Variables
J. Summary
K. Problems
L. Theoretical exercises
M. Self-Test Problems and Exercises
11. Chapter 7 Properties of Expectation
*
*
833 of 848

A. Contents
B. 7.1 Introduction
C. 7.2 Expectation of Sums of Random Variables
D. 7.3 Moments of the Number of Events that Occur
E. 7.4 Covariance, Variance of Sums, and Correlations
F. 7.5 Conditional Expectation
G. 7.6 Conditional Expectation and Prediction
H. 7.7 Moment Generating Functions
I. 7.8 Additional Properties of Normal Random Variables
J. 7.9 General Definition of Expectation
K. Summary
L. Problems
M. Theoretical exercises
N. Self-Test Problems and Exercises
12. Chapter 8 Limit Theorems
A. Contents
B. 8.1 Introduction
C. 8.2 Chebyshev’s Inequality and the Weak Law of Large Numbers
D. 8.3 The Central Limit Theorem
E. 8.4 The Strong Law of Large Numbers
F. 8.5 Other Inequalities and a Poisson Limit Result
G. 
8.6 Bounding the Error Probability When Approximating a Sum of
Independent Bernoulli Random Variables by a Poisson Random Variable
H. 8.7 The Lorenz Curve
I. Summary
J. Problems
K. Theoretical Exercises
L. Self-Test Problems And Exercises
13. Chapter 9 Additional Topics in Probability
A. Contents
B. 9.1 The Poisson Process
C. 9.2 Markov Chains
D. 9.3 Surprise, Uncertainty, and Entropy
E. 9.4 Coding Theory and Entropy
F. Summary
G. Problems and Theoretical Exercises
H. Self-Test Problems and Exercises
I. References
14. Chapter 10 Simulation
834 of 848

A. Contents
B. 10.1 Introduction
C. 10.2 General Techniques for Simulating Continuous Random Variables
D. 10.3 Simulating from Discrete Distributions
E. 10.4 Variance Reduction Techniques
F. Summary
G. Problems
H. Self-Test Problems and Exercises
I. Reference
15. Answers to Selected Problems
16. Solutions to Self-Test Problems and Exercises
17. Index
18. Credits
19. Common Discrete Distributions
20. Common Continuous Distributions
1. Table 9.1 Repetition of Bits Encoding Scheme.
1. Frontmatter
2. Start of Content
3. backmatter
4. List of Illustrations
5. List of Tables
1. i
2. ii
3. iii
4. iv
5. v
6. vi
7. vii
8. viii
9. ix
10. x
11. xi
835 of 848

12. xii
13. 1
14. 2
15. 3
16. 4
17. 5
18. 6
19. 7
20. 8
21. 9
22. 10
23. 11
24. 12
25. 13
26. 14
27. 15
28. 16
29. 17
30. 18
31. 19
32. 20
33. 21
34. 22
35. 23
36. 24
37. 25
38. 26
39. 27
40. 28
41. 29
42. 30
43. 31
44. 32
45. 33
46. 34
47. 35
48. 36
49. 37
50. 38
51. 39
52. 40
836 of 848

53. 41
54. 42
55. 43
56. 44
57. 45
58. 46
59. 47
60. 48
61. 49
62. 50
63. 51
64. 52
65. 53
66. 54
67. 55
68. 56
69. 57
70. 58
71. 59
72. 60
73. 61
74. 62
75. 63
76. 64
77. 65
78. 66
79. 67
80. 68
81. 69
82. 70
83. 71
84. 72
85. 73
86. 74
87. 75
88. 76
89. 77
90. 78
91. 79
92. 80
93. 81
837 of 848

94. 82
95. 83
96. 84
97. 85
98. 86
99. 87
100. 88
101. 89
102. 90
103. 91
104. 92
105. 93
106. 94
107. 95
108. 96
109. 97
110. 98
111. 99
112. 100
113. 101
114. 102
115. 103
116. 104
117. 105
118. 106
119. 107
120. 108
121. 109
122. 110
123. 111
124. 112
125. 113
126. 114
127. 115
128. 116
129. 117
130. 118
131. 119
132. 120
133. 121
134. 122
838 of 848

135. 123
136. 124
137. 125
138. 126
139. 127
140. 128
141. 129
142. 130
143. 131
144. 132
145. 133
146. 134
147. 135
148. 136
149. 137
150. 138
151. 139
152. 140
153. 141
154. 142
155. 143
156. 144
157. 145
158. 146
159. 147
160. 148
161. 149
162. 150
163. 151
164. 152
165. 153
166. 154
167. 155
168. 156
169. 157
170. 158
171. 159
172. 160
173. 161
174. 162
175. 163
839 of 848

176. 164
177. 165
178. 166
179. 167
180. 168
181. 169
182. 170
183. 171
184. 172
185. 173
186. 174
187. 175
188. 176
189. 177
190. 178
191. 179
192. 180
193. 181
194. 182
195. 183
196. 184
197. 185
198. 186
199. 187
200. 188
201. 189
202. 190
203. 191
204. 192
205. 193
206. 194
207. 195
208. 196
209. 197
210. 198
211. 199
212. 200
213. 201
214. 202
215. 203
216. 204
840 of 848

217. 205
218. 206
219. 207
220. 208
221. 209
222. 210
223. 211
224. 212
225. 213
226. 214
227. 215
228. 216
229. 217
230. 218
231. 219
232. 220
233. 221
234. 222
235. 223
236. 224
237. 225
238. 226
239. 227
240. 228
241. 229
242. 230
243. 231
244. 232
245. 233
246. 234
247. 235
248. 236
249. 237
250. 238
251. 239
252. 240
253. 241
254. 242
255. 243
256. 244
257. 245
841 of 848

258. 246
259. 247
260. 248
261. 249
262. 250
263. 251
264. 252
265. 253
266. 254
267. 255
268. 256
269. 257
270. 258
271. 259
272. 260
273. 261
274. 262
275. 263
276. 264
277. 265
278. 266
279. 267
280. 268
281. 269
282. 270
283. 271
284. 272
285. 273
286. 274
287. 275
288. 276
289. 277
290. 278
291. 279
292. 280
293. 281
294. 282
295. 283
296. 284
297. 285
298. 286
842 of 848

299. 287
300. 288
301. 289
302. 290
303. 291
304. 292
305. 293
306. 294
307. 295
308. 296
309. 297
310. 298
311. 299
312. 300
313. 301
314. 302
315. 303
316. 304
317. 305
318. 306
319. 307
320. 308
321. 309
322. 310
323. 311
324. 312
325. 313
326. 314
327. 315
328. 316
329. 317
330. 318
331. 319
332. 320
333. 321
334. 322
335. 323
336. 324
337. 325
338. 326
339. 327
843 of 848

340. 328
341. 329
342. 330
343. 331
344. 332
345. 333
346. 334
347. 335
348. 336
349. 337
350. 338
351. 339
352. 340
353. 341
354. 342
355. 343
356. 344
357. 345
358. 346
359. 347
360. 348
361. 349
362. 350
363. 351
364. 352
365. 353
366. 354
367. 355
368. 356
369. 357
370. 358
371. 359
372. 360
373. 361
374. 362
375. 363
376. 364
377. 365
378. 366
379. 367
380. 368
844 of 848

381. 369
382. 370
383. 371
384. 372
385. 373
386. 374
387. 375
388. 376
389. 377
390. 378
391. 379
392. 380
393. 381
394. 382
395. 383
396. 384
397. 385
398. 386
399. 387
400. 388
401. 389
402. 390
403. 391
404. 392
405. 393
406. 394
407. 395
408. 396
409. 397
410. 398
411. 399
412. 400
413. 401
414. 402
415. 403
416. 404
417. 405
418. 406
419. 407
420. 408
421. 409
845 of 848

422. 410
423. 411
424. 412
425. 413
426. 414
427. 415
428. 416
429. 417
430. 418
431. 419
432. 420
433. 421
434. 422
435. 423
436. 424
437. 425
438. 426
439. 427
440. 428
441. 429
442. 430
443. 431
444. 432
445. 433
446. 434
447. 435
448. 436
449. 437
450. 438
451. 439
452. 440
453. 441
454. 442
455. 443
456. 444
457. 445
458. 446
459. 447
460. 448
461. 449
462. 450
846 of 848

463. 451
464. 452
465. 453
466. 454
467. 455
468. 456
469. 457
470. 458
471. 459
472. 460
473. 461
474. 462
475. 463
476. 464
477. 465
478. 466
479. 467
480. 468
481. 469
482. 470
483. 471
484. 472
485. 473
486. 474
487. 475
488. 476
489. 477
490. 478
491. 479
492. 480
493. 481
494. 482
495. 483
496. 484
497. 485
498. 486
499. 487
500. 488
501. 489
502. 490
503. 491
847 of 848

504. 492
505. 493
506. 494
507. 495
508. 496
509. 497
510. 498
511. 499
512. 500
513. 501
514. 502
515. 503
516. 504
517. 505
518. 506
519. 507
520. 508
521. 509
522. 510
848 of 848


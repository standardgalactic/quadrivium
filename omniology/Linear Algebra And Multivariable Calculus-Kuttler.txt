Math 302 Lecture Notes
Kenneth Kuttler
October 6, 2006

2

Contents
1
Introduction
11
I
Vectors, Vector Products, Lines
13
2
Vectors And Points In Rn 5 Sept.
19
2.1
Rn Ordered n−tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
Vectors And Algebra In Rn
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3
Geometric Meaning Of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.4
Geometric Meaning Of Vector Addition
. . . . . . . . . . . . . . . . . . . . .
22
2.5
Distance Between Points In Rn . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.6
Geometric Meaning Of Scalar Multiplication
. . . . . . . . . . . . . . . . . .
26
2.7
Unit Vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.8
Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.9
Vectors And Physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3
Vector Products
39
3.1
The Dot Product 6 Sept.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.1.1
Deﬁnition In terms Of Coordinates . . . . . . . . . . . . . . . . . . . .
39
3.1.2
The Geometric Meaning Of The Dot Product, The Included Angle . .
40
3.1.3
The Cauchy Schwarz Inequality . . . . . . . . . . . . . . . . . . . . . .
42
3.1.4
The Triangle Inequality . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.1.5
Direction Cosines Of A Line . . . . . . . . . . . . . . . . . . . . . . . .
44
3.1.6
Work And Projections . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2
The Cross Product 7 Sept.
. . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.1
The Geometric Description Of The Cross Product In Terms Of The
Included Angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.2
The Coordinate Description Of The Cross Product . . . . . . . . . . .
50
3.2.3
The Box Product, Triple Product . . . . . . . . . . . . . . . . . . . .
52
3.2.4
A Proof Of The Distributive Law For The Cross Product∗. . . . . . .
53
3.2.5
Torque, Moment Of A Force
. . . . . . . . . . . . . . . . . . . . . . .
54
3.2.6
Angular Velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.2.7
Center Of Mass∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.3
Further Explanations∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.3.1
The Distributive Law For The Cross Product∗. . . . . . . . . . . . .
57
3.3.2
Vector Identities And Notation∗
. . . . . . . . . . . . . . . . . . . . .
59
3.3.3
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3

4
CONTENTS
II
Planes And Systems Of Equations
69
4
Planes 11 Sept.
73
4.1
Finding Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.1.1
Planes From A Normal And A Point . . . . . . . . . . . . . . . . . . .
73
4.1.2
The Angle Between Two Planes
. . . . . . . . . . . . . . . . . . . . .
74
4.1.3
The Plane Which Contains Three Points . . . . . . . . . . . . . . . . .
75
4.1.4
Intercepts Of A Plane . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.1.5
Distance Between A Point And A Plane Or A Point And A Line∗
. .
77
5
Systems Of Linear Equations 12,13 Sept.
79
5.1
Systems Of Equations, Geometric Interpretations
. . . . . . . . . . . . . . .
79
5.2
Systems Of Equations, Algebraic Procedures
. . . . . . . . . . . . . . . . . .
82
5.2.1
Elementary Operations
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.2.2
Gauss Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.3
The Rank Of A Matrix 14 Sept. . . . . . . . . . . . . . . . . . . . . . . .
94
5.4
Theory Of Row Reduced Echelon Form∗. . . . . . . . . . . . . . . . . . . . .
96
5.4.1
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . .
99
III
Linear Independence And Matrices
107
6
Spanning Sets And Linear Independence 18,19 Sept.
111
6.0.2
Spanning Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.0.3
Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
6.0.4
Recognizing Linear Dependence . . . . . . . . . . . . . . . . . . . . . . 118
6.0.5
Discovering Dependence Relations
. . . . . . . . . . . . . . . . . . . . 119
7
Matrices
121
7.1
Matrix Operations And Algebra 20,21 Sept.
. . . . . . . . . . . . . . 121
7.1.1
Addition And Scalar Multiplication Of Matrices
. . . . . . . . . . . . 121
7.1.2
Multiplication Of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . 124
7.1.3
The ijth Entry Of A Product . . . . . . . . . . . . . . . . . . . . . . . 127
7.1.4
Properties Of Matrix Multiplication
. . . . . . . . . . . . . . . . . . . 129
7.1.5
The Transpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
7.1.6
The Identity And Inverses . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.2
Finding The Inverse Of A Matrix, Gauss Jordan Method 21,22 Sept.133
7.3
Elementary Matrices 22 Sept.
. . . . . . . . . . . . . . . . . . . . . . . . 138
7.4
Block Multiplication Of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.4.1
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 146
IV
LU Decomposition, Subspaces, Linear Transformations
151
8
The LU Factorization 25 Sept.
155
8.0.2
Deﬁnition Of An LU Decomposition . . . . . . . . . . . . . . . . . . . 155
8.0.3
Finding An LU Decomposition By Inspection . . . . . . . . . . . . . . 155
8.0.4
Using Multipliers To Find An LU Decomposition . . . . . . . . . . . . 156
8.0.5
Solving Systems Using The LU Decomposition . . . . . . . . . . . . . 157

CONTENTS
5
9
Rank Of A Matrix 26,27 Sept.
159
9.1
The Row Reduced Echelon Form Of A Matrix . . . . . . . . . . . . . . . . . . 159
9.2
The Rank Of A Matrix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
9.2.1
The Deﬁnition Of Rank . . . . . . . . . . . . . . . . . . . . . . . . . . 163
9.2.2
Finding The Row And Column Space Of A Matrix . . . . . . . . . . . 164
9.3
Linear Independence And Bases . . . . . . . . . . . . . . . . . . . . . . . . . . 166
9.3.1
Linear Independence And Dependence . . . . . . . . . . . . . . . . . . 166
9.3.2
Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.3.3
The Basis Of A Subspace . . . . . . . . . . . . . . . . . . . . . . . . . 170
9.3.4
Finding The Null Space Or Kernel Of A Matrix
. . . . . . . . . . . . 172
9.3.5
Rank And Existence Of Solutions To Linear Systems∗. . . . . . . . . 174
9.3.6
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 175
10 Linear Transformations 27 Sept.
181
10.1 Constructing The Matrix Of A Linear Transformation . . . . . . . . . . . . . 182
10.1.1 Rotations of R2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
10.1.2 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.1.3 Matrices Which Are One To One Or Onto . . . . . . . . . . . . . . . . 186
10.1.4 The General Solution Of A Linear System . . . . . . . . . . . . . . . . 187
10.1.5 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 190
V
Eigenvalues, Eigenvectors, Determinants, Diagonalization
193
11 Determinants 2,3 Oct.
197
11.1 Basic Techniques And Properties . . . . . . . . . . . . . . . . . . . . . . . . . 197
11.1.1 Cofactors And 2 × 2 Determinants . . . . . . . . . . . . . . . . . . . . 197
11.1.2 The Determinant Of A Triangular Matrix . . . . . . . . . . . . . . . . 200
11.1.3 Properties Of Determinants . . . . . . . . . . . . . . . . . . . . . . . . 201
11.1.4 Finding Determinants Using Row Operations . . . . . . . . . . . . . . 203
11.1.5 A Formula For The Inverse . . . . . . . . . . . . . . . . . . . . . . . . 204
12 Eigenvalues And Eigenvectors Of A Matrix 4-6 Oct.
209
12.0.6 Deﬁnition Of Eigenvectors And Eigenvalues . . . . . . . . . . . . . . . 209
12.0.7 Finding Eigenvectors And Eigenvalues . . . . . . . . . . . . . . . . . . 211
12.0.8 A Warning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
12.0.9 Defective And Nondefective Matrices . . . . . . . . . . . . . . . . . . . 215
12.0.10Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
12.0.11Migration Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
12.0.12Complex Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
12.0.13The Estimation Of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . 228
12.1 The Mathematical Theory Of Determinants∗. . . . . . . . . . . . . . . 229
12.1.1
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
12.2 The Cayley Hamilton Theorem∗. . . . . . . . . . . . . . . . . . . . . . . 241
12.2.1 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 242
VI
Curves, Curvilinear Motion, Surfaces
253
13 Quadric Surfaces 9 Oct.
257

6
CONTENTS
14 Curves In Space 10,11 Oct.
261
14.1 Limits Of A Vector Valued Function Of One Variable
. . . . . . . . . . . . . 261
14.2 The Derivative And Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
14.2.1 Arc Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
14.2.2 Geometric And Physical Signiﬁcance Of The Derivative . . . . . . . . 267
14.2.3 Diﬀerentiation Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
14.2.4 Leibniz’s Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
14.2.5 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 271
15 Newton’s Laws Of Motion∗
273
15.0.6 Kinetic Energy∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
15.0.7 Impulse And Momentum∗. . . . . . . . . . . . . . . . . . . . . . . . . 278
15.0.8 Conservation Of Momentum∗. . . . . . . . . . . . . . . . . . . . . . . 278
15.0.9 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 279
16 Physics Of Curvilinear Motion 12 Oct.
281
16.0.10The Acceleration In Terms Of The Unit Tangent And Normal . . . . . 281
16.0.11The Curvature Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
16.0.12The Circle Of Curvature* . . . . . . . . . . . . . . . . . . . . . . . . . 286
16.1 Geometry Of Space Curves∗
. . . . . . . . . . . . . . . . . . . . . . . . . 288
16.2 Independence Of Parameterization∗
. . . . . . . . . . . . . . . . . . . . 291
16.2.1 Hard Calculus∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
16.2.2 Independence Of Parameterization∗
. . . . . . . . . . . . . . . . . . . 295
16.3 Product Rule For Matrices∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
16.4 Moving Coordinate Systems∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . 298
VII
Functions Of Many Variables
301
17 Functions Of Many Variables 16 Oct.
305
17.1 The Graph Of A Function Of Two Variables . . . . . . . . . . . . . . . . . . . 305
17.2 The Domain Of A Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
17.3 Open And Closed Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
17.4 Continuous Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
17.5 Suﬃcient Conditions For Continuity . . . . . . . . . . . . . . . . . . . . . . . 312
17.6 Properties Of Continuous Functions
. . . . . . . . . . . . . . . . . . . . . . . 313
18 Limits Of A Function 17-23 Oct.
315
18.1 The Directional Derivative And Partial Derivatives . . . . . . . . . . . . . . . 318
18.1.1 The Directional Derivative
. . . . . . . . . . . . . . . . . . . . . . . . 318
18.1.2 Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
18.1.3 Mixed Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . 323
18.2 Some Fundamentals∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
18.2.1 The Nested Interval Lemma∗. . . . . . . . . . . . . . . . . . . . . . . 328
18.2.2 The Extreme Value Theorem∗
. . . . . . . . . . . . . . . . . . . . . . 329
18.2.3 Sequences And Completeness∗
. . . . . . . . . . . . . . . . . . . . . . 330
18.2.4 Continuity And The Limit Of A Sequence∗
. . . . . . . . . . . . . . . 333

CONTENTS
7
VIII
Diﬀerentiability
335
19 Diﬀerentiability 24-26 Oct.
339
19.1 The Deﬁnition Of Diﬀerentiability
. . . . . . . . . . . . . . . . . . . . . . . . 339
19.2 C1 Functions And Diﬀerentiability . . . . . . . . . . . . . . . . . . . . . . . . 341
19.3 The Directional Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
19.3.1 Separable Diﬀerential Equations∗. . . . . . . . . . . . . . . . . . . . . 344
19.3.2 Exercises With Answers∗
. . . . . . . . . . . . . . . . . . . . . . . . . 347
19.3.3 A Heat Seaking Particle . . . . . . . . . . . . . . . . . . . . . . . . . . 348
19.4 The Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
19.4.1 Related Rates Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 351
19.5 Normal Vectors And Tangent Planes 26 Oct. . . . . . . . . . . . . . . . 353
20 Extrema Of Functions Of Several Variables 30 Oct.
355
20.1 Local Extrema
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
20.2 The Second Derivative Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
20.2.1 Functions Of Two Variables . . . . . . . . . . . . . . . . . . . . . . . . 358
20.2.2 Functions Of Many Variables∗
. . . . . . . . . . . . . . . . . . . . . . 359
20.3 Lagrange Multipliers, Constrained Extrema 31 Oct. . . . . . . . . . . 362
20.3.1 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 367
21 The Derivative Of Vector Valued Functions, What Is The Derivative?∗
371
21.1 C1 Functions∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
21.2 The Chain Rule∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
21.2.1 The Chain Rule For Functions Of One Variable∗. . . . . . . . . . . . 377
21.2.2 The Chain Rule For Functions Of Many Variables∗. . . . . . . . . . . 377
21.2.3 The Derivative Of The Inverse Function∗
. . . . . . . . . . . . . . . . 381
21.2.4 Acceleration In Spherical Coordinates∗. . . . . . . . . . . . . . . . . . 381
21.3 Proof Of The Chain Rule∗. . . . . . . . . . . . . . . . . . . . . . . . . . . 384
21.4 Proof Of The Second Derivative Test∗. . . . . . . . . . . . . . . . . . . 386
22 Implicit Function Theorem∗
389
22.1 The Method Of Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . . . . 393
22.2 The Local Structure Of C1 Mappings
. . . . . . . . . . . . . . . . . . . . . . 394
IX
Multiple Integrals
397
23 The Riemann Integral On Rn
403
23.1 Methods For Double Integrals 1 Nov.
. . . . . . . . . . . . . . . . . . . 403
23.1.1 Density Mass And Center Of Mass . . . . . . . . . . . . . . . . . . . . 410
23.2 Double Integrals In Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . 411
23.3 Methods For Triple Integrals 2-7 Nov. . . . . . . . . . . . . . . . . . . . 416
23.3.1 Deﬁnition Of The Integral . . . . . . . . . . . . . . . . . . . . . . . . . 416
23.3.2 Iterated Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
23.3.3 Mass And Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
23.3.4 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 423

8
CONTENTS
24 The Integral In Other Coordinates 8-10 Nov.
427
24.1 Diﬀerent Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
24.1.1 Review Of Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . . 428
24.1.2 General Two Dimensional Coordinates . . . . . . . . . . . . . . . . . . 429
24.1.3 Three Dimensions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
24.1.4 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 436
24.2 The Moment Of Inertia ∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . 442
24.2.1 The Spinning Top∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
24.2.2 Kinetic Energy∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
24.3 Finding The Moment Of Inertia And Center Of Mass 13 Nov.
. . . 447
24.4 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
X
Line Integrals
455
25 Line Integrals 14 Nov.
459
25.0.1
Orientations And Smooth Curves
. . . . . . . . . . . . . . . . . . . . 459
25.0.2 The Integral Of A Function Deﬁned On A Smooth Curve . . . . . . . 461
25.0.3 Vector Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
25.0.4 Line Integrals And Work
. . . . . . . . . . . . . . . . . . . . . . . . . 464
25.0.5 Another Notation For Line Integrals . . . . . . . . . . . . . . . . . . . 466
25.0.6 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 467
25.1 Path Independent Line Integrals 15 Nov. . . . . . . . . . . . . . . . . . 468
25.1.1 Finding The Scalar Potential, (Recover The Function From Its Gradient)469
25.1.2 Terminology
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
XI
Green’s Theorem, Integrals On Surfaces
473
26 Green’s Theorem 20 Nov.
477
26.1 An Alternative Explanation Of Green’s Theorem . . . . . . . . . . . . . . . . 479
26.2 Area And Green’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
27 The Integral On Two Dimensional Surfaces In R3 27-28 Nov.
485
27.1 Parametrically Deﬁned Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . 485
27.2 The Two Dimensional Area In R3 . . . . . . . . . . . . . . . . . . . . . . . . . 487
27.2.1 Surfaces Of The Form z = f (x, y)
. . . . . . . . . . . . . . . . . . . . 494
27.3 Flux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
27.3.1 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 496
XII
Divergence Theorem
501
28 The Divergence Theorem 29-30 Nov.
505
28.1 Divergence Of A Vector Field
. . . . . . . . . . . . . . . . . . . . . . . . 505
28.2 The Divergence Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
28.2.1 Coordinate Free Concept Of Divergence, Flux Density . . . . . . . . . 510
28.3 The Weak Maximum Principle∗. . . . . . . . . . . . . . . . . . . . . . . 510
28.4 Some Applications Of The Divergence Theorem∗. . . . . . . . . . . . 511
28.4.1 Hydrostatic Pressure∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . 511
28.4.2 Archimedes Law Of Buoyancy∗. . . . . . . . . . . . . . . . . . . . . . 512
28.4.3 Equations Of Heat And Diﬀusion∗. . . . . . . . . . . . . . . . . . . . 512

CONTENTS
9
28.4.4 Balance Of Mass∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
28.4.5 Balance Of Momentum∗. . . . . . . . . . . . . . . . . . . . . . . . . . 514
28.4.6 Bernoulli’s Principle∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . 519
28.4.7 The Wave Equation∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
28.4.8 A Negative Observation∗
. . . . . . . . . . . . . . . . . . . . . . . . . 521
28.4.9 Electrostatics∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
XIII
Stoke’s Theorem
523
29 Stoke’s Theorem 4-5 Dec.
527
29.1 Curl Of A Vector Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
29.2 Green’s Theorem, A Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
29.3 Stoke’s Theorem From Green’s Theorem . . . . . . . . . . . . . . . . . . . . . 529
29.3.1 Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
29.3.2 Conservative Vector Fields And Stoke’s Theorem . . . . . . . . . . . . 533
29.3.3 Some Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
29.3.4 Vector Identities∗. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
29.3.5 Vector Potentials∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
29.3.6 Maxwell’s Equations And The Wave Equation∗. . . . . . . . . . . . . 536
XIV
Some Iterative Techniques For Linear Algebra
539
30 Iterative Methods For Linear Systems
541
30.1
Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
30.2 Gauss Seidel Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
31 Iterative Methods For Finding Eigenvalues
551
31.1 The Power Method For Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . 551
31.1.1 Rayleigh Quotient
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
31.2 The Shifted Inverse Power Method . . . . . . . . . . . . . . . . . . . . . . . . 556
XV
The Correct Version Of The Riemann Integral ∗
563
A The Theory Of The Riemann Integral∗∗
565
A.1 An Important Warning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
A.2 The Deﬁnition Of The Riemann Integral . . . . . . . . . . . . . . . . . . . . . 565
A.3 Basic Properties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
A.4 Iterated Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
A.5 The Change Of Variables Formula
. . . . . . . . . . . . . . . . . . . . . . . . 584
A.6
Some Observations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
Copyright c⃝2005,

10
CONTENTS

Introduction
These are the lecture notes for my section of Math 302. They are pretty much in the order
of the syllabus for the course. You don’t need to read the starred sections and chapters and
subsections. These are there to provide depth in the subject. To quote from the mission
statement of BYU, “ Depth comes when students realize the eﬀect of rigorous, coherent, and
progressively more sophisticated study. Depth helps students distinguish between what is
fundamental and what is only peripheral; it requires focus, provides intense concentration.
...” To see clearly what is peripheral you need to read the fundamental and diﬃcult concepts,
most of which are presented in the starred sections. These are not always easy to read and I
have indicated the most diﬃcult with a picture of a dragon. Some are not much harder than
what is presented in the course. A good example is the one which deﬁnes the derivative. If
you don’t learn this material, you will have trouble understanding many fundamental topics.
Some which come to mind are basic continuum mechanics (The deformation gradient is a
derivative.) and Newton’s method for solving nonlinear systems of equations.(The entire
method involves looking at the derivative and its inverse.)
If you don’t want to learn
anything more than what you will be tested on, then you can omit these sections. This is
up to you. It is your choice.
A word about notation might help. Most of the linear algebra works in any ﬁeld. Exam-
ples are the rational numbers, the integers modulo a prime number, the complex numbers,
or the real numbers. Therefore, I will often write F to denote this ﬁeld. If you don’t like
this, just put in R and you will be ﬁne. This is the main one of interest. However, I at least
want you to realize that everything holds for the complex numbers in addition to the reals.
In many applications this is essential so it does not hurt to begin to realize this. Also, I will
write vectors in terms of bold letters. Thus u will denote a vector. Sometimes people write
something like ⃗u to indicate a vector. However, the bold face is the usual notation so I am
using this in these notes. On the board, I will likely write the other notation. The norm
or length of a vector is often written as ||u|| . I will usually write it as |u| . This is standard
notation also although most books use the double bar notation. The notation I am using
emphasizes that the norm is just like the absolute value which is an important connection
to make. It also seems less cluttered. You need to understand that either notation means
the same thing.
For a more substantial treatment of certain topics, there is a complete calculus book on
my web page. There are signiﬁcant generalizations which unify all the notions of volume
into one beautiful theory. I have not pursued this topic in these notes but it is in the calculus
book. There are other things also, especially all the one variable theory if you need a review.
11

12
INTRODUCTION

Part I
Vectors, Vector Products, Lines
13


15
Outcomes
Vectors in Two and Three Dimensions
A. Evaluate the distance between two points in 3-space.
B. Deﬁne vector and identify examples of vectors.
C. Be able to represent a vector in each of the following ways for n = 2, 3:
(a) as a directed arrow in n-space
(b) as an ordered n-tuple
(c) as a linear combinations of unit coordinate vectors
D. Carry out the vector operations:
(a) addition
(b) scalar multiplication
(c) magnitude (or norm or length)
(d) normalize a vector (ﬁnd the vector of unit length in the direction of a given
vector)
E. Represent the operations of vector addition, scalar multiplication and norm geomet-
rically.
F. Recall, apply and verify the basic properties of vector addition, scalar multiplication
and norm.
G. Model and solve application problems using vectors.
Reading: Multivariable Calculus 1.1, Linear Algebra 1.1
Outcome Mapping:
A. 1,2,4
B. A1,A2
C. 8,9,11,13,14
D. 9,11,12,13
E. 8,10
F. 17,A3,A4
G. A5
Vector Products
A. Evaluate a dot product from the angle formula or the coordinate formula.
B. Interpret the dot product geometrically.
C. Evaluate the following using the dot product:
i. the angle between two vectors.

16
ii. the magnitude of a vector.
iii. the projection of a vector onto another vector.
iv. the component of a vector in the direction of another vector.
v. the work done by a constant force on an object.
D. Evaluate a cross product from the angle formula or the coordinate formula.
E. Interpret the cross product geometrically.
F. Evaluate the following using the cross product:
i. the area of a parallelogram.
ii. the area or a triangle.
iii. physical quantities such as moment of force and angular velocity.
G. Find the volume of a parallelepiped using the scalar triple product.
H. Recall, apply and derive the algebraic properties of the dot and cross products.
Reading: Multivariable Calculus 1.2-3, Linear Algebra 1.2
Outcome Mapping:
A. 1,2bd,3,7
B. 3
C. 2egi
D. 2kmp,7dgh
E. 4
F. 5,15,B5
G. 6,B6
H. 8,17,B1,B2,B3,B4
Lines in Space
A. Represent a line in 3-space by a vector parameterization, a set of scalar parametric
equations or using symmetric form.
B. Find a parameterization of a line given information about
(a) a point of the line and the direction of the line or
(b) two points contained in the line.
(c) the direction cosines of the line.
C. Determine the direction of a line given its parameterization.
D. Find the angle between two lines.
E. Determine a point of intersection between a line and a surface.

17
Reading: Multivariable Calculus 1.5, Linear Algebra 1.3
Outcome Mapping:
A. 3,4
B. 3,4
C. 1
D. 2
E. 11,14

18

Vectors And Points In Rn 5
Sept.
2.1
Rn Ordered n−tuples
The notation, Rn refers to the collection of ordered lists of n real numbers. More precisely,
consider the following deﬁnition.
Deﬁnition 2.1.1 Deﬁne
Rn ≡{(x1, · · ·, xn) : xj ∈R for j = 1, · · ·, n} .
(x1, · · ·, xn) = (y1, · · ·, yn) if and only if for all j = 1, ···, n, xj = yj. When (x1, · · ·, xn) ∈Rn,
it is conventional to denote (x1, · · ·, xn) by the single bold face letter, x. The numbers, xj
are called the coordinates. The set
{(0, · · ·, 0, t, 0, · · ·, 0) : t ∈R }
for t in the ith slot is called the ith coordinate axis coordinate axis, the xi axis for short.
The point 0 ≡(0, · · ·, 0) is called the origin.
Thus (1, 2, 4) ∈R3 and (2, 1, 4) ∈R3 but (1, 2, 4) ̸= (2, 1, 4) because, even though the
same numbers are involved, they don’t match up. In particular, the ﬁrst entries are not
equal.
Why would anyone be interested in such a thing? First consider the case when n = 1.
Then from the deﬁnition, R1 = R. Recall that R is identiﬁed with the points of
a line.
Look at the number line again. Observe that this amounts to identifying a point on this
line with a real number. In other words a real number determines where you are on this
line. Now suppose n = 2 and consider two lines which intersect each other at right angles
as shown in the following picture.
2
6
· (2, 6)
−8
3
·
(−8, 3)
19

20
VECTORS AND POINTS IN RN 5 SEPT.
Notice how you can identify a point shown in the plane with the ordered pair, (2, 6) .
You go to the right a distance of 2 and then up a distance of 6. Similarly, you can identify
another point in the plane with the ordered pair (−8, 3) . Go to the left a distance of 8 and
then up a distance of 3. The reason you go to the left is that there is a −sign on the eight.
From this reasoning, every ordered pair determines a unique point in the plane. Conversely,
taking a point in the plane, you could draw two lines through the point, one vertical and the
other horizontal and determine unique points, x1 on the horizontal line in the above picture
and x2 on the vertical line in the above picture, such that the point of interest is identiﬁed
with the ordered pair, (x1, x2) . In short, points in the plane can be identiﬁed with ordered
pairs similar to the way that points on the real line are identiﬁed with real numbers. Now
suppose n = 3. As just explained, the ﬁrst two coordinates determine a point in a plane.
Letting the third component determine how far up or down you go, depending on whether
this number is positive or negative, this determines a point in space. Thus, (1, 4, −5) would
mean to determine the point in the plane that goes with (1, 4) and then to go below this
plane a distance of 5 to obtain a unique point in space. You see that the ordered triples
correspond to points in space just as the ordered pairs correspond to points in a plane and
single real numbers correspond to points on a line.
You can’t stop here and say that you are only interested in n ≤3. What if you were
interested in the motion of two objects?
You would need three coordinates to describe
where the ﬁrst object is and you would need another three coordinates to describe where
the other object is located. Therefore, you would need to be considering R6. If the two
objects moved around, you would need a time coordinate as well. As another example,
consider a hot object which is cooling and suppose you want the temperature of this object.
How many coordinates would be needed? You would need one for the temperature, three
for the position of the point in the object and one more for the time. Thus you would need
to be considering R5. Many other examples can be given. Sometimes n is very large. This
is often the case in applications to business when they are trying to maximize proﬁt subject
to constraints. It also occurs in numerical analysis when people try to solve hard problems
on a computer.
There are other ways to identify points in space with three numbers but the one presented
is the most basic. In this case, the coordinates are known as Cartesian coordinates after
Descartes1 who invented this idea in the ﬁrst half of the seventeenth century. I will often
not bother to draw a distinction between the point in n dimensional space and its Cartesian
coordinates.
2.2
Vectors And Algebra In Rn
There are two algebraic operations done with points of Rn. One is addition and the other
is multiplication by numbers, called scalars.
Deﬁnition 2.2.1 If x ∈Rn and a is a number, also called a scalar, then ax ∈Rn
is deﬁned by
ax = a (x1, · · ·, xn) ≡(ax1, · · ·, axn) .
(2.1)
This is known as scalar multiplication. If x, y ∈Rn then x + y ∈Rn and is deﬁned by
x + y = (x1, · · ·, xn) + (y1, · · ·, yn)
≡(x1 + y1, · · ·, xn + yn)
(2.2)
1Ren´e Descartes 1596-1650 is often credited with inventing analytic geometry although it seems the ideas
were actually known much earlier. He was interested in many diﬀerent subjects, physiology, chemistry, and
physics being some of them. He also wrote a large book in which he tried to explain the book of Genesis
scientiﬁcally. Descartes ended up dying in Sweden.

2.3.
GEOMETRIC MEANING OF VECTORS
21
An element of Rn, x ≡(x1, · · ·, xn) is often called a vector. The above deﬁnition is known
as vector addition.
With this deﬁnition, the algebraic properties satisfy the conclusions of the following
theorem.
Theorem 2.2.2 For v, w vectors in Rn and α, β scalars, (real numbers), the fol-
lowing hold.
v + w = w + v,
(2.3)
the commutative law of addition,
(v + w) + z = v+ (w + z) ,
(2.4)
the associative law for addition,
v + 0 = v,
(2.5)
the existence of an additive identity,
v+ (−v) = 0,
(2.6)
the existence of an additive inverse, Also
α (v + w) = αv+αw,
(2.7)
(α + β) v =αv+βv,
(2.8)
α (βv) = αβ (v) ,
(2.9)
1v = v.
(2.10)
In the above 0 = (0, · · ·, 0).
You should verify these properties all hold. For example, consider 2.7
α (v + w) = α (v1 + w1, · · ·, vn + wn)
= (α (v1 + w1) , · · ·, α (vn + wn))
= (αv1 + αw1, · · ·, αvn + αwn)
= (αv1, · · ·, αvn) + (αw1, · · ·, αwn)
= αv + αw.
As usual subtraction is deﬁned as x −y ≡x+ (−y) .
2.3
Geometric Meaning Of Vectors
Deﬁnition 2.3.1 Let x = (x1, · · ·, xn) be the coordinates of a point in Rn. Imagine
an arrow with its tail at 0 = (0, · · ·, 0) and its point at x as shown in the following picture
in the case of R3.
¡
¡
¡

3r
(x1, x2, x3) = x
Then this arrow is called the position vector of the point, x.

22
VECTORS AND POINTS IN RN 5 SEPT.
Thus every point determines such a vector and conversely, every such vector (arrow)
which has its tail at 0 determines a point of Rn, namely the point of Rn which coincides
with the point of the vector.
Imagine taking the above position vector and moving it around, always keeping it point-
ing in the same direction as shown in the following picture.
¡
¡
¡

3r
(x1, x2, x3) = x

3

3

3
After moving it around, it is regarded as the same vector because it points in the same
direction and has the same length.2Thus each of the arrows in the above picture is regarded
as the same vector.
The components of this vector are the numbers, x1, · · ·, xn. You
should think of these numbers as directions for obtainng an arrow. Starting at some point,
(a1, a2, · · ·, an) in Rn, you move to the point (a1 + x1, · · ·, an) and from there to the point
(a1 + x1, a2 + x2, a3 · ··, an) and then to (a1 + x1, a2 + x2, a3 + x3, · · ·, an) and continue this
way until you obtain the point (a1 + x1, a2 + x2, · · ·, an + xn) . The arrow having its tail
at (a1, a2, · · ·, an) and its point at (a1 + x1, a2 + x2, · · ·, an + xn) looks just like the arrow
which has its tail at 0 and its point at (x1, · · ·, xn) so it is regarded as the same vector.
2.4
Geometric Meaning Of Vector Addition
It was explained earlier that an element of Rn is an n tuple of numbers and it was also
shown that this can be used to determine a point in three dimensional space in the case
where n = 3 and in two dimensional space, in the case where n = 2. This point was speciﬁed
relative to some coordinate axes.
Consider the case where n = 3 for now. If you draw an arrow from the point in three
dimensional space determined by (0, 0, 0) to the point (a, b, c) with its tail sitting at the
point (0, 0, 0) and its point at the point (a, b, c) , this arrow is called the position vector
of the point determined by u ≡(a, b, c) . One way to get to this point is to start at (0, 0, 0)
and move in the direction of the x1 axis to (a, 0, 0) and then in the direction of the x2 axis
to (a, b, 0) and ﬁnally in the direction of the x3 axis to (a, b, c) . It is evident that the same
arrow (vector) would result if you began at the point, v ≡(d, e, f) , moved in the direction
of the x1 axis to (d + a, e, f) , then in the direction of the x2 axis to (d + a, e + b, f) , and
ﬁnally in the x3 direction to (d + a, e + b, f + c) only this time, the arrow would have its
tail sitting at the point determined by v ≡(d, e, f) and its point at (d + a, e + b, f + c) . It
is said to be the same arrow (vector) because it will point in the same direction and have
the same length. It is like you took an actual arrow, the sort of thing you shoot with a bow,
and moved it from one location to another keeping it pointing the same direction. This
is illustrated in the following picture in which v + u is illustrated. Note the parallelogram
determined in the picture by the vectors u and v.
2I will discuss how to deﬁne length later. For now, it is only necessary to observe that the length should
be deﬁned in such a way that it does not change when such motion takes place.

2.5.
DISTANCE BETWEEN POINTS IN RN
23
¡
¡
¡
¡
¡
¡
¡

u
¤
¤
¤
¤
¤
¤
¤
¤
v
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
u + v
@
@
I
¡
¡
¡
¡

u
x1
x3
x2
Thus the geometric signiﬁcance of (d, e, f) + (a, b, c) = (d + a, e + b, f + c) is this. You
start with the position vector of the point (d, e, f) and at its point, you place the vector
determined by (a, b, c) with its tail at (d, e, f) . Then the point of this last vector will be
(d + a, e + b, f + c) . This is the geometric signiﬁcance of vector addition. Also, as shown
in the picture, u + v is the directed diagonal of the parallelogram determined by the two
vectors u and v. A similar interpretation holds in Rn, n > 3 but I can’t draw a picture in
this case.
Since the convention is that identical arrows pointing in the same direction represent
the same vector, the geometric signiﬁcance of vector addition is as follows in any number of
dimensions.
Procedure 2.4.1 Let u and v be two vectors. Slide v so that the tail of v is on the
point of u. Then draw the arrow which goes from the tail of u to the point of the slid vector,
v. This arrow represents the vector u + v.
-¡
¡
¡
¡

©©©©©©©©
*
u
u + v
v
2.5
Distance Between Points In Rn
How is distance between two points in Rn deﬁned?
Deﬁnition 2.5.1 Let x = (x1, · · ·, xn) and y = (y1, · · ·, yn) be two points in Rn.
Then |x −y| to indicates the distance between these points and is deﬁned as
distance between x and y ≡|x −y| ≡
Ã n
X
k=1
|xk −yk|2
!1/2
.

24
VECTORS AND POINTS IN RN 5 SEPT.
This is called the distance formula. Thus |x| ≡|x −0| . The symbol, B (a, r) is deﬁned
by
B (a, r) ≡{x ∈Rn : |x −a| < r} .
This is called an open ball of radius r centered at a. It means all points in Rn which are
closer to a than r.
First of all note this is a generalization of the notion of distance in R. There the distance
between two points, x and y was given by the absolute value of their diﬀerence. Thus |x −y|
is equal to the distance between these two points on R. Now |x −y| =
³
(x −y)2´1/2
where
the square root is always the positive square root. Thus it is the same formula as the above
deﬁnition except there is only one term in the sum. Geometrically, this is the right way to
deﬁne distance which is seen from the Pythagorean theorem. Often people use two lines
to denote this distance, ||x −y||. However, I want to emphasize this is really just like the
absolute value. Also, the notation I am using is fairly standard.
Consider the following picture in the case that n = 2.
(x1, x2)
(y1, x2)
(y1, y2)
There are two points in the plane whose Cartesian coordinates are (x1, x2) and (y1, y2)
respectively. Then the solid line joining these two points is the hypotenuse of a right triangle
which is half of the rectangle shown in dotted lines. What is its length? Note the lengths
of the sides of this triangle are |y1 −x1| and |y2 −x2| . Therefore, the Pythagorean theorem
implies the length of the hypotenuse equals
³
|y1 −x1|2 + |y2 −x2|2´1/2
=
³
(y1 −x1)2 + (y2 −x2)2´1/2
which is just the formula for the distance given above. In other words, this distance deﬁned
above is the same as the distance of plane geometry in which the Pythagorean theorem
holds.
Now suppose n = 3 and let (x1, x2, x3) and (y1, y2, y3) be two points in R3. Consider the
following picture in which one of the solid lines joins the two points and a dotted line joins

2.5.
DISTANCE BETWEEN POINTS IN RN
25
the points (x1, x2, x3) and (y1, y2, x3) .
(x1, x2, x3)
(y1, x2, x3)
(y1, y2, x3)
(y1, y2, y3)
By the Pythagorean theorem, the length of the dotted line joining (x1, x2, x3) and
(y1, y2, x3) equals
³
(y1 −x1)2 + (y2 −x2)2´1/2
while the length of the line joining (y1, y2, x3) to (y1, y2, y3) is just |y3 −x3| . Therefore, by
the Pythagorean theorem again, the length of the line joining the points (x1, x2, x3) and
(y1, y2, y3) equals
(·³
(y1 −x1)2 + (y2 −x2)2´1/2¸2
+ (y3 −x3)2
)1/2
=
³
(y1 −x1)2 + (y2 −x2)2 + (y3 −x3)2´1/2
,
which is again just the distance formula above.
This completes the argument that the above deﬁnition is reasonable. Of course you
cannot continue drawing pictures in ever higher dimensions but there is no problem with
the formula for distance in any number of dimensions. Here is an example.
Example 2.5.2 Find the distance between the points in R4, a = (1, 2, −4, 6) and b = (2, 3, −1, 0)
Use the distance formula and write
|a −b|2 = (1 −2)2 + (2 −3)2 + (−4 −(−1))2 + (6 −0)2 = 47
Therefore, |a −b| =
√
47.
All this amounts to deﬁning the distance between two points as the length of a straight
line joining these two points. However, there is nothing sacred about using straight lines.
One could deﬁne the distance to be the length of some other sort of line joining these points.
It won’t be done in this book but sometimes this sort of thing is done.
Another convention which is usually followed, especially in R2 and R3 is to denote the
ﬁrst component of a point in R2 by x and the second component by y. In R3 it is customary
to denote the ﬁrst and second components as just described while the third component is
called z.
Example 2.5.3 Describe the points which are at the same distance between (1, 2, 3) and
(0, 1, 2) .

26
VECTORS AND POINTS IN RN 5 SEPT.
Let (x, y, z) be such a point. Then
q
(x −1)2 + (y −2)2 + (z −3)2 =
q
x2 + (y −1)2 + (z −2)2.
Squaring both sides
(x −1)2 + (y −2)2 + (z −3)2 = x2 + (y −1)2 + (z −2)2
and so
x2 −2x + 14 + y2 −4y + z2 −6z = x2 + y2 −2y + 5 + z2 −4z
which implies
−2x + 14 −4y −6z = −2y + 5 −4z
and so
2x + 2y + 2z = −9.
(2.11)
Since these steps are reversible, the set of points which is at the same distance from the two
given points consists of the points, (x, y, z) such that 2.11 holds.
There are certain properties of the distance which are obvious. Two of them which follow
directly from the deﬁnition are
|x −y| = |y −x| ,
|x −y| ≥0 and equals 0 only if y = x.
The third fundamental property of distance is known as the triangle inequality. Recall that
in any triangle the sum of the lengths of two sides is always at least as large as the third
side. I will show you a proof of this pretty soon. This is usually stated as
|x + y| ≤|x| + |y| .
Here is a picture which illustrates the statement of this inequality in terms of geometry.
-

3
¢
¢
¢
¢
x + y
x
y
2.6
Geometric Meaning Of Scalar Multiplication
As discussed earlier, x = (x1, x2, x3) determines a vector. You draw the line from 0 to
x placing the point of the vector on x. What is the length of this vector?
The length
of this vector is deﬁned to equal |x| as in Deﬁnition 2.5.1. Thus the length of x equals
p
x2
1 + x2
2 + x2
3. When you multiply x by a scalar, α, you get (αx1, αx2, αx3) and the length
of this vector is deﬁned as
r³
(αx1)2 + (αx2)2 + (αx3)2´
= |α|
p
x2
1 + x2
2 + x2
3. Thus the
following holds.
|αx| = |α| |x| .
In other words, multiplication by a scalar magniﬁes the length of the vector. What about
the direction? You should convince yourself by drawing a picture that if α is negative, it
causes the resulting vector to point in the opposite direction while if α > 0 it preserves the
direction the vector points.

2.6.
GEOMETRIC MEANING OF SCALAR MULTIPLICATION
27
You can think of vectors as quantities which have direction and magnitude, little arrows.
Thus any two little arrows which have the same length and point in the same direction are
considered to be the same vector even if their tails are at diﬀerent points.
£
£
£
£
£
£
£
£
£
£
£
£
You can always slide such an arrow and place its tail at the origin. If the resulting
point of the vector is (a, b, c) , it is clear the length of the little arrow is
√
a2 + b2 + c2.
Geometrically, the way you add two geometric vectors is to place the tail of one on the
point of the other and then to form the vector which results by starting with the tail of the
ﬁrst and ending with this point as illustrated in the following picture. Also when (a, b, c)
is referred to as a vector, you mean any of the arrows which have the same direction and
magnitude as the position vector of this point. Geometrically, for u = (u1, u2, u3) , αu is any
of the little arrows which have the same direction and magnitude as (αu1, αu2, αu3) .
£
£
£
£
£
£

1

1
¡
¡
¡
¡
¡
¡
¡
¡

u
v
u + v
The following example is art which illustrates these deﬁnitions and conventions.
Exercise 2.6.1 Here is a picture of two vectors, u and v.
u
¡
¡
¡
¡
¡

v
HHHHH
j
Sketch a picture of u + v, u −v, and u+2v.
First here is a picture of u + v. You ﬁrst draw u and then at the point of u you place the
tail of v as shown. Then u + v is the vector which results which is drawn in the following
pretty picture.

28
VECTORS AND POINTS IN RN 5 SEPT.
u
¡
¡
¡
¡
¡

v
HHHHH
j
u + v

:
Next consider u −v. This means u+ (−v) . From the above geometric description of
vector addition, −v is the vector which has the same length but which points in the opposite
direction to v. Here is a picture.
u
¡
¡
¡
¡
¡

−v
H
H
H
H
H
Y
u + (−v)
6
Finally consider the vector u+2v. Here is a picture of this one also.
u
¡
¡
¡
¡
¡

2v
HHHHHHHHHH
j
u + 2v
-
2.7
Unit Vectors
Let v be a vector,
v = (v1, · · ·, vn) .
The direction vector for v is deﬁned as v/ |v| . This vector points in the same direction
as v because it consists of the scalar, 1/ |v| times v. This vector is called a unit vector
because |v/ |v|| = |v| / |v| = 1. That is, it has length equal to 1. The process of dividing a
vector by its length is called normalizing. It provides you with a vector which has unit
length and the same direction as the original vector.
2.8
Lines
To begin with consider the case n = 1, 2. In the case where n = 1, the only line is just
R1 = R. Therefore, if x1 and x2 are two diﬀerent points in R, consider
x = x1 + t (x2 −x1)

2.8.
LINES
29
where t ∈R and the totality of all such points will give R. You see that you can always
solve the above equation for t, showing that every point on R is of this form. Now consider
the plane. Does a similar formula hold? Let (x1, y1) and (x2, y2) be two diﬀerent points
in R2 which are contained in a line, l. Suppose that x1 ̸= x2. Then if (x, y) is an arbitrary
point on l,
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
(x1, y1)
(x2, y2)
(x, y)
Now by similar triangles,
m ≡y2 −y1
x2 −x1
= y −y1
x −x1
and so the point slope form of the line, l, is given as
y −y1 = m (x −x1) = y2 −y1
x2 −x1
(x −x1) .
(2.12)
Now consider points of the form
(x, y) = (x1, y1) + t (x2 −x1, y2 −y1) .
(2.13)
Do these points satisfy the above equation of the line? Is
y1 + t (y2 −y1) −y1 =
µ y2 −y1
x2 −x1
¶
(x1 + t (x2 −x1) −x1)?
Yes, this is so. Both sides equal t (y2 −y1). Conversely, if (x, y) is a point which satisﬁes
the equation, 2.12 does there exist a value of t such that this point is of the form (x1, y1) +
t (x2 −x1, y2 −y1)? If the point satisﬁes 2.12, it is of the form
µ
x, y1 +
µ y2 −y1
x2 −x1
¶
(x −x1)
¶
.
Now let t =
x−x1
x2−x1 so
x = x1 + t (x2 −x1) .
Then in terms of t, the above reduces to
µ
x1 + t (x2 −x1) , y1 +
µ y2 −y1
x2 −x1
¶
t (x2 −x1)
¶
= (x1, y1) + t (x2 −x1, y2 −y1) .
It follows the set of points in R2 obtained from 2.12 and 2.13 are the same. The following
is the deﬁnition of a line in Rn.

30
VECTORS AND POINTS IN RN 5 SEPT.
Deﬁnition 2.8.1 A line in Rn containing the two diﬀerent points, x1 and x2 is the
collection of points of the form
x = x1 + t
¡
x2 −x1¢
where t ∈R. This is known as a parametric equation and the variable t is called the
parameter.
Often t denotes time in applications to Physics. Note this deﬁnition agrees with the
usual notion of a line in two dimensions and so this is consistent with earlier concepts.
From now on, you should think of lines in this way. Forget about the stupid special case in
R2 which you had drilled in to your head in high school. The concept of a line is really very
simple and it holds in any number of dimensions, not just in two dimensions. It is given in
the above deﬁnition.
Lemma 2.8.2 Let a, b ∈Rn with a ̸= 0. Then x = ta + b, t ∈R, is a line.
Proof: Let x1 = b and let x2 −x1 = a so that x2 ̸= x1. Then ta + b = x1 + t
¡
x2 −x1¢
and so x = ta + b is a line containing the two diﬀerent points, x1 and x2. This proves the
lemma.
Deﬁnition 2.8.3 The vector a in the above lemma is called a direction vector for
the line.
Direction vectors are what it is all about, not slope. Slope is ﬁne in two dimensions
but we live in three dimensions. Slope is a trivial and stupid concept designed mainly to
give children something to do in high school. The correct and worthwhile notion is that of
direction vector. This is a new concept. Do not try to ﬁt it in to the stuﬀyou saw earlier.
Do not try to put the new wine in the old bottles, to quote the scripture. It only creates
confusion and you do not need that.
Example 2.8.4 Find the line through (1, 2) and (4, 7) .
A vector equation of this line is (x, y) = (1, 2) + t (3, 5) . Now if you want to get the
equation in the form you are used to seeing in high school,
x = 1 + 3t, y = 2 + 5t
Solving the ﬁrst one for t, you get t = (x −1) /3 and now plugging this in to the second
yields,
y = 2 + 5
µx −1
3
¶
so y −2 = 5
3 (x −1) which is the usual point slope form for this line.
Now that you know about lines, it is possible to give a more analytical description of a
vector as a directed line segment.
Deﬁnition 2.8.5 Let p and q be two points in Rn, p ̸= q. The directed line seg-
ment from p to q, denoted by −→
pq, is deﬁned to be the collection of points,
x = p + t (q −p) , t ∈[0, 1]
with the direction corresponding to increasing t. In the deﬁnition, when t = 0, the point p is
obtained and as t increases other points on this line segment are obtained until when t = 1,
you get the point, q. This is what is meant by saying the direction corresponds to increasing
t.

2.8.
LINES
31
Think of −→
pq as an arrow whose point is on q and whose base is at p as shown in the
following picture.






q
p
This line segment is a part of a line from the above Deﬁnition.
Example 2.8.6 Find a parametric equation for the line through the points (1, 2, 0) and
(2, −4, 6) .
Use the deﬁnition of a line given above to write
(x, y, z) = (1, 2, 0) + t (1, −6, 6) , t ∈R.
The vector (1, −6, 6) is obtained by (2, −4, 6) −(1, 2, 0) as indicated above.
The reason for the word, “a”, rather than the word, “the” is there are inﬁnitely many
diﬀerent parametric equations for the same line. To see this replace t with 3s. Then you
obtain a parametric equation for the same line because the same set of points is obtained.
The diﬀerence is they are obtained from diﬀerent values of the parameter. What happens
is this: The line is a set of points but the parametric description gives more information
than that. It tells how the set of points are obtained. Obviously, there are many ways to
trace out a given set of points and each of these ways corresponds to a diﬀerent parametric
equation for the line.
Example 2.8.7 Find a parametric equation for the line which contains the point (1, 2, 0)
and has direction vector, (1, 2, 1) .
From the above this is just
(x, y, z) = (1, 2, 0) + t (1, 2, 1) , t ∈R.
(2.14)
Sometimes people elect to write a line like the above in the form
x = 1 + t, y = 2 + 2t, z = t, t ∈R.
(2.15)
This is a set of scalar parametric equations which amounts to the same thing as 2.14.
There is one other form for a line which is sometimes considered useful. It is the so called
symmetric form. Consider the line of 2.15. You can solve for the parameter, t to write
t = x −1, t = y −2
2
, t = z.
Therefore,
x −1 = y −2
2
= z.
This is the symmetric form of the line. Later, it will become clear that this expresses the
line as the intersection of two planes but this is not important at this time.

32
VECTORS AND POINTS IN RN 5 SEPT.
Example 2.8.8 Suppose the symmetric form of a line is
x −2
3
= y −1
2
= z + 3.
Find the line in parametric form.
Let t = x−2
3 , t = y−1
2
and t = z + 3. Then solving for x, y, z, you get
x = 3t + 2, y = 2t + 1, z = t −3, t ∈R.
Written in terms of vectors this is
(2, 1, −3) + t (3, 2, 1) = (x, y, z) , t ∈R.
Example 2.8.9 A relation such as x2 + y2/4 + z2/9 = 1 describes something called a level
surface. It consists of the points in Rn, (x, y, z) which satisfy the relation. Now here are
parametric equations for a line: x = t, y = 1 + 2t, z = 1 −t. Find where this line intersects
the above level surface.
This sort of problem is not hard if you don’t panic. The points on the line are of the
form (t, 1 + 2t, 1 −t) where t ∈R. All you have to do is to ﬁnd values of t where this also
satisﬁes the condition for being on the level surface. Thus you need t such that
(t)2 + (1 + 2t)2 /4 + (1 −t)2 /9 = 1.
This is just a quadratic equation. Expanding the left side yields 19
9 t2 + 13
36 + 7
9t and so you
have to solve the quadratic equation,
19
9 t2 + 13
36 + 7
9t = 1
First simplify this to get the equation
76t2 + 28t −23 = 0.
Then the quadratic formula gives two solutions for t, t = −7
38 +
9
38
√
6, −7
38 −
9
38
√
6. Now
you can obtain two points of intersection by plugging these values of t into the equation for
the line. The two points are
µ
−7
38 + 9
38
√
6, 12
19 + 9
19
√
6, 45
38 −9
38
√
6
¶
and
µ
−7
38 −9
38
√
6, 12
19 −9
19
√
6, 45
38 + 9
38
√
6
¶
.
Possibly you would not have guessed these points. You likely would not have found them
by drawing a picture either.
2.9
Vectors And Physics
Suppose you push on something. What is important? There are really two things which
are important, how hard you push and the direction you push. This illustrates the concept
of force.
Also you can see that the concept of a geometric vector is useful for deﬁning
something like force.

2.9.
VECTORS AND PHYSICS
33
Deﬁnition 2.9.1 Force is a vector. The magnitude of this vector is a measure of
how hard it is pushing. It is measured in units such as Newtons or pounds or tons. Its
direction is the direction in which the push is taking place.
Of course this is a little vague and will be left a little vague until the presentation of
Newton’s second law later.
Vectors are used to model force and other physical vectors like velocity. What was just
described would be called a force vector. It has two essential ingredients, its magnitude and
its direction. Geometrically think of vectors as directed line segments or arrows as shown in
the following picture in which all the directed line segments are considered to be the same
vector because they have the same direction, the direction in which the arrows point, and
the same magnitude (length).
£
£
£
£
£
£
£
£
£
£
£
£
Because of this fact that only direction and magnitude are important, it is always possible
to put a vector in a certain particularly simple form. Let −→
pq be a directed line segment or
vector. Then from Deﬁnition 2.8.5 it follows that −→
pq consists of the points of the form
p + t (q −p)
where t ∈[0, 1] . Subtract p from all these points to obtain the directed line segment con-
sisting of the points
0 + t (q −p) , t ∈[0, 1] .
The point in Rn, q −p, will represent the vector.
Geometrically, the arrow, −→
pq, was slid so it points in the same direction and the base is
at the origin, 0. For example, see the following picture.
£
£
£
£
£
£
£
£
£
In this way vectors can be identiﬁed with points of Rn.
Deﬁnition 2.9.2 Let x = (x1, · · ·, xn) ∈Rn.
The position vector of this point is
the vector whose point is at x and whose tail is at the origin, (0, · · ·, 0). If x = (x1, · · ·, xn)
is called a vector, the vector which is meant is this position vector just described. Another
term associated with this is standard position. A vector is in standard position if the tail
is placed at the origin.
It is customary to identify the point in Rn with its position vector.

34
VECTORS AND POINTS IN RN 5 SEPT.
The magnitude of a vector determined by a directed line segment −→
pq is just the distance
between the point p and the point q. By the distance formula this equals
Ã n
X
k=1
(qk −pk)2
!1/2
= |p −q|
and for v any vector in Rn the magnitude of v equals
¡Pn
k=1 v2
k
¢1/2 = |v|.
Example 2.9.3 Consider the vector, v ≡(1, 2, 3) in Rn. Find |v| .
First, the vector is the directed line segment (arrow) which has its base at 0 ≡(0, 0, 0)
and its point at (1, 2, 3) . Therefore,
|v| =
p
12 + 22 + 32 =
√
14.
What is the geometric signiﬁcance of scalar multiplication? As noted earlier, if a vector,
vIf a represents the vector, v in the sense that when it is slid to place its tail at the origin,
the element of Rn at its point is a, what is rv?
|rv| =
Ã n
X
k=1
(rai)2
!1/2
=
Ã n
X
k=1
r2 (ai)2
!1/2
=
¡
r2¢1/2
Ã n
X
k=1
a2
i
!1/2
= |r| |v| .
Thus the magnitude of rv equals |r| times the magnitude of v. If r is positive, then the
vector represented by rv has the same direction as the vector, v because multiplying by the
scalar, r, only has the eﬀect of scaling all the distances. Thus the unit distance along any
coordinate axis now has length r and in this rescaled system the vector is represented by a.
If r < 0 similar considerations apply except in this case all the ai also change sign. From
now on, a will be referred to as a vector instead of an element of Rn representing a vector
as just described. The following picture illustrates the eﬀect of scalar multiplication.
£
££
v
£
£
£
££
2v £
£
£
££
−2v
Note there are n special vectors which point along the coordinate axes. These are
ei ≡(0, · · ·, 0, 1, 0, · · ·, 0)
where the 1 is in the ith slot and there are zeros in all the other spaces. See the picture in
the case of R3.
-
y
e2
6
z
e3
¡
¡
ª
x
e1
¡
¡
¡
¡
¡
¡
¡

2.9.
VECTORS AND PHYSICS
35
The direction of ei is referred to as the ith direction. Given a vector, v = (a1, · · ·, an) ,
aiei is the ith component of the vector.
Thus aiei = (0, · · ·, 0, ai, 0, · · ·, 0) and so this
vector gives something possibly nonzero only in the ith direction. Also, knowledge of the ith
component of the vector is equivalent to knowledge of the vector because it gives the entry
in the ith slot and for v = (a1, · · ·, an) ,
v =
n
X
k=1
aiei.
What does addition of vectors mean physically? Suppose two forces are applied to some
object. Each of these would be represented by a force vector and the two forces acting
together would yield an overall force acting on the object which would also be a force vector
known as the resultant. Suppose the two vectors are a = Pn
k=1 aiei and b = Pn
k=1 biei.
Then the vector, a involves a component in the ith direction, aiei while the component in
the ith direction of b is biei. Then it seems physically reasonable that the resultant vector
should have a component in the ith direction equal to (ai + bi) ei. This is exactly what is
obtained when the vectors, a and b are added.
a + b = (a1 + b1, · · ·, an + bn) .
=
n
X
i=1
(ai + bi) ei.
Thus the addition of vectors according to the rules of addition in Rn which were presented
earlier, yields the appropriate vector which duplicates the cumulative eﬀect of all the vectors
in the sum.
What is the geometric signiﬁcance of vector addition? Suppose u, v are vectors,
u = (u1, · · ·, un) , v = (v1, · · ·, vn)
Then u + v = (u1 + v1, · · ·, un + vn) . How can one obtain this geometrically? Consider the
directed line segment, −→
0u and then, starting at the end of this directed line segment, follow
the directed line segment −−−−−−→
u (u + v) to its end, u + v. In other words, place the vector u in
standard position with its base at the origin and then slide the vector v till its base coincides
with the point of u. The point of this slid vector, determines u + v. To illustrate, see the
following picture
£
£
£
£
£
£

1

1
¡
¡
¡
¡
¡
¡
¡
¡

u
v
u + v
Note the vector u + v is the diagonal of a parallelogram determined from the two vec-
tors u and v and that identifying u + v with the directed diagonal of the parallelogram
determined by the vectors u and v amounts to the same thing as the above procedure.
An item of notation should be mentioned here. In the case of Rn where n ≤3, it is
standard notation to use i for e1, j for e2, and k for e3. Now here are some applications of
vector addition to some problems.
Example 2.9.4 There are three ropes attached to a car and three people pull on these ropes.
The ﬁrst exerts a force of 2i+3j−2k Newtons, the second exerts a force of 3i+5j + k Newtons

36
VECTORS AND POINTS IN RN 5 SEPT.
and the third exerts a force of 5i −j+2k. Newtons. Find the total force in the direction of
i.
To ﬁnd the total force add the vectors as described above.
This gives 10i+7j + k
Newtons. Therefore, the force in the i direction is 10 Newtons.
As mentioned earlier, the Newton is a unit of force like pounds.
Example 2.9.5 An airplane ﬂies North East at 100 miles per hour. Write this as a vector.
A picture of this situation follows.
¡
¡
¡
¡
¡

The vector has length 100. Now using that vector as the hypotenuse of a right triangle
having equal sides, the sides should be each of length 100/
√
2. Therefore, the vector would
be (100/
√
2)i + (100/
√
2)j.
This example also motivates the concept of velocity.
Deﬁnition 2.9.6 The speed of an object is a measure of how fast it is going. It
is measured in units of length per unit time. For example, miles per hour, kilometers per
minute, feet per second. The velocity is a vector having the speed as the magnitude but also
speciﬁng the direction.
Thus the velocity vector in the above example is (100/
√
2)i + (100/
√
2)j.
Example 2.9.7 The velocity of an airplane is 100i+j+k measured in kilometers per hour
and at a certain instant of time its position is (1, 2, 1) . Here imagine a Cartesian coordinate
system in which the third component is altitude and the ﬁrst and second components are
measured on a line from West to East and a line from South to North. Find the position of
this airplane one minute later.
Consider the vector (1, 2, 1) , is the initial position vector of the airplane. As it moves,
the position vector changes. After one minute the airplane has moved in the i direction a
distance of 100 × 1
60 = 5
3 kilometer. In the j direction it has moved
1
60 kilometer during this
same time, while it moves
1
60 kilometer in the k direction. Therefore, the new displacement
vector for the airplane is
(1, 2, 1) +
µ5
3, 1
60, 1
60
¶
=
µ8
3, 121
60 , 121
60
¶
Example 2.9.8 A certain river is one half mile wide with a current ﬂowing at 4 miles per
hour from East to West. A man swims directly toward the opposite shore from the South
bank of the river at a speed of 3 miles per hour. How far down the river does he ﬁnd himself
when he has swam across? How far does he end up swimming?
Consider the following picture.

2.9.
VECTORS AND PHYSICS
37

4
6
3
You should write these vectors in terms of components. The velocity of the swimmer in
still water would be 3j while the velocity of the river would be −4i. Therefore, the velocity
of the swimmer is −4i + 3j. Since the component of velocity in the direction across the river
is 3, it follows the trip takes 1/6 hour or 10 minutes. The speed at which he travels is
√
42 + 32 = 5 miles per hour and so he travels 5 × 1
6 = 5
6 miles. Now to ﬁnd the distance
downstream he ﬁnds himself, note that if x is this distance, x and 1/2 are two legs of a
right triangle whose hypotenuse equals 5/6 miles. Therefore, by the Pythagorean theorem
the distance downstream is
q
(5/6)2 −(1/2)2 = 2
3 miles.

38
VECTORS AND POINTS IN RN 5 SEPT.

Vector Products
3.1
The Dot Product 6 Sept.
Quiz
1. Given two points in R3, (x1, y1, z1) and (x2, y2, z2) , show the point
µx1 + x2
2
, y1 + y2
2
, z1 + z2
2
¶
is on the line between these two points and is the same distance from each of them.
2. Given the two points in R3, (x1, y1, z1) and (x2, y2, z2) , describe the set of all points
which are equidistant from these two points in terms of a simple equation.
3. An airplane heads due north at a speed of 120 miles per hour. The wind is blowing
north east at a speed of 30 miles per hour. Find the resulting speed of the airplane.
3.1.1
Deﬁnition In terms Of Coordinates
There are two ways of multiplying vectors which are of great importance in applications.
The ﬁrst of these is called the dot product, also called the scalar product and sometimes
the inner product.
Deﬁnition 3.1.1 Let a, b be two vectors in Rn deﬁne a · b as
a · b ≡
n
X
k=1
akbk.
With this deﬁnition, there are several important properties satisﬁed by the dot product.
In the statement of these properties, α and β will denote scalars and a, b, c will denote
vectors.
Proposition 3.1.2 The dot product satisﬁes the following properties.
a · b = b · a
(3.1)
a · a ≥0 and equals zero if and only if a = 0
(3.2)
(αa + βb) · c =α (a · c) + β (b · c)
(3.3)
c · (αa + βb) = α (c · a) + β (c · b)
(3.4)
|a|2 = a · a
(3.5)
39

40
VECTOR PRODUCTS
You should verify these properties. Also be sure you understand that 3.4 follows from
the ﬁrst three and is therefore redundant. It is listed here for the sake of convenience.
Example 3.1.3 Find (1, 2, 0, −1) · (0, 1, 2, 3) .
This equals 0 + 2 + 0 + −3 = −1.
Example 3.1.4 Find the magnitude of a = (2, 1, 4, 2) . That is, ﬁnd |a| .
This is
p
(2, 1, 4, 2) · (2, 1, 4, 2) = 5.
3.1.2
The Geometric Meaning Of The Dot Product, The Included
Angle
Given two vectors, a and b, the included angle is the angle between these two vectors which
is less than or equal to 180 degrees. The dot product can be used to determine the included
angle between two vectors. To see how to do this, consider the following picture.
©©©©©
*
PPPPPPPP
q
A
A
A
A
AU
A
A
A
A
AU
b
a
a −b
θ
By the law of cosines,
|a −b|2 = |a|2 + |b|2 −2 |a| |b| cos θ.
Also from the properties of the dot product,
|a −b|2 = (a −b) · (a −b)
= |a|2 + |b|2 −2a · b
and so comparing the above two formulas,
a · b = |a| |b| cos θ.
(3.6)
In words, the dot product of two vectors equals the product of the magnitude of the two
vectors multiplied by the cosine of the included angle. Note this gives a geometric de-
scription of the dot product which does not depend explicitly on the coordinates of the
vectors.
Example 3.1.5 Find the angle between the vectors 2i + j −k and 3i + 4j + k.
The dot product of these two vectors equals 6+4−1 = 9 and the norms are √4 + 1 + 1 =
√
6 and √9 + 16 + 1 =
√
26. Therefore, from 3.6 the cosine of the included angle equals
cos θ =
9
√
26
√
6 = . 720 58
Now the cosine is known, the angle can be determined by solving the equation, cos θ = .
720 58. This will involve using a calculator or a table of trigonometric functions. The answer

3.1.
THE DOT PRODUCT 6 SEPT.
41
is θ = . 766 16 radians or in terms of degrees, θ = . 766 16 × 360
2π = 43. 898◦. Recall how this
last computation is done. Set up a proportion,
x
.76616 = 360
2π because 360◦corresponds to 2π
radians. However, in calculus, you should get used to thinking in terms of radians and not
degrees. This is because all the important calculus formulas are deﬁned in terms of radians.
Example 3.1.6 Find the magnitude of the vector 2i + 3j −k
As discussed above, this has magnitude equal to
p
(2i + 3j −k) · (2i + 3j −k) =
√
4 + 9 + 1 =
√
14.
Example 3.1.7 Let u, v be two vectors whose magnitudes are equal to 3 and 4 respectively
and such that if they are placed in standard position with their tails at the origin, the angle
between u and the positive x axis equals 30◦and the angle between v and the positive x axis
is -30◦. Find u · v.
From the geometric description of the dot product in 3.6
u · v = 3 × 4 × cos (60◦) = 3 × 4 × 1/2 = 6.
Observation 3.1.8 Two vectors are said to be perpendicular or orthogonal if the
included angle is π/2 radians (90◦). You can tell if two nonzero vectors are perpendicular by
simply taking their dot product. If the answer is zero, this means they are are perpendicular
because cos θ = 0.
Example 3.1.9 Determine whether the two vectors, 2i + j −k and 1i + 3j + 5k are perpen-
dicular.
When you take this dot product you get 2 + 3 −5 = 0 and so these two are indeed
perpendicular.
Deﬁnition 3.1.10 When two lines intersect, the angle between the two lines is the
smaller of the two angles determined.
Example 3.1.11 Find the angle between the two lines, (1, 2, 0) + t (1, 2, 3) and (0, 4, −3) +
t (−1, 2, −3) .
These two lines intersect, when t = 0 in the ﬁrst and t = −1 in the second. It is only a
matter of ﬁnding the angle between the direction vectors. One angle determined is given by
cos θ = −6
14 = −3
7 .
(3.7)
We don’t want this angle because it is obtuse. The angle desired is the acute angle given by
cos θ = 3
7.
It is obtained by using replacing one of the direction vectors with −1 times it.

42
VECTOR PRODUCTS
3.1.3
The Cauchy Schwarz Inequality
The dot product satisﬁes a fundamental inequality known as the Cauchy Schwarz in-
equality.
Theorem 3.1.12 The dot product satisﬁes the inequality
|a · b| ≤|a| |b| .
(3.8)
Furthermore equality is obtained if and only if one of a or b is a scalar multiple of the other.
Geometric Proof: From the geometric description of the dot product,
|a · b| = ||a| |b| cos θ| ≤|a| |b|
because cos θ is a number between −1 and 1. Equality occurs if and only if cos θ = ±1. This
corresponds to b being a scalar multiple of a. If cos θ = −1, then b points in the opposite
direction to a and if cos θ = 1 then b points in the same direction as a.
The Cauchy Schwarz inequality is important in many contexts other than vectors in
Rn. What follows is a vastly superior algebraic proof. In general it is this way. Algebraic
methods are nearly always to be preferred to geometric reasoning.
Algebraic Proof: First note that if b = 0 both sides of 3.8 equal zero and so the
inequality holds in this case. Therefore, it will be assumed in what follows that b ̸= 0.
Deﬁne a function of t ∈R
f (t) = (a + tb) · (a + tb) .
Then by 3.2, f (t) ≥0 for all t ∈R. Also from 3.3,3.4,3.1, and 3.5
f (t) = a · (a + tb) + tb · (a + tb)
= a · a + t (a · b) + tb · a + t2b · b
= |a|2 + 2t (a · b) + |b|2 t2.
Now
f (t) = |b|2
Ã
t2 + 2ta · b
|b|2 + |a|2
|b|2
!
= |b|2

t2 + 2ta · b
|b|2 +
Ã
a · b
|b|2
!2
−
Ã
a · b
|b|2
!2
+ |a|2
|b|2


= |b|2


Ã
t + a · b
|b|2
!2
+

|a|2
|b|2 −
Ã
a · b
|b|2
!2


≥0
for all t ∈R. In particular f (t) ≥0 when t = −
³
a · b/ |b|2´
, the value of t which yields
the minimum value of f, which implies
|a|2
|b|2 −
Ã
a · b
|b|2
!2
≥0.
(3.9)
Multiplying both sides by |b|4,
|a|2 |b|2 ≥(a · b)2

3.1.
THE DOT PRODUCT 6 SEPT.
43
which yields 3.8. If equality in the Cauchy Schwarz inequality holds, then the minimum
value of f (t) is zero and so for some t, (a + tb) · (a + tb) = |a + tb| = 0 so that a = −tb.
This proves the theorem.
Another Algebraic Proof: Let f (t) be given as above. Thus as above f (t) ≥0 for
all t ∈R. Thus as above,
f (t) = |a|2 + 2t (a · b) + |b|2 t2 ≥0
The graph of f (t) is a parabola which must open up and cannot cross the t axis. Thus
f (t) = 0 has either one real root or no real roots. Now recall the quadratic formula. This
condition implies the stuﬀunder the square root sign in the quadratic formula must be
nonpositive. Applied to this function of t it says
4 (a · b)2 −4 |a|2 |b|2 ≤0
which is just the Cauchy Schwarz inequality. As before, equality in this inequality implies
f has one real zero. Thus the minimum value of f is 0. This means a + tb = 0 for some t
and so one vector is a multiple of the other. This proves the theorem.
You should note that the algebraic arguments were based only on the properties of
the dot product listed in 3.1 - 3.5. This means that whenever something satisﬁes these
properties, the Cauchy Schwartz inequality holds. There are many other instances of these
properties besides vectors in Rn.
3.1.4
The Triangle Inequality
The Cauchy Schwartz inequality allows a proof of the triangle inequality for distances in
Rn in much the same way as the triangle inequality for the absolute value.
Theorem 3.1.13 (Triangle inequality) For a, b ∈Rn
|a + b| ≤|a| + |b|
(3.10)
and equality holds if and only if one of the vectors is a nonnegative scalar multiple of the
other. Also
||a| −|b|| ≤|a −b|
(3.11)
Proof: By properties of the dot product and the Cauchy Schwartz inequality,
|a + b|2 = (a + b) · (a + b)
= (a · a) + (a · b) + (b · a) + (b · b)
= |a|2 + 2 (a · b) + |b|2
≤|a|2 + 2 |a · b| + |b|2
≤|a|2 + 2 |a| |b| + |b|2
= (|a| + |b|)2 .
Taking square roots of both sides you obtain 3.10.
It remains to consider when equality occurs.
If either vector equals zero, then that
vector equals zero times the other vector and the claim about when equality occurs is
veriﬁed. Therefore, it can be assumed both vectors are nonzero. To get equality in the
second inequality above, Theorem 3.1.12 implies one of the vectors must be a multiple of

44
VECTOR PRODUCTS
the other. Say b = αa. If α < 0 then equality cannot occur in the ﬁrst inequality because
in this case
(a · b) = α |a|2 < 0 < |α| |a|2 = |a · b|
Therefore, α ≥0.
To get the other form of the triangle inequality,
a = a −b + b
so
|a| = |a −b + b|
≤|a −b| + |b| .
Therefore,
|a| −|b| ≤|a −b|
(3.12)
Similarly,
|b| −|a| ≤|b −a| = |a −b| .
(3.13)
It follows from 3.12 and 3.13 that 3.11 holds. This is because ||a| −|b|| equals the left side
of either 3.12 or 3.13 and either way, ||a| −|b|| ≤|a −b| . This proves the theorem.
3.1.5
Direction Cosines Of A Line
Now that the dot product and distance has been deﬁned, it is possible to mention some
archaic terminology which is sometimes found.
Suppose x = a + tb is a vector equation for a line in Rn where, as explained before, the
vector, b is called a direction vector. When b is a unit vector (|b| = 1), the components
of b are called direction cosines. Say b = (b1, · · ·, bn) . Thus, from the deﬁnition of the
dot product, bk = b · ek where ek is the unit vector for the kth coordinate axis consisting of
all zeros except for a 1 in the kth slot. So why in the world do people call these “direction
cosines”? It is because the cosine of the angle, θk between the unit vector b and the vector,
ek is given by
cos θk ≡b · ek
|b| |ek| = b · ek = bk.
There, isn’t that interesting? Now you know why these are called direction cosines. So
what importance does it have? If someone gives you the “direction cosines” of a line, they
are just using jargon to identify the components of a unit vector which serves as a direction
vector for the line.
Example 3.1.14 A line, l in R3 contains the point (1, 2, 3) and letting θk be the angle
between a direction vector and ek,
cos (θ1) =
1
√
5, cos (θ2) =
1
√
5, cos (θ3) = −
√
15
5 ,
ﬁnd a vector equation for the line.
The information in this example is nothing more than a jargon laden statement that a
direction vector for the line is
Ã
1
√
5, 1
√
5, −
√
15
5
!
.

3.1.
THE DOT PRODUCT 6 SEPT.
45
Therefore, a vector equation for the line is
(x, y, z) = (1, 2, 3) + t
Ã
1
√
5, 1
√
5, −
√
15
5
!
.
Of course if you like to wallow in terminology, you could also say parametric equations for
this line are
x = 1 + t 1
√
5, y = 2 + t 1
√
5, z = 3 + t
Ã
−
√
15
5
!
.
Symmetric equations for the line are obtained by solving for the parameter. Thus symmetric
equations for the line are
√
5 (x −1) =
√
5 (y −2) = −5
√
15 (z −3) .
Isn’t this exciting? No doubt there are other monumental trivialities and stupid observations
which could be drawn. The fundamental and signiﬁcant ingredients of a line are the direction
vector and a point on the line. These are the most important things to understand.
3.1.6
Work And Projections
An important application of the dot product is the concept of work. The physical concept
of work does not in any way correspond to the notion of work employed in ordinary con-
versation. For example, if you were to slide a 150 pound weight oﬀa table which is three
feet high and shuﬄe along the ﬂoor for 50 yards, sweating profusely and exerting all your
strength to keep the weight from falling on your feet, keeping the height always three feet
and then deposit this weight on another three foot high table, the physical concept of work
would indicate that the force exerted by your arms did no work during this project even
though the muscles in your hands and arms would likely be very tired. The reason for
such an unusual deﬁnition is that even though your arms exerted considerable force on the
weight, enough to keep it from falling, the direction of motion was at right angles to the
force they exerted. The only part of a force which does work in the sense of physics is the
component of the force in the direction of motion (This is made more precise below.). The
work is deﬁned to be the magnitude of the component of this force times the distance over
which it acts in the case where this component of force points in the direction of motion and
(−1) times the magnitude of this component times the distance in case the force tends to
impede the motion. Thus the work done by a force on an object as the object moves from
one point to another is a measure of the extent to which the force contributes to the motion.
This is illustrated in the following picture in the case where the given force contributes to
the motion.

¡
¡
¡¡


:
C
C
CCO
F
F||
F⊥
q
qp2
p1
θ
In this picture the force, F is applied to an object which moves on the straight line from
p1 to p2. There are two vectors shown, F|| and F⊥and the picture is intended to indicate
that when you add these two vectors you get F while F|| acts in the direction of motion and
F⊥acts perpendicular to the direction of motion. Only F|| contributes to the work done

46
VECTOR PRODUCTS
by F on the object as it moves from p1 to p2. F|| is called the projection of the force
in the direction of motion. From trigonometry, you see the magnitude of F|| should equal
|F| |cos θ| . Thus, since F|| points in the direction of the vector from p1 to p2, the total work
done should equal
|F|
¯¯−−−→
p1p2
¯¯ cos θ = |F| |p2 −p1| cos θ
If the included angle had been obtuse, then the work done by the force, F on the object
would have been negative because in this case, the force tends to impede the motion from
p1 to p2 but in this case, cos θ would also be negative and so it is still the case that the
work done would be given by the above formula. Thus from the geometric description of
the dot product given above, the work equals
|F| |p2 −p1| cos θ = F· (p2−p1) .
This explains the following deﬁnition.
Deﬁnition 3.1.15 Let F be a force acting on an object which moves from the point,
p1 to the point p2. Then the work done on the object by the given force equals F· (p2 −p1) .
The concept of writing a given vector, F in terms of two vectors, one which is parallel
to a given vector, D and the other which is perpendicular can also be explained with no
reliance on trigonometry, completely in terms of the algebraic properties of the dot product.
As before, this is mathematically more signiﬁcant than any approach involving geometry or
trigonometry because it extends to more interesting situations. This is done next.
Theorem 3.1.16 Let F and D be nonzero vectors. Then there exist unique vectors
F|| and F⊥such that
F = F|| + F⊥
(3.14)
where F|| is a scalar multiple of D, also referred to as
projD (F) ,
and F⊥· D = 0. The vector projD (F) is called the projection of F onto D.
Proof: Suppose 3.14 and F|| = αD. Taking the dot product of both sides with D and
using F⊥· D = 0, this yields
F · D = α |D|2
which requires α = F · D/ |D|2 . Thus there can be no more than one vector, F||. It follows
F⊥must equal F −F||. This veriﬁes there can be no more than one choice for both F|| and
F⊥.
Now let
F|| ≡F · D
|D|2 D
and let
F⊥= F −F|| = F−F · D
|D|2 D
Then F|| = α D where α = F·D
|D|2 . It only remains to verify F⊥· D = 0. But
F⊥· D = F · D−F · D
|D|2 D · D
= F · D −F · D = 0.
This proves the theorem.

3.1.
THE DOT PRODUCT 6 SEPT.
47
Deﬁnition 3.1.17 The component of the vector F in the direction, D equals the
scalar
F · D
|D| .
Thus
projD (F) = F · D
|D|
D
|D|.
In words, the projection of F on D equals the component of F in the direction D times the
unit vector in the direction of D.
Example 3.1.18 Let F = (1, 2, 3) . Find the projection of F on D = (2, 1, 1) and also ﬁnd
the component of F in the direction, D.
projD (F)
=
F · D
|D|
D
|D|
=
2 + 2 + 3
√4 + 1 + 1
(2, 1, 1)
√4 + 1 + 1
=
7
6 (2, 1, 1) =
µ7
3, 7
6, 7
6
¶
and the component of F in the direction of D is
2 + 2 + 3
√4 + 1 + 1 = 7
6
√
6.
Example 3.1.19 Let F = 2i+7j−3k Newtons. Find the work done by this force in moving
from the point (1, 2, 3) to the point (−9, −3, 4) along the straight line segment joining these
points where distances are measured in meters.
According to the deﬁnition, this work is
(2i+7j −3k) · (−10i −5j + k) = −20 + (−35) + (−3)
= −58 Newton meters.
Note that if the force had been given in pounds and the distance had been given in feet,
the units on the work would have been foot pounds. In general, work has units equal to
units of a force times units of a length. Instead of writing Newton meter, people write joule
because a joule is by deﬁnition a Newton meter. That word is pronounced “jewel” and it is
the unit of work in the metric system of units. Also be sure you observe that the work done
by the force can be negative as in the above example. In fact, work can be either positive,
negative, or zero. You just have to do the computations to ﬁnd out.
Example 3.1.20 Find proju (v) if u = 2i + 3j −4k and v = i −2j + k.
From the above discussion in Theorem 3.1.16, this is just
1
4 + 9 + 16 (i −2j + k) · (2i + 3j −4k) (2i + 3j −4k)
=
−8
29 (2i + 3j −4k) = −16
29i −24
29j + 32
29k.

48
VECTOR PRODUCTS
Example 3.1.21 Suppose a, and b are vectors and b⊥= b −proja (b) . What is the mag-
nitude of b⊥in terms of the included angle?
|b⊥|2 = (b −proja (b)) · (b −proja (b))
=
Ã
b−b · a
|a|2 a
!
·
Ã
b−b · a
|a|2 a
!
= |b|2 −2(b · a)2
|a|2
+
Ã
b · a
|a|2
!2
|a|2
= |b|2
Ã
1 −(b · a)2
|a|2 |b|2
!
= |b|2 ¡
1 −cos2 θ
¢
= |b|2 sin2 (θ)
where θ is the included angle between a and b which is less than π radians. Therefore,
taking square roots,
|b⊥| = |b| sin θ.
3.2
The Cross Product 7 Sept.
Quiz
1. Find the cosine of the angle between the two vectors (1, 2, 0) and (2, 0, 1) .
2. Suppose u, v are vectors. Show the parallelogram identity.
|u + v|2 + |u −v|2 = 2 |u|2 + 2 |v|2
You must show this in any dimension, not just in two or three dimensions.
3. Find the projection of the vector (1, 2, 3) onto the vector (2, 3, 1) .
4. Given two vectors, u, v in Rn, show using the properties of the dot product alone that
u−u · v
|v|2 v
is perpendicular to v.
5. x = u + tv for t ∈R is a line. Suppose z is a point in Rn. Find a formula for the
distance between z and this line.
3.2.1
The Geometric Description Of The Cross Product In Terms
Of The Included Angle
The cross product is the other way of multiplying two vectors in R3. It is very diﬀerent
from the dot product in many ways. First the geometric meaning is discussed and then
a description in terms of coordinates is given. Both descriptions of the cross product are
important. The geometric description is essential in order to understand the applications
to physics and geometry while the coordinate description is the only way to practically
compute the cross product.

3.2.
THE CROSS PRODUCT 7 SEPT.
49
Deﬁnition 3.2.1 Three vectors, a, b, c form a right handed system if when you
extend the ﬁngers of your right hand along the vector, a and close them in the direction of
b, the thumb points roughly in the direction of c.
For an example of a right handed system of vectors, see the following picture.
X
X
X
X
X
X
y
©
©
©
©

£
£
£
£
£
£
a
b
c
In this picture the vector c points upwards from the plane determined by the other two
vectors. You should consider how a right hand system would diﬀer from a left hand system.
Try using your left hand and you will see that the vector, c would need to point in the
opposite direction as it would for a right hand system.
From now on, the vectors, i, j, k will always form a right handed system. To repeat,
if you extend the ﬁngers of your right hand along i and close them in the direction j, the
thumb points in the direction of k.
The following is the geometric description of the cross product. It gives both the direction
and the magnitude and therefore speciﬁes the vector.
Deﬁnition 3.2.2 Let a and b be two vectors in R3. Then a × b is deﬁned by the
following two rules.
1. |a × b| = |a| |b| sin θ where θ is the included angle.
2. a × b · a = 0, a × b · b = 0, and a, b, a × b forms a right hand system.
Note that |a × b| is the area of the parallelogram spanned by a and b.

3
-
b
a
θ
|b|sin(θ)
©
©

The cross product satisﬁes the following properties.
a × b = −(b × a) , a × a = 0,
(3.15)

50
VECTOR PRODUCTS
For α a scalar,
(αa) ×b = α (a × b) = a× (αb) ,
(3.16)
For a, b, and c vectors, one obtains the distributive laws,
a× (b + c) = a × b + a × c,
(3.17)
(b + c) × a = b × a + c × a.
(3.18)
Formula 3.15 follows immediately from the deﬁnition. The vectors a × b and b × a have
the same magnitude, |a| |b| sin θ, and an application of the right hand rule shows they have
opposite direction. Formula 3.16 is also fairly clear. If α is a nonnegative scalar, the direction
of (αa) ×b is the same as the direction of a × b,α (a × b) and a× (αb) while the magnitude
is just α times the magnitude of a × b which is the same as the magnitude of α (a × b)
and a× (αb) . Using this yields equality in 3.16. In the case where α < 0, everything works
the same way except the vectors are all pointing in the opposite direction and you must
multiply by |α| when comparing their magnitudes. The distributive laws are much harder
to establish but the second follows from the ﬁrst quite easily. Thus, assuming the ﬁrst, and
using 3.15,
(b + c) × a = −a× (b + c)
= −(a × b + a × c)
= b × a + c × a.
A proof of the distributive law is given in a later section for those who are interested.
3.2.2
The Coordinate Description Of The Cross Product
Now from the deﬁnition of the cross product,
i × j = k
j × i = −k
k × i = j
i × k = −j
j × k = i
k × j = −i
With this information, the following gives the coordinate description of the cross product.
Proposition 3.2.3 Let a = a1i+a2j+a3k and b = b1i+b2j+b3k be two vectors. Then
a × b = (a2b3 −a3b2) i+ (a3b1 −a1b3) j+
+ (a1b2 −a2b1) k.
(3.19)
Proof: From the above table and the properties of the cross product listed,
(a1i + a2j + a3k) × (b1i + b2j + b3k) =
a1b2i × j + a1b3i × k + a2b1j × i + a2b3j × k+
+a3b1k × i + a3b2k × j
= a1b2k −a1b3j −a2b1k + a2b3i + a3b1j −a3b2i
= (a2b3 −a3b2) i+ (a3b1 −a1b3) j+ (a1b2 −a2b1) k
(3.20)
This proves the proposition.

3.2.
THE CROSS PRODUCT 7 SEPT.
51
It is probably impossible for most people to remember 3.19.
Fortunately, there is a
somewhat easier way to remember it.
a × b =
¯¯¯¯¯¯
i
j
k
a1
a2
a3
b1
b2
b3
¯¯¯¯¯¯
(3.21)
where you expand the determinant along the top row. This yields
(a2b3 −a3b2) i−(a1b3 −a3b1) j+ (a1b2 −a2b1) k
(3.22)
which is the same as 3.20.
You will see determinants later in the course but some of you have already seen them.
All you need here is how to evaluate 2 × 2 and 3 × 3 determinants.
¯¯¯¯
x
y
z
w
¯¯¯¯ = xw −yz
and
¯¯¯¯¯¯
a
b
c
x
y
z
u
v
w
¯¯¯¯¯¯
= a
¯¯¯¯
y
z
v
w
¯¯¯¯ −b
¯¯¯¯
x
z
u
w
¯¯¯¯ + c
¯¯¯¯
x
y
u
v
¯¯¯¯ .
Some of you are wondering what the rule is. You look at an entry in the top row and cross
out the row and column which contain that entry. If the entry is in the ith column, you
multiply (−1)1+i times the determinant of the 2 × 2 which remains. This is the cofactor.
You take the element in the top row times this cofactor and add all such up.
Example 3.2.4 Find (i −j + 2k) × (3i −2j + k) .
Use 3.21 to compute this.
¯¯¯¯¯¯
i
j
k
1
−1
2
3
−2
1
¯¯¯¯¯¯
=
¯¯¯¯
−1
2
−2
1
¯¯¯¯ i−
¯¯¯¯
1
2
3
1
¯¯¯¯ j+
¯¯¯¯
1
−1
3
−2
¯¯¯¯ k
= 3i + 5j + k.
Example 3.2.5 Find the area of the parallelogram determined by the vectors, (i −j + 2k)
and (3i −2j + k) . These are the same two vectors in Example 3.2.4.
From Example 3.2.4 and the geometric description of the cross product, the area is just
the norm of the vector obtained in Example 3.2.4. Thus the area is √9 + 25 + 1 =
√
35.
Example 3.2.6 Find the area of the triangle determined by (1, 2, 3) , (0, 2, 5) , and (5, 1, 2) .
This triangle is obtained by connecting the three points with lines. Picking (1, 2, 3) as a
starting point, there are two displacement vectors, (−1, 0, 2) and (4, −1, −1) such that the
given vector added to these displacement vectors gives the other two vectors. The area of
the triangle is half the area of the parallelogram determined by (−1, 0, 2) and (4, −1, −1) .
Thus (−1, 0, 2) × (4, −1, −1) = (2, 7, 1) and so the area of the triangle is 1
2
√4 + 49 + 1 =
3
2
√
6.
Observation 3.2.7 In general, if you have three points (vectors) in R3, P, Q, R the
area of the triangle is given by
1
2 |(Q −P) × (R −P)| .

52
VECTOR PRODUCTS
-






r
r
r
P
Q
R
3.2.3
The Box Product, Triple Product
Deﬁnition 3.2.8 A parallelepiped determined by the three vectors, a, b, and c con-
sists of
{ra+sb + tc : r, s, t ∈[0, 1]} .
That is, if you pick three numbers, r, s, and t each in [0, 1] and form ra+sb + tc, then the
collection of all such points is what is meant by the parallelepiped determined by these three
vectors.
The following is a picture of such a thing.
-











3













a
b
c
6
a × b
θ
You notice the area of the base of the parallelepiped, the parallelogram determined by
the vectors, a and b has area equal to |a × b| while the altitude of the parallelepiped is
|c| cos θ where θ is the angle shown in the picture between c and a × b. Therefore, the
volume of this parallelepiped is the area of the base times the altitude which is just
|a × b| |c| cos θ = a × b · c.
This expression is known as the box product and is sometimes written as [a, b, c] . You
should consider what happens if you interchange the b with the c or the a with the c. You
can see geometrically from drawing pictures that this merely introduces a minus sign. In any
case the box product of three vectors always equals either the volume of the parallelepiped
determined by the three vectors or else minus this volume.
Example 3.2.9 Find the volume of the parallelepiped determined by the vectors, i + 2j −
5k, i + 3j −6k,3i + 2j + 3k.
According to the above discussion, pick any two of these, take the cross product and
then take the dot product of this with the third of these vectors. The result will be either

3.2.
THE CROSS PRODUCT 7 SEPT.
53
the desired volume or minus the desired volume.
(i + 2j −5k) × (i + 3j −6k)
=
¯¯¯¯¯¯
i
j
k
1
2
−5
1
3
−6
¯¯¯¯¯¯
=
3i + j + k
Now take the dot product of this vector with the third which yields
(3i + j + k) · (3i + 2j + 3k) = 9 + 2 + 3 = 14.
This shows the volume of this parallelepiped is 14 cubic units.
Observation 3.2.10 Suppose you have three vectors, u = (a, b, c) , v = (d, e, f) , and
w = (g, h, i) . Then u · v × w is given by the following.
u · v × w
=
(a, b, c) ·
¯¯¯¯¯¯
i
j
k
d
e
f
g
h
i
¯¯¯¯¯¯
=
a
¯¯¯¯
e
f
h
i
¯¯¯¯ −b
¯¯¯¯
d
f
g
i
¯¯¯¯ + c
¯¯¯¯
d
e
g
h
¯¯¯¯
=
¯¯¯¯¯¯
a
b
c
d
e
f
g
h
i
¯¯¯¯¯¯
.
The message is that to take the box product, you can simply take the determinant of the
3 × 3 “matrix” as described above.
Example 3.2.11 Find the volume of the parallelepiped determined by the vectors, (1, 2, −1) , (2, 1, 5) ,
and (−3, 1, 2).
As just observed, it suﬃces to take the absolute value of the following determinant.
¯¯¯¯¯¯
1
2
−1
2
1
5
−3
1
2
¯¯¯¯¯¯
= −46
Thus the volume of this parallelepiped is 46.
There is a fundamental observation which comes directly from the geometric deﬁnitions
of the cross product and the dot product.
Lemma 3.2.12 Let a, b, and c be vectors. Then (a × b) ·c = a· (b × c) .
Proof: This follows from observing that either (a × b) ·c and a· (b × c) both give the
volume of the parallelepiped or they both give −1 times the volume.
3.2.4
A Proof Of The Distributive Law For The Cross Product∗
Here is a proof of the distributive law for the cross product. Let x be a vector. From the
above observation,
x · a× (b + c) = (x × a) · (b + c)
= (x × a) · b+ (x × a) · c
= x · a × b + x · a × c
= x· (a × b + a × c) .

54
VECTOR PRODUCTS
Therefore,
x· [a× (b + c) −(a × b + a × c)] = 0
for all x.
In particular, this holds for x = a× (b + c) −(a × b + a × c) showing that
a× (b + c) = a × b + a × c and this proves the distributive law for the cross product.
Example 3.2.13 Find the volume of the parallelepiped determined by the vectors,
(−1, 2, 3) , (2, −1, 1) , (3, −2, 3)
As just explained you only have to ﬁnd the following 3 × 3 determinants.
¯¯¯¯¯¯
−1
2
3
2
−1
1
3
−2
3
¯¯¯¯¯¯
= −1
¯¯¯¯
−1
1
−2
3
¯¯¯¯ −2
¯¯¯¯
2
1
3
3
¯¯¯¯ + 3
¯¯¯¯
2
−1
3
−2
¯¯¯¯ = −8
Now volume is always nonnegative so you take the absolute value of this number. The
volume of the parallelepiped is 8.
3.2.5
Torque, Moment Of A Force
Imagine you are using a wrench to loosen a nut. The idea is to turn the nut by applying a
force to the end of the wrench. If you push or pull the wrench directly toward or away from
the nut, it should be obvious from experience that no progress will be made in turning the
nut. The important thing is the component of force perpendicular to the wrench. It is this
component of force which will cause the nut to turn. For example see the following picture.
©©©©©©©©©
*£
£
£
££
F
£
£
£
££
©©©
F
R
A
A
AK
F⊥
θ
θ
In the picture a force, F is applied at the end of a wrench represented by the position
vector, R and the angle between these two is θ. Then the tendency to turn will be |R| |F⊥| =
|R| |F| sin θ, which you recognize as the magnitude of the cross product of R and F. If there
were just one force acting at one point whose position vector is R, perhaps this would be
suﬃcient, but what if there are numerous forces acting at many diﬀerent points with neither
the position vectors nor the force vectors in the same plane; what then? To keep track of
this sort of thing, deﬁne for each R and F, the torque vector,
τ ≡R × F.
This is also called the moment of the force, F. That way, if there are several forces acting at
several points, the total torque or moment can be obtained by simply adding up the torques
associated with the diﬀerent forces and positions.
Example 3.2.14 Suppose R1 = 2i −j+3k, R2 = i+2j−6k meters and at the points de-
termined by these vectors there are forces, F1 = i −j+2k and F2 = i −5j + k Newtons
respectively. Find the total torque about the origin produced by these forces acting at the
given points.

3.2.
THE CROSS PRODUCT 7 SEPT.
55
It is necessary to take R1 × F1 + R2 × F2. Thus the total torque equals
¯¯¯¯¯¯
i
j
k
2
−1
3
1
−1
2
¯¯¯¯¯¯
+
¯¯¯¯¯¯
i
j
k
1
2
−6
1
−5
1
¯¯¯¯¯¯
= −27i −8j −8k Newton meters
Example 3.2.15 Find if possible a single force vector, F which if applied at the point
i + j + k will produce the same torque as the above two forces acting at the given points.
This is fairly routine. The problem is to ﬁnd F = F1i + F2j + F3k which produces the
above torque vector. Therefore,
¯¯¯¯¯¯
i
j
k
1
1
1
F1
F2
F3
¯¯¯¯¯¯
= −27i −8j −8k
which reduces to (F3 −F2) i+ (F1 −F3) j+ (F2 −F1) k = −27i −8j −8k. This amounts to
solving the system of three equations in three unknowns, F1, F2, and F3,
F3 −F2 = −27
F1 −F3 = −8
F2 −F1 = −8
However, there is no solution to these three equations. (Why?) Therefore no single force
acting at the point i + j + k will produce the given torque.
3.2.6
Angular Velocity
Deﬁnition 3.2.16 In a rotating body, a vector, Ωis called an angular velocity
vector if the velocity of a point having position vector, u relative to the body is given by
Ω× u.
The existence of an angular velocity vector is the key to understanding motion in a
moving system of coordinates. It is used to explain the motion on the surface of the rotating
earth. For example, have you ever wondered why low pressure areas rotate counter clockwise
in the upper hemisphere but clockwise in the lower hemisphere? To quantify these things,
you will need the concept of an angular velocity vector.
Details are presented later for
interesting examples.
Here is a simple example.
Think of a coordinate system ﬁxed in
the rotating body. Thus if you were riding on the rotating body, you would observe this
coordinate system as ﬁxed but it is not ﬁxed.
Example 3.2.17 A wheel rotates counter clockwise about the vector i + j + k at 60 revo-
lutions per minute. This means that if the thumb of your right hand were to point in the
direction of i + j + k your ﬁngers of this hand would wrap in the direction of rotation. Find
the angular velocity vector for this wheel. Assume the unit of distance is meters and the
unit of time is minutes.
Let ω = 60 × 2π = 120π. This is the number of radians per minute corresponding to
60 revolutions per minute. Then the angular velocity vector is 120π
√
3 (i + j + k) . Note this
gives what you would expect in the case the position vector to the point is perpendicular
to i + j + k and at a distance of r. This is because of the geometric description of the cross
product. The magnitude of the vector is r120π meters per minute and corresponds to the
speed and an exercise with the right hand shows the direction is correct also. However, if
this body is rigid, this will work for every other point in it, even those for which the position
vector is not perpendicular to the given vector. A complete analysis of this is given later.

56
VECTOR PRODUCTS
Example 3.2.18 A wheel rotates counter clockwise about the vector i + j + k at 60 revolu-
tions per minute exactly as in Example 3.2.17. Let {u1, u2, u3} denote an orthogonal right
handed system attached to the rotating wheel in which u3 =
1
√
3 (i + j + k) . Thus u1 and u2
depend on time. Find the velocity of the point of the wheel located at the point 2u1+3u2−u3.
Note this point is not ﬁxed in space. It is moving.
Since {u1, u2, u3} is a right handed system like i, j, k, everything applies to this system
in the same way as with i, j, k. Thus the cross product is given by
(au1 + bu2 + cu3) × (du1 + eu2 + fu3)
=
¯¯¯¯¯¯
u1
u2
u3
a
b
c
d
e
f
¯¯¯¯¯¯
Therefore, in terms of the given vectors ui, the angular velocity vector is
120πu3
the velocity of the given point is
¯¯¯¯¯¯
u1
u2
u3
0
0
120π
2
3
−1
¯¯¯¯¯¯
=
−360πu1 + 240πu2
in meters per minute. Note how this gives the answer in terms of these vectors which are
ﬁxed in the body, not in space. Since ui depends on t, this shows the answer in this case
does also. Of course this is right. Just think of what is going on with the wheel rotating.
Those vectors which are ﬁxed in the wheel are moving in space. The velocity of a point in
the wheel should be constantly changing. However, its speed will not change. The speed
will be the magnitude of the velocity and this is
p
(−360πu1 + 240πu2) · (−360πu1 + 240πu2)
which from the properties of the dot product equals
q
(−360π)2 + (240π)2 = 120
√
13π
because the ui are given to be orthogonal.
3.2.7
Center Of Mass∗
The mass of an object is a measure of how much stuﬀthere is in the object. An object has
mass equal to one kilogram, a unit of mass in the metric system, if it would exactly balance
a known one kilogram object when placed on a balance. The known object is one kilogram
by deﬁnition. The mass of an object does not depend on where the balance is used. It
would be one kilogram on the moon as well as on the earth. The weight of an object is
something else. It is the force exerted on the object by gravity and has magnitude gm where
g is a constant called the acceleration of gravity. Thus the weight of a one kilogram object
would be diﬀerent on the moon which has much less gravity, smaller g, than on the earth.
An important idea is that of the center of mass. This is the point at which an object will
balance no matter how it is turned.

3.3.
FURTHER EXPLANATIONS∗
57
Deﬁnition 3.2.19 Let an object consist of p point masses, m1, · · ··, mp with the
position of the kth of these at Rk. The center of mass of this object, R0 is the point satisfying
p
X
k=1
(Rk −R0) × gmku = 0
for all unit vectors, u.
The above deﬁnition indicates that no matter how the object is suspended, the total
torque on it due to gravity is such that no rotation occurs. Using the properties of the cross
product,
Ã p
X
k=1
Rkgmk −R0
p
X
k=1
gmk
!
× u = 0
(3.23)
for any choice of unit vector, u. You should verify that if a × u = 0 for all u, then it must
be the case that a = 0. Then the above formula requires that
p
X
k=1
Rkgmk −R0
p
X
k=1
gmk= 0.
dividing by g, and then by Pp
k=1 mk,
R0 =
Pp
k=1 Rkmk
Pp
k=1 mk
.
(3.24)
This is the formula for the center of mass of a collection of point masses. To consider the
center of mass of a solid consisting of continuously distributed masses, you need the methods
of calculus.
Example 3.2.20 Let m1 = 5, m2 = 6, and m3 = 3 where the masses are in kilograms.
Suppose m1 is located at 2i + 3j + k, m2 is located at i −3j + 2k and m3 is located at
2i −j + 3k. Find the center of mass of these three masses.
Using 3.24
R0 = 5 (2i + 3j + k) + 6 (i −3j + 2k) + 3 (2i −j + 3k)
5 + 6 + 3
= 11
7 i −3
7j + 13
7 k
3.3
Further Explanations∗
3.3.1
The Distributive Law For The Cross Product∗
This section gives a proof for 3.17 which is independent of volume considerations. It is
included here for the interested student. If you are satisﬁed with taking the distributive law
on faith or are happy with the other argument given, it is not necessary to read this section.
The proof given here is quite clever and follows the one given in [7]. The other approach
based on areas is found in [23] and is discussed brieﬂy earlier.
Lemma 3.3.1 Let b and c be two vectors. Then b × c = b × c⊥where c|| + c⊥= c and
c⊥· b = 0.

58
VECTOR PRODUCTS
Proof: Consider the following picture.
-
b
¡
¡¡

θ
c
6
c⊥
Now c⊥= c −c· b
|b|
b
|b| and so c⊥is in the plane determined by c and b. Therefore, from
the geometric deﬁnition of the cross product, b × c and b × c⊥have the same direction.
Now, referring to the picture,
|b × c⊥| = |b| |c⊥|
= |b| |c| sin θ
= |b × c| .
Therefore, b × c and b × c⊥also have the same magnitude and so they are the same vector.
With this, the proof of the distributive law is in the following theorem.
Theorem 3.3.2 Let a, b, and c be vectors in R3. Then
a× (b + c) = a × b + a × c
(3.25)
Proof: Suppose ﬁrst that a · b = a · c = 0. Now imagine a is a vector coming out of the
page and let b, c and b + c be as shown in the following picture.
-
b
¡
¡¡

c

1
b + c
H
H
6
a × b
B
B
B
B
B
B
B
B
B
B
B
B
B
B
BM
a × (b + c)
@
@
@
@
@
I
a × c
Then a × b, a× (b + c) , and a × c are each vectors in the same plane, perpendicular to a
as shown. Thus a × c · c = 0, a× (b + c) · (b + c) = 0, and a × b · b = 0. This implies that
to get a × b you move counterclockwise through an angle of π/2 radians from the vector, b.
Similar relationships exist between the vectors a× (b + c) and b + c and the vectors a × c
and c. Thus the angle between a × b and a× (b + c) is the same as the angle between b + c
and b and the angle between a × c and a× (b + c) is the same as the angle between c and
b + c. In addition to this, since a is perpendicular to these vectors,
|a × b| = |a| |b| , |a× (b + c)| = |a| |b + c| , and
|a × c| = |a| |c| .

3.3.
FURTHER EXPLANATIONS∗
59
Therefore,
|a× (b + c)|
|b + c|
= |a × c|
|c|
= |a × b|
|b|
= |a|
and so
|a× (b + c)|
|a × c|
= |b + c|
|c|
, |a× (b + c)|
|a × b|
= |b + c|
|b|
showing the triangles making up the parallelogram on the right and the four sided ﬁgure on
the left in the above picture are similar. It follows the four sided ﬁgure on the left is in fact
a parallelogram and this implies the diagonal is the vector sum of the vectors on the sides,
yielding 3.25.
Now suppose it is not necessarily the case that a · b = a · c = 0. Then write b = b|| +b⊥
where b⊥· a = 0. Similarly c = c|| + c⊥. By the above lemma and what was just shown,
a× (b + c) = a× (b + c)⊥
= a× (b⊥+ c⊥)
= a × b⊥+ a × c⊥
= a × b + a × c.
This proves the theorem.
3.3.2
Vector Identities And Notation∗
To begin with consider u × (v × w) and it is desired to simplify this quantity. It turns out
this is an important quantity which comes up in many diﬀerent contexts. Let u = (u1, u2, u3)
and let v and w be deﬁned similarly.
v × w
=
¯¯¯¯¯¯
i
j
k
v1
v2
v3
w1
w2
w3
¯¯¯¯¯¯
=
(v2w3 −v3w2) i+ (w1v3 −v1w3) j+ (v1w2 −v2w1) k
Next consider u× (v × w) which is given by
u× (v × w) =
¯¯¯¯¯¯
i
j
k
u1
u2
u3
(v2w3 −v3w2)
(w1v3 −v1w3)
(v1w2 −v2w1)
¯¯¯¯¯¯
.
When you multiply this out, you get
i (v1u2w2 + u3v1w3 −w1u2v2 −u3w1v3) + j (v2u1w1 + v2w3u3 −w2u1v1 −u3w2v3)
+k (u1w1v3 + v3w2u2 −u1v1w3 −v2w3u2)
and if you are clever, you see right away that
(iv1 + jv2 + kv3) (u1w1 + u2w2 + u3w3) −(iw1 + jw2 + kw3) (u1v1 + u2v2 + u3v3) .
Thus
u× (v × w) = v (u · w) −w (u · v) .
(3.26)
A related formula is
(u × v) × w
=
−[w× (u × v)]
=
−[u (w · v) −v (w · u)]
=
v (w · u) −u (w · v) .
(3.27)

60
VECTOR PRODUCTS
This derivation is simply wretched and it does nothing for other identities which may arise
in applications.
Actually, the above two formulas, 3.26 and 3.27 are suﬃcient for most
applications if you are creative in using them, but there is another way. This other way
allows you to discover such vector identities as the above without any creativity or any
cleverness.
Therefore, it is far superior to the above nasty computation.
It is a vector
identity discovering machine and it is this which is the main topic in what follows.
There are two special symbols, δij and εijk which are very useful in dealing with vector
identities. To begin with, here is the deﬁnition of these symbols.
Deﬁnition 3.3.3 The symbol, δij, called the Kroneker delta symbol is deﬁned as
follows.
δij ≡
½
1 if i = j
0 if i ̸= j
.
With the Kroneker symbol, i and j can equal any integer in {1, 2, · · ·, n} for any n ∈N.
Deﬁnition 3.3.4 For i, j, and k integers in the set, {1, 2, 3} , εijk is deﬁned as
follows.
εijk ≡



1 if (i, j, k) = (1, 2, 3) , (2, 3, 1) , or (3, 1, 2)
−1 if (i, j, k) = (2, 1, 3) , (1, 3, 2) , or (3, 2, 1)
0 if there are any repeated integers
.
The subscripts ijk and ij in the above are called indices. A single one is called an index.
This symbol, εijk is also called the permutation symbol.
The way to think of εijk is that ε123 = 1 and if you switch any two of the numbers in
the list i, j, k, it changes the sign. Thus εijk = −εjik and εijk = −εkji etc. You should
check that this rule reduces to the above deﬁnition. For example, it immediately implies
that if there is a repeated index, the answer is zero. This follows because εiij = −εiij and
so εiij = 0.
It is useful to use the Einstein summation convention when dealing with these symbols.
Simply stated, the convention is that you sum over the repeated index. Thus aibi means
P
i aibi. Also, δijxj means P
j δijxj = xi. When you use this convention, there is one very
important thing to never forget. It is this: Never have an index be repeated more than once.
Thus aibi is all right but aiibi is not. The reason for this is that you end up getting confused
about what is meant. If you want to write P
i aibici it is best to simply use the summation
notation. There is a very important reduction identity connecting these two symbols.
Lemma 3.3.5 The following holds.
εijkεirs = (δjrδks −δkrδjs) .
Proof: If {j, k} ̸= {r, s} then every term in the sum on the left must have either εijk
or εirs contains a repeated index. Therefore, the left side equals zero. The right side also
equals zero in this case. To see this, note that if the two sets are not equal, then there is
one of the indices in one of the sets which is not in the other set. For example, it could be
that j is not equal to either r or s. Then the right side equals zero.
Therefore, it can be assumed {j, k} = {r, s} . If i = r and j = s for s ̸= r, then there
is exactly one term in the sum on the left and it equals 1. The right also reduces to 1 in
this case. If i = s and j = r, there is exactly one term in the sum on the left which is
nonzero and it must equal -1. The right side also reduces to -1 in this case. If there is
a repeated index in {j, k} , then every term in the sum on the left equals zero. The right
also reduces to zero in this case because then j = k = r = s and so the right side becomes
(1) (1) −(−1) (−1) = 0.

3.3.
FURTHER EXPLANATIONS∗
61
Proposition 3.3.6 Let u, v be vectors in Rn where the Cartesian coordinates of u are
(u1, · · ·, un) and the Cartesian coordinates of v are (v1, · · ·, vn). Then u · v = uivi. If u, v
are vectors in R3, then
(u × v)i = εijkujvk.
Also, δikak = ai.
Proof: The ﬁrst claim is obvious from the deﬁnition of the dot product. The second is
veriﬁed by simply checking it works. For example,
u × v ≡
¯¯¯¯¯¯
i
j
k
u1
u2
u3
v1
v2
v3
¯¯¯¯¯¯
and so
(u × v)1 = (u2v3 −u3v2) .
From the above formula in the proposition,
ε1jkujvk ≡u2v3 −u3v2,
the same thing. The cases for (u × v)2 and (u × v)3 are veriﬁed similarly. The last claim
follows directly from the deﬁnition.
With this notation, you can easily discover vector identities and simplify expressions
which involve the cross product.
Example 3.3.7 Discover a formula which simpliﬁes (u × v) ×w.
From the above reduction formula,
((u × v) ×w)i
=
εijk (u × v)j wk
=
εijkεjrsurvswk
=
−εjikεjrsurvswk
=
−(δirδks −δisδkr) urvswk
=
−(uivkwk −ukviwk)
=
u · wvi −v · wui
=
((u · w) v −(v · w) u)i .
Since this holds for all i, it follows that
(u × v) ×w = (u · w) v −(v · w) u.
3.3.3
Exercises With Answers
1. Draw the vector u = (1, −2) , the vector v = (2, 3) , and the vector (1, −2) + (2, 3) =
u + v.
A
A
A
A
A
AU











1
u
v
u + v

62
VECTOR PRODUCTS
2. Let u = (1, 2, −5) , v = (3, −1, 2) and w = (2, 0, 3) Find the following.
(a) (2u + v) · w
This is (2 (1, 2, −5) + (3, −1, 2)) · (2, 0, 3) = −14. Here is why.
2 (1, 2, −5) + (3, −1, 2) = (5, 3, −8)
and
(5, 3, −8) · (2, 0, 3) = −14
(b) (u−3v) · w
This is ((1, 2, −5) −3 (3, −1, 2)) · (2, 0, 3) = −49
3. Find the cosine of the angle between the two vectors, (1, 2, 5) and (3, −2, 1) .
cos θ =
(1,2,5)·(3,−2,1)
√1+4+25√9+4+1 =
1
105
√
30
√
14 = . 195 18.
4. Here are two vectors. (1, 2, 3) and (3, 2, 1) . Find a vector which is perpendicular to
both of these vectors.
One way to do this is to take the cross product of the two vectors. (1, 2, 3)×(3, 2, 1) =
(−4, 8, −4) . A vector perpendicular to both of these vectors is (−1, 2, 1) . Note nothing
is changed as far as being perpendicular is concerned by division by 4.
5. Given two vectors in Rn, u, v show that
u · v =1
4
³
|u + v|2 −|u −v|2´
.
This is really easy if you remember the axioms for the dot product. Otherwise it is
very troublesome. Start with the right side.
1
4
³
|u + v|2 −|u −v|2´
=
1
4 ((u + v) · (u + v) −(u −v) · (u −v))
=
1
4 [u · u+ + v · v−{u · u + v · v−2u · v}]
=
1
4 [2u · v−(−2u · v)] = u · v.
6. If |u| = 3, |v| = 4, and u · v = 5, ﬁnd |u + v| .
This is easy if you know the properties of the dot product. Otherwise it is trouble.
|u + v|2
=
|u|2 + |v|2 + 2u · v
=
9 + 16 + 50 = 75.
Therefore, |u + v| = 5
√
5.
7. Find vectors in R3, u, v such that u · v =6 and |u| = 2 while |v| = 3. You see that
equality holds in the Cauchy Schwarz inequality and so one of these vectors must
be a multiple of the other. It must be a positive multiple of the other because the
dot product is positive which implies the angle between the vectors is 0 and not π.
Let u = (0, 0, 2) , v = (0, 0, 3) . This appears to work.
You should ﬁnd some other
examples. What if u =
¡√
2/2,
√
2/2,
√
3
¢
. In this case |u| = 2 also. Can you ﬁnd v
such that the above will hold?

3.3.
FURTHER EXPLANATIONS∗
63
8. The projection of u onto v, denoted by projv (u) is given by the formula
projv (u) = u · v
|v|2 v
Show u−projv (u) is perpendicular to v. Also show projv (au + bw) = a (projv (u))+
b (projv (w)) .
This is another of those things which is very easy if you know the properties of the
dot product but lots of trouble if you don’t. Of course you can persist in not learning
these things if you want. It is up to you.
v· (u−projv (u)) = v·
Ã
u−u · v
|v|2 v
!
= v · u−u · v
|v|2 v · v = v · u −u · v = 0
This does it. The dot product equals zero and so the two vectors are perpendicular.
As to the other claim,
(au + bw) ·v
|v|2
v
=
au · v
|v|2 v + bw · v
|v|2 v
≡
a (projv (u)) + b (projv (w)) .
Now that was real easy wasn’t it. Note I never said anything about u, v being lists of
numbers. I just used the properties of the dot product.
9. Find the angle between the vectors 3i −j −k and i + 4j + 2k.
cos θ =
3−4−2
√9+1+1√1+16+4 = −. 197 39. Therefore, you have to solve the equation cos θ =
−. 197 39, Solution is : θ = 1. 769 5 radians. You need to use a calculator or table to
solve this.
10. Find proju (v) where v = (1, 3, −2) and u = (1, 2, 3) .
Remember to ﬁnd this you take v·u
u·uu. Thus the answer is
1
14 (1, 2, 3) .
11. If F is a force and D is a vector, show projD (F) = (|F| cos θ) u where u is the unit
vector in the direction of D, u = D/ |D| and θ is the included angle between the two
vectors, F and D. |F| cos θ is sometimes called the component of the force, F in the
direction, D.
projD (F) = F·D
D·DD = |F| |D| cos θ
1
|D|2 D = |F| cos θ D
|D|.
12. A boy drags a sled for 100 feet along the ground by pulling on a rope which is 40
degrees from the horizontal with a force of 10 pounds. How much work does this force
do?
The component of force is 10 cos
¡ 40
180π
¢
and it acts for 100 feet so the work done is
10 cos
µ 40
180π
¶
× 100 = 766. 04
13. If a, b, and c are vectors. Show that (b + c)⊥= b⊥+ c⊥where b⊥= b−proja (b) .
14. Find (1, 0, 3, 4) · (2, 7, 1, 3) . (1, 0, 3, 4) · (2, 7, 1, 3) = 17.

64
VECTOR PRODUCTS
15. Prove from the axioms of the dot product the parallelogram identity, |a + b|2 +
|a −b|2 = 2 |a|2 + 2 |b|2 .
Use the properties of the dot product and the deﬁnition of the norm in terms of the
dot product. Thus the left side is
a · a + b · b + 2 (a · b) + a · a + b · b −2a · b =2 |a|2 + 2 |b|2 .
16. Find all vectors, (x, y) which are perpendicular to (1, 2) .
You need x + 2y = 0 so x = −2y and you can write all desired vectors in the form
(−2y, y) : y ∈R.
17. Find the line through (1, 2, 1) and (2, 0, 3) .
First get a direction vector which in this case is (1, −2, 2) . Then the equation of the
line is
(x, y, z) = (1, 2, 1) + t (1, −2, 2) = (1 + t, 2 −2t, 1 + 2t) .
Thus a parametric form for this line is x = 1 + t, y = 2 −2t, z = 1 + 2t and a vector
equation for this line is


x
y
z

=


1
2
1

+ t


1
−2
2


if you want to write the vectors as column vectors.
18. In R2, the equation of a line is given as 2x + 3y = 6. Find a vector equation of this
line.
One way to do it is to get a couple of points on the line and then do as in the previous
problem. Two points on this line are (0, 2) and (3, 0) . Then a direction vector for the
line is (−3, 2) and so a vector equation of the line is
(x, y) = (0, 2) + t (−3, 2) .
Written parametrically, this would be x = −3t, y = 2 + 2t.
19. Suppose you have the vector equation for a line joining the two points, p, q. This is
x = p + t (q −p)
Note this works because when t = 0 the right side is p and when t = 1, the right side
is q. Now ﬁnd the point which is 1/3 of the way between p and q.
This point would be obtained by letting t = 1/3. Thus the point is
x1/3 = 2
3p+1
3q.
Does it work?
¯¯x1/3 −p
¯¯ =
¯¯¯¯−1
3p+1
3q
¯¯¯¯ = 1
3 |q −p| .
Seems to work just ﬁne. I suppose you could also ﬁnd points which are 1/5 of the way
between p and q also.

3.3.
FURTHER EXPLANATIONS∗
65
20. The wind blows from West to East at a speed of 30 kilometers per hour and an airplane
which travels at 300 Kilometers per hour in still air is heading North West. What is
the velocity of the airplane relative to the ground? What is the component of this
velocity in the direction North?
Let the positive y axis point in the direction North and let the positive x axis point in
the direction East. The velocity of the wind is 30i. The plane moves in the direction
i + j. A unit vector in this direction is
1
√
2 (i + j) . Therefore, the velocity of the plane
relative to the ground is 30i+ 300
√
2 (i + j) = 150
√
2j +
¡
30 + 150
√
2
¢
i. The component
of velocity in the direction North is 150
√
2.
21. In the situation of Problem 20 how many degrees to the West of North should the
airplane head in order to ﬂy exactly North. What will be the speed of the airplane
relative to the ground?
In this case the unit vector will be −sin (θ) i + cos (θ) j. Therefore, the velocity of the
plane will be
300 (−sin (θ) i + cos (θ) j)
and this is supposed to satisfy
300 (−sin (θ) i + cos (θ) j) + 30i = 0i+?j.
Therefore, you need to have sin θ = 1/10, which means θ = . 100 17 radians. Therefore,
the degrees should be .1×180
π
= 5. 729 6 degrees. In this case the velocity vector of the
plane relative to the ground is 300
³ √
99
10
´
j.
22. In the situation of 21 suppose the airplane uses 34 gallons of fuel every hour at that
air speed and that it needs to ﬂy North a distance of 600 miles. Will the airplane have
enough fuel to arrive at its destination given that it has 63 gallons of fuel?
The airplane needs to ﬂy 600 miles at a speed of 300
³ √
99
10
´
. Therefore, it takes
600
³
300
³ √
99
10
´´ = 2. 010 1 hours to get there. Therefore, the plane will need to use about
68 gallons of gas. It won’t make it.
23. A certain river is one half mile wide with a current ﬂowing at 3 miles per hour from
East to West. A man swims directly toward the opposite shore from the South bank
of the river at a speed of 2 miles per hour. How far down the river does he ﬁnd himself
when he has swam across? How far does he end up swimming?
The velocity of the man relative to the earth is then −3i + 2j. Since the component
of j equals 2 it follows he takes 1/8 of an hour to get across. Durring this time he
is swept downstream at the rate of 3 miles per hour and so he ends up 3/8 of a mile
down stream. He has gone
q¡ 3
8
¢2 +
¡ 1
2
¢2 = . 625 miles in all.
24. Three forces are applied to a point which does not move.
Two of the forces are
2i −j + 3k Newtons and i −3j −2k Newtons.
Find the third force.
Call it ai + bj + ck Then you need a + 2 + 1 = 0, b −1 −3 = 0, and c + 3 −2 = 0.
Therefore, the force is −3i + 4j −k.
25. If you only assume 3.23 holds for u = i, j, k, show that this implies 3.23 holds for all
unit vectors, u.
Suppose than that (Pp
k=1 Rkgmk −R0
Pp
k=1 gmk)×u = 0 for u = i, j, k. Then if u is
an arbitrary unit vector, u must be of the form ai+bj+ck. Now from the distributive

66
VECTOR PRODUCTS
property of the cross product and letting w = (Pp
k=1 Rkgmk −R0
Pp
k=1 gmk), this
says
(Pp
k=1 Rkgmk −R0
Pp
k=1 gmk) × u
= w × (ai + bj + ck)
= aw × i + bw × j + cw × k
= 0 + 0 + 0 = 0.
26. Let m1 = 4, m2 = 3, and m3 = 1 where the masses are in kilograms and the distance
is in meters. Suppose m1 is located at 2i −j + k, m2 is located at 2i −3j + k and m3
is located at 2i + j + 3k. Find the center of mass of these three masses.
Let the center of mass be located at ai + bj + ck. Then (4 + 3 + 1) (ai + bj + ck) =
4 (2i −j + k)+3 (2i −3j + k)+1 (2i + j + 3k) = 16i−12j+10k. Therefore, a = 2, b =
−3
2 and c = 5
4. The center of mass is then 2i −3
2j + 5
4k.
27. Find the angular velocity vector of a rigid body which rotates counter clockwise about
the vector i −j + k at 20 revolutions per minute. Assume distance is measured in
meters.
The angular velocity is 20 × 2π = 40π. Then Ω= 40π 1
√
3 (i −j + k) .
28. Find the area of the triangle determined by the three points, (1, 2, 3) , (1, 2, 6) and
(−3, 2, 1) .
The three points determine two displacement vectors from the point (1, 2, 3) , (0, 0, 3)
and (−4, 0, −2) . To ﬁnd the area of the parallelogram determined by these two dis-
placement vectors, you simply take the norm of their cross product. To ﬁnd the area
of the triangle, you take one half of that. Thus the area is
(1/2) |(0, 0, 3) × (−4, 0, −2)| = 1
2 |(0, −12, 0)| = 6.
29. Find the area of the parallelogram determined by the vectors, (1, 0, 3) and (4, −2, 1) .
|(1, 0, 3) × (4, −2, 1)| = |(6, 11, −2)| = √26 + 121 + 4 =
√
151.
30. Find the volume of the parallelepiped determined by the vectors, i −7j −5k, i + 2j −
6k,3i −3j + k.
Remember you just need to take the absolute value of the determinant having the
given vectors as rows. Thus the volume is the absolute value of
¯¯¯¯¯¯
1
−7
−5
1
2
−6
3
−3
1
¯¯¯¯¯¯
= 162
31. Suppose a, b, and c are three vectors whose components are all integers. Can you
conclude the volume of the parallelepiped determined from these three vectors will
always be an integer?
Hint: Consider what happens when you take the determinant of a matrix which has
all integers.
32. Using the notion of the box product yielding either plus or minus the volume of the
parallelepiped determined by the given three vectors, show that
(a × b) ·c = a· (b × c)

3.3.
FURTHER EXPLANATIONS∗
67
In other words, the dot and the cross can be switched as long as the order of the
vectors remains the same. Hint: There are two ways to do this, by the coordinate
description of the dot and cross product and by geometric reasoning. It is best if you
use the geometric reasoning. Here is a picture which might help.
-¡
¡
¡
¡
¡
¡
¡
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¡
¡
¡
¡
¡
¡
¡
¤
¤
¤
¤
¤
¤
¤
¡
¡
¡
¡
¡
¡
¡
a
-

b
c
¤¤
6
a × b
XXXXXXXXXXX
z
b × c
In this picture there is an angle between a × b and c. Call it θ. Now if you take
|a × b| |c| cos θ this gives the area of the base of the parallelepiped determined by a
and b times the altitude of the parallelepiped, |c| cos θ. This is what is meant by the
volume of the parallelepiped. It also equals a × b · c by the geometric description of
the dot product. Similarly, there is an angle between b × c and a. Call it α. Then
if you take |b × c| |a| cos α this would equal the area of the face determined by the
vectors b and c times the altitude measured from this face, |a| cos α. Thus this also is
the volume of the parallelepiped. and it equals a · b × c. The picture is not completely
representative. If you switch the labels of two of these vectors, say b and c, explain
why it is still the case that a · b × c = a × b · c. You should draw a similar picture
and explain why in this case you get −1 times the volume of the parallelepiped.
33. Discover a vector identity for(u × v) ×w.
((u × v) ×w)i
=
εijk (u × v)j wk = εijkεjrsurvswk = (δisδkr −δirδks) urvswk
=
ukwkvi −uivkwk = (u · w) vi −(v · w) ui.
Therefore, (u × v) ×w = (u · w) v −(v · w) u.
34. Discover a vector identity for (u × v) · (z × w) .
Start with εijkujvkεirszrws and then go to work on it using the reduction identities
for the permutation symbol.
35. Discover a vector identity for (u × v) × (z × w) in terms of box products.
You will save time if you use the identity for (u × v) ×w or u× (v × w) .

68
VECTOR PRODUCTS

Part II
Planes And Systems Of
Equations
69


71
Outcomes
Planes
A. Find the equation of a plane in 3-space given a point and a normal vector, three points,
a sketch of a plane or a geometric description of the plane.
B. Determine a normal vector and the intercepts of a given plane.
C. Sketch the graph of a plane given its equation.
D. Determine the angle between two planes.
E. Find the equation of a plane determined by lines.
Reading: Multivariable Calculus 1.3, Linear Algebra 1.3
Outcome Mapping:
A. 1,3
B. 2,4
C. 4
D. 2
E. Problem 9 in Section 1.5 of Multivariable Calculus
Systems of Linear Equations
A. Deﬁne linear equation and system of linear equations. Deﬁne solution and solution
set for both an linear equation and a system of linear equations.
B. Relate the following types of solution sets of a system of two or three variables to the
intersections of lines in a plane or the intersection of planes in three space:
(a) a unique solution.
(b) inﬁnitely many solutions.
(c) no solution.
C. Represent a linear system as an augmented matrix and vice versa.
D. Transform a system to a triangular pattern and then apply back substitution to solve
the linear system.
E. Represent the solution set to a linear system using parametric equations.
Reading: Linear Algebra 2.1
Outcome Mapping:
A. 1-6, 7-10
B. 15-18
C. 27-30, 31-32
D. 19-24, 25-26, 33-38

72
E. 11-14, 39-40
Direct Methods for Solving Linear Systems
A. Identify matrices that are in row echelon form and reduced row echelon form.
B. Determine whether a system of linear equations has no solution, a unique solution or
an inﬁnite number of solutions from its echelon form.
C. Apply elementary row operations to transform systems of linear equations.
D. Solve systems of linear equations using Gaussian elimination.
E. Solve systems of linear equations using Gauss-Jordan elimination.
F. Deﬁne and evaluate the rank of a matrix.
G. Apply the Rank Theorem relate the rank of an augmented matrix to the solution set
of a system in the case of homogeneous and nonhomogeneous systems.
H. Model and solve application problems using linear systems.
Reading: Linear Algebra 2.2
Outcome Mapping:
A. 1-8,24
B. 39-44
C. 9-14,15-16,17-18,19-22
D. 25-34
E. 23
F. 35-38
G. 45-52, (2.4: 1-47)

Planes 11 Sept.
4.1
Finding Planes
Quiz
1. Let a = (1, 2, 3) , b = (2, −1, 1) . Find a vector which is perpendicular to both of these
vectors.
2. Find the area of the parallelogram determined by the above two vectors.
3. Find the cosine of the angle between the above two vectors.
4. Find the sine of the angle between the above two vectors.
5. Find the volume of the parallelepiped determined by the vectors, a = (1, 2, 3) , b = (2, −1, 1)
and c = (1, 1, 1).
4.1.1
Planes From A Normal And A Point
A plane is a long ﬂat thing. It can also be considered geometrically in terms of a dot product.
To ﬁnd the equation of a plane, you need two things, a point contained in the plane and a
vector normal to the plane. Let p0 = (x0, y0, z0) denote the position vector of a point in
the plane, let p = (x, y, z) be the position vector of an arbitrary point in the plane, and let
n denote a vector normal to the plane. This means that
n· (p −p0) = 0
whenever p is the position vector of a point in the plane. The following picture illustrates
the geometry of this idea.
£
£
£
£
£
£
£
£
££
£
£
£
£
£
£
£
£
£
¡
¡
¡
¡
¡
 p0
¢
¢
¢
¢
¢

p i
¤
¤
¤¤n
P
Expressed equivalently, the plane is just the set of all points p such that the vector,
p −p0 is perpendicular to the given normal vector, n.
73

74
PLANES 11 SEPT.
Example 4.1.1 Find the equation of the plane with normal vector, n = (1, 2, 3) containing
the point (2, −1, 5) .
From the above, the equation of this plane is
(1, 2, 3) · (x −2, y + 1, z −5) = x −15 + 2y + 3z = 0
Example 4.1.2 2x + 4y −5z = 11 is the equation of a plane. Find the normal vector and
a point on this plane.
You can write this in the form 2
¡
x −11
2
¢
+ 4 (y −0) + (−5) (z −0) = 0. Therefore, a
normal vector to the plane is 2i + 4j −5k and a point in this plane is
¡ 11
2 , 0, 0
¢
. Of course
there are many other points in the plane.
Proposition 4.1.3 If (a, b, c) ̸= (0, 0, 0) , then ax + by + cz = d is the equation of a
plane with normal vector ai + bj + ck. Conversely, any plane can be written in this form.
Proof: One of a, b, c is nonzero. Suppose for example that c ̸= 0. Then the equation can
be written as
a (x −0) + b (y −0) + c
µ
z −d
c
¶
= 0
Therefore,
¡
0, 0, d
c
¢
is a point on the plane and a normal vector is ai + bj + ck. Suppose
a ̸= 0. Then the points which satisfy ax + by + cz = d are the same as the points which
satisfy
a
µ
x −d
a
¶
+ b (y −0) + c (z −0) = 0.
Thus a point on the plane is
¡ d
a, 0, 0
¢
and a normal vector is (a, b, c) as claimed. (You can
do something similar if b ̸= 0. Note there are many points on the plane. This just picks out
one.)
The converse follows from the above discussion involving the point and a normal vector.
This proves the proposition.
Example 4.1.4 Find a normal vector to the plane 2x + 5y −z = 12.3.
A normal vector is (2, 5, −1). A point on this plane is (0, 0, −12.3). Of course there are
many other points on this plane.
4.1.2
The Angle Between Two Planes
Deﬁnition 4.1.5 Suppose two planes intersect.
The angle between the planes is
deﬁned to be the angle between their normal vectors.
Example 4.1.6 Find the angle between the two planes, x+2y −z = 6 and 3x+2y −z = 7.
The two normal vectors are (1, 2, −1) and (3, 2, −1) . Therefore, the cosine of the angle
desired is
cos θ =
(1, 2, −1) · (3, 2, −1)
q
12 + 22 + (−1)2q
32 + 22 + (−1)2 = . 872 87
Now use a calculator or table to ﬁnd what the angle is.
cos θ = . 872 87, Solution is :
{θ = . 509 74} . This value is in radians.

4.1.
FINDING PLANES
75
4.1.3
The Plane Which Contains Three Points
Sometimes you need to ﬁnd the equation of a plane which contains three points. Consider
the following picture.

3
-
(a0, b0, c0)
s
s
s
(a1, b1, c1)
(a2, b2, c2)
a
b
You have plenty of points but you need a normal. This can be obtained by taking a × b
where a = (a1 −a0, b1 −b0, c1 −c0) and b = (a2 −a0, b2 −b0, c2 −c0) .
Example 4.1.7 Find the equation of the plane which contains the three points, (1, 2, 1) , (3, −1, 2) ,
and (4, 2, 1) .
You just need to get a normal vector to this plane. This can be done by taking the cross
products of the two vectors,
(3, −1, 2) −(1, 2, 1) and (4, 2, 1) −(1, 2, 1)
Thus a normal vector is (2, −3, 1) × (3, 0, 0) = (0, 3, 9) . Therefore, the equation of the plane
is
0 (x −1) + 3 (y −2) + 9 (z −1) = 0
or 3y + 9z = 15 which is the same as y + 3z = 5. When you have what you think is the
plane containing the three points, you ought to check it by seeing if it really does contain
the three points.
Example 4.1.8 Find the equation of the plane which contains the three points, (1, 2, 1) , (3, −1, 2) ,
and (4, 2, 1) another way.
Letting (x, y, z) be a point on the plane, the volume of the parallelepiped spanned by
(x, y, z) −(1, 2, 1) and the two vectors, (2, −3, 1) and (3, 0, 0) must be equal to zero. Thus
the equation of the plane is
¯¯¯¯¯¯
3
0
0
2
−3
1
x −1
y −2
z −1
¯¯¯¯¯¯
= 0.
Hence −9z + 15 −3y = 0 and dividing by 3 yields the same answer as the above.
Example 4.1.9 Find the equation of the plane containing the points (1, 2, 3) and the line
(0, 1, 1) + t (2, 1, 2) = (x, y, z).
There are several ways to do this. One is to ﬁnd three points and use any of the above
procedures.
Let t = 0 and then let t = 1 to get two points on the line.
This yields
(1, 2, 3) , (0, 1, 1) , and (2, 2, 3) . Then procede as above.

76
PLANES 11 SEPT.
Example 4.1.10 Find the equation of the plane which contains the two lines, given by the
following parametric expressions in which t ∈R.
(2t, 1 + t, 1 + 2t) = (x, y, z) , (2t + 2, 1, 3 + 2t) = (x, y, z)
Note ﬁrst that you don’t know there even is such a plane. However, if there is, you could
ﬁnd it by obtaining three points, two on one line and one on another and then using any of
the above procedures for ﬁnding the plane. From the ﬁrst line, two points are (0, 1, 1) and
(2, 2, 3) while a third point can be obtained from second line, (2, 1, 3) . You need a normal
vector and then use any of these points. To get a normal vector, form (2, 0, 2) × (2, 1, 2) =
(−2, 0, 2) . Therefore, the plane is −2x+0 (y −1)+2 (z −1) = 0. This reduces to z −x = 1.
If there is a plane, this is it. Now you can simply verify that both of the lines are really in
this plane. From the ﬁrst, (1 + 2t) −2t = 1 and the second, (3 + 2t) −(2t + 2) = 1 so both
lines lie in the plane.
4.1.4
Intercepts Of A Plane
One way to understand how a plane looks is to connect the points where it intercepts the
x, y, and z axes. This allows you to visualize the plane somewhat and is a good way to
sketch the plane. Not surprisingly these points are called intercepts.
Example 4.1.11 Sketch the plane which has intercepts (2, 0, 0) , (0, 3, 0) , and (0, 0, 4) .
¡
¡
x
y
z
You see how connecting the intercepts gives a fairly good geometric description of the
plane. These lines which connect the intercepts are also called the traces of the plane. Thus
the line which joins (0, 3, 0) to (0, 0, 4) is the intersection of the plane with the yz plane. It
is the trace on the yz plane.
Example 4.1.12 Identify the intercepts of the plane, 3x −4y + 5z = 11.
The easy way to do this is to divide both sides by 11.
x
(11/3) +
y
(−11/4) +
z
(11/5) = 1
The intercepts are (11/3, 0, 0) , (0, −11/4, 0) and (0, 0, 11/5) . You can see this by letting
both y and z equal to zero to ﬁnd the point on the x axis which is intersected by the plane.
The other axes are handled similarly.
In general, to ﬁnd the intercepts of a plane of the form ax + by + cz = d where d ̸= 0
and none of a, b, or c are equal to 0, divide by d. This gives
x
(d/a) +
y
(d/b) +
z
(d/c) = 1
the intercepts are
¡ d
a, 0, 0
¢
,
¡
0, d
b , 0
¢
,
¡
0, 0, d
c
¢
.

4.1.
FINDING PLANES
77
4.1.5
Distance Between A Point And A Plane Or A Point And A
Line∗
There exists a stupid formula for the distance between a point and a plane. I will ﬁrst
illustrate with an example.
Example 4.1.13 Find the distance from the point (1, 2, 3) to the plane x −y + z = 3.
The distance is the length of the line segment normal to the plane which goes from the
given point to the given plane. In this example, a direction vector for this line is (1, −1, 1) ,
a normal vector to the plane. Thus the equation for the desired line is
(x, y, z)
=
(1, 2, 3) + t (1, −1, 1)
=
(1 + t, 2 −t, 3 + t)
Lets ﬁnd the value of t at which the line intersects the plane. Thus
(1 + t) −(2 −t) + (3 + t) = 3
and so t = 1
3. Therefore, the line segment is the one which joins (1, 2, 3) to
µ
1 + 1
3, 2 −1
3, 3 + 1
3
¶
=
µ4
3, 5
3, 10
3
¶
.
Now it follows the desired distance is
sµ
1 −4
3
¶2
+
µ
2 −5
3
¶2
+
µ
3 −10
3
¶2
= 1
3
√
3
In the general case there is a simple and interesting geometrical consideration which will
lead to a stupid formula which you can then use with no thought to do an uninteresting
task, ﬁnding the distance from a point to a plane.
Example 4.1.14 Find the distance from the point (x0, y0, z0) to the plane ax+by +cz = d.
Here (a, b, c) ̸= (0, 0, 0) .
Consider the following picture in which P0 is a point in the plane and X0 = (x0, y0, z0)
is the point whose distance to the plane is to be found. The normal to the plane is n.


C
C
C
CO
¡
¡
¡
¡
¡
¡
¡
¡

n
θ
r (x0, y0, z0) = X0
P0
r
Then from the picture, what you want is to take the projection of the vector X0 −P0
onto the line determined by the point, P0 in the direction, n. That is, you need
|X0 −P0| |cos θ|
=
|X0 −P0|
¯¯¯¯
(X0 −P0) · n
|X0 −P0| |n|
¯¯¯¯
=
¯¯¯¯(X0 −P0) · n
|n|
¯¯¯¯

78
PLANES 11 SEPT.
As drawn in the picture, |X0 −P0| cos θ will be positive but if you had n pointing the
opposite direction this would be negative. However, either way, it’s absolute value would
give the right answer. This is why the absolute value is taken in the above. From this the
stupid formula will follow easily. Suppose a ̸= 0. Things work the same if b or c are not
zero. Then as explained above, you can take P0 =
¡ d
a, 0, 0
¢
and n = (a, b, c) . Therefore, the
above expression is
¯¯¯¯
µ
x0 −d
a, y0, z0
¶
·
(a, b, c)
√
a2 + b2 + c2
¯¯¯¯ = |ax0 + by0 + cz0 −d|
√
a2 + b2 + c2
and it is this last expression which is the stupid formula. Here is the same example done in
an ad hoc manner earlier but this time through the use of a stupid formula.
Example 4.1.15 Find the distance from the point (1, 2, 3) to the plane x −y + z = 3.
Lets apply the stupid formula. a = 1, b = −1, c = 1, d = 3, x0 = 1, y0 = 2, z0 = 3. Then
plugging in to the formula, you get
|1 × 1 + (−1) × 2 + 1 × 3 −3|
√1 + 1 + 1
= 1
3
√
3
which gives the same answer much more easily. Those of you who expect to ﬁnd the distance
from a given point to a plane repeatedly, should certainly cherish and memorize this formula
because it will save you lots of time. The rest of you should try to understand its derivation
which is genuinely interesting and worth while. Unfortunately, ﬁnding the distance from a
point to a plane is an excellent test question.
A similar formula holds for the distance from a point to a line in R2. Recall from high
school algebra, a line can be written as
ax + by = c
Then if (x0, y0) is a point and you want the distance from this point to the given line, it
equals
|ax0 + by0 −c|
√
a2 + b2
.
You should derive this stupid formula from the same geometric considerations used to get
the stupid formula for a point and a plane.
Of course it all generalizes. The same reasoning will yield a stupid formula for the dis-
tance between (y1, · · ·, yn) and the level surface, called a hyper1 plane given by Pn
k=1 akxk =
d. You can probably guess what it is by analogy to the above but it is better to derive it
directly using the same sort of geometric reasoning just given.
1Words such as “hyper” give an aura of signiﬁcance to things which are in reality trivial while obfus-
cating the real issues. They constitute an example of pretentious jargon which militates against correct
understanding.

Systems Of Linear Equations
12,13 Sept.
Quiz
1. The intercepts of a plane are (1, 0, 0) , (0, 2, 0) , and (0, 0, −1) . Find the equation of
the plane.
2. A plane has a normal vector (1, 2, −3) and contains the point (1, 1, 2) . Find the equa-
tion of the plane.
3. Find the equation of a plane which has the three points, (1, 2, 1) , (2, −2, 1) , (0, 3, 0) .
5.1
Systems Of Equations, Geometric Interpretations
As you know, equations like 2x + 3y = 6 can be graphed as straight lines. To ﬁnd the
solution to two such equations, you could graph the two straight lines and the ordered pairs
identifying the point (or points) of intersection would give the x and y values of the solution
to the two equations because such an ordered pair satisﬁes both equations. The following
picture illustrates what can occur with two equations involving two variables.
¡
¡
¡
¡
¡
¡
¡
¡
HHHHHHH
H
x
y
one solution
¡
¡
¡
¡
¡
¡
¡¡
¡
¡
¡
¡
¡
¡¡
x
y
two parallel lines
no solutions
¡
¡
¡
¡
¡
¡
¡
¡
x
y
inﬁnitely
many solutions
In the ﬁrst example of the above picture, there is a unique point of intersection. In the
second, there are no points of intersection. The other thing which can occur is that the
two lines are really the same line. For example, x + y = 1 and 2x + 2y = 2 are relations
which when graphed yield the same line. In this case there are inﬁnitely many points in the
simultaneous solution of these two equations, every ordered pair which is on the graph of
the line. It is always this way when considering linear systems of equations. There is either
no solution, exactly one or inﬁnitely many although the reasons for this are not completely
comprehended by considering a simple picture in two dimensions.
Example 5.1.1 Find the solution to the system x + y = 3, y −x = 5.
79

80
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
You can verify the solution is (x, y) = (−1, 4) . You can see this geometrically by graphing
the equations of the two lines. If you do so correctly, you should obtain a graph which looks
something like the following in which the point of intersection represents the solution of the
two equations.
¡
¡
¡
¡
@
@
@
x
(x, y) = (−1, 4) -
r
Example 5.1.2 You can also imagine other situations such as the case of three intersecting
lines having no common point of intersection or three intersecting lines which do intersect
at a single point as illustrated in the following picture.
¡
¡
¡
¡
@
@
@
x
y
¡
¡
¡
¡
¡


x
@
@
@
@
@@
y
In the case of the ﬁrst picture above, there would be no solution to the three equations
whose graphs are the given lines. In the case of the second picture there is a solution to the
three equations whose graphs are the given lines.
The points, (x, y, z) satisfying an equation in three variables like 2x + 4y −5z = 8
form a plane in three dimensions and geometrically, when you solve systems of equations
involving three variables, you are taking intersections of planes.
Consider the following
picture involving two planes.
¡
¡
¡¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡¡
@
@
@
@
@
¡
¡
¡
¡
¡
¡
¡¡@
@
@
@
@
@
¡
¡
¡
¡
¡
¡
¡
¡
@
@
@
@@
Notice how these two planes intersect in a line. It could also happen the two planes
could fail to intersect.

5.1.
SYSTEMS OF EQUATIONS, GEOMETRIC INTERPRETATIONS
81
Now imagine a third plane. One thing that could happen is this third plane could have
an intersection with one of the ﬁrst planes which results in a line which fails to intersect the
ﬁrst line as illustrated in the following picture.
¡
¡¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡¡
@
@
@
@
@
¡
¡
¡
¡
¡
¡
¡¡@
@@
@@
@
¡
¡
¡
¡
¡
¡
¡
¡
@
@
@
@@
¡
¡
¡
¡
¡
¡
¡¡
¡
¡
¡
¡
¡
¡
¡¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
ª
New Plane
Thus there is no point which lies in all three planes. The picture illustrates the situation
in which the line of intersection of the new plane with one of the original planes forms a line
parallel to the line of intersection of the ﬁrst two planes. However, in three dimensions, it
is possible for two lines to fail to intersect even though they are not parallel. Such lines are
called skew lines. You might consider whether there exist two skew lines, each of which
is the intersection of a pair of planes selected from a set of exactly three planes such that
there is no point of intersection between the three planes. You can also see that if you tilt
one of the planes you could obtain every pair of planes having a nonempty intersection in a
line and yet there may be no point in the intersection of all three.
It could happen also that the three planes could intersect in a single point as shown in
the following picture.
¡
¡
¡¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡¡
@
@
@
@
@
¡
¡
¡
¡
¡
¡
¡¡
@
¡
¡
¡
¡
¡
¡
@
@
@
@@
r
¡
¡
¡
ª
New Plane
In this case, the three planes have a single point of intersection. The three planes could
also intersect in a line.

82
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
¡
¡
¡¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡¡
@
@
@
@
@
¡
¡
¡
¡
¡
¡
¡¡@
@
@
@
@
@
¡
¡
¡
¡
¡
¡
¡
¡
@
@
@
@@





¡¡
¡
¡
¡
¡
¡
¡
¡¡





Thus in the case of three equations having three variables, the planes determined by these
equations could intersect in a single point, a line, or even fail to intersect at all. You see
that in three dimensions there are many possibilities. If you want to waste some time, you
can try to imagine all the things which could happen but this will not help for dimensions
higher than 3 which is where many of the important applications lie.
In higher dimensions it is customary to refer to the set of points described by relations
like x + y −2z + 4w = 8 as hyper-planes.1 Such pictures as above are useful in two or
three dimensions for gaining insight into what can happen but they are not adequate for
obtaining the exact solution set of the linear system. The only rational and useful way to
deal with this subject is through the use of algebra. Indeed, a major reason for studying
mathematics is to obtain freedom from always having to draw a picture in order to do a
computation or ﬁnd out something important.
5.2
Systems Of Equations, Algebraic Procedures
5.2.1
Elementary Operations
Deﬁnition 5.2.1 A system of linear equations is a set of p equations for the n
variables, x1, · · ·, xn which is of the form
n
X
k=1
amkxk = dm, m = 1, 2, · · ·, p
Written less compactly it is a set of equations of the following form
a11x1 + a12x2 + · · · + a1nxn = d1
a21x1 + a22x2 + · · · + a2nxn = d2
...
ap1x1 + ap2x2 + · · · + apnxn = dp
1The evocative semi word, “hyper” conveys absolutely no meaning but is traditional usage which makes
the terminology sound more impressive than something like long wide comparatively ﬂat thing which does
convey some meaning. However, in such cases as these pretentious jargon is nearly always preferred. Later
we will discuss some terms which are not just evocative but yield real understanding.

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
83
The problem is to ﬁnd the values of x1, x2 · ··, xn which satisfy all p equations. This is called
the solution set of the system of equations. In other words, (a1, · · ·, an) is in the solution
set of the system of equations if when you plug a1 in place of x1, a2 in place of x2 etc., each
equation in the system is satisﬁed.
Consider the following example.
Example 5.2.2 Find x and y such that
x + y = 7 and 2x −y = 8.
(5.1)
The set of ordered pairs, (x, y) which solve both equations is called the solution set.
You can verify that (x, y) = (5, 2) is a solution to the above system. The interesting
question is this: If you were not given this information to verify, how could you determine
the solution? You can do this by using the following basic operations on the equations, none
of which change the set of solutions of the system of equations.
Deﬁnition 5.2.3 Elementary operations are those operations consisting of the
following.
1. Interchange the order in which the equations are listed.
2. Multiply any equation by a nonzero number.
3. Replace any equation with itself added to a multiple of another equation.
Example 5.2.4 To illustrate the third of these operations on this particular system, con-
sider the following.
x + y = 7
2x −y = 8
The system has the same solution set as the system
x + y = 7
−3y = −6 .
To obtain the second system, take the second equation of the ﬁrst system and add -2 times
the ﬁrst equation to obtain
−3y = −6.
Now, this clearly shows that y = 2 and so it follows from the other equation that x + 2 = 7
and so x = 5.
Of course a linear system may involve many equations and many variables. The solution
set is still the collection of solutions to the equations. In every case, the above operations
of Deﬁnition 5.2.3 do not change the set of solutions to the system of linear equations.
Theorem 5.2.5 Suppose you have two equations, involving the variables, (x1, · · ·, xn)
E1 = f1, E2 = f2
(5.2)
where E1 and E2 are expressions involving the variables and f1 and f2 are constants. (In
the above example there are only two variables, x and y and E1 = x + y while E2 = 2x −y.)
Then the system E1 = f1, E2 = f2 has the same solution set as
E1 = f1, E2 + aE1 = f2 + af1.
(5.3)
Also the system E1 = f1, E2 = f2 has the same solutions as the system, E2 = f2, E1 = f1.
The system E1 = f1, E2 = f2 has the same solution as the system E1 = f1, aE2 = af2
provided a ̸= 0.

84
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Proof: If (x1, · · ·, xn) solves E1 = f1, E2 = f2 then it solves the ﬁrst equation in
E1 = f1, E2+aE1 = f2+af1. Also, it satisﬁes aE1 = af1 and so, since it also solves E2 = f2
it must solve E2 +aE1 = f2 +af1. Therefore, if (x1, · · ·, xn) solves E1 = f1, E2 = f2 it must
also solve E2 + aE1 = f2 + af1. On the other hand, if it solves the system E1 = f1 and
E2 + aE1 = f2 + af1, then aE1 = af1 and so you can subtract these equal quantities from
both sides of E2+aE1 = f2+af1 to obtain E2 = f2 showing that it satisﬁes E1 = f1, E2 = f2.
The second assertion of the theorem which says that the system E1 = f1, E2 = f2 has the
same solution as the system, E2 = f2, E1 = f1 is seen to be true because it involves nothing
more than listing the two equations in a diﬀerent order. They are the same equations.
The third assertion of the theorem which says E1 = f1, E2 = f2 has the same solution
as the system E1 = f1, aE2 = af2 provided a ̸= 0 is veriﬁed as follows: If (x1, · · ·, xn) is a
solution of E1 = f1, E2 = f2, then it is a solution to E1 = f1, aE2 = af2 because the second
system only involves multiplying the equation, E2 = f2 by a. If (x1, · · ·, xn) is a solution of
E1 = f1, aE2 = af2, then upon multiplying aE2 = af2 by the number, 1/a, you ﬁnd that
E2 = f2.
Stated simply, the above theorem shows that the elementary operations do not change
the solution set of a system of equations.
Here is an example in which there are three equations and three variables. You want to
ﬁnd values for x, y, z such that each of the given equations are satisﬁed when these values
are plugged in to the equations.
Example 5.2.6 Find the solutions to the system,
x + 3y + 6z = 25
2x + 7y + 14z = 58
2y + 5z = 19
(5.4)
To solve this system replace the second equation by (−2) times the ﬁrst equation added
to the second. This yields the system
x + 3y + 6z = 25
y + 2z = 8
2y + 5z = 19
(5.5)
Now take (−2) times the second and add to the third. More precisely, replace the third
equation with (−2) times the second added to the third. This yields the system
x + 3y + 6z = 25
y + 2z = 8
z = 3
(5.6)
At this point, you can tell what the solution is. This system has the same solution as the
original system and in the above, z = 3. Then using this in the second equation, it follows
y + 6 = 8 and so y = 2. Now using this in the top equation yields x + 6 + 18 = 25 and so
x = 1. This process is called back substitution.
Alternatively, in 5.6 you could have continued as follows. Add (−2) times the bottom
equation to the middle and then add (−6) times the bottom to the top. This yields
x + 3y = 7
y = 2
z = 3

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
85
Now add (−3) times the second to the top. This yields
x = 1
y = 2
z = 3
,
a system which has the same solution set as the original system. This avoided back substi-
tution and led to the same solution set.
5.2.2
Gauss Elimination
A less cumbersome way to represent a linear system is to write it as an augmented matrix.
For example the linear system, 5.4 can be written as


1
3
6
| 25
2
7
14
| 58
0
2
5
| 19

.
It has exactly the same information as the original system but here it is understood there is
an x column,


1
2
0

, a y column,


3
7
2

and a z column,


6
14
5

. The rows correspond
to the equations in the system. Thus the top row in the augmented matrix corresponds to
the equation,
x + 3y + 6z = 25.
Now when you replace an equation with a multiple of another equation added to itself, you
are just taking a row of this augmented matrix and replacing it with a multiple of another
row added to it. Thus the ﬁrst step in solving 5.4 would be to take (−2) times the ﬁrst row
of the augmented matrix above and add it to the second row,


1
3
6
| 25
0
1
2
| 8
0
2
5
| 19

.
Note how this corresponds to 5.5. Next take (−2) times the second row and add to the
third,


1
3
6
| 25
0
1
2
| 8
0
0
1
| 3


This augmented matrix corresponds to the system
x + 3y + 6z = 25
y + 2z = 8
z = 3
which is the same as 5.6. By back substitution you obtain the solution x = 1, y = 6, and
z = 3.
In general a linear system is of the form
a11x1 + · · · + a1nxn = b1
...
am1x1 + · · · + amnxn = bm
,
(5.7)

86
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
where the xi are variables and the aij and bi are constants. This system can be represented
by the augmented matrix,



a11
· · ·
a1n
|
b1
...
...
|
...
am1
· · ·
amn
|
bm


.
(5.8)
Changes to the system of equations in 5.7 as a result of an elementary operations translate
into changes of the augmented matrix resulting from a row operation. Note that Theorem
5.2.5 implies that the row operations deliver an augmented matrix for a system of equations
which has the same solution set as the original system.
Deﬁnition 5.2.7 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to it.
Gauss elimination is a systematic procedure to simplify an augmented matrix to a
reduced form.
In the following deﬁnition, the term “leading entry” refers to the ﬁrst
nonzero entry of a row when scanning the row from left to right.
Deﬁnition 5.2.8 An augmented matrix is in echelon form also called row ech-
elon form if
1. All nonzero rows are above any rows of zeros.
2. Each leading entry of a row is in a column to the right of the leading entries of any
rows above it.
Deﬁnition 5.2.9 An augmented matrix is in row reduced echelon form if
1. All nonzero rows are above any rows of zeros.
2. Each leading entry of a row is in a column to the right of the leading entries of any
rows above it.
3. All entries in a column above and below a leading entry are zero.
4. Each leading entry is a 1, the only nonzero entry in its column.
The relation between these two deﬁnitions is as described in the following picture.
row echelon form
row reduced echelon form
Thus if the matrix is in row reduced echelon form, it is in row echelon form but not
necessarily the other way around. You can usually ﬁnd the solution to a system of equations
by row reducing to row echelon form. You typically don’t have to go all the way to the row
reduced echelon form but the row reduced echelon form is very important because, unlike
a row echelon form, it is unique. It is also easier to use in the case where the system of
equations has an inﬁnite solution set.

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
87
Example 5.2.10 Here are some augmented matrices which are in row reduced echelon form.




1
0
0
5
8
|
0
0
0
1
2
7
|
0
0
0
0
0
0
|
1
0
0
0
0
0
|
0



,






1
0
0
|
0
0
1
0
|
0
0
0
1
|
0
0
0
0
|
1
0
0
0
|
0






.
Example 5.2.11 Here are augmented matrices in echelon form which are not in row re-
duced echelon form but which are in echelon form.




1
0
6
5
8
|
2
0
0
2
2
7
|
3
0
0
0
0
0
|
1
0
0
0
0
0
|
0



,






1
3
5
|
4
0
2
0
|
7
0
0
3
|
0
0
0
0
|
1
0
0
0
|
0






Example 5.2.12 Here are some augmented matrices which are not in echelon form.






0
0
0
|
0
1
2
3
|
3
0
1
0
|
2
0
0
0
|
1
0
0
0
|
0






,


1
2
|
3
2
4
|
−6
4
0
|
7

,




0
2
3
|
3
1
5
0
|
2
7
5
0
|
1
0
0
1
|
0



.
Deﬁnition 5.2.13 A pivot position in a matrix is the location of a leading entry
in an echelon form resulting from the application of row operations to the matrix. A pivot
column is a column that contains a pivot position.
For example consider the following.
Example 5.2.14 Suppose
A =


1
2
3
|
4
3
2
1
|
6
4
4
4
|
10


Where are the pivot positions and pivot columns?
Replace the second row by −3 times the ﬁrst added to the second. This yields


1
2
3
|
4
0
−4
−8
|
−6
4
4
4
|
10

.
This is not in reduced echelon form so replace the bottom row by −4 times the top row
added to the bottom. This yields


1
2
3
|
4
0
−4
−8
|
−6
0
−4
−8
|
−6

.
This is still not in reduced echelon form. Replace the bottom row by −1 times the middle
row added to the bottom. This yields


1
2
3
|
4
0
−4
−8
|
−6
0
0
0
|
0



88
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
which is in echelon form, although not in reduced echelon form. Therefore, the pivot posi-
tions in the original matrix are the locations corresponding to the ﬁrst row and ﬁrst column
and the second row and second columns as shown in the following:


1
2
3
|
4
3
2
1
|
6
4
4
4
|
10


Thus the pivot columns in the matrix are the ﬁrst two columns.
The following is the algorithm for obtaining a matrix which is in row reduced echelon
form.
Algorithm 5.2.15
This algorithm tells how to start with a matrix and do row operations on it in such a
way as to end up with a matrix in row reduced echelon form.
1. Find the ﬁrst nonzero column from the left.
This is the ﬁrst pivot column.
The
position at the top of the ﬁrst pivot column is the ﬁrst pivot position. Switch rows if
necessary to place a nonzero number in the ﬁrst pivot position.
2. Use row operations to zero out the entries below the ﬁrst pivot position.
3. Ignore the row containing the most recent pivot position identiﬁed and the rows above
it. Repeat steps 1 and 2 to the remaining submatrix, the rectangular array of numbers
obtained from the original matrix by deleting the rows you just ignored. Repeat the
process until there are no more rows to modify. The matrix will then be in echelon
form.
4. Moving from right to left, use the nonzero elements in the pivot positions to zero out
the elements in the pivot columns which are above the pivots.
5. Divide each nonzero row by the value of the leading entry. The result will be a matrix
in row reduced echelon form.
This row reduction procedure applies to both augmented matrices and non augmented
matrices. There is nothing special about the augmented column with respect to the row
reduction procedure.
Example 5.2.16 Here is a matrix.






0
0
2
3
2
0
1
1
4
3
0
0
1
2
2
0
0
0
0
0
0
0
0
2
1






Do row reductions till you obtain a matrix in echelon form. Then complete the process by
producing one in reduced echelon form.
The pivot column is the second. Hence the pivot position is the one in the ﬁrst row and
second column. Switch the ﬁrst two rows to obtain a nonzero entry in this pivot position.






0
1
1
4
3
0
0
2
3
2
0
0
1
2
2
0
0
0
0
0
0
0
0
2
1







5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
89
Step two is not necessary because all the entries below the ﬁrst pivot position in the resulting
matrix are zero. Now ignore the top row and the columns to the left of this ﬁrst pivot
position. Thus you apply the same operations to the smaller matrix,




2
3
2
1
2
2
0
0
0
0
2
1



.
The next pivot column is the third corresponding to the ﬁrst in this smaller matrix and the
second pivot position is therefore, the one which is in the second row and third column. In
this case it is not necessary to switch any rows to place a nonzero entry in this position
because there is already a nonzero entry there. Multiply the third row of the original matrix
by −2 and then add the second row to it. This yields






0
1
1
4
3
0
0
2
3
2
0
0
0
−1
−2
0
0
0
0
0
0
0
0
2
1






.
The next matrix the steps in the algorithm are applied to is


−1
−2
0
0
2
1

.
The ﬁrst pivot column is the ﬁrst column in this case and no switching of rows is necessary
because there is a nonzero entry in the ﬁrst pivot position. Therefore, the algorithm yields
for the next step






0
1
1
4
3
0
0
2
3
2
0
0
0
−1
−2
0
0
0
0
0
0
0
0
0
−3






.
Now the algorithm will be applied to the matrix,
µ
0
−3
¶
There is only one column and it is nonzero so this single column is the pivot column.
Therefore, the algorithm yields the following matrix for the echelon form.






0
1
1
4
3
0
0
2
3
2
0
0
0
−1
−2
0
0
0
0
−3
0
0
0
0
0






.
To complete placing the matrix in reduced echelon form, multiply the third row by 3 and
add −2 times the fourth row to it. This yields






0
1
1
4
3
0
0
2
3
2
0
0
0
−3
0
0
0
0
0
−3
0
0
0
0
0







90
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Next multiply the second row by 3 and take 2 times the fourth row and add to it. Then
add the fourth row to the ﬁrst.






0
1
1
4
0
0
0
6
9
0
0
0
0
−3
0
0
0
0
0
−3
0
0
0
0
0






.
Next work on the fourth column in the same way.






0
3
3
0
0
0
0
6
0
0
0
0
0
−3
0
0
0
0
0
−3
0
0
0
0
0






Take −1/2 times the second row and add to the ﬁrst.






0
3
0
0
0
0
0
6
0
0
0
0
0
−3
0
0
0
0
0
−3
0
0
0
0
0






.
Finally, divide by the value of the leading entries in the nonzero rows.






0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0






.
The above algorithm is the way a computer would obtain a reduced echelon form for a
given matrix. It is not necessary for you to pretend you are a computer but if you like to do
so, the algorithm described above will work. The main idea is to do row operations in such
a way as to end up with a matrix in echelon form or row reduced echelon form because when
this has been done, the resulting augmented matrix will allow you to describe the solutions
to the linear system of equations in a meaningful way.
Example 5.2.17 Give the complete solution to the system of equations, 5x+10y−7z = −2,
2x + 4y −3z = −1, and 3x + 6y + 5z = 9.
The augmented matrix for this system is


2
4
−3
|
−1
5
10
−7
|
−2
3
6
5
|
9


Multiply the second row by 2, the ﬁrst row by 5, and then take (−1) times the ﬁrst row and
add to the second. Then multiply the ﬁrst row by 1/5. This yields


2
4
−3
|
−1
0
0
1
|
1
3
6
5
|
9



5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
91
Now, combining some row operations, take (−3) times the ﬁrst row and add this to 2 times
the last row and replace the last row with this. This yields.


2
4
−3
|
−1
0
0
1
|
1
0
0
1
|
21

.
One more row operation, taking (−1) times the second row and adding to the bottom yields.


2
4
−3
|
−1
0
0
1
|
1
0
0
0
|
20

.
This is impossible because the last row indicates the need for a solution to the equation
0x + 0y + 0z = 20
and there is no such thing because 0 ̸= 20. This shows there is no solution to the three given
equations. When this happens, the system is called inconsistent. In this case it is very
easy to describe the solution set. The system has no solution.
Here is another example based on the use of row operations.
Example 5.2.18 Give the complete solution to the system of equations, 3x −y −5z = 9,
y −10z = 0, and −2x + y = −6.
The augmented matrix of this system is


3
−1
−5
|
9
0
1
−10
|
0
−2
1
0
|
−6


Replace the last row with 2 times the top row added to 3 times the bottom row. This gives


3
−1
−5
|
9
0
1
−10
|
0
0
1
−10
|
0

.
The entry, 3 in this sequence of row operations is called the pivot. It is used to create
zeros in the other places of the column. Next take −1 times the middle row and add to the
bottom. Here the 1 in the second row is the pivot.


3
−1
−5
|
9
0
1
−10
|
0
0
0
0
|
0


Take the middle row and add to the top and then divide the top row which results by 3.


1
0
−5
|
3
0
1
−10
|
0
0
0
0
|
0

.
This is in reduced echelon form. The equations corresponding to this reduced echelon form
are y = 10z and x = 3 + 5z. Apparently z can equal any number. Lets call this number,
t. 2Therefore, the solution set of this system is x = 3 + 5t, y = 10t, and z = t where t
2In this context t is called a parameter.

92
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
is completely arbitrary. The system has an inﬁnite set of solutions which are given in the
above simple way. This is what it is all about, ﬁnding the solutions to the system.
There is some terminology connected to this which is useful. Recall how each column
corresponds to a variable in the original system of equations. The variables corresponding to
a pivot column are called basic variables . The other variables are called free variables.
In Example 5.2.18 there was one free variable, z, and two basic variables, x and y. In de-
scribing the solution to the system of equations, the free variables are assigned a parameter.
In Example 5.2.18 this parameter was t. Sometimes there are many free variables and in
these cases, you need to use many parameters. Here is another example.
Example 5.2.19 Find the solution to the system
x + 2y −z + w = 3
x + y −z + w = 1
x + 3y −z + w = 5
The augmented matrix is


1
2
−1
1
|
3
1
1
−1
1
|
1
1
3
−1
1
|
5

.
Take −1 times the ﬁrst row and add to the second. Then take −1 times the ﬁrst row and
add to the third. This yields


1
2
−1
1
|
3
0
−1
0
0
|
−2
0
1
0
0
|
2


Now add the second row to the bottom row


1
2
−1
1
|
3
0
−1
0
0
|
−2
0
0
0
0
|
0


(5.9)
This matrix is in echelon form and you see the basic variables are x and y while the free
variables are z and w. Assign s to z and t to w. Then the second row yields the equation,
y = 2 while the top equation yields the equation, x + 2y −s + t = 3 and so since y = 2, this
gives x + 4 −s + t = 3 showing that x = −1 + s −t, y = 2, z = s, and w = t. It is customary
to write this in the form




x
y
z
w



=




−1 + s −t
2
s
t



.
(5.10)
This is another example of a system which has an inﬁnite solution set but this time
the solution set depends on two parameters, not one. Most people ﬁnd it less confusing
in the case of an inﬁnite solution set to ﬁrst place the augmented matrix in row reduced
echelon form rather than just echelon form before seeking to write down the description of
the solution. In the above, this means we don’t stop with the echelon form 5.9. Instead we
ﬁrst place it in reduced echelon form as follows.


1
0
−1
1
|
−1
0
1
0
0
|
2
0
0
0
0
|
0

.

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
93
Then the solution is y = 2 from the second row and x = −1 + z −w from the ﬁrst. Thus
letting z = s and w = t, the solution is given in 5.10.
The number of free variables is always equal to the number of diﬀerent parameters
used to describe the solution. If there are no free variables, then either there is no solution
as in the case where row operations yield an echelon form like


1
2
|
3
0
4
|
−2
0
0
|
1


or there is a unique solution as in the case where row operations yield an echelon form like


1
2
2
|
3
0
4
3
|
−2
0
0
4
|
1

.
Also, sometimes there are free variables and no solution as in the following:


1
2
2
|
3
0
4
3
|
−2
0
0
0
|
1

.
There are a lot of cases to consider but it is not necessary to make a major production of
this. Do row operations till you obtain a matrix in echelon form or reduced echelon form
and determine whether there is a solution. If there is, see if there are free variables. In this
case, there will be inﬁnitely many solutions. Find them by assigning diﬀerent parameters
to the free variables and obtain the solution. If there are no free variables, then there will
be a unique solution which is easily determined once the augmented matrix is in echelon
or row reduced echelon form. In every case, the process yields a straightforward way to
describe the solutions to the linear system. As indicated above, you are probably less likely
to become confused if you place the augmented matrix in row reduced echelon form rather
than just echelon form.
In summary,
Deﬁnition 5.2.20 A system of linear equations is a list of equations,
a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
am1x1 + am2x2 + · · · + amnxn = bm
where aij are numbers, and bj is a number. The above is a system of m equations in the
n variables, x1, x2 · ··, xn. Nothing is said about the relative size of m and n. Written more
simply in terms of summation notation, the above can be written in the form
n
X
j=1
aijxj = fj, i = 1, 2, 3, · · ·, m
It is desired to ﬁnd (x1, · · ·, xn) solving each of the equations listed.
As illustrated above, such a system of linear equations may have a unique solution, no
solution, or inﬁnitely many solutions and these are the only three cases which can occur for
any linear system. Furthermore, you do exactly the same things to solve any linear system.

94
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
You write the augmented matrix and do row operations until you get a simpler system in
which it is possible to see the solution, usually obtaining a matrix in echelon or reduced
echelon form. All is based on the observation that the row operations do not change the
solution set. You can have more equations than variables, fewer equations than variables,
etc. It doesn’t matter. You always set up the augmented matrix and go to work on it.
Deﬁnition 5.2.21 A system of linear equations is called consistent if there exists
a solution. It is called inconsistent if there is no solution.
These are reasonable words to describe the situations of having or not having a solu-
tion. If you think of each equation as a condition which must be satisﬁed by the variables,
consistent would mean there is some choice of variables which can satisfy all the condi-
tions. Inconsistent means there is no choice of the variables which can satisfy each of the
conditions.
5.3
The Rank Of A Matrix 14 Sept.
The notion of an augmented matrix was used to solve systems of equations. In general, a
matrix is simply a rectangular array of numbers.
Deﬁnition 5.3.1 A matrix, A is called an m × n matrix if it has m rows and n
columns.
Example 5.3.2 The matrix,


1
2
3
4
5
6


is a 3 × 2 matrix because it has two columns, (These stand upright.) and three rows.
Corresponding to such a rectangular array of numbers, there is a row reduced echelon
form discussed above. The following theorem is of fundamental signiﬁcance.
Theorem 5.3.3 Given an m × n matrix, the row reduced echelon form is unique.
This is a remarkable theorem because there are many ways to do row operations and
eventually end up with something in row reduced echelon form. It is remarkable that you
always get the same thing. Now it is easy to describe the rank of a matrix.
Deﬁnition 5.3.4 The rank of a matrix, A equals the number of nonzero rows in its
row reduced echelon form. This is the same as the number of pivot columns.
Example 5.3.5 Find the rank of the matrix,
A =


1
2
3
1
0
2
1
1
1
4
4
2


To ﬁnd the rank, you obtain the row reduced echelon form and count the number of
nonzero rows or equivalently the number of pivot columns. First take −1 times the top row
and add to the bottom row. This yields


1
2
3
1
0
2
1
1
0
2
1
1



5.3.
THE RANK OF A MATRIX 14 SEPT.
95
Now add −1 times the second row to the bottom. This yields


1
2
3
1
0
2
1
1
0
0
0
0


Now take −1 times the second row and add to the top.


1
0
2
0
0
2
1
1
0
0
0
0


Finally, multiply the second row by 1/2 to get


1
0
2
0
0
1
1/2
1/2
0
0
0
0


which is in row reduced echelon form. The rank of this matrix is therefore 2.
Note that from the process used to obtain the row reduced echelon form, once you have
obtained an echelon form, you know the correct number of rows in the ﬁnal result. Thus you
can simply take the number of nonzero rows in an echelon form and this will be the rank.
Note also that the rank is the number of pivot columns. In this case the pivot columns are
the ﬁrst two.
Deﬁnition 5.3.6 A homogeneous system of linear equations is one with aug-
mented matrix of the form
¡ A
|
0 ¢
where 0 is a column of zeros and A is an m × n matrix.
Example 5.3.7 An example of a homogeneous system of equations is x+y = 0, 3x−y = 0.
It has augmented matrix,
µ
1
1
|
0
3
−1
|
0
¶
.
The nice thing about homogeneous systems is that they are always consistent. Simply
let all the variables equal zero and you obtain a solution. However, there may be other
solutions besides this one. This is related to the concept of rank and free variables.
Theorem 5.3.8 Let A be an m × n matrix. Form the augmented matrix,
¡
A
|
0
¢
where 0 is the column of zeros.
Thus A is the coeﬃcient matrix of a system of linear
equations with n variables. Then the number of free variables = n −rank (A) .
Proof: The basic variables correspond to the pivot columns of A and the free variables
correspond to the other columns.
However, the rank of A equals the number of pivot
columns.
As a corollary here is a theorem which is called the Rank theorem.
Corollary 5.3.9 (Rank Theorem) Let A be an m × n matrix.
Form the augmented
matrix,
¡
A
|
b
¢
where b is an m×1 column. Thus A is the coeﬃcient matrix of a system of linear equations
with n variables. Then if the system of equations represented by the above augmented matrix
is consistent, number of free variables = n −rank (A) .

96
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Proof: Since the equations represented by the above augmented matrix are consistent,
the same argument as in Theorem 5.3.8 holds. The leading entry in the last nonzero row
cannot be in the last column because if it were, then the system would fail to be consistent.
5.4
Theory Of Row Reduced Echelon Form∗
This material will be done much more easily later after the introduction of elementary
matrices. You can wait to read it till then. However, if you wish to understand what is
going on right now, I am giving an explanation. First recall the row operations.
Deﬁnition 5.4.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to itself.
In rough terms, the following lemma states that linear relationships between columns in
a matrix are preserved by row operations.
Deﬁnition 5.4.2 The vector, u is a linear combination of the vectors, v1, ···, vm
if there exist scalars, c1, · · ·, cm such that
u
=
c1v1 + c2v2 + · · · + cmvm
=
m
X
k=1
ckvk.
Example 5.4.3
3


2
3
1

+ 5


1
−2
4

+ (−2)


5
3
7

=


1
−7
9


Thus


1
−7
9

is a linear combination of the vectors,


2
3
1

,


1
−2
4

, and


5
3
7

. In
this case the scalars are 3, 5, and −2.
Deﬁnition 5.4.4 When dealing with an m×n matrix, A, the element in the ith row
and the jth column is denoted as Aij. Thus the jth column is







A1j
A2j
A3j
...
Amj







Lemma 5.4.5 Let B and A be two m × n matrices and suppose B results from a row
operation applied to A. Then the kth column of B is a linear combination of the i1, · · ·, ir
columns of B if and only if the kth column of A is a linear combination of the i1, · · ·, ir
columns of A. Furthermore, the scalars in the linear combination are the same. (The linear
relationship between the kth column of A and the i1, · · ·, ir columns of A is the same as the
linear relationship between the kth column of B and the i1, · · ·, ir columns of B.)

5.4.
THEORY OF ROW REDUCED ECHELON FORM∗
97
Proof: This is obvious in the case of the ﬁrst two row operations and a little less obvious
in the case of the third. Therefore, consider the third. Suppose the sth row of B equals the
sth row of A added to c times the qth row of A. Therefore,
Bij = Aij if i ̸= s, Bsj = Asj + cAqj.
The assumption about the kth column of B is equivalent to saying that for each p,
Bpk =
r
X
j=1
αjBpij.
(5.11)
For p ̸= s, this is equivalent to saying
Apk =
r
X
j=1
αjApij
(5.12)
because for these values of p, Bpj = Apj. For p = s, this is equivalent to saying
Ask + cAqk =
r
X
j=1
αj
¡
Asij + cAqij
¢
.
(5.13)
but from 5.12, applied to p = q,
cAqk = c
r
X
j=1
αjAqij
and so from 5.13, it follows 5.11 is equivalent to 5.12 for all p, including p = s. This proves
the lemma.
Now I will present a review of the row reduced echelon form. It is convenient to describe
it slightly diﬀerently to use Lemma 5.4.5.
Deﬁnition 5.4.6 Let ei denote the column vector which has all zero entries except
for the ith slot which is one. An m × n matrix is said to be in row reduced echelon form
if, in viewing succesive columns from left to right, the ﬁrst nonzero column encountered is
e1 and if you have encountered e1, e2, · · ·, ek, the next column is either ek+1 or is a linear
combination of the vectors, e1, e2, · · ·, ek.
Theorem 5.4.7 Let A be an m × n matrix. Then A has a row reduced echelon
form determined by a simple process.
Proof: Viewing the columns of A from left to right take the ﬁrst nonzero column. Pick
a nonzero entry in this column and switch the row containing this entry with the top row of
A. Now divide this new top row by the value of this nonzero entry to get a 1 in this position
and then use row operations to make all entries below this element equal to zero. Thus the
ﬁrst nonzero column is now e1. Denote the resulting matrix by A1. Consider the submatrix
of A1 to the right of this column and below the ﬁrst row. Do exactly the same thing for it
that was done for A. This time the e1 will refer to Rm−1. Use this 1 and row operations to
zero out every element above it in the rows of A1. Call the resulting matrix, A2. Thus A2
satisﬁes the conditions of the above deﬁnition up to the column just encountered. Continue
this way till every column has been dealt with and the result must be in row reduced echelon
form.

98
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
The following diagram illustrates the above procedure. Say the matrix looked something
like the following.





0
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
∗
∗
...
...
...
...
...
...
...
0
∗
∗
∗
∗
∗
∗





First step would yield something like





0
1
∗
∗
∗
∗
∗
0
0
∗
∗
∗
∗
∗
...
...
...
...
...
...
...
0
0
∗
∗
∗
∗
∗





For the second step you look at the lower right corner as described,



∗
∗
∗
∗
∗
...
...
...
...
...
∗
∗
∗
∗
∗



and if the ﬁrst column consists of all zeros but the next one is not all zeros, you would get
something like this.



0
1
∗
∗
∗
...
...
...
...
...
0
0
∗
∗
∗



Thus, after zeroing out the term in the top row above the 1, you get the following for the
next step in the computation of the row reduced echelon form for the original matrix.





0
1
∗
0
∗
∗
∗
0
0
0
1
∗
∗
∗
...
...
...
...
...
...
...
0
0
0
0
∗
∗
∗




.
Next you look at the lower right matrix below the top two rows and to the right of the ﬁrst
four columns and repeat the process.
Deﬁnition 5.4.8 The ﬁrst pivot column of A is the ﬁrst nonzero column of A. The
next pivot column is the ﬁrst column after this which becomes e2 in the row reduced echelon
form. The third is the next column which becomes e3 in the row reduced echelon form and
so forth.
There are three choices for row operations at each step in the above theorem. A natural
question is whether the same row reduced echelon matrix always results in the end from
following the above algorithm applied in any way. The next corollary says this is the case.
Deﬁnition 5.4.9 Two matrices are said to be row equivalent if one can be ob-
tained from the other by a sequence of row operations.
It has been shown above that every matrix is row equivalent to one which is in row
reduced echelon form.

5.4.
THEORY OF ROW REDUCED ECHELON FORM∗
99
Corollary 5.4.10 The row reduced echelon form is unique.
That is if B, C are two
matrices in row reduced echelon form and both are row equivalent to A, then B = C.
Proof: Suppose B and C are both row reduced echelon forms for the matrix, A. Then
they clearly have the same zero columns since row operations leave zero columns unchanged.
If B has the sequence e1, e2, · · ·, er occuring for the ﬁrst time in the positions, i1, i2, · · ·, ir
the description of the row reduced echelon form means that if bk is the kth column of B such
that ij−1 < k < ij then bk is a linear combination of the columns in positions i1, i2, · · ·, ir.
By Lemma 5.4.5 the same is true for ck, the kth column of C. Therefore, ck is not equal
to ej for any j because ej is not obtained as a linear combinations of the ei for i < j. It
follows the ej for C can only occur in positions i1, i2, · · ·, ir. Furthermore, position ij in C
must contain ej because if not, then cij would be a linear combination of e1, · · ·, ej−1 in C
but not in B, thus contradicting Lemma 5.4.5. Therefore, both B and C have the sequence
e1, e2, · · ·, er occuring for the ﬁrst time in the positions, i1, i2, · · ·, ir. By Lemma 5.4.5, the
columns between the ik and ik+1 position are linear combinations involving the same scalars
of the columns in the i1, · · ·, ik position. This is equivalent to the assertion that each of
these columns is identical and this proves the corollary.
This suggests that to ﬁnd the rank of a matrix, one should do row operations until a
matrix is obtained in which its rank is obvious.
5.4.1
Exercises With Answers
1. Find the distance from the point, (1, 2, 1) to the plane 3x + y −z = 7.
You can use the stupid formula for this.
|3 + 2 −1 −7|
√9 + 1 + 1
= 3
11
√
11
2. Find the cosine of the angle between the planes x −y + z = 7 and 2x + y −3z = 4.
You just need to consider the normal vectors which are (1, −1, 1) and (2, 1, −3) . Then
the cosine of the angle desired is
cos θ =
¯¯¯¯
(2, 1, −3) · (1, −1, 1)
√1 + 1 + 1√4 + 1 + 9
¯¯¯¯ = 1
21
√
3
√
14
3. Here are vector equations for two lines. (x, y, z) = (1, 2, 0) + t (2, 1, 1) and (x, y, z) =
(3, 0, 1) + t (1, −2, 1) . The angle between the direction vectors is not 0 or π and so
the lines are not parallel. If they were two lines in R2, this means they would need to
intersect. However, these two lines do not intersect. If they did, there would exist s, t
such that
(1, 2, 0) + t (2, 1, 1) = (3, 0, 1) + s (1, −2, 1)
and this would require the following system of equations would need to hold.
1 + 2t = 3 + s
2 + t = −2s
t = 1 + s
The augmented matrix for this system is


2
−1
|
2
1
2
|
−2
1
−1
|
1



100
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
The row reduced echelon form is


1
0
|
0
0
1
|
0
0
0
|
1


and so there is no solution. These lines are called skew lines. Imagine two airplanes,
one going from South to North and the other going from East to West. The ﬁrst
travels at 40000 feet and the second at 35000 feet. Their paths never cross. Of course
the extra dimension is not present in two dimensions and so their paths would cross
if they were moving in a plane. Note also that to consider the question whether the
lines intersect, you must look at possibly diﬀerent values for the parameters.
4. Let two skew lines be given in Problem 3. Find two parallel planes which contain the
two lines.
This is easy if you can ﬁnd the normal vector of the two planes. To say the planes
are parallel requires them to have the same normal vector.
The two lines were
(x, y, z) = (1, 2, 0) + t (2, 1, 1) and (x, y, z) = (3, 0, 1) + t (1, −2, 1) . Therefore, the
normal vector needs to be perpendicular to both direction vectors. You need n =
(2, 1, 1) × (1, −2, 1) = (3, −1, −5) . Now the equation of the ﬁrst plane is
(3, −1, −5) · (x −1, y −2, z) = 0
and the equation of the second plane is
(3, −1, −5) · (x −3, y, z −1) = 0
The two planes are therefore, 3x −y −5z = 1 and 3x −y −5z = 4. You see these are
parallel planes because they have the same normal vector and the ﬁrst contains the
ﬁrst line while the second contains the second line.
5. Here is an augmented matrix in which ∗denotes an arbitrary number and ■denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?




■
∗
∗
∗
∗
|
∗
0
■
∗
∗
0
|
∗
0
0
■
∗
∗
|
■
0
0
0
0
■
|
∗




In this case the system is consistent and there is an inﬁnite set of solutions. To see
it is consistent, the bottom equation would yield a unique solution for x5.
Then
letting x4 = t, and substituting in to the other equations, beginning with the equation
determined by the third row and then proceding up to the next row followed by the
ﬁrst row, you get a solution for each value of t. There is a free variable which comes
from the fourth column which is why you can say x4 = t. Therefore, the solution is
inﬁnite.
6. Here is an augmented matrix in which ∗denotes an arbitrary number and ■denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?


■
∗
∗
|
∗
0
0
■
|
■
0
0
∗
|
0



5.4.
THEORY OF ROW REDUCED ECHELON FORM∗
101
In this case there is no solution because you could use a row operation to place a 0 in
the third row and third column position, like this:


■
∗
∗
|
∗
0
0
■
|
■
0
0
0
|
■


This would give a row of zeros equal to something nonzero.
7. Find h such that
µ
1
h
|
4
3
7
|
7
¶
is the augmented matrix of an inconsistent matrix.
Doing a row operation by taking −3 times the top row and adding to the bottom, this
gives
µ
1
h
|
4
0
7 −3h
|
7 −12
¶
.
The system will be inconsistent if 7 −3h = 0 or in other words, h = 7/3.
8. Determine if the system is consistent.
x + 2y + 3z −w = 2
x −y + 2z + w = 1
2x + 3y −z = 1
4x + 2y + z = 5
The augmented matrix is




1
2
3
−1
|
2
1
−1
2
1
|
1
2
3
−1
0
|
1
4
2
1
0
|
5




A reduced echelon form for this is




9
0
0
0
|
14
0
9
0
0
|
−6
0
0
9
0
|
1
0
0
0
9
|
−13



.
Therefore, there is a unique solution. In particular the system is consistent.
9. Find the point, (x1, y1) which lies on both lines, 5x + 3y = 1 and 4x −y = 3.
You solve the system of equations whose augmented matrix is
µ
5
3
|
1
4
−1
|
3
¶
A reduced echelon form is
µ
17
0
10
0
17
−11
¶
and so the solution is x = 17/10 and y = −11/17.

102
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
10. Do the three lines, 3x + 2y = 1, 2x −y = 1, and 4x + 3y = 3 have a common point of
intersection? If so, ﬁnd the point and if not, tell why they don’t have such a common
point of intersection.
This is asking for the solution to the three equations shown. The augmented matrix
is


3
2
|
1
2
−1
|
1
4
3
|
3


A reduced echelon form is


1
0
|
0
0
1
|
0
0
0
|
1


and this would require 0x + 0y = 1 which is impossible so there is no solution to this
system of equations and hence no point on each of the three lines.
11. Find the general solution of the system whose augmented matrix is


1
2
0
|
2
1
1
4
|
2
2
3
4
|
4

.
A reduced echelon form for the matrix is


1
0
8
2
0
1
−4
0
0
0
0
0

.
Therefore, y = 4z and x = 2 −8z. Apparently z can equal anything so we let z = t
and then the solution is
x = 2 −8t, y = 4t, z = t.
12. Find the point, (x1, y1) which lies on both lines, x + 2y = 1 and 3x −y = 3.
The solution is y = 0 and x = 1.
13. Find the point of intersection of the two lines x + y = 3 and x + 2y = 1.
The solution is (5, −2) .
14. Do the three lines, x + 2y = 1, 2x −y = 1, and 4x + 3y = 3 have a common point of
intersection? If so, ﬁnd the point and if not, tell why they don’t have such a common
point of intersection.
To solve this set up the augmented matrix and go to work on it. The augmented
matrix is


1
2
|
1
2
−1
|
1
4
3
|
3


A reduced echelon matrix for this is


1
0
|
3
5
0
1
|
1
5
0
0
|
0


Therefore, there is a point in the intersection of these and it is y = 1/5 and x = 3/5.
Thus the point is (3/5, 1/5) .

5.4.
THEORY OF ROW REDUCED ECHELON FORM∗
103
15. Do the three planes, x + 2y −3z = 2, x + y + z = 1, and 3x + 2y + 2z = 0 have
a common point of intersection? If so, ﬁnd one and if not, tell why there is no such
point.
You need to ﬁnd (x, y, z) which solves each equation. The augmented matrix is


1
2
−3
|
2
1
1
1
|
1
3
2
2
|
0


A reduced echelon form for the matrix is


1
0
0
|
−2
0
1
0
|
13
5
0
0
1
|
2
5


and so you should let (x, y, z) = (−2, 13/5, 2/5) .
16. Here is an augmented matrix in which ∗denotes an arbitrary number and ■denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?




■
∗
∗
∗
∗
|
∗
0
■
∗
∗
0
|
∗
0
0
■
∗
∗
|
∗
0
0
0
0
■
|
∗




You could do another set of row operations and reduce the matrix to one of the form




■
∗
∗
∗
0
|
∗
0
■
∗
∗
0
|
∗
0
0
■
∗
0
|
∗
0
0
0
0
■
|
∗




It follows there exists a solution but the solution is not unique because x4 is a free
variable. You can pick it to be anything you like and the system will yield values for
the other variables.
17. Here is an augmented matrix in which ∗denotes an arbitrary number and ■denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?


■
∗
∗
|
∗
0
■
∗
|
∗
0
0
■
|
∗


In this case there is a unique solution to the system. To see this, you could do more
row operations and reduce this to something of the form


■
0
0
|
∗
0
■
0
|
∗
0
0
■
|
∗

.

104
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
18. Here is an augmented matrix in which ∗denotes an arbitrary number and ■denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?




■
∗
∗
∗
∗
|
∗
0
■
0
∗
0
|
∗
0
0
0
■
∗
|
∗
0
0
0
0
■
|
∗




In this case, you could do more row operations and get something of the form




■
0
∗
0
0
|
∗
0
■
0
0
0
|
∗
0
0
0
■
0
|
∗
0
0
0
0
■
|
∗




Now you can determine the answer.
19. Find h such that
µ
2
h
|
4
3
6
|
7
¶
is the augmented matrix of an inconsistent matrix.
Take −3 times the top row and add to 2 times the bottom. This yields
µ
2
h
|
4
0
12 −3h
|
2
¶
Now if h = 4 the system is inconsistent because it would have the bottom row equal
to
¡ 0
0
|
2
¢
.
20. Choose h and k such that the augmented matrix shown has one solution. Then choose
h and k such that the system has no solutions. Finally, choose h and k such that the
system has inﬁnitely many solutions.
µ
1
h
|
2
2
4
|
k
¶
.
If h ̸= 2 then k can be anything and the system represented by the augmented matrix
will have a unique solution. Suppose then that h = 2. Then taking −2 times the top
row and adding to the bottom row gives
µ
1
2
|
2
0
0
|
k −4
¶
If k ̸= 4 there is no solution. However, if k = 4 you are left with the single equation,
x+2y = 2 and there are inﬁnitely many solutions to this. In fact anything of the form
(2 −2y, y) will work just ﬁne.
21. Determine if the system is consistent.
x + 2y + z −w = 2
x −y + z + w = 1
2x + y −z = 1
4x + 2y + z = 5

5.4.
THEORY OF ROW REDUCED ECHELON FORM∗
105
This system is inconsistent.
To see this, write the augmented matrix and do row
operations. The augmented matrix is




1
2
1
−1
|
2
1
−1
1
1
|
1
2
1
−1
0
|
1
4
2
1
0
|
5




A reduced echelon form for this matrix is




1
0
0
1
3
|
0
0
1
0
−2
3
|
0
0
0
1
0
|
0
0
0
0
0
|
1




and the bottom row shows there is no solution.
22. Find the general solution of the system whose augmented matrix is


1
2
0
|
2
1
3
4
|
2
1
0
2
|
1


A reduced echelon form for this matrix is


1
0
0
|
6
5
0
1
0
|
2
5
0
0
1
|
−1
10


and so the solution is unique and is z = −1/10, y = 2/5, and x = 6/5.
23. Find the general solution of the system whose augmented matrix is
µ
1
1
0
|
5
1
0
3
|
2
¶
.
A reduced echelon form for this matrix is
µ
1
0
3
|
2
0
1
−3
|
3
¶
and so the general solution is of the form y = 3 + 3z, x = 2 −3z with z arbitrary.
24. Find the general solution of the system whose augmented matrix is




1
0
2
1
1
|
3
0
1
0
4
2
|
1
2
2
0
0
1
|
3
1
0
1
0
2
|
2



.
You do the usual thing, row operations on the matrix to obtain a reduced echelon
form. A reduced echelon form is




1
0
0
0
9
2
|
7
6
0
1
0
0
−4
|
1
3
0
0
1
0
−5
2
|
5
6
0
0
0
1
3
2
|
1
6




Therefore, the general solution is x4 = 1/6 −3/2x5, x3 = 5/6 + 5/2x5, x2 = 1/3 + 4x5,
and x1 = 7/6 −9/2x5 with x5 arbitrary.

106
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.

Part III
Linear Independence And
Matrices
107


109
Outcomes
Spanning sets and Linear Independence
A. Explain what is meant by the span of a set of vectors both geometrically and alge-
braically.
B. Determine the span of a set of vectors. Determine if a given vector is in the span of a
set of vectors.
C. Deﬁne linear independence.
D. Determine whether a set of vectors is linearly dependent or linearly independent. For
sets that are linearly dependent, determine a dependence relation.
E. Prove theorems about span and linear independence.
Reading: Linear Algebra 2.3
Supplemental Problems:
A1. Study the deﬁnition of linear independence. Write it from memory.
Outcome Mapping:
A. 13-16,17
B. 1-6,7-12
C. A1
D. 22-31
E. 18-21,42-48

110

Spanning Sets And Linear
Independence 18,19 Sept.
Quiz
1. Find a parametric equation for the line determined by the two points, (1, 2, 1) and
(2, −1, 3) .
2. Find an equation of the plane containing the point (0, 1, 0) and the line (1, 1, 1) +
t (2, −1, 1) .
3. An equation contains the point (0, 0, 0) and is perpendicular to the vector (1, 1, 1) .
Find an equation of this plane.
4. Here are some equations. Find the complete solution.
x + y + 4z = 1
−2x + y −2z = −2
x + 2z = 1
6.0.2
Spanning Sets
Deﬁnition 6.0.11 The vector, u is a linear combination of the vectors, v1, · ·
·, vm if there exist scalars, c1, · · ·, cm such that
u
=
c1v1 + c2v2 + · · · + cmvm
=
m
X
k=1
ckvk.
When u is a linear combination of {v1, · · ·, vm} , we say u is in the span of v1, · · ·, vm
written
u ∈span (v1, · · ·, vm) .
Equivalently, span (v1, · · ·, vm) equals the set of all linear combinations of the vectors v1, · ·
·, vm. If V = span (v1, · · ·, vm) , then {v1, · · ·, vm} is called a spanning set for V.
You can consider the geometric signiﬁcance of the span of a few vectors in three or two
dimensional space.
Example 6.0.12 Consider the span of one vector in R3.
111

112
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
¡
¡
¡
¡
©©©
*v
©©©©©©©©©©
x
y
z
You see there is a vector, v and the span of this single vector, {tv such that t ∈R} gives
the indicated line which goes through the origin, (0, 0, 0) having v as a direction vector.
Example 6.0.13 You can get an idea of the appearance of the span of two vectors in R3.
These are just planes which pass through the origin. Here is a picture.
y
z
x
¢
¢
¢
¢
¢
¢
¢
¢
¢
HHHHHHHHHH¢
¢
¢
¢
¢
¢
¢
¢
¢
H
H
H
H
H
H
H
HHH
¡

1u
v
¤
¤
¤
¤
Lets consider why the displayed plane really is the span of the two vectors which lie in
this plane as shown.

113
y
z
x
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
HHHHHHHHHHHHHHHHHHHH¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢
¢¢
¢
¢
¢
¢
H
H
H
H
H
H
H
H
H
H
H
H
H
H
HHHHHH
¡
¡

1







)
su
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
¤
tv + 0u
tv + s1u
tv + s2u
As indicated in the above picture, a typical thing in the span of these two vectors is of the
form su + tv where s and t are real numbers. By specifying s, you determine a point on the
line through the origin, (0, 0, 0) having direction vector, u. Then through this point, there
is a line having direction vector, v. We have drawn three such lines in the above picture,
one for s = 0, s1, and s2. The totality of all such lines yields the span of the two vectors, u
and v and you see from geometric considerations it is just a plane.
Geometric considerations such as these don’t take you anywhere because as soon as you
encounter more than three dimensions, you can’t draw a meaningful picture. The notions of
span and spanning set and linear combination are best understood according to the above
deﬁnition and are algebraic in nature. Here is an example.
Example 6.0.14
3


2
3
1

+ 5


1
−8
2

+ (−2)


1
3
7

=


9
−37
−1


Thus


9
−37
−1

is a linear combination of the vectors,


2
3
1

,


1
−8
2

, and


1
3
7

. In
this case the scalars are 3, 5, and −2.
The following theorem is nothing but a restatement of the deﬁnition of what it means
for a vector to be in the span of some other vectors.
Theorem 6.0.15 Let A be an m × n matrix and let b be an m × 1 vector. Then if
the columns of A are a1, · · ·, an, b ∈span (a1, · · ·, an) if and only if the system of equations

114
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
represented by the augmented matrix,
¡
A
|
b
¢
(6.1)
is consistent.
Proof: Suppose ﬁrst the system of equations just described is consistent. Let
ak =





a1k
a2k
...
amk




, b =





b1
b2
...
bm





To say the system is consistent is to say there exist x1, · · ·, xn solving the following system
of equations.
a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
am1x1 + am2x2 + · · · + amnxn = bm
(6.2)
But from the way we add vectors, this can be written as
x1





a11
a21
...
am1




+ x2





a12
a22
...
am2




+ · · · + xn





a1n
a2n
...
amn




=





b1
b2
...
bm





(6.3)
which says the same thing as b ∈span (a1, · · ·, an).
Next suppose b ∈span (a1, · · ·, an) . This says there exist scalars, x1, · · ·, xn such that
6.3 holds. This says the same thing as 6.2 and so the system of equations represented by
6.1 is consistent. This proves the theorem.
Example 6.0.16 Show that a spanning set for R3 is {e1, e2, e3} where
e1 =


1
0
0

, e2 =


0
1
0

, e3 =


0
0
1

.
This is really easy. If


x
y
z

∈R3, you can write it as a linear combination of the above
three vectors as follows.


x
y
z

= x


1
0
0

+ y


0
1
0

+ z


0
0
1

.
Of course it isn’t always so easy.
Example 6.0.17 Is


1
1
0

,


1
0
1

,


0
1
0


a spanning set for R3?

115
From Theorem 6.0.15 it is required to show the system of equations represented by the
augmented matrix,


1
1
0
|
a
1
0
1
|
b
0
1
0
|
c


has a solution for any choice of a, b, c. Take (−1) times the top row and add to the middle
row.


1
1
0
|
a
0
−1
1
|
b −a
0
1
0
|
c


Now take the second row and add to the bottom.


1
1
0
|
a
0
−1
1
|
b −a
0
0
1
|
c


You can see at this point that there will be a solution which you can obtain by back
substitution. Therefore, the vectors are a spanning set for R3.
Example 6.0.18 Is


1
1
0

,


1
0
1

,


2
1
1


a spanning set for R3?
By Theorem 6.0.15 you must consider the system of equations represented by


1
1
2
|
a
1
0
1
|
b
0
1
1
|
c


and see if there is a solution for any choice of a, b, c. Take (−1) times the top row and add
to the second.


1
1
2
|
a
0
−1
−1
|
b −a
0
1
1
|
c


Now add the second row to the bottom.


1
1
2
|
a
0
−1
−1
|
b −a
0
0
0
|
c + b −a


(6.4)
It follows that to obtain a solution to this system you must have c + b −a = 0. Therefore,
the vector,


0
1
0

fails to be in
span




1
1
0

,


1
0
1

,


2
1
1




along with many others.

116
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
Example 6.0.19 In the above example what is the span of the three given vectors?
Let a = b + c so 6.4 reduces to


1
1
2
|
a
0
−1
−1
|
b −a
0
0
0
|
0


and now you can add the middle row to the top row to obtain


1
0
1
|
b
0
−1
−1
|
b −a
0
0
0
|
0


Multiply the second row by (−1) to get the result in row reduced echelon form


1
0
1
|
b
0
1
1
|
a −b
0
0
0
|
0


Now there exists a solution to this. So what is the span of these vectors? It is


b + c
b
c

: b, c ∈R.
That is, you can take either b or c to be anything you want and put it in the above formula
for a vector and it will be in the span of the three vectors. Thus


2
1
1

,


0
1
−1

,


3
2
1


are examples of vectors in the span of





1
1
0

,


1
0
1

,


2
1
1




.
6.0.3
Linear Independence
When a vector is in the span of some other vectors you can say it is dependent on these
other vectors and that all the vectors involved are a dependent set of vectors. The precise
deﬁnition follows.
Deﬁnition 6.0.20 A set of vectors, {v1, · · ·, vn} is dependent if there exist scalars,
c1, c2, · · ·, cn not all zero such that
c1v1 + c2v2 + · · · + cnvn = 0.
People often refer to this as: There exists a nontrivial linear combination of the vectors which
equals zero. (It is nontrivial because some ck is nonzero.) The set of vectors, {v1, · · ·, vn}
is independent if it is not dependent. Thus there is no nontrivial linear combination which
equals zero. Or equivalently, if
c1v1 + c2v2 + · · · + cnvn = 0

117
then each ci = 0. If you ﬁnd scalars, c1, c2, · · ·, cn not all zero such that
c1v1 + c2v2 + · · · + cnvn = 0
this equation is called a dependence relation.
The following theorem is important.
Theorem 6.0.21 A set of vectors, {v1, · · ·, vn} is dependent if and only if one of
the vectors is a linear combination of the others.
Proof: Suppose ﬁrst that {v1, · · ·, vn} is dependent. Then there exist scalars, c1, ···, cn
not all zero such that
c1v1 + c2v2 + · · · + cnvn = 0
Let ck be one of the scalars which is not zero. Then from the above equation,
ckvk = −c1v1 −c2v2 · · · −ck−1vk−1 −ck+1vk · · · −cnvn
Now divide both sides by ck to obtain
vk
=
(−c1/ck) v1 + (−c2/ck) v2 + · · · + (−ck−1/ck) vk−1
+ (−ck+1/ck) vk + · · · + (−cn/ck) vn.
and this shows vk is a linear combination of the other vectors.
Now suppose
vk = d1v1 + · · · + dk−1vk−1 + dk+1vk+1 + · · · + dnvn.
Then
0 = d1v1 + · · · + dk−1vk−1 + (−1) vk + dk+1vk+1 + · · · + dnvn
and so there is a nontrivial linear combination which equals zero. In fact (−1) ̸= 0. This
proves the theorem.
Observation 6.0.22 Any set of vectors containing the zero vector is dependent. To see
this, multiply the zero vector by 1 and all the other vectors by 0 and then add them together.
You have a nontrivial linear combination equal to zero.
How can you tell if {a1, · · ·, an} is independent or dependent? It must be one or the
other. How can you determine which it is? Let
ak =





a1k
a2k
...
amk





Then a linear combination of the aj where aj is multiplied by the scalar, xj is of the form
x1





a11
a21
...
am1




+ x2





a12
a22
...
am2




+ · · · + xn





a1n
a2n
...
amn






118
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
which is the same as





x1a11 + x2a12 + · · · + xna1n
x1a21 + x2a22 + · · · + xna2n
...
x1am + x2a2m + · · · + xnamn





Therefore,
x1





a11
a21
...
am1




+ x2





a12
a22
...
am2




+ · · · + xn





a1n
a2n
...
amn




=





0
0
...
0





if and only if





x1a11 + x2a12 + · · · + xna1n = 0
x1a21 + x2a22 + · · · + xna2n = 0
...
x1am + x2a2m + · · · + xnamn = 0





Thus if A is an m × n matrix, the columns of A are dependent if and only if there exists
a nonzero (nontrivial) solution to the system of equations represented by the augmented
matrix,
¡ A
|
0 ¢
.
If {v1, · · ·, vm} are dependent, it follows from Theorem 6.0.21 that one of the vectors is
a linear combination of the others. Say vk is a linear combination of the others. This means
a suitable linear combination of the other vectors added to vk yields 0.
This leads directly to the following theorem.
6.0.4
Recognizing Linear Dependence
Theorem 6.0.23 Let {v1, · · ·, vm} be vectors in Rn. Make these vectors the rows
of a matrix, A. Thus A is of the form



−
v1
−
...
−
vm
−


.
Then the vectors are dependent if and only if rank (A) < m.
Proof: If the vectors are dependent, then a linear combination gives the zero vector.
Thus the row reduced echelon form has at least one row of zeros and so rank (A) < m.
If rank (A) < m, then the row reduced echelon form has at least one row of zeros. This
row of zeros was obtained from doing row operations and so the rows are dependent. This
proves the theorem.
Now recall Theorem 5.3.8 on Page 95 which is listed here for convenience.
Theorem 6.0.24 Let A be an m × n matrix. Form the augmented matrix,
¡
A
|
0
¢
(6.5)
so that A is the coeﬃcient matrix of a system of linear equations with n variables. Then the
number of free variables = n −rank (A) .

119
This theorem will be used to establish the following.
Theorem 6.0.25 Any set of n vectors in Rm is linearly dependent if n > m.
Proof: Let the set of n vectors be {a1, · · ·, an} and make them the column vectors of a
matrix, A. Thus A is of the form


|
|
a1
· · ·
an
|
|

.
Consider the augmented matrix of Theorem 5.3.8 listed above. The number of free variables
equals n −rank (A) and rank (A) is no more than m because there are only m rows in the
matrix. Therefore, the number of free variables in the system of equations represented by
6.5 equals n −rank (A) > n −m > 0. Since there exist free variables, there exist non zero
solutions to the system represented by 6.5 which implies a nontrivial linear combination of
the vectors equals zero. Thus the set of vectors is dependent. This proves the theorem.
6.0.5
Discovering Dependence Relations
Suppose you have some vectors, {v1, · · ·, vn} and you wonder whether they are independent
or dependent. If dependent, can you ﬁnd a dependence relation? How do you go about
answering this question? Recall the deﬁnition. You are looking for scalars, x1, · · ·, xn such
that
x1v1 + · · · + xnvn = 0
and you are trying to ﬁnd whether there are any nonzero solutions to this vector equation.
If the vectors, vi are in Fm, then the above is really just a homogeneous system of equations
because there is an equation for each component. Thus you form the augmented matrix,
¡
v1
· · ·
vn
|
0
¢
and row reduce to ﬁnd the solution. If the only solution is x1 = x2 = · · · = xn = 0, then
the vectors are linearly independent. If there exists something nonzero in the system of
equations, then you have produced a dependence relation.
Example 6.0.26 Here are some vectors in R3.


1
2
0

,


0
1
1

,


2
5
1

,


1
1
1

. I know
by Theorem 6.0.25 that these vectors are dependent. However, I would like to ﬁnd a depen-
dence relation.
To do this, I let them be the columns of an augmented matrix as shown.


1
0
2
1
|
0
2
1
5
1
|
0
0
1
1
1
|
0


Then I row reduce this in order to ﬁnd the solutions. The row reduced echelon form is


1
0
2
0
0
0
1
1
0
0
0
0
0
1
0

.

120
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
Therefore, The solutions are x4 = 0, x2 = −x3, and x1 = −2x3. Thus, letting x3 = t, all the
solutions are
(−2t, −t, t, 0) : t ∈R
and so if you let t = 1, you ﬁnd the dependence relation,
−2


1
2
0

+ (−1)


0
1
1

+ 1


2
5
1

+ 0


1
1
1

=


0
0
0

.
There is nothing new here at all! It is just another way of asking for the solution to a
homogeneous system of linear equations.1
1This is the style in linear algebra books these days, ask the same question over and over again in
disguised form to give the illusion of learning something new. However, there is much more to linear algebra
than row reduction of augmented matrices.

Matrices
7.1
Matrix Operations And Algebra 20,21 Sept.
Quiz
1. Here are three points: (1, 1, 1) , (2, 0, 1) , (0, 1, 0) . Find an equation of a plane which
contains all three points.
2. Find the equation of a plane which is parallel to the plane whose equations is x+2y +
z = 7 which contains the point (1, 2, 1) .
3. Here are three vectors: (1, 2, 1) , (2, 1, 0) , (−2, 0, 1) . Find the volume of the paral-
lelepiped determined by these three vectors.
4. Here are three vectors.


1
2
2




1
3
1




1
1
3

. Determine whether the vectors are
dependent. If they are dependent, ﬁnd a dependence relation.
5. Here is a system of equations.
3x + 4y + z = 4
x + 2y + z = 2
y + z = 1
Find the complete solution.
7.1.1
Addition And Scalar Multiplication Of Matrices
You have now solved systems of equations by writing them in terms of an augmented matrix
and then doing row operations on this augmented matrix. It turns out such rectangular
arrays of numbers are important from many other diﬀerent points of view. Numbers are
also called scalars. In these notes numbers will always be either real or complex numbers.
A matrix is a rectangular array of numbers. Several of them are referred to as matrices.
For example, here is a matrix.


1
2
3
4
5
2
8
7
6
−9
1
2


The size or dimension of a matrix is deﬁned as m × n where m is the number of rows and n
is the number of columns. The above matrix is a 3 × 4 matrix because there are three rows
and four columns. The ﬁrst row is (1 2 3 4) , the second row is (5 2 8 7) and so forth. The
121

122
MATRICES
ﬁrst column is


1
5
6

. When specifying the size of a matrix, you always list the number of
rows before the number of columns. Also, you can remember the columns are like columns
in a Greek temple. They stand upright while the rows just lay there like rows made by
a tractor in a plowed ﬁeld. Elements of the matrix are identiﬁed according to position in
the matrix. For example, 8 is in position 2, 3 because it is in the second row and the third
column. You might remember that you always list the rows before the columns by using
the phrase Rowman Catholic. The symbol, (aij) refersto a matrix. The entry in the ith
row and the jth column of this matrix is denoted by aij. Using this notation on the above
matrix, a23 = 8, a32 = −9, a12 = 2, etc.
There are various operations which are done on matrices. Matrices can be added mul-
tiplied by a scalar, and multiplied by other matrices. To illustrate scalar multiplication,
consider the following example in which a matrix is being multiplied by the scalar, 3.
3


1
2
3
4
5
2
8
7
6
−9
1
2

=


3
6
9
12
15
6
24
21
18
−27
3
6

.
The new matrix is obtained by multiplying every entry of the original matrix by the given
scalar. If A is an m × n matrix, −A is deﬁned to equal (−1) A.
Two matrices must be the same size to be added. The sum of two matrices is a matrix
which is obtained by adding the corresponding entries. Thus


1
2
3
4
5
2

+


−1
4
2
8
6
−4

=


0
6
5
12
11
−2

.
Two matrices are equal exactly when they are the same size and the corresponding entries
are identical. Thus


0
0
0
0
0
0

̸=
µ
0
0
0
0
¶
because they are diﬀerent sizes. As noted above, you write (cij) for the matrix C whose
ijth entry is cij. In doing arithmetic with matrices you must deﬁne what happens in terms
of the cij sometimes called the entries of the matrix or the components of the matrix.
The above discussion stated for general matrices is given in the following deﬁnition.
Deﬁnition 7.1.1 (Scalar Multiplication) If A = (aij) and k is a scalar, then kA =
(kaij) .
Example 7.1.2 7
µ
2
0
1
−4
¶
=
µ
14
0
7
−28
¶
.
Deﬁnition 7.1.3 (Addition) If A = (aij) and B = (bij) are two m × n matrices.
Then A + B = C where
C = (cij)
for cij = aij + bij.
Example 7.1.4
µ
1
2
3
1
0
4
¶
+
µ
5
2
3
−6
2
1
¶
=
µ
6
4
6
−5
2
5
¶

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
123
To save on notation, we will often use Aij to refer to the ijth entry of the matrix, A.
Deﬁnition 7.1.5 (The zero matrix) The m × n zero matrix is the m × n matrix
having every entry equal to zero. It is denoted by 0.
Example 7.1.6 The 2 × 3 zero matrix is
µ
0
0
0
0
0
0
¶
.
Note there are 2×3 zero matrices, 3×4 zero matrices, etc. In fact there is a zero matrix
for every size.
Deﬁnition 7.1.7 (Equality of matrices) Let A and B be two matrices. Then A = B
means that the two matrices are of the same size and for A = (aij) and B = (bij) , aij = bij
for all 1 ≤i ≤m and 1 ≤j ≤n.
The following properties of matrices can be easily veriﬁed. You should do so.
• Commutative Law Of Addition.
A + B = B + A,
(7.1)
• Associative Law for Addition.
(A + B) + C = A + (B + C) ,
(7.2)
• Existence of an Additive Identity
A + 0 = A,
(7.3)
• Existence of an Additive Inverse
A + (−A) = 0,
(7.4)
Also for α, β scalars, the following additional properties hold.
• Distributive law over Matrix Addition.
α (A + B) = αA + αB,
(7.5)
• Distributive law over Scalar Addition
(α + β) A = αA + βA,
(7.6)
• Associative law for Scalar Multiplication
α (βA) = αβ (A) ,
(7.7)
• Rule for Multiplication by 1.
1A = A.
(7.8)
As an example, consider the Commutative Law of Addition.
Let A + B = C and
B + A = D. Why is D = C?
Cij = Aij + Bij = Bij + Aij = Dij.
Therefore, C = D because the ijth entries are the same. Note that the conclusion follows
from the commutative law of addition of numbers.

124
MATRICES
7.1.2
Multiplication Of Matrices
Deﬁnition 7.1.8 Matrices which are n×1 or 1×n are called vectors and are often
denoted by a bold letter. Thus the n × 1 matrix
x =



x1
...
xn



is also called a column vector. The 1 × n matrix
(x1 · · · xn)
is called a row vector.
Although the following description of matrix multiplication may seem strange, it is in
fact the most important and useful of the matrix operations. To begin with consider the case
where a matrix is multiplied by a column vector. We will illustrate the general deﬁnition
by ﬁrst considering a special case.
µ
1
2
3
4
5
6
¶ 

7
8
9

=?
One way to remember this is as follows. Slide the vector, placing it on top the two rows as
shown and then do the indicated operation.


7
1
8
2
9
3
7
4
8
5
9
6

→
µ 7 × 1 + 8 × 2 + 9 × 3
7 × 4 + 8 × 5 + 9 × 6
¶
=
µ 50
122
¶
.
multiply the numbers on the top by the numbers on the bottom and add them up to get a
single number for each row of the matrix as shown above.
In more general terms,
µ
a11
a12
a13
a21
a22
a23
¶ 

x1
x2
x3

=
µ
a11x1 + a12x2 + a13x3
a21x1 + a22x2 + a23x3
¶
.
Another way to think of this is
x1
µ
a11
a21
¶
+ x2
µ
a12
a22
¶
+ x3
µ
a13
a23
¶
Thus you take x1 times the ﬁrst column, add to x2 times the second column, and ﬁnally
x3 times the third column. In general, here is the deﬁnition of how to multiply an (m × n)
matrix times a (n × 1) matrix.
Deﬁnition 7.1.9 Let A = Aij be an m × n matrix and let v be an n × 1 matrix,
v =



v1
...
vn




7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
125
Then Av is an m × 1 matrix and the ith component of this matrix is
(Av)i = Ai1v1 + Ai2v2 + · · · + Ainvn =
n
X
j=1
Aijvj.
Thus
Av =



Pn
j=1 A1jvj
...
Pn
j=1 Amjvj


.
(7.9)
In other words, if
A = (a1, · · ·, an)
where the ak are the columns,
Av =
n
X
k=1
vkak
This follows from 7.9 and the observation that the jth column of A is





A1j
A2j
...
Amj





so 7.9 reduces to
v1





A11
A21
...
Am1




+ v2





A12
A22
...
Am2




+ · · · + vn





A1n
A2n
...
Amn





Note also that multiplication by an m × n matrix takes an n × 1 matrix, and produces an
m × 1 matrix.
Here is another example.
Example 7.1.10 Compute


1
2
1
3
0
2
1
−2
2
1
4
1






1
2
0
1



.
First of all this is of the form (3 × 4) (4 × 1) and so the result should be a (3 × 1) . Note
how the inside numbers cancel. To get the element in the second row and ﬁrst and only
column, compute
4
X
k=1
a2kvk
=
a21v1 + a22v2 + a23v3 + a24v4
=
0 × 1 + 2 × 2 + 1 × 0 + (−2) × 1 = 2.

126
MATRICES
You should do the rest of the problem and verify


1
2
1
3
0
2
1
−2
2
1
4
1






1
2
0
1



=


8
2
5

.
The next task is to multiply an m × n matrix times an n × p matrix. Before doing so,
the following may be helpful.
For A and B matrices, in order to form the product, AB the number of columns of A
must equal the number of rows of B.
(m ×
these must match!
[
n) (n × p
) = m × p
Note the two outside numbers give the size of the product. Remember:
If the two middle numbers don’t match, you can’t multiply the matrices!
Deﬁnition 7.1.11 When the number of columns of A equals the number of rows of
B the two matrices are said to be conformable and the product, AB is obtained as follows.
Let A be an m × n matrix and let B be an n × p matrix. Then B is of the form
B = (b1, · · ·, bp)
where bk is an n × 1 matrix or column vector. Then the m × p matrix, AB is deﬁned as
follows:
AB ≡(Ab1, · · ·, Abp)
(7.10)
where Abk is an m × 1 matrix or column vector which gives the kth column of AB.
Example 7.1.12 Multiply the following.
µ 1
2
1
0
2
1
¶ 

1
2
0
0
3
1
−2
1
1


The ﬁrst thing you need to check before doing anything else is whether it is possible to
do the multiplication. The ﬁrst matrix is a 2×3 and the second matrix is a 3×3. Therefore,
is it possible to multiply these matrices. According to the above discussion it should be a
2 × 3 matrix of the form







First column
z
}|
{
µ
1
2
1
0
2
1
¶ 

1
0
−2

,
Second column
z
}|
{
µ
1
2
1
0
2
1
¶ 

2
3
1

,
Third column
z
}|
{
µ
1
2
1
0
2
1
¶ 

0
1
1









You know how to multiply a matrix times a vector and so you do so to obtain each of the
three columns. Thus
µ 1
2
1
0
2
1
¶ 

1
2
0
0
3
1
−2
1
1

=
µ −1
9
3
−2
7
3
¶
.

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
127
Example 7.1.13 Multiply the following.


1
2
0
0
3
1
−2
1
1


µ 1
2
1
0
2
1
¶
First check if it is possible. This is of the form (3 × 3) (2 × 3) . The inside numbers do not
match and so you can’t do this multiplication. This means that anything you write will be
absolute nonsense because it is impossible to multiply these matrices in this order. Aren’t
they the same two matrices considered in the previous example? Yes they are. It is just
that here they are in a diﬀerent order. This shows something you must always remember
about matrix multiplication.
Order Matters!
Matrix Multiplication Is Not Commutative!
This is very diﬀerent than multiplication of numbers!
7.1.3
The ijth Entry Of A Product
It is important to describe matrix multiplication in terms of entries of the matrices. What
is the ijth entry of AB? It would be the ith entry of the jth column of AB. Thus it would
be the ith entry of Abj. Now
bj =



B1j
...
Bnj



and from the above deﬁnition, the ith entry is
n
X
k=1
AikBkj.
(7.11)
In terms of pictures of the matrix, you are doing





A11
A12
· · ·
A1n
A21
A22
· · ·
A2n
...
...
...
Am1
Am2
· · ·
Amn










B11
B12
· · ·
B1p
B21
B22
· · ·
B2p
...
...
...
Bn1
Bn2
· · ·
Bnp





Then as explained above, the jth column is of the form





A11
A12
· · ·
A1n
A21
A22
· · ·
A2n
...
...
...
Am1
Am2
· · ·
Amn










B1j
B2j
...
Bnj





which is a m × 1 matrix or column vector which equals





A11
A21
...
Am1




B1j +





A12
A22
...
Am2




B2j + · · · +





A1n
A2n
...
Amn




Bnj.

128
MATRICES
The second entry of this m × 1 matrix is
A21B1j + A22B2j + · · · + A2nBnj =
m
X
k=1
A2kBkj.
Similarly, the ith entry of this m × 1 matrix is
Ai1B1j + Ai2B2j + · · · + AinBnj =
m
X
k=1
AikBkj.
This shows the following deﬁnition for matrix multiplication in terms of the ijth entries of
the product coincides with Deﬁnition 7.1.11.
Deﬁnition 7.1.14 Let A = (Aij) be an m×n matrix and let B = (Bij) be an n×p
matrix. Then AB is an m × p matrix and
(AB)ij =
n
X
k=1
AikBkj.
(7.12)
Example 7.1.15 Multiply if possible


1
2
3
1
2
6


µ
2
3
1
7
6
2
¶
.
First check to see if this is possible. It is of the form (3 × 2) (2 × 3) and since the inside
numbers match, the two matrices are conformable and it is possible to do the multiplication.
The result should be a 3 × 3 matrix. The answer is of the form




1
2
3
1
2
6


µ
2
7
¶
,


1
2
3
1
2
6


µ
3
6
¶
,


1
2
3
1
2
6


µ
1
2
¶

where the commas separate the columns in the resulting product. Thus the above product
equals


16
15
5
13
15
5
46
42
14

,
a 3 × 3 matrix as desired. In terms of the ijth entries and the above deﬁnition, the entry in
the third row and second column of the product should equal
X
j
a3kbk2
=
a31b12 + a32b22
=
2 × 3 + 6 × 6 = 42.
You should try a few more such examples to verify the above deﬁnition in terms of the ijth
entries works for other entries.
Example 7.1.16 Multiply if possible


1
2
3
1
2
6




2
3
1
7
6
2
0
0
0

.
This is not possible because it is of the form (3 × 2) (3 × 3) and the middle numbers
don’t match. In other words the two matrices are not conformable in the indicated order.

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
129
Example 7.1.17 Multiply if possible


2
3
1
7
6
2
0
0
0




1
2
3
1
2
6

.
This is possible because in this case it is of the form (3 × 3) (3 × 2) and the middle
numbers do match so the matrices are conformable. When the multiplication is done it
equals


13
13
29
32
0
0

.
Check this and be sure you come up with the same answer.
Example 7.1.18 Multiply if possible


1
2
1

¡
1
2
1
0
¢
.
In this case you are trying to do (3 × 1) (1 × 4) . The inside numbers match so you can
do it. Verify


1
2
1

¡ 1
2
1
0 ¢
=


1
2
1
0
2
4
2
0
1
2
1
0


7.1.4
Properties Of Matrix Multiplication
As pointed out above, sometimes it is possible to multiply matrices in one order but not
in the other order. What if it makes sense to multiply them in either order? Will the two
products be equal then?
Example 7.1.19 Compare
µ 1
2
3
4
¶ µ 0
1
1
0
¶
and
µ 0
1
1
0
¶ µ 1
2
3
4
¶
.
The ﬁrst product is
µ
1
2
3
4
¶ µ
0
1
1
0
¶
=
µ
2
1
4
3
¶
.
The second product is
µ 0
1
1
0
¶ µ 1
2
3
4
¶
=
µ 3
4
1
2
¶
.
You see these are not equal. Again you cannot conclude that AB = BA for matrix mul-
tiplication even when multiplication is deﬁned in both orders. However, there are some
properties which do hold.
Proposition 7.1.20 If all multiplications and additions make sense, the following hold
for matrices, A, B, C and a, b scalars.
A (aB + bC) = a (AB) + b (AC)
(7.13)
(B + C) A = BA + CA
(7.14)
A (BC) = (AB) C
(7.15)

130
MATRICES
Proof: Using Deﬁnition 7.1.14,
(A (aB + bC))ij
=
X
k
Aik (aB + bC)kj
=
X
k
Aik (aBkj + bCkj)
=
a
X
k
AikBkj + b
X
k
AikCkj
=
a (AB)ij + b (AC)ij
=
(a (AB) + b (AC))ij .
Thus A (B + C) = AB + AC as claimed. Formula 7.14 is entirely similar.
Formula 7.15 is the associative law of multiplication. Using Deﬁnition 7.1.14,
(A (BC))ij
=
X
k
Aik (BC)kj
=
X
k
Aik
X
l
BklClj
=
X
l
(AB)il Clj
=
((AB) C)ij .
This proves 7.15.
7.1.5
The Transpose
Another important operation on matrices is that of taking the transpose. The following
example shows what is meant by this operation, denoted by placing a T as an exponent on
the matrix.


1
4
3
1
2
6


T
=
µ
1
3
2
4
1
6
¶
What happened? The ﬁrst column became the ﬁrst row and the second column became the
second row. Thus the 3 × 2 matrix became a 2 × 3 matrix. The number 3 was in the second
row and the ﬁrst column and it ended up in the ﬁrst row and second column. Here is the
deﬁnition.
Deﬁnition 7.1.21 Let A be an m × n matrix. Then AT denotes the n × m matrix
which is deﬁned as follows.
¡
AT ¢
ij = Aji
Example 7.1.22
µ
1
2
−6
3
5
4
¶T
=


1
3
2
5
−6
4

.
The transpose of a matrix has the following important properties.
Lemma 7.1.23 Let A be an m × n matrix and let B be a n × p matrix. Then
(AB)T = BT AT
(7.16)

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
131
and if α and β are scalars,
(αA + βB)T = αAT + βBT
(7.17)
Proof: From the deﬁnition,
³
(AB)T ´
ij
=
(AB)ji
=
X
k
AjkBki
=
X
k
¡
BT ¢
ik
¡
AT ¢
kj
=
¡
BT AT ¢
ij
The proof of Formula 7.17 is left as an exercise and this proves the lemma.
Deﬁnition 7.1.24 An n × n matrix, A is said to be symmetric if A = AT . It is
said to be skew symmetric if A = −AT .
Example 7.1.25 Let
A =


2
1
3
1
5
−3
3
−3
7

.
Then A is symmetric.
Example 7.1.26 Let
A =


0
1
3
−1
0
2
−3
−2
0


Then A is skew symmetric.
7.1.6
The Identity And Inverses
There is a special matrix called I and referred to as the identity matrix. It is always a
square matrix, meaning the number of rows equals the number of columns and it has the
property that there are ones down the main diagonal and zeroes elsewhere. Here are some
identity matrices of various sizes.
(1) ,
µ
1
0
0
1
¶
,


1
0
0
0
1
0
0
0
1

,




1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1



.
The ﬁrst is the 1 × 1 identity matrix, the second is the 2 × 2 identity matrix, the third is
the 3 × 3 identity matrix, and the fourth is the 4 × 4 identity matrix. By extension, you can
likely see what the n × n identity matrix would be. It is so important that there is a special
symbol to denote the ijth entry of the identity matrix
Iij = δij
where δij is the Kroneker symbol deﬁned by
δij =
½
1 if i = j
0 if i ̸= j
It is called the identity matrix because it is a multiplicative identity in the following
sense.

132
MATRICES
Lemma 7.1.27 Suppose A is an m×n matrix and In is the n×n identity matrix. Then
AIn = A. If Im is the m × m identity matrix, it also follows that ImA = A.
Proof:
(AIn)ij
=
X
k
Aikδkj
=
Aij
and so AIn = A. The other case is left as an exercise for you.
Deﬁnition 7.1.28 An n×n matrix, A has an inverse, A−1 if and only if AA−1 =
A−1A = I. Such a matrix is called invertible.
It is very important to observe that the inverse of a matrix, if it exists, is unique. Another
way to think of this is that if it acts like the inverse, then it is the inverse.
Theorem 7.1.29 Suppose A−1 exists and AB = BA = I. Then B = A−1.
Proof:
A−1 = A−1I = A−1 (AB) =
¡
A−1A
¢
B = IB = B.
Unlike ordinary multiplication of numbers, it can happen that A ̸= 0 but A may fail to
have an inverse. This is illustrated in the following example.
Example 7.1.30 Let A =
µ
1
1
1
1
¶
. Does A have an inverse?
One might think A would have an inverse because it does not equal zero. However,
µ 1
1
1
1
¶ µ −1
1
¶
=
µ 0
0
¶
and if A−1 existed, this could not happen because you could write
µ
0
0
¶
= A−1
µµ
0
0
¶¶
= A−1
µ
A
µ
−1
1
¶¶
=
=
¡
A−1A
¢ µ
−1
1
¶
= I
µ
−1
1
¶
=
µ
−1
1
¶
,
a contradiction. Thus the answer is that A does not have an inverse.
Example 7.1.31 Let A =
µ
1
1
1
2
¶
. Show
µ
2
−1
−1
1
¶
is the inverse of A.
To check this, multiply
µ
1
1
1
2
¶ µ
2
−1
−1
1
¶
=
µ
1
0
0
1
¶
and
µ
2
−1
−1
1
¶ µ
1
1
1
2
¶
=
µ
1
0
0
1
¶
showing that this matrix is indeed the inverse of A.

7.2. FINDING THE INVERSE OF A MATRIX, GAUSS JORDAN METHOD 21,22 SEPT.133
7.2
Finding The Inverse Of A Matrix, Gauss Jordan
Method 21,22 Sept.
Quiz
1. Multiply the matrices if possible.
µ 1
1
2
0
1
1
¶ 

1
1
1
0
1
2


2. Multiply the matrices if possible.
¡
1
1
2
0
¢




1
0
1
1




3. Multiply the matrices if possible.




1
0
1
1




¡ 1
1
2
0 ¢
4. True or False. In each case the capital letters are matrices of an appropriate size and
the lower case letters represent numbers.
(a) A2 −B2 = (A −B) (A + B)
(b) (AB)T = AT BT
(c) (aA + bB) C = aAC + bCB
(d) If AB = 0, then either A = 0 or B = 0.
(e) A/A = 1
(f) (AB) C = A (BC)
In the last example, how would you ﬁnd A−1? You wish to ﬁnd a matrix,
µ
x
z
y
w
¶
such that
µ
1
1
1
2
¶ µ
x
z
y
w
¶
=
µ
1
0
0
1
¶
.
This requires the solution of the systems of equations,
x + y = 1, x + 2y = 0
and
z + w = 0, z + 2w = 1.
Writing the augmented matrix for these two systems gives
µ
1
1
|
1
1
2
|
0
¶
(7.18)

134
MATRICES
for the ﬁrst system and
µ
1
1
|
0
1
2
|
1
¶
(7.19)
for the second. Lets solve the ﬁrst system. Take (−1) times the ﬁrst row and add to the
second to get
µ
1
1
|
1
0
1
|
−1
¶
Now take (−1) times the second row and add to the ﬁrst to get
µ
1
0
|
2
0
1
|
−1
¶
.
Putting in the variables, this says x = 2 and y = −1.
Now solve the second system, 7.19 to ﬁnd z and w. Take (−1) times the ﬁrst row and
add to the second to get
µ
1
1
|
0
0
1
|
1
¶
.
Now take (−1) times the second row and add to the ﬁrst to get
µ
1
0
|
−1
0
1
|
1
¶
.
Putting in the variables, this says z = −1 and w = 1. Therefore, the inverse is
µ
2
−1
−1
1
¶
.
Didn’t the above seem rather repetitive? Note that exactly the same row operations
were used in both systems. In each case, the end result was something of the form (I|v)
where I is the identity and v gave a column of the inverse. In the above,
µ
x
y
¶
, the ﬁrst
column of the inverse was obtained ﬁrst and then the second column
µ
z
w
¶
.
To simplify this procedure, you could have written
µ
1
1
|
1
0
1
2
|
0
1
¶
and row reduced till you obtained
µ
1
0
|
2
−1
0
1
|
−1
1
¶
and read oﬀthe inverse as the 2 × 2 matrix on the right side.
This is the reason for the following simple procedure for ﬁnding the inverse of a matrix.
This procedure is called the Gauss-Jordan procedure.
Procedure 7.2.1 Suppose A is an n × n matrix. To ﬁnd A−1 if it exists, form the
augmented n × 2n matrix,
(A|I)
and then, if possible do row operations until you obtain an n × 2n matrix of the form
(I|B) .
(7.20)
When this has been done, B = A−1. If it is impossible to row reduce to a matrix of the form
(I|B) , then A has no inverse.

7.2. FINDING THE INVERSE OF A MATRIX, GAUSS JORDAN METHOD 21,22 SEPT.135
Example 7.2.2 Let A =


1
2
2
1
0
2
3
1
−1

. Find A−1 if it exists.
Set up the augmented matrix, (A|I)


1
2
2
|
1
0
0
1
0
2
|
0
1
0
3
1
−1
|
0
0
1


Next take (−1) times the ﬁrst row and add to the second followed by (−3) times the ﬁrst
row added to the last. This yields


1
2
2
|
1
0
0
0
−2
0
|
−1
1
0
0
−5
−7
|
−3
0
1

.
Then take 5 times the second row and add to -2 times the last row.


1
2
2
|
1
0
0
0
−10
0
|
−5
5
0
0
0
14
|
1
5
−2


Next take the last row and add to (−7) times the top row. This yields


−7
−14
0
|
−6
5
−2
0
−10
0
|
−5
5
0
0
0
14
|
1
5
−2

.
Now take (−7/5) times the second row and add to the top.


−7
0
0
|
1
−2
−2
0
−10
0
|
−5
5
0
0
0
14
|
1
5
−2

.
Finally divide the top row by -7, the second row by -10 and the bottom row by 14 which
yields







1
0
0
|
−1
7
2
7
2
7
0
1
0
|
1
2
−1
2
0
0
0
1
|
1
14
5
14
−1
7







.
Therefore, the inverse is







−1
7
2
7
2
7
1
2
−1
2
0
1
14
5
14
−1
7







Example 7.2.3 Let A =


1
2
2
1
0
2
2
2
4

. Find A−1 if it exists.

136
MATRICES
Write the augmented matrix, (A|I)


1
2
2
|
1
0
0
1
0
2
|
0
1
0
2
2
4
|
0
0
1


and proceed to do row operations attempting to obtain
¡
I|A−1¢
. Take (−1) times the top
row and add to the second. Then take (−2) times the top row and add to the bottom.


1
2
2
|
1
0
0
0
−2
0
|
−1
1
0
0
−2
0
|
−2
0
1


Next add (−1) times the second row to the bottom row.


1
2
2
|
1
0
0
0
−2
0
|
−1
1
0
0
0
0
|
−1
−1
1


At this point, you can see there will be no inverse because you have obtained a row of zeros
in the left half of the augmented matrix, (A|I) . Thus there will be no way to obtain I on
the left.
Example 7.2.4 Let A =


1
0
1
1
−1
1
1
1
−1

. Find A−1 if it exists.
Form the augmented matrix,


1
0
1
|
1
0
0
1
−1
1
|
0
1
0
1
1
−1
|
0
0
1

.
Now do row operations until the n × n matrix on the left becomes the identity matrix. This
yields after some computations,


1
0
0
|
0
1
2
1
2
0
1
0
|
1
−1
0
0
0
1
|
1
−1
2
−1
2


and so the inverse of A is the matrix on the right,


0
1
2
1
2
1
−1
0
1
−1
2
−1
2

.
Checking the answer is easy. Just multiply the matrices and see if it works.


1
0
1
1
−1
1
1
1
−1




0
1
2
1
2
1
−1
0
1
−1
2
−1
2

=


1
0
0
0
1
0
0
0
1

.
Always check your answer because if you are like some of us, you will usually have made a
mistake.

7.2. FINDING THE INVERSE OF A MATRIX, GAUSS JORDAN METHOD 21,22 SEPT.137
Example 7.2.5 In this example, it is shown how to use the inverse of a matrix to ﬁnd
the solution to a system of equations. Consider the following system of equations. Use the
inverse of a suitable matrix to give the solutions to this system.


x + z = 1
x −y + z = 3
x + y −z = 2

.
The system of equations can be written in terms of matrices as


1
0
1
1
−1
1
1
1
−1




x
y
z

=


1
3
2

.
(7.21)
More simply, this is of the form Ax = b. Suppose you ﬁnd the inverse of the matrix, A−1.
Then you could multiply both sides of this equation by A−1 to obtain
x =
¡
A−1A
¢
x = A−1 (Ax) = A−1b.
This gives the solution as x = A−1b. Note that once you have found the inverse, you can
easily get the solution for diﬀerent right hand sides without any eﬀort. It is always just
A−1b. In the given example, the inverse of the matrix is


0
1
2
1
2
1
−1
0
1
−1
2
−1
2


This was shown in Example 7.2.4. Therefore, from what was just explained the solution to
the given system is


x
y
z

=


0
1
2
1
2
1
−1
0
1
−1
2
−1
2




1
3
2

=


5
2
−2
−3
2

.
What if the right side of 7.21 had been


0
1
3

?
What would be the solution to


1
0
1
1
−1
1
1
1
−1




x
y
z

=


0
1
3

?
By the above discussion, it is just


x
y
z

=


0
1
2
1
2
1
−1
0
1
−1
2
−1
2




0
1
3

=


2
−1
−2

.
This illustrates why once you have found the inverse of a given matrix, you can use it to
solve many diﬀerent systems easily.
Here is a formula for the inverse of a 2×2 matrix.

138
MATRICES
Theorem 7.2.6 Let A =
µ a
b
c
d
¶
where ad −bc ̸= 0. Then
A−1 =
1
ad −cb
µ
d
−b
−c
a
¶
.
Proof: Just multiply and verify it works.
1
ad −cb
µ
d
−b
−c
a
¶ µ
a
b
c
d
¶
=
µ
1
0
0
1
¶
.
Therefore, this is indeed the inverse.
The expression, ad−cb is the determinant of the given matrix. Recall, this was discussed
in connection with the cross product. This will be discussed in more generality later.
7.3
Elementary Matrices 22 Sept.
Quiz
1. Here is a matrix.


1
−1
0
1
−1
2
−1
2
−1
3
2
1
2


Find its inverse.
2. The inverse of


1
1
2
1
2
0
1
2
1
2
1
−1
0

is


1
−1
0
1
−1
−1
−1
3
1

. Use this fact to write the so-
lution to the system


x + 1
2y + 1
2z = a
1
2y + 1
2z = b
x −y = c


in terms of a, b, c.
The elementary matrices result from doing a row operation to the identity matrix.
Deﬁnition 7.3.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to it.
The elementary matrices are given in the following deﬁnition.
Deﬁnition 7.3.2 The elementary matrices consist of those matrices which result by
applying a row operation to an identity matrix. Those which involve switching rows of the
identity are called permutation matrices1.
1More generally, a permutation matrix is a matrix which comes by permuting the rows of the identity
matrix, not just switching two rows.

7.3.
ELEMENTARY MATRICES 22 SEPT.
139
As an example of why these elementary matrices are interesting, consider the following.


0
1
0
1
0
0
0
0
1




a
b
c
d
x
y
z
w
f
g
h
i

=


x
y
z
w
a
b
c
d
f
g
h
i


A 3×4 matrix was multiplied on the left by an elementary matrix which was obtained from
row operation 1 applied to the identity matrix. This resulted in applying the operation 1
to the given matrix. This is what happens in general.
Now consider what these elementary matrices look like. First consider the one which
involves switching row i and row j where i < j. This matrix is of the form
























1
0
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
0
0
...
...
...
1
...
0
· · ·
0
0
· · ·
· · ·
0
1
· · ·
· · ·
0
...
...
1
0
· · ·
0
...
...
...
...
...
...
0
· · ·
· · ·
0
· · ·
0
1
0
· · ·
· · ·
0
0
· · ·
0
1
0
· · ·
· · ·
0
· · ·
· · ·
0
...
1
...
...
...
0
0
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
0
1
























The two exceptional rows are shown. The ith row was the jth and the jth row was the ith
in the identity matrix. Now consider what this does to a column vector.
























1
0
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
0
0
...
...
...
1
...
0
· · ·
0
0
· · ·
· · ·
0
1
· · ·
· · ·
0
...
...
1
0
· · ·
0
...
...
...
...
...
...
0
· · ·
· · ·
0
· · ·
0
1
0
· · ·
· · ·
0
0
· · ·
0
1
0
· · ·
· · ·
0
· · ·
· · ·
0
...
1
...
...
...
0
0
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
0
1

















































v1
...
...
vi
...
...
...
vj
...
...
vn

























=

























v1
...
...
vj
...
...
...
vi
...
...
vn

























Now denote by P ij the elementary matrix which comes from the identity from switching
rows i and j. From what was just explained consider multiplication on the left by this

140
MATRICES
elementary matrix.
P ij













a11
a12
· · ·
· · ·
· · ·
· · ·
a1p
...
...
...
ai1
ai2
· · ·
· · ·
· · ·
· · ·
aip
...
...
...
aj1
aj2
· · ·
· · ·
· · ·
· · ·
ajp
...
...
...
an1
an2
· · ·
· · ·
· · ·
· · ·
anp













From the way you multiply matrices this is a matrix which has the indicated collumns.













P ij













a11
...
ai1
...
aj1
...
an1













, P ij













a12
...
ai2
...
aj2
...
an2













, · · ·, P ij













a1p
...
aip
...
ajp
...
anp


























=


























a11
...
aj1
...
ai1
...
an1













,













a12
...
aj2
...
ai2
...
an2













, · · ·,













a1p
...
ajp
...
aip
...
anp


























=













a11
a12
· · ·
· · ·
· · ·
· · ·
a1p
...
...
...
aj1
aj2
· · ·
· · ·
· · ·
· · ·
ajp
...
...
...
ai1
ai2
· · ·
· · ·
· · ·
· · ·
aip
...
...
...
an1
an2
· · ·
· · ·
· · ·
· · ·
anp













This has established the following lemma.
Lemma 7.3.3 Let P ij denote the elementary matrix which involves switching the ith and
the jth rows. Then
P ijA = B
where B is obtained from A by switching the ith and the jth rows.
Next consider the row operation which involves multiplying the ith row by a nonzero
constant, c. The elementary matrix which results from applying this operation to the ith

7.3.
ELEMENTARY MATRICES 22 SEPT.
141
row of the identity matrix is of the form















1
0
· · ·
· · ·
· · ·
· · ·
0
0
...
...
...
1
...
...
c
...
...
1
...
...
...
0
0
· · ·
· · ·
· · ·
· · ·
0
1















Now consider what this does to a column vector.















1
0
· · ·
· · ·
· · ·
· · ·
0
0
...
...
...
1
...
...
c
...
...
1
...
...
...
0
0
· · ·
· · ·
· · ·
· · ·
0
1



























v1
...
vi−1
vi
vi+1
...
vn












=












v1
...
vi−1
cvi
vi+1
...
vn












Denote by E (c, i) this elementary matrix which multiplies the ith row of the identity by the
nonzero constant, c. Then from what was just discussed and the way matrices are multiplied,
E (c, i)













a11
a12
· · ·
· · ·
· · ·
· · ·
a1p
...
...
...
ai1
ai2
· · ·
· · ·
· · ·
· · ·
aip
...
...
...
aj2
aj2
· · ·
· · ·
· · ·
· · ·
ajp
...
...
...
an1
an2
· · ·
· · ·
· · ·
· · ·
anp














142
MATRICES
equals a matrix having the columns indicated below.
=













E (c, i)













a11
...
ai1
...
aj1
...
an1













, E (c, i)













a12
...
ai2
...
aj2
...
an2













, · · ·, E (c, i)













a1p
...
aip
...
ajp
...
anp


























=













a11
a12
· · ·
· · ·
· · ·
· · ·
a1p
...
...
...
cai1
cai2
· · ·
· · ·
· · ·
· · ·
caip
...
...
...
aj2
aj2
· · ·
· · ·
· · ·
· · ·
ajp
...
...
...
an1
an2
· · ·
· · ·
· · ·
· · ·
anp













This proves the following lemma.
Lemma 7.3.4 Let E (c, i) denote the elementary matrix corresponding to the row oper-
ation in which the ith row is multiplied by the nonzero constant, c. Thus E (c, i) involves
multiplying the ith row of the identity matrix by c. Then
E (c, i) A = B
where B is obtained from A by multiplying the ith row of A by c.
Finally consider the third of these row operations. Denote by E (c × i + j) the elementary
matrix which replaces the jth row with itself added to c times the ith row added to it. In
case i < j this will be of the form















1
0
· · ·
· · ·
· · ·
0
0
0
...
...
...
1
...
...
...
...
...
...
c
· · ·
1
...
...
...
0
0
· · ·
· · ·
· · ·
· · ·
0
1
















7.3.
ELEMENTARY MATRICES 22 SEPT.
143
Now consider what this does to a column vector.















1
0
· · ·
· · ·
· · ·
0
0
0
...
...
...
1
...
...
...
...
...
...
c
· · ·
1
...
...
...
0
0
· · ·
· · ·
· · ·
· · ·
0
1




























v1
...
vi
...
vj
...
vn













=













v1
...
vi
...
cvi + vj
...
vn













Now from this and the way matrices are multiplied,
E (c × i + j)













a11
a12
· · ·
· · ·
· · ·
· · ·
a1p
...
...
...
ai1
ai2
· · ·
· · ·
· · ·
· · ·
aip
...
...
...
aj2
aj2
· · ·
· · ·
· · ·
· · ·
ajp
...
...
...
an1
an2
· · ·
· · ·
· · ·
· · ·
anp













equals a matrix of the following form having the indicated columns.













E (c × i + j)













a11
...
ai1
...
aj2
...
an1













, E (c × i + j)













a12
...
ai2
...
aj2
...
an2













, · · ·E (c × i + j)













a1p
...
aip
...
ajp
...
anp


























=













a11
a12
· · ·
· · ·
· · ·
· · ·
a1p
...
...
...
ai1
ai2
· · ·
· · ·
· · ·
· · ·
aip
...
...
...
aj2 + cai1
aj2 + cai2
· · ·
· · ·
· · ·
· · ·
ajp + caip
...
...
...
an1
an2
· · ·
· · ·
· · ·
· · ·
anp













The case where i > j is handled similarly. This proves the following lemma.
Lemma 7.3.5 Let E (c × i + j) denote the elementary matrix obtained from I by replac-
ing the jth row with c times the ith row added to it. Then
E (c × i + j) A = B
where B is obtained from A by replacing the jth row of A with itself added to c times the
ith row of A.

144
MATRICES
The next theorem is the main result.
Theorem 7.3.6 To perform any of the three row operations on a matrix, A it
suﬃces to do the row operation on the identity matrix obtaining an elementary matrix, E
and then take the product, EA. Furthermore, each elementary matrix is invertible and its
inverse is an elementary matrix.
Proof: The ﬁrst part of this theorem has been proved in Lemmas 7.3.3 - 7.3.5.
It
only remains to verify the claim about the inverses. Consider ﬁrst the elementary matrices
corresponding to row operation of type three.
E (−c × i + j) E (c × i + j) = I
This follows because the ﬁrst matrix takes c times row i in the identity and adds it to row j.
When multiplied on the left by E (−c × i + j) it follows from the ﬁrst part of this theorem
that you take the ith row of E (c × i + j) which coincides with the ith row of I since that
row was not changed, multiply it by −c and add to the jth row of E (c × i + j) which was
the jth row of I added to c times the ith row of I. Thus E (−c × i + j) multiplied on the
left, undoes the row operation which resulted in E (c × i + j). The same argument applied
to the product
E (c × i + j) E (−c × i + j)
replacing c with −c in the argument yields that this product is also equal to I. Therefore,
E (c × i + j)−1 = E (−c × i + j) .
Similar reasoning shows that for E (c, i) the elementary matrix which comes from mul-
tiplying the ith row by the nonzero constant, c,
E (c, i)−1 = E
¡
c−1, i
¢
.
Finally, consider P ij which involves switching the ith and the jth rows.
P ijP ij = I
because by the ﬁrst part of this theorem, multiplying on the left by P ij switches the ith
and jth rows of P ij which was obtained from switching the ith and jth rows of the identity.
First you switch them to get P ij and then you multiply on the left by P ij which switches
these rows again and restores the identity matrix. Thus
¡
P ij¢−1 = P ij.
Corollary 7.3.7 Let A be an invertible n × n matrix. Then A equals a ﬁnite product of
elementary matrices.
Proof: Since A−1 is given to exist, it follows A must have rank n and so the row reduced
echelon form of A is I. Therefore, by Theorem 7.3.6 there is a sequence of elementary
matrices, E1, · · ·, Ep which accomplish successive row operations such that
(EpEp−1 · · · E1) A = I.
But now multiply on the left on both sides by E−1
p
then by E−1
p−1 and then by E−1
p−2 etc.
until you get
A = E−1
1 E−1
2
· · · E−1
p−1E−1
p
and by Theorem 7.3.6 each of these in this product is an elementary matrix.

7.4.
BLOCK MULTIPLICATION OF MATRICES
145
7.4
Block Multiplication Of Matrices
Consider the following problem
µ
A
B
C
D
¶ µ
E
F
G
H
¶
You know how to do this. You get
µ
AE + BG
AF + BH
CE + DG
CF + DH
¶
.
Now what if instead of numbers, the entries, A, B, C, D, E, F, G are matrices of a size such
that the multiplications and additions needed in the above formula all make sense. Would
the formula be true in this case? I will show below that it is true.
Suppose A is a matrix of the form



A11
· · ·
A1m
...
...
...
Ar1
· · ·
Arm



(7.22)
where Aij is a si × pj matrix where si does not depend on j and pj does not depend on
i. Such a matrix is called a block matrix, also a partitioned matrix. Let n = P
j pj
and k = P
i si so A is an k × n matrix. What is Ax where x ∈Fn? From the process of
multiplying a matrix times a vector, the following lemma follows.
Lemma 7.4.1 Let A be an m × n block matrix as in 7.22 and let x ∈Fn. Then Ax is of
the form
Ax =



P
j A1jxj
...
P
j Arjxj



where x = (x1, · · ·, xm)T and xi ∈Fpi.
Suppose also that B is a block matrix of the form



B11
· · ·
B1p
...
...
...
Br1
· · ·
Brp



(7.23)
and A is a block matrix of the form



A11
· · ·
A1m
...
...
...
Ap1
· · ·
Apm



(7.24)
and that for all i, j, it makes sense to multiply BisAsj for all s ∈{1, · · ·, m}. (That is the
two matrices, Bis and Asj are conformable.) and that for each s, BisAsj is the same size so
that it makes sense to write P
s BisAsj.
Theorem 7.4.2 Let B be a block matrix as in 7.23 and let A be a block matrix as
in 7.24 such that Bis is conformable with Asj and each product, BisAsj is of the same size
so they can be added. Then BA is a block matrix such that the ijth block is of the form
X
s
BisAsj.
(7.25)

146
MATRICES
Proof: Let Bis be a qi × ps matrix and Asj be a ps × rj matrix. Also let x ∈Fn and
let x = (x1, · · ·, xm)T and xi ∈Fri so it makes sense to multiply Asjxj. Then from the
associative law of matrix multiplication and Lemma 7.4.1 applied twice,






B11
· · ·
B1p
...
...
...
Br1
· · ·
Brp






A11
· · ·
A1m
...
...
...
Ap1
· · ·
Apm









x1
...
xm



=



B11
· · ·
B1p
...
...
...
Br1
· · ·
Brp






P
j A1jxj
...
P
j Arjxj



=



P
s
P
j B1sAsjxj
...
P
s
P
j BrsAsjxj


=



P
j (P
s B1sAsj) xj
...
P
j (P
s BrsAsj) xj



=



P
s B1sAs1
· · ·
P
s B1sAsm
...
...
...
P
s BrsAs1
· · ·
P
s BrsAsm






x1
...
xm



By Lemma 7.4.1, this shows that (BA) x equals the block matrix whose ijth entry is given
by 7.25 times x. Since x is an arbitrary vector in Fn, this proves the theorem.
The message of this theorem is that you can formally multiply block matrices as though
the blocks were numbers. You just have to pay attention to the preservation of order.
7.4.1
Exercises With Answers
1. Here are some matrices:
A
=


1
2
3
2
3
7
1
0
1

, B =
µ
3
−1
2
−3
2
1
¶
,
C
=


1
2
3
1
1
1

, D =
µ
−1
2
2
−3
¶
, E =
µ
2
3
¶
.
Find if possible −3A, 3B −A, AC, CB, EA, DCT If it is not possible explain why.
−3A = −3


1
2
3
2
3
7
1
0
1

=


−3
−6
−9
−6
−9
−21
−3
0
−3


3B −A is nonsense because the matrices B and A are not of the same size.
AC =


1
2
3
2
3
7
1
0
1




1
2
3
1
1
1

=


10
7
18
14
2
3


There is no problem here because you are doing (3 × 3) (3 × 2) .
CB =


1
2
3
1
1
1


µ
3
−1
2
−3
2
1
¶
=


−3
3
4
6
−1
7
0
1
3



7.4.
BLOCK MULTIPLICATION OF MATRICES
147
There is no problem here because you are doing (3 × 2) (2 × 3) and the inside numbers
match. EA is nonsense because it is of the form (2 × 1) (3 × 3) so since the inside
numbers do not match the matrices are not conformable.
DCT =
µ −1
2
2
−3
¶ µ 1
3
1
2
1
1
¶
=
µ
3
−1
1
−4
3
−1
¶
.
2. Let A =
µ
0
2
3
4
¶
, B =
µ
1
2
1
k
¶
. Is it possible to choose k such that AB = BA?
If so, what should k equal?
We just multiply and see if it can happen.
AB =
µ
0
2
3
4
¶ µ
1
2
1
k
¶
=
µ
2
2k
7
6 + 4k
¶
.
On the other hand,
BA =
µ
1
2
1
k
¶ µ
0
2
3
4
¶
=
µ
6
10
3k
2 + 4k
¶
.
If these were equal you would need to have 6 = 2 which is not the case. Therefore,
there is no way to choose k such that these two matrices will commute.
3. Let x = (−1, 0, 3) and y = (3, 1, 2) . Find xT y.
xT y =


−1
0
3

¡
3
1
2
¢
=


−3
−1
−2
0
0
0
9
3
6

.
4. Write




4x1 −x2 + 2x3
2x3 + 7x1
2x3
3x3 + 3x2 + x1



in the form A




x1
x2
x3
x4



where A is an appropriate matrix.




4
−1
2
0
7
0
2
0
0
0
2
0
1
3
3
0








x1
x2
x3
x4



.
5. Let
A =


1
2
5
2
1
4
1
0
2

.
Find A−1 if possible. If A−1 does not exist, determine why.


1
2
5
2
1
4
1
0
2


−1
=


−2
3
4
3
−1
0
1
−2
1
3
−2
3
1

.
6. Let
A =




1
2
0
2
1
5
2
0
2
1
−3
2
1
2
1
2





148
MATRICES
Find A−1 if possible. If A−1 does not exist, determine why.




1
2
0
2
1
5
2
0
2
1
−3
2
1
2
1
2




−1
=




−3
1
6
5
6
13
6
1
1
6
−1
6
−5
6
−1
0
0
1
1
−1
4
−1
4
−1
4



.
7. Show that if A−1 exists for an n × n matrix, then it is unique. That is, if BA = I and
AB = I, then B = A−1.
From AB = I, multiply both sides by A−1. Thus A−1 (AB) = A−1. Then from the
associative property of matrix multiplication, A−1 = A−1 (AB) =
¡
A−1A
¢
B = IB =
B.
8. Suppose A, B are two matrices. Show (AB)−1 = B−1A−1.
All you have to do is multiply it. If it acts like the inverse, it is the inverse.
¡
B−1A−1¢
(AB) = B−1 ¡
A−1A
¢
B = B−1IB = B−1B = I.
Therefore, B−1A−1 = (AB)−1 .
9. Show
¡
A−1¢T =
¡
AT ¢−1.
AT ¡
A−1¢T =
¡
A−1A
¢T = IT = I and so
¡
A−1¢T =
¡
AT ¢−1.
10. Here are elementary matrices. Find their inverses.
(a)


1
0
0
a
1
0
0
0
1




1
0
0
a
1
0
0
0
1


−1
=


1
0
0
−a
1
0
0
0
1


(b)


1
0
0
0
2
0
0
0
1




1
0
0
0
2
0
0
0
1


−1
=


1
0
0
0
1
2
0
0
0
1


(c)


0
1
0
1
0
0
0
0
1




0
1
0
1
0
0
0
0
1


−1
=


0
1
0
1
0
0
0
0
1


11. When you have an n × n matrix, A, An =
n times
z
}|
{
A × A × · · · × A. If A−1 exists, show
¡
A−1¢n = (An)−1.

7.4.
BLOCK MULTIPLICATION OF MATRICES
149
The equation is true if n = 1. Suppose it is true for n. Then by the induction hypoth-
esis,
¡
A−1¢n+1
=
¡
A−1¢n A−1 = (An)−1 A−1
=
(A (An))−1 =
¡
An+1¢−1 .

150
MATRICES

Part IV
LU Decomposition, Subspaces,
Linear Transformations
151


153
Outcomes
A. Find the LU factorization of a matrix.
B. Use the LU factorization of a matrix to solve a system of linear equations.
*C. Find the PT LU factorization of a matrix.
*D. Use that PT LU factorization of a matrix to solve a system of linear equations.
*E. Find the inverse of a matrix using the LU factorization.
Reading: Linear Algebra 3.4
Outcome Mapping:
A. 7-12,13-14
B. 1-6
*C. 19-22,23-25
*D. 27-28
*E. 15-18,30
A. Deﬁne subspace of Rn.
Determine whether or not a given set of vectors forms a
subspace of Rn.
B. Deﬁne row space, column space, and null space for a matrix. Determine whether or
not a given vector is in one of these spaces.
C. Deﬁne basis and dimension. Given a subspace, determine its dimension and a basis.
Verify whether or not a given set of vectors is a basis for the subspace.
D. Deﬁne rank and nullity. Determine the rank and nullity of a given matrix.
*E. Prove and recall theorems involving the rank, nullity, and invertibility of matrices.
*F. Find the coordinates of a vector with respect to a given basis.
Reading: Linear Algebra 3.5
Outcome Mapping:
A. 1-10
B. 11-16
C. 17-20,21-26,27-30,31-34,45-48
D. 35-42,43-44
*E. 55-64
*F. 49-50
A. Deﬁne linear transformation.
Determine whether or not a given transformation is
linear.

154
B. Determine the matrix that represents a given linear transformation of vectors.
C. Prove and recall theorems involving linear transformations.
*D. Find compositions and inverses of linear transformations.
Reading: Linear Algebra 3.6
Outcome Mapping:
A. 1-10,46-51
B. 11-14,15-28
C. 29,40-45,52-55
*D. 30-35,36-39

The LU Factorization 25 Sept.
8.0.2
Deﬁnition Of An LU Decomposition
An LU decomposition of a matrix involves writing the given matrix as the product of a
lower triangular matrix which has the main diagonal consisting entirely of ones L, and an
upper triangular matrix U in the indicated order. This is the version discussed here but it
is sometimes the case that the L has numbers other than 1 down the main diagonal. It is
still a useful concept. The L goes with “lower” and the U with “upper”. It turns out many
matrices can be written in this way and when this is possible, people get excited about slick
ways of solving the system of equations, Ax = y. It is for this reason that you want to study
the LU decomposition. It allows you to work only with triangular matrices. It turns out
that it takes about 2n3/3 operations to use Gauss elimination but only n3/3 to obtain an
LU factorization.
First it should be noted not all matrices have an LU decomposition and so we will
emphasize the techniques for achieving it rather than formal proofs.
Example 8.0.3 Can you write
µ
0
1
1
0
¶
in the form LU as just described?
To do so you would need
µ
1
0
x
1
¶ µ
a
b
0
c
¶
=
µ
a
b
xa
xb + c
¶
=
µ
0
1
1
0
¶
.
Therefore, b = 1 and a = 0. Also, from the bottom rows, xa = 1 which can’t happen
and have a = 0. Therefore, you can’t write this matrix in the form LU. It has no LU
decomposition. This is what we mean above by saying the method lacks generality.
8.0.3
Finding An LU Decomposition By Inspection
Which matrices have an LU decomposition? It turns out it is those whose row reduced
echelon form can be achieved without switching rows and which only involve row operations
of type 3 in which row j is replaced with a multiple of row i added to row j for i < j.
Example 8.0.4 Find an LU decomposition of A =


1
2
0
2
1
3
2
1
2
3
4
0

.
One way to ﬁnd the LU decomposition is to simply look for it directly. You need


1
2
0
2
1
3
2
1
2
3
4
0

=


1
0
0
x
1
0
y
z
1




a
d
h
j
0
b
e
i
0
0
c
f

.
155

156
THE LU FACTORIZATION 25 SEPT.
Then multiplying these you get


a
d
h
j
xa
xd + b
xh + e
xj + i
ya
yd + zb
yh + ze + c
yj + iz + f


and so you can now tell what the various quantities equal. From the ﬁrst column, you
need a = 1, x = 1, y = 2. Now go to the second column. You need d = 2, xd + b = 3 so
b = 1, yd + zb = 3 so z = −1. From the third column, h = 0, e = 2, c = 6. Now from the
fourth column, j = 2, i = −1, f = −5. Therefore, an LU decomposition is


1
0
0
1
1
0
2
−1
1




1
2
0
2
0
1
2
−1
0
0
6
−5

.
You can check whether you got it right by simply multiplying these two.
8.0.4
Using Multipliers To Find An LU Decomposition
There is also a convenient procedure for ﬁnding an LU decomposition. It turns out that it
is only necessary to keep track of the multipliers which are used to row reduce to upper
triangular form. This procedure is described in the following examples.
Example 8.0.5 Find an LU decomposition for A =


1
2
3
2
1
−4
1
5
2


Write the matrix next to the identity matrix as shown.


1
0
0
0
1
0
0
0
1




1
2
3
2
1
−4
1
5
2

.
The process involves doing row operations to the matrix on the right while simultaneously
updating successive columns of the matrix on the left. First take −2 times the ﬁrst row and
add to the second in the matrix on the right.


1
0
0
2
1
0
0
0
1




1
2
3
0
−3
−10
1
5
2


Note the way we updated the matrix on the left. We put a 2 in the second entry of the ﬁrst
column because we used −2 times the ﬁrst row added to the second row. Now replace the
third row in the matrix on the right by −1 times the ﬁrst row added to the third. Thus the
next step is


1
0
0
2
1
0
1
0
1




1
2
3
0
−3
−10
0
3
−1


Finally, we will add the second row to the bottom row and make the following changes


1
0
0
2
1
0
1
−1
1




1
2
3
0
−3
−10
0
0
−11

.
At this point, we stop because the matrix on the right is upper triangular. An LU decom-
position is the above.
The justiﬁcation for this gimmick is in my linear algebra book on the web.

157
Example 8.0.6 Find an LU decomposition for A =




1
2
1
2
1
2
0
2
1
1
2
3
1
3
2
1
0
1
1
2



.
We will use the same procedure as above. However, this time we will do everything for
one column at a time. First multiply the ﬁrst row by (−1) and then add to the last row.
Next take (−2) times the ﬁrst and add to the second and then (−2) times the ﬁrst and add
to the third.




1
0
0
0
2
1
0
0
2
0
1
0
1
0
0
1








1
2
1
2
1
0
−4
0
−3
−1
0
−1
−1
−1
0
0
−2
0
−1
1



.
This ﬁnishes the ﬁrst column of L and the ﬁrst column of U. Now take −(1/4) times the
second row in the matrix on the right and add to the third followed by −(1/2) times the
second added to the last.




1
0
0
0
2
1
0
0
2
1/4
1
0
1
1/2
0
1








1
2
1
2
1
0
−4
0
−3
−1
0
0
−1
−1/4
1/4
0
0
0
1/2
3/2




This ﬁnishes the second column of L as well as the second column of U. Since the matrix
on the right is upper triangular, stop. The LU decomposition has now been obtained. This
technique is called Dolittle’s method.
This process is entirely typical of the general case. The matrix U is just the ﬁrst upper
triangular matrix you come to in your quest for the row reduced echelon form using only
the row operation which involves replacing a row by itself added to a multiple of another
row. The matrix, L is what you get by updating the identity matrix as illustrated above.
You should note that for a square matrix, the number of row operations necessary to
reduce to LU form is about half the number needed to place the matrix in row reduced
echelon form. This is why an LU decomposition is of interest in solving systems of equations.
8.0.5
Solving Systems Using The LU Decomposition
The reason people care about the LU decomposition is it allows the quick solution of systems
of equations. Here is an example.
Example 8.0.7 Suppose you want to ﬁnd the solutions to


1
2
3
2
4
3
1
1
1
2
3
0






x
y
z
w



=


1
2
3

.
Of course one way is to write the augmented matrix and grind away. However, this
involves more row operations than the computation of the LU decomposition and it turns
out that the LU decomposition can give the solution quickly. Here is how. The following is
an LU decomposition for the matrix.


1
2
3
2
4
3
1
1
1
2
3
0

=


1
0
0
4
1
0
1
0
1




1
2
3
2
0
−5
−11
−7
0
0
0
−2

.

158
THE LU FACTORIZATION 25 SEPT.
Let Ux = y and consider Ly = b where in this case, b = (1, 2, 3)T . Thus


1
0
0
4
1
0
1
0
1




y1
y2
y3

=


1
2
3


which yields very quickly that y =


1
−2
2

. Now you can ﬁnd x by solving Ux = y. Thus
in this case,


1
2
3
2
0
−5
−11
−7
0
0
0
−2






x
y
z
w



=


1
−2
2


which yields
x =






−3
5 + 7
5t
9
5 −11
5 t
t
−1






, t ∈R.

Rank Of A Matrix 26,27 Sept.
Quiz
1. Here is a matrix. Find an LU factorization.


1
2
0
3
−2
−3
−4
1
−1
2
1
1


2. An LU factorization for


1
2
0
3
−1
−4
−4
1
−1
−5
1
6

is


1
0
0
−1
1
0
−1
3
2
1




1
2
0
3
0
−2
−4
4
0
0
7
3


Use this to solve the system


1
2
0
3
−1
−4
−4
1
−1
−5
1
6






x
y
z
w



=


0
0
0


Show your work. You must use the LU factorization to receive any credit.
9.1
The Row Reduced Echelon Form Of A Matrix
Recall the row operations used to solve a system of equations which were presented earlier.
Deﬁnition 9.1.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to itself.
Recall that putting a matrix in row reduced echelon form involves doing row operations
as described on Page 88. In this section we review the description of the row reduced echelon
form and prove the row reduced echelon form for a given matrix is unique. That is, every
matrix can be row reduced to a unique row reduced echelon form. Of course this is not true
159

160
RANK OF A MATRIX 26,27 SEPT.
of the echelon form. The signiﬁcance of this is that it becomes possible to use the deﬁnite
article in referring to the row reduced echelon form and hence important conclusions about
the original matrix may be logically deduced from an examination of its unique row reduced
echelon form. Also recall the deﬁnition of linear combination and span.
Deﬁnition 9.1.2 Let v1, · · ·, vk, u be vectors. Then u is said to be a linear com-
bination of the vectors {v1, · · ·, vk} if there exist scalars, c1, · · ·, ck such that
u =
k
X
i=1
civi.
The collection of all linear combinations of the vectors, {v1, · · ·, vk} is known as the span
of these vectors and is written as span (v1, · · ·, vk).
Another way to say the same thing as expressed in the earlier deﬁnition of row reduced
echelon form found on Page 86 is the following which is a more useful description when
proving the major assertions about the row reduced echelon form.
Deﬁnition 9.1.3 Let ei denote the column vector which has all zero entries except
for the ith slot which is one. An m × n matrix is said to be in row reduced echelon form
if, in viewing successive columns from left to right, the ﬁrst nonzero column encountered is
e1 and if you have encountered e1, e2, · · ·, ek, the next column is either ek+1 or is a linear
combination of the vectors, e1, e2, · · ·, ek.
Theorem 9.1.4 Let A be an m × n matrix. Then A has a row reduced echelon
form determined by a simple process.
Proof: Viewing the columns of A from left to right take the ﬁrst nonzero column. Pick
a nonzero entry in this column and switch the row containing this entry with the top row of
A. Now divide this new top row by the value of this nonzero entry to get a 1 in this position
and then use row operations to make all entries below this entry equal to zero. Thus the
ﬁrst nonzero column is now e1. Denote the resulting matrix by A1. It has been obtained
from A through a sequence of row operations.
Consider the submatrix of A1 to the right of this column and below the ﬁrst row. Do
exactly the same thing for this submatrix that was done for A. This time the e1 will refer to
Fm−1. Use the ﬁrst 1 obtained by the above process which is in the top row of this submatrix
and row operations to zero out every entry above it in the rows of A1. Call the resulting
matrix, A2. Thus A2 satisﬁes the conditions of the above deﬁnition up to the column just
encountered. Continue this way till every column has been dealt with and the result must
be in row reduced echelon form. This proves the theorem.
The following diagram illustrates the above procedure. Say the matrix looked something
like the following.





0
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
∗
∗
...
...
...
...
...
...
...
0
∗
∗
∗
∗
∗
∗





First step would yield something like





0
1
∗
∗
∗
∗
∗
0
0
∗
∗
∗
∗
∗
...
...
...
...
...
...
...
0
0
∗
∗
∗
∗
∗






9.1.
THE ROW REDUCED ECHELON FORM OF A MATRIX
161
For the second step you look at the lower right corner as described,



∗
∗
∗
∗
∗
...
...
...
...
...
∗
∗
∗
∗
∗



and if the ﬁrst column consists of all zeros but the next one is not all zeros, you would get
something like this.



0
1
∗
∗
∗
...
...
...
...
...
0
0
∗
∗
∗



Thus, after zeroing out the term in the top row above the 1, you get the following for the
next step in the computation of the row reduced echelon form for the original matrix.





0
1
∗
0
∗
∗
∗
0
0
0
1
∗
∗
∗
...
...
...
...
...
...
...
0
0
0
0
∗
∗
∗




.
Next you look at the lower right matrix below the top two rows and to the right of the ﬁrst
four columns and repeat the process.
Recall the following deﬁnition which was discussed earlier.
Deﬁnition 9.1.5 The ﬁrst pivot column of A is the ﬁrst nonzero column of A.
The next pivot column is the ﬁrst column after this which becomes e2 in the row reduced
echelon form. The third is the next column which becomes e3 in the row reduced echelon
form and so forth.
There are three choices for row operations at each step in the above theorem. A natural
question is whether the same row reduced echelon matrix always results in the end from
following the above algorithm applied in any way. The next corollary says this is the case.
To prove this corollary, the following fundamental lemma will be used.
In rough terms, this lemma states that linear relationships between columns in a
matrix are preserved by row operations.
Lemma 9.1.6 Let A and B be two m × n matrices and suppose B results from a row
operation applied to A. Then the kth column of B is a linear combination of the i1, · · ·, ir
columns of B if and only if the kth column of A is a linear combination of the i1, · · ·, ir
columns of A. Furthermore, the scalars in the linear combination are the same. (The linear
relationship between the kth column of A and the i1, · · ·, ir columns of A is the same as the
linear relationship between the kth column of B and the i1, · · ·, ir columns of B.)
Proof: Let A equal the following matrix in which the ak are the columns
¡
a1
a2
· · ·
an
¢
and let B equal the following matrix in which the columns are given by the bk
¡
b1
b2
· · ·
bn
¢
Then by Theorem 7.3.6 on Page 144 bk = Eak where E is an elementary matrix. Suppose
then that one of the columns of A is a linear combination of some other columns of A. Say
ak =
X
r∈S
crar.

162
RANK OF A MATRIX 26,27 SEPT.
Then multiplying by E,
bk = Eak =
X
r∈S
crEar =
X
r∈S
crbr.
This proves the lemma.
Deﬁnition 9.1.7 Two matrices are said to be row equivalent if one can be ob-
tained from the other by a sequence of row operations.
It has been shown above in Theorem 9.1.4 that every matrix is row equivalent to one
which is in row reduced echelon form.
Corollary 9.1.8 The row reduced echelon form is unique. That is if B, C are two ma-
trices in row reduced echelon form and both are row equivalent to A, then B = C.
Proof: Suppose B and C are both row reduced echelon forms for the matrix, A. Then
they clearly have the same zero columns since row operations leave zero columns unchanged.
If B has the sequence e1, e2, · · ·, er occuring for the ﬁrst time in the positions, i1, i2, · · ·, ir
the description of the row reduced echelon form means that if bk is the kth column of B such
that ij−1 < k < ij then bk is a linear combination of the columns in positions i1, i2, ···, ik−1.
By Lemma 9.1.6 the same is true for ck, the kth column of C. Therefore, ck is not equal
to ej for any j because ej is not obtained as a linear combinations of the ei for i < j. It
follows the ej for C can only occur in positions i1, i2, · · ·, ir. Furthermore, position ij in C
must contain ej because if not, then cij would be a linear combination of e1, · · ·, ej−1 in C
but not in B, thus contradicting Lemma 9.1.6. Therefore, both B and C have the sequence
e1, e2, · · ·, er occuring for the ﬁrst time in the positions, i1, i2, · · ·, ir. By Lemma 9.1.6, the
columns between the ik and ik+1 position are linear combinations involving the same scalars
of the columns in the i1, · · ·, ik position. This is equivalent to the assertion that each of
these columns is identical and this proves the corollary.
Example 9.1.9 Find the row reduced echelon form of the matrix,


0
0
2
3
0
2
0
1
0
1
1
5


The ﬁrst nonzero column is the second in the matrix. Switch the third and ﬁrst rows to
obtain


0
1
1
5
0
2
0
1
0
0
2
3


Now we multiply the top row by −2 and add to the second.


0
1
1
5
0
0
−2
−9
0
0
2
3


Next, add the second row to the bottom and then divide the bottom row by −6


0
1
1
5
0
0
−2
−9
0
0
0
1



9.2.
THE RANK OF A MATRIX
163
Next use the bottom row to obtain zeros in the last column above the 1 and divide the
second row by −2


0
1
1
0
0
0
1
0
0
0
0
1


Finally, add −1 times the middle row to the top.


0
1
0
0
0
0
1
0
0
0
0
1

.
This is in row reduced echelon form.
Example 9.1.10 Find the row reduced echelon form for the matrix,


1
2
0
2
−1
3
4
3
0
5
4
5


You should verify that the row reduced echelon form is


1
0
−8
5
0
0
1
4
5
1
0
0
0
0

.
9.2
The Rank Of A Matrix
9.2.1
The Deﬁnition Of Rank
To begin, here is a deﬁnition to introduce some terminology.
Deﬁnition 9.2.1 Let A be an m×n matrix. The column space of A is the subspace
of Fm spanned by the columns. The row space is the subspace of Fn spanned by the rows.
Earlier the rank was deﬁned to be the number of nonzero rows in the row reduced echelon
form. This is ﬁne. However, it is useful to tie this in to the notion of spans of columns and
rows.
Deﬁnition 9.2.2 The row space of a matrix, A is the span of the rows, denoted as
row (A) and the column space of a matrix is the span of the columns, denoted as col (A).
The row rank of a matrix is the number of nonzero rows in the row reduced echelon form
and the column rank is the number of columns in the row reduced echelon form which are
one of the ek. Thus the column rank equals the number of pivot columns. Thus the row rank
equals the column rank. This is also called the rank of the matrix. The rank of a matrix, A
is denoted by rank (A) .
Example 9.2.3 Consider the matrix,
µ 1
2
3
2
4
6
¶
What is its rank?

164
RANK OF A MATRIX 26,27 SEPT.
The row reduced echelon form is
µ 1
2
3
0
0
0
¶
. There is one pivot column so the column
rank is 1. There is one nonzero row so the row rank is 1.
Example 9.2.4 Find the rank of the matrix,




1
2
1
3
0
−4
3
2
1
2
3
2
1
6
5
4
−3
−2
1
7



.
From the above deﬁnition, all you have to do is ﬁnd the row reduced echelon form and
then count up the number of nonzero rows.
But the row reduced echelon form of this
matrix is




1
0
0
0
−17
4
0
1
0
0
1
0
0
1
0
−45
4
0
0
0
1
9
2




and so the rank of this matrix is 4.
Example 9.2.5 Find the rank of the matrix




1
2
1
3
0
−4
3
2
1
2
3
2
1
6
5
0
7
4
10
7




The row reduced echelon form is




1
0
0
3
2
5
2
0
1
0
−4
−17
0
0
1
19
2
63
2
0
0
0
0
0




and so this time the rank is 3.
9.2.2
Finding The Row And Column Space Of A Matrix
The row reduced echelon form also can be used to obtain an eﬃcient description of the row
and column space of a matrix. Of course you can get the column space by simply saying
that it equals the span of all the columns but often you can get the column space as the
span of fewer columns than this. This is what we mean by an “eﬃcient description”. This
is illustrated in the next example.
Example 9.2.6 Find the rank of the following matrix and describe the column and row
spaces eﬃciently.


1
2
1
3
2
1
3
6
0
2
3
7
8
6
6


(9.1)
The row reduced echelon form is


1
0
−9
9
2
0
1
5
−3
0
0
0
0
0
0

.

9.2.
THE RANK OF A MATRIX
165
Therefore, the rank of this matrix equals 2. All columns of this row reduced echelon form
are in
span




1
0
0

,


0
1
0



.
For example,


−9
5
0

= −9


1
0
0

+ 5


0
1
0

.
By Lemma 9.1.6, all columns of the original matrix, are similarly contained in the span
of the ﬁrst two columns of that matrix. For example, consider the third column of the
original matrix.


1
6
8

= −9


1
1
3

+ 5


2
3
7

.
How did I know to use −9 and 5 for the coeﬃcients? This is what Lemma 9.1.6 says! It says
linear relationships are all preserved. Therefore, the column space of the original matrix
equals the span of the ﬁrst two columns. This is the desired eﬃcient description of the
column space.
What about an eﬃcient description of the row space? When row operations are used, the
resulting vectors remain in the row space. Thus the rows in the row reduced echelon form
are in the row space of the original matrix. Furthermore, by reversing the row operations,
each row of the original matrix can be obtained as a linear combination of the rows in the
row reduced echelon form. It follows that the span of the nonzero rows in the row reduced
echelon equals the span of the original rows. In the above example, the row space equals
the span of the two vectors,
¡
1
0
−9
9
2
¢
and
¡
0
1
5
−3
0
¢
.
Example 9.2.7 Find the rank of the following matrix and describe the column and row
spaces eﬃciently.




1
2
1
3
2
1
3
6
0
2
1
2
1
3
2
1
3
2
4
0




(9.2)
The row reduced echelon form is




1
0
0
0
13
2
0
1
0
2
−5
2
0
0
1
−1
1
2
0
0
0
0
0



.
and so the rank is 3, the row space is the span of the vectors,
¡
0
0
1
−1
1
2
¢
,
¡
0
1
0
2
−5
2
¢
,
and
¡
1
0
0
0
13
2
¢
and the column space is the span of the ﬁrst three columns in the original matrix,
span








1
1
1
1



,




2
3
2
3



,




1
6
1
2







.

166
RANK OF A MATRIX 26,27 SEPT.
Example 9.2.8 Find the rank of the following matrix and describe the column and row
spaces eﬃciently.


1
2
3
0
1
2
1
3
2
4
−1
2
1
3
1

.
The row reduced echelon form is


1
0
1
0
21
17
0
1
1
0
−2
17
0
0
0
1
14
17

.
It follows the rank is three and the column space is the span of the ﬁrst, second and fourth
columns of the original matrix.
span




1
2
−1

,


2
1
2

,


0
2
3




while the row space is the span of the vectors
¡
0
0
0
1
14
17
¢
,
¡
0
1
1
0
−2
17
¢
,
and
¡
1
0
1
0
21
17
¢
.
Procedure 9.2.9 To ﬁnd the rank of a matrix, obtain the row reduced echelon form
for the matrix. Then count the number of nonzero rows or equivalently the number of pivot
columns. This is the rank. The row space is the span of the nonzero rows in the row reduced
echelon form and the column space is the span of the pivot columns of the original matrix.
9.3
Linear Independence And Bases
Quiz
1. Let u = (1, 2, 1) and v = (1, 1, 2) . Find proju (v) .
2. Here are two vectors:


1
1
2

,


0
1
1

. Find an equation of the plane which equals
the span of these two vectors.
3. Here is a matrix:


1
2
1
1
0
1
2
1
3

. Find an LU factorization of this matrix.
4. Here are three points: (1, 2, 1) , (2, 1, 0) , (0, 1, 1) . Find the area of the triangle deter-
mined by these three points.
9.3.1
Linear Independence And Dependence
First we review the concept of linear independence and dependence which was presented
earlier. We deﬁne what it means for vectors in Fn to be linearly independent and then give
equivalent descriptions. In the following discussion, the symbol,
¡
v1
v2
· · ·
vk
¢
denotes the matrix which has the vector, v1 as the ﬁrst column, v2 as the second column
and so forth until vk is the kth column.

9.3.
LINEAR INDEPENDENCE AND BASES
167
Deﬁnition 9.3.1 Let {v1, · · ·, vk} be vectors in Fn. Then this collection of vectors
is said to be linearly independent if whenever
k
X
i=1
civi = 0
it follows each ci = 0. If this condition does not hold, then the set of vectors is said to be
dependent.
The following theorem is very important.
Theorem 9.3.2 A set of vectors, {v1, · · ·, vk} is linearly independent if and only
if none of the vectors is a linear combination of the others.
Proof: Suppose {v1, · · ·, vk} is linearly independent. If vk = Pk−1
i=1 civi, then 0 = Pk−1
i=1 civi+
(−1) vk and this would mean the vectors are not linearly independent after all. Therefore,
vk cannot be a linear combination of the other vectors. Similarly, none of the other vi can
be a linear combination of the other vectors.
Next suppose none of the vectors is a linear combination of the others. If Pk
i=1 civi = 0,
then if some cl ̸= 0, you could write
clvl = −
X
i̸=l
civi
and then divide by cl to obtain
vl = −
X
i̸=l
µci
cl
¶
vi
showing that one of the vectors is a linear combination of the others which, by assump-
tion, does not happen. Therefore, each of the ci = 0 which shows {v1, · · ·, vk} is linearly
independent. This proves the theorem.
In words, this says a set of vectors is linearly independent if and only if none of the
vectors is “dependent” on the other vectors.
Lemma 9.3.3 The set of vectors, {v1, · · ·, vk} ⊆Fm is linearly independent if and only if
whenever {w1, · · ·, wr} is a set of vectors in Fm,each of the ﬁrst k columns of the n×(k + r)
matrix
¡
v1
v2
· · ·
vk
w1
· · ·
wr
¢
is a pivot column.
Proof: Suppose ﬁrst {v1, · · ·, vk} is linearly independent. Then by Theorem 9.3.2 none
of the vk is a linear combination of the others. It follows each must be a pivot column vk
becoming ek in the row reduced echelon form. If this didn’t happen, then you could apply
Lemma 9.1.6 and conclude one of the vk is a combination of the others.
Next suppose each are pivot columns. Then the row reduced echelon form of the above
matrix is of the form
¡
e1
e2
· · ·
ek
w′
1
· · ·
w′
r
¢
.
None of the ek is a linear combination of the others and so by Lemma 9.1.6, the same is
true of the {v1, · · ·, vk} . In other words {v1, · · ·, vk} is independent.
Corollary 9.3.4 Let {v1, · · ·, vk} be a set of vectors in Fn. Then if k > n, it must
be the case that {v1, · · ·, vk} is not linearly independent. In other words, if k > n, then
{v1, · · ·, vk} is dependent.

168
RANK OF A MATRIX 26,27 SEPT.
Proof: If k > n, then the columns of
¡
v1
v2
· · ·
vk
¢
cannot each be a pivot
column because there are at most n pivot columns due to the fact the matrix has only n
rows.
Example 9.3.5 Determine whether the vectors,











1
2
3
0








2
1
0
1








0
1
1
2








3
2
2
−1











are
linearly independent. If they are linearly dependent, exhibit one of the vectors as a linear
combination of the others.
Form the matrix mentioned above.




1
2
0
3
2
1
1
2
3
0
1
2
0
1
2
−1




Then the row reduced echelon form of this matrix is




1
0
0
1
0
1
0
1
0
0
1
−1
0
0
0
0



.
Thus not all the columns are pivot columns and so the vectors are not linear independent.
Note the fourth column is of the form
1




1
0
0
0



+ 1




0
1
0
0



+ (−1)




1
1
−1
0




From Lemma 9.1.6, the same linear relationship exists between the columns of the original
matrix. Thus
1




1
2
3
0



+ 1




2
1
0
1



+ (−1)




0
1
1
2



=




3
2
2
−1



.
Note the usefullness of the row reduced echelon form in discovering hidden linear rela-
tionships in collections of vectors.
Example 9.3.6 Determine whether the vectors,











1
2
3
0








2
1
0
1








0
1
1
2








3
2
2
0











are lin-
early independent.
If they are linearly dependent, exhibit one of the vectors as a linear
combination of the others.
The matrix used to ﬁnd this is




1
2
0
3
2
1
1
2
3
0
1
2
0
1
2
0





9.3.
LINEAR INDEPENDENCE AND BASES
169
The row reduced echelon form is




1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1




and so every column is a pivot column. Therefore, these vectors are linearly independent
and there is no way to obtain one of the vectors as a linear combination of the others.
9.3.2
Subspaces
It turns out that the span of a set of vectors is something called a subspace. What follows is
an easier to remember description of subspaces. Furthermore, every such thing is the span
of a set of vectors.
Deﬁnition 9.3.7 Let V be a nonempty collection of vectors in Fn. Then V is called
a subspace if whenever α, β are scalars and u, v are vectors in V, the linear combination,
αu + βv is also in V .
Theorem 9.3.8 V is a subspace of Fn if and only if there exist vectors of V,
{u1, · · ·, uk} such that V = span (u1, · · ·, uk) .
Proof: Pick a vector of V, u1. If V = span {u1} , then stop. You have found your
list of vectors. If V ̸= span (u1) , then there exists u2 a vector of V which is not a vector
in span (u1) . Consider span (u1, u2) . If V = span (u1, u2) , stop.
Otherwise, pick u3 /∈
span (u1, u2) . Continue this way. Here is why {u1, · · ·, uk} is a linearly independent set.
Suppose it is not so. Then you can let l be the largest index such that ul is a linear
combination of the other vectors. Then
ul =
l−1
X
i=1
ciui +
k
X
j=l+1
djuj.
By the construction, at least one of the dj must be nonzero since otherwise, ul would be a
linear combination of the preceding vectors which is not allowed by the construction. But
then you could solve for that uj in terms of the other vectors and contradict the choice of l.
Therefore, from Corollary 9.3.4 the process stops when k is no larger than n. This proves
one half of the theorem.
For the other half, suppose V = span (u1, · · ·, uk) and let Pk
i=1 ciui and Pk
i=1 diui be
two vectors in V. Now let α and β be two scalars. Then
α
k
X
i=1
ciui + β
k
X
i=1
diui =
k
X
i=1
(αci + βdi) ui
which is one of the things in span (u1, · · ·, uk) showing that span (u1, · · ·, uk) has the prop-
erties of a subspace. This proves the theorem.
Contained within the proof is the following corollary.
Corollary 9.3.9 If V is a subspace of Fn, then there exist vectors of V, {u1, · · ·, uk}
such that V = span (u1, · · ·, uk) and {u1, · · ·, uk} is linearly independent.
Proof: In the proof we eventually obtain {u1, · · ·, uk} such that V = span (u1, · · ·, uk)
and {u1, · · ·, uk} is linearly independent.
The message is that subspaces of Fn consist of spans of ﬁnite, linearly independent
collections of vectors of Fn.

170
RANK OF A MATRIX 26,27 SEPT.
9.3.3
The Basis Of A Subspace
It was just shown in Corollary 9.3.9 that every subspace of Fn is equal to the span of a
linearly independent collection of vectors of Fn.
Such a collection of vectors is called a
basis.
Deﬁnition 9.3.10 Let V be a subspace of Fn. Then {u1, · · ·, uk} is a basis for V
if the following two conditions hold.
1. span (u1, · · ·, uk) = V.
2. {u1, · · ·, uk} is linearly independent.
The plural of basis is bases.1
The main theorem about bases is the following.
Theorem 9.3.11 Let V be a subspace of Fn and suppose {u1, · · ·, uk} and {v1, · · ·, vm}
are two bases for V . Then k = m.
Proof: Suppose k < m. Then since {u1, · · ·, uk} is a basis for V, each vi is a linear
combination of the vectors of {u1, · · ·, uk} . Consider the matrix
¡
u1
· · ·
uk
v1
· · ·
vm
¢
in which each of the ui is a pivot column by Lemma 9.3.3 because the {u1, · · ·, uk} are
linearly independent. Therefore, the row reduced echelon form of this matrix is
¡
e1
· · ·
ek
w1
· · ·
wm
¢
(9.3)
where each wj has zeroes below the kth row. This is because of Lemma 9.1.6 which implies
each wi is a linear combination of the e1, ···, ek due to the fact each vk is a linear combination
of the uj vectors. Discarding the bottom n−k rows of zeroes in the above, yields the matrix,
¡
e′
1
· · ·
e′
k
w′
1
· · ·
w′
m
¢
in which all vectors are in Fk. Since m > k, it follows from Corollary 9.3.4 that the vectors,
{w′
1, · · ·, w′
m} are dependent. Therefore, some w′
j is a linear combination of the other w′
i.
Therefore, wj is a linear combination of the other wi in 9.3. By Lemma 9.1.6 again, the
same linear relationship exists between the {v1, · · ·, vm} showing that {v1, · · ·, vm} is not
linearly independent and contradicting the assumption that {v1, · · ·, vm} is a basis.
It
follows k ≤m. Similarly, m ≤k. This proves the theorem.
The following deﬁnition can now be stated.
Deﬁnition 9.3.12 Let V be a subspace of Fn. Then the dimension of V is deﬁned
to be the number of vectors in a basis.
Corollary 9.3.13 The dimension of Fn is n.
Proof: You only need to exhibit a basis for Fn which has n vectors. Such a basis is
{e1, · · ·, en}.
Corollary 9.3.14 Let A be an m × n matrix. Then rank (A) equals the dimension of
col (A) and this equals the dimension of row (A).
1To see why the plural of basis is bases, try to say basiss. It involves much hissing.

9.3.
LINEAR INDEPENDENCE AND BASES
171
Proof: The rank of A equals the number of pivot columns. By Lemma 9.1.6 these pivot
columns are linearly independent and span col (A) . Therefore, the dimension of col (A)
equals the rank of A. The number of nonzero rows in the row reduced echelon form equals
the rank of A also. Furthermore, these rows are independent and span row (A) . Thus the
rank of A equals the dimension of the row space as claimed.
From this corollary, the following is obvious.
Corollary 9.3.15 rank(A) = rank
¡
AT ¢
.
Corollary 9.3.16 Suppose {v1, · · ·, vn} is linearly independent and each vi is a vector
in Fn. Then {v1, · · ·, vn} is a basis for Fn. Suppose {v1, · · ·, vm} spans Fn. Then m ≥n.
If {v1, · · ·, vn} spans Fn, then {v1, · · ·, vn} is linearly independent.
Proof: First suppose {v1, · · ·, vn} is linearly independent. Let u be a vector of Fn and
consider the matrix,
¡
v1
· · ·
vn
u
¢
.
By Lemma 9.3.3, on Page 167 each vi is a pivot column, the row reduced echelon form is
¡
e1
· · ·
en
w
¢
and so, since w is in span (e1, · · ·, en) , it follows from Lemma 9.1.6 that u is in span (v1, · · ·, vn).
Therefore, {v1, · · ·, vn} is a basis as claimed.
For the second claim, if any of the vi is a linear combination of the others, then delete
that vector from the list. This yields a shorter list of vectors which has the same span. Now
do the same with this shorter list eventually obtaining vectors {v′
1, · · ·, v′
l} with l ≤m that
spans Fn and has the property that no vector is a linear combination of the others. Thus
{v′
1, · · ·, v′
l} is a basis for Fn and so by Theorem 9.3.11, l = n. Therefore, m ≥l = n.
Finally consider the third claim. If {v1, · · ·, vn} is not linearly independent, then some
vector is a linear combination of the others. Delete that vector from the list. The new list
of vectors still has the same span. If it is linearly independent, stop. If not, some vector
is a linear combination of the others. Delete that vector. This does not change the span.
Continue this way, ﬁnally obtaining a shorter list of vectors, {v′
1, · · ·, v′
m} which spans Fn
and is also linearly independent. But then this contradicts Theorem 9.3.11 because this
would yield two bases having diﬀerent sizes. This proves the corollary.
By way of review, here are a few more examples of the sort worked earlier on which you
can use the new terminology, row (A), col (A) and basis.
Example 9.3.17 Find the rank of the following matrix. If the rank is r, identify r columns
in the original matrix which have the property that every other column may be written
as a linear combination of these. Also ﬁnd a basis for the row and column spaces of the
matrices.


1
2
3
2
1
5
−4
−1
−2
3
1
0


The row reduced echelon form is


1
0
0
27
70
0
1
0
1
10
0
0
1
33
70


and so the rank of the matrix is 3. A basis for the column space is the ﬁrst three columns of
the original matrix. I know they span because the ﬁrst three columns of the row reduced

172
RANK OF A MATRIX 26,27 SEPT.
echelon form above span the column space of that matrix. They are linearly independent
because the ﬁrst three columns of the row reduced echelon form are linearly independent.
By Lemma 9.1.6 all linear relationships are preserved and so these ﬁrst three vectors form
a basis for the column space. The four rows of the row reduced echelon form form a basis
for the row space of the original matrix.
Example 9.3.18 Find the rank of the following matrix. If the rank is r, identify r columns
in the original matrix which have the property that every other column may be written
as a linear combination of these. Also ﬁnd a basis for the row and column spaces of the
matrices.


1
2
3
0
1
1
1
2
−6
2
−2
3
1
0
2


The row reduced echelon form is


1
0
1
0
−1
7
0
1
1
0
4
7
0
0
0
1
−11
42

.
A basis for the column space of this row reduced echelon form is the ﬁrst second and fourth
columns. Therefore, a basis for the column space in the original matrix is the ﬁrst second
and fourth columns. The rank of the matrix is 3. A basis for the row space of the original
matrix is the columns of the row reduced echelon form.
9.3.4
Finding The Null Space Or Kernel Of A Matrix
Let A be an m × n matrix.
Deﬁnition 9.3.19 ker (A), also referred to as the null space of A is deﬁned as
follows.
ker (A) = {x : Ax = 0}
and to ﬁnd ker (A) one must solve the system of equations Ax = 0. This is also denoted as
null (A) .
That is, null (A) = ker (A) = {x : Ax = 0} which equals all the vectors which A sends to 0.
This is not new! There is just some new terminology being used. To repeat, ker (A) is
the solution to the system Ax = 0.
Example 9.3.20 Let
A =


1
2
1
0
−1
1
2
3
3

.
Find ker (A).
You need to solve the equation Ax = 0. To do this you write the augmented matrix and
then obtain the row reduced echelon form and the solution. The augmented matrix is


1
2
1
|
0
0
−1
1
|
0
2
3
3
|
0



9.3.
LINEAR INDEPENDENCE AND BASES
173
Next place this matrix in row reduced echelon form,


1
0
3
|
0
0
1
−1
|
0
0
0
0
|
0


Note that x1 and x2 are basic variables while x3 is a free variable. Therefore, the solution
to this system of equations, Ax = 0 is given by


3t
t
t

: t ∈R.
Example 9.3.21 Let
A =




1
2
1
0
1
2
−1
1
3
0
3
1
2
3
1
4
−2
2
6
0




Find the null space of A.
You need to solve the equation, Ax = 0. The augmented matrix is




1
2
1
0
1
|
0
2
−1
1
3
0
|
0
3
1
2
3
1
|
0
4
−2
2
6
0
|
0




Its row reduced echelon form is




1
0
3
5
6
5
1
5
|
0
0
1
1
5
−3
5
2
5
|
0
0
0
0
0
0
|
0
0
0
0
0
0
|
0




It follows x1 and x2 are basic variables and x3, x4, x5 are free variables. Therefore, ker (A)
is given by






¡
−3
5
¢
s1 +
¡ −6
5
¢
s2 +
¡ 1
5
¢
s3
¡
−1
5
¢
s1 +
¡ 3
5
¢
s2 +
¡
−2
5
¢
s3
s1
s2
s3






: s1, s2, s3 ∈R.
We write this in the form
s1






−3
5
−1
5
1
0
0






+ s2






−6
53
5
0
1
0






+ s3






1
5
−2
5
0
0
1






: s1, s2, s3 ∈R.
In other words, the null space of this matrix equals the span of the three vectors above.
Thus
ker (A) = span












−3
5
−1
5
1
0
0






,






−6
53
5
0
1
0






,






1
5
−2
5
0
0
1












.

174
RANK OF A MATRIX 26,27 SEPT.
This is the same as
ker (A) = span












3
51
5
−1
0
0






,






6
5
−3
5
0
−1
0






,






−1
52
5
0
0
−1












.
Notice also that the three vectors above are linearly independent and so the dimension of
ker (A) is 3. This is generally the way it works. The number of free variables equals the
dimension of the null space while the number of basic variables equals the number of pivot
columns which equals the rank. We state this in the following theorem.
Deﬁnition 9.3.22 The dimension of the null space of a matrix is called the nullity2
and written as nullity(A) .
Theorem 9.3.23 Let A be an m × n matrix. Then rank (A) +nullity(A) = n.
This implies the following corollary.
Corollary 9.3.24 Let A be an n × n matrix. Then A is onto if and only if A is one to
one.
The following theorem is an interesting review of the transpose of a matrix.
Theorem 9.3.25 Let A be a real m×n matrix. Then rank
¡
AT A
¢
= rank (A) and
AT A is invertible if and only if rank (A) = n.
Proof: There are various ways to show this. From Theorem 9.3.23
n = rank (A) + nullity (A) = rank
¡
AT A
¢
+ nullity
¡
AT A
¢
.
I will show null (A) = null
¡
AT A
¢
because this implies the two nullitys above are equal. Sup-
pose Ax = 0. Then AT Ax = 0 also. Hence null (A) ⊆null
¡
AT A
¢
. Next suppose AT Ax = 0.
Then
xT AT Ax = (Ax)T Ax = Ax·Ax
and so Ax = 0. Hence null (A) ⊇null
¡
AT A
¢
.
If AT A is invertible, then this implies A is one to one and so the columns of A are
independent. Hence rank (A) = n. If rank (A) = n, then the dimension of the column space
is n and so since the column vectors span the column space, they are a basis for it. In
particular they are independent. Hence A is one to one. It follows as above that AT A is
also one to one mapping Fn to Fn. The n columns of AT A are linearly independent because
the matrix is one to one. Therefore, by Corollary 9.3.16 these columns are a basis for Fn
and so AT A is both onto and one to one. Hence it is invertible.
9.3.5
Rank And Existence Of Solutions To Linear Systems∗
Consider the linear system of equations,
Ax = b
(9.4)
2Isn’t it amazing how many diﬀerent words are available for use in linear algebra?

9.3.
LINEAR INDEPENDENCE AND BASES
175
where A is an m × n matrix, x is a n × 1 column vector, and b is an m × 1 column vector.
Suppose
A =
¡
a1
· · ·
an
¢
where the ak denote the columns of A. Then if x = (x1, · · ·, xn)T is a solution of the system
9.4, it follows
x1a1 + · · · + xnan = b
which says that b is a vector in span (a1, · · ·, an) . This shows that there exists a solution
to the system, 9.4 if and only if b is contained in span (a1, · · ·, an) . In words, there is a
solution to 9.4 if and only if b is in the column space of A. In terms of rank, the following
proposition describes the situation.
Proposition 9.3.26 Let A be an m × n matrix and let b be an m × 1 column vector.
Then there exists a solution to 9.4 if and only if
rank
¡ A
|
b ¢
= rank (A) .
(9.5)
Proof: Place
¡
A
|
b
¢
and A in row reduced echelon form, respectively B and C. If
the above condition on rank is true, then both B and C have the same number of nonzero
rows. In particular, you cannot have a row of the form
¡
0
· · ·
0
■
¢
where ■̸= 0 in B. Therefore, there will exist a solution to the system.
Conversely, suppose there exists a solution. This means there cannot be such a row in
B described above. Therefore, B and C must have the same number of zero rows and so
they have the same number of nonzero rows. Therefore, the rank of the two matrices in 9.5
is the same.
9.3.6
Exercises With Answers
1. Find the rank of the following matrices. If the rank is r, identify r columns in the
original matrix which have the property that every other column may be written
as a linear combination of these. Also ﬁnd a basis for the row and column spaces of
the matrices.
(a)




9
2
0
3
7
1
6
1
0
0
2
1




From using row operations we obtain the row reduced echelon form which is




1
0
0
0
1
0
0
0
1
0
0
0




Therefore, a basis for the column space of the original matrix is the ﬁrst three
columns of the original matrix. A basis for the row space is just
¡
1
0
0
¢
,
¡
0
1
0
¢
,
and
¡
0
0
1
¢
.

176
RANK OF A MATRIX 26,27 SEPT.
(b)




3
0
3
10
9
1
1
1
0
2
2
0




In this case the row reduced echelon form is




1
0
1
0
1
−1
0
0
0
0
0
0




and so a basis for the column space of the original matrix consists of the ﬁrst two
columns of the original matrix and a basis for the row space is
¡
1
0
1
¢
and
¡
0
1
−1
¢
.
(c)




0
1
7
8
1
9
2
0
3
2
5
1
6
8
0
1
1
2
0
2
3
0
2
1
3
0
3
4




The row reduced echelon form of this matrix is




0
1
0
1
0
1
0
0
0
1
1
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1




and so a basis for the column space of the original matrix consists of the second,
third, ﬁfth, and seventh columns of the original matrix. A basis for the row space
consists of the rows of this last matrix in row reduced echelon form.
2. Let H denote span




1
1
0

,


1
4
5

,


1
3
1

,


1
1
1



. Find the dimension of H
and determine a basis.
Make these the columns of a matrix and ask for the rank of this matrix.


1
1
1
1
1
4
3
1
0
5
1
1


The row reduced echelon form is


1
0
0
8
7
0
1
0
2
7
0
0
1
−3
7


A basis for H is





1
1
0

,


1
4
5

,


1
3
1





and so H has dimension 3.
3. Here are three vectors. Determine whether they are linearly independent or linearly
dependent.


1
0
1

,


2
0
1

,


3
0
0



9.3.
LINEAR INDEPENDENCE AND BASES
177
You need to consider the solutions to the equation
c1


1
0
1

+ c2


2
0
1

+ c3


3
0
0

=


0
0
0


and determine whether there is a solution other than the obvious one, c1 = c2 = c3 = 0.
The augmented matrix for the system of equations is


1
2
3
|
0
0
0
0
|
0
1
1
0
|
0


Taking −1 times the top row and adding to the bottom and then switching the two
bottom rows yields


1
2
3
|
0
0
−1
−3
|
0
0
0
0
|
0


Next take 2 times the second row and add to the top. This yields


1
0
−3
|
0
0
−1
−3
|
0
0
0
0
|
0


There are solutions other than the zero solution because c3 is a free variable. Therefore,
these vectors are not linearly independent.
4. Here are four vectors. Determine whether they span R3. Are these vectors linearly
independent?


1
2
3

,


4
0
3

,


3
2
0

,


2
1
6


The vectors can’t possibly be linearly independent. If they were, they would constitute
a linearly independent set consisting of four vectors even though there exists a spanning
set of only three,


1
0
0

,


0
1
0

,


0
0
1


However, the four given vectors might still span R3 even though they are not a basis.
What does it take to span R3? Given a vector (x, y, z)T ∈R3, do there exist scalars
c1, c2, c3, and c4 such that
c1


1
2
3

+ c2


4
0
3

+ c3


3
2
0

+ c4


2
1
6

=


x
y
z

?
Consider the augmented matrix of the above,


1
4
3
2
|
x
2
0
2
1
|
y
3
3
0
6
|
z



178
RANK OF A MATRIX 26,27 SEPT.
Doing row operations till an echelon form is obtained leads to


1
0
0
5
4
|
1
4y + 2
9z −1
6x
0
1
0
3
4
|
−1
4y + 1
6x + 1
9z
0
0
1
−3
4
|
−2
9z + 1
6x + 1
4y


and you see there is a solution to the desired system of equations. In fact there are
inﬁnitely many because c4 is a free variable. Therefore, the four vectors do span R3.
5. Consider the vectors of the form





2t + 6s
s −2t
3t + s

: s, t ∈R


.
Is this set of vectors a subspace of R3? If so, explain why, give a basis for the subspace
and ﬁnd its dimension.
This is indeed a subspace. You only need to verify the set of vectors is closed with
respect to the vector space operations. Let


2t1 + 6s1
s1 −2t1
3t1 + s1

and


2t + 6s
s −2t
3t + s

be two
vectors in the given set of vectors.
α


2t + 6s
s −2t
3t + s

+ β


2t1 + 6s1
s1 −2t1
3t1 + s1


=


2αt + 6αs + 2βt1 + 6βs1
αs −2αt + βs1 −2βt1
3αt + αs + 3βt1 + βs1


=


2 (αt + βt1) + 6 (αs + βs1)
αs + βs1 −2 (αt + βt1)
3 (αt + βt1) + αs + βs1


If we let T ≡αt + βt1 and S ≡αs + βs1, this is seen to be of the form


2T + 6S
S −2T
3T + S


which is the way the vectors in the given set are described. Another way to see this is
to notice that the vectors in the given set are of the form
t


2
−2
3

+ s


6
1
1


so it consists of the span of the two vectors,


2
−2
3

,


6
1
1

.
(9.6)
Recall that the span of a set of vectors is always a subspace. You can also verify these
vectors in 9.6 form a linearly independent set and so they are a basis.

9.3.
LINEAR INDEPENDENCE AND BASES
179
6. Let M =
©
u = (u1, u2, u3, u4) ∈R4 : u3 ≥u2
ª
. Is M a subspace? Explain.
This is not a subspace because if u ∈M, is such that u3 > u2, then consider (−1) u.
If this were in M you would need to have −u3 > −u2 and so u3 < u2 which cannot
be true if u3 > u2. Thus M is not closed under scalar multiplication so it is not a
subspace.
7. Let w, w1 be given vectors in R2 and deﬁne
M =
©
u = (u1, u2) ∈R2 : w · u = 0 and w1 · u = 0
ª
.
Is M a subspace? Explain.
Suppose u′ and u are both in M. What about αu′ + βu?
w· (αu′ + βu) = αw · u′ + βw · u = α0 + β0 = 0
Similarly,
w1· (αu′ + βu) = αw1·u′ + βw1·u = α0 + β0 = 0
and so αu′ + βu ∈M. This has veriﬁed that M is a subspace.

180
RANK OF A MATRIX 26,27 SEPT.

Linear Transformations 27 Sept.
Quiz
1. Find the rank of the matrix




1
2
1
1
2
1
1
1
2
7
3
3
1
8
3
3




2. For A the above matrix, ﬁnd
null (A) = ker (A) .
That is, ﬁnd its null space.
3. For A the matrix of Problem 1 ﬁnd a basis for the column space of this matrix.
4. For A the matrix of Problem 1 ﬁnd a basis for the row space of this matrix.
An m × n matrix can be used to transform vectors in Fn to vectors in Fm through the
use of matrix multiplication.
Example 10.0.27 Consider the matrix,
µ
1
2
0
2
1
0
¶
. Think of it as a function which
takes vectors in F3 and makes them in to vectors in F2 as follows. For


x
y
z

a vector
in F3, multiply on the left by the given matrix to obtain the vector in F2. Here are some
numerical examples.
µ
1
2
0
2
1
0
¶ 

1
2
3

=
µ
5
4
¶
,
µ
1
2
0
2
1
0
¶ 

1
−2
3

=
µ
−3
0
¶
,
µ
1
2
0
2
1
0
¶ 

10
5
−3

=
µ
20
25
¶
,
µ
1
2
0
2
1
0
¶ 

0
7
3

=
µ
14
7
¶
,
More generally,
µ
1
2
0
2
1
0
¶ 

x
y
z

=
µ
x + 2y
2x + y
¶
The idea is to deﬁne a function which takes vectors in F3 and delivers new vectors in F2.
181

182
LINEAR TRANSFORMATIONS 27 SEPT.
This is an example of something called a linear transformation.
Deﬁnition 10.0.28 Let X and Y be vector spaces and let T : X →Y be a function.
Thus for each x ∈X, Tx ∈Y. Then T is a linear transformation if whenever α, β are
scalars and x1 and x2 are vectors in X,
T (αx1 + βx2) = α1Tx1 + βTx2.
In words, linear transformations distribute across + and allow you to factor out scalars.
At this point, recall the properties of matrix multiplication. The pertinent property is 7.14
on Page 129. Recall it states that for a and b scalars,
A (aB + bC) = aAB + bAC
In particular, for A an m × n matrix and B and C, n × 1 matrices (column vectors) the
above formula holds which is nothing more than the statement that matrix multiplication
gives an example of a linear transformation.
Deﬁnition 10.0.29 A linear transformation is called one to one (often written
as 1 −1) if it never takes two diﬀerent vectors to the same vector. Thus T is one to one if
whenever x ̸= y
Tx ̸= Ty.
Equivalently, if T (x) = T (y) , then x = y.
In the case that a linear transformation comes from matrix multiplication, it is common
usage to refer to the matrix as a one to one matrix when the linear transformation it
determines is one to one.
Deﬁnition 10.0.30 A linear transformation mapping X to Y is called onto if
whenever y ∈Y there exists x ∈X such that T (x) = y.
Thus T is onto if everything in Y gets hit. In the case that a linear transformation
comes from matrix multiplication, it is common to refer to the matrix as onto when the
linear transformation it determines is onto. Also it is common usage to write TX, T (X) ,or
Im (T) as the set of vectors of Y which are of the form Tx for some x ∈X. In the case that
T is obtained from multiplication by an m × n matrix, A, it is standard to simply write
A (Fn) AFn, or Im (A) to denote those vectors in Fm which are obtained in the form Ax for
some x ∈Fn.
10.1
Constructing The Matrix Of A Linear Transfor-
mation
It turns out that if T is any linear transformation which maps Fn to Fm, there is always an
m × n matrix, A with the property that
Ax = Tx
(10.1)
for all x ∈Fn. Here is why. Suppose T : Fn →Fm is a linear transformation and you want
to ﬁnd the matrix deﬁned by this linear transformation as described in 10.1. Then if x ∈Fn
it follows
x =
n
X
i=1
xiei

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
183
where ei is the vector which has zeros in every slot but the ith and a 1 in this slot. Then
since T is linear,
Tx
=
n
X
i=1
xiT (ei)
=


|
|
T (e1)
· · ·
T (en)
|
|





x1
...
xn



≡
A



x1
...
xn



and so you see that the matrix desired is obtained from letting the ith column equal T (ei) .
We state this as the following theorem.
Theorem 10.1.1 Let T be a linear transformation from Fn to Fm. Then the ma-
trix, A satisfying 10.1 is given by


|
|
T (e1)
· · ·
T (en)
|
|


where Tei is the ith column of A.
10.1.1
Rotations of R2
Sometimes you need to ﬁnd a matrix which represents a given linear transformation which
is described in geometrical terms. The idea is to produce a matrix which you can multiply
a vector by to get the same thing as some geometrical description. A good example of this
is the problem of rotation of vectors.
Example 10.1.2 Determine the matrix which represents the linear transformation deﬁned
by rotating every vector through an angle of θ.
Let e1 ≡
µ
1
0
¶
and e2 ≡
µ
0
1
¶
. These identify the geometric vectors which point
along the positive x axis and positive y axis as shown.
-
6
e1
e2

184
LINEAR TRANSFORMATIONS 27 SEPT.
From the above, you only need to ﬁnd Te1 and Te2, the ﬁrst being the ﬁrst column of
the desired matrix, A and the second being the second column. From drawing a picture and
doing a little geometry, you see that
Te1 =
µ cos θ
sin θ
¶
, Te2 =
µ −sin θ
cos θ
¶
.
Therefore, from Theorem 10.1.1,
A =
µ cos θ
−sin θ
sin θ
cos θ
¶
Example 10.1.3 Find the matrix of the linear transformation which is obtained by ﬁrst
rotating all vectors through an angle of φ and then through an angle θ. Thus you want the
linear transformation which rotates all angles through an angle of θ + φ.
Let Tθ+φ denote the linear transformation which rotates every vector through an angle
of θ + φ. Then to get Tθ+φ, you could ﬁrst do Tφ and then do Tθ where Tφ is the linear
transformation which rotates through an angle of φ and Tθ is the linear transformation
which rotates through an angle of θ. Denoting the corresponding matrices by Aθ+φ, Aφ,
and Aθ, you must have for every x
Aθ+φx = Tθ+φx = TθTφx = AθAφx.
Consequently, you must have
Aθ+φ
=
µ
cos (θ + φ)
−sin (θ + φ)
sin (θ + φ)
cos (θ + φ)
¶
= AθAφ
=
µ cos θ
−sin θ
sin θ
cos θ
¶ µ cos φ
−sin φ
sin φ
cos φ
¶
.
You know how to multiply matrices. Do so to the pair on the right. This yields
µ cos (θ + φ)
−sin (θ + φ)
sin (θ + φ)
cos (θ + φ)
¶
=
µ cos θ cos φ −sin θ sin φ
−cos θ sin φ −sin θ cos φ
sin θ cos φ + cos θ sin φ
cos θ cos φ −sin θ sin φ
¶
.
Don’t these look familiar? They are the usual trig. identities for the sum of two angles
derived here using linear algebra concepts.
You do not have to stop with two dimensions. You can consider rotations and other
geometric concepts in any number of dimensions.
This is one of the major advantages
of linear algebra. You can break down a diﬃcult geometrical procedure into small steps,
each corresponding to multiplication by an appropriate matrix. Then by multiplying the
matrices, you can obtain a single matrix which can give you numerical information on the
results of applying the given sequence of simple procedures. That which you could never
visualize can still be understood to the extent of ﬁnding exact numerical answers. Another
example follows.
Example 10.1.4 Find the matrix of the linear transformation which is obtained by ﬁrst
rotating all vectors through an angle of π/6 and then reﬂecting through the x axis.
As shown in Example 10.1.3, the matrix of the transformation which involves rotating
through an angle of π/6 is
µ
cos (π/6)
−sin (π/6)
sin (π/6)
cos (π/6)
¶
=
µ
1
2
√
3
−1
2
1
2
1
2
√
3
¶

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
185
The matrix for the transformation which reﬂects all vectors through the x axis is
µ
1
0
0
−1
¶
.
Therefore, the matrix of the linear transformation which ﬁrst rotates through π/6 and then
reﬂects through the x axis is
µ
1
0
0
−1
¶ µ
1
2
√
3
−1
2
1
2
1
2
√
3
¶
=
µ
1
2
√
3
−1
2
−1
2
−1
2
√
3
¶
.
10.1.2
Projections
In Physics it is important to consider the work done by a force ﬁeld on an object. This
involves the concept of projection onto a vector. Suppose you want to ﬁnd the projection
of a vector, v onto the given vector, u, denoted by proju (v) This is done using the dot
product as follows.
proju (v) =
³v · u
u · u
´
u
Because of properties of the dot product, the map v →proju (v) is linear,
proju (αv+βw)
=
µαv+βw · u
u · u
¶
u = α
³v · u
u · u
´
u + β
³w · u
u · u
´
u
=
α proju (v) + β proju (w) .
Example 10.1.5 Let the projection map be deﬁned above and let u = (1, 2, 3)T . Does this
linear transformation come from multiplication by a matrix? If so, what is the matrix?
You can ﬁnd this matrix in the same way as in the previous example. Let ei denote the
vector in Rn which has a 1 in the ith position and a zero everywhere else. Thus a typical
vector, x = (x1, · · ·, xn)T can be written in a unique way as
x =
n
X
j=1
xjej.
From the way you multiply a matrix by a vector, it follows that proju (ei) gives the ith
column of the desired matrix. Therefore, it is only necessary to ﬁnd
proju (ei) ≡
³ ei·u
u · u
´
u
For the given vector in the example, this implies the columns of the desired matrix are
1
14


1
2
3

, 2
14


1
2
3

, 3
14


1
2
3

.
Hence the matrix is
1
14


1
2
3
2
4
6
3
6
9

.

186
LINEAR TRANSFORMATIONS 27 SEPT.
10.1.3
Matrices Which Are One To One Or Onto
Lemma 10.1.6 Let A be an m × n matrix.
Then A (Fn) = span (a1, · · ·, an) where
a1, · · ·, an denote the columns of A. In fact, for x = (x1, · · ·, xn)T ,
Ax =
n
X
k=1
xkak.
Proof: This follows from the deﬁnition of matrix multiplication in Deﬁnition 7.1.9 on
Page 124.
The following is a theorem of major signiﬁcance. First here is an interesting observation.
Observation 10.1.7 Let A be an m × n matrix. Then A is one to one if and only if
Ax = 0 implies x = 0.
Here is why: A0 = A (0 + 0) = A0 + A0 and so A0 = 0.
Now suppose A is one to one and Ax = 0. Then since A0 = 0, it follows x = 0. Thus if
A is one to one and Ax = 0, then x = 0.
Next suppose the condition that Ax = 0 implies x = 0 is valid. Then if Ax = Ay, then
A (x −y) = 0 and so from the condition, x −y = 0 so that x = y. Thus A is one to one.
Theorem 10.1.8 Suppose A is an n×n matrix. Then A is one to one if and only
if A is onto. Also, if B is an n × n matrix and AB = I, then it follows BA = I.
Proof: First suppose A is one to one. Consider the vectors, {Ae1, · · ·, Aen} where ek is
the column vector which is all zeros except for a 1 in the kth position. This set of vectors is
linearly independent because if
n
X
k=1
ckAek = 0,
then since A is linear,
A
Ã n
X
k=1
ckek
!
= 0
and since A is one to one, it follows
n
X
k=1
ckek = 0
which implies each ck = 0. Therefore, {Ae1, · · ·, Aen} must be a basis for Fn by Corollary
9.3.16. It follows that for y ∈Fn there exist constants, ci such that
y =
n
X
k=1
ckAek = A
Ã n
X
k=1
ckek
!
showing that, since y was arbitrary, A is onto.
Next suppose A is onto. This implies the span of the columns of A equals Fn and by
Corollary 9.3.16 this implies the columns of A are independent. If Ax = 0, then letting
x = (x1, · · ·, xn)T , it follows
n
X
i=1
xiai = 0

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
187
and so each xi = 0. If Ax = Ay, then A (x −y) = 0 and so x = y. This shows A is one to
one.
Now suppose AB = I. Why is BA = I? Since AB = I it follows B is one to one since
otherwise, there would exist, x ̸= 0 such that Bx = 0 and then ABx = A0 = 0 ̸= Ix.
Therefore, from what was just shown, B is also onto. In addition to this, A must be one
to one because if Ay = 0, then y = Bx for some x and then x = ABx = Ay = 0 showing
y = 0. Now from what is given to be so, it follows (AB) A = A and so using the associative
law for matrix multiplication,
A (BA) −A = A (BA −I) = 0.
But this means (BA −I) x = 0 for all x since otherwise, A would not be one to one. Hence
BA = I as claimed. This proves the theorem.
This theorem shows that if an n × n matrix, B acts like an inverse when multiplied on
one side of A it follows that B = A−1and it will act like an inverse on both sides of A.
The conclusion of this theorem pertains to square matrices only. For example, let
A =


1
0
0
1
1
0

, B =
µ 1
0
0
1
1
−1
¶
(10.2)
Then
BA =
µ 1
0
0
1
¶
but
AB =


1
0
0
1
1
−1
1
0
0

.
10.1.4
The General Solution Of A Linear System
Recall the following deﬁnition which was discussed above.
Deﬁnition 10.1.9 T is a linear transformation if whenever x, y are vectors and
a, b scalars,
T (ax + by) = aTx + bTy.
Thus linear transformations distribute across addition and pass scalars to the outside. A
linear system is one which is of the form
Tx = b.
If Txp = b, then xp is called a particular solution to the linear system.
For example, if A is an m × n matrix and TA is determined by
TA (x) = Ax,
then from the properties of matrix multiplication, TA is a linear transformation. In this
setting, we will usually write A for the linear transformation as well as the matrix. There
are many other examples of linear transformations other than this. In diﬀerential equations,
you will encounter linear transformations which act on functions to give new functions. In
this case, the functions are considered as vectors.

188
LINEAR TRANSFORMATIONS 27 SEPT.
Deﬁnition 10.1.10 Let T be a linear transformation. Deﬁne
ker (T) ≡{x : Tx = 0} .
In words, ker (T) is called the kernel of T. As just described, ker (T) consists of the set of
all vectors which T sends to 0. This is also called the null space of T. It is also called the
solution space of the equation Tx = 0.
The above deﬁnition states that ker (T) is the set of solutions to the equation,
Tx = 0.
In the case where T is really a matrix, you have been solving such equations for quite some
time. However, sometimes linear transformations act on vectors which are not in Fn.
Example 10.1.11 Let
d
dx denote the linear transformation deﬁned on X, the functions
which are deﬁned on R and have a continuous derivative. Find ker
¡ d
dx
¢
.
The example asks for functions, f which the property that df
dx = 0. As you know from
calculus, these functions are the constant functions. Thus ker
¡ d
dx
¢
= constant functions.
When T is a linear transformation, systems of the form Tx = 0 are called homogeneous
systems. Thus the solution to the homogeneous system is known as ker (T) .
Systems of the form Tx = b where b ̸= 0 are called nonhomogeneous systems. It
turns out there is a very interesting and important relation between the solutions to the
homogeneous systems and the solutions to the nonhomogeneous systems.
Theorem 10.1.12 Suppose xp is a solution to the linear system,
Tx = b
Then if y is any other solution to the linear system, there exists x ∈ker (T) such that
y = xp + x.
Proof: Consider y −xp ≡y+ (−1) xp. Then T
¡
y −xp
¢
= Ty −Txp = b −b = 0. Let
x ≡y −xp. This proves the theorem.
Sometimes people remember the above theorem in the following form. The solutions
to the nonhomogeneous system, Tx = b are given by xp + ker (T) where xp is a particular
solution to Tx = b.
We have been vague about what T is and what x is on purpose.
This theorem is
completely algebraic in nature and will work whenever you have linear transformations. In
particular, it will be important in diﬀerential equations. For now, here is a familiar example.
Example 10.1.13 Let
A =


1
2
3
0
2
1
1
2
4
5
7
2


Find ker (A). Equivalently, ﬁnd the solution space to the system of equations Ax = 0.
This asks you to ﬁnd {x : Ax = 0} . In other words you are asked to solve the system,
Ax = 0. Let x = (x, y, z, w)T . Then this amounts to solving


1
2
3
0
2
1
1
2
4
5
7
2






x
y
z
w



=


0
0
0



10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
189
This is the linear system
x + 2y + 3z = 0
2x + y + z + 2w = 0
4x + 5y + 7z + 2w = 0
and you know how to solve this using row operations, (Gauss Elimination). Set up the
augmented matrix,


1
2
3
0
|
0
2
1
1
2
|
0
4
5
7
2
|
0


Then row reduce to obtain the row reduced echelon form,




1
0
−1
3
4
3
|
0
0
1
5
3
−2
3
|
0
0
0
0
0
|
0



.
This yields x = 1
3z −4
3w and y = 2
3w −5
3z. Thus ker (A) consists of vectors of the form,






1
3z −4
3w
2
3w −5
3z
z
w






= z





1
3
−5
3
1
0




+ w





−4
3
2
3
0
1




.
Example 10.1.14 The general solution of a linear system of equations is just the set of
all solutions. Find the general solution to the linear system,


1
2
3
0
2
1
1
2
4
5
7
2






x
y
z
w



=


9
7
25


given that
¡
1
1
2
1
¢T =
¡
x
y
z
w
¢T is one solution.
Note the matrix on the left is the same as the matrix in Example 10.1.13. Therefore,
from Theorem 10.1.12, you will obtain all solutions to the above linear system in the form
z





1
3
−5
3
1
0




+ w





−4
3
2
3
0
1




+




1
1
2
1




because




x
y
z
w



=




1
1
2
1



is a particular solution to the given system of equations.

190
LINEAR TRANSFORMATIONS 27 SEPT.
10.1.5
Exercises With Answers
1. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 5π/12.
You note that 5π/12 = 2π/3 −π/4. Therefore, you can ﬁrst rotate through −π/4
and then rotate through 2π/3 to get the rotation through 5π/12. The matrix of the
transformation with respect to the usual coordinates which rotates through −π/4 is
µ
√
2/2
√
2/2
−
√
2/2
√
2/2
¶
and the matrix of the transformation which rotates through 2π/3 is
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
.
Multiplying these gives
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶ µ
√
2/2
√
2/2
−
√
2/2
√
2/2
¶
=
µ
−1
4
√
2 + 1
4
√
3
√
2
−1
4
√
2 −1
4
√
3
√
2
1
4
√
3
√
2 + 1
4
√
2
−1
4
√
2 + 1
4
√
3
√
2
¶
and this is the matrix of the desired transformation. Note this shows that
cos (5π/12) = −1
4
√
2 + 1
4
√
3
√
2 ≈. 258 819 05
sin (5π/12) = 1
4
√
3
√
2 + 1
4
√
2 ≈. 965 925 83.
2. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 2π/3 and then reﬂects across the x axis.
What does it do to e1? First you rotate e1 through the given angle to obtain
µ −1/2
√
3/2
¶
and then this becomes
µ
−1/2
−
√
3/2
¶
.
This is the ﬁrst column of the desired matrix. Next e2 ﬁrst is rotated through the
given angle to give
µ
−
√
3/2
−1/2
¶
and then it is reﬂected across the x axis to give
µ
−
√
3/2
1/2
¶
and this gives the second column of the desired matrix. Thus the matrix is
µ
−1/2
−
√
3/2
−
√
3/2
1/2
¶
.

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
191
3. Find the matrix for proju (v) where u = (1, −2, 3)T .
Recall
proju (v) = v · u
||u||2 u
Therefore,
proju (e1)
=
1
14


1
−2
3

, proju (e2) = −2
14


1
−2
3

,
proju (e2)
=
3
14


1
−2
3

.
Hence the desired matrix is
1
14


1
−2
3
−2
4
−6
3
−6
9

.
4. Show that the function Tu deﬁned by Tu (v) ≡v −proju (v) is also a linear transfor-
mation.
Tu (αv + βw) = αv + βw−proju (αv + βw)
which from 3 equals
α (v −proju (v)) + β (w −proju (w)) = αTuv + βTuw.
This is what it takes to be a linear transformation.
5. If A, B, and C are each n × n matrices and ABC is invertible, why are each of A, B,
and C invertible.
0 ̸= det (ABC) = det (A) det (B) det (C) and so none of det (A) , det (B) , or det (C)
can equal zero. Therefore, each is invertible. You should do this another way, showing
that each of A, B, and C is one to one and then using a theorem presented earlier.
6. Give an example of a 3 × 1 matrix with the property that the linear transformation
determined by this matrix is one to one but not onto.
Here is one.


1
0
0

. If


1
0
0

x =


0
0
0

, then x = 0 but this is certainly not onto
as a map from R1 to R3 because it does not ever yield


1
1
0

.
7. Find the matrix of the linear transformation from R3 to R3 which ﬁrst rotates every
vector through an angle of π/4 about the z axis when viewed from the positive z axis
and then rotates every vector through an angle of π/6 about the x axis when viewed
from the positive x axis.
The matrix of the linear transformation which accomplishes the ﬁrst rotation is


√
2/2
−
√
2/2
0
√
2/2
√
2/2
0
0
0
1



192
LINEAR TRANSFORMATIONS 27 SEPT.
and the matrix which accomplishes the second rotation is


1
0
0
0
√
3/2
−1/2
0
1/2
√
3/2


Therefore, the matrix of the desired linear transformation is


1
0
0
0
√
3/2
−1/2
0
1/2
√
3/2




√
2/2
−
√
2/2
0
√
2/2
√
2/2
0
0
0
1


=


1
2
√
2
−1
2
√
2
0
1
4
√
3
√
2
1
4
√
3
√
2
−1
2
1
4
√
2
1
4
√
2
1
2
√
3


This might not be the ﬁrst thing you would think of.

Part V
Eigenvalues, Eigenvectors,
Determinants, Diagonalization
193


195
Outcomes
A. Interpret the eigenvalue problem algebraically.
i. Determine whether a given vector is an eigenvector.
ii. Verify that a given value is an eigenvalue.
B. Interpret the eigenvalue problem geometrically. Determine eigenvalues and eigenvec-
tors based on:
i. an understanding of the linear transformation determined by the matrix
ii. from the graph of the eigenspace.
C. Find the eigenvalues and eigenvectors of a general 2 × 2 matrix.
Reading: Linear Algebra 4.1
Homework: 4.1:
Outcome Mapping:
A. 1-6,7-12
B. 13-18,19-22
C. 23-26,27-30,31-34,35-38
A. Apply the Laplace Expansion to evaluate determinants of n × n matrices.
B. Recall and apply the properties of determinants to evaluate determinants, including:
i. det(AB) = det(A) det(B)
ii. det(kA) = kn det(A)
iii. det(A−1) =
1
det(A)
iv. det(AT ) = det(A)
C. Recall the eﬀects that row operations have on the determinants of matrices. Relate
to the determinants of elementary matrices.
D. Prove theorems involving determinants.
E. Evaluate matrix inverses using the adjoint method. Determine whether or not a matrix
has an inverse based on its determinant.
F. Use Cramer’s rule to solve a linear system.
Reading: Linear Algebra 4.2
Homework: 4.2:
Outcome Mapping:
A. 1-6,7-15,16-20

196
B. 35-38,47-52
C. 26-33,35-40
D. 21,41-44,53-56,66
E. 45-46,61-64,65
F. 57-60
A. Given an n × n matrix, compute
i. the characteristic polynomial
ii. the eigenvalues
iii. a basis for each eigenspace
iv. the algebraic and geometric multiplicities of each eigenvalue
B. Solve application problems involving eigenvalues and eigenvectors.
C. Recall and prove theorems involving eigenvalues and eigenvectors.
Reading: Linear Algebra 4.3
Homework: 4.3:
Outcome Mapping:
A. 1-12
B. 15-22,26-31,33-38
C. 23-25,32,39-42
A. Deﬁne similarity. Determine whether or not two matrices are similar.
B. Determine if a matrix is diagonalizable. Find the diagonalization of a matrix.
C. Find powers of a matrix using the diagonalization of a matrix.
D. Prove theorems involving the similarity and diagonalization of matrices.
Reading: Linear Algebra 4.4
Homework: 4.4:
Outcome Mapping:
A. 1-4,36-39
B. 5-7,8-15,24-29
C. 16-23
D. 30-35,40-50

Determinants 2,3 Oct.
Quiz
1. A linear transformation involves ﬁrst rotating the vectors in R2 counterclockwise
through an angle of 30 degrees and then reﬂecting across the x axis. Find the matrix
of this linear transformation.
2. A linear transformation involves projecting all vectors on to the span of the vector
(1, 1, 1) . Find the matrix of this linear transformation.
11.1
Basic Techniques And Properties
11.1.1
Cofactors And 2 × 2 Determinants
Let A be an n × n matrix. The determinant of A, denoted as det (A) is a number. If the
matrix is a 2×2 matrix, this number is very easy to ﬁnd.
Deﬁnition 11.1.1 Let A =
µ
a
b
c
d
¶
. Then
det (A) ≡ad −cb.
The determinant is also often denoted by enclosing the matrix with two vertical lines. Thus
det
µ
a
b
c
d
¶
=
¯¯¯¯
a
b
c
d
¯¯¯¯ .
Example 11.1.2 Find det
µ
2
4
−1
6
¶
.
From the deﬁnition this is just (2) (6) −(−1) (4) = 16.
Having deﬁned what is meant by the determinant of a 2 × 2 matrix, what about a 3 × 3
matrix?
Deﬁnition 11.1.3 Suppose A is a 3 × 3 matrix.
The ijth minor, denoted as
minor(A)ij , is the determinant of the 2 × 2 matrix which results from deleting the ith row
and the jth column.
Example 11.1.4 Consider the matrix,


1
2
3
4
3
2
3
2
1

.
197

198
DETERMINANTS 2,3 OCT.
The (1, 2) minor is the determinant of the 2 × 2 matrix which results when you delete the
ﬁrst row and the second column. This minor is therefore
det
µ
4
2
3
1
¶
= −2.
The (2, 3) minor is the determinant of the 2 × 2 matrix which results when you delete the
second row and the third column. This minor is therefore
det
µ 1
2
3
2
¶
= −4.
Deﬁnition 11.1.5 Suppose A is a 3 × 3 matrix. The ijth cofactor is deﬁned to be
(−1)i+j ×
¡
ijth minor
¢
. In words, you multiply (−1)i+j times the ijth minor to get the ijth
cofactor. The cofactors of a matrix are so important that special notation is appropriate
when referring to them. The ijth cofactor of a matrix, A will be denoted by cof (A)ij . It is
also convenient to refer to the cofactor of an entry of a matrix as follows. For aij an entry
of the matrix, its cofactor is just cof (A)ij . Thus the cofactor of the ijth entry is just the
ijth cofactor.
Example 11.1.6 Consider the matrix,
A =


1
2
3
4
3
2
3
2
1

.
The (1, 2) minor is the determinant of the 2 × 2 matrix which results when you delete the
ﬁrst row and the second column. This minor is therefore
det
µ
4
2
3
1
¶
= −2.
It follows
cof (A)12 = (−1)1+2 det
µ
4
2
3
1
¶
= (−1)1+2 (−2) = 2
The (2, 3) minor is the determinant of the 2 × 2 matrix which results when you delete the
second row and the third column. This minor is therefore
det
µ
1
2
3
2
¶
= −4.
Therefore,
cof (A)23 = (−1)2+3 det
µ 1
2
3
2
¶
= (−1)2+3 (−4) = 4.
Similarly,
cof (A)22 = (−1)2+2 det
µ 1
3
3
1
¶
= −8.
Deﬁnition 11.1.7 The determinant of a 3 × 3 matrix, A, is obtained by picking a
row (column) and taking the product of each entry in that row (column) with its cofactor and
adding these up. This process when applied to the ith row (column) is known as expanding
the determinant along the ith row (column).

11.1.
BASIC TECHNIQUES AND PROPERTIES
199
Example 11.1.8 Find the determinant of
A =


1
2
3
4
3
2
3
2
1

.
Here is how it is done by “expanding along the ﬁrst column”.
1
cof(A)11
z
}|
{
(−1)1+1
¯¯¯¯
3
2
2
1
¯¯¯¯ + 4
cof(A)21
z
}|
{
(−1)2+1
¯¯¯¯
2
3
2
1
¯¯¯¯ + 3
cof(A)31
z
}|
{
(−1)3+1
¯¯¯¯
2
3
3
2
¯¯¯¯ = 0.
You see, we just followed the rule in the above deﬁnition. We took the 1 in the ﬁrst column
and multiplied it by its cofactor, the 4 in the ﬁrst column and multiplied it by its cofactor,
and the 3 in the ﬁrst column and multiplied it by its cofactor. Then we added these numbers
together.
You could also expand the determinant along the second row as follows.
4
cof(A)21
z
}|
{
(−1)2+1
¯¯¯¯
2
3
2
1
¯¯¯¯ + 3
cof(A)22
z
}|
{
(−1)2+2
¯¯¯¯
1
3
3
1
¯¯¯¯ + 2
cof(A)23
z
}|
{
(−1)2+3
¯¯¯¯
1
2
3
2
¯¯¯¯ = 0.
Observe this gives the same number.
You should try expanding along other rows and
columns. If you don’t make any mistakes, you will always get the same answer.
What about a 4 × 4 matrix? You know now how to ﬁnd the determinant of a 3 × 3
matrix. The pattern is the same.
Deﬁnition 11.1.9 Suppose A is a 4×4 matrix. The ijth minor is the determinant
of the 3 × 3 matrix you obtain when you delete the ith row and the jth column. The ijth
cofactor, cof (A)ij is deﬁned to be (−1)i+j ×
¡
ijth minor
¢
. In words, you multiply (−1)i+j
times the ijth minor to get the ijth cofactor.
Deﬁnition 11.1.10 The determinant of a 4×4 matrix, A, is obtained by picking a
row (column) and taking the product of each entry in that row (column) with its cofactor and
adding these up. This process when applied to the ith row (column) is known as expanding
the determinant along the ith row (column).
Example 11.1.11 Find det (A) where
A =




1
2
3
4
5
4
2
3
1
3
4
5
3
4
3
2




As in the case of a 3 × 3 matrix, you can expand this along any row or column. Lets
pick the third column. det (A) =
3 (−1)1+3
¯¯¯¯¯¯
5
4
3
1
3
5
3
4
2
¯¯¯¯¯¯
+ 2 (−1)2+3
¯¯¯¯¯¯
1
2
4
1
3
5
3
4
2
¯¯¯¯¯¯
+
4 (−1)3+3
¯¯¯¯¯¯
1
2
4
5
4
3
3
4
2
¯¯¯¯¯¯
+ 3 (−1)4+3
¯¯¯¯¯¯
1
2
4
5
4
3
1
3
5
¯¯¯¯¯¯
.

200
DETERMINANTS 2,3 OCT.
Now you know how to expand each of these 3×3 matrices along a row or a column. If you do
so, you will get −12 assuming you make no mistakes. You could expand this matrix along
any row or any column and assuming you make no mistakes, you will always get the same
thing which is deﬁned to be the determinant of the matrix, A. This method of evaluating
a determinant by expanding along a row or a column is called the method of Laplace
expansion.
Note that each of the four terms above involves three terms consisting of determinants
of 2×2 matrices and each of these will need 2 terms. Therefore, there will be 4×3×2 = 24
terms to evaluate in order to ﬁnd the determinant using the method of Laplace expansion.
Suppose now you have a 10 × 10 matrix and you follow the above pattern for evaluating
determinants. By analogy to the above, there will be 10! = 3, 628 , 800 terms involved in
the evaluation of such a determinant by Laplace expansion along a row or column. This is
a lot of terms.
In addition to the diﬃculties just discussed, you should regard the above claim that you
always get the same answer by picking any row or column with considerable skepticism. It
is incredible and not at all obvious. However, it requires a little eﬀort to establish it. This
is done in the section on the theory of the determinant. The above examples motivate the
following deﬁnitions, the second of which is incredible.
Deﬁnition 11.1.12 Let A = (aij) be an n×n matrix and suppose the determinant
of a (n −1) × (n −1) matrix has been deﬁned. Then a new matrix called the cofactor
matrix, cof (A) is deﬁned by cof (A) = (cij) where to obtain cij delete the ith row and
the jth column of A, take the determinant of the (n −1) × (n −1) matrix which results,
(This is called the ijth minor of A. ) and then multiply this number by (−1)i+j. Thus
(−1)i+j ×
¡
the ijth minor
¢
equals the ijth cofactor. To make the formulas easier to remem-
ber, cof (A)ij will denote the ijth entry of the cofactor matrix.
With this deﬁnition of the cofactor matrix, here is how to deﬁne the determinant of an
n × n matrix.
Deﬁnition 11.1.13 Let A be an n × n matrix where n ≥2 and suppose the deter-
minant of an (n −1) × (n −1) has been deﬁned. Then
det (A) =
n
X
j=1
aij cof (A)ij =
n
X
i=1
aij cof (A)ij .
(11.1)
The ﬁrst formula consists of expanding the determinant along the ith row and the second
expands the determinant along the jth column.
Theorem 11.1.14 Expanding the n × n matrix along any row or column always
gives the same answer so the above deﬁnition is a good deﬁnition.
11.1.2
The Determinant Of A Triangular Matrix
Notwithstanding the diﬃculties involved in using the method of Laplace expansion, certain
types of matrices are very easy to deal with.
Deﬁnition 11.1.15 A matrix M, is upper triangular if Mij = 0 whenever i > j.
Thus such a matrix equals zero below the main diagonal, the entries of the form Mii, as
shown.






∗
∗
· · ·
∗
0
∗
...
...
...
...
...
∗
0
· · ·
0
∗







11.1.
BASIC TECHNIQUES AND PROPERTIES
201
A lower triangular matrix is deﬁned similarly as a matrix for which all entries above the
main diagonal are equal to zero.
You should verify the following using the above theorem on Laplace expansion.
Corollary 11.1.16 Let M be an upper (lower) triangular matrix.
Then det (M) is
obtained by taking the product of the entries on the main diagonal.
Example 11.1.17 Let
A =




1
2
3
77
0
2
6
7
0
0
3
33.7
0
0
0
−1




Find det (A) .
From the above corollary, it suﬃces to take the product of the diagonal elements. Thus
det (A) = 1 × 2 × 3 × (−1) = −6. Without using the corollary, you could expand along the
ﬁrst column. This gives
1
¯¯¯¯¯¯
2
6
7
0
3
33.7
0
0
−1
¯¯¯¯¯¯
+ 0 (−1)2+1
¯¯¯¯¯¯
2
3
77
0
3
33.7
0
0
−1
¯¯¯¯¯¯
+
0 (−1)3+1
¯¯¯¯¯¯
2
3
77
2
6
7
0
0
−1
¯¯¯¯¯¯
+ 0 (−1)4+1
¯¯¯¯¯¯
2
3
77
2
6
7
0
3
33.7
¯¯¯¯¯¯
and the only nonzero term in the expansion is
1
¯¯¯¯¯¯
2
6
7
0
3
33.7
0
0
−1
¯¯¯¯¯¯
.
Now expand this along the ﬁrst column to obtain
1 ×
µ
2 ×
¯¯¯¯
3
33.7
0
−1
¯¯¯¯ + 0 (−1)2+1
¯¯¯¯
6
7
0
−1
¯¯¯¯ + 0 (−1)3+1
¯¯¯¯
6
7
3
33.7
¯¯¯¯
¶
= 1 × 2 ×
¯¯¯¯
3
33.7
0
−1
¯¯¯¯
Next expand this last determinant along the ﬁrst column to obtain the above equals
1 × 2 × 3 × (−1) = −6
which is just the product of the entries down the main diagonal of the original matrix.
11.1.3
Properties Of Determinants
There are many properties satisﬁed by determinants. Some of these properties have to do
with row operations. Recall the row operations.
Deﬁnition 11.1.18 The row operations consist of the following
1. Switch two rows.

202
DETERMINANTS 2,3 OCT.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to itself.
Theorem 11.1.19 Let A be an n × n matrix and let A1 be a matrix which results
from multiplying some row of A by a scalar, c. Then c det (A) = det (A1).
Example 11.1.20 Let A =
µ
1
2
3
4
¶
, A1 =
µ
2
4
3
4
¶
. det (A) = −2, det (A1) = −4.
Theorem 11.1.21 Let A be an n × n matrix and let A1 be a matrix which results
from switching two rows of A. Then det (A) = −det (A1) . Also, if one row of A is a multiple
of another row of A, then det (A) = 0.
Example 11.1.22 Let A =
µ
1
2
3
4
¶
and let A1 =
µ
3
4
1
2
¶
. det A = −2, det (A1) = 2.
Theorem 11.1.23 Let A be an n × n matrix and let A1 be a matrix which results
from applying row operation 3. That is you replace some row by a multiple of another row
added to itself. Then det (A) = det (A1).
Example 11.1.24 Let A =
µ
1
2
3
4
¶
and let A1 =
µ
1
2
4
6
¶
. Thus the second row of A1
is one times the ﬁrst row added to the second row. det (A) = −2 and det (A1) = −2.
Theorem 11.1.25 In Theorems 11.1.19 - 11.1.23 you can replace the word, “row”
with the word “column”.
There are two other major properties of determinants which do not involve row opera-
tions.
Theorem 11.1.26 Let A and B be two n × n matrices. Then
det (AB) = det (A) det (B).
Also,
det (A) = det
¡
AT ¢
.
Example 11.1.27 Compare det (AB) and det (A) det (B) for
A =
µ
1
2
−3
2
¶
, B =
µ
3
2
4
1
¶
.
First
AB =
µ
1
2
−3
2
¶ µ
3
2
4
1
¶
=
µ
11
4
−1
−4
¶
and so
det (AB) = det
µ
11
4
−1
−4
¶
= −40.
Now
det (A) = det
µ
1
2
−3
2
¶
= 8
and
det (B) = det
µ
3
2
4
1
¶
= −5.
Thus det (A) det (B) = 8 × (−5) = −40.

11.1.
BASIC TECHNIQUES AND PROPERTIES
203
11.1.4
Finding Determinants Using Row Operations
Theorems 11.1.23 - 11.1.25 can be used to ﬁnd determinants using row operations.
As pointed out above, the method of Laplace expansion will not be practical for any
matrix of large size. Here is an example in which all the row operations are used.
Example 11.1.28 Find the determinant of the matrix,
A =




1
2
3
4
5
1
2
3
4
5
4
3
2
2
−4
5




Replace the second row by (−5) times the ﬁrst row added to it. Then replace the third
row by (−4) times the ﬁrst row added to it. Finally, replace the fourth row by (−2) times
the ﬁrst row added to it. This yields the matrix,
B =




1
2
3
4
0
−9
−13
−17
0
−3
−8
−13
0
−2
−10
−3




and from Theorem 11.1.23, it has the same determinant as A. Now using other row opera-
tions, det (B) =
¡ −1
3
¢
det (C) where
C =




1
2
3
4
0
0
11
22
0
−3
−8
−13
0
6
30
9



.
The second row was replaced by (−3) times the third row added to the second row. By
Theorem 11.1.23 this didn’t change the value of the determinant. Then the last row was
multiplied by (−3) . By Theorem 11.1.19 the resulting matrix has a determinant which is
(−3) times the determinant of the unmultiplied matrix. Therefore, we multiplied by −1/3
to retain the correct value. Now replace the last row with 2 times the third added to it.
This does not change the value of the determinant by Theorem 11.1.23. Finally switch
the third and second rows. This causes the determinant to be multiplied by (−1) . Thus
det (C) = −det (D) where
D =




1
2
3
4
0
−3
−8
−13
0
0
11
22
0
0
14
−17




You could do more row operations or you could note that this can be easily expanded along
the ﬁrst column followed by expanding the 3×3 matrix which results along its ﬁrst column.
Thus
det (D) = 1 (−3)
¯¯¯¯
11
22
14
−17
¯¯¯¯ = 1485
and so det (C) = −1485 and det (A) = det (B) =
¡ −1
3
¢
(−1485) = 495.
Example 11.1.29 Find the determinant of the matrix




1
2
3
2
1
−3
2
1
2
1
2
5
3
−4
1
2





204
DETERMINANTS 2,3 OCT.
Replace the second row by (−1) times the ﬁrst row added to it. Next take −2 times the
ﬁrst row and add to the third and ﬁnally take −3 times the ﬁrst row and add to the last
row. This yields




1
2
3
2
0
−5
−1
−1
0
−3
−4
1
0
−10
−8
−4



.
By Theorem 11.1.23 this matrix has the same determinant as the original matrix. Remember
you can work with the columns also. Take −5 times the last column and add to the second
column. This yields




1
−8
3
2
0
0
−1
−1
0
−8
−4
1
0
10
−8
−4




By Theorem 11.1.25 this matrix has the same determinant as the original matrix. Now take
(−1) times the third row and add to the top row. This gives.




1
0
7
1
0
0
−1
−1
0
−8
−4
1
0
10
−8
−4




which by Theorem 11.1.23 has the same determinant as the original matrix. Lets expand
it now along the ﬁrst column. This yields the following for the determinant of the original
matrix.
det


0
−1
−1
−8
−4
1
10
−8
−4


which equals
8 det
µ
−1
−1
−8
−4
¶
+ 10 det
µ
−1
−1
−4
1
¶
= −82
We suggest you do not try to be fancy in using row operations. That is, stick mostly to
the one which replaces a row or column with a multiple of another row or column added to
it. Also note there is no way to check your answer other than working the problem more
than one way. To be sure you have gotten it right you must do this.
11.1.5
A Formula For The Inverse
The deﬁnition of the determinant in terms of Laplace expansion along a row or column
also provides a way to give a formula for the inverse of a matrix. Recall the deﬁnition of
the inverse of a matrix. Also recall the deﬁnition of the cofactor matrix given in Deﬁnition
11.1.12 on Page 200. This cofactor matrix was just the matrix which results from replacing
the ijth entry of the matrix with the ijth cofactor.
The following theorem says that to ﬁnd the inverse, take the transpose of the cofactor
matrix and divide by the determinant. The transpose of the cofactor matrix is called the
adjugate or sometimes the classical adjoint of the matrix A. In other words, A−1 is
equal to one divided by the determinant of A times the adjugate matrix of A. This is what
the following theorem says with more precision.

11.1.
BASIC TECHNIQUES AND PROPERTIES
205
Theorem 11.1.30 A−1 exists if and only if det(A) ̸= 0. If det(A) ̸= 0, then
A−1 =
¡
a−1
ij
¢
where
a−1
ij = det(A)−1 cof (A)ji
for cof (A)ij the ijth cofactor of A.
Example 11.1.31 Find the inverse of the matrix,
A =


1
2
3
3
0
1
1
2
1


First ﬁnd the determinant of this matrix. Using Theorems 11.1.23 - 11.1.25 on Page 202,
the determinant of this matrix equals the determinant of the matrix,


1
2
3
0
−6
−8
0
0
−2


which equals 12. The cofactor matrix of A is


−2
−2
6
4
−2
0
2
8
−6

.
Each entry of A was replaced by its cofactor. Therefore, from the above theorem, the inverse
of A should equal
1
12


−2
−2
6
4
−2
0
2
8
−6


T
=







−1
6
1
3
1
6
−1
6
−1
6
2
3
1
2
0
−1
2







.
Does it work? You should check to see if it does. When the matrices are multiplied







−1
6
1
3
1
6
−1
6
−1
6
2
3
1
2
0
−1
2









1
2
3
3
0
1
1
2
1

=


1
0
0
0
1
0
0
0
1


and so it is correct.
Example 11.1.32 Find the inverse of the matrix,
A =







1
2
0
1
2
−1
6
1
3
−1
2
−5
6
2
3
−1
2








206
DETERMINANTS 2,3 OCT.
First ﬁnd its determinant. This determinant is 1
6. The inverse is therefore equal to
6















¯¯¯¯¯¯
1
3
−1
2
2
3
−1
2
¯¯¯¯¯¯
−
¯¯¯¯¯¯
−1
6
−1
2
−5
6
−1
2
¯¯¯¯¯¯
¯¯¯¯¯¯
−1
6
1
3
−5
6
2
3
¯¯¯¯¯¯
−
¯¯¯¯¯¯
0
1
2
2
3
−1
2
¯¯¯¯¯¯
¯¯¯¯¯¯
1
2
1
2
−5
6
−1
2
¯¯¯¯¯¯
−
¯¯¯¯¯¯
1
2
0
−5
6
2
3
¯¯¯¯¯¯
¯¯¯¯¯¯
0
1
2
1
3
−1
2
¯¯¯¯¯¯
−
¯¯¯¯¯¯
1
2
1
2
−1
6
−1
2
¯¯¯¯¯¯
¯¯¯¯¯¯
1
2
0
−1
6
1
3
¯¯¯¯¯¯















T
.
Expanding all the 2 × 2 determinants this yields
6







1
6
1
3
1
6
1
3
1
6
−1
3
−1
6
1
6
1
6







T
=


1
2
−1
2
1
1
1
−2
1


Always check your work.


1
2
−1
2
1
1
1
−2
1









1
2
0
1
2
−1
6
1
3
−1
2
−5
6
2
3
−1
2







=


1
0
0
0
1
0
0
0
1


and so we got it right. If the result of multiplying these matrices had been something other
than the identity matrix, you would know there was an error. When this happens, you
need to search for the mistake if you am interested in getting the right answer. A common
mistake is to forget to take the transpose of the cofactor matrix.
Proof of Theorem 11.1.30: From the deﬁnition of the determinant in terms of ex-
pansion along a column, and letting (air) = A, if det (A) ̸= 0,
n
X
i=1
air cof (A)ir det(A)−1 = det(A) det(A)−1 = 1.
Now consider
n
X
i=1
air cof (A)ik det(A)−1
when k ̸= r. Replace the kth column with the rth column to obtain a matrix, Bk whose
determinant equals zero by Theorem 11.1.21. However, expanding this matrix, Bk along
the kth column yields
0 = det (Bk) det (A)−1 =
n
X
i=1
air cof (A)ik det (A)−1
Summarizing,
n
X
i=1
air cof (A)ik det (A)−1 = δrk ≡
½ 1 if r = k
0 if r ̸= k
.

11.1.
BASIC TECHNIQUES AND PROPERTIES
207
Now
n
X
i=1
air cof (A)ik =
n
X
i=1
air cof (A)T
ki
which is the krth entry of cof (A)T A. Therefore,
cof (A)T
det (A) A = I.
(11.2)
Using the other formula in Deﬁnition 11.1.13, and similar reasoning,
n
X
j=1
arj cof (A)kj det (A)−1 = δrk
Now
n
X
j=1
arj cof (A)kj =
n
X
j=1
arj cof (A)T
jk
which is the rkth entry of A cof (A)T . Therefore,
Acof (A)T
det (A) = I,
(11.3)
and it follows from 11.2 and 11.3 that A−1 =
¡
a−1
ij
¢
, where
a−1
ij = cof (A)ji det (A)−1 .
In other words,
A−1 = cof (A)T
det (A) .
Now suppose A−1 exists. Then by Theorem 11.1.26,
1 = det (I) = det
¡
AA−1¢
= det (A) det
¡
A−1¢
so det (A) ̸= 0. This proves the theorem.
This way of ﬁnding inverses is especially useful in the case where it is desired to ﬁnd the
inverse of a matrix whose entries are functions.
Example 11.1.33 Suppose
A (t) =


et
0
0
0
cos t
sin t
0
−sin t
cos t


Show that A (t)−1 exists and then ﬁnd it.
First note det (A (t)) = et ̸= 0 so A (t)−1 exists. The cofactor matrix is
C (t) =


1
0
0
0
et cos t
et sin t
0
−et sin t
et cos t


and so the inverse is
1
et


1
0
0
0
et cos t
et sin t
0
−et sin t
et cos t


T
=


e−t
0
0
0
cos t
−sin t
0
sin t
cos t

.

208
DETERMINANTS 2,3 OCT.

Eigenvalues And Eigenvectors
Of A Matrix 4-6 Oct.
Quiz
1. Here is a matrix.


1
0
2
2
1
3
3
2
1


Find its determinant.
2. Use the theory of determinants to ﬁnd the inverse of the matrix,


2
1
1
1
0
1
0
1
1


3. Let C = F T F where F is an n × n real matrix. Show det (C) ≥0.
4. Show that if A−1 exists, then det
¡
A−1¢
= 1/ det (A).
Spectral Theory refers to the study of eigenvalues and eigenvectors of a matrix. It is of
fundamental importance in many areas. Row operations will no longer be such a useful tool
in this subject.
12.0.6
Deﬁnition Of Eigenvectors And Eigenvalues
In this section, F = C.
To illustrate the idea behind what will be discussed, consider the following example.
Example 12.0.34 Here is a matrix.


0
5
−10
0
22
16
0
−9
−2

.
Multiply this matrix by the vector


−5
−4
3


209

210
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
and see what happens. Then multiply it by


1
0
0


and see what happens. Does this matrix act this way for some other vector?
First


0
5
−10
0
22
16
0
−9
−2




−5
−4
3

=


−50
−40
30

= 10


−5
−4
3

.
Next


0
5
−10
0
22
16
0
−9
−2




1
0
0

=


0
0
0

= 0


1
0
0

.
When you multiply the ﬁrst vector by the given matrix, it stretched the vector, multiplying
it by 10. When you multiplied the matrix by the second vector it sent it to the zero vector.
Now consider


0
5
−10
0
22
16
0
−9
−2




1
1
1

=


−5
38
−11

.
In this case, multiplication by the matrix did not result in merely multiplying the vector by
a number.
In the above example, the ﬁrst two vectors were called eigenvectors and the numbers, 10
and 0 are called eigenvalues. Not every number is an eigenvalue and not every vector is an
eigenvector.
Deﬁnition 12.0.35 Let M be an n×n matrix and let x ∈Cn be a nonzero vector
for which
Mx = λx
(12.1)
for some scalar, λ. Then x is called an eigenvector and λ is called an eigenvalue (char-
acteristic value) of the matrix, M.
Note: Eigenvectors are never equal to zero!
The set of all eigenvalues of an n × n matrix, M, is denoted by σ (M) and is referred to as
the spectrum of M.
The eigenvectors of a matrix M are those vectors, x for which multiplication by M
results in a scalar multiple of x. Since the zero vector, 0 has no direction this would make
no sense for the zero vector. As noted above, 0 is never allowed to be an eigenvector. How
can eigenvectors and eigenvalues be identiﬁed?
There is an important characterization of when a matrix is invertible in terms of deter-
minants. This is proved completely in the section on the theory of determinants where a
formula is given for the inverse in terms of the determinant and cofactors.
Theorem 12.0.36 Let M be an n × n matrix and let TM denote the linear trans-
formation determined by M. Thus TMx = Mx. Then the following are equivalent.
1. TM is one to one.

211
2. TM is onto.
3. det (M) ̸= 0.
Suppose x satisﬁes 12.1. Then
(M −λI) x = 0
for some x ̸= 0. (Equivalently, you could write (λI −M) x = 0.) Sometimes we will use
(λI −M) x = 0 and sometimes (M −λI) x = 0. It makes absolutely no diﬀerence and you
should use whichever you like better. Therefore, the matrix M −λI cannot have an inverse
because if it did, the equation could be solved,
x =
³
(M −λI)−1 (M −λI)
´
x = (M −λI)−1 ((M −λI) x) = (M −λI)−1 0 = 0,
and this would require x = 0, contrary to the requirement that x ̸= 0. By Theorem 12.0.36,
det (M −λI) = 0.
(12.2)
(Equivalently you could write det (λI −M) = 0.) The expression, det (λI −M) or equiva-
lently, det (M −λI) is a polynomial called the characteristic polynomial and the above
equation is called the characteristic equation. For M an n × n matrix, it follows from the
theorem on expanding a matrix by its cofactor that det (M −λI) is a polynomial of degree
n. As such, the equation, 12.2 has a solution, λ ∈C by the fundamental theorem of algebra.
Is it actually an eigenvalue? The answer is yes by Theorem 12.0.36. Since λI −M has no
inverse due to its determinant equaling zero, it must fail to be one to one and so there must
exist a nonzero vector which it maps to zero. This proves the following corollary.
Corollary 12.0.37 Let M be an n × n matrix and det (M −λI) = 0. Then there exists
a nonzero vector, x ∈Cn such that (M −λI) x = 0.
12.0.7
Finding Eigenvectors And Eigenvalues
As an example, consider the following.
Example 12.0.38 Find the eigenvalues and eigenvectors for the matrix,
A =


5
−10
−5
2
14
2
−4
−8
6

.
You ﬁrst need to identify the eigenvalues. Recall this requires the solution of the equation
det (A −λI) = 0.
In this case this equation is
det




5
−10
−5
2
14
2
−4
−8
6

−λ


1
0
0
0
1
0
0
0
1



= 0
When you expand this determinant and simplify, you ﬁnd the equation you need to solve is
(λ −5)
¡
λ2 −20λ + 100
¢
= 0

212
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
and so the eigenvalues are
5, 10, 10.
We have listed 10 twice because it is a zero of multiplicity two due to
λ2 −20λ + 100 = (λ −10)2 .
Having found the eigenvalues, it only remains to ﬁnd the eigenvectors. First ﬁnd the
eigenvectors for λ = 5. As explained above, this requires you to solve the equation,




5
−10
−5
2
14
2
−4
−8
6

−5


1
0
0
0
1
0
0
0
1






x
y
z

=


0
0
0

.
That is you need to ﬁnd the solution to


0
−10
−5
2
9
2
−4
−8
1




x
y
z

=


0
0
0


By now this is an old problem. You set up the augmented matrix and row reduce to get the
solution. Thus the matrix you must row reduce is


0
−10
−5
|
0
2
9
2
|
0
−4
−8
1
|
0

.
(12.3)
The row reduced echelon form is



1
0
−5
4
|
0
0
1
1
2
|
0
0
0
0
|
0



and so the solution is any vector of the form



5
4t
−1
2 t
t


= t



5
4
−1
2
1



where t ∈F. You would obtain the same collection of vectors if you replaced t with 4t. Thus
a simpler description for the solutions to this system of equations whose augmented matrix
is in 12.3 is
t


5
−2
4


(12.4)
where t ∈F. Now you need to remember that you can’t take t = 0 because this would result
in the zero vector and
Eigenvectors are never equal to zero!
Other than this value, every other choice of z in 12.4 results in an eigenvector. It is a good
idea to check your work! To do so, we will take the original matrix and multiply by this
vector and see if we get 5 times this vector.


5
−10
−5
2
14
2
−4
−8
6




5
−2
4

=


25
−10
20

= 5


5
−2
4



213
so it appears this is correct. Always check your work on these problems if you care about
getting the answer right.
The parameter, t is sometimes called a free variable. The set of vectors in 12.4 is
called the eigenspace and it equals ker (A −λI) . You should observe that in this case
the eigenspace has dimension 1 because the eigenspace is the span of a single vector. In
general, you obtain the solution from the row echelon form and the number of diﬀerent free
variables gives you the dimension of the eigenspace. Just remember that not every vector
in the eigenspace is an eigenvector. The vector, 0 is not an eigenvector although it is in the
eigenspace because
Eigenvectors are never equal to zero!
Next consider the eigenvectors for λ = 10. These vectors are solutions to the equation,




5
−10
−5
2
14
2
−4
−8
6

−10


1
0
0
0
1
0
0
0
1






x
y
z

=


0
0
0


That is you must ﬁnd the solutions to


−5
−10
−5
2
4
2
−4
−8
−4




x
y
z

=


0
0
0


which reduces to consideration of the augmented matrix,


−5
−10
−5
|
0
2
4
2
|
0
−4
−8
−4
|
0


The row reduced echelon form for this matrix is


1
2
1
0
0
0
0
0
0
0
0
0


and so the eigenvectors are of the form


−2s −t
s
t

= s


−2
1
0

+ t


−1
0
1

.
You can’t pick t and s both equal to zero because this would result in the zero vector and
Eigenvectors are never equal to zero!
However, every other choice of t and s does result in an eigenvector for the eigenvalue λ = 10.
As in the case for λ = 5 you should check your work if you care about getting it right.


5
−10
−5
2
14
2
−4
−8
6




−1
0
1

=


−10
0
10

= 10


−1
0
1


so it worked. The other vector will also work. Check it.

214
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
12.0.8
A Warning
The above example shows how to ﬁnd eigenvectors and eigenvalues algebraically. You may
have noticed it is a bit long. Sometimes students try to ﬁrst row reduce the matrix be-
fore looking for eigenvalues. This is a terrible idea because row operations destroy the
eigenvalues. The eigenvalue problem is really not about row operations.
The general eigenvalue problem is the hardest problem in algebra and people still do
research on ways to ﬁnd eigenvalues and their eigenvectors. If you are doing anything which
would yield a way to ﬁnd eigenvalues and eigenvectors for general matrices without too
much trouble, the thing you are doing will certainly be wrong. The problems you will see
in these notes are not too hard because they are cooked up by us to be easy. Later we
will describe general methods to compute eigenvalues and eigenvectors numerically. These
methods work even when the problem is not cooked up to be easy.
If you are so fortunate as to ﬁnd the eigenvalues as in the above example, then ﬁnding the
eigenvectors does reduce to row operations and this part of the problem is easy. However,
ﬁnding the eigenvalues along with the eigenvectors is anything but easy because for an
n × n matrix, it involves solving a polynomial equation of degree n. If you only ﬁnd a good
approximation to the eigenvalue, it won’t work. It either is or is not an eigenvalue and
if it is not, the only solution to the equation, (M −λI) x = 0 will be the zero solution as
explained above and
Eigenvectors are never equal to zero!
Here is another example.
Example 12.0.39 Let
A =


2
2
−2
1
3
−1
−1
1
1


First ﬁnd the eigenvalues.
det




2
2
−2
1
3
−1
−1
1
1

−λ


1
0
0
0
1
0
0
0
1



= 0
This reduces to λ3 −6λ2 + 8λ = 0 and the solutions are 0, 2, and 4.
0 Can be an Eigenvalue!
Now ﬁnd the eigenvectors. For λ = 0 the augmented matrix for ﬁnding the solutions is


2
2
−2
|
0
1
3
−1
|
0
−1
1
1
|
0


and the row reduced echelon form is


1
0
−1
0
0
1
0
0
0
0
0
0



215
Therefore, the eigenvectors are of the form
t


1
0
1


where t ̸= 0.
Next ﬁnd the eigenvectors for λ = 2. The augmented matrix for the system of equations
needed to ﬁnd these eigenvectors is


0
2
−2
|
0
1
1
−1
|
0
−1
1
−1
|
0


and the row reduced echelon form is


1
0
0
0
0
1
−1
0
0
0
0
0


and so the eigenvectors are of the form
t


0
1
1


where t ̸= 0.
Finally ﬁnd the eigenvectors for λ = 4. The augmented matrix for the system of equations
needed to ﬁnd these eigenvectors is


−2
2
−2
|
0
1
−1
−1
|
0
−1
1
−3
|
0


and the row reduced echelon form is


1
−1
0
0
0
0
1
0
0
0
0
0

.
Therefore, the eigenvectors are of the form
t


1
1
0


where t ̸= 0.
12.0.9
Defective And Nondefective Matrices
Deﬁnition 12.0.40 By the fundamental theorem of algebra, it is possible to write
the characteristic equation in the form
(λ −λ1)r1 (λ −λ2)r2 · · · (λ −λm)rm = 0
where ri is some integer no smaller than 1. Thus the eigenvalues are λ1, λ2, · · ·, λm. The
algebraic multiplicity of λj is deﬁned to be rj.

216
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Example 12.0.41 Consider the matrix,
A =


1
1
0
0
1
1
0
0
1


(12.5)
What is the algebraic multiplicity of the eigenvalue λ = 1?
In this case the characteristic equation is
det (A −λI) = (1 −λ)3 = 0
or equivalently,
det (λI −A) = (λ −1)3 = 0.
Therefore, λ is of algebraic multiplicity 3.
Deﬁnition 12.0.42 The geometric multiplicity of an eigenvalue is the dimen-
sion of the eigenspace,
ker (A −λI) .
Example 12.0.43 Find the geometric multiplicity of λ = 1 for the matrix in 12.5.
We need to solve


0
1
0
0
0
1
0
0
0




x
y
z

=


0
0
0

.
The augmented matrix which must be row reduced to get this solution is therefore,


0
1
0
|
0
0
0
1
|
0
0
0
0
|
0


This requires z = y = 0 and x is arbitrary. Thus the eigenspace is
t


1
0
0

, t ∈F.
It follows the geometric multiplicity of λ = 1 is 1.
Deﬁnition 12.0.44 An n × n matrix is called defective if the geometric multi-
plicity is not equal to the algebraic multiplicity for some eigenvalue. Sometimes such an
eigenvalue for which the geometric multiplicity is not equal to the algebraic multiplicity is
called a defective eigenvalue. If the geometric multiplicity for an eigenvalue equals the alge-
braic multiplicity, the eigenvalue is sometimes referred to as nondefective.
Here is another more interesting example of a defective matrix.
Example 12.0.45 Let
A =


2
−2
−1
−2
−1
−2
14
25
14

.
Find the eigenvectors and eigenvalues.

217
In this case the eigenvalues are 3, 6, 6 where we have listed 6 twice because it is a zero
of algebraic multiplicity two, the characteristic equation being
(λ −3) (λ −6)2 = 0.
It remains to ﬁnd the eigenvectors for these eigenvalues. First consider the eigenvectors for
λ = 3. You must solve




2
−2
−1
−2
−1
−2
14
25
14

−3


1
0
0
0
1
0
0
0
1






x
y
z

=


0
0
0

.
The augmented matrix is


−1
−2
−1
|
0
−2
−4
−2
|
0
14
25
11
|
0


and the row reduced echelon form is


1
0
−1
0
0
1
1
0
0
0
0
0


so the eigenvectors are nonzero vectors of the form


t
−t
t

= t


1
−1
1


Next consider the eigenvectors for λ = 6. This requires you to solve




2
−2
−1
−2
−1
−2
14
25
14

−6


1
0
0
0
1
0
0
0
1






x
y
z

=


0
0
0


and the augmented matrix for this system of equations is


−4
−2
−1
|
0
−2
−7
−2
|
0
14
25
8
|
0


The row reduced echelon form is



1
0
1
8
0
0
1
1
4
0
0
0
0
0



and so the eigenvectors for λ = 6 are of the form
t



−1
8
−1
4
1



or written more simply,
t


−1
−2
8



218
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
where t ∈F.
Note that in this example the eigenspace for the eigenvalue, λ = 6 is of dimension 1
because there is only one parameter. However, this eigenvalue is of multiplicity two as a
root to the characteristic equation. Thus this eigenvalue is a defective eigenvalue. However,
the eigenvalue 3 is nondefective. The matrix is defective because it has a defective eigenvalue.
The word, defective, seems to suggest there is something wrong with the matrix. This
is in fact the case. Defective matrices are a lot of trouble in applications and we may wish
they never occurred. However, they do occur as the above example shows. When you study
linear systems of diﬀerential equations, you will have to deal with the case of defective
matrices and you will see how awful they are. The reason these matrices are so horrible
to work with is that it is impossible to obtain a basis of eigenvectors. When you study
diﬀerential equations, solutions to ﬁrst order systems are expressed in terms of eigenvectors
of a certain matrix times eλt where λ is an eigenvalue. In order to obtain a general solution
of this sort, you must have a basis of eigenvectors. For a defective matrix, such a basis does
not exist and so you have to go to something called generalized eigenvectors. Unfortunately,
it is never explained in beginning diﬀerential equations courses why there are enough
generalized eigenvectors and eigenvectors to represent the general solution. In fact, this
reduces to a diﬃcult question in linear algebra equivalent to the existence of something
called the Jordan Canonical form which is much more diﬃcult than everything discussed in
the entire diﬀerential equations course. If you become interested in this, see a good book in
linear algebra. The good ones do discuss this topic. There is such a linear algebra book on
my web page.
Ultimately, the algebraic issues which will occur in diﬀerential equations are a red herring
anyway. The real issues relative to existence of solutions to systems of ordinary diﬀerential
equations are analytical, having much more to do with calculus than with linear algebra
although this will likely not be made clear when you take a beginning diﬀerential equations
class.
In terms of algebra, this lack of a basis of eigenvectors says that it is impossible to obtain
a diagonal matrix which is similar to the given matrix.
Although there may be repeated roots to the characteristic equation, 12.2 and it is not
known whether the matrix is defective in this case, there is an important theorem which
holds when considering eigenvectors which correspond to distinct eigenvalues.
Theorem 12.0.46 Suppose Mvi = λivi, i = 1, · · ·, r , vi ̸= 0, and that if i ̸= j,
then λi ̸= λj. Then the set of eigenvectors, {v1, · · ·, vr} is linearly independent.
Proof: If the conclusion of this theorem is not true, then there exist non zero scalars,
ckj such that
m
X
j=1
ckjvkj = 0.
(12.6)
Take m to be the smallest number possible for an expression of the form 12.6 to hold. Then
solving for vk1
vk1 =
X
kj̸=k1
dkjvkj
(12.7)
where dkj = ckj/ck1 ̸= 0. Multiplying both sides by M,
λk1vk1 =
X
kj̸=k1
dkjλkjvkj,
which from 12.7 yields
X
kj̸=k1
dkjλk1vkj =
X
kj̸=k1
dkjλkjvkj

219
and therefore,
0 =
X
kj̸=k1
dkj
¡
λk1 −λkj
¢
vkj,
a sum having fewer than m terms. However, from the assumption that m is as small as
possible for 12.6 to hold with all the scalars, ckj non zero, it follows that for some j ̸= 1,
dkj
¡
λk1 −λkj
¢
= 0
which implies λk1 = λkj, a contradiction.
12.0.10
Diagonalization
Deﬁnition 12.0.47 Let A be an n × n matrix. Then A is diagonalizable if there
exists an invertible matrix, S such that
S−1AS = D
where D is a diagonal matrix. This means D has a zero as every entry except for the main
diagonal.
Theorem 12.0.48 An n × n matrix is diagonalizable if and only if Fn has a basis
of eigenvectors of A. Furthermore, you can take the matrix, S described above to be given
as
S =
¡
v1
v2
· · ·
vn
¢
where here the vk are the eigenvectors in the basis for Fn.
If A is diagonalizable, the
eigenvalues of A are the diagonal entries of the diagonal matrix.
Proof: Suppose there exists a basis of eigenvectors, {vk} where Avk = λkvk. Then let
S be given as above. It follows S−1 exists and is of the form
S−1 =





wT
1
wT
2
...
wT
n





where wT
k vj = δkj. Then



λ1
0
...
0
λn



=





wT
1
wT
2
...
wT
n





¡ λ1v1
λ2v2
· · ·
λnvn
¢
=





wT
1
wT
2
...
wT
n





¡
Av1
Av2
· · ·
Avn
¢
=
S−1AS
Next suppose A is diagonalizable so S−1AS = D. Let S =
¡
v1
v2
· · ·
vn
¢
where
the columns are the vk and
D =



λ1
0
...
0
λn




220
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Then
AS = SD =
¡
v1
v2
· · ·
vn
¢



λ1
0
...
0
λn



and so
¡
Av1
Av2
· · ·
Avn
¢
=
¡
λ1v1
λ2v2
· · ·
λnvn
¢
showing the vi are eigenvectors of A and the λk are eigenvectors. Now the vk form a basis
for Fn because the matrix, S having these vectors as columns is given to be invertible. This
proves the theorem.
Deﬁnition 12.0.49 Let A, B be two diagonal matrices. Then A is said to be sim-
ilar to B if there exists an invertible matrix, S such that B = S−1AS.
Example 12.0.50 Let A =


2
0
0
1
4
−1
−2
−4
4

. Find a matrix, S such that S−1AS = D,
a diagonal matrix.
Solving det (λI −A) = 0 yields the eigenvalues are 2 and 6 with 2 an eigenvalue of mul-
tiplicity two. Solving (2I −A) x = 0 to ﬁnd the eigenvectors, you ﬁnd that the eigenvectors
are
a


−2
1
0

+ b


1
0
1


where a, b are scalars. An eigenvector for λ = 6 is


0
1
−2

. Let the matrix S be
S =


−2
1
0
1
0
1
0
1
−2


That is, the columns are the eigenvectors. Then
S−1 =


−1
4
1
2
1
4
1
2
1
1
2
1
4
1
2
−1
4

.
S−1AS
=


−1
4
1
2
1
4
1
2
1
1
2
1
4
1
2
−1
4




2
0
0
1
4
−1
−2
−4
4




−2
1
0
1
0
1
0
1
−2


=


2
0
0
0
2
0
0
0
6

.
Example 12.0.51 Here is a matrix. A =


2
1
0
0
1
0
−1
−1
1

Find A50.

221
Sometimes this sort of problem can be made easy by using diagonalization. In this case
there are eigenvectors,


0
0
1

,


−1
1
0

,


−1
0
1

,
the ﬁrst two corresponding to λ = 1 and the last corresponding to λ = 2. Then let the
eigenvectors be the columns of the matrix, S. Thus
S =


0
−1
−1
0
1
0
1
0
1


Then also
S−1 =


1
1
1
0
1
0
−1
−1
0


and
S−1AS
=


1
1
1
0
1
0
−1
−1
0




2
1
0
0
1
0
−1
−1
1




0
−1
−1
0
1
0
1
0
1


=


1
0
0
0
1
0
0
0
2

= D
Now it follows
A = SDS−1 =


0
−1
−1
0
1
0
1
0
1




1
0
0
0
1
0
0
0
2




1
1
1
0
1
0
−1
−1
0

.
Now note that
¡
SDS−1¢2 = SDS−1SDS−1 = SD2S−1 and
¡
SDS−1¢3 = SDS−1SDS−1SDS−1 = SD3S−1,
etc. In general, you can see that
¡
SDS−1¢n = SDnS−1
In other words, An = SDnS−1. Therefore,
A50
=
SD50S−1
=


0
−1
−1
0
1
0
1
0
1




1
0
0
0
1
0
0
0
2


50 

1
1
1
0
1
0
−1
−1
0

.
Now


1
0
0
0
1
0
0
0
2


50
=


1
0
0
0
1
0
0
0
250

.

222
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
It follows
A50
=


0
−1
−1
0
1
0
1
0
1




1
0
0
0
1
0
0
0
250




1
1
1
0
1
0
−1
−1
0


=


250
−1 + 250
0
0
1
0
1 −250
1 −250
1

.
That isn’t too hard. However, this would have been horrendous if you had tried to multiply
A50 by hand.
This technique of diagonalization is also important in solving the diﬀerential equations
resulting from vibrations. Sometimes you have systems of diﬀerential equation and when
you diagonalize an appropriate matrix, you “decouple” the equations. This is very nice. It
makes hard problems trivial.
The above example is entirely typical. If A = SDS−1 then Am = SDmS−1 and it is easy
to compute Dm. More generally, you can deﬁne functions of the matrix using power series
in this way. However, the real interesting case is when A is defective. This is much more
interesting. You can always speak of things like sin (A) for A an n × n matrix. However,
more interesting functions have no power series and you have to work harder for these. This
is enough on this. One can go on and on.
12.0.11
Migration Matrices
There are applications of the eigenvalue problem which are of great importance and feature
only one eigenvalue.
Consider the following table.
A
B
A
1/4
2/3
B
3/4
1/3
In this table, 1/4 is the probability that someone in location A ends up in A after a single
unit of time. 2/3 is the probability that a person in location B ends up in location A after
a single unit of time. 3/4 is the probability that a person in location A ends up in location
B after a single unit of time and 1/3 is the probability that a person in location B ends
up in location B. Instead of the word probability, you could use the word “proportion” and
the numbers would then represent the proportion of people in the various locations who end
up in the other location after one unit of time. Thue 1/4 is the proportion of people in A
who end up in A,etc. Then this matrix is called a stochastic matrix, a Markov matrix or
a Migration matrix. In the case the numbers are interpreted as probabilities, it is called a
Markov or Stochastic matrix. In the case where they are proportions it is called a migration
matrix.
Consider it as a migration matrix and suppose that initially there are 200 people in
location A and 120 in location B. You might wonder how many there would be in the two
locations after one unit of time. This is easy to ﬁgure out. Those in A after one unit of
time consist of those in A who were in A to begin with added to those in A who started oﬀ
in B. Thus
# in A = 1
4 (200) + 2
3 (120) = 130
# in B = 3
4 (200) + 1
3 (120) = 190.

223
You can see that this amounts to nothing more than matrix multiplication. Thus, letting
(a1, b1)T be deﬁned by
µ
a1
b1
¶
=
µ
# in A after one unit of time
# in B after one unit of time
¶
It follows
µ a1
b1
¶
=
µ
1
4
2
3
3
4
1
3
¶ µ 200
120
¶
=
µ 130
190
¶
Now with this vector as new input, you can determine how many are in the two locations
after another unit of time using the same procedure. Thus letting an denote the numbers
in location A after n units of time and bn the number in B after n units of time,
µ
a2
b2
¶
=
µ
1
4
2
3
3
4
1
3
¶ µ
a1
b1
¶
=
µ
1
4
2
3
3
4
1
3
¶ µ
1
4
2
3
3
4
1
3
¶ µ
200
120
¶
=
µ
1
4
2
3
3
4
1
3
¶2 µ
200
120
¶
=
µ
159. 166 667
160. 833 333
¶
.
Obviously you need to round oﬀif you are considering people doing the migrating. Then
by analogy,
µ
an
bn
¶
=
µ
1
4
2
3
3
4
1
3
¶ µ
an−1
bn−1
¶
=
µ
1
4
2
3
3
4
1
3
¶ µ
1
4
2
3
3
4
1
3
¶n−1 µ
a0
b0
¶
=
µ
1
4
2
3
3
4
1
3
¶n µ
a0
b0
¶
After 50 units of time you would have
µ
a50
b50
¶
=
µ
1
4
2
3
3
4
1
3
¶50 µ
a0
b0
¶
=
µ
. 470 588 235
. 470 588 235
. 529 411 765
. 529 411 765
¶ µ
a0
b0
¶
After 100 units of time, you would have
µ
a100
b100
¶
=
µ
1
4
2
3
3
4
1
3
¶100 µ
a0
b0
¶
=
µ . 470 588 235
. 470 588 235
. 529 411 765
. 529 411 765
¶ µ a0
b0
¶
You can’t detect any diﬀerence between these two answers. In general, if you wanted to know
about how many would be in the two locations, you would need to take a limit. However,
there is a better way.
More generally here is a deﬁnition.
Deﬁnition 12.0.52 Let n locations be denoted by the numbers 1, 2, · · ·, n. Also
suppose it is the case that each year aij denotes the proportion of residents in location j

224
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
which move to location i. Also suppose no one escapes or emigrates from without these n
locations. This last assumption requires P
i aij = 1. Such matrices in which the columns
are nonnegative numbers which sum to one are called Markov matrices. In this context
describing migration, they are also called migration matrices.
Example 12.0.53 Here is an example of one of these matrices.
µ
.4
.2
.6
.8
¶
Thus if it is considered as a migration matrix, .4 is the proportion of residents in location
1 which stay in location one in a given time period while .6 is the proportion of residents in
location 1 which move to location 2 and .2 is the proportion of residents in location 2 which
move to location 1. Considered as a Markov matrix, these numbers are usually identiﬁed
with probabilities.
If v = (x1, · · ·, xn)T where xi is the population of location i at a given instant, you obtain
the population of location i one year later by computing P
j aijxj = (Av)i . Therefore, the
population of location i after k years is
¡
Akv
¢
i . An obvious application of this would be to
a situation in which you rent trailers which can go to various parts of a city and you observe
through experiments the proportion of trailers which go from point i to point j in a single
day. Then you might want to ﬁnd how many trailers would be in all the locations after 8
days.
Proposition 12.0.54 Let A = (aij) be a migration matrix. Then 1 is always an eigen-
value for A.
Proof: Remember that det
¡
BT ¢
= det (B) . Therefore,
det (A −λI) = det
³
(A −λI)T ´
= det
¡
AT −λI
¢
because IT = I. Thus the characteristic equation for A is the same as the characteristic
equation for AT and so A and AT have the same eigenvalues. We will show that 1 is an
eigenvalue for AT and then it will follow that 1 is an eigenvalue for A.
Remember that for a migration matrix, P
i aij = 1. Therefore, if AT = (bij) so bij = aji,
it follows that
X
j
bij =
X
j
aji = 1.
Therefore, from matrix multiplication,
AT



1
...
1


=



P
j bij
...
P
j bij


=



1
...
1



which shows that



1
...
1


is an eigenvector for AT corresponding to the eigenvalue, λ = 1.
As explained above, this shows that λ = 1 is an eigenvalue for A because A and AT have
the same eigenvalues.

225
Example 12.0.55 Consider the migration matrix,


.6
0
.1
.2
.8
0
.2
.2
.9

for locations 1, 2, and
3. Suppose initially there are 100 residents in location 1, 200 in location 2 and 400 in location
4. Find the population in the three locations after 10 units of time.
From the above, it suﬃces to consider


.6
0
.1
.2
.8
0
.2
.2
.9


10 

100
200
400

=


115. 085 829 22
120. 130 672 44
464. 783 498 34


Of course you would need to round these numbers oﬀ.
A related problem asks for how many there will be in the various locations after a long
time. It turns out that if some power of the migration matrix has all positive entries, then
there is a limiting vector, x = limk→∞Akx0 where x0 is the initial vector describing the
number of inhabitants in the various locations initially. This vector will be an eigenvector
for the eigenvalue 1 because
x = lim
k→∞Akx0 = lim
k→∞Ak+1x0 = A lim
k→∞Akx = Ax,
and the sum of its entries will equal the sum of the entries of the initial vector, x0 because
this sum is preserved for every multiplication by A since
X
i
X
j
aijxj =
X
j
xj
ÃX
i
aij
!
=
X
j
xj.
Here is an example. It is the same example as the one above but here it will involve the
long time limit.
Example 12.0.56 Consider the migration matrix,


.6
0
.1
.2
.8
0
.2
.2
.9

for locations 1, 2, and
3. Suppose initially there are 100 residents in location 1, 200 in location 2 and 400 in location
4. Find the population in the three locations after a long time.
You just need to ﬁnd the eigenvector which goes with the eigenvalue 1 and then normalize
it so the sum of its entries equals the sum of the entries of the initial vector. Thus you need
to ﬁnd a solution to




1
0
0
0
1
0
0
0
1

−


.6
0
.1
.2
.8
0
.2
.2
.9






x
y
z

=


0
0
0


The augmented matrix is


. 4
0
−. 1
|
0
−. 2
. 2
0
|
0
−. 2
−. 2
. 1
|
0


and its row reduced echelon form is


1
0
−. 25
0
0
1
−. 25
0
0
0
0
0



226
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore, the eigenvectors are
s


(1/4)
(1/4)
1


and all that remains is to choose the value of s such that
1
4s + 1
4s + s = 100 + 200 + 400
This yields s = 1400
3
and so the long time limit would equal
1400
3


(1/4)
(1/4)
1

=


116. 666 666 666 666 7
116. 666 666 666 666 7
466. 666 666 666 666 7

.
You would of course need to round these numbers oﬀ. You see that you are not far oﬀafter
just 10 units of time. Therefore, you might consider this as a useful procedure because it is
probably easier to solve a simple system of equations than it is to raise a matrix to a large
power.
Example 12.0.57 Suppose a migration matrix is







1
5
1
2
1
5
1
4
1
4
1
2
11
20
1
4
3
10







. Find the comparison
between the populations in the three locations after a long time.
This amounts to nothing more than ﬁnding the eigenvector for λ = 1. Solve









1
0
0
0
1
0
0
0
1

−







1
5
1
2
1
5
1
4
1
4
1
2
11
20
1
4
3
10
















x
y
z

=


0
0
0

.
The augmented matrix is







4
5
−1
2
−1
5
|
0
−1
4
3
4
−1
2
|
0
−11
20
−1
4
7
10
|
0







The row echelon form is



1
0
−16
19
0
0
1
−18
19
0
0
0
0
0



and so an eigenvector is


16
18
19

.

227
Thus there will be 18
16
th more in location 2 than in location 1. There will be 19
18
th more in
location 3 than in location 2.
You see the eigenvalue problem makes these sorts of determinations fairly simple.
There are many other things which can be said about these sorts of migration prob-
lems. They include things like the gambler’s ruin problem which asks for the probability
that a compulsive gambler will eventually lose all his money. However those problems are
not so easy although they still involve eigenvalues and eigenvectors.
12.0.12
Complex Eigenvalues
Sometimes you have to consider eigenvalues which are complex numbers. This occurs in
diﬀerential equations for example. You do these problems exactly the same way as you do
the ones in which the eigenvalues are real. Here is an example.
Example 12.0.58 Find the eigenvalues and eigenvectors of the matrix
A =


1
0
0
0
2
−1
0
1
2

.
You need to ﬁnd the eigenvalues. Solve
det




1
0
0
0
2
−1
0
1
2

−λ


1
0
0
0
1
0
0
0
1



= 0.
This reduces to (λ −1)
¡
λ2 −4λ + 5
¢
= 0. The solutions are λ = 1, λ = 2 + i, λ = 2 −i.
There is nothing new about ﬁnding the eigenvectors for λ = 1 so consider the eigenvalue
λ = 2 + i. You need to solve

(2 + i)


1
0
0
0
1
0
0
0
1

−


1
0
0
0
2
−1
0
1
2






x
y
z

=


0
0
0


In other words, you must consider the augmented matrix,


1 + i
0
0
|
0
0
i
1
|
0
0
−1
i
|
0


for the solution. Divide the top row by (1 + i) and then take −i times the second row and
add to the bottom. This yields


1
0
0
|
0
0
i
1
|
0
0
0
0
|
0


Now multiply the second row by −i to obtain


1
0
0
|
0
0
1
−i
|
0
0
0
0
|
0



228
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore, the eigenvectors are of the form
t


0
i
1

.
You should ﬁnd the eigenvectors for λ = 2 −i. These are
t


0
−i
1

.
As usual, if you want to get it right you had better check it.


1
0
0
0
2
−1
0
1
2




0
−i
1

=


0
−1 −2i
2 −i

= (2 −i)


0
−i
1


so it worked.
12.0.13
The Estimation Of Eigenvalues
There are ways to estimate the eigenvalues for matrices. The most famous is known as
Gerschgorin’s theorem. This theorem gives a rough idea where the eigenvalues are just from
looking at the matrix.
Theorem 12.0.59 Let A be an n × n matrix. Consider the n Gerschgorin discs
deﬁned as
Di ≡


λ ∈C : |λ −aii| ≤
X
j̸=i
|aij|


.
Then every eigenvalue is contained in some Gerschgorin disc.
This theorem says to add up the absolute values of the entries of the ith row which are
oﬀthe main diagonal and form the disc centered at aii having this radius. The union of
these discs contains σ (A) .
Proof: Suppose Ax = λx where x ̸= 0. Then for A = (aij)
X
j̸=i
aijxj = (λ −aii) xi.
Therefore, picking k such that |xk| ≥|xj| for all xj, it follows that |xk| ̸= 0 since |x| ̸= 0
and
|xk|
X
j̸=i
|akj| ≥
X
j̸=i
|akj| |xj| ≥|λ −aii| |xk| .
Now dividing by |xk|, it follows λ is contained in the kth Gerschgorin disc.
Example 12.0.60 Here is a matrix. Estimate its eigenvalues.


2
1
1
3
5
0
0
1
9



12.1.
THE MATHEMATICAL THEORY OF DETERMINANTS∗
229
According to Gerschgorin’s theorem the eigenvalues are contained in the disks
D1
=
{λ ∈C : |λ −2| ≤2} ,
D2
=
{λ ∈C : |λ −5| ≤3} ,
D3
=
{λ ∈C : |λ −9| ≤1}
It is important to observe that these disks are in the complex plane. In general this is the
case. If you want to ﬁnd eigenvalues they will be complex numbers.
x
iy
r
2
r
8
r
9
So what are the values of the eigenvalues? In this case they are real. You can compute
them by graphing the characteristic polynomial, λ3 −16λ2 + 70λ −66 and then zoom-
ing in on the zeros. If you do this you ﬁnd the solution is {λ = 1. 295 3} , {λ = 5. 590 5} ,
{λ = 9. 114 2} . Of course these are only approximations and so this information is useless
for ﬁnding eigenvectors. However, in many applications, it is the size of the eigenvalues
which is important and so these numerical values would be helpful for such applications.
Because of this example, you might think there is no real reason for Gerschgorin’s theorem.
Why not just compute the characteristic equation and graph and zoom? This is ﬁne up to
a point, but what if the matrix was huge? Then it might be hard to ﬁnd the characteristic
polynomial. Remember the diﬃculties in expanding a big matrix along a row or column.
You would need a better way to come up with the characteristic polynomial. Also, what
if the eigenvalue were complex? You don’t see these by following this procedure. However,
Gerschgorin’s theorem will at least estimate them.
There are also more advanced versions of this theorem which depend on the theory of
functions of a complex variable covering the case where the Gerschgorin disks are disjoint.
In this case, you can assert each disk contains an eigenvalue. In fact, if k of the Gerschgorin
disks are disjoint from the other disks then they contain k eigenvalues. To see this proved,
see the linear algebra book on my web page. Don’t bother to look at it if you have not had
a substantial course on complex analysis because it won’t make any sense. Math is not like
comparative literature, history, or humanities. You can’t read the advanced topics until you
have mastered the basic topics even if you are real smart.
12.1
The Mathematical Theory Of Determinants∗
This material is deﬁnitely not for the faint of heart. It is only for people who want

230
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
to see everything proved. It is a fairly complete and unusually elementary treatment of
the subject. There will be some repetition between this section and the earlier section on
determinants. The main purpose is to give all the missing proofs. Two books which give
a good introduction to determinants are Apostol [2] and Rudin [21]. A recent book which
also has a good introduction is Baker [4]. Most linear algebra books do not do an honest
job presenting this topic.
It is easiest to give a diﬀerent deﬁnition of the determinant which is clearly well deﬁned
and then prove the earlier one in terms of Laplace expansion. Let (i1, · · ·, in) be an ordered
list of numbers from {1, · · ·, n} . This means the order is important so (1, 2, 3) and (2, 1, 3)
are diﬀerent.
The following Lemma will be essential in the deﬁnition of the determinant.
Lemma 12.1.1 There exists a unique function, sgnn which maps each list of numbers
from {1, · · ·, n} to one of the three numbers, 0, 1, or −1 which also has the following prop-
erties.
sgnn (1, · · ·, n) = 1
(12.8)
sgnn (i1, · · ·, p, · · ·, q, · · ·, in) = −sgnn (i1, · · ·, q, · · ·, p, · · ·, in)
(12.9)
In words, the second property states that if two of the numbers are switched, the value of the
function is multiplied by −1. Also, in the case where n > 1 and {i1, · · ·, in} = {1, · · ·, n} so
that every number from {1, · · ·, n} appears in the ordered list, (i1, · · ·, in) ,
sgnn (i1, · · ·, iθ−1, n, iθ+1, · · ·, in) ≡
(−1)n−θ sgnn−1 (i1, · · ·, iθ−1, iθ+1, · · ·, in)
(12.10)
where n = iθ in the ordered list, (i1, · · ·, in) .
Proof: To begin with, it is necessary to show the existence of such a function. This is
clearly true if n = 1. Deﬁne sgn1 (1) ≡1 and observe that it works. No switching is possible.
In the case where n = 2, it is also clearly true. Let sgn2 (1, 2) = 1 and sgn2 (2, 1) = 0 while
sgn2 (2, 2) = sgn2 (1, 1) = 0 and verify it works. Assuming such a function exists for n,
sgnn+1 will be deﬁned in terms of sgnn . If there are any repeated numbers in (i1, · · ·, in+1) ,
sgnn+1 (i1, · · ·, in+1) ≡0. If there are no repeats, then n + 1 appears somewhere in the
ordered list. Let θ be the position of the number n + 1 in the list. Thus, the list is of the
form (i1, · · ·, iθ−1, n + 1, iθ+1, · · ·, in+1) . From 12.10 it must be that
sgnn+1 (i1, · · ·, iθ−1, n + 1, iθ+1, · · ·, in+1) ≡
(−1)n+1−θ sgnn (i1, · · ·, iθ−1, iθ+1, · · ·, in+1) .
It is necessary to verify this satisﬁes 12.8 and 12.9 with n replaced with n + 1. The ﬁrst of
these is obviously true because
sgnn+1 (1, · · ·, n, n + 1) ≡(−1)n+1−(n+1) sgnn (1, · · ·, n) = 1.
If there are repeated numbers in (i1, · · ·, in+1) , then it is obvious 12.9 holds because both
sides would equal zero from the above deﬁnition. It remains to verify 12.9 in the case where
there are no numbers repeated in (i1, · · ·, in+1) . Consider
sgnn+1
³
i1, · · ·,
rp, · · ·,
sq, · · ·, in+1
´
,

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTS∗
231
where the r above the p indicates the number, p is in the rth position and the s above the
q indicates that the number, q is in the sth position. Suppose ﬁrst that r < θ < s. Then
sgnn+1
µ
i1, · · ·,
rp, · · ·,
θ
n + 1, · · ·,
sq, · · ·, in+1
¶
≡
(−1)n+1−θ sgnn
³
i1, · · ·,
rp, · · ·,
s−1
q , · · ·, in+1
´
while
sgnn+1
µ
i1, · · ·,
rq, · · ·,
θ
n + 1, · · ·,
sp, · · ·, in+1
¶
=
(−1)n+1−θ sgnn
³
i1, · · ·,
rq, · · ·,
s−1
p , · · ·, in+1
´
and so, by induction, a switch of p and q introduces a minus sign in the result. Similarly, if
θ > s or if θ < r it also follows that 12.9 holds. The interesting case is when θ = r or θ = s.
Consider the case where θ = r and note the other case is entirely similar.
sgnn+1
³
i1, · · ·,
r
n + 1, · · ·,
sq, · · ·, in+1
´
=
(−1)n+1−r sgnn
³
i1, · · ·,
s−1
q , · · ·, in+1
´
(12.11)
while
sgnn+1
³
i1, · · ·,
rq, · · ·,
s
n + 1, · · ·, in+1
´
=
(−1)n+1−s sgnn
³
i1, · · ·,
rq, · · ·, in+1
´
.
(12.12)
By making s −1 −r switches, move the q which is in the s −1th position in 12.11 to the
rth position in 12.12. By induction, each of these switches introduces a factor of −1 and so
sgnn
³
i1, · · ·,
s−1
q , · · ·, in+1
´
= (−1)s−1−r sgnn
³
i1, · · ·,
rq, · · ·, in+1
´
.
Therefore,
sgnn+1
³
i1, · · ·,
r
n + 1, · · ·,
sq, · · ·, in+1
´
= (−1)n+1−r sgnn
³
i1, · · ·,
s−1
q , · · ·, in+1
´
= (−1)n+1−r (−1)s−1−r sgnn
³
i1, · · ·,
rq, · · ·, in+1
´
= (−1)n+s sgnn
³
i1, · · ·,
rq, · · ·, in+1
´
= (−1)2s−1 (−1)n+1−s sgnn
³
i1, · · ·,
rq, · · ·, in+1
´
= −sgnn+1
³
i1, · · ·,
rq, · · ·,
s
n + 1, · · ·, in+1
´
.
This proves the existence of the desired function.
To see this function is unique, note that you can obtain any ordered list of distinct
numbers from a sequence of switches. If there exist two functions, f and g both satisfying
12.8 and 12.9, you could start with f (1, · · ·, n) = g (1, · · ·, n) and applying the same sequence
of switches, eventually arrive at f (i1, · · ·, in) = g (i1, · · ·, in) . If any numbers are repeated,
then 12.9 gives both functions are equal to zero for that ordered list. This proves the lemma.
In what follows sgn will often be used rather than sgnn because the context supplies the
appropriate n.

232
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Deﬁnition 12.1.2 Let f be a real valued function which has the set of ordered lists
of numbers from {1, · · ·, n} as its domain. Deﬁne
X
(k1,···,kn)
f (k1 · · · kn)
to be the sum of all the f (k1 · · · kn) for all possible choices of ordered lists (k1, · · ·, kn) of
numbers of {1, · · ·, n} . For example,
X
(k1,k2)
f (k1, k2) = f (1, 2) + f (2, 1) + f (1, 1) + f (2, 2) .
Deﬁnition 12.1.3 Let (aij) = A denote an n × n matrix. The determinant of A,
denoted by det (A) is deﬁned by
det (A) ≡
X
(k1,···,kn)
sgn (k1, · · ·, kn) a1k1 · · · ankn
where the sum is taken over all ordered lists of numbers from {1, · · ·, n}. Note it suﬃces to
take the sum over only those ordered lists in which there are no repeats because if there are,
sgn (k1, · · ·, kn) = 0 and so that term contributes 0 to the sum.
Let A be an n × n matrix, A = (aij) and let (r1, · · ·, rn) denote an ordered list of n
numbers from {1, · · ·, n}. Let A (r1, · · ·, rn) denote the matrix whose kth row is the rk row
of the matrix, A. Thus
det (A (r1, · · ·, rn)) =
X
(k1,···,kn)
sgn (k1, · · ·, kn) ar1k1 · · · arnkn
(12.13)
and
A (1, · · ·, n) = A.
Proposition 12.1.4 Let
(r1, · · ·, rn)
be an ordered list of numbers from {1, · · ·, n}. Then
sgn (r1, · · ·, rn) det (A)
=
X
(k1,···,kn)
sgn (k1, · · ·, kn) ar1k1 · · · arnkn
(12.14)
=
det (A (r1, · · ·, rn)) .
(12.15)
Proof: Let (1, · · ·, n) = (1, · · ·, r, · · ·s, · · ·, n) so r < s.
det (A (1, · · ·, r, · · ·, s, · · ·, n)) =
(12.16)
X
(k1,···,kn)
sgn (k1, · · ·, kr, · · ·, ks, · · ·, kn) a1k1 · · · arkr · · · asks · · · ankn,
and renaming the variables, calling ks, kr and kr, ks, this equals
=
X
(k1,···,kn)
sgn (k1, · · ·, ks, · · ·, kr, · · ·, kn) a1k1 · · · arks · · · askr · · · ankn

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTS∗
233
=
X
(k1,···,kn)
−sgn

k1, · · ·,
These got switched
z
}|
{
kr, · · ·, ks
, · · ·, kn

a1k1 · · · askr · · · arks · · · ankn
= −det (A (1, · · ·, s, · · ·, r, · · ·, n)) .
(12.17)
Consequently,
det (A (1, · · ·, s, · · ·, r, · · ·, n)) =
−det (A (1, · · ·, r, · · ·, s, · · ·, n)) = −det (A)
Now letting A (1, · · ·, s, · · ·, r, · · ·, n) play the role of A, and continuing in this way, switching
pairs of numbers,
det (A (r1, · · ·, rn)) = (−1)p det (A)
where it took p switches to obtain(r1, · · ·, rn) from (1, · · ·, n). By Lemma 12.1.1, this implies
det (A (r1, · · ·, rn)) = (−1)p det (A) = sgn (r1, · · ·, rn) det (A)
and proves the proposition in the case when there are no repeated numbers in the ordered
list, (r1, · · ·, rn). However, if there is a repeat, say the rth row equals the sth row, then the
reasoning of 12.16 -12.17 shows that A (r1, · · ·, rn) = 0 and also sgn (r1, · · ·, rn) = 0 so the
formula holds in this case also.
Observation 12.1.5 There are n! ordered lists of distinct numbers from {1, · · ·, n} .
To see this, consider n slots placed in order. There are n choices for the ﬁrst slot. For
each of these choices, there are n −1 choices for the second. Thus there are n (n −1) ways
to ﬁll the ﬁrst two slots. Then for each of these ways there are n−2 choices left for the third
slot. Continuing this way, there are n! ordered lists of distinct numbers from {1, · · ·, n} as
stated in the observation.
With the above, it is possible to give a more symmetric description of the determinant
from which it will follow that det (A) = det
¡
AT ¢
.
Corollary 12.1.6 The following formula for det (A) is valid.
det (A) = 1
n!·
X
(r1,···,rn)
X
(k1,···,kn)
sgn (r1, · · ·, rn) sgn (k1, · · ·, kn) ar1k1 · · · arnkn.
(12.18)
And also det
¡
AT ¢
= det (A) where AT is the transpose of A. (Recall that for AT =
¡
aT
ij
¢
,
aT
ij = aji.)
Proof: From Proposition 12.1.4, if the ri are distinct,
det (A) =
X
(k1,···,kn)
sgn (r1, · · ·, rn) sgn (k1, · · ·, kn) ar1k1 · · · arnkn.
Summing over all ordered lists, (r1, · · ·, rn) where the ri are distinct, (If the ri are not
distinct, sgn (r1, · · ·, rn) = 0 and so there is no contribution to the sum.)
n! det (A) =
X
(r1,···,rn)
X
(k1,···,kn)
sgn (r1, · · ·, rn) sgn (k1, · · ·, kn) ar1k1 · · · arnkn.
This proves the corollary since the formula gives the same number for A as it does for AT .

234
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Corollary 12.1.7 If two rows or two columns in an n × n matrix, A, are switched,
the determinant of the resulting matrix equals (−1) times the determinant of the original
matrix. If A is an n × n matrix in which two rows are equal or two columns are equal then
det (A) = 0. Suppose the ith row of A equals (xa1 + yb1, · · ·, xan + ybn). Then
det (A) = x det (A1) + y det (A2)
where the ith row of A1 is (a1, · · ·, an) and the ith row of A2 is (b1, · · ·, bn) , all other rows
of A1 and A2 coinciding with those of A. In other words, det is a linear function of each
row A. The same is true with the word “row” replaced with the word “column”.
Proof:
By Proposition 12.1.4 when two rows are switched, the determinant of the
resulting matrix is (−1) times the determinant of the original matrix. By Corollary 12.1.6 the
same holds for columns because the columns of the matrix equal the rows of the transposed
matrix. Thus if A1 is the matrix obtained from A by switching two columns,
det (A) = det
¡
AT ¢
= −det
¡
AT
1
¢
= −det (A1) .
If A has two equal columns or two equal rows, then switching them results in the same
matrix. Therefore, det (A) = −det (A) and so det (A) = 0.
It remains to verify the last assertion.
det (A) ≡
X
(k1,···,kn)
sgn (k1, · · ·, kn) a1k1 · · · (xaki + ybki) · · · ankn
= x
X
(k1,···,kn)
sgn (k1, · · ·, kn) a1k1 · · · aki · · · ankn
+y
X
(k1,···,kn)
sgn (k1, · · ·, kn) a1k1 · · · bki · · · ankn
≡x det (A1) + y det (A2) .
The same is true of columns because det
¡
AT ¢
= det (A) and the rows of AT are the columns
of A.
Deﬁnition 12.1.8 A vector, w, is a linear combination of the vectors {v1, · · ·, vr}
if there exists scalars, c1, · · ·cr such that w = Pr
k=1 ckvk. This is the same as saying w ∈
span {v1, · · ·, vr} .
The following corollary is also of great use.
Corollary 12.1.9 Suppose A is an n × n matrix and some column (row) is a linear
combination of r other columns (rows). Then det (A) = 0.
Proof: Let A =
¡
a1
· · ·
an
¢
be the columns of A and suppose the condition that
one column is a linear combination of r of the others is satisﬁed. Then by using Corollary
12.1.7 you may rearrange the columns to have the nth column a linear combination of the
ﬁrst r columns. Thus an = Pr
k=1 ckak and so
det (A) = det
¡
a1
· · ·
ar
· · ·
an−1
Pr
k=1 ckak
¢
.
By Corollary 12.1.7
det (A) =
r
X
k=1
ck det
¡
a1
· · ·
ar
· · ·
an−1
ak
¢
= 0.
The case for rows follows from the fact that det (A) = det
¡
AT ¢
. This proves the corollary.
Recall the following deﬁnition of matrix multiplication.

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTS∗
235
Deﬁnition 12.1.10 If A and B are n × n matrices, A = (aij) and B = (bij),
AB = (cij) where
cij ≡
n
X
k=1
aikbkj.
One of the most important rules about determinants is that the determinant of a product
equals the product of the determinants.
Theorem 12.1.11 Let A and B be n × n matrices. Then
det (AB) = det (A) det (B) .
Proof: Let cij be the ijth entry of AB. Then by Proposition 12.1.4,
det (AB) =
X
(k1,···,kn)
sgn (k1, · · ·, kn) c1k1 · · · cnkn
=
X
(k1,···,kn)
sgn (k1, · · ·, kn)
ÃX
r1
a1r1br1k1
!
· · ·
ÃX
rn
anrnbrnkn
!
=
X
(r1···,rn)
X
(k1,···,kn)
sgn (k1, · · ·, kn) br1k1 · · · brnkn (a1r1 · · · anrn)
=
X
(r1···,rn)
sgn (r1 · · · rn) a1r1 · · · anrn det (B) = det (A) det (B) .
This proves the theorem.
Lemma 12.1.12 Suppose a matrix is of the form
M =
µ
A
∗
0
a
¶
(12.19)
or
M =
µ
A
0
∗
a
¶
(12.20)
where a is a number and A is an (n −1) × (n −1) matrix and ∗denotes either a column
or a row having length n −1 and the 0 denotes either a column or a row of length n −1
consisting entirely of zeros. Then
det (M) = a det (A) .
Proof: Denote M by (mij) . Thus in the ﬁrst case, mnn = a and mni = 0 if i ̸= n while
in the second case, mnn = a and min = 0 if i ̸= n. From the deﬁnition of the determinant,
det (M) ≡
X
(k1,···,kn)
sgnn (k1, · · ·, kn) m1k1 · · · mnkn
Letting θ denote the position of n in the ordered list, (k1, · · ·, kn) then using the earlier
conventions used to prove Lemma 12.1.1, det (M) equals
X
(k1,···,kn)
(−1)n−θ sgnn−1
µ
k1, · · ·, kθ−1,
θ
kθ+1, · · ·,
n−1
kn
¶
m1k1 · · · mnkn

236
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Now suppose 12.20. Then if kn ̸= n, the term involving mnkn in the above expression equals
zero. Therefore, the only terms which survive are those for which θ = n or in other words,
those for which kn = n. Therefore, the above expression reduces to
a
X
(k1,···,kn−1)
sgnn−1 (k1, · · ·kn−1) m1k1 · · · m(n−1)kn−1 = a det (A) .
To get the assertion in the situation of 12.19 use Corollary 12.1.6 and 12.20 to write
det (M) = det
¡
M T ¢
= det
µµ
AT
0
∗
a
¶¶
= a det
¡
AT ¢
= a det (A) .
This proves the lemma.
In terms of the theory of determinants, arguably the most important idea is that of
Laplace expansion along a row or a column. This will follow from the above deﬁnition of a
determinant.
Deﬁnition 12.1.13 Let A = (aij) be an n × n matrix. Then a new matrix called
the cofactor matrix, cof (A) is deﬁned by cof (A) = (cij) where to obtain cij delete the ith
row and the jth column of A, take the determinant of the (n −1) × (n −1) matrix which
results, (This is called the ijth minor of A. ) and then multiply this number by (−1)i+j. To
make the formulas easier to remember, cof (A)ij will denote the ijth entry of the cofactor
matrix.
The following is the main result. Earlier this was given as a deﬁnition and the outrageous
totally unjustiﬁed assertion was made that the same number would be obtained by expanding
the determinant along any row or column. The following theorem proves this assertion.
Theorem 12.1.14 Let A be an n × n matrix where n ≥2. Then
det (A) =
n
X
j=1
aij cof (A)ij =
n
X
i=1
aij cof (A)ij .
(12.21)
The ﬁrst formula consists of expanding the determinant along the ith row and the second
expands the determinant along the jth column.
Proof: Let (ai1, · · ·, ain) be the ith row of A. Let Bj be the matrix obtained from A
by leaving every row the same except the ith row which in Bj equals (0, · · ·, 0, aij, 0, · · ·, 0) .
Then by Corollary 12.1.7,
det (A) =
n
X
j=1
det (Bj)
Denote by Aij the (n −1) × (n −1) matrix obtained by deleting the ith row and the jth
column of A. Thus cof (A)ij ≡(−1)i+j det
¡
Aij¢
. At this point, recall that from Proposition
12.1.4, when two rows or two columns in a matrix, M, are switched, this results in multi-
plying the determinant of the old matrix by −1 to get the determinant of the new matrix.
Therefore, by Lemma 12.1.12,
det (Bj)
=
(−1)n−j (−1)n−i det
µµ
Aij
∗
0
aij
¶¶
=
(−1)i+j det
µµ
Aij
∗
0
aij
¶¶
= aij cof (A)ij .

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTS∗
237
Therefore,
det (A) =
n
X
j=1
aij cof (A)ij
which is the formula for expanding det (A) along the ith row. Also,
det (A)
=
det
¡
AT ¢
=
n
X
j=1
aT
ij cof
¡
AT ¢
ij
=
n
X
j=1
aji cof (A)ji
which is the formula for expanding det (A) along the ith column. This proves the theorem.
Note that this gives an easy way to write a formula for the inverse of an n × n matrix.
Theorem 12.1.15 A−1 exists if and only if det(A) ̸= 0. If det(A) ̸= 0, then
A−1 =
¡
a−1
ij
¢
where
a−1
ij = det(A)−1 cof (A)ji
for cof (A)ij the ijth cofactor of A.
Proof: By Theorem 12.1.14 and letting (air) = A, if det (A) ̸= 0,
n
X
i=1
air cof (A)ir det(A)−1 = det(A) det(A)−1 = 1.
Now consider
n
X
i=1
air cof (A)ik det(A)−1
when k ̸= r. Replace the kth column with the rth column to obtain a matrix, Bk whose
determinant equals zero by Corollary 12.1.7. However, expanding this matrix along the kth
column yields
0 = det (Bk) det (A)−1 =
n
X
i=1
air cof (A)ik det (A)−1
Summarizing,
n
X
i=1
air cof (A)ik det (A)−1 = δrk.
Using the other formula in Theorem 12.1.14, and similar reasoning,
n
X
j=1
arj cof (A)kj det (A)−1 = δrk
This proves that if det (A) ̸= 0, then A−1 exists with A−1 =
¡
a−1
ij
¢
, where
a−1
ij = cof (A)ji det (A)−1 .
Now suppose A−1 exists. Then by Theorem 12.1.11,
1 = det (I) = det
¡
AA−1¢
= det (A) det
¡
A−1¢
so det (A) ̸= 0. This proves the theorem.
The next corollary points out that if an n × n matrix, A has a right or a left inverse,
then it has an inverse.

238
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Corollary 12.1.16 Let A be an n × n matrix and suppose there exists an n × n matrix,
B such that BA = I. Then A−1 exists and A−1 = B. Also, if there exists C an n×n matrix
such that AC = I, then A−1 exists and A−1 = C.
Proof: Since BA = I, Theorem 12.1.11 implies
det B det A = 1
and so det A ̸= 0. Therefore from Theorem 12.1.15, A−1 exists. Therefore,
A−1 = (BA) A−1 = B
¡
AA−1¢
= BI = B.
The case where CA = I is handled similarly.
The conclusion of this corollary is that left inverses, right inverses and inverses are all
the same in the context of n × n matrices.
Theorem 12.1.15 says that to ﬁnd the inverse, take the transpose of the cofactor matrix
and divide by the determinant. The transpose of the cofactor matrix is called the adjugate
or sometimes the classical adjoint of the matrix A. It is an abomination to call it the adjoint
although you do sometimes see it referred to in this way. In words, A−1 is equal to one over
the determinant of A times the adjugate matrix of A.
In case you are solving a system of equations, Ax = y for x, it follows that if A−1 exists,
x =
¡
A−1A
¢
x = A−1 (Ax) = A−1y
thus solving the system. Now in the case that A−1 exists, there is a formula for A−1 given
above. Using this formula,
xi =
n
X
j=1
a−1
ij yj =
n
X
j=1
1
det (A) cof (A)ji yj.
By the formula for the expansion of a determinant along a column,
xi =
1
det (A) det



∗
· · ·
y1
· · ·
∗
...
...
...
∗
· · ·
yn
· · ·
∗


,
where here the ith column of A is replaced with the column vector, (y1 · · · ·, yn)T , and the
determinant of this modiﬁed matrix is taken and divided by det (A). This formula is known
as Cramer’s rule.
Deﬁnition 12.1.17 A matrix M, is upper triangular if Mij = 0 whenever i > j.
Thus such a matrix equals zero below the main diagonal, the entries of the form Mii as
shown.






∗
∗
· · ·
∗
0
∗
...
...
...
...
...
∗
0
· · ·
0
∗






A lower triangular matrix is deﬁned similarly as a matrix for which all entries above the
main diagonal are equal to zero.
With this deﬁnition, here is a simple corollary of Theorem 12.1.14.

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTS∗
239
Corollary 12.1.18 Let M be an upper (lower) triangular matrix.
Then det (M) is
obtained by taking the product of the entries on the main diagonal.
Deﬁnition 12.1.19 A submatrix of a matrix A is the rectangular array of numbers
obtained by deleting some rows and columns of A. Let A be an m × n matrix. The deter-
minant rank of the matrix equals r where r is the largest number such that some r × r
submatrix of A has a non zero determinant. The row rank is deﬁned to be the dimension
of the span of the rows. The column rank is deﬁned to be the dimension of the span of the
columns.
Theorem 12.1.20 If A has determinant rank, r, then there exist r rows of the
matrix such that every other row is a linear combination of these r rows.
Proof: Suppose the determinant rank of A = (aij) equals r. If rows and columns are
interchanged, the determinant rank of the modiﬁed matrix is unchanged. Thus rows and
columns can be interchanged to produce an r × r matrix in the upper left corner of the
matrix which has non zero determinant. Now consider the r + 1 × r + 1 matrix, M,





a11
· · ·
a1r
a1p
...
...
...
ar1
· · ·
arr
arp
al1
· · ·
alr
alp





where C will denote the r×r matrix in the upper left corner which has non zero determinant.
I claim det (M) = 0.
There are two cases to consider in verifying this claim. First, suppose p > r. Then the
claim follows from the assumption that A has determinant rank r. On the other hand, if
p < r, then the determinant is zero because there are two identical columns. Expand the
determinant along the last column and divide by det (C) to obtain
alp = −
r
X
i=1
cof (M)ip
det (C) aip.
Now note that cof (M)ip does not depend on p. Therefore the above sum is of the form
alp =
r
X
i=1
miaip
which shows the lth row is a linear combination of the ﬁrst r rows of A. Since l is arbitrary,
this proves the theorem.
Corollary 12.1.21 The determinant rank equals the row rank.
Proof: From Theorem 12.1.20, the row rank is no larger than the determinant rank.
Could the row rank be smaller than the determinant rank? If so, there exist p rows for
p < r such that the span of these p rows equals the row space. But this implies that the
r × r submatrix whose determinant is nonzero also has row rank no larger than p which is
impossible if its determinant is to be nonzero because at least one row is a linear combination
of the others.
Corollary 12.1.22 If A has determinant rank, r, then there exist r columns of the
matrix such that every other column is a linear combination of these r columns. Also the
column rank equals the determinant rank.

240
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Proof: This follows from the above by considering AT . The rows of AT are the columns
of A and the determinant rank of AT and A are the same. Therefore, from Corollary 12.1.21,
column rank of A = row rank of AT = determinant rank of AT = determinant rank of A.
The following theorem is of fundamental importance and ties together many of the ideas
presented above.
Theorem 12.1.23 Let A be an n × n matrix. Then the following are equivalent.
1. det (A) = 0.
2. A, AT are not one to one.
3. A is not onto.
Proof: Suppose det (A) = 0. Then the determinant rank of A = r < n. Therefore,
there exist r columns such that every other column is a linear combination of these columns
by Theorem 12.1.20. In particular, it follows that for some m, the mth column is a linear
combination of all the others.
Thus letting A =
¡
a1
· · ·
am
· · ·
an
¢
where the
columns are denoted by ai, there exists scalars, αi such that
am =
X
k̸=m
αkak.
Now consider the column vector, x ≡
¡
α1
· · ·
−1
· · ·
αn
¢T . Then
Ax = −am +
X
k̸=m
αkak = 0.
Since also A0 = 0, it follows A is not one to one. Similarly, AT is not one to one by the
same argument applied to AT . This veriﬁes that 1.) implies 2.).
Now suppose 2.). Then since AT is not one to one, it follows there exists x ̸= 0 such that
AT x = 0.
Taking the transpose of both sides yields
xT A = 0
where the 0 is a 1 × n matrix or row vector. Now if Ay = x, then
|x|2 = xT (Ay) =
¡
xT A
¢
y = 0y = 0
contrary to x ̸= 0. Consequently there can be no y such that Ay = x and so A is not onto.
This shows that 2.) implies 3.).
Finally, suppose 3.). If 1.) does not hold, then det (A) ̸= 0 but then from Theorem
12.1.15 A−1 exists and so for every y ∈Fn there exists a unique x ∈Fn such that Ax = y.
In fact x = A−1y. Thus A would be onto contrary to 3.). This shows 3.) implies 1.) and
proves the theorem.
Corollary 12.1.24 Let A be an n × n matrix. Then the following are equivalent.
1. det(A) ̸= 0.
2. A and AT are one to one.
3. A is onto.
Proof: This follows immediately from the above theorem.

12.2.
THE CAYLEY HAMILTON THEOREM∗
241
12.1.1
Exercises
1. Let m < n and let A be an m × n matrix. Show that A is not one to one. Hint:
Consider the n × n matrix, A1 which is of the form
A1 ≡
µ A
0
¶
where the 0 denotes an (n −m) × n matrix of zeros. Thus det A1 = 0 and so A1 is
not one to one. Now observe that A1x is the vector,
A1x =
µ Ax
0
¶
which equals zero if and only if Ax = 0.
12.2
The Cayley Hamilton Theorem∗
Deﬁnition 12.2.1 Let A be an n × n matrix.
The characteristic polynomial is
deﬁned as
pA (t) ≡det (tI −A)
and the solutions to pA (t) = 0 are called eigenvalues. For A a matrix and p (t) = tn +
an−1tn−1 + · · · + a1t + a0, denote by p (A) the matrix deﬁned by
p (A) ≡An + an−1An−1 + · · · + a1A + a0I.
The explanation for the last term is that A0 is interpreted as I, the identity matrix.
The Cayley Hamilton theorem states that every matrix satisﬁes its characteristic equa-
tion, that equation deﬁned by PA (t) = 0. It is one of the most important theorems in linear
algebra. The following lemma will help with its proof.
Lemma 12.2.2 Suppose for all |λ| large enough,
A0 + A1λ + · · · + Amλm = 0,
where the Ai are n × n matrices. Then each Ai = 0.
Proof: Multiply by λ−m to obtain
A0λ−m + A1λ−m+1 + · · · + Am−1λ−1 + Am = 0.
Now let |λ| →∞to obtain Am = 0. With this, multiply by λ to obtain
A0λ−m+1 + A1λ−m+2 + · · · + Am−1 = 0.
Now let |λ| →∞to obtain Am−1 = 0. Continue multiplying by λ and letting λ →∞to
obtain that all the Ai = 0. This proves the lemma.
With the lemma, here is a simple corollary.
Corollary 12.2.3 Let Ai and Bi be n × n matrices and suppose
A0 + A1λ + · · · + Amλm = B0 + B1λ + · · · + Bmλm
for all |λ| large enough. Then Ai = Bi for all i. Consequently if λ is replaced by any n × n
matrix, the two sides will be equal. That is, for C any n × n matrix,
A0 + A1C + · · · + AmCm = B0 + B1C + · · · + BmCm.

242
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Proof: Subtract and use the result of the lemma.
With this preparation, here is a relatively easy proof of the Cayley Hamilton theorem.
Theorem 12.2.4 Let A be an n × n matrix and let p (λ) ≡det (λI −A) be the
characteristic polynomial. Then p (A) = 0.
Proof: Let C (λ) equal the transpose of the cofactor matrix of (λI −A) for |λ| large.
(If |λ| is large enough, then λ cannot be in the ﬁnite list of eigenvalues of A and so for such
λ, (λI −A)−1 exists.) Therefore, by Theorem 12.1.15
C (λ) = p (λ) (λI −A)−1 .
Note that each entry in C (λ) is a polynomial in λ having degree no more than n −1.
Therefore, collecting the terms,
C (λ) = C0 + C1λ + · · · + Cn−1λn−1
for Cj some n × n matrix. It follows that for all |λ| large enough,
(A −λI)
¡
C0 + C1λ + · · · + Cn−1λn−1¢
= p (λ) I
and so Corollary 12.2.3 may be used. It follows the matrix coeﬃcients corresponding to
equal powers of λ are equal on both sides of this equation. Therefore, if λ is replaced with
A, the two sides will be equal. Thus
0 = (A −A)
¡
C0 + C1A + · · · + Cn−1An−1¢
= p (A) I = p (A) .
This proves the Cayley Hamilton theorem.
12.2.1
Exercises With Answers
1. Find the following determinant by expanding along the second column.
¯¯¯¯¯¯
1
3
1
2
1
5
2
1
1
¯¯¯¯¯¯
This is
3 (−1)2+1
¯¯¯¯
2
5
2
1
¯¯¯¯ + 1 (−1)1+1
¯¯¯¯
1
1
2
1
¯¯¯¯ + 1 (−1)3+2
¯¯¯¯
1
1
2
5
¯¯¯¯ = 20.
2. Compute the determinant by cofactor expansion. Pick the easiest row or column to
use.
¯¯¯¯¯¯¯¯
2
0
0
1
2
1
1
0
0
0
0
3
2
3
3
1
¯¯¯¯¯¯¯¯
You ought to use the third row. This yields
3
¯¯¯¯¯¯
2
0
0
2
1
1
2
3
3
¯¯¯¯¯¯
= (3) (2)
¯¯¯¯
1
1
3
3
¯¯¯¯ = 0.

12.2.
THE CAYLEY HAMILTON THEOREM∗
243
3. Find the determinant using row and column operations.
¯¯¯¯¯¯¯¯
5
4
3
2
3
2
4
3
−1
2
3
3
2
1
2
−2
¯¯¯¯¯¯¯¯
Replace the ﬁrst row by 5 times the third added to it and then replace the second by
3 times the third added to it and then the last by 2 times the third added to it. This
yields
¯¯¯¯¯¯¯¯
0
14
18
17
0
8
13
12
−1
2
3
3
0
5
8
4
¯¯¯¯¯¯¯¯
Now lets replace the third column by −1 times the last column added to it.
¯¯¯¯¯¯¯¯
0
14
1
17
0
8
1
12
−1
2
0
3
0
5
4
4
¯¯¯¯¯¯¯¯
Now replace the top row by −1 times the second added to it and the bottom row by
−4 times the second added to it. This yields
¯¯¯¯¯¯¯¯
0
6
0
5
0
8
1
12
−1
2
0
3
0
−27
0
−44
¯¯¯¯¯¯¯¯
.
(12.22)
This looks pretty good because it has a lot of zeros. Expand along the ﬁrst column
and next along the second,
(−1)
¯¯¯¯¯¯
6
0
5
8
1
12
−27
0
−44
¯¯¯¯¯¯
= (−1) (1)
¯¯¯¯
6
5
−27
−44
¯¯¯¯ = 129.
Alternatively, you could continue doing row and column operations. Switch the third
and ﬁrst row in 12.22 to obtain
−
¯¯¯¯¯¯¯¯
−1
2
0
3
0
8
1
12
0
6
0
5
0
−27
0
−44
¯¯¯¯¯¯¯¯
Next take 9/2 times the third row and add to the bottom.
−
¯¯¯¯¯¯¯¯
−1
2
0
3
0
8
1
12
0
6
0
5
0
0
0
−44 + (9/2) 5
¯¯¯¯¯¯¯¯
.
Finally, take −6/8 times the second row and add to the third.
−
¯¯¯¯¯¯¯¯
−1
2
0
3
0
8
1
12
0
0
−6/8
5 + (−6/8) (12)
0
0
0
−44 + (9/2) 5
¯¯¯¯¯¯¯¯
.

244
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore, since the matrix is now upper triangular, the determinant is
−((−1) (8) (−6/8) (−44 + (9/2) 5)) = 129.
4. An operation is done to get from the ﬁrst matrix to the second. Identify what was
done and tell how it will aﬀect the value of the determinant.
µ
a
b
c
d
¶
,
µ
a
c
b
d
¶
This involved taking the transpose so the determinant of the new matrix is the same
as the determinant of the ﬁrst matrix.
5. Show that for A a 2 × 2 matrix det (aA) = a2 det (A) where a is a scalar.
a2 det (A) = a det (A1) where the ﬁrst row of A is replaced by a times it to get A1.
Then a det (A1) = A2 where A2 is obtained from A by multiplying both rows by a. In
other words, A2 = aA. Thus the conclusion is established.
6. Use Cramer’s rule to ﬁnd y in
2x + 2y + z = 3
2x −y −z = 2
x + 2z = 1
From Cramer’s rule,
y =
¯¯¯¯¯¯
2
3
1
2
2
−1
1
1
2
¯¯¯¯¯¯
¯¯¯¯¯¯
2
2
1
2
−1
−1
1
0
2
¯¯¯¯¯¯
= 5
13.
7. Here is a matrix,


et
e−t cos t
e−t sin t
et
−e−t cos t −e−t sin t
−e−t sin t + e−t cos t
et
2e−t sin t
−2e−t cos t


Does there exist a value of t for which this matrix fails to have an inverse? Explain.
det


et
e−t cos t
e−t sin t
et
−e−t cos t −e−t sin t
−e−t sin t + e−t cos t
et
2e−t sin t
−2e−t cos t

= 5ete2(−t) cos2 t+5ete2(−t) sin2 t
= 5e−t which is never equal to zero for any value of t and so there is no value of t for
which the matrix has no inverse.
8. Use the formula for the inverse in terms of the cofactor matrix to ﬁnd if possible the
inverse of the matrix


1
2
3
0
6
1
4
1
1

.
First you need to take the determinant
det


1
2
3
0
6
1
4
1
1

= −59

12.2.
THE CAYLEY HAMILTON THEOREM∗
245
and so the matrix has an inverse. Now you need to ﬁnd the cofactor matrix.








¯¯¯¯
6
1
1
1
¯¯¯¯
−
¯¯¯¯
0
1
4
1
¯¯¯¯
¯¯¯¯
0
6
4
1
¯¯¯¯
−
¯¯¯¯
2
3
1
1
¯¯¯¯
¯¯¯¯
1
3
4
1
¯¯¯¯
−
¯¯¯¯
1
2
4
1
¯¯¯¯
¯¯¯¯
2
3
6
1
¯¯¯¯
−
¯¯¯¯
1
3
0
1
¯¯¯¯
¯¯¯¯
1
2
0
6
¯¯¯¯








=


5
4
−24
1
−11
7
−16
−1
6

.
Thus the inverse is
1
−59


5
4
−24
1
−11
7
−16
−1
6


T
=
1
−59


5
1
−16
4
−11
−1
−24
7
6

.
If you check this, it does work.
9. Find the eigenvectors and eigenvalues of the matrix, A =


8
−3
1
−2
7
1
0
0
10

. Deter-
mine whether the matrix is defective. If nondefective, diagonalize the matrix with an
appropriate similarity transformation.
First you need to write the characteristic equation.
det

λ


1
0
0
0
1
0
0
0
1

−


8
−3
1
−2
7
1
0
0
10



= det


λ −8
3
−1
2
λ −7
−1
0
0
λ −10


(12.23)
= λ3 −25λ2 + 200λ −500 = 0
Next you need to ﬁnd the solutions to this equation. Of course this is a real joy. If
there are any rational zeros they are
±factor of 500
factor of 1
I hope to ﬁnd a rational zero. If there are none, then I don’t know what to do at this
point. This is a really lousy method for ﬁnding eigenvalues and eigenvectors. It only
works if things work out well. Lets try 10. You can plug it in and see if it works or
you can use synthetic division.
0
10
1
−25
200
−500
10
−150
500
1
−15
50
0
Yes, it appears 10 works and you can factor the polynomial as (λ −10)
¡
λ2 −15λ + 50
¢
which factors further to (λ −10) (λ −5) (λ −10) so you ﬁnd the eigenvalues are 5, 10,

246
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
and 10. It remains to ﬁnd the eigenvectors. First ﬁnd an eigenvector for λ = 5. To do
this, you ﬁnd a vector which is sent to 0 by the matrix on the right in 12.23 in which
you let λ = 5. Thus the augmented matrix of the system of equations you need to
solve to get the eigenvector is


5 −8
3
−1
|
0
2
5 −7
−1
|
0
0
0
5 −10
|
0


Now the row reduced echelon form is


1
−1
0
|
0
0
0
1
|
0
0
0
0
|
0


and so you need x = y and z = 0. An eigenvector is (1, 1, 0)T . Now you have the
glorious opportunity to solve for the eigenvectors associated with λ = 10. You do it
the same way. The augmented matrix for the system of equations you solve to ﬁnd
the eigenvectors is


10 −8
3
−1
|
0
2
10 −7
−1
|
0
0
0
10 −10
|
0


The row reduced echelon form is


1
3
2
−1
2
|
0
0
0
0
|
0
0
0
0
|
0


and so you need x = −3
2y + 1
2z. It follows the eigenvectors for λ = 10 are
µ
−3
2y + 1
2z, y, z
¶T
where x, y ∈R, not both equal to zero. Why? Let y = 2 and z = 0. This gives the
vector,
(−3, 2, 0)T
as one of the eigenvectors. You could also let y = 0 and z = 2 to obtain another
eigenvector,
(1, 0, 2)T .
If there exists a basis of eigenvectors, then the matrix is nondefective and as discussed
above, the matrix can be diagonalized by considering S−1AS where the columns of S
are the eigenvectors. In this case, I have found three eigenvectors and so it remains
to determine whether these form a basis. Remember how to do this. You let them be
the columns of a matrix and then ﬁnd the rank of this matrix. If it is three, then they
are a basis because they are linearly independent and the vectors are in R3. This is
equivalent to the following matrix has an inverse.


1
−3
1
1
2
0
0
0
2




1
−3
1
1
2
0
0
0
2


−1
=


2
5
3
5
−1
5
−1
5
1
5
1
10
0
0
1
2



12.2.
THE CAYLEY HAMILTON THEOREM∗
247
Then to diagonalize


2
5
3
5
−1
5
−1
5
1
5
1
10
0
0
1
2




8
−3
1
−2
7
1
0
0
10




1
−3
1
1
2
0
0
0
2

=


5
0
0
0
10
0
0
0
10


Isn’t this stuﬀmarvelous! You can know this matrix is nondefective at the point when
you ﬁnd the eigenvectors for the repeated eigenvalue. This eigenvalue was repeated
with multiplicity 2 and there were two parameters, y and z in the description of
the eigenvectors. Therefore, the matrix is nondefective. Also note that there is no
uniqueness for the similarity transformation.
10. Now consider the matrix,


2
1
0
0
1
0
−1
0
1

. Find its eigenvectors and eigenvalues and
determine whether it is defective.
The characteristic equation is
det

λ


1
0
0
0
1
0
0
0
1

−


2
1
0
0
1
0
−1
0
1



= 0
thus the characteristic equation is
(λ −2) (λ −1)2 = 0.
The zeros are 1, 1, 2. Lets ﬁnd the eigenvectors for λ = 1. The augmented matrix for
the system you need to solve is


−1
−1
0
|
0
0
0
0
|
0
1
0
0
|
0


The row reduced echelon form is


1
0
0
|
0
0
1
0
|
0
0
0
0
|
0


Then you ﬁnd x = y = 0 and there is no restriction on z. Thus the eigenvectors are of
the form
(0, 0, z)T , z ∈R.
The eigenvalue had multiplicity 2 but the eigenvectors depend on only one parameter.
Therefore, the matrix is defective and cannot be diagonalized. The other eigenvector
comes from row reducing the following
2


1
0
0
|
0
0
1
0
|
0
0
0
1
|
0

−


2
1
0
|
0
0
1
0
|
0
−1
0
1
|
0

=


0
−1
0
|
0
0
1
0
|
0
1
0
1
|
0



248
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
The row reduced echelon form is


1
0
1
|
0
0
1
0
|
0
0
0
0
|
0


Therefore the eigenvectors are of the form (x, 0, −x)T . One such eigenvector is (1, 0, −1)T .
11. Let M be an n × n matrix. Then deﬁne the adjoint of M,denoted by M ∗to be the
transpose of the conjugate of M. For example,
µ
2
i
1 + i
3
¶∗
=
µ
2
1 −i
−i
3
¶
.
A matrix, M, is self adjoint if M ∗= M. Show the eigenvalues of a self adjoint matrix
are all real. If the self adjoint matrix has all real entries, it is called symmetric. Show
that the eigenvalues and eigenvectors of a symmetric matrix occur in conjugate pairs.
First note that for x a vector, x∗x = |x|2 . This is because
x∗x =
X
k
xkxk =
X
k
|xk|2 ≡|x|2 .
Also note that (AB)∗= B∗A∗because this holds for transposes.This implies that for
A an n × m matrix,
x∗A∗x = (Ax)∗x
Then if Mx = λx
λx∗x
=
(λx)∗x = (Mx)∗x = x∗M ∗x
=
x∗Mx = x∗λx = λx∗x
and so λ = λ showing that λ must be real.
12. Suppose A is an n × n matrix consisting entirely of real entries but a + ib is a complex
eigenvalue having the eigenvector, x + iy. Here x and y are real vectors. Show that
then a −ib is also an eigenvalue with the eigenvector, x −iy. Hint: You should
remember that the conjugate of a product of complex numbers equals the product of
the conjugates. Here a + ib is a complex number whose conjugate equals a −ib.
If A is real then the characteristic equation has all real coeﬃcients. Therefore, letting
p (λ) be the characteristic polynomial,
0 = p (λ) = p (λ) = p
¡
λ
¢
showing that λ is also an eigenvalue.
13. Find the eigenvalues and eigenvectors of the matrix


−10
−2
11
−18
6
−9
10
−10
−2

.
Determine whether the matrix is defective.
The matrix has eigenvalues −12 and 18. Of these, −12 is repeated with multiplicity
two. Therefore, you need to see whether the eigenspace has dimension two. If it does,

12.2.
THE CAYLEY HAMILTON THEOREM∗
249
then the matrix is non defective. If it does not, then the matrix is defective. The row
reduced echelon form for the system you need to solve is


2
−2
11
|
0
−18
18
−9
|
0
10
−10
10
|
0


and its row reduced echelon form is


1
−1
0
|
0
0
0
1
|
0
0
0
0
|
0


Therefore, the eigenspace is of the form


t
t
0


This is only one dimensional and so the matrix is defective.
14. Here is a matrix. A =


1
2
0
0
−1
0
0
−2
1

. Find a formula for An where n is an integer.
First you ﬁnd the eigenvectors and eigenvalues.


1
2
0
0
−1
0
0
−2
1

, eigenvectors:


1
0
0

,


0
0
1

↔1,


−1
1
1

↔−1.
The matrix, S used to diagonalize the matrix is obtained by letting these vectors be
the columns of S. Then S−1 is given by
S−1 =


1
1
0
0
−1
1
0
1
0


Then S−1AS equals


1
1
0
0
−1
1
0
1
0




1
2
0
0
−1
0
0
−2
1




1
0
−1
0
0
1
0
1
1


=


1
0
0
0
1
0
0
0
−1

≡D
Then A = SDS−1 and An = SDnS−1. Now it is easy to ﬁnd Dn.
Dn =


1
0
0
0
1
0
0
0
(−1)n



250
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore,
An
=


1
0
−1
0
0
1
0
1
1




1
0
0
0
1
0
0
0
(−1)n




1
1
0
0
−1
1
0
1
0


=


1
1 −(−1)n
0
0
(−1)n
0
0
−1 + (−1)n
1

.
15. Suppose the eigenvalues of A are λ1, · · ·, λn and that A is nondefective. Show that
eAt = S



eλ1t
· · ·
0
...
...
...
0
· · ·
eλnt


S−1 where S is the matrix which satisﬁes S−1AS = D.
The diagonal matrix, D has the same characteristic equation as A why? and so it has
the same eigenvalues. However the eigenvalues of D are the diagonal entries and so
the diagonal entries of D are the eigenvalues of A. Now
S−1tAS = tD
and
(tD)n =



(λ1t)n
· · ·
0
...
...
...
0
· · ·
(λnt)n



Therefore,
∞
X
n=0
1
n! (tD)n
=
∞
X
n=0
¡
S−1tAS
¢n
n!
=
S−1
∞
X
n=0
(tA)n
n!
S.
Now the left side equals
∞
X
n=0
1
n! (tD)n
=
∞
X
n=0
1
n!



(λ1t)n
· · ·
0
...
...
...
0
· · ·
(λnt)n



=



P∞
n=0
(λ1t)n
n!
· · ·
0
...
...
...
0
· · ·
P∞
n=0
(λnt)n
n!



=



eλ1t
· · ·
0
...
...
...
0
· · ·
eλnt


.
Therefore,
etA ≡
∞
X
n=0
(tA)n
n!
= S



eλ1t
· · ·
0
...
...
...
0
· · ·
eλnt


S−1.

12.2.
THE CAYLEY HAMILTON THEOREM∗
251
Do you think you understand this? If so, think again. What exactly do you mean by
an inﬁnite sum? Actually there is no problem here. You can do this just ﬁne and the
sums converge in the sense that the ijth entries converge in the partial sums. Think
about this. You know what you need from calculus to see this.
16. Show that if A is similar to B then AT is similar to BT .
This is easy. A = S−1BS and so AT = ST BT ¡
S−1¢T = ST BT ¡
ST ¢−1 .
17. Suppose Am = 0 for some m a positive integer. Show that if A is diagonalizable, then
A = 0.
Since Am = 0 suppose S−1AS = D. Then raising to the mth power, Dm = S−1AmS =
0. Therefore, D = 0. But then A = S0S−1 = 0.
18. Find the complex eigenvalues and eigenvectors of the matrix


1
1
−6
7
−5
−6
−1
7
2

.
Determine whether the matrix is defective.
After wading through much aﬄiction you ﬁnd the eigenvalues are −6, 2 + 6i, 2 −6i.
Since these are distinct, the matrix cannot be defective. We must ﬁnd the eigenvectors
for these eigenvalues. The augmented matrix for the system of equations which must
be solved to ﬁnd the eigenvectors associated with 2 −6i is


−1 + 6i
1
−6
|
0
7
−7 + 6i
−6
|
0
−1
7
6i
|
0

.
The row reduced echelon form is


1
0
i
0
0
1
i
0
0
0
0
0


and so the eigenvectors are of the form
t


−i
−i
1

.
You can check this as follows


1
1
−6
7
−5
−6
−1
7
2




−i
−i
1

=


−6 −2i
−6 −2i
2 −6i


and
(2 −6i)


−i
−i
1

=


−6 −2i
−6 −2i
2 −6i

.
It follows that the eigenvectors for λ = 2 + 6i are
t


i
i
1

.

252
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
This is because A is real. If Av = λv, then taking the conjugate,
Av = Av = λv.
It only remains to ﬁnd the eigenvector for λ = −6. The augmented matrix to row
reduce is


7
1
−6
|
0
7
1
−6
|
0
−1
7
8
|
0


The row reduced echelon form is


1
0
−1
|
0
0
1
1
|
0
0
0
0
|
0

.
Then an eigenvector is


−1
1
−1

.

Part VI
Curves, Curvilinear Motion,
Surfaces
253


255
Outcomes
Curves in Space
A. Identify the domain of a vector function.
B. Identify a curve given its parameterization.
C. Determine combinations of vector functions such as sums, vector products and scalar
products.
D. Deﬁne limit, derivative and integral for vector functions.
E. Evaluate limits, derivatives and integrals of vector functions.
F. Find the line tangent to a curve at a given point.
G. Describe what is meant by arc length.
H. Evaluate the arc length of a curve.
I. Recall, derive and apply rules to combinations of vector functions for the following:
(a) limits
(b) diﬀerentiation
(c) integration
Reading: Multivariable Calculus 1.6
Outcome Mapping:
A. 1
B. 2
C. 3,4
D. C1
E. C2,C3,C4
F. 5,16
G. C5
H. 6
I. 7,8,14
Curvilinear Motion
A. Sketch the curve determined by a vector function in 2-space or 3-space.
B. Parameterize a curve in 2-space or 3-space.
C. Given the position vector function of a moving object, calculate the velocity, speed,
and acceleration of the object.
D. Model and analyze curvilinear motion in applications.

256
Reading: Multivariable Calculus 1.7
Outcome Mapping:
A. D1
B. D2
C. 1
D. 3,5,6,10,18
Curvature
A. Recall the deﬁnitions of unit tangent, unit normal, binormal and osculating plane for
a space curve. Illustrate each graphically.
B. Calculate the curvature, the radius of curvature, the center of curvature and the
osculating plane for a space curve.
C. Derive formulas for the curvature of a parameterized curve and the curvature of a
plane curve given as a function.
D. Determine the tangential and normal components of acceleration for a given path.
Reading: Multivariable Calculus 1.8
Learning Module: Moving Trihedron
Outcome Mapping:
A. E1,E2
B. 1,5
C. 4
D. 2
Surfaces
A. Identify standard quadratic surfaces given their functions or graphs.
B. Sketch the graph of a quadratic surface by identifying the intercepts, traces, sections,
symmetry and boundedness or unboundedness of the surface.
Reading: Multivariable Calculus 1.4
Outcome Mapping:
A. 2,3
B. 3,4,6

Quadric Surfaces 9 Oct.
Quiz
1. Find the eigenvectors of the matrix,


4
1
0
2
8
3
−2
−2
3


given the eigenvalues are 6 and 3. Also tell whether the matrix is defective or non
defective.
2. Here is a matrix.
A =


3
4
1
4
0
1
4
3
4
0
−1
4
−1
4
1
2


Find A50. The eigenvalues of this matrix are 1 and 1/2 and eigenvectors for these
eigenvalues are


1
1
−1

for λ = 1 and


0
0
1

and


−1
1
0

for λ = 1/2.
3. Here is a Markov matrix. This is also called a migration matrix or a stochastic matrix.
A =


1/2
1/3
1/4
0
1/3
1/4
1/2
1/3
1/2


Find
lim
n→∞An


16
30
5

.
Recall the equation of an arbitrary plane is an equation of the form ax + by + cz = d.
More generally, a set of points of the following form
{(x, y, z) : f (x, y, z) = c}
is called a level surface. There are some standard level surfaces which involve certain vari-
ables being raised to a power of 2 which are suﬃciently important that they are given names,
usually involving the portentous semi-word “oid”. These are graphed below using Maple, a
computer algebra system.
257

258
QUADRIC SURFACES 9 OCT.
–4
–2
0
2
4
x
–4
–2
0
2
4
y
–4
–2
0
2
4
z2/a2 −x2/b2 −y2/c2 = 1
hyperboloid of two sheets
–4
–2
0
2
4
x
–4
–2
0
2
4
y
–4
–2
0
2
4
x2/b2 + y2/c2 −z2/a2 = 1
hyperboloid of one sheet
–4
–2
0
2
4
x
–4
–2
0
2
4
y
–4
–2
0
2
4
z = x2/a2 −y2/b2
hyperbolic paraboloid
–2
–1
0
1
2
x
–2
–1
0
1
y
0
1
2
3
4
x2/b2 + y2/c2 = z
elliptic paraboloid
–1
0
1
x
–2
–1
0
1
2
y
–2
–1
0
1
2
x2/a2 + y2/b2 + z2/c2 = 1
ellipsoid
–1
0
1
x
–2
–1
0
1
2
y
–2
–1
0
1
2
x2/b2 + y2/c2 = z2/a2
elliptic cone
Why do the graphs of these level surfaces look the way they do? Consider ﬁrst the

259
hyperboloid of two sheets. The equation deﬁning this surface can be written in the form
z2
a2 −1 = x2
b2 + y2
c2 .
Suppose you ﬁx a value for z. What ordered pairs, (x, y) will satisfy the equation? If z2
a2 < 1,
there is no such ordered pair because the above equation would require a negative number
to equal a nonnegative one. This is why there is a gap and there are two sheets. If z2
a2 > 1,
then the above equation is the equation for an ellipse. That is why if you slice the graph by
letting z = z0 the result is an ellipse in the plane z = z0.
Consider the hyperboloid of one sheet.
x2
b2 + y2
c2 = 1 + z2
a2 .
This time, it doesn’t matter what value z takes. The resulting equation for (x, y) is an
ellipse.
Similar considerations apply to the elliptic paraboloid as long as z > 0 and the ellipsoid.
The elliptic cone is like the hyperboloid of two sheets without the 1. Therefore, z can have
any value. In case z = 0, (x, y) = (0, 0) . Viewed from the side, it appears straight, not
curved like the hyperboloid of two sheets.This is because if (x, y, z) is a point on the surface,
then if t is a scalar, it follows (tx, ty, tz) is also on this surface.
The most interesting of these graphs is the hyperbolic paraboloid1, z = x2
a2 −y2
b2 . If z > 0
this is the equation of a hyperbola which opens to the right and left while if z < 0 it is a
hyperbola which opens up and down. As z passes from positive to negative, the hyperbola
changes type and this is what yields the shape shown in the picture.
Not surprisingly, you can ﬁnd intercepts and traces of quadric surfaces just as with
planes.
Example 13.0.5 Find the trace on the xy plane of the hyperbolic paraboloid, z = x2 −y2.
This occurs when z = 0 and so this reduces to y2 = x2. In other words, this trace is just
the two straight lines, y = x and y = −x.
Example 13.0.6 Find the intercepts of the ellipsoid, x2 + 2y2 + 4z2 = 9.
To ﬁnd the intercept on the x axis, let y = z = 0 and this yields x = ±3. Thus there are
two intercepts, (3, 0, 0) and (−3, 0, 0) . The other intercepts are left for you to ﬁnd. You can
see this is an aid in graphing the quadric surface. The surface is said to be bounded if there is
some number, C such that whenever, (x, y, z) is a point on the surface,
p
x2 + y2 + z2 < C.
The surface is called unbounded if no such constant, C exists. Ellipsoids are bounded but
the other quadric surfaces are not bounded.
Example 13.0.7 Why is the hyperboloid of one sheet, x2 + 2y2 −z2 = 1 unbounded?
Let z be very large. Does there correspond (x, y) such that (x, y, z) is a point on the
hyperboloid of one sheet? Certainly. Simply pick any (x, y) on the ellipse x2 +2y2 = 1+z2.
Then
p
x2 + y2 + z2 is large, at lest as large as z. Thus it is unbounded.
You can also ﬁnd intersections between lines and surfaces.
Example 13.0.8 Find the points of intersection of the line (x, y, z) = (1 + t, 1 + 2t, 1 + t)
with the surface, z = x2 + y2.
1It is traditional to refer to this as a hyperbolic paraboloid. Not a parabolic hyperboloid.

260
QUADRIC SURFACES 9 OCT.
First of all, there is no guarantee there is any intersection at all. But if it exists, you
have only to solve the equation for t
1 + t = (1 + t)2 + (1 + 2t)2
This occurs at the two values of t = −1
2 + 1
10
√
5, t = −1
2 −1
10
√
5. Therefore, the two points
are
(1, 1, 1) +
µ
−1
2 + 1
10
√
5
¶
(1, 2, 1) , and (1, 1, 1) +
µ
−1
2 −1
10
√
5
¶
(1, 2, 1)
That is
µ1
2 + 1
10
√
5, 1
5
√
5, 1
2 + 1
10
√
5
¶
,
µ1
2 −1
10
√
5, −1
5
√
5, 1
2 −1
10
√
5
¶
.
A cylinder generated by a curve, C is the surface generated by moving the curve C
through space along a straight line. If you are given a level surface of the form f (x, y) = c
this will yield a cylinder parallel to the z axis. Here is why: If z = 0, then f (x, y) = c is a
curve in the plane, z = 0. If z = 1, then you get exactly the same curve but just shifted up
to a height of 1. Similarly, f (y, z) = c gives a cylinder parallel to the x axis and f (x, z) = c
gives one which is parallel to the y axis.
Example 13.0.9 Consider the cylinder x2 + y2 = 1. Sketch its graph.
y
z
x
¡
¡
You see that at every height above or below the z = 0 plane if you slice it at that level,
you will just see the graph of x2 + y2 = 1 at that level. This is a circle of radius 1. Since
the equation describing the surface does not depend on z, this is why it looks the same at
every level.

Curves In Space 10,11 Oct.
14.1
Limits Of A Vector Valued Function Of One Vari-
able
Quiz
1. Find the determinant of the matrix,


1
2
1
2
1
−1
−1
−3
1


2. Find all eigenspaces and eigenvalues for the matrix,


2
1
0
0
2
1
0
0
1


3. Find the eigenspace for the eigenvalue λ = 2 for the matrix


1
1
0
0
2
0
−1
1
2


A vector valued function is just one which has vector values. For example, consider
¡
cos t, t2, t + 1
¢
where t ∈[0, 2].
Each value of t corresponds to a point in R3 whose coordinates are
as given. Thus when t = 0, the point in R3 is (1, 0, 1) and when t = π/2 the point is
³
0,
¡ π
2
¢2 , π
2 + 1
´
, etc. Often t will be considered as time. Thus, in this case, the vector
valued function gives the coordinates of a point which is moving in three dimensions as a
function of time. Imagine a ﬂy buzzing around the room for example. Let the origin be a
corner of the room and consider the position vector of the ﬂy. This position vector could
be described by a vector valued function of the form (x (t) , y (t) , z (t)) where t is in some
interval. Here x (t) is the x coordinate of the ﬂy, y (t) , the y coordinate, and z (t) , the z
coordinate corresponding to a given time. Later the physical signiﬁcance of all this will be
discussed more. For right now, t will just be in some interval and general vector valued
functions will be considered.
261

262
CURVES IN SPACE 10,11 OCT.
Deﬁnition 14.1.1 Let x (t) = (x1 (t) , · · ·, xn (t)) for t ∈[a, b] be a vector valued
function. The curve parameterized by this vector valued function is the set of points in
Rn which are obtained by letting t vary over the interval, [a, b]. The vector valued function is
also called a parameterization of this curve. The variable, t is called a parameter. More
generally, if x (t) = (x1 (t) , · · ·, xn (t)) is given where each xi (t) is a formula the domain
of x is deﬁned to be the set where each of the xi (t) is deﬁned. It is denoted by D(x).
Example 14.1.2 Let x (t) =
¡ 1
t ,
√
1 −t2, sin (t)
¢
ﬁnd the domain of x.
You need each function to make sense. Thus you must have −1 ≤t ≤1 and t ̸= 0. The
domain is [−1, 0) ∪(0, 1].
In useful situations the domain will typically be an interval.
One can give a meaning to
lim
s→t+ f (s) , lim
s→t−f (s) , lim
s→∞f (s) ,
and
lim
s−∞f (s) .
Deﬁnition 14.1.3 In the case where D (f) is only assumed to satisfy D (f) ⊇
(t, t + r) ,
lim
s→t+ f (s) = L
if and only if for all ε > 0 there exists δ > 0 such that if
0 < s −t < δ,
then
|f (s) −L| < ε.
In the case where D (f) is only assumed to satisfy D (f) ⊇(t −r, t) ,
lim
s→t−f (s) = L
if and only if for all ε > 0 there exists δ > 0 such that if
0 < t −s < δ,
then
|f (s) −L| < ε.
One can also consider limits as a variable “approaches” inﬁnity. Of course nothing is “close”
to inﬁnity and so this requires a slightly diﬀerent deﬁnition.
lim
t→∞f (t) = L
if for every ε > 0 there exists l such that whenever t > l,
|f (t) −L| < ε
(14.1)
and
lim
t→−∞f (t) = L
if for every ε > 0 there exists l such that whenever t < l, 14.1 holds.

14.2.
THE DERIVATIVE AND INTEGRAL
263
Note that in all of this the deﬁnitions are identical to the case of scalar valued functions.
The only diﬀerence is that here |·| refers to the norm or length in Rp where maybe p > 1.
Observation 14.1.4 Let f (t) = (f1 (t) , · · ·, fn (t)) and let L = (L1, · · ·, Ln) . Then
limt→a f (t) = L if and only if limt→a fk (t) = Lk for each k.
Example 14.1.5 Let f (t) =
¡
cos t, sin t, t2 + 1, ln (t)
¢
. Find limt→π/2 f (t) .
Using the above observation, this limit equals
µ
lim
t→π/2 cos t, lim
t→π/2 (sin t) , lim
t→π/2
¡
t2 + 1
¢
, lim
t→π/2 ln (t)
¶
=
µ
0, 1,
µπ2
4 + 1
¶
, ln
³π
2
´¶
.
Example 14.1.6 Let f (t) =
¡ sin t
t , t2, t + 1
¢
. Find limt→0 f (t) .
Recall that limt→0 sin t
t
= 1. Then using the above observation, limt→0 f (t) = (1, 0, 1) .
14.2
The Derivative And Integral
The following deﬁnition is on the derivative and integral of a vector valued function of one
variable.
Deﬁnition 14.2.1 The derivative of a function, f ′ (t) , is deﬁned as the following
limit whenever the limit exists. If the limit does not exist, then neither does f ′ (t) .
lim
h→0
f (t + h) −f (x)
h
≡f ′ (t)
The function of h on the left is called the diﬀerence quotient just as it was for a scalar
valued function. If f (t) = (f1 (t) , · · ·, fp (t)) and
R b
a fi (t) dt exists for each i = 1, ···, p, then
R b
a f (t) dt is deﬁned as the vector,
ÃZ b
a
f1 (t) dt, · · ·,
Z b
a
fp (t) dt
!
.
This is what is meant by saying f ∈R ([a, b]) . In other words, f is Riemann integrable.
That is you can take the integral.
It is easier to write f ∈R ([a, b]) than to write f is Riemann integrable. Thus, if you see
f ∈R ([a, b]) , think:
R b
a f (x) dx exists.
This is exactly like the deﬁnition for a scalar valued function. As before,
f ′ (x) = lim
y→x
f (y) −f (x)
y −x
.
As in the case of a scalar valued function, diﬀerentiability implies continuity but not the
other way around.
Theorem 14.2.2 If f ′ (t) exists, then f is continuous at t.

264
CURVES IN SPACE 10,11 OCT.
Proof: Suppose ε > 0 is given and choose δ1 > 0 such that if |h| < δ1,
¯¯¯¯
f (t + h) −f (t)
h
−f ′ (t)
¯¯¯¯ < 1.
then for such h, the triangle inequality implies
|f (t + h) −f (t)| < |h| + |f ′ (t)| |h| .
Now letting δ < min
³
δ1,
ε
1+|f ′(x)|
´
it follows if |h| < δ, then
|f (t + h) −f (t)| < ε.
Letting y = h + t, this shows that if |y −t| < δ,
|f (y) −f (t)| < ε
which proves f is continuous at t. This proves the theorem.
As in the scalar case, there is a fundamental theorem of calculus.
Theorem 14.2.3 If f ∈R ([a, b]) and if f is continuous at t ∈(a, b) , then
d
dt
µZ t
a
f (s) ds
¶
= f (t) .
Proof: Say f (t) = (f1 (t) , · · ·, fp (t)) . Then it follows
1
h
Z t+h
a
f (s) ds −1
h
Z t
a
f (s) ds =
Ã
1
h
Z t+h
t
f1 (s) ds, · · ·, 1
h
Z t+h
t
fp (s) ds
!
and limh→0 1
h
R t+h
t
fi (s) ds = fi (t) for each i = 1, · · ·, p from the fundamental theorem of
calculus for scalar valued functions. Therefore,
lim
h→0
1
h
Z t+h
a
f (s) ds −1
h
Z t
a
f (s) ds = (f1 (t) , · · ·, fp (t)) = f (t)
and this proves the claim.
Example 14.2.4 Let f (x) = c where c is a constant. Find f ′ (x) .
The diﬀerence quotient,
f (x + h) −f (x)
h
= c −c
h
= 0
Therefore,
lim
h→0
f (x + h) −f (x)
h
= lim
h→0 0 = 0
Example 14.2.5 Let f (t) = (at, bt) where a, b are constants. Find f ′ (t) .
From the above discussion this derivative is just the vector valued functions whose com-
ponents consist of the derivatives of the components of f. Thus f ′ (t) = (a, b) .

14.2.
THE DERIVATIVE AND INTEGRAL
265
14.2.1
Arc Length
C is a smooth curve in Rn if there exists an interval, [a, b] ⊆R and functions xi : [a, b] →R
such that the following conditions hold
1. xi is continuous on [a, b] .
2. x′
i exists and is continuous and bounded on [a, b] , with x′
i (a) deﬁned as the derivative
from the right,
lim
h→0+
xi (a + h) −xi (a)
h
,
and x′
i (b) deﬁned similarly as the derivative from the left.
3. For p (t) ≡(x1 (t) , · · ·, xn (t)) , t →p (t) is one to one on (a, b) .
4. |p′ (t)| ≡
³Pn
i=1 |x′
i (t)|2´1/2
̸= 0 for all t ∈[a, b] .
5. C = ∪{(x1 (t) , · · ·, xn (t)) : t ∈[a, b]} .
The functions, xi (t) , deﬁned above are giving the coordinates of a point in Rn and the
list of these functions is called a parameterization for the smooth curve. Note the natural
direction of the interval also gives a direction for moving along the curve. Such a direction
is called an orientation. The integral is used to deﬁne what is meant by the length of such a
smooth curve. Consider such a smooth curve having parameterization (x1, · · ·, xn) . Forming
a partition of [a, b], a = t0 < · · · < tn = b and letting pi = ( x1 (ti), · · ·, xn (ti) ), you could
consider the polygon formed by lines from p0 to p1 and from p1 to p2 and from p3 to p4
etc. to be an approximation to the curve, C. The following picture illustrates what is meant
by this.
¡
¡
¡
¡
¡
¡
""""""
p0
p1
p2
p3
Now consider what happens when the partition is reﬁned by including more points. You
can see from the following picture that the polygonal approximation would appear to be
even better and that as more points are added in the partition, the sum of the lengths of
the line segments seems to get close to something which deserves to be deﬁned as the length
of the curve, C.
p0
p1
p2
p3

266
CURVES IN SPACE 10,11 OCT.
Thus the length of the curve is approximated by
n
X
k=1
|p (tk) −p (tk−1)| .
Since the functions in the parameterization are diﬀerentiable, it is reasonable to expect this
to be close to
n
X
k=1
|p′ (tk−1)| (tk −tk−1)
which is seen to be a Riemann sum for the integral
Z b
a
|p′ (t)| dt
and it is this integral which is deﬁned as the length of the curve.
Would the same length be obtained if another parameterization were used? This is a
very important question because the length of the curve should depend only on the curve
itself and not on the method used to trace out the curve. The answer to this question is
that the length of the curve does not depend on parameterization. It is proved in Section
16.2.2 which starts on Page 295.
Does the deﬁnition of length given above correspond to the usual deﬁnition of length in
the case when the curve is a line segment? It is easy to see that it does so by considering
two points in Rn, p and q. A parameterization for the line segment joining these two points
is
fi (t) ≡tpi + (1 −t) qi, t ∈[0, 1] .
Using the deﬁnition of length of a smooth curve just given, the length according to this
deﬁnition is
Z 1
0
Ã n
X
i=1
(pi −qi)2
!1/2
dt = |p −q| .
Thus this new deﬁnition which is valid for smooth curves which may not be straight line
segments gives the usual length for straight line segments.
Deﬁnition 14.2.6 A curve C is piecewise smooth if there exist points on this curve,
p0, p1, · · ·, pn such that, denoting Cpk−1pk the part of the curve joining pk−1 and pk, it
follows Cpk−1pk is a smooth curve and ∪n
k=1Cpk−1pk = C. In other words, it is piecewise
smooth if it consists of a ﬁnite number of smooth curves linked together.
To ﬁnd the length of a piecewise smooth curve, just sum the lengths of the smooth pieces
described above.
Example 14.2.7 The parameterization for a smooth curve is r (t) =
¡
t, 2t2, t2¢
for t ∈
[0, 1] . Find the length of this curve.
From the above, the length is
Z 1
0
|r′ (t)| dt =
Z 1
0
q
1 + (4t)2 + (2t)2dt = 1
2
√
21 + 1
20
√
5 ln
³
2
√
5 +
√
21
´
.
You need to use a trig substitution of some sort to do this integral but it is routine. Of
course, if you can’t ﬁnd an antiderivative, then you solve it numerically.

14.2.
THE DERIVATIVE AND INTEGRAL
267
Example 14.2.8 The parameterization for a smooth curve is r (t) =
¡
t, t3, t2¢
for t ∈[0, 1] .
Find the length of this curve.
The length is
Z 1
0
q
1 + (3t2)2 + (2t)2dt =
Z 1
0
p
1 + 9t4 + 4t2dt
and I have no clue how to ﬁnd an antiderivative for this. Therefore, I ﬁnd the integral
numerically.
Z 1
0
p
1 + 9t4 + 4t2dt = 1. 863
This is all right to do. Numerical methods are allowed and somtimes that is all you can get.
14.2.2
Geometric And Physical Signiﬁcance Of The Derivative
Suppose r is a vector valued function of a parameter, t not necessarily time and consider
the following picture of the points traced out by r.
r

r
©©©©©©©©©©©©©©
r
r              
r
r(t)
r(t + h)
-

1
©©©
*:
In this picture there are unit vectors in the direction of the vector from r (t) to r (t + h) .
You can see that it is reasonable to suppose these unit vectors, if they converge, converge
to a unit vector, T which is tangent to the curve at the point r (t) . Now each of these unit
vectors is of the form
r (t + h) −r (t)
|r (t + h) −r (t)| ≡Th.
Thus Th →T, a unit tangent vector to the curve at the point r (t) . Therefore,
r′ (t)
≡
lim
h→0
r (t + h) −r (t)
h
= lim
h→0
|r (t + h) −r (t)|
h
r (t + h) −r (t)
|r (t + h) −r (t)|
=
lim
h→0
|r (t + h) −r (t)|
h
Th = |r′ (t)| T.
In the case that t is time, the expression |r (t + h) −r (t)| is a good approximation for
the distance travelled by the object on the time interval [t, t + h] . The real distance would
be the length of the curve joining the two points but if h is very small, this is essentially
equal to |r (t + h) −r (t)| as suggested by the picture below.

268
CURVES IN SPACE 10,11 OCT.
r
r
r(t)
r(t + h)
Therefore,
|r (t + h) −r (t)|
h
gives for small h, the approximate distance travelled on the time interval, [t, t + h] divided
by the length of time, h. Therefore, this expression is really the average speed of the object
on this small time interval and so the limit as h →0, deserves to be called the instantaneous
speed of the object. Thus |r′ (t)| T represents the speed times a unit direction vector, T
which deﬁnes the direction in which the object is moving. Thus r′ (t) is the velocity of the
object. This is the physical signiﬁcance of the derivative when t is time.
How do you go about computing r′ (t)? Letting r (t) = (r1 (t) , · · ·, rq (t)) , the expression
r (t0 + h) −r (t0)
h
(14.2)
is equal to
µr1 (t0 + h) −r1 (t0)
h
, · · ·, rq (t0 + h) −rq (t0)
h
¶
.
Then as h converges to 0, 14.2 converges to
v ≡(v1, · · ·, vq)
where vk = r′
k (t) . This by Observation 14.1.4, which says that the term in 14.2 gets close
to a vector, v if and only if all the coordinate functions of the term in 14.2 get close to the
corresponding coordinate functions of v.
In the case where t is time, this simply says the velocity vector equals the vector whose
components are the derivatives of the components of the displacement vector, r (t) .
In any case, the vector, T determines a direction vector which is tangent to the curve at
the point, r (t) and so it is possible to ﬁnd parametric equations for the line tangent to the
curve at various points.
Example 14.2.9 Let r (t) =
¡
sin t, t2, t + 1
¢
for t ∈[0, 5] . Find a tangent line to the curve
parameterized by r at the point r (2) .
From the above discussion, a direction vector has the same direction as r′ (2) . There-
fore, it suﬃces to simply use r′ (2) as a direction vector for the line. r′ (2) = (cos 2, 4, 1) .
Therefore, a parametric equation for the tangent line is
(sin 2, 4, 3) + t (cos 2, 4, 1) = (x, y, z) .
Example 14.2.10 Let r (t) =
¡
sin t, t2, t + 1
¢
for t ∈[0, 5] . Find the velocity vector when
t = 1.
From the above discussion, this is simply r′ (1) = (cos 1, 2, 1) .

14.2.
THE DERIVATIVE AND INTEGRAL
269
14.2.3
Diﬀerentiation Rules
There are rules which relate the derivative to the various operations done with vectors such
as the dot product, the cross product, and vector addition and scalar multiplication.
Theorem 14.2.11 Let a, b ∈R and suppose f ′ (t) and g′ (t) exist. Then the fol-
lowing formulas are obtained.
(af + bg)′ (t) = af ′ (t) + bg′ (t) .
(14.3)
(f · g)′ (t) = f ′ (t) · g (t) + f (t) · g′ (t)
(14.4)
If f, g have values in R3, then
(f × g)′ (t) = f (t) × g′ (t) + f ′ (t) × g (t)
(14.5)
The formulas, 14.4, and 14.5 are referred to as the product rule.
Proof: The ﬁrst formula is left for you to prove. Consider the second, 14.4.
lim
h→0
f · g (t + h) −fg (t)
h
= lim
h→0
f (t + h) · g (t + h) −f (t + h) · g (t)
h
+ f (t + h) · g (t) −f (t) · g (t)
h
= lim
h→0
µ
f (t + h) · (g (t + h) −g (t))
h
+ (f (t + h) −f (t))
h
· g (t)
¶
= lim
h→0
n
X
k=1
fk (t + h) (gk (t + h) −gk (t))
h
+
n
X
k=1
(fk (t + h) −fk (t))
h
gk (t)
=
n
X
k=1
fk (t) g′
k (t) +
n
X
k=1
f ′
k (t) gk (t)
= f ′ (t) · g (t) + f (t) · g′ (t) .
Formula 14.5 is left as an exercise which follows from the product rule and the deﬁnition of
the cross product in terms of components given on Page 50. You can also see this is true
by using the distributive law of the cross product.
f (t + h) ×g (t + h) −f (t) ×g (t)
=
f (t + h) ×g (t + h) −f (t + h) ×g (t) + f (t + h) ×g (t) −f (t) ×g (t)
and so
1
h (f (t + h) ×g (t + h) −f (t) ×g (t))
=
f (t + h) ×
µg (t + h) −g (t)
h
¶
+
µf (t + h) −f (t)
h
¶
×g (t)
Now assuming the cross product is continuous, (This is obvious from either the component
or the geometric description of the cross product.) you can take a limit in the above as
h →0 and obtain
f (t) × g′ (t) + f ′ (t) × g (t) .
It is exactly like the product rule for scalar valued functions except you need to be very
careful about the order in which things are multiplied becasue the cross product is not
commutative.

270
CURVES IN SPACE 10,11 OCT.
Example 14.2.12 Let
r (t) =
¡
t2, sin t, cos t
¢
and let p (t) = (t, ln (t + 1) , 2t). Find (r (t) × p (t))′ .
From 14.5 this equals(2t, cos t, −sin t) × (t, ln (t + 1) , 2t) +
¡
t2, sin t, cos t
¢
×
³
1,
1
t+1, 2
´
=
¡
2 (cos t) t + sin t ln (t + 1) , −(sin t) t −4t2, 2t ln (t + 1) −(cos t) t
¢
+
µ
2 sin t −cos t
t + 1, cos t −2t2,
t2
t + 1 −sin t
¶
= (2 (cos t) t + sin t ln (t + 1) + 2 sin t −cos t
t+1 , −(sin t) t −6t2 + cos t,
2t ln (t + 1) −(cos t) t +
t2
t+1 −sin t)
Example 14.2.13 Let r (t) =
¡
t2, sin t, cos t
¢
Find
R π
0 r (t) dt.
This equals
¡R π
0 t2 dt,
R π
0 sin t dt,
R π
0 cos t dt
¢
=
¡ 1
3π3, 2, 0
¢
.
Example 14.2.14 An object has position r (t) =
³
t3,
t
1+1,
√
t2 + 2
´
kilometers where t is
given in hours. Find the velocity of the object in kilometers per hour when t = 1.
Recall the velocity at time t was r′ (t) . Therefore, ﬁnd r′ (t) and plug in t = 1 to ﬁnd
the velocity.
r′ (t) =
Ã
3t2, 1 (1 + t) −t
(1 + t)2
, 1
2
¡
t2 + 2
¢−1/2 2t
!
=
Ã
3t2,
1
(1 + t)2 ,
1
p
(t2 + 2)
t
!
When t = 1, the velocity is
r′ (1) =
µ
3, 1
4, 1
√
3
¶
kilometers per hour.
Obviously, this can be continued. That is, you can consider the possibility of taking the
derivative of the derivative and then the derivative of that and so forth. The main thing to
consider about this is the notation and it is exactly like it was in the case of a scalar valued
function presented earlier. Thus r′′ (t) denotes the second derivative.
When you are given a vector valued function of one variable, sometimes it is possible to
give a simple description of the curve which results. Usually it is not possible to do this!
Example 14.2.15 Describe the curve which results from the vector valued function, r (t) =
(cos 2t, sin 2t, t) where t ∈R.
The ﬁrst two components indicate that for r (t) = (x (t) , y (t) , z (t)) , the pair, (x (t) , y (t))
traces out a circle. While it is doing so, z (t) is moving at a steady rate in the positive di-
rection. Therefore, the curve which results is a cork skrew shaped thing called a helix.
As an application of the theorems for diﬀerentiating curves, here is an interesting appli-
cation. It is also a situation where the curve can be identiﬁed as something familiar.

14.2.
THE DERIVATIVE AND INTEGRAL
271
Example 14.2.16 Sound waves have the angle of incidence equal to the angle of reﬂection.
Suppose you are in a large room and you make a sound. The sound waves spread out and
you would expect your sound to be inaudible very far away. But what if the room were shaped
so that the sound is reﬂected oﬀthe wall toward a single point, possibly far away from you?
Then you might have the interesting phenomenon of someone far away hearing what you
said quite clearly. How should the room be designed?
Suppose you are located at the point P0 and the point where your sound is to be reﬂected
is P1. Consider a plane which contains the two points and let r (t) denote a parameterization
of the intersection of this plane with the walls of the room. Then the condition that the angle
of reﬂection equals the angle of incidence reduces to saying the angle between P0 −r (t) and
−r′ (t) equals the angle between P1 −r (t) and r′ (t) . Draw a picture to see this. Therefore,
(P0 −r (t)) · (−r′ (t))
|P0 −r (t)| |r′ (t)|
= (P1 −r (t)) · (r′ (t))
|P1 −r (t)| |r′ (t)| .
This reduces to
(r (t) −P0) · (−r′ (t))
|r (t) −P0|
= (r (t) −P1) · (r′ (t))
|r (t) −P1|
(14.6)
Now
(r (t) −P1) · (r′ (t))
|r (t) −P1|
= d
dt |r (t) −P1|
and a similar formula holds for P1 replaced with P0. This is because
|r (t) −P1| =
p
(r (t) −P1) · (r (t) −P1)
and so using the chain rule and product rule,
d
dt |r (t) −P1|
=
1
2 ((r (t) −P1) · (r (t) −P1))−1/2 2 ((r (t) −P1) · r′ (t))
=
(r (t) −P1) · (r′ (t))
|r (t) −P1|
.
Therefore, from 14.6,
d
dt (|r (t) −P1|) + d
dt (|r (t) −P0|) = 0
showing that |r (t) −P1| + |r (t) −P0| = C for some constant, C.This implies the curve of
intersection of the plane with the room is an ellipse having P0 and P1 as the foci.
14.2.4
Leibniz’s Notation
Leibniz’s notation also generalizes routinely. For example, dy
dt = y′ (t) with other similar
notations holding.
14.2.5
Exercises With Answers
1. Find the following limits if possible
(a) limx→0+
³
|x|
x , sin 2x/x, tan x
x
´
= (1, 2, 1)
(b) limx→0+
³
x
|x|, cos x, e2x´
= (1, 1, 1)
(c) limx→4
³
x2−16
x+4 , x −7, tan 7x
5x
´
=
¡
0, −3, 7
5
¢

272
CURVES IN SPACE 10,11 OCT.
2. Let r (t) =
³
4 + (t −1)2 ,
√
t2 + 1 (t −1)3 , (t−1)3
t5
´
describe the position of an object
in R3 as a function of t where t is measured in seconds and r (t) is measured in meters.
Is the velocity of this object ever equal to zero? If so, ﬁnd the value of t at which this
occurs and the point in R3 at which the velocity is zero.
You need to diﬀerentiate this. r′ (t) =
µ
2 (t −1) , (t −1)2 4t2−t+3
√
(t2+1), −(t −1)2 2t−5
t6
¶
.
Now you need to ﬁnd the value(s) of t where r′ (t) = 0.
3. Let r (t) =
¡
sin t, t2, 2t + 1
¢
for t ∈[0, 4] . Find a tangent line to the curve parameter-
ized by r at the point r (2) .
r′ (t) = (cos t, 2t, 2). When t = 2, the point on the curve is (sin 2, 4, 5) . A direction
vector is r′ (2) and so a tangent line is r (t) = (sin 2, 4, 5) + t (cos 2, 4, 2) .
4. Let r (t) =
¡
sin t, cos
¡
t2¢
, t + 1
¢
for t ∈[0, 5] . Find the velocity when t = 3.
r′ (t) =
¡
cos t, −2t sin
¡
t2¢
, 1
¢
. The velocity when t = 3 is just r′ (3) = (cos 3, −6 sin (9) , 1) .
5. Suppose r (t), s (t) , and p (t) are three diﬀerentiable functions of t which have values
in R3. Find a formula for (r (t) × s (t) · p (t))′ .
From the product rules for the cross and dot product, this equals
(r (t) × s (t))′·p (t)+r (t)×s (t)·p′ (t) = r′ (t)×s (t)·p (t)+r (t)×s′ (t)·p (t)+r (t)×s (t)·p′ (t)
6. If r′ (t) = 0 for all t ∈(a, b), show there exists a constant vector, c such that r (t) = c
for all t ∈(a, b) .
Do this by considering standard one variable calculus and on the components of r (t) .
7. If F′ (t) = f (t) for all t ∈(a, b) and F is continuous on [a, b] , show
R b
a f (t) dt =
F (b) −F (a) .
Do this by considering standard one variable calculus and on the components of r (t) .
8. Verify that if Ω× u = 0 for all u, then Ω= 0.
Geometrically this says that if Ωis not equal to zero then it is parallel to every vector.
Why does this make it obvious that Ωmust equal zero?

Newton’s Laws Of Motion∗
I assume you have seen basic mechanics as found in introductory physics course. However,
if you need a review, the following section is oﬀered. Read it if you need to. Otherwise,
skip it. Calculus was invented to solve problems in physics and engineering, not to do cute
geometry. The material which follows on physics of motion on a space curve will make more
sense to you if you know Newton’s laws.
Deﬁnition 15.0.17 Let r (t) denote the position of an object. Then the accelera-
tion of the object is deﬁned to be r′′ (t) .
Newton’s1 ﬁrst law is: “Every body persists in its state of rest or of uniform motion in
a straight line unless it is compelled to change that state by forces impressed on it.”
Newton’s second law is:
F = ma =mr′′ (t)
(15.1)
where a is the acceleration and m is the mass of the object.
Newton’s third law states: “To every action there is always opposed an equal reaction;
or, the mutual actions of two bodies upon each other are always equal, and directed to
contrary parts.”
Of these laws, only the second two are independent of each other, the ﬁrst law being
implied by the second. The third law says roughly that if you apply a force to something,
the thing applies the same force back.
The second law is the one of most interest. Note that the statement of this law depends
on the concept of the derivative because the acceleration is deﬁned as a derivative. Newton
used calculus and these laws to solve profound problems involving the motion of the planets
and other problems in mechanics. The next example involves the concept that if you know
the force along with the initial velocity and initial position, then you can determine the
position.
Example 15.0.18 Let r (t) denote the position of an object of mass 2 kilogram at time
t and suppose the force acting on the object is given by F (t) =
¡
t, 1 −t2, 2e−t¢
. Suppose
r (0) = (1, 0, 1) meters, and r′ (0) = (0, 1, 1) meters/sec. Find r (t) .
1Isaac Newton 1642-1727 is often credited with inventing calculus although this is not correct since most
of the ideas were in existence earlier. However, he made major contributions to the subject partly in order
to study physics and astronomy. He formulated the laws of gravity, made major contributions to optics, and
stated the fundamental laws of mechanics listed here. He invented a version of the binomial theorem when
he was only 23 years old and built a reﬂecting telescope. He showed that Kepler’s laws for the motion of the
planets came from calculus and his laws of gravitation. In 1686 he published an important book, Principia,
in which many of his ideas are found. Newton was also very interested in theology and had strong views on
the nature of God which were based on his study of the Bible and early Christian writings. He ﬁnished his
life as Master of the Mint.
273

274
NEWTON’S LAWS OF MOTION∗
By Newton’s second law, 2r′′ (t) = F (t) =
¡
t, 1 −t2, 2e−t¢
and so
r′′ (t) =
¡
t/2,
¡
1 −t2¢
/2, e−t¢
.
Therefore the velocity is given by
r′ (t) =
µt2
4 , t −t3/3
2
, −e−t
¶
+ c
where c is a constant vector which must be determined from the initial condition given for
the velocity. Thus letting c = (c1, c2, c3) ,
(0, 1, 1) = (0, 0, −1) + (c1, c2, c3)
which requires c1 = 0, c2 = 1, and c3 = 2. Therefore, the velocity is found.
r′ (t) =
µt2
4 , t −t3/3
2
+ 1, −e−t + 2
¶
.
Now from this, the displacement must equal
r (t) =
µ t3
12, t2/2 −t4/12
2
+ t, e−t + 2t
¶
+ (C1, C2, C3)
where the constant vector, (C1, C2, C3) must be determined from the initial condition for
the displacement. Thus
r (0) = (1, 0, 1) = (0, 0, 1) + (C1, C2, C3)
which means C1 = 1, C2 = 0, and C3 = 0. Therefore, the displacement has also been found.
r (t) =
µ t3
12 + 1, t2/2 −t4/12
2
+ t, e−t + 2t
¶
meters.
Actually, in applications of this sort of thing acceleration does not usually come to you as a
nice given function written in terms of simple functions you understand. Rather, it comes as
measurements taken by instruments and the position is continuously being updated based
on this information. Another situation which often occurs is the case when the forces on
the object depend not just on time but also on the position or velocity of the object.
Example 15.0.19 An artillery piece is ﬁred at ground level on a level plain. The angle of
elevation is π/6 radians and the speed of the shell is 400 meters per second. How far does
the shell ﬂy before hitting the ground?
Neglect air resistance in this problem. Also let the direction of ﬂight be along the positive
x axis. Thus the initial velocity is the vector, 400 cos (π/6) i + 400 sin (π/6) j while the only
force experienced by the shell after leaving the artillery piece is the force of gravity, −mgj
where m is the mass of the shell. The acceleration of gravity equals 9.8 meters per sec2 and
so the following needs to be solved.
mr′′ (t) = −mgj, r (0) = (0, 0) , r′ (0) = 400 cos (π/6) i + 400 sin (π/6) j.
Denoting r (t) as (x (t) , y (t)) ,
x′′ (t) = 0, y′′ (t) = −g.

275
Therefore, y′ (t) = −gt+C and from the information on the initial velocity, C = 400 sin (π/6) =
200. Thus
y (t) = −4.9t2 + 200t + D.
D = 0 because the artillery piece is ﬁred at ground level which requires both x and y to
equal zero at this time. Similarly, x′ (t) = 400 cos (π/6) so x (t) = 400 cos (π/6) t = 200
√
3t.
The shell hits the ground when y = 0 and this occurs when −4.9t2 +200t = 0. Thus t = 40.
816 326 530 6 seconds and so at this time,
x = 200
√
3 (40. 816 326 530 6) = 14139. 190 265 9 meters.
The next example is more complicated because it also takes in to account air resistance. We
do not live in a vacume.
Example 15.0.20 A lump of “blue ice” escapes the lavatory of a jet ﬂying at 600 miles
per hour at an altitude of 30,000 feet. This blue ice weighs 64 pounds near the earth and
experiences a force of air resistance equal to (−.1) r′ (t) pounds.
Find the position and
velocity of the blue ice as a function of time measured in seconds. Also ﬁnd the velocity
when the lump hits the ground.
Such lumps have been known to surprise people on the
ground.
The ﬁrst thing needed is to obtain information which involves consistent units. The blue
ice weighs 32 pounds near the earth. Thus 32 pounds is the force exerted by gravity on the
lump and so its mass must be given by Newton’s second law as follows.
64 = m × 32.
Thus m = 2 slugs. The slug is the unit of mass in the system involving feet and pounds.
The jet is ﬂying at 600 miles per hour. I want to change this to feet per second. Thus it
ﬂies at
600 × 5280
60 × 60
= 880 feet per second.
The explanation for this is that there are 5280 feet in a mile and so it goes 600×5280 feet
in one hour. There are 60 × 60 seconds in an hour. The position of the lump of blue ice will
be computed from a point on the ground directly beneath the airplane at the instant the
blue ice escapes and regard the airplane as moving in the direction of the positive x axis.
Thus the initial displacement is
r (0) = (0, 30000) feet
and the initial velocity is
r′ (0) = (880, 0) feet/sec.
The force of gravity is
(0, −64) pounds
and the force due to air resistance is
(−.1) r′ (t) pounds.
Newtons second law yields the following initial value problem for r (t) = (r1 (t) , r2 (t)) .
2 (r′′
1 (t) , r′′
2 (t)) = (−.1) (r′
1 (t) , r′
2 (t)) + (0, −64) , (r1 (0) , r2 (0)) = (0, 30000) ,
(r′
1 (0) , r′
2 (0)) = (880, 0)

276
NEWTON’S LAWS OF MOTION∗
Therefore,
2r′′
1 (t) + (.1) r′
1 (t) = 0
2r′′
2 (t) + (.1) r′
2 (t) = −64
r1 (0) = 0
r2 (0) = 30000
r′
1 (0) = 880
r′
2 (0) = 0
(15.2)
To save on repetition solve
mr′′ + kr′ = c, r (0) = u, r′ (0) = v.
Divide the diﬀerential equation by m and get
r′′ + (k/m) r′ = c/m.
Now multiply both sides by e(k/m)t. You should check this gives
d
dt
³
e(k/m)tr′´
= (c/m) e(k/m)t
Therefore,
e(k/m)tr′ = 1
k e
k
m tc + C
and using the initial condition, v = c/k + C and so
r′ (t) = (c/k) + (v −(c/k)) e−k
m t
Now this implies
r (t) = (c/k) t −1
k me−k
m t ³
v −c
k
´
+ D
(15.3)
where D is a constant to be determined from the initial conditions. Thus
u = −m
k
³
v −c
k
´
+ D
and so
r (t) = (c/k) t −1
k me−k
m t ³
v −c
k
´
+
³
u + m
k
³
v −c
k
´´
.
Now apply this to the system 15.2 to ﬁnd
r1 (t) = −1
(.1)2
µ
exp
µ−(.1)
2
t
¶¶
(880) +
µ 2
(.1) (880)
¶
= −17600.0 exp (−.0 5t) + 17600.0
and
r2 (t) = (−64/ (.1)) t −
1
(.1)2
µ
exp
µ
−(.1)
2 t
¶¶ µ 64
(.1)
¶
+
µ
30000 +
2
(.1)
µ 64
(.1)
¶¶
= −640.0t −12800.0 exp (−.0 5t) + 42800.0
This gives the coordinates of the position. What of the velocity? Using 15.3 in the same
way to obtain the velocity,
r′
1 (t) = 880.0 exp (−.0 5t) ,
r′
2 (t) = −640.0 + 640.0 exp (−.0 5t) .
(15.4)

277
To determine the velocity when the blue ice hits the ground, it is necessary to ﬁnd the value
of t when this event takes place and then to use 15.4 to determine the velocity. It hits
ground when r2 (t) = 0. Thus it suﬃces to solve the equation,
0 = −640.0t −12800.0 exp (−.0 5t) + 42800.0.
This is a fairly hard equation to solve using the methods of algebra. In fact, I do not have
a good way to ﬁnd this value of t using algebra. However if plugging in various values of t
using a calculator you eventually ﬁnd that when t = 66.14,
−640.0 (66.14) −12800.0 exp (−.0 5 (66.14)) + 42800.0 = 1.588 feet.
This is close enough to hitting the ground and so plugging in this value for t yields the
approximate velocity,
(880.0 exp (−.0 5 (66.14)) , −640.0 + 640.0 exp (−.0 5 (66.14))) = (32. 23, −616. 56) .
Notice how because of air resistance the component of velocity in the horizontal direction
is only about 32 feet per second even though this component started out at 880 feet per
second while the component in the vertical direction is -616 feet per second even though this
component started oﬀat 0 feet per second. You see that air resistance can be very important
so it is not enough to pretend, as is often done in beginning physics courses that everything
takes place in a vacuum. Actually, this problem used several physical simpliﬁcations. It was
assumed the force acting on the lump of blue ice by gravity was constant. This is not really
true because it actually depends on the distance between the center of mass of the earth and
the center of mass of the lump. It was also assumed the air resistance is proportional to the
velocity. This is an over simpliﬁcation when high speeds are involved. However, increasingly
correct models can be studied in a systematic way as above.
15.0.6
Kinetic Energy∗
Newton’s second law is also the basis for the notion of kinetic energy. When a force is
exerted on an object which causes the object to move, it follows that the force is doing work
which manifests itself in a change of velocity of the object. How is the total work done on
the object by the force related to the ﬁnal velocity of the object? By Newton’s second law,
and letting v be the velocity,
F (t) = mv′ (t) .
Now in a small increment of time, (t, t + dt) , the work done on the object would be approx-
imately equal to
dW = F (t) · v (t) dt.
(15.5)
If no work has been done at time t = 0,then 15.5 implies
dW
dt = F · v, W (0) = 0.
Hence,
dW
dt = mv′ (t) · v (t) = m
2
d
dt |v (t)|2 .
Therefore, the total work done up to time t would be W (t) = m
2 |v (t)|2 −m
2 |v0|2 where |v0|
denotes the initial speed of the object. This diﬀerence represents the change in the kinetic
energy.

278
NEWTON’S LAWS OF MOTION∗
15.0.7
Impulse And Momentum∗
Implulse
Work and energy involve a force acting on an object for some distance. Impulse involves a
force which acts on an object for an interval of time.
Deﬁnition 15.0.21 Let F be a force which acts on an object during the time in-
terval, [a, b] . The impulse of this force is
Z b
a
F (t) dt.
This is deﬁned as
ÃZ b
a
F1 (t) dt,
Z b
a
F2 (t) dt,
Z b
a
F3 (t) dt
!
.
The linear momentum of an object of mass m and velocity v is deﬁned as
Linear momentum = mv.
The notion of impulse and momentum are related in the following theorem.
Theorem 15.0.22 Let F be a force acting on an object of mass m. Then the
impulse equals the change in momentum. More precisely,
Z b
a
F (t) dt = mv (b) −mv (a) .
Proof: This is really just the fundamental theorem of calculus and Newton’s second law
applied to the components of F.
Z b
a
F (t) dt =
Z b
a
mdv
dt dt = mv (b) −mv (a)
(15.6)
15.0.8
Conservation Of Momentum∗
Now suppose two point masses, A and B collide. Newton’s third law says the force exerted
by mass A on mass B is equal in magnitude but opposite in direction to the force exerted by
mass B on mass A. Letting the collision take place in the time interval, [a, b] and denoting
the two masses by mA and mB and their velocities by vA and vB it follows that
mAvA (b) −mAvA (a) =
Z b
a
(Force of B on A) dt
and
mBvB (b) −mBvB (a)
=
Z b
a
(Force of A on B) dt
=
−
Z b
a
(Force of B on A) dt
=
−(mAvA (b) −mAvA (a))
and this shows
mBvB (b) + mAvA (b) = mBvB (a) + mAvA (a) .
In other words, in a collision between two masses the total linear momentum before the colli-
sion equals the total linear momentum after the collision. This is known as the conservation
of linear momentum. This law is why rockets work. Think about it.

279
15.0.9
Exercises With Answers
1. Show the solution to v′ + rv = c with the initial condition, v (0) = v0 is v (t) =
¡
v0 −c
r
¢
e−rt + (c/r) . If v is velocity and r = k/m where k is a constant for air
resistance and m is the mass, and c = f/m, argue from Newton’s second law that
this is the equation for ﬁnding the velocity, v of an object acted on by air resistance
proportional to the velocity and a constant force, f, possibly from gravity. Does there
exist a terminal velocity? What is it?
Multiply both sides of the diﬀerential equation by ert. Then the left side becomes
d
dt (ertv) = ertc. Now integrate both sides. This gives ertv (t) = C + ert
r c. You ﬁnish
the rest.
2. Suppose an object having mass equal to 5 kilograms experiences a time dependent
force, F (t) = e−ti + cos (t) j + t2k meters per sec2. Suppose also that the object is
at the point (0, 1, 1) meters at time t = 0 and that its initial velocity at this time is
v = i + j −k meters per sec. Find the position of the object as a function of t.
This is done by using Newton’s law.
Thus 5 d2r
dt2 = e−ti + cos (t) j + t2k and so
5 dr
dt = −e−ti + sin (t) j +
¡
t3/3
¢
k + C. Find the constant, C by using the given initial
velocity. Next do another integration obtaining another constant vector which will be
determined by using the given initial position of the object.
3. Fill in the details for the derivation of kinetic energy. In particular verify that mv′ (t)·
v (t) = m
2
d
dt |v (t)|2. Also, why would dW = F (t) · v (t) dt?
Remember |v|2 = v · v. Now use the product rule.
4. Suppose the force acting on an object, F is always perpendicular to the velocity of
the object. Thus F · v = 0. Show the Kinetic energy of the object is constant. Such
forces are sometimes called forces of constraint because they do not contribute to the
speed of the object, only its direction.
0 = F · v = mv′ · v. Explain why this is
d
dt
³
m 1
2 |v|2´
, the derivative of the kinetic
energy.

280
NEWTON’S LAWS OF MOTION∗

Physics Of Curvilinear Motion
12 Oct.
16.0.10
The Acceleration In Terms Of The Unit Tangent And Nor-
mal
A ﬂy buzzing around the room, a person riding a roller coaster, and a satellite orbiting the
earth all have something in common. They are moving over some sort of curve in three
dimensions.
Denote by R (t) the position vector of the point on the curve which occurs at time t.
Assume that R′, R′′ exist and is continuous. Thus R′ = v, the velocity and R′′ = a is the
acceleration.
¡
¡
¡
¡
s

1
R(t)
x
z
y
Lemma 16.0.23 Deﬁne T (t) ≡R′ (t) / |R′ (t)| . Then |T (t)| = 1 and if T′ (t) ̸= 0, then
there exists a unit vector, N (t) perpendicular to T (t) and a scalar valued function, κ (t) ,
with T′ (t) = κ (t) |v| N (t) .
Proof: It follows from the deﬁnition that |T| = 1. Therefore, T · T = 1 and so, upon
diﬀerentiating both sides,
T′ · T + T · T′ = 2T′ · T = 0.
Therefore, T′ is perpendicular to T. Let
N (t) ≡T′
|T′|.
Then letting |T′| ≡κ (t) |v (t)| , it follows
T′ (t) = κ (t) |v (t)| N (t) .
This proves the lemma.
281

282
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Deﬁnition 16.0.24 The vector, T (t) is called the unit tangent vector and the
vector, N (t) is called the principal normal.
The function, κ (t) in the above lemma
is called the curvature.
The radius of curvature is deﬁned as ρ = 1/κ.
The plane
determined by the two vectors, T and N is called the osculating1 plane. It identiﬁes a
particular plane which is in a sense tangent to this space curve.
The important thing about this is that it is possible to write the acceleration as the sum
of two vectors, one perpendicular to the direction of motion and the other in the direction
of motion.
Theorem 16.0.25 For R (t) the position vector of a space curve, the acceleration
is given by the formula
a
=
d |v|
dt T + κ |v|2 N
(16.1)
≡
aT T + aNN.
Furthermore, a2
T + a2
N = |a|2.
Proof:
a
=
dv
dt = d
dt (R′) = d
dt (|v| T)
=
d |v|
dt T + |v| T′
=
d |v|
dt T + |v|2 κN.
This proves the ﬁrst part.
For the second part,
|a|2
=
(aT T + aNN) · (aT T + aNN)
=
a2
T T · T + 2aNaT T · N + a2
NN · N
=
a2
T + a2
N
because T · N = 0. This proves the theorem.
Finally, it is well to point out that the curvature is a property of the curve itself, and
does not depend on the parameterization of the curve. If the curve is given by two diﬀerent
vector valued functions, R (t) and R (τ) , then from the formula above for the curvature,
κ (t) = |T′ (t)|
|v (t)| =
¯¯ dT
dτ
dτ
dt
¯¯
¯¯ dR
dτ
dτ
dt
¯¯ =
¯¯ dT
dτ
¯¯
¯¯ dR
dτ
¯¯ ≡κ (τ) .
From this, it is possible to give an important formula from physics. Suppose an object
orbits a point at constant speed, v. In the above notation, |v| = v. What is the centripetal
acceleration of this object? You may know from a physics class that the answer is v2/r
where r is the radius. This follows from the above quite easily. The parameterization of the
object which is as described is
R (t) =
³
r cos
³v
r t
´
, r sin
³v
r t
´´
.
1To osculate means to kiss. Thus this plane could be called the kissing plane. However, that does not
sound formal enough so we call it the osculating plane.

283
Therefore, T =
¡
−sin
¡ v
r t
¢
, cos
¡ v
r t
¢¢
and T′ =
¡
−v
r cos
¡ v
r t
¢
, −v
r sin
¡ v
r t
¢¢
. Thus,
κ = |T′ (t)| /v = 1
r .
I hope it is not surprising that the curvature of a circle of radius r is 1/r. It follows
a =dv
dt T + v2κN =v2
r N.
The vector, N points from the object toward the center of the circle because it is a positive
multiple of the vector,
¡
−v
r cos
¡ v
r t
¢
, −v
r sin
¡ v
r t
¢¢
.
Formula 16.1 also yields an easy way to ﬁnd the curvature. Take the cross product of
both sides with v, the velocity. Then
a × v
=
d |v|
dt T × v + |v|2 κN × v
=
d |v|
dt T × v + |v|3 κN × T
Now T and v have the same direction so the ﬁrst term on the right equals zero. Taking
the magnitude of both sides, and using the fact that N and T are two perpendicular unit
vectors,
|a × v| = |v|3 κ
and so
κ = |a × v|
|v|3
.
(16.2)
Example 16.0.26 Let R (t) =
¡
cos (t) , t, t2¢
for t ∈[0, 3] . Find the speed, velocity, curva-
ture, and write the acceleration in terms of normal and tangential components.
First of all v (t) = (−sin t, 1, 2t) and so the speed is given by
|v| =
q
sin2 (t) + 1 + 4t2.
Therefore,
aT = d
dt
µq
sin2 (t) + 1 + 4t2
¶
=
sin (t) cos (t) + 4t
p
(2 + 4t2 −cos2 t)
.
It remains to ﬁnd aN. To do this, you can ﬁnd the curvature ﬁrst if you like.
a (t) = R′′ (t) = (−cos t, 0, 2) .
Then
κ
=
|(−cos t, 0, 2) × (−sin t, 1, 2t)|
µq
sin2 (t) + 1 + 4t2
¶3
=
q
4 + (−2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
µq
sin2 (t) + 1 + 4t2
¶3
Then
aN = κ |v|2

284
PHYSICS OF CURVILINEAR MOTION 12 OCT.
=
q
4 + (−2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
µq
sin2 (t) + 1 + 4t2
¶3
¡
sin2 (t) + 1 + 4t2¢
=
q
4 + (−2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
q
sin2 (t) + 1 + 4t2
.
You can observe the formula a2
N + a2
T = |a|2 holds. Indeed a2
N + a2
T =


q
4 + (−2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
q
sin2 (t) + 1 + 4t2


2
+
Ã
sin (t) cos (t) + 4t
p
(2 + 4t2 −cos2 t)
!2
=
4 + (−2 sin t + 2 (cos t) t)2 + cos2 t
sin2 t + 1 + 4t2
+ (sin t cos t + 4t)2
2 + 4t2 −cos2 t = cos2 t + 4 = |a|2
Some Simple Techniques
Recall the formula for acceleration is
a = aT T + aNN
(16.3)
where aT = d|v|
dt
and aN = κ |v|2 . Of course one way to ﬁnd aT and aN is to just ﬁnd
|v| , d|v|
dt and κ and plug in. However, there is another way which might be easier. Take the
dot product of both sides with T. This gives,
a · T = aT T · T + aNN · T = aT .
Thus
a = (a · T) T + aNN
and so
a −(a · T) T = aNN
(16.4)
and taking norms of both sides,
|a −(a · T) T| = aN.
Also from 16.4,
a −(a · T) T
|a −(a · T) T| = aNN
aN |N| = N.
Also recall
κ = |a × v|
|v|3
, a2
T + a2
N = |a|2
This is usually easier than computing T′/ |T′| . To illustrate the use of these simple obser-
vations, consider the example worked above which was fairly messy. I will make it easier by
selecting a value of t.
Example 16.0.27 Let R (t) =
¡
cos (t) , t, t2¢
for t ∈[0, 3] . Find the speed, velocity, curva-
ture, and write the acceleration in terms of normal and tangential components when t = 0.
Also ﬁnd N at the point where t = 0.

285
First I need to ﬁnd the velocity and acceleration. Thus
v = (−sin t, 1, 2t) , a = (−cos t, 0, 2)
and consequently,
T =
(−sin t, 1, 2t)
q
sin2 (t) + 1 + 4t2
.
When t = 0, this reduces to
v (0) = (0, 1, 0) , a = (−1, 0, 2) , |v (0)| = 1, T = (0, 1, 0) ,
and consequently,
T = (0, 1, 0) .
Then the tangential component of acceleration when t = 0 is
aT = (−1, 0, 2) · (0, 1, 0) = 0
Now |a|2 = 5 and so aN =
√
5 because a2
T + a2
N = |a|2. Thus
√
5 = κ |v (0)|2 = κ · 1 = κ.
Next lets ﬁnd N. From a = aT T + aNN it follows
(−1, 0, 2) = 0 · T +
√
5N
and so
N =
1
√
5 (−1, 0, 2) .
This was pretty easy.
Example 16.0.28 Find a formula for the curvature of the curve given by the graph of
y = f (x) for x ∈[a, b] . Assume whatever you like about smoothness of f.
You need to write this as a parametric curve. This is most easily accomplished by letting
t = x. Thus a parameterization is
(t, f (t) , 0) : t ∈[a, b] .
Then you can use the formula given above. The acceleration is (0, f ′′ (t) , 0) and the velocity
is (1, f ′ (t) , 0) . Therefore,
a × v = (0, f ′′ (t) , 0) × (1, f ′ (t) , 0) = (0, 0, −f ′′ (t)) .
Therefore, the curvature is given by
|a × v|
|v|3
=
|f ′′ (t)|
³
1 + f ′ (t)2´3/2 .
Sometimes curves don’t come to you parametrically. This is unfortunate when it occurs
but you can sometimes ﬁnd a parametric description of such curves. It should be emphasized
that it is only sometimes when you can actually ﬁnd a parameterization. General systems
of nonlinear equations cannot be solved using algebra.
Example 16.0.29 Find a parameterization for the intersection of the surfaces y + 3z =
2x2 + 4 and y + 2z = x + 1.

286
PHYSICS OF CURVILINEAR MOTION 12 OCT.
You need to solve for x and y in terms of x. This yields
z = 2x2 −x + 3, y = −4x2 + 3x −5.
Therefore, letting t = x, the parameterization is (x, y, z) =
¡
t, −4t2 −5 + 3t, −t + 3 + 2t2¢
.
Example 16.0.30 Find a parameterization for the straight line joining (3, 2, 4) and (1, 10, 5) .
(x, y, z) = (3, 2, 4) + t (−2, 8, 1) = (3 −2t, 2 + 8t, 4 + t) where t ∈[0, 1] . Note where this
came from. The vector, (−2, 8, 1) is obtained from (1, 10, 5) −(3, 2, 4) . Now you should
check to see this works.
16.0.11
The Curvature Vector
The main item of interest for us is the scalar curvature deﬁned above. Recall this was given
by
κ = |T′|
|v| .
The curvature vector is nothing more than
κ ≡T′
|v|.
16.0.12
The Circle Of Curvature*
In addition to the osculating plane, you can consider something called the circle of curvature.
The idea is that near a point on the space curve, the space curve is like a circle. This circle
has radius equal to 1/κ, the radius of curvature, lies in the osculating plane, and its center
is located by moving a distance of 1/κ (radiius of curvature) along the line determined by
the point on the curve and the principle normal in the direction of the principle normal. It
is an attempt to ﬁnd the circle which best resembles the curve locally.
T
-
N
?
t
t
Here is an example to illustrate this fussy concept.
Example 16.0.31 Consider the curve having a parameterization, (cos (t) , sin (t) , et). Find
the circle of curvature at the point where t = π/4.

287
First ﬁnd the curvature and the two vectors, T, N. The vector, T (t) =(−sin t,cos t,et)
√
1+e2t
. At
the point of interest this is
T =
³
−
√
2
2 ,
√
2
2 , eπ/4´
√
1 + eπ/2
To ﬁnd N next appears to be painful. Therefore, I will ﬁrst ﬁnd the acceleration and then
use the formula for acceleration to dredge up N.
a =
¡
−cos t, −sin t, et¢
Thus the curvature is easy to ﬁnd.
κ
=
|(−cos t, −sin t, et) × (−sin t, cos t, et)|
¡√
1 + e2t¢3
=
¯¯¡
−(sin t) et −et cos t, −(sin t) et + et cos t, −cos2 t −sin2 t
¢¯¯
¡√
1 + e2t¢3
=
¡
2e2t + 1
¢1/2
¡√
1 + e2t¢3
It follows that at the point of interest,
Ã
−
√
2
2 , −
√
2
2 , eπ/4
!
=
Ã
−
√
2
2 ,
√
2
2 , eπ/4
!
+
¡
2eπ/2 + 1
¢1/2
³√
1 + eπ/2
´3
³
1 + eπ/2´
N
From this, you can ﬁnd the components of N without too much trouble.
−
√
2
2
= −
√
2
2 +
¡
2eπ/2 + 1
¢1/2
³√
1 + eπ/2
´3
³
1 + eπ/2´
N1
and so N1 = 0. Next,
−
√
2
2
=
√
2
2 +
¡
2eπ/2 + 1
¢1/2
³√
1 + eπ/2
´3
³
1 + eπ/2´
N2
and so
N2 = −
√
2
q¡
1 + eπ/2¢
q¡
2eπ/2 + 1
¢.
Finally,
eπ/4 = eπ/4 +
¡
2eπ/2 + 1
¢1/2
³√
1 + eπ/2
´3
³
1 + eπ/2´
N3
and so N3 = 0 also.
From this you can ﬁnd the location of the center of curvature. It is at the point
Ã√
2
2 ,
√
2
2 , eπ/4
!
+



¡
2eπ/2 + 1
¢1/2
³√
1 + eπ/2
´3



−1 
0, −
√
2
q¡
1 + eπ/2¢
q¡
2eπ/2 + 1
¢, 0

.

288
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Simplifying this yields
Ã
1
2
√
2, −1
2
√
22e
1
2 π + 1 + 2eπ
2e
1
2 π + 1
, e
1
4 π
!
for the center of curvature. The circle of curvature is the circle in the osculating plane which
has the above point as the center and radius equal to



¡
2eπ/2 + 1
¢1/2
³√
1 + eπ/2
´3



−1
=
1
r³
2e
1
2 π + 1
´
Ãr³
1 + e
1
2 π´!3
.
If you try the same problem for (cos (t) , sin (t) , t) , you may ﬁnd the computations much
simpler.
One can of course go on and on fussing about geometrical dodads of this sort. The
evolute is the locus of centers of curvatures. Imagine ﬁnding such a center of curvature as
above for each t and considering the resulting curve. This is the evolute. Then if you really
like to do this sort of thing, you could think about the evolute of the evolute. There are
sure to be some wonderful conclusions hidden in this procedure.
The signiﬁcant geometrical concepts are discussed in Section 16.1. These lead to the
Serrat Frenet formulas which cause some people who like this sort of thing to wax ecstatic
over their virtues. These formulas are indeed interesting, unlike the fussy stuﬀabove about
the circle of curvature. However, it is not required reading so skip it if you are not interested.
It is a system of diﬀerential equations which completely describes the geometry of the space
curve.
16.1
Geometry Of Space Curves∗
If you are interested in more on space curves, you should read this section. Otherwise,
procede to the exercises. Denote by R (s) the function which takes s to a point on this
curve where s is arc length. Thus R (s) equals the point on the curve which occurs when
you have traveled a distance of s along the curve from one end.
This is known as the
parameterization of the curve in terms of arc length.
Note also that it incorporates an
orientation on the curve because there are exactly two ends you could begin measuring
length from. In this section, assume anything about smoothness and continuity to make the
following manipulations valid. In particular, assume that R′ exists and is continuous.
Lemma 16.1.1 Deﬁne T (s) ≡R′ (s) . Then |T (s)| = 1 and if T′ (s) ̸= 0, then there
exists a unit vector, N (s) perpendicular to T (s) and a scalar valued function, κ (s) with
T′ (s) = κ (s) N (s) .
Proof: First, s =
R s
0 |R′ (r)| dr because of the deﬁnition of arc length.
Therefore,
from the fundamental theorem of calculus, 1 = |R′ (s)| = |T (s)| . Therefore, T · T = 1
and so upon diﬀerentiating this on both sides, yields T′ · T + T · T′ = 0 which shows
T · T′ = 0. Therefore, the vector, T′ is perpendicular to the vector, T. In case T′ (s) ̸= 0,
let N (s) =
T′(s)
|T′(s)| and so T′ (s) = |T′ (s)| N (s) , showing the scalar valued function is
κ (s) = |T′ (s)| . This proves the lemma.
The radius of curvature is deﬁned as ρ =
1
κ. Thus at points where there is a lot of
curvature, the radius of curvature is small and at points where the curvature is small, the
radius of curvature is large. The plane determined by the two vectors, T and N is called
the osculating plane. It identiﬁes a particular plane which is in a sense tangent to this space

16.1.
GEOMETRY OF SPACE CURVES∗
289
curve. In the case where |T′ (s)| = 0 near the point of interest, T (s) equals a constant and
so the space curve is a straight line which it would be supposed has no curvature. Also, the
principal normal is undeﬁned in this case. This makes sense because if there is no curving
going on, there is no special direction normal to the curve at such points which could be
distinguished from any other direction normal to the curve. In the case where |T′ (s)| = 0,
κ (s) = 0 and the radius of curvature would be considered inﬁnite.
Deﬁnition 16.1.2 The vector, T (s) is called the unit tangent vector and the vector,
N (s) is called the principal normal. The function, κ (s) in the above lemma is called
the curvature.When T′ (s) ̸= 0 so the principal normal is deﬁned, the vector, B (s) ≡
T (s) × N (s) is called the binormal.
The binormal is normal to the osculating plane and B′ tells how fast this vector changes.
Thus it measures the rate at which the curve twists.
Lemma 16.1.3 Let R (s) be a parameterization of a space curve with respect to arc
length and let the vectors, T, N, and B be as deﬁned above. Then B′ = T × N′ and there
exists a scalar function, τ (s) such that B′ = τN.
Proof: From the deﬁnition of B = T × N, and you can diﬀerentiate both sides and get
B′ = T′ × N + T × N′. Now recall that T′ is a multiple called curvature multiplied by N
so the vectors, T′ and N have the same direction and B′ = T × N′. Therefore, B′ is either
zero or is perpendicular to T. But also, from the deﬁnition of B, B is a unit vector and so
B (s) · B (s) = 0. Diﬀerentiating this,B′ (s) · B (s) + B (s) · B′ (s) = 0 showing that B′ is
perpendicular to B also. Therefore, B′ is a vector which is perpendicular to both vectors,
T and B and since this is in three dimensions, B′ must be some scalar multiple of N and it
is this multiple called τ. Thus B′ = τN as claimed.
Lets go over this last claim a little more. The following situation is obtained. There
are two vectors, T and B which are perpendicular to each other and both B′ and N are
perpendicular to these two vectors, hence perpendicular to the plane determined by them.
Therefore, B′ must be a multiple of N. Take a piece of paper, draw two unit vectors on it
which are perpendicular. Then you can see that any two vectors which are perpendicular
to this plane must be multiples of each other.
The scalar function, τ is called the torsion.
In case T′ = 0, none of this is deﬁned
because in this case there is not a well deﬁned osculating plane. The conclusion of the
following theorem is called the Serret Frenet formulas.
Theorem 16.1.4 (Serret Frenet) Let R (s) be the parameterization with respect to
arc length of a space curve and T (s) = R′ (s) is the unit tangent vector. Suppose |T′ (s)| ̸= 0
so the principal normal, N (s) =
T′(s)
|T′(s)| is deﬁned. The binormal is the vector B ≡T × N
so T, N, B forms a right handed system of unit vectors each of which is perpendicular to
every other. Then the following system of diﬀerential equations holds in R9.
B′ = τN, T′ = κN, N′ = −κT −τB
where κ is the curvature and is nonnegative and τ is the torsion.
Proof:
κ ≥0 because κ = |T′ (s)| . The ﬁrst two equations are already established. To
get the third, note that B × T = N which follows because T, N, B is given to form a right
handed system of unit vectors each perpendicular to the others. (Use your right hand.)
Now take the derivative of this expression. thus
N′
=
B′ × T + B × T′
=
τN × T+κB × N.

290
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Now recall again that T, N, B is a right hand system. Thus N × T = −B and B × N = −T.
This establishes the Frenet Serret formulas.
This is an important example of a system of diﬀerential equations in R9. It is a remarkable
result because it says that from knowledge of the two scalar functions, τ and κ, and initial
values for B, T, and N when s = 0 you can obtain the binormal, unit tangent, and principal
normal vectors.
It is just the solution of an initial value problem of the sort discussed
earlier. Having done this, you can reconstruct the entire space curve starting at some point,
R0 because R′ (s) = T (s) and so R (s) = R0 +
R s
0 T′ (r) dr.
The vectors, B, T, and N are vectors which are functions of position on the space curve.
Often, especially in applications, you deal with a space curve which is parameterized by a
function of t where t is time. Thus a value of t would correspond to a point on this curve and
you could let B (t) , T (t) , and N (t) be the binormal, unit tangent, and principal normal at
this point of the curve. The following example is typical.
Example 16.1.5 Given the circular helix, R (t) = (a cos t) i + (a sin t) j + (bt) k, ﬁnd the
arc length, s (t) ,the unit tangent vector, T (t) , the principal normal, N (t) , the binormal,
B (t) , the curvature, κ (t) , and the torsion, τ (t) . Here t ∈[0, T] .
The arc length is s (t) =
R t
0
¡√
a2 + b2¢
dr =
¡√
a2 + b2¢
t. Now the tangent vector is
obtained using the chain rule as
T
=
dR
ds = dR
dt
dt
ds =
1
√
a2 + b2 R′ (t)
=
1
√
a2 + b2 ((−a sin t) i + (a cos t) j + bk)
The principal normal:
dT
ds
=
dT
dt
dt
ds
=
1
a2 + b2 ((−a cos t) i + (−a sin t) j + 0k)
and so
N =dT
ds /
¯¯¯¯
dT
ds
¯¯¯¯ = −((cos t) i + (sin t) j)
The binormal:
B
=
1
√
a2 + b2
¯¯¯¯¯¯
i
j
k
−a sin t
a cos t
b
−cos t
−sin t
0
¯¯¯¯¯¯
=
1
√
a2 + b2 ((b sin t) i−b cos tj + ak)
Now the curvature, κ (t) =
¯¯ dT
ds
¯¯ =
r³
a cos t
a2+b2
´2
+
³
a sin t
a2+b2
´2
=
a
a2+b2 . Note the curvature
is constant in this example. The ﬁnal task is to ﬁnd the torsion. Recall that B′ = τN where
the derivative on B is taken with respect to arc length. Therefore, remembering that t is a
function of s,
B′ (s)
=
1
√
a2 + b2 ((b cos t) i+ (b sin t) j) dt
ds
=
1
a2 + b2 ((b cos t) i+ (b sin t) j)
=
τ (−(cos t) i −(sin t) j) = τN

16.2.
INDEPENDENCE OF PARAMETERIZATION∗
291
and it follows −b/
¡
a2 + b2¢
= τ.
An important application of the usefulness of these ideas involves the decomposition of
the acceleration in terms of these vectors of an object moving over a space curve.
Corollary 16.1.6 Let R (t) be a space curve and denote by v (t) the velocity, v (t) =
R′ (t) and let v (t) ≡|v (t)| denote the speed and let a (t) denote the acceleration. Then
v = vT and a = dv
dt T + κv2N.
Proof:
T = dR
ds = dR
dt
dt
ds = v dt
ds. Also, s =
R t
0 v (r) dr and so ds
dt = v which implies
dt
ds = 1
v. Therefore, T = v/v which implies v = vT as claimed.
Now the acceleration is just the derivative of the velocity and so by the Serrat Frenet
formulas,
a
=
dv
dt T + v dT
dt
=
dv
dt T + v dT
ds v = dv
dt T + v2κN
Note how this decomposes the acceleration into a component tangent to the curve and one
which is normal to it. Also note that from the above, v |T′| T′(t)
|T′| = v2κN and so |T′|
v
= κ
and N = T′(t)
|T′|
16.2
Independence Of Parameterization∗
This section is for those who want to really understand what is going on. If you are
content, do not read this section. It may upset you. However, if you do decide to read it,
you might learn something so there is some beneﬁt for the anguish you might endure in the
attempt.
Recall that if p (t) : t ∈[a, b] was a parameterization of a smooth curve, C, the length of
C is deﬁned as
Z b
a
|p′ (t)| dt
If some other parameterization were used to trace out C, would the same answer be obtained?
The answer is yes. This is indeed fortunate because the length of a curve should only depend
on the curve itself, not on some parameterization. To answer this question in a satisfactory
manner requires some hard calculus. To answer it even more satisfactorily, you need to
consider some very advanced mathematics involving something called Hausdorﬀmeasure.

292
PHYSICS OF CURVILINEAR MOTION 12 OCT.
16.2.1
Hard Calculus∗
Deﬁnition 16.2.1 A sequence {an}∞
n=1 converges to a,
lim
n→∞an = a or an →a
if and only if for every ε > 0 there exists nε such that whenever n ≥nε ,
|an −a| < ε.
In words the deﬁnition says that given any measure of closeness, ε, the terms of the
sequence are eventually all this close to a. Note the similarity with the concept of limit.
Here, the word “eventually” refers to n being suﬃciently large. The limit of a sequence, if
it exists, is unique.
Theorem 16.2.2 If limn→∞an = a and limn→∞an = a1 then a1 = a.
Proof: Suppose a1 ̸= a. Then let 0 < ε < |a1 −a| /2 in the deﬁnition of the limit. It
follows there exists nε such that if n ≥nε, then |an −a| < ε and |an −a1| < ε. Therefore,
for such n,
|a1 −a|
≤
|a1 −an| + |an −a|
<
ε + ε < |a1 −a| /2 + |a1 −a| /2 = |a1 −a| ,
a contradiction.
Deﬁnition 16.2.3 Let {an} be a sequence and let n1 < n2 < n3, · · · be any strictly
increasing list of integers such that n1 is at least as large as the ﬁrst index used to deﬁne
the sequence {an} . Then if bk ≡ank, {bk} is called a subsequence of {an} .
Theorem 16.2.4 Let {xn} be a sequence with limn→∞xn = x and let {xnk} be a
subsequence. Then limk→∞xnk = x.
Proof: Let ε > 0 be given. Then there exists nε such that if n > nε, then |xn −x| < ε.
Suppose k > nε. Then nk ≥k > nε and so
|xnk −x| < ε
showing limk→∞xnk = x as claimed.
There is a very useful way of thinking of continuity in terms of limits of sequences found
in the following theorem. In words, it says a function is continuous if it takes convergent
sequences to convergent sequences whenever possible.
Theorem 16.2.5 A function f : D (f) →R is continuous at x ∈D (f) if and only
if, whenever xn →x with xn ∈D (f) , it follows f (xn) →f (x) .
Proof: Suppose ﬁrst that f is continuous at x and let xn →x. Let ε > 0 be given. By
continuity, there exists δ > 0 such that if |y −x| < δ, then |f (x) −f (y)| < ε. However,
there exists nδ such that if n ≥nδ, then |xn −x| < δ and so for all n this large,
|f (x) −f (xn)| < ε
which shows f (xn) →f (x) .
Now suppose the condition about taking convergent sequences to convergent sequences
holds at x. Suppose f fails to be continuous at x. Then there exists ε > 0 and xn ∈D (f)
such that |x −xn| < 1
n , yet
|f (x) −f (xn)| ≥ε.
But this is clearly a contradiction because, although xn →x, f (xn) fails to converge to
f (x) . It follows f must be continuous after all. This proves the theorem.

16.2.
INDEPENDENCE OF PARAMETERIZATION∗
293
Deﬁnition 16.2.6 A set, K ⊆R is sequentially compact if whenever {an} ⊆K is
a sequence, there exists a subsequence, {ank} such that this subsequence converges to a point
of K.
The following theorem is part of a major advanced calculus theorem known as the Heine
Borel theorem.
Theorem 16.2.7 Every closed interval, [a, b] is sequentially compact.
Proof: Let {xn} ⊆[a, b] ≡I0. Consider the two intervals
£
a, a+b
2
¤
and
£ a+b
2 , b
¤
each of
which has length (b −a) /2. At least one of these intervals contains xn for inﬁnitely many
values of n. Call this interval I1. Now do for I1 what was done for I0. Split it in half and
let I2 be the interval which contains xn for inﬁnitely many values of n. Continue this way
obtaining a sequence of nested intervals I0 ⊇I1 ⊇I2 ⊇I3 · ·· where the length of In is
(b −a) /2n. Now pick n1 such that xn1 ∈I1, n2 such that n2 > n1 and xn2 ∈I2, n3 such
that n3 > n2 and xn3 ∈I3, etc.
(This can be done because in each case the intervals
contained xn for inﬁnitely many values of n.) By the nested interval lemma there exists a
point, c contained in all these intervals. Furthermore,
|xnk −c| < (b −a) 2−k
and so limk→∞xnk = c ∈[a, b] . This proves the theorem.
Lemma 16.2.8 Let φ : [a, b] →R be a continuous function and suppose φ is 1 −1 on
(a, b). Then φ is either strictly increasing or strictly decreasing on [a, b] . Furthermore, φ−1
is continuous.
Proof: First it is shown that φ is either strictly increasing or strictly decreasing on
(a, b) .
If φ is not strictly decreasing on (a, b), then there exists x1 < y1, x1, y1 ∈(a, b) such that
(φ (y1) −φ (x1)) (y1 −x1) > 0.
If for some other pair of points, x2 < y2 with x2, y2 ∈(a, b) , the above inequality does not
hold, then since φ is 1 −1,
(φ (y2) −φ (x2)) (y2 −x2) < 0.
Let xt ≡tx1 + (1 −t) x2 and yt ≡ty1 + (1 −t) y2. Then xt < yt for all t ∈[0, 1] because
tx1 ≤ty1 and (1 −t) x2 ≤(1 −t) y2
with strict inequality holding for at least one of these inequalities since not both t and (1 −t)
can equal zero. Now deﬁne
h (t) ≡(φ (yt) −φ (xt)) (yt −xt) .
Since h is continuous and h (0) < 0, while h (1) > 0, there exists t ∈(0, 1) such that
h (t) = 0. Therefore, both xt and yt are points of (a, b) and φ (yt) −φ (xt) = 0 contradicting
the assumption that φ is one to one. It follows φ is either strictly increasing or strictly
decreasing on (a, b) .
This property of being either strictly increasing or strictly decreasing on (a, b) carries
over to [a, b] by the continuity of φ. Suppose φ is strictly increasing on (a, b) , a similar

294
PHYSICS OF CURVILINEAR MOTION 12 OCT.
argument holding for φ strictly decreasing on (a, b) . If x > a, then pick y ∈(a, x) and from
the above, φ (y) < φ (x) . Now by continuity of φ at a,
φ (a) = lim
x→a+ φ (z) ≤φ (y) < φ (x) .
Therefore, φ (a) < φ (x) whenever x ∈(a, b) . Similarly φ (b) > φ (x) for all x ∈(a, b).
It only remains to verify φ−1 is continuous. Suppose then that sn →s where sn and s are
points of φ ([a, b]) . It is desired to verify that φ−1 (sn) →φ−1 (s) . If this does not happen,
there exists ε > 0 and a subsequence, still denoted by sn such that
¯¯φ−1 (sn) −φ−1 (s)
¯¯ ≥ε.
Using the sequential compactness of [a, b] there exists a further subsequence, still denoted
by n, such that φ−1 (sn) →t1 ∈[a, b] , t1 ̸= φ−1 (s) . Then by continuity of φ, it follows
sn →φ (t1) and so s = φ (t1) . Therefore, t1 = φ−1 (s) after all. This proves the lemma.
Corollary 16.2.9 Let f : (a, b) →R be one to one and continuous. Then f (a, b) is an
open interval, (c, d) and f −1 : (c, d) →(a, b) is continuous.
Proof: Since f is either strictly increasing or strictly decreasing, it follows that f (a, b) is
an open interval, (c, d) . Assume f is decreasing. Now let x ∈(a, b). Why is f −1 is continuous
at f (x)? Since f is decreasing, if f (x) < f (y) , then y ≡f −1 (f (y)) < x ≡f −1 (f (x)) and
so f −1 is also decreasing. Let ε > 0 be given. Let ε > η > 0 and (x −η, x + η) ⊆(a, b) .
Then f (x) ∈(f (x + η) , f (x −η)) . Let δ = min (f (x) −f (x + η) , f (x −η) −f (x)) . Then
if
|f (z) −f (x)| < δ,
it follows
z ≡f −1 (f (z)) ∈(x −η, x + η) ⊆(x −ε, x + ε)
so
¯¯f −1 (f (z)) −x
¯¯ =
¯¯f −1 (f (z)) −f −1 (f (x))
¯¯ < ε.
This proves the theorem in the case where f is strictly decreasing. The case where f is
increasing is similar.
Theorem 16.2.10 Let f : [a, b] →R be continuous and one to one.
Suppose
f ′ (x1) exists for some x1 ∈[a, b] and f ′ (x1) ̸= 0. Then
¡
f −1¢′ (f (x1)) exists and is given
by the formula,
¡
f −1¢′ (f (x1)) =
1
f ′(x1).
Proof: By Lemma 16.2.8 f is either strictly increasing or strictly decreasing and f −1 is
continuous on [a, b]. Therefore there exists η > 0 such that if 0 < |f (x1) −f (x)| < η, then
0 < |x1 −x| =
¯¯f −1 (f (x1)) −f −1 (f (x))
¯¯ < δ
where δ is small enough that for 0 < |x1 −x| < δ,
¯¯¯¯
x −x1
f (x) −f (x1) −
1
f ′ (x1)
¯¯¯¯ < ε.
It follows that if 0 < |f (x1) −f (x)| < η,
¯¯¯¯
f −1 (f (x)) −f −1 (f (x1))
f (x) −f (x1)
−
1
f ′ (x1)
¯¯¯¯ =
¯¯¯¯
x −x1
f (x) −f (x1) −
1
f ′ (x1)
¯¯¯¯ < ε
Therefore, since ε > 0 is arbitrary,
lim
y→f(x1)
f −1 (y) −f −1 (f (x1))
y −f (x1)
=
1
f ′ (x1)
and this proves the theorem.
The following obvious corollary comes from the above by not bothering with end points.

16.2.
INDEPENDENCE OF PARAMETERIZATION∗
295
Corollary 16.2.11 Let f : (a, b) →R be continuous and one to one. Suppose f ′ (x1)
exists for some x1 ∈(a, b) and f ′ (x1) ̸= 0. Then
¡
f −1¢′ (f (x1)) exists and is given by the
formula,
¡
f −1¢′ (f (x1)) =
1
f ′(x1).
This is one of those theorems which is very easy to remember if you neglect the diﬃcult
questions and simply focus on formal manipulations. Consider the following.
f −1 (f (x)) = x.
Now use the chain rule on both sides to write
¡
f −1¢′ (f (x)) f ′ (x) = 1,
and then divide both sides by f ′ (x) to obtain
¡
f −1¢′ (f (x)) =
1
f ′ (x).
Of course this gives the conclusion of the above theorem rather eﬀortlessly and it is formal
manipulations like this which aid in remembering formulas such as the one given in the
theorem.
16.2.2
Independence Of Parameterization∗
Here is the precise deﬁnition of what is meant by a smooth curve.
Deﬁnition 16.2.12 C is a smooth curve in Rn if there exists an interval, [a, b] ⊆
R and functions xi : [a, b] →R such that the following conditions hold
1. xi is continuous on [a, b] .
2. x′
i exists and is continuous and bounded on [a, b] , with x′
i (a) deﬁned as the derivative
from the right,
lim
h→0+
xi (a + h) −xi (a)
h
,
and x′
i (b) deﬁned similarly as the derivative from the left.
3. For p (t) ≡(x1 (t) , · · ·, xn (t)) , t →p (t) is one to one on (a, b) .
4. |p′ (t)| ≡
³Pn
i=1 |x′
i (t)|2´1/2
̸= 0 for all t ∈[a, b] .
5. C = ∪{(x1 (t) , · · ·, xn (t)) : t ∈[a, b]} .
The functions, xi (t) , deﬁned above are giving the coordinates of a point in Rn and the
list of these functions is called a parameterization for the smooth curve. Note the natural
direction of the interval also gives a direction for moving along the curve. Such a direction
is called an orientation. The integral is used to deﬁne what is meant by the length of such
a smooth curve. Consider such a smooth curve having parameterization (x1, · · ·, xn) .
Theorem 16.2.13 Let φ : [a, b] →[c, d] be one to one and suppose φ′ exists and is
continuous on [a, b] . Then if f is a continuous function deﬁned on [a, b] which is Riemann
integrable2,
Z d
c
f (s) ds =
Z b
a
f (φ (t))
¯¯φ′ (t)
¯¯ dt
2Recall that all continuous functions of this sort are Riemann integrable.

296
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Proof: Let F ′ (s) = f (s) . (For example, let F (s) =
R s
a f (r) dr.) Then the ﬁrst integral
equals F (d) −F (c) by the fundamental theorem of calculus. By Lemma 16.2.8, φ is either
strictly increasing or strictly decreasing. Suppose φ is strictly decreasing. Then φ (a) = d
and φ (b) = c. Therefore, φ′ ≤0 and the second integral equals
−
Z b
a
f (φ (t)) φ′ (t) dt =
Z a
b
d
dt (F (φ (t))) dt
= F (φ (a)) −F (φ (b)) = F (d) −F (c) .
The case when φ is increasing is similar but easier. This proves the theorem.
Lemma 16.2.14 Let f : [a, b] →C, g : [c, d] →C be parameterizations of a smooth curve
which satisfy conditions 1 - 5. Then φ (t) ≡g−1 ◦f (t) is 1 −1 on (a, b) , continuous on
[a, b] , and either strictly increasing or strictly decreasing on [a, b] .
Proof: It is obvious φ is 1 −1 on (a, b) from the conditions f and g satisfy. It only
remains to verify continuity on [a, b] because then the ﬁnal claim follows from Lemma 16.2.8.
If φ is not continuous on [a, b] , then there exists a sequence, {tn} ⊆[a, b] such that tn →t
but φ (tn) fails to converge to φ (t) . Therefore, for some ε > 0 there exists a subsequence,
still denoted by n such that |φ (tn) −φ (t)| ≥ε. Using the sequential compactness of [c, d] ,
(See Theorem 16.2.7 on Page 293.) there is a further subsequence, still denoted by n such
that {φ (tn)} converges to a point, s, of [c, d] which is not equal to φ (t) . Thus g−1◦f (tn) →s
and still tn →t. Therefore, the continuity of f and g imply f (tn) →g (s) and f (tn) →f (t) .
Therefore, g (s) = f (t) and so s = g−1 ◦f (t) = φ (t) , a contradiction. Therefore, φ is
continuous as claimed.
Theorem 16.2.15 The length of a smooth curve is not dependent on parameter-
ization.
Proof:
Let C be the curve and suppose f : [a, b] →C and g : [c, d] →C both satisfy
conditions 1 - 5. Is it true that
R b
a |f ′ (t)| dt =
R d
c |g′ (s)| ds?
Let φ (t) ≡g−1◦f (t) for t ∈[a, b]. Then by the above lemma φ is either strictly increasing
or strictly decreasing on [a, b] . Suppose for the sake of simplicity that it is strictly increasing.
The decreasing case is handled similarly.
Let s0 ∈φ ([a + δ, b −δ]) ⊂(c, d) . Then by assumption 4, g′
i (s0) ̸= 0 for some i. By
continuity of g′
i, it follows g′
i (s) ̸= 0 for all s ∈I where I is an open interval contained
in [c, d] which contains s0. It follows that on this interval, gi is either strictly increasing
or strictly decreasing. Therefore, J ≡gi (I) is also an open interval and you can deﬁne a
diﬀerentiable function, hi : J →I by
hi (gi (s)) = s.
This implies that for s ∈I,
h′
i (gi (s)) =
1
g′
i (s).
(16.5)
Now letting s = φ (t) for s ∈I, it follows t ∈J1, an open interval. Also, for s and t related
this way, f (t) = g (s) and so in particular, for s ∈I,
gi (s) = fi (t) .
Consequently,
s = hi (fi (t)) = φ (t)

16.3.
PRODUCT RULE FOR MATRICES∗
297
and so, for t ∈J1,
φ′ (t) = h′
i (fi (t)) f ′
i (t) = h′
i (gi (s)) f ′
i (t) =
f ′
i (t)
g′
i (φ (t))
(16.6)
which shows that φ′ exists and is continuous on J1, an open interval containing φ−1 (s0) .
Since s0 is arbitrary, this shows φ′ exists on [a + δ, b −δ] and is continuous there.
Now f (t) = g◦
¡
g−1 ◦f
¢
(t) = g (φ (t)) and it was just shown that φ′ is a continuous
function on [a −δ, b + δ] . It follows
f ′ (t) = g′ (φ (t)) φ′ (t)
and so, by Theorem 16.2.13,
Z φ(b−δ)
φ(a+δ)
|g′ (s)| ds
=
Z b−δ
a+δ
|g′ (φ (t))|
¯¯φ′ (t)
¯¯ dt
=
Z b−δ
a+δ
|f ′ (t)| dt.
Now using the continuity of φ, g′, and f ′ on [a, b] and letting δ →0+ in the above, yields
Z d
c
|g′ (s)| ds =
Z b
a
|f ′ (t)| dt
and this proves the theorem.
16.3
Product Rule For Matrices∗
Another kind of multiplication is matrix multiplication. Here is the concept of the product
rule extended to matrix multiplication.
Deﬁnition 16.3.1 Let A (t) be an m × n matrix. Say A (t) = (Aij (t)) . Suppose
also that Aij (t) is a diﬀerentiable function for all i, j. Then deﬁne A′ (t) ≡
¡
A′
ij (t)
¢
. That
is, A′ (t) is the matrix which consists of replacing each entry by its derivative. Such an m×n
matrix in which the entries are diﬀerentiable functions is called a diﬀerentiable matrix.
The next lemma is just a version of the product rule.
Lemma 16.3.2 Let A (t) be an m × n matrix and let B (t) be an n × p matrix with the
property that all the entries of these matrices are diﬀerentiable functions. Then
(A (t) B (t))′ = A′ (t) B (t) + A (t) B′ (t) .
Proof: (A (t) B (t))′ =
¡
C′
ij (t)
¢
where Cij (t) = Aik (t) Bkj (t) and the repeated index
summation convention is being used. Therefore,
C′
ij (t)
=
A′
ik (t) Bkj (t) + Aik (t) B′
kj (t)
=
(A′ (t) B (t))ij + (A (t) B′ (t))ij
=
(A′ (t) B (t) + A (t) B′ (t))ij
Therefore, the ijth entry of A (t) B (t) equals the ijth entry of A′ (t) B (t) + A (t) B′ (t) and
this proves the lemma.

298
PHYSICS OF CURVILINEAR MOTION 12 OCT.
16.4
Moving Coordinate Systems∗
Let i (t) , j (t) , k (t) be a right handed3 orthonormal basis of vectors for each t. It is assumed
these vectors are C1 functions of t. Letting the positive x axis extend in the direction of
i (t) , the positive y axis extend in the direction of j (t), and the positive z axis extend in the
direction of k (t) , yields a moving coordinate system. Now let u = (u1, u2, u3) ∈R3 and let
t0 be some reference time. For example you could let t0 = 0. Then deﬁne the components
of u with respect to these vectors, i, j, k at time t0 as
u ≡u1i (t0) + u2j (t0) + u3k (t0) .
Let u (t) be deﬁned as the vector which has the same components with respect to i, j, k but
at time t. Thus
u (t) ≡u1i (t) + u2j (t) + u3k (t) .
and the vector has changed although the components have not.
For example, this is exactly the situation in the case of apparently ﬁxed basis vectors
on the earth if u is a position vector from the given spot on the earth’s surface to a point
regarded as ﬁxed with the earth due to its keeping the same coordinates relative to coordinate
axes which are ﬁxed with the earth.
Now deﬁne a linear transformation Q (t) mapping R3 to R3 by
Q (t) u ≡u1i (t) + u2j (t) + u3k (t)
where
u ≡u1i (t0) + u2j (t0) + u3k (t0)
Thus letting v, u ∈R3 be vectors and α, β, scalars,
Q (t) (αu + βv) ≡(αu1 + βv1) i (t) + (αu2 + βv2) j (t) + (αu3 + βv3) k (t)
=
(αu1i (t) + αu2j (t) + αu3k (t)) + (βv1i (t) + βv2j (t) + βv3k (t))
=
α (u1i (t) + u2j (t) + u3k (t)) + β (v1i (t) + v2j (t) + v3k (t))
≡
αQ (t) u + βQ (t) v
showing that Q (t) is a linear transformation. Also, Q (t) preserves all distances because,
since the vectors, i (t) , j (t) , k (t) form an orthonormal set,
|Q (t) u| =
Ã 3
X
i=1
¡
ui¢2
!1/2
= |u| .
For simplicity, let
i (t) = e1 (t) , j (t) = e2 (t) , k (t) = e3 (t)
and
i (t0) = e1 (t0) , j (t0) = e2 (t0) , k (t0) = e3 (t0) .
Then using the repeated index summation convention,
u (t) = ujej (t) = ujej (t) · ei (t0) ei (t0)
3Recall that right handed implies i × j = k.

16.4.
MOVING COORDINATE SYSTEMS∗
299
and so with respect to the basis, i (t0) = e1 (t0) , j (t0) = e2 (t0) , k (t0) = e3 (t0) , the matrix
of Q (t) is
Qij (t) = ei (t0) · ej (t)
Recall this means you take a vector, u ∈R3 which is a list of the components of u with
respect to i (t0) , j (t0) , k (t0) and when you multiply by Q (t) you get the components of
u (t) with respect to i (t0) , j (t0) , k (t0) . I will refer to this matrix as Q (t) to save notation.
Lemma 16.4.1 Suppose Q (t) is a real, diﬀerentiable n × n matrix which preserves dis-
tances. Then Q (t) Q (t)T = Q (t)T Q (t) = I. Also, if u (t) ≡Q (t) u, then there exists a
vector, Ω(t) such that
u′ (t) = Ω(t) × u (t) .
Proof: Recall that (z · w) = 1
4
³
|z + w|2 −|z −w|2´
. Therefore,
(Q (t) u·Q (t) w)
=
1
4
³
|Q (t) (u + w)|2 −|Q (t) (u −w)|2´
=
1
4
³
|u + w|2 −|u −w|2´
=
(u · w) .
This implies
³
Q (t)T Q (t) u · w
´
= (u · w)
for all u, w. Therefore, Q (t)T Q (t) u = u and so Q (t)T Q (t) = Q (t) Q (t)T = I. This proves
the ﬁrst part of the lemma.
It follows from the product rule, Lemma 16.3.2 that
Q′ (t) Q (t)T + Q (t) Q′ (t)T = 0
and so
Q′ (t) Q (t)T = −
³
Q′ (t) Q (t)T ´T
.
(16.7)
From the deﬁnition, Q (t) u = u (t) ,
u′ (t) = Q′ (t) u =Q′ (t)
=u
z
}|
{
Q (t)T u (t).
Then writing the matrix of Q′ (t) Q (t)T with respect to i (t0) , j (t0) , k (t0) , it follows from
16.7 that the matrix of Q′ (t) Q (t)T is of the form


0
−ω3 (t)
ω2 (t)
ω3 (t)
0
−ω1 (t)
−ω2 (t)
ω1 (t)
0


for some time dependent scalars, ωi. Therefore,


u1
u2
u3


′
(t)
=


0
−ω3 (t)
ω2 (t)
ω3 (t)
0
−ω1 (t)
−ω2 (t)
ω1 (t)
0




u1
u2
u3

(t)
=


w2 (t) u3 (t) −w3 (t) u2 (t)
w3 (t) u1 (t) −w1 (t) u3 (t)
w1 (t) u2 (t) −w2 (t) u1 (t)



300
PHYSICS OF CURVILINEAR MOTION 12 OCT.
where the ui are the components of the vector u (t) in terms of the ﬁxed vectors i (t0) , j (t0) , k (t0) .
Therefore,
u′ (t) = Ω(t) ×u (t) = Q′ (t) Q (t)T u (t)
(16.8)
where
Ω(t) = ω1 (t) i (t0) +ω2 (t) j (t0) +ω3 (t) k (t0) .
because
Ω(t) × u (t) ≡
¯¯¯¯¯¯
i (t0)
j (t0)
k (t0)
w1
w2
w3
u1
u2
u3
¯¯¯¯¯¯
≡
i (t0) (w2u3 −w3u2) + j (t0) (w3u1 −w1u3) + k (t0) (w1u2 −w2u1) .
This proves the lemma and yields the existence part of the following theorem.
Theorem 16.4.2 Let i (t) , j (t) , k (t) be as described. Then there exists a unique
vector Ω(t) such that if u (t) is a vector whose components are constant with respect to
i (t) , j (t) , k (t) , then
u′ (t) = Ω(t) × u (t) .
Proof: It only remains to prove uniqueness. Suppose Ω1 also works. Then u (t) = Q (t) u
and so u′ (t) = Q′ (t) u and
Q′ (t) u = Ω×Q (t) u = Ω1×Q (t) u
for all u. Therefore,
(Ω−Ω1) ×Q (t) u = 0
for all u and since Q (t) is one to one and onto, this implies (Ω−Ω1) ×w = 0 for all w and
thus Ω−Ω1 = 0. This proves the theorem.
Deﬁnition 16.4.3 A rigid body in R3 has a moving coordinate system with the
property that for an observer on the rigid body, the vectors, i (t) , j (t) , k (t) are constant.
More generally, a vector u (t) is said to be ﬁxed with the body if to a person on the body, the
vector appears to have the same magnitude and same direction independent of t. Thus u (t)
is ﬁxed with the body if u (t) = u1i (t) + u2j (t) + u3k (t).
The following comes from the above discussion.
Theorem 16.4.4 Let B (t) be the set of points in three dimensions occupied by a
rigid body. Then there exists a vector Ω(t) such that whenever u (t) is ﬁxed with the rigid
body,
u′ (t) = Ω(t) × u (t) .

Part VII
Functions Of Many Variables
301


303
Outcomes
Functions of Several variables
A. Identify the domain and range of a function of several variables.
B. Represent a function of two variables by level curves or a function of three variables
by level surfaces.
C. Identify the characteristics of a function from its graph or from a graph of its level
curves (or level surfaces).
D. Represent combinations of multivariable functions algebraically.
Reading: Multivariable Calculus 2.1
Outcome Mapping:
A. 1
B. 2,7,8
C. 3,5,6
D. 9
Limits and Continuity
A. Describe a delta neighborhood of a point in 2- or 3-space.
B. Evaluate the limit of a function of several variables for a given approach or show that
it does not exists.
C. Determine whether a function is continuous at a given point. Interpret the deﬁnition
of continuity of a function of several variables graphically.
D. Determine whether a set in 2- or 3-space is open, closed or neither. Determine whether
a set is compact.
E. Recall and apply the Extreme Value Theorem.
Reading: Multivariable Calculus 2.2
Outcome Mapping:
A. F1
B. 1,2,3
C. 8,F4
D. F3,11,12,13
E. F2
Partial Derivatives
A. Interpret the deﬁnition of a partial derivative of a function of two variables graphically.
B. Evaluate the partial derivatives of a function of several variables.

304
D. Evaluate the higher order partial derivatives of a function of several variables.
E. State the conditions under which mixed partial derivatives are equal.
F. Verify equations involving partial derivatives.
G. Evaluate the gradient of a function.
H. Prove identities involving the gradient.
Reading: Multivariable Calculus 2.3
Outcome Mapping:
A. G1,4,18
B. 3,5,6
D. 5,7
E. G3
F. 12,15
G. 9
H. 10

Functions Of Many Variables 16
Oct.
Quiz
1. Let r (t) = (cos (t) , sin (t) , 2t). Find a,aT , and aN. Also ﬁnd κ and write the acceler-
ation as the sum of two terms, one in the direction of the unit tangent vector and the
other in the direction of the principle normal. Find ⃗κ, the curvature vector which is a
completely useless concept.
2. Here is a matrix which happens to have −1 as an eigenvalue. Find the eigenspace
corresponding to this eigenvalue.


1
2
2
−2
−3
−2
2
2
1


Is the matrix defective or nondefective?
3. In Problem 2 ﬁnd the determinant of the matrix.
4. Raise the matrix of Problem 2 to the 15th power exactly.
17.1
The Graph Of A Function Of Two Variables
With vector valued functions of many variables, it doesn’t take long before it is impossible
to draw meaningful pictures.
This is because one needs more than three dimensions to
accomplish the task and we can only visualize things in three dimensions. Ultimately, one
of the main purposes of calculus is to free us from the tyranny of art. In calculus, we are
permitted and even required to think in a meaningful way about things which cannot be
drawn. However, it is certainly interesting to consider some things which can be visualized
and this will help to formulate and understand more general notions which make sense in
contexts which cannot be visualized. One of these is the concept of a scalar valued function
of two variables.
Let f (x, y) denote a scalar valued function of two variables evaluated at the point (x, y) .
Its graph consists of the set of points, (x, y, z) such that z = f (x, y) . How does one go about
depicting such a graph? The usual way is to ﬁx one of the variables, say x and consider
the function z = f (x, y) where y is allowed to vary and x is ﬁxed. Graphing this would
give a curve which lies in the surface to be depicted. Then do the same thing for other
values of x and the result would depict the graph desired graph. Computers do this very
305

306
FUNCTIONS OF MANY VARIABLES 16 OCT.
well. The following is the graph of the function z = cos (x) sin (2x + y) drawn using Maple,
a computer algebra system.1.
Notice how elaborate this picture is.
The lines in the drawing correspond to taking
one of the variables constant and graphing the curve which results. The computer did this
drawing in seconds but you couldn’t do it as well if you spent all day on it. I used a grid
consisting of 70 choices for x and 70 choices for y.
Sometimes attempts are made to understand three dimensional objects like the above
graph by looking at contour graphs in two dimensions. The contour graph of the above
three dimensional graph is below and comes from using the computer algebra system again.
–4
–2
0
2
4
y
–4
–2
2
4
x
This is in two dimensions and the diﬀerent lines in two dimensions correspond to points
on the three dimensional graph which have the same z value. If you have looked at a weather
map, these lines are called isotherms or isobars depending on whether the function involved is
temperature or pressure. In a contour geographic map, the contour lines represent constant
altitude. If many contour lines are close to each other, this indicates rapid change in the
altitude, temperature, pressure, or whatever else may be measured.
A scalar function of three variables, cannot be visualized because four dimensions are
required. However, some people like to try and visualize even these examples. This is done
by looking at level surfaces in R3 which are deﬁned as surfaces where the function assumes
a constant value. They play the role of contour lines for a function of two variables. As a
simple example, consider f (x, y, z) = x2 + y2 + z2. The level surfaces of this function would
be concentric spheres centered at 0. (Why?) Another way to visualize objects in higher
dimensions involves the use of color and animation. However, there really are limits to what
you can accomplish in this direction. So much for art.
However, the concept of level curves is quite useful because these can be drawn.
Example 17.1.1 Determine from a contour map where the function, f (x, y) = sin
¡
x2 + y2¢
is steepest.
1I used Maple and exported the graph as an eps. ﬁle which I then imported into this document.

17.2.
THE DOMAIN OF A FUNCTION
307
–3
–2
–1
1
2
3
y
–3
–2
–1
1
2
3
x
In the picture, the steepest places are where the contour lines are close together because
they correspond to various values of the function. You can look at the picture and see where
they are close and where they are far. This is the advantage of a contour map.
17.2
The Domain Of A Function
As usual the domain of a function is either speciﬁed or if it is unspeciﬁed, it is the set of
all points for which the function makes sense. If f is the name of the function its domain is
denoted as D(f).
Example 17.2.1 Find the domain of the function, f (x, y) =
p
1 −(x2 + y2).
You need to have 1 ≥x2+y2 and so the domain of this function is
©
(x, y) ∈R2 : x2 + y2 ≤1
ª
.
This is just the inside of the unit circle centered at (0, 0) . It also includes the edge of this
unit circle.
Sometimes the domain is given to you in a very artiﬁcial way.
Example 17.2.2 Let D (f) =
©
(x, y) ∈R2 : x2 + y2 ≤1
ª
∪(3, 7) . Let f (x, y) = x + 2y for
(x, y) ∈
©
(x, y) ∈R2 : x2 + y2 ≤1
ª
and let f (3, 7) = 33.
In this case, the domain of the function is as given above and the function is given the
deﬁnition just described.
Now remember from calculus of functions of one variable some of the things you did.
One of the most important was to consider the derivative of a function. Recall the deﬁnition
of the derivative, f ′ (x).
lim
y→x
f (y) −f (x)
y −x
≡f ′ (x) .
In order to write this deﬁnition you need to have f deﬁned for all values of y near x, That
is, you need to have f deﬁned on an open interval containing x of the form (x −δ, x + δ)
for some δ > 0. Otherwise, you can’t consider f (y) . This is one reason for the importance
of the concepts in the next section.
17.3
Open And Closed Sets
We are going to consider functions deﬁned on subsets of Rn and their properties. The next
deﬁnition will end up being quite important. It describe a type of subset of Rn with the
property that if x is in this set, then so is y whenever y is close enough to x. It is essential
you understand a few kinds of sets.

308
FUNCTIONS OF MANY VARIABLES 16 OCT.
Deﬁnition 17.3.1 Let x ∈Rn. Then B (x, r) , called the ball centered at x having
radius r is deﬁned to be the set of all points of Rn, y which have the property that these
points are closer than r to x. Thus y ∈B (x, r) means |y −x| < r. Written formally, this
is
B (x, r) ≡{y ∈Rn : |y −x| < r} .
To say that B (x, r) ⊆D (f) means that whenever y is closer to x than r, it follows
y ∈D (f) . Now recall this is the sort of thing which you must start with, even in one
dimension, to consider the concept of the derivative of a function.
Therefore, it is not
surprising that such an idea would be important in Rn.
Deﬁnition 17.3.2 Let U ⊆Rn. U is an open set if whenever x ∈U, there exists
r > 0 such that B (x, r) ⊆U. More generally, if U is any subset of Rn, x ∈U is an interior
point of U if there exists r > 0 such that x ∈B (x, r) ⊆U. In other words U is an open set
exactly when every point of U is an interior point of U.
If there is something called an open set, surely there should be something called a closed
set and here is the deﬁnition of one.
Deﬁnition 17.3.3 A subset, C, of Rn is called a closed set if Rn \ C is an open
set. They symbol, Rn \ C denotes everything in Rn which is not in C. It is also called the
complement of C. The symbol, SC is a short way of writing Rn \ S. A bounded set is one
which is contained in a large enough ball. In Rn a set which is both closed and bounded is
compact. 2
To illustrate this deﬁnition, consider the following picture.
qx
U
B(x, r)
You see in this picture how the edges are dotted. This is because an open set, can’t
include the edges or the set would fail to be open.
For example, consider what would
happen if you picked a point out on the edge of U in the above picture. Every open ball
centered at that point would have in it some points which are outside U. Therefore, such a
point would violate the above deﬁnition. You also see the edges of B (x, r) dotted suggesting
that B (x, r) ought to be an open set. This is intuitively clear but does require a proof. This
will be done in the next theorem and will give examples of open sets. Also, you can see that
if x is close to the edge of U, you might have to take r to be very small.
It is roughly the case that open sets don’t have their skins while closed sets do. So
why might it be important to consider closed sets? Remember from one variable calculus
the theorem which says that a continuous function achieves its maximum and minimum
on a closed interval. The closed interval contains its “skin”, the end points of the interval.
2Actually the term compact has independent meaning and there is a theorem called the Heine Borel
theorem which states that in Rn closed and bounded sets are compact. See the section on theory for more
on this. This is not just useless jargon and gratuitous terminology.

17.3.
OPEN AND CLOSED SETS
309
Similar theorems will end up holding for functions of n variables. Here is a picture of a
closed set, C.
B(x, r)
x
q
C
Note that x /∈C and since Rn \ C is open, there exists a ball, B (x, r) contained entirely
in Rn \C. If you look at Rn \C, what would be its skin? It can’t be in Rn \C and so it must
be in C. This is a rough heuristic explanation of what is going on with these deﬁnitions.
Also note that Rn and ∅are both open and closed. Here is why. If x ∈∅, then there must
be a ball centered at x which is also contained in ∅. This must be considered to be true
because there is nothing in ∅so there can be no example to show it false3. Therefore, from
the deﬁnition, it follows ∅is open. It is also closed because if x /∈∅, then B (x, 1) is also
contained in Rn \ ∅= Rn. Therefore, ∅is both open and closed. From this, it follows Rn is
also both open and closed.
Theorem 17.3.4 Let x ∈Rn and let r ≥0. Then B (x, r) is an open set. Also,
D (x, r) ≡{y ∈Rn : |y −x| ≤r}
is a closed set.
3To a mathematician, the statement: Whenever a pig is born with wings it can ﬂy must be taken as
true. We do not consider biological or aerodynamic considerations in such statements. There is no such
thing as a winged pig and therefore, all winged pigs must be superb ﬂyers since there can be no example of
one which is not. On the other hand we would also consider the statement: Whenever a pig is born with
wings it can’t possibly ﬂy, as equally true. The point is, you can say anything you want about the elements
of the empty set and no one can gainsay your statement. Therefore, such statements are considered as true
by default. You may say this is a very strange way of thinking about truth and ultimately this is because
mathematics is not about truth. It is more about consistency and logic.

310
FUNCTIONS OF MANY VARIABLES 16 OCT.
Proof: Suppose y ∈B (x,r) . It is necessary to show there exists r1 > 0 such that
B (y, r1) ⊆B (x, r) . Deﬁne r1 ≡r −|x −y| . Then if |z −y| < r1, it follows from the above
triangle inequality that
|z −x|
=
|z −y + y −x|
≤
|z −y| + |y −x|
<
r1 + |y −x| = r −|x −y| + |y −x| = r.
Note that if r = 0 then B (x, r) = ∅, the empty set. This is because if y ∈Rn, |x −y| ≥0
and so y /∈B (x, 0) . Since ∅has no points in it, it must be open because every point in it,
(There are none.) satisﬁes the desired property of being an interior point.
Now suppose y /∈D (x, r) . Then |x −y| > r and deﬁning δ ≡|x −y| −r, it follows that
if z ∈B (y, δ) , then by the triangle inequality,
|x −z|
≥
|x −y| −|y −z| > |x −y| −δ
=
|x −y| −(|x −y| −r) = r
and this shows that B (y, δ) ⊆Rn \D (x, r) . Since y was an arbitrary point in Rn \D (x, r) ,
it follows Rn\D (x, r) is an open set which shows from the deﬁnition that D (x, r) is a closed
set as claimed.
A picture which is descriptive of the conclusion of the above theorem which also implies
the manner of proof is the following.
y
x
q
q
6
-
r
r1
B(x, r)
y
x
q
q
6
-
r
r1
D(x, r)
Recall R2 consists of ordered pairs, (x, y) such that x ∈R and y ∈R. R2 is also written
as R × R. In general, the following deﬁnition holds.
Deﬁnition 17.3.5 The Cartesian product of two sets, A×B, means {(a, b) : a ∈A, b ∈B} .
If you have n sets, A1, A2, · · ·, An
n
Y
i=1
Ai = {(x1, x2, · · ·, xn) : each xi ∈Ai} .
Now suppose A ⊆Rm and B ⊆Rn. Then if (x, y) ∈A × B, x = (x1, · · ·, xm) and
y = (y1, · · ·, yn), the following identiﬁcation will be made.
(x, y) = (x1, · · ·, xm, y1, · · ·, yn) ∈Rn+m.
Similarly, starting with something in Rn+m, you can write it in the form (x, y) where x ∈Rm
and y ∈Rn. The following theorem has to do with the Cartesian product of two closed sets
or two open sets. Also here is an important deﬁnition.
Deﬁnition 17.3.6 A set, A ⊆Rn is said to be bounded if there exist ﬁnite inter-
vals, [ai, bi] such that
A ⊆
n
Y
i=1
[ai, bi] .

17.4.
CONTINUOUS FUNCTIONS
311
Theorem 17.3.7 Let U be an open set in Rm and let V be an open set in Rn.
Then U × V is an open set in Rn+m. If C is a closed set in Rm and H is a closed set in
Rn, then C × H is a closed set in Rn+m. If C and H are bounded, then so is C × H.
Proof: Let (x, y) ∈U × V. Since U is open, there exists r1 > 0 such that B (x, r1) ⊆U.
Similarly, there exists r2 > 0 such that B (y, r2) ⊆V . Now
B ((x, y) , δ) ≡


(s, t) ∈Rn+m :
m
X
k=1
|xk −sk|2 +
n
X
j=1
|yj −tj|2 < δ2



Therefore, if δ ≡min (r1, r2) and (s, t) ∈B ((x, y) , δ) , then it follows that s ∈B (x, r1) ⊆U
and that t ∈B (y, r2) ⊆V which shows that B ((x, y) , δ) ⊆U × V. Hence U × V is open as
claimed.
Next suppose (x, y) /∈C × H. It is necessary to show there exists δ > 0 such that
B ((x, y) , δ) ⊆Rn+m \ (C × H) . Either x /∈C or y /∈H since otherwise (x, y) would be a
point of C ×H. Suppose therefore, that x /∈C. Since C is closed, there exists r > 0 such that
B (x, r) ⊆Rm \C. Consider B ((x, y) , r) . If (s, t) ∈B ((x, y) , r) , it follows that s ∈B (x, r)
which is contained in Rm \ C. Therefore, B ((x, y) , r) ⊆Rn+m \ (C × H) showing C × H is
closed. A similar argument holds if y /∈H.
If C is bounded, there exist [ai, bi] such that C ⊆Qm
i=1 [ai, bi] and if H is bounded,
H ⊆Qm+n
i=m+1 [ai, bi] for intervals [am+1, bm+1] , · · ·, [am+n, bm+n] . Therefore, C × H ⊆
Qm+n
i=1 [ai, bi] and this establishes the last part of this theorem.
17.4
Continuous Functions
What was done in beginning calculus for scalar functions is generalized here to include the
case of a vector valued function of possibly many variables. What follows is the correct
deﬁnition of continuity. The one you are used to seeing in terms of the value of the function
corresponding to the value of its limit is not correct in general. This one you are used to
seeing is only correct if the point of the domain of the function is a limit point of the domain,
discussed brieﬂy later (Don’t worry about it too much. Just use the correct deﬁnition and
you will be ﬁne.). It isn’t a big deal for functions of one variables because you usually
are dealing with functions deﬁned on intervals and it happens that all the points are limit
points. In multiple dimensions, however, the earlier deﬁnition is woefully inadequate and
will lead you to profound confusion, confusion which is so severe you will have to relearn
everything you thought you understood. I know this from bitter personal experience.
Deﬁnition 17.4.1 A function f : D (f) ⊆Rp →Rq is continuous at x ∈D (f) if
for each ε > 0 there exists δ > 0 such that whenever y ∈D (f) and
|y −x| < δ
it follows that
|f (x) −f (y)| < ε.
f is continuous if it is continuous at every point of D (f) .
Note the total similarity to the scalar valued case.

312
FUNCTIONS OF MANY VARIABLES 16 OCT.
17.5
Suﬃcient Conditions For Continuity
The next theorem is a fundamental result which allows less worry about the ε δ deﬁnition
of continuity.
Theorem 17.5.1 The following assertions are valid
1. The function, af +bg is continuous at x when f, g are continuous at x ∈D (f)∩D (g)
and a, b ∈R.
2. If and f and g are each real valued functions continuous at x, then fg is continuous
at x. If, in addition to this, g (x) ̸= 0, then f/g is continuous at x.
3. If f is continuous at x, f (x) ∈D (g) ⊆Rp, and g is continuous at f (x) ,then g ◦f is
continuous at x.
4. If f = (f1, · · ·, fq) : D (f) →Rq, then f is continuous if and only if each fk is a
continuous real valued function.
5. The function f : Rp →R, given by f (x) = |x| is continuous.
The proof of this theorem is given later. Its conclusions are not surprising. For example
the ﬁrst claim says that (af + bg) (y) is close to (af + bg) (x) when y is close to x provided
the same can be said about f and g. For the second claim, if y is close to x, f (x) is close to
f (y) and so by continuity of g at f (x), g (f (y)) is close to g (f (x)) . To see the third claim
is likely, note that closeness in Rp is the same as closeness in each coordinate. The fourth
claim is immediate from the triangle inequality.
For functions deﬁned on Rn, there is a notion of polynomial just as there is for functions
deﬁned on R.
Deﬁnition 17.5.2 Let α be an n dimensional multi-index. This means
α = (α1, · · ·, αn)
where each αi is a natural number or zero. Also, let
|α| ≡
n
X
i=1
|αi|
The symbol, xαmeans
xα ≡xα1
1 xα2
2 · · · xαn
3 .
An n dimensional polynomial of degree m is a function of the form
p (x) =
X
|α|≤m
dαxα.
where the dα are real numbers.
The above theorem implies that polynomials are all continuous.

17.6.
PROPERTIES OF CONTINUOUS FUNCTIONS
313
17.6
Properties Of Continuous Functions
Functions of many variables have many of the same properties as functions of one variable.
First there is a version of the extreme value theorem generalizing the one dimensional case.
Theorem 17.6.1 Let C be closed and bounded and let f : C →R be continuous.
Then f achieves its maximum and its minimum on C. This means there exist, x1, x2 ∈C
such that for all x ∈C,
f (x1) ≤f (x) ≤f (x2) .
The above theorems are proved in an optional section.

314
FUNCTIONS OF MANY VARIABLES 16 OCT.

Limits Of A Function 17-23 Oct.
Quiz
1. The position vector of an object is r (t) =
¡
et, sin (t) , t2 −1
¢
. Find the unit tangent
vector when t = 0.
2. Show that for v (t) a vector valued function
d
dt |v (t)| = v′·v
|v| . (Note that in the case
where v is velocity, this implies a · T = d
dt |v| .)
3. Suppose r (t) =
¡
t2, cos (t) , sin (t)
¢
. Find the curvature when t = 0.
4. Suppose r (t) =
¡
2t1/2, 2
3t3/2,
√
2t
¢
for t ∈[1, 2] . Find the length of this curve.
5. Find the matrix of the linear transformation which projects all vectors onto the line
y = x.
As in the case of scalar valued functions of one variable, a concept closely related to
continuity is that of the limit of a function. The notion of limit of a function makes sense
at points, x, which are limit points of D (f) and this concept is deﬁned next. It is a harder
concept than the concept of continuity.
Deﬁnition 18.0.2 Let A ⊆Rm be a set. A point, x, is a limit point of A if B (x, r)
contains inﬁnitely many points of A for every r > 0.
Deﬁnition 18.0.3 Let f : D (f) ⊆Rp →Rq be a function and let x be a limit
point of D (f) . Then
lim
y→x f (y) = L
if and only if the following condition holds. For all ε > 0 there exists δ > 0 such that if
0 < |y −x| < δ, and y ∈D (f)
then,
|L −f (y)| < ε.
Theorem 18.0.4 If limy→x f (y) = L and limy→x f (y) = L1, then L = L1.
Proof: Let ε > 0 be given.
There exists δ > 0 such that if 0 < |y −x| < δ and
y ∈D (f) , then
|f (y) −L| < ε, |f (y) −L1| < ε.
Pick such a y. There exists one because x is a limit point of D (f) . Then
|L −L1| ≤|L −f (y)| + |f (y) −L1| < ε + ε = 2ε.
315

316
LIMITS OF A FUNCTION 17-23 OCT.
Since ε > 0 was arbitrary, this shows L = L1.
As in the case of functions of one variable, one can deﬁne what it means for limy→x f (x) =
±∞.
Deﬁnition 18.0.5 If f (x) ∈R, limy→x f (x) = ∞if for every number l, there
exists δ > 0 such that whenever |y −x| < δ and y ∈D (f) , then f (x) > l.
The following theorem is just like the one variable version of calculus.
Theorem 18.0.6 Suppose limy→x f (y) = L and limy→x g (y) = K where K, L ∈
Rq. Then if a, b ∈R,
lim
y→x (af (y) + bg (y)) = aL + bK,
(18.1)
lim
y→x f · g (y) = L · K
(18.2)
and if g is scalar valued with limy→x g (y) = K ̸= 0,
lim
y→x f (y) g (y) = LK.
(18.3)
Also, if h is a continuous function deﬁned near L, then
lim
y→x h ◦f (y) = h (L) .
(18.4)
Suppose limy→x f (y) = L. If |f (y) −b| ≤r for all y suﬃciently close to x, then |L −b| ≤r
also.
Proof: The proof of 18.1 is left for you. It is like a corresponding theorem for continuous
functions. Now 18.2is to be veriﬁed. Let ε > 0 be given. Then by the triangle inequality,
|f · g (y) −L · K| ≤|fg (y) −f (y) · K| + |f (y) · K −L · K|
≤|f (y)| |g (y) −K| + |K| |f (y) −L| .
There exists δ1 such that if 0 < |y −x| < δ1 and y ∈D (f) , then
|f (y) −L| < 1,
and so for such y, the triangle inequality implies, |f (y)| < 1 + |L| . Therefore, for 0 <
|y −x| < δ1,
|f · g (y) −L · K| ≤(1 + |K| + |L|) [|g (y) −K| + |f (y) −L|] .
(18.5)
Now let 0 < δ2 be such that if y ∈D (f) and 0 < |x −y| < δ2,
|f (y) −L| <
ε
2 (1 + |K| + |L|), |g (y) −K| <
ε
2 (1 + |K| + |L|).
Then letting 0 < δ ≤min (δ1, δ2) , it follows from 18.5 that
|f · g (y) −L · K| < ε
and this proves 18.2.
The proof of 18.3 is left to you.
Consider 18.4. Since h is continuous near L, it follows that for ε > 0 given, there exists
η > 0 such that if |y −L| < η, then
|h (y) −h (L)| < ε

317
Now since limy→x f (y) = L, there exists δ > 0 such that if 0 < |y −x| < δ, then
|f (y) −L| < η.
Therefore, if 0 < |y −x| < δ,
|h (f (y)) −h (L)| < ε.
It only remains to verify the last assertion. Assume |f (y) −b| ≤r. It is required to show
that |L −b| ≤r. If this is not true, then |L −b| > r. Consider B (L, |L −b| −r) . Since L
is the limit of f, it follows f (y) ∈B (L, |L −b| −r) whenever y ∈D (f) is close enough to
x. Thus, by the triangle inequality,
|f (y) −L| < |L −b| −r
and so
r
<
|L −b| −|f (y) −L| ≤||b −L| −|f (y) −L||
≤
|b −f (y)| ,
a contradiction to the assumption that |b −f (y)| ≤r.
The next theorem gives the correct relation between continuity and the limit.
Theorem 18.0.7 For f : D (f) →Rq and x ∈D (f) a limit point of D (f) , f is
continuous at x if and only if
lim
y→x f (y) = f (x) .
Proof: First suppose f is continuous at x a limit point of D (f) . Then for every ε > 0
there exists δ > 0 such that if |y −x| < δ and y ∈D (f) , then |f (x) −f (y)| < ε. In
particular, this holds if 0 < |x −y| < δ and this is just the deﬁnition of the limit. Hence
f (x) = limy→x f (y) .
Next suppose x is a limit point of D (f) and limy→x f (y) = f (x) . This means that if ε > 0
there exists δ > 0 such that for 0 < |x −y| < δ and y ∈D (f) , it follows |f (y) −f (x)| < ε.
However, if y = x, then |f (y) −f (x)| = |f (x) −f (x)| = 0 and so whenever y ∈D (f) and
|x −y| < δ, it follows |f (x) −f (y)| < ε, showing f is continuous at x.
The following theorem is important.
Theorem 18.0.8 Suppose f : D (f) →Rq. Then for x a limit point of D (f) ,
lim
y→x f (y) = L
(18.6)
if and only if
lim
y→x fk (y) = Lk
(18.7)
where f (y) ≡(f1 (y) , · · ·, fp (y)) and L ≡(L1, · · ·, Lp) .
In the case where q = 3 and limy→x f (y) = L and limy→x g (y) = K, then
lim
y→x f (y) × g (y) = L × K.
(18.8)
Proof: Suppose 18.6.
Then letting ε > 0 be given there exists δ > 0 such that if
0 < |y −x| < δ, it follows
|fk (y) −Lk| ≤|f (y) −L| < ε
which veriﬁes 18.7.

318
LIMITS OF A FUNCTION 17-23 OCT.
Now suppose 18.7 holds.
Then letting ε > 0 be given, there exists δk such that if
0 < |y −x| < δk, then
|fk (y) −Lk| <
ε
√p.
Let 0 < δ < min (δ1, · · ·, δp) . Then if 0 < |y −x| < δ, it follows
|f (y) −L|
=
Ã p
X
k=1
|fk (y) −Lk|2
!1/2
<
Ã p
X
k=1
ε2
p
!1/2
= ε.
It remains to verify 18.8. But from the ﬁrst part of this theorem and the description of the
cross product presented earlier in terms of the permutation symbol,
lim
y→x (f (y) × g (y))i
=
lim
y→x εijkfj (y) gk (y)
=
εijkLjKk = (L × K)i .
If you did not read about the permutation symbol, you can simply write out the cross
product and observe that the desired limit holds for each component. Therefore, from the
ﬁrst part of this theorem, this establishes 18.8. This completes the proof.
Example 18.0.9 Find lim(x,y)→(3,1)
³
x2−9
x−3 , y
´
.
It is clear that lim(x,y)→(3,1)
x2−9
x−3
= 6 and lim(x,y)→(3,1) y = 1. Therefore, this limit
equals (6, 1) .
Example 18.0.10 Find lim(x,y)→(0,0)
xy
x2+y2 .
First of all observe the domain of the function is R2 \ {(0, 0)} , every point in R2 except
the origin. Therefore, (0, 0) is a limit point of the domain of the function so it might make
sense to take a limit. However, just as in the case of a function of one variable, the limit
may not exist. In fact, this is the case here. To see this, take points on the line y = 0.
At these points, the value of the function equals 0. Now consider points on the line y = x
where the value of the function equals 1/2. Since arbitrarily close to (0, 0) there are points
where the function equals 1/2 and points where the function has the value 0, it follows there
can be no limit. Just take ε = 1/10 for example. You can’t be within 1/10 of 1/2 and also
within 1/10 of 0 at the same time.
Note it is necessary to rely on the deﬁnition of the limit much more than in the case of
a function of one variable and there are no easy ways to do limit problems for functions of
more than one variable. It is what it is and you will not deal with these concepts without
suﬀering and anguish.
18.1
The Directional Derivative And Partial Deriva-
tives
18.1.1
The Directional Derivative
The directional derivative is just what its name suggests. It is the derivative of a function
in a particular direction. The following picture illustrates the situation in the case of a
function of two variables.

18.1.
THE DIRECTIONAL DERIVATIVE AND PARTIAL DERIVATIVES
319
©©©©©©©©
*
6
-
¡
¡
¡
¡
ª
x
z
y
v
(x0, y0)
z = f(x, y)
In this picture, v ≡(v1, v2) is a unit vector in the xy plane and x0 ≡(x0, y0) is a point in
the xy plane. When (x, y) moves in the direction of v, this results in a change in z = f (x, y)
as shown in the picture. The directional derivative in this direction is deﬁned as
lim
t→0
f (x0 + tv1, y0 + tv2) −f (x0, y0)
t
.
It tells how fast z is changing in this direction.
If you looked at it from the side, you
would be getting the slope of the indicated tangent line. A simple example of this is a
person climbing a mountain. He could go various directions, some steeper than others. The
directional derivative is just a measure of the steepness in a given direction. This motivates
the following general deﬁnition of the directional derivative.
Deﬁnition 18.1.1 Let f : U →R where U is an open set in Rn and let v be a unit
vector. For x ∈U, deﬁne the directional derivative of f in the direction, v, at the point
x as
Dvf (x) ≡lim
t→0
f (x + tv) −f (x)
t
.
Example 18.1.2 Find the directional derivative of the function, f (x, y) = x2y in the di-
rection of i + j at the point (1, 2) .
First you need a unit vector which has the same direction as the given vector. This unit
vector is v ≡
³
1
√
2,
1
√
2
´
. Then to ﬁnd the directional derivative from the deﬁnition, write the
diﬀerence quotient described above. Thus f (x + tv) =
³
1 +
t
√
2
´2 ³
2 +
t
√
2
´
and f (x) = 2.
Therefore,
f (x + tv) −f (x)
t
=
³
1 +
t
√
2
´2 ³
2 +
t
√
2
´
−2
t
,
and to ﬁnd the directional derivative, you take the limit of this as t →0. However, this
diﬀerence quotient equals 1
4
√
2
¡
10 + 4t
√
2 + t2¢
and so, letting t →0,
Dvf (1, 2) =
µ5
2
√
2
¶
.

320
LIMITS OF A FUNCTION 17-23 OCT.
There is something you must keep in mind about this. The direction vector must always
be a unit vector1.
18.1.2
Partial Derivatives
Quiz
1. Let r (t) =
¡
t2, cosh (t) + t, sin (t)
¢
. Find κ when t = 0. Remember κ is the curvature.
Also ﬁnd the normal and tangential components of acceleration and the oscullating
plane at the point where t = 0.
2. Suppose |r (t)| = 33 for all t. Show that r′ (t) · r (t) = 0. Does it follow that r′ (t) = 0?
3. Suppose r (t) =
¡
2t1/2, 2
3t3/2,
√
2t
¢
for t ∈[1, 2] . Find the length of this curve.
There are some special unit vectors which come to mind immediately. These are the
vectors, ei where
ei = (0, · · ·, 0, 1, 0, · · ·0)T
and the 1 is in the ith position.
Thus in case of a function of two variables, the directional derivative in the direction
i = e1 is the slope of the indicated straight line in the following picture.
y
z = f(x, y)
¡
¡
¡
¡
¡
x
s
¡
¡
¡
ª
e1
As in the case of a general directional derivative, you ﬁx y and take the derivative of
the function, x →f(x, y). More generally, even in situations which cannot be drawn, the
deﬁnition of a partial derivative is as follows.
1Actually, there is a more general formulation of the notion of directional derivative known as the Gateaux
derivative in which the length of v is not one but it is not considered here.

18.1.
THE DIRECTIONAL DERIVATIVE AND PARTIAL DERIVATIVES
321
Deﬁnition 18.1.3 Let U be an open subset of Rn and let f : U →R. Then letting
x = (x1, · · ·, xn)T be a typical element of Rn,
∂f
∂xi
(x) ≡Deif (x) .
This is called the partial derivative of f. Thus,
∂f
∂xi
(x)
≡
lim
t→0
f (x+tei) −f (x)
t
=
lim
t→0
f (x1, · · ·, xi + t, · · ·xn) −f (x1, · · ·, xi, · · ·xn)
t
,
and to ﬁnd the partial derivative, diﬀerentiate with respect to the variable of interest and
regard all the others as constants. Other notation for this partial derivative is fxi, f,i, or
Dif. If y = f (x) , the partial derivative of f with respect to xi may also be denoted by
∂y
∂xi
or yxi.
Example 18.1.4 Find ∂f
∂x, ∂f
∂y , and ∂f
∂z if f (x, y) = y sin x + x2y + z.
From the deﬁnition above, ∂f
∂x = y cos x+2xy, ∂f
∂y = sin x+x2, and ∂f
∂z = 1. Having taken
one partial derivative, there is no reason to stop doing it. Thus, one could take the partial
derivative with respect to y of the partial derivative with respect to x, denoted by
∂2f
∂y∂x or
fxy. In the above example,
∂2f
∂y∂x = fxy = cos x + 2x.
Also observe that
∂2f
∂x∂y = fyx = cos x + 2x.
Higher order partial derivatives are deﬁned by analogy to the above. Thus in the above
example,
fyxx = −sin x + 2.
These partial derivatives, fxy are called mixed partial derivatives.
There is an interesting relationship between the directional derivatives and the partial
derivatives, provided the partial derivatives exist and are continuous.
Deﬁnition 18.1.5 Suppose f : U ⊆Rn →R where U is an open set and the
partial derivatives of f all exist and are continuous on U. Under these conditions, deﬁne the
gradient of f denoted ∇f (x) to be the vector
∇f (x) = (fx1 (x) , fx2 (x) , · · ·, fxn (x))T .
Proposition 18.1.6 In the situation of Deﬁnition 18.1.5 and for v a unit vector,
Dvf (x) = ∇f (x) · v.
This proposition will be proved in a more general setting later. For now, you can use it
to compute directional derivatives.
Example 18.1.7 Find the directional derivative of the function, f (x, y) = sin
¡
2x2 + y3¢
at (1, 1) in the direction
³
1
√
2,
1
√
2
´T
.

322
LIMITS OF A FUNCTION 17-23 OCT.
First ﬁnd the gradient.
∇f (x, y) =
¡
4x cos
¡
2x2 + y3¢
, 3y2 cos
¡
2x2 + y3¢¢T .
Therefore,
∇f (1, 1) = (4 cos (3) , 3 cos (3))T
The directional derivative is therefore,
(4 cos (3) , 3 cos (3))T ·
µ 1
√
2, 1
√
2
¶T
= 7
2 (cos 3)
√
2.
Another important observation is that the gradient gives the direction in which the function
changes most rapidly.
Proposition 18.1.8 In the situation of Deﬁnition 18.1.5, suppose ∇f (x) ̸= 0. Then
the direction in which f increases most rapidly, that is the direction in which the directional
derivative is largest, is the direction of the gradient.
Thus v = ∇f (x) / |∇f (x)| is the
unit vector which maximizes Dvf (x) and this maximum value is |∇f (x)| . Similarly, v =
−∇f (x) / |∇f (x)| is the unit vector which minimizes Dvf (x) and this minimum value is
−|∇f (x)| .
Proof: Let v be any unit vector. Then from Proposition 18.1.6,
Dvf (x) = ∇f (x) · v = |∇f (x)| |v| cos θ = |∇f (x)| cos θ
where θ is the included angle between these two vectors, ∇f (x) and v. Therefore, Dvf (x)
is maximized when cos θ = 1 and minimized when cos θ = −1. The ﬁrst case corresonds to
the angle between the two vectors being 0 which requires they point in the same direction
in which case, it must be that v = ∇f (x) / |∇f (x)| and Dvf (x) = |∇f (x)| . The second
case occurs when θ is π and in this case the two vectors point in opposite directions and the
directional derivative equals −|∇f (x)| .
The concept of a directional derivative for a vector valued function is also easy
to deﬁne although the geometric signiﬁcance expressed in pictures is not.
Deﬁnition 18.1.9 Let f : U →Rp where U is an open set in Rn and let v be a
unit vector. For x ∈U, deﬁne the directional derivative of f in the direction, v, at the point
x as
Dvf (x) ≡lim
t→0
f (x + tv) −f (x)
t
.
Example 18.1.10 Let f (x, y) =
¡
xy2, yx
¢T . Find the directional derivative in the direction
(1, 2)T at the point (x, y) .
First, a unit vector in this direction is
¡
1/
√
5, 2/
√
5
¢T and from the deﬁnition, the desired
limit is
lim
t→0
³¡
x + t
¡
1/
√
5
¢¢ ¡
y + t
¡
2/
√
5
¢¢2 −xy2,
¡
x + t
¡
1/
√
5
¢¢ ¡
y + t
¡
2/
√
5
¢¢
−xy
´
t
=
lim
t→0
µ4
5xy
√
5 + 4
5xt + 1
5
√
5y2 + 4
5ty + 4
25t2√
5, 2
5x
√
5 + 1
5y
√
5 + 2
5t
¶
=
µ4
5xy
√
5 + 1
5
√
5y2, 2
5x
√
5 + 1
5y
√
5
¶
.

18.1.
THE DIRECTIONAL DERIVATIVE AND PARTIAL DERIVATIVES
323
You see from this example and the above deﬁnition that all you have to do is to form
the vector which is obtained by replacing each component of the vector with its directional
derivative. In particular, you can take partial derivatives of vector valued functions and use
the same notation.
Example 18.1.11 Find the partial derivative with respect to x of the function f (x, y, z, w) =
¡
xy2, z sin (xy) , z3x
¢T .
From the above deﬁnition, fx (x, y, z) = D1f (x, y, z) =
¡
y2, zy cos (xy) , z3¢T .
Example 18.1.12 Let f, g be two functions deﬁned on an open subset of R3 which have
partial derivatives. Find a formula for ∇(fg) .
This equals
³
(fg)x , (fg)y , (fg)z
´
=
(fxg + fgx, fyg + fgy, fzg + fgz)
=
g (fx, fy, fz) + f (gx, gy, gz) = g∇f + f∇g
Example 18.1.13 Let f, g be functions and a, b be scalars, you should verify that ∇(af + bg) =
a∇f + b∇g.
Example 18.1.14 Let h (x, y) =
½
1 if x ≥0
0 if x < 0 . Find ∂h
∂x and ∂h
∂y .
If x > 0 or if x < 0, both partial derivatives exist and equal 0. What of points like (0, y)?
∂h
∂x (0, y) does not exist but
∂h
∂y (0, y) ≡lim
t→0
h (0, y + t) −h (0, 0)
t
= lim
t→0
1 −1
t
= 0.
Do not be afraid to use the deﬁnition of the partial derivatives. Sometimes it is the only
way to ﬁnd the partial derivative.
Example 18.1.15 Let u (x, y) = ln
¡
x2 + y2¢
. Find uxx + uyy.
First ﬁnd ux. This equals 2
x
x2+y2 . Next ﬁnd uxx. This involves taking the partial deriva-
tive of ux. Thus it equals
2 y2 −x2
(x2 + y2)2
Similarly uyy = 2
x2−y2
(x2+y2)2 and so uxx + uyy = 0. Of course this assumes (x, y) ̸= (0, 0).
18.1.3
Mixed Partial Derivatives
Under certain conditions the mixed partial derivatives will always be equal. The simple
condition is that if they exist and are continuous, then they are equal. This astonishing
fact is due to Euler in 1734. For reasons I cannot understand, calculus books hardly ever
include a proof of this important result. It is not all that hard. Here it is.
Theorem 18.1.16 Suppose f : U ⊆R2 →R
where U is an open set on which
fx, fy, fxy and fyx exist. Then if fxy and fyx are continuous at the point (x, y) ∈U, it
follows
fxy (x, y) = fyx (x, y) .

324
LIMITS OF A FUNCTION 17-23 OCT.
Proof: Since U is open, there exists r > 0 such that B ((x, y) , r) ⊆U. Now let |t| , |s| <
r/2 and consider
∆(s, t) ≡1
st{
h(t)
z
}|
{
f (x + t, y + s) −f (x + t, y) −
h(0)
z
}|
{
(f (x, y + s) −f (x, y))}.
(18.9)
Note that (x + t, y + s) ∈U because
|(x + t, y + s) −(x, y)|
=
|(t, s)| =
¡
t2 + s2¢1/2
≤
µr2
4 + r2
4
¶1/2
=
r
√
2 < r.
As implied above, h (t) ≡f (x + t, y + s)−f (x + t, y). Therefore, by the mean value theorem
from calculus and the (one variable) chain rule,
∆(s, t)
=
1
st (h (t) −h (0)) = 1
sth′ (αt) t
=
1
s (fx (x + αt, y + s) −fx (x + αt, y))
for some α ∈(0, 1) . Applying the mean value theorem again,
∆(s, t) = fxy (x + αt, y + βs)
where α, β ∈(0, 1).
If the terms f (x + t, y) and f (x, y + s) are interchanged in 18.9, ∆(s, t) is also un-
changed and the above argument shows there exist γ, δ ∈(0, 1) such that
∆(s, t) = fyx (x + γt, y + δs) .
Letting (s, t) →(0, 0) and using the continuity of fxy and fyx at (x, y) ,
lim
(s,t)→(0,0) ∆(s, t) = fxy (x, y) = fyx (x, y) .
This proves the theorem.
The following is obtained from the above by simply ﬁxing all the variables except for the
two of interest.
Corollary 18.1.17 Suppose U is an open subset of Rn and f : U →R has the property
that for two indices, k, l, fxk, fxl, fxlxk, and fxkxl exist on U and fxkxl and fxlxk are both
continuous at x ∈U. Then fxkxl (x) = fxlxk (x) .
It is necessary to assume the mixed partial derivatives are continuous in order to assert
they are equal. The following is a well known example [3].
Example 18.1.18 Let
f (x, y) =
(
xy(x2−y2)
x2+y2
if (x, y) ̸= (0, 0)
0 if (x, y) = (0, 0)

18.2.
SOME FUNDAMENTALS∗
325
From the deﬁnition of partial derivatives it follows immediately that fx (0, 0) = fy (0, 0) =
0. Using the standard rules of diﬀerentiation, for (x, y) ̸= (0, 0) ,
fx = y x4 −y4 + 4x2y2
(x2 + y2)2
, fy = xx4 −y4 −4x2y2
(x2 + y2)2
Now
fxy (0, 0)
≡
lim
y→0
fx (0, y) −fx (0, 0)
y
=
lim
y→0
−y4
(y2)2 = −1
while
fyx (0, 0)
≡
lim
x→0
fy (x, 0) −fy (0, 0)
x
=
lim
x→0
x4
(x2)2 = 1
showing that although the mixed partial derivatives do exist at (0, 0) , they are not equal
there.
18.2
Some Fundamentals∗
This section contains the proofs of the theorems which were stated without proof along with
some other signiﬁcant topics which will be useful later. These topics are of fundamental
signiﬁcance but are diﬃcult. They are here to provide depth. If you want something more
than a superﬁcial knowledge, you should read this section. However, if you don’t want to
deal with challenging topics, don’t read this stuﬀ. Don’t even look at it.
Theorem 18.2.1 The following assertions are valid
1. The function, af +bg is continuous at x when f, g are continuous at x ∈D (f)∩D (g)
and a, b ∈R.
2. If and f and g are each real valued functions continuous at x, then fg is continuous
at x. If, in addition to this, g (x) ̸= 0, then f/g is continuous at x.
3. If f is continuous at x, f (x) ∈D (g) ⊆Rp, and g is continuous at f (x) ,then g ◦f is
continuous at x.
4. If f = (f1, · · ·, fq) : D (f) →Rq, then f is continuous if and only if each fk is a
continuous real valued function.
5. The function f : Rp →R, given by f (x) = |x| is continuous.
Proof: Begin with 1.) Let ε > 0 be given. By assumption, there exist δ1 > 0 such
that whenever |x −y| < δ1, it follows |f (x) −f (y)| <
ε
2(|a|+|b|+1) and there exists δ2 > 0
such that whenever |x −y| < δ2, it follows that |g (x) −g (y)| <
ε
2(|a|+|b|+1). Then let
0 < δ ≤min (δ1, δ2) . If |x −y| < δ, then everything happens at once. Therefore, using the
triangle inequality
|af (x) + bf (x) −(ag (y) + bg (y))|

326
LIMITS OF A FUNCTION 17-23 OCT.
≤|a| |f (x) −f (y)| + |b| |g (x) −g (y)|
< |a|
µ
ε
2 (|a| + |b| + 1)
¶
+ |b|
µ
ε
2 (|a| + |b| + 1)
¶
< ε.
Now begin on 2.) There exists δ1 > 0 such that if |y −x| < δ1, then |f (x) −f (y)| < 1.
Therefore, for such y,
|f (y)| < 1 + |f (x)| .
It follows that for such y,
|fg (x) −fg (y)| ≤|f (x) g (x) −g (x) f (y)| + |g (x) f (y) −f (y) g (y)|
≤|g (x)| |f (x) −f (y)| + |f (y)| |g (x) −g (y)|
≤(1 + |g (x)| + |f (y)|) [|g (x) −g (y)| + |f (x) −f (y)|] .
Now let ε > 0 be given. There exists δ2 such that if |x −y| < δ2, then
|g (x) −g (y)| <
ε
2 (1 + |g (x)| + |f (y)|),
and there exists δ3 such that if |x −y| < δ3, then
|f (x) −f (y)| <
ε
2 (1 + |g (x)| + |f (y)|)
Now let 0 < δ ≤min (δ1, δ2, δ3) . Then if |x −y| < δ, all the above hold at once and
|fg (x) −fg (y)| ≤
(1 + |g (x)| + |f (y)|) [|g (x) −g (y)| + |f (x) −f (y)|]
< (1 + |g (x)| + |f (y)|)
µ
ε
2 (1 + |g (x)| + |f (y)|) +
ε
2 (1 + |g (x)| + |f (y)|)
¶
= ε.
This proves the ﬁrst part of 2.) To obtain the second part, let δ1 be as described above and
let δ0 > 0 be such that for |x −y| < δ0,
|g (x) −g (y)| < |g (x)| /2
and so by the triangle inequality,
−|g (x)| /2 ≤|g (y)| −|g (x)| ≤|g (x)| /2
which implies |g (y)| ≥|g (x)| /2, and |g (y)| < 3 |g (x)| /2.
Then if |x −y| < min (δ0, δ1) ,
¯¯¯¯
f (x)
g (x) −f (y)
g (y)
¯¯¯¯ =
¯¯¯¯
f (x) g (y) −f (y) g (x)
g (x) g (y)
¯¯¯¯
≤|f (x) g (y) −f (y) g (x)|
³
|g(x)|2
2
´
= 2 |f (x) g (y) −f (y) g (x)|
|g (x)|2

18.2.
SOME FUNDAMENTALS∗
327
≤
2
|g (x)|2 [|f (x) g (y) −f (y) g (y) + f (y) g (y) −f (y) g (x)|]
≤
2
|g (x)|2 [|g (y)| |f (x) −f (y)| + |f (y)| |g (y) −g (x)|]
≤
2
|g (x)|2
·3
2 |g (x)| |f (x) −f (y)| + (1 + |f (x)|) |g (y) −g (x)|
¸
≤
2
|g (x)|2 (1 + 2 |f (x)| + 2 |g (x)|) [|f (x) −f (y)| + |g (y) −g (x)|]
≡M [|f (x) −f (y)| + |g (y) −g (x)|]
where
M ≡
2
|g (x)|2 (1 + 2 |f (x)| + 2 |g (x)|)
Now let δ2 be such that if |x −y| < δ2, then
|f (x) −f (y)| < ε
2M −1
and let δ3 be such that if |x −y| < δ3, then
|g (y) −g (x)| < ε
2M −1.
Then if 0 < δ ≤min (δ0, δ1, δ2, δ3) , and |x −y| < δ, everything holds and
¯¯¯¯
f (x)
g (x) −f (y)
g (y)
¯¯¯¯ ≤M [|f (x) −f (y)| + |g (y) −g (x)|]
< M
hε
2M −1 + ε
2M −1i
= ε.
This completes the proof of the second part of 2.) Note that in these proofs no eﬀort is
made to ﬁnd some sort of “best” δ. The problem is one which has a yes or a no answer.
Either is it or it is not continuous.
Now begin on 3.). If f is continuous at x, f (x) ∈D (g) ⊆Rp, and g is continuous
at f (x) ,then g ◦f is continuous at x. Let ε > 0 be given. Then there exists η > 0 such
that if |y −f (x)| < η and y ∈D (g) , it follows that |g (y) −g (f (x))| < ε. It follows from
continuity of f at x that there exists δ > 0 such that if |x −z| < δ and z ∈D (f) , then
|f (z) −f (x)| < η. Then if |x −z| < δ and z ∈D (g ◦f) ⊆D (f) , all the above hold and so
|g (f (z)) −g (f (x))| < ε.
This proves part 3.)
Part 4.) says: If f = (f1, · · ·, fq) : D (f) →Rq, then f is continuous if and only if each
fk is a continuous real valued function. Then
|fk (x) −fk (y)| ≤|f (x) −f (y)|
≡
Ã q
X
i=1
|fi (x) −fi (y)|2
!1/2
≤
q
X
i=1
|fi (x) −fi (y)| .
(18.10)

328
LIMITS OF A FUNCTION 17-23 OCT.
Suppose ﬁrst that f is continuous at x. Then there exists δ > 0 such that if |x −y| < δ,
then |f (x) −f (y)| < ε. The ﬁrst part of the above inequality then shows that for each
k = 1, · · ·, q, |fk (x) −fk (y)| < ε. This shows the only if part. Now suppose each function,
fk is continuous. Then if ε > 0 is given, there exists δk > 0 such that whenever |x −y| < δk
|fk (x) −fk (y)| < ε/q.
Now let 0 < δ ≤min (δ1, · · ·, δq) . For |x −y| < δ, the above inequality holds for all k and
so the last part of 18.10 implies
|f (x) −f (y)| ≤
q
X
i=1
|fi (x) −fi (y)|
<
q
X
i=1
ε
q = ε.
This proves part 4.)
To verify part 5.), let ε > 0 be given and let δ = ε. Then if |x −y| < δ, the triangle
inequality implies
|f (x) −f (y)| = ||x| −|y||
≤|x −y| < δ = ε.
This proves part 5.) and completes the proof of the theorem.
18.2.1
The Nested Interval Lemma∗
Here is a multidimensional version of the nested interval lemma.
Lemma 18.2.2 Let Ik = Qp
i=1
£
ak
i , bk
i
¤
≡
©
x ∈Rp : xi ∈
£
ak
i , bk
i
¤ª
and suppose that for
all k = 1, 2, · · ·,
Ik ⊇Ik+1.
Then there exists a point, c ∈Rp which is an element of every Ik.
Proof: Since Ik ⊇Ik+1, it follows that for each i = 1, · · ·, p ,
£
ak
i , bk
i
¤
⊇
£
ak+1
i
, bk+1
i
¤
.
This implies that for each i,
ak
i ≤ak+1
i
, bk
i ≥bk+1
i
.
(18.11)
Consequently, if k ≤l,
al
i ≤bl
i ≤bk
i .
(18.12)
Now deﬁne
ci ≡sup
©
al
i : l = 1, 2, · · ·
ª
By the ﬁrst inequality in 18.11,
ci = sup
©
al
i : l = k, k + 1, · · ·
ª
(18.13)
for each k = 1, 2 · · · . Therefore, picking any k,18.12 shows that bk
i is an upper bound for
the set,
©
al
i : l = k, k + 1, · · ·
ª
and so it is at least as large as the least upper bound of this
set which is the deﬁnition of ci given in 18.13. Thus, for each i and each k,
ak
i ≤ci ≤bk
i .
Deﬁning c ≡(c1, · · ·, cp) , c ∈Ik for all k. This proves the lemma.
If you don’t like the proof,you could prove the lemma for the one variable case ﬁrst and
then do the following.

18.2.
SOME FUNDAMENTALS∗
329
Lemma 18.2.3 Let Ik = Qp
i=1
£
ak
i , bk
i
¤
≡
©
x ∈Rp : xi ∈
£
ak
i , bk
i
¤ª
and suppose that for
all k = 1, 2, · · ·,
Ik ⊇Ik+1.
Then there exists a point, c ∈Rp which is an element of every Ik.
Proof: For each i = 1, · · ·, p,
£
ak
i , bk
i
¤
⊇
£
ak+1
i
, bk+1
i
¤
and so by the nested interval
theorem for one dimensional problems, there exists a point ci ∈
£
ak
i , bk
i
¤
for all k. Then
letting c ≡(c1, · · ·, cp) it follows c ∈Ik for all k. This proves the lemma.
18.2.2
The Extreme Value Theorem∗
Deﬁnition 18.2.4 A set, C ⊆Rp is said to be bounded if C ⊆Qp
i=1 [ai, bi] for
some choice of intervals, [ai, bi] where −∞< ai < bi < ∞. The diameter of a set, S, is
deﬁned as
diam (S) ≡sup {|x −y| : x, y ∈S} .
A function, f having values in Rp is said to be bounded if the set of values of f is a bounded
set.
Thus diam (S) is just a careful description of what you would think of as the diameter.
It measures how stretched out the set is.
Lemma 18.2.5 Let C ⊆Rp be closed and bounded and let f : C →R be continuous.
Then f is bounded.
Proof: Suppose not. Since C is bounded, it follows C ⊆Qp
i=1 [ai, bi] ≡I0 for some
closed intervals, [ai, bi]. Consider all sets of the form Qp
i=1 [ci, di] where [ci, di] equals either
£
ai, ai+bi
2
¤
or [ci, di] =
£ ai+bi
2
, bi
¤
. Thus there are 2p of these sets because there are two
choices for the ith slot for i = 1, · · ·, p. Also, if x and y are two points in one of these sets,
|xi −yi| ≤2−1 |bi −ai| .
Observe that diam (I0) =
³Pp
i=1 |bi −ai|2´1/2
because for x, y ∈I0, |xi −yi| ≤|ai −bi| for
each i = 1, · · ·, p,
|x −y| =
Ã p
X
i=1
|xi −yi|2
!1/2
≤2−1
Ã p
X
i=1
|bi −ai|2
!1/2
≡2−1 diam (I0) .
Denote by {J1, · · ·, J2p} these sets determined above. It follows the diameter of each set is
no larger than 2−1 diam (I0) . In particular, since d ≡(d1, · · ·, dp) and c ≡(c1, · · ·, cp) are
two such points, for each Jk,
diam (Jk) ≡
Ã p
X
i=1
|di −ci|2
!1/2
≤2−1 diam (I0)
Since the union of these sets equals all of I0, it follows
C = ∪2p
k=1Jk ∩C.

330
LIMITS OF A FUNCTION 17-23 OCT.
If f is not bounded on C, it follows that for some k, f is not bounded on Jk ∩C. Let I1 ≡
Jk and let C1 = C ∩I1. Now do to I1 and C1 what was done to I0 and C to obtain I2 ⊆I1,
and for x, y ∈I2,
|x −y| ≤2−1 diam (I1) ≤2−2 diam (I2) ,
and f is unbounded on I2 ∩C1 ≡C2. Continue in this way obtaining sets, Ik such that
Ik ⊇Ik+1 and diam (Ik) ≤2−k diam (I0) and f is unbounded on Ik ∩C. By the nested
interval lemma, there exists a point, c which is contained in each Ik.
Claim: c ∈C.
Proof of claim: Suppose c /∈C. Since C is a closed set, there exists r > 0 such that
B (c, r) is contained completely in Rp \ C. In other words, B (c, r) contains no points of C.
Let k be so large that diam (I0) 2−k < r. Then since c ∈Ik, and any two points of Ik are
closer than diam (I0) 2−k, Ik must be contained in B (c, r) and so has no points of C in it,
contrary to the manner in which the Ik are deﬁned in which f is unbounded on Ik ∩C.
Therefore, c ∈C as claimed.
Now for k large enough, and x ∈C ∩Ik, the continuity of f implies |f (c) −f (x)| < 1
contradicting the manner in which Ik was chosen since this inequality implies f is bounded
on Ik ∩C. This proves the theorem.
Here is a proof of the extreme value theorem.
Theorem 18.2.6 Let C be closed and bounded and let f : C →R be continuous.
Then f achieves its maximum and its minimum on C. This means there exist, x1, x2 ∈C
such that for all x ∈C,
f (x1) ≤f (x) ≤f (x2) .
Proof: Let M = sup {f (x) : x ∈C} . Then by Lemma 18.2.5, M is a ﬁnite number. Is
f (x2) = M for some x2? if not, you could consider the function,
g (x) ≡
1
M −f (x)
and g would be a continuous and unbounded function deﬁned on C, contrary to Lemma
18.2.5. Therefore, there exists x2 ∈C such that f (x2) = M. A similar argument applies to
show the existence of x1 ∈C such that
f (x1) = inf {f (x) : x ∈C} .
This proves the theorem.
18.2.3
Sequences And Completeness∗
Deﬁnition 18.2.7 A function whose domain is deﬁned as a set of the form
{k, k + 1, k + 2, · · ·}
for k an integer is known as a sequence. Thus you can consider f (k) , f (k + 1) , f (k + 2) ,
etc.
Usually the domain of the sequence is either N, the natural numbers consisting of
{1, 2, 3, · · ·} or the nonnegative integers, {0, 1, 2, 3, · · ·} . Also, it is traditional to write f1, f2,
etc. instead of f (1) , f (2) , f (3) etc. when referring to sequences. In the above context, fk is
called the ﬁrst term, fk+1 the second and so forth. It is also common to write the sequence,
not as f but as {fi}∞
i=k or just {fi} for short. The letter used for the name of the sequence
is not important. Thus it is all right to let a be the name of a sequence or to refer to it as
{ai} . When the sequence has values in Rp, it is customary to write it in bold face. Thus
{ai} would refer to a sequence having values in Rp for some p > 1.

18.2.
SOME FUNDAMENTALS∗
331
Example 18.2.8 Let {ak}∞
k=1 be deﬁned by ak ≡k2 + 1.
This gives a sequence. In fact, a7 = a (7) = 72 + 1 = 50 just from using the formula for
the kth term of the sequence.
It is nice when sequences come in this way from a formula for the kth term. However,
this is often not the case. Sometimes sequences are deﬁned recursively. This happens, when
the ﬁrst several terms of the sequence are given and then a rule is speciﬁed which determines
an+1 from knowledge of a1, · · ·, an. This rule which speciﬁes an+1 from knowledge of ak for
k ≤n is known as a recurrence relation.
Example 18.2.9 Let a1 = 1 and a2 = 1. Assuming a1, · · ·, an+1 are known, an+2 ≡
an + an+1.
Thus the ﬁrst several terms of this sequence, listed in order, are 1, 1, 2, 3, 5, 8,· · ·.
This particular sequence is called the Fibonacci sequence and is important in the study of
reproducing rabbits.
Example 18.2.10 Let ak = (k, sin (k)) . Thus this sequence has values in R2.
Deﬁnition 18.2.11 Let {an} be a sequence and let n1 < n2 < n3, ··· be any strictly
increasing list of integers such that n1 is at least as large as the ﬁrst index used to deﬁne
the sequence {an} . Then if bk ≡ank, {bk} is called a subsequence of {an} .
For example, suppose an =
¡
n2 + 1
¢
. Thus a1 = 2, a3 = 10, etc. If
n1 = 1, n2 = 3, n3 = 5, · · ·, nk = 2k −1,
then letting bk = ank, it follows
bk =
³
(2k −1)2 + 1
´
= 4k2 −4k + 2.
Deﬁnition 18.2.12 A sequence, {ak} is said to converge to a if for every ε >
0 there exists nε such that if n > nε, then |a −aε| < ε. The usual notation for this is
limn→∞an = a although it is often written as an →a.
The following theorem says the limit, if it exists, is unique.
Theorem 18.2.13 If a sequence, {an} converges to a and to b then a = b.
Proof: There exists nε such that if n > nε then |an −a| <
ε
2 and if n > nε, then
|an −b| < ε
2. Then pick such an n.
|a −b| < |a −an| + |an −b| < ε
2 + ε
2 = ε.
Since ε is arbitrary, this proves the theorem.
The following is the deﬁnition of a Cauchy sequencein Rp.
Deﬁnition 18.2.14 {an} is a Cauchy sequence if for all ε > 0, there exists nε such
that whenever n, m ≥nε,
|an−am| < ε.
A sequence is Cauchy means the terms are “bunching up to each other” as m, n get
large.

332
LIMITS OF A FUNCTION 17-23 OCT.
Theorem 18.2.15 The set of terms in a Cauchy sequence in Rp is bounded in the
sense that for all n, |an| < M for some M < ∞.
Proof: Let ε = 1 in the deﬁnition of a Cauchy sequence and let n > n1. Then from the
deﬁnition,
|an−an1| < 1.
It follows that for all n > n1,
|an| < 1 + |an1| .
Therefore, for all n,
|an| ≤1 + |an1| +
n1
X
k=1
|ak| .
This proves the theorem.
Theorem 18.2.16 If a sequence {an} in Rp converges, then the sequence is a
Cauchy sequence.
Also, if some subsequence of a Cauchy sequence converges, then the
original sequence converges.
Proof: Let ε > 0 be given and suppose an→a. Then from the deﬁnition of convergence,
there exists nε such that if n > nε, it follows that
|an−a| < ε
2
Therefore, if m, n ≥nε + 1, it follows that
|an−am| ≤|an−a| + |a −am| < ε
2 + ε
2 = ε
showing that, since ε > 0 is arbitrary, {an} is a Cauchy sequence. It remains to show the last
claim. Suppose then that {an} is a Cauchy sequence and a = limk→∞ank where {ank}∞
k=1
is a subsequence. Let ε > 0 be given. Then there exists K such that if k, l ≥K, then
|ak −al| < ε
2. Then if k > K, it follows nk > K because n1, n2, n3, · · · is strictly increasing
as the subscript increases. Also, there exists K1 such that if k > K1, |ank −a| < ε
2. Then
letting n > max (K, K1) , pick k > max (K, K1) . Then
|a −an| ≤|a −ank| + |ank −an| < ε
2 + ε
2 = ε.
This proves the theorem.
Deﬁnition 18.2.17 A set, K in Rp is said to be sequentially compact if every
sequence in K has a subsequence which converges to a point of K.
Theorem 18.2.18 If I0 = Qp
i=1 [ai, bi] where ai ≤bi, then I0 is sequentially
compact.
Proof: Let {ai}∞
i=1 ⊆I0 and consider all sets of the form Qp
i=1 [ci, di] where [ci, di]
equals either
£
ai, ai+bi
2
¤
or [ci, di] =
£ ai+bi
2
, bi
¤
. Thus there are 2p of these sets because
there are two choices for the ith slot for i = 1, · · ·, p. Also, if x and y are two points in one
of these sets,
|xi −yi| ≤2−1 |bi −ai| .

18.2.
SOME FUNDAMENTALS∗
333
diam (I0) =
³Pp
i=1 |bi −ai|2´1/2
,
|x −y| =
Ã p
X
i=1
|xi −yi|2
!1/2
≤2−1
Ã p
X
i=1
|bi −ai|2
!1/2
≡2−1 diam (I0) .
In particular, since d ≡(d1, · · ·, dp) and c ≡(c1, · · ·, cp) are two such points,
D1 ≡
Ã p
X
i=1
|di −ci|2
!1/2
≤2−1 diam (I0)
Denote by {J1, · · ·, J2p} these sets determined above. Since the union of these sets equals
all of I0 ≡I, it follows that for some Jk, the sequence, {ai} is contained in Jk for inﬁnitely
many k. Let that one be called I1. Next do for I1 what was done for I0 to get I2 ⊆I1 such
that the diameter is half that of I1 and I2 contains {ak} for inﬁnitely many values of k.
Continue in this way obtaining a nested sequence of intervals, {Ik} such that Ik ⊇Ik+1, and
if x, y ∈Ik, then |x −y| ≤2−k diam (I0) , and In contains {ak} for inﬁnitely many values
of k for each n. Then by the nested interval lemma, there exists c such that c is contained
in each Ik. Pick an1 ∈I1. Next pick n2 > n1 such that an2 ∈I2. If an1, · · ·, ank have been
chosen, let ank+1 ∈Ik+1 and nk+1 > nk. This can be done because in the construction, In
contains {ak} for inﬁnitely many k. Thus the distance between ank and c is no larger than
2−k diam (I0) and so limk→∞ank = c ∈I0. This proves the theorem.
Theorem 18.2.19 Every Cauchy sequence in Rp converges.
Proof: Let {ak} be a Cauchy sequence. By Theorem 18.2.15 there is some interval,
Qp
i=1 [ai, bi] containing all the terms of {ak} . Therefore, by Theorem 18.2.18 a subsequence
converges to a point of this interval. By Theorem 18.2.16 the original sequence converges.
This proves the theorem.
18.2.4
Continuity And The Limit Of A Sequence∗
Just as in the case of a function of one variable, there is a very useful way of thinking of
continuity in terms of limits of sequences found in the following theorem. In words, it says
a function is continuous if it takes convergent sequences to convergent sequences whenever
possible.
Theorem 18.2.20 A function f : D (f) →Rq is continuous at x ∈D (f) if and
only if, whenever xn→x with xn ∈D (f) , it follows f (xn) →f (x) .
Proof: Suppose ﬁrst that f is continuous at x and let xn→x. Let ε > 0 be given. By
continuity, there exists δ > 0 such that if |y −x| < δ, then |f (x) −f (y)| < ε. However,
there exists nδ such that if n ≥nδ, then |xn−x| < δ and so for all n this large,
|f (x) −f (xn)| < ε
which shows f (xn) →f (x) .
Now suppose the condition about taking convergent sequences to convergent sequences
holds at x. Suppose f fails to be continuous at x. Then there exists ε > 0 and xn ∈D (f)
such that |x −xn| < 1
n , yet
|f (x) −f (xn)| ≥ε.

334
LIMITS OF A FUNCTION 17-23 OCT.
But this is clearly a contradiction because, although xn→x, f (xn) fails to converge to f (x) .
It follows f must be continuous after all. This proves the theorem.

Part VIII
Diﬀerentiability
335


337
Outcomes
Diﬀerentiability and the Chain Rule
A. Deﬁne diﬀerentiability for a function of several variables.
B. Evaluate partial derivatives from the deﬁnition. Describe the relationship between the
derivative of a multivariable function and its partial derivatives.
C. Describe the relationship between the existence of partial derivatives and the existence
of a derivative for a function of several variables.
D. Apply the chain rule to evaluate derivatives.
E. Solve related rates problems using the chain rule.
Reading: Multivariable Calculus 2.4
Outcome Mapping:
A. H1
B. H1,1
C. G2
D. 3,6
E. 4,7,8
Directional Derivatives
A. Give a graphical interpretation of the gradient.
B. Evaluate the directional derivative of a function.
C. Give a graphical interpretation of directional derivative.
D. Prove that a diﬀerential function f increases most rapidly in the direction of the
gradient (the rate of change is then ∥f(⃗x)∥) and it decreases most rapidly in the
opposite direction (the rate of change is then −∥f(⃗x)∥).
E. Find the path of a heat seeking or a heat repelling particle.
Reading: Multivariable Calculus 2.5
Outcome Mapping:
A. 1,11,H2
B. 4,6
C. 2,3,H3
D. H4
E. 5,7,8
Normal Vectors and Tangent Planes
A. Interpret the gradient of a function as a normal to a level curve or a level surface.

338
B. Find the normal line and tangent plane to a smooth surface at a given point.
C. Find the angles between curves and surfaces.
Reading: Multivariable Calculus 2.6
Outcome Mapping:
A. 1,3,4
B. 9,11
C. 14,15,16,17
Extrema of Functions of Several Variables
A. Identify local extreme values graphically.
B. Determine the local extreme values and saddle points of a function of two variables.
When possible, apply the second partial derivatives test.
C. Identify the extreme values of a function deﬁned on a closed and bounded region.
D. Solve word problems involving maximum and minimum values.
Reading: Multivariable Calculus 2.7
Outcome Mapping:
A. 1,2
B. 3
C. 4
D. 6,9,12
Constrained Extrema
A. Graphically interpret the method of Lagrange.
B. Determine the extreme values of a function subject to side constraints by applying the
method of Lagrange.
C. Apply the method of Lagrange to solve word problems.
Reading: Multivariable Calculus 2.9
Outcome Mapping:
A. 1,2
B. 3
C. 4,8,14

Diﬀerentiability 24-26 Oct.
19.1
The Deﬁnition Of Diﬀerentiability
Quiz
1. Let f (x, y) = x2y + sin (xy) . Find ∇f (x, y) .
2. Let f (x, y) = x2y + sin (xy) . Find Dvf (1, 1) where v is in the direction of (1, 2) .
3. Let f (x, y) = x2y + sin (xy) . Find the largest value of Dvf (1, 2) for all v. That is,
ﬁnd the largest directional derivative of this function.
First remember what it means for a function of one variable to be diﬀerentiable.
f ′ (x) ≡lim
h→0
f (x + h) −f (x)
h
Another way to say this is contained in the following observation.
Observation 19.1.1 Suppose a function, f of one variable has a derivative at x. Then
lim
h→0
|f (x + h) −f (x) −f ′ (x) h|
|h|
= 0.
For a function of n variables, there is a similar deﬁnition of what it means for a function
to be diﬀerentiable.
Deﬁnition 19.1.2 Let U be an open set in Rn and suppose f : U →R is a function.
Then f is diﬀerentiable at x ∈U if for v = (v1, · · ·, vn)
lim
|v|→0
¯¯¯f (x + v) −f (x) −Pn
k=1
∂f
∂xk (x) vk
¯¯¯
|v|
= 0.
Deﬁnition 19.1.3 A function of a vector, v is called o (v) if
lim
|v|→0
o (v)
|v|
= 0.
(19.1)
Thus the function f (x + h) −f (x) −f ′ (x) h is o (h) . When we say a function is o (h) ,
it is used like an adjective. It is like saying the function is white or black or green or fat or
thin. The term is used very imprecisely. Thus
o (v) = o (v) + o (v) , o (v) = 45o (v) , o (v) = o (v) −o (v) , etc.
339

340
DIFFERENTIABILITY 24-26 OCT.
When you add two functions with the property of the above deﬁnition, you get another one
having that same property. When you multiply by 45 the property is also retained as it
is when you subtract two such functions. How could something so sloppy be useful? The
notation is useful precisely because it prevents obsession over things which are not relevant
and should be ignored.
Deﬁnition 19.1.2 is then equivalent to the following very simple statement.
Deﬁnition 19.1.4 Let U be an open set in Rn and suppose f : U →R is a function.
Then f is diﬀerentiable at x ∈U if
f (x + v) −f (x) −
n
X
k=1
∂f
∂xk
(x) vk = o (v) .
The ﬁrst deﬁnition says nothing more than f (x + v) −f (x) −Pn
k=1
∂f
∂xk (x) vk = o (v)
because it says
lim
|v|→0
¯¯¯f (x + v) −f (x) −Pn
k=1
∂f
∂xk (x) vk
¯¯¯
|v|
= 0.
The following is fundamental.
Proposition 19.1.5 If f is diﬀerentiable at x, then f is continuous at x.
Proof: From the deﬁnition of diﬀerentiability,
|f (x + v) −f (x)| ≤
¯¯¯¯¯
n
X
k=1
∂f
∂xk
(x) vk + o (v)
¯¯¯¯¯
Let ε > 0 be given. Then clearly if |v| is suﬃciently small, the right side of the above is less
than ε. Thus the function is continuous at x.
So which functions are diﬀerentiable? Are there simple ways to look at a function and
say that it is clearly diﬀerentiable? Existence of partial derivatives is needed in order to even
write the above expression but it turns out this is not enough. Here is a simple example.
Example 19.1.6 Let
f (x, y) =
½
xy
x2+y2 if (x, y) ̸= (0, 0)
0 if (x, y) = (0, 0)
Then
fx (0, 0) ≡lim
h→0
f (h, 0) −f (0, 0)
h
= lim
h→0
0
h = 0
Also
fy (0, 0) ≡lim
h→0
f (0, h) −f (0, 0)
h
= lim
h→0
0
h = 0
so both partial derivatives exist. However, the function is not even continuous at (0, 0) . This
is because it equals zero on the entire y axis but along the line, y = x the function equals
1/2. By Proposition 19.1.5 it cannot be diﬀerentiable.

19.2.
C1 FUNCTIONS AND DIFFERENTIABILITY
341
19.2
C1 Functions And Diﬀerentiability
It turns out that if the partial derivatives are continuous then the function is diﬀerentiable.
I will show this next. First, remember the Cauchy Schwarz inequality, which I will list here
for convenience.
¯¯¯¯¯
n
X
i=1
aibi
¯¯¯¯¯ ≤
Ã n
X
i=1
a2
i
!1/2 Ã n
X
i=1
b2
i
!1/2
.
Theorem 19.2.1 Suppose f : U →R where U is an open set. Suppose also that
all partial derivatives of f exist on U and are continuous. Then f is diﬀerentiable at every
point of U.
Proof: If you ﬁx all the variables but one, you can apply the fundamental theorem of
calculus as follows.
f (x+vkek) −f (x) =
Z 1
0
∂f
∂xk
(x + tvkek) vkdt.
(19.2)
Here is why. Let h (t) = f (x + tvkek) . Then
h (t + ∆t) −h (t)
∆t
= f (x + tvkek + ∆tvkek) −f (x + tvkek)
∆tvk
vk
and so, taking the limit as ∆t →0 yields
h′ (t) = ∂f
∂xk
(x + tvkek) vk
Therefore,
f (x+vkek) −f (x) = h (1) −h (0) =
Z 1
0
h′ (t) dt =
Z 1
0
∂f
∂xk
(x + tvkek) vkdt.
Now I will use this observation to prove the theorem. Let v = (v1, · · ·, vn) with |v|
suﬃciently small. Thus v = Pn
k=1 vkek. For the purposes of this argument, deﬁne
n
X
k=n+1
vkek ≡0.

342
DIFFERENTIABILITY 24-26 OCT.
Then with this convention, and using 19.2,
f (x + v) −f (x)
=
n
X
i=1
Ã
f
Ã
x+
n
X
k=i
vkek
!
−f
Ã
x+
n
X
k=i+1
vkek
!!
=
n
X
i=1
Z 1
0
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
vidt
=
n
X
i=1
Z 1
0
Ã
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
vi −∂f
∂xi
(x) vi
!
dt
+
n
X
i=1
Z 1
0
∂f
∂xi
(x) vidt
=
n
X
i=1
∂f
∂xi
(x) vi +
Z 1
0
n
X
i=1
Ã
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
!
vidt
=
n
X
i=1
∂f
∂xi
(x) vi + o (v)
and this shows f is diﬀerentiable at x because it satisﬁes the conditions of Deﬁnition 19.1.4.
Some explanation of the step to the last line is in order. The messy thing at the end is
o (v) because of the continuity of the partial derivatives. In fact, from the Cauchy Schwarz
inequality,
Z 1
0
n
X
i=1
Ã
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
!
vidt
≤
Z 1
0


n
X
i=1
¯¯¯¯¯
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
¯¯¯¯¯
2

1/2
dt
Ã n
X
i=1
v2
i
!1/2
=
Z 1
0


n
X
i=1
¯¯¯¯¯
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
¯¯¯¯¯
2

1/2
dt |v|
Thus, dividing by |v| and taking a limit as |v| →0, the quotient is nothing but
Z 1
0


n
X
i=1
¯¯¯¯¯
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
¯¯¯¯¯
2

1/2
dt
which converges to 0 due to continuity of the partial derivatives of f.
This proves the
theorem.
To help you keep the various terms straight, here is a pretty diagram.

19.3.
THE DIRECTIONAL DERIVATIVE
343
Continuous
|x| + |y|
Partial derivatives
xy
x2+y2
derivative
C1
You might ask whether there are examples of functions which are diﬀerentiable but not
C1. Of course there are. There are easy examples of this even for functions of one variable.
Here is one.
f (x) =
½
x2 sin 1
x if x ̸= 0
0 if x = 0
.
You should show that f ′ (0) = 0 but the derivative is 2x sin 1
x −cos 1
x for x ̸= 0 and this
function fails to even have a limit as x →0. This is a great test question. You ask for f ′ (0)
and it is really easy if you use the deﬁnition. However, people usually ﬁnd f ′ (x) and then
try to plug in x = 0. This is doomed to failure and makes the question very easy to grade.
19.3
The Directional Derivative
Here I will prove the formula for the directional derivative presented earlier. Recall that for
v a unit vector, (|v| = 1)
Dv (f) (x) ≡lim
t→0
f (x+tv) −f (x)
t
.
Theorem 19.3.1 Suppose f is diﬀerentiable at x. Then Dv (f) (x) exists and is
given by
Dv (f) (x) = ∇f (x) · v
Proof: By diﬀerentiability of f at x,
f (x+tv) −f (x)
t
=
1
t
Ã n
X
k=1
∂f
∂xk
(x) tvk + o (tv)
!
=
n
X
k=1
∂f
∂xk
(x) vk + o (tv)
|tv|
Taking the limit as t →0,
Dvf (x) =
n
X
k=1
∂f
∂xk
(x) vk ≡∇f (x) · v.
This proves the theorem.
What is the direction in which the largest directional derivative results? You want to
maximize ∇f (x)·v = |∇f (x)| |v| cos θ where θ is the included angle between v and ∇f (x) .

344
DIFFERENTIABILITY 24-26 OCT.
Clearly this occurs when θ = 0. Therefore, the largest value of the directional derivative is
when v = ∇f (x) / |∇f (x)| . The value of the directional derivative in this direction, is
∇f (x) · ∇f (x) / |∇f (x)| = |∇f (x)| .
Similarly, the smallest value for the directional derivative occurs when v = −∇f (x) / |∇f (x)|
because this corresponds to θ = π and cos θ = −1. The smallest value of the directional
derivative is then −|∇f (x)| .
19.3.1
Separable Diﬀerential Equations∗
If you do not know how to solve simple diﬀerential equations, read this section. Otherwise
skip it.
Diﬀerential equations are just equations which involve an unknown function and some
of its derivatives. For example, a diﬀerential equation is
y′ = 1 + y2.
(19.3)
You might check and see that a solution to this diﬀerential equation is y = tan x.
Here is another easier diﬀerential equation.
y′ = x2.
A solution to this one is of the form
y = x3
3 + C
where C is any constant. In general, you are familiar with diﬀerential equations of the form
y′ = f (x) .
The problem is just to ﬁnd an antiderivative of the given function. However, equations like
the one in 19.3 are not so obvious. It turns out there are many recipes for ﬁnding solutions
to diﬀerential equations of various sorts. One of the easiest kinds of diﬀerential equations
to solve are those which are separable.
Separable diﬀerential equations are those which can be written in the form
dy
dx = f (x)
g (y) .
The equation in 19.3 is an example of a separable diﬀerential equation. Just let f (x) = 1
and g (y) =
1
1+y2 .
The reason these are called separable is that if you formally cross multiply,
g (y) dy = f (x) dx
and the variables are “separated”. Here is how you solve these. Find G′ (y) = g (y) and
F ′ (x) = f (x) . That is, pick G ∈
R
g (y) dy and F ∈
R
f (x) dx. Suppose F (x) −G (y) = c
speciﬁes y as a diﬀerentiable function of x, then x →y (x) solves the separable diﬀerential
equation because by the chain rule,
F ′ (x) −G′ (y) y′ = f (x) −g (y) y′
and so
f (x) = g (y) y′

19.3.
THE DIRECTIONAL DERIVATIVE
345
so
y′ = f (x)
g (y) .
This is why the solutions are given in the form
F (x) −G (y) = c
where c is a constant, or equivalently
G (y) −F (x) = c
where c is a constant.
Example 19.3.2 Find the solutions to the diﬀerential equation,
y′ = x2
y
which satisﬁes the initial condition, y (0) = 1. Since there is a diﬀerential equation along
with an initial condition, this is called an initial value problem.
To solve this you separate the variables and write
ydy = x2dx
and then from the above discussion,
y2
2 −x3
3 = C.
(19.4)
You want y = 1 when x = 0 and so you must have
1
2 = C.
The solution is
y =
s
2
µx3
3 + 1
2
¶
where it was necessary to pick the positive square root because otherwise, you would not
have y (0) = 1.
Sometimes you can’t solve for y in terms of x.
Example 19.3.3 Find the solutions to the diﬀerential equation,
y′ =
x2
y sin y .
In this case,
(y sin y) dy = x2
and so
R
y sin y = sin y −y cos y
sin y −y cos y −x3
3 = C
gives the solutions. I would not like to try and solve this for y in terms of x. Therefore, in
this case, it is customary to leave the solution in this form and refer to it as an implicitly
deﬁned solution. The point is, the above equation does deﬁne y as a function of x near
typical points on the level curve but it might not be possible to algebraically ﬁnd y as a
function of x. You notice in the above argument for ﬁnding solutions, it was never assumed
that you could algebraically ﬁnd y as a function of x in F (x) −G (y) = C.
Here is an interesting example which is non trivial.

346
DIFFERENTIABILITY 24-26 OCT.
Example 19.3.4 What is the equation of a hanging chain?
Consider the following picture of a portion of this chain.






q
q
T0
T(x)
-
6
θ
T(x) cos θ
T(x) sin θ
?
ρl(x)g
In this picture, ρ denotes the density of the chain which is assumed to be constant and
g is the acceleration due to gravity. T (x) and T0 represent the magnitude of the tension in
the chain at x and at 0 respectively, as shown. Let the bottom of the chain be at the origin
as shown. If this chain does not move, then all these forces acting on it must balance. In
particular,
T (x) sin θ = l (x) ρg, T (x) cos θ = T0.
Therefore, dividing these yields
sin θ
cos θ = l (x)
≡c
z }| {
ρg/T0.
Now letting y (x) denote the y coordinate of the hanging chain corresponding to x,
sin θ
cos θ = tan θ = y′ (x) .
Therefore, this yields
y′ (x) = cl (x) .
Now diﬀerentiating both sides of the diﬀerential equation,
y′′ (x) = cl′ (x) = c
q
1 + y′ (x)2
and so
y′′ (x)
q
1 + y′ (x)2 = c.
Let z (x) = y′ (x) so the above diﬀerential equation becomes
dz
dx = c
p
1 + z2,
a separable diﬀerential equation. Thus
dz
√
1 + z2 = cdx.
Now
R
dz
√
1+z2 = arcsinh (z) + C and so the solutions are of the form
arcsinh (z) −cx = d

19.3.
THE DIRECTIONAL DERIVATIVE
347
where d is some constant. Thus
y′ = z = sinh (cx + d)
and so
y (x) ∈
Z
sinh (cx + C) dx = cosh (cx + d)
c
+ k
where k is some constant. Therefore,
y (x) = 1
c cosh (cx + d) + k
where d and k are some constants and c = ρg/T0. Curves of this sort are called catenaries.
Note these curves result from an assumption the only forces acting on the chain are as
shown.
19.3.2
Exercises With Answers∗
1. Find the solution to the initial value problem,
y′ = x
y2 , y (0) = 1.
Separating the variables, you get y2dy = xdx and so y3
3 −x2
2 = c. From the initial
condition, 1
3 = c and so the solution is
y3
3 −x2
2 = 1
3
2. Find the solution to the initial value problem,
tan (y) y′ = sin x, y
³π
4
´
= π
4 .
Separating the variables, tan (y) dy = sin (x) dx and so ln |sec (y)| + cos (x) = c. Now
from the initial condition,
ln
³√
2
´
+
√
2
2
= c
and so ln |sec (y)| + cos (x) = ln
¡√
2
¢
+
√
2
2
3. Find the solution to the initial value problem,
y′ = y
x, y (1) = 1.
Separating the variables, gives dy
y = dx
x and so y−x = c. But from the initial condition,
c = 0. Hence y = x.

348
DIFFERENTIABILITY 24-26 OCT.
19.3.3
A Heat Seaking Particle
Suppose the temperature is given as T (x, y, z) and a particle tries to go in the direction of
most rapid rate of change of temperature. In other words this particle likes it hot. This
means it moves in the direction of the gradient of T. In other words,
(x′, y′, z′)T = k (x, y, z) ∇T (x, y, z) .
Of course you don’t know what k (x, y, z) is but if you did and if you also knew T, then you
would have a system of diﬀerential equations for the position of the particle as a function
of time. If you were given an initial position, you could then ask for the solution to the
resulting intial value problem. Of course you won’t be able to solve the equations in general.
These sorts of things require numerical methods. Also, in interesting examples, everything
would also depend on t. The following pseudo application has to do with a situation which
I will cook up so that I will be able to solve everything.
Example 19.3.5 A heat seaking particle starts at (1, 2, 1) . The temperature is T (x, y, z) =
x2 + y + z3 and assume that k = 1. Find the motion of the heat seaking particle.
As explained above, you need (x′, y′, z′)T = ∇T (x, y, z) and so
dx
dt = 2x, dy
dt = 1, dz
dt = 3z2
because ∇T =
¡
2x, 1, 3z2¢T . It is very fortunate that the equations are not coupled. Con-
sider the ﬁrst one. Separating the variables,
dx
x = 2dt
and so ln (x)−2t = c. From the initial condition which states that at t = 0, x = 1, it follows
c = 0. Therefore, x = e2t. Next consider the second of the diﬀerential equations. This one
says y = t + c and from the initial condition, c = 2 so the second gives y = t + 2. Finally
the last equation separates to give
dz
z2 = 3dt
and so
−1
z
= 3t + c.
In this case the initial data gives c = −1. Therefore, z = −
1
3t−1. It follows the path of the
particle is of the form
µ
e2t, t + 2, −
1
3t −1
¶
.
Note that this only makes sense for t ∈[0, 1
3). This type of thing is typical of nonlinear
diﬀerential equations.
I think you can see how to do similar problems in which the particle is heat avoiding.
You just put in a minus sign by ∇T.
19.4
The Chain Rule
Remember what this was all about for a function of one variable. You had z = f (y) and
y = g (x) and you wanted to ﬁnd dz
dx. Remember the answer was
dz
dx = dz
dy
dy
dx.

19.4.
THE CHAIN RULE
349
The chain rule was one of the most important rules for diﬀerentiation. Its importance is no
less for functions of many variables.
The problem is this: z = f (y1, y2, · · ·, yn) and yk = gk (x1, · · ·, xm) . You want to ﬁnd
∂z
∂xk for each k = 1, 2, · · ·, m. It turns out to be exactly the same sort of formula which
works. In this case the formula is
∂z
∂xk
=
n
X
i=1
∂z
∂yi
∂yi
∂xk
.
People who use the repeated index summation convention write this as
∂z
∂yi
∂yi
∂xk
which is formally just like it was for a function of one variable. I think this is one reason
for the attractiveness of this repeated summation convention. Here is an example.
Example 19.4.1 Suppose z = y1 + y2y2
3 and y1 = sin (x1) + x2, y2 = cos (x3) , and y3 =
x2
1 + sin x2 + x4. Find
∂z
∂x2 and
∂z
∂x4 .
From the above formula,
∂z
∂x2
= ∂z
∂y1
∂y1
∂x2
+ ∂z
∂y2
∂y2
∂x2
+ ∂z
∂y3
∂y3
∂x2
=
1 × 1 + y2
3 × 0 + 2y2y3 cos x2
=
1 + 2y2y3 cos x2.
If you want to put this in terms of the x variables, it is
∂z
∂x2
=
1 + 2y2y3 cos x2
=
1 + 2 cos (x3)
¡
x2
1 + sin x2 + x4
¢
cos x2
Now consider the other partial derivative.
∂z
∂x4
=
∂z
∂y1
∂y1
∂x4
+ ∂z
∂y2
∂y2
∂x4
+ ∂z
∂y3
∂y3
∂x4
=
∂z
∂y1
× 0 + ∂z
∂y2
× 0 + 2y2y3 × 1
=
2 cos (x3)
¡
x2
1 + sin x2 + x4
¢
.
Be sure you can ﬁnd and place the partial derivatives in terms of the independent vari-
ables, x. It is just as correct to leave the answer in terms of y and x but sometimes people
may insist you place the answer in terms of x.
The next task is to explain why the above formula works. The argument I will give
applies to one dimension also. Therefore, you can consider it a review of what you should
have seen in beginning calculus.
Lemma 19.4.2 Suppose U is an open set in Rn and f : U →R. Suppose x ∈U and for
all v small enough,
f (x + v) −f (x) =
n
X
i=1
aivi + o (v) .
Then ai = ∂f
∂xi (x) and f is diﬀerentiable.

350
DIFFERENTIABILITY 24-26 OCT.
Proof: Let t be a small nonzero number. Then since the ith component of ek equals
zero unless i = k when it is 1,
f (x + tek) −f (x)
=
akt + o (tek)
=
akt + o (t)
Now divide by t and take a limit.
∂f
∂xk
(x) = lim
t→0
f (x + tek) −f (x)
t
= lim
t→0
µ
ak + o (t)
t
¶
= ak.
This proves the lemma.
Lemma 19.4.3 Let U be an open set and suppose g is diﬀerentiable at x ∈U. Then
o (g (x + v) −g (x)) = o (v) .
Proof: I need to show
lim
|v|→0
o (g (x + v) −g (x))
|v|
= 0.
Let ε > 0 be given. Since g is continuous at x, there exists δ1 > 0 such that if |v| < δ1, then
|o (g (x + v) −g (x))|
|g (x + v) −g (x)|
< ε
Hence, for such v,
|o (g (x + v) −g (x))| < ε |g (x + v) −g (x)|
(19.5)
Since g is diﬀerentiable at x, there exists δ2 > 0 such that if |v| < δ2,
¯¯¯g (x + v) −g (x) −Pn
k=1
∂g
∂xk (x) vk
¯¯¯
|v|
< ε
Hence for |v| < δ2,
|g (x + v) −g (x)| <
¯¯¯¯¯
n
X
k=1
∂g
∂xk
(x) vk
¯¯¯¯¯ + ε |v|
(19.6)
Let δ ≤min (δ1, δ2) . Then if |v| < δ, both 19.5 and 19.6 hold and so by the Cauchy Schwarz
inequality,
|o (g (x + v) −g (x))|
<
ε |g (x + v) −g (x)|
<
ε
Ã¯¯¯¯¯
n
X
k=1
∂g
∂xk
(x) vk
¯¯¯¯¯ + ε |v|
!
≤
ε
Ã n
X
k=1
¯¯¯¯
∂g
∂xk
(x)
¯¯¯¯
2!1/2
|v| + ε |v| .
Dividing both sides by |v| ,
o (g (x + v) −g (x))
|v|
≤ε


Ã n
X
k=1
¯¯¯¯
∂g
∂xk
(x)
¯¯¯¯
2!1/2
+ 1


and since ε > 0 is arbitrary, this establishes the lemma.
With these lemmas, it is easy to prove the chain rule.

19.4.
THE CHAIN RULE
351
Theorem 19.4.4 Let V be an open set in Rn and let U be an open set in Rm. Also
let g :U →V be a vector valued function having the property that for
g (x) = (g1 (x) , · · ·, gm (x)) ,
each gk is diﬀerentiable at x ∈U. Also suppose f : V →R is diﬀerentiable at g (x) . Then
for z ≡f ◦g, yi = gi (x) ,
∂z
∂xk
(x) =
n
X
i=1
∂z
∂yi
(g (x)) ∂yi
∂xk
(x) .
Proof: Using Lemma 19.4.3 as needed,
f ◦g (x + v) −f ◦g (x)
=
n
X
i=1
∂z
∂yi
(g (x)) (gi (x + v) −gi (x)) + o (gi (x + v) −gi (x))
=
n
X
i=1
∂z
∂yi
(g (x)) (gi (x + v) −gi (x)) + o (v)
=
n
X
i=1
∂z
∂yi
(g (x))
Ã m
X
k=1
∂yi
∂xk
(x) vk + o (v)
!
+ o (v)
=
n
X
i=1
m
X
k=1
∂z
∂yi
(g (x)) ∂yi
∂xk
(x) vk + o (v)
=
m
X
k=1
Ã n
X
i=1
∂z
∂yi
(g (x)) ∂yi
∂xk
(x)
!
vk + o (v)
Now by Lemma 19.4.2,
∂f ◦g
∂xk
(x) ≡∂z
∂xk
=
n
X
i=1
∂z
∂yi
(g (x)) ∂yi
∂xk
(x) .
This proves the theorem.
19.4.1
Related Rates Problems
Sometimes several variables are related and given information about how one variable is
changing, you want to ﬁnd how the others are changing. The following law is discussed later
in the book, on Page 519.
Example 19.4.5 Bernoulli’s law states that in an incompressible ﬂuid,
v2
2g + z + P
γ = C
where C is a constant. Here v is the speed, P is the pressure, and z is the height above
some reference point. The constants, g and γ are the acceleration of gravity and the weight
density of the ﬂuid. Suppose measurements indicate that dv
dt = −3, and dz
dt = 2. Find dP
dt
when v = 7 and z = 8 in terms of g and γ.
This is just an exercise in using the chain rule. Diﬀerentiate the two sides with respect
to t.
1
g v dv
dt + dz
dt + 1
γ
dP
dt = 0.

352
DIFFERENTIABILITY 24-26 OCT.
Then when v = 7 and z = 8, ﬁnding dP
dt involves nothing more than solving the following
for dP
dt .
7
g (−3) + 2 + 1
γ
dP
dt = 0
Thus
dP
dt = γ
µ21
g −2
¶
at this instant in time.
Example 19.4.6 In Bernoulli’s law above, each of v, z, and P are functions of (x, y, z) ,
the position of a point in the ﬂuid. Find a formula for ∂P
∂x in terms of the partial derivatives
of the other variables.
This is an example of the chain rule. Diﬀerentiate both sides with respect to x.
v
g vx + zx + 1
γ Px = 0
and so
Px = −
µvvx + zxg
g
¶
γ
Example 19.4.7 Suppose a level curve is of the form f (x, y) = C and that near a point
on this level curve, y is a diﬀerentiable function of x. Find dy
dx.
This is an example of the chain rule. Diﬀerentiate both sides with respect to x. This
gives
fx + fy
dy
dx = 0.
Solving for dy
dx gives
dy
dx = −fx (x, y)
fy (x, y) .
Example 19.4.8 Suppose a level surface is of the form f (x, y, z) = C. and that near a
point, (x, y, z) on this level surface, z is a C1 function of x and y. Find a formula for zx.
This is an exaple of the use of the chain rule. Diﬀerentiate both sides of the equation
with respect to x. Since yx = 0, this yields
fx + fzzx = 0.
Then solving for zx gives
zx = −fx (x, y, z)
fz (x, y, z)
Example 19.4.9 Polar coordinates are
x = r cos θ, y = r sin θ.
Thus if f is a C1 scalar valued function you could ask to express fx in terms of the variables,
r and θ. Do so.

19.5.
NORMAL VECTORS AND TANGENT PLANES 26 OCT.
353
This is an example of the chain rule. f = f (r, θ) and so
fx = frrx + fθθx.
This will be done if you can ﬁnd rx and θx. However you must ﬁnd these in terms of r and
θ, not in terms of x and y. Using the chain rule on the two equations for the transformation,
1
=
rx cos θ −(r sin θ) θx
0
=
rx sin θ + (r cos θ) θx
Solving these using Cramer’s rule yields
rx = cos (θ) , θx = −sin (θ)
r
Hence fx in polar coordinates is
fx = fr (r, θ) cos (θ) −fθ (r, θ)
µsin (θ)
r
¶
19.5
Normal Vectors And Tangent Planes 26 Oct.
Quiz
1. Let z = xy2 and let x = ts + ps while y = 2s2 + t. Find ∂z
∂s when (s, t, p) = (1, 1, 1) .
2. A level surface is given by x3y + z2 = 2. Find zx at the point (1, 1, 1) on the level
surface.
3. Suppose x = t3 + s and y = s3 + t. Find ∂z
∂x completely in terms of partial derivativs
and functions of the new variables, s, t.
The gradient has fundamental geometric signiﬁcance illustrated by the following picture.
>
^
¤ ∇f(x0, y0, z0)
x′
1(t0)
x′
2(s0)
In this picture, the surface is a piece of a level surface of a function of three vari-
ables, f (x, y, z) . Thus the surface is deﬁned by f (x, y, z) = c or more completely as
{(x, y, z) : f (x, y, z) = c} . For example, if f (x, y, z) = x2 + y2 + z2, this would be a piece
of a sphere. There are two smooth curves in this picture which lie in the surface having
parameterizations, x1 (t) = (x1 (t) , y1 (t) , z1 (t)) and x2 (s) = (x2 (s) , y2 (s) , z2 (s)) which
intersect at the point, (x0, y0, z0) on this surface1. This intersection occurs when t = t0 and
s = s0. Since the points, x1 (t) for t in an interval lie in the level surface, it follows
f (x1 (t) , y1 (t) , z1 (t)) = c
1Do there exist any smooth curves which lie in the level surface of f and pass through the point
(x0, y0, z0)? It turns out there do if ∇f (x0, y0, z0) ̸= 0 and if the function, f, is C1. However, this is a
consequence of the implicit function theorem, one of the greatest theorems in all mathematics and a topic
for an advanced calculus class. See the the section on the implicit function theorem for the most elementary
treatment of this theorem that I know.

354
DIFFERENTIABILITY 24-26 OCT.
for all t in some interval. Therefore, taking the derivative of both sides and using the chain
rule on the left,
∂f
∂x (x1 (t) , y1 (t) , z1 (t)) x′
1 (t) +
∂f
∂y (x1 (t) , y1 (t) , z1 (t)) y′
1 (t) + ∂f
∂z (x1 (t) , y1 (t) , z1 (t)) z′
1 (t) = 0.
In terms of the gradient, this merely states
∇f (x1 (t) , y1 (t) , z1 (t)) · x′
1 (t) = 0.
Similarly,
∇f (x2 (s) , y2 (s) , z2 (s)) · x′
2 (s) = 0.
Letting s = s0 and t = t0, it follows
∇f (x0, y0, z0) · x′
1 (t0) = 0, ∇f (x0, y0, z0) · x′
2 (s0) = 0.
It follows ∇f (x0, y0, z0) is perpendicular to both the direction vectors of the two indicated
curves shown. Surely if things are as they should be, these two direction vectors would
determine a plane which deserves to be called the tangent plane to the level surface of f at
the point (x0, y0, z0) and that ∇f (x0, y0, z0) is perpendicular to this tangent plane at the
point, (x0, y0, z0).
Example 19.5.1 Find the equation of the tangent plane to the level surface, f (x, y, z) = 6
of the function, f (x, y, z) = x2 + 2y2 + 3z2 at the point (1, 1, 1) .
First note that (1, 1, 1) is a point on this level surface.
To ﬁnd the desired plane it
suﬃces to ﬁnd the normal vector to the proposed plane. But ∇f (x, y, z) = (2x, 4y, 6z) and
so ∇f (1, 1, 1) = (2, 4, 6) . Therefore, from this problem, the equation of the plane is
(2, 4, 6) · (x −1, y −1, z −1) = 0
or in other words,
2x −12 + 4y + 6z = 0.
Example 19.5.2 The point,
¡√
3, 1, 4
¢
is on both the surfaces, z = x2 + y2 and z = 8 −
¡
x2 + y2¢
. Find the cosine of the angle between the two tangent planes at this point.
Recall this is the same as the angle between two normal vectors. Of course there is some
ambiguity here because if n is a normal vector, then so is −n and replacing n with −n
in the formula for the cosine of the angle will change the sign. We agree to look for the
acute angle and its cosine rather than the optuse angle. The normals are
¡
2
√
3, 2, −1
¢
and
¡
2
√
3, 2, 1
¢
. Therefore, the cosine of the angle desired is
¡
2
√
3
¢2 + 4 −1
17
= 15
17.
Example 19.5.3 The point,
¡
1,
√
3, 4
¢
is on the surface, z = x2 +y2. Find the line perpen-
dicular to the surface at this point.
All that is needed is the direction vector of this line. The surface is the level surface,
x2 + y2 −z = 0. The normal to this surface is given by the gradient at this point. Thus the
desired line is
³
1,
√
3, 4
´
+ t
³
2, 2
√
3, −1
´
.

Extrema Of Functions Of
Several Variables 30 Oct.
Quiz
1. Let z = x2 sin (y) and let x = t2s+r while y = t2 −s. Find zt when (s, t, r) = (1, 1, 1) .
2. The temperature in space is given by T (x, y, z) = x + 2y2 + z2. Find the path of a
heat seeking particle which starts at the point (0, 1, 1) . This is an ill deﬁned problem.
Make the usual assumptions.
3. The ideal gas law is PV = kT where k is a constant. Suppose at some time dT
dt =
1, dP
dt = −1. Find dV
dt at this instant if P = 2, V = 6, T = 100.
4. There are two surfaces, x2 + y2 = 1 and x2 + y2 + z2 = 5 which intersect in a curve.
Find an equation of the tangent line to this curve at the point,
³ √
3
2 , 1
2, 2
´
.
5. A level surface is x2 + 2y2 + 3z2 = 6. Find the tangent plane at the point, (1, 1, 1).
Quiz
1. Let z = x sin
¡
x2 + y2¢
. Find ∂z
∂x.
2. Let z3 sin (x) + y4z = 7. Find ∂z
∂x.
3. Suppose f (x, y) is given by
f (x, y) =
(
2yx+x3+xy2
x2+y2
if (x, y) ̸= (0, 0)
0 if (x, y) = (0, 0)
Find fx (0, 0) if possible.
4. Find parametric equations of the line which is perpendicular to the surface 3x2+2y2+
z2 = 6 at the point (1, 1, 1).
5. Let u (x, y) = f (x + y) + g (x −y) . Compute uxx −uyy. D’Lambert did this problem
back in the mid 1700’s and it turned out to be very important.
6. A function of two variables, f (x, y) is called homogeneous of degree α if f (tx, ty) =
tαf (x, y) . Establish Euler’s identity which states that for such homogeneous functions,
xfx (x, y) + yfy (x, y) = αf (x, y) .
355

356
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
This identity dates from early in the 1700’s also.
Hint: Use the chain rule and
diﬀerentiate both sides of f (tx, ty) = tαf (x, y) with respect to t using the chain rule
and then plug in t = 1.
Suppose f : D (f) →R where D (f) ⊆Rn.
20.1
Local Extrema
Deﬁnition 20.1.1 A point x ∈D (f) ⊆Rn is called a local minimum if f (x) ≤
f (y) for all y ∈D (f) suﬃciently close to x. A point x ∈D (f) is called a local maximum
if f (x) ≥f (y) for all y ∈D (f) suﬃciently close to x. A local extremum is a point of
D (f) which is either a local minimum or a local maximum. The plural for extremum is
extrema. The plural for minimum is minima and the plural for maximum is maxima.
Procedure 20.1.2 To ﬁnd candidates for local extrema which are interior points of
D (f) where f is a diﬀerentiable function, you simply identify those points where ∇f equals
the zero vector. To justify this, note that the graph of f is the level surface
F (x,z) ≡z −f (x) = 0
and the local extrema at such interior points must have horizontal tangent planes. Therefore,
a normal vector at such points must be a multiple of (0, · · ·, 0, 1) . Thus ∇F at such points
must be a multiple of this vector. That is, if x is such a point,
k (0, · · ·, 0, 1) = (−fx1 (x) , · · ·, −fxn (x) , 1) .
Thus ∇f (x) = 0.
This is illustrated in the following picture.
¡
¡
¡
¡
¡
¡
¡¡
¡
¡
¡
¡
¡
¡
¡¡
6
r
z = f(x)
Tangent Plane
A more rigorous explanation is as follows. Let v be any vector in Rn and suppose x is
a local maximum (minimum) for f. Then consider the real valued function of one variable,
h (t) ≡f (x + tv) for small |t| . Since f has a local maximum (minimum), it follows that h
is a diﬀerentiable function of the single variable t for small t which has a local maximum
(minimum) when t = 0. Therefore, h′ (0) = 0. But h′ (t) = Pn
i=1
∂f
∂xi (x+tv) vi by the chain
rule. Therefore,
h′ (0) =
n
X
i=1
∂f
∂xi
(x) vi = 0

20.1.
LOCAL EXTREMA
357
and since v is arbitrary, it follows
∂f
∂xi (x) = 0 for each i. Thus
¡
fx1 (x)
· · ·
fxn (x)
¢T = 0
and so ∇f (x) = 0. This proves the following theorem.
Theorem 20.1.3 Suppose U is an open set contained in D (f) such that f is C1
on U and suppose x ∈U is a local minimum or local maximum for f. Then ∇f (x) = 0.
Deﬁnition 20.1.4 A singular point
for f is a point x where ∇f (x) = 0. This
is also called a critical point. By analogy with the one variable case, a point where the
gradient does not exist will also be called a critical point.
Example 20.1.5 Find the critical points for the function, f (x, y) ≡xy−x−y for x, y > 0.
Note that here D (f) is an open set and so every point is an interior point. Where is the
gradient equal to zero?
fx = y −1 = 0, fy = x −1 = 0
and so there is exactly one critical point, (1, 1) .
Example 20.1.6 Find the volume of the smallest tetrahedron made up of the coordinate
planes in the ﬁrst octant and a plane which is tangent to the sphere x2 + y2 + z2 = 4.
The normal to the sphere at a point, (x0, y0, z0) on a point of the sphere is
³
x0, y0,
p
4 −x2
0 −y2
0
´
and so the equation of the tangent plane at this point is
x0 (x −x0) + y0 (y −y0) +
q
4 −x2
0 −y2
0
µ
z −
q
4 −x2
0 −y2
0
¶
= 0
When x = y = 0,
z =
4
p
(4 −x2
0 −y2
0)
When z = 0 = y,
x = 4
x0
,
and when z = x = 0,
y = 4
y0
.
Therefore, letting (x, y) take the place of (x0, y0) for simplicity, the function to minimize is
f (x, y) = 1
6
64
xy
p
(4 −x2 −y2)
This is because in beginning calculus it was shown that the volume of a pyramid is 1/3 the
area of the base times the height. Therefore, you simply need to ﬁnd the gradient of this
and set it equal to zero. Thus upon taking the partial derivatives, you need to have
−4 + 2x2 + y2
x2y (−4 + x2 + y2)
p
(4 −x2 −y2)
= 0,
and
−4 + x2 + 2y2
xy2 (−4 + x2 + y2)
p
(4 −x2 −y2)
= 0.
Therefore, x2 + 2y2 = 4 and 2x2 + y2 = 4. Thus x = y and so x = y =
2
√
3. It follows from
the equation for z that z =
2
√
3 also. How do you know this is not the largest tetrahedron?

358
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
Example 20.1.7 An open box is to contain 32 cubic feet. Find the dimensions which will
result in the least surface area.
Let the height of the box be z and the length and width be x and y respectively. Then
xyz = 32 and so z = 32/xy. The total area is xy + 2xz + 2yz and so in terms of the two
variables, x and y, the area is
A = xy + 64
y + 64
x
To ﬁnd best dimensions you note these must result in a local minimum.
Ax = yx2 −64
x2
= 0, Ay = xy2 −64
y2
.
Therefore, yx2 −64 = 0 and xy2 −64 = 0 so xy2 = yx2. For sure the answer excludes the
case where any of the variables equals zero. Therefore, x = y and so x = 4 = y. Then z = 2
from the requirement that xyz = 32. How do you know this gives the least surface area?
Why doesn’t this give the largest surface area?
20.2
The Second Derivative Test
20.2.1
Functions Of Two Variables
In the special case of a function of two variables, f (x, y) which is the only case considered
in most calculus books, the second derivative test is given in the following theorem. It is a
black box formulation of the second derivative test.
Theorem 20.2.1 (Second Derivative Test) Let f be a function of two variables
deﬁned on an open set, U whose second order partial derivatives exist and are continuous.
That is, f ∈C2 (U) . Suppose (a, b) ∈U is a point where both partial derivatives of f
vanishes. That is fx (a, b) = fy (a, b) = 0. Let
D ≡fxx (a, b) fyy (a, b) −(fxy (a, b))2 .
Then
1. If D > 0 and fxx (a, b) < 0, then f has a local maximum at (a, b).
2. If D > 0 and fxx (a, b) > 0, then f has a local mimimum at (a, b) .
3. If D < 0, then f has a saddle point at (a, b) .
4. If D = 0, the test fails.
The above is really a statement about the eigenvalues of the Hessian matrix,
H ≡
µ
fxx (a, b)
fx,y (a, b)
fx,y (a, b)
fyy (a, b)
¶
at a point (a, b) where the partial derivatives of f vanish. It reduces to the following much
simpler statement.
If both eigenvalues of H are positive, then f has a local minimum
at (a, b) . If both eigenvalues are negative, then f has a local maximum at (a, b). If one
eigenvalue is positive and one is negative, then you have a saddle point at (a, b) . If at
least one eigenvalue equals zero, then the test fails. Here is a picture which may help you
remember this second version of this test.

20.2.
THE SECOND DERIVATIVE TEST
359
Use whichever version of this theorem you ﬁnd easiest to remember. However, in the case
of a function of many variables, the description I just gave has an obvious generalization.
This is presented next. If you are not interested in it, I think you can skip it because it isn’t
included in the book for the course.
20.2.2
Functions Of Many Variables∗
There is a version of the second derivative test for a function of many variables in the case
that the function and its ﬁrst and second partial derivatives are all continuous. A discussion
of its proof is given in Section 21.4.
Deﬁnition 20.2.2 The matrix, H (x) whose ijth entry at the point x is
∂2f
∂xi∂xj (x)
is called the Hessian matrix.
The following theorem says that if all the eigenvalues of the Hessian matrix at a critical
point are positive, then the critical point is a local minimum. If all the eigenvalues of the
Hessian matrix at a critical point are negative, then the critical point is a local maximum.
Finally, if some of the eigenvalues of the Hessian matrix at the critical point are positive and
some are negative then the critical point is a saddle point. The following picture illustrates
the situation.
Theorem 20.2.3 Let f : U →R for U an open set in Rn and let f be a C2
function and suppose that at some x ∈U, ∇f (x) = 0. Also let µ and λ be respectively, the
largest and smallest eigenvalues of the matrix, H (x) . If λ > 0 then f has a local minimum
at x. If µ < 0 then f has a local maximum at x. If either λ or µ equals zero, the test
fails. If λ < 0 and µ > 0 there exists a direction in which when f is evaluated on the
line through the critical point having this direction, the resulting function of one variable
has a local minimum and there exists a direction in which when f is evaluated on the line
through the critical point having this direction, the resulting function of one variable has a
local maximum. This last case is called a saddle point.
Example 20.2.4 Let f (x, y) = 2x4 −4x3 +14x2 +12yx2 −12yx−12x+2y2 +4y +2. Find
the critical points and determine whether they are local minima, local maxima, or saddle
points.
fx (x, y) = 8x3 −12x2 + 28x + 24yx −12y −12 and fy (x, y) = 12x2 −12x + 4y + 4. The
points at which both fx and fy equal zero are
¡ 1
2, −1
4
¢
, (0, −1) , and (1, −1) .

360
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
The Hessian matrix is
µ
24x2 + 28 + 24y −24x
24x −12
24x −12
4
¶
.
and the thing to determine is the sign of its eigenvalues evaluated at the critical points.
First consider the point
¡ 1
2, −1
4
¢
. This matrix is
µ
16
0
0
4
¶
and its eigenvalues are 16, 4
showing that this is a local minimum.
Next consider (0, −1) at this point the Hessian matrix is
µ
4
−12
−12
4
¶
and the eigen-
values are 16, −8. Therefore, this point is a saddle point.
Finally consider the point (1, −1) . At this point the Hessian is
µ
4
12
12
4
¶
and the
eigenvalues are 16, −8 so this point is also a saddle point.
The geometric signiﬁcance of a saddle point was explained above. In one direction it
looks like a local minimum while in another it looks like a local maximum. In fact, they
do look like a saddle. Here is a picture of the graph of the above function near the saddle
point, (0, −1) .
–0.1
0
0.1
0.2
x
2
–1.1
–1
–0.9
–0.8
y
4
2
0
2
4
6
You see it is a lot like the place where you sit on a saddle. If you want to get a better
picture, you could graph instead
f (x, y) = arctan
¡
2x4 −4x3 + 14x2 + 12yx2 −12yx −12x + 2y2 + 4y + 2
¢
.
Since arctan is a strictly increasing function, it preserves all the information about whether
the given function is increasing or decreasing in certain directions. Below is a graph of this
function which illustrates the behavior near the point (1, −1) .

20.2.
THE SECOND DERIVATIVE TEST
361
2
–1
0
1
2
x
–2
–1
0
1
y
5
1
5
0
5
1
5
Or course sometimes the second derivative test is inadequate to determine what is going
on. This should be no surprise since this was the case even for a function of one variable.
For a function of two variables, a nice example is the Monkey saddle.
Example 20.2.5 Suppose f (x, y) = arctan
¡
6xy2 −2x3 −3y4¢
. Show (0, 0) is a critical
point for which the second derivative test gives no information.
Before doing anything it might be interesting to look at the graph of this function of two
variables plotted using Maple.
This picture should indicate why this is called a monkey saddle. It is because the monkey
can sit in the saddle and have a place for his tail. Now to see (0, 0) is a critical point, note
that
∂(arctan (g (x, y)))
∂x
=
1
1 + g (x, y)2 gx (x, y)
and that a similar formula holds for the partial derivative with respect to y. Therefore, it
suﬃces to verify that for
g (x, y) = 6xy2 −2x3 −3y4
gx (0, 0) = gy (0, 0) = 0.
gx (x, y) = 6y2 −6x2, gy (x, y) = 12xy −12y3

362
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
and clearly (0, 0) is a critical point. So are (1, 1) and (1, −1) . Now gxx (0, 0) = 0 and so does
gxy (0, 0) and gyy (0, 0) . This implies fxx, fxy, fyy are all equal to zero at (0, 0) also. (Why?)
Therefore, the Hessian matrix is the zero matrix and clearly has only the zero eigenvalue.
Therefore, the second derivative test is totally useless at this point.
However, suppose you took x = t and y = t and evaluated this function on this line.
This reduces to h (t) = f (t, t) = arctan(4t3 −3t4), which is strictly increasing near t = 0.
This shows the critical point, (0, 0) of f is neither a local max. nor a local min. Next let
x = 0 and y = t. Then p (t) ≡f (0, t) = −3t4. Therefore, along the line, (0, t) , f has a local
maximum at (0, 0) .
The following example is for a function of three variables.
Example 20.2.6 Find the critical points of the following function of three variables and
classify them as local minimums, local maximums or saddle points.
f (x, y, z) = 5
6x2 + 4x + 16 −7
3xy −4y −4
3xz + 12z + 5
6y2 −4
3zy + 1
3z2
First you need to locate the critical points. This involves taking the gradient.
∇
µ5
6x2 + 4x + 16 −7
3xy −4y −4
3xz + 12z + 5
6y2 −4
3zy + 1
3z2
¶
=
µ5
3x + 4 −7
3y −4
3z, −7
3x −4 + 5
3y −4
3z, −4
3x + 12 −4
3y + 2
3z
¶
Next you need to set the gradient equal to zero and solve the equations. This yields y =
5, x = 3, z = −2. Now to use the second derivative test, you assemble the Hessian matrix
which is


5
3
−7
3
−4
3
−7
3
5
3
−4
3
−4
3
−4
3
2
3

.
Note that in this simple example, the Hessian matrix is constant and so all that is left
is to consider the eigenvalues. Writing the characteristic equation and solving yields the
eigenvalues are 2, −2, 4. Thus the given point is a saddle point.
20.3
Lagrange Multipliers, Constrained Extrema 31 Oct.
Lagrange multipliers are used to solve extremum problems for a function deﬁned on a level
set of another function. For example, suppose you want to maximize xy given that x+y = 4.
This is not too hard to do using methods developed earlier. Solve for one of the variables,
say y, in the constraint equation, x+y = 4 to ﬁnd y = 4−x. Then the function to maximize
is f (x) = x (4 −x) and the answer is clearly x = 2. Thus the two numbers are x = y = 2.
This was easy because you could easily solve the constraint equation for one of the variables
in terms of the other. Now what if you wanted to maximize f (x, y, z) = xyz subject to the
constraint that x2 +y2 +z2 = 4? It is still possible to do this using using similar techniques.
Solve for one of the variables in the constraint equation, say z, substitute it into f, and then
ﬁnd where the partial derivatives equal zero to ﬁnd candidates for the extremum. However,
it seems you might encounter many cases and it does look a little fussy. However, sometimes
you can’t solve the constraint equation for one variable in terms of the others. Also, what
if you had many constraints. What if you wanted to maximize f (x, y, z) subject to the
constraints x2 + y2 = 4 and z = 2x + 3y2. Things are clearly getting more involved and

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
363
messy. It turns out that at an extremum, there is a simple relationship between the gradient
of the function to be maximized and the gradient of the constraint function. This relation
can be seen geometrically as in the following picture.
¤
¤
q
(x0, y0, z0)
¤
¤
¤
¤
p
∇g(x0, y0, z0)
∇f(x0, y0, z0)
In the picture, the surface represents a piece of the level surface of g (x, y, z) = 0 and
f (x, y, z) is the function of three variables which is being maximized or minimized on the
level surface and suppose the extremum of f occurs at the point (x0, y0, z0) . As shown
above, ∇g (x0, y0, z0) is perpendicular to the surface or more precisely to the tangent plane.
However, if x (t) = (x (t) , y (t) , z (t)) is a point on a smooth curve which passes through
(x0, y0, z0) when t = t0, then the function, h (t) = f (x (t) , y (t) , z (t)) must have either a
maximum or a minimum at the point, t = t0. Therefore, h′ (t0) = 0. But this means
0
=
h′ (t0) = ∇f (x (t0) , y (t0) , z (t0)) · x′ (t0)
=
∇f (x0, y0, z0) · x′ (t0)
and since this holds for any such smooth curve, ∇f (x0, y0, z0) is also perpendicular to the
surface. This picture represents a situation in three dimensions and you can see that it is
intuitively clear that this implies ∇f (x0, y0, z0) is some scalar multiple of ∇g (x0, y0, z0).
Thus
∇f (x0, y0, z0) = λ∇g (x0, y0, z0)
This λ is called a Lagrange multiplier after Lagrange who considered such problems in
the 1700’s. I think he did it in the context of the calculus of variations in the presence of
constraints.
Of course the above argument is at best only heuristic. It does not deal with the question
of existence of smooth curves lying in the constraint surface passing through (x0, y0, z0) .
Nor does it consider all cases, being essentially conﬁned to three dimensions. In addition to
this, it fails to consider the situation in which there are many constraints. However, I think
it is likely a geometric notion like that presented above which led Lagrange to formulate the
method.
Example 20.3.1 Maximize xyz subject to x2 + y2 + z2 = 27.

364
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
Here f (x, y, z) = xyz while g (x, y, z) = x2+y2+z2−27. Then ∇g (x, y, z) = (2x, 2y, 2z)
and ∇f (x, y, z) = (yz, xz, xy) . Then at the point which maximizes this function1,
(yz, xz, xy) = λ (2x, 2y, 2z) .
Therefore, each of 2λx2, 2λy2, 2λz2 equals xyz. It follows that at any point which maximizes
xyz, |x| = |y| = |z| . Therefore, the only candidates for the point where the maximum occurs
are (3, 3, 3) , (−3, −3, 3) (−3, 3, 3) , etc. The maximum occurs at (3, 3, 3) which can be veriﬁed
by plugging in to the function which is being maximized.
The method of Lagrange multipliers allows you to consider maximization of functions
deﬁned on closed and bounded sets.
Recall that any continuous function deﬁned on a
closed and bounded set has a maximum and a minimum on the set. Candidates for the
extremum on the interior of the set can be located by setting the gradient equal to zero.
The consideration of the boundary can then sometimes be handled with the method of
Lagrange multipliers.
Example 20.3.2 Maximize f (x, y) = xy + y subject to the constraint, x2 + y2 ≤1.
Here I know there is a maximum because the set is the closed circle, a closed and bounded
set. Therefore, it is just a matter of ﬁnding it. Look for singular points on the interior of
the circle. ∇f (x, y) = (y, x + 1) = (0, 0) . There are no points on the interior of the circle
where the gradient equals zero. Therefore, the maximum occurs on the boundary of the
circle. That is the problem reduces to maximizing xy + y subject to x2 + y2 = 1. From the
above,
(y, x + 1) −λ (2x, 2y) = 0.
Hence y2 −2λxy = 0 and x (x + 1) −2λxy = 0 so y2 = x (x + 1). Therefore from the
constraint, x2 + x (x + 1) = 1 and the solution is x = −1, x = 1
2. Then the candidates for a
solution are (−1, 0) ,
³
1
2,
√
3
2
´
,
³
1
2, −
√
3
2
´
. Then
f (−1, 0) = 0, f
Ã
1
2,
√
3
2
!
= 3
√
3
4 , f
Ã
1
2, −
√
3
2
!
= −3
√
3
4 .
It follows the maximum value of this function is 3
√
3
4
and it occurs at
³
1
2,
√
3
2
´
. The minimum
value is −3
√
3
4
and it occurs at
³
1
2, −
√
3
2
´
.
Example 20.3.3 Find candidates for the maximum and minimum values of the function,
f (x, y) = xy −x2 on the set,
©
(x, y) : x2 + 2xy + y2 ≤4
ª
.
First, the only point where ∇f equals zero is (x, y) = (0, 0) and this is in the desired set.
In fact it is an interior point of this set. This takes care of the interior points. What about
those on the boundary x2 + 2xy + y2 = 4? The problem is to maximize xy −x2 subject to
the constraint, x2 + 2xy + y2 = 4. The Lagrangian is xy −x2 −λ
¡
x2 + 2xy + y2 −4
¢
and
this yields the following system.
y −2x −λ (2x + 2y) = 0
x −2λ (x + y) = 0
x2 + 2xy + y2 = 4
1There exists such a point because the sphere is closed and bounded.

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
365
From the ﬁrst two equations,
(2 + 2λ) x −(1 −2λ) y = 0
(1 −2λ) x −2λy = 0
Since not both x and y equal zero, it follows
det
µ 2 + 2λ
2λ −1
1 −2λ
−2λ
¶
= 0
which yields
λ = 1/8
Therefore,
y = 3x
(20.1)
From the constraint equation,
x2 + 2x (3x) + (3x)2 = 4
and so
x = 1
2 or −1
2
Now from 20.1, the points of interest on the boundary of this set are
µ1
2, 3
2
¶
, and
µ
−1
2, −3
2
¶
.
(20.2)
f
µ1
2, 3
2
¶
=
µ1
2
¶ µ3
2
¶
−
µ1
2
¶2
=
1
2
f
µ
−1
2, −3
2
¶
=
µ
−1
2
¶ µ
−3
2
¶
−
µ
−1
2
¶2
=
1
2
It follows the candidates for maximum and minimum are
¡ 1
2, 3
2
¢
, (0, 0) , and
¡
−1
2, −3
2
¢
.
Therefore it appears that (0, 0) yields a minimum and either
¡ 1
2, 3
2
¢
or
¡
−1
2, −3
2
¢
yields a
maximum. However, this is a little misleading. How do you even know a maximum or a
minimum exists? The set, x2 + 2xy + y2 ≤4 is an unbounded set which lies between the
two lines x + y = 2 and x + y = −2. In fact there is no minimum. For example, take
x = 100, y = −98. Then xy −x2 = x (y −x) = 100 (−98 −100) which is a large negative
number much less than 0, the answer for the point (0, 0).
There are no magic bullets here. It was still required to solve a system of nonlinear
equations to get the answer. However, it does often help to do it this way.
The above generalizes to a general procedure which is described in the following major
Theorem. All correct proofs of this theorem will involve some appeal to the implicit or
inverse function theorem or to fundamental existence theorems from diﬀerential equations.
A complete proof is very fascinating but it will not come cheap. Good advanced calculus
books will usually give a correct proof. I have also given a complete proof later starting on
Page 389. First here is a simple deﬁnition explaining one of the terms in the statement of
this theorem.

366
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
Deﬁnition 20.3.4 Let A be an m × n matrix. A submatrix is any matrix which
can be obtained from A by deleting some rows and some columns.
Theorem 20.3.5 Let U be an open subset of Rn and let f : U →R be a C1
function. Then if x0 ∈U is either a local maximum or local minimum of f subject to the
constraints
gi (x) = 0, i = 1, · · ·, m
(20.3)
and if some m × m submatrix of
Dg (x0) ≡



g1x1 (x0)
g1x2 (x0)
· · ·
g1xn (x0)
...
...
...
gmx1 (x0)
gmx2 (x0)
· · ·
gmxn (x0)



has nonzero determinant, then there exist scalars, λ1, · · ·, λm such that



fx1 (x0)
...
fxn (x0)


= λ1



g1x1 (x0)
...
g1xn (x0)


+ · · · + λm



gmx1 (x0)
...
gmxn (x0)



(20.4)
holds.
To help remember how to use 20.4 it may be helpful to do the following. First write the
Lagrangian,
L = f (x) −
m
X
i=1
λigi (x)
and then proceed to take derivatives with respect to each of the components of x and also
derivatives with respect to each λi and set all of these equations equal to 0. The formula 20.4
is what results from taking the derivatives of L with respect to the components of x. When
you take the derivatives with respect to the Lagrange multipliers, and set what results equal
to 0, you just pick up the constraint equations. This yields n + m equations for the n + m
unknowns, x1, · · ·, xn, λ1, · · ·, λm. Then you proceed to look for solutions to these equations.
Of course these might be impossible to ﬁnd using methods of algebra, but you just do your
best and hope it will work out.
Example 20.3.6 Minimize xyz subject to the constraints x2 + y2 + z2 = 4 and x −2y = 0.
Form the Lagrangian,
L = xyz −λ
¡
x2 + y2 + z2 −4
¢
−µ (x −2y)
and proceed to take derivatives with respect to every possible variable, leading to the fol-
lowing system of equations.
yz −2λx −µ
=
0
xz −2λy + 2µ
=
0
xy −2λz
=
0
x2 + y2 + z2
=
4
x −2y
=
0
Now you have to ﬁnd the solutions to this system of equations. In general, this could be
very hard or even impossible. If λ = 0, then from the third equation, either x or y must

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
367
equal 0. Therefore, from the ﬁrst two equations, µ = 0 also. If µ = 0 and λ ̸= 0, then from
the ﬁrst two equations, xyz = 2λx2 and xyz = 2λy2 and so either x = y or x = −y, which
requires that both x and y equal zero thanks to the last equation. But then from the fourth
equation, z = ±2 and now this contradicts the third equation. Thus µ and λ are either both
equal to zero or neither one is and the expression, xyz equals zero in this case. However, I
know this is not the best value for a minimizer because I can take x = 2
q
3
5, y =
q
3
5, and
z = −1. This satisﬁes the constraints and the product of these numbers equals a negative
number. Therefore, both µ and λ must be non zero. Now use the last equation eliminate x
and write the following system.
5y2 + z2 = 4
y2 −λz = 0
yz −λy + µ = 0
yz −4λy −µ = 0
From the last equation, µ = (yz −4λy) . Substitute this into the third and get
5y2 + z2 = 4
y2 −λz = 0
yz −λy + yz −4λy = 0
y = 0 will not yield the minimum value from the above example. Therefore, divide the last
equation by y and solve for λ to get λ = (2/5) z. Now put this in the second equation to
conclude
5y2 + z2 = 4
y2 −(2/5) z2 = 0 ,
a system which is easy to solve.
Thus y2 = 8/15 and z2 = 4/3. Therefore, candidates
for minima are
³
2
q
8
15,
q
8
15, ±
q
4
3
´
, and
³
−2
q
8
15, −
q
8
15, ±
q
4
3
´
, a choice of 4 points to
check. Clearly the one which gives the smallest value is
Ã
2
r
8
15,
r
8
15, −
r
4
3
!
or
³
−2
q
8
15, −
q
8
15, −
q
4
3
´
and the minimum value of the function subject to the constraints
is −2
5
√
30 −2
3
√
3.
You should rework this problem ﬁrst solving the second easy constraint for x and then
producing a simpler problem involving only the variables y and z.
20.3.1
Exercises With Answers
1. Maximize x + 3y −6z subject to the constraint, x2 + 2y2 + z2 = 9.
The Lagrangian is L = x + 3y −6z −λ
¡
x2 + 2y2 + z2 −9
¢
. Now take the derivative
with respect to x. This gives the equation 1 −2λx = 0. Next take the derivative with
respect to y. This gives the equation 3 −4λy = 0. The derivative with respect to
z gives −6 −2λz = 0. Clearly λ ̸= 0 since this would contradict the ﬁrst of these
equations. Similarly, none of the variables, x, y, z can equal zero. Solving each of
these equations for λ gives
1
2x =
3
4y =
−3
z . Thus y =
3x
2 and z = −6x. Now you
use the constraint equation plugging in these values for y and z.
x2 + 2
¡ 3x
2
¢2 +
(−6x)2 = 9. This gives the values for x as x =
3
83
√
166, x = −3
83
√
166. From the

368
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
three equations above, this also determines the values of z and y.
y =
9
166
√
166 or
−9
166
√
166 and z = −18
83
√
166 or 18
83
√
166. Thus there are two points to look at. One
will give the minimum value and the other will give the maximum value. You know the
minimum and maximum exist because of the extreme value theorem. The two points
are
¡ 3
83
√
166,
9
166
√
166, −18
83
√
166
¢
and
¡
−3
83
√
166, −9
166
√
166, 18
83
√
166
¢
. Now you just
need to ﬁnd which is the minimum and which is the maximum. Plug these in to the
function you are trying to maximize.
¡ 3
83
√
166
¢
+ 3
¡ 9
166
√
166
¢
−6
¡
−18
83
√
166
¢
will
clearly be the maximum value occuring at
¡ 3
83
√
166,
9
166
√
166, −18
83
√
166
¢
. The other
point will obviously yield the minimum because this one is positive and the other one
is negative. If you use a calculator to compute this you get
¡ 3
83
√
166
¢
+3
¡ 9
166
√
166
¢
−
6
¡
−18
83
√
166
¢
= 19. 326.
2. Find the dimensions of the largest rectangle which can be inscribed in a the ellipse
x2 + 4y2 = 4.
This is one which you could do without Lagrange multipliers. However, it is easier with
Lagrange multipliers. Let a corner of the rectangle be at (x, y) . Then the area of the
rectangle will be 4xy and since (x, y) is on the ellipse, you have the constraint x2+4y2 =
4. Thus the problem is to maximize 4xy subject to x2+4y2 = 4. The Lagrangian is then
L = 4xy−λ
¡
x2 + 4y2 −4
¢
and so you get the equations 4y−2λx = 0 and 4x−8λy = 0.
You can’t have both x and y equal to zero and satisfy the constraint. Therefore, the
determinant of the matrix of coeﬃcients must equal zero.
Thus
¯¯¯¯
−2λ
4
4
−8λ
¯¯¯¯ =
16λ2 −16 = 0. This is because the system of equations is of the form
µ
−2λ
4
4
−8λ
¶ µ
x
y
¶
=
µ
0
0
¶
.
If the matrix has an inverse, then the only solution would be x = y = 0 which as
noted above can’t happen. Therefore, λ = ±1. First suppose λ = 1. Then the ﬁrst
equation says 2y = x. Pluggin this in to the constraint equation, x2 + x2 = 4 and so
x = ±
√
2. Therefore, y = ±
√
2
2 . This yields the dimensions of the largest rectangle to
be 2
√
2 ×
√
2.
You can check all the other cases and see you get the same thing in
the other cases as well.
3. Maximize 2x + y subject to the condition that x2
4 + y2 ≤1.
The maximum of this function clearly exists because of the extreme value theorem
since the condition deﬁnes a closed and bounded set in R2. However, this function
does not achieve its maximum on the interior of the given ellipse deﬁned by x2
4 +y2 ≤1
because the gradient of the function which is to be maximized is never equal to zero.
Therefore, this function must achieve its maximum on the set x2
4 + y2 = 1. Thus you
want to maximuze 2x + y subject to x2
4 + y2 = 1. This is just like Problem 1. You can
ﬁnish this.
4. Find the points on y2x = 16 which are closest to (0, 0) .
You want to minimize x2 + y2 subject to y2x −16. Of course you really want to
minimize
p
x2 + y2 but the ordered pair which minimized x2 + y2 is the same as the
ordered pair which minimize
p
x2 + y2 so it is pointless to drag around the square
root. The Lagrangian is x2 + y2 −λ
¡
y2x −16
¢
. Diﬀerentiating with respect to x and
y gives the equations 2x −λy2 = 0 and 2y −2λyx = 0. Neither x nor y can equal
zero and solve the constraint. Therefore, the second equation implies λx = 1. Hence
λ = 1
x = 2x
y2 . Therefore, 2x2 = y2 and so 2x3 = 16 and so x = 2. Therefore, y = ±2
√
2.

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
369
The points are
¡
2, 2
√
2
¢
and
¡
2, −2
√
2
¢
. They both give the same answer. Note how ad
hoc these procedures are. I can’t give you a simple strategy for solving these systems
of nonlinear equations by algebra because there is none. Sometimes nothing you do
will work.
5. Find points on xy = 1 farthest from (0, 0) if any exist. If none exist, tell why. What
does this say about the method of Lagrange multipliers?
If you graph xy = 1 you see there is no farthest point. However, there is a closest
point and the method of Lagrange multipliers will ﬁnd this closest point. This shows
that the answer you get has to be carefully considered to determine whether you have
a maximum or a minimum or perhaps neither.
6. A curve is formed from the intersection of the plane, 2x + y + z = 3 and the cylinder
x2 + y2 = 4. Find the point on this curve which is closest to (0, 0, 0) .
You want to maximize x2 + y2 + z2 subject to the two constraints 2x + y + z = 3 and
x2 + y2 = 4. This means the Lagrangian will have two multipliers.
L = x2 + y2 + z2 −λ (2x + y + z −3) −µ
¡
x2 + y2 −4
¢
Then this yields the equations 2x −2λ −2µx = 0, 2y −λ −2µy, and 2z −λ = 0. The
last equation says λ = 2z and so I will replace λ with 2z where ever it occurs. This
yields
x −2z −µx = 0, 2y −2z −2µy = 0.
This shows x (1 −µ) = 2y (1 −µ) . First suppose µ = 1. Then from the above equa-
tions, z = 0 and so the two constraints reduce to 2y + x = 3 and x2 + y2 = 4 and
2y + x = 3. The solutions are
¡ 3
5 −2
5
√
11, 6
5 + 1
5
√
11, 0
¢
,
¡ 3
5 + 2
5
√
11, 6
5 −1
5
√
11, 0
¢
.
The other case is that µ ̸= 1 in which case x = 2y and the second constraint
yields that y = ± 2
√
5 and x = ± 4
√
5. Now from the ﬁrst constraint, z = −2
√
5 + 3
in the case where y =
2
√
5 and z = 2
√
5 + 3 in the other case.
This yields the
points
³
4
√
5,
2
√
5, −2
√
5 + 3
´
and
³
−4
√
5, −2
√
5, 2
√
5 + 3
´
. This appears to have ex-
hausted all the possibilities and so it is now just a matter of seeing which of these
points gives the best answer.
An answer exists because of the extreme value the-
orem.
After all, this constraint set is closed and bounded.
The ﬁrst candidate
listed above yields for the answer
¡ 3
5 −2
5
√
11
¢2 +
¡ 6
5 + 1
5
√
11
¢2 = 4. The second can-
didate listed above yields
¡ 3
5 + 2
5
√
11
¢2 +
¡ 6
5 −1
5
√
11
¢2 = 4 also.
Thus these two
give equally good results. Now consider the last two candidates.
³
4
√
5
´2
+
³
2
√
5
´2
+
¡
−2
√
5 + 3
¢2 = 4 +
¡
−2
√
5 + 3
¢2 which is larger than 4. Finally the last candidate
yields
³
−4
√
5
´2
+
³
−2
√
5
´2
+
¡
2
√
5 + 3
¢2 = 4 +
¡
2
√
5 + 3
¢2 also larger than 4. There-
fore, there are two points on the curve of intersection which are closest to the origin,
¡ 3
5 −2
5
√
11, 6
5 + 1
5
√
11, 0
¢
and
¡ 3
5 + 2
5
√
11, 6
5 −1
5
√
11, 0
¢
. Both are a distance of 4 from
the origin.
7. Here are two lines. x = (1 + 2t, 2 + t, 3 + t)T and x = (2 + s, 1 + 2s, 1 + 3s)T . Find
points p1 on the ﬁrst line and p2 on the second with the property that |p1 −p2| is at
least as small as the distance between any other pair of points, one chosen on one line
and the other on the other line.
Hint: Do you need to use Lagrange multipliers for this?

370
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
8. Find the point on x2 + y2 + z2 = 1 closest to the plane x + y + z = 10.
You want to minimize (x −a)2+(y −b)2+(z −c)2 subject to the constraints a+b+c =
10 and x2 + y2 + z2 = 1. There seem to be a lot of variables in this problem, 6 in
all. Start taking derivatives and hope for a miracle. This yields 2 (x −a) −2µx =
0, 2 (y −b)−2µy = 0, 2 (z −c)−2µz = 0. Also, taking derivatives with respect to a, b,
and c you obtain 2 (x −a) + λ = 0, 2 (y −b) + λ = 0, 2 (z −c) + λ = 0. Comparing
the ﬁrst equations in each list, you see λ = 2µx and then comparing the second two
equations in each list, λ = 2µy and similarly, λ = 2µz. Therefore, if µ ̸= 0, it must
follow that x = y = z. Now you can see by sketching a rough graph that the answer
you want has each of x, y, and z nonnegative. Therefore, using the constraint for
these variables, the point desired is
³
1
√
3,
1
√
3,
1
√
3
´
which you could probably see was
the answer from the sketch. However, this could be made more diﬃcult rather easily
such that the sketch won’t help but Lagrange multipliers will.

The Derivative Of Vector
Valued Functions, What Is The
Derivative?∗
If you are going to do this stuﬀ, you might as well do it right and include the case of vector
valued functions. You know everything about matrices at this point for it to all make perfect
sense. Therefore, I think it is well worth your time to read this although you are unlikely
to see it on a test. It is not any harder than what has been presented. It also tells you
what the derivative is. This is essential information if you are going to understand Newton’s
method for nonlinear systems. It is also essential if you want to read a really good book on
continuum mechanics and is needed in many other physical and engineering applications.
Also included is a proof of the second derivative test.
Recall the following deﬁnition.
Deﬁnition 21.0.7 A function, T which maps Rn to Rp is called a linear transfor-
mation if for every pair of scalars, a, b and vectors, x, y ∈Rn, it follows that T (ax + by) =
aT (x) + bT (y) .
Recall that from the properties of matrix multiplication, it follows that if A is an n × p
matrix, and if x, y are vectors in Rn, then A (ax + by) = aA (x) + bA (y) . Thus you can
deﬁne a linear transformation by multiplying by a matrix. Of course the simplest example is
that of a 1×1 matrix or number. You can think of the number 3 as a linear transformation,
T mapping R to R according to the rule Tx = 3x. It satisﬁes the properties needed for
a linear transformation because 3 (ax + by) = a3x + b3y = aTx + bTy. The case of the
derivative of a scalar valued function of one variable is of this sort. You get a number for
the derivative. However, you can think of this number as a linear transformation. Of course
it is not worth the fuss to do so for a function of one variable but this is the way you must
think of it for a function of n variables.
Deﬁnition 21.0.8 Let f : U →Rp where U is an open set in Rn for n, p ≥1 and
let x ∈U be given. Then f is deﬁned to be diﬀerentiable at x ∈U if and only if there
exist column vectors, vi such that for h = (h1 · ··, hn)T ,
f (x + h) = f (x) +
n
X
i=1
vihi + o (h) .
(21.1)
The derivative of the function, f, denoted by Df (x) , is the linear transformation deﬁned by
multiplying by the matrix whose columns are the p × 1 vectors, vi. Thus if w is a vector in
371

372THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
Rn,
Df (x) w ≡


|
|
v1
· · ·
vn
|
|

w.
It is common to think of this matrix as the derivative but strictly speaking, this is
incorrect. The derivative is a “linear transformation” determined by multiplication by this
matrix, called the standard matrix because it is based on the standard basis vectors for
Rn. The subtle issues involved in a thorough exploration of this issue will be avoided for
now. It will be ﬁne to think of the above matrix as the derivative. Other notations which
are often used for this matrix or the linear transformation are f ′ (x) , J (x) , and even ∂f
∂x or
df
dx.
Theorem 21.0.9 Suppose f is as given above in 21.1. Then
vk = lim
h→0
f (x+hek) −f (x)
h
≡∂f
∂xk
(x) ,
the kth partial derivative.
Proof: Let h = (0, · · ·, h, 0, · · ·, 0)T = hek where the h is in the kth slot. Then 21.1
reduces to
f (x + h) = f (x) + vkh + o (h) .
Therefore, dividing by h
f (x+hek) −f (x)
h
= vk + o (h)
h
and taking the limit,
lim
h→0
f (x+hek) −f (x)
h
= lim
h→0
µ
vk + o (h)
h
¶
= vk
and so, the above limit exists. This proves the theorem.
Let f : U →Rq where U is an open subset of Rp and f is diﬀerentiable. It was just
shown
f (x + v) = f (x) +
p
X
j=1
∂f (x)
∂xj
vj + o (v) .
Taking the ith coordinate of the above equation yields
fi (x + v) = fi (x) +
p
X
j=1
∂fi (x)
∂xj
vj + o (v)
and it follows that the term with a sum is nothing more than the ith component of J (x) v
where J (x) is the q × p matrix,






∂f1
∂x1
∂f1
∂x2
· · ·
∂f1
∂xp
∂f2
∂x1
∂f2
∂x2
· · ·
∂f2
∂xp
...
...
...
...
∂fq
∂x1
∂fq
∂x2
· · ·
∂fq
∂xp






.

21.1.
C1 FUNCTIONS∗
373
This gives the form of the matrix which deﬁnes the linear transformation, Df (x) . Thus
f (x + v) = f (x) + J (x) v + o (v)
(21.2)
and to reiterate, the linear transformation which results by multiplication by this q × p
matrix is known as the derivative.
Sometimes x, y, z is written instead of x1, x2, and x3. This is to save on notation and is
easier to write and to look at although it lacks generality. When this is done it is understood
that x = x1, y = x2, and z = x3. Thus the derivative is the linear transformation determined
by


f1x
f1y
f1z
f2x
f2y
f2z
f3x
f3y
f3z

.
Example 21.0.10 Let A be a constant m×n matrix and consider f (x) = Ax. Find Df (x)
if it exists.
f (x + h) −f (x) = A (x + h) −A (x) = Ah = Ah + o (h) .
In fact in this case, o (h) = 0. Therefore, Df (x) = A. Note that this looks the same as the
case in one variable, f (x) = ax.
21.1
C1 Functions∗
Given a function of many variables, how can you tell if it is diﬀerentiable? Sometimes you
have to go directly to the deﬁnition and verify it is diﬀerentiable from the deﬁnition. For
example, you may have seen the following important example in one variable calculus.
Example 21.1.1 Let f (x) =
½
x2 sin
¡ 1
x
¢
if x ̸= 0
0 if x = 0
. Find Df (0) .
f (h) −f (0) = 0h + h2 sin
¡ 1
h
¢
= o (h) and so Df (0) = 0. If you ﬁnd the derivative
for x ̸= 0, it is totally useless information if what you want is Df (0) . This is because the
derivative, turns out to be discontinuous. Try it. Find the derivative for x ̸= 0 and try to
obtain Df (0) from it. You see, in this example you had to revert to the deﬁnition to ﬁnd
the derivative.
It isn’t really too hard to use the deﬁnition even for more ordinary examples.
Example 21.1.2 Let f (x, y) =
µ
x2y + y2
y3x
¶
. Find Df (1, 2) .
First of all note that the thing you are after is a 2 × 2 matrix.
f (1, 2) =
µ
6
8
¶
.
Then
f (1 + h1, 2 + h2) −f (1, 2)

374THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
=
µ
(1 + h1)2 (2 + h2) + (2 + h2)2
(2 + h2)3 (1 + h1)
¶
−
µ
6
8
¶
=
µ
5h2 + 4h1 + 2h1h2 + 2h2
1 + h2
1h2 + h2
2
8h1 + 12h2 + 12h1h2 + 6h2
2 + 6h2
2h1 + h3
2 + h3
2h1
¶
=
µ
4
5
8
12
¶ µ
h1
h2
¶
+
µ
2h1h2 + 2h2
1 + h2
1h2 + h2
2
12h1h2 + 6h2
2 + 6h2
2h1 + h3
2 + h3
2h1
¶
=
µ
4
5
8
12
¶ µ
h1
h2
¶
+ o (h) .
Therefore, the standard matrix of the derivative is
µ
4
5
8
12
¶
.
Most of the time, there is an easier way to conclude a derivative exists and to ﬁnd it. It
involves the notion of a C1 function.
Deﬁnition 21.1.3 When f : U →Rp for U an open subset of Rn and the vector
valued functions,
∂f
∂xi are all continuous, (equivalently each ∂fi
∂xj is continuous), the function
is said to be C1 (U) . If all the partial derivatives up to order k exist and are continuous,
then the function is said to be Ck.
It turns out that for a C1 function, all you have to do is write the matrix described in
Theorem 21.0.9 and this will be the derivative. There is no question of existence for the
derivative for such functions. This is the importance of the next theorem.
Theorem 21.1.4 Suppose f : U →Rp where U is an open set in Rn. Suppose also
that all partial derivatives of f exist on U and are continuous. Then f is diﬀerentiable at
every point of U.
Proof: If you ﬁx all the variables but one, you can apply the fundamental theorem of
calculus as follows.
f (x+vkek) −f (x) =
Z 1
0
∂f
∂xk
(x + tvkek) vkdt.
(21.3)
Here is why. Let h (t) = f (x + tvkek) . Then
h (t + h) −h (t)
h
= f (x + tvkek + hvkek) −f (x + tvkek)
hvk
vk
and so, taking the limit as h →0 yields
h′ (t) = ∂f
∂xk
(x + tvkek) vk
Therefore,
f (x+vkek) −f (x) = h (1) −h (0) =
Z 1
0
h′ (t) dt =
Z 1
0
∂f
∂xk
(x + tvkek) vkdt.
Now I will use this observation to prove the theorem. Let v = (v1, · · ·, vn) with |v|
suﬃciently small. Thus v = Pn
k=1 vkek. For the purposes of this argument, deﬁne
n
X
k=n+1
vkek ≡0.

21.1.
C1 FUNCTIONS∗
375
Then with this convention,
f (x + v) −f (x)
=
n
X
i=1
Ã
f
Ã
x+
n
X
k=i
vkek
!
−f
Ã
x+
n
X
k=i+1
vkek
!!
=
n
X
i=1
Z 1
0
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
vidt
=
n
X
i=1
Z 1
0
Ã
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
vi −∂f
∂xi
(x) vi
!
dt
+
n
X
i=1
Z 1
0
∂f
∂xi
(x) vidt
=
n
X
i=1
∂f
∂xi
(x) vi
+
Z 1
0
n
X
i=1
Ã
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
!
vidt
=
n
X
i=1
∂f
∂xi
(x) vi + o (v)
and this shows f is diﬀerentiable at x.
Some explanation of the step to the last line is in order. The messy thing at the end is
o (v) because of the continuity of the partial derivatives. In fact, from the Cauchy Schwarz
inequality,
Z 1
0
n
X
i=1
Ã
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
!
vidt
≤
Z 1
0


n
X
i=1
¯¯¯¯¯
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
¯¯¯¯¯
2

1/2
dt
Ã n
X
i=1
v2
i
!1/2
=
Z 1
0


n
X
i=1
¯¯¯¯¯
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
¯¯¯¯¯
2

1/2
dt |v|
Thus, dividing by |v| and taking a limit as |v| →0, the quotient is nothing but
Z 1
0


n
X
i=1
¯¯¯¯¯
∂f
∂xi
Ã
x+
n
X
k=i+1
vkek + tviei
!
−∂f
∂xi
(x)
¯¯¯¯¯
2

1/2
dt
which converges to 0 due to continuity of the partial derivatives of f.
This proves the
theorem.
Here is an example to illustrate.
Example 21.1.5 Let f (x, y) =
µ
x2y + y2
y3x
¶
. Find Df (x, y) .
From Theorem 21.1.4 this function is diﬀerentiable because all possible partial derivatives
are continuous. Thus
Df (x, y) =
µ
2xy
x2 + 2y
y3
3y2x
¶
.

376THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
In particular,
Df (1, 2) =
µ
4
5
8
12
¶
.
Not surprisingly, the above theorem has an extension to more variables. First this is
illustrated with an example.
Example 21.1.6 Let f (x1, x2, x3) =


x2
1x2 + x2
2
x2x1 + x3
sin (x1x2x3)

. Find Df (x1, x2, x3) .
All possible partial derivatives are continuous so the function is diﬀerentiable.
The
matrix for this derivative is therefore the following 3 × 3 matrix


2x1x2
x2
1 + 2x2
0
x2
x1
1
x2x3 cos (x1x2x3)
x1x3 cos (x1x2x3)
x1x2 cos (x1x2x3)


Example 21.1.7 Suppose f (x, y, z) = xy + z2. Find Df (1, 2, 3) .
Taking the partial derivatives of f, fx = y, fy = x, fz = 2z. These are all continuous.
Therefore, the function has a derivative and fx (1, 2, 3) = 1, fy (1, 2, 3) = 2, and fz (1, 2, 3) =
6. Therefore, Df (1, 2, 3) is given by
Df (1, 2, 3) = (1, 2, 6) .
Also, for (x, y, z) close to (1, 2, 3) ,
f (x, y, z)
≈
f (1, 2, 3) + 1 (x −1) + 2 (y −2) + 6 (z −3)
=
11 + 1 (x −1) + 2 (y −2) + 6 (z −3) = −12 + x + 2y + 6z
When a function is diﬀerentiable at x0 it follows the function must be continuous there.
This is the content of the following important lemma.
Lemma 21.1.8 Let f : U →Rq where U is an open subset of Rp. If f is diﬀerentiable,
then f is continuous at x0. Furthermore, if C ≥max
n¯¯¯ ∂f
∂xi (x0)
¯¯¯ , i = 1, · · ·, p
o
, then when-
ever |x −x0| is small enough,
|f (x) −f (x0)| ≤(Cp + 1) |x −x0|
(21.4)
Proof: Suppose f is diﬀerentiable. Since o (v) satisﬁes 19.1, there exists δ1 > 0 such
that if |x −x0| < δ1, then |o (x −x0)| < |x −x0| . But also,
¯¯¯¯¯
p
X
i=1
∂f
∂xi
(x0) (xi −x0i)
¯¯¯¯¯ ≤C
p
X
i=1
|xi −x0i| ≤Cp |x −x0|
Therefore, if |x −x0| < δ1,
|f (x) −f (x0)|
≤
¯¯¯¯¯
p
X
i=1
∂f
∂xi
(x0) (xi −x0i)
¯¯¯¯¯ + |x −x0|
<
(Cp + 1) |x −x0|
which veriﬁes 21.4. Now letting ε > 0 be given, let δ = min
³
δ1,
ε
Cp+1
´
. Then for |x −x0| <
δ,
|f (x) −f (x0)| < (Cp + 1) |x −x0| < (Cp + 1)
ε
Cp + 1 = ε
showing f is continuous at x0.

21.2.
THE CHAIN RULE∗
377
21.2
The Chain Rule∗
21.2.1
The Chain Rule For Functions Of One Variable∗
First recall the chain rule for a function of one variable. Consider the following picture.
I
g→J
f→R
Here I and J are open intervals and it is assumed that g (I) ⊆J. The chain rule says that if
f ′ (g (x)) exists and g′ (x) exists for x ∈I, then the composition, f ◦g also has a derivative
at x and
(f ◦g)′ (x) = f ′ (g (x)) g′ (x) .
Recall that f ◦g is the name of the function deﬁned by f ◦g (x) ≡f (g (x)) . In the notation
of this chapter, the chain rule is written as
Df (g (x)) Dg (x) = D (f ◦g) (x) .
(21.5)
21.2.2
The Chain Rule For Functions Of Many Variables∗
Let U ⊆Rn and V ⊆Rp be open sets and let f be a function deﬁned on V having values in
Rq while g is a function deﬁned on U such that g (U) ⊆V as in the following picture.
U
g→V
f→Rq
The chain rule says that if the linear transformations (matrices) on the left in 21.5 both
exist then the same formula holds in this more general case. Thus
Df (g (x)) Dg (x) = D (f ◦g) (x)
Note this all makes sense because Df (g (x)) is a q × p matrix and Dg (x) is a p × n matrix.
Remember it is all right to do (q × p) (p × n) . The middle numbers match. More precisely,
Theorem 21.2.1 (Chain rule) Let U be an open set in Rn, let V be an open set
in Rp, let g : U →Rp be such that g (U) ⊆V, and let f : V →Rq. Suppose Dg (x) exists
for some x ∈U and that Df (g (x)) exists. Then D (f ◦g) (x) exists and furthermore,
D (f ◦g) (x) = Df (g (x)) Dg (x) .
(21.6)
In particular,
∂(f ◦g) (x)
∂xj
=
p
X
i=1
∂f (g (x))
∂yi
∂gi (x)
∂xj
.
(21.7)
There is an easy way to remember this in terms of the repeated index summation con-
vention presented earlier. Let y = g (x) and z = f (y) . Then the above says
∂z
∂yi
∂yi
∂xk
= ∂z
∂xk
.
(21.8)
Remember there is a sum on the repeated index. In particular, for each index, r,
∂zr
∂yi
∂yi
∂xk
= ∂zr
∂xk
.
The proof of this major theorem will be given at the end of this section. It will include
the chain rule for functions of one variable as a special case. First here are some examples.

378THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
Example 21.2.2 Let f (u, v) = sin (uv) and let u (x, y, t) = t sin x+cos y and v (x, y, t, s) =
s tan x + y2 + ts. Letting z = f (u, v) where u, v are as just described, ﬁnd ∂z
∂t and ∂z
∂x.
From 21.8,
∂z
∂t = ∂z
∂u
∂u
∂t + ∂z
∂v
∂v
∂t = v cos (uv) sin (x) + us cos (uv) .
Here y1 = u, y2 = v, t = xk. Also,
∂z
∂x = ∂z
∂u
∂u
∂x + ∂z
∂v
∂v
∂x = v cos (uv) t cos (x) + us sec2 (x) cos (uv) .
Clearly you can continue in this way taking partial derivatives with respect to any of the
other variables.
Example 21.2.3 Let w = f (u1, u2) = u2 sin (u1) and u1 = x2y + z, u2 = sin (xy) . Find
∂w
∂x , ∂w
∂y , and ∂w
∂z .
The derivative of f is of the form (wx, wy, wz) and so it suﬃces to ﬁnd the derivative of f
using the chain rule. You need to ﬁnd Df (u1, u2) Dg (x, y, z) where g (x, y) =
µ
x2y + z
sin (xy)
¶
.
Then Dg (x, y, z) =
µ
2xy
x2
1
y cos (xy)
x cos (xy)
0
¶
. Also Df (u1, u2) = (u2 cos (u1) , sin (u1)) .
Therefore, the derivative is
Df (u1, u2) Dg (x, y, z) = (u2 cos (u1) , sin (u1))
µ
2xy
x2
1
y cos (xy)
x cos (xy)
0
¶
=
¡
2u2 (cos u1) xy + (sin u1) y cos xy, u2 (cos u1) x2 + (sin u1) x cos xy, u2 cos u1
¢
= (wx, wy, wz)
Thus ∂w
∂x = 2u2 (cos u1) xy+(sin u1) y cos xy = 2 (sin (xy))
¡
cos
¡
x2y + z
¢¢
xy+
¡
sin
¡
x2y + z
¢¢
y cos xy
. Similarly, you can ﬁnd the other partial derivatives of w in terms of substituting in for u1
and u2 in the above. Note
∂w
∂x = ∂w
∂u1
∂u1
∂x + ∂w
∂u2
∂u2
∂x .
In fact, in general if you have w = f (u1, u2) and g (x, y, z) =
µ
u1 (x, y, z)
u2 (x, y, z)
¶
, then
D (f ◦g) (x, y, z) is of the form
¡
wu1
wu2
¢ µ
u1x
u1y
u1z
u2x
u2y
u2z
¶
=
¡
wu1ux + wu2u2x
wu1uy + wu2u2y
wu1uz + wu2u2z
¢
.
Example 21.2.4 Let w = f (u1, u2, u3) = u2
1 + u3 + u2 and g (x, y, z) =


u1
u2
u3

=


x + 2yz
x2 + y
z2 + x

. Find ∂w
∂x and ∂w
∂z .

21.2.
THE CHAIN RULE∗
379
By the chain rule,
(wx, wy, wz) =
¡
wu1
wu2
wu3
¢


u1x
u1y
u1z
u2x
u2y
u2z
u3x
u3y
u3z


=
¡
wu1u1x + wu2u2x + wu3u3x
wu1u1y + wu2u2y + wu3u3y
wu1u1z + wu2u2z + wu3u3z
¢
Note the pattern.
wx
=
wu1u1x + wu2u2x + wu3u3x,
wy
=
wu1u1y + wu2u2y + wu3u3y,
wz
=
wu1u1z + wu2u2z + wu3u3z.
Therefore,
wx = 2u1 (1) + 1 (2x) + 1 (1) = 2 (x + 2yz) + 2x + 1 = 4x + 4yz + 1
and
wz = 2u1 (2y) + 1 (0) + 1 (2z) = 4 (x + 2yz) y + 2z = 4yx + 8y2z + 2z.
Of course to ﬁnd all the partial derivatives at once, you just use the chain rule. Thus you
would get
¡
wx
wy
wz
¢
=
¡
2u1
1
1
¢


1
2z
2y
2x
1
0
1
0
2z


=
¡
2u1 + 2x + 1
4u1z + 1
4u1y + 2z
¢
=
¡
4x + 4yz + 1
4zx + 8yz2 + 1
4yx + 8y2z + 2z
¢
Example 21.2.5 Let f (u1, u2) =
µ
u2
1 + u2
sin (u2) + u1
¶
and g (x1, x2, x3) =
µ
u1 (x1, x2, x3)
u2 (x1, x2, x3)
¶
=
µ
x1x2 + x3
x2
2 + x1
¶
. Find D (f ◦g) (x1, x2, x3) .
To do this,
Df (u1, u2) =
µ
2u1
1
1
cos u2
¶
, Dg (x1, x2, x3) =
µ
x2
x1
1
1
2x2
0
¶
.
Then
Df (g (x1, x2, x3)) =
µ 2 (x1x2 + x3)
1
1
cos
¡
x2
2 + x1
¢
¶
and so by the chain rule,
D (f ◦g) (x1, x2, x3)
=
Df(g(x))
z
}|
{
µ 2 (x1x2 + x3)
1
1
cos
¡
x2
2 + x1
¢
¶
Dg(x)
z
}|
{
µ
x2
x1
1
1
2x2
0
¶
=
µ (2x1x2 + 2x3) x2 + 1
(2x1x2 + 2x3) x1 + 2x2
2x1x2 + 2x3
x2 + cos
¡
x2
2 + x1
¢
x1 + 2x2
¡
cos
¡
x2
2 + x1
¢¢
1
¶
Therefore, in particular,
∂f1 ◦g
∂x1
(x1, x2, x3) = (2x1x2 + 2x3) x2 + 1,

380THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
∂f2 ◦g
∂x3
(x1, x2, x3) = 1, ∂f2 ◦g
∂x2
(x1, x2, x3) = x1 + 2x2
¡
cos
¡
x2
2 + x1
¢¢
.
etc.
In diﬀerent notation, let
µ
z1
z2
¶
= f (u1, u2) =
µ
u2
1 + u2
sin (u2) + u1
¶
. Then
∂z1
∂x1
= ∂z1
∂u1
∂u1
∂x1
+ ∂z1
∂u2
∂u2
∂x1
= 2u1x2 + 1 = 2 (x1x2 + x3) x2 + 1.
Example 21.2.6 Let f (u1, u2, u3) =


z1
z2
z3

=


u2
1 + u2u3
u2
1 + u3
2
ln
¡
1 + u2
3
¢

and let g (x1, x2, x3, x4) =


u1
u2
u3

=


x1 + x2
2 + sin (x3) + cos (x4)
x2
4 −x1
x2
3 + x4

. Find (f ◦g)′ (x) .
Df (u) =



2u1
u3
u2
2u1
3u2
2
0
0
0
2u3
(1+u2
3)



Similarly,
Dg (x) =


1
2x2
cos (x3)
−sin (x4)
−1
0
0
2x4
0
0
2x3
1

.
Then by the chain rule, D (f ◦g) (x) = Df (u) Dg (x) where u = g (x) as described above.
Thus D (f ◦g) (x) =



2u1
u3
u2
2u1
3u2
2
0
0
0
2u3
(1+u2
3)





1
2x2
cos (x3)
−sin (x4)
−1
0
0
2x4
0
0
2x3
1


=


2u1 −u3
4u1x2
2u1 cos x3 + 2u2x3
−2u1 sin x4 + 2u3x4 + u2
2u1 −3u2
2
4u1x2
2u1 cos x3
−2u1 sin x4 + 6u2
2x4
0
0
4
u3
1+u2
3 x3
2
u3
1+u2
3


(21.9)
where each ui is given by the above formulas. Thus ∂z1
∂x1 equals
2u1 −u3
=
2
¡
x1 + x2
2 + sin (x3) + cos (x4)
¢
−
¡
x2
3 + x4
¢
=
2x1 + 2x2
2 + 2 sin x3 + 2 cos x4 −x2
3 −x4.
while ∂z2
∂x4 equals
−2u1 sin x4 + 6u2
2x4 = −2
¡
x1 + x2
2 + sin (x3) + cos (x4)
¢
sin (x4) + 6
¡
x2
4 −x1
¢2 x4.
If you wanted
∂z
∂x2 it would be the second column of the above matrix in 21.9. Thus
∂z
∂x2
equals



∂z1
∂x2
∂z2
∂x2
∂z3
∂x2


=


4u1x2
4u1x2
0

=


4
¡
x1 + x2
2 + sin (x3) + cos (x4)
¢
x2
4
¡
x1 + x2
2 + sin (x3) + cos (x4)
¢
x2
0

.
I hope that by now it is clear that all the information you could desire about various partial
derivatives is available and it all reduces to matrix multiplication and the consideration of
entries of the matrix obtained by multiplying the two derivatives.

21.2.
THE CHAIN RULE∗
381
21.2.3
The Derivative Of The Inverse Function∗
Example 21.2.7 Let f : U →V where U and V are open sets in Rnand f is one to one and
onto. Suppose also that f and f −1 are both diﬀerentiable. How are Df −1 and Df related?
This can be done as follows. From the assumptions, x = f −1 (f (x)) . Let Ix = x. Then
by Example 21.0.10 on Page 373 DI = I. By the chain rule,
I = DI = Df −1 (f (x)) (Df (x)) .
Therefore,
Df (x)−1 = Df −1 (f (x)) .
This is equivalent to
Df
¡
f −1 (y)
¢−1 = Df −1 (y)
or
Df (x)−1 = Df −1 (y) , y = f (x) .
This is just like a similar situation for functions of one variable. Remember
¡
f −1¢′ (f (x)) =
1/f ′ (x) . In terms of the repeated index summation convention, suppose y = f (x) so that
x = f −1 (y) . Then the above can be written as
δij = ∂xi
∂yk
(f (x)) ∂yk
∂xj
(x) .
21.2.4
Acceleration In Spherical Coordinates∗
This is an interesting example which can be done with more elegance in a more general
setting. However, the more general approach also depends on the chain rule and this is
what it is all about, giving examples of the use of the chain rule. Read it if it interests you.
Example 21.2.8 Recall spherical coordinates are given by
x = ρ sin φ cos θ, y = ρ sin φ sin θ, z = ρ cos φ.
If an object moves in three dimensions, describe its acceleration in terms of spherical coor-
dinates and the vectors,
eρ = (sin φ cos θ, sin φ sin θ, cos φ)T ,
eθ = (−ρ sin φ sin θ, ρ sin φ cos θ, 0)T ,
and
eφ = (ρ cos φ cos θ, ρ cos φ sin θ, −ρ sin φ)T .
Why these vectors? Note how they were obtained. Let
r (ρ, θ, φ) = (ρ sin φ cos θ, ρ sin φ sin θ, ρ cos φ)T
and ﬁx φ and θ, letting only ρ change, this gives a curve in the direction of increasing ρ.
Thus it is a vector which points away from the origin. Letting only φ change and ﬁxing θ and
ρ, this gives a vector which is tangent to the sphere of radius ρ and points South. Similarly,
letting θ change and ﬁxing the other two gives a vector which points East and is tangent to
the sphere of radius ρ. It is thought by most people that we live on a large sphere. The model
of a ﬂat earth is not believed by anyone except perhaps beginning physics students. Given we
live on a sphere, what directions would be most meaningful? Wouldn’t it be the directions of
the vectors just described?

382THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
Let r (t) denote the position vector of the object from the origin. Thus
r (t) = ρ (t) eρ (t) =
³
(x (t) , y (t) , z (t))T ´
Now this implies the velocity is
r′ (t) = ρ′ (t) eρ (t) + ρ (t) (eρ (t))′ .
(21.10)
You see, eρ = eρ (ρ, θ, φ) where each of these variables is a function of t.
∂eρ
∂φ = (cos φ cos θ, cos φ sin θ, −sin φ)T = 1
ρeφ,
∂eρ
∂θ = (−sin φ sin θ, sin φ cos θ, 0)T = 1
ρeθ,
and
∂eρ
∂ρ = 0.
Therefore, by the chain rule,
deρ
dt
=
∂eρ
∂φ
dφ
dt + ∂eρ
∂θ
dθ
dt
=
1
ρ
dφ
dt eφ + 1
ρ
dθ
dt eθ.
By 21.10,
r′ = ρ′eρ + dφ
dt eφ + dθ
dt eθ.
(21.11)
Now things get interesting. This must be diﬀerentiated with respect to t. To do so,
∂eθ
∂θ = (−ρ sin φ cos θ, −ρ sin φ sin θ, 0)T =?
where it is desired to ﬁnd a, b, c such that ? = aeθ + beφ + ceρ. Thus


−ρ sin φ sin θ
ρ cos φ cos θ
sin φ cos θ
ρ sin φ cos θ
ρ cos φ sin θ
sin φ sin θ
0
−ρ sin φ
cos φ




a
b
c

=


−ρ sin φ cos θ
−ρ sin φ sin θ
0


Using Cramer’s rule, the solution is a = 0, b = −cos φ sin φ, and c = −ρ sin2 φ. Thus
∂eθ
∂θ
=
(−ρ sin φ cos θ, −ρ sin φ sin θ, 0)T
=
(−cos φ sin φ) eφ +
¡
−ρ sin2 φ
¢
eρ.
Also,
∂eθ
∂φ = (−ρ cos φ sin θ, ρ cos φ cos θ, 0)T = (cot φ) eθ
and
∂eθ
∂ρ = (−sin φ sin θ, sin φ cos θ, 0)T = 1
ρeθ.
Now in 21.11 it is also necessary to consider eφ.
∂eφ
∂φ = (−ρ sin φ cos θ, −ρ sin φ sin θ, −ρ cos φ)T = −ρeρ

21.2.
THE CHAIN RULE∗
383
∂eφ
∂θ
=
(−ρ cos φ sin θ, ρ cos φ cos θ, 0)T
=
(cot φ) eθ
and ﬁnally,
∂eφ
∂ρ = (cos φ cos θ, cos φ sin θ, −sin φ)T = 1
ρeφ.
With these formulas for various partial derivatives, the chain rule is used to obtain r′′ which
will yield a formula for the acceleration in terms of the spherical coordinates and these
special vectors. By the chain rule,
d
dt (eρ)
=
∂eρ
∂θ θ′ + ∂eρ
∂φ φ′ + ∂eρ
∂ρ ρ′
=
θ′
ρ eθ + φ′
ρ eφ
d
dt (eθ)
=
∂eθ
∂θ θ′ + ∂eθ
∂φ φ′ + ∂eθ
∂ρ ρ′
=
θ′ ¡
(−cos φ sin φ) eφ +
¡
−ρ sin2 φ
¢
eρ
¢
+ φ′ (cot φ) eθ + ρ′
ρ eθ
d
dt (eφ)
=
∂eφ
∂θ θ′ + ∂eφ
∂φ φ′ + ∂eφ
∂ρ ρ′
=
¡
θ′ cot φ
¢
eθ + φ′ (−ρeρ) +
µρ′
ρ eφ
¶
By 21.11,
r′′ = ρ′′eρ + φ′′eφ + θ′′eθ + ρ′ (eρ)′ + φ′ (eφ)′ + θ′ (eθ)′
and from the above, this equals
ρ′′eρ + φ′′eφ + θ′′eθ + ρ′
µθ′
ρ eθ + φ′
ρ eφ
¶
+
φ′
µ¡
θ′ cot φ
¢
eθ + φ′ (−ρeρ) +
µρ′
ρ eφ
¶¶
+
θ′
µ
θ′ ¡
(−cos φ sin φ) eφ +
¡
−ρ sin2 φ
¢
eρ
¢
+ φ′ (cot φ) eθ + ρ′
ρ eθ
¶
and now all that remains is to collect the terms. Thus r′′ equals
r′′
=
³
ρ′′ −ρ
¡
φ′¢2 −ρ
¡
θ′¢2 sin2 (φ)
´
eρ +
µ
φ′′ + 2ρ′φ′
ρ
−
¡
θ′¢2 cos φ sin φ
¶
eφ +
+
µ
θ′′ + 2θ′ρ′
ρ
+ 2φ′θ′ cot (φ)
¶
eθ.
and this gives the acceleration in spherical coordinates. Note the prominent role played
by the chain rule. All of the above is done in books on mechanics for general curvilinear
coordinate systems and in the more general context, special theorems are developed which
make things go much faster but these theorems are all exercises in the chain rule.
As an example of how this could be used, consider a rocket. Suppose for simplicity that
it experiences a force only in the direction of eρ, directly away from the earth. Of course

384THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
this force produces a corresponding acceleration which can be computed as a function of
time. As the fuel is burned, the rocket becomes less massive and so the acceleration will be
an increasing function of t. However, this would be a known function, say a (t). Suppose
you wanted to know the latitude and longitude of the rocket as a function of time. (There
is no reason to think these will stay the same.) Then all that would be required would be
to solve the system of diﬀerential equations1,
ρ′′ −ρ
¡
φ′¢2 −ρ
¡
θ′¢2 sin2 (φ)
=
a (t) ,
φ′′ + 2ρ′φ′
ρ
−
¡
θ′¢2 cos φ sin φ
=
0,
θ′′ + 2θ′ρ′
ρ
+ 2φ′θ′ cot (φ)
=
0
along with initial conditions, ρ (0) = ρ0 (the distance from the launch site to the center of
the earth.), ρ′ (0) = ρ1(the initial vertical component of velocity of the rocket, probably 0.)
and then initial conditions for φ, φ′, θ, θ′. The initial value problems could then be solved
numerically and you would know the distance from the center of the earth as a function of
t along with θ and φ. Thus you could predict where the booster shells would fall to earth so
you would know where to look for them. Of course there are many variations of this. You
might want to specify forces in the eθ and eφ direction as well and attempt to control the
position of the rocket or rather its payload. The point is that if you are interested in doing
all this in terms of φ, θ, and ρ, the above shows how to do it systematically and you see it is
all an exercise in using the chain rule. More could be said here involving moving coordinate
systems and the Coriolis force. You really might want to do everything with respect to a
coordinate system which is ﬁxed with respect to the moving earth.
21.3
Proof Of The Chain Rule∗
As in the case of a function of one variable, it is important to consider the derivative
of a composition of two functions. The proof of the chain rule depends on the following
fundamental lemma.
Lemma 21.3.1 Let g : U →Rp where U is an open set in Rn and suppose g has a
derivative at x ∈U. Then o (g (x + v) −g (x)) = o (v) .
Proof: It is necessary to show
lim
v→0
|o (g (x + v) −g (x))|
|v|
= 0.
(21.12)
From Lemma 21.1.8, there exists δ > 0 such that if |v| < δ, then
|g (x + v) −g (x)| ≤(Cn + 1) |v| .
(21.13)
Now let ε > 0 be given. There exists η > 0 such that if |g (x + v) −g (x)| < η, then
|o (g (x + v) −g (x))| <
µ
ε
Cn + 1
¶
|g (x + v) −g (x)|
(21.14)
1You won’t be able to ﬁnd the solution to equations like these in terms of simple functions. The existence
of such functions is being assumed. The reason they exist often depends on the implicit function theorem,
a big theorem in advanced calculus.

21.3.
PROOF OF THE CHAIN RULE∗
385
Let |v| < min
³
δ,
η
Cn+1
´
. For such v, |g (x + v) −g (x)| ≤η, which implies
|o (g (x + v) −g (x))|
<
µ
ε
Cn + 1
¶
|g (x + v) −g (x)|
<
µ
ε
Cn + 1
¶
(Cn + 1) |v|
and so
|o (g (x + v) −g (x))|
|v|
< ε
which establishes 21.12. This proves the lemma.
Recall the notation f ◦g (x) ≡f (g (x)) . Thus f ◦g is the name of a function and this
function is deﬁned by what was just written. The following theorem is known as the chain
rule.
Theorem 21.3.2 (Chain rule) Let U be an open set in Rn, let V be an open set
in Rp, let g : U →Rp be such that g (U) ⊆V, and let f : V →Rq. Suppose Dg (x) exists
for some x ∈U and that Df (g (x)) exists. Then D (f ◦g) (x) exists and furthermore,
D (f ◦g) (x) = Df (g (x)) Dg (x) .
(21.15)
In particular,
∂(f ◦g) (x)
∂xj
=
p
X
i=1
∂f (g (x))
∂yi
∂gi (x)
∂xj
.
(21.16)
Proof: From the assumption that Df (g (x)) exists,
f (g (x + v)) = f (g (x)) +
p
X
i=1
∂f (g (x))
∂yi
(gi (x + v) −gi (x)) + o (g (x + v) −g (x))
which by Lemma 21.3.1 equals
(f ◦g) (x + v) = f (g (x + v)) = f (g (x)) +
p
X
i=1
∂f (g (x))
∂yi
(gi (x + v) −gi (x)) + o (v) .
Now since Dg (x) exists, the above becomes
(f ◦g) (x + v)
=
f (g (x)) +
p
X
i=1
∂f (g (x))
∂yi


n
X
j=1
∂gi (x)
∂xj
vj + o (v)

+ o (v)
=
f (g (x)) +
p
X
i=1
∂f (g (x))
∂yi


n
X
j=1
∂gi (x)
∂xj
vj

+
p
X
i=1
∂f (g (x))
∂yi
o (v) + o (v)
=
(f ◦g) (x) +
n
X
j=1
Ã p
X
i=1
∂f (g (x))
∂yi
∂gi (x)
∂xj
!
vj + o (v)
because Pp
i=1
∂f(g(x))
∂yi
o (v)+o (v) = o (v) . This establishes 21.16 because of Theorem 21.0.9
on Page 372. Thus
(D (f ◦g) (x))kj
=
p
X
i=1
∂fk (g (x))
∂yi
∂gi (x)
∂xj
=
p
X
i=1
Df (g (x))ki (Dg (x))ij .

386THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
Then 21.15 follows from the deﬁnition of matrix multiplication.
21.4
Proof Of The Second Derivative Test∗
Deﬁnition 21.4.1 The matrix,
³
∂2f
∂xi∂xj (x)
´
is called the Hessian matrix, denoted
by H (x) .
Now recall the Taylor formula with the Lagrange form of the remainder. Since most
people don’t pay any attention to this important topic when they take calculus, here is a
statement and proof of this theorem.
Theorem 21.4.2 Suppose f has n + 1 derivatives on an interval, (a, b) and let
c ∈(a, b) . Then if x ∈(a, b) , there exists ξ between c and x such that
f (x) = f (c) +
n
X
k=1
f (k) (c)
k!
(x −c)k + f (n+1) (ξ)
(n + 1)! (x −c)n+1 .
(In this formula, the symbol P0
k=1 ak will denote the number 0.)
Proof: If n = 0 then the theorem is true because it is just the mean value theorem.
Suppose the theorem is true for n−1, n ≥1. It can be assumed x ̸= c because if x = c there
is nothing to show. Then there exists K such that
f (x) −
Ã
f (c) +
n
X
k=1
f (k) (c)
k!
(x −c)k + K (x −c)n+1
!
= 0
(21.17)
In fact,
K =
−f (x) +
³
f (c) + Pn
k=1
f (k)(c)
k!
(x −c)k´
(x −c)n+1
.
Now deﬁne F (t) for t in the closed interval determined by x and c by
F (t) ≡f (x) −
Ã
f (t) +
n
X
k=1
f (k) (c)
k!
(x −t)k + K (x −t)n+1
!
.
The c in 21.17 got replaced by t.
Therefore, F (c) = 0 by the way K was chosen and also F (x) = 0. By the mean value
theorem or Rolle’s theorem, there exists t1 between x and c such that F ′ (t1) = 0. Therefore,
0
=
f ′ (t1) −
n
X
k=1
f (k) (c)
k!
k (x −t1)k−1 −K (n + 1) (x −t1)n
=
f ′ (t1) −
Ã
f ′ (c) +
n−1
X
k=1
f (k+1) (c)
k!
(x −t1)k
!
−K (n + 1) (x −t1)n
=
f ′ (t1) −
Ã
f ′ (c) +
n−1
X
k=1
f ′(k) (c)
k!
(x −t1)k
!
−K (n + 1) (x −t1)n
By induction applied to f ′, there exists ξ between x and t1 such that the above simpliﬁes
to
0
=
f ′(n) (ξ) (x −t1)n
n!
−K (n + 1) (x −t1)n
=
f (n+1) (ξ) (x −t1)n
n!
−K (n + 1) (x −t1)n

21.4.
PROOF OF THE SECOND DERIVATIVE TEST∗
387
therefore,
K = f (n+1) (ξ)
(n + 1) n! = f (n+1) (ξ)
(n + 1)!
and the formula is true for n. This proves the theorem.
The term f (n+1)(ξ)
(n+1)! (x −c)n+1 , is called the remainder and this particular form of the
remainder is called the Lagrange form of the remainder.
Now let f : U →R where U is an open subset of Rn. Suppose f ∈C2 (U) . Let x ∈U
and let r > 0 be such that
B (x,r) ⊆U.
Then for ||v|| < r consider
f (x+tv) −f (x) ≡h (t)
for t ∈[0, 1] . Then from Taylor’s theorem for the case where m = 2 and the chain rule,
using the repeated index summation convention and the chain rule,
h′ (t) = ∂f
∂xi
(x + tv) vi, h′′ (t) =
∂2f
∂xj∂xi
(x + tv) vivj.
Thus
h′′ (t) = vT H (x + tv) v.
From Theorem 21.4.2 there exists t ∈(0, 1) such that
f (x + v) = f (x) + ∂f
∂xi
(x) vi+1
2vT H (x + tv) v
By the continuity of the second partial derivative
f (x + v) = f (x) + ∇f (x) · v+1
2vT H (x) v+
1
2
¡
vT (H (x+tv) −H (x)) v
¢
(21.18)
where the last term satisﬁes
lim
|v|→0
1
2
¡
vT (H (x+tv) −H (x)) v
¢
|v|2
= 0
(21.19)
because of the continuity of the entries of H (x) .
Recall the following important theorem from linear algebra.
Theorem 21.4.3 If A is a real symmetric matrix, then A is Hermitian and there
exists a real unitary matrix, U such that U T AU = D where D is a diagonal matrix. In
particular, it has all real eigenvalues and an orthonormal basis of eigenvectors.
Theorem 21.4.4 Suppose x is a critical point for f. That is, suppose
∂f
∂xi (x) = 0
for each i. Then if H (x) has all positive eigenvalues, x is a local minimum. If H (x) has all
negative eigenvalues, then x is a local maximum. If H (x) has a positive eigenvalue, then
there exists a direction in which f has a local minimum at x, while if H (x) has a negative
eigenvalue, there exists a direction in which f has a local maximum at x.

388THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?∗
Proof: Since ∇f (x) = 0, formula 21.18 implies
f (x + v) = f (x) + 1
2vT H (x) v+1
2
¡
vT (H (x+tv) −H (x)) v
¢
(21.20)
and by continuity of the second derivatives, these mixed second derivatives are equal and
so H (x) is a symmetric matrix . Thus, by Theorem 21.4.3 H (x) has all real eigenvalues.
Suppose ﬁrst that H (x) has all positive eigenvalues and that all are larger than δ2 > 0.
Then by this corollary, H (x) has an orthonormal basis of eigenvectors, {vi}n
i=1 and so if
u is an arbitrary vector, there exist scalars, ui such that u = Pn
j=1 ujvj. Taking the dot
product of both sides with vj it follows uj = u · vj. Thus
uT H (x) u
=
Ã n
X
k=1
ukvT
k
!
H (x)


n
X
j=1
ujvj


=
X
k,j
ukvT
k H (x) vjuj
=
n
X
j=1
u2
jλj ≥δ2
n
X
j=1
u2
j = δ2 |u|2 .
From 21.20 and 21.19, if v is small enough,
f (x + v) ≥f (x) + 1
2δ2 |v|2 −1
4δ2 |v|2 = f (x) + δ2
4 |v|2 .
This shows the ﬁrst claim of the theorem. The second claim follows from similar reasoning.
Suppose H (x) has a positive eigenvalue λ2. Then let v be an eigenvector for this eigenvalue.
Then from 21.20, replacing v with sv and letting t depend on s,
f (x+sv) = f (x) +1
2s2vT H (x) v+
1
2s2 ¡
vT (H (x+tsv) −H (x)) v
¢
which implies
f (x+sv)
=
f (x) +1
2s2λ2 |v|2 +1
2s2 ¡
vT (H (x+tsv) −H (x)) v
¢
≥
f (x) +1
4s2λ2 |v|2
whenever s is small enough. Thus in the direction v the function has a local minimum at
x. The assertion about the local maximum in some direction follows similarly. This proves
the theorem.

Implicit Function Theorem∗
The implicit function theorem is one of the greatest theorems in mathematics. There
are many versions of this theorem which are of far greater generality than the one given
here. The proof given here is like one found in one of Caratheodory’s books on the calculus
of variations. It is not as elegant as some of the others which are based on a contraction
mapping principle but it may be more accessible. However, it is an advanced topic. Don’t
waste your time with it unless you have ﬁrst read and understood the material on rank and
determinants found in the chapter on the mathematical theory of determinants. You will
also need to use the extreme value theorem for a function of n variables and the chain rule
as well as everything about matrix multiplication.
Deﬁnition 22.0.5 Suppose U is an open set in Rn × Rm and (x, y) will denote a
typical point of Rn × Rm with x ∈Rn and y ∈Rm. Let f : U →Rp be in C1 (U) . Then
deﬁne
D1f (x, y)
≡



f1,x1 (x, y)
· · ·
f1,xn (x, y)
...
...
fp,x1 (x, y)
· · ·
fp,xn (x, y)


,
D2f (x, y)
≡



f1,y1 (x, y)
· · ·
f1,ym (x, y)
...
...
fp,y1 (x, y)
· · ·
fp,ym (x, y)


.
Thus Df (x, y) is a p × (n + m) matrix of the form
Df (x, y) =
¡
D1f (x, y)
|
D2f (x, y)
¢
.
Note that D1f (x, y) is an p × n matrix and D2f (x, y) is a p × m matrix.
389

390
IMPLICIT FUNCTION THEOREM∗
Theorem 22.0.6 (implicit function theorem) Suppose U is an open set in Rn×Rm.
Let f : U →Rn be in C1 (U) and suppose
f (x0, y0) = 0, D1f (x0, y0)−1 exists.
(22.1)
Then there exist positive constants, δ, η, such that for every y ∈B (y0, η) there exists a
unique x (y) ∈B (x0, δ) such that
f (x (y) , y) = 0.
(22.2)
Furthermore, the mapping, y →x (y) is in C1 (B (y0, η)).
Proof: Let
f (x, y) =





f1 (x, y)
f2 (x, y)
...
fn (x, y)




.
Deﬁne for
¡
x1, · · ·, xn¢
∈B (x0, δ)
n and y ∈B (y0, η) the following matrix.
J
¡
x1, · · ·, xn, y
¢
≡



f1,x1
¡
x1, y
¢
· · ·
f1,xn
¡
x1, y
¢
...
...
fn,x1 (xn, y)
· · ·
fn,xn (xn, y)


.
Then by the assumption of continuity of all the partial derivatives and the extreme value
theorem, there exists r > 0 and δ0, η0 > 0 such that if δ ≤δ0 and η ≤η0, it follows that for
all
¡
x1, · · ·, xn¢
∈B (x0, δ)
n and y ∈B (y0, η),
det
¡
J
¡
x1, · · ·, xn, y
¢¢
> r > 0.
(22.3)
and B (x0, δ0)× B (y0, η0) ⊆U. By continuity of all the partial derivatives and the extreme
value theorem, it can also be assumed there exists a constant, K such that for all (x, y) ∈
B (x0, δ0)× B (y0, η0) and i = 1, 2, · · ·, n, the ith row of D2f (x, y) , given by D2fi (x, y)
satisﬁes
|D2fi (x, y)| < K,
(22.4)
and for all
¡
x1, · · ·, xn¢
∈B (x0, δ0)
n and y ∈B (y0, η0) the ith row of the matrix, J
¡
x1, · · ·, xn, y
¢−1
which equals eT
i
³
J
¡
x1, · · ·, xn, y
¢−1´
satisﬁes
¯¯¯eT
i
³
J
¡
x1, · · ·, xn, y
¢−1´¯¯¯ < K.
(22.5)
(Recall that ei is the column vector consisting of all zeros except for a 1 in the ith position.)
To begin with it is shown that for a given y ∈B (y0, η) there is at most one x ∈B (x0, δ)
such that f (x, y) = 0.
Pick y ∈B (y0, η) and suppose there exist x, z ∈B (x0, δ) such that f (x, y) = f (z, y) =
0. Consider fi and let
h (t) ≡fi (x + t (z −x) , y) .
Then h (1) = h (0) and so by the mean value theorem, h′ (ti) = 0 for some ti ∈(0, 1) .
Therefore, from the chain rule and for this value of ti,
h′ (ti) = Dfi (x + ti (z −x) , y) (z −x) = 0.
(22.6)

391
Then denote by xi the vector, x + ti (z −x) . It follows from 22.6 that
J
¡
x1, · · ·, xn, y
¢
(z −x) = 0
and so from 22.3 z −x = 0. (The matrix, in the above is invertible since its determinant is
nonzero.) Now it will be shown that if η is chosen suﬃciently small, then for all y ∈B (y0, η) ,
there exists a unique x (y) ∈B (x0, δ) such that f (x (y) , y) = 0.
Claim: If η is small enough, then the function, hy (x) ≡|f (x, y)|2 achieves its minimum
value on B (x0, δ) at a point of B (x0, δ) . (The existence of a point in B (x0, δ) at which hy
achieves its minimum follows from the extreme value theorem.)
Proof of claim: Suppose this is not the case. Then there exists a sequence ηk →0
and for some yk having |yk−y0| < ηk, the minimum of hykon B (x0, δ) occurs on a point
of B (x0, δ), xk such that |x0−xk| = δ. Now taking a subsequence, still denoted by k, it
can be assumed that xk →x with |x −x0| = δ and yk →y0. This follows from the fact
that
n
x ∈B (x0, δ) : |x −x0| = δ
o
is a closed and bounded set and is therefore sequentially
compact.
Let ε > 0. Then for k large enough, the continuity of y →hy (x0) implies
hyk (x0) < ε because hy0 (x0) = 0 since f (x0, y0) = 0. Therefore, from the deﬁnition of xk,
it is also the case that hyk (xk) < ε. Passing to the limit yields hy0 (x) ≤ε. Since ε > 0
is arbitrary, it follows that hy0 (x) = 0 which contradicts the ﬁrst part of the argument in
which it was shown that for y ∈B (y0, η) there is at most one point, x of B (x0, δ) where
f (x, y) = 0. Here two have been obtained, x0 and x. This proves the claim.
Choose η < η0 and also small enough that the above claim holds and let x (y) denote
a point of B (x0, δ) at which the minimum of hy on B (x0, δ) is achieved. Since x (y) is an
interior point, you can consider hy (x (y) + tv) for |t| small and conclude this function of t
has a zero derivative at t = 0. Now
hy (x (y) + tv) =
n
X
i=1
f 2
i (x (y) + tv, y)
and so from the chain rule,
d
dthy (x (y) + tv) =
n
X
i=1
2fi (x (y) + tv, y) ∂fi (x (y) + tv, y)
∂xj
vj.
Therefore, letting t = 0, it is required that for every v,
n
X
i=1
2fi (x (y) , y) ∂fi (x (y) , y)
∂xj
vj = 0.
In terms of matrices this reduces to
0 = 2f (x (y) , y)T D1f (x (y) , y) v
for every vector v. Therefore,
0 = f (x (y) , y)T D1f (x (y) , y)
From 22.3, it follows f (x (y) , y) = 0. This proves the existence of the function y →x (y)
such that f (x (y) , y) = 0 for all y ∈B (y0, η) .
It remains to verify this function is a C1 function. To do this, let y1 and y2 be points of
B (y0, η) . Then as before, consider the ith component of f and consider the same argument
using the mean value theorem to write
0 = fi (x (y1) , y1) −fi (x (y2) , y2)
= fi (x (y1) , y1) −fi (x (y2) , y1) + fi (x (y2) , y1) −fi (x (y2) , y2)
= D1fi
¡
xi, y1
¢
(x (y1) −x (y2)) + D2fi
¡
x (y2) , yi¢
(y1 −y2) .
(22.7)

392
IMPLICIT FUNCTION THEOREM∗
where yi is a point on the line segment joining y1 and y2. Thus from 22.4 and the Cauchy
Schwarz inequality,
¯¯D2fi
¡
x (y2) , yi¢
(y1 −y2)
¯¯ ≤K |y1 −y2| .
Therefore, letting M
¡
y1, · · ·, yn¢
≡M denote the matrix having the ith row equal to
D2fi
¡
x (y2) , yi¢
, it follows
|M (y1 −y2)| ≤
ÃX
i
K2 |y1 −y2|2
!1/2
= √mK |y1 −y2| .
(22.8)
Also, from 22.7,
J
¡
x1, · · ·, xn, y1
¢
(x (y1) −x (y2)) = −M (y1 −y2)
(22.9)
and so from 22.8 and 22.10,
|x (y1) −x (y2)|
=
¯¯¯J
¡
x1, · · ·, xn, y1
¢−1 M (y1 −y2)
¯¯¯
(22.10)
=
Ã n
X
i=1
¯¯¯eT
i J
¡
x1, · · ·, xn, y1
¢−1 M (y1 −y2)
¯¯¯
2
!1/2
≤
Ã n
X
i=1
K2 |M (y1 −y2)|2
!1/2
≤
Ã n
X
i=1
K2 ¡√mK |y1 −y2|
¢2
!1/2
=
K2√mn |y1 −y2|
(22.11)
It follows as in the proof of the chain rule that
o (x (y + v) −x (y)) = o (v) .
(22.12)
Now let y ∈B (y0, η) and let |v| be suﬃciently small that y + v ∈B (y0, η) . Then
0
=
f (x (y + v) , y + v) −f (x (y) , y)
=
f (x (y + v) , y + v) −f (x (y + v) , y) + f (x (y + v) , y) −f (x (y) , y)
= D2f (x (y + v) , y) v + D1f (x (y) , y) (x (y + v) −x (y)) + o (|x (y + v) −x (y)|)
=
D2f (x (y) , y) v + D1f (x (y) , y) (x (y + v) −x (y)) +
o (|x (y + v) −x (y)|) + (D2f (x (y + v) , y) v−D2f (x (y) , y) v)
=
D2f (x (y) , y) v + D1f (x (y) , y) (x (y + v) −x (y)) + o (v) .
Therefore,
x (y + v) −x (y) = −D1f (x (y) , y)−1 D2f (x (y) , y) v + o (v)
which shows that Dx (y) = −D1f (x (y) , y)−1 D2f (x (y) , y) and y →Dx (y) is continuous.
This proves the theorem.

22.1.
THE METHOD OF LAGRANGE MULTIPLIERS
393
22.1
The Method Of Lagrange Multipliers
As an application of the implicit function theorem, consider the method of Lagrange mul-
tipliers.
Recall the problem is to maximize or minimize a function subject to equality
constraints. Let f : U →R be a C1 function where U ⊆Rn and let
gi (x) = 0, i = 1, · · ·, m
(22.13)
be a collection of equality constraints with m < n. Now consider the system of nonlinear
equations
f (x)
=
a
gi (x)
=
0, i = 1, · · ·, m.
Recall x0 is a local maximum if f (x0) ≥f (x) for all x near x0 which also satisﬁes the
constraints 22.13. A local minimum is deﬁned similarly. Let F : U × R →Rm+1 be deﬁned
by
F (x,a) ≡





f (x) −a
g1 (x)
...
gm (x)




.
(22.14)
Now consider the m + 1 × n matrix,





fx1 (x0)
· · ·
fxn (x0)
g1x1 (x0)
· · ·
g1xn (x0)
...
...
gmx1 (x0)
· · ·
gmxn (x0)




.
If this matrix has rank m + 1 then some m + 1 × m + 1 submatrix has nonzero determinant.
It follows from the implicit function theorem there exists m+1 variables, xi1, ···, xim+1 such
that the system
F (x,a) = 0
(22.15)
speciﬁes these m + 1 variables as a function of the remaining n −(m + 1) variables and a
in an open set of Rn−m. Thus there is a solution (x,a) to 22.15 for some x close to x0
whenever a is in some open interval. Therefore, x0 cannot be either a local minimum or a
local maximum. It follows that if x0 is either a local maximum or a local minimum, then
the above matrix must have rank less than m + 1 which requires the rows to be linearly
dependent. Thus, there exist m scalars,
λ1, · · ·, λm,
and a scalar µ, not all zero such that
µ



fx1 (x0)
...
fxn (x0)


= λ1



g1x1 (x0)
...
g1xn (x0)


+ · · · + λm



gmx1 (x0)
...
gmxn (x0)


.
(22.16)
If the column vectors



g1x1 (x0)
...
g1xn (x0)


, · · ·



gmx1 (x0)
...
gmxn (x0)



(22.17)

394
IMPLICIT FUNCTION THEOREM∗
are linearly independent, then, µ ̸= 0 and dividing by µ yields an expression of the form



fx1 (x0)
...
fxn (x0)


= λ1



g1x1 (x0)
...
g1xn (x0)


+ · · · + λm



gmx1 (x0)
...
gmxn (x0)



(22.18)
at every point x0 which is either a local maximum or a local minimum. This proves the
following theorem.
Theorem 22.1.1 Let U be an open subset of Rn and let f : U →R be a C1
function. Then if x0 ∈U is either a local maximum or local minimum of f subject to the
constraints 22.13, then 22.16 must hold for some scalars µ, λ1, · · ·, λm not all equal to zero.
If the vectors in 22.17 are linearly independent, it follows that an equation of the form 22.18
holds.
22.2
The Local Structure Of C1 Mappings
Deﬁnition 22.2.1 Let U be an open set in Rn and let h : U →Rn. Then h is
called primitive if it is of the form
h (x) =
¡
x1
· · ·
α (x)
· · ·
xn
¢T .
Thus, h is primitive if it only changes one of the variables. A function, F : Rn →Rn is
called a ﬂip if
F (x1, · · ·, xk, · · ·, xl, · · ·, xn) = (x1, · · ·, xl, · · ·, xk, · · ·, xn)T .
Thus a function is a ﬂip if it interchanges two coordinates. Also, for m = 1, 2, · · ·, n,
Pm (x) ≡
¡
x1
x2
· · ·
xm
0
· · ·
0
¢T
It turns out that if h (0) = 0,Dh (0)−1 exists, and h is C1 on U, then h can be written
as a composition of primitive functions and ﬂips. This is a very interesting application of
the inverse function theorem.
Theorem 22.2.2 Let h : U →Rn be a C1 function with h (0) = 0,Dh (0)−1
exists. Then there an open set, V ⊆U containing 0, ﬂips, F1, · · ·, Fn−1, and primitive
functions, Gn, Gn−1, · · ·, G1 such that for x ∈V,
h (x) = F1 ◦· · · ◦Fn−1 ◦Gn ◦Gn−1 ◦· · · ◦G1 (x) .
Proof: Let
h1 (x) ≡h (x) =
¡
α1 (x)
· · ·
αn (x)
¢T
Dh (0) e1 =
¡
α1,1 (0)
· · ·
αn,1 (0)
¢T
where αk,1 denotes ∂αk
∂x1 . Since Dh (0) is one to one, the right side of this expression cannot
be zero. Hence there exists some k such that αk,1 (0) ̸= 0. Now deﬁne
G1 (x) ≡
¡
αk (x)
x2
· · ·
xn
¢T

22.2.
THE LOCAL STRUCTURE OF C1 MAPPINGS
395
Then the matrix of DG (0) is of the form





αk,1 (0)
· · ·
· · ·
αk,n (0)
0
1
0
...
...
...
0
0
· · ·
1





and its determinant equals αk,1 (0) ̸= 0. Therefore, by the inverse function theorem, there
exists an open set, U1 containing 0 and an open set, V2 containing 0 such that G1 (U1) = V2
and G1 is one to one and onto such that it and its inverse are both C1. Let F1 denote the
ﬂip which interchanges xk with x1. Now deﬁne
h2 (y) ≡F1 ◦h1 ◦G−1
1
(y)
Thus
h2 (G1 (x))
≡
F1 ◦h1 (x)
(22.19)
=
¡
αk (x)
· · ·
α1 (x)
· · ·
αn (x)
¢T
Therefore,
P1h2 (G1 (x)) =
¡
αk (x)
0
· · ·
0
¢T .
Also
P1 (G1 (x)) =
¡
αk (x)
x2
· · ·
xn
¢T
so P1h2 (y) = P1 (y) for all y ∈V2. Also, h2 (0) = 0 and Dh2 (0)−1 exists because of the
deﬁnition of h2 above and the chain rule. Also, since F2
1 = identity, it follows from 22.19
that
h (x) = h1 (x) = F1 ◦h2 ◦G1 (x) .
(22.20)
Suppose then that for m ≥2,
Pm−1hm (x) = Pm−1 (x)
(22.21)
for all x ∈Um, an open subset of U containing 0 and hm (0) = 0,Dhm (0)−1 exists. From
22.21, hm (x) must be of the form
hm (x) =
¡
x1
· · ·
xm−1
α1 (x)
· · ·
αn (x)
¢T
where these αk are diﬀerent than the ones used earlier. Then
Dhm (0) em =
¡
0
· · ·
0
α1,m (0)
· · ·
αn,m (0)
¢T ̸= 0
because Dhm (0)−1 exists. Therefore, there exists a k such that αk,m (0) ̸= 0, not the same
k as before. Deﬁne
Gm+1 (x) ≡
¡
x1
· · ·
xm−1
αk (x)
xm+1
· · ·
xn
¢T
(22.22)
Then Gm+1 (0) = 0 and DGm+1 (0)−1 exists similar to the above. In fact det (DGm+1 (0)) =
αk,m (0). Therefore, by the inverse function theorem, there exists an open set, Vm+1 contain-
ing 0 such that Vm+1 = Gm+1 (Um) with Gm+1 and its inverse being one to one continuous
and onto. Let Fm be the ﬂip which ﬂips xm and xk. Then deﬁne hm+1 on Vm+1 by
hm+1 (y) = Fm ◦hm ◦G−1
m+1 (y) .

396
IMPLICIT FUNCTION THEOREM∗
Thus for x ∈Um,
hm+1 (Gm+1 (x)) = (Fm ◦hm) (x) .
(22.23)
and consequently,
Fm ◦hm+1 ◦Gm+1 (x) = hm (x)
(22.24)
It follows
Pmhm+1 (Gm+1 (x))
=
Pm (Fm ◦hm) (x)
=
¡
x1
· · ·
xm−1
αk (x)
0
· · ·
0
¢T
and
Pm (Gm+1 (x)) =
¡ x1
· · ·
xm−1
αk (x)
0
· · ·
0 ¢T .
Therefore, for y ∈Vm+1,
Pmhm+1 (y) = Pm (y) .
As before, hm+1 (0) = 0 and Dhm+1 (0)−1 exists. Therefore, we can apply 22.24 repeatedly,
obtaining the following:
h (x)
=
F1 ◦h2 ◦G1 (x)
=
F1 ◦F2 ◦h3 ◦G2 ◦G1 (x)
...
=
F1 ◦· · · ◦Fn−1 ◦hn ◦Gn−1 ◦· · · ◦G1 (x)
where
Pn−1hn (x) = Pn−1 (x) =
¡
x1
· · ·
xn−1
0
¢T
and so hn (x) is of the form
hn (x) =
¡
x1
· · ·
xn−1
α (x)
¢T .
Therefore, deﬁne the primitive function, Gn (x) to equal hn (x). This proves the theorem.

Part IX
Multiple Integrals
397


399
Outcomes
Double Integrals
A. Compare the deﬁnition of the double integral to the method of repeated integration
geometrically.
B. Evaluate double integrals over a rectangle by repeated integration.
C. Apply a double integral to calculate the volume or mass of a solid.
Reading: Multivariable Calculus 3.1
Outcome Mapping:
A. J1
B. 1,2
C. 3,4
Double Integrals Over General Regions
A. Evaluate double integrals over general regions.
B. Evaluate double integrals by interpreting them as known volumes.
C. Rewrite a double integral changing the order of integration.
D. Apply double integrals to calculate volumes of solids.
E. Evaluate the physical characteristics of a plate such as mass, centroid, center of mass
and moment of inertia.
Reading: Multivariable Calculus 3.2
Outcome Mapping:
A. 1
B. 2
C. 3
D. 4
E. 5,6
Double Integrals in Polar Coordinates
A. Represent a region in both Cartesian and polar coordinates.
B. Evaluate double integrals in polar coordinates.
C. Convert a double integral in Cartesian coordinates to a double integral in polar coor-
dinates and then evaluate.
D. Evaluate areas and volumes using polar coordinates
E. Evaluate the physical characteristics of a plate such as centroid, mass, and center of
mass using polar coordinates.

400
F. Make conversions of algebraic expressions between Cartesian coordinates and cylin-
drical coordinates.
Reading: Multivariable Calculus 3.3
Outcome Mapping:
A. 1,4
B. 5a
C. 8
D. 5b,6,7
E. 5c
F. 9,10,11,12
Triple Integrals
A. Find the volume of a solid using triple integration in Cartesian coordinates.
B. Evaluate the physical characteristics of a solid such as mass, centroid and center of
mass using Cartesian coordinates.
Reading: Multivariable Calculus 3.4
Outcome Mapping:
A. 1,4,7
B. 2,4,7
Triple Integrals in Cylindrical Coordinats
A. Describe regions in both Cartesian coordinates and cylindrical coordinates.
B. Evaluate triple integrals using cylindrical coordinates.
C. Find volumes by applying triple integration in cylindrical coordinates.
D. Evaluate the physical characteristics of a solid such as mass, centroid and center of
mass using cylindrical coordinates.
Reading: Multivariable Calculus 3.5
Outcome Mapping:
A. 2,3
B. 5
C. 6
D. 7,8,11
Triple Integrals in Spherical Coordinats
A. Describe regions in both Cartesian coordinates and spherical coordinates.
B. Evaluate triple integrals using spherical coordinates.

401
C. Find volumes by applying triple integration in spherical coordinates.
D. Evaluate the physical characteristics of a solid such as mass, centroid and center of
mass using spherical coordinates.
E. Convert a triple integral in Cartesian coordinates to cylindrical or spherical coordinates
and then evaluate.
F. Make conversions of algebraic expressions between Cartesian coordinates and spherical
coordinates.
Reading: Multivariable Calculus 3.6
Outcome Mapping:
A. 1,2,3
B. 5
C. 6
D. 7
E. 10
F. 11,12,13,14,15
The Jacobian
A. Find the Jacobian of a transformation.
B. Change variables in a multiple integration to obtain a more simple integral and then
evaluate.
Reading: Multivariable Calculus 3.7
Outcome Mapping:
A. 1
B. 3,5,6

402

The Riemann Integral On Rn
23.1
Methods For Double Integrals 1 Nov.
Quiz
1. Maximize 2x + y subject to the condition that x2
4 + y2
9 ≤1.
2. A curve is formed from the intersection of the plane, 2x + 3y + z = 3 and the cylinder
x2 + y2 = 4. Find the point on this curve which is closest to (0, 0, 0) .
3. Find the points on y2x = 9 which are closest to (0, 0) .
This chapter is on the Riemann integral for a function of n variables.
It begins by
introducing the basic concepts and applications of the integral. The proofs of the theorems
involved are diﬃcult and are left till the end. To begin with consider the problem of ﬁnding
the volume under a surface of the form z = f (x, y) where f (x, y) ≥0 and f (x, y) = 0 for
all (x, y) outside of some bounded set. To solve this problem, consider the following picture.

Q
QQ
z = f(x, y)
In this picture, the volume of the little prism which lies above the rectangle Q and the
graph of the function would lie between MQ (f) v (Q) and mQ (f) v (Q) where
MQ (f) ≡sup {f (x) : x ∈Q} , mQ (f) ≡inf {f (x) : x ∈Q} ,
(23.1)
and v (Q) is deﬁned as the area of Q. Now consider the following picture.
403

404
THE RIEMANN INTEGRAL ON RN
In this picture, it is assumed f equals zero outside the circle and f is a bounded nonneg-
ative function. Then each of those little squares are the base of a prism of the sort in the
previous picture and the sum of the volumes of those prisms should be the volume under
the surface, z = f (x, y) . Therefore, the desired volume must lie between the two numbers,
X
Q
MQ (f) v (Q) and
X
Q
mQ (f) v (Q)
where the notation, P
Q MQ (f) v (Q) , means for each Q, take MQ (f) , multiply it by the
area of Q, v (Q) , and then add all these numbers together. Thus in P
Q MQ (f) v (Q) , adds
numbers which are at least as large as what is desired while in P
Q mQ (f) v (Q) numbers
are added which are at least as small as what is desired. Note this is a ﬁnite sum because
by assumption, f = 0 except for ﬁnitely many Q, namely those which intersect the circle.
The sum, P
Q MQ (f) v (Q) is called an upper sum, P
Q mQ (f) v (Q) is a lower sum, and
the desired volume is caught between these upper and lower sums.
None of this depends in any way on the function being nonnegative. It also does not
depend in any essential way on the function being deﬁned on R2, although it is impossible to
draw meaningful pictures in higher dimensional cases. To deﬁne the Riemann integral, it is
necessary to ﬁrst give a description of something called a grid. First you must understand
that something like [a, b] × [c, d] is a rectangle in R2, having sides parallel to the axes. The
situation is illustrated in the following picture.
c
d
a
b
[a, b] × [c, d]

23.1.
METHODS FOR DOUBLE INTEGRALS 1 NOV.
405
(x, y) ∈[a, b] × [c, d] , means x ∈[a, b] and also y ∈[c, d] and the points which do this
comprise the rectangle just as shown in the picture.
Deﬁnition 23.1.1 For i = 1, 2, let
©
αi
k
ª∞
k=−∞be points on R which satisfy
lim
k→∞αi
k = ∞,
lim
k→−∞αi
k = −∞, αi
k < αi
k+1.
(23.2)
For such sequences, deﬁne a grid on R2 denoted by G or F as the collection of rectangles
of the form
Q =
£
α1
k, α1
k+1
¤
×
£
α2
l , α2
l+1
¤
.
(23.3)
If G is a grid, another grid, F is a reﬁnement of G if every box of G is the union of boxes
of F.
For G a grid, the expression,
X
Q∈G
MQ (f) v (Q)
is called the upper sum associated with the grid, G as described above in the discussion of
the volume under a surface. Again, this means to take a rectangle from G multiply MQ (f)
deﬁned in 23.1 by its area, v (Q) and sum all these products for every Q ∈G. The symbol,
X
Q∈G
mQ (f) v (Q) ,
called a lower sum, is deﬁned similarly. With this preparation it is time to give a deﬁnition
of the Riemann integral of a function of two variables.
Deﬁnition 23.1.2 Let f : R2 →R be a bounded function which equals zero for all
(x, y) outside some bounded set. Then
R
f dV is deﬁned to be the unique number which lies
between all upper sums and all lower sums. In the case of R2, it is common to replace the
V with A and write this symbol as
R
f dA where A stands for area.
This deﬁnition begs a diﬃcult question. For which functions does there exist a unique
number between all the upper and lower sums? This interesting and fundamental question
is discussed in any advanced calculus book and may be seen in the appendix on the theory
of the Riemann integral. It is a hard problem which was only solved in the ﬁrst part of the
twentieth century. When it was solved, it was also realized that the Riemann integral was
not the right integral to use. First consider the question: How can the Riemann integral
be computed? Consider the following picture in which f equals zero outside the rectangle
[a, b] × [c, d] .

406
THE RIEMANN INTEGRAL ON RN
It depicts a slice taken from the solid deﬁned by {(x, y) : 0 ≤y ≤f (x, y)} . You see these
when you look at a loaf of bread. If you wanted to ﬁnd the volume of the loaf of bread, and
you knew the volume of each slice of bread, you could ﬁnd the volume of the whole loaf by
adding the volumes of individual slices. It is the same here. If you could ﬁnd the volume
of the slice represented in this picture, you could add these up and get the volume of the
solid. The slice in the picture corresponds to constant y and is assumed to be very thin,
having thickness equal to h. Denote the volume of the solid under the graph of z = f (x, y)
on [a, b] × [c, y] by V (y) . Then
V (y + h) −V (y) ≈h
Z b
a
f (x, y) dx
where the integral is obtained by ﬁxing y and integrating with respect to x. It is hoped that
the approximation would be increasingly good as h gets smaller. Thus, dividing by h and
taking a limit, it is expected that
V ′ (y) =
Z b
a
f (x, y) dx, V (c) = 0.
Therefore, the volume of the solid under the graph of z = f (x, y) is given by
Z d
c
ÃZ b
a
f (x, y) dx
!
dy
(23.4)
but this was also the result of
R
f dV. Therefore, it is expected that this is a way to evaluate
R
f dV. Note what has been gained here. A hard problem, ﬁnding
R
f dV, is reduced to a
sequence of easier problems. First do
Z b
a
f (x, y) dx
getting a function of y, say F (y) and then do
Z d
c
ÃZ b
a
f (x, y) dx
!
dy =
Z d
c
F (y) dy.
Of course there is nothing special about ﬁxing y ﬁrst. The same thing should be obtained
from the integral,
Z b
a
ÃZ d
c
f (x, y) dy
!
dx
(23.5)
These expressions in 23.4 and 23.5 are called iterated integrals.
They are tools for
evaluating
R
f dV which would be hard to ﬁnd otherwise.
In practice, the parenthesis
is usually omitted in these expressions. Thus
Z b
a
ÃZ d
c
f (x, y) dy
!
dx =
Z b
a
Z d
c
f (x, y) dy dx
and it is understood that you are to do the inside integral ﬁrst and then when you have
done it, obtaining a function of x, you integrate this function of x.
I have presented this for the case where f (x, y) ≥0 and the integral represents a vol-
ume, but there is no diﬀerence in the general case where f is not necessarily nonnegative.

23.1.
METHODS FOR DOUBLE INTEGRALS 1 NOV.
407
Throughout, I have been assuming the notion of volume has some sort of independent mean-
ing. This assumption is nonsense and is one of many reasons the above explanation does
not rise to the level of a proof. It is only intended to make things plausible. A careful
presentation which is not for the faint of heart is in an appendix.
Another aspect of this is the notion of integrating a function which is deﬁned on some
set, not on all R2. For example, suppose f is deﬁned on the set, S ⊆R2. What is meant by
R
S f dV ?
Deﬁnition 23.1.3 Let f : S →R where S is a subset of R2. Then denote by f1 the
function deﬁned by
f1 (x, y) ≡
½
f (x, y) if (x, y) ∈S
0 if (x, y) /∈S
.
Then
Z
S
f dV ≡
Z
f1 dV.
Example 23.1.4 Let f (x, y) = x2y + yx for (x, y) ∈[0, 1] × [0, 2] ≡R. Find
R
R f dV.
This is done using iterated integrals like those deﬁned above. Thus
Z
R
f dV =
Z 1
0
Z 2
0
¡
x2y + yx
¢
dy dx.
The inside integral yields
Z 2
0
¡
x2y + yx
¢
dy = 2x2 + 2x
and now the process is completed by doing
R 1
0 to what was just obtained. Thus
Z 1
0
Z 2
0
¡
x2y + yx
¢
dy dx =
Z 1
0
¡
2x2 + 2x
¢
dx = 5
3.
If the integration is done in the opposite order, the same answer should be obtained.
Z 2
0
Z 1
0
¡
x2y + yx
¢
dx dy
Z 1
0
¡
x2y + yx
¢
dx = 5
6y
Now
Z 2
0
Z 1
0
¡
x2y + yx
¢
dx dy =
Z 2
0
µ5
6y
¶
dy = 5
3.
If a diﬀerent answer had been obtained it would have been a sign that a mistake had been
made.
Example 23.1.5 Let f (x, y) = x2y + yx for (x, y) ∈R where R is the triangular region
deﬁned to be in the ﬁrst quadrant, below the line y = x and to the left of the line x = 4.
Find
R
R f dV.

408
THE RIEMANN INTEGRAL ON RN
x
y
¡
¡
¡
¡
¡
¡
4
R
Now from the above discussion,
Z
R
f dV =
Z 4
0
Z x
0
¡
x2y + yx
¢
dy dx
The reason for this is that x goes from 0 to 4 and for each ﬁxed x between 0 and 4, y goes
from 0 to the slanted line, y = x. Thus y goes from 0 to x. This explains the inside integral.
Now
R x
0
¡
x2y + yx
¢
dy = 1
2x4 + 1
2x3 and so
Z
R
f dV =
Z 4
0
µ1
2x4 + 1
2x3
¶
dx = 672
5 .
What of integration in a diﬀerent order? Lets put the integral with respect to y on the
outside and the integral with respect to x on the inside. Then
Z
R
f dV =
Z 4
0
Z 4
y
¡
x2y + yx
¢
dx dy
For each y between 0 and 4, the variable x, goes from y to 4.
Z 4
y
¡
x2y + yx
¢
dx = 88
3 y −1
3y4 −1
2y3
Now
Z
R
f dV =
Z 4
0
µ88
3 y −1
3y4 −1
2y3
¶
dy = 672
5 .
Here is a similar example.
Example 23.1.6 Let f (x, y) = x2y for (x, y) ∈R where R is the triangular region deﬁned
to be in the ﬁrst quadrant, below the line y = 2x and to the left of the line x = 4. Find
R
R f dV.
x
y
¢
¢
¢
¢
¢
¢
¢
¢¢
4
R
Put the integral with respect to x on the outside ﬁrst. Then
Z
R
f dV =
Z 4
0
Z 2x
0
¡
x2y
¢
dy dx

23.1.
METHODS FOR DOUBLE INTEGRALS 1 NOV.
409
because for each x ∈[0, 4] , y goes from 0 to 2x. Then
Z 2x
0
¡
x2y
¢
dy = 2x4
and so
Z
R
f dV =
Z 4
0
¡
2x4¢
dx = 2048
5
Now do the integral in the other order. Here the integral with respect to y will be on
the outside. What are the limits of this integral? Look at the triangle and note that x goes
from 0 to 4 and so 2x = y goes from 0 to 8. Now for ﬁxed y between 0 and 8, where does x
go? It goes from the x coordinate on the line y = 2x which corresponds to this y to 4. What
is the x coordinate on this line which goes with y? It is x = y/2. Therefore, the iterated
integral is
Z 8
0
Z 4
y/2
¡
x2y
¢
dx dy.
Now
Z 4
y/2
¡
x2y
¢
dx = 64
3 y −1
24y4
and so
Z
R
f dV =
Z 8
0
µ64
3 y −1
24y4
¶
dy = 2048
5
the same answer.
A few observations are in order here. In ﬁnding
R
S f dV there is no problem in setting
things up if S is a rectangle. However, if S is not a rectangle, the procedure always is
agonizing. A good rule of thumb is that if what you do is easy it will be wrong. There
are no shortcuts! There are no quick ﬁxes which require no thought! Pain and suﬀering
is inevitable and you must not expect it to be otherwise. Always draw a picture and then
begin agonizing over the correct limits. Even when you are careful you will make lots of
mistakes until you get used to the process.
Sometimes an integral can be evaluated in one order but not in another.
Example 23.1.7 For R as shown below, ﬁnd
R
R sin
¡
y2¢
dV.
x
8
¢
¢
¢
¢
¢
¢
¢
¢¢
4
R
Setting this up to have the integral with respect to y on the inside yields
Z 4
0
Z 8
2x
sin
¡
y2¢
dy dx.
Unfortunately, there is no antiderivative in terms of elementary functions for sin
¡
y2¢
so
there is an immediate problem in evaluating the inside integral. It doesn’t work out so the

410
THE RIEMANN INTEGRAL ON RN
next step is to do the integration in another order and see if some progress can be made.
This yields
Z 8
0
Z y/2
0
sin
¡
y2¢
dx dy =
Z 8
0
y
2 sin
¡
y2¢
dy
and
R 8
0
y
2 sin
¡
y2¢
dy = −1
4 cos 64 + 1
4 which you can verify by making the substitution,
u = y2. Thus
Z
R
sin
¡
y2¢
dy = −1
4 cos 64 + 1
4.
This illustrates an important idea. The integral
R
R sin
¡
y2¢
dV is deﬁned as a number.
It is the unique number between all the upper sums and all the lower sums. Finding it is
another matter. In this case it was possible to ﬁnd it using one order of integration but
not the other. The iterated integral in this other order also is deﬁned as a number but it
can’t be found directly without interchanging the order of integration. Of course sometimes
nothing you try will work out.
23.1.1
Density Mass And Center Of Mass
Consider a two dimensional material.
Of course there is no such thing but a ﬂat plate
might be modeled as one. The density ρ is a function of position and is deﬁned as follows.
Consider a small chunk of area, dV located at the point whose Cartesian coordinates are
(x, y) . Then the mass of this small chunk of material is given by ρ (x, y) dV. Thus if the
material occupies a region in two dimensional space, U, the total mass of this material is
Z
U
ρ dV
In other words you integrate the density to get the mass. Now by letting ρ depend on
position, you can include the case where the material is not homogeneous.
Here is an
example.
Example 23.1.8 Let ρ (x, y) denote the density of the plane region determined by the curves
1
3x + y = 2, x = 3y2, and x = 9y. Find the total mass if ρ (x, y) = y.
You need to ﬁrst draw a picture of the region, R. A rough sketch follows.
PPPPPPP
P
             
(3, 1)
(9/2, 1/2)
(0, 0)
x = 3y2
(1/3)x + y = 2
x = 9y
This region is in two pieces, one having the graph of x = 9y on the bottom and the
graph of x = 3y2 on the top and another piece having the graph of x = 9y on the bottom
and the graph of 1
3x + y = 2 on the top. Therefore, in setting up the integrals, with the
integral with respect to x on the outside, the double integral equals the following sum of
iterated integrals.
has x=3y2 on top
z
}|
{
Z 3
0
Z √
x/3
x/9
y dy dx +
has 1
3 x+y=2 on top
z
}|
{
Z
9
2
3
Z 2−1
3 x
x/9
y dy dx

23.2.
DOUBLE INTEGRALS IN POLAR COORDINATES
411
You notice it is not necessary to have a perfect picture, just one which is good enough to
ﬁgure out what the limits should be. The dividing line between the two cases is x = 3 and
this was shown in the picture. Now it is only a matter of evaluating the iterated integrals
which in this case is routine and gives 1.
The concept of center of mass of a plate occupying the bounded open set, U is also
easy to express in terms of double integrals. Letting ρ denote the density of the plate, the
moment of a small chunk of mass having coordinates (x, y) about the y axis is xρ (x, y) dV
and the moment of the same small chunk of mass about the x axis is yρ (x, y) dV. Therefore
the center of mass, (xc, yc) is deﬁned in the usual way.
Deﬁnition 23.1.9 The center of mass of a plate occuying the bounded open set, U
is deﬁned as (xc, yc) where
xc ≡
R
U xρ (x, y) dV
R
U ρ (x, y) dV , yc ≡
R
U yρ (x, y) dV
R
U ρ (x, y) dV .
In other words, the total moment about the y axis equals xc times the total mass. That
is, if you placed the total mass at the single point, (xc, yc) this point mass would produce
the same moments about the x and y axes as the original plate.
Example 23.1.10 In Example 23.1.8, suppose the density is ρ (x, y) = y as it is in that
example. Find the total mass and the center of mass.
First, the total mass was found above. Then the center of mass is
xc =
R 3
0
R √
x/3
x/9
xy dy dx +
R 9
2
3
R 2−1
3 x
x/9
xy dy dx
R 9
2
3
R 2−1
3 x
x/9
y dy dx +
R 3
0
R √
x/3
x/9
y dy dx
=
39
16
1 = 39
16
yc =
R 3
0
R √
x/3
x/9
y2 dy dx +
R 9
2
3
R 2−1
3 x
x/9
y2 dy dx
R 9
2
3
R 2−1
3 x
x/9
y dy dx +
R 3
0
R √
x/3
x/9
y dy dx
= 47
80.
Thus the center of mass is
¡ 39
16, 47
80
¢
.
23.2
Double Integrals In Polar Coordinates
Remember polar coordinates,
x = r cos θ
y = r sin θ
where θ ∈[0, 2π] and r > 0. If you assign a given value to r, the points obtained yield a circle
and if you give a value to θ the points yield a ray from the origin. Thus assigning many
diﬀerent values for r and many diﬀerent values for θ yields a grid of the sort illustrated in
the following picture.

412
THE RIEMANN INTEGRAL ON RN
     






¥
¥
¥
¥
¥
J
J
J
J
J
D
D
D
D
D
Q
Q
Q
Q
By contrast, the grid on the right is obtained by assigning diﬀerent values for x and y.
For the grid on the right, if the vertical lines are dx apart and the horizontal lines are dy
apart, the area of one of those little boxes would be dxdy. This is the increment of area
in rectangular coordinates. Now consider the grid on the left which is obtained by setting
each of the two polar variables equal to various constants. What is the area of one of those
little curvy rectangles if the values for r and θ are very small? Zoom in on one of them as
illustrated in the following picture.
dr
rdθ
The angle between the two straight lines is dθ and so the length of one of the curved
sides is approximately rdθ while the length of the straight sides is dr. Therefore, the area
of the little curvy rectangle is approximately equal to rdrdθ. This is the increment of area
in polar coordinates.
Later, I will present a uniﬁed way to change variables. For now, consider the following
problems which illustrate the use of polar coordinates to compute integrals over areas.
Example 23.2.1 Find the area of a circle of radius R.
Denote by D this circle. Then the area of the circle is
R
dA and you need to write dA in
terms of polar coordinates. As described above, dA = rdrdθ. To compute the integral, note
that in terms of the variables, θ and r, this region is actually the rectangle, [0, R] × [0, 2π] .
Therefore, the integral equals
Z 2π
0
Z R
0
rdrdθ = πR2
which you have already heard about.
Example 23.2.2 Find the volume of the ball of radius R.
It is enough to ﬁnd the volume of the top half of this ball and then multiply it by 2.
Corresponding to the small curvy rectangle as described above having polar coordinates (r, θ)
the height of the ball over this point is
√
R2 −r2. Therefore, the volume of a small prism
having as a base the small curvy rectangle described above is
√
R2 −r2rdrdθ. Summing
these using the integral, the desired volume is
Z
D
p
R2 −r2dA =
Z 2π
0
Z R
0
p
R2 −r2rdrdθ = 2
3πR3

23.2.
DOUBLE INTEGRALS IN POLAR COORDINATES
413
and so the volume of the whole ball is
4
3πR3
which is another formula you might have seen.
Example 23.2.3 Find the area inside r = 1 + cos θ for θ ∈[0, 2π] .
This is the graph of a cardioid. You saw this in beginning calculus. Let its inside be
denoted by C for cardioid. Then the desired area is
Z
C
dA
and you need to set up an iterated integral and put in the correct form for dA. For each θ,
you have that r goes from 0 to 1 + cos θ. Therefore, the desired area is given by the iterated
integral,
Z 2π
0
Z 1+cos θ
0
rdrdθ = 3
2π
This was really easy because of polar coordinates. If you try to do this in rectangular
coordinates it will not work very well.
Example 23.2.4 A plate occupies the region inside the curve, r = 2 cos θ for θ ∈
£
−π
2 , π
2
¤
.
The density in terms of polar coordinates is δ = r. Find the mass and center of mass of this
plate.
First of all the mass is given by
Z π/2
−π/2
Z 2 cos(θ)
0
r2drdθ = 32
9
The volume element is rdrdθ and I summed these up multiplied, by the density which was
r and that is the above integral.
Now to compute the center of mass, recall
xc ≡
R
U xρ (x, y) dV
R
U ρ (x, y) dV , yc ≡
R
U yρ (x, y) dV
R
U ρ (x, y) dV
I need to place x and y in terms of the polar coordinates. Thus x = r cos θ, y = r sin θ. A
small contribution to the moment about the y axis is r cos (θ) × r × rdrdθ. Thus
xc =
R π/2
−π/2
R 2 cos(θ)
0
r2 cos (θ)
dA
z }| {
rdrdθ
R π/2
−π/2
R 2 cos(θ)
0
r ×
dA
z }| {
rdrdθ
= 6
5.
Similarly,
yc =
R π/2
−π/2
R 2 cos(θ)
0
r2 sin (θ) rdrdθ
R π/2
−π/2
R 2 cos(θ)
0
r2drdθ
= 0
Example 23.2.5 Let a plate occupy the region, C which is inside the polar graph, r =
2 + cos θ for θ ∈[0, 2π]. Suppose the density of this plate is given by δ (r, θ) = r. Find the
mass and center of mass of the plate.

414
THE RIEMANN INTEGRAL ON RN
Here you need to evaluate the following to get the total mass.
Z 2π
0
Z 2+cos θ
0
r × rdrdθ =
Z 2π
0
Z 2+cos θ
0
r2drdθ = 22
3 π
Now recall the center of mass is given by
xc ≡
R
U xρ (x, y) dV
R
U ρ (x, y) dV , yc ≡
R
U yρ (x, y) dV
R
U ρ (x, y) dV
Thus
xc =
R 2π
0
R 2+cos θ
0
(r cos θ) r2drdθ
22
3 π
= 57
44
and
yc =
R 2π
0
R 2+cos θ
0
(r sin θ) r2drdθ
22
3 π
= 0
I think this might be impossible if you tried to do it in rectangular coordinates. However,
it is just a little tedious in polar coordinates. Be sure you understand the set up. This is
usually the thing which gives people the most trouble in these kinds of problems.
Example 23.2.6 Let f (x, y) = sin
¡
x2 + y2¢
for (x, y) in the circle, D =
©
(x, y) : x2 + y2 ≤9
ª
.
Find
Z
D
fdA.
You don’t want to try this in rectangular coordinates even though the function is given in
rectangular coordinates. You should change it to polar coordinates for two reasons. The ﬁrst
is that x2 + y2 = r2 and it is easier to look at sin
¡
r2¢
than sin
¡
x2 + y2¢
. The main reason
is that the integration is taking place on a circle which is a rectangle in polar coordinates
and as explained earlier, it is easy to integrate over rectangles. In this case the rectangle is
[0, 2π] × [0, 3] . Thus the integral to work is
Z 2π
0
Z 3
0
sin
¡
r2¢
dA
z }| {
rdrdθ = −π cos 9 + π.
Example 23.2.7 Remember the formula for the area between two polar graphs r = f (θ)
and r = g (θ) , g (θ) > f (θ) for θ ∈[a, b] is given by
1
2
Z b
a
³
g (θ)2 −f (θ)2´
dθ.
Show this formula from one variable calculus follows from the form of the area increment
given here.
Denote by R the region between the two graphs. Then you need to ﬁnd
Z
R
dA =
Z b
a
Z g(θ)
f(θ)
rdrdθ = 1
2
Z b
a
³
g (θ)2 −f (θ)2´
dθ
(23.6)
which is the formula done earlier.
Here is an example.

23.2.
DOUBLE INTEGRALS IN POLAR COORDINATES
415
Example 23.2.8 Find the area of the region inside the cardioid, r = 1 + cos θ and outside
the circle, r = 1 for θ ∈
£
−π
2 , π
2
¤
.
As is usual in such cases, it is a good idea to graph the curves involved to get an idea
what is wanted. It is very important to ﬁgure out which function is farther from the origin.
1
0
0.5
-0.5
-1
2
1.5
-0.5
1
-1
0
0.5
desired
region
Then you need
Z π/2
−π/2
Z 1+cos θ
1
rdrdθ = 2 + 1
4π
You could also work it using the formula derived in 23.6 which is like what you did in one
variable calculus.
Example 23.2.9 Let f (x, y) = ex2+y2 for (x, y) in the pie shaped region P deﬁned by
r ∈[0, 2] and θ ∈[0, π/6] .
Be sure you can see why the integral wanted is
Z 2
0
Z π/6
0
er2rdθdr = 1
12e4π −1
12π
Example 23.2.10 Find
Z 1
−1
Z √
1−x2
−
√
1−x2
p
1 + x2 + y2dydx.
In this example you are integrating the function, f (x, y) =
p
1 + x2 + y2 over the circle
of radius 1 centered at the origin. Therefore, changing to polar coordinates it equals
Z 2π
0
Z 1
0
p
1 + r2rdrdθ = 4
3π
√
2 −2
3π
In this case, I think you could have done it without changing to polar coordinates but it
would involve wading through much aﬄiction and sorrow. Of course if you like adversity,
you could try to do it this way.
Example 23.2.11 Find
R ∞
0
e−x2dx.

416
THE RIEMANN INTEGRAL ON RN
Let I =
R ∞
0
e−x2dx. Then
I2
=
µZ ∞
0
e−x2dx
¶ µZ ∞
0
e−y2dy
¶
=
Z ∞
0
Z ∞
0
e−(x2+y2)dxdy
=
Z π/2
0
Z ∞
0
e−r2rdrdθ = 1
4π
It follows I =
√π
2 . This is a very important formula. You showed, (hopefully) in one variable
calculus that this integral exists. Now with the aid of polar coordinates you can actually
ﬁnd it.
23.3
Methods For Triple Integrals 2-7 Nov.
23.3.1
Deﬁnition Of The Integral
The integral of a function of three variables is similar to the integral of a function of two
variables.
Deﬁnition 23.3.1 For i = 1, 2, 3 let
©
αi
k
ª∞
k=−∞be points on R which satisfy
lim
k→∞αi
k = ∞,
lim
k→−∞αi
k = −∞, αi
k < αi
k+1.
(23.7)
For such sequences, deﬁne a grid on R3 denoted by G or F as the collection of boxes of the
form
Q =
£
α1
k, α1
k+1
¤
×
£
α2
l , α2
l+1
¤
×
£
α3
p, α3
p+1
¤
.
(23.8)
If G is a grid, F is called a reﬁnement of G if every box of G is the union of boxes of F.
For G a grid,
X
Q∈G
MQ (f) v (Q)
is the upper sum associated with the grid, G where
MQ (f) ≡sup {f (x) : x ∈Q}
and if Q = [a, b]×[c, d]×[e, f] , then v (Q) is the volume of Q given by (b −a) (d −c) (f −e) .
Letting
mQ (f) ≡inf {f (x) : x ∈Q}
the lower sum associated with this partition is
X
Q∈G
mQ (f) v (Q) ,
With this preparation it is time to give a deﬁnition of the Riemann integral of a function
of three variables. This deﬁnition is just like the one for a function of two variables.
Deﬁnition 23.3.2 Let f : R3 →R be a bounded function which equals zero outside
some bounded subset of R3.
R
f dV is deﬁned as the unique number between all the upper
sums and lower sums.

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
417
As in the case of a function of two variables there are all sorts of mathematical questions
which are dealt with later.
The way to think of integrals is as follows. Located at a point x, there is an “inﬁnites-
imal” chunk of volume, dV. The integral involves taking this little chunk of volume, dV ,
multiplying it by f (x) and then adding up all such products. Upper sums are too large and
lower sums are too small but the unique number between all the lower and upper sums is
just right and corresponds to the notion of adding up all the f (x) dV. Even the notation
is suggestive of this concept of sum. It is a long thin S denoting sum. This is the fun-
damental concept for the integral in any number of dimensions and all the deﬁnitions and
technicalities are designed to give precision and mathematical respectability to this notion.
To consider how to evaluate triple integrals, imagine a sum of the form P
ijk aijk where
there are only ﬁnitely many choices for i, j, and k and the symbol means you simply add
up all the aijk. By the commutative law of addition, these may be added systematically in
the form, P
k
P
j
P
i aijk. A similar process is used to evaluate triple integrals and since
integrals are like sums, you might expect it to be valid. Speciﬁcally,
Z
f dV =
Z ?
?
Z ?
?
Z ?
?
f (x, y, z) dx dy dz.
In words, sum with respect to x and then sum what you get with respect to y and ﬁnally,
with respect to z. Of course this should hold in any other order such as
Z
f dV =
Z ?
?
Z ?
?
Z ?
?
f (x, y, z) dz dy dx.
This is proved in an appendix1.
Having discussed double and triple integrals, the deﬁnition of the integral of a function
of n variables is accomplished in the same way.
Deﬁnition 23.3.3 For i = 1, · · ·, n, let
©
αi
k
ª∞
k=−∞be points on R which satisfy
lim
k→∞αi
k = ∞,
lim
k→−∞αi
k = −∞, αi
k < αi
k+1.
(23.9)
For such sequences, deﬁne a grid on Rn denoted by G or F as the collection of boxes of the
form
Q =
n
Y
i=1
£
αi
ji, αi
ji+1
¤
.
(23.10)
If G is a grid, F is called a reﬁnement of G if every box of G is the union of boxes of F.
Deﬁnition 23.3.4 Let f be a bounded function which equals zero oﬀa bounded set,
D, and let G be a grid. For Q ∈G, deﬁne
MQ (f) ≡sup {f (x) : x ∈Q} , mQ (f) ≡inf {f (x) : x ∈Q} .
(23.11)
Also deﬁne for Q a box, the volume of Q, denoted by v (Q) by
v (Q) ≡
n
Y
i=1
(bi −ai) , Q ≡
n
Y
i=1
[ai, bi] .
1All of these fundamental questions about integrals can be considered more easily in the context of the
Lebesgue integral. However, this integral is more abstract than the Riemann integral.

418
THE RIEMANN INTEGRAL ON RN
Now deﬁne upper sums, UG (f) and lower sums, LG (f) with respect to the indicated grid,
by the formulas
UG (f) ≡
X
Q∈G
MQ (f) v (Q) , LG (f) ≡
X
Q∈G
mQ (f) v (Q) .
Then a function of n variables is Riemann integrable if there is a unique number between
all the upper and lower sums. This number is the value of the integral.
In this book most integrals will involve no more than three variables. However, this does
not mean an integral of a function of more than three variables is unimportant. Therefore,
I will begin to refer to the general case when theorems are stated.
Deﬁnition 23.3.5 For E ⊆Rn,
XE (x) ≡
½
1 if x ∈E
0 if x /∈E
.
Deﬁne
R
E f dV ≡
R
XEf dV when fXE ∈R (Rn) .
23.3.2
Iterated Integrals
As before, the integral is often computed by using an iterated integral. In general it is
impossible to set up an iterated integral for ﬁnding
R
E fdV for arbitrary regions, E but
when the region is suﬃciently simple, one can make progress. Suppose the region, E over
which the integral is to be taken is of the form E = {(x, y, z) : a (x, y) ≤z ≤b (x, y)} for
(x, y) ∈R, a two dimensional region. This is illustrated in the following picture in which
the bottom surface is the graph of z = a (x, y) and the top is the graph of z = b (x, y).
¡
¡
¡
¡
¡
¡
x
z
y
R
Then
Z
E
fdV =
Z
R
Z b(x,y)
a(x,y)
f (x, y, z) dzdA

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
419
It might be helpful to think of dV = dzdA. Now
R b(x,y)
a(x,y) f (x, y, z) dz is a function of x
and y and so you have reduced the triple integral to a double integral over R of this
function of x and y. Similar reasoning would apply if the region in R3 were of the form
{(x, y, z) : a (y, z) ≤x ≤b (y, z)} or {(x, y, z) : a (x, z) ≤y ≤b (x, z)} .
Example 23.3.6 Find the volume of the region, E in the ﬁrst octant between z = 1−(x + y)
and z = 0.
In this case, R is the region shown.
@
@
@
@
@@
x
y
R
1
Thus the region, E is between the plane z = 1−(x + y) on the top, z = 0 on the bottom,
and over R shown above. Thus
Z
E
1dV
=
Z
R
Z 1−(x+y)
0
dzdA
=
Z 1
0
Z 1−x
0
Z 1−(x+y)
0
dzdydx = 1
6
Of course iterated integrals have a life of their own although this will not be explored
here. You can just write them down and go to work on them. Here are some examples.
Example 23.3.7 Find
R 3
2
R x
3
R x
3y (x −y) dz dy dx.
The inside integral yields
R x
3y (x −y) dz = x2 −4xy + 3y2. Next this must be integrated
with respect to y to give
R x
3
¡
x2 −4xy + 3y2¢
dy = −3x2 + 18x −27. Finally the third
integral gives
Z 3
2
Z x
3
Z x
3y
(x −y) dz dy dx =
Z 3
2
¡
−3x2 + 18x −27
¢
dx = −1.
Example 23.3.8 Find
R π
0
R 3y
0
R y+z
0
cos (x + y) dx dz dy.
The inside integral is
R y+z
0
cos (x + y) dx = 2 cos z sin y cos y+2 sin z cos2 y−sin z−sin y.
Now this has to be integrated.
Z 3y
0
Z y+z
0
cos (x + y) dx dz =
Z 3y
0
¡
2 cos z sin y cos y + 2 sin z cos2 y −sin z −sin y
¢
dz
= −1 −16 cos5 y + 20 cos3 y −5 cos y −3 (sin y) y + 2 cos2 y.
Finally, this last expression must be integrated from 0 to π. Thus
Z π
0
Z 3y
0
Z y+z
0
cos (x + y) dx dz dy
=
Z π
0
¡
−1 −16 cos5 y + 20 cos3 y −5 cos y −3 (sin y) y + 2 cos2 y
¢
dy
=
−3π

420
THE RIEMANN INTEGRAL ON RN
Example 23.3.9 Here is an iterated integral:
R 2
0
R 3−3
2 x
0
R x2
0
dz dy dx. Write as an iterated
integral in the order dz dx dy.
The inside integral is just a function of x and y. (In fact, only a function of x.) The order
of the last two integrals must be interchanged. Thus the iterated integral which needs to be
done in a diﬀerent order is
Z 2
0
Z 3−3
2 x
0
f (x, y) dy dx.
As usual, it is important to draw a picture and then go from there.
J
J
J
J
J
JJ
3 −3
2x = y
3
2
Thus this double integral equals
Z 3
0
Z
2
3 (3−y)
0
f (x, y) dx dy.
Now substituting in for f (x, y) ,
Z 3
0
Z
2
3 (3−y)
0
Z x2
0
dz dx dy.
Example 23.3.10 Find the volume of the bounded region determined by 3y + 3z = 2, x =
16 −y2, y = 0, x = 0.
In the yz plane, the following picture corresponds to x = 0.
@
@
@
@
@
@
3y + 3z = 2
2
3
2
3
Therefore, the outside integrals taken with respect to z and y are of the form
R 2
3
0
R 2
3 −y
0
dz dy
and now for any choice of (y, z) in the above triangular region, x goes from 0 to 16 −y2.
Therefore, the iterated integral is
Z
2
3
0
Z
2
3 −y
0
Z 16−y2
0
dx dz dy = 860
243
Example 23.3.11 Find the volume of the region determined by the intersection of the two
cylinders, x2 + y2 ≤9 and y2 + z2 ≤9.
The ﬁrst listed cylinder intersects the xy plane in the disk, x2 + y2 ≤9. What is the
volume of the three dimensional region which is between this disk and the two surfaces,
z =
p
9 −y2 and z = −
p
9 −y2? An iterated integral for the volume is
Z 3
−3
Z √
9−y2
−√
9−y2
Z √
9−y2
−√
9−y2 dz dx dy = 144.

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
421
Note I drew no picture of the three dimensional region. If you are interested, here it is.
One of the cylinders is parallel to the z axis, x2 + y2 ≤9 and the other is parallel to
the x axis, y2 + z2 ≤9. I did not need to be able to draw such a nice picture in order to
work this problem. This is the key to doing these. Draw pictures in two dimensions and
reason from the two dimensional pictures rather than attempt to wax artistic and consider
all three dimensions at once. These problems are hard enough without making them even
harder by attempting to be an artist.
23.3.3
Mass And Density
As an example of the use of triple integrals, consider a solid occupying a set of points,
U ⊆R3 having density ρ. Thus ρ is a function of position and the total mass of the solid
equals
Z
U
ρ dV.
This is just like the two dimensional case. The mass of an inﬁnitesimal chunk of the solid
located at x would be ρ (x) dV and so the total mass is just the sum of all these,
R
U ρ (x) dV.
Example 23.3.12 Find the volume of R where R is the bounded region formed by the plane
1
5x + y + 1
5z = 1 and the planes x = 0, y = 0, z = 0.
When z = 0, the plane becomes 1
5x + y = 1. Thus the intersection of this plane with the
xy plane is this line shown in the following picture.
``````````
1
5
Therefore, the bounded region is between the triangle formed in the above picture by
the x axis, the y axis and the above line and the surface given by 1
5x + y + 1
5z = 1 or
z = 5
¡
1 −
¡ 1
5x + y
¢¢
= 5 −x −5y. Therefore, an iterated integral which yields the volume
is
Z 5
0
Z 1−1
5 x
0
Z 5−x−5y
0
dz dy dx = 25
6 .
Example 23.3.13 Find the mass of the bounded region, R formed by the plane 1
3x + 1
3y +
1
5z = 1 and the planes x = 0, y = 0, z = 0 if the density is ρ (x, y, z) = z.

422
THE RIEMANN INTEGRAL ON RN
This is done just like the previous example except in this case there is a function to
integrate. Thus the answer is
Z 3
0
Z 3−x
0
Z 5−5
3 x−5
3 y
0
z dz dy dx = 75
8 .
Example 23.3.14 Find the total mass of the bounded solid determined by z = 9 −x2 −y2
and x, y, z ≥0 if the mass is given by ρ (x, y, z) = z
When z = 0 the surface, z = 9 −x2 −y2 intersects the xy plane in a circle of radius 3
centered at (0, 0) . Since x, y ≥0, it is only a quarter of a circle of interest, the part where
both these variables are nonnegative. For each (x, y) inside this quarter circle, z goes from
0 to 9 −x2 −y2. Therefore, the iterated integral is of the form,
Z 3
0
Z √
(9−x2)
0
Z 9−x2−y2
0
z dz dy dx = 243
8 π
Example 23.3.15 Find the volume of the bounded region determined by x ≥0, y ≥0, z ≥0,
and 1
7x + y + 1
4z = 1, and x + 1
7y + 1
4z = 1.
When z = 0, the plane 1
7x+y+ 1
4z = 1 intersects the xy plane in the line whose equation
is
1
7x + y = 1
while the plane, x + 1
7y + 1
4z = 1 intersects the xy plane in the line whose equation is
x + 1
7y = 1.
Furthermore, the two planes intersect when x = y as can be seen from the equations,
x + 1
7y = 1 −z
4 and 1
7x + y = 1 −z
4 which imply x = y. Thus the two dimensional picture
to look at is depicted in the following picture.
D
D
D
D
D
D
D
D
D
D
D
DD
`````````````
x + 1
7y + 1
4z = 1
y + 1
7x + 1
4z = 1
¡
¡
¡¡
R1
R2
y = x
You see in this picture, the base of the region in the xy plane is the union of the two
triangles, R1 and R2. For (x, y) ∈R1, z goes from 0 to what it needs to be to be on the
plane, 1
7x + y + 1
4z = 1. Thus z goes from 0 to 4
¡
1 −1
7x −y
¢
. Similarly, on R2, z goes from
0 to 4
¡
1 −1
7y −x
¢
. Therefore, the integral needed is
Z
R1
Z 4(1−1
7 x−y)
0
dz dV +
Z
R2
Z 4(1−1
7 y−x)
0
dz dV

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
423
and now it only remains to consider
R
R1 dV and
R
R2 dV. The point of intersection of these
lines shown in the above picture is
¡ 7
8, 7
8
¢
and so an iterated integral is
Z 7/8
0
Z 1−x
7
x
Z 4(1−1
7 x−y)
0
dz dy dx +
Z 7/8
0
Z 1−y
7
y
Z 4(1−1
7 y−x)
0
dz dx dy = 7
6.
23.3.4
Exercises With Answers
1. Evaluate the integral
R 7
4
R 3x
5
R x
5y dz dy dx
Answer:
−3417
2
2. Find
R 4
0
R 2−5x
0
R 4−2x−y
0
(2x) dz dy dx
Answer:
−2464
3
3. Find
R 2
0
R 2−5x
0
R 1−4x−3y
0
(2x) dz dy dx
Answer:
−196
3
4. Evaluate the integral
R 8
5
R 3x
4
R x
4y (x −y) dz dy dx
Answer:
114 607
8
5. Evaluate the integral
R π
0
R 4y
0
R y+z
0
cos (x + y) dx dz dy
Answer:
−4π
6. Evaluate the integral
R π
0
R 2y
0
R y+z
0
sin (x + y) dx dz dy
Answer:
−19
4
7. Fill in the missing limits.
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dx dz dy,
R 1
0
R z
0
R 2z
0
f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dy dz dx,
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dz dy dx,
R 1
0
R √z
z/2
R y+z
0
f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dx dz dy,
R 7
5
R 5
2
R 3
0 f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dz dy dx.
Answer:
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R 1
0
R 1
y
R z
0 f (x, y, z) dx dz dy,
R 1
0
R z
0
R 2z
0
f (x, y, z) dx dy dz =
R 2
0
R 1
x/2
R z
0 f (x, y, z) dy dz dx,
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R 1
0
hR x
0
R 1
x f (x, y, z) dz dy +
R 1
x
R 1
y f (x, y, z) dz dy
i
dx,
R 1
0
R √z
z/2
R y+z
0
f (x, y, z) dx dy dz =

424
THE RIEMANN INTEGRAL ON RN
R 1/2
0
R 2y
y2
R y+z
0
f (x, y, z) dx dz dy +
R 1
1/2
R 1
y2
R y+z
0
f (x, y, z) dx dz dy
R 7
5
R 5
2
R 3
0 f (x, y, z) dx dy dz =
R 3
0
R 5
2
R 7
5 f (x, y, z) dz dy dx
8. Find the volume of R where R is the bounded region formed by the plane 1
5x+y+ 1
4z =
1 and the planes x = 0, y = 0, z = 0.
Answer:
R 5
0
R 1−1
5 x
0
R 4−4
5 x−4y
0
dz dy dx = 10
3
9. Find the volume of R where R is the bounded region formed by the plane 1
5x+ 1
2y+ 1
4z =
1 and the planes x = 0, y = 0, z = 0.
Answer:
R 5
0
R 2−2
5 x
0
R 4−4
5 x−2y
0
dz dy dx = 20
3
10. Find the mass of the bounded region, R formed by the plane 1
4x + 1
2y + 1
3z = 1 and
the planes x = 0, y = 0, z = 0 if the density is ρ (x, y, z) = y
Answer:
R 4
0
R 2−1
2 x
0
R 3−3
4 x−3
2 y
0
(y) dz dy dx = 2
11. Find the mass of the bounded region, R formed by the plane 1
2x + 1
2y + 1
4z = 1 and
the planes x = 0, y = 0, z = 0 if the density is ρ (x, y, z) = z2
Answer:
R 2
0
R 2−x
0
R 4−2x−2y
0
¡
z2¢
dz dy dx = 64
15
12. Here is an iterated integral:
R 3
0
R 3−x
0
R x2
0
dz dy dx. Write as an iterated integral in the
following orders: dz dx dy, dx dz dy, dx dy dz, dy dx dz, dy dz dx.
Answer:
Z 3
0
Z x2
0
Z 3−x
0
dy dz dx,
Z 9
0
Z 3
√z
Z 3−x
0
dy dx dz,
Z 9
0
Z 3−√z
0
Z 3−y
√z
dx dy dz,
Z 3
0
Z 3−y
0
Z x2
0
dz dx dy,
Z 3
0
Z (3−y)2
0
Z 3−y
√z
dx dz dy
13. Find the volume of the bounded region determined by 5y + 2z = 4, x = 4 −y2, y =
0, x = 0.
Answer:
R 4
5
0
R 2−5
2 y
0
R 4−y2
0
dx dz dy = 1168
375
14. Find the volume of the bounded region determined by 4y + 3z = 3, x = 4 −y2, y =
0, x = 0.
Answer:
R 3
4
0
R 1−4
3 y
0
R 4−y2
0
dx dz dy = 375
256
15. Find the volume of the bounded region determined by 3y + z = 3, x = 4 −y2, y =
0, x = 0.
Answer:
R 1
0
R 3−3y
0
R 4−y2
0
dx dz dy = 23
4

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
425
16. Find the volume of the region bounded by x2 + y2 = 16, z = 3x, z = 0, and x ≥0.
Answer:
R 4
0
R √
(16−x2)
−√
(16−x2)
R 3x
0
dz dy dx = 128
17. Find the volume of the region bounded by x2 + y2 = 25, z = 2x, z = 0, and x ≥0.
Answer:
R 5
0
R √
(25−x2)
−√
(25−x2)
R 2x
0
dz dy dx = 500
3
18. Find the volume of the region determined by the intersection of the two cylinders,
x2 + y2 ≤9 and y2 + z2 ≤9.
Answer:
8
R 3
0
R √
(9−y2)
0
R √
(9−y2)
0
dz dx dy = 144
19. Find the total mass of the bounded solid determined by z = a2−x2−y2 and x, y, z ≥0
if the mass is given by ρ (x, y, z) = z
Answer:
R 4
0
R √
(16−x2)
0
R 16−x2−y2
0
(z) dz dy dx = 512
3 π
20. Find the total mass of the bounded solid determined by z = a2−x2−y2 and x, y, z ≥0
if the mass is given by ρ (x, y, z) = x + 1
Answer:
R 5
0
R √
(25−x2)
0
R 25−x2−y2
0
(x + 1) dz dy dx = 625
8 π + 1250
3
21. Find the volume of the region bounded by x2 + y2 = 9, z = 0, z = 5 −y
Answer:
R 3
−3
R √
(9−x2)
−√
(9−x2)
R 5−y
0
dz dy dx = 45π
22. Find the volume of the bounded region determined by x ≥0, y ≥0, z ≥0, and
1
2x + y + 1
2z = 1, and x + 1
2y + 1
2z = 1.
Answer:
R 2
3
0
R 1−1
2 x
x
R 2−x−2y
0
dz dy dx +
R 2
3
0
R 1−1
2 y
y
R 2−2x−y
0
dz dx dy = 4
9
23. Find the volume of the bounded region determined by x ≥0, y ≥0, z ≥0, and
1
7x + y + 1
3z = 1, and x + 1
7y + 1
3z = 1.
Answer:
R 7
8
0
R 1−1
7 x
x
R 3−3
7 x−3y
0
dz dy dx +
R 7
8
0
R 1−1
7 y
y
R 3−3x−3
7 y
0
dz dx dy = 7
8
24. Find the mass of the solid determined by 25x2 + 4y2 ≤9, z ≥0, and z = x + 2 if the
density is ρ (x, y, z) = x.
Answer:
R 3
5
−3
5
R 1
2
√
(9−25x2)
−1
2
√
(9−25x2)
R x+2
0
(x) dz dy dx =
81
1000π
25. Find
R 1
0
R 35−5z
0
R 7−z
1
5 x
(7 −z) cos
¡
y2¢
dy dx dz.
Answer:
You need to interchange the order of integration.
R 1
0
R 7−z
0
R 5y
0
(7 −z) cos
¡
y2¢
dx dy dz =
5
4 cos 36 −5
4 cos 49

426
THE RIEMANN INTEGRAL ON RN
26. Find
R 2
0
R 12−3z
0
R 4−z
1
3 x
(4 −z) exp
¡
y2¢
dy dx dz.
Answer:
You need to interchange the order of integration.
R 2
0
R 4−z
0
R 3y
0
(4 −z) exp
¡
y2¢
dx dy dz
= −3
4e4 −9 + 3
4e16
27. Find
R 2
0
R 25−5z
0
R 5−z
1
5 y
(5 −z) exp
¡
x2¢
dx dy dz.
Answer:
You need to interchange the order of integration.
Z 2
0
Z 5−z
0
Z 5x
0
(5 −z) exp
¡
x2¢
dy dx dz = −5
4e9 −20 + 5
4e25
28. Find
R 1
0
R 10−2z
0
R 5−z
1
2 y
sin x
x
dx dy dz.
Answer:
You need to interchange the order of integration.
Z 1
0
Z 5−z
0
Z 2x
0
sin x
x
dy dx dz =
−2 sin 1 cos 5 + 2 cos 1 sin 5 + 2 −2 sin 5
29. Find
R 20
0
R 2
0
R 6−z
1
5 y
sin x
x
dx dz dy +
R 30
20
R 6−1
5 y
0
R 6−z
1
5 y
sin x
x
dx dz dy.
Answer:
You need to interchange the order of integration.
Z 2
0
Z 30−5z
0
Z 6−z
1
5 y
sin x
x
dx dy dz =
Z 2
0
Z 6−z
0
Z 5x
0
sin x
x
dy dx dz
= −5 sin 2 cos 6 + 5 cos 2 sin 6 + 10 −5 sin 6

The Integral In Other
Coordinates 8-10 Nov.
24.1
Diﬀerent Coordinates
As mentioned above, the fundamental concept of an integral is a sum of things of the form
f (x) dV where dV is an “inﬁnitesimal” chunk of volume located at the point, x. Up to
now, this inﬁnitesimal chunk of volume has had the form of a box with sides dx1, ···, dxn so
dV = dx1 dx2 · · · dxn but its form is not important. It could just as well be an inﬁnitesimal
parallelepiped or parallelogram for example. In what follows, this is what it will be.
First recall the deﬁnition of the box product given in Deﬁnition 3.2.8 on Page 52. The
absolute value of the box product of three vectors gave the volume of the parallelepiped
determined by the three vectors.
Deﬁnition 24.1.1 Let u1, u2, u3 be vectors in R3. The parallelepiped determined
by these vectors will be denoted by P (u1, u2, u3) and it is deﬁned as
P (u1, u2, u3) ≡



3
X
j=1
sjuj : sj ∈[0, 1]


.
Lemma 24.1.2 The volume of the parallelepiped, P (u1, u2, u3) is given by
¯¯det
¡
u1
u2
u3
¢¯¯
where
¡
u1
u2
u3
¢
is the matrix having columns u1, u2, and u3.
Proof: Recall from the discussion of the box product or triple product,
volume of P (u1, u2, u3) ≡|[u1, u2, u3]| =
¯¯¯¯¯¯
det


uT
1
uT
2
uT
3


¯¯¯¯¯¯
where


uT
1
uT
2
uT
3

is the matrix having rows equal to the vectors, u1, u2 and u3 arranged
horizontally. Since the determinant of a matrix equals the determinant of its transpose,
volume of P (u1, u2, u3) = |[u1, u2, u3]| =
¯¯det
¡ u1
u2
u3
¢¯¯ .
This proves the lemma.
427

428
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Deﬁnition 24.1.3 In the case of two vectors, P (u1, u2) will denote the parallelo-
gram determined by u1 and u2. Thus
P (u1, u2) ≡



2
X
j=1
sjuj : sj ∈[0, 1]


.
Lemma 24.1.4 The area of the parallelogram, P (u1, u2) is given by
¯¯det
¡
u1
u2
¢¯¯
where
¡
u1
u2
¢
is the matrix having columns u1 and u2.
Proof: Letting u1 = (a, b)T and u2 = (c, d)T , consider the vectors in R3 deﬁned by
bu1 ≡(a, b, 0)T and bu2 ≡(c, d, 0)T . Then the area of the parallelogram determined by the
vectors, bu1 and bu2 is the norm of the cross product of bu1 and bu2. This follows directly from
the geometric deﬁnition of the cross product given in Deﬁnition 3.2.2 on Page 49. But this
is the same as the area of the parallelogram determined by the vectors u1, u2. Taking the
cross product of bu1 and bu2 yields k (ad −bc) . Therefore, the norm of this cross product is
|ad −bc|
which is the same as
¯¯det
¡ u1
u2
¢¯¯
where
¡
u1
u2
¢
denotes the matrix having the two vectors u1, u2 as columns. This proves
the lemma.
It always works this way. The n dimensional volume of the n dimensional parallelepiped
determined by the vectors, {v1, · · ·, vn} is always
¯¯det
¡ v1
· · ·
vn
¢¯¯
This general fact will not be used in what follows.
24.1.1
Review Of Polar Coordinates
Earlier it was shown based on geometric reasoning that the appropriate increment of area
in polar coordinates is rdrdθ. Consider this again in a slightly diﬀerent way. Recall the
transformation equations for polar coordinates,
h (r, θ) =
µ
x
y
¶
=
µ
r cos θ
r sin θ
¶
As before, you need to approximate the area of the curvy shape shown in the following
picture.
s
s
s
h(r, θ)
h(r, θ + ∆θ)
h(r + ∆r, θ)

24.1.
DIFFERENT COORDINATES
429
As ∆θ gets smaller, the curvy shape will be better and better approximated by the
dotted parallelogram shown in the picture.
So what is the area of this parallelogram?
The parallelogram is determined by the vectors, h (r, θ + ∆θ) −h (r, θ) and h (r + ∆r, θ) −
h (r, θ) . For very small ∆r and ∆θ, these two vectors are essentially equal to hθ (r, θ) ∆θ
and hr (r, θ) ∆r. Therefore, from the above discussion, the increment of area is essentially
equal to
¯¯det
¡
hθ (r, θ) ∆θ
hr (r, θ) ∆r
¢¯¯ =
¯¯det
¡
hθ (r, θ)
hr (r, θ)
¢¯¯ ∆r∆θ
Now
hθ (r, θ) =
µ
−r sin θ
r cos θ
¶
, hr (r, θ) =
µ
cos θ
sin θ
¶
and so the above reduces to
¯¯¯¯
µ
−r sin θ
cos θ
r cos θ
sin θ
¶¯¯¯¯ ∆r∆θ = r∆r∆θ
and this is why the area increment in polar coordinates is rdrdθ. Note the emphasis on alge-
braic techniques to ﬁnd the area increment. The same approach works for other coordinates,
not just polar coordinates.
24.1.2
General Two Dimensional Coordinates
Suppose U is a set in R2 and h is a C1 function1 mapping U one to one onto h (U) , a set
in R2. Consider a small square inside U. The following picture is of such a square having a
corner at the point, u0 and sides as indicated. The image of this square is also represented.
-
6
u0
∆u1e1
∆u2e2
∆V
u
h(u0)
h(u0 + ∆u1e1)
u
h(∆V)
u
h(u0 + ∆u2e2)
For small ∆ui you would expect the sides going from h (u0) to h (u0 + ∆u1e1) and from
h (u0) to h (u0 + ∆u2e2) to be almost the same as the vectors, h (u0 + ∆u1e1)−h (u0) and
h (u0 + ∆u2e2) −h (u0) which are approximately equal to
∂h
∂u1 (u0) ∆u1 and
∂h
∂u2 (u0) ∆u2
respectively. Therefore, the area of h (∆V ) for small ∆ui is essentially equal to the area
of the parallelogram determined by the two vectors,
∂h
∂u1 (u0) ∆u1 and
∂h
∂u2 (u0) ∆u2. By
Lemma 24.1.4 this equals
¯¯det
¡
∂h
∂u1 (u0) ∆u1
∂h
∂u2 (u0) ∆u2
¢¯¯ =
¯¯det
¡
∂h
∂u1 (u0)
∂h
∂u2 (u0) ¢¯¯ ∆u1∆u2
1By this is meant h is the restriction to U of a function deﬁned on an open set containing U which is
C1. If you like, you can assume U is open but this is not necessary. Neither is C1.

430
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Thus an inﬁnitesimal chunk of area in h (U) located at u0 is of the form
¯¯det
¡
∂h
∂u1 (u0)
∂h
∂u2 (u0) ¢¯¯ dV
where dV is a corresponding chunk of area located at the point u0. This shows the following
change of variables formula is reasonable.
Z
h(U)
f (x) dV (x) =
Z
U
f (h (u))
¯¯det
¡
∂h
∂u1 (u)
∂h
∂u2 (u) ¢¯¯ dV (u)
Deﬁnition 24.1.5 Let h : U →h (U) be a one to one and C1 mapping.
The
(volume) area element in terms of u is deﬁned as
¯¯det
¡
∂h
∂u1 (u)
∂h
∂u2 (u) ¢¯¯ dV (u) . The
factor,
¯¯det
¡
∂h
∂u1 (u)
∂h
∂u2 (u) ¢¯¯ is called the Jacobian. It equals
¯¯¯¯¯¯¯¯
det




∂h1
∂u1
(u1, u2)
∂h1
∂u2
(u1, u2)
∂h2
∂u1
(u1, u2)
∂h2
∂u2
(u1, u2)




¯¯¯¯¯¯¯¯
.
It is traditional to call two dimensional volumes area. However, it is probably better
to simply always refer to it as volume. Thus there is 2 dimensional volume, 3 dimensional
volume, etc. Sometimes you can get confused by too many diﬀerent words to describe things
which are really not essentially diﬀerent.
Example 24.1.6 Find the area element for polar coordinates.
Here the u coordinates are θ and r. The polar coordinate transformations are
µ
x
y
¶
=
µ
r cos θ
r sin θ
¶
Therefore, the volume (area) element is
¯¯¯¯det
µ
cos θ
−r sin θ
sin θ
r cos θ
¶¯¯¯¯ dθdr = rdθdr.
Example 24.1.7 Suppose x = u2 −v2 and y = 2uv Find the area element in terms of u
and v.
You are given
µ
x
y
¶
=
µ
u2 −v2
2uv
¶
and so the area element is
¯¯¯¯det
µ
2u
−2v
2v
2u
¶¯¯¯¯ dudv =
¡
4u2 + 4v2¢
dudv
Example 24.1.8 Suppose new coordinates are given by u = x + y and v = y/x ﬁnd the
area element in terms of the new coordinates, u and v.
You need to solve for x and y ﬁrst. This yields y =
uv
v+1, x =
u
v+1. Thus
µ
x
y
¶
=
µ
u
v+1
uv
v+1
¶
and so the area increment equals
¯¯¯¯¯det
Ã
1
v+1
−
u
(v+1)2
v
v+1
u
v+1 −v
u
(v+1)2
!¯¯¯¯¯ dudv =
¯¯¯¯¯
u
(v + 1)2
¯¯¯¯¯ dudv

24.1.
DIFFERENT COORDINATES
431
Example 24.1.9 The area density is given by ρ (x, y, z) = x and a plate occupies the region
between x + y = 1, x + y = 4 and the lines y = x and y = 2x. Find the x coordinate of the
center of mass of this plate.
Here is a picture of the two dimensional region occupied by this plate.
@
@
@
@
@
@
@
@
@
@
¡
¡
¡
¡
¡
¢
¢
¢
¢
¢
¢
R
It is labeled as R. Thus the total mass and the x coordinate of the center of mass are
given respectively as
Z
R
xdA, xc =
R
R x2dA
R
R xdA
So now you just set up the iterated integrals and go to work. Good luck if you try this. It
is much better to change the variables. Let u = x + y and v = y/x as in Example 24.1.8.
In these new coordinates the horrible quadrilateral becomes the rectangle
(u, v) ∈[1, 4] × [1, 2] .
That is u goes between 1 and 4 while v goes between 1 and 2. Remember it is easy to
integrate over rectangles. Thus using the result of Example 24.1.8
µ
x
y
¶
=
µ
u
v+1
uv
v+1
¶
and the area element is
u
(v + 1)2 dudv
Therefore, the total mass is
Z 4
1
Z 2
1
x
z
}|
{
µ
u
v + 1
¶
dA
z
}|
{
u
(v + 1)2 dudv = 49
200
and the x coordinate of the center of mass is
R 4
1
R 2
1
³
u
v+1
´2
u
(v+1)2 dudv
49
200
= 117
196
It is a tedious problem but not all that hard if you change the variables.
24.1.3
Three Dimensions
Quiz

432
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
1. Find
R 1
0
R 1
√y sin
¡
x3¢
dxdy.
2. Find the x coordinate of the center of mass of the solid whose density is δ (x, y, z) = x
which occupies the three dimensional region below the plane z = y and above the
triangular region in the xy plane determined by x ∈[0, 1] and 0 ≤y ≤x.
3. Maximize xy subject to the constraint x2 + y2 = 1.
4. Find the tangent plane to the surface, z2 + x2 + 3y2 = 5 at the point (1, 1, 1) .
The situation is no diﬀerent for coordinate systems in any number of dimensions al-
though I will concentrate here on three dimensions. A rectangular chunk of volume in the
u space corresponds to the curvy parallelepiped shown below which is approximated by the
parallelepiped shown on the left determined by the 3 vectors
n
∂x(u0)
∂ui dui
o3
i=1 for x = f (u)
where u ∈U, a subset of R3 and x is a point in V = f (U) , a subset of 3 dimensional space.
Thus, letting the Cartesian coordinates of x be given by x = (x1, x2, x3)T , each xi
being a function of u, an inﬁnitesimal box located at u0 corresponds to an inﬁnitesimal
parallelepiped located at f (u0) which is determined by the 3 vectors
n
∂x(u0)
∂ui dui
o3
i=1 . From
Lemma 24.1.2, the volume of this inﬁnitesimal parallelepiped located at f (u0) is given by
¯¯¯¯
·∂x (u0)
∂u1
du1, ∂x (u0)
∂u2
du2, ∂x (u0)
∂u3
du3
¸¯¯¯¯ =
¯¯¯¯
·∂x (u0)
∂u1
, ∂x (u0)
∂u2
, ∂x (u0)
∂u3
¸¯¯¯¯ du1du2du3
(24.1)
=
¯¯¯det
³
∂x(u0)
∂u1
∂x(u0)
∂u2
∂x(u0)
∂u3
´¯¯¯ du1du2du3
There is also no change in going to higher dimensions than 3.
Deﬁnition 24.1.10 Let x = f (u) be as described above. Then for n = 2, 3, the
symbol,
∂(x1,···xn)
∂(u1,···,un), called the Jacobian determinant, is deﬁned by
det
³
∂x(u0)
∂u1
· · ·
∂x(u0)
∂un
´
≡∂(x1, · · ·, xn)
∂(u1, · · ·, un).
Also, the symbol,
¯¯¯ ∂(x1,···,xn)
∂(u1,···,un)
¯¯¯ du1 · · · dun is called the volume element.

24.1.
DIFFERENT COORDINATES
433
This has given motivation for the following fundamental procedure often called the
change of variables formula which holds under fairly general conditions.
Procedure 24.1.11 Suppose U is an open subset of Rn for n = 2, 3 and suppose
f : U →f (U) is a C1 function which is one to one, x = f (u).2Then if h : f (U) →R,
Z
U
h (f (u))
¯¯¯¯
∂(x1, · · ·, xn)
∂(u1, · · ·, un)
¯¯¯¯ dV =
Z
f(U)
h (x) dV.
Now consider spherical coordinates. Recall the geometrical meaning of these coordinates
illustrated in the following picture.
-
6
¡
¡
¡
¡
¡
¡
¡
ª
x1
(x1, y1, 0)
y1
(ρ, φ, θ)
(r, θ, z1)
(x1, y1, z1)
z1
ρ
r
θ
φ
•
x
y
z
Thus there is a relationship between these coordinates and rectangular coordinates given
by
x = ρ sin φ cos θ, y = ρ sin φ sin θ, z = ρ cos φ
(24.2)
where φ ∈[0, π], θ ∈[0, 2π), and ρ > 0. Thus (ρ, φ, θ) is a point in R3, more speciﬁcally in
the set
U = (0, ∞) × [0, π] × [0, 2π)
and corresponding to such a (ρ, φ, θ) ∈U there exists a unique point, (x, y, z) ∈V where
V consists of all points of R3 other than the origin, (0, 0, 0) . This (x, y, z) determines a
unique point in three dimensional space as mentioned earlier. From the above argument,
the volume element is
¯¯¯det
³
∂x(ρ0,φ0,θ0)
∂ρ
∂x(ρ0,φ0,θ0)
∂φ
∂x(ρ0,φ0,θ0)
∂θ
´¯¯¯ dρdθdφ.
The mapping between spherical and rectangular coordinates is written as
x =


x
y
z

=


ρ sin φ cos θ
ρ sin φ sin θ
ρ cos φ

= f (ρ, φ, θ)
(24.3)
2This will cause non overlapping inﬁnitesimal boxes in U to be mapped to non overlapping inﬁnitesimal
parallelepipeds in V.
Also, in the context of the Riemann integral we should say more about the set U in any case the function,
h. These conditions are mainly technical however, and since a mathematically respectable treatment will
not be attempted for this theorem, I think it best to give a memorable version of it which is essentially
correct in all examples of interest. For a typical precise theorem see the appendix on the Riemann integral.

434
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Therefore, det
³
∂x(ρ0,φ0,θ0)
∂ρ
, ∂x(ρ0,φ0,θ0)
∂φ
, ∂x(ρ0,φ0,θ0)
∂θ
´
=
det


sin φ cos θ
ρ cos φ cos θ
−ρ sin φ sin θ
sin φ sin θ
ρ cos φ sin θ
ρ sin φ cos θ
cos φ
−ρ sin φ
0

= ρ2 sin φ
which is positive because φ ∈[0, π] .
Example 24.1.12 Find the volume of a ball, BR of radius R.
In this case, U = (0, R] × [0, π] × [0, 2π) and use spherical coordinates. Then 24.3 yields
a set in R3 which clearly diﬀers from the ball of radius R only by a set having volume equal
to zero. It leaves out the point at the origin is all. Therefore, the volume of the ball is
Z
BR
1 dV
=
Z
U
ρ2 sin φ dV
=
Z R
0
Z π
0
Z 2π
0
ρ2 sin φ dθ dφ dρ = 4
3R3π.
The reason this was eﬀortless, is that the ball, BR is realized as a box in terms of the
spherical coordinates. Remember what was pointed out earlier about setting up iterated
integrals over boxes.
Example 24.1.13 Find the volume element for cylindrical coordinates.
In cylindrical coordinates,


x
y
z

=


r cos θ
r sin θ
z


Therefore, the Jacobian determinant is
det


cos θ
−r sin θ
0
sin θ
r cos θ
0
0
0
1

= r.
It follows the volume element in cylindrical coordinates is r dθ dr dz.
Example 24.1.14 This example uses spherical coordinates to verify an important conclu-
sion about gravitational force. Let the hollow sphere, H be deﬁned by a2 < x2 + y2 + z2 < b2
and suppose this hollow sphere has constant density taken to equal α. Now place a unit mass
at the point (0, 0, z0) where |z0| ∈[a, b] . Show the force of gravity acting on this unit mass
is
µ
αG
R
H
(z−z0)
[x2+y2+(z−z0)2]
3/2 dV
¶
k and then show that if |z0| > b then the force of gravity
acting on this point mass is the same as if the entire mass of the hollow sphere were placed
at the origin, while if |z0| < a, the total force acting on the point mass from gravity equals
zero. Here G is the gravitation constant and α is the density. In particular, this shows that
the force a planet exerts on an object is as though the entire mass of the planet were situated
at its center3.
3This was shown by Newton in 1685 and allowed him to assert his law of gravitation applied to the
planets as though they were point masses. It was a major accomplishment.

24.1.
DIFFERENT COORDINATES
435
Without loss of generality, assume z0 > 0. Let dV be a little chunk of material located
at the point (x, y, z) of H the hollow sphere. Then according to Newton’s law of gravity,
the force this small chunk of material exerts on the given point mass equals
xi + yj + (z −z0) k
|xi + yj + (z −z0) k|
1
³
x2 + y2 + (z −z0)2´Gα dV =
(xi + yj + (z −z0) k)
1
³
x2 + y2 + (z −z0)2´3/2 Gα dV
Therefore, the total force is
Z
H
(xi + yj + (z −z0) k)
1
³
x2 + y2 + (z −z0)2´3/2 Gα dV.
By the symmetry of the sphere, the i and j components will cancel out when the integral is
taken. This is because there is the same amount of stuﬀfor negative x and y as there is for
positive x and y. Hence what remains is
αGk
Z
H
(z −z0)
h
x2 + y2 + (z −z0)2i3/2 dV
as claimed. Now for the interesting part, the integral is evaluated. In spherical coordinates
this integral is.
Z 2π
0
Z b
a
Z π
0
(ρ cos φ −z0) ρ2 sin φ
(ρ2 + z2
0 −2ρz0 cos φ)3/2 dφ dρ dθ.
(24.4)
Rewrite the inside integral and use integration by parts to obtain this inside integral equals
1
2z0
Z π
0
¡
ρ2 cos φ −ρz0
¢
(2z0ρ sin φ)
(ρ2 + z2
0 −2ρz0 cos φ)3/2 dφ =
1
2z0
Ã
−2
−ρ2 −ρz0
p
(ρ2 + z2
0 + 2ρz0)
+ 2
ρ2 −ρz0
p
(ρ2 + z2
0 −2ρz0)
−
Z π
0
2ρ2
sin φ
p
(ρ2 + z2
0 −2ρz0 cos φ)
dφ
!
.
(24.5)
There are some cases to consider here.
First suppose z0 < a so the point is on the inside of the hollow sphere and it is always
the case that ρ > z0. Then in this case, the two ﬁrst terms reduce to
2ρ (ρ + z0)
q
(ρ + z0)2 + 2ρ (ρ −z0)
q
(ρ −z0)2 = 2ρ (ρ + z0)
(ρ + z0)
+ 2ρ (ρ −z0)
ρ −z0
= 4ρ
and so the expression in 24.5 equals
1
2z0
Ã
4ρ −
Z π
0
2ρ2
sin φ
p
(ρ2 + z2
0 −2ρz0 cos φ)
dφ
!
=
1
2z0
Ã
4ρ −1
z0
Z π
0
ρ
2ρz0 sin φ
p
(ρ2 + z2
0 −2ρz0 cos φ)
dφ
!

436
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
=
1
2z0
µ
4ρ −2ρ
z0
¡
ρ2 + z2
0 −2ρz0 cos φ
¢1/2 |π
0
¶
=
1
2z0
µ
4ρ −2ρ
z0
[(ρ + z0) −(ρ −z0)]
¶
= 0.
Therefore, in this case the inner integral of 24.4 equals zero and so the original integral will
also be zero.
The other case is when z0 > b and so it is always the case that z0 > ρ. In this case the
ﬁrst two terms of 24.5 are
2ρ (ρ + z0)
q
(ρ + z0)2 + 2ρ (ρ −z0)
q
(ρ −z0)2 = 2ρ (ρ + z0)
(ρ + z0)
+ 2ρ (ρ −z0)
z0 −ρ
= 0.
Therefore in this case, 24.5 equals
1
2z0
Ã
−
Z π
0
2ρ2
sin φ
p
(ρ2 + z2
0 −2ρz0 cos φ)
dφ
!
=
−ρ
2z2
0
ÃZ π
0
2ρz0 sin φ
p
(ρ2 + z2
0 −2ρz0 cos φ)
dφ
!
which equals
−ρ
z2
0
³¡
ρ2 + z2
0 −2ρz0 cos φ
¢1/2 |π
0
´
=
−ρ
z2
0
[(ρ + z0) −(z0 −ρ)] = −2ρ2
z2
0
.
Thus the inner integral of 24.4 reduces to the above simple expression. Therefore, 24.4
equals
Z 2π
0
Z b
a
µ
−2
z2
0
ρ2
¶
dρ dθ = −4
3π b3 −a3
z2
0
and so
αGk
Z
H
(z −z0)
h
x2 + y2 + (z −z0)2i3/2 dV = αGk
µ
−4
3π b3 −a3
z2
0
¶
= −kGtotal mass
z2
0
.
24.1.4
Exercises With Answers
1. Find the area of the bounded region, R, determined by 3x+3y = 1, 3x+3y = 8, y = 3x,
and y = 4x.
Answer:
Let u = y
x, v = 3x + 3y. Then solving these equations for x and y yields
½
x = 1
3
v
1 + u, y = 1
3u
v
1 + u
¾
.
Now
∂(x, y)
∂(u, v) = det
Ã
−1
3
v
(1+u)2
1
3+3u
1
3
v
(1+u)2
1
3
u
1+u
!
= −1
9
v
(1 + u)2 .

24.1.
DIFFERENT COORDINATES
437
Also, u ∈[3, 4] while v ∈[1, 8] . Therefore,
Z
R
dV =
Z 4
3
Z 8
1
¯¯¯¯¯−1
9
v
(1 + u)2
¯¯¯¯¯ dv du =
Z 4
3
Z 8
1
1
9
v
(1 + u)2 dv du = 7
40
2. Find the area of the bounded region, R, determined by 5x+y = 1, 5x+y = 9, y = 2x,
and y = 5x.
Answer:
Let u = y
x, v = 5x + y. Then solving these equations for x and y yields
½
x =
v
5 + u, y = u
v
5 + u
¾
.
Now
∂(x, y)
∂(u, v) = det
Ã
−
v
(5+u)2
1
5+u
5
v
(5+u)2
u
5+u
!
= −
v
(5 + u)2 .
Also, u ∈[2, 5] while v ∈[1, 9] . Therefore,
Z
R
dV =
Z 5
2
Z 9
1
¯¯¯¯¯−
v
(5 + u)2
¯¯¯¯¯ dv du =
Z 5
2
Z 9
1
v
(5 + u)2 dv du = 12
7
3. A solid, R is determined by 5x + 3y = 4, 5x + 3y = 9, y = 2x, and y = 5x and the
density is ρ = x. Find the total mass of R.
Answer:
Let u = y
x, v = 5x + 3y. Then solving these equations for x and y yields
½
x =
v
5 + 3u, y = u
v
5 + 3u
¾
.
Now
∂(x, y)
∂(u, v) = det
Ã
−3
v
(5+3u)2
1
5+3u
5
v
(5+3u)2
u
5+3u
!
= −
v
(5 + 3u)2 .
Also, u ∈[2, 5] while v ∈[4, 9] . Therefore,
Z
R
ρ dV =
Z 5
2
Z 9
4
v
5 + 3u
¯¯¯¯¯−
v
(5 + 3u)2
¯¯¯¯¯ dv du =
Z 5
2
Z 9
4
µ
v
5 + 3u
¶ Ã
v
(5 + 3u)2
!
dv du = 4123
19 360.

438
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
4. A solid, R is determined by 2x + 2y = 1, 2x + 2y = 10, y = 4x, and y = 5x and the
density is ρ = x + 1. Find the total mass of R.
Answer:
Let u = y
x, v = 2x + 2y. Then solving these equations for x and y yields
½
x = 1
2
v
1 + u, y = 1
2u
v
1 + u
¾
.
Now
∂(x, y)
∂(u, v) = det
Ã
−1
2
v
(1+u)2
1
2+2u
1
2
v
(1+u)2
1
2
u
1+u
!
= −1
4
v
(1 + u)2 .
Also, u ∈[4, 5] while v ∈[1, 10] . Therefore,
Z
R
ρ dV
=
Z 5
4
Z 10
1
(x + 1)
¯¯¯¯¯−1
4
v
(1 + u)2
¯¯¯¯¯ dv du
=
Z 5
4
Z 10
1
(x + 1)
Ã
1
4
v
(1 + u)2
!
dv du
5. A solid, R is determined by 4x + 2y = 1, 4x + 2y = 9, y = x, and y = 6x and the
density is ρ = y−1. Find the total mass of R.
Answer:
Let u = y
x, v = 4x + 2y. Then solving these equations for x and y yields
½
x = 1
2
v
2 + u, y = 1
2u
v
2 + u
¾
.
Now
∂(x, y)
∂(u, v) = det
Ã
−1
2
v
(2+u)2
1
4+2u
v
(2+u)2
1
2
u
2+u
!
= −1
4
v
(2 + u)2 .
Also, u ∈[1, 6] while v ∈[1, 9] . Therefore,
Z
R
ρ dV =
Z 6
1
Z 9
1
µ1
2u
v
2 + u
¶−1 ¯¯¯¯¯−1
4
v
(2 + u)2
¯¯¯¯¯ dv du = −4 ln 2 + 4 ln 3
6. Find the volume of the region, E, bounded by the ellipsoid, 1
4x2 + 1
9y2 + 1
49z2 = 1.
Answer:
Let u = 1
2x, v = 1
3y, w = 1
7z. Then (u, v, w) is a point in the unit ball, B. Therefore,
Z
B
∂(x, y, z)
∂(u, v, w) dV =
Z
E
dV.
But ∂(x,y,z)
∂(u,v,w) = 42 and so the answer is
(volume of B) × 42 = 4
3π42 = 56π.

24.1.
DIFFERENT COORDINATES
439
7. Here are three vectors. (4, 1, 4)T , (5, 0, 4)T , and(3, 1, 5)T . These vectors determine a
parallelepiped, R, which is occupied by a solid having density ρ = x. Find the mass
of this solid.
Answer:
Let


4
5
3
1
0
1
4
4
5




u
v
w

=


x
y
z

. Then this maps the unit cube,
Q ≡[0, 1] × [0, 1] × [0, 1]
onto R and
∂(x, y, z)
∂(u, v, w) =
¯¯¯¯¯¯
det


4
5
3
1
0
1
4
4
5


¯¯¯¯¯¯
= |−9| = 9
so the mass is
Z
R
x dV =
Z
Q
(4u + 5v + 3w) (9) dV
=
Z 1
0
Z 1
0
Z 1
0
(4u + 5v + 3w) (9) du dv dw = 54
8. Here are three vectors. (3, 2, 6)T , (4, 1, 6)T , and (2, 2, 7)T . These vectors determine a
parallelepiped, R, which is occupied by a solid having density ρ = y. Find the mass of
this solid.
Answer:
Let


3
4
2
2
1
2
6
6
7




u
v
w

=


x
y
z

. Then this maps the unit cube,
Q ≡[0, 1] × [0, 1] × [0, 1]
onto R and
∂(x, y, z)
∂(u, v, w) =
¯¯¯¯¯¯
det


3
4
2
2
1
2
6
6
7


¯¯¯¯¯¯
= |−11| = 11
and so the mass is
Z
R
x dV =
Z
Q
(2u + v + 2w) (11) dV
=
Z 1
0
Z 1
0
Z 1
0
(2u + v + 2w) (11) du dv dw = 55
2 .

440
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
9. Here are three vectors. (2, 2, 4)T , (3, 1, 4)T , and (1, 2, 5)T . These vectors determine
a parallelepiped, R, which is occupied by a solid having density ρ = y + x. Find the
mass of this solid.
Answer:
Let


2
3
1
2
1
2
4
4
5




u
v
w

=


x
y
z

. Then this maps the unit cube,
Q ≡[0, 1] × [0, 1] × [0, 1]
onto R and
∂(x, y, z)
∂(u, v, w) =
¯¯¯¯¯¯
det


2
3
1
2
1
2
4
4
5


¯¯¯¯¯¯
= |−8| = 8
and so the density is 4u + 4v + 3w
Z
R
x dV =
Z
Q
(4u + 4v + 3w) (8) dV
=
Z 1
0
Z 1
0
Z 1
0
(4u + 4v + 3w) (8) du dv dw = 44.
10. Let D =
©
(x, y) : x2 + y2 ≤25
ª
. Find
R
D e36x2+36y2 dx dy.
Answer:
This is easy in polar coordinates. x = r cos θ, y = r sin θ. Thus ∂(x,y)
∂(r,θ) = r and in terms
of these new coordinates, the disk, D, is the rectangle,
R = {(r, θ) ∈[0, 5] × [0, 2π]} .
Therefore,
Z
D
e36x2+36y2 dV =
Z
R
e36r2r dV =
Z 5
0
Z 2π
0
e36r2r dθ dr = 1
36π
¡
e900 −1
¢
.
Note you wouldn’t get very far without changing the variables in this.
11. Let D =
©
(x, y) : x2 + y2 ≤9
ª
. Find
R
D cos
¡
36x2 + 36y2¢
dx dy.
Answer:
This is easy in polar coordinates. x = r cos θ, y = r sin θ. Thus ∂(x,y)
∂(r,θ) = r and in terms
of these new coordinates, the disk, D, is the rectangle,
R = {(r, θ) ∈[0, 3] × [0, 2π]} .
Therefore,
Z
D
cos
¡
36x2 + 36y2¢
dV =
Z
R
cos
¡
36r2¢
r dV =

24.1.
DIFFERENT COORDINATES
441
Z 3
0
Z 2π
0
cos
¡
36r2¢
r dθ dr = 1
36 (sin 324) π.
12. The ice cream in a sugar cone is described in spherical coordinates by ρ ∈[0, 8] , φ ∈
£
0, 1
4π
¤
, θ ∈[0, 2π] . If the units are in centimeters, ﬁnd the total volume in cubic
centimeters of this ice cream.
Answer:
Remember that in spherical coordinates, the volume element is ρ2 sin φ dV and so the
total volume of this is
R 8
0
R 1
4 π
0
R 2π
0
ρ2 sin φ dθ dφ dρ = −512
3
√
2π + 1024
3 π.
13. Find the volume between z = 5 −x2 −y2 and z =
p
(x2 + y2).
Answer:
Use cylindrical coordinates. In terms of these coordinates the shape is
h −r2 ≥z ≥r, r ∈
·
0, 1
2
√
21 −1
2
¸
, θ ∈[0, 2π] .
Also, ∂(x,y,z)
∂(r,θ,z) = r. Therefore, the volume is
Z 2π
0
Z
1
2
√
21−1
2
0
Z 5−r2
0
r dz dr dθ = 39
4 π + 1
4π
√
21
14. A ball of radius 12 is placed in a drill press and a hole of radius 4 is drilled out with
the center of the hole a diameter of the ball. What is the volume of the material which
remains?
Answer:
You know the formula for the volume of a sphere and so if you ﬁnd out how much stuﬀ
is taken away, then it will be easy to ﬁnd what is left. To ﬁnd the volume of what is
removed, it is easiest to use cylindrical coordinates. This volume is
Z 4
0
Z 2π
0
Z √
(144−r2)
−√
(144−r2)
r dz dθ dr = −4096
3
√
2π + 2304π.
Therefore, the volume of what remains is 4
3π (12)3 minus the above. Thus the volume
of what remains is
4096
3
√
2π.
15. A ball of radius 11 has density equal to
p
x2 + y2 + z2 in rectangular coordinates.
The top of this ball is sliced oﬀby a plane of the form z = 1. What is the mass of
what remains?
Answer:
Z 2π
0
Z arcsin( 2
11
√
30)
0
Z sec φ
0
ρ3 sin φ dρ dφ dθ +
Z 2π
0
Z π
arcsin( 2
11
√
30)
Z 11
0
ρ3 sin φ dρ dφ dθ
= 24 623
3
π

442
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
16. Find
R
S
y
x dV where S is described in polar coordinates as 1 ≤r ≤2 and 0 ≤θ ≤π/4.
Answer:
Use x = r cos θ and y = r sin θ. Then the integral in polar coordinates is
Z π/4
0
Z 2
1
(r tan θ) dr dθ = 3
4 ln 2.
17. Find
R
S
³¡ y
x
¢2 + 1
´
dV where S is given in polar coordinates as 1 ≤r ≤2 and
0 ≤θ ≤1
4π.
Answer:
Use x = r cos θ and y = r sin θ. Then the integral in polar coordinates is
Z
1
4 π
0
Z 2
1
¡
1 + tan2 θ
¢
r dr dθ.
18. Use polar coordinates to evaluate the following integral. Here S is given in terms of
the polar coordinates.
R
S sin
¡
4x2 + 4y2¢
dV where r ≤2 and 0 ≤θ ≤1
6π.
Answer:
Z
1
6 π
0
Z 2
0
sin
¡
4r2¢
r dr dθ = −1
48π cos 16 + 1
48π
19. Find
R
S e2x2+2y2 dV where S is given in terms of the polar coordinates, r ≤2 and
0 ≤θ ≤1
3π.
Answer:
The integral is
Z
1
3 π
0
Z 2
0
re2r2 dr dθ = 1
12π
¡
e8 −1
¢
.
20. Compute the volume of a sphere of radius R using cylindrical coordinates.
Answer:
Using cylindrical coordinates, the integral is
R 2π
0
R R
0
R √
R2−r2
−
√
R2−r2 r dz dr dθ = 4
3πR3.
24.2
The Moment Of Inertia ∗
In order to appreciate the importance of this concept, it is necessary to discuss its physical
signiﬁcance.
24.2.1
The Spinning Top∗
To begin with consider a spinning top as illustrated in the following picture.

24.2.
THE MOMENT OF INERTIA ∗
443
¡
¡
¡
¡
¡
¡
¡
x
y
z
R ¡
¡
¡
¡
¡
¡
 Ωa
¡
¡
¡
¡
¡

@
@
@
@
R
u
y
θ
α
For the purpose of this discussion, consider the top as a large number of point masses,
mi, located at the positions, ri (t) for i = 1, 2, · · ·, N and these masses are symmetrically
arranged relative to the axis of the top. As the top spins, the axis of symmetry is observed
to move around the z axis. This is called precession and you will see it occur whenever you
spin a top. What is the speed of this precession? In other words, what is θ′? The following
discussion follows one given in Sears and Zemansky [24].
Imagine a coordinate system which is ﬁxed relative to the moving top. Thus in this
coordinate system the points of the top are ﬁxed. Let the standard unit vectors of the
coordinate system moving with the top be denoted by i (t) , j (t) , k (t).
From Theorem
16.4.2 on Page 300, there exists an angular velocity vector Ω(t) such that if u (t) is the
position vector of a point ﬁxed in the top, (u (t) = u1i (t) + u2j (t) + u3k (t)),
u′ (t) = Ω(t) × u (t) .
The vector Ωa shown in the picture is the vector for which
r′
i (t) ≡Ωa × ri (t)
is the velocity of the ith point mass due to rotation about the axis of the top.
Thus
Ω(t) = Ωa (t) + Ωp (t) and it is assumed Ωp (t) is very small relative to Ωa. In other words,
it is assumed the axis of the top moves very slowly relative to the speed of the points in the
top which are spinning very fast around the axis of the top. The angular momentum, L is
deﬁned by
L ≡
N
X
i=1
ri × mivi
(24.6)
where vi equals the velocity of the ith point mass. Thus vi = Ω(t) × ri and from the above

444
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
assumption, vi may be taken equal to Ωa × ri. Therefore, L is essentially given by
L
≡
N
X
i=1
miri × (Ωa × ri)
=
N
X
i=1
mi
³
|ri|2 Ωa −(ri · Ωa) ri
´
.
By symmetry of the top, this last expression equals a multiple of Ωa. Thus L is parallel to
Ωa. Also,
L · Ωa
=
N
X
i=1
miΩa · ri × (Ωa × ri)
=
N
X
i=1
mi (Ωa × ri) · (Ωa × ri)
=
N
X
i=1
mi |Ωa × ri|2 =
N
X
i=1
mi |Ωa|2 |ri|2 sin2 (βi)
where βi denotes the angle between the position vector of the ith point mass and the axis
of the top. Since this expression is positive, this also shows L has the same direction as Ωa.
Let ω ≡|Ωa| . Then the above expression is of the form
L · Ωa = Iω2,
where
I ≡
N
X
i=1
mi |ri|2 sin2 (βi) .
Thus, to get I you take the mass of the ith point mass, multiply it by the square of its
distance to the axis of the top and add all these up. This is deﬁned as the moment of inertia
of the top about the axis of the top. Letting u denote a unit vector in the direction of the
axis of the top, this implies
L = Iωu.
(24.7)
Note the simple description of the angular momentum in terms of the moment of inertia.
Referring to the above picture, deﬁne the vector, y to be the projection of the vector, u on
the xy plane. Thus
y = u −(u · k) k
and
(u · i) = (y · i) = sin α cos θ.
(24.8)
Now also from 24.6,
dL
dt
=
N
X
i=1
mi
=0
z }| {
r′
i × vi + ri × miv′
i
=
N
X
i=1
ri × miv′
i = −
N
X
i=1
ri × migk

24.2.
THE MOMENT OF INERTIA ∗
445
where g is the acceleration of gravity. From 24.7, 24.8, and the above,
dL
dt · i
=
Iω
µdu
dt · i
¶
= Iω
µdy
dt · i
¶
=
(−Iω sin α sin θ) θ′ = −
N
X
i=1
ri × migk · i
=
−
N
X
i=1
migri · k × i = −
N
X
i=1
migri · j.
(24.9)
To simplify this further, recall the following deﬁnition of the center of mass.
Deﬁnition 24.2.1 Deﬁne the total mass, M by
M =
N
X
i=1
mi
and the center of mass, r0 by
r0 ≡
PN
i=1 rimi
M
.
(24.10)
In terms of the center of mass, the last expression equals
−Mgr0 · j
=
−Mg (r0 −(r0 · k) k + (r0 · k) k) · j
=
−Mg (r0 −(r0 · k) k) · j
=
−Mg |r0 −(r0 · k) k| cos θ
=
−Mg |r0| sin α cos
³π
2 −θ
´
.
Note that by symmetry, r0 (t) is on the axis of the top, is in the same direction as L, u, and
Ωa, and also |r0| is independent of t. Therefore, from the second line of 24.9,
(−Iω sin α sin θ) θ′ = −Mg |r0| sin α sin θ.
which shows
θ′ = Mg |r0|
Iω
.
(24.11)
From 24.11, the angular velocity of precession does not depend on α in the picture. It
also is slower when ω is large and I is large.
The above discussion is a considerable simpliﬁcation of the problem of a spinning top
obtained from an assumption that Ωa is approximately equal to Ω. It also leaves out all
considerations of friction and the observation that the axis of symmetry wobbles. This is
wobbling is called nutation. The full mathematical treatment of this problem involves the
Euler angles and some fairly complicated diﬀerential equations obtained using techniques
discussed in advanced physics classes. Lagrange studied these types of problems back in the
1700’s.

446
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
24.2.2
Kinetic Energy∗
The next problem is that of understanding the total kinetic energy of a collection of moving
point masses. Consider a possibly large number of point masses, mi located at the positions
ri for i = 1, 2, · · ·, N. Thus the velocity of the ith point mass is r′
i = vi. The kinetic energy
of the mass mi is deﬁned by
1
2mi |r′
i|2 .
(This is a very good time to review the presentation on kinetic energy given on Page 277.)
The total kinetic energy of the collection of masses is then
E =
N
X
i=1
1
2mi |r′
i|2 .
(24.12)
As these masses move about, so does the center of mass, r0. Thus r0 is a function of t
just as the other ri. From 24.12 the total kinetic energy is
E
=
N
X
i=1
1
2mi |r′
i −r′
0 + r′
0|2
=
N
X
i=1
1
2mi
h
|r′
i −r′
0|2 + |r′
0|2 + 2 (r′
i −r′
0 · r′
0)
i
.
(24.13)
Now
N
X
i=1
mi (r′
i −r′
0 · r′
0)
=
Ã N
X
i=1
mi (ri −r0)
!′
· r′
0
=
0
because from 24.10
N
X
i=1
mi (ri −r0)
=
N
X
i=1
miri −
N
X
i=1
mir0
=
N
X
i=1
miri −
N
X
i=1
mi
ÃPN
i=1 rimi
PN
i=1 mi
!
= 0.
Let M ≡PN
i=1 mi be the total mass. Then 24.13 reduces to
E
=
N
X
i=1
1
2mi
h
|r′
i −r′
0|2 + |r′
0|2i
=
1
2M |r′
0|2 +
N
X
i=1
1
2mi |r′
i −r′
0|2 .
(24.14)
The ﬁrst term is just the kinetic energy of a point mass equal to the sum of all the masses
involved, located at the center of mass of the system of masses while the second term
represents kinetic energy which comes from the relative velocities of the masses taken with
respect to the center of mass. It is this term which is considered more carefully in the case
where the system of masses maintain distance between each other.
To illustrate the contrast between the case where the masses maintain a constant distance
and one in which they don’t, take a hard boiled egg and spin it and then take a raw egg

24.3. FINDING THE MOMENT OF INERTIA AND CENTER OF MASS 13 NOV.447
and give it a spin. You will certainly feel a big diﬀerence in the way the two eggs respond.
Incidentally, this is a good way to tell whether the egg has been hard boiled or is raw and
can be used to prevent messiness which could occur if you think it is hard boiled and it
really isn’t.
Now let e1 (t) , e2 (t) , and e3 (t) be an orthonormal set of vectors which is ﬁxed in the
body undergoing rigid body motion. This means that ri (t) −r0 (t) has components which
are constant in t with respect to the vectors, ei (t) . By Theorem 16.4.2 on Page 300 there
exists a vector, Ω(t) which does not depend on i such that
r′
i (t) −r′
0 (t) = Ω(t) × (ri (t) −r0 (t)) .
Now using this in 24.14,
E
=
1
2M |r′
0|2 +
N
X
i=1
1
2mi |Ω(t) × (ri (t) −r0 (t))|2
=
1
2M |r′
0|2 + 1
2
Ã N
X
i=1
mi |ri (t) −r0 (t)|2 sin2 θi
!
|Ω(t)|2
=
1
2M |r′
0|2 + 1
2
Ã N
X
i=1
mi |ri (0) −r0 (0)|2 sin2 θi
!
|Ω(t)|2
where θi is the angle between Ω(t) and the vector, ri (t)−r0 (t) . Therefore, |ri (t) −r0 (t)| sin θi
is the distance between the point mass, mi located at ri and a line through the center of
mass, r0 with direction, Ωas indicated in the following picture.
©©©©©©©©©©
*
¡
¡
¡
¡
¡

rmi
Ω(t)
ri(t) −r0(t)
θi
Thus the expression, PN
i=1 mi |ri (0) −r0 (0)|2 sin2 θi plays the role of a mass in the
deﬁnition of kinetic energy except instead of the speed, substitute the angular speed, |Ω(t)| .
It is this expression which is called the moment of inertia about the line whose direction is
Ω(t) .
In both of these examples, the center of mass and the moment of inertia occurred in a
natural way.
24.3
Finding The Moment Of Inertia And Center Of
Mass 13 Nov.
The methods used to evaluate multiple integrals make possible the determination of centers
of mass and moments of inertia. In the case of a solid material rather than ﬁnitely many
point masses, you replace the sums with integrals. The sums are essentially approximations
of the integrals which result. This leads to the following deﬁnition.
Deﬁnition 24.3.1 Let a solid occupy a region R such that its density is δ (x) for
x a point in R and let L be a line. For x ∈R, let l (x) be the distance from the point, x to
the line L. The moment of inertia of the solid is deﬁned as
Z
R
l (x)2 δ (x) dV.

448
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Letting (xc, yc, zc) denote the Cartesian coordinates of the center of mass,
xc
=
R
R xδ (x) dV
R
R δ (x) dV , yc =
R
R yδ (x) dV
R
R δ (x) dV ,
zc
=
R
R zδ (x) dV
R
R δ (x) dV
where x, y, z are the Cartesian coordinates of the point at x.
Example 24.3.2 Let a solid occupy the three dimensional region R and suppose the density
is ρ. What is the moment of inertia of this solid about the z axis? What is the center of
mass?
Here the little masses would be of the form ρ (x) dV where x is a point of R. Therefore,
the contribution of this mass to the moment of inertia would be
¡
x2 + y2¢
ρ (x) dV
where the Cartesian coordinates of the point x are (x, y, z) . Then summing these up as an
integral, yields the following for the moment of inertia.
Z
R
¡
x2 + y2¢
ρ (x) dV.
(24.15)
To ﬁnd the center of mass, sum up rρ dV for the points in R and divide by the total
mass. In Cartesian coordinates, where r = (x, y, z) , this means to sum up vectors of the
form (xρ dV, yρ dV, zρ dV ) and divide by the total mass. Thus the Cartesian coordinates of
the center of mass are
µR
R xρ dV
R
R ρ dV ,
R
R yρ dV
R
R ρ dV ,
R
R zρ dV
R
R ρ dV
¶
≡
R
R rρ dV
R
R ρ dV .
Here is a speciﬁc example.
Example 24.3.3 Find the moment of inertia about the z axis and center of mass of the solid
which occupies the region, R deﬁned by 9 −
¡
x2 + y2¢
≥z ≥0 if the density is ρ (x, y, z) =
p
x2 + y2.
This moment of inertia is
R
R
¡
x2 + y2¢ p
x2 + y2 dV and the easiest way to ﬁnd this
integral is to use cylindrical coordinates. Thus the answer is
Z 2π
0
Z 3
0
Z 9−r2
0
r3r dz dr dθ = 8748
35 π.
To ﬁnd the center of mass, note the x and y coordinates of the center of mass,
R
R xρ dV
R
R ρ dV ,
R
R yρ dV
R
R ρ dV
both equal zero because the above shape is symmetric about the z axis and ρ is also sym-
metric in its values. Thus xρ dV will cancel with −xρ dV and a similar conclusion will hold
for the y coordinate. It only remains to ﬁnd the z coordinate of the center of mass, zc. In
polar coordinates, ρ = r and so,
zc =
R
R zρ dV
R
R ρ dV
=
R 2π
0
R 3
0
R 9−r2
0
zr2 dz dr dθ
R 2π
0
R 3
0
R 9−r2
0
r2 dz dr dθ
= 18
7 .

24.4.
EXERCISES WITH ANSWERS
449
Thus the center of mass will be
¡
0, 0, 18
7
¢
.
A short comment about terminology is in order. When the density is constant, the center
of mass is called the centroid. Thus the centroid is a purely geometrical concept because
the densities will cancel from the integrals.
24.4
Exercises With Answers
1. Let R denote the ﬁnite region bounded by z = 4 −x2 −y2 and the xy plane. Find zc,
the z coordinate of the center of mass if the density, σ is a constant.
The region, R is a dome shaped region above the circle centered at the origin having
radius 2. Therefore, using polar or cylindrical coordinates
zc =
R
R zσdV
R
R σdV
=
R 2π
0
R 2
0
R 4−r2
0
zrdzdrdθ
R 2π
0
R 2
0
R 4−r2
0
rdzdrdθ
= 4
3
2. Let R denote the ﬁnite region bounded by z = 4 −x2 −y2 and the xy plane. Find zc,
the z coordinate of the center of mass if the density, σ is equals σ (x, y, z) = z.
This problem is just like the one above except here the density is not constant. Thus
zc =
R
R z2dV
R
R zdV
=
R 2π
0
R 2
0
R 4−r2
0
z2rdzdrdθ
R 2π
0
R 2
0
R 4−r2
0
zrdzdrdθ
= 2
3. Find the mass and center of mass of the region between the surfaces z = −y2 + 8 and
z = 2x2 + y2 if the density equals σ = 1.
To ﬁnd where (x, y) is you let −y2 + 8 = 2x2 + y2 and this shows the two surfaces
intersect in the circle x2 + y2 = 4. Using cylindrical coordinates,
zc
=
R
R zdV
R
R 1dV =
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) zrdzdrdθ
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) rdzdrdθ
=
¡ 224
3 π
¢
16π
= 14
3
You can ﬁnd the the others the same way.
xc =
R
R xdV
R
R 1dV =
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) (r cos (θ)) rdzdrdθ
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) rdzdrdθ
= 0
yc =
R
R ydV
R
R 1dV =
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) (r sin (θ)) rdzdrdθ
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) rdzdrdθ
= 0
Thus the center of mass is
¡
0, 0, 14
3
¢
. The mass is
Z 2π
0
Z 2
0
Z 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ)
rdzdrdθ = 16π

450
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
4. Find the mass and center of mass of the region between the surfaces z = −y2 + 8 and
z = 2x2 + y2 if the density equals σ (x, y, z) = x2.
This is just like the problem above only now the density is not constant.
zc =
R
R zx2dV
R
R x2dV
=
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) z
¡
r2 cos2 (θ)
¢
rdzdrdθ
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) r (r2 cos2 (θ)) dzdrdθ
= 11
2
yc =
R
R yx2dV
R
R x2dV
=
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) (r sin (θ))
¡
r2 cos2 (θ)
¢
rdzdrdθ
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) r (r2 cos2 (θ)) dzdrdθ
= 0
xc =
R
R xx2dV
R
R x2dV
=
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) (r cos (θ))
¡
r2 cos2 (θ)
¢
rdzdrdθ
R 2π
0
R 2
0
R 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ) r (r2 cos2 (θ)) dzdrdθ
= 0
So in this case the center of mass is
¡
0, 0, 11
2
¢
. The mass is
Z 2π
0
Z 2
0
Z 8−r2 sin2(θ)
2r2 cos2(θ)+r2 sin2(θ)
r
¡
r2 cos2 (θ)
¢
dzdrdθ = 32
3 π
5. The two cylinders, x2 +y2 = 4 and y2 +z2 = 4 intersect in a region, R. Find the mass
and center of mass if the density, σ, is given by σ (x, y, z) = z2.
The ﬁrst cylinder is parallel to the z axis. Let D denote the circle of radius 2 in the xy
plane. Then the region just described has (x, y) in the circle of radius 2 and z between
−
p
4 −y2 and
p
4 −y2. It follows the total mass is
Z 2
−2
Z √
4−y2
−√
4−y2
Z √
4−y2
−√
4−y2 z2dzdxdy = 2048
45 .
By symmetry, the center of mass will be (0, 0, 0) .
6. The two cylinders, x2 +y2 = 4 and y2 +z2 = 4 intersect in a region, R. Find the mass
and center of mass if the density, σ, is given by σ (x, y, z) = 4 + z.
The total mass is
Z 2
−2
Z √
4−y2
−√
4−y2
Z √
4−y2
−√
4−y2 (4 + z) dzdxdy = 512
3
zc =
R 2
−2
R √
4−y2
−√
4−y2
R √
4−y2
−√
4−y2 z (4 + z) dzdxdy
¡ 512
3
¢
= 4
15
xc =
R 2
−2
R √
4−y2
−√
4−y2
R √
4−y2
−√
4−y2 x (4 + z) dzdxdy
¡ 512
3
¢
= 0
yc =
R 2
−2
R √
4−y2
−√
4−y2
R √
4−y2
−√
4−y2 y (4 + z) dzdxdy
¡ 512
3
¢
= 0
and so the center of mass is
¡
0, 0, 4
15
¢
.

24.4.
EXERCISES WITH ANSWERS
451
7. Find the mass and center of mass of the set, (x, y, z) such that x2
4 + y2
9 + z2 ≤1 if the
density is σ (x, y, z) = 4 + y + z.
This is the inside of an ellipsoid. Denote this by R. Then the total mass is
Z
R
σdV =
Z
R
(4 + y + z) dV
Lets change the variables. Let x = 2u, y = 3v, z = w. When this is done, (u, v, w)
will be in the unit ball. The Jacobian of this transformation is 6. Now changing the
variables the above integral equals
Z
B
(4 + 3v + w) 6dV
where here B is the unit ball. When integrating over a ball, you ought to suspect that
spherical coordinates would be a good idea. Change the variables again in the above
integral to spherical coordinates.
w = ρ cos φ, v = ρ sin φ sin θ, u = ρ sin φ cos θ.
Then the above integral in spherical coordinates is
Z π
0
Z 2π
0
Z 1
0
(4 + 3ρ sin (φ) sin (θ) + ρ cos φ) 6ρ2 sin φdρdθdφ = 32π.
To ﬁnd the center of mass, it would be
zc =
R
R z (4 + y + z) dV
32π
=
R
R z2dV
32π
.
Now to get this, I have used symmetry of the region. This equals
R
R w2dV
32π
=
R π
0
R 2π
0
R 1
0 (ρ cos (φ))2 6ρ2 sin φdρdθdφ
32π
= 1
20
yc
=
R
R y (4 + y + z) dV
32π
=
R
R y2dV
32π
=
R π
0
R 2π
0
R 1
0 (3 (ρ sin (φ) sin (θ)))2 6ρ2 sin φdρdθdφ
32π
= 9
20
I think you get the idea. You can now ﬁnd xc in the same way.
8. Let R denote the ﬁnite region bounded by z = 9 −x2 −y2 and the xy plane. Find the
moment of inertia of this shape about the z axis given the density equals 1.
Using cylindrical coordinates, this is
Z
R
r2dV =
Z 2π
0
Z 3
0
Z 9−r2
0
r2rdzdrdθ = 243
2 π
9. Let R denote the ﬁnite region bounded by z = 9 −x2 −y2 and the xy plane. Find the
moment of inertia of this shape about the x axis given the density equals 1.
It is like the above except diﬀerent.
Z 2π
0
Z 3
0
Z 9−r2
0
¡
r2 sin2 (θ) + z2¢
rdzdrdθ = 1215
2
π

452
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
10. Let B be a solid ball of constant density and radius R. Find the moment of inertia
about a line through a diameter of the ball. You should get 2
5R2M where M is the
mass.
The constant density of the ball is 3
4
M
πR3 . For simplicity let the line be the z axis. I
will also use spherical coordinates since this is a ball. Then the moment of inertia is
Z π
0
Z 2π
0
Z R
0
3
4
M
πR3 (ρ sin (φ))2 ρ2 sin (φ) dρdθdφ = 2
5R2M.
11. Let B be a solid ball of density, σ = ρ where ρ is the distance to the center of the ball
which has radius R. Find the moment of inertia about a line through a diameter of
the ball. Write your answer in terms of the total mass and the radius as was done in
the constant density case.
Z π
0
Z 2π
0
Z R
0
ρ (ρ sin (φ))2 ρ2 sin (φ) dρdθdφ = 4
9πR6
Also the total mass is
M =
Z π
0
Z 2π
0
Z R
0
ρρ2 sin (φ) dρdθdφ = πR4
Therefore, the moment of inertia is
4
9MR2.
12. Let C be a solid cylinder of constant density and radius R. Find the moment of inertia
about the axis of the cylinder
You should get 1
2R2M where M is the mass.
The density is
M
πR2h where h is the height of the cylinder. Using cylindrical coordinates,
the moment of inertia is
Z 2π
0
Z R
0
Z h
0
µ M
πR2h
¶
r2rdzdrdθ = 1
2R2M
13. Let C be a solid cylinder of constant density and radius R and mass M and let B be
a solid ball of radius R and mass M. The cylinder and the sphere are placed on the
top of an inclined plane and allowed to roll to the bottom. Which one will arrive ﬁrst
and why?
The sphere will win. This is because it takes less torque to produce a given angular
acceleration in the sphere than in the cylinder because the moment of inertia for the
sphere is less than the moment of inertia of the cylinder. Thus a given torque about
the axis of roation, which will be identical in both will produce faster rotation in the
sphere than in the cylinder. Another way to look at it is that they both have the
same total energy when they get to the bottom. This energy comes from two parts,
one involving rotation and the other translation of the center of mass. If the center of
mass of both were moving at the same speed, this would be a contradiction because
the diﬀerent moments of inertia would then require the kinetic energy of one to be
greater than that of the other.

24.4.
EXERCISES WITH ANSWERS
453
14. Suppose a solid of mass M occupying the region, B has moment of inertia, Il about a
line, l which passes through the center of mass of M and let l1 be another line parallel
to l and at a distance of a from l. Then the parallel axis theorem states Il1 = Il +a2M.
Prove the parallel axis theorem. Hint: Choose axes such that the z axis is l and l1
passes through the point (a, 0) in the xy plane.
Consider the following picture in which, as suggested, the line, l is the z axis and l1
goes through (a, 0) in the xy plane and is parallel to l.
l1
l
B
For x a point in B, let the coordinates of this point be (x, y, z) . Then the displacement
vector from a point, (a, 0, z) on l1 to the point, (x, y, z) is (x −a, −y, 0) and so the
square of the distance is x2 −2xa + a2 + y2. Therefore, from the deﬁnition of moment
of inertia, the moment of inertia about l1 is
Il1 ≡
Z
B
δ (x, y, z)
¡
x2 −2xa + a2 + y2¢
dV
Since the line goes through the center of mass, this reduces to
Z
B
δ (x, y, z)
¡
x2 + y2¢
dV +
Z
B
δ (x, y, z) a2dV ≡Il + a2M
15. Using the parallel axis theorem ﬁnd the moment of inertia of a solid ball of radius R
and mass M about an axis located at a distance of a from the center of the ball. Your
answer should be Ma2 + 2
5MR2.
16. Consider all axes in computing the moment of inertia of a solid. Will the smallest
possible moment of inertia always result from using an axis which goes through the
center of mass?
The answer is yes. To see this, consider the parallel axis theorem above.

454
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.

Part X
Line Integrals
455


457
Outcomes
Line Integrals
A. Evaluate the work done by a varying force over a curved path.
B. Evaluate line integrals in general including line integrals with respect to arc length.
C. Evaluate the physical characteristics of a wire such as centroid, mass, and center of
mass using line integrals.
Reading: Multivariable Calculus 4.2
Outcome Mapping:
A. 1,9
B. 2,3,4
C. 5,6
Path Independent Line Integrals
A. Recall and apply the Fundamental Theorem for Line Integrals.
B. Determine whether or not a force ﬁeld is conservative, and if so, ﬁnd its potential.
C. Evaluate the circulation of a force ﬁeld or the work done by a force ﬁeld on a object
moving along a given path.
Reading: Multivariable Calculus 4.3
Outcome Mapping:
A. K1,1
B. 3,6
C. 2,4
Recovering a Function from its Gradient
A. Analyze the characteristics of a vector ﬁeld. Sketch a vector ﬁeld.
B. Determine whether a vector ﬁeld is a gradient.
C. Determine whether a diﬀerential form is exact.
D. Recover a function from its gradient or diﬀerential form, if possible.
Reading: Multivariable Calculus 4.1
Outcome Mapping:
A. 1,7
B. 5
C. 4,6
D. 4,5,7

458

Line Integrals 14 Nov.
The concept of the integral can be extended to functions which are not deﬁned on an interval
of the real line but on some curve in Rn. This is done by deﬁning things in such a way that
the more general concept reduces to the earlier notion. First it is necessary to consider what
is meant by arc length.
25.0.1
Orientations And Smooth Curves
Recall the notion of a smooth curve.
C is a smooth curve in Rn if there exists an interval, [a, b] ⊆R and functions xi :
[a, b] →R such that the following conditions hold
1. xi is continuous on [a, b] .
2. x′
i exists and is continuous and bounded on [a, b] , with x′
i (a) deﬁned as the derivative
from the right,
lim
h→0+
xi (a + h) −xi (a)
h
,
and x′
i (b) deﬁned similarly as the derivative from the left.
3. For p (t) ≡(x1 (t) , · · ·, xn (t)) , t →p (t) is one to one on (a, b) .
4. |p′ (t)| ≡
³Pn
i=1 |x′
i (t)|2´1/2
̸= 0 for all t ∈[a, b] .
5. C = ∪{(x1 (t) , · · ·, xn (t)) : t ∈[a, b]} .
The functions, xi (t) , deﬁned above are giving the coordinates of a point in Rn and the
list of these functions is called a parameterization for the smooth curve. Note the natural
direction of the interval also gives a direction for moving along the curve. Such a direction
is called an orientation.
The proof that curve length is well deﬁned for a smooth curve contains a result which
deserves to be stated as a corollary. It is proves in the Section which starts on Page 295.
This is one of those sections you should read only if you are interested.
Corollary 25.0.1 Let C be a smooth curve and let f : [a, b] →C and g : [c, d] →C be
two parameterizations satisfying 1 - 5. Then g−1 ◦f is either strictly increasing or strictly
decreasing.
Deﬁnition 25.0.2 If g−1 ◦f is increasing, then f and g are said to be equivalent
parameterizations and this is written as f ∼g. It is also said that the two parameterizations
give the same orientation for the curve when f ∼g.
459

460
LINE INTEGRALS 14 NOV.
When the parameterizations are equivalent, they preserve the direction, of motion along
the curve and this also shows there are exactly two orientations of the curve since either
g−1 ◦f is increasing or it is decreasing. This is not hard to believe. In simple language, the
message is that there are exactly two directions of motion along a curve. The diﬃculty is in
proving this is actually the case based only on the assumption that the parameterizations
of the curve are one to one.
Lemma 25.0.3 The following hold for ∼.
f ∼f,
(25.1)
If f ∼g then g ∼f,
(25.2)
If f ∼g and g ∼h, then f ∼h.
(25.3)
Proof: Formula 25.1 is obvious because f −1 ◦f (t) = t so it is clearly an increasing
function. If f ∼g then f −1 ◦g is increasing. Now g−1 ◦f must also be increasing because
it is the inverse of f −1 ◦g. This veriﬁes 25.2. To see 25.3, f −1 ◦h =
¡
f −1 ◦g
¢
◦
¡
g−1 ◦h
¢
and so since both of these functions are increasing, it follows f −1 ◦h is also increasing. This
proves the lemma.
The symbol, ∼is called an equivalence relation.
If C is such a smooth curve just
described, and if f : [a, b] →C is a parameterization of C, consider g (t) ≡f ((a + b) −t) ,
also a parameterization of C. Now by Corollary 25.0.1, if h is a parameterization, then if
f −1 ◦h is not increasing, it must be the case that g−1 ◦h is increasing. Consequently, either
h ∼g or h ∼f. These parameterizations, h, which satisfy h ∼f are called the equivalence
class determined by f and those h ∼g are called the equivalence class determined by g.
These two classes are called orientations of C. They give the direction of motion on C.
You see that going from f to g corresponds to tracing out the curve in the opposite direction.
Sometimes people wonder why it is required, in the deﬁnition of a smooth curve that
p′ (t) ̸= 0. Imagine t is time and p (t) gives the location of a point in space. If p′ (t) is allowed
to equal zero, the point can stop and change directions abruptly, producing a pointy place
in C. Here is an example.
Example 25.0.4 Graph the curve
¡
t3, t2¢
for t ∈[−1, 1] .
In this case, t = x1/3 and so y = x2/3. Thus the graph of this curve looks like the picture
below. Note the pointy place. Such a curve should not be considered smooth! If it were a
banister and you were sliding down it, it would be clear at a certain point that the curve is
not smooth. I think you may even get the point of this from the picture below.
So what is the thing to remember from all this? First, there are certain conditions which
must be satisﬁed for a curve to be smooth. These are listed in 1 - 5. Next, if you have any
curve, there are two directions you can move over this curve, each called an orientation.
This is illustrated in the following picture.

461
p
q
p
q
Either you move from p to q or you move from q to p.
Deﬁnition 25.0.5 A curve C is piecewise smooth if there exist points on this curve,
p0, p1, · · ·, pn such that, denoting Cpk−1pk the part of the curve joining pk−1 and pk, it
follows Cpk−1pk is a smooth curve and ∪n
k=1Cpk−1pk = C. In other words, it is piecewise
smooth if it consists of a ﬁnite number of smooth curves linked together.
Note that Example 25.0.4 is an example of a piecewise smooth curve although it is not
smooth.
25.0.2
The Integral Of A Function Deﬁned On A Smooth Curve
Letting r (t) , t ∈[a, b] be the position vector of a smooth curve, recall that the total length
of this curve is given by
l =
Z b
a
|r′ (t)| dt.
(25.4)
Remember that if you interpret t as time, |r′ (t)| is the speed and the above integral says
that to get the total distance you simply integrate the speed. A small chunk of distance
traveled is dl = |r′ (t)| dt. This says the same thing as
dl
dt = |r′ (t)|
which was discussed earlier. Of course it follows from 25.4 and the fundamental theorem of
calculus. The distance for the parameter between a and t is
l (t) =
Z t
a
|r′ (s)| ds
and so by the fundamental theorem of calculus,
l′ (t) = dl
dt = |r′ (t)| .
For this reason, the increment of arc length is dl = |r′ (t)| dt. Think of it as giving an
inﬁnitesimal contribution to the integral. For C a smooth curve with a parameterization,
r : [a, b] →C and a function, f deﬁned on C, deﬁne the symbol,
Z
C
fdl ≡
Z b
a
f (r (t)) |r′ (t)| dt.
Example 25.0.6 Let C be a smooth curve which has parameterization given by r (t) =
(cos 2t, sin (2t) , t) for t ∈[0, 2π] . Suppose f (x, y, z) = x2 + y. Find
R
C fdl.

462
LINE INTEGRALS 14 NOV.
The increment of length is
q
4 cos2 (2t) + 4 sin2 (2t) + 1dt =
√
5dt. Now the desired in-
tegral is
Z 2π
0
¡
cos2 2t + sin (2t)
¢ √
5dt =
√
5π
One can deﬁne things like density with respect to arc length in the usual way. As just
explained, a little chunk of length is dl = |r′ (t)| dt. The density is a function δ (x, y, z) which
has the property that a little chunk of mass is given by dm = δ (r (t)) dl.
Deﬁnition 25.0.7 Let δ be the density with respect to arc length. Then the total
mass of a smooth curve, C having parameterization r : [a, b] →R3 is
Z
C
δ (x, y, z) dl =
Z b
a
δ (r (t)) |r′ (t)| dt
the center of mass can be given in a similar manner as before. Thus
xc
≡
R
C xδ (x, y, z) dl
R
C δ (x, y, z) dl , yc =
R
C yδ (x, y, z) dl
R
C δ (x, y, z) dl
zc
=
R
C zδ (x, y, z) dl
R
C δ (x, y, z) dl
and the only thing you need to do is to evaluate the integrals after changing everything to
give a one dimensional integral with respect to the parameter t.
Example 25.0.8 Let a smooth curve be given by the parameterization, r (t) = (cos t, sin t, t) :
t ∈[0, 10] . This is a helix in case you are interested.
Suppose the density is given by
δ (x, y, z) = x2. Find the total mass and the center of mass.
The increment of arc length is dl =
q
sin2 (t) + cos2 (t) + 1dt =
√
2dt. Then the total
mass is
Z 10
0
cos2 (t)
√
2dt = 1
2
√
2 cos (10) sin (10) + 5
√
2
The center of mass is given by
xc =
R 10
0
(cos (t)) cos2 (t)
√
2dt
1
2
√
2 cos (10) sin (10) + 5
√
2 =
1
3
√
2 sin 10 cos2 10 + 2
3
√
2 sin 10
1
2
√
2 cos 10 sin 10 + 5
√
2
yc =
R 10
0
(sin (t)) cos2 (t)
√
2dt
1
2
√
2 cos (10) sin (10) + 5
√
2 = −1
3
¡
cos3 10
¢ √
2 + 1
3
√
2
1
2
√
2 cos 10 sin 10 + 5
√
2
zc =
R 10
0
t cos2 (t)
√
2dt
1
2
√
2 cos (10) sin (10) + 5
√
2 = 5
√
2 cos 10 sin 10 + 99
4
√
2 + 1
4
√
2 cos2 10
1
2
√
2 cos 10 sin 10 + 5
√
2
25.0.3
Vector Fields
A vector ﬁeld is nothing but a function which has values which are vectors. For example,
consider the force acting on a unit mass by the sun. This determines a force vector which
depends on the location of the point. Thus each point in space has associated with it a
vector which is the force which the sun exerts on a particle of mass 1 which is placed at
that point.

463
Some people ﬁnd it useful to try and draw pictures to illustrate a vector valued function
or vector ﬁeld. This can be a very useful idea in the case where the function takes points in
D ⊆R2 and delivers a vector in R2.
For many points, (x, y) ∈D, you draw an arrow of the appropriate length and direction
with its tail at (x, y). The picture of all these arrows can give you an understanding of what
is happening. For example if the vector valued function gives the velocity of a ﬂuid at the
point, (x, y) , the picture of these arrows can give an idea of the motion of the ﬂuid. When
they are long the ﬂuid is moving fast, when they are short, the ﬂuid is moving slowly the
direction of these arrows is an indication of the direction of motion. The only sensible way
to produce such a picture is with a computer. Otherwise, it becomes a worthless exercise
in busy work. Furthermore, it is of limited usefulness in three dimensions because in three
dimensions such pictures are too cluttered to convey much insight.
Example 25.0.9 Draw a picture of the vector ﬁeld, (−x, y) which gives the velocity of a
ﬂuid ﬂowing in two dimensions.
–2
–1
0
1
2
y
–2
–1
1
2
x
In this example, drawn by Maple, you can see how the arrows indicate the motion of
this ﬂuid.
Example 25.0.10 Draw a picture of the vector ﬁeld (y, x) for the velocity of a ﬂuid ﬂowing
in two dimensions.
–2
–1
0
1
2
y
–2
–1
1
2
x
So much for art. Get the computer to do it and it can be useful. If you try to do it, you
will mainly waste time.

464
LINE INTEGRALS 14 NOV.
Example 25.0.11 Draw a picture of the vector ﬁeld (y cos (x) + 1, x sin (y) −1) for the
velocity of a ﬂuid ﬂowing in two dimensions.
–2
–1
0
1
2
y
–2
–1
1
2
x
25.0.4
Line Integrals And Work
The interesting concept of line integral has to do with integrals which involve vector ﬁelds,
not scalar valued functions as above. The most signiﬁcant application is to work.
First, it is necessary to give some discussion of the concept of orientation. Let C be a
smooth curve contained in Rp. A curve, C is an “oriented curve” if the only parameteriza-
tions considered are those which lie in exactly one of the two equivalence classes discussed in
Deﬁnition 25.0.2, each of which is called an “orientation”. In simple language, orientation
speciﬁes a direction over which motion along the curve is to take place. Thus, it speciﬁes
the order in which the points of C are encountered. The pair of concepts consisting of the
set of points making up the curve along with a direction of motion along the curve is called
an oriented curve.
Deﬁnition 25.0.12 Suppose F (x) ∈Rp is given for each x ∈C where C is a
smooth oriented curve and suppose x →F (x) is continuous. The mapping x →F (x) is
called a vector ﬁeld. In the case that F (x) is a force, it is called a force ﬁeld.
Next the concept of work done by a force ﬁeld, F on an object as it moves along the
curve, C, in the direction determined by the given orientation of the curve will be deﬁned.
This is new. Earlier the work done by a force which acts on an object moving in a straight
line was discussed but here the object moves over a curve. In order to deﬁne what is meant
by the work, consider the following picture.
¡
¡
¡
¡
¡

x(t)
F(x(t))
x(t + h)

465
In this picture, the work done by a force, F on an object which moves from the point
x (t) to the point x (t + h) along the straight line shown would equal F · (x (t + h) −x (t)) .
It is reasonable to assume this would be a good approximation to the work done in moving
along the curve joining x (t) and x (t + h) provided h is small enough. Also, provided h is
small,
x (t + h) −x (t) ≈x′ (t) h
where the wriggly equal sign indicates the two quantities are close.
In the notation of
Leibniz, one writes dt for h and
dW = F (x (t)) · x′ (t) dt
Thus the total work along the whole curve should be given by the integral,
Z b
a
F (x (t)) · x′ (t) dt
This motivates the following deﬁnition of work.
Deﬁnition 25.0.13 Let F (x) be given above. Then the work done by this force
ﬁeld on an object moving over the curve C in the direction determined by the speciﬁed
orientation is deﬁned as
Z
C
F · dR ≡
Z b
a
F (x (t)) · x′ (t) dt
where the function, x is one of the allowed parameterizations of C in the given orientation
of C. In other words, there is an interval, [a, b] and as t goes from a to b, x (t) moves in the
direction determined from the given orientation of the curve.
Theorem 25.0.14 The symbol,
R
C F · dR, is well deﬁned in the sense that every
parameterization in the given orientation of C gives the same value for
R
C F · dR.
Proof: Suppose g : [c, d] →C is another allowed parameterization. Thus g−1 ◦f is an
increasing function, φ. Letting s = φ (t) and changing variables, and using the fact φ is
increasing,
Z d
c
F (g (s)) · g′ (s) ds =
Z b
a
F (g (φ (t))) · g′ (φ (t)) φ′ (t) dt
=
Z b
a
F (f (t)) · d
dt
¡
g
¡
g−1 ◦f (t)
¢¢
dt =
Z b
a
F (f (t)) · f ′ (t) dt.
This proves the theorem.
Regardless the physical interpretation of F, this is called the line integral. When F is
interpreted as a force, the line integral measures the extent to which the motion over the
curve in the indicated direction is aided by the force. If the net eﬀect of the force on the
object is to impede rather than to aid the motion, this will show up as negative work.
Does the concept of work as deﬁned here coincide with the earlier concept of work when
the object moves over a straight line when acted on by a constant force?
Let p and q be two points in Rn and suppose F is a constant force acting on an object
which moves from p to q along the straight line joining these points.
Then the work
done is F · (q −p) . Is the same thing obtained from the above deﬁnition?
Let x (t) ≡
p+t (q −p) , t ∈[0, 1] be a parameterization for this oriented curve, the straight line in the

466
LINE INTEGRALS 14 NOV.
direction from p to q. Then x′ (t) = q −p and F (x (t)) = F. Therefore, the above deﬁnition
yields
Z 1
0
F · (q −p) dt = F · (q −p) .
Therefore, the new deﬁnition adds to but does not contradict the old one.
Example 25.0.15 Suppose for t ∈[0, π] the position of an object is given by r (t) = ti +
cos (2t) j + sin (2t) k. Also suppose there is a force ﬁeld deﬁned on R3, F (x, y, z) ≡2xyi +
x2j + k. Find
Z
C
F · dR
where C is the curve traced out by this object which has the orientation determined by the
direction of increasing t.
To ﬁnd this line integral use the above deﬁnition and write
Z
C
F · dR =
Z π
0
¡
2t (cos (2t)) ,t2,1
¢
·
(1, −2 sin (2t) , 2 cos (2t)) dt
In evaluating this replace the x in the formula for F with t, the y in the formula for F
with cos (2t) and the z in the formula for F with sin (2t) because these are the values of
these variables which correspond to the value of t. Taking the dot product, this equals the
following integral.
Z π
0
¡
2t cos 2t −2 (sin 2t) t2 + 2 cos 2t
¢
dt = π2
Example 25.0.16 Let C denote the oriented curve obtained by r (t) =
¡
t, sin t, t3¢
where
the orientation is determined by increasing t for t ∈[0, 2] . Also let F = (x, y, xz + z) . Find
R
C F·dR.
You use the deﬁnition.
Z
C
F · dR
=
Z 2
0
¡
t, sin (t) , (t + 1) t3¢
·
¡
1, cos (t) , 3t2¢
dt
=
Z 2
0
¡
t + sin (t) cos (t) + 3 (t + 1) t5¢
dt
=
1251
14
−1
2 cos2 (2) .
25.0.5
Another Notation For Line Integrals
Deﬁnition 25.0.17 Let F (x, y, z) = (P (x, y, z) , Q (x, y, z) , R (x, y, z)) and let C
be an oriented curve. Then another way to write
R
C F·dR is
Z
C
Pdx + Qdy + Rdz
This last is referred to as the integral of a diﬀerential form, Pdx + Qdy + Rdz. The
study of diﬀerential forms is important. Formally, dR = (dx, dy, dz) and so the integrand in
the above is formally F·dR. Other occurances of this notation are handled similarly in 2 or
higher dimensions.

467
25.0.6
Exercises With Answers
1. Suppose for t ∈[0, 2π] the position of an object is given by r (t) = 2ti + cos (t) j +
sin (t) k. Also suppose there is a force ﬁeld deﬁned on R3,
F (x, y, z) ≡2xyi +
¡
x2 + 2zy
¢
j + y2k.
Find the work,
Z
C
F · dR
where C is the curve traced out by this object which has the orientation determined
by the direction of increasing t.
You might think of dR = r′ (t) dt to help remember what to do.
Then from the
deﬁnition,
Z
C
F · dR =
Z 2π
0
¡
2 (2t) (sin t) , 4t2 + 2 sin (t) cos (t) , sin2 (t)
¢
· (2, −sin (t) , cos (t)) dt
=
Z 2π
0
¡
8t sin t −
¡
2 sin t cos t + 4t2¢
sin t + sin2 t cos t
¢
dt = 16π2 −16π
2. Here is a vector ﬁeld,
¡
y, x2 + z, 2yz
¢
and here is the parameterization of a curve, C.
R (t) = (cos 2t, 2 sin 2t, t) where t goes from 0 to π/4. Find
R
C F· dR.
dR = (−2 sin (2t) , 4 cos (2t) , 1) dt.
Then by the deﬁnition,
Z
C
F · dR =
Z π/4
0
¡
2 sin (2t) , cos2 (2t) + t, 4t sin (2t)
¢
· (−2 sin (2t) , 4 cos (2t) , 1) dt
=
Z π/4
0
¡
−4 sin2 2t + 4
¡
cos2 2t + t
¢
cos 2t + 4t sin 2t
¢
dt = 4
3
3. Suppose for t ∈[0, 1] the position of an object is given by r (t) = ti + tj + tk. Also
suppose there is a force ﬁeld deﬁned on R3,
F (x, y, z) ≡yzi + xzj + xyk.
Find
Z
C
F · dR
where C is the curve traced out by this object which has the orientation determined
by the direction of increasing t. Repeat the problem for r (t) = ti + t2j + tk.
You should get the same answer in this case. This is because the vector ﬁeld happens
to be conservative. (More on this later.)

468
LINE INTEGRALS 14 NOV.
25.1
Path Independent Line Integrals 15 Nov.
Sometimes the line integral giving the work done by a force ﬁeld depends only on the
endpoints of the curve. This is very nice when it happens because it makes the line integral
very easy to compute. It also has great physical signiﬁcance.
Deﬁnition 25.1.1 A vector ﬁeld, F deﬁned in a three dimensional region is said
to be conservative1 if for every piecewise smooth closed curve, C, it follows
R
C F· dR = 0.
Deﬁnition 25.1.2 Let (x, p1, · · ·, pn, y) be an ordered list of points in Rp. Let
p (x, p1, · · ·, pn, y)
denote the piecewise smooth curve consisting of a straight line segment from x to p1 and
then the straight line segment from p1 to p2 · ·· and ﬁnally the straight line segment from
pn to y. This is called a polygonal curve. An open set in Rp, U, is said to be a region if
it has the property that for any two points, x, y ∈U, there exists a polygonal curve joining
the two points.
Conservative vector ﬁelds are important because of the following theorem, sometimes
called the fundamental theorem for line integrals.
Theorem 25.1.3 Let U be a region in Rp and let F : U →Rp be a continuous
vector ﬁeld. Then F is conservative if and only if there exists a scalar valued function of p
variables, φ such that F = ∇φ. Furthermore, if C is an oriented curve which goes from x
to y in U, then
Z
C
F · dR = φ (y) −φ (x) .
(25.5)
Thus the line integral is path independent in this case. This function, φ is called a scalar
potential for F.
Proof: To save space and fussing over things which are unimportant, denote by p (x0, x)
a polygonal curve from x0 to x. Thus the orientation is such that it goes from x0 to x. The
curve p (x, x0) denotes the same set of points but in the opposite order. Suppose ﬁrst F is
conservative. Fix x0 ∈U and let
φ (x) ≡
Z
p(x0,x)
F· dR.
This is well deﬁned because if q (x0, x) is another polygonal curve joining x0 to x, Then the
curve obtained by following p (x0, x) from x0 to x and then from x to x0 along q (x, x0) is
a closed piecewise smooth curve and so by assumption, the line integral along this closed
curve equals 0. However, this integral is just
Z
p(x0,x)
F· dR+
Z
q(x,x0)
F· dR =
Z
p(x0,x)
F· dR−
Z
q(x0,x)
F· dR
which shows
Z
p(x0,x)
F· dR =
Z
q(x0,x)
F· dR
1There is no such thing as a liberal vector ﬁeld.

25.1.
PATH INDEPENDENT LINE INTEGRALS 15 NOV.
469
and that φ is well deﬁned. For small t,
φ (x + tei) −φ (x)
t
=
R
p(x0,x+tei) F · dR−
R
p(x0,x) F · dR
t
=
R
p(x0,x) F · dR+
R
p(x,x+tei) F · dR−
R
p(x0,x) F · dR
t
.
Since U is open, for small t, the ball of radius |t| centered at x is contained in U. There-
fore, the line segment from x to x + tei is also contained in U and so one can take
p (x, x + tei) (s) = x + s (tei) for s ∈[0, 1]. Therefore, the above diﬀerence quotient re-
duces to
1
t
Z 1
0
F (x + s (tei)) · tei ds
=
Z 1
0
Fi (x + s (tei)) ds
=
Fi (x + st (tei))
by the mean value theorem for integrals. Here st is some number between 0 and 1. By
continuity of F, this converges to Fi (x) as t →0. Therefore, ∇φ = F as claimed.
Conversely, if ∇φ = F, then if R : [a, b] →Rp is any C1 curve joining x to y,
Z b
a
F (R (t)) ·R′ (t) dt
=
Z b
a
∇φ (R (t)) ·R′ (t) dt
=
Z b
a
d
dt (φ (R (t))) dt
=
φ (R (b)) −φ (R (a))
=
φ (y) −φ (x)
and this veriﬁes 25.5 in the case where the curve joining the two points is smooth. The
general case follows immediately from this by using this result on each of the pieces of the
piecewise smooth curve. For example if the curve goes from x to p and then from p to y,
the above would imply the integral over the curve from x to p is φ (p) −φ (x) while from p
to y the integral would yield φ (y) −φ (p) . Adding these gives φ (y) −φ (x) . The formula
25.5 implies the line integral over any closed curve equals zero because the starting and
ending points of such a curve are the same. This proves the theorem.
25.1.1
Finding The Scalar Potential, (Recover The Function From
Its Gradient)
Example 25.1.4 Let F (x, y, z) = (cos x −yz sin (xz) , cos (xz) , −yx sin (xz)) . Let C be a
piecewise smooth curve which goes from (π, 1, 1) to
¡ π
2 , 3, 2
¢
. Find
R
C F · dR.
The speciﬁcs of the curve are not given so the problem is nonsense unless the vector ﬁeld
is conservative. Therefore, it is reasonable to look for the function, φ satisfying ∇φ = F.
Such a function satisﬁes
φx = cos x −y (sin xz) z
and so, assuming φ exists,
φ (x, y, z) = sin x + y cos (xz) + ψ (y, z) .
I have to add in the most general thing possible, ψ (y, z) to ensure possible solutions are not
being thrown out. It wouldn’t be good at this point to add in a constant since the answer

470
LINE INTEGRALS 14 NOV.
could involve a function of either or both of the other variables. Now from what was just
obtained,
φy = cos (xz) + ψy = cos xz
and so it is possible to take ψy = 0. Consequently, φ, if it exists is of the form
φ (x, y, z) = sin x + y cos (xz) + ψ (z) .
Now diﬀerentiating this with respect to z gives
φz = −yx sin (xz) + ψz = −yx sin (xz)
and this shows ψ does not depend on z either. Therefore, it suﬃces to take ψ = 0 and
φ (x, y, z) = sin (x) + y cos (xz) .
Therefore, the desired line integral equals
sin
³π
2
´
+ 3 cos (π) −(sin (π) + cos (π)) = −1.
The above process for ﬁnding φ will not lead you astray in the case where there does not
exist a scalar potential. As an example, consider the following.
Example 25.1.5 Let F (x, y, z) =
¡
x, y2x, z
¢
. Find a scalar potential for F if it exists.
If φ exists, then φx = x and so φ = x2
2 + ψ (y, z) . Then φy = ψy (y, z) = xy2 but this
is impossible because the left side depends only on y and z while the right side depends
also on x. Therefore, this vector ﬁeld is not conservative and there does not exist a scalar
potential.
Example 25.1.6 Let F (x, y, z) =
¡
2yx + 1 + y, x2 + x, 1
¢
. Find a scalar potential for F if
it exists.
You need φx = 2yx+1+y and so φ = yx2+x+yx+ψ (y, z) . Then you need φy = x2+x+
ψy = x2+x which shows ψy = 0 and so ψ = ψ (z) . Hence φ = yx2+x+yx+ψ (z) Now ﬁnally,
φz = ψ′ (z) = 1 and so ψ (z) = z will work. A scalar potential is φ (x, y, z) = yx2+x+yx+z.
Example 25.1.7 Let F (x, y, z) =
¡
1, 2yz + z cos y, y2 + sin y
¢
. Find a scalar potential for
F if it exists.
You need φx = 1 and so φ = x + ψ (y, z) . Then you need φy = ψy = 2yz + z cos y and
so ψ = y2z + z sin y + g (z) . Hence φ = x + y2z + z sin y + g (z) and you still don’t know g.
But you must have φz = y2 + sin y + g′ (z) = y2 + sin y and so g is a constant. You can take
it to equal zero. Hence φ = x + y2z + z sin y is a scalar potential.
When you are ﬁnding one of these scalar potentials, be sure to check your work. Take
what you think is the answer and ﬁnd its gradient. If you get the given vector ﬁeld, rejoice.
If not, it is wrong. Start over again.
Example 25.1.8 Let the vector ﬁeld, F be given in Example 25.1.7. Find
R
C F·dR where
C is an oriented curve which goes from (0, π, 2) to (1, π/2, 2) .
This is very easy. It is just φ (1, π/2, 2) −φ (0, π, 2) where φ is the scalar potential in
this example. Thus it equals
µ
1 +
³π
2
´2
2 + 2
¶
−
¡
π22
¢
= 3 −3
2π2

25.1.
PATH INDEPENDENT LINE INTEGRALS 15 NOV.
471
25.1.2
Terminology
For a vector ﬁeld, F (x, y, z) = F1 (x, y, z) i + F2 (x, y, z) j + F3 (x, y, z) k, F is called conser-
vative if it is the gradient of a scalar potential. Thus F is conservative if there exists a scalar
function, φ such that ∇φ = F. This was discussed above. Another way to say this is that
the diﬀerential form F1dx + F2dy + F3dz is exact. This terminology holds with obvious
modiﬁcations in any number of dimensions.

472
LINE INTEGRALS 14 NOV.

Part XI
Green’s Theorem, Integrals On
Surfaces
473


475
Outcomes
Green’s Theorem
A. Recall and verify Green’s Theorem.
B. Apply Green’s Theorem to evaluate line integrals.
C. Apply Green’s Theorem to ﬁnd the area of a region.
Reading: Multivariable Calculus 4.4
Outcome Mapping:
A. L1,L2,3,5
B. 1
C. 2
Surface Integrals
A. Determine the area of a given surface using integration.
B. Evaluate the physical characteristics of a surface such as centroid, mass, and center of
mass using surface integrals.
C. Find the ﬂux of a vector ﬁeld through a surface.
Reading: Multivariable Calculus 4.5
Outcome Mapping:
A. 1
B. 1,2
C. 3
Parametric Surfaces
A. Write a parameterization for a given surface.
B. Identify a surface from its parameterization.
C. Describe a surface from its nets. Sketch a parametric surface.
Reading: Multivariable Calculus 4.6
Outcome Mapping:
A. 3,5,9
B. 1,2
C. 6,7
Integrals over Parametric Surfaces
A. Graphically describe a surface in terms of its parameterization.
B. Determine a (unit) normal vector to a surface from a parameterization of the surface.

476
C. Determine the plane tangent to a surface at a given point.
D. Evaluate the physical characteristics of parameterized surfaces such as centroid, mass,
and center of mass.
E. Find the ﬂux of a ﬂow through a parametric surface.
Reading: Multivariable Calculus 4.7
Outcome Mapping:
A. 1,4
B. 1,4
C. 1,4
D. 1,4
E. 1,4

Green’s Theorem 20 Nov.
Green’s theorem is an important theorem which relates line integrals to integrals over a
surface in the plane. It can be used to establish the much more signiﬁcant Stoke’s theorem
but is interesting for it’s own sake. Historically, it was important in the development of
complex analysis. I will ﬁrst establish Green’s theorem for regions of a particular sort and
then show that the theorem holds for many other regions also. Suppose a region is of the
form indicated in the following picture in which
U
=
{(x, y) : x ∈(a, b) and y ∈(b (x) , t (x))}
=
{(x, y) : y ∈(c, d) and x ∈(l (y) , r (y))} .
9
z
:
y
q
q
q
q
q
q
q
q
U
x = r(y)
x = l(y)
y = t(x)
y = b(x)
c
d
a
b
I will refer to such a region as being convex in both the x and y directions.
Lemma 26.0.9 Let F (x, y) ≡(P (x, y) , Q (x, y)) be a C1 vector ﬁeld deﬁned near U
where U is a region of the sort indicated in the above picture which is convex in both the x
and y directions. Suppose also that the functions, r, l, t, and b in the above picture are all C1
functions and denote by ∂U the boundary of U oriented such that the direction of motion is
counter clockwise. (As you walk around U on ∂U, the points of U are on your left.) Then
Z
∂U
Pdx + Qdy ≡
Z
∂U
F·dR =
Z
U
µ∂Q
∂x −∂P
∂y
¶
dA.
(26.1)
Proof: First consider the right side of 26.1.
Z
U
µ∂Q
∂x −∂P
∂y
¶
dA
=
Z d
c
Z r(y)
l(y)
∂Q
∂x dxdy −
Z b
a
Z t(x)
b(x)
∂P
∂y dydx
=
Z d
c
(Q (r (y) , y) −Q (l (y) , y)) dy +
Z b
a
(P (x, b (x))) −P (x, t (x)) dx.
(26.2)
477

478
GREEN’S THEOREM 20 NOV.
Now consider the left side of 26.1. Denote by V the vertical parts of ∂U and by H the
horizontal parts.
Z
∂U
F·dR =
=
Z
∂U
((0, Q) + (P, 0)) · dR
=
Z d
c
(0, Q (r (s) , s)) · (r′ (s) , 1) ds +
Z
H
(0, Q (r (s) , s)) · (±1, 0) ds
−
Z d
c
(0, Q (l (s) , s)) · (l′ (s) , 1) ds +
Z b
a
(P (s, b (s)) , 0) · (1, b′ (s)) ds
+
Z
V
(P (s, b (s)) , 0) · (0, ±1) ds −
Z b
a
(P (s, t (s)) , 0) · (1, t′ (s)) ds
=
Z d
c
Q (r (s) , s) ds −
Z d
c
Q (l (s) , s) ds +
Z b
a
P (s, b (s)) ds −
Z b
a
P (s, t (s)) ds
which coincides with 26.2. This proves the lemma.
Corollary 26.0.10 Let everything be the same as in Lemma 26.0.9 but only assume the
functions r, l, t, and b are continuous and piecewise C1 functions. Then the conclusion this
lemma is still valid.
Proof: The details are left for you. All you have to do is to break up the various line
integrals into the sum of integrals over sub intervals on which the function of interest is C1.
From this corollary, it follows 26.1 is valid for any triangle for example.
Now suppose 26.1 holds for U1, U2, · · ·, Um and the open sets, Uk have the property that
no two have nonempty intersection and their boundaries intersect only in a ﬁnite number
of piecewise smooth curves. Then 26.1 must hold for U ≡∪m
i=1Ui, the union of these sets.
This is because
Z
U
µ∂Q
∂x −∂P
∂y
¶
dA =
=
m
X
k=1
Z
Uk
µ∂Q
∂x −∂P
∂y
¶
dA
=
m
X
k=1
Z
∂Uk
F · dR =
Z
∂U
F · dR
because if Γ = ∂Uk ∩∂Uj, then its orientation as a part of ∂Uk is opposite to its orientation
as a part of ∂Uj and consequently the line integrals over Γ will cancel, points of Γ also not
being in ∂U. As an illustration, consider the following picture for two such Uk.
¡
¡
@
@
@
@
@
@
¡

¡


@
@
@
R
I

:
ª

ª
U1
U2

26.1.
AN ALTERNATIVE EXPLANATION OF GREEN’S THEOREM
479
Similarly, if U ⊆V and if also ∂U ⊆V and both U and V are open sets for which
26.1 holds, then the open set, V \ (U ∪∂U) consisting of what is left in V after deleting U
along with its boundary also satisﬁes 26.1. Roughly speaking, you can drill holes in a region
for which 26.1 holds and get another region for which this continues to hold provided 26.1
holds for the holes. To see why this is so, consider the following picture which typiﬁes the
situation just described.
V
:
y
9
z
U
9
z
:
y
Then
Z
∂V
F·dR =
Z
V
µ∂Q
∂x −∂P
∂y
¶
dA
=
Z
U
µ∂Q
∂x −∂P
∂y
¶
dA +
Z
V \U
µ∂Q
∂x −∂P
∂y
¶
dA
=
Z
∂U
F·dR +
Z
V \U
µ∂Q
∂x −∂P
∂y
¶
dA
and so
Z
V \U
µ∂Q
∂x −∂P
∂y
¶
dA =
Z
∂V
F·dR−
Z
∂U
F·dR
which equals
Z
∂(V \U)
F · dR
where ∂V is oriented as shown in the picture. (If you walk around the region, V \ U with
the area on the left, you get the indicated orientation for this curve.)
You can see that 26.1 is valid quite generally. This veriﬁes the following theorem.
Theorem 26.0.11 (Green’s Theorem) Let U be an open set in the plane and let
∂U be piecewise smooth and let F (x, y) = (P (x, y) , Q (x, y)) be a C1 vector ﬁeld deﬁned
near U. Then it is often1 the case that
Z
∂U
F · dR =
Z
U
µ∂Q
∂x (x, y) −∂P
∂y (x, y)
¶
dA.
26.1
An Alternative Explanation Of Green’s Theorem
Consider the following picture.
1For a general version see the advanced calculus book by Apostol.
The general versions involve the
concept of a rectiﬁable Jordan curve. You need to be able to take the area integral and to take the line
integral around the boundary.

480
GREEN’S THEOREM 20 NOV.
b
a
x
y = B(x)
y = T(x)
X
X
X
X
X
y
¤
¤
¤
¤
¤
T = (−ny, nx)
n = (nx, ny)
U
In this picture n is the unit outer normal to U and the vector, T shown in the picture is
the unit tangent vector in the direction of counter clockwise motion around U. To see that it
really does point in the correct direction, take the cross product, (nx, ny, 0) × (−ny, nx, 0) .
This equals k. Applying the right hand rule, this shows the vector, (−ny, nx) really does
point in the direction indicated by the picture.
Next I will establish Gauss’ theorem for regions like U. The boundary of U is denoted
by ∂U.
Lemma 26.1.1 (Gauss)Let (H (x, y) , K (x, y)) be a C1 vector ﬁeld deﬁned near U. Then
for n the unit outer normal,
Z
U
(Hx + Ky) dA =
Z
∂U
(H, K) · ndl
Proof:
A parameterization for the top is (x, T (x)) and a parameterization for the
bottom is (x, B (x)) where in both cases, x ∈[a, b] . Thus dl =
q
1 + T ′ (x)2dx on the top
and dl =
q
1 + B′ (x)2dx on the bottom. Thus also, on the top, you can ﬁnd the exterior
normal by considering it as the level surface, y −T (x) = 0. Thus a unit normal to this
surface is
n = (−T ′ (x) , 1)
q
1 + T ′ (x)2 = (nx, ny)
and you see that since the y component is positive, it is the outer normal, pointing away
from U. Similarly, the unit outer normal on the bottom is given by
n = (B′ (x) , −1)
q
1 + B′ (x)2 = (nx, ny)

26.1.
AN ALTERNATIVE EXPLANATION OF GREEN’S THEOREM
481
First consider
Z
U
KydA
=
Z b
a
Z T (x)
B(x)
Kydydx =
Z b
a
(K (x, T (x)) −K (x, B (x))) dx
=
Z b
a
K (x, T (x)) dx −
Z b
a
K (x, B (x)) dx
=
Z b
a
K (x, T (x))
ny
z
}|
{
1
q
1 + T ′ (x)2
dl
z
}|
{
q
1 + T ′ (x)2dx
+
Z b
a
K (x, B (x))
ny
z
}|
{


−1
q
1 + B′ (x)2


dl
z
}|
{
q
1 + B′ (x)2dx
=
Z
∂U
Knydl
Similar reasoning shows that
Z
U
HxdA =
Z
∂U
Hnxdl
Therefore, this proves the Lemma because from the above,
Z
U
(Hx + Ky) dA
=
Z
U
HxdA +
Z
U
KydA =
Z
∂U
Hnxdl +
Z
∂U
Knydl
=
Z
∂U
(H, K) · ndl
Now this theorem holds for many regions much more general than the one shown. In
fact, it holds for any region which is made up by pasting together regions like the above.
This is because the area integrals add and the integrals on the parts of the boundary which
are shared by two pieces cancel due to the fact they have the exterior normals which are in
opposite directions. For example, consider the following picture. If the divergence theorem
holds for each Vi in the following picture, then it holds for the union of these two.
V1
V2
This theorem is also called the divergence theorem. This is because the divergence of
the vector ﬁeld, (H (x, y) , K (x, y)) is deﬁned as Hx (x, y) + Ky (x, y) .
Theorem 26.1.2 (Green’s Theorem) Let U be any bounded open set in R2 for
which the above Gauss’ theorem holds and let
F (x, y) ≡(P (x, y) , Q (x, y))
be a C1 vector ﬁeld deﬁned near U. Then
Z
U
(Qx −Py) dA =
Z
∂U
F·dR

482
GREEN’S THEOREM 20 NOV.
where the line integral is oriented in the counter clockwise direction.
Proof: If r (t) is a parameterization of ∂U near a point on ∂U, then recall the unit
tangent vector, T as shown in the above picture satisﬁes |r′ (t)| T = r′ (t) . Thus F·dR is
of the form F (r (t)) ·r′ (t) dt = F · T |r′ (t)| dt = F · Tdl because dl = |r′ (t)| dt. Then using
Lemma 26.1.1 and letting (H, K) = (Q, −P)
Z
U
(Qx −Py) dA
=
Z
U
(Hx + Ky) dA
=
Z
∂U
(H, K) · (nx, ny) dl
=
Z
∂U
(Q, −P) · (nx, ny) dl
=
Z
∂U
(P, Q) · (−ny, nx) dl
=
Z
∂U
F · Tdl =
Z
∂U
F·dR.
This proves Green’s theorem.
26.2
Area And Green’s Theorem
Proposition 26.2.1 Let U be an open set in R2 for which Green’s theorem holds. Then
Area of U =
Z
∂U
F·dR
where F (x, y) = 1
2 (−y, x) , (0, x) , or (−y, 0) .
Proof: This follows immediately from Green’s theorem.
Example 26.2.2 Use Proposition 26.2.1 to ﬁnd the area of the ellipse
x2
a2 + y2
b2 ≤1.
You can parameterize the boundary of this ellipse as
x = a cos t, y = b sin t, t ∈[0, 2π] .
Then from Proposition 26.2.1,
Area equals
=
1
2
Z 2π
0
(−b sin t, a cos t) · (−a sin t, b cos t) dt
=
1
2
Z 2π
0
(ab) dt = πab.
Example 26.2.3 Find
R
∂U F·dR where U is the set,
©
(x, y) : x2 + 3y2 ≤9
ª
and F (x, y) =
(y, −x) .
One way to do this is to parameterize the boundary of U and then compute the line
integral directly. It is easier to use Green’s theorem. The desired line integral equals
Z
U
((−1) −1) dA = −2
Z
U
dA.
Now U is an ellipse having area equal to 3
√
3 and so the answer is −6
√
3.

26.2.
AREA AND GREEN’S THEOREM
483
Example 26.2.4 Find
R
∂U F·dR where U is the set, {(x, y) : 2 ≤x ≤4, 0 ≤y ≤3} and
F (x, y) =
¡
x sin y, y3 cos x
¢
.
From Green’s theorem this line integral equals
Z 4
2
Z 3
0
¡
−y3 sin x −x cos y
¢
dydx
=
81
4 cos 4 −6 sin 3 −81
4 cos 2.
This is much easier than computing the line integral because you don’t have to break the
boundary in pieces and consider each separately.
Example 26.2.5 Find
R
∂U F·dR where U is the set, {(x, y) : 2 ≤x ≤4, x ≤y ≤3} and
F (x, y) = (x sin y, y sin x) .
From Green’s theorem this line integral equals
Z 4
2
Z 3
x
(y cos x −x cos y) dydx
=
−3
2 sin 4 −6 sin 3 −8 cos 4 −9
2 sin 2 + 4 cos 2.

484
GREEN’S THEOREM 20 NOV.

The Integral On Two
Dimensional Surfaces In R3
27-28 Nov.
27.1
Parametrically Deﬁned Surfaces
Deﬁnition 27.1.1 Let S be a subset of R3. Then S is a smooth surface if there
exists an open set, U ⊆R2 and a C1 function, r deﬁned on U such that r (U) = S, r is one
to one, and for all (u, v) ∈U,
ru × rv ̸= 0.
(27.1)
This last condition ensures that there is always a well deﬁned normal on S. This function,
r is called a parameterization of the surface. It is just like a parameterization of a curve but
here there are two parameters, u, v.
One way to think of this is that there is a piece of rubber occupying U in the plane
and then it is taken and stretched in three dimensions. This gives S. Here is an interesting
example which is already familiar.
Example 27.1.2 Let (φ, θ) ∈(0, π) × (0, 2π) and for such (φ, θ) ,
r (φ, θ) ≡


2 sin (φ) cos (θ)
2 sin (φ) sin (θ)
2 cos (φ)


This gives most of a sphere of radiius 2 for S. You should check condition 27.1. You will
ﬁnd that |rφ × rθ| = 4 sin (φ) ̸= 0.
Example 27.1.3 Let R > r. Consider
r (u, v) =


(R + r cos (u)) cos (v)
(R + r cos (u)) sin (v)
r sin (u)


where (u, v) ∈(0, 2π) × (0, 2π) . This surface is most of the surface of a torus (donut) with
small radius equal to r. It is obtained by revolving the circle of radius r centered at (R, 0, 0)
about the z axis. Here is a picture.
485

486
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
In the above I have assumed U is open. However, this is generalized later. It is amazing
how far this can be generalized in applications to integration.
In general, if you ﬁx u and consider r (u, v) as a function of v, this yields a smooth curve
which lies in the surface, S. By ﬁxing diﬀerent values of u you obtain many diﬀerent curves
in S. Similarly you can ﬁx v and consider r (u, v) as a function of u. The curves which result
in this way are called a net for the surface. This is the way a computer graphs a surface. It
graphs lots of diﬀerent curves as just described. You can see this in the above picture of a
torus. The curves which make up the shape shown correspond to one of the variables in the
parameterization being ﬁxed. Now at a point, r (u, v) of S, there are two vectors tangent to
S at this point, ru (u, v) and rv (u, v) . These two vectors determine a plane which can be
considered tangent to the surface at the point, r (u, v). You can ﬁnd an equation for this
plane if you can obtain a normal vector. However, this is easy. You simply take rv × ru to
obtain a vector which is normal to the tangent plane. Here is a picture. The two curves
correspond to u →r (u, v) and v →r (u, v) . The vectors ru and rv are tangent to the
respective curves as shown. Then taking the cross product gives a normal to the surface at
that point.

:






ru
rv
v →r(u, v)
@
@
R
u →r(u, v)
BBM
B
B
B
B
B
BM
rv × ru
Example 27.1.4 Let S be the surface deﬁned in Example 27.1.3 in which R = 2 and r = 1.
Find a tangent plane to the point
r
³π
4 , π
4
´
.
This point is
³√
2 + 1
2,
√
2 + 1
2,
√
2
2
´
. I only need to ﬁnd a normal vector in order to ﬁnd

27.2.
THE TWO DIMENSIONAL AREA IN R3
487
the plane.
ru × rv
=


−sin u cos v
−sin u sin v
cos u

×


−(2 + cos u) sin v
(2 + cos u) cos v
0


=


−(cos u) (2 + cos u) cos v
−(cos u) (2 + cos u) sin v
−
¡
sin u cos2 v
¢
(2 + cos u) −
¡
sin u sin2 v
¢
(2 + cos u)


Now plugging in the desired values of u and v, a normal vector is


−1 −1
4
√
2
−1 −1
4
√
2
−
√
2 −1
2

.
I don’t like the minus signs so the normal vector I will use is
µ
1 + 1
4
√
2, 1 + 1
4
√
2,
√
2 + 1
2
¶T
.
Now it follows the equation of the tangent plane is
µ
1 + 1
4
√
2
¶ µ
x −
√
2 −1
2
¶
+
µ
1 + 1
4
√
2
¶ µ
y −
√
2 −1
2
¶
+
µ√
2 + 1
2
¶ µ
z −1
2
√
2
¶
= 0.
You could simplify this if you wanted.
µ
1 + 1
4
√
2
¶
x +
µ
1 + 1
4
√
2
¶
y +
µ√
2 + 1
2
¶
z = 5
2
√
2 + 3.
27.2
The Two Dimensional Area In R3
Consider the boundary of some three dimensional region such that a function, is deﬁned on
this boundary. Imagine taking the value of this function at a point, multiplying this value
by the area of an inﬁnitesimal chunk of area located at this point and then adding these up.
This is just the notion of the integral presented earlier only now there is a diﬀerence because
this inﬁnitesimal chunk of area should be considered as two dimensional even though it is
in three dimensions. However, it is not really all that diﬀerent from what was done earlier.
It all depends on the following fundamental deﬁnition which is just a review of the fact
presented earlier that the area of a parallelogram determined by two vectors in R3 is the
norm of the cross product of the two vectors.
Deﬁnition 27.2.1 Let u1, u2 be vectors in R3. The 2 dimensional parallelogram
determined by these vectors will be denoted by P (u1, u2) and it is deﬁned as
P (u1, u2) ≡



2
X
j=1
sjuj : sj ∈[0, 1]


.
Then the area of this parallelogram is
area P (u1, u2) ≡|u1 × u2| .

488
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
Suppose then that x = f (u) where u ∈U, a subset of R2 and x is a point in V, a
subset of 3 dimensional space. Thus, letting the Cartesian coordinates of x be given by
x = (x1, x2, x3)T , each xi being a function of u, an inﬁnitesimal rectangle located at u0
corresponds to an inﬁnitesimal parallelogram located at f (u0) which is determined by the 2
vectors
n
∂f(u0)
∂ui
dui
o2
i=1 , each of which is tangent to the surface deﬁned by x = f (u) . (No
sum on the repeated index.)
dV
u0
du2
du1

:






















fu2(u0)du2
fu1(u0)du1
f(dV )








+
From Deﬁnition 27.2.1, the volume of this inﬁnitesimal parallelepiped located at f (u0)
is given by
¯¯¯¯
∂f (u0)
∂u1
du1 × ∂f (u0)
∂u2
du2
¯¯¯¯
=
¯¯¯¯
∂f (u0)
∂u1
× ∂f (u0)
∂u2
¯¯¯¯ du1du2
(27.2)
=
|fu1 × fu2| du1du2
(27.3)
It might help to think of a lizard. The inﬁnitesimal parallelogram is like a very small
scale on a lizard. This is the essence of the idea. To deﬁne the area of the lizard sum up
areas of individual scales If the scales are small enough, their sum would serve as a good
approximation to the area of the lizard.

27.2.
THE TWO DIMENSIONAL AREA IN R3
489
1.This motivates the following fundamental procedure which I hope is extremely familiar
from the earlier material.
Procedure 27.2.2 Suppose U is a subset of R2 and suppose f : U →f (U) ⊆R3
is a one to one and C1 function. Then if h : f (U) →R, deﬁne the 2 dimensional surface
integral,
R
f(U) h (x) dA according to the following formula.
Z
f(U)
h (x) dA ≡
Z
U
h (f (u)) |fu1 (u) × fu2 (u)| du1du2.
Deﬁnition 27.2.3 It is customary to write |fu1 (u) × fu2 (u)| = ∂(x1,x2,x3)
∂(u1,u2)
because
this new notation generalizes to far more general situations for which the cross product is
not deﬁned. For example, one can consider three dimensional surfaces in R8.
First here is a simple example where the surface is actually in the plane.
Example 27.2.4 Find the area of the region labelled A in the following picture. The two
circles are of radius 1, one has center (0, 0) and the other has center (1, 0) .
A
π/3
The circles bounding these disks are x2 +y2 = 1 and (x −1)2 +y2 = x2 +y2 −2x+1 = 1.
Therefore, in polar coordinates these are of the form r = 1 and r = 2 cos θ.
The set A corresponds to the set U, in the (θ, r) plane determined by θ ∈
£
−π
3 , π
3
¤
and
for each value of θ in this interval, r goes from 1 up to 2 cos θ. Therefore, the area of this
region is of the form,
Z
U
1 dV =
Z π/3
−π/3
Z 2 cos θ
1
∂(x1, x2, x3)
∂(θ, r)
dr dθ.
It is necessary to ﬁnd ∂(x1,x2)
∂(θ,r) . The mapping f : U →R2 takes the form
f (θ, r) = (r cos θ, r sin θ)T .
Here x3 = 0 and so
∂(x1, x2, x3)
∂(θ, r)
=
¯¯¯¯¯¯
¯¯¯¯¯¯
i
j
k
∂x1
∂θ
∂x2
∂θ
∂x3
∂θ
∂x1
∂r
∂x2
∂r
∂x3
∂r
¯¯¯¯¯¯
¯¯¯¯¯¯
=
¯¯¯¯¯¯
¯¯¯¯¯¯
i
j
k
−r sin θ
r cos θ
0
cos θ
sin θ
0
¯¯¯¯¯¯
¯¯¯¯¯¯
= r
1This beautiful lizard is a Sceloporus magister. It was photographed by C. Riley Nelson who is in the
Zoology department at Brigham Young University c⃝2004 in Kane Co. Utah. The lizard is a little less
than one foot in length.

490
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
Therefore, the area element is r dr dθ. It follows the desired area is
Z π/3
−π/3
Z 2 cos θ
1
r dr dθ = 1
2
√
3 + 1
3π.
Notice how the area element reduced to the area element for polar coordinates.
Example 27.2.5 Consider the surface given by z = x2 for (x, y) ∈[0, 1] × [0, 1] = U. Find
the surface area of this surface.
The ﬁrst step in using the above is to write this surface in the form x = f (u) . This is
easy to do if you let u = (x, y) . Then f (x, y) =
¡
x, y, x2¢
. If you like, let x = u1 and y = u2.
What is ∂(x1,x2,x3)
∂(x,y)
= |fx × fy|?
fx =


1
0
2x

, fy =


0
1
0


and so
|fx × fy| =
¯¯¯¯¯¯


1
0
2x

×


0
1
0


¯¯¯¯¯¯
=
p
1 + 4x2
and so the area element is
√
1 + 4x2 dx dy and the surface area is obtained by integrating
the function, h (x) ≡1. Therefore, this area is
Z
U
dA =
Z 1
0
Z 1
0
p
1 + 4x2 dx dy = 1
2
√
5 −1
4 ln
³
−2 +
√
5
´
which can be obtained by using the trig. substitution, 2x = tan θ on the inside integral.
Note this all depends on being able to write the surface in the form, x = f (u) for
u ∈U ⊆Rp. Surfaces obtained in this form are called parametrically deﬁned surfaces.
These are best but sometimes you have some other description of a surface and in these
cases things can get pretty intractable. For example, you might have a level surface of the
form 3x2 + 4y4 + z6 = 10. In this case, you could solve for z using methods of algebra.
Thus z =
6p
10 −3x2 −4y4 and a parametric description of part of this level surface is
³
x, y,
6p
10 −3x2 −4y4
´
for (x, y) ∈U where U =
©
(x, y) : 3x2 + 4y4 ≤10
ª
. But what if
the level surface was something like
sin
¡
x2 + ln
¡
7 + y2 sin x
¢¢
+ sin (zx) ez = 11 sin (xyz)?
I really don’t see how to use methods of algebra to solve for some variable in terms of the
others. It isn’t even clear to me whether there are any points (x, y, z) ∈R3 satisfying this
particular relation. However, if a point satisfying this relation can be identiﬁed, the implicit
function theorem from advanced calculus can usually be used to assert one of the variables
is a function of the others, proving the existence of a parameterization at least locally. The
problem is, this theorem doesn’t give us the answer in terms of known functions so this
isn’t much help. Finding a parametric description of a surface is a hard problem and there
are no easy answers. This is a good example which illustrates the gulf between theory and
practice.
Example 27.2.6 Let U = [0, 12] × [0, 2π] and let f : U →R3 be given by f (t, s) ≡
(2 cos t + cos s, 2 sin t + sin s, t)T . Find a double integral for the surface area. A graph of
this surface is drawn below.

27.2.
THE TWO DIMENSIONAL AREA IN R3
491
It looks like something you would use to make sausages2. Anyway,
ft =


−2 sin t
2 cos t
1

, fs =


−sin s
cos s
0


and
ft × fs =


−cos s
−sin s
−2 sin t cos s + 2 cos t sin s


and so
∂(x1, x2, x3)
∂(t, s)
= |ft × fs| =
p
5 −4 sin2 t sin2 s −8 sin t sin s cos t cos s −4 cos2 t cos2 s.
Therefore, the desired integral giving the area is
Z 2π
0
Z 12
0
p
5 −4 sin2 t sin2 s −8 sin t sin s cos t cos s −4 cos2 t cos2 s dt ds.
If you really needed to ﬁnd the number this equals, how would you go about ﬁnding it?
This is an interesting question and there is no single right answer. You should think about
this. It is important in some physical applications to get the number even when you can’t
ﬁnd the antiderivative. Here is an example for which you will be able to ﬁnd the integrals.
Example 27.2.7 Let U = [0, 2π] × [0, 2π] and for (t, s) ∈U, let
f (t, s) = (2 cos t + cos t cos s, −2 sin t −sin t cos s, sin s)T .
Find the area of f (U) . This is the surface of a donut shown below. The fancy name for
this shape is a torus.
2At Volwerth’s in Hancock Michigan, they make excellent sausages and hot dogs. The best are made
from “natural casings” which are the linings of intestines.

492
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
To ﬁnd its area,
ft =


−2 sin t −sin t cos s
−2 cos t −cos t cos s
0

, fs =


−cos t sin s
sin t sin s
cos s


and so |ft × fs| = (cos s + 2) so the area element is (cos s + 2) ds dt and the area is
Z 2π
0
Z 2π
0
(cos s + 2) ds dt = 8π2
Example 27.2.8 Let U = [0, 2π] × [0, 2π] and for (t, s) ∈U, let
f (t, s) = (2 cos t + cos t cos s, −2 sin t −sin t cos s, sin s)T .
Find
Z
f(U)
h dV
where h (x, y, z) = x2.
Everything is the same as the preceding example except this time it is an integral of a
function. The area element is (cos s + 2) ds dt and so the integral called for is
Z
f(U)
h dA =
Z 2π
0
Z 2π
0


x on the surface
z
}|
{
2 cos t + cos t cos s


2
(cos s + 2) ds dt = 22π2
Example 27.2.9 Let U = [−5, 5] × [0, 3π] and for (s, t) ∈U, let
f (s, t) = (3s cos t, 3s sin t, 4t) .
Find a formula for the area of f (U) in terms of integrals. This is called a helicoid. Here is
a picture of it.

27.2.
THE TWO DIMENSIONAL AREA IN R3
493
The area element is
|(3 cos (t) , 3 sin (t) , 0) × (−3 sin (t) , 3s cos (t) , 4)| dsdt =
q
144 +
¡
9 (cos2 t) s + 9 sin2 t
¢2dsdt
Therefore, the area is given by the double integral,
Z 5
−5
Z 3π
0
q
144 +
¡
9 (cos2 t) s + 9 sin2 t
¢2dsdt
You can deﬁne the center of mass and density of a surface in exactly the same way as
was done before.
Deﬁnition 27.2.10 Let S be a surface with area (volume) element dS. The den-
sity with respect to area is a function which integrated gives the mass. Thus if δ (x) is
the density, the mass of S is
Z
S
δ (x, y, z) dS.
The center of mass is deﬁned exactly as before.
xc
≡
R
S δ (x, y, z) xdS
R
S δ (x, y, z) dS , yc ≡
R
S δ (x, y, z) ydS
R
S δ (x, y, z) dS
zc
≡
R
S δ (x, y, z) zdS
R
S δ (x, y, z) dS .
There is no new thing here.
You simply are integrating over a surface rather than
a volume.
Of course you must put the variables, x, y, z as well as dS in terms of the
parameters used to compute the integrals.
Example 27.2.11 The surface is given by (x, y, z) = (sin φ cos θ, sin φ sin θ, cos φ) where
(φ, θ) ∈(0, π) × (0, 2π) . Thus the surface is the surface of a sphere of radius 1. Review
spherical coordinates at this time if this is not obvious to you. Suppose the density of a
point on this surface corresponding to (φ, θ) is sin2 φ. That is, the density is equal to the
square of the distance to the z axis. Find the total mass and the center of mass of this
surface.
First ﬁnd the area element. This equals
dS
=
|(cos φ cos θ, cos φ sin θ, −sin φ) × (−sin φ sin θ, sin φ cos θ, 0)| dθdφ
=
q¡
sin2 φ cos θ
¢2 +
¡
sin2 φ sin θ
¢2 +
¡
cos φ cos2 θ sin φ + cos φ sin2 θ sin φ
¢2dθdφ
=
sin (φ) dθdφ

494
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
To ﬁnd the total mass you must integrate this area element times the density. Thus the
total mass is
Z π
0
Z 2π
0
¡
sin2 (φ)
¢
sin (φ) dθdφ = 8
3π
Next you want to ﬁnd the center of mass. By symmetry, it should be at the origin. As
before, the center of mass does not need to be in the surface just as it did not need to be
in the three dimensional shape. Now on the surface, x = sin φ cos θ, y = sin φ sin θ, and
z = cos φ. Consider the formulas for this.
xc
=
R π
0
R 2π
0
(sin (φ) cos (θ))
¡
sin2 (φ)
¢
sin (φ) dθdφ
¡ 8
3π
¢
= 0,
yc
=
R π
0
R 2π
0
(sin (φ) sin (θ))
¡
sin2 (φ)
¢
sin (φ) dθdφ
¡ 8
3π
¢
= 0,
zc
=
R π
0
R 2π
0
(cos (φ))
¡
sin2 (φ)
¢
sin (φ) dθdφ
¡ 8
3π
¢
= 0.
Example 27.2.12 In the above example suppose δ (x, y, z) = z + 1. What is the mass and
center of mass?
The total mass is
Z π
0
Z 2π
0
(1 + cos (φ)) sin (φ) dθdφ = 4π.
Next, the center of mass is given by
xc
=
R π
0
R 2π
0
(sin (φ) cos (θ)) (1 + cos (φ)) sin (φ) dθdφ
4π
= 0,
yc
=
R π
0
R 2π
0
(sin (φ) sin (θ)) (1 + cos (φ)) sin (φ) dθdφ
4π
= 0,
zc
=
R π
0
R 2π
0
(cos (φ)) (1 + cos (φ)) sin (φ) dθdφ
4π
= 1
3
27.2.1
Surfaces Of The Form z = f (x, y)
The special case where a surface is in the form z = f (x, y) , (x, y) ∈U, yields a simple
formula which is used most often in this situation. You write the surface parametrically in
the form f (x, y) = (x, y, f (x, y))T such that (x, y) ∈U. Then
fx =


1
0
fx

, fy =


0
1
fy


and
|fx × fy| =
q
1 + f 2y + f 2x
so the area element is
q
1 + f 2y + f 2x dx dy.
When the surface of interest comes in this simple form, people generally use this area element
directly rather than worrying about a parameterization and taking cross products.

27.2.
THE TWO DIMENSIONAL AREA IN R3
495
In the case where the surface is of the form x = f (y, z) for (y, z) ∈U, the area element
is obtained similarly and is
q
1 + f 2y + f 2z dy dz.
I think you can guess what the area element is if y = f (x, z) .
There is also a simple geometric description of these area elements. Consider the surface
z = f (x, y) . This is a level surface of the function of three variables z −f (x, y) . In fact the
surface is simply z−f (x, y) = 0. Now consider the gradient of this function of three variables.
The gradient is perpendicular to the surface and the third component is positive in this case.
This gradient is (−fx, −fy, 1) and so the unit upward normal is just
1
√
1+f 2
x+f 2
y (−fx, −fy, 1) .
Now consider the following picture.

B
B
B
B
BBM
6
k
n θ
θ
dV
dxdy
In this picture, you are looking at a chunk of area on the surface seen on edge and so it
seems reasonable to expect to have dx dy = dV cos θ. But it is easy to ﬁnd cos θ from the
picture and the properties of the dot product.
cos θ = n · k
|n| |k| =
1
q
1 + f 2x + f 2y
.
Therefore, dA =
q
1 + f 2x + f 2y dx dy as claimed. In this context, the surface involved is
referred to as S because the vector valued function, f giving the parameterization will not
have been identiﬁed.
Example 27.2.13 Let z =
p
x2 + y2 where (x, y) ∈U for U =
©
(x, y) : x2 + y2 ≤4
ª
Find
Z
S
h dS
where h (x, y, z) = x + z and S is the surface described as
³
x, y,
p
x2 + y2
´
for (x, y) ∈U.
Here you can see directly the angle in the above picture is π
4 and so dV =
√
2 dx dy. If
you don’t see this or if it is unclear, simply compute
q
1 + f 2x + f 2y and you will ﬁnd it is
√
2. Therefore, using polar coordinates,
Z
S
h dS
=
Z
U
³
x +
p
x2 + y2
´ √
2 dA
=
√
2
Z 2π
0
Z 2
0
(r cos θ + r) r dr dθ
=
16
3
√
2π.
One other issue is worth mentioning. Suppose fi : Ui →R3 where Ui are sets in R2
and suppose f1 (U1) intersects f2 (U2) along C where C = h (V ) for V ⊆R1. Then deﬁne

496
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
integrals and areas over f1 (U1) ∪f2 (U2) as follows.
Z
f1(U1)∪f2(U2)
g dA ≡
Z
f1(U1)
g dA +
Z
f2(U2)
g dA.
Admittedly, the set C gets added in twice but this doesn’t matter because its 2 dimensional
volume equals zero and therefore, the integrals over this set will also be zero.
I have been purposely vague about precise mathematical conditions necessary for the
above procedures. This is because the precise mathematical conditions which are usually
cited are very technical and at the same time far too restrictive. The most general condi-
tions under which these sorts of procedures are valid include things like Lipschitz functions
deﬁned on very general sets. These are functions satisfying a Lipschitz condition of the
form |f (x) −f (y)| ≤K |x −y| . For example, y = |x| is Lipschitz continuous. However,
this function does not have a derivative at every point. So it is with Lipschitz functions.
However, it turns out these functions have derivatives at enough points to push everything
through but this requires considerations involving the Lebesgue integral. Lipschitz functions
are also not the most general kind of function for which the above is valid. There are many
very interesting issues here which can keep you fascinated for years.
27.3
Flux
Imagine a surface, S which is ﬁxed in space and let v be a vector ﬁeld representing the
velocity of a ﬂuid ﬂowing through this surface. It is reasonable to ask how fast the ﬂuid
crosses the surface in terms of units of mass per units of time. This is expressed in terms of
the surface integral,
Z
S
ρv · ndA
where ρ is the density and n is the normal vector to the surface in the direction in which the
crossing is taking place. The vector ﬁeld, ρv is called the ﬂux. To get the rate of transfer
of mass across the surface, you take the dot product of the ﬂux with the appropriate unit
normal vector and integrate this over the surface. People also speak of heat ﬂux. In general,
when they speak of ﬂux, they mean the thing you dot with a unit normal vector and
integrate to ﬁnd the rate at which something crosses a surface. A little later, this idea will
be explored much more when the divergence theorem is established. It is a very important
idea. You should think about the physical reasons the ﬂux of such a ﬂuid is given as above.
Why do you use the unit normal for example? Why not some normal which has diﬀerent
length? Why do you need to take the dot product with the normal? In general situations,
people assume formulas about the ﬂux in terms of other quantities such as temperature or
concentration. I will mention some later at a convenient place.
27.3.1
Exercises With Answers
1. Find a parameterization for the intersection of the planes x + y + 2z = −3 and
2x −y + z = −4.
Answer:
(x, y, z) =
¡
−t −7
3, −t −2
3, t
¢
2. Find a parameterization for the intersection of the plane 4x + 2y + 4z = 0 and the
circular cylinder x2 + y2 = 16.
Answer:

27.3.
FLUX
497
The cylinder is of the form x = 4 cos t, y = 4 sin t and z = z. Therefore, from the
equation of the plane, 16 cos t+8 sin t+4z = 0. Therefore, z = −16 cos t−8 sin t and this
shows the parameterization is of the form (x, y, z) = (4 cos t, 4 sin t, −16 cos t −8 sin t)
where t ∈[0, 2π] .
3. Find a parameterization for the intersection of the plane 3x + 2y + z = 4 and the
elliptic cylinder x2 + 4z2 = 1.
Answer:
The cylinder is of the form x = cos t, 2z = sin t and y = y. Therefore, from the equation
of the plane, 3 cos t+2y + 1
2 sin t = 4. Therefore, y = 2−3
2 cos t−1
4 sin t and this shows
the parameterization is of the form (x, y, z) =
¡
cos t, 2 −3
2 cos t −1
4 sin t, 1
2 sin t
¢
where
t ∈[0, 2π] .
4. Find a parameterization for the straight line joining (4, 3, 2) and (1, 7, 6) .
Answer:
(x, y, z) = (4, 3, 2) + t (−3, 4, 4) = (4 −3t, 3 + 4t, 2 + 4t) where t ∈[0, 1] .
5. Find a parameterization for the intersection of the surfaces y + 3z = 4x2 + 4 and
4y + 4z = 2x + 4.
Answer:
This is an application of Cramer’s rule. y = −2x2 −1
2 + 3
4x, z = −1
4x + 3
2 + 2x2.
Therefore, the parameterization is (x, y, z) =
¡
t, −2t2 −1
2 + 3
4t, −1
4t + 3
2 + 2t2¢
.
6. Find the area of S if S is the part of the circular cylinder x2 + y2 = 16 which lies
between z = 0 and z = 4 + y.
Answer:
Use the parameterization, x = 4 cos v, y = 4 sin v and z = u with the parameter
domain described as follows. The parameter, v goes from −π
2 to 3π
2 and for each v in
this interval, u should go from 0 to 4+4 sin v. To see this observe that the cylinder has
its axis parallel to the z axis and if you look at a side view of the surface you would
see something like this:
y
z
¡
¡
¡
¡
¡
¡
¡
¡
¡
¡
The positive x axis is coming out of the paper toward you in the above picture and
the angle v is the usual angle measured from the positive x axis. Therefore, the area
is just A =
R 3π/2
−π/2
R 4+4 sin v
0
4 du dv = 32π.

498
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
7. Find the area of S if S is the part of the cone x2 + y2 = 9z2 between z = 0 and z = h.
Answer:
When z = h , x2 + y2 = 9h2 which is the boundary of a circle of radius ah. A
parameterization of this surface is x = u, y = v, z =
1
3
p
(u2 + v2) where (u, v) ∈
D, a disk centered at the origin having radius ha. Therefore, the volume is just
R
D
p
1 + z2u + z2v dA =
R ha
−ha
R √
(9h2−u2)
−√
(9h2−u2)
1
3
√
10 dv du = 3πh2√
10
8. Parametrizing the cylinder x2 + y2 = 4 by x = 2 cos v, y = 2 sin v, z = u, show that
the area element is dA = 2 du dv
Answer:
It is necessary to compute
|fu × fv| =
¯¯¯¯¯¯


0
0
1

×


−2 sin v
2 cos v
0


¯¯¯¯¯¯
= 2.
and so the area element is as described.
9. Find the area enclosed by the limacon r = 2 + cos θ.
Answer:
You can graph this region and you see it is sort of an oval shape and that θ ∈[0, 2π]
while r goes from 0 up to 2 + cos θ. Now x = r cos θ and y = r sin θ are the x and y
coordinates corresponding to r and θ in the above parameter domain. Therefore, the
area of the limacon equals
R
P
¯¯¯ ∂(x,y)
∂(r,θ)
¯¯¯ dr dθ =
R 2π
0
R 2+cos θ
0
r dr dθ because the Jacobian
equals r in this case. Therefore, the area equals
R 2π
0
R 2+cos θ
0
r dr dθ = 9
2π.
10. Find the surface area of the paraboloid z = h
¡
1 −x2 −y2¢
between z = 0 and z = h.
Answer:
Let R denote the unit circle. Then the area of the surface above this circle would be
R
R
p
1 + 4x2h2 + 4y2h2 dA. Changing to polar coordinates, this becomes
R 2π
0
R 1
0
¡√
1 + 4h2r2¢
r dr dθ =
π
6h2
³¡
1 + 4h2¢3/2 −1
´
.
11. Evaluate
R
S (1 + x) dA where S is the part of the plane 2x + 3y + 3z = 18 which is in
the ﬁrst octant.
Answer:
R 6
0
R 6−2
3 x
0
(1 + x) 1
3
√
22 dy dx = 28
√
22
12. Evaluate
R
S (1 + x) dA where S is the part of the cylinder x2 +y2 = 16 between z = 0
and z = h.
Answer:
Parametrize the cylinder as x = 4 cos θ and y = 4 sin θ while z = t and the parameter
domain is just [0, 2π] × [0, h] . Then the integral to evaluate would be
Z 2π
0
Z h
0
(1 + 4 cos θ) 4 dt dθ = 8hπ.
Note how 4 cos θ was substituted for x and the area element is 4 dt dθ .

27.3.
FLUX
499
13. Evaluate
R
S (1 + x) dA where S is the hemisphere x2 + y2 + z2 = 16 between x = 0
and x = 4.
Answer:
Parametrize the sphere as x = 4 sin φ cos θ, y = 4 sin φ sin θ, and z = 4 cos φ and
consider the values of the parameters. Since it is referred to as a hemisphere and
involves x > 0, θ ∈
£
−π
2 , π
2
¤
and φ ∈[0, π] . Then the area element is
p
a4 sin φ dθ dφ
and so the integral to evaluate is
Z π
0
Z π/2
−π/2
(1 + 4 sin φ cos θ) 16 sin φ dθ dφ = 96π
14. For (θ, α) ∈[0, 2π] × [0, 2π] , let
f (θ, α) ≡(cos θ (2 + cos α) , −sin θ (2 + cos α) , sin α)T .
Find the area of f ([0, 2π] × [0, 2π]) .
Answer:
|fθ × fα|
=
¯¯¯¯¯¯


−sin (θ) (2 + cos α)
−cos (θ) (2 + cos α)
0

×


−cos θ sin α
sin θ sin α
cos α


¯¯¯¯¯¯
=
¡
4 + 4 cos α + cos2 α
¢1/2
and so the area element is
¡
4 + 4 cos α + cos2 α
¢1/2 dθ dα.
Therefore, the area is
Z 2π
0
Z 2π
0
¡
4 + 4 cos α + cos2 α
¢1/2 dθ dα =
Z 2π
0
Z 2π
0
(2 + cos α) dθ dα = 8π2.
15. For (θ, α) ∈[0, 2π] × [0, 2π] , let
f (θ, α) ≡(cos θ (4 + 2 cos α) , −sin θ (4 + 2 cos α) , 2 sin α)T .
Also let h (x) = cos α where α is such that
x = (cos θ (4 + 2 cos α) , −sin θ (4 + 2 cos α) , 2 sin α)T .
Find
R
f([0,2π]×[0,2π]) h dA.
Answer:
|fθ × fα|
=
¯¯¯¯¯¯


−sin (θ) (4 + 2 cos α)
−cos (θ) (4 + 2 cos α)
0

×


−2 cos θ sin α
2 sin θ sin α
2 cos α


¯¯¯¯¯¯
=
¡
64 + 64 cos α + 16 cos2 α
¢1/2
and so the area element is
¡
64 + 64 cos α + 16 cos2 α
¢1/2 dθ dα.

500
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
Therefore, the desired integral is
Z 2π
0
Z 2π
0
(cos α)
¡
64 + 64 cos α + 16 cos2 α
¢1/2 dθ dα
=
Z 2π
0
Z 2π
0
(cos α) (8 + 4 cos α) dθ dα = 8π2
16. For (θ, α) ∈[0, 2π] × [0, 2π] , let
f (θ, α) ≡(cos θ (3 + cos α) , −sin θ (3 + cos α) , sin α)T .
Also let h (x) = cos2 θ where θ is such that
x = (cos θ (3 + cos α) , −sin θ (3 + cos α) , sin α)T .
Find
R
f([0,2π]×[0,2π]) h dV.
Answer:
The area element is
¡
9 + 6 cos α + cos2 α
¢1/2 dθ dα.
Therefore, the desired integral is
Z 2π
0
Z 2π
0
¡
cos2 θ
¢ ¡
9 + 6 cos α + cos2 α
¢1/2 dθ dα
=
Z 2π
0
Z 2π
0
¡
cos2 θ
¢
(3 + cos α) dθ dα = 6π2
17. For (θ, α) ∈[0, 25] × [0, 2π] , let
f (θ, α) ≡(cos θ (4 + 2 cos α) , −sin θ (4 + 2 cos α) , 2 sin α + θ)T .
Find a double integral which gives the area of f ([0, 25] × [0, 2π]) .
Answer:
In this case, the area element is
¡
68 + 64 cos α + 12 cos2 α
¢1/2 dθ dα
and so the surface area is
Z 2π
0
Z 2π
0
¡
68 + 64 cos α + 12 cos2 α
¢1/2 dθ dα.
18. For (θ, α) ∈[0, 2π] × [0, 2π] , and β a ﬁxed real number, deﬁne f (θ, α) ≡
(cos θ (2 + cos α) , −cos β sin θ (2 + cos α) + sin β sin α,
sin β sin θ (2 + cos α) + cos β sin α)T .
Find a double integral which gives the area of f ([0, 2π] × [0, 2π]) .
Answer:
After many computations, the area element is
¡
4 + 4 cos α + cos2 α
¢1/2 dθ dα. There-
fore, the area is
R 2π
0
R 2π
0
(2 + cos α) dθ dα = 8π2.

Part XII
Divergence Theorem
501


503
Outcomes
Flux Density and Divergence
A. Explain what is meant by the ﬂux density and divergence of a vector ﬁeld.
B. Evaluate the divergence of a vector ﬁeld.
C. Evaluate the Laplacian of a function.
D. Derive formulas involving divergence, gradient and Laplacian.
Reading: Multivariable Calculus 5.1
Outcome Mapping:
A. M1
B. 1
C. 2,3
D. 4
The Divergence Theorem
A. Recall and verify the Divergence Theorem.
B. Apply the Divergence Theorem to evaluate the ﬂux through a surface.
Reading: Multivariable Calculus 5.2
Outcome Mapping:
A. N1,N2,4,8
B. 1,2

504

The Divergence Theorem 29-30
Nov.
28.1
Divergence Of A Vector Field
Here the important concepts of divergence is deﬁned.
Deﬁnition 28.1.1 Let f : U →Rp for U ⊆Rp denote a vector ﬁeld. A scalar
valued function is called a scalar ﬁeld. The function, f is called a Ck vector ﬁeld if the
function, f is a Ck function. For a C1 vector ﬁeld, as just described ∇· f (x) ≡div f (x)
known as the divergence, is deﬁned as
∇· f (x) ≡div f (x) ≡
p
X
i=1
∂fi
∂xi
(x) .
Using the repeated summation convention, this is often written as
fi,i (x) ≡∂ifi (x)
where the comma indicates a partial derivative is being taken with respect to the ith variable
and ∂i denotes diﬀerentiation with respect to the ith variable. In words, the divergence is
the sum of the ith derivative of the ith component function of f for all values of i. Also
∇2f ≡∇· (∇f) .
This last symbol is important enough that it is given a name, the Laplacian.It is also
denoted by ∆. Thus ∇2f = ∆f. In addition for f a vector ﬁeld, the symbol f · ∇is deﬁned
as a “diﬀerential operator” in the following way.
f · ∇(g) ≡f1 (x) ∂g (x)
∂x1
+ f2 (x) ∂g (x)
∂x2
+ · · · + fp (x) ∂g (x)
∂xp
.
Thus f · ∇takes vector ﬁelds and makes them into new vector ﬁelds.
This deﬁnition is in terms of a given coordinate system but later a coordinate free def-
inition of div is presented. For now, everything is deﬁned in terms of a given Cartesian
coordinate system. The divergence has profound physical signiﬁcance and this will be dis-
cussed later. For now it is important to understand how to ﬁnd it. Be sure you understand
that for f a vector ﬁeld, div f is a scalar ﬁeld meaning it is a scalar valued function of three
variables. For a scalar ﬁeld, f, ∇f is a vector ﬁeld described earlier.
505

506
THE DIVERGENCE THEOREM 29-30 NOV.
Example 28.1.2 Let f (x) = xyi + (z −y) j + (sin (x) + z) k. Find div f
First the divergence of f is
∂(xy)
∂x
+ ∂(z −y)
∂y
+ ∂(sin (x) + z)
∂z
= y + (−1) + 1 = y.
28.2
The Divergence Theorem
Why does anyone care about the divergence of a vector ﬁeld? The answer is contained in
this section. In short, it is because of the divergence theorem which relates the ﬂux over the
boundary to a volume integral of the divergence. It is also called Gauss’s theorem.
Deﬁnition 28.2.1 A subset, V of R3 is called cylindrical in the x direction if it is
of the form
V = {(x, y, z) : φ (y, z) ≤x ≤ψ (y, z) for (y, z) ∈D}
where D is a subset of the yz plane. V is cylindrical in the z direction if
V = {(x, y, z) : φ (x, y) ≤z ≤ψ (x, y) for (x, y) ∈D}
where D is a subset of the xy plane, and V is cylindrical in the y direction if
V = {(x, y, z) : φ (x, z) ≤y ≤ψ (x, z) for (x, z) ∈D}
where D is a subset of the xz plane. If V is cylindrical in the z direction, denote by ∂V the
boundary of V deﬁned to be the points of the form (x, y, φ (x, y)) , (x, y, ψ (x, y)) for (x, y) ∈
D, along with points of the form (x, y, z) where (x, y) ∈∂D and φ (x, y) ≤z ≤ψ (x, y) .
Points on ∂D are deﬁned to be those for which every open ball contains points which are in
D as well as points which are not in D. A similar deﬁnition holds for ∂V in the case that
V is cylindrical in one of the other directions.
The following picture illustrates the above deﬁnition in the case of V cylindrical in the
z direction.
z = ψ(x, y)
z = φ(x, y)
¡
¡
¡
¡
x
z
y
Of course, many three dimensional sets are cylindrical in each of the coordinate direc-
tions. For example, a ball or a rectangle or a tetrahedron are all cylindrical in each direction.

28.2.
THE DIVERGENCE THEOREM
507
The following lemma allows the exchange of the volume integral of a partial derivative for
an area integral in which the derivative is replaced with multiplication by an appropriate
component of the unit exterior normal.
Lemma 28.2.2 Suppose V is cylindrical in the z direction and that φ and ψ are the
functions in the above deﬁnition. Assume φ and ψ are C1 functions and suppose F is a C1
function deﬁned on V. Also, let n = (nx, ny, nz) be the unit exterior normal to ∂V. Then
Z
V
∂F
∂z (x, y, z) dV =
Z
∂V
Fnz dA.
Proof: From the fundamental theorem of calculus,
Z
V
∂F
∂z (x, y, z) dV
=
Z
D
Z ψ(x,y)
φ(x,y)
∂F
∂z (x, y, z) dz dx dy
(28.1)
=
Z
D
[F (x, y, ψ (x, y)) −F (x, y, φ (x, y))] dx dy
Now the unit exterior normal on the top of V, the surface (x, y, ψ (x, y)) is
1
q
ψ2
x + ψ2
y + 1
¡
−ψx, −ψy, 1
¢
.
This follows from the observation that the top surface is the level surface, z −ψ (x, y) = 0
and so the gradient of this function of three variables is perpendicular to the level surface.
It points in the correct direction because the z component is positive. Therefore, on the top
surface,
nz =
1
q
ψ2
x + ψ2
y + 1
Similarly, the unit normal to the surface on the bottom is
1
q
φ2
x + φ2
y + 1
¡
φx, φy, −1
¢
and so on the bottom surface,
nz =
−1
q
φ2
x + φ2
y + 1
Note that here the z component is negative because since it is the outer normal it must
point down. On the lateral surface, the one where (x, y) ∈∂D and z ∈[φ (x, y) , ψ (x, y)] ,
nz = 0.
The area element on the top surface is dA =
q
ψ2
x + ψ2
y + 1 dx dy while the area element
on the bottom surface is
q
φ2
x + φ2
y + 1 dx dy. Therefore, the last expression in 28.1 is of the
form,
Z
D
F (x, y, ψ (x, y))
nz
z
}|
{
1
q
ψ2
x + ψ2
y + 1
dA
z
}|
{
q
ψ2
x + ψ2
y + 1 dx dy+
Z
D
F (x, y, φ (x, y))
nz
z
}|
{


−1
q
φ2
x + φ2
y + 1


dA
z
}|
{
q
φ2
x + φ2
y + 1 dx dy

508
THE DIVERGENCE THEOREM 29-30 NOV.
+
Z
Lateral surface
Fnz dA,
the last term equaling zero because on the lateral surface, nz = 0. Therefore, this reduces
to
R
∂V Fnz dA as claimed.
The following corollary is entirely similar to the above.
Corollary 28.2.3 If V is cylindrical in the y direction, then
Z
V
∂F
∂y dV =
Z
∂V
Fny dA
and if V is cylindrical in the x direction, then
Z
V
∂F
∂x dV =
Z
∂V
Fnx dA
With this corollary, here is a proof of the divergence theorem.
Theorem 28.2.4 Let V be cylindrical in each of the coordinate directions and let
F be a C1 vector ﬁeld deﬁned on V. Then
Z
V
∇· F dV =
Z
∂V
F · n dA.
Proof: From the above lemma and corollary,
Z
V
∇· F dV
=
Z
V
∂F1
∂x + ∂F2
∂y + ∂F3
∂y dV
=
Z
∂V
(F1nx + F2ny + F3nz) dA
=
Z
∂V
F · n dA.
This proves the theorem.
The divergence theorem holds for much more general regions than this. Suppose for
example you have a complicated region which is the union of ﬁnitely many disjoint regions
of the sort just described which are cylindrical in each of the coordinate directions. Then
the volume integral over the union of these would equal the sum of the integrals over the
disjoint regions. If the boundaries of two of these regions intersect, then the area integrals
will cancel out on the intersection because the unit exterior normals will point in opposite
directions. Therefore, the sum of the integrals over the boundaries of these disjoint regions
will reduce to an integral over the boundary of the union of these. Hence the divergence
theorem will continue to hold. For example, consider the following picture. If the divergence
theorem holds for each Vi in the following picture, then it holds for the union of these two.
V1
V2
General formulations of the divergence theorem involve Hausdorﬀmeasures and the
Lebesgue integral, a better integral than the old fashioned Riemann integral which has been
obsolete now for almost 100 years. When all is said and done, one ﬁnds that the conclusion
of the divergence theorem is usually true and it can be used with conﬁdence.

28.2.
THE DIVERGENCE THEOREM
509
Example 28.2.5 Let V = [0, 1] × [0, 1] × [0, 1] . That is, V is the cube in the ﬁrst octant
having the lower left corner at (0, 0, 0) and the sides of length 1. Let F (x, y, z) = xi+yj+zk.
Find the ﬂux integral in which n is the unit exterior normal.
Z
∂V
F · ndS
You can certainly inﬂict much suﬀering on yourself by breaking the surface up into 6
pieces corresponding to the 6 sides of the cube, ﬁnding a parameterization for each face and
adding up the appropriate ﬂux integrals. For example, n = k on the top face and n = −k
on the bottom face. On the top face, a parameterization is (x, y, 1) : (x, y) ∈[0, 1] × [0, 1] .
The area element is just dxdy. It isn’t really all that hard to do it this way but it is much
easier to use the divergence theorem. The above integral equals
Z
V
div (F) dV =
Z
V
3dV = 3.
Example 28.2.6 This time, let V be the unit ball,
©
(x, y, z) : x2 + y2 + z2 ≤1
ª
and let
F (x, y, z) = x2i + yj+ (z −1) k. Find
Z
∂V
F · ndS.
As in the above you could do this by brute force.
A parameterization of the ∂V is
obtained as
x = sin φ cos θ, y = sin φ sin θ, z = cos φ
where (φ, θ) ∈(0, π) × (0, 2π]. Now this does not include all the ball but it includes all but
the point at the top and at the bottom. As far as the ﬂux integral is concerned these points
contribute nothing to the integral so you can neglect them. Then you can grind away and
get the ﬂux integral which is desired. However, it is so much easier to use the divergence
theorem! Using spherical coordinates,
Z
∂V
F · ndS
=
Z
V
div (F) dV =
Z
V
(2x + 1 + 1) dV
=
Z π
0
Z 2π
0
Z 1
0
(2 + 2ρ sin (φ) cos θ) ρ2 sin (φ) dρdθdφ = 8
3π
Example 28.2.7 Suppose V is an open set in R3 for which the divergence theorem holds.
Let F (x, y, z) = xi + yj + zk. Then show
Z
∂V
F · ndS = 3 × volume(V ).
This follows from the divergenc theorem.
Z
∂V
F · ndS =
Z
V
div (F) dV = 3
Z
V
dV = 3 × volume(V ).
The message of the divergence theorem is the relation between the volume integral and
an area integral. This is the exciting thing about this marvelous theorem. It is not its utility
as a method for evaluations of boring problems. This will be shown in the examples of its
use which follow.

510
THE DIVERGENCE THEOREM 29-30 NOV.
28.2.1
Coordinate Free Concept Of Divergence, Flux Density
The divergence theorem also makes possible a coordinate free deﬁnition of the divergence.
Theorem 28.2.8 Let B (x, δ) be the ball centered at x having radius δ and let F
be a C1 vector ﬁeld. Then letting v (B (x, δ)) denote the volume of B (x, δ) given by
Z
B(x,δ)
dV,
it follows
div F (x) = lim
δ→0+
1
v (B (x, δ))
Z
∂B(x,δ)
F · n dA.
(28.2)
Proof: The divergence theorem holds for balls because they are cylindrical in every
direction. Therefore,
1
v (B (x, δ))
Z
∂B(x,δ)
F · n dA =
1
v (B (x, δ))
Z
B(x,δ)
div F (y) dV.
Therefore, since div F (x) is a constant,
¯¯¯¯¯div F (x) −
1
v (B (x, δ))
Z
∂B(x,δ)
F · n dA
¯¯¯¯¯
=
¯¯¯¯¯div F (x) −
1
v (B (x, δ))
Z
B(x,δ)
div F (y) dV
¯¯¯¯¯
=
¯¯¯¯¯
1
v (B (x, δ))
Z
B(x,δ)
(div F (x) −div F (y)) dV
¯¯¯¯¯
≤
1
v (B (x, δ))
Z
B(x,δ)
|div F (x) −div F (y)| dV
≤
1
v (B (x, δ))
Z
B(x,δ)
ε
2 dV < ε
whenever ε is small enough due to the continuity of div F. Since ε is arbitrary, this shows
28.2.
How is this deﬁnition independent of coordinates? It only involves geometrical notions
of volume and dot product. This is why. Imagine rotating the coordinate axes, keeping
all distances the same and expressing everything in terms of the new coordinates.
The
divergence would still have the same value because of this theorem.
You also see the physical signiﬁcance of the divergence from this.
It measures the
tendency of the vector ﬁeld to “diverge” from a point.
28.3
The Weak Maximum Principle∗
There is a fundamental result having great signiﬁcance which involves ∇2 called the max-
imum principle. This principle says that if ∇2u ≥0 on a bounded open set, U, then u
achieves its maximum value on the boundary of U. It is a very important result which ties
in many earlier topics. Don’t read it if you are not interested.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREM∗
511
Theorem 28.3.1 Let U be a bounded open set in Rn and suppose u ∈C2 (U) ∩
C
¡
U
¢
such that ∇2u ≥0 in U. Then letting ∂U = U\U, it follows that max
©
u (x) : x ∈U
ª
=
max {u (x) : x ∈∂U} .
Proof: If this is not so, there exists x0 ∈U such that u (x0) > max {u (x) : x ∈∂U} ≡
M. Since U is bounded, there exists ε > 0 such that
u (x0) > max
n
u (x) + ε |x|2 : x ∈∂U
o
.
Therefore, u (x) + ε |x|2 also has its maximum in U because for ε small enough,
u (x0) + ε |x0|2 > u (x0) > max
n
u (x) + ε |x|2 : x ∈∂U
o
for all x ∈∂U.
Now let x1 be the point in U at which u (x) + ε |x|2 achieves its maximum.
As an
exercise you should show that ∇2 (f + g) = ∇2f +∇2g and therefore, ∇2 ³
u (x) + ε |x|2´
=
∇2u (x) + 2nε. (Why?) Therefore,
0 ≥∇2u (x1) + 2nε ≥2nε,
a contradiction. This proves the theorem.
28.4
Some Applications Of The Divergence Theorem∗
There are numerous applications of the divergence theorem. Some are listed here. You
might want to read this if you are interested in applications. However, it won’t be needed
for tests.
28.4.1
Hydrostatic Pressure∗
Imagine a ﬂuid which does not move which is acted on by an acceleration, g. Of course the
acceleration is usually the acceleration of gravity. Also let the density of the ﬂuid be ρ, a
function of position. What can be said about the pressure, p, in the ﬂuid? Let B (x, ε) be a
small ball centered at the point, x. Then the force the ﬂuid exerts on this ball would equal
−
Z
∂B(x,ε)
pn dA.
Here n is the unit exterior normal at a small piece of ∂B (x, ε) having area dA. By the
divergence theorem, this integral equals
−
Z
B(x,ε)
∇p dV.
Also the force acting on this small ball of ﬂuid is
Z
B(x,ε)
ρg dV.
Since it is given that the ﬂuid does not move, the sum of these forces must equal zero. Thus
Z
B(x,ε)
ρg dV =
Z
B(x,ε)
∇p dV.

512
THE DIVERGENCE THEOREM 29-30 NOV.
Since this must hold for any ball in the ﬂuid of any radius, it must be that
∇p = ρg.
(28.3)
It turns out that the pressure in a lake at depth z is equal to 62.5z. This is easy to see
from 28.3. In this case, g = gk where g = 32 feet/sec2. The weight of a cubic foot of water
is 62.5 pounds. Therefore, the mass in slugs of this water is 62.5/32. Since it is a cubic foot,
this is also the density of the water in slugs per cubic foot. Also, it is normally assumed
that water is incompressible1. Therefore, this is the mass of water at any depth. Therefore,
∂p
∂xi+∂p
∂y j+∂p
∂z k =62.5
32 × 32k.
and so p does not depend on x and y and is only a function of z. It follows p (0) = 0, and
p′ (z) = 62.5. Therefore, p (x, y, z) = 62.5z. This establishes the claim. This is interesting
but 28.3 is more interesting because it does not require ρ to be constant.
28.4.2
Archimedes Law Of Buoyancy∗
Archimedes principle states that when a solid body is immersed in a ﬂuid the net force acting
on the body by the ﬂuid is directly up and equals the total weight of the ﬂuid displaced.
Denote the set of points in three dimensions occupied by the body as V. Then for dA
an increment of area on the surface of this body, the force acting on this increment of area
would equal −p dAn where n is the exterior unit normal. Therefore, since the ﬂuid does not
move,
Z
∂V
−pn dA =
Z
V
−∇p dV =
Z
V
ρg dV k
Which equals the total weight of the displaced ﬂuid and you note the force is directed upward
as claimed. Here ρ is the density and 28.3 is being used. There is an interesting point in the
above explanation. Why does the second equation hold? Imagine that V were ﬁlled with
ﬂuid. Then the equation follows from 28.3 because in this equation g = −gk.
28.4.3
Equations Of Heat And Diﬀusion∗
Let x be a point in three dimensional space and let (x1, x2, x3) be Cartesian coordinates of
this point. Let there be a three dimensional body having density, ρ = ρ (x, t).
The heat ﬂux, J, in the body is deﬁned as a vector which has the following property.
Rate at which heat crosses S =
Z
S
J · n dA
where n is the unit normal in the desired direction. Thus if V is a three dimensional body,
Rate at which heat leaves V =
Z
∂V
J · n dA
where n is the unit exterior normal.
Fourier’s law of heat conduction states that the heat ﬂux, J satisﬁes J = −k∇(u) where
u is the temperature and k = k (u, x, t) is called the coeﬃcient of thermal conductivity.
This changes depending on the material. It also can be shown by experiment to change
1There is no such thing as an incompressible ﬂuid but this doesn’t stop people from making this assump-
tion.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREM∗
513
with temperature. This equation for the heat ﬂux states that the heat ﬂows from hot places
toward colder places in the direction of greatest rate of decrease in temperature. Let c (x, t)
denote the speciﬁc heat of the material in the body. This means the amount of heat within
V is given by the formula
R
V ρ (x, t) c (x, t) u (x, t) dV. Suppose also there are sources for the
heat within the material given by f (x,u, t) . If f is positive, the heat is increasing while if
f is negative the heat is decreasing. For example such sources could result from a chemical
reaction taking place. Then the divergence theorem can be used to verify the following
equation for u. Such an equation is called a reaction diﬀusion equation.
∂
∂t (ρ (x, t) c (x, t) u (x, t)) = ∇· (k (u, x, t) ∇u (x, t)) + f (x, u, t) .
(28.4)
Take an arbitrary V for which the divergence theorem holds. Then the time rate of
change of the heat in V is
d
dt
Z
V
ρ (x, t) c (x, t) u (x, t) dV =
Z
V
∂(ρ (x, t) c (x, t) u (x, t))
∂t
dV
where, as in the preceding example, this is a physical derivation so the consideration of
hard mathematics is not necessary. Therefore, from the Fourier law of heat conduction,
d
dt
R
V ρ (x, t) c (x, t) u (x, t) dV =
Z
V
∂(ρ (x, t) c (x, t) u (x, t))
∂t
dV =
rate at which heat enters
z
}|
{
Z
∂V
−J · n dA
+
Z
V
f (x, u, t) dV
=
Z
∂V
k∇(u) · n dA +
Z
V
f (x, u, t) dV =
Z
V
(∇· (k∇(u)) + f) dV.
Since this holds for every sample volume, V it must be the case that the above reaction
diﬀusion equation, 28.4 holds. Note that more interesting equations can be obtained by
letting more of the quantities in the equation depend on temperature. However, the above
is a fairly hard equation and people usually assume the coeﬃcient of thermal conductivity
depends only on x and that the reaction term, f depends only on x and t and that ρ and c
are constant. Then it reduces to the much easier equation,
∂
∂tu (x, t) = 1
ρc∇· (k (x) ∇u (x, t)) + f (x,t) .
(28.5)
This is often referred to as the heat equation. Sometimes there are modiﬁcations of this
in which k is not just a scalar but a matrix to account for diﬀerent heat ﬂow properties
in diﬀerent directions. However, they are not much harder than the above. The major
mathematical diﬃculties result from allowing k to depend on temperature.
It is known that the heat equation is not correct even if the thermal conductivity did
not depend on u because it implies inﬁnite speed of propagation of heat. However, this does
not prevent people from using it.
28.4.4
Balance Of Mass∗
Let y be a point in three dimensional space and let (y1, y2, y3) be Cartesian coordinates of
this point. Let V be a region in three dimensional space and suppose a ﬂuid having density,
ρ (y, t) and velocity, v (y,t) is ﬂowing through this region. Then the mass of ﬂuid leaving V
per unit time is given by the area integral,
R
∂V ρ (y, t) v (y, t) · n dA while the total mass of
the ﬂuid enclosed in V at a given time is
R
V ρ (y, t) dV. Also suppose mass originates at the

514
THE DIVERGENCE THEOREM 29-30 NOV.
rate f (y, t) per cubic unit per unit time within this ﬂuid. Then the conclusion which can
be drawn through the use of the divergence theorem is the following fundamental equation
known as the mass balance equation.
∂ρ
∂t + ∇· (ρv) = f (y, t)
(28.6)
To see this is so, take an arbitrary V for which the divergence theorem holds. Then the
time rate of change of the mass in V is
∂
∂t
Z
V
ρ (y, t) dV =
Z
V
∂ρ (y, t)
∂t
dV
where the derivative was taken under the integral sign with respect to t. (This is a physical
derivation and therefore, it is not necessary to fuss with the hard mathematics related to
the change of limit operations.
You should expect this to be true under fairly general
conditions because the integral is a sort of sum and the derivative of a sum is the sum of
the derivatives.) Therefore, the rate of change of mass,
∂
∂t
R
V ρ (y, t) dV, equals
Z
V
∂ρ (y, t)
∂t
dV
=
rate at which mass enters
z
}|
{
−
Z
∂V
ρ (y, t) v (y, t) · n dA +
Z
V
f (y, t) dV
=
−
Z
V
(∇· (ρ (y, t) v (y, t)) + f (y, t)) dV.
Since this holds for every sample volume, V it must be the case that the equation of
continuity holds. Again, there are interesting mathematical questions here which can be
explored but since it is a physical derivation, it is not necessary to dwell too much on them.
If all the functions involved are continuous, it is certainly true but it is true under far more
general conditions than that.
Also note this equation applies to many situations and f might depend on more than
just y and t. In particular, f might depend also on temperature and the density, ρ. This
would be the case for example if you were considering the mass of some chemical and f
represented a chemical reaction. Mass balance is a general sort of equation valid in many
contexts.
28.4.5
Balance Of Momentum∗
This example is a little more substantial than the above. It concerns the balance of mo-
mentum for a continuum. To see a full description of all the physics involved, you should
consult a book on continuum mechanics. One of the most elegant, possibly the most elegant
is the book by Gurtin [13]. To read this book, you will need to know what the derivative of
a function of many variables is. This is also the case in this section.
The situation is of a material in three dimensions and it deforms and moves about in
three dimensions. This means this material is not a rigid body. Let B0 denote an open set
identifying a chunk of this material at time t = 0 and let Bt be an open set which identiﬁes
the same chunk of material at time t > 0.
Let y (t, x) = (y1 (t, x) , y2 (t, x) , y3 (t, x)) denote the position with respect to Cartesian
coordinates at time t of the point whose position at time t = 0 is x = (x1, x2, x3) . The
coordinates, x are sometimes called the reference coordinates and sometimes the material
coordinates and sometimes the Lagrangian coordinates. The coordinates, y are called the

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREM∗
515
Eulerian coordinates or sometimes the spacial coordinates and the function, (t, x) →y (t, x)
is called the motion2. Thus
y (0, x) = x.
(28.7)
The derivative,
D2y (t, x)
is called the deformation gradient. Recall the notation means you ﬁx t and consider the func-
tion, x →y (t, x) , taking its derivative. Since it is a linear transformation, it is represented
by the usual matrix, whose ijth entry is given by
Fij (x) = ∂yi (t, x)
∂xj
.
Let ρ (t, y) denote the density of the material at time t at the point, y and let ρ0 (x) denote
the density of the material at the point, x. Thus ρ0 (x) = ρ (0, x) = ρ (0, y (0, x)) . The ﬁrst
task is to consider the relationship between ρ (t, y) and ρ0 (x) .
Lemma 28.4.1 ρ0 (x) = ρ (t, y (t, x)) det (F) and in any reasonable physical motion,
det (F) > 0.
Proof: Let V0 represent a small chunk of material at t = 0 and let Vt represent the same
chunk of material at time t. I will be a little sloppy and refer to V0 as the small chunk of
material at time t = 0 and Vt as the chunk of material at time t rather than an open set
representing the chunk of material. Then by the change of variables formula for multiple
integrals,
Z
Vt
dV =
Z
V0
|det (F)| dV.
If det (F) = 0 for some t the above formula shows that the chunk of material went from
positive volume to zero volume and this is not physically possible. Therefore, it is impossible
that det (F) can equal zero.
However, at t = 0, F = I, the identity because of 28.7.
Therefore, det (F) = 1 at t = 0 and if it is assumed t →det (F) is continuous it follows
by the intermediate value theorem that det (F) > 0 for all t. Of course it is not known for
sure this function is continuous but the above shows why it is at least reasonable to expect
det (F) > 0.
Now using the change of variables formula,
mass of Vt
=
Z
Vt
ρ (t, y) dV =
Z
V0
ρ (t, y (t, x)) det (F) dV
=
mass of V0 =
Z
V0
ρ0 (x) dV.
Since V0 is arbitrary, it follows ρ0 (x) = ρ (t, y (t, x)) det (F) as claimed. Note this shows
that det (F) is a magniﬁcation factor for the density.
Now consider a small chunk of material, Bt at time t which corresponds to B0 at time
t = 0. The total linear momentum of this material at time t is
Z
Bt
ρ (t, y) v (t, y) dV
where v is the velocity. By Newton’s second law, the time rate of change of this linear
momentum should equal the total force acting on the chunk of material. In the following
2Apparently, the terminology is all mixed up in so far as the names Euler and Lagrange are concerned.

516
THE DIVERGENCE THEOREM 29-30 NOV.
derivation, dV (y) will indicate the integration is taking place with respect to the variable,
y. By Lemma 28.4.1 and the change of variables formula for multiple integrals
d
dt
µZ
Bt
ρ (t, y) v (t, y) dV (y)
¶
=
d
dt
µZ
B0
ρ (t, y (t, x)) v (t, y (t, x)) det (F) dV (x)
¶
=
d
dt
µZ
B0
ρ0 (x) v (t, y (t, x)) dV (x)
¶
=
Z
B0
ρ0 (x)
·∂v
∂t + ∂v
∂yi
∂yi
∂t
¸
dV (x)
=
Z
Bt
ρ (t, y) det (F)
·∂v
∂t + ∂v
∂yi
∂yi
∂t
¸
1
det (F) dV (y)
=
Z
Bt
ρ (t, y)
·∂v
∂t + ∂v
∂yi
∂yi
∂t
¸
dV (y) .
This uses the repeated index summation convention. Having taken the derivative of the
total momentum, it is time to consider the total force acting on the chunk of material.
The force comes from two sources, a body force, b and a force which act on the boundary
of the chunk of material called a traction force. Typically, the body force is something like
gravity in which case, b = −gρk, assuming the Cartesian coordinate system has been chosen
in the usual manner. It could also be centrifugal force which might result if the body were
undergoing some sort of rigid motion. The traction force is of the form
Z
∂Bt
s (t, y, n) dA
where n is the unit exterior normal. Thus the traction force depends on position, time, and
the orientation of the boundary of Bt. Cauchy showed the existence of a linear transforma-
tion, T (t, y) such that T (t, y) n = s (t, y, n) . It follows there is a matrix, Tij (t, y) such that
the ith component of s is given by si (t, y, n) = Tij (t, y) nj. Cauchy also showed this matrix
is symmetric, Tij = Tji. (This comes from conservation of angular momentum.) It is called
the Cauchy stress. Using Newton’s second law to equate the time derivative of the total
linear momentum with the applied forces and using the usual repeated index summation
convention,
Z
Bt
ρ (t, y)
·∂v
∂t + ∂v
∂yi
∂yi
∂t
¸
dV (y) =
Z
Bt
b (t, y) dV (y) +
Z
∂Bt
Tij (t, y) nj dA.
Here is where the divergence theorem is used. In the last integral, the multiplication by nj
is exchanged for the jth partial derivative and an integral over Bt. Thus
Z
Bt
ρ (t, y)
·∂v
∂t + ∂v
∂yi
∂yi
∂t
¸
dV (y) =
Z
Bt
b (t, y) dV (y) +
Z
Bt
∂(Tij (t, y))
∂yj
dV (y) .
Since Bt was arbitrary, it follows
ρ (t, y)
·∂v
∂t + ∂v
∂yi
∂yi
∂t
¸
=
b (t, y) + ∂(Tij (t, y))
∂yj
≡
b (t, y) + div (T)
where here div T is a vector whose ith component is given by
(div T)i = ∂Tij
∂yj
.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREM∗
517
The term, ∂v
∂t + ∂v
∂yi
∂yi
∂t , is the total derivative with respect to t of the velocity v, written as
˙v. Thus you might see this written as
ρ ˙v = b + div (T) .
The above formulation of the balance of momentum involves the spatial coordinates, y
but people also like to formulate momentum balance in terms of the material coordinates, x.
The spacial coordinates are ﬁne if you are looking for example at the ﬂow of a ﬂuid through
some ﬁxed region in space. However, if you are interested in the deformation of a material,
then you might want to consider things like traction boundary conditions. To make sense
of these, you should be dealing with the reference or material coordinates. Of course this
changes everything.
The momentum in terms of the material coordinates is
Z
B0
ρ0 (x) v (t, x) dV
and so, since x does not depend on t,
d
dt
µZ
B0
ρ0 (x) v (t, x) dV
¶
=
Z
B0
ρ0 (x) vt (t, x) dV.
As indicated earlier, this is a physical derivation and so the mathematical questions related
to interchange of limit operations are ignored. This must equal the total applied force. Thus
Z
B0
ρ0 (x) vt (t, x) dV =
Z
B0
b0 (t, x) dV +
Z
∂Bt
TijnjdA,
(28.8)
the ﬁrst term on the right being the contribution of the body force given per unit volume
in the material coordinates and the last term being the traction force discussed earlier. The
task is to write this last integral as one over ∂B0. For y ∈∂Bt there is a unit outer normal,
n. Here y = y (t, x) for x ∈∂B0. Then deﬁne N to be the unit outer normal to B0 at the
point, x. Near the point y ∈∂Bt the surface, ∂Bt is given parametrically in the form
y = y (s, t) for (s, t) ∈D ⊆R2 and it can be assumed the unit normal to ∂Bt near this
point is
n = ys (s, t) × yt (s, t)
|ys (s, t) × yt (s, t)|
with the area element given by |ys (s, t) × yt (s, t)| ds dt. This is true for y ∈Pt ⊆∂Bt, a
small piece of ∂Bt. Therefore, the last integral in 28.8 is the sum of integrals over small
pieces of the form
Z
Pt
TijnjdA
(28.9)
where Pt is parametrized by y (s, t) , (s, t) ∈D. Thus the integral in 28.9 is of the form
Z
D
Tij (y (s, t)) (ys (s, t) × yt (s, t))j ds dt.
Using the repeated index summation convention, and the chain rule, this equals
Z
D
Tij (y (s, t))
µ ∂y
∂xα
∂xα
∂s × ∂y
∂xβ
∂xβ
∂t
¶
j
ds dt.

518
THE DIVERGENCE THEOREM 29-30 NOV.
Remember y = y (t, x) and it is always assumed the mapping x →y (t, x) is one to one and
so, since on the surface ∂Bt near y, the points are functions of (s, t) , it follows x is also a
function of (s, t) . Now by the properties of the cross product, this last integral equals
Z
D
Tij (x (s, t)) ∂xα
∂s
∂xβ
∂t
µ ∂y
∂xα
× ∂y
∂xβ
¶
j
ds dt
(28.10)
where here x (s, t) is the point of ∂B0 which corresponds with y (s, t) ∈∂Bt.
Thus
Tij (x (s, t)) = Tij (y (s, t)) . (Perhaps this is a slight abuse of notation because Tij is deﬁned
on ∂Bt, not on ∂B0, but it avoids introducing extra symbols.) Next 28.10 equals
Z
D
Tij (x (s, t)) ∂xα
∂s
∂xβ
∂t εjab
∂ya
∂xα
∂yb
∂xβ
ds dt
=
Z
D
Tij (x (s, t)) ∂xα
∂s
∂xβ
∂t εcabδjc
∂ya
∂xα
∂yb
∂xβ
ds dt
=
Z
D
Tij (x (s, t)) ∂xα
∂s
∂xβ
∂t εcab
=δjc
z
}|
{
∂yc
∂xp
∂xp
∂yj
∂ya
∂xα
∂yb
∂xβ
ds dt
=
Z
D
Tij (x (s, t)) ∂xα
∂s
∂xβ
∂t
∂xp
∂yj
=εpαβ det(F )
z
}|
{
εcab
∂yc
∂xp
∂ya
∂xα
∂yb
∂xβ
ds dt
=
Z
D
(det F) Tij (x (s, t)) εpαβ
∂xα
∂s
∂xβ
∂t
∂xp
∂yj
ds dt.
Now ∂xp
∂yj = F −1
pj
and also
εpαβ
∂xα
∂s
∂xβ
∂t = (xs × xt)p
so the result just obtained is of the form
Z
D
(det F) F −1
pj Tij (x (s, t)) (xs × xt)p ds dt =
Z
D
(det F) Tij (x (s, t))
¡
F −T ¢
jp (xs × xt)p ds dt.
This has transformed the integral over Pt to one over P0, the part of ∂B0 which corresponds
with Pt. Thus the last integral is of the form
Z
P0
det (F)
¡
TF −T ¢
ip NpdA
Summing these up over the pieces of ∂Bt and ∂B0 yields the last integral in 28.8 equals
Z
∂B0
det (F)
¡
TF −T ¢
ip NpdA
and so the balance of momentum in terms of the material coordinates becomes
Z
B0
ρ0 (x) vt (t, x) dV =
Z
B0
b0 (t, x) dV +
Z
∂B0
det (F)
¡
TF −T ¢
ip NpdA

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREM∗
519
The matrix, det (F)
¡
TF −T ¢
ip is called the ﬁrst Piola Kirchhoﬀstress, S. An application
of the divergence theorem yields
Z
B0
ρ0 (x) vt (t, x) dV =
Z
B0
b0 (t, x) dV +
Z
B0
∂
³
det (F)
¡
TF −T ¢
ip
´
∂xp
dV.
Since B0 is arbitrary, a balance law for momentum in terms of the material coordinates is
obtained
ρ0 (x) vt (t, x)
=
b0 (t, x) +
∂
³
det (F)
¡
TF −T ¢
ip
´
∂xp
=
b0 (t, x) + div
¡
det (F)
¡
TF −T ¢¢
=
b0 (t, x) + div S.
(28.11)
The main purpose of this presentation is to show how the divergence theorem is used
in a signiﬁcant way to obtain balance laws and to indicate a very interesting direction for
further study. To continue, one needs to specify T or S as an appropriate function of things
related to the motion, y. Often the thing related to the motion is something called the
strain and such relationships between the stress and the strain are known as constitutive
laws.
The proper formulation of constitutive laws involves more physical considerations
such as frame indiﬀerence in which it is required the response of the system cannot depend
on the manner in which the Cartesian coordinate system was chosen. There are also many
other physical properties which can be included and which require a certain form for the
constitutive equations. These considerations are outside the scope of this book and require
a considerable amount of linear algebra.
There are also balance laws for energy which you may study later but these are more
problematic than the balance laws for mass and momentum. However, the divergence the-
orem is used in these also.
28.4.6
Bernoulli’s Principle∗
Consider a possibly moving ﬂuid with constant density, ρ and let P denote the pressure
in this ﬂuid. If B is a part of this ﬂuid the force exerted on B by the rest of the ﬂuid is
R
∂B −PndA where n is the outer normal from B. Assume this is the only force which matters
so for example there is no viscosity in the ﬂuid. Thus the Cauchy stress in rectangular
coordinates should be
T =


−P
0
0
0
−P
0
0
0
−P

.
Then
div T = −∇P.
Also suppose the only body force is from gravity, a force of the form
−ρgk
and so from the balance of momentum
ρ ˙v = −ρgk −∇P (x) .
(28.12)
Now in all this the coordinates are the spacial coordinates and it is assumed they are
rectangular. Thus
x = (x, y, z)T

520
THE DIVERGENCE THEOREM 29-30 NOV.
and v is the velocity while ˙v is the total derivative of v = (v1, v2, v3)T given by vt + viv,i.
Take the dot product of both sides of 28.12 with v. This yields
(ρ/2) d
dt |v|2 = −ρg dz
dt −d
dtP (x) .
Therefore,
d
dt
Ã
ρ |v|2
2
+ ρgz + P (x)
!
= 0
and so there is a constant, C′ such that
ρ |v|2
2
+ ρgz + P (x) = C′
For convenience deﬁne γ to be the weight density of this ﬂuid. Thus γ = ρg. Divide by γ.
Then
|v|2
2g + z + P (x)
γ
= C.
this is Bernoulli’s3 principle. Note how if you keep the height the same, then if you raise
|v| , it follows the pressure drops.
This is often used to explain the lift of an airplane wing. The top surface is curved which
forces the air to go faster over the top of the wing causing a drop in pressure which creates
lift. It is also used to explain the concept of a venturi tube in which the air loses pressure
due to being pinched which causes it to ﬂow faster. In many of these applications, the
assumptions used in which ρ is constant and there is no other contribution to the traction
force on ∂B than pressure so in particular, there is no viscosity, are not correct. However, it
is hoped that the eﬀects of these deviations from the ideal situation above are small enough
that the conclusions are still roughly true. You can see how using balance of momentum
can be used to consider more diﬃcult situations. For example, you might have a body force
which is more involved than gravity.
28.4.7
The Wave Equation∗
As an example of how the balance law of momentum is used to obtain an important equation
of mathematical physics, suppose S = kF where k is a constant and F is the deformation
gradient and let u ≡y −x. Thus u is the displacement. Then from 28.11 you can verify
the following holds.
ρ0 (x) utt (t, x) = b0 (t, x) + k∆u (t, x)
(28.13)
In the case where ρ0 is a constant and b0 = 0, this yields
utt −c∆u = 0.
The wave equation is utt −c∆u = 0 and so the above gives three wave equations, one for
each component.
3There were many Bernoullis. This is Daniel Bernoulli. He seems to have been nicer than some of the
others. Daniel was actually a doctor who was interested in mathematics.He lived from 1700-1782.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREM∗
521
28.4.8
A Negative Observation∗
Many of the above applications of the divergence theorem are based on the assumption that
matter is continuously distributed in a way that the above arguments are correct. In other
words, a continuum. However, there is no such thing as a continuum. It has been known
for some time now that matter is composed of atoms. It is not continuously distributed
through some region of space as it is in the above. Apologists for this contradiction with
reality sometimes say to consider enough of the material in question that it is reasonable to
think of it as a continuum. This mystical reasoning is then violated as soon as they go from
the integral form of the balance laws to the diﬀerential equations expressing the traditional
formulation of these laws. However, these laws continue to be used and seem to lead to
useful physical models which have value in predicting the behavior of physical systems.
This is what justiﬁes their use, not any fundamental truth. The possibility exists that the
reason for this is the numerical methods used to solve the partial diﬀerential equations may
be better physical models than the ballance laws themselves. It is an area where people still
sometimes disagree.
28.4.9
Electrostatics∗
Coloumb’s law says that the electric ﬁeld intensity at x of a charge q located at point, x0 is
given by
E = k q (x −x0)
|x −x0|3
where the electric ﬁeld intensity is deﬁned to be the force experienced by a unit positive
charge placed at the point, x. Note that this is a vector and that its direction depends on
the sign of q. It points away from x0 if q is positive and points toward x0 if q is negative.
The constant, k is a physical constant like the gravitation constant. It has been computed
through careful experiments similar to those used with the calculation of the gravitation
constant.
The interesting thing about Coloumb’s law is that E is the gradient of a function. In
fact,
E = ∇
µ
qk
1
|x −x0|
¶
.
The other thing which is signiﬁcant about this is that in three dimensions and for x ̸= x0,
∇· ∇
µ
qk
1
|x −x0|
¶
= ∇· E = 0.
(28.14)
This is left as an exercise for you to verify.
These observations will be used to derive a very important formula for the integral,
Z
∂U
E · ndS
where E is the electric ﬁeld intensity due to a charge, q located at the point, x0 ∈U, a
bounded open set for which the divergence theorem holds.
Let Uε denote the open set obtained by removing the open ball centered at x0 which
has radius ε where ε is small enough that the following picture is a correct representation
of the situation.

522
THE DIVERGENCE THEOREM 29-30 NOV.
¡¡

q x0
ε
Uε
B(x0, ε) = Bϵ
Then on the boundary of Bε the unit outer normal to Uε is −x−x0
|x−x0|. Therefore,
Z
∂Bε
E · ndS
=
−
Z
∂Bε
k q (x −x0)
|x −x0|3 · x −x0
|x −x0|dS
=
−kq
Z
∂Bε
1
|x −x0|2 dS = −kq
ε2
Z
∂Bε
dS
=
−kq
ε2 4πε2 = −4πkq.
Therefore, from the divergence theorem and observation 28.14,
−4πkq +
Z
∂U
E · ndS =
Z
∂Uε
E · ndS =
Z
Uε
∇· EdV = 0.
It follows that
4πkq =
Z
∂U
E · ndS.
If there are several charges located inside U, say q1, q2, · · ·, qn, then letting Ei denote the
electric ﬁeld intensity of the ith charge and E denoting the total resulting electric ﬁeld
intensity due to all these charges,
Z
∂U
E · ndS
=
n
X
i=1
Z
∂U
Ei · ndS
=
n
X
i=1
4πkqi = 4πk
n
X
i=1
qi.
This is known as Gauss’s law and it is the fundamental result in electrostatics.

Part XIII
Stoke’s Theorem
523


525
Outcomes
Circulation Density and Curl
A. Explain what is meant by the circulation density and curl of a vector ﬁeld.
B. Evaluate the curl of a vector ﬁeld
C. Derive and apply formulas involving divergence, gradient and curl.
Reading: Multivariable Calculus 5.3
Outcome Mapping:
A. O1,3
B. 1,4,6
C. 2
Stoke’s Theorem
A. Recall and verify Stoke’s theorem.
B. Apply Stoke’s theorem to calculate the circulation (or work) of a vector ﬁeld around
a simple closed curve.
C. Recall and apply the divergence and curl tests.
Reading: Multivariable Calculus 5.4
Outcome Mapping:
A. P1,1,12
B. 2,3,9
C. P2,4,5,10

526

Stoke’s Theorem 4-5 Dec.
29.1
Curl Of A Vector Field
Here the important concepts of curl is deﬁned.
Deﬁnition 29.1.1 Let f : U →R3 for U ⊆R3 denote a vector ﬁeld. The curl of
the vector ﬁeld yields another vector ﬁeld and it is deﬁned as follows.
(curl (f) (x))i ≡(∇× f (x))i ≡εijk∂jfk (x)
where here ∂j means the partial derivative with respect to xj and the subscript of i in
(curl (f) (x))i means the ith Cartesian component of the vector, curl (f) (x) . Thus the curl
is evaluated by expanding the following determinant along the top row.
¯¯¯¯¯¯
i
j
k
∂
∂x
∂
∂y
∂
∂z
f1 (x, y, z)
f2 (x, y, z)
f3 (x, y, z)
¯¯¯¯¯¯
.
Note the similarity with the cross product. Sometimes the curl is called rot. (Short for
rotation not decay.)
This deﬁnition is in terms of a given coordinate system but later coordinate free deﬁni-
tions of the curl is presented. For now, everything is deﬁned in terms of a given Cartesian
coordinate system. The curl has profound physical signiﬁcance and this will be discussed
later. For now it is important to understand how to ﬁnd it. Be sure you understand that
for f a vector ﬁeld, curl f is another vector ﬁeld.
Example 29.1.2 Let f (x) = xyi + (z −y) j + (sin (x) + z) k. Find curl f.
curl f is obtained by evaluating
¯¯¯¯¯¯
i
j
k
∂
∂x
∂
∂y
∂
∂z
xy
z −y
sin (x) + z
¯¯¯¯¯¯
=
i
µ ∂
∂y (sin (x) + z) −∂
∂z (z −y)
¶
−j
µ ∂
∂x (sin (x) + z) −∂
∂z (xy)
¶
+
k
µ ∂
∂x (z −y) −∂
∂y (xy)
¶
= −i −cos (x) j −xk.
527

528
STOKE’S THEOREM 4-5 DEC.
29.2
Green’s Theorem, A Review
Theorem 29.2.1 (Green’s Theorem) Let U be an open set in the plane and let ∂U
be piecewise smooth and let F (x, y) = (P (x, y) , Q (x, y)) be a C1 vector ﬁeld deﬁned near
U. Then it is often1 the case that
Z
∂U
F · dR =
Z
U
µ∂Q
∂x (x, y) −∂P
∂y (x, y)
¶
dA.
Proof: Suppose the divergence theorem holds for U. Consider the following picture.
X
X
X
X
y
¤
¤
¤¤
(x′, y′)
(y′, −x′)
U
Since it is assumed that motion around U is counter clockwise, the tangent vector, (x′, y′)
is as shown. Now the unit exterior normal is either
1
q
(x′)2 + (y′)2 (−y′, x′)
or
1
q
(x′)2 + (y′)2 (y′, −x′)
Again, the counter clockwise motion shows the correct unit exterior normal is the second
of the above. To see this note that since the area should be on the left as you walk around
the edge, you need to have the unit normal point in the direction of (x′, y′, 0) × k which
equals (y′, −x′, 0). Now let F (x, y) = (Q (x, y) , −P (x, y)) . Also note the area element on
∂U is
q
(x′)2 + (y′)2dt. Suppose the boundary of U consists of m smooth curves, the ith of
which is parameterized by (xi, yi) with the parameter, t ∈[ai, bi] . Then by the divergence
theorem,
Z
U
(Qx −Py) dA =
Z
U
div (F) dA =
Z
∂U
F · ndS
=
m
X
i=1
Z bi
ai
(Q (xi (t) , yi (t)) , −P (xi (t) , yi (t))) ·
1
q
(x′
i)2 + (y′
i)2 (y′
i, −x′
i)
dS
z
}|
{
q
(x′
i)2 + (y′
i)2dt
=
m
X
i=1
Z bi
ai
(Q (xi (t) , yi (t)) , −P (xi (t) , yi (t))) · (y′
i, −x′
i) dt
=
m
X
i=1
Z bi
ai
Q (xi (t) , yi (t)) y′
i (t) + P (xi (t) , yi (t)) x′
i (t) dt ≡
Z
∂U
Pdx + Qdy
This proves Green’s theorem from the divergence theorem.
1For a general version see the advanced calculus book by Apostol.
The general versions involve the
concept of a rectiﬁable Jordan curve.

29.3.
STOKE’S THEOREM FROM GREEN’S THEOREM
529
29.3
Stoke’s Theorem From Green’s Theorem
Stoke’s theorem is a generalization of Green’s theorem which relates the integral over a
surface to the integral around the boundary of the surface. These terms are a little diﬀerent
from what occurs in R2. To describe this, consider a sock. The surface is the sock and its
boundary will be the edge of the opening of the sock in which you place your foot. Another
way to think of this is to imagine a region in R2 of the sort discussed above for Green’s
theorem. Suppose it is on a sheet of rubber and the sheet of rubber is stretched in three
dimensions. The boundary of the resulting surface is the result of the stretching applied to
the boundary of the original region in R2. Here is a picture describing the situation.
∂S
R
I
S
Recall the following deﬁnition of the curl of a vector ﬁeld.
Deﬁnition 29.3.1 Let
F (x, y, z) = (F1 (x, y, z) , F2 (x, y, z) , F3 (x, y, z))
be a C1 vector ﬁeld deﬁned on an open set, V in R3. Then
∇× F ≡
¯¯¯¯¯¯
i
j
k
∂
∂x
∂
∂y
∂
∂z
F1
F2
F3
¯¯¯¯¯¯
≡
µ∂F3
∂y −∂F2
∂z
¶
i+
µ∂F1
∂z −∂F3
∂x
¶
j+
µ∂F2
∂x −∂F1
∂y
¶
k.
This is also called curl (F) and written as indicated, ∇× F.
The following lemma gives the fundamental identity which will be used in the proof of
Stoke’s theorem.
Lemma 29.3.2 Let R : U →V ⊆R3 where U is an open subset of R2 and V is an open
subset of R3. Suppose R is C2 and let F be a C1 vector ﬁeld deﬁned in V.
(Ru × Rv) · (∇× F) (R (u, v)) = ((F ◦R)u · Rv −(F ◦R)v · Ru) (u, v) .
(29.1)
Proof: Start with the left side and let xi = Ri (u, v) for short.
(Ru × Rv) · (∇× F) (R (u, v))
=
εijkxjuxkvεirs
∂Fs
∂xr
=
(δjrδks −δjsδkr) xjuxkv
∂Fs
∂xr
=
xjuxkv
∂Fk
∂xj
−xjuxkv
∂Fj
∂xk
=
Rv · ∂(F ◦R)
∂u
−Ru · ∂(F ◦R)
∂v

530
STOKE’S THEOREM 4-5 DEC.
which proves 29.1.
For those of you who do not know the permutation symbol and the reduction identities
used in the above, it is possible, but more trouble, to establish the identity by brute force.
Letting x, y, z denote the components of R (u) and f1, f2, f3 denote the components of F,
and letting a subscripted variable denote the partial derivative with respect to that variable,
the left side of 29.1 equals
¯¯¯¯¯¯
i
j
k
xu
yu
zu
xv
yv
zv
¯¯¯¯¯¯
·
¯¯¯¯¯¯
i
j
k
∂x
∂y
∂z
f1
f2
f3
¯¯¯¯¯¯
= (f3y −f2z) (yuzv −zuyv) + (f1z −f3x) (zuxv −xuzv) + (f2x −f1y) (xuyv −yuxv)
= f3yyuzv + f2zzuyv + f1zzuxv + f3xxuzv + f2xxuyv + f1yyuxv
−(f2zyuzv + f3yzuyv + f1zxuzv + f3xzuxv + f2xyuxv + f1yxuyv)
= f1yyuxv + f1zzuxv + f2xxuyv + f2zzuyv + f3xxuzv + f3yyuzv
−(f1yyvxu + f1zzvxu + f2xxvyu + f2zzvyu + f3xxvzu + f3yyvzu)
At this point I become clever and add in and subtract oﬀcertain terms. Then
= f1xxuxv + f1yyuxv + f1zzuxv + f2xxuyv + f2xyuyv
+f2zzuyv + f3xxuzv + f3yyuzv + f3zzuzv
−
µ
f1xxvxu + f1yyvxu + f1zzvxu + f2xxvyu + f2xyvyu
+f2zzvyu + f3xxvzu + f3yyvzu + f3zzvzu
¶
= ∂f1 ◦R (u, v)
∂u
xv + ∂f2 ◦R (u, v)
∂u
yv + ∂f3 ◦R (u, v)
∂u
zv
−
µ∂f1 ◦R (u, v)
∂v
xu + ∂f2 ◦R (u, v)
∂v
yu + ∂f3 ◦R (u, v)
∂v
zu
¶
= ((F ◦R)u · Rv −(F ◦R)v · Ru) (u, v) .
This proves the lemma. Not how much trouble this was and how I had to be clever by
adding in and subtracting oﬀthe appropriate terms. With the reduction identities for the
permutation symbol no cleverness at all was required. The desired identity just fell out of
completely routine manipulations. This is a good example which illustrates the utility of
good notation.
The proof of Stoke’s theorem given next follows [7]. First, it is convenient to give a
deﬁnition.
Deﬁnition 29.3.3 A vector valued function, R :U ⊆Rm →Rn is said to be in
Ck ¡
U, Rn¢
if it is the restriction to U of a vector valued function which is deﬁned on Rm
and is Ck. That is, this function has continuous partial derivatives up to order k.
Theorem 29.3.4 (Stoke’s Theorem) Let U be any region in R2 for which the con-
clusion of Green’s theorem holds and let R ∈C2 ¡
U, R3¢
be a one to one function satisfying
|(Ru × Rv) (u, v)| ̸= 0 for all (u, v) ∈U and let S denote the surface,
S
≡
{R (u, v) : (u, v) ∈U} ,
∂S
≡
{R (u, v) : (u, v) ∈∂U}
where the orientation on ∂S is consistent with the counter clockwise orientation on ∂U (U
is on the left as you walk around ∂U). Then for F a C1 vector ﬁeld deﬁned near S,
Z
∂S
F · dR =
Z
S
curl (F) · ndS

29.3.
STOKE’S THEOREM FROM GREEN’S THEOREM
531
where n is the normal to S deﬁned by
n ≡Ru × Rv
|Ru × Rv|.
Proof: Letting C be an oriented part of ∂U having parametrization, r (t) ≡(u (t) , v (t))
for t ∈[α, β] and letting R (C) denote the oriented part of ∂S corresponding to C,
Z
R(C)
F · dR =
=
Z β
α
F (R (u (t) , v (t))) · (Ruu′ (t) + Rvv′ (t)) dt
=
Z β
α
F (R (u (t) , v (t))) Ru (u (t) , v (t)) u′ (t) dt
+
Z β
α
F (R (u (t) , v (t))) Rv (u (t) , v (t)) v′ (t) dt
=
Z
C
((F ◦R) · Ru, (F ◦R) · Rv) · dr.
Since this holds for each such piece of ∂U, it follows
Z
∂S
F · dR =
Z
∂U
((F ◦R) · Ru, (F ◦R) · Rv) · dr.
By the assumption that the conclusion of Green’s theorem holds for U, this equals
Z
U
[((F ◦R) · Rv)u −((F ◦R) · Ru)v] dA
=
Z
U
[(F ◦R)u · Rv + (F ◦R) · Rvu −(F ◦R) · Ruv −(F ◦R)v · Ru] dA
=
Z
U
[(F ◦R)u · Rv −(F ◦R)v · Ru] dA
the last step holding by equality of mixed partial derivatives, a result of the assumption
that R is C2. Now by Lemma 29.3.2, this equals
Z
U
(Ru × Rv) · (∇× F) dA
=
Z
U
∇× F· (Ru × Rv) dA
=
Z
S
∇× F · ndS
because dS = |(Ru × Rv)| dA and n = (Ru×Rv)
|(Ru×Rv)|. Thus
(Ru × Rv) dA
=
(Ru × Rv)
|(Ru × Rv)| |(Ru × Rv)| dA
=
ndS.
This proves Stoke’s theorem.
Note that there is no mention made in the ﬁnal result that R is C2. Therefore, it is not
surprising that versions of this theorem are valid in which this assumption is not present. It
is possible to obtain extremely general versions of Stoke’s theorem if you use the Lebesgue
integral.

532
STOKE’S THEOREM 4-5 DEC.
29.3.1
Orientation
It turns out there are more general formulations of Stoke’s theorem than what is presented
above. However, it is always necessary for the surface, S to be orientable. This means it
is possible to obtain a vector ﬁeld for a unit normal to the surface which is a continuous
function of position on S. An example of a surface which is not orientable is the famous
Mobeus band, obtained by taking a long rectangular piece of paper and glueing the ends
together after putting a twist in it. Here is a picture of one.
There is something quite interesting about this Mobeus band and this is that it can
be written parametrically with a simple parameter domain. The picture above is a maple
graph of the parametrically deﬁned surface
R (θ, v) ≡



x = 4 cos θ + v cos θ
2
y = 4 sin θ + v cos θ
2
z = v sin θ
2
, θ ∈[0, 2π] , v ∈[−1, 1] .
An obvious question is why the normal vector, R,θ × R,v/ |R,θ × R,v| is not a continuous
function of position on S. You can see easily that it is a continuous function of both θ and
v. However, the map, R is not one to one. In fact, R (0, 0) = R (2π, 0) . Therefore, near
this point on S, there are two diﬀerent values for the above normal vector. In fact, a short
computation will show this normal vector is
¡
4 sin 1
2θ cos θ −1
2v, 4 sin 1
2θ sin θ + 1
2v, −8 cos2 1
2θ sin 1
2θ −8 cos3 1
2θ + 4 cos 1
2θ
¢
q
16 sin2 ¡ θ
2
¢
+ v2
2 + 4 sin
¡ θ
2
¢
v (sin θ −cos θ) +
¡
−8 cos2 1
2θ sin 1
2θ −8 cos3 1
2θ + 4 cos 1
2θ
¢2
and you can verify that the denominator will not vanish. Letting v = 0 and θ = 0 and 2π
yields the two vectors,
(0, 0, −1) , (0, 0, 1)
so there is a discontinuity. This is why I was careful to say in the statement of Stoke’s
theorem given above that R is one to one.
The Mobeus band has some usefulness. In old machine shops the equipment was run by
a belt which was given a twist to spread the surface wear on the belt over twice the area.
The above explanation shows that R,θ × R,v/ |R,θ × R,v| fails to deliver an orientation
for the Mobeus band. However, this does not answer the question whether there is some
orientation for it other than this one. In fact there is none. You can see this by looking at
the ﬁrst of the two pictures below or by making one and tracing it with a pencil. There
is only one side to the Mobeus band. An oriented surface must have two sides, one side
identiﬁed by the given unit normal which varies continuously over the surface and the other
side identiﬁed by the negative of this normal. The second picture below was taken by Dr.
Ouyang when he was at meetings in Paris and saw it at a museum.

29.3.
STOKE’S THEOREM FROM GREEN’S THEOREM
533
29.3.2
Conservative Vector Fields And Stoke’s Theorem
Recall the following deﬁnition.
Deﬁnition 29.3.5 A vector ﬁeld, F deﬁned in a three dimensional region is said
to be conservative2 if for every piecewise smooth closed curve, C, it follows
R
C F· dR = 0.
Stokes theorem provides an easy to use criterion for determining whether a given vector
ﬁeld is conservative.
Deﬁnition 29.3.6 A set of points in three dimensional space, V is simply connected
if every piecewise smooth closed curve, C is the edge of a surface, S which is contained
entirely within V in such a way that Stokes theorem holds for the surface, S and its edge,
C.
C
R
I
S
2There is no such thing as a liberal vector ﬁeld.

534
STOKE’S THEOREM 4-5 DEC.
This is like a sock. The surface is the sock and the curve, C goes around the opening of
the sock.
As an application of Stoke’s theorem, here is a useful theorem which gives a way to check
whether a vector ﬁeld is conservative.
Theorem 29.3.7 For a three dimensional simply connected open set, V and F a
C1 vector ﬁeld deﬁned in V, F is conservative if ∇× F = 0 in V.
Proof: If ∇×F = 0 then taking an arbitrary closed curve, C, and letting S be a surface
bounded by C which is contained in V, Stoke’s theorem implies
0 =
Z
S
∇× F · n dA =
Z
C
F· dR.
Thus F is conservative.
Example 29.3.8 Determine whether the vector ﬁeld,
¡
4x3 + 2
¡
cos
¡
x2 + z2¢¢
x, 1, 2
¡
cos
¡
x2 + z2¢¢
z
¢
is conservative.
Since this vector ﬁeld is deﬁned on all of R3, it only remains to take its curl and see if
it is the zero vector.
¯¯¯¯¯¯
i
j
k
∂x
∂y
∂z
4x3 + 2
¡
cos
¡
x2 + z2¢¢
x
1
2
¡
cos
¡
x2 + z2¢¢
z
¯¯¯¯¯¯
.
This is obviously equal to zero. Therefore, the given vector ﬁeld is conservative. Can you ﬁnd
a potential function for it? Let φ be the potential function. Then φz = 2
¡
cos
¡
x2 + z2¢¢
z
and so φ (x, y, z) = sin
¡
x2 + z2¢
+ g (x, y) . Now taking the derivative of φ with respect to
y, you see gy = 1 so g (x, y) = y +h (x) . Hence φ (x, y, z) = y +g (x)+sin
¡
x2 + z2¢
. Taking
the derivative with respect to x, you get 4x3 +2
¡
cos
¡
x2 + z2¢¢
x = g′ (x)+2x cos
¡
x2 + z2¢
and so it suﬃces to take g (x) = x4. Hence φ (x, y, z) = y + x4 + sin
¡
x2 + z2¢
.
29.3.3
Some Terminology
If F = (P, Q, R) is a vector ﬁeld. Then the statement that F is conservative is the same as
saying the diﬀerential form Pdx + Qdy + Rdz is exact. Some people like to say things in
terms of vector ﬁelds and some say it in terms of diﬀerential forms. In Example 29.3.8, the
diﬀerential form
¡
4x3 + 2
¡
cos
¡
x2 + z2¢¢
x
¢
dx + dy +
¡
2
¡
cos
¡
x2 + z2¢¢
z
¢
dz is exact.
29.3.4
Vector Identities∗
There are many interesting identities which relate the gradient, divergence and curl.
Theorem 29.3.9 Assuming f, g are a C2 vector ﬁelds whenever necessary, the
following identities are valid.
1. ∇· (∇× f) = 0
2. ∇× ∇φ = 0

29.3.
STOKE’S THEOREM FROM GREEN’S THEOREM
535
3. ∇× (∇× f) = ∇(∇· f) −∇2f where ∇2f is a vector ﬁeld whose ith component is
∇2fi.
4. ∇· (f × g) = g· (∇× f) −f· (∇× g)
5. ∇× (f × g) = (∇· g) f−(∇· f) g+ (g·∇) f−(f·∇) g
Proof: These are all easy to establish if you use the repeated index summation conven-
tion and the reduction identities discussed on Page 60.
∇· (∇× f)
=
∂i (∇× f)i
=
∂i (εijk∂jfk)
=
εijk∂i (∂jfk)
=
εjik∂j (∂ifk)
=
−εijk∂j (∂ifk)
=
−εijk∂i (∂jfk)
=
−∇· (∇× f) .
This establishes the ﬁrst formula. The second formula is done similarly. Now consider the
third.
(∇× (∇× f))i
=
εijk∂j (∇× f)k
=
εijk∂j (εkrs∂rfs)
=
=εijk
z}|{
εkij εkrs∂j (∂rfs)
=
(δirδjs −δisδjr) ∂j (∂rfs)
=
∂j (∂ifj) −∂j (∂jfi)
=
∂i (∂jfj) −∂j (∂jfi)
=
¡
∇(∇· f) −∇2f
¢
i
This establishes the third identity.
Consider the fourth identity.
∇· (f × g)
=
∂i (f × g)i
=
∂iεijkfjgk
=
εijk (∂ifj) gk + εijkfj (∂igk)
=
(εkij∂ifj) gk −(εjik∂igk) fk
=
∇× f · g −∇× g · f.
This proves the fourth identity.
Consider the ﬁfth.
(∇× (f × g))i
=
εijk∂j (f × g)k
=
εijk∂jεkrsfrgs
=
εkijεkrs∂j (frgs)
=
(δirδjs −δisδjr) ∂j (frgs)
=
∂j (figj) −∂j (fjgi)
=
(∂jgj) fi + gj∂jfi −(∂jfj) gi −fj (∂jgi)
=
((∇· g) f + (g · ∇) (f) −(∇· f) g −(f · ∇) (g))i

536
STOKE’S THEOREM 4-5 DEC.
and this establishes the ﬁfth identity.
I think the important thing about the above is not that these identities can be proved
and are valid as much as the method by which they were proved. The reduction identities on
Page 60 were used to discover the identities. There is a diﬀerence between proving something
someone tells you about and both discovering what should be proved and proving it. This
notation and the reduction identity make the discovery of vector identities fairly routine
and this is why these things are of great signiﬁcance.
29.3.5
Vector Potentials∗
One of the above identities says ∇· (∇× f) = 0. Suppose now ∇· g = 0. Does it follow that
there exists f such that g =∇× f ? It turns out that this is usually the case and when such
an f exists, it is called a vector potential. Here is one way to do it, assuming everything
is deﬁned so the following formulas make sense.
f (x, y, z) =
µZ z
0
g2 (x, y, t) dt, −
Z z
0
g1 (x, y, t) dt +
Z x
0
g3 (t, y, 0) dt, 0
¶T
.
(29.2)
In verifying this you need to use the following manipulation which will generally hold under
reasonable conditions but which has not been carefully shown yet.
∂
∂x
Z b
a
h (x, t) dt =
Z b
a
∂h
∂x (x, t) dt.
(29.3)
The above formula seems plausible because the integral is a sort of a sum and the derivative
of a sum is the sum of the derivatives. However, this sort of sloppy reasoning will get you
into all sorts of trouble. The formula involves the interchange of two limit operations, the
integral and the limit of a diﬀerence quotient. Such an interchange can only be accomplished
through a theorem. The following gives the necessary result. This lemma is stated without
proof.
Lemma 29.3.10 Suppose h and ∂h
∂x are continuous on the rectangle R = [c, d] × [a, b] .
Then 29.3 holds.
29.3.6
Maxwell’s Equations And The Wave Equation∗
Many of the ideas presented above are useful in analyzing Maxwell’s equations.
These
equations are derived in advanced physics courses. They are
∇× E + 1
c
∂B
∂t
=
0
(29.4)
∇· E
=
4πρ
(29.5)
∇× B −1
c
∂E
∂t
=
4π
c f
(29.6)
∇· B
=
0
(29.7)
and it is assumed these hold on all of R3 to eliminate technical considerations having to do
with whether something is simply connected.
In these equations, E is the electrostatic ﬁeld and B is the magnetic ﬁeld while ρ and f
are sources. By 29.7 B has a vector potential, A1 such that B = ∇× A1. Now go to 29.4
and write
∇× E+1
c ∇× ∂A1
∂t
= 0

29.3.
STOKE’S THEOREM FROM GREEN’S THEOREM
537
showing that
∇×
µ
E + 1
c
∂A1
∂t
¶
= 0
It follows E + 1
c
∂A1
∂t
has a scalar potential, ψ1 satisfying
∇ψ1 = E + 1
c
∂A1
∂t .
(29.8)
Now suppose φ is a time dependent scalar ﬁeld satisfying
∇2φ −1
c2
∂2φ
∂t2 = 1
c
∂ψ1
∂t −∇· A1.
(29.9)
Next deﬁne
A ≡A1 + ∇φ, ψ ≡ψ1 + 1
c
∂φ
∂t .
(29.10)
Therefore, in terms of the new variables, 29.9 becomes
∇2φ −1
c2
∂2φ
∂t2 = 1
c
µ∂ψ
∂t −1
c
∂2φ
∂t2
¶
−∇· A + ∇2φ
which yields
0 = ∂ψ
∂t −c∇· A.
(29.11)
Then it follows from Theorem 29.3.9 on Page 534 that A is also a vector potential for B.
That is
∇× A = B.
(29.12)
From 29.8
∇
µ
ψ −1
c
∂φ
∂t
¶
= E + 1
c
µ∂A
∂t −∇∂φ
∂t
¶
and so
∇ψ = E + 1
c
∂A
∂t .
(29.13)
Using 29.6 and 29.13,
∇× (∇× A) −1
c
∂
∂t
µ
∇ψ −1
c
∂A
∂t
¶
= 4π
c f.
(29.14)
Now from Theorem 29.3.9 on Page 534 this implies
∇(∇· A) −∇2A −∇
µ1
c
∂ψ
∂t
¶
+ 1
c2
∂2A
∂t2 = 4π
c f
and using 29.11, this gives
1
c2
∂2A
∂t2 −∇2A = 4π
c f.
(29.15)
Also from 29.13, 29.5, and 29.11,
∇2ψ
=
∇· E+1
c
∂
∂t (∇· A)
=
4πρ + 1
c2
∂2ψ
∂t2

538
STOKE’S THEOREM 4-5 DEC.
and so
1
c2
∂2ψ
∂t2 −∇2ψ = −4πρ.
(29.16)
This is very interesting. If a solution to the wave equations, 29.16, and 29.15 can be
found along with a solution to 29.11, then letting the magnetic ﬁeld be given by 29.12 and
letting E be given by 29.13 the result is a solution to Maxwells equations. This is signiﬁcant
because wave equations are easier to think of than Maxwell’s equations. Note the above
argument also showed that it is always possible, by solving another wave equation, to get
29.11 to hold.

Part XIV
Some Iterative Techniques For
Linear Algebra
539


Iterative Methods For Linear
Systems
Consider the problem of solving the equation
Ax = b
(30.1)
where A is an n × n matrix. In many applications, the matrix A is huge and composed
mainly of zeros. For such matrices, the method of Gauss elimination (row operations) is
not a good way to solve the system because the row operations can destroy the zeros and
storing all those zeros takes a lot of room in a computer. These systems are called sparse.
To solve them it is common to use an iterative technique. I am following the treatment
given to this subject by Nobel and Daniel [20].
There are two main methods which are used to obtain solutions iteratively, the Jacobi
method and the Gauss Seidel method. I will illustrate with an example and then describe
the method precisely.
30.1
Jacobi Method
Example 30.1.1 Use the Jacobi method to ﬁnd the solutions to the following system of
equations.
7x + y = 11
x −5y = 7
It is profoundly stupid to use the Jacobi method on such a 2 × 2 system. You should
simply use row operations. If you do, the solution is
©
y = −19
18, x = 31
18
ª
. In terms of decimals
this is {y = −1. 055 555 56, x = 1. 722 222 22} . Now I will proceed to show how to use the
Jacobi method to also ﬁnd this solution.
Here are steps which describe the Jacobi method. You write the system as
µ 7
1
1
−5
¶ µ x
y
¶
=
µ 11
7
¶
Next you split the matrix as follows
µ
7
0
0
−5
¶ µ
x
y
¶
+
µ
0
1
1
0
¶ µ
x
y
¶
=
µ
11
7
¶
That is you write the matrix as the sum of a diagonal matrix plus the oﬀdiagonal terms.
Then if you have a solution, you would need
µ
7
0
0
−5
¶ µ
x
y
¶
= −
µ
0
1
1
0
¶ µ
x
y
¶
+
µ
11
7
¶
541

542
ITERATIVE METHODS FOR LINEAR SYSTEMS
You could write this as
µ
x
y
¶
=
−
µ
7
0
0
−5
¶−1 µ
0
1
1
0
¶ µ
x
y
¶
+
µ
7
0
0
−5
¶−1 µ
11
7
¶
=
µ
0
−1
7
1
5
0
¶ µ
x
y
¶
+
µ
11
7
−7
5
¶
This suggests a way to approach the problem through a process of iterations. You pick an
initial guess for (x, y) say (0, 0) . (It really doesn’t matter what you pick. When the method
works it will do so for any initial choice. ) Call this initial guess
µ
x0
y0
¶
and then you obtain the next guess,
µ
x1
y1
¶
as follows
µ
x1
y1
¶
=
µ
0
−1
7
1
5
0
¶ µ
x0
y0
¶
+
µ
11
7
−7
5
¶
Then to get the next guess you do the same thing.
µ
x2
y2
¶
=
µ
0
−1
7
1
5
0
¶ µ
x1
y1
¶
+
µ
11
7
−7
5
¶
Continuing this way, this method hopefully will give guesses which are increasingly close to
the true solution. Lets apply this to this example.
µ x1
y1
¶
=
µ 0
−1
7
1
5
0
¶ µ 0
0
¶
+
µ
11
7
−7
5
¶
=
µ
1. 571 428 57
−1. 4
¶
Now you ﬁnd the next guess.
µ x2
y2
¶
=
µ 0
−1
7
1
5
0
¶ µ 1. 571 428 57
−1. 4
¶
+
µ
11
7
−7
5
¶
=
µ
1. 771 428 57
−1. 085 714 29
¶
Things are still changing so I will try the next guess.
µ
x3
y3
¶
=
µ
0
−1
7
1
5
0
¶ µ
1. 771 428 57
−1. 085 714 29
¶
+
µ
11
7
−7
5
¶
=
µ
1. 726 530 61
−1. 045 714 29
¶
Lets do another iteration.
µ x4
y4
¶
=
µ 0
−1
7
1
5
0
¶ µ
1. 726 530 61
−1. 045 714 29
¶
+
µ
11
7
−7
5
¶
=
µ
1. 720 816 33
−1. 054 693 88
¶
.

30.1.
JACOBI METHOD
543
This should be pretty close because the guesses are not changing much from one to the next.
The exact solution was
{y = −1. 055 555 56, x = 1. 722 222 22}
Actually, you don’t do it this way. The following gives the way a computer would do
it. You do not invert the matrix as I did. However, for the purposes of illustration and for
small systems there is no harm in doing it as I did above, especially since for small systems
of equations it is a stupid idea to use an iterative method in the ﬁrst place.
Deﬁnition 30.1.2 The Jacobi iterative technique, also called the method of simul-
taneous corrections is deﬁned as follows. Let x1 be an initial vector, say the zero vector or
some other vector. The method generates a succession of vectors, x2, x3, x4, ··· and hopefully
this sequence of vectors will converge to the solution to 30.1. The vectors in this list are
called iterates and they are obtained according to the following procedure. Letting A = (aij) ,
aiixr+1
i
= −
X
j̸=i
aijxr
j + bi.
(30.2)
In terms of matrices, letting
A =



∗
· · ·
∗
...
...
...
∗
· · ·
∗



The iterates are deﬁned as






∗
0
· · ·
0
0
∗
...
...
...
...
...
0
0
· · ·
0
∗











xr+1
1
xr+1
2...
xr+1
n





=
−






0
∗
· · ·
∗
∗
0
...
...
...
...
...
∗
∗
· · ·
∗
0











xr
1
xr
2
...
xr
n




+





b1
b2
...
bn





(30.3)
The matrix on the left in 30.3 is obtained by retaining the main diagonal of A and
setting every other entry equal to zero. The matrix on the right in 30.3 is obtained from A
by setting every diagonal entry equal to zero and retaining all the other entries unchanged.
Example 30.1.3 Use the Jacobi method to solve the system




3
1
0
0
1
4
1
0
0
2
5
1
0
0
2
4








x1
x2
x3
x4



=




1
2
3
4




Of course this is solved most easily using row reductions. The Jacobi method is useful
when the matrix is 1000×1000 or larger. This example is just to illustrate how the method
works. First lets solve it using row operations. The augmented matrix is




3
1
0
0
1
1
4
1
0
2
0
2
5
1
3
0
0
2
4
4





544
ITERATIVE METHODS FOR LINEAR SYSTEMS
The row reduced echelon form is




1
0
0
0
6
29
0
1
0
0
11
29
0
0
1
0
8
29
0
0
0
1
25
29




which in terms of decimals is approximately equal to




1.0
0
0
0
. 206
0
1.0
0
0
. 379
0
0
1.0
0
. 275
0
0
0
1.0
. 862



.
In terms of the matrices, the Jacobi iteration is of the form




3
0
0
0
0
4
0
0
0
0
5
0
0
0
0
4








xr+1
1
xr+1
2
xr+1
3
xr+1
4



= −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0








xr
1
xr
2
xr
3
xr
4



+




1
2
3
4



.
Multiplying by the invese of the matrix on the left, 1this iteration reduces to




xr+1
1
xr+1
2
xr+1
3
xr+1
4



= −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0








xr
1
xr
2
xr
3
xr
4



+




1
31
23
5
1



.
(30.4)
Now iterate this starting with
x1 ≡




0
0
0
0



.
Thus
x2 = −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0








0
0
0
0



+




1
31
23
5
1



=




1
31
23
5
1




Then
x3 = −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0




x2
z }| {




1
31
23
5
1



+




1
31
23
5
1



=




. 166
. 26
. 2
. 7




x4 = −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0




x3
z
}|
{




. 166
. 26
. 2
. 7



+




1
31
23
5
1



=




. 24
. 408 5
. 356
. 9




1You certainly would not compute the invese in solving a large system. This is just to show you how the
method works for this simple example. You would use the ﬁrst description in terms of indices.

30.2.
GAUSS SEIDEL METHOD
545
x5 = −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0




x4
z
}|
{




. 24
. 408 5
. 356
. 9



+




1
31
23
5
1



=




. 197
. 351
. 256 6
. 822




x6 = −




0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0




x5
z
}|
{




. 197
. 351
. 256 6
. 822



+




1
31
23
5
1



=




. 216
. 386
. 295
. 871



.
You can keep going like this. Recall the solution is approximately equal to




. 206
. 379
. 275
. 862




so you see that with no care at all and only 6 iterations, an approximate solution has been
obtained which is not too far oﬀfrom the actual solution.
It is important to realize that a computer would use 30.2 directly. Indeed, writing the
problem in terms of matrices as I have done above destroys every beneﬁt of the method.
However, it makes it a little easier to see what is happening and so this is why I have
presented it in this way.
30.2
Gauss Seidel Method
Example 30.2.1 Solve the following system of equations using the Gauss Seidel method.
It is the same example as in Example 30.1.1.
7x + y = 11
x −5y = 7
The solution to this system is is : {y = −1. 055 555 56, x = 1. 722 222 22} . Now I will use
the Gauss Seidel method to get this solution. The system is of the form
µ
7
0
1
−5
¶ µ
x
y
¶
+
µ
0
1
0
0
¶ µ
x
y
¶
=
µ
11
7
¶
Note the diﬀerence! Here you split the matrix diﬀerently. Then the iteration scheme is just
as before,
µ 7
0
1
−5
¶ µ x
y
¶
= −
µ 0
1
0
0
¶ µ x
y
¶
+
µ 11
7
¶
and so the solution satisﬁes
µ
x
y
¶
=
−
µ
7
0
1
−5
¶−1 µ
0
1
0
0
¶ µ
x
y
¶
+
µ
7
0
1
−5
¶−1 µ
11
7
¶
=
µ
0
−1
7
0
−1
35
¶ µ
x
y
¶
+
µ
11
7
−38
35
¶
The corresponding iteration scheme yields
µ
xn+1
yn+1
¶
=
µ
0
−1
7
0
−1
35
¶ µ
xn
yn
¶
+
µ
11
7
−38
35
¶

546
ITERATIVE METHODS FOR LINEAR SYSTEMS
Starting with an initial guess of x0 = y0 = 0,consider the following iterations.
µ x1
y1
¶
=
µ 0
−1
7
0
−1
35
¶ µ 0
0
¶
+
µ
11
7
−38
35
¶
=
µ
1. 571 428 57
−1. 085 714 29
¶
µ
x2
y2
¶
=
µ
0
−1
7
0
−1
35
¶ µ
1. 571 428 57
−1. 085 714 29
¶
+
µ
11
7
−38
35
¶
=
µ
1. 726 530 61
−1. 054 693 88
¶
µ
x3
y3
¶
=
µ
0
−1
7
0
−1
35
¶ µ
1. 726 530 61
−1. 054 693 88
¶
+
µ
11
7
−38
35
¶
=
µ
1. 722 099 13
−1. 055 580 17
¶
These guesses are pretty close so it seems this should be close. Note the exact solution
is {y = −1. 055 555 56, x = 1. 722 222 22} . I think you can see this method worked a little
better than the Jacobi method although both are pretty good.
The following is the precise description of the method. As before, you don’t write out
the matrices and invert that matrix like above.
Deﬁnition 30.2.2 The Gauss Seidel method, also called the method of successive
corrections is given as follows. For A = (aij) , the iterates for the problem Ax = b are
obtained according to the formula
i
X
j=1
aijxr+1
j
= −
n
X
j=i+1
aijxr
j + bi.
(30.5)
In terms of matrices, letting
A =



∗
· · ·
∗
...
...
...
∗
· · ·
∗



The iterates are deﬁned as






∗
0
· · ·
0
∗
∗
...
...
...
...
...
0
∗
· · ·
∗
∗











xr+1
1
xr+1
2...
xr+1
n





=
−






0
∗
· · ·
∗
0
0
...
...
...
...
...
∗
0
· · ·
0
0











xr
1
xr
2
...
xr
n




+





b1
b2
...
bn





(30.6)

30.2.
GAUSS SEIDEL METHOD
547
In words, you set every entry in the original matrix which is strictly above the main
diagonal equal to zero to obtain the matrix on the left. To get the matrix on the right,
you set every entry of A which is on or below the main diagonal equal to zero. Using the
iteration procedure of 30.5 directly, the Gauss Seidel method makes use of the very latest
information which is available at that stage of the computation.
The following example is the same as the example used to illustrate the Jacobi method.
Example 30.2.3 Use the Gauss Seidel method to solve the system




3
1
0
0
1
4
1
0
0
2
5
1
0
0
2
4








x1
x2
x3
x4



=




1
2
3
4




In terms of matrices, this procedure is




3
0
0
0
1
4
0
0
0
2
5
0
0
0
2
4








xr+1
1
xr+1
2
xr+1
3
xr+1
4



= −




0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0








xr
1
xr
2
xr
3
xr
4



+




1
2
3
4



.
Multiplying by the inverse of the matrix on the left2 this yields




xr+1
1
xr+1
2
xr+1
3
xr+1
4



= −




0
1
3
0
0
0
−1
12
1
4
0
0
1
30
−1
10
1
5
0
−1
60
1
20
−1
10








xr
1
xr
2
xr
3
xr
4



+




1
35
12
13
30
47
60




As before, I will be totally unoriginal in the choice of x1. Let it equal the zero vector.
Therefore,
x2 =




1
35
12
13
30
47
60



.
Now
x3 = −




0
1
3
0
0
0
−1
12
1
4
0
0
1
30
−1
10
1
5
0
−1
60
1
20
−1
10




x2
z
}|
{




1
35
12
13
30
47
60



+




1
35
12
13
30
47
60



=




. 194
. 343
. 306
. 846



.
It follows
x4 = −




0
1
3
0
0
0
−1
12
1
4
0
0
1
30
−1
10
1
5
0
−1
60
1
20
−1
10




x3
z
}|
{




1
35
12
13
30
47
60



+




1
35
12
13
30
47
60



=




. 194
. 343
. 306
. 846




2As in the case of the Jacobi iteration, the computer would not do this.
It would use the iteration
procedure in terms of the entries of the matrix directly. Otherwise all beneﬁt to using this method is lost.

548
ITERATIVE METHODS FOR LINEAR SYSTEMS
and so
x5 = −




0
1
3
0
0
0
−1
12
1
4
0
0
1
30
−1
10
1
5
0
−1
60
1
20
−1
10




x4
z
}|
{




. 194
. 343
. 306
. 846



+




1
35
12
13
30
47
60



=




. 219
. 368 75
. 283 3
. 858 35



.
Recall the answer is




. 206
. 379
. 275
. 862




so the iterates are already pretty close to the answer. You could continue doing these iterates
and it appears they converge to the solution. Now consider the following example.
Example 30.2.4 Use the Gauss Seidel method to solve the system




1
4
0
0
1
4
1
0
0
2
5
1
0
0
2
4








x1
x2
x3
x4



=




1
2
3
4




The exact solution is given by doing row operations on the augmented matrix. When
this is done the row echelon form is




1
0
0
0
6
0
1
0
0
−5
4
0
0
1
0
1
0
0
0
1
1
2




and so the solution is approximately




6
−5
4
1
1
2



=




6.0
−1. 25
1.0
. 5




The Gauss Seidel iterations are of the form




1
0
0
0
1
4
0
0
0
2
5
0
0
0
2
4








xr+1
1
xr+1
2
xr+1
3
xr+1
4



= −




0
4
0
0
0
0
1
0
0
0
0
1
0
0
0
0








xr
1
xr
2
xr
3
xr
4



+




1
2
3
4




and so, multiplying by the inverse of the matrix on the left, the iteration reduces to the
following in terms of matrix multiplication.
xr+1 = −




0
4
0
0
0
−1
1
4
0
0
2
5
−1
10
1
5
0
−1
5
1
20
−1
10



xr +




1
1
41
23
4



.

30.2.
GAUSS SEIDEL METHOD
549
This time, I will pick an initial vector close to the answer. Let
x1 =




6
−1
1
1
2




This is very close to the answer. Now lets see what the Gauss Seidel iteration does to it.
x2 = −




0
4
0
0
0
−1
1
4
0
0
2
5
−1
10
1
5
0
−1
5
1
20
−1
10








6
−1
1
1
2



+




1
1
41
23
4



=




5.0
−1.0
. 9
. 55




You can’t expect to be real close after only one iteration. Lets do another.
x3 = −




0
4
0
0
0
−1
1
4
0
0
2
5
−1
10
1
5
0
−1
5
1
20
−1
10








5.0
−1.0
. 9
. 55



+




1
1
41
23
4



=




5.0
−. 975
. 88
. 56




x4 = −




0
4
0
0
0
−1
1
4
0
0
2
5
−1
10
1
5
0
−1
5
1
20
−1
10








5.0
−. 975
. 88
. 56



+




1
1
41
23
4



=




4. 9
−. 945
. 866
. 567




The iterates seem to be getting farther from the actual solution. Why is the process which
worked so well in the other examples not working here? A better question might be: Why
does either process ever work at all?.
Both iterative procedures for solving
Ax = b
(30.7)
are of the form
Bxr+1 = −Cxr + b
where A = B + C. In the Jacobi procedure, the matrix C was obtained by setting the
diagonal of A equal to zero and leaving all other entries the same while the matrix, B
was obtained by making every entry of A equal to zero other than the diagonal entries
which are left unchanged. In the Gauss Seidel procedure, the matrix B was obtained from
A by making every entry strictly above the main diagonal equal to zero and leaving the
others unchanged and C was obtained from A by making every entry on or below the main
diagonal equal to zero and leaving the others unchanged. Thus in the Jacobi procedure,
B is a diagonal matrix while in the Gauss Seidel procedure, B is lower triangular. Using
matrices to explicitly solve for the iterates, yields
xr+1 = −B−1Cxr + B−1b.
(30.8)
This is what you would never have the computer do but this is what will allow the statement
of a theorem which gives the condition for convergence of these and all other similar methods.
Let {λ1, · · ·, λn} be the eigenvalues.
Deﬁnition 30.2.5 The spectral radius of a matrix, M, denoted as ρ (M) is
max {|λ1| , · · ·, |λn|} .
That is it is the maximum of the absolute values of the eigenvalues of M.

550
ITERATIVE METHODS FOR LINEAR SYSTEMS
The following gives the condition under which any of these iterates as in 30.8 converge.
Theorem 30.2.6 Suppose ρ
¡
B−1C
¢
< 1. Then the iterates in 30.8 converge to
the unique solution of 30.7.
The following deﬁnition is useful.
Deﬁnition 30.2.7 Suppose A is an n × n matrix. Then A is said to be strictly
diagonally dominant if for every i,
|Aii| >
X
j±i
|Aij| .
That is, the absolute value of the entry in the iith position is larger than the sum of the
absolute values of all the other entries on the ith row.
Theorem 30.2.8 In either the Jacobi or the Gauss Seidel methods, if the matrix
of coeﬃcients which gets split to yield an iteration technique is strictly diagonally dominant,
then the method converges. This means the iterates get close to the solution to the original
system of equations as the iteration progresses.

Iterative Methods For Finding
Eigenvalues
Quiz
1. Let F = (x, y, zx) and let S be the surface having parameterization r (u, v) = (uv, u + v, v)
for (u, v) ∈[0, 1] × [0, 2] . Find the ﬂux integral,
Z
S
F · ndS
where n is the unit normal to the surface which has the same direction as the para-
metric normal, N (u, v) = ru × rv.
2. Find the ﬂux integral,
Z
S
F · ndS
of F = ∇× G where G (x, y, z) =
³
sin
¡
x2yz
¢
, ln
¡
x4 + 7z2 + 1
¢
, ex2+y2z5 sin (z)
´
on
the level surface, S given by the ellipsoid x2/6 + y2/7 + z2/2 = 1.
3. Let C be the oriented curve consisting of directed line segments which go from (0, 0, 0)
to (3, 2, 1) to (1, 2, 3) to (33.5, 45.7, 67.23) and then to (1, 1, 1) . Find the line integral,
Z
C
(2xy + 1) dx +
¡
x2 + 1
¢
dy + 2zdz.
4. Find the Laplacian of x3 −3xy2.
5. Find the circulation density (curl) of the vector ﬁeld
¡
x2y, yz, z + x
¢
.
31.1
The Power Method For Eigenvalues
As indicated earlier, the eigenvalue eigenvector problem is extremely diﬃcult. Consider for
example what happens if you cannot ﬁnd the eigenvalues exactly. Then you can’t ﬁnd an
eigenvector because there isn’t one due to the fact that A −λI is invertible whenever λ
is not exactly equal to an eigenvalue. Therefore the straightforward way of solving this
problem fails right away, even if you can approximate the eigenvalues. The power method
allows you to approximate the largest eigenvalue and also the eigenvector which goes with
it.
By considering the inverse of the matrix, you can also ﬁnd the smallest eigenvalue.
551

552
ITERATIVE METHODS FOR FINDING EIGENVALUES
The method works in the situation of a nondefective matrix, A which has an eigenvalue of
algebraic multiplicity 1, λn which has the property that |λk| < |λn| for all k ̸= n. Note that
for a real matrix this excludes the case that λn could be complex. Why? Such an eigenvalue
is called a dominant eigenvalue.
Let {x1, · · ·, xn} be a basis of eigenvectors for Fn such that Axn = λnxn. Now let u1 be
some nonzero vector. Since {x1, · · ·, xn} is a basis, there exists unique scalars, ci such that
u1 =
n
X
k=1
ckxk.
Assume you have not been so unlucky as to pick u1 in such a way that cn = 0. Then let
Auk = uk+1 so that
um = Amu1 =
n−1
X
k=1
ckλm
k xk + λm
n cnxn.
(31.1)
For large m the last term, λm
n cnxn, determines quite well the direction of the vector on the
right. This is because |λn| is larger than |λk| and so for a large m, the sum, Pn−1
k=1 ckλm
k xk,
on the right is fairly insigniﬁcant. Therefore, for large m, um is essentially a multiple of the
eigenvector, xn, the one which goes with λn. The only problem is that there is no control
of the size of the vectors um. You can ﬁx this by scaling. Let S2 denote the entry of Au1
which is largest in absolute value. We call this a scaling factor. Then u2 will not be just
Au1 but Au1/S2. Next let S3 denote the entry of Au2 which has largest absolute value and
deﬁne u3 ≡Au2/S3. Continue this way. The scaling just described does not destroy the
relative insigniﬁcance of the term involving a sum in 31.1. Indeed it amounts to nothing
more than changing the units of length. Also note that from this scaling procedure, the
absolute value of the largest element of uk is always equal to 1. Therefore, for large m,
um =
λm
n cnxn
S2S3 · · · Sm
+ (relatively insigniﬁcant term) .
Therefore, the entry of Aum which has the largest absolute value is essentially equal to the
entry having largest absolute value of
A
µ
λm
n cnxn
S2S3 · · · Sm
¶
= λm+1
n
cnxn
S2S3 · · · Sm
≈λnum
and so for large m, it must be the case that λn ≈Sm+1. This suggests the following
procedure.
Finding the largest eigenvalue with its eigenvector.
1. Start with a vector, u1 which you hope has a component in the direction of xn. The
vector, (1, · · ·, 1)T is usually a pretty good choice.
2. If uk is known,
uk+1 = Auk
Sk+1
where Sk+1 is the entry of Auk which has largest absolute value.
3. When the scaling factors, Sk are not changing much, Sk+1 will be close to the eigen-
value and uk+1 will be close to an eigenvector.
4. Check your answer to see if it worked well.

31.1.
THE POWER METHOD FOR EIGENVALUES
553
Example 31.1.1 Find the largest eigenvalue of A =


5
−14
11
−4
4
−4
3
6
−3

.
The power method will now be applied to ﬁnd the largest eigenvalue for the above matrix.
Letting u1= (1, · · ·, 1)T , we will consider Au1 and scale it.


5
−14
11
−4
4
−4
3
6
−3




1
1
1

=


2
−4
6

.
Scaling this vector by dividing by the largest entry gives
1
6


2
−4
6

=



1
3
−2
3
1


= u2
Now lets do it again.


5
−14
11
−4
4
−4
3
6
−3





1
3
−2
3
1


=


22
−8
−6


Then
u3 = 1
22


22
−8
−6

=



1
−4
11
−3
11


=


1.0
−. 363 636 36
−. 272 727 27

.
Continue doing this


5
−14
11
−4
4
−4
3
6
−3




1.0
−. 363 636 36
−. 272 727 27

=


7. 090 909 1
−4. 363 636 4
1. 636 363 7


Then
u4 =


1. 0
−. 615 38
. 230 77


So far the scaling factors are changing fairly noticeably so continue.


5
−14
11
−4
4
−4
3
6
−3




1. 0
−. 615 38
. 230 77

=


16. 154
−7. 384 6
−1. 384 6


u5 =


1.0
−. 457 14
−8. 571 3 × 10−2




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 457 14
−8. 571 3 × 10−2

=


10. 457
−5. 485 7
. 514 3



554
ITERATIVE METHODS FOR FINDING EIGENVALUES
u6 =


1.0
−. 524 6
4. 918 2 × 10−2




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 524 6
4. 918 2 × 10−2

=


12. 885
−6. 295 1
−. 295 15


u7 =


1.0
−. 488 56
−2. 290 6 × 10−2




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 488 56
−2. 290 6 × 10−2

=


11. 588
−5. 862 6
. 137 36


u8 =


1.0
−. 505 92
1. 185 4 × 10−2




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 505 92
1. 185 4 × 10−2

=


12. 213
−6. 071 1
−7. 108 2 × 10−2


u9 =


1.0
−. 497 1
−5. 820 2 × 10−3




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 497 1
−5. 820 2 × 10−3

=


11. 895
−5. 965 1
3. 486 1 × 10−2


u10 =


1.0
−. 501 48
2. 930 7 × 10−3




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 501 48
2. 930 7 × 10−3

=


12. 053
−6. 017 6
−1. 767 2 × 10−2


u11 =


1.0
−. 499 26
−1. 466 2 × 10−3


At this point, you could stop because the scaling factors are not changing by much.
They went from 11. 895 to 12. 053. It looks like the eigenvalue is something like 12 which is
in fact the case. The eigenvector is approximately u11. The true eigenvector for λ = 12 is


1
−.5
0


and so you see this is pretty close. If you didn’t know this, observe


5
−14
11
−4
4
−4
3
6
−3




1.0
−. 499 26
−1. 466 2 × 10−3

=


11. 974
−5. 991 2
8. 838 6 × 10−3


(31.2)

31.1.
THE POWER METHOD FOR EIGENVALUES
555
and
12. 053


1.0
−. 499 26
−1. 466 2 × 10−3

=


12. 053
−6. 017 6
−1. 767 2 × 10−2

.
(31.3)
31.1.1
Rayleigh Quotient
In the above procedure, you can sometimes estimate the eigenvalue a little diﬀerently. If
Ax = λx then
λ = Ax · x
|x|2
and so, in the method above, you might get an estimate for the eigenvalue in this way. The
above is called the Rayleigh quotient. In 31.2 where an approximate eigenvector has been
found, you could estimate the eigenvalue as




5
−14
11
−4
4
−4
3
6
−3




1.0
−. 499 26
−1. 466 2 × 10−3



·


1.0
−. 499 26
−1. 466 2 × 10−3


1 + (−. 499 26)2 + (−1. 466 2 × 10−3)2
=
11. 978 788
The scaling factor was 12. 053 and the Rayleigh quotient gave 11. 978 788. I guess that at
least in this case the scaling factor wins. Lets look at a symmetric matrix. The book says
the convergence of the Rayleigh quotients is about twice as fast as the scaling factors for
symmetric matrices.
Example 31.1.2 Use the Rayleigh quotient with the power method to estimate the dominant
eigenvalue for the matrix,


2
0
1
0
3
0
1
0
5


It turns out that the eigenvalues of this matrix are 3, 7
2 + 1
2
√
13, 7
2 −1
2
√
13. In terms of
decimals, 3, 5. 302 775 64, 1. 697 224 36, and so the dominant eigenvalue is 5. 302 775 64.
Use the power method with an initial approximation (1, 1, 1)T . Thus


2
0
1
0
3
0
1
0
5




1
1
1

=


3.0
3.0
6.0


and so
u1 = 1
6


3.0
3.0
6.0

=


. 5
. 5
1.0


Next iteration,


2
0
1
0
3
0
1
0
5




. 5
. 5
1.0

=


2.0
1. 5
5. 5


Then
u2 = 1
5.5


2.0
1. 5
5. 5

=


. 363 636 364
. 272 727 273
1.0



556
ITERATIVE METHODS FOR FINDING EIGENVALUES
The scaling factor, 5.5 is an approximation to the dominant eigenvalue, 5. 302 775 64. Lets
see what is obtained from the Rayleigh quotient.




2
0
1
0
3
0
1
0
5




. 363 636 364
. 272 727 273
1.0



·


. 363 636 364
. 272 727 273
1.0


(. 363 636 364)2 + (. 272 727 273)2 + 1
= 5. 150 684 93
This is slightly better than the scaling factor, 5.5. Lets do another iteration. Lets see if we
get a dramatic increase in accuracy.


2
0
1
0
3
0
1
0
5




. 363 636 364
. 272 727 273
1.0

=


1. 727 272 73
. 818 181 819
5. 363 636 36


Now
u3 =
1
5. 363 636 36


1. 727 272 73
. 818 181 819
5. 363 636 36

=


. 322 033 899
. 152 542 373
1. 0


The scaling factor, 5. 363 636 36 is an approximation to the dominant eigenvalue, 5. 302 775 64.
Lets try the Rayleigh quotient again. This gives




2
0
1
0
3
0
1
0
5




. 322 033 899
. 152 542 373
1. 0



·


. 322 033 899
. 152 542 373
1. 0


(. 322 033 899)2 + (. 152 542 373)2 + 1
=
5. 254 142 24
The Rayleigh quotient is still just a little bit closer.
31.2
The Shifted Inverse Power Method
This method can ﬁnd various eigenvalues and eigenvectors. It is a signiﬁcant generalization
of the above simple procedure and yields very good results. The situation is this: You have
a number, α which is close to λ, some eigenvalue of an n × n matrix, A. You don’t know
λ but you know that α is closer to λ than to any other eigenvalue. Your problem is to ﬁnd
both λ and an eigenvector which goes with λ. Another way to look at this is to start with
α and seek the eigenvalue, λ, which is closest to α along with an eigenvector associated
with λ. If α is an eigenvalue of A, then you have what you want. Therefore, we will always
assume α is not an eigenvalue of A and so (A −αI)−1 exists. The method is based on the
following lemma. When using this method it is nice to choose α fairly close to an eigenvalue.
Otherwise, the method will converge slowly. In order to get some idea where to start, you
could use Gerschgorin’s theorem but this theorem will only give a rough idea where to look.
There isn’t a really good way to know how to choose α for general cases. As we mentioned
earlier, the eigenvalue problem is very diﬃcult to solve in general.
Lemma 31.2.1 Let {λk}n
k=1 be the eigenvalues of A. If xk is an eigenvector of A for
the eigenvalue λk, then xk is an eigenvector for (A −αI)−1 corresponding to the eigenvalue
1
λk−α.

31.2.
THE SHIFTED INVERSE POWER METHOD
557
Proof: Let λk and xk be as described in the statement of the lemma. Then
(A −αI) xk = (λk −α) xk
and so
1
λk −αxk = (A −αI)−1 xk.
This proves the lemma.
In explaining why the method works, we will assume A is nondefective. This is not
necessary!
This method is much better than it might seem from the explanation we are
about to give. Pick u1, an initial vector and let Axk = λkxk, where {x1, · · ·, xn} is a basis
of eigenvectors which exists from the assumption that A is nondefective. Assume α is closer
to λn than to any other eigenvalue. Since A is nondefective, there exist constants, ak such
that
u1 =
n
X
k=1
akxk.
Possibly λn is a repeated eigenvalue. Then combining the terms in the sum which involve
eigenvectors for λn, a simpler description of u1 is
u1 =
m
X
j=1
ajxj + y
where y is an eigenvector for λn which is assumed not equal to 0. (If you are unlucky in your
choice for u1, this might not happen and things won’t work.) Now the iteration procedure
is deﬁned as
uk+1 ≡(A −αI)−1 uk
Sk
where Sk is the element of (A −αI)−1 uk which has largest absolute value. From Lemma
31.2.1,
uk+1
=
Pm
j=1 aj
³
1
λj−α
´k
xj +
³
1
λn−α
´k
y
S2 · · · Sk
=
³
1
λn−α
´k
S2 · · · Sk


m
X
j=1
aj
µλn −α
λj −α
¶k
xj + y

.
Now it is being assumed that λn is the eigenvalue which is closest to α and so for large k,
the term,
m
X
j=1
aj
µλn −α
λj −α
¶k
xj ≡Ek
is very small while for every k ≥1, uk is a moderate sized vector because every entry has
absolute value less than or equal to 1. Thus
uk+1 =
³
1
λn−α
´k
S2 · · · Sk
(Ek + y) ≡Ck (Ek + y)
where Ek →0, y is some eigenvector for λn, and Ck is of moderate size, remaining bounded
as k →∞. Therefore, for large k,
uk+1 −Cky = CkEk≈0

558
ITERATIVE METHODS FOR FINDING EIGENVALUES
and multiplying by (A −αI)−1 yields
(A −αI)−1 uk+1 −(A −αI)−1 Cky
=
(A −αI)−1 uk+1 −Ck
µ
1
λn −α
¶
y
≈
(A −αI)−1 uk+1 −
µ
1
λn −α
¶
uk+1≈0.
Therefore, for large k, uk is approximately equal to an eigenvector of (A −αI)−1. Therefore,
(A −αI)−1 uk ≈
1
λn −αuk
and so you could take the dot product of both sides with uk and approximate λn by solving
the following for λn.
(A −αI)−1 uk · uk
|uk|2
=
1
λn −α
How else can you ﬁnd the eigenvalue from this? Suppose uk = (w1, · · ·, wn)T and from
the construction |wi| ≤1 and wk = 1 for some k. Then
Skuk+1 = (A −αI)−1 uk ≈(A −αI)−1 (Ck−1y) =
1
λn −α (Ck−1y) ≈
1
λn −αuk.
Hence the entry of (A −αI)−1 uk which has largest absolute value is approximately
1
λn−α
and so it is likely that you can estimate λn using the formula
Sk =
1
λn −α.
Of course this would fail if (A −αI)−1 uk had more than one entry having equal absolute
value.
Here is how you use the shifted inverse power method to ﬁnd the eigenvalue
and eigenvector closest to α.
1. Find (A −αI)−1 .
2. Pick u1. It is important that u1 = Pm
j=1 ajxj +y where y is an eigenvector which goes
with the eigenvalue closest to α and the sum is in an “invariant subspace corresponding
to the other eigenvalues”. Of course you have no way of knowing whether this is so
but it typically is so. If things don’t work out, just start with a diﬀerent u1. You were
unlucky in your choice.
3. If uk has been obtained,
uk+1 = (A −αI)−1 uk
Sk
where Sk is the element of uk which has largest absolute value.
4. When the scaling factors, Sk are not changing much and the uk are not changing
much, ﬁnd the approximation to the eigenvalue by solving
Sk =
1
λ −α
for λ. The eigenvector is approximated by uk+1.

31.2.
THE SHIFTED INVERSE POWER METHOD
559
5. Check your work by multiplying by the original matrix to see how well what you have
found works.
Example 31.2.2 Find the eigenvalue of A =


5
−14
11
−4
4
−4
3
6
−3

which is closest to −7.
Also ﬁnd an eigenvector which goes with this eigenvalue.
In this case the eigenvalues are −6, 0, and 12 so the correct answer is −6 for the eigen-
value. Then from the above procedure, we will start with an initial vector,
u1 ≡


1
1
1

.
We want the eigenvalue closest to −7. Thus we could use the above method. First we ﬁnd




5
−14
11
−4
4
−4
3
6
−3

+ 7


1
0
0
0
1
0
0
0
1




−1
=


68
133
122
133
−65
133
4
133
15
133
4
133
−3
7
−6
7
4
7


Then beginning with u1 above, the next iterate is
u2 =


68
133
122
133
−65
133
4
133
15
133
4
133
−3
7
−6
7
4
7




1
1
1

=


. 939 849 624
. 172 932 331
−. 714 285 714


Thus S2 = . 939 849 624 and


. 939 849 624
. 172 932 331
−. 714 285 714


1
. 939 849 624 =


1.0
. 184
−. 76


Then doing another iteration,
u3 =


68
133
122
133
−65
133
4
133
15
133
4
133
−3
7
−6
7
4
7




1.0
. 184
−. 76

=


1. 051 488 72
2. 796 992 48 × 10−2
−1. 020 571 43


Dividing by the largest element, this yields
1
1. 051 488 72


1. 051 488 72
2. 796 992 48 × 10−2
−1. 020 571 43

=


1.0
2. 660 030 89 × 10−2
−. 970 596 651


The next iteration is
u4 =


68
133
122
133
−65
133
4
133
15
133
4
133
−3
7
−6
7
4
7




1.0
2. 660 030 89 × 10−2
−. 970 596 651

=


1. 010 030 23
3. 884 346 09 × 10−3
−1. 005 998 35


The scaling factors are not changing by very much so this looks like a good time to stop.
Thus you solve the following for λ.
1
λ + 7 = 1. 010 030 23.

560
ITERATIVE METHODS FOR FINDING EIGENVALUES
This yields
1
λ+7 = 1. 010 030 23 which yields λ = −6. 009 930. This is pretty close to the
true eigenvalue, −6. How well does u4 work as an eigenvector?


5
−14
11
−4
4
−4
3
6
−3




1. 010 030 23
3. 884 346 09 × 10−3
−1. 005 998 35

=


−6. 070 211 55
−5. 901 356 4 × 10−4
6. 071 391 82


while
−6. 009 930


1. 010 030 23
3. 884 346 09 × 10−3
−1. 005 998 35

=


−6. 070 210 98
−2. 334 464 81 × 10−2
6. 045 979 66

.
Example 31.2.3 Consider the symmetric matrix, A =


1
2
3
2
1
4
3
4
2

. Find the middle
eigenvalue and an eigenvector which goes with it.
Since A is symmetric, it follows it has three real eigenvalues which are solutions to
p (λ)
=
det

λ


1
0
0
0
1
0
0
0
1

−


1
2
3
2
1
4
3
4
2




=
λ3 −4λ2 −24λ −17 = 0
If you use your graphing calculator to graph this polynomial, you ﬁnd there is an eigenvalue
somewhere between −.9 and −.8 and that this is the middle eigenvalue. Of course you could
zoom in and ﬁnd it very accurately without much trouble but what about the eigenvector
which goes with it? If you try to solve

(−.8)


1
0
0
0
1
0
0
0
1

−


1
2
3
2
1
4
3
4
2






x
y
z

=


0
0
0


there will be only the zero solution because the matrix on the left will be invertible and the
same will be true if you replace −.8 with a better approximation like −.86 or −.855. This is
because all these are only approximations to the eigenvalue and so the matrix in the above
is nonsingular for all of these. Therefore, you will only get the zero solution and
Eigenvectors are never equal to zero!
However, there exists such an eigenvector and you can ﬁnd it using the shifted inverse power
method. Pick α = −.855. You know this is close to the true eigenvalue. Then you ﬁnd




1
2
3
2
1
4
3
4
2

+ .855


1
0
0
0
1
0
0
0
1




−1
=


−367. 501 105
215. 955 47
83. 601 203 4
215. 955 47
−127. 169 104
−48. 753 063 2
83. 601 203 4
−48. 753 063 2
−19. 191 368 6


The ﬁrst step of the iteration is then
u1 =


−367. 501 105
215. 955 47
83. 601 203 4
215. 955 47
−127. 169 104
−48. 753 063 2
83. 601 203 4
−48. 753 063 2
−19. 191 368 6




1
1
1

=


−67. 944 431 6
40. 033 302 8
15. 656 771 6



31.2.
THE SHIFTED INVERSE POWER METHOD
561
Dividing by the largest entry to normalize the vector on the right,


−67. 944 431 6
40. 033 302 8
15. 656 771 6


1
−67. 944 431 6 =


1. 0
−. 589 206 53
−. 230 434 949


Then the next approximation is
u2
=


−367. 501 105
215. 955 47
83. 601 203 4
215. 955 47
−127. 169 104
−48. 753 063 2
83. 601 203 4
−48. 753 063 2
−19. 191 368 6




1. 0
−. 589 206 53
−. 230 434 949


=


−514. 008 117
302. 118 746
116. 749 189


Divide this by the largest element.


−514. 008 117
302. 118 746
116. 749 189


1
−514. 008 117 =


1.0
−. 587 770 38
−. 227 134 913


Clearly these vectors are not changing much. An approximate eigenvector is then


1.0
−. 587 770 38
−. 227 134 913


and to ﬁnd the eigenvalue you solve
1
λ+.855 = −514. 008 117, which yields λ = −. 856 945 495.
How well does it work?


1
2
3
2
1
4
3
4
2




1.0
−. 587 770 38
−. 227 134 913

=


−. 856 945 499
. 503 689 968
. 194 648 654


while
−. 856 945 495


1.0
−. 587 770 38
−. 227 134 913

=


−. 856 945 495
. 503 687 179
. 194 642 24


I think you can see that for all practical purposes, this has found the eigenvalue and an
eigenvector.

562
ITERATIVE METHODS FOR FINDING EIGENVALUES

Part XV
The Correct Version Of The
Riemann Integral ∗
563


The Theory Of The Riemann
Integral∗∗
R
Jordan the contented dragon
A.1
An Important Warning
If you read and understand this appendix on the Riemann integral you will become ab-
normal if you are not already that way. You will laugh at atrocious puns. You will be
unpopular with well adjusted conﬁdent people. Furthermore, your conﬁdence will be com-
pletely shattered. Virtually nothing will be obvious to you ever again. Consider whether
it would be better to accept the superﬁcial presentation given earlier than to attempt to
acquire deep understanding of the integral, risking your self esteem and conﬁdence, before
proceeding further.
A.2
The Deﬁnition Of The Riemann Integral
The deﬁnition of the Riemann integral of a function of n variables uses the following deﬁni-
tion.
565

566
THE THEORY OF THE RIEMANN INTEGRAL∗∗
Deﬁnition A.2.1 For i = 1, · · ·, n, let
©
αi
k
ª∞
k=−∞be points on R which satisfy
lim
k→∞αi
k = ∞,
lim
k→−∞αi
k = −∞, αi
k < αi
k+1.
(1.1)
For such sequences, deﬁne a grid on Rn denoted by G or F as the collection of boxes of the
form
Q =
n
Y
i=1
£
αi
ji, αi
ji+1
¤
.
(1.2)
If G is a grid, F is called a reﬁnement of G if every box of G is the union of boxes of F.
Lemma A.2.2 If G and F are two grids, they have a common reﬁnement, denoted here
by G ∨F.
Proof: Let
©
αi
k
ª∞
k=−∞be the sequences used to construct G and let
©
βi
k
ª∞
k=−∞be
the sequence used to construct F. Now let
©
γi
k
ª∞
k=−∞denote the union of
©
αi
k
ª∞
k=−∞and
©
βi
k
ª∞
k=−∞. It is necessary to show that for each i these points can be arranged in order.
To do so, let γi
0 ≡αi
0. Now if
γi
−j, · · ·, γi
0, · · ·, γi
j
have been chosen such that they are in order and all distinct, let γi
j+1 be the ﬁrst element
of
©
αi
k
ª∞
k=−∞∪
©
βi
k
ª∞
k=−∞
(1.3)
which is larger than γi
j and let γi
−(j+1) be the last element of 1.3 which is strictly smaller
than γi
−j. The assumption 1.1 insures such a ﬁrst and last element exists. Now let the grid
G ∨F consist of boxes of the form
Q ≡
n
Y
i=1
£
γi
ji, γi
ji+1
¤
.
The Riemann integral is only deﬁned for functions, f which are bounded and are equal
to zero oﬀsome bounded set, D. In what follows f will always be such a function.
Deﬁnition A.2.3 Let f be a bounded function which equals zero oﬀa bounded set,
D, and let G be a grid. For Q ∈G, deﬁne
MQ (f) ≡sup {f (x) : x ∈Q} , mQ (f) ≡inf {f (x) : x ∈Q} .
(1.4)
Also deﬁne for Q a box, the volume of Q, denoted by v (Q) by
v (Q) ≡
n
Y
i=1
(bi −ai) , Q ≡
n
Y
i=1
[ai, bi] .
Now deﬁne upper sums, UG (f) and lower sums, LG (f) with respect to the indicated grid,
by the formulas
UG (f) ≡
X
Q∈G
MQ (f) v (Q) , LG (f) ≡
X
Q∈G
mQ (f) v (Q) .
A function of n variables is Riemann integrable when there is a unique number between all
the upper and lower sums. This number is the value of the integral.

A.2.
THE DEFINITION OF THE RIEMANN INTEGRAL
567
Note that in this deﬁnition, MQ (f) = mQ (f) = 0 for all but ﬁnitely many Q ∈G so
there are no convergence questions to be considered here.
Lemma A.2.4 If F is a reﬁnement of G then
UG (f) ≥UF (f) , LG (f) ≤LF (f) .
Also if F and G are two grids,
LG (f) ≤UF (f) .
Proof: For P ∈G let bP denote the set,
{Q ∈F : Q ⊆P} .
Then P = ∪bP and
LF (f) ≡
X
Q∈F
mQ (f) v (Q) =
X
P ∈G
X
Q∈b
P
mQ (f) v (Q)
≥
X
P ∈G
mP (f)
X
Q∈b
P
v (Q) =
X
P ∈G
mP (f) v (P) ≡LG (f) .
Similarly, the other inequality for the upper sums is valid.
To verify the last assertion of the lemma, use Lemma A.2.2 to write
LG (f) ≤LG∨F (f) ≤UG∨F (f) ≤UF (f) .
This proves the lemma.
This lemma makes it possible to deﬁne the Riemann integral.
Deﬁnition A.2.5 Deﬁne an upper and a lower integral as follows.
I (f) ≡inf {UG (f) : G is a grid} ,
I (f) ≡sup {LG (f) : G is a grid} .
Lemma A.2.6 I (f) ≥I (f) .
Proof: From Lemma A.2.4 it follows for any two grids G and F,
LG (f) ≤UF (f) .
Therefore, taking the supremum for all grids on the left in this inequality,
I (f) ≤UF (f)
for all grids F. Taking the inﬁmum in this inequality, yields the conclusion of the lemma.
Deﬁnition A.2.7 A bounded function, f which equals zero oﬀa bounded set, D, is
said to be Riemann integrable, written as f ∈R (Rn) exactly when I (f) = I (f) . In this
case deﬁne
Z
f dV ≡
Z
f dx = I (f) = I (f) .
As in the case of integration of functions of one variable, one obtains the Riemann
criterion which is stated as the following theorem.

568
THE THEORY OF THE RIEMANN INTEGRAL∗∗
Theorem A.2.8 (Riemann criterion) f ∈R (Rn) if and only if for all ε > 0 there
exists a grid G such that
UG (f) −LG (f) < ε.
Proof: If f ∈R (Rn), then I (f) = I (f) and so there exist grids G and F such that
UG (f) −LF (f) ≤I (f) + ε
2 −
³
I (f) −ε
2
´
= ε.
Then letting H = G ∨F, Lemma A.2.4 implies
UH (f) −LH (f) ≤UG (f) −LF (f) < ε.
Conversely, if for all ε > 0 there exists G such that
UG (f) −LG (f) < ε,
then
I (f) −I (f) ≤UG (f) −LG (f) < ε.
Since ε > 0 is arbitrary, this proves the theorem.
A.3
Basic Properties
It is important to know that certain combinations of Riemann integrable functions are
Riemann integrable. The following theorem will include all the important cases.
Theorem A.3.1 Let f, g ∈R (Rn) and let φ : K →R be continuous where K is a
compact set in R2 containing f (Rn) × g (Rn). Also suppose that φ (0, 0) = 0. Then deﬁning
h (x) ≡φ (f (x) , g (x)) ,
it follows that h is also in R (Rn).
Proof: Let ε > 0 and let δ1 > 0 be such that if (yi, zi) , i = 1, 2 are points in K, such
that |z1 −z2| ≤δ1 and |y1 −y2| ≤δ1, then
|φ (y1, z1) −φ (y2, z2)| < ε.
Let 0 < δ < min (δ1, ε, 1) . Let G be a grid with the property that for Q ∈G, the diameter
of Q is less than δ and also for k = f, g,
UG (k) −LG (k) < δ2.
(1.5)
Then deﬁning for k = f, g,
Pk ≡{Q ∈G : MQ (k) −mQ (k) > δ} ,
it follows
δ2 >
X
Q∈G
(MQ (k) −mQ (k)) v (Q) ≥
X
Pk
(MQ (k) −mQ (k)) v (Q) ≥δ
X
Pk
v (Q)

A.3.
BASIC PROPERTIES
569
and so for k = f, g,
ε > δ >
X
Pk
v (Q) .
(1.6)
Suppose for k = f, g,
MQ (k) −mQ (k) ≤δ.
Then if x1, x2 ∈Q,
|f (x1) −f (x2)| < δ, and |g (x1) −g (x2)| < δ.
Therefore,
|h (x1) −h (x2)| ≡|φ (f (x1) , g (x1)) −φ (f (x2) , g (x2))| < ε
and it follows that
|MQ (h) −mQ (h)| ≤ε.
Now let
S ≡{Q ∈G : 0 < MQ (k) −mQ (k) ≤δ, k = f, g} .
Thus the union of the boxes in S is contained in some large box, R, which depends only
on f and g and also, from the assumption that φ (0, 0) = 0, MQ (h) −mQ (h) = 0 unless
Q ⊆R. Then
UG (h) −LG (h) ≤
X
Q∈Pf
(MQ (h) −mQ (h)) v (Q) +
X
Q∈Pg
(MQ (h) −mQ (h)) v (Q) +
X
Q∈S
δv (Q) .
Now since K is compact, it follows φ (K) is bounded and so there exists a constant, C,
depending only on h and φ such that MQ (h)−mQ (h) < C. Therefore, the above inequality
implies
UG (h) −LG (h) ≤C
X
Q∈Pf
v (Q) + C
X
Q∈Pg
v (Q) +
X
Q∈S
δv (Q) ,
which by 1.6 implies
UG (h) −LG (h) ≤2Cε + δv (R) ≤2Cε + εv (R) .
Since ε is arbitrary, the Riemann criterion is satisﬁed and so h ∈R (Rn).
Corollary A.3.2 Let f, g ∈R (Rn) and let a, b ∈R. Then af + bg, fg, and |f| are all
in R (Rn) . Also,
Z
Rn (af + bg) dx = a
Z
Rn f dx + b
Z
Rn g dx,
(1.7)
and
Z
|f| dx ≥
¯¯¯¯
Z
f dx
¯¯¯¯ .
(1.8)
Proof: Each of the combinations of functions described above is Riemann integrable
by Theorem A.3.1. For example, to see af + bg ∈R (Rn) consider φ (y, z) ≡ay + bz. This
is clearly a continuous function of (y, z) such that φ (0, 0) = 0. To obtain |f| ∈R (Rn) , let
φ (y, z) ≡|y| . It remains to verify the formulas. To do so, let G be a grid with the property
that for k = f, g, |f| and af + bg,
UG (k) −LG (k) < ε.
(1.9)

570
THE THEORY OF THE RIEMANN INTEGRAL∗∗
Consider 1.7. For each Q ∈G pick a point in Q, xQ. Then
X
Q∈G
k (xQ) v (Q) ∈[LG (k) , UG (k)]
and so
¯¯¯¯¯¯
Z
k dx −
X
Q∈G
k (xQ) v (Q)
¯¯¯¯¯¯
< ε.
Consequently, since
X
Q∈G
(af + bg) (xQ) v (Q)
= a
X
Q∈G
f (xQ) v (Q) + b
X
Q∈G
g (xQ) v (Q) ,
it follows
¯¯¯¯
Z
(af + bg) dx −a
Z
f dx −b
Z
g dx
¯¯¯¯ ≤
¯¯¯¯¯¯
Z
(af + bg) dx −
X
Q∈G
(af + bg) (xQ) v (Q)
¯¯¯¯¯¯
+
¯¯¯¯¯¯
a
X
Q∈G
f (xQ) v (Q) −a
Z
f dx
¯¯¯¯¯¯
+
¯¯¯¯¯¯
b
X
Q∈G
g (xQ) v (Q) −b
Z
g dx
¯¯¯¯¯¯
≤ε + |a| ε + |b| ε.
Since ε is arbitrary, this establishes Formula 1.7 and shows the integral is linear.
It remains to establish the inequality 1.8. By 1.9, and the triangle inequality for sums,
Z
|f| dx + ε ≥
X
Q∈G
|f (xQ)| v (Q) ≥
≥
¯¯¯¯¯¯
X
Q∈G
f (xQ) v (Q)
¯¯¯¯¯¯
≥
¯¯¯¯
Z
f dx
¯¯¯¯ −ε.
Then since ε is arbitrary, this establishes the desired inequality. This proves the corollary.
Which functions are in R (Rn)? Begin with step functions deﬁned below.
Deﬁnition A.3.3 If
Q ≡
n
Y
i=1
[ai, bi]
is a box, deﬁne int (Q) as
int (Q) ≡
n
Y
i=1
(ai, bi) .
f is called a step function if there is a grid, G such that f is constant on int (Q) for each
Q ∈G, f is bounded, and f (x) = 0 for all x outside some bounded set.
The next corollary states that step functions are in R (Rn) and shows the expected
formula for the integral is valid.

A.3.
BASIC PROPERTIES
571
Corollary A.3.4 Let G be a grid and let f be a step function such that f = fQ on
int (Q) for each Q ∈G. Then f ∈R (Rn) and
Z
f dx =
X
Q∈G
fQv (Q) .
Proof: Let Q be a box of G,
Q ≡
n
Y
i=1
£
αi
ji, αi
ji+1
¤
,
and suppose g is a bounded function, |g (x)| ≤C, and g = 0 oﬀQ, and g = 1 on int (Q) .
Thus, g is the simplest sort of step function. Reﬁne G by including the extra points,
αi
ji + η and αi
ji+1 −η
for each i = 1, · · ·, n. Here η is small enough that for each i, αi
ji + η < αi
ji+1 −η. Also let L
denote the largest of the lengths of the sides of Q. Let F be this reﬁned grid and denote by
Qη the box
n
Y
i=1
£
αi
ji + η, αi
ji+1 −η
¤
.
Now deﬁne the box, Bk by
Bk ≡
£
α1
j1, α1
j1+1
¤
× · · · ×
h
αk−1
jk−1, αk−1
jk−1+1
i
×
£
αk
jk, αk
jk + η
¤
×
h
αk+1
jk+1, αk+1
jk+1+1
i
× · · · ×
£
αn
jn, αn
jn+1
¤
or
Bk ≡
£
α1
j1, α1
j1+1
¤
× · · · ×
h
αk−1
jk−1, αk−1
jk−1+1
i
×
£
αk
jk −η, αk
jk
¤
×
h
αk+1
jk+1, αk+1
jk+1+1
i
× · · · ×
£
αn
jn, αn
jn+1
¤
.
In words, replace the closed interval in the kth slot used to deﬁne Q with a much thinner
closed interval at one end or the other while leaving the other intervals used to deﬁne Q the
same. This is illustrated in the following picture.
Qη
Q
Bk
Bk
The important thing to notice, is that every point of Q is either in Qη or one of the sets,
Bk. Therefore,
LF (g) ≥v (Qη) −
n
X
k=1
2Cv (Bk) ≥v (Qη) −4CLn−1nη

572
THE THEORY OF THE RIEMANN INTEGRAL∗∗
= v (Qη) −Kη
(1.10)
where K is a constant which does not depend on η. Similarly,
UF (g) ≤v (Qη) + Kη.
(1.11)
This implies UF (g) −LF (g) < 2Kη and since η is arbitrary, the Riemann criterion veriﬁes
that g ∈R (Rn) . Formulas 1.10 and 1.11 also verify that
v (Qη) ∈[UF (g) −Kη, LF (g) + Kη]
⊆[LF (g) −Kη, UF (g) + Kη] .
But also
Z
g dx ∈[LF (g) , UF (g)] ⊆[LF (g) −Kη, UF (g) + Kη]
and so
¯¯¯¯
Z
g dx −v (Qη)
¯¯¯¯ ≤4Kη.
Now letting η →0, yields
R
g dx = v (Q) .
Now let f be as described in the statement of the Corollary. Let fQ be the value of f
on int (Q) , and let gQ be a function of the sort just considered which equals 1 on int (Q) .
Then f is of the form
f =
X
Q∈G
fQgQ
with all but ﬁnitely many of the fQ equal zero. Therefore, the above is really a ﬁnite sum
and so by Corollary A.3.2, f ∈R (Rn) and
Z
f dx =
X
Q∈G
fQ
Z
gQ dx =
X
Q∈G
fQv (Q) .
This proves the corollary.
There is a good deal of sloppiness inherent in the above description of a step function
due to the fact that the boxes may be diﬀerent but match up on an edge. It is convenient
to be able to consider a more precise sort of function and this is done next.
For Q a box of the form
Q =
k
Y
i=1
[ai, bi] ,
deﬁne the half open box, Q′ by
Q′ =
k
Y
i=1
(ai, bi].
The reason for considering these sets is that if G is a grid, the sets, Q′ where Q ∈G are
disjoint. Deﬁning a step function, φ as
φ (x) ≡
X
Q∈G
φQXQ′ (x) ,
the number, φQ is the value of φ on the set, Q′. As before, deﬁne
MQ′ (f) ≡sup {f (x) : x ∈Q′} , mQ′ (f) ≡inf {f (x) : x ∈Q′} .
The next lemma will be convenient a little later.

A.3.
BASIC PROPERTIES
573
Lemma A.3.5 Suppose f is a bounded function which equals zero oﬀsome bounded set.
Then f ∈R (Rn) if and only if for all ε > 0 there exists a grid, G such that
X
Q∈G
(MQ′ (f) −mQ′ (f)) v (Q) < ε.
(1.12)
Proof: Since Q′ ⊆Q,
MQ′ (f) −mQ′ (f) ≤MQ (f) −mQ (f)
and therefore, the only if part of the equivalence is obvious.
Conversely, let G be a grid such that 1.12 holds with ε replaced with ε
2. It is necessary
to show there is a grid such that 1.12 holds with no primes on the Q. Let F be a reﬁnement
of G obtained by adding the points αi
k + ηk where ηk ≤η and is also chosen so small that
for each i = 1, · · ·, n,
αi
k + ηk < αi
k+1.
You only need to have ηk > 0 for the ﬁnitely many boxes of G which intersect the bounded
set where f is not zero. Then for
Q ≡
n
Y
i=1
£
αi
ki, αi
ki+1
¤
∈G,
Let
bQ ≡
n
Y
i=1
£
αi
ki + ηki, αi
ki+1
¤
and denote by bG the collection of these smaller boxes. For each set, Q in G there is the
smaller set, bQ along with n boxes, Bk, k = 1, · · ·, n, one of whose sides is of length ηk
and the remainder of whose sides are shorter than the diameter of Q such that the set,
Q is the union of bQ and these sets, Bk. Now suppose f equals zero oﬀthe ball B
¡
0, R
2
¢
.
Then without loss of generality, you may assume the diameter of every box in G which
has nonempty intersection with B (0,R) is smaller than R
3 . (If this is not so, simply reﬁne
G to make it so, such a reﬁnement leaving 1.12 valid because reﬁnements do not increase
the diﬀerence between upper and lower sums in this context either.) Suppose there are P
sets of G contained in B (0,R) (So these are the only sets of G which could have nonempty
intersection with the set where f is nonzero.) and suppose that for all x, |f (x)| < C/2.
Then
X
Q∈F
(MQ (f) −mQ (f)) v (Q) ≤
X
b
Q∈b
G
³
M b
Q (f) −m b
Q (f)
´
v (Q)
+
X
Q∈F\ b
G
(MQ (f) −mQ (f)) v (Q)
The ﬁrst term on the right of the inequality in the above is no larger than ε/2 because
M b
Q (f) −m b
Q (f) ≤MQ′ (f) −mQ′ (f) for each Q. Therefore, the above is dominated by
≤ε/2 + CPnRn−1η < ε
whenever η is small enough. Since ε is arbitrary, f ∈R (Rn) as claimed.
Deﬁnition A.3.6 A bounded set, E is a Jordan set in Rn or a contented set in Rn
if XE ∈R (Rn). Also, for G a grid and E a set, denote by ∂G (E) those boxes of G which
have nonempty intersection with both E and Rn \ E.

574
THE THEORY OF THE RIEMANN INTEGRAL∗∗
The next theorem is a characterization of those sets which are Jordan sets.
Theorem A.3.7 A bounded set, E, is a Jordan set if and only if for every ε > 0
there exists a grid, G, such that
X
Q∈∂G(E)
v (Q) < ε.
Proof: If Q /∈∂G (E) , then
MQ (XE) −mQ (XE) = 0
and if Q ∈∂G (E) , then
MQ (XE) −mQ (XE) = 1.
It follows that UG (XE) −LG (XE) = P
Q∈∂G(E) v (Q) and this implies the conclusion of the
theorem by the Riemann criterion.
Note that if E is a Jordan set and if f ∈R (Rn) , then by Corollary A.3.2, XEf ∈R (Rn) .
Deﬁnition A.3.8 For E a Jordan set and fXE ∈R (Rn) .
Z
E
f dV ≡
Z
Rn XEf dV.
Also, a bounded set, E, has Jordan content 0 or content 0 if for every ε > 0 there exists a
grid, G such that
X
Q∩E̸=∅
v (Q) < ε.
This symbol says to sum the volumes of all boxes from G which have nonempty intersection
with E.
Note that any ﬁnite union of sets having Jordan content 0 also has Jordan content 0.
(Why?)
Deﬁnition A.3.9 Let A be any subset of Rn. Then ∂A denotes those points, x with
the property that if U is any open set containing x, then U contains points of A as well as
points of AC.
Corollary A.3.10 If a bounded set, E ⊆Rn is contented, then ∂E has content 0.
Proof: Let ε > 0 be given and suppose E is contented. Then there exists a grid, G such
that
X
Q∈∂G(E)
v (Q) <
ε
2n + 1.
(1.13)
Now reﬁne G if necessary to get a new grid, F such that all boxes from F which have
nonempty intersection with ∂E have sides no larger than δ where δ is the smallest of all the
sides of all the Q in the above sum. Recall that ∂G (E) consists of those boxes of G which
have nonempty intersection with both E and Rn \ E.
Let x ∈∂E. Then since the dimension is n, there are at most 2n boxes from F which
contain x. Furthermore, at least one of these boxes is in ∂F (E) and is therefore a subset
of a box from ∂G (E) . Here is why. If x is an interior point of some Q ∈F, then there are
points of both E and EC contained in Q and so x ∈Q ∈∂F (E) and there are no other
boxes from F which contain x. If x is not an interior point of any Q ∈F, then the interior
of the union of all the boxes from F which do contain x is an open set and therefore, must

A.3.
BASIC PROPERTIES
575
contain points of E and points from EC. If x ∈E, then one of these boxes must contain
points which are not in E since otherwise, x would fail to be in ∂E. Pick that box. It is in
∂F (E) and contains x. On the other hand, if x /∈E, one of these boxes must contain points
of E since otherwise, x would fail to be in ∂E. Pick that box. This shows that every set
from F which contains a point of ∂E shares this point with a box of ∂G (E) .
Let the boxes from ∂G (E) be {P1, · · ·, Pm} . Let S (Pi) denote those sets of F which
contain a point of ∂E in common with Pi. Then if Q ∈S (Pi) , either Q ⊆Pi or it intersects
Pi on one of its 2n faces. Therefore, the sum of the volumes of those boxes of S (Pi) which
intersect Pi on a particular face of Pi is no larger than v (Pi) . Consequently,
X
Q∈S(Pi)
v (Q) ≤2nv (Pi) + v (Pi) ,
the term v (Pi) accounting for those boxes which are contained in Pi. Therefore, for Q ∈F,
X
Q∩∂E̸=∅
v (Q) =
m
X
i=1
X
Q∈S(Pi)
v (Q) ≤
m
X
i=1
(2n + 1) v (Pi) < ε
from 1.13. This proves the corollary.
Theorem A.3.11 If a bounded set, E, has Jordan content 0, then E is a Jordan
(contented) set and if f is any bounded function deﬁned on E, then fXE ∈R (Rn) and
Z
E
f dV = 0.
Proof: Let ε > 0. Then let G be a grid such that
X
Q∩E̸=∅
v (Q) < ε.
Then every set of ∂G (E) contains a point of E so
X
Q∈∂G(E)
v (Q) ≤
X
Q∩E̸=∅
v (Q) < ε
and since ε was arbitrary, this shows from Theorem A.3.7 that E is a Jordan set. Now let
M be a positive number larger than all values of f, let m be a negative number smaller
than all values of f and let ε > 0 be given. Let G be a grid with
X
Q∩E̸=∅
v (Q) <
ε
1 + (M −m).
Then
UG (fXE) ≤
X
Q∩E̸=∅
Mv (Q) ≤
εM
1 + (M −m)
and
LG (fXE) ≥
X
Q∩E̸=∅
mv (Q) ≥
εm
1 + (M −m)
and so
UG (fXE) −LG (fXE)
≤
X
Q∩E̸=∅
Mv (Q) −
X
Q∩E̸=∅
mv (Q)
=
(M −m)
X
Q∩E̸=∅
v (Q) <
ε (M −m)
1 + (M −m) < ε.

576
THE THEORY OF THE RIEMANN INTEGRAL∗∗
This shows fXE ∈R (Rn) . Now also,
mε ≤
Z
fXE dV ≤Mε
and since ε is arbitrary, this shows
Z
E
f dV ≡
Z
fXE dV = 0
and proves the theorem.
Corollary A.3.12 If fXEi ∈R (Rn) for i = 1, 2, ···, r and for all i ̸= j, Ei ∩Ej is either
the empty set or a set of Jordan content 0, then letting F ≡∪r
i=1Ei, it follows fXF ∈R (Rn)
and
Z
fXF dV ≡
Z
F
f dV =
r
X
i=1
Z
Ei
f dV.
Proof: This is true if r = 1. Suppose it is true for r. It will be shown that it is true
for r + 1. Let Fr = ∪r
i=1Ei and let Fr+1 be deﬁned similarly. By the induction hypothesis,
fXFr ∈R (Rn) . Also, since Fr is a ﬁnite union of the Ei, it follows that Fr ∩Er+1 is either
empty or a set of Jordan content 0.
−fXFr∩Er+1 + fXFr + fXEr+1 = fXFr+1
and by Theorem A.3.11 each function on the left is in R (Rn) and the ﬁrst one on the left
has integral equal to zero. Therefore,
Z
fXFr+1 dV =
Z
fXFr dV +
Z
fXEr+1 dV
which by induction equals
r
X
i=1
Z
Ei
f dV +
Z
Er+1
f dV =
r+1
X
i=1
Z
Ei
f dV
and this proves the corollary.
What functions in addition to step functions are integrable? As in the case of integrals of
functions of one variable, this is an important question. It turns out the Riemann integrable
functions are characterized by being continuous except on a very small set. To begin with
it is necessary to deﬁne the oscillation of a function.
Deﬁnition A.3.13 Let f be a function deﬁned on Rn and let
ωf,r (x) ≡sup {|f (z) −f (y)| : z, y ∈B (x,r)} .
This is called the oscillation of f on B (x,r) . Note that this function of r is decreasing in
r. Deﬁne the oscillation of f as
ωf (x) ≡lim
r→0+ ωf,r (x) .
Note that as r decreases, the function, ωf,r (x) decreases. It is also bounded below by
0 and so the limit must exist and equals inf {ωf,r (x) : r > 0} . (Why?) Then the following
simple lemma whose proof follows directly from the deﬁnition of continuity gives the reason
for this deﬁnition.

A.3.
BASIC PROPERTIES
577
Lemma A.3.14 A function, f, is continuous at x if and only if ωf (x) = 0.
This concept of oscillation gives a way to deﬁne how discontinuous a function is at a
point. The discussion will depend on the following fundamental lemma which gives the
existence of something called the Lebesgue number.
Deﬁnition A.3.15 Let C be a set whose elements are sets of Rn and let K ⊆Rn.
The set, C is called a cover of K if every point of K is contained in some set of C. If the
elements of C are open sets, it is called an open cover.
Lemma A.3.16
Let K be sequentially compact and let C be an open cover of K. Then
there exists r > 0 such that whenever x ∈K, B(x, r) is contained in some set of C .
Proof: Suppose this is not so. Then letting rn = 1/n, there exists xn ∈K such that
B (xn, rn) is not contained in any set of C. Since K is sequentially compact, there is a
subsequence, xnk which converges to a point, x ∈K. But there exists δ > 0 such that
B (x, δ) ⊆U for some U ∈C. Let k be so large that 1/k < δ/2 and |xnk −x| < δ/2 also.
Then if z ∈B (xnk, rnk) , it follows
|z −x| ≤|z −xnk| + |xnk −x| < δ
2 + δ
2 = δ
and so B (xnk, rnk) ⊆U contrary to supposition. Therefore, the desired number exists after
all.
Theorem A.3.17 Let f be a bounded function which equals zero oﬀa bounded set
and let W denote the set of points where f fails to be continuous. Then f ∈R (Rn) if W
has content zero. That is, for all ε > 0 there exists a grid, G such that
X
Q∈GW
v (Q) < ε
(1.14)
where
GW ≡{Q ∈G : Q ∩W ̸= ∅} .
Proof: Let W have content zero. Also let |f (x)| < C/2 for all x ∈Rn, let ε > 0 be
given, and let G be a grid which satisﬁes 1.14. Since f equals zero oﬀsome bounded set,
there exists R such that f equals zero oﬀof B
¡
0, R
2
¢
. Thus W ⊆B
¡
0, R
2
¢
. Also note that if
G is a grid for which 1.14 holds, then this inequality continues to hold if G is replaced with
a reﬁned grid. Therefore, you may assume the diameter of every box in G which intersects
B (0, R) is less than R
3 and so all boxes of G which intersect the set where f is nonzero are
contained in B (0,R) . Since W is bounded, GW contains only ﬁnitely many boxes. Letting
Q ≡
n
Y
i=1
[ai, bi]
be one of these boxes, enlarge the box slightly as indicated in the following picture.
Q
¡
¡¡
˜Q

578
THE THEORY OF THE RIEMANN INTEGRAL∗∗
The enlarged box is an open set of the form,
eQ ≡
n
Y
i=1
(ai −ηi, bi + ηi)
where ηi is chosen small enough that if
n
Y
i=1
( bi + ηi −(ai −ηi)) ≡v
³
eQ
´
,
then
X
Q∈GW
v
³
eQ
´
< ε.
For each x ∈Rn, let rx be such that
ωf,rx (x) < ε + ωf (x) .
(1.15)
Now let C denote all intersections of the form eQ∩B (x,rx) such that x ∈B (0,R) so that C is
an open cover of the compact set, B (0,R). Let δ be a Lebesgue number for this open cover
of B (0,R) and let F be a reﬁnement of G such that every box in F has diameter less than δ.
Now let F1 consist of those boxes of F which have nonempty intersection with B (0,R/2) .
Thus all boxes of F1 are contained in B (0,R) and each one is contained in some set of C.
Now let CW be those open sets of C, eQ ∩B (x,rx) , for which x ∈W and let FW be those
sets of F1 which are subsets of some set of CW . Then
UF (f) −LF (f) =
X
Q∈FW
(MQ (f) −mQ (f)) v (Q)
+
X
Q∈F1\FW
(MQ (f) −mQ (f)) v (Q) .
If Q ∈F1 \ FW , then Q must be a subset of some set of C \CW since it is not in any set of
CW . Therefore, from 1.15 and the observation that x /∈W,
MQ (f) −mQ (f) ≤ε.
Therefore,
UF (f) −LF (f) ≤
X
Q∈FW
Cv (Q) +
X
Q∈F1\FW
εv (Q)
≤Cε + ε (2R)n .
Since ε is arbitrary, this proves the theorem.1
From Theorem A.3.7 you get a pretty good idea of what constitutes a contented set.
These sets are essentially those which have thin boundaries. Most sets you are likely to
think of will fall in this category. However, it is good to give speciﬁc examples of sets which
are contented.
1In fact one cannot do any better. It can be shown that if a function is Riemann integrable, then it must
be the case that for all ε > 0, 1.14 is satisﬁed for some grid, G. This along with what was just shown is
known as Lebesgue’s theorem after Lebesgue who discovered it in the early years of the twentieth century.
Actually, he also invented a far superior integral which has been the integral of serious mathematicians since
that time. To prove the converse of this theorem would take us too far in that direction and it would not
be reasonable to pay any more attention to this inferior integral.

A.3.
BASIC PROPERTIES
579
Theorem A.3.18 Suppose E is a bounded contented set in Rn and f, g : E →R
are two functions satisfying f (x) ≥g (x) for all x ∈E and fXE and gXE are both in
R (Rn) . Now deﬁne
P ≡{(x,xn+1) : x ∈E and g (x) ≤xn+1 ≤f (x)} .
Then P is a contented set in Rn+1.
Proof: Let G be a grid such that for k = f, g,
UG (k) −LG (k) < ε/4.
(1.16)
Also let K ≥Pm
j=1 vn (Qj) for all x ∈E.
Let the boxes of G which have nonempty
intersection with E be {Q1, · · ·, Qm} and let {ai}∞
i=−∞be a sequence on R, ai < ai+1 for
all i, which includes
MQj (fXE) +
ε
4mK , MQj (fXE) , MQj (gXE) , mQj (fXE) , mQj (gXE) , mQj (gXE) −
ε
4mK
for all j = 1, · · ·, m. Now deﬁne a grid on Rn+1 as follows.
G′ ≡{Q × [ai, ai+1] : Q ∈G, i ∈Z}
In words, this grid consists of all possible boxes of the form Q × [ai, ai+1] where Q ∈G
and ai is a term of the sequence just described. It is necessary to verify that for P ∈G′,
XP ∈R
¡
Rn+1¢
. This is done by showing that UG′ (XP )−LG′ (XP ) < ε and then noting that
ε > 0 was arbitrary. For G′ just described, denote by Q′ a box in G′. Thus Q′ = Q×[ai, ai+1]
for some i.
UG′ (XP ) −LG′ (XP )
≡
X
Q′∈G′
(MQ′ (XP ) −mQ′ (XP )) vn+1 (Q′)
=
∞
X
i=−∞
m
X
j=1
³
MQ′
j (XP ) −mQ′
j (XP )
´
vn (Qj) (ai+1 −ai)
and all sums are bounded because the functions, f and g are given to be bounded. Therefore,
there are no limit considerations needed here. Thus
UG′ (XP ) −LG′ (XP ) =
m
X
j=1
vn (Qj)
∞
X
i=−∞
¡
MQj×[ai,ai+1] (XP ) −mQj×[ai,ai+1] (XP )
¢
(ai+1 −ai) .
Consider the inside sum with the aid of the following picture.
MQj(g)
mQj(g)
q
a
Qj
xn+1
x
xn+1 = g(x)
xn+1 = f(x)
0
0
0
0
0
0
0
0
0

580
THE THEORY OF THE RIEMANN INTEGRAL∗∗
In this picture, the little rectangles represent the boxes Qj×[ai, ai+1] for ﬁxed j. The part
of P having x contained in Qj is between the two surfaces, xn+1 = g (x) and xn+1 = f (x)
and there is a zero placed in those boxes for which MQj×[ai,ai+1] (XP )−mQj×[ai,ai+1] (XP ) =
0. You see, XP has either the value of 1 or the value of 0 depending on whether (x, y) is con-
tained in P. For the boxes shown with 0 in them, either all of the box is contained in P or none
of the box is contained in P. Either way, MQj×[ai,ai+1] (XP )−mQj×[ai,ai+1] (XP ) = 0 on these
boxes. However, on the boxes intersected by the surfaces, the value of MQj×[ai,ai+1] (XP ) −
mQj×[ai,ai+1] (XP ) is 1 because there are points in this box which are not in P as well
as points which are in P. Because of the construction of G′ which included all values of
MQj (fXE) +
ε
4mK , MQj (fXE) , MQj (gXE) , mQj (fXE) , mQj (gXE) for all j = 1, · · ·, m,
∞
X
i=−∞
¡
MQj×[ai,ai+1] (XP ) −mQj×[ai,ai+1] (XP )
¢
(ai+1 −ai) ≤
X
{i:mQj (g)≤ai<MQj (g)}
1 (ai+1 −ai) +
X
{i:mQj (f)≤ai<MQj (f)}
1 (ai+1 −ai)
³
MQj (fXE) +
ε
4mK −MQj (fXE)
´
+
³
mQj (gXE) −
³
mQj (gXE) −
ε
4mK
´´
=
¡
MQj (gXE) −mQj (gXE)
¢
+
¡
MQj (fXE) −mQj (fXE)
¢
+ ε
2m


m
X
j=1
v (Qj)


−1
.
(Note the inequality.) The last two terms which add to
ε
2m
³Pm
j=1 v (Qj)
´−1
come from the
case where ai = MQj (f) or ai+1 = mQj (f). Therefore, by 1.16,
UG′ (XP ) −LG′ (XP ) ≤
m
X
j=1
vn (Qj)
£¡
MQj (gXE) −mQj (gXE)
¢
+
¡
MQj (fXE) −mQj (fXE)
¢¤
+
m
X
j=1
v (Qj) ε
2m


m
X
j=1
v (Qj)


−1
=
UG (f) −LG (f) + UG (g) −LG (g) + ε
2
<
ε
4 + ε
4 + ε
2 = ε.
Since ε > 0 is arbitrary, this proves the theorem.
Corollary A.3.19 Suppose f and g are continuous functions deﬁned on E, a contented
set in Rn and that g (x) ≤f (x) for all x ∈E. Then
P ≡{(x,xn+1) : x ∈E and g (x) ≤xn+1 ≤f (x)}
is a contented set in Rn.
Proof: Extend f and g to equal 0 oﬀE. The set of discontinuities of f and g is contained
in ∂E and Corollary A.3.10 on Page 574 implies this is a set of content 0.
Therefore,
from Theorem A.3.17, for k = f, g, it follows that kXE is in R (Rn) because the set of

A.4.
ITERATED INTEGRALS
581
discontinuities is contained in ∂E. The conclusion now follows from Theorem A.3.18. This
proves the corollary.
As an example of how this can be applied, it is obvious a closed interval is a contented
set in R. Therefore, if f, g are two continuous functions with f (x) ≥g (x) for x ∈[a, b] , it
follows from the above theorem or its corollary that the set,
P1 ≡{(x, y) : g (x) ≤y ≤f (x)}
is a contented set in R2. Now using the theorem and corollary again, suppose f1 (x, y) ≥
g1 (x, y) for (x, y) ∈P1 and f, g are continuous. Then the set
P2 ≡{(x, y, z) : g1 (x, y) ≤z ≤f1 (x, y)}
is a contented set in R3. Clearly you can continue this way obtaining examples of contented
sets.
Note that as a special case of Corollary A.3.4 on Page 571, it follows that every box is
a contented set.
A.4
Iterated Integrals
To evaluate an n dimensional Riemann integral, one uses iterated integrals. Formally, an
iterated integral is deﬁned as follows. For f a function deﬁned on Rn+m,
y →f (x, y)
is a function of y for each x ∈Rn+m. Therefore, it might be possible to integrate this
function of y and write
Z
Rm f (x, y) dVy.
Now the result is clearly a function of x and so, it might be possible to integrate this and
write
Z
Rn
Z
Rm f (x, y) dVy dVx.
This symbol is called an iterated integral, because it involves the iteration of two lower
dimensional integrations. Under what conditions are the two iterated integrals equal to the
integral
Z
Rn+m f (z) dV ?
Deﬁnition A.4.1 Let G be a grid on Rn+m deﬁned by the n + m sequences,
©
αi
k
ª∞
k=−∞i = 1, · · ·, n + m.
Let Gn be the grid on Rn obtained by considering only the ﬁrst n of these sequences and
let Gm be the grid on Rm obtained by considering only the last m of the sequences. Thus a
typical box in Gm would be
n+m
Y
i=n+1
£
αi
ki, αi
ki+1
¤
, ki ≥n + 1
and a box in Gn would be of the form
n
Y
i=1
£
αi
ki, αi
ki+1
¤
, ki ≤n.

582
THE THEORY OF THE RIEMANN INTEGRAL∗∗
Lemma A.4.2 Let G, Gn, and Gmbe the grids deﬁned above. Then
G = {R × P : R ∈Gn and P ∈Gm} .
Proof: If Q ∈G, then Q is clearly of this form. On the other hand, if R × P is one of
the sets described above, then from the above description of R and P, it follows R × P is
one of the sets of G. This proves the lemma.
Now let G be a grid on Rn+m and suppose
φ (z) =
X
Q∈G
φQXQ′ (z)
(1.17)
where φQ equals zero for all but ﬁnitely many Q. Thus φ is a step function. Recall that for
Q =
n+m
Y
i=1
[ai, bi] , Q′ ≡
n+m
Y
i=1
(ai, bi]
Letting (x, y) = z, Lemma A.4.2 implies
φ (z) = φ (x, y) =
X
R∈Gn
X
P ∈Gm
φR×P XR′×P ′ (x, y)
=
X
R∈Gn
X
P ∈Gm
φR×P XR′ (x) XP ′ (y) .
(1.18)
For a function of two variables, h, denote by h (·, y) the function, x →h (x, y) and
h (x, ·) the function y →h (x, y) . The following lemma is a preliminary version of Fubini’s
theorem.
Lemma A.4.3 Let φ be a step function as described in 1.17. Then
φ (x, ·) ∈R (Rm) ,
(1.19)
Z
Rm φ (·, y) dVy ∈R (Rn) ,
(1.20)
and
Z
Rn
Z
Rm φ (x, y) dVy dVx =
Z
Rn+m φ (z) dV.
(1.21)
Proof: To verify 1.19, note that φ (x, ·) is the step function
φ (x, y) =
X
P ∈Gm
φR×P XP ′ (y) .
Where x ∈R′. By Corollary A.3.4, this veriﬁes 1.19. From the description in 1.18 and this
corollary,
Z
Rm φ (x, y) dVy =
X
R∈Gn
X
P ∈Gm
φR×P XR′ (x) v (P)
=
X
R∈Gn
Ã X
P ∈Gm
φR×P v (P)
!
XR′ (x) ,
(1.22)
another step function. Therefore, Corollary A.3.4 applies again to verify 1.20. Finally, 1.22
implies
Z
Rn
Z
Rm φ (x, y) dVy dVx =
X
R∈Gn
X
P ∈Gm
φR×P v (P) v (R)

A.4.
ITERATED INTEGRALS
583
=
X
Q∈G
φQv (Q) =
Z
Rn+m φ (z) dV.
and this proves the lemma.
From 1.22,
MR′
1
µZ
Rm φ (·, y) dVy
¶
≡sup
( X
R∈Gn
Ã X
P ∈Gm
φR×P v (P)
!
XR′ (x) : x ∈R′
1
)
=
X
P ∈Gm
φR1×P v (P)
(1.23)
because
R
Rm φ (·, y) dVy has the constant value given in 1.23 for x ∈R′
1. Similarly,
mR′
1
µZ
Rm φ (·, y) dVy
¶
≡inf
( X
R∈Gn
Ã X
P ∈Gm
φR×P v (P)
!
XR′ (x) : x ∈R′
1
)
=
X
P ∈Gm
φR1×P v (P) .
(1.24)
Theorem A.4.4 (Fubini) Let f ∈R (Rn+m) and suppose also that f (x, ·) ∈
R (Rm) for each x. Then
Z
Rm f (·, y) dVy ∈R (Rn)
(1.25)
and
Z
Rn+m f (z) dV =
Z
Rn
Z
Rm f (x, y) dVy dVx.
(1.26)
Proof: Let G be a grid such that UG (f) −LG (f) < ε and let Gn and Gm be as deﬁned
above. Let
φ (z) ≡
X
Q∈G
MQ′ (f) XQ′ (z) , ψ (z) ≡
X
Q∈G
mQ′ (f) XQ′ (z) .
By Corollary A.3.4, and the observation that MQ′ (f) ≤MQ (f) and mQ′ (f) ≥mQ (f) ,
UG (f) ≥
Z
φ dV, LG (f) ≤
Z
ψ dV.
Also f (z) ∈(ψ (z) , φ (z)) for all z. Thus from 1.23,
MR′
µZ
Rm f (·, y) dVy
¶
≤MR′
µZ
Rm φ (·, y) dVy
¶
=
X
P ∈Gm
MR′×P ′ (f) v (P)
and from 1.24,
mR′
µZ
Rm f (·, y) dVy
¶
≥mR′
µZ
Rm ψ (·, y) dVy
¶
=
X
P ∈Gm
mR′×P ′ (f) v (P) .
Therefore,
X
R∈Gn
·
MR′
µZ
Rm f (·, y) dVy
¶
−mR′
µZ
Rm f (·, y) dVy
¶¸
v (R) ≤

584
THE THEORY OF THE RIEMANN INTEGRAL∗∗
X
R∈Gn
X
P ∈Gm
[MR′×P ′ (f) −mR′×P ′ (f)] v (P) v (R) ≤UG (f) −LG (f) < ε.
This shows, from Lemma A.3.5 and the Riemann criterion, that
R
Rm f (·, y) dVy ∈R (Rn) .
It remains to verify 1.26. First note
Z
Rn+m f (z) dV ∈[LG (f) , UG (f) ] .
Next, by Lemma A.4.3,
LG (f) ≤
Z
Rn+m ψ dV =
Z
Rn
Z
Rm ψ dVy dVx ≤
Z
Rn
Z
Rm f (x, y) dVy dVx
≤
Z
Rn
Z
Rm φ (x, y) dVy dVx =
Z
Rn+m φ dV ≤UG (f) .
Therefore,
¯¯¯¯
Z
Rn
Z
Rm f (x, y) dVy dVx −
Z
Rn+m f (z) dV
¯¯¯¯ ≤ε
and since ε > 0 is arbitrary, this proves Fubini’s theorem2.
Corollary A.4.5 Suppose E is a bounded contented set in Rn and let φ, ψ be continuous
functions deﬁned on E such that φ (x) ≥ψ (x) . Also suppose f
is a continuous bounded
function deﬁned on the set,
P ≡{(x, y) : ψ (x) ≤y ≤φ (x)} ,
It follows fXP ∈R
¡
Rn+1¢
and
Z
P
f dV =
Z
E
Z φ(x)
ψ(x)
f (x, y) dy dVx.
Proof: Since f is continuous, there is no problem in writing f (x, ·) X[ψ(x),φ(x)] (·) ∈
R
¡
R1¢
. Also, fXP ∈R
¡
Rn+1¢
because P is contented thanks to Corollary A.3.19. There-
fore, by Fubini’s theorem
Z
P
f dV
=
Z
Rn
Z
R
fXP dy dVx
=
Z
E
Z φ(x)
ψ(x)
f (x, y) dy dVx
proving the corollary.
Other versions of this corollary are immediate and should be obvious whenever encoun-
tered.
A.5
The Change Of Variables Formula
First recall Theorem 22.2.2 on Page 394 which is listed here for convenience.
2Actually, Fubini’s theorem usually refers to a much more profound result in the theory of Lebesgue
integration.

A.5.
THE CHANGE OF VARIABLES FORMULA
585
Theorem A.5.1 Let h : U →Rn be a C1 function with h (0) = 0,Dh (0)−1 exists.
Then there exists an open set, V ⊆U containing 0, ﬂips, F1, · · ·, Fn−1, and primitive
functions, Gn, Gn−1, · · ·, G1 such that for x ∈V,
h (x) = F1 ◦· · · ◦Fn−1 ◦Gn ◦Gn−1 ◦· · · ◦G1 (x) .
Also recall Theorem 16.2.13 on Page 295.
Theorem A.5.2 Let φ : [a, b] →[c, d] be one to one and suppose φ′ exists and is
continuous on [a, b] . Then if f is a continuous function deﬁned on [a, b] ,
Z d
c
f (s) ds =
Z b
a
f (φ (t))
¯¯φ′ (t)
¯¯ dt
The following is a simple corollary to this theorem.
Corollary A.5.3 Let φ : [a, b] →[c, d] be one to one and suppose φ′ exists and is
continuous on [a, b] . Then if f is a continuous function deﬁned on [a, b] ,
Z
R
X[a,b]
¡
φ−1 (x)
¢
f (x) dx =
Z
R
X[a,b] (t) f (φ (t))
¯¯φ′ (t)
¯¯ dt
Lemma A.5.4 Let h : V →Rn be a C1 function and suppose H is a compact subset of
V. Then there exists a constant, C independent of x ∈H such that
|Dh (x) v| ≤C |v| .
Proof: Consider the compact set, H × ∂B (0, 1) ⊆R2n. Let f : H × ∂B (0, 1) →R be
given by f (x, v) = |Dh (x) v| . Then let C denote the maximum value of f. It follows that
for v ∈Rn,
¯¯¯¯Dh (x) v
|v|
¯¯¯¯ ≤C
and so the desired formula follows when you multiply both sides by |v|.
Deﬁnition A.5.5 Let A be an open set. Write Ck (A; Rn) to denote a Ck function
whose domain is A and whose range is in Rn. Let U be an open set in Rn. Then h ∈
Ck ¡
U; Rn¢
if there exists an open set, V ⊇U and a function, g ∈C1 (V ; Rn) such that
g = h on U. f ∈Ck ¡
U
¢
means the same thing except that f has values in R.
Theorem A.5.6 Let U be a bounded open set such that ∂U has zero content and
let h ∈C
¡
U; Rn¢
be one to one and Dh (x)−1 exists for all x ∈U. Then h (∂U) = ∂(h (U))
and ∂(h (U)) has zero content.
Proof: Let x ∈∂U and let g = h where g is a C1 function deﬁned on an open set
containing U. By the inverse function theorem, g is locally one to one and an open mapping
near x. Thus g (x) = h (x) and is in an open set containing points of g (U) and points of
g
¡
U C¢
. These points of g
¡
U C¢
cannot equal any points of h (U) because g is one to one
locally. Thus h (x) ∈∂(h (U)) and so h (∂U) ⊆∂(h (U)) . Now suppose y ∈∂(h (U)) .
By the inverse function theorem y cannot be in the open set h (U) . Since y ∈∂(h (U)),
every ball centered at y contains points of h (U) and so y ∈h (U)\h (U) . Thus there exists
a sequence, {xn} ⊆U such that h (xn) →y. But then, by the inverse function theorem,
xn →h−1 (y) and so h−1 (y) ∈∂U. Therefore, y ∈h (∂U) and this proves the two sets are
equal. It remains to verify the claim about content.

586
THE THEORY OF THE RIEMANN INTEGRAL∗∗
First let H denote a compact set whose interior contains U which is also in the interior
of the domain of g. Now since ∂U has content zero, it follows that for ε > 0 given, there
exists a grid, G such that if G′ are those boxes of G which have nonempty intersection with
∂U, then
X
Q∈G′
v (Q) < ε.
and by reﬁning the grid if necessary, no box of G has nonempty intersection with both U
and HC. Reﬁning this grid still more, you can also assume that for all boxes in G′,
li
lj
< 2
where li is the length of the ith side. (Thus the boxes are not too far from being cubes.)
Let C be the constant of Lemma A.5.4 applied to g on H.
Now consider one of these boxes, Q ∈G′. If x, y ∈Q, it follows from the chain rule that
g (y) −g (x) =
Z 1
0
Dg (x+t (y −x)) (y −x) dt
By Lemma A.5.4 applied to H
|g (y) −g (x)|
≤
Z 1
0
|Dg (x+t (y −x)) (y −x)| dt
≤
C
Z 1
0
|x −y| dt ≤C diam (Q)
=
C
Ã n
X
i=1
l2
i
!1/2
≤C√nL
where L is the length of the longest side of Q. Thus diam (g (Q)) ≤C√nL and so g (Q) is
contained in a cube having sides equal to C√nL and volume equal to
Cnnn/2Ln ≤Cnnn/22nl1l2 · · · ln = Cnnn/22nv (Q) .
Denoting by PQ this cube, it follows
h (∂U) ⊆∪Q∈G′v (PQ)
and
X
Q∈G′
v (PQ) ≤Cnnn/22n X
Q∈G′
v (Q) < εCnnn/22n.
Since ε > 0 is arbitrary, this shows h (∂U) has content zero as claimed.
Theorem A.5.7 Suppose f ∈C
¡
U
¢
where U is a bounded open set with ∂U having
content 0. Then fXU ∈R (Rn).
Proof: Let H be a compact set whose interior contains U which is also contained in
the domain of g where g is a continuous functions whose restriction to U equals f. Consider
gXU, a function whose set of discontinuities has content 0. Then gXU = fXU ∈R (Rn) as
claimed. This is by the big theorem which tells which functions are Riemann integrable.
The following lemma is obvious from the deﬁnition of the integral.

A.5.
THE CHANGE OF VARIABLES FORMULA
587
Lemma A.5.8 Let U be a bounded open set and let fXU ∈R (Rn) . Then
Z
f (x + p) XU−p (x) dx =
Z
f (x) XU (x) dx
A few more lemmas are needed.
Lemma A.5.9 Let S be a nonempty subset of Rn. Deﬁne
f (x) ≡dist (x, S) ≡inf {|x −y| : y ∈S} .
Then f is continuous.
Proof: Consider |f (x) −f (x1)|and suppose without loss of generality that f (x1) ≥
f (x) . Then choose y ∈S such that f (x) + ε > |x −y| . Then
|f (x1) −f (x)|
=
f (x1) −f (x) ≤f (x1) −|x −y| + ε
≤
|x1 −y| −|x −y| + ε
≤
|x −x1| + |x −y| −|x −y| + ε
=
|x −x1| + ε.
Since ε is arbitrary, it follows that |f (x1) −f (x)| ≤|x −x1| and this proves the lemma.
Theorem A.5.10 (Urysohn’s lemma for Rn) Let H be a closed subset of an open
set, U. Then there exists a continuous function, g : Rn →[0, 1] such that g (x) = 1 for all
x ∈H and g (x) = 0 for all x /∈U.
Proof: If x /∈C, a closed set, then dist (x, C) > 0 because if not,
there would exist
a sequence of points of C converging to x and it would follow that x ∈C. Therefore,
dist (x, H) + dist
¡
x, U C¢
> 0 for all x ∈Rn. Now deﬁne a continuous function, g as
g (x) ≡
dist
¡
x, U C¢
dist (x, H) + dist (x, U C).
It is easy to see this veriﬁes the conclusions of the theorem and this proves the theorem.
Deﬁnition A.5.11 Deﬁne spt(f) (support of f) to be the closure of the set {x :
f(x) ̸= 0}. If V is an open set, Cc(V ) will be the set of continuous functions f, deﬁned on
Rn having spt(f) ⊆V .
Deﬁnition A.5.12 If K is a compact subset of an open set, V , then K ≺φ ≺V if
φ ∈Cc(V ), φ(K) = {1}, φ(Rn) ⊆[0, 1].
Also for φ ∈Cc(Rn), K ≺φ if
φ(Rn) ⊆[0, 1] and φ(K) = 1.
and φ ≺V if
φ(Rn) ⊆[0, 1] and spt(φ) ⊆V.

588
THE THEORY OF THE RIEMANN INTEGRAL∗∗
Theorem A.5.13 (Partition of unity) Let K be a compact subset of Rn and sup-
pose
K ⊆V = ∪n
i=1Vi, Vi open.
Then there exist ψi ≺Vi with
n
X
i=1
ψi(x) = 1
for all x ∈K.
Proof: Let K1 = K\∪n
i=2Vi. Thus K1 is compact because it is the intersection of a closed
set with a compact set and K1 ⊆V1. Let K1 ⊆W1 ⊆W 1 ⊆V1 with W 1compact. To obtain
W1, use Theorem A.5.10 to get f such that K1 ≺f ≺V1 and let W1 ≡{x : f (x) ̸= 0} . Thus
W1, V2, · · ·Vn covers K and W 1 ⊆V1. Let K2 = K \ (∪n
i=3Vi ∪W1). Then K2 is compact
and K2 ⊆V2. Let K2 ⊆W2 ⊆W 2 ⊆V2 W 2 compact. Continue this way ﬁnally obtaining
W1, ···, Wn, K ⊆W1 ∪···∪Wn, and W i ⊆Vi W i compact. Now let W i ⊆Ui ⊆U i ⊆Vi , U i
compact.
Wi
Ui Vi
By Theorem A.5.10, there exist functions, φi, γ such that U i ≺φi ≺Vi, ∪n
i=1W i ≺γ ≺
∪n
i=1Ui. Deﬁne
ψi(x) =
½ γ(x)φi(x)/ Pn
j=1 φj(x) if Pn
j=1 φj(x) ̸= 0,
0 if Pn
j=1 φj(x) = 0.
If x is such that Pn
j=1 φj(x) = 0, then x /∈∪n
i=1U i. Consequently γ(y) = 0 for all y near x
and so ψi(y) = 0 for all y near x. Hence ψi is continuous at such x. If Pn
j=1 φj(x) ̸= 0, this
situation persists near x and so ψi is continuous at such points. Therefore ψi is continuous.
If x ∈K, then γ(x) = 1 and so Pn
j=1 ψj(x) = 1. Clearly 0 ≤ψi (x) ≤1 and spt(ψj) ⊆Vj.
This proves the theorem.
The next lemma contains the main ideas.
Lemma A.5.14 Let U be a bounded open set with ∂U having content 0. Also let h ∈
C1 ¡
U; Rn¢
be one to one on U with Dh (x)−1 exists for all x ∈U. Let f ∈C
¡
U
¢
be
nonnegative. Then
Z
Xh(U) (z) f (z) dVn =
Z
XU (x) f (h (x)) |det Dh (x)| dVn
Proof: Let ε > 0 be given. Then by Theorem A.5.7,
x →XU (x) f (h (x)) |det Dh (x)|
is Riemann integrable. Therefore, there exists a grid, G such that, letting
g (x) = XU (x) f (h (x)) |det Dh (x)| ,
LG (g) + ε > UG (g) .

A.5.
THE CHANGE OF VARIABLES FORMULA
589
Let K denote the union of the boxes, Q of G for which mQ (g) > 0. Thus K is a compact
subset of U and it is only the terms from these boxes which contribute anything nonzero to
the lower sum. By Theorem 22.2.2 on Page 394 which is stated above, it follows that for
p ∈K, there exists an open set contained in U which contains p,Op such that for x ∈Op−p,
h (x + p) −h (p) = F1 ◦· · · ◦Fn−1 ◦Gn ◦· · · ◦G1 (x)
where the Gi are primitive functions and the Fj are ﬂips. Finitely many of these open sets,
{Oj}q
j=1 cover K. Let the distinguished point for Oj be denoted by pj. Now reﬁne G if
necessary such that the diameter of every cell of the new G which intersects U is smaller
than a Lebesgue number for this open cover. Denote by G′ those boxes of G whose union
equals the set, K. Thus every box of G′ is contained in one of these Oj. By Theorem A.5.13
there exists a partition of unity,
©
ψj
ª
on h (K) such that ψj ≺h (Oj). Then
LG (g)
≤
X
Q∈G′
Z
XQ (x) f (h (x)) |det Dh (x)| dx
=
X
Q∈G′
q
X
j=1
Z
XQ (x)
¡
ψjf
¢
(h (x)) |det Dh (x)| dx.
(1.27)
Consider the term
R
XQ (x)
¡
ψjf
¢
(h (x)) |det Dh (x)| dx. By Lemma A.5.8 and Fubini’s the-
orem this equals
Z
Rn−1
Z
R
XQ−pj (x)
¡
ψjf
¢
(h (pi) + F1 ◦· · · ◦Fn−1 ◦Gn ◦· · · ◦G1 (x)) ·
|DF (Gn ◦· · · ◦G1 (x))| |DGn (Gn−1 ◦· · · ◦G1 (x))| |DGn−1 (Gn−2 ◦· · · ◦G1 (x))| ·
· · · |DG2 (G1 (x))| |DG1 (x)| dx1dVn−1.
(1.28)
Here dVn−1 is with respect to the variables, x2, · · ·, xn. Also F denotes F1 ◦· · · ◦Fn−1. Now
G1 (x) = (α (x) , x2, · · ·, xn)T
and is one to one. Therefore, ﬁxing x2, · · ·, xn, x1 →α (x) is one to one. Also, DG1 (x) =
∂α
∂x1 (x) . Fixing x2, · · ·, xn, change the variable,
y1 = α (x1, x2, · · ·, xn) .
Thus
x = (x1, x2, · · ·, xn)T = G−1
1
(y1, x2, · · ·, xn) ≡G−1
1
(x′)
Then in 1.28 you can use Corollary A.5.3 to write 1.28 as
Z
Rn−1
Z
R
XQ−pj
¡
G−1
1
(x′)
¢ ¡
ψjf
¢ ¡
h (pi) + F1 ◦· · · ◦Fn−1 ◦Gn ◦· · · ◦G1
¡
G−1
1
(x′)
¢¢
·
¯¯DF
¡
Gn ◦· · · ◦G1
¡
G−1
1
(x′)
¢¢¯¯ ¯¯DGn
¡
Gn−1 ◦· · · ◦G1
¡
G−1
1
(x′)
¢¢¯¯ ·
¯¯DGn−1
¡
Gn−2 ◦· · · ◦G1
¡
G−1
1
(x′)
¢¢¯¯ · · ·
¯¯DG2
¡
G1
¡
G−1
1
(x′)
¢¢¯¯ dy1dVn−1
(1.29)
which reduces to
Z
Rn XQ−pj
¡
G−1
1
(x′)
¢ ¡
ψjf
¢
(h (pi) + F1 ◦· · · ◦Fn−1 ◦Gn ◦· · · ◦G2 (x′)) ·
|DF (Gn ◦· · · ◦G2 (x′))| |DGn (Gn−1 ◦· · · ◦G2 (x′))| |DGn−1 (Gn−2 ◦· · · ◦G2 (x′))| ·
· · · |DG2 (x′)| dVn.
(1.30)

590
THE THEORY OF THE RIEMANN INTEGRAL∗∗
Now use Fubini’s theorem again to make the inside integral taken with respect to x2. Exactly
the same process yields
Z
Rn−1
Z
R
XQ−pj
¡
G−1
1
◦G−1
2
(x′′)
¢ ¡
ψjf
¢
(h (pi) + F1 ◦· · · ◦Fn−1 ◦Gn ◦· · · ◦G3 (x′′)) ·
|DF (Gn ◦· · · ◦G3 (x′′))| |DGn (Gn−1 ◦· · · ◦G3 (x′′))| |DGn−1 (Gn−2 ◦· · · ◦G3 (x′′))| ·
· · ·dy2dVn−1.
(1.31)
Now F is just a composition of ﬂips and so |DF (Gn ◦· · · ◦G3 (x′′))| = 1 and so this term
can be replaced with 1. Continuing this process, eventually yields an expression of the form
Z
Rn XQ−pj
¡
G−1
1
◦· · · ◦G−1
n−2 ◦G−1
n−1 ◦G−1
n
◦F−1 (y)
¢ ¡
ψjf
¢
(h (pi) + y) dVn.
(1.32)
Denoting by G−1 the expression, G−1
1
◦· · · ◦G−1
n−2 ◦G−1
n−1 ◦G−1
n ,
XQ−pj
¡
G−1
1
◦· · · ◦G−1
n−2 ◦G−1
n−1 ◦G−1
n
◦F−1 (y)
¢
= 1
exactly when G−1 ◦F−1 (y) ∈Q −pj. Now recall that h (pj + x) −h (pj) = F ◦G (x) and
so the above holds exactly when
y
=
h
¡
pj + G−1 ◦F−1 (y)
¢
−h (pj) ∈h (pj + Q −pj) −h (pj)
=
h (Q) −h (pj) .
Thus 1.32 reduces to
Z
Rn Xh(Q)−h(pj) (y)
¡
ψjf
¢
(h (pi) + y) dVn =
Z
Rn Xh(Q) (z)
¡
ψjf
¢
(z) dVn.
It follows from 1.27
UG (g) −ε
≤
LG (g) ≤
X
Q∈G′
Z
XQ (x) f (h (x)) |det Dh (x)| dx
=
X
Q∈G′
q
X
j=1
Z
XQ (x)
¡
ψjf
¢
(h (x)) |det Dh (x)| dx
=
X
Q∈G′
q
X
j=1
Z
Rn Xh(Q) (z)
¡
ψjf
¢
(z) dVn
=
X
Q∈G′
Z
Rn Xh(Q) (z) f (z) dVn ≤
Z
Xh(U) (z) f (z) dVn
which implies the inequality,
Z
XU (x) f (h (x)) |det Dh (x)| dVn ≤
Z
Xh(U) (z) f (z) dVn
But now you can use the same information just derived to obtain equality. x = h−1 (z) and
so from what was just done,
Z
XU (x) f (h (x)) |det Dh (x)| dVn

A.6.
SOME OBSERVATIONS
591
=
Z
Xh−1(h(U)) (x) f (h (x)) |det Dh (x)| dVn
≥
Z
Xh(U) (z) f (z)
¯¯det Dh
¡
h−1 (z)
¢¯¯ ¯¯det Dh−1 (z)
¯¯ dVn
=
Z
Xh(U) (z) f (z) dVn
from the chain rule. In fact,
I = Dh
¡
h−1 (z)
¢
Dh−1 (z)
and so
1 =
¯¯det Dh
¡
h−1 (z)
¢¯¯ ¯¯det Dh−1 (z)
¯¯ .
This proves the lemma.
The change of variables theorem follows.
Theorem A.5.15 Let U be a bounded open set with ∂U having content 0. Also
let h ∈C1 ¡
U; Rn¢
be one to one on U with Dh (x)−1 exists for all x ∈U. Let f ∈C
¡
U
¢
.
Then
Z
Xh(U) (z) f (z) dz =
Z
XU (x) f (h (x)) |det Dh (x)| dx
Proof: You note that the formula holds for f + ≡|f|+f
2
and f −≡|f|−f
2
. Now f =
f + −f −and so
Z
Xh(U) (z) f (z) dz
=
Z
Xh(U) (z) f + (z) dz −
Z
Xh(U) (z) f −(z) dz
=
Z
XU (x) f + (h (x)) |det Dh (x)| dx −
Z
XU (x) f −(h (x)) |det Dh (x)| dx
=
Z
XU (x) f (h (x)) |det Dh (x)| dx.
A.6
Some Observations
Some of the above material is very technical. This is because it gives complete answers to
the fundamental questions on existence of the integral and related theoretical considerations.
However, most of the diﬃculties are artifacts. They shouldn’t even be considered! It was
realized early in the twentieth century that these diﬃculties occur because, from the point
of view of mathematics, this is not the right way to deﬁne an integral! Better results are
obtained much more easily using the Lebesgue integral. Many of the technicalities related
to Jordan content disappear almost magically when the right integral is used. However, the
Lebesgue integral is more abstract than the Riemann integral and it is not traditional to
consider it in a beginning calculus course. If you are interested in the fundamental properties
of the integral and the theory behind it, you should abandon the Riemann integral which is
an antiquated relic and begin to study the integral of the last century. An introduction to
it is in [21]. Another very good source is [11]. This advanced calculus text does everything
in terms of the Lebesgue integral and never bothers to struggle with the inferior Riemann
integral. A more general treatment is found in [17], [18], [22], and [19]. There is also a still
more general integral called the generalized Riemann integral. A recent book on this subject
is [5]. It is far easier to deﬁne than the Lebesgue integral but the convergence theorems are
much harder to prove. An introduction is also in [17].

592
THE THEORY OF THE RIEMANN INTEGRAL∗∗

Bibliography
[1] Apostol, T. M., Calculus second edition, Wiley, 1967.
[2] Apostol T. Calculus Volume II Second edition, Wiley 1969.
[3] Apostol, T. M., Mathematical Analysis, Addison Wesley Publishing Co., 1974.
[4] Baker, Roger, Linear Algebra, Rinton Press 2001.
[5] Bartle R.G., A Modern Theory of Integration, Grad. Studies in Math., Amer. Math.
Society, Providence, RI, 2000.
[6] Chahal J. S. , Historical Perspective of Mathematics 2000 B.C. - 2000 A.D.
[7] Davis H. and Snider A., Vector Analysis Wm. C. Brown 1995.
[8] D’Angelo, J. and West D. Mathematical Thinking Problem Solving and Proofs,
Prentice Hall 1997.
[9] Edwards C.H. Advanced Calculus of several Variables, Dover 1994.
[10] Fitzpatrick P. M., Advanced Calculus a course in Mathematical Analysis, PWS Pub-
lishing Company 1996.
[11] Fleming W., Functions of Several Variables, Springer Verlag 1976.
[12] Greenberg, M. Advanced Engineering Mathematics, Second edition, Prentice Hall,
1998
[13] Gurtin M. An introduction to continuum mechanics, Academic press 1981.
[14] Hardy G., A Course Of Pure Mathematics, Tenth edition, Cambridge University Press
1992.
[15] Horn R. and Johnson C. matrix Analysis, Cambridge University Press, 1985.
[16] Karlin S. and Taylor H. A First Course in Stochastic Processes, Academic Press,
1975.
[17] Kuttler K. L., Basic Analysis, Rinton
[18] Kuttler K.L., Modern Analysis CRC Press 1998.
[19] Lang S. Real and Functional analysis third edition Springer Verlag 1993. Press, 2001.
[20] Nobel B. and Daniel J. Applied Linear Algebra, Prentice Hall, 1977.
[21] Rudin, W., Principles of mathematical analysis, McGraw Hill third edition 1976
593

594
BIBLIOGRAPHY
[22] Rudin W., Real and Complex Analysis, third edition, McGraw-Hill, 1987.
[23] Salas S. and Hille E., Calculus One and Several Variables, Wiley 1990.
[24] Sears and Zemansky, University Physics, Third edition, Addison Wesley 1963.
[25] Tierney John, Calculus and Analytic Geometry, fourth edition, Allyn and Bacon,
Boston, 1969.

Index
C1, 374
Ck, 374
∆, 505
∇2, 505
adjugate, 204, 238
agony, pain and suﬀering, 409
algebraic multiplicity, 215
angle between planes, 74
angle between vectors, 40
angular velocity, 55
angular velocity vector, 300
arc length, 265, 295, 459
area of a parallelogram, 49
augmented matrix, 85
back substitution, 84
balance of momentum, 514
barallelepiped
volume, 52
bases, 170
basic variables, 92
basis, 170
binormal, 289
block matrix, 145
bounded, 329
box product, 52
Cartesian coordinates, 20
catenary, 347
Cauchy Schwarz inequality, 42
Cauchy sequence, 331
Cauchy sequence, 331
Cauchy stress, 516
Cayley Hamilton theorem, 241
center of mass, 57, 445, 448
center of mass of a plate , 411
center of mass of a surface, 493
centroid, 449
chain rule, 385
change of variables formula, 433
characteristic equation, 211
characteristic polynomial, 241
characteristic value, 210
circular helix, 290
classical adjoint, 204
closed set, 308
cofactor, 200, 236
cofactor matrix, 200
complement, 308
complex eigenvalues, 227
component, 35, 63
component of a force, 46
components of a matrix, 122
conformable, 126
conservation of linear momentum, 278
conservation of mass, 514
conservative, 468, 533
consistent, 94
constitutive laws, 519
contented set, 573
continuity
limit of a sequence, 333
continuous function, 311
continuous functions
properties, 313
converge, 331
Coordinates, 19
Cramer’s rule, 238
critical point, 357
cross product, 49
area of parallelogram, 49
coordinate description, 50
distributive law, 57
geometric description, 49
limits, 318
curl, 505
curvature, 282, 289
defective, 216
defective eigenvalue, 216
deformation gradient, 515
density and mass, 410
density with respect to area, 493
dependent, 116
595

596
INDEX
derivative of a function, 263
determinant, 232
product, 235
transpose, 233
diagonalizable, 219
diameter, 329
diﬀerence quotient, 263
diﬀerentiable matrix, 297
diﬀerentiation rules, 269
directed line segment, 30
direction cosines, 44
direction vector, 28, 30
directional derivative, 319
distance formula, 24
divergence, 505
divergence theorem, 506
Dolittle’s method, 157
domain, 262, 307
dominant eigenvalue, 552
donut, 491
dot product, 39
echelon form, 86
eigenspace, 213
eigenvalue, 210
eigenvalues, 241
eigenvector, 210
Einstein summation convention, 60
elementary matrices, 138
entries of a matrix, 122
equality of mixed partial derivatives, 325
Eulerian coordinates, 515
Fibonacci sequence, 331
force, 33
force ﬁeld, 464
free variables, 92
Frenet Serret formulas, 290
fundamental theorem line integrals, 468
Gauss Elimination, 94
Gauss elimination, 86
Gauss Jordan method for inverses, 134
Gauss Seidel method, 546
Gauss’s theorem, 506
general solution, 189
geometric multiplicity, 216
Gerschgorin’s theorem, 228
gradient, 321
Green’s theorem, 479, 528
grid, 404, 405, 416
grids, 565
Heine Borel, 293
Hessian matrix, 359, 386
homogeneous equations, 95
implicit function theorem, 390
impulse, 278
inconsistent, 91, 94
increment of area, 412
independent, 116
inner product, 39
intercepts, 76
intercepts of a surface, 259
interior point, 308
inverses and determinants, 206, 237
invertible, 132
iterated integrals, 406
Jacobian, 430
Jacobian determinant, 432
Jocobi method, 543
Jordan content, 574
Jordan set, 573
joule, 47
ker, 188
kernel, 188
kilogram, 56
kinetic energy, 277
Kroneker delta, 60
Lagrange multipliers, 366, 393, 394
Lagrange remainder, 387
Lagrangian coordinates, 514
Laplace expansion, 200, 236
leading entry, 86
Lebesgue number, 577
Lebesgue’s theorem, 578
length of smooth curve, 266
limit of a function, 262, 315
limits and continuity, 317
line integral, 465
linear combination, 96, 111, 160, 234
linear momentum, 278
linear transformation, 182, 372
linearly independent, 167
lizards
surface area, 488
local extremum, 356
local maximum, 356
local minimum, 356
lower sum, 418, 566

INDEX
597
main diagonal, 201
mass ballance, 514
material coordinates, 514
matrix, 121
inverse, 132
left inverse, 238
lower triangular, 200, 238
right inverse, 238
self adjoint, 248
symmetric, 248
upper triangular, 200, 238
migration matrix, 224
minor, 200, 236
mixed partial derivatives, 323
moment of a force, 54
moment of inertia, 447
motion, 515
moving coordinate system, 298
multi-index, 312
nested interval lemma, 328
Newton, 36
second law, 273
Newton’s laws, 273
nondefective eigenvalue, 216
normal vector to plane, 73
null space, 188
nullity, 174
one to one, 182
onto, 182
open set, 308
orientable, 532
orientation, 464
oriented curve, 464
origin, 19
orthogonal, 41
osculating plane, 282, 288
parallelepiped, 52
parameter, 30, 262
parameterization, 262, 295
parametric equation, 30
parametrization, 265, 459
partial derivative, 321
partition of unity, 588
permutation matrices, 138
permutation symbol, 60
perpendicular, 41
Piola Kirchhoﬀstress, 519
pivot, 91
pivot column, 87, 98, 161
pivot position, 87
plane containing three points, 75
planes, 73
polynomials in n variables, 312
position vector, 21, 22, 33
power method, 551
precession of a top, 443
pretentious jargon
hyper plane, 78, 82
oid, 257
principal normal, 282, 289
product rule
cross product, 269
dot product, 269
matrices, 297
projection of a vector, 46
quadric surfaces, 257
radius of curvature, 282, 288
rank of a matrix, 163, 239
rank theorem, 95
raw eggs, 447
recurrence relation, 331
recursively deﬁned sequence, 331
reﬁnement of a grid, 405, 416
reﬁnement of grids, 565
resultant, 35
Riemann criterion, 567
Riemann integral, 405, 416
Riemann integral, 567
right handed system, 48
rot, 505
row equivalent, 98, 162
row operations, 96, 138, 159, 201
row reduced echelon form, 97, 160
saddle point, 359
scalar ﬁeld, 505
scalar multiplication, 20
scalar potential, 468
scalar product, 39
scalars, 20, 121
scaling factor, 552
second derivative test, 388
separable diﬀerential equations, 344
sequences, 330
sequential compactness, 292, 332
sequentially compact, 332
shifted inverse power method, 556
simultaneous corrections, 543
singular point, 357

598
INDEX
skew lines, 81, 100
skew symmetric, 131
smooth curve, 265, 295, 459
smooth surface, 485
solution set, 83
solution space, 188
spacial coordinates, 515
span, 111, 160, 234
spanning set, 111
spectrum, 210
speed, 36
spherical coordinates, 381
standard matrix, 372
standard position, 33
Stoke’s theorem, 530
support of a function, 587
symmetric, 131
symmetric form of a line, 31, 32
torque vector, 54
torsion, 289
torus, 491
trace of a surface, 259
traces, 76
triangle inequality, 26, 43
unit tangent vector, 282, 289
unit vector, 28
upper sum, 418, 566
vector, 21
vector ﬁeld, 462, 464, 505
vector ﬁelds, 463
vector potential, 536
vector valued function
continuity, 312
derivative, 263
integral, 263
limit theorems, 316
vectors, 32
velocity, 36
volume element, 432
work, 465

